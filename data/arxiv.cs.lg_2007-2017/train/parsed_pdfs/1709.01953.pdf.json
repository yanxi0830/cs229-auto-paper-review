{
  "name" : "1709.01953.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Implicit Regularization in Deep Learning",
    "authors" : [ "Behnam Neyshabur", "Yury Makarychev", "Ruslan Salakhutdinov", "Gregory Shakhnarovich" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Implicit Regularization in Deep Learning\nby\nBehnam Neyshabur\nA thesis submitted in partial fulfillment of the requirements for\nthe degree of\nDoctor of Philosophy in Computer Science\nat the\nTOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO\nAugust, 2017\nThesis Committee: Nathan Srebro (Thesis advisor),\nYury Makarychev, Ruslan Salakhutdinov,\nGregory Shakhnarovich\nar X\niv :1\n70 9.\n01 95\n3v 1\n[ cs\n.L G\n] 6\nS ep\n2 01\n7\nImplicit Regularization in Deep Learning by\nBehnam Neyshabur\nAbstract\nIn an attempt to better understand generalization in deep learning, we study several possible explanations. We show that implicit regularization induced by the optimization method is playing a key role in generalization and success of deep learning models. Motivated by this view, we study how different complexity measures can ensure generalization and explain how optimization algorithms can implicitly regularize complexity measures. We empirically investigate the ability of these measures to explain different observed phenomena in deep learning. We further study the invariances in neural networks, suggest complexity measures and optimization algorithms that have similar invariances to those in neural networks and evaluate them on a number of learning tasks.\nThesis Advisor: Nathan Srebro Title: Professor\nIn memory of my dear friend, Sina Masihabadi\n4\nAcknowledgments\nI would like to thank my advisor Nati Srebro without whom I would not be even pursuing a PhD in the first place. Nati’s classes made me interested in optimization and machine learning and later I was delighted when he agreed to be my advisor. From the very first few meetings, I realized how his excitement and passion for research energizes me. I will not forget our countless long meetings that did not seem long to me at all. Looking back, I think Nati was the best mentor I could have hoped for. Among the skills I started to pick up from Nati, my favorite is asking the right question and formalizing it. About three years ago, excited about recent advances in deep learning, I walked into Nati’s office and I told him that I want to understand what makes deep learning models perform well in practice. He helped me to ask the right questions and formalize them. Nati has had a great influence in shaping my thoughts and therefore this dissertation. I am forever grateful for all I have learned from him.\nI would like to thank all of my committee members from whom I benefited during my PhD study. I am thankful to Ruslan Salakhutdinov who has been advising me in several projects and I have benefited a lot from discussions with him and his advices regarding my career. I thank Yury Makarychev for his collaborations and advices during early years of PhD. I always felt free to knock at his office door whenever I was stuck with theoretical questions and needed more insight. I thank Greg Shakhnarovich who has been kind enough to answer my questions on deep learning and metric learning whenever I showed up at his office. Moreover, I think his well-prepared machine learning course had a great impact on making me interested in machine learning.\nTTIC’s faculty have a close and friendly relationship with students and I have benefited from that. I am thankful to Jinbo Xu, my interim advisor who encouraged me to continue PhD in the area that fits my interests the best. I am pleased that we continued collaboration on computational biology projects. Madhur Tulsiani deserves a special thanks for being a great director of graduate studies. Madhur’s desire to improve the PhD program at TTIC and his support made me feel comfortable and free to discuss my thoughts and suggestions with him many times. I would like to acknowledge that I really enjoyed the mathematical foundation course taught by David McAllester and his views on deep learning which he discussed in his deep learning course at TTIC. I regret that I only started collaborating with him in last few months of my PhD and I wish I would have had more discussions with him during these years. I was not able to collaborate with Karen Livescu and Kevin Gimpel during my PhD years but I have enjoyed many conversations with them and I felt supported by them. I also thank Sadaoki Furui, Julia Chuzhoy and Matthew Walter for their effort in enhancing TTIC’s PhD program. I had the pleasure of chatting with Avrim Blum a few times since he has joined TTIC and I am excited about his new appointment at TTIC.\nDuring my PhD years, I have enjoyed collaborating with several research faculties at TTIC. I would like to thank Srinadh Bhojanapalli with whom I have spent a lot of time in the last two years for being a good friend, mentor and collaborator. I am very grateful for everything he has offered me. My earlier works in deep learning was in collaboration with Ryota Tomioka and I am thankful his help and support. Suriya Gunasekar is another research faculty at TTIC who has been an amazing friend and collaborator. Other than her help and support, I have also enjoyed countless discussions with her on random topics. I would like to thank Aly Khan and Ayan Chakrabarti for what I have learned from them during our collaborations on different projects. Finally, I have learned from and enjoyed chatting with many other research faculty and postdocs at TTIC including but not limited to Michael Maire, Hammad Naveed, Mesrob Ohannessian, Mehrdad Mahdavi, Weiran Wang, Herman Kamper, Ofer Meshi, Qixing Huangi, Subhransu Maji, Raman Arora, and George Papandreou.\nI would like to thank TTIC students. I am indebted to Payman Yadollahpour who kindly hosted me and my wife, Somaye, for several days after we arrived to US and helped us numerous times in various occasions. I am grateful for knowing him and for all the moments we have shared. Hao Tang also deserves a special thanks. He was the person I used to discuss all my ideas and thoughts with.\n5\nTherefore, everything presented in this thesis is somehow impacted by the discussions with him. I thank Mrinalkanti Ghosh for the countless times I interrupted his work with a theoretical question and he patiently helped me try to investigate it. Shubhendu Trivedi is another student whom I had several fruitful discussion with. I am also thankful for things I have learned from and memories I have shared with Bahador Nooraei, Mohammadreza Mostajabi, Haris Angelidakis, Vikas Garg, Kaustav Kundu, Abhishek Sen, Siqi Sun, Hai Wang, Heejin Choi, Qingming Tang, Lifu Tu, Blake Woodworth, Shubham Toshniwal, Shane Settle, Nicholas Kolkin, Falcon Dai, Charles Schaff, Rachit Nimavat, and Ruotian Luo.\nI want to thank TTIC staff for making my life much easier and helping me whenever I had any problems. On top of the list is Chrissy Novak. She has always patiently and kindly listened to my complaints, suggestions, and problems, and tried her best to resolve or improve any issues. Adam Bohlander has always been helpful and quick in resolving any IT issues. I think Adam’s great expertise has saved me several hundreds of hours. Liv Leader was one of TTIC’s staff when we arrived to US. She helped us during the first few months of being in US. The first party we were invited in US was Liv’s housewarming party. I also thank Mary Marre, Amy Minick, Jessica Johnston and other TTIC staffs for their effort.\nBeyond TTIC, I am grateful for my internship at MSR Silicon Valley with Rina Panigrahy. Perhaps, several hundred hours of meeting and discussions with Rina in this internship whose goal was to better understand neural networks theoretically had a great influence on making me interested in deep learning. I am also thankful for collaboration and discussions I had with Robert Schapire, Alekh Agarwal, Haipeng Luo and John Langford during my internship at MSR New York City. I am grateful to Anna Choromanska for several discussions on neural networks and for great career advices I received from her. I thank Tony Wu for being a good friend and collaborator. I have learned a lot about deep learning from several meetings with him.\nMy deepest gratitude goes to my parents and my brother who have shaped who I am today. Words cannot capture how grateful I am to Somaye without whom everything in my life would have been drastically different. I will thank her in person.\nI am dedicating this thesis to the memory of Sina Massihabadi. Sina was a friend of mine and we were in the same high school and university. He was one of the finest human beings I have ever met. Later, he moved to US and started his PhD in industrial engineering in Texas A&M university. He was not able to finish his PhD as he died in a tragic car accident when he was driving to the airport to pick up his mother whom he was not able to visit for several years due to visa restrictions on Iranians.\n6\nContents"
    }, {
      "heading" : "1 Introduction 12",
      "text" : "1.1 Main Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 13"
    }, {
      "heading" : "2 Preliminaries 15",
      "text" : "2.1 The Statistical Learning Framework . . . . . . . . . . . . . . . . . . 15\n2.2 Feedforward Neural Networks with Shared Weights . . . . . . . . . . 16\nI Implicit Regularization and Generalization 18"
    }, {
      "heading" : "3 Generalization and Capacity Control 19",
      "text" : "3.1 VC Dimension: A Cardinality-Based Arguments . . . . . . . . . . . 19\n3.2 Norms and Margins: Counting Real-Valued Functions . . . . . . . . . 20\n3.3 Robustness: Lipschitz Continuity with Respect to Input . . . . . . . . 21\n3.4 PAC-Bayesian Framework: Sharpness with Respect to Parameters . . 22"
    }, {
      "heading" : "4 On the Role of Implicit Regularization in Generalization 25",
      "text" : "4.1 Network Size and Generalization . . . . . . . . . . . . . . . . . . . . 26\n4.2 A Matrix Factorization Analogy . . . . . . . . . . . . . . . . . . . . 29"
    }, {
      "heading" : "5 Norm-based Capacity Control 31",
      "text" : "5.1 Group Norm Regularization . . . . . . . . . . . . . . . . . . . . . . 32\n5.2 Per-Unit and Path Regularization . . . . . . . . . . . . . . . . . . . . 36\n5.3 Overall Regularization . . . . . . . . . . . . . . . . . . . . . . . . . 38\n5.4 Depth Independent Regularization . . . . . . . . . . . . . . . . . . . 40\n5.5 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n7\n5.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51"
    }, {
      "heading" : "6 Sharpness/PAC-Bayes Generalization Bounds 52",
      "text" : "6.1 Spectrally-Normalized Margin Bounds . . . . . . . . . . . . . . . . . 52\n6.2 Generalization Bound based on Expected Sharpness . . . . . . . . . . 53\n6.3 Supporting Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n6.4 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56"
    }, {
      "heading" : "7 Empirical Investigation 63",
      "text" : "7.1 Complexity Measures . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n7.2 Experiments Settings . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n7.3 True Labels Vs. Random Labels . . . . . . . . . . . . . . . . . . . . 65\n7.4 Different Global Minima . . . . . . . . . . . . . . . . . . . . . . . . 66\n7.5 Increasing Network Size . . . . . . . . . . . . . . . . . . . . . . . . 67"
    }, {
      "heading" : "II Geometry of Optimization and Generalization 69",
      "text" : ""
    }, {
      "heading" : "8 Invariances 70",
      "text" : "8.1 Invariances in Feedforward and Recurrent Neural Networks . . . . . . 70\n8.2 Understanding Invariances . . . . . . . . . . . . . . . . . . . . . . . 72\n8.3 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76"
    }, {
      "heading" : "9 Path-Normalization for Feedforward and Recurrent Neural Networks 80",
      "text" : "9.1 Path-regularizer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n9.2 Path-SGD for Feedforward Networks . . . . . . . . . . . . . . . . . 81\n9.3 Extending to Networks with Shared Weights . . . . . . . . . . . . . . 81\n9.4 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83"
    }, {
      "heading" : "10 Experiments on Path-SGD 87",
      "text" : "10.1 Experiments on Fully Connected Feedforward Networks . . . . . . . 87\n10.2 Experiments on Recurrent Neural Networks . . . . . . . . . . . . . . 90"
    }, {
      "heading" : "11 Data-Dependent Path Normalization 93",
      "text" : "11.1 A Unified Framework . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n8\n11.2 DD-Path Normalization: A Batch Normalization Approach . . . . . . 96\n11.3 DD-Path-SGD: A Steepest Descent Approach . . . . . . . . . . . . . 98\n11.4 Node-wise invariance . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n11.5 Supporting Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\nConclusion 103\nBibliography 104\n9\nList of Figures\n4.1 The training and test error of a two-layer perceptron for varying number of hidden units . . 27\n4.2 The generalization behavior a two-layer perceptron for varying number of hidden units . . . 28\n6.1 Verifying the conditions of Theorem 27 on a multi-layer perceptron . . . . . . . . . . 54\n6.2 Condition C1 in Theorem 27 on a learned network. . . . . . . . . . . . . . . . . . 57\n6.3 Ratio of activations that flip based on the magnitude of perturbation. . . . . . . . . . . 57\n6.4 Condition C3 in Theorem 27 for random initialization and learned network . . . . . . . 58\n7.1 Comparing complexity measures on a VGG network trained with true or random labels. . . 66\n7.2 Sharpness and PAC-Bayes measures on a VGG network trained with true or random labels. . 66\n7.3 Experiments on global minima with poor generalization. . . . . . . . . . . . . . . . 67\n7.4 The effect of increasing network size on generalization. . . . . . . . . . . . . . . . 68\n8.1 Invariance in fully connected feedforward networks . . . . . . . . . . . . . . . . . 71\n8.2 Invariances in recurrent neural networks (RNNs). . . . . . . . . . . . . . . . . . . 72\n8.3 A 3 layer network with 10 parameters and 8 paths. . . . . . . . . . . . . . . . . . 76\n8.4 Schematic illustration of the linear dependence of four paths. . . . . . . . . . . . . . 79\n10.1 Comparing Path-SGD to other optimization methods on 4 dataset without dropout . . . . . 88\n10.2 Comparing Path-SGD to other optimization methods on 4 dataset with dropout . . . . . . 89\n10.3 The contribution of the second term. . . . . . . . . . . . . . . . . . . . . . . . 90\n10.4 Test errors for the addition problem of different lengths. . . . . . . . . . . . . . . . 91\n11.1 An example of layered feedforward network and notation used in the chapter . . . . . . . 94\n10\nList of Tables\n2.1 Forward computations for feedforward and recurrent networks . . . . . . . . . . . . 17\n10.1 General information on datasets used in the experiments on feedforward networks. . . . . 88\n10.2 Test error (MSE) for the adding problem and test classification error for the sequential MNIST. 91\n10.3 Test BPC for PTB and text8. . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n11.1 Some of the choices for Rv in the proposed unified framework. . . . . . . . . . . . . 95\n11\nChapter 1\nIntroduction\nDeep learning refers to training typically complex and highly over-parameterized models that benefit from learning a hierarchy of representations. The terms “neural networks” and “deep learning” are often used interchangeably as many modern deep learning models are slight modifications of different types of neural networks suggested originally around 1950-2000 [1]. Interest in deep learning was revived around 2006 [2, 3] and since then, it has had enormous practical successes in different areas [4]. The rapid growth of practical works and new concepts in this field has created a considerable gap between our theoretical understanding of deep learning and practical advances.\nFrom the learning viewpoint, we often look into three different properties to investigate the effectiveness of a model: expressive power, optimization, and generalization. Given a function class/model the expressive power is about understanding what functions can be realized or approximated by the functions in the function class. Given a loss function as an evaluation measure, the optimization aspect refers to the ability to efficiently find a function with a minimal loss on the training data and generalization addresses the model’s ability to perform well on the new unseen data.\nAll above aspects of neural networks have been studied before. Neural networks have great expressive power. Universal approximation theorem states that for any given precision, feed-forward networks with a single hidden layer containing a finite number of hidden units can approximate any continuous function [5]. More broadly, any O(T ) time computable function can be captured by an O(T 2) sized network, and so the expressive power of such networks is indeed great [6, Theorem 9.25].\nGeneralization of neural networks as a function of network size is also fairly well understood. With hard-threshold activations, the VC-dimension, and hence sample complexity, of the class of functions realizable with a feed-forward network is equal, up to logarithmic factors, to the number of edges in the network [7, 8], corresponding to the number of parameters. With continuous activation functions the VC-dimension could be higher, but is fairly well understood and is still controlled by the size of the network.1\nAt the same time, we also know that learning even moderately sized networks is computationally intractable—not only is it NP-hard to minimize the empirical error, even with only three hidden units, but it is hard to learn small feed-forward networks using any learning method (subject to cryptographic assumptions). That is, even for binary classification using a network with a single hidden layer and a logarithmic (in the input size) number of hidden units, and even if we know the true targets are exactly captured by such a small network, there is likely no efficient algorithm that can ensure error better than 1/2 [10, 11]—not if the algorithm tries to fit such a network, not even if it tries to fit a much larger\n1Using weights with very high precision and vastly different magnitudes it is possible to shatter a number of points quadratic in the number of edges when activations such as the sigmoid, ramp or hinge are used [8, Chapter 20.4]. But even with such activations, the VC dimension can still be bounded by the size and depth [9, 7, 8].\n12\nnetwork, and in fact no matter how the algorithm represents predictors. And so, merely knowing that some not-too-large architecture is excellent in expressing does not explain why we are able to learn using it, nor using an even larger network. These results, however, are not suggesting any insights on the practical success of deep learning. In contrast to our theoretical understanding, it is possible to train (optimize) very large neural networks and despite their large sizes, they generalize to unseen data. Why is it then that we succeed in learning very large neural networks? Can we identify a property that makes them possible to train? Why do these networks generalize to unseen data despite their large capacity in terms of the number of parameters?\nIn such an over-parameterized setting, the objective has multiple global minima, all minimize the training error, but many of them do not generalize well. Hence, just minimizing the training error is not sufficient for learning: picking the wrong global minima can lead to bad generalization behavior. In such situations, generalization behavior depends implicitly on the algorithm used to minimize the training error. Different algorithmic choices for optimization such as the initialization, update rule, learning rate, and stopping condition, will lead to different global minima with different generalization behavior [12, 13, 14].\nWhat is the bias introduced by these algorithmic choices for neural networks? What is the relevant notion of complexity or capacity control?\nThe goal of this dissertation is to understand the implicit regularization by studying the optimization, regularization, and generalization in deep learning and the relationship between them. The dissertation is divided into two parts. In the first part, we study different approaches to explain generalization in neural networks. We discuss how some of the complexity measures derived by these approaches can explain implicit regularization. In the second part, we investigate the transformations under which the function computed by a network remains the same and therefore argue for complexity measures and optimization algorithms that have similar invariances. We find complexity measures that have similar invariances to neural networks and optimization algorithms that implicitly regularize them. Using these optimization algorithms for different learning tasks, we indeed observe that they have better generalization abilities.\n1.1 Main Contributions\n1. Part I:\n(a) The Role of Implicit Regularization (Chapter 4) We design experiments to highlight the role of implicit regularization in the success of deep learning models.\n(b) Norm-based capacity control (Chapter 5): We prove generalization bounds for the class of fully connected feedforward networks with the bounded norm. We further show that for some norms, this bound is independent of the number of hidden units.\n(c) Generalization Guarantee by PAC-Bayes Framework (Chapter 6): We show how PAC-Bayes framework can be employed to obtain generalization bounds for neural networks by making a connection between sharpness and PAC-Bayes theory.\n(d) Implicit Regularization by SGD (Chapter 6): We show that networks learned by SGD satisfy several conditions that lead to flat minima.\n(e) Empirical Investigation of Generalization in Deep Learning (Chapter 7): We design experiments to compare the ability of different complexity measures to explain the implicit regularization and generalization in deep learning.\n2. Part II:\n(a) Invariances in neural networks (Chapter 8): We characterize a large class of invariances in feedforward and recurrent neural networks caused by rescaling issues and suggest a measure called the Path-norm that is invariant to the rescaling of the weights.\n13\n(b) Path-normalized optimization (Chapter 9 and 10): Inspired by our understanding of invariances in neural networks and the importance of implicit regularization, we suggest a new method called Path-SGD whose updates are the approximate steepest descent direction with respect to the Path-norm. We show Path-SGD achieves better generalization error than SGD in both fully connected and recurrent neural networks on different benchmarks.\n(c) Data-dependent path normalization (Chapter 11): We propose a unified framework for neural net normalization, regularization, and optimization, which includes Path-SGD and BatchNormalization and interpolates between them across two different dimensions. Through this framework, we investigate the issue of invariance of the optimization, data dependence and the connection with natural gradient.\n14\nChapter 2\nPreliminaries\nIn this chapter, we present the basic setup and notations used throughout this dissertation."
    }, {
      "heading" : "2.1 The Statistical Learning Framework",
      "text" : "In this section, we briefly review the statistical learning framework. More details on the formal model can be found in Shalev-Shwartz and Ben-David [8].\nIn the statistical batch learning framework, the learner is given a training set S = {(x1,y1), . . . , (xm,ym)} of m training points in X × Y that are independently and identically distributed (i.i.d.) according to an unknown distribution D. For simplicity, we will focus on the task of classification where the goal of the learner is to output a predictor f : X → Y with minimum expected error on samples generated from the distribution D: LD(f) = P(x,y)∼D [f(x) 6= y] (2.1.1) Since the learner does not have access to the distribution D, it cannot evaluate or minimize the expected loss. It can however, obtain an estimate of the expected loss of a predictor f using the training set S:\nLS(f) = |{(x,y) ∈ S | f(x) 6= y}|\nm (2.1.2)\nWhen the distribution D and training set S is clear from the context, we use L(f) and L̂(f) instead of LD and LS(f) respectively. We also define the expected margin loss for any margin γ > 0, as follows:\nLγ(f) = P(x,y)∼D [ f(x)[y] ≤ γ + max\nj 6=y f(x)[j]\n] (2.1.3)\nLet L̂γ(f) be the empirical estimate of the above expected margin loss. Since setting γ = 0 corresponds to the classification loss, we will use L0(f) = L(f) and L̂0(fw) = L̂(f) to refer to the expected risk and the training error.\nMinimizing the loss in the equation (2.1.2) which is called the training error does not guarantee low expected error. For example, a predictor that only memorizes the set S to output the right label for the data in the training set can get zero training error while its expected loss might be very close to the random guess. We are therefore interested in controlling the difference LD(f)− LS(f) which we will refer to as generalization error. This quantity reflects the difference between memorizing and learning.\nAn interesting observation is that if f is chosen in advance and is not dependent on the distribution D or\n15\ntraining set S , then the generalization error can be simply bounded by concentration inequalities such as Hoeffding’s inequality and relatively small number of samples are required to get small generalization error. However, since the predictor is chosen by the learning algorithm using the training set S, one need to make sure that this bound holds for the set all predictors that could be chosen by the learning algorithm. It is therefore preferred to limit the search space of the learning algorithm to a small enough set F of predictors called model class to be able to bound the generalization. We consider the statistical capacity of a model class in terms of the number of examples required to ensure generalization, i.e. that the population (or test error) is close to the training error, even when minimizing the training error. This also roughly corresponds to the maximum number of examples on which one can obtain small training error even with random labels.\nIn the next section, we define a meta-model class of feedforward networks with shared weights that include several well-known model classes such as fully connected, convolutional and recurrent neural networks."
    }, {
      "heading" : "2.2 Feedforward Neural Networks with Shared Weights",
      "text" : "We denote a feedforward network by a triple (G,w, σ) where G = (V,E) is a directed acyclic graph over the set of nodes V that corresponds to units v ∈ V in the network, including special input nodes Vin ⊂ V with no incoming edges and special output nodes Vout ⊆ V with no outgoing edges, w : E → R is the weights assigned to the edges and σ : R→ R is an activation function. Feedforward network (G,w, σ) computes the function fG,w,σ : Rnin → Rnout for a given input vector x ∈ Rnin as follows: For any input node v ∈ Vin, its output hv is the corresponding coordinate of x 1; for any internal node v (all nodes except the input and output nodes) the output value is defined according to the forward propagation equation:\nhv = σ  ∑ (u→v)∈E wu→v · hu  (2.2.1) and for any output node v ∈ Vout, no non-linearity is applied and its output hv = ∑ (u→v)∈E wu→v · hu corresponds to coordinates of the computed function fG,w,σ(x). When the graph structure G and the activation function σ is clear from the context, we use the shorthand fw = fG,w,σ to refer to the function computed by weights w.\nWe will focus mostly on the hinge, or RELU (REctified Linear Unit) activation, which is currently in popular use [15, 16, 17], σRELU(z) = [z]+ = max(z, 0). When the activation will not be specified, we will implicitly be referring to the RELU. The RELU has several convenient properties which we will exploit, some of them shared with other activation functions:\nLipshitz ReLU is Lipschitz continuous with Lipschitz constant one. This property is also shared by the sigmoid and the ramp activation σ(z) = min(max(0, z), 1).\nIdempotency ReLU is idempotent, i.e. σRELU(σRELU(z)) = σRELU(z). This property is also shared by the ramp and hard threshold activations. Non-Negative Homogeneity For a non-negative scalar c ≥ 0 and any input z ∈ R we have σRELU(c · z) = c · σRELU(z). This property is important as it allows us to scale the incoming weights to a unit by c > 0 and scale the outgoing edges by 1/c without changing the function computed by the network. For layered graphs, this means we can scale Wi by c and compensate by scaling Wi+1\nby 1/c.\n1We might want to also add a special “bias” node with hbias = 1, or just rely on the inputs having a fixed “bias coordinate”.\n16\nWhen investigating a class of feedforward networks, in order to account for weight sharing, we separate the weights from actual parameters. Given a parameter vector θ ∈ Rnparam and a mapping π : E → {1, . . . , nparam} from edges to parameter indices, the weight of any edge e ∈ E is we = θπ(e). We also refer to the set of edges that share the ith parameter θi as Ei = {e ∈ E|π(e) = i}. That is, for any e1, e2 ∈ Ei, π(e1) = π(e2) and therefore we1 = we2 = θπ(e1). Given a graph G, activation function σ and mapping π, we consider the hypothesis class FG,σ,π = { fG,w,σ|θ ∈ Rk;∀e∈E w(e) = θπ(e)\n} of functions computable using some setting of parameters. When π is a one-to-one mapping, we use weights w to refer to the parameters θ and drop π and use FG,σ to refer to the hypothesis class. We will refer to the size of the network, which is the overall number of edges nedge = |E|, the depth d of the network, which is the length of the longest directed path in G, and the in-degree (or width) H of a network, which is the maximum in-degree of a vertex in G.\nIf the mapping π is a one-to-one mapping, then there is no weight sharing and it corresponds to standard feedforward networks. Fully connected neural networks (FCNNs) are a well-known family of standard feedforward networks in which every hidden unit in each layer is connected to all hidden units in the previous and next layers. On the other hand, weight sharing exists if π is a many-to-one mapping. Two well-known examples of feedforward networks with shared weights are convolutional neural networks (CNNs) and recurrent neural networks (RNNs). We mostly use the general notation of feedforward networks with shared weights as this will be more comprehensive. However, when focusing on FCNNs or RNNs, it is helpful to discuss them using a more familiar notation which we briefly introduce next.\nFully Connected Neural Networks Let us consider a layered fully-connected network where nodes are partitioned into layers. Let ni be the number of nodes in layer i. For all nodes v on layer i, we recover the layered recursive formula hi = σ ( Wihi−1 ) where hi ∈ Rni is the vector of outputs in layer i and Wi ∈ Rni×ni−1 is the weight matrix in layer i with entries wu→v, for each u in layer i − 1 and v in layer i. This description ignores the bias term, which could be modeled as a direct connection from vbias into every node on every layer, or by introducing a bias unit (with output fixed to 1) at each layer.\nRecurrent Neural Networks Time-unfolded RNNs are feedforward networks with shared weights that map an input sequence to an output sequence. Each input node corresponds to either a coordinate of the input vector at a particular time step or a hidden unit at time 0. Each output node also corresponds to a coordinate of the output at a specific time step. Finally, each internal node refers to some hidden unit at time t ≥ 1. When discussing RNNs, it is useful to refer to different layers and the values calculated at different time-steps. We use a notation for RNN structures in which the nodes are partitioned into layers and hit denotes the output of nodes in layer i at time step t. Let x = (x1, . . . ,xT ) be the input at different time steps where T is the maximum number of propagations through time and we refer to it as the length of the RNN. For 0 ≤ i < d, let Wiin ∈ Rni×ni−1 and Wirec ∈ Rni×ni be the input and recurrent parameter matrices of layer i and Wout ∈ Rnd×nd−1 be the output parameter matrix.The output of the function implemented by RNN can then be calculated as fw,t(x) = hdt . Note that in this notations, weight matrices Win, Wrec and Wout correspond to “free” parameters of the model that are shared in different time steps. Table 2.1 shows forward computations for layered feedforward networks and RNNs.\n17\nPart I\nImplicit Regularization and Generalization\n18\nChapter 3\nGeneralization and Capacity Control\nIn section 2.1 we briefly discussed viewing the statistical capacity of a model class in terms of the number of examples required to ensure generalization. Given a model class F , such as all the functions representable by some feedforward or convolutional networks, one can consider the capacity of the entire class F—this corresponds to learning with a uniform “prior” or notion of complexity over all models in the class. Alternatively, we can also consider some complexity measure, which we take as a mapping that assigns a non-negative number to every predictor in the class - µ : {F ,S} → R+, where S is the training set. It is then sufficient to consider the capacity of the restricted class Fµ,α = {f : f ∈ F , µ(f) ≤ α} for a given α ≥ 0. One can then ensure generalization of a learned predictor f in terms of the capacity of Fµ,µ(f). Having a good predictor with low complexity, and being biased toward low complexity (in terms of µ) can then be sufficient for learning, even if the capacity of the entire F is high. And if we are indeed relying on µ for ensuring generalization (and in particular, biasing toward models with lower complexity under µ), we would expect a learned f with a lower value of µ(f) to generalize better.\nFor some complexity measures, we allow µ to depend also on the training set. If this is done carefully, we can still ensure generalization for the restricted class Fµ,α. When considering a complexity measure µ, we can investigate whether it is sufficient for generalization, and analyze the capacity of Fµ,α. Understanding the capacity corresponding to different complexity measures also allows us to relate between different measures and provides guidance as to what and how we should measure: From the above discussion, it is clear that any monotone transformation of a complexity measures leads to an equivalent notion of complexity. Furthermore, complexity is meaningful only in the context of a specific model class F , e.g. specific architecture or network size. The capacity, as we consider it (in units of sample complexity), provides a yardstick by which to measure complexity (we should be clear though, that we are vague regarding the scaling of the generalization error itself, and only consider the scaling in terms of complexity and model class, thus we obtain only a very crude yardstick sufficient for investigating trends and relative phenomena, not a quantitative yardstick).\nWe next look at different ways of controlling the capacity."
    }, {
      "heading" : "3.1 VC Dimension: A Cardinality-Based Arguments",
      "text" : "Consider a finite model class F . Given any predictor f ∈ F , training error L(f) is the average of independent random variables and the expected error the excepted value of the training error. We can therefore use Hoeffding’s inequality upper bounds the generalization error with high probability:\nP [ L(f)− L̂(f) ≥ t ] ≤ e−2mt2 (3.1.1)\n19\nThe above bound is for any given f . However, since the learning algorithm can output any predictor from class F , we need to make sure that all predictors in F have low generalization error which can be done through a union bound over model class F :\nP [ ∃f∈F L(f)− L̂(f) ≥ t ] ≤ ∑ f∈F P [ L(f)− L̂(f) ≥ t ] ≤ |F|e−2mt2 (3.1.2)\nSetting the r.h.s. of the above inequality to small probability 0 < δ < 1, we can say that with probability 1− δ over the choice of samples in the training set, the following generalization bound holds:\nL(f) ≤ L̂(f) + √\nln |F|+ ln(1/δ) m\n(3.1.3)\nThe above simple yet effective approach gives us an intuition about the relationship between the capacity and generalization. Many of the approaches of controlling the capacity that we will study later follow similar arguments. Here, the term ln |F| corresponds to the complexity of the model class. Even though many model classes that we consider are not finite based on the definition, one can argue that all parametrized model classes used in practice are finite since the parameters are stored with finite precision For any model, if b bits are used to store each parameter, then we have ln |F| ≤ bnparam which is is linear in the total number of parameters.\nEven without making an assumption on the precision of parameters, it is possible to get similar generalization bound using Vapnik-Chervonenkis dimension (VC dimension) which can be thought as the logarithm of the“intrinsic” cardinality. VC-dimension is defined as the size of the largest setW = {xi}mi=1 such that for any mapping g :W → {±}m, there is a predictor in F that achieves zero training error on the training set S = {(xi, g(xi) | xi ∈ W}. The VC-dimension of many known model classes is a linear or low-degree polynomial of the number of parameters. The following generalization bound then holds with probability 1− δ [18, 19]:\nL(f) ≤ L̂(f) +O (√\nVC-dim(F) lnm+ ln(1/δ) m\n) (3.1.4)\nFeedforward Networks The VC dimension of feedforward networks can also be bounded in terms of the number of parameters nparam[20, 21, 22, 23]. In particular, Bartlett [24] and Harvey et al. [25], following Bartlett et al. [22], give the following tight (up to logarithmic factors) bound on the VC dimension and hence capacity of feedforward networks with ReLU activations:\nVC-dim = Õ(d ∗ nparam) (3.1.5)\nIn the over-parametrized settings, where the number of parameters is more than the number of samples, complexity measures that depend on the total number of parameters are too weak and cannot explain the generalization behavior. Neural networks used in practice often have significantly more parameters than samples, and indeed can perfectly fit even random labels, obviously without generalizing [26]. Moreover, measuring complexity in terms of number of parameters cannot explain the reduction in generalization error as the number of hidden units increase [27]. We will discuss more details about network size as the capacity control in Chapter 4."
    }, {
      "heading" : "3.2 Norms and Margins: Counting Real-Valued Functions",
      "text" : "The model classes that we learn are often functions with real-valued outputs and for each task, we use a different loss and prediction method based on the predicted scores. For example, for the binary\n20\nclassification, thresholding the only real-valued output gives us the binary labels. For the multi-class classification, the output dimension is usually equal to the number of classes and the class with maximum score is chosen as the predicted label. For simplicity, we focus on binary classification here. Since the model class has real-valued output, can not directly use VC-dimension here. Instead, we can use a similar concept called subgraph VC-dimension which is similar to VC-dimension with the difference being that here we count the number of different behavior with a given margin. This means for the binary case, we require yf(x) ≥ η for some margin η. There are different techniques that bound subgraph-VC dimension such as Covering Numbers and Rademacher Complexities. Here, we focus on the Rademacher Complexity since most of the results by Covering Numbers can be also proved through Rademacher complexities with less effort. The empirical Rademacher complexity of a class F of function mapping from X to R with respect to a set {x1, . . . , xm} is defined as:\nRm(F) = Eξ∈{±1}m [ 1\nm sup f∈F ∣∣∣∣∣ m∑ i=1 ξif(xi) ∣∣∣∣∣ ]\n(3.2.1)\nThe relationship between Rademacher complexity and subgraph VC-dimension is as follows: Rm(F) = O (√\nVC-dim(F) m\n) (3.2.2)\nIt is possible to get the following generalization error for any margin γ > 0 with probability 1− δ over the choice of training examples for every f ∈ F :\nL0(f) ≤ L̂γ(f) + 2 Rm(F)\nγ +\n√ 8 ln(2/δ)\nm (3.2.3)\nFeedforward Networks [28] proved that the Rademacher complexity of fully connected feedforward networks on set S can be bounded based on the `1 norm of the weights of hidden units in each layer as follows:\nRm(F) ≤\n√ 4d ln (nin) ∏d i=1 ‖Wi‖ 2 1,∞maxx∈S ‖x‖∞\nm (3.2.4)\nwhere ‖Wi‖1,∞ is the maximum over hidden units in layer i of the `1 norm of incoming weights to the hidden unit [28]. This suggests that the capacity scales roughly as ∏d i=1 ‖Wi‖ 2 1,∞. In Chapter 5 we show how the capacity can be controlled for a large family of norms."
    }, {
      "heading" : "3.3 Robustness: Lipschitz Continuity with Respect to Input",
      "text" : "Some of the measures/norms also control the Lipschitz constant of the model class with respect to its input such as the capacity based on (3.2.4). Is the capacity control achieved through the bound on the Lipschitz constant? Is bounding the Lipschitz constant alone enough for generalization? To answer these questions, and in order to understand capacity control in terms of Lipschitz continuity more broadly, we review here the relevant guarantees.\nGiven an input space X and metricM, a function f : X → R on a metric space (X ,M) is called a Lipschitz function if there exists a constant CM, such that |f(x)− f(y)| ≤ CMM(x, y). Luxburg and Bousquet [29] studied the capacity of functions with bounded Lipschitz constant on metric space (X ,M) with a finite diameter diamM(X ) = supx,y∈XM(x, y) and showed that the capacity is proportional to ( CM γ )n diamM(X ) where γ is the margin. This capacity bound is weak as it has an exponential dependence on input size.\n21\nAnother related approach is through algorithmic robustness as suggested by Xu and Mannor [30]. Given > 0, the model fw found by a learning algorithm is K robust if X can be partitioned into K disjoint sets, denoted as {Ci}Ki=1, such that for any pair (x, y) in the training set s ,1\nx, z ∈ Ci ⇒ |`(w,x)− `(w, z)| ≤ (3.3.1)\nXu and Mannor [30] showed the capacity of a model class whose models are K-robust scales as K. For the model class of functions with bounded Lipschitz C‖.‖, K is proportional to C‖.‖ γ -covering number of the input domain X under norm ‖.‖ where γ is the margin to get error . However, the covering number of the input domain can be exponential in the input dimension and the capacity can still grow as( C‖.‖ γ )n 2.\nFeedforward Networks Returning to our original question, theC`∞ andC`2 Lipschitz constants of the network can be bounded by ∏d i=1 ‖Wi‖1,∞ (hence `1-path norm) and ∏d i=1 ‖Wi‖2, respectively [30, 31].\nThis will result in a very large capacity bound that scales as (∏d\ni=1‖Wi‖2 γ\n)n , which is exponential in both\nthe input dimension and depth of the network. This shows that simply bounding the Lipschitz constant of the network is not enough to get a reasonable capacity control and the capacity bounds of the previous Section are not merely a consequence of bounding the Lipschitz constant."
    }, {
      "heading" : "3.4 PAC-Bayesian Framework: Sharpness with Respect to Parameters",
      "text" : "The notion of sharpness as a generalization measure was recently suggested by Keskar et al. [13] and corresponds to robustness to adversarial perturbations on the parameter space:\nζα(W) = max|ui|≤α(|wi|+1) L̂(fw+u)− L̂(fw)\n1 + L̂(fw) ' max |ui|≤α(|wi|+1) L̂(fw+u)− L̂(fw), (3.4.1)\nwhere the training error L̂(fw) is generally very small in the case of neural networks in practice, so we can simply drop it from the denominator without a significant change in the sharpness value.\nInstead, we advocate viewing a related notion of expected sharpness in the context of the PAC-Bayesian framework. Viewed this way, it becomes clear that sharpness controls only one of two relevant terms, and must be balanced with some other measure such as norm. Together, sharpness and norm do provide capacity control and can explain many of the observed phenomena. This connection between sharpness and the PAC-Bayes framework was also recently noted by Dziugaite and Roy [32].\nThe PAC-Bayesian framework [33, 34] provides guarantees on the expected error of a randomized predictor (hypothesis), drawn from a distribution denoted Q and sometimes referred to as a “posterior” (although it need not be the Bayesian posterior), that depends on the training data. Let fw be any predictor (not necessarily a neural network) learned from training data. We consider a distributionQ over predictors with weights of the form w + u, where w is a single predictor learned from the training set, and u is a random variable. Then, given a “prior” distribution P over the hypothesis that is independent of the training data, with probability at least 1− δ over the draw of the training data, the expected error\n1Xu and Mannor [30] have defined the robustness as a property of learning algorithm given the model class and the training set. Here since we are focused on the learned model, we introduce it as a property of the model.\n2Similar to margin-based bounds, we drop the term that depends on the diameter of the input space.\n22\nof fw+u can be bounded as follows [35]:\nEu [L(fw+u)] ≤ Eu [ L̂(fw+u) ] + √ Eu [ L̂(fw+u) ] K +K (3.4.2)\nwhere K = 2(KL(w+u‖P )+ln 2m δ ) m−1 . When the training loss Eu [ L̂(fw+u) ] is smaller than K, then the last term dominates. This is often the case for neural networks with small enough perturbation. One can also get the the following weaker bound:\nEu [L(fw+u)] ≤ Eu [ L̂(fw+u) ] + 2\n√ 2 ( KL (w + u‖P ) + ln 2m\nδ ) m− 1 (3.4.3)\nThe above inequality clearly holds for K ≥ 1 and for K < 1 it can be derived from Equation (3.4.2) by upper bounding the loss in the second term by 1. We can rewrite the above bound as follows:\nEu [L(fw+u)] ≤ L̂(fw) + Eu [ L̂(fw+u) ] − L̂(fw)︸ ︷︷ ︸\nexpected sharpness\n+2\n√ 2\nm− 1\n( KL (w + u‖P ) + ln 2m\nδ\n) (3.4.4)\nAs we can see, the PAC-Bayes bound depends on two quantities - i) the expected sharpness and ii) the Kullback Leibler (KL) divergence to the “prior” P . The bound is valid for any distribution measure P , any perturbation distribution u and any method of choosing w dependent on the training set.\nNext, we present a result that gives a margin-based generalization bound using the PAC-Bayesian framework. The proof of the lemma uses similar ideas as in the proof for the case of linear separators, discussed by Langford and Shawe-Taylor [36] and McAllester [35]. This is a general result that holds for any hypothesis class and not specific to neural networks.\nLemma 1. Let fw(x) : X → Rk be any predictor (not necessarily a neural network) with parameters w and P be any distribution on the parameters that is independent of the training data. For any γ > 0, consider any set Sw of perturbations with the following property:\nSw ⊆ { w + u ∣∣∣∣maxx∈X |fw+u(x)− fw(x)|∞ < γ4 }\nLet u be a random variable such that P [u ∈ Sw] ≥ 12 . Then, for any δ > 0, with probability 1− δ over the training set, the generalization error can be bounded as follows:\nL0(fw) ≤ L̂γ(fw) + 4\n√ KLSw (w + u‖P ) + ln 4mδ\nm− 1\nwhere KLSw(Q||P ) = ∫ Sw q(x) ln q(x) p(x)dx.\nProof. Let q be the probability density function for w + u. We consider the distribution Q̃ with the following probability density function:\nq̃(r) = 1\nZ { q(r) r ∈ Sw 0 otherwise.\nwhere Z is a normalizing constant and by the lemma assumption Z = P [w + u ∈ Sw] ≥ 12 . Therefore, we have: KL(Q̃‖P ) = ∫ q̃(r) ln q̃(r)\np(r) dr ≤ 2 ∫ Sw q(r) ln q(r) p(r) dr + 1 (3.4.5)\n23\nConsider w + ũ to be the random perturbation centered at w drawn from Q̃. By the definition of Q̃, we know that for any perturbation ũ:\nmax x∈X |fw+ũ(x)− fw(x)|∞ <\nγ 4 (3.4.6)\nTherefore, the perturbation ũ can change the margin between two output units of fw by at most γ2 ; i.e. for any perturbation ũ drawn from Q̃:\nmax i,j∈[k],x∈X\n|(|fw+ũ(x)[i]− fw+ũ(x)[j]|)− (|fw(x)[i]− fw(x)[j]|)| < γ\n2\nSince the above bound holds for any x in the domain X , we can get the following inequalities:\nL0(fw) ≤ L γ 2 (fw+ũ) L̂ γ 2 (fw+ũ) ≤ L̂γ(fw)\nNow using the above inequalities together with the equation (3.4), with probability 1− δ over the training set we have:\nL0(fw) ≤ Eũ [ L γ\n2 (fw+ũ) ] ≤ Eũ [ L̂ γ\n2 (fw+ũ)\n] + 2 √ 2(KL (w + ũ‖P ) + ln 2mδ )\nm− 1\n≤ L̂γ(fw) + 2\n√ 2(KL (w + ũ‖P ) + ln 2mδ )\nm− 1\n≤ L̂γ(fw) + 4\n√ KLSw (w + u‖P ) + ln 4mδ\nm− 1 ,\nFeedforward Networks This connection between sharpness and the PAC-Bayesian framework was also recently noticed by Dziugaite and Roy [32], who optimize the PAC-Bayes generalization bound over a family of multivariate Gaussian distributions, extending the work of Langford and Caruana [37]. They show that the optimized PAC-Bayes bounds are numerically non-vacuous for feedforward networks trained on a binary classification variant of MNIST dataset.\n24\nChapter 4"
    }, {
      "heading" : "On the Role of Implicit Regularization",
      "text" : "in Generalization\nCentral to any form of learning is an inductive bias that induces some sort of capacity control (i.e. restricts or encourages predictors to be “simple” in some way), which in turn allows for generalization. The success of learning then depends on how well the inductive bias captures reality (i.e. how expressive is the hypothesis class of “simple” predictors) relative to the capacity induced, as well as on the computational complexity of fitting a “simple” predictor to the training data.\nLet us consider learning with feed-forward networks from this perspective. If we search for the weights minimizing the training error, we are essentially considering the hypothesis class of predictors representable with different weight vectors, typically for some fixed architecture. We showed in Section 3.1 that the capacity can then be controlled by the size (number of weights) of the network. Our justification for using such networks is then that many interesting and realistic functions can be represented by not-too-large (and hence bounded capacity) feed-forward networks. Indeed, in many cases we can show how specific architectures can capture desired behaviors. More broadly, any O(T ) time computable function can be captured by an O(T 2) sized network, and so the expressive power of such networks is indeed great [6, Theorem 9.25].\nAt the same time, we also know that learning even moderately sized networks is computationally intractable—not only is it NP-hard to minimize the empirical error, even with only three hidden units, but it is hard to learn small feed-forward networks using any learning method (subject to cryptographic assumptions). That is, even for binary classification using a network with a single hidden layer and a logarithmic (in the input size) number of hidden units, and even if we know the true targets are exactly captured by such a small network, there is likely no efficient algorithm that can ensure error better than 1/2 [10, 11]—not if the algorithm tries to fit such a network, not even if it tries to fit a much larger network, and in fact no matter how the algorithm represents predictors. And so, merely knowing that some not-too-large architecture is excellent in expressing reality does not explain why we are able to learn using it, nor using an even larger network. Why is it then that we succeed in learning using multilayer feed-forward networks? Can we identify a property that makes them possible to learn? An alternative inductive bias?\nHere, we make our first steps at shedding light on this question by going back to our understanding of network size as the capacity control at play.\nOur main observation, based on empirical experimentation with single-hidden-layer networks of increasing size (increasing number of hidden units), is that size does not behave as a capacity control parameter, and in fact there must be some other, implicit, capacity control at play. We suggest that this hidden capacity control might be the real inductive bias when learning with deep networks.\n25\nIn order to try to gain an understanding at the possible inductive bias, we draw an analogy to matrix factorization and understand dimensionality versus norm control there. Based on this analogy we suggest that implicit norm regularization might be central also for deep learning, and also there we should think of bounded-norm models with capacity independent of number of hidden units."
    }, {
      "heading" : "4.1 Network Size and Generalization",
      "text" : "Consider training a feedforward network by finding the weights minimizing the training error. Specifically, we will consider a fully connected feedforward networks with one hidden layer that includes H hidden units. The weights learned by minimizing a soft-max cross entropy loss 1 on n labeled training examples. The total number of weights is then H(|Vin|+ |Vout|). What happens to the training and test errors when we increase the network size H? The training error will necessarily decrease. The test error might initially decrease as the approximation error is reduced and the network is better able to capture the targets. However, as the size increases further, we loose our capacity control and generalization ability, and should start overfitting. This is the classic approximation-estimation tradeoff behavior.\nConsider, however, the results shown in Figure 4.1, where we trained networks of increasing size on the MNIST and CIFAR-10 datasets. Training was done using stochastic gradient descent with momentum and diminishing step sizes, on the training error and without any explicit regularization. As expected, both training and test error initially decrease. More surprising is that if we increase the size of the network past the size required to achieve zero training error, the test error continues decreasing! This behavior is not at all predicted by, and even contrary to, viewing learning as fitting a hypothesis class controlled by network size. For example for MNIST, 32 units are enough to attain zero training error. When we allow more units, the network is not fitting the training data any better, but the estimation error, and hence the generalization error, should increase with the increase in capacity. However, the test error goes down. In fact, as we add more and more parameters, even beyond the number of training examples, the generalization error does not go up.\nWe also further tested this phenomena under some artificial mutilations to the data set. First, we wanted to artificially ensure that the approximation error was indeed zero and does not decrease as we add more units. To this end, we first trained a network with a small number H0 of hidden units (H0 = 4 on MNIST and H0 = 16 on CIFAR) on the entire dataset (train+test+validation). This network did have some disagreements with the correct labels, but we then switched all labels to agree with the network creating a “censored” data set. We can think of this censored data as representing an artificial source distribution which can be exactly captured by a network with H0 hidden units. That is, the approximation error is zero for networks with at least H0 hidden units, and so does not decrease further. Still, as can be seen in the middle row of Figure 4.2, the test error continues decreasing even after reaching zero training error.\nNext, we tried to force overfitting by adding random label noise to the data. We wanted to see whether now the network will use its higher capacity to try to fit the noise, thus hurting generalization. However,\n1When using soft-max cross-entropy, the loss is never exactly zero for correct predictions with finite margins/confidences. Instead, if the data is seperable, in order to minimize the loss the weights need to be scaled up toward infinity and the cross entropy loss goes to zero, and a global minimum is never attained. In order to be able to say that we are actually reaching a zero loss solution, and hence a global minimum, we use a slightly modified soft-max which does not noticeably change the results in practice. This truncated loss returns the same exact value for wrong predictions or correct prediction with confidences less than a threshold but returns zero for correct predictions with large enough margins: Let {si}ki=1 be the scores for k possible labels and c be the correct labels. Then the soft-max cross-entropy loss can be written as `(s, c) = ln ∑ i exp(si − sc) but we instead use the differentiable\nloss function ˆ̀(s, c) = ln ∑ i f(si − sc) where f(x) = exp(x) for x ≥ −11 and f(x) = exp(−11)[x+ 13]2+/4 otherwise. Therefore, we only deviate from the soft-max cross-entropy when the margin is more than 11, at which point the effect of this deviation is negligible (we always have\n∣∣∣`(s, c)− ˆ̀(s, c)∣∣∣ ≤ 0.000003k)—if there are any actual errors the behavior on them would completely dominate correct examples with margin over 11, and if there are no errors we are just capping the amount by which we need to scale up the weights.\n26\nas can be seen in the bottom row of Figure 4.2, even with five percent random labels, there is no significant overfitting and test error continues decreasing as network size increases past the size required for achieving zero training error.\nWhat is happening here? A possible explanation is that the optimization is introducing some implicit regularization. That is, we are implicitly trying to find a solution with small “complexity”, for some notion of complexity, perhaps norm. This can explain why we do not overfit even when the number of parameters is huge. Furthermore, increasing the number of units might allow for solutions that actually have lower “complexity”, and thus generalization better. Perhaps an ideal then would be an infinite network controlled only through this hidden complexity.\nWe want to emphasize that we are not including any explicit regularization, neither as an explicit penalty term nor by modifying optimization through, e.g., drop-outs, weight decay, or with one-pass stochastic methods. We are using a stochastic method, but we are running it to convergence—we achieve zero surrogate loss and zero training error. In fact, we also tried training using batch conjugate gradient descent and observed almost identical behavior. But it seems that even still, we are not getting to some random global minimum—indeed for large networks the vast majority of the many global minima of the training error would horribly overfit. Instead, the optimization is directing us toward a “low complexity” global minimum.\nAlthough we do not know what this hidden notion of complexity is, as a final experiment we tried to see the effect of adding explicit regularization in the form of weight decay. The results are shown in the top row of figure 4.2. There is a slight improvement in generalization but we still see that increasing the network size helps generalization.\n27\n28"
    }, {
      "heading" : "4.2 A Matrix Factorization Analogy",
      "text" : "To gain some understanding at what might be going on, let us consider a slightly simpler model which we do understand much better. Instead of rectified linear activations, consider a feed-forward network with a single hidden layer, and linear activations, i.e.:\nfU,V(x) = UVx (4.2.1)\nThis is of course simply a matrix-factorization model, where fW(x) = Wx and W = VU. Controlling capacity by limiting the number of hidden units exactly corresponds to constraining the rank of W, i.e. biasing toward low dimensional factorizations. Such a low-rank inductive bias is indeed sensible, though computationally intractable to handle with most loss functions.\nHowever, in the last decade we have seen much success for learning with low norm factorizations. In such models, we do not constrain the inner dimensionality H of U,V, and instead only constrain, or regularize, their norm. For example, constraining the Frobenius norm of U and V corresponds to using the trace-norm as an inductive bias [38]:\n‖W‖tr = minW=VU 1 2 (‖U‖2F + ‖V‖ 2 F ). (4.2.2)\nOther norms of the factorization lead to different regularizers.\nUnlike the rank, the trace-norm (as well as other factorization norms) is convex, and leads to tractable learning problems [39, 38]. In fact, even if learning is done by a local search over the factor matrices U and V (i.e. by a local search over the weights of the network), if the dimensionality is high enough and the norm is regularized, we can ensure convergence to a global minima [40]. This is in stark contrast to the dimensionality-constrained low-rank situation, where the limiting factor is the number of hidden units, and local minima are abundant [41].\nFurthermore, the trace-norm and other factorization norms are well-justified as sensible inductive biases. We can ensure generalization based on having low trace-norm, and a low-trace norm model corresponds to a realistic factor model with many factors of limited overall influence. In fact, empirical evidence suggests that in many cases low-norm factorization are a more appropriate inductive bias compared to low-rank models.\nWe see, then, that in the case of linear activations (i.e. matrix factorization), the norm of the factorization is in a sense a better inductive bias than the number of weights: it ensures generalization, it is grounded in reality, and it explain why the models can be learned tractably.\nRecently, Gunasekar et al. [42] provided empirical and theoretical evidence on the implicit regularization of gradient descent for matrix factorization. They showed that gradient descent on the full dimensional factorizations without any explicit regularization indeed converges to the minimum trace norm solution with initialization close enough to the origin and small enough step size.\nLet us interpret the experimental results of Section 4.1 in this light. Perhaps learning is succeeding not because there is a good representation of the targets with a small number of units, but rather because there is a good representation with small overall norm, and the optimization is implicitly biasing us toward low-norm models. Such an inductive bias might potentially explain both the generalization ability and the computational tractability of learning, even using local search.\nUnder this interpretation, we really should be using infinite-sized networks, with an infinite number of hidden units. Fitting a finite network (with implicit regularization) can be viewed as an approximation to fitting the “true” infinite network. This situation is also common in matrix factorization: e.g., a very successful approach for training low trace-norm models, and other infinite-dimensional bounded-norm factorization models, is to approximate them using a finite dimensional representation [43, 44]. The finite dimensionality is then not used at all for capacity (statistical complexity) control, but purely for\n29\ncomputational reasons. Indeed, increasing the allowed dimensionality generally improves generalization performance, as it allows us to better approximate the true infinite model. Inspired by these experiments, in order to understand the implicit regularization in deep learning, we will next look at ways of controlling the capacity independent of the number of hidden units.\n30\nChapter 5\nNorm-based Capacity Control\nAs we discussed in Section 3.1 statistical complexity, or capacity, of unregularized feed-forward neural networks, as a function of the network size and depth, is fairly well understood. But feedforward networks are often trained with some kind of regularization, such as weight decay, early stopping, “max regularization”, or more exotic regularization such as drop-outs. We also showed in Chapter 4 that even without any explicit regularization, the capacity of neural networks is being controlled by a form of implicit regularization caused by optimization which does not depend on the size of the network. What is the effect of such regularization on the induced model class and its capacity?\nFor linear prediction (a one-layer feed-forward network) we know that using regularization the capacity of the class can be bounded only in terms of the norms, with no (or a very weak) dependence on the number of edges (i.e. the input dimensionality or number of linear coefficients). E.g., we understand very well how the capacity of `2-regularized linear predictors can be bounded in terms of the norm alone (when the norm of the data is also bounded), even in infinite dimension.\nA central question we ask is: can we bound the capacity of feed-forward network in terms of norm-based regularization alone, without relying on network size and even if the network size (number of nodes or edges) is unbounded or infinite? What type of regularizers admit such capacity control? And how does the capacity behave as a function of the norm, and perhaps other network parameters such as depth?\nBeyond the central question of capacity control, we also analyze the convexity of the resulting model class—unlike unregularized size-controlled feed-forward networks, infinite magnitude-controlled networks have the potential of yielding convex model classes (this is the case, e.g., when we move from rank-based control on matrices, which limits the number of parameters to magnitude based control with the trace-norm or max-norm). A convex class might be easier to optimize over and might be convenient in other ways.\nIn this chapter we focus on two natural types of norm regularization: bounding the norm of the incoming weights of each unit (per-unit regularization) and bounding the overall norm of all the weights in the system jointly (overall regularization, e.g. limiting the overall sum of the magnitudes, or square magnitudes, in the system). We generalize both of these with a single notion of group-norm regularization: we take the `p norm over the weights in each unit and then the `q norm over units. In Section 5.1 we present this regularizer and obtain a tight understanding of when it provides for size-independent capacity control and a characterization of when it induces convexity. We then apply these generic results to per-unit regularization (Section 5.2) and overall regularization (Section 5.3), noting also other forms of regularization that are equivalent to these two. In particular, we show how per-unit regularization is equivalent to a novel path-based regularizer and how overall `2 regularization for two-layer networks is equivalent to so-called “convex neural networks” [45]. In terms of capacity control, we show that per-unit regularization allows size-independent capacity-control only with a per-unit `1-norm, and that\n31\noverall `p regularization allows for size-independent capacity control only when p ≤ 2, even if the depth is bounded. In any case, even if we bound the sum of all magnitudes in the system, we show that an exponential dependence on the depth is unavoidable.\nAs far as we are aware, prior work on size-independent capacity control for feed-forward networks considered only per-unit `1 regularization, and per-unit `2 regularization for two-layered networks (see discussion and references at the beginning of Section 5.2). Recently, Bartlett et al. [46] have shown a generalization bound based on the product of spectral norm of the layers using covering numbers. In Chapter 6, we show a simpler prove for a tighter bound. Here, we extend the scope significantly, and provide a broad characterization of the types of regularization possible and their properties. In particular, we consider overall norm regularization, which is perhaps the most natural form of regularization used in practice (e.g. in the form of weight decay). We hope our study will be useful in thinking about, analyzing and designing learning methods using feed-forward networks. Another motivation for us is that complexity of large-scale optimization is often related to scale-based, not dimension-based complexity. Understanding when the scale-based complexity depends exponentially on the depth of a network might help shed light on understanding the difficulties in optimizing deep networks.\nPreliminaries and Notations We denote by Fd,H the class of fully connected feedforward networks with a single output node and use the shorthand Fd = Fd,∞. We will consider various measures µ(w) of the magnitude of the weights w. Such a measure induces a complexity measure on functions f ∈ Fd,H defined by µd,H(f) = inffw=f µ(w). The sublevel sets of the complexity measure µ\nd,H form a family of hypothesis classes Fd,Hµ≤a = {f ∈ Fd,H | µd,H(f) ≤ a}. For binary function g : {±1}nin → ±1 we say that g is realized by f with unit margin if ∀xf(x)g(x) ≥ 1. A set of points Sin is shattered with unit margin by a model class F if all g : Sin → ±1 can be realized with unit margin by some fw ∈ F ."
    }, {
      "heading" : "5.1 Group Norm Regularization",
      "text" : "Considering the grouping of weights going into each edge of the network, we will consider the following generic group-norm type regularizer, parametrized by 1 ≤ p, q ≤ ∞:\nµp,q(w) = ∑ v∈V  ∑ (u→v)∈E |wu→v|p q/p  1/q . (5.1.1)\nHere and elsewhere we allow q =∞ with the usual conventions that (∑ zqi )1/q = sup zi and 1/q = 0 when it appears in other contexts. When q = ∞ the group regularizer (9.1.1) imposes a per-unit regularization, where we constrain the norm of the incoming weights of each unit separately, and when q = p the regularizer (9.1.1) is an “overall” weight regularizer, constraining the overall norm of all weights in the system. E.g., when q = p = 1 we are paying for the sum of all magnitudes of weights in the network, and q = p = 2 corresponds to overall weight-decay where we pay for the sum of square magnitudes of all weights (i.e. the overall Euclidean norm of the weights).\n32\nFor a layered graph, we have:\nµp,q(w) =  d∑ k=1 H∑ i=1  H∑ j=1 ∣∣W k[i, j]∣∣p q/p  1/q = d1/q ( 1 d d∑ k=1 ∥∥W k∥∥q p,q )1/q\n≥ d1/q (\nd∏ k=1 ∥∥W k∥∥ p,q\n)1/d def = d1/q d √ ψp,q(w) (5.1.2)\nwhere ψp,q(w) = d∏ k=1 ∥∥W k∥∥ p,q aggregates the layers by multiplication instead of summation. The inequality (5.1.2) holds regardless of the activation function, and so for any σ we have:\nψd,Hp,q (f) ≤ ( µd,H(f)p,q\nd1/q\n)d . (5.1.3)\nBut due to the homogeneity of the RELU activation, when this activation is used we can always balance the norm between the different layers without changing the computed function so as to achieve equality in (5.1.2):\nClaim 2. For any fw ∈ Fd,H , µd,Hp,q (f) = d1/q d √ ψd,Hp,q (f).\nProof. Let w be weights that realizes f and are optimal with respect to ψp,g; i.e. ψp,q(w) = ψd,Hp,q (w). Let W̃ k = d √ ψp,q(w)W k/ ∥∥W k∥∥\np,q , and observe that they also realize f . We now have:\nµd,Hp,q (f) ≤ µp,q(W̃ ) = (∑d\nk=1 ∥∥∥W̃ k∥∥∥q p,q )1/q = ( d ( ψp,q(w) )q/d)1/q = d1/q d √ ψd,Hp,q (f)\nwhich together with (5.1.2) completes the proof.\nThe two measures are therefore equivalent when we use RELUs, and define the same level sets, or family of model classes, which we refer to simply as Fd,Hp,q . In the remainder of this Section, we investigate convexity and generalization properties of these model classes."
    }, {
      "heading" : "5.1.1 Generalization and Capacity",
      "text" : "In order to understand the effect of the norm on the sample complexity, we bound the Rademacher complexity of the classes Fd,Hp,q . Recall that the Rademacher Complexity is a measure of the capacity of a model class on a specific sample, which can be used to bound the difference between empirical and expected error, and thus the excess generalization error of empirical risk minimization (see, e.g., [47] for a complete treatment, and Section 5.5.1 for the exact definitions we use). In particular, the Rademacher complexity typically scales as √ C/m, which corresponds to a sample complexity of O(C/ 2), where m is the sample size and C is the effective measure of capacity of the model class.\nTheorem 3. For any d, q ≥ 1, any 1 ≤ p <∞ and any set S = {x1, . . . , xm} ⊆ Rnin :\nRm(Fd,Hψp,q≤ψ) ≤ ψ ( 2H [ 1 p∗− 1 q ]+ )(d−1) Rlinearm,p,nin\n≤ √√√√ψ2 (2H [ 1p∗− 1q ]+)2(d−1) min{p∗, 4 log(2nin)}maxi ‖xi‖2p∗ m\n33\nand so:\nRm(Fd,Hµp,q≤µ) ≤ µ d ( 2H [ 1 p∗− 1 q ]+/ q √ d )(d−1) Rlinearm,p,nin\n≤ √√√√µ2d (2H [ 1p∗− 1q ]+/ q√d)2(d−1) min{p∗, 4 log(2nin)}maxi ‖xi‖2p∗ m\nwhere the second inequalities hold only if 1 ≤ p ≤ 2, Rlinearm,p,nin is the Rademacher complexity of nindimensional linear predictors with unit `p norm with respect to a set of m samples and p∗ is such that 1 p∗ + 1 p = 1.\nProof sketch We prove the bound by induction, showing that for any q, d > 1 and 1 ≤ p <∞,\nRm(Fd,Hψp,q≤ψ) ≤ 2H [ 1 p∗− 1 q ]+Rm(Fd−1,Hψp,q≤ψ).\nThe intuition is that when p∗ < q, the Rademacher complexity increases by simply distributing the weights among neurons and if p∗ ≥ q then the supremum is attained when the output neuron is connected to a neuron with highest Rademacher complexity in the lower layer and all other weights in the top layer are set to zero. For a complete proof, see Section 5.5.1.\nNote that for 2 ≤ p <∞, the bound on the Rademacher complexity scales with m 1p (see Section 5.5.1) because:\nRlinearm,p,nin ≤ √ 2 ‖X‖2,p∗ m ≤ √ 2 maxi ‖xi‖p∗ m 1 p\n(5.1.4)\nThe bound in Theorem 3 depends on both the magnitude of the weights, as captured by µp,q(w) or ψp,q(w), and also on the width H of the network (the number of nodes in each layer). However, the dependence on the width H disappears, and the bound depends only on the magnitude, as long as q ≤ p∗ (i.e. 1/p+1/q ≥ 1). This happens, e.g., for overall `1 and `2 regularization, for per-unit `1 regularization, and whenever 1/p+ 1/q = 1. In such cases, we can omit the size constraint and state the theorem for an infinite-width layered network (i.e. a network with an infinitely countable number of units, when the number of units is allowed to be as large as needed):\nCorollary 4. For any d ≥ 1, 1 ≤ p <∞ and 1 ≤ q ≤ p∗ = p/(p−1), and any set S = {x1, . . . , xm} ⊆ Rnin ,\nRm(Fd,Hψp,q≤ψ) ≤ ψ2 (d−1)Rlinearm,p,nin\n≤ √√√√ψ2 (2H [ 1p∗− 1q ]+)2(d−1) min{p∗, 4 log(2nin)}maxi ‖xi‖2p∗ m\nand so:\nRm(Fd,Hµp,q≤µ) ≤ ( 2µ/ q √ d )d Rlinearm,p,nin\n≤ √√√√(2µ/ q√d)2d min{p∗, 4 log(2nin)}maxi ‖xi‖2p∗ m\nwhere the second inequalities hold only if 1 ≤ p ≤ 2 and Rlinearm,p,nin is the Rademacher complexity of nin-dimensional linear predictors with unit `p norm with respect to a set of m samples.\n34"
    }, {
      "heading" : "5.1.2 Tightness",
      "text" : "We next investigate the tightness of the complexity bound in Theorem 3, and show that when 1/p+1/q < 1 the dependence on the width H is indeed unavoidable. We show not only that the bound on the Rademacher complexity is tight, but that the implied bound on the sample complexity is tight, even for binary classification with a margin over binary inputs. To do this, we show how we can shatter the m = 2nin points {±1}nin using a network with small group-norm: Theorem 5. For any p, q ≥ 1 (and 1/p∗ + 1/p = 1) and any depth d ≥ 2, the m = 2nin points {±1}nin can be shattered with unit margin by Fd,Hψp,q≤ψ with:\nψ ≤ n1/pin m1/p+1/qH−(d−2)[1/p ∗−1/q]+\nProof. Consider a size m subset Sm of 2nin vertices of the nin dimensional hypercube {−1,+1}nin . We construct the first layer using m units. Each unit has a unique weight vector consisting of +1 and −1’s and will output a positive value if and only if the sign pattern of the input x ∈ Sm matches that of the weight vector. The second layer has a single unit and connects to all m units in the first layer. For any m dimensional sign pattern b ∈ {−1,+1}m, we can choose the weights of the second layer to be b, and the network will output the desired sign for each x ∈ Sm with unit margin. The norm of the network is at most (m · nq/pin )1/q · m1/p = n 1/p in · m(1/p+1/q). This establishes the claim for d = 2. For d > 2 and 1/p + 1/q ≥ 1, we obtain the same norm and unit margin by adding d − 2 layers with one unit in each layer connected to the previous layer by a unit weight. For d > 2 and 1/p + 1/q < 1, we show the dependence on H by recursively replacing the top unit with H copies of it and adding an averaging unit on top of that. More specifically, given the above d = 2 layer network, we make H copies of the output unit with rectified linear activation and add a 3rd layer with one output unit with uniform weight 1/H to all the copies in the 2nd layer. Since this operation does not change the output of the network, we have the same margin and now the norm of the network is (m ·nq/pin )1/q · (Hmq/p)1/q · (H(1/Hp))1/p = n 1/p in ·m(1/p+1/q) ·H1/q−1/p ∗ . That is, we have reduced the norm by factor H1/q−1/p ∗ . By repeating this process, we get the geometric reduction in the norm H(d−2)(1/q−1/p ∗), which concludes the proof.\nTo understand this lower bound, first consider the bound without the dependence on the widthH . We have that for any depth d ≥ 2, ψ ≤ mrnin = mr logm (since 1/p ≤ 1 always) where r = 1/p + 1/q ≤ 2. This means that for any depth d ≥ 2 and any p, q the sample complexity of learning the class scales as m = Ω(ψ1/r/ logψ) ≥ Ω̃(√ψ). This shows a polynomial dependence on ψ, though with a lower exponent than the ψ2 (or higher for p > 2) dependence in Theorem 3. Still, if we now consider the complexity control as a function of µp,q we get a sample complexity of at least Ω(µd/2/ logµ), establishing that if we control the group-norm as in (9.1.1), we cannot avoid a sample complexity which depends exponentially on the depth. Note that in our construction, all other factors in Theorem 3, namely maxi ‖xi‖ and log nin, are logarithmic (or double-logarithmic) in m. Next we consider the dependence on the width H when 1/p + 1/q < 1. Here we have to use depth d ≥ 3, and we see that indeed as the width H and depth d increase, the magnitude control ψ can decrease as H(1/p\n∗−1/q)(d−2) without decreasing the capacity, matching Theorem 1 up to an offset of 2 on the depth. In particular, we see that in this regime we can shatter an arbitrarily large number of points with arbitrarily low ψ by using enough hidden units, and so the capacity of Fdp,q is indeed infinite and it cannot ensure any generalization."
    }, {
      "heading" : "5.1.3 Convexity",
      "text" : "Finally we establish a sufficient condition for the model classes Fdp,q to be convex. We are referring to convexity of the functions in the Fdp,q independent of a specific representation. If we consider a, possibly\n35\nregularized, empirical risk minimization problem on the weights, the objective (the empirical risk) would never be a convex function of the weights (for depth d ≥ 2), even if the regularizer is convex in w (which it always is for p, q ≥ 1). But if we do not bound the width of the network, and instead rely on magnitude-control alone, we will see that the resulting model class, and indeed the complexity measure, may be convex (with respect to taking convex combinations of functions, not of weights).\nTheorem 6. For any d, p, q ≥ 1 such that 1q ≤ 1d−1 ( 1− 1p ) , ψdp,q(f) is a semi-norm in Fd.\nIn particular, under the condition of the Theorem, ψdp,q is convex, and hence its sublevel sets Fdp,q are convex, and so µdp,q is quasi-convex (but not convex).\nProof sketch To show convexity, consider two functions f, g ∈ Fdψp,q≤ψ and 0 < α < 1, and let U and V be the weights realizing f and g respectively with ψp,q(U) ≤ ψ and ψp,q(V ) ≤ ψ. We will construct weights w realizing αf + (1− α)g with ψp,q(w) ≤ ψ. This is done by first balancing U and V s.t. at each layer ‖Ui‖p,q = d √ ψp,q(U) and ‖Vi‖p,q = d √ ψp,q,(V ) and then placing U and V side by side, with no interaction between the units calculating f and g until the output layer. The output unit has weights αUd coming in from the f -side and weights (1− α)Vd coming in from the g-side. In Section 5.5.2 we show that under the condition in the theorem, ψp,q(w) ≤ ψ. To complete the proof, we also show ψdp,q is homogeneous and that this is sufficient for convexity."
    }, {
      "heading" : "5.2 Per-Unit and Path Regularization",
      "text" : "In this Section we will focus on the special case of q = ∞, i.e. when we constrain the norm of the incoming weights of each unit separately.\nPer-unit `1-regularization was studied by [9, 48, 47] who showed generalization guarantees. A two-layer network of this form with RELU activation was also considered by [49], who studied its approximation ability and suggested heuristics for learning it. Per-unit `2 regularization in a two-layer network was considered by [50], who showed it is equivalent to using a specific kernel. We now introduce Path regularization and discuss its equivalence to Per-Unit regularization.\nPath Regularization Consider a regularizer which looks at the sum over all paths from input nodes to the output node, of the product of the weights along the path:\nφp(w) = ( ∑ vin[i] e1→v1 e2→v2··· ek→vout k∏ i=1 |wei |p )1/p\n(5.2.1)\nwhere p ≥ 1 controls the norm used to aggregate the paths. We can motivate this regularizer as follows: if a node does not have any high-weight paths going out of it, we really don’t care much about what comes into it, as it won’t have much effect on the output. The path-regularizer thus looks at the aggregated influence of all the weights.\nReferring to the induced regularizer φGp (f) = minfw=f φp(w) (with the usual shorthands for layered graphs), we now observe that for layered graphs, path regularization and per-unit regularization are equivalent:\nTheorem 7. For p ≥ 1, any d and (finite or infinite) H , for any fw ∈ Fd,H : φd,Hp (f) = ψd,Hp,∞\nIt is important to emphasize that even for layered graphs, it is not the case that for all weights φp(w) = ψp,∞(w). E.g., a high-magnitude edge going into a unit with no non-zero outgoing edges will affect\n36\nψp,∞(w) but not φp(w), as will having high-magnitude edges on different layers in different paths. In a sense path regularization is as more careful regularizer less fooled by imbalance. Nevertheless, in the proof of Theorem 7 in Section 5.5.3, we show we can always balance the weights such that the two measures are equal.\nThe equivalence does not extend to non-layered graphs, since the lengths of different paths might be different. Again, we can think of path regularizer as more refined regularizer taking into account the local structure. However, if we consider all DAGs of depth at most d (i.e. with paths of length at most d), the notions are again equivalent (see proof in Section 5.5.3):\nTheorem 8. For any p ≥ 1 and any d: ψdp,∞(f) = min G ∈ DAG(d) φGp (f).\nIn particular, for any graph G of depth d, we have that φGp (f) ≥ ψdp,∞(f). Combining this observation with Corollary 4 allows us to immediately obtain a generalization bound for path regularization on any, even non-layered, graph:\nCorollary 9. For any graph G of depth d and any set S = {x1, . . . , xm} ⊆ Rnin :\nRm(FGφ1≤φ) ≤\n√ 4d−1φ2 · 4 log(2nin) sup ‖xi‖2∞\nm\nNote that in order to apply Corollary 4 and obtain a width-independent bound, we had to limit ourselves to p = 1. We further explore this issue next.\nCapacity As was previously noted, size-independent generalization bounds for bounded depth networks with bounded per-unit `1 norm have long been known (and make for a popular homework problem). These correspond to a specialization of Corollary 4 for the case p = 1, q = ∞. Furthermore, the kernel view of [50] allows obtaining size-independent generalization bound for two-layer networks with bounded per-unit `2 norm (i.e. a single infinite hidden layer of all possible unit-norm units, and a bounded `2-norm output unit). However, the lower bound of Theorem 5 establishes that for any p > 1, once we go beyond two layers, we cannot ensure generalization without also controlling the size (or width) of the network.\nConvexity An immediately consequence of Theorem 6 is that per-unit regularization, if we do not constrain the network width, is convex for any p ≥ 1. In fact, ψdp,∞ is a (semi)norm. However, as discussed above, for depth d > 2 this is meaningful only for p = 1, as ψdp,∞ collapses for p > 1.\nHardness Since the classes Fd1,∞ are convex, we might hope that this might make learning computationally easier. Indeed, one can consider functional-gradient or boosting-type strategies for learning a predictor in the class [51]. However, as Bach [49] points out, this is not so easy as it requires finding the best fit for a target with a RELU unit, which is not easy. Indeed, applying results on hardness of learning intersections of halfspaces, which can be represented with small per-unit norm using two-layer networks, we can conclude that, subject to certain complexity assumptions, it is not possible to efficiently PAC learn Fd1,∞, even for depth d = 2 when ψ1,∞ increases superlinearly:\nCorollary 10. Subject to the the strong random CSP assumptions in [11], it is not possible to efficiently PAC learn (even improperly) functions {±1}nin → {±1} realizable with unit margin by F21,∞ when ψ1,∞ = ω(nin) (e.g. when ψ1,∞ = nin log nin). Moreover, subject to intractability of Q̃(n1.5in )-unique shortest vector problem, for any > 0, it is not possible to efficiently PAC learn (even improperly) functions {±1}nin → {±1} realizable with unit margin by F21,∞ when ψ1,∞ = n1+ in .\n37\nThis is a corollary of Theorem 24 in the Section 5.5.4. Either versions of corollary 10 precludes the possibility of learning in time polynomial in ψ1,∞, though it still might be possible to learn in poly(nin) time when ψ1,∞ is sublinear.\nSharing We conclude this Section with an observation on the type of networks obtained by per-unit, or equivalently path, regularization.\nTheorem 11. For any p ≥ 1 and d > 1 and any fw ∈ Fd, there exists a layered graph G(V,E) of depth d, such that fw ∈ FG and ψGp,∞(f) = φGp (f) = ψdp,∞(f), and the out-degree of every internal (non-input) node in G is one. That is, the subgraph of G induced by the non-input vertices is a tree directed toward the output vertex.\nWhat the Theorem tells us is that we can realize every function as a tree with optimal per-unit norm. If we think of learning with an infinite fully-connected layered network, we can always restrict ourselves to models in which the non-zero-weight edges form a tree. This means that when using per-unit regularization we have no incentive to “share” lower-level units—each unit will only have a single outgoing edge and will only be used by a single down-stream unit. This seems to defy much of the intuition and power of using deep networks, where we expect lower layers to represent generic feature useful in many higher-level features. In effect, we are not encouraging any transfer between learning different aspects of the function (or between different tasks or classes, if we do have multiple output units). Per-unit regularization therefore misses out on much of the inductive bias that we might like to impose when using deep learning (namely, promoting sharing).\nProof. [of Theorem 11] For any fw ∈ FDAG(d), we show how to construct such G̃ and w̃. We first sort the vertices of G based on topological ordering such that the out-degree of the first vertex is zero. Let G0 = G and w0 = w. At each step i, we first set Gi = Gi−1 and wi = wi−1 and then pick the vertex u that is the ith vector in the topological ordering. If the out-degree of u is at most 1. Otherwise, for any edge (u→ v) we create a copy of vertex u that we call it uv , add the edge (uv → v) to Gi and connect all incoming edges of u with the same weights to every such uv and finally we delete the vertex u from Gi together with all incoming and outgoing edges of u. It is easy to indicate that fGi,wi = fGi−1,wi−1 . After at most |V | such steps, all internal nodes have out-degree one and hence the subgraph induced by non-input vertices will be a tree."
    }, {
      "heading" : "5.3 Overall Regularization",
      "text" : "In this Section, we will focus on “overall” `p regularization, corresponding to the choice q = p, i.e. when we bound the overall (vectorized) norm of all weights in the system:\nµp,p(w) = (∑ e∈E |w(e)|p )1/p .\nCapacity For p ≤ 2, Corollary 4 provides a generalization guarantee that is independence of the width— we can conclude that if we use weight decay (overall `2 regularization), or any tighter `p regularization, there is no need to limit ourselves to networks of finite size (as long as the corresponding dual-norm of the inputs are bounded). However, in Section 5.1.2 we saw that with d ≥ 3 layers, the regularizer degenerates and leads to infinite capacity classes if p > 2. In any case, even if we bound the overall `1-norm, the complexity increases exponentially with the depth.\n38\nConvexity The conditions of Theorem 6 for convexity of Fd2,2 are ensured when p ≥ d. For depth d = 1, i.e. a single unit, this just confirms that `p-regularized linear prediction is convex for p ≥ 1. For depth d = 2, we get convexity with `2 regularization, but not `1. For depth d > 2 we would need p > d ≥ 3, however for such values of p we know from Theorem 5 that Fdp,p degenerates to an infinite capacity class if we do not control the width (if we do control the width, we do not get convexity). This leaves us with F22,2 as the interesting convex class. Below we show an explicit convex characterization of F22,2 by showing it is equivalent to so-called “convex neural nets”. Convex Neural Nets [45] over inputs in Rnin are two-layer networks with a fixed infinite hidden layer consisting of all units with weights w ∈ G for some base class G ∈ Rnin , and a second `1-regularized layer. Since over finite data the weights in the second layer can always be taken to have finite support (i.e. be non-zero for only a finite number of first-layer units), and we can approach any function with countable support, we can instead think of a network in F2 where the bottom layer is constraint to G and the top layer is `1 regularized. Focusing on G = {w | ‖w‖p ≤ 1}, this corresponds to imposing an `p constraint on the bottom layer, and `1 regularization on the top layer and yields the following complexity measure over F2:\nνp(f) = inf flayer(d),W=f,s.t.∀j‖W1[j,:]‖p≤1\n‖W2‖1 . (5.3.1)\nThis is similar to per-unit regularization, except we impose different norms at different layers (if p 6= 1). We can see that F2νp≤ν = ν · conv(σ(G)), and is thus convex for any p. Focusing on RELU activation we have the equivalence:\nTheorem 12. µ22,2(f) = 2ν2(f).\nThat is, overall `2 regularization with two layers is equivalent to a convex neural net with `2-constrained units on the bottom layer and `1 (not `2!) regularization on the output.\nProof. We can calculate:\nmin fW=f µ22,2(w) = min fW=f H∑ j=1 ( nin∑ i=1 |W1[j, i]|2 + |W2[j]|2 )\n= min fW=f H∑ j=1 2 √∑nin i=1 |W1[j, i]|2 · |W2[j]| (5.3.2)\n= 2 min fW=f H∑ j=1 |W2[j]| s.t. √∑nin i=1 |W1[j, i]|2 ≤ 1. (5.3.3)\nHere (5.3.2) is the arithmetic-geometric mean inequality for which we can achieve equality by balancing the weights (as in Claim 2) and (5.3.3) again follows from the homogeneity of the RELU which allows us to rebalance the weights.\nHardness As with Fd1,∞, we might hope that the convexity of F22,2 might make it computationally easy to learn. However, by the same reduction from learning intersection of halfspaces (Theorem 24 in Section 5.5.4) we can again conclude that we cannot learn in time polynomial in µ22,2:\nCorollary 13. Subject to the the strong random CSP assumptions in [11], it is not possible to efficiently PAC learn (even improperly) functions {±1}nin → {±1} realizable with unit margin by F2p,p when µ2p,p = ω(n 1 p in ). (e.g. when ψ1,∞ = nin log nin). Moreover, subject to intractability of Q̃(n 1.5 in )-unique shortest vector problem, for any > 0, it is not possible to efficiently PAC learn (even improperly) functions {±1}nin → {±1} realizable with unit margin by F21,∞ when ψ1,∞ = n 1 p+ in .\n39"
    }, {
      "heading" : "5.4 Depth Independent Regularization",
      "text" : "Up until now we discussed relying on magnitude-based regularization instead of directly controlling network size, thus allowing unbounded and even infinite width. But we still relied on a finite bound on the depth in all our derivations. Can the explicit dependence on the depth be avoided, and replaced with only a measure of scale of the weights?\nWe already know we cannot rely only on a bound on the group-norm µp,q when the depth is unbounded, as we know from Theorem 5 that in terms of µp,q the sample complexity necessarily increases exponentially with the depth: if we allow arbitrarily deep graphs we can shrink µp,q toward zero without changing the scale of the computed function. However, controlling the ψ-measure, or equivalently the path-regularizer φ, in arbitrarily-deep graphs is sensible, and we can define:\nψp,q = inf d≥1 ψdp,q(f) = lim d→∞ ψdp,q(f) or: φp = inf G φGp (f) (5.4.1)\nwhere the minimization is over any DAG. From Theorem 8 we can conclude that φp(f) = ψp,∞(f). In any case, ψp,q(f) is a sensible complexity measure, that does not collapse despite the unbounded depth. Can we obtain generalization guarantees for the class Fψp,q≤ψ ? Unfortunately, even when 1/p+ 1/q ≥ 1 and we can obtain width-independent bounds, the bound in Corollary 4 still has a dependence on 4d, even if ψp,q is bounded. Can such a dependence be avoided?\nFor anti-symmetric Lipschitz-continuous activation functions (i.e. such that σ(−z) = −σ(z)), such as the ramp, and for per-unit `p-regularization µd1,∞ we can avoid the factor of 4 d\nTheorem 14. For any anti-symmetric 1-Lipschitz function σ and any set S = {x1, . . . , xm} ⊆ Rnin :\nRm(Fdµ1,∞≤µ) ≤\n√ 4µ2d log(2nin) sup ‖xi‖2∞\nm\nThe proof is again based on an inductive argument similar to Theorem 3 and you can find it in Section 5.5.1.\nHowever, the ramp is not homogeneous and so the equivalent between µ, ψ and φ breaks down. Can we obtain such a bound also for the RELU? At the very least, what we can say is that an inductive argument such that used in the proofs of Theorems 3 and 14 cannot be used to avoid an exponential dependence on the depth. To see this, consider ψ1,∞ ≤ 1 (this choice is arbitrary if we are considering the Rademacher complexity), for which we have\nFd+1ψ1,∞<1 = [ conv(Fdψ1,∞<1) ] + , (5.4.2)\nwhere conv(·) is the symmetric convex hull, and [·]+ = max(z, 0) is applied to each function in the class. In order to apply the inductive argument without increasing the complexity exponentially with the depth, we would need the operation [conv(H)]+ to preserve the Rademacher complexity, at least for non-negative convex conesH. However we show a simple example of a non-negative convex coneH for whichRm ([conv(H)]+) > Rm (H). We will specify H as a set of vectors in Rm, corresponding to the evaluation of h(xi) of different functions in the class on the m points xi in the sample. In our construction, we will have only m = 3 points. Consider H = conv({(∞, ′,∞), (′,∞,∞)}), in which case H′ def= [conv(H)]+ = conv({(∞, ′,∞), (′,∞,∞), (′.5, ′, ′)}). It is not hard to verify thatRm(H′) = ∞3∞6 > ∞∈∞6 = Rm(H).\n40"
    }, {
      "heading" : "5.5 Proofs",
      "text" : ""
    }, {
      "heading" : "5.5.1 Rademacher Complexities",
      "text" : "The sample based Rademacher complexity of a class F of function mapping from X to R with respect to a set S = {x1, . . . , xm} is defined as:\nRm(F) = E [ξ ∈ {±1}m] 1\nm sup fw∈F ∣∣∣∣∣ m∑ i=1 ξif(xi) ∣∣∣∣∣ In this section, we prove an upper bound for the Rademacher complexity of the class Fd,HRELUψp,q≤ψ , i.e., the class of functions that can be represented as depth d, width H network with rectified linear activations, and the layer-wise group norm complexity ψp,q bounded by ψ. As mentioned in the main text, our proof is an induction with respect to the depth d. We start with d = 1 layer neural networks, which is essentially the class of linear separators.\n`p-regularized Linear Predictors\nFor completeness, we prove the upper bounds on the Rademacher complexity of class of linear separators with bounded `p norm. The upper bounds presented here are particularly similar to generalization bounds in [52] and [53]. We first mention two already established lemmas that we use in the proofs.\nTheorem 15. (Khintchine-Kahane Inequality) For any 0 < p < ∞ and S = {z1, . . . , zm}, if the random variable ξ is uniform over {±1}m, then(\nE [ξ] ∣∣∣∣∣ m∑ i=1 ξizi ∣∣∣∣∣ p) 1p ≤ Cp ( m∑ i=1 |zi|2 ) 1 2\nwhere Cp is a constant depending only on p.\nThe sharp value of the constant Cp was found by Haagerup [54] but for our analysis, it is enough to note that if p ≥ 1 we have Cp ≤ √p. Lemma 16. (Massart Lemma) Let A be a finite set of m dimensional vectors. Then\nE [ξ]max a∈A\n1\nm m∑ i=1 ξiai ≤ max a∈A ‖a‖2 √ 2 log |A| m ,\nwhere |A| is the cardinality of A.\nWe are now ready to show upper bounds on Rademacher complexity of linear separators with bounded `p norm.\nLemma 17. (Rademacher complexity of linear separators with bounded `p norm) For any d, q ≥ 1, For any 1 ≤ p ≤ 2,\nRm(F1ψp,q≤ψ) ≤\n√ ψ2 min{p∗, 4 log(2nin)}maxi ‖xi‖2p∗\nm\nand for any 2 < p <∞\nRm(F1ψp,q≤ψ) ≤ √ 2ψ ‖X‖2,p∗ m ≤ √ 2ψmaxi ‖xi‖p∗ m 1 p\n41\nwhere p∗ is such that 1p∗ + 1 p = 1.\nProof. First, note that F1 is the class of linear functions and hence for any function fw ∈ F1, we have that ψp,q(w) = ‖w‖p. Therefore, we can write the Rademacher complexity for a set S = {x1, . . . , xm} as:\nRm(F1ψp,q≤ψ) = E [ξ ∈ {±1}m] 1\nm sup ‖w‖p≤ψ ∣∣∣∣∣ m∑ i=1 ξiw >xi ∣∣∣∣∣ = E [ξ ∈ {±1}m] 1\nm sup ‖w‖p≤ψ ∣∣∣∣∣w> m∑ i=1 ξixi ∣∣∣∣∣ = ψE [ξ ∈ {±1}m] 1\nm ∥∥∥∥∥ m∑ i=1 ξixi ∥∥∥∥∥ p∗\nFor 1 ≤ p ≤ min {\n2, 2 log(2nin)2 log(2nin)−1\n} (and therefore 2 log(2nin) ≤ p∗), we have\nRm(F1ψp,q≤ψ) = ψE [ξ ∈ {±1}m] 1\nm ∥∥∥∥∥ m∑ i=1 ξixi ∥∥∥∥∥ p∗\n≤ n 1 p∗ in ψE [ξ ∈ {±1}m] [ 1\nm ∥∥∥∥∥ m∑ i=1 ξixi ∥∥∥∥∥ ∞ ]\n≤ n 1 2 log(2nin) in ψE [ξ ∈ {±1}m] 1\nm ∥∥∥∥∥ m∑ i=1 ξixi ∥∥∥∥∥ ∞\n≤ √\n2ψE [ξ ∈ {±1}m] 1 m ∥∥∥∥∥ m∑ i=1 ξixi ∥∥∥∥∥ ∞\nWe now use the Massart Lemma viewing each feature (xi[j])mi=1 for j = 1, . . . , nin as a member of a finite model class and obtain\nRm(F1ψp,q≤ψ) ≤ √ 2ψE [ξ ∈ {±1}m] 1 m ∥∥∥∥∥ m∑ i=1 ξixi ∥∥∥∥∥ ∞\n≤ 2ψ √ log(2nin)\nm max j=1...,nin ‖(xi[j])mi=1‖2 ≤ 2ψ √ log(2nin)\nm max i=1,...,m ‖xi‖∞ ≤ 2ψ √ log(2nin)\nm max i=1,...,m ‖xi‖p∗\n42\nIf min {\n2, 2 log(2nin)2 log(2nin)−1\n} < p <∞, by Khintchine-Kahane inequality we have\nRm(F1ψp,q≤ψ) = ψE [ξ ∈ {±1}m]  1 m ∥∥∥∥∥ m∑ i=1 ξixi ∥∥∥∥∥ p∗  ≤ ψ 1\nm  nin∑ j=1 E [ξ ∈ {±1}m] ∣∣∣∣∣ m∑ i=1 ξixi[j] ∣∣∣∣∣ p∗ 1/p ∗\n≤ ψ √ p∗\nm (∑nin j=1 ‖(xi[j])mi=1‖p ∗ 2 )1/p∗ = ψ √ p∗ m ‖X‖2,p∗\nIf p∗ ≥ 2, by Minskowski inequality we have that ‖X‖2,p∗ ≤ m1/2 maxi ‖xi‖p∗ . Otherwise, by subadditivity of the function f(z) = z p∗ 2 , we get ‖X‖2,p∗ ≤ m1/p ∗ maxi ‖xi‖p∗ .\nTheorem 3\nWe define the model class Fd,H,H to be the class of functions from X to RH computed by a layered network of depth d, layer size H and H outputs.\nFor the proof of theorem 3, we need the following two technical lemmas. The first is the well-known contraction lemma:\nLemma 18. (Contraction Lemma) Let function φ : R → R be Lipschitz with constant Lφ such that φ satisfies φ(0) = 0. Then for any class F of functions mapping from X to R and any set S = {x1, . . . , xm}:\nE [ξ ∈ {±1}m] [ 1\nm sup fw∈F ∣∣∣∣∣ m∑ i=1 ξiφ(f(xi)) ∣∣∣∣∣ ] ≤ 2LφE [ξ ∈ {±1}m] [ 1 m sup fw∈F ∣∣∣∣∣ m∑ i=1 ξif(xi)) ∣∣∣∣∣ ]\nNext, the following lemma reduces the maximization over a matrix W ∈ RH×H that appears in the computation of Rademacher complexity to H independent maximizations over a vector w ∈ RH (the proof is deferred to subsubsection 5.5.1):\nLemma 19. For any p, q ≥ 1, d ≥ 2, ξ ∈ {±1}m and fw ∈ Fd,H,H we have\nsup W\n1\n‖W‖p,q ∥∥∥∥∥ m∑ i=1 ξi[W [f(xi)]+]+ ∥∥∥∥∥ p∗ = H [ 1 p∗− 1 q ]+ sup w 1 ‖w‖p ∣∣∣∣∣ m∑ i=1 ξi[w >[f(xi)]+]+ ∣∣∣∣∣ where p∗ is such that 1p∗ + 1 p = 1.\nTheorem 3. For any d, p, q ≥ 1 and any set S = {x1, . . . , xm} ⊆ Rnin :\nRm(Fd,Hψp,q≤ψ) ≤ √√√√ψ2 (2H [ 1p∗− 1q ]+)2(d−1) min{p∗, 2 log(2nin)} sup ‖xi‖2p∗ m\nand so:\nRm(Fd,Hµp,q≤µ) ≤ √√√√µ2d (2H [ 1p∗− 1q ]+/ q√d)2(d−1) min{p∗, 2 log(2nin)} sup ‖xi‖2p∗ m\n43\nwhere p∗ is such that 1p∗ + 1 p = 1.\nProof. By the definition of Rademacher complexity if ξ is uniform over {±1}m, we have:\nRm(Fd,Hψp,q≤ψ) = E [ξ]  1 m\nsup fw∈Fd,Hψp,q≤ψ ∣∣∣∣∣ m∑ i=1 ξif(xi) ∣∣∣∣∣ \n= E [ξ]\n[ 1\nm sup\nfw∈Fd,H\nψ\nψp,q(w) ∣∣∣∣∣ m∑ i=1 ξif(xi) ∣∣∣∣∣ ]\n= E [ξ]\n[ 1\nm sup g∈Fd−1,H,H sup w\nψ\nψp,q(g) ‖w‖p ∣∣∣∣∣ m∑ i=1 ξiw >[g(xi)]+ ∣∣∣∣∣ ]\n= E [ξ]  1 m sup g∈Fd−1,H,H\nψ\nψp,q(g) ∥∥∥∥∥ m∑ i=1 ξi[g(xi)]+ ∥∥∥∥∥ p∗  = E [ξ]  1 m sup h∈Fd−2,H,H ψ ψp,q(h) sup W 1 ‖W‖p,q ∥∥∥∥∥ m∑ i=1 ξi[W [h(xi)]+]+ ∥∥∥∥∥ p∗\n = H [ 1 p∗− 1 q ]+E [ξ] [ 1\nm sup\nh∈Fd−2,H,H\nψ\nψp,q(h) sup w\n1\n‖w‖p ∣∣∣∣∣ m∑ i=1 ξi[w >[h(xi)]+]+ ∣∣∣∣∣ ]\n(5.5.1)\n= H [ 1 p∗− 1 q ]+E [ξ]  1 m\nsup g∈Fd−1,H\nψp,q≤ψ\n∣∣∣∣∣ m∑ i=1 ξi[g(xi)]+ ∣∣∣∣∣ \n≤ 2H [ 1p∗− 1q ]+E [ξ]  1 m\nsup g∈Fd−1,H\nψp,q≤ψ\n∣∣∣∣∣ m∑ i=1 ξig(xi) ∣∣∣∣∣  (5.5.2)\n= 2H [ 1 p∗− 1 q ]+Rm(Fd−1,Hψp,q≤ψ)\nwhere the equality (5.5.1) is obtained by lemma 19 and inequality (5.5.2) is by Contraction Lemma. This will give us the bound on Rademacher complexity of Fd,Hψp,q≤ψ based on the Rademacher complexity of Fd−1,Hψp,q≤ψ . Applying the same argument on all layers and using lemma 17 to bound the complexity of the first layer completes the proof.\nProof of Lemma 19\nProof. It is immediate that the right hand side of the equality in the statement is always less than or equal to the left hand side because given any vector w in the right hand side, by setting each row of matrix w in the left hand side we get the equality. Therefore, it is enough to prove that the left hand side is less than or equal to the right hand side. For the convenience of notations, let g(w) def= |∑mi=1 ξiw>[f(xi)]+|. Define w̃ to be:\nw̃ def = arg max\nw\ng(w) ‖w‖p\n44\nIf q ≤ p∗, then the right hand side of equality in the lemma statement will reduce to g(w̃)/ ‖w̃‖p and therefore we need to show that for any matrix V ,\ng(w̃) ‖w̃‖p ≥ ‖g(V )‖p∗ ‖V ‖p,q .\nSince q ≤ p∗, we have ‖V ‖p,p∗ ≤ ‖V ‖p,q and hence it is enough to prove the following inequality:\ng(w̃) ‖w̃‖p ≥ ‖g(V )‖p∗ ‖V ‖p,p∗ .\nOn the other hand, if q > p∗, then we need to prove the following inequality holds:\nH 1 p∗−\n1 q g(w̃) ‖w̃‖p ≥ ‖g(V )‖p∗ ‖V ‖p,q\nSince q > p∗, we have that ‖V ‖p,p∗ ≤ H 1 p∗− 1 q ‖V ‖p,q . Therefore, it is again enough to show that:\ng(w̃) ‖w̃‖p ≥ ‖g(V )‖p∗ ‖V ‖p,p∗ .\nWe can rewrite the above inequality in the following form:\nH∑ i=1 ( g(w̃) ‖Vi‖p ‖w̃‖p )p∗ ≥ H∑ i=1 g(Vi) p∗\nBy the definition of w̃, we know that the above inequality holds for each term in the sum and hence the inequality is true.\nTheorem 14\nThe proof is similar to the proof of theorem 3 but here bounding µ1,∞ by µ means the `1 norm of input weights to each neuron is bounded by µ. We use a different version of Contraction Lemma in the proof that is without the absolute value:\nLemma 20. (Contraction Lemma (without the absolute value)) Let function φ : R→ R be Lipschitz with constant Lφ. Then for any class F of functions mapping from X to R and any set S = {x1, . . . , xm}:\nE [ξ ∈ {±1}m] [ 1\nm sup fw∈F m∑ i=1 ξiφ(f(xi))\n] ≤ LφE [ξ ∈ {±1}m] [ 1\nm sup fw∈F m∑ i=1 ξif(xi))\n]\nTheorem 14. For any anti-symmetric 1-Lipschitz function σ and any set S = {x1, . . . , xm} ⊆ Rnin :\nRm(Fdµ1,∞≤µ) ≤\n√ 2µ2d log(2nin) sup ‖xi‖2∞\nm\n45\nProof. Assuming ξ is uniform over {±1}m, we have:\nRm(Fd,Hµ1,∞≤µ) = E [ξ]  1 m\nsup fw∈Fd,Hµ1,∞≤µ ∣∣∣∣∣ m∑ i=1 ξif(xi) ∣∣∣∣∣ \n= E [ξ]  1 m\nsup fw∈Fd,Hµ1,∞≤µ m∑ i=1 ξif(xi)  = E [ξ]  1 m\nsup g∈Fd−1,H,H\nµ1,∞≤µ\nsup ‖w‖1≤µ w> m∑ i=1 ξiσ(g(xi))  = E [ξ]  1 m\nsup g∈Fd−1,H,H\nµ1,∞≤µ\n∥∥∥∥∥ m∑ i=1 ξiσ(g(xi)) ∥∥∥∥∥ ∞  = E [ξ]  1 m\nsup g∈Fd−1,H\nµ1,∞≤µ\n∣∣∣∣∣ m∑ i=1 ξiσ(g(xi)) ∣∣∣∣∣  (5.5.3)\n= E [ξ]  1 m\nsup g∈Fd−1,H\nµ1,∞≤µ\nm∑ i=1 ξiσ(g(xi))  ≤ E [ξ]  1 m\nsup g∈Fd−1,H\nµ1,∞≤µ\nm∑ i=1 ξig(xi)  (5.5.4) = E [ξ]  1 m\nsup g∈Fd−1,H\nµ1,∞≤µ\n∣∣∣∣∣ m∑ i=1 ξig(xi) ∣∣∣∣∣ \n= Rm(Fd−1,Hµ1,∞≤µ)\nwhere the equality (5.5.3) is by anti-symmetric property of σ and inequality (5.5.4) is by the version of Contraction Lemma without the absolute value. This will give us the bound on Rademacher complexity of Fd,Hµ1,∞≤µ based on the Rademacher complexity of F d−1,H µ1,∞≤µ. Applying the same argument on all layers and using lemma 17 to bound the complexity of the first layer completes the proof."
    }, {
      "heading" : "5.5.2 Proof that ψdp,q(w) is a semi-norm in Fd",
      "text" : "We repeat the statement here for convenience. Theorem 6. For any d, p, q ≥ 1 such that 1q ≤ 1d−1 ( 1− 1p ) , ψdp,q(f) is a semi-norm in Fd.\nProof. The proof consists of three parts. First we show that the level set Fdψdp,q≤ψ = {fw ∈ F d : ψdp,q(f) ≤ ψ} is a convex set if the condition on d, p, q is satisfied. Next, we establish the non-negative homogeneity of ψdp,q(f). Finally, we show that if a function α : Fd → R is non-negative homogeneous and every sublevel set {fw ∈ Fd : α(f) ≤ ψ} is convex, then α satisfies the triangular inequality.\nConvexity of the level sets First we show that for any two functions f1, f2 ∈ Fdψp,q≤ψ and 0 ≤ α ≤ 1, the function g = αf1 + (1− α)f2 is in the model class Fdψp,q≤ψ . We prove this by constructing weights\n46\nw that realizes g. Let U and V be the weights of two neural networks such that ψp,q(U) = ψdp,q(f1) ≤ ψ and ψp,q(V ) = ψdp,q(f2) ≤ ψ. For every layer i = 1, . . . , d let\nŨi = d √ ψp,q(U)Ui/‖Ui‖p,q, Ṽi = d √ ψp,q(V )Vi/‖Vi‖p,q.\nand set W1 = [ Ũ1 Ṽ1 ] for the first layer, Wi = [ Ũi 0 0 Ṽi ] for the intermediate layers and Wd =[\nαŨd (1− α)Ṽd ] for the output layer.\nThen for the defined w, we have fW = αf1 + (1− α)f2 for rectified linear and any other non-negative homogeneous activation function. Moreover, for any i < d, the norm of each layer is\n‖Wi‖p,q = ( ψp,q(U) q d + ψp,q(V ) q d ) 1 q ≤ 2 1qψ 1d (5.5.5)\nand in layer d we have:\n‖Wd‖p = ( αpψp,q(U) p d + (1− α)pψp,q(V ) p d ) 1 p ≤ 21/p−1ψ1/d (5.5.6)\nCombining inequalities (5.5.5) and (5.5.6), we get ψdp,q(fW ) ≤ 2 d−1 q + 1 pψ ≤ ψ, where the last inequality holds because we assume that 1q ≤ 1d−1 ( 1− 1p ) . Thus for every ψ ≥ 0, Fdψp,q≤ψ is a convex set.\nNon-negative homogeneity For any function fw ∈ Fd and any α ≥ 0, letU be the weights realizing f with ψdp,q(f) = ψp,q(U). Then d √ αU realizes αf establishing ψdp,q(αf) ≤ ψp,q( d √ αU) = αψp,q(U) = αψdp,q(U) = αψ d p,q(f). This establishes the non-negative homogeneity of ψ d p,q .\nConvex sublevel sets and homogeneity imply triangular inequality Let α(f) be non-negative homogeneous and assume that every sublevel set {fw ∈ Fd : α(f) ≤ ψ} is convex. Then for f1, f2 ∈ Fd, defining ψ1 def = α(f1), ψ2 def = α(f2), f̃1 def = (ψ1 + ψ2)f1/ψ1, and f̃2 def = (ψ1 + ψ2)f2/ψ2, we have\nα(f1 + f2) = α\n( ψ1\nψ1 + ψ2 f̃1 + ψ2 ψ1 + ψ2 f̃2\n) ≤ ψ1 + ψ2 = α(f1) + α(f2).\nHere the inequality is due to the convexity of the level set and the fact that α(f̃1) = α(f̃2) = ψ1 + ψ2, because of the homogeneity. Therefore α satisfies the triangular inequality and thus it is a seminorm."
    }, {
      "heading" : "5.5.3 Path Regularization",
      "text" : "Theorem 7\nLemma 21. For any function fw ∈ Fd,Hψp,∞≤ψ there is a layered network with weights w such that ψp,∞(w) = ψ d,H p,∞(f) and for any internal unit v, ∑ (u→v)∈E |wu→v|p = 1.\nProof. Let w be the weights of a network such that ψp,∞(w) = ψd,Hp,∞(f). We now construct a network with weights w̃ such that ψp,∞(w) = ψd,Hp,∞(f) and for any internal unit v, ∑ (u→v)∈E |w̃(u→ v)|p = 1. We do this by an incremental algorithm. Let w0 = w. At each step i, we do the following.\n47\nConsider the first layer, Set Vk to be the set of neurons in the layer k. Let x be the maximum of `p norms of input weights to each neuron in set V1 and let Ux ⊆ V1 be the set of neurons whose `p norms of their input weight is exactly x. Now let y be the maximum of `p norms of input weights to each neuron in the set V1 \\ Ux and let Uy be the set of the neurons such that the `p norms of their input weights is exactly y. Clearly y < x. We now scale down the input weights of neurons in set Ux by y/x and scale up all the outgoing edges of vertices in Ux by x/y (y cannot be zero for internal neurons based on the definition). It is straightforward that the new network realizes the same function and the `p,∞ norm of the first layer has changed by a factor y/x. Now for every neuron v ∈ V2, let r(v) be the `p norm of the new incoming weights divided by `p norm of the original incoming weights. We know that r(v) ≤ x/y. We again scaly down the input weights of everyv ∈ V2 by 1/r(v) and scale up all the outgoing edges of v by r(v). Continuing this operation to on each layer, each time we propagate the ratio to the next layer while the network always realizes the same function and for each layer k, we know that for every v ∈ Vk, r(v) ≤ x/y. After this operation, in the network, the `p,∞ norm of the first layer is scaled down by y/x while the `p,∞ norm of the last layer is scaled up by at most x/y and the `p,∞ norm of the rest of the layers has remained the same. Therefore, if wi is the new weight setting, we have ψp,∞(wi) ≤ ψp,∞(wi−1). After continuing the above step at most |V1| − 1 times, the `p norm of input weights is the same for all neurons in V1. We can then run the same algorithm on other layers and at the end we have a network with weight setting w̃ such that the for each k < d, `p norm of input weight to each of the neurons in layer k is equal to each other and ψp,∞(w̃) ≤ ψp,∞(w). This is in fact an equality because weight setting w′ realizes function f and we know that ψp,∞(w) = ψd,Hp,∞(f). A simple scaling of weights in layers gives completes the proof.\nTheorem 7. For p ≥ 1, any d and (finite or infinite) H , for any fw ∈ Fd,H : φd,Hp (f) = ψd,Hp,∞.\nProof. By the Lemma 21, there is a layered network with weights w̃ such that ψp,∞(w̃) = ψd,Hp,∞(f) and for any internal unit v, ∑ (u→v)∈E |w̃(u→ v)|p = 1. Let w be the weights of the layered network that corresponds to the function w̃. Then we have:\nvp(w̃) =  ∑ vin[i] e1→v1 e2→v2··· ek→vout k∏ i=1 |w̃(ei)|p  1 p\n(5.5.7)\n=  H∑ id−1=1 · · · H∑ i1=1 nin∑ i0=1 |Wd[id−1]|p d−1∏ k=1 |W k[ik, ik−1]|p  1p (5.5.8)\n=  H∑ id−1=1 |Wd[id−1]|p · · · H∑ i1=1 |W k[i2, i1]|p nin∑ i0=1 |W k[i1, i0]|p  1p (5.5.9)\n=  H∑ id−1=1 |Wd[id−1]|p · · · H∑ i1=1 |W k[i2, i1]|p  1p (5.5.10)\n=  H∑ id−1=1 |Wd[id−1]|p · · · H∑ i2=1 |W k[i3, i2]|p  1p (5.5.11)\n=  H∑ id−1=1 |Wd[id−1]|p  1p = `p(Wd) = ψp,∞(w) (5.5.12)\n(5.5.13)\n48\nwhere inequalities 5.5.8 to 5.5.12 are due to the fact that the `p norm of input weights to each internal neuron is exactly 1 and the last equality is again because `p,∞ of all layers is exactly 1 except the layer d.\nProof of Theorem 8\nIn this section, without loss of generality, we assume that all the internal nodes in a DAG have incoming edges and outgoing edges because otherwise we can just discard them. Let nout(v) be the longest directed path from vertex v to vout and nin(v) be the longest directed path from any input vertex vin[i] to v. We say graph G is a sublayered graph if G is a subgraph of a layered graph.\nWe first show the necessary and sufficient conditions under which a DAG is a sublayered graph.\nLemma 22. The graph G(E, V ) is a sublayered graph if and only if any path from input nodes to the output nodes has length d where d is the length of the longest path in G\nProof. Since the internal nodes have incoming edges and outgoing edges; hence if G is a sublayered graph it is straightforward by induction on the layers that for every vertex v in layer i, there is a vertex u in layer i+ 1 such that (v → u) ∈ E and this proves the necessary condition for being sublayered graph. To show the sufficient condition, for any internal node u, u has nin(v) distance from the input node in every path that includes u (otherwise we can build a path that is longer than d). Therefore, for each vertex v ∈ V , we can place vertex v in layer nin(v) and all the outgoing edges from v will be to layer nin(v) + 1.\nLemma 23. If the graph G(E, V ) is not a sublayered graph then there exists a directed edge (u→ v) such that nin(u) + nout(v) < d− 1 where d the length of the longest path in G.\nProof. We prove the lemma by an inductive argument. If G is not sublayered, by lemma 22, we know that there exists a path v0 → . . . vi · · · → vd′ where v0 is an input node (nin(v0) = 0), vd′ = vout (nout(vd′ = 0) and d′ < d. Now consider the vertex v1. We need to have nout(v1) = d− 1 otherwise if nout(v1) < d− 1 we get nin(u) + nout(v) < d− 1 and if nout(v1) > d− 1 there will be path in G that is longer than d. Also, since nout(v1) = d− 1 and the longest path in G has length d, we have nin(v1) = 1. By applying the same inductive argument on each vertex vi in the path we get nin(vi) = i and nout(vi) = d−i. Note that if the condition nin(u)+nout(v) < d−1 is not satisfied in one of the steps of the inductive argument, the lemma is proved. Otherwise, we have nin(vd′−1) = d′ − 1 and nout(vd′−1) = d− d′ + 1 and therefore nin(vd′−1) + nout(vout) = d′ − 1 < d− 1 that proves the lemma.\nTheorem 8. For any p ≥ 1 and any d: ψdp,∞(f) = min G ∈ DAG(d) φGp (f).\nProof. Consider any fw ∈ FDAG(d) where the graph G(E, V ) is not sublayered. Let ρ be the total number of paths from input nodes to the output nodes. Let T be sum over paths of the length of the path. We indicate an algorithm to change G into a sublayered graph G̃ of depth d with weights w̃ such that fw = fG̃,w̃ and φ(w) = φ(w̃). Let G0 = G and w0 = w.\nAt each step i, we consider the graph Gi−1. If Gi−1 is sublayered, we are done otherwise by lemma 23, there exists an edge (u→ v) such that nin(u) + nout(v) < d− 1. Now we add a new vertex ṽi to graph Gi−1, remove the edge (u→ v), add two edges (u→ ṽi) and (ṽi → v) and return the graph as Gi and since we had nin(u) + nout(v) < d − 1 in Gi−1, the longest path in Gi still has length d. We also set w(u → ṽi) = √ |wu→v| and w(ṽi → v) = sign(wu→v) √ |wu→v|. Since we are using rectified linear\n49\nunits activations, for any x > 0, we have [x]+ = x and therefore:\nw(ṽi → v) [w(u→ ṽi)o(u)]+ = sign(wu→v) √ |wu→v| [√ |wu→v|o(u) ] +\n= sign(wu→v) √ |wu→v| √ |wu→v|o(u)\n= wu→vo(u)\nSo we conclude that fGi,wi = fGi−1,wi−1 . Clearly, since we didn’t change the length of any path from input vertices to the output vertex, we have φ(w) = φ(w̃). Let Ti be sum over paths of the length of the path in Gi. It is clear that Ti−1 ≤ Ti because we add a new edge into a path at each step. We also know by lemma 22 that if Ti = ρd, then Gi is a sublayered graph. Therefore, after at most ρd− T0 steps, we return a sublayered graph G̃ and weights w̃ such that fw = fG̃,w̃. We can easily turn the sublayered graph G̃ a layered graph by adding edges with zero weights and this together with Theorem 7 completes the proof."
    }, {
      "heading" : "5.5.4 Hardness of Learning Neural Networks",
      "text" : "Daniely et al. [11] show in Theorem 5.4 and in Section 7.2 that subject to the strong random CSP assumption, for any k = ω(1) the model class of intersection of homogeneous halfspaces over {±1}n with normals in {±1} is not efficiently PAC learnable (even improperly)1. Furthermore, for any > 0, [55] prove this hardness result subject to intractability of Q̃(n1.5in )-unique shortest vector problem for k = n in.\nIf it is not possible to efficiently PAC learn intersection of halfspaces (even improperly), we can conclude it is also not possible to efficiently PAC learn any model class which can represent such intersection. In Theorem 24 we show that intersection of homogeneous half spaces can be realized with unit margin by neural networks with bound norm.\nTheorem 24. For any k > 0, the intersection of k homogeneous half spaces is realizable with unit margin by F2ψp,q≤ψ where ψ = 4n 1 p ink 2.\nProof. The proof is by a construction that is similar to the one in [56]. For each hyperplane 〈wi, x〉 > 0, where wi ∈ {±1}nin , we include two units in the first layer: g+i (x) = [〈wi, x〉]+ and g−i (x) = [〈wi, x〉 − 1]+. We set all incoming weights of the output node to be 1. Therefore, this network is realizing the following function:\nf(x) = k∑ i=1 ([〈wi, x〉]+ − [〈wi, x〉 − 1]+)\nSince all inputs and all weights are integer, the outputs of the first layer will be integer, ([〈wi, x〉]+ − [〈wi, x〉 − 1]+) will be zero or one, and f realizes the intersection of the k halfspaces with unit margin. Now, we just need to make sure that ψ2p,q(f) is bounded by ψ = 4n 1 p ink 2:\nψ2p,q(f) = n 1 p in (2k) 1 q (2k) 1 p\n≤ n 1 p\nin (2k) 2 = ψ.\n1Their Theorem 5.4 talks about unrestricted halfspaces, but the construction in Section 7.2 uses only data in {±1}nin and halfspaces specified by 〈w, x〉 > 0 with w ∈ {±1}nin\n50"
    }, {
      "heading" : "5.6 Discussion",
      "text" : "We presented a general framework for norm-based capacity control for feed-forward networks, and analyzed when the norm-based control is sufficient and to what extent capacity still depends on other parameters. In particular, we showed that in depth d > 2 networks, per-unit control with p > 1 and overall regularization with p > 2 is not sufficient for capacity control without also controlling the network size. This is in contrast with linear models, where with any p < ∞ we have only a weak dependence on dimensionality, and two-layer networks where per-unit p = 2 is also sufficient for capacity control. We also obtained generalization guarantees for perhaps the most natural form of regularization, namely `2 regularization, and showed that even with such control we still necessarily have an exponential dependence on the depth.\nAlthough the additive µ-measure and multiplication ψ-measure are equivalent at the optimum, they behave rather differently in terms of optimization dynamics (based on anecdotal empirical experience) and understanding the relationship between them, as well as the novel path-based regularizer can be helpful in practical regularization of neural networks.\nAlthough we obtained a tight characterization of when size-independent capacity control is possible, the precise polynomial dependence of margin-based classification (and other tasks) on the norm in might not be tight and can likely be improved, though this would require going beyond bounding the Rademacher complexity of the real-valued class. In particular, Theorem 3 gives the same bound for per-unit `1 regularization and overall `1 regularization, although we would expect the later to have lower capacity.\nBeyond the open issue regarding depth-independent ψ-based capacity control, another interesting open question is understanding the expressive power of Fdψp,q≤ψ, particularly as a function of the depth d. Clearly going from depth d = 1 to depth d = 2 provides additional expressive power, but it is not clear how much additional depth helps. The class F2 already includes all binary functions over {±1}nin and is dense among continuous real-valued functions. But can the ψ-measure be reduced by increasing the depth? Viewed differently: ψdp,q(f) is monotonically non-increasing in d, but are there functions for it continues decreasing? Although it seems obvious there are functions that require high depth for efficient representation, these questions are related to decade-old problems in circuit complexity and might not be easy to resolve.\n51\nChapter 6\nSharpness/PAC-Bayes Generalization Bounds\nSo far we discussed norm based and sharpness based complexity measures to understand capacity. We also have discussed how to combine these two notions and the tradeoff in scaling between them under the PAC-Bayes framework. We next show how to utilize the general PAC-Bayes bound in Lemma 1 to prove generalization guarantees for feedforward networks based on the spectral norm of its layers."
    }, {
      "heading" : "6.1 Spectrally-Normalized Margin Bounds",
      "text" : "As we discussed in Section 3.4, understanding the sharpness of the network is the key step to obtain a generalization bound using PAC-Bayes framework. The following lemma shows that the sharpness can be bounded by the product of spectral norm of the layers.\nLemma 25 (Perturbation Bound). For any B, d > 0, let fw : XB,n → Rk be a d-layer network. Then for any x ∈ XB,n and any perturbation u such that ‖Ui‖2 ≤ 1d ‖Wi‖2, the sharpness of fw can be bounded as follows:\n|fw+u(x)− fw(x)|2 ≤ eB ( d∏ i=1 ‖Wi‖2 ) d∑ i=1 ‖Ui‖2 ‖Wi‖2 . (6.1.1)\nNext, we derive a generalization guarantee using Lemmas 1 and 25.\nTheorem 26 (Generalization Bound). For any B, d, h > 0, let fw : XB,n → Rk be a d-layer feedforward network with ReLU activations. Then for any probability δ, margin γ > 0, the following generalization bound holds with probability 1− δ over the training set:\nL0(fw) ≤ ˆ̀γ(fw) +O √B2d2h ln(dh)Πdi=1 ‖Wi‖2∑di=1 (‖Wi‖2F / ‖Wi‖22)+ ln dmδ γ2m  . (6.1.2) Proof. The proof involves mainly two steps. In the first step we calculate what is the maximum allowed perturbation of parameters to satisfy a given margin condition γ, using Lemma 25. In the second step we calculate the KL term in the PAC-Bayes bound in Lemma 1, for this value of perturbation.\nLet β = ( Πdi=1 ‖Wi‖2 )1/d and consider the reparametrization W̃i = β‖Wi‖2Wi. Since for feedforward network with ReLU activations fw̃ = fw, the bound in the theorem statement is invariant to this reparametrization. W.l.o.g. we assume that for any layer i, ‖Wi‖2 = β. Choose the prior P to be\n52\nN (0, σ2pI) and consider the random perturbation u ∼ N (0, σ2qI). The following inequality holds on the spectral norm of Ui [57]:\nPUi∼N(0,σq) [‖Ui‖2 > t] ≤ 2he−t 2/2hσ2q . (6.1.3)\nTaking a union bond over the layers, we get spectral norm of perturbation in each layer is bounded by σq √ 2h ln(4dh). Define set S as { u ∣∣ ‖Ui‖2 ≤ σq√2h ln(4dh)}. Given the bound on spectral norm of each layer, u ∈ S with probability at least 12 . Let β̂ be an estimate of β that is picked before observing data. If |β̂ − β| ≤ 1dβ, then 1eβd−1 ≤ β̂d−1 ≤ eβd−1. Using Lemma 25 with probability at least 12 :\nmax x∈XB,n\n|fw+u(x)− fw(x)| ≤ edBβd−1 ‖Ui‖2 ≤ e2dBβ̂d−1σq √\n2h ln(4dh) ≤ γ 4 ,\nwhere we choose σq = γ 42dBβ̂d−1 √ h ln(4hd) to get the last inequality.\nLet q(z) be the density function of the posterior. We now calculate the KL-term in Lemma 1 for σp = σq on the set S:\nKLS(w + u||P ) = ∫ S q(z) 2 〈z,w〉 − |w|2 2σ2q dz ≤ |w| 2 2σ2q ≤ O ( B2d2h ln(dh) Πdi=1 ‖Wi‖22 γ2 d∑ i=1 ‖Wi‖2F ‖Wi‖22 )\nFinally it remains to show how to find the estimates β̂. We only need to consider values of β in the range ( γ\n2B )1/d ≤ β ≤ (γ√m2B )1/d. For β outside this range the theorem statement holds trivially. Recall that LHS of the theorem statement, L0(fw) is always bounded by 1. If βd < γ2B then for any x, |fw(x)| ≤ βdB ≤ γ/2 and therefore Lγ = 1. Alternately, if βd > γ √ m\n2B , then the second term in equation 3.4 is greater than one. Hence, we only need to consider values of β in the range discussed above. Since we need |β̂ − β| ≤ 1dβ ≤ 1d ( γ 2B )1/d , the size of this cover is dm 1 2d . Taking a union bound over this cover and using Lemma 1 gives us the theorem statement."
    }, {
      "heading" : "6.2 Generalization Bound based on Expected Sharpness",
      "text" : "We showed how bounding the sharpness could give us a generalization bound. We now establish sufficient conditions to bound the expected sharpness of a feedforward network with ReLU activations. Such conditions serve as a useful guideline in studying what helps an optimization method to converge to less sharp optima. Unlike existing generalization bounds [28, 58, 29, 30, 31], our sharpness based bound does not suffer from exponential dependence on depth.\nNow we discuss the conditions that affect the sharpness of a network. As discussed earlier, weak interactions between layers can cause the network to have high sharpness value. Condition C1 below prevents such weak interactions (cancellations). A network can also have high sharpness if the changes in the number of activations is exponential in the perturbations to its weights, even for small perturbations. Condition C2 avoids such extreme situations on activations. Finally, if a non-active node with large weights becomes active because of the perturbations in lower layers, that can lead to huge changes to the output of the network. Condition C3 prevents having such spiky (in magnitude) hidden units. This leads us to the following three conditions, that help in avoiding such pathological cases.\n(C1) : Given x, let x = W0 and D0 = I . Then, for all 0 ≤ a < c < b ≤ d, ‖ ( Πbi=aDiWi ) ‖F ≥\nµ√ hc ‖Πbi=c+1DiWi‖F ‖ (Πci=aDiWi) ‖F .\n(C2) : Given x, for any level k, 1hk ∑ i∈[hk] 1Wk,iΠk−1j=1DjWjx≤δ ≤ C2δ.\n(C3) : For all i, ‖Wi‖22,∞hi ≤ C23‖DiWi‖2F .\n53\nHere, Wk,i denotes the weights of the ith output node in layer k. ‖Wi‖2,∞ denotes the maximum L2 norm of a hidden unit in layer i. Now we state our result on the generalization error of a ReLU network, in terms of average sharpness and its norm. Let ‖x‖ = 1 and h = maxdi=1 hi. Theorem 27. Let Ui be a random hi × hi−1 matrix with each entry distributed according to N (0, σ2i ). Then, under the conditions C1, C2, C3, with probability ≥ 1− δ,\nEu∼N (0,σ)n [L(fw+u)]− ˆ̀(fw) ≤ O ([ Πdi=1 (1 + γi)− 1\n+Πdi=1 (1 + γiC2C3) ( Πdi=1(1 + γiCδC2)− 1 )] CL ∑ x ‖fw(x)‖F m ) + √√√√ 1 m ( d∑ i=1 ‖Wi‖2F σ2i + ln 2m δ ) .\nwhere γi = σi √ hi √ hi−1 µ2‖Wi‖F and Cδ = 2 √ ln(dh/δ).\nTo understand the above generalization error bound, consider choosing γi = σCδd , and we get a bound that simplifies as follows:\nEu∼N (0,σ)n [L(fw+u]− ˆ̀(fw) ≤ O ( σ (1 + (1 + σC2C3)C2)CL ∑ x ‖fw(x)‖F\nm\n)\n+ √√√√ 1 m ( d2 µ4 d∑ i=1 hihi−1 σ2 + ln 2m δ )\nIf we choose large σ, then the network will have higher expected sharpness but smaller ’norm’ and vice versa. Now one can optimize over the choice of σ to balance between the terms on the right hand side and get a better capacity bound. For any reasonable choice of σ, the generalization error above, depends only linearly on depth and does not have any exponential dependence, unlike other notions of generalization. Also the error gets worse with decreasing µ and increasing C2, C3 as the sharpness of the network increases which is in accordance with our discussion of the conditions above.\nAdditionally the conditions C1 − C3 actually hold for networks trained in practice as we verify in Figure 6.1, and our experiments suggest that, µ ≥ 1/4, C2 ≤ 5 and C3 ≤ 3. More details on the verification and comparing the conditions on learned network with those of random weights, are presented in Section 6.4.\n54\nProof of Theorem 27 We bound the expectation as follows: E ∣∣∣ˆ̀(fw+u(x))− ˆ̀(fw(x))∣∣∣ ≤ CLE‖fw+u(x)− fw(x)‖F (i) = CLE‖(W + u)d ( Πd−1i=1 D̂i(W + u)i ) ∗ x−Wd ( Πd−1i=1DiWi ) ∗ x‖F\n≤ CLE‖(W + u)d ( Πd−1i=1Di(W + u)i ) ∗ x−Wd ( Πd−1i=1DiWi ) ∗ x‖F\n+ CLE‖(W + u)d ( Πd−1i=1 D̂i(W + u)i ) ∗ x− (W + u)d ( Πd−1i=1Di(W + u)i ) ∗ x‖F\n≤ CLE‖(W + u)d ( Πd−1i=1Di(W + u)i ) ∗ x−Wd ( Πd−1i=1DiWi ) ∗ x‖F + CLE‖Errd‖F ,\n(6.2.1)\nwhere Errd = ‖(W + u)d ( Πd−1i=1 D̂i(W + u)i ) ∗ x− (W + u)d ( Πd−1i=1Di(W + u)i ) ∗ x‖F . (i) D̂i is the diagonal matrix with 0’s and 1’s corresponding to the activation pattern of the perturbed network fw+u(x).\nThe first term in the equation (6.2.1) corresponds to error due to perturbation of a network with unchanged activations (linear network). Intuitively this is small when any subset of successive layers of the network do no interact weakly with each other (not orthogonal to each other). Condition C1 captures this intuition and we bound this error in Lemma 6.4.1.\nLemma 28. Let Ui be a random hi × hi−1 matrix with each entry distributed according to N (0, σ2i ). Then, under the condition C1,\nE‖(W + u)d ( Πd−1i=1Di(W + u)i ) ∗ x−Wd ( Πd−1i=1DiWi ) ∗ x‖F\n≤ (\nΠdi=1\n( 1 + σi √ hihi−1\nµ2‖DiWi‖F\n) − 1 ) ‖fw(x)‖F .\nThe second term in the equation (6.2.1) captures the perturbation error due to change in activations. If a tiny perturbation can cause exponentially many changes in number of active nodes, then that network will have huge sharpness. Condition C2 and C3 essentially characterize the behavior of sensitivity of activation patterns to perturbations, leading to a bound on this term in Lemma 29.\nLemma 29. Let Ui be a random hi × hi−1 matrix with each entry distributed according to N (0, σ2i ). Then, under the conditions C1, C2 and C3, with probability ≥ 1− δ, for all 1 ≤ k ≤ d,\n‖D̂k −Dk‖1 ≤ O ( C2hkCδσk‖fk−1w ‖F ) and\nE [‖]Errk‖F ≤ O ( Πki=1 (1 + γiC2C3) ( Πki=1(1 + γiCδC2)− 1 ) ‖fkw‖F ) .\nwhere γi = σi √ hi √ hi−1 µ2‖DiWi‖F and Cδ = 2 √ ln(dh/δ).\nHence, from Lemma 6.4.1 and Lemma 29 we get, E ∣∣∣ˆ̀(fw+u(x))− ˆ̀(fw(x))∣∣∣ ≤ [ Πdi=1 (1 + γi)− 1 + Πdi=1 (1 + γiC2C3) ( Πdi=1(1 + γiCδC2)− 1 )] CL‖fw(x)‖F .\nHere γi = σi √ hi √ hi−1\nµ2‖DiWi‖F . Substituting the above bound on expected sharpness in the PAC-Bayes result (equa-\ntion (3.4)), gives the result.\n55"
    }, {
      "heading" : "6.3 Supporting Results",
      "text" : ""
    }, {
      "heading" : "6.3.1 Supporting Lemma",
      "text" : "Lemma 30. Let A ,B be n1×n2 and n3×n4 matrices and u be a n2×n3 entrywise random Gaussian matrix with uij ∼ N (0, σ). Then,\nE [‖A ∗ u ∗B‖F ] ≤ σ‖A‖F ‖B‖F .\nProof. By Jensen’s inequality,\nE [‖A ∗ u ∗B‖F ]2 ≤ E [ ‖A ∗ u ∗B‖2F ] = E  ∑\nij ∑ kl AikuklBlj\n2 \n= ∑ ij ∑ kl A2ikE [ u2kl ] B2lj\n= σ2‖A‖2F ‖B‖2F ."
    }, {
      "heading" : "6.3.2 Conditions in Theorem 27",
      "text" : "In this section, we compare the conditions in Theorem 27 of a learned network with that of its random initialization. We trained a 10-layer feedforward network with 1000 hidden units in each layer on MNIST dataset. Figures 6.2, 6.3 and 6.4 compare condition C1, C2 and C3 on learned weights to that of random initialization respectively. Interestingly, we observe that the network with learned weights is very similar to its random initialization in terms of these conditions."
    }, {
      "heading" : "6.4 Proofs",
      "text" : ""
    }, {
      "heading" : "6.4.1 Proof of Lemma 25",
      "text" : "Proof. Let ∆i = ∣∣f iw+u(x)− f iw(x)∣∣2 be the sharpness of layer i. We will prove using induction that for any i ≥ 0:\n∆i ≤ ( 1 + 1\nd )i( i∏ j=1 ‖Wj‖2 ) |x|2 i∑ j=1 ‖Uj‖2 ‖Wj‖2 .\nThe above inequality together with ( 1 + 1d )d ≤ e proves the lemma statement. The induction base clearly holds since ∆0 = |x− x|2 = 0. For any i ≥ 1, we have the following:\n56\n∆i+1 = ∣∣(Wi+1 + Ui+1)φi(f iw+u(x))−Wi+1φi(f iw(x))∣∣2\n= ∣∣(Wi+1 + Ui+1) (φi(f iw+u(x))− φi(f iw(x)))+ Ui+1φi(f iw(x))∣∣2\n≤ (‖Wi+1‖2 + ‖Ui+1‖2) ∣∣φi(f iw+u(x))− φi(f iw(x))∣∣2 + ‖Ui+1‖2 ∣∣φi(f iw(x))∣∣2\n≤ (‖Wi+1‖2 + ‖Ui+1‖2) ∣∣f iw+u(x)− f iw(x)∣∣2 + ‖Ui+1‖2 ∣∣f iw(x)∣∣2\n= ∆i (‖Wi+1‖2 + ‖Ui+1‖2) + ‖Ui+1‖2 ∣∣f iw(x)∣∣2,\nwhere the last inequality is by the Lipschitz property of the activation function and using φ(0) = 0. The `2 norm of outputs of layer i is bounded by |x|2Πij=1 ‖Wj‖2 and by the lemma assumption we have\n57\n‖Ui+1‖2 ≤ 1d ‖Wi+1‖2. Therefore, using the induction step, we get the following bound:\n∆i+1 ≤ ∆i ( 1 + 1\nd\n) ‖Wi+1‖2 + ‖Ui+1‖2 |x|2 i∏ j=1 ‖Wj‖2\n≤ ( 1 + 1\nd )i+1(i+1∏ j=1 ‖Wj‖2 ) |x|2 i∑ j=1 ‖Uj‖2 ‖Wj‖2 + ‖Ui+1‖2 ‖Wi+1‖2 |x|2 i+1∏ j=1 ‖Wi‖2\n≤ ( 1 + 1\nd )i+1(i+1∏ j=1 ‖Wj‖2 ) |x|2 i+1∑ j=1 ‖Uj‖2 ‖Wj‖2 .\nThis completes the proof."
    }, {
      "heading" : "6.4.2 Proof of Lemma 28",
      "text" : "Proof. Define g{W−i−j ,ui,j}(x) as the network fW with weights in layers i, j,, Wi,Wj replaced by Ui,uj . Hence,\n‖(W + u)d ( Πd−1i=1Di(W + u)i ) ∗ x−Wd ( Πd−1i=1DiWi ) ∗ x‖F\n≤ ‖ ∑ i g({W−i, Ui}, x)‖F + ‖ ∑ i,j g({W−i−j ,ui,j}, x)‖F + · · ·+ ‖fu(x)‖F (6.4.1)\nBase case: First we show the bound for terms with one noisy layer. Let g({W−k,uk}, x) denote fW (x) with weights in layer k, Wk replaced by uk. Now notice that,\nE‖g({W−k,uk}, x)‖F = E‖WdΠd−1i=k+1DiWi ∗Dkuk ∗ ( Πk−1i=1 DiWi ) ∗ x‖F\n(i) ≤ σk‖WdΠd−1i=k+1DiWi‖F ‖‖ ( Πk−1i=1 DiWi ) ∗ x‖F\n(ii) ≤ σk √ hkhk−1\nµ2‖DkWk‖F ‖Wd\n( Πd−1i=1DiWi ) ∗ x‖F\n= σk\n√ hkhk−1\nµ2‖DkWk‖F ‖fW (x)‖F .\n(i) follows from Lemma 30. (ii) follows from condition C1.\nInduction step: Let for any set s ⊂ [d], |s| = k, the following holds:\nE‖g({W−i,ui}i∈s, x)‖F ≤ ‖fW (x)‖FΠi∈sσi √ hihi−1\nµ2‖DiWi‖F .\n58\nWe will prove this now for terms with k + 1 noisy layers.\nE‖g({W−i,ui}i∈s∪{j}, x)‖F ≤ σj √ hjhj−1\nµ2‖DjWj‖ E‖g({W−i,ui}i∈s, x)‖F\n≤ σj √ hjhj−1\nµ2‖DjWj‖ ‖fW (x)‖FΠi∈sσi\n√ hihi−1\nµ2‖DiWi‖F = ‖fW (x)‖FΠi∈s∪{j}σi √ hihi−1\nµ2‖DiWi‖F\nSubstituting the above expression in equation (6.4.1) gives,\n‖(W + u)d ( Πd−1i=1Di(W + u)i ) ∗ x−Wd ( Πd−1i=1DiWi ) ∗ x‖F\n≤ (\nΠdi=1\n( 1 + σi √ hi √ hi−1\nµ2‖DiWi‖F\n) − 1 ) ‖fW (x)‖F ."
    }, {
      "heading" : "6.4.3 Proof of Lemma 29",
      "text" : "Proof. We prove this lemma by induction on k. Recall that D̂i is the diagonal matrix with 0’s and 1’s corresponding to the activation pattern of the perturbed network fW+u(x). Let 1E denote the indicator function, that is 1 if the event E is true, 0 else. We also use fkW (x) to denote the network truncated to level k, in particular fkW (x) = Π k i=1DkWkx.\nBase case:\n‖D̂1 −D1‖1 = ∑ i 1〈(W+u)1,i,x〉∗〈W1,i,x〉<0 = ∑ i 1〈(W )1,i,x〉2<−〈(u)1,i,x〉∗〈(W )1,i,x〉\n≤ ∑ i 1|〈(W )1,i,x〉|<|〈(u)1,i,x〉|.\nSince u1 is a random Gaussian matrix, and ‖x‖ ≤ 1, for any i, |〈(u)1,i, x〉| ≤ σ1(1 + δ1) √\n2 ln(h1) with probability greater than 1− δ1. Hence, with probability ≥ 1− δ1,\n‖D̂1 −D1‖1 ≤ ∑ i 1|〈(W )1,i,x〉|≤σ1 √ 20 ln(h1) ≤ C2h1σ1(1 + δ1) √ 2 ln(h1) = C2h1σ1Cδ1 .\nThis completes the base case for k = 1. D̂1 is a random variable that depends on u1. Hence, in the remainder of the proof, to avoid this dependence, we separately bound D̂1 −D using the expression above and compute expectation only with respect to u1. With probability ≥ 1− δ1,\n59\nE‖Err1‖F = E‖D̂1 ∗ (W + u)1x−D1 ∗ (W + u)1x‖F ≤ E‖(D̂1 −D1) ∗W1x‖F + E‖(D̂1 −D1) ∗ u1x‖F (i)\n≤ √ C2h1σ1Cδ1σ1 + √ C2h1σ1Cδ1σ1\n= 2 √ C2h1σ1Cδ1σ1.\n(i) follows because, each hidden node in E‖(D̂1−D1)∗W1x‖F has norm less than σ1Cδ1 (as it changed its activation), number of such units is less than C2h1σ1Cδ1 .\nk = 1 case does not capture all the intricacies and dependencies of higher layer networks. Hence we also evaluate the bounds for k = 2.\n‖D̂2 −D2‖1 ≤ ∑ i 1〈(W+u)2,i,f1W+u〉∗〈W2,i,f1W 〉≤0 ≤ ∑ i 1|〈W2,i,f1W 〉|≤|〈u2,i,f1W+u〉|+|〈W2,i,f1W+u−f1W 〉| Let Cδ2 = (1 + δ2) √ 2 ln(h2). Then, with probability ≥ 1− δ1 − δ2,\n∣∣〈u2,i, f1W+u〉∣∣+ ∣∣〈W2,i, f1W+u − f1W 〉∣∣ ≤ Cδ2σ2 ( ‖f1W ‖F + 2 √ C2h1σ1Cδ1σ1 ) + ‖W2,i‖2 √ C2h1σ1Cδ1σ1\n≤ Cδ2σ2 ( ‖f1W ‖F + 2 √ C2h1σ1Cδ1σ1 ) + C3\n‖D2W2‖F√ h2\n2 √ C2h1σ1Cδ1σ1\n(i) ≤ Cδ2σ2 ( ‖f1W ‖F + 2 √ σ̂1√\nhi + hi−1 σ̂1\n) + 2σ̂1 C3‖fW (x)‖ 1/d F\nµ\n√ σ̂1√\nhi + hi−1\n= Cδ2σ2 ( ‖f1W ‖F + γ1σ̂1 ) + C3‖fW (x)‖ 1/d F\nµ γ1σ̂1\nwhere, γi = 2 √\nσ̂1√ hi+hi−1 . (i) follows from conditionC1, which results in Πdi=2 µ‖DiWi‖F√ hi µ‖D1W1x‖F√ h1 ≤\n‖fW (x)‖F . Hence, if we consider the rebalanced network1 where all layers have same values for µ‖DiWi‖F√\nhi , we get, µ‖DiWi‖F√ hi ≤ ‖fW (x)‖ 1/d F . Also the above equations follow from setting, σi =\nσ̂i C2Cδi √ hi+hi−1 . Hence, with probability ≥ 1− δ1 − δ2,\n‖D̂2 −D2‖1 ≤ C2 ∗ h2 ( Cδ2σ2 ( ‖f1W ‖F + γ1σ̂1 ) + C3‖fW (x)‖ 1/d F\nµ γ1σ̂1\n) .\nSince, we choose σi to scale as some small number O(σ), in the above expression the first term scales as O(σ) and the last two terms decay at least as O(σ32). Hence we do not include them in the computation of Err.\n1The parameters of ReLu networks can be scaled between layers without changing the function\n60\nE‖Err2‖F = E‖D̂2(W + u)2 ∗ D̂1 ∗ (W + u)1x−D2(W + u)2 ∗D1 ∗ (W + u)1x‖F ≤ E‖(D̂2 −D2)(W + u)2 ∗ (D̂1 −D1) ∗ (W + u)1x‖F + E‖D2(W + u)2 ∗ (D̂1 −D1) ∗ (W + u)1x‖F\n+ E‖(D̂2 −D2)(W + u)2 ∗D1 ∗ (W + u)1x‖F .\nWe will bound now the first term in the above expression. With probability ≥ 1− δ1 − δ2,\nE‖(D̂2 −D2)(W + u)2 ∗ (D̂1 −D1) ∗ (W + u)1x‖F ≤ E‖(D̂2 −D2)W2 ∗ (D̂1 −D1) ∗W1x‖F + E‖(D̂2 −D2)W2 ∗ (D̂1 −D1) ∗ u1x‖F\n+ E‖(D̂2 −D2)u2 ∗ (D̂1 −D1) ∗W1x‖F + E‖(D̂2 −D2)u2 ∗ (D̂1 −D1) ∗ u1x‖F ≤ 2 √ C2 ∗ h2Cδ2σ2‖f1W ‖FCδ2σ2‖f1W ‖F √ C2 ∗ h1 ∗ Cδ1σ1Cδ1σ1\n+ 2 √ C2 ∗ h2Cδ2σ2‖f1W ‖FCδ2σ2 √ h1 √ C2 ∗ h1 ∗ Cδ1σ1Cδ1σ1 +O(σ2)\n≤ 4‖f2W ‖F Cδ2σ2Cδ1σ1\n√ h1\nµ‖D2W2‖F Π2i=1\n√ C2hiCδiσi.\nInduction step:\nNow we assume the statement for all i ≤ k and prove it for k+ 1. ‖D̂k−Dk‖1 ≤ C2hkCδkσk‖fk−1W ‖F and E [‖]Errk‖F ≤ Πki=1 ( 1 + σi √ hi−1\nµ2Ci2,∞C2\n)( Πki=1(1 + σ̂i 3/2 C2 )− 1 ) ‖fkw‖F . Now we prove the state-\nment for k + 1.\n‖D̂k+1 −Dk+1‖1 = ∑ i 1〈(W+u)k+1,i,Πki=1D̂i(W+u)i∗x〉∗〈W2,i,D1W1x〉≤0\n≤ ∑ i 1|〈Wk+1,i,Πki=1D̂i(W+u)i∗x〉|≤|〈uk+1,i,Πki=1D̂i(W+u)i∗x〉|\n= ∑ i 1|〈Wk+1,i,fkW+u〉|≤|〈uk+1,i,fkW+u〉|\n≤ ∑ i 1|〈Wk+1,i,fkW 〉|≤|〈uk+1,i,fkW 〉|+|〈uk+1,i,fkW+u−fkW 〉|+|〈Wk+1,i,fkW+u−fkW 〉|\nHence, with probability ≥ 1−∑ki=1 δi, ‖D̂k+1 −Dk+1‖1 ≤ C2hk+1 [ Cδkσk+1(‖fkW ‖F + ‖fkW+u − fkW ‖F ) + ‖Wk+1,i‖‖fkW+u − fkW ‖F\n] ≤ C2hk+1Cδkσk+1‖fkW ‖F + C2hk+1Cδkσk+1‖fkW+u − fkW ‖F + C2hk+1‖Wk+1,i‖‖fkW+u − fkW ‖F .\nNow we will show that the last two terms in the above expression scale as O(σ2). For that, first notice that ‖fkW+u − fkW ‖F ≤ ( Πki=1 ( 1 + σi √ hihi−1\nµ2‖DiWi‖F\n) − 1 ) ‖fW (x)‖F + Errk, from lemma 28. Note\nthat the second term in the above expression clearly scale as O(σ2).\nHence,\n‖D̂k+1 −Dk+1‖1 ≤ C2hk+1Cδkσk+1‖fkW ‖F +O(σ2)\n61\n‖Errk+1‖ = ‖fk+1W+u − f̃k+1W+u‖F = ‖D̂k+1(W + u)k+1Πk+1i=1 D̂i(W + u)ix−Dk+1(W + u)k+1Πk+1i=1Di(W + u)ix‖F ≤ ‖(D̂k+1 −Dk+1)(W + u)k+1Πk+1i=1Di(W + u)ix‖F + ‖D̂k+1(W + u)k+1Errk‖F ≤ ‖(D̂k+1 −Dk+1)(W + u)k+1Πk+1i=1Di(W + u)ix‖F + ‖(D̂k+1 −Dk+1)(W + u)k+1Errk‖F\n+ ‖Dk+1(W + u)k+1Errk‖F\nSubstituting the bounds for D̂k+1 −Dk+1 and Errk gives us, with probability ≥ 1− ∑k+1 i=1 δi.\nE‖Errk+1‖ ≤ √ C2hk+1Cδkσk+1‖fkW ‖FCδkσk+1‖fkW ‖FE‖Πk+1i=1Di(W + u)ix‖F\n+ E‖Errk‖F (√ C2hk+1Cδkσk+1‖fkW ‖FCδkσk+1‖fkW ‖F + ‖Dk+1Wk+1‖F + σk+1 √ hk+1 ) Now we bound the above terms following the same approach as in proof of Lemma 28, by considering all possible replacements of Wi with Ui. That gives us the result.\n62\nChapter 7\nEmpirical Investigation\nIn this chapter we investigate the ability of the discussed measures to explain the different generalization phenomenon."
    }, {
      "heading" : "7.1 Complexity Measures",
      "text" : "Capacity control in terms of norm, when using a zero/one loss (i.e. counting errors) requires us in addition to account for scaling of the output of the neural networks, as the loss is insensitive to this scaling but the norm only makes sense in the context of such scaling. For example, dividing all the weights by the same number will scale down the output of the network but does not change the 0/1 loss, and hence it is possible to get a network with arbitrary small norm and the same 0/1 loss. Using a scale sensitive losses, such as the cross entropy loss, does address this issue (if the outputs are scaled down toward zero, the loss becomes trivially bad), and one can obtain generalization guarantees in terms of norm and the cross entropy loss.\nHowever, we should be careful when comparing the norms of different models learned by minimizing the cross entropy loss, in particular when the training error goes to zero. When the training error goes to zero, in order to push the cross entropy loss (or any other positive loss that diminish at infinity) to zero, the outputs of the network must go to infinity, and thus the norm of the weights (under any norm) should also go to infinity. This means that minimizing the cross entropy loss will drive the norm toward infinity. In practice, the search is terminated at some finite time, resulting in large, but finite norm. But the value of this norm is mostly an indication of how far the optimization is allowed to progress—using a stricter stopping criteria (or higher allowed number of iterations) would yield higher norm. In particular, comparing the norms of models found using different optimization approaches is meaningless, as they would all go toward infinity.\nInstead, to meaningfully compare norms of the network, we should explicitly take into account the scaling of the outputs of the network. One way this can be done, when the training error is indeed zero, is to consider the “margin” of the predictions in addition to the norms of the parameters. We refer to the margin for a single data point x as the difference between the score of the correct label and the maximum score of other labels, i.e.\nfw(x)[ytrue]− max y 6=ytrue fw(x)[y] (7.1.1)\nIn order to measure scale over an entire training set, one simple approach is to consider the “hard margin”, which is the minimum margin among all training points. However, this definition is very sensitive to extreme points as well as to the size of the training set. We consider instead a more robust notion that allows a small portion of data points to violate the margin. For a given training set and small value > 0,\n63\nwe define the margin γmargin as the lowest value of γ such that d me data point have margin lower than γ where m is the size of the training set. We found empirically that the qualitative and relative nature of our empirical results is almost unaffected by reasonable choices of (e.g. between 0.001 and 0.1).\nThe norm-based measures we investigate in this work and their corresponding capacity bounds are as follows 1:\n• `2 norm with capacity proportional to 1γ2margin ∏d i=1 4 ‖Wi‖ 2 F [58].\n• `1-path norm with capacity proportional to 1γ2margin (∑ j∈ ∏d k=0[hk] ∣∣∣∏di=1 2Wi[ji, ji−1]∣∣∣)2[28, 58]. • `2-path norm with capacity proportional to 1γ2margin ∑ j∈ ∏d k=0[hk] ∏d i=1 4hiW 2 i [ji, ji−1].\n• spectral norm with capacity proportional to 1 γ2margin\n∏d i=1 hi ‖Wi‖ 2 2.\nwhere ∏d k=0[hk] is the Cartesian product over sets [hk]. The above bounds indicate that capacity can be bounded in terms of either `2-norm or `1-path norm independent of number of parameters. The `2-path norm dependence on the number of hidden units in each layer is unavoidable. However, it is not clear that the dependence on the number of parameters is needed for the bound based on the spectral norm.\nPAC-Bayes Bound A simple way to instantiate the PAC-Based bound discussed in Section 3.4 is to set P to be a zero mean, σ2 variance Gaussian distribution. Choosing the perturbation to also be a zero mean spherical Gaussian with variance σ2 in every direction, yields the following guarantee (w.p. 1− δ over the training set):\nE ∼N (0,σ)n [L(fw+ )] ≤ L̂(fw) + E ∼N (0,σ)n [ L̂(fw+ ) ] − L̂(fw)︸ ︷︷ ︸\nexpected sharpness\n+4 √√√√√ 1m ( ‖w‖22 2σ2︸ ︷︷ ︸\nKL\n+ ln 2m\nδ\n) , (7.1.2)\nAnother interesting approach is to set the variance of the perturbation to each parameter with respect to the magnitude of the parameter. For example if σi = α|wi|+β, then the KL term in the above expression changes to ∑ i w2i 2σ2i .\nThe above generalization guarantees give a clear way to think about capacity control jointly in terms of both the expected sharpness and the norm, and as we discussed earlier indicates that sharpness by itself cannot control the capacity without considering the scaling. In the above generalization bound, norms and sharpness interact in a direct way depending on σ, as increasing the norm by decreasing σ causes decrease in sharpness and vice versa. It is therefore important to find the right balance between the norm and sharpness by choosing σ appropriately in order to get a reasonable bound on the capacity."
    }, {
      "heading" : "7.2 Experiments Settings",
      "text" : "In experiment with different network sizes, we train a two layer perceptron with ReLU activation and varying number of hidden units without Batch Normalization or dropout. In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU\n1We have dropped the term that only depend on the norm of the input. The bounds based on `2-path norm and spectral norm can be derived directly from the those based on `1-path norm and `2 norm respectively. Without further conditions on weights, exponential dependence on depth is tight but the 4d dependence might be loose [58]. We will also discuss a rather loose bound on the capacity based on the spectral norm in Section 3.3.\n64\nactivations and apply 2× 2 max-pooling with window size 2 and dropout after each stack. Convolutional layers are followed by 4× 4 average pooling, a fully connected layer with 512 hidden units and finally a linear layer is added for prediction.\nIn all experiments we train the networks using stochastic gradient descent (SGD) with mini-batch size 64, fixed learning rate 0.01 and momentum 0.9 without weight decay. In all experiments where achieving zero training error is possible, we continue training until the cross-entropy loss is less than 10−4.\nWhen calculating norms on a network with a Batch Normalization layer, we reparametrize the network to one that represents the exact same function without Batch Normalization as suggested in [60]. In all our figures we plot norm divided by margin to avoid scaling issues (see Section 7.1), where we set the margin over training set S to be 5th-percentile of the margins of the data points in S, i.e. Prc5 {fw(xi)[yi]−maxy 6=yi fw(x)[y]|(xi, yi) ∈ S} . We have also investigated other versions of the margin and observed similar behavior to this notion.\nWe calculate the sharpness, as suggested in [13] - for each parameter wi we bound the magnitude of perturbation by α(|wi|+ 1) for α = 5.10−4. In order to compute the maximum perturbation (maximize the loss), we perform 2000 updates of stochastic gradient ascent starting from the minimum, with mini-batch size 64, fixed step size 0.01 and momentum 0.9.\nTo compute the expected sharpness, we perturb each parameterwi of the model with noise generated from Gaussian distribution with zero mean and standard deviation, α(10|wi|+ 1). The expected sharpness is average over 1000 random perturbations each of which are averaged over a mini-batch of size 64. We compute the expected sharpness for different choices of α. For each value of α the KL divergence can be\ncalculated as 1α2 ∑ i ( wi (10|wi|+1) )2 ."
    }, {
      "heading" : "7.3 True Labels Vs. Random Labels",
      "text" : "As an initial empirical investigation of the appropriateness of the different complexity measures, we compared the complexity (under each of the above measures) of models trained on true versus random labels. We would expect to see two phenomena: first, the complexity of models trained on true labels should be substantially lower than those trained on random labels, corresponding to their better generalization ability. Second, when training on random labels, we expect capacity to increase almost linearly with the number of training examples, since every extra example requires new capacity in order to fit it’s random label. However, when training on true labels we expect the model to capture the true functional dependence between input and output and thus fitting more training examples should only require small increases in the capacity of the network. The results are reported in Figure 7.1. We indeed observe a gap between the complexity of models learned on real and random labels for all four norms, with the difference in increase in capacity between true and random labels being most pronounced for the `2 norm and `2-path norm.\nIn our experiments on PAC-Bayes bound, we observe that looking at both sharpness and norm in Equation 7.1.1 jointly indeed makes a better predictor for the generalization error. As discussed earlier, Dziugaite and Roy [32] numerically optimize the overall PAC-Bayes generalization bound over a family of multivariate Gaussian distributions (different choices of perturbations and priors). Since the precise way the sharpness and KL-divergence are combined is not tight, certainly not in (7.1.2), nor in the more refined bound used by Dziugaite and Roy [32], we prefer shying away from numerically optimizing the balance between sharpness and the KL-divergence. Instead, we propose using bi-criteria plots, where sharpness and KL-divergence are plotted against each other, as we vary the perturbation variance. For example, in the center and right panels of Figure 7.2 we show such plots for networks trained on true and random labels respectively. We see that although sharpness by itself is not sufficient for explaining generalization in this setting (as we saw in the left panel), the bi-criteria plots are significantly lower for the true labels. Even more so, the change in the bi-criteria plot as we increase the number of samples\n65\nsize of traning set 10K 20K 30K 40K 50K10\n20\n1025\n1030 true labels random labels\nsize of traning set 10K 20K 30K 40K 50K10\n25\n1030\n1035\nsize of traning set 10K 20K 30K 40K 50K10\n0\n102\n104\nsize of traning set 10K 20K 30K 40K 50K10\n5\n1010\n1015 `2 norm `1-path norm `2-path norm spectral norm\nFigure 7.1: Comparing different complexity measures on a VGG network trained on subsets of CIFAR10 dataset with true (blue line) or random (red line) labels. We plot norm divided by margin to avoid scaling issues (see Section 7.1), where for each complexity measure, we drop the terms that only depend on depth or number of hidden units; e.g. for `2-path norm we plot γ−2margin ∑ j∈ ∏d k=0 [hk] ∏d i=1W 2 i [ji, ji−1].We also set the margin over training set S to be 5th-percentile of the margins of the data points in S, i.e. Prc5 {fw(xi)[yi]−maxy 6=yi fw(x)[y]|(xi, yi) ∈ S}. In all experiments, the training error of the learned network is zero. The plots indicate that these measures can explain the generalization as the complexity of model learned with random labels is always higher than the one learned with true labels. Furthermore, the gap between the complexity of models learned with true and random labels increases as we increase the size of the training set.\nsize of traning set 10K 20K 30K 40K 50K\nsh ar p n es s\n0.4\n0.6\n0.8\n1\n1.2 true labels random labels\nKL #108 0 1 2 3e\nx p ec te d sh a rp n es s 0 0.05 0.1 0.15 0.2 0.25 0.3\nKL 2K 4K 6Ke\nx p ec te d sh ar p n es s\n0\n0.04\n0.08 0.12 5K 10K 30K 50K\nKL #108 0 1 2 3e\nx p ec te d sh a rp n es s 0 0.05 0.1 0.15 0.2 0.25 0.3\nKL 2K 4K 6Ke\nx p ec te d sh ar p n es s\n0\n0.04\n0.08 0.12 5K 10K 30K 50K\nFigure 7.2: Sharpness and PAC-Bayes measures on a VGG network trained on subsets of CIFAR10 dataset with true or random labels. In the left panel, we plot max sharpness, which we calculate as suggested by Keskar et al. [13] where the perturbation for parameter wi has magnitude 5.10−4(|wi|+ 1). The middle and right plots demonstrate the relationship between expected sharpness and KL divergence in PAC-Bayes analysis for true and random labels respectively. For PAC-Bayes plots, each point in the plot correspond to a choice of variable α where the standard deviation of the perturbation for the parameter i is α(10|wi|+ 1). The corresponding KL to each α is nothing but weighted `2 norm where the weight for each parameter is the inverse of the standard deviation of the perturbation.\ntrue labels random labels\nis significantly larger with random labels, correctly capturing the required increase in capacity. For example, to get a fixed value of expected sharpness such as = 0.05, networks trained with random labels require higher norm compared to those trained with true labels. This behavior is in agreement with our earlier discussion, that sharpness is sensitive to scaling of the parameters and is not a capacity control measure as it can be artificially changed by scaling the network. However, combined with the norm, sharpness does seem to provide a capacity measure."
    }, {
      "heading" : "7.4 Different Global Minima",
      "text" : "Given different global minima of the training loss on the same training set and with the same model class, can these measures indicate which model is going to generalize better? In order to verify this property, we can calculate each measure on several different global minima and see if lower values of the measure imply lower generalization error. In order to find different global minima for the training loss, we design an experiment where we force the optimization methods to converge to different global minima with varying generalization abilities by forming a confusion set that includes samples with random labels. The\n66\noptimization is done on the loss that includes examples from both the confusion set and the training set. Since deep learning models have very high capacity, the optimization over the union of confusion set and training set generally leads to a point with zero error over both confusion and training sets which thus is a global minima for the training set.\nWe randomly select a subset of CIFAR10 dataset with 10000 data points as the training set and our goal is to find networks that have zero error on this set but different generalization abilities on the test set. In order to do that, we train networks on the union of the training set with fixed size 10000 and confusion sets with varying sizes that consists of CIFAR10 samples with random labels; and we evaluate the learned model on an independent test set. The trained network achieves zero training error but as shown in Figure 7.3, the test error of the model increases with increasing size of the confusion set. The middle panel of this Figure suggests that the norm of the learned networks can indeed be predictive of their generalization behavior. However, we again observe that sharpness has a poor behavior in these experiments. The right panel of this figure also suggests that PAC-Bayes measure of joint sharpness and KL divergence, has better behavior - for a fixed expected sharpness, networks that have higher generalization error, have higher norms."
    }, {
      "heading" : "7.5 Increasing Network Size",
      "text" : "We also repeat the experiments conducted by Neyshabur et al. [27] where a fully connected feedforward network is trained on MNIST dataset with varying number of hidden units and we check the values of different complexity measures on each of the learned networks.The left panel in Figure 7.4 shows the training and test error for this experiment. While 32 hidden units are enough to fit the training data, we observe that networks with more hidden units generalize better. Since the optimization is done without any explicit regularization, the only possible explanation for this phenomenon is the implicit regularization by the optimization algorithm. Therefore, we expect a sensible complexity measure to decrease beyond 32 hidden units and behave similar to the test error. Different measures are reported for learned networks. The middle panel suggest that all margin/norm based complexity measures decrease for larger networks up to 128 hidden units. For networks with more hidden units, `2 norm and `1-path norm increase with the size of the network. The middle panel suggest that `2-path norm can provide some explanation for this phenomenon. However, as we discussed in Section 7.1, the actual complexity measure based on `2-path norm also depends on the number of hidden units and taking this into account indicates that the measure based on `2-path norm cannot explain this phenomenon. This is also the case for the margin based measure that depends on the spectral norm. In subsection 3.3 we discussed another\n67\ncomplexity measure that also depends the spectral norm through Lipschitz continuity or robustness argument. Even though this bound is very loose, it is monotonic with respect to the spectral norm that is reported in the plots. Unfortunately, we do observe some increase in spectral norm by increasing number of hidden units beyond 512. The right panel shows that the joint PAC-Bayes measure decrease for larger networks up to size 128 but fails to explain this generalization behavior for larger networks. This suggests that the measures looked so far are not sufficient to explain all the generalization phenomenon observed in neural networks.\n68\nPart II\nGeometry of Optimization and Generalization\n69\nChapter 8\nInvariances\nIn Chapter 4, we discussed how optimization is related to generalization due to the implicit regularization. Revisiting the choice of gradient descent, we recall that optimization is also inherently tied to a choice of geometry or measure of distance, norm or divergence. Gradient descent for example is tied to the `2 norm as it is the steepest descent with respect to `2 norm in the parameter space, while coordinate descent corresponds to steepest descent with respect to the `1 norm and exp-gradient (multiplicative weight) updates is tied to an entropic divergence. Moreover, at least when the objective function is convex, convergence behavior is tied to the corresponding norms or potentials. For example, with gradient descent, or SGD, convergence speeds depend on the `2 norm of the optimum. The norm or divergence can be viewed as a regularizer for the updates. There is therefore also a strong link between regularization for optimization and regularization for learning: optimization may provide implicit regularization in terms of its corresponding geometry, and for ideal optimization performance the optimization geometry should be aligned with inductive bias driving the learning [61].\nIs the `2 geometry on the weights the appropriate geometry for the space of deep networks? Or can we suggest a geometry with more desirable properties that would enable faster optimization and perhaps also better implicit regularization? As suggested above, this question is also linked to the choice of an appropriate regularizer for deep networks.\nFocusing on networks with RELU activations in this section, we observe that scaling down the incoming edges to a hidden unit and scaling up the outgoing edges by the same factor yields an equivalent network computing the same function. Since predictions are invariant to such rescalings, it is natural to seek a geometry, and corresponding optimization method, that is similarly invariant. In this chapter, we study invariances in feedforward networks with shared weights."
    }, {
      "heading" : "8.1 Invariances in Feedforward and Recurrent Neural Networks",
      "text" : "Feedforward networks are highly over-parameterized, i.e. there are many weight settings w that represent the same function fw. Since our true object of interest is the function f , and not the identity w of the weghts, it would be beneficial if optimization would depend only on fw and not get “distracted” by difference in w that does not affect fw. It is therefore helpful to study the transformations on the weights that will not change the function presented by the network and come up with methods that their performance is not affected by such transformations.\nDefinition 1. We say a class of neural networks is invariant to a transformation T if for any parameter setting p and its corresponding weights w, fw = fT (w). Similarly, we say an update rule A is invariant to T if for any p and its corresponding w, fA(w) = fA(T (w)).\n70\nInvariances have also been studied as different mappings from the parameter space to the same function space [62] while we define the transformation as a mapping inside a fixed parameter space. A very important invariance in feedforward networks is node-wise rescaling [63]. For any internal node v and any scalar α > 0, we can multiply all incoming weights into v (i.e. wu→v for any (u→ v) ∈ E) by α and all the outgoing weights (i.e. wv→u for any (v → u) ∈ E) by 1/α without changing the function computed by the network. Not all node-wise rescaling transformations can be applied in feedforward nets with shared weights. This is due to the fact that some weights are forced to be equal and therefore, we are only allowed to change them by the same scaling factor.\nDefinition 2. Given a class of neural networks, we say an invariant transformation T that is defined over edge weights is feasible for parameter mapping π if the shared weights remain equal after the transformation, i.e. for any i and for any e, e′ ∈ Ei, T (w)e = T (w)e′ .\nWe have discussed the complete characterize all feasible node-wise invariances of RNNs in [64].\nUnfortunately, gradient descent is not rescaling invariant. The main problem with the gradient updates is that scaling down the weights of an edge will also scale up the gradient which, as we see later, is exactly the opposite of what is expected from a rescaling invariant update.\nFurthermore, gradient descent performs very poorly on “unbalanced” networks. We say that a network is balanced if the norm of incoming weights to different units are roughly the same or within a small range. For example, Figure 8.1(a) shows a huge gap in the performance of SGD initialized with a randomly generated balanced network, when training on MNIST, compared to a network initialized with unbalanced weights. Here the unbalanced weights are generated by applying a sequence of random rescaling functions on the balanced weights to create a rescaling equivalent unbalanced network.\nIn an unbalanced network, gradient descent updates could blow up the smaller weights, while keeping the larger weights almost unchanged. This is illustrated in Figure 8.1(b). If this were the only issue, one could scale down all the weights after each update. However, in an unbalanced network, the relative changes in the weights are also very different compared to a balanced network. For example, Figure\n71\n8.1(c) shows how two rescaling equivalent networks could end up computing a very different function after only a single update.\nTherefore, it is helpful to understand what are the feasible node-wise rescalings for RNNs. In the following theorem, we characterize all feasible node-wise invariances in RNNs.\nTheorem 31. For any α such that αij > 0, any Recurrent Neural Network with ReLU activation is invariant to the transformation Tα ([Win,Wrec,Wout]) = [Tin,α (Win) , Trec,α (Wrec) , Tout,α (Wout)] where for any i, j, k:\nTin,α(Win)i[j, k] = { αijW i in[j, k] i = 1,(\nαij/α i−1 k ) Wiin[j, k] 1 < i < d,\n(8.1.1)\nTrec,α(Wrec)i[j, k] = ( αij/α i k ) Wirec[j, k], Tout,α(Wout)[j, k] = ( 1/αd−1k ) Wout[j, k].\nFurthermore, any feasible node-wise rescaling transformation can be presented in the above form.\nThe proof is given in Section 8.3. The above theorem shows that there are many transformations under which RNNs represent the same function. An example of such invariances is shown in Fig. 8.2. Therefore, we would like to have optimization algorithms that are invariant to these transformations and in order to do so, we need to look at measures that are invariant to such mappings."
    }, {
      "heading" : "8.2 Understanding Invariances",
      "text" : "The goal of this section is to discuss whether being invariant to node-wise rescaling transformations is sufficient or not.\nIdeally we would like our algorithm to be at least invariant to all the transformations to which the model G is invariant. Note that this is different than the invariances studied in [62], in that they study algorithms that are invariant to reparametrizations of the same model but we look at transformations within the the parameter space that preserve the function in the model. This will eliminate the need for non-trivial initialization. Thus our goal is to characterize the whole variety of transformations to which the model is invariant and check if the algorithm is invariant to all of them.\nWe first need to note that invariance can be composed. If a network G is invariant to transformations T1 and T2, it is also invariant to their composition T1 ◦ T2. This is also true for an algorithm. If an algorithm is invariant to transformations T1 and T2, it is also invariant to their composition. This is because fT2◦T1◦A(w) = fT2◦A(T1◦w) = fA(T2◦T1(w)).\nThen it is natural to talk about the basis of invariances. The intuition is that although there are infinitely many transformations to which the model (or an algorithm) is invariant, they could be generated as compositions of finite number of transformations.\n72\nIn fact, in the infinitesimal limit the directions of infinitesimal changes in the parameters to which the function fw is insensitive form a subspace. This is because for a fixed input x, we have\nfw+∆(x) = fw(x) + ∑\ne∈E\n∂fw(x)\n∂we ·∆e +O(‖∆‖2), (8.2.1)\nwhere E is the set of edges, due to a Taylor expansion around w. Thus the function fw is insensitive (up to O(‖∆‖2)) to any change in the direction ∆ that lies in the (right) null space of the Jacobian matrix ∂fw(x)/∂w for all input x simultaneously. More formally, the subspace can be defined as\nN(w) = ⋂\nx∈R|Vin| Null\n( ∂fw(x)\n∂w\n) . (8.2.2)\nAgain, any change to w in the direction ∆ that lies in N(w) leaves the function fw unchanged (up to O(‖∆‖2)) at every input x. Therefore, if we can calculate the dimension of N(w) and if we have dimN(w) = |Vinternal|, where we denote the number of internal nodes by |Vinternal|, then we can conclude that all infinitesimal transformations to which the model is invariant can be spanned by infinitesimal node-wise rescaling transformations.\nNote that the null space N(w) and its dimension is a function of w. Therefore, there are some points in the parameter space that have more invariances than other points. For example, suppose that v is an internal node with ReLU activation that receives connections only from other ReLU units (or any unit whose output is nonnegative). If all the incoming weights to v are negative including the bias, the output of node v will be zero regardless of the input, and the function fw will be insensitive to any transformation to the outgoing weights of v. Nevertheless we conjecture that as the network size grows, the chance of being in such a degenerate configuration during training will diminish exponentially.\nWhen we study the dimension of N(w), it is convenient to analyze the dimension of the span of the row vectors of the Jacobian matrix ∂fw(x)/∂w instead. We define the degrees of freedom of model G at w as\ndG(w) = dim (⋃ x∈R|Vin| Span ( ∂fw(x) ∂w [v, :] : v ∈ Vout )) , (8.2.3)\nwhere ∂fw(x)[v, :]/∂w denotes the vth row vector of the Jacobian matrix and x runs over all possible input x. Intuitively, dG(w) is the dimension of the set of directions that changes fw(x) for at least one input x.\nDue to the rank nullity theorem dG(w) and the dimension of N(w) are related as follows:\ndG(w) + dim (N(w)) = |E|,\nwhere |E| is the number of parameters. Therefore, again if dG(w) = |E| − |Vinternal|, then we can conclude that infinitesimally speaking, all transformations to which the model is invariant can be spanned by node-wise rescaling transformations.\nConsidering only invariances that hold uniformly over all input x could give an under-estimate of the class of invariances, i.e., there might be some invariances that hold for many input x but not all. An alternative approach for characterizing invariances is to define a measure of distance between functions that the neural network model represents based on the input distribution, and infinitesimally study the subspace of directions to which the distance is insensitive. We can define distance between two functions f and g as\nD(f, g) = E [x ∼ D] [m(f(x), g(x))] ,\nwhere m : R|Vout|×|Vout| → R is a (possibly asymmetric) distance measure between two vectors z, z′ ∈\n73\nR|Vout|, which we require that m(z, z) = 0 and ∂m/∂z′z=z′ = 0. For example, m(z, z′) = ‖z− z′‖2. The second-order Taylor expansion of the distance D can be written as\nD(fw‖fw+∆) = 1\n2 ∆> · F (w) ·∆ + o(‖∆‖2),\nwhere\nF (w) = E [x ∼ D] [( ∂fw(x)\n∂w\n)> · ∂ 2m(z, z′)\n∂z′2 ∣∣∣∣ z=z′=fw(x) · ( ∂fw(x) ∂w )]\nand ∂2m(z, z′)/∂z′2|z=z′=fw(x) is the Hessian of the distance measure m at z = z′ = fw(x). Using the above expression, we can define the input distribution dependent version of N(w) and dG(w) as\nND(w) = NullF (w), dG,D(w) = rankF (w).\nAgain due to the rank-nullity theorem we have dG,D(w) + dim(ND(w)) = |E|. As a special case, we obtain the Kullback-Leibler divergence DKL, which is commonly considered as the way to study invariances, by choosing m as the conditional Kullback-Leibler divergence of output y given the network output as\nm(z, z′) = E [y ∼ q(y|z)] [ log\nq(y|z) q(y|z′)\n] ,\nwhere q(y|z) is a link function, which can be, e.g., the soft-max q(y|z) = ezy/∑|Vout|y′=1 ezy′ . However, note that the invariances in terms of DKL depends not only on the input distribution but also on the choice of the link function q(y|z)."
    }, {
      "heading" : "8.2.1 Path-based characterization of the network",
      "text" : "A major challenge in studying the degrees of freedom (8.2.3) is the fact that the Jacobian ∂fw(x)/∂w depends on both parameter w and input x. In this section, we first tease apart the two dependencies by rewriting fw(x) as the sum over all directed paths from every input node to each output node as follows:\nfw(x)[v] = ∑\np∈Π(v) gp(x) · πp(w) · x[head(p)], (8.2.4)\nwhere Π(v) is the set of all directed path from any input node to v, head(p) is the first node of path p, gp(x) takes 1 if all the rectified linear units along path p is active and zero otherwise, and πp(w) = ∏ e∈E(p) w(e) is the product of the weights along path p; E(p) denotes the set of edges that appear along path p.\nLet Π = ∪v∈VoutΠ(v) be the set of all directed paths. We define the path-Jacobian matrix J(w) ∈ R|Π|×|E| as J(w) = (∂πp(w)/∂we)p∈Π,e∈E . In addition, we define φ(x) as a |Π| dimensional vector with gp(x) · x[head(p)] in the corresponding entry. The Jacobian of the network fw(x) can now be expressed as\n∂fw(x)[v]\n∂w = Jv(w)\n>φv(x), (8.2.5)\nwhere where Jv(w) and φv(x) are the submatrix (or subvector) of J(w) and φ(x) that corresponds to\n74\noutput node v, respectively1. Expression (8.2.5) clearly separates the dependence to the parameters w and input x.\nNow we have the following statement (the proof is given in Section 8.3).\nTheorem 32. The degrees-of-freedom dG(w) of neural network model G is at most the rank of the path Jacobian matrix J(w). The equality holds if dim ( Span(φ(x) : x ∈ R|Vin|) ) = |Π|; i.e. when the dimension of the space spanned by φ(x) equals the total number of paths |Π|.\nAn analogous statement holds for the input distribution dependent degrees of freedom dG,D(w), namely, dG,D(w) ≤ rankJ(w) and the equality holds if the rank of the |Π| × |Π| path covariance matrix (E [x ∼ D] [ ∂2m(z, z′)/∂z′v∂z ′ v′φp(x)φp′(x) ] )p,p′∈Π is full, where v and v′ are the end nodes of paths p and p′, respectively.\nIt remains to be understood when the dimension of the span of the path vectors φ(x) become full. The answer depends on w. Unfortunately, there is no typical behavior as we know from the example of an internal ReLU unit connected to ReLU units by negative weights. In fact, we can choose any number of internal units in the network to be in this degenerate state creating different degrees of degeneracy. Another way to introduce degeneracy is to insert a linear layer in the network. This will superficially increase the number of paths but will not increase the dimension of the span of φ(x). For example, consider a linear classifier zout = 〈w,x〉 with |Vin| inputs. If the whole input space is spanned by x, the dimension of the span of φ(x) is |Vin|, which agrees with the number of paths. Now let’s insert a linear layer with units V1 in between the input and the output layers. The number of paths has increased from |Vin| to |Vin| · |V1|. However the dimension of the span of φ(x) = ~1|V1| ⊗ x is still |Vin|, because the linear units are always active. Nevertheless we conjecture that there is a configuration w such that dim ( Span(φ(x) : x ∈ R|Vin|) ) = |Π| and the set of such w grows as the network becomes larger."
    }, {
      "heading" : "8.2.2 Combinatorial characterization of the rank of path Jacobian",
      "text" : "Finally, we show that the rank of the path-Jacobian matrix J(w) is determined purely combinatorially by the graph G except a subset of the parameter space with zero Lebesgue measure. The proof is given in Section 8.3.\nTheorem 33. The rank of the path Jacobian matrix J(w) is generically (excluding set of parameters with zero Lebesgue measure) equal to the number of parameters |E| minus the number of internal nodes of the network.\nNote that the dimension of the space spanned by node-wise rescaling equals the number of internal nodes. Therefore, node-wise rescaling is the only type of invariance for a ReLU network with fixed architecture G, if dim ( Span(φ(x) : x ∈ R|Vin|) ) = |Π| at parameter w.\nAs an example, let us consider a simple 3 layer network with 2 nodes in each layer except for the output layer, which has only 1 node (see Figure 8.3). The network has 10 parameters (4, 4, and 2 in each layer respectively) and 8 paths. The Jacobian (∂fw(x)/∂w) can be written as (∂fw(x)/∂w) = J(w)> ·φ(x),\n1Note that although path activation gp(x) is a function of w, it is insensitive to an infinitesimal change in the parameter, unless the input to one of the rectified linear activation functions along path p is at exactly zero, which happens with probability zero. Thus we treat gp(x) as constant here.\n75\nwhere\nJ(w) =  w5w9 w5w9 w6w9 w6w9 w7w10 w7w10\nw8w10 w8w10\nw9w1 w9w2\nw9w3 w9w4\nw10w1 w10w2\nw10w3 w10w4\nw5w1 w5w2 w6w3 w6w4\nw7w1 w7w2 w8w3 w8w4  (8.2.6)\nand φ(x)> = [ g1(x)x[1] g2(x)x[2] g3(x)x[1] g4(x)x[2] g5(x)x[1] g6(x)x[2] g7(x)x[1] g8(x)x[2] ] .\nThe rank of J(w) in (8.2.6) is (generically) equal to 10− 4 = 6, which is smaller than both the number of parameters and the number of paths."
    }, {
      "heading" : "8.3 Proofs",
      "text" : ""
    }, {
      "heading" : "8.3.1 Proof of Theorem 31",
      "text" : "We first show that any RNN is invariant to Tα by induction on layers and time-steps. More specifically, we prove that for any 0 ≤ t ≤ T and 1 ≤ i < d, ~hit ( Tα( ~W ) ) [j] = αij ~hit( ~W )[j]. The statement is\nclearly true for t = 0; because for any i, j, ~hi0 ( Tα( ~W ) ) [j] = αij ~hi0( ~W )[j] = 0.\nNext, we show that for i = 1, if we assume that the statement is true for t = t′, then it is also true for t = t′ + 1:\n~h1t′+1 ( Tα( ~W ) ) [j] = ∑ j′ Tin,α(Win)1[j, j′]~xt′+1[j′] + Trec,α(Wrec)1[j, j′]~h1t′ ( Tα( ~W ) ) [j′]  +\n= ∑ j′ α1jW 1 in[j, j ′]~xt′+1[j ′] + ( α1j/α 1 j′ ) W1rec[j, j ′]α1j′ ~h1t′( ~W ))[j′]  +\n= α1j ~hit( ~W )[j]\n76\nWe now need to prove the statement for 1 < i < d. Assuming that the statement is true for t ≤ t′ and the layers before i, we have:\n~hit′+1 ( Tα( ~W ) ) [j] = ∑ j′ Tin,α(Win)i[j, j′]~hi−1t′+1 ( Tα( ~W ) ) [j′] + Trec,α(Wrec)i[j, j′]~hit′ ( Tα( ~W ) ) [j′]  +\n= ∑ j′ αij αi−1j′ Wiin[j, j ′]αi−1j′ ~hi−1t′+1( ~W ))[j′] + αij αij′ Wirec[j, j ′]αij′ ~hit′( ~W ))[j ′]  +\n= αij ~hit( ~W )[j]\nFinally, we can show that the output is invariant for any j at any time step t:\nfT ( ~W ),t(~xt)[j] = ∑ j′ Tout,α(Wout)[j, j′]~hd−1t (Tα( ~W )[j′] = ∑ j′ (1/αd−1j′ )Wout[j, j ′]αd−1j′ ~hd−1t ( ~W )[j ′]\n= ∑ j′ Wout[j, j ′]~hd−1t ( ~W )[j ′] = f ~W,t(~xt)[j]\nWe now show that any feasible node-wise rescaling can be presented as Tα. Recall that node-wise rescaling invariances for a general feedforward network can be written as T̃β(~w)u→v = (βv/βu)wu→v for some β where βv > 0 for internal nodes and βv = 1 for any input/output nodes. An RNN with T = 0 has no weight sharing and for each node v with index j in layer i, we have βv = αij . For any T > 0 however, we there is no invariance that is not already counted. The reason is that by fixing the values of βv for the nodes in time step 0, due to the feasibility, the values of β for nodes in other time-steps should be tied to the corresponding value in time step 0. Therefore, all invariances are included and can be presented in form of Tα."
    }, {
      "heading" : "8.3.2 Proof of Theorem 32",
      "text" : "Proof. First we see that (8.2.5) is true because\n∂fw(x)[v] ∂w = ( ∑ p∈Π(v) ∂πp(w) ∂we · gp(x) · x[head(p)] ) e∈E = Jv(w)> · φv(x).\nTherefore,⋃ x∈R|Vin| Span ( ∂fw(x)[v] ∂w : v ∈ Vout ) = ⋃ x∈R|Vin| Span (Jv(w)> · φv(x) : v ∈ Vout)\n= J(w)> · Span ( φ(x) : x ∈ R|Vin| ) . (8.3.1)\nConsequently, any vector of the form (∂fw(x)[v]∂we )e∈E for a fixed input x lies in the span of the row vectors of the path Jacobian J(x).\nThe second part says dG(w) = rankJ(w) if dim ( Span(φ(x) : x ∈ R|Vin|) ) = |Π|, which is the number of rows of J(w). We can see that this is true from expression (11.2.7).\n77"
    }, {
      "heading" : "8.3.3 Proof of Theorem 33",
      "text" : "Proof. First, J(w) can be written as an Hadamard product between path incidence matrix M and a rank-one matrix as follows:\nJ(w) = M ◦ ( w−1 · π>(w) ) ,\nwhere M is the path incidence matrix whose i, j entry is one if the ith edge is part of the jth path, w−1 is an entry-wise inverse of the parameter vector w, π(w) = (πp(w)) is a vector containing the product along each path in each entry, and > denotes transpose. Since we can rewrite\nJ(w) = diag(w−1) ·M · diag(π(w)),\nwe see that (generically) the rank of J(w) is equal to the rank of zero-one matrix M .\nNote that the rank of M is equal to the number of linearly independent columns of M , in other words, the number of linearly independent paths. In general, most paths are not independent. For example, in Figure 8.3, we can see that the column corresponding to the path w2w7w10 can be produced by combining 3 columns corresponding to paths w1w5w9, w1w7w10, and w2w5w9.\nIn order to count the number of independent paths, we use mathematical induction. For simplicity, consider a layered graph with d layers. All the edges from the (d− 1)th layer nodes to the output layer nodes are linearly independent, because they correspond to different parameters. So far we have ndnd−1 independent paths.\nNext, take one node u0 (e.g., the leftmost node) from the (d − 2)th layer. All the paths starting from this node through the layers above are linearly independent. However, other nodes in this layer only contributes linearly to the number of independent paths. This is the case because we can take an edge (u, v), where u is one of the remaining nd−2 − 1 vertices in the (d− 2)th layer and v is one of the nd−1 nodes in the (d− 1)th layer, and we can take any path (say p0) from there to the top layer. Then this is the only independent path that uses the edge (u, v), because any other combination of edge (u, v) and path p from v to the top layer can be produced as follows (see Figure 8.4):\n(u, v)→ p = (u, v)→ p0 − (u0, v)→ p0 + (u0, v)→ p.\nTherefore after considering all nodes in the d− 2th layer, we have\nndnd−1 + nd−1(nd−2 − 1)\nindependent paths. Doing this calculation inductively, we have\nndnd−1 + nd−1(nd−2 − 1) + · · ·+ n1(n0 − 1)\nindependent paths, where n0 is the number of input units. This number is clearly equal to the number of parameters (ndnd−1 + · · ·+ n1n0) minus the number of internal nodes (nd−1 + · · ·+ n1).\n78\n79\nChapter 9\nPath-Normalization for Feedforward and Recurrent Neural Networks\nAs we discussed, optimization is inherently tied to a choice of geometry, here represented by a choice of complexity measure or “norm”1. In Chapter 8, we studies the invariances in neural networks. We would to have a complexity measure that has similar invariance properties as neural networks. In Section 9.1 we introduce the path-regularizer which is invariant to node-wise rescaling transformations explained in Chapter 8. In Section 9.2, we derive Path-SGD optimization algorithm for standard feed-forward networks which is the steepest descent with respect to the path-regularizer. Finally, we extend Path-SGD to recurrent neural networks in Section 9.3."
    }, {
      "heading" : "9.1 Path-regularizer",
      "text" : "We consider the generic group-norm type regularizer in equation (9.1.1). As we discussed before, two simple cases of above group-norm are q1 = q2 = 1 and q1 = q2 = 2 that correspond to overall `1 regularization and weight decay respectively. Another form of regularization that is shown to be very effective in RELU networks is the max-norm regularization, which is the maximum over all units of norm of incoming edge to the unit2 [67, 68]. The max-norm correspond to “per-unit\" regularization when we set q2 =∞ in equation (9.1.1) and can be written in the following form (for q1 = 2):\nµ2,∞(w) = sup v∈V  ∑ (u→v)∈E ∣∣w(u→v)∣∣2 1/2 (9.1.1)\nWeight decay is probably the most commonly used regularizer. On the other hand, per-unit regularization might not seem ideal as it is very extreme in the sense that the value of regularizer corresponds to the highest value among all nodes. However, the situation is very different for networks with RELU activations (and other activation functions with non-negative homogeneity property). In these cases, per-unit `2 regularization has shown to be very effective [68]. The main reason could be because RELU networks can be rebalanced in such a way that all hidden units haveneyshabur2015norm the same norm.\n1The path-norm which we define is a norm on functions, not on weights, but as we prefer not getting into this technical discussion here, we use the term “norm” very loosely to indicate some measure of magnitude [65].\n2This definition of max-norm is a bit different than the one used in the context of matrix factorization [66]. The later is similar to the minimum upper bound over `2 norm of both outgoing edges from the input units and incoming edges to the output units in a two layer feed-forward network.\n80\nHence, per-unit regularization will not be a crude measure anymore.\nSince µp,∞ is not rescaling invariant and the values of the scale measure are different for rescaling equivalent networks, it is desirable to look for the minimum value of a regularizer among all rescaling equivalent networks. Surprisingly, for a feed-forward network, the minimum `2 per-unit regularizer among all rescaling equivalent networks can be calculated in close form and we call it the path-regularizer [65, 69].\nThe path-regularizer is the sum over all paths from input nodes to output nodes of the product of squared weights along the path. To define it formally, let P be the set of directed paths from input to output units so that for any pathneyshabur2015path ζ = ( ζ0, . . . , ζlen(ζ) ) ∈ P of length len(ζ), we have that ζ0 ∈ Vin, ζlen(ζ) ∈ Vout and for any 0 ≤ i ≤ len(ζ)− 1, (ζi → ζi+1) ∈ E. We also abuse the notation and denote e ∈ ζ if for some i, e = (ζi, ζi+1). Then the path regularizer can be written as:\nγ2net(w) = ∑ ζ∈P len(ζ)−1∏ i=0 w2ζi→ζi+1 (9.1.2)\nThe above formulation of the path-regularizer involves an exponential number of terms. However, it can be computed efficiently by dynamic programming in a single forward step using the following equivalent recursive definition:\nγ2v(w) = ∑\n(u→v)∈E\nγ2u(w)w 2 u→v , γ 2 net(w) = ∑ u∈Vout γ2u(w) (9.1.3)"
    }, {
      "heading" : "9.2 Path-SGD for Feedforward Networks",
      "text" : "We consider an approximate steepest descent step with respect to the path-norm. More formally, for a network without shared weights, where the parameters are the weights themselves, consider the diagonal quadratic approximation of the path-regularizer about the current iterate w(t):\nγ̂2net(w (t) + ∆w) = γ2net(w (t)) + 〈 ∇γ2net(w(t)),∆w 〉 + 1\n2 ∆w> diag\n( ∇2γ2net(w(t)) ) ∆w (9.2.1)\nUsing the corresponding quadratic norm ‖w −w′‖2γ̂2net(w(t)+∆w) = 1 2 ∑ e∈E ∂2γ2net ∂w2e\n(we − w′e)2, we can define an approximate steepest descent step as:\nw(t+1) = min w\nη 〈 ∇L(w),w −w(t) 〉 + ∥∥∥w −w(t)∥∥∥2\nγ̂2net(w (t)+∆w)\n. (9.2.2)\nSolving (9.2.2) yields the update:\nw(t+1)e = w (t) e −\nη\nκe(w(t))\n∂L\n∂we (w(t)) where: κe(w) =\n1\n2\n∂2γ2net(w)\n∂w2e . (9.2.3)\nThe stochastic version that uses a subset of training examples to estimate ∂L∂wu→v (w (t)) is called PathSGD [69]. We now show how Path-SGD can be extended to networks with shared weights."
    }, {
      "heading" : "9.3 Extending to Networks with Shared Weights",
      "text" : "When the networks has shared weights, the path-regularizer is a function of parameters p and therefore the quadratic approximation should also be with respect to the iterate p(t) instead of w(t) which results\n81\nin the following update rule: p(t+1) = min p η 〈 ∇L(p),p− p(t) 〉 + ∥∥∥p− p(t)∥∥∥\nγ̂2net(p (t)+∆p)\n. (9.3.1)\nwhere ‖p− p′‖2γ̂2net(p(t)+∆p) = 1 2 ∑m i=1 ∂2γ2net ∂p2i (pi − p′i)2. Solving (9.3.1) gives the following update:\np (t+1) i = p (t) i −\nη\nκi(p(t))\n∂L ∂pi (p(t)) where: κi(p) = 1 2\n∂2γ2net(p)\n∂p2i . (9.3.2)\nThe second derivative terms κi are specified in terms of their path structure as follows:\nLemma 34. κi(p) = κ (1) i (p) + κ (2) i (p) where\nκ (1) i (p) = ∑ e∈Ei ∑ ζ∈P 1e∈ζ len(ζ)−1∏ j=0\ne 6=(ζj→ζj+1)\np2π(ζj→ζj+1) = ∑ e∈Ei κe(w), (9.3.3)\nκ (2) i (p) = p 2 i ∑ e1,e2∈Ei e1 6=e2 ∑ ζ∈P 1e1,e2∈ζ len(ζ)−1∏ j=0\ne1 6=(ζj→ζj+1) e2 6=(ζj→ζj+1)\np2π(ζj→ζj+1), (9.3.4)\nand κe(w) is defined in (9.2.3).\nThe second term κ(2)i (p) measures the effect of interactions between edges corresponding to the same parameter (edges from the same Ei) on the same path from input to output. In particular, if for any path from an input unit to an output unit, no two edges along the path share the same parameter, then κ(2)(p) = 0. For example, for any feedforward or Convolutional neural network, κ(2)(p) = 0. But for RNNs, there certainly are multiple edges sharing a single parameter on the same path, and so we could have κ(2)(p) 6= 0. The above lemma gives us a precise update rule for the approximate steepest descent with respect to the path-regularizer. The following theorem confirms that the steepest descent with respect to this regularizer is also invariant to all feasible node-wise rescaling for networks with shared weights.\nTheorem 35. For any feedforward networks with shared weights, the update (9.3.2) is invariant to all feasible node-wise rescalings. Moreover, a simpler update rule that only uses κ(1)i (p) in place of κi(p) is also invariant to all feasible node-wise rescalings.\nEquations (9.3.3) and (9.3.4) involve a sum over all paths in the network which is exponential in depth of the network. We next show that both of these equations can be calculated efficiently."
    }, {
      "heading" : "9.3.1 Simple and Efficient Computations",
      "text" : "We show how to calculate κ(1)i (~p) and κ (2) i (~p) by considering a network with the same architecture but with squared weights: Theorem 36. For any network N (G, π, p), consider N (G, π, p̃) where for any i, p̃i = p2i . Define the function g : R|Vin| → R to be the sum of outputs of this network: g(x) = ∑|Vout|i=1 f~̃p(x)[i]. Then κ(1) and κ(2) can be calculated as follows where ~1 is the all-ones input vector:\nκ(1)(~p) = ∇~̃pg(~1), κ (2) i (~p) = ∑ (u→v),(u′→v′)∈Ei\n(u→v) 6=(u′→v′)\np̃i ∂g(~1)\n∂hv′(~̃p)\n∂hu′(~̃p) ∂hv(~̃p) hu(~̃p). (9.3.5)\n82\nIn the process of calculating the gradient∇~̃pg(~1), we need to calculate hu(~̃p) and ∂g(~1)/∂hv(~̃p) for any u, v. Therefore, the only remaining term to calculate (besides∇p̃g(~1)) is ∂hu′(~̃p)/∂hv(~̃p). Recall that T is the length (maximum number of propagations through time) and d is the number of layers in an RNN. Let H be the number of hidden units in each layer and B be the size of the mini-batch. Then calculating the gradient of the loss at all points in the minibatch (the standard work required for any mini-batch gradient approach) requires time O(BdTH2). In order to calculate κ(1)i (~p), we need to calculate the gradient ∇~̃pg(1) of a similar network at a single input—so the time complexity is just an additional O(dTH2). The second term κ(2)(~p) can also be calculated for RNNs in O(dTH2(T +H)) 3. Therefore, the ratio of time complexity of calculating the first term and second term with respect to the gradient over mini-batch is O(1/B) and O((T +H)/B) respectively. Calculating only κ(1)i (~p) is therefore very cheap with minimal per-minibatch cost, while calculating κ(2)i (~p) might be expensive for large networks. Beyond the low computational cost, calculating κ(1)i (~p) is also very easy to implement as it requires only taking the gradient with respect to a standard feed-forward calculation in a network with slightly modified weights—with most deep learning libraries it can be implemented very easily with only a few lines of code."
    }, {
      "heading" : "9.4 Proofs",
      "text" : ""
    }, {
      "heading" : "9.4.1 Proof of Lemma 34",
      "text" : "We prove the statement simply by calculating the second derivative of the path-regularizer with respect to each parameter:\nκi(~p) = 1\n2 ∂2γ2net ∂p2i = 1 2 ∂ ∂pi  ∂ ∂pi ∑ ζ∈P len(ζ)−1∏ j=0 w2ζj→ζj+1  = 1\n2\n∂\n∂pi  ∂ ∂pi ∑ ζ∈P len(ζ)−1∏ j=0 p2π(ζj→ζj+1)  = 1 2 ∑ ζ∈P ∂ ∂pi  ∂ ∂pi len(ζ)−1∏ j=0 p2π(ζj→ζj+1) \n3 For an RNN, κ(2)(Win) = 0 and κ(2)(Wout) = 0 because only recurrent weights are can be shared multiple times along an input-output path. κ(2)(Wrec) can be written and calculated in the matrix form: κ(2)(Wirec) = W′irec ∑T−3 t1=0 ((W′irec)t1)> ∑T−t1−1t2=2 ∂g(~1)∂~hi t1+t2+1 (~̃p) ( ~hit2 (~̃p) )> where for any i, j, k we have W′irec[j, k] = (Wirec[j, k])2. The only terms that require extra computation are powers of Wrec which can be done in O(dTH3) and the rest of the matrix computations need O(dT 2H2).\n83\nTaking the second derivative then gives us both terms after a few calculations:\nκi(~p) = 1\n2 ∑ ζ∈P ∂ ∂pi  ∂ ∂pi len(ζ)−1∏ j=0 p2π(ζj→ζj+1)  = ∑ ζ∈P ∂ ∂pi pi ∑ e∈Ei ~1e∈ζ len(ζ)−1∏ j=0\ne 6=(ζj→ζj+1\np2π(ζj→ζj+1)  = ∑ ζ∈P pi ∂ ∂pi ∑ e∈Ei ~1e∈ζ len(ζ)−1∏ j=0\ne6=(ζj→ζj+1\np2π(ζj→ζj+1) + ∑ e∈Ei ~1e∈ζ len(ζ)−1∏ j=0\ne6=(ζj→ζj+1\np2π(ζj→ζj+1)\n\n= p2i ∑\ne1,e2∈Ei e1 6=e2\n∑ ζ∈P ~1e1,e2∈ζ len(ζ)−1∏ j=0\ne1 6=(ζj→ζj+1) e2 6=(ζj→ζj+1)\np2π(ζj→ζj+1) + ∑ e∈Ei ∑ ζ∈P ~1e∈ζ len(ζ)−1∏ j=0\ne6=(ζj→ζj+1)\np2π(ζj→ζj+1)\n"
    }, {
      "heading" : "9.4.2 Proof of Theorem 35",
      "text" : "Node-wise rescaling invariances for a feedforward network can be written as Tβ(~w)u→v = (βv/βu)wu→v for some β where βv > 0 for internal nodes and βv = 1 for any input/output nodes. Any feasible invariance for a network with shared weights can also be written in the same form. The only difference is that some of βvs are now tied to each other in a way that shared weights have the same value after transformation. First, note that since the network is invariant to the transformation, the following statement holds by an induction similar to Theorem 31 but in the backward direction:\n∂L ∂hv (Tβ(~p)) = 1 βv ∂L ∂hu (~p) (9.4.1)\nfor any (u→ v) ∈ E. Furthermore, by the proof of the Theorem 31 we have that for any (u→ v) ∈ E, hu(Tβ(~p)) = βuhu(~p). Therefore,\n∂L\n∂Tβ(~p)i (Tβ(~p)) = ∑ (u→v)∈Ei ∂L ∂hv (Tβ(~p))hu(Tβ(~p)) = βu′ βv′ ∂L ∂pi (~p) (9.4.2)\nwhere (u′ → v′) ∈ Ei. In order to prove the theorem statement, it is enough to show that for any edge (u→ v) ∈ Ei, κi(Tβ(~p)) = (βu/βv)2κi(~p) because this property gives us the following update:\nTβ(~p)i − η κi(Tβ(~p)) ∂L(Tβ(~p)) ∂Tβ(~p)i = βv βu pi −\nη\n(βu/βv)2κi(~p) βu βv ∂L ∂pi (~p) = Tβ(~p+)i\nTherefore, it is remained to show that for any edge (u→ v) ∈ Ei v, κi(Tβ(~p)) = (βu/βv)2κi(~p). We show that this is indeed true for both terms κ(1) and κ(2) separately.\nWe first prove the statement for κ(1). Consider each path ζ ∈ P . By an inductive argument along the path, it is easy to see that multiplying squared weights along this path is invariant to the transformation:\nlen(ζ)−1∏ j=0 Tβ(~p)2π(ζj→ζj+1) = len(ζ)−1∏ j=0 p2π(ζj→ζj+1)\n84\nTherefore, we have that for any edge e ∈ E and any ζ ∈ P ,\nlen(ζ)−1∏ j=0\ne6=(ζj→ζj+1)\nTβ(~p)2π(ζj→ζj+1) = ( βu βv )2 len(ζ)−1∏ j=0\ne 6=(ζj→ζj+1)\np2π(ζj→ζj+1)\nTaking sum over all paths ζ ∈ P and all edges e = (u→ v) ∈ E completes the proof for κ(1). Similarly for κ(2), considering any two edges e1 6= e2 and any path ζP , we have that:\nTβ(~p)2i len(ζ)−1∏\nj=0 e1 6=(ζj→ζj+1) e2 6=(ζj→ζj+1)\nTβ(~p)2π(ζj→ζj+1) = ( βv βu )2 p2i ( βu βv )4 len(ζ)−1∏ j=0\ne1 6=(ζj→ζj+1) e2 6=(ζj→ζj+1)\np2π(ζj→ζj+1)\nwhere (u→ v) ∈ Ei. Again, taking sum over all paths ζ and all edges e1 6= e2 proves the statement for κ(2) and consequently for κ(1) + κ(2)."
    }, {
      "heading" : "9.4.3 Proof of Theorem 36",
      "text" : "First, note that based on the definitions in the theorem statement, for any node v, hv(~̃p) = γ2v(p) and therefore g(~1) = γ2net(p). Using Lemma 34, main observation here is that for each edge e ∈ Ei and each path ζ ∈ P , the corresponding term in κ(1) is nothing but product of the squared weights along the path except the weights that correspond to the edge e:\n~1e∈ζ len(ζ)−1∏ j=0\ne 6=(ζj→ζj+1)\np2π(ζj→ζj+1)\nThis path can therefore be decomposed into a path from input to edge e and a path from edge e to the output. Therefore, for any edge e, we can factor out the number corresponding to the paths that go through e and rewrite κ(1) as follows:\nκ(1)(p) = ∑\n(u→v)∈Ei  ∑ ζ∈Pin→u len(ζ)−1∏ j=0 p2π(ζj→ζj+1)  ∑ ζ∈Pv→out len(ζ)−1∏ j=0 p2π(ζj→ζj+1)  (9.4.3) where Pin→u is the set of paths from input nodes to node v and Pv→out is defined similarly for the output nodes.\nBy induction on layers of N (G, π, ~̃p), we get the following:\n∑ ζ∈Pin→u len(ζ)−1∏ j=0 p2π(ζj→ζj+1) = hu(~̃p) (9.4.4)\n∑ ζ∈Pv→out len(ζ)−1∏ j=0 p2π(ζj→ζj+1) = ∂g(1) ∂hv(~̃p) (9.4.5)\nTherefore, κ(1) can be written as:\nκ(1)(p) = ∑\n(u→v)∈Ei\n∂g(1)\n∂hv(~̃p) hu(~̃p) = ∑ (u→v)∈Ei ∂g(1) ∂w′u→v = ∂g(1) ∂p̃i (9.4.6)\n85\nNext, we show how to calculate the second term, i.e. κ(2). Each term in κ(2) corresponds to a path that goes through two edges. We can decompose such paths and rewrite κ(2) similar to the first term:\nκ(2)(p) = p2i ∑\n(u→v)∈Ei (u′→v′)∈Ei\n(u→v)6=(u′→v′)\n ∑ ζ∈Pin→u len(ζ)∏ j=0 p2π(ζj→ζj+1)   ∑ ζ∈Pv→u′ len(ζ)−1∏ j=0 p2π(ζj→ζj+1)  ∑ ζ∈Pv′→out len(ζ)−1∏ j=0 p2π(ζj→ζj+1)\n =\n∑ (u→v)∈Ei\n(u′→v′)∈Ei (u→v)6=(u′→v′)\np̃i ∂g(~1)\n∂hv′(~̃p)\n∂hu′(~̃p) ∂hv(~̃p) hu(~̃p)\nwhere Pu→v is the set of all directed paths from node u to node v.\n86\nChapter 10\nExperiments on Path-SGD\nIn this Chapter, we compare Path-SGD to other optimization methods on fully connected and recurrent neural networks."
    }, {
      "heading" : "10.1 Experiments on Fully Connected Feedforward Networks",
      "text" : "We compare `2-Path-SGD to two commonly used optimization methods in deep learning, SGD and AdaGrad. We conduct our experiments on four common benchmark datasets: the standard MNIST dataset of handwritten digits [70]; CIFAR-10 and CIFAR-100 datasets of tiny images of natural scenes [71]; and Street View House Numbers (SVHN) dataset containing color images of house numbers collected by Google Street View [72]. Details of the datasets are shown in Table 10.1.\nIn all of our experiments, we trained feed-forward networks with two hidden layers, each containing 4000 hidden units. We used mini-batches of size 100 and the step-size of 10−α, where α is an integer between 0 and 10. To choose α, for each dataset, we considered the validation errors over the validation set (10000 randomly chosen points that are kept out during the initial training) and picked the one that reaches the minimum error faster. We then trained the network over the entire training set. All the networks were trained both with and without dropout. When training with dropout, at each update step, we retained each unit with probability 0.5.\nWe tried both balanced and unbalanced initializations. In balanced initialization, incoming weights to each unit v are initialized to i.i.d samples from a Gaussian distribution with standard deviation 1/ √\nfan-in(v). In the unbalanced setting, we first initialized the weights to be the same as the balanced weights. We then picked 2000 hidden units randomly with replacement. For each unit, we multiplied its incoming edge and divided its outgoing edge by 10c, where c was chosen randomly from log-normal distribution.\nThe optimization results are shown in Figure 10.1. For each of the four datasets, the plots for objective function (cross-entropy), the training error and the test error are shown from left to right where in each plot the values are reported on different epochs during the optimization. Although we proved that Path-SGD updates are the same for balanced and unbalanced initializations, to verify that despite numerical issues they are indeed identical, we trained Path-SGD with both balanced and unbalanced initializations. Since the curves were exactly the same we only show a single curve. The dropout is used for the experiments on CIFAR-100 and SVHN. Please see [69] for a more complete set of experimental results.\nWe can see in Figure 10.2 that as expected, the unbalanced initialization considerably hurts the performance of SGD and AdaGrad (in many cases their training and test errors are not even in the range of the\n87\nplot to be displayed), while Path-SGD performs essentially the same. Another interesting observation is that even in the balanced settings, not only does Path-SGD often get to the same value of objective\n88\nCross-Entropy Training Loss 0/1 Training Error 0/1 Test Error\nfunction, training and test error faster, but also the final generalization error for Path-SGD is sometimes considerably lower than SGD and AdaGrad). The plots for test errors could also imply that implicit regularization due to steepest descent with respect to path-regularizer leads to a solution that generalizes better. This view is similar to observations in [73] on the role of implicit regularization in deep learning.\nThe results suggest that Path-SGD outperforms SGD and AdaGrad in two different ways. First, it can achieve the same accuracy much faster and second, the implicit regularization by Path-SGD leads to a local minima that can generalize better even when the training error is zero. This can be better analyzed by looking at the plots for more number of epochs which we have provided in [69]. We should also point that Path-SGD can be easily combined with AdaGrad or Adam to take advantage of the adaptive stepsize or used together with a momentum term. This could potentially perform even better compare to Path-SGD.\n89"
    }, {
      "heading" : "10.2 Experiments on Recurrent Neural Networks",
      "text" : ""
    }, {
      "heading" : "10.2.1 The Contribution of the Second Term",
      "text" : "As we discussed in section 9.3.1, the second term κ(2) in the update rule can be computationally expensive for large networks. In this section we investigate the significance of the second term and show that at least in our experiments, the contribution of the second term is negligible. To compare the two terms κ(1) and κ(2), we train a single layer RNN with H = 200 hidden units for the task of word-level language modeling on Penn Treebank (PTB) Corpus [74]. Fig. 10.3 compares the performance of SGD vs. Path-SGD with/without κ(2). We clearly see that both version of Path-SGD are performing very similarly and both of them outperform SGD significantly. This results in Fig. 10.3 suggest that the first term is more significant and therefore we can ignore the second term.\nTo better understand the importance of the two terms, we compared the ratio of the norms ∥∥κ(2)∥∥ 2 / ∥∥κ(1)∥∥ 2 for different RNN lengths T and number of hidden units H . The table in Fig. 10.3 shows that the contribution of the second term is bigger when the network has fewer number of hidden units and the length of the RNN is larger (H is small and T is large). However, in many cases, it appears that the first term has a much bigger contribution in the update step and hence the second term can be safely ignored. Therefore, in the rest of our experiments, we calculate the Path-SGD updates only using the first term κ(1)."
    }, {
      "heading" : "10.2.2 Addition problem",
      "text" : "Training Recurrent Neural Networks is known to be hard for modeling long-term dependencies due to the gradient vanishing/exploding problem [75, 76]. In this section, we consider synthetic problems that are specifically designed to test the ability of a model to capture the long-term dependency structure. Specifically, we consider the addition problem and the sequential MNIST problem.\nAddition Problem: The addition problem was introduced in [77]. Here, each input consists of two sequences of length T , one of which includes numbers sampled from the uniform distribution with range [0, 1] and the other sequence serves as a mask which is filled with zeros except for two entries. These two entries indicate which of the two numbers in the first sequence we need to add and the task is to output the result of this addition. Sequential MNIST: In sequential MNIST, each digit image is reshaped into a sequence of length 784, turning the digit classification task into sequence classification with long-term dependencies [78, 79].\nFor both tasks, we closely follow the experimental protocol in [78]. We train a single-layer RNN\n90\nconsisting of 100 hidden units with path-SGD, referred to as RNN-Path. We also train an RNN of the same size with identity initialization, as was proposed in [78], using SGD as our baseline model, referred to as IRNN. We performed grid search for the learning rates over {10−2, 10−3, 10−4} for both our model and the baseline. Non-recurrent weights were initialized from the uniform distribution with range [−0.01, 0.01]. Similar to [79], we found the IRNN to be fairly unstable (with SGD optimization typically diverging). Therefore for IRNN, we ran 10 different initializations and picked the one that did not explode to show its performance.\nIn our first experiment, we evaluate Path-SGD on the addition problem. The results are shown in Fig. 10.4 with increasing the length T of the sequence: {100, 400, 750}. We note that this problem becomes much harder as T increases because the dependency between the output (the sum of two numbers) and the corresponding inputs becomes more distant. We also compare RNN-Path with the previously published results, including identity initialized RNN [78] (IRNN), unitary RNN [79] (uRNN), and np-RNN1 introduced by [80]. Table 10.2 shows the effectiveness of using Path-SGD. Perhaps more surprisingly, with the help of path-normalization, a simple RNN with the identity initialization is able to achieve a 0% error on the sequences of length 750, whereas all the other methods, including LSTMs, fail. This shows that Path-SGD may help stabilize the training and alleviate the gradient problem, so as to perform well on longer sequence. We next tried to model the sequences length of 1000, but we found that for such very long sequences RNNs, even with Path-SGD, fail to learn.\nNext, we evaluate Path-SGD on the Sequential MNIST problem. Table 10.2, right column, reports test error rates achieved by RNN-Path compared to the previously published results. Clearly, using Path-SGD helps RNNs achieve better generalization. In many cases, RNN-Path outperforms other RNN methods (except for LSTMs), even for such a long-term dependency problem.\n1The original paper does not include any result for 750, so we implemented np-RNN for comparison. However, in our implementation the np-RNN is not able to even learn sequences of length of 200. Thus we put “>2” for length of 750.\n91"
    }, {
      "heading" : "10.2.3 Language Modeling Tasks",
      "text" : "In this section we evaluate Path-SGD on a language modeling task. We consider two datasets, Penn Treebank (PTB-c) and text8 2. PTB-c: We performed experiments on a tokenized Penn Treebank Corpus, following the experimental protocol of [83]. The training, validations and test data contain 5017k, 393k and 442k characters respectively. The alphabet size is 50, and each training sequence is of length 50. text8: The text8 dataset contains 100M characters from Wikipedia with an alphabet size of 27. We follow the data partition of [82], where each training sequence has a length of 180. Performance is evaluated using bits-per-character (BPC) metric, which is log2 of perplexity.\nSimilar to the experiments on the synthetic datasets, for both tasks, we train a single-layer RNN consisting of 2048 hidden units with path-SGD (RNN-Path). Due to the large dimension of hidden space, SGD can take a fairly long time to converge. Instead, we use Adam optimizer [84] to help speed up the training, where we simply use the path-SGD gradient as input to the Adam optimizer.\nWe also train three additional baseline models: a ReLU RNN with 2048 hidden units, a tanh RNN with 2048 hidden units, and an LSTM with 1024 hidden units, all trained using Adam. We performed grid search for learning rate over {10−3, 5 · 10−4, 10−4} for all of our models. For ReLU RNNs, we initialize the recurrent matrices from uniform[−0.01, 0.01], and uniform[−0.2, 0.2] for non-recurrent weights. For LSTMs, we use orthogonal initialization [85] for the recurrent matrices and uniform[−0.01, 0.01] for non-recurrent weights. The results are summarized in Table 10.3.\nWe also compare our results to an RNN that uses hidden activation regularizer [83] (TRec,β = 500), Multiplicative RNNs trained by Hessian Free methods [82] (HF-MRNN), and an RNN with smooth version of ReLU [81]. Table 10.3 shows that path-normalization is able to outperform RNN-ReLU and RNN-tanh, while at the same time shortening the performance gap between plain RNN and other more complicated models (e.g. LSTM by 57% on PTB and 54% on text8 datasets). This demonstrates the efficacy of path-normalized optimization for training RNNs with ReLU activation.\n2http://mattmahoney.net/dc/textdata\n92\nChapter 11\nData-Dependent Path Normalization\nIn this chapter, we focus on two efficient alternative optimization approaches proposed recently for feed-forward neural networks that are based on intuitions about parametrization, normalization and the geometry of parameter space: Path-SGD [14] was derived as steepest descent algorithm with respect to particular regularizer (the `2-path regularizer, i.e. the sum over all paths in the network of the squared product over all weights in the path [58]) and is invariant to weight reparametrization. Batch-normalization [86] was derived by adding normalization layers in the network as a way of controlling the variance of the input each unit receives in a data-dependent fashion. In this chapter, we propose a unified framework which includes both approaches, and allows us to obtain additional methods which interpolate between them. Using our unified framework, we can also tease apart and combine two different aspects of these two approaches: data-dependence and invariance to weight reparametrization.\nOur unified framework is based on first choosing a per-node complexity measure we refer to as γv (defined in Section 11.1). The choice of complexity measure is parametrized by a choice of “normalization matrix” R, and different choices for this matrix incorporate different amounts of data dependencies: for pathSGD, R is a non-data-dependent diagonal matrix, while for batch normalization it is a data-dependent covariance matrix, and we can interpolate between the two extremes. Once γv is defined, and for any choice of R, we identify two different optimization approaches: one relying on a normalized reparameterization at each layer, as in batch normalization (Section 11.2), and the other an approximate steepest descent as in path-SGD, which we refer to as DDP-SGD (Data Dependent Path SGD) and can be implemented efficiently via forward and backward propagation on the network (Section 11.3). We can now mix and match between the choice of R (i.e. the extent of data dependency) and the choice of optimization approach.\nOne particular advantage of the approximate steepest descent approach (DDP-SGD) over the normalization approach is that it is invariant to weight rebalancing (discussed in Section 11.4). This is true regardless of the amount of data-dependence used. That is, it operates more directly on the model (the function defined by the weights) rather than the parametrization (the values of the weights themselves). This brings us to a more general discussion of parametrization invariance in feedforward networks (Section 8.2).\nOur unified framework and study of in invariances also allows us to relate the different optimization approaches to Natural Gradients [87]. In particular, we show that DDP-SGD with full data-dependence can be seen as an efficient approximation of the natural gradient using only the diagonal of the Fisher information matrix (Section 11.3).\nNotation This chapter requires more involved notation that is slightly different that the notation of the rest of the dissertation. The notation is summarized in Figure 11.1.\n93\nRelated Works There has been an ongoing effort for better understanding of the optimization in deep networks and several heuristics have been suggested to improve the training [88, 89, 90, 91]. Natural gradient algorithm [87] is known to have a very strong invariance property; it is not only invariant to reparametrization, but also to the choice of network architecture. However it is known to be computationally demanding and thus many approximations have been proposed [92, 93, 94]. However, such approximations make the algorithms less invariant than the original natural gradient algorithm. [95] also discuss the connections between Natural Gradients and some of the other proposed methods for training neural networks, namely Hessian-Free Optimization [96], Krylov Subspace Descent [97] and TONGA [98].\n[62] also recently studied the issue of invariance and proposed computationally efficient approximations and alternatives to natural gradient. They study invariances as different mappings from parameter space to the same function space while we look at the invariances as transformations (inside a fixed parameter space) to which the function is invariant in the model space (see Section 8.2). Unit-wise algorithms suggested in Olivier’s work are based on block-diagonal approximations of Natural Gradient in which blocks correspond to non-input units. The computational cost of the these unit-wise algorithms is quadratic in the number of incoming weights. To alleviate this cost, [62] also proposed quasi-diagonal approximations which avoid the quadratic dependence but they are only invariant to affine transformations of activation functions. The quasi-diagonal approximations are more similar to DDP-SGD in terms of computational complexity and invariances (see Section 11.4). In particular, ignoring the non-diagonal terms related to the biases in quasi-diagonal natural gradient suggested in [62], it is then equivalent to diagonal Natural Gradient which is itself equivalent to special case of DDP-SGD when Rv is the second moment (see Table 11.1 and the discussion on relation to the Natural Gradient in Section 11.3)."
    }, {
      "heading" : "11.1 A Unified Framework",
      "text" : "We define a complexity measure on each node as follows:\nγv(w) = √ w>→vRvw→v (11.1.1)\n94\nwhere Rv is a positive semidefinite matrix that could depend on the computations feeding into v, and captures both the complexity of the nodes feeding into v and possibly their interactions. We consider several possibilities for Rv , summarized also in Table 1.\nA first possibility is to set Rv = diag ( γ2 N in(v) ) to a diagonal matrix consisting of the complexities of the incoming units. This choice does not depend on the source distribution (i.e. the data), and also ignores the effect of activations (since the activation pattern depends on the input distribution) and of dependencies between different paths in the network. Intuitively, with this choice of Rv , the measure γv(w) captures the “potential” (data independent) variability or instability at the node.\nAnother possibility is to set Rv to either the covariance (centralized second moment) or to the (uncentralized) second moment matrix of the outputs feeding into v. In this case, γ2v(w) would evaluate to the variance or (uncentralized) second moment of zv . We could also linearly combined the data independent measure, which measures inherent instability, with one of these the data-dependent measure to obtain:\nγ2v(w) = αS(zv) + (1− α) ∑\nu∈N in(v)\nγ2u(w)w 2 u→v (v /∈ Vin), (11.1.2)\nwhere S(zv) is either the variance or uncentralized second moment, and α is a parameter.\nThe complexity measure above is defined for each node of the network separately, and propagates through the network. To get an overall measure of complexity we sum over the output units and define the following complexity measure for the function fw as represented by the network:\nγ2net(w) = ∑ v∈Vout γ2v(w). (11.1.3)\nFor Rv = diag ( γ2 N in(v) ) , this complexity measure agrees with the `2-Path-regularizer as introduced by [58]. This is the sum over all paths in the network of the squared product of weights along the path. The path-regularizer is also equivalent to looking at the minimum over all “node rescalings” of w (i.e. all possibly rebalancing of weights yielding the same function fw) of the maxv ‖w→v‖. But, unlike this max-norm measure, the path-regularizer does not depend on the rebalancing and is invariant to node rescalings [58].\nFor data-dependent choices of Rv, we also get a similar invariance property. We refer to the resulting complexity measure, γ2net(w), as the Data-Dependent-Path (DDP) regularizer.\nAfter choosing Rv , we will think of γv as specifying the basic “geometry” and bias (for both optimization and learning) over weights. In terms of learning, we will (implicitly) prefer weights with smaller γv measure, and correspondingly in terms of optimization we will bias toward smaller γv “balls” (i.e. search over the part of the space where γv is smaller). We will consider two basic ways of doing this: In Section 11.2 we will consider methods that explicitly try to keep γv small for all internal nodes in the network, that is explicitly search over simpler weights. Any scaling is pushed to the output units, and this scaling\n95\nhopefully does not grow too much due. In Section 11.3 we will consider (approximate) steepest descent methods with respect to the overall γnet, i.e. updates that aim at improving the training objective while being small in terms of their effect on γnet."
    }, {
      "heading" : "11.2 DD-Path Normalization: A Batch Normalization Approach",
      "text" : "In this Section, we discuss an optimization approach based on ensuring γv for all internal nodes v are fixed and equal to one—that is, the complexity of all internal nodes is “normalized”, and any scaling happens only at the output nodes. We show that with a choice of Rv = Cov ( hN in(v)) ) , this is essentially equivalent to Batch Normalization [86].\nBatch-Normalization [86] was suggested as an alternate architecture, with special “normalization” layers, that ensure the variance of node outputs are normalized throughout training. Considering a feed-forward network as a graph, for each node v, the Batch-Normalization architecture has as parameters an (unnormalized) incoming weight vector w̃ and two additional scalars cv, bv ∈ R specifying scaling and shift respectively. The function computed by the network is then given by a forward propagation similar to standard feed-forward ReLU networks except that for each node an un-normalized activation is first computed:\nz̃v = 〈 w̃→v,hN in(v) 〉 (11.2.1)\nThen, this activation is normalized to obtain the normalized activation, which is also scaled and shifted, and the output of the unit is the output of the activation function for this activation value:\nzv = cv z̃v − E [z̃v]√\nvar(z̃v) + bv\nhv = [zv]+\n(11.2.2)\nThe variance and expectation are actually calculated on a “mini-batch” of training examples, giving the method its name. Batch-normalization then proceeds by training the architecture specified in (11.2.1) and (11.2.2) through mini-batch stochastic gradient descent, with each gradient mini-batch also used for estimating the variance and expectation in (11.2.2) for all points in the mini-batch.\nInstead of viewing batch-normalization as modifying the architecture, or forward propagation, we can view it as a re-parameterization, or change of variables, of the weights in standard feed-forward networks. In particular, instead of specifying the weights directly through w, we specify them through w̃,b and c, with the mapping:\nγ̃2v = w̃ > →vRvw̃→v Rv = Cov(hN in(v)) (11.2.3)\nwu→v = { c w̃u→vγ̃v u 6= vbias b− cE[〈w̃→v,hN in(v)〉]γ̃v u = vbias\n(11.2.4)\nThe model class of functions used by Batch-Normalization is thus exactly the same model class corresponding to standard feed-forward network, just the parameterization is different. However, the change of variables from w to w̃,b, c changes the geometry implied by the parameter space, and consequently the trajectory (in model space) of gradient updates—effectively transforming the gradient direction by the Jacobian between the two parameterizations. Batch-Normalization can thus be viewed as an alternate optimization on the same model class as standard feed-forward networks, but with a different geometry. The reparametrization ensures that γv(w) = cv for all nodes—that is, the complexity is explicit in the parameterization and thus gets implicitly regularized through the implicit regularization inherent in stochastic gradient updates.\nThe re-parameterization (11.2.4) is redundant and includes more parameters than the original param-\n96\neterization w—in addition to one parameter per edge, it includes also two additional parameters per node, namely the shift bv and scaling cv. The scaling parameters at internal nodes can be avoided and removed by noting that in ReLU networks, due to the node-rescaling property, all scaling can be done at the output nodes. That is, fixing cv = 1 for all internal v does not actually change the model class (all functions realizable by the model can be realized this way). Similarly, we can also avoid the additional shift parameter bv and rely only on bias units and bias weights w̃vbias→v that get renormalized together with weights. The bias term w̃vbias→v does not affect normalization (since it is deterministic and so has no effect on the variance), it just gets rescaled with the other weights.\nWe thus propose using a simpler reparametrization (change of variables), with the same number of parameters, using only w̃ and defining for each internal unit:\nwu→v = w̃u→v γ̃v\n(11.2.5)\nwith γ̃v as in (11.2.3), and with the output nodes un-normalized: w→Vout = w̃→Vout . This ensures that for all internal nodes γv(w) = 1.\nGoing beyond Batch-Normalization, we can also use the same approach with other choices of Rv, including all those in Table 1: We work with a reparametrization w̃, defined through (11.2.3) and (11.2.5) but with different choices of Rv, and take gradient (or stochastic gradient) steps with respect to w̃. Expectations in the definition of Rv can be estimated on the stochastic gradient descent mini-batch as in Batch-Normalization, or on independent samples of labeled or unlabeled examples. We refer to such methods as “DDP-Normalized” optimization. Gradients in DDP-Normalization can be calculated implemented very efficiently similar to Batch-Normalization (see Section 11.5.2).\nWhen using this type of DDP-Normalization, we ensure that for any internal node γv(w) = 1 (the value of γ̃v can be very different from 1, but what is fixed is the value of γv as defined in (11.1.1) in terms of the weights w, which in turn can be derived from w̃ through (11.2.4)), and so the overall complexity γnet(w) depends only on the scaling at the output layer.\nAnother interesting property of DDP-Normalization updates is that for any internal node v, the updates direction of w̃→v is exactly orthogonal to the weights:\nTheorem 37. For any weight w̃ in DDP-Normalization and any non-input node v /∈ Vin〈 w̃→v, ∂L\n∂w̃→v\n〉 = 0 (11.2.6)\nProof. First we see that (8.2.5) is true because\n∂fw(x)[v] ∂w = ( ∑ p∈Π(v) ∂πp(w) ∂we · gp(x) · x[head(p)] ) e∈E = Jv(w)> · φv(x).\nTherefore,⋃ x∈R|Vin| Span ( ∂fw(x)[v] ∂w : v ∈ Vout ) = ⋃ x∈R|Vin| Span (Jv(w)> · φv(x) : v ∈ Vout)\n= J(w)> · Span ( φ(x) : x ∈ R|Vin| ) . (11.2.7)\nConsequently, any vector of the form (∂fw(x)[v]∂we )e∈E for a fixed input x lies in the span of the row vectors of the path Jacobian J(x).\nThe second part says dG(w) = rankJ(w) if dim ( Span(φ(x) : x ∈ R|Vin|) ) = |Π|, which is the number of rows of J(w). We can see that this is true from expression (11.2.7).\n97\nThe fact that the gradient is orthogonal to the parameters means weight updates in DDP-Normalization are done in a way that it prevents the norm of weights to change considerably after each updates."
    }, {
      "heading" : "11.3 DD-Path-SGD: A Steepest Descent Approach",
      "text" : "We now turn to a more direct approach of using our complexity measure for optimization. To do so, let us first recall the strong connection between geometry, regularization and optimization through the specific example of gradient descent.\nGradient descent can be thought of as steepest descent with respect to the Euclidean norm—that is, it takes a step in a direction that maximizes improvement in the objective while also being small in terms of the Euclidean norm of the step. The step can also be viewed as a regularized optimization of the linear approximation given by the gradient, where the regularizer is squared Euclidean norm. Gradient Descent is then inherently linked to the Euclidean norm—runtime of optimization is controlled by the Euclidean norm of the optimum and stochastic gradient descent yields implicit Euclidean norm regularization. A change in norm or regularizer, which we think of as a change of geometry, would then yield different optimization procedure linked to that norm.\nWhat we would like is to use the DDP-regularizer γnet(w) to define our geometry, and for that we need a distance (or divergence) measure corresponding to it by which we can measure the “size” of each step, and require steps to be small under this measure. We cannot quite do this, but instead we use a diagonal quadratic approximation of γnet(w) about our current iterate, and then take a steepest descent step w.r.t. the quadratic norm defined by this approximation.\nSpecifically, given a choice of Rv and so complexity measure γnet(w), for the current iterate w(t) we define the following quadratic approximation:\nγ̂2net(w (t) +∆w) = γ2net(w (t))+ 〈 ∇γ2net(w(t)),∆w 〉 + 1\n2 ∆w> diag\n( ∇2γ2net(w(t)) ) ∆w (11.3.1)\nand the corresponding quadratic norm:\n‖w′ −w‖2γ̂2net = ‖w ′ −w‖2diag( 12∇2γ2net(w(t))) = ∑ (u→v)∈G 1 2 ∂2γ2net ∂w2u→v (w′u→v −wu→v)2. (11.3.2)\nWe can now define the DDP-update as:\nw(t+1) = min w\nη 〈 ∇L(w),w −w(t) 〉 + 1\n2 ‖w′ −w‖2γ̂2net . (11.3.3)\nAnother way of viewing the above approximation is as taking a diagonal quadratic approximation of the Bergman divergence of the regularizer. Solving (11.3.3) yields the update:\nw(t+1)u→v = wu→v − η\nκu→v(w)\n∂L\n∂wu→v (w(t)) where: κu→v(w) =\n1\n2 ∂2γ2net ∂w2u→v . (11.3.4)\nInstead of using the full gradient, we can also use a limited number of training examples to obtain stochastic estimates of ∂L∂wu→v (w (t))—we refer to the resulting updates as DDP-SGD.\nFor the choice Rv = diag(γ2N in(v)), we have that γ 2 net is the Path-norm and we recover Path-SGD [14]. As was shown there, the Path-SGD updates can be calculated efficiently using a forward and backward propagation on the network, similar to classical back-prop. In Section 11.5.2 we show how this type of computation can be done more generally also for other choices of Rv in Table 1.\n98\nRelation to the Natural Gradient\nThe DDP updates are similar in some ways to Natural Gradient updates, and it is interesting to understand this connection. Like the DDP, the Natural Gradients direction is a steepest descent direction, but it is based on a divergence measure calculated directly on the function fw, and not the parameterization w, and as such is invariant to reparametrizations. The natural gradient is defined as a steepest descent direction with respect to the KL-divergence between probability distributions, and so to refer to it we must refer to some probabilistic model. In our case, this will be a conditional probability model for labels Y conditioned on the inputs X, taking expectation with respect to the true marginal data distribution over X.\nWhat we will show that for the choice Rv = E[hN in(v)h>N in(v)], the DDP update can also be viewed as an approximate Natural Gradient update. More specifically, it is a diagonal approximation of the Natural Gradient for a conditional probability model q(Y|X; w) (of the labels Y given an input X) parametrized by w and specified by adding spherical Gaussian noise to the outputs of the network: Y|X ∼ N (fw(X), I|Vout|). Given the conditional probability distribution q(Y|x; w), we can calculate the expected Fisher information matrix. This is a matrix indexed by parameters of the model, in our case edges e = (u→ v) on the graph and their corresponding weights we, with entries defined as follows:\nF (w)[e, e′] = Ex∼p(X)EY∼q(Y|x;w) [ ∂ log q(Y|x; w)\n∂we\n∂ log q(Y|x; w) ∂we′\n] , (11.3.5)\nwhere x ∼ p(X) refers to the marginal source distribution (the data distribution). That is, we use the true marginal distributing over X, and the model conditional distribution Y|X, ignoring the true labels. The Natural Gradient updates can then be written as(see Section 11.5.2 for more information):\nw(t+1) = w(t) − ηF (w(t))−1∇wL(w(t)). (11.3.6)\nIf we approximate the Fisher information matrix with its diagonal elements, the update step normalizes each dimension of the gradient with the corresponding element on the diagonal of the Fisher information matrix:\nw(t+1)e = w (t) e −\nη\nF (w)[e, e]\n∂L\n∂we (w(t)). (11.3.7)\nUsing diagonal approximation of Fisher information matrix to normalize the gradient values has been suggested before as a computationally tractable alternative to the full Natural Gradient [99, 100]. [62] also suggested a “quasi-diagonal\" approximations that includes, in addition to the diagonal, also some non-diagonal terms corresponding to the relationship between the bias term and every other incoming weight into a unit.\nFor our Gaussian probability model, where log q(Y|X) = 12 ‖Y − fw(X)‖ 2\n+ const, the diagonal can be calculated as:\nF (w)[e, e] = EX∼p(X) [ ∑ v′∈Vout ( ∂fw(X)[v ′] ∂we )2] , (11.3.8)\nusing (11.5.12). We next prove that this update is equivalent to DDP-SGD for a specific choice of Rv, namely the second moment.\nTheorem 38. The Diagonal Natural Gradient indicated in equations (11.3.7) and (11.3.8) is equivalent to DDP-SGD for Rv = E [ hN in(v)h > N in(v) ] .\n99\nProof. We calculate the scaling factor κu→v(w) for DDP-SGD as follows:\nκu→v(w) = 1\n2 ∂2γ2net ∂w2u→v = 1 2 ∑ v′∈Vout ∂2E[z2v′ ] ∂w2u→v = ∑ v′∈Vout\n∂\n∂wu→v\n( 1\n2 ∂E[z2v′ ] ∂wu→v ) = ∑ v′∈Vout ∂ ∂wu→v ( E [ zv′ ∂zv′ ∂wu→v ]) = ∑ v′∈Vout ∂ ∂wu→v ( E [ zv′hu ∂zv′ ∂zv ])\n= ∑ v′∈Vout E [ h2u ( ∂zv′ ∂zv )2] = E [ h2u ∑ v′∈Vout ( ∂zv′ ∂zv )2]\n= E [ ∑ v′∈Vout ( ∂fw(X)[v ′] ∂we )2] = F (w)[u→ v, u→ v]\nTherefore, the scaling factors in DDP-SGD with Rv = E [ hN in(v)h > N in(v) ] are exactly the diagonal elements of the Fisher Information matrix used in the Natural Gradient updates."
    }, {
      "heading" : "11.4 Node-wise invariance",
      "text" : "In this section, we show that DDP-SGD is invariant to node-wise rescalings, while DDP-Normalization does not have favorable invariance properties."
    }, {
      "heading" : "11.4.1 DDP-SGD on feedforward networks",
      "text" : "In Chaper 8, we observed that feedforward ReLU networks are invariant to node-wise rescaling. To see if DDP-SGD is also invariant to such rescaling, consider a rescaled w′ = T (w), where T is a rescaling by ρ at node v. Let w+ denote the weights after a step of DDP-SGD. To establish invariance to node-rescaling we need to show that w′+ = T (w+). For the outgoing weights from v we have:\nw′+v→j = ρwv→j − ρ2η\nκv→j(w)\n∂L\nρ∂wv→j (w)\n= ρ ( wv→j −\nη\nκv→j(w)\n∂L\n∂wv→j (w)\n) = ρw+v→j\nSimilar calculations can be done for incoming weights to the node v. The only difference is that ρ will be substituted by 1/ρ. Moreover, note that due to non-negative homogeneity of ReLU activation function, the updates for the rest of the weights remain exactly the same. Therefore, DDP-SGD is node-wise rescaling invariant."
    }, {
      "heading" : "11.4.2 SGD on DDP-Normalized networks",
      "text" : "Since DDP-Normalized networks are reparametrization of feedforward networks, their invariances are different. Since the operations in DDP-Normalized networks are based on w̃, we should study the invariances for w̃. The invariances in this case are given by rescaling of incoming weights into a node,\n100\ni.e. for an internal node v and scaling ρ > 0:\nT (w̃)k→v = ρw̃k→v (∀k ∈ N in(v))\nwhile all other weights are unchanged. The DDP-Normalized networks are invariant to the above transformation because the output of each node is normalized. The SGD update rule is however not invariant to this transformation:\nT (w̃)+k→v = ρw̃k→v − η ∂L\nρ∂w̃k→v (w̃) 6= ρ\n( w̃k→v − η ∂L\n∂w̃k→v (w̃)\n) = ρw̃+k→v"
    }, {
      "heading" : "11.5 Supporting Results",
      "text" : ""
    }, {
      "heading" : "11.5.1 Implementation of DDP-Normalization",
      "text" : "Given any batch of n data points to estimate mean, variance and the gradient, the stochastic gradients for the weight w̃ (weights in the DDP-Normalized network) can then be calculated through the chain rule:\n∂L\n∂w̃→v =\n1\nnγ̃v n∑ i=1 ∂L ∂z (i) v\nh(i) N in(v) − 1 n n∑ j=1 h (j) N in(v) − ẑ (i) v 2γ̃2v ∂γ̃2v ∂w̃→v  (11.5.1) ∂L\n∂z (i) u\n= 1\nγ̃v  ∑ v∈N out(u) w̃u→v  ∂L ∂z (i) v − 1 n n∑ j=1 ∂L ∂z (j) v ( 1− αẑ (i) v ẑ (j) v γ̃2v ) z (i) u ≥0\n(11.5.2)\nwhere ẑ(i)v = z̃ (i) v − 1n ∑n j=1 z̃ (j) v and we have:\n∂γ̃2v ∂w̃→v = 2(1− α)w̃→v + 2α n n∑ i=1 ẑ(i)v\nh(i) N in(v) − 1 n n∑ j=1 h (j) N in(v)  (11.5.3) Similar to Batch-Normalization, all the above calculations can be efficiently carried out as vector operations with negligible extra memory and computations."
    }, {
      "heading" : "11.5.2 Implementation of DDP-SGD",
      "text" : "In order to compute the second derivatives κe(w) = ∂2γ2net ∂w2e , we first calculate the first derivative. The backpropagation can be done through γ2u and z (i) u but this makes it difficult to find the second derivatives. Instead we propagate the loss through γ2u and the second order terms of the form z (i) u1 z (i) u2 :\n∂γ2net ∂γ2u\n= (1− α) ∑\nv∈N out(u)\n∂γ2net ∂γ2v w2u→v (11.5.4)\n101\n∂γ2net\n∂(z (i) u1 z (i) u2 )\n= α [ ∂γ2net ∂γ2u1 ] u1=u2 +  ∑ (v1,v2)∈(N out(u1))2 ∂γ2net ∂(z (i) v1 z (i) v2 ) wu1→v1wu2→v2  z (i) u1 >0,z (i) u2 >0\n(11.5.5) Now we can calculate the partials for wu→v as follows:\n∂γ2net ∂wu→v = 2(1− α)∂γ 2 net ∂γ2v γ2uwu→v + 2 n∑ i=1 ∑ v′∈N out(u) ∂γ2net ∂(z (i) v z (i) v′ ) h(i)u z (i) v′ (11.5.6)\nSince the partials ∂γ 2 net ∂γ2u and ∂γ 2 net\n∂(z (i) u1 z (i) u2\n) do not depend on wu→v, the second order derivative can be\ncalculated directly:\nκu→v(w) = 1\n2 ∂2γ2net ∂w2u→v = (1− α)∂γ 2 net ∂γ2v γ2u + n∑ i=1 ∂γ2net ∂ ( z (i) v 2) (h(i)u )2 (11.5.7)"
    }, {
      "heading" : "11.5.3 Natural Gradient",
      "text" : "The natural gradient algorithm [87] achieves invariance by applying the inverse of the Fisher information matrix F (w(t)) at the current parameter w(t) to the negative gradient direction as follows:\nw(t+1) = w(t) + η∆(natural),\nwhere\n∆(natural) = argmin ∆∈R|E| 〈 −∂L ∂w (w(t)),∆ 〉 , s.t. ∆>F (w(t))∆ ≤ δ2 (11.5.8)\n= −F−1(w(t))∂L ∂w (w(t)). (11.5.9)\nHere F (w) is the Fisher information matrix at point w and is defined with respect to the probabilistic view of the feedforward neural network model, which we describe in more detail below.\nSuppose that we are solving a classification problem and the final layer of the network is fed into a softmax layer that determines the probability of candidate classes given the input x. Then the neural network with the softmax layer can be viewed as a conditional probability distribution\nq(y|x) = exp(fw(x)[vy])∑ v∈Vout exp(fw(x)[v]) , (11.5.10)\nwhere vy is the output node corresponding to class y. If we are solving a regression problem a Gaussian distribution is probably more appropriate for q(y|x). Given the conditional probability distribution q(y|x), the Fisher information matrix can be defined as follows:\nF (w)[e, e′] = Ex∼p(X)Ey∼q(y|x) [ ∂ log q(y|x)\n∂we\n∂ log q(y|x) ∂we′\n] , (11.5.11)\nwhere p(x) is the marginal distribution of the data.\n102\nSince we have\n∂ log q(y|x) ∂wu→v = ∂ log q(y|x) ∂zv · hu = ∑ v′∈Vout ∂ log q(y|x) ∂zv′ · ∂zv′ ∂zv · hu (11.5.12)\nusing the chain rule, each entry of the Fisher information matrix can be computed efficiently by forward and backward propagations on a minibatch.\n103\nConclusion\nIn this dissertation, we tried to explain generalization in deep learning with a view that is central around implicit regularization by the optimization algorithm, showing that the implicit regularization is the main component that should be taken into account. We proved several generalization guarantees based on different complexity measures for neural networks and investigated whether implicit regularization is indeed penalizing the complexity of the model based on any of those measures. Finally, we designed optimization algorithms to implicitly regularize complexity measures that are more suitable for neural networks and provided empirical evidence indicating that these algorithms lead to better generalization than SGD for feedforward and recurrent networks.\n104"
    } ],
    "references" : [ {
      "title" : "Deep learning in neural networks: An overview",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Greedy layer-wise training of deep networks",
      "author" : [ "Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Multilayer feedforward networks are universal approximators",
      "author" : [ "Kurt Hornik", "Maxwell Stinchcombe", "Halbert White" ],
      "venue" : "Neural networks,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1989
    }, {
      "title" : "Introduction to the Theory of Computation",
      "author" : [ "Michael Sipser" ],
      "venue" : "Thomson Course Technology,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Neural network learning: Theoretical foundations",
      "author" : [ "Martin Anthony", "Peter L. Bartlett" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Understanding Machine Learning: From Theory to Algorithms",
      "author" : [ "Shai Shalev-Shwartz", "Shai Ben-David" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network",
      "author" : [ "Peter L. Bartlett" ],
      "venue" : "IEEE transactions on information theory,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "Cryptographic hardness for learning intersections of halfspaces",
      "author" : [ ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "From average case complexity to improper learning",
      "author" : [ "Amit Daniely", "Nati Linial", "Shai Shalev-Shwartz" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Entropy-sgd: Biasing gradient descent into wide valleys",
      "author" : [ "Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun" ],
      "venue" : "arXiv preprint arXiv:1611.01838,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "On large-batch training for deep learning: Generalization gap and sharp minima",
      "author" : [ "Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang" ],
      "venue" : "arXiv preprint arXiv:1609.04836,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Path-SGD: Path-normalized optimization in deep neural networks",
      "author" : [ "Behnam Neyshabur", "Ruslan Salakhutdinov", "Nathan Srebro" ],
      "venue" : "In Advanced in Neural Information Processsing Systems (NIPS),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E Hinton" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Deep sparse rectifier",
      "author" : [ "Xavier Glorot Antoine Bordes", "Yoshua Bengio" ],
      "venue" : "networks. AISTATS,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "On rectified linear units for speech processing",
      "author" : [ "M.D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean", "G.E. Hinton" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "On the uniform convergence of relative frequencies of events to their probabilities",
      "author" : [ "Vladimir N Vapnik", "A Ya Chervonenkis" ],
      "venue" : "Theory of Probability & Its Applications,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1971
    }, {
      "title" : "Neural network learning: Theoretical foundations",
      "author" : [ "Martin Anthony", "Peter L Bartlett" ],
      "venue" : "cambridge university press,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network",
      "author" : [ "Peter L Bartlett" ],
      "venue" : "IEEE transactions on Information Theory,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1998
    }, {
      "title" : "Almost linear vc dimension bounds for piecewise polynomial networks",
      "author" : [ "Peter L Bartlett", "Vitaly Maiorov", "Ron Meir" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1998
    }, {
      "title" : "Understanding machine learning: From theory to algorithms",
      "author" : [ "Shai Shalev-Shwartz", "Shai Ben-David" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "The impact of the nonlinearity on the VC-dimension of a deep network",
      "author" : [ "P.L. Bartlett" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2017
    }, {
      "title" : "Nearly-tight vc-dimension bounds for piecewise linear neural networks",
      "author" : [ "Nick Harvey", "Chris Liaw", "Abbas Mehrabian" ],
      "venue" : "arXiv preprint arXiv:1703.02930,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2017
    }, {
      "title" : "Understanding deep learning requires rethinking generalization",
      "author" : [ "Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2017
    }, {
      "title" : "In search of the real inductive bias: On the role of implicit regularization in deep learning",
      "author" : [ "Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro" ],
      "venue" : "Proceeding of the International Conference on Learning Representations workshop track,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Rademacher and gaussian complexities: Risk bounds and structural results",
      "author" : [ "Peter L Bartlett", "Shahar Mendelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2002
    }, {
      "title" : "Distance-based classification with lipschitz functions",
      "author" : [ "Ulrike von Luxburg", "Olivier Bousquet" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2004
    }, {
      "title" : "Robustness and generalization",
      "author" : [ "Huan Xu", "Shie Mannor" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2012
    }, {
      "title" : "Generalization error of invariant classifiers",
      "author" : [ "Jure Sokolic", "Raja Giryes", "Guillermo Sapiro", "Miguel RD Rodrigues" ],
      "venue" : "arXiv preprint arXiv:1610.04574,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2016
    }, {
      "title" : "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data",
      "author" : [ "Gintare Karolina Dziugaite", "Daniel M Roy" ],
      "venue" : "arXiv preprint arXiv:1703.11008,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2017
    }, {
      "title" : "Some PAC-Bayesian theorems",
      "author" : [ "David A McAllester" ],
      "venue" : "In Proceedings of the eleventh annual conference on Computational learning theory,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1998
    }, {
      "title" : "PAC-Bayesian model averaging",
      "author" : [ "David A McAllester" ],
      "venue" : "In Proceedings of the twelfth annual conference on Computational learning theory,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1999
    }, {
      "title" : "Simplified pac-bayesian margin bounds",
      "author" : [ "David McAllester" ],
      "venue" : "Lecture notes in computer science,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2003
    }, {
      "title" : "Pac-bayes & margins",
      "author" : [ "John Langford", "John Shawe-Taylor" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2003
    }, {
      "title" : "not) bounding the true error",
      "author" : [ "John Langford", "Rich Caruana" ],
      "venue" : "In Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2001
    }, {
      "title" : "Maximum-margin matrix factorization",
      "author" : [ "Nathan Srebro", "Jason Rennie", "Tommi S. Jaakkola" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2004
    }, {
      "title" : "A rank minimization heuristic with application to minimum order system approximation",
      "author" : [ "Maryam Fazel", "Haitham Hindi", "Stephen P. Boyd" ],
      "venue" : "Proceedings of American Control Conference,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2001
    }, {
      "title" : "Computational enhancements in low-rank semidefinite programming",
      "author" : [ "Samuel Burer", "Changhui Choi" ],
      "venue" : "Optimization Methods and Software,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2006
    }, {
      "title" : "Weighted low-rank approximations",
      "author" : [ "Nathan Srebro", "Tommi S. Jaakkola" ],
      "venue" : "ICML, pages 720–727,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2003
    }, {
      "title" : "Implicit regularization in matrix factorization",
      "author" : [ "Suriya Gunasekar", "Blake Woodworth", "Srinadh Bhojanapalli", "Behnam Neyshabur", "Nathan Srebro" ],
      "venue" : "arXiv preprint arXiv:1705.09280,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2017
    }, {
      "title" : "Fast maximum margin matrix factorization for collaborative prediction",
      "author" : [ "Jasson DM Rennie", "Nathan Srebro" ],
      "venue" : "In Proceedings of the 22nd international conference on Machine learning,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2005
    }, {
      "title" : "Collaborative filtering in a non-uniform world: Learning with the weighted trace norm",
      "author" : [ "Nathan Srebro", "Ruslan Salakhutdinov" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2010
    }, {
      "title" : "Convex neural networks. Advances in neural information processing",
      "author" : [ "Yoshua Bengio", "Nicolas L. Roux", "Pascal Vincent", "Olivier Delalleau", "Patrice Marcotte" ],
      "venue" : null,
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2005
    }, {
      "title" : "Spectrally-normalized margin bounds for neural networks",
      "author" : [ "Peter Bartlett", "Dylan J Foster", "Matus Telgarsky" ],
      "venue" : "arXiv preprint arXiv:1706.08498,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2017
    }, {
      "title" : "Rademacher and gaussian complexities: Risk bounds and structural results",
      "author" : [ "Peter L. Bartlett", "Shahar Mendelson" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2003
    }, {
      "title" : "Empirical margin distributions and bounding the generalization error of combined classifiers",
      "author" : [ "Vladimir Koltchinskii", "Dmitry Panchenko" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2002
    }, {
      "title" : "Breaking the curse of dimensionality with convex neural networks",
      "author" : [ "Francis Bach" ],
      "venue" : "Technical report,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2014
    }, {
      "title" : "Kernel methods for deep learning",
      "author" : [ "Youngmin Cho", "Lawrence K. Saul" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2009
    }, {
      "title" : "Efficient agnostic learning of neural networks with bounded fan-in",
      "author" : [ "Wee Sun Lee", "Peter L Bartlett", "Robert C Williamson" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 1996
    }, {
      "title" : "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization",
      "author" : [ "Sham M Kakade", "Karthik Sridharan", "AmbujTewari" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2009
    }, {
      "title" : "A new perspective on learning linear separators with large lqlp margins",
      "author" : [ "Maria-Florina Balcan", "Christopher Berlind" ],
      "venue" : "Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2014
    }, {
      "title" : "The best constants in the khintchine inequality",
      "author" : [ "Uffe Haagerup" ],
      "venue" : "Studia Mathematica,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 1981
    }, {
      "title" : "Cryptographic hardness for learning intersections of halfspaces",
      "author" : [ "Adam R Klivans", "Alexander A Sherstov" ],
      "venue" : null,
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2006
    }, {
      "title" : "On the computational efficiency of training neural networks",
      "author" : [ "Roi Livni", "Shai Shalev-Shwartz", "Ohad Shamir" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2014
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "Joel A Tropp" ],
      "venue" : "Foundations of computational mathematics,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2012
    }, {
      "title" : "Norm-based capacity control in neural networks",
      "author" : [ "Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro" ],
      "venue" : "In Proceeding of the 28th Conference on Learning Theory (COLT),",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 2014
    }, {
      "title" : "Data-dependent path normalization in neural networks",
      "author" : [ "Behnam Neyshabur", "Ryota Tomioka", "Ruslan Salakhutdinov", "Nathan Srebro" ],
      "venue" : "In the International Conference on Learning Representations,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2016
    }, {
      "title" : "On the universality of online mirror descent",
      "author" : [ "Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 2011
    }, {
      "title" : "Riemannian metrics for neural networks ii: recurrent networks and learning symbolic data sequences",
      "author" : [ "Yann Ollivier" ],
      "venue" : "Information and Inference, page iav007,",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2015
    }, {
      "title" : "Data-dependent path normalization in neural networks",
      "author" : [ "Behnam Neyshabur", "Ryota Tomioka", "Ruslan Salakhutdinov", "Nathan Srebro" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2016
    }, {
      "title" : "Path-normalized optimization of recurrent neural networks with relu activations",
      "author" : [ "Behnam Neyshabur", "Yuhuai Wu", "Ruslan Salakhutdinov", "Nathan Srebro" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2016
    }, {
      "title" : "Norm-based capacity control in neural networks",
      "author" : [ "Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro" ],
      "venue" : "In The 28th Conference on Learning Theory,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2015
    }, {
      "title" : "Rank, trace-norm and max-norm",
      "author" : [ "Nathan Srebro", "Adi Shraibman" ],
      "venue" : "In Learning Theory,",
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 2005
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "68",
      "shortCiteRegEx" : "68",
      "year" : 1929
    }, {
      "title" : "Path-sgd: Path-normalized optimization in deep neural networks",
      "author" : [ "Behnam Neyshabur", "Ruslan R Salakhutdinov", "Nati Srebro" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "69",
      "shortCiteRegEx" : "69",
      "year" : 2015
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "70",
      "shortCiteRegEx" : "70",
      "year" : 1998
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky", "Geoffrey Hinton" ],
      "venue" : "Computer Science Department,",
      "citeRegEx" : "71",
      "shortCiteRegEx" : "71",
      "year" : 2009
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng" ],
      "venue" : "In NIPS workshop on deep learning and unsupervised feature learning,",
      "citeRegEx" : "72",
      "shortCiteRegEx" : "72",
      "year" : 2011
    }, {
      "title" : "In search of the real inductive bias: On the role of implicit regularization in deep learning",
      "author" : [ "Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro" ],
      "venue" : "International Conference on Learning Representations (ICLR) workshop track,",
      "citeRegEx" : "73",
      "shortCiteRegEx" : "73",
      "year" : 2015
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "74",
      "shortCiteRegEx" : "74",
      "year" : 1993
    }, {
      "title" : "The vanishing gradient problem during learning recurrent neural nets and problem solutions",
      "author" : [ "Sepp Hochreiter" ],
      "venue" : "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,",
      "citeRegEx" : "75",
      "shortCiteRegEx" : "75",
      "year" : 1998
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Yoshua Bengio", "Patrice Simard", "Paolo Frasconi" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "76",
      "shortCiteRegEx" : "76",
      "year" : 1994
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "77",
      "shortCiteRegEx" : "77",
      "year" : 1997
    }, {
      "title" : "A simple way to initialize recurrent networks of rectified linear units",
      "author" : [ "Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton" ],
      "venue" : "arXiv preprint arXiv:1504.00941,",
      "citeRegEx" : "78",
      "shortCiteRegEx" : "78",
      "year" : 2015
    }, {
      "title" : "Unitary evolution recurrent neural networks",
      "author" : [ "Martin Arjovsky", "Amar Shah", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1511.06464,",
      "citeRegEx" : "79",
      "shortCiteRegEx" : "79",
      "year" : 2015
    }, {
      "title" : "Improving performance of recurrent neural network with relu nonlinearity",
      "author" : [ "Sachin S. Talathi", "Aniket Vartak" ],
      "venue" : "In the International Conference on Learning Representations workshop track,",
      "citeRegEx" : "80",
      "shortCiteRegEx" : "80",
      "year" : 2014
    }, {
      "title" : "Regularization and nonlinearities for neural language models: when are they needed",
      "author" : [ "Marius Pachitariu", "Maneesh Sahani" ],
      "venue" : "arXiv preprint arXiv:1301.5650,",
      "citeRegEx" : "81",
      "shortCiteRegEx" : "81",
      "year" : 2013
    }, {
      "title" : "Subword language modeling with neural networks",
      "author" : [ "Tomáš Mikolov", "Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink", "J Cernocky" ],
      "venue" : "(http://www.fit.vutbr.cz/ imikolov/rnnlm/char.pdf),",
      "citeRegEx" : "82",
      "shortCiteRegEx" : "82",
      "year" : 2012
    }, {
      "title" : "Regularizing RNNs by stabilizing activations",
      "author" : [ "David Krueger", "Roland Memisevic" ],
      "venue" : "In Proceeding of the International Conference on Learning Representations,",
      "citeRegEx" : "83",
      "shortCiteRegEx" : "83",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "In Proceeding of the International Conference on Learning Representations,",
      "citeRegEx" : "84",
      "shortCiteRegEx" : "84",
      "year" : 2015
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "Andrew M Saxe", "James L McClelland", "Surya Ganguli" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "85",
      "shortCiteRegEx" : "85",
      "year" : 2014
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "In ICML,",
      "citeRegEx" : "86",
      "shortCiteRegEx" : "86",
      "year" : 2015
    }, {
      "title" : "Natural gradient works efficiently in learning",
      "author" : [ "Shun-Ichi Amari" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "87",
      "shortCiteRegEx" : "87",
      "year" : 1998
    }, {
      "title" : "Efficient backprop. In Neural Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524",
      "author" : [ "Yann Le Cun", "Léon Bottou", "Genevieve B. Orr", "Klaus-Robert Müller" ],
      "venue" : null,
      "citeRegEx" : "88",
      "shortCiteRegEx" : "88",
      "year" : 1998
    }, {
      "title" : "Exploring strategies for training deep neural networks",
      "author" : [ "Hugo Larochelle", "Yoshua Bengio", "Jérôme Louradour", "Pascal Lamblin" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "89",
      "shortCiteRegEx" : "89",
      "year" : 2009
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "90",
      "shortCiteRegEx" : "90",
      "year" : 2010
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton" ],
      "venue" : "In ICML,",
      "citeRegEx" : "91",
      "shortCiteRegEx" : "91",
      "year" : 2013
    }, {
      "title" : "Scaling up natural gradient by sparsely factorizing the inverse Fisher matrix",
      "author" : [ "Roger Grosse", "Ruslan Salakhudinov" ],
      "venue" : "In ICML,",
      "citeRegEx" : "92",
      "shortCiteRegEx" : "92",
      "year" : 2015
    }, {
      "title" : "Optimizing neural networks with Kronecker-factored approximate curvature",
      "author" : [ "James Martens", "Roger Grosse" ],
      "venue" : "In ICML,",
      "citeRegEx" : "93",
      "shortCiteRegEx" : "93",
      "year" : 2015
    }, {
      "title" : "Revisiting natural gradient for deep networks",
      "author" : [ "Razvan Pascanu", "Yoshua Bengio" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "95",
      "shortCiteRegEx" : "95",
      "year" : 2014
    }, {
      "title" : "Deep learning via hessian-free optimization",
      "author" : [ "James Martens" ],
      "venue" : "In ICML,",
      "citeRegEx" : "96",
      "shortCiteRegEx" : "96",
      "year" : 2010
    }, {
      "title" : "Krylov subspace descent for deep learning",
      "author" : [ "Oriol Vinyals", "Daniel Povey" ],
      "venue" : "In ICML,",
      "citeRegEx" : "97",
      "shortCiteRegEx" : "97",
      "year" : 2011
    }, {
      "title" : "Topmoumoute online natural gradient algorithm",
      "author" : [ "Nicolas L Roux", "Pierre-Antoine Manzagol", "Yoshua Bengio" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "98",
      "shortCiteRegEx" : "98",
      "year" : 2008
    }, {
      "title" : "Neural networks-tricks of the trade",
      "author" : [ "Yann LeCun", "Leon Bottou", "Genevieve B Orr", "Klaus-Robert Muller" ],
      "venue" : "Springer Lecture Notes in Computer Sciences,",
      "citeRegEx" : "99",
      "shortCiteRegEx" : "99",
      "year" : 1998
    }, {
      "title" : "Lecun. No more pesky learning rates",
      "author" : [ "Tom Schaul", "Sixin Zhang", "Yann" ],
      "venue" : "In ICML,",
      "citeRegEx" : "100",
      "shortCiteRegEx" : "100",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The terms “neural networks” and “deep learning” are often used interchangeably as many modern deep learning models are slight modifications of different types of neural networks suggested originally around 1950-2000 [1].",
      "startOffset" : 216,
      "endOffset" : 219
    }, {
      "referenceID" : 1,
      "context" : "Interest in deep learning was revived around 2006 [2, 3] and since then, it has had enormous practical successes in different areas [4].",
      "startOffset" : 50,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "Interest in deep learning was revived around 2006 [2, 3] and since then, it has had enormous practical successes in different areas [4].",
      "startOffset" : 50,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "Universal approximation theorem states that for any given precision, feed-forward networks with a single hidden layer containing a finite number of hidden units can approximate any continuous function [5].",
      "startOffset" : 201,
      "endOffset" : 204
    }, {
      "referenceID" : 5,
      "context" : "With hard-threshold activations, the VC-dimension, and hence sample complexity, of the class of functions realizable with a feed-forward network is equal, up to logarithmic factors, to the number of edges in the network [7, 8], corresponding to the number of parameters.",
      "startOffset" : 220,
      "endOffset" : 226
    }, {
      "referenceID" : 6,
      "context" : "With hard-threshold activations, the VC-dimension, and hence sample complexity, of the class of functions realizable with a feed-forward network is equal, up to logarithmic factors, to the number of edges in the network [7, 8], corresponding to the number of parameters.",
      "startOffset" : 220,
      "endOffset" : 226
    }, {
      "referenceID" : 8,
      "context" : "That is, even for binary classification using a network with a single hidden layer and a logarithmic (in the input size) number of hidden units, and even if we know the true targets are exactly captured by such a small network, there is likely no efficient algorithm that can ensure error better than 1/2 [10, 11]—not if the algorithm tries to fit such a network, not even if it tries to fit a much larger 1Using weights with very high precision and vastly different magnitudes it is possible to shatter a number of points quadratic in the number of edges when activations such as the sigmoid, ramp or hinge are used [8, Chapter 20.",
      "startOffset" : 305,
      "endOffset" : 313
    }, {
      "referenceID" : 9,
      "context" : "That is, even for binary classification using a network with a single hidden layer and a logarithmic (in the input size) number of hidden units, and even if we know the true targets are exactly captured by such a small network, there is likely no efficient algorithm that can ensure error better than 1/2 [10, 11]—not if the algorithm tries to fit such a network, not even if it tries to fit a much larger 1Using weights with very high precision and vastly different magnitudes it is possible to shatter a number of points quadratic in the number of edges when activations such as the sigmoid, ramp or hinge are used [8, Chapter 20.",
      "startOffset" : 305,
      "endOffset" : 313
    }, {
      "referenceID" : 7,
      "context" : "But even with such activations, the VC dimension can still be bounded by the size and depth [9, 7, 8].",
      "startOffset" : 92,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "But even with such activations, the VC dimension can still be bounded by the size and depth [9, 7, 8].",
      "startOffset" : 92,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "But even with such activations, the VC dimension can still be bounded by the size and depth [9, 7, 8].",
      "startOffset" : 92,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "Different algorithmic choices for optimization such as the initialization, update rule, learning rate, and stopping condition, will lead to different global minima with different generalization behavior [12, 13, 14].",
      "startOffset" : 203,
      "endOffset" : 215
    }, {
      "referenceID" : 11,
      "context" : "Different algorithmic choices for optimization such as the initialization, update rule, learning rate, and stopping condition, will lead to different global minima with different generalization behavior [12, 13, 14].",
      "startOffset" : 203,
      "endOffset" : 215
    }, {
      "referenceID" : 12,
      "context" : "Different algorithmic choices for optimization such as the initialization, update rule, learning rate, and stopping condition, will lead to different global minima with different generalization behavior [12, 13, 14].",
      "startOffset" : 203,
      "endOffset" : 215
    }, {
      "referenceID" : 6,
      "context" : "More details on the formal model can be found in Shalev-Shwartz and Ben-David [8].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "We will focus mostly on the hinge, or RELU (REctified Linear Unit) activation, which is currently in popular use [15, 16, 17], σRELU(z) = [z]+ = max(z, 0).",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 14,
      "context" : "We will focus mostly on the hinge, or RELU (REctified Linear Unit) activation, which is currently in popular use [15, 16, 17], σRELU(z) = [z]+ = max(z, 0).",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 15,
      "context" : "We will focus mostly on the hinge, or RELU (REctified Linear Unit) activation, which is currently in popular use [15, 16, 17], σRELU(z) = [z]+ = max(z, 0).",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "The following generalization bound then holds with probability 1− δ [18, 19]:",
      "startOffset" : 68,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : "Feedforward Networks The VC dimension of feedforward networks can also be bounded in terms of the number of parameters nparam[20, 21, 22, 23].",
      "startOffset" : 125,
      "endOffset" : 141
    }, {
      "referenceID" : 18,
      "context" : "Feedforward Networks The VC dimension of feedforward networks can also be bounded in terms of the number of parameters nparam[20, 21, 22, 23].",
      "startOffset" : 125,
      "endOffset" : 141
    }, {
      "referenceID" : 19,
      "context" : "Feedforward Networks The VC dimension of feedforward networks can also be bounded in terms of the number of parameters nparam[20, 21, 22, 23].",
      "startOffset" : 125,
      "endOffset" : 141
    }, {
      "referenceID" : 20,
      "context" : "Feedforward Networks The VC dimension of feedforward networks can also be bounded in terms of the number of parameters nparam[20, 21, 22, 23].",
      "startOffset" : 125,
      "endOffset" : 141
    }, {
      "referenceID" : 21,
      "context" : "In particular, Bartlett [24] and Harvey et al.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 22,
      "context" : "[25], following Bartlett et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[22], give the following tight (up to logarithmic factors) bound on the VC dimension and hence capacity of feedforward networks with ReLU activations: VC-dim = Õ(d ∗ nparam) (3.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "Neural networks used in practice often have significantly more parameters than samples, and indeed can perfectly fit even random labels, obviously without generalizing [26].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 24,
      "context" : "Moreover, measuring complexity in terms of number of parameters cannot explain the reduction in generalization error as the number of hidden units increase [27].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 25,
      "context" : "Feedforward Networks [28] proved that the Rademacher complexity of fully connected feedforward networks on set S can be bounded based on the `1 norm of the weights of hidden units in each layer as follows: Rm(F) ≤ √ 4d ln (nin) ∏d i=1 ‖Wi‖ 2 1,∞maxx∈S ‖x‖∞ m (3.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 25,
      "context" : "4) where ‖Wi‖1,∞ is the maximum over hidden units in layer i of the `1 norm of incoming weights to the hidden unit [28].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 26,
      "context" : "Luxburg and Bousquet [29] studied the capacity of functions with bounded Lipschitz constant on metric space (X ,M) with a finite diameter diamM(X ) = supx,y∈XM(x, y) and showed that the capacity is proportional to ( CM γ )n diamM(X ) where γ is the margin.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 27,
      "context" : "Another related approach is through algorithmic robustness as suggested by Xu and Mannor [30].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 27,
      "context" : "1) Xu and Mannor [30] showed the capacity of a model class whose models are K-robust scales as K.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 27,
      "context" : "Feedforward Networks Returning to our original question, theC`∞ andC`2 Lipschitz constants of the network can be bounded by ∏d i=1 ‖Wi‖1,∞ (hence `1-path norm) and ∏d i=1 ‖Wi‖2, respectively [30, 31].",
      "startOffset" : 191,
      "endOffset" : 199
    }, {
      "referenceID" : 28,
      "context" : "Feedforward Networks Returning to our original question, theC`∞ andC`2 Lipschitz constants of the network can be bounded by ∏d i=1 ‖Wi‖1,∞ (hence `1-path norm) and ∏d i=1 ‖Wi‖2, respectively [30, 31].",
      "startOffset" : 191,
      "endOffset" : 199
    }, {
      "referenceID" : 11,
      "context" : "[13] and corresponds to robustness to adversarial perturbations on the parameter space: ζα(W) = max|ui|≤α(|wi|+1) L̂(fw+u)− L̂(fw) 1 + L̂(fw) ' max |ui|≤α(|wi|+1) L̂(fw+u)− L̂(fw), (3.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "This connection between sharpness and the PAC-Bayes framework was also recently noted by Dziugaite and Roy [32].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 30,
      "context" : "The PAC-Bayesian framework [33, 34] provides guarantees on the expected error of a randomized predictor (hypothesis), drawn from a distribution denoted Q and sometimes referred to as a “posterior” (although it need not be the Bayesian posterior), that depends on the training data.",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 31,
      "context" : "The PAC-Bayesian framework [33, 34] provides guarantees on the expected error of a randomized predictor (hypothesis), drawn from a distribution denoted Q and sometimes referred to as a “posterior” (although it need not be the Bayesian posterior), that depends on the training data.",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 27,
      "context" : "Then, given a “prior” distribution P over the hypothesis that is independent of the training data, with probability at least 1− δ over the draw of the training data, the expected error 1Xu and Mannor [30] have defined the robustness as a property of learning algorithm given the model class and the training set.",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 32,
      "context" : "of fw+u can be bounded as follows [35]: Eu [L(fw+u)] ≤ Eu [ L̂(fw+u) ] + √ Eu [ L̂(fw+u) ] K +K (3.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 33,
      "context" : "The proof of the lemma uses similar ideas as in the proof for the case of linear separators, discussed by Langford and Shawe-Taylor [36] and McAllester [35].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 32,
      "context" : "The proof of the lemma uses similar ideas as in the proof for the case of linear separators, discussed by Langford and Shawe-Taylor [36] and McAllester [35].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 29,
      "context" : "Feedforward Networks This connection between sharpness and the PAC-Bayesian framework was also recently noticed by Dziugaite and Roy [32], who optimize the PAC-Bayes generalization bound over a family of multivariate Gaussian distributions, extending the work of Langford and Caruana [37].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 34,
      "context" : "Feedforward Networks This connection between sharpness and the PAC-Bayesian framework was also recently noticed by Dziugaite and Roy [32], who optimize the PAC-Bayes generalization bound over a family of multivariate Gaussian distributions, extending the work of Langford and Caruana [37].",
      "startOffset" : 284,
      "endOffset" : 288
    }, {
      "referenceID" : 8,
      "context" : "That is, even for binary classification using a network with a single hidden layer and a logarithmic (in the input size) number of hidden units, and even if we know the true targets are exactly captured by such a small network, there is likely no efficient algorithm that can ensure error better than 1/2 [10, 11]—not if the algorithm tries to fit such a network, not even if it tries to fit a much larger network, and in fact no matter how the algorithm represents predictors.",
      "startOffset" : 305,
      "endOffset" : 313
    }, {
      "referenceID" : 9,
      "context" : "That is, even for binary classification using a network with a single hidden layer and a logarithmic (in the input size) number of hidden units, and even if we know the true targets are exactly captured by such a small network, there is likely no efficient algorithm that can ensure error better than 1/2 [10, 11]—not if the algorithm tries to fit such a network, not even if it tries to fit a much larger network, and in fact no matter how the algorithm represents predictors.",
      "startOffset" : 305,
      "endOffset" : 313
    }, {
      "referenceID" : 35,
      "context" : "For example, constraining the Frobenius norm of U and V corresponds to using the trace-norm as an inductive bias [38]:",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 36,
      "context" : "Unlike the rank, the trace-norm (as well as other factorization norms) is convex, and leads to tractable learning problems [39, 38].",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 35,
      "context" : "Unlike the rank, the trace-norm (as well as other factorization norms) is convex, and leads to tractable learning problems [39, 38].",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 37,
      "context" : "by a local search over the weights of the network), if the dimensionality is high enough and the norm is regularized, we can ensure convergence to a global minima [40].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 38,
      "context" : "This is in stark contrast to the dimensionality-constrained low-rank situation, where the limiting factor is the number of hidden units, and local minima are abundant [41].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 39,
      "context" : "[42] provided empirical and theoretical evidence on the implicit regularization of gradient descent for matrix factorization.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 40,
      "context" : ", a very successful approach for training low trace-norm models, and other infinite-dimensional bounded-norm factorization models, is to approximate them using a finite dimensional representation [43, 44].",
      "startOffset" : 196,
      "endOffset" : 204
    }, {
      "referenceID" : 41,
      "context" : ", a very successful approach for training low trace-norm models, and other infinite-dimensional bounded-norm factorization models, is to approximate them using a finite dimensional representation [43, 44].",
      "startOffset" : 196,
      "endOffset" : 204
    }, {
      "referenceID" : 42,
      "context" : "In particular, we show how per-unit regularization is equivalent to a novel path-based regularizer and how overall `2 regularization for two-layer networks is equivalent to so-called “convex neural networks” [45].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 43,
      "context" : "[46] have shown a generalization bound based on the product of spectral norm of the layers using covering numbers.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 44,
      "context" : ", [47] for a complete treatment, and Section 5.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "Per-unit `1-regularization was studied by [9, 48, 47] who showed generalization guarantees.",
      "startOffset" : 42,
      "endOffset" : 53
    }, {
      "referenceID" : 45,
      "context" : "Per-unit `1-regularization was studied by [9, 48, 47] who showed generalization guarantees.",
      "startOffset" : 42,
      "endOffset" : 53
    }, {
      "referenceID" : 44,
      "context" : "Per-unit `1-regularization was studied by [9, 48, 47] who showed generalization guarantees.",
      "startOffset" : 42,
      "endOffset" : 53
    }, {
      "referenceID" : 46,
      "context" : "A two-layer network of this form with RELU activation was also considered by [49], who studied its approximation ability and suggested heuristics for learning it.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 47,
      "context" : "Per-unit `2 regularization in a two-layer network was considered by [50], who showed it is equivalent to using a specific kernel.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 47,
      "context" : "Furthermore, the kernel view of [50] allows obtaining size-independent generalization bound for two-layer networks with bounded per-unit `2 norm (i.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 48,
      "context" : "Indeed, one can consider functional-gradient or boosting-type strategies for learning a predictor in the class [51].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 46,
      "context" : "However, as Bach [49] points out, this is not so easy as it requires finding the best fit for a target with a RELU unit, which is not easy.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "Subject to the the strong random CSP assumptions in [11], it is not possible to efficiently PAC learn (even improperly) functions {±1}nin → {±1} realizable with unit margin by F2 1,∞ when ψ1,∞ = ω(nin) (e.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 42,
      "context" : "Convex Neural Nets [45] over inputs in Rin are two-layer networks with a fixed infinite hidden layer consisting of all units with weights w ∈ G for some base class G ∈ Rin , and a second `1-regularized layer.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 9,
      "context" : "Subject to the the strong random CSP assumptions in [11], it is not possible to efficiently PAC learn (even improperly) functions {±1}nin → {±1} realizable with unit margin by F2 p,p when μp,p = ω(n 1 p in ).",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 49,
      "context" : "The upper bounds presented here are particularly similar to generalization bounds in [52] and [53].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 50,
      "context" : "The upper bounds presented here are particularly similar to generalization bounds in [52] and [53].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 51,
      "context" : "The sharp value of the constant Cp was found by Haagerup [54] but for our analysis, it is enough to note that if p ≥ 1 we have Cp ≤ √p.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "[11] show in Theorem 5.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 52,
      "context" : "Furthermore, for any > 0, [55] prove this hardness result subject to intractability of Q̃(n in )-unique shortest vector problem for k = n in.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 53,
      "context" : "The proof is by a construction that is similar to the one in [56].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 54,
      "context" : "The following inequality holds on the spectral norm of Ui [57]: PUi∼N(0,σq) [‖Ui‖2 > t] ≤ 2he−t /2hσ q .",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 25,
      "context" : "Unlike existing generalization bounds [28, 58, 29, 30, 31], our sharpness based bound does not suffer from exponential dependence on depth.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 55,
      "context" : "Unlike existing generalization bounds [28, 58, 29, 30, 31], our sharpness based bound does not suffer from exponential dependence on depth.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 26,
      "context" : "Unlike existing generalization bounds [28, 58, 29, 30, 31], our sharpness based bound does not suffer from exponential dependence on depth.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 27,
      "context" : "Unlike existing generalization bounds [28, 58, 29, 30, 31], our sharpness based bound does not suffer from exponential dependence on depth.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 28,
      "context" : "Unlike existing generalization bounds [28, 58, 29, 30, 31], our sharpness based bound does not suffer from exponential dependence on depth.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 55,
      "context" : "The norm-based measures we investigate in this work and their corresponding capacity bounds are as follows 1: • `2 norm with capacity proportional to 1 γ2 margin ∏d i=1 4 ‖Wi‖ 2 F [58].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 25,
      "context" : "• `1-path norm with capacity proportional to 1 γ2 margin (∑ j∈ ∏d k=0[hk] ∣∣∣∏di=1 2Wi[ji, ji−1]∣∣∣)2[28, 58].",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 55,
      "context" : "• `1-path norm with capacity proportional to 1 γ2 margin (∑ j∈ ∏d k=0[hk] ∣∣∣∏di=1 2Wi[ji, ji−1]∣∣∣)2[28, 58].",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 56,
      "context" : "In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU 1We have dropped the term that only depend on the norm of the input.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 61,
      "context" : "In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU 1We have dropped the term that only depend on the norm of the input.",
      "startOffset" : 116,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU 1We have dropped the term that only depend on the norm of the input.",
      "startOffset" : 116,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU 1We have dropped the term that only depend on the norm of the input.",
      "startOffset" : 116,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU 1We have dropped the term that only depend on the norm of the input.",
      "startOffset" : 116,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU 1We have dropped the term that only depend on the norm of the input.",
      "startOffset" : 135,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU 1We have dropped the term that only depend on the norm of the input.",
      "startOffset" : 135,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU 1We have dropped the term that only depend on the norm of the input.",
      "startOffset" : 135,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU 1We have dropped the term that only depend on the norm of the input.",
      "startOffset" : 155,
      "endOffset" : 169
    }, {
      "referenceID" : 2,
      "context" : "In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU 1We have dropped the term that only depend on the norm of the input.",
      "startOffset" : 155,
      "endOffset" : 169
    }, {
      "referenceID" : 0,
      "context" : "In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU 1We have dropped the term that only depend on the norm of the input.",
      "startOffset" : 155,
      "endOffset" : 169
    }, {
      "referenceID" : 2,
      "context" : "In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU 1We have dropped the term that only depend on the norm of the input.",
      "startOffset" : 175,
      "endOffset" : 189
    }, {
      "referenceID" : 2,
      "context" : "In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU 1We have dropped the term that only depend on the norm of the input.",
      "startOffset" : 175,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "In the rest of the experiments, we train a modified version of the VGG architecture [59] with the configuration 2 × [64, 3, 3, 1], 2 × [128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU 1We have dropped the term that only depend on the norm of the input.",
      "startOffset" : 175,
      "endOffset" : 189
    }, {
      "referenceID" : 55,
      "context" : "Without further conditions on weights, exponential dependence on depth is tight but the 4d dependence might be loose [58].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 57,
      "context" : "When calculating norms on a network with a Batch Normalization layer, we reparametrize the network to one that represents the exact same function without Batch Normalization as suggested in [60].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 11,
      "context" : "We calculate the sharpness, as suggested in [13] - for each parameter wi we bound the magnitude of perturbation by α(|wi|+ 1) for α = 5.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 29,
      "context" : "As discussed earlier, Dziugaite and Roy [32] numerically optimize the overall PAC-Bayes generalization bound over a family of multivariate Gaussian distributions (different choices of perturbations and priors).",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 29,
      "context" : "2), nor in the more refined bound used by Dziugaite and Roy [32], we prefer shying away from numerically optimizing the balance between sharpness and the KL-divergence.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "[13] where the perturbation for parameter wi has magnitude 5.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[27] where a fully connected feedforward network is trained on MNIST dataset with varying number of hidden units and we check the values of different complexity measures on each of the learned networks.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 58,
      "context" : "There is therefore also a strong link between regularization for optimization and regularization for learning: optimization may provide implicit regularization in terms of its corresponding geometry, and for ideal optimization performance the optimization geometry should be aligned with inductive bias driving the learning [61].",
      "startOffset" : 324,
      "endOffset" : 328
    }, {
      "referenceID" : 59,
      "context" : "Invariances have also been studied as different mappings from the parameter space to the same function space [62] while we define the transformation as a mapping inside a fixed parameter space.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 60,
      "context" : "A very important invariance in feedforward networks is node-wise rescaling [63].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 61,
      "context" : "We have discussed the complete characterize all feasible node-wise invariances of RNNs in [64].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 59,
      "context" : "Note that this is different than the invariances studied in [62], in that they study algorithms that are invariant to reparametrizations of the same model but we look at transformations within the the parameter space that preserve the function in the model.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "and φ(x)> = [ g1(x)x[1] g2(x)x[2] g3(x)x[1] g4(x)x[2] g5(x)x[1] g6(x)x[2] g7(x)x[1] g8(x)x[2] ] .",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "and φ(x)> = [ g1(x)x[1] g2(x)x[2] g3(x)x[1] g4(x)x[2] g5(x)x[1] g6(x)x[2] g7(x)x[1] g8(x)x[2] ] .",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "and φ(x)> = [ g1(x)x[1] g2(x)x[2] g3(x)x[1] g4(x)x[2] g5(x)x[1] g6(x)x[2] g7(x)x[1] g8(x)x[2] ] .",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "and φ(x)> = [ g1(x)x[1] g2(x)x[2] g3(x)x[1] g4(x)x[2] g5(x)x[1] g6(x)x[2] g7(x)x[1] g8(x)x[2] ] .",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "and φ(x)> = [ g1(x)x[1] g2(x)x[2] g3(x)x[1] g4(x)x[2] g5(x)x[1] g6(x)x[2] g7(x)x[1] g8(x)x[2] ] .",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "and φ(x)> = [ g1(x)x[1] g2(x)x[2] g3(x)x[1] g4(x)x[2] g5(x)x[1] g6(x)x[2] g7(x)x[1] g8(x)x[2] ] .",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "and φ(x)> = [ g1(x)x[1] g2(x)x[2] g3(x)x[1] g4(x)x[2] g5(x)x[1] g6(x)x[2] g7(x)x[1] g8(x)x[2] ] .",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "and φ(x)> = [ g1(x)x[1] g2(x)x[2] g3(x)x[1] g4(x)x[2] g5(x)x[1] g6(x)x[2] g7(x)x[1] g8(x)x[2] ] .",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 64,
      "context" : "Another form of regularization that is shown to be very effective in RELU networks is the max-norm regularization, which is the maximum over all units of norm of incoming edge to the unit2 [67, 68].",
      "startOffset" : 189,
      "endOffset" : 197
    }, {
      "referenceID" : 64,
      "context" : "In these cases, per-unit `2 regularization has shown to be very effective [68].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 62,
      "context" : "1The path-norm which we define is a norm on functions, not on weights, but as we prefer not getting into this technical discussion here, we use the term “norm” very loosely to indicate some measure of magnitude [65].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 63,
      "context" : "2This definition of max-norm is a bit different than the one used in the context of matrix factorization [66].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 62,
      "context" : "Surprisingly, for a feed-forward network, the minimum `2 per-unit regularizer among all rescaling equivalent networks can be calculated in close form and we call it the path-regularizer [65, 69].",
      "startOffset" : 186,
      "endOffset" : 194
    }, {
      "referenceID" : 65,
      "context" : "Surprisingly, for a feed-forward network, the minimum `2 per-unit regularizer among all rescaling equivalent networks can be calculated in close form and we call it the path-regularizer [65, 69].",
      "startOffset" : 186,
      "endOffset" : 194
    }, {
      "referenceID" : 65,
      "context" : "The stochastic version that uses a subset of training examples to estimate ∂L ∂wu→v (w ) is called PathSGD [69].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 66,
      "context" : "We conduct our experiments on four common benchmark datasets: the standard MNIST dataset of handwritten digits [70]; CIFAR-10 and CIFAR-100 datasets of tiny images of natural scenes [71]; and Street View House Numbers (SVHN) dataset containing color images of house numbers collected by Google Street View [72].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 67,
      "context" : "We conduct our experiments on four common benchmark datasets: the standard MNIST dataset of handwritten digits [70]; CIFAR-10 and CIFAR-100 datasets of tiny images of natural scenes [71]; and Street View House Numbers (SVHN) dataset containing color images of house numbers collected by Google Street View [72].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 68,
      "context" : "We conduct our experiments on four common benchmark datasets: the standard MNIST dataset of handwritten digits [70]; CIFAR-10 and CIFAR-100 datasets of tiny images of natural scenes [71]; and Street View House Numbers (SVHN) dataset containing color images of house numbers collected by Google Street View [72].",
      "startOffset" : 306,
      "endOffset" : 310
    }, {
      "referenceID" : 65,
      "context" : "Please see [69] for a more complete set of experimental results.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 69,
      "context" : "This view is similar to observations in [73] on the role of implicit regularization in deep learning.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 65,
      "context" : "This can be better analyzed by looking at the plots for more number of epochs which we have provided in [69].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 70,
      "context" : "To compare the two terms κ and κ, we train a single layer RNN with H = 200 hidden units for the task of word-level language modeling on Penn Treebank (PTB) Corpus [74].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 71,
      "context" : "Training Recurrent Neural Networks is known to be hard for modeling long-term dependencies due to the gradient vanishing/exploding problem [75, 76].",
      "startOffset" : 139,
      "endOffset" : 147
    }, {
      "referenceID" : 72,
      "context" : "Training Recurrent Neural Networks is known to be hard for modeling long-term dependencies due to the gradient vanishing/exploding problem [75, 76].",
      "startOffset" : 139,
      "endOffset" : 147
    }, {
      "referenceID" : 73,
      "context" : "Addition Problem: The addition problem was introduced in [77].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "Here, each input consists of two sequences of length T , one of which includes numbers sampled from the uniform distribution with range [0, 1] and the other sequence serves as a mask which is filled with zeros except for two entries.",
      "startOffset" : 136,
      "endOffset" : 142
    }, {
      "referenceID" : 74,
      "context" : "Sequential MNIST: In sequential MNIST, each digit image is reshaped into a sequence of length 784, turning the digit classification task into sequence classification with long-term dependencies [78, 79].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 75,
      "context" : "Sequential MNIST: In sequential MNIST, each digit image is reshaped into a sequence of length 784, turning the digit classification task into sequence classification with long-term dependencies [78, 79].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 74,
      "context" : "For both tasks, we closely follow the experimental protocol in [78].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 74,
      "context" : "Adding Adding Adding 100 400 750 sMNIST IRNN [78] 0 16.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 75,
      "context" : "0 uRNN [79] 0 3 16.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 75,
      "context" : "9 LSTM [79] 0 2 16.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 76,
      "context" : "8 np-RNN[80] 0 2 >2 3.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 77,
      "context" : "PTB text8 RNN+smoothReLU [81] - 1.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 78,
      "context" : "55 HF-MRNN [82] 1.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 79,
      "context" : "54 RNN-ReLU[83] 1.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 79,
      "context" : "65 RNN-tanh[83] 1.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 79,
      "context" : "55 TRec,β = 500[83] 1.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 74,
      "context" : "We also train an RNN of the same size with identity initialization, as was proposed in [78], using SGD as our baseline model, referred to as IRNN.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 75,
      "context" : "Similar to [79], we found the IRNN to be fairly unstable (with SGD optimization typically diverging).",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 74,
      "context" : "We also compare RNN-Path with the previously published results, including identity initialized RNN [78] (IRNN), unitary RNN [79] (uRNN), and np-RNN1 introduced by [80].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 75,
      "context" : "We also compare RNN-Path with the previously published results, including identity initialized RNN [78] (IRNN), unitary RNN [79] (uRNN), and np-RNN1 introduced by [80].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 76,
      "context" : "We also compare RNN-Path with the previously published results, including identity initialized RNN [78] (IRNN), unitary RNN [79] (uRNN), and np-RNN1 introduced by [80].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 79,
      "context" : "PTB-c: We performed experiments on a tokenized Penn Treebank Corpus, following the experimental protocol of [83].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 78,
      "context" : "We follow the data partition of [82], where each training sequence has a length of 180.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 80,
      "context" : "Instead, we use Adam optimizer [84] to help speed up the training, where we simply use the path-SGD gradient as input to the Adam optimizer.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 81,
      "context" : "For LSTMs, we use orthogonal initialization [85] for the recurrent matrices and uniform[−0.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 79,
      "context" : "We also compare our results to an RNN that uses hidden activation regularizer [83] (TRec,β = 500), Multiplicative RNNs trained by Hessian Free methods [82] (HF-MRNN), and an RNN with smooth version of ReLU [81].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 78,
      "context" : "We also compare our results to an RNN that uses hidden activation regularizer [83] (TRec,β = 500), Multiplicative RNNs trained by Hessian Free methods [82] (HF-MRNN), and an RNN with smooth version of ReLU [81].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 77,
      "context" : "We also compare our results to an RNN that uses hidden activation regularizer [83] (TRec,β = 500), Multiplicative RNNs trained by Hessian Free methods [82] (HF-MRNN), and an RNN with smooth version of ReLU [81].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 12,
      "context" : "In this chapter, we focus on two efficient alternative optimization approaches proposed recently for feed-forward neural networks that are based on intuitions about parametrization, normalization and the geometry of parameter space: Path-SGD [14] was derived as steepest descent algorithm with respect to particular regularizer (the `2-path regularizer, i.",
      "startOffset" : 242,
      "endOffset" : 246
    }, {
      "referenceID" : 55,
      "context" : "the sum over all paths in the network of the squared product over all weights in the path [58]) and is invariant to weight reparametrization.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 82,
      "context" : "Batch-normalization [86] was derived by adding normalization layers in the network as a way of controlling the variance of the input each unit receives in a data-dependent fashion.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 83,
      "context" : "Our unified framework and study of in invariances also allows us to relate the different optimization approaches to Natural Gradients [87].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 84,
      "context" : "Related Works There has been an ongoing effort for better understanding of the optimization in deep networks and several heuristics have been suggested to improve the training [88, 89, 90, 91].",
      "startOffset" : 176,
      "endOffset" : 192
    }, {
      "referenceID" : 85,
      "context" : "Related Works There has been an ongoing effort for better understanding of the optimization in deep networks and several heuristics have been suggested to improve the training [88, 89, 90, 91].",
      "startOffset" : 176,
      "endOffset" : 192
    }, {
      "referenceID" : 86,
      "context" : "Related Works There has been an ongoing effort for better understanding of the optimization in deep networks and several heuristics have been suggested to improve the training [88, 89, 90, 91].",
      "startOffset" : 176,
      "endOffset" : 192
    }, {
      "referenceID" : 87,
      "context" : "Related Works There has been an ongoing effort for better understanding of the optimization in deep networks and several heuristics have been suggested to improve the training [88, 89, 90, 91].",
      "startOffset" : 176,
      "endOffset" : 192
    }, {
      "referenceID" : 83,
      "context" : "Natural gradient algorithm [87] is known to have a very strong invariance property; it is not only invariant to reparametrization, but also to the choice of network architecture.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 88,
      "context" : "However it is known to be computationally demanding and thus many approximations have been proposed [92, 93, 94].",
      "startOffset" : 100,
      "endOffset" : 112
    }, {
      "referenceID" : 89,
      "context" : "However it is known to be computationally demanding and thus many approximations have been proposed [92, 93, 94].",
      "startOffset" : 100,
      "endOffset" : 112
    }, {
      "referenceID" : 90,
      "context" : "[95] also discuss the connections between Natural Gradients and some of the other proposed methods for training neural networks, namely Hessian-Free Optimization [96], Krylov Subspace Descent [97] and TONGA [98].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 91,
      "context" : "[95] also discuss the connections between Natural Gradients and some of the other proposed methods for training neural networks, namely Hessian-Free Optimization [96], Krylov Subspace Descent [97] and TONGA [98].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 92,
      "context" : "[95] also discuss the connections between Natural Gradients and some of the other proposed methods for training neural networks, namely Hessian-Free Optimization [96], Krylov Subspace Descent [97] and TONGA [98].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 93,
      "context" : "[95] also discuss the connections between Natural Gradients and some of the other proposed methods for training neural networks, namely Hessian-Free Optimization [96], Krylov Subspace Descent [97] and TONGA [98].",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 59,
      "context" : "[62] also recently studied the issue of invariance and proposed computationally efficient approximations and alternatives to natural gradient.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 59,
      "context" : "To alleviate this cost, [62] also proposed quasi-diagonal approximations which avoid the quadratic dependence but they are only invariant to affine transformations of activation functions.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 59,
      "context" : "In particular, ignoring the non-diagonal terms related to the biases in quasi-diagonal natural gradient suggested in [62], it is then equivalent to diagonal Natural Gradient which is itself equivalent to special case of DDP-SGD when Rv is the second moment (see Table 11.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 55,
      "context" : "For Rv = diag ( γ N in(v) ) , this complexity measure agrees with the `2-Path-regularizer as introduced by [58].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 55,
      "context" : "But, unlike this max-norm measure, the path-regularizer does not depend on the rebalancing and is invariant to node rescalings [58].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 82,
      "context" : "We show that with a choice of Rv = Cov ( hN in(v)) ) , this is essentially equivalent to Batch Normalization [86].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 82,
      "context" : "Batch-Normalization [86] was suggested as an alternate architecture, with special “normalization” layers, that ensure the variance of node outputs are normalized throughout training.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "For the choice Rv = diag(γ N in(v)), we have that γ 2 net is the Path-norm and we recover Path-SGD [14].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 94,
      "context" : "Using diagonal approximation of Fisher information matrix to normalize the gradient values has been suggested before as a computationally tractable alternative to the full Natural Gradient [99, 100].",
      "startOffset" : 189,
      "endOffset" : 198
    }, {
      "referenceID" : 95,
      "context" : "Using diagonal approximation of Fisher information matrix to normalize the gradient values has been suggested before as a computationally tractable alternative to the full Natural Gradient [99, 100].",
      "startOffset" : 189,
      "endOffset" : 198
    }, {
      "referenceID" : 59,
      "context" : "[62] also suggested a “quasi-diagonal\" approximations that includes, in addition to the diagonal, also some non-diagonal terms corresponding to the relationship between the bias term and every other incoming weight into a unit.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 83,
      "context" : "The natural gradient algorithm [87] achieves invariance by applying the inverse of the Fisher information matrix F (w) at the current parameter w to the negative gradient direction as follows: w = w + η∆,",
      "startOffset" : 31,
      "endOffset" : 35
    } ],
    "year" : 2017,
    "abstractText" : "In an attempt to better understand generalization in deep learning, we study several possible explanations. We show that implicit regularization induced by the optimization method is playing a key role in generalization and success of deep learning models. Motivated by this view, we study how different complexity measures can ensure generalization and explain how optimization algorithms can implicitly regularize complexity measures. We empirically investigate the ability of these measures to explain different observed phenomena in deep learning. We further study the invariances in neural networks, suggest complexity measures and optimization algorithms that have similar invariances to those in neural networks and evaluate them on a number of learning tasks. Thesis Advisor: Nathan Srebro Title: Professor",
    "creator" : "LaTeX with hyperref package"
  }
}