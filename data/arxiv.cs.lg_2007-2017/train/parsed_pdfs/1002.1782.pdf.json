{
  "name" : "1002.1782.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Distributed Sensor Selection",
    "authors" : [ "Daniel Golovin", "Matthew Faulkner", "Andreas Krause" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Categories and Subject Descriptors C.2.1 [Computer-Communication Networks]: Network Architecture and Design; G.3 [Probability and Statistics]: Experimental Design; I.2.6 [AI]: Learning\nGeneral Terms Algorithms, Measurement\nKeywords Sensor networks, approximation algorithms, distributed multiarmed bandit algorithms, submodular optimization"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "A key challenge in deploying sensor networks for realworld applications such as environmental monitoring [19], building automation [25] and others is to decide when to activate the sensors in order to obtain the most useful information from the network (e.g., accurate predictions at unobserved\nlocations) and to minimize power consumption. This sensor selection problem has received considerable attention [1, 32, 10], and algorithms with performance guarantees have been developed [1, 16]. However, many of the existing approaches make simplifying assumptions. Many approaches assume (1) that the sensors can perfectly observe a particular sensing region, and nothing outside the region [1]. This assumption does not allow us to model settings where multiple noisy sensors can help each other obtain better predictions. There are also approaches that base their notion of utility on more detailed models, such as improvement in prediction accuracy w.r.t. some statistical model [10] or detection performance [18]. However, most of these approaches make two crucial assumptions: (2) The model, upon which the optimization is based, is known in advance (e.g., based on domain knowledge or data from a pilot deployment) and (3), a centralized optimization selects the sensors (i.e., some centralized processor selects the sensors which obtain highest utility w.r.t. the model). We are not aware of any approach that simultaneously addresses the three main challenges (1), (2) and (3) above and still provides theoretical guarantees.\nIn this paper, we develop an efficient algorithm, called Distributed Online Greedy (DOG), which addresses these three central challenges. Prior work [17] has shown that many sensing tasks satisfy an intuitive diminishing returns property, submodularity, which states that activating a new sensor helps more if few sensors have been activated so far, and less if many sensors have already been activated. Our algorithm applies to any setting where the true objective is submodular [23], thus capturing a variety of realistic sensor models. Secondly, our algorithm does not require the model to be specified in advance: it learns to optimize the objective function in an online manner. Lastly, the algorithm is distributed; the sensors decide whether to activate themselves based on local information. We analyze our algorithm in the no-regret model, proving convergence properties similar to the best bounds for any centralized solution.\nA bandit approach toward sensor selection. At the heart of our approach is a novel distributed algorithm for multi-\nar X\niv :1\n00 2.\n17 82\nv3 [\ncs .L\nG ]\n1 3\nM ay\n2 01\narmed bandit (MAB) problems. In the classical multiarmed bandit [24] setting, we picture a slot machine with multiple arms, where each arm generates a random payoff with unknown mean. Our goal is to devise a strategy for pulling arms to maximize the total reward accrued. The difference between the optimal arm’s payoff and the obtained payoff is called the regret. Known algorithms can achieve average per-round regret of O(√n log n/ √ T ) where n is the number of arms, and T the number of rounds (see e.g. the survey of [13]). Suppose we would like to, at every time step, select k sensors. The sensor selection problem can then be cast as a multiarmed bandit problem, where there is one arm for each possible set of k sensors, and the payoff is the accrued utility for the selected set. Since the number of possible sets, and thus the number of arms, is exponentially large, the resulting regret bound is O(nk/2√log n/ √ T ), i.e., exponential in k. However, when the utility function is submodular, the payoffs of these arms are correlated. Recent results [28] show that this correlation due to submodularity can be exploited by reducing the nk-armed bandit problem to k separate n-armed bandit problems, with only a bounded loss in performance. Existing bandit algorithms, such as the widely used EXP3 algorithm [2], are centralized in nature. Consequently, the key challenge in distributed online submodular sensing is how to devise a distributed bandit algorithm. In Sec. 4 and 5, we develop a distributed variant of EXP3 using novel algorithms to sample from and update a probability distribution in a distributed way. Roughly, we develop a scheme where each sensor maintains its own weight, and activates itself independently from all other sensors purely depending on this weight.\nObservation specific selection. A shortcoming of centralized sensor selection is that the individual sensors’ current measurements are not considered in the selection process. In many applications, obtaining sensor measurements is less costly than transmitting the measurements across the network. For example, cell phones used in participatory sensing [5] can inexpensively obtain measurements on a regular basis, but it is expensive to constantly communicate measurements over the network. In Sec. 6, we extend our distributed selection algorithm to activate sensors depending on their observations, and analyze the tradeoff between power consumption and the utility obtained under observation specific activation.\nCommunication models. We analyze our algorithms under two models of communication cost: In the broadcast model, each sensor can broadcast a message to all other sensors at unit cost. In the star network model, messages can only be between a sensor and the base station, and each message has unit cost. In Sec. 4 we formulate and analyze a distributed algorithm for sensor selection under the simpler broadcast model. Then, in Sec. 5 we show how the algorithm can be\nextended to the star network model.\nOur main contributions. • Distributed EXP3, a novel distributed implementation\nof the classic multiarmed bandit algorithm. • Distributed Online Greedy (DOG) and LAZYDOG,\nnovel algorithms for distributed online sensor selection, which apply to many settings, only requiring the utility function to be submodular. • OD-DOG, an extension of DOG to allow for observation-\ndependent selection. • We analyze our algorithm in the no-regret model and\nprove that it attains the optimal regret bounds attainable by any efficient centralized algorithm. • We evaluate our approach on several real-world sensing\ntasks including monitoring a 12,527 node network.\nFinally, while we do not consider multi-hop or general network topologies in this paper, we believe that the ideas behind our algorithms will likely prove valuable for sensor selection in those models as well."
    }, {
      "heading" : "2. THE SENSOR SELECTION PROBLEM",
      "text" : "We now formalize the sensor selection problem. Suppose a network of sensors has been deployed at a set of locations V with the task of monitoring some phenomenon (e.g., temperature in a building). Constraints on communication bandwidth or battery power typically require us to select a subset A of these sensors for activation, according to some utility function. The activated sensors then send their data to a server (base station). We first review the traditional offline setting where the utility function is specified in advance, illustrating how submodularity allows us to obtain provably near-optimal selections. We then address the more challenging setting where the utility function must be learned from data in an online manner."
    }, {
      "heading" : "2.1 The Offline Sensor Selection Problem",
      "text" : "A standard offline sensor selection algorithm chooses a set of sensors that maximizes a known sensing quality objective function f(A), subject to some constraints, e.g., on the number of activated sensors. One possible choice for the sensing quality is based on prediction accuracy (we will discuss other possible choices later on). In many applications, measurements are correlated across space, which allows us to make predictions at the unobserved locations. For example, prior work [10] has considered the setting where a random variable Xs is associated with every location s ∈ V , and a joint probability distribution P (XV ) models the correlation between sensor values. Here, XV = [X1, . . . ,Xn] is the random vector over all measurements. If some measurements XA = xA are obtained at a subset of locations, then\nthe conditional distribution P (XV \\A | XA = xA) allows predictions at the unobserved locations, e.g., by predicting E[XV \\A | XA = xA]. Furthermore, this conditional distribution quantifies the uncertainty in the prediction: Intuitively, we would like to select sensors that minimize the predictive uncertainty. One way to quantify the predictive uncertainty is the mean squared prediction error,\nMSE(XV \\A | xA) = 1\nn\n∑\ns∈V \\A E[(Xs−E[Xs | xA])2 | xA].\nIn general, the measurements xA that sensors A will make is not known in advance. Thus, we can base our optimization on the expected mean squared prediction error,\nEMSE(A) = ∫ dp(xA) MSE(XV \\A | xA).\nEquivalently, we can maximize the reduction in mean squared prediction error,\nfEMSE(A) = EMSE(∅)− EMSE(A).\nBy definition, fEMSE(∅) = 0, i.e., no sensors obtain no utility. Furthermore, fEMSE is monotonic: if A ⊆ B ⊆ V , then fEMSE(A) ≤ fEMSE(B), i.e., adding more sensors always helps. That means, fEMSE is maximized by the set of all sensors V . However, in practice, we would like to only select a small set of, e.g., at most k sensors due to bandwidth and power constraints:\nA∗ = arg max A fEMSE(A) s.t. |A| ≤ k.\nUnfortunately, this optimization problem is NP-hard, so we cannot expect to efficiently find the optimal solution. Fortunately, it can be shown [9] that in many settings1, the function fEMSE satisfies an intuitive diminishing returns property called submodularity. A set function f : 2V → R is called submodular if, for all A ⊆ B ⊆ V and s ∈ V \\ B it holds that f(A∪ {s})− f(A) ≥ f(B ∪ {s})− f(B). Many other natural objective functions for sensor selection satisfy submodularity as well [17]. For example, the sensing region model where fREG(A) is the total area covered by all sensors A is submodular. The detection model where fDET (A) counts the expected number of targets detected by sensors A is submodular as well.\nA fundamental result of Nemhauser et al. [23] is that for monotone submodular functions, a simple greedy algorithm, which starts with the empty set A0 = ∅ and iteratively adds the element\nsk = arg max s∈V \\Ak−1\nf(Ak−1 ∪ {s}); Ak = Ak−1 ∪ {sk}\nwhich maximally improves the utility obtains a near-optimal\n1For Gaussian models and conditional suppressorfreeness [9]\nsolution: For the set Ak it holds that f(Ak) ≥ (1− 1/e) max|A|≤k f(A),\ni.e., the greedy solution obtains at least a constant fraction of (1− 1/e) ≈ 63% of the optimal value.\nOne fundamental problem with this offline approach is that it requires the function f to be specified in advance, i.e., before running the greedy algorithm. For the function fEMSE, this means that the probabilistic model P (XV ) needs to be known in advance. While for some applications some prior data, e.g., from pilot deployments, may be accessible, very often no such prior data is available. This leads to a “chickenand-egg” problem, where sensors need to be activated to collect data in order to learn a model, but also the model is required to inform the sensor selection. This is akin to the “exploration–exploitation tradeoff” in reinforcement learning [2], where an agent needs to decide whether to explore and gather information about effectiveness of an action, or to exploit, i.e., choose actions known to be effective. In the following, we devise an online monitoring scheme based on this analogy."
    }, {
      "heading" : "2.2 The Online Sensor Selection Problem",
      "text" : "We now consider the more challenging problem where the objective function is not specified in advance, and needs to be learned during the monitoring task. We assume that we intend to monitor the environment for a number T of time steps (rounds). In each round t, a set St of sensors is selected, and these sensors transmit their measurements to a server (base station). The server then determines a sensing quality ft(St) quantifying the utility obtained from the resulting analysis. For example, if our goal is spatial prediction, the server would build a model based on the previously collected sensor data, pick a random sensor s, make prediction for the variable Xs, and then compare the prediction µs with the sensor reading xs. The error ft = σ2s−(µs−xs)2 is an unbiased estimate of the reduction in EMSE. In the following analysis, we will only assume that the objective functions ft are bounded (w.l.o.g., take values in [0, 1]), monotone, and submodular, and that we have some way of computing ft(S) for any subset of sensors S. Our goal is to maximize the total reward obtained by the system over T rounds, ∑T t=1 ft(St).\nWe seek to develop a protocol for selecting the sets St of sensors at each round, such that after a small number of rounds the average performance of our online algorithm converges to the same performance of the offline strategy (that knows the objective functions). We thus compare our protocol against all strategies that can select a fixed set of k sensors for use in all of the rounds; the best such strategy obtains reward maxS⊆V :|S|≤k ∑T t=1 ft(S). The difference between this quantity and what our protocol obtains is known as its regret, and an algorithm is said to be no-regret if its\naverage regret tends to zero (or less)2 as T →∞. When k = 1, our problem is simply the well-studied multiarmed bandit (MAB) problem, for which many no-regret algorithms are known [13]. For general k, because the average of several submodular functions remains submodular, we can apply the result of Nemhauser et al. [23] (cf., Sec. 2.1) to prove that a simple greedy algorithm obtains a (1− 1/e) approximation to the optimal offline solution. Feige [12] showed that this is optimal in the sense that obtaining a (1− 1/e+ ) approximation for any > 0 is NP-hard. These facts suggest that we cannot expect any efficient online algorithm to converge to a solution better than (1 − 1/e) maxS⊆V :|S|≤k ∑T t=1 ft(S). We therefore define the (1− 1/e)-regret of a sequence of (possibly random) sets {St}Tt=1 as\nRT := (1− 1/e) · max S⊆V :|S|≤k\nT∑\nt=1\nft(S) − T∑\nt=1\nE [ft(St)]\nwhere the expectation is taken over the distribution for each St. We say an online algorithm producing a sequence of sets has no-(1− 1/e)-regret if lim supT→∞ RTT ≤ 0."
    }, {
      "heading" : "3. CENTRALIZED ALGORITHM FOR ONLINE SENSOR SELECTION",
      "text" : "Before developing the distributed algorithm for online sensor selection, we will first review a centralized algorithm which is guaranteed to achieve no (1 − 1/e)-regret. In Sec. 4 we will show how this centralized algorithm can be implemented efficiently in a distributed manner. This algorithm starts with the greedy algorithm for a known submodular function mentioned in Sec. 2.1, and adapts it to the online setting. Doing so requires an online algorithm for selecting a single sensor as a subroutine, and we review such an algorithm in Sec. 3.1 before discussing the centralized algorithm for selecting multiple sensors in Sec. 3.2."
    }, {
      "heading" : "3.1 Centralized Online Single Sensor Selection",
      "text" : "Let us first consider the case where k = 1, i.e., we would like to select one sensor at each round. This simpler problem can be interpreted as an instance of the multiarmed bandit problem (as introduced in Sec. 2.2), where we have one arm for each possible sensor. In this case, the EXP3 algorithm [2] is a centralized solution for no-regret single sensor selection. EXP3 works as follows: It is parameterized by a learning rate η, and an exploration probability γ. It maintains a set of weights ws, one for each arm (sensor) s, initialized to 1. At every round t, it will select each arm s with probability\nps = (1− γ) ws∑ s′ ws′ + γ n ,\n2Formally, if RT is the total regret for the first T rounds, no-regret means lim supT→∞ RT /T ≤ 0.\ni.e., with probability γ it explores, picking an arm uniformly at random, and with probability (1−γ) it exploits, picking an arm s with probability proportional to its weight ws. Once an arm s has been selected, a feedback r = ft({s}) is obtained, and the weight ws is updated to\nws ← ws exp(ηr/ps). Auer et al. [2] showed that with appropriately chosen learning rate η and exploration probability γ it holds that the cumulative regret RT of EXP3 is O( √ Tn lnn), i.e., the average regret RT /T converges to zero."
    }, {
      "heading" : "3.2 Centralized Selection of Multiple Sensors",
      "text" : "In principle, we could interpret the sensor selection problem as a ( n k ) -armed bandit problem, and apply existing no-regret algorithms such as EXP3. Unfortunately, this approach does not scale, since the number of arms grows exponentially with k. However, in contrast to the traditional multiarmed bandit problem, where the arms are assumed to have independent payoffs, in the sensor selection case, the utility function is submodular and thus the payoffs are correlated across different sets. Recently, Streeter and Golovin showed how this submodularity can be exploited, and developed a no-(1−1/e)regret algorithm for online maximization of submodular functions [28]. The key idea behind their algorithm, OGunit, is to turn the offline greedy algorithm into an online algorithm by replacing the greedy selection of the element sk that maximizes the benefit sk = arg maxs f({s1, ..., sk−1} ∪ {s}) by a bandit algorithm. As shown in the pseudocode below, OGUNIT maintains k bandit algorithms, one for each sensor to be selected. At each round t, it selects k sensors according to the choices of the k bandit algorithms Ei 3. Once the elements have been selected, the ith bandit algorithm Ei receives as feedback the incremental benefit ft(s1, . . . , si)− ft(s1, . . . , si−1), i.e., how much additional utility is obtained by adding sensor si to the set of already selected sensors. Below we define [m] := {1, 2, . . . ,m}.\nAlgorithm OGUNIT from [28]: Initialize k multiarmed bandit algorithms E1, E2, . . . , Ek, each with action set V . For each round t ∈ [T ]\nFor each stage i ∈ [k] in parallel Ei selects an action vti For each i ∈ [k] in parallel feedback ft( { vtj : j ≤ i } )− ft( { vtj : j < i } ) to Ei. Output St = {at1, at2, . . . , atk}.\nIn [27] it is shown that OGUNIT has a ( 1− 1e ) -regret bound of O(kR) in this feedback model assuming each Ei has expected regret at most R. Thus, when using EXP3 as a subroutine, OGUNIT has no-(1− 1/e)-regret. 3Bandits with duplicate choices are handled in Sec. 4.6.1 of [28]\nUnfortunately, EXP3 (and in fact all MAB algorithms with no-regret guarantees for non-stochastic reward functions) require sampling from some distribution with weights associated with the sensors. If n is small, we could simply store these weights on the server, and run the bandit algorithms Ei there. However, this solution does not scale to large numbers of sensors. Thus the key problem for online sensor selection is to develop a multiarmed bandit algorithm which implements distributed sampling across the network, with minimal overhead of communication. In addition, the algorithm needs to be able to maintain the distributions (the weights) associated with each Ei in a distributed fashion."
    }, {
      "heading" : "4. DISTRIBUTED ALGORITHM FOR ONLINE SENSOR SELECTION",
      "text" : "We will now develop DOG, an efficient algorithm for distributed online sensor selection. For now we make the following assumptions:\n1. Each sensor v ∈ V is able to compute its contribution to the utility ft(S ∪ {v})− ft(S), where S are a subset of sensors that have already been selected.\n2. Each sensor can broadcast to all other sensors. 3. The sensors have calibrated clocks and unique, linearly\nordered identifiers.\nThese assumptions are reasonable in many applications: (1) In target detection, for example, the objective function ft(S) counts the number of targets detected by the sensors S. Once previously selected sensors have broadcasted which targets they detected, the new sensor s can determine how many additional targets have been detected. Similarly, in statistical estimation, one sensor (or a small number of sensors) randomly activates each round and broadcasts its value. After sensors S have been selected and announced their measurements, the new sensor s can then compute the improvement in prediction accuracy over the previously collected data. (2) The assumption that broadcasts are possible may be realistic for dense deployments and fairly long range transmissions. In Sec. 5 we will show how assumptions (1) and (2) can be relaxed.\nAs we have seen in Sec. 3, the key insight in developing a centralized algorithm for online selection is to replace the greedy selection of the sensor which maximally improves the total utility over the set of previously selected sensors by a bandit algorithm. Thus, a natural approach for developing a distributed algorithm for sensor selection is to first consider the single sensor case."
    }, {
      "heading" : "4.1 Distributed Selection of a Single Sensor",
      "text" : "The key challenge in developing a distributed version of EXP3 is to find a way to sample exactly one element from a\nprobability distribution p over sensors in a distributed manner. This problem is distinct from randomized leader election [22], where the objective is to select exactly one element but the element need not be drawn from a specified distribution. We note that under the multi-hop communication model, sampling one element from the uniform distribution given a rooted spanning tree can be done via a simple random walk [20], but that under the broadcast and star network models this approach degenerates to centralized sampling. Our algorithm, in contrast, samples from an arbitrary distribution by allowing sensors to individually decide to activate. Our bottom-up approach also has two other advantages: (1) it is amenable to modification of the activation probabilities based on local observations, as we discuss in Sec. 6, and (2) since it does not rely on any global state of the network such as a spanning tree, it can gracefully cope with significant edge or node failures.\nA naive distributed sampling scheme. A naive distributed algorithm would be to let each sensor keep track of all activation probabilities p. Then, one sensor (e.g., with the lowest identifier) would broadcast a single random number u uniformly distributed in [0, 1], and the sensor v for which∑v−1 i=1 pi ≤ u < ∑v i=1 pi would activate. However, for large sensor network deployments, this algorithm would require each sensor to store a large amount of global information (all activation probabilities p). Instead, each sensor v could store only their own probability mass pv; the sensors would then, in order of their identifiers, broadcast their probabilities pv, and stop once the sum of the probabilities exceeds u. This approach only requires a constant amount of local information, but requires an impractical Θ(n) messages to be sent, and sent sequentially over Θ(n) time steps.\nDistributed multinomial sampling. In this section we present a protocol that requires only O(1) messages in expectation, and only a constant amount of local information.\nFor a sampling procedure with input distribution p, we let p̂ denote the resulting distribution, where in all cases at most one sensor is selected, and nothing is selected with probability 1−∑v p̂v . A simple approach towards distributed sampling would be to activate each sensor v ∈ V independently from each other with probability pv . While in expectation, exactly one sensor is activated, with probability ∏ v(1 − pv) > 0 no sensor is activated; also since sensors are activated independently, there is a nonzero probability that more than one sensor is activated. Using a synchronized clock, the sensors could determine if no sensor is activated. In this case, they could simply repeat the selection procedure until at least one sensor is activated. One naive approach would be to repeat the selection procedure until exactly one sensor is activated. However with two sensors and p1 = ε, p2 = 1− ε this algorithm yields p̂1 = ε2/(1 − 2ε + 2ε2) = O(ε2), so the first\nsensor is severely underrepresented. Another simple protocol would be to select exactly one sensor uniformly at random from the set of activated sensors, which can be implemented using few messages.\nThe Simple Protocol: For each sensor v in parallel\nSample Xv ∼ Bernoulli(pv). If (Xv = 1), Xv activates.\nAll active sensors S coordinate to select a single sensor uniformly at random from S, e.g., by electing the minimum ID sensor in S to do the sampling.\nIt is not hard to show that with this protocol, for all sensors v,\np̂v = pv · E [ 1\n|S|\n∣∣∣∣ v ∈ S ] ≥ pv/E [|S| | v ∈ S] ≥ pv/2\nby appealing to Jensen’s inequality. Since p̂v ≤ pv, we find that this simple protocol maintains a ratio rv := p̂v/pv ∈ [ 12 , 1]. Unfortunately, this analysis is tight, as can be seen from the example with two sensors and p1 = ε, p2 = 1− ε.\nTo improve upon the simple protocol, first consider running it on an example with p1 = p2 = · · · = pn = 1/n. Since the protocol behaves exactly the same under permutations of sensor labels, by symmetry we have p̂1 = p̂2 = · · · = p̂n, and thus ri = rj for all i, j. Now consider an input distribution p where there exists integers N and k1, k2, . . . , kn such that pv = kv/N for all v. Replace each v with kv fictitious sensors, each with probability mass 1/N , and each with a label indicating v. Run the simple protocol with the fictitious sensors, selecting a fictitious sensor v′, and then actually select the sensor indicated by the label of v′. By symmetry this process selects each fictitious sensor with probability (1 − β)/N , where β is the probability that nothing at all is selected, and thus the process selects sensor v with probability kv(1−β)/N = (1−β)pv (since at most one fictitious sensor is ever selected).\nWe may thus consider the following improved protocol which incorporates the above idea, simulating this modification to the protocol exactly when pv = kv/N for all v.\nThe Improved Protocol(N ): For each sensor v in parallel\nSample Xv ∼ Binomial(dN · pve , 1/N). If (Xv ≥ 1), then activate sensor v.\nFrom the active sensors S, select sensor v with probability Xv/ ∑ v′∈S Xv′ .\nThis protocol ensures the ratios rv := p̂v/pv are the same for all sensors, provided each pv is a multiple of 1/N . Assuming the probabilities are rational, there will be a sufficiently large N to satisfy this condition. To reduce β := Pr [S = ∅] in the simple protocol, we may sample each Xv from Bernoulli(α · pv) for any α ∈ [1, n]. The symmetry\nargument remains unchanged. This in turn suggests sampling Xv from Binomial(dN · pve , α/N) in the improved protocol. Taking the limit as N → ∞, the binomial distribution becomes Poisson, and we obtain the desired protocol.\nThe Poisson Multinomial Sampling (PMS) Protocol(α): Same as the improved protocol, except each sensor v samples Xv ∼ Poisson(αpv)\nStraight-forward calculation shows that Pr [S = ∅] = ∏\nv\nexp {−α · pv} = exp { − ∑\nv\nα·pv } = e−α\nLet C be the number of messages. Then E [C] = ∑\nv\nPr [Xv ≥ 1] = ∑\nv\n(1−e−αpv ) ≤ ∑\nv\nαpv = α\nHere we have used linearity of expectation, and 1 + x ≤ ex for all x ∈ R. In summary, we have the following result about our protocol:\nPROPOSITION 1. Fix any fixed p and α > 0. The PMS Protocol always selects at most one sensor, ensures\n∀v : Pr [v selected] = (1− e−α)pv and requires no more than α messages in expectation.\nIn order to ensure that exactly one sensor is selected, whenever S = ∅ we can simply rerun the protocol with fresh random seeds as many times as needed until S is non-empty. Using α = 1, this modification will require only O(1) messages in expectation and at most O(log n) messages with high probability in the broadcast model. We can combine this protocol with EXP3 to get the following result.\nTHEOREM 2. In the broadcast model, running EXP3 using the PMS Protocol with α = 1, and rerunning the protocol whenever nothing is selected, yields exactly the same regret bound as standard EXP3, and in each round at most e/(e−1)+2 ≈ 3.582 messages are broadcast in expectation.\nThe regret bound for EXP3 is O( √\nOPTn log n), where OPT is the total reward of the best action. Our variant simulates EXP3, and thus has identical regret. Proofs of our theoretical results can be found in the Appendix.\nRemark. Running our variant of EXP3 requires that each sensor know the number of sensors, n, in order to compute its activation probability. If each sensor v has only a reasonable estimate of nv of n, however, our algorithm still performs well. For example, it is possible to prove that if all of the sensors have the same estimate nv = cn for some constant c > 0, then the upper bound on expected regret, R(c), grows as R(c) ≈ R(1) · max {c, 1/c}. The expected number of activations in this case increases by at most ( 1 c − 1 ) γ. In\ngeneral underestimating n leads to more activations, and underestimating or overestimating n can lead to more regret. This graceful degradation of performance with respect to the error in estimating n holds for all of our algorithms."
    }, {
      "heading" : "4.2 The Distributed Online Greedy Algorithm",
      "text" : "We now use our single sensor selection algorithm to develop our main algorithm, the Distributed Online Greedy algorithm (DOG). It is based on the distributed implementation of EXP3 using the PMS Protocol. Suppose we would like to select k sensors at each round t. Each sensor v maintains k weights wv,1, . . . , wv,k and normalizing constants Zv,1, . . . , Zv,k. The algorithm proceeds in k stages, synchronized using the common clock. In stage i, a single sensor is selected using the PMS Protocol applied to the distribution (1−γ)wv,i/Zv,i+γ/n. Suppose sensors S = {v1, . . . , vi−1} have been selected in stages 1 through i − 1. The sensor v selected at stage i then computes its local rewards πv,i using the utility function ft(S ∪ {vi}) − ft(S). It then computes its new weight\nw′v,i = wv,i exp(ηπv,i/pv,i),\nand broadcasts the difference between its new and old weights ∆v,i = w ′ v,i−wv,i. All sensors then update their ith normalizers using Zv,i ← Zv,i+∆v,i. Fig. 1 presents the pseudo-code of the DOG algorithm. Thus given Theorem 12 of [27] we have the following result about the DOG algorithm:\nTHEOREM 3. The DOG algorithm selects, at each round t a set St ⊆ V of k sensors such that\n1 T E\n[ T∑\nt=1\nft(St)\n] ≥ 1− 1 e\nT max |S|≤k\nT∑\nt=1\nft(S)−O ( k √ n log n\nT\n) .\nIn expectation, only O(k) messages are exchanged each round."
    }, {
      "heading" : "5. THE STAR NETWORK MODEL",
      "text" : "In some applications, the assumption that sensors can broadcast messages to all sensors may be unrealistic. Furthermore, in some applications sensors may not be able to compute the marginal benefits ft(S ∪ {s})− ft(S) (since this calculation may be computationally complex). In this section, we analyze LAZYDOG, a variant of our DOG algorithm, which replace the above assumptions by the assumption that there is a dedicated base station4 available which computes utilities and which can send non-broadcast messages to individual sensors.\nWe make the following assumptions: 4Though the existence of such a base station means the protocol is not completely distributed, it is realistic in sensor network applications where the sensor data needs to be accumulated somewhere for analysis.\n1. Every sensor stores its probability mass pv with it, and can only send messages to and receive messages from the base station.\n2. The base station is able, after receiving messages from a set S of sensors, to compute the utility ft(S) and send this utility back to the active sensors.\nThese conditions arise, for example, when cell phones in participatory sensor networks can contact the base station, but due to privacy constraints cannot directly call other phones. We do not assume that the base station has access to all weights of the sensors – we will only require the base station to have O(k + log n) memory. In the fully distributed algorithm DOG that relies on broadcasts, it is easy for the sensors to maintain their normalizers Zv,i, since they receive information about rewards from all selected sensors. The key challenge when removing the broadcast assumption is to maintain the normalizers in an appropriate manner."
    }, {
      "heading" : "5.1 Lazy renormalization & Distributed EXP3",
      "text" : "EXP3 (and all MAB with no-regret guarantees against arbitrary reward functions) must maintain a distribution over actions, and update this distribution in response to feedback about the environment. In EXP3, each sensor v requires only wv(t) and a normalizer Z(t) := ∑ v′ wv′(t) to compute pv(t) 5. The former changes only when v is selected. In the broadcast model the latter can simply be broadcast at the end of each round. In the star network model (or, more generally in multi-hop models), standard flooding echo aggregation techniques could be used to compute and distribute the new normalizer, though with high communication cost. We show that a lazy renormalization scheme can significantly reduce the amount of communication needed by a distributed bandit algorithm without altering its regret bounds whatsoever. Thus our lazy scheme is complementary to standard aggregation techniques.\nOur lazy renormalization scheme for EXP3 works as follows. Each sensor v maintains its weight wv(t) and an estimate Zv(t) for Z(t) := ∑ v′ wv′(t), Initially, wv(0) = 1 and Zv(0) = n for all v. The central server stores Z(t). Let\nρ(x, y) := (1− γ)x y + γ n .\nEach sensor then proceeds to activate as in the sampling procedure of Sec. 4.1 as if its probability mass in round t were qv = ρ(wv(t), Zv(t)) instead of its true value of ρ(wv(t), Z(t)). A single sensor is selected by the server with respect to the true value Z(t), resulting in a selection from the desired distribution. Moreover, v’s estimate Zv(t) is only updated on rounds when it communicates with the 5We let x(t) denote the value of variable x at the start of round t, to ease analysis. We do not actually need to store the historical values of the variables over multiple time steps.\nserver under these circumstances. This allows the estimated probabilities of all of the sensors to sum to more than one, but has the benefit of significantly reducing the communication cost in the star network model under certain assumptions. We call the result Distributed EXP3, give its pseudocode for round t in Fig. 2.\nSince the sensors underestimate their normalizers, they may activate more frequently than in the broadcast model. Fortunately, the amount of “overactivation” remains bounded. We prove Theorem 4 and Corollary 5 in Appendix B.\nTHEOREM 4. The number of sensor activations in any round of the Distributed EXP3 algorithm is at mostα+ (e− 1) in expectation and O(α+ log n) with high probability, and the number of messages is at most twice the number of activations.\nUnfortunately, there is still an e−α probability of nothing being selected. To address this, we can set α = c lnn for some c ≥ 1, and if nothing is selected, transmit a message to each of the n sensors to rerun the protocol.\nCOROLLARY 5. There is a distributed implementation of EXP3 that always selects a sensor in each round, has the same regret bounds as standard EXP3, ensures that the number of sensor activations in any round is at most lnn+O(1) in expectation orO(log n) with high probability, and in which the number of messages is at most twice the number of activations."
    }, {
      "heading" : "5.2 LazyDOG",
      "text" : "Once we have the distributed EXP3 variant described above, we can use it for the bandit subroutines in the OGUNIT algorithm (cf. Sec. 3.2). We call the result the LAZYDOG algorithm, due to its use of lazy renormalization. The lazy distributed EXP3 still samples sensors from the same distribution as the regular distributed EXP3, so LAZYDOG has precisely the same performance guarantees with respect to∑ t ft(St) as DOG. It works in the star network communication model, and requires few messages or sensor activations. Corollary 5 immediately implies the following result.\nCOROLLARY 6. The number of sensors that activate each round in LAZYDOG is at most k lnn+O(k) in expectation andO(k log n) with high probability, the number of messages is at most twice the number of activations, and the (1− 1/e)regret of LAZYDOG is the same as DOG.\nIf we are not concerned about the exact number of sensors selected in each round, but only want to ensure roughly k sensors are picked in expectation, then we can reduce the number of sensor activations and messages to O(k), by running LAZYDOG with k′ := dk/(1− e−α)e stages for some constant α, and allowing each stage to run the Poisson Multinomial Sampling Protocol with lazy renormalization without rerunning it if nothing is selected. This is of course optimal up to constants, as we must send at least one message per selected sensor.\nTHEOREM 7. The variant of LAZYDOG that runs the Poisson Multinomial Sampling Protocol (α) with lazy renormalization for k′ := dk/(1− e−α)e stages, but does not rerun it if nothing is selected in a given stage, has the following guarantees: (1) the number of sensors that activate each round in LAZYDOG is at most k′(α+ e− 1) in expectation and O(αk log n) with high probability, (2) the number of messages is at most twice the number of activations, (3) the expected number of sensors selected in each round is at most k′ and (4) its (1− 1/e)-regret is at most k′/k times that of DOG.\nWe defer the proof to Appendix C."
    }, {
      "heading" : "6. OBSERVATION-DEPENDENT SAMPLING",
      "text" : "Theorem 3 states that DOG is guaranteed to do nearly as well as the offline greedy algorithm run on an instance with objective function fΣ := ∑ t ft. Thus the reward of DOG is asymptotically near-optimal on average. In many applications, however, we would like to perform well on rounds with “atypical” objective functions. For example, in an outbreak detection application as we discuss in Sec. 7, we would like to get very good data on rounds with significant events, even if the nearest sensors typically report “boring” readings that contribute very little to the objective function.\nFor now, suppose that we are only running a single MAB instance to select a single sensor in each round. If we have access to a black-box for evaluating ft on round t, then we can perform well on atypical rounds at the cost of some additional communication by having each sensor v take a local reading of its environment and estimate its payoff π̄ = ft({v}) if selected. This value, which serves as a measure of how interesting its input is, can then be used to decide whether to boost v’s probability for reporting its sensor reading to the server. In the simplest case, we can imagine that each v has a threshold τv such that v activates with probability 1 if π̄ ≥ τv , and with its normal probability otherwise. In the case where we select k > 1 sensors in each round, each sensor can have a threshold for each of the k stages, where in each stage it computes π̄ = ft(S ∪ {v}) − ft(S) where S is the set of currently selected sensors. Since the activation probability only goes up, we can retain the performance guarantees of DOG if we are careful to adjust the feedback properly.\nIdeally, we wish that the sensors learn what their thresholds τv should be. We treat the selection of τv in each round as an online decision problem that each v must play. We construct a particular game that the sensors play, where the strategies are the thresholds (suitably discretized), there is an activation cost cv that v pays if π̄v ≥ τv , and the payoffs are defined as follows: Let πv = ft(S ∪ {v}) − ft(S) be the marginal benefit of selecting v given that sensor set S has already been selected. LetA be the set of sensors that activate in the current iteration of the game, and let max ( π(A\\v) ) := max (πv′ : v ′ ∈ A \\ {v}). The particular reward function ψv we choose for each sensor v for each iteration of the game is\nψv(τ) =\n{ cv −max ( πv −max ( π(A\\v) ) , 0 )\nif π̄ < τ max ( πv −max ( π(A\\v) ) , 0 ) − cv if π̄ ≥ τ\nbased on empirical performance. Thus, if a sensor activates (π̄ ≥ τ ), its payoff is the improvement over the best payoff πv′ among all sensors v′ ∈ A minus its activation cost. In case multiple sensors activate, the highest reward is retained.\nIn the broadcast model where each sensor can compute its marginal benefit, we can use any standard no-regret algorithm for combining expert advice, such as Randomized Weighted Majority (WMR) [21], to play this game and obtain no regret guarantees6 for selecting τv. In our context a sensor using WMR simply maintains weights w(τi) = exp (η · ψtotal(τi)) for each possible threshold τi, where η > 0 is a learning parameter, and ψtotal(τi) is the total cumulative reward for playing τi in every round so far. On each step each threshold is picked with probability proportional to its weight. In the more restricted star network model, we can use a modification of WMR that feeds back unbiased estimates for ψt(τi), the 6We leave it as an open problem to determine if the outcome is close to optimal when all sensors play low regret strategies (i.e., is the price of total anarchy [4] small in any variant of this game with a reasonable way of splitting the value from the information?)\npayoff to the sensor for using a threshold of τi in round t, and thus obtains reasonably good estimates of ψtotal(τi) after many rounds. We give pseudocode in Fig. 3. In it, we assume that an activated sensor can compute the reward of playing any threshold.\nWe incorporate these ideas into the DOG algorithm, to obtain what we call the Observation-Dependent Distributed Online Greedy algorithm (OD-DOG). In the extreme case that cv = 0 for all v the sensors will soon set their thresholds so low that each sensor activates in each round. In this case OD-DOG will exactly simulate the offline greedy algorithm run on each round. In other words, if we let G(f) be the result of running the offline greedy algorithm on the problem\narg max {f(S) : S ⊂ V, |S| ≤ k}\nthen OD-DOG will obtain a value of ∑ t ft(G(ft)); in con-\ntrast, DOG gets roughly ∑ t ft(G( ∑ t ft)), which may be significantly smaller. Note that Feige’s result [12] implies that the former value is the best we can hope for from efficient algorithms (assuming P 6= NP). Of course, querying each sensor in each round is impractical when querying sensors is expensive. In the other extreme case where cv =∞ for all v, OD-DOG will simulate DOG after a brief learning phase. In general, by adjusting the activation costs cv we can smoothly trade off the cost of sensor communication with the value of the resulting data."
    }, {
      "heading" : "7. EXPERIMENTS",
      "text" : "In this section, we evaluate our DOG algorithm on several real-world sensing problems."
    }, {
      "heading" : "7.1 Data sets",
      "text" : "Temperature data. In our first data set, we analyze temperature measurements from the network of 46 sensors deployed\nat Intel Research Berkeley. Our training data consisted of samples collected at 30 second intervals on 3 consecutive days (starting Feb. 28th 2004), the testing data consisted of the corresponding samples on the two following days. The objective functions used for this application are based on the expected reduction in mean squared prediction error fEMSE, as introduced in Sec. 2.\nPrecipitation data. Our second data set consists of precipitation data collected during the years 1949 - 1994 in the states of Washington and Oregon [30]. Overall 167 regions of equal area, approximately 50 km apart, reported the daily precipitation. To ensure the data could be reasonably modeled using a Gaussian process we applied preprocessing as described in [19]. As objective functions we again use the expected reduction in mean squared prediction error fEMSE.\nWater network monitoring. Our third data set is based on the application of monitoring for outbreak detection. Consider a city water distribution network for delivering drinking water to households. Accidental or malicious intrusions can cause contaminants to spread over the network, and we want to install sensors to detect these contaminations as quickly as possible. In August 2006, the Battle of Water Sensor Networks (BWSN) [11] was organized as an international challenge to find the best sensor placements for a real metropolitan water distribution network, consisting of 12,527 nodes. In this challenge, a set of intrusion scenarios is specified, and for each scenario a realistic simulator provided by the EPA is used to simulate the spread of the contaminant for a 48 hour period. An intrusion is considered detected when one selected node shows positive contaminant concentration. The goal of BWSN was to minimize impact measures, such as the expected population affected, which is calculated using a realistic disease model. For a security-critical sensing task such as protecting drinking water from contamination, it is important to develop sensor selection schemes that maximize detection performance even in adversarial environments (i.e., where an adversary picks the contamination strategy knowing our network deployment and selection algorithm). The algorithms developed in this paper apply to such adversarial settings. We reproduce the experimental setup detailed in [18]. For each contamination event i, we define a separate submodular objective function fi(S) that measures the expected population protected when detecting the contamination from sensors S. In [18], Krause et al. showed that the functions fi(A) are monotone submodular functions."
    }, {
      "heading" : "7.2 Convergence experiments",
      "text" : "In our first set of experiments, we analyzed the convergence of our DOG algorithm. For both the temperature [T] and precipitation [R] data sets, we first run the offline greedy algorithm using the fEMSE objective function to pick k = 5\nsensors. We compare its performance to the DOG algorithm, where we feed back the same objective function at every round. We use an exploration probability γ = 0.01 and a learning rate inversely proportional to the maximum achievable reward fEMSE(V ). Fig. 4(a) presents the results for the temperature data set. Note that even after only a small number of rounds (≈ 100), the algorithm obtains 95% of the performance of the offline algorithm. After about 13,000 iterations, the algorithm obtains 99% of the offline performance, which is the best that can be expected with a .01 exploration probability. Fig. 4(b) show the same experiment on the precipitation data set. In this more complex problem, after 100 iterations, 76% of the offline performance is obtained, which increases to 87% after 500,000 iterations."
    }, {
      "heading" : "7.3 Observation dependent activation",
      "text" : "We also experimentally evaluate our OD-DOG algorithm with observation specific sensor activations. We choose different values for the activation cost cv, which we vary as multiples of the total achievable reward. The activation cost cv lets us smoothly trade off the average number of sensors activating each round and the average obtained reward. The resulting activation strategies are used to select a subset of size k = 10 from a collection of 12,527 sensors. Fig. 4(c) presents rates of convergence using the OD-DOG algorithm under a fixed objective function which considers all contamination events. In Fig. 4(d), convergence rates are presented under a varying objective function, which selects a different contamination event on each round. For low activation costs, the performance quickly converges to or exceeds the performance of the offline solution. Even under the lowest activation costs in our experiments, the average number of extra activations per stage in the OD-DOG algorithm is at most 5. These results indicate that observation specific activation can lead to drastically improved performance at small additional activation cost."
    }, {
      "heading" : "8. RELATED WORK",
      "text" : "Sensor Selection. The problem of deciding when to selectively turn on sensors in sensor networks in order to conserve power was first discussed by [26] and [32]. Many approaches for optimizing sensor placements and selection assume that sensors have a fixed region [15, 14, 3]. These regions are usually convex or even circular. Further, it is assumed that everything within a region can be perfectly observed, and everything outside cannot be measured by the sensors. For complex applications such as environmental monitoring, these assumptions are unrealistic, and the direct optimization of prediction accuracy is desired. The problem of selecting observations for monitoring spatial phenomena has been investigated extensively in geostatistics [8], and more generally (Bayesian) experimental design [6]. Several approaches have been pro-\nposed to activate sensors in order to minimize uncertainty [32] or prediction error [10]. However, these approaches do not have performance guarantees. Submodularity has been used to analyze algorithms for placing [19] or selecting [31] a fixed set of sensors. These approaches however assume that the model is known in advance.\nSubmodular optimization. The problem of centralized maximization of a submodular function has been studied by [23], who proved that the greedy algorithm gives a factor (1− 1/e) approximation. Several algorithms have since been developed for maximizing submodular functions under more complex constraints (see [29] for an overview). Streeter and Golovin developed an algorithm for online optimization of submodular functions, which we build on in this paper [28]."
    }, {
      "heading" : "9. CONCLUSIONS",
      "text" : "In this paper, we considered the problem of repeatedly selecting subsets St from a large set of deployed sensors, in order to maximize a sequence of submodular utility functions f1, . . . , fT . We developed an efficient Distributed Online Greedy algorithm DOG, and proved it suffers no (1− 1/e)regret, essentially the best possible performance obtainable unless P = NP. Our algorithm is fully distributed, requiring only a small number of messages to be exchanged at each round with high probability. We analyze our algorithm both in the broadcast model, and in the star network model, where a separate base station is responsible for computing utilities of selected sets of sensors. Our LAZYDOG algorithm for the latter model uses lazy renormalization in order to reduce the number of messages required from Θ(n) to O(k log n), and the server memory required from Θ(n) to O(k + log n), where k is the desired number of sensors to be selected. In addition, we developed OD-DOG, an extension of DOG that allows observation-dependent sensor selection. We empirically demonstrate the effectiveness of our algorithms on three real-world sensing tasks, demonstrating how our DOG algorithm’s performance converges towards the performance of a clairvoyant offline greedy algorithm. In addition, our results with the OD-DOG algorithm indicate that a small number of extra sensor activations can lead to drastically improved convergence. We believe that our results provide an interesting step towards a principled study of distributed active learning and information gathering.\nAcknowledgments. The authors wish to thank Phillip Gibbons and the anonymous referees for their valuable help and suggestions. This research was partially supported by ONR grant N00014-09-1-1044, NSF grant CNS-0932392, a gift from Microsoft Corporation and the Caltech Center for the Mathematics of Information."
    }, {
      "heading" : "10. REFERENCES",
      "text" : "[1] Z. Abrams, A. Goel, and S. Plotkin. Set k-cover algorithms for energy efficient monitoring in wireless sensor networks. In IPSN, pages 424–432, 2004.\n[2] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48–77, 2002.\n[3] X. Bai, S. Kumar, Z. Yun, D. Xuan, and T. H. Lai. Deploying wireless sensors to achieve both coverage and connectivity. In ACM MobiHoc, 2006.\n[4] A. Blum, M. Hajiaghayi, K. Ligett, and A. Roth. Regret minimization and the price of total anarchy. In STOC, pages 373–382, 2008.\n[5] J. Burke, D. Estrin, M. Hansen, A. Parker, N. Ramanathan, S. Reddy, and M. B. Srivastava. Participatory sensing. In World Sensor Web Workshop, ACM Sensys, 2006.\n[6] K. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Stat. Sci., 10(3):273–304, Aug. 1995.\n[7] Fan Chung and Linyuan Lu. Concentration inequalities and martingale inequalities: A survey. Internet Mathematics, 3(1):79–127, 2006.\n[8] N. A. C. Cressie. Statistics for Spatial Data. Wiley, 1991. [9] A. Das and D. Kempe. Algorithms for subset selection\nin linear regression. In STOC, pages 45–54, 2008.\n[10] A. Deshpande, C. Guestrin, S. Madden, J. Hellerstein, and W. Hong. Model-driven data acquisition in sensor networks. In VLDB, pages 588–599, 2004.\n[11] A. Ostfeld et al. The battle of the water sensor networks (bwsn): A design challenge for engineers and algorithms. Journal of Water Resources Planning and Management, 134(6):556–568, 2008.\n[12] U. Feige. A threshold of lnn for approximating set cover. Journal of the ACM, 45(4):634–652, 1998.\n[13] D. P. Foster and R. Vohra. Regret in the on-line decision problem. Games and Economic Behavior, 29(1-2):7–35, October 1999.\n[14] H. H. Gonzalez-Banos and J. Latombe. A randomized art-gallery algorithm for sensor placement. In Proc. 17th ACM Symp. Comp. Geom., pages 232–240, 2001.\n[15] D. S. Hochbaum and W. Maas. Approximation schemes for covering and packing problems in image processing and VLSI. Journal of the ACM, 32:130–136, 1985.\n[16] A. Krause and C. Guestrin. Near-optimal nonmyopic value of information in graphical models. In Proc. of Uncertainty in Artificial Intelligence (UAI), 2005.\n[17] A. Krause and C. Guestrin. Near-optimal observation selection using submodular functions. In AAAI Nectar track, pages 1650–1654, 2007.\n[18] A. Krause, J. Leskovec, C. Guestrin, J. VanBriesen, and C. Faloutsos. Efficient sensor placement optimization\nfor securing large water distribution networks. J. Wat. Res. Plan. Mgmt., 136(6), 2008.\n[19] A. Krause, A. Singh, and C. Guestrin. Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies. In JMLR, volume 9, pages 235–284, 2008.\n[20] F. Kuhn, T. Locher, and R. Wattenhofer. Distributed selection: a missing piece of data aggregation. Commun. ACM, 51(9):93–99, 2008.\n[21] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Computation, 108(2):212–261, 1994.\n[22] K. Nakano and S. Olariu. A survey on leader election protocols for radio networks. In Parallel Architectures, Algorithms and Networks, 2002. I-SPAN ’02., pages 63–68, 2002.\n[23] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functions - I. Mathematical Programming, 14(1):265–294, 1978.\n[24] H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58:527–535, 1952.\n[25] V. Singhvi, A. Krause, C. Guestrin, J. Garrett, and H.S. Matthews. Intelligent light control using sensor networks. In SenSys, pages 218–229, 2005.\n[26] S. Slijepcevic and M. Potkonjak. Power efficient organization of wireless sensor networks. In ICC, pages 472–476, 2001.\n[27] M. Streeter and D. Golovin. An online algorithm for maximizing submodular functions. Technical Report CMU-CS-07-171, Carnegie Mellon University, 2007.\n[28] M. Streeter and D. Golovin. An online algorithm for maximizing submodular functions. In NIPS, pages 1577–1584, 2008.\n[29] J. Vondrák. Submodularity in Combinatorial Optimization. PhD thesis, Charles University, Prague, Czech Republic, 2007.\n[30] M. Widmann and C. S. Bretherton. 50 km resolution daily precipitation for the pacific northwest. http://www.jisao.washington.edu/data_sets/widmann/, May 1999.\n[31] J.L. Williams, J.W. Fisher III, and A.S. Willsky. Performance guarantees for information theoretic active inference. In AISTATS, 2007.\n[32] F. Zhao, J. Shin, and J. Reich. Information-driven dynamic sensor collaboration for tracking applications. IEEE Signal Processing, 19(2):61–72, 2002.\nAPPENDIX"
    }, {
      "heading" : "A. RESULTS IN THE BROADCAST MODEL",
      "text" : "PROOF OF THEOREM 2. To prove the regret bounds, note that in every round the distribution over sensor selections in the variant of EXP3 we describe (that uses the distributed multinomial sampling scheme and repeatedly reruns the protocol in order to always select some sensor in each round) is precisely the same as the original EXP3. Thus the regret bounds for EXP3 [2] carry over unchanged. We next bound the number of broadcasts. Fix a round, and let S set of sensors that activate in that round. The total number of broadcasts is then |S| + 2; using their calibrated clocks, each sensor (re)samples Xv ∼ Poisson(αpv) and activates if Xv ≥ 1. If no sensors activate before a specified timeout period, the default behavior is to rerun the sampling step. Eventually |S| ≥ 1 sensors activate in the same period. A distinguished sensor in S then determines the selected sensor v, broadcasts id(v), and v broadcasts its observed reward. We prove E [|S|] ≤ α/(1 − e−α) in Proposition 8. When α = 1, this gives us the claimed bound on the number of broadcasts.\nPROPOSITION 8. Rerunning the Poisson Multinomial Sampling Protocol until an element is selected results in at most α/(1− e−α) elements being activated in expectation. Moreover, this value is tight.\nPROOF. Let Xv ∼ Bernoulli(α · pv) be the indicator random variable for the activation of v, and let X := ∑ vXv. The expected number of sensor activations is then\nE [X | X ≥ 1] = E [X] /Pr [X ≥ 1] . In the limit as maxv pv tends to zero, X converges to a Poisson random variable with mean α. In this case, E[X]Pr[X≥1] = α/(1 − e−α) To see that this is an upper bound, consider an arbitrary distribution p on the sensors, and fix some v with x := pv > 0. We claim that replacing v with two sensors v1 and v2 with positive probability mass x1 and x2 with x = x1 + x2 can only serve to increase the expected number of sensor activations, because E [X] is unchanged, and Pr [X ≥ 1] decreases. The latter is true essentially because Pr [∃i ∈ {1, 2} : vi activates] = 1 − (1 − x1)(1 − x2) = x− x1x2 < x. To complete the proof, notice that repeating this process with v = arg max(pv) and xi = x/2 ensures X converges to a Poisson variable with mean α, while only increasing E [X | X ≥ 1]."
    }, {
      "heading" : "B. RESULTS IN THE STAR NETWORK MODEL",
      "text" : "In this section we will prove that lazy renormalization samples sensors from a proper scaled distribution (1 − e−α)pv where pv is the input distribution. We then bound the communication overhead of using lazy renormalization for any\nMAB algorithm satisfying certain assumptions enumerated below, and then show how these bounds apply to EXP3.\nPROPOSITION 9. The lazy renormalization scheme of Sec. 5.1, described in pseudocode in Fig. 2, samples v with probability (1 − e−α)pv, where pv = ρ(wv(t), Z(t)) is the desired probability mass for v.\nPROOF. Lazy renormalization selects each sensor v with probability (1− e−α)pv , because of the way the random bits rv are shared in order to implement a coupled distribution for sensor activation and selection. Note that it would be sufficient to run the Poisson Multinomial Sampling Protocol on the correct (possibly oversampled) probabilities, αpv, since then Prop. 1 ensures that each v is selected with probability (1 − e−α)pv. The difficulty is that v does not have access to the correct normalizer Z(t), but only its estimate (lower bound) for it, Zv(t). To overcome this difficulty, we define a joint probability distribution over two random variables (Xv, Yv), where\nXv = Xv(R) := { 1 if R ≥ 1− α · ρ(wv(t), Zv(t)) 0 otherwise\nYv = Yv(R) := min\n{ b : b∑\na=0\ne−λλa\na! ≥ R\n}\nand λ := α · ρ(wv(t), Z(t)), and R is sampled uniformly at random from [0, 1]. Now, note that Yv is distributed as Poisson(λ). Also note that Yv ≥ 1 implies Xv ≥ 1, because Yv ≥ 1 implies R ≥ e−λ and e−λ ≥ 1− λ ≥ 1− α · ρ(wv(t), Zv(t)) since 1 + x ≤ ex for all x ∈ R, and ρ(wv(t), Zv(t)) ≥ ρ(wv(t), Z(t)) due to fact that Zv(t) ≤ Z(t). It follows that we can use the event Xv ≥ 1 as a conservative indicator that v should activate. In this case, it will send its sampled value for R, namely rv, and its weight wv(t) to the server. The server knows Z(t), and then can use rv and wv(t) to compute Yv(rv), the sample from Poisson(λ) that v would have drawn had it known Z(t). The resulting distribution on selected sensors is thus exactly the same as in the Poisson Multinomial Sampling Protocol without lazy renormalization. Invoking Prop. 1 thus completes the proof.\nWe now describe the assumptions that are sufficient to ensure lazy renormalization has low communication costs. Fix an action v and a multiarmed bandit algorithm. Let pv(t) ∈ [0, 1] be the random variable denoting the probability the algorithm assigns to v on round t. The value of pv(t) depends on the random choices made by the algorithm and the payoffs observed by it on previous rounds. We assume the following about each pv(t).\n1. pv(t) can be computed from local information v possesses and global information the server has.\n2. There exists an > 0 such that pv(t) ≥ for all t. 3. pv(t) < pv(t+ 1) implies v was selected in round t.\n4. There exists ̂ > 0 such that pv(t+ 1) ≥ pv(t)/(1 + ̂) for all t.\nMany MAB algorithms satisfy these conditions. For example, all MAB algorithms with non-trivial no-regret guarantees against adversarial payoff functions must continually explore all their options, which effectively mandates pv(t) ≥ for some > 0. In Lemma 1 we prove that EXP3 does so with = γ/n and ̂ = (e − 1) γn , assuming payoffs in [0, 1]. In this case, Theorem 10 bounds the expected increase in sensor communications due to lazy renormalization by a factor of 1 + e−1α .\nTHEOREM 10. Fix a multiarmed bandit instance with possibly adversarial payoff functions, and a MAB algorithm satisfying the above assumptions on its distribution over actions {pv(t)}v∈V . Let qv(t) be the corresponding random estimates for pv(t) maintained under lazy renormalization with oversampling parameter α. Then for all v and t,\nE [qv(t)/pv(t)] ≤ 1 + ̂\nα\nand\nE [qv(t)] ≤ ( 1 + ̂\nα\n) E [pv(t)] .\nPROOF. Fix v, and let p(t) := pv(t), q(t) := qv(t). We begin by bounding Pr [q(t) ≥ λp(t)] for λ ≥ 1. Let t0 be the most recent round in which q(t0) = p(t0). We assume q(0) = p(0), so t0 exists. Then q(t) = p(t0) ≥ λp(t) implies p(t0)/p(t) ≥ λ. By assumption p(t′)/p(t′+ 1) ≤ (1 + ̂) for all t′, so p(t0)/p(t) ≤ (1+ ̂)t−t0 . Thus λ ≤ (1+ ̂)t−t0 and t− t0 ≥ ln(λ)/ ln(1 + ̂). Define t(λ) := ln(λ)/ ln(1 + ̂).\nBy definition of t0, there were no activations under lazy renormalization in rounds t0 through t− 1 inclusive, which occurs with probability\n∏t−1 t′=t0\n(1−αq(t′)) = (1−αq(t))t−t0 ≤ (1 − αq(t))dt(λ)e, where α is the oversampling parameter in the protocol. We now bound E [q(t)/p(t) | q(t)]. Recall that E [X] = ∫∞ x=0\nPr [X ≥ x] dx for any non-negative random variable X . It will also be convenient to define ω := ln(1/(1− αq(t)))/ ln(1 + ̂) and assume for now that ω > 1. Conditioning on q(t), we see that\nE [q(t)/p(t) | q(t)] = ∫∞ λ=0\nPr [q(t) ≥ λp(t)] dλ = 1 + ∫∞ λ=1\nPr [q(t) ≥ λp(t)] dλ ≤ 1 + ∫∞ λ=1\n(1− αq(t))t(λ)dλ = 1 + ∫∞ λ=1 λln(1−αq(t))/ ln(1+̂)dλ = 1 + ∫∞ λ=1 λ−ωdλ\n= 1 + 1ω−1\nUsing ln (\n1 1−x\n) ≥ x for all x < 1 and ln(1 + x) ≤ x for\nall x > −1, we can show that ω ≥ αq(t)/̂ so 1 + 1ω−1 ≤ αq(t)/(αq(t) − ̂). Thus, if αq(t) > ̂ then ω > 1 and we obtain E [q(t)/p(t) | q(t)] ≤ αq(t)/(αq(t)− ̂).\nIf q(t) >> ̂, this gives a good bound. If q(t) is small, we rely on the assumption that p(t) ≥ for all t to get a trivial bound of q(t)/p(t) ≤ q(t)/ . We thus conclude E [q(t)/p(t) | q(t)] ≤ min (αq(t)/(αq(t)− ̂), q(t)/ ) . (B.1) Setting q(t) = (̂/α+ ) to maximize this quantity yields an unconditional bound of E [q(t)/p(t)] ≤ 1 + ̂/α .\nTo bound E [q(t)] in terms of E [p(t)], note that for all q\nq/E [p(t) | q(t) = q] ≤ E [q(t)/p(t) | q(t) = q] ≤ 1 + ̂/α\nwhere the first line is by Jensen’s inequality, and the second is by equation B.1. Thus q ≤ (1 + ̂/α )E [p(t) | q(t) = q] for all q. Taking the expectation with respect to q then proves E [qv(t)] ≤ ( 1 + ̂α ) E [pv(t)] as claimed.\nLEMMA 1. EXP3 with η = γ/n satisfies the conditions of Theorem 10 with = γ/n and ̂ = (e− 1) γn .\nPROOF. The former equality is an easy observation. To prove the latter equality, fix a round t and a selected action v. Let wv(t) be the weight of v in round t, and W (t) be the total weight of all actions in round t. Let π be the payoff to v in round t. Given the update rule wv(t + 1) = wv(t) exp ( γ n π(v,t) pv(t) ) , only the probabilities of the other actions will be decreased. It is not hard to see that they will be decreased by a multiplicative factor of at most W (t)/W (t+ 1), no matter what the learning parameter γ is. By the update rule,\nW (t+ 1) = W (t) + wv(t)\n( exp ( γ\nn\nπ\npv(t)\n) − 1 ) .\nLet p := pv(t) and x := γnπ. Dividing the above equation by W (t), we get\nW (t+ 1)\nW (t) = 1 + p (exp (x/p)− 1) (B.2)\n≤ 1 + p ( x/p+ (e− 2)(x/p)2 ) (B.3)\n≤ 1 + x+ (e− 2)x2/p (B.4) where in the second line we have used ex ≤ 1+x+(e−2)x2 for x ∈ [0, 1]. Note π ≤ 1 implies x ≤ γ/n ≤ p, so W (t+1) W (t) ≤ 1+(e−1)x ≤ 1+(e−1) γ n . It follows that setting ̂ = (e−1) γn is sufficient to ensure pv(t+1) ≥ pv(t)/(1+ ̂) for all t.\nWe now prove Theorem 4 and Corollary 5.\nPROOF OF THEOREM 4.. We prove in Lemma 1 that EXP3 satisfies the conditions of Theorem 10 with = γ/n and\n̂ = (e− 1) γn . Thus by Theorem 10\nE\n[∑\nv\nqv(t)\n] ≤ (1 + (e− 1)/α)E [∑\nv\npv(t)\n]\n= (1 + (e− 1)/α) because ∑ v pv(t) = 1. Each sensor v activates with probability αqv(t), so the expected number of activations is\nE [ α ∑\nv\nqv(t)\n] ≤ α (1 + (e− 1)/α) .\nThat proves the claimed bounds in expectation. To prove bounds with high probability, note that a sensor activates with probability αqv(t) in round t, where qv(t) is a random variable. Fix t. Let [E ] denote the indicator variable for the event E , i.e., [E ] = 1 if E occurs, and [E ] = 0 otherwise. Then we can write [v activates in round t] = [αqv(t) ≥ R], where R is sampled uniformly at random from [0, 1] and R is independent of qv(t). Then if fR is the probability density functions of R we can write\nPr [R ≤ αqv(t)] = ∫ 1\nr=0\nPr [αqv(t) ≥ R | R = r] fR(r)dr\n=\n∫ 1\nr=0\nPr [αqv(t) ≥ r] fR(r)dr\n=\n∫ 1\nr=0\nPr [αqv(t) ≥ r] dr\n= E [αqv(t)]\nThus the number of sensor activations is a sum of |V | binary random variables with cumulative mean µ := ∑ v E [αqv(t)]. We have already bounded this mean as µ ≤ α+(e−1). From here a simple application of a Chernoff-Hoeffding bound suffices to prove that with high probability this sum is at most O(α + log n). Let A be the number of sensor activations. Then, e.g., Theorem 5 of [7] immediately implies\nPr [A ≥ µ(1 + δ)] ≤ exp ( − δ 2µ2\n2µ+ 2δµ3\n)\nFor δ ≥ 1, this yields Pr [A ≥ µ(1 + δ)] ≤ exp ( − 3δµ8 ) . Setting δ = 1 + 8c lnn3µ ensures this probability is at most n−c, hence Pr [ A ≥ 2µ+ 83c lnn ] ≤ n−c. Noting that µ ≤ α + (e − 1) completes the high probability bound on the number of activations.\nAs for the number of messages, note that each message involves a sensor as sender or receiver, and by inspection the protocol only involves two messages per activated node.\nPROOF OF COROLLARY 5.. Use the distributed EXP3 protocol with lazy renormalization with α = lnn. We have already established that the probability of nothing being selected is e−α or 1/n in this case. If nothing is selected,\nsend out n messages, one to each sensor, to rerun the protocol. The expected number of messages sent to initiate additional runs of the protocol is ∑∞ x=1 nx/n\nx = (1− 1/n)−2 = 1+O(1/n). LetX be the number of sensor activations. As in the proof of Proposition 8, if Y is the expected number of sensor activations without rerunning the protocol when nothing is selected, then E [X] = E [Y ] /Pr [Y ≥ 1]. By Theorem 4 E [Y ] ≤ α (1 + (e− 1)/α). Since Pr [Y ≥ 1] = 1 − e−α, we conclude\nE [X] ≤ lnn+ (e− 1) +O ( lnn\nn\n) .\nThe with-high-probability bounds on the number of sensor activations are proved as in the proof of Corollary 5.\nAs for the number of messages, note that other than messages sent to initiate additional runs of the protocol, there are only two messages per activated node. Finally, the regret bounds for distributed EXP3 are the same as standard EXP3 because by design the two algorithms select sensors from exactly the same distribution in each round. Note that the distribution in any given round is a random object depending on the algorithm’s choices in the previous rounds, however on each round the distribution on distributions is the same for both EXP3 variants, as can be readily proved by induction on the round number.\nC. ALGORITHM OGUNIT WITH FAULTY ACTIONS\nIn order to prove Theorem 7, we need a guarantee on the performance of OGUNIT if its elements are may fail to give any benefit. We provide this in the form of Theorem 11.\nSuppose we run DOG with the Poisson Multinomial Sampling Protocol with lazy renormalization, and do not resample on stages where no sensor activates. Then with some probability during any given stage i ∈ [k], no sensors activate and the server receives no information. Suppose that this probability is at most δ in each stage. We have shown in section 4.1 that δ ≤ e−α where α is the oversampling parameter. We claim that we can compensate for this possibility by running DOG for k/(1− δ) stages in each round rather than k, because of the following guarantee for OGUNIT.\nTHEOREM 11. Fix finite set V , k ∈ N, and a sequence of monotone submodular functions f1, . . . , fT : 2V → [0, 1]. Let OPTk = maxS⊂V,|S|≤k ∑t t=1 ft(S). For all v ∈ V let v′ be a random element which is v with probability 1−δv and is null7 with probability δv. Let f ′t(S′) := E [ft(S)] where S the set obtained by including every element v′ of S′ in it independently with probability δv. Let S′1, . . . , S ′ T be the sequence of random sets obtained from running OGUNIT with 7Here, a null element always contributes nothing in the way of utility, so that ft(S ∪ {null}) = ft(S) for all t and S.\nactions V ′ := {v′ : v ∈ V } and objective functions {f ′t}Tt=1 and k′ = k/(1 − δ) stages, where δ = maxv δv. Suppose the algorithms for each stage have expected regret at most r. Then\nE\n[ T∑\nt=1\nf ′t(S ′ t)\n] ≥ (\n1− 1 e\n) OPTk − ⌈ k\n1− δ\n⌉ r.\nPROOF. It suffices to prove the analogous result in the offline case; the “meta-actions” analysis in [27] can then be used to complete the proof. So consider a set of elements V and the “faulty” versions V ′. Fix a monotone submodular f : 2V → [0, 1] and define f ′ as above. Run the offline greedy algorithm on f ′ to try to find the best set of k′ = k1−δ elements in V ′. Let g′i be the chosen element in stage i, and let G′i = { g′j : 1 ≤ j ≤ i } . Let Gi denote the realization of G′i after sampling, so that Gi ⊆ {g : g′ ∈ G′i}. Let S∗ = arg maxS⊆V,|S|≤k(f(S)). We claim that for all i\nE [ f(G′i+1)− f(G′i) ∣∣ Gi ] ≥ (1− δ) f(S ∗)− f(Gi) k\nbecause\nf(S∗)− f(Gi) ≤ f(Gi ∪ S∗)− f(Gi) ≤ ∑\nv∈S∗ (f(Gi + v)− f(Gi))\n≤ k ·max v (f(Gi + v)− f(Gi))\nand maxv′ (E [f(G′i + v′)− f(G′i) | Gi]) is at least equal to (1−δ) maxv (f(Gi + v)− f(Gi)). Removing the conditioning on Gi we get\nE [ f(G′i+1)− f(G′i) ] ≥ (1− δ) f(S ∗)− E [f(G′i)] k\nLet Φ(i) = f(S∗) − E [f(G′i)]. The previous equation implies Φ(i + 1) ≤ Φ(i) ( 1− 1−δk ) . By induction Φ(i) ≤ f(S∗) ( 1− 1−δk )i . Using 1 − x ≤ e−x we conclude that\nΦ(dk/(1− δ)e) ≤ f(S∗)/e and f ′(Gk′) ≥ ( 1− 1e ) f(S∗).\nWe are now ready to prove Theorem 7.\nPROOF THEOREM 7. To bound the number of sensor activations, we note there are k′ := dk/(1− e−α)e rounds, and each round activates at most α+(e−1) sensors in expectation andO(α+log n) sensors with high probability by Theorem 4 (which proves these bounds in the higher communication case where we do rerun the PMS Protocol protocol if nothing is selected). This, and the fact that α/(1 − e−α) = O(α) for α > 0 yields the claimed activation bounds. It is an easy observation that the number of messages is at most twice the number of activations. Clearly, at most one sensor per stage is activated, so at most k′ are activated over one round. Finally, the regret bound follows from Theorem 11, using δ = e−α.\nOnline Distributed Sensor Selection\nDaniel Golovin Caltech\nMatthew Faulkner Caltech\nAndreas Krause Caltech\nABSTRACT A key problem in sensor networks is to decide which sensors to query when, in order to obtain the most useful information (e.g., for performing accurate prediction), subject to constraints (e.g., on power and bandwidth). In many applications the utility function is not known a priori, must be learned from data, and can even change over time. Furthermore for large sensor networks solving a centralized optimization problem to select sensors is not feasible, and thus we seek a fully distributed solution. In this paper, we present Distributed Online Greedy (DOG), an efficient, distributed algorithm for repeatedly selecting sensors online, only receiving feedback about the utility of the selected sensors. We prove very strong theoretical no-regret guarantees that apply whenever the (unknown) utility function satisfies a natural diminishing returns property called submodularity. Our algorithm has extremely low communication requirements, and scales well to large sensor deployments. We extend DOG to allow observationdependent sensor selection. We empirically demonstrate the effectiveness of our algorithm on several real-world sensing tasks.\nCategories and Subject Descriptors C.2.1 [Computer-Communication Networks]: Network Architecture and Design; G.3 [Probability and Statistics]: Experimental Design; I.2.6 [AI]: Learning\nGeneral Terms Algorithms, Measurement\nKeywords Sensor networks, approximation algorithms, distributed mul-\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. IPSN’10, April 12–16, 2010, Stockholm, Sweden. Copyright 2010 ACM 978-1-60558-955-8/10/04 ...$5.00.\ntiarmed bandit algorithms, submodular optimization"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "A key challenge in deploying sensor networks for realworld applications such as environmental monitoring [?], building automation [?] and others is to decide when to activate the sensors in order to obtain the most useful information from the network (e.g., accurate predictions at unobserved locations) and to minimize power consumption. This sensor selection problem has received considerable attention [?, ?, ?], and algorithms with performance guarantees have been developed [?, ?]. However, many of the existing approaches make simplifying assumptions. Many approaches assume (1) that the sensors can perfectly observe a particular sensing region, and nothing outside the region [?]. This assumption does not allow us to model settings where multiple noisy sensors can help each other obtain better predictions. There are also approaches that base their notion of utility on more detailed models, such as improvement in prediction accuracy w.r.t. some statistical model [?] or detection performance [?]. However, most of these approaches make two crucial assumptions: (2) The model, upon which the optimization is based, is known in advance (e.g., based on domain knowledge or data from a pilot deployment) and (3), a centralized optimization selects the sensors (i.e., some centralized processor selects the sensors which obtain highest utility w.r.t. the model). We are not aware of any approach that simultaneously addresses the three main challenges (1), (2) and (3) above and still provides theoretical guarantees.\nIn this paper, we develop an efficient algorithm, called Distributed Online Greedy (DOG), which addresses these three central challenges. Prior work [?] has shown that many sensing tasks satisfy an intuitive diminishing returns property, submodularity, which states that activating a new sensor helps more if few sensors have been activated so far, and less if many sensors have already been activated. Our algorithm applies to any setting where the true objective is submodular [?], thus capturing a variety of realistic sensor models. Secondly, our algorithm does not require the model to be specified in advance: it learns to optimize the objective function in an on-\nar X\niv :1\n00 2.\n17 82\nv3 [\ncs .L\nG ]\n1 3\nM ay\n2 01\n0\nline manner. Lastly, the algorithm is distributed; the sensors decide whether to activate themselves based on local information. We analyze our algorithm in the no-regret model, proving convergence properties similar to the best bounds for any centralized solution.\nA bandit approach toward sensor selection. At the heart of our approach is a novel distributed algorithm for multiarmed bandit (MAB) problems. In the classical multiarmed bandit [?] setting, we picture a slot machine with multiple arms, where each arm generates a random payoff with unknown mean. Our goal is to devise a strategy for pulling arms to maximize the total reward accrued. The difference between the optimal arm’s payoff and the obtained payoff is called the regret. Known algorithms can achieve average per-round regret of O(√n log n/ √ T ) where n is the number of arms, and T the number of rounds (see e.g. the survey of [?]). Suppose we would like to, at every time step, select k sensors. The sensor selection problem can then be cast as a multiarmed bandit problem, where there is one arm for each possible set of k sensors, and the payoff is the accrued utility for the selected set. Since the number of possible sets, and thus the number of arms, is exponentially large, the resulting regret bound is O(nk/2√log n/ √ T ), i.e., exponential in k. However, when the utility function is submodular, the payoffs of these arms are correlated. Recent results [?] show that this correlation due to submodularity can be exploited by reducing the nk-armed bandit problem to k separate n-armed bandit problems, with only a bounded loss in performance. Existing bandit algorithms, such as the widely used EXP3 algorithm [?], are centralized in nature. Consequently, the key challenge in distributed online submodular sensing is how to devise a distributed bandit algorithm. In Sec. 4 and 5, we develop a distributed variant of EXP3 using novel algorithms to sample from and update a probability distribution in a distributed way. Roughly, we develop a scheme where each sensor maintains its own weight, and activates itself independently from all other sensors purely depending on this weight.\nObservation specific selection. A shortcoming of centralized sensor selection is that the individual sensors’ current measurements are not considered in the selection process. In many applications, obtaining sensor measurements is less costly than transmitting the measurements across the network. For example, cell phones used in participatory sensing [?] can inexpensively obtain measurements on a regular basis, but it is expensive to constantly communicate measurements over the network. In Sec. 6, we extend our distributed selection algorithm to activate sensors depending on their observations, and analyze the tradeoff between power consumption and the utility obtained under observation specific activation.\nCommunication models. We analyze our algorithms under\ntwo models of communication cost: In the broadcast model, each sensor can broadcast a message to all other sensors at unit cost. In the star network model, messages can only be between a sensor and the base station, and each message has unit cost. In Sec. 4 we formulate and analyze a distributed algorithm for sensor selection under the simpler broadcast model. Then, in Sec. 5 we show how the algorithm can be extended to the star network model.\nOur main contributions. • Distributed EXP3, a novel distributed implementation\nof the classic multiarmed bandit algorithm. • Distributed Online Greedy (DOG) and LAZYDOG,\nnovel algorithms for distributed online sensor selection, which apply to many settings, only requiring the utility function to be submodular. • OD-DOG, an extension of DOG to allow for observation-\ndependent selection. • We analyze our algorithm in the no-regret model and\nprove that it attains the optimal regret bounds attainable by any efficient centralized algorithm. • We evaluate our approach on several real-world sensing\ntasks including monitoring a 12,527 node network.\nFinally, while we do not consider multi-hop or general network topologies in this paper, we believe that the ideas behind our algorithms will likely prove valuable for sensor selection in those models as well."
    }, {
      "heading" : "2. THE SENSOR SELECTION PROBLEM",
      "text" : "We now formalize the sensor selection problem. Suppose a network of sensors has been deployed at a set of locations V with the task of monitoring some phenomenon (e.g., temperature in a building). Constraints on communication bandwidth or battery power typically require us to select a subset A of these sensors for activation, according to some utility function. The activated sensors then send their data to a server (base station). We first review the traditional offline setting where the utility function is specified in advance, illustrating how submodularity allows us to obtain provably near-optimal selections. We then address the more challenging setting where the utility function must be learned from data in an online manner."
    }, {
      "heading" : "2.1 The Offline Sensor Selection Problem",
      "text" : "A standard offline sensor selection algorithm chooses a set of sensors that maximizes a known sensing quality objective function f(A), subject to some constraints, e.g., on the number of activated sensors. One possible choice for the sensing quality is based on prediction accuracy (we will discuss other possible choices later on). In many applications, measurements are correlated across space, which allows us to make predictions at the unobserved locations. For example,\nprior work [?] has considered the setting where a random variable Xs is associated with every location s ∈ V , and a joint probability distribution P (XV ) models the correlation between sensor values. Here, XV = [X1, . . . ,Xn] is the random vector over all measurements. If some measurements XA = xA are obtained at a subset of locations, then the conditional distribution P (XV \\A | XA = xA) allows predictions at the unobserved locations, e.g., by predicting E[XV \\A | XA = xA]. Furthermore, this conditional distribution quantifies the uncertainty in the prediction: Intuitively, we would like to select sensors that minimize the predictive uncertainty. One way to quantify the predictive uncertainty is the mean squared prediction error,\nMSE(XV \\A | xA) = 1\nn\n∑\ns∈V \\A E[(Xs−E[Xs | xA])2 | xA].\nIn general, the measurements xA that sensors A will make is not known in advance. Thus, we can base our optimization on the expected mean squared prediction error,\nEMSE(A) = ∫ dp(xA) MSE(XV \\A | xA).\nEquivalently, we can maximize the reduction in mean squared prediction error,\nfEMSE(A) = EMSE(∅)− EMSE(A).\nBy definition, fEMSE(∅) = 0, i.e., no sensors obtain no utility. Furthermore, fEMSE is monotonic: if A ⊆ B ⊆ V , then fEMSE(A) ≤ fEMSE(B), i.e., adding more sensors always helps. That means, fEMSE is maximized by the set of all sensors V . However, in practice, we would like to only select a small set of, e.g., at most k sensors due to bandwidth and power constraints:\nA∗ = arg max A fEMSE(A) s.t. |A| ≤ k.\nUnfortunately, this optimization problem is NP-hard, so we cannot expect to efficiently find the optimal solution. Fortunately, it can be shown [?] that in many settings1, the function fEMSE satisfies an intuitive diminishing returns property called submodularity. A set function f : 2V → R is called submodular if, for allA ⊆ B ⊆ V and s ∈ V \\B it holds that f(A∪{s})−f(A) ≥ f(B∪{s})−f(B). Many other natural objective functions for sensor selection satisfy submodularity as well [?]. For example, the sensing region model where fREG(A) is the total area covered by all sensors A is submodular. The detection model where fDET (A) counts the expected number of targets detected by sensors A is submodular as well.\nA fundamental result of Nemhauser et al. [?] is that for monotone submodular functions, a simple greedy algorithm,\n1For Gaussian models and conditional suppressorfreeness [?]\nwhich starts with the empty set A0 = ∅ and iteratively adds the element\nsk = arg max s∈V \\Ak−1\nf(Ak−1 ∪ {s}); Ak = Ak−1 ∪ {sk}\nwhich maximally improves the utility obtains a near-optimal solution: For the set Ak it holds that\nf(Ak) ≥ (1− 1/e) max|A|≤k f(A),\ni.e., the greedy solution obtains at least a constant fraction of (1− 1/e) ≈ 63% of the optimal value.\nOne fundamental problem with this offline approach is that it requires the function f to be specified in advance, i.e., before running the greedy algorithm. For the function fEMSE, this means that the probabilistic model P (XV ) needs to be known in advance. While for some applications some prior data, e.g., from pilot deployments, may be accessible, very often no such prior data is available. This leads to a “chickenand-egg” problem, where sensors need to be activated to collect data in order to learn a model, but also the model is required to inform the sensor selection. This is akin to the “exploration–exploitation tradeoff” in reinforcement learning [?], where an agent needs to decide whether to explore and gather information about effectiveness of an action, or to exploit, i.e., choose actions known to be effective. In the following, we devise an online monitoring scheme based on this analogy."
    }, {
      "heading" : "2.2 The Online Sensor Selection Problem",
      "text" : "We now consider the more challenging problem where the objective function is not specified in advance, and needs to be learned during the monitoring task. We assume that we intend to monitor the environment for a number T of time steps (rounds). In each round t, a set St of sensors is selected, and these sensors transmit their measurements to a server (base station). The server then determines a sensing quality ft(St) quantifying the utility obtained from the resulting analysis. For example, if our goal is spatial prediction, the server would build a model based on the previously collected sensor data, pick a random sensor s, make prediction for the variable Xs, and then compare the prediction µs with the sensor reading xs. The error ft = σ2s−(µs−xs)2 is an unbiased estimate of the reduction in EMSE. In the following analysis, we will only assume that the objective functions ft are bounded (w.l.o.g., take values in [0, 1]), monotone, and submodular, and that we have some way of computing ft(S) for any subset of sensors S. Our goal is to maximize the total reward obtained by the system over T rounds, ∑T t=1 ft(St).\nWe seek to develop a protocol for selecting the sets St of sensors at each round, such that after a small number of rounds the average performance of our online algorithm converges to the same performance of the offline strategy (that knows the objective functions). We thus compare our\nprotocol against all strategies that can select a fixed set of k sensors for use in all of the rounds; the best such strategy obtains reward maxS⊆V :|S|≤k ∑T t=1 ft(S). The difference between this quantity and what our protocol obtains is known as its regret, and an algorithm is said to be no-regret if its average regret tends to zero (or less)2 as T →∞.\nWhen k = 1, our problem is simply the well-studied multiarmed bandit (MAB) problem, for which many no-regret algorithms are known [?]. For general k, because the average of several submodular functions remains submodular, we can apply the result of Nemhauser et al. [?] (cf., Sec. 2.1) to prove that a simple greedy algorithm obtains a (1− 1/e) approximation to the optimal offline solution. Feige [?] showed that this is optimal in the sense that obtaining a (1− 1/e+ ) approximation for any > 0 is NP-hard. These facts suggest that we cannot expect any efficient online algorithm to converge to a solution better than (1 − 1/e) maxS⊆V :|S|≤k ∑T t=1 ft(S). We therefore define the (1− 1/e)-regret of a sequence of (possibly random) sets {St}Tt=1 as\nRT := (1− 1/e) · max S⊆V :|S|≤k\nT∑\nt=1\nft(S) − T∑\nt=1\nE [ft(St)]\nwhere the expectation is taken over the distribution for each St. We say an online algorithm producing a sequence of sets has no-(1− 1/e)-regret if lim supT→∞ RTT ≤ 0."
    }, {
      "heading" : "3. CENTRALIZED ALGORITHM FOR ONLINE SENSOR SELECTION",
      "text" : "Before developing the distributed algorithm for online sensor selection, we will first review a centralized algorithm which is guaranteed to achieve no (1 − 1/e)-regret. In Sec. 4 we will show how this centralized algorithm can be implemented efficiently in a distributed manner. This algorithm starts with the greedy algorithm for a known submodular function mentioned in Sec. 2.1, and adapts it to the online setting. Doing so requires an online algorithm for selecting a single sensor as a subroutine, and we review such an algorithm in Sec. 3.1 before discussing the centralized algorithm for selecting multiple sensors in Sec. 3.2."
    }, {
      "heading" : "3.1 Centralized Online Single Sensor Selection",
      "text" : "Let us first consider the case where k = 1, i.e., we would like to select one sensor at each round. This simpler problem can be interpreted as an instance of the multiarmed bandit problem (as introduced in Sec. 2.2), where we have one arm for each possible sensor. In this case, the EXP3 algorithm [?] is a centralized solution for no-regret single sensor selection. EXP3 works as follows: It is parameterized by a learning rate η, and an exploration probability γ. It maintains a set of 2Formally, if RT is the total regret for the first T rounds, no-regret means lim supT→∞ RT /T ≤ 0.\nweights ws, one for each arm (sensor) s, initialized to 1. At every round t, it will select each arm s with probability\nps = (1− γ) ws∑ s′ ws′ + γ n ,\ni.e., with probability γ it explores, picking an arm uniformly at random, and with probability (1−γ) it exploits, picking an arm s with probability proportional to its weight ws. Once an arm s has been selected, a feedback r = ft({s}) is obtained, and the weight ws is updated to\nws ← ws exp(ηr/ps). Auer et al. [?] showed that with appropriately chosen learning rate η and exploration probability γ it holds that the cumulative regret RT of EXP3 is O( √ Tn lnn), i.e., the average regret RT /T converges to zero."
    }, {
      "heading" : "3.2 Centralized Selection of Multiple Sensors",
      "text" : "In principle, we could interpret the sensor selection problem as a ( n k ) -armed bandit problem, and apply existing no-regret algorithms such as EXP3. Unfortunately, this approach does not scale, since the number of arms grows exponentially with k. However, in contrast to the traditional multiarmed bandit problem, where the arms are assumed to have independent payoffs, in the sensor selection case, the utility function is submodular and thus the payoffs are correlated across different sets. Recently, Streeter and Golovin showed how this submodularity can be exploited, and developed a no-(1− 1/e)-regret algorithm for online maximization of submodular functions [?]. The key idea behind their algorithm, OGunit, is to turn the offline greedy algorithm into an online algorithm by replacing the greedy selection of the element sk that maximizes the benefit sk = arg maxs f({s1, ..., sk−1} ∪ {s}) by a bandit algorithm. As shown in the pseudocode below, OGUNIT maintains k bandit algorithms, one for each sensor to be selected. At each round t, it selects k sensors according to the choices of the k bandit algorithms Ei 3. Once the elements have been selected, the ith bandit algorithm Ei receives as feedback the incremental benefit ft(s1, . . . , si)−ft(s1, . . . , si−1), i.e., how much additional utility is obtained by adding sensor si to the set of already selected sensors. Below we define [m] := {1, 2, . . . ,m}.\nAlgorithm OGUNIT from [?]: Initialize k multiarmed bandit algorithms E1, E2, . . . , Ek, each with action set V . For each round t ∈ [T ]\nFor each stage i ∈ [k] in parallel Ei selects an action vti For each i ∈ [k] in parallel feedback ft( { vtj : j ≤ i } )− ft( { vtj : j < i } ) to Ei.\nOutput St = {at1, at2, . . . , atk}. 3Bandits with duplicate choices are handled in Sec. 4.6.1 of [?]\nIn [?] it is shown that OGUNIT has a ( 1− 1e ) -regret bound of O(kR) in this feedback model assuming each Ei has expected regret at most R. Thus, when using EXP3 as a subroutine, OGUNIT has no-(1− 1/e)-regret.\nUnfortunately, EXP3 (and in fact all MAB algorithms with no-regret guarantees for non-stochastic reward functions) require sampling from some distribution with weights associated with the sensors. If n is small, we could simply store these weights on the server, and run the bandit algorithms Ei there. However, this solution does not scale to large numbers of sensors. Thus the key problem for online sensor selection is to develop a multiarmed bandit algorithm which implements distributed sampling across the network, with minimal overhead of communication. In addition, the algorithm needs to be able to maintain the distributions (the weights) associated with each Ei in a distributed fashion."
    }, {
      "heading" : "4. DISTRIBUTED ALGORITHM FOR ONLINE SENSOR SELECTION",
      "text" : "We will now develop DOG, an efficient algorithm for distributed online sensor selection. For now we make the following assumptions:\n1. Each sensor v ∈ V is able to compute its contribution to the utility ft(S ∪ {v})− ft(S), where S are a subset of sensors that have already been selected.\n2. Each sensor can broadcast to all other sensors. 3. The sensors have calibrated clocks and unique, linearly\nordered identifiers.\nThese assumptions are reasonable in many applications: (1) In target detection, for example, the objective function ft(S) counts the number of targets detected by the sensors S. Once previously selected sensors have broadcasted which targets they detected, the new sensor s can determine how many additional targets have been detected. Similarly, in statistical estimation, one sensor (or a small number of sensors) randomly activates each round and broadcasts its value. After sensors S have been selected and announced their measurements, the new sensor s can then compute the improvement in prediction accuracy over the previously collected data. (2) The assumption that broadcasts are possible may be realistic for dense deployments and fairly long range transmissions. In Sec. 5 we will show how assumptions (1) and (2) can be relaxed.\nAs we have seen in Sec. 3, the key insight in developing a centralized algorithm for online selection is to replace the greedy selection of the sensor which maximally improves the total utility over the set of previously selected sensors by a bandit algorithm. Thus, a natural approach for developing a distributed algorithm for sensor selection is to first consider the single sensor case."
    }, {
      "heading" : "4.1 Distributed Selection of a Single Sensor",
      "text" : "The key challenge in developing a distributed version of EXP3 is to find a way to sample exactly one element from a probability distribution p over sensors in a distributed manner. This problem is distinct from randomized leader election [?], where the objective is to select exactly one element but the element need not be drawn from a specified distribution. We note that under the multi-hop communication model, sampling one element from the uniform distribution given a rooted spanning tree can be done via a simple random walk [?], but that under the broadcast and star network models this approach degenerates to centralized sampling. Our algorithm, in contrast, samples from an arbitrary distribution by allowing sensors to individually decide to activate. Our bottom-up approach also has two other advantages: (1) it is amenable to modification of the activation probabilities based on local observations, as we discuss in Sec. 6, and (2) since it does not rely on any global state of the network such as a spanning tree, it can gracefully cope with significant edge or node failures.\nA naive distributed sampling scheme. A naive distributed algorithm would be to let each sensor keep track of all activation probabilities p. Then, one sensor (e.g., with the lowest identifier) would broadcast a single random number u uniformly distributed in [0, 1], and the sensor v for which∑v−1 i=1 pi ≤ u < ∑v i=1 pi would activate. However, for large sensor network deployments, this algorithm would require each sensor to store a large amount of global information (all activation probabilities p). Instead, each sensor v could store only their own probability mass pv; the sensors would then, in order of their identifiers, broadcast their probabilities pv, and stop once the sum of the probabilities exceeds u. This approach only requires a constant amount of local information, but requires an impractical Θ(n) messages to be sent, and sent sequentially over Θ(n) time steps.\nDistributed multinomial sampling. In this section we present a protocol that requires only O(1) messages in expectation, and only a constant amount of local information.\nFor a sampling procedure with input distribution p, we let p̂ denote the resulting distribution, where in all cases at most one sensor is selected, and nothing is selected with probability 1−∑v p̂v . A simple approach towards distributed sampling would be to activate each sensor v ∈ V independently from each other with probability pv . While in expectation, exactly one sensor is activated, with probability ∏ v(1 − pv) > 0 no sensor is activated; also since sensors are activated independently, there is a nonzero probability that more than one sensor is activated. Using a synchronized clock, the sensors could determine if no sensor is activated. In this case, they could simply repeat the selection procedure until at least one sensor is activated. One naive approach would be to repeat the selection procedure until exactly one sensor is activated.\nHowever with two sensors and p1 = ε, p2 = 1− ε this algorithm yields p̂1 = ε2/(1 − 2ε + 2ε2) = O(ε2), so the first sensor is severely underrepresented. Another simple protocol would be to select exactly one sensor uniformly at random from the set of activated sensors, which can be implemented using few messages.\nThe Simple Protocol: For each sensor v in parallel\nSample Xv ∼ Bernoulli(pv). If (Xv = 1), Xv activates.\nAll active sensors S coordinate to select a single sensor uniformly at random from S, e.g., by electing the minimum ID sensor in S to do the sampling.\nIt is not hard to show that with this protocol, for all sensors v,\np̂v = pv · E [ 1\n|S|\n∣∣∣∣ v ∈ S ] ≥ pv/E [|S| | v ∈ S] ≥ pv/2\nby appealing to Jensen’s inequality. Since p̂v ≤ pv, we find that this simple protocol maintains a ratio rv := p̂v/pv ∈ [ 12 , 1]. Unfortunately, this analysis is tight, as can be seen from the example with two sensors and p1 = ε, p2 = 1− ε.\nTo improve upon the simple protocol, first consider running it on an example with p1 = p2 = · · · = pn = 1/n. Since the protocol behaves exactly the same under permutations of sensor labels, by symmetry we have p̂1 = p̂2 = · · · = p̂n, and thus ri = rj for all i, j. Now consider an input distribution p where there exists integers N and k1, k2, . . . , kn such that pv = kv/N for all v. Replace each v with kv fictitious sensors, each with probability mass 1/N , and each with a label indicating v. Run the simple protocol with the fictitious sensors, selecting a fictitious sensor v′, and then actually select the sensor indicated by the label of v′. By symmetry this process selects each fictitious sensor with probability (1 − β)/N , where β is the probability that nothing at all is selected, and thus the process selects sensor v with probability kv(1−β)/N = (1−β)pv (since at most one fictitious sensor is ever selected).\nWe may thus consider the following improved protocol which incorporates the above idea, simulating this modification to the protocol exactly when pv = kv/N for all v.\nThe Improved Protocol(N ): For each sensor v in parallel\nSample Xv ∼ Binomial(dN · pve , 1/N). If (Xv ≥ 1), then activate sensor v.\nFrom the active sensors S, select sensor v with probability Xv/ ∑ v′∈S Xv′ .\nThis protocol ensures the ratios rv := p̂v/pv are the same for all sensors, provided each pv is a multiple of 1/N . Assuming the probabilities are rational, there will be a sufficiently large N to satisfy this condition. To reduce β :=\nPr [S = ∅] in the simple protocol, we may sample each Xv from Bernoulli(α · pv) for any α ∈ [1, n]. The symmetry argument remains unchanged. This in turn suggests sampling Xv from Binomial(dN · pve , α/N) in the improved protocol. Taking the limit as N → ∞, the binomial distribution becomes Poisson, and we obtain the desired protocol.\nThe Poisson Multinomial Sampling (PMS) Protocol(α): Same as the improved protocol, except each sensor v samples Xv ∼ Poisson(αpv)\nStraight-forward calculation shows that Pr [S = ∅] = ∏\nv\nexp {−α · pv} = exp { − ∑\nv\nα·pv } = e−α\nLet C be the number of messages. Then E [C] = ∑\nv\nPr [Xv ≥ 1] = ∑\nv\n(1−e−αpv ) ≤ ∑\nv\nαpv = α\nHere we have used linearity of expectation, and 1 + x ≤ ex for all x ∈ R. In summary, we have the following result about our protocol:\nPROPOSITION 1. Fix any fixed p and α > 0. The PMS Protocol always selects at most one sensor, ensures\n∀v : Pr [v selected] = (1− e−α)pv and requires no more than α messages in expectation.\nIn order to ensure that exactly one sensor is selected, whenever S = ∅ we can simply rerun the protocol with fresh random seeds as many times as needed until S is non-empty. Using α = 1, this modification will require only O(1) messages in expectation and at most O(log n) messages with high probability in the broadcast model. We can combine this protocol with EXP3 to get the following result.\nTHEOREM 2. In the broadcast model, running EXP3 using the PMS Protocol with α = 1, and rerunning the protocol whenever nothing is selected, yields exactly the same regret bound as standard EXP3, and in each round at most e/(e−1)+2 ≈ 3.582 messages are broadcast in expectation.\nThe regret bound for EXP3 is O( √\nOPTn log n), where OPT is the total reward of the best action. Our variant simulates EXP3, and thus has identical regret. Proofs of our theoretical results can be found in the Appendix.\nRemark. Running our variant of EXP3 requires that each sensor know the number of sensors, n, in order to compute its activation probability. If each sensor v has only a reasonable estimate of nv of n, however, our algorithm still performs well. For example, it is possible to prove that if all of the sensors have the same estimate nv = cn for some constant c > 0, then the upper bound on expected regret, R(c), grows\nas R(c) ≈ R(1) · max {c, 1/c}. The expected number of activations in this case increases by at most ( 1 c − 1 ) γ. In general underestimating n leads to more activations, and underestimating or overestimating n can lead to more regret. This graceful degradation of performance with respect to the error in estimating n holds for all of our algorithms."
    }, {
      "heading" : "4.2 The Distributed Online Greedy Algorithm",
      "text" : "We now use our single sensor selection algorithm to develop our main algorithm, the Distributed Online Greedy algorithm (DOG). It is based on the distributed implementation of EXP3 using the PMS Protocol. Suppose we would like to select k sensors at each round t. Each sensor v maintains k weights wv,1, . . . , wv,k and normalizing constants Zv,1, . . . , Zv,k. The algorithm proceeds in k stages, synchronized using the common clock. In stage i, a single sensor is selected using the PMS Protocol applied to the distribution (1−γ)wv,i/Zv,i+γ/n. Suppose sensors S = {v1, . . . , vi−1} have been selected in stages 1 through i − 1. The sensor v selected at stage i then computes its local rewards πv,i using the utility function ft(S ∪ {vi}) − ft(S). It then computes its new weight\nw′v,i = wv,i exp(ηπv,i/pv,i),\nand broadcasts the difference between its new and old weights ∆v,i = w ′ v,i−wv,i. All sensors then update their ith normalizers using Zv,i ← Zv,i+∆v,i. Fig. 1 presents the pseudo-code of the DOG algorithm. Thus given Theorem 12 of [?] we have the following result about the DOG algorithm:\nTHEOREM 3. The DOG algorithm selects, at each round t a set St ⊆ V of k sensors such that\n1 T E\n[ T∑\nt=1\nft(St)\n] ≥ 1− 1 e\nT max |S|≤k\nT∑\nt=1\nft(S)−O ( k √ n log n\nT\n) .\nIn expectation, only O(k) messages are exchanged each round."
    }, {
      "heading" : "5. THE STAR NETWORK MODEL",
      "text" : "In some applications, the assumption that sensors can broadcast messages to all sensors may be unrealistic. Furthermore, in some applications sensors may not be able to compute the marginal benefits ft(S ∪ {s})− ft(S) (since this calculation may be computationally complex). In this section, we analyze LAZYDOG, a variant of our DOG algorithm, which replace the above assumptions by the assumption that there is a dedicated base station4 available which computes utilities and which can send non-broadcast messages to individual sensors. 4Though the existence of such a base station means the protocol is not completely distributed, it is realistic in sensor network applications where the sensor data needs to be accumulated somewhere for analysis.\nWe make the following assumptions:\n1. Every sensor stores its probability mass pv with it, and can only send messages to and receive messages from the base station.\n2. The base station is able, after receiving messages from a set S of sensors, to compute the utility ft(S) and send this utility back to the active sensors.\nThese conditions arise, for example, when cell phones in participatory sensor networks can contact the base station, but due to privacy constraints cannot directly call other phones. We do not assume that the base station has access to all weights of the sensors – we will only require the base station to have O(k + log n) memory. In the fully distributed algorithm DOG that relies on broadcasts, it is easy for the sensors to maintain their normalizers Zv,i, since they receive information about rewards from all selected sensors. The key challenge when removing the broadcast assumption is to maintain the normalizers in an appropriate manner."
    }, {
      "heading" : "5.1 Lazy renormalization & Distributed EXP3",
      "text" : "EXP3 (and all MAB with no-regret guarantees against arbitrary reward functions) must maintain a distribution over actions, and update this distribution in response to feedback about the environment. In EXP3, each sensor v requires only wv(t) and a normalizer Z(t) := ∑ v′ wv′(t) to compute pv(t) 5. The former changes only when v is selected. In the broadcast model the latter can simply be broadcast at the end of each round. In the star network model (or, more generally in multi-hop models), standard flooding echo aggregation techniques could be used to compute and distribute the new normalizer, though with high communication cost. We show that a lazy renormalization scheme can significantly reduce the amount of communication needed by a distributed bandit algorithm without altering its regret bounds whatsoever. Thus our lazy scheme is complementary to standard aggregation techniques.\nOur lazy renormalization scheme for EXP3 works as follows. Each sensor v maintains its weight wv(t) and an estimate Zv(t) for Z(t) := ∑ v′ wv′(t), Initially, wv(0) = 1 and Zv(0) = n for all v. The central server stores Z(t). Let\nρ(x, y) := (1− γ)x y + γ n .\nEach sensor then proceeds to activate as in the sampling procedure of Sec. 4.1 as if its probability mass in round t were qv = ρ(wv(t), Zv(t)) instead of its true value of ρ(wv(t), Z(t)). A single sensor is selected by the server with respect to the true value Z(t), resulting in a selection from the desired distribution. Moreover, v’s estimate Zv(t) 5We let x(t) denote the value of variable x at the start of round t, to ease analysis. We do not actually need to store the historical values of the variables over multiple time steps.\nis only updated on rounds when it communicates with the server under these circumstances. This allows the estimated probabilities of all of the sensors to sum to more than one, but has the benefit of significantly reducing the communication cost in the star network model under certain assumptions. We call the result Distributed EXP3, give its pseudocode for round t in Fig. 2.\nSince the sensors underestimate their normalizers, they may activate more frequently than in the broadcast model. Fortunately, the amount of “overactivation” remains bounded. We prove Theorem 4 and Corollary 5 in Appendix B.\nTHEOREM 4. The number of sensor activations in any round of the Distributed EXP3 algorithm is at mostα+ (e− 1) in expectation and O(α+ log n) with high probability, and the number of messages is at most twice the number of activations.\nUnfortunately, there is still an e−α probability of nothing being selected. To address this, we can set α = c lnn for some c ≥ 1, and if nothing is selected, transmit a message to each of the n sensors to rerun the protocol.\nCOROLLARY 5. There is a distributed implementation of EXP3 that always selects a sensor in each round, has the same regret bounds as standard EXP3, ensures that the number of sensor activations in any round is at most lnn+O(1) in expectation orO(log n) with high probability, and in which\nthe number of messages is at most twice the number of activations."
    }, {
      "heading" : "5.2 LazyDOG",
      "text" : "Once we have the distributed EXP3 variant described above, we can use it for the bandit subroutines in the OGUNIT algorithm (cf. Sec. 3.2). We call the result the LAZYDOG algorithm, due to its use of lazy renormalization. The lazy distributed EXP3 still samples sensors from the same distribution as the regular distributed EXP3, so LAZYDOG has precisely the same performance guarantees with respect to∑ t ft(St) as DOG. It works in the star network communication model, and requires few messages or sensor activations. Corollary 5 immediately implies the following result.\nCOROLLARY 6. The number of sensors that activate each round in LAZYDOG is at most k lnn+O(k) in expectation andO(k log n) with high probability, the number of messages is at most twice the number of activations, and the (1− 1/e)regret of LAZYDOG is the same as DOG.\nIf we are not concerned about the exact number of sensors selected in each round, but only want to ensure roughly k sensors are picked in expectation, then we can reduce the number of sensor activations and messages to O(k), by running LAZYDOG with k′ := dk/(1− e−α)e stages for some constant α, and allowing each stage to run the Poisson Multinomial Sampling Protocol with lazy renormalization without\nrerunning it if nothing is selected. This is of course optimal up to constants, as we must send at least one message per selected sensor.\nTHEOREM 7. The variant of LAZYDOG that runs the Poisson Multinomial Sampling Protocol (α) with lazy renormalization for k′ := dk/(1− e−α)e stages, but does not rerun it if nothing is selected in a given stage, has the following guarantees: (1) the number of sensors that activate each round in LAZYDOG is at most k′(α+ e− 1) in expectation and O(αk log n) with high probability, (2) the number of messages is at most twice the number of activations, (3) the expected number of sensors selected in each round is at most k′ and (4) its (1− 1/e)-regret is at most k′/k times that of DOG.\nWe defer the proof to Appendix C."
    }, {
      "heading" : "6. OBSERVATION-DEPENDENT SAMPLING",
      "text" : "Theorem 3 states that DOG is guaranteed to do nearly as well as the offline greedy algorithm run on an instance with objective function fΣ := ∑ t ft. Thus the reward of DOG is asymptotically near-optimal on average. In many applications, however, we would like to perform well on rounds with “atypical” objective functions. For example, in an outbreak detection application as we discuss in Sec. 7, we\nwould like to get very good data on rounds with significant events, even if the nearest sensors typically report “boring” readings that contribute very little to the objective function. For now, suppose that we are only running a single MAB instance to select a single sensor in each round. If we have access to a black-box for evaluating ft on round t, then we can perform well on atypical rounds at the cost of some additional communication by having each sensor v take a local reading of its environment and estimate its payoff π̄ = ft({v}) if selected. This value, which serves as a measure of how interesting its input is, can then be used to decide whether to boost v’s probability for reporting its sensor reading to the server. In the simplest case, we can imagine that each v has a threshold τv such that v activates with probability 1 if π̄ ≥ τv , and with its normal probability otherwise. In the case where we select k > 1 sensors in each round, each sensor can have a threshold for each of the k stages, where in each stage it computes π̄ = ft(S ∪ {v}) − ft(S) where S is the set of currently selected sensors. Since the activation probability only goes up, we can retain the performance guarantees of DOG if we are careful to adjust the feedback properly.\nIdeally, we wish that the sensors learn what their thresholds τv should be. We treat the selection of τv in each round as an online decision problem that each v must play. We construct a particular game that the sensors play, where the strategies are the thresholds (suitably discretized), there is an activation cost cv that v pays if π̄v ≥ τv , and the payoffs are defined as follows: Let πv = ft(S ∪ {v}) − ft(S) be the marginal benefit of selecting v given that sensor set S has already been selected. LetA be the set of sensors that activate in the current iteration of the game, and let max ( π(A\\v) ) := max (πv′ : v ′ ∈ A \\ {v}). The particular reward function ψv we choose for each sensor v for each iteration of the game is\nψv(τ) =\n{ cv −max ( πv −max ( π(A\\v) ) , 0 )\nif π̄ < τ max ( πv −max ( π(A\\v) ) , 0 ) − cv if π̄ ≥ τ\nbased on empirical performance. Thus, if a sensor activates (π̄ ≥ τ ), its payoff is the improvement over the best payoff πv′ among all sensors v′ ∈ A minus its activation cost. In case multiple sensors activate, the highest reward is retained.\nIn the broadcast model where each sensor can compute its marginal benefit, we can use any standard no-regret algorithm for combining expert advice, such as Randomized Weighted Majority (WMR) [?], to play this game and obtain no regret guarantees6 for selecting τv. In our context a sensor using WMR simply maintains weights w(τi) = exp (η · ψtotal(τi)) for each possible threshold τi, where η > 0 is a learning parameter, and ψtotal(τi) is the total cumulative reward for playing τi in every round so far. On each step each threshold 6We leave it as an open problem to determine if the outcome is close to optimal when all sensors play low regret strategies (i.e., is the price of total anarchy [?] small in any variant of this game with a reasonable way of splitting the value from the information?)\nis picked with probability proportional to its weight. In the more restricted star network model, we can use a modification of WMR that feeds back unbiased estimates for ψt(τi), the payoff to the sensor for using a threshold of τi in round t, and thus obtains reasonably good estimates of ψtotal(τi) after many rounds. We give pseudocode in Fig. 3. In it, we assume that an activated sensor can compute the reward of playing any threshold.\nWe incorporate these ideas into the DOG algorithm, to obtain what we call the Observation-Dependent Distributed Online Greedy algorithm (OD-DOG). In the extreme case that cv = 0 for all v the sensors will soon set their thresholds so low that each sensor activates in each round. In this case OD-DOG will exactly simulate the offline greedy algorithm run on each round. In other words, if we let G(f) be the result of running the offline greedy algorithm on the problem\narg max {f(S) : S ⊂ V, |S| ≤ k} then OD-DOG will obtain a value of ∑ t ft(G(ft)); in con-\ntrast, DOG gets roughly ∑ t ft(G( ∑ t ft)), which may be significantly smaller. Note that Feige’s result [?] implies that the former value is the best we can hope for from efficient algorithms (assuming P 6= NP). Of course, querying each sensor in each round is impractical when querying sensors is expensive. In the other extreme case where cv =∞ for all v, OD-DOG will simulate DOG after a brief learning phase. In general, by adjusting the activation costs cv we can smoothly trade off the cost of sensor communication with the value of the resulting data."
    }, {
      "heading" : "7. EXPERIMENTS",
      "text" : "In this section, we evaluate our DOG algorithm on several real-world sensing problems.\n7.1 Data sets\nTemperature data. In our first data set, we analyze temperature measurements from the network of 46 sensors deployed at Intel Research Berkeley. Our training data consisted of samples collected at 30 second intervals on 3 consecutive days (starting Feb. 28th 2004), the testing data consisted of the corresponding samples on the two following days. The objective functions used for this application are based on the expected reduction in mean squared prediction error fEMSE, as introduced in Sec. 2.\nPrecipitation data. Our second data set consists of precipitation data collected during the years 1949 - 1994 in the states of Washington and Oregon [?]. Overall 167 regions of equal area, approximately 50 km apart, reported the daily precipitation. To ensure the data could be reasonably modeled using a Gaussian process we applied preprocessing as described in [?]. As objective functions we again use the expected reduction in mean squared prediction error fEMSE.\nWater network monitoring. Our third data set is based on the application of monitoring for outbreak detection. Consider a city water distribution network for delivering drinking water to households. Accidental or malicious intrusions can cause contaminants to spread over the network, and we want to install sensors to detect these contaminations as quickly as possible. In August 2006, the Battle of Water Sensor Networks (BWSN) [?] was organized as an international challenge to find the best sensor placements for a real metropolitan water distribution network, consisting of 12,527 nodes. In this challenge, a set of intrusion scenarios is specified, and for each scenario a realistic simulator provided by the EPA is used to simulate the spread of the contaminant for a 48 hour period. An intrusion is considered detected when one selected node shows positive contaminant concentration. The goal of BWSN was to minimize impact measures, such as the expected population affected, which is calculated using a realistic disease model. For a security-critical sensing task such as protecting drinking water from contamination, it is important to develop sensor selection schemes that maximize detection performance even in adversarial environments (i.e., where an adversary picks the contamination strategy knowing our network deployment and selection algorithm). The algorithms developed in this paper apply to such adversarial settings. We reproduce the experimental setup detailed in [?]. For each contamination event i, we define a separate submodular objective function fi(S) that measures the expected population protected when detecting the contamination from sensors S. In [?], Krause et al. showed that the functions fi(A) are monotone submodular functions."
    }, {
      "heading" : "7.2 Convergence experiments",
      "text" : "In our first set of experiments, we analyzed the convergence of our DOG algorithm. For both the temperature [T]\nand precipitation [R] data sets, we first run the offline greedy algorithm using the fEMSE objective function to pick k = 5 sensors. We compare its performance to the DOG algorithm, where we feed back the same objective function at every round. We use an exploration probability γ = 0.01 and a learning rate inversely proportional to the maximum achievable reward fEMSE(V ). Fig. 4(a) presents the results for the temperature data set. Note that even after only a small number of rounds (≈ 100), the algorithm obtains 95% of the performance of the offline algorithm. After about 13,000 iterations, the algorithm obtains 99% of the offline performance, which is the best that can be expected with a .01 exploration probability. Fig. 4(b) show the same experiment on the precipitation data set. In this more complex problem, after 100 iterations, 76% of the offline performance is obtained, which increases to 87% after 500,000 iterations."
    }, {
      "heading" : "7.3 Observation dependent activation",
      "text" : "We also experimentally evaluate our OD-DOG algorithm with observation specific sensor activations. We choose different values for the activation cost cv, which we vary as multiples of the total achievable reward. The activation cost cv lets us smoothly trade off the average number of sensors activating each round and the average obtained reward. The resulting activation strategies are used to select a subset of size k = 10 from a collection of 12,527 sensors. Fig. 4(c) presents rates of convergence using the OD-DOG algorithm under a fixed objective function which considers all contamination events. In Fig. 4(d), convergence rates are presented under a varying objective function, which selects a different contamination event on each round. For low activation costs, the performance quickly converges to or exceeds the performance of the offline solution. Even under the lowest activation costs in our experiments, the average number of extra activations per stage in the OD-DOG algorithm is at most 5. These results indicate that observation specific activation can lead to drastically improved performance at small additional activation cost."
    }, {
      "heading" : "8. RELATED WORK",
      "text" : "Sensor Selection. The problem of deciding when to selectively turn on sensors in sensor networks in order to conserve power was first discussed by [?] and [?]. Many approaches for optimizing sensor placements and selection assume that sensors have a fixed region [?, ?, ?]. These regions are usually convex or even circular. Further, it is assumed that everything within a region can be perfectly observed, and everything outside cannot be measured by the sensors. For complex applications such as environmental monitoring, these assumptions are unrealistic, and the direct optimization of prediction accuracy is desired. The problem of selecting observations for monitoring spatial phenomena has been investigated ex-\ntensively in geostatistics [?], and more generally (Bayesian) experimental design [?]. Several approaches have been proposed to activate sensors in order to minimize uncertainty [?] or prediction error [?]. However, these approaches do not have performance guarantees. Submodularity has been used to analyze algorithms for placing [?] or selecting [?] a fixed set of sensors. These approaches however assume that the model is known in advance.\nSubmodular optimization. The problem of centralized maximization of a submodular function has been studied by [?], who proved that the greedy algorithm gives a factor (1− 1/e) approximation. Several algorithms have since been developed for maximizing submodular functions under more complex constraints (see [?] for an overview). Streeter and Golovin developed an algorithm for online optimization of submodular functions, which we build on in this paper [?]."
    }, {
      "heading" : "9. CONCLUSIONS",
      "text" : "In this paper, we considered the problem of repeatedly selecting subsets St from a large set of deployed sensors, in order to maximize a sequence of submodular utility functions f1, . . . , fT . We developed an efficient Distributed Online Greedy algorithm DOG, and proved it suffers no (1− 1/e)regret, essentially the best possible performance obtainable unless P = NP. Our algorithm is fully distributed, requiring only a small number of messages to be exchanged at each round with high probability. We analyze our algorithm both in the broadcast model, and in the star network model, where a separate base station is responsible for computing utilities of selected sets of sensors. Our LAZYDOG algorithm for the latter model uses lazy renormalization in order to reduce the number of messages required from Θ(n) to O(k log n), and the server memory required from Θ(n) to O(k + log n), where k is the desired number of sensors to be selected. In addition, we developed OD-DOG, an extension of DOG that allows observation-dependent sensor selection. We empirically demonstrate the effectiveness of our algorithms on three real-world sensing tasks, demonstrating how our DOG algorithm’s performance converges towards the performance of a clairvoyant offline greedy algorithm. In addition, our results with the OD-DOG algorithm indicate that a small number of extra sensor activations can lead to drastically improved convergence. We believe that our results provide an interesting step towards a principled study of distributed active learning and information gathering.\nAcknowledgments. The authors wish to thank Phillip Gibbons and the anonymous referees for their valuable help and suggestions. This research was partially supported by ONR grant N00014-09-1-1044, NSF grant CNS-0932392, a gift from Microsoft Corporation and the Caltech Center for the Mathematics of Information.\nAPPENDIX"
    }, {
      "heading" : "A. RESULTS IN THE BROADCAST MODEL",
      "text" : "PROOF OF THEOREM 2. To prove the regret bounds, note that in every round the distribution over sensor selections in the variant of EXP3 we describe (that uses the distributed multinomial sampling scheme and repeatedly reruns the protocol in order to always select some sensor in each round) is precisely the same as the original EXP3. Thus the regret bounds for EXP3 [?] carry over unchanged. We next bound the number of broadcasts. Fix a round, and let S set of sensors that activate in that round. The total number of broadcasts is then |S| + 2; using their calibrated clocks, each sensor (re)samples Xv ∼ Poisson(αpv) and activates if Xv ≥ 1. If no sensors activate before a specified timeout period, the default behavior is to rerun the sampling step. Eventually |S| ≥ 1 sensors activate in the same period. A distinguished sensor in S then determines the selected sensor v, broadcasts id(v), and v broadcasts its observed reward. We prove E [|S|] ≤ α/(1 − e−α) in Proposition 8. When α = 1, this gives us the claimed bound on the number of broadcasts.\nPROPOSITION 8. Rerunning the Poisson Multinomial Sampling Protocol until an element is selected results in at most α/(1− e−α) elements being activated in expectation. Moreover, this value is tight.\nPROOF. Let Xv ∼ Bernoulli(α · pv) be the indicator ran-\ndom variable for the activation of v, and let X := ∑ vXv. The expected number of sensor activations is then\nE [X | X ≥ 1] = E [X] /Pr [X ≥ 1] .\nIn the limit as maxv pv tends to zero, X converges to a Poisson random variable with mean α. In this case, E[X]Pr[X≥1] = α/(1 − e−α) To see that this is an upper bound, consider an arbitrary distribution p on the sensors, and fix some v with x := pv > 0. We claim that replacing v with two sensors v1 and v2 with positive probability mass x1 and x2 with x = x1 + x2 can only serve to increase the expected number of sensor activations, because E [X] is unchanged, and Pr [X ≥ 1] decreases. The latter is true essentially because Pr [∃i ∈ {1, 2} : vi activates] = 1 − (1 − x1)(1 − x2) = x− x1x2 < x. To complete the proof, notice that repeating this process with v = arg max(pv) and xi = x/2 ensures X converges to a Poisson variable with mean α, while only increasing E [X | X ≥ 1]."
    }, {
      "heading" : "B. RESULTS IN THE STAR NETWORK MODEL",
      "text" : "In this section we will prove that lazy renormalization samples sensors from a proper scaled distribution (1 − e−α)pv where pv is the input distribution. We then bound the communication overhead of using lazy renormalization for any MAB algorithm satisfying certain assumptions enumerated\nbelow, and then show how these bounds apply to EXP3.\nPROPOSITION 9. The lazy renormalization scheme of Sec. 5.1, described in pseudocode in Fig. 2, samples v with probability (1 − e−α)pv, where pv = ρ(wv(t), Z(t)) is the desired probability mass for v.\nPROOF. Lazy renormalization selects each sensor v with probability (1− e−α)pv , because of the way the random bits rv are shared in order to implement a coupled distribution for sensor activation and selection. Note that it would be sufficient to run the Poisson Multinomial Sampling Protocol on the correct (possibly oversampled) probabilities, αpv, since then Prop. 1 ensures that each v is selected with probability (1 − e−α)pv. The difficulty is that v does not have access to the correct normalizer Z(t), but only its estimate (lower bound) for it, Zv(t). To overcome this difficulty, we define a joint probability distribution over two random variables (Xv, Yv), where\nXv = Xv(R) := { 1 if R ≥ 1− α · ρ(wv(t), Zv(t)) 0 otherwise\nYv = Yv(R) := min\n{ b : b∑\na=0\ne−λλa\na! ≥ R\n}\nand λ := α · ρ(wv(t), Z(t)), and R is sampled uniformly at random from [0, 1]. Now, note that Yv is distributed as Poisson(λ). Also note that Yv ≥ 1 implies Xv ≥ 1, because Yv ≥ 1 implies R ≥ e−λ and\ne−λ ≥ 1− λ ≥ 1− α · ρ(wv(t), Zv(t)) since 1 + x ≤ ex for all x ∈ R, and ρ(wv(t), Zv(t)) ≥ ρ(wv(t), Z(t)) due to fact that Zv(t) ≤ Z(t). It follows that we can use the event Xv ≥ 1 as a conservative indicator that v should activate. In this case, it will send its sampled value for R, namely rv, and its weight wv(t) to the server. The server knows Z(t), and then can use rv and wv(t) to compute Yv(rv), the sample from Poisson(λ) that v would have drawn had it known Z(t). The resulting distribution on selected sensors is thus exactly the same as in the Poisson Multinomial Sampling Protocol without lazy renormalization. Invoking Prop. 1 thus completes the proof.\nWe now describe the assumptions that are sufficient to ensure lazy renormalization has low communication costs. Fix an action v and a multiarmed bandit algorithm. Let pv(t) ∈ [0, 1] be the random variable denoting the probability the algorithm assigns to v on round t. The value of pv(t) depends on the random choices made by the algorithm and the payoffs observed by it on previous rounds. We assume the following about each pv(t).\n1. pv(t) can be computed from local information v possesses and global information the server has.\n2. There exists an > 0 such that pv(t) ≥ for all t. 3. pv(t) < pv(t+ 1) implies v was selected in round t.\n4. There exists ̂ > 0 such that pv(t+ 1) ≥ pv(t)/(1 + ̂) for all t.\nMany MAB algorithms satisfy these conditions. For example, all MAB algorithms with non-trivial no-regret guarantees against adversarial payoff functions must continually explore all their options, which effectively mandates pv(t) ≥ for some > 0. In Lemma 1 we prove that EXP3 does so with = γ/n and ̂ = (e − 1) γn , assuming payoffs in [0, 1]. In this case, Theorem 10 bounds the expected increase in sensor communications due to lazy renormalization by a factor of 1 + e−1α .\nTHEOREM 10. Fix a multiarmed bandit instance with possibly adversarial payoff functions, and a MAB algorithm satisfying the above assumptions on its distribution over actions {pv(t)}v∈V . Let qv(t) be the corresponding random estimates for pv(t) maintained under lazy renormalization with oversampling parameter α. Then for all v and t,\nE [qv(t)/pv(t)] ≤ 1 + ̂\nα\nand\nE [qv(t)] ≤ ( 1 + ̂\nα\n) E [pv(t)] .\nPROOF. Fix v, and let p(t) := pv(t), q(t) := qv(t). We begin by bounding Pr [q(t) ≥ λp(t)] for λ ≥ 1. Let t0 be the most recent round in which q(t0) = p(t0). We assume q(0) = p(0), so t0 exists. Then q(t) = p(t0) ≥ λp(t) implies p(t0)/p(t) ≥ λ. By assumption p(t′)/p(t′+ 1) ≤ (1 + ̂) for all t′, so p(t0)/p(t) ≤ (1+ ̂)t−t0 . Thus λ ≤ (1+ ̂)t−t0 and t− t0 ≥ ln(λ)/ ln(1 + ̂). Define t(λ) := ln(λ)/ ln(1 + ̂).\nBy definition of t0, there were no activations under lazy renormalization in rounds t0 through t− 1 inclusive, which occurs with probability\n∏t−1 t′=t0\n(1−αq(t′)) = (1−αq(t))t−t0 ≤ (1 − αq(t))dt(λ)e, where α is the oversampling parameter in the protocol. We now bound E [q(t)/p(t) | q(t)]. Recall that E [X] = ∫∞ x=0\nPr [X ≥ x] dx for any non-negative random variable X . It will also be convenient to define ω := ln(1/(1− αq(t)))/ ln(1 + ̂) and assume for now that ω > 1. Conditioning on q(t), we see that\nE [q(t)/p(t) | q(t)] = ∫∞ λ=0\nPr [q(t) ≥ λp(t)] dλ = 1 + ∫∞ λ=1\nPr [q(t) ≥ λp(t)] dλ ≤ 1 + ∫∞ λ=1\n(1− αq(t))t(λ)dλ = 1 + ∫∞ λ=1 λln(1−αq(t))/ ln(1+̂)dλ = 1 + ∫∞ λ=1 λ−ωdλ\n= 1 + 1ω−1\nUsing ln (\n1 1−x\n) ≥ x for all x < 1 and ln(1 + x) ≤ x for\nall x > −1, we can show that ω ≥ αq(t)/̂ so 1 + 1ω−1 ≤ αq(t)/(αq(t) − ̂). Thus, if αq(t) > ̂ then ω > 1 and we obtain E [q(t)/p(t) | q(t)] ≤ αq(t)/(αq(t)− ̂).\nIf q(t) >> ̂, this gives a good bound. If q(t) is small, we rely on the assumption that p(t) ≥ for all t to get a trivial bound of q(t)/p(t) ≤ q(t)/ . We thus conclude E [q(t)/p(t) | q(t)] ≤ min (αq(t)/(αq(t)− ̂), q(t)/ ) . (B.1) Setting q(t) = (̂/α+ ) to maximize this quantity yields an unconditional bound of E [q(t)/p(t)] ≤ 1 + ̂/α .\nTo bound E [q(t)] in terms of E [p(t)], note that for all q\nq/E [p(t) | q(t) = q] ≤ E [q(t)/p(t) | q(t) = q] ≤ 1 + ̂/α\nwhere the first line is by Jensen’s inequality, and the second is by equation B.1. Thus q ≤ (1 + ̂/α )E [p(t) | q(t) = q] for all q. Taking the expectation with respect to q then proves E [qv(t)] ≤ ( 1 + ̂α ) E [pv(t)] as claimed.\nLEMMA 1. EXP3 with η = γ/n satisfies the conditions of Theorem 10 with = γ/n and ̂ = (e− 1) γn .\nPROOF. The former equality is an easy observation. To prove the latter equality, fix a round t and a selected action v. Let wv(t) be the weight of v in round t, and W (t) be the total weight of all actions in round t. Let π be the payoff to v in round t. Given the update rule wv(t + 1) = wv(t) exp ( γ n π(v,t) pv(t) ) , only the probabilities of the other actions will be decreased. It is not hard to see that they will be decreased by a multiplicative factor of at most W (t)/W (t+ 1), no matter what the learning parameter γ is. By the update rule,\nW (t+ 1) = W (t) + wv(t)\n( exp ( γ\nn\nπ\npv(t)\n) − 1 ) .\nLet p := pv(t) and x := γnπ. Dividing the above equation by W (t), we get\nW (t+ 1)\nW (t) = 1 + p (exp (x/p)− 1) (B.2)\n≤ 1 + p ( x/p+ (e− 2)(x/p)2 ) (B.3)\n≤ 1 + x+ (e− 2)x2/p (B.4) where in the second line we have used ex ≤ 1+x+(e−2)x2 for x ∈ [0, 1]. Note π ≤ 1 implies x ≤ γ/n ≤ p, so W (t+1) W (t) ≤ 1+(e−1)x ≤ 1+(e−1) γ n . It follows that setting ̂ = (e−1) γn is sufficient to ensure pv(t+1) ≥ pv(t)/(1+ ̂) for all t.\nWe now prove Theorem 4 and Corollary 5.\nPROOF OF THEOREM 4.. We prove in Lemma 1 that EXP3 satisfies the conditions of Theorem 10 with = γ/n and\n̂ = (e− 1) γn . Thus by Theorem 10\nE\n[∑\nv\nqv(t)\n] ≤ (1 + (e− 1)/α)E [∑\nv\npv(t)\n]\n= (1 + (e− 1)/α) because ∑ v pv(t) = 1. Each sensor v activates with probability αqv(t), so the expected number of activations is\nE [ α ∑\nv\nqv(t)\n] ≤ α (1 + (e− 1)/α) .\nThat proves the claimed bounds in expectation. To prove bounds with high probability, note that a sensor activates with probability αqv(t) in round t, where qv(t) is a random variable. Fix t. Let [E ] denote the indicator variable for the event E , i.e., [E ] = 1 if E occurs, and [E ] = 0 otherwise. Then we can write [v activates in round t] = [αqv(t) ≥ R], where R is sampled uniformly at random from [0, 1] and R is independent of qv(t). Then if fR is the probability density functions of R we can write\nPr [R ≤ αqv(t)] = ∫ 1\nr=0\nPr [αqv(t) ≥ R | R = r] fR(r)dr\n=\n∫ 1\nr=0\nPr [αqv(t) ≥ r] fR(r)dr\n=\n∫ 1\nr=0\nPr [αqv(t) ≥ r] dr\n= E [αqv(t)]\nThus the number of sensor activations is a sum of |V | binary random variables with cumulative mean µ := ∑ v E [αqv(t)]. We have already bounded this mean as µ ≤ α+(e−1). From here a simple application of a Chernoff-Hoeffding bound suffices to prove that with high probability this sum is at most O(α + log n). Let A be the number of sensor activations. Then, e.g., Theorem 5 of [?] immediately implies\nPr [A ≥ µ(1 + δ)] ≤ exp ( − δ 2µ2\n2µ+ 2δµ3\n)\nFor δ ≥ 1, this yields Pr [A ≥ µ(1 + δ)] ≤ exp ( − 3δµ8 ) . Setting δ = 1 + 8c lnn3µ ensures this probability is at most n−c, hence Pr [ A ≥ 2µ+ 83c lnn ] ≤ n−c. Noting that µ ≤ α + (e − 1) completes the high probability bound on the number of activations.\nAs for the number of messages, note that each message involves a sensor as sender or receiver, and by inspection the protocol only involves two messages per activated node.\nPROOF OF COROLLARY 5.. Use the distributed EXP3 protocol with lazy renormalization with α = lnn. We have already established that the probability of nothing being selected is e−α or 1/n in this case. If nothing is selected,\nsend out n messages, one to each sensor, to rerun the protocol. The expected number of messages sent to initiate additional runs of the protocol is ∑∞ x=1 nx/n\nx = (1− 1/n)−2 = 1+O(1/n). LetX be the number of sensor activations. As in the proof of Proposition 8, if Y is the expected number of sensor activations without rerunning the protocol when nothing is selected, then E [X] = E [Y ] /Pr [Y ≥ 1]. By Theorem 4 E [Y ] ≤ α (1 + (e− 1)/α). Since Pr [Y ≥ 1] = 1 − e−α, we conclude\nE [X] ≤ lnn+ (e− 1) +O ( lnn\nn\n) .\nThe with-high-probability bounds on the number of sensor activations are proved as in the proof of Corollary 5.\nAs for the number of messages, note that other than messages sent to initiate additional runs of the protocol, there are only two messages per activated node. Finally, the regret bounds for distributed EXP3 are the same as standard EXP3 because by design the two algorithms select sensors from exactly the same distribution in each round. Note that the distribution in any given round is a random object depending on the algorithm’s choices in the previous rounds, however on each round the distribution on distributions is the same for both EXP3 variants, as can be readily proved by induction on the round number.\nC. ALGORITHM OGUNIT WITH FAULTY ACTIONS\nIn order to prove Theorem 7, we need a guarantee on the performance of OGUNIT if its elements are may fail to give any benefit. We provide this in the form of Theorem 11.\nSuppose we run DOG with the Poisson Multinomial Sampling Protocol with lazy renormalization, and do not resample on stages where no sensor activates. Then with some probability during any given stage i ∈ [k], no sensors activate and the server receives no information. Suppose that this probability is at most δ in each stage. We have shown in section 4.1 that δ ≤ e−α where α is the oversampling parameter. We claim that we can compensate for this possibility by running DOG for k/(1− δ) stages in each round rather than k, because of the following guarantee for OGUNIT.\nTHEOREM 11. Fix finite set V , k ∈ N, and a sequence of monotone submodular functions f1, . . . , fT : 2V → [0, 1]. Let OPTk = maxS⊂V,|S|≤k ∑t t=1 ft(S). For all v ∈ V let v′ be a random element which is v with probability 1−δv and is null7 with probability δv. Let f ′t(S′) := E [ft(S)] where S the set obtained by including every element v′ of S′ in it independently with probability δv. Let S′1, . . . , S ′ T be the sequence of random sets obtained from running OGUNIT with actions V ′ := {v′ : v ∈ V } and objective functions {f ′t}Tt=1 7Here, a null element always contributes nothing in the way of utility, so that ft(S ∪ {null}) = ft(S) for all t and S.\nand k′ = k/(1 − δ) stages, where δ = maxv δv. Suppose the algorithms for each stage have expected regret at most r. Then\nE\n[ T∑\nt=1\nf ′t(S ′ t)\n] ≥ (\n1− 1 e\n) OPTk − ⌈ k\n1− δ\n⌉ r.\nPROOF. It suffices to prove the analogous result in the offline case; the “meta-actions” analysis in [?] can then be used to complete the proof. So consider a set of elements V and the “faulty” versions V ′. Fix a monotone submodular f : 2V → [0, 1] and define f ′ as above. Run the offline greedy algorithm on f ′ to try to find the best set of k′ = k1−δ elements in V ′. Let g′i be the chosen element in stage i, and let G′i = { g′j : 1 ≤ j ≤ i } . Let Gi denote the realization of G′i after sampling, so that Gi ⊆ {g : g′ ∈ G′i}. Let S∗ = arg maxS⊆V,|S|≤k(f(S)). We claim that for all i\nE [ f(G′i+1)− f(G′i) ∣∣ Gi ] ≥ (1− δ) f(S ∗)− f(Gi) k\nbecause\nf(S∗)− f(Gi) ≤ f(Gi ∪ S∗)− f(Gi) ≤ ∑\nv∈S∗ (f(Gi + v)− f(Gi))\n≤ k ·max v (f(Gi + v)− f(Gi))\nand maxv′ (E [f(G′i + v′)− f(G′i) | Gi]) is at least equal to (1−δ) maxv (f(Gi + v)− f(Gi)). Removing the conditioning on Gi we get\nE [ f(G′i+1)− f(G′i) ] ≥ (1− δ) f(S ∗)− E [f(G′i)] k\nLet Φ(i) = f(S∗) − E [f(G′i)]. The previous equation implies Φ(i + 1) ≤ Φ(i) ( 1− 1−δk ) . By induction Φ(i) ≤ f(S∗) ( 1− 1−δk )i . Using 1 − x ≤ e−x we conclude that\nΦ(dk/(1− δ)e) ≤ f(S∗)/e and f ′(Gk′) ≥ ( 1− 1e ) f(S∗).\nWe are now ready to prove Theorem 7.\nPROOF THEOREM 7. To bound the number of sensor activations, we note there are k′ := dk/(1− e−α)e rounds, and each round activates at most α+(e−1) sensors in expectation andO(α+log n) sensors with high probability by Theorem 4 (which proves these bounds in the higher communication case where we do rerun the PMS Protocol protocol if nothing is selected). This, and the fact that α/(1 − e−α) = O(α) for α > 0 yields the claimed activation bounds. It is an easy observation that the number of messages is at most twice the number of activations. Clearly, at most one sensor per stage is activated, so at most k′ are activated over one round. Finally, the regret bound follows from Theorem 11, using δ = e−α."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2010,
    "abstractText" : "A key problem in sensor networks is to decide which sensors<lb>to query when, in order to obtain the most useful information<lb>(e.g., for performing accurate prediction), subject to con-<lb>straints (e.g., on power and bandwidth). In many applications<lb>the utility function is not known a priori, must be learned<lb>from data, and can even change over time. Furthermore for<lb>large sensor networks solving a centralized optimization prob-<lb>lem to select sensors is not feasible, and thus we seek a fully<lb>distributed solution. In this paper, we present Distributed<lb>Online Greedy (DOG), an efficient, distributed algorithm for<lb>repeatedly selecting sensors online, only receiving feedback<lb>about the utility of the selected sensors. We prove very strong theoretical no-regret guarantees that apply whenever the (un-<lb>known) utility function satisfies a natural diminishing returns<lb>property called submodularity. Our algorithm has extremely<lb>low communication requirements, and scales well to large sensor deployments. We extend DOG to allow observation-<lb>dependent sensor selection. We empirically demonstrate the<lb>effectiveness of our algorithm on several real-world sensing<lb>tasks.",
    "creator" : "TeX"
  }
}