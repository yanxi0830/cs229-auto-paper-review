{
  "name" : "1503.06960.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Proper PAC learning is compressing",
    "authors" : [ "Shay Moran", "Amir Yehudayoff" ],
    "emails" : [ "shaymrn@cs.technion.ac.il.", "amir.yehudayoff@gmail.com." ],
    "sections" : [ {
      "heading" : null,
      "text" : "We prove that proper PAC learnability implies compression. Namely, if a concept C ⊆ ΣX is properly PAC learnable with d samples, then C has a sample compression scheme of size 2O(d). In particular, every boolean concept class with constant VC dimension has a sample compression scheme of constant size. This answers a question of Littlestone and Warmuth (1986). The proof uses an approximate minimax phenomenon for boolean matrices of low VC dimension."
    }, {
      "heading" : "1 Introduction",
      "text" : "Learning and compression are known to be deeply related to each other. Learning procedures perform compression, and compression is an evidence of and is useful in learning. For example, support vector machines, which are commonly applied to solve classification problems, perform compression (see Chapter 6 in [6]), and compression can be used to boost the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]).\nAbout thirty years ago, Littlestone and Warmuth [16] provided a mathematical framework for studying compression in the context of learning theory. In a nutshell, they showed that compression implies learnability and asked whether learnability implies compression."
    }, {
      "heading" : "1.1 Definitions",
      "text" : "Concepts and samples. Let Σ, X be finite sets (we focus on this case to eliminate measurability and similar issues but the arguments presented here are more general). A concept is a function c : X → Σ. A concept class C ⊆ ΣX is a collection of concepts. A subset Y of X is thought of as a collection of sample points. For Y ⊆ X and c ∈ C, let c|Y be the restriction of the function c to the set Y . We think of c|Y as the labeling ∗Departments of Computer Science, Technion-IIT, Israel and Max Planck Institute for Informatics, Saarbrücken, Germany. shaymrn@cs.technion.ac.il. †Department of Mathematics, Technion-IIT, Israel. amir.yehudayoff@gmail.com. Horev fellow – supported by the Taub foundation. Research is also supported by ISF and BSF.\nar X\niv :1\n50 3.\n06 96\n0v 1\n[ cs\n.L G\n] 2\n4 M\nar 2\nof Y according to c. A C-labelled sample is a pair (Y, y), where Y ⊆ X and y = c|Y for some c ∈ C. The size of a C-labelled sample (Y, y) is |Y |. For an integer k, denote by LC(k) the set of C-labelled samples of size at most k. Denote by LC(∞) the set of all C-labelled samples of finite size.\nPAC learning. Probably approximately correct (PAC) learning was defined in Valiant’s seminal work [25]. We use the following definition. The concept class C is PAC learnable with d samples if there is a map that generates hypotheses H : LC(d) → ΣX so that for every c ∈ C and for every probability distribution µ on X,\nPr µd\n[{ Y ∈ Xd : µ({x ∈ X : hY (x) 6= c(x)}) ≤ 1/3 }] ≥ 2/3,\nwhere hY = H(Y, c|Y ). Roughly speaking, an hypothesis generated by H using d independent samples is a µ-approximation of c with reasonable probability. If the image of H is contained in C, we say that C is properly PAC learnable.\nVC dimension. A boolean concept class is C ⊆ {0, 1}X . A set Y ⊆ X is shattered in C if for every Z ⊆ Y there is c ∈ C so that c(x) = 1 for all x ∈ Z and c(x) = 0 for all x ∈ Y − Z. The Vapnik-Chervonenkis (VC) dimension of C, denoted VC(C), is the maximum size of a shattered set in C [26].\nA fundamental and well-known result of Blumer, Eherenfeucht, Haussler, and Warmuth [4], which is based on an earlier work of Vapnik and Chervonenkis [26], states that every boolean concept class C can be properly PAC learned with1 O(VC(C)) examples (in fact the sample complexity of PAC learning for boolean classes is captured by the VC dimension).\nSample compression schemes. Sample compression schemes were defined by Littlestone and Warmuth [16]. Roughly speaking, a sample compression scheme takes a long list of samples and compresses it to a short sub-list of samples in a way that allows to invert the compression. Formally, a k-sample compression scheme for C with information I, where I is a finite set, consists of two maps κ, ρ for which the following hold:\n(κ) The compression map\nκ : LC(∞)→ LC(k)× I\ntakes (Y, y) to ((Z, z), i) with Z ⊆ Y and y|Z = z.\n(ρ) The reconstruction map\nρ : LC(k)× I → ΣX\n1Big O and Ω notation means up to absolute constants.\nis so that for all (Y, y) in LC(∞),\nρ(κ(Y, y))|Y = y.\nThe size of the scheme is2 k + log(|I|+ 1), and its kernel size is k. In the language of coding theory, the side information I can be thought of as list decoding; the map ρ has a short list of possible reconstructions of a given (Z, z), and the information i indicates which element in the list is the correct one.\nSee [9, 10, 18] for more discussions of this definition, and some insightful examples."
    }, {
      "heading" : "1.2 Background",
      "text" : "Littlestone and Warmuth [16] proved that compression implies learnability (see Theorem 1.1 below), and asked whether learnability implies compression for boolean concept classes: “Are there concept classes with finite dimension for which there is no scheme with bounded kernel size and bounded additional information?”\nThis question and variants of it lead to a rich body of work that revealed profound properties of VC dimension and learning. These works also discovered and utilized connections between sample compression schemes, and model theory, topology, combinatorics, and geometry.\nFloyd and Warmuth [9, 10] constructed sample compression schemes of size log |C| for every concept class C. Freund [11] showed how to compress a sample of size m to a sample of size O(d log(m)) with some side information for boolean classes of VC dimension d.\nAs the study of sample compression schemes deepened, many insightful and optimal schemes for special cases have been constructed: Floyd [9], Helmbold et al. [12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al. [22], Rubinstein and Rubinstein [23], Livni and Simon [17] and more.\nFinally, in our recent work with Shpilka and Wigderson [18], we constructed sample compression schemes of size O(d · 2d · log log |C|) using some side information for every boolean concept class C of VC dimension d.\nCompression implies learnability. Littlestone and Warmuth proved that the sample complexity of PAC learning is at most (roughly) the size of a compression scheme [16].\nTheorem 1.1 (Compression implies learnability [16]). Let C ⊆ ΣX and c ∈ C. Let µ be a distribution on X, and x1, . . . , xm be m independent samples from µ. Let Y = (x1, . . . , xm)\n2Logarithms in this text are of base two.\nand y = c|Y . Let κ, ρ be a k-sample compression scheme for C with additional information I. Let h = ρ(κ(Y, y)). Then, for every > 0,\nPr µm\n[ µ({x ∈ X : h(x) 6= c(x)}) > ] < |I| k∑ j=0 ( m j ) (1− )m−j.\nIn particular, C can be PAC learned with O(k log(k) + log(|I|+ 1)) samples.\nProof sketch. There are ∑k\nj=0 ( m j ) subsets T of [m] of size at most k. There are |I| choices\nfor i ∈ I. Each choice of T, i yields a function hT,i = ρ((T, yT ), i) that is measurable with respect to xT = (xt : t ∈ T ). The function h is one of the functions in {hT,i : |T | ≤ k, i ∈ I}. For each hT,i, the coordinates in [m] − T are independent, and so if µ({x ∈ X : hT,i(x) 6= c(x)}) > then the probability that all these m− |T | samples agree with c is less than (1− )m−|T |. The union bound completes the proof."
    }, {
      "heading" : "1.3 Learning is compressing",
      "text" : "Our main theorem says that proper PAC learnability implies sample compression schemes of constant size.\nTheorem 1.2 (Proper learnability implies compression). If C ⊆ ΣX is properly PAC learnable with d samples, then C has a sample compression scheme of size 2O(d).\nThe theorem specifically answers Littlestone and Warmuth’s question [16]; every boolean concept class of finite VC dimension has a sample compression scheme of finite size. The theorem, however, only provides an exponential dependence on d, whereas many of the known compression schemes for special cases (e.g. [10, 3, 13, 23, 17]) have size O(d). Warmuth’s question [27] whether O(d)-sample compression schemes always exist remains open.\nOur construction (see Section 3) of sample compression schemes is overall quite short and simple, but uses a different perspective of the problem than in previous work (mentioned above). It is inspired by Freund’s work [11] where majority is used to boost the accuracy of learning procedures. It also uses several known properties of PAC learnability and VC dimension, together with von Neumann’s minimax theorem (these appear in Section 2)."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Sample complexity. There are many generalization of VC dimension to non-boolean concept classes (see [2] and references within). Here we use the following one. Let C ⊆ ΣX .\nFor every c ∈ C, define a boolean concept class Bc ⊆ {0, 1}X as the set of all bh, for h ∈ C, defined by bh(x) = 1 if and only if h(x) = c(x). Define the distinguishing dimension of C as\nDD(C) = max{VC(Bc) : c ∈ C}.\nThis definition of dimension is similar to notions used in [19, 7, 2]. If C is boolean then VC(C) = DD(C).\nVapnik and Chervonenkis [26] and Blumer et al. [4] proved that VC dimension is equivalent to the sample complexity of PAC learning. The distinguishing dimension is a lower bound on the sample complexity of PAC learning (see [4, 8, 2]).\nTheorem 2.1 (Lower bound for sample complexity [4, 8, 2]). The number of samples needed to PAC learn C is at least Ω(DD(C)).\nDual classes. Let C ⊆ {0, 1}X be a boolean concept class. The dual concept class C∗ ⊆ {0, 1}C of C is defined as the set of all functions fx : C → {0, 1} so that fx(c) = 1 if and only if c(x) = 1. If we think of C as a binary matrix whose rows are concepts in C and columns are elements of X, then C∗ corresponds to the distinct rows of the transposed matrix. Assouad [1] bounded VC(C∗) in terms of VC(C).\nClaim 2.2 (VC dimension of dual [1]). If VC(C) = d then VC(C∗) ≤ 2d+1.\nApproximations. The following theorem shows that every distribution can be approximated by a distribution of small support, when the statistical tests belong to a class of small VC dimension. This phenomenon was first proved by Vapnik and Chervonenkis [26], and was later quantitively improved in [14, 24].\nTheorem 2.3 (Approximations for bounded VC dimension [26, 14, 24]). Let C ⊆ {0, 1}X of VC dimension d. Let µ be a distribution on X. For all > 0, there exists a multi-set Y ⊆ X of size |Y | ≤ O(d/ 2) such that for all c ∈ C,∣∣∣∣µ({x ∈ X : c(x) = 1})− |{x ∈ Y : c(x) = 1}||Y | ∣∣∣∣ ≤ .\nMinimax. Von Neumann’s minimax theorem [20] is a seminal result in game theory (see the textbook [21]). Assume that there are 2 players, a row player and a column player. A pure strategy of the row player is r ∈ [m] and a pure strategy of the column player is j ∈ [n]. Let M be a boolean matrix so that M(r, j) = 1 if and only if the row player wins the game when the pure strategies r, j are played.\nThe minimax theorem says that if for every mixed strategy (a distribution on pure strategies) q of the column player, there is a mixed strategy p of the row player that\nguarantees the row player wins with probability at least V , then there is a mixed strategy p of the row player so that for all mixed strategies q of the column player, the row player wins with probability at least V . A similar statement holds for the column player. This implies that there is a pair of mixed strategies that form a Nash equilibrium (see [21]).\nTheorem 2.4 (Minimax [20]). Let M ∈ Rm×n be a real matrix. Then,\nmin p∈∆m max q∈∆n ptMq = max q∈∆n min p∈∆m ptMq,\nwhere ∆` is the set of distributions on [`].\nThe arguments in the proof of Theorem 1.2 below imply the following variant of the minimax theorem, which may be of interest in the context of game theory. The minimax theorem holds for a general matrix M . In other words, there is no assumption on the set of winning/losing states in the game.\nWe observe that a combinatorial restriction on the winning/losing states in the game implies that there is an approximate efficient equilibrium state. Namely, if the rows of M have VC dimension d, then for every > 0, there is a multi-set of O(2d/ 2) pure strategies R ⊆ [m] for the row player, and a multi-set of O(d/ 2) pure strategies J ⊆ [n] for the column player, so that a uniformly random choice from R, J guarantees the players a gain that is -close to the gain in the equilibrium strategy.\nLipton, Markakis and Mehta [15] call such a pair of mixed strategies an -Nash equilibrium. They showed that in every game there are -Nash equilibriums with logarithmic support, and used this to find an approximate Nash equilibrium in quasi-polynomial time. The ideas presented here show that if the matrix of the game has constant VC dimension then there are -Nash equilibriums with constant support, and that consequently an approximate Nash equilibrium can be found in polynomial time."
    }, {
      "heading" : "3 A compression scheme",
      "text" : "In the proof of Theorem 1.2, we use the following simple lemma. The lemma can be seen as an approximate, combinatorial version of Carathéodory’s theorem from convex geometry. Let C ⊆ {0, 1}n ⊂ Rn and denote by K the convex hull of C in Rn. Carathéodory’s theorem says that every point p ∈ K is a convex combination of at most n + 1 points from C. Lemma 3.1 says that if C has constant VC dimension then every p ∈ K can be approximated by a convex combination of small support. Namely, if VC(C) = d then p can be -approximated in `∞ by a convex combination of at most O(2 d/ 2) points from C.\nLemma 3.1 (Sampling for bounded VC dimension). Let C ⊆ {0, 1}X of VC dimension d. Let p be a distribution on concepts in C, and let > 0. Then, there is a multi-set F ⊆ C of size |F | ≤ O(2d/ 2) so that for every x ∈ X,∣∣∣∣p({c ∈ C : c(x) = 1})− |{f ∈ F : f(x) = 1}||F | ∣∣∣∣ ≤ .\nProof. By Claim 2.2, the VC dimension of the dual class C∗ is at most 2d+1. Every x ∈ X corresponds to a concept in C∗. The distribution p is a distribution on the domain of the functions in C∗. The lemma follows by Theorem 2.3 applied to C∗."
    }, {
      "heading" : "3.1 The construction",
      "text" : "Proof of Theorem 1.2. Since C is properly PAC learnable with d samples, let\nH : LC(d)→ C\nbe so that for every c ∈ C and for every probability distribution q on X, there is Z ⊆ supp(q) of size |Z| ≤ d so that q({x ∈ X : hZ(x) 6= c(x)}) ≤ 1/3 where hZ = H(Z, c|Z).\nCompression. Let (Y, y) ∈ LC(∞). Let\nH = HY,y = {H(Z, z) : Z ⊆ Y, |Z| ≤ d, z = y|Z} ⊆ C.\nThe compression is based on the following claim.\nClaim 3.2. There are T sets Z1, Z2, . . . , ZT ⊆ Y , each of size at most d, with T ≤ K := 2O(d) so that the following holds. For t ∈ [T ], let\nft = H(Zt, y|Zt). (1)\nThen, for every x ∈ Y ,\n|{t ∈ [T ] : ft(x) = y(x)}| > T/2. (2)\nGiven the claim, the compression κ(Y, y) is defined as\nZ = ⋃ t∈[T ] Zt and z = y|Z .\nThe additional information i ∈ I allows to recover the sets Z1, . . . , ZT from the set Z. There are many possible ways to encode this information, but the size of I can be chosen\nto be at most kk with k := K · d+ 1 ≤ 2O(d).\nProof of Claim 3.2. By choice of H, for every distribution q on Y , there is h ∈ H so that\nq ({x ∈ Y : h(x) = y(x)}) ≥ 2/3.\nBy Theorem 2.4, there is a distribution p on H such that for every x ∈ Y ,\np({h ∈ H : h(x) = y(x)}) ≥ 2/3.\nLet B ⊆ {0, 1}Y be the set of concepts bh, for h ∈ H, defined by bh(x) = 1 if and only if h(x) = y(x). The distribution p induces a distribution pB on B so that for every x ∈ Y ,\npB({b ∈ B : b(x) = 1}) ≥ 2/3.\nSince C is PAC learnable with d samples, Theorem 2.1 implies DD(C) ≤ O(d). Hence, VC(B) ≤ O(d). By Lemma 3.1 applied to B and pB with = 1/8, there is a multi-set E ⊆ B of size |E| ≤ K := 2O(d) so that for every x ∈ Y ,\n|{e ∈ E : e(x) = 1}| |E| ≥ pB({b ∈ B : b(x) = 1})− 1/8 > 1/2.\nThe multi-set E ⊆ B corresponds to a multi-set F = {f1, f2, . . . , fT} ⊆ H of size T = |E| so that for every x ∈ Y ,\n|{t ∈ [T ] : ft(x) = y(x)}| > T/2. (3)\nFor every t ∈ [T ], let Zt be a subset of Y of size |Zt| ≤ d so that\nH(Zt, y|Zt) = ft.\nReconstruction. Given ((Z, z), i), the information i is interpreted as a list of T subsets Z1, . . . , ZT of Z, each of size at most d. For t ∈ [T ], let\nht = H(Zt, z|Zt).\nDefine h = ρ((Z, z), i) as follows: For every x ∈ X, let h(x) be a symbol that appears most in the list\nλx((Z, z), i) = (h1(x), h2(x), . . . , hT (x)),\nwhere ties are arbitrarily broken.\nCorrectness. Fix (Y, y) ∈ LC(∞). Let ((Z, z), i) = κ(Y, y) and h = ρ((Z, z), i). For x ∈ Y , consider the list\nφx(Y, y) = (f1(x), f2(x), . . . , fT (x))\ndefined in the compression process of (Y, y). The list φx(Y, y) is identical to the list λx((Z, z), i); this follows from Equation (1), from that i allows to correctly recover Z1, . . . , ZT , and from that y|Zt = z|Zt for all t ∈ [T ]. By (3), for every x ∈ Y , the symbol y(x) appears in more than half of the list λx((Z, z), i) so indeed h(x) = y(x)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Amir Shpilka and Avi Wigderson for helpful discussions. We also thank Ben Lee Volk for comments on an earlier version of this text."
    } ],
    "references" : [ {
      "title" : "Densite et dimension",
      "author" : [ "P. Assouad" ],
      "venue" : "Ann. Institut Fourter, 3:232–282",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "and P",
      "author" : [ "S. Ben-David", "N. Cesa-Bianchi", "D. Haussler" ],
      "venue" : "M. Long. Characterizations of learnability for classes of {0,...,n}-valued functions. J. Comput. Syst. Sci., 50(1):74– 86",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Combinatorial variability of Vapnik-Chervonenkis classes with applications to sample compression schemes",
      "author" : [ "S. Ben-David", "A. Litman" ],
      "venue" : "Discrete Applied Mathematics, 86(1):3–25",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Learnability and the Vapnik-Chervonenkis dimension",
      "author" : [ "A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M.K. Warmuth" ],
      "venue" : "J. Assoc. Comput. Mach., 36(4):929–965",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Externally definable sets and dependent pairs",
      "author" : [ "A. Chernikov", "P. Simon" ],
      "venue" : "Israel Journal of Mathematics, 194(1):409–425",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "An Introduction to Support Vector Machines and other kernel-based learning methods",
      "author" : [ "N. Cristianini", "J. Shawe-Taylor" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Universal Donsker classes and metric entropy",
      "author" : [ "R.M. Dudley" ],
      "venue" : "Ann. Probab., 15(4):1306–1326,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1987
    }, {
      "title" : "A general lower bound on the number of examples needed for learning",
      "author" : [ "A. Ehrenfeucht", "D. Haussler", "M.J. Kearns", "L.G. Valiant" ],
      "venue" : "Inf. Comput., 82(3):247–261",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Space-bounded learning and the vapnik-chervonenkis dimension",
      "author" : [ "S. Floyd" ],
      "venue" : "COLT, pages 349–364",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Sample compression",
      "author" : [ "S. Floyd", "M.K. Warmuth" ],
      "venue" : "learnability, and the vapnikchervonenkis dimension. Machine Learning, 21(3):269–304",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Boosting a weak learning algorithm by majority",
      "author" : [ "Y. Freund" ],
      "venue" : "Inf. Comput., 121(2):256–285",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Learning integer lattices",
      "author" : [ "D.P. Helmbold", "R.H. Sloan", "M.K. Warmuth" ],
      "venue" : "SIAM J. Comput., 21(2):240–266",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Unlabeled compression schemes for maximum classes",
      "author" : [ "D. Kuzmin", "M.K. Warmuth" ],
      "venue" : "Journal of Machine Learning Research, 8:2047–2081",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Improved bounds on the sample complexity of learning",
      "author" : [ "Y. Li", "P.M. Long", "A. Srinivasan" ],
      "venue" : "SODA, pages 309–318",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Playing large games using simple strategies",
      "author" : [ "R.J. Lipton", "E. Markakis", "A. Mehta" ],
      "venue" : "ACM Conference on Electronic Commerce, pages 36–41, New York, NY, USA",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Relating data compression and learnability",
      "author" : [ "N. Littlewood", "M. Warmuth" ],
      "venue" : "Unpublished",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Honest compressions and their application to compression schemes",
      "author" : [ "R. Livni", "P. Simon" ],
      "venue" : "COLT, pages 77–92",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Teaching and compressing for low VC-dimension",
      "author" : [ "S. Moran", "A. Shpilka", "A. Wigderson", "A. Yehudayoff" ],
      "venue" : "ECCC, TR15-025",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On learning sets and functions",
      "author" : [ "B.K. Natarajan" ],
      "venue" : "Machine Learning, 4:67–97",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Zur theorie der gesellschaftsspiele",
      "author" : [ "J. von Neumann" ],
      "venue" : "Mathematische Annalen,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1928
    }, {
      "title" : "Game Theory",
      "author" : [ "G. Owen" ],
      "venue" : "Academic Press",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Shifting: One-inclusion mistake bounds and sample compression",
      "author" : [ "B.I.P. Rubinstein", "P.L. Bartlett", "J.H. Rubinstein" ],
      "venue" : "J. Comput. Syst. Sci., 75(1):37–59",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A geometric approach to sample compression",
      "author" : [ "B.I.P. Rubinstein", "J.H. Rubinstein" ],
      "venue" : "Journal of Machine Learning Research, 13:1221–1261",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Sharper bounds for Gaussian and empirical processes",
      "author" : [ "M. Talagrand" ],
      "venue" : "Ann. Probab., 22(1):28–76",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "A theory of the learnable",
      "author" : [ "L.G. Valiant" ],
      "venue" : "Commun. ACM, 27:1134–1142",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "On the uniform convergence of relative frequencies of events to their probabilities",
      "author" : [ "V.N. Vapnik", "A.Ya. Chervonenkis" ],
      "venue" : "Theory Probab. Appl.,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1971
    }, {
      "title" : "Compressing to VC dimension many points",
      "author" : [ "M.K. Warmuth" ],
      "venue" : "COLT/Kernel, pages 743–744",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "For example, support vector machines, which are commonly applied to solve classification problems, perform compression (see Chapter 6 in [6]), and compression can be used to boost the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]).",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 15,
      "context" : "For example, support vector machines, which are commonly applied to solve classification problems, perform compression (see Chapter 6 in [6]), and compression can be used to boost the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]).",
      "startOffset" : 221,
      "endOffset" : 229
    }, {
      "referenceID" : 10,
      "context" : "For example, support vector machines, which are commonly applied to solve classification problems, perform compression (see Chapter 6 in [6]), and compression can be used to boost the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]).",
      "startOffset" : 221,
      "endOffset" : 229
    }, {
      "referenceID" : 5,
      "context" : "For example, support vector machines, which are commonly applied to solve classification problems, perform compression (see Chapter 6 in [6]), and compression can be used to boost the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]).",
      "startOffset" : 247,
      "endOffset" : 250
    }, {
      "referenceID" : 15,
      "context" : "About thirty years ago, Littlestone and Warmuth [16] provided a mathematical framework for studying compression in the context of learning theory.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 24,
      "context" : "Probably approximately correct (PAC) learning was defined in Valiant’s seminal work [25].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 25,
      "context" : "The Vapnik-Chervonenkis (VC) dimension of C, denoted VC(C), is the maximum size of a shattered set in C [26].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 3,
      "context" : "A fundamental and well-known result of Blumer, Eherenfeucht, Haussler, and Warmuth [4], which is based on an earlier work of Vapnik and Chervonenkis [26], states that every boolean concept class C can be properly PAC learned with O(VC(C)) examples (in fact the sample complexity of PAC learning for boolean classes is captured by the VC dimension).",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 25,
      "context" : "A fundamental and well-known result of Blumer, Eherenfeucht, Haussler, and Warmuth [4], which is based on an earlier work of Vapnik and Chervonenkis [26], states that every boolean concept class C can be properly PAC learned with O(VC(C)) examples (in fact the sample complexity of PAC learning for boolean classes is captured by the VC dimension).",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 15,
      "context" : "Sample compression schemes were defined by Littlestone and Warmuth [16].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "See [9, 10, 18] for more discussions of this definition, and some insightful examples.",
      "startOffset" : 4,
      "endOffset" : 15
    }, {
      "referenceID" : 9,
      "context" : "See [9, 10, 18] for more discussions of this definition, and some insightful examples.",
      "startOffset" : 4,
      "endOffset" : 15
    }, {
      "referenceID" : 17,
      "context" : "See [9, 10, 18] for more discussions of this definition, and some insightful examples.",
      "startOffset" : 4,
      "endOffset" : 15
    }, {
      "referenceID" : 15,
      "context" : "2 Background Littlestone and Warmuth [16] proved that compression implies learnability (see Theorem 1.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 8,
      "context" : "Floyd and Warmuth [9, 10] constructed sample compression schemes of size log |C| for every concept class C.",
      "startOffset" : 18,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "Floyd and Warmuth [9, 10] constructed sample compression schemes of size log |C| for every concept class C.",
      "startOffset" : 18,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : "Freund [11] showed how to compress a sample of size m to a sample of size O(d log(m)) with some side information for boolean classes of VC dimension d.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 8,
      "context" : "As the study of sample compression schemes deepened, many insightful and optimal schemes for special cases have been constructed: Floyd [9], Helmbold et al.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 12,
      "context" : "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 21,
      "context" : "[22], Rubinstein and Rubinstein [23], Livni and Simon [17] and more.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[22], Rubinstein and Rubinstein [23], Livni and Simon [17] and more.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 16,
      "context" : "[22], Rubinstein and Rubinstein [23], Livni and Simon [17] and more.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "Finally, in our recent work with Shpilka and Wigderson [18], we constructed sample compression schemes of size O(d · 2 · log log |C|) using some side information for every boolean concept class C of VC dimension d.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 15,
      "context" : "Littlestone and Warmuth proved that the sample complexity of PAC learning is at most (roughly) the size of a compression scheme [16].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "1 (Compression implies learnability [16]).",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 15,
      "context" : "The theorem specifically answers Littlestone and Warmuth’s question [16]; every boolean concept class of finite VC dimension has a sample compression scheme of finite size.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "[10, 3, 13, 23, 17]) have size O(d).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "[10, 3, 13, 23, 17]) have size O(d).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "[10, 3, 13, 23, 17]) have size O(d).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 22,
      "context" : "[10, 3, 13, 23, 17]) have size O(d).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 16,
      "context" : "[10, 3, 13, 23, 17]) have size O(d).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 26,
      "context" : "Warmuth’s question [27] whether O(d)-sample compression schemes always exist remains open.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "It is inspired by Freund’s work [11] where majority is used to boost the accuracy of learning procedures.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "There are many generalization of VC dimension to non-boolean concept classes (see [2] and references within).",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : "This definition of dimension is similar to notions used in [19, 7, 2].",
      "startOffset" : 59,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "This definition of dimension is similar to notions used in [19, 7, 2].",
      "startOffset" : 59,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "This definition of dimension is similar to notions used in [19, 7, 2].",
      "startOffset" : 59,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : "Vapnik and Chervonenkis [26] and Blumer et al.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "[4] proved that VC dimension is equivalent to the sample complexity of PAC learning.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "The distinguishing dimension is a lower bound on the sample complexity of PAC learning (see [4, 8, 2]).",
      "startOffset" : 92,
      "endOffset" : 101
    }, {
      "referenceID" : 7,
      "context" : "The distinguishing dimension is a lower bound on the sample complexity of PAC learning (see [4, 8, 2]).",
      "startOffset" : 92,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "The distinguishing dimension is a lower bound on the sample complexity of PAC learning (see [4, 8, 2]).",
      "startOffset" : 92,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "1 (Lower bound for sample complexity [4, 8, 2]).",
      "startOffset" : 37,
      "endOffset" : 46
    }, {
      "referenceID" : 7,
      "context" : "1 (Lower bound for sample complexity [4, 8, 2]).",
      "startOffset" : 37,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "1 (Lower bound for sample complexity [4, 8, 2]).",
      "startOffset" : 37,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "Assouad [1] bounded VC(C∗) in terms of VC(C).",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "2 (VC dimension of dual [1]).",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 25,
      "context" : "This phenomenon was first proved by Vapnik and Chervonenkis [26], and was later quantitively improved in [14, 24].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : "This phenomenon was first proved by Vapnik and Chervonenkis [26], and was later quantitively improved in [14, 24].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 23,
      "context" : "This phenomenon was first proved by Vapnik and Chervonenkis [26], and was later quantitively improved in [14, 24].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 25,
      "context" : "3 (Approximations for bounded VC dimension [26, 14, 24]).",
      "startOffset" : 43,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "3 (Approximations for bounded VC dimension [26, 14, 24]).",
      "startOffset" : 43,
      "endOffset" : 55
    }, {
      "referenceID" : 23,
      "context" : "3 (Approximations for bounded VC dimension [26, 14, 24]).",
      "startOffset" : 43,
      "endOffset" : 55
    }, {
      "referenceID" : 19,
      "context" : "Von Neumann’s minimax theorem [20] is a seminal result in game theory (see the textbook [21]).",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "Von Neumann’s minimax theorem [20] is a seminal result in game theory (see the textbook [21]).",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "This implies that there is a pair of mixed strategies that form a Nash equilibrium (see [21]).",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 19,
      "context" : "4 (Minimax [20]).",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 14,
      "context" : "Lipton, Markakis and Mehta [15] call such a pair of mixed strategies an -Nash equilibrium.",
      "startOffset" : 27,
      "endOffset" : 31
    } ],
    "year" : 2017,
    "abstractText" : "We prove that proper PAC learnability implies compression. Namely, if a concept C ⊆ ΣX is properly PAC learnable with d samples, then C has a sample compression scheme of size 2O(d). In particular, every boolean concept class with constant VC dimension has a sample compression scheme of constant size. This answers a question of Littlestone and Warmuth (1986). The proof uses an approximate minimax phenomenon for boolean matrices of low VC dimension.",
    "creator" : "LaTeX with hyperref package"
  }
}