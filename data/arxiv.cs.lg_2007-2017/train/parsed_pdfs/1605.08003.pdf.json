{
  "name" : "1605.08003.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Tight Complexity Bounds for Optimizing Composite Objectives",
    "authors" : [ "Blake Woodworth" ],
    "emails" : [ "blake@ttic.edu", "nati@ttic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 5.\n08 00\n3v 2\n[ m\nat h.\nO C\n] 2\n7 O\nct 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider minimizing the average of m ≥ 2 convex functions:\nmin x∈X\n{\nF (x) := 1\nm\nm ∑\ni=1\nfi(x)\n}\n(1)\nwhere X ⊆ Rd is a closed, convex set, and where the algorithm is given access to the following gradient (or subgradient in the case of non-smooth functions) and prox oracle for the components:\nhF (x, i, β) = [ fi(x), ∇fi(x), proxfi(x, β) ]\n(2)\nwhere\nproxfi(x, β) = argmin u∈X\n{\nfi(u) + β\n2 ‖x− u‖2\n}\n(3)\nA natural question is how to leverage the prox oracle, and how much benefit it provides over gradient access alone. The prox oracle is potentially much more powerful, as it provides global, rather then local, information about the function. For example, for a single function (m = 1), one prox oracle call (with β = 0) is sufficient for exact optimization. Several methods have recently been suggested for optimizing a sum or average of several functions using prox accesses to each component, both in the distributed setting where each components might be handled on a different machine (e.g. ADMM [7], DANE [18], DISCO [20]) or for functions that can be decomposed into several “easy” parts (e.g. PRISMA [13]). But as far as we are aware, no meaningful lower bound was previously known on the number of prox oracle accesses required even for the average of two functions (m = 2).\nThe optimization of composite objectives of the form (1) has also been extensively studied in the context of minimizing empirical risk over m samples. Recently, stochastic methods such as SDCA [16], SAG [14], SVRG [8], and other variants, have been presented which leverage the finite nature of the problem to reduce the variance in stochastic gradient estimates and obtain guarantees that dominate both batch and stochastic gradient descent. As methods with improved complexity, such as accelerated SDCA [17], accelerated SVRG, and Katyusha [3], have been presented, researchers have also tried to obtain lower bounds on the best possible complexity in this settings—but as we survey below, these have not been satisfactory so far.\nIn this paper, after briefly surveying methods for smooth, composite optimization, we present methods for optimizing non-smooth composite objectives, which show that prox oracle access can indeed be leveraged to improve over methods using merely subgradient access (see Section 3). We then turn to studying lower bounds. We consider algorithms that access the objective F only through the oracle hF and provide lower bounds on the number of such oracle accesses (and thus the runtime) required to find ǫ-suboptimal solutions. We consider optimizing both Lipschitz (non-smooth) functions and smooth functions, and guarantees that do and do not depend on strong convexity, distinguishing between deterministic optimization algorithms and randomized algorithms. Our upper and lower bounds are summarized in Table 1.\nAs shown in the table, we provide matching upper and lower bounds (up to a log factor) for all function and algorithm classes. In particular, our bounds establish the optimality (up to log factors) of accelerated SDCA, SVRG, and SAG for randomized finite-sum optimization, and also the optimality of our deterministic smoothing algorithms for non-smooth composite optimization.\nOn the power of gradient vs prox oracles For non-smooth functions, we show that having access to prox oracles for the components can reduce the polynomial dependence on ǫ from 1/ǫ2 to 1/ǫ, or from 1/(λǫ) to 1/ √ λǫ for λ-strongly convex functions. However, all of the optimal complexities for smooth functions can be attained with only component gradient access using accelerated gradient descent (AGD) or accelerated SVRG. Thus the worst-case complexity cannot be improved (at least not significantly) by using the more powerful prox oracle.\nOn the power of randomization We establish a significant gap between deterministic and randomized algorithms for finite-sum problems. Namely, the dependence on the number of components must be linear in m for any deterministic algorithm, but can be reduced to √ m (in the typically significant term) using randomization. We emphasize that the randomization here is only in the algorithm—not in the oracle. We always assume the oracle returns an exact answer (for the requested component) and is not a stochastic oracle. The distinction is that the algorithm is allowed to flip coins in deciding what operations and queries to perform but the oracle must return an exact answer to that query (of course, the algorithm could simulate a stochastic oracle).\nPrior Lower Bounds Several authors recently presented lower bounds for optimizing (1) in the smooth and strongly convex setting using component gradients. Agarwal and Bottou [1] presented a lower bound of Ω ( m+ √\nmγ λ log 1 ǫ\n)\n. However, their bound is valid only for deterministic algorithms (thus not including SDCA, SVRG, SAG, etc.)—we not only consider randomized algorithms, but also show a much higher lower\nbound for deterministic algorithms (i.e. the bound of Agarwal and Bottou is loose). Improving upon this, Lan [9] shows a similar lower bound for a restricted class of randomized algorithms: the algorithm must select which component to query for a gradient by drawing an index from a fixed distribution, but the algorithm must otherwise be deterministic in how it uses the gradients, and its iterates must lie in the span of the gradients it has received. This restricted class includes SAG, but not SVRG nor perhaps other realistic attempts at improving over these. Furthermore, both bounds allow only gradient accesses, not prox computations. Thus SDCA, which requires prox accesses, and potential variants are not covered by such lower bounds. We prove as similar lower bound to Lan’s, but our analysis is much more general and applies to any randomized algorithm, making any sequence of queries to a gradient and prox oracle, and without assuming that iterates lie in the span of previous responses. In addition to smooth functions, we also provide lower bounds for non-smooth problems which were not considered by these previous attempts. Another recent observation [15] was that with access only to random component subgradients without knowing the component’s identity, an algorithm must make Ω(m2) queries to optimize well. This shows how relatively subtle changes in the oracle can have a dramatic effect on the complexity of the problem. Since the oracle we consider is quite powerful, our lower bounds cover a very broad family of algorithms, including SAG, SVRG, and SDCA.\nOur deterministic lower bounds are inspired by a lower bound on the number of rounds of communication required for optimization when each fi is held by a different machine and when iterates lie in the span of certain permitted calculations [5]. Our construction for m = 2 is similar to theirs (though in a different setting), but their analysis considers neither scaling with m (which has a different role in their setting) nor randomization.\nNotation and Definitions We use ‖·‖ to denote the standard Euclidean norm on Rd. We say that a function f is L-Lipschitz continuous on X if ∀x, y ∈ X |f(x)− f(x)| ≤ L ‖x− y‖; γ-smooth on X if it is differentiable and its gradient is γ-Lipschitz on X ; and λ-strongly convex on X if ∀x, y ∈ X fi(y) ≥ fi(x) + 〈∇fi(x), y − x〉 + λ2 ‖x− y‖\n2. We consider optimizing (1) under four combinations of assumptions: each component fi is either L-Lipschitz or γ-smooth, and either F (x) is λ-strongly convex or its domain is bounded, X ⊆ {x : ‖x‖ ≤ B}."
    }, {
      "heading" : "2 Optimizing Smooth Sums",
      "text" : "We briefly review the best known methods for optimizing (1) when the components are γ-smooth, yielding the upper bounds on the right half of Table 1. These upper bounds can be obtained using only component gradient access, without need for the prox oracle.\nWe can obtain exact gradients of F (x) by computing allm component gradients∇fi(x). Running accelerated gradient descent (AGD) [12] on F (x) using these exact gradients achieves the upper complexity bounds for deterministic algorithms and smooth problems (see Table 1).\nSAG [14], SVRG [8] and related methods use randomization to sample components, but also leverage the finite nature of the objective to control the variance of the gradient estimator used. Accelerating these methods using the Catalyst framework [10] ensures that for λ-strongly convex objectives we have E [ F (x(k))− F (x∗) ] < ǫ after k = O (( m+ √\nmγ λ\n) log2 ǫ0ǫ ) iterations, where F (0)− F (x∗) = ǫ0. Katyusha [3] is a more direct approach to accelerating SVRG which avoids extraneous log-factors, yielding the complexity k = O (( m+ √\nmγ λ\n) log ǫ0ǫ ) indicated in Table 1.\nWhen F is not strongly convex, adding a regularizer to the objective and instead optimizing Fλ(x) =\nF (x) + λ2 ‖x‖ 2 with λ = ǫ/B2 results in an oracle complexity of O\n((\nm+\n√\nmγB2\nǫ\n)\nlog ǫ0ǫ\n)\n. The log-factor\nin the second term can be removed using the more delicate reduction of Allen-Zhu and Hazan [4], which involves optimizing Fλ(x) for progressively smaller values of λ, yielding the upper bound in the table.\nKatyusha and Catalyst-accelerated SAG or SVRG use only gradients of the components. Accelerated SDCA [17] achieves a similar complexity using gradient and prox oracle access."
    }, {
      "heading" : "3 Leveraging Prox Oracles for Lipschitz Sums",
      "text" : "In this section, we present algorithms for leveraging the prox oracle to minimize (1) when each component is L-Lipschitz. This will be done by using the prox oracle to “smooth” each component, and optimizing the new, smooth sum which approximates the original problem. This idea was used in order to apply Katyusha [3] and accelerated SDCA [17] to non-smooth objectives. We are not aware of a previous explicit presentation of the AGD-based deterministic algorithm, which achieves the deterministic upper complexity indicated in Table 1.\nThe key is using a prox oracle to obtain gradients of the β-Moreau envelope of a non-smooth function, f , defined as:\nf (β)(x) = inf u∈X\nf(u) + β\n2 ‖x− u‖2 (4)\nLemma 1 ([13, Lemma 2.2], [6, Proposition 12.29], following [11]). Let f be convex and L-Lipschitz continuous. For any β > 0,\n1. f (β) is β-smooth\n2. ∇(f (β))(x) = β(x − proxf (x, β))\n3. f (β)(x) ≤ f(x) ≤ f (β)(x) + L22β\nConsequently, we can consider the smoothed problem\nmin x∈X\n{\nF̃ (β)(x) := 1\nm\nm ∑\ni=1\nf (β) i (x)\n}\n. (5)\nWhile F̃ (β) is not, in general, the β-Moreau envelope of F , it is β-smooth, we can calculate the gradient of its components using the oracle hF , and F̃ (β)(x) ≤ F (x) ≤ F̃ (β)(x) + L22β . Thus, to obtain an ǫ-suboptimal solution to (1) using hF , we set β = L 2/ǫ and apply any algorithm which can optimize (5) using gradients of the L2/ǫ-smooth components, to within ǫ/2 accuracy. With the rates presented in Section 2, using AGD on (5) yields a complexity of O (\nmLB ǫ\n)\nin the deterministic setting. When the functions are λ-strongly convex, smoothing with a fixed β results in a spurious log-factor. To avoid this, we again apply the reduction of Allen-Zhu and Hazan [4], this time optimizing F̃ (β) for increasingly large values of β. This leads to the upper bound of O (\nmL√ λǫ\n)\nwhen used with AGD (see Appendix A for details).\nSimilarly, we can apply an accelerated randomized algorithm (such as Katyusha) to the smooth problem F̃ (β) to obtain complexities of O ( m log ǫ0ǫ + √ mLB ǫ ) and O ( m log ǫ0ǫ + √ mL√ λǫ ) —this matches the presentation of Allen-Zhu [3] and is similar to that of Shalev-Shwartz and Zhang [17].\nFinally, if m > L2B2/ǫ2 or m > L2/(λǫ), stochastic gradient descent is a better randomized alternative, yielding complexities of O(L2B2/ǫ2) or O(L2/(λǫ))."
    }, {
      "heading" : "4 Lower Bounds for Deterministic Algorithms",
      "text" : "We now turn to establishing lower bounds on the oracle complexity of optimizing (1). We first consider only deterministic optimization algorithms. What we would like to show is that for any deterministic optimization algorithm we can construct a “hard” function for which the algorithm cannot find an ǫ-suboptimal solution until it has made many oracle accesses. Since the algorithm is deterministic, we can construct such a function by simulating the (deterministic) behavior of the algorithm. This can be viewed as a game, where an adversary controls the oracle being used by the algorithm. At each iteration the algorithm queries the oracle with some triplet (x, i, β) and the adversary responds with an answer. This answer must be consistent with all previous answers, but the adversary ensures it is also consistent with a composite function F that\nthe algorithm is far from optimizing. The “hard” function is then gradually defined in terms of the behavior of the optimization algorithm.\nTo help us formulate our constructions, we define a “round” of queries as a series of queries in which ⌈m2 ⌉ distinct functions fi are queried. The first round begins with the first query and continues until exactly ⌈m2 ⌉ unique functions have been queried. The second round begins with the next query, and continues until exactly ⌈m2 ⌉ more distinct components have been queried in the second round, and so on until the algorithm terminates. This definition is useful for analysis but requires no assumptions about the algorithm’s querying strategy."
    }, {
      "heading" : "4.1 Non-Smooth Components",
      "text" : "We begin by presenting a lower bound for deterministic optimization of (1) when each component fi is convex and L-Lipschitz continuous, but is not necessarily strongly convex, on the domain X = {x : ‖x‖ ≤ B}. Without loss of generality, we can consider L = B = 1. We will construct functions of the following form:\nfi(x) = 1√ 2 |b − 〈x, v0〉|+ 1 2 √ k\nk ∑\nr=1\nδi,r |〈x, vr−1〉 − 〈x, vr〉| . (6)\nwhere k = ⌊ 112ǫ⌋, b = 1√k+1 , and {vr} is an orthonormal set of vectors in R d chosen according to the behavior of the algorithm such that vr is orthogonal to all points at which the algorithm queries hF before round r, and where δi,r are indicators chosen so that δi,r = 1 if the algorithm does not query component i in round r (and zero otherwise). To see how this is possible, consider the following truncations of (6):\nf ti (x) = 1√ 2 |b− 〈x, v0〉|+ 1 2 √ k\nt−1 ∑\nr=1\nδi,r |〈x, vr−1〉 − 〈x, vr〉| (7)\nDuring each round t, the adversary answers queries according to f ti , which depends only on vr, δi,r for r < t, i.e. from previous rounds. When the round is completed, δi,t is determined and vt is chosen to be orthogonal to the vectors {v0, ..., vt−1} as well as every point queried by the algorithm so far, thus defining f t+1i for the next round. In Appendix B.1 we prove that these responses based on f ti are consistent with fi.\nThe algorithm can only learn vr after it completes round r—until then every iterate is orthogonal to it by construction. The average of these functions reaches its minimum of F (x∗) = 0 at x∗ = b\n∑k r=0 vr, so we\ncan view optimizing these functions as the task of discovering the vectors vr—even if only vk is missing, a suboptimality better than b/(6 √ k) > ǫ cannot be achieved. Therefore, the deterministic algorithm must complete at least k rounds of optimization, each comprising at least ⌈\nm 2\n⌉\nqueries to hF in order to optimize F . The key to this construction is that even though each term |〈x, vr−1〉 − 〈x, vr〉| appears in m/2 components, and hence has a strong effect on the average F (x), we can force a deterministic algorithm to make Ω(m) queries during each round before it finds the next relevant term. We obtain (for complete proof see Appendix B.1):\nTheorem 1. For any L,B > 0, any 0 < ǫ < LB12 , any m ≥ 2, and any deterministic algorithm A with access to hF , there exists a dimension d = O ( mLB ǫ ) , and m functions fi defined over X = { x ∈ Rd : ‖x‖ ≤ B } , which are convex and L-Lipschitz continuous, such that in order to find a point x̂ for which F (x̂)−F (x∗) < ǫ, A must make Ω (\nmLB ǫ\n)\nqueries to hF .\nFurthermore, we can always reduce optimizing a function over ‖x‖ ≤ B to optimizing a strongly convex function by adding the regularizer ǫ ‖x‖2 /(2B2) to each component, implying (see complete proof in Appendix B.2):\nTheorem 2. For any L, λ > 0, any 0 < ǫ < L 2\n288λ , any m ≥ 2, and any deterministic algorithm A with access to hF , there exists a dimension d = O (\nmL√ λǫ\n)\n, and m functions fi defined over X ⊆ Rd, which are LLipschitz continuous and λ-strongly convex, such that in order to find a point x̂ for which F (x̂)−F (x∗) < ǫ, A must make Ω (\nmL√ λǫ\n)\nqueries to hF ."
    }, {
      "heading" : "4.2 Smooth Components",
      "text" : "When the components fi are required to be smooth, the lower bound construction is similar to (6), except it is based on squared differences instead of absolute differences. We consider the functions:\nfi(x) = 1\n8\n(\nδi,1\n( 〈x, v0〉2 − 2a 〈x, v0〉 ) + δi,k 〈x, vk〉2 + k ∑\nr=1\nδi,r (〈x, vr−1〉 − 〈x, vr〉)2 )\n(8)\nwhere δi,r and vr are as before. Again, we can answer queries at round t based only on δi,r, vr for r < t. This construction yields the following lower bounds (full details in Appendix B.3):\nTheorem 3. For any γ,B, ǫ > 0, any m ≥ 2, and any deterministic algorithm A with access to hF , there exists a sufficiently large dimension d = O ( m √ γB2/ǫ )\n, and m functions fi defined over X = {\nx ∈ Rd : ‖x‖ ≤ B } , which are convex and γ-smooth, such that in order to find a point x̂ ∈ Rd for which F (x̂)− F (x∗) < ǫ, A must make Ω ( m √ γB2/ǫ )\nqueries to hF .\nIn the strongly convex case, we use a very similar construction, adding the term λ ‖x‖2 /2, which gives the following bound (see Appendix B.4):\nTheorem 4. For any γ, λ > 0 such that γλ > 73, any ǫ > 0, any ǫ0 > 3γǫ λ , any m ≥ 2, and any deterministic algorithm A with access to hF , there exists a sufficiently large dimension d = O ( m √γ λ log ( λǫ0 γǫ )) , and m functions fi defined over X ⊆ Rd, which are γ-smooth and λ-strongly convex and where F (0)− F (x∗) = ǫ0, such that in order to find a point x̂ for which F (x̂)− F (x∗) < ǫ, A must make Ω ( m √\nγ λ log\n(\nλǫ0 γǫ\n))\nqueries\nto hF ."
    }, {
      "heading" : "5 Lower Bounds for Randomized Algorithms",
      "text" : "We now turn to randomized algorithms for (1). In the deterministic constructions, we relied on being able to set vr and δi,r based on the predictable behavior of the algorithm. This is impossible for randomized algorithms, we must choose the “hard” function before we know the random choices the algorithm will make—so the function must be “hard” more generally than before.\nPreviously, we chose vectors vr orthogonal to all previous queries made by the algorithm. For randomized algorithms this cannot be ensured. However, if we choose orthonormal vectors vr randomly in a high dimensional space, they will be nearly orthogonal to queries with high probability. Slightly modifying the absolute or squared difference from before makes near orthogonality sufficient. This issue increases the required dimension but does not otherwise affect the lower bounds.\nMore problematic is our inability to anticipate the order in which the algorithm will query the components, precluding the use of δi,r. In the deterministic setting, if a term revealing a new vr appeared in half of the components, we could ensure that the algorithm must make m/2 queries to find it. However, a randomized algorithm could find it in two queries in expectation, which would eliminate the linear dependence on m in the lower bound! Alternatively, if only one component included the term, a randomized algorithm would indeed need Ω(m) queries to find it, but that term’s effect on suboptimality of F would be scaled down by m, again eliminating the dependence on m. To establish a Ω( √ m) lower bound for randomized algorithms we must take a new approach. We define ⌊\nm 2\n⌋\npairs of functions which operate on ⌊ m 2 ⌋ orthogonal subspaces of Rd. Each pair of functions resembles the constructions from the previous section, but since there are many of them, the algorithm must solve Ω(m) separate optimization problems in order to optimize F ."
    }, {
      "heading" : "5.1 Lipschitz Continuous Components",
      "text" : "First consider the non-smooth, non-strongly-convex setting and assume for simplicity m is even (otherwise we simply let the last function be zero). We define the helper function ψc, which replaces the absolute\nvalue operation and makes our construction resistant to small inner products between iterates and not-yetdiscovered components: ψc(z) = max (0, |z| − c) (9) Next, we define m/2 pairs of functions, indexed by i = 1..m/2:\nfi,1(x) = 1√ 2 |b− 〈x, vi,0〉|+ 1 2 √ k\nk ∑\nr even\nψc (〈x, vi,r−1〉 − 〈x, vi,r〉) (10)\nfi,2(x) = 1\n2 √ k\nk ∑\nr odd\nψc (〈x, vi,r−1〉 − 〈x, vi,r〉)\nwhere {vi,r}r=0..k,i=1..m/2 are random orthonormal vectors and k = Θ( 1ǫ√m ). With c sufficiently small and the dimensionality sufficiently high, with high probability the algorithm only learns the identity of new vectors vi,r by alternately querying fi,1 and fi,2; so revealing all k + 1 vectors requires at least k + 1 total queries. Until vi,k is revealed, an iterate is Ω(ǫ)-suboptimal on (fi,1 + fi,2)/2. From here, we show that an ǫ-suboptimal solution to F (x) can be found only after at least k + 1 queries are made to at least m/4 pairs, for a total of Ω(mk) queries. This time, since the optimum x∗ will need to have inner product b with Θ(mk) vectors vi,r , we need to have b = Θ( 1√ mk ) = Θ( √ ǫ/ √ m), and the total number of queries is Ω(mk) = Ω( √ m ǫ ). The Ω(m) term of the lower bound follows trivially since we require ǫ = O(1/ √ m), (proofs in Appendix C.1):\nTheorem 5. For any L,B > 0, any 0 < ǫ < LB 10 √ m , any m ≥ 2, and any randomized algorithm A with access to hF , there exists a dimension d = O ( L4B6 ǫ4 log ( LB ǫ ) )\n, and m functions fi defined over X = {\nx ∈ Rd : ‖x‖ ≤ B } , which are convex and L-Lipschitz continuous, such that to find a point x̂ for which\nE [F (x̂)− F (x∗)] < ǫ, A must make Ω ( m+ √ mLB ǫ ) queries to hF .\nAn added regularizer gives the result for strongly convex functions (see Appendix C.2):\nTheorem 6. For any L, λ > 0, any 0 < ǫ < L 2\n200λm , any m ≥ 2, and any randomized algorithm A with access to hF , there exists a dimension d = O ( L4\nλ3ǫ log L√ λǫ\n)\n, and m functions fi defined over X ⊆ Rd, which are L-Lipschitz continuous and λ-strongly convex, such that in order to find a point x̂ for which E [F (x̂)− F (x∗)] < ǫ, A must make Ω (\nm+ √ mL√ λǫ ) queries to hF .\nThe large dimension required by these lower bounds is the cost of omitting the assumption that the algorithm’s queries lie in the span of previous oracle responses. If we do assume that the queries lie in that span, the necessary dimension is only on the order of the number of oracle queries needed. When ǫ = Ω(LB/ √ m) in the non-strongly convex case or ǫ = Ω ( L2/(λm) )\nin the strongly convex case, the lower bounds for randomized algorithms presented above do not apply. Instead, we can obtain a lower bound based on an information theoretic argument. We first uniformly randomly choose a parameter p, which is either (1/2 − 2ǫ) or (1/2 + 2ǫ). Then for i = 1, ...,m, in the non-strongly convex case we make fi(x) = x with probability p and fi(x) = −x with probability 1 − p. Optimizing F (x) to within ǫ accuracy then implies recovering the bias of the Bernoulli random variable, which requires Ω(1/ǫ2) queries based on a standard information theoretic result [2, 19]. Setting fi(x) = ±x+ λ2 ‖x‖ 2 gives a Ω(1/(λǫ)) lower bound in the λ-strongly convex setting. This is formalized in Appendix C.5."
    }, {
      "heading" : "5.2 Smooth Components",
      "text" : "When the functions fi are smooth and not strongly convex, we define another helper function φc:\nφc(z) =\n\n \n  0 |z| ≤ c 2(|z| − c)2 c < |z| ≤ 2c z2 − 2c2 |z| > 2c\n(11)\nand the following pairs of functions for i = 1, ...,m/2:\nfi,1(x) = 1\n16\n( 〈x, vi,0〉2 − 2a 〈x, vi,0〉+ k ∑\nr even\nφc (〈x, vi,r−1〉 − 〈x, vi,r〉) )\n(12)\nfi,2(x) = 1\n16\n( φc (〈x, vi,k〉) + k ∑\nr odd\nφc (〈x, vi,r−1〉 − 〈x, vi,r〉) )\nwith vi,r as before. The same arguments apply, after replacing the absolute difference with squared difference. A separate argument is required in this case for the Ω(m) term in the bound, which we show using a construction involving m simple linear functions (see Appendix C.3).\nTheorem 7. For any γ,B, ǫ > 0, any m ≥ 2, and any randomized algorithm A with access to hF , there exists a sufficiently large dimension d = O ( γ2B6\nǫ2 log ( γB2 ǫ ) +B2m logm ) and m functions fi defined over\nX = { x ∈ Rd : ‖x‖ ≤ B } , which are convex and γ-smooth, such that to find a point x̂ ∈ Rd for which E [F (x̂)− F (x∗)] < ǫ, A must make Ω ( m+ √ mγB2\nǫ\n)\nqueries to hF .\nIn the strongly convex case, we add the term λ ‖x‖2 /2 to fi,1 and fi,2 (see Appendix C.4) to obtain:\nTheorem 8. For any m ≥ 2, any γ, λ > 0 such that γλ > 161m, any ǫ > 0, any ǫ0 > 60ǫ √ γ λm , and any randomized algorithm A, there exists a dimension d = O (\nγ2.5ǫ0 λ2.5ǫ log\n3 (\nλǫ0 γǫ\n) + mγǫ0λǫ logm )\n, domain X ⊆ R\nd, x0 ∈ X , and m functions fi defined on X which are γ-smooth and λ-strongly convex, and such that F (x0) − F (x∗) = ǫ0 and such that in order to find a point x̂ ∈ X such that E [F (x̂)− F (x∗)] < ǫ, A must make Ω ( m+ √\nmγ λ log\n(\nǫ0 ǫ\n√\nmλ γ\n))\nqueries to hF .\nRemark: We consider (1) as a constrained optimization problem, thus the minimizer of F could be achieved on the boundary of X , meaning that the gradient need not vanish. If we make the additional assumption that the minimizer of F lies on the interior of X (and is thus the unconstrained global minimum), Theorems 1-8 all still apply, with a slight modification to Theorems 3 and 7. Since the gradient now needs to vanish on X , 0 is always O(γB2)-suboptimal, and only values of ǫ in the range 0 < ǫ < γB2128 and 0 < ǫ < 9γB2\n128 result in a non-trivial lower bound (see Remarks at the end of Appendices B.3 and C.3)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We provide a tight (up to a log factor) understanding of optimizing finite sum problems of the form (1) using a component prox oracle.\nRandomized optimization of (1) has been the subject of much research in the past several years, starting with the presentation of SDCA and SAG, and continuing with accelerated variants. Obtaining lower bounds can be very useful for better understanding the problem, for knowing where it might or might not be possible to improve or where different assumptions would be needed to improve, and for establishing optimality of optimization methods. Indeed, several attempts have been made at lower bounds for the finite sum setting [1, 9]. But as we explain in the introduction, these were unsatisfactory and covered only limited classes of methods. Here we show that in a fairly general sense, accelerated SDCA, SVRG, SAG, and Katyusha are optimal up to a log factor. Improving on their runtime would require additional assumptions, or perhaps a stronger oracle. However, even if given “full” access to the component functions, all algorithms that we can think of utilize this information to calculate a prox vector. Thus, it is unclear what realistic oracle would be more powerful than the prox oracle we consider.\nOur results highlight the power of randomization, showing that no deterministic algorithm can beat the linear dependence on m and reduce it to the √ m dependence of the randomized algorithms.\nThe deterministic algorithm for non-smooth problems that we present in Section 3 is also of interest in its own right. It avoids randomization, which is not usually problematic, but makes it fully parallelizable unlike the optimal stochastic methods. Consider, for example, a supervised learning problem where fi(x) = ℓ(〈φi, x〉, yi) is the (non-smooth) loss on a single training example (φi, yi), and the data is distributed across machines. Calculating a prox oracle involves applying the Fenchel conjugate of the loss function ℓ, but even if a closed form is not available, this is often easy to compute numerically, and is used in algorithms such as SDCA. But unlike SDCA, which is inherently sequential, we can calculate all m prox operations in parallel on the different machines, average the resulting gradients of the smoothed function, and take an accelerated gradient step to implement our optimal deterministic algorithm. This method attains a recent lower bound for distributed optimization, resolving a question raised by Arjevani and Shamir [5], and when the number of machines is very large improves over all other known distributed optimization methods for the problem.\nIn studying finite sum problems, we were forced to explicitly study lower bounds for randomized optimization as opposed to stochastic optimization (where the source of randomness is the oracle, not the algorithm). Even for the classic problem of minimizing a smooth function using a first order oracle, we could not locate a published proof that applies to randomized algorithms. We provide a simple construction using ǫ-insensitive differences that allows us to easily obtain such lower bounds without reverting to assuming the iterates are spanned by previous responses (as was done, e.g., in [9]), and could potentially be useful for establishing randomized lower bounds also in other settings.\nAcknowledgements: We thank Ohad Shamir for his helpful discussions and for pointing out [4]."
    }, {
      "heading" : "A Upper bounds for non-smooth sums",
      "text" : "Consider the case where the components are not strongly convex. As shown in lemma 1, we can use a single call to a prox oracle to obtain the gradient of\nf (β)(x) = inf u∈X\nf(u) + β\n2 ‖x− u‖2\nwhich is a β-smooth approximation to f . We then consider the new optimization problem:\nmin x∈X\n{\nF̃ (β)(x) := 1\nm\nm ∑\ni=1\nf (β) i (x)\n}\n. (13)\nAlso by lemma 1, setting β = L 2\nǫ ensures that F̃ (β)(x) ≤ F (x) ≤ F̃ (β)(x) + ǫ2 for all x. Consequently, any\npoint which is ǫ2 -suboptimal for F̃ (β) will be ǫ-suboptimal for F . This technique therefore reduces the task of optimizing an instance of an L-Lipschitz finite sum to that of optimizing an L 2\nǫ -smooth finite sum.\nSolving (13) to ǫ2 -suboptimality using AGD requires O ( mLB ǫ ) gradients for F̃ (β) which requires that same number of prox oracles from hF . Formally:\nTheorem 9. For any L,B > 0, any ǫ < LB, and any m ≥ 1 functions fi which are convex and L-Lipschitz continuous over the domain X ⊆ { x ∈ Rd : ‖x‖ ≤ B } , applying AGD to (13) for β = L 2\nǫ , will result in a\npoint x̂ such that F (x̂)− F (x∗) < ǫ after O ( mLB ǫ ) queries to hF .\nWhen the component functions are λ-strongly convex, a more sophisticated strategy is required to avoid an extra log factor. The solution is the AdaptSmooth algorithm [4]. This involves solving O(log 1ǫ ) smooth and strongly convex subproblems, where the tth subproblem is reducing the suboptimality of the βt-smooth and λ-strongly convex function F (βt)(x) by a factor of four, where βt = L2\nǫ0 2t and where ǫ0 ≤ L\n2\nλ upper bounds the\ninitial suboptimality. Using this method results in an ǫ-suboptimal solution for F after ∑log\nǫ0 ǫ\nt=0 Time(βt, λ) queries to hF . In the case of AGD, Time(γ, λ) = O ( m √\nγ λ\n)\nand\nlog ǫ0 ǫ\n∑\nt=0\nTime\n(\nL2 ǫ0 2t, λ\n) = O ( mL√ λǫ )\nTheorem 10. For any L, λ, ǫ > 0, and any m ≥ 1 functions fi, which are L-Lipschitz continuous and λ-strongly convex on the domain X ⊆ Rd, applying AdaptSmooth with AGD will find a point x̂ ∈ X such that F (x̂)− F (x∗) < ǫ after O (\nmL√ λǫ\n)\nqueries to hF .\nTo conclude our presentation of upper bounds, we emphasize that the smoothing methods described in this section will only improve oracle complexity when used with accelerated methods. For example, using non-accelerated gradient descent on F̃ (β) in the not strongly convex case leads to an oracle complexity of O ( mL2B2\nǫ2\n)\n, which is no better than the convergence rate of gradient descent applied directly to F ."
    }, {
      "heading" : "B Lower bounds for deterministic algorithms",
      "text" : "B.1 Non-smooth and not strongly convex components\nTheorem 1. For any L,B > 0, any 0 < ǫ < LB12 , any m ≥ 2, and any deterministic algorithm A with access to hF , there exists a dimension d = O ( mLB ǫ ) , and m functions fi defined over X = { x ∈ Rd : ‖x‖ ≤ B } , which are convex and L-Lipschitz continuous, such that in order to find a point x̂ for which F (x̂)−F (x∗) < ǫ, A must make Ω (\nmLB ǫ\n)\nqueries to hF .\nProof. Without loss of generality, we can assume L = B = 1. For particular values b and k to be decided upon later, we use the functions (6):\nfi(x) = 1√ 2 |b− 〈x, v0〉|+ 1 2 √ k\nk ∑\nr=1\nδi,r |〈x, vr−1〉 − 〈x, vr〉|\nIt is straightforward to confirm that fi is both 1-Lipschitz and convex (for orthonormal vectors vr and indicators δi,r ∈ {0, 1}). As explained in the main text, the orthonormal vectors vr ∈ Rd and indicators δi,r ∈ {0, 1} are chosen according to the behavior of the algorithm A. At the end of each round t, we set δi,t = 1 iff the algorithm did not query function i during round t (and zero otherwise), and we set vt to be orthogonal to the vectors {v0, ..., vt−1} as well as every query made by the algorithm so far. Orthogonalizing the vectors in this way is possible as long as the dimension is at least as large as the number of oracle queries A has made so far plus t. We are allowed to construct vt and δi,t in this way as long as the algorithm’s execution up until round t, and thus our choice of vt and δi,t, depends only on vr and δi,r for r < t. We can enforce this condition by answering the queries during round t according to\nf ti (x) = 1√ 2 |b− 〈x, v0〉|+ 1 2 √ k\nt−1 ∑\nr=1\nδi,r |〈x, vr−1〉 − 〈x, vr〉|\nFor non-smooth functions, the subgradient oracle is not uniquely defined—-many different subgradients might be a valid response. However, in order to say that an algorithm successfully optimizes a function, it must be able to do so no matter which subgradient is receives. Conversely, to show a lower bound, it is sufficient to show that for some valid subgradient the algorithm fails. And so, in constructing a “hard” instance to optimize we are actually constructing both a function and a subgradient oracle for it, with specific subgradient responses. Therefore, answering the algorithm’s queries during round t according to f ti is valid so long as the subgradient we return is a valid subgradient for fi (the converse need not be true) and the prox returned is exactly the prox of fi. For now, assume that this query-answering strategy is consistent (we will prove this last).\nThen if d = ⌈mǫ ⌉+k+1 and if x is an iterate generated both before A completes round k and before it makes ⌈mǫ ⌉ queries to hF (so that the dimension is large enough to orthogonalize each vt as described above), then 〈x, vk〉 = 0 by construction. This allows us to bound the suboptimality of F (x) (since ⌈m2 ⌉ functions are queried during each round,\n∑m i=1 δi,r = ⌊m2 ⌋):\nF (x) = 1\nm\nm ∑\ni=1\nfi(x)\n= 1√ 2 |b− 〈x, v0〉|+ ⌊m2 ⌋ 2m √ k k ∑\nr=1\n|〈x, vr−1〉 − 〈x, vr〉|\nF is non-negative and F (xb) = 0 where xb = b ∑k r=0 vr. Choosing b = 1√ k+1 makes ‖xb‖ = 1 so that xb ∈ X . Therefore, F achieves its minimum on X and\nF (x) − F (x∗) = 1√ 2 |b− 〈x, v0〉|+ ⌊m2 ⌋ 2m √ k k ∑\nr=1\n|〈x, vr−1〉 − 〈x, vr〉| − 0\n≥ 1√ 2 |b− 〈x, v0〉|+ 1 6 √ k |〈x, v0〉 − 〈x, vk〉| = 1√ 2 |b− 〈x, v0〉|+ 1 6 √ k |〈x, v0〉|\n≥ min z∈R 1√ 2 |b− z|+ 1 6 √ k |z| = b\n6 √ k\n≥ 1 12k\nWhere the final inequality holds when k ≥ 1. Setting k = ⌊ 112ǫ⌋ implies F (x)−F (x∗) ≥ ǫ. Therefore, A must either query hF more than ⌈mǫ ⌉ times or complete k rounds to reach an ǫ-suboptimal solution. Completing each round requires at least ⌈m2 ⌉ queries to hF , so when ǫ ≤ 112 , this implies a lower bound of\nmin\n(\nm\nǫ ,\n⌊\n1\n12ǫ\n⌋\nm\n2\n)\n≥ m 48ǫ\nTo complete the proof, it remains to show that the subgradients and proxs of f ti are consistent with those of fi at every time t. Since every function operates on the (k+1)-dimensional subspace of R\nd spanned by {vr}, it will be convenient to decompose vectors into two components: x = xv + x⊥ where xv =\n∑k r=0 〈x, vr〉 vr\nand x⊥ = x− xv. Note that f ti (x) = f ti (xv). Lemma 2. For any t ≤ k and any x such that xv ∈ span {v0, v1, ..., vt−1}, if function i is queried during round t, then ∂f ti (x) ⊆ ∂fi(x).\nProof. All subgradients of fi have the form\nsign ( b− 〈x, v0〉 )\n√ 2\nv0 + 1\n2 √ k\nk ∑\nr=1\nδi,rsign ( 〈x, vr−1〉 − 〈x, vr〉 ) (vr−1 − vr)\nwhere we define sign(0) = 0. Since function i is queried during round t, δi,t = 0, and since 〈x, vr−1〉 = 0 = 〈x, vr〉 for all r > t, ∂fi(x) contains all subgradients of the form\nsign ( b− 〈x, v0〉 )\n√ 2\nv0 + 1\n2 √ k\nt−1 ∑\nr=1\nδi,rsign ( 〈x, vr−1〉 − 〈x, vr〉 ) (vr−1 − vr)\nwhich is exactly ∂f ti (x).\nLemma 3. For any t ≤ k and any x such that xv ∈ span {v0, v1, ..., vt−1}, if function i is queried during round t then ∀β > 0, proxft\ni (x, β) = proxfi(x, β).\nProof. Consider the definition of the prox oracle from equation 3\nproxfi(x, β) = argmin u\nfi(u) + β\n2 ‖x− u‖2\n= argmin uv ,u⊥\nfi(u v) +\nβ\n2\n∥ ∥xv + x⊥ − uv − u⊥ ∥ ∥ 2\n= argmin uv\nfi(u v) +\nβ 2 ‖xv − uv‖2 + argmin\nu⊥\nβ\n2\n∥ ∥x⊥ − u⊥ ∥ ∥ 2\n= x⊥ + proxfi(x v, β)\nNext, we further decompose xv = x− + x+ where\nx− = t−1 ∑\nr=0\n〈xv, vr〉 vr and x+ = k ∑\nr=t\n〈xv, vr〉 vr\nNote that x+ = 0 and since function i is queried during round t, δi,t = 0. Therefore,\nproxfi(x v , β) = argmin\nu−,u+ fi(u\n− + u+) + β\n2\n∥ ∥x− − u− − u+ ∥ ∥ 2 (14)\n= argmin u−,u+ 1√ 2 ∣ ∣b− 〈 u−, v0 〉∣ ∣+ 1 2 √ k\nt−1 ∑\nr=1\nδi,r ∣ ∣ 〈 u−, vr−1 〉 − 〈 u−, vr 〉∣ ∣\n+ 1\n2 √ k\nk ∑\nr=t+1\nδi,r ∣ ∣ 〈 u+, vr−1 〉 − 〈 u+, vr 〉∣ ∣+ β\n2\n(\n∥ ∥x− − u− ∥ ∥ 2 + ∥ ∥u+ ∥ ∥\n2 )\n= proxft i (xv, β)\nThe last equality follows from the fact that that the minimization is completely separable between u− and u+, allowing us to minimized over each variable separately. The terms containing u+ are non-negative and can be simultaneously equal to 0 when u+ = 0. Therefore, proxft\ni (x, β) = proxfi(x, β).\nThese lemmas show that the subgradients and proxs of f ti at vectors which are queried during round t are consistent with the subgradients and proxs of fi. This confirms that our construction is sound, and completes the proof.\nB.2 Non-smooth and strongly convex components\nTheorem 2. For any L, λ > 0, any 0 < ǫ < L 2 288λ , any m ≥ 2, and any deterministic algorithm A with access to hF , there exists a dimension d = O (\nmL√ λǫ\n)\n, and m functions fi defined over X ⊆ Rd, which are LLipschitz continuous and λ-strongly convex, such that in order to find a point x̂ for which F (x̂)−F (x∗) < ǫ, A must make Ω (\nmL√ λǫ\n)\nqueries to hF .\nProof. Suppose towards contradiction that the contrary were true, and there is an A which can find a point x̂ for which F (x̂) − F (x∗) < ǫ after at most o (\nmL√ λǫ\n)\nqueries to hF . Then A could be used to minimize the\nsum F̃ of m functions f̃i, which are convex and L-Lipschitz continuous over a domain of {x : ‖x‖ ≤ B} by adding a regularizer. Let\nF (x) = 1\nm\nm ∑\ni=1\nfi(x) := 1\nm\nm ∑\ni=1\nf̃i(x) + λ\n2 ‖x‖2\nNote that fi is λ-strongly convex and since f̃i is L-Lipschitz on the B-bounded domain, fi is (L + λB)Lipschitz continuous on the same domain. Furthermore, by setting λ = ǫB2 ,\nF̃ (x) ≤ F (x) ≤ F̃ (x) + ǫ 2B2 ‖x‖2 ≤ F̃ (x) + ǫ 2\nBy assumption, A can find an x̂ such that F (x̂)− F (x∗) < ǫ2 using o ( m(L+λB)√ λǫ ) = o ( mLB ǫ ) queries to hF , and ǫ\n2 > F (x̂)− F (x∗) ≥ F̃ (x̂)− F̃ (x̃∗)− ǫ 2\nThus x̂ is ǫ-suboptimal for F̃ . However, this contradicts the conclusion of theorem 1 when the parameters of the strongly convex problem correspond to parameters of a non-strongly convex problem to which theorem 1 applies. In particular, for any values L > 0, λ > 0, 0 < ǫ < L 2\n288λ , and dimension d = O ( mL√ λǫ ) there is a\ncontradiction.\nB.3 Smooth and not strongly convex components\nTheorem 3. For any γ,B, ǫ > 0, any m ≥ 2, and any deterministic algorithm A with access to hF , there exists a sufficiently large dimension d = O ( m √ γB2/ǫ )\n, and m functions fi defined over X = {\nx ∈ Rd : ‖x‖ ≤ B } , which are convex and γ-smooth, such that in order to find a point x̂ ∈ Rd for which F (x̂)− F (x∗) < ǫ, A must make Ω ( m √ γB2/ǫ )\nqueries to hF .\nProof. This proof will be very similar to the proof of theorem 1. Without loss of generality, we can assume that γ = B = 1. For a values a and k to be fixed later, we define:\nfi(x) = 1\n8\n(\nδi,1\n( 〈x, v0〉2 − 2a 〈x, v0〉 ) +\nk ∑\nr=1\nδi,r (〈x, vr−1〉 − 〈x, vr〉)2 + δi,k 〈x, vk〉2 )\nWe define the orthonormal vectors vr ∈ Rd and indicators δi,t ∈ {0, 1} as in the proof of theorem 1. That is, at the end of round t, we set δi,t = 1 if the algorithm A does not query function i during round t (and zero otherwise) and we construct vt to be orthogonal to {v0, ..., vt−1} as well as every point queried by the algorithm so far. Orthogonalizing the vectors is possible as long as the dimension is at least as large as the number of oracle queries A has made so far plus t. As before, we are allowed to construct vt and δi,t in this way as long as the algorithm’s execution up until round t, and thus our choice of vt and δi,t, depends only on vr and δi,r for r < t. We enforce this condition by answering the queries during round t < k according to\nf ti (x) = 1\n8\n(\nδi,1\n( 〈x, v0〉2 − 2a 〈x, v0〉 ) +\nt−1 ∑\nr=1\nδi,r (〈x, vr−1〉 − 〈x, vr〉)2 )\nWe will assume for now that this query-answering strategy is self-consistent, and prove it later. This allows us to bound the suboptimality of F (x). Note that since exactly ⌈m2 ⌉ functions are queried each round, ∑m\ni=1 δi,r = ⌊m2 ⌋, so let\nF t(x) = 1\nm\nm ∑\ni=1\nf ti (x) + δi,t 〈x, vt−1〉 2\n= ⌊m2 ⌋ 8m\n( 〈x, v0〉2 − 2a 〈x, v0〉+ t−1 ∑\nr=1\n(〈x, vr−1〉 − 〈x, vr〉)2 + 〈x, vt−1〉2 )\nThen if d = ⌈ m√ ǫ ⌉ + k + 1, and if x is an iterate generated both before A completes round q := ⌊k2⌋ and before it makes ⌈ m√ ǫ ⌉ queries to hF , then 〈x, vr〉 = 0 for all r ≥ q by construction. Then, for this x, F q(x) = F k+1(x) = F (x). By first order optimality conditions for F t, its optimum x∗t must satisfy that:\n2 〈x∗t , v0〉 − 〈x∗t , v1〉 = a 〈x∗t , vr−1〉 − 2 〈x∗t , vr〉+ 〈x∗t , vr+1〉 = 0 for 1 ≤ r ≤ t− 2\n〈x∗t , vt−2〉 − 2 〈x∗t , vt−1〉 = 0\nIt is straightforward to confirm that the solution to this system of equations is\nx∗t = a t−1 ∑\nr=0\n(\n1− r + 1 t+ 1\n)\nvr\nthat\nF t(x∗t ) = − a2⌊m2 ⌋ 8m ( 1− 1 t+ 1 )\nand that\n‖x∗t ‖2 = a2 t−1 ∑\nr=0\n(\n1− r + 1 t+ 1\n)2\n= a2\n(\nt− 2 t+ 1\nt−1 ∑\nr=0\n(r + 1) + 1\n(t+ 1)2\nt−1 ∑\nr=0\n(r + 1)2\n)\n= a2 ( t− 2 t+ 1 t(t+ 1) 2 +\n1 (t+ 1)2 t(t+ 1)(2t+ 1) 6\n)\n≤ a 2t\n3\nThus, we set a = √\n3 k+1 , ensuring\n∥ ∥x∗k+1 ∥ ∥ = 1 so that x∗k+1 = x ∗ ∈ X . Furthermore, for the iterate x made\nbefore q rounds of queries,\nF (x) − F (x∗) = F q(x) − F k+1(x∗k+1) ≥ F q(x∗q)− F k+1(x∗k+1)\n= − 3⌊ m 2 ⌋\n8m(k + 1)\n(\n1− 1 ⌊k2 ⌋+ 1\n)\n+ 3⌊m2 ⌋\n8m(k + 1)\n(\n1− 1 k + 2\n)\n≥ 1 32k2\nwhere the last inequality holds as long as k ≥ 2. So, when ǫ < 1128 and we let k = ⌊ 1√32ǫ⌋, this ensures that\nF (x)− F (x∗) = F q(x)− F k+1(x∗k+1) ≥ ǫ\nand therefore, A must complete at least q rounds or make more than ⌈ m√ ǫ ⌉ queries to hF in order to reach an ǫ-suboptimal point. This implies a lower bound of\nmin\n(⌈\nm√ ǫ\n⌉\n, q ⌈m\n2\n⌉\n)\n≥ m 16 √ 6ǫ\nTo complete the proof, it remains to show that the gradient and prox of f ti is consistent with those of fi at every time t. Since every function operates on the (k + 1)-dimensional subspace of Rd spanned by {vr}, it will be convenient to decompose vectors into two components: x = xv + x⊥ where xv =\n∑k r=0 〈x, vr〉 vr and\nx⊥ = x− xv. Note that f ti (x) = f ti (xv). Lemma 4. For any t ≤ k and any x such that xv ∈ span {v0, v1, ..., vt−1}, if function i is queried during round t, then ∇f ti (x) = ∇fi(x).\nProof. Since function i is queried during round t, δi,t = 0 so\n∇fi(x) = 1\n4\n( δi,1 (〈x, v1〉 v0 − 2v0) + t−1 ∑\nr=1\nδi,r (〈x, vr−1〉 − 〈x, vr〉) (vr−1 − vr) ) = ∇f ti (x)\nLemma 5. For any t ≤ k and any x such that xv ∈ span {v0, v1, ..., vt−1}, if function i is queried during round t then ∀β > 0, proxft\ni (x, β) = proxfi(x, β).\nProof. Up until the last step, this proof is identical to the proof of lemma 3, thus we pick up at (14):\nproxfi(x v, β) = argmin\nu−,u+ fi(u\n− + u+) + β\n2\n∥ ∥x− − u− − u+ ∥ ∥ 2\n= argmin u−,u+\n1\n8\n(\nδi,1\n(\n〈 u−, v0 〉2 − 2a 〈 u−, v0 〉\n)\n+\nt−1 ∑\nr=1\nδi,r (〈 u−, vr−1 − vr 〉)2\n+\nk ∑\nr=t+1\nδi,r (〈 u+, vr−1 − vr 〉)2 + δi,k 〈 u+, vk 〉2\n)\n+ β\n2\n(\n∥ ∥x− − u− ∥ ∥ 2 + ∥ ∥u+ ∥ ∥\n2 )\n= proxfti (x v, β)\nThe final step comes from the fact that the argmin is separable over u− and u+, meaning we can minimize the two terms individually. The terms which contain u+ are non-negative and equal to zero when u+ = 0.\nThese lemmas show that the gradient and prox of f ti at vectors which are queried during round t are consistent with the gradient and prox of fi. This confirms that our construction is sound. This proves the lower bound for ǫ < γB 2\n128 , we can extend the same lower bound to ǫ ≥ γB2\n128 using the following, very simple construction. Let\nfi(x) =\n{\n0 if function i is queried in the first m− 1 queries 2mǫ 〈x, v〉\nwhere v is a unit vector that is orthogonal to all of the first m−1 queries. This function is trivially 1-smooth. By construction, the algorithm must make at least m queries to learn the identity of v. Until it has done so, any iterate will have objective value zero, while the optimum F (x∗) = F (v) = −2ǫ. Therefore, the algorithm must make at least m queries to reach an ǫ-suboptimal solution. For ǫ ≥ γB 2\n128\nm ≥ m √ γB2\n128ǫ\nTherefore a lower bound of Ω\n(\nm √ γB2\nǫ\n)\napplies for any ǫ > 0.\nRemark: If make the additional assumption that F is minimized on the interior of X , since 0 is O(γB2)suboptimal, only 0 < ǫ < γB 2\n128 gives a non-trivial lower bound. This lower bound is shown by the first construction presented in the previous proof.\nB.4 Smooth and strongly convex components\nTheorem 4. For any γ, λ > 0 such that γλ > 73, any ǫ > 0, any ǫ0 > 3γǫ λ , any m ≥ 2, and any deterministic algorithm A with access to hF , there exists a sufficiently large dimension d = O ( m √ γ λ log ( λǫ0 γǫ )) , and m functions fi defined over X ⊆ Rd, which are γ-smooth and λ-strongly convex and where F (0)− F (x∗) = ǫ0, such that in order to find a point x̂ for which F (x̂)− F (x∗) < ǫ, A must make Ω ( m √\nγ λ log\n(\nλǫ0 γǫ\n))\nqueries\nto hF .\nProof. We will prove the theorem for 1-smooth and λ-strongly convex components for any λ < 173 . This can be extended to arbitrary constants γ and λ′ by taking λ = λ ′\nγ .\nFor any k and for ζ and C to be defined later, let\nfi(x) = 1− λ 8\n(\nδi,1\n( 〈x, v0〉2 − 2C 〈x, v0〉 ) + δi,kζ 〈x, vk〉2 + k ∑\nr=1\nδi,r 〈x, vr−1 − vr〉2 ) + λ\n2 ‖x‖2\nwhere the vectors vr and indicators δi,r are defined in the same way as in the previous proof. This function is just a multiple of the construction in the proof of theorem 3 plus the λ ‖x‖2 /2 term. It is clear that the norm term is uninformative for learning the identity of vectors vr, as the component of the gradients and proxs which is due to that term is simply a scaling of the query point. Thus, for any iterate x generated by A before completing t rounds of optimization, 〈x, vr〉 = 0 for all r ≥ t so long as the dimension is greater than the total number of queries made to hF so far plus k+1; a fact which follows directly from the previous proof. Since exactly ⌈m2 ⌉ functions are queried per round, ∑m i=1 δi,r = ⌊m2 ⌋ and thus\nF (x) = λ(Q − 1)\n8\n(\n〈x, v0〉2 − 2C 〈x, v0〉+ ζ 〈x, vk〉2 + k ∑\nr=1\n〈x, vr−1 − vr〉2 ) + λ\n2 ‖x‖2\nwhere\nQ = ⌊m2 ⌋ m ( 1 λ − 1) + 1\nBy the first order optimality conditions for F (x), its optimum x∗ must satisfy that:\n2 Q+ 1 Q− 1 〈x ∗, v0〉 − 〈x∗, v1〉 = C\n〈x∗, vr−1〉 − 2 Q+ 1 Q− 1 〈x ∗, vr〉+ 〈x∗, vr+1〉 = 0\n(\n1 + ζ + 4\nQ− 1\n)\n〈x∗, vk〉 − 〈x∗, vk−1〉 = 0\nDefining q := √ Q−1√ Q+1 < 1 and setting ζ = 1− q, it is straightforward to confirm that\nx∗ = C k ∑\nr=0\nqr+1vr\nand also that\nF (x∗) = −λC 2\n8\n(\n√ Q− 1 )2\nThus, F (0)−F (x∗) = λC28 (√ Q− 1 )2 , so by choosing C appropriately, we can make the initial suboptimality of our construction take any value ǫ0. Since F is λ-strongly convex, for any xt, F (xt)−F (x∗) ≥ λ2 ‖xt − x∗‖ 2 . Let xt be an iterate which is generated before t rounds of optimization have been completed, implying that 〈xt, vr〉 = 0 for all r ≥ t. So\nF (xt)− F (x∗) F (0)− F (x∗) ≥ λ 2 ‖xt − x∗‖ 2 λC2\n8\n(√ Q− 1 )2\n≥ 4 C2\nC2 ∑k r=t q 2r+2\n(√ Q− 1 )2\n= 4(q2t+2 − q2k+4)\n(1− q2) (√ Q− 1 )2\n= (q2t − q2k+2)√\nQ\nIf we set k + 1 = ⌈ t− 12 log q ⌉ then\nF (xt)− F (x∗) F (0)− F (x∗) ≥ (q2t − q2k+2)√ Q\n≥ q 2t\n2 √ Q\n= 1\n2 √ Q exp\n(\n−2t log 1 q\n)\n= 1\n2 √ Q exp\n( −2t log ( 1 + 2√\nQ− 1\n))\n≥ 1 2 √ Q exp ( −4t√ Q− 1 )\nand when t = ⌊√\nQ−1 4 log ǫ0 2 √ Qǫ\n⌋\nF (xt)− F (x∗) F (0)− F (x∗) ≥ ǫ\nǫ0\nTherefore, the algorithm must complete at least t rounds of queries before it can reach an ǫ-suboptimal point. If ǫ0 > 3ǫ λ and λ < 1 73 , since each round includes at least m 2 oracle queries, this implies a lower bound of m\n2 ⌊√ Q− 1 4 log ǫ0 2 √ Qǫ ⌋ ≥ m 40 √ λ log ǫ0 2 √ Qǫ ≥ m 40 √ λ log √ λǫ0 2ǫ\nqueries to hF in order to reach an ǫ-suboptimal point."
    }, {
      "heading" : "C Lower bounds for randomized algorithms",
      "text" : "To prove the deterministic lower bounds, we constructed vectors vr adversarially, orthogonalizing them to queries made by the algorithm. In the randomized setting, this is impossible, as we cannot anticipate query points. Our solution was to instead draw the important directions vi,r randomly in high dimensions. The intuition is that a given vector, in this case the query made by the algorithm, will have a very small inner product with a random unit vector with high probability if the dimension is large enough.\nUsing this fact, we construct helper functions ψc and φc to replace the absolute and squared difference functions used in the deterministic lower bounds. These functions are both flat at 0 on the interval [−c, c], meaning that the algorithm’s query needs to have a significant inner product with vi,r before the oracle needs to give that vector away as a gradient or prox. We will show that each one of our constructions satisfies the following property:\nProperty 1. For all i, all t ≤ k and x such that ∀r ≥ t |〈x, vi,r〉| < c2 , if t is odd, then\n∂fi,1(x) ⊆ span {x, vi,0, ..., vi,t−1} and ∂fi,2(x) ⊆ span {x, vi,0, ..., vi,t}\nproxfi,1 (x, β) ∈ span {x, vi,0, ..., vi,t−1} and proxfi,2(x, β) ∈ span {x, vi,0, ..., vi,t} and if t is even, then\n∂fi,1(x) ⊆ span {x, vi,0, ..., vi,t} and ∂fi,2(x) ⊆ span {x, vi,0, ..., vi,t−1}\nproxfi,1 (x, β) ∈ span {x, vi,0, ..., vi,t} and proxfi,2(x, β) ∈ span {x, vi,0, ..., vi,t−1}\nIn other words, when x has a small inner product with vi,r for all r ≥ t, then querying either fi,1 or fi,2 at x will reveal at most vi,t. Our bounds on the complexity of optimizing our functions are based on the principle that the algorithm can only learn one vi,r per query, so we need to control the probability that the hypotheses of these lemmas hold for every query made by the algorithm. In this section, we will bound how large the dimensionality of the problem needs to be to ensure that with high probability, only one vector is revealed to the algorithm by each oracle response.\nWe consider the following setup:\n• For i = 1, 2, ...,m/2, fi,1 and fi,2 are pairs of component functions that satisfy Property 1 to be optimized by the randomized algorithm, we can assume that m is even by letting the last component function be 0 if m is odd, at the cost of a factor of (m− 1)/m to the complexity.\n• For i = 1, 2, ...,m/2 and r = 1, ..., k, {vi,r} is a uniformly random set of orthonormal vectors in Rd. • We denote the nth query made by the algorithm q(n) = ( i(n), j(n), x(n), β(n) ) , which is a query to\nfunction fi(n),j(n) at the point x (n) with the prox parameter β(n). We require that ∃B s.t.\n∥ ∥x(n) ∥ ∥ ≤ B for all n; this will be justified in the individual lower bound proofs. The nth query is allowed to depend on the previous n − 1 queries, the oracle’s responses to those queries, and the randomness in the algorithm.\n• For n = 1, ..., N , let Sn = span { x(t) : t < n, i(t) = i(n) } and let ⊥ (Sin) be its orthogonal complement. • Let PSv be the projection of the vector v onto the subspace S, and P⊥S v be its projection onto ⊥ (S).\n• Let t(n) = ∑n−1\nn′=1 1\n(\ni(n ′) = i(n)\n)\n. The counter t(n) keeps track of the number of times that function\nfi(n),1 or fi(n),2 has been queried by the algorithm before the n th query.\n• Let Un = { vi(n),r : r ≥ t(n) } . This is the set of vectors vi,r which are supposed to be “unknown” to\nthe algorithm before the nth query.\nUltimately, we want to prove the following statement:\nP\n( ∀n ∀v ∈ Un ∣ ∣ ∣ 〈 x(n), v 〉∣ ∣ ∣ < c\n2\n)\n> 1− δ (15)\nwhen the dimension is adequately large. The main difficulty here is that queries made by the algorithm are allowed to depend on the oracle’s responses to previous queries, which in turn depends on the vectors v. Therefore, there is a complicated statistical dependence between x(n) and v ∈ Un for each n > 1, which makes analyzing the distribution of the inner product hard. We will get around this by proving a slightly different statement, and then show that it implies (15).\nDefine the following “good” event:\nGn =\n[\n∀v ∈ Un\n∣ ∣ ∣ ∣ ∣ 〈 x(n) ∥ ∥x(n) ∥ ∥ , P⊥Snv 〉∣ ∣ ∣ ∣ ∣ < α ]\nwhere α = c 2B( √ N+1) . The following lemma shows why Gn is a useful thing to look at: Lemma 6. For any c > 0 and N , [\n⋂N n=1 Gn\n]\n=⇒ [ ∀n ≤ N ∀v ∈ Un 〈 x(n), v 〉 < c2 ] .\nProof. Because ∥ ∥x(n) ∥ ∥ ≤ B and ⋂nn′=1 Gn′ ∣\n∣ ∣\n〈 x(n), v 〉∣ ∣ ∣ = ∥ ∥ ∥x(n) ∥ ∥ ∥\n∣ ∣ ∣ ∣ ∣ 〈 x(n) ∥ ∥x(n) ∥ ∥ , P⊥Snv 〉 + 〈 x(n) ∥ ∥x(n) ∥ ∥ , PSnv 〉∣ ∣ ∣ ∣ ∣\n≤ B ( α+ √ n− 1α ) ≤ Bα+Bα √ N ≤ c 2\nNow that we know that [∀n Gn] implies (15), we prove the following:\nLemma 7. For any set of functions satisfying Property 1, any c > 0, any 0 < δ < 1, any k,N , and any dimension d ≥ 32B2Nc2 log ( kN δ ) , P ( ∀n ≤ N ∀v ∈ Un ∣ ∣ 〈 x(n), v 〉∣ ∣ < c2 ) > 1− δ\nProof.\nP\n(\nN ⋂\nn=1\nGn\n)\n= N ∏\nn=1\nP (Gn | G<n)\nwhere G<n is shorthand for ⋂n−1\nn′=1 Gn′ . We lower bound each term of the product by showing that for any sequence of n queries q(1), ..., q(n)\nP\n\n  ∀v ∈ Un\n∣ ∣ ∣ ∣ ∣ 〈 x(n) ∥ ∥x(n) ∥ ∥ , P⊥Snv 〉∣ ∣ ∣ ∣ ∣ < α G<n, q (1), ..., q(n)\n\n \n= P\n\n  ∀v ∈ Un\n∣ ∣ ∣ ∣ ∣ 〈 P⊥Snx (n) ∥ ∥x(n) ∥ ∥ , P⊥Snv 〉∣ ∣ ∣ ∣ ∣ < α G<n, q (1), ..., q(n)\n\n \n> 1− δ′\nMarginally, each v is uniformly random on the unit sphere. Consequently, v projected onto any fixed subspace is independent of the projection onto the orthogonal complement of that subspace when conditioned on the norm of the projection, so P⊥Snv ⊥ PSnv ∣\n∣ ‖PSnv‖. Conditioned on G<n, 6 and Property 1 ensures that the oracle’s responses to the first n − 1 queries were independent of v. So, v is independent of the queries conditioned on G<n and ‖PStv‖, and those events depend only on v’s projection onto the subspace St. Therefore, P ⊥ Sn v remains uniformly distributed on the sphere of radius √\n1− ‖PSnv‖2 in the subspace ⊥ (Sn). Furthermore, since the projection operator is non-expansive:\nP\n\n \n∣ ∣ ∣ ∣ ∣ 〈 P⊥Snx (n) ∥ ∥x(n) ∥ ∥ , P⊥Snv 〉∣ ∣ ∣ ∣ ∣ ≥ α v ∈ Un, G<n, q(1), ..., q(n), ‖PSnv‖\n\n \n< P\n\n \n∣ ∣ ∣ ∣ ∣ 〈 P⊥Snx (n) ∥\n∥P⊥Snx (n)\n∥ ∥\n, P⊥Snv ∥\n∥P⊥Snv ∥ ∥\n〉∣\n∣ ∣ ∣ ∣ ≥ α v ∈ Un, G<n, q(1), ..., q(n), ‖PSnv‖\n\n \nLet d′ = dim(⊥ (Sn)), then this is the inner product between two unit vectors, one fixed, and one uniformly random on the unit sphere in Rd ′ . The set of vectors for which the absolute value of the inner product is greater than α are two “ends” of the sphere which lie above and below circles of radius √ 1− α2. The total surface area of the two portions of the sphere is strictly less than the surface area of the sphere of radius√ 1− α2. Therefore,\nP\n\n \n∣ ∣ ∣ ∣ ∣ 〈 P⊥Snx (n) ∥\n∥P⊥Snx (n)\n∥ ∥\n, P⊥Snv ∥\n∥P⊥Snv ∥ ∥\n〉∣\n∣ ∣ ∣ ∣ ≥ α v ∈ Un, G<n, q(1), ..., q(n), ‖PSnv‖\n\n \n<\n√ 1− α2d ′−1\n1d′−1 =\n( 1− α2 )\nd′−1 2\nand since d′ ≥ d− n and 1− x ≤ e−x\n( 1− α2 ) d′−1 2 ≤ e− α2(d−n−1) 2 ≤ e− α2(d−N−1) 2\nUsing this result and a union bound over the set Un which has size at most k:\nP\n\n  ∀v ∈ Un\n∣ ∣ ∣ ∣ ∣ 〈 x(n) ∥ ∥x(n) ∥ ∥ , P⊥Snv 〉∣ ∣ ∣ ∣ ∣ < α G<n, q (1), ..., q(n), ‖PSnv‖\n\n  > 1− ke−\nα2(d−N−1) 2\nSince this argument applied for any n, any sequence of queries q(1), ..., q(n), and any value of ‖PSnv‖ that is consistent with G<n, this implies that\n∀n P (Gn | G<n) > 1− ke− α2(d−N−1) 2\n=⇒ P ( N ⋂\nn=1\nGn\n)\n>\n(\n1− ke− α2(d−N−1) 2\n)N\n≥ 1− kNe− α2(d−N−1) 2\nSo, when\nd ≥ 2 α2 log\n(\nkN\nδ\n)\n+N + 1\nthen\nP\n(\nN ⋂\nn=1\nGn\n)\n> 1− kNe− α2(d−N−1) 2\n≥ 1− kNe− log kNδ = 1− δ\nApplying lemma 6 completes the proof.\nTogether, Lemma 7 and Property 1 allow us to ensure that any algorithm can only learn one important vector per query with high probability as long as the dimension is large enough. What is left is to show that Property 1 holds for each of our constructions and to bound the suboptimality of any iterate that has small inner product with the vectors in Un.\nC.1 Non-smooth and not strongly convex components\nWe first consider the Lipschitz and non-strongly convex setting and prove theorem 5:\nTheorem 5. For any L,B > 0, any 0 < ǫ < LB 10 √ m , any m ≥ 2, and any randomized algorithm A with access to hF , there exists a dimension d = O ( L4B6 ǫ4 log ( LB ǫ ) )\n, and m functions fi defined over X = {\nx ∈ Rd : ‖x‖ ≤ B } , which are convex and L-Lipschitz continuous, such that to find a point x̂ for which\nE [F (x̂)− F (x∗)] < ǫ, A must make Ω ( m+ √ mLB ǫ ) queries to hF .\nAs shown in Equations (9) and (10), we define\nψc(z) = max (0, |z| − c)\nand for values b, c, and k to be fixed later we define m/2 pairs of functions, indexed by i = 1..m/2:\nfi,1(x) = 1√ 2 |b− 〈x, vi,0〉|+ 1 2 √ k\nk ∑\nr even\nψc (〈x, vi,r−1〉 − 〈x, vi,r〉)\nfi,2(x) = 1\n2 √ k\nk ∑\nr odd\nψc (〈x, vi,r−1〉 − 〈x, vi,r〉)\nAssume for now that m is even. If m is odd, then we simply set one of the functions to 0 and the oracle complexity is reduced by a factor proportional to m−1m . At the end of this proof, we will show that the functions fi,· satisfy Property 1. Since the domain, and therefore the queries made to the oracle are bounded by B, Property 1 and Lemma 7 ensure that when the dimension is at least d = 32B 2N\nc2 log(10kN), for iterate x generated after N oracle queries, 〈x, vi,r〉 ≥ c2 for no more than N vectors vi,r with probability 9 10 . We now bound the suboptimality of (fi,1 + fi,2)/2 for any x where 〈x, vi,k〉 < c2 .\n1 2 (fi,1(x) + fi,2(x)) = 1 2 √ 2 |b− 〈x, vi,0〉|+ 1 4 √ k\nk ∑\nr=1\nψc (〈x, vi,r−1〉 − 〈x, vi,r〉)\nIt is straightforward to confirm that this function is minimized when 〈x, vi,r〉 = b for all r. Since this is also true for every i, F is minimized at xb = b ∑ m 2\ni=1 ∑k r=0 vi,r. In order that ‖xb‖ = 1 so that xb ∈ X , we set\nb = √\n2 m(k+1) . Thus,\n1 2 (fi,1(x) + fi,2(x)) − 1 2 (fi,1(x ∗) + fi,2(x ∗)) ≥ 1 2 (fi,1(x) + fi,2(x)) − 0\n≥ 1 2 √ 2 |b− 〈x, vi,0〉|+ 1 4 √ k\nk ∑\nr=1\n|〈x, vi,r−1〉 − 〈x, vi,r〉| − c\n≥ − k 4 √ k c+ 1 2 √ 2 |b− 〈x, vi,0〉|+ 1 4 √ k |〈x, vi,0〉 − 〈x, vi,k〉| ≥ − k 4 √ k c+ 1 2 √ 2 |b− 〈x, vi,0〉|+ 1 4 √ k |〈x, vi,0〉| − 1 4 √ k c 2 ≥ −2k + 1 8 √ k c+min z∈R 1 2 √ 2 |b− z|+ 1 4 √ k |z| = −2k + 1 8 √ k c+ b 4 √ k ≥ −2k + 1 8 √ k c+ 1 4k √ m\nTherefore, we set c = ǫ√ k and k = ⌊ 1 10ǫ √ m ⌋ so that\n1 2 (fi,1(x) + fi,2(x)) − 1 2 (fi,1(x ∗) + fi,2(x ∗)) ≥ − ǫ 2 + 5 2 ǫ = 2ǫ\nThis ensures that if 〈x, vi,k〉 < c2 for at least m/4 i’s, then x cannot be ǫ-suboptimal for F . Therefore, after N = m(k+1)4 queries in dimension d = 32B2N c2 log(10kN) = 32B2 c2 ( m(k+1) 4 ) log ( 10mk(k+1) 4 )\nthen F (x)− F (x∗) ≥ ǫ with probability 910 . When ǫ < 110√m , A must make at least\nm(k + 1) 4 ≥ m 4 +\n√ m\n80ǫ\nqueries with probability 910 Finally, we prove that Property 1 holds for our construction:\nProof of Propetry 1 for Lipschitz, non-strongly convex construction. First we prove the properties about the gradients:\nConsider the case when t is odd. From (9), it is clear that dψcdz (z) = 0 when |z| < c. Furthermore, for r > t,\n|〈x, vi,r−1〉 − 〈x, vi,r〉| < c. Therefore, any subgradient g1 ∈ ∂fi,1(x) and g2 ∈ ∂fi,2(x) can be expressed as\ng1 = sign(b− 〈x, vi,0〉)√\n2 vi,0 +\n1\n2 √ k\nt−1 ∑\nr even\nψ′c(〈x, vi,r−1〉 − 〈x, vi,r〉)(vi,r−1 − vi,r)\ng2 = 1\n2 √ k\nt ∑\nr odd\nψ′c(〈x, vi,r−1〉 − 〈x, vi,r〉)(vi,r−1 − vi,r)\nwhere sign(0) can take any value in the range [−1, 1] and where ψ′c is a subderivative of ψc. It is clear from these expressions that ∂fi,1(x) ⊆ span {vi,0, ..., vi,t−1} and ∂fi,2(x) ⊆ span {vi,1, ..., vi,t}. The proof for the case when t is even follows the same line of reasoning.\nWe now prove the properties about the proxs:\nSince each pair of functions fi,· operates on a separate (k + 1)-dimensional subspace, it will be useful to decompose vectors into x = xvi + x ⊥ i where x v i = ∑k r=0 〈x, vi,r〉 vi,r and x⊥i = x− xvi . First, note that\nproxfi,1(x, β) = argmin u\nfi,1(u) + β\n2 ‖x− u‖2\n= argmin uv i ,u⊥ i\nfi,1(u v i ) +\nβ\n2\n∥ ∥xvi + x ⊥ i − uvi − u⊥i ∥ ∥ 2\n= argmin uv i ,u⊥ i\nfi,1(u v i ) +\nβ 2 ‖xvi − uvi ‖2 + ∥ ∥x⊥i − u⊥i ∥ ∥ 2\n= argmin uv i\nfi,1(u v i ) +\nβ 2 ‖xvi − uvi ‖2 + argmin\nu⊥ i\nβ\n2\n∥ ∥x⊥i − u⊥i ∥ ∥ 2\n= x⊥i + argmin uv i fi,1(u v i ) +\nβ 2 ‖xvi − uvi ‖ 2\n= x⊥i + proxfi,1(x v i , β)\n(and similarly for fi,2). From there, the proof is similar to the proof of lemma 3. First, consider the function fi,2 and let t\n′ ≥ t be the smallest even number which is not smaller than t. It will be convenient to further decompose vectors into xvi = x − + x+ where x− = ∑t′−1 r=0 〈xvi , vi,r〉 vi,r and x+ = ∑k r=t′ 〈xvi , vi,r〉 vi,r . So\nfi,2(x − + x+) =\n1\n2 √ k\n∑\nr∈{1,3,...,t′−1} ψc\n(〈 x−, vi,r−1 〉 − 〈 x−, vi,r 〉)\n+ 1\n2 √ k\n∑\nr∈{t′+1,t′+3,...k} ψc\n(〈 x+, vi,r−1 〉 − 〈 x+, vi,r 〉)\n= fi,2(x −) + fi,2(x +)\nTherefore,\nproxfi,2(x v i , β) = argmin\nu−,u+ fi,2(u\n− + u+) + β\n2\n∥ ∥x− + x+ − u− − u+ ∥ ∥ 2\n= argmin u−,u+\nfi,2(u − + u+) +\nβ\n2\n∥ ∥x− − u− ∥ ∥ 2 +\nβ\n2\n∥ ∥x+ − u+ ∥ ∥ 2\n= argmin u−,u+\nfi,2(u −) + fi,2(u +) + β\n2\n∥ ∥x− − u− ∥ ∥ 2 +\nβ\n2\n∥ ∥x+ − u+ ∥ ∥ 2\n= argmin u−\nfi,2(u −) +\nβ\n2\n∥ ∥x− − u− ∥ ∥ 2 + argmin\nu+ fi,2(u\n+) + β\n2\n∥ ∥x+ − u+ ∥ ∥ 2\nSince |〈x+, vi,r〉| < c2 for all r ≥ t, |〈x+, vi,r−1〉 − 〈x+, vi,r〉| < c for r > t, which implies that fi,2(x+) = 0.\nTherefore, the objective of the second argmin is non-negative and is equal to zero when u+ = x+ so\nproxfi,2(x v i , β) = x + + argmin u− fi,2(u −) +\nβ\n2\n∥ ∥x− − u− ∥ ∥ 2\nTherefore, when t is even, t′ = t and proxfi,2(x, β) ∈ span {x, vi,0, ..., vi,t−1}, and when t is odd, t′ = t + 1 and proxfi,2 (x, β) ∈ span {x, vi,0, ..., vi,t}. A very similar line of reasoning can be used to show the statement for fi,1.\nRemark: As was mentioned before, Lemma 7 applies when the norm of every query point is bounded by B. Since all points in the domain of the optimization problem have norm bounded by B, this is not problematic. However, we can slightly modify our construction to make optimizing F hard even for algorithms that are allowed to query outside of the domain.\nWe could redefine our functions as follows:\nf ′i,j(x) =\n{\nfi,j(x) ‖x‖ ≤ B fi,j ( B x‖x‖ ) + L (‖x‖ −B) ‖x‖ > B\nf ′i,j is still continuous, and L-Lipschitz, and it also has the property that it behaves exactly like fi,j on B-ball. However, querying the oracle of f ′i,j outside of the B-ball gives no more information about the function than querying at B x‖x‖ . In fact, an algorithm that was only allowed to query within the B-ball would be able to simulate the oracle of F ′. Therefore, since the algorithm that is not allowed to query at large vectors cannot optimize F ′ quickly, and it could simulate queries with unbounded norm, it follows that querying with unbounded norm cannot improve the rate of convergence. This fact is needed in the proof of Theorem 6 below.\nC.2 Non-smooth and strongly convex components\nWe now prove Theorem 6 using a reduction from the Lipschitz and non-strongly convex setting:\nTheorem 6. For any L, λ > 0, any 0 < ǫ < L 2\n200λm , any m ≥ 2, and any randomized algorithm A with access to hF , there exists a dimension d = O ( L4\nλ3ǫ log L√ λǫ\n)\n, and m functions fi defined over X ⊆ Rd, which are L-Lipschitz continuous and λ-strongly convex, such that in order to find a point x̂ for which E [F (x̂)− F (x∗)] < ǫ, A must make Ω (\nm+ √ mL√ λǫ ) queries to hF .\nProof. Just as in the proof of Theorem 2, we assume towards contradiction that there is an algorithm A which can optimize F using o (\nm+ √ mL√ λǫ ) queries to hF in expectation. Then A could be used to minimize\nthe sum F̃ of m functions f̃i, which are convex and L-Lipschitz continuous over the domain {x : ‖x‖ ≤ B} by adding a regularizer. Let\nF (x) = 1\nm\nm ∑\ni=1\nfi(x) := 1\nm\nm ∑\ni=1\nf̃i(x) + λ\n2 ‖x‖2\nNote that fi is λ-strongly convex and since f̃i is L-Lipschitz, fi is (L+λB)-Lipschitz continuous on the same domain. Furthermore, by setting λ = ǫB2 ,\nF̃ (x) ≤ F (x) ≤ F̃ (x) + ǫ 2B2 ‖x‖2 ≤ F̃ (x) + ǫ 2\nBy assumption, A can find an x̂ such that F (x̂) − F (x∗) < ǫ2 using o (\nm+ √ m(L+λB)√\nλǫ\n) = o ( m+ √ mLB ǫ )\nqueries to hF , and ǫ\n2 > F (x̂)− F (x∗) ≥ F̃ (x̂)− F̃ (x̃∗)− ǫ 2\nThus x̂ is ǫ-suboptimal for F̃ . However, this contradicts the conclusion of theorem 5 when L > 0, λ > 0, 0 < ǫ < L 2\n200λm , and d = Ω ( L4 λ3ǫ log L√ λǫ ) leads to contradiction.\nC.3 Smooth and not strongly convex components\nTheorem 7. For any γ,B, ǫ > 0, any m ≥ 2, and any randomized algorithm A with access to hF , there exists a sufficiently large dimension d = O ( γ2B6\nǫ2 log ( γB2 ǫ ) +B2m logm ) and m functions fi defined over\nX = { x ∈ Rd : ‖x‖ ≤ B } , which are convex and γ-smooth, such that to find a point x̂ ∈ Rd for which E [F (x̂)− F (x∗)] < ǫ, A must make Ω ( m+ √ mγB2\nǫ\n)\nqueries to hF .\nWithout loss of generality, we can assume that γ = B = 1. We will first consider the case where ǫ = O ( 1 m ) and prove that A must make Ω (√\nm ǫ\n)\nqueries to hF . Afterwards, we will show a lower bound of Ω(m) in the large-ǫ regime where that term dominates.\nThe function construction in this case is very similar to the non-smooth randomized construction. As in Equation (11)\nφc(z) =\n\n \n  0 |z| ≤ c 2(|z| − c)2 c < |z| ≤ 2c z2 − 2c2 |z| > 2c\nThe key properties of this function for this proof are that it is convex, everywhere differentiable and 4-smooth, and when |z| ≤ c, the function is constant at 0. It is also useful to note that\n0 ≤ z2 − φc(z) ≤ 2c2 (16)\nAs in Equation (12), for values a and k to be fixed later, we define the pairs of functions for i = 1, ...,m/2:\nfi,1(x) = 1\n16\n\n〈x, vi,0〉2 − 2a 〈x, vi,0〉+ ∑\nr∈{2,4,...}≤k φc (〈x, vi,r−1〉 − 〈x, vi,r〉)\n\n\nfi,2(x) = 1\n16\n\n\n∑\nr∈{1,3,...}≤k φc (〈x, vi,r−1〉 − 〈x, vi,r〉) + φc (〈x, vi,k〉)\n\n\nwith orthonormal vectors vi,r chosen randomly on the unit sphere in R d as for Theorem 5.\nAt the end of this proof, we will show that the functions fi,· satisfy Property 1. Since the domain, and therefore the queries made to the oracle are bounded by B, Property 1 and Lemma 7 ensure that when the dimension is at least d = 32B 2N\nc2 log(10kN) then after N oracle queries, 〈x, vi,r〉 ≥ c2 for no more than N vectors vi,r with probability 9 10 . Now, we will bound the suboptimality of Fi(x) := (fi,1(x)+fi,2(x))/2 at an iterate x such that |〈x, vi,r〉| < c2 for all r ≥ t. From the definition of φc:\nFi(x) = 1\n32\n(\n〈x, vi,0〉2 − 2a 〈x, vi,0〉+ k ∑\nr=1\nφc (〈x, vi,r−1〉 − 〈x, vi,r〉) + φc (〈x, vi,k〉) )\n= 1\n32\n(\n〈x, vi,0〉2 − 2a 〈x, vi,0〉+ t ∑\nr=1\nφc (〈x, vi,r−1〉 − 〈x, vi,r〉) )\nFi(x) ≤ 1\n32\n(\n〈x, vi,0〉2 − 2a 〈x, vi,0〉+ t ∑\nr=1\n(〈x, vi,r−1〉 − 〈x, vi,r〉)2 + 〈x, vi,t〉2 )\nFi(x) ≥ 1\n32\n(\n〈x, vi,0〉2 − 2a 〈x, vi,0〉+ t ∑\nr=1\n(〈x, vi,r−1〉 − 〈x, vi,r〉)2 + 〈x, vi,t〉2 )\n− t+ 1 16 c2\nDefine\nF t+1i (x) := 1\n32\n(\n〈x, vi,0〉2 − 2a 〈x, vi,0〉+ t ∑\nr=1\n(〈x, vi,r−1〉 − 〈x, vi,r〉)2 + 〈x, vi,t〉2 )\nand note that in the proof of Theorem 3 we already showed that that the optimum of F ti is achieved at\nx∗i,t = a t−1 ∑\nr=0\n(\n1− r + 1 t+ 1\n)\nvi,r\nand\nF ti ( x∗i,t )\n= −a 2\n32\n(\n1− 1 t+ 1\n)\nand ∥\n∥x∗i,t ∥ ∥\n2 ≤ a 2t\n3\nTherefore, setting a = √\n6 m(k+1) ensures that\n∥ ∥ ∥ ∑ m 2 i=1 x ∗ i,k+1 ∥ ∥\n∥ ≤ 1. It is not necessarily true that x∗ = ∑\nm 2 i=1 x ∗ i,k+1, but it serves as an upper bound on the optimum.\nLet q := ⌊k2⌋ and consider an iterate x generated by A before it makes q− 1 queries to the functions fi,1 and fi,2. When 〈x, vi,r〉 < c2 for all r ≥ q,\nFi(x)− Fi(x∗) ≥ F qi (x)− qc2\n16 − Fi(x∗i,k+1)\n≥ F qi (x∗i,q)− F k+1i (x∗i,k+1)− qc2\n16\n= −a 2\n32\n(\n1− 1 q + 1\n)\n+ a2\n32\n(\n1− 1 k + 2\n)\n− qc 2\n16\n≥ 1 32k2m − kc 2 32\nwhere the last inequality holds as long as k ≥ 2. When ǫ < 1320m , setting c = √ 16ǫ k and k = ⌊ 1√80ǫm⌋ ≥ 2, ensures that\nFi(x)− Fi(x∗) ≥ 5 2 ǫ− ǫ 2 = 2ǫ\nTherefore, if 〈x, vi,r〉 < c2 for all r ≥ q is true for at least m4 of the i’s, then x cannot be ǫ-suboptimal for F . So, for N = mq4 in dimension d = 32B2N c2 log(10kN) = 32B2 c2 ( mq 4 )\nlog(10kN), with probability 910 , the algorithm must make at least mq4 queries in order to reach an ǫ-suboptimal point. This gives a lower bound of\nm 4 q ≥\n√ m\n48 √ 10ǫ\nwhich holds with probability 910 . To complete the first half of the proof, we prove that Property 1 holds for this construction:\nProof of Property 1 for smooth and non-strongly convex construction. First we prove the properties about gradients:\nConsider the case when t is odd. From equation 16, we can see that dφcdz (z) = 0 when |z| < c. Furthermore, for r > t, |〈x, vi,r−1〉 − 〈x, vi,r〉| < c. We can therefore express the gradients:\n∇fi,1(x) = 1\n16\n( 2 〈x, vi,0〉 − 2avi,0 + t−1 ∑\nr even\nφ′c (〈x, vi,r−1〉 − 〈x, vi,r〉) (vi,r−1 − vi,r) )\n∇fi,2(x) = 1\n16\n(\nt ∑\nr odd\nφ′c (〈x, vi,r−1〉 − 〈x, vi,r〉) (vi,r−1 − vi,r) + φ′c (〈x, vi,k〉) vi,k )\nIt is clear from these expressions that ∇fi,1(x) ∈ span {vi,0, ..., vi,t−1} and ∇fi,2(x) ∈ span {vi,0, ..., vi,t}. The proof for the case when t is even follows the same line of reasoning.\nNow, we prove the properties about proxs:\nWe follow the same line of reasoning as in the Lipschitz and non-strongly convex case. The only necessary addition is to show, that when t′ ≥ t is the smallest even number which is not smaller than t and u− = ∑t′−1\nr=0 〈uvi , vi,r〉 vi,r and u+ = ∑k r=t′ 〈uvi , vi,r〉 vi,r, then fi,2(u− + u+) = fi,2(u−) + fi,2(u+):\nfi,2(u − + u+) =\n1\n16\n(\n∑\nr∈{1,3,...,t′−1} φc\n(〈 u−, vi,r−1 〉 − 〈 u−, vi,r 〉)\n+ ∑\nr∈{t′+1,t′+3,...}<k φc\n(〈 u+, vi,r−1 〉 − 〈 u+, vi,r 〉) + φc (〈 u+, vi,k 〉)\n)\n= fi,2(u −) + fi,2(u +)\nThis same reasoning applies for fi,1 or odd t ′.\nSo far, we have shown a lower bound of Ω (√ m ǫ ) when ǫ = O ( 1 m ) . We now show a lower bound of Ω(m) for all ǫ > 0, which accounts for the first term in the lower bound Ω ( m+ √\nm ǫ\n)\nwhich dominates when\nǫ = Ω ( 1 m ) . Consider the 0-smooth functions\nfi(x) = C 〈x, vi〉\nfor any constant C > 0, and where the orthonormal vectors vi are randomly chosen as before. F reaches its minimum on the unit ball at\nargmin x:‖x‖≤1 F (x) = −1√ m\nm ∑\ni=1\nvi\nand F (x∗) = − C√ m . Using similar analysis as inside the proof of Lemma 7, if d = 2B 2 (\n1 4 √ m\n)2 log 2m ≤\n32B2m log 2m then P ( ∃i which has not been queried s.t. |〈x, vi〉| ≥ 14√m ) < 12 . So if fewer than m 2 functions have been queried, then with probability at least 12 :\nF (x) − F (x∗) ≥ (\n−C √ 31\n8 √ m + −C 8 √ m\n)\n− −C√ m ≥ 0.16C√ m\nso\nE [F (x)− F (x∗)] ≥ 0.08C√ m\nTherefore, by simply choosing C = ǫ √ m\n0.08 , we ensure that such a point x is at least ǫ-suboptimal, completing the proof for all ǫ > 0.\nAs noted above, the queries made to the oracle must be bounded for Lemma 7. Since the domain of F is the B-ball, this is easy to satisfy. If we want to ensure that our construction is still hard to optimize, even if the algorithm is allowed to query arbitrarily large vectors, then we can modify our construction in the following way;\nf ′i,j(x) =\n{\nfi,j(x) ‖x‖ ≤ B fi,j ( B x‖x‖ ) + 〈 ∇fi,j ( B x‖x‖ ) , x−B x‖x‖ 〉 ‖x‖ > B\nThis function is continuous and smooth, and also has the property that querying the oracle at a point x outside of the B-ball is cannot be more informative than querying at B x‖x‖ . That is, an algorithm that is not allowed to query outside the B-ball can simulate such queries using its restricted oracle. Since this restricted algorithm cannot optimize quickly, but can still calculate the oracle outputs that it would have recieved by querying large vectors, it follows that an unrestricted algorithm could not optimize this function quickly either.\nRemark: Another variant of (1) that one might consider is an unconstrained optimization problem, where we assume that the minimizer of F lies on the interior of that ball. In other words, we could consider a version of (1) where the gradient of F must vanish on the interior of X . In this case, there is little reason to consider any ǫ larger than γB 2\n2 , since F (0) − F (x∗) ≤ γB2 2 always (by\nsmoothness F (0) − F (x∗) ≤ 〈∇F (x∗), x0 − x∗〉 + γ2 ‖x∗‖ 2 ≤ γB22 ). Consequently, when ǫ ≥ γB2\n2 there is a trivial upper bound of zero oracle queries, as just returning the zero vector guarantees ǫ-suboptimality. We can construct functions so that Theorem 7 still applies for 0 < ǫ < 9γB 2\n128 . In the previous proof, the first construction is still valid in the unconstrained case since the minimizer lies within the unit ball. For the Ω(m) term, consider the 1-smooth functions (assume w.l.o.g. that γ = B = 1)\nfi(x) = √ m 〈x, vi〉+ ‖x‖2 2\nwhere the m orthonormal vectors vi are drawn randomly from the unit sphere in R d as in the previous construction. The gradient of F vanishes at x∗ = − 1√ m ∑m i=1 vi, (note ‖x∗‖ = 1) and F (x∗) = − 12 . Using similar techniques as inside the proof of Lemma 7 if d = 2B 2 (\n1 4 √ m\n)2 log 10m ≤ 32B2m log 10m, then for any iterate\nx generated by A before fi has been queried, P ( ∃i which has not been queried s.t. |〈x, vi〉| ≥ 14√m ) < 910 . Furthermore, if |〈x, vi〉| < 14√m for more than m 2 of the functions, then\nF (x) = 1\nm\nm ∑\ni=1\n√ m 〈x, vi〉+ ‖x‖2 2\n≥ 1 m\n(\nm 2 · √ m −1√ m + m 2 · √ m −1 4 √ m ) + m 2 · 1m + m2 · 116m 2\n= −23 64\nTherefore, if fewer than m2 functions have been queried, then with probability at least 9 10 :\nF (x)− F (x∗) ≥ −23 64 − −1 2 = 9 64\nso\nE [F (x) − F (x∗)] ≥ 9 128\nThis proves a lower bound of Ω(m) for 0 < ǫ < 9γB 2\n128 .\nC.4 Smooth and strongly convex components\nIn the smooth and strongly convex case, we cannot use the same simple reduction that was used to prove Theorem 6. Using that construction, we would be able to show a lower bound of m, but would not be able to show any dependence on ǫ, so the lower bound would be loose. Instead, we will use an explicit construction similar to the one used in Theorem 7.\nTheorem 8. For any m ≥ 2, any γ, λ > 0 such that γλ > 161m, any ǫ > 0, any ǫ0 > 60ǫ √ γ λm , and any randomized algorithm A, there exists a dimension d = O (\nγ2.5ǫ0 λ2.5ǫ log\n3 (\nλǫ0 γǫ\n) + mγǫ0λǫ logm )\n, domain X ⊆ R\nd, x0 ∈ X , and m functions fi defined on X which are γ-smooth and λ-strongly convex, and such that F (x0) − F (x∗) = ǫ0 and such that in order to find a point x̂ ∈ X such that E [F (x̂)− F (x∗)] < ǫ, A must make Ω ( m+ √\nmγ λ log\n(\nǫ0 ǫ\n√\nmλ γ\n))\nqueries to hF .\nProof. We will prove the theorem for a 1-smooth, λ-strongly convex problem, for λ < 173m , which can be generalized by scaling.\nAs in the proof for the non-strongly convex case, we introduce the 4-smooth helper function\nφc(z) =\n\n \n  0 |z| ≤ c 2(|z| − c)2 c < |z| ≤ 2c z2 − 2c2 |z| > 2c\nusing which we will construct m/2 pairs of functions, which will each be based on the following. As in previous proofs, we randomly select orthonormal vectors vi,r from R d. Then, for constants k, C, and ζ to be decided upon later; with λ̃ := m · λ; and for i = 1, ..., ⌊m/2⌋ define the following pairs of functions (if m is odd, let fm(x) = λ̃ 2m ‖x‖ 2 ):\nfi,1(x) = 1− λ̃ 16\n(\n〈x, vi,0〉2 − 2C 〈x, vi,0〉 k ∑\nr even\nφc (〈x, vi,r−1〉 − 〈x, vi,r〉) ) + λ̃\n2m ‖x‖2\nfi,2(x) = 1− λ̃ 16\n(\nζφc(〈x, vi,k〉) + k ∑\nr odd\nφc (〈x, vi,r−1〉 − 〈x, vi,r〉) ) + λ̃\n2m ‖x‖2\nWhen λ̃ ∈ [0, 1] these function are 1-smooth and λ-strongly convex. These functions also have Property 1, but we will omit the proof, as it follows directly from the proof in Appendix C.3. Intuitively, the squared norm reveals no new information about the vectors vi,r besides what is already included in the query point x.\nWhen all of the queries are bounded by B, Property 1 along with Lemma 7 ensures that when d = 32B2N\nc2 log(10kN), after the algorithm make N queries 〈x, vi,r〉 ≥ c2 for at most N of the vectors vi,r with probability 910 . For this probability bound to apply, we need that all of the queries made by the algorithm are within a B-ball around the origin. We know that F (0) − F (x∗) = ǫ0, and by strong-convexity F (0) ≥ F (x∗) + λ2 ‖x∗‖ 2 , therefore, ‖x∗‖ ≤ √ 2ǫ0 λ =: B. Since the optimum point must lie in the B-ball around the origin, we will restrict the algorithm to query only at points within the B-ball. At the end of the proof, we will show that with a small modification to the functions outside of the B-ball, querying at vectors of large norm cannot help the algorithm.\nNow it remains to lower bound the suboptimality of the pair fi,1 and fi,2 at an iterate which is orthogonal to all vectors vi,r for r > t:\nIn order to bound the suboptimality of a pair of functions i, it will be convenient to bundle up all of the terms which affect the value of 〈x∗, vi,r〉 from all m of the component functions. Most of those terms are contained in fi,1 and fi,2, however, ‖x‖2 terms in each of the other components also affect the value of 〈x∗, vi,r〉. For each i, consider the projection operator Pi which projects a vector x onto the subspace spanned by {vi,r}kr=0, and P⊥ projecting onto the space orthogonal to vi,r for all i, r. Now decompose\nλ̃ 2m ‖x‖2 = λ̃ 2m\n\n\n⌊m2 ⌋ ∑\ni=1\n‖Pix‖2 + ‖P⊥x‖2  \nGather all m of the λ̃2m ‖Pix‖ 2 terms and split them amongst fi,1 and fi,2 to make the following modified functions:\nf̃i,1(x) = fi,1 − λ̃ 2m ‖x‖2 + λ̃ 4 ‖Pix‖2\nf̃i,2(x) = fi,2 − λ̃ 2m ‖x‖2 + λ̃ 4 ‖Pix‖2\nAfter this shuffle, all of the terms affecting 〈x∗, vi,r〉 are contained in these two functions which will help the analysis. Note that there is also a remaining λ̃2 ‖P⊥x‖ 2 term, however, this term is not very important to\ntrack since we are bounding the suboptimality of F , which can only increase by considering that non-negative term and P⊥x∗ = ~0. Now, consider\n1\n2\n( f̃i,1(x) + f̃i,2(x) ) = 1− λ̃ 32\n(\n〈x, vi,0〉2 −2C 〈x, vi,0〉+ ζφc(〈x, vi,k〉)\n+\nk ∑\nr=1\nφc (〈x, vi,r−1〉 − 〈x, vi,r〉) ) + λ̃\n4 ‖Pix‖2\nIf we define\nF ti (x) := 1− λ̃ 32\n( 〈x, vi,0〉2 −2C 〈x, vi,0〉+ 〈x, vi,t〉2 + t ∑\nr=1\n(〈x, vi,r−1〉 − 〈x, vi,r〉)2 ) + λ̃\n4 ‖Pix‖2\nand\nFi(x) := 1− λ̃ 32\n( 〈x, vi,0〉2 −2C 〈x, vi,0〉+ ζ 〈x, vi,k〉2 + k ∑\nr=1\n(〈x, vi,r−1〉 − 〈x, vi,r〉)2 ) + λ̃\n4 ‖Pix‖2\nthen when |〈x, vi,r〉| < c2 F ti (x) ≤ 1\n2\n( f̃i,1(x) + f̃i,2(x) ) + (1− λ̃)(t+ 1)\n16 c2\nand for any y 1\n2\n( f̃i,1(y) + f̃i,2(y) ) ≤ Fi(y)\nand, conveniently, Fi is very similar to the construction from Appendix B.4. In particular, let Q̃ := 1 2 ( 1 λ̃ − 1) + 1, then\nF ti (x) = 1\n2\n(\nλ̃(Q̃ − 1) 8\n( 〈x, vi,0〉2 −2C 〈x, vi,0〉+ 〈x, vi,t〉2 + t ∑\nr=1\n(〈x, vi,r−1〉 − 〈x, vi,r〉)2 ) + λ̃\n2 ‖Pix‖2\n)\nFi(x) = 1\n2\n(\nλ̃(Q̃− 1) 8\n( 〈x, vi,0〉2 −2C 〈x, vi,0〉+ ζ 〈x, vi,k〉2 + k ∑\nr=1\n(〈x, vi,r−1〉 − 〈x, vi,r〉)2 ) + λ̃\n2 ‖Pix‖2\n)\nWe have already showed in Appendix B.4 that if x̂ := argminx Fi(x), and if\nC > 12\n√ ǫ\nλ̃(\n√ Q̃− 1) =⇒ 2ǫi0 := 2 (Fi(0)− Fi(x̂)) >\n30ǫ\nλ̃\nζ = 2 √\nQ̃+ 1\nλ̃ < 1\n73\nt =\n   \n√\nQ̃− 1 4 log ǫi0\n20\n√\nQ̃ǫ\n   \n|〈x, vi,r〉| ≤ c\n2 ∀r > t\nthen, 2 (\nF ti (x)− Fi(x̂) ) ≥ 10ǫ\nTherefore,\n10ǫ ≤ 2 ( F ti (x)− Fi(x̂) )\n≤ 2 ( 1\n2\n( f̃i,1(x) + f̃i,2(x) ) + (1− λ̃)(k + ζ) 16 c2 − 1 2 ( f̃i,1(x̂) + f̃i,2(x̂) )\n)\n≤ ( f̃i,1(x) + f̃i,2(x) ) + (1 − λ̃)(k + ζ)\n8 c2 −\n(\nf̃i,1(x ∗) + f̃i,2(x ∗) )\nSo (\nf̃i,1(x) + f̃i,2(x) ) − ( f̃i,1(x ∗)− f̃i,2(x∗) ) ≥ 10ǫ− (1 − λ̃)(k + ζ) 8 c2\nSetting\nc =\n√\n16ǫ\n(1− λ̃)(k + ζ) then\n( f̃i,1(x) + f̃i,2(x) ) − ( f̃i,1(x ∗)− f̃i,2(x∗) ) ≥ 10ǫ− (1− λ̃)(k + ζ) 8 c2 = 8ǫ\nTherefore, if at least m/4 of the pairs i it holds that |〈x, vi,r〉| < c2 for r > t, then\nF (x)− F (x∗) ≥ 1 m\n⌊m2 ⌋ ∑\ni=1\n( f̃i,1(x) + f̃i,2(x) ) − ( f̃i,1(x ∗)− f̃i,2(x∗) )\n≥ m 4 · 1 m · 8ǫ = 2ǫ\nAs a consequence of this, when the dimension is d = 32c2 ( 2ǫ0 λ ) ( mt 4 ) log ( 5kmt 2 ) then with probability 910 the optimization algorithm must make at least t queries to each of at least m4 pairs of functions in order to reach an ǫ-suboptimal solution in expectation. So, when\nλ ≤ 1 161m\nǫ0 ǫ ≥ 60√ mλ\nthis gives a lower bound of\n⌈m\n4\n⌉ · t ≥ m 4\n   \n√\nQ̃− 1 4 log ǫi0\n20\n√\nQ̃ǫ\n   \n≥ m 4\n   \n√\nQ̃− 1 4 log ǫ0\n40\n√\nQ̃ǫ\n   \n≥ m 4\n√\nQ̃− 1 8 log ǫ0\n40\n√\nQ̃ǫ\n≥ m 4\n3\n40 √ mλ\nlog ǫ0 √ mλ\n30ǫ\n= 3\n160\n√\nm λ log\nǫ0 √ mλ\n30ǫ\n= Ω\n(\n√\nm λ log\nǫ0 √ mλ\nǫ\n)\nThe same argument as was used in the discussion after theorem 7 to show the Ω(m) term of the lower bound can be used here, as the function in that construction was both smooth and strongly convex.\nAs mentioned above, Lemma 7 requires that the norm of all query points be bounded by B. We argued above that the optimum of F must lie within the B-ball around the origin. Even so, we can slightly modify our construction to show that even if the algorithm were allowed to query arbitrarily large points, it still would not be able to optimize F quickly. Define:\nf ′i,j(x) =\n\n\n fi,j(x) ‖x‖ ≤ B fi,j ( B x‖x‖ ) + 〈 ∇fi,j ( B x‖x‖ ) , x−B x‖x‖ 〉 + λ2 ∥ ∥ ∥x−B x‖x‖ ∥ ∥ ∥ 2 ‖x‖ > B\nThis new function is continuous, γ-smooth, and λ-strongly convex, and it also has the property that querying the function at a point x outside the B-ball, it is no more informative than querying at B x‖x‖ . That is, an algorithm that was not allowed to query outside the B-ball could simulate the result of such queries. Since that restricted algorithm can’t optimize F ′ well, as proven above, another algorithm which could query at arbitrary points, therefore could not either.\nC.5 Non-smooth components when ǫ is large\nTheorem 11. For any L,B > 0, any 10√ m < ǫ < 14 , and any m ≥ 161, there exists m functions fi which are convex and L-Lipschitz continuous defined on X = {x ∈ R : |x| ≤ B} such that for any randomized algorithm A for solving problem (1) using access to hF , A must make at least Ω ( L2B2\nǫ2\n)\nqueries to hF in order to find\na point x̂ such that E[F (x̂)− F (x∗)] < ǫ.\nProof. Without loss of generality, we can assume that L = B = 1. We construct m functions fi on R 1 in the following manner: first, sample p from the following distribution\np =\n{\n1 2 − 2ǫ w.p. 12 1 2 + 2ǫ w.p. 1 2\nThen for i = 1, ...,m, we define\nfi(x) =\n{\nx w.p. p −x w.p. 1− p\nConsider now the task of optimizing F (x) = 1m ∑m i=1 fi(x) = Y x m . Clearly, F is optimized at −sign(Y ), and as long as |Y | > 2mǫ, then any x which is ǫ-suboptimal given sign(Y ) = +1 must be at least 3ǫ-suboptimal given sign(Y ) = −1. Using Chernoff bounds, P(|Y | ≤ 2mǫ) ≤ exp(− ǫ2m2 ) < exp(−5). Therefore, since the expected suboptimality of an iterate x is at least\nE [F (x)− F (x∗)] ≥ 3P (sign(x) 6= −sign(Y )||Y | ≥ 2mǫ)P (|Y | ≥ 2mǫ) · ǫ > 3(1− exp(−5))P (sign(x) 6= −sign(Y )||Y | ≥ 2mǫ) · ǫ\nTherefore, until the algorithm has made enough queries so that\nP (sign(x) 6= −sign(Y )||Y | ≥ 2mǫ) < 1 3− 3 exp(−5)\nthe expected suboptimality is greater than ǫ. By a standard information theoretic result [2, 19], achieving that probability of success at predicting the sign of Y implies a comparable level of accuracy at distinguishing between p = 0.5 + 2ǫ and p = 0.5− 2ǫ, and that requires at least 1128ǫ2 queries to hF .\nIt is straightforward to show a lower bound of Ω ( L2\nλǫ\n)\nfor strongly convex functions using the same reduction\nby regularization as in the proofs of theorems 2 and 6. We also note that this lower bound implies a lower bound of Ω(m) for smooth functions, whether strongly convex or not. Each function fi in this construction is linear, and therefore is trivially 0-smooth. We make the gradient of each function arbitrarily large by multiplying each fi by a large number. As the multiplier grows, the algorithm need be more and more certain of the sign of Y in order to achieve a expected suboptimality of less than ǫ. Thus for a sufficiently large multiplier, the algorithm must query Ω(m) functions. We cannot force it to query more than that, of course, since it only needs to query m functions to know the sign of Y with probability 1."
    } ],
    "references" : [ {
      "title" : "A lower bound for the optimization of finite sums",
      "author" : [ "Alekh Agarwal", "Leon Bottou" ],
      "venue" : "arXiv preprint arXiv:1410.0723,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Information-theoretic lower bounds on the oracle complexity of convex optimization",
      "author" : [ "Alekh Agarwal", "Martin J Wainwright", "Peter L Bartlett", "Pradeep K Ravikumar" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Katyusha: The first truly accelerated stochastic gradient descent",
      "author" : [ "Zeyuan Allen-Zhu" ],
      "venue" : "arXiv preprint arXiv:1603.05953,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Optimal black-box reductions between optimization objectives",
      "author" : [ "Zeyuan Allen-Zhu", "Elad Hazan" ],
      "venue" : "arXiv preprint arXiv:1603.05642,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Communication complexity of distributed convex learning and optimization",
      "author" : [ "Yossi Arjevani", "Ohad Shamir" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Convex analysis and monotone operator theory in Hilbert spaces",
      "author" : [ "Heinz H Bauschke", "Patrick L Combettes" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein" ],
      "venue" : "Foundations and Trends R  © in Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "An optimal randomized incremental gradient method",
      "author" : [ "Guanghui Lan" ],
      "venue" : "arXiv preprint arXiv:1507.02000,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "A universal catalyst for first-order optimization",
      "author" : [ "Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Yu Nesterov" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate o (1/k2)",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Soviet Mathematics Doklady,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1983
    }, {
      "title" : "Prisma: Proximal iterative smoothing algorithm",
      "author" : [ "Francesco Orabona", "Andreas Argyriou", "Nathan Srebro" ],
      "venue" : "arXiv preprint arXiv:1206.2372,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "Mark Schmidt", "Nicolas Le Roux", "Francis Bach" ],
      "venue" : "arXiv preprint arXiv:1309.2388,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Stochastic optimization for machine learning. Slides of presentation at “Optimization Without Borders 2016",
      "author" : [ "Shai Shalev-Shwartz" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Communication efficient distributed optimization using an approximate newton-type method",
      "author" : [ "Ohad Shamir", "Nathan Srebro", "Tong Zhang" ],
      "venue" : "arXiv preprint arXiv:1312.7853,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Assouad, fano, and le cam",
      "author" : [ "Bin Yu" ],
      "venue" : "In Festschrift for Lucien Le Cam,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "ADMM [7], DANE [18], DISCO [20]) or for functions that can be decomposed into several “easy” parts (e.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 17,
      "context" : "ADMM [7], DANE [18], DISCO [20]) or for functions that can be decomposed into several “easy” parts (e.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "PRISMA [13]).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 15,
      "context" : "Recently, stochastic methods such as SDCA [16], SAG [14], SVRG [8], and other variants, have been presented which leverage the finite nature of the problem to reduce the variance in stochastic gradient estimates and obtain guarantees that dominate both batch and stochastic gradient descent.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 13,
      "context" : "Recently, stochastic methods such as SDCA [16], SAG [14], SVRG [8], and other variants, have been presented which leverage the finite nature of the problem to reduce the variance in stochastic gradient estimates and obtain guarantees that dominate both batch and stochastic gradient descent.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : "Recently, stochastic methods such as SDCA [16], SAG [14], SVRG [8], and other variants, have been presented which leverage the finite nature of the problem to reduce the variance in stochastic gradient estimates and obtain guarantees that dominate both batch and stochastic gradient descent.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 16,
      "context" : "As methods with improved complexity, such as accelerated SDCA [17], accelerated SVRG, and Katyusha [3], have been presented, researchers have also tried to obtain lower bounds on the best possible complexity in this settings—but as we survey below, these have not been satisfactory so far.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 2,
      "context" : "As methods with improved complexity, such as accelerated SDCA [17], accelerated SVRG, and Katyusha [3], have been presented, researchers have also tried to obtain lower bounds on the best possible complexity in this settings—but as we survey below, these have not been satisfactory so far.",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "Agarwal and Bottou [1] presented a lower bound of Ω ( m+ √ mγ λ log 1 ǫ ) .",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 8,
      "context" : "Improving upon this, Lan [9] shows a similar lower bound for a restricted class of randomized algorithms: the algorithm must select which component to query for a gradient by drawing an index from a fixed distribution, but the algorithm must otherwise be deterministic in how it uses the gradients, and its iterates must lie in the span of the gradients it has received.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "Another recent observation [15] was that with access only to random component subgradients without knowing the component’s identity, an algorithm must make Ω(m) queries to optimize well.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 4,
      "context" : "Our deterministic lower bounds are inspired by a lower bound on the number of rounds of communication required for optimization when each fi is held by a different machine and when iterates lie in the span of certain permitted calculations [5].",
      "startOffset" : 240,
      "endOffset" : 243
    }, {
      "referenceID" : 11,
      "context" : "Running accelerated gradient descent (AGD) [12] on F (x) using these exact gradients achieves the upper complexity bounds for deterministic algorithms and smooth problems (see Table 1).",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "SAG [14], SVRG [8] and related methods use randomization to sample components, but also leverage the finite nature of the objective to control the variance of the gradient estimator used.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 7,
      "context" : "SAG [14], SVRG [8] and related methods use randomization to sample components, but also leverage the finite nature of the objective to control the variance of the gradient estimator used.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "Accelerating these methods using the Catalyst framework [10] ensures that for λ-strongly convex objectives we have E [ F (x(k))− F (x∗) ] < ǫ after k = O (( m+ √ mγ λ ) log ǫ0 ǫ ) iterations, where F (0)− F (x∗) = ǫ0.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "Katyusha [3] is a more direct approach to accelerating SVRG which avoids extraneous log-factors, yielding the complexity k = O (( m+ √ mγ λ ) log ǫ0 ǫ ) indicated in Table 1.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 3,
      "context" : "The log-factor in the second term can be removed using the more delicate reduction of Allen-Zhu and Hazan [4], which involves optimizing Fλ(x) for progressively smaller values of λ, yielding the upper bound in the table.",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 16,
      "context" : "Accelerated SDCA [17] achieves a similar complexity using gradient and prox oracle access.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "This idea was used in order to apply Katyusha [3] and accelerated SDCA [17] to non-smooth objectives.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 16,
      "context" : "This idea was used in order to apply Katyusha [3] and accelerated SDCA [17] to non-smooth objectives.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "29], following [11]).",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "To avoid this, we again apply the reduction of Allen-Zhu and Hazan [4], this time optimizing F̃ (β) for increasingly large values of β.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "—this matches the presentation of Allen-Zhu [3] and is similar to that of Shalev-Shwartz and Zhang [17].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "—this matches the presentation of Allen-Zhu [3] and is similar to that of Shalev-Shwartz and Zhang [17].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "Optimizing F (x) to within ǫ accuracy then implies recovering the bias of the Bernoulli random variable, which requires Ω(1/ǫ) queries based on a standard information theoretic result [2, 19].",
      "startOffset" : 184,
      "endOffset" : 191
    }, {
      "referenceID" : 18,
      "context" : "Optimizing F (x) to within ǫ accuracy then implies recovering the bias of the Bernoulli random variable, which requires Ω(1/ǫ) queries based on a standard information theoretic result [2, 19].",
      "startOffset" : 184,
      "endOffset" : 191
    }, {
      "referenceID" : 0,
      "context" : "Indeed, several attempts have been made at lower bounds for the finite sum setting [1, 9].",
      "startOffset" : 83,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "Indeed, several attempts have been made at lower bounds for the finite sum setting [1, 9].",
      "startOffset" : 83,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "This method attains a recent lower bound for distributed optimization, resolving a question raised by Arjevani and Shamir [5], and when the number of machines is very large improves over all other known distributed optimization methods for the problem.",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : ", in [9]), and could potentially be useful for establishing randomized lower bounds also in other settings.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 3,
      "context" : "Acknowledgements: We thank Ohad Shamir for his helpful discussions and for pointing out [4].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "References [1] Alekh Agarwal and Leon Bottou.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2] Alekh Agarwal, Martin J Wainwright, Peter L Bartlett, and Pradeep K Ravikumar.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Zeyuan Allen-Zhu.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Zeyuan Allen-Zhu and Elad Hazan.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Yossi Arjevani and Ohad Shamir.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Heinz H Bauschke and Patrick L Combettes.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Rie Johnson and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Guanghui Lan.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Yu Nesterov.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Yurii Nesterov.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Francesco Orabona, Andreas Argyriou, and Nathan Srebro.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] Mark Schmidt, Nicolas Le Roux, and Francis Bach.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] Shai Shalev-Shwartz.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Shai Shalev-Shwartz and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] Shai Shalev-Shwartz and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] Ohad Shamir, Nathan Srebro, and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] Bin Yu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "The solution is the AdaptSmooth algorithm [4].",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "+ λ̃ 2m ‖x‖ When λ̃ ∈ [0, 1] these function are 1-smooth and λ-strongly convex.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "By a standard information theoretic result [2, 19], achieving that probability of success at predicting the sign of Y implies a comparable level of accuracy at distinguishing between p = 0.",
      "startOffset" : 43,
      "endOffset" : 50
    }, {
      "referenceID" : 18,
      "context" : "By a standard information theoretic result [2, 19], achieving that probability of success at predicting the sign of Y implies a comparable level of accuracy at distinguishing between p = 0.",
      "startOffset" : 43,
      "endOffset" : 50
    } ],
    "year" : 2016,
    "abstractText" : "We provide tight upper and lower bounds on the complexity of minimizing the average of m convex functions using gradient and prox oracles of the component functions. We show a significant gap between the complexity of deterministic vs randomized optimization. For smooth functions, we show that accelerated gradient descent (AGD) and an accelerated variant of SVRG are optimal in the deterministic and randomized settings respectively, and that a gradient oracle is sufficient for the optimal rate. For non-smooth functions, having access to prox oracles reduces the complexity and we present optimal methods based on smoothing that improve over methods using just gradient accesses.",
    "creator" : "LaTeX with hyperref package"
  }
}