{
  "name" : "1502.04585.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Ladder: A Reliable Leaderboard for Machine Learning Competitions",
    "authors" : [ "Avrim Blum", "Moritz Hardt" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this work, we introduce a notion of leaderboard accuracy tailored to the format of a competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from an actual competition hosted by Kaggle.\nNotably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever."
    }, {
      "heading" : "1 Introduction",
      "text" : "Machine learning competitions have become an extremely popular format for solving prediction and classification problems of all kinds. A number of companies such as Netflix have organized major competitions in the past and some start-ups like Kaggle specialize in hosting machine learning competitions. In a typical competition hundreds of participants will compete for prize money by repeatedly submitting classifiers to the host in an attempt to improve on their previously best score. The score reflects the performance of the classifier on some subset of the data, which are typically partitioned into two sets: a training set and a test set. The training set is publicly available with both the individual instances and their corresponding class labels. The test set is publicly available as well, but the class labels are withheld. Predicting these missing class labels is the goal of the participant and a valid submission is simply a list of labels—one for each point in the test set.\nThe central component of any competition is the leaderboard which ranks all teams in the competition by the score of their best submission. This leads to the fundamental problem of maintaining a leaderboard that accurately reflects the true strength of a classifier. What makes this problem so challenging is that participants may begin to incorporate the feedback from the\nar X\niv :1\n50 2.\n04 58\n5v 1\n[ cs\n.L G\n] 1\n6 Fe\nb 20\nleaderboard into the design of their classifier thus creating a dependence between the classifier and the data on which it is evaluated. In such cases, it is well known that the holdout set no longer gives an unbiased estimate of the classifier’s true performance. To counteract this problem, existing solutions such as the one used by Kaggle further partition the test set into two parts. One part of the test set is used for computing scores on the public leaderboard. The other is used to rank all submissions after the competition ended. This final ranking is often referred to as the private leaderboard. While this solution increases the quality of the private leaderboard, it does not address the problem of maintaining accuracy on the public leaderboard. Indeed, numerous posts on the forums of Kaggle report on the problem of “overfitting to the holdout” meaning that some scores on the public leaderboard are inflated compared to final scores. To mitigate this problem Kaggle primarily restricts the rate of re-submission and to some extent the numerical precision of the released scores.\nYet, in spite of its obvious importance, there is relatively little theory on how to design a leaderboard with rigorous quality guarantees. Basic questions remain difficult to assess, such as, can we a priori quantify how accurate existing leaderboard mechanisms are and can we design better methods?\nWhile the theory of estimating the true loss of a classifier or set of classifiers from a finite sample is decades old, much of theory breaks down due to the sequential and adaptive nature of the estimation problem that arises when maintaining a leaderboard. First of all, there is no a priori understanding of which learning algorithms are going to be used, the complexity of the classifiers they are producing, and how many submissions there are going to be. Indeed, submissions are just a list of labels and do not even specify how these labels were obtained. Second, any submission might incorporate statistical information about the withheld class labels that was revealed by the score of previous submissions. In such cases, the public leaderboard may no longer provide an unbiased estimate of the true score. To make matters worse, very recent results suggest that maintaining accurate estimates on a sequence of many adaptively chosen classifiers may be computationally intractable [HU, SU]."
    }, {
      "heading" : "1.1 Our Contributions",
      "text" : "We introduce a notion of accuracy called leaderboard accuracy tailored to the format of a competition. Intuitively, high leaderboard accuracy entails that each score represented on the leaderboard is close to the true score of the corresponding classifier on the unknown distribution from which the data were drawn. Our primary theoretical contributions are the following.\n1. We show that there is a simple and natural algorithm we call Ladder that achieves high leaderboard accuracy in a fully adaptive model of estimation in which we place no restrictions on the data analyst whatsoever. In fact, we don’t even limit the number of submissions an analyst can make. Formally, our worst-case upper bound shows that if we normalize scores to be in [0,1] the maximum error of our algorithm on any estimate is never worse than O((log(k)/n)1/3) where k is the number of submissions and n is the size of the data used to compute the leaderboard. In contrast, we observe that the error of the Kaggle mechanism (and similar solutions) scales with the number of submissions as √ k so\nthat our algorithm features an exponential improvement in k.\n2. We also prove an information-theoretic lower bound on the leaderboard accuracy demonstrating that no estimator can achieve error smaller than Ω((log(k)/n)1/2).\nComplementing our theoretical worst-case upper bound and lower bound, we make a number of practical contributions:\n1. We provide a parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever.\n2. To demonstrate the strength of our parameter-free algorithm we conduct two opposing experiments. The first is an adversarial—yet practical—attack on the leaderboard that aims to create as much of a bias as possible with a given number of submissions. We compare the performance of the Kaggle mechanism to that of the Ladder mechanism under this attack. We observe that the accuracy of the Kaggle mechanism diminishes rapidly with the number of submissions, while our algorithm encounters only a small bias in its estimates.\n3. In a second experiment, we evaluate our algorithm on real submission files from a Kaggle competition. The data set presents a difficult benchmark as little overfitting occurred and the errors of the Kaggle leaderboard were generally within the expected statistical deviations given the properties of the data set. Even on this benchmark our algorithm produced a leaderboard that is very close to that computed by Kaggle. Through a sequence of significance tests we assess that the differences between the two leaderboards on this competition are not statistically significant.\nIn summary, our algorithm supports strong theoretical results while suggesting a simple and practical solution. Importantly, it is one and the same parameter-free algorithm that withstands our adversarial attack and simultaneously achieves high utility in a real Kaggle competition.\nAn important aspect of our algorithm is that it only releases a score to the participant if the score presents a statistically significant improvement over the previously best submission of the participant. Intuitively, this prevents the participant from exploiting or overfitting to minor fluctuations in the observed score values."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "There is a vast literature on preventing overfitting in the context of model assessment and selection. See, for example, Chapter 7 of [HTF] for background. Two particularly popular practical approaches are various forms of cross-validation and bootstrapping. It is important to note though that when scoring a submission for the leaderboard, neither of these techniques applies. One problem is that participants submit only a list of labels and not the corresponding learning algorithms. In particular, the organizer of the competition has no means of retraining the model on a different split of the data. Similarly, the natural bootstrap estimate of the expected loss of a classifier given a finite sample is simply the empirical average of the loss on the finite sample, which is what existing solutions release anyway. The other substantial obstacle is that even if these methods applied, their theoretical guarantees in the adaptive setting of estimation are largely not understood.\nA highly relevant recent work [DFH+], that inspired us, studies a more general question: Given a sequence of adaptively chosen bounded functions f1, . . . , fk : X→ {0,1} over a domain X, estimate the expectations of these function Ef1, . . . ,Efk over an unknown distribution D, given n samples from this distribution. If we think of each function as expressing the loss of one\nclassifier submitted to the leaderboard, then such an algorithm could in principle be used in our setting. The main result of [DFH+] is an algorithm that achieves maximum error\nO ( min { log(k)3/7(log |X |)1/7/n2/7, (log |X | log(k)/n)1/4 }) .\nThis bound readily implies a corresponding result for leaderboard accuracy albeit worse than the one we show. One issue is that this algorithm requires the entire test set to be withheld and not just the labels as is required in the Kaggle application. The bigger obstacle is that the algorithm is unfortunately not computationally efficient and this is inherent. In fact, no computationally efficient algorithm can give non-trivial error on k > n2+o(1) adaptively chosen functions as was shown recently [HU, SU] under a standard computational hardness assumption.\nMatching this hardness result, there is a computationally efficient algorithm in [DFH+] that achieves an error bound ofO(k1/5 log(k)3/5/n2/5) which implies a bound on leaderboard accuracy that is worse than ours for all k > n1/3. They also give an algorithm (called EffectiveRounds) with accuracy O( √ r log(k)/n) when the number of “rounds of adaptivity” is at most r. While we do not have a bound on r in our setting better than k1, the proof technique relies on sample splitting and a similar argument could be used to prove our upper bound. However, our argument does not require sample splitting and this is very important for the practical applicability of the algorithm.\nWe sidestep the hardness result by going to a more specialized notion of accuracy that is surprisingly still sufficient for the leaderboard application. However, it does not resolve the more general question raised in [DFH+]. In particular, we do not always provide a loss estimate for each submitted classifier, but only for those that made a significant improvement over the previous best. This seemingly innocuous change is enough to circumvent the aforementioned hardness results."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Ben Hamner at Kaggle Inc., for providing us with the submission files from the Photo Quality competition, as well as many helpful discussions. We are grateful to Aaron Roth for pointing out an argument similar to that appearing in the proof of Theorem 3.1 in a different context. We thank John Duchi for many stimulating discussions."
    }, {
      "heading" : "1.3 Preliminaries",
      "text" : "Let X be a data domain and Y be a finite set of class labels, e.g., X = Rd and Y = {0,1}. Rather than speaking of the score of a classifier we will use the term loss with the understanding that smaller is better. A loss function is a mapping of the form ` : Y ×Y → [0,1] and a classifier is a mapping f : X→ Y . A standard loss function is the 0/1-loss defined as `01(y,y′) = 1 if y , y′ and 0 otherwise.\nWe assume that we are given a sample S = {(x1, y1), . . . , (xn, yn)} drawn i.i.d. from an unknown distribution D over X ×Y . We define the empirical loss of a classifier f on the sample S as\nRS(f ) def= 1 n n∑ i=1 `(f (xi), yi) .\n1The parameter r corresponds to the depth of the adaptive tree we define in the proof of Theorem 3.1. While we bound the size of the tree, the depth could be as large as k.\nThe true loss is defined as RD(f )\ndef= E (x,y)∼D [`(f (x), y))] .\nThroughout this paper we assume that S consists of n i.i.d. draws from D and ` is a loss function with bounded range."
    }, {
      "heading" : "2 Sequential and Adaptive Loss Estimation",
      "text" : "In this section we formally define the adaptive model of estimation that we work in and present our definition of leaderboard accuracy. Given a sequence of classifiers f1, . . . , fk and a finite sample S of size n, a fundamental estimation problem is to compute estimates R1, . . . ,Rk such that Pr {∃t ∈ [k] : |Rt −RD(ft)| > ε} 6 δ . (1) The standard way of estimating the true loss is via the empirical loss. If we assume that all functions f1, . . . , fk are fixed independently of the sample S, then Hoeffding’s bound and the union bound imply\nPr {∃t ∈ [k] : |RS(ft)−RD(ft)| > ε} 6 2k exp(−2ε2n) . (2)\nIn the adaptive setting, however, we assume that the classifier ft may be chosen as a function of the previous estimates and the previously chosen classifiers. Formally, there exists a mapping A such that for all t ∈ [k] : ft =A(f1,R1, . . . , ft−1,Rt−1) . We will assume for simplicity that A is a deterministic algorithm. The tuple (f1,R1, . . . , ft−1,Rt−1) is nevertheless a random variable due to the random sample used to compute the estimates.\nUnfortunately, in the case where the choice of ft depends on previous estimates, we may no longer apply Hoeffding’s bound to control RS(ft). In fact, recent work [HU, SU] shows that no computationally efficient estimator can achieve error o(1) on more than n2+o(1) adaptively chosen functions (under a standard hardness assumption). Since we’re primarily interested in a computationally efficient algorithm, these hardness results demonstrate that the goal of achieving the accuracy guarantee specified in inequality (1) is too stringent in the adaptive setting when k is large. We will therefore introduce a weaker notion of accuracy called leaderboard accuracy under which we can circumvent the hardness results and nevertheless achieve a guarantee strong enough for our application."
    }, {
      "heading" : "2.1 Leaderboard Accuracy",
      "text" : "The goal of an accurate leaderboard is to guarantee that at each step t 6 k, the leaderboard accurately reflects the best classifier among those classifiers f1, . . . , fk submitted so far. In other words, while we do not need an accurate estimate for each ft , we wish to maintain that the t-th estimate Rt correctly reflects the minimum loss achieved by any classifier so far. This leads to the following definition.\nDefinition 2.1. Given an adaptively chosen sequence of classifiers f1, . . . , fk , we define the leaderboard error of estimates R1, . . . ,Rk as\nlberr(R1, . . . ,Rk) def= max\n16t6k ∣∣∣∣∣min16i6tRD(fi)−Rt ∣∣∣∣∣ (3)\nGiven an algorithm that achieves high leaderboard accuracy there are two simple ways to extend it to provide a full leaderboard:\n1. Use one instance of the algorithm for each team to maintain the best score achieved by each team.\n2. Use one instance of the algorithm for each rank on the leaderboard. When a new submission comes in, evaluate it against each instance in descending order to determine its place on the leaderboard.\nThe first variant is straightforward to implement, but requires the assumption that competitors don’t use several accounts (a practice that is typically against the terms of use of a competition). The second variant is more conservative and does not need this assumption."
    }, {
      "heading" : "3 The Ladder Mechanism",
      "text" : "We introduce an algorithm called the Ladder Mechanism that achieves small leaderboard accuracy. The algorithm is very simple. For each given function, it compares the empirical loss estimate of the function to the previously smallest loss. If the estimate is below the previous best by some margin, it releases the estimate and updates the best estimate. Importantly, if the estimate is not smaller by a margin, the algorithm releases the previous best loss (rather than the new estimate). A formal description follows in Figure 1.\nTheorem 3.1. For any sequence of adaptively chosen classifiers f1, . . . , fk , the Ladder Mechanism satisfies for all t 6 k and ε > 0,\nPr {∣∣∣∣∣min16i6tRD(fi)−Rt ∣∣∣∣∣ > ε+ η} 6 exp(−2ε2n+ (1/η + 2)log(4t/η) + 1) . (4) In particular, for some η =O(n−1/3 log1/3(kn)), the Ladder Mechanism achieves with high probability,\nlberr(R1, . . . ,Rk) 6O (\nlog1/3(kn) n1/3\n) .\nProof. LetA be the adaptive analyst generating the function sequence. Fix t 6 k. The algorithmA naturally defines a rooted tree T of depth t recursively defined as follows:\n1. The root is labeled by f1 =A(∅).\n2. Each node at depth 1 < i < t corresponds to one realization (h1, r1, . . . ,hi−1, ri−1) of the random variable (f1,R1, . . . , fi−1,Ri−1) and is labeled by hi =A(h1, r1, . . . ,hi−1, ri−1). Its children are defined by each possible value of the output Ri of Ladder Mechanism on the sequence h1, r1, . . . , ri−1,hi .\nClaim 3.2. Let B = (1/η + 2)log(4t/η). Then, |T | 6 2B.\nProof. To prove the claim, we will uniquely encode each node in the tree using B bits of information. The claim then follows directly. The compression argument is as follows. We use blog(t)c 6 log(2t) bits to specify the depth of the node in the tree. We then specify the index of each 1 6 i 6 t for which Ri 6 Ri−1 − η together with the value Ri . Note that since Ri ∈ [0,1] there can be at most d1/ηe 6 (1/η) + 1 many such steps. Moreover, there are at most d1/ηe many possible values for Ri = [RS(fi)]η . Hence, specifying all such indices requires at most (1/η + 1)(log(2/η) + log(2t)) bits. It is easy that this uniquely identifies each node in the graph, since for every index i not explicitly listed we know that Ri = Ri−1. The total number of bits we used is:\n(1/η + 1)(log(2/η) + log(2t)) + log(2t) 6 (1/η + 2)log(4t/η) = B.\nThe theorem now follows by applying a union bound over all nodes in T and using Hoeffding’s inequality for each fixed node. Let F be the set of all functions appearing in T .\nPr {∃f ∈ F : |RD(f )−RS(f )| > ε} 6 2|F|exp(−2ε2n) 6 2B+1 exp(−2ε2n) 6 2exp(−2ε2n+B) .\nIn particular,\nPr {∣∣∣∣∣min16i6tRD(fi)− min16i6tRS(fi) ∣∣∣∣∣ > ε} 6 2exp(−2ε2n+B) . Moreover, it is clear that conditioned on the event that∣∣∣∣∣min16i6tRD(fi)− min16i6tRS(fi)\n∣∣∣∣∣ 6 ε, at step i∗ where the minimum of RD(fi) is attained, the Ladder Mechanism must output an estimate Ri∗ which is within ε+ η of RD(fi∗). This concludes the proof."
    }, {
      "heading" : "3.1 A lower bound on leaderboard accuracy",
      "text" : "We next show that Ω( √\nlog(k)/n) is a lower bound on the best possible leaderboard accuracy that we might hope to achieve. This is true even if the functions are not adaptively chosen but fixed ahead of time.\nTheorem 3.3. There are classifiers f1, . . . fk and a bounded loss function for which we have the minimax lower bound\ninf R sup D E [lberr(R(x1, . . . ,xn))] >Ω\n √ logk n  . Here the infimum is taken over all estimators R : Xn→ [0,1]k that take n samples from a distribution D and produce k estimates R1, . . . ,Rk = θ̂(x1, . . . ,xn). The expectation is taken over n samples from D.\nProof. We will reduce the problem of mean estimation in a certain high-dimensional distribution family to that of obtaining small leaderboard error. Our lower bound then follows from lower bounds for the corresponding mean estimation problem.\nLet X = Rk and take the functions f1, . . . , fk to be the k coordinate projections fi(x) = xi , for 1 6 i 6 k. Let the loss function be the projection onto its first argument `(y′ , y) = y′ so that `(fi(x), y) = xi . Consider the family of distributions Dε = {D1, . . . ,Dk} where Di is uniform over {0,1}k except that the i-th coordinate satisfies Ex∼Di xi = 1/2− ε for some ε ∈ (0,1/4) that we will determine later. Now, we have\nRDi (fj ) = 1/2 i , j1/2− ε o.w. Denote the mean of a distributionD by θ(D) = Ex∼D [x] and note that θ(D) = (RD(f1), . . . ,RD(fk))>. We claim that obtaining small leaderboard error on f1, . . . , fk is at least as hard estimating the means of an unknown distribution in Dε. Formally,\ninf R sup D∈Dε\nE [lberr(R(x1, . . . ,xn))] > 1 3 inf θ̂ sup D∈Dε\nE [∥∥∥θ̂(x1, . . . ,xn)−θ(D)∥∥∥∞] . (5)\nIndeed, let R be the estimator that achieves minimax leaderboard accuracy. Define the estimator θ̂ as follows:\n1. Given x1, . . . ,xn compute R1, . . .Rk = R(x1, . . . ,xn).\n2. Let i be the first coordinate in the sequence R1, . . . ,Rk which is less than 1/2− ε/2.\n3. Output the vector θ̂(x1, . . . ,xn) which is 1/2− ε in the i-th coordinate and 1/2 everywhere else.\nNote that the `∞-error of θ̂ is always at most ε, since all means parameters in the family Dε are ε-close in `∞-norm. Suppose then that lberr(R(x1, . . . ,xn)) 6 ε/3 and suppose that Di is the unknown distribution for some i ∈ [k]. In this case, we claim that ‖θ(x1, . . . ,xn)−θ(Di)‖∞ = 0. Indeed, the first coordinate for which R(x1, . . . ,xn) is less than 1/2− ε/2 must be i. This follows from the definition of leaderboard error and the assumption that R had error ε/3 on x1, . . . ,xn. This establishes inequality (5).\nFinally, it is well known and follows from Fano’s inequality that for some ε = Ω( √ log(k)/n),\ninf θ̂ sup D∈Dε\nE [∥∥∥θ̂(x1, . . . ,xn)−θ(D)∥∥∥∞] > ε .\nFor completeness we include the argument. Let V be a random index in [k] and assume that X is a random sample from Di conditional on V = i. Note that the set P = {θ(Di)}i∈[k] forms an (ε/2)-packing in the `∞-norm. Hence, by Fano’s inequality (see e.g. [Has, Tsy]),\ninf θ̂ sup D∈Dε\nE [‖ θ̂(x1, . . . ,xn)−θ(D)‖∞] > ε 4\n( 1−\nI(V ;Xn) + log2 log |P |\n) ,\nwhere I(V ;X) is the mutual information between V and X. Moreover, it is known that\nI(V ;Xn) 6 1 |P |2 ∑ i,j∈[k] Dkl ( Dni ∥∥∥∥Dnj ) 6O(ε2n) .\nIn the second inequality we used that the Kullback-Leibler divergence between a Bernoulli random variable with bias 1/2 and another one with bias 1/2−ε is at mostO(ε2) for all 0 < ε < 1/4. Moreover, the Kullback-Leibler divergence of n independent samples is at most n times the divergence of a single sample. We conclude that\ninf θ̂ sup D∈Dε\nE [∥∥∥θ̂(x1, . . . ,xn)−θ(D)∥∥∥∞] > ε4 ( 1− O(ε2n) + log2 logk ) .\nSetting ε = c √ log(k)/n for small enough constant c > 0 completes the proof."
    }, {
      "heading" : "4 A parameter-free Ladder mechanism",
      "text" : "When applying the Ladder Mechanism in practice it can be difficult to choose a fixed step size η ahead of time that will work throughout an entire competition. We therefore now give a completely parameter-free version of our algorithm that we will use in our experiments. The algorithm adaptively finds a suitable step size based on previous submissions to the algorithm. The idea is to perform a statistical significance test to judge whether the given submission improves upon the previous one. The test is such that as the best classifier gets increasingly accurate, the step size shrinks accordingly.\nThe empirical loss of a classifier is the average of n bounded numbers and follows a very accurate normal approximation for sufficiently large n so long as the loss is not biased too much towards 0. In our setting, the typical loss if bounded away form 0 so that the normal approximation is reasonable. In order to test whether the empirical loss of one classifier is significantly below the empirical loss of another classifier, it is appropriate to perform a onesided paired t-test. A paired test has substantially more statistical power in settings where the loss vectors that are being compared are highly correlated as is common in a competition.\nTo recall the definition of the test, we denote the sample standard deviation of an n-dimensional vector vector u as std(u) = √\n1 n−1 ∑n i=1(ui −mean(u))2 , where mean(u) denotes the average of the\nentries in u. With this notation, the paired t-test statistic given two vectors u and v is defined as\nt = √ n · mean(u − v)\nstd(u − v) . (6)\nKeeping this definition in mind, our parameter-free Ladder mechanism in Figure 2 is now very natural. On top of the loss estimate, it also maintains the loss vector of the previously best classifier (starting with the trivial all zeros loss vector).\nThe algorithm in Figure 2 releases the estimate of RS(ft) up to an error of 1/n which is significantly below the typical step size of Ω(1/ √ n). Looking back at our analysis, this is not a problem since such an estimate only reveals log(n) bits of information which is the same up to constant factors as an estimate that is accurate to within 1/ √ n. The more critical quantity is the step size as it controls how often the algorithm releases a new estimate. In the following sections we will show that the parameter-free Ladder mechanism achieves high accuracy both under a strong attack as well as on a real Kaggle competition."
    }, {
      "heading" : "4.1 Remark on the interpretation of the significance test",
      "text" : "For sufficiently large n, the test statistic on the left hand side of (6) is well approximated by a Student’s t-distribution with n− 1 degrees of freedom. The test performed in our algorithm at\neach step corresponds to refuting the null hypothesis roughly at the 0.15 significance level. It is important to note, however, that our use of this significance test is primarily heuristic. This is because for t > 1, due to the adaptive choices of the analyst, the function ft may in general not be independent of the sample S. In such a case, the Student approximation is no longer valid. Besides we apply the test many times, but do not control for multiple comparisons. Nevertheless, the significance test is an intuitive guide for deciding which improvements are statistically significant."
    }, {
      "heading" : "5 The boosting attack",
      "text" : "In this section we describe a new canonical attack that an adversarial analyst might perform in order to boost their ranking on the public leaderboard. Besides being practical in some cases, the attack also serves as an analytical tool to assess the accuracy of concrete mechanisms.\nFor simplicity we describe the attack only for the 0/1-loss although it generalizes to other reasonable functions such as the clipped logarithmic loss often used by Kaggle. We assume that the hidden solution is a vector y ∈ {0,1}n. The analyst may submit a vector u ∈ {0,1}n and observe (up to small enough error) the loss\n`01(y,u) def= 1 n n∑ i=1 `01(ui , yi)\nThe attack proceeds as follows:\n1. Pick u1, . . . ,uk ∈ {0,1}n uniformly at random.\n2. Observe loss estimates l1, . . . , lk ∈ [0,1].\n3. Let I = {i : li 6 1/2} .\n4. Output u∗ = maj({ui : i ∈ I}) , where the majority function is applied coordinate-wise.\nThe vector y corresponds to the target set of labels used for the public leaderboard which the analyst does not know. The vectors u1, . . . ,uk represent the labels given by a sequence of k classifiers.\nThe next theorem follows from a standard “boosting argument” using properties of the majority function and the fact that each ui for i ∈ I has a somewhat larger than expected correlation with y.\nTheorem 5.1. Assume that |li − `01(y,ui)| 6 n−1/2 for all i ∈ [k]. Then, the boosting attack finds a vector u∗ ∈ {0,1}n so that with probability 2/3,\n1 n n∑ i=1 `01(u ∗ i , yi) 6 1 2 −Ω  √ k n  . The previous theorem in particular demonstrates that the Kaggle mechanism has poor leaderboard accuracy if it is invoked with rounding parameter α 6 1/ √ n. The currently used rounding parameter is 10−5 which satisfies this assumption for all n 6 1010.\nCorollary 5.2. There is a sequence of adaptively chosen classifiers f1, . . . , fk such that if Ri denotes the minimum of the first i loss estimates returned by the Kaggle mechanism (as described in Figure 7) with accuracy α 6 1/ √ n where n is the size of the data set, then with probability 2/3 the estimates R1, . . . ,Rk have leaderboard error\nlberr(R1, . . . ,Rk) >Ω  √ k n  ."
    }, {
      "heading" : "5.1 Experiments with the boosting attack",
      "text" : "Figure 3 compares the performance of the Ladder mechanism with that of the standard Kaggle mechanism under the boosting attack. We chose N = 12000 as the total number of labels of which n = 4000 labels are used for determining the public leaderboard under either mechanism. Other parameter settings lead to a similar picture, but these settings correspond roughly to the properties of the real data set that we will analyze later. The Kaggle mechanism gives answers that are accurate up to a rounding error of 10−5. Note that 1/ √ 4000 ≈ 0.0158 so that the rounding error is well below the critical level of 1/ √ n. The vector y in the description of our attack corresponds to the 4000 labels used for the public leaderboard. Since the answers given by Kaggle only depend on these labels, the remaining labels play no role in the attack. Importantly, the attack does not need to know the indices of the labels used for the public leaderboard within the entire vector of labels.\nThe 8000 coordinates not used for the leaderboard remain unbiased random bits throughout the attack as no information is revealed. In particular, the final submission u∗ is completely random on those 8000 coordinates and only biased on the other 4000 coordinates used for the leaderboard. Therefore, once we evaluate the final submission u∗ on the test set consisting of the remaining 8000 coordinates, the resulting loss is close to its expected value of 1/2, i.e. the expected loss of a random 0/1-vector. What we observe, however, is that the Kaggle mechanism gives a strongly biased estimate of the loss of u∗.\nThe blue line in Figure 3 displays the performance of the parameter-free version of the Ladder mechanism. Instead of selecting all the vectors with loss at most 1/2 we modified the attack to be more effective against the Ladder Mechanism. Specifically, we selected all those\nvectors that successfully lowered the score compared to the previous best. As we have no information about the correlation of the remaining vectors, there is no benefit in including them in the boosting step. Even with this more effective attack, the Ladder mechanism gives a result that is correct to within the expected maximum deviation of the score on k random vectors. The intuitive reason is that every time a vector lowers the best score seen so far, the probability of a subsequent vector crossing the new threshold drops off by a constant factor. In particular there cannot be more than O(log(k)) such steps thus creating a bias of at most O( √ log(k)/n) in the boosting step."
    }, {
      "heading" : "6 Experiments on real Kaggle data",
      "text" : "To demonstrate the utility of the Ladder mechanism we turn to real submission data from Kaggle’s “Photo Quality Prediction” challenge2. Here is some basic information about the competition.\nNumber of test samples 12000 – used for private leaderboard 8400 – used for public leaderboard 3600 Number of submissions 1830 – processed successfully 1785 Number of teams 200\nOur first experiment is to use the parameter-free Ladder mechanism in place of the Kaggle mechanism across all 1785 submissions and recompute both the public and the private leaderboard. The resulting rankings turn out to be very close to those computed by Kaggle. For example, Table 1 shows the only perturbations in the ranking among the top 10 submissions.\n2https://www.kaggle.com/c/PhotoQualityPrediction\nFigure 4 plots the public versus private scores of the leading 50 submissions (w.r.t the private leaderboard). The diagonal line indicates an equal private and public score. The plot a small amount of underfitting between the public and private scores. That is, the losses on the public leaderboard generally tend to be slightly higher than on the private leaderboard. This appears to be due random fluctuations in the proportion of hard examples in the public holdout set.\nTo assess this possibility and gain further insight into the magnitude of statistical deviations of the scores, we randomly split the private holdout set into two equally sized parts and recompute the leaderboards on each part. We repeat the process 20 times independently and look at the standard deviations of the scores across these 20 repetitions. Figure 5 shows the results demonstrating that the statistical deviations due to random splitting are large relative to the difference in mean scores. In particular the amount of underfitting observed on the original split is within one standard deviation of the mean scores which cluster close to the diagonal line. We also observed that the top 50 scores are highly correlated so that across different splits the points are either mostly above or mostly below the diagonal line. This must be due to the fact that the best submissions in this competition used related classifiers that fail to predict roughly the same label set."
    }, {
      "heading" : "6.1 Statistical significance analysis",
      "text" : "To get a better sense of the statistical significance of the difference between the scores of competing submissions we performed a sequence of significance tests. Specifically, we considered the top 10 submissions taken from the Kaggle public leaderboard and tested on the private data\nif the true score of the top submission is significantly different from the rank r submission for r = 2,3, . . . ,10. A suitable test of significance is the paired t-test. The score of a submission is the mean of a large number of samples in the interval [0,2] and follows a sufficiently accurate normal approximation. We chose a paired t-test rather than an unpaired t-test, because it has far greater statistical power in our setting. This is primarily due to the strong correlation between competing submissions. See Equation 6 for a definition of the test statistic. Note that the data that determined the selection of the top 10 classifiers is independent of the data used to perform the significance tests.\nFigure 6 plots the resulting p-values before and after correction for multiple comparisons. We see that after applying a Bonferroni correction, the only submissions with a significantly different mean are 8 and 9.\nThese observations give further evidence that the small perturbations we saw in the top 10 leaderboard between the Kaggle mechanism and the Ladder mechanism are below the level of statistical significance."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We hope that the Ladder mechanism will be helpful in making machine learning competitions more reliable. Beyond the scope of machine learning competitions, it is conceivable that the Ladder mechanism could be useful in other domains where overfitting is currently a concern. For example, in the context of false discovery in the empirical sciences [Ioa, GL], one could imagine using the the Ladder mechanism as a way of keeping track of scientific progress on important public data sets.\nOur algorithm can also be seen as an intuitive explanation for why overfitting to the holdout is sometimes not a major problem even in the adaptive setting. If indeed every analyst only uses the holdout set to test if their latest submission is well above the previous best, then they effectively simulate our algorithm.\nA beautiful theoretical problem is to resolve the gap between our upper and lower bound. On the practical side, it would be interesting to use the Ladder mechanism in a real competition. One interesting question is if the Ladder mechanism actually encourages higher quality submissions by requiring a certain level of statistically significant improvement over previous submissions."
    }, {
      "heading" : "A Kaggle reference mechanism",
      "text" : "As we did for the Ladder Mechanism we describe the algorithm as if the analyst was submitting classifiers f : X→ Y . In reality the analyst only submits a list of labels. It is easy to see that such a list of labels is sufficient to compute the empirical loss which is all the algorithm needs to do. The input set S in the description of our algorithm corresponds to the set of data points (and corresponding labels) that Kaggle uses for the public leaderboard."
    } ],
    "references" : [ {
      "title" : "Preserving statistical validity in adaptive data analysis",
      "author" : [ "DFH+] Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Aaron Roth" ],
      "venue" : "In Proc. 47th Symposium on Theory of Computing (STOC). ACM,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2014
    }, {
      "title" : "The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition",
      "author" : [ "Andrew Gelman", "Eric Loken" ],
      "venue" : "or “p-hacking” and the research hypothesis was posited ahead of time,",
      "citeRegEx" : "Gelman and Loken.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gelman and Loken.",
      "year" : 2013
    }, {
      "title" : "Has’minskii. A lower bound on the risks of nonparametric estimates of densities in the uniform metric",
      "author" : [ "Z. R" ],
      "venue" : "Theory of Probability and Applications,",
      "citeRegEx" : "R.,? \\Q1978\\E",
      "shortCiteRegEx" : "R.",
      "year" : 1978
    }, {
      "title" : "The Elements of Statistical Learning",
      "author" : [ "HTF] Trevor Hastie", "Robert Tibshirani", "Jerome Friedman" ],
      "venue" : null,
      "citeRegEx" : "Hastie et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2001
    }, {
      "title" : "Preventing false discovery in interactive data analysis is hard",
      "author" : [ "HU] Moritz Hardt", "Jonathan Ullman" ],
      "venue" : "In Proc. 55th Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Hardt and Ullman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hardt and Ullman.",
      "year" : 2014
    }, {
      "title" : "Why Most Published Research Findings Are False",
      "author" : [ "Ioa] John P.A. Ioannidis" ],
      "venue" : "PLoS Medicine,",
      "citeRegEx" : "Ioannidis.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ioannidis.",
      "year" : 2005
    }, {
      "title" : "Interactive fingerprinting codes and the hardness of preventing false discovery",
      "author" : [ "SU] Thomas Steinke", "Jonathan Ullman" ],
      "venue" : "CoRR, abs/1410.1228,",
      "citeRegEx" : "Steinke and Ullman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Steinke and Ullman.",
      "year" : 2014
    }, {
      "title" : "Introduction to Nonparametric Estimation",
      "author" : [ "Tsy] Alexandre B. Tsybakov" ],
      "venue" : "Springer, 1st edition,",
      "citeRegEx" : "Tsybakov.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tsybakov.",
      "year" : 2008
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of re-submission. In this work, we introduce a notion of leaderboard accuracy tailored to the format of a competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from an actual competition hosted by Kaggle. Notably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever.",
    "creator" : "LaTeX with hyperref package"
  }
}