{
  "name" : "1702.06941.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Algebraic Formalization of Forward and Forward-backward Algorithms",
    "authors" : [ "Ai Azuma", "Masashi Shimbo", "Yuji Matsumoto" ],
    "emails" : [ "ai-a@is.naist.jp", "shimbo@is.naist.jp", "matsu@is.naist.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: forward-backward algorithm, inside-outside algorithm, sum-product algorithm, back propagation, semiring"
    }, {
      "heading" : "1. Introduction",
      "text" : "In this paper, we propose an algebraic formalization of the two important classes of dynamic programming algorithms on computation over commutative semirings. One of the classes is called forward algorithms, in which the order of computation is consistent with the dependencies among intermediate values. The other is called forward-backward algorithms, in which forward and backward passes are combined.\nAlgebraic generalizations of “forward algorithms” are formalized for many kinds of data structures, but they are developed independently. Here, the term “forward algorithms” includes not only the ordinary forward algorithm (Rabiner, 1989) on trellises (Forney, 1973) for sequence labeling like hidden Markov models (HMMs) or linear-chain conditional random\nc©2000 Ai Azuma, Masashi Shimbo, and Yuji Matsumoto.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/v1/XXXXXX.html.\nar X\niv :1\n70 2.\n06 94\n1v 1\n[ cs\n.L G\n] 2\n2 Fe\nfields (CRFs, Lafferty et al., 2001), but also the inside algorithm on derivation forests for CYK parsing, a properly scheduled unidirectional message passing on acyclic factor graphs, etc.1 Examples of data structures on which computation is algebraically generalized include trellises for sequence labeling, the set of derivations by weighted deduction system or logic programming (Goodman, 1999; Lopez, 2009; Eisner and Filardo, 2011; Kimmig et al., 2011), junction trees (Aji and McEliece, 2000), factor graphs (Kschischang et al., 2001), directed graphs (Mohri, 2002), directed hypergraphs2 (Klein and Manning, 2004; Huang, 2008), binary decision diagrams (BDDs, Wilson, 2005), and sum-product networks (Friesen and Domingos, 2016).\nPrevious studies have not considered any algebraic formalization of “bidirectional counterparts” of forward algorithms. Hereinafter, they are called “forward-backward algorithms.”1 Forward-backward algorithms include the ordinary forward-backward algorithm (Rabiner, 1989) for the ordinary forward algorithm on trellises for sequence labeling, the inside-outside algorithm (Lari and Young, 1990) for the inside algorithm on derivation forests for CYK parsing, the sum-product algorithm (Kschischang et al., 2001) for a unidirectional message-passing on acyclic factor graphs, and so on. In the current status of such one-sided research stream, we overlook an integrated and organic linkage between forward and forward-backward algorithms.\nIn some machine learning tasks, complicated variants of forward or forward-backward algorithms are necessary to be designed. However, there is no systematic framework to design such algorithms. Examples of such complicated computation include “forward-only computation” of the Baum-Welch algorithm (Tan, 1993; Sivaprakasam and Shanmugan, 1995; Turin, 1998; Miklós and Meyer, 2005; Churbanov and Winters-Hilt, 2008), the entropy gradient of CRFs (Mann and McCallum, 2007), the gradient of entropy or risk of acyclic hypergraphs (Li and Eisner, 2009), Hessian-vector products of CRFs (Tsuboi et al., 2011), cross moments of factor graphs (Ilić et al., 2012). It is a great loss to the research community that individual studies independently manage to develop these algorithms.\nHere, we outline our approach in this paper by comparing our formalization with the usual formalizations of target algorithms. In Fig. 1, we illustrate our grasp of the current status of the formalizations of target algorithms. Each of them is formalized independently. The reason is mainly because each formalization is built on top of the individual semantics of the target data structure. In contrast, we present only one formalization. Figure 2 illustrates a rough sketch of the formalization presented in this paper. Before we formalize forward and forward-backward algorithms, we introduce a unified abstraction of computation on a variety of data structures. The abstraction completely separates the formalization of the algorithms from details of data structures. Accordingly, we need only one formalization of forward and forward-backward algorithms while maximizing the range of their application. In addition, there is one more important point in Fig. 2. Forward-backward algorithms are built on top of forward algorithms. The formalization in this way provides new insight into forward-backward algorithms and the relationship between forward and forward-backward algorithms.\n1. These confusing uses of the terms “forward algorithms” and “forward-backward algorithms” in this paper may bring discomfort. However, the formalization in this paper justifies them. 2. To be more precise, a subset of directed hypergraphs called B-graphs (Gallo et al., 1993).\nOur contribution in this paper is roughly four-folded. First, we propose a computation model that can represent arbitrary computation consisting of a finite number of applications of additions and/or multiplications of a commutative semiring. The formalization presented in this paper is applicable to computation on various kinds of data structures including trellises for sequence labeling, derivation forests or acyclic hypergraphs for CYK parsing, acyclic factor graphs, a variety of decision diagrams, and so on. This wide applicability is due to the versatility of the proposed computation model on which the formalization is built. Second, algebraic structures underlying complicated computation with forward algorithms are revealed. Third, we propose a systematic framework to design complicated variants of forward algorithms by the aid of the revealed underlying algebraic structures.\nThe framework allows us to compose a complicated and difficult-to-design forward algorithm from primitive and easy-to-design forward algorithms. Fourth, an algebraic formalization of forward-backward algorithms is proposed. It naturally reveals a relationship between forward and forward-backward algorithms. In particular, it turns out that what can be computed by forward-backward algorithms is a specific case of what can be computed by forward algorithms. This fact immediately implies that what can be computed by forwardbackward algorithms can be always computed by forward algorithms. In addition, what can be computed by some instances of forward algorithms can be also done by forward-backward algorithms. The transformations between forward and forward-backward algorithms can be done in a completely systematic way, which even include a systematic transformation from the forward mode of automatic differentiation (AD) to the reverse mode (a.k.a. back propagation). We also identify a time-space trade-off between corresponding forward and forward-backward algorithms.\nThis paper is organized as follows: Section 2 describes an algebraic formalization of forward algorithms, and Section 3 describes an algebraic formalization of forward-backward algorithms. In more detail, Section 2.1 introduces a computation model on which we build formalizations throughout this paper. Sections 2.2 and 2.3 reveal algebraic structures underlying complicated forward algorithms. Section 2.3 also offers a systematic framework to compose complicated and difficult-to-design forward algorithms from primitive and easyto-design forward algorithms. Section 3.1 formalizes forward-backward algorithms in an algebraic way. Section 3.2 compares forward and forward-backward algorithms from the point of view of time-space trade-offs. Section 3.3 provides a brief note on so-called checkpoints, which trade off time and space complexity in forward-backward algorithms. Section 4 concludes this paper."
    }, {
      "heading" : "2. Forward Algorithms",
      "text" : ""
    }, {
      "heading" : "2.1 Commutative Semiring and Computation Graph",
      "text" : "Unless otherwise stated, when the extensional definition of a set, say, X = {x1, . . . , xn}, is given, we henceforth assume that the listed elements are pairwise distinct (xi 6= xj for i 6= j in other words) and thus |X| = n.\nLet N0 denote the set of all non-negative integers and N the set of all positive integers. In this subsection, we first introduce some notations related to directed acyclic graphs (dags) and then definitions related to the theory of semiring. They are combined to formalize a model of computation over a commutative semiring.\nIn this paper, parallel arcs are allowed in dags. For this reason, the arc set E of a dag G = (V,E) is a set equipped with the head function head : E → V and tail function tail : E → V that map an arc to its head and tail, respectively. For a dag G = (V,E), we denote:\n• the set of all in-arcs of a node v ∈ V by E−G(v), i.e., E − G(v) = {e ∈ E | head(e) = v},\n• the set of all out-arcs of a node v ∈ V by E+G(v), i.e., E + G(v) = {e ∈ E | tail(e) = v}, • the set of all source nodes (i.e., nodes without any in-arc) by src(G), i.e., src(G) ={ v ∈ V ∣∣ E−G(v) = ∅}, and\n• the set of all sink nodes (i.e., nodes without any out-arc) by snk(G), i.e., snk(G) ={ v ∈ V ∣∣ E+G(v) = ∅}. Definition 1 (Commutative Monoid) Let M 6= ∅ be a set, · a binary operation on M , and 1M an element of M . Then (M, ·, 1M ) is called a commutative monoid if and only if it satisfies, for every a, b, c ∈M :\n• 1M is the identity element, i.e., 1M · a = a · 1M = a,\n• the operation obeys the commutative law, i.e., a · b = b · a, and\n• the operation obeys the associative law, i.e., a · (b · c) = (a · b) · c.\nWhen there will be no confusion, an algebraic structure is often denoted simply by its underlying set. This leads to objectionable notations, e.g., M = (M, ·, 1M ).\nDefinition 1 is “multiplicatively-written,” that is to say, the operation is denoted by a multiplication-suggestive symbol, and the identity element is denoted by “1.” However, a commutative monoid may be “additively-written” as the situation demands, i.e., the operation is denoted by an addition-suggestive symbol such as “+,” and the identity element is denoted by “0.” For an additively-written commutative monoid M = (M,+, 0M ), the summation symbol∑ is used. To be precise, let X be a set, and let each element x ∈ X be associated with an element ax ∈ M . Even if X is an infinite set, we assume X ′ = {ax | x ∈ X and ax 6= 0M} is a finite set. Then we define\n∑ x∈X ax =\n{ 0M if X\n′ = ∅,∑ x∈X′ ax otherwise.\n(1)\nIn particular, if |X ′| = n and there exists a ∈M such that ax = a for every ax 6= 0M , then (1) defines the n repetitions of a, denoted by na. For a multiplicatively-written commutative monoid (M ′, ·, 1M ′), the product symbol ∏ is also defined in a similar fashion. Note, however,∏\nx∈X ax = 1M ′ if X ′ = ∅. The n-th power of a ∈M ′, denoted by an, is defined likewise.\nDefinition 2 (Monoid Homomorphism) Let M = (M, ·, 1M ) and M ′ = (M ′, , 1M ′) be commutative monoids. Then a mapping f : M → M ′ is called a (monoid) homomorphism from M to M ′ if and only if it satisfies, for every a, b ∈M :\n• f(a · b) = f(a) f(b), and\n• f(1M ) = 1M ′ .\nDefinition 3 (Commutative Semiring) Let S 6= ∅ be a set, + and · binary operations on S, and 0S and 1S elements of S. Then (S,+, ·, 0S , 1S) is called a commutative semiring if and only if it satisfies:\n• (S,+, 0S) is a commutative monoid,\n• (S, ·, 1S) is a commutative monoid,\n• the two operations are connected by the distributive law, i.e., a·(b+ c) = (a · b)+(a · c) and (a+ b) · c = (a · c) + (b · c) for every a, b, c ∈ S, and\n• 0S is absorbing, i.e., 0S · a = a · 0S = 0S for every a ∈ S.\nFor a commutative semiring S = (S,+, ·, 0S , 1S), (S,+, 0S) and (S, ·, 1S) are called the additive monoid and multiplicative monoid of S, respectively. For every a ∈ S, the n repetitions of a, denoted by na, (resp. the n-th power of a, denoted by an) is defined in terms of one defined on the additive (resp. multiplicative) monoid of S.\nNow we are ready to introduce a model of computation over a commutative semiring. The computation model to be introduced is so-called computation graphs (a.k.a. Kantorovich graphs, Rall, 1981) but where the kinds of operations are limited to two binary ones, i.e., addition and multiplication of the commutative semiring. Each value involved in the computation is attached to a source node of a dag, and each internal node of the dag designates either addition or multiplication operation of the commutative semiring. Intermediate values during the computation are associated with nodes and arcs by the forward variable, and dependencies among them are represented by arcs.\nDefinition 4 (Commutative Semiring Computation Graph3) Let G = (V,E) be a finite dag, op a mapping from V \\ src(G) to {“+”, “·”}, S a commutative semiring, and ξ a mapping from src(G) to S. Then the quadruple (G, op, S, ξ) is called a commutative semiring computation graph.\nUnless otherwise noted, we henceforth use the term “computation graph” to denote a commutative semiring computation graph for the sake of brevity.\nDefinition 5 (Forward Variable4) Let G = (V,E) be a finite dag, S = (S,+, ·, 0S , 1S) a commutative semiring, and G = (G, op, S, ξ) a computation graph. Then the forward variable of G, denoted by αG , is a mapping from V ∪ E to S that is defined by, for every node v ∈ V and every arc e ∈ E,\nαG(v) =  ξ(v) if v ∈ src(G),∑ e∈E−G (v) αG(e) if op(v) = “+”,∏\ne∈E−G (v) αG(e) otherwise (i.e., if op(v) = “·”),\nαG(e) = αG(tail(e)) .\n(2)\n3. The underlying graph of a commutative semiring computation graph is allowed to be disconnected. Disconnected commutative semiring computation graphs are necessary to model computation on “disconnected” data structures (e.g., disconnected factor graphs). 4. For purely technical reasons, forward variables (and backward variables in Definition 20) are defined not only on nodes but also on arcs. In Section 3, we formalize forward-backward algorithms with the partially ordered set (poset) naturally induced by computation graphs. If the poset were induced only by nodes, the poset would belong to the class of arbitrary posets for which some problems are intractable. In contrast, defining forward variables on both nodes and arcs makes the induced posets fall into a tractable subset called chain-antichain-complete posets, edge-induced posets, N-free posets, or quasi-series-parallel posets, which are equivalent to each other (Möhring, 1989). In fact, the proofs of some statements in Section 3 rely on the tractability of this subclass.\nIt is obvious that the above mutually recursive definition of αG on V ∪E is well-defined since G is a finite dag.\nForward algorithms are algorithms to compute values of the forward variable of computation graphs. Leaving aside “scheduling problems,” we can readily compute values of the forward variable of a computation graph since forward variables are constructively defined in Definition 5. We postpone the presentation of the pseudo-code of forward algorithms with a full discussion of scheduling problems and others until Section 3 because some additional notions are necessary to be introduced.\nLet G = (G, op, S, ξ) be a computation graph, and src(G) = {s1, . . . , sn}. By induction on the recursive definition (2) in Definition 5, it is easy to show that, for every node and arc t ∈ V ∪ E, the forward variable αG(t) is of the form\nαG(t) = ∑\ni∈N0n ct,i (ξ(s1))\ni1 · · · (ξ(sn))in , (3)\nwhere i = (i1, . . . , in), and ct,i ∈ N0 for every t ∈ V ∪ E and i ∈ N0n but only finitely many of the coefficients ct,i in the summand are different from 0, and the values of ct,i are dependent only on G, op, and t, i.e., they are independent of S and ξ. Note that while the right-hand side in (3) is an infinite sum over N0n at first glance, it is actually a finite sum and well-defined because of the condition imposed on ct,i.\nThe fact that αG(t) takes on the form of (3) can be rephrased as follows. The pair (G, op) solely determines a polynomial in the indeterminates x1, . . . , xn over N0, which is of the form ∑\ni∈N0n ct,ix1\ni1 · · ·xnin (4)\nfor every node and arc t ∈ V ∪E. On the other hand, the pair (S, ξ) specifies “substitution” of the polynomial, or “replacing xi by ξ(si) for every i” in other words, and αG(t) is equal to the result of the substitution on every t ∈ V ∪ E.5\nThe modularity offered by algebraic abstraction is based on the division of the roles between the two pairs (G, op) and (S, ξ) in a given computation graph (G, op, S, ξ). On the one hand, the pair (G, op) represents intermediate procedure that applies additionand multiplication-like operations to given values. In this procedure, the details of the underlying set and the two operations are completely abstracted away; they can be anything that obeys the axioms of commutative semiring. On the other hand, the pair (S, ξ) specifies these details and gives the abstract computation specified by (G, op) concrete meaning.\nIn order to formalize this modularity, we define the free forward variable6 on a computation graph by using the polynomial semiring over N0. Let N0[x1, . . . , xn] denote the set of all polynomials in the indeterminates x1, . . . , xn over N0, that is,\nN0[x1, . . . , xn] ={∑ i∈N0n cix1 i1 · · ·xnin ∣∣ i = (i1, . . . , in), and ci ∈ N0 but almost all ci are 0.} .\n5. See Hebisch and Weinert (1998, Chapter II) for the formal definitions of indeterminates, polynomials, substitution, etc., in particular, Definition II.1.1, Theorems II.1.3, II.1.6, II.1.8, and Remark II.1.10. 6. The term “free forward variable” is named after the fact that N0[x1, . . . , xn] is the free commutative semiring on {x1, . . . , xn}.\nThen we can equip N0[x1, . . . , xn] with the equality relation, addition, and multiplication in the usual way to make it a commutative semiring. The resulting semiring is called the polynomial semiring in the indeterminates x1, . . . , xn over N0.7\nDefinition 6 (Free Forward Variable) Let G = (V,E) be a finite dag, op a mapping from V \\ src(G) to {“+”, “·”}, src(G) = {s1, . . . , sn}, and χ : src(G) → N0[x1, . . . , xn] a function that maps each si ∈ src(G) to xi. Then, the free forward variable of (G, op) with respect to χ, denoted by α(G,op,χ), is a mapping α(G,op,χ) : V ∪ E → N0[x1, . . . , xn] defined by\nα(G,op,χ)(t) = α(G,op,N0[x1,...,xn],χ)(t)\nfor every node and arc t ∈ V ∪ E.\nBy using the free forward variable of a given computation graph (G, op, S, ξ), the division of the roles between the two pairs (G, op) and (S, ξ) can be summarized in the following lemma.\nLemma 1 (Substitution Principle of Free Forward Variable) Let G = (G, op, S, ξ) be a computation graph, and src(G) = {s1, . . . , sn}. Assume that one defines χ : src(G)→ N0[x1, . . . , xn] mapping si to xi for every si ∈ src(G), and obtains the following form of the free forward variable of (G, op) w.r.t. χ\nα(G,op,χ)(t) = ∑\ni∈N0n ct,ix1\ni1 · · ·xnin\nfor every node and arc t ∈ V ∪ E, then\nαG(t) = ∑\ni∈N0n ct,i (ξ(s1))\ni1 · · · (ξ(sn))in .\nProof The statement can be established by induction on the finite dag G.\nFigure 3 shows the graphical illustration of the substitution principle of the free forward variable for an example computation graph.\nIntroducing free forward variables allows us to analyze how the computation changes depending on various structures equipped in S and ξ without getting into details of the computation structure represented by (G, op). In what follows, when a computation graph (G, op, S, ξ) is considered, details of (G, op) are specified only by the free forward variable, and a pure focus is placed on how each structure equipped in S and ξ affects the computation. The free forward variable provides information about the computation structure specified by (G, op) to the extent that is necessary and sufficient for the development of this and the subsequent sections.\nMoreover, free forward variable combined with “substitution” (i.e., “replacing xi by ξ(si) for every i”) can model any computation insofar as the computation consists of a finite number of applications of additions and/or multiplications that obey the axioms\n7. See Hebisch and Weinert (1998, Theorem II.1.3, Definition II.1.4, and Remark II.1.10) for the formal definitions of the equality relation, addition, and multiplication.\nof a commutative semiring. Therefore, this computation model subsumes such diverse computations as the ordinary forward algorithm on trellises for sequence labeling, the inside algorithm on derivation forests or hypergraphs for CYK parsing, a unidirectional messagepassing on acyclic factor graphs.\nIn what follows, for various data structures, we illustrate the corresponding computation graphs.\nExample 1 (Computation Graph for Sequence Labeling) The upper diagram in Fig. 4 shows an example trellis for sequence labeling with 3 states numbered 0, 1, and 2, and the lower directed graph (G, op) in Fig. 4 shows the computation graph (without specifying the domain S and the values on source nodes ξ(si)) corresponding to the ordinary forward algorithm on the upper trellis. In the computation graph, the source node s0 conceptually represents the residence in the state 0 at time t = 0. The same applies to s1 and s2. The source node s3 conceptually represents the transition from the state 0 at time t = 0 to the same state at time t = 1. The same applies to si for every i ∈ {4, . . . , 11}. The source node s12 conceptually represents the residence in the state 0 at time t = 1. The same applies to s13 and s14. The same goes for all following source nodes.\nConsider the free forward variable of the lower computation graph in Fig. 4 with respect to χ : si 7→ xi. In the sum of the values of the free forward variable over all sink nodes, which obviously takes on the form (4), each term corresponds to a sequence (in other words, a joint assignment of states throughout the full period of time) in the upper trellis in Fig.\n4. Moreover, each factor in a term corresponds to the residence in a state at a moment in time or a transition in the sequence that the term corresponds to.\nFor example, each of all terms involving the factor x0 conceptually represents a sequence passing through the state 0 at time t = 0 in the upper trellis in Fig. 4. For another example, each of all terms involving the factor x3 conceptually represents a sequence containing the arc (transition) between the state 0 at time t = 0 and the same state at time t = 1 in the upper trellis in Fig 4.\nFurther consider an example of the concrete meaning of a “substitution.” Let ξ : src(G)→ R be defined by\nξ(si) =  P (T = 0, S = i)P (O = o0 | S = i) if i ∈ {0, 1, 2}, P (T = t+ 1, S = k | T = t, S = j) if 12t+ 3 ≤ i ≤ 12t+ 11 for some t ∈ N0, j = ((i− 3) mod 12− (imod 3))/3, and k = (i− 3) mod 12,\nP (O = ot | S = (imod 12)) if 12t ≤ i ≤ 12t+ 2 for some t ∈ N,\nwhere T is a discrete variable ranging over the full period of time, S is a variable ranging over all the state numbers, O is a variable ranging over a given domain of discrete observations, oi is the observation at time t = i, and mod is the modulo operator. Then, it is obvious what the substitution of the free forward variable by ξ means. That is, the values of the forward variable of the computation graph (G, op,R, ξ) completely correspond to the ordinary forward variables of the HMMs on the upper trellis in Fig. 4 for a given observation sequence (o0, o1, . . . ).\nExample 2 (Computation Graph for Acyclic Factor Graph) The upper diagram in Fig. 5 illustrates an example acyclic factor graph borrowed from Kschischang et al. (2001). The directed arrows in the upper factor graph indicate an example generalized forward/backward (GFB) schedule. The lower directed graph is the computation graph corresponding to the unidirectional message passing on the upper factor graph with respect to the GFB schedule, provided that all variables of the factor graph take on the binary values 0 or 1.\nTable 1 shows what each source node of the computation graph in Fig. 5 conceptually represents. Each source node represents either an assignment of a value to a variable or an evaluation of a factor with specific arguments.\nConsider the free forward variable of the lower computation graph in Fig. 5 with respect to χ : si 7→ xi. In the value of the free forward variable on the last + node, which obviously takes on the form (4), each term corresponds to a configuration of the upper factor graph. Moreover, each factor in a term corresponds to either an assignment of a value to a variable or an evaluation of a factor with specific arguments that is consistent with the configuration the term corresponds to.\nFurther consider an example of the concrete meaning of a “substitution.” Let the codomain of all factors be the set of real numbers, S = R, and ξ be defined such that ξ(si) = 1 if and only if si represents an assignment of a value to a variable, and if si represents an evaluation of a factor with specific arguments then ξ(si) is equal to the value of the evaluation. Then, the value of the forward variable of (G, op,R, ξ) on the last + node is equal to the sum of values over all the possible configurations in the upper factor graph.\nExample 3 (Computation Graph for Zero-suppressed Binary Decision Diagram) The left diagram in Fig. 6 illustrates an example zero-suppressed binary decision diagram (ZDD, Minato, 1993; Knuth, 2009). The right directed graph is the computation graph representing the Boolean function expressed by the left ZDD.\nThe computation graph corresponding to a ZDD such as the one in Fig. 6 can be systematically constructed. It can be constructed as we traverse the ZDD from the top node to the bottom (0 and 1) nodes.\nIn the computation graph, each source node conceptually represents an argument of the Boolean function expressed by the ZDD. s0 in the computation graph in Fig 6 conceptually represents A in the ZDD, s1 for B, and s2 for C.\nConsider that the free forward variable of the right computation graph in Fig. 6 with respect to χ : si 7→ xi. The value of the free forward variable on the sink node is equal to x0x1 + x0x2 + x2. This is the polynomial representation of the Boolean function. Each term in the polynomial represents a specific joint assignment of the Boolean values to the arguments of the Boolean function that yields 1. For example, the term x0x1 represents the joint assignment A = 1, B = 1, and C = 0.\nFurther consider that an example of the concrete meaning of a “substitution.” Let ξ be a function mapping each source node of the computation graph to a non-negative real number. Then, the value of the forward variable (G, op,R, ξ) on the sink node is equal to the summation of the weights of all joint assignments of the Boolean values to the arguments of the Boolean function that yield 1, each of whose weight, in turn, is expressed by the production of the weights ξ over all arguments that are substituted by 1. This value is fundamental in the combination of logic-based formalisms with statistical inference (e.g., Poole, 1993; Sato and Kameya, 2001; De Raedt et al., 2007).\nAlthough we only show a computation graph for a ZDD as an example, we can construct computation graphs representing the Boolean functions expressed by a variety of decision diagrams, including (reduced and ordered) BDDs (Akers, 1978; Bryant, 1992), case-factor\ndiagrams (McAllester et al., 2008), and/or multi-valued decision diagrams (Mateescu et al., 2008), and so on."
    }, {
      "heading" : "2.2 Commutative Semiring Computation Graph Parametrized by Monoid Homomorphism",
      "text" : "Our goal in this subsection is to elucidate another aspect of modularity offered by an algebraic abstraction that is not described in past research. If an additional condition is imposed on ξ of a computation graph (G, op, S, ξ), values of the forward variable of the computation graph can be shown to be equal to a form that is “globally” changed from the original form (3).\nLet us introduce a motivating example. Consider the dynamic programming computation of the normalization constant for linear-chain CRFs. The normalization constant is of the form ∑\ny\nexp (∑ i λ · F (y,x, i) ) , (5)\nwhere i ranges over positions in the sequence. The efficiency of dynamic programming is based on a step-by-step factorization of the target formula by using the distributive law of semiring. At first glance, the distributive law does not appear to be applicable to formula (5) since the function exp is applied to each summand. However, one can transform formula (5) into an equivalent one to suit the distributive law, i.e.,∑\ny\nexp(λ · F (y,x, 1)) · exp(λ · F (y,x, 2)) · · ·\nby using the addition law of the exponential function. The transformation of a formula like (5) into an equivalent one compatible with dynamic programming is very important for many machine learning tasks. In structured prediction,\nin particular, the number of summands may grow as an exponential function of the problem size. Therefore, naive computation of a formula like (5) is intractable in those cases.\nThis subsection presents an algebraic abstraction of the transformation mentioned above. That is, we present an algebraic condition that allows transformation of some formulas that are seemingly unsuitable for application of the distributive law and efficient computation by dynamic programming into equivalent ones that suit them. This algebraic abstraction covers some interesting and important problem instances that were not formalized in an algebraic way in past research.\nDefinition 7 (Sextuple Specifying Commutative Semiring Computation Graph Parametrized by Monoid Homomorphism) A sextuple (G, op,M, φ, S, f) is said to specify the f -parametrized commutative semiring computation graph (G, op, S, f ◦ φ) if and only if:\n• G = (V,E) is a finite dag,\n• op is a mapping from V \\ src(G) to {“+”, “·”},\n• M is a commutative monoid,\n• φ is a mapping from src(G) to M ,\n• S = (S,+, ·, 0S , 1S) is a commutative semiring, and\n• f is a monoid homomorphism from M to the multiplicative monoid (S, ·, 1S) of S.\nTheorem 1 Let a sextuple (G, op,M, φ, S, f) specify the f -parametrized computation graph G = (G, op, S, f ◦ φ), and src(G) = {s1, . . . , sn}. Further assume that one defines χ : src(G)→ N0[x1, . . . , xn] mapping si to xi for every si ∈ src(G), and obtains the following form of the free forward variable of (G, op) w.r.t. χ\nα(G,op,χ)(t) = ∑\ni∈N0n ct,ix1\ni1 · · ·xnin\nfor every node and arc t ∈ V ∪ E. Then, αG(t) = ∑\ni∈N0n ct,if\n( (φ(s1)) i1 · · · (φ(sn))in ) . (6)\nProof The statement follows from Lemma 1 and the homomorphism of f .\nAn important implication of Theorem 1 is as follows. The number of summands of the summation in the right-hand side of (6) might be exponentially large compared to the size of G, and the function f appears to interfere with a factorization of each summand. Therefore, at first glance, the summand of the summation of the right-hand side of (6) cannot be factorized nor computed efficiently by using the distributive law of the semiring. Actually, Eq. (6) means that it can be computed efficiently with time and space complexity proportional to the size of G since αG in the left-hand side is defined and computed by recursion on G.\nIn what follows, we give some important or interesting examples of parametrized computation graphs.\nExample 4 (Commutative Semiring Computation Graph Parametrized by Identity Function) Let S = (S,+, ·, 0S , 1S) be a commutative semiring, and the sextuple (G, op, (S, ·, 1S), φ, S, idS) specify the idS-parametrized computation graph where idS is the identity function on S. Note that idS is the trivial automorphism on the multiplicative monoid of S. The idS-parametrized computation graph is equivalent to the computation graph (G, op, S, φ).\nExample 4 shows that any computation graph can be interpreted as a computation graph parametrized by a suitable monoid homomorphism, and a formalization in terms of parametrized computation graphs is a pure extension of one in terms of plain computation graphs.\nExample 5 (exp-parametrized Commutative Semiring Computation Graph) Let R = (R,+, ·, 0, 1) be the ordinary semiring of real numbers, and (G, op, (R,+, 0), φ,R, exp) specify the exp-parametrized computation graph G = (G, op,R, exp ◦ φ). Note that the exponential function exp is a monoid homomorphism from (R,+, 0) to the multiplicative monoid (R, ·, 1) of R, that is, exp(a+ b) = exp(a)·exp(b) for every a, b ∈ R, which is nothing other than the addition law of the exponential function. Let src(G) = {s1, . . . , sn}. Further assume that one defines χ : src(G)→ N0[x1, . . . , xn] mapping si to xi for every si ∈ src(G), and obtains the following form of the free forward variable of (G, op) w.r.t. χ\nα(G,op,χ)(t) = ∑\ni∈N0n ct,ix1\ni1 · · ·xnin\nfor every node and arc t ∈ V ∪ E. Then,\nαG(t) = ∑\ni∈N0n ct,i exp(i1φ(s1) + · · ·+ inφ(sn)) .\nExample 5 corresponds to the example cited in the beginning of this subsection.\nExample 6 ((cos, sin)-parametrized Commutative Semiring Computation Graph) Let R2 = (R2,+, ·, (0, 0), (1, 0)) be the commutative semiring equipped with the addition and multiplication defined by\n(a, b) + (c, d) = (a+ c, b+ d) and\n(a, b) · (c, d) = (ac− bd, ad+ bc) ,\nrespectively. We can easily confirm that R2 is a commutative semiring by the isomorphism between this semiring and the ordinary semiring of complex numbers. Further let (cos, sin) : R → R2 be a function that maps each θ ∈ R to (cos(θ), sin(θ)), and let (G, op, (R,+, 0), φ,R2, (cos, sin)) specify the (cos, sin)-parametrized computation graph G = (G, op,R2, (cos, sin) ◦ φ). Note that (cos, sin) is a monoid homomorphism from (R,+, 0) to the multiplicative monoid of R2, that is, (cos, sin)(a+ b) = ((cos, sin)(a)) · ((cos, sin)(b)) for every a, b ∈ R, which follows from the addition law of the trigonometric functions cos and sin. Let src(G) = {s1, . . . , sn}. Now further assume that one defines χ : src(G)→ N0[x1, . . . , xn]\nmapping si to xi for every si ∈ src(G), and obtains the following form of the free forward variable of (G, op) w.r.t. χ\nα(G,op,χ)(t) = ∑\ni∈N0n ct,ix1\ni1 · · ·xnin\nfor every node and arc t ∈ V ∪ E. Then, αG(t) = ∑\ni∈N0n ct,i ((cos, sin)(i1φ(s1) + · · ·+ inφ(sn))) .\nDefinition 8 (Binomial Convolution Semiring) Let S = (S,+, ·, 0S , 1S) be a commutative semiring, and n a non-negative integer. For every (ai)i=0,...,n , (bi)i=0,...,n ∈ Sn+1, the binary operations + and on Sn+1 are defined by(\n(ai)i=0,...,n\n) + (\n(bi)i=0,...,n ) = (ai + bi)i=0,...,n (7)\nand, using binomial coefficients ( n k ) = n!k!(n−k)! ∈ N0,\n( (ai)i=0,...,n ) ( (bi)i=0,...,n ) =  ∑ j∈{k∈N0|k≤i} ( i j ) (aj · bi−j)  i=0,...,n , (8)\nrespectively. Further let 0BCnS = (0S , . . . , 0S) and 1BC n S = (1S , 0S , . . . , 0S). Then BC n S =( Sn+1,+, , 0BCnS , 1BCnS ) is called the n-th order binomial convolution semiring over S.\nLemma 2 Let S be a commutative semiring and n a non-negative integer. Then the n-th order binomial convolution semiring BCnS over S is a commutative semiring.\nNote that BC1R is isomorphic to the ordinary semiring of dual numbers (Yaglom, 1968). It is also known as the (first-order) expectation semiring (Eisner, 2001). BC1S , where S is a commutative semiring, plays a vital part in the formalization of forward-backward algorithms in Section 3.\nExample 7 (Parametrized Commutative Semiring Computation Graph for Sequence of Powers) Let S = (S,+, ·, 0S , 1S) be a commutative semiring, n a non-negative integer, PnS : S → Sn+1 a function that maps each a ∈ S to (ai)i=0,...,n, and the sextuple (G, op, (S,+, 0S) , φ,BC n S , PnS ) specify the PnS -parametrized computation graph G = (G, op,BCnS ,PnS ◦ φ). Note that PnS is a monoid homomorphism from (S,+, 0S) to the multiplicative monoid ( Sn+1, , 1BCnS ) of BCnS , that is, PnS (a+ b) = (PnS (a)) (PnS (b)) for every a, b ∈ S, which is nothing other than the binomial theorem.8 Let src(G) = {s1, . . . , sm}. Further assume that one defines χ : src(G) → N0[x1, . . . , xm] mapping si to xi for every si ∈ src(G), and obtains the following form of the free forward variable of (G, op) w.r.t. χ\nα(G,op,χ)(t) = ∑\ni∈N0m ct,ix1\ni1 · · ·xmim\n8. More precisely, this is the binomial theorem generalized to any commutative semiring. The proof of the generalization is omitted because it is trivial (cf. Hebisch and Weinert, 1998, Exercise I.2.12).\nfor every node and arc t ∈ V ∪ E. Then, αG(t) = ∑\ni∈N0m ct,iPnS (i1φ(s1) + · · ·+ imφ(sm))\n= ∑\ni∈N0m ct,i\n( (i1φ(s1) + · · ·+ imφ(sm))j ) j=0,...,n\n=  ∑ i∈N0m ct,i(i1φ(s1) + · · ·+ imφ(sm))j  j=0,...,n .\n(9)\nNote that the first equality in (9) uses Theorem 1, and the last equality uses the fact that the addition of BCnS is done component-wise.\nExample 8 (Parametrized Commutative Semiring Computation Graph for Sequence of Higher-order Derivatives) Let n be a non-negative integer, l a positive integer, FCn,l the set of all n-differentiable functions having l independent variables from an open subset D of Rl to R, the binary operation · on FCn,l defined pointwise (i.e., for every f, g ∈ FCn,l, f ·g is defined as a function that maps each x ∈ Rl to f(x)·g(x)), 1FCn,l ∈ FCn,l the constant function whose value is always 1, k ∈ {1, . . . , l}, x0 ∈ D, ∆nk,x0 : FCn,l → R n+1\na function that maps each f ∈ FCn,l to ( ∂i\n∂xki f(x) ∣∣∣ x=x0 ) i=0,...,n where ∂ 0 ∂xk0 f(x) = f(x)\nand x = (x1, . . . , xl), and the sextuple ( G, op, ( FCn,l, ·, 1FCn,l ) , φ,BCnR,∆ n k,x0 ) specify the\n∆nk,x0-parametrized computation graph G = ( G, op,BCnR,∆ n k,x0 ◦ φ )\n. Note that FCn,l =( FCn,l, ·, 1FCn,l ) is a commutative monoid, and ∆nk,x0 is a monoid homomorphism from FCn,l\nto the multiplicative monoid ( Rn+1, , 1BCnR ) of BCnR, that is, ∆ n k,x0 (f · g) = ( ∆nk,x0(f) ) (\n∆nk,x0(g) )\nfor every f, g ∈ FCn,l, which is nothing other than the general Leibniz rule followed by an evaluation at the point x = x0. Let src(G) = {s1, . . . , sm}. Now further assume that one defines χ : src(G)→ N0[y1, . . . , ym] mapping si to yi for every si ∈ src(G), and obtains the following form of the free forward variable of (G, op) w.r.t. χ\nα(G,op,χ)(t) = ∑\ni∈N0m ct,iy1\ni1 · · · ymim\nfor every node and arc t ∈ V ∪ E. Then, αG(t) = ∑\ni∈N0m ct,i∆\nn k,x0 ( (φ(x; s1)) i1 · · · (φ(x; sm))im )\n= ∑\ni∈N0m ct,i\n( ∂j\n∂xkj\n( (φ(x; s1)) i1 · · · (φ(x; sm))im )∣∣∣∣\nx=x0 ) j=0,...,n\n=  ∂j ∂xkj  ∑ i∈N0m ct,i(φ(x; s1)) i1 · · · (φ(x; sm))im ∣∣∣∣∣∣ x=x0  j=0,...,n .\n(10)\nNote that the first equality in (10) uses Theorem 1, and the last equality uses linearity of differentiation and the fact that the addition of BCnR is done component-wise.\nNote that the instance of forward algorithms on the parametrized computation graph( G, op,BC1R,∆ 1 k,x0 ◦ φ ) specified by the sextuple ( G, op,FC1,l, φ,BC1R,∆1k,x0 ) , which is obtained by setting n = 1 in Example 8, is equivalent to the forward mode of AD (Griewank and Walther, 2008) on the computation graph (G, op, (R,+, ·, 0, 1) , φ)."
    }, {
      "heading" : "2.3 Tensor Product of Semialgebras for Forward Algorithms",
      "text" : "In the previous subsection, we introduced the notion of a parametrized computation graph. The goal of this subsection is to provide a systematic way to “compose” a new parametrized computation graph from those that have the same computation structure (G, op). In the composed graph, the values of its forward variable are the “composition” of those of the original computation graphs.\nIn short, our contribution in this subsection is to reveal algebraic structures underlying complicated computation with forward algorithms and to construct a systematic framework to compose a complicated and difficult-to-design forward algorithm from primitive and easyto-design forward algorithms.\nTo illustrate our motivation, let us introduce computation with the second-order expectation semiring (Li and Eisner, 2009). Roughly speaking, for a computation graph with the following free forward variable\nα(G,op,χ)(t) = ∑\ni∈N0n ct,ix1\ni1 · · ·xnin ,\nthe second-order expectation semiring is used to compute values of the form∑ i∈N0n ct,i ( (µ(s1)) i1 · · · (µ(sn))in ) (i1φ(s1) + · · ·+ inφ(sn)) (i1ψ(s1) + · · ·+ inψ(sn)) . (11)\nThe second-order expectation semiring is a commutative semiring on quadruples of real numbers equipped with the following addition and multiplication,\n(p1, r1, s1, t1) + (p2, r2, s2, t2) = (p1 + p2, r1 + r2, s1 + s2, t1 + t2) (p1, r1, s1, t1) · (p2, r2, s2, t2) = (p1p2, p1r2 + p2r1, p1s2 + p2s1, p1t2 + p2t1 + r1s2 + r2s1) .\n(12)\nHowever, the derivation of this semiring in Li and Eisner (2009) is ad-hoc and limited to the above form.\nActually, the complex Eqs. (12) can be derived systematically. First observe that the summand in formula (11) consists of ct,i and three other factors. Now consider summations with simpler summands that consist of ct,i and only one of the other three factors,\ni.e., sums of the form ∑\ni∈N0n ct,i (µ(s1)) i1 · · · (µ(sn))in , ∑ i∈N0n ct,i (i1φ(s1) + · · ·+ inφ(sn)),\nand ∑\ni∈N0n ct,i (i1ψ(s1) + · · ·+ inψ(sn)). These simpler summations can be computed easily, as we have already specified the computation graphs for these summations in Example 4, and Example 7 with S = R and n = 1. The goal of this subsection is to give a way that allows systematic derivation of (12) from the computation graphs for these simpler summations, and to elucidate the underlying abstract structure. At the end of this subsection,\nwe replicate the derivation of (12) on a comprehensive mathematical foundation, and show how to construct a systematic way to compute (11).\nLet us state our goal more formally. When two sextuples (G, op,M, φ, S, f) and (G, op, M ′, ψ, S′, g) are already known to specify the f - and g-parametrized computation graphs (G, op, S, f ◦ φ) and (G, op, S′, g ◦ ψ), respectively, and the form of the free forward variable of (G, op) with respect to χ : src(G) → N0[x1, . . . , xn] mapping si to xi for every si ∈ src(G) = {s1, . . . , sn} is known to be equal to\nα(G,op,χ)(t) = ∑\ni∈N0n ct,ix1\ni1 · · ·xnin\nfor every node and arc t ∈ V ∪E, then the goal is to construct a systematic way to compute∑ i∈N0n ct,iB ( f ( (φ(s1)) i1 · · · (φ(sn))in ) , g ( (ψ(s1)) i1 · · · (ψ(sn))in )) , (13)\nwhere B is any bilinear mapping. Moreover, for m sextuples (G, op,Mi, φi, Si, fi) (i = 1, . . . ,m) specifying the fi-parametrized computation graph (G, op, Si, fi ◦ φi), respectively, we also construct a systematic way to compute∑\ni∈N0n ct,iM\n( f1 ( (φ1(s1)) i1 · · · (φ1(sn))in ) , . . . , fm ( (φm(s1)) i1 · · · (φm(sn))in )) , (14)\nwhere M is any m-linear mapping. The above-mentioned goal is feasible under some reasonable assumptions. However, there are things to rigorously formalize in order to achieve the goal. Therefore, we introduce the definitions of semimodule, basis, semialgebra, structure constants, bilinear mapping, tensor product, n-linear mapping, and so on. Semimodule just models “vector-like” objects. A semimodule generalizes the concept of ordinary vector space (over a field), wherein the corresponding scalars are elements of a semiring. Bases of a semimodule are a concept analogous to that of ordinary vector space. A semialgebra is a semimodule equipped with “multiplication between vectors.” Structure constants provide a primitive description of “multiplication between vectors” for a semialgebra. A bilinear mapping roughly performs a “multiplication-like” operation between elements of two semimodules and yields an element of another semimodule. Therefore, bilinear mappings include not only an ordinary multiplication as a closed binary operation but also various multiplication-like operations involving vectors such as scalar product, inner product, outer product, and so on. The notion of tensor product for semialgebras offers a systematic way to “compose” the multiplication structures of given semialgebras as well as their domains. Once the tensor product of semialgebras is constructed, we can easily compute any bilinear mapping on the semialgebras via a succinct linear mapping. n-linear mapping is the “n-ary extension” of bilinear mapping.\nHereinafter, there is an application limitation of formalization. We consider only a class of semirings called cancellative semirings. A ring is always a cancellative semiring. Thus, the ordinary semiring of real numbers and its variants, including Examples 5, 6, and 8, and Examples 4 and 7 with S = R are cancellative semirings. They are frequently used in a learning or optimization phase in machine learning tasks. In contrast, non-cancellative semirings, including Boolean semiring, max-plus (tropical) semiring and its variants, are\nnot covered in the subsequent part of this paper, although they are frequently used during a prediction phase.9\nDefinitions, theorems, etc. introduced in this subsection are also used to formalize forward-backward algorithms in an algebraic way in the next section.\nDefinition 9 (Semimodule,10 Hebisch and Weinert 1998; Golan 1999) Let M = (M,+, 0M ) be a commutative monoid, S = (S,+, ·, 0S , 1S) a commutative semiring, and let there exist a mapping from S×M to M , denoted by the juxtaposition of an element of S and an element of M , and called scalar multiplication. Then M is called a semimodule over S or an S-semimodule if and only if the following conditions are satisfied for every σ, τ ∈ S and a, b ∈M :\n• σ(a+ b) = σa+ σb,\n• (σ + τ)a = σa+ τa,\n• (σ · τ)a = σ(τa),\n• 1Sa = a, and\n• 0Sa = 0M .\nDefinition 10 (S-homomorphism, Hebisch and Weinert 1998; Golan 1999) Let S be a commutative semiring, and let (M,+, 0M ) and (N,+, 0N ) be S-semimodules. Then a mapping f : M → N is called an S-homomorphism or a linear mapping if and only if the following conditions are satisfied for every σ ∈ S and m,m′ ∈M :\n• f(m+m′) = f(m) + f(m′), and\n• f(σm) = σf(m).\nDefinition 11 (Basis of Semimodule, Hebisch and Weinert 1998; Golan 1999) Let (M,+, 0M ) be a semimodule over a commutative semiring (S,+, ·, 0S , 1S), and U 6= ∅ a subset of M . Then a ∈ M is called a linear combination of elements u ∈ U (over S) if and only if a = ∑ u∈U σuu holds for σu ∈ S but only finitely many of the coefficients σu are different from 0S . If every element in M can be obtained in this way, U is said to generate (M,+, 0M ) by linear combinations. Further, U is called linearly independent (over S) if ∑ u∈U σuu = ∑ u∈U τuu for only finitely many non-zero coefficients σu, τu ∈ S implies σu = τu for every u ∈ U . Finally, U is called a basis of (M,+, 0M ) if and only if U is linearly independent and generates (M,+, 0M ) by linear combinations.\n9. See Golan (1999, Chapters 15 and 16) for the case of non-cancellative semirings. In particular, for non-cancellative semirings that have an element x such that a + x = x holds for every element a (e.g., Boolean semiring and max-plus semiring), the tensor product of semimodules, which is a core notion in the subsequent part, degenerates to a trivial structure, and formalization becomes also trivial and meaningless. 10. In this paper, we only define semimodules over a commutative semiring. Therefore, it is irrelevant to distinguish between a left S-semimodule M and the right one if we set mσ = σm for every σ ∈ S and m ∈M . Note that an S-semimodule is an (S, S)-bisemimodule (Golan, 1999) in this setting.\nDefinition 12 (Extension by Linearity) Let M = (M,+, 0M ) and N = (N,+, 0N ) be semimodules over a commutative semiring S, and U ⊆M a basis of M . Further, for every m ∈ M , let m = ∑ u∈U σm,uu (σm,u ∈ S) be its unique expression as a linear combination of elements of U . Then, a mapping f : U → N is said to be extended by linearity to a mapping g : M → N if and only if g is defined by g(m) = ∑ u∈U σm,uf(u) for every m ∈M . g is well-defined since the linear combination is unique. g is also called the extension by linearity of f . Clearly, g is an S-homomorphism.\nDefinition 13 (Bilinear (S-balanced) Mapping, Golan 1999) Let S be a commutative semiring, and let M , N , and P be S-semimodules. Then a mapping B : M × N → P is bilinear or S-balanced if and only if, for every m,m′ ∈M , n, n′ ∈ N , and σ ∈ S, we have:\n• B (m+m′, n) = B (m,n) +B (m′, n) ,\n• B (m,n+ n′) = B (m,n) +B (m,n′) , and\n• B (σm, n) = B (m,σn) = σB (m,n) .\nDefinition 14 (Cancellativeness, Hebisch and Weinert 1998; Golan 1999) A commutative semiring S = (S,+, ·, 0S , 1S) is called cancellative if and only if, for every a ∈ S, a+ b = a+ c for some b, c ∈ S implies b = c. A semimodule M = (M,+, 0M ) is cancellative if and only if, for every m ∈M , m+m′ = m+m′′ for some m′,m′′ ∈M implies m′ = m′′.\nDefinition 15 (Tensor Product of Semimodules, Takahashi 1982; Golan 199911) Let S be a commutative semiring, let M and N be S-semimodules, and P a cancellative Ssemimodule. The tensor product of M and N over S, denoted by M ⊗S N , is an Ssemimodule equipped with a bilinear mapping ⊗ : M × N → M ⊗S N such that for any bilinear mapping B : M × N → P there is a unique S-homomorphism L : M ⊗S N → P making L(m⊗ n) = B(m,n) for every m ∈M and n ∈ N .\nFigure 7 illustrates what Definition 15 says. Roughly speaking, for any bilinear mapping B : M×N → P , B(m,n) can be calculated by way of M⊗SN . This is not just a roundabout way of calculating B(m,n), but of great help in analyzing a complicated B because a linearalgebraic method can be used on M ⊗S N .\nLemma 3 (Existence of Tensor Product of Semimodules, Takahashi 1982; Golan 1999) Let S be a commutative semiring, and let M = (M,+, 0M ) and N = (N,+, 0N ) be S-semimodules. Then the tensor product M ⊗S N exists, 0M ⊗ 0N is the zero element of M ⊗S N , and {m⊗ n}m∈M,n∈N generates M ⊗S N by linear combinations.\nProof See Takahashi (1982, Sections 3 and 4) or Golan (1999, Chapter16).\nLemma 4 (Basis of Tensor Product of Semimodules) Let S be a cancellative commutative semiring, and let M and N be S-semimodules. Further let U and V be bases of M and N , respectively. Then {u⊗ v}u∈U, v∈V is a basis of the tensor product M ⊗S N . 11. This definition is a specific case of the general definition of the tensor product of semimodules and corre-\nsponds to Takahashi (1982, Corollary 4.4) and Golan (1999, Proposition 16.15). The general definition (Takahashi, 1982; Golan, 1999) does not assume cancellativeness of P .\nDefinition 16 (Semialgebra, Hebisch and Weinert 1998) Let (S,+, ·, 0S , 1S) be a commutative semiring. Then (A,+, ·, 0A) is called a semialgebra over S or an S-semialgebra if and only if the following conditions are satisfied:\n• (A,+, 0A) is an S-semimodule,\n• · is a binary operation on A, called multiplication, and (A,+, ·, 0A) satisfies the distributive law, i.e., a · (b+ c) = (a · b) + (a · c) and (a+ b) · c = (a · c) + (b · c) for every a, b, c ∈ A, and\n• there exists a basis U of (A,+, 0A) such that (σu) · (τv) = (σ · τ)(u · v) holds for every σ, τ ∈ S and u, v ∈ U . Such a basis U is called a semialgebra basis of (A,+, ·, 0A).\nAn S-semialgebra (A,+, ·, 0A) is called unital if and only if there exists the identity element of the multiplication. It is called commutative or associative if and only if the multiplication is commutative or associative, respectively. Hereinafter, a unital semialgebra (A,+, ·, 0A) where 1A ∈ A is the identity element of the multiplication is denoted by (A,+, ·, 0A, 1A).\nNote that a commutative semiring S = (S,+, ·, 0S , 1S) can be always considered as a commutative unital associative S-semialgebra with a semialgebra basis {1S}. At the same time, a commutative unital associative semialgebra itself can be always considered as a commutative semiring.\nIn this paper, we are only interested in semialgebras that are themselves commutative semirings, that is, commutative unital associative semialgebras.\nDefinition 17 (Structure Constants, Hebisch and Weinert 1998) Let (A,+, ·, 0A) be a semialgebra over a commutative semiring S = (S,+, ·, 0S , 1S), and U a semialgebra basis of (A,+, ·, 0A). Then, for every u, v ∈ U , we obtain the unique linear combination of elements of U for the product of u and v\nu · v = ∑ w∈U σwu,vw ,\nwhere σwu,v ∈ S but only finitely many of the coefficients σwu,v are different from 0S . σwu,v are called the structure constants of the S-semialgebra (A,+, ·, 0A) with respect to the semialgebra basis U .\nThe structure constants of a semialgebra serve as an alternative definition of the multiplication of the semialgebra. This means that, if the definition of the multiplication is given then we easily obtain the structure constants of the semialgebra, and conversely, if the structure constants are given then we can determine the multiplication between every two elements of the semialgebra by using the linear combinations of the basis for the operands.\nDefinition 18 (Tensor Product of Semialgebras) Let S be a cancellative commutative semiring, A = (A,+, ·, 0A, 1A) and A′ = (A′,+, ·, 0A′ , 1A′) commutative unital associative S-semialgebras, and U and V semialgebra bases of A and A′, respectively. Further let σu ′′ u,u′ (u, u\n′, u′′ ∈ U) (resp. τv′′v,v′ (v, v′, v′′ ∈ V )) be the structure constants of A (resp. A′) with respect to U (resp. V ). Let t and t′ be two elements of A ⊗S A′, and let t =∑\n(u,v)∈U×V ρu,v (u⊗ v), and t′ = ∑ (u,v)∈U×V ρ ′ u,v (u⊗ v) be their unique expressions as linear combinations of the basis {u⊗ v}u∈U,v∈V . We define the binary operation t · t′ of the two elements by\nt · t′ = ∑\n(u,v)∈U×V ∑ (u′,v′)∈U×V ∑ (u′′,v′′)∈U×V ( ρu,v · ρ′u′,v′ · σu ′′ u,u′ · τv ′′ v,v′ ) ( u′′ ⊗ v′′ ) . (15)\nThe tensor product of S-semimodules A ⊗S A′ equipped with this operation is called the tensor product of semialgebras A and A′ over S.\nLemma 5 Let S,A,A′, U, V, σu ′′ u,u′ , τ v′′ v,v′ , and the operation · be defined as in Definition 18. Then, the following statements hold:\n• A ⊗S A′ = (A⊗S A′,+, ·, 0A ⊗ 0A′ , 1A ⊗ 1A′) is a commutative unital associative Ssemialgebra,\n• W = {u⊗ v}u∈U,v∈V is a semialgebra basis of A⊗S A′,\n• the structure constants of A ⊗S A′ with respect to the basis W are ωu ′′⊗v′′ u⊗v,u′⊗v′ =\nσu ′′ u,u′ · τv ′′ v,v′ (u⊗ v, u′ ⊗ v′, u′′ ⊗ v′′ ∈W ), and\n• for every a, b ∈ A and a′, b′ ∈ A′,( a⊗ a′ ) · ( b⊗ b′ ) = (a · b)⊗ ( a′ · b′ ) . (16)\nThe following theorem shows how to compute formula (13).\nTheorem 2 Let (S,+, ·, 0S , 1S) be a cancellative commutative semiring, A = (A,+, ·, 0A, 1A) and A\n′ = (A′,+, ·, 0A′ , 1A′) commutative unital associative S-semialgebras, U and U ′ semialgebra bases of A and A′, respectively, and the sextuples (G, op, (M, ·, 1M ), φ,A, f) and (G, op, (N, ·, 1N ), ψ,A′, g) specify the f - and g-parametrized computation graphs (G, op, A, f ◦ φ) and (G, op, A′, g ◦ ψ), respectively. Note that M × N = (M × N, ·, (1M , 1N )) is a commutative monoid equipped with component-wise multiplication (m,n) · (m′, n′) = (m ·m′, n · n′) for every m,m′ ∈M and n, n′ ∈ N . Further let φ× ψ : src(G)→M ×N be a function that maps each v ∈ src(G) to (φ(v), ψ(v)), f ⊗ g : M ×N → A⊗S A′ a function that maps each (m,n) ∈ M × N to f(m) ⊗ g(n), and src(G) = {s1, . . . , sn}. Now further\nassume that one defines χ : src(G)→ N0[x1, . . . , xn] mapping si to xi for every si ∈ src(G), and obtains the following form of the free forward variable of (G, op) w.r.t. χ\nα(G,op,χ)(t) = ∑\ni∈N0n ct,ix1\ni1 · · ·xnin\nfor every node and arc t ∈ V ∪ E. Then, αG(t) = ∑\ni∈N0n ct,i\n(( f ( (φ(s1)) i1 · · · (φ(sn))in )) ⊗ ( g ( (ψ(s1)) i1 · · · (ψ(sn))in ))) ,\nwhere G = (G, op, A⊗SA′, (f⊗g)◦(φ×ψ)) is the computation graph specified by the sextuple (G, op,M ×N,φ× ψ,A⊗S A′, f ⊗ g). Moreover, for any bilinear mapping B from A× A′ to a cancellative S-semimodule P , we can construct an S-homomorphism L : A⊗S A′ → P such that\nL(αG(t)) = ∑\ni∈N0n ct,iB\n( f ( (φ(s1)) i1 · · · (φ(sn))in ) , g ( (ψ(s1)) i1 · · · (ψ(sn))in )) .\nIn fact, the extension by linearity of the mapping L′(u⊗u′) = B(u, u′) for every u ∈ U and u′ ∈ U ′ has this very effect.\nLet A,A′, A′′ be S-semialgebras, and U,U ′, U ′′ semialgebra bases of A,A′, A′′, respectively. Then, there exists the trivial isomorphism between (A ⊗S A′) ⊗S A′′ and A ⊗S (A′ ⊗S A′′), that is, the mapping u ⊗ (u′ ⊗ u′′) 7→ (u⊗ u′) ⊗ u′′ (u ∈ U, u′ ∈ U ′, u′′ ∈ U ′′) is extended by linearity to the isomorphism. Therefore, tensor product is essentially associative, and we can omit parentheses indicating the association order of multiple tensor products.\nBy noting this fact, and using Theorem 2 and Lemmas 4 and 5, we obtain the following statement how to compute formula (14).\nCorollary 1 Let S be a cancellative commutative semiring, n a non-negative integer, Ai = (Ai,+, ·, 0Ai , 1Ai) (i = 1, . . . , n) commutative unital associative S-semialgebras, Ui (i = 1, . . . , n) a semialgebra basis of Ai, the sextuples (G, op,Mi, φi, Ai, fi) (i = 1, . . . , n) specify the fi-parametrized computation graph (G, op, Ai, fi ◦ φi), respectively, φ1 × · · · × φn : src(G)→M1×· · ·×Mn a function that maps each v ∈ src(G) to (φi(v))i=1,...,n, and f1⊗ · · ·⊗fn a function that maps each (mi)i=1,...,n ∈M1×· · ·×Mn to (f1(m1))⊗· · ·⊗(fn(mn)). Then the sextuple (G, op,M1×· · ·×Mn, φ1×· · ·×φn, A1⊗S · · ·⊗SAn, f1⊗· · ·⊗fn) specifies the parametrized computation graph G = (G, op, A1⊗S · · ·⊗SAn, (f1⊗· · ·⊗fn)◦(φ1×· · ·×φn)). Let src(G) = {s1, . . . , s`}. Now further assume that one defines χ : src(G)→ N0[x1, . . . , x`] mapping si to xi for every si ∈ src(G), and obtains the following form of the free forward variable of (G, op) w.r.t. χ\nα(G,op,χ)(t) = ∑ i∈N0` ct,ix1 i1 · · ·x`i`\nfor every node and arc t ∈ V ∪ E. Then,\nαG(t) =∑ i∈N0` ct,i (( f1 ( (φ1(s1)) i1 · · · (φ1(s`))i` )) ⊗ · · · ⊗ ( fn ( (φn(s1)) i1 · · · (φn(s`))i` ))) .\nMoreover, for any n-linear mappingM from M1×· · ·×Mn to a cancellative S-semimodule P (i.e., a mapping that is linear if all but one of its arguments are fixed, that is, M(. . . ,mi + mi ′, . . . ) = M(. . . ,mi, . . . ) +M(. . . ,mi′, . . . ) and M(. . . , σmi, . . . ) = σM(. . . ,mi, . . . ) hold for every mi,mi ′ ∈ Mi and σ ∈ S), we can construct an S-homomorphism L : A1 ⊗S · · · ⊗S An → P such that\nL(αG(t)) =∑ i∈N0` ct,iM (( f1 ( (φ1(s1)) i1 · · · (φ1(s`))i` )) , · · · , ( fn ( (φn(s1)) i1 · · · (φn(s`))i` ))) .\nIn fact, the extension by linearity of the mapping L′(u1 ⊗ · · · ⊗ un) = M(u1, . . . , un) for every ui ∈ Ui has this very effect.\nProof By induction with Theorem 2, and Lemmas 4 and 5, we can easily prove the statement.\nHere, we present a systematic framework to design complicated forward algorithms.\nFramework 1 (Framework to Design Complicated Forward Algorithms)\n1. Identify the underlying abstract computation structure (G, op).\n2. If computation problem on (G, op) at hand is of the form∑ i∈N0n ct,iL1 ( f1 ( (φ1(s1)) i1 · · · (φ1(sn))in )) · · ·\nLm ( fm ( (φm(s1)) i1 · · · (φm(sn))in )) , (17)\nwhere t ∈ V ∪ E, i = (i1, . . . , in), S is a cancellative commutative semiring, P is a unital commutative associative S-semialgebra, and, for every j ∈ {1, . . . ,m}, Aj is a unital commutative associative S-semialgebra, (G, op,Mj , φj , Aj , fj) specifies the fj-parametrized computation graph (G, op, Aj , fj ◦ φj), and Lj : Aj → P is an Shomomorphism,\n3. then define the m-linear mapping M(a1, . . . , am) = L1(a1) · · ·Lm(am) for every aj ∈ Aj , and construct the extension by linearity L\n′∗ : A1⊗S · · ·⊗SAm → P of the mapping L′(u1 ⊗ · · · ⊗ um) =M(u1, . . . , um) for every uj ∈ Uj , where Uj is a semialgebra basis of Aj .\n4. Thus, L′∗(αG(t)) is equal to (17) by Corollary 1, where G is the parametrized computation graph G = (G, op, A1 × · · · ×Am, (f1 ⊗ · · · ⊗ fm) ◦ (φ1 × · · ·φm)) specified by the sextuple (G, op,M1 × · · · ×Mm, φ1 × · · · × φm, A1 ⊗S · · · ⊗S Am, f1 ⊗ · · · ⊗ fm).\nIn this framework, it is important to have a large “portfolio” of parametrized computation graphs, including Examples 4 through 8, because it is a key to identify a computation problem at hand as one that takes on the form (17).\nThe rest of this subsection demonstrates how to apply this framework to concrete contexts.\nExample 9 (Marginalization and Expectation Problems) Consider an abstract computation structure (G, op), and the following computation problem∑\ni∈N0n ct,i\n( (φ(s1)) i1 · · · (φ(sn))in ) (i1ψ(s1) + · · ·+ inψ(sn)) , (18)\nwhere src(G) = {s1, . . . , sn}, i = (i1, . . . , in), t ∈ V ∪E, ct,i ∈ N0 for every t and i, φ and ψ are a function that maps each element of src(G) to a real number. Both marginalization and expectation problems take on the form (18). They are important computation problems in many inference procedures.\nFor example, see Example 1. Let φ in (18) be equal to ξ defined in Example 1. Then, as explained in Example 1, the factor involving φ in the summand in (18) is equal to the joint probability of a sequence in HMMs. Therefore, for example, if ψ is defined as the indicator function that returns 1 on s0 of the computation graph in Fig. 4 and 0 on all the other source nodes, then (18) is equal to the marginal probability with respect to the residence in the state 0 at time t = 0. Likewise, marginalization with respect to any state or transition takes on the form (18).\nExpectation problem also takes on the form (18). The factor involving ψ in the summand in (18) is equal to the “count” of a feature defined by ψ in a sequence. Therefore, (18) can be interpreted as the feature expectation (first-order moment) defined by a feature ψ with respect to the probability weight defined by the factor involving φ.\nHere, note that formula (18) is of a form that Framework 1 applies to. For (17), by setting m = 2, f1 = idR, L1 = idR, f2 = P1R, and L2 : R2 → R, where P1R is that defined in Example 7, and L2 is a function that maps each (a0, a1) to a1, (17) becomes equal to (18).\nThus, we systematically construct the instance of forward algorithms that is used to compute (18). From Framework 1, L′∗(αG(t)) is equal to formula (18), where G =( G, op,R⊗R BC1R, ( idR⊗P1R ) ◦ (φ× ψ) ) is the computation graph specified by the sextuple(\nG, op, (R, ·, 1)× (R,+, 0), φ× ψ,R⊗R BC1R, idR⊗P1R ) , ê0 = (1, 0) ∈ R2, ê1 = (0, 1) ∈ R2, and L′∗ : R ⊗R BC1R → R is the R-homomorphism that is obtained by extension by linearity of the mapping L′ : {1⊗ êi}i=0,1 → R such that L′(1⊗ ê0) = 0 and L′(1⊗ ê1) = 1. Note that the resulting algebraic structure is isomorphic to the (first-order) expectation semiring (Eisner, 2001; Li and Eisner, 2009).\nThe above-mentioned arguments can be applied to data structures other than trellises. The same applies to acyclic factor graphs (e.g., by repeating the above arguments for Example 2), hypergraphs, a variety of decision diagrams (e.g., by repeating the above arguments for Example 3), and so on.\nExample 9 may be a little surprising. It says that both marginalization and expectation problems can be worked out only by forward passes on computation graphs. It does not appear to agree with the fact that these computation problems are usually solved by the combination of forward and backward passes (e.g., the ordinary forward-backward algorithm on trellises, the inside-outside algorithm on CYK derivations, the sum-product algorithm on acyclic factor graphs, the EM algorithm on decision diagrams (Ishihata et al., 2008), and so on). However, “forward-only” algorithms for these computations has been already proposed for certain kinds of data structures, e.g., the forward-only algorithm for HMMs (Tan, 1993; Sivaprakasam and Shanmugan, 1995; Turin, 1998; Miklós and Meyer, 2005; Churbanov and\nWinters-Hilt, 2008) and the forward algorithm with the (first-order) expectation semiring on hypergraphs (Li and Eisner, 2009). Example 9 generalizes these algorithms to any computation consisting of a finite number of applications of additions and/or multiplications on various kinds of data structures.\nWith a slight modification to Example 9, we also obtain the forward-only algorithm to compute a feature expectation for CRFs on trellises and factor graphs and the log-linear model on various data structures. That is, it is obtained by replacing the factor involving φ in (18) with exp(i1φ(s1) + · · ·+ inφ(sn)) and considering the parametrized computation graph specified by the sextuple (G, op, (R,+, 0)× (R,+, 0), φ× ψ,R⊗R BC1R, exp⊗P1R).\nNow, let us return to the example of the second-order expectation semiring introduced in the beginning of this subsection. We replicate the algebraic structure and computation result of the second-order semiring, but simply as an instance of Framework 1.\nExample 10 (Second-order Expectation Semiring) First observe that formula (11) is an instance of formula (17). In fact, for (17), by setting m = 3, φ1 = µ, f1 = idR, L1 = idR, φ2 = φ, f2 = P1R, L2(r0, r1) = r1 for every r0, r1 ∈ R, φ3 = ψ, f3 = P1R, L3 = L2, (17) becomes equal to (11).\nThus, we can systematically construct the instance of forward algorithms that computes (11). From Framework 1, L′∗(αG(t)) is equal to (11), where G = (G, op,R ⊗R BC1R ⊗R BC1R, (idR⊗P1R⊗P1R) ◦ (µ×φ×ψ)) is the parametrized computation graph specified by the sextuple ( G, op, (R, ·, 1)× (R,+, 0)× (R,+, 0), µ×φ×ψ,R⊗R BC1R⊗R BC1R, idR⊗P1R⊗P1R ) , ê0 = (1, 0) ∈ R2, ê1 = (0, 1) ∈ R2, and L′∗ : R⊗RBC1R⊗RBC1R → R is the R-homomorphism that is obtained by extension by linearity of the mapping L′ : {1⊗ êi ⊗ êj}i,j=0,1 → R such that L′(1⊗ êi ⊗ êj) = 1 if i = j = 1 and L′(1⊗ êi ⊗ êj) = 0 otherwise.\nNext, we replicate the derivation of the algebraic structure of the second-order semiring, that is, Eqs. (12). The ordinary semiring of R can be viewed as a commutative unital associative R-semialgebra with a semialgebra basis {1}. The structure constants of this semialgebra w.r.t. this basis is ρ11,1 = 1. BC 1 R can be viewed as a commutative unital associative R-semialgebra with a semialgebra basis {ê0, ê1}. The structure constants of this semialgebra w.r.t. this basis is σê0ê0,ê0 = 1, σ ê1 ê0,ê0 = 0, σê0ê0,ê1 = 0, σ ê1 ê0,ê1 = 1, σê0ê1,ê0 = 0, σ ê1 ê1,ê0 = 1, σê0ê1,ê1 = 0, and σ ê1 ê1,ê1\n= 0. Consider the tensor product of semialgebras R⊗R BC1R ⊗R BC1R. By noting that R is a cancellative commutative semiring, W = {1⊗ êi ⊗ êj}i∈{0,1}, j∈{0,1} is a semialgebra basis of R ⊗R BC1R ⊗R BC1R, so every element of R ⊗R BC1R ⊗R BC1R can be written in the form ∑ i∈{0,1}, j∈{0,1} ηi,j(1⊗ êi ⊗ êj) for some ηi,j ∈ R. Therefore, the addition of R⊗R BC1R ⊗R BC1R is defined by\n(η0,0(1⊗ ê0 ⊗ ê0) + η0,1(1⊗ ê0 ⊗ ê1) + η1,0(1⊗ ê1 ⊗ ê0) + η1,1(1⊗ ê1 ⊗ ê1)) + ( η′0,0(1⊗ ê0 ⊗ ê0) + η′0,1(1⊗ ê0 ⊗ ê1) + η′1,0(1⊗ ê1 ⊗ ê0) + η′1,1(1⊗ ê1 ⊗ ê1) ) = ( η0,0 + η ′ 0,0 ) (1⊗ ê0 ⊗ ê0) + ( η0,1 + η ′ 0,1 ) (1⊗ ê0 ⊗ ê1)\n+ ( η1,0 + η ′ 1,0 ) (1⊗ ê1 ⊗ ê0) + ( η1,1 + η ′ 1,1 ) (1⊗ ê1 ⊗ ê1) .\nThe structure constants of R⊗RBC1R⊗RBC1R w.r.t. W are τ 1⊗êi3⊗êj3 1⊗êi1⊗êj1 ,1⊗êi2⊗êj2 = ρ11,1 ·σ êi3 êi1 ,êi2 · σ êj3 êj1 ,êj2 (i1, i2, i3, j1, j2, j3 ∈ {0, 1}). Therefore, the multiplication of R ⊗R BC1R ⊗R BC1R is\ndefined by\n(η0,0(1⊗ ê0 ⊗ ê0) + η0,1(1⊗ ê0 ⊗ ê1) + η1,0(1⊗ ê1 ⊗ ê0) + η1,1(1⊗ ê1 ⊗ ê1)) · ( η′0,0(1⊗ ê0 ⊗ ê0) + η′0,1(1⊗ ê0 ⊗ ê1) + η′1,0(1⊗ ê1 ⊗ ê0) + η′1,1(1⊗ ê1 ⊗ ê1) ) = ( η0,0η ′ 0,0 ) (1⊗ ê0 ⊗ ê0)\n+ ( η0,0η ′ 0,1 + η0,1η ′ 0,0 ) (1⊗ ê0 ⊗ ê1) + ( η0,0η ′ 1,0 + η1,0η ′ 0,0 ) (1⊗ ê1 ⊗ ê0)\n+ ( η0,0η ′ 1,1 + η1,1η ′ 0,0 + η0,1η ′ 1,0 + η1,0η ′ 0,1 ) (1⊗ ê1 ⊗ ê1) .\nIt is obvious that the algebraic structure with the above addition and multiplication is isomorphic to the one with (12)."
    }, {
      "heading" : "3. Forward-backward Algorithms",
      "text" : ""
    }, {
      "heading" : "3.1 Backward Invariants and Forward-backward Algorithms",
      "text" : "The goal of this section is to answer the question “what can be calculated by forwardbackward algorithms.” More precisely, computation by forward-backward algorithms is characterized in an algebraic way.\nFirst of all, we articulate the answer to the above question proposed in this paper. Forward-backward algorithms compute ∑ v∈snk(G) αG(v) of the parametrized computation graph G = (G, op, A⊗S BC1S , (f ⊗ g) ◦ (φ×ψ)) that is specified by the sextuple (G, op,M × N,φ×ψ,A⊗SBC1S , f⊗g), where S is a cancellative commutative semiring, A is a commutative unital associative S-semialgebra, and (G, op,M, φ,A, f) and ( G, op, N, ψ,BC1S , g ) spec-\nify the f - and g-parametrized computation graphs (G, op, A, f ◦ φ) and ( G, op,BC1S , g ◦ ψ ) , respectively. Note that BC1S is the first-order binomial convolution semiring over S (see Definition 8). In other words, computation by some instances of forward algorithms can be replaced by that of forward-backward algorithms.\nDespite its name “forward-backward algorithms,” the proposed characterization subsumes a quite wide range of existing algorithms, including the ordinary forward-backward algorithm on trellises for sequence labeling, the inside-outside algorithm on derivation forests for CYK parsing, the sum-product algorithm on acyclic factor graphs, the EM algorithm on a variety of decision diagrams, the reverse mode of AD, and so on. In addition, not only the standard version of these algorithms but also their variants are formalized in a unified way.\nForward-backward algorithms formalized in this section are characterized by a combination of forward and backward passes on the computation graph specified by the sextuple( G, op,M ×N,φ× ψ,A⊗S BC1S , f ⊗ g ) instead of a forward-only pass on the computation graph. Since the “dimension” of BC1S is equal to 2, every element of A ⊗S BC1S can be considered as a 2-dimensional vector-like object by ignoring the S-semialgebra structure of A. Hereinafter, each of the components is called the zeroth component and first component, respectively. Roughly speaking, the first step of forward-backward algorithms consists of a forward pass on the computation graph to compute the zeroth component, and the second one a backward pass to compute the first component.\nThe characterization of forward-backward algorithms described above leads to an immediate but very important consequence. That is, computation by forward-backward algo-\nrithms can be also done by forward algorithms. This consequence has been partly referred to in Example 9 and the following paragraph already.\nMoreover, we can turn things around. Computation of some instances of forward algorithms can be also done by forward-backward algorithms. One of the most important implications of this fact is the derivation of the reverse mode of AD (a.k.a. back propagation) from the forward mode. Because the forward mode of AD is an instance of forward algorithms (cf. Example 8) and can be replaced with the corresponding forward-backward algorithm, the reverse mode of AD can be derived by the formalization in this subsection.\nNote that BC1S is a central player in the subsequent part. It cannot be overemphasized that this algebra is important in the formalization of forward-backward algorithms. For example, the following characteristics of forward-backward algorithms are entirely ascribable to the multiplication structures of BC1S : forward-backward algorithms can be split into two stages of a forward pass and backward one, and a characteristic “sum-product” computation pattern appears in a backward pass.\nTo formalize forward-backward algorithms, additional definitions, lemmas, and a theorem are introduced below. Before going into the details of the formalization, we give a rough sketch of the flow of the discussion below. First, we introduce two mappings named the zeroth projection P0 and first projection P1 (Definition 19) to pick up each (the zeroth or first) component of elements of A⊗S BC1S , and prove some properties of the projections (Lemmas 6 and 7) used in the subsequent part. Then, we show that the zeroth component of values of the forward variable of computation graphs can be computed independently from their first component (Lemma 8). This computation of the zeroth component constitutes the “forward part” of forward-backward algorithms. After that, we introduce definitions and statements to “reverse” computation of the first component of values of the forward variable. First, we transform calculations of the first component on multiplication nodes into “backwardable” ones (Lemma 9). Next, we introduce the backward variable βG (Definition 20), which is defined by backward recursion and designed such that the sum of the values of a function of βG over src(G) is equal to the first component of the sum of the values of αG over snk(G). The remaining part is devoted to prove the equation\nP1 (∑ v∈snk(G) αG(v) ) = ∑ v∈src(G) (a function of βG(v)). We introduce terms and concepts of order theory, and consider computation graphs as partially ordered sets by using Definition 21. Antichain cutsets, which are defined in Definition 22, play a key role in the proof of interest. Antichain cutsets are “cut sets that are crossed against the direction of a computation graph.” We can construct a series of antichain cutsets along the direction of a computation graph. We prove the equation of interest by induction on the series of antichain cutsets. Lemma 10 constitutes induction steps of the proof, and Theorem 3 is the final result. Thereupon, we show that the first component of ∑ v∈snk(G) αG(v) can be computed by βG in place of αG . Computation of values of βG by backward recursion constitutes the “backward part” of forward-backward algorithms.\nDefinition 19 (Zeroth and First Projections of A⊗S BC1S) Let S = (S,+, ·, 0S , 1S) be a cancellative commutative semiring, A = (A,+, ·, 0A, 1A) a commutative unital associative S-semialgebra, U a semialgebra basis of A, and ê0 = (1S , 0S), ê1 = (0S , 1S). Then {ê0, ê1} is clearly a semialgebra basis of BC1S . If every element of A ⊗S BC1S is written as a linear combination of elements of the basis {u ⊗ êi}u∈U, i∈{0,1}, say, ∑ u∈U ∑ i∈{0,1} σu,i(u ⊗ êi) =\n(∑ u∈U σu,0u ) ⊗ ê0 + (∑ u∈U σu,1u ) ⊗ ê1, σu,i ∈ S are uniquely determined, and thus so are∑\nu∈U σu,0u and ∑ u∈U σu,1u. Therefore, for every element ∑ u∈U ∑\ni∈{0,1} σu,i(u ⊗ êi) of A⊗S BC1S , the two mappings from A⊗S BC1S to A, the zeroth projection P0\nP0 ∑ u∈U ∑ i∈{0,1} σu,i(u⊗ êi)  = ∑ u∈U σu,0u\nand the first projection P1\nP1 ∑ u∈U ∑ i∈{0,1} σu,i(u⊗ êi)  = ∑ u∈U σu,1u\ncan be defined.\nLemma 6 Let S, A, U , ê0, and ê1 be defined as in Definition 19. Then x = P0(x) ⊗ ê0 + P1(x)⊗ ê1 holds for every x ∈ A⊗S BC1S .\nLemma 7 Let S, A, and U be defined as in Definition 19. Then P0(x+ y) = P0(x) +P0(y) and P1(x+ y) = P1(x) + P1(y) hold for every two elements x, y of A⊗S BC1S .\nLemma 8 Let S = (S,+, ·, 0S , 1S) be a cancellative commutative semiring, A = (A,+, ·, 0A, 1A) a commutative unital associative S-semialgebra, and G = (G, op, A⊗S BC1S , ξ) a computation graph. Further let G′ be the computation graph G′ = (G, op, A,P0 ◦ ξ). Then, for every node and arc t ∈ V ∪ E of G, we have\nP0(αG(t)) = αG′(t) . (19)\nLemma 9 Let S, A, and G = ( G, op, A⊗S BC1S , ξ ) be defined as in Lemma 8. Then, for every node v ∈ V , we have\nP1  ∏ e∈E−G (v) αG(e)  = ∑ e∈E−G (v) P1(αG(e)) ·  ∏ e′∈E−G (v)\\{e} P0 ( αG ( e′ )) .\nDefinition 20 (Backward Variable) Let S, A, and G = ( G, op, A⊗S BC1S , ξ ) be defined as in Lemma 8. Then the backward variable of G, denoted by βG , is a mapping from V ∪E to A that is defined by, for every node v ∈ V and arc e ∈ E,\nβG(v) =\n{ 1A if v ∈ snk(G),∑\ne′∈E+G(v) βG(e\n′) otherwise,\nβG(e) =  βG(head(e)) if op(head(e)) = “+”, βG(head(e)) ·  ∏ e′∈E−G(head(e))\\{e} P0 ( αG ( e′ ))\notherwise (i.e., op(head(e)) = “·”).\n(20)\nNote that the mutually recursive definition of βG on V and E in Definition 20 is welldefined since G is a finite dag.\nHere, let us note that a characteristic “sum-product” computation pattern appears in the computation of values of a backward variable βG . By expanding the recursive definition of βG(v) in Eqs. (20) in just one step, we obtain the following form\nβG(v) = ∑\ne∈E+G(v)\nβG(e) ·  ∏ e′∈E−G(head(e))\\{e} P0 ( αG ( e′ )) .\nThis computation pattern can be commonly found in the ordinary forward-backward algorithm on trellises, the inside-outside algorithm on derivations by CYK parsing, the sumproduct algorithm on acyclic factor graphs (as the name suggests), the EM algorithm on a variety of decision diagrams, and even the reverse mode of AD. On trellises for sequence labeling, this sum-product computation pattern degenerates into a “forward-backward” computation pattern since, in the computation graph corresponding to a trellis (e.g., Fig. 4), multiplication nodes always have exactly two parent nodes and one of the parent nodes is always a source node.\nHereinafter, formalization is given in terms of order theory rather than graph theory, as the former provides more useful terms and concepts for our purpose.\nIn a partially ordered set (poset hereinafter) O = (O,≤), x ∈ O is said to be covered by y ∈ O (or y covers x) if and only if x < y and there does not exist any z ∈ O such that x < z and z < y. Let x ≺ y denote that x is covered by y. For x ∈ O, the covering set of x is defined by {y ∈ O | x ≺ y}. Likewise, the covered set of x is defined by {y ∈ O | y ≺ x}. For every two elements x, y ∈ O, x and y are called comparable if and only if either x ≤ y or y ≤ x holds, and incomparable if and only if they are not comparable, i.e., neither x ≤ y nor y ≤ x holds. Let X be a subset of O. X is called a chain if and only if every two elements of X are comparable (in other words, X is a totally ordered subset of O). X is called an antichain if and only if every two distinct elements of X are incomparable. ↑X is defined by ↑X = {y ∈ O | ∃x ∈ X s.t. x ≤ y}.12 ↓X is defined likewise, i.e., ↓X = {y ∈ O | ∃x ∈ X s.t. y ≤ x}.\nDefinition 21 (Poset Induced by DAG) Let G = (V,E) be a dag. Then a poset O = (V ∪ E,≤) is called the poset induced by G if and only if, x ≤ y holds for every x, y ∈ V ∪E if and only if there exists a directed path (v0, e1, v1, . . . , el, vl) in G and either of the following conditions holds:\n• there exist i, j ∈ N0 such that i ≤ j, x = vi, and y = vj ,\n• there exist i, j ∈ N such that i ≤ j, x = ei, and y = ej ,\n• there exist i ∈ N0 and j ∈ N such that i < j, x = vi, and y = ej , or\n• there exist i ∈ N and j ∈ N0 such that i ≤ j, x = ei, and y = vj .\n12. A subset U ⊆ O of a poset (O,≤) is called an upper set if and only if u ≤ x implies x ∈ U for every u ∈ U and x ∈ O, and the notation ↑X where X ⊆ O usually denotes the smallest upper set of O containing X in order theory. However, this definition is equivalent to our one (the proof is omitted). We use the latter as it is easier to understand. The same applies to ↓X by replacing upper set with lower set.\nIn other words, x ≤ y holds if and only if y is “reachable” from x but “reachability” includes one not only among nodes but also among nodes and arcs.\nDefinition 22 (Antichain Cutset, Rival and Zaguia 1985) Let O = (O,≤) be a poset, and C a subset of O. Then C is called an antichain cutset of O if and only if C is an antichain of O and C intersects every maximal chain of O.\nLet O be a poset and AC(O) the set of all antichain cutsets of O. Then (AC(O) ,≤) constitutes a poset (actually the lattice of antichain cutsets, Higgs, 1986) if we set\nC ≤ C ′ if and only if, for every x ∈ C, there exists x′ ∈ C ′ such that x ≤ x′\nfor every two antichain cutsets C,C ′ ∈ AC(O). As the following lemma shows, if a poset is induced by a dag, one can obtain an antichain cutset “adjacent” to a given antichain cutset.13 Moreover, an invariant on forward and backward variables holds for both the adjacent antichain cutsets.\nLemma 10 Let S, A, and G = ( G, op, A⊗S BC1S , ξ ) be defined as in Lemma 8. Further let O = (V ∪ E,≤) be the poset induced by G, AC(O) the set of all antichain cutsets of O, (AC(O) ,≤) the lattice of antichain cutsets of O, and C ⊆ V ∪ E an antichain cutset of O satisfying C 6= snk(G). Then one can obtain x ∈ C such that there exists y ∈ V ∪ E satisfying x ≺ y and y is a minimal element of ↑C \\ C. Further let D′ be the covering set of x. Then, for every d′ ∈ D′, the covered set of d′ is a subset of C. Further let D = {z ∈ V ∪ E | ∃w ∈ D′ s.t. z ≺ w}. Then C ′ = (C ∪D′) \\ D is an antichain cutset of O satisfying C < C ′. C ′ is called the covering antichain cutset of C with respect to x. Moreover, we have ∑\nc∈C P1(αG(c)) · βG(c) = ∑ c∈C′ P1(αG(c)) · βG(c) .\nHere, we present the main theorem of this subsection. Intuitively, the theorem says that computation of the first component of ∑ v∈snk(G) αG(v) of G = ( G, op, A⊗S BC1S , ξ ) can be made in terms of the backward variable. This result guarantees the correctness of computation of the first component of the values of the forward variable by a backward pass in forward-backward algorithms.\nTheorem 3 (Backward Invariants) Let S, A, G = ( G, op, A⊗S BC1S , ξ ) , and O be defined as in Lemma 10. Then, for every two antichain cutsets C,C ′ ∈ AC(O),∑ c∈C P1(αG(c)) · βG(c) = ∑ c∈C′ P1(αG(c)) · βG(c) .\n13. We can construct a poset such that there is an arbitrarily complicated substructure but no antichain cutset between two antichain cutsets. See Fig. 4 (and Fig. 5) in Rival and Zaguia (1985) for example. However, in the poset induced by a dag, we can find another antichain cutset right close to every given antichain cutset. This is because the poset induced by a dag falls into a tractable subset of posets, as pointed out in the footnote 4.\nEspecially,\nP1  ∑ v∈snk(G) αG(v)  = ∑ v∈src(G) P1(ξ(v)) · βG(v) .\nNow, the pseudo-codes of forward and forward-backward algorithms are shown in Algorithms 1 and 2, respectively. Forward algorithms just compute values of the forward variable of a computation graph. Although this variable has been constructively defined in Definition 5 already, the pseudo-code of forward algorithms is also presented here in order to discuss two important problems, “reduction of space complexity” and “scheduling” in forward algorithms.\nAntichain cutsets of the poset induced by a computation graph are a key to reduce space complexity in forward and forward-backward algorithms because an antichain cutset is a minimal subset of V ∪E where values of the forward (resp. backward) variable should be stored at each point in time during execution of a forward (resp. backward) pass. The reason is explained as follows. If values of the forward (resp. backward) variable are stored only in a subset of V ∪ E that some maximal chain of the poset induced by G does not intersect, values of the forward (resp. backward) variable cannot be computed on some nodes or arcs. If values of the forward (resp. backward) variable are stored in a subset of V ∪ E that is not an antichain of the poset induced by G, some of them are not necessary or can be obtained later during further computation.\nTherefore, Algorithm 1 is described on an antichain cutset to antichain cutset basis. In the while loop from Line 6 through Line 22, values of the forward variable are computed with respect to each antichain cutset. Lemma 10 guarantees that each update of C in Line 21 preserves the loop invariant that C is an antichain cutset. If one is interested in the values of the forward variable only on snk(G) and does not execute forward-backward algorithms that require intermediate values of the forward variable, the values of the forward variable on the subset D of the old antichain cutset in Line 20 are no longer necessary and can be discarded. By doing so, one can minimalize space complexity of forward algorithms for such a case.14\nMoreover, Algorithm 1 is “properly scheduled.” That is, for every d′ ∈ D′ where D′ is constructed in Line 8, the value of the forward variable on every element covered by d′ is guaranteed to be computed before the value of the forward variable on d′ is computed. This fact is an immediate consequence of Lemma 10.\nOne may imagine computation of values of the forward variable that is not antichaincutset-to-antichain-cutset-basis. That is, one could always skip storing the values of the forward variable on every arc and store them as the possibly partial sum or product in every internal node. However, such procedure can be also modeled on an antichain cutset\n14. “Minimalization” of space complexity described here should be distinguished from the problem of “minimization” of space complexity. The problem of “minimization” of space complexity of forward algorithms for such a case described here could be formalized as the minimization problem of the max number of values of the forward variable that are simultaneously stored. In short, this problem is formalized as a (one-shot) pebble game. It is easy to show that this minimization problem is NP-hard for the class of arbitrary computation graphs (cf. Wu et al., 2014) but the details are omitted because it is out of the scope of this paper.\nAlgorithm 1 Forward Algorithm Require: G = (V,E) is a finite dag. op is a mapping from src(G) to {“+”, “·”}. S is a commutative semiring. ξ is a mapping from src(G) to S. (V ∪ E,<) is the poset induced by G. Ensure: G = (G, op, S, ξ) then α(t) = αG(t). 1: procedure Forward(G, op, S, ξ) 2: for all v ∈ src(G) do 3: α(v)← ξ(v) 4: end for 5: C ← src(G) 6: while C 6= snk(G) do 7: Find x ∈ C s.t. ∃y ∈ ↑C \\ C. x ≺ y and y is a minimal element of ↑C \\ C. 8: D′ ← {d′ ∈ V ∪ E | x ≺ d′} 9: for all t ∈ D′ do 10: if t ∈ V then 11: if op(t) = “+” then 12: α(t)← ∑ e∈E−G(t) α(e) 13: else 14: α(t)← ∏ e∈E−G(t) α(e) 15: end if 16: else 17: α(t)← α(tail(t)) 18: end if 19: end for 20: D ← {d ∈ V ∪ E | ∃d′ ∈ D′. d ≺ d′} 21: C ← (C ∪D′) \\D 22: end while 23: end procedure\nto antichain cutset basis by imposing the restriction that each internal node of computation graphs is binary.\nAlgorithm 1 assumes the existence of an oracle in Line 7 in order for the algorithms to be antichain-cutset-to-antichain-cutset-basis. The oracle is required to return, for a given antichain cutset C, x ∈ C such that there exists y ∈ ↑C \\ C and x ≺ y. However, this assumption is not strong because, if an order among nodes and/or arcs required for computation of values of the forward variable to be properly scheduled is already known (e.g., a topological order among nodes), then it can be also used to construct the oracle without any remarkably additional computation cost.\nAlgorithm 2 is the pseudo-code of forward-backward algorithms. Note that the forward variable α(t) in Algorithm 2 is that of the computation graph (G, op, A,P0 ◦ ξ), not( G, op, A⊗S BC1S , ξ ) . Computation of the backward variable from Line 11 through 19 is a straightforward implementation of Definition 20 except that computation in Line 17 uses Lemma 8. Algorithm 2 is also described on an antichain cutset to antichain cutset basis. Therefore the discussion about the problems of space complexity and scheduling for Algo-\nAlgorithm 2 Forward-Backward Algorithm Require: G = (V,E) is a finite dag. op is a mapping from src(G) to {“+”, “·”}. S is a cancellative commutative semiring. A = (A,+, ·, 0A, 1A) is a commutative unital associative S-semialgebra. ξ is a mapping from src(G) to A⊗S BC1S . (V ∪ E,<) is the poset induced by G.\nEnsure: ∑ v∈snk(G) αG(v) = ∑ v∈snk(G) α(v) ⊗ ê0 + ∑\nv∈src(G) (P1(ξ(v)) · β(v)) ⊗ ê1 where G = ( G, op, A⊗S BC1S , ξ ) .\n1: procedure Forward-Backward(G, op, A, ξ) 2: Forward(G, op, A,P0 ◦ ξ) to compute α(t) for every t ∈ V ∪ E. 3: for all t ∈ snk(G) do 4: β(v)← 1A 5: end for 6: C ← snk(G) 7: while C 6= src(G) do 8: Find x ∈ C s.t. ∃y ∈ ↓C \\ C. y ≺ x and y is a maximal element of ↓C \\ C. 9: D′ ← {d′ ∈ V ∪ E | d′ ≺ x}\n10: for all t ∈ D′ do 11: if t ∈ V then 12: β(t)← ∑ e∈E+G(v) β(e) 13: else 14: if op(head(t)) = “+” then 15: β(t)← β(head(t)) 16: else 17: β(t)← β(head(t)) · (∏ e′∈E−G(head(t))\\{t} α(e′) ) 18: end if 19: end if 20: end for 21: D ← {d ∈ V ∪ E | ∃d′ ∈ D. d′ ≺ d} 22: C ← (C ∪D′) \\D 23: end while 24: end procedure\nrithm 1 also applies to Algorithm 2 in a straightforward way. Although we omit the proof that the update of antichain cutsets in Line 22 is correct, it can be trivially shown by the order dual of Lemma 10. Lemma 6 and Theorem 3 guarantee that the postcondition holds.\nIt is obvious that we can compute ∑\nv∈snk(G) αG(v) where G = (G, op, A⊗S BC 1 S , ξ) by\ncalling either Forward(G, op, A⊗SBC1S , ξ) or Forward-Backward(G, op, A, ξ). In other words, we can compute ∑ v∈snk(G) αG(v) by Forward in place of Forward-Backward. Therefore, when we already know details of an instance of forward-backward algorithms to compute a formula at hand, by identifying corresponding G, op, A, and ξ, we can systematically transform the instance of forward-backward algorithms to the corresponding instance of forward-only algorithms. For example, by setting G, op, A, and ξ to be equal to the computation structure underlying the Baum-Welch algorithm for HMMs (cf. Examples 1 and\n9), we can immediately derive the forward-only algorithm for the Baum-Welch algorithm on HMMs."
    }, {
      "heading" : "3.2 Conditions Favoring Forward-backward Algorithms",
      "text" : "In the previous subsection, it was concluded that what is computed by forward-backward algorithms can be also always computed by forward algorithms. This conclusion naturally raises the following question: ”Why or when forward-backward algorithms are necessary?” This question is answered in this subsection.\nActually, there is a set of conditions to make forward-backward algorithms much more favorable than forward algorithms. In addition, the conditions are met in many usual machine learning tasks. We present them below.\nCondition 1 (Conditions Favoring Forward-backward Algorithms) Let n be a nonnegative integer, S a cancellative semiring, (G, op,M, φ,A, f) specify the f -parametrized computation graph, and ( G, op, Ni, ψi,BC 1 S , gi ) (i = 1, . . . , n) specify the gi-parametrized computation graph, respectively. Then the set of the following conditions is called the conditions favoring forward-backward algorithms:\n• ∑\nv∈snk(G) αGi(v) is required to be computed for i = 1, . . . , n, where Gi is the (f ⊗ gi)parametrized computation graph specified by the sextuple (G, op,M×Ni, φ×ψi, A⊗S BC1S , f ⊗ gi), and\n• the values of P0(((f ⊗ gi) ◦ (φ× ψi))(v)) are independent of i for every v ∈ src(G).\nThe values of α computed in Algorithms 1 depend only on G, op, S, and P0(ξ(v)) where v ∈ src(G). Therefore, when Condition 1 is met, we need to execute Algorithm 1 only once no matter how many computation graphs are required to be computed. Likewise, because the values of β computed in Algorithm 2 depend only on G, op, A, and α computed by Algorithm 1, when Condition 1 is met, we need to compute β in Algorithm 2 only once no matter how many computation graphs are required to be computed. Moreover, the values of ∑\nv∈snk(G) αG(v) depend only on the values of α(v) on every v ∈ snk(G), the values of β(v) on every v ∈ src(G), and the values of P1(ξ(v)) on every v ∈ src(G). Therefore, in order to compute the values of ∑ v∈snk(G) αGi(v) of n computation graphs Gi (i = 1, . . . , n) satisfying Condition 1, we need exactly one forward pass and exactly one backward pass on the computation graph, where computation cost is irrelevant of n, and evaluation of P1(((f ⊗ gi) ◦ (φ× ψi))(v)) for i = 1, . . . , n and only every source node v ∈ src(G).\nAs a result, in the case where n computation graphs (n > 1) satisfying Condition 1 are required to be computed, forward-backward algorithms are much more efficient than forward algorithms. When such computation is done only by forward algorithms, we need to execute a forward pass for each computation graph and totally n forward passes, so computation cost is proportional to n(|src(G)|+ |E|). On the other hand, computation cost by forwardbackward algorithms is proportional to 2(|src(G)|+ |E|) + n|src(G)|. Because |src(G)| is usually much smaller than |E|, we can conclude that forward-backward algorithms are much more favorable than forward algorithms in such case.\nNext, we show that Condition 1 is met in many usual machine learning tasks by some examples.\nExample 11 (Baum-Welch Algorithm) Recall Example 9. In order to perform the training of the parameters of HMMs by the Baum-Welch algorithm, we need to calculate (18) multiple times with different definitions of ψ. They take on the following form∑\ni∈N0n ct,i ( (φ(s1)) i1 · · · (φ(sn))in ) (i1ψj(s1) + · · ·+ inψj(sn)) , (21)\nwhere j ∈ {1, . . . ,m}. ψj is defined by either of following: an indicator function that is equal to 1 if and only if the argument source node corresponds to the residence at each state (in this case, (21) becomes equal to the expectation of the residence of the state), an indicator function that is equal to 1 if and only if the argument source node corresponds to each kind of transitions (in this case, (21) becomes equal to the expectation of the kind of the transitions), or an indicator function that is equal to 1 if and only if the argument source node corresponds to each kind of observation emissions from a state to an observation (in this case, (21) becomes equal to the expectation of the kind of the observation emissions). As explained in Example 9, the values of the form (21) is obtained by the parametrized computation graph Gj = (G, op,R⊗R BC1R, (idR⊗P1R) ◦ (φ× ψj)) specified by the sextuple (G, op, (R, ·, 1)× (R,+, 0), φ×ψj ,R⊗R BC1R, idR⊗P1R) and the application of the extension by linearity of the mapping L : R ⊗R R2 → R satisfying L(1 ⊗ ê0) = 0 and L(1 ⊗ ê1) = 1. Because P0(((idR⊗P1R) ◦ (φ × ψj))(v)) = φ(v) for every j ∈ {1, . . . ,m} and v ∈ src(G), the set of Gj satisfies Condition 1. Therefore, forward-backward algorithms are much more favorable than forward algorithms in this case.\nWith a slight modification to Example 11, we also obtain the instances of forwardbackward algorithms to compute feature expectations for CRFs on trellises and factor graphs and the log-linear model on various data structures. See the paragraph following Example 9.\nNote that forward-backward algorithms are favored for the cases cited in Example 11 only from the point of view of time complexity. If space rather than time complexity is the main constraint, forward-only algorithms are acceptable even when Condition 1 is met because space complexity of forward-backward algorithms is proportional to the size of G while that of forward-only algorithms is proportional to the maximum size of antichain cutsets during execution of Forward (see the discussion of space complexity of Algorithm 1 in the previous subsection). In the context of the forward-only computation of the BaumWelch algorithm, the analysis described above is completely consistent with the investigation in Khreich et al. (2010).\nExample 12 (Reverse Mode of AD (Back Propagation)) Let m be a positive integer, Fm the set of all differentiable functions havingm independent variables from an open subset of D of Rm to R, 1Fm ∈ Fm the constant function whose value is always 1, Fm = (Fm, ·, 1Fm) the commutative monoid equipped with the pointwise multiplication · on Fm, ψ a mapping from src(G) to Fm, G = (G, op,Fm, ψ) a computation graph, and src(G) = {s1, . . . , sn}. Let us define χ : src(G)→ N0 [x0, . . . , xn] mapping si to xi for every si ∈ src(G), and obtain the following form of the free forward variable of (G, op) w.r.t. χ\nα(G,op,χ)(t) = ∑\ni∈N0n ct,ix1\ni1 · · ·xnin\nfor every node and arc t ∈ V ∪ E. Then\nαG(t) = ∑\ni∈N0n ct,i (ψ(x; s1))\ni1 · · · (ψ(x; sn))in .\nFurther let us calculate the value of the gradient of αG evaluated at a point x = x0, which is of the following form\n∂\n∂x  ∑ i∈N0n ct,i (ψ(x; s1)) i1 · · · (ψ(x; sn))in ∣∣∣∣∣∣ x=x0\n=  ∂ ∂xk  ∑ i∈N0n ct,i (ψ(x; s1)) i1 · · · (ψ(x; sn))in ∣∣∣∣∣∣ x=x0  k=1,...,m ,\n(22)\nwhere x = (x1, . . . , xm). The value of the form (22) is obtained by the computation graphs G′k = ( G, op,R⊗R BC1R, ( idR⊗∆1k,x0 ) ◦ (φ× ψ) ) (k = 1, . . . ,m) that are specified by the\nsextuple ( G, op, (R, ·, 1)×Fm, φ× ψ,R⊗R BC1R, idR⊗∆1k,x0 ) , respectively, followed by the R-homomorphism L : R ⊗R BC1R → R satisfying L(p⊗ (q, r)) = pr, where φ : src(G) → R is defined by φ(v) = 1 for every v ∈ src(G). Because P0 ((( idR⊗∆1k,x0 ) ◦ (φ× ψ) ) (v)) ) = ψ(x0; v) is independent of k, the set of the computation graphs {G1, . . . ,Gm} meets Condition 1. Therefore, forward-backward algorithms are much more favorable than forward algorithms in this case.\nNote that the forward algorithm for the computation graphs G′k (k = 1, . . . ,m) in Example 12 is equivalent to the forward mode of AD for the computation graph G, and the corresponding forward-backward algorithm is equivalent to the reverse mode of AD. The analysis of trade-off between time and space complexity about forward and forwardbackward algorithms described in this paper is completely consistent with the one between the forward and reverse modes of AD for the case of Example 12 (cf. Griewank and Walther, 2008, Sections I.3 and I.4)."
    }, {
      "heading" : "3.3 Checkpoints for Forward-backward Algorithms",
      "text" : "This subsection provides a brief note on technique to adjust trade-off between time and space complexity of forward-backward algorithms. The technique is based on so-called checkpoints. In executing an instance of forward-backward algorithms, values of the forward variable of a computation graph are not necessarily stored in every element of V ∪E during the forward pass. Instead, they are stored in the elements of some subsets of V ∪E, which are called checkpoints, and then necessary values of the forward variable in the backward pass are recomputed from stored values close to them.\nOne of obviously good candidates for checkpoints is V . Forward variables are defined on V ∪E for technical reasons, but the values of a forward variable on E are not necessarily stored in the forward pass of forward-backward algorithms. Instead, the value of a forward variable on every arc can be recomputed from the one stored in the tail of the arc. By doing\nso, space complexity required for forward-backward algorithms can be made proportional to |V | while time complexity is kept proportional to |V |+ |E|.\nAnother good candidate for checkpoints is antichain cutsets of the poset induced by a computation graph because if values of the forward variable of the computation graph are stored in every element of an antichain cutset C then the values of the forward variable on every element of ↑C \\ C can be recomputed from the values stored in C. By adjusting granularity of antichain cutsets where values of the forward variable are stored, we can adjust trade-off between time and space complexity in forward-backward algorithms to a certain degree. In particular, by repeating the construction of a covering antichain cutset described in Lemma 10 from src(G), we obtain a maximal chain of the lattice of antichain cutsets from src(G) through snk(G). By considering the succession of these antichain cutsets as an analogue of the sequence of the time slices in trellises for sequence labeling, discussion on variants of technique based on checkpoints on sequence labeling can be also applied to any computation graph."
    }, {
      "heading" : "4. Conclusion",
      "text" : "In this paper, we propose an algebraic formalization of forward and forward-backward algorithms. The formalization consists of (1) a unified abstraction of any computation consisting of a finite number of additions and/or multiplications, which is enough to support the development of the unified formalization of the algorithms, (2) the elucidation of algebraic structures underlying complicated forward algorithms, (3) a systematic framework to construct complicated and difficult-to-design forward algorithms from simple and easyto-design forward algorithms, and (4) an algebraic characterization of forward-backward algorithms.\nAlthough we present only two pseudo-codes (i.e., Algorithms 1 and 2) in this paper, they subsume a wide range of existing algorithms due to their versatility. To our knowledge, the formalization described in this paper subsumes (a part of) the following papers: Forney (1973, Section III), Rabiner (1989, Sections III.A and III.B), Lari and Young (1990, Section 2), Tan (1993, Chapter 3), Turin (1998, Section IV.B), Goodman (1999, Sections 2 through 4) (provided that all derivations are acyclic, and the derivation forest is finite), Aji and McEliece (2000, Section III), Kschischang et al. (2001, Section 4), Klein and Manning (2004, Section 4), Miklós and Meyer (2005, Section titled “Methods and results”), Mann and McCallum (2007), Churbanov and Winters-Hilt (2008, Subsection titled “Linear memory Baum-Welch using a backward sweep with scaling”), Ishihata et al. (2008), Griewank and Walther (2008, Section I.3) (provided that involved operations are limited to the addition or multiplication of real numbers), Huang (2008, Section 5.1), Li and Eisner (2009, Sections 3 through 5), Azuma and Matsumoto (2010, Section 2), Kimmig et al. (2011, Section 4), Tsuboi et al. (2011, Section titled “Hessian-vector Products of CRFs”), Ilić et al. (2012).\nThe formalization presented in this paper not only subsumes a wide range of existing algorithms but also extends them to an infinite number of their variants by using Framework 1 described in Section 2.3. The extension can be done in a systematic way. One only needs to identify the underlying abstract computation structure (G, op), and the computation problem at hand as an instance of formula (17).\nThe formalization presented in this paper also allows another direction of extension. Some of the algorithms proposed in the above-mentioned papers are of the forward-backward type. However, we build a linking bridge between forward and forward-backward algorithms. Consequently, we also obtain the forward-only version of many existing algorithms of the forward-backward type. This increases choices of algorithms according to trade-off between time and space requirements.\nIn addition, the unified formalization accelerates the “synergy” among discussions and techniques in a variety of existing algorithms. Based on the unified formalization, it is easy to transfer a discussion or technique in the context of a specific algorithm to the context of other algorithms.\nFor example, consider the discussion in Eisner (2016). He has pointed out that the inside-outside algorithm can be derived by back propagation. The derivation is automatic as the reverse mode of AD can be automatically derived from computation on the underlying computation graph. However, his discussion is limited to the context of parsing. Moreover, the probability distribution is assumed to be log-linear because his discussion is based on the fact that the partition function of a log-linear distribution is also its moment generating function.\nIn contrast, our formalization appears to show the possibility that the discussion in Eisner (2016) can be generalized to much wider contexts. That is, Algorithm 2 can be automatically derived from Algorithm 1. This idea naturally arises from the fact that the relationship between the forward and reverse modes of AD is an instance of the relationship between Algorithms 1 and 2. For such transformation, we can use a long history and accumulation of discussions and experiences in the context of AD (e.g., see Griewank and Walther, 2008, Chapter 6)."
    }, {
      "heading" : "Appendix A. Proof of Lemma 2",
      "text" : "(This proof uses some notions defined after Lemma 2 because using these notions significantly simplifies the proof. Of course, these forward references never lead to circular reasoning.) Proof Let S = (S,+, ·, 0S , 1S). It is obvious that ( Sn+1,+, 0BCnS ) is a commutative monoid from the definition of the addition (7), and is also an S-semimodule by being\nequipped with scalar multiplication σ (\n(si)i=0,...,n ) = (σ · si)i=0,...,n for every σ ∈ S and\n(si)i=0,...,n ∈ Sn+1. Let êi ∈ Sn+1 (i = 0, . . . , n) be defined by ê0 = (1S , 0S , 0S , . . . ) , ê1 = (0S , 1S , 0S , . . . ), and so on. Clearly, U = {ê0, . . . , ên} is a basis of the S-semimodule( Sn+1,+, 0BCnS ) . For every a = (ai)i=0,...,n , b = (bi)i=0,...,n , c = (ci)i=0,...,n ∈ Sn+1, one obtains\na (b+ c) = (\n(ai)i=0,...,n\n) (\n(bi)i=0,...,n + (ci)i=0,...,n ) = (\n(ai)i=0,...,n\n) (\n(bi + ci)i=0,...,n ) =\n ∑ j∈{k∈N0|k≤i} ( i j ) (aj · (bi−j + ci−j))  i=0,...,n\n=  ∑ j∈{k∈N0|k≤i} ( i j ) (aj · bi−j) +  ∑ j∈{k∈N0|k≤i} ( i j ) (aj · ci−j)  i=0...,n\n=  ∑ j∈{k∈N0|k≤i} ( i j ) (aj · bi−j)  i=0,...,n +  ∑ j∈{k∈N0|k≤i} ( i j ) (aj · ci−j)  i=0,...,n\n= (\n(ai)i=0,...,n\n) (\n(bi)i=0,...,n\n) + (\n(ai)i=0,...,n\n) (\n(ci)i=0,...,n ) = (a b) + (a c) .\nLikewise, one also easily obtains (a+ b) c = (a c) + (b c), so distributes over +. Moreover, from the definition of the multiplication (8),\n(σêi) (τ êj) = (σ · τ)(êi êj) =\n{( i+j i ) ((σ · τ)êi+j) if i+ j ≤ n,\n0BCnS otherwise, for every σ, τ ∈ S and êi, êj ∈ U . Therefore ( Sn+1,+, , 0BCnS ) is an S-semialgebra with a semialgebra basis U . Because\nêi êj = êj êi =\n{( i+j i ) êi+j = ( i+j j ) êi+j if i+ j ≤ n,\n0BCnS otherwise,\nfor every êi, êj ∈ U , the S-semialgebra (Sn+1,+, , 0BCnS ) is commutative by Hebisch and Weinert (1998, Theorem V.2.4). The structure constants of the S-semialgebra ( Sn+1,+, ,\n0BCnS ) w.r.t. U is\nσêkêi,êj =\n{( i+j i ) 1S if i+ j = k,\n0S otherwise,\nfor every êi, êj , êk ∈ U . Now one obtains\n∑ êl∈U σêlêi,êj · σ êm êl,êk =\n{ σ êi+j êi,êj · σêmêm−k,êk = (( i+j i ) 1S ) · (( m m−k ) 1S ) if i+ j + k = m,\n0S otherwise,\n∑ êl∈U σêlêj ,êk · σ êm êi,êl =\n{ σ êj+k êj ,êk · σêmêi,êm−i = (( j+k j ) 1S ) · (( m m−i ) 1S ) if i+ j + k = m,\n0S otherwise, and if i+ j + k = m, using the identity of binomial coefficients ( n h )( n−h k ) = ( n k )( n−k h ) ,((\ni+ j\ni\n) 1S ) · (( m\nm− k\n) 1S ) = (( i+ j\ni\n)( m\nm− k\n)) 1S\n=\n(( i+ j\ni\n)( i+ j + k\ni+ j\n)) 1S = (( i+ j + k − k\ni\n)( i+ j + k\nk\n)) 1S\n=\n(( i+ j + k − i\nk\n)( i+ j + k\ni\n)) 1S = (( j + k\nk\n)( i+ j + k\nj + k\n)) 1S\n=\n(( j + k\nj\n)( m\nm− i\n)) 1S = (( j + k\nj\n) 1S ) · (( m\nm− i\n) 1S ) .\nTherefore, ∑\nêl∈U σ êl êi,êj · σêmêl,êk = ∑ êl∈U σ êl êj ,êk\n· σêmêi,êl for every êi, êj , êk, êm ∈ U , and thus the S-semialgebra ( Sn+1,+, , 0BCnS ) is associative by Hebisch and Weinert (1998, Theorem\nV.2.4). Finally, 1BCnS êi = êi 1BCnS = êi for every êi ∈ U , and the S-semialgebra( Sn+1,+, , 0BCnS ) is unital by Hebisch and Weinert (1998, Exercise V.2.2). Thus ( Sn+1,+,\n, 0BCnS , 1BCnS ) is a commutative unital associative S-semialgebra, and BCnS = ( Sn+1,+, ,\n0BCnS , 1BC n S\n) is itself a commutative semiring."
    }, {
      "heading" : "Appendix B. Proof of Lemma 4",
      "text" : "Proof Let S = (S,+, ·, 0S , 1S). From Lemma 3, every element t ∈M ⊗SN can be written as a finite sum t = ∑ i ρi(mi ⊗ ni) for some mi ∈ M , ni ∈ N , and ρi ∈ S. Moreover, mi and ni can be written as unique linear combinations of the bases U and V , respectively, so we have mi = ∑ u∈U σi,uu and ni = ∑ v∈V τi,vv for some σi,u, τi,v ∈ S. Thus, by using\nbilinearity of the tensor product, we obtain t = ∑ i ρi ((∑ u∈U σi,uu ) ⊗ (∑ v∈V τi,vv ))\n=∑ (u,v)∈U×V ( ∑ i (ρi · σi,u · τi,v)) (u⊗ v). Therefore, {u⊗ v}u∈U, v∈V generates M ⊗S N by linear combinations. Next, every element m ∈ M and n ∈ N can be written in a unique manner as the linear combination of the elements of the bases U and V , respectively, so we have m =∑ u∈U σuu and n = ∑ v∈V τvv for some σu, τv ∈ S. Let u′ and v′ be elements of U and V , respectively. Now consider the function Bu′,v′ : M×N → S defined by setting Bu′,v′ (m,n) = Bu′,v′ (∑ u∈U σuu, ∑ v∈V τvv ) = σu′ ·τv′ . Clearly Bu′,v′ is bilinear. Thus, by Definition 15 and Lemma 3, there exists an S-homomorphism Lu′,v′ : M⊗SN → S satisfying Lu′,v′ (m⊗ n) = Bu′,v′(m,n) = (σu′ · τv′). In particular, for every u ∈ U and v ∈ V ,\nLu′,v′ (u⊗ v) =\n{ 1S u = u ′ and v = v′,\n0S otherwise.\nAssume that ∑ (u,v)∈U×V σu,v (u⊗ v) = ∑ (u,v)∈U×V τu,v (u⊗ v) (23)\nfor σu,v, τu,v ∈ S but only finitely many of the coefficients σu,v and τu,v are different from 0S . Applying Lu′,v′ to both sides of Eq. (23) tells us σu′,v′ = τu′,v′ . Since u\n′ and v′ are arbitrarily chosen from U and V , respectively, we obtain σu,v = τu,v for every u ∈ U and v ∈ V . It follows that {u⊗ v}u∈U, v∈V is linearly independent. Therefore {u⊗ v}u∈U, v∈V is a basis of M ⊗S N ."
    }, {
      "heading" : "Appendix C. Proof of Lemma 5",
      "text" : "Proof From Definition 15 and Lemma 3, A ⊗S A′ = (A⊗S A′,+, 0A ⊗ 0A′) is an Ssemimodule. Moreover, by Lemma 4, W = {u⊗ v}u∈U, v∈V is a basis of A⊗S A′.\nFor every three elements of the tensor product of S-semimodules A⊗SA′ written as linear combinations of the basis W , say, t = ∑ (u,v)∈U×V ρu,v (u⊗ v) , t′ = ∑ (u,v)∈U×V ρ ′ u,v (u⊗ v) ,\nand t′′ = ∑\n(u,v)∈U×V ρ ′′ u,v (u⊗ v) , where ρu,v, ρ′u,v, ρ′′u,v ∈ S, using the definition of the\noperation (15), we have t · ( t′ + t′′ )\n=  ∑ (u,v)∈U×V ρu,v (u⊗ v)  ·  ∑ (u,v)∈U×V ( ρ′u,v + ρ ′′ u,v ) (u⊗ v)  =\n∑ (u,v)∈U×V ∑ (u′,v′)∈U×V ∑ (u′′,v′′)∈U×V ( ρu,v · ( ρ′u′,v′ + ρ ′′ u′,v′ ) · σu′′u,u′ · τv ′′ v,v′ ) ( u′′ ⊗ v′′ ) =\n∑ (u,v)∈U×V ∑ (u′,v′)∈U×V ∑ (u′′,v′′)∈U×V ( ρu,v · ρ′u′,v′ · σu ′′ u,u′ · τv ′′ v,v′ ) ( u′′ ⊗ v′′ ) +\n∑ (u,v)∈U×V ∑ (u′,v′)∈U×V ∑ (u′′,v′′)∈U×V ( ρu,v · ρ′′u′,v′ · σu ′′ u,u′ · τv ′′ v,v′ ) ( u′′ ⊗ v′′ )\n=  ∑ (u,v)∈U×V ρu,v (u⊗ v)  ·  ∑ (u,v)∈U×V ρ′u,v (u⊗ v)  +\n ∑ (u,v)∈U×V ρu,v (u⊗ v)  ·  ∑ (u,v)∈U×V ρ′′u,v (u⊗ v)  = t · t′ + t · t′′ .\nLikewise, one easily obtains (t+ t′) · t′′ = t · t′′ + t′ · t′′, so the operation is distributive. For every ρ, ρ′ ∈ S and every two elements of the basis u ⊗ v, u′ ⊗ v′ ∈ W , we have, using (15),\n(ρ(u⊗ v)) · ( ρ′ ( u′ ⊗ v′ )) = ∑ (u′′,v′′)∈U×V ( ρ · ρ′ · σu′′u,u′ · τv ′′ v,v′ ) ( u′′ ⊗ v′′ ) =\n∑ (u′′,v′′)∈U×V ( ρ · ρ′ ) (( σu ′′ u,u′ · τv ′′ v,v′ ) ( u′′ ⊗ v′′ )) = ( ρ · ρ′\n) ∑ (u′′,v′′)∈U×V (( σu ′′ u,u′ · τv ′′ v,v′ ) ( u′′ ⊗ v′′ )) = ( ρ · ρ′ ) ( (u⊗ v) · ( u′ ⊗ v′ )) .\n(24)\nTherefore W is a semialgebra basis of A ⊗S A′, and A ⊗S A′ = (A⊗S A′,+, ·, 0A ⊗ 0A′) is an S-semialgebra.\nFor every a, b ∈ A and a′, b′ ∈ A′, we have( a⊗ a′ ) · ( b⊗ b′ ) = ( a · a′ ) ⊗ ( b · b′ ) , (25)\nbecause, by using unique linear combinations of the basis W , a = ∑ u∈U αuu, b = ∑\nu∈U βuu, a′ = ∑ v∈V α ′ vv, and b ′ = ∑ v∈V β ′ vv, bilinearity of tensor product, (15), (24), the definition of structure constants, and the fact that U and V are semialgebra bases of A and A′, respectively, we obtain(\na⊗ a′ ) · ( b⊗ b′ ) =\n((∑ u∈U αuu ) ⊗ (∑ v∈V α′vv )) · ((∑ u∈U βuu ) ⊗ (∑ v∈V β′vv ))\n=  ∑ (u,v)∈U×V ( αu · α′v ) (u⊗ v)  ·  ∑ (u,v)∈U×V ( βu · β′v ) (u⊗ v)  =\n∑ (u,v)∈U×V ∑ (u′,v′)∈U×V ∑ (u′′,v′′)∈U×V ( αu · α′v · βu′ · β′v′ · σu ′′ u,u′ · τv ′′ v,v′ ) ( u′′ ⊗ v′′ ) =\n∑ (u,v)∈U×V ∑ (u′,v′)∈U×V (∑ u′′∈U ( αu · βu′ · σu ′′ u,u′ ) u′′ ) ⊗ (∑ v′′∈V ( α′v · β′v′ · τv ′′ v,v′ ) v′′ )\n= ∑\n(u,v)∈U×V ∑ (u′,v′)∈U×V ( (αu · βu′) ( u · u′ )) ⊗ (( α′v · β′v′ ) ( v · v′ )) =\n∑ (u,v)∈U×V ∑ (u′,v′)∈U×V ( (αuu) · ( βu′u ′))⊗ ((α′vv) · (β′v′v′)) =\n((∑ u∈U αuu ) · (∑ u∈U βuu )) ⊗ ((∑ v∈V α′vv ) · (∑ v∈V β′vv )) = ( a · a′ ) ⊗ ( b · b′ ) .\nFor every two elements of the basis u ⊗ v, u′ ⊗ v′ ∈ W , using (25) and commutativity of A and A′, (u⊗ v) · (u′ ⊗ v′) = (u · u′) ⊗ (v · v′) = (u′ · u) ⊗ (v′ · v) = (u′ ⊗ v′) · (u⊗ v). From Hebisch and Weinert (1998, Theorem V.2.4), this equation is sufficient condition for A⊗S A′ to be commutative.\nFor every t, t′ ∈ A⊗S A′ and µ ∈ S,\n(µt) · t′ = t · ( µt′ ) = µ ( t · t′ ) (26)\nholds because, using the unique linear combinations of the basis W for t and t′, say, t =∑ (u,v)∈U×V ρu,v (u⊗ v) and t′ = ∑ (u,v)∈U×V ρ ′ u,v (u⊗ v), and (15),\n(µt) · t′\n= µ  ∑\n(u,v)∈U×V\nρu,v (u⊗ v)  ·  ∑\n(u,v)∈U×V\nρ′u,v (u⊗ v)  =\n ∑ (u,v)∈U×V (µ · ρu,v) (u⊗ v)  ·  ∑ (u,v)∈U×V ρ′u,v (u⊗ v)  =\n∑ (u,v)∈U×V ∑ (u′,v′)∈U×V ∑ (u′′,v′′)∈U×V ( µ · ρu,v · ρ′u′,v′ · σu ′′ u,u′ · τv ′′ v,v′ ) ( u′′ ⊗ v′′ ) = µ\n∑ (u,v)∈U×V ∑ (u′,v′)∈U×V ∑ (u′′,v′′)∈U×V ( ρu,v · ρ′u′,v′ · σu ′′ u,u′ · τv ′′ v,v′ ) ( u′′ ⊗ v′′ )\n= µ  ∑ (u,v)∈U×V ρu,v (u⊗ v)  ·  ∑ (u,v)∈U×V ρ′u,v (u⊗ v)  = µ ( t · t′ )\nLikewise, one easily obtains t · (µt′) = µ(t · t′). For every element of A⊗S A′ written in the unique linear combinations of the basis W ,\nsay, t = ∑\n(u,v)∈U×V ρu,v(u⊗ v), using (26) and (25),\n(1A ⊗ 1A′) · t = (1A ⊗ 1A′) ·  ∑ (u,v)∈U×V ρu,v(u⊗ v)  = ∑ (u,v)∈U×V ρu,v((1A ⊗ 1A′) · (u⊗ v))\n= ∑\n(u,v)∈U×V\nρu,v((1A · u)⊗ (1A′ · v)) = ∑\n(u,v)∈U×V\nρu,v(u⊗ v) = t .\nTherefore, 1A ⊗ 1A′ is the identity element of A⊗S A′, and A⊗S A′ is unital. From (15), the structure constants of A⊗S A′ with respect to the semialgebra basis W is clearly ωu ′′⊗v′′ u⊗v,u′⊗v′ = σ u′′ u,u′ · τv ′′ v,v′ .\nSince A and A′ are associative semialgebras, by using Hebisch and Weinert (1998, Theorem V.2.4), ∑ u′′′∈U σ u′′′ u,u′ · σu ′′′′ u′′′,u′′ = ∑ u′′′∈U σ u′′′ u′,u′′ · σu ′′′′ u,u′′′ holds for every u, u\n′, u′′, u′′′′ ∈ U , and ∑ v′′′∈V τ v′′′ v,v′ · τv ′′′′ v′′′,v′′ = ∑ v′′′∈V τ v′′′ v′,v′′ · τv ′′′′ v,v′′′ for every v, v\n′, v′′, v′′′′ ∈ V . Therefore,∑ (u′′′,v′′′)∈U×V ωu ′′′⊗v′′′ u⊗u,u′⊗v′ · ω u′′′′⊗v′′′′ u′′′⊗v′′′,u′′⊗v′′ = ∑ (u′′′,v′′′)∈U×V σu ′′′ u,u′ · τv ′′′ v,v′ · σu ′′′′ u′′′,u′′ · τv ′′′′ v′′′,v′′\n= ( ∑ u′′′∈U σu ′′′ u,u′ · σu ′′′′ u′′′,u′′ ) · ( ∑ v′′′∈V τv ′′′ v,v′ · τv ′′′′ v′′′,v′′ )\n= ( ∑ u′′′∈U σu ′′′ u′,u′′ · σu ′′′′ u,u′′′ ) · ( ∑ v′′′∈V τv ′′′ v′,v′′ · τv ′′′′ v,v′′′ ) =\n∑ (u′′′,v′′′)∈U×V σu ′′′ u′,u′′ · τv ′′′ v′,v′′ · σu ′′′′ u′′′,u′′ · τv ′′′′ v′′′,v′′ = ∑ (u′′′,v′′′)∈U×V ωu ′′′⊗v′′′ u′⊗v′,u′′⊗v′′ · ω u′′′′⊗v′′′′ u⊗v,u′′′⊗v′′′\nholds for every (u, v) , (u′, v′) , (u′′, v′′) , (u′′′′, v′′′′) ∈ U × V . Again by Hebisch and Weinert (1998, Theorem V.2.4), this equation is sufficient condition for A⊗S A′ to be associative.\nTherefore, A⊗S A′ is a commutative unital associative S-semialgebra."
    }, {
      "heading" : "Appendix D. Proof of Theorem 2",
      "text" : "Proof For every (m,n) , (m′, n′) ∈M ×N , using the homomorphism of f and g, and the equation (16), we have\n(f ⊗ g) ( (m,n) · ( m,n′ )) = (f ⊗ g) ( m ·m′, n · n′ ) = ( f ( m ·m′ )) ⊗ ( g ( n · n′ )) = ( f(m) · f ( m′ )) ⊗ ( g(m) · g ( n′ )) = (f(m)⊗ g(n)) · ( f ( m′ ) ⊗ g ( n′ ))\n= ((f ⊗ g) (m,n)) · ( (f ⊗ g) ( m′, n′ )) ,\nso f ⊗ g is a monoid homomorphism from M ⊗ N to the multiplicative monoid (A ⊗S A′, ·, 1A ⊗ 1A′) of the tensor product of the semialgebras A ⊗S A′. Thus the sextuple G = (G, op,M ×N,φ × ψ,A ⊗S A′, f ⊗ g) specifies the (f ⊗ g)-parametrized computation\ngraph (G, op, A⊗S A′, (f ⊗ g) ◦ (φ× ψ)). Therefore, using Theorem 1, we have\nαG(t) = ∑\ni∈N0n ct,i\n( (f ⊗ g) (( ((φ× ψ) (s1))i1 ) · · · ( ((φ× ψ) (sn))in )))\n= ∑\ni∈N0n ct,i\n( (f ⊗ g) ( (φ(s1) , ψ(s1)) i1 · · · (φ(sn) , ψ(sn))in ))\n= ∑\ni∈N0n ct,i\n( (f ⊗ g) ( (φ(s1)) i1 · · · (φ(sn))in , (ψ(s1))i1 · · · (ψ(sn))in ))\n= ∑\ni∈N0n ct,i\n( f ( (φ(s1)) i1 · · · (φ(sn))in ) ⊗ g ( (ψ(s1)) i1 · · · (ψ(sn))in )) .\nFinally, Definition 15 guarantees the existence of L. In fact, a function L that maps each element of A ⊗S A′ written as the linear combination of the basis {u⊗ u′}u∈U, u′∈U ′ , say,∑\nu∈U,u′∈U ′ ρu,u′(u⊗ u′) where ρu,u′ ∈ S, and U and U ′ are bases of A and A′, respectively, to ∑\nu∈U,u′∈U ′ ρu,u′B(u, u ′) is an S-homomorphism and satisfies\nL(αG(t)) = ∑\ni∈N0n ct,iB\n( f ( (φ(s1)) i1 · · · (φ(sn))in ) , g ( (ψ(s1)) i1 · · · (ψ(sn))in )) ."
    }, {
      "heading" : "Appendix E. Proof of Lemma 6",
      "text" : "Proof For every element of A⊗SBC1S written as the unique linear combination of elements of the basis {u⊗ êi}u∈U, i∈{0,1}, say, x = ∑ u∈U ∑ i∈{0,1} σu,i (u⊗ êi), we have\nx = ∑ u∈U ∑ i∈{0,1} σu,i (u⊗ êi) = ∑ u∈U σu,0 (u⊗ ê0) + ∑ u∈U σu,1 (u⊗ ê1)\n= ∑ u∈U (σu,0u)⊗ ê0 + ∑ u∈U (σu,1u)⊗ ê1 = P0(x)⊗ ê0 + P1(x)⊗ ê1 ."
    }, {
      "heading" : "Appendix F. Proof of Lemma 7",
      "text" : "Proof For every two elements of A⊗S BC1S written as linear combinations of elements of a basis {u⊗ êi}u∈U, i∈{0,1}, say, x = ∑ u∈U ∑ i∈{0,1} σu,i(u⊗ êi) and y = ∑ u∈U ∑ i∈{0,1} τu,i(u⊗ êi), we have\nP0(x+ y)\n= P0 ∑ u∈U ∑ i∈{0,1} (σu,i + τu,i) (u⊗ êi)  = ∑ u∈U (σu,0 + τu,0)u\n= ∑ u∈U σu,0u+ ∑ u∈U τu,0u\n= P0 ∑ u∈U ∑ i∈{0,1} σu,i(u⊗ êi) + P0 ∑ u∈U ∑ i∈{0,1} τu,i(u⊗ êi)  = P0(x) + P0(y) .\nLikewise, it is easy to show P1(x+ y) = P1(x) + P1(y) for every two elements x, y ∈ A⊗S BC1S ."
    }, {
      "heading" : "Appendix G. Proof of Lemma 8",
      "text" : "Proof If t ∈ src(G), from Definition 5, we have P0(αG(t)) = P0(ξ(t)) = (P0 ◦ ξ) (t) = αG′(t).\nConsider the case where t is an element of V such that t /∈ src(G). Assume the induction hypothesis that (19) holds for every element of E−G(v). If op(v) = “+” then, by Definition 5, Lemma 7, and the induction hypothesis, we have\nP0(αG(t)) = P0  ∑ e∈E−G(t) αG(e)  = ∑ e∈E−G(t) P0(αG(e)) = ∑ e∈E−G(t) αG′(t) .\nOtherwise (i.e., op(t) = “·”), by using Definition 5, Lemma 7, the linear combination of elements of a basis {u⊗êi}u∈U, i∈{0,1} of A⊗SBC1S for αG(t), say, αG(t) = ∑ u∈U, i∈{0,1} ηt,u,i(u⊗ êi) where ηt,u,i ∈ S, and the induction hypothesis, and by noting that x = (P0(x)) ⊗ ê0 + (P1(x))⊗ ê1, P0((P0(x))⊗ ê0) = P0(x), and P0((P1(x))⊗ ê1) = 0A for every x ∈ A⊗S BC1S , and êi1 · · · êin = ê0 if and only if ij = 0 for all j, we have\nP0(αG(t)) = P0  ∏ e∈E−G(t) αG(e)  = P0  ∏ e∈E−G(t) ∑ u∈U, i∈{0,1} ηe,u,i (u⊗ êi)  =\n∏ e∈E−G(t) (∑ u∈U ηe,u,0 (u⊗ ê0) ) = ∏ e∈E−G(t) P0(αG(e)) = ∏ e∈E−G(t) αG′(e) = αG′(t) .\nConsider the case where t is an element of E. Assume the induction hypothesis that (19) holds for tail(t). Then, by using Definition 5 and the induction hypothesis, we have P0(αG(t)) = P0(αG(tail(t))) = αG′(tail(t)) = αG′(t).\nTherefore, we have proven that the equation (19) holds for every t ∈ V ∪E by induction on the finite dag G."
    }, {
      "heading" : "Appendix H. Proof of Lemma 9",
      "text" : "Proof Let ae,0 = P0(αG(e)) and ae,1 = P1(αG(e)), and let E − G(v) = {e1, . . . , en}. Note that, for the elements ê0 = (1S , 0S) , ê1 = (0S , 1S) of a basis of BC 1 S , êi1 · êi2 · · · êin = ê0 if and only if i1 = i2 = · · · = in = 0, êi1 · êi2 · · · êin = ê1 if and only if only one of ij is equal\nto 1 and all the others are equal to 0, and êi1 · êi2 · · · êin = 0BC1S otherwise, because of the definition of the multiplication of BC1S (cf. Definition 8). By noting above-mentioned facts, and using the linear combination of elements of the basis {u, êi}u∈U, i∈{0,1} for an element of A⊗S BC1S , and the equation (16), we have∏ e∈E−G (v) αG(e)\n= ∏\ne∈E−G (v)\n(ae,0 ⊗ ê0 + ae,1 ⊗ ê1)\n= (ae1,0 ⊗ ê0) · (ae2,0 ⊗ ê0) · · · (aen,0 ⊗ ê0) + (ae1,0 ⊗ ê0) · (ae2,0 ⊗ ê0) · · · (aen,1 ⊗ ê1)\n+ · · · + (ae1,1 ⊗ ê1) · (ae2,1 ⊗ ê1) · · · (aen,1 ⊗ ê1)\n= (ae0,0 · ae1,0 · · · aen,0)⊗ (ê0 · ê0 · · · ê0) + (ae0,0 · ae1,0 · · · aen,1)⊗ (ê0 · ê0 · · · ê1) + · · · + (ae0,1 · ae1,1 · · · aen,1)⊗ (ê1 · ê1 · · · ê1) = (ae0,0 · ae1,0 · · · aen,0)⊗ ê0 + (ae1,1 · ae1,0 · · · aen,0 + ae1,0 · ae1,1 · · · aen,0 + · · ·+ ae1,0 · ae1,0 · · · aen,1)⊗ ê1\n=  ∏ e∈E−G (v) ae,0 ⊗ ê0 +  ∑ e∈E−G (v) ae,1 ·  ∏ e′∈E−G (v)\\{e} ae′,0  ⊗ ê1\n=  ∏ e∈E−G (v) P0 (αG(e)) ⊗ ê0 +  ∑ e∈E−G (v) P1(αG(e)) ·  ∏ e′∈E−G (v)\\{e} P0 ( αG ( e′ )) ⊗ ê1 . Therefore, we finally obtain\nP1  ∏ e∈E−G (v) αG(e)  = ∑ e∈E−G (v) P1(αG(e)) ·  ∏ e′∈E−G (v)\\{e} P0 ( αG ( e′ )) .\nAppendix I. Proof of Lemma 10\nProof Let x ∦ y denote that x and y are comparable for every x, y ∈ O. Since C 6= snk(G), it is obvious that ↑C \\ C 6= ∅. Since G is finite (and thus so are O and ↑C \\ C), there exists at least one minimal element y in ↑C \\ C. Because a maximal chain of O passing through y intersects the antichain cutset C, there exists at least one element x ∈ C such that x < y. If there exists z ∈ V ∪ E such that x < z < y then z is\nan element of ↑C \\ C, but this contradicts the fact that y is a minimal element in ↑C \\ C. Therefore x ≺ y is shown by contradiction, and one can obtain x ∈ C such that there exists y ∈ V ∪ E satisfying x ≺ y and y is a minimal element of ↑C \\ C.\n1) If x ∈ V , it is obvious that D′ = E+G(x) and D = {x}. For every d′ ∈ D′, the covered set of d′ is the singleton set {x}, so the covered set of d′ is a subset of C. C ′ = (C ∪D′) \\D is clearly an antichain cutset. C < C ′ is obvious. By using Definitions 5 and 20, and noting that tail(e) = x holds in the summand of ∑ e∈E+G(x) , we have\n∑ e∈E+G(x) P1(αG(e)) · βG(e) = ∑ e∈E+G(x) P1(αG(tail(e))) · βG(e)\n= ∑\ne∈E+G(x)\nP1(αG(x)) · βG(e) = P1(αG(x)) · ∑\ne∈E+G(x)\nβG(e) = P1(αG(x)) · βG(x)\nBy noting D ⊆ C and C ∩D′ = ∅, and using the above equation, we finally obtain\n∑ c∈C′ P1(αG(c)) · βG(c)\n= ∑ c∈C P1(αG(c)) · βG(c) + ∑ d′∈D′ P1 ( αG ( d′ )) · βG ( d′ ) − ∑ d∈D P1(αG(d)) · βG(d)\n= ∑ c∈C P1(αG(c)) · βG(c) + ∑\ne∈E+G(x)\nP1(αG(e)) · βG(e)− P1(αG(x)) · βG(x)\n= ∑ c∈C P1(αG(c)) · βG(c) .\n2) If x ∈ E, it is obvious that D′ = {head(x)} and D = E−G(head(x)). The covered set by an element of D′ is D = E−G(head(x)). Assume that there exists d ∈ D such that d /∈ C. Because C intersects every maximal chain of O, there exists c ∈ C such that c ∦ d. Further assume that d < c then d ≺ head(x) ≤ c thus x ≺ d′ ≤ c, but this contradicts the fact that C is an antichain. Further assume that c < d then c < d ≺ head(x) but this also contradicts the fact that head(x) is a minimal element of ↑C \\ C. Therefore we conclude that d ∈ D =⇒ d ∈ C by contradiction and thus D ⊆ C.\nBecause C intersects every maximal chain of O, and every maximal chain of O intersecting D also intersects D′ (i.e., passes through head(x)), C ′ = (C ∪D′) \\D intersects every maximal chain of O. Assume that C ′ = (C ∪D′) \\D is not an antichain. Then, because C is an antichain, and D′ is also an antichain since it is a singleton set, there exists x′ ∈ C and y′ ∈ D′ such that x′ 6= y′ and x′ ∦ y′. y′ < x′ is not possible, because the covering set of y′ is the singleton set consisting only of y, and it follows that x ≺ y ≤ x′ holds, but this contradicts the fact that C is an antichain. x′ < y′ is neither possible, because it follows that x′ < y′ ≺ y, but this contradicts the fact that y is a minimal element of ↑C \\C. Hence, we have proven C ′ is an antichain by contradiction. Therefore, C ′ = (C ∪D′) \\ D is an antichain cutset. C < C ′ is obvious.\n2-a) If op(head(x)) = “+”, by using Definitions 5 and 20, and Lemma 7, and noting that head(x) = head(e) holds in the summand of ∑ e∈E−G(head(x)) , we have\nP1(αG(head(x))) · βG(head(x)) = P1  ∑ e∈E−G(head(x)) αG(e)  · βG(head(x)) =\n ∑ e∈E−G(head(x)) P1(αG(e))  · βG(head(x)) = ∑ e∈E−G(head(x)) (P1(αG(e)) · βG(head(x)))\n= ∑\ne∈E−G(head(x))\n(P1(αG(e)) · βG(head(e))) = ∑\ne∈E−G(head(x))\nP1(αG(e)) · βG(e)\nBy noting D ⊆ C and C ∩D′ = ∅, and using the above equation, we finally obtain∑ c∈C′ P1(αG(c)) · βG(c)\n= ∑ c∈C P1(αG(c)) · βG(c) + ∑ d′∈D′ P1 ( αG ( d′ )) · βG ( d′ ) − ∑ d∈D P1(αG(d)) · βG(d)\n= ∑ c∈C P1(αG(c)) · βG(c)\n+ P1(αG(head(x))) · βG(head(x))− ∑\ne∈E−G(head(x))\nP1(αG(e)) · βG(e)\n= ∑ c∈C P1(αG(c)) · βG(c) .\n2-b) If op(head(x)) = “·”, by using Definitions 5 and 20, and Lemma 9, and noting that head(x) = head(e) holds in the summand of ∑ e∈E−G(head(x)) , we have\nP1(αG(head(x))) · βG(head(x)) = P1  ∏ e∈E−G(head(x)) αG(e)  · βG(head(x)) =\n ∑ e∈E−G(head(x)) P1(αG(e)) ·  ∏ e′∈E−G(head(x))\\{e} P0 ( αG ( e′ ))  · βG(head(x)) =\n∑ e∈E−G(head(x)) P1(αG(e)) ·  ∏ e′∈E−G(head(x))\\{e} P0 ( αG ( e′ )) · βG(head(x))  =\n∑ e∈E−G(head(x)) P1(αG(e)) ·  ∏ e′∈E−G(head(e))\\{e} P0 ( αG ( e′ )) · βG(head(e)) \n= ∑\ne∈E−G(head(x))\nP1(αG(e)) · βG(e) .\nBy noting D ⊆ C and C ∩D′ = ∅, and using the above equation, we finally obtain∑ c∈C′ P1(αG(c)) · βG(c)\n= ∑ c∈C P1(αG(c)) · βG(c) + ∑ d′∈D′ P1 ( αG ( d′ )) · βG ( d′ ) − ∑ d∈D P1(αG(d)) · βG(d)\n= ∑ c∈C P1(αG(c)) · βG(c)\n+ P1(αG(head(x))) · βG(head(x))− ∑\ne∈E−G(head(x))\nP1(αG(e)) · βG(e)\n= ∑ c∈C P1(αG(c)) · βG(c) ."
    }, {
      "heading" : "Appendix J. Proof of Theorem 3",
      "text" : "Proof Let C be an antichain cutset of O. Then, by repeating the construction of a covering antichain cutset described in Lemma 10, one can obtain a strictly ascending chain of the lattice of antichain cutsets (AC (O) ,≤) starting at C, say, C = C1 < C2 < · · · < Cn = snk(G) where Ci+1 is a covering antichain cutset of Ci for i = 1, . . . , n − 1, and, as it is written, this ascending chain eventually terminates at Cn = snk(G) since G is finite. Moreover, again by Lemma 10, we have ∑ c∈Ci P1(αG(c)) ·βG(c) = ∑ c∈Ci+1 P1(αG(c)) ·βG(c)\nfor i = 1, . . . , n − 1. By induction on the chain C = C1 < · · · < Cn = snk(G), one obtains∑ c∈C P1(αG(c)) ·βG(c) = ∑ c∈snk(G) P1(αG(c)) ·βG(c). The above discussion can be repeated\non another antichain cutset C ′ ∈ AC(O). Therefore, we finally obtain ∑\nc∈C P1(αG(c)) · βG(c) = ∑ c∈snk(G) P1(αG(c)) · βG(c) = ∑ c∈C′ P1(αG(a)) · βG(c). It is obvious that src(G)\nand snk(G) are an antichain cutset of O, so ∑ c∈src(G) P1(ξ(c)) ·βG(c) = ∑ c∈src(G) P1(αG(c)) ·\nβG(c) = ∑ c∈snk(G) P1(αG(c)) · βG(c) = ∑ c∈snk(G) P1(αG(c)) = P1 (∑ c∈snk(G) αG(c) ) clearly holds."
    } ],
    "references" : [ {
      "title" : "The generalized distributive law",
      "author" : [ "Srinivas M. Aji", "Robert J. McEliece" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Aji and McEliece.,? \\Q2000\\E",
      "shortCiteRegEx" : "Aji and McEliece.",
      "year" : 2000
    }, {
      "title" : "Binary decision diagrams",
      "author" : [ "Sheldon B. Akers" ],
      "venue" : "IEEE Transactions on Computers,",
      "citeRegEx" : "Akers.,? \\Q1978\\E",
      "shortCiteRegEx" : "Akers.",
      "year" : 1978
    }, {
      "title" : "A generalization of forward-backward algorithm",
      "author" : [ "Ai Azuma", "Yuji Matsumoto" ],
      "venue" : "Transactions of the Japanese Society for Artificial Intelligence,",
      "citeRegEx" : "Azuma and Matsumoto.,? \\Q2010\\E",
      "shortCiteRegEx" : "Azuma and Matsumoto.",
      "year" : 2010
    }, {
      "title" : "Symbolic Boolean manipulation with ordered binary decision diagrams",
      "author" : [ "Randal E. Bryant" ],
      "venue" : "ACM Computing Surveys (CSUR),",
      "citeRegEx" : "Bryant.,? \\Q1992\\E",
      "shortCiteRegEx" : "Bryant.",
      "year" : 1992
    }, {
      "title" : "Implementing em and Viterbi algorithms for hidden Markov model in linear memory",
      "author" : [ "Alexander Churbanov", "Stephen Winters-Hilt" ],
      "venue" : "BMC Bioinformatics,",
      "citeRegEx" : "Churbanov and Winters.Hilt.,? \\Q2008\\E",
      "shortCiteRegEx" : "Churbanov and Winters.Hilt.",
      "year" : 2008
    }, {
      "title" : "Problog: A probabilistic prolog and its application in link discovery",
      "author" : [ "Luc De Raedt", "Angelika Kimmig", "Hannu Toivonen" ],
      "venue" : "In Proceedings of the 20th International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "Raedt et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Raedt et al\\.",
      "year" : 2007
    }, {
      "title" : "Expectation semirings: Flexible em for learning finite-state transducers",
      "author" : [ "Jason Eisner" ],
      "venue" : "In Proceedings of the ESSLLI Workshop on Finite-State Methods in NLP (FSMNLP),",
      "citeRegEx" : "Eisner.,? \\Q2001\\E",
      "shortCiteRegEx" : "Eisner.",
      "year" : 2001
    }, {
      "title" : "Inside-outside and forward-backward algorithms are just backprop (tutorial paper)",
      "author" : [ "Jason Eisner" ],
      "venue" : "In Proceedings of the EMNLP 16 Workshop on Structured Prediction for NLP,",
      "citeRegEx" : "Eisner.,? \\Q2016\\E",
      "shortCiteRegEx" : "Eisner.",
      "year" : 2016
    }, {
      "title" : "Dyna: Extending datalog for modern ai",
      "author" : [ "Jason Eisner", "Nathaniel W. Filardo" ],
      "venue" : "In Datalog Reloaded,",
      "citeRegEx" : "Eisner and Filardo.,? \\Q2011\\E",
      "shortCiteRegEx" : "Eisner and Filardo.",
      "year" : 2011
    }, {
      "title" : "The viterbi algorithm",
      "author" : [ "G. David Forney", "Jr." ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Forney and Jr.,? \\Q1973\\E",
      "shortCiteRegEx" : "Forney and Jr.",
      "year" : 1973
    }, {
      "title" : "The sum-product theorem: A foundation for learning tractable models",
      "author" : [ "Abram L. Friesen", "Pedro Domingos" ],
      "venue" : "In Proceedings of the 33rd International Conference on Machine Learning (ICML 2016),",
      "citeRegEx" : "Friesen and Domingos.,? \\Q2016\\E",
      "shortCiteRegEx" : "Friesen and Domingos.",
      "year" : 2016
    }, {
      "title" : "Directed hypergraphs and applications",
      "author" : [ "Giorgio Gallo", "Giustino Longo", "Stefano Pallottino", "Sang Nguyen" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "Gallo et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Gallo et al\\.",
      "year" : 1993
    }, {
      "title" : "Semirings and their applications",
      "author" : [ "Jonathan S. Golan" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "Golan.,? \\Q1999\\E",
      "shortCiteRegEx" : "Golan.",
      "year" : 1999
    }, {
      "title" : "Evaluating derivatives: Principles and techniques of algorithmic differentiation",
      "author" : [ "Andreas Griewank", "Andrea Walther" ],
      "venue" : "SIAM, second edition,",
      "citeRegEx" : "Griewank and Walther.,? \\Q2008\\E",
      "shortCiteRegEx" : "Griewank and Walther.",
      "year" : 2008
    }, {
      "title" : "Semirings – algebraic theory and applications in computer science",
      "author" : [ "Udo Hebisch", "Hanns J. Weinert" ],
      "venue" : "World Scientific,",
      "citeRegEx" : "Hebisch and Weinert.,? \\Q1998\\E",
      "shortCiteRegEx" : "Hebisch and Weinert.",
      "year" : 1998
    }, {
      "title" : "Lattices of crosscuts",
      "author" : [ "Denis Higgs" ],
      "venue" : "Algebra Universalis,",
      "citeRegEx" : "Higgs.,? \\Q1986\\E",
      "shortCiteRegEx" : "Higgs.",
      "year" : 1986
    }, {
      "title" : "Advanced dynamic programming in semiring and hypergraph frameworks",
      "author" : [ "Liang Huang" ],
      "venue" : "In Proceedings of the 22nd International Conference on Computational Linguistics (COLING",
      "citeRegEx" : "Huang.,? \\Q2008\\E",
      "shortCiteRegEx" : "Huang.",
      "year" : 2008
    }, {
      "title" : "Computation of crossmoments using message passing over factor graphs",
      "author" : [ "Velimir M. Ilić", "Miomir S. Stanković", "Branimir T. Todorović" ],
      "venue" : "Advances in Mathematics of Communications,",
      "citeRegEx" : "Ilić et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ilić et al\\.",
      "year" : 2012
    }, {
      "title" : "Propositionalizing the em algorithm by bdds",
      "author" : [ "Masakazu Ishihata", "Yoshitaka Kameya", "Taisuke Sato", "Shin-ichi Minato" ],
      "venue" : "In Late breaking papers of the 18th International Conference on Inductive Logic Programming",
      "citeRegEx" : "Ishihata et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ishihata et al\\.",
      "year" : 2008
    }, {
      "title" : "On the memory complexity of the forward-backward algorithm",
      "author" : [ "Wael Khreich", "Eric Granger", "Ali Miri", "Robert Sabourin" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "Khreich et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Khreich et al\\.",
      "year" : 2010
    }, {
      "title" : "An algebraic prolog for reasoning about possible worlds",
      "author" : [ "Angelika Kimmig", "Guy Van den Broeck", "Luc De Raedt" ],
      "venue" : "In Proceedings of the 25th AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Kimmig et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kimmig et al\\.",
      "year" : 2011
    }, {
      "title" : "Parsing and hypergraphs",
      "author" : [ "Dan Klein", "Christopher D. Manning" ],
      "venue" : "In New Developments in Parsing Technology,",
      "citeRegEx" : "Klein and Manning.,? \\Q2004\\E",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2004
    }, {
      "title" : "The art of computer programming, volume 4, fascicle 1: Bitwise tricks & techniques; binary decision",
      "author" : [ "Donald E. Knuth" ],
      "venue" : "diagrams. Addison-Wesley,",
      "citeRegEx" : "Knuth.,? \\Q2009\\E",
      "shortCiteRegEx" : "Knuth.",
      "year" : 2009
    }, {
      "title" : "Factor graphs and the sum-product algorithm",
      "author" : [ "Frank R. Kschischang", "Brendan J. Frey", "Hans-Andrea Loeliger" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Kschischang et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Kschischang et al\\.",
      "year" : 2001
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando Pereira" ],
      "venue" : "In Proceedings of the 18th International Conference on Machine Learning (ICML",
      "citeRegEx" : "Lafferty et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "The estimation of stochastic context-free grammars using the inside-outside algorithm",
      "author" : [ "Karim Lari", "Steve J. Young" ],
      "venue" : "Computer Speech & Language,",
      "citeRegEx" : "Lari and Young.,? \\Q1990\\E",
      "shortCiteRegEx" : "Lari and Young.",
      "year" : 1990
    }, {
      "title" : "First- and second-order expectation semirings with applications to minimum-risk training on translation forests",
      "author" : [ "Zhifei Li", "Jason Eisner" ],
      "venue" : "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Li and Eisner.,? \\Q2009\\E",
      "shortCiteRegEx" : "Li and Eisner.",
      "year" : 2009
    }, {
      "title" : "Translation as weighted deduction",
      "author" : [ "Adam Lopez" ],
      "venue" : "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL",
      "citeRegEx" : "Lopez.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lopez.",
      "year" : 2009
    }, {
      "title" : "Efficient computation of entropy gradient for semisupervised conditional random fields",
      "author" : [ "Gideon S. Mann", "Andrew McCallum" ],
      "venue" : "In Proceedings of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACLHLT",
      "citeRegEx" : "Mann and McCallum.,? \\Q2007\\E",
      "shortCiteRegEx" : "Mann and McCallum.",
      "year" : 2007
    }, {
      "title" : "And/or multi-valued decision diagrams (aomdds) for graphical models",
      "author" : [ "Robert Mateescu", "Rina Dechter", "Radu Marinescu" ],
      "venue" : "Journal of Artificial Intelligence Research (JAIR),",
      "citeRegEx" : "Mateescu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mateescu et al\\.",
      "year" : 2008
    }, {
      "title" : "Case-factor diagrams for structured probabilistic modeling",
      "author" : [ "David McAllester", "Michael Collins", "Fernando Pereira" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "McAllester et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "McAllester et al\\.",
      "year" : 2008
    }, {
      "title" : "A linear memory algorithm for Baum-Welch training",
      "author" : [ "István Miklós", "Irmtraud M. Meyer" ],
      "venue" : "BMC Bioinformatics,",
      "citeRegEx" : "Miklós and Meyer.,? \\Q2005\\E",
      "shortCiteRegEx" : "Miklós and Meyer.",
      "year" : 2005
    }, {
      "title" : "Zero-suppressed bdds for set manipulation in combinatorial problems",
      "author" : [ "Shin-ichi Minato" ],
      "venue" : "In Proceedings of the 30th Design Automation Conference,",
      "citeRegEx" : "Minato.,? \\Q1993\\E",
      "shortCiteRegEx" : "Minato.",
      "year" : 1993
    }, {
      "title" : "Semiring frameworks and algorithms for shortest-distance problems",
      "author" : [ "Mehryar Mohri" ],
      "venue" : "Journal of Automata, Languages and Combinatorics,",
      "citeRegEx" : "Mohri.,? \\Q2002\\E",
      "shortCiteRegEx" : "Mohri.",
      "year" : 2002
    }, {
      "title" : "Computationally tractable classes of ordered sets. In Algorithms and Order, volume 255 of Nato Science Series C",
      "author" : [ "Rolf H. Möhring" ],
      "venue" : null,
      "citeRegEx" : "Möhring.,? \\Q1989\\E",
      "shortCiteRegEx" : "Möhring.",
      "year" : 1989
    }, {
      "title" : "Probabilistic Horn abduction and Bayesian networks",
      "author" : [ "David Poole" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Poole.,? \\Q1993\\E",
      "shortCiteRegEx" : "Poole.",
      "year" : 1993
    }, {
      "title" : "A tutorial on hidden Markov models and selected applications in speech recognition",
      "author" : [ "Lawrence R. Rabiner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Rabiner.,? \\Q1989\\E",
      "shortCiteRegEx" : "Rabiner.",
      "year" : 1989
    }, {
      "title" : "Automatic differentiation: Techniques and applications, volume 120 of Lecture Notes in Computer Science",
      "author" : [ "Louis B. Rall" ],
      "venue" : null,
      "citeRegEx" : "Rall.,? \\Q1981\\E",
      "shortCiteRegEx" : "Rall.",
      "year" : 1981
    }, {
      "title" : "Parameter learning of logic programs for symbolicstatistical modeling",
      "author" : [ "Taisuke Sato", "Yoshitaka Kameya" ],
      "venue" : "Journal of Artificial Intelligence Research (JAIR),",
      "citeRegEx" : "Sato and Kameya.,? \\Q2001\\E",
      "shortCiteRegEx" : "Sato and Kameya.",
      "year" : 2001
    }, {
      "title" : "A forward-only recursion based hmm for modeling burst errors in digital channels",
      "author" : [ "Srinivas Sivaprakasam", "K. Sam Shanmugan" ],
      "venue" : "In IEEE Global Telecommunications Conference (GLOBECOM’95),",
      "citeRegEx" : "Sivaprakasam and Shanmugan.,? \\Q1995\\E",
      "shortCiteRegEx" : "Sivaprakasam and Shanmugan.",
      "year" : 1995
    }, {
      "title" : "On the bordism categories iii",
      "author" : [ "Michihiro Takahashi" ],
      "venue" : "Mathematics Seminar Notes,",
      "citeRegEx" : "Takahashi.,? \\Q1982\\E",
      "shortCiteRegEx" : "Takahashi.",
      "year" : 1982
    }, {
      "title" : "Adaptive channel/code matching",
      "author" : [ "Narciso L. Tan" ],
      "venue" : "Ph.D. thesis, University of Southern California,",
      "citeRegEx" : "Tan.,? \\Q1993\\E",
      "shortCiteRegEx" : "Tan.",
      "year" : 1993
    }, {
      "title" : "Fast Newton-cg method for batch learning of conditional random fields",
      "author" : [ "Yuta Tsuboi", "Yuya Unno", "Hisashi Kashima", "Naoaki Okazaki" ],
      "venue" : "In Proceedings of the 25th AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Tsuboi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Tsuboi et al\\.",
      "year" : 2011
    }, {
      "title" : "Unidirectional and parallel Baum-Welch algorithms",
      "author" : [ "William Turin" ],
      "venue" : "IEEE Transactions on Speech and Audio Processing,",
      "citeRegEx" : "Turin.,? \\Q1998\\E",
      "shortCiteRegEx" : "Turin.",
      "year" : 1998
    }, {
      "title" : "Decision diagrams for the computation of semiring valuations",
      "author" : [ "Nic Wilson" ],
      "venue" : "In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI-05),",
      "citeRegEx" : "Wilson.,? \\Q2005\\E",
      "shortCiteRegEx" : "Wilson.",
      "year" : 2005
    }, {
      "title" : "Inapproximability of treewidth, oneshot pebbling, and related layout problems",
      "author" : [ "Yu Wu", "Per Austrin", "Toniann Pitassi", "David Liu" ],
      "venue" : "Journal of Artificial Intelligence Research (JAIR),",
      "citeRegEx" : "Wu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2014
    }, {
      "title" : "Complex numbers in geometry",
      "author" : [ "Isaak M. Yaglom" ],
      "venue" : null,
      "citeRegEx" : "Yaglom.,? \\Q1968\\E",
      "shortCiteRegEx" : "Yaglom.",
      "year" : 1968
    } ],
    "referenceMentions" : [ {
      "referenceID" : 36,
      "context" : "Here, the term “forward algorithms” includes not only the ordinary forward algorithm (Rabiner, 1989) on trellises (Forney, 1973) for sequence labeling like hidden Markov models (HMMs) or linear-chain conditional random",
      "startOffset" : 85,
      "endOffset" : 100
    }, {
      "referenceID" : 27,
      "context" : "1 Examples of data structures on which computation is algebraically generalized include trellises for sequence labeling, the set of derivations by weighted deduction system or logic programming (Goodman, 1999; Lopez, 2009; Eisner and Filardo, 2011; Kimmig et al., 2011), junction trees (Aji and McEliece, 2000), factor graphs (Kschischang et al.",
      "startOffset" : 194,
      "endOffset" : 269
    }, {
      "referenceID" : 8,
      "context" : "1 Examples of data structures on which computation is algebraically generalized include trellises for sequence labeling, the set of derivations by weighted deduction system or logic programming (Goodman, 1999; Lopez, 2009; Eisner and Filardo, 2011; Kimmig et al., 2011), junction trees (Aji and McEliece, 2000), factor graphs (Kschischang et al.",
      "startOffset" : 194,
      "endOffset" : 269
    }, {
      "referenceID" : 20,
      "context" : "1 Examples of data structures on which computation is algebraically generalized include trellises for sequence labeling, the set of derivations by weighted deduction system or logic programming (Goodman, 1999; Lopez, 2009; Eisner and Filardo, 2011; Kimmig et al., 2011), junction trees (Aji and McEliece, 2000), factor graphs (Kschischang et al.",
      "startOffset" : 194,
      "endOffset" : 269
    }, {
      "referenceID" : 0,
      "context" : ", 2011), junction trees (Aji and McEliece, 2000), factor graphs (Kschischang et al.",
      "startOffset" : 24,
      "endOffset" : 48
    }, {
      "referenceID" : 23,
      "context" : ", 2011), junction trees (Aji and McEliece, 2000), factor graphs (Kschischang et al., 2001), directed graphs (Mohri, 2002), directed hypergraphs2 (Klein and Manning, 2004; Huang, 2008), binary decision diagrams (BDDs, Wilson, 2005), and sum-product networks (Friesen and Domingos, 2016).",
      "startOffset" : 64,
      "endOffset" : 90
    }, {
      "referenceID" : 33,
      "context" : ", 2001), directed graphs (Mohri, 2002), directed hypergraphs2 (Klein and Manning, 2004; Huang, 2008), binary decision diagrams (BDDs, Wilson, 2005), and sum-product networks (Friesen and Domingos, 2016).",
      "startOffset" : 25,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : ", 2001), directed graphs (Mohri, 2002), directed hypergraphs2 (Klein and Manning, 2004; Huang, 2008), binary decision diagrams (BDDs, Wilson, 2005), and sum-product networks (Friesen and Domingos, 2016).",
      "startOffset" : 62,
      "endOffset" : 100
    }, {
      "referenceID" : 16,
      "context" : ", 2001), directed graphs (Mohri, 2002), directed hypergraphs2 (Klein and Manning, 2004; Huang, 2008), binary decision diagrams (BDDs, Wilson, 2005), and sum-product networks (Friesen and Domingos, 2016).",
      "startOffset" : 62,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : ", 2001), directed graphs (Mohri, 2002), directed hypergraphs2 (Klein and Manning, 2004; Huang, 2008), binary decision diagrams (BDDs, Wilson, 2005), and sum-product networks (Friesen and Domingos, 2016).",
      "startOffset" : 174,
      "endOffset" : 202
    }, {
      "referenceID" : 36,
      "context" : "”1 Forward-backward algorithms include the ordinary forward-backward algorithm (Rabiner, 1989) for the ordinary forward algorithm on trellises for sequence labeling, the inside-outside algorithm (Lari and Young, 1990) for the inside algorithm on derivation forests for CYK parsing, the sum-product algorithm (Kschischang et al.",
      "startOffset" : 79,
      "endOffset" : 94
    }, {
      "referenceID" : 25,
      "context" : "”1 Forward-backward algorithms include the ordinary forward-backward algorithm (Rabiner, 1989) for the ordinary forward algorithm on trellises for sequence labeling, the inside-outside algorithm (Lari and Young, 1990) for the inside algorithm on derivation forests for CYK parsing, the sum-product algorithm (Kschischang et al.",
      "startOffset" : 195,
      "endOffset" : 217
    }, {
      "referenceID" : 23,
      "context" : "”1 Forward-backward algorithms include the ordinary forward-backward algorithm (Rabiner, 1989) for the ordinary forward algorithm on trellises for sequence labeling, the inside-outside algorithm (Lari and Young, 1990) for the inside algorithm on derivation forests for CYK parsing, the sum-product algorithm (Kschischang et al., 2001) for a unidirectional message-passing on acyclic factor graphs, and so on.",
      "startOffset" : 308,
      "endOffset" : 334
    }, {
      "referenceID" : 41,
      "context" : "Examples of such complicated computation include “forward-only computation” of the Baum-Welch algorithm (Tan, 1993; Sivaprakasam and Shanmugan, 1995; Turin, 1998; Miklós and Meyer, 2005; Churbanov and Winters-Hilt, 2008), the entropy gradient of CRFs (Mann and McCallum, 2007), the gradient of entropy or risk of acyclic hypergraphs (Li and Eisner, 2009), Hessian-vector products of CRFs (Tsuboi et al.",
      "startOffset" : 104,
      "endOffset" : 220
    }, {
      "referenceID" : 39,
      "context" : "Examples of such complicated computation include “forward-only computation” of the Baum-Welch algorithm (Tan, 1993; Sivaprakasam and Shanmugan, 1995; Turin, 1998; Miklós and Meyer, 2005; Churbanov and Winters-Hilt, 2008), the entropy gradient of CRFs (Mann and McCallum, 2007), the gradient of entropy or risk of acyclic hypergraphs (Li and Eisner, 2009), Hessian-vector products of CRFs (Tsuboi et al.",
      "startOffset" : 104,
      "endOffset" : 220
    }, {
      "referenceID" : 43,
      "context" : "Examples of such complicated computation include “forward-only computation” of the Baum-Welch algorithm (Tan, 1993; Sivaprakasam and Shanmugan, 1995; Turin, 1998; Miklós and Meyer, 2005; Churbanov and Winters-Hilt, 2008), the entropy gradient of CRFs (Mann and McCallum, 2007), the gradient of entropy or risk of acyclic hypergraphs (Li and Eisner, 2009), Hessian-vector products of CRFs (Tsuboi et al.",
      "startOffset" : 104,
      "endOffset" : 220
    }, {
      "referenceID" : 31,
      "context" : "Examples of such complicated computation include “forward-only computation” of the Baum-Welch algorithm (Tan, 1993; Sivaprakasam and Shanmugan, 1995; Turin, 1998; Miklós and Meyer, 2005; Churbanov and Winters-Hilt, 2008), the entropy gradient of CRFs (Mann and McCallum, 2007), the gradient of entropy or risk of acyclic hypergraphs (Li and Eisner, 2009), Hessian-vector products of CRFs (Tsuboi et al.",
      "startOffset" : 104,
      "endOffset" : 220
    }, {
      "referenceID" : 4,
      "context" : "Examples of such complicated computation include “forward-only computation” of the Baum-Welch algorithm (Tan, 1993; Sivaprakasam and Shanmugan, 1995; Turin, 1998; Miklós and Meyer, 2005; Churbanov and Winters-Hilt, 2008), the entropy gradient of CRFs (Mann and McCallum, 2007), the gradient of entropy or risk of acyclic hypergraphs (Li and Eisner, 2009), Hessian-vector products of CRFs (Tsuboi et al.",
      "startOffset" : 104,
      "endOffset" : 220
    }, {
      "referenceID" : 28,
      "context" : "Examples of such complicated computation include “forward-only computation” of the Baum-Welch algorithm (Tan, 1993; Sivaprakasam and Shanmugan, 1995; Turin, 1998; Miklós and Meyer, 2005; Churbanov and Winters-Hilt, 2008), the entropy gradient of CRFs (Mann and McCallum, 2007), the gradient of entropy or risk of acyclic hypergraphs (Li and Eisner, 2009), Hessian-vector products of CRFs (Tsuboi et al.",
      "startOffset" : 251,
      "endOffset" : 276
    }, {
      "referenceID" : 26,
      "context" : "Examples of such complicated computation include “forward-only computation” of the Baum-Welch algorithm (Tan, 1993; Sivaprakasam and Shanmugan, 1995; Turin, 1998; Miklós and Meyer, 2005; Churbanov and Winters-Hilt, 2008), the entropy gradient of CRFs (Mann and McCallum, 2007), the gradient of entropy or risk of acyclic hypergraphs (Li and Eisner, 2009), Hessian-vector products of CRFs (Tsuboi et al.",
      "startOffset" : 333,
      "endOffset" : 354
    }, {
      "referenceID" : 42,
      "context" : "Examples of such complicated computation include “forward-only computation” of the Baum-Welch algorithm (Tan, 1993; Sivaprakasam and Shanmugan, 1995; Turin, 1998; Miklós and Meyer, 2005; Churbanov and Winters-Hilt, 2008), the entropy gradient of CRFs (Mann and McCallum, 2007), the gradient of entropy or risk of acyclic hypergraphs (Li and Eisner, 2009), Hessian-vector products of CRFs (Tsuboi et al., 2011), cross moments of factor graphs (Ilić et al.",
      "startOffset" : 388,
      "endOffset" : 409
    }, {
      "referenceID" : 17,
      "context" : ", 2011), cross moments of factor graphs (Ilić et al., 2012).",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "To be more precise, a subset of directed hypergraphs called B-graphs (Gallo et al., 1993).",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 34,
      "context" : "In contrast, defining forward variables on both nodes and arcs makes the induced posets fall into a tractable subset called chain-antichain-complete posets, edge-induced posets, N-free posets, or quasi-series-parallel posets, which are equivalent to each other (Möhring, 1989).",
      "startOffset" : 261,
      "endOffset" : 276
    }, {
      "referenceID" : 23,
      "context" : "5 illustrates an example acyclic factor graph borrowed from Kschischang et al. (2001). The directed arrows in the upper factor graph indicate an example generalized forward/backward (GFB) schedule.",
      "startOffset" : 60,
      "endOffset" : 86
    }, {
      "referenceID" : 22,
      "context" : "6 illustrates an example zero-suppressed binary decision diagram (ZDD, Minato, 1993; Knuth, 2009).",
      "startOffset" : 65,
      "endOffset" : 97
    }, {
      "referenceID" : 38,
      "context" : "This value is fundamental in the combination of logic-based formalisms with statistical inference (e.g., Poole, 1993; Sato and Kameya, 2001; De Raedt et al., 2007).",
      "startOffset" : 98,
      "endOffset" : 163
    }, {
      "referenceID" : 1,
      "context" : "Although we only show a computation graph for a ZDD as an example, we can construct computation graphs representing the Boolean functions expressed by a variety of decision diagrams, including (reduced and ordered) BDDs (Akers, 1978; Bryant, 1992), case-factor",
      "startOffset" : 220,
      "endOffset" : 247
    }, {
      "referenceID" : 3,
      "context" : "Although we only show a computation graph for a ZDD as an example, we can construct computation graphs representing the Boolean functions expressed by a variety of decision diagrams, including (reduced and ordered) BDDs (Akers, 1978; Bryant, 1992), case-factor",
      "startOffset" : 220,
      "endOffset" : 247
    }, {
      "referenceID" : 30,
      "context" : "diagrams (McAllester et al., 2008), and/or multi-valued decision diagrams (Mateescu et al.",
      "startOffset" : 9,
      "endOffset" : 34
    }, {
      "referenceID" : 29,
      "context" : ", 2008), and/or multi-valued decision diagrams (Mateescu et al., 2008), and so on.",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 46,
      "context" : "Note that BCR is isomorphic to the ordinary semiring of dual numbers (Yaglom, 1968).",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "It is also known as the (first-order) expectation semiring (Eisner, 2001).",
      "startOffset" : 59,
      "endOffset" : 73
    }, {
      "referenceID" : 13,
      "context" : "Note that the instance of forward algorithms on the parametrized computation graph ( G, op,BCR,∆ 1 k,x0 ◦ φ ) specified by the sextuple ( G, op,FC1,l, φ,BC1R,∆k,x0 ) , which is obtained by setting n = 1 in Example 8, is equivalent to the forward mode of AD (Griewank and Walther, 2008) on the computation graph (G, op, (R,+, ·, 0, 1) , φ).",
      "startOffset" : 257,
      "endOffset" : 285
    }, {
      "referenceID" : 26,
      "context" : "To illustrate our motivation, let us introduce computation with the second-order expectation semiring (Li and Eisner, 2009).",
      "startOffset" : 102,
      "endOffset" : 123
    }, {
      "referenceID" : 6,
      "context" : "However, the derivation of this semiring in Li and Eisner (2009) is ad-hoc and limited to the above form.",
      "startOffset" : 51,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "Note that an S-semimodule is an (S, S)-bisemimodule (Golan, 1999) in this setting.",
      "startOffset" : 52,
      "endOffset" : 65
    }, {
      "referenceID" : 40,
      "context" : "The general definition (Takahashi, 1982; Golan, 1999) does not assume cancellativeness of P .",
      "startOffset" : 23,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "The general definition (Takahashi, 1982; Golan, 1999) does not assume cancellativeness of P .",
      "startOffset" : 23,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : "Note that the resulting algebraic structure is isomorphic to the (first-order) expectation semiring (Eisner, 2001; Li and Eisner, 2009).",
      "startOffset" : 100,
      "endOffset" : 135
    }, {
      "referenceID" : 26,
      "context" : "Note that the resulting algebraic structure is isomorphic to the (first-order) expectation semiring (Eisner, 2001; Li and Eisner, 2009).",
      "startOffset" : 100,
      "endOffset" : 135
    }, {
      "referenceID" : 18,
      "context" : ", the ordinary forward-backward algorithm on trellises, the inside-outside algorithm on CYK derivations, the sum-product algorithm on acyclic factor graphs, the EM algorithm on decision diagrams (Ishihata et al., 2008), and so on).",
      "startOffset" : 195,
      "endOffset" : 218
    }, {
      "referenceID" : 26,
      "context" : "Winters-Hilt, 2008) and the forward algorithm with the (first-order) expectation semiring on hypergraphs (Li and Eisner, 2009).",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 19,
      "context" : "In the context of the forward-only computation of the BaumWelch algorithm, the analysis described above is completely consistent with the investigation in Khreich et al. (2010).",
      "startOffset" : 155,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "B), Goodman (1999, Sections 2 through 4) (provided that all derivations are acyclic, and the derivation forest is finite), Aji and McEliece (2000, Section III), Kschischang et al. (2001, Section 4), Klein and Manning (2004, Section 4), Miklós and Meyer (2005, Section titled “Methods and results”), Mann and McCallum (2007), Churbanov and Winters-Hilt (2008, Subsection titled “Linear memory Baum-Welch using a backward sweep with scaling”), Ishihata et al.",
      "startOffset" : 123,
      "endOffset" : 324
    }, {
      "referenceID" : 0,
      "context" : "B), Goodman (1999, Sections 2 through 4) (provided that all derivations are acyclic, and the derivation forest is finite), Aji and McEliece (2000, Section III), Kschischang et al. (2001, Section 4), Klein and Manning (2004, Section 4), Miklós and Meyer (2005, Section titled “Methods and results”), Mann and McCallum (2007), Churbanov and Winters-Hilt (2008, Subsection titled “Linear memory Baum-Welch using a backward sweep with scaling”), Ishihata et al. (2008), Griewank and Walther (2008, Section I.",
      "startOffset" : 123,
      "endOffset" : 465
    }, {
      "referenceID" : 0,
      "context" : "B), Goodman (1999, Sections 2 through 4) (provided that all derivations are acyclic, and the derivation forest is finite), Aji and McEliece (2000, Section III), Kschischang et al. (2001, Section 4), Klein and Manning (2004, Section 4), Miklós and Meyer (2005, Section titled “Methods and results”), Mann and McCallum (2007), Churbanov and Winters-Hilt (2008, Subsection titled “Linear memory Baum-Welch using a backward sweep with scaling”), Ishihata et al. (2008), Griewank and Walther (2008, Section I.3) (provided that involved operations are limited to the addition or multiplication of real numbers), Huang (2008, Section 5.1), Li and Eisner (2009, Sections 3 through 5), Azuma and Matsumoto (2010, Section 2), Kimmig et al. (2011, Section 4), Tsuboi et al. (2011, Section titled “Hessian-vector Products of CRFs”), Ilić et al. (2012). The formalization presented in this paper not only subsumes a wide range of existing algorithms but also extends them to an infinite number of their variants by using Framework 1 described in Section 2.",
      "startOffset" : 123,
      "endOffset" : 840
    }, {
      "referenceID" : 6,
      "context" : "For example, consider the discussion in Eisner (2016). He has pointed out that the inside-outside algorithm can be derived by back propagation.",
      "startOffset" : 40,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "For example, consider the discussion in Eisner (2016). He has pointed out that the inside-outside algorithm can be derived by back propagation. The derivation is automatic as the reverse mode of AD can be automatically derived from computation on the underlying computation graph. However, his discussion is limited to the context of parsing. Moreover, the probability distribution is assumed to be log-linear because his discussion is based on the fact that the partition function of a log-linear distribution is also its moment generating function. In contrast, our formalization appears to show the possibility that the discussion in Eisner (2016) can be generalized to much wider contexts.",
      "startOffset" : 40,
      "endOffset" : 651
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we propose an algebraic formalization of the two important classes of dynamic programming algorithms called forward and forward-backward algorithms. They are generalized extensively in this study so that a wide range of other existing algorithms is subsumed. Forward algorithms generalized in this study subsume the ordinary forward algorithm on trellises for sequence labeling, the inside algorithm on derivation forests for CYK parsing, a unidirectional message passing on acyclic factor graphs, the forward mode of automatic differentiation on computation graphs with addition and multiplication, and so on. In addition, we reveal algebraic structures underlying complicated computation with forward algorithms. By the aid of the revealed algebraic structures, we also propose a systematic framework to design complicated variants of forward algorithms. Forwardbackward algorithms generalized in this study subsume the ordinary forward-backward algorithm on trellises for sequence labeling, the inside-outside algorithm on derivation forests for CYK parsing, the sum-product algorithm on acyclic factor graphs, the reverse mode of automatic differentiation (a.k.a. back propagation) on computation graphs with addition and multiplication, and so on. We also propose an algebraic characterization of what can be computed by forward-backward algorithms and elucidate the relationship between forward and forward-backward algorithms.",
    "creator" : "TeX"
  }
}