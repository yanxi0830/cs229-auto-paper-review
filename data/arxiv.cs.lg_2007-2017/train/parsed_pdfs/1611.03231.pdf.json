{
  "name" : "1611.03231.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Policy Search with High-Dimensional Context Variables",
    "authors" : [ "Voot Tangkaratt", "Herke van Hoof", "Simone Parisi", "Gerhard Neumann", "Jan Peters", "Masashi Sugiyama" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 1.\n03 23\n1v 1\n[ st\nat .M\nL ]\n1 0\nN ov\n2 01\nDirect contextual policy search methods learn to improve policy parameters and simultaneously generalize these parameters to different context or task variables. However, learning from high-dimensional context variables, such as camera images, is still a prominent problem in many real-world tasks. A naive application of unsupervised dimensionality reduction methods to the context variables, such as principal component analysis, is insufficient as task-relevant input may be ignored. In this paper, we propose a contextual policy search method in the model-based relative entropy stochastic search framework with integrated dimensionality reduction. We learn a model of the reward that is locally quadratic in both the policy parameters and the context variables. Furthermore, we perform supervised linear dimensionality reduction on the context variables by nuclear norm regularization. The experimental results show that the proposed method outperforms naive dimensionality reduction via principal component analysis and a state-ofthe-art contextual policy search method."
    }, {
      "heading" : "Introduction",
      "text" : "An autonomous agent often requires different policies for solving tasks with different contexts. For instance, in a ball hitting task the robot has to adapt his controller according to the ball position, i.e., the context. Direct policy search approaches (Baxter and Bartlett, 2000; Rosenstein and Barto, 2001; Deisenroth, Neumann, and Peters, 2013) allow the agent to learn a separate policy for each context through trial and error. However, learning optimal policies for many large contexts, such as in the presence of continuous context variables, is impracticable. On the other hand, direct contextual policy search approaches (Kober, Oztop, and Peters, 2011; Neumann, 2011; da Silva, Konidaris, and Barto, 2012) represent\nthe contexts by real-valued vectors and are able to learn a context-dependent distribution over the policy parameters. Such a distribution can generalize across context values and therefore the agent is able to adapt to unseen contexts.\nYet, direct policy search methods (both contextual and plain) usually require a lot of evaluations of the objective and may converge prematurely. To alleviate these issues, Abdolmaleki et al. (2015) recently proposed a stochastic search framework called model-based relative entropy stochastic search (MORE). In this framework, the new search distribution can be computed efficiently in a closed form using a learned model of the objective function. MORE outperformed state-of-the-art methods in stochastic optimization problems and single-context policy search problems, but its application to contextual policy search has not been explored yet. One of the contributions in this paper is a novel contextual policy search method in the MORE framework.\nHowever, a naive extension of the original MORE would still suffer from high-dimensional contexts. Learning from high-dimensional variables, in fact, is still an important problem in statistics and machine learning (Bishop, 2006). Nowadays, high-dimensional data (e.g., camera images) can often be obtained quite easily, but obtaining informative low-dimensional variables (e.g., exact ball positions) is non-trivial and requires prior knowledge and/or human guidance.\nIn this paper, we propose to handle high-dimensional context variables by learning a low-rank representation of the objective function. We show that learning a lowrank representation corresponds to performing linear dimensionality reduction on the context variables. Since optimization with a rank constraint is generally NPhard, we minimize the nuclear norm (also called trace norm), which is a convex surrogate of the rank function (Recht, Fazel, and Parrilo, 2010). This minimization allows us to learn a low-rank representation in a fully supervised manner by just solving a convex optimization problem. We evaluate the proposed method on a syn-\nthetic task with known ground truth and on robotic ball hitting tasks based on camera images. The evaluation shows that the proposed method with nuclear norm minimization outperforms the methods that naively perform principal component analysis to reduce the dimensionality of context variables."
    }, {
      "heading" : "Contextual Policy Search",
      "text" : "In this section, we formulate the direct contextual policy search problem and briefly discuss existing methods."
    }, {
      "heading" : "Problem Formulation",
      "text" : "The direct contextual policy search is formulated as follows. An agent observes the context variable c ∈ Rdc and draws a parameter θ ∈ Rdθ from a search distribution π(θ|c). Subsequently, the agent executes a policy with the parameter θ and observes a scalar reward computed by a reward function R(θ, c). The goal is to find a search distribution π(θ|c) maximizing the expected reward\n∫∫ µ(c)π(θ|c)R(θ, c)dθdc, (1)\nwhere µ(c) denotes the context distribution. We assume that the reward function R(θ, c) itself is unknown, but the agent can always access the reward value."
    }, {
      "heading" : "Related Work",
      "text" : "In the basic direct contextual policy search framework, the agent iteratively collects samples {(θn, cn, R(θn, cn))}Nn=1 using a sampling distribution q(θ|c). Subsequently, it computes a new search distribution π(θ|c) such that the expected reward increases or is maximized. In literature, different approaches have been used to compute the new search distribution, e.g., evolutionary strategies (Hansen, Müller, and Koumoutsakos, 2003), expectation-maximization algorithms (Kober, Oztop, and Peters, 2011), or information theoretic approaches (Deisenroth, Neumann, and Peters, 2013).\nMost of the existing direct contextual policy search methods focus on tasks with low-dimensional context variables. To learn from high-dimensional context variables, usually the problem of learning a lowdimensional context representation is separated from the direct policy search by preprocessing the context space. However, unsupervised linear dimensionality reduction techniques are insufficient in problems where the latent representation contains distractor dimensions that do not influence the reward. A prominent example is principal component analysis (PCA) (Jolliffe, 1986), that does not take the supervisory signal into\naccount and therefore cannot discriminate between relevant and irrelevant latent dimensions. On the other hand, supervised linear dimensionality reduction techniques require a suitable response variable. However, defining such a variable can be subjective. Moreover, they often involve non-convex optimization and suffer from local optima (Fukumizu, Bach, and Jordan, 2009; Suzuki and Sugiyama, 2013).\nIn the last years, non-linear dimensionality reduction techniques based on deep learning have gained popularity (Bengio, 2009). For instance, Watter et al. (2015) proposed a generative deep network to learn lowdimensional representations of images in order to capture information about the system transition dynamics and allow optimal control problems to be solved in lowdimensional spaces. More recently, Silver et al. (2016) successfully trained a machine to play a high-level game of go using a deep convolutional network. Although their work does not directly focus on dimensionality reduction, the deep convolutional network is known to be able to extract meaningful representation of data. Thus, the effect of dimensionality reduction is achieved.\nHowever, deep learning approaches generally require large datasets that are difficult to obtain in real-world scenarios (e.g., robotics). Furthermore, they involve solving non-convex optimization, which can suffer from local optima.\nIn this paper, we tackle the issues raised above. First, the proposed approach integrates supervised linear dimensionality reduction on the context variables by learning a low-rank representation for the reward model. Second, the problem is formalized as a convex optimization problem and is therefore guaranteed to converge to a global optimum."
    }, {
      "heading" : "Contextual MORE",
      "text" : "The original MORE (Abdolmaleki et al., 2015) finds a search distribution (without context) that maximizes the expected reward while upper-bounding the KullbackLeibler (KL) divergence (Kullback and Leibler, 1951) and lower-bounding the entropy. The KL and the entropy are bounded to control the exploration-exploitation trade-off. The key insight of MORE is to learn a reward model to efficiently compute a new search distribution in closed form. Below, we propose our method called contextual model-based relative entropy stochastic search (C-MORE), which is a direct contextual policy search method in the MORE framework."
    }, {
      "heading" : "Learning the Search Distribution",
      "text" : "The goal of C-MORE is to find a search distribution π(θ|c) that maximizes the expected reward while upperbounding the expected KL divergence between π(θ|c)\nand q(θ|c), and lower-bounding the expected entropy of π(θ|c). Formally,\nmax π\n∫∫ µ(c)π(θ|c)R(θ, c)dθdc,\ns.t. ∫∫ µ(c)π(θ|c) log π(θ|c)\nq(θ|c) dθdc ≤ ǫ,\n− ∫∫\nµ(c)π(θ|c) log π(θ|c)dθdc ≥ β, ∫∫\nµ(c)π(θ|c)dθdc = 1,\nwhere the KL upper-bound ǫ and the entropy lowerbound β are parameters specified by the user. The former is fixed for the whole learning process. The latter is adaptively changed according to the percentage of the relative difference between the sampling policy’s expected entropy and the minimal entropy, as described by Abdolmaleki et al. (2015), i.e.,\nβ = γ(E[H(q)]−H0) +H0,\nwhere E[H(q)] = − ∫∫\nµ(c)q(θ|c) log q(θ|c)dθdc is the sampling policy’s expected entropy and H0 is the minimal entropy. In the experiments, we set γ = 0.99 and H0 = −150. The above optimization problem can be solved by the method of Lagrange multipliers1. The solution is given by π(θ|c) = q(θ|c) ηη+ω exp ( R(θ, c)\nη + ω\n) exp ( −η + ω − γ\nη + ω\n) ,\nwhere η > 0 and ω > 0 are the Lagrange multipliers obtained by minimizing the dual function\ng(η, ω) = ηǫ− ωβ + (η + ω)× ∫ µ(c)\nlog (∫ q(θ|c) ηη+ω exp ( R(θ, c)\nη + ω\n) dθ ) dc.\n(2)\nEvaluating the above integral is not trivial due to the integration over q(θ|c) ηη+ω , that cannot be approximated straightforwardly by sample averages. Below, we describe how to solve this issue and evaluate the dual function from data."
    }, {
      "heading" : "Dual Function Evaluation via the Quadratic Model",
      "text" : "We assume that the reward function R(θ, c) can be approximated by a quadratic model\nR̂(θ, c) = θ⊤Aθ + c⊤Bc+ 2θ⊤Dc\n+ θ⊤r1 + c ⊤r2 + r0, (3)\n1All derivations are given in the supplementary material.\nAlgorithm 1: C-MORE Input: Parameters ǫ and β, initial distribution\nπ(θ|c) 1 for k = 1, . . . ,K do 2 for n = 1, . . . , N do 3 Observe context cn ∼ µ(c) 4 Draw parameter θn ∼ π(θ|cn) 5 Execute task with θn and receive R(θn, cn)\n6 Learn the quadratic model R̂(θ, c) 7 Solve argminη>0,ω>0 g(η, ω) using Eq. (5) 8 Set new search distribution π(θ|c) using Eq. (6)\nwhere A ∈ Rdθ×dθ ,B ∈ Rdc×dc ,D ∈ Rdθ×dc , r1 ∈ R\ndθ , r2 ∈ Rdc , and r0 ∈ R are the model parameters. Matrices A and B are symmetric. We also assume the sampling distribution q(θ|c) to be Gaussian of the form\nq(θ|c) = N (θ|b+Kc,Q). (4)\nUnder these assumptions, the dual function in Eq. (2) can be expressed by\ng(η, ω) = ηǫ− ωβ + 1 2\n( f⊤F−1f − ηb⊤Q−1b\n+ (η + ω) log |2πF−1(η + ω)| − η log |2πQ| )\n+ ∫ µ(c) ( c⊤m+ 1\n2 c⊤Mc\n) dc, (5)\nwhere\nf = ηQ−1b+ r1,\nF = ηQ−1 − 2A, m = L⊤F−1f − ηK⊤Q−1b, M = L⊤F−1L− ηK⊤Q−1K, L = ηQ−1K + 2D.\nSince the context distribution µ(c) is unknown, we approximate the expectation in Eq. (5) by sample averages. The dual function can be minimized by standard non-linear optimization routines such as IPOPT (Wächter and Biegler, 2006). Finally, using Eq. (3) and Eq. (4) the new search distribution π(θ|c) is computed in closed form as\nπ(θ|c) = N ( θ|F−1f + F−1Lc,F−1(η + ω) ) . (6)\nTo ensure that the covarianceF−1(η+ω) is positive definite, the model parameterA is constrained to be negative definite. C-MORE is summarized in Algorithm 1."
    }, {
      "heading" : "Learning the Quadratic Model",
      "text" : "The performance of C-MORE depends on the accuracy of the quadratic model. For many problems, the reward function R(θ, c) is not quadratic and the quadratic model is not suitable to approximate the entire reward function. However, the reward function is often smooth and it can be locally approximated by a quadratic model. Therefore, we locally approximate the reward function by learning a new quadratic model for each policy update. The quadratic model can be learned by regression methods such as ridge regression2 (Bishop, 2006). However, ridge regression is prone to error when the context is high-dimensional. Below, we address this issue by firstly showing that performing linear dimensionality reduction on the context variables yields a low-rank matrix of parameters. Secondly, we propose a nuclear norm minimization approach to learn a low-rank matrix without explicitly performing dimensionality reduction."
    }, {
      "heading" : "Dimensionality Reduction and Low-Rank Representation",
      "text" : "Linear dimensionality reduction learns a low-rank matrix W and projects the data onto a lower dimensional subspace. Performing linear dimensionality reduction on the context variables yields the following quadratic model\nR̂(θ, c) = θ⊤Aθ + c⊤W⊤B̃Wc+ 2θ⊤D̃Wc\n+ θ⊤r1 + c ⊤W⊤r̃2 + r0, (7)\nwhere W ∈ Rdz×dc denotes a rank-dz matrix with dz < dc. The model parameters A, B̃, D̃, r1, r̃2 and r0 can be learned by ridge regression. However, the matrix B = W⊤B̃W is low-rank, i.e., rank(B) = dz < dc. Thus, performing linear dimensionality reduction on the contexts makes B low-rank. Note that the rank of D = D̃W depends on θ and is problem dependent. Hence, we do not consider the rank of D for dimensionality reduction.\nThere are several linear dimensionality reduction methods that can be applied to learn W . Principal component analysis (PCA) (Jolliffe, 1986) is a common method used in statistics and machine learning. However, being unsupervised, it does not take the regression targets into account, i.e., the reward. Alternative supervised techniques, such as KDR (Fukumizu, Bach, and Jordan, 2009) and LSDR (Suzuki and Sugiyama, 2013), do not take the regression model, i.e., the quadratic\n2After learning the parameters, A is enforced to be negative definite by truncating its positive eigenvalues. Subsequently, we re-learn the remainder parameters. An alternative approach is projected gradient descend, but it is more computationally demanding and requires step size tuning.\nmodel, into account. On the contrary, in projection regression (Friedman and Stuetzle, 1981; Vijayakumar and Schaal, 2000) the model parameters and the projection matrix are learned simultaneously. However, applying this approach to the model in Eq. (7) requires alternately optimizing for the model parameters and the projection matrix and is computationally expensive.\nIn the original MORE, Bayesian dimensionality reduction (Gönen, 2013) is applied to perform linear supervised dimensionality reduction on θ, i.e., the algorithm considers a projectionWθ. The matrix W is sampled from a prior distribution and the algorithm learns the model parameters using weighted average over the sampled W . However, for high-dimensionalW , this approach requires an impractically large amount of samples W to obtain an accurate model, leading to computationally expensive updates."
    }, {
      "heading" : "Learning a Low-Rank Matrix with Nuclear Norm Regularization",
      "text" : "The quadratic model in Eq. (3) can be re-written as\nR̂(x) = x⊤Hx,\nwhere the input vectorx and the parameter matrix H are defined as\nx =\n  θ\nc\n1\n  , H =  \nA D 0.5r1 D⊤ B 0.5r2 0.5r⊤1 0.5r ⊤ 2 r0\n  .\nNote that H is symmetric since both A and B are symmetric. As discussed in the previous section, we desire B to be low-rank. Unlike Eq. (7), we do not consider dimensionality reduction for the linear terms in c, i.e., 2θ⊤Dc and c⊤r2. Instead, we learn H by solving the following convex optimization problem\nmin H\n[J (H) + λ∗‖B‖∗] ,\ns.t. A is negative definite, (8)\nwhere J (H) denotes the differentiable part\nJ (H) = 1 2N\nN∑\nn=1\n( x⊤nHxn −R(θn, cn) )2 + λ\n2 ‖H‖2F,\nwhere λ > 0 and λ∗ > 0 are regularization parameters. The Frobenius norm ‖ · ‖F is defined as ‖H‖F =√\ntr(HH⊤). The nuclear norm of a matrix ‖ · ‖∗ is defined as the ℓ1-norm of its singular values . This optimization problem can be explained as follows. The term J (H) consists of the mean squared error and the ℓ2-regularization term. Thus, minimizing J (H) corresponds to ridge regression. Minimizing the nuclear norm\n‖B‖∗ shrinks the singular values of B. Thus, the solution tends to have sparse singular values and to be lowrank. The negative definite constraint further ensures that the covariance matrix in Eq. (6) is positive definite.\nThe convexity of this optimization problem can be verified by checking the following conditions. First, the convexity of the mean squared error can be proven following Boyd and Vandenberghe, 2004, page 74. Let g(t) = Ĵ (Z + tV ) be the mean squared error and Z and V are symmetric matrices. Then we have that ∇2g(t) = 1\nN\n∑ (x⊤nV xn)\n2 ≥ 0. Thus, the mean squared error is convex. Since the Frobenius norm is convex, J (H) is convex as well. Second, a set of negative definite matrices is convex since y⊤(aX + (1 − a)Y )y < 0 for any negative definite matrices X and Y , 0 ≤ a ≤ 1, and any vector y (Boyd and Vandenberghe, 2004). Third, the nuclear norm is a convex function (Recht, Fazel, and Parrilo, 2010). Note that, since the gradient ∇J (H) is symmetric, H is guaranteed to be symmetric as well given that the initial solution is also symmetric.\nIt is also possible to enforce the matrix H (rather than B) to be low-rank, implying that both θ and c can be projected onto a common low-dimensional subspace. However, this is often not the case, and regularizing by the nuclear norm of H did not perform well in our experiments. We may also directly constrain rank(B) = dz in Eq. (8) instead of performing nuclear norm regularization. However, minimization problems with rank constraints are NP-hard. On the contrary, the nuclear norm is the convex envelop of the rank function and can be optimized more efficiently (Recht, Fazel, and Parrilo, 2010). For this reason, the nuclear norm has been a popular surrogate to a low-rank constraint in many applications, such as matrix completion (Candès and Tao, 2010) and multi-task learning (Pong et al., 2010).\nSince the optimization problem in Eq. (8) is convex, any convex optimization method can be used (Boyd and Vandenberghe, 2004). For our experiments, we use the accelerated proximal gradient descend (APG) (Toh and Yun, 2009). The pseudocode of our implementation of APG for solving Eq. (8) is given in the supplementary material. Note that APG requires computing the SVD of the matrix B. Since computing the exact SVD of a high-dimensional matrix can be computationally expensive, we approximate it by randomized SVD (Halko, Martinsson, and Tropp, 2011)."
    }, {
      "heading" : "Experiments",
      "text" : "We evaluate the proposed method on three problems. We start by studying C-MORE behavior in a scenario where we know the true reward model and the true lowdimensional context. Subsequently, we focus our attention on two simulated robotic ball hitting tasks. In the\nfirst task, a toy 2-DoF planar robot arm has to hit a ball placed on a plane. In the second task, a simulated 6-DoF robot arm has to hit a ball placed in a three-dimensional space. In both cases, the robots accomplish their task by using raw camera images as context variables. However, in the latter case we have limited data and therefore sample efficiency is of primary importance.\nThe evaluation is performed on three different versions of C-MORE, according to the model learning approach: using only ridge regression (C-MORE Ridge), aided by a low-dimensional context variables learned by PCA (C-MORE Ridge+PCA) and aided by nuclear norm regularization (C-MORE Nuc. Norm). We also use C-REPS (Deisenroth, Neumann, and Peters, 2013) with PCA as baseline. For the ball hitting tasks, we also tried to preprocess the context space with an autoencoder. However, the learned representation performed poorly, possibly due to the limited amount of data at our disposal, and therefore this method is not reported.\nFor each case study, first, the experiments are presented and then the results are reported and discussed. For additional details, we refer to the supplementary material."
    }, {
      "heading" : "Quadratic Cost Function Optimization",
      "text" : "In the first experiment, we want to study the performance of the algorithms in a setup where we are able to analytically compute both the reward and the true lowdimensional context. To this aim, we define the following problem\nR(θ, c) = −(||θ − T 1c̃||2)2, c̃ = ĨT−12 c, T 1 ∈ Rdθ×dc̃, T 2 ∈ Rdc×dc , Ĩ ∈ Rdc̃×dc , dc̃ < dc,\nwhere I is the identity matrix, Ĩ is a rectangular matrix with ones in its main diagonal and zeros otherwise, c̃ is the true low-dimensional context, and T 1 is to match the dimension of the true context and the parameter θ in order to compute the reward. This setup is particularly interesting because only a subset of the observed context influences the reward. First, the observed context c is linearly transformed by T−12 . Subsequently, thanks to the matrix Ĩ , only the first dc̃ elements are kept to compose the true context, while the remainder is treated as noise. Finally, the reward is computed by linearly transforming the true context by T 1.\nSetup. We set dc̃ = 3, dθ = 10, dc = 25, while the elements of T 1,T 2 are chosen uniformly randomly in [0, 1]. The sampling Gaussian distribution is initialized with random mean and covariance Q = 10, 000I. For learning, we collect 35 new samples and keeps track of the samples collected during the last 20 iterations to stabilize the policy update. The evaluation is performed\nat each iteration over 1,000 contexts. Each context element is drawn from a uniform random distribution in [−10, 10]. Since we can generate a large amount of data in this setting, we pre-train PCA using 10,000 random context samples and fixed the dimensionality to dz = 20 (chosen by cross-validation). The learning is performed for a maximum of 100 iterations. If the KL divergence is lower than 0.1, then the learning is considered to be converged and the policy is not updated anymore.\nResults. As shown in Figure 1 , C-MORE Nuc. Norm clearly outperforms all the competitors, learning an almost optimal policy and being the only one to converge within the maximum number of iterations. It is also the only algorithm correctly learning the true context dimensionality, as nuclear norm successfully regularizes B to have rank three. On the contrary, PCA does not help C-MORE much and yields only slightly better results than plain ridge regression. PCA cannot in fact determine task-relevant dimensions as non-relevant dimensions have equally-high variance."
    }, {
      "heading" : "Ball Hitting with a 2-DoF Robot Arm",
      "text" : "Figure 5: 2-DoF hitting task. The context observed by the robot (blue and red lines) consists of a virtual green ball and the background image.\nIn this task, a simulated planar robot arm (shown aside) has to hit a green virtual ball placed on RGB camera images of size 32 × 24. The context is defined by the observed pixels, for a total of 2304 context variables. The ball is randomly and uniformly placed in the robot workspace. Noise drawn\nfrom a uniform random distribution in [−30, 30]\nis also added to the context, to simulate different light conditions. The robot controls the joint accelerations at each time step by a linear-in-parameter controller with Gaussian basis functions, for a total of 32 parameters θ to be learned. The reward R(θ, c) is the negative cumulative joint accelerations plus the negative distance between the end-effector and the ball at the final time step.\nSetup. For learning, the agent collects 50 samples at each iteration and keeps samples from the last four previous iterations. The evaluation is performed at each iteration over 500 contexts. Pixel values are normalized in [−1, 1]. The sampling Gaussian distribution is initialized with random mean and identity covariance. For both C-MORE Nuc. Norm and C-MORE PCA, we perform 5-fold cross-validation every 100 policy updates to choose the values of λ∗ and dz, respectively, based on the mean squared error between the collected returns and the model-predicted ones. For C-REPS PCA, we tried different values of dz ∈ {10, 20, 30, 40} and selected dz = 10 which gave the best result.\nResults. Figure 2 shows the averaged reward against the number of iterations. Once again, C-MORE aided by nuclear norm regularization performs the best, achieving the highest average reward. At the 1000th iteration, the learned controller hits the ball with 76% accuracy. The rank of its learned matrix B is approximately 31, which shows that the algorithm successfully learns a low-rank model representation. On the contrary, preprocessing the context space through PCA still helps C-MORE (the rank of its learned B is approximately 25), but yields poor results for C-REPS, which suffers of premature\nconvergence."
    }, {
      "heading" : "Ball Hitting with a 6-DoF Robot Arm",
      "text" : "Similarly to the previous task, here a 6-DoF robotic arm has to hit a ball placed on a three-dimensional space, as shown in Figure 3. The context is once again defined by the vectorized pixels of RGB images of size 32 × 24, for a total of 2304 context variables. Note that Figure 3a shows an image before we rescale it to size 32 × 24. However, unlike the 2-DoF task, the ball is directly recorded by a real camera placed near the physical robot, and it is not virtually generated on the images. Furthermore, the robot is controlled by dynamic motor primitives (Ijspeert, Nakanishi, and Schaal, 2002) (DMPs), which are non-linear dynamical systems. We use one DMP per joint, with five basis functions per DMP. We also learn the goal attractor of the DMPs, for a total of 36 parameters θ to be learned. The reward R(θ, c) is computed as the negative cumulative joint accelerations and minimum distance between the endeffector and the ball as well.\nSetup. The image dataset is collected by taking pictures with the ball placed at 50 different positions. To increase the number of data, we add a uniform random noise in [−30, 30] to the context to simulate different light conditions. Therefore, although some samples determine the same ball position, they are considered different due to the added noise. The search distribution is initialized by imitation learning using 50 demonstration samples. For learning, the agent collects 50 samples at each iteration and always keeps samples from the last four previous iterations.\nResults. We only evaluate C-MORE with nuclear norm and PCA since they performed well in the previous evaluation. Figure 4 shows that nuclear norm again out-\nperforms PCA. At the 500th iteration, the learned controller hits the ball with 80% accuracy. Considering that the robot is not able to hit the ball in some contexts and can achieve a maximum accuracy of 90%, this accuracy is impressive for the task. The averaged rank of matrix B learned by the nuclear norm approach is approximately 25, which shows that minimizing the nuclear norm successfully learns a low-rank matrix. For PCA, the averaged rank of B is approximately 30."
    }, {
      "heading" : "Conclusion",
      "text" : "Learning with high-dimensional context variables is a challenging and prominent problem in machine learning. In this paper, we proposed C-MORE, a novel contextual policy search method with integrated dimensionality reduction. C-MORE learns a reward model that is locally quadratic in the policy parameters and the context variables. By enforcing the model representation to be low-rank, we perform supervised linear dimensionality reduction. Unlike existing techniques relying on nonconvex formulations, the nuclear norm allows us to learn the low-rank representation by solving a convex optimization problem, thus guaranteeing convergence to a global optimum. The main disadvantage of the proposed method is that it demands more computation time due to the nuclear norm regularization. Although we did not encounter severe problems in our experiments, for very large dimensional tasks this issue can be mitigated by using more efficient techniques, such as active subspace selection (Hsieh and Olsen, 2014).\nIn this paper, we only focused on linear dimensionality reduction techniques. Recently, non-linear techniques based on deep network has been showing impressive performance (Bengio, 2009; Watter et al., 2015). In future work, we will incorporate deep network into C-MORE, e.g., by using a deep convolutional network to represent the reward model."
    }, {
      "heading" : "Derivations of C-MORE",
      "text" : "In this section, we derive C-MORE in details. C-MORE solves\nmax π\n∫∫ µ(c)π(θ|c)R(θ, c)dθdc,\ns.t. ∫∫ µ(c)π(θ|c) log π(θ|c)\nq(θ|c) dθdc ≤ ǫ,\n− ∫∫\nµ(c)π(θ|c) log π(θ|c)dθdc ≥ β, ∫∫\nµ(c)π(θ|c)dθdc = 1,\nby the method of Lagrange multipliers. Firstly, we write the Lagrangian L with the Lagrange multipliers η > 0, ω > 0, and γ, which correspond to the first, second, and third constraints, respectively L(π, η, ω, γ) = ∫∫ µ(c)π(θ|c)R(θ, c)dθdc + η ( ǫ− ∫∫ µ(c)π(θ|c) log π(θ|c)\nq(θ|c) dθdc )\n+ ω ( − ∫∫ µ(c)π(θ|c) log π(θ|c)dθdc− β ) + γ (∫∫ µ(c)π(θ|c)dθdc− 1 ) .\nThen, we maximize the Lagrangian L(π, η, ω, γ) w.r.t. the primal variable π. The derivative of the Lagrangian w.r.t. π is\n∂πL(π, η, ω, γ) = ∫∫ µ(c) ( R(θ, c)− (η + ω) logπ(θ|c) + η log q(θ|c) ) dθdc− (η + ω − γ).\nBy setting this derivative to zero, we have\n0 = ∫∫ µ(c) ( R(θ, c)− (η + ω) log π(θ|c) + η log q(θ|c) ) dθdc− (η + ω − γ)\n= R(θ, c)− (η + ω) log π(θ|c) + η log q(θ|c)− (η + ω − γ). This gives us\nlog π(θ|c) = R(θ, c) η + ω + η η + ω log q(θ|c)− η + ω − γ η + ω ,\nπ(θ|c) = q(θ|c) η η+ω exp\n( R(θ, c)\nη + ω\n) exp ( −η + ω − γ\nη + ω\n) . (9)\nThe last exponential term in Eq. (9) is the normalization constant for the search distribution π(θ|c) since it does not depend on θ or c. Thus, we have\nexp ( η + ω − γ η + ω ) = ∫ q(θ|c) ηη+ω exp ( R(θ, c) η + ω ) dθ,\nη + ω − γ = (η + ω) log (∫ q(θ|c) η η+ω exp ( R(θ, c)\nη + ω\n) dθ ) .\n(The minus sign in the exponent in Eq. (9) becomes the inverse operator and cancels out). This normalization term will be used to derive the dual function. Next, we substitute the term log π(θ|c) back to the Lagrangian\nL(π∗, η, ω, γ) = ∫∫ µ(c)π(θ|c)R(θ, c)dθdc\n− η (∫∫ µ(c)π(θ|c) [ R(θ, c)\nη + ω +\nη η + ω log q(θ|c)− η + ω − γ η + ω\n] dθdc )\n− ω (∫∫ µ(c)π(θ|c) [ R(θ, c)\nη + ω +\nη η + ω log q(θ|c)− η + ω − γ η + ω\n] dθdc )\n+ η ∫∫ µ(c)π(θ|c) log q(θ|c)dθdc+ γ (∫∫ µ(c)π(θ|c)dθdc− 1 ) + ηǫ− ωβ.\nMost terms cancel out and we only have\nL(π∗, η, ω, γ) = ηǫ− ωβ − γ + ∫ µ(c) (η + ω) dc\n= ηǫ− ωβ + ∫ µ(c) (η + ω − γ) dc = ηǫ− ωβ + (η + ω) ∫ µ(c) log (∫ q(θ|c) ηη+ω exp ( R(θ, c)\nη + ω\n) dθ ) dc\n= g(η, ω).\nThe Lagrange multipliers η > 0 and ω > 0 are obtained by minimizing the dual function g(η, ω). Then, the search distribution in Eq. (9) can be computed using these Lagrange multipliers."
    }, {
      "heading" : "Evaluating the Dual Function",
      "text" : "Here, we show how to compute the new search distribution in closed form. Recall that our quadratic model is\nR̂(θ, c) = θ⊤Aθ + c⊤Bc+ 2θ⊤Dc+ θ⊤r1 + c ⊤r2 + r0,\nwith symmetric A and B. Also recall that the sampling distribution q(θ|c) is Gaussian\nq(θ|c) = N (θ|b+Kc,Q)\n= 1\n|2πQ| 12 exp ( −1 2 [θ − (b+Kc)]⊤Q−1[θ − (b+Kc)] ) .\nThe quadratic model and the Gaussian distribution allow us to compute the dual function from data as follows. Firstly, we consider the term\nq(θ|c) η η+ω exp\n( R(θ, c)\nη + ω\n) .\nUsing the Gaussian distribution q(θ|c) and replacing R(θ, c) with R̂(θ, c) yield q(θ|c) ηη+ω exp ( R̂(θ, c)\nη + ω\n)\n= 1\n|2πQ| η 2(η+ω)\nexp ( − η 2(η + ω) [θ − (b+Kc)]⊤Q−1[θ − (b+Kc)] )\n× exp ( θ⊤Aθ + c⊤Bc+ 2θ⊤Dc+ θ⊤r1 + c\n⊤r2 + r0 η + ω\n)\n= 1\n|2πQ| η 2(η+ω)\nexp ( − 1\nη + ω\n( − η\n2 [θ − (b+Kc)]⊤Q−1[θ − (b+Kc)]\n+ θ⊤Aθ + c⊤Bc + 2θ⊤Dc + θ⊤r1 + c ⊤r2 + r0\n))\n= 1\n|2πQ| η 2(η+ω)\nexp\n( 1\n2(η + ω)\n( − ηθ⊤Q−1θ + 2ηθ⊤Q−1b+ 2ηθ⊤Q−1Kc− ηb⊤Q−1b\n− 2ηb⊤Q−1Kc− ηc⊤K⊤Q−1Kc+ 2θ⊤Aθ + 2c⊤Bc+ 4θ⊤Dc+ 2θ⊤r1 + 2c⊤r2 + 2r0 ))\n= 1\n|2πQ| η 2(η+ω)\nexp\n( 1\n2(η + ω)\n( θ⊤ ( −ηQ−1 + 2A ) θ + θ⊤ ( 2ηQ−1b+ 2r1 )\n+ θ⊤ ( 2ηQ−1K + 4D ) c +G\n))\n= 1\n|2πQ| η 2(η+ω)\nexp\n( 1\n2(η + ω)\n( − (θ⊤Fθ − 2θ⊤f − 2θ⊤Lc) )) exp ( G\n2(η + ω)\n) ,\nwhere\nF = ηQ−1 − 2A, f = ηQ−1b+ r1,\nL = ηQ−1K + 2D,\nG = −ηb⊤Q−1b− 2ηb⊤Q−1Kc− ηc⊤K⊤Q−1Kc+ 2c⊤Bc + 2c⊤r2 + 2r0. Next, we “complete the square” by considering the following quadratic term\n[θ − (F−1f + F−1Lc)]⊤F [θ − (F−1f + F−1Lc)] = θ⊤Fθ − 2θ⊤f − 2θ⊤Lc+ (F−1f + F−1Lc)⊤F (F−1f + F−1Lc) = ( θ⊤Fθ − 2θ⊤f − 2θ⊤Lc ) + f⊤F−1f + 2f⊤F−1Lc+ c⊤L⊤F−1Lc.\nTherefore, we have\nq(θ|c) ηη+ω exp ( R(θ, c)\nη + ω\n)\n= 1\n|2πQ| η 2(η+ω)\nexp\n( 1\n2(η + ω)\n( − ([θ − (F−1f + F−1Lc)]⊤F [θ − (F−1f + F−1Lc)]\n+ f⊤F−1f + 2f⊤F−1Lc+ c⊤L⊤F−1Lc) )) exp ( G\n2(η + ω)\n) . (10)\nUsing the above result, the inner integral term in the dual function is\n∫ q(θ|c) η η+ω exp ( R̂(θ, c)\nη + ω\n) dθ\n= |2πF−1(η + ω)| 12\n|2πQ| η 2(η+ω)\nexp\n( 1\n2(η + ω)\n( f⊤F−1f + 2f⊤F−1Lc + c⊤L⊤F−1Lc )) exp ( G\n2(η + ω)\n) ,\nwhere the squared exponential term in Eq. (10) depending on θ is “integrated out” and becomes the inverted normalization term |2πF−1(η + ω)| 12 . Plugging this term back to the dual function yields\ng(η, ω) = ηǫ− ωβ + 1 2\n( f⊤F−1f − ηb⊤Q−1b+ (η + ω) log |2πF−1(η + ω)| − η log |2πQ| )\n+ ∫ µ(c) ( f⊤F−1L− ηb⊤Q−1K ) cdc + 1\n2\n∫ µ(c)c⊤ ( L⊤F−1L− ηK⊤Q−1K ) cdc\n= ηǫ− ωβ + 1 2\n( f⊤F−1f − ηb⊤Q−1b+ (η + ω) log |2πF−1(η + ω)| − η log |2πQ| )\n+ ∫ µ(c) ( c⊤m+ 1\n2 c⊤Mc\n) dc,\nwhere\nm = L⊤F−1f − ηK⊤Q−1b, M = L⊤F−1L− ηK⊤Q−1K.\nThe expectation over µ(c) can be approximated by the context samples. The term 2c⊤Bc + 2c⊤r2 + 2r0 in G does not appear in the dual function since this is constant w.r.t. η and ω. Similarly to the dual function, by using Eq. (9) and Eq. (10) we compute the new search distribution in closed form as\nπ(θ|c) ∝ q(θ|c) ηη+ω exp ( R̂(θ, c)\nη + ω\n)\n∝ exp (\n1\n2(η + ω)\n( − ([θ − (F−1f + F−1Lc)]⊤F [θ − (F−1f + F−1Lc)]\n))\n= N ( θ|F−1f + F−1Lc,F−1(η + ω) ) .\nProof of Convexity of Ĵ (H)\nTo show that Ĵ (H) = 1 2N ∑N n=1 ( x⊤nHxn −R(θn, cn) )2 is a convex function, we follow the proof in Boyd and Vandenberghe 2004, page 74. Let g(t) = Ĵ (Z+tV ) with symmetric matrices Z and V and scalar t. Then, we can verify the convexity of Ĵ through g. Through simple calculation,\nwe have that\ng(t) = 1\n2N\nN∑\nn=1\n( x⊤n (Z + tV )xn − R(θn, cn) )2\n= 1\n2N\nN∑\nn=1\n( (x⊤nZxn) 2 + 2t(x⊤nZxn)(x ⊤ nV xn)− 2(x⊤nZxn)R(xn)\n+ t2(x⊤nV xn) 2 − 2t(x⊤nV xn)R(xn) +R(xn)2\n) .\nThe first and second derivatives of g(t) are\n∇g(t) = 1 N\nN∑\nn=1\n( (x⊤Zx)(x⊤V x) + t(x⊤nV xn) 2 − (x⊤nV xn)R(xn) ) ,\n∇2g(t) = 1 N\nN∑\nn=1\n(x⊤nV xn) 2 ≥ 0.\nSince the second derivative is non-negative, the function Ĵ (H) is convex."
    }, {
      "heading" : "Experiments Details",
      "text" : "Here, we provide all additional details regarding the experiments that are not mentioned in the paper."
    }, {
      "heading" : "Quadratic Cost Function Optimization",
      "text" : "The sampling Gaussian distribution mean is initialized uniformly randomly in [0, 5], while the initial covariance is Q = 10, 000I. For nuclear norm, the regularization parameters are set to λ∗ = 0.00002 and λ = 0.00001. For PCA, the dimensionality candidates are dz ∈ {3, 6, 10, 20}. Figure 6 shows the performance with different dimensionality. The regularization parameter for the ℓ2-regularization is set at λ = 0.00001. The step size for accelerated proximal gradient (APG) is fixed at 0.001. We also normalize the gradient such that the Frobenius norm is 1. The maximum iteration of APG is set at K = 300. C-REPS and C-MORE KL divergence is set to 0.9."
    }, {
      "heading" : "Ball Hitting with a 2-DoF Robot Arm",
      "text" : "We use a linear-in-parameter controller with Gaussian basis functions to control the joint accelerations q̈ of the robot\nq̈ = [ θ⊤1 ϕ(q), θ ⊤ 2 ϕ(q) ]⊤ ,\nwhere θ = [ θ⊤1 , θ ⊤ 2 ]⊤ is the policy parameters vector, q ∈ R2 is the joint angles vector, and ϕ is the Gaussian basis functions vector with 16 Gaussian centers placed at {0, π 2 , π, 3π 2 }×{0, π 2 , π, 3π 2 } for both joints. The number of total parameter θ is 32. The reward R(θ, c) is computed as the negative cumulative joint accelerations plus the negative distance between the end-effector and the ball at\nthe final time step\nR(θ, c) = −0.05 40∑\nt=1\n|q̈t|+ 10 exp ( −(ballx − effx)\n2 − (bally − effy)2 50\n) ,\nwhere q̈t denotes the joint accelerations at time step t, ballx and effx denotes the x coordinate of the ball and end-effector at the final time step, respectively. Variables bally and effy are also also defined similarly. We use the following transition dynamics to govern the joint angles\nq̇ ← q̇ +∆q̈, q ← q +∆q̇,\nwhere ∆ = 0.1. Each arm has length 7.5. The robot is initialized such that the end-effector is at the bottom position.\nThe sampling Gaussian distribution N (θ|b + Kc,Q) is initialized by bij ∼ N (0, 1), Kij ∼ N (0, 0.012), for each entry (i, j) and Q = I. For nuclear norm, the regularization parameter candidates are λ∗ ∈ {1× 10−7, 5× 10−7, 1× 10−6, 2× 10−6}. For PCA, the dimensionlaity candidates are d z ∈ {10, 20, 30, 40}. The regularization parameter for the ℓ2-regularization is set at λ = 0.0001. The step size for accelerated proximal gradient (APG) is fixed at 0.001. We also normalize the gradient such that the Frobenius norm is 1. The maximum iteration of APG is initialize at K = 500 and it is reduced by 2 after each update to reduce the computation time until a minimum of K = 300. K is reset to 500 when cross-validation is performed.\nFigure 7 shows the performance of C-REPS with PCA on different dimensionality d z and different KL upper-bound ǫ. The best result of C-REPS is the one reported in the main paper for the comparison with C-MORE. Figure 8 shows the rank of B against update iteration averaged over ten trials. C-MORE with nuclear norm is quite unstable early on. However, the rank stabilizes after some iterations and the rank converges to 31."
    }, {
      "heading" : "Ball Hitting with a 6-DoF Robot Arm",
      "text" : "In this task, the reward function is defined as\nR(θ, c) = { −0.05∑ |qt|+ 10 ∗ exp(−distance2), if distance > 0.1 cm. −0.05∑qt + 10 ∗ exp(−distance2) + 20, otherwise,\nwhere distance denotes the minimum distance between the ball and the end-effector along the trajectory. APG and PCA setup is the same as in the 2-DoF robot experiment. Figure 9 and Figure 10 show the average reward and hit accuracy averaged over 50 contexts on three individual trial, respectively. On all trials, the nuclear norm performs better than PCA and consistently achieves 80% hit accuracy.\nAlgorithm 2: APG for solving the nuclear norm minimization problem Input: Parameters λ and λ∗, gradient step size τ , maximum number of iteration K , initial solution H0\n1 Initialize H−1 = H0 and t−1 = t0 = 1 2 for k = 1, . . . ,K do 3 Set intermediate point\nY k = Hk + tk\n−1 − 1\ntk (Hk −Hk−1)\n4 Do gradient descent using the differentiable term\nY + = Y k + τ∇J (Y k),\nwhere\nY + =\n \nA+ D+ 0.5r1+ D⊤+ B+ 0.5r2+\n0.5r⊤1+ 0.5r ⊤ 2+ r0+\n \n5 Shrink singular values of B+\nB∗ = U max(Σ− λ∗I,0)V ⊤,\nwhere B+ = UΣV ⊤ is the SVD of B+\n6 Truncate positive eigenvalues of A+\nA∗ = P min(Λ,0)P ⊤,\nwhere A+ = PΛP ⊤ is the eigendecomposition of A+\n7 Update solution\nHk+1 =\n \nA∗ D+ 0.5r1+ D⊤+ B∗ 0.5r2+\n0.5r⊤1+ 0.5r ⊤ 2+ r0+\n \n8 Update parameter tk+1 = 1+\n√ 1+4(tk)2\n2\n9 if stopping criterion is met then 10 return"
    } ],
    "references" : [ {
      "title" : "Model-based relative entropy stochastic search",
      "author" : [ "A. Abdolmaleki", "R. Lioutikov", "J. Peters", "N. Lau", "L.P. Reis", "G. Neumann" ],
      "venue" : "Advances in Neural Information Processing Systems 28, 3537–3545.",
      "citeRegEx" : "Abdolmaleki et al\\.,? 2015",
      "shortCiteRegEx" : "Abdolmaleki et al\\.",
      "year" : 2015
    }, {
      "title" : "Reinforcement learning in POMDP’s via direct gradient ascent",
      "author" : [ "J. Baxter", "P.L. Bartlett" ],
      "venue" : "The 17th International Conference on Machine Learning, 41–48.",
      "citeRegEx" : "Baxter and Bartlett,? 2000",
      "shortCiteRegEx" : "Baxter and Bartlett",
      "year" : 2000
    }, {
      "title" : "Learning deep architectures for AI",
      "author" : [ "Y. Bengio" ],
      "venue" : "Foundations and Trends in Machine Learning 2(1):1– 127.",
      "citeRegEx" : "Bengio,? 2009",
      "shortCiteRegEx" : "Bengio",
      "year" : 2009
    }, {
      "title" : "Pattern Recognition and Machine Learning (Information Science and Statistics)",
      "author" : [ "C.M. Bishop" ],
      "venue" : "Secaucus, NJ, USA: Springer-Verlag New York, Inc.",
      "citeRegEx" : "Bishop,? 2006",
      "shortCiteRegEx" : "Bishop",
      "year" : 2006
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : "New York, NY, USA: Cambridge University Press.",
      "citeRegEx" : "Boyd and Vandenberghe,? 2004",
      "shortCiteRegEx" : "Boyd and Vandenberghe",
      "year" : 2004
    }, {
      "title" : "The power of convex relaxation: near-optimal matrix completion",
      "author" : [ "E.J. Candès", "T. Tao" ],
      "venue" : "IEEE Transactions on Information Theory 56(5):2053–2080.",
      "citeRegEx" : "Candès and Tao,? 2010",
      "shortCiteRegEx" : "Candès and Tao",
      "year" : 2010
    }, {
      "title" : "Learning parameterized skills",
      "author" : [ "B.C. da Silva", "G. Konidaris", "A.G. Barto" ],
      "venue" : "In The 29th International Conference on Machine Learning",
      "citeRegEx" : "Silva et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Silva et al\\.",
      "year" : 2012
    }, {
      "title" : "A survey on policy search for robotics",
      "author" : [ "M.P. Deisenroth", "G. Neumann", "J. Peters" ],
      "venue" : "Foundations and Trends in Robotics 2(1-2):1–142.",
      "citeRegEx" : "Deisenroth et al\\.,? 2013",
      "shortCiteRegEx" : "Deisenroth et al\\.",
      "year" : 2013
    }, {
      "title" : "Projection pursuit regression",
      "author" : [ "J.H. Friedman", "W. Stuetzle" ],
      "venue" : "Journal of the American Statistical Association 76(376):817–823.",
      "citeRegEx" : "Friedman and Stuetzle,? 1981",
      "shortCiteRegEx" : "Friedman and Stuetzle",
      "year" : 1981
    }, {
      "title" : "Kernel dimension reduction in regression",
      "author" : [ "K. Fukumizu", "F.R. Bach", "M.I. Jordan" ],
      "venue" : "The Annals of Statistics 37(4):1871–1905.",
      "citeRegEx" : "Fukumizu et al\\.,? 2009",
      "shortCiteRegEx" : "Fukumizu et al\\.",
      "year" : 2009
    }, {
      "title" : "Bayesian supervised dimensionality reduction",
      "author" : [ "M. Gönen" ],
      "venue" : "IEEE Transanscation on Cybernetics 43(6):2179–2189.",
      "citeRegEx" : "Gönen,? 2013",
      "shortCiteRegEx" : "Gönen",
      "year" : 2013
    }, {
      "title" : "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "author" : [ "N. Halko", "P. Martinsson", "J.A. Tropp" ],
      "venue" : "SIAM Review 53(2):217–288.",
      "citeRegEx" : "Halko et al\\.,? 2011",
      "shortCiteRegEx" : "Halko et al\\.",
      "year" : 2011
    }, {
      "title" : "Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES)",
      "author" : [ "N. Hansen", "S.D. Müller", "P. Koumoutsakos" ],
      "venue" : "Evolutionary Computation 11(1):1–18.",
      "citeRegEx" : "Hansen et al\\.,? 2003",
      "shortCiteRegEx" : "Hansen et al\\.",
      "year" : 2003
    }, {
      "title" : "Nuclear norm minimization via active subspace selection",
      "author" : [ "C. Hsieh", "P.A. Olsen" ],
      "venue" : "The 31st International Conference on Machine Learning, 575– 583.",
      "citeRegEx" : "Hsieh and Olsen,? 2014",
      "shortCiteRegEx" : "Hsieh and Olsen",
      "year" : 2014
    }, {
      "title" : "Learning attractor landscapes for learning motor primitives",
      "author" : [ "A.J. Ijspeert", "J. Nakanishi", "S. Schaal" ],
      "venue" : "Advances in Neural Information Processing Systems 15, 1523–1530.",
      "citeRegEx" : "Ijspeert et al\\.,? 2002",
      "shortCiteRegEx" : "Ijspeert et al\\.",
      "year" : 2002
    }, {
      "title" : "Principal Component Analysis",
      "author" : [ "I.T. Jolliffe" ],
      "venue" : "Springer Verlag.",
      "citeRegEx" : "Jolliffe,? 1986",
      "shortCiteRegEx" : "Jolliffe",
      "year" : 1986
    }, {
      "title" : "Reinforcement learning to adjust robot movements to new situations",
      "author" : [ "J. Kober", "E. Oztop", "J. Peters" ],
      "venue" : "The 22nd International Joint Conference on Artificial Intelligence, 2650–2655.",
      "citeRegEx" : "Kober et al\\.,? 2011",
      "shortCiteRegEx" : "Kober et al\\.",
      "year" : 2011
    }, {
      "title" : "On information and sufficiency",
      "author" : [ "S. Kullback", "R.A. Leibler" ],
      "venue" : "The Annals of Mathematical Statistics 22(1):79–86.",
      "citeRegEx" : "Kullback and Leibler,? 1951",
      "shortCiteRegEx" : "Kullback and Leibler",
      "year" : 1951
    }, {
      "title" : "Variational inference for policy search in changing situations",
      "author" : [ "G. Neumann" ],
      "venue" : "The 28th International Conference on Machine Learning, 817–824.",
      "citeRegEx" : "Neumann,? 2011",
      "shortCiteRegEx" : "Neumann",
      "year" : 2011
    }, {
      "title" : "Trace norm regularization: Reformulations, algorithms, and multi-task learning",
      "author" : [ "T.K. Pong", "P. Tseng", "S. Ji", "J. Ye" ],
      "venue" : "SIAM Journal on Optimization 20(6):3465–3489.",
      "citeRegEx" : "Pong et al\\.,? 2010",
      "shortCiteRegEx" : "Pong et al\\.",
      "year" : 2010
    }, {
      "title" : "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
      "author" : [ "B. Recht", "M. Fazel", "P.A. Parrilo" ],
      "venue" : "SIAM Review 52(3):471–501.",
      "citeRegEx" : "Recht et al\\.,? 2010",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2010
    }, {
      "title" : "Robot weightlifting by direct policy search",
      "author" : [ "M.T. Rosenstein", "A.G. Barto" ],
      "venue" : "The 17th International Joint Conference on Artificial Intelligence, 839–846.",
      "citeRegEx" : "Rosenstein and Barto,? 2001",
      "shortCiteRegEx" : "Rosenstein and Barto",
      "year" : 2001
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree search",
      "author" : [ "K.", "T. Graepel", "D. Hassabis" ],
      "venue" : "Nature 529:484–503.",
      "citeRegEx" : "K. et al\\.,? 2016",
      "shortCiteRegEx" : "K. et al\\.",
      "year" : 2016
    }, {
      "title" : "Sufficient dimension reduction via squared-loss mutual information estimation",
      "author" : [ "T. Suzuki", "M. Sugiyama" ],
      "venue" : "Neural Computation 25:725–758.",
      "citeRegEx" : "Suzuki and Sugiyama,? 2013",
      "shortCiteRegEx" : "Suzuki and Sugiyama",
      "year" : 2013
    }, {
      "title" : "An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems",
      "author" : [ "K. Toh", "S. Yun" ],
      "venue" : "International Symposium on Mathematical Programming.",
      "citeRegEx" : "Toh and Yun,? 2009",
      "shortCiteRegEx" : "Toh and Yun",
      "year" : 2009
    }, {
      "title" : "Locally weighted projection regression: Incremental real time learning in high dimensional space",
      "author" : [ "S. Vijayakumar", "S. Schaal" ],
      "venue" : "The 17th International Conference on Machine Learning, 1079–1086.",
      "citeRegEx" : "Vijayakumar and Schaal,? 2000",
      "shortCiteRegEx" : "Vijayakumar and Schaal",
      "year" : 2000
    }, {
      "title" : "On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming",
      "author" : [ "A. Wächter", "L.T. Biegler" ],
      "venue" : "Mathametical Programming 106(1):25–57.",
      "citeRegEx" : "Wächter and Biegler,? 2006",
      "shortCiteRegEx" : "Wächter and Biegler",
      "year" : 2006
    }, {
      "title" : "Embed to control: A locally linear latent dynamics model for control from raw images",
      "author" : [ "M. Watter", "J.T. Springenberg", "J. Boedecker", "M.A. Riedmiller" ],
      "venue" : "Advances in Neural Information Processing Systems 28, 2746–2754.",
      "citeRegEx" : "Watter et al\\.,? 2015",
      "shortCiteRegEx" : "Watter et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Direct policy search approaches (Baxter and Bartlett, 2000; Rosenstein and Barto, 2001; Deisenroth, Neumann, and Peters, 2013) allow the agent to learn a separate policy for each context through trial and error.",
      "startOffset" : 32,
      "endOffset" : 126
    }, {
      "referenceID" : 21,
      "context" : "Direct policy search approaches (Baxter and Bartlett, 2000; Rosenstein and Barto, 2001; Deisenroth, Neumann, and Peters, 2013) allow the agent to learn a separate policy for each context through trial and error.",
      "startOffset" : 32,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "On the other hand, direct contextual policy search approaches (Kober, Oztop, and Peters, 2011; Neumann, 2011; da Silva, Konidaris, and Barto, 2012) represent the contexts by real-valued vectors and are able to learn a context-dependent distribution over the policy parameters.",
      "startOffset" : 62,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "Learning from high-dimensional variables, in fact, is still an important problem in statistics and machine learning (Bishop, 2006).",
      "startOffset" : 116,
      "endOffset" : 130
    }, {
      "referenceID" : 0,
      "context" : "To alleviate these issues, Abdolmaleki et al. (2015) recently proposed a stochastic search framework called model-based relative entropy stochastic search (MORE).",
      "startOffset" : 27,
      "endOffset" : 53
    }, {
      "referenceID" : 15,
      "context" : "A prominent example is principal component analysis (PCA) (Jolliffe, 1986), that does not take the supervisory signal into account and therefore cannot discriminate between relevant and irrelevant latent dimensions.",
      "startOffset" : 58,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : "Moreover, they often involve non-convex optimization and suffer from local optima (Fukumizu, Bach, and Jordan, 2009; Suzuki and Sugiyama, 2013).",
      "startOffset" : 82,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "In the last years, non-linear dimensionality reduction techniques based on deep learning have gained popularity (Bengio, 2009).",
      "startOffset" : 112,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "In the last years, non-linear dimensionality reduction techniques based on deep learning have gained popularity (Bengio, 2009). For instance, Watter et al. (2015) proposed a generative deep network to learn lowdimensional representations of images in order to capture information about the system transition dynamics and allow optimal control problems to be solved in lowdimensional spaces.",
      "startOffset" : 113,
      "endOffset" : 163
    }, {
      "referenceID" : 2,
      "context" : "In the last years, non-linear dimensionality reduction techniques based on deep learning have gained popularity (Bengio, 2009). For instance, Watter et al. (2015) proposed a generative deep network to learn lowdimensional representations of images in order to capture information about the system transition dynamics and allow optimal control problems to be solved in lowdimensional spaces. More recently, Silver et al. (2016) successfully trained a machine to play a high-level game of go using a deep convolutional network.",
      "startOffset" : 113,
      "endOffset" : 427
    }, {
      "referenceID" : 0,
      "context" : "The original MORE (Abdolmaleki et al., 2015) finds a search distribution (without context) that maximizes the expected reward while upper-bounding the KullbackLeibler (KL) divergence (Kullback and Leibler, 1951) and lower-bounding the entropy.",
      "startOffset" : 18,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : ", 2015) finds a search distribution (without context) that maximizes the expected reward while upper-bounding the KullbackLeibler (KL) divergence (Kullback and Leibler, 1951) and lower-bounding the entropy.",
      "startOffset" : 146,
      "endOffset" : 174
    }, {
      "referenceID" : 0,
      "context" : "The latter is adaptively changed according to the percentage of the relative difference between the sampling policy’s expected entropy and the minimal entropy, as described by Abdolmaleki et al. (2015), i.",
      "startOffset" : 176,
      "endOffset" : 202
    }, {
      "referenceID" : 26,
      "context" : "The dual function can be minimized by standard non-linear optimization routines such as IPOPT (Wächter and Biegler, 2006).",
      "startOffset" : 94,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "The quadratic model can be learned by regression methods such as ridge regression2 (Bishop, 2006).",
      "startOffset" : 83,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "Principal component analysis (PCA) (Jolliffe, 1986) is a common method used in statistics and machine learning.",
      "startOffset" : 35,
      "endOffset" : 51
    }, {
      "referenceID" : 23,
      "context" : "Alternative supervised techniques, such as KDR (Fukumizu, Bach, and Jordan, 2009) and LSDR (Suzuki and Sugiyama, 2013), do not take the regression model, i.",
      "startOffset" : 91,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "On the contrary, in projection regression (Friedman and Stuetzle, 1981; Vijayakumar and Schaal, 2000) the model parameters and the projection matrix are learned simultaneously.",
      "startOffset" : 42,
      "endOffset" : 101
    }, {
      "referenceID" : 25,
      "context" : "On the contrary, in projection regression (Friedman and Stuetzle, 1981; Vijayakumar and Schaal, 2000) the model parameters and the projection matrix are learned simultaneously.",
      "startOffset" : 42,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "In the original MORE, Bayesian dimensionality reduction (Gönen, 2013) is applied to perform linear supervised dimensionality reduction on θ, i.",
      "startOffset" : 56,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "Second, a set of negative definite matrices is convex since y(aX + (1 − a)Y )y < 0 for any negative definite matrices X and Y , 0 ≤ a ≤ 1, and any vector y (Boyd and Vandenberghe, 2004).",
      "startOffset" : 156,
      "endOffset" : 185
    }, {
      "referenceID" : 5,
      "context" : "For this reason, the nuclear norm has been a popular surrogate to a low-rank constraint in many applications, such as matrix completion (Candès and Tao, 2010) and multi-task learning (Pong et al.",
      "startOffset" : 136,
      "endOffset" : 158
    }, {
      "referenceID" : 19,
      "context" : "For this reason, the nuclear norm has been a popular surrogate to a low-rank constraint in many applications, such as matrix completion (Candès and Tao, 2010) and multi-task learning (Pong et al., 2010).",
      "startOffset" : 183,
      "endOffset" : 202
    }, {
      "referenceID" : 4,
      "context" : "(8) is convex, any convex optimization method can be used (Boyd and Vandenberghe, 2004).",
      "startOffset" : 58,
      "endOffset" : 87
    }, {
      "referenceID" : 24,
      "context" : "For our experiments, we use the accelerated proximal gradient descend (APG) (Toh and Yun, 2009).",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "Although we did not encounter severe problems in our experiments, for very large dimensional tasks this issue can be mitigated by using more efficient techniques, such as active subspace selection (Hsieh and Olsen, 2014).",
      "startOffset" : 197,
      "endOffset" : 220
    }, {
      "referenceID" : 2,
      "context" : "Recently, non-linear techniques based on deep network has been showing impressive performance (Bengio, 2009; Watter et al., 2015).",
      "startOffset" : 94,
      "endOffset" : 129
    }, {
      "referenceID" : 27,
      "context" : "Recently, non-linear techniques based on deep network has been showing impressive performance (Bengio, 2009; Watter et al., 2015).",
      "startOffset" : 94,
      "endOffset" : 129
    } ],
    "year" : 2016,
    "abstractText" : "Direct contextual policy search methods learn to improve policy parameters and simultaneously generalize these parameters to different context or task variables. However, learning from high-dimensional context variables, such as camera images, is still a prominent problem in many real-world tasks. A naive application of unsupervised dimensionality reduction methods to the context variables, such as principal component analysis, is insufficient as task-relevant input may be ignored. In this paper, we propose a contextual policy search method in the model-based relative entropy stochastic search framework with integrated dimensionality reduction. We learn a model of the reward that is locally quadratic in both the policy parameters and the context variables. Furthermore, we perform supervised linear dimensionality reduction on the context variables by nuclear norm regularization. The experimental results show that the proposed method outperforms naive dimensionality reduction via principal component analysis and a state-ofthe-art contextual policy search method.",
    "creator" : "LaTeX with hyperref package"
  }
}