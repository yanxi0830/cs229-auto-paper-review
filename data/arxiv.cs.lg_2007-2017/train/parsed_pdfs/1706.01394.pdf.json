{
  "name" : "1706.01394.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Sebastian Casalaina-Martin", "Rafael Frongillo", "Bo Waggoner" ],
    "emails" : [ "CASA@MATH.COLORADO.EDU", "RAF@COLORADO.EDU", "TDMORGAN@SEAS.HARVARD.EDU", "BWAG@SEAS.UPENN.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In machine learning and statistics, empirical risk minimization (ERM) is a dominant inference technique, wherein a model is chosen which minimizes some loss function over a data set. As the choice of loss function used in ERM may have a large impact on the model chosen, how should one choose this loss? A growing body of work in property elicitation seeks to answer this question, by viewing a loss function as “incentivizing” the prediction of a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is well-known that squared loss elicits the mean, and hence least-squares regression finds the best fit to the conditional means of the data.1\nA natural question, which is still open in the vector-valued case, is the following: for which conditional statistics do there exist loss functions which elicit them? Positive examples include the mean, median, other quantiles, moments, and several others. Perhaps surprisingly, however, there are negative examples as well: it is well-known that the variance is not elicitable, meaning there is no loss function for which minimizing the loss will yield the variance of the data or distribution.\n1. There are also contributions from microeconomics, and crowdsourcing in particular, where one wishes to incentivize humans rather than algorithms, but the mathematics is the same.\nc© 2017 S. Casalaina-Martin, R. Frongillo, T. Morgan & B. Waggoner.\nar X\niv :1\n70 6.\n01 39\n4v 1\n[ cs\n.L G\n] 5\nThe usual approach to dealing with non-elicitable statistics is called indirect elicitation: elicit other conditional statistics from which one can compute the desired statistic. For example, the variance of a distribution can be written as (2nd moment) - (1st moment)2, and as mentioned above, moments are elicitable. The question of how many such auxiliary statistics are required gives rise to the concept of elicitation complexity; since the variance cannot be elicited with one but can with two, we say it is 2-elicitable (Lambert et al., 2008; Frongillo and Kash, 2015c).\nIn this paper, we explore an alternative approach to dealing with non-elicitable statistics, by allowing the loss function to depend on multiple data points simultaneously. In the language of property elicitation, this corresponds to loss functions such as `(r, y1, y2) which judge the “correctness” of the report r based on two (or more) observations y1 and y2. Assuming these observations are drawn independently from the same distribution, this intuitively gives the loss function more power, and could potentially render previously non-elicitable statistics elicitable. In fact, the variance is one such example: if y1 and y2 are both drawn i.i.d. from p, it is easy to see that 12(y1− y2) 2 will be an unbiased estimator for the variance of p, hence `(r, y1, y2) = (r− 12(y1−y2) 2)2 elicits the variance for the usual reason that squared error elicits expected values. Examples of settings where such i.i.d. observations are readily obtained include: active learning, uncertainty quantification & robust engineering design (Beyer and Sendhoff, 2007), and replication of scientific experiments.\nBeyond the variance, are there other non-elicitable statistics which we can elicit with multiple i.i.d. observations? Moreover, what is the tradeoff between the number of observations and the number of reports? One would expect the elicitation complexity, in the usual number-of-reports sense, to drop as observations are added, but how fast is unclear. Indeed, we will see several examples where the complexity drops dramatically, such a the k-norm of the distribution p. In Section 4 we develop new techniques to prove complexity bounds using algebraic geometry, which show for example that the complexity of the k-norm drops from the support size of p (minus 1) with 1 observation, to 1 with k observations. We call the feasible (# reports, # observations) pairs the elicitation frontier, for which the given statistic is elicitable, a concept we explore in Section 5.\nFinally, in Section 6 we apply multi-observation elicitation to regression. Traditional elicitation complexity expresses a conditional statistic Γ as a link of other statistics, but as we illustrate, situations can arise where these other statistics have a much more complicated relationship with the covariates than Γ does. We give an example where fitting a model to the conditional variance directly (using nearby data points as proxies for i.i.d. observations) is much better than fitting separate models to the conditional first and second moments and combining these to obtain the variance."
    }, {
      "heading" : "1.1. Related work",
      "text" : "Our work is inspired in part by Frongillo et al. (2015) which proposes a way to elicit the confidence (inverse of variance) of an agent’s estimate of the bias of a coin by simply flipping it twice. In our terminology, this follows from the fact that the variance is (1, 2)-elicitable. Multi-observation losses have been previously introduced to learn embeddings (Hadsell et al., 2006; Schroff et al., 2015; Ustinova and Lempitsky, 2016), though an explicit property/statistic is never discussed."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "We are interested in a space Y from which observations y are drawn, which will be a finite set unless otherwise specified. We will denote by P ⊆ ∆Y a set of probability distributions of interest. (Generally in this paper, P is simply the entire simplex.) We refer to the set ∆Ym of all distributions\non m outcomes as the m-product space. To capture the assumption that we may collect m ∈ {1, 2, . . .} observations which are each i.i.d. from the same distribution p ∈ ∆Y , we will write pm ∈ ∆Ym to denote their joint distribution, pm(y1, . . . , ym) = ∏ i p(yi). The set of all such distributions is denoted Pm = {pm : p ∈ P} ⊆ ∆Ym , which we will think of as a manifold in the m-product space.\nWith this notation in hand, we can define the central concepts in elicitation complexity in our context. Properties include any typical statistic,2 for instance, the mean when Y ⊆ R is the property Γ(p) = ∑ y p(y)y.\nDefinition 1 (Property) A property is a function Γ : P → R, whereR ⊆ Rk for some k ≥ 1.\nIntuitively, properties represent the information desired about the data or underlying distribution. R is sometimes called the report space. The central notion of property elicitation is the relationship between a loss function ` and the minimizer of its expected loss. If this minimizer is a particular property Γ, we say ` elicits Γ. We simply extend this usual definition to allow for multiple observations in the expected loss.\nDefinition 2 (Loss function, elicits) An m-observation loss function is a function ` : R× Ym → R, where `(r, y1, . . . , ym) is the loss for prediction r ∈ R scored against realized observations yi ∈ Y . We say ` (directly) elicits a property Γ : P → R if for all p ∈ P we have {Γ(p)} = argminr∈R E(y1,...,ym)∼pm [`(r, y1, . . . , ym)].\nIt is useful to consider a property in terms of its level sets, the set of distributions sharing the same particular value of the property. For example, when the property is the mean of a distribution on {1, 2, 3, 4}, both p = ( 1 2 , 0, 0, 1 2 ) and p = ( 0, 12 , 1 2 , 0 ) lie in the level set Γ2.5.\nDefinition 3 (Level set) A level set Γr of a property Γ : P → R is, for r ∈ R, the set of distributions with property r, i.e. Γr = {p ∈ P | Γ(p) = r}.\nAn important technical condition on a property, and one which we will need for the notion of indirect elicitability, is that it be identifiable, meaning that its level sets can be described by linear equalities.\nDefinition 4 (Identifiable) A property Γ : P → R, with R ⊆ Rk, is identifiable with m observations if there exists some V : R× Ym → Rk such that Γ(p) = r ⇐⇒ Epm [V (r,y)] = 0 ∈ Rk, where y = (y1, . . . , ym) is drawn from pm. We also say it is m-identifiable.\nIdentifiability is a geometric restriction on properties that is intuitively similar to continuity of the property (cf. Lambert et al. (2008); Steinwart et al. (2014)). Technically, observe that differentiable loss functions generally elicit an identifiable property, as any local optimum should have∑\ni ∂ ∂ri ` (r,y) = 0, meaning that the gradient of ` itself gives an identification function. Following Frongillo and Kash (2015a), we will often assume that properties are identifiable. Notice that any property can be “indirectly” elicited by using a proper scoring rule, which elicits the entire distribution, and then computing the property from the distribution. But this requires a report of dimension |Y|− 1, whereas to indirectly elicit the variance of y, for example, requires just\n2. As defined, statistics like the median would not be included unless restrictions were placed on P for them to be single-valued (distributions in general may have multiple medians); we may instead extend our definition to include set-valued statistics, which would not substantially alter our results, and in fact we do lift this restriction in Section 3.1.\ntwo reports, e.g. r1 = Ey and r2 = Ey2, along with a “link function” ψ(r) = r2− r21. The question of elicitation complexity, studied by Lambert et al. (2008) and Frongillo and Kash (2015c), is how many dimensions d are needed to indirectly elicit the property of interest Γ via some elicitable Γ̂ : P → Rd; one hopes that d is much smaller than |Y|. Here we augment this question by another degree of freedom: how many dimensions d, and observations m, are needed to indirectly elicit Γ?\nDefinition 5 ((d,m)-elicitable) A property Γ : P → R is (d,m)-elicitable if there exists a ddimensional and identifiable property Γ̂ : P → R̂ where R̂ ⊆ Rd, an m-observation loss function ` : R̂ × Ym → R, and a “link” function ψ : R̂ → R, such that\n1. ` directly elicits Γ̂, and 2. Γ(p) = ψ ( Γ̂(p) ) .\nThe elicitation frontier of Γ is the set of (d,m) such that Γ is (d,m)-elicitable, but neither (d−1,m)nor (d,m− 1)-elicitable.\nWe may say that a property’s “report complexity” is d if (d, 1) lies on its frontier, and its “observation complexity” is m if (1,m) does."
    }, {
      "heading" : "2.1. Illustrative example",
      "text" : "Recall our observation that the variance is not (1, 1)-elicitable, and the “traditional” fix is to utilize (2, 1)-elicitability: minimize a loss function over two dimensions (say first and second moments), mapping the result to the variance via a link function. We observed instead that it is possible to utilize (1, 2)-elicitability: minimize a loss function that takes two observations over a single scalar, the variance itself. Can this tradeoff be more extreme? In particular, are there cases where additional observations drastically decrease the report complexity? Consider the 2-norm of a distribution: Γ(p) = ‖p‖2 = √∑ y p(y)\n2. We show in Section 5.2 that ‖p‖2 has report complexity |Y| − 1 (where Y is the outcome set) for 1 observation – no single-observation loss function can do better than solving for the entire distribution. However, recall that ‖p‖22 = ∑ y p 2 y = Pr[y1 = y2] for two i.i.d. observations y1, y2, or in other words, ‖p‖22 = Ep1{y1 = y2}. The two-norm is actually elicitable with two observations and a single dimension using e.g. loss function `(r, y1, y2) = (r − 1{y1 = y2})2, then simply computing ‖p‖2 = √ r. In other words, the two-norm’s elicitation frontier on Y consists of the points (|Y| − 1, 1) and (1, 2). The goal for this paper is to investigate the (algebraic-)geometric reasons underpinning why a property might have low or high observation complexity, as well as providing general results and examples based on these ideas. We next introduce the geometric foundations for this investigation."
    }, {
      "heading" : "3. Geometric Fundamentals",
      "text" : "The most basic (yet powerful) lower bound in property elicitation says that elicitable properties’ level sets must be convex sets (Lambert et al., 2008). Indeed, this is used to prove the variance is not (1,1)-elicitable; but the variance is elicitable with two observations. The geometry is not “broken” here, but merely lives in a higher-dimensional space. When reasoning about eliciting a property Γ : P → R using m observations, it often useful to instead think of eliciting the property using a single random draw from a distribution on m-tuples of outcomes.\nRemark 6 Since P is isomorphic to Pm, a property Γ : P → R is directly elicitable with m observations if and only if the induced property Γm : Pm → R is directly elicitable with 1 observation. In particular, a sufficient condition for (d,m)-elicitability of Γ is that there exists some (d, 1)-elicitable Γ′ : ∆Ym → R that coincides with Γm on Pm. One can elicit Γ using the same loss that elicits Γ′, treating the m-tuple of observations as a single draw from the larger space.\nThis gives us one initial way to demonstrate that a property is elicitable withm observations. For example, the loss function `(r, a, b) = ( r − 12(a− b) 2 )2 elicits the variance with two observations a, b, but if we consider distributions on all of Y ×Y , including non-i.i.d. distributions, it actually is still a valid loss function eliciting a property that coincides with the variance when a, b are i.i.d. To see this, just note that it still elicits an expectation: ∑ a,b p ′(a, b)12(a− b) 2 where p′ is a distribution on R2.\nHowever, considering elicitation on the larger space ∆Ym does not resolve the problem in either the necessary or sufficient directions. First, Pm is not a convex set for m > 1, so conditions on the convexity of level sets do not naturally extend here. An example of this is shown in Figure 1. Second, coming up with an “extended property” may be difficult or non-obvious. For example, it is not so clear whether the above loss function elicits anything natural on ∆Y2 (it is not the covariance, for instance, which is zero for i.i.d. distributions). More fundamentally, it is not clear whether such extensions should generally exist. (Proving or constructing a counterexample is an interesting open problem.) In general, we hope to be able to accomplish much more by restricting to Pm because it is only a tiny |Y|-dimensional manifold in a |Y|m-dimensional space.\nA tighter sufficient condition is given by Frongillo and Kash (2014), which states that essentially all loss functions eliciting a property on any set, such as Pm, also elicit some “extension” of\nthat property on the convex hull of that set. So while the higher-dimensional approach is helpful, it does not preclude reasoning about the space Pm as a manifold inside ∆Ym .\nMost significantly, Pm is not a convex space, which makes lower bounds on elicitation complexity nontrivial as well. However, the result of Frongillo and Kash (2014) shows that it suffices to provide lower bounds for elicitation on the convex hull of Pm, which we will denote conv(Pm). Quite naturally then, we explore what leverage we can gain by reasoning about conv(Pm).\nTheorem 7 The property Γ : P → R is not directly elicitable with m observations if there exists r1, r2 ∈ Γ(P), p1,1, . . . , p1,k1 ∈ Γr1 , p2,1, . . . , p2,k2 ∈ Γr2 , λ1,1, . . . , λ1,k1 ∈ [0, 1] and λ2,1, . . . , λ2,k2 ∈ [0, 1] such that r1 6= r2, ∑k1 i=1 λ1,i = 1, ∑k2 i=1 λ2,i = 1 and\nk1∑ i=1 λ1,ip m 1,i = k2∑ i=1 λ2,ip m 2,i.\nIn other words, a property is not elicitable if there is a convex combination of one of its level sets in the m-product space that equals a convex combination of another one of its level sets in the m-product space.\nTheorem 7 allows us to prove for example that the fourth central moment is not directly elicitable with two observations. Consider a Bernoulli random variable Y ∼ p, then two of the level sets of the fourth central moment Γ(p) = EY∼p[(Y −EY∼p[Y ])4] are given in Figure 2. When we project these level sets into the 2-product space we can easily find a pair of points from each level set whose connecting lines intersects in conv(∆Ym). These lines are convex combinations of points in the same level set, so by Theorem 7 the lines’ intersection implies that Γ is not directly elicitable with two observations."
    }, {
      "heading" : "3.1. Finite Properties",
      "text" : "Finite properties are those where R, the range of Γ, is a finite set. This corresponds to a “multiplechoice question” (Lambert and Shoham, 2009). In this section, we must allow Γ : P ⇒ R to be a set-valued function, possibly assigning multiple possible correct reports to a single distribution; this is necessary for “boundary” cases, such as the mode of the uniform distribution on a finite set. (Similarly, we cannot require identifiability.) We have a finite set of outcomes Y , the distributions considered are all P = ∆Y , and Γ(p) must be nonempty.\nWe are interested in understanding which finite properties can be elicited with m observations. Previously, this question was studied for the case of one observation by Lambert (2011), who characterized elicitable properties by the shape of their level sets: they are intersections of Voronoi diagrams in R|Y| with the simplex ∆Y . In our setting, a Voronoi diagram is specified by a finite set of points {xr : r ∈ R} ⊆ RY m , with each cell Tr = {x : ‖x − xr‖ ≤ ‖x − xr′‖∀r′ ∈ R} consisting of those points in RYm closest in Euclidean distance to xr. Using the geometric constructions above, we can simply apply the main result of Lambert (2011) to finite properties in the m-product space; the result is a characterization of elicitable finite properties with m observations.\nCorollary 8 A finite property Γ : ∆Y ⇒ R is directly elicitable with m samples if and only if there exists a Voronoi diagram in RYm with {xr : r ∈ R} satisfying Γmr = Tr ∩ Pm. Here Γmr = {pm ∈ Pm : p ∈ Γr}.\nMultiple observations afford considerable flexibility in the level sets of such an elicitable Γ. In particular, whereas before the cell boundaries between level sets were restricted to hyperplanes,\nwithm observations these boundaries can be defined by nearly arbitrarym-degree polynomials. We illustrate this flexibility and visualize the cell boundaries in Figure 3. In particular, we show that a classic negative example, where an agent is asked to report whether their belief has low or high variance, is easily elicited with two observations."
    }, {
      "heading" : "4. Lower Bounds via Geometry",
      "text" : "In this section we discuss lower bounds on elicitation complexity. For technical reasons we will here require P to be a C∞ submanifold of ∆Y with corners. Our lower bounds will also generally require Γ to be a C∞ function, in which case we call it a C∞ property.\nWe begin in the first subsection by recalling the structure of the level sets of identifiable properties, and then introduce a technique for obtaining from this some lower bounds on elicitation complexity via differential geometry. In the next subsection we focus on polynomial properties, and explain some results that use algebraic geometry to obtain sharp bounds."
    }, {
      "heading" : "4.1. Preliminaries on identifiable properties",
      "text" : "We start by recalling a general method, introduced in Frongillo and Kash (2015c), for showing lower bounds on elicitation complexity: Given a property Γ, if one can show that no level set from any Γ̂, which is m-identifiable and directly elicitable with m observations, can be contained in a particular level set of Γ, then Γ cannot be (d,m)-elicitable. This follows immediately from the definitions: if Γ is indirectly elicited via Γ̂ and link ψ, so that Γ = ψ ◦ Γ̂, then we have the following relationship between the level sets of Γ and Γ̂:\nΓr = ⋃\nr̂:ψ(r̂)=r\nΓ̂r̂. (1)\nIn other words, the level sets of Γ are obtained by combining some of the level sets of Γ̂. For instance, if ψ is a bijection, then the level sets of Γ and Γ̂ are identical. This method was used successfully in Frongillo and Kash (2015c) to show lower bounds on the report complexity (d) of\na property, with m = 1. In this section, we will use the same method to show lower bounds on observation complexity (m), with d = 1.\nOur main tool for obtaining these lower bounds will be that the level sets of any directly mobservation-elicitable, identifiable Γ̂ have a specific structure, namely, such a level set is the zero set of a polynomial of degree at most m:\nFact 1 If a property Γ̂(p) ism-identifiable, then each level set of Γ̂ is the set of zeros of a polynomial in p of degree at most m.\nProof The condition EpV (r,y) = 0 is ∑\ny1,...,ym p(y1) · · · p(ym)V (r, y1, . . . , ym) = 0 .\nCombined with the equality (1) above, Fact 1 tells us that the level sets of indirectly elicitable Γ are unions of zero sets of polynomials. As we are focusing on the d = 1 case, however, both Γ and Γ̂ are real-valued functions, so with enough regularity, their level sets should coincide. Before making a precise statement, we introduce the following definition:\nDefinition 9 (C∞ (d,m)-elicitable) We say that a C∞ property Γ : P → Rd′ is C∞ (d,m)elicitable if in the definition of (d,m)-elicitable, Γ̂ can be taken to be C∞ and ψ can be taken to be C∞ in an open neighborhood of the image of Γ̂.\nCorollary 10 Suppose that a C∞ property Γ : P → R is C∞ (1,m)-elicitable. Let r ∈ R, let Z ⊆ Γ−1(r) be a connected component of the level set, and assume that Z admits a point that is not a critical point of Γ; i.e., there is a point p ∈ Z such that the differential of Γ at p is nonzero. Then Z is a connected component of the set of zeros of a polynomial of degree at most m. Moreover, if Γ−1(r) is connected, then Γ−1(r) is the zero set of a polynomial of degree at most m.\nProof Let Γ̂ and ψ be as in Definition 9. We have a commutative diagram:\nP Γ̂ //\nΓ\nR\nψ R\nSince Z is connected, we have that Γ̂(Z) ⊆ R is connected, and is therefore an interval (see e.g., Browder (1996), Theorem 6.76, 6.77, p.148). The claim is that this interval is a point. Indeed, assume the opposite. Then since ψ is by definition constant on the interval Γ̂(Z), we would have that the differential Dψ vanishes at each point of of Γ̂(Z). Then since DΓ = Dψ ◦DΓ̂ we would have that DΓ vanishes at every point of Z. But this would contradict our assumption. Thus Γ̂(Z) is a point.\nIt then follows from Fact 1 that Γ̂−1(Γ̂(Z)) is the zero set of a polynomial of degree at most m. We now use the inclusions\nZ ⊆ Γ̂−1(Γ̂(Z)) ⊆ Γ−1(r).\nBy virtue of the inclusion on the right, every connected component of Γ̂−1(Γ̂(Z)) is contained in a connected component of Γ−1(r). This proves the first assertion of the lemma. The last assertion of the lemma also follows from these inclusions, since in that case one is assuming Z = Γ−1(r).\nRemark 11 For concreteness, we summarize the contrapositive of Corollary 10 in the way in which we will use it in examples: Suppose that Γ : P → R is a C∞ property, and there exists an r ∈ R such that the level set Γ−1(r) is connected, and contains a point P ∈ Γ−1(r) that is not a critical point for Γ. Then if Γ−1(r) is not the zero locus of a degreem polynomial in p(y1), . . . , p(ym), then Γ is not C∞ (1,m)-elicitable.\nAs a consequence of Corollary 10, we can immediately show the existence of C∞ properties with infinite observation complexity; i.e., properties that are not C∞ (1,m) elicitable for any m. The proof gives such an example for |Y| = 3, a surprising result given that all properties have report complexity |Y|−1 = 2; i.e., all of the C∞ properties are C∞ (2, 1)-elicitable. Note that if |Y| = 2, then all C∞ properties are C∞ (1, 1)-elicitable.\nProposition 12 There are C∞ properties that are not C∞ (1,m)-elicitable for any finite m.\nProof Take Y = {1, 2, 3}, P = ∆◦Y = {p ∈ ∆Y : p(y) > 0 ∀y ∈ Y}, and Γ(p) = p1 − (1/2) sin(1/p2). It is immediate that Γ has no critical points. Here the level sets Γr satisfy r = p1 − (1/2) sin(1/p2), in other words, satisfy the equation p1 = (1/2) sin(1/p2) + r. For p2 sufficiently small, the level set Γ0 = {p ∈ ∆3 : p1 = (1/2) sin(1/p2)} is simply the graph of (1/2) sin(1/x), which intersects the line p1 = 0 infinitely many times, and hence by the Fundamental Theorem of Algebra is not the zero set of any polynomial. Corollary 10 now implies that Γ is not (1,m)-elicitable for any m."
    }, {
      "heading" : "4.2. Polynomial properties and lower bounds using algebraic geometry",
      "text" : "We now describe some lower bounds for elicitation complexity of polynomial properties. The motivation for these lower bounds is the intuition that, in general, a polynomial property Γ : P → R of degree k should not be C∞ (1,m)-identifiable for any m < k, since the zero set of a degree k polynomial should not be the zero set of a degreem polynomial whenm < k. This statement can of course fail in special cases (e.g., Example 1 below). Indeed, there are some subtleties regarding zero sets of polynomials in Euclidean open sets, considered in Appendix C, that must be addressed to draw such a conclusion. Nevertheless, for a general polynomial property this expectation holds (see Remark C.4 for a precise definition of generality), and in the appendix we provide some elementary techniques for confirming this expectation in particular examples (see Corollary C.3). For instance, we show (Example C.1):\nCorollary 13 If |Y| ≥ 3, then for any natural number k, the k-norm of a distribution, Γ(p) = ( ∑\ny p(y) k)1/k, is not C∞ (1, k − 1)-elicitable.\nExample 1 In contrast to the case considered in Corollary 13, we emphasize that there are polynomial properties Γ : P → R of degree k that are C∞ (1,m)-elicitable for some m < k. For instance, take Γ̂ : P → R to be any polynomial property of degree m > 0, let ψ : R → R be any polynomial function of degree m′ > 1, and set Γ = ψ ◦ Γ̂. Then Γ is of degree k = mm′ > m, but Γ is C∞ (1,m)-elicitable, by Lemma 15."
    }, {
      "heading" : "5. Examples and Elicitation Frontiers",
      "text" : "We now combine our complexity lower bounds with upper bounds to make progress toward determining the elicitation frontiers of some potential properties of interest. See Figure 4 for a depiction of some of the elicitation frontiers described. We begin with some general, straightforward, but versatile upper bounds.\nLemma 14 For all 1 ≤ i ≤ n, 1 ≤ j ≤ m, let fij : Y → R be an arbitrary function such that Ep[fij(Y )] exists for all p ∈ P . Then Γ(p) = ∑n i=1 ∏m j=1 Ep[fij(Y )] is (1,m)-elicitable.\nProof Using Y1, . . . , Ym which are i.i.d. from p, then {fi1(Y1), . . . fim(Ym)} will be independent for all i. Using properties of expectations (linearity and independence), we have\nn∑ i=1 m∏ j=1 E[fij(Y )] = n∑ i=1 m∏ j=1 E[fij(Yj)] = n∑ i=1 E  m∏ j=1 fij(Yj)  = E  n∑ i=1 m∏ j=1 fij(Yj)  (2) Now we see that using squared loss (or any loss for the mean) one can leverage these m samples to\nelicit the desired sum of products, e.g. `(r, y1, . . . , ym) = ( r − ∑n i=1 ∏m j=1 fij(yj) )2 .\nThe proof of Lemma 14 simply constructs an unbiased estimator of the property of interest and elicits the mean of the estimator via squared error. By a very natural extension, this technique also applies to ratios of expectations, as they are elicitable (Gneiting, 2011): construct two unbiased estimators, and elicit the ratio of their means. We will give two instances of such ratios in the next subsection.\nThe following result establishes an upper bound that by now may seem natural: Under some conditions, a property that is itself an m-degree polynomial in p is (1,m)-elicitable.\nLemma 15 Suppose that Γ : P → R is a property such that Γ = ψ ◦ Γ′ where Γ′ : P → R is polynomial of degree m, and ψ : R → R is a function that is C∞ on an open neighborhood of the image of Γ′. Then Γ′ is directly (1,m)-elicitable, and Γ is C∞ (1,m)-elicitable.\nProof It is enough to show Γ′ is directly (1,m)-elicitable. This follows immediately from Lemma 14. Indeed, it is clear from the lemma that it is enough to show the result for monic monomials. For this one takes the fij in Lemma 14 to be characteristic functions 1y for y ∈ Y ."
    }, {
      "heading" : "5.1. Ratios of expectations: index of dispersion and Sharpe ratio",
      "text" : "The index of dispersion of a random variable Y with positive mean is defined to be Var(Y )/E[Y ] (Cox and Lewis, 1966). The Sharpe ratio of a random variable Y , which is a commonly-used measure of the risk-adjusted return of an investment, is defined similarly as E[Y ]/ √ Var(Y ) (Sharpe, 1966). Both the index of dispersion and the square of the Sharpe ratio are (1, 2)-elicitable by the above discussion: Var(Y ) = Ep[12(Y1 − Y2)\n2], Ep[Y ] = Ep[Y1], and Ep[Y ]2 = Ep[Y1Y2], so any ratios of these terms is (1, 2)-elicitable. (The link function for the Sharpe ratio is thus the square root.) For example, the index of dispersion is elicited by the loss `(r, y1, y2) = r(y1 − y2)2 − r2y1.\nTo finish describing the elicitation frontiers for these properties, we note that neither is (1, 1)- elicitable as the level sets are not convex, but both are (2, 1)-elicitable as we now show. For the\nindex of dispersion, we can take r1 = E[Y ] and r2 = E[Y 2], both elicitable as means, and then compute the property by (r2−r21)/r1. Similarly, for the same r1, r2, the Sharpe ratio can be written as r1/ √ r2 − r21."
    }, {
      "heading" : "5.2. Norms of distributions",
      "text" : "As we have previously discussed, the 2-norm is (1, 2) elicitable. For general k, the k-norm is (1, k) elicitable with the following loss function `(r, y1, . . . , yk) = (r − 1{y1 = . . . = yk})2. (This case also follows from Lemma 15.) This is a tight bound on the observation complexity, as we proved in Corollary 13 that the k-norm is not (1, k−1) elicitable. As it turns out, the report complexity of the k-norm is |Y|−1, meaning it is as hard to elicit with one observation as the entire distribution. This follows from Theorem 2 of Frongillo and Kash (2015c), specifically Section 4.2, as ‖p‖k is a convex function of p. An interesting open question, and one that will require additional algebraic tools, is the k-norm’s elicitation frontier when we allow multiple dimensions and multiple observations.\nCorollary 16 For |Y| ≥ 3, the elicitation frontier of the k-norm contains (|Y| − 1, 1) and (1, k)."
    }, {
      "heading" : "5.3. Central Moments",
      "text" : "The nth central moment µn of a random variable Y is defined as\nµn = E[(Y − E[Y ])n] = n∑ i=0 (−1)i ( n i ) E[Y ]i · E[Y n−i] , (3)\nwhich we see is (n, 1)-elicitable by simply elicitingE[Y i] for all i ∈ {1, . . . , n} and then combining the results. As we will show, µn is also (1, n)-elicitable, and moreover, we can achieve other dimension-observation tradeoffs in between, such as (b √ nc+1, d √ ne). The key idea is to partition the binomial sum (3) into k partial sums and factor out the highest power of E[Y ] from each, such that the jth partial sum can be written as\nE[Y ] j·n k n k −1∑ i=0 (−1)i ( n j·n k + i ) · E[Y ]i · E [ Y (j+1)·n k −1−i ] . (4)\nDoing so gives the following result. Theorem 17 The nth central moment is (k + 1, ⌈ n/k ⌉ )- elicitable; 0 < k ≤ n\nProof Consider the partial sum (4) without the E[Y ]j·n/k factor; by Lemma 14 each such factored sum is (1, ⌈ n/k ⌉ )-elicitable, as the maximum number of terms in any product is ⌈ n/k ⌉ . Since we have k such factored sums, and need to additionally elicit the mean E[Y ] to compute their factors, the entire sum can be elicited using ⌈ n/k ⌉ observations and k + 1 dimensions.\nWhen k = 0, we can do much better than m = ∞: by Lemma 14, as the maximum number of terms in any product of (3) is n, the term (E[Y ])n, we have than µn is (1, n)-elicitable. For lower bounds, little is known beyond µn not being (1, 1)-elicitable (Frongillo and Kash, 2015b)."
    }, {
      "heading" : "6. Multi-Observation Regression",
      "text" : "One of the earliest problems in modern statistics was the estimation of biodiversity in a geographic region (Fisher et al., 1943). One scalar measure of diversity of a distribution is the (inverse of the) 2-norm, which we will take here as an example.3 Consider a dataset of species samples: pairs (x, y) where x gives the features of the geographic region and y is a categorical giving the species to which this sample belongs. Suppose we wish to regress the diversity of species against geographic features such as climate. The single-observation approach would require a surrogate loss function `(f̂(x), y) and a link f(x) = ψ(f̂(x)). We claim that any single-observation loss function `(f(x), y) is poorly suited for this task. For the 2-norm, lower bounds on report complexity show that the best possible approach has dimensionality f̂ : x → Rd−1 where d is the number of unique species in the dataset (which may have a very long tail). So this approach requires, in essence, fitting f̂ to the entire distribution over species as a function of geographic region, a task of immense idiosyncrasy and complexity compared to the end goal of e.g. estimating a scalar measure of diversity as a function of rainfall level.\nOn the other hand, a two-observation loss function `(f(x), y1, y2) can be used to directly learn an f estimating the desired diversity measure, e.g. 2-norm, as a function of geographic features. One can then use empirical risk minimization to directly learn relationships between, e.g. rainfall level and this measure of species diversity.\nMulti-observation regression does introduce an additional challenge, however: risk in this context is naturally defined as Ex,y`(f(x),y) where y = (y1, . . . , ym) is a set of observations drawn i.i.d. conditioned on x, but our data points are of the form (x, y). If e.g. x comes from a continuous space, we may not have any sets of m samples y1, . . . , ym belonging to the same x. One natural setting where this poses no concern is in active learning where we may choose to re-draw the label for a given x. In a more standard regression framework, we propose to leverage the intuition that the distribution of y conditioned on x generally changes gradually as a function of x.4 Pragmatically, with dense enough data points, we can simply group together nearby x values and “merge” them into a data point of the form (x̄, y1, . . . , ym) where x̄ is an average and the yi are drawn independently and approximately identically from approximately the distribution of Y conditioned on x̄. For this paper, we demonstrate the idea in simulations below and give a basic proof-of-concept theoretical result in Appendix B, leaving a more thorough investigation to future work.\n3. A similar intuition will hold for most if not all elicitable measures of diversity. 4. Phrased differently, at least it seems reasonable to parameterize the rate of change and expect learning bounds to\ndepend on this parameter.\nIn general, the cases where the multi-observation approach can be useful are those where the property of interest is believed to follow a simple functional form, but the conditional statistics given by the indirect elicitation approach are expected to follow unknown or complicated trends as a function of features. For another example, one could imagine learning the noise (e.g. variance) of a medical test, e.g. white blood cell count, as a function of patient features, in order to improve the test. The indirect elicitation approach suggests first fitting a model for estimating the mean of the test’s outcome as a function of patient data, then fitting the expected square of the statistic, and then computing an estimate for the variance by combining them. In general, these prediction problems may be highly complex and nonlinear even when the noise in the test might follow some simple linear relationship with e.g. height or age. The multi-observation approach allows direct regression of the noise versus features. Formally, we show a basic extension of classic risk guarantees in Appendix B, under the assumption that x is distributed uniformly on [0, 1] and a closeness condition on the conditional distribution of Y given X ."
    }, {
      "heading" : "6.1. Simulation",
      "text" : "Here we describe some simulations run as a proof of concept of multi-observation regression. Our data points are of the form (x, y) ∈ R×R where x is drawn uniformly at random from the interval [0, 1]. Given x, y = a sin(4πx)+Z, where a is a constant and Z ∼ N(0, 1) is drawn independently for each sample, we wish to learn Var(Y |X).\nOur multi-observation loss function here is `(f(x), y1, y2) = (f(x) − 12(y1 − y2) 2)2. We approximate (x, y1, y2) samples by sorting the (xi, yi) pairs by xi, and making samples of the form (12(xi + xi+1), yi, yi+1). We compare to the single observation approach, in which we estimate E[Y |X] and E[Y 2|X] and then combine them to estimate Var(Y |X).\nThe point of these simulations is to demonstrate that multi-observation regression can greatly outperform single observation regression in the case when the function is in a known concept class, and the statistics needed to indirectly elicit it with a single observation are not in a known concept class. As such, our multi-observation regression fits a linear function to Var(Y |X), and our single observation regression fits linear functions to E[Y |X] and E[Y 2|X]. The true Var(Y |X) = 1 is indeed a linear function, while the true moment functions E[Y |X = x] = a sin(x) and E[Y 2|X = x] = a2 sin2(x) + 1 are very far from linear.\nFigure 5 gives the results for a = 1 and a = 10. Both plots show the mean squared error of the variance functions reported by the two regression methods (averaged over 4000 simulations) as a function of the number of samples. In both cases we see that for sufficiently many samples, the two observation regression significantly outperforms the single observation regression."
    }, {
      "heading" : "7. Conclusion and Future Work",
      "text" : "An immediate host of directions is the proving of upper and lower bounds on elicitation frontiers for various properties. In particular, our lower bounds here focus on techniques for lower-bounding observation complexity (the (1,m) case), leaving open approaches for lower bounds on (d,m) complexity for d ≥ 2. Another direction is to formalize learning guarantees for multi-observation regression under suitable assumptions on slow-changing conditional distributions."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Karthik Kannan for contributing the upper bound for central moments. Sebastian CasalainaMartin was partially supported by NSA grant H98230-16-1-0053. Tom Morgan was funded in part by NSF grants CCF-1320231 and CNS-1228598. Bo Waggoner is supported by the Warren Center for Network and Data Sciences at the University of Pennsylvania."
    }, {
      "heading" : "Appendix A. Overlapping Level Sets: Proof of Theorem 7",
      "text" : "Theorem 7 states that a property is not elicitable if there is a convex combination of one of its level sets in the m-product space that equals a convex combination of another one of its level sets in the m-product space. To reason about these level sets we will need the following theorem.\nTheorem A.1 (Theorem 3.5, Frongillo and Kash (2014)) The property Γ : P ′ → R (where P ′ ⊆ ∆Y ′) is directly elicitable by the loss function ` if and only if there exists some convex G : conv(P ′) → R̄ with G(P ′) ⊆ R, some D ⊆ δG, and some bijection φ : Γ(P ′) → D with Γ(p) = φ−1(D ∩ δGp), such that for all r ∈ R and y ∈ Y ′,\n`(r, y) = φ(r)(pr − y)−G(pr),\nwhere {pr} ⊆ P ′ satisfies r̂ = Γ(pr̂) for all r̂.\nHere δGr is the set of subgradients to G at r. Proof of Theorem 7 5 Let Y ′ = Ym and P ′ = Pm. Let ` be a loss function that elicits Γ of the form given by Theorem A.1, and let G, {pr} and φ be the corresponding values defined in Theorem A.1. We will let Γ′ : Pm ⇒ R be the property that is elicited by ` on conv(Pm).\nNote that Γ′ is not necessarily single-valued everywhere on conv(Pm). This is because we cannot guarantee that there is a unique value that minimizes the loss function for distributions in the interior of conv(Pm). However, we can show that whenever q ∈ conv(Pm) can be written as a convex combination of points on Pm that all have property value r then Ey∼q`(r, y) is uniquely minimized at r, thus r is the unique property value of Γ′ at q. This implies the theorem, as if q can be written as a convex combination of two separate level sets of Γ then there must not be an ` of the form specified in Theorem A.1 which elicits it.\nIf q = ∑k\ni=1 λip m i for p ∈ Γr∗ , λ1, . . . , λk ∈ [0, 1] and ∑k i=1 λi = 1 then\nEy∼q`(r, y) = φ(r)(qr − q)−G(qr)\n= φ(r) ( qr −\nk∑ i=1 λip m i\n) −G(qr)\n= k∑ i=1 λi (φ(r)(qr − pmi )−G(qr))\n= k∑ i=1 λiEy∼pmi L(r, y).\nWe know that each term of the final sum is uniquely minimized by r = r∗, thus Ey∼q`(r, y) is uniquely minimized by r∗.\n5. An alternate proof can also be constructed using results of Osband (1985)."
    }, {
      "heading" : "Appendix B. Regression",
      "text" : "In this section, we give a proof-of-concept showing that classic risk bounds for ERM can go through with only slight modification with multi-observation loss functions, under a natural assumption.\nRegression can be naturally formulated in the multi-observation setting as follows: Given a hypothesis class F : X → R and loss function ` : R × Ym → R, given access to an unknown distribution D on X and conditional distributions {Dx ∈ ∆Y : x ∈ X}, approximately minimize\nRisk(f) = Ex∼D,y∼Dx`(f(x), y1, . . . , ym).\nThe central challenge that arises, new to the multi-observation setting, is that the data we are given is of the form (x1, y1), . . . , (xn, yn) where xi ∼ D and yi ∼ Dxi i.i.d. We may only obtain a single y for any given x. In this section, we give an example of how this obstacle can be overcome under natural assumptions.\nFor simplicity, let us suppose thatX ⊆ Rd (in this section, d is not being used for dimensionality of the report space). The key idea is that, if the distribution Dx changes slowly as a function of x, then with enough samples, then a set of m close neighbors x1, . . . , xm can be viewed as approximating a single x with m “almost i.i.d.” conditional draws y1, . . . , ym. We formalize this intuition here using a Lipschitz condition on the total variation distance:\nDTV (Dx,Dx′) ≤ K‖x− x′‖2.\nHowever, the exact formalization is less important than the general idea, and we expect that future work will be able to prove similar results with a variety of similar assumptions.\nOur approach will be to cluster the data into groups of size m having nearby xs, then treat each group as a single sample of the form (x∗, y1, . . . , ym) with each yi approximately i.i.d. from Dx∗ . We then have n′ “samples” of this form, where n′ is the number of clusters. Of course, for this approach, it is necessary that that m be small compared to the total number of samples n ≈ n′m; we are often interested in the m = 2 case where our theory and simulations already show dramatic differences from the traditional case of m = 1.\nA classic risk bound translated into our setting is the following, whereRn denotes the Rademacher complexity of a hypothesis class.\nTheorem B.1 (Bartlett and Mendelson (2002)) Suppose ` is L-Lipschitz in its first argument and bounded by c, {xi}ni=1 are drawn i.i.d. from a distribution D, and each yi is drawn independently from Dxi . Then with probability at least 1− δ, for all f ∈ F ,\nRisk(f) ≤ Riskemp(f, {xi,yi}ni=1) + 2LRn(F) + c √ log 1/δ\n2n .\nHere the probability is over the randomness in {xi,yi}.\nIn other words, if we could actually sample a set yi = (yi,1, . . . , yi,m) from Dxi i.i.d., we would reduce to the standard setting. This theorem is leveraged to prove specific ERM risk bounds depending on F . Here we just show that this bound changes only slightly in the multi-observation case, with an increase in sample complexity.\nOur “cluster-points” algorithm roughly functions as follows: draw n i.i.d. data points x∗1, . . . , x ∗ n and n′ = Ω(n(m+ log(n/δ))/ ) “scatter points” of the form (x, y). Assign to each x∗i a set y ∗ i of sizem where for each y∗ij , its corresponding x has ‖x−x∗i ‖2 ≤ . We first show that this is possible with probability 1− δ, in two lemmas.\nLemma B.2 Given x ∈ [0, 1], < 1 and Ω((m+ log(1/δ′))/ ) i.i.d. from the uniform distribution over [0, 1], with probability at least 1− δ′, at least m of the samples fall within of x.\nProof The probability that a given sample falls within of x is at least . If we take s samples, then by a standard Chernoff bound we have that the probability of fewer than m samples falling within of x is upper bounded by\ne−(1− m s) 2 s/2.\nSolving for s when this is δ′ gives us the Lemma.\nLemma B.3 Let D be the uniform distribution on [0, 1]. n′ = O(n(m+ log(n/δ))/ ) samples of the form (x, y) where x ∼ D and y ∼ Dx are sufficient to find, with probability at least 1− δ, a set of n independent samples of the form (x∗, y∗1, . . . , y ∗ m) where x\n∗ ∼ D and the y∗i s are independent and of the form y∗i ∼ Dx′ for |x′ − x∗| ≤ .\nProof First we take m samples and use there x values as our m x∗s. For each x∗, we take a new set of n′/m = O((m + log(n/δ))/ ) samples (x1, y1), . . . , (xn′/m, yn′/m). Let j1, . . . , jm be m distinct indices such that for all i, |xji − x∗| ≤ . By Lemma B.2 (setting δ′ = δ/n) such a set will exist with probability at least 1− δ/n. We then construct the sample\n(x∗, y∗1, . . . , y ∗ m) = (x ∗, yj1 , . . . , yjm).\nBy a union bound, this algorithm will succeed with probability at least 1− δ, and the produced samples trivially fulfill the distributional requirements of the Lemma.\nNow we obtain the desired result. Note that we can choose as small as desired, e.g. = 1/n2, with a blowup of 1/ in the sample complexity. However, a more sophisticated bound would preferably use higher-powered concentration inequalities or a more carefully tailored assumption in order to get a bound holding with higher probability.\nTheorem B.4 Suppose ` is L-Lipschitz in its first argument and bounded by c, D is uniform on [0, 1], and {x∗i ,yi}ni=1 are drawn according to our cluster-points algorithm, taking n′ = O((m + log(n/δ))/ ) total samples. Then with probability at least 1− 2δ −mnK , for all f ∈ F ,\nRisk(f) ≤ Riskemp(f, {x∗i ,yi}ni=1) + 2LRn(F) + c √ log 1/δ\n2n .\nAgain the probability is over the randomness in {x∗i ,yi}.\nProof With probability 1 − δ, our “cluster-points” algorithm succeeds in finding {x∗i }ni=1 drawn i.i.d. and {yi}ni=1 drawn from -close points. We wish to consider Riskemp(f, {x∗i ,yi}ni=1), where each yi is Km -close in total variation distance to y∗i , as each member is K close. So the whole quantity, by the properties of total variation distance, is mnK -close to Riskemp(f, {xi,yi}ni=1), and we apply Theorem B.1."
    }, {
      "heading" : "Appendix C. Zero sets of Polynomials over the Real Numbers",
      "text" : "Consider a polynomial f(x1, . . . , xn) in the set R[x1, . . . , xn] of polynomials in n variables with real coefficients. The zero set of f(x1, . . . , xn) is by definition the set\nZ(f(x1, . . . , xn)) := {(α1, . . . , αn) ∈ Rn : f(α1, . . . , αn) = 0} ⊆ Rn.\nRecall that a nonconstant polynomial f(x1, . . . , xn) ∈ R[x1, . . . , xn] is said to be irreducible if it cannot be written as the product of two polynomials in R[x1, . . . , xn] of strictly lower degree. Recall also that a subset U ⊆ Rn is said to be open in the Euclidean topology if for every α = (α1, . . . , αn) ∈ U , there exists a real number α > 0, depending on α, such that the ball of radius α centered at α, B α(α1, . . . , αn), is contained in U :\nB α(α1, . . . , αn) := { (β1, . . . , βn) ∈ Rn : √ (β1 − α1)2 + · · ·+ (βn − αn)2 < α } ⊆ U.\nWith this terminology, we can state the following theorem:\nTheorem C.1 Suppose that f(x1, . . . , xn) ∈ R[x1, . . . , xn] is a nonconstant irreducible polynomial, and U ⊆ Rn is an open subset in the Euclidean topology. If there is a point\n(α1, . . . , αn) ∈ Z(f(x1, . . . , xn)) ∩ U ⊆ Rn\nsuch that ( ∂f\n∂x1 (α1, . . . , αn), . . . ,\n∂f\n∂xn (α1, . . . , αn)\n) 6= (0, . . . , 0) ∈ Rn, (5)\nthen there are no nonzero polynomials of degree less than the degree of f(x1, . . . , xn) that vanish at every point of the zero set Z(f(x1, . . . , xn)) ∩ U .\nWe expect the theorem is well known; for instance, the case where U = Rn is a special case of (Bochnak et al., 1998, Thm. 4.5.1). The proof of (Bochnak et al., 1998, Thm. 4.5.1) easily generalizes to our situation. For the convenience of the reader, in Theorem D.1 below we include a generalization of (Bochnak et al., 1998, Thm. 4.5.1) that impiles Theorem C.1.\nRemark C.2 (Checking the conditions of Theorem C.1) There are many techniques for checking that a polynomial f(x1, . . . , xn) ∈ R[x1, . . . , xn] is irreducible and satisfies the condition (5) for all (α1, . . . , αn) ∈ Z(f(x1, . . . , xn)) ∩ U , and therefore satisfies the hypotheses of Theorem C.1. For n ≥ 2, we recall the following elementary condition that suffices. Suppose f(x1, . . . , xn) is a nonconstant polynomial of degree d. The homogenization of f(x1, . . . , xn) is the degree d homogeneous (all monomials of degree d) polynomial F (X0, X1, . . . , Xn) ∈ R[X0, . . . , Xn] that is obtained from f(x1, . . . , xn) by replacing xi with Xi for i = 1, . . . , n, and then multiplying each monomial by a power of X0 until it is of degree d. For instance, if f(x1, x2) = x21 + 2x2 + 3, then F (X0, X1, X2) = X 2 1 + 2X0X2 + 3X\n2 0 . If the complex zero set{\n(α0, . . . , αn) ∈ Cn+1 : ∂F\n∂X0 (α0, . . . , αn) = · · · =\n∂F\n∂Xn (α0, . . . , αn) = (0, . . . , 0)\n} ⊆ Cn+1\n(6) is equal to {(0, . . . , 0)} or ∅, then f(x1, . . . , xn) is irreducible and satisfies (5) for all (α1, . . . , αn) ∈ Z(f(x1, . . . , xn)). This is by no means a necessary condition for f(x1, . . . , xn) to satisfy the conditions of Theorem C.1, but it is easy to implement in examples. There are a number of other techniques that can be used, including using computer algebra systems.\nUsing the technique outlined in the remark, and standard results in algebraic geometry, it is elementary to establish the following corollary:\nCorollary C.3 Let n ≥ 2, let U ⊆ Rn be a nonempty open subset in the Euclidean topology, let f(x1, . . . , xn) ∈ R[x1, . . . , xn], and for each c ∈ R define\nfc(x1, . . . , xn) := f(x1, . . . , xn) + c.\nLet Fc(X0, · · · , Xn) be the homogenization of fc(x1, . . . , xn). If for some c0 ∈ R the complex zero set (6) for Fc0(X0, . . . , Xn) is equal to {(0, . . . , 0)} ⊆ Cn+1 or ∅, then there is a nonempty open subset B ⊆ R in the Euclidean topology such that for all c ∈ B, there are no nonzero polynomials of degree less than d that vanish at every point of the zero set Z(fc(x1, . . . , xn)) ∩ U .\nAs a consequence:\nExample C.1 For a given pair of natural numbers n and d with n ≥ 2, suppose that:\n• For c ∈ R, we set fc(x1, . . . , xn) := xd1 + · · ·+ xdn + (1− x1 − · · · − xn)d + c. • U := {(α1, . . . , αn) ∈ Rn : α1, . . . , αn > 0, ∑n i=1 αi < 1}.\nThere exists a nonempty open subsetB ⊆ R in the Euclidean topology such that for all c ∈ B, there are no nonzero polynomials of degree less than d that vanish at every point ofZ(fc(x1, . . . , xn))∩U .\nWe can confirm this using the approach in Corollary C.3:\nFc = cX d 0 +X d 1 + · · ·+Xdn + (X0 −X1 − · · · −Xn)d\n∂X0Fc = cdX d−1 0 + d(X0 −X1 − · · · −Xn) d−1, ∂X1Fc = dX d−1 1 − d(X0 −X1 − · · · −Xn) d−1,\n...\n∂XnFc = dX d−1 n − d(X0 −X1 − · · · −Xn)d−1.\nTo use Corollary C.3, we need to consider the complex zero set (6):{ (α0, . . . , αn) ∈ Cn+1 : ∂X0Fc(α0, . . . , αn) = · · · = ∂XnFc(α0, . . . , αn) = (0, . . . , 0) } ⊆ Cn+1,\nand show that for some c ∈ R it is either empty or equal to {(0, . . . , 0)}. We consider the case c = 0. Under this assumption, we have\n0 = ∂X0Fc = cdX d−1 0 + d(X0 −X1 − · · · −Xn) d−1 ⇐⇒ (X0 −X1 − · · · −Xn) = 0.\nThen, assuming X0 −X1 − · · · −Xn = 0, we have for i = 1, . . . , n that\n0 = ∂XiFc = dX d−1 i − d(X0 −X1 − · · · −Xn) d−1 ⇐⇒ Xi = 0.\nWith this new information, returning to ∂X0Fc, we see that we also must have\nX0 = 0.\nIn other words, the complex zero set is {(0, . . . , 0)} ⊆ Cn, so that our example satisfies the conditions of Corollary C.3.\nRemark C.4 Most polynomials f(x1, . . . , xn) ∈ R[x1, . . . , xn], n ≥ 2, satisfy the hypotheses of Corollary C.3. More precisely, there is a dense open subset (the complement of linear subspace) of an ( n+d d ) -dimensional real vector space that parameterizes degree-d polynomials in n variables. That subset contains a dense open subset Ω (the complement of the discriminant locus; see e.g., Fulton (1998)) such that every f(x1, . . . , xn) ∈ Ω satisfies the hypotheses of the corollary; i.e., there is some c0 ∈ R (for instance c0 = 0) such that the complex zero set (6) for Fc0(X0, . . . , Xn) is equal to {(0, . . . , 0)} ⊆ Cn+1 or ∅. On the other hand, as described in Example C.2 below, it is easy to find polynomials f(x1, . . . , xn) ∈ R[x1, . . . , xn] of degree d ≥ 2, and nonempty open subsets U ⊆ Rn, so that for every c ∈ R there exist nonzero polynomials of degree less than d that vanish at every point of the zero set Z(fc(x1, . . . , xn)) ∩ U .\nExample C.2 Consider the polynomial f(x1, . . . , xn) = x21, and take U = R>0×Rn−1. Then for every c ∈ R there is a linear polynomial that vanishes at every point of Z(fc(x1, . . . , xn))∩U ; for c > 0, we can take any linear polynomial, and for c ≤ 0, we can take x1 − √ −c.\nWe can construct many more similar examples in the following way. Let h(x1) ∈ R[x1] be a polynomial of degree at least 2. We have for every c ∈ R that h(x1)+c factors in R[x1] as a product of linear terms and a product of quadratic terms each having no real root. For simplicity, let us assume that for all c 6= 0, the polynomial h(x1) + c has a root that is not real; e.g., h(x1) = xm1 for some natural number m ≥ 3. Let g(x1, . . . , xn) ∈ R[x1, . . . , xn] be any nonconstant polynomial. Let λ be a real root of h(x1) (if there is one), and let U be the complement of the zero set of g(x1, . . . , xn) − λ, or simply Rn if there is no real root. Then f(x1, . . . , xn) := h(g(x1, . . . , xn)) has the property that for every c ∈ R, there is a nonzero polynomial of degree less than the degree of f(x1, . . . , xn) that vanishes at every point of the zero set Z(fc(x1, . . . , xn)) ∩ U .\nRemark C.5 Theorem C.1 and Corollary C.3 are not interesting in the case n = 1. For f(x) ∈ R[x] (irreducible or not) there are no nonzero polynomials of degree less than d that vanish at every point of the zero set Z(f(x)) ∩ U if and only if all of the roots of f(x) are real, distinct, and lie in U . There are standard techniques to check this condition (e.g., (Bochnak et al., 1998, pp.12–14)). In Example C.1 with n = 1, by inspection one finds that for d = 1, 2 the condition holds if and only if −1 < c < 1, and for d = 3, 4 the condition does not hold for any c."
    }, {
      "heading" : "Appendix D. The Real Nullstellenstatz for Principal Ideals and Open Sets",
      "text" : "The main goal of this section is to prove the following theorem generalizing the well known real Nullstellenstatz for principal ideals (e.g., (Bochnak et al., 1998, Thm. 4.5.1)) to allow for Euclidean open sets.\nTheorem D.1 Let K be a real closed field (e.g., K = R). Let f(x1, . . . , xn) ∈ K[x1, . . . , xn] be a nonconstant polynomial, and let U ⊆ Kn be an open subset in the Euclidean topology. Suppose that\nf(x1, . . . , xn) = f1(x1, . . . , xn) m1 · · · fr(x1, . . . , xn)mr (7)\nis a factorization into powers of distinct nonconstant irreducible polynomials. The following are equivalent:\n1. (f) = I(Z(f) ∩ U).\n2. m1 = · · · = mr = 1 and for each i = 1, . . . , r there is a point α(i) ∈ Z(fi) ∩ U with\n(∂x1fi(α (i)), . . . , ∂xnfi(α (i))) 6= 0 ∈ Kn.\nFor K = R, this is equivalent to having for each i that Z(fi) ∩ U is a smooth (n − 1)- dimensional submanifold of an open neighborhood of α(i).\n3. m1 = · · · = mr = 1 and for each i = 1, . . . , r the sign of the polynomial fi changes on an open ball in U (i.e., for i = 1, . . . , n there is an open ball B(i) ⊆ U and points α(i), β(i) ∈ B(i) such that fi(α(i))fi(β(i)) < 0).\n4. m1 = · · · = mr = 1 and for each i = 1, . . . , r the semi-algebraic Krull dimension of the topological spaceZ(fi)∩U (i.e., the Krull dimension of the ring K[x1, . . . , xn]/I(Z(fi)∩U)) satisfies\ndim(Z(fi) ∩ U) = n− 1.\nWe expect this result is known to the experts (the case where f is irreducible and U = Rn is (Bochnak et al., 1998, Thm. 4.5.1)), but for lack of a reference we provide a proof in §D.6. See §D.1 for an explanation of the notation.\nRemark D.2 The case n = 1 is elementary and has the following simple interpretation: we have (f(x)) = I(Z(f(x)) ∩ U) if and only if all of the roots of f(x) in an algebraic closure K are distinct, and lie in U ⊆ K. There are standard techniques to check this condition (e.g., (Bochnak et al., 1998, pp.12–14)). Remark D.3 If f(x1, . . . , xn) is given as in (7), then the radical of the ideal (f) is the ideal√ (f) = (f1 · · · fr). Thus Theorem D.1 also gives conditions for when there is an equality √ (f) = I(Z(f) ∩ U).\nD.1. Notation and conventions\nLet K be a field. Given an ideal I ⊆ K[x1, . . . , xn] we will be interested in both the closed subscheme V (I) ⊆ AnK , as well as the zero set\nV (I)(SpecK) ' ZK(I) := {α ∈ Kn : f(α) = 0, for all f ∈ I} ⊆ Kn.\nIf the field is clear from the context, we will write Z(I) = ZK(I). For a subset S ⊆ Kn, we denote as usual the ideal of polynomials vanishing on S as\nI(S) := {g(x1, . . . , xn) ∈ K[x1, . . . , xn] : g(s) = 0 for all s ∈ S}.\nWe refer the reader to (Bochnak et al., 1998, Def. 1.1.9, Def. 1.2.1) for a review of the definition of a real closed field. In particular, such a field K is of characteristic 0 and is an ordered field; the Euclidean topology on Kn then has a basis given by the open balls\nB (α) := {β ∈ Kn : n∑ i=1 (βi − αi)2 < 2}\nfor all α ∈ Kn and all ∈ K with > 0.\nD.2. The principal Nullstellensatz\nFor an ideal I ⊆ K[x1, . . . , xn], there is a natural inclusion √ I ⊆ I(Z(I)). (8)\nHilbert’s Nullstellensatz asserts that over an algebraically closed field K = K, this inclusion is an equality. Focusing on principal ideals, this reads√\n(f) = I(Z(f)), (K = K); (9)\nin other words (f) = I(Z(f)) whenever f is reduced and K = K is algebraically closed. Over nonalgebraically closed fields (9) clearly fails; i.e., one may have√\n(f) ( I(Z(f)). For instance, trivially, one has in Q[x] that √\n(x2 + 1) = (x2 + 1) ( Q[x] = I(∅) = I(Z(x2 + 1)). The following example is a little more interesting:\nExample D.1 Consider f(x, y) = x2+y2−x3 ∈ R[x, y], and the zero setZ(f) ⊆ R2. It is a cubic plane curve with an isolated point at (0, 0) ∈ R2. In particular, if we takeU = B (0, 0) to be a small ball around (0, 0) in R2, then √ (x2 + y2 − x3) = (x2+y2−x3) 6= (x, y) = I(Z(x2+y2−x3)∩U). On the other hand, it is true that (x2 + y2 − x3) = I(Z(x2 + y2 − x3)).\nD.3. The connection with dimension\nProposition D.4 Let f(x1, . . . , xn) ∈ K[x1, . . . , xn] be a nonconstant irreducible polynomial, and let U ⊆ Kn be any subset. The following are equivalent:\n1. (f) = I(Z(f) ∩ U).\n2. The semi-algebraic Krull dimension of the topological space Z(f)∩U (i.e., the Krull dimension of the ring K[x1, . . . , xn]/I(Z(f) ∩ U)) satisfies\ndim(Z(f) ∩ U) = n− 1.\nProof (1) =⇒ (2). By assumption we have (f) = I(Z(f) ∩ U). Now the Krull dimension of K[x1, . . . , xn] is n (e.g., (Atiyah and Macdonald, 1969, Exe. 11.7)). Consequently, since f is neither a zero divisor nor a unit, we have that the Krull dimension of K[x1, . . . , xn]/(f) is (n− 1) (e.g., (Atiyah and Macdonald, 1969, Cor. 11.7); using that f is irreducible, this is even easier). Note that this direction does not require that f be irreducible.\n(2) =⇒ (1). We have inclusions\n(f) ⊆ I(Z(f) ∩ U) ⊆ K[x1, . . . , xn]. (10)\nAs above, since f is neither a zero divisor nor a unit, we have that the Krull dimension of the ring K[x1, . . . , xn]/(f) is (n− 1). By assumption, the Krull dimension of K[x1, . . . , xn]/I(Z(f)∩U) is also (n− 1). Now since (f) is prime (finally using that f is irreducible), and has the same Krull dimension as the ideal I(Z(f)∩U), it follows from the containment (10) and the definition of Krull dimension that the two ideals are equal.\nD.4. The connection with smoothness\nWe say a zero set Z(I) ⊆ Kn is smooth at a point α ∈ Z(I) if the associated scheme V (I) ⊆ AnK is smooth at the point (x1−α1, . . . , xn−αn) ∈ V (I). We will also simply say that V (I) is smooth at α. Recall that if I = (f) is principal, and α ∈ Z(f), then V (f) is smooth at (x1−α1, . . . , xn−αn) if and only if (∂x1f(α), . . . , ∂xnf(α)) 6= 0 ∈ Kn.\nLemma D.5 Suppose K is perfect. Let f(x1, . . . , xn) ∈ K[x1, . . . , xn] be a nonconstant polynomial, and let U ⊆ Kn be any subset. Then:\n1. (f) = I(Z(f) ∩ U),\nimplies\n(2) There is a point α(0) ∈ Z(f) ∩ U with\n(∂x1f(α (0)), . . . , ∂xnf(α (0))) 6= 0 ∈ Kn.\nIn other words, there is a point in U at which V (f) is a smooth scheme.\nProof We will show the contrapositive. Suppose that (2) fails. This means that ∂x1f, . . . , ∂xnf ∈ I(Z(f) ∩ U). But since f is nonconstant and K is perfect, either there is an i such that ∂xif is nonzero, or char(K) = p > 0 and there exists a polynomial g ∈ K[x1, . . . , xn] such that f = gp (e.g., (Cox et al., 2015, Ch. 9 Ex. 10, p.524)). In the first case, since ∂xif is nonzero of degree less than the degree of f , it cannot be a multiple of f , and therefore is not in (f). Thus (f) ( I(Z(f) ∩ U), and (1) fails. In the second case, where f = gp, we have g ∈ I(Z(f) ∩ U), while g /∈ (f), again for degree reasons, so that (1) also fails in this case.\nThe following example shows that the converse to Lemma D.5 need not hold.\nExample D.2 Let K = Q and let f(x1, x2) = x31 + x32 − 1. Then Z(f) ⊆ Q2 is a finite set of points, and in particular one can show that (f) ( I(Z(f)). On the other hand, at the point say (1, 0) ∈ Z(f), one has (∂x1f(1, 0), ∂x2f(1, 0)) = (3, 0) 6= 0 ∈ Q2.\nNevertheless, a converse to Lemma D.5 does hold over the real and complex numbers. This is essentially because the implicit function theorem asserts that condition (2) implies that the zero set is an (n − 1)-dimensional manifold in a neighborhood of the given point. In fact, one can also establish the converse over real closed fields:\nLemma D.6 Suppose K = K is real closed or equal to C. Let f(x1, . . . , xn) ∈ K[x1, . . . , xn] be a nonconstant irreducible polynomial, and let U ⊆ Kn be an open subset in the Euclidean topology. Then:\n1. (f) = I(Z(f) ∩ U),\nis implied by\n(2) There is a point α(0) ∈ Z(f) ∩ U with\n(∂x1f(α (0)), . . . , ∂xnf(α (0))) 6= 0 ∈ Kn.\nIn other words, there is a point in U at which V (f) is a smooth scheme.\nProof Consider the case K = K is real closed. Let (Z(f) ∩ U)Zar ⊆ Kn be the closure in the Zariski topology. Now using condition (2), and (iii) =⇒ (ii) of (Bochnak et al., 1998, Prop. 3.3.10), we have that dimK[x1, . . . , xn]/I((Z(f) ∩ U) Zar ) = n − 1. (We are applying (Bochnak et al., 1998, Prop. 3.3.10) with V = (Z(f) ∩ U)Zar and P1 = f .) Now we observe that I(Z(f)∩U) = I((Z(f) ∩ U)Zar), and conclude that dim(Z(f)∩U) = n−1. Note that so far we did not use that f was irreducible, as this is not required in (Bochnak et al., 1998, Prop. 3.3.10). To conclude (1), we use Proposition D.4, and the assumption that f is irreducible. The case where K = C is standard, and can be proven in a similar way.\nD.5. The connection with the sign of the polynomial\nLemma D.7 Suppose K = K is real closed. Let f(x1, . . . , xn) ∈ K[x1, . . . , xn] be a nonconstant irreducible polynomial, and let U ⊆ Kn be an open subset in the Euclidean topology. Then the following are equivalent:\n1. (f) = I(Z(f) ∩ U).\n2. The sign of the polynomial f changes on an open ball in U (i.e., there is an open ball B ⊆ U such that f(α)f(β) < 0 for some α, β ∈ B ).\nProof (1) =⇒ (2). Assuming (1), then from Lemma D.5, there is a point α(0) ∈ Z(f) ∩ U with (∂x1f(α (0)), . . . , ∂xnf(α (0))) 6= 0 ∈ Kn. In other words, there is an i such that ∂xif(α(0)) 6= 0. Then consider the polynomial in one variable\nφ(xi) := f(α (0) 1 , . . . , xi, . . . , α (0) n ).\nWe have φ(α(0)i ) = 0. But since φ ′(α (0) i ) = ∂xif(α (0) i ) is non-zero, the function φ(xi) is monotone in a real interval around α(0)i , and so it changes sign (Bochnak et al., 1998, Cor. 1.2.7). Therefore f changes sign. (Note that we did not use that f was irreducible.)\n(2) =⇒ (1). (Bochnak et al., 1998, Lem. 4.5.2) states the following: Let B ⊆ Kn be an open ball (including the case where B = Kn) and let U1 and U2 be two disjoint nonempty semialgebraic open subsets of B . Then we have dim(B − (U1 ∪ U2)) ≥ n− 1. Now apply this in our situation, with\nU1 = {α ∈ B : f(α) > 0} and U2 = {α ∈ B : f(α) < 0},\nso that B − (U1 ∪ U2) = Z(f) ∩B . Then\nn− 1 = dimZ(f) ≥ dim(Z(f) ∩ U) ≥ dim(Z(f) ∩B ) ≥ n− 1.\nAs mentioned above, we have the equality dimZ(f) = n − 1 on the left since f is neither a zero divisor nor a unit. Note that so far we did not use that f was irreducible. To conclude (1), we use Proposition D.4, and the assumption that f is irreducible.\nD.6. Proof of Theorem D.1\nProof [Proof of Theorem D.1] We have now proved the theorem under the hypothesis that f is irreducible (Proposition D.4, Lemma D.5, Lemma D.6, Lemma D.7). We now reduce to this case. First, it is clear that (2) ⇐⇒ (3) ⇐⇒ (4), from the irreducible case. Also, it is clear that if (1) holds (i.e., (f) = I(Z(f) ∩ U)), we must have that m1 = m2 = · · · = mr = 1. Indeed, if say m1 > 1, then f1fm22 · · · fmrr ∈ I(Z(f)∩U), but for degree reasons f1f m2 2 · · · fmrr is not a multiple of f = fm11 · · · fmrr and thus (1) fails. So from here on, we assume m1 = m2 = · · · = mr = 1. (1) =⇒ (2). Suppose that (2) fails. Then there is some i, j so that ∂xjfi ∈ I(Z(fi) ∩ U) and is nonzero. Therefore f1 · · · ∂xjfi · · · fr ∈ I(Z(f) ∩ U)) and is nonzero. But for degree reasons, it is not a multiple of f = f1 · · · fi · · · fr and thus (1) fails.\n(2) =⇒ (1). This follows from the fact that r⋂ i=1 (fi) = (f1 · · · fr) (K[x1, . . . , xn] is a UFD)\n= (f)\n⊆ I(Z(f) ∩ U))\n= I ( r⋃ i=1 (Z(fi) ∩ U) )\n= r⋂ i=1 I(Z(fi) ∩ U),\nsince, assuming (2) and the special case of Theorem D.1 for irreducible polynomials, then for all i, we have (fi) = I(Z(fi) ∩ U), forcing the containment above to be an equality."
    } ],
    "references" : [ {
      "title" : "On consistent surrogate risk minimization and property elicitation",
      "author" : [ "Arpit Agarwal", "Shivani Agarwal" ],
      "venue" : "In JMLR Workshop and Conference Proceedings,",
      "citeRegEx" : "Agarwal and Agarwal.,? \\Q2015\\E",
      "shortCiteRegEx" : "Agarwal and Agarwal.",
      "year" : 2015
    }, {
      "title" : "Introduction to commutative algebra",
      "author" : [ "M.F. Atiyah", "I.G. Macdonald" ],
      "venue" : null,
      "citeRegEx" : "Atiyah and Macdonald.,? \\Q1969\\E",
      "shortCiteRegEx" : "Atiyah and Macdonald.",
      "year" : 1969
    }, {
      "title" : "Rademacher and gaussian complexities: Risk bounds and structural results",
      "author" : [ "Peter L Bartlett", "Shahar Mendelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bartlett and Mendelson.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bartlett and Mendelson.",
      "year" : 2002
    }, {
      "title" : "Robust optimization–a comprehensive survey",
      "author" : [ "Hans-Georg Beyer", "Bernhard Sendhoff" ],
      "venue" : "Computer methods in applied mechanics and engineering,",
      "citeRegEx" : "Beyer and Sendhoff.,? \\Q2007\\E",
      "shortCiteRegEx" : "Beyer and Sendhoff.",
      "year" : 2007
    }, {
      "title" : "Real algebraic geometry, volume 36 of Ergebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and Related Areas (3)",
      "author" : [ "Jacek Bochnak", "Michel Coste", "Marie-Françoise Roy" ],
      "venue" : "ISBN 3-540-64663-9",
      "citeRegEx" : "Bochnak et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Bochnak et al\\.",
      "year" : 1998
    }, {
      "title" : "Mathematical analysis: An introduction",
      "author" : [ "Andrew Browder" ],
      "venue" : "Undergraduate Texts in Mathematics. Springer-Verlag, New York,",
      "citeRegEx" : "Browder.,? \\Q1996\\E",
      "shortCiteRegEx" : "Browder.",
      "year" : 1996
    }, {
      "title" : "Ideals, varieties, and algorithms",
      "author" : [ "David A. Cox", "John Little", "Donal O’Shea" ],
      "venue" : "Undergraduate Texts in Mathematics. Springer, Cham, fourth edition,",
      "citeRegEx" : "Cox et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cox et al\\.",
      "year" : 2015
    }, {
      "title" : "The statistical analysis of series of events",
      "author" : [ "David R Cox", "Peter AW Lewis" ],
      "venue" : "Monographs on Applied Probability and Statistics,",
      "citeRegEx" : "Cox and Lewis.,? \\Q1966\\E",
      "shortCiteRegEx" : "Cox and Lewis.",
      "year" : 1966
    }, {
      "title" : "The relation between the number of species and the number of individuals in a random sample of an animal population",
      "author" : [ "Ronald A. Fisher", "A. Steven Corbet", "Carrington B. Williams" ],
      "venue" : "The Journal of Animal Ecology,",
      "citeRegEx" : "Fisher et al\\.,? \\Q1943\\E",
      "shortCiteRegEx" : "Fisher et al\\.",
      "year" : 1943
    }, {
      "title" : "General truthfulness characterizations via convex analysis",
      "author" : [ "Rafael Frongillo", "Ian Kash" ],
      "venue" : "In Web and Internet Economics,",
      "citeRegEx" : "Frongillo and Kash.,? \\Q2014\\E",
      "shortCiteRegEx" : "Frongillo and Kash.",
      "year" : 2014
    }, {
      "title" : "Vector-Valued Property Elicitation",
      "author" : [ "Rafael Frongillo", "Ian Kash" ],
      "venue" : "In Proceedings of the 28th Conference on Learning Theory, pages",
      "citeRegEx" : "Frongillo and Kash.,? \\Q2015\\E",
      "shortCiteRegEx" : "Frongillo and Kash.",
      "year" : 2015
    }, {
      "title" : "On Elicitation Complexity and Conditional Elicitation",
      "author" : [ "Rafael Frongillo", "Ian A. Kash" ],
      "venue" : "arXiv preprint arXiv:1506.07212,",
      "citeRegEx" : "Frongillo and Kash.,? \\Q2015\\E",
      "shortCiteRegEx" : "Frongillo and Kash.",
      "year" : 2015
    }, {
      "title" : "On Elicitation Complexity",
      "author" : [ "Rafael Frongillo", "Ian A. Kash" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Frongillo and Kash.,? \\Q2015\\E",
      "shortCiteRegEx" : "Frongillo and Kash.",
      "year" : 2015
    }, {
      "title" : "Elicitation for Aggregation",
      "author" : [ "Rafael M. Frongillo", "Yiling Chen", "Ian A. Kash" ],
      "venue" : "Proceedings of the 29th AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Frongillo et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Frongillo et al\\.",
      "year" : 2015
    }, {
      "title" : "Intersection theory, volume 2 of Ergebnisse der Mathematik und ihrer Grenzgebiete",
      "author" : [ "William Fulton" ],
      "venue" : null,
      "citeRegEx" : "Fulton.,? \\Q1998\\E",
      "shortCiteRegEx" : "Fulton.",
      "year" : 1998
    }, {
      "title" : "Making and Evaluating Point Forecasts",
      "author" : [ "T. Gneiting" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Gneiting.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gneiting.",
      "year" : 2011
    }, {
      "title" : "Dimensionality reduction by learning an invariant mapping",
      "author" : [ "Raia Hadsell", "Sumit Chopra", "Yann LeCun" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Hadsell et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hadsell et al\\.",
      "year" : 2006
    }, {
      "title" : "Eliciting truthful answers to multiple-choice questions",
      "author" : [ "Nicolas S. Lambert", "Yoav Shoham" ],
      "venue" : "In Proceedings of the 10th ACM Conference on Electronic Commerce,",
      "citeRegEx" : "Lambert and Shoham.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lambert and Shoham.",
      "year" : 2009
    }, {
      "title" : "Eliciting properties of probability distributions",
      "author" : [ "Nicolas S. Lambert", "David M. Pennock", "Yoav Shoham" ],
      "venue" : "In Proceedings of the 9th ACM Conference on Electronic Commerce,",
      "citeRegEx" : "Lambert et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Lambert et al\\.",
      "year" : 2008
    }, {
      "title" : "Elicitation and Evaluation of Statistical Forecasts",
      "author" : [ "N.S. Lambert" ],
      "venue" : null,
      "citeRegEx" : "Lambert.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lambert.",
      "year" : 2011
    }, {
      "title" : "Providing Incentives for Better Cost Forecasting",
      "author" : [ "Kent Harold Osband" ],
      "venue" : "University of California, Berkeley,",
      "citeRegEx" : "Osband.,? \\Q1985\\E",
      "shortCiteRegEx" : "Osband.",
      "year" : 1985
    }, {
      "title" : "Facenet: A unified embedding for face recognition and clustering",
      "author" : [ "Florian Schroff", "Dmitry Kalenichenko", "James Philbin" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Schroff et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schroff et al\\.",
      "year" : 2015
    }, {
      "title" : "Mutual fund performance",
      "author" : [ "William F Sharpe" ],
      "venue" : "Journal of Business,",
      "citeRegEx" : "Sharpe.,? \\Q1966\\E",
      "shortCiteRegEx" : "Sharpe.",
      "year" : 1966
    }, {
      "title" : "Elicitation and Identification of Properties",
      "author" : [ "Ingo Steinwart", "Chloé Pasin", "Robert Williamson", "Siyu Zhang" ],
      "venue" : "In Proceedings of The 27th Conference on Learning Theory,",
      "citeRegEx" : "Steinwart et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Steinwart et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning deep embeddings with histogram loss",
      "author" : [ "Evgeniya Ustinova", "Victor Lempitsky" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Ustinova and Lempitsky.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ustinova and Lempitsky.",
      "year" : 2016
    }, {
      "title" : "∆Y ′) is directly elicitable by the loss function ` if and only if there exists some convex G : conv(P ′) → R̄ with G(P ′) ⊆ R, some D ⊆ δG, and some bijection φ : Γ(P ′) → D with Γ(p) = φ−1(D ∩ δGp)",
      "author" : [ "Frongillo", "Kash" ],
      "venue" : null,
      "citeRegEx" : "Frongillo and Kash,? \\Q2014\\E",
      "shortCiteRegEx" : "Frongillo and Kash",
      "year" : 2014
    }, {
      "title" : "Lem. 4.5.2) states the following: Let B ⊆ Kn be an open ball (including the case where B = Kn) and let U1 and U2 be two disjoint nonempty semialgebraic open subsets of B",
      "author" : [ "Bochnak et al", "Cor" ],
      "venue" : "(Bochnak et al.,",
      "citeRegEx" : "al. et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "As the choice of loss function used in ERM may have a large impact on the model chosen, how should one choose this loss? A growing body of work in property elicitation seeks to answer this question, by viewing a loss function as “incentivizing” the prediction of a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is well-known that squared loss elicits the mean, and hence least-squares regression finds the best fit to the conditional means of the data.",
      "startOffset" : 298,
      "endOffset" : 414
    }, {
      "referenceID" : 15,
      "context" : "As the choice of loss function used in ERM may have a large impact on the model chosen, how should one choose this loss? A growing body of work in property elicitation seeks to answer this question, by viewing a loss function as “incentivizing” the prediction of a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is well-known that squared loss elicits the mean, and hence least-squares regression finds the best fit to the conditional means of the data.",
      "startOffset" : 298,
      "endOffset" : 414
    }, {
      "referenceID" : 23,
      "context" : "As the choice of loss function used in ERM may have a large impact on the model chosen, how should one choose this loss? A growing body of work in property elicitation seeks to answer this question, by viewing a loss function as “incentivizing” the prediction of a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is well-known that squared loss elicits the mean, and hence least-squares regression finds the best fit to the conditional means of the data.",
      "startOffset" : 298,
      "endOffset" : 414
    }, {
      "referenceID" : 0,
      "context" : "As the choice of loss function used in ERM may have a large impact on the model chosen, how should one choose this loss? A growing body of work in property elicitation seeks to answer this question, by viewing a loss function as “incentivizing” the prediction of a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is well-known that squared loss elicits the mean, and hence least-squares regression finds the best fit to the conditional means of the data.",
      "startOffset" : 298,
      "endOffset" : 414
    }, {
      "referenceID" : 18,
      "context" : "The question of how many such auxiliary statistics are required gives rise to the concept of elicitation complexity; since the variance cannot be elicited with one but can with two, we say it is 2-elicitable (Lambert et al., 2008; Frongillo and Kash, 2015c).",
      "startOffset" : 208,
      "endOffset" : 257
    }, {
      "referenceID" : 3,
      "context" : "observations are readily obtained include: active learning, uncertainty quantification & robust engineering design (Beyer and Sendhoff, 2007), and replication of scientific experiments.",
      "startOffset" : 115,
      "endOffset" : 141
    }, {
      "referenceID" : 16,
      "context" : "Multi-observation losses have been previously introduced to learn embeddings (Hadsell et al., 2006; Schroff et al., 2015; Ustinova and Lempitsky, 2016), though an explicit property/statistic is never discussed.",
      "startOffset" : 77,
      "endOffset" : 151
    }, {
      "referenceID" : 21,
      "context" : "Multi-observation losses have been previously introduced to learn embeddings (Hadsell et al., 2006; Schroff et al., 2015; Ustinova and Lempitsky, 2016), though an explicit property/statistic is never discussed.",
      "startOffset" : 77,
      "endOffset" : 151
    }, {
      "referenceID" : 24,
      "context" : "Multi-observation losses have been previously introduced to learn embeddings (Hadsell et al., 2006; Schroff et al., 2015; Ustinova and Lempitsky, 2016), though an explicit property/statistic is never discussed.",
      "startOffset" : 77,
      "endOffset" : 151
    }, {
      "referenceID" : 13,
      "context" : "Related work Our work is inspired in part by Frongillo et al. (2015) which proposes a way to elicit the confidence (inverse of variance) of an agent’s estimate of the bias of a coin by simply flipping it twice.",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 14,
      "context" : "Lambert et al. (2008); Steinwart et al.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 14,
      "context" : "Lambert et al. (2008); Steinwart et al. (2014)).",
      "startOffset" : 0,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "Following Frongillo and Kash (2015a), we will often assume that properties are identifiable.",
      "startOffset" : 10,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : "The question of elicitation complexity, studied by Lambert et al. (2008) and Frongillo and Kash (2015c), is how many dimensions d are needed to indirectly elicit the property of interest Γ via some elicitable Γ̂ : P → Rd; one hopes that d is much smaller than |Y|.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 9,
      "context" : "(2008) and Frongillo and Kash (2015c), is how many dimensions d are needed to indirectly elicit the property of interest Γ via some elicitable Γ̂ : P → Rd; one hopes that d is much smaller than |Y|.",
      "startOffset" : 11,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "Geometric Fundamentals The most basic (yet powerful) lower bound in property elicitation says that elicitable properties’ level sets must be convex sets (Lambert et al., 2008).",
      "startOffset" : 153,
      "endOffset" : 175
    }, {
      "referenceID" : 9,
      "context" : "A tighter sufficient condition is given by Frongillo and Kash (2014), which states that essentially all loss functions eliciting a property on any set, such as Pm, also elicit some “extension” of that property on the convex hull of that set.",
      "startOffset" : 43,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "A tighter sufficient condition is given by Frongillo and Kash (2014), which states that essentially all loss functions eliciting a property on any set, such as Pm, also elicit some “extension” of that property on the convex hull of that set. So while the higher-dimensional approach is helpful, it does not preclude reasoning about the space Pm as a manifold inside ∆Ym . Most significantly, Pm is not a convex space, which makes lower bounds on elicitation complexity nontrivial as well. However, the result of Frongillo and Kash (2014) shows that it suffices to provide lower bounds for elicitation on the convex hull of Pm, which we will denote conv(Pm).",
      "startOffset" : 43,
      "endOffset" : 538
    }, {
      "referenceID" : 17,
      "context" : "This corresponds to a “multiplechoice question” (Lambert and Shoham, 2009).",
      "startOffset" : 48,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : "This corresponds to a “multiplechoice question” (Lambert and Shoham, 2009). In this section, we must allow Γ : P ⇒ R to be a set-valued function, possibly assigning multiple possible correct reports to a single distribution; this is necessary for “boundary” cases, such as the mode of the uniform distribution on a finite set. (Similarly, we cannot require identifiability.) We have a finite set of outcomes Y , the distributions considered are all P = ∆Y , and Γ(p) must be nonempty. We are interested in understanding which finite properties can be elicited with m observations. Previously, this question was studied for the case of one observation by Lambert (2011), who characterized elicitable properties by the shape of their level sets: they are intersections of Voronoi diagrams in R|Y| with the simplex ∆Y .",
      "startOffset" : 49,
      "endOffset" : 669
    }, {
      "referenceID" : 17,
      "context" : "This corresponds to a “multiplechoice question” (Lambert and Shoham, 2009). In this section, we must allow Γ : P ⇒ R to be a set-valued function, possibly assigning multiple possible correct reports to a single distribution; this is necessary for “boundary” cases, such as the mode of the uniform distribution on a finite set. (Similarly, we cannot require identifiability.) We have a finite set of outcomes Y , the distributions considered are all P = ∆Y , and Γ(p) must be nonempty. We are interested in understanding which finite properties can be elicited with m observations. Previously, this question was studied for the case of one observation by Lambert (2011), who characterized elicitable properties by the shape of their level sets: they are intersections of Voronoi diagrams in R|Y| with the simplex ∆Y . In our setting, a Voronoi diagram is specified by a finite set of points {xr : r ∈ R} ⊆ RY m , with each cell Tr = {x : ‖x − xr‖ ≤ ‖x − xr′‖∀r ∈ R} consisting of those points in RYm closest in Euclidean distance to xr. Using the geometric constructions above, we can simply apply the main result of Lambert (2011) to finite properties in the m-product space; the result is a characterization of elicitable finite properties with m observations.",
      "startOffset" : 49,
      "endOffset" : 1131
    }, {
      "referenceID" : 9,
      "context" : "Preliminaries on identifiable properties We start by recalling a general method, introduced in Frongillo and Kash (2015c), for showing lower bounds on elicitation complexity: Given a property Γ, if one can show that no level set from any Γ̂, which is m-identifiable and directly elicitable with m observations, can be contained in a particular level set of Γ, then Γ cannot be (d,m)-elicitable.",
      "startOffset" : 95,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "This method was used successfully in Frongillo and Kash (2015c) to show lower bounds on the report complexity (d) of",
      "startOffset" : 37,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : ", Browder (1996), Theorem 6.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 15,
      "context" : "By a very natural extension, this technique also applies to ratios of expectations, as they are elicitable (Gneiting, 2011): construct two unbiased estimators, and elicit the ratio of their means.",
      "startOffset" : 107,
      "endOffset" : 123
    }, {
      "referenceID" : 7,
      "context" : "Ratios of expectations: index of dispersion and Sharpe ratio The index of dispersion of a random variable Y with positive mean is defined to be Var(Y )/E[Y ] (Cox and Lewis, 1966).",
      "startOffset" : 158,
      "endOffset" : 179
    }, {
      "referenceID" : 22,
      "context" : "The Sharpe ratio of a random variable Y , which is a commonly-used measure of the risk-adjusted return of an investment, is defined similarly as E[Y ]/ √ Var(Y ) (Sharpe, 1966).",
      "startOffset" : 162,
      "endOffset" : 176
    }, {
      "referenceID" : 9,
      "context" : "This follows from Theorem 2 of Frongillo and Kash (2015c), specifically Section 4.",
      "startOffset" : 31,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "Multi-Observation Regression One of the earliest problems in modern statistics was the estimation of biodiversity in a geographic region (Fisher et al., 1943).",
      "startOffset" : 137,
      "endOffset" : 158
    } ],
    "year" : 2017,
    "abstractText" : "We study loss functions that measure the accuracy of a prediction based on multiple data points simultaneously. To our knowledge, such loss functions have not been studied before in the area of property elicitation or in machine learning more broadly. As compared to traditional loss functions that take only a single data point, these multi-observation loss functions can in some cases drastically reduce the dimensionality of the hypothesis required. In elicitation, this corresponds to requiring many fewer reports; in empirical risk minimization, it corresponds to algorithms on a hypothesis space of much smaller dimension. We explore some examples of the tradeoff between dimensionality and number of observations, give some geometric characterizations and intuition for relating loss functions and the properties that they elicit, and discuss some implications for both elicitation and machine-learning contexts.",
    "creator" : "LaTeX with hyperref package"
  }
}