{
  "name" : "1703.02567.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions",
    "authors" : [ "Sevi Baltaoglu", "Lang Tong", "Qing Zhao" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "√ T log T ). Also, by showing that the regret growth rate is lower bounded by Ω( √ T ) for any bidding strategy,\nwe conclude that DPDS algorithm is order optimal up to a √\nlog T term. We also evaluate the performance of DPDS empirically in the context of virtual bidding in wholesale electricity markets by using historical data from the New York energy market.\nIndex Terms\nRepeated auctions, Online learning, Dynamic programming, Virtual bidding\nI. INTRODUCTION\nWe consider the problem of optimal bidding in a multi-commodity uniform-price auction (MUPA), which is widely used in the wholesale electricity markets as detailed in Sec. I-A below as a motivating application. Specifically, a bidder has K different goods to bid on at an auction. With the objective to maximize his T-period expected profit, at each period t, the bidder’s goal is to determine how much to bid for each good k ∈ {1, 2, ...,K}, i.e. xt,k, subject to a budget constraint.\nIf a bid xt,k for good k is greater than or equal to its auction clearing price λt,k at period t, then the bid gets cleared (accepted) for good k, and the bidder pays λt,k. His revenue resulting from the cleared bid xt,k will be the good’s spot price (utility) πt,k. In particular, the payoff obtained from good k at period t is (πt,k − λt,k)1{xt,k ≥ λt,k} where 1{xt,k ≥ λt,k} indicates if the bid is cleared or not. We assume that (πt, λt) are drawn from an unknown distribution and i.i.d. over time. This assumption implies that auction clearing price is independent from bid xt which is reasonable for any market where the individual’s bid does not affect the market price too much.\nAt the end of each period, the bidder observes the auction clearing and spot prices of all goods. Therefore, before choosing the bid of period t, all the information the bidder has is a vector It−1 containing his observation and decision history {xi, λi, πi}t−1i=1 . Consequently, a bidding policy µ of a virtual bidder is defined as a sequence of decision rules, i.e., µ = (µ0, µ1..., µT−1), such that, at time t− 1, µt−1 maps the information history vector It−1 to the bid xt of period t. The performance of any bidding policy µ is measured by its regret, the difference between the total expected payoff of policy µ and the total expected payoff of the optimal bidding strategy under known distribution."
    }, {
      "heading" : "A. Motivating applications",
      "text" : "The problem given above is motivated by the virtual bidding problem in the two-settlement wholesale electricity market, which consists of a day-ahead (DA) market and a real-time (RT) market. In the DA market, the independent system operator (ISO) receives offers to sell and bids to buy from generators and retailers for each hour of the next day. To determine the optimal day-ahead dispatch of the next day and day-ahead electricity prices at each location, ISO solves an economic dispatch problem with the objective of maximizing social surplus while taking transmission and operational constraints into account. Note that, due to system congestion and losses, wholesale electricity prices vary from location to location. For example, transmission congestion may prevent scheduling the least expensive resources at some locations. In the RT market, ISO adjusts the dayahead dispatch according to the real-time operating conditions, and the real-time wholesale price compensates deviations in the real-time consumption from the day-ahead schedule.\nThe differences between DA and RT prices occur frequently, both as a result of generators and retailers exercising locational market power [1] and as a result of price spikes in the RT market due to unplanned outages and unpredictable weather conditions [2]. To promote price convergence between DA and RT markets, in the early 2000s, virtual trading was introduced in the wholesale electricity markets by most of the ISOs [3]. Virtual trading is a financial mechanism that allows market participants and outside financial entities to arbitrage on the differences between DA and RT prices. Empirical and analytical studies have shown that increased competition in the wholesale market due to virtual trading results in price convergence and increased market efficiency [1], [2], [4].\nSevi Baltaoglu, Lang Tong, and Qing Zhao are with the School of Electrical and Computer Engineering, Cornell University, Ithaca, NY, 14850 USA e-mail: {msb372,lt35,qz16}@cornell.edu.\nar X\niv :1\n70 3.\n02 56\n7v 4\n[ cs\n.G T\n] 2\n8 A\npr 2\n01 7\n2 Virtual transactions make up a significant portion of the wholesale electricity markets. For example, the overall total volume of cleared virtual transactions in five big ISO markets1 was 13%2 of the total load in 2013 [3]. In the same year, total payoff resulting from all virtual transactions was around 250 million dollars in the PJM market [1] and 45 million dollars in NYISO market [5].\nA virtual bid is a bid to buy (sell) energy in the DA market at a specific location with an obligation to sell (buy) back exactly the same amount in the RT market at the same location if the bid is cleared (accepted). Specifically, a virtual bid to buy in the DA market is cleared if the offered bid price is higher than the DA market price. Similarly, a virtual offer to sell in the DA market is cleared if it is below the DA market price. Consequently, for the problem of virtual bidding, different locations and/or different hours of the day can be thought of as different goods. Then, the DA market and RT market prices of a location correspond to its auction clearing and spot prices, respectively. The problem formulation captures virtual bids to buy in the DA market exactly. With slight modification, the model also applies to virtual bids to sell in the DA market.\nThe problem studied in this paper may find applications in other types of repeated auctions, where the auction may be of the double, uniform-price, or second-price types. For example, in the case of online advertising auctions, different goods can correspond to different types of advertising space an advertiser may consider to bid on."
    }, {
      "heading" : "B. Main results and related work",
      "text" : "We propose an online learning approach, referred to as dynamic programming on discrete set (DPDS), that is inspired by a pseudo-polynomial dynamic programming approach to 0-1 Knapsack problems. DPDS allocates the limited budget of the bidder among K goods optimally in polynomial time both in terms of the number of goods K and in terms of the time horizon T . We show that the expected payoff of DPDS converges to that of the optimal strategy under known distribution by a rate no slower than √ log t/t. Hence, this convergence results in a regret upper bound of O( √ T log T ). By showing that, for any bidding strategy, the regret growth rate is lower bounded by Ω( √ T ), we prove that DPDS algorithm is order optimal up to a √\nlog T term. We also evaluate the performance of DPDS empirically in the context of virtual bidding by using historical data from the New York energy market. Our empirical results show that cumulative payoff of DPDS increases consistently, and DPDS outperforms other algorithms considered.\nSince the bidder can calculate the reward that could have been obtained by selecting any given bid value regardless of its own decision, our problem falls into the category of experts or full-feedback multi-armed bandit (MAB) problem where the reward of all arms (experts) are observable at each period regardless of the chosen arm. For the case of finite number of arms, Kleinberg et al. [6] studied the stochastic setting of this problem and showed that constant regret is achievable by choosing the arm with the highest average reward at each period. However, this constant depends on how close the mean rewards of the arms are both for upper and lower bounds of regret. If they are closer to each other, then the constant is larger. A special case of the general adversarial setting of the problem was studied by Cesa-Bianchi et al. [7] who looked at the problem of predicting a sequence of binary outcomes using the advice of a team of experts and provided matching upper and lower bounds of the form Θ( √ T logK) where K is the number of experts (arms). Later, Freund and Shapire [8], and Auer et al. [9] showed that the Hedge algorithm, a variation of Littlestone and Warmuth’s [10] weighted majority algorithm, achieves the matching upper bound for the general setting.\nThe stochastic experts MAB problem where the set of arms is an uncountable metric space (X , d) rather than finite was studied by Kleinberg and Slivkins [11] (see [12] for an extended version). Since there are uncountable number of arms, it is assumed that, in each period, a payoff function drawn from an i.i.d. distribution is observed rather than the individual payoff of each arm. Under the assumption of Lipschitz expected payoff function, they showed that if the completion of X is a compact metric space, the instance-specific regret of any algorithm is lower bounded by Ω( √ T ). They also showed that their algorithm—NaiveExperts—achieves a regret upper bound of O(T γ) for any γ > (b + 1)/(b + 2) where b is the isometry invariant of the metric space. Even though our problem is a special case of the problem considered by Kleinberg and Slivkins [11], computational complexity of the direct implementation of NaiveExperts algorithm grows exponentially with the dimension (number of goods in our case) and it is computationally intractable in practice. Also, the lower bound in [11] doesn’t imply a lower bound for our problem with a specific payoff. Krichene et al. [13] studied the adversarial setting in the case of a compact continuous decision space and proposed an extension of the Hedge algorithm, which is shown to achieve O( √ T log T ) regret under the assumption of Lipschitz payoff functions. For our problem, it is reasonable to assume that the expected payoff function is Lipschitz; yet it is clear that, at each period, the payoff realization is a step function which is not Lipschitz. Hence, Lipschitz assumption of O( √ T log T ) doesn’t hold for our problem.\nStochastic gradient descent methods, which have very low computational complexity at each period, i.e. O(K), are extensively studied in the literature of continuum-armed bandit problems [14], [15], [16]. However, either the concavity or the unimodality of the expected payoff function is required for regret guarantees of stochastic gradient descent methods to hold. This may not be the case in our problem depending on the underlying distribution of the auction clearing and spot market prices.\n1These markets are PJM,MISO,CAISO,NYISO, and ISONE. 2This number goes up to 38% with the inclusion of up-to-congestion transactions of PJM.\n3 A relevant work that takes an online learning perspective for the problem of a bidder engaging in repeated auctions is Weed et al. [17]. They are motivated by online advertising auctions and studied the partial information setting of the same problem as ours but without a budget constraint and with a single good. For the stochastic setting, under the margin condition, i.e., the probability of auction clearing price occurring between mean utility and u above mean utility is bounded by Cαuα for some α > 0, they showed that their algorithm, inspired by the UCB1 algorithm of Auer et al. [18], achieves regret that ranges from O(log T ) to O( √ T log T ) depending on the value of α. They also provided matching lower bounds up to a logarithmic factor. However, their lower bound does not imply a lower bound for the full information setting we study here. Also, we can’t use a similar online learning approach due to the fact that goods are coupled with a budget constraint in our case. Furthermore, we do not have margin condition and we let utility (RT price) of the good to depend on the auction clearing price (DA price).\nSome other examples of related work on online advertising auctions studied the online learning problem of an advertiser who wants to maximize number of clicks with a budget constraint [19], [20]. However, the budget constraint is in this case is over the time horizon T and the bidder bids on a single good at each period whereas our problem deals with the allocation of a fixed budget among K goods at each period."
    }, {
      "heading" : "II. PROBLEM FORMULATION",
      "text" : "Let λt = [λt,1, ..., λt,K ]ᵀ and πt = [πt,1, ..., πt,K ]ᵀ be the vector of auction clearing and spot market prices at period t, respectively. Similarly, let xt = [xt,1, ..., xt,K ]ᵀ be the vector of bids for period t. Then, the total expected payoff at period t given bid xt can be expressed as\nr(xt) = E ((πt − λt)ᵀ1{xt ≥ λt}|xt) ,\nwhere the expectation is taken using the joint distribution of (πt, λt). We assume that the payoff resulting from bidding on any good k ∈ {1, ...,K}, i.e. (πt,k − λt,k)1{xt,k ≥ λt,k}, is a bounded random variable with support in [l, u] for any choice of bid.3\nThe objective is to determine a bidding policy µ that maximizes the expected T-period payoff subject to a budget constraint for each individual period:\nmaximize µ E ( T∑ t=1 r(xµt ) ) subject to ‖xµt ‖1 ≤ B, for all t = 1, ..., T,\nxµt ≥ 0, for all t = 1, ..., T,\n(1)\nwhere B is the auction budget of the bidder, and xµt denotes the bid determined by policy µ. We assume that the auction (day-ahead) prices are drawn from a distribution with positive support4. Hence, a zero virtual bid for any good k is equivalent to not bidding for that good because it will not get cleared."
    }, {
      "heading" : "A. Optimal solution under known distribution and regret",
      "text" : "If the joint distribution f(., .) of πt and λt is known, the optimization problem (1) decouples to solving for each time instant separately. Since (πt, λt) is i.i.d. over t, an optimal solution under known model does not depend on t and is given by\nx∗ = arg max xt∈F r(xt) (2)\nwhere F = {x ∈ <K : x ≥ 0, ‖x‖1 ≤ B} is the feasible set of bids. Optimal solution x∗ may not be unique or it may not have a closed form. The following example illustrates a case where there isn’t a closed form solution and shows that even in the case of known distribution, the problem is a combinatorial stochastic optimization and it is not easy to calculate an optimal solution.\nExample. Let’s consider the case where λt and πt are independent, and λt,k is exponentially distributed with mean λ̄k > 0 and the mean of πt,k is π̄k > 0 for all k ∈ {1, ..,K}. Since the optimal policy would be not bidding for good k if π̄k ≤ 0, we exclude the case π̄k ≤ 0 w.l.o.g.\nFor this example, we can use the concavity of r(x) in the interval [0, π̄] where π̄ = [π̄1, ..., π̄K ]ᵀ to obtain the unique optimal solution x∗ which is characterized by\nx∗k =  π̄k if ∑K k=1 π̄k ≤ B, 0 if ∑K k=1 π̄k > B and π̄k/λ̄k < γ ∗,\nzk satisfying (π̄k − zk)e−zk/λ̄k/λ̄k = γ∗ if ∑K k=1 π̄k > B and π̄k/λ̄k ≥ γ∗,\n3This is reasonable in the case of virtual bidding, since the DA and RT prices are bounded due to offer and bid caps. 4We point out that supply offers with negative bid prices are not prohibited in electricity markets. However, negative day-ahead prices are very rare due to\nlack of any motivation for generators to submit such an offer.\n4 λ1,k xk r̂1,k(xk) π1,k − λ1,k\n(a) t = 1 λ′k,1\nxk\nr̂4,k(xk)\nr′k,1\nλ′k,2 λ ′ k,3 λ ′ k,4\nr′k,2 r′k,3\nr′k,4\n(b) t = 4\nFig. 1. Piece-wise constant average payoff function of good k\nwhere γ∗ > 0 is chosen such that ‖x∗‖1 = B is satisfied. This solution has a ”water-filling” interpretation. Specifically, the optimal solution takes the form of a water-filling strategy and cannot be expressed in closed-form.\nWe measure the performance of a bidding policy µ by its regret, the difference between the expected T-period payoff of µ and that of x∗, i.e.,\nRµT (f) = T∑ t=1 E(r(x∗)− r(xµt )), (3)\nwhere the expectation is taken with respect to the randomness induced by µ. Since the incremental regret E(r(x∗) − r(xµt )) is non-negative, the regret is monotonically increasing, and we are interested in bidding policies that have a sub-linear regret growth."
    }, {
      "heading" : "III. ONLINE LEARNING APPROACH FOR OPTIMAL BIDDING STRATEGY",
      "text" : "The idea behind our online learning approach is to maximize the sample mean of the expected payoff function. Direct implementation of this approach, however, involves an integer programming that is NP-hard. In this section, we propose a polynomial-time algorithm based on a dynamic programming on a discretized feasible set. We show that this approach achieves the order optimal regret among all online learning policies."
    }, {
      "heading" : "A. Approximate expected payoff function and its optimization",
      "text" : "Regardless of the bidding policy, one can observe the auction clearing and spot prices of each period at the end of that period. Therefore, the average payoff that could have been obtained by bidding x up to the current period can be calculated for any fixed value of x ∈ F .\nThe average payoff r̂t,k(xk) for any good k as a function of the bid value xk for good k can be calculated at the end of period t using observations up to t, i.e.,\nr̂t,k(xk) = (1/t) t∑ i=1 (πi,k − λi,k)1{xk≥λi,k}.\nFor example, at the end of first period, r̂t,k(xk) = (π1,k − λ1,k)1{xk≥λ1,k} as illustrated in Fig. 1a. For, t ≥ 2, this can be expressed recursively;\nr̂t,k(xk) = { t−1 t r̂t−1,k(xk) if xk < λt,k, t−1 t r̂t−1,k(xk) + 1 t (πt,k − λt,k) if xk ≥ λt,k.\n(4)\nSince each observation introduces a new breakpoint, and the value of average payoff function is constant between two consecutive breakpoints, we observe that r̂t,k(xk) is a piece-wise constant function with at most t breakpoints. Let the vector of order statistics of the observed auction clearing prices {λi,k}ti=1 at the end of period t be λ′k = [λ(1),k, ..., λ(t),k]ᵀ, and let the vector of associated average payoffs be r′k, i.e., r ′ k,i = r̂t,k(λ(i),k). Then, r̂t,k(xk) can be expressed by the pair (λ ′ k, r ′ k). As an example, see Fig. 1b for the case t = 4. For a vector y, let ym:n = (ym, ym+1, ..., yn) denote the sequence of entries from m to n. Observe that at each period t, the pair (λ′k, r ′ k) can be updated recursively as follows:\n(λ′k, r ′ k) = { (λ1,k, π1,k − λ1,k), if t = 1 ([λ′k,1:ik , λt,k, λ ′ k,ik+1:t−1] ᵀ, [ t−1t r ′ k,1:ik , t−1t r ′ k,ik:t−1 + 1 t (πt,k − λt,k)] ᵀ), if t > 1 (5)\nwhere ik = arg maxi:λ′k,i<λt,k i at period t.\n5 Consequently, overall average payoff function r̂t(x) can be expressed as a sum of average payoff functions of individual goods, i.e.,\nr̂t(x) = K∑ k=1 r̂t,k(xk). (6)\nIf we maximize the average payoff r̂t(x) given in (6) instead of the unknown expected payoff r(x), choosing xk = λ′k,i for any i ∈ {1, ..., t} contributes the same amount to the overall payoff as choosing any xk ∈ [λ′k,i, λ′k,i+1) if i < t − 1 and any xk ≥ λ′k,i if i = t due to the piece-wise constant structure of the average payoff function. However, choosing xk = λ′k,i utilizes a smaller portion of the budget. Hence, a feasible solution x ∈ F to the maximization problem of (6) can be obtained by solving the following integer linear program:\nmaximize {zk}Kk=1 K∑ k=1 (r′k) ᵀzk\nsubject to K∑ k=1 (λ′k) ᵀzk ≤ B,\n1ᵀzk ≤ 1, ∀k = 1, ...,K, zk,i ∈ {0, 1}, ∀i = 1, ..., t;∀k = 1, ...,K.\n(7)\nwhere the bid value xk = (λ′k) ᵀzk for good k.\nObserve that (7) is a multiple choice knapsack problem (MCKP) [21] which reduces to 0-1 knapsack with K items when t = 1. Unfortunately, (7) is NP-hard. If we had a polynomial-time algorithm that finds a solution x ∈ F to the maximization problem of the average payoff function r̂t(x) given in (6), then we could have obtained the solution of (7) in polynomial-time too by setting zk,i = 1 where i = arg maxi:xk≥λk,i i for each k. Therefore, the maximization problem of (6) in the feasible set F is also NP-hard."
    }, {
      "heading" : "B. Dynamic programming on discrete set (DPDS) policy",
      "text" : "Next, we present an approach that discretizes the feasible set using intervals of equal length and optimizes the average payoff on this new discrete set via a dynamic program. Although this approach doesn’t solve the problem given in (7), the solution can be arbitrarily close to the optimal depending on the choice of the interval length under the assumption of the Lipschitz continuous expected payoff function. To exploit the smoothness of Lipschitz continuous functions, discretization approach of the continuous feasible set has been used in the continuous MAB literature previously [14], [11]. However, different than MAB literature, in this paper, discretization approach is utilized to reduce the computational complexity of an NP-hard problem besides the exploitation of smoothness of the payoff function.\nLet αt be an integer sequence increasing with t and Dt = {0, B/αt, 2B/αt, ..., B} as illustrated in Fig. 2. Then, the new discrete set is given as\nFt = {x ∈ F : xk ∈ Dt,∀k ∈ {1, ...,K}}.\nOur goal is to optimize r̂t(.) on the new set Ft rather than F , i.e.,\nmax xt+1∈Ft r̂t(xt+1). (8)\nNow, we use dynamic programming approach that has been used to solve 0-1 Knapsack problems including MCKP such as (7) [22]. However, direct implementation of this approach results in pseudo-polynomial computational complexity in the case of 0-1 Knapsack problems. The discretization of the feasible set with equal interval length reduces the computational complexity to polynomial time.\nWe define the maximum payoff one can collect with budget b among goods {1, ..., n} when the bid value xk is restricted to the set Dt for each good k as\nVn(b) = max {xi}ni=1: ∑n i=1 xi≤b,xi∈Dt∀i n∑ i=1 r̂t(xi).\nThen, the following recursion can be used to solve for VK(B) which gives us the optimal solution to (8):\nVn(jB/αt) = { 0 if n = 0, j ∈ {0, 1, ..., αt}, max 0≤i≤j (r̂t,n(iB/αt) + Vn−1((j − i)B/αt)) if 1 ≤ n ≤ K, j ∈ {0, 1, ..., αt}. (9)\nThis is the Bellman equation where Vn(b) is the maximum total payoff one can collect using remaining budget b and remaining n goods. Its optimality can be shown via a simple induction argument. Recall that r̂t,n(0) = 0 for all (t, n) pairs due to the assumption of positive day-ahead prices.\n6 λ′k,1 xk r̂4,k(xk) r′k,1 λ′k,2 λ ′ k,3 λ ′ k,4 r′k,2 r′k,3 r′k,4 B/α4 2B/α4 3B/α4 4B/α40\nFig. 2. Example of the discretization of the decision space for good k when t = 4\nRecursion (9) can be solved starting from n = 1 and proceeding to n = K, where, for each n, Vn(b) is calculated for all b ∈ Dt. Since the computation of Vn(b) requires at most αt + 1 comparison for any fixed value of n ∈ {1, ...,K} and b ∈ Dt, it has a computational complexity on the order of Kα2t given the average payoff values r̂t,n(xn) for all xn ∈ Dt and n ∈ {1, ...,K}. For each n ∈ {1, ...,K}, computation of r̂t,n(xn) for all xn ∈ Dt introduces an additional computational complexity of at most on the order of t. Hence, total computational complexity of DPDS is O(K max(t, α2t )) at each period t. The full description of DPDS is given in Appendix A."
    }, {
      "heading" : "C. Convergence and regret of DPDS policy",
      "text" : "Under the assumption of Lipschitz continuous payoff function, Theorem 1 shows that the value of DPDS policy converges to the value of the optimal policy under known model with a rate faster than or equal to √ log t/t if the DPDS algorithm parameter αt = tγ with γ ≥ 1/2. Consequently, the regret growth rate of DPDS is upper bounded by O( √ T log T ). If γ = 1/2, then the computational complexity of the algorithm is bounded by O(Kt) at each period t, and total complexity over the entire horizon is O(KT 2).\nTheorem 1: Let xDPDSt+1 denote the bid of DPDS policy for period t+ 1. If r(.) is Lipschitz continuous on F with p-norm and Lipschitz constant L, then, for any γ > 0 and for DPDS parameter choice αt ≥ 2,\nE(r(x∗)−r(xDPDSt+1)) ≤ LK1/pB/αt+ √ 2(γ + 1)K + 1K(u− l) √ log t/t+4 min(K(u− l), LK1/pB)αKt t−(γ+1)K−1/2, (10)\nand for any αt = max(dtγe, 2) with γ ≥ 1/2,\nRDPDST (f) ≤ 2(LK1/pB + 4 min(K(u− l), LK1/pB)) √ T + 2 √ 2(γ + 1)K + 1K(u− l) √ T log T . (11)\nActually, we can relax the uniform Lipschitz continuity condition. Under the weaker condition of |r(x∗)−r(x)| ≤ L‖x∗−x‖qp for all x ∈ F and for some constant L > 0, the incremental regret bound given in (10) becomes\nE(r(x∗)− r(xDPDSt+1)) ≤ LKq/p(B/αt)q +K(u− l)( √ 2(γ + 1)K + 1 √ log t/t+ 4αKt t −(γ+1)K−1/2).\nThe proof of Theorem 1 is derived by showing that the value of x∗t+1 = arg maxx∈Ft r(x) converges to the value of x ∗ due to Lipschitz continuity, and the value of xDPDSt+1 converges to the the value of x ∗ t+1 via the use of concentration inequality inspired by [18], [14]. The full proof of Theorem 1 is given in Appendix A."
    }, {
      "heading" : "D. Lower bound of regret for any bidding policy",
      "text" : "We now show that DPDS in fact achieves the slowest possible regret growth. Specifically, Theorem 2 states that, for any bidding policy µ and horizon T , there exists a distribution f for which the regret growth is slower than or equal to the square root of the horizon T .\nTheorem 2: Consider the case where K = 1, B = 1, λt and πt are independent random variables with distributions\nfλ(λt) = −11{(1− )/2 ≤ λt ≤ (1 + )/2}\nand fπ(πt) = Bernoulli(π̄), respectively. Let = T−1/2/2 √\n5. Then, for f(λt, πt) = fλ(λt)fπ(πt) and for any bidding policy µ,\nRµT (f) ≥ (1/16 √ 5) √ T ,\neither for π̄ = 1/2 + or for π̄ = 1/2− .\n7 (a) Regret when B=13.845 (b) Regret when B=25.828\nFig. 3. Regret with respect to √ t\nAs seen in Theorem 2, we choose a specific distribution for the auction clearing and spot prices. Observe that for this specific distribution the payoff function is Lipschitz continuous with Lipschitz constant L = 3/2 because magnitude of the derivative of the payoff function |r′(x)| ≤ |π̄ − x|/ ≤ 3/2 for (1 − )/2 ≤ x ≤ (1 + )/2 and r′(x) = 0 otherwise. So, it satisfies the condition for upper bound given in Theorem 1.\nThe proof of Theorem 2 is obtained by showing that every time the bid is cleared it incurs an incremental regret greater than /2 under the distribution with π̄ = (1/2− ); otherwise, it incurs the same regret under the distribution with π̄ = (1/2 + ). However, to distinguish between these two distributions one needs Ω(T ) samples for = T−1/2/2 √ 5 which results in a regret lower bound of Ω( √ T ). The bound is obtained by adapting a similar argument used by [23] in the context of non-stochastic multi-armed bandit problem. Refer to Appendix A for the full proof of Theorem 2."
    }, {
      "heading" : "IV. NUMERICAL RESULTS",
      "text" : "In this section, we first present a simulation example to illustrate the regret growth rate of DPDS. For the simulation example, 1000 Monte Carlo runs were used to calculate the average performance. Then, we present an empirical example using real historical data obtained from New York ISO website. In both cases, we set the DPDS algorithm parameter αt = t."
    }, {
      "heading" : "A. Simulation example",
      "text" : "To illustrate the regret growth rate of DPDS, we consider an example with K = 5. In this example, πt and λt are independent, λt is exponentially distributed with mean λ̄ = [4, 6, 8, 8, 4]ᵀ, and πt is uniformly distributed with mean π̄ = [5, 8, 8, 9, 3]ᵀ and support in [π̄ − 1, π̄ + 1]. Previously, in Sec. II-A, we stated the characterization of the optimal solution for this example. By using this, we determined an optimal solution and an associated budget B.\nAs a benchmark comparison we consider two different algorithms. The first one is the sliding window (SW) which calculates the average payoff function of each good every day from the prices of last ten days only. Then, it determines the optimal solution maximizing the total average payoff by solving the integer linear program given in (7). The second one, referred to as SA, is a variant of Kiefer-Wolfowitz stochastic approximation method. SA approximates the gradient of the payoff function using the current observation and updates the bid of each k as follows;\nxt+1,k = xt,k + at ((πt,k − λt,k)(1{xt,k + ct ≥ λt} − 1{xt,k ≥ λt})) /ct.\nThen, xt+1 is projected to the feasible set F . To give a good result for B = 13.845, step size at and ct were carefully chosen to be 5.5/t and 2.5/t1/4, respectively.\nThe regret performances for budgets 13.845 and 25.828 are given in Fig. 3a and Fig. 3b, respectively. In both cases, DPDS outperforms, and its order of regret growth is actually better than the square root of the time horizon. When the algorithm parameters of SA are tuned well, then we observe that its performance may get close to DPDS as in Fig. 3a. However, when we increase the budget from 13.845 to 25.828, the performance of SA deteriorates significantly."
    }, {
      "heading" : "B. Empirical example",
      "text" : "New York ISO, that consists of eleven zones, allows virtual bids at zonal nodes only. So, to test our algorithm, we use historical DA and RT prices of these eleven zones starting from the beginning of 2014 until the end of 2016. Since the price for each hour is different at each zone, there are K = 11× 24 different goods to bid on every day. New York ISO DA market for day t+ 1 closes at 5:00 am on day t. This means that the RT prices of all hours of day t cannot be observed before the bid submission for day t. Therefore, in our implementation, the most recent information used before the submission for day t+ 1\n8 was the observations from day t− 1. Also, for each cleared virtual bid, there is an associated transaction cost 5 that changes yearly. This transaction cost was $0.0976, $0.1046, and $0.085 per MWh for years 2014, 2015, and 2016, respectively. In our implementation, we take this into account and subtract this cost from the real-time prices of the respective years.\nWe compare DPDS with two different algorithms. One of them is SW as given in Sec. IV-A. The second one concerns with the problem where you don’t have a budget constraint, and you assume that you always bid high enough to get accepted for the goods you choose to bid on; yet the number of goods that you can bid on cannot be greater than N every day. Hence, you want to choose the N best arms (goods) to bid on. So, fixed arm (FA) algorithms, FA1 and FA2, chooses the N best arms according to their reward (DA and RT price difference) sample means. Then, FA1 bids on the arms with positive reward sample means only among these N arms, and FA2 bids on all of these N arms.\nWe run DPDS and SW algorithms for DA budget B of 500, 1000, 1500, and 2000 dollars, and FA1 and FA2 algorithms for N of 12, 24, 36, and 48 arms. For different B and N values, Table I shows each algorithm’s total payoff that is obtained in the last two years after the first year of data used for initial training. Table II and Table III summarizes the average DA market spending for cleared bids and the average number of arms cleared in the DA market for each algorithm during the last two years. In Table I, observe that the DPDS payoff is always better than all instances of other algorithms, and it increases with increasing B whereas SW always incurs a loss that gets larger with increasing B. FA1 and FA2 payoffs are decreasing with increasing N except when N=24. Even when you compare DPDS and FA algorithms at instances of B and N values for which the DA budget spending for cleared bids or the average number of goods cleared are closest, the performance of DPDS is significantly better. This highlights the importance of choosing the correct bid value rather than bidding high enough to get accepted.\nFor the case of B=$500 and N=12, the cumulative payoff trajectory over three years without any prior training and over the last two year after the first year of training are given in Fig. 4a and Fig. 4b, respectively. At the beginning of the horizon in Fig. 4a, all the algorithms suffers a loss due to lack of any training. Especially, FA algorithms experience a sharp decay over the course of ten days. However, DPDS recovers its loss and actually makes around $13,000 in total whereas all others suffer a large loss. In Fig. 4b, observe that,except SW, algorithms perform better after a year of training. Since SW uses observations only from last ten days, the decay in its performance stays the same. The performances under B=$2000 and N=48 is given in Fig. 5. The trends in Fig. 5 is similar to the ones observed in Fig. 4. However, the performance of DPDS improves with higher B whereas the performance of all others deteriorates with higher B and N."
    }, {
      "heading" : "ACKNOWLEDGEMENT",
      "text" : "We would like to thank Professor Robert Kleinberg for the insightful discussion.\n5This transaction cost is referred to as rate schedule 1 for virtual resources in NYISO website.\n9 (a) Cumulative payoff over three years without prior training (b) Cumulative payoff over the last two years after a year of training"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "<lb>We study the online learning problem of a bidder who participates in repeated auctions. With the goal of maximizing his total<lb>T-period payoff, the bidder wants to determine the optimal allocation of his fixed budget among his bids for K different goods<lb>at each period. As a bidding strategy, we propose a polynomial time algorithm, referred to as dynamic programming on discrete<lb>set (DPDS), which is inspired by the dynamic programming approach to Knapsack problems. We show that DPDS achieves the<lb>regret order of O(<lb>√<lb>T log T ). Also, by showing that the regret growth rate is lower bounded by Ω(<lb>√<lb>T ) for any bidding strategy,<lb>we conclude that DPDS algorithm is order optimal up to a<lb>√<lb>log T term. We also evaluate the performance of DPDS empirically<lb>in the context of virtual bidding in wholesale electricity markets by using historical data from the New York energy market. Index Terms<lb>Repeated auctions, Online learning, Dynamic programming, Virtual bidding",
    "creator" : "LaTeX with hyperref package"
  }
}