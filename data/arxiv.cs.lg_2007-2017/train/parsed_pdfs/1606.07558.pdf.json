{
  "name" : "1606.07558.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Satisfying Real-world Goals with Dataset Constraints",
    "authors" : [ "Andrew Cotter", "Michael Friedlander", "Gabriel Goh", "Maya Gupta" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Real-world goals",
      "text" : "We consider a broad set of design goals and issues important for making classifiers work well in real-world applications, ranging from the standard precision and recall, to some new proposals. The key theme is that these goals can be expressed in terms of the positive and negative classification rates on multiple datasets, as we show in Table 1.\nCoverage: One may wish to control how often a classifier predicts the positive (or negative) class. For example, one may want to ensure that only 10% of customers are selected to receive a printed catalog due to budget constraints, or perhaps to compensate for a biased training set. In practice, constraining the “coverage rate” (the expected proportion of positive predictions) is often easier than measuring e.g. accuracy or precision because coverage can be computed on unlabeled data—labeling data can be expensive, but acquiring a large number of unlabeled examples is often very easy.\nCoverage was also considered by Mann and McCallum [2007], who proposed what they call “label regularization”, in which one adds a regularizer that penalizes the relative entropy between the mean classifier score for each class and the desired distribution, with an additional correction to avoid degeneracies.\nChurn: Work does not stop once a machine learning model has been adopted. There will be new training data, improved features, and potentially new model structures. Hence, in practice, one will deploy a series of models, each improving slightly upon the last. In this setting, determining whether each candidate should be deployed is surprisingly challenging: if we evaluate on the same held-out testing set every time a new candidate is proposed, and deploy it if it outperforms its predecessor, then every compare-and-deploy decision will increase the statistical dependence between the deployed model and the testing dataset, causing the model sequence to fit the originally-independent testing data. This problem is magnified if, as is typical, the candidate models tend to disagree only on a relatively small number of examples near the true decision boundary. For example, with a fixed test set of 10 000 random examples, only 100 may be near to the decision boundary, so the risk of the model sequence fitting these 100 examples is heightened.\nar X\niv :1\n60 6.\n07 55\n8v 1\n[ cs\n.L G\n] 2\n4 Ju\nn 20\nA simple and safe solution is to draw a fresh testing sample every time one wishes to compare two models in the sequence, only considering examples on which the two models disagree. Because labeling data is expensive, one would like these freshly sampled testing datasets to be as small as possible. It is here that the problem of “churn” arises. Imagine that model A, our deployed model, is 70% accurate, and that model B, our candidate, is 75% accurate. In the best case, only 5% of test samples would be labeled differently, and all differences would be “wins” for classifier B. Then only a dozen or so examples would need to be labeled in order to establish that B is the statistically significantly better classifier with 95% confidence. In the worst case, model A would be correct and model B is incorrect 25% of the time, model B correct and model A incorrect 30% of the time, and both models correct the remaining 45% of the time. Then 55% of testing examples will be labeled differently, and closer to 1000 examples would need to be labeled to determine that model B is better.\nWe define the “churn rate” as the expected proportion of examples on which the prediction of the model being considered (model B above) differs from that of a baseline model (model A). During training, we propose constraining the empirical churn rate with respect to a given baseline model on a large unlabeled dataset.\nStability: A special case of minimizing churn is to ensure stability of an online classifier as it evolves, by constraining it to not deviate too far from a trusted classifier on a large held-out unlabeled dataset.\nFairness: A practitioner may be required to guarantee fairness of a learned classifier, in the sense that it makes positive predictions for members of different subgroups at certain rates. For example, one might require that housing loans be given equally to people of different genders. As noted by Zafar et al. [2015], fairness is sometimes specified by a proportion, such as the 80% rule in US law that certain decisions must be in favor of group B individuals at least 80% as often as in favor of group A individuals [e.g. Biddle, 2005, Vuolo and Levy, 2013]. Zafar et al. [2015] propose learning fair classifiers by imposing linear constraints on the covariance between the predicted labels and the values of certain features. In our framework, rate constraints such as the 80% rule can be imposed directly.\nRecall and Precision: Requirements of real-world classifiers are often expressed in terms of precision and recall, especially when examples are highly imbalanced between positives and negatives. In our framework, we can handle this problem via Neyman-Pearson classification [e.g. Scott and Nowak, 2005, Davenport et al., 2010], in which one seeks to minimize the false negative rate subject to a constraint on the false positive rate. Indeed, our ramp-loss formulation is equivalent to that of Gasso et al. [2011] in this setting.\nEgregious Examples: For certain classification applications, examples may be discovered that are particularly embarrassing if classified incorrectly. One standard approach to handling such examples is to increase their weights during training, but this is difficult to get right, because too large a weight may distort the classifier too much in the surrounding feature space, whereas too small a weight may not fix the problem. Worse, over time the dataset will often be augmented with new training examples and new features, causing the ideal weights to drift. We propose instead simply adding a constraint ensuring that some proportion of a set of such egregious examples is correctly classified. Such constraints should be used with extreme care: if too many are added then the problem may become infeasible."
    }, {
      "heading" : "2 Optimization problem",
      "text" : "A key aspect of many of the goals of Section 1 is that they are defined on different datasets. For example, we might seek to maximize the accuracy of our classifier on a set of labeled examples drawn in some biased manner, require that its recall be at least 90% on 50 small datasets sampled in an unbiased manner from 50 different countries, desire low churn relative to a deployed classifier on a large unbiased unlabeled dataset, and require that 100 given egregious examples be classified correctly.\nAnother characteristic common to the metrics of Section 1 is that they can be expressed in terms of the positive and negative classification rates on various datasets, where for simplicity, we treat all datasets as unlabeled, as described in Table 1. We’ll restrict our attention to the problem of learning a linear classification function f(x) = 〈w, x〉 − b parameterized by a weight vector w ∈ Rd and bias b ∈ R, for which these rates are:\nwhere 1 is an indicator function that is 1 if its argument is positive, 0 otherwise. In words, sp(D;w, b) and sn(D;w, b) denote the proportion of positive or negative predictions, respectively, that f makes on D. Table 2 specifies how the metrics of Section 1 can be expressed in terms of the sps and sns.\nWe propose handling these goals by minimizing an `2-regularized positive linear combination of prediction rates on different datasets, subject to upper-bound constraints on other positive linear combinations of such prediction rates:\nminimize w∈Rd,b∈R\n∑k i=1 ( α (0) i sp(Di;w, b) + β (0) i sn(Di;w, b) ) + λ2 ‖w‖ 2 2 (2)\ns.t. ∑k i=1 ( α (j) i sp(Di;w, b) + β (j) i sn(Di;w, b) ) ≤ γ(j) j ∈ {1, . . . ,m}\nwhere λ is the parameter on the `2 regularizer, there are k unlabeled datasets D1, . . . , Dk and m constraints. The metrics minimized by the objective and bounded by the constraints are specified via the choices of the nonnegative coefficients α(0)i , α (j) i , β (0) i and β (j) i for the ith dataset and, where applicable, the jth constraint—a user should base these choices on Table 2. Note that because sp + sn = 1, it is possible to transform any linear combination of rates into an equivalent positive linear combination, plus a constant (see Appendix A1 for an example).\nWe cannot optimize Problem 2 directly because the rate functions sp and sn are discontinuous. We can, however, work around this difficulty by following Cotter et al. [2013] and training a classifier that makes\n1Appendices may be found in the supplementary material\nAlgorithm 1 Proposed majorization-minimization procedure for (approximately) optimizing the ramp version of Problem 2. Starting from an initial feasible solution w(0), b0, we repeatedly find a convex upper bound problem that is tight at the current candidate solution, and optimize it to yield the next candidate. See Section 2.1 for details, and Section 2.2 for how one can perform the inner optimizations on line 3.\nMajorizationMinimization ( w(0), b0, T ) 1 For t ∈ {1, 2, . . . , T} 2 Construct an instance of Problem 3 with w′ = w(t−1) and b′ = bt−1 3 Optimize this convex optimization problem to yield w(t) and bt 4 Return w(t), bt\nrandomized predictions based on the ramp function [Collobert et al., 2006]:\nσ(z) = max{0,min{1, 1/2 + z}}\nwhere the randomized classifier parameterized by w and b will make a positive prediction on x with probability σ (〈w, x〉 − b), and will make a negative prediction otherwise. We can then define the expected positive and negative rates (with the expectation being taken w.r.t. the randomness of the classifier) on an unlabeled dataset D as:\nrp (D;w, b) = 1 |D| ∑ x∈Dσ (〈w, x〉 − b) , rn (D;w, b) = rp (D;−w,−b)\nSubstituting these expected rates for sp and sn gives a continuous (but non-convex) problem that we will henceforth refer to as the “ramp version” of Problem 2."
    }, {
      "heading" : "2.1 Optimizing the ramp problem",
      "text" : "To address the non-convexity of the ramp version of our problem, we will iteratively optimize approximations, by, starting from an initial candidate solution, constructing a convex optimization problem upper-bounding the ramp version of Problem 2 that is tight at the current candidate, optimizing this convex problem to yield the next candidate, and repeating.\nOur choice of a ramp for σ makes finding such tight convex upper bounds easy: both the hinge function max {0, 1/2 + z} and constant-1 function upper bound σ, with the former being tight for all z ≤ 1/2, and the\nlatter for all z ≥ 1/2 (see Figure 1). We’ll therefore define the following upper bounds on σ and 1− σ, with the additional parameter z′ determining which of the two bounds (hinge or constant) will be used, such that the bounds will always be tight for z = z′:\nσ̌p (z; z ′) = { max {0, 1/2 + z} if z′ ≤ 1/2 1 otherwise , σ̌n(z; z ′) = σ̌p (−z;−z′)\nBased upon these we define the following upper bounds on the expected rates:\nřp (D;w, b;w ′, b′) = 1|D| ∑ x∈D σ̌p (〈w, x〉 − b; 〈w′, x〉 − b′) řn (D;w, b;w ′, b′) = 1|D| ∑ x∈D σ̌n (〈w, x〉 − b; 〈w′, x〉 − b′)\nwhich have the properties that both řp and řn are convex in w and b, are upper bounds on the original ramp-based rates:\nřp (D;w, b;w ′, b′) ≥ rp (D;w, b) and řn (D;w, b;w′, b′) ≥ rn (D;w, b)\nand are tight at w′, b′:\nřp (D;w ′, b′;w′, b′) = rp (D;w ′, b′) and řn (D;w′, b′;w′, b′) = rn (D;w′, b′)\nSubstituting these bounds into the ramp version of Equation 2 yields:\nminimize w∈Rd,b∈R\n∑k i=1 ( α (0) i řp (Di;w, b;w ′, b′) + β (0) i řn (Di;w, b;w ′, b′) ) + λ2 ‖w‖ 2 2 (3)\ns.t. ∑k i=1 ( α (j) i řp (Di;w, b;w ′, b′) + β (j) i řn (Di;w, b;w ′, b′) ) ≤ γ(j) j ∈ {1, . . . ,m}\nAs desired, this problem upper bounds the ramp version of Problem 2, is tight at w′, b′, and is convex (because we only permit positive linear combinations of rates, and any positive linear combination of convex functions is convex).\nAlgorithm 1 contains our proposed (approximate) optimization procedure for solving Problem 3. Given an initial feasible solution, it’s straightforward to verify inductively, using the fact that we construct tight convex upper bounds at every step, that every convex subproblem will have a feasible solution, every w(t), bt will be feasible w.r.t. the ramp version of Problem 2, and every pair (w(t+1), bt+1) will have an objective function value that is no higher that that of (w(t), bt). In other words, no iteration can make negative progress.\nThe non-convexity of the ramp version of Problem 2 will cause Algorithm 1 to arrive at a suboptimal solution that depends on the initial (w(0), b0)."
    }, {
      "heading" : "2.2 Optimizing the convex subproblems",
      "text" : "Our approach for optimizing the constrained convex subproblems is based on the idea of adding Lagrange multipliers v over the constraints in Problem 3 to obtain the equivalent unconstrained problem:\nmaximize v 0 z(v) = min w,b\nΨ (w, b, v;w′, b′) (4)\nwhere the function:\nΨ (w, b, v;w′, b′) = ∑k i=1 (( α (0) i + ∑m j=1vjα (j) i ) řp (Di;w, b;w ′, b′) (5)\n+ ( β (0) i + ∑m j=1vjβ (j) i ) řn (Di;w, b;w ′, b′) ) + λ2 ‖w‖ 2 2 − ∑m j=1vjγ (j)\nis convex in w and b, and concave in the multipliers v. For the purposes of this section, w′ and b′ are fixed constants.\nAlgorithm 2 Skeleton of a cutting-plane algorithm that solves Problem 4 to within . Here, V ⊆ Rm is compact and convex and l0, u0 ∈ R are finite with l0 ≤ maxv z(v) ≤ u0. There are several options for the CutChooser function on line 8—please see Appendix C for details. The SVMOptimizer function returns w(t) and bt approximately minimizing Ψ(w, b, v(t);w′, b′), and a lower bound lt ≤ z(v) for which ut − lt ≤ t for ut as defined on line 9.\nCuttingPlane (l0, u0,V, ) 1 Initialize g(0) ∈ Rm to the all-zero vector 2 For t ∈ {1, 2, . . . } 3 Let ht (v) = mins∈{0,1,...,t−1} ( us + 〈 g(s), v − v(s)\n〉) 4 Let Lt = maxs∈{0,1,...,t−1} ls and Ut = maxv∈V ht (v) 5 If Ut − Lt ≤ then 6 Let s ∈ {1, . . . , t− 1} be an index maximizing ls 7 Return w(s), bs, v(s) 8 Let v(t), t = CutChooser (ht, Lt) 9 Let w(t), bt, lt = SVMOptimizer ( v(t), Lt, ht ( v(t) ) , t )\n10 Let ut = Ψ(w(t), bt, v(t);w′, b′) and g(t) = ∇vΨ(w(t), bt, v(t);w′, b′)\nThe key insight is that evaluating z(v) is, thanks to our use of hinge and constant upper-bounds on our ramp σ, equivalent to optimization of a support vector machine (SVM) with per-example weights—see Appendix D for details. This observation enables us to solve the saddle system in an inside-out manner. On the “inside”, we optimize over (w, b) for a fixed v using an off-the-shelf SVM solver. On the “outside”, the resulting (w, b)-optimizer is used as a component in an outer optimization over v. Notice that this outer optimization is very low-dimensional, since v ∈ Rm, where m is the number of constraints.\nAlgorithm 2 contains a skeleton of the cutting-plane algorithm that we use for this outer optimization over v. Because this algorithm is intended to be used as an outer loop in a nested optimization routine, it does not expect that z(v) can be evaluated or differentiated exactly. Rather, it’s based upon the idea of possibly making “shallow” cuts [Bland et al., 1981] by choosing a desired accuracy t at each iteration, and expecting the SVMOptimizer to return a solution with suboptimality t. More precisely, the SVMOptimizer function approximately evaluates z(v(t)) for a given fixed v(t) by constructing the corresponding SVM problem and finding a (w(t), bt) for which the primal and dual objective function values differ by at most t.\nAfter finding (w(t), bt), the SVMOptimizer then evaluates the dual objective function value of the SVM to determine lt. The primal objective function value ut and its gradient g(t) w.r.t. v (calculated on line 9 of Algorithm 2) define the cut ut + 〈 g(t), v − v(t) 〉 . Notice that since Ψ(w(t), bt, v;w′, b′) is a linear function of v, it is equal to this cut function, which therefore upper-bounds minw,b Ψ(w, b, v;w′, b′). One advantage of this cutting-plane formulation is that typical CutChooser implementations will choose\nt to be large in the early iterations, and will only shrink it to be or smaller once we’re close to convergence. We leave the details of the analysis to the appendices—a summary can be found in Appendix E."
    }, {
      "heading" : "3 Related work",
      "text" : "One strategy to satisfy some of the goals described in Section 1 is to first train an unconstrained classifier, and then adjust the decision threshold to satisfy the goals as a second step. Another standard approach is to reweight groups of examples as a preprocessing step—notice that, in our formulation, the Lagrange multipliers v play an identical role to such weights, with the difference being that they are dynamically chosen so as to satisfy constraints, rather than being provided by the user.\nCollobert et al. [2006] also use a ramp loss as a relaxation of 0/1 loss (for optimizing accuracy), but do not consider constraints. The most related prior work is that of Gasso et al. [2011]. They also use an iterative ramp loss approximation, but only tackle the Neyman-Pearson problem, and their algorithm is less straightforward than that presented here. Gasso et al. [2011] compared their Neyman-Pearson classifier\nto that of Davenport et al. [2010], which differs in that it uses a hinge loss approximation instead of the ramp loss, and found with the ramp-loss they achieved similar or slightly better results with up to 10× less computation.\nNarasimhan et al. [2015] considered optimizing for the F-measure and other quantities that can be written as concave functions of the TP and TN. In their proposed stochastic dual solver, they adaptively linearize concave functions of the rate functions (Equation 1). Joachims [2005] indirectly optimizes upper-bounds on functions of sp(D+), sp(D−), sn(D+), sn(D−) using a hinge loss approximation."
    }, {
      "heading" : "4 Experiments",
      "text" : "We demonstrate the accuracy of the proposed algorithm for satisfying these goals on a benchmark dataset for fairness, and a real-world problem with churn and recall constraints."
    }, {
      "heading" : "4.1 Fairness",
      "text" : "We compare training for fairness on the benchmark Adult dataset 2, the same dataset used by Zafar et al. [2015]. The 32 561 training and 16 281 testing examples, derived from the 1994 Census, are 123-dimensional and sparse. Each feature contains categorical attributes such as race, gender, education levels and relationship status. A positive class label means that individual’s income exceeds 50k. Let DM and DF denote the sets of male and female examples. The number of positive labels in DM is roughly six times that of DF . The goal is to train a classifier that respects the fairness constraint sp ( DM ) ≤ sp ( DF ) /κ. for a parameter κ ∈ (0, 1] (where κ = 0.8 corresponds to the 80% rule mentioned in Section 1). Our publicly-available Julia implementation3 for these experiments uses LIBLINEAR [Fan et al., 2008] to implement the SVMOptimizer function, and does not include an unregularized bias b. The outer optimization over v does not use the m-dimensional cutting plane algorithm of Algorithm 2, instead using a simpler one-dimensional variant (observe that these experiments involve only one constraint).\n2“a9a” from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html 3Link redacted for blind review\nWe compare to the method of Zafar et al. [2015], which proposed handling fairness with the constraint: 〈w, x̄〉 ≤ c, x̄ = ∣∣DM ∣∣−1∑x∈DMx − ∣∣DF ∣∣−1∑x∈DF x. (6)\nWe optimized an SVM subject to this constraint (see Appendix B for details), for a range of c values. Results in Figure 2 show the proposed method is much more accurate for any desired fairness, and achieves fairness ratios not reachable with the approach of Zafar et al. [2015] for any choice of c. It is also easier to control: the values of c in Zafar et al. [2015] do not have a clear interpretation, whereas κ is an effective proxy for the fairness ratio."
    }, {
      "heading" : "4.2 Churn",
      "text" : "Our second set of experiments demonstrates meeting real-world requirements on a proprietary problem from a large internet services company : predicting whether a user interface element should be shown to a user, based on 31 informative features. We are given a currently-deployed model, which we refer to as the baseline. The goals are threefold: train a classifier that (i) has high accuracy, (ii) has no worse recall than the baseline, and (iii) has low churn w.r.t. the baseline.\nWe are given three datasets, D1, D2 and D3, consisting of 131 840, 53 877 and 68 892 examples, respectively. The datasets D1 and D2 are hand-labeled, while D3 is unlabeled. In addition, D1 was chosen via active sampling, while D2 and D3 are sampled i.i.d. from the underlying data distribution. For all three datasets, we split out 80% for training and reserved 20% for testing. We address the three goals in the proposed framework by simultaneously training the classifier to minimize the number of errors on D1 plus the number of false positives on D2, subject to the constraints that the recall on D2 be at least as high as the baseline recall (we’re essentially performing Neyman-Pearson classification on D2), and that the churn w.r.t. the baseline on D3 be no larger than a given target parameter.\nWe found that 31-dimensional linear models were not capable of outperforming the baseline model. Instead, we use a fixed feature transformation Φ that maps each x to a roughly 30 000-dimensional feature vector, and train classifiers that are linear with respect to Φ(x).\nThese experiments use a proprietary C++ implementation of Algorithm 2, using the combined SDCA and cutting plane approach of Appendix D to implement the inner optimizations over w and b, with the CutChooser helper functions being as described in Appendices C.1 and D.2.1. We performed 5 iterations of the majorization-minimization procedure of Algorithm 1.\nThe results in Figure 3 show the achieved churn and error rates on train and test sets plotted over a range of churn constraint values (blue line), compared to training an SVM without constraints and then changing its decision threshold to achieve the desired recall (green line). When using deterministic thresholding of the learned classifier (the blue curves, which significantly outperformed randomized classification–the red curves–in this experiment), the proposed method achieves lower churn and better accuracy for all targeted churn rates, while also meeting the recall constraint.\nAs expected, the empirical churn is extremely close to the targeted churn on the training set for the stochastic relaxation (red dashed line, and black line, top left plot), but less so on the 20% held-out test set (top right plot). We hypothesize this disparity is due to overfitting, as the classifier has 30 000 parameters, and D3 is relatively small. However, except for the lowest targeted churn, the actual classifier churn (blue line, top plots) is substantially lower than the targeted churn."
    }, {
      "heading" : "A Ratio metrics",
      "text" : "Problem 2 minimizes an objective function and imposes upper-bound constraints on metrics that are written as linear combinations of positive and negative rates—we refer to such as “linear combination metrics”. Some metrics of interest, however, cannot be written in this form. One important subclass are the so-called “ratio metrics”, which are ratios of linear combinations of rates. Examples of ratio metrics are precision, F-score, win/loss ratio and win/change ratio (recall is a linear combination metric, since its denominator is a constant).\nRatio metrics may not be used directly in the objective of Problem 2, but can be included in constraints by multiplying through by the denominator, then shifting the constraint coefficients to be non-negative. For example, the constraint that precision must be greater than 90% can be expressed as follows:∣∣D+∣∣ sp (D+) ≥0.9 (∣∣D+∣∣ sp (D+)+ ∣∣D−∣∣ sp (D−))\n0.1 ∣∣D+∣∣ sp (D+)− 0.9 ∣∣D−∣∣ sp (D−) ≥0\n−0.1 ∣∣D+∣∣ sp (D+)+ 0.9 ∣∣D−∣∣ sp (D−) ≤0\n0.1 ∣∣D+∣∣ sn (D+)+ 0.9 ∣∣D−∣∣ sp (D−) ≤0.1 ∣∣D+∣∣ ,\nwhere we used the fact that sp (D+) + sn (D+) = 1 on the last line—this is an example of a fact that we noted in Section 2: since positive and negative rates must sum to one, it is possible to write any linear combination of rates as a positive linear combination, plus a constant.\nMultiplying through by the denominator is fine for the indicator version of Problem 2, but a natural question is whether, by using a stochastic classifier and optimizing the ramp version, we’re doing the “right thing” in expectation. The answer is: only partly. Since the expectation of a ratio is not the ratio of expectations, e.g. a precision constraint in our original problem (Problem 2) becomes only a constraint on a precision-like quantity (the ratio of the expectations of the precision’s numerator and denominator) in our relaxed problem."
    }, {
      "heading" : "B Summary Examples",
      "text" : "We note that the constraints of Zafar et al. [2015] can be interpreted as a relaxation of the constraint −c ≤ sp(DA;w)− sp(DB ;w) ≤ c under the linear approximation\nsp(D;w, b) ≈ 1 |D| ∑ x∈D (〈w, x〉 − b)\ngiving:\nsp(D A;w, b)− sp(DB ;w, b) ≈\n1 |DA| ∑ x∈DA (〈w, x〉 − b)− 1 |DB | ∑ x∈DB (〈w, x〉 − b) = 〈w, x̄〉\nwhere x̄ is defined as in Equation 6. We can therefore simulate the approach of Zafar et al. [2015] within our framework by adding the constraints:\n〈w, x̄〉 ≤ c ⇐⇒ max{0, 1− 〈w, x̄〉} ≤ c+ 1 c ≤ 〈w, x̄〉 ⇐⇒ max{0, 1 + 〈w, x̄〉} ≤ c+ 1\nand solving the hinge constrained optimization problem described in Problem 3. Going further, we could implement these constraints as egregious examples using the constraint:\n〈w, x̄〉 ≤ c ⇐⇒ 〈 w, 1\n4c x̄\n〉 ≤ 1\n4 ⇐⇒ 1 2 +\n〈 w, 1\n4c x̄\n〉 ≤ 3\n4 ⇐⇒ min { max { 1\n2 +\n〈 w, 1\n4c x̄\n〉 , 0 } , 1 } ≤ 3\n4 ⇐⇒ rp(x̄) ≤\n3\n4\npermitting us to perform an analogue of their approximations in ramp form."
    }, {
      "heading" : "C Cutting plane algorithm",
      "text" : "We’ll now discuss some variants of Algorithm 2. To simplify the presentation, we’ll define:\nψ (v) = min w,b\nΨ(w, b, v;w′, b′)\nas the function that we are attempting to maximize. We assume that:\n• V ⊆ Rm is compact and convex.\n• ψ : V → R is concave.\n• ψ has a (not necessarily unique) maximizer v∗ = argmaxv∈V ψ (v).\nC.1 Maximization-based We’re primarily interested in proving convergence rates, and will do so in Appendix C.2. With that said, there is one easy-to-implement variant of Algorithm 2 for which we have not proved a convergence rate, but that we use in some of our experiments due to its simplicity:\nDefinition 1. (Maximization-based Algorithm 2) CutChooser chooses v(t) = argmaxv∈V ht (v) and t = (Ut − Lt)/2.\nObserve that this v(t) can be found at the same time as Ut is computed, since both result from optimization of the same linear program. However, despite the ease of implementing this variant, we have not proved any convergence rates about it.\nC.2 Center of mass-based We’ll now discuss a variant of Algorithm 2 that chooses v(t) and t based on the center of mass of the “superlevel hypograph” determined by ht and Lt. The hypograph of ht is the set of m+ 1-dimensional points (v, z) for which z ≤ ht(z), and the intersection of the hypograph with the half-space containing all points for which z ≥ Lt is what we call the “superlevel hypograph”. Notice that, in the context of Algorithm 2, the superlevel hypograph defined by ht and Lt corresponds to the set of pairs of candidate maximizers and their possible function values at the tth iteration. Because this variant is based on finding a cut center in the m+ 1-dimensional hypograph, rather than an m-dimensional level set (which is arguably more typical), this is an instance of what Boyd and Vandenberghe [2011] call an “epigraph cutting plane method”.\nThroughout this section, we will take µ to be the Lebesgue measure (either 1-dimensional, m-dimensional, or m+ 1-dimensional, depending on context). We also must define some notation for dealing with superlevel sets and hypographs. For a concave f : V → R and y ∈ R, define:\nL (f, y) = {v ∈ V | f (v) ≥ y}\nas the superlevel set of f at y. Further define:\nG (f, y) = {(v, z) ∈ V × R | f (v) ≥ z ≥ y}\nas the superlevel hypograph of f above y. With these definitions in place, we’re ready to explicitly state the center of mass-based rule for the CutChooser function on line 8 of Algorithm 2:\nDefinition 2. (Center of mass-based Algorithm 2) CutChooser takes (v(t), zt) to be the center of mass of S(ht, Lt), and define t = (zt − Lt)/2.\nOur final bit of “setup“ before getting to our results is to state two classic theorems, plus a corollary, which will be needed for our proofs. The first enables us to interpolate the areas of superlevel sets:\nTheorem 1. Suppose that the superlevel sets of a concave f : V → R at y1 and y2 are nonempty, and take γ ∈ [0, 1]. Then:\n(µ (L (f, γy1 + (1− γ) y2))) 1/m ≥ γ (µ (L (f, y1))) 1/m + (1− γ) (µ (L (f, y2))) 1/m\nProof. This is the Brunn-Minkowski inequality [e.g. Ball, 1997].\nThis theorem has the immediate useful corollary:\nCorollary 1. Suppose that f : V → R is concave with a maximizer v∗ ∈ V, and that δ ≥ 0. Then:( δ\nm+ 1\n) µ (L (f, f (v∗)− δ)) ≤ µ (G (f, f (v∗) + δ)) ≤ δµ (L (f, f (v∗)− δ))\nProof. By Theorem 1 (lower-bounding the second term on the RHS by zero), for 0 ≤ z ≤ δ:\nµ (L (f, f (v∗)− z)) ≥ (z δ )m µ (L (f, f (v∗)− δ))\nFrom which integrating µ (G (f, f (v∗)− δ)) = ∫ δ 0 µ (L (f, f (v∗)− z))mµ(z) yields the claimed lower bound. The upper bound follows immediately from the fact that the superlevel sets shrink as z increases (i.e. µ (L (f, z′)) ≤ µ (L (f, z)) for z′ ≥ z).\nThe second classic result enables us to bound how much “progress” is made by a cut based on the center of mass of a superlevel hypograph:\nTheorem 2. Suppose that S ⊆ Rm is a convex set. If we let z ∈ S be the center of mass of S, then for any half-space H 3 z:\nµ (S ∩H) µ (S)\n≥ ( m\nm+ 1 )m ≥ 1 e\nProof. This is Theorem 2 of Grünbaum [1960].\nWith the preliminaries out of the way, we’re ready to move on to our first result: bounding the volumes of the superlevel hypographs of our hts, assuming that we base our cuts on the centers of mass of the superlevel hypographs:\nLemma 1. In the context of Algorithm 2, suppose that we choose v(t) and t as in Definition 2. Then: µ (G (ht+1, Lt+1)) ≤ (\n1− 1 2e\n) µ (G (ht, Lt))\nfrom which it follows that:\nµ (G (ht, Lt)) ≤ (\n1− 1 2e\n)t−1 (u0 − l0)µ (V)\nfor all t.\nProof. We’ll consider two cases: ut ≤ zt and ut > zt, corresponding to making a “deep” or “shallow” cut, respectively.\nDeep cut case: If ut ≤ zt, then the hyperplane ut + 〈 g(t), v − v(t) 〉 passes below the center of mass of\nS(ht, Lt), implying by Theorem 2 that: µ (G (ht+1, Lt+1)) ≤ µ (G (ht+1, Lt)) ≤ (\n1− 1 e\n) µ (G (ht, Lt))\nShallow cut case: Now suppose that ut > zt. Applying Theorem 2 to the level cut {(v, z) | z ≤ zt} at zt:\n1 e µ (G (ht, Lt)) ≤ ∫ zt Lt µ ({v ∈ V | ht (v) ≥ z}) dµ(z)\n≤ ∫ (zt+Lt)/2 Lt µ ({v ∈ V | ht (v) ≥ z}) dµ(z)\n+ ∫ zt (zt+Lt)/2 µ ({v ∈ V | ht (v) ≥ z}) dµ(z)\nSince ht is concave, its superlevel sets shrink for larger z, so the first integral on the RHS above is larger than the second, implying that:\n1\n2e µ (G (ht, Lt)) ≤ ∫ (zt+Lt)/2 Lt µ ({v ∈ V | ht (v) ≥ z}) dµ(z)\nThe fact that t = (zt − Lt)/2 implies that lt > (zt + Lt)/2, so Lt+1 > (zt + Lt)/2, and:\n1\n2e µ (G (ht, Lt)) ≤ ∫ Lt+1 Lt µ ({v ∈ V | ht (v) ≥ z}) dµ(z)\nshowing that we will cut off at least a 1/2e-proportion of the total volume, completing the proof of the first claim.\nThe second claim follows immediately by iterating the first, and observing that µ (G (h1, L1)) = (u0 − l0)µ (V).\nThe above result shows that the volumes of the superlevel hypographs of the hts shrink at an exponential rate. However, our actual stopping condition (line 5 of Algorithm 2) depends not on the volume, but rather the “height” Ut − Lt, so we would prefer a bound on this height, rather than the volume. We find such a bound in the (proof of the) following lemma, which establishes how many iterations must elapse before the stopping condition is satisfied:\nLemma 2. In the context of Algorithm 2, suppose that we choose v(t) and t as in Definition 2. Then there is a iteration count T satisfying:\nT = O ( m ln ( u0 − l0 ) + ln ( µ (V)\nµ (L (ψ, l0)) )) such that, if t ≥ T , then Ut − Lt ≤ . Hence, Algorithm 2 will terminate after T iterations.\nProof. By Corollary 1:\nµ (G (ht, Lt)) ≥ ( Ut − Lt m+ 1 ) µ (L (ht, Lt))\nIf Lt ≤ ψ (v∗)− , then µ (L (ht, Lt)) ≥ µ (L (ht, ψ (v∗)− )) because ht is concave. If Lt > ψ (v∗)− , then by Theorem 1:\nµ (L (ht, Lt)) ≥ (\nUt − Lt Ut − ψ (v∗) +\n)m µ (L (ht, ψ (v ∗)− ))\nIn either case, Lt ≤ ψ (v∗) by definition, and we’ll assume that Ut − Lt > (this will lead to a contradiction), so: µ (G (ht, Lt)) ≥ 2−m ( Ut − Lt m+ 1 ) µ (L (ht, ψ (v ∗)− ))\nApplying Lemma 1 yields that:( 1− 1\n2e\n)t−1 (u0 − l0)µ (V) ≥ 2−m ( Ut − Lt m+ 1 ) µ (L (ht, ψ (v ∗)− ))\nNext observe that, by Theorem 1:\nµ (L (ht, ψ (v ∗)− )) ≥\n( Ut − ψ (v∗) +\nUt − l0\n)m µ (L (ht, l0)) ≥ (\nu0 − l0\n)m µ (L (ψ, l0))\nCombining the previous two equations gives: Ut − Lt ≤ (\n1− 1 2e\n)t−1 (m+ 1) ( 2 )m (u0 − l0)m+1 ( µ (V)\nµ (L (ψ, l0)) ) Simplifying this inequality yields that, if we have performed the claimed number of iterations, then Ut−Lt ≤ (this contradicts our earlier assumption that Ut − Lt > , so this is technically a proof by contradiction).\nThe second term in the bound on T measures how closely V matches with the set of all points z on which ψ (z) exceeds our initial lower bound l0. Observe that if l0 ≤ ψ (v) for all v ∈ V, then µ (L (ψ, l0)) = µ (V), and this term will vanish.\nBounding the number of cutting-plane iterations that will be performed is not enough to establish how quickly our procedure will converge, since we rely on performing an inner SVM optimizations with target suboptimality t, and the runtime of these inner optimizations naturally will depend on the magnitudes of the ts, which are bounded in our final lemma:\nLemma 3. In the context of Algorithm 2, suppose that we choose v(t) and t as in Definition 2. Then:\nt ≥ Ut − Lt\n2e (m+ 1)\nand in particular, for all t (before termination):\nt ≥\n2e (m+ 1)\nsince we terminate as soon as Ut − Lt ≤ . Proof. Because ht is concave:\nµ (G (ht, Lt))− µ (G (ht, zt)) ≤ (zt − Lt)µ (L (ht, Lt))\nwhere zt is as in Lemma 1. By Corollary 1, µ (L (ht, Lt)) ≤ m+1Ut−Ltµ (G (ht, Lt)), which combined with the above inequality gives that:\nµ (G (ht, Lt))− µ (G (ht, zt)) µ (G (ht, Lt)) ≤ zt − Lt Ut − Lt (m+ 1)\nBy Theorem 2, the LHS is at least 1/e, and zt − Lt = 2 t, giving the claimed result."
    }, {
      "heading" : "D SVM optimization",
      "text" : "We’ll now move onto a discussion of how we propose implementing the SVMOptimizer of Algorithm 2. The easier-to-analyze approach, based on an inner SDCA optimization over w [Shalev-Shwartz and Zhang, 2013] and an outer cutting plane optimization over b (Algorithm 3), will be described in Appendices D.1 and D.2. The easier-to-implement version, which simply calls an off-the-shelf SVM solver, will be described in Appendix D.4.\nD.1 SDCA w-optimization To simplify the presentation, we’re going to begin by reformulating Equation 5 in such a way that all of the datasets are “mashed together”, with the coefficients being defined on a per-example basis, rather than per-dataset. To this end, for fixed w′ and b′, we define, for every i ∈ {1, . . . , k} and every x ∈ Di:\nα̌ (0) i,x =\n{ α (0) i if 〈w′, x〉 − b′ ≤ 1/2\n0 otherwise\nβ̌ (0) i,x =\n{ β (0) i if 〈w′, x〉 − b′ ≥ −1/2\n0 otherwise\nThis takes care of the loss coefficients. For the constraint coefficients, define:\nα̌ (j) i,x =\n{ α (j) i if 〈w′, x〉 − b′ ≤ 1/2\n0 otherwise\nβ̌ (j) i,x =\n{ β (j) i if 〈w′, x〉 − b′ ≥ −1/2\n0 otherwise\nand finally, we need to handle the constraint upper bounds:\nγ̌(j) =γ(j) − k∑ i=1 1 |Di| ( α (j) i |{x ∈ Di | 〈w ′, x〉 − b′ > 1/2}|\n+β (j) i |{x ∈ Di | 〈w\n′, x〉 − b′ < −1/2}| )\nObserve that the α̌(0)i,xs, β̌ (0) i,x s, α̌ (j) i,xs, β̌ (j) i,x s, and γ̌ (j)s all have implicit dependencies on w′ and b′. In terms of these definitions, the Ψ defined in Equation 5 can be written as:\nΨ (w, b, v;w′, b′) = k∑ i=1 1 |Di| ∑ x∈Di α̌(0)i,x + m∑ j=1 vjα̌ (j) i,x max{0, 1 2 + (〈w, x〉 − b) }\n+ β̌(0)i,x + m∑ j=1 vj β̌ (j) i,x max{0, 1 2 − (〈w, x〉 − b) } + λ\n2 ‖w‖22 − m∑ j=1 vj γ̌ (j)\nThis formulation makes it clear that minimizing Ψ as a function of w and b is equivalent to optimizing an SVM, since Ψ is just a positive linear combination of hinge losses, plus a `2 regularizer, plus a term that does not depend on w or b. Since Ψ can have both “positive” and “negative” hinge losses associated with the same\nexample, however, it’s slightly simpler to combine both hinge losses together into a single piecewise linear per-example loss, rather than decomposing it into two separate hinges:\n`i,x (z) = α̌i,x max\n{ 0, 1\n2 + z\n} + β̌i,x max { 0, 1 2 − z }\nwhere:\nα̌i,x = n\n|Di| α̌(0)i,x + m∑ j=1 vjα̌ (j) i,x  and β̌i,x = n|Di| β̌(0)i,x + m∑ j=1 vj β̌ (j) i,x  Here, n = ∑k i=1 |Di| is the total number of examples across all of the datasets—we introduced the n factor here so that Ψ will be written in terms of the average loss (rather than the total loss). Although it is not represented explicitly in our notation, it should be emphasized that `i,x implicitly depends on v, w′ and b′.\nAs the sum of two hinges, the `i,xs are Lipschitz continuous in z, with the Lipschitz constant being:\nL = max i∈{1,...,k}\nn\n|Di| (α(0)i + β(0)i )+ V m∑ j=1 ( α (j) i + β (j) i ) (7) where V = maxj∈{1,...,m} vj is a uniform upper bound on the magnitudes of the Lagrange multipliers associated with the constraints. Notice that, if the datasets are comparable in size, then n/ |Di| will be on the order of k, so L will typically not be as large as the n-dependence of its definition would appear to imply.\nWe may now write Ψ in terms of the loss functions `i,x:\nΨ (w, b, v;w′, b′) = 1\nn k∑ i=1 ∑ x∈Di `i,x (〈w, x〉 − b) + λ 2 ‖w‖22 − m∑ j=1 vj γ̌ (j)\nThis is the form considered by Shalev-Shwartz and Zhang [2013], so we may apply SDCA:\nTheorem 3. If we use SDCA [Shalev-Shwartz and Zhang, 2013] to optimize Equation 8 for fixed b and v, then we will find a suboptimal solution with duality gap ′′ after performing at most:\nT ′′ = O ( max { 0, n ln ( λn\nL2X2\n)} + n+ L2X2\nλ ′′ ) iterations, where X = maxi∈{1,...,k}maxx∈Di ‖x‖2 is a uniform upper bound on the norms of the training examples.\nProof. This is Theorem 2 of Shalev-Shwartz and Zhang [2013].\nSDCA works by, rather than directly minimizing Ψ over w, instead maximizing the following over the dual variables ξ:\nΨ∗ (ξ, b, v;w′, b′) = (8)\n− 1 n k∑ i=1 ∑ x∈Di `∗i,x (ξi,x)− 1 2λ ∥∥∥∥∥ 1n k∑ i=1 ∑ x∈Di ξi,xx ∥∥∥∥∥ 2\n2\n− 1 n k∑ i=1 ∑ x∈Di ξi,xb− m∑ j=1 vj γ̌ (j)\nusing stochastic coordinate ascent, where:\nw = − 1 λn k∑ i=1 ∑ x∈Di ξi,xx\nis the primal solution w corresponding to a given set of dual variables ξ, and:\n`∗i,x (ξi,x) = 1\n2 ∣∣ξi,x − α̌i,x + β̌i,x∣∣− 1 2 ( α̌i,x + β̌i,x ) is the Fenchel conjugate of `i,x, and is defined for −β̌i,x ≤ ξi,x ≤ α̌i,x (these bounds become box constraints on the ξs of Equation 8).\nAlgorithm 3 Skeleton of a cutting-plane algorithm that finds a b ∈ B minimizing (to within ) minw Ψ(w, b, v;w\n′, b′), where B ⊆ R is a closed bounded interval. It is assumed that l0, u0 ∈ R are finite, and that l0 ≤ maxv minw Ψ(w, b, v;w′, b′) ≤ u0. The u′0 increase that is “maybe” performed on line 2, and the CutChooser function on line 9, are discussed in Appendix D.2. The SDCAOptimizer function is as described in Appendix D.1.\nSVMOptimizer (v, l′0, u′0, ′) 1 Initialize g′0 ∈ R to zero 2 Maybe update u′0 = u′0 + (u′0 − l′0) // needed for Lemma 4, but might not help in practice 3 For t ∈ {1, 2, . . . } 4 Let h′t (b) = maxs∈{0,1,...,t−1} (l′s + g′s (b− bs)) 5 Let L′t = minb∈B h′t (b) and U ′t = mins∈{0,1,...,t−1} u′s 6 If U ′t − L′t ≤ ′ then 7 Let s ∈ {1, . . . , t− 1} be an index minimizing u′s 8 Return w(s), bs, L′t 9 Let bt, t = CutChooser (h′t, U ′t) 10 Let ξ(t), w(t) = SDCAOptimizer (bt, v, ′t) 11 Let u′t = Ψ(w(t), bt, v;w′, b′) 12 Let l′t = Ψ∗(ξ(t), bt, v;w′, b′) and g′t =\n∂ ∂′b Ψ∗(ξ(t), bt, v;w ′, b′)\nD.2 Cutting plane b-optimization Having described in the previous section how we may optimize over w for fixed b and v using SDCA, we now move on to the problem of creating the SVMOptimizer needed by Algorithm 2, which must optimize over both w and b.\nMany linear SVM optimizers do not natively handle an unregularized bias parameter b, and this has long been recognized as a potential issue. For example, Shalev-Shwartz et al. [2011] suggest using Pegasos to perform inner optimizations over w, and a bisection-based outer optimization over b. Our proposal is little different from this, except that Algorithm 3, rather than using bisection, optimizes over b using essentially the same cutting plane algorithm as we used in Algorithm 2. Our two cutting-plane algorithms are essentially identical, except that optimizing over b is a minimization problem (over v is maximization), and we might increase u0 on line 2 for a technical reason (it will be needed by the proof of Lemma 4, but is probably not helpful in practice).\nD.2.1 Minimization-based\nPerhaps the easiest-to-implement version of Algorithm 3 is based on the idea of simply solving for the minimizer of h′t at every iteration.\nDefinition 3. (Minimization-based Algorithm 3) Do not increase u′0 on line 2, and have CutChooser choose bt = argminb∈B h′t (b) and ′t = (Ut − Lt)/2.\nAs was the case in Appendix C.1, we have no convergence rates for this version.\nD.3 Center of mass-based Essentially the same center of mass-based approach as was described in Appendix C.2 can be used in this setting, except that we must find the center of mass of a 2-dimensional sublevel epigraph, rather than an m+ 1-dimensional superlevel hypograph:\nDefinition 4. (Center of mass-based Algorithm 3) Do increase u′0 on line 2, have CutChooser take (bt, zt) to be the center of mass of {(b, z) | h′t (b) ≤ z ≤ U ′t}, and choose ′t = (U ′t − zt)/2.\nDue to the similarity between Algorithms 3 and 2, we can simply recycle the results of Appendix C.2, with the troublesome second term in the bound of Lemma 2 removed by combining the “maybe” portion of Algorithm 3 with the Lipschitz continuity of Ψ as a function of b:\nLemma 4. In the context of Algorithm 3, suppose that we choose bt and ′t as in Definition 4. Then there is a iteration count T ′ satisfying:\nT ′ = O ( ln ( LB (u′0 − l′0)\n′ )) such that, if t ≥ T ′ , then U ′t − L′t ≤ ′, where B = [−B,B] is the set of allowed biases and L is as in Equation 7. Hence, Algorithm 3 will terminate after T ′ iterations.\nProof. Starting from (and adapting) the final equation in the proof of Lemma 2:\nU ′t − L′t ≤32 (\n1− 1 2e\n)t−1( 1\n′\n) (u′0 − l′0) 2\n· (\nB\nµ ({b ∈ [−B,B] | minw∈Rd Ψ (w, b, v;w′, b′) ≤ u′0}) ) Observe that, as a function of b, Ψ (w, b, v;w′, b′) is L-Lipschitz. Hence, if we let w∗ ∈ Rd, b∗ ∈ [−B,B] be the optimal weight and bias, then:\nµ ({b ∈ [−B,B] | Ψ (w∗, b, v;w′, b′) ≤ u′0}) ≥ min { 2B, u′0 − b∗\nL } Since minw∈Rd Ψ (w, b, v;w′, b′) ≤ Ψ (w∗, b, v;w′, b′), it follows that:\nU ′t − L′t ≤ 32 (\n1− 1 2e\n)t−1( 1\n′\n) (u′0 − l′0) 2 max { 1\n2 ,\nLB\nu′0 − b∗ } This is the reason that we increased u′0 on line 2 of Algorithm 3, since doing so has the result that u′0 − b∗ ≥ u′0 − l′0, so:\nU ′t − L′t ≤ 32 (\n1− 1 2e\n)t−1( 1\n′\n) (u′0 − l′0) max { u′0 − l′0\n2 , LB } The same reasoning as was used in the proof of Lemma 2 then gives the claimed bound on T ′ .\nIn addition to the above result, the obvious analogue of Lemma 3 holds as well:\nLemma 5. In the context of Algorithm 3, suppose that we choose bt and ′t as in Definition 4. Then:\n′t ≥ U ′t − L′t\n2e\nand in particular, for all t (before termination):\n′t ≥ ′\n2e\nsince we terminate as soon as U ′t − L′t ≤ ′.\nProof. Same as Lemma 3.\nIn Appendix E, we’ll combine these results with those of Appendices D.1 and C to bound the overall convergence rate of Algorithm 2.\nD.4 Kernelization The foregoing discussion covers the case in which we wish to learn a linear classifier, and use an SVM optimizer (SDCA) that doesn’t handle an unregularized bias. It’s clear that we could freely substitute another linear SVM optimizer for SDCA, as long as it finds both a primal and dual solution, enabling us to calculate the lower and upper bounds required by Algorithm 2.\nOur technique is easily kernelized—the resulting algorithm simply depends on inner kernel SVM optimizations, rather than linear SVM optimizations. SDCA can be used in the kernel setting, but the per-iteration cost increases from O(d) arithmetic operations to O(n) kernel evaluations, where n is the total size of all of the datasets. Kernel-specific optimizers, such as LIBSVM [Chang and Lin, 2011], will generally work better than SDCA in practice, since they typically have the same per-iteration cost, but each iteration is “smarter”. More importantly, such optimizers usually jointly optimize over w and b, eliminating the need for Algorithm 3. Hence, an implementation based on such an optimizer is the simplest version of our proposed approach, since it would be nothing but Algorithms 1 and 2, with the SVMOptimizer helper function being a call to an off-the-shelf solver."
    }, {
      "heading" : "E Overall convergence rates",
      "text" : "We may now combine the results in Appendices C and D into one bound on the overall convergence rate of Algorithm 2, assuming that we use Algorithm 3, rather than an off-the-shelf SVM solver, to implement the SVMOptimizer. First, for the center of mass-based versions:\nTheorem 4. Assuming that SVMOptimizer is implemented as in Algorithm 3, with the CutChooser functions in Algorithms 2 and 3 being implemented using the center of mass (as in Definitions 2 and 4), and assuming that we have access to an oracle function for finding the center of mass of compact convex polytopes, then Algorithm 2 will perform:\nO ( m ln ( u0 − l0 ) + ln ( µ (V)\nA (ψ, l0) )) iterations, each of which contains a single call to Algorithm 3, with each such call requiring:\nO (ln (LBm))\niterations, each of which contains a single call to SDCAOptimizer, with each such call requiring:\nO ( max { 0, n ln ( λn\nL2X2\n)} + n+ L2X2m\nλ ) iterations, each of which requires O(d) arithmetic operations.\nProof. Follows immediately from combining Lemmas 2, 3, 4 and 5 with Theorem 3, and using the fact that the bounds Lt and Ut are passed from Algorithm 2 to Algorithm 3 as u′0 = Ut and l′0 = Lt.\nIn terms of only n, m, d and , dropping all of the other factors , the overall cost of finding an -suboptimal solution to Problem 3 is therefore Õ ( dnm ln (1/ ) + dm2/ ) total arithmetic operations in the inner SDCA\noptimizers, Õ (m ln (1/ )) calls to the center of mass oracles in Algorithms 2 and 3, and another Õ (m ln (1/ )) calls to a linear programming oracle for finding Ut in Algorithm 2 and Lt in Algorithm 3.\nIt must be pointed out that our reliance on a center of mass oracle is unrealistic, since finding the center of mass is a computationally difficult problem [Nemirovski, 1994, Section 3.2]. With that said, we hope that these results can provide a basis for future work."
    } ],
    "references" : [ {
      "title" : "An elementary introduction to modern convex geometry",
      "author" : [ "K. Ball" ],
      "venue" : "Flavors of Geometry,",
      "citeRegEx" : "Ball.,? \\Q1997\\E",
      "shortCiteRegEx" : "Ball.",
      "year" : 1997
    }, {
      "title" : "Adverse Impact and Test Validation: A Practitioner’s Guide to Valid and Defensible Employment Testing",
      "author" : [ "D. Biddle" ],
      "venue" : null,
      "citeRegEx" : "Biddle.,? \\Q2005\\E",
      "shortCiteRegEx" : "Biddle.",
      "year" : 2005
    }, {
      "title" : "Feature article—the ellipsoid method: A survey",
      "author" : [ "R.G. Bland", "D. Goldfarb", "M.J. Todd" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Bland et al\\.,? \\Q1981\\E",
      "shortCiteRegEx" : "Bland et al\\.",
      "year" : 1981
    }, {
      "title" : "Localization and cutting-plane methods, April 2011. Stanford EE 364b lecture notes",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "Boyd and Vandenberghe.,? \\Q2011\\E",
      "shortCiteRegEx" : "Boyd and Vandenberghe.",
      "year" : 2011
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "Chang and Lin.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chang and Lin.",
      "year" : 2011
    }, {
      "title" : "Trading convexity for scalability",
      "author" : [ "R. Collobert", "F. Sinz", "J. Weston", "L. Bottou" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2006
    }, {
      "title" : "Learning optimally sparse support vector machines",
      "author" : [ "A. Cotter", "S. Shalev-Shwartz", "N. Srebro" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Cotter et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cotter et al\\.",
      "year" : 2013
    }, {
      "title" : "Tuning support vector machines for minimax and NeymanPearson classification",
      "author" : [ "M. Davenport", "R.G. Baraniuk", "C.D. Scott" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Davenport et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Davenport et al\\.",
      "year" : 2010
    }, {
      "title" : "LIBLINEAR: A library for large linear classification",
      "author" : [ "R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Fan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2008
    }, {
      "title" : "Batch and online learning algorithms for nonconvex Neyman-Pearson classification",
      "author" : [ "G. Gasso", "A. Pappaionannou", "M. Spivak", "L. Bottou" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "Gasso et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gasso et al\\.",
      "year" : 2011
    }, {
      "title" : "Partitions of mass-distributions and convex bodies by hyperplanes",
      "author" : [ "B. Grünbaum" ],
      "venue" : "Pacific Journal of Mathematics,",
      "citeRegEx" : "Grünbaum.,? \\Q1960\\E",
      "shortCiteRegEx" : "Grünbaum.",
      "year" : 1960
    }, {
      "title" : "A support vector method for multivariate performance measures",
      "author" : [ "T. Joachims" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Joachims.,? \\Q2005\\E",
      "shortCiteRegEx" : "Joachims.",
      "year" : 2005
    }, {
      "title" : "Simple, robust, scalable semi-supervised learning with expectation regularization",
      "author" : [ "G.S. Mann", "A. McCallum" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Mann and McCallum.,? \\Q2007\\E",
      "shortCiteRegEx" : "Mann and McCallum.",
      "year" : 2007
    }, {
      "title" : "Optimizing non-decomposable performance measures: a tale of two classes",
      "author" : [ "H. Narasimhan", "P. Kar", "P. Jain" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Narasimhan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2015
    }, {
      "title" : "Lecture notes: Efficient methods in convex programming",
      "author" : [ "A. Nemirovski" ],
      "venue" : "URL http://www2.isye. gatech.edu/~nemirovs/Lect_EMCO.pdf",
      "citeRegEx" : "Nemirovski.,? \\Q1994\\E",
      "shortCiteRegEx" : "Nemirovski.",
      "year" : 1994
    }, {
      "title" : "A Neyman-Pearson approach to statistical learning",
      "author" : [ "C.D. Scott", "R.D. Nowak" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Scott and Nowak.,? \\Q2005\\E",
      "shortCiteRegEx" : "Scott and Nowak.",
      "year" : 2005
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Zhang.",
      "year" : 2013
    }, {
      "title" : "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2011
    }, {
      "title" : "Disparate impact doctrine in fair housing",
      "author" : [ "M.S. Vuolo", "N.B. Levy" ],
      "venue" : "New York Law Journal,",
      "citeRegEx" : "Vuolo and Levy.,? \\Q2013\\E",
      "shortCiteRegEx" : "Vuolo and Levy.",
      "year" : 2013
    }, {
      "title" : "Fairness constraints: A mechanism for fair classification",
      "author" : [ "M.B. Zafar", "I. Valera", "M.G. Rodriguez", "K.P. Gummadi" ],
      "venue" : "In ICML Workshop on Fairness, Accountability, and Transparency in Machine Learning,",
      "citeRegEx" : "Zafar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zafar et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Coverage was also considered by Mann and McCallum [2007], who proposed what they call “label regularization”, in which one adds a regularizer that penalizes the relative entropy between the mean classifier score for each class and the desired distribution, with an additional correction to avoid degeneracies.",
      "startOffset" : 32,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "As noted by Zafar et al. [2015], fairness is sometimes specified by a proportion, such as the 80% rule in US law that certain decisions must be in favor of group B individuals at least 80% as often as in favor of group A individuals [e.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "Biddle, 2005, Vuolo and Levy, 2013]. Zafar et al. [2015] propose learning fair classifiers by imposing linear constraints on the covariance between the predicted labels and the values of certain features.",
      "startOffset" : 0,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "Biddle, 2005, Vuolo and Levy, 2013]. Zafar et al. [2015] propose learning fair classifiers by imposing linear constraints on the covariance between the predicted labels and the values of certain features. In our framework, rate constraints such as the 80% rule can be imposed directly. Recall and Precision: Requirements of real-world classifiers are often expressed in terms of precision and recall, especially when examples are highly imbalanced between positives and negatives. In our framework, we can handle this problem via Neyman-Pearson classification [e.g. Scott and Nowak, 2005, Davenport et al., 2010], in which one seeks to minimize the false negative rate subject to a constraint on the false positive rate. Indeed, our ramp-loss formulation is equivalent to that of Gasso et al. [2011] in this setting.",
      "startOffset" : 0,
      "endOffset" : 800
    }, {
      "referenceID" : 6,
      "context" : "We can, however, work around this difficulty by following Cotter et al. [2013] and training a classifier that makes 1Appendices may be found in the supplementary material",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : "randomized predictions based on the ramp function [Collobert et al., 2006]:",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : "Rather, it’s based upon the idea of possibly making “shallow” cuts [Bland et al., 1981] by choosing a desired accuracy t at each iteration, and expecting the SVMOptimizer to return a solution with suboptimality t.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : "Collobert et al. [2006] also use a ramp loss as a relaxation of 0/1 loss (for optimizing accuracy), but do not consider constraints.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "Collobert et al. [2006] also use a ramp loss as a relaxation of 0/1 loss (for optimizing accuracy), but do not consider constraints. The most related prior work is that of Gasso et al. [2011]. They also use an iterative ramp loss approximation, but only tackle the Neyman-Pearson problem, and their algorithm is less straightforward than that presented here.",
      "startOffset" : 0,
      "endOffset" : 192
    }, {
      "referenceID" : 5,
      "context" : "Collobert et al. [2006] also use a ramp loss as a relaxation of 0/1 loss (for optimizing accuracy), but do not consider constraints. The most related prior work is that of Gasso et al. [2011]. They also use an iterative ramp loss approximation, but only tackle the Neyman-Pearson problem, and their algorithm is less straightforward than that presented here. Gasso et al. [2011] compared their Neyman-Pearson classifier",
      "startOffset" : 0,
      "endOffset" : 379
    }, {
      "referenceID" : 19,
      "context" : "Green dots: Zafar et al. [2015]. Green line: unconstrained SVM.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "to that of Davenport et al. [2010], which differs in that it uses a hinge loss approximation instead of the ramp loss, and found with the ramp-loss they achieved similar or slightly better results with up to 10× less computation.",
      "startOffset" : 11,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "to that of Davenport et al. [2010], which differs in that it uses a hinge loss approximation instead of the ramp loss, and found with the ramp-loss they achieved similar or slightly better results with up to 10× less computation. Narasimhan et al. [2015] considered optimizing for the F-measure and other quantities that can be written as concave functions of the TP and TN.",
      "startOffset" : 11,
      "endOffset" : 255
    }, {
      "referenceID" : 7,
      "context" : "to that of Davenport et al. [2010], which differs in that it uses a hinge loss approximation instead of the ramp loss, and found with the ramp-loss they achieved similar or slightly better results with up to 10× less computation. Narasimhan et al. [2015] considered optimizing for the F-measure and other quantities that can be written as concave functions of the TP and TN. In their proposed stochastic dual solver, they adaptively linearize concave functions of the rate functions (Equation 1). Joachims [2005] indirectly optimizes upper-bounds on functions of sp(D), sp(D), sn(D), sn(D) using a hinge loss approximation.",
      "startOffset" : 11,
      "endOffset" : 513
    }, {
      "referenceID" : 8,
      "context" : "Our publicly-available Julia implementation3 for these experiments uses LIBLINEAR [Fan et al., 2008] to implement the SVMOptimizer function, and does not include an unregularized bias b.",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 18,
      "context" : "1 Fairness We compare training for fairness on the benchmark Adult dataset 2, the same dataset used by Zafar et al. [2015]. The 32 561 training and 16 281 testing examples, derived from the 1994 Census, are 123-dimensional and sparse.",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 19,
      "context" : "We compare to the method of Zafar et al. [2015], which proposed handling fairness with the constraint: 〈w, x̄〉 ≤ c, x̄ = ∣∣DM ∣∣−1∑x∈DMx − ∣∣DF ∣∣−1∑x∈DF x.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 19,
      "context" : "We compare to the method of Zafar et al. [2015], which proposed handling fairness with the constraint: 〈w, x̄〉 ≤ c, x̄ = ∣∣DM ∣∣−1∑x∈DMx − ∣∣DF ∣∣−1∑x∈DF x. (6) We optimized an SVM subject to this constraint (see Appendix B for details), for a range of c values. Results in Figure 2 show the proposed method is much more accurate for any desired fairness, and achieves fairness ratios not reachable with the approach of Zafar et al. [2015] for any choice of c.",
      "startOffset" : 28,
      "endOffset" : 440
    }, {
      "referenceID" : 19,
      "context" : "We compare to the method of Zafar et al. [2015], which proposed handling fairness with the constraint: 〈w, x̄〉 ≤ c, x̄ = ∣∣DM ∣∣−1∑x∈DMx − ∣∣DF ∣∣−1∑x∈DF x. (6) We optimized an SVM subject to this constraint (see Appendix B for details), for a range of c values. Results in Figure 2 show the proposed method is much more accurate for any desired fairness, and achieves fairness ratios not reachable with the approach of Zafar et al. [2015] for any choice of c. It is also easier to control: the values of c in Zafar et al. [2015] do not have a clear interpretation, whereas κ is an effective proxy for the fairness ratio.",
      "startOffset" : 28,
      "endOffset" : 530
    }, {
      "referenceID" : 19,
      "context" : "We note that the constraints of Zafar et al. [2015] can be interpreted as a relaxation of the constraint −c ≤ sp(D;w)− sp(D ;w) ≤ c under the linear approximation sp(D;w, b) ≈ 1 |D| ∑",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 19,
      "context" : "We can therefore simulate the approach of Zafar et al. [2015] within our framework by adding the constraints:",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "Because this variant is based on finding a cut center in the m+ 1-dimensional hypograph, rather than an m-dimensional level set (which is arguably more typical), this is an instance of what Boyd and Vandenberghe [2011] call an “epigraph cutting plane method”.",
      "startOffset" : 190,
      "endOffset" : 219
    }, {
      "referenceID" : 10,
      "context" : "This is Theorem 2 of Grünbaum [1960].",
      "startOffset" : 21,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "The easier-to-analyze approach, based on an inner SDCA optimization over w [Shalev-Shwartz and Zhang, 2013] and an outer cutting plane optimization over b (Algorithm 3), will be described in Appendices D.",
      "startOffset" : 75,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "If we use SDCA [Shalev-Shwartz and Zhang, 2013] to optimize Equation 8 for fixed b and v, then we will find a suboptimal solution with duality gap ′′ after performing at most:",
      "startOffset" : 15,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "This is the form considered by Shalev-Shwartz and Zhang [2013], so we may apply SDCA: Theorem 3.",
      "startOffset" : 31,
      "endOffset" : 63
    }, {
      "referenceID" : 16,
      "context" : "This is Theorem 2 of Shalev-Shwartz and Zhang [2013]. SDCA works by, rather than directly minimizing Ψ over w, instead maximizing the following over the dual variables ξ: Ψ∗ (ξ, b, v;w′, b′) = (8)",
      "startOffset" : 21,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : "For example, Shalev-Shwartz et al. [2011] suggest using Pegasos to perform inner optimizations over w, and a bisection-based outer optimization over b.",
      "startOffset" : 13,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "Kernel-specific optimizers, such as LIBSVM [Chang and Lin, 2011], will generally work better than SDCA in practice, since they typically have the same per-iteration cost, but each iteration is “smarter”.",
      "startOffset" : 43,
      "endOffset" : 64
    } ],
    "year" : 2017,
    "abstractText" : "The goal of minimizing misclassification error on a training set is often just one of several real-world goals that might be defined on different datasets. For example, one may require a classifier to also make positive predictions at some specified rate for some subpopulation (fairness), or to achieve a specified empirical recall. Other real-world goals include reducing churn with respect to a previously deployed model, or stabilizing online training. In this paper we propose handling multiple goals on multiple datasets by training with dataset constraints, using the ramp penalty to accurately quantify costs, and present an efficient algorithm to approximately optimize the resulting non-convex constrained optimization problem. Experiments on both benchmark and real-world industry datasets demonstrate the effectiveness of our approach.",
    "creator" : "LaTeX with hyperref package"
  }
}