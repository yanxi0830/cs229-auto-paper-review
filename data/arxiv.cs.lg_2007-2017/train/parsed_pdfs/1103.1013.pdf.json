{
  "name" : "1103.1013.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "IVOR W. TSANG" ],
    "emails" : [ "QMAO1@ntu.edu.sg.", "IvorTsang@ntu.edu.sg." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n10 3.\n10 13\nv2 [\ncs .L\nG ]\n4 M\nay 2"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Machine learning methods have been widely applied to a variety of learning tasks (e.g. classification, ranking, structure prediction, etc) arising in computer vision, text mining, natural language processing and bioinformatics applications. Depending on applications, specific performance measures are required to evaluate the success of a learning algorithm. For instance, the error rate is a sound judgment for evaluating the classification performance of a learning method on datasets with balanced positive and negative examples. On the contrary, in text classification where positive examples are usually very few, one can simply assign all testing examples with the negative class (the major class), this trivial solution can easily achieve very low error rate due to the extreme imbalance of the data. However, the goal of text classification is to correctly detect positive examples. Hence, the error rate is considered as a poor criterion for the problems with highly skewed class distributions [11]. To address this issue, F1-score and Precision/Recall Breakeven Point (PRBEP) are employed as the evaluation criteria for text classification. Besides this, in information retrieval, search engine systems are required to return the top k documents (images) with the highest precision because most users only scan the first few of them presented by the system, so precision/recall at k are preferred choices.\nInstead of optimizing the error rate, Support Vector Machine for multivariate performance measures (SVMperf ) [11] was proposed to directly optimize the losses based on a variety of multivariate performance measures. A smoothing version of SVMperf [37] was proposed to accelerate the convergence of the optimization problem specially designed\nQi Mao and Ivor W. Tsang are with School of Computer Engineering, Nanyang Technological University, Singapore 639798, e-mail {QMAO1,IvorTsang}@ntu.edu.sg.\n1\nfor PRBEP and area under the Receiver Operating Characteristic curve (AUC). Structural SVMs are considered as the general framework for optimizing a variety of loss functions [27, 13, 28]. Other works optimize specific multivariate performance measures, such as F-score [21], normalize discount cumulative gain (NDCG) [29], ordinal regression [12], ranking loss [16] and so on.\nFor some real applications, such as image and document retrievals, a set of sparse yet discriminative features is a necessity for rapid prediction on massive databases. However, the learned weight vector of the aforementioned methods is usually non-sparse. In addition, there are many noisy or non-informative features in text documents and images. Even though the task-specific performance measures can be optimized directly, learning with these noisy or non-informative features may still hurt both prediction performance and efficiency. To alleviate these issues, one can resort to embedded feature selection methods [15], which can be categorized into the following two major directions.\nOne way is to consider the sparsity of a decision weight vector w by replacing l2-norm ‖w‖2 regularization in the structural risk functional (e.g. SVM, logistic regression) with l1-norm ‖w‖1 [39, 8, 23]. A thorough study to compare several recently developed l1regularized algorithms has been conducted in [33]. According to this study, coordinate descent method using one-dimensional Newton direction (CDN) achieves the state-ofthe-art performance by solving l1-regularized models on large-scale and high-dimensional datasets. To achieve a sparser solution, the Approximation of the zeRO norm Minimization (AROM) was proposed [30] to optimize l0 models. Its resultant problem is non-convex, so it easily suffers from local optima. However, the recent results [18] and theoretical studies [17, 36] have showed that lp models (where p < 1) even with a local optimal solution can achieve better prediction performance than convex l1 models, which are asymptotically biased [18].\nAnother way is to sort the weights of a SVM classifier and remove the smallest weights iteratively, which is known as SVM with Recursive Feature Elimination (SVM-RFE) [9]. However, as discussed in [32], such nested “monotonic” feature selection scheme leads to suboptimal performance. Non-monotonic feature selection (NMMKL) [32] has been proposed to solve this problem, but each feature corresponding to one kernel makes NMMKL infeasible for high-dimensional problems. Recently, Tan et al. [26] proposed Feature Generating Machine (FGM), which shows great scalability to non-monotonic feature selection on large-scale and very high-dimensional datasets.\nThe aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only. To fulfill the needs of different applications, it is imperative to have a feature selection method designed for optimizing task-specific performance measures.\nTo this end, we first propose a generalized sparse regularizer for feature selection. After that, a unified feature selection framework is presented for general loss functions based on the proposed regularizer. Particularly, in this paper, optimizing multivariate performance measures is studied in this framework. To our knowledge, this is the first work to optimize multivariate performance measures for feature selection. Due to exponential number of constraints brought by non-smooth multivariate loss functions [11, 13] and exponential number of feature subset combinations [26], the resultant optimization problem is very challenging for high-dimensional data. To tackle this challenge, we propose a two-layer cutting plane algorithm, including group feature generation (see Section 5.1) and group\nfeature selection (see Section 5.2), to solve this problem effectively and efficiently. Specifically, Multiple Kernel Learning (MKL) trained in the primal by cutting plane algorithm is proposed to deal with exponential size of constraints induced by multivariate losses.\nThis paper is an extension of our preliminary work [19]. The main contributions of this paper are listed as follows.\n• The implementation details and the convergence proof of the proposed two-layer cutting plane algorithm and MKL algorithm trained in the primal are presented. • Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details. By comparing with these methods, the advantages of our proposed methods are summarized as follows: (1) The tradeoff parameter C in l1 SVM [33] is too sensitive to be tuned properly\nsince it controls both margin loss and the sparsity of w. However, our method alleviates this problem by introducing an additional parameter B to control the sparsity of w. This separation makes parameter tuning for our methods much easier than those of SKM [3] and l1 SVM. (2) NMMKL [32] uses the similar parameter separation strategy, but it is intractable for this method to handle high-dimensional datasets, let alone optimize multivariate losses. The proposed method can readily optimize multivariate losses for high-dimensional problems. (3) FGM [26] is a special case of the propose framework when optimizing square hinge loss with indicator variables in integer domain. The proposed framework is formulated in the real domain for general loss functions. In particular, we provide a natural extension of FGM for multivariate losses. (4) The proposed framework can be interpreted by l0-norm constraint, so it can be considered as one of l0 methods. This gives another interpretation of the additional parameter B. • Recall that Multiple-Instance Learning via Embedded instance Selection (MILES) [6], which transforms multiple instance learning (MIL) into a feature selection problem by embedding bags into an instance-based feature space and selecting the most important features, achieves state-of-the-art performance for multiple instance learning problems. Under our unified feature selection framework, we extend MILES and study MIL for multivariate performance measure. To our best knowledge, this is seldom studied in MIL scenarios, but it is important for the real world applications of MIL tasks. • Extensive experiments on several challenging and very high-dimensional real world datasets show that the proposed method yields better performance than the stateof-the-art feature selection methods, and outperforms SVMperf using all features in terms of multivariate performance measures. The experimental results on the multiple instance dataset show that our proposed method achieves promising results.\nThe rest of the paper is organized as follows: We briefly review SVMperf in Section 2. We then introduce the proposed generalized sparse regularizer in Section 3. In particular, we study the feature selection framework for multivariate performance measures, its algorithm and its application to multiple instance learning in Section 4, 5 and 7, respectively. Section 6 gives the analysis of connections to a variety of feature selection methods. The extensive empirical results are shown in Section 8. Finally, conclusive remarks are presented in the last section.\nIn the sequel, A 0 means that the matrix A is symmetric and positive semidefinite (psd). We denote the transpose of a vector/matrix by the superscript T and lp norm of a vector v by ||v||p. Binary operator ⊙ represents the elementwise product between two vectors/matrices."
    }, {
      "heading" : "2. SVM FOR MULTIVARIATE PERFORMANCE MEASURE",
      "text" : "Given a training sample of input-output pairs (xi, yi) ∈ X × Y for i = 1, . . . , n drawn from some fixed but unknown probability distribution with X ⊆ Rm and Y ∈ {−1,+1}. The learning problem is treated as a multivariate prediction problem by defining the hypotheses h : X → Y that map a tuple x ∈ X of n feature vectors x = (x1, . . . , xn) to a tuple y ∈ Y of n labels y = (y1, . . . , yn) where X = X × . . . ,X and Y ⊆ {−1,+1}n. The linear discriminative function of SVMperf is defined as\n(1) hw(x) = argmax y′∈Y f(x, y′) = argmax y′∈Y\nn∑\ni=1\ny′iw T xi,\nwhere w = [w1, . . . , wm]T is the weight vector. To learn the hypothesis (1) from training data, large margin method is employed to obtain the good generalization performance by enforcing the constraints that the decision value of the ground truth labels y should be larger than any possible labels y′ ∈ Y\\{y}, i.e., f(x, y′) ≥ f(x, y′) + ∆(y, y′), where ∆(y, y′) is some type of multivariate loss functions (several instantiated losses are presented in Section 5.4). Structural SVMs [28, 13] are proposed to solve the corresponding soft-margin case by 1-slack variable formula as,\nmin w,ξ≥0\n1\n2 ‖w‖22 + Cξ(2)\ns.t. ∀y′ ∈ Y\\y : wT n∑\ni=1\n(yi − y′i)xi ≥ ∆(y, y′)− ξ,\nwhere C is a regularization parameter that trades off the empirical risk and the model complexity.\nThe optimization problem (2) is convex, but there is the exponential size of constraints. Fortunately, this problem can be solved in polynomial time by adopting the sparse approximation algorithm of structural SVMs. As shown in [11], optimizing the learning model subject to one specific multivariate measure can really boost the performance of this measure."
    }, {
      "heading" : "3. GENERALIZED SPARSE REGULARIZER",
      "text" : "In this paper, we focus on minimizing the regularized empirical loss functional as\n(3) min w Ω(w) + Cℓ(w),\nwhere Ω(.) is a regularization function and ℓ(.) is any loss function, including multivariate performance measure losses.\nSince l2-norm regularization is used in (2), the learned weight vector w is non-sparse, and so the linear discriminant function in (1) would involve many features for the prediction. As discussed in Section 1, selecting a small set of discriminative features is crucial to many real applications. In order to enforce the sparsity on w, we propose a new sparse\nregularizer\nΩ(w) = min d∈D\n1\n2\nm∑\nj=1\n|wj |p dj ,\nwhere d is in the real domain of D = {d|∑mj=1 dj = B, 0 ≤ dj ≤ 1, ∀j = 1, . . . ,m}, p > 0 and B > 0 are two parameters. The optimal solution of the new proposed regularizer should satisfy wj = 0 if dj = 0 since |wj |p = 0 with p > 0 induces wj = 0, otherwise the objective value approaches to infinite. The l1-norm constraint ∑m j=1 dj = B and 0 ≤ dj ≤ 1 will force some dj to be zero, so the correspondingwj is zero, ∀j = 1, . . . ,m. Hence, the parameter B is interpreted as a budget to control the sparsity of w.\nThis regularizer is similar to SimpleMKL [24] with each feature corresponding to one kernel, but SimpleMKL is a special case of D with B = 1, which also can be interpreted by the quadratic variational formulation of l1 norm [2]. However, it is different from l1 when B 6= 1. To explain the difference, we consider the problem (2) under the general framework (3). In the separable case, parameter C does not affect the optimum solution since the error ξ = 0. If l1 norm is applied to replace l2 in Problem (2), the sparsity of w will be fixed once optimal solution is reached. Hence, parameter B in D now can be considered as the only factor to enforce sparsity on w. However, in the non-separable case where errors are allowed, parameter C will also influence the sparsity of w, but B is expected to enforce the sparsity of w more explicitly when C becomes larger. This argument will be empirically justified in Section 8.1.\nThe learning algorithm with the proposed generalized sparse regularizer is formulated as\nmin d∈D min w\n1\n2\nm∑\nj=1\n|wj |p dj + Cℓ(w).(4)\nThis formulation is more general for feature selection.\nLemma 1. If p ≥ 2, Problem (4) is jointly convex with respect to w and d; otherwise, it is not jointly convex.\nProof. We only need to prove that, if p ≥ 2, g(wj , dj) = |wj| p\ndj where dj > 0 is jointly con-\nvex with respect to wj and dj . The convexity of g in its domain is established when the fol-\nlowing holds: ∇2g =\n  2|wj | p d3 j\n− p|wj | p−1\nd2 j\n− p|wj| p−1\nd2 j\np(p−1)|wj | p−2\ndj\n  0 ⇔ [ 2|wj |2 −p|wj |dj\n−p|wj|dj p(p− 1)d2j\n]\n0, which is equivalent to vT∇2gv ≥ 0 for any nonzero vector v. WLOG, we assume v = [1 a]T where a is any real number, then this condition is reduced to: 2|wj |2 − 2ap|wj|dj + a2p(p− 1)d2j ≥ 0 ⇔ 2 ( |wj | − apdj2 )2 ≥ a 2d2jp(2−p) 2 . This condition always holds when p ≥ 2, which completes the proof.\nIn what follows, we focus on the convex formulation with p = 2. In Section 6, we will discuss the relationships with a variety of the state-of-the-art feature selection methods."
    }, {
      "heading" : "4. FEATURE SELECTION FOR MULTIVARIATE PERFORMANCE MEASURES",
      "text" : "To optimize the multivariate loss functions and learn a sparse feature representation simultaneously, we propose to solve the following jointly convex problem over d and (w, ξ)\nin the case of p = 2,\nmin d∈D min w,ξ≥0\n1\n2\nm ∑\nj=1\n|wj | 2\ndj + Cξ(5)\ns.t. ∀y′ ∈ Y\\y : wT 1\nn\nn ∑\ni=1\n(yi − y ′ i)xi ≥ ∆(y, y ′)− ξ.\nThe partial dual with respect to (w, ξ) is obtained by Lagrangian function L(w, ξ, α, τ) with dual variablesα ≥ 0 and τ ≥ 0 as follows: 12 ∑m j=1 |wj | 2 dj +Cξ−τξ−∑y′∈Y\\y αy′(wT 1n ∑n i=1(yi− y′i)xi−∆(y, y′)+ξ). As the gradients of Lagrangian function with respect to (w, ξ) vanish at the optimal points, we obtain the KKT conditions: wj = dj ∑ y′∈Y\\y αy′ 1 n ∑n i=1(yi −\ny′i)xj,i and ∑\ny′∈Y\\y αy′ ≤ C. By substituting KKT conditions back to L(w, ξ, α, τ), we obtain the dual problem as\nmin d∈D max α∈A −1 2\n∑\ny′\n∑\ny′′\nαy′αy′′Q d y′,y′′ +\n∑\ny′\nαy′by′ ,(6)\nwhere ∆(y, y) = 0, ∆(y, y′) > 0 if y 6= y′,\nQdy′,y′′ =\nm∑\nj=1\ndj\n( ∑\ny′∈Y\\y\nαy′ 1\nn\nn∑\ni=1\n(yi − y′i)xj,i )2\n=\nm∑\nj=1\n( ∑\ny′∈Y\\y\nαy′ 1\nn\nn∑\ni=1\n(yi − y′i)xj,i √ dj\n)2\n= 〈ay′ , ay′′〉, ay′ = 1n ∑n i=1(yi − y′i)(xi ⊙ √ d), by′ = 1n∆(y, y\n′), and A = {α|∑y′ αy′ ≤ C,α ≥ 0}. Problem (6) is a challenging problem because of the exponential size of α and highdimensional vector d for high-dimensional problems."
    }, {
      "heading" : "5. TWO-LAYER CUTTING PLANE ALGORITHM",
      "text" : "In this section, we propose a two-layer cutting plane algorithm to solve Problem (6) efficiently and effectively. The two layers, namely group feature generation and group feature selection, will be described in Section 5.1 and 5.2, respectively. The two-layer cutting plane algorithm will be presented in Section 5.3 and 5.4.\n5.1. Group Feature Generation. By denoting S(α, d) = − 12 ∑ y′ ∑ y′′ αy′αy′′Q d y′,y′′ +∑\ny′ αy′by′ , Problem (6) turns out to be\nmin d∈D max α∈A S(α, d).\nSince domains D and A are nonempty, the function S(α∗,d) is closed and convex for all d ∈ D given any α∗ ∈ A, and the function S(α,d∗) is closed and concave for all α ∈ A given any d∗ ∈ D, the saddle-point property: mind∈D maxα∈A S(α, d) = maxα∈A mind∈D S(α, d) holds [4].\nWe further denote Fd(α) = −S(α, d), and then the equivalent optimization problems are obtained as\nmin α∈A max d∈D Fd(α) or min α∈A,γ γ : γ ≥ Fd(α), ∀d ∈ D.(7)\nCutting plane algorithm [14] could be used here to solve this problem. Since maxd∈D Fd(α) ≥ Fdt(α), ∀dt ∈ D, the lower bound approximation of (30) can be obtained bymaxd∈D Fd(α) ≥ maxt=1,...,T Fdt(α). Then we minimize Problem (30) over the set {dt}Tt=1 by,\nmin α∈A max t=1,...,T Fdt(α) or min α∈A,γ γ :γ≥Fdt(α), ∀t=1,. . .,T.(8)\nAs from [22], such cutting plane algorithm can converge to a robust optimal solution within tens of iterations with the exact worst-case analysis. Specifically, for a fixed αt, the worstcase analysis can be done by solving,\n(9) dt = argmax d∈D Fd(αt),\nwhich is referred to as the group generation procedure. Even though Problem (8) and (9) cannot be solved directly due to the exponential size of α, we will show that they are readily solved in Section 5.2 and Section 5.4, respectively.\n5.2. Group Feature Selection. By introducing dual variables µ = [µ1, µ2, . . . , µT ]T ≥ 0, we can transform (8) to an MKL problem as follows,\n(10) max α∈A min µ∈MT\n− 1\n2\n∑\ny′\n∑\ny′′\nαy′αy′′\n(\nT ∑\nt=1\nµtQ dt\ny′,y′′\n)\n+ ∑\ny′\nαy′by′ ,\nwhere MT = { ∑T\nt=1 µt = 1, µt ≥ 0, ∀t = 1, . . . , T }. However, due to the exponential size of α, the complexity of Problem (29) remains. In this case, state-of-the-art multiple kernel learning algorithms [25, 24, 31] do not work any more. The following proposition shows that we can indirectly solve Problem (29) in the primal form.\nProposition 1. The primal form of Problem (29) is\nmin w1,...,wT ,ξ≥0\n1\n2\n( T∑\nt=1\n‖wt‖2 )2 + Cξ(11)\ns.t. ξ ≥ by′ − T∑\nt=1\n〈wt, aty′〉, ∀y′ ∈ Y\\y.\nAccording to KKT conditions, the solution of (29) is\nwt = µt ∑\ny′\nαy′aty′(12)\nwhere µt is a dual value of the tth constraint of (8).\nThe detailed proof of Proposition 1 is given in the supplementary material. Here, we define the regularization term as Ω(w) = 12 (∑T t=1 ‖wt‖2 )2 with w = [w1, . . . ,wT ]T and\nthe empirical risk function as (13) Remp(w) = max ( 0, max\ny′∈Y\\y by′ −\nT∑\nt=1\n〈wt, aty′〉 ) ,\nwhich is a convex but non-smooth function w.r.t w. Then we can apply the bundle method [27] to solve this primal problem. Problem (29) is transformed as\nmin w\nJ (w) = Ω(w) + CRemp(w).\nSince Remp(w) is a convex function, its subgradient exists everywhere in its domain [10]. Suppose wk is a point where Remp(w) is finite, we can formulate the lower bound according to the definition of subgradient,\nRemp(w) ≥ Remp(wk) + 〈w − wk, pk〉 = 〈w, pk〉+Remp(wk)− 〈wk, pk〉\nwhere subgradient pk ∈ ∂wRemp(wk) is at wk. In order to obtain pk, we need to solve the following inference problem\n(14) yk = arg max y′∈Y\\y\nby′ − T∑\nt=1\n〈wt, aty′〉\nwhich is a problem of integer programming. We delay the discussion of this problem to Section 5.4. After that, we can obtain the subgraident pkt = −atyk , so that Remp(w k) = byk − ∑T t=1〈wt, atyk〉 = byk + 〈w k, pk〉.\nGiven the subgradient sequence p1, p2, . . . , pK , the tighter lower bound for Remp(w) can be reformulated as follows,\nRemp(w) ≥ RKemp(w) = max ( 0, max\n1≤k≤K 〈w, pk〉+ qk\n) ,\nwhere qk = Remp(wk)−〈wk, pk〉 = byk . Following the bundle method [27], the criterion for selecting the next point wK+1 is to solve the following problem,\nmin w1,...,wT ,ξ≥0\n1\n2\n( T∑\nt=1\n‖wt‖2 )2 + Cξ(15)\ns.t. ξ ≥ 〈w, pk〉+ qk, ∀k = 1, . . . ,K. The following Corollary shows that Problem (15) can be easily solved by QCQP solvers, and the number of variables is independent of the number of examples.\nCorollary 1. In terms of Proposition 1, the dual form of Problem (15) is\nmax α∈AK max θ\n−θ + K∑\nk=1\nαkq k(16)\ns.t. 1\n2 ∥∥∥∥∥ K∑\nk=1\nαkpkt ∥∥∥∥∥ 2\n2\n≤ θ, ∀t = 1, . . . , T,\nwhere AK = { ∑K\nk=1 αk ≤ C,αk ≥ 0, ∀k = 1, . . . ,K}, and which is a QCQP problem with T + 1 constraints and K + 1 variables.\nThe proof of Corollary 1 follows the same derivation of Proposition 1 with pkt = −atyk , qk = byk and the size of αk as K . Consequently, the primal variables are recovered by wt = −µt ∑ k αkp k t .\nLet JK(w) = Ω(w)+CRKemp(w), the ǫ-optimal condition in Algorithm 1 is min0≤k≤K J (wK)− JK(wK) ≤ ǫ. The convergence proof in [27] does not apply in this case as the Fenchel dual of Ω(w) fails to satisfy the strong convexity assumption if K > 1. As K = 1, Algorithm 1 is exactly the bundle method [27]. When K ≥ 2, we can adapt the proof of Theorem 5 in [13] for the following convergence results.\nAlgorithm 1 Group feature selection\n1: Input: x = (x1, . . . , xn), y = (y1, . . . , yn), an initial group set W , ǫ, C 2: Y = ∅, k = 0 3: repeat 4: k = k + 1 5: Finding the most violated y′ 6: Compute pk and qk 7: Y = Y ∪ {y′} 8: Solving Problem (16) over W and Y 9: until ǫ-optimal\nTheorem 1. For any 0 < C, 0 < ǫ ≤ 4R2C and any training example (x1, y1), . . . , (xn, yn), Algorithm 1 converges to the desired precision ǫ after at most,\n⌈ log2 ( ∆\n4R2C\n)⌉ + ⌈ 16R2C\nǫ\n⌉\niterations. R2 = maxdt,y′ ‖ 1n ∑n i=1(yi − y′i)(xi ⊙ √\ndt)‖2, ∆ = maxy′ ∆(y′, y) and ⌈.⌉ is the integer ceiling function.\nProof. We adapt the proof of Theorem 5 in [13], and sketch the necessary changes corresponding to Problem (29). For a given set WT , the dual objective of (8) can be reformulated as\nmax α∈A min d∈WT\nΘd(α) = − 1\n2\n∑\ny′\n∑\ny′′\nαy′αy′′Q d y′,y′′ +\n∑\ny′\nαy′by′ .\nSince there are the T constrained quadratic problems, we consider each d ∈ WT at one time as maxα∈A Θd(α), where Qd is positive semi-definite, and derivative ∂Θd(α) = b− Qdα. The Lemma 2 in [13] states that a line search starting at α along an ascent direction η with maximum step-size C > 0 improves the objective by at least max0≤β≤C { Θd(α+ βη) − Θd(α) } ≥ 12 min { C, ∂Θd(α) T η ηT Qdη } ∂Θd(α) T η. If we consider subgradient descent method, the line search along the subgradient of objective is ∂Θd∗(α) where d ∗ = mind∈WT Θd(α). Therefore, the maximum improvement is\nmax 0≤β≤C\n{Θd∗(α+ βη) −Θd∗(α)}\n≥ 1 2 min\n{ C, ∂Θd∗(α) T η\nηTQd ∗ η\n} ∂Θd∗(α) T η\n≥ 1 2 min d∈WT\n{ C, ∂Θd(α) T η\nηTQdη\n} ∂Θd(α) T η.(17)\nWe can see that it is a special case of [13] if T = 1. According to Theorem 5 in [13], for a newly added constraint ŷ and some γd > 0, we can obtain ∂Θd(α)T η = γd by setting the ascent direction ηŷ = 1 for the newly added ŷ and ηy = − 1Cαy for the others. Here, we set γ = mind∈WT γd so as to be the lower bound of ∂Θd(α)\nT η, ∀d ∈ WT . In addition, the upper bound for ηTQdη ≤ 4R2, ∀d ∈ WT can also be obtained by the fact that ηTQdη = Qdŷ,ŷ − 2C ∑ y′ αy′Q d y′,ŷ + 1 C2 ∑ y′ ∑ y′′ αy′αy′′Q d y′,y′′ ≤ R2 + 2CCR2 +\n1 C2 C2R2 = 4R2, ∀d ∈ WT . By substituting them back to (17), the similar result shows\nthe increase of the objective is at least\nmin\n{ Cγ\n2 , γ2 8R2\n} .\nMoreover, the initial optimality gap is at most C∆. Following the remaining derivation in [13], the overall bound results are obtained.\nRemark 1: Problem (15) is similar to Support Kernel Machine (SKM) [3] in which the multiple Gaussian kernels are built on random subsets of features, with varying widths. However, our method can automatically choose the most violated subset of features as a group instead of a subset of random features. Such random features lead to a local optimum; while our method could guarantee the ǫ-optimality stated in Theorem 1. However, due to the extra cost of computing nonlinear kernel, the current model are only implemented for linear kernel with learned subsets of features.\nRemark 2: The original Problem (30) could be easily formulated as a QCQP problem with exponential size of variables α needed to be optimized and huge number of base kernels in the quadratic term. Unfortunately, the standard MKL methods cannot handle Problem (30) even for a small dataset, let alone the standard QCQP solver. However, Corollary 1 makes it practical to solve a sequence of small QCQP problems directly using standard off-line QCQP solvers, such as Mosek. Note that state-of-the-art MKL solvers can also be used to solve the small QCQP problems, but they are not preferred because their solutions are less accurate than that of standard QCQP solvers, which can solve Problem (16) more accurately in this case.\n5.3. The Proposed Algorithm. Algorithm 1 can obtain the ǫ-optimal solution for the original dual problem (8). By denoting Gd(α) = 12 || ∑K k=1 αkp k||22 − ∑K k=1 αkq k, the group feature generation layer can directly use the ǫ-optimal solution of the objective Gd(α) to approximate the original objective Fd(α). The two-layer cutting plane algorithm is presented in Algorithm 2. From the description of Algorithm 2, it is clear to see that\nAlgorithm 2 The Two-Layer Cutting Plane Algorithm\n1: Input: x = (x1, . . . , xn), y = (y1, . . . , yn), ǫ, C 2: W = ∅, t = 0 3: repeat 4: t = t+ 1 5: Finding the most violated dt 6: W = W ∪ {dt} 7: Call group feature selection(x, y, W , ǫ, C) 8: until ǫ-optimal\ngroups are dynamically generated and augmented into active set W for group selection. In terms of the convergence proof of FGM in [26] and Theorem 1, we can obtain the following theorem to illustrate the approximation with an ǫ-optimal solution to the original problem.\nTheorem 2. After Algorithm 2 stops in a finite number of steps, the difference between optimal solution (d∗, α∗) of Problem (29) and the solution (d, α) of Algorithm 2 is Fd(α)− Fd∗(α∗) ≤ ǫ.\nThe detailed proof of Theorem 2 is given in the supplementary material.\n5.4. Finding the Most Violated y′ and d. Algorithm 1 and Algorithm2 need to find the most violated y′ and d, respectively. In this subsection, we discuss how to obtain these quantities efficiently. Algorithm 1 needs to calculate the subgradient of the empirical risk function RKemp(w). Since R K emp(w) is a pointwise supremum function, the subgradient should be in the convex hull of the gradient of the decomposed functions with the largest objective. Here, we just take one of these subgradients by solving\nyk = arg max y′∈Y\\y\n∆(y′, y)− n∑\ni=1\n(yi − y′i)vi,(18)\nwhere vi = ∑T t=1 w T t (xi ⊙ √ dt). After obtaining yk, it is easy to compute pkt = − 1 n ∑n i=1(yi − yki )(xi ⊙ √ dt) and qk = 1 n ∑n i=1 ∆(y\nk, y). For finding the most violated y′, it depends on how to define the loss ∆(y, y′) in Problem (18). One of the instances is the Hamming loss which can be decomposed and computed independently, i.e., ∆(y, y′) = ∑n i=1 δ(yi, y ′ i), where δ is an indicator function with δ(yi, y ′ i) = 0 if yi = y ′ i, otherwise 1. However, there are some multivariate performance measures which could not be solved independently. Fortunately, there are a series of structured loss functions, such as Area Under ROC (AUC), Average Precision (AP), ranking and contingency table scores and other measures listed in [11, 34, 27], which can be implemented efficiently in our algorithms. In this paper, we only use several multivariate performance measures based on contingency table as the showcases and their finding yk could be solved in time complexity O(n2) [11]. Given the true labels y and predicted labels y′, the contingency tables is defined as follows\ny=1 y=-1 y’=1 a b y’=-1 c d\nF1-score: The Fβ-score is a weighted harmonic average of Precision and Recall. Ac-\ncording to the contingency table, we can obtain Fβ = (1+β2)a\n(1+β2)a+b+β2c . The most common choice is β = 1. The corresponding balanced F1 measure loss can be written as ∆F1(a, b, c, d) = 100(1− F1). Then, Algorithm 2 in [11] can be directly applied.\nPrecision/Recall@k: In search engine systems, most users scan only the first few links that are presented. In this situation, Prec@k and Rec@k measure the precision and recall of a classifier that predicts exactly k documents, i.e., Prec@k = a\na+b and Rec@k = a a+c ,\nsubject to a + b = k. The corresponding loss could be defined as ∆Prec@k = 100(1 − Prec@k) and ∆Rec@k = 100(1 − Rec@k). And the procedure of finding most violated y is similar to F-score, while the only difference is keeping constraint a + b = k and removing a+ b 6= k.\nPrecision/Recall Break-Even Point (PRBEP): The Precision/Recall Break-Even Point requires that the precision and its recall are equal. According to above definition, we can see PRBEP only adds a constraint a + b = a + c, or b = c. The corresponding loss is defined as ∆PRBEP = 100(1 − PRBEP ). Finding the most violated y should enforce the constraint b = c.\nAfter t iterations in Algorithm 2, we transform α in Problem (9) from the exponential size to a small size αt. Now, finding the most violated d becomes\ndt =argmax d∈D Gd(αt)(19)\n=argmax d∈D\n1\n2\n∥∥∥∥ K∑\nk=1\nαtkp k ∥∥∥∥ 2\n2\n− K∑\nk=1\nαtkq k\n=argmax d∈D\n1\n2\n∥∥∥∥ 1\nn\nK∑\nk=1\nαtk\nn∑\ni=1\n(yi − yki )(xi ⊙ √ d) ∥∥∥∥ 2\n=argmax d∈D\n1\n2n2\nm∑\nj=1\nc2jdj\nwhere cj = ∑K k=1 α t k ∑n i=1(yi − yki )xi,j . With the budget constraint ∑m i=1 di = B in D, (19) can be solved by first sorting c2j ’s in the descent order and then setting the first B numbers corresponding to dtj to 1 and the rest to 0. This takes only O(m logm) operations."
    }, {
      "heading" : "6. RELATIONS TO EXISTING METHODS",
      "text" : "In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26]. It can be easily adapted to the general framework (4).\n6.1. Connections to SKM and l1 SVM. Let D1 = {d| ∑m\nj=1 dj = 1, dj ≥ 0, ∀j = 1, . . . ,m} be in the real domain. We observe that D = D1 when B = 1. According to [24], we transform Problem (5) in the special case of B = 1 to the following equivalent optimization problem,\nmin w,ξ≥0\n1\n2\n( m∑\nj=1\n|wj | )2 + Cξ(20)\ns.t. ∀y′ ∈ Y\\y : wT 1 n\nn∑\ni=1\n(yi − y′i)xi ≥ ∆(y, y′)− ξ.\nSKM [3] attempts to obtain the sparsity of w by penalizing the square of a weighted block l1-norm ( ∑k j=1 γj ||wj ||2)2 where k is the number of groups and wj is the weight vector\nfor the features in the jth group. The regularizer ( ∑m\nj=1 |wj |)2 used in (20) is the square of the l1 norm (||w||1)2, which is a special case of SKM when k = m and γj = 1, i.e., each group contains only one feature. Minimizing the square of the l1-norm is very similar to l1-norm SVM [33] by setting Ω(w) = ||w||1 with the non-negative (convex) loss function.\nRegardless of l1-norm or the square of l1-norm, the parameter C is too sensitive to be tuned properly since it controls both margin loss and the sparsity of w. However, our method alleviates this problem by two parameters C and B which control margin loss and sparsity of w, respectively. This separation makes parameter tuning of our method easier than those of SKM and l1 SVM.\n6.2. Connection to NMMKL. Instead of directly solving Problem (20), we formulate a more general problem (5) by introducing an additional budget parameter B, which directly controls the sparsity of w. The advantage is to make parameter tuning easily done since C is not sensitive to the sparsity of w. This strategy is also used in NMMKL [32], but one feature corresponding to one base kernel makes NMMKL intractable for highdimensional problems. The multivariate loss is even hard to be optimized by NMMKL since there are exponential dual variables in the dual form of NMMKL from the exponential number of constraints. However, our method can readily optimize multivariate loss on high-dimensional data.\n6.3. Connection to FGM. According to the work [40], we can reformulate Problem (20) as an equivalent optimization problem\nmin d∈D1 min w,ξ≥0\n1\n2\nm∑\nj=1\ndj |wj |2 + Cξ(21)\ns.t.∀y′ ∈ Y\\y : 1 n\nm∑\nj=1\ndjwj\nn∑\ni=1\n(yi − y′i)xj,i ≥ ∆(y, y′)− ξ.\nAfter the substitutions of vj = √ djwj , ∀j = 1, . . . ,m and the general case of D, we can obtain the following problem\nmin d∈D min v,ξ≥0\n1\n2 ‖v‖22 + Cξ(22)\ns.t.∀y′ ∈ Y\\y : vT 1 n\nn∑\ni=1\n(yi − y′i)(xi ⊙ √ d) ≥ ∆̃(y, y′)− ξ,\nwhere v = [v1, . . . , vm]T . After deriving Lagrangian dual problem of (22), we observe that it is same as Problem (6). Problem (19) always finds the most violated d in the integer domain {0, 1}m, so the solutions of the following problem solved by the proposed twolayer cutting plane algorithm is the same as the solutions of Problem (6)\nmin d∈D2 min v,ξ≥0\n1\n2 ‖v‖22 + Cξ(23)\ns.t.∀y′ ∈ Y\\y : vT 1 n\nn∑\ni=1\n(yi − y′i)(xi ⊙ d) ≥ ∆̃(y, y′)− ξ,\nwhere the integer domain D2 = {d| ∑m\nj=1 dj ≤ B,d ∈ {0, 1}m}. This formula can be equally derived as the extension of FGM for multivariate performance measures by defining the new hypotheses\n(24) h̃v(x) = argmax y′∈Y\nn∑\ni=1\ny′i(v ⊙ d)T xi,\nwhere h̃v : X → Y and d ∈ D2. It is not trivial to perform the extension of FGM to optimize multivariate loss because original FGM method [26] cannot directly apply to solve the exponential number of constraints. And our domain of d is in real domain D which is more general than the integer domainD2 used in FGM and the proposed extension (23), even though the final solutions of (5) and (23) are the same.\n6.4. Connection to l0 SVM. The following Lemma indicates that the proposed formula can be interpreted by l0-norm constraint.\nLemma 2. (23) is equivalent to the following problem\nminw̃,ξ≥0 1\n2 ‖w̃‖22 + Cξ(25)\ns.t. ∀y′ ∈ Y\\y : w̃T 1 n\nn∑\ni=1\n(yi − y′i)xi ≥ ∆̃(y, y′)− ξ,\n‖w̃‖0 ≤ B.\nProof. Note, at the optimality of (22), WLOG, suppose dj = 0, the corresponding vj must be 0. Thus, ‖v‖0 ≤ ‖d‖0. Let w̃ = v⊙d, we have ‖w̃‖0 = ‖v⊙d‖0 ≤ min{‖v‖0, ‖d‖0} ≤ ‖d‖0 = ∑m j=1 dj ≤ B. Moreover, ‖w̃‖22 = ‖v ⊙ d‖22 = ‖v‖22 at the optimality. Therefore, the optimal solution of (22) is a feasible solution of (25). On the other hand, for the optimal w̃ in (25), let v = w̃ and di = δ(w̃i) where δ(t) = 1 if t 6= 0; otherwise, 0. So, the optimal solution of (25) is a feasible solution of (22).\nThis gives another interpretation of parameter B from the perspective of l0-norm. Since l0-norm ||w̃||0 represents the number of non-zero entries of w̃, so B in our method can be considered as the parameter which directly controls the sparsity of w."
    }, {
      "heading" : "7. MULTIPLE INSTANCE LEARNING FOR MULTIVARIATE PERFORMANCE MEASURES",
      "text" : "We have already illustrated the proposed framework by optimizing multivariate performance measures for feature selection in Section 4. In this section, we extend this approach to solve multiple instance learning problems which have been employed to solve a variety of learning problems, e.g., drug activity prediction [7], image retrieval [35], natural scene classification [20] and text categorization [1], but it is seldom optimized for multivariate performance measures in the literature. However, it is crucial to optimize the task specific performance measures, e.g., F score is widely considered as the most important evaluation criterion for a learning method in image retrieval.\nMulti-instance learning was formally introduced in the context of drug activity prediction [7]. In this learning scenario, a bag is represented by a set of instances where each instance is represented by a feature vector. The classification label is only assigned to each bag instead of the instances in this bag. We name a bag as a positive bag if there is at least one positive instance in this bag, otherwise it is called negative bag. The learning problem is to decide whether the given unlabeled bag is positive or not. By defining a similarity measure between a bag and an instance, Multiple-Instance Learning via Embedded instance Selection (MILES) [6] successfully transforms multiple instance learning into a feature selection problem by embedding bags into an instance-based feature space and selecting the most important features.\nBefore discussing the transformation in MILES, we first give the notations of multiple instance learning problem. Following the notations in [6], we denote ith positive bags as B+i = {x+i,j} n + i j=1 which consists of n + i instances x + i,j , j = 1, . . . , n + i . Similarly, the ith negative bags is denoted as B−i = {x−i,j} n− i\nj=1. All instances belongs to the same feature space X . The number of positive bags and negative bags are ℓ+ and ℓ−, respectively. The instances in all bags are rearranged as {x1, . . . ,xn} where n = ∑ℓ +\ni=1 n + i + ∑ℓ− i=1 n − i .\nBy considering each instance in the training bags as a candidate for target concepts, the embedded feature space is represented as\n(26) x̂i = [s(x1,Bi), . . . , s(xn,Bi)]T ∈ Rn, where the similarity measure between the bag Bi and the instance xk is defined as the most-likely-cause estimator\n(27) s(xk,Bi) = max j exp\n( −||xi,j − x\nk||2 2σ2\n) .\nIt follows the intuition that the similarity between a concept and a bag is determined by the concept and the closest instance in this bag. The corresponding labels are constructed as follows: ŷi = 1 if Bi is a positive bag, otherwise ŷi = −1. For a given ℓ+ positive bags and ℓ− negative bags, we form a new classification representation of the multiple instance learning problem as {x̂i, ŷi}ℓ ++ℓ− i=1 . For each instance x k, the new feature representation corresponds to the values of the kth feature variable s(xk, ·) is [s(xk,B+1 ), . . . , s(x k,B+ ℓ+ ), s(xk,B−1 ), . . . , s(x k,B− ℓ− )]\nwhere the feature induced by xk provides the useful information for separating the positive and negative bags. The linear discriminant function\n(28) ŷ = sign(〈w, x̂〉+ b) wherew and b are the model parameters. The embedding induces a possible high-dimensional space when the number of instances in the training set is large. Since some instances may not be responsible for the label of the bags or might be similar to each other, many features are redundant or irrelevant, so MILES employs L1-SVM to select a subset of mapped features that is most relevant to the classification problem. However, L1-SVM cannot fulfill to obtain a high performance over the task-specific measures because it only focuses on optimizing zero-one loss function. Our proposed Algorithm 2 is a natural alternative feature selection method for multi-variate performance measures. The proposed algorithm for multiple instance learning to optimize multivariate measures is shown in Algorithm 3.\nAlgorithm 3 Learning a bag classifier\n1: Input: positive bags {B+i }ℓ + i=1, negative bags {B−i }ℓ −\ni=1, C, and ǫ 2: Construct the embedding representation of training data\n{(x̂i, ŷi)}, ∀i = 1, . . . , ℓ+ + ℓ−\n3: x = [x̂1, . . . , x̂ℓ++ℓ− ] and y = [ŷ1, . . . , ŷℓ++ℓ− ] 4: call Algorithm 2 with arguments (x,y,C,ǫ) 5: Output: parameters w\nAccording to Algorithm 3, we do not need the model parameter b since the structural SVM is irrelevant to the relative offset b, i.e., ŷ = argmaxy∈{−1,+1} y〈ŵ, x̂〉."
    }, {
      "heading" : "8. EXPERIMENTS",
      "text" : "In this Section, we conduct extensive experiments to evaluate the performance of our proposed method and state-of-the-art feature selection methods: 1) SVM-RFE [9]; 2) l1SVM; 3) FGM [26]; 4) l1-bmrm-F11, which is l1 regularized SVM for optimizing F1 score\n1http://users.cecs.anu.edu.au/˜chteo/BMRM.html\nby bundle method [27]. SVM-RFE and FGM use Liblinear software 2 as the QP solver for their SVM subproblems. For l1-SVM, we also use Liblinear software, which implements the state-of-the-art l1-SVM algorithm [33]. In addition to the comparison for 0-1 loss, we also perform experiments on image data for F1 measure. Furthermore, several specific measures on the contingency table are investigated on Text datasets by comparing with SVMperf [11]. All the datasets shown in Table 1 are of high dimensions.\nFor convenience, we name our proposed two-layer cutting plane algorithm FS∆multi, where ∆ represents different type of multivariate performance measures. We implemented Algorithm 2 in MATLAB for all the multivariate performance measures listed above, using Mosek as the QCQP solver for Problem (16) which yields a worse-case complexity of O(KT 2). Removing inactive constraints from the working set [13] in the inner layer is employed for speedup the QCQP problem. Since the values of both K and T are much smaller than the number of examples n and its dimensionality m, the QCQP is very efficient as well as more accurate for large-scale and high-dimensional datasets. Furthermore, the codes simultaneously solve the primal and its dual form. So the optimal µ and α can be obtained after solving Problem (16).\nFor a test pattern x, the discriminant function can be obtained by f(x) = 〈w ⊙ d̃, x〉 where w = ∑n i=1 βixi, βi = 1 n ∑K k=1 αk(yi − yki ), and d̃ = ∑T t=1 µt √ dt. This leads to the faster prediction since only a few of the selected features are involved. After computing pk, the matrices of Problem (16) can be incrementally updated, so it can be done totally in O(TK2).\n8.1. Parameter Sensitivity Analysis. Before comparing FS∆multi with other methods, we first conduct empirical studies for the parameter sensitivity analysis on News20.binary. The goal is to examine the relationships among parameters C and B, performance measures and the number of selected features with the range of C in [0.1, 1, 10, 100]× n and B in [2, 5, 10, 50, 100, 150, 200, 250].\nFigure 1(a-b) show the testing accuracy and F1 scores as well as the number of selected features by varying C and B. We observe that the results are very sensitive to C when B is very small. This indicates that the l1 model, which is equivalent to the proposed method in the case of B = 1, is vulnerable to the choice of C. On the other hand, the results are rather insensitive to C when B is large. Hence, the proposed method is less sensitive to C than l1 model. We also observe that the proposed method prefers a large C value for better performances. Figure 1(c-d) demonstrate the corresponding relationships among parameters B, C and the number of selected features of Figure 1(a-b). We observe that B and the number of selected features always exhibits a linear trend with a constant slope. Moreover, the slope remains the same when C ≥ 10, but a small C will increase the slope. This means that, compared with B, parameter C has less influence on the sparsity of w,\n2http://www.csie.ntu.edu.tw/˜cjlin/liblinear/\nand the learned feature selection model becomes stabilized when C ≥ 10. These empirical results are consistent to the discussions of parameter B in Section 3.\nSince large C needs more iterations to converge according to Theorem 1, the compromise is to set C not too large and let B dominate the selection of features. According to these observations, we can safely fix C and study the results by varying B to compare with other methods in the following experiments.\n8.2. Time Complexity Analysis. We empirically study the time complexity of FSF1multi by comparing with other methods. Two datasets News20.binary and Image (Desert) are used for illustration. The detailed setting are shown in Section 8.3 and Section 8.4, respectively. Figure 2 gives the training time over five different methods. On News20.binary dataset, we cannot report the training time for l1-bmrm-F1 since l1-bmrm-F1 cannot terminate after more than two days with the maximum iteration 1000 and parameter λ ∈ [10−7, 102] due to the extremely high dimensionality. We observe that the proposed methods are slower than l1-SVM, but much faster than SVM-RFE and l1-bmrm-F1. In addition, on Image dataset, when the termination condition with the relative difference between the objective and its convex linear lower bound lower than 0.1 is set, l1-bmrm-F1 also cannot converge\nafter the maximum iteration, which is consistent with the discussion in Appendix C of [27] that bundle method with l1 regularizer cannot guarantee the convergence. This leads to the similar number of selected features (e.g., 98 in Figure 2(b)) even though λ is decreasing gradually.\nThese observations implies that our proposed two-layer cutting plane method needs less time for training with guaranteed convergence than bundle method. Moreover, our method can work on large scale and high dimensional data for optimizing user-specified measure, but bundle method cannot. As aforementioned, l1-bmrm-F1 is much slower on the high dimensional datasets in our experiments, so we can only report its results in Section 8.4.\n8.3. Feature Selection for Accuracy. Since [11] has proven that SVM∆multi with Hamming loss, namely∆Err(y, y′) = 2(b+c), is the same as SVM. In this subsection, we evaluate the accuracy performances of FS∆multi for Hamming loss function, namely FS hamming multi as well as other state-of-the-art feature selection methods. We compare these methods on two binary datasets, News20.binary 3 and URL1 in Table 1. Both datasets are used in [26], and they are already split into training and testing sets.\nWe test FGM and SVM-RFE in the grid CFGM = [0.001, 0.01, 0.1, 1, 5, 10]and choose CFGM = 5 which gives good performance for both FGM and SVM-RFE. This is the same as [26]. For FShammingmulti , we do the experiments by fixing CFGMmulti as 0.1 × n for URL1 and 1.0 × n for New20.binary. The setting for budget parameter B = [2, 5, 10, 50, 100, 150, 200, 250] for News20.binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1. The elimination scheme of features for SVM-RFE method can be referred to [26]. For l1-SVM, we report the results of different C values so as to obtain different number of selected features.\nFigure 3 reports testing accuracy on different datasets. The testing accuracy is comparable among different methods, but both FShammingmulti and FGM can obtain better prediction performances than SVM-RFE in a small number (less than 20) of selected features on both News20.binary and URL1. These results show that the proposed method with Hamming loss can work well on feature selection tasks especially when choosing only a few features. FShammingmulti also performs better than l1-SVM on News20.binary in most range of selected features. This is possibly because l1 models are more sensitive to noisy or redundant features on News20.binary dataset.\n3http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets\nFigure 4 shows that our method with the small B will select smaller number of features than the large B. We also observed that most of features selected by the small B also appeared in the subset of features using the large B. This phenomenon can be obviously observed on News20.binary. This leads to the conclusion that FShammingmulti can select the important features in the given datasets due to the insensitivity of parameter B. However, we notice that not all the features in the selected subset of features with smaller B fall into that of subset of features with the large B, so our method is non-monotonic feature selection. This argument is consistent with the test accuracy in Figure 3. News20.binary seems to be monotonic datasets from Figure 4, since FShammingmulti , FGM and SVM-RFE demonstrate similar performance. However, URL1 is more likely to be non-monotonic, as our method and FGM can do better than SVM-RFE. All the facts imply that the proposed method is comparable with FGM and SVM-RFE. And it also demonstrates the nonmonotonic property for feature selection.\n8.4. Feature Selection for Image Retrieval. In this subsection, we demonstrate the specific multivariate performance measures are important to select features for real applications. In particular, we evaluate F1 measure (commonly used performance measure) for\nthe task of image retrieval. Due to the success of transforming multiple instance learning into a feature selection problem by embedded instance selection, we use the same strategy in Algorithm 4.1 of [6] to construct a dense and high-dimensional dataset on a preprocessed image data 4. This dataset is used in [38] for multi-instance learning. It contains five categories and 2, 000 images. Each image is represented as a bag of nine instances generated by the SBN method [20]. Each image bag is represented by a collection of nine\n4http://lamda.nju.edu.cn/data MIMLimage.ashx\n15-dimensional feature vectors. After that, following [6], the natural scene image retrieval problem turns out to be a feature selection task to select relevant embedded instances for prediction. The Image dataset are split randomly with the proportion of 60% for training and 40% for testing (Table 1). Since F1-score is used for performance metric, we perform FS∆multi for F1-score, namely FS F1 multi as well as other state-of-the-art feature selection methods. As mentioned above, FGM and FShammingmulti have similar performances, we will not report the results of FGM here. FShammingmulti and FS ∆ multi use the fixed C = 10 × n. For other methods, we use the previous settings. The testing F1 values of all methods on each category are reported in Figure 5.\nFrom Figure 5, we observe that FSF1multi and FS hamming multi achieve significantly improved performance over l1-SVM in term of F1-score especially when choosing less than 100 features. Moreover, SVM-RFE also outperforms l1-SVM on three categories out of five. This verifies that ℓ1 penalty does not perform as well as ℓ0 methods like FS F1 multi and FS hamming multi on dense and high-dimensional datasets. It is possibly because ℓ1-norm penalty is very sensitive to dense and noisy features. We also observe that FSF1multi performs better than FShammingmulti and SVM-RFE on four over five categories. l1-bmrm-F1 performs competitively but it is unstable and time-consuming as shown in Section 8.2. All these facts imply that directly optimizing F1 measure is useful to boost F1 performance measure, and our proposed FSF1multi is efficient and effective.\n8.5. Multivariate Performance Measures for Document Retrieval. In this subsection, we focus on feature selection for different multivariate performance measures on imbalanced text data shown in Table 1. For multiclass classification problems, one vs. rest strategy is used. The comparing model is SVMperf 5. Following [11], we use the same notation SVM∆multi for different multivariate performance measures. The command used for training SVMperf can work for different measures by -l option 6. In our experiments, we search the Cperf in the same range [2−6, . . . , 26] as in [11]. We choose the one which\n5www.cs.cornell.edu/People/tj/svm light/svm perf.html 6 svm perf learn -c Cperf -w 3 –b 0 train file train model\ndemonstrates the best performance of SVM∆multi to each multivariate performance measure for comparison. FS∆multi and FS hamming multi fix CFGMmulti = 0.1 × n for News20 except 5.0×n for Sector. For Rec@k, we use k as twice the number of positive examples, namely Rec@2p. The evaluation for this measure uses the same strategy to label twice the number of positive examples as positive in the test datasets, and then calculate Rec@2p.\nTable 2 shows the macro-average of the performance over all classes in a collection in which both FS∆multi and FS hamming multi at B = 250 are listed. The improvement of FS ∆ multi over FShammingmulti and SVM ∆ multi with respect to different B values are reported in Figure 6. From Table 2, FS∆multi is consistently better than FS hamming multi on all multivariate performance measures and two multiclass datasets. Similar results can be obtained comparing with SVM∆multi, while the only exception is the measure Rec@2p on News20 where SVM∆multi is a little better than FS ∆ multi. The largest gains are observed for F1 score on all two text classification tasks. This implies that a small number of features selected by FS∆multi is enough to obtain comparable or even better performances for different measures than SVM∆multi using all features.\nFrom Figure 6, FS∆multi consistently performs better than FS hamming multi for all of the multivariate performance measures from the figures in the left-hand side. Moreover, the figures in the right-hand side show that the small number of features are good for F1 measures,\nbut poor for other measures. As the number of features increases, Rec@2p and PRBEP can approach to the results of SVM∆multi and all curves become flat. The performance of PRBEP and Rec@2p is relatively stable when sufficient features are selected, but our method can choose very few features for fast prediction. For F1 measure, our method is consistently better than SVM∆multi, and the results show significant improvement over all range of B. This improvement may be due to the reduction of noisy or non-informative features. Furthermore, FS∆multi can achieve better performance measures than FS hamming multi .\nWe also compared different feature selection algorithms such as SVM-RFE and l1-SVM on Sector and News20 in the same setting as the previous sections. The results in terms of F1 measure are reported in Figure 7. We clearly observe that FS∆multi outperforms l1-SVM on both datasets, and comparable or even better than SVM-RFE. For a small number of features, FS∆multi can still demonstrate very good F1 measure."
    }, {
      "heading" : "9. CONCLUSION",
      "text" : "In this paper, we propose a generalized sparse regularizer for feature selection, and the unified feature selection framework for general loss functions. We particularly study in details for multivariate losses. To solve the resultant optimization problem, a two-layer cutting plane algorithm was proposed. The convergence property of the proposed algorithm is studied. Moreover, connections to a variety of state-of-the-art feature selection methods are discussed in details. A variety of analyses by comparing with the various feature selection methods show that the proposed method is superior to others. Experimental results show that the proposed method is comparable with FGM and SVM-RFE and better than l1 models on feature selection task, and outperforms SVM for multivariate performance measures on full set of features."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "This work was supported by Singapore A*star under Grant SERC 112 280 4005\nAppendices"
    }, {
      "heading" : "A. PROOF OF PROPOSITION 1",
      "text" : "Since the loss term ∆(y′, y′) = 0 for all y′ ∈ Y , we can equivalently transform Problem\nmin w1,...,wT ,ξ≥0\n1\n2\n( T∑\nt=1\n‖wt‖2 )2 + Cξ\ns.t. ξ ≥ by′ − T∑\nt=1\n〈wt, aty′〉, ∀y′ ∈ Y\\y,\ninto the following optimization problem\nmin w1,...,wT ,ξ≥0\n1\n2\n( T∑\nt=1\n‖wt‖2 )2 + Cξ\ns.t. ξ ≥ by′ − T∑\nt=1\n〈wt, aty′〉, ∀y′ ∈ Y.\nBy introducing a new variable u ∈ R and moving out summation operator from objective to be a constraint, we can obtain the equivalent optimization problem as\nmin w,ξ≥0\n1 2 u2 + Cξ\ns.t. ξ ≥ by′ − T∑\nt=1\n〈wt, aty′〉, ∀y′ ∈ Y\nT∑\nt=1\n‖wt‖ ≤ u.\nWe can further simplify above problem by introducing another variables ρ ∈ Rm such that ‖wt‖ ≤ ρt, ∀t = 1, . . . , T , to be\nmin w,u,ρ,ξ≥0\n1 2 u2 + Cξ\ns.t. ξ ≥ by′ − T∑\nt=1\n〈wt, aty′〉, ∀y′ ∈ Y\nT∑\nt=1\nρt ≤ u\n||wt|| ≤ ρt, ∀t = 1, . . . , T.\nWe know that for each t, ‖wt‖ ≤ ρt is a second-order cone constraint. Following the recipe of [5], the self-dual cone ‖vt‖2 ≤ ηt, ∀t = 1, . . . , T can be introduced to form the\nLagrangian function as follows\nL(w, ξ, u, ρ;α, τ, γ, v, η)\n= 1\n2 u2 + Cξ −\n∑\ny′\nαy′ ( ξ − by′ + T∑\nt=1\n〈wt, aty′〉 ) − τξ\n+γ\n( T∑\nt=1\nρt − u ) − T∑\nt=1\n(〈vt,wt〉+ ηtρt),\nwith dual variables αt ∈ R+, τ ∈ R+, γ ∈ R+. The derivatives of the Lagrangian with respect to the primal variables have to vanish which leads to the following KKT conditions:\nvt = − ∑\ny′\nαy′a t y′ , ∀t = 1, . . . , T\nC − ∑\ny′\nαy′ − τ = 0\nu = γ\nγ = ηt, ∀t = 1, . . . , T\nBy substituting all the primal variables with dual variables by above KKT conditions, we can obtain the following dual problem,\nmax α,γ\n−1 2 γ2 +\n∑\ny′\nαy′by′\ns.t. ∥∥∥ ∑\ny′\nαy′a t y′ ∥∥∥ ≤ γ, ∀t = 1, . . . , T\n∑\ny′\nαy′ ≤ C, αy′ ≥ 0, ∀y′ ∈ Y\nBy setting θ = 12γ 2 and A = {∑y′ αy′ ≤ C,αy′ ≥ 0, ∀y′ ∈ Y}, we can reformulate above problem as\nmax θ,α∈A\n−θ + ∑\ny′\nαy′by′\ns.t. 1\n2 αTQtα ≤ θ, ∀t = 1, . . . , T\nwhere Qty′,y′′ = 〈aty′ , aty′′〉. According to the property of self-dual cone [3], we can obtain the primal solution from its dual as wt = −µtvt = µt ∑ y′ αy′a t y′ where µj is the dual\nvariable of the jth quadratic constraint such that ∑m\nj=1 µj = 1, µj ∈ R+, ∀j = 1, . . . ,m. By constructing Lagrangian with dual variables µ with respect to θ, we can recover Problem\n(29) max α∈A min µ∈MT −1 2\n∑\ny′\n∑\ny′′\nαy′αy′′\n( T∑\nt=1\nµtQ dt y′,y′′\n) + ∑\ny′\nαy′by′ ,\nwhere MT = { ∑T t=1 µt = 1, µt ≥ 0, ∀t = 1, . . . , T }. This completes the proof."
    }, {
      "heading" : "B. PROOF OF THEOREM 2",
      "text" : "Given the Problem\nmin α∈A max d∈D Fd(α) or min α∈A,γ γ : γ ≥ Fd(α), ∀d ∈ D,(30)\nwe have the equivalent optimization problem as\nmax α∈A,γ\n−γ\ns.t. γ ≥ Fd(α), ∀d ∈ D.\nThe outer layer of Algorithm 2 can generate a sequence of configurations ofd as {d1, . . . ,dk} after k iterations. In the kth iteration, the most violated constraint dk+1 is found in terms of αk, so thatFdk+1(αk) = maxd∈D Fd(α) according to Problem dt = argmaxd∈D Fd(αt). Hence, we can construct two sequences {γ\nk } and {γk} such that\nγ k = max 1≤t≤k\nFdt(αt)(31)\nγk = min 1≤t≤k Fdt+1(αt) = min 1≤t≤k max d∈D Fd(αt)(32)\nSuppose that we can solve minα∈A max1≤t≤k Fdt(α) exactly. Due to the equivalence to Problem (29), it means that we can obtain the exact solution of the problem (29). Based on this assumption, equation (31) can be further reformed as\nγ k = max 1≤t≤k Fdt(αt) = min α∈A max 1≤t≤k Fdt(αt).(33)\nThis turns out to be the same problem of FGM [26]. For self-completeness, we give the theorem as follows,\nTheorem 3 ([26]). Let (α∗, γ∗) be the globally optimal solution pair of Problem (30), sequences {γ\nk } and {γk} have the following property\n(34) γ k ≤ γk ≤ γk.\nAs k increases, {γ k } is monotonically increasing and {γk} is monotonically decreasing.\nBased on above theorem, global optimal solution can be obtained after a finite number of iterations. However, the assumption of the accurate solution for (29) usually has no formal guarantee. We have already proven in Theorem 1 that the inner problem of Algorithm 2 can reach the desired precision ǫ after a finite number of iterations by Algorithm 1. Therefore, according to Algorithm 2, we can construct the following sequence\nγ′ k = max 1≤t≤k Fdt(αt) ≤ min α∈A max 1≤t≤k Fdt(αt) + ǫ.(35)\nBy combining inequalities (34) and (35), we obtain the following inequalities\n(36) γ′ k − ǫ ≤ γ k ≤ γk ≤ γk.\nAfter a finite number of iterations, the global optimal solution is γ∗ = γ k = γk = γk. Hence, the solution of the Algorithm 2 may be not less than the lower bound γ′ k\nby ǫ. It is complete for Theorem 2."
    } ],
    "references" : [ {
      "title" : "Support vector machines for multiple-instance learning",
      "author" : [ "S. Andrews", "I. Tsochantaridis", "T. Hofmann" ],
      "venue" : "NIPS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Optimization with sparsity-inducing penalties",
      "author" : [ "F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski" ],
      "venue" : "Foundations and Trends in Machine Learning, 4:1–106,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multiple kernel learning, conic duality, and the SMO algorithm",
      "author" : [ "F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan" ],
      "venue" : "ICML,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Convex Analysis and Nonlinear Optimization",
      "author" : [ "J.M. Borwein", "A.S. Lewis" ],
      "venue" : "Springer,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge University Press, Cambridge, UK.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "MILES: Multiple-instance learning via embedded instance selection",
      "author" : [ "Y. Chen", "J. Bi", "J.Z. Wang" ],
      "venue" : "TPAMI, 28:1931–1947,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Solving the multiple instance problem with axisparallel rectangles",
      "author" : [ "T.G. Dietterich", "R.H. Lathrop", "T. Lozano-Perez" ],
      "venue" : "Artificial Intelligence, 89:31–71,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A feature selection newton method for support vector machine classification",
      "author" : [ "G.M. Fung", "O.L. Mangasarian" ],
      "venue" : "Computational Optimization and Applications, 28:185–202,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Gene selection for cancer classification using support vector machines",
      "author" : [ "I. Guyou", "J. Weston", "S. Barnhill", "V. Vapnik" ],
      "venue" : "Machine Learning, 46:389–422,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Convex Analysis and Minimization Algorithms",
      "author" : [ "J.B. Hiriart-Urruty", "C. Lemarechal" ],
      "venue" : "Springer-Verlag,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "A support vector method for multivariate performance measures",
      "author" : [ "T. Joachims" ],
      "venue" : "ICML,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Training linear SVMs in linear time",
      "author" : [ "T. Joachims" ],
      "venue" : "SIGKDD,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Cutting-plane training of structural SVMs",
      "author" : [ "T. Joachims", "T. Finley", "C.J. Yu" ],
      "venue" : "Machine Learning, 77:27– 59,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The cutting plane algorithm for solving convex programs",
      "author" : [ "J.E. Kelley" ],
      "venue" : "Journal of the Society for Industrial and Applied Mathematics, 8(4):703–712,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1960
    }, {
      "title" : "Embedded methods",
      "author" : [ "T.N. Lal", "O. Chapelle", "J. Weston", "A. Elisseeff" ],
      "venue" : "I. Guyon, S. Gunn, M. Nikravesh, and L. A. Zadeh, editors, Feature Extraction: Foundations and Applications, Studies in Fuzziness and Soft Computing, number 207, pages 137–165. Springer,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Direct optimization of ranking measures",
      "author" : [ "Q.V. Le", "A. Smola" ],
      "venue" : "JMLR, 1:1–48,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A risk ratio comparison of l0 and l1 penalized regressions",
      "author" : [ "D. Lin", "D.P. Foster", "L.H. Ungar" ],
      "venue" : "Technical report, University of Pennsylvania,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Sparse logistic regression with lp penalty for biomarker identification",
      "author" : [ "Z. Liu", "F. Jiang", "G. Tian", "S. Wang", "F. Sato", "S.J. Meltzer", "M. Tan" ],
      "venue" : "Statistical Applications in Genetics and Molecular Biology, 6(1),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Optimizing performance measures for feature selection",
      "author" : [ "Q. Mao", "I.W. Tsang" ],
      "venue" : "ICDM,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Multiple-instance learning for natural scene classification",
      "author" : [ "O. Maron", "A.L. Ratan" ],
      "venue" : "ICML,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Optimizing f-measure with support vector machines",
      "author" : [ "D.R. Musicant", "V. Kumar", "A. Ozgur" ],
      "venue" : "Proceedings of the 16th International Florida Artificial Intelligence Research Society Conference,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Cutting-set methods for robust convex optimization with pessimizing oracles",
      "author" : [ "A. Mutapcic", "S. Boyd" ],
      "venue" : "Optimization Methods & Software, 24(3):381406,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Feature selection, l1 vs",
      "author" : [ "A.Y. Ng" ],
      "venue" : "l2 regularization, and rotational invariance. In ICML,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "SimpleMKL",
      "author" : [ "A. Rakotomamonjy", "F.R. Bach", "Y. Grandvalet", "S. Canu" ],
      "venue" : "JMLR, 3:1439–1461,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Large scale multiple kernel learning",
      "author" : [ "S. Sonnenburg", "G. Rätsch", "C. Schäfer", "B. Scholköpf" ],
      "venue" : "JMLR, 7,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning sparse SVM for feature selection on very high dimensional datasets",
      "author" : [ "M. Tan", "L. Wang", "I.W. Tsang" ],
      "venue" : "ICML,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Bundle methods for regularized risk minimization",
      "author" : [ "C.H. Teo", "S.V.N. Vishwanathan", "A. Smola", "Quoc V. Le" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2010
    }, {
      "title" : "Large margin methods for structured and interdependent output variables",
      "author" : [ "I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altum" ],
      "venue" : "JMLR, 6:1453–1484,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Learning to rank by optimizing ndcg measure",
      "author" : [ "H. Valizadengan", "R. Jin", "R. Zhang", "J. Mao" ],
      "venue" : "NIPS,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Use of the zero-norm with linear models and kernel methods",
      "author" : [ "J. Weston", "A. Elisseeff", "B. Scholköpf" ],
      "venue" : "JMLR, 3:1439–1461,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "An extended level method for efficient multiple kernel learning",
      "author" : [ "Z. Xu", "R. Jin", "I. King", "M.R. Lyu" ],
      "venue" : "NIPS,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Non-monotonic feature selection",
      "author" : [ "Z. Xu", "R. Jin", "J. Ye", "Michael R. Lyu", "I. King" ],
      "venue" : "In ICML,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2009
    }, {
      "title" : "A comparison of optimization methods and software for large-scale l1-regularized linear classification",
      "author" : [ "G.-X. Yuan", "K.-W. Chang", "C.-J. Hsieh", "C.-J. Lin" ],
      "venue" : "JMLR, 11:3183–3234,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A support vector method for optimizing average precision",
      "author" : [ "Y. Yue", "T. Finley", "F. Radlinski", "T. Joachims" ],
      "venue" : "SIGIR,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Content-based image retrieval using multiple-instance learning",
      "author" : [ "Q. Zhang", "S.A. Goldman", "W. Yu", "J. Fritts" ],
      "venue" : "ICML,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Analysis of multi-stage convex relaxation for sparse regularization",
      "author" : [ "T. Zhang" ],
      "venue" : "JMLR, 11:1081–1107, Mar",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Smoothing multivariate performance measures",
      "author" : [ "X. Zhang", "A. Saha", "S.V.N. Vishwanathan" ],
      "venue" : "UAI,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Multi-instance multi-label learning with application to scene classification",
      "author" : [ "Z.-H. Zhou", "M.-L. Zhang" ],
      "venue" : "NIPS,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "1-norm support vector machine",
      "author" : [ "J. Zhu", "S. Rossett", "T. Hastie", "R. Tibshirani" ],
      "venue" : "NIPS,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Multiclass multiple kernel learning",
      "author" : [ "A. Zien", "C.S. Ong" ],
      "venue" : "ICML,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Hence, the error rate is considered as a poor criterion for the problems with highly skewed class distributions [11].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 10,
      "context" : "Instead of optimizing the error rate, Support Vector Machine for multivariate performance measures (SVM ) [11] was proposed to directly optimize the losses based on a variety of multivariate performance measures.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 36,
      "context" : "A smoothing version of SVM [37] was proposed to accelerate the convergence of the optimization problem specially designed",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 26,
      "context" : "Structural SVMs are considered as the general framework for optimizing a variety of loss functions [27, 13, 28].",
      "startOffset" : 99,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "Structural SVMs are considered as the general framework for optimizing a variety of loss functions [27, 13, 28].",
      "startOffset" : 99,
      "endOffset" : 111
    }, {
      "referenceID" : 27,
      "context" : "Structural SVMs are considered as the general framework for optimizing a variety of loss functions [27, 13, 28].",
      "startOffset" : 99,
      "endOffset" : 111
    }, {
      "referenceID" : 20,
      "context" : "Other works optimize specific multivariate performance measures, such as F-score [21], normalize discount cumulative gain (NDCG) [29], ordinal regression [12], ranking loss [16] and so on.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 28,
      "context" : "Other works optimize specific multivariate performance measures, such as F-score [21], normalize discount cumulative gain (NDCG) [29], ordinal regression [12], ranking loss [16] and so on.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "Other works optimize specific multivariate performance measures, such as F-score [21], normalize discount cumulative gain (NDCG) [29], ordinal regression [12], ranking loss [16] and so on.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 15,
      "context" : "Other works optimize specific multivariate performance measures, such as F-score [21], normalize discount cumulative gain (NDCG) [29], ordinal regression [12], ranking loss [16] and so on.",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 14,
      "context" : "To alleviate these issues, one can resort to embedded feature selection methods [15], which can be categorized into the following two major directions.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 38,
      "context" : "SVM, logistic regression) with l1-norm ‖w‖1 [39, 8, 23].",
      "startOffset" : 44,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "SVM, logistic regression) with l1-norm ‖w‖1 [39, 8, 23].",
      "startOffset" : 44,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "SVM, logistic regression) with l1-norm ‖w‖1 [39, 8, 23].",
      "startOffset" : 44,
      "endOffset" : 55
    }, {
      "referenceID" : 32,
      "context" : "A thorough study to compare several recently developed l1regularized algorithms has been conducted in [33].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 29,
      "context" : "To achieve a sparser solution, the Approximation of the zeRO norm Minimization (AROM) was proposed [30] to optimize l0 models.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 17,
      "context" : "However, the recent results [18] and theoretical studies [17, 36] have showed that lp models (where p < 1) even with a local optimal solution can achieve better prediction performance than convex l1 models, which are asymptotically biased [18].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 16,
      "context" : "However, the recent results [18] and theoretical studies [17, 36] have showed that lp models (where p < 1) even with a local optimal solution can achieve better prediction performance than convex l1 models, which are asymptotically biased [18].",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 35,
      "context" : "However, the recent results [18] and theoretical studies [17, 36] have showed that lp models (where p < 1) even with a local optimal solution can achieve better prediction performance than convex l1 models, which are asymptotically biased [18].",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 17,
      "context" : "However, the recent results [18] and theoretical studies [17, 36] have showed that lp models (where p < 1) even with a local optimal solution can achieve better prediction performance than convex l1 models, which are asymptotically biased [18].",
      "startOffset" : 239,
      "endOffset" : 243
    }, {
      "referenceID" : 8,
      "context" : "Another way is to sort the weights of a SVM classifier and remove the smallest weights iteratively, which is known as SVM with Recursive Feature Elimination (SVM-RFE) [9].",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 31,
      "context" : "However, as discussed in [32], such nested “monotonic” feature selection scheme leads to suboptimal performance.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 31,
      "context" : "Non-monotonic feature selection (NMMKL) [32] has been proposed to solve this problem, but each feature corresponding to one kernel makes NMMKL infeasible for high-dimensional problems.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 25,
      "context" : "[26] proposed Feature Generating Machine (FGM), which shows great scalability to non-monotonic feature selection on large-scale and very high-dimensional datasets.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 29,
      "context" : "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 31,
      "context" : "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 25,
      "context" : "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "Due to exponential number of constraints brought by non-smooth multivariate loss functions [11, 13] and exponential number of feature subset combinations [26], the resultant optimization problem is very challenging for high-dimensional data.",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "Due to exponential number of constraints brought by non-smooth multivariate loss functions [11, 13] and exponential number of feature subset combinations [26], the resultant optimization problem is very challenging for high-dimensional data.",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 25,
      "context" : "Due to exponential number of constraints brought by non-smooth multivariate loss functions [11, 13] and exponential number of feature subset combinations [26], the resultant optimization problem is very challenging for high-dimensional data.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 18,
      "context" : "This paper is an extension of our preliminary work [19].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "• Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 31,
      "context" : "• Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 32,
      "context" : "• Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 29,
      "context" : "• Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 25,
      "context" : "• Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 32,
      "context" : "By comparing with these methods, the advantages of our proposed methods are summarized as follows: (1) The tradeoff parameter C in l1 SVM [33] is too sensitive to be tuned properly since it controls both margin loss and the sparsity of w.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : "This separation makes parameter tuning for our methods much easier than those of SKM [3] and l1 SVM.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 31,
      "context" : "(2) NMMKL [32] uses the similar parameter separation strategy, but it is intractable for this method to handle high-dimensional datasets, let alone optimize multivariate losses.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 25,
      "context" : "(3) FGM [26] is a special case of the propose framework when optimizing square hinge loss with indicator variables in integer domain.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 5,
      "context" : "• Recall that Multiple-Instance Learning via Embedded instance Selection (MILES) [6], which transforms multiple instance learning (MIL) into a feature selection problem by embedding bags into an instance-based feature space and selecting the most important features, achieves state-of-the-art performance for multiple instance learning problems.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 27,
      "context" : "Structural SVMs [28, 13] are proposed to solve the corresponding soft-margin case by 1-slack variable formula as,",
      "startOffset" : 16,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "Structural SVMs [28, 13] are proposed to solve the corresponding soft-margin case by 1-slack variable formula as,",
      "startOffset" : 16,
      "endOffset" : 24
    }, {
      "referenceID" : 10,
      "context" : "As shown in [11], optimizing the learning model subject to one specific multivariate measure can really boost the performance of this measure.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 23,
      "context" : "This regularizer is similar to SimpleMKL [24] with each feature corresponding to one kernel, but SimpleMKL is a special case of D with B = 1, which also can be interpreted by the quadratic variational formulation of l1 norm [2].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "This regularizer is similar to SimpleMKL [24] with each feature corresponding to one kernel, but SimpleMKL is a special case of D with B = 1, which also can be interpreted by the quadratic variational formulation of l1 norm [2].",
      "startOffset" : 224,
      "endOffset" : 227
    }, {
      "referenceID" : 3,
      "context" : "Since domains D and A are nonempty, the function S(α,d) is closed and convex for all d ∈ D given any α ∈ A, and the function S(α,d) is closed and concave for all α ∈ A given any d ∈ D, the saddle-point property: mind∈D maxα∈A S(α, d) = maxα∈A mind∈D S(α, d) holds [4].",
      "startOffset" : 264,
      "endOffset" : 267
    }, {
      "referenceID" : 13,
      "context" : "Cutting plane algorithm [14] could be used here to solve this problem.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 21,
      "context" : "As from [22], such cutting plane algorithm can converge to a robust optimal solution within tens of iterations with the exact worst-case analysis.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 24,
      "context" : "In this case, state-of-the-art multiple kernel learning algorithms [25, 24, 31] do not work any more.",
      "startOffset" : 67,
      "endOffset" : 79
    }, {
      "referenceID" : 23,
      "context" : "In this case, state-of-the-art multiple kernel learning algorithms [25, 24, 31] do not work any more.",
      "startOffset" : 67,
      "endOffset" : 79
    }, {
      "referenceID" : 30,
      "context" : "In this case, state-of-the-art multiple kernel learning algorithms [25, 24, 31] do not work any more.",
      "startOffset" : 67,
      "endOffset" : 79
    }, {
      "referenceID" : 26,
      "context" : "Then we can apply the bundle method [27] to solve this primal problem.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "Since Remp(w) is a convex function, its subgradient exists everywhere in its domain [10].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 26,
      "context" : "Following the bundle method [27], the criterion for selecting the next point w is to solve the following problem,",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 26,
      "context" : "The convergence proof in [27] does not apply in this case as the Fenchel dual of Ω(w) fails to satisfy the strong convexity assumption if K > 1.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 26,
      "context" : "As K = 1, Algorithm 1 is exactly the bundle method [27].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "When K ≥ 2, we can adapt the proof of Theorem 5 in [13] for the following convergence results.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "We adapt the proof of Theorem 5 in [13], and sketch the necessary changes corresponding to Problem (29).",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : "The Lemma 2 in [13] states that a line search starting at α along an ascent direction η with maximum step-size C > 0 improves the objective by at least max0≤β≤C { Θd(α+ βη) − Θd(α) } ≥ 12 min { C, ∂Θd(α) T η ηT Qdη } ∂Θd(α) T η.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "We can see that it is a special case of [13] if T = 1.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "According to Theorem 5 in [13], for a newly added constraint ŷ and some γd > 0, we can obtain ∂Θd(α) η = γd by setting the ascent direction ηŷ = 1 for the newly added ŷ and ηy = − 1 Cαy for the others.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "Following the remaining derivation in [13], the overall bound results are obtained.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "Remark 1: Problem (15) is similar to Support Kernel Machine (SKM) [3] in which the multiple Gaussian kernels are built on random subsets of features, with varying widths.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : "In terms of the convergence proof of FGM in [26] and Theorem 1, we can obtain the following theorem to illustrate the approximation with an ǫ-optimal solution to the original problem.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "Fortunately, there are a series of structured loss functions, such as Area Under ROC (AUC), Average Precision (AP), ranking and contingency table scores and other measures listed in [11, 34, 27], which can be implemented efficiently in our algorithms.",
      "startOffset" : 182,
      "endOffset" : 194
    }, {
      "referenceID" : 33,
      "context" : "Fortunately, there are a series of structured loss functions, such as Area Under ROC (AUC), Average Precision (AP), ranking and contingency table scores and other measures listed in [11, 34, 27], which can be implemented efficiently in our algorithms.",
      "startOffset" : 182,
      "endOffset" : 194
    }, {
      "referenceID" : 26,
      "context" : "Fortunately, there are a series of structured loss functions, such as Area Under ROC (AUC), Average Precision (AP), ranking and contingency table scores and other measures listed in [11, 34, 27], which can be implemented efficiently in our algorithms.",
      "startOffset" : 182,
      "endOffset" : 194
    }, {
      "referenceID" : 10,
      "context" : "In this paper, we only use several multivariate performance measures based on contingency table as the showcases and their finding y could be solved in time complexity O(n) [11].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 10,
      "context" : "Then, Algorithm 2 in [11] can be directly applied.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].",
      "startOffset" : 200,
      "endOffset" : 203
    }, {
      "referenceID" : 31,
      "context" : "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 32,
      "context" : "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].",
      "startOffset" : 224,
      "endOffset" : 228
    }, {
      "referenceID" : 29,
      "context" : "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 25,
      "context" : "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].",
      "startOffset" : 250,
      "endOffset" : 254
    }, {
      "referenceID" : 23,
      "context" : "According to [24], we transform Problem (5) in the special case of B = 1 to the following equivalent optimization problem,",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 2,
      "context" : "SKM [3] attempts to obtain the sparsity of w by penalizing the square of a weighted block l1-norm ( ∑k j=1 γj ||wj ||2) where k is the number of groups and wj is the weight vector for the features in the jth group.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 32,
      "context" : "Minimizing the square of the l1-norm is very similar to l1-norm SVM [33] by setting Ω(w) = ||w||1 with the non-negative (convex) loss function.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 31,
      "context" : "This strategy is also used in NMMKL [32], but one feature corresponding to one base kernel makes NMMKL intractable for highdimensional problems.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 39,
      "context" : "According to the work [40], we can reformulate Problem (20) as an equivalent optimization problem",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 25,
      "context" : "It is not trivial to perform the extension of FGM to optimize multivariate loss because original FGM method [26] cannot directly apply to solve the exponential number of constraints.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : ", drug activity prediction [7], image retrieval [35], natural scene classification [20] and text categorization [1], but it is seldom optimized for multivariate performance measures in the literature.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 34,
      "context" : ", drug activity prediction [7], image retrieval [35], natural scene classification [20] and text categorization [1], but it is seldom optimized for multivariate performance measures in the literature.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 19,
      "context" : ", drug activity prediction [7], image retrieval [35], natural scene classification [20] and text categorization [1], but it is seldom optimized for multivariate performance measures in the literature.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : ", drug activity prediction [7], image retrieval [35], natural scene classification [20] and text categorization [1], but it is seldom optimized for multivariate performance measures in the literature.",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 6,
      "context" : "Multi-instance learning was formally introduced in the context of drug activity prediction [7].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "By defining a similarity measure between a bag and an instance, Multiple-Instance Learning via Embedded instance Selection (MILES) [6] successfully transforms multiple instance learning into a feature selection problem by embedding bags into an instance-based feature space and selecting the most important features.",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : "Following the notations in [6], we denote ith positive bags as Bi = {xi,j} n + i j=1 which consists of n + i instances x + i,j , j = 1, .",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 8,
      "context" : "EXPERIMENTS In this Section, we conduct extensive experiments to evaluate the performance of our proposed method and state-of-the-art feature selection methods: 1) SVM-RFE [9]; 2) l1SVM; 3) FGM [26]; 4) l1-bmrm-F1, which is l1 regularized SVM for optimizing F1 score 1http://users.",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 25,
      "context" : "EXPERIMENTS In this Section, we conduct extensive experiments to evaluate the performance of our proposed method and state-of-the-art feature selection methods: 1) SVM-RFE [9]; 2) l1SVM; 3) FGM [26]; 4) l1-bmrm-F1, which is l1 regularized SVM for optimizing F1 score 1http://users.",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 26,
      "context" : "by bundle method [27].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 32,
      "context" : "For l1-SVM, we also use Liblinear software, which implements the state-of-the-art l1-SVM algorithm [33].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "Furthermore, several specific measures on the contingency table are investigated on Text datasets by comparing with SVM [11].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : "Removing inactive constraints from the working set [13] in the inner layer is employed for speedup the QCQP problem.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "1, 1, 10, 100]× n and B in [2, 5, 10, 50, 100, 150, 200, 250].",
      "startOffset" : 27,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "1, 1, 10, 100]× n and B in [2, 5, 10, 50, 100, 150, 200, 250].",
      "startOffset" : 27,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "1, 1, 10, 100]× n and B in [2, 5, 10, 50, 100, 150, 200, 250].",
      "startOffset" : 27,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "binary dataset, we cannot report the training time for l1-bmrm-F1 since l1-bmrm-F1 cannot terminate after more than two days with the maximum iteration 1000 and parameter λ ∈ [10, 10] due to the extremely high dimensionality.",
      "startOffset" : 175,
      "endOffset" : 183
    }, {
      "referenceID" : 9,
      "context" : "binary dataset, we cannot report the training time for l1-bmrm-F1 since l1-bmrm-F1 cannot terminate after more than two days with the maximum iteration 1000 and parameter λ ∈ [10, 10] due to the extremely high dimensionality.",
      "startOffset" : 175,
      "endOffset" : 183
    }, {
      "referenceID" : 26,
      "context" : "after the maximum iteration, which is consistent with the discussion in Appendix C of [27] that bundle method with l1 regularizer cannot guarantee the convergence.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "Since [11] has proven that SVMmulti with Hamming loss, namely∆Err(y, y) = 2(b+c), is the same as SVM.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 25,
      "context" : "Both datasets are used in [26], and they are already split into training and testing sets.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 25,
      "context" : "This is the same as [26].",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : "The setting for budget parameter B = [2, 5, 10, 50, 100, 150, 200, 250] for News20.",
      "startOffset" : 37,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "The setting for budget parameter B = [2, 5, 10, 50, 100, 150, 200, 250] for News20.",
      "startOffset" : 37,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : "The setting for budget parameter B = [2, 5, 10, 50, 100, 150, 200, 250] for News20.",
      "startOffset" : 37,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.",
      "startOffset" : 16,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.",
      "startOffset" : 16,
      "endOffset" : 46
    }, {
      "referenceID" : 9,
      "context" : "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.",
      "startOffset" : 16,
      "endOffset" : 46
    }, {
      "referenceID" : 19,
      "context" : "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.",
      "startOffset" : 16,
      "endOffset" : 46
    }, {
      "referenceID" : 29,
      "context" : "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.",
      "startOffset" : 16,
      "endOffset" : 46
    }, {
      "referenceID" : 39,
      "context" : "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.",
      "startOffset" : 16,
      "endOffset" : 46
    }, {
      "referenceID" : 25,
      "context" : "The elimination scheme of features for SVM-RFE method can be referred to [26].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "1 of [6] to construct a dense and high-dimensional dataset on a preprocessed image data .",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 37,
      "context" : "This dataset is used in [38] for multi-instance learning.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 19,
      "context" : "Each image is represented as a bag of nine instances generated by the SBN method [20].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "After that, following [6], the natural scene image retrieval problem turns out to be a feature selection task to select relevant embedded instances for prediction.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : "Following [11], we use the same notation SVMmulti for different multivariate performance measures.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 10,
      "context" : ", 2] as in [11].",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "Following the recipe of [5], the self-dual cone ‖vt‖2 ≤ ηt, ∀t = 1, .",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "According to the property of self-dual cone [3], we can obtain the primal solution from its dual as wt = −μtvt = μt ∑ y αy′a t y where μj is the dual variable of the j quadratic constraint such that ∑m j=1 μj = 1, μj ∈ R+, ∀j = 1, .",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 25,
      "context" : "This turns out to be the same problem of FGM [26].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 25,
      "context" : "For self-completeness, we give the theorem as follows, Theorem 3 ([26]).",
      "startOffset" : 66,
      "endOffset" : 70
    } ],
    "year" : 2013,
    "abstractText" : "Feature selection with specific multivariate performance measures is the key to the success of many applications, such as image retrieval and text classification. The existing feature selection methods are usually designed for classification error. In this paper, we propose a generalized sparse regularizer. Based on the proposed regularizer, we present a unified feature selection framework for general loss functions. In particular, we study the novel feature selection paradigm by optimizing multivariate performance measures. The resultant formulation is a challenging problem for high-dimensional data. Hence, a twolayer cutting plane algorithm is proposed to solve this problem, and the convergence is presented. In addition, we adapt the proposed method to optimize multivariate measures for multiple instance learning problems. The analyses by comparing with the state-ofthe-art feature selection methods show that the proposed method is superior to others. Extensive experiments on large-scale and high-dimensional real world datasets show that the proposed method outperforms l1-SVM and SVM-RFE when choosing a small subset of features, and achieves significantly improved performances over SVM in terms of F1-score.",
    "creator" : "LaTeX with hyperref package"
  }
}