{
  "name" : "1705.09269.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Geometric Methods for Robust Data Analysis in High Dimension",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Geometric Methods for Robust Data Analysis in High\nDimension\nDissertation\nPresented in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy in the Graduate School of The Ohio State\nUniversity\nBy\nJoseph Timothy Anderson, B.S, M.S\nGraduate Program in Department of Computer Science and Engineering\nThe Ohio State University\n2017\nDissertation Committee: Luis Rademacher, Co-Advisor\nAnastasios Sidiropolous, Co-Advisor\nMikhail Belkin\nFacundo Mémoli\nar X\niv :1\n70 5.\n09 26\n9v 1\n[ cs\n.L G\n] 2\n5 M\nay 2\n01 7\nc© Copyright by\nJoseph Timothy Anderson\n2017\nAbstract\nData-driven applications are growing. Machine learning and data analysis now finds both scientific and industrial application in biology, chemistry, geology, medicine, and physics. These applications rely on large quantities of data gathered from automated sensors and user input. Furthermore, the dimensionality of many datasets is extreme: more details are being gathered about single user interactions or sensor readings. All of these applications encounter problems with a common theme: use observed data to make inferences about the world. Our work obtains the first provably efficient algorithms for Independent Component Analysis (ICA) in the presence of heavy-tailed data. The main tool in this result is the centroid body (a well-known topic in convex geometry), along with optimization and random walks for sampling from a convex body. This is the first algorithmic use of the centroid body and it is of independent theoretical interest, since it effectively replaces the estimation of covariance from samples, and is more generally accessible.\nWe demonstrate that ICA is itself a powerful geometric primitive. That is, having access to an efficient algorithm for ICA enables us to efficiently solve other important problems in machine learning. The first such reduction is a solution to the open problem of efficiently learning the intersection of n + 1 halfspaces in Rn, posed in [43]. This reduction relies on a non-linear transformation of samples from such an intersection of halfspaces (i.e. a simplex ) to samples which are approximately from a\nii\nlinearly transformed product distribution. Through this transformation of samples, which can be done efficiently, one can then use an ICA algorithm to recover the vertices of the intersection of halfspaces.\nFinally, we again use ICA as an algorithmic primitive to construct an efficient solution to the widely-studied problem of learning the parameters of a Gaussian mixture model. Our algorithm again transforms samples from a Gaussian mixture model into samples which fit into the ICA model and, when processed by an ICA algorithm, result in recovery of the mixture parameters. Our algorithm is effective even when the number of Gaussians in the mixture grows with the ambient dimension, even polynomially in the dimension. In addition to the efficient parameter estimation, we also obtain a complexity lower bound for a low-dimension Gaussian mixture model.\niii\nFor my father,\nwho always believed that we can do better.\niv"
    }, {
      "heading" : "Acknowledgments",
      "text" : "There are many who make this work possible. I thank my excellent faculty committee: Misha Belkin, Facundo Mémoli, Tasos Sidiropoulos, and Luis Rademacher. Much of this thesis was in collaboration with them, and I am in their debt. Special thanks go to Luis Rademacher for his years of advice, support, and teaching.\nThe departments at Saint Vincent College and The Ohio State University have\nprovided me with excellent environments for research and learning.\nThroughout my education I’ve had many helpful friends and collaborators; they\neach provided interesting and spirited discussion and insight.\nI am particularly grateful to Br. David Carlson for his instruction, advice, support, and friendship over the years. A large portion of my early adulthood was spent in his tutelage, and I am convinced nothing could have made it a better experience.\nI thank my family, especially my parents and sister, for their unfathomable support\nand encouragement.\nFinally, I thank my wife Sarah for her unfailing love and being an inspiring example\nof a smart, kind, and wonderful person.\nThis work was supported by NSF grants CCF 1350870, and CCF 1422830.\nv\nVita\n23 February 1990 . . . . . . . . . . . . . . . . . . . . . . . . . . . Born - Jeannette, PA\n2012 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .B.S. Computing & Information Science and Mathematics Saint Vincent College 2016 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .M.S. Computer Science & Engineering The Ohio State University 2012-present . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Graduate Teaching Associate The Ohio State University.\nPublications\nResearch Publications\nJ. Anderson, N. Goyal, A. Nandi, L. Rademacher. “Heavy-Tailed Analogues of the Covariance Matrix for ICA”. Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17), Feb. 2017.\nJ. Anderson, N. Goyal, A. Nandi, L. Rademacher. “Heavy-Tailed Independent Component Analysis”. 56th Annual IEEE Symposium on Foundations of Computer Science (FOCS), IEEE 290-209, 2015.\nJ. Anderson, M. Belkin, N. Goyal, L. Rademacher, J. Voss. “The More The Merrier: The Blessing of Dimensionality for Learning Large Gaussian Mixtures”. Conference on Learning Theory (COLT), JMLR W&CP 35:1135-1164, 2014.\nJ. Anderson, N. Goyal, L. Rademacher. “Efficiently Learning Simplices”. Conference on Learning Theory (COLT), JMLR W&CP 30:1020-1045, 2013.\nvi\nJ. Anderson, M. Gundam, A. Joginipelly, D. Charalampidis. “FPGA implementation of graph cut based image thresholding”. Southeastern Symposium on System Theory (SSST), March. 2012.\nFields of Study\nMajor Field: Computer Science and Engineering\nStudies in:\nTheory of Computer Science Luis Rademacher Machine Learning Mikhail Belkin Mathematics Facundo Mémoli\nvii\nTable of Contents\nPage\nAbstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ii\nDedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv\nAcknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v\nVita . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi\nList of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi\n1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.1 Organization of this Thesis . . . . . . . . . . . . . . . . . . . . . . 2\n2. Robust signal separation via convex geometry . . . . . . . . . . . . . . . 4\n2.1 Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.2.1 Heavy-Tailed distributions . . . . . . . . . . . . . . . . . . . 16 2.2.2 Stable Distributions . . . . . . . . . . . . . . . . . . . . . . 17 2.2.3 Convex optimization . . . . . . . . . . . . . . . . . . . . . . 18 2.2.4 Algorithmic convexity . . . . . . . . . . . . . . . . . . . . . 20 2.2.5 The centroid body . . . . . . . . . . . . . . . . . . . . . . . 21\n2.3 Membership oracle for the centroid body . . . . . . . . . . . . . . . 23 2.3.1 Mean estimation using 1 + γ moments . . . . . . . . . . . . 24 2.3.2 Membership oracle for the polar of the centroid body . . . . 26 2.3.3 Membership oracle for the centroid body . . . . . . . . . . . 28 2.4 Orthogonalization via the uniform distribution in the centroid body 31 2.5 Gaussian damping . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2.5.1 The fourth cumulant of Gaussian damping of heavy-tailed distributions . . . . . . . . . . . . . . . . . . . . . . . . . . 39\nviii\n2.5.2 Symmetrization . . . . . . . . . . . . . . . . . . . . . . . . . 41 2.6 Putting things together . . . . . . . . . . . . . . . . . . . . . . . . 42 2.7 Improving orthogonalization . . . . . . . . . . . . . . . . . . . . . . 53\n2.7.1 Orthogonalization via centroid body scaling . . . . . . . . . 54 2.7.2 Orthogonalization via covariance . . . . . . . . . . . . . . . 56 2.7.3 New Membership oracle for the centroid body . . . . . . . . 60\n2.8 Empirical Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 2.8.1 Heavy-tailed ICA when A is orthogonal: Gaussian damping\nand experiments . . . . . . . . . . . . . . . . . . . . . . . . 68 2.8.2 Experiments on synthetic heavy-tailed data . . . . . . . . . 69 2.8.3 ICA on speech data . . . . . . . . . . . . . . . . . . . . . . 71\n3. Learning simplices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n3.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 3.2 Computing the moments of a simplex . . . . . . . . . . . . . . . . . 86 3.3 Subroutine for finding the vertices of a rotated standard simplex . . 87 3.4 Learning simplices . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 3.5 The local and global maxima of the 3rd moment of the standard simplex and the isotropic simplex . . . . . . . . . . . . . . . . . . . 99 3.6 Probabilistic results used . . . . . . . . . . . . . . . . . . . . . . . 103 3.7 Reducing simplex learning to ICA . . . . . . . . . . . . . . . . . . 108\n4. Learning Gaussian Mixture Models . . . . . . . . . . . . . . . . . . . . . 112\n4.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118 4.1.1 Gaussian Mixture Model. . . . . . . . . . . . . . . . . . . . 119 4.1.2 Underdetermined ICA. . . . . . . . . . . . . . . . . . . . . . 120 4.1.3 ICA Results. . . . . . . . . . . . . . . . . . . . . . . . . . . 121 4.2 Learning GMM means using underdetermined ICA: The basic idea 123 4.3 Correctness of the Algorithm and Reduction . . . . . . . . . . . . . 126 4.4 Smoothed Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 4.5 Recovery of Gaussian Weights . . . . . . . . . . . . . . . . . . . . . 151 4.6 Addendum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n4.6.1 Properties of Cumulants . . . . . . . . . . . . . . . . . . . . 153 4.6.2 Rudelson-Vershynin subspace bound . . . . . . . . . . . . . 154 4.6.3 Carbery-Wright anticoncentration . . . . . . . . . . . . . . . 154 4.6.4 Lemmas on the Poisson Distribution . . . . . . . . . . . . . 155 4.6.5 Bounds on Stirling Numbers of the Second Kind . . . . . . 156 4.6.6 Values of Higher Order Statistics . . . . . . . . . . . . . . . 157 4.6.7 Total Variation Distance . . . . . . . . . . . . . . . . . . . . 157\nix\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\nx\nList of Figures\nFigure Page\n2.1 The error of ICA with and without damping (solid lines and dashed lines, resp.), with unitary mixing matrix. The error is averaged over ten trials, in 3 and 10 dimensions where η = (6, 6, 2.1) and η = (6, . . . , 6, 2.1, 2.1), resp. . . . . . . . . . . . . . . . . . . . . . . . . . 69\n2.2 The difference between the errors of FastICA with and without damping in 2 dimensions, averaged over 40 trials. For a single cell, the parameters are given by the coordinates, η = (i, j). Red indicates that FastICA without damping does better than FastICA with damping, white indicates that the error difference is 0 and the blue indicates that FastICA with damping performs better than without damping. Black indicates that FastICA without damping failed (did not return two independent components). . . . . . . . . . . . . . . . . . . . . . . 70\n2.3 The Frobenius error of the recovered mixing matrix with the ‘pow3’ and ‘tanh’ contrast functions, on 10-dimensional data, averaged over ten trials. The mixing matrix A is random with unit norm columns, not orthogonal. In the left and middle figures, the distribution has η = (6, . . . , 6, 2.1, 2.1) while in the right figure, η = (2.1, . . . , 2.1) (see Section 2.8.2 for a discussion). . . . . . . . . . . . . . . . . . . . . . 71\n2.4 Microphone (blue circles) and human speaker (red “×”) layouts for the “perimeterreg” (left) and “ceilreg” (right) voice separation data sets. 72\n2.5 Stability parameter estimates for two voice recording data sets. Each line is one of the speech signals, we can see that the stability parameter is typically less than 2, and in the left data set, most of the signals have parameter close to 2, while in the right, the parameter is more commonly between 1.7 and 2. . . . . . . . . . . . . . . . . . . . . . . 73\nxi\n2.6 (Left): Error of estimated mixing matrix on the “perimeterreg” data, averaged over ten trials. . . . . . . . . . . . . . . . . . . . . . . . . . 74\n2.7 1) Stability parameter α estimates of each component in the “perimeterreg” data. Values below 2 are in the heavy-tailed regime. 2): Smallest singular value and condition number of the orthogonalization matrix BA computed via the centroid body and the covariance. The data was sampled with parameter η = (6, 6, 6, 6, 6, 6, 6, 6, 2.1, 2.1). . . . . . 74\nxii\nChapter 1: INTRODUCTION\nThe “curse of dimensionality” is a well-known problem encountered in the study of algorithms. This curse describes a phenomenon found in many fields of applied mathematics, for instance in numerical analysis, combinatorics, and computational geometry. In machine learning, many questions arise regarding how well existing algorithms scale as the dimension of the input data increases. These questions are well motivated in practice, as data includes more details about observed objects or events. The primary contribution of this research is to demonstrate that several important problems in machine learning are, in fact, efficiently solvable in high-dimension. This work outlines two main contributions toward this goal. First, we use the centroid body, from convex geometry, as a new algorithmic tool, giving the first algorithm for Independent Component Analysis which can tolerate heavy-tailed data. Second, we show that ICA itself can be used as an effective algorithmic primitive and that, with access to an efficient ICA algorithm, one can efficiently solve two other important open problems in machine learning: learning an intersection of n + 1 halfspaces in Rn, and learning the parameters of a Gaussian Mixture Model.\nThe fact that ICA can be used as a new algorithmic primitive, as we demonstrate, introduces new understanding of statistical data analysis. The second contribution in particular, where we show an efficient algorithm to learn the vertices of a simplex, has\n1\na key step in the reduction where one pre-processes the input in a carefully chosen – but simple – manner which results in data that will fit the standard ICA model. This pre-processing step is a random non-linear scaling inspired by the study of `p balls [16]. This scaling procedure is non-trivial and serves as an example of when one can exploit the geometry of a problem to gain statistical insight, and to frame the problem in a new light which has been thoroughly studied."
    }, {
      "heading" : "1.1 Organization of this Thesis",
      "text" : "The rest of this thesis is organized as follows. Chapter 2 will introduce the signal separation framework that will be of importance throughout this work: Independent Component Analysis (ICA). We will develop the general theory behind ICA and our method for improving the state-of-the art ICA method, extending the framework to be approachable when the input data has “heavy-tailed” properties. Our algorithm for ICA is theoretically sound, and comes with provable guarantees for polynomial time and sample complexity. We then present a more practical variation on our new ICA algorithm, and demonstrate its effectiveness on both synthetic and real-world heavy-tailed data. This chapter is based on [10] and [9].\nChapter 3 details an important first step in demonstrating that ICA can be used an effective algorithmic primitive. This chapter shows that ICA can be used to recover an arbitrary simplex with polynomial sample size. This chapter is based on work published in [11].\nChapter 4 presents a second reduction to ICA which recovers the parameters of a Gaussian Mixture Model efficiently in high dimension. Furthermore, we give a\n2\ncomplexity lower bound for learning a Gaussian Mixture in low dimension which also yields a complexity lower bound for ICA itself in certain situations. This chapter is based on [8].\n3\nChapter 2: ROBUST SIGNAL SEPARATION VIA\nCONVEX GEOMETRY\nThe blind source separation problem is the general problem of recovering underlying “source signals” that have been mixed in some unknown way and are presented to an observer. Independent component analysis (ICA) is a popular model for blind source separation where the mixing is performed linearly. Formally, if S is an n-dimensional random vector from an unknown product distribution and A is an invertible linear transformation, one is tasked with recovering the matrix A and the signal S, using only access to i.i.d. samples of the transformed signal, namely X = AS. Due to natural ambiguities, the recovery of A is possible only up to the signs and permutations of the columns. Moreover, for the recovery to be possible the distributions of the random variables Si must not be a Gaussian distribution. ICA has applications in diverse areas such as neuroscience, signal processing, statistics, machine learning. There is vast literature on ICA; see, e.g., [31, 57, 33].\nSince the formulation of the ICA model, a large number of algorithms have been devised employing a diverse set of techniques. Many of these existing algorithms break the problem into two phases: first, find a transformation which, when applied to the observed samples, gives a new distribution which is isotropic, i.e. a rotation of a (centered) product distribution; second, one typically uses an optimization procedure\n4\nfor a functional applied to the samples, such as the fourth directional moment, to recover the axes (or basis) of this product distribution.\nTo our knowledge, all known efficient algorithms for ICA with provable guarantees require higher moment assumptions such as finiteness of the fourth or higher moments for each component Si. Some of the most relevant works, e.g. algorithms of [39, 43], explicitly require the fourth moment to be finite. Algorithms in [111, 48], which make use of the characteristic function also seem to require at least the fourth moment to be finite: while the characteristic function exists for distributions without moments, the algorithms in these papers use the second or higher derivatives of the (second) characteristic function, and for this to be well-defined one needs the moments of that order to exist. Furthermore, certain anticoncentration properties of these derivatives are needed which require that fourth or higher moments exist.\nThus the following question arises: is ICA provably efficiently solvable when the moment condition is weakened so that, say, only the second moment exists, or even when no moments exist? By heavy-tailed ICA we mean the ICA problem with weak or no moment conditions (the precise moment conditions will be specified when needed).\nOur focus in this chapter will be efficient algorithms for heavy-tailed ICA with provable guarantees and finite sample analysis. While we consider this problem to be interesting in its own right, it is also of interest in practice in a range of applications, e.g. [62, 64, 95, 28, 29, 91, 108]. The problem could also be interesting from the perspective of robust statistics because of the following informal connection: algorithms solving heavy-tailed ICA might work by focusing on samples in a small (but not low-probability) region in order to get reliable statistics about the data and\n5\nignore the long tail. Thus if the data for ICA is corrupted by outliers, the outliers are less likely to affect such an algorithm.\nIn this work, heavy-tailed distributions on the real line are those for which low order moments are not finite. Specifically, we will be interested in the case when the fourth or lower order moments are not finite as this is the case that is not covered by previous algorithms. We hasten to clarify that in some ICA literature the word heavy-tailed is used with a different and less standard meaning, namely distributions with positive kurtosis; this meaning will not be used in the present work.\nHeavy-tailed distributions arise in a wide variety of contexts including signal processing and finance; see [78, 82] for an extensive bibliography. Some of the prominent examples of heavy-tailed distributions are the Pareto distribution with shape parameter α which has moments of order less than α, the Cauchy distributions, which has moments of order less than 1; many more examples can be found on the Wikipedia page for heavy-tailed distributions. An abundant (and important in applications) supply of heavy-tailed distributions comes from stable distributions; see, e.g., [78]. There is also some theoretical work on learning mixtures of heavy-tailed distributions, e.g., [36, 27].\nIn several applied ICA models with heavy tails it is reasonable to assume that the distributions have finite first moment. In applications to finance (e.g., [30]), heavy tailed distributions are commonly used to model catastrophic but somewhat unlikely scenarios. A standard measures of risk in that literature, the so called conditional value at risk [87], is only finite when the first moment is finite. Therefore, it is reasonable to assume for some of our results that the distributions have finite first moment.\n6"
    }, {
      "heading" : "2.1 Main result",
      "text" : "Our main result is an efficient algorithm that can recover the mixing matrix A in the model X = AS when each Si has 1 + γ moments for a constant γ > 0. The following theorem states more precisely the guarantees of our algorithm. The theorem below refers to the algorithm Fourier PCA [48] which solves ICA under the fourth moment assumption. The main reason to use this algorithm is that finite sample guarantees have been proved for it; we could have plugged in any other algorithm with such guarantee. The theorem below also refers to Gaussian damping, which is an algorithmic technique we introduce in this chapter and will be explained shortly.\nTheorem 2.1.1 (Heavy-tailed ICA). Let X = AS be an ICA model such that the distribution of S is absolutely continuous, for all i we have E(|Si|1+γ) ≤M <∞ and normalized so that E|Si| = 1, and the columns of A have unit norm. Let ∆ > 0 be such that for each i ∈ [n] if Si has finite fourth moment then its fourth cumulant satisfies |κ4(Si)| ≥ ∆. Then, given 0 < ≤ n2, δ > 0, sM ≥ σmax(A), sm ≤ σmin(A), Algorithm 1 combined with Gaussian damping and Fourier PCA outputs b1, . . . , bn ∈ Rn such that there are signs αi ∈ {−1, 1} and a permutation π : [n] → [n] satisfying ‖Ai − αibπ(i)‖ ≤ , with polyγ(n,M, 1/sm, sM , 1/∆, R, 1/R, 1/ , 1/δ) time and sample complexity and with probability at least 1−δ. Here R is a parameter of the distributions of the Si as described below. The degree of the polynomial is O(1/γ).\nWe note here that the assumption that S has an absolutely continuous distribution is mostly for convenience in the analysis of Gaussian damping and not essential. In particular, it is not used in Algorithm 1.\n7\nIntuitively, R in the theorem statement above measures how large a ball we need to restrict the distribution to, which has at least a constant (actually 1/poly(n) suffices) probability mass and, moreover, each Si when restricted to the interval [−R,R] has fourth cumulant at least Ω(∆). We show that all sufficiently large R satisfy the above conditions and we can efficiently compute such an R; see the discussion after Theorem 2.1.2 (the restatement in Sec. 2.5). For standard heavy-tailed distributions, such as the Pareto distribution, R behaves nicely. For example, consider the Pareto distribution with shape parameter = 2 and scale parameter = 1, i.e. the distribution with density 2/t3 for t ≥ 1 and 0 otherwise. For this distribution it’s easily seen that R = Ω(∆1/2) suffices for the cumulant condition to be satisfied.\nTheorem 2.1.1 requires that the (1 + γ)-moment of the components Si be finite. However, if the matrix A in the ICA model is unitary (i.e. ATA = I, or in other words, A is a rotation matrix) then we do not need any moment assumptions:\nTheorem 2.1.2. Let X = AS be an ICA model such that A ∈ Rn×n is unitary (i.e., ATA = I) and the distribution of S is absolutely continuous. Let ∆ > 0 be such that for each i ∈ [n] if Si has finite fourth moment then |κ4(Si)| ≥ ∆. Then, given , δ > 0, Gaussian damping combined with Fourier PCA outputs b1, . . . , bn ∈ Rn such that there are signs αi ∈ {−1, 1} and a permutation π : [n] → [n] satisfying ‖Ai − αibπ(i)‖ ≤ , in poly(n,R, 1/∆, 1/ , 1/δ) time and sample complexity and with probability at least 1− δ. Here R is a parameter of the distributions of the Si as described above.\nIdea of the algorithm. Like many ICA algorithms, our algorithm has two phases: first orthogonalize the independent components (reduce to the pure rotation case),\n8\nand then determine the rotation. In our heavy-tailed setting, each of these phases requires a novel approach and analysis in the heavy-tailed setting.\nA standard orthogonalization algorithm is to put X in isotropic position using the covariance matrix Cov(X) := E(XXT ). This approach requires finite second moment of X, which, in our setting, is not necessarily finite. Our orthogonalization algorithm (Section 2.4) only needs finite (1 + γ)-absolute moment and that each Si is symmetrically distributed. (The symmetry condition is not needed for our ICA algorithm, as one can reduce the general case to the symmetric case, see Section 2.5.2). In order to understand the first absolute moment, it is helpful to look at certain convex bodies in-\nduced by the first and second moment. The directional second moment EX ( (uTX)2 ) is a quadratic form in u and its square root is the support function of a convex body, Legendre’s inertia ellipsoid, up to some scaling factor (see [73] for example). Similarly, one can show that the directional absolute first moment is the support function of a convex body, the centroid body. When the signals Si are symmetrically distributed, the centroid body of X inherits these symmetries making it absolutely symmetric (see Section 2.2 for definitions) up to an affine transformation. In this case, a linear transformation that puts the centroid body in isotropic position also orthogonalizes the independent components (Lemma 10). In summary, the orthogonalization algorithm is the following: find a linear transformation that puts the centroid body of X in isotropic position. One such matrix is given by the inverse of the square root of the covariance matrix of the uniform distribution in the centroid body. Then apply that transformation to X to orthogonalize the independent components.\nWe now discuss how to determine the rotation (the second phase of our algorithm). The main idea is to reduce heavy-tailed case to a case where all moments exist and\n9\nto use an existing ICA algorithm (from [48] in our case) to handle the resulting ICA instance. We use Gaussian damping to achieve such a reduction. By Gaussian damping we mean to multiply the density of the orthogonalized ICA model by a spherical Gaussian density.\nWe elaborate now on our contributions that make the algorithm possible.\nCentroid body and orthogonalization. The centroid body of a compact set was first defined in [81]. It is defined as the convex set whose support function equals the directional absolute first moment of the given compact set. We generalize the notion of centroid body to any probability measure having finite first moment (see Section 2.2 for the background on convexity and Section 2.2.5 for our formal definition of the centroid body for probability measures). In order to put the centroid body in approximate isotropic position, we estimate its covariance matrix. For this, we use uniformly random samples from the centroid body. There are known methods to generate approximately random points from a convex body given by a membership oracle. We implement an efficient membership oracle for the centroid body of a probability measure with 1 + γ moments. The implementation works by first implementing a membership oracle for the polar of the centroid body via sampling and then using it via the ellipsoid method (see [50]) to construct a membership oracle for the centroid body. As far as we know this is the first use of the centroid body as an algorithmic tool.\nAn alternative approach to orthogonalization in ICA one might consider is to use the empirical covariance matrix of X even when the distribution is heavy-tailed. A specific problem with this approach is that when the second moment does not exist,\n10\nthe diagonal entries would be very different and grow without bound. This problem gets worse when one collects more samples. This wide range of diagonal values makes the second phase of an ICA algorithm very unstable.\nLinear equivariance and high symmetry. A fundamental property of the centroid body, for our analysis, is that the centroid body is linearly equivariant, that is, if one applies an invertible linear transformation to a probability measure then the corresponding centroid body transforms in the same way (already observed in [81]). In a sense that we make precise (Lemma 9), high symmetry and linear equivariance of an object defined from a given probability measure are sufficient conditions to construct from such object a matrix that orthogonalizes the independent components of a given ICA model. This is another way to see the connection between the centroid body and Legendre’s ellipsoid of inertia for our purposes: Legendre’s ellipsoid of inertia of a distribution is linearly equivariant and has the required symmetries.\nGaussian damping. Here we confine ourselves to the special case of ICA when the ICA matrix is unitary, that is ATA = I. A natural idea to deal with heavy-tailed distributions is to truncate the distribution in far away regions and hope that the truncated distribution still gives us a way to extract information. In our setting, this could mean, for example, that we consider the random variable obtained from X conditioned on the even that X lies in the ball of radius R centered at the origin. Instead of the ball we could restrict to other sets. Unfortunately, in general the resulting random variable does not come from an ICA model (i.e., does not have independent components in any basis). Nevertheless one may still be able to use this random variable for recovering A. We do not know how to get an algorithmic\n11\nhandle on it even in the case of unitary A. Intuitively, restricting to a set breaks the product structure of the distribution that is crucial for recovering the independent components.\nWe give a novel technique to solve heavy-tailed ICA for unitary A. No moment assumptions on the components are needed for our technique. We call this technique Gaussian damping. Gaussian damping can also be thought of as restriction, but instead of being restriction to a set it is a “restriction to a spherical Gaussian distribution.” Let us explain. Suppose we have a distribution on Rn with density ρX(·). If we restrict this distribution to a set A (which we assume to be nice: full-dimensional and without any measure theoretic issues) then the density of the restricted distribution is 0 outside A and is proportional to ρX(x) for x ∈ A. One can also think of the density of the restriction as being proportional to the product of ρX(x) and the density of the uniform distribution on A. In the same vein, by restriction to the Gaussian distribution with density proportional to e−‖x‖ 2/R2 we simply mean the distribution with density proportional to ρX(x) e −‖x‖2/R2 . In other words, the density of the restriction is obtained by multiplying the two densities. By Gaussian damping of a distribution we mean the distribution obtained by this operation.\nGaussian damping provides a tool to solve the ICA problem for unitary A by virtue of the following properties: (1) The damped distribution has finite moments of all orders. This is an easy consequence of the fact that Gaussian density decreases super-polynomially. More precisely, one dimensional moment of order d given by the\nintegral ∫ t∈R t dρ(t)e−t 2/R2 dt is finite for all d ≥ 0 for any distribution. (2) Gaussian damping retains the product structure. Here we use the property of spherical\n12\nGaussians that it’s the (unique) class of spherically symmetric distributions with independent components, i.e., the density factors: e−‖x‖ 2/R2 = e−x 2 1/R 2 · · · e−x2n/R2 (we are hiding a normalizing constant factor). Hence the damped density also factors when expressed in terms of the components of s = A−1x (again ignoring normalizing constant factors):\nρX(x)e −‖x‖2/R2 = ρS(s)e −‖s‖2/R2 = ρS1(s1)e −x21/R2 · · · ρSn(Sn)e−x 2 n/R 2 .\nThus we have converted our heavy-tailed ICA model X = AS into another ICA model XR = ASR where XR and SR are obtained by Gaussian damping of X and S, resp. To this new model we can apply the existing ICA algorithms which require at least the fourth moment to exist. This allows us to estimate matrix A (up to signs and permutations of the columns).\nIt remains to explain how we get access to the damped random variable XR. This is done by a simple rejection sampling procedure. Damping does not come free and one has to pay for it in terms of higher sample and computational complexity, but this increase in complexity is mild in the sense that the dependence on various parameters of the problem is still of similar nature as for the non-heavy-tailed case. A new parameter R is introduced here which parameterizes the Gaussian distribution used for damping. We explained the intuitive meaning of R after the statement of Theorem 2.1.1 and that it’s a well-behaved quantity for standard distributions.\nGaussian damping as contrast function. Another way to view Gaussian damping is in terms of contrast functions, a general idea that in particular has been used fruitfully in the ICA literature. Briefly, given a function f : R→ R, for u on the unit sphere in Rn, we compute g(u) := Ef(uTX). Now the properties of the function g(u)\n13\nas u varies over the unit sphere, such as its local extrema, can help us infer properties of the underlying distribution. In particular, one can solve the ICA problem for appropriately chosen contrast function f . In ICA, algorithms with provable guarantees use contrast functions such as moments or cumulants (e.g., [39, 43]). Many other contrast functions are also used. Gaussian damping furnishes a novel class of contrast functions that also leads to provable guarantees. E.g., the function given by f(t) = t4e−t 2/R2 is in this class. We do not use the contrast function view in this work.\nPrevious work related to damping. To our knowledge the damping technique, and more generally the idea of reweighting the data, is new in the context of ICA. But the general idea of reweighting is not new in other somewhat related contexts as we now discuss. In robust statistics (see, e.g., [55]), reweighting idea is used for outlier removal by giving less weight to far away data points. Apart from this high-level similarity we are not aware of any closer connections to our setting; in particular, the weights used are generally different.\nAnother related work is [22], on isotropic PCA, affine invariant clustering, and learning mixtures of Gaussians. This work uses Gaussian reweighting. However, again we are unaware of any more specific connection to our problem.\nFinally, in [111, 48] a different reweighting, using a “Fourier weight” eiu T x (here u ∈ Rn is a fixed vector and x ∈ Rn is a data point) is used in the computation of the covariance matrix. This covariance matrix is useful for solving the ICA problem. But as discussed before, results here do not seem to be amenable to our heavy-tailed setting.\n14"
    }, {
      "heading" : "2.2 Preliminaries",
      "text" : "In this section, we review technical definitions and results which will be used in\nsubsequent sections.\nFor a random vector X ∈ Rn, we denote the distribution function of X as FX\nand, if it exists, the density is denoted fX .\nFor a real-value random variable X, cumulants of X are polynomials in the moments of X. For j ≥ 1, the jth cumulant is denoted κj(X). Denoting mj := EXj. examples: κ1(X) = m1, κ2(X) = m2−m21, κ3(X) = m3−3m2m1+2m31. In general, cumulants can be defined as the coefficients in the logarithm of the moment generating function of X:\nlog(EX(etX)) = ∞∑ i=1 κi(X) ti i! .\nThe first two cumulants are the same as the expectation and the variance, resp. Cumulants have the property that for two independent r.v.sX, Y we have κj(X+Y ) = κj(X)+κj(Y ) (assuming that the first j moments exist for both X and Y ). Cumulants are order-j homogeneous, i.e. if α ∈ R and X is a r.v., then κj(αX) = αjκj(X). The first two cumulants of the standard Gaussian distribution are the mean 0 and the variance 1, and all subsequent Guassian cumulants have value 0.\nAn n-dimensional convex body is a compact convex subset of Rn with non-empty interior. We say a convex body K ⊆ Rn is absolutely symmetric if (x1, . . . , xn) ∈ K ⇔ (±x1, . . . ,±xn) ∈ K. Similarly, we say random variable X (and its distribution) is absolutely symmetric if, for any choice of signs αi ∈ {−1, 1}, (x1, . . . , xn) has the same distribution as (α1x1, . . . , αnxn). We say that an n-dimensional random vector X is symmetric if X has the same distribution as −X. Note that if X is symmetric with\n15\nindependent components (mutually independent coordinates) then its components are also symmetric.\nFor a convex body K, the Minkowski functional of K is pK(x) = inf{α ≥ 0 : αx ∈ K}. The Minkowski sum of two sets A,B ⊆ Rn is the set A+B = {a+b : a ∈ A, b ∈ B}.\nThe singular values of a matrix A ∈ Rm×n will be ordered in the decreasing order:\nσ1 ≥ σ2 ≥ . . . ≥ σmin(m,n). By σmin(A) we mean σmin(m,n).\nWe say that a matrix A ∈ Rn×n is unitary if ATA = I, or in other words A is a rotation matrix. (Normally for matrices with real-valued entries such matrices are called orthogonal matrices and the word unitary is reserved for their complex counterparts, however the word orthogonal matrix can lead to confusion in the present work.) For matrix C ∈ Rn×n, denote by ‖C‖2 the spectral norm and by ‖C‖F the Frobenius norm. We will need the following inequality about the stability of matrix inversion (see for example [102, Chapter III, Theorem 2.5]).\nLemma 1. Let ‖·‖ be a matrix norm such that ‖AB‖ ≤ ‖A‖‖B‖. Let matrices C,E ∈ Rn×n be such that ‖C−1E‖2 ≤ 1, and let C̃ = C + E. Then\n‖C̃−1 − C−1‖ ‖C−1‖ ≤ ‖C −1E‖ 1− ‖C−1E‖ . (2.1)\nThis implies that if ‖E‖2 = ‖C̃ − C‖2 ≤ 1/(2‖C−1‖2), then\n‖C̃−1 − C−1‖2 ≤ 2‖C −1‖22‖E‖2. (2.2)"
    }, {
      "heading" : "2.2.1 Heavy-Tailed distributions",
      "text" : "In general, heavy-tailed distributions are those whose tail probabilities go to zero more slowly than an inverse exponential. Formally, one may write that the distribution of X is heavy-tailed if eλxP (X > x) diverges as x approaches infinity. However,\n16\nfor our applications, we need only care about the existence of higher moments of the distribution. The primary statistical assumption made in the following work assumes only that EX1+γ < ∞ for some γ > 0. The addition of 1 in the above is primarily so that means are well-defined. Naturally, from this assumption, one can construct heavy-tailed distributions, e.g. with density proportional to 1/x2+γ, which will have first moment, up to the 1 + γ moment, but no higher."
    }, {
      "heading" : "2.2.2 Stable Distributions",
      "text" : "The Stable Distributions are an important class of probability distributions characterized by one key property: the family itself is closed under addition and scaler multiplication. That is, if you have two random variables X and Y with the same stable distribution, X + Y will also be a stable distribution with slightly different parameters. Two well-known members of this family are the Gaussian and Cauchy distribution, both of which will be important throughout this work.\nStable distributions are fully characterized by four parameters and can be written as Stable(α, β, c, µ) where α ∈ (0, 2] is the stable parameter (α = 2 for Gaussian and α = 1 for Cauchy), β ∈ [−1, 1] is a skewness parameter, c ∈ (0,∞) is scale, and µ ∈ R is location. The PDF and CDF are not expressible analytically in general, but stable distributions are those for which the characteristic function can be written as\nφ(x;α, β, c, µ) = exp (ixu− |ct|α(1− iβsgn (x) Φ)\nwhere sgn () is the sign function and\nΦ = { tan(πα/2) if α 6= 1 − 2 π log|t| otherwise .\nA useful property which will be used later, and gives a formalization of the closure property mentioned above is the following: if X1, . . . Xn are iid with stable density\n17\nρ(x;α, β, c, µ) and we let\nY = n∑ i=1 ki(Xi − µ)\nwhere ki ∈ R, then Y will have density\ns−1ρ(x/s;α, β, c, 0)\nwhere\ns = ( n∑ i=1 |ki|α ) ."
    }, {
      "heading" : "2.2.3 Convex optimization",
      "text" : "We need the following result: Given a membership oracle for a convex body K one can implement efficiently a membership oracle for K◦, the polar of K. This follows from applications of the ellipsoid method from [50]. Specifically, we use the following facts: (1) a validity oracle for K can be constructed from a membership oracle for K [50, Theorem 4.3.2]; (2) a membership oracle for K◦ can be constructed from a validity oracle for K [50, Theorem 4.4.1].\nThe definitions and theorems in this section all come (occasionally with slight rephrasing) from [50] except for the notion of ( , δ)-weak oracle. [The definitions below use rational numbers instead of real numbers. This is done in [50] as they work out in detail the important low level issues of how the numbers in the algorithm are represented as general real numbers cannot be directly handled by computers. These low-level details can also be worked out for the arguments here, but as is customary, we will not describe these and use real numbers for the sake of exposition.] In this section K ⊆ Rn is a convex body. For y ∈ Rn, the distance of y to K is given by d(y,K) := minz∈K ‖y − z‖. Define S(K, ) := {y ∈ Rn : d(y,K) ≤ } and S(K,− ) := {y ∈ Rn : S(y, ) ⊆ K}. Let Q denote the set of rational numbers.\n18\nDefinition 1 ([50]). The -weak membership problem for K is the following: Given a point y ∈ Qn and a rational number > 0, either (i) assert that y ∈ S(K, ), or (ii) assert that y 6∈ S(K,− ). An -weak membership oracle for K is an oracle that solves the weak membership problem for K. For δ ∈ [0, 1], an ( , δ)-weak membership oracle for K acts as follows: Given a point y ∈ Qn, with probability at least 1 − δ it solves the -weak membership problem for y,K, and otherwise its output can be arbitrary.\nDefinition 2 ([50]). The -weak validity problem for K is the following: Given a vector c ∈ Qn, a rational number γ, and a rational number > 0, either (i) assert that cTx ≤ γ + for all x ∈ S(K,− ), or (ii) assert that cTx ≥ γ − for some x ∈ S(K, ). The notion of -weak validity oracle and ( , δ)-weak validity oracle can be defined similarly to Def. 1.\nDefinition 3 ([50, Section 2.1]). We say that an oracle algorithm is an oraclepolynomial time algorithm for a certain problem defined on a class of convex sets if the running time of the algorithm is bounded by a polynomial in the encoding length of K and in the encoding length of the possibly existing further input, for every convex set K in the given class.\nThe encoding length of a convex set will be specified below, depending on how the\nconvex set is presented.\nTheorem 2.2.1 (Theorem 4.3.2 in [50]). Let R > r > 0 and a0 ∈ Rn. There exists an oracle-polynomial time algorithm that solves the weak validity problem for every convex body K ⊆ Rn contained in the ball of radius R and containing a ball of radius\n19\nr centered at a0 given by a weak membership oracle. The encoding length of K is n plus the length of the binary encoding of R, r, and a0.\nWe remark that the Theorem 4.3.2 as stated in [50] is stronger than the above statement in that it constructs a weak violation oracle (not defined here) which gives a weak validity oracle which suffices for us. The algorithm given by Theorem 4.3.2 makes a polynomial (in the encoding length of K) number of queries to the weak membership oracle.\nLemma 2 (Lemma 4.4.1 in [50]). There exists an oracle-polynomial time algorithm that solves the weak membership problem for K◦, where K is a convex body contained in the ball of radius R and containing a ball of radius r centered at 0 given by a weak validity oracle. The encoding length of K is n plus the length of the binary encoding of R and r.\nOur algorithms and proofs will need more quantitative details from the proofs of\nthe above theorem and lemma. These will be mentioned when we need them."
    }, {
      "heading" : "2.2.4 Algorithmic convexity",
      "text" : "We state here a special case of a standard result in algorithmic convexity: There is an efficient algorithm to estimate the covariance matrix of the uniform distribution in a centrally symmetric convex body given by a weak membership oracle. The result follows from the random walk-based algorithms to generate approximately uniformly random points from a convex body [41, 60, 70]. Most papers use access to a membership oracle for the given convex body. In this work we only have access to an -weak membership oracle. As discussed in [41, Section 6, Remark 2], essentially the same algorithm implements efficient sampling when given an -weak membership\n20\noracle. The problem of estimating the covariance matrix of a convex body was introduced in [60, Section 5.2]. That paper analyzes the estimation from random points. There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].\nTheorem 2.2.2. Let K ⊆ Rn be a centrally symmetric convex body given by a weak membership oracle so that rBn2 ⊆ K ⊆ RBn2 . Let Σ = Cov(K). Then there exists a randomized algorithm that, when given access to the weak membership of K and inputs r, R, δ, c > 0, it outputs a matrix Σ̃ such that with probability at least 1 − δ over the randomness of the algorithm, we have\n(∀u ∈ Rn) (1− c)uTΣu ≤ uT Σ̃u ≤ (1 + c)uTΣu. (2.3)\nThe running time of the algorithm is poly(n, log(R/r), 1/ c, log(1/δ)).\nNote that (2.3) implies ‖Σ̃− Σ‖2 ≤ c‖Σ‖2. The guarantee in (2.3) has the advantage of being invariant under linear transformations in the following sense: if one applies an invertible linear transformation C to the underlying convex body, the covariance matrix and its estimate become CΣCT and CΣ̃CT , respectively. These matrices satisfy\n(∀u ∈ Rn) (1− c)uTCΣCTu ≤ uTCΣ̃CTu ≤ (1 + c)uTCΣCTu.\nThis fact will be used later."
    }, {
      "heading" : "2.2.5 The centroid body",
      "text" : "The main tool in our orthogonalization algorithm is the centroid body of a distribution, which we use as a first moment analogue of the covariance matrix. In\n21\nconvex geometry, the centroid body is a standard (see, e.g., [81, 73, 44]) convex body associated to (the uniform distribution on) a given convex body. Here we use a generalization of the definition from the case of the uniform distribution on a convex body to more general probability measures. Let X ∈ Rn be a random vector with finite first moment, that is, for all u ∈ Rn we have E(|〈u,X〉|) < ∞. Following [81], consider the function h(u) = E(|〈u,X〉|). Then it is easy to see that h(0) = 0, h is positively homogeneous, and h is subadditive. Therefore, it is the support function of a compact convex set [81, Section 3], [93, Theorem 1.7.1], [44, Section 0.6]. This justifies the following definition:\nDefinition 4 (Centroid body). Let X ∈ Rn be a random vector with finite first moment, that is, for all u ∈ Rn we have E(|〈u,X〉|) <∞. The centroid body of X is the compact convex set, denoted ΓX, whose support function is hΓX(u) = E(|〈u,X〉|). For a probability measure P, we define ΓP, the centroid body of P, as the centroid body of any random vector distributed according to P.\nThe following lemma says that the centroid body is equivariant under linear transformations. It is a slight generalization of statements in [81] and [44, Theorem 9.1.3].\nLemma 3. Let X be a random vector on Rn. Let A : Rn → Rn be an invertible linear transformation. Then Γ(AX) = A(ΓX).\nProof. x ∈ Γ(AX) ⇔ ∀u〈x, u〉 ≤ E(|〈u,AX〉|) ⇔ ∀u〈x, u〉 ≤ E(|〈ATu,X〉|) ⇔ ∀v〈x,A−Tv〉 ≤ E(|〈v,X〉|) ⇔ ∀v〈A−1x, v〉 ≤ E(|〈v,X〉|) ⇔ A−1x ∈ Γ(X) ⇔ x ∈ AΓ(X).\n22\nLemma 4 ([44, Section 0.8], [93, Remark 1.7.7]). Let K ⊆ Rn be a convex body with support function hK and such that the origin is in the interior of K. Then K ◦ is a convex body and has radial function rK◦(u) = 1/hK(u) for u ∈ Sn−1.\nLemma 4 implies that testing membership in K◦ is a one-dimensional problem if we have access to the support function hK(·): We can decide if a point z is in K◦ by testing if ‖z‖ ≤ 1/hK(z/‖z‖) instead of needing to check 〈z, u〉 ≤ 1 for all u ∈ K. In our application K = ΓX. We can estimate hΓX(z/‖z‖) by taking the empirical average of |〈x(i), z/‖z‖〉| where the x(i) are samples of X. This leads to an approximate oracle for (ΓX)◦ which will suffice for our application. The details are in Sec. 2.3."
    }, {
      "heading" : "2.3 Membership oracle for the centroid body",
      "text" : "In this section we provide an efficient weak membership oracle (Subroutine 2) for the centroid body ΓX of the r.v. X. This is done by first providing a weak membership oracle (Subroutine 1) for the polar body (ΓX)◦. We begin with a lemma that shows that under certain general conditions the centroid body is “well-rounded.” This property will prove useful in the membership tests.\nLemma 5. Let S = (S1, . . . , Sn) ∈ Rn be an absolutely symmetrically distributed random vector such that E(|Si|) = 1 for all i. Then Bn1 ⊆ ΓS ⊆ [−1, 1]n. Moreover, n−1/2Bn2 ⊆ (ΓS)◦ ⊆ √ nBn2 .\nProof. The support function of ΓS is hΓS(θ) = E|〈S, θ〉| (Def. 4). Then, for each canonical vector ei, hΓS(ei) = hΓS(−ei) = E|Si| = 1. Thus, ΓS is contained in [−1, 1]n. Moreover, since [−1, 1]n ⊆ √ nBn2 , we get ΓS ⊆ √ nBn2 .\n23\nWe claim now that each canonical vector ei is contained in ΓS. To see why, first note that since the support function is 1 along each canonical direction, ΓS will touch the facets of the unit hypercube. Say, there is a point (1, x2, x3, . . . , xn) that touches facet associated to canonical vector e1. But the symmetry of the Sis implies that ΓS is absolutely symmetric, so that (1,±x2,±x3, ...,±xn) is also in the centroid body. Convexity implies that (1, 0, . . . , 0) = e1 is in the centroid body. The same argument applied to all ± canonical vectors implies that they are all contained in the centroid body, and this with convexity implies that the centroid body contains Bn1 . In particular, it contains n−1/2Bn2 ."
    }, {
      "heading" : "2.3.1 Mean estimation using 1 + γ moments",
      "text" : "We will need to estimate the support function of the centroid body in various directions. To this end we need to estimate the first absolute moment of the projection to a direction. Our assumption that each component Si has finite (1 + γ)-moment will allow us to do this with a reasonable small probability of error. This is done via the following Chebyshev-type inequality.\nLet X be a real-valued symmetric random variable such that E|X|1+γ ≤ M for some M > 1 and 0 < γ < 1. Then we will prove that the empirical average of the expectation of X converges to the expectation of X.\nLet ẼN [|X|] be the empirical average obtained from N independent samples\nX(1), . . . , X(N), i.e., (|X(1)|+ · · ·+ |X(N)|)/N . Lemma 6. Let ∈ (0, 1). With the notation above, for N ≥ ( 8M ) 1 2 + 1 γ , we have\nPr[|ẼN [|X|]− E[|X|]| > ] ≤ 8M\n2Nγ/3 .\n24\nProof. Let T > 1 be a threshold whose precise value we will choose later. We have\nPr[|X| ≥ T ] ≤ E[|X| 1+γ]\nT 1+γ =\nM\nT 1+γ .\nBy the union bound,\nPr[∃i ∈ [N ] such that |X(i)| > T ] ≤ NM T 1+γ . (2.4)\nDefine a new random variable XT by\nXT = { X if |X| ≤ T , 0 otherwise.\nUsing the symmetry of XT we have\nVar[|XT |] ≤ E[X2T ] ≤ T E[|XT |] ≤ T E[|X|] ≤ TM1/(1+γ). (2.5)\nBy the Chebyshev inequality and (2.5) we get\nPr[|ẼN [|XT |]− E[|XT |]| > ′] ≤ Var[|XT |] N ′2 ≤ TM 1/(1+γ) N ′2 . (2.6)\nPutting (2.4) and (2.6) together for 0 < ′ < 1/2 we get\nPr[|ẼN [|X|]− E[|XT |]| > ′] ≤ NM T 1+γ + TM1/(1+γ) N ′2 . (2.7)\nChoosing\nN := T 1+γ/2\n′Mγ/(2(1+γ)) , (2.8)\nthe RHS of the previous equation becomes 2M 1−γ/(2(1+γ))\n′T γ/2 . The choice of N is made\nto minimize the RHS; we ignore integrality issues. Pick T0 > 0 so that |E[|X|] − E[|XT0|]| < ′. To estimate T0, note that\nM = E[|X|1+γ] ≥ T γ0 E[||X| − |XT0||],\n25\nHence\n|E[|X|]− E[|XT0 |]| ≤ E[||X| − |XT0||] ≤M/T γ 0 . (2.9)\nWe want M/T γ0 ≤ ′ which is equivalent to T0 ≥ (M ′ ) 1/γ. We set T0 := ( M ′ )1/γ. Then, for T ≥ T0 putting together (2.7) and (2.9) gives\nPr[|ẼN [|X|]− E[|X|]| > 2 ′] ≤ 2M1−γ/(2(1+γ))\n′T γ/2 .\nSetting = 2 ′ (so that ∈ (0, 1)) and expressing the RHS of the last equation in terms of N via (2.8) (and eliminating T ), and using our assumptions γ, ∈ (0, 1), M > 1 to get a simpler upper bound, we get\nPr[|ẼN [|X|]− E[|X|]| > ] ≤ 22+\nγ (2+γ)M1− γ 2(1+γ)\n+ γ 2\n2(2+γ)(1+γ)\n1+ γ 2+γN γ 2+γ\n≤ 8M 2Nγ/3 .\nCondition T ≥ T0, when expressed in terms of N via (2.8), becomes\nN ≥ 2 3 2 + 1 γ\n1 2 + 1 γ\nM 1 2 + 1 γ − γ 2(1+γ) ≥ ( 8M ) 1 2 + 1 γ ."
    }, {
      "heading" : "2.3.2 Membership oracle for the polar of the centroid body",
      "text" : "As mentioned before, our membership oracle for (ΓX)◦ (Subroutine 1) is based on the fact that 1/hΓX is the radial function of (ΓX) ◦, and that hΓX is the directional absolute first moment of X, which can be efficiently estimated by sampling.\nLemma 7 (Correctness of Subroutine 1). Let γ > 0 be a constant and X = AS be given by a symmetric ICA model such that for all i we have E(|Si|1+γ) ≤ M < ∞ and normalized so that E|Si| = 1. Let , δ > 0. Given sM ≥ σmax(A) , sm ≤ σmin(A), Subroutine 1 is an ( , δ)-weak membership oracle for (ΓX) ◦ with using time\n26\nSubroutine 1 Weak Membership Oracle for (ΓX)◦ Input: Query point y ∈ Qn, samples from symmetric ICA model X = AS, bounds sM ≥ σmax(A), sm ≤ σmin(A), closeness parameter , failure probability δ. Output: Weak membership decision for y ∈ (ΓX)◦. 1: Generate iid samples x(1), x(2), . . . , x(N) of X for N =\npolyγ(n,M, 1/sm, sM , 1/ , 1/δ). 2: Compute\nh̃ = 1\nN N∑ i=1 |〈x(i), y ‖y‖ 〉|.\n3: If ‖y‖ ≤ 1/h̃, report y as feasible. Otherwise, report y as infeasible.\nand sample complexity polyγ(n,M, 1/sm, sM , 1/ , 1/δ). The degree of the polynomial is O(1/γ).\nProof. Recall from Def. 1 that we need to show that, with probability at least 1 − δ, Subroutine 1 outputs TRUE when y ∈ S((ΓX)◦, ) and FALSE when y 6∈ S((ΓX)◦,− ); otherwise, the output can be either TRUE or FALSE arbitrarily.\nFix a point y ∈ Qn and let θ := y/‖y‖ denote the direction of y. The algorithm estimates the radial function of (ΓX)◦ along θ, which is 1/hΓX(θ) (see Lemma 4). In the following computation, we simplify the notation by using h = hΓX(θ). It is enough to show that with probability at least 1− δ the algorithm’s estimate, 1/h̃, of the radial function is within of the true value, 1/h.\nFor X(1), . . . , X(N), i.i.d. copies of X, the empirical estimator for h is h̃ :=\n1 N ∑N i=1|〈X(i), θ〉|.\nWe want to apply Lemma 6 to 〈X, θ〉. For this we need a bound on its (1 + γ)moment. The following simple bound is sufficient for our purposes: Let u = AT θ.\n27\nThen |ui| ≤ σmax(A) for all i. Then\nE |〈X, θ〉|1+γ = E ∣∣θTAS∣∣1+γ = E |〈S, u〉|1+γ = E| n∑\ni=1 Siui|1+γ ≤ n∑ i=1 E |Siui|1+γ\n= n∑ i=1 E |Si|1+γ |ui|1+γ ≤M n∑ i=1 |ui|1+γ ≤Mnσmax(A)1+γ ≤Mns1+γM .\n(2.10)\nLemma 6 implies that, for 1 > 0 to be fixed later, and for\nN >\n( 8Mns1+γM\n21δ\n)3/γ , (2.11)\nwe have P (|h̃− h| > 1) ≤ δ.\nLemmas 5 and 3 give that rBn2 ⊆ ΓX for r := sm/ √ n ≤ σmin(A)/ √ n. It follows\nthat h ≥ r. If |h̃− h| ≤ 1 and 1 ≤ r/2, then we have∣∣∣∣1h − 1h̃ ∣∣∣∣ = |h− h̃|hh̃ ≤ 1r(r − 1) ≤ 2 1r2 , which in turn gives, when 1 = min{r2 /2, r/2},\nP (∣∣∣∣1h̃ − 1h ∣∣∣∣ ≤ ) ≥ P (∣∣∣∣1h̃ − 1h ∣∣∣∣ ≤ 2 1r2 ) ≥ P (|h̃− h| ≤ 1) ≥ 1− δ.\nPlugging in the value of 1 and, in turn of r, into (2.11) gives that it suffices to take\nN > polyγ(n,M, 1/sm, sM , 1/ , 1/δ)."
    }, {
      "heading" : "2.3.3 Membership oracle for the centroid body",
      "text" : "We now describe how the weak membership oracle for the centroid body ΓX is constructed using the weak membership oracle for (ΓX)◦, provided by Subroutine 1.\nWe will use the following notation: For a convex body K ∈ Rn, , δ > 0, R ≥ r > 0 such that rBn2 ⊆ K ⊆ RBn2 , oracle WMEMK( , δ, R, r) is an ( , δ)-weak membership\n28\noracle for K. Similarly, oracle WVALK( , δ, R, r) is an ( , δ)-weak validity oracle. Lemma 5 along with the equivariance of Γ (Lemma 3) gives (sm/ √ n)Bn2 ⊆ ΓX ⊆ (sM √ n)Bn2 . Then 1/( √ nsM)B n 2 ⊆ (ΓX)◦ ⊆ ( √ n/sm)B n 2 . Set r := 1/( √ nsM) and R := √ n/sm).\nDetailed description of Subroutine 2. There are two main steps:\n1. Use Subroutine 1 to get an ( 2, δ)-weak membership oracle WMEM(ΓX)◦( 2, δ, R, r)\nfor (ΓX)◦. Theorem 4.3.2 of [50] (stated as Theorem 2.2.1 here) is used in Lemma 8 to get an algorithm to implement an ( 1, δ)-weak validity oracle WVAL(ΓX)◦( 1, δ, R, r) running in oracle polynomial time; WVAL(ΓX)◦( 1, δ, R, r) invokes WMEM(ΓX)◦( 2, δ/Q,R, r) a polynomial number of times, specifically Q = poly(n, logR) (see proof of Lemma 8). The proof of Theorem 4.3.2 can be modified so that 2 ≥ 1/poly(1/ 1, R, 1/r).\n2. Lemma 4.4.1 of [50] (Lemma 2 here) gives an algorithm to construct an ( , δ)-\nweak membership oracle WMEMΓX( , δ, 1/r, 1/R) from WVAL(ΓX)◦( 1, δ, R, r). Lemma 4.4.1 in [50] shows WMEMΓX( , δ, 1/r, 1/R) calls WVAL(ΓX)◦( 1, δ, R, r) once, with 1 ≥ 1/poly(1/ , ‖y‖, 1/r) (where y is the query point).\nLemma 8 (Correctness of Subroutine 2). Let X = AS be given by a symmetric ICA model such that for all i we have E(|Si|1+γ) ≤ M < ∞ and normalized so that E|Si| = 1. Then, given a query point x ∈ Rn, 0 < ≤ n2, δ > 0, sM ≥ σmax(A), and sm ≤ σmin(A), Subroutine 2 is an -weak membership oracle for x and ΓX with probability 1− δ using time and sample complexity poly(n,M, 1/sm, sM , 1/ , 1/δ).\n29\nSubroutine 2 Weak Membership Oracle for ΓX Input: Query point x ∈ Rn, samples from symmetric ICA model X = AS, bounds sM ≥ σmax(A), sm ≤ σmin(A), closeness parameter , failure probability δ, access to a weak membership oracle for (ΓX)◦. Output: ( , δ)-weak membership decision for x ∈ ΓX. 1: Construct WVAL(ΓX)◦( 1, δ, R, r) by invoking WMEM(ΓX)◦( 2, δ/Q,R, r). (See\nStep 1 in the detailed description.) 2: Construct WMEMΓX( , δ, 1/r, 1/R) by invoking WVAL(ΓX)◦( 1, δ, R, r). (See\nStep 2 in the detailed description.) 3: Return the output of running WMEMΓX( , δ, 1/r, 1/R) on x.\nProof. We first prove that WVAL(ΓX)◦( 1, δ, R, r) (abbreviated to WVAL(ΓX)◦ hereafter) works correctly. To this end we need to show that for any given input, WVAL(ΓX)◦ acts as an 1-weak validity oracle with probability at least 1 − δ. Oracle WVAL(ΓX)◦ makes Q queries to WMEM(ΓX)◦( 2, δ/Q,R, r). If the answer to all these queries were correct then Theorem 4.3.2 from [50] would apply and would give that WVAL(ΓX)◦ outputs an answer as expected. Since these Q queries are adaptive we cannot directly apply the union bound to say that the probability of all of them being correct is at least 1− Q(δ/Q) = 1− δ. However, a more careful bound allows us to do essentially that.\nLet q1, . . . , qk be the sequence of queries, where qi depends on the result of the previous queries. For i = 1, . . . , k, let Bi be the event that the answer to query qi by Subroutine 1 is not correct according to the definition of the oracle it implements. These events are over the randomness of Subroutine 1 and event Bi involves the randomness of q1, . . . , qi, as the queries could be adaptively chosen. By the union\nbound, the probability that all answers are correct is at least 1 − ∑k\ni=1 Pr(Bi). It is\nenough to show that Pr(Bi) ≤ δ/Q. To see this, we can condition on the randomness associated to q1, . . . qi−1. That makes qi deterministic, and the probability of failure is\n30\nnow just the probability that Subroutine 1 fails. More precisely, Pr(Bi | q1, . . . , qi−1) ≤ δ/Q, so that\nPr(Bi) = ∫ Pr(Bi | q1, . . . , qi−1) Pr(q1, . . . , qi−1) dq1, . . . , dqi−1 ≤ δ/Q.\nThis proves that the first step works correctly. Correctness of the second step follows directly because the algorithm for construction of the oracle involves a single call to the input oracle as mentioned in Step 2 of the detailed description.\nFinally, to prove that the running time of Subroutine 2 is as claimed the main thing to note is that, as mentioned in Step 1 of the detailed description, 2 is polynomially small in 1 and 1 is polynomially small in and so 2 is polynomially small in ."
    }, {
      "heading" : "2.4 Orthogonalization via the uniform distribution in the",
      "text" : "centroid body\nThe following lemma says that linear equivariance allows orthogonalization:\nLemma 9. Let U be a family of n-dimensional product distributions. Let Ū be the closure of U under invertible linear transformations. Let Q(P) be an n-dimensional distribution defined as a function of P ∈ Ū . Assume that U and Q satisfy:\n1. For all P ∈ U , Q(P) is absolutely symmetric.\n2. Q is linear equivariant (that is, for any invertible linear transformation T we\nhave Q(TP) = TQ(P)).\n3. For any P ∈ Ū , Cov(Q(P)) is positive definite.\nThen for any symmetric ICA model X = AS with PS ∈ U we have Cov(Q(PX))−1/2 is an orthogonalizer of X.\n31\nProof. Consider a symmetric ICA model X = AS with PS ∈ U . Assumptions 1 and 3 imply D := Cov(Q(PS)) is diagonal and positive definite. This with Assumption 2 gives Cov(Q(PX)) = Cov(AQ(PS)) = ADAT = AD1/2(AD1/2)T . Let B = Cov(Q(PX))−1/2 (the unique symmetric positive definite square root). We have B = RD−1/2A−1 for some unitary matrix R (see [53, pg 406]). Thus, BA = RD−1/2 has orthogonal columns, that is, it is an orthogonalizer for X.\nThe following lemma applies the previous lemma to the special case when the\ndistribution Q(P) is the uniform distribution on ΓP.\nLemma 10. Let X be a random vector drawn from a symmetric ICA model X = AS such that for all i we have 0 < E|Si| <∞. Let Y be uniformly random in ΓX. Then Cov(Y )−1/2 is an orthogonalizer of X.\nProof. We will use Lemma 9. After a scaling of each Si, we can assume without loss of generality that E(Si) = 1. This will allow us to use Lemma 5. Let U = {PW : PW is an absolutely symmetric product distribution and E|Wi| = 1, for all i}. For P ∈ Ū , let Q(P) be the uniform distribution on the centroid body of P. For all PW ∈ U , the symmetry of the Wi’s implies that ΓPW , that is, Q(PW ), is absolutely symmetric. By the equivariance of Γ (from Lemma 3) and Lemma 5 it follows that Q is linear equivariant. Let P ∈ Ū . Then there exist A and PW ∈ U such that P = APW . So we get Cov(Q(P)) = Cov(AQ(PW )) = ACov(ΓPW )AT . From Lemma 5 we know Bn1 ⊆ ΓPW so that Cov(ΓPW ) is a diagonal matrix with positive diagonal entries. This implies that Cov(Q(P)) is positive definite and thus by Lemma 9, Cov(Y )−1/2 is an orthogonalizer of X.\n32\nAlgorithm 1 Orthogonalization via the uniform distribution in the centroid body Input: Samples from symmetric ICA model X = AS, bounds sM ≥ σmax(A), sm ≤ σmin(A), error parameters and δ, access to an ( , δ)-weak membership oracle for ΓX provided by Subroutine 2. Output: A matrix B which orthogonalizes the independent components of X. 1: Let Σ̃ be an estimate of Cov(ΓX) obtained via Theorem 2.2.2 sampling algorithm\nsuch as the one in [41] with c = /(2(n+1) 4), r = sm/ √ n, R = sM √ n, and same\nδ. 2: Return B = Σ̃−1/2.\nTheorem 2.4.1 (Correctness of algorithm 1). Let X = AS be given by a symmetric ICA model such that for all i we have E(|Si|1+γ) ≤ M < ∞ and normalized so that E|Si| = 1. Then, given 0 < ≤ n2, δ > 0, sM ≥ σmax(A) , sm ≤ σmin(A) , Algorithm 1 outputs a matrix B so that ‖ATBTBA−D‖2 ≤ , for a diagonal matrix D with diagonal entries d1, . . . , dn satisfying 1/(n+ 1) 2 ≤ di ≤ 1. with probability at least 1− δ using polyγ(n,M, 1/sm, sM , 1/ , 1/δ) time and sample complexity.\nProof. From Lemma 5 we know Bn1 ⊆ ΓS ⊆ [−1, 1]n. Using the equivariance of Γ (Lemma 3), we get σmin(A)/ √ nBn2 ⊆ ΓX ⊆ σmax(A) √ nBn2 . Thus, to satisfy the roundness condition of Theorem 2.2.2 we can take r := sm/ √ n ≤ σmin(A)/ √ n, R := sM √ n ≥ σmax(A) √ n.\nLet Σ̃ be the estimate of Σ := Cov(ΓX) computed by the algorithm. Let ∆̃ := A−1Σ̃A−T be the estimate of ∆ := Cov ΓS obtained from Σ̃ according to how covariance matrices transform under invertible linear transformations of the underlying random vector. As in the proof of Lemma 9, we have Σ̃ = A∆̃AT and B = R∆̃−1/2A−1 for some unitary matrix R. Thus, we have ATBTBA = ∆̃−1. It is natural then to set D := ∆−1 = Cov(ΓS)−1. Let d1, . . . , dn be the diagonal entries of D. We have, using\n33\nLemma 1,\n‖ATBTBA−D‖2 = ‖∆̃ −1 −∆−1‖2\n= ‖∆−1‖2 ‖∆−1(∆̃−∆)‖2\n1− ‖∆−1(∆̃−∆)‖2 . (2.12)\nAs in (2.2), we show that ‖∆−1(∆̃−∆)‖2 is small:\nWe first bound (di), the diagonal entries of D = ∆ −1. Let dmax := maxi di and dmin := mini di. We find simple estimates of these quantities: We have dmin = 1/‖∆‖2 and ‖∆‖2 is the maximum variance of ΓS along coordinate axes. From Lemma 5 we know ΓS ⊆ [−1, 1]n, so that ‖∆‖2 ≤ 1 and dmin ≥ 1. Similarly, dmax = 1/σmin(∆), where σmin(∆) is the smallest diagonal entry of ∆. In other words, it is the minimum variance of ΓS along coordinate axes. From Lemma 5 we know ΓS ⊇ Bn1 , so that ΓS ⊇ [−ei, ei] for all i and Lemma 11 below implies σmin(∆) ≥ 1/(n + 1)2. That is, dmax ≤ (n+ 1)2.\nFrom the bounds on di, Theorem 2.2.2 and the fact discussed after it, we have\n‖∆−1(∆̃−∆)‖ ≤ dmax‖∆‖ c ≤ (n+ 1)2 c ≤ 1/2,\nwhen c ≤ 1/(2(n+ 1)2).\nThis in (2.12) with Theorem 2.2.2 again gives\n‖ATBTBA−D‖ ≤\n2‖∆−1‖‖∆−1(∆̃−∆)‖ ≤ 2‖∆−1‖2‖∆̃−∆‖ ≤ 2d2max c‖∆‖ ≤ 2(n+ 1)4 c.\nThe claim follows by setting c = /(2(n + 1) 4). The sample and time complexity of the algorithm comes from the calls to Subroutine 2. The number of calls is given by Theorem 2.2.2. This leads to the complexity as claimed.\n34\nLemma 11. Let K ⊆ Rn be an absolutely symmetric convex body such that K contains the segment [−e1, e1] = conv{e1,−e1} (where e1 is the first canonical vector). Let X = (X1, . . . , Xn) be uniformly random in K. Then var(X1) ≥ 1/(n+ 1)2.\nProof. Let D be a diagonal linear transformation so that DK isotropic. Let d11 be the first entry of D. It is known that any n-dimensional isotropic convex body is contained in the ball of radius n + 1 [73, 99],[59, Theorem 4.1]. Note that DK contains the segment [−d11e1, d11e1]. This implies d11 ≤ (n + 1). Also, by isotropy we have, 1 = var(d11X1) = d 2 11 var(X1). The claim follows."
    }, {
      "heading" : "2.5 Gaussian damping",
      "text" : "In this section we give an efficient algorithm for the heavy-tailed ICA problem when the ICA matrix is a unitary matrix; no assumptions on the existence of moments of the Si will be required.\nThe basic idea behind our algorithm is simple and intuitive: using X we construct another ICA model XR = ASR, where R > 0 is a parameter which will be chosen later. The components of SR have light-tailed distributions; in particular, all moments exist. We show how to generate samples of XR efficiently using samples of X. Using the new ICA model, the matrix A can be estimated by applying existing ICA algorithms.\nFor a random variable Z we will denote its the probability density function by ρZ(·). The density of XR is obtained by multiplying the density of X by a Gaussian damping factor. More precisely,\nρXR(x) ∝ ρX(x)e−‖x‖ 2/R2 .\n35\nDefine\nKXR := ∫ Rn ρX(x)e −‖x‖2/R2 dx,\nthen\nρXR(x) = 1\nKXR ρX(x)e\n−‖x‖2/R2 .\nWe will now find the density of SR. Note that if x is a value of XR, and s = A −1x\nis the corresponding value of SR, then we have ρXR(x) = 1\nKXR ρX(x)e\n−‖x‖2/R2 = 1\nKXR ρS(s)e\n−‖As‖2/R2 = 1\nKXR ρS(s)e\n−‖s‖2/R2 =: ρSR(s),\nwhere we used that A is a unitary matrix so that ‖As‖ = ‖s‖. Also, ρX(x) = ρS(s) follows from the change of variable formula and the fact that |det (A)| = 1. We also used crucially the fact that the Gaussian distribution is spherically-symmetric. We have now specified the new ICA model XR = ASR, and what remains is to show how to generate samples of XR.\nRejection sampling. Given access to samples from ρX we will use rejection sampling (see e.g. [Robert–Casella] ) to generate samples from ρXR .\n1. Generate x ∼ ρX .\n2. Generate z ∼ U [0, 1].\n3. If z ∈ [0, e−‖x‖2/R2 ], output x; else, go to the first step.\nThe probability of outputting a sample with a single trial in the above algorithm is KXR . Thus, the expected number of trials in the above algorithm for generating a sample is 1/KXR .\n36\nWe now choose R. There are two properties that we want R sufficiently large so as to satisfy: (1) KXR ≥ C1 and |κ4Sj,R| ≥ 1/nC2 where C1 ∈ (0, 1/2) and C2 > 0 are constants. Such a choice of R exists and can be made efficiently; we outline this after the statement of Theorem 2.1.2. Thus, the expected number of trials in rejection sampling before generating a sample is bounded above by 1/C1. The lower bound on KXR will also be useful in bounding the moments of the Sj,R, where Sj,R is the random variable obtained by Gaussian damping of Sj with parameter R, that is to say\nρSj,R(x) ∝ ρSj(x)e−‖x‖ 2/R2 ‘\nDefine\nKSj,R := ∫ R ρsj(sj)e −s2j/R2dsj\nand let KS−j,R be the product of KSk,R over k ∈ [n] \\ {j}. By s−j ∈ Rn−1 we denote the vector s ∈ Rn with its jth element removed, then notice that\nKXR = KSR = KS1,RKS2,R . . . KSn,R , (2.13) KXR = KSj,RKS−j,S . (2.14)\nWe can express the densities of individual components of SR as follows:\nρSj,R(sj) = ∫ Rn−1 ρSR(s) ds−j = 1 KXR ∫ Rn−1 ρS(s)e −‖s‖2/R2 ds−j = KS−j,R KXR ρSj(sj) e −s2j/R2 .\nThis allows us to derive bounds on the moments of Sj,R:\nE[S4j,R] = KS−j,R KXR ∫ R s4j ρSj(sj)e −s2j/R2dsj\n≤ KS−j,R KXR ( max z∈R z4e−z 2/R2 )∫ R ρSj(sj) dsj < KS−j,R KXR R4 ≤ 1 KXR R4 ≤ 1 C1 R4. (2.15)\n37\nWe now state Theorem 4.2 from [48] in a special case by setting parameters k and ki in that theorem to 4 for i ∈ [n]. The algorithm analyzed in Theorem 4.2 of [48] is called Fourier PCA.\nTheorem 2.5.1. [48] Let X ∈ Rn be given by an ICA model X = AS where A ∈ Rn×n is unitary and the Si are mutually independent, E[S4i ] ≤M4 for some positive constant M4, and |κ4(Si)| ≥ ∆. For any > 0 with probability at least 1− δ, Fourier PCA will recover vectors {b1, . . . , bn} such that there exist signs αi = ±1 and a permutation π : [n]→ [n] satisfying\n‖Ai − αibπ(i)‖ ≤ ,\nusing poly(n,M4, 1/∆, 1/ , 1/δ) samples. The running time of the algorithm is also of the same form.\nCombining the above theorem with Gaussian damping gives the following theorem. As previously noted, since we are doing rejection sampling in Gaussian damping, the expected number of trials to generate N samples of XR is N/KXR . One can similarly prove high probability guarantees for the number of trials needed to generate N samples.\nWe remark that the choice of R in the above theorem can be made algorithmically in an efficient way. Theorem 2.5.2 below shows that as we increase R the cumulant κ4(Sj,R) goes to infinty. This shows that for any ∆ > 0 there exists R so as to satisfy the condition of the above theorem, namely |κ4(Si,R)| ≥ ∆. We now briefly indicate how such an R can be found efficiently (same sample and computational costs as in Theorem 2.1.2 above): For a given R, we can certainly estimate KXR =∫ Rn ρX(x)e −‖x‖2/R2dx from samples, i.e. by the empirical mean 1 N ∑ i∈[N ] e −‖x(i)‖2/R2\n38\nof samples x(1), . . . , x(N). This allows us to search for R so that KXR is as large as we want. This also gives us an upper bound on the fourth moment via Eq. (2.15). To ensure that the fourth cumulants of all Si,R are large, note that for a ∈ Rn we\nhave κ4(a1S1,R + · · · + anSn,R) = ∑ i∈[n] a 4 iκ4(Si,R). We can estimate this quantity empirically, and minimize over a on the unit sphere (the minimization can be done, e.g., using the algorithm in [43]). This would give an estimate of mini∈[n] κ4(Si,R) and allows us to search for an appropriate R.\nFor the algorithm to be efficient, we also need KXR ≥ C1. This is easily achieved as we can empirically estimate KXR using the number of trials required in rejection sampling, and search for sufficiently large R that makes the estimate sufficiently larger than C1."
    }, {
      "heading" : "2.5.1 The fourth cumulant of Gaussian damping of heavy-",
      "text" : "tailed distributions\nIt is clear that if r.v. X is such that E(X4) =∞ and E(X2) <∞, then κ4(XR) = E(X4R)− 3(E(X2R))2 →∞ as R→∞. However, it does not seem clear when we have E(X2) = ∞ as well. We will show that in this case we also get κ4(XR) → ∞ as R→∞.\nWe will confine our discussion to symmetric random variables for simplicity of exposition; for the purpose of our application of the theorem this is w.l.o.g. by the argument in Sec. 2.5.2.\nTheorem 2.5.2. Let X be a symmetric real-valued random variable with E(X4) =∞. Then κ4(XR)→∞ as R→∞.\n39\nProof. Fix a symmetric r.v. X with EX2 =∞; as previously noted, if EX2 <∞ then the theorem is easily seen to be true. Since X is symmetric and we will be interested in the fourth cumulant, we can restrict our attention to the positive part of X. So in the following we will actually assume that X is a positive random variable. Fix C > 100 to be any large positive constant. Fix a small positive constant 1 < 1\n100C .\nAlso fix another small positive constant 2 < 1/10. Then there exists a > 0 such that Pr[X ≥ a] = ∫ ∞ a ρX(x)dx ≤ 1. (2.16)\nLet m̃2(R) := ∫∞ 0 x2ρX(x)e −x2/R2dx. Recall that KXR = ∫∞ 0 e−x 2/R2ρ(x)dx =\nE e−X2/R2 . Note that if R ≥ a (which we assume in the sequel), then\n1 > KXR > 1− 1 e . (2.17)\nSince m̃2(R)→∞ as R→∞, by choosing R sufficiently large we can ensure that∫ ∞ a x2ρX(x)e −x2/R2dx ≥ (1− 2) ∫ ∞ 0 x2ρX(x)e −x2/R2dx. (2.18)\nMoreover, we choose R to be sufficiently large so that √ Cm̃2(R) > a. Then\nm̃2(R) = ∫ ∞ 0 x2ρX(x)e −x2/R2dx\n= ∫ a 0 x2ρX(x)e −x2/R2dx\n+ ∫ √Cm̃2(R) a x2ρX(x)e −x2/R2dx+ ∫ ∞ √ Cm̃2(R) x2ρX(x)e −x2/R2dx\n≤ 2 ∫ ∞\n0\nx2ρX(x)e −x2/R2dx\n+ ∫ √Cm̃2(R) a x2ρX(x)e −x2/R2dx+ ∫ ∞ √ Cm̃2(R) x2ρX(x)e −x2/R2dx\n≤ 2 m̃2(R) + 1C m̃2(R) + ∫ ∞ √ Cm̃2(R) x2ρX(x)e −x2/R2dx (by (2.16)) = (C 1 + 2) m̃2(R) +\n∫ ∞ √ Cm̃2(R) x2ρX(x)e −x2/R2dx.\n40\nSummarizing the previous sequence of inequalities:∫ ∞ √ Cm̃2(R) x2ρX(x)e −x2/R2dx ≥ (1− C 1 − 2) m̃2(R). (2.19)\nNow\nEX4R > KXR EX 4 R = ∫ ∞ 0 x4ρX(x)e −x2/R2dx (the inequality uses (2.17))\n≥ ∫ ∞ √ Cm̃2(R) x4ρX(x)e −x2/R2dx\n≥ Cm̃2(R) ∫ ∞ √ Cm̃2(R) x2ρX(x)e −x2/R2dx ≥ Cm̃2(R)(1− C 1 − 2) m̃2(R) (by (2.19)) = C(1− C 1 − 2) m̃2(R)2 = C(1− C 1 − 2) (KXR EX2R)2\n≥ C(1− C 1 − 2) (\n1− 1 e\n)2 (EX2R)2 (by (2.17)).\nNow note that C(1 − C 1 − 2) ( 1− 1 e )2 > 10 for our choice of the parameters. Thus κ4(XR) = EX4R−3(EX2R)2 > 7(EX2R)2, and by our assumption EX2R →∞ with R→∞."
    }, {
      "heading" : "2.5.2 Symmetrization",
      "text" : "As usual we work with the ICA model X = AS. Suppose that we have an ICA algorithm that works when each of the component random variable Si is symmetric, i.e. its probability density function satisfies φi(y) = φi(−y) for all y, with a polynomial dependence on the upper bound M4 on the fourth moment of the Si and inverse polynomial dependence on the lower bound ∆ on the fourth cumulants of the Si. Then we show that we also have an algorithm without the symmetry assumption and with a similar dependence on M4 and ∆. We show that without loss of generality\n41\nwe may restrict our attention to symmetric densities, i.e. we can assume that each of the Si has density function satisfying φi(y) = φi(−y). To this end, let S ′i be an independent copy of Si and set S̄i := Si − S ′i. Similarly, let X̄i := Xi −X ′i. Clearly, the Si and Xi have symmetric densities. The new random variables still satisfy the ICA model: X̄i = AS̄i. Moreover, the moments and cumulants of the S̄i behave similarly to those of the Si: For the fourth moment, assuming it exists, we have E[S̄4i ] = E[(Si−S ′i)4] ≤ 24E[S4i ]. The inequality above can easily be proved using the binomial expansion and Hölder’s inequality:\nE[(Si − S ′i)4] =\nE[S4i ]+4E[S3i ]E[S ′i] + 6E[S2i ]E[(S ′i)2] + 4E[Si]E[(S ′i)3] + E[(S ′i)4] ≤ 16E[S4i ].\nThe final inequality follows from the fact that for each term in the LHS, e.g. E[S3i ]E[S ′i] we have E[S3i ]E[S ′i] ≤ E[S4i ]3/4E[(S ′i)4]1/4 = E[S4i ].\nFor the fourth cumulant, again assuming its existence, we have κ4(S̄i) = κ4(Si −\nS ′i) = κ4(Si) + κ4(−Si) = 2κ4(Si).\nThus if the fourth cumulant of Si is away from 0 then so is the fourth cumulant\nof S̄i."
    }, {
      "heading" : "2.6 Putting things together",
      "text" : "In this section we combine the orthogonalization procedure (Algorithm 1 with performance guarantees in Theorem 2.4.1) with ICA for unitary A via Gaussian damping to prove our main theorem, Theorem 2.1.1.\nAs noted in the introduction, intuitively, R in the theorem statement above measures how large a ball we need to restrict the distribution to so that there is at least a constant (or 1/poly(n) if needed) probability mass in it and moreover each Si when\n42\nrestricted to the interval [−R,R] has the fourth cumulant at least Ω(∆). Formally,\nR > 0 is such that ∫ Rn ρX̂(x)e −‖x‖2/R2 dx ≥ p(n) > 0, where 1/poly(n) < p(n) < 1 can be chosen, and for simplicity, we will fix to 1/2. Moreover, R satisfies that κ4(Si,R) ≥ Ω(∆/n4) for all u ∈ Sn−1 and i ∈ [n], where Si,R is the Gaussian damping with parameter R of Si.\nIn Sec. 2.5.2 we saw that the moments and cumulants of the non-symmetric random variable behave similarly to those of the symmetric random variable. So by the argument of Sec. 2.5.2 we assume that our ICA model is symmetric. Theorem 2.4.1 shows that Algorithm 1 gives us a new ICA model with the ICA matrix having approximately orthogonal columns. We will apply Gaussian damping to this new ICA model. In Theorem 2.4.1, it was convenient to use the normalization E|Si| = 1 for all i. But for the next step of Gaussian damping we will use a different normalization, namely, the columns of the ICA matrix have unit length. This will require us to rescale (in the analysis, not algorithmically) the components of S appropriately as we now describe.\nAlgorithm 1 provides us with a matrix B such that the columns of C = BA are approximately orthogonal: CTC ≈ D where D is a diagonal matrix. Thus, we can rewrite our ICA model as Y = CS, where Y = BX. We rescale Ci, the ith column of C, by multiplying it by 1/‖Ci‖. Denoting by L the diagonal matrix with the ith diagonal entry 1/‖Ci‖, the matrix obtained after the above rescaling of C is CL and we have (CL)TCL ≈ I. We can again rewrite our ICA model as Y = (CL)(L−1S). Setting Ê := CL and T := L−1S we can rewrite our ICA model as Ŷ = ÊT . This is the model we will plug into the Gaussian damping procedure. Had Algorithm 1 provided us with perfect orthogonalizer B (so that CTC = D) we would obtain a\n43\nmodel Y = ET where E is unitary. We do get however that Ê ≈ E. To continue with a more standard ICA notation, from here on we will write X̂ = ÊS for Ŷ = ÊT and X = ES for Y = ET .\nApplying Gaussian damping to X = ES gives us a new ICA model XR = ESR as we saw in Sec. 2.5. But the model we have access to is X̂ = ÊS. We will apply Gaussian damping to it to get the r.v. X̂R. Formally, X̂R is defined starting with the model X̂ = ÊS just as we defined XR starting with the model X = ES (recall that for a random variable Z, we denote its probability density function by ρZ(·)):\nρX̂R(x) := 1\nKX̂R ρX̂(x)e\n−‖x‖2/R2 ,\nwhere KX̂R := ∫ Rn ρX̂(x)e −‖x‖2/R2 dx. The parameter R has been chosen so that KX̂R > C1 := 1/2 and κ4 〈X̂R, u〉 ≥ ∆ for all u ∈ § n−1. By the discussion after Theorem 2.1.2 (the restatement in Sec. 2.5), this choice of R can be made efficiently. (The discussion there is in terms of the directional moments of S, but note that the directional moments of X also give us directional moments of S. We omit further details.) But now since the matrix Ê in our ICA model is only approximately unitary, after applying Gaussian damping the obtained random variable X̂R is not given by an ICA model (in particular, it may not have independent coordinates in any basis), although it is close to XR in a sense to be made precise soon.\nBecause of this, Theorem 2.5.1 is not directly usable for plugging in the samples of X̂R. To address this discrepancy we will need a robust version of Theorem 2.5.1 which also requires us to specify in a precise sense that X̂R and XR are close. To this end, we need some standard terminology from probability theory. The characteristic function of r.v. X ∈ Rn is defined to be φX(u) = E(eiu TX), where u ∈ Rn. The cumulant generating function, also known as the second characteristic function, is defined by\n44\nψX(u) = log φX(u). The algorithm in [48] estimates the second derivative of ψX(u) and computes its eigendecomposition. (In [111] and [48], this second derivative is interpreted as a kind of covariance matrix of X but with the twist that a certain “Fourier” weight is used in the expectation computation for the covariance matrix. We will not use this interpretation here.) Set ΨX(u) := D 2ψX(u), the Hessian matrix of ψX(u). We can now state the robust version of Theorem 2.5.1.\nTheorem 2.6.1. Let X be an n-dimensional random vector given by an ICA model X = AS where A ∈ Rn×n is unitary and the Si are mutually independent, E[S4i ] ≤M4 and |κ4(Si)| ≥ ∆ for positive constants M4 and ∆. Also let 2.6.1 ∈ [0, 1]. Suppose that we have another random variable X̂ that is close to X in the following sense:\n|ΨX̂(u)−ΨX(u)| ≤ 2.6.1,\nfor any u ∈ Rn with ‖u‖ ≤ 1. Moreover, E[〈X, u〉4] ≤M4 for ‖u‖ ≤ 1. When Fourier PCA is given samples of X̂ it will recover vectors {b1, . . . , bn} such that there exist signs αi ∈ {−1, 1} and a permutation π : [n]→ [n] satisfying\n‖Ai − αibπ(i)‖ ≤ 2.6.1 ( M4 δ∆ )5 ,\nin poly(n,M4, 1/∆, 1/ 2.6.1, 1/δ2.6.1) samples and time complexity and with probability at least 1− δ2.6.1.\nWhile this theorem is not stated in [48], it is easy to derive from their proof of Theorem 2.5.1; we now briefly sketch the proof of Theorem 2.6.1 indicating the changes one needs to make to the proof of Theorem 2.5.1 in [48].\nProof. Ideally, for input model X = AS with A unitary, algorithm Fourier PCA would proceed by diagonalizing ΨX(u). But it can only compute an approximation\n45\nΨ̃X(u) which is the empirical estimate for ΨX(u). For all u with ‖u‖ ≤ 1, it is shown that with high probability we have\n‖Ψ̃X(u)−ΨX(u)‖F < . (2.20)\nThen, a matrix perturbation argument is invoked to show that if the diagonalization procedure used in Fourier PCA is applied to Ψ̃X(u) instead of ΨX(u), one still recovers a good approximation of A. This previous step uses a random u chosen from a Gaussian distribution so that the eigenvalues of ΨX(u) are sufficiently spaced apart for the eigenvectors to be recoverable (the assumptions on the distribution ensure that the requirement of ‖u‖ ≤ 1 is satisfied with high probability). The only property of Ψ̃X(u) used in this argument is (2.20). To prove Theorem 2.6.1, we show that the estimate Ψ̃X̂ is also good:\n‖Ψ̃X̂(u)−ΨX(u)‖F < ‖Ψ̃X̂(u)−ΨX̂(u)‖F + ‖ΨX̂(u)−ΨX(u)‖F < 2 ,\nwhere we ensured that ‖Ψ̃X̂(u)−ΨX̂(u)‖F < by taking sufficiently many samples of X̂ to get a good estimate with probability at least δ; as in [48], a standard concentration argument shows that poly(n,M4, 1/∆, 1/ , 1/δ) samples suffice for this purpose. Thus the diagonalization procedure can be applied to Ψ̃X̂(u). The upper bound of 2\nabove translates into error < ( M4 δ∆ )5 in the final recovery guarantee, with the extra factor coming from the eigenvalue gaps of ΨX(u).\nTo apply Theorem 2.6.1 to our situation, we need\nΨ̃X̂R(u) ≈ ΨXR(u). (2.21)\nThis will follow from the next lemma.\n46\nNote that\nΨX(u) = D 2ψX(u) =\nD2φX(u)\nφX(u) − (DφX(u))\nT (DφX(u))\nφX(u)2 . (2.22)\n(The gradient DφX(u) is a row vector.) Thus, to show (2.21) it suffices to show that each expression on the RHS of the previous equation is appropriately close:\nLemma 12. Let λ ∈ [0, 1/2], and let E, Ê ∈ Rn×n such that E is unitary and ‖E − Ê‖2 ≤ λ2/3. Let XR and X̂R be the random variables obtained by applying Gaussian damping to the ICA models X = ES and X̂ = ÊS, resp. Then, for ‖u‖ ≤ 1, we have\n∣∣φXR(u)− φX̂R(u)∣∣ ≤ Rλ2/3 + 4λ+ 4λKXR , ‖DφXR(u)−DφX̂R(u)‖ ≤ O(nλR 2),\n‖D2φXR(u)−D2φX̂R(u)‖F ≤ O(n 2λR3).\nProof. We will only prove the first inequality; proofs of the other two are very similar and will be omitted. In the second equality in the displayed equations below\nwe use that ∫ Rn e iuT xe−‖x‖ 2/R2ρX̂(x) dx = ∫ Rn e iuT Êse−‖Ês‖ 2 /R2ρS(s) ds. One way to\nsee this is to think of the two integrals as expectations: E ( eiu T X̂e−‖X̂‖ 2 /R2 ) =\n47\nE ( eiu T ÊSe−‖ÊS‖ 2 /R2 ) .\n|φXR(u)− φX̂R(u)|\n= ∣∣∣∣∣ 1KXR ∫ Rn eiu T xe−‖x‖ 2/R2ρX(x)dx− 1 KX̂R ∫ Rn eiu T xe−‖x‖ 2/R2ρX̂(x)dx ∣∣∣∣∣ =\n∣∣∣∣∣ 1KXR ∫ Rn eiu TEse−‖Es‖ 2/R2ρS(s)ds− 1 KX̂R ∫ Rn eiu T Êse−‖Ês‖ 2 /R2ρS(s)ds ∣∣∣∣∣ ≤ 1 KXR ∣∣∣∣∫ Rn eiu TEse−‖Es‖ 2/R2ρS(s)ds− ∫ Rn eiu T Êse−‖Ês‖ 2 /R2ρS(s)ds\n∣∣∣∣ +\n∣∣∣∣∣ 1KXR − 1KX̂R ∣∣∣∣∣ · ∣∣∣∣∫ Rn eiu T Êse−‖Ês‖ 2 /R2ρS(s)ds ∣∣∣∣ ≤ 1 KXR ∫ Rn\n∣∣∣eiuTEse−‖Es‖2/R2 − eiuT Êse−‖Ês‖2/R2∣∣∣ ρS(s)ds︸ ︷︷ ︸ G\n+ ∣∣∣∣∣ 1KXR − 1KX̂R ∣∣∣∣∣KX̂R︸ ︷︷ ︸\nH\n. (2.23)\nNow\nG = ∫ Rn ∣∣∣eiuT (Ê−E)se(‖Es‖2−‖Ês‖2)/R2 − 1∣∣∣ e−‖Es‖2/R2ρS(s)ds. We have\n∣∣∣eiuT (Ê−E)se(‖Es‖2−‖Ês‖2)/R2 − 1∣∣∣ ≤ ∣∣∣eiuT (Ê−E)s − 1∣∣∣+ ∣∣∣e(‖Es‖2−‖Ês‖2)/R2 − 1∣∣∣ , and so\nG ≤ ∫ Rn ∣∣∣eiuT (Ê−E)s − 1∣∣∣ e−‖Es‖2/R2ρS(s)ds (2.24) +\n∫ Rn ∣∣∣e(‖Es‖2−‖Ês‖2)/R2 − 1∣∣∣ e−‖Es‖2/R2ρS(s)ds. (2.25) For the first summand in (2.24) note that\n∣∣∣eiuT (Ê−E)s − 1∣∣∣ ≤ ∣∣∣uT (Ê − E)s∣∣∣ . 48\nThis follows from the fact that for real θ we have\n∣∣eiθ − 1∣∣2 = (cos θ − 1)2 + sin2 θ = 2− 2 cos θ = 4 sin2(θ/2) ≤ θ2. So,∫\nRn ∣∣∣uT (Ê − E)s∣∣∣ e−‖Es‖2/R2ρS(s)ds ≤ ‖u‖‖Ê − E‖2 ∫ Rn ‖s‖e−‖s‖ 2/R2ρS(s)ds (using ‖Es‖ = ‖s‖ as E is unitary)\n≤ ‖u‖‖Ê − E‖2 (\nmax z∈R\nze−z 2/R2 )∫ Rn ρS(s)ds\n≤ ‖u‖‖Ê − E‖2R ≤ Rλ2/3,\nwhere the last inequality used our assumption that ‖u‖ ≤ 1.\nWe will next bound second summand in (2.24). Note that ‖Es‖ = ‖s‖ and ‖Es‖2 − ‖Ês‖ 2 ≤ (‖E‖+ ‖Ê‖)(‖E − Ê‖)‖s‖2. Since ‖E − Ê‖ ≤ λ2/3 << 1, we get ‖Es‖2 − ‖Ês‖ 2 ≤ (‖E‖ + ‖Ê‖)(‖E − Ê‖)‖s‖2 ≤ λ2‖s‖2. We will use that eλ − 1 < λ+λ2 for λ ∈ [0, 1/2] which is satisfied by our assumption. Now the second summand in (2.24) can be bounded as follows.\n49\n∫ Rn ∣∣∣e(‖Es‖2−‖Ês‖2)/R2 − 1∣∣∣ e−‖Es‖2/R2ρS(s)ds ≤ ∫ Rn\n∣∣∣eλ2‖s‖2/R2 − 1∣∣∣ e−‖s‖2/R2ρS(s)ds ≤ ∫ Rn\n∣∣∣eλ2‖s‖2/R2 − 1∣∣∣ e−‖s‖2/R2ρS(s)ds =\n∫ ‖s‖≤R/ √ λ ∣∣∣eλ2‖s‖2/R2 − 1∣∣∣ e−‖s‖2/R2ρS(s)ds +\n∫ ‖s‖>R/ √ λ ∣∣∣eλ2‖s‖2/R2 − 1∣∣∣ e−‖s‖2/R2ρS(s)ds ≤ ∫ ‖s‖≤R/ √ λ (λ+ λ2)e−‖s‖ 2/R2ρS(s)ds+ ∫ ‖s‖>R/ √ λ e−(1−λ 2)‖s‖2/R2ρS(s)ds ≤ (λ+ λ2)KXR + e−(1−λ 2)/λ ≤ 2λKXR + 2λ (using λ < 1/2).\nCombining our estimates gives\nG ≤ Rλ 2\n3KXR + 2λ+\n2λ\nKXR .\nFinally, to bound H, note that\n1\nKXR ∣∣KXR −KX̂R∣∣ = 1KXR ∣∣∣∣∫ Rn e−‖x‖ 2/R2ρX(x)dx− ∫ Rn e−‖x‖ 2/R2ρX̂(x)dx ∣∣∣∣ = 1\nKXR ∣∣∣∣∫ Rn e−‖Es‖ 2/R2ρS(s)ds− ∫ Rn e−‖Ês‖ 2 /R2ρS(s)ds ∣∣∣∣ . This we just upper-bounded above by 2λ+ 2λ\nKXR .\nThus we have the final estimate ∣∣φXR(u)− φX̂R(u)∣∣ ≤ G+H ≤ Rλ2/3 + 4λ+ 4λKXR . The proofs of the other two upper bounds in the lemma follow the same general\npattern with slight changes.\n50\nWe are now ready to prove Theorem 2.1.1.\nProof of Theorem 2.1.1. We continue with the context set after the statement of Theorem 2.1.1. The plan is to apply Theorem 2.6.1 to X̂R and XR. To this end we begin by showing that the premise of Theorem 2.6.1 is satisfied.\nTheorem 2.4.1, with δ2.4.1 = δ/2 and 2.4.1 to be specified, provides us with a matrix B such that the columns of BA are approximately orthogonal: ‖(BA)TBA−D‖2 ≤\n2.4.1 for some diagonal matrix D. Now set Ê := BAL, where L = diag(L1, . . . , Ln) =\ndiag(1/ √ d1, . . . , 1/ √ dn). Theorem 2.4.1 implies that\n1 ≤ Li ≤ (n+ 1). (2.26)\nThen\n‖ÊT Ê − I‖2 = ‖(BAL) T (BAL)− I‖2\n= ‖LT (BA)TBAL− LTDL‖2 ≤ ‖L‖ 2 2‖(BA) TBA−D‖2 ≤ (n+ 1) 2 2.4.1,\nbecause Li ≤ (n+ 1) by (2.26). For Ê as above, there exists a unitary E such that\n‖E − Ê‖2 ≤ (n+ 1) 2 2.4.1, (2.27)\nby Lemma 13 below.\nBy our choice of R, the components of SR satisfy κ4(Si,R) ≥ ∆ and ES4i,R ≤ 2R4 (via (2.15) and our choice C1 = 1/2). Hence M4 ≤ 2R4. The latter bound via (2.27) gives E[〈X, u〉4] ≤ 2(1 + (n+ 1)2 2.4.1)R4 for all u ∈ §n−1.\nFinally, Lemma 12 with (2.22) and simple estimates give ‖ΨX̂R −ΨXR‖F ≤\nO(n4R4 1/2 2.4.1).\n51\nWe are now ready to apply Theorem 2.6.1 with 2.6.1 = O(n 4R4 1/2 2.4.1) and δ2.6.1 = δ/2. This gives that Fourier PCA produces output b1, . . . , bn such that there are signs αi = ±1 and permutation π : [n]→ [n] such that\n‖Ai − αibπ(i)‖ ≤ O(n4R4 1/22.4.1) ( R4\nδ∆\n)5 , (2.28)\nwith poly(n, 1/∆, R, 1/R, 1/ 2.4.1, 1/δ) sample and time complexity. Choose 2.4.1 so that the RHS of (2.28) is .\nThe number of samples and time needed for orthogonalization is\npolyγ(n,M, 1/sm, SM , 1/ 2.4.1, 1/δ). Substituting the value of 2.4.1 the previous bound becomes polyγ(n,M, 1/sm, SM , 1/∆, R, 1/R, 1/ , 1/δ). The probability of error, coming from the applications of Theorem 2.4.1 and 2.6.1 is at most δ/2 + δ/2 = δ.\nLemma 13. Let Ê ∈ Rn×n be such that ‖ÊT Ê − I‖2 ≤ . Then there exists a unitary matrix E ∈ Rn×n such that ‖E − Ê‖2 ≤ .\nProof. This is related to a special case of the so-called orthogonal Procrustes problem [46, Section 12.4.1], where one looks for a unitary matrix E that minimizes ‖E − Ê‖F . A formula for an optimal E is E = UV T , where UΣV T is the singular value decomposition of Ê, with singular values (σi). Although we do not need the fact that this E minimizes ‖E − Ê‖F , it is good for our purpose:\n‖Ê − E‖2 = ‖UΣV T − UV T‖2 = ‖Σ− I‖2 = maxi |σi − 1|.\nBy our assumption\n‖ÊT Ê − I‖2 = ‖V Σ 2V T − I‖2 = ‖Σ 2 − I‖2 = maxi |σ 2 i − 1| = max i (σi + 1)|σi− 1| ≤ . This implies maxi|σi − 1| ≤ . The claim follows.\n52"
    }, {
      "heading" : "2.7 Improving orthogonalization",
      "text" : "As noted above, the technique in [10], while being provably efficient and correct, suffers from practical implementation issues. Here we discuss two alternatives: orthogonalization by centroid body scaling and orthogonalization by using the empirical covariance. The former, orthogonalization via centroid body scaling, uses the samples already present in the algorithm rather than relying on a random walk to draw samples which are approximately uniform in the algorithm’s approximation of the centroid body (as is done in [10]). This removes the dependence on random walks and the ellipsoid algorithm; instead, we use samples that are distributed according to the original heavy-tailed distribution but non-linearly scaled to lie inside the centroid body. We prove in Lemma 14 that the covariance of this subset of samples is enough to orthogonalize the mixing matrix A. Secondly, we prove that one can, in fact, “forget” that the data is heavy tailed and orthogonalize by using the empirical covariance of the data, even though it diverges, and that this is enough to orthogonalize the mixing matrix A. However, as observed in experimental results, in general this has a downside compared to orthogonalization via centroid body in that it could cause numerical instability during the “second” phase of ICA as the data obtained is less well-conditioned. This is illustrated directly in the table in Figure 2.5 containing the singular value and condition number of the mixing matrix BA in the approximately orthogonal ICA model.\n53"
    }, {
      "heading" : "2.7.1 Orthogonalization via centroid body scaling",
      "text" : "In [10], another orthogonalization procedure, namely orthogonalization via the uniform distribution in the centroid body is theoretically proven to work. Their procedure does not suffer from the numerical instabilities and composes well with the second phase of ICA algorithms. An impractical aspect of that procedure is that it needs samples from the uniform distribution in the centroid body.\nWe described orthogonalization via centroid body in Section 1, except for the estimation of p(x), the Minkowski functional of the centroid body. The complete procedure is stated in Subroutine 3.\nWe now explain how to estimate the Minkowski functional. The Minkowski functional was informally described in Section 1. The Minkowski functional of ΓX is formally defined by p(x) := inf{t > 0 : x ∈ tΓX}. Our estimation of p(x) is based on an explicit linear program (LP) (2.36) that gives the Minkowski functional of the centroid body of a finite sample of X exactly and then arguing that a sample estimate is close to the actual value for ΓX. For clarity of exposition, we only analyze formally a special case of LP (2.36) that decides membership in the centroid body of a finite sample of X (LP (2.35)) and approximate membership in ΓX. This analysis is in Section 2.7.3. Accuracy guarantees for the approximation of the Minkowski functional follow from this analysis.\nLemma 14. Let X be a random vector drawn from an ICA model X = AS such that for all i we have E|Si| = 1 and Si is symmetrically distributed. Let Y = tanh p(X)p(X) X where p(X) is the Minkoswki functional of ΓX. Then Cov(Y )−1/2 is an orthogonalizer of X.\n54\nSubroutine 3 Orthogonalization via centroid body scaling Input: Samples (X(i))Ni=1 of ICA model X = AS so each Si is symmetric with (1+γ) moments. Output: Matrix B approximate orthogonalizer of A 1: for i = 1 : N do, 2: Let λ∗ be the optimal value of (2.36) with q = X(i). Let di = 1/λ\n∗. Let Y (i) = tanh di\ndi X(i).\n3: end for 4: Let C = 1\nN ∑N i=1 Y (i)Y (i) T . Output B = C−1/2.\nProof. We will be applying Lemma 9. Let U denote the set of absolutely symmetric product distributions PW over Rn such that E|Wi| = 1 for all i. For PV ∈ Ū , let Q(PV ) be equal to the distribution obtained by scaling V as described earlier, that is, distribution of αV , where α = tanh p(V ) p(V ) , p(V ) is the Minkoswki functional of ΓPV .\nFor all PW ∈ U , Wi is symmetric and E|Wi| = 1 which implies that αW , that is, Q(PW ) is absolutely symmetric. Let PV ∈ Ū . Then Q(PV ) is equal to the distribution of αV . For any invertible linear transformation T and measurable set M, we have Q(TPV )(M) = Q(PTV )(M) = PαTV (M) = PαV (T−1M) = TQ(PV )(M). Thus Q is linear equivariant. Let P ∈ Ū . Then there exist A and PW ∈ U such that P = APW . We get Cov(Q(P)) = Cov(AQ(PW )). Let Wα = αW . Thus, Cov(AQ(PW )) = AE(WαW Tα )AT where E(WαW Tα ) is a diagonal matrix with elements E(α2W 2i ) which are non-zero because we assume E|Wi| = 1. This implies that Cov(Q(P)) is positive definite and thus by Lemma 9, Cov(Y )−1/2 is an orthogonalizer of X.\n55"
    }, {
      "heading" : "2.7.2 Orthogonalization via covariance",
      "text" : "Here we show the somewhat surprising fact that orthogonalization of heavy-tailed signals is sometimes possible by using the “standard” approach: inverting the empirical covariance matrix. The advantage here, is that it is computationally very simple, specifically that having heavy-tailed data incurs very little computational penalty on the process of orthogonalization alone. It’s standard to use covariance matrix for whitening when the second moments of all independent components exist [57]: Given samples from the ICA model X = AS, we compute the empirical covariance matrix Σ̃ which tends to the true covariance matrix as we take more samples and set B = Σ̃−1/2. Then one can show that BA is a rotation matrix, and thus by pre-multiplying the data by B we obtain an ICA model Y = BX = (BA)S, where the mixing matrix BA is a rotation matrix, and this model is then amenable to various algorithms. In the heavy-tailed regime where the second moment does not exist for some of the components, there is no true covariance matrix and the empirical covariance diverges as we take more samples. However, for any fixed number of samples one can still compute the empirical covariance matrix. In previous work (e.g., [28]), the empirical covariance matrix was used for whitening in the heavy-tailed regime with good empirical performance; [28] also provided some theoretical analysis to explain this surprising performance. However, their work (both experimental and theoretical) was limited to some very special cases (e.g., only one of the components is heavy-tailed, or there are only two components both with stable distributions without finite second moment).\nWe will show that the above procedure (namely pre-multiplying the data by B := Σ̃−1/2) “works” under considerably more general conditions, namely if (1+γ)-moment exists for γ > 0 for each independent component Si. By “works” we mean that instead\n56\nof whitening the data (that is BA is rotation matrix) it does something slightly weaker but still just as good for the purpose of applying ICA algorithms in the next phase. It orthogonalizes the data, that is now BA is close to a matrix whose columns are orthogonal. In other words, (BA)T (BA) is close to a diagonal matrix (in a sense made precise in Theorem 2.7.1).\nLet X be a real-valued symmetric random variable such that E(|X|1+γ) ≤ M for some M > 1 and 0 < γ < 1. The following lemma from [10] says that the empirical average of the absolute value of X converges to the expectation of |X|. The proof, which we omit, follows an argument similar to the proof of the Chebyshev’s inequality. Let ẼN [|X|] be the empirical average obtained from N independent samples X(1), . . . , X(N), i.e., (|X(1)|+ · · ·+ |X(N)|)/N .\nTheorem 2.7.1 (Orthogonalization via covariance matrix). Let X be given by ICA model X = AS. Assume that there exist t, p,M > 0 and γ ∈ (0, 1) such that for all i we have\n(a) E(|Si|1+γ) ≤M <∞, (b) (normalization) E|Si| = 1, and (c) Pr(|Si| ≥ t) ≥ p. Let x(1), . . . , x(N) be i.i.d. samples according to X. Let Σ̃ =\n(1/N) ∑N\nk=1 x (k)x(k) T and B = Σ̃−1/2. Then for any , δ ∈ (0, 1), ‖(BA)TBA−D‖2 ≤\nfor a diagonal matrix D with diagonal entries d1, . . . , dn satisfying 0 < di, 1/di ≤ max{2/pt2, N4} for all i with probability 1−δ when N ≥ poly(n,M, 1/p, 1/t, 1/ , 1/δ).\nProof idea. For i 6= j we have E(SiSj) = 0 (due to our symmetry assumption on S) and E(|SiSj|) = E(|Si|)E(|Sj|) < ∞. We have (BA)TBA = L−1, where\nL = (1/N) ∑N\nk=1 s (k)s(k)\nT . The off-diagonal entries of L converge to 0: We have\nLi,j = ESiSj = (ESi)(ESj). Now by our assumption that (1 + γ)-moments exist,\n57\nLemma 6 is applicable and implies that empirical average ẼSi tends to the true average ESi as we increase the number of samples. The true average is 0 because of our assumption of symmetry (alternatively, we could just assume that the Xi and hence Si have been centered). The diagonal entries of L are bounded away from 0: This is clear when the second moment is finite, and follows easily by hypothesis (c) when it is not. Finally, one shows that if in L the diagonal entries highly dominate the off-diagonal entries, then the same is true of L−1. Proof. We have (BA)TBA = L−1, where L = (1/N) ∑N\nk=1 s (k)s(k)\nT . By assumption,\nELij = 0 for i 6= j. Note that E|sisj|1+γ ≤M2 and so by Lemma 6, for i 6= j,\nP (|Lij| > 1) ≤ 8M2\n21N γ/3\nwhen N ≥ (8M2 1 ) 1 2 + 1 γ .\nNow let D := diag(L−111 , L −1 22 , . . . , L −1 nn). Then when |Lij| < 1 for all i 6= j, we have\n‖L−D−1‖2 ≤ ‖L−D−1‖F ≤ n 1. The union bound then implies\nP (‖L−D−1‖2 < n 1) ≥ P (‖L−D −1‖F < n 1)\n≥ P (∀i 6= j, |Lij| ≤ 1) ≥ 1− 8n 2M2\n21N γ/3\n(2.29)\nwhen N ≥ (8M2 1 ) 1 2 + 1 γ .\nNext, we aim to bound ‖D‖2 which can be done by writing\n‖D‖2 = 1\nσmin(D−1) =\n1\nmini∈[n] Lii (2.30)\nwhere Lii = (1/N) ∑N k=1 s (k) i 2 . Consider the random variable 1(s2i ≥ t2). We can\ncalculate E ∑\nj 1(s (j) i\n2 ≥ t2) ≥ Np and use a Chernoff bound to see\nP ∑ k∈[N ] 1(s (k) i 2 ≥ t2) ≤ Np 2  ≤ exp(− Np 8 ) (2.31)\n58\nand when ∑\nk∈[N ] 1(s (k) i\n2 ≥ t2) ≥ Np\n2 , we have Lii ≥ t2p/2. Then with probability\nat least 1 − n exp(−Np/8), all entries of D−1 are at least t2p/2. Using this, if N ≥ N1 := (8/p) ln(3n/δ) then ‖D‖2 ≤ 2/pt2 with probability at least 1− δ/3.\nSimilarly, suppose that ‖D‖2 ≤ 2/pt2 and choose 1 = min{ t4p2 4n · 2 , 1 pt2 } and\nN2 := max\n{( 24n2M2\n21δ\n)3/γ , ( 8M2\n1\n) 1 2 + 1 γ }\nso that when N ≥ N2, we have ‖L−D−1‖2 ≤ 1/(2‖D‖2) and ‖L−D−1‖2 ≤ t4p2 /8 with probability at least 1− δ/3. Invoking (2.2), when N ≥ max{N1, N2}, we have\n‖L−1 −D‖2 ≤ 2‖D‖2‖L−D −1‖2 ≤ 2\n4 p2t4 t4p2 8 = (2.32)\nwith probability at least 1− 2δ/3.\nFinally, we upper bound 1/di for a fixed i by using Markov’s inequality:\nP\n( 1\ndi > N5\n) = P (Lii > N 4) = P ( N∑ j s (j) i 2 > N5 ) ≤ NP (S2i > N4) ≤ NP (|Si| > N2)\n≤ N E|Si| N2 = 1 N\n(2.33)\nso that 1/di ≤ N4 for all i with probability at least 1 − δ/3 when N ≥ N3 := n/3δ. Therefore, when N ≥ max{N1, N2, N3}, we have ‖L−1 −D‖2 ≤ , di ≤ 2/pt2, and 1/di ≤ N4 for all i with overall probability at least 1− δ.\nWe used Lemma 1. In Theorem 2.7.1, the diagonal entries are lower bounded, which avoids some degeneracy, but they could still grow quite large because of the heavy tails. This is a real drawback of orthogonalization via covariance. HTICA, using the more sophisticated orthogonalization via centroid body scaling does not have this problem. We can\n59\nsee this in the right table of Figure 2.5, where the condition number of “centroid” is much smaller than the condition number of “covariance.”"
    }, {
      "heading" : "2.7.3 New Membership oracle for the centroid body",
      "text" : "We will now describe and theoretically justify a new and practically efficient - weak membership oracle for ΓX, which is a black-box that can answer approximate membership queries in ΓX. More precisely:\nDefinition 5. The -weak membership problem for K ⊆ Rn is the following: Given a point y ∈ Qn and a rational number > 0, either (i) assert that y ∈ K , or (ii) assert that y 6∈ K− . An -weak membership oracle for K is an oracle that solves the weak membership problem for K. For δ ∈ [0, 1], an ( , δ)-weak membership oracle for K acts as follows: Given a point y ∈ Qn, with probability at least 1− δ it solves the\n-weak membership problem for y,K, and otherwise its output can be arbitrary.\nWe start with an informal description of the algorithm and its correctness. The algorithm implementing the oracle (Subroutine 4) is the following: Let q ∈ Rn be a query point. Let X1, . . . , XN be a sample of random vector X. Given the sample, let Y be uniformly distributed in {X1, . . . , XN}. Output YES if q ∈ ΓY , else output NO.\nIdea of the correctness of the algorithm: If q is not in (ΓX) , then there is a hyperplane separating q from (ΓX) . Let {x : aTx = b} be the hyperplane, satisfying ‖a‖ = 1, aT q > b and aTx ≤ b for every x ∈ (ΓX) . Thus, we have h(ΓX) (a) ≤ b and hΓX(a) ≤ b− . We have\nhΓY (a) = E(|aTY |) = (1/N) N∑ i=1 |aTXi|.\n60\nBy Lemma 6, (1/N) ∑N\ni=1|aTXi| is within of E|aTX| = hΓX(a) ≤ b − when N is\nlarge enough with probability at least 1−δ over the sample X1, . . . , XN . In particular, hΓY (a) ≤ b, which implies q /∈ ΓY and the algorithm outputs NO, with probability at least 1− δ.\nIf q is in (ΓX)− , let y = q + q̂ ∈ ΓX. We will prove the following claim: Informal claim (Lemma 17): For p ∈ ΓX, for large enough N and with probability\nat least 1− δ there is z ∈ ΓY so that ‖z − p‖ ≤ /10.\nThis claim applied to p = y to get z, convexity of ΓY and the fact that ΓY\ncontains B ' σmin(A)Bn2 (Lemma 15) imply that q ∈ conv(B ∪ {z}) ⊆ ΓY and the algorithm outputs YES.\nWe will prove the claim now. Let p ∈ ΓX. By the dual characterization of the centroid body (Proposition 1), there exists a function λ : Rn → R such that p = E(λ(X)X) with −1 ≤ λ ≤ 1. Let z = 1 N ∑N i=1 λ(Xi)Xi. We have EXi(λ(Xi)Xi) = p and EXi(|λ(Xi)Xi|1+γ) ≤ EXi(|Xi|1+γ) ≤ M . By Lemma 6 and a union bound over every coordinate we get P(‖p− z‖ ≥ ) ≤ δ for N large enough.\nFormal Argument\nLemma 15. Let S = (S1, . . . , Sn) ∈ Rn be an absolutely symmetrically distributed random vector such that E(|Si|) = 1 and E(|Si|1+γ) ≤ M <∞ for all i. Let S(i), i = 1, . . . , N be a sample of i.i.d. copies of S. Let Y be a random vector, uniformly distributed in S(1), . . . , S(N). Then (1− ′)Bn1 ⊆ ΓY whenever\nN ≥ ( 16Mn4 ( ′)2 δ′ ) 1 2 + 3 γ .\n61\nProof. From Lemma 5 we know ±ei ∈ ΓS. It is enough to apply Lemma 17 to ±ei with = ′/ √ n and δ = δ′/(2n). This gives, for any θ ∈ Sn−1, hΓY (θ) ≥ hΓS(θ)− ≥ hBn1 (θ)− ≥ (1− √ n )hBn1 (θ) = (1− ′)hBn1 (θ). In particular, ΓY ⊇ (1− ′)Bn1 .\nProposition 1 (Dual characterization of centroid body). Let X be a n-dimensional random vector with finite first moment, that is, for all u ∈ Rn we have E(|〈u,X〉|) < ∞. Then\nΓX = {E ( λ(X)X ) : λ : Rn → [−1, 1] is measurable}. (2.34)\nProof. Let K denote the rhs of the conclusion.We will show that K is a non-empty, closed convex set and show that hK = hΓX , which implies (2.34).\nBy definition, K is a non-empty bounded convex set. To see that it is closed, let (yk)k be a sequence in K such that yk → y ∈ Rn. Let λk be the function associated to yk ∈ K according to the definition of K. Let PX be the distribution of X. We have ‖λk‖L∞(PX) ≤ 1 and, passing to a subsequence kj, (λkj) converges to λ ∈ L∞(PX) in the weak-∗ topology σ(L∞(PX), L1(PX)), where −1 ≤ λ ≤ 1.\n1 This implies limj E(λkj(X)Xi) = limj ∫ Rn λkj(x)xi dPX(x) = ∫ Rn λ(x)xi dPX(x) = E(λ(X)Xi). Thus, we have y = limj ykj = limj E((λkj(X)X) = E(λ(X)X) and K is closed.\nTo conclude, we compute hK and see that it is the same as the definition of hΓX . In the following equations λ ranges over functions such that λ : Rn → R is\n1This is a standard argument, see [?] for the background. Map x 7→ xi is in L1(PX). [?, Theorem 4.13] gives that L1(PX) is a separable Banach space. [?, Theorem 3.16] (Banach-Alaoglu-Bourbaki) gives that the unit ball in L∞(PX) is compact in the weak-* topology. [?, Theorem 3.28] gives that the unit ball in L∞(PX) is metrizable and therefore sequentially compact in the weak-* topology. Therefore, any bounded sequence in L∞(PX) has a convergent subsequence in the weak-* topology.\n62\nBorel-measurable and −1 ≤ λ ≤ 1.\nhK(θ) = sup y∈K 〈y, θ〉\n= sup λ\nE(λ(X)〈X, θ〉)\nand setting λ∗(x) = sgn〈x, θ〉,\n= E(λ∗(X)〈X, θ〉)\n= E(|〈X, θ〉|).\nLemma 16 (LP). Let X be a random vector uniformly distributed in {x(i)}Ni=1 ⊆ Rn. Let q ∈ Rn. Then:\n1. ΓX = 1 N ∑N i=1[−x(i), x(i)].\n2. Point q ∈ ΓX iff there is a solution λ ∈ RN to the following linear feasibility\nproblem:\n1\nN N∑ i=1 λix (i) = q\n− 1 ≤ λi ≤ 1 ∀i. (2.35)\n3. Let λ∗ be the optimal value of (always feasible) linear program\nλ∗ = maxλ\ns.t. 1\nN N∑ i=1 λix (i) = λq − 1 ≤ λi ≤ 1 ∀i\n(2.36)\nwith λ∗ =∞ if the linear program is unbounded. Then the Minkowski functional of ΓX at q is 1/λ∗.\n63\nProof. 1. This is proven in [72]. It is also a special case of Proposition 1. We\ninclude an argument here for completeness. Let K := 1 N ∑N i=1[−x(i), x(i)]. We compute hK to see it is the same as hΓX in the definition of ΓX (Definition 4). As K and ΓX are non-empty compact convex sets, this implies K = ΓX. We have\nhK(y) = sup λi∈[−1,1]\n1\nN N∑ i=1 λix (i) · y\n= max λi∈{−1,1}\n1\nN N∑ i=1 λix (i) · y\n= 1\nN N∑ i=1 |x(i) · y|\n= E(|X · y|).\n2. This follows immediately from part 1.\n3. This follows from part 1 and the definition of Minkowski functional.\nSubroutine 4 Weak Membership Oracle for ΓX Input: Query point q ∈ Rn, samples from symmetric ICA model X = AS, bounds sM ≥ σmax(A), sm ≤ σmin(A), closeness parameter , failure probability δ. Output: ( , δ)-weak membership decision for q ∈ ΓX. 1: Let N = poly(n,M, 1/sm, sM , 1/ , 1/δ). 2: Let (x(i))Ni=1 be an i.i.d. sample of X. 3: Check the feasibility of linear program (2.35). If feasible, output YES, otherwise\noutput NO.\nProposition 2 (Correctness of Subroutine 4). Let X = AS be given by an ICA model such that for all i we have E(|Si|1+γ) ≤ M < ∞, Si is symmetrically distributed and normalized so that E|Si| = 1. Then, given a query point q ∈ Rn,\n64\n> 0, δ > 0, sM ≥ σmax(A), and sm ≤ σmin(A), Subroutine 4 is an -weak membership oracle for q and ΓX with probability 1− δ using time and sample complexity poly(n,M, 1/sm, sM , 1/ , 1/δ).\nProof. Let Y be uniformly random in (x(i))Ni=1. There are two cases corresponding to the guarantees of the oracle:\n• Case q /∈ (ΓX) . Then there is a hyperplane separating q from (ΓX) . Let\n{x ∈ Rn : aTx = b} be the separating hyperplane, parameterized so that a ∈ Rn, b ∈ R, ‖a‖ = 1, aT q > b and aTx ≤ b for every x ∈ (ΓX) . In this case h(ΓX) (a) ≤ b and hΓX(a) ≤ b− . At the same time, hΓY (a) = E(|aTY |) =\n(1/N) ∑N\ni=1|aTx(i)|.\nWe want to apply Lemma 6 to aTX to get that hΓY (a) = (1/N) ∑N i=1|aTx(i)| is within of hΓX(a) = E(|aTX|). For this we need a bound on the (1+γ)-moment of aTX. We use the bound given in 2.11.\n• Case q ∈ (ΓX)− . Let y = q + q̂ = q(1 + ‖q‖). Let α = 1 + ‖q‖ . Then y ∈ ΓX.\nInvoke Lemma 17 for i.i.d. sample (x(i))Ni=1 of X with p = y and equal to some\n1 > 0 to be fixed later to conclude y ∈ (ΓY ) 1 . That is, there exist z ∈ ΓY\nsuch that\n‖z − y‖ ≤ 1. (2.37)\nLet w = z/α. Given (2.37) and the relationships y = αq and z = αw, we have\n‖w − q‖ ≤ ‖z − y‖ ≤ 1. (2.38)\nFrom Lemma 15 with ′ = 1/2 and equivariance of the centroid body (Lemma 3) we get ΓY ⊇ σmin(A) 2 √ n Bn2 . This and convexity of ΓY imply conv{ σmin(A) 2 √ n Bn2 ∪\n65\n{z}} ⊆ ΓY . In particular, the ball around w of radius\nr := ( 1− 1\nα\n) σmin(A)\n2 √ n\nis contained in ΓY . The choice 1 = r ≥ and (2.38) imply q ∈ ΓY and Subroutine 4 outputs YES whenever\nN ≥ ( 8Mn2\nr2δ\n) 1 2 + 1 γ\n.\nTo conclude, remember that q ∈ (ΓX)− . Therefore ‖q‖+ ≤ √ nσmax(A) (from Lemma 5 and equivariance of the centroid body, Lemma 3). This implies\nr = ‖q‖+ σmin(A) 2 √ n\n≥ σmin(A) 2nσmax(A)\nThe claim follows.\nLemma 17. Let X be a n-dimensional random vector such that for all coordinates i we have E(|Xi|1+γ) ≤ M < ∞. Let p ∈ ΓX. Let (X(i))Ni=1 be an i.i.d. sample of X.\nLet Y be uniformly random in (X(i))Ni=1. Let > 0, δ > 0. If N ≥ ( 8Mn2\n2δ\n) 1 2\n+ 3 γ , then,\nwith probability at least 1− δ, p ∈ (ΓY ) .\nProof. By Proposition 1, there exists a measurable function λ : Rn → R, −1 ≤ λ ≤ 1 such that p = E(Xλ(X)). Let\nz = 1\nN N∑ i=1 X(i)λ(X(i)).\nBy Proposition 1, z ∈ ΓY .\nWe have EX(i)(X(i)λ(X(i))) = p and, for every coordinate j,\nEX(i)(|X (i) j λ(X (i))|1+γ) ≤ EX(i)(|X (i) j |1+γ) ≤M.\n66\nBy Lemma 6 and for any fixed coordinate j we have, over the choice of (X(i))Ni=1,\nP(|pj − zj| ≥ / √ n) ≤ 8M\n( / √ n)2Nγ/3\n= 8Mn\n2Nγ/3\nwhenever N ≥ (8M √ n/ ) 1 2 + 1 γ . A union bound over n choices of j gives:\nP(‖p− z‖ ≥ ) ≤ 8Mn 2\n2Nγ/3 .\nSo P(‖p− z‖ ≥ ) ≤ δ whenever\nN ≥ ( 8Mn2\n2δ )3/γ and N ≥ (8M √ n/ ) 1 2 + 1 γ . The claim follows."
    }, {
      "heading" : "2.8 Empirical Study",
      "text" : "In this section, we show experimentally that heavy-tailed data poses a significant challenge for current ICA algorithms, and compare them with HTICA in different settings. We observe some clear situations where heavy-tails seriously affect the standard ICA algorithms, and that these problems are frequently avoided by using the heavy-tailed ICA framework. In some cases, HTICA does not help much, but maintains the same performance of plain FastICA.\nTo generate the synthetic data, we create a simple heavy-tailed density function fη(x) proportional to (|x|+ 1.5)−η, which is symmetric, and for η > 1, fη is the density of a distribution which has finite k < η − 1 moment. The signal S is generated with each Si independently distributed from fηi . The mixing matrix A ∈ Rn×n is generated with each coordinate i.i.d. N (0, 1), columns normalized to unit length. To compare the quality of recovery, the columns of the estimated mixing matrix, Ã are permuted to align with the closest matching column of A, via the Hungarian algorithm. We use\n67\nthe Frobenius norm to measure the error, but all experiments were also performed using the well-known Amari index [6]; the results have similar behavior and are not presented here."
    }, {
      "heading" : "2.8.1 Heavy-tailed ICA when A is orthogonal: Gaussian damp-",
      "text" : "ing and experiments\nFocusing on the third step above, where the mixing matrix already has orthogonal columns, ICA algorithms already suffer dramatically from the presence of heavytailed data. As proposed in [10], Gaussian damping is a preprocessing technique that converts data from an ICA model X = AS, where A is unitary (columns are orthogonal with unit l2-norm) to data from a related ICA model XR = ASR, where R > 0 is a parameter to be chosen. The independent components of SR have finite moments of all orders and so the existing algorithms can estimate A.\nUsing samples of X, we construct the damped random variable XR, with pdf\nρXR(x) ∝ ρX(x) exp(−‖x‖ 2/R2). To normalize the right hand side, we can estimate\nKXR = E exp(−‖X‖ 2/R2)\nso that\nρXR(x) = ρX(x) exp(−‖x‖ 2/R2)/KXR .\nIf x is a realization of XR, then s = A −1x is a realization of the random variable SR and we have that SR has pdf ρSR(s) = ρXR(x). To generate samples from this distribution, we use rejection sampling on samples from ρX . When performing the damping, we binary search over R so that about 25% of the samples are rejected. For more details about the technical requirements for choosing R, see [10].\n68\nFigure 2.1 shows that, when A is already a perfectly orthogonal matrix, but where S may have heavy-tailed coordinates, several standard ICA algorithms perform better after damping the data. In fact, without damping, some do not appear to converge to a correct solution. We compare ICA with and without damping in this case: (1) FastICA using the fourth cumulant (“FastICA - pow3”), (2) FastICA using log cosh (“FastICA - tanh”), (3) JADE, and (4) Second Order Joint Diagonalization as in, e.g., [26] ."
    }, {
      "heading" : "2.8.2 Experiments on synthetic heavy-tailed data",
      "text" : "We now present the results of HTICA using different orthogonalization techniques: (1) Orthogonalization via covariance (Section 2.7.2 (2) Orthogonalization via the centroid body (Section 2.7.1) (3) the ground truth, directly inverting the mixing matrix (oracle), and (4) No orthogonalization, and also no damping (for comparison with plain FastICA) (identity).\n69\nThe “mixed” regime in the left and middle of Figure 2.3 (where some signals are not heavy-tailed) demonstrates a very dramatic contrast between different orthogonalization methods, even when only two heavy-tailed signals are present.\nIn the experiment with different methods of orthogonalization it was observed that when all exponents are the same or very close, orthogonalization via covariance performs better than orthogonalization via centroid and the true mixing matrix as seen in Figure 2.3. A partial explanation is that, given the results in Figure 2.1, we know that equal exponents favor FastICA without damping and orthogonalization (identity in Figure 2.3). The line showing the performance with no orthogonalization\n70\nand no damping (“identity”) behaves somewhat erratically, most likely due the presence of the heavy-tailed samples. Additionally, damping and the choice of parameter R is sensitive to scaling. A scaled-up distribution will be somewhat hurt because fewer samples will survive damping."
    }, {
      "heading" : "2.8.3 ICA on speech data",
      "text" : "While the above study on synthetic data provides interesting situations where heavy-tails can cause problems for ICA, we provide some results here which use realworld data, specifically human speech. To study the performance of HTICA on voice data, we first examine whether the data is heavy-tailed. The motivation to use speech data comes from observations by the signal processing community (e.g. [63]) that speech data can be modeled by α-stable distributions. For an α-stable distribution, with α ∈ (0, 2), only the moments of order less than α will be finite. We present here some results on a data set of human speech according to the standard cocktail party model, from [40].\n71\nThe physical setup of the experiments (the human speakers and microphones) is\nshown in Figure 2.4.\nTo estimate whether the data is heavy-tailed, as in [63], we estimate parameter α of a best-fit α-stable distribution. This estimate is in Figure 2.5 for one of the data sets collected. We can see that the estimated α is clearly in the heavy-tailed regime for some signals.\nUsing data from [40], we perform the same experiment as in Section 2.8.2: generate a random mixing matrix with unit length columns, mix the data, and try to recover the mixing matrix. Although the mixing is synthetic, the setting makes the resulting mixed signals same as real. Specifically, the experiment was conducted in a room with chairs, carpet, plasterboard walls, and windows on one side. There was natural noise including vents, computers, florescent lights, and traffic noise through the windows.\n72\nFigure 2.5 demonstrates that HTICA (orthogonalizing with centroid body scaling, Section 2.7.1) applied to speech data yields some noticeable improvement in the recovery of the mixing matrix, primarily in that it is less susceptible to data that causes FastICA to have large error “spikes.” Moreover, in many cases, running only FastICA on the mixed data failed to even recover all of the speech signals, while HTICA succeeded. In these cases, we had to re-start FastICA until it recovered all the signals.\n73\n74\nChapter 3: LEARNING SIMPLICES\nWe are given uniformly random samples from an unknown convex body in Rn, how many samples are needed to approximately reconstruct the body? It seems intuitively clear, at least for n = 2, 3, that if we are given sufficiently many such samples then we can reconstruct (or learn) the body with very little error. For general n, it is known to require 2Ω( √ n) samples [47] (see also [65] for a similar lower bound in a different but related model of learning). This is an information-theoretic lower bound and no computational considerations are involved. As mentioned in [47], it turns out that if the body has few facets (e.g. polynomial in n), then polynomial in n samples are sufficient for approximate reconstruction. This is an information-theoretic upper bound and no efficient algorithms (i.e., with running time poly(n)) are known. (We remark that to our knowledge the same situation holds for polytopes with poly(n) vertices.) Here, we study the reconstruction problem for the special case when the input bodies are restricted to be (full-dimensional) simplices. We show that in this case one can in fact learn the body efficiently. More precisely, the algorithm knows that the input body is a simplex but only up to an affine transformation, and the problem is to recover this affine transformation. This answers a question of [43, Section 6].\n75\nThe problem of learning a simplex is also closely related to the well-studied problem of learning intersections of half-spaces. Suppose that the intersection of n + 1 half-spaces in Rn is bounded, and we are given poly(n) uniformly random samples from it. Then our learning simplices result directly implies that we can learn the n+ 1 half-spaces. This also has the advantage of being a proper learning algorithm, meaning that the output of the algorithm is a set of n + 1 half-spaces, unlike many of the previous algorithms.\nPrevious work. Perhaps the first approach to learning simplices that comes to mind is to find a minimum volume simplex containing the samples. This can be shown to be a good approximation to the original simplex. (Such minimum volume estimators have been studied in machine learning literature, see e.g. [94] for the problem of estimating the support of a probability distribution. We are not aware of any technique that applies to our situation and provides theoretical guarantees.) However, the problem of finding a minimum volume simplex is in general NP-hard [79]. This hardness is not directly applicable for our problem because our input is a random sample and not a general point set. Nevertheless, we do not have an algorithm for directly finding a minimum volume simplex; instead we use ideas similar to those used in Independent Component Analysis (ICA). ICA studies the following problem: Given a sample from an affine transformation of a random vector with independently distributed coordinates, recover the affine transformation (up to some unavoidable ambiguities). [43] gave an efficient algorithm for this problem (with some restrictions on the allowed distributions, but also with some weaker requirements than full independence) along with most of the details of a rigorous analysis (a complete analysis\n76\nof a special case can be found in [13]; see also [106] for a generalization of ICA to subspaces along with a rigorous analysis). The problem of learning parallelepipeds from uniformly random samples is a special case of this problem. [43] asked if one could learn other convex bodies, and in particular simplices, efficiently from uniformly random samples. [76] gave a simpler and rigorous algorithm and analysis for the case of learning parallelepipeds with similarities to the popular FastICA algorithm of [56]. The algorithm in [76] is a first order algorithm unlike Frieze et al.’s second order algorithm.\nThe algorithms in both [43, 76] make use of the fourth moment function of the probability distribution. Briefly, the fourth moment in direction u ∈ Rn is E(u ·X)4, where X ∈ Rn is the random variable distributed according to the input distribution. The moment function can be estimated from the samples. The independent components of the distribution correspond to local maxima or minima of the moment function, and can be approximately found by finding the local maxima/minima of the moment function estimated from the sample.\nMore information on ICA including historical remarks can be found in [58, 34]. Ideas similar to ICA have been used in statistics in the context of projection pursuit since the mid-seventies. It is not clear how to apply ICA to the simplex learning problem directly as there is no clear independence among the components. Let us note that [43] allow certain kinds of dependencies among the components, however this does not appear to be useful for learning simplices.\nLearning intersections of half-spaces is a well-studied problem in learning theory. The problem of PAC-learning intersections of even two half-spaces is open, and there is evidence that it is hard at least for sufficiently large number of half-spaces: E.g.,\n77\n[67] prove that learning intersections of n half-spaces in Rn (for constant > 0) is hard under standard cryptographic assumptions (PAC-learning is possible, however, if one also has access to a membership oracle in addition to random samples [68]). Because of this, much effort has been expended on learning when the distribution of random samples is some simple distribution, see e.g. [66, 104, 105] and references therein. This line of work makes substantial progress towards the goal of learning intersections of k half-spaces efficiently, however it falls short of being able to do this in time polynomial in k and n; in particular, these algorithms do not seem to be able to learn simplices. The distribution of samples in these works is either the Gaussian distribution or the uniform distribution over a ball. [43] and [47] consider the uniform distribution over the intersection. Note that this requires that the intersection be bounded. Note also that one only gets positive samples in this case unlike other work on learning intersections of half-spaces. The problem of learning convex bodies can also be thought of as learning a distribution or density estimation problem for a special class of distributions.\n[49] show how to reconstruct a polytope with N vertices in Rn, given its first O(nN) moments in (n + 1) random directions. In our setting, where we have access to only a polynomial number of random samples, it’s not clear how to compute moments of such high orders to the accuracy required for the algorithm of [49] even for simplices.\nA recent and parallel work of [7] is closely related to ours. They show that tensor decomposition methods can be applied to low-order moments of various latent variable models to estimate their parameters. The latent variable models considered by them include Gaussian mixture models, hidden Markov models and latent Dirichlet\n78\nallocations. The tensor methods used by them and the local optima technique we use seem closely related. One could view our work, as well as theirs, as showing that the method of moments along with existing algorithmic techniques can be applied to certain unsupervised learning problems.\nIn Section 3.6, we give the theorems and proofs which allow the efficient reduction\nto ICA.\nOur results For clarity of the presentation, we use the following machine model for the running time: a random access machine that allows the following exact arithmetic operations over real numbers in constant time: addition, subtraction, multiplication, division and square root.\nThe estimation error is measured using total variation distance, denoted dTV (see\nSection 3.1).\nTheorem 3.0.1. There is an algorithm (Algorithm 2 below) such that given access to random samples from a simplex SINPUT ⊆ Rn, with probability at least 1− δ over the sample and the randomness of the algorithm, it outputs n + 1 vectors that are the vertices of a simplex S so that dTV (S, SINPUT ) ≤ . The algorithm runs in time polynomial in n, 1/ and 1/δ.\nAs mentioned earlier, our algorithm uses ideas from ICA. Our algorithm uses the third moment instead of the fourth moment used in certain versions of ICA. The third moment is not useful for learning symmetric bodies such as the cube as it is identically 0. It is however useful for learning a simplex where it provides useful information, and is easier to handle than the fourth moment. One of the main contributions of our work is the understanding of the third moment of a simplex and the structure\n79\nof local maxima. This is more involved than in previous work as the simplex has no obvious independence structure, and the moment polynomial one gets has no obvious structure unlike for ICA.\nThe probability of success of the algorithm can be “boosted” so that the dependence of the running time on δ is only linear in log(1/δ) as follows: The following discussion uses the space of simplices with total variation distance as the underlying metric space. Let be the target distance. Take an algorithm that succeeds with probability 5/6 and error parameter ′ to be fixed later (such as Algorithm 2 with δ = 1/6). Run the algorithm t = O(log 1/δ) times to get t simplices. By a Chernoff-type argument, at least 2t/3 simplices are within ′ of the input simplex with probability at least 1− δ/2.\nBy sampling, we can estimate the distances between all pairs of simplices with additive error less than ′/10 in time polynomial in t, 1/ ′ and log 1/δ so that all estimates are correct with probability at least 1 − δ/2. For every output simplex, compute the number of output simplices within estimated distance (2+1/10) ′. With probability at least 1 − δ both of the desirable events happen, and then necessarily there is at least one output simplex, call it S, that has 2t/3 output simplices within estimated distance (2 + 1/10) ′. Any such S must be within (3 + 2/10) ′ of the input simplex. Thus, set ′ = /(3 + 2/10).\nWhile our algorithm for learning simplices uses techniques for ICA, we have to do substantial work to make those techniques work for the simplex problem. We also show a more direct connection between the problem of learning a simplex and ICA: a randomized reduction from the problem of learning a simplex to ICA. The connection is based on a known representation of the uniform measure on a simplex as\n80\na normalization of a vector having independent coordinates. Similar representations are known for the uniform measure in an n-dimensional `p ball (denoted ` n p ) [16] and the cone measure on the boundary of an `np ball [92, 83, 98] (see Section 3.1 for the definition of the cone measure). These representations lead to a reduction from the problem of learning an affine transformation of an `np ball to ICA. These reductions show connections between estimation problems with no obvious independence structure and ICA. They also make possible the use of any off-the-shelf implementation of ICA. However, the results here do not supersede our result for learning simplices because to our knowledge no rigorous analysis is available for the ICA problem when the distributions are the ones in the above reductions.\nIdea of the algorithm. The new idea for the algorithm is that after putting the samples in a suitable position (see below), the third moment of the sample can be used to recover the simplex using a simple FastICA-like algorithm. We outline our algorithm next.\nAs any full-dimensional simplex can be mapped to any other full-dimensional simplex by an invertible affine transformation, it is enough to determine the translation and linear transformation that would take the given simplex to some canonical simplex. As is well-known for ICA-like problems (see, e.g., [43]), this transformation can be determined up to a rotation from the mean and the covariance matrix of the uniform distribution on the given simplex. The mean and the covariance matrix can be estimated efficiently from a sample. A convenient choice of an n-dimensional simplex is the convex hull of the canonical vectors in Rn+1. We denote this simplex ∆n and call it the standard simplex. So, the algorithm begins by picking an arbitrary invertible\n81\naffine transformation T that maps Rn onto the hyperplane {x ∈ Rn+1 : 1 · x = 1}. We use a T so that T−1(∆n) is an isotropic 2 simplex. In this case, the algorithm brings the sample set into isotropic position and embeds it in Rn+1 using T . After applying these transformations we may assume (at the cost of small errors in the final result) that our sample set is obtained by sampling from an unknown rotation of the standard simplex that leaves the all-ones vector (denoted 1 from now on) invariant (thus this rotation keeps the center of mass of the standard simplex fixed), and the problem is to recover this rotation.\nTo find the rotation, the algorithm will find the vertices of the rotated simplex approximately. This can be done efficiently because of the following characterization of the vertices: Project the vertices of the simplex onto the hyperplane through the origin orthogonal to 1 and normalize the resulting vectors. Let V denote this set of n + 1 points. Consider the problem of maximizing the third moment of the uniform distribution in the simplex along unit vectors orthogonal to 1. Then V is the complete set of local maxima and the complete set of global maxima (Theorem 3.5.1). A fixed point-like iteration (inspired by the analysis of FastICA [56] and of gradient descent in [76]) starting from a random point in the unit sphere finds a local maximum efficiently with high probability. By the analysis of the coupon collector’s problem, O(n log n) repetitions are highly likely to find all local maxima.\nIdea of the analysis. In the analysis, we first argue that after putting the sample in isotropic position and mapping it through T , it is enough to analyze the algorithm in the case where the sample comes from a simplex S that is close to a simplex S ′ that is the result of applying a rotation leaving 1 invariant to the standard simplex.\n2See Section 3.1.\n82\nThe closeness here depends on the accuracy of the sample covariance and mean as an estimate of the input simplex’s covariance matrix and mean. A sample of size O(n) guarantees ([3, Theorem 4.1], [100, Corollary 1.2]) that the covariance and mean are close enough so that the uniform distributions on S and S ′ are close in total variation. We show that the subroutine that finds the vertices (Subroutine 5), succeeds with some probability when given a sample from S ′. By definition of total variation distance, Subroutine 5 succeeds with almost as large probability when given a sample from S (an argument already used in [76]). As an additional simplifying assumption, it is enough to analyze the algorithm (Algorithm 2) in the case where the input is isotropic, as the output distribution of the algorithm is equivariant with respect to affine invertible transformations as a function of the input distribution."
    }, {
      "heading" : "3.1 Preliminaries",
      "text" : "An n-simplex is the convex hull of n+1 points in Rn that do not lie on an (n−1)dimensional affine hyperplane. It will be convenient to work with the standard nsimplex ∆n living in Rn+1 defined as the convex hull of the n+1 canonical unit vectors e1, . . . , en+1; that is\n∆n = {(x0, . . . , xn) ∈ Rn+1 : x0 + · · ·+ xn = 1 and xi ≥ 0 for all i}.\nThe canonical simplex Ωn living in Rn is given by\n{(x0, . . . , xn−1) ∈ Rn : x0 + · · ·+ xn−1 ≤ 1 and xi ≥ 0 for all i}.\nNote that ∆n is the facet of Ωn+1 opposite to the origin.\nLet Bn denote the n-dimensional Euclidean ball.\n83\nThe complete homogeneous symmetric polynomial of degree d in variables u0, . . . , un,\ndenoted hn(u0, . . . , un), is the sum of all monomials of degree d in the variables:\nhd(u0, . . . , un) = ∑\nk0+···+kn=d\nuk00 · · ·uknn = ∑\n0≤i0≤i1≤···≤id≤n\nui0ui1 · · ·uid .\nAlso define the d-th power sum as\npd(u0, . . . , un) = u d 0 + . . .+ u d n.\nFor a vector u = (u0, u1, . . . , un), we define\nu(2) = (u20, u 2 1, . . . , u 2 n).\nVector 1 denotes the all ones vector (the dimension of the vector will be clear from the context).\nA random vector X ∈ Rn is isotropic if E(X) = 0 and E(XXT ) = I. A compact set in Rn is isotropic if a uniformly distributed random vector in it is isotropic. The\ninradius of an isotropic n-simplex is √ (n+ 2)/n, the circumradius is √ n(n+ 2).\nThe total variation distance between two probability measures is dTV (µ, ν) = supA|µ(A) − ν(A)| for measurable A. For two compact sets K,L ⊆ Rn, we define the total variation distance dTV (K,L) as the total variation distance between the corresponding uniform distributions on each set. It can be expressed as\ndTV (K,L) =\n{ volK\\L\nvolK if volK ≥ volL,\nvolL\\K volL if volL > volK.\nThis identity implies the following elementary estimate:\nLemma 18. Let K,L be two compact sets in Rn. Let 0 < α ≤ 1 ≤ β such that αK ⊆ L ⊆ βK. Then dTV (K,L) ≤ 2 (1− (α/β)n).\n84\nProof. We have dTV (αK, βK) = 1 − (α/β)n. Triangle inequality implies the desired inequality.\nLemma 19. Consider the coupon collector’s problem with n coupons where every coupon occurs with probability at least α. Let δ > 0. Then with probability at least 1− δ all coupons are collected after α−1(log n+ log 1/δ) trials.\nProof. The probability that a particular coupon is not collected after that many trials is at most\n(1− α)α−1(logn+log 1/δ) ≤ e− logn−log 1/δ = δ/n.\nThe union bound over all coupons implies the claim.\nFor a point x ∈ Rn, ‖x‖p = ( ∑n i=1 |xi|p)1/p is the standard `p norm. The unit `np\nball is defined by\nBnp = {x ∈ Rn : ‖x‖p ≤ 1}.\nThe Gamma distribution is denoted as Gamma(α, β) and has density f(x;α, β) =\nβα\nΓ(α) xα−1e−βx1x≥0, with shape parameters α, β > 0. Gamma(1, λ) is the exponential distribution, denoted Exp(λ). The Gamma distribution also satisfies the following additivity property: If X is distributed as Gamma(α, β) and Y is distributed as Gamma(α′, β), then X + Y is distributed as Gamma(α + α′, β).\nThe cone measure on the surface ∂K of centrally symmetric convex body K in\nRn [16, 92, 83, 98] is defined by\nµK(A) = vol(ta; a ∈ A, 0 ≤ t ≤ 1)\nvol(K) .\nIt is easy to see that µBnp is uniform on ∂B n p for p ∈ {1, 2,∞}.\nFrom [92] and [83] we have the following representation of the cone measure on\n∂Bnp :\n85\nTheorem 3.1.1. Let G1, G2, . . . , Gn be iid random variables with density proportional to exp(−|t|p). Then the random vector X = G/‖G‖p is independent of ‖G‖p. Moreover, X is distributed according to µBnp .\nFrom [16], we also have the following variation, a representation of the uniform\ndistribution in Bnp :\nTheorem 3.1.2. Let G = (G1, . . . , Gn) be iid random variables with density proportional to exp(−|t|p). Let Z be a random variable distributed as Exp(1), independent of G. Then the random vector\nV = G(∑n i=1|Gi|p + Z )1/p\nis uniformly distributed in Bnp .\nSee e.g. [20, Section 20] for the change of variable formula in probability."
    }, {
      "heading" : "3.2 Computing the moments of a simplex",
      "text" : "The k-th moment mk(u) over ∆ n is the function\nu 7→ EX∈∆n((u ·X)k).\nIn this section we present a formula for the moment over ∆n. Similar more general formulas appear in [69]. We will use the following result from [51] for αi ≥ 0:∫ Ωn+1 xα00 · · ·xαnn dx = α0! · · ·αn! (n+ 1 + ∑ i αi)! .\nFrom the above we can easily derive a formula for integration over ∆n:∫ ∆n xα00 · · ·xαnn dx = √ n+ 1 · α0! · · ·αn! (n+ ∑ i αi)! .\n86\nNow ∫ ∆n (x0u0 + . . .+ xnun) kdx\n= ∑\nk0+···+kn=k\n( k\nk0!, . . . , kn!\n) uk00 . . . u kn n ∫ ∆n xk00 . . . x kn n dx\n= ∑\nk0+···+kn=k\n( k\nk0!, . . . , kn!\n) uk00 u k0 0 . . . u kn n √ n+ 1 · k0! . . . kn! (n+ ∑ i ki)!\n= k! √ n+ 1\n(n+ k)! ∑ k0+···+kn=k uk00 . . . u kn n\n= k! √ n+ 1\n(n+ k)! hk(u).\nThe variant of Newton’s identities for the complete homogeneous symmetric polynomial gives the following relations which can also be verified easily by direct computation:\n3h3(u) = h2(u)p1(u) + h1(u)p2(u) + p3(u),\n2h2(u) = h1(u)p1(u) + p2(u) = p1(u) 2 + p2(u).\nDivide the above integral by the volume of the standard simplex |∆n| = √ n+ 1/n!\nto get the moment:\nm3(u) = 3! √ n+ 1\n(n+ 3)! h3(u)/|∆n|\n= 2(h2(u)p1(u) + h1(u)p2(u) + p3(u))\n(n+ 1)(n+ 2)(n+ 3)\n= (p1(u)\n3 + 3p1(u)p2(u) + 2p3(u))\n(n+ 1)(n+ 2)(n+ 3) ."
    }, {
      "heading" : "3.3 Subroutine for finding the vertices of a rotated standard",
      "text" : "simplex\nIn this section we solve the following simpler problem: Suppose we have poly(n) samples from a rotated copy S of the standard simplex, where the rotation is such\n87\nthat it leaves 1 invariant. The problem is to approximately estimate the vertices of the rotated simplex from the samples.\nWe will analyze our algorithm in the coordinate system in which the input simplex is the standard simplex. This is only for convenience in the analysis and the algorithm itself does not know this coordinate system.\nAs we noted in the introduction, our algorithm is inspired by the algorithm of [76] for the related problem of learning hypercubes and also by the FastICA algorithm in [56]. New ideas are needed for our algorithm for learning simplices; in particular, our update rule is different. With the right update rule in hand the analysis turns out to be quite similar to the one in [76].\nWe want to find local maxima of the sample third moment. A natural approach to do this would be to use gradient descent or Newton’s method (this was done in [43]). Our algorithm, which only uses first order information, can be thought of as a fixed point algorithm leading to a particularly simple analysis and fast convergence. Before stating our algorithm we describe the update rule we use.\nWe will use the abbreviation Cn = (n + 1)(n + 2)(n + 3)/6. Then, from the\nexpression for m3(u) we get\n∇m3(u) = 1\n6Cn\n( 3p1(u) 2 1 + 3p2(u)1 + 6p1(u)u+ 6u (2) ) .\nSolving for u(2) we get\nu(2) = Cn∇m3(u)− 1\n2 p1(u)\n2 1− 1\n2 p2(u)1− p1(u)u\n= Cn∇m3(u)− 1 2 (u · 1)21− 1 2 (u · u)21− (u · 1)u. (3.1)\nWhile the above expressions are in the coordinate system where the input simplex is the canonical simplex, the important point is that all terms in the last expression\n88\ncan be computed in any coordinate system that is obtained by a rotation leaving 1 invariant. Thus, we can compute u(2) as well independently of what coordinate system we are working in. This immediately gives us the algorithm below. We denote by m̂3(u) the sample third moment, i.e., m̂3(u) = 1 t ∑t i=1(u · ri)3 for t samples. This is a polynomial in u, and the gradient is computed in the obvious way. Moreover, the gradient of the sample moment is clearly an unbiased estimator of the gradient of the moment; a bound on the deviation is given in the analysis (Lemma 20). For each evaluation of the gradient of the sample moment, we use a fresh sample.\nIt may seem a bit alarming that the fixed point-like iteration is squaring the coordinates of u, leading to an extremely fast growth (see Equation 3.1 and Subroutine 1). But, as in other algorithms having quadratic convergence like certain versions of Newton’s method, the convergence is very fast and the number of iterations is small. We show below that it is O(log(n/δ)), leading to a growth of u that is polynomial in n and 1/δ. The boosting argument described in the introduction makes the final overall dependence in δ to be only linear in log(1/δ).\nWe state the following subroutine for Rn instead of Rn+1 (thus it is learning a rotated copy of ∆n−1 instead of ∆n). This is for notational convenience so that we work with n instead of n+ 1.\nLemma 20. Let c > 0 be a constant, n > 20, and 0 < δ < 1. Suppose that Subroutine 1 uses a sample of size t = 217n2c+22(1 δ )2 ln 2n 5r δ for each evaluation of the gradient and runs for r = log 4(c+3)n 2 lnn\nδ iterations. Then with probability at least 1− δ Subroutine\n1 outputs a vector within distance 1/nc from a vertex of the input simplex. With respect of the process of picking a sample and running the algorithm, each vertex is equally likely to be the nearest.\n89\nSubroutine 5 Find one vertex of a rotation of the standard simplex ∆n−1 via a fixed point iteration-like algorithm\nInput: Samples from a rotated copy of the n-dimensional standard simplex (for a rotation that leaves 1 invariant). Output: An approximation to a uniformly random vertex of the input simplex. Pick u(1) ∈ Sn−1, uniformly at random. for i = 1 to r do\nu(i+ 1) :=Cn−1∇m̂3(u(i))− 1 2 (u(i) · 1)21− 1 2 (u(i) · u(i))21− (u(i) · 1)u(i).\nNormalize u(i+ 1) by dividing by ‖u(i+ 1)‖2. end for Output u(r + 1).\nNote that if we condition on the sample, different vertices are not equally likely over the randomness of the algorithm. That is, if we try to find all vertices running the algorithm multiple times on a fixed sample, different vertices will be found with different likelihoods.\nProof. Our analysis has the same outline as that of [76]. This is because the iteration that we get is the same as that of [76] except that cubing is replaced by squaring (see below); however some details in our proof are different. In the proof below, several of the inequalities are quite loose and are so chosen to make the computations simpler.\nWe first prove the lemma assuming that the gradient computations are exact and then show how to handle samples. We will carry out the analysis in the coordinate system where the given simplex is the standard simplex. This is only for the purpose of the analysis, and this coordinate system is not known to the algorithm. Clearly, u(i+ 1) = (u(i)21, . . . , u(i) 2 n). It follows that,\nu(i+ 1) = (u(1)2 i 1 , . . . , u(1) 2i n ).\n90\nNow, since we choose u(1) randomly, with probability at least (1− (n2− n)δ′) one of the coordinates of u(1) is greater than all the other coordinates in absolute value by a factor of at least (1 + δ′), where 0 < δ′ < 1. (A similar argument is made in [76] with different parameters. We briefly indicate the proof for our case: The probability that the event in question does not happen is less than the probability that there are two coordinates u(1)a and u(1)b such that their absolute values are within factor 1+δ′, i.e. 1/(1+δ′) ≤ |u(1)a|/|u(1)b| < 1+δ′. The probability that for given a, b this event happens can be seen as the Gaussian area of the four sectors (corresponding to the four choices of signs of u(1)a, u(1)b) in the plane each with angle less than 2δ′. By symmetry, the Gaussian volume of these sectors is 2δ′/(π/2) < 2δ′. The\nprobability that such a pair (a, b) exists is less than 2 ( n 2 ) δ′.) Assuming this happens, then after r iterations, the ratio between the largest coordinate (in absolute value) and the absolute value of any other coordinate is at least (1 + δ′)2 r . Thus, one of the coordinates is very close to 1 and others are very close to 0, and so u(r + 1) is very close to a vertex of the input simplex.\nNow we drop the assumption that the gradient is known exactly. For each evaluation of the gradient we use a fresh subset of samples of t points. Here t is chosen so that each evaluation of the gradient is within `2-distance 1/n c1 from its true value with probability at least 1 − δ′′, where c1 will be set at the end of the proof. An application of the Chernoff bound yields that we can take t = 200n2c1+4 ln 2n 3\nδ′′ ; we\nomit the details. Thus all the r evaluations of the gradient are within distance 1/nc1 from their true values with probability at least 1− rδ′′.\nWe assumed that our starting vector u(1) has a coordinate greater than every other coordinate by a factor of (1+δ′) in absolute value; let us assume without loss of\n91\ngenerality that this is the first coordinate. Hence |u(1)1| ≥ 1/ √ n. When expressing u(2) in terms of the gradient, the gradient gets multiplied by Cn−1 < n 3 (we are assuming n > 20), keeping this in mind and letting c2 = c1 − 3 we get for j 6= 1\n|u(i+ 1)1| |u(i+ 1)j| ≥ u(i) 2 1 − 1/nc2 u(i)2j + 1/n c2 ≥ u(i) 2 1(1− n−(c2−1)) u(i)2j + 1/n c2 .\nIf u(i)2j > 1/n c2−c3 , where 1 ≤ c3 ≤ c2 − 2 will be determined later, then we get\n|u(i+ 1)1|/|u(i+ 1)j| > 1− 1/nc2−1 1 + 1/nc3 · ( u(i)1 u(i)j )2 > (1− 1/nc3)2\n( u(i)1 u(i)j )2 . (3.2)\nElse,\n|u(i+ 1)1|/|u(i+ 1)j| > 1/n− 1/nc2\n1/nc2−c3 + 1/nc2\n> ( 1− 1\nnc3\n)2 · nc2−c3−1\n> 1\n2 nc2−c3−1,\nwhere we used c3 ≥ 1 and n > 20 in the last inequality.\nWe choose c3 so that ( 1− 1\nnc3\n)2 (1 + δ′) > (1 + δ′/2). (3.3)\nFor this, δ′ ≥ 32/nc3 or equivalently c3 ≥ (ln (32/δ′))/ lnn suffices.\nFor c3 satisfying (3.3) we have (1− 1nc3 ) 2(1+δ′)2 > (1+δ′). It then follows from (3.2) that the first coordinate continues to remain the largest in absolute value by a factor of at least (1 + δ′) after each iteration. Also, once we have |u(i)1|/|u(i)j| > 12n c2−c3−1, we have |u(i′)1|/|u(i′)j| > 12n c2−c3−1 for all i′ > i.\n92\n(3.2) gives that after r iterations we have\n|u(r + 1)1| |u(r + 1)j| > (1− 1/nc3)2+22+...+2r ( u(1)1 u(1)j )2r ≥ (1− 1/nc3)2r+1−2(1 + δ′)2r .\nNow if r is such that (1−1/nc3)2r+1−2(1+δ′)2r > 1 2 nc2−c3−1, we will be guaranteed that |u(r + 1)1|/|u(r + 1)j| > 12n c2−c3−1. This condition is satisfied if we have (1 − 1/nc3)2 r+1 (1 + δ′)2 r > 1\n2 nc2−c3−1, or equivalently ((1 − 1/nc3)2(1 + δ′))2r ≥ 1 2 nc2−c3−1.\nNow using (3.3) it suffices to choose r so that (1 + δ′/2)2 r ≥ 1 2 nc2−c3−1. Thus we can take r = log(4(c2 − c3)(lnn)/δ′).\nHence we get |u(r + 1)1|/|u(r + 1)j| > 12n c2−c3−1. It follows that for u(r + 1), the `2-distance from the vertex (1, 0, . . . , 0) is at most 8/n c2−c3−2 < 1/nc2−c3−3 for n > 20; we omit easy details.\nNow we set our parameters: c3 = 1 + (ln(32/δ ′)/ lnn) and c2 − c3 − 3 = c and c1 = c2 + 3 = 7 + c + ln(32/δ ′)/ lnn satisfies all the constraints we imposed on c1, c2, c3. Choosing δ ′′ = δ′/r, we get that the procedure succeeds with probability at least 1 − (n2 − n)δ′ − rδ′′ > 1 − n2δ′. Now setting δ′ = δ/n2 gives the overall probability of error δ, and the number of samples and iterations as claimed in the lemma."
    }, {
      "heading" : "3.4 Learning simplices",
      "text" : "In this section we give our algorithm for learning general simplices, which uses the subroutine from the previous section. The learning algorithm uses an affine map T : Rn → Rn+1 that maps some isotropic simplex to the standard simplex. We describe now a way of constructing such a map: Let A be a matrix having as columns an orthonormal basis of 1⊥ in Rn+1. To compute one such A, one can start with the\n93\n(n+1)-by-(n+1) matrix B that has ones in the diagonal and first column, everything else is zero. Let QR = B be a QR-decomposition of B. By definition we have that the first column of Q is parallel to 1 and the rest of the columns span 1⊥. Given this, let A be the matrix formed by all columns of Q except the first. We have that the set {AT ei} is the set of vertices of a regular n-simplex. Each vertex is at distance√( 1− 1\nn+ 1\n)2 +\nn\n(n+ 1)2 =\n√ n\nn+ 1\nfrom the origin, while an isotropic simplex has vertices at distance √ n(n+ 2) from the origin. So an affine transformation that maps an isotropic simplex in Rn to the standard simplex in Rn+1 is T (x) = 1√ (n+1)(n+2) Ax+ 1 n+1 1n+1.\nTo simplify the analysis, we pick a new sample r(1), . . . , r(t3) to find every vertex, as this makes every vertex equally likely to be found when given a sample from an isotropic simplex. (The core of the analysis is done for an isotropic simplex; this is enough as the algorithm’s first step is to find an affine transformation that puts the input simplex in approximately isotropic position. The fact that this approximation is close in total variation distance implies that it is enough to analyze the algorithm for the case of exact isotropic position, the analysis carries over to the approximate case with a small loss in the probability of success. See the proof below for the details.) A practical implementation may prefer to select one such sample outside of the for loop, and find all the vertices with just that sample—an analysis of this version would involve bounding the probability that each vertex is found (given the sample, over the choice of the starting point of gradient descent) and a variation of the coupon collector’s problem with coupons that are not equally likely.\n94\nAlgorithm 2 Learning a simplex.\nInput: Error parameter > 0. Probability of failure parameter δ > 0. Oracle access to random points from some n-dimensional simplex SINPUT . Output: V = {v(1), . . . , v(n + 1)} ⊆ Rn (approximations to the vertices of the simplex). Estimate the mean and covariance using t1 = poly(n, 1/ , 1/δ) samples p(1), . . . , p(t1):\nµ = 1\nt1 ∑ i p(i),\nΣ = 1\nt1 ∑ i (p(i)− µ)(p(i)− µ)T .\nCompute a matrix B so that Σ = BBT (say, Cholesky decomposition). Let U = ∅. for i = 1 to m (with m = poly(n, log 1/δ)) do\nGet t3 = poly(n, 1/ , log 1/δ) samples r(1), . . . r(t3) and use µ,B to map them to samples s(i) from a nearly-isotropic simplex: s(i) = B−1(r(i)− µ).\nEmbed the resulting samples in Rn+1 as a sample from an approximately rotated standard simplex: Let l(i) = T (s(i)).\nInvoke Subroutine 5 with sample l(1), . . . , l(t3) to get u ∈ Rn+1. Let ũ be the nearest point to u in the affine hyperplane {x : x · 1 = 1}. If ũ is\nnot within 1/ √ 2 of a point in U , add ũ to U . (Here 1/ √\n2 is half of the edge length of the standard simplex.) end for Let\nV = BT−1(U) + µ = √ (n+ 1)(n+ 2)BAT ( U − 1\nn+ 1 1\n) + µ.\n95\nTheorem 3.0.1. As a function of the input simplex, the distribution of the output of the algorithm is equivariant under invertible affine transformations. Namely, if we apply an affine transformation to the input simplex, the distribution of the output is equally transformed.3 The notion of error, total variation distance, is also invariant under invertible affine transformations. Therefore, it is enough to analyze the algorithm when the input simplex is in isotropic position. In this case ‖p(i)‖ ≤ n + 1 (see Section 2.2) and we can set t1 ≤ poly(n, 1/ ′, log(1/δ)) so that ‖µ‖ ≤ ′ with probability at least 1 − δ/10 (by an easy application of Chernoff’s bound), for some ′ to be fixed later. Similarly, using results from [3, Theorem 4.1], a choice of t1 ≤ n ′−2 polylog(1/ ′) polylog(1/δ) implies that the empirical second moment matrix\nΣ̄ = 1\nt1 ∑ i p(i)p(i)T\nsatisfies ‖Σ̄− I‖ ≤ ′ with probability at least 1− δ/10. We have Σ = Σ̄− µµT and this implies ‖Σ− I‖ ≤ ‖Σ̄− I‖ + ‖µµT‖ ≤ 2 ′. Now, s(1), . . . , s(t3) is an iid sample from a simplex S ′ = B−1(SINPUT −µ). Simplex S ′ is close in total variation distance to some isotropic simplex4 SISO. More precisely, Lemma 21 below shows that\ndTV (S ′, SISO) ≤ 12n ′, (3.4)\nwith probability at least 1− δ/5.\n3To see this: the equivariance of the algorithm as a map between distributions is implied by the equivariance of the algorithm on any given input sample. Now, given the input sample, if we apply an affine transformation to it, this transformation is undone except possibly for a rotation by the step s(i) = B−1(r(i)− µ). A rotation may remain because of the ambiguity in the characterization of B. But the steps of the algorithm that follow the definition of s(i) are equivariant under rotation, and the ambiguous rotation will be removed at the end when B is applied again in the last step.\n4The isotropic simplex SISO will typically be far from the (isotropic) input simplex, because of the ambiguity up to orthogonal transformations in the characterization of B.\n96\nAssume for a moment that s(1), . . . , s(t3) are from SISO. The analysis of Subroutine 5 (fixed point-like iteration) given in Lemma 20 would guarantee the following: Successive invocations to Subroutine 5 find approximations to vertices of T (SISO) within Euclidean distance ′′ for some ′′ to be determined later and t3 = poly(n, 1/ ′′, log 1/δ). We ask for each invocation to succeed with probability at least 1− δ/(20m) with m = n(log n+ log 20/δ). Note that each vertex is equally likely to be found. The choice of m is so that, if all m invocations succeed (which happens with probability at least 1− δ/20), then the analysis of the coupon collector’s problem, Lemma 19, implies that we fail to find a vertex with probability at most δ/20. Overall, we find all vertices with probability at least 1− δ/10.\nBut in reality samples s(1), . . . , s(t3) are from S ′, which is only close to SISO. The\nestimate from (3.4) with appropriate ′ = poly(1/n, ′′, δ) gives\ndTV (S ′, SISO) ≤\nδ\n10\n1\nt3m ,\nwhich implies that the total variation distance between the joint distribution of all t3m samples used in the loop and the joint distribution of actual samples from the isotropic simplex SISO is at most δ/10, and this implies that the loop finds approximations to all vertices of T (SISO) when given samples from S ′ with probability at least 1− δ/5. The points in U are still within Euclidean distance ′′ of corresponding vertices of T (SISO).\nTo conclude, we turn our estimate of distances between estimated and true vertices into a total variation estimate, and map it back to the input simplex. Let S ′′ = conv T−1U . As T maps an isotropic simplex to a standard simplex, we have that√ (n+ 1)(n+ 2)T is an isometry, and therefore the vertices of S ′′ are within distance\n′′/ √\n(n+ 1)(n+ 2) of the corresponding vertices of SISO. Thus, the corresponding\n97\nsupport functions are uniformly within\n′′′ = ′′/ √ (n+ 1)(n+ 2)\nof each other on the unit sphere. This and the fact that SISO ⊇ Bn imply\n(1− ′′′)SISO ⊆ S ′′ ⊆ (1 + ′′′)SISO.\nThus, by Lemma 18, dTV (S ′′, SISO) ≤ 1− (1−\n′′′\n1+ ′′′ )n ≤ 1− (1− ′′′)2n ≤ 2n ′′′ ≤ 2 ′′ and\nthis implies that the total variation distance between the uniform distributions on conv V and the input simplex is at most 2 ′′. Over all random choices, this happens with probability at least 1− 2δ/5. We set ′′ = /2.\nLemma 21. Let SINPUT be an n-dimensional isotropic simplex. Let Σ be an n-by-n positive definite matrix such that ‖Σ− I‖ ≤ < 1/2. Let µ be an n-dimensional vector such that ‖µ‖ ≤ . Let B be an n-by-n matrix such that Σ = BBT . Let S be the simplex B−1(SINPUT − µ). Then there exists an isotropic simplex SISO such that dTV (S, SISO) ≤ 6n .\nProof. We use an argument along the lines of the orthogonal Procrustes problem (nearest orthogonal matrix to B−1, already in [76, Proof of Theorem 4]): Let UDV T be the singular value decomposition of B−1. Let R = UV T be an orthogonal matrix (that approximates B−1). Let SISO = RSINPUT .\nWe have S = UDV T (SINPUT−µ). Let σmin, σmax be the minimum and maximum\nsingular values of D, respectively. This implies:\nσminUV T (SINPUT − µ) ⊆ S ⊆ σmaxUV T (SINPUT − µ),\nσmin(SISO −Rµ) ⊆ S ⊆ σmax(SISO −Rµ). (3.5)\n98\nAs SISO ⊇ Bn, ‖µ‖ ≤ 1, R is orthogonal and SISO is convex, we have\nSISO −Rµ ⊇ (1− ‖µ‖)SISO.\nAlso,\nSISO −Rµ ⊆ SISO + ‖µ‖Bn\n⊆ SISO(1 + ‖µ‖).\nThis in (3.5) gives\nσmin(1− ‖µ‖)SISO ⊆ S ⊆ σmax(1 + ‖µ‖)SISO.\nThis and Lemma 18 imply\ndTV (S, SISO) ≤ 2 ( 1− ( σmin(1− ‖µ‖) σmax(1 + ‖µ‖) )n) .\nThe estimate on Σ gives σmin ≥ √ 1− , σmax ≤ √ 1 + . Thus\ndTV (S, SISO) ≤ 2 ( 1− ( 1− 1 + )3n/2) ≤ 2 ( 1− (1− )3n\n) ≤ 6n ."
    }, {
      "heading" : "3.5 The local and global maxima of the 3rd moment of the",
      "text" : "standard simplex and the isotropic simplex\nIn this section we study the structure of the set of local maxima of the third moment as a function of the direction (which happens to be essentially u 7→ ∑ u3i as discussed in Section 3.2). This is not necessary for our algorithmic result, however it gives insight into the geometry of the third moment (the location of local\n99\nmaxima/minima and stationary points) and suggests that more direct optimization algorithms like gradient descent and Newton’s method will also work, although we will not prove that.\nTheorem 3.5.1. Let K ⊆ Rn be an isotropic simplex. Let X be random in K. Let V = {xi}n+1i=1 ⊆ Rn be the set of normalized vertices of K. Then V is a complete set of local maxima and a complete set of global maxima of F : Sn−1 → R given by F (u) = E((u ·X)3).\nProof idea: Embed the simplex in Rn+1. Show that the third moment is proportional to the complete homogeneous symmetric polynomial of degree 3, which for the relevant directions is proportional to the sum of cubes. To conclude, use first and second order optimality conditions to characterize the set of local maxima.\nProof. Consider the standard simplex\n∆n = conv{e1, . . . , en+1} ⊆ Rn+1\nand identify it with V via a linear map A : Rn+1 → Rn so that A(∆n) = V . Let Y be random in ∆n. Consider G : Sn → R given by G(v) = m3(v) = E((v · Y )3). Let U = {v ∈ Rn+1 : v · 1 = 0, ‖v‖ = 1} be the equivalent feasible set for the embedded problem. We have G(v) = cF (Av) for any v ∈ U and some constant c > 0 independent of v. To get the theorem, it is enough to show that the local maxima of G in U are precisely the normalized versions of the projections of the canonical vectors onto the hyperplane orthogonal to 1 = (1, . . . , 1). According to Section 3.2, for v ∈ U we have\nG(v) ∝ p3(v).\n100\nUsing a more convenient but equivalent constant, we want to enumerate the local maxima of the problem\nmax 1\n3 p3(v)\ns.t. v · v = 1\nv · 1 = 0\nv ∈ Rn+1.\n(3.6)\nThe Lagrangian function is\nL(v, λ1, λ2) = 1\n3 ∑ i v3i − λ1 ∑ i vi − λ2 1 2 ((∑ i v2i ) − 1 ) .\nThe first order condition is ∇vL = 0, that is,\nv2i = λ1 + λ2vi for i = 1, . . . , n+ 1. (3.7)\nConsider this system of equations on v for any fixed λ1, λ2. Let f(x) = x 2, g(x) = λ1 + λ2x. The first order condition says f(vi) = g(vi), where f is convex and g is affine. That is, the vis can take at most two different values. As our optimization problem (3.6) is symmetric under permutation of the coordinates, we conclude that, after putting the coordinates of a point v in non-increasing order, if v is a local maximum of (3.6), then v must be of the form\nv = (a, . . . , a, b, . . . , b),\nwhere a > 0 > b and there are exactly α as and β bs, for α, β ∈ {1, . . . , n}.\nWe will now study the second order necessary condition (SONC) to eliminate from the list of candidates all vectors with α > 1. It is easy to see that the surviving vectors are exactly the promised scaled projections of the canonical vectors. This vectors must all be local and global maxima: At least one of them must be a global\n101\nmaximum as we are maximizing a continuous function over a compact set and all of them have the same objective value so all of them are local and global maxima.\nThe SONC at v asks for the Hessian of the Lagrangian to be negative semidefinite when restricted to the tangent space to the constraint set at v [71, Section 11.5]. We compute the Hessian (recall that v(2) is the vector of the squared coordinates of v):\n∇vL = v(2) − λ11− λ2v\n∇2vL = 2 diag(v)− λ2I\nwhere diag(v) is the (n+ 1)-by-(n+ 1) matrix having the entries of v in the diagonal and 0 elsewhere.\nA vector in the tangent space is any z ∈ Rn+1 such that z · 1 = 0, v · z = 0, and definiteness of the Hessian is determined by the sign of zT∇2vLz for any such z, where\nzT∇2vLz = n+1∑ i=1 z2i (2vi − λ2).\nSuppose v is a critical point with α ≥ 2. To see that such a v cannot be a local maximum, it is enough to show 2a > λ2, as in that case we can take z = (1,−1, 0, . . . , 0) to make the second derivative of L positive in the direction z.\nIn terms of α, β, a, b, the constraints of (3.6) are αa + βb = 0, αa2 + βb2 = 1, and this implies a = √\nβ α(n+1)\n, b = − √\nα β(n+1) . The inner product between the first\norder condition (3.7) and v implies λ2 = ∑ v3i = αa 3 + βb3. It is convenient to consider the change of variable γ = α/(n + 1), as now candidate critical points are parameterized by certain discrete values of γ in (0, 1). This gives β = (1− γ)(n+ 1),\n102\na = √ (1− γ)/(γ(n+ 1)) and\nλ2 = (n+ 1)\n[ γ ( 1− γ\nγ(n+ 1) )3/2 − (1− γ) ( γ\n(1− γ)(n+ 1) )3/2] =\n1√ (n+ 1)γ(1− γ)\n[ (1− γ)2 − γ2 ] =\n1√ (n+ 1)γ(1− γ) [1− 2γ].\nThis implies\n2a− λ2 = 1√\n(n+ 1)γ(1− γ) [2(1− γ)− 1 + 2γ]\n= 1√\n(n+ 1)γ(1− γ) .\nIn (0, 1), the function given by γ 7→ 2a− λ2 = 1√ (n+1)γ(1−γ) is convex and symmetric around 1/2, where it attains its global minimum value, 2/ √ n+ 1, which is positive."
    }, {
      "heading" : "3.6 Probabilistic results used",
      "text" : "In this section we show the probabilistic results underlying the reductions from learning simplices and `np balls to ICA. The results are Theorems 3.6.1 and 3.6.2. They each show a simple non-linear rescaling of the respective uniform distributions that gives a distribution with independent components (Definition 6).\nTheorem 3.6.1 below gives us, in a sense, a “reversal” of the representation of the cone measure on ∂Bnp , seen in Theorem 3.1.1. Given any random point in the standard simplex, we can apply a simple non-linear scaling and recover a distribution with independent components.\n103\nDefinition 6. We say that a random vector X has independent components if it is an affine transformation of a random vector having independent coordinates.\nTheorem 3.6.1. Let X be a uniformly random vector in the (n − 1)-dimensional standard simplex ∆n−1. Let T be a random scalar distributed as Gamma(n, 1). Then the coordinates of TX are iid as Exp(1).\nMoreover, if A : Rn → Rn is an invertible linear transformation, then the random\nvector TA(X) has independent components.\nProof. In the case where p = 1, Theorem 3.1.1 restricted to the positive orthant implies that for random vector G = (G1, . . . , Gn), if each Gi is an iid exponential random variable Exp(1), then (G/‖G‖1, ‖G‖1) has the same (joint) distribution as (X,T ). Given the measurable function f(x, t) = xt, f(X,T ) has the same distribution as f(G/‖G‖1, ‖G‖1). That is, XT and G have the same distribution5.\nFor the second part, we know TA(X) = A(TX) by linearity. By the previous argument the coordinates of TX are independent. This implies that A(TX) has independent components.\nThe next lemma complements the main result in [16], Theorem 1 (Theorem 3.1.2 elsewhere here). They show a representation of the uniform distribution in Bnp , but they do not state the independence that we need for our reduction to ICA.\nLemma 22. Let p ∈ [1,∞). Let G = (G1, . . . , Gn) be iid random variables with density proportional to exp(−|t|p). Let W be a nonnegative random variable with distribution Exp(1) and independent of G. Then the random vector\nG\n(‖G‖pp +W )1/p\n5See [52, Theorem 1.1] for a similar argument in this context.\n104\nis independent of (‖G‖pp +W )1/p.\nProof idea. We aim to compute the joint density, showing that it is a product of individual densities. To avoid complication, we raise everything to the pth power, which eliminates extensive use of the chain rule involved in the change of variables.\nProof. It is enough to show the claim conditioning on the orthant in which G falls, and by symmetry it is enough to prove it for the positive orthant. Let random variable H = (Gp1, G p 2, . . . , G p n). Since raising (strictly) positive numbers to the pth power is injective, it suffices to show that the random vector\nX = H∑n\ni=1Hi +W\nis independent of the random vector Y = ∑n\ni=1Hi +W .\nFirst, let U be the interior of the support of (X, Y ), that is U = {x ∈ Rn : xi >\n0, ∑\ni xi < 1} × {y ∈ R : y > 0} and consider h : U → Rn and w : U → R where\nh(x, y) = xy\nand\nw(x, y) = y − n∑ i=1 h(x, y)i = y − n∑ i=1 xi · y = y\n( 1−\nn∑ i=1 xi\n) .\nThe random vector (H,W ) has a density ρH,W supported on V = intRn+1+ and\n(x, y) 7→ (h(x, y), w(x, y))\n105\nis one-to-one from U onto V . Let J(x, y) be the determinant of its Jacobian. This Jacobian is  y 0 · · · 0 x1 0 y · · · 0 x2 ...\n... 0 0 · · · y xn −y −y · · · −y 1− ∑n i=1 xi  which, by adding each of the first n rows to the last row, reduces to y 0 · · · 0 x1 0 y · · · 0 x2 ...\n... 0 0 · · · y xn 0 0 · · · 0 1\n ,\nthe determinant of which is trivially J(x, y) = yn.\nWe have that J(x, y) is nonzero in U . Thus, (X, Y ) has density ρX,Y supported\non U given by\nρX,Y (x, y) = fH,W ( h(x, y), w(x, y) ) · |J(x)|.\nIt is easy to see6 that each Hi = G p i has density Gamma(1/p, 1) and thus ∑n i=1Hi has density Gamma(n/p, 1) by the additivity of the Gamma distribution. We then compute the joint density\nρX,Y (x, y) = ρH,W ( h(x, y), w(x, y) ) · yn\n= ρH,W\n( xy, y(1− n∑ i=1 xi) ) · yn.\nSince W is independent of H,\nρX,Y (x, y) = ρW\n( y ( 1− n∑ i=1 xi )) · yn n∏ i=1 ρHi(xiy)\n6See for example [16, proof of Theorem 3].\n106\nwhere n∏ i=1 ( ρHi(xiy) ) · ρW ( y ( 1− n∑ i=1 xi )) · yn ∝ n∏ i=1 [ e−xiy(xiy) 1 p −1 ] exp ( −y(1− n∑ i=1 xi) ) yn\n∝ ( n∏ i=1 x 1 p −1 i ) yn/p.\nThe result follows.\nWith this in mind, we show now our analog of Theorem 3.6.1 for Bnp .\nTheorem 3.6.2. Let X be a uniformly random vector in Bnp . Let T be a random scalar distributed as Gamma((n/p)+1, 1). Then the coordinates of T 1/pX are iid, each with density proportional to exp(−|t|p). Moreover, if A : Rn → Rn is an invertible linear transformation, then the random vector given by T 1/pA(X) has independent components.\nProof. Let G = (G1, . . . , Gn) where each Gi is iid as Gamma(1/p, 1). Also, let W be an independent random variable distributed as Exp(1). Let S = (∑n i=1 |Gi|p+W )1/p .\nBy Lemma 22 and Theorem 3.1.2 we know (G/S, S) has the same joint distribution as (X,T 1/p). Then for the measurable function f(x, t) = xt, we immediately have f(X,T 1/p) has the same distribution as f(G/S, S) and thus XT 1/p has the same distribution as G.\nFor the second part, since T is a scalar, we have T 1/pA(X) = A(T 1/pX). By the previous argument we have that the coordinates of T 1/pX are independent. Thus, A(T 1/pX) has independent components.\nThis result shows that one can obtain a vector with independent components from a sample in a linearly transformed `p ball. In Section 3.7 we show that they are related\n107\nin such as way that one can recover the linear transformation from the independent components via ICA."
    }, {
      "heading" : "3.7 Reducing simplex learning to ICA",
      "text" : "We now show randomized reductions from the following two natural statistical\nestimation problems to ICA:\nProblem 1 (simplex). Given uniformly random points from an n-dimensional simplex, estimate the simplex.\nThis is the same problem of learning a simplex as in the rest of the section, we\njust restate it here for clarity.\nTo simplify the presentation for the second problem, we ignore the estimation of the mean of an affinely transformed distribution. That is, we assume that the `np ball to be learned has only been linearly transformed.\nProblem 2 (linearly transformed `np balls). Given uniformly random points from a linear transformation of the `np -ball, estimate the linear transformation.\nThese problems do not have an obvious independence structure. Nevertheless, known representations of the uniform measure in an `np ball and the cone measure (defined in Section 3.1) on the surface of an `np ball can be slightly extended to map a sample from those distributions into a sample with independent components by a non-linear scaling step. The use of a non-linear scaling step to turn a distribution into one having independent components has been done before [96, 97], but there it is applied after finding a transformation that makes the distribution axis-aligned. This alignment is attempted using ICA (or variations of PCA) on the original distribution\n108\n[96, 97], without independent components, and therefore the use of ICA is somewhat heuristic. One of the contributions of our reduction is that the rescaling we apply is “blind”, namely, it can be applied to the original distribution. In fact, the distribution does not even need to be isotropic (“whitened”). The distribution resulting from the reduction has independent components and therefore the use of ICA on it is well justified.\nThe reductions are given in Algorithms 3 and 4. To state the reductions, we denote by ICA(s(1), s(2), . . .) the invocation of an ICA routine. It takes samples s(1), s(2), . . . of a random vector Y = AX + µ, where the coordinates of X are independent, and returns an approximation to a square matrix M such that M(Y − E(Y )) is isotropic and has independent coordinates. The theory of ICA [32, Theorem 11] implies that if X is isotropic and at most one coordinate is distributed as a Gaussian, then such an M exists and it satisfies MA = DP , where P is a permutation matrix and D is a diagonal matrix with diagonal entries in {−1, 1}. We thus need the following definition to state our reduction: Let cp,n = (EX∈Bnp (X 2 1 )) 1/2. That is, the uniform distribution in Bnp /cp,n is isotropic.\nAs we do not state a full analysis of any particular ICA routine, we do not state\nexplicit approximation guarantees.\nAlgorithm 3 works as follows: Let X be an (n + 1)-dimensional random vector with iid coordinates distributed as Exp(1). Let V be the matrix having columns (v(i), 1) for i = 1, . . . , n + 1. Let Y be random according to the distribution that results from scaling in the algorithm. Theorem 3.6.1 implies that Y and V X have the same distribution. Also, X−1 is isotropic and Y and V (X−1)+V 1 have the same distribution. Thus, the discussion about ICA earlier in this section gives that the only\n109\nAlgorithm 3 Reduction from Problem 1 to ICA\nInput: A uniformly random sample p(1), . . . , p(t) from an n-dimensional simplex S. Output: Vectors ṽ(1), . . . , ṽ(n+ 1) such that their convex hull is close to S. Embed the sample in Rn+1: Let p′(i) = (p(i), 1). For every i = 1, . . . , t, generate a random scalar T (i) distributed as Gamma(n + 1, 1). Let q(i) = p′(i)T (i). Invoke ICA(q(1), . . . , q(t)) to obtain a approximately separating matrix M̃ . Compute the inverse of M̃ and multiply every column by the sign of its last entry to get a matrix Ã. Remove the last row of Ã and return the columns of the resulting matrix as ṽ(1), . . . , ṽ(n+ 1).\nseparating matrices M are such that MV = DP where P is a permutation matrix and D is a diagonal matrix with diagonal entries in {−1, 1}. That is, V P T = M−1D. As the last row of V is all ones, the sign change step in Algorithm 3 undoes the effect of D and recovers the correct orientation.\nAlgorithm 4 Reduction from Problem 2 to ICA\nInput: A uniformly random sample p(1), . . . , p(t) from A(Bnp ) for a known parameter p ∈ [1,∞), where A : Rn → Rn is an unknown invertible linear transformation. Output: A matrix Ã such that ÃBnp is close to A(B n p ). For every i = 1, . . . , t, generate a random scalar T (i) distributed as Gamma((n/p)+ 1, 1). Let q(i) = p(i)T (i)1/p. Invoke ICA(q(1), . . . , q(t)) to obtain an approximately separating matrix M̃ . Output Ã = c−1p,nM̃ −1.\nSimilarly, Algorithm 4 works as follows: Let X be a random vector with iid coordinates, each with density proportional to exp(−|t|p). Let Y be random according to the distribution that results from scaling in the algorithm. Theorem 3.6.2 implies that Y and AX have the same distribution. Also, X/cp,n is isotropic and we have Y\n110\nand Acp,n(X/cp,n) have the same distribution. Thus, the discussion about ICA earlier in this section gives that the only separating matrices M are such that MAcp,n = DP where P is a permutation matrix and D is a diagonal matrix with diagonal entries in {−1, 1}. That is, AP TD−1 = c−1p,nM−1. The fact that Bnp is symmetric with respect to coordinate permutations and sign changes implies that AP TD−1Bnp = AB n p and is the same as c−1p,nM −1. When p 6= 2, the assumptions in the discussion above about ICA are satisfied and Algorithm 4 is correct. When p = 2, the distribution of the scaled sample is Gaussian and this introduces ambiguity with respect to rotations in the definition of M , but this ambiguity is no problem as it is counteracted by the fact that the `2 ball is symmetric with respect to rotations.\n111\nChapter 4: LEARNING GAUSSIAN MIXTURE MODELS\nThe question of recovering a probability distribution from a finite set of samples is one of the most fundamental questions of statistical inference. While classically such problems have been considered in low dimension, more recently inference in high dimension has drawn significant attention in statistics and computer science literature.\nIn particular, an active line of investigation in theoretical computer science has dealt with the question of learning a Gaussian Mixture Model in high dimension. This line of work was started in [37] where the first algorithm to recover parameters using a number of samples polynomial in the dimension was presented. The method relied on random projections to a low dimensional space and required certain separation conditions for the means of the Gaussians. Significant work was done in order to weaken the separation conditions and to generalize the result (see e.g., [38, 12, 103, 1, 42]). Much of this work has polynomial sample and time complexity but requires strong separation conditions on the Gaussian components. A completion of the attempts to weaken the separation conditions was achieved in [18] and [74], where it was shown that arbitrarily small separation was sufficient for learning a general mixture with a fixed number of components in polynomial time. Moreover, a one-dimensional example given in [74] showed that an exponential dependence on the number of components\n112\nwas unavoidable unless strong separation requirements were imposed. Thus the question of polynomial learnability appeared to be settled. It is worth noting that while quite different in many aspects, all of these papers used a general scheme similar to that in the original work [38] by reducing high-dimensional inference to a small number of low-dimensional problems through appropriate projections.\nHowever, a surprising result was recently proved in [54]. The authors showed that a mixture of d Gaussians in dimension d could be learned using a polynomial number of samples, assuming a non-degeneracy condition on the configuration of the means. The result in [54] is inherently high-dimensional as that condition is never satisfied when the means belong to a lower-dimensional space. Thus the problem of learning a mixture gets progressively computationally easier as the dimension increases, a “blessing of dimensionality!” It is important to note that this was quite different from much of the previous work, which had primarily used projections to lower-dimension spaces.\nStill, there remained a large gap between the worst case impossibility of efficiently learning more than a fixed number of Gaussians in low dimension and the situation when the number of components is equal to the dimension. Moreover, it was not completely clear whether the underlying problem was genuinely easier in high dimension or our algorithms in low dimension were suboptimal. The one-dimensional example in [74] cannot answer this question as it is a specific worst-case scenario, which can be potentially ruled out by some genericity condition.\nWe take a step to eliminate this gap by showing that even very large mixtures of Gaussians can be polynomially learned. More precisely, we show that a mixture of m Gaussians with equal known covariance can be polynomially learned as long as m is\n113\nbounded from above by a polynomial of the dimension n and a certain more complex non-degeneracy condition for the means is satisfied. We show that if n is high enough, these non-degeneracy conditions are generic in the smoothed complexity sense. Thus for any fixed d, O(nd) generic Gaussians can be polynomially learned in dimension n.\nFurther, we prove that no such condition can exist in low dimension. A measure of non-degeneracy must be monotone in the sense that adding Gaussian components must make the condition number worse. However, we show that for k2 points uniformly sampled from [0, 1] there are (with high probability) two mixtures of unit Gaussians with means on non-intersecting subsets of these points, whose L1 distance is O∗(e−k) and which are thus not polynomially identifiable. More generally, in dimension n the distance becomes O∗(exp(− n √ k)). That is, the conditioning improves as the dimension increases, which is consistent with our algorithmic results.\nTo summarize, our contributions are as follows:\n(1) We show that for any q, a mixture of nq Gaussians in dimension n can be learned in time and number of samples polynomial in n and a certain “condition number” σ. For sufficiently high dimension, this results in an algorithm polynomial from the smoothed analysis point of view (Theorem 4.0.1). To do that we provide smoothed analysis of the condition number using certain results from [90] and anti-concentration inequalities. The main technical ingredient of the algorithm is a new “Poissonization” technique to reduce Gaussian mixture estimation to a problem of recovering a linear map of a product distribution known as underdetermined Independent Component Analysis (ICA). We combine this with the recent work on efficient algorithms for underdetermined ICA from [48] to obtain the necessary bounds.\n114\n(2) We show that in low dimension polynomial identifiability fails in a certain generic sense (see Theorem 4.0.3). Thus the efficiency of our main algorithm is truly a consequence of the “blessing of dimensionality” and no comparable algorithm exists in low dimension. The analysis is based on results from approximation theory and Reproducing Kernel Hilbert Spaces.\nMoreover, we combine the approximation theory results with the Poissonizationbased technique to show how to embed difficult instances of low-dimensional Gaussian mixtures into the ICA setting, thus establishing exponential information-theoretic lower bounds for underdetermined ICA in low dimension. To the best of our knowledge, this is the first such result in the literature.\nWe discuss our main contributions more formally now. The notion of Khatri–Rao\npower A d of a matrix A is defined in Section 4.1.\nTheorem 4.0.1 (Learning a GMM with Known Identical Covariance). Suppose m ≥ n and let , δ > 0. Let w1N (µ1,Σ) + · · · + wmN (µm,Σ) be an n-dimensional GMM, i.e. µi ∈ Rn, wi > 0, and Σ ∈ Rn×n. Let B be the n×m matrix whose ith column is\nµi/‖µi‖. If there exists d ∈ N so that σm ( B d ) > 0, then Algorithm 6 recovers each µi to within accuracy with probability 1− δ. Its sample and time complexity are at most\npoly ( md 2 , σd 2 , ud 2 , wd 2 , dd 2 , rd 2 , 1/ , 1/δ, 1/b, logd 2( 1/(b δ) ))\nwhere w ≥ maxi(wi)/mini(wi), u ≥ maxi ‖µi‖, r ≥ ( maxi ‖µi‖ + 1)/(mini ‖µi‖) ) ,\n0 < b ≤ σm(B d) are bounds provided to the algorithm, and σ = √ λmax(Σ).\n115\nWe note that the requirement that m ≥ n is due to the invocation of Theorem 1.3 of [48]; it should not be difficult, however, to adapt the algorithm to use a method similar to that of [54] to handle the case where m < n.\nGiven that the means have been estimated, the weights can be recovered using the tensor structure of higher order cumulants (see Section 4.1 for the definition of cumulants). This is shown in Appendix 4.5.\nWe show that σmin(A d) is large in the smoothed analysis sense, namely, if we start with a base matrix A and perturb each entry randomly to get Ã, then σmin(Ã d) is likely to be large:\nTheorem 4.0.2. For n > 1, let M ∈ Rn×( n 2) be an arbitrary matrix. Let N ∈ Rn×( n 2) be a randomly sampled matrix with each entry iid from N (0, σ2), for σ > 0. Then, for some absolute constant C,\nP ( σmin((M +N) 2) ≤ σ2/n7 ) ≤ 2C/n.\nWe point out the simultaneous and independent work of [19], where the authors prove learnability results related to our Theorems 4.0.1 and 4.0.2. We now provide a comparison. The results in [19], which are based on tensor decompositions, are stronger in that they can learn mixtures of axis-aligned Gaussians (with non-identical covariance matrices) without requiring to know the covariance matrices in advance. Their results hold under a smoothed analysis setting similar to ours. To learn a mixture of roughly n`/2 Gaussians up to an accuracy of their algorithm has running time and sample complexity poly`(n, 1/ , 1/ρ) and succeeds with probability at least 1− exp(−Cn1/3`), where the means are perturbed by adding an n-dimensional Gaussian from N (0, Inρ2/n). On the one hand, the success probability of their algorithm\n116\nis much better (as a function of n, exponentially close to 1 as opposed to polynomially close to 1, as in our result). On the other hand, this comes at a high price in terms of the running time and sample complexity: The polynomial poly`(n, 1/ , 1/ρ) above has degree exponential in `, unlike the degree of our bound which is polynomial in `. Thus, in this respect, the two results can be regarded as incomparable points on an error vs running time (and sample complexity) trade-off curve. Our result is based on a reduction from learning GMMs to ICA which could be of independent interest given that both problems are extensively studied in somewhat disjoint communities. Moreover, our analysis can be used in the reverse direction to obtain hardness results for ICA.\nThe technique of Poissonization is, to the best of our knowledge, new in the GMM setting, though we note that it has been previously applied in computational learning. For instance, in [?], it has been used for the estimation of properties of discrete probability distributions such as the support size and the entropy.\nFinally, in Section 4.4 we show that in low dimension the situation is very different from the high-dimensional generic efficiency given by Theorems 4.0.1 and 4.0.2: The problem is generically hard. More precisely, we show:\nTheorem 4.0.3. Let X be a set of 4k2 points uniformly sampled from [0, 1]n. Then with high probability there exist two mixtures with equal number of unit Gaussians p, q centered on disjoint subsets of X, such that, for some C > 0,\n‖p− q‖L1(Rn) < e−C(k/log k) 1/n .\nHere we would like to note that the assumption that X is random is convenient as it provides a natural model for genericity, guarantees (with high probability) small fill\n117\n(Section 4.4) and also ensures that the means of the Gaussian mixture components of p and q are not too close. In particular, it is not difficult to verify that with high probability any pair of the random means are at least 1/poly(k) separated. However the randomness assumption is not essential. In fact, the above theorem will hold for an arbitrary set of points with sufficiently small fill (see Section 4.4).\nCombining the above lower bound with our reduction provides a similar lower bound for ICA; see a discussion on the connection with ICA below. Our lower bound gives an information-theoretic barrier. This is in contrast to conjectured computational barriers that arise in related settings based on the noisy parity problem (see [54] for pointers). The only previous information-theoretic lower bound for learning GMMs we are aware of is due to [74] and holds for two specially designed onedimensional mixtures."
    }, {
      "heading" : "4.1 Preliminaries",
      "text" : "The singular values of a matrix A ∈ Rm×n will be ordered in the decreasing order:\nσ1 ≥ σ2 ≥ · · · ≥ σmin(m,n). By σmin(A) we mean σmin(m,n).\nFor a real-valued random variable X, the cumulants of X are certain polynomials in the moments of X. For j ≥ 1, the jth cumulant is denoted κj(X). Denoting mj := EXj, we have, for example: κ1(X) = m1, κ2(X) = m2 − m21, and κ3(X) = m3 − 3m2m1 + 2m31. In general, cumulants can be defined as certain coefficients of a Taylor expansion of the logarithm of the moment generating function\nof X: log(EX(etX)) = ∑∞ j=1 κj(X) tj j! . The first two cumulants are the same as the expectation and the variance, resp. Cumulants have the property that for two independent random variables X, Y we have κj(X + Y ) = κj(X) + κj(Y ) (assuming that\n118\nthe first j moments exist for both X and Y ). Cumulants are degree-j homogeneous, i.e. if α ∈ R and X is a random variable, then κj(αX) = αjκj(X). The third and higher cumulants of the Gaussian distribution are 0."
    }, {
      "heading" : "4.1.1 Gaussian Mixture Model.",
      "text" : "For i = 1, 2, . . . ,m, define Gaussian random vectors ηi ∈ Rn with distribution ηi ∼ N (µi,Σi) where µi ∈ Rn and Σi ∈ Rn×n. Let h be an integer-valued random variable which takes on value i ∈ [m] with probability wi > 0, henceforth called\nweights. (Hence ∑m\ni=1wi = 1.) Then, the random vector drawn as Z = ηh is said to be\na Gaussian Mixture Model (GMM) w1N (µ1,Σ1)+ . . .+wmN (µm,Σm). The sampling of Z can be interpreted as first picking one of the components i ∈ [m] according to the weights, and then sampling a Gaussian vector from component i. We will be primarily interested in the mixture of identical Gaussians of known covariance. In particular, there exists known Σ ∈ Rn×n such that Σi = Σ for each i. Letting η ∼ N (0,Σ), and denoting by eh the random variable which takes on the i th canonical vector ei with probability wi, we can write the GMM model as follows:\nZ = [µ1|µ2| · · · |µm]eh + η . (4.1)\nIn this formulation, eh acts as a selector of a Gaussian mean. Conditioning on h = i, we have Z ∼ N (µi,Σ), which is consistent with the GMM model.\nGiven samples from the GMM, the goal is to recover the unknown parameters of\nthe GMM, namely the means µ1, . . . , µm and the weights w1, . . . , wm.\n119"
    }, {
      "heading" : "4.1.2 Underdetermined ICA.",
      "text" : "In the basic formulation of ICA, the observed random variable X ∈ Rn is drawn according to the model X = AS, where S ∈ Rm is a latent random vector whose components Si are independent random variables, and A ∈ Rn×m is an unknown mixing matrix. The probability distributions of the Si are unknown except that they are not Gaussian. The ICA problem is to recover A to the extent possible. The underdetermined ICA problem corresponds the case m ≥ n. We cannot hope to recover A fully because if we flip the sign of the ith column of A, or scale this column by some nonzero factor, then the resulting mixing matrix with an appropriately scaled Si will again generate the same distribution on X as before. There is an additional ambiguity that arises from not having an ordering on the coordinates Si: If P is a permutation matrix, then PS gives a new random vector with independent reordered coordinates, AP T gives a new mixing matrix with reordered columns, and X = AP TPS provides the same samples as X = AS since P T is the inverse of P . As AP T is a permutation of the columns of A, this ambiguity implies that we cannot recover the order of the columns of A. However, it turns out that under certain genericity requirements, we can recover A up to these necessary ambiguities, that is to say we can recover the directions (up to sign) of the columns of A, even in the underdetermined setting.\nIt will be important for us to work with an ICA model where there is Gaussian noise in the data: X = AS + η, where η ∼ N (0,Σ) is an additive Gaussian noise independent of S, and the covariance of η given by Σ ∈ Rn×n is in general unknown and not necessarily spherical. We will refer to this model as the noisy ICA model.\n120\nWe define the flattening operation vec (·) from a tensor to a vector in the natural way. Namely, when T ∈ Rn` is a tensor, then vec (T )δ(i1,...,i`) = Ti1,...,i` where\nδ(i1, . . . , i`) = 1 + ∑` j=1 n `−j(ij − 1) is a bijection with indices ij running from 1 to n. Roughly speaking, each index is being converted into a digit in a base n number up to the final offset by 1. This is the same flattening that occurs to go from a tensor outer product of vectors to the Kronecker product of vectors.\nThe ICA algorithm from [48] to which we will be reducing learning a GMM relies on the shared tensor structure of the derivatives of the second characteristic function and the higher order multi-variate cumulants. This tensor structure motivates the following form of the Khatri-Rao product:\nDefinition 7. Given matrices A ∈ Rn1×m, B ∈ Rn2×m, a column-wise Khatri-Rao product is defined by A B := [vec (A1 ⊗B1) | · · · |vec (Am ⊗Bm)], where Ai is the ith column of A, Bi is the i th column of B, ⊗ denotes the Kronecker product and vec (A1 ⊗B1) is flattening of the tensor A1 ⊗ B1 into a vector. The related KhatriRao power is defined by A ` = A · · · A (` times).\nThis form of the Khatri-Rao product arises when performing a change of coordinates under the ICA model using either higher order cumulants or higher order derivative tensors of the second characteristic function."
    }, {
      "heading" : "4.1.3 ICA Results.",
      "text" : "Theorem 4.1.1 (Theorem 4.1.1, from [48]) allows us to recover A up to the necessary ambiguities in the noisy ICA setting. The theorem establishes guarantees for an algorithm from [48] for noisy underdetermined ICA, UnderdeterminedICA. This algorithm takes as input a tensor order parameter d, number of signals m, access\n121\nto samples according to the noisy underdetermined ICA model with unknown noise, accuracy parameter , confidence parameter δ, bounds on moments and cumulants M and ∆, a bound on the conditioning parameter σm, and a bound on the cumulant order k. It returns approximations to the columns of A up to sign and permutation.\nICA Results. The following theorem (from [48]) allows us to recover A up to the necessary ambiguities in the noisy ICA setting. The theorem establishes guarantees for an algorithm from [48] for noisy underdetermined ICA, UnderdeterminedICA. This algorithm takes as input a tensor order parameter d, number of signals m, access to samples according to the noisy underdetermined ICA model with unknown noise, accuracy parameter , confidence parameter δ, bounds on moments and cumulants M and ∆, a bound on the conditioning parameter σm, and moment order k. It returns approximations to the columns of A up to sign and permutation.\nTheorem 4.1.1 ([48]). Let a random vector x ∈ Rn be given by an underdetermined ICA model with unknown Gaussian noise x = As + η where A ∈ Rn×m, with unit norm columns, and the covariance matrix Σ ∈ Rn×n are unknown. Let d ∈ 2N be such that σm(A d/2) > 0. Let k > d be such that for each si, there is a ki satisfying d < ki ≤ k and |κki(si)| ≥ ∆, and E|si|k ≤ M . Moreover, suppose that the noise also satisfies the same moment condition: E|〈u, ηi〉|k ≤ M for any unit vector u ∈ Rn (this is satisfied if we have k!σk ≤ M where σ2 is the maximum eigenvalue of Σ). Then algorithm UnderdeterminedICA returns a set of n-dimensional vectors (Ãi) m i=1 so that for some permutation π of [m] and signs αi ∈ {−1, 1} we have ‖αiÃπ(i) − Ai‖ ≤ for all i ∈ [m]. Its sample and time com-\nplexity are poly ( nk,mk 2 ,Mk, 1/∆k, 1/σm(A d/2)k, 1/ , 1/δ ) .\n122"
    }, {
      "heading" : "4.2 Learning GMM means using underdetermined ICA: The",
      "text" : "basic idea\nIn this section we give an informal outline of the proof of our main result, namely learning the means of the components in GMMs via reduction to the underdetermined ICA problem. Our reduction will be discussed in two parts. The first part gives the main idea of the reduction and will demonstrate how to recover the means µi up to their norms and signs, i.e. we will get ±µi/‖µi‖. We will then present the reduction in full. It combines the basic reduction with some preprocessing of the data to recover the µi’s themselves. The reduction relies on some well-known properties of the Poisson distribution stated in the lemma below; its proof can be found in Appendix 4.6.4.\nLemma 23. Fix a positive integer k, and let pi ≥ 0 be such that p1 + · · · + pk = 1. If X ∼ Poisson(λ) and (Y1, . . . , Yk)|X=x ∼ Multinom(x; p1, . . . , pk) then Yi ∼ Poisson(piλ) for all i and Y1, . . . , Yk are mutually independent.\nBasic Reduction: The main idea. Recall the GMM from equation (4.1) is given by Z = [µ1| · · · |µm]eh + η. Henceforth, we will set A = [µ1| · · · |µm]. We can write the GMM in the form Z = Aeh + η, which is similar in form to the noisy ICA model, except that eh does not have independent coordinates. We now describe how a single sample of an approximate noisy ICA problem is generated.\nThe reduction involves two internal parameters λ and τ that we will set later. We generate a Poisson random variable R ∼ Poisson(λ), and we run the following experiment R times: At the ith step, generate sample Zi from the GMM. Output the sum of the outcomes of these experiments: Y = Z1 + · · ·+ ZR.\n123\nLet Si be the random variable denoting the number of times samples from the i th component were chosen in the above experiment. Thus S1 + · · ·+Sm = R. Note that S1, . . . , Sm are not observable although we know their sum. By Lemma 23, each Si has distribution Poisson(wiλ), and the random variables Si are mutually independent. Let S := (S1, . . . , Sm) T .\nFor a non-negative integer t, we define η(t) := ∑t\ni=1 ηi where the ηi are iid ac-\ncording to ηi ∼ N (0,Σ). In this definition t can be a r.v., in which case the ηi are sampled independent of t. Using d = to indicate that two random variables have the same distribution, then\nY d = AS + η(R) . (4.2)\nIf there were no Gaussian noise in the GMM (i.e. if we were sampling from a discrete set of points) then the model in (4.2) becomes Y = AS, which is the ICA model without noise, and so we could recover A up to necessary ambiguities. However, the model Y d = AS+η(R) fails to satisfy even the assumptions of the noisy ICA model, both because η(R) is not independent of S and because η(R) is not distributed as a Gaussian random vector.\nAs the covariance of the additive Gaussian noise is known, we may add additional noise to the samples of Y to obtain a good approximation of the noisy ICA model. Parameter τ , the second parameter of the reduction, is chosen so that with high probability we have R ≤ τ . Conditioning on the event R ≤ τ we draw X according to the rule\nX = Y + η(τ −R) d= AS + η(R) + η(τ −R),\n124\nwhere η(R), η(τ − R), and S are drawn independently conditioned on R. Then, conditioned on R ≤ τ , we have X d= AS + η(τ).\nNote that we have only created an approximation to the ICA model. In particular, restricting ∑m\ni=1 Si = R ≤ τ can be accomplished using rejection sampling, but the\ncoordinate random variables S1, . . . , Sm would no longer be independent. We have two models of interest: (1) X d = AS+ η(τ), a noisy ICA model with no restriction on\nR = ∑m\ni=1 Si, and (2) X d = (AS + η(τ))|R≤τ the restricted model.\nWe are unable to produce samples from the first model, but it meets the assumptions of the noisy ICA problem. Pretending we have samples from model (1), we can apply Theorem 4.1.1 to recover the Gaussian means up to sign and scaling. On the other hand, we can produce samples from model (2), and depending on the choice of τ , the statistical distance between models (1) and (2) can be made arbitrarily close to zero. It will be demonstrated that given an appropriate choice of τ , running UnderdeterminedICA on samples from model (2) is equivalent to running UnderdeterminedICA on samples from model (1) with high probability, allowing for recovery of the Gaussian mean directions ±µi/‖µi‖ up to some error.\nFull reduction. To be able to recover the µi without sign or scaling ambiguities, we add an extra coordinate to the GMM as follows. The new means µ′i are µi with an additional coordinate whose value is 1 for all i, i.e. µ′i := ( µi 1 ) . Moreover, this coordinate has no noise. In other words, each Gaussian component now has an\n(n+ 1)× (n+ 1) covariance matrix Σ′ := (\nΣ 0 0 0\n) . It is easy to construct samples\nfrom this new GMM given samples from the original: If the original samples were\n125\nu1, u2 . . ., then the new samples are u ′ 1, u ′ 2 . . . where u ′ i := ( ui 1 ) . The reduction proceeds similarly to the above on the new inputs.\nUnlike before, we will define the ICA mixing matrix to be A′ := [\nµ′1 ‖µ′1‖ | · · · | µ ′ m ‖µ′m‖ ] such that it has unit norm columns. The role of matrix A in the basic reduction will now be played by A′. Since we are normalizing the columns of A′, we have to scale the ICA signal S obtained in the basic reduction to compensate for this: Define S ′i := ‖µ′i‖Si. Thus, the ICA models obtained in the full reduction are:\nX ′ = A′S ′ + η′(τ), (4.3) X ′ = (A′S ′ + η′(τ)), |R≤τ (4.4)\nwhere we define η′(τ) =\n( η(τ)\n0\n) . As before, we have an ideal noisy ICA model\n(4.3) from which we cannot sample, and an approximate noisy ICA model (4.4) which can be made arbitrarly close to (4.3) in statistical distance by choosing τ appropriately. With appropriate application of Theorem 4.1.1 to these models, we can recover estimates (up to sign) {Ã′1, . . . , Ã′m} of the columns of A′.\nBy construction, the last coordinate of Ã′i now tells us both the signs and magnitudes of the µi: Let Ã ′ i(1 : n) ∈ Rn be the vector consisting of the first n coordinates of Ã′i, and let Ã ′ i(n+ 1) be the last coordinate of Ã ′ i. Then\nµi = A′i(1 : n) A′i(n+ 1) ≈ Ã ′ i(1 : n) Ã′i(n+ 1) ,\nwith the sign indeterminancy canceling in the division."
    }, {
      "heading" : "4.3 Correctness of the Algorithm and Reduction",
      "text" : "Subroutine 5 is used to capture the main process of the reduction: Let Σ be the covariance matrix of the GMM, λ be an integer chosen as input, and a threshold\n126\nSubroutine 5 Single sample reduction from GMM to approximate ICA Input: Covariance parameter Σ, access to samples from a mixture of m identical Gaussians in Rn with variance Σ, Poisson threshold τ , Poisson parameter λ, Output: Y (a sample from model (4.6)).\n1: Generate R according to Poisson(λ). 2: if R > τ then return failure. 3: end if 4: Let Y = 0. 5: for j = 1 to R do 6: Get a sample Zj from the GMM. 7: Let Z ′j = (Zj, 1) to embed the sample in Rn+1 8: Y = Y + Zj. 9: end for\n10: Let Σ′ = ( Σ 0 0 0 ) (add a row and column of all zeros) 11: Generate η according to N (0, (τ −R)Σ′). 12: Y = Y + η. return Y .\nvalue τ also computed elsewhere and provided as input. Let R ∼ Poisson(λ). If R is larger τ , the subroutine returns a failure notice and the calling algorithm halts immediately. A requirement, then, should be that the threshold is chosen so that the chance of failure is very small; in our case, τ is chosen so that the chance of failure is half of the confidence parameter given to Algorithm 6. The algorithm then goes through the process described in the full reduction: sampling from the GMM, lifting the sample by appending a 1, then adding a lifted gaussian noise so that the total noise has distribution N (0, τΣ). The resulting sample is from the model given by (4.4).\nAlgorithm 6 works as follows: it takes as input the parameters of the GMM (covariance matrix, number of means), tensor order (as required by UnderdeterminedICA), error parameters, and bounds on certain properties of the weights and\n127\nAlgorithm 6 Use ICA to learn the means of a GMM Input: Covariance matrix Σ, number of components m, upper bound on tensor order parameter d, access to samples from a mixture of m identical, spherical Gaussians in Rn with covariance Σ, confidence parameter δ, accuracy parameter , upper bound w ≥ maxi(wi)/mini(wi), upper bound on the norm of the mixture means u, r ≥ (maxi ‖µi‖+ 1)/(mini ‖µi‖), and lower bound b so 0 < b ≤ σm(A d/2). Output: {µ̃1, µ̃2, . . . , µ̃m} ⊆ Rn (approximations to the means of the GMM). 1: Let δ2 = δ1 = δ/2. 2: Let σ = supv∈Sn−1 √ Var(vTη(1)), for η(1) ∼ N (0,Σ).\n3: Let λ = m be the parameter to be used to generate the Poisson random variable in Subroutine 5. 4: Let τ = 4 ( log(1/δ2) + log(q(Θ)) )\nmax ((eλ)2, 4Cd2) (the threshold used to add noise in the samples from Subroutine 5, C is a universal constant, and q(Θ) is a polynomial defined as (4.18) in the proof of Theorem 4.0.1).\n5: Let ∗ = (√ 1 + u2 + 2(1 + u2) )−1 .\n6: Let M = max ( (τσ)d+1, (w/( √ 1 + u2)d+1 ) (d+ 1)d+1. 7: Let k = d+ 1. 8: Let ∆ = w. 9: Invoke UnderdeterminedICA with access to Subroutine 5, parameters δ1,\n∗, ∆, M , and d to obtain Ã′ (whose columns approximate the normalized means up to sign and permutation). If any calls to Subroutine 5 result in failure, the algorithm will halt completely. 10: Divide each column of Ã′ by the value of its last entry. 11: Remove the last row of Ã′ to obtain B̃. return the columns of B̃ as {µ̃1, µ̃2, . . . , µ̃m}.\n128\nmeans. The algorithm then calculates various internal paremeters: a bound on directional covariances, error parameters to be split between the “Poissonization” process and the call to UnderdeterminedICA, the threshold parameter τ and Poisson parameter λ to be used in Subroutine 5, and values explicitly needed by the proof of UnderdeterminedICA in [48]. Other internal values are given by a constanct C and a polynomial q(Θ); these are determined by the proof of Theorem 4.0.1. Essentially, C is a constant so that one can cleanly compute a value of τ that will involve a polynomial, q(Θ), of all the other parameters. The algorithm then calls UnderdeterminedICA, but instead of giving samples from the GMM, it allows access to Subroutine 5. It is then up to UnderdeterminedICA to generate samples as needed (bounded by the polynomial in Theorem 4.0.1). In the case that Subroutine 5 returns a failure, the entire algorithm process halts, and returns nothing. If no failure occurs, the matrix returned by UnderdeterminedICA will be the matrix of normalized means embedded in Rn+1, and the algorithm de-normalizes, removes the last row, and then has approximations to the means of the GMM.\nThe bounds are used instead of actual values to allow flexibility – in the context under which the algorithm is invoked – on what the algorithm needs to succeed. However, the closer the bounds are to the actual values, the more efficient the algorithm will be.\nWe show that the reduction results in a model that allows one to use the ICA algorithm UnderdeterminedICA presented in [48] (see Section 2.2, Theorem 4.1.1). That is, we can choose the parameters, τ and λ, for the Poissonization and threshold that yield a model which will satisfy the assumptions of Theorem 4.1.1. This also\n129\ninvolves translating the bounds on cumulants and higher moments of our Poisson signals and added noise to obtain feasible bounds for Theorem 4.1.1.\nSketch of correctness argument. The proof of correctness of Algorithm 6 has two main parts. In the first part (section 4.3), we analyze the sample complexity of recovering the Gaussian means using UnderdeterminedICA when samples are taken from the ideal noisy ICA model (4.3). In the second part, we note that we do not have access to the ideal model (4.3), and that we can only sample from the approximate noisy ICA model (4.4) using the full reduction. Choosing τ appropriately, we use total variation distance to argue that with high probability, running UnderdeterminedICA with samples from the approximate noisy ICA model will produce equally valid results as running UnderdeterminedICA with samples from the ideal noisy ICA model. The total variation distance bound is explored in section 4.3.\nThese ideas are combined in section 4.3 to prove the correctness of Algorithm 6. One additional technicality arises from the implementation of Algorithm 6. Samples can be drawn from the noisy ICA model X ′ = (AS ′ + η′(τ))|R≤τ using rejection sampling on R. In order to guarantee Algorithm 6 executes in polynomial time, when a sample of R needs to be rejected, Algorithm 6 terminates in explicit failure. To complete the proof, we argue that with high probability, Algorithm 6 does not explicitly fail.\nError Analysis of the Ideal Noisy ICA Model The proposed full reduction from Section 4.2 provides us with two models. The first is a noisy ICA model from\n130\nwhich we cannot sample:\n(Ideal ICA) X ′ = A′S ′ + η′(τ) . (4.5)\nThe second is a model that fails to satisfy the assumption that S ′ has independent coordinates, but it is a model from which we can sample:\n(Approximate ICA) X ′ = (A′S ′ + η′(τ))|R≤τ . (4.6)\nBoth models rely on the choice of two parameters, λ and τ . The dependence on τ is explicit in the models. The dependence on λ can be summarized in the unrestricted model as Si = 1 ‖µ′i‖ S ′i ∼ Poisson(wiλ) independently of each other, and R = ∑m i=1 Si ∼ Poisson(λ).\nThe probability of choosing R > τ will be seen to be exponentially small in τ . For this reason, running UnderdeterminedICA with polynomially many samples from model (4.5) will with high probability be equivalent to running the ICA Algorithm with samples from model (4.6). This notion will be made precise later using total variation distance.\nFor the remainder of this subsection, we proceed as if samples are drawn from the ideal noisy ICA model (4.5). Thus, to recover the columns of A′, it suffices to run UnderdeterminedICA on samples of X ′. Theorem 4.1.1 can be used for this analysis so long as we can obtain the necessary bounds on the cumulants of S ′, moments of S ′, and the moments of η′(τ). We define wmin := miniwi and wmax := maxiwi. Then, the cumulants of S ′ are bounded by the following lemma:\nLemma 24. Given ` ∈ Z+, κ`(S ′i) ≥ wiλ for each S ′i. In particular, then κ`(S ′i) ≥ wminλ.\n131\nProof. By construction, S ′i = ‖µ′i‖Si. By the homogeneity property of univariate cumulants,\nκ`(S ′ i) = κ`(‖µ′i‖Si) = ‖µ′i‖ ` κ`(Si)\nAs µ′i(n+ 1) = 1, ‖µ′i‖ ≥ 1. The cumulants of the Poisson distribution are given in Lemma 37. It follows that κ`(S ′ i) ≥ κ`(Si) = wiλ.\nThe bounds on the moments of S ′i for each i can be computed using the following\nlemma:\nLemma 25. For ` ∈ Z+, we have ES ′`i ≤ (‖µ′i‖wiλ)```.\nProof. Let Y denote a random variable drawn from Poisson(α). It is known (see [86]) that\nEY ` = ∑̀ i=1 αi { ` i } where { ` i } denotes Stirling number of the second kind. Using Lemma 36, it follows that\nEY ` ≤ ∑̀ i=1 αi``−1 ≤ `α```−1 = α```.\nSince S ′i = µ ′ iSi where Si ∼ Poisson(λwi), it follows that ES ′`i = ‖µ′i‖ `ES`i ≤ ‖µ′i‖ `(wiλi) ```.\nThe absolute moments of Gaussian random variables are well known. For com-\npleteness, the bounds are provided in Lemma 38 of Appendix 4.6.6. Defining σ = supv∈Sn−1 √ Var(vTη′(1)); vectors µ′max = argmaxi‖µ′i‖, µ′min = argmini‖µ′i‖, and similarly µmax and µmin for later; and choosing λ = m, we can now show a polynomial bound for the error in recovering the columns of A′ using UnderdeterminedICA.\n132\nTheorem 4.3.1 (ICA specialized to the ideal case). Suppose that samples of X ′ are taken from the unrestricted ICA model (4.5) choosing parameter λ = m and τ a constant. Suppose that UnderdeterminedICA is run using these samples. Suppose σm(A ′ d/2) > 0. Fix ∈ (0, 1/2) and δ ∈ (0, 1/2). Then with probability 1− δ, when the number of samples N is:\nN ≥ poly ( nd,md 2 , (τσ)d 2 , ‖µ′max‖ d2 , (wmax/wmin) d2 , dd 2 , 1/σm(A ′ d/2)d, 1/ , 1/δ ) (4.7) the columns of A′ are recovered within error up to their signs. That is, denoting the columns returned from UnderdeterminedICA by Ã′1, . . . , Ã ′ m, there exists α1, · · · , αm,∈ {−1,+1} and a permutation p of [m] such that ‖A′i − αiÃ′p(i)‖ < for each i.\nProof. Obtaining the sample bound is an exercise of rewriting the parameters associated with the model X ′ = A′S ′+ η′(τ) in a way which can be used by Theorem 4.1.1. In what follows, where new parameters are introduced without being described, they will correspond to parameters of the same name defined in and used by the statement of Theorem 4.1.1.\nParameter d is fixed. We must choose k1, . . . , km and k such that d < ki ≤ k and κki(S ′ i) is bounded away from 0. It suffices to choose k1 = · · · = km = k = d + 1. By Lemma 24, κd+1(S ′ i) ≥ wminλ = wminm for each i. As wmax ≥ 1m ∑m i=1wi = 1 m , we have that κd+1(S ′ i) ≥ wminwmax for each i, giving a somewhat more natural condition number. In the notation of Theorem 4.1.1, we have a constant\n∆ = wmin wmax\n(4.8)\nsuch that κd+1(S ′ i) ≥ ∆ for each i.\n133\nNow we consider the upper bound M on the absolute moments of both η′(τ) and on S ′i. As the Poisson distribution takes on non-negative values, it follows that S ′i = ‖µ′i‖Si takes on non-negative values. Thus, the moments and absolute moments of S ′i coincide. Using Lemma 25, we have that E|S ′i|d+1 = E(S ′i)d+1 ≤ (‖µ′i‖wiλ)d+1(d+ 1)d+1. Thus, for M to bound the (d + 1)th moment of S ′i, it suffices that M ≥ (‖µ′max‖wmaxλ)d+1(d+ 1)d+1. Noting that\nwmaxλ = wmaxm = wmax 1/m ≤ wmax wmin\nit suffices that M ≥ (‖µ′max‖wmaxwmin ) d+1(d + 1)d+1, giving a more natural condition number.\nNow we bound the absolute moments of the Gaussian distribution. As d ∈ 2N, it follows that d+ 1 is odd. Given a unit vector u ∈ Rn, it follows from Lemma 38 that\nE|〈u, η′(τ)〉|d+1 = Var(〈u, η′(τ)〉) (d+1) 2 2d/2 (d/2)! 1√ π\n= τ d+1Var(〈u, η′(1)〉) (d+1) 2 2d/2 (d/2)! 1√ π .\nσ gives a clear upper bound for Var(〈u, η′(1)〉)1/2, and (d+ 1)d+1 gives a clear upper bound to 1√ π 2d/2(d/2)!. As such, it suffices that M ≥ (τσ)d+1(d + 1)d+1 in order to guarantee that M ≥ E|〈u, η′(τ)〉|d+1. Using the obtained bounds for M from the Poisson and Normal variables, it suffices that M be taken such that\nM ≥ max (\n(τσ)d+1, (‖µ′max‖ wmax wmin\n)d+1 ) (d+ 1)d+1 (4.9)\nto guarantee that M bounds all required order d+ 1 absolute moments.\n134\nWe can now apply Theorem 4.1.1, using the parameter values k = d+ 1, ∆ from\n(4.8), and M from (4.9). Then with probability 1− δ,\nN ≥ poly ( n2d+1,md 2 , (τσ)d 2 , ‖µ′max‖ d2 , (wmax/wmin) d2 , (d+ 1)d 2 ,\n1/σm(A ′ d/2)d+1, 1/ , 1/δ ) (4.10)\nsamples suffice to recover up to sign the columns of A′ within accuracy. More precisely, letting Ã′1, . . . , Ã ′ m give the columns produced by UnderdeterminedICA, then there exists parameters α1, . . . , αm such that αi ∈ {−1,+1} captures the sign indeterminacy, and a permutation p on [m] such that ‖A′i − Ã′p(i)‖ < for each i.\nThe poly bound in (4.10) is equivalent to the poly bound in (4.7).\nTheorem 4.3.1 allows us to recover the columns of A′ up to sign. However, what we really want to recover are the means of the original Gaussian mixture model, which are the columns of A. Recalling the correspondence between A′ and A laid out in section 4.2, the Gaussian means µ1, . . . , µm which form the columns of A are related to the columns µ′1, . . . , µ ′ m of A ′ by the rule µi = µ ′ i(1 : n)/µ ′ i(n+ 1). Using this rule, we can construct estimate the Gaussian means from the estimates of the columns of A′. By propogating the errors from Theorem 4.3.1, we arrive at the following result:\nTheorem 4.3.2 (Recovery of Gaussian means in Ideal Case). Suppose that UnderdeterminedICA is run using samples of X ′ from the ideal noisy ICA model (4.5) choosing parameters λ = m and τ a constant. Define B ∈ Rn×m such that Bi = Ai/‖Ai‖. Suppose further that σm(B d/2) > 0. Let Ã′1, · · · , Ã′m be the returned estimates of the columns of A′ (from model (4.5)) by UnderdeterminedICA. Let µ̃i = Ã ′ i(1 : n)/Ã ′ i(n+1) for each i. Fix error parameters ∈ (0, 1/2) and δ ∈ (0, 1/2).\n135\nWhen at least\nN ≥ (4.11)\npoly ( nd,md 2 , (τσ)d 2 , ‖µmax‖d 2 , ( wmax wmin )d2 , dd 2 , ( ‖µmax‖+ 1 ‖µmin‖ )d2 ,\n1 σm(B d/2)d , 1 , 1 δ ) (4.12)\nsamples are used, then with probability 1− δ there exists a permutation p of [m] such that ‖µ̃p(i) − µi‖ < for each i.\nProof. Let ∗ > 0 (to be chosen later) give a desired bound on the errors of the columns of A′. Then, from Theorem 4.3.1, using\nN ≥ poly ( nd,md 2 , (τσ)d 2 , ‖µ′max‖ d2 , (wmax/wmin) d2 , dd 2 , 1/σm(A ′ d/2)d, 1/ ∗, 1/δ ) (4.13) samples suffices with probability 1− δ to produce column estimates Ã′1, . . . , Ã′m such that for an unknown permutation p and signs α1, . . . , αm, αp(1)Ã ′ p(1), . . . , αp(m)Ã ′ p(m) give ∗-close estimates of the columns A′1, . . . , A ′ m respectively of A ′. In order to avoid notational clutter, we will assume without loss of generality that p is the identity map, and hence that ‖αiÃ′i − αA′i‖ < ∗ holds.\nThis proof proceeds in two steps. First, we replace the dependencies in (4.13) on parameters from the lifted GMM model generated by the full reduction with dependencies based on the GMM model we are trying to learn. Then, we propagate the error from recovering the columns Ã′i to that of recovering µ̃i.\nStep 1: GMM Dependency Replacements. In the following two claims, we consider alternative lower bounds for N for recovering column estimators Ã′1, . . . , Ã ′ m which are ∗-close up to sign to the columns of A′. In particular, so long as we use at\n136\nleast as many samples of X ′ as in (4.13) when calling UnderdeterminedICA, then A′ will be recovered with the desired precision with probability 1− δ. Claim. The poly(‖µ′max‖ d2 , dd 2 ) dependence in (4.13) can be replaced by a poly(‖µmax‖d 2 , dd 2 ) dependence.\nProof of Claim. By construction, µ′max =\n( µmax\n1\n) . By the triangle inequality,\n‖µ′max‖ d2 ≤ (‖µmax‖+ 1)d 2\nwhere (‖µmax‖+1)d 2 is a polynomial q of ‖µmax‖ with coefficients bounded by (d2)d 2 = d2d 2 = poly(dd 2 ). The maximal power of ‖µmax‖ in q(‖µmax‖) is dd 2 . It follows that q(‖µmax‖) = poly(‖µmax‖d 2 , dd 2 ). N\nClaim. The poly(1/σm(A ′ d/2)d) in (4.13) can be replaced by a poly((‖µmax‖+1‖µmin‖ ) d2 , 1/σm(B d/2)d) dependence.\nProof of Claim. First define A′ to be the unnormalized version of A′. That is, A′i := µ′i. Then, A ′ = A′ diag ‖µ′1‖, . . . , ‖µ′m‖ implies A′ d/2 = A′ d/2 diag ‖µ′1‖ d/2, . . . ‖µ′m‖ d/2. Thus, σm(A ′ d/2) ≤ σm(A′ d/2)‖µ′max‖ d/2.\nNext, we note that A′ = ( A 1 ) where 1 is an all ones row vector. It follows that\nthe rows of A d/2 are a strict subset of the rows of A′ d/2. Thus,\nσm(A d/2) = inf ‖u‖=1 ‖A d/2u‖ ≤ inf ‖u‖=1 ‖A′ d/2u‖ = σm(A′ d/2) .\nFinally, B = A diag (\n1 ‖µ1‖ , . . . , 1 ‖µm‖\n) andB d/2 = A d/2 diag ( 1\n‖µ1‖d/2 , . . . , 1 ‖µm‖d/2\n) .\nIt follows that σm(B d/2) ≤ σm(A d/2) 1‖µmin‖d/2 . Chaining together inequalities yields:\nσm(B d/2) ≤ ‖µ ′ max‖ d/2\n‖µmin‖d/2 σm(A\n′ d/2) or alternatively\n‖µ′max‖ d/2\n‖µmin‖d/2 · 1 σm(B d/2) ≥ 1 σm(A′ d/2) .\n137\nAs µ′max = (µ T max 1) T , the triangle inequality implies ‖µ′max‖ ≤ ‖µmax‖ + 1. As we require the dependency of at least N > poly((1/σm(A ′ d/2))d) samples, it suffices to have the replacement dependency of N > poly((‖µmax‖+1‖µmin‖ ) d 2 ·d(1/σm(B d/2)d) = poly((‖µmax‖+1‖µmin‖ ) d2(1/σm(B d/2)d) samples. N\nThus, it is sufficient to call UnderdeterminedICA with\nN ≥ (4.14)\npoly ( nd,md 2 , (τσ)d 2 , ‖µmax‖d 2 , (wmax wmin )d2 , dd 2 , ( ‖µmax‖+ 1 ‖µmin‖ )d2 ,\n1\nσm(B d/2)d ,\n1 ∗ , 1\nδ ) (4.15)\nsamples to achieve the desired ∗ accuracy on the returned estimates of the columns of A′ with probability 1− δ.\nStep 2: Error propagation. What remains to be shown is that an appropriate choice of ∗ enforces ‖µi − µ̃i‖ < by propagating the error.\nRecall that A′i = ( µi 1 ) · ‖ ( µi 1 ) ‖ −1 , making A′i(n+ 1) = 1√ 1+‖µi‖2 . Thus,\nA′i(n+ 1) ≥ 1√\n1 + ‖µmax‖2 . (4.16)\nWe have that:\n‖µi − µ̃i‖ = ‖ A′i(1 : n) A′i(n+ 1) − Ã ′ i(1 : n) Ã′i(n+ 1) ‖\n= ‖ A ′ i(1 : n)\nA′i(n+ 1) − αiÃ\n′ i(1 : n)\nA′i(n+ 1) + αiÃ\n′ i(1 : n)\nA′i(n+ 1) − αiÃ\n′ i(1 : n)\nαiÃ′i(n+ 1) ‖\n≤ ‖A ′ i(1 : n)− αiÃ′i(1 : n)‖ |A′i(n+ 1)| + ‖Ã′i(1 : n)‖|αiÃ′i(n+ 1)− A′i(n+ 1)| |A′i(n+ 1)αiÃ′i(n+ 1)|\n≤ ∗ √ 1 + ‖µmax‖2 + |αiÃ′i(n+ 1)− A′i(n+ 1)|\n|A′i(n+ 1)|[|A′i(n+ 1)| − |αiÃ′i(n+ 1)− A′i(n+ 1)|]\n138\nwhich follows in part by applying (4.16) for the left summand and noting that Ã′i is a unit vector for the right summand, giving the bound ‖Ã′i(1 : n)‖ ≤ 1. Continuing with the restriction that ∗ < 1 2\n1√ 1+‖µmax‖2 ,\n‖µi − µ̃i‖ ≤ ∗ √ 1 + ‖µmax‖2 + ∗ √\n1 + ‖µmax‖2[ 1√\n1+‖µmax‖2 − ∗ ] ≤ ∗ (√ 1 + ‖µmax‖2 + 2(1 + ‖µmax‖2) ) .\nThen, in order to guarantee that ‖µi − µ̃i‖ < , it suffices to choose ∗ such that\n∗ (√ 1 + ‖µmax‖2 + 2(1 + ‖µmax‖2) ) ≤ ,\nwhich occurs when\n∗ ≤ (√ 1 + ‖µmax‖2 + 2(1 + ‖µmax‖2) ) . (4.17) As < 1\n2 , the restriction ∗ < 1 2 √ 1 + ‖µ2max‖ holds automatically for the choice of ∗ in\n(4.17). The sample bound from (4.15) contains the dependencyN > poly( 1 ∗ , ‖µmax‖d\n2\n).\nPropagating the error gives a replacement dependency of\nN > poly\n( 1\n, √ 1 + ‖µmax‖2, ‖µmax‖d 2 ) = poly( 1 , ‖µmax‖d 2 )\nas d is non-negative. This propagated dependency is reflected in (4.12).\nDistance of the Sampled Model to the Ideal Model An important part of the reduction is that the coordinates of S are mutually independent. Without the threshold τ , this is true (c.f. Lemma 23). However, without the threshold, one cannot know how to add more noise so that the total noise on each sample is iid. We show\n139\nthat we can choose the threshold τ large enough that the samples still come from a distribution with arbitrarily small total variation distance to the one with truly independent coordinates.\nLemma 26. Fix δ > 0. Let S ∼ Poisson(λ) for λ ≥ ln δ. Let b = eλ, If τ > eλ, τ ≥ 1, and τ ≥ ln(1/δ)− λ, then P (S > τ) < δ.\nProof. By the Chernoff bound (See Theorem A.1.15 in [5]),\nP (S > λ(1 + )) ≤ ( e (1 + )−(1+ ) )λ .\nFor any τ > λ, letting = τ/λ− 1, we get\nP (S > τ) ≤ e −λ(eλ)τ\nτ τ .\nTo get P (S > τ) < δ, it suffices that τ − τ logb τ ≤ logb(δeλ). Note that\nτ(1− logb τ) = τ − τ logb τ = logb ( bτ (1/τ)τ ) .\nIf τ − τ logb τ ≤ logb ( δeλ ) , then we have\nlogb ( bτ (1/τ)τ ) ≤ logb(δeλ)\nwhich then implies it suffices that\nbτ τ τ = (eλ)τ τ τ ≤ λτ/τ τ ≤ (1/e)τ ≤ δeλ\nwhich holds for τ ≥ ln (\n1 δeλ\n) = ln(1/δ)− λ, giving the desired result.\nLemma 27. Let N, δ > 0, N ∈ N, and T1, T2, . . . , TN be iid with distribution Poisson(λ). If τ ≥ ln(N/δ)− λ then\nP (⋃ i {Ti > τ} ) < δ.\n140\nProof. By Lemma 26 τ ≥ ln(N/δ) − λ implies P (Ti > τ) < δ/N for every i. The union bound gives us the desired result.\nWe have now that if we choose our threshold τ large enough, our samples can be statistically close (See Appendix 4.6.7) to ones that would come from the truly independent distribution. This claim is made formal now:\nLemma 28. Fix δ > 0. Let τ > 0. Let F be a poisson distribution with parameter λ and have corresponding density ρF . Let G be a discrete distribution with density ρG(x) = ρF (x)/F (τ) when 0 ≤ x ≤ τ and 0 otherwise. Then dTV (F,G) = 1− F (τ).\nProof. Since we are working with discrete distributions, we can write\ndTV (F,G) = 1\n2 ∞∑ i=0 |f(i)− g(i)|.\nThen we can compute\ndTV (F,G) = |F (τ)− 1|\n2F (τ) τ∑ i=0 ρF (i) + 1 2 ∞∑ i=τ+1 ρF (i) = |F (τ)− 1| 2 + 1− F (τ) 2 = 1− F (τ).\nProof of Theorem 4.0.1 We now show that after the reduction is applied, we can use the ICA routine given in [48] to learn the GMM. Instead of requiring exact values of each parameter, we simply require a bound on each. The algorithm remains polynomial on those bounds, and hence polynomial on the true values.\nProof. The algorithm is provided parameters: Covariance matrix Σ, upper bound on tensor order parameter d, access to samples from a mixture of m identical, spherical Gaussians in Rn with covariance Σ, confidence parameter δ, accuracy parameter ,\n141\nupper bound w ≥ maxi(wi)/mini(wi), upper bound on the norm of the mixture means\nu, lower bound v so 0 < b ≤ σm(A d/2), and r ≥ ( maxi ‖µi‖+ 1)/(mini ‖µi‖) ) .\nThe algorithm then needs to fix the number of samples N , sampling threshold τ , Poisson parameter λ, and two new errors δ1 and δ2 so that δ1 +δ2 ≤ δ. For simplicity,\nwe will take δ1 = δ2 = δ/2. Then fix σ = supv∈Sn−1 √ Var(vTη(1)) for η(1) ∼ N (0,Σ). Recall that B is the matrix whose ith column is µi/‖µi‖. Let A′ be the matrix whose ith column is (µi, 1)/‖(µi, 1)‖.\nStep 1 Assume that after drawing samples from Subroutine 5, the signals Si are mutually independent (as in the “ideal” model given by (4.5)) and the mean matrix B satisfies σm(B d/2) ≥ b > 0. Then by Theorem 4.3.2, with probability of error δ1, the call to UnderdeterminedICA in Algorithm 6 recovers the columns of B to within and up to a permutation using N samples where\nN ≥ p ( τ d 2 ,Θ ) = poly ( nd,md 2 , (τσ)d 2 , ud 2 , wd 2 , dd 2 , rd 2 , 1/bd, 1/ , 1/δ1 ) where p(τ d 2 ,Θ) is the bound on N promised by Theorem 4.3.2 and Θ is all its arguments except the dependence in τ . So then we have that with at least N samples in this “ideal” case, we can recover approximations to the true means in Rn up to a permutation and within distance.\nStep 2 We need to show that after getting N samples from the reduction, the resulting distribution is still close in total variation to the independent one. We will choose a new δ′ = δ2/(2N). Let R ∼ Poisson(λ). Given δ′, Lemma 28 shows that for τ ≥ ln(1/δ′)− λ, with probability 1− δ′, R ≤ τ .\n142\nTake N iid random variables X1, X2, . . . , XN with distribution F = Poisson(λ) and\ndensity ρF . LetG be a distribution with density function ρG(x) = (ρF (x)10≤x≤τ )/F (τ). Let Y1, Y2, . . . , YN be iid random variables with distribution G. Denote the joint distribution of the Xi’s by F ′ and the joint distribution of the Yi’s as G ′ . By the union bound and the fact that total variation distance satisfies the triangle inequality,\ndTV (F ′, G′) ≤ N∑ i=1 dTV (F,G) = NdTV (F,G).\nThen for our choice of τ , by Lemma 26 and Lemma 28, we have\ndTV (F ′, G′) ≤ NdTV (F,G) = NP (X1 > τ) ≤ Nδ′ = δ2/2.\nBy the same union bound argument, the probability that the algorithm fails (when R > τ) is at most δ2/2, since it has to draw N samples. So with high probability, the algorithm does not fail; otherwise, it still does not take more than polynomial time, and will terminate instead of returning a false result.\nStep 3 We know that N is at least a polynomial which can be written in terms of the dependence on τ as p(τ d 2 ,Θ). This means there will be a power of τ which dominates all of the τ factors in p, and in particular, will be τCd 2 for some C. It then\nsuffices to choose C so that p ( τ d 2 ,Θ ) ≤ τCd2q(Θ) ≤ N , where\nq(Θ) = poly ( nd,md 2 , σd 2 , ud 2 , wd 2 , dd 2 , rd 2 , 1/bd, 1/ , 1/δ1 ) . (4.18)\nThen, with the proper choice of τ (to be specified shortly), from step 2 we have\np ( τ d 2 ,Θ ) ≤ τCd2q(Θ) ≤ N = δ2\nδ′ ≤ δ2τ\nτeλ\n(eλ)τ =\nδτ τeλ\n2(eλ)τ .\n143\nSince λ ≥ 1 it suffices to choose τ so that\n2 δ q(Θ)τCd 2 ≤ τ τ τCd2(eλ)τ . (4.19)\nFinally, we claim that\nτ = 4 ( log(2/δ) + log(q(Θ)) ) max ( (eλ)2, 4Cd2 ) = O ( (λ2 + d2) log q(Θ)\nδ ) is enough for the desired bound on the sample size. Observe that 4(log(2/δ) + log(q(Θ))) ≥ 1.\nAn useful fact is that for general x, a, b ≥ 1, x ≥ max(2a, b2) satisfies xa ≤ xx/bx. This captures the essence of our situation nicely. Letting eλ play the role of b, Cd2 play the role of a and x play the role of τ , to satisfy (4.19), it suffices that\n2 δ q(Θ) ≤ τ\nτ/2τ τ/4τ τ/4 τCd2(eλ)2 .\nWe can see that τ τ/2 ≥ (eλ)2 and τ τ/4 ≥ τCd2 by construction. But we also get τ/4 ≥ log(2/δ) + log q(Θ) which implies τ τ/4 ≥ eτ/4 ≥ 2 δ q(Θ). Thus for our choice of τ , which also preserves the requirement in Step 2, there is a corresponding set of choices for N , where the required sample size remains polynomial as\npoly ( nd,md 2 , (τσ)d 2 , ud 2 , wd 2 , dd 2 , rd 2 , 1/bd, 1/ , 1/δ )\nwhere we used the bound q(Θ) ≤ (ndmd2σd2ud2wd2(d+ 1)d2rd2/bdδ1 )O(1)."
    }, {
      "heading" : "4.4 Smoothed Analysis",
      "text" : "In this section, we prove that in high enough dimension, a Gaussian Mixture will satisfy the non-degeneracy conditions discussed above. We start with a base matrix M ∈ Rn×( n 2) and add a perturbation matrix N ∈ Rn×( n 2) with each entry\n144\ncoming iid from N (0, σ2) for some σ > 0. (We restrict the discussion to the second power for simplicity; extension to higher power is straightforward.) As in [48], it will be convenient to work with the multilinear part of the Khatri–Rao product: For a column vector Ak ∈ Rn define A 2k ∈ R( n 2), a subvector of A 2k ∈ Rn 2 , given by (A 2k )ij := (Ak)i(Ak)j for 1 ≤ i < j ≤ n. Then for a matrix A = [A1, . . . , Am] we have A 2 := [A 21 , . . . , A 2 m ].\nTheorem 4.4.1. With the above notation, for any base matrix M with dimensions as above, we have, for some absolute constant C,\nP ( σmin((M +N) 2) ≤ σ 2\nn7\n) ≤ 2C\nn .\nTheorem 4.0.2 follows by noting that σmin(A 2) ≥ σmin(A 2).\nProof. In the following, for a vector space V (over the reals) dist(v, V ′) denotes the distance between vector v ∈ V and subspace V ′ ⊆ V ; more precisely, dist(v, V ′) := minv′∈V ′ ‖v − v′‖2. We will use a lower bound on σmin(A), found in Appendix 4.6.2.\nWith probability 1, the columns of the matrix (M+N) 2 are linearly independent.\nThis can be proved along the lines of a similar result in [48]. Fix k ∈ ( n 2 ) and let u ∈ R( n 2) be a unit vector orthogonal to the subspace spanned by the columns of (M + N) 2 other than column k. Vector u is well-defined with probability 1. Then the distance of the k’th column Ck from the span of the rest of the columns is given\n145\nby\nuTCk = u T (Mk +Nk) 2 = ∑\n1≤i<j≤n\nuij(Mik +Nik)(Mjk +Njk)\n= ∑\n1≤i<j≤n\nuijMikMjk + ∑\n1≤i<j≤n\nuijMikNjk (4.20)\n+ ∑\n1≤i<j≤n\nuijNikMjk + ∑\n1≤i<j≤n\nuijNikNjk\n=: P (N1k, . . . , Nnk). (4.21)\nNow note that this is a quadratic polynomial in the random variables Nik. We will apply the anticoncentration inequality of Carbery–Wright [23] to this polynomial to conclude that the distance between the k’th column of (M + N) 2 and the span of the rest of the columns is unlikely to be very small (see Appendix 4.6.3 for the precise result).\nUsing ‖u‖2 = 1, the variance of our polynomial in (4.21) becomes\nVar (P (N1k, . . . , Nnk)) = σ2 (∑\nj (∑ i:i<j uijMik )2 + ∑ i (∑ j:i<j uijMjk )2) + σ4 ∑ i<j u2ij ≥ σ4.\nIn our application, random variables Nik for i ∈ [n] are not standard Gaussians but are iid Gaussian with variance σ2, and our polynomial does not have unit variance. After adjusting for these differences using the estimate on the variance of P above,\nLemma 32 gives P (|P (N1k, . . . , Nnk)− t| ≤ ) ≤ 2C √ /σ2 = 2C √ /σ. Therefore,\nP (∃k : dist(Ck, C−k) ≤ ) ≤ ( n 2 ) 2C √ /σ by the union bound over the choice of k.\nNow choosing = σ2/n6, Lemma 31 gives P (σmin((M +N) 2) ≤ σ2/n7) ≤ 2C/n.\nWe note that while the above discussion is restricted to Gaussian perturbation, the same technique would work for a much larger class of perturbations. To this end,\n146\nwe would require a version of the Carbery-Wright anticoncentration inequality which is applicable in more general situations. We omit such generalizations here.\nThe curse of low dimensionality for Gaussian mixtures\nIn this section we prove that for small n there is a large class of superpolynomially close mixtures in Rn with fixed variance. This goes beyond the specific example of exponential closeness given in [74] as we demonstrate that such mixtures are ubiquitous as long as there is no lower bound on the separation between the components.\nSpecifically, let S be a cube [0, 1]n ⊂ Rn. We will show that for any two sets of k points X and Y in S, with fill h (we say that X has fill h, if there is a point of X within distance h of any point of S), there exist two mixtures p, q with means on non-overlapping subsets of X ∪ Y , which are (nearly) exponentially close in 1/h in the L1(Rn) norm. Note that the fill of a sample from the uniform distribution on the cube can be bounded (with high probability) by O( log k k1/n ).\nWe start by defining some of the key objects. Let K(x, z) be the unit Gaussian kernel. Let K be the integral operator corresponding to the convolution with a unit\nGaussian: Kg(z) = ∫ RK(x, z)g(x)dx. Let X be any subset of k points in [0, 1] n. Let KX be the kernel matrix corresponding to X, (KX)ij = K(xi, xj). It is known to\nbe positive definite. The interpolant is defined as fX,k(x) = ∑ wiK(xi, x), where the coefficients wi are chosen so that (∀i)fX,k(xi) = f(xi). It is easy to see that such interpolant exists and is unique, obtained by solving a linear system involving KX .\nWe will need some properties of the Reproducing Kernel Hilbert Space H corresponding to the kernel K. In particular, we need the bound ‖f‖∞ ≤ ‖f‖H and the reproducing property, 〈f(·), K(x, ·)〉H = f(x),∀f ∈ H.\n147\nLemma 29. Let g be any positive function with L2 norm 1 supported on [0, 1] n and let f = Kg. If fill X is h, there exists A > 0 such that\n‖f − fX,k‖L∞(Rn) < exp(A log h\nh )\nProof. From [85], Corollary 5.1 (taking λ = 0) we have that for some A > 0 and h sufficiently small\n‖f − fX,k‖L∞([0,1]n) < exp(A log h\nh )\nand\n‖f − fX,k‖L2([0,1]n) < exp(A log h\nh )\nNote that the norm is on [0, 1] while we need to control the norm on R.\nThat, however, does not allow us to directly control the function behavior outside of the interval. To do that we need a bound on the RKHS norm of f − fX,k. We first observe for any xi ∈ X, f(xi)− fX,k(xi) = 0. Thus, from the reproducing property of RKHS, 〈f − fX,k, fX,k〉H = 0. Using properties of RKHS with respect to the operator K (see, e.g., Proposition 10.28 of [109])\n‖f − fX,k‖2H = 〈f − fX,k, f − fX,k〉H\n= 〈f − fX,k, f〉H = 〈f − fX,k,Kg〉H = 〈f − fX,k, g〉L2([0,1]n) ≤ ‖f − fX,k‖L2(X)‖g‖L2(X) < exp(A log h\nh )\nThus\n‖f − fX,k‖L∞(Rn) ≤ ‖f − fX,k‖H < exp(A log h\nh )\n148\nTheorem 4.4.2. Let X and Y be any two subsets of [0, 1]n with fill h. Then there exist two Gaussian mixtures p and q (with positive coefficients summing to one, but not necessarily the same number of components), which are centered on two nonintersecting subsets of X ∪ Y and such that\n‖p− q‖L1(Rn) < exp(B log h\nh )\nfor some constant B > 0.\nProof. To simplify the notation we assume that n = 1. The general case follows verbatim, except that the interval of integration, [−1/h, 1/h], and its complement need to be replaced by the sphere of radius 1/h and its complement respectively.\nLet fX,k and fY,k be the interpolants as above. We see that ‖fX,k − fY,k‖L∞(R) <\n2 exp(A log h h ). fX,k and fY,k are both linear combinations of Gaussians possibly with negative coefficients and so is fX,k − fY,k . By collecting positive and negative coefficients we write fX,k − fY,k = p1 − p2, where, p1 and p2 are mixtures with positive coefficients only.\nPut p1 = ∑ i∈S1 αiK(xi, x), p2 = ∑ i∈S2 βiK(xi, x), where S1 and S2 are nonoverlapping subsets of X ∪ Y . Now we need to ensure that the coefficients can be normalized to sum to 1.\nLet α = ∑ αi, β = ∑ βi. By integrating over the interval [0, 1], and since f is strictly positive on the interval, it is easy to see that α > C (and by the same token β > C), where C is some universal constant. We have\n|α− β| = ∣∣∣∣∫\nR p1(x)− p2(x)dx ∣∣∣∣ ≤ ‖p1 − p2‖L1 ‖p1 − p2‖L1 ≤ ∫ [−1/h,1/h] ‖fX,k − fY,k‖L∞(R)dx+ (α + β) ∫ x/∈[−1/h,1/h] K(0, x− 1)dx\n149\nNoticing that the first summand is bounded by 2 h exp(A log h h ) and the integral in the second summand is smaller than e−1/h for h sufficiently small, it follows immediately, that |1− β α | < exp(A′ log h h ) for some A′ and h sufficiently small.\nHence, we have∥∥∥∥ 1αp1 − 1βp2 ∥∥∥∥ L1 ≤ 1 C ∥∥∥∥βαp1 − p2 ∥∥∥∥ L1 ≤ 1 C ∣∣∣∣1− βα ∣∣∣∣ ‖p1‖L1 + 1C ‖p1 − p2‖L1 Collecting exponential inequalities completes the proof.\nTheorem 4.0.3. For convenience we will use a set of 4k2 points instead of k2. Clearly it does not affect the exponential rate. By a simple covering set argument (cutting the cube into mn cubes with size 1/m) and basic probability (the coupon collector’s problem), we see that the fill h of 2nmn logm points is at most O( √ n/m) with probability 1−o(1). Hence, given k points, we have h = O( √ n( log k\nk )1/n). We see that\nwith a smaller probability (but still close to 1 for large k), we can sample k points 4k times and still have the same fill on each group of k.\nPairing the sets of k points into 2k pairs of sets arbitrarily and applying Theorem 4.4.2 (to k+k points) we obtain 2k pairs of exponentially close mixtures with at most 2k components each. If one of the pairs has the same number of components, we are done. If not, by the pigeon-hole principle for at least two pairs of mixtures p1 ≈ q1 and p2 ≈ q2 the differences of the number of components (an integer number between 0 and 2k − 2) must coincide. Assume without loss of generality that p1 has no more components than q1 and p2 has no more components than q2.Taking p = 1 2 (p1 + q2) and q = 1 2 (p2 + q1) completes the proof.\n150"
    }, {
      "heading" : "4.5 Recovery of Gaussian Weights",
      "text" : "Multivariate cumulant tensors and their properties. Our technique for the recovery of the Gaussian weights relies on the tensor properties of multivariate cumulants that have been used in the ICA literature.\nGiven a random vector Y ∈ Rn, the moment generating function of Y is defined as MY (t) := EY (exp(tTY )). The cumulant generating function is the logarithm of the moment generating function: gY (t) := log(EY (exp(tTY )).\nSimilarly to the univariate case, multivariate cumulants are defined using the coefficients of the Taylor expansion of the cumulant generating function. We use both κj1,...,j`Y and κ(Yj1 , . . . , Yj`) to denote the order-` cross cumulant between the random variables Yj1 , Yj2 , . . . , Yj` . Then, the cross-cumulants κ j1,...,j` Y are given by the formula κj1,...,j`Y = ∂ ∂tj1 · · · ∂ ∂tj` gY (t) ∣∣ t=0 . When unindexed, κY will denote the full order-` tensor containing all cross-cumulants, with the order of the tensor being made clear by context. In the special case where j1 = · · · = j` = j, we obtain the order-` univariate cumulant κ`(Yj) = κ j,...,j Y (j repeated ` times) previously defined. We will use some well known properties of multivariate cumulants, found in Appendix 4.6.1.\nThe most theoretically justified ICA algorithms have relied on the tensor structure of multivariate cumulants, including the early, popular practical algorithm JADE [25]. In the fully determined ICA setting in which the number source signals does not exceed the ambient dimension, the papers [14] and [17] demonstrate that ICA with additive Gaussian noise can be solved in polynomial time and using polynomial samples. The tensor structure of the cumulants was (to the best of our knowledge) first exploited in [24] and later in [4] to solve underdetermined ICA. Finally, [48] provides\n151\nan algorithm with rigorous polynomial time and sampling bounds for underdetermined ICA in the presence of Gaussian noise.\nWeight recovery (main idea). Under the basic ICA reduction (see section 4.2) using the Poisson distribution with parameter λ, we have that X = AS+η is observed such that A = [µ1| · · · |µm] and Si ∼ Poisson(wiλ). As A has already been recovered, what remains to be recovered are the weights w1, · · · , wm. These can be recovered using the tensor structure of higher order cumulants. The critical relationship is captured by the following Lemma:\nLemma 30. Suppose that X = AS + η gives a noisy ICA model. When κX is of order ` > 2, then vec (κX) = A `(κ`(S1), . . . , κ`(Sm)) T .\nProof. It is easily seen that the Gaussian component has no effect on the cumulant:\nκX = κAS+η = κAS + κη = κAS\nThen, we expand κX :\nκi1,··· ,i`X = κ i1,··· ,i` AS = κ((AS)i1 , · · · , (AS)i`)\n= κ ( m∑ j1=1 Ai1j1Sj1 , · · · , m∑ j`=1 Ai`j`Sj` )\n= ∑\nj1,··· ,j`∈[m]\n(∏̀ k=1 Aikjk ) κ(Sj1 , · · · , Sj`) by multilinearity\nBut, by independence, κ(Sj1 , · · · , Sjm) = 0 whenever j1 = j2 = · · · = j` fails to hold. Thus,\nκi1,··· ,i`X = m∑ j=1 (∏̀ k=1 Aikj ) κ`(Sj) = m∑ j=1 ( (Aj) ⊗`) i1,··· ,i` κ`(Sj)\nFlattening yields: vec (κX) = A `(κ`(S1), · · · , κ`(Sm))T .\n152\nIn particular, we have that Si ∼ Poisson(wiλ) with wi the probability of sampling from the ith Gaussian. Given knowledge of A and the cumulants of the Poisson distribution, we can recover the Gaussian weights.\nTheorem 4.5.1. Suppose that X = AS + η(τ) is the unrestricted noisy ICA model from the basic reduction (see section 4.2). Let ` > 2 be such that A ` has linearly independent columns, and let (A `)† be its Moore-Penrose pseudoinverse. Let κX be of order `. Then 1 λ (A `)†vec (κX) is the vector of mixing weights (w1, . . . , wm) T of the Gaussian mixture model.\nProof. From Lemma 37, κ`(Si) = λwi. Then Lemma 30 implies that vec (κX) = λA `(w1, . . . , wm) T . Multiplying on the left by 1 λ (A `)† gives the result."
    }, {
      "heading" : "4.6 Addendum",
      "text" : ""
    }, {
      "heading" : "4.6.1 Properties of Cumulants",
      "text" : "The following properties of multivariate cumulants are well known and are largely\ninherited from the definition of the cumulant generating function:\n• (Symmetry) Let σ give a permutation of k indices. Then, κi1,··· ,i`Y = κ σ(i1),··· ,σ(i`) Y .\n• (Multilinearity of coordinate random variables) Given constants α1, · · · , α`,\nthen\nκ(α1Yi1 , · · · , α`Yi`) = (∏̀ i=1 αi ) κ(Yi1 , · · · , Yi`) .\nAlso, given a scalar random variable Z, then\nκ(Yi1 + Z, Yi2 , · · · , Yi`) = κ(Yi1 , Yi2 , · · · , Yi`) + κ(Z, Yi2 , · · · , Yi`)\nwith symmetry implying the additive multilinear property for all other coordinates.\n153\n• (Independence) If there exists ij, ik such that Yij and Yik are independent ran-\ndom variables, then the cross-cumulant κi1,··· ,i`Y = 0. Combined with multilinearity, it follows that when there are two independent random vectors Y and Z, then κY+Z = κY + κZ .\n• (Vanishing Gaussians) When ` ≥ 3, then for the Gaussian random variable η,\nκη = 0."
    }, {
      "heading" : "4.6.2 Rudelson-Vershynin subspace bound",
      "text" : "Lemma 31 (Rudelson–Vershynin [90]). If A ∈ Rn×m has columns C1, . . . , Cm, then denoting C−i = span (Cj : j 6= i), we have\n1√ m min i∈[m] dist(Ci, C−i) ≤ σmin(A),\nwhere as usual σmin(A) = σmin(m,n)(A)."
    }, {
      "heading" : "4.6.3 Carbery-Wright anticoncentration",
      "text" : "The version of the anticoncentration inequality we use is explicitly given in [75]\nwhich in turn follows immediately from [23]:\nLemma 32 ([75]). Let Q(x1, . . . , xn) be a multilinear polynomial of degree d. Suppose that Var (Q) = 1 when xi ∼ N (0, 1) for all i. Then there exists an absolute constant C such that for t ∈ R and > 0,\nPr (x1,...,xn)∼N (0,In)\n(|Q(x1, . . . , xn)− t| ≤ ) ≤ Cd 1/d.\n154"
    }, {
      "heading" : "4.6.4 Lemmas on the Poisson Distribution",
      "text" : "The following lemmas are well-known; see, e.g., [35]. We provide proofs for com-\npleteness.\nLemma 33. If X ∼ Poisson(λ) and Y |X=x ∼ Bin(x, p) then Y ∼ Poisson(pλ).\nProof.\nP (Y = y) = ∞∑\nx:x≥y\nP (Y = y : X = x)P (X = x)\n= ∞∑\nx:x≥y\n( x\ny\n) py(1− p)x−yλ xe−λ\nx!\n= pye−λ ∞∑\nx:x≥y\nλx x!\n( x\ny\n) (1− p)x−y\n= (pλ)ye−λ\ny!\n∞∑ x:x≥y (λ(1− p))x−y (x− y)!\n= (pλ)ye−λ\ny! e(1−p)λ\n= (pλ)ye−pλ\ny! .\nLemma 34. Fix a positive integer k, and let pi ≥ 0 be such that p1 + · · · + pk = 1. If X ∼ Poisson(λ) and (Y1, . . . , Yk)|X=x ∼ Multinom(x; p1, . . . , pk) then Yi ∼ Poisson(piλ) for all i and Y1, . . . , Yk are mutually independent.\nProof. The first part of the lemma (i.e., Yi ∼ Poisson(piλ) for all i) follows from Lemma 33. For the second part, let’s prove it for the binomial case (k = 2); the\n155\ngeneral case is similar.\nP (Y1 = y1, Y2 = y2) = P (Y1 = y1, Y2 = y2 : X = y1 + y2)P (X = y1 + y2)\n= ( y1 + y2 y1 ) py1(1− p)y2 · λ y1+y2e−λ (y1 + y2)! = (pλ)y1e−pλ\ny1! · ((1− p)λ) y2e−(1−p)λ y2!\n= P (Y1 = y1) · P (Y2 = y2) ."
    }, {
      "heading" : "4.6.5 Bounds on Stirling Numbers of the Second Kind",
      "text" : "The following bound comes from [84] Theorem 3.\nLemma 35. If n ≥ 2 and 1 ≤ r ≤ n− 1 are integers, then { n r } ≤ 1 2 ( n r ) rn−r.\nFrom this, we can derive a somewhat looser bound on the Stirling numbers of the\nsecond kind which does not depend on r: Lemma 36. If n, r ∈ Z+ such that r ≤ n, then { n r } ≤ nn−1. Proof. The Stirling number { n k } of the second kind gives a count of the number of ways of splitting a set of n labeled objects into k unlabeled subsets. In the case where\nr = n, then { n r } = 1 As n ≥ 1, it is clear that for these choices of n and r, { n r } ≤ nn−1.\nBy the restriction 1 ≤ r ≤ n, when n = 1, then n = r giving that { n r } = 1. As such, the only remaining cases to consider are when n ≥ 2 and 1 ≤ r ≤ n − 1, the cases where Lemma 35 applies.\nWhen n ≥ 2 and 1 ≤ r ≤ n− 1, then{ n\nr\n} ≤ 1\n2\n( n\nr\n) rn−r = 1\n2\nn! r!(n− r)! rn−r ≤ 1 2 nrrn−r−1 < 1 2 nrnn−r−1 = 1 2 nn−1 ,\nwhich is slightly stronger than the desired upper bound.\n156"
    }, {
      "heading" : "4.6.6 Values of Higher Order Statistics",
      "text" : "In this appendix, we gather together some of the explicit values for higher order statistics of the Poisson and Normal distributions required for the analysis of our reduction from learning a Gaussian Mixture Model to learning an ICA model from samples.\nLemma 37 (Cumulants of the Poisson distribution). Let X ∼ Poisson(λ). Then, κ`(X) = λ for every positive integer `.\nProof. The moment generating function of the Poisson distribution is given byM(t) = exp(λ(et−1)). The cumulant generating function is thus g(t) = log(M(t)) = λ(et−1). The `th derivative (` ≥ 1) is given by g(`)(t) = λet.\nBy definition, κ`(X) = g (`)(0) = λ.\nLemma 38 (Absolute moments of the Gaussian distribution). For a Gaussian random variable η ∼ N(0, σ2), the absolute moments of η are given by:\nE|η|` = σ ` `! 2 ` 2 ( ` 2 )!\nif ` is even\nσ`2 ` 2 ( `−1 2 )! 1√ π if ` is odd.\nThe case that ` is even in Lemma 38 is well known, and can be found for instance\nin [61, Section 3.4]. For general `, it is known (see [110]) that\nE|η|` = σ`2 ` 2 Γ\n( `+ 1\n2 ) 1√ π .\nWhen ` is odd, `+1 2 is an integer, allowing the Gamma function to simplify to a\nfactorial: Γ ( `+1\n2\n) = ( `−1\n2\n) !. This gives the case where ` is odd in Lemma 38."
    }, {
      "heading" : "4.6.7 Total Variation Distance",
      "text" : "Total variation is a type of statistical distance metric between probability distributions. In words, the total variation between two measures is the largest difference\n157\nbetween the measures on a single event. Clearly, this distance is bounded above by 1.\nFor probability measures F and G on a sample space Ω with sigma-algebra Σ, the\ntotal variation is denoted and defined as:\ndTV (F,G) := sup A∈Σ |F (A)−G(A)|.\nEquivalently, when F and G are distribution functions having densities f and g,\nrespectively,\ndTV (F,G) = 1\n2 ∫ Ω |f − g|dµ\nwhere µ is an arbitrary positive measure for which F and G are absolutely continuous.\nMore specifically, when F and G are discrete distributions with known densities,\nwe can write\ndTV (F,G) = 1\n2 ∞∑ k=0 |f(k)− g(k)|\nwhere we choose µ that simply assigns unit measure to each atom of Ω (in this case, absolute continuity is trivial since µ(A) = 0 only when A is empty and thus F (A) must also be 0). For more discussion, one can see Definition 15.3 in [77] and Sect. 11.6 in [88].\n158\nBibliography\n[1] D. Achlioptas and F. McSherry. On spectral learning of mixture of distributions. In The 18th Annual Conference on Learning Theory, 2005. [2] Radoslaw Adamczak, Alexander Litvak, Alain Pajor, and Nicole TomczakJaegermann. Quantitative estimates of the convergence of the empirical covariance matrix in logconcave ensembles. J. Amer. Math. Soc., 233:535–561, 2011. [3] Rados law Adamczak, Alexander E. Litvak, Alain Pajor, and Nicole TomczakJaegermann. Quantitative estimates of the convergence of the empirical covariance matrix in log-concave ensembles. J. Amer. Math. Soc., 23(2):535–561, 2010. [4] Laurent Albera, Anne Ferréol, Pierre Comon, and Pascal Chevalier. Blind identification of overcomplete mixtures of sources (biome). Linear algebra and its applications, 391:3–30, 2004. [5] Noga Alon and Joel H Spencer. The probabilistic method. Wiley, 2004. [6] Shun Amari, Andrzej Cichocki, Howard H Yang, et al. A new learning algorithm for blind signal separation. Advances in neural information processing systems, pages 757–763, 1996. [7] Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. CoRR, abs/1210.7559, 2012. [8] Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, and James Voss. The more, the merrier: the blessing of dimensionality for learning large gaussian mixtures. In Proceedings of The 27th Conference on Learning Theory, pages 1135–1164, 2014. [9] Joseph Anderson, Navin Goyal, Anupama Nandi, and Luis Rademacher. Analogues of the covariance matrix for ica. Thirty-First AAAI Conference on Artificial Intelligence (AAAI 2017).\n159\n[10] Joseph Anderson, Navin Goyal, Anupama Nandi, and Luis Rademacher. Heavy-tailed independent component analysis. 56th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2015), 2015. arXiv preprint arXiv:1509.00727. [11] Joseph Anderson, Navin Goyal, and Luis Rademacher. Efficient learning of simplices. In Conference on Learning Theory, pages 1020–1045, 2013. [12] S. Arora and R. Kannan. Learning Mixtures of Arbitrary Gaussians. In 33rd ACM Symposium on Theory of Computing, 2001. [13] Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with unknown Gaussian noise, and implications for Gaussian mixtures and autoencoders. In NIPS, 2012. arXiv:1206.5349. [14] Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ICA with unknown gaussian noise, with implications for gaussian mixtures and autoencoders. In NIPS, pages 2384–2392, 2012. [15] Guillaume Aubrun. Sampling convex bodies: a random matrix approach. Proc. Amer. Math. Soc., 135(5):1293–1303 (electronic), 2007. [16] F. Barthe, O. Guédon, S. Mendelson, and A. Naor. A probabilistic approach to the geometry of the `np -ball. The Annals of Probability, 33(2):480–513, 2005. [17] Mikhail Belkin, Luis Rademacher, and James Voss. Blind signal separation in the presence of Gaussian noise. In JMLR W&CP, volume 30: COLT, pages 270–287, 2013. [18] Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In FOCS, pages 103–112, 2010. [19] Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed analysis of tensor decompositions. CoRR, abs/1311.3651, 2013. [20] Patrick Billingsley. Probability and measure. Wiley Series in Probability and Mathematical Statistics. John Wiley & Sons Inc., New York, third edition, 1995. A Wiley-Interscience Publication. [21] Jean Bourgain. Random points in isotropic convex sets. In Convex geometric analysis (Berkeley, CA, 1996), volume 34 of Math. Sci. Res. Inst. Publ., pages 53–58. Cambridge Univ. Press, Cambridge, 1999. [22] S. Charles Brubaker and Santosh Vempala. Isotropic PCA and affine-invariant clustering. In 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2008, October 25-28, 2008, Philadelphia, PA, USA, pages 551–560, 2008.\n160\n[23] Anthony Carbery and James Wright. Distributional and Lq norm inequalities for polynomials over convex bodies in Rn. Mathematical Research Letters, 8:233–248, 2001. [24] J-F Cardoso. Super-symmetric decomposition of the fourth-order cumulant tensor. blind identification of more sources than sensors. In Acoustics, Speech, and Signal Processing, 1991. ICASSP-91., 1991 International Conference on, pages 3109–3112. IEEE, 1991. [25] J.-F. Cardoso and A. Souloumiac. Blind beamforming for non-gaussian signals. In Radar and Signal Processing, IEE Proceedings F, volume 140, pages 362–370, 1993. [26] J.F. Cardoso. Source separation using higher order moments. In International Conference on Acoustics, Speech, and Signal Processing, 1989. [27] Kamalika Chaudhuri and Satish Rao. Beyond gaussians: Spectral methods for learning mixtures of heavy-tailed distributions. In 21st Annual Conference on Learning Theory - COLT 2008, Helsinki, Finland, July 9-12, 2008, pages 21–32, 2008. [28] Aiyou Chen and Peter J. Bickel. Robustness of prewhitening against heavytailed sources. In Independent Component Analysis and Blind Signal Separation, Fifth International Conference, ICA 2004, Granada, Spain, September 22-24, 2004, Proceedings, pages 225–232, 2004. [29] Aiyou Chen and Peter J Bickel. Consistent independent component analysis and prewhitening. Signal Processing, IEEE Transactions on, 53(10):3625–3632, 2005. [30] Ying Chen, Wolfgang Hrdle, and Vladimir Spokoiny. Portfolio value at risk based on independent component analysis. Journal of Computational and Applied Mathematics, 205(1):594 – 607, 2007. [31] P. Comon. Independent Component Analysis, a new concept? Signal Processing, Elsevier, 36(3):287–314, April 1994. Special issue on Higher-Order Statistics. hal-00417283. [32] Pierre Comon. Independent component analysis, a new concept? Signal processing, 36(3):287–314, 1994. [33] Pierre Comon and Christian Jutten, editors. Handbook of Blind Source Separation. Academic Press, 2010. [34] Pierre Comon and Christian Jutten, editors. Handbook of Blind Source Separation. Academic Press, 2010.\n161\n[35] A. Dasgupta. Probability for Statistics and Machine Learning. Springer, 2011. [36] Anirban Dasgupta, John E. Hopcroft, Jon M. Kleinberg, and Mark Sandler. On learning mixtures of heavy-tailed distributions. In 46th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2005), 23-25 October 2005, Pittsburgh, PA, USA, Proceedings, pages 491–500, 2005. [37] S. Dasgupta. Learning Mixture of Gaussians. In 40th Annual Symposium on Foundations of Computer Science, 1999. [38] S. Dasgupta and L. Schulman. A Two Round Variant of EM for Gaussian Mixtures. In 16th Conference on Uncertainty in Artificial Intelligence, 2000. [39] Nathalie Delfosse and Philippe Loubaton. Adaptive blind separation of independent sources: A deflation approach. Signal Processing, 45(1):59 – 83, 1995. [40] Kevin D. Donohue. http://www.engr.uky.edu/~donohue/audio/Data/ audioexpdata.htm, 2009. Accessed: 2016-05-01. [41] Martin E. Dyer, Alan M. Frieze, and Ravi Kannan. A random polynomial time algorithm for approximating the volume of convex bodies. J. ACM, 38(1):1–17, 1991. [42] J. Feldman, R. A. Servedio, and R. O’Donnell. PAC Learning Axis Aligned Mixtures of Gaussians with No Separation Assumption. In The 19th Annual Conference on Learning Theory, 2006. [43] Alan M. Frieze, Mark Jerrum, and Ravi Kannan. Learning linear transformations. In FOCS, pages 359–368, 1996. [44] Richard J Gardner. Geometric tomography, volume 58. Cambridge University Press Cambridge, 1995. [45] A. Giannopoulos, M. Hartzoulaki, and A. Tsolomitis. Random points in isotropic unconditional convex bodies. J. London Math. Soc. (2), 72(3):779–798, 2005. [46] Gene H. Golub and Charles F. Van Loan. Matrix computations. Johns Hopkins Studies in the Mathematical Sciences. Johns Hopkins University Press, Baltimore, MD, third edition, 1996. [47] Navin Goyal and Luis Rademacher. Learning convex bodies is hard. In COLT, 2009. [48] Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier PCA and robust tensor decomposition. In Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 584–593, 2014.\n162\n[49] Nick Gravin, Jean Lasserre, Dmitrii V. Pasechnik, and Sinai Robins. The inverse moment problem for convex polytopes. Discrete & Computational Geometry, 48(3):596–621, 2012. [50] M. Grötschel, L. Lovász, and A. Schrijver. Geometric Algorithms and Combinatorial Optimization. Springer, 1988. [51] A. Grundmann and H. M. Moeller. Invariant integration formulas for the nsimplex by combinatorial methods. SIAM J. Numer. Anal., 15:282–290, 1978. [52] A. K. Gupta and D. Song. Lp-norm spherical distribution. J. Statist. Plann. Inference, 60(2):241–260, 1997. [53] Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012. [54] Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: moment methods and spectral decompositions. In ITCS, pages 11–20, 2013. [55] P. J. Huber and Elvezio Ronchetti. Robust Statistics. John Wiley, second edition, 2009. [56] Aapo Hyvärinen. Fast and robust fixed-point algorithms for independent component analysis. IEEE Transactions on Neural Networks, 10(3):626–634, 1999. [57] Aapo Hyvarinen, Juha Karhunen, and Erkki Oja. Independent Component Analysis. John Wiley and Sons, 2001. [58] Aapo Hyvärinen, Juha Karhunen, and Erkki Oja. Independent Component Analysis. Wiley, 2001. [59] R. Kannan, L. Lovász, and M. Simonovits. Isoperimetric problems for convex bodies and a localization lemma. Discrete Comput. Geom., 13(3-4):541–559, 1995. [60] Ravi Kannan, László Lovász, and Miklós Simonovits. Random walks and an O∗(n5) volume algorithm for convex bodies. Random Structures Algorithms, 11(1):1–50, 1997. [61] Maurice Kendall, Alan Stuart, and J. Keith Ord. Kendall’s advanced theory of statistics. Vol. 1. Halsted Press, sixth edition, 1994. Distribution theory. [62] Preben Kidmose. Independent component analysis using the spectral measure for alpha-stable distributions. In Proceedings of IEEE-EURASIP Workshop on Nonlinear Signal and Image Processing, NSIP 2001.\n163\n[63] Preben Kidmose. Alpha-stable distributions in signal processing of audio signals. In 41st Conference on Simulation and Modelling, Scandinavian Simulation Society, pages 87–94, 2000. [64] Preben Kidmose. Blind Separation of Heavy Tail Signals. PhD thesis, Technical University of Denmark, 2001. [65] Adam R. Klivans, Ryan O’Donnell, and Rocco A. Servedio. Learning geometric concepts via Gaussian surface area. In FOCS, pages 541–550, 2008. [66] Adam R. Klivans and Alexander A. Sherstov. A lower bound for agnostically learning disjunctions. In COLT, pages 409–423, 2007. [67] Adam R. Klivans and Alexander A. Sherstov. Cryptographic hardness for learning intersections of halfspaces. J. Comput. Syst. Sci., 75(1):2–12, 2009. [68] Stephen Kwek and Leonard Pitt. PAC learning intersections of halfspaces with membership queries. Algorithmica, 22(1/2):53–75, 1998. [69] Jean B. Lasserre and Konstantin E. Avrachenkov. The multi-dimensional version of ∫ abxpdx. American Math. Month., 108(2):151–154, 2001. [70] László Lovász and Santosh Vempala. Simulated annealing in convex bodies and\nan O*(n4) volume algorithm. J. Comput. Syst. Sci., 72(2):392–417, 2006.\n[71] David G. Luenberger and Yinyu Ye. Linear and nonlinear programming. International Series in Operations Research & Management Science, 116. Springer, New York, third edition, 2008. [72] P. McMullen. On zonotopes. Trans. Amer. Math. Soc., 159:91–109, 1971. [73] V. D. Milman and A. Pajor. Isotropic position and inertia ellipsoids and zonoids of the unit ball of a normed n-dimensional space. In Geometric aspects of functional analysis (1987–88), volume 1376 of Lecture Notes in Math., pages 64–104. Springer, Berlin, 1989. [74] A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In 51st Annual IEEE Symposium on Foundations of Computer Science (FOCS 2010), 2010. [75] Elchanan Mossel, Ryan O’Donnell, and Krzysztof Oleszkiewicz. Noise stability of functions with low influences: Invariance and optimality. Annals of Math., 171:295–341, 2010. [76] Phong Q. Nguyen and Oded Regev. Learning a parallelepiped: Cryptanalysis of GGH and NTRU signatures. J. Cryptology, 22(2):139–160, 2009.\n164\n[77] Ole A Nielsen. An Introduction to Integration Theory and Measure Theory. Wiley, 1997. [78] J. P. Nolan. Stable Distributions - Models for Heavy Tailed Data. Birkhauser, Boston, 2015. [79] Asa Packer. NP-hardness of largest contained and smallest containing simplices for V- and H-polytopes. Discrete & Computational Geometry, 28(3):349–377, 2002. [80] G. Paouris. Concentration of mass on convex bodies. Geom. Funct. Anal., 16(5):1021–1049, 2006. [81] C. M. Petty. Centroid surfaces. Pacific J. Math., 11(4):1535–1547, 1961. [82] S. T. Rachev. Handbook of Heavy Tailed Distributions in Finance, Volume 1: Handbooks in Finance, Book 1. Elsevier, 2003. [83] S.T. Rachev and L. Ruschendorf. Approximate independence of distributions on spheres and their stability properties. The Annals of Probability, 19(3):1311– 1337, 1991. [84] B.C. Rennie and A.J. Dobson. On Stirling numbers of the second kind. Journal of Combinatorial Theory, 7(2):116 – 121, 1969. [85] Christian Rieger and Barbara Zwicknagl. Sampling inequalities for infinitely smooth functions, with applications to interpolation and machine learning. Advances in Computational Mathematics, 32(1):103–129, 2010. [86] John Riordan. Moment recurrence relations for binomial, poisson and hypergeometric frequency distributions. Annals of Mathematical Statistics, 8:103–111, 1937. [87] R.T. Rockafellar and S.P. Uryasev. Optimization of conditional value-at-risk. The Journal of Risk, (1):21–41, 2000. [88] Halsey Lawrence Royden, Patrick Fitzpatrick, and Prentice Hall. Real analysis, volume 4. Prentice Hall New York, 1988. [89] M. Rudelson. Random vectors in the isotropic position. J. Funct. Anal., 164(1):60–72, 1999. [90] Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix. Comm. Pure Appl. Math., 62(12):1707–1739, 2009.\n165\n[91] M. Sahmoudi, K. Abed-Meraim, M. Lavielle, E. Kuhn, and Ph. Ciblat. Blind source separation of noisy mixtures using a semi-parametric approach with application to heavy-tailed signals. In Proc. of EUSIPCO 2005. [92] G. Schechtman and J. Zinn. On the volume of the intersection of two lnp balls. In Proc. Amer. Math. Soc, volume 110, pages 217–224, 1990. [93] Rolf Schneider. Convex bodies: the Brunn-Minkowski theory, volume 44 of Encyclopedia of Mathematics and its Applications. Cambridge University Press, Cambridge, 1993. [94] Bernhard Schölkopf, John C. Platt, John Shawe-Taylor, Alex J. Smola, and Robert C. Williamson. Estimating the support of a high-dimensional distribution. Neural Computation, 13(7):1443–1471, 2001. [95] Yoav Shereshevski, Arie Yeredor, and Hagit Messer. Super-efficiency in blind signal separation of symmetric heavy-tailed sources. In Statistical Signal Processing, 2001. Proceedings of the 11th IEEE Signal Processing Workshop on, pages 78–81. IEEE, 2001. [96] Fabian Sinz and Matthias Bethge. Lp-nested symmetric distributions. J. Mach. Learn. Res., 11:3409–3451, 2010. [97] Fabian H. Sinz and Matthias Bethge. The conjoint effect of divisive normalization and orientation selectivity on redundancy reduction. In NIPS, pages 1521–1528, 2008. [98] D. Song and A. K. Gupta. Lp-norm uniform distribution. Proc. Amer. Math. Soc., 125(2):595–601, 1997. [99] G. Sonnevend. Applications of analytic centers for the numerical solution of semiinfinite, convex programs arising in control theory. DFG report Nr. 170/1989, Univ. Würzburg, Inst. f. angew. Mathematik, 1989. [100] Nikhil Srivastava and Roman Vershynin. Covariance estimation for distributions with 2 + moments, 2011. [101] Nikhil Srivastava and Roman Vershynin. Covariance estimation for distributions with 2 + ε moments. Ann. Probab., 41(5):3081–3111, 2013. [102] Gilbert W Stewart and Ji-guang Sun. Matrix perturbation theory. 1990. [103] S. Vempala and G. Wang. A Spectral Algorithm for Learning Mixtures of Distributions. In 43rd Annual Symposium on Foundations of Computer Science, 2002.\n166\n[104] Santosh Vempala. Learning convex concepts from Gaussian distributions with pca. In FOCS, pages 541–550, 2010. [105] Santosh Vempala. Learning convex concepts from Gaussian distributions with PCA. In FOCS, pages 124–130, 2010. [106] Santosh S. Vempala and Ying Xiao. Structure from local optima: Learning subspace juntas via higher order PCA. CoRR, abs/1108.3329, 2011. [107] Roman Vershynin. How close is the sample covariance matrix to the actual covariance matrix? J. Theoret. Probab., 25(3):655–686, 2012. [108] Baijie Wang, Ercan E Kuruoglu, and Junying Zhang. Ica by maximizing nonstability. In Independent Component Analysis and Signal Separation, pages 179–186. Springer, 2009. [109] Holger Wendland. Scattered data approximation, volume 17. Cambridge University Press Cambridge, 2005. [110] A. Winkelbauer. Moments and Absolute Moments of the Normal Distribution. ArXiv e-prints, September 2012. [111] Arie Yeredor. Blind source separation via the second characteristic function. Signal Processing, 80(5):897–902, 2000.\n167"
    } ],
    "references" : [ {
      "title" : "On spectral learning of mixture of distributions",
      "author" : [ "D. Achlioptas", "F. McSherry" ],
      "venue" : "The 18th Annual Conference on Learning Theory,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Quantitative estimates of the convergence of the empirical covariance matrix in logconcave ensembles",
      "author" : [ "Radoslaw Adamczak", "Alexander Litvak", "Alain Pajor", "Nicole Tomczak- Jaegermann" ],
      "venue" : "J. Amer. Math. Soc.,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Quantitative estimates of the convergence of the empirical covariance matrix in log-concave ensembles",
      "author" : [ "Rados  law Adamczak", "Alexander E. Litvak", "Alain Pajor", "Nicole Tomczak- Jaegermann" ],
      "venue" : "J. Amer. Math. Soc.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Blind identification of overcomplete mixtures of sources (biome)",
      "author" : [ "Laurent Albera", "Anne Ferréol", "Pierre Comon", "Pascal Chevalier" ],
      "venue" : "Linear algebra and its applications,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2004
    }, {
      "title" : "The probabilistic method",
      "author" : [ "Noga Alon", "Joel H Spencer" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "A new learning algorithm for blind signal separation",
      "author" : [ "Shun Amari", "Andrzej Cichocki", "Howard H Yang" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1996
    }, {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "Anima Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M. Kakade", "Matus Telgarsky" ],
      "venue" : "CoRR, abs/1210.7559,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "The more, the merrier: the blessing of dimensionality for learning large gaussian mixtures",
      "author" : [ "Joseph Anderson", "Mikhail Belkin", "Navin Goyal", "Luis Rademacher", "James Voss" ],
      "venue" : "In Proceedings of The 27th Conference on Learning Theory,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Analogues of the covariance matrix for ica",
      "author" : [ "Joseph Anderson", "Navin Goyal", "Anupama Nandi", "Luis Rademacher" ],
      "venue" : "Thirty-First AAAI Conference on Artificial Intelligence (AAAI",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2017
    }, {
      "title" : "Heavy-tailed independent component analysis",
      "author" : [ "Joseph Anderson", "Navin Goyal", "Anupama Nandi", "Luis Rademacher" ],
      "venue" : "56th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2015),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Efficient learning of simplices",
      "author" : [ "Joseph Anderson", "Navin Goyal", "Luis Rademacher" ],
      "venue" : "In Conference on Learning Theory, pages 1020–1045,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Learning Mixtures of Arbitrary Gaussians",
      "author" : [ "S. Arora", "R. Kannan" ],
      "venue" : "33rd ACM Symposium on Theory of Computing,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Provable ICA with unknown Gaussian noise, and implications for Gaussian mixtures and autoencoders",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Ankur Moitra", "Sushant Sachdeva" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Provable ICA with unknown gaussian noise, with implications for gaussian mixtures and autoencoders",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Ankur Moitra", "Sushant Sachdeva" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Sampling convex bodies: a random matrix approach",
      "author" : [ "Guillaume Aubrun" ],
      "venue" : "Proc. Amer. Math. Soc.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "A probabilistic approach to the geometry of the  `p -ball",
      "author" : [ "F. Barthe", "O. Guédon", "S. Mendelson", "A. Naor" ],
      "venue" : "The Annals of Probability, 33(2):480–513,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Blind signal separation in the presence of Gaussian noise",
      "author" : [ "Mikhail Belkin", "Luis Rademacher", "James Voss" ],
      "venue" : "In JMLR W&CP,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Polynomial learning of distribution families",
      "author" : [ "Mikhail Belkin", "Kaushik Sinha" ],
      "venue" : "In FOCS, pages 103–112,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Smoothed analysis of tensor decompositions",
      "author" : [ "Aditya Bhaskara", "Moses Charikar", "Ankur Moitra", "Aravindan Vijayaraghavan" ],
      "venue" : "CoRR, abs/1311.3651,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Probability and measure. Wiley Series in Probability and Mathematical Statistics",
      "author" : [ "Patrick Billingsley" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1995
    }, {
      "title" : "Random points in isotropic convex sets. In Convex geometric analysis",
      "author" : [ "Jean Bourgain" ],
      "venue" : "of Math. Sci. Res. Inst. Publ.,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1996
    }, {
      "title" : "Isotropic PCA and affine-invariant clustering",
      "author" : [ "S. Charles Brubaker", "Santosh Vempala" ],
      "venue" : "Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2008
    }, {
      "title" : "Distributional and L norm inequalities for polynomials over convex bodies in R",
      "author" : [ "Anthony Carbery", "James Wright" ],
      "venue" : "Mathematical Research Letters,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2001
    }, {
      "title" : "Super-symmetric decomposition of the fourth-order cumulant tensor",
      "author" : [ "J-F Cardoso" ],
      "venue" : "blind identification of more sources than sensors. In Acoustics, Speech, and Signal Processing, 1991. ICASSP-91., 1991 International Conference on, pages 3109–3112. IEEE,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Blind beamforming for non-gaussian signals",
      "author" : [ "J.-F. Cardoso", "A. Souloumiac" ],
      "venue" : "Radar and Signal Processing, IEE Proceedings F, volume 140, pages 362–370,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Source separation using higher order moments",
      "author" : [ "J.F. Cardoso" ],
      "venue" : "International Conference on Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Beyond gaussians: Spectral methods for learning mixtures of heavy-tailed distributions",
      "author" : [ "Kamalika Chaudhuri", "Satish Rao" ],
      "venue" : "In 21st Annual Conference on Learning Theory - COLT",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2008
    }, {
      "title" : "Robustness of prewhitening against heavytailed sources. In Independent Component Analysis and Blind Signal Separation",
      "author" : [ "Aiyou Chen", "Peter J. Bickel" ],
      "venue" : "Fifth International Conference,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2004
    }, {
      "title" : "Consistent independent component analysis and prewhitening",
      "author" : [ "Aiyou Chen", "Peter J Bickel" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2005
    }, {
      "title" : "Portfolio value at risk based on independent component analysis",
      "author" : [ "Ying Chen", "Wolfgang Hrdle", "Vladimir Spokoiny" ],
      "venue" : "Journal of Computational and Applied Mathematics,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2007
    }, {
      "title" : "Independent Component Analysis, a new concept",
      "author" : [ "P. Comon" ],
      "venue" : "Signal Processing, Elsevier,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1994
    }, {
      "title" : "Independent component analysis, a new concept",
      "author" : [ "Pierre Comon" ],
      "venue" : "Signal processing,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1994
    }, {
      "title" : "Handbook of Blind Source Separation",
      "author" : [ "Pierre Comon", "Christian Jutten", "editors" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2010
    }, {
      "title" : "Handbook of Blind Source Separation",
      "author" : [ "Pierre Comon", "Christian Jutten", "editors" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2010
    }, {
      "title" : "Probability for Statistics and Machine Learning",
      "author" : [ "A. Dasgupta" ],
      "venue" : "Springer,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On learning mixtures of heavy-tailed distributions",
      "author" : [ "Anirban Dasgupta", "John E. Hopcroft", "Jon M. Kleinberg", "Mark Sandler" ],
      "venue" : "In 46th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2005),",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2005
    }, {
      "title" : "Learning Mixture of Gaussians",
      "author" : [ "S. Dasgupta" ],
      "venue" : "40th Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A Two Round Variant of EM for Gaussian Mixtures",
      "author" : [ "S. Dasgupta", "L. Schulman" ],
      "venue" : "16th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Adaptive blind separation of independent sources: A deflation approach",
      "author" : [ "Nathalie Delfosse", "Philippe Loubaton" ],
      "venue" : "Signal Processing,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 1995
    }, {
      "title" : "A random polynomial time algorithm for approximating the volume of convex bodies",
      "author" : [ "Martin E. Dyer", "Alan M. Frieze", "Ravi Kannan" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 1991
    }, {
      "title" : "PAC Learning Axis Aligned Mixtures of Gaussians with No Separation Assumption",
      "author" : [ "J. Feldman", "R.A. Servedio", "R. O’Donnell" ],
      "venue" : "In The 19th Annual Conference on Learning Theory,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2006
    }, {
      "title" : "Learning linear transformations",
      "author" : [ "Alan M. Frieze", "Mark Jerrum", "Ravi Kannan" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 1996
    }, {
      "title" : "Geometric tomography, volume 58",
      "author" : [ "Richard J Gardner" ],
      "venue" : null,
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 1995
    }, {
      "title" : "Random points in isotropic unconditional convex bodies",
      "author" : [ "A. Giannopoulos", "M. Hartzoulaki", "A. Tsolomitis" ],
      "venue" : "J. London Math. Soc. (2), 72(3):779–798,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Matrix computations. Johns Hopkins Studies in the Mathematical Sciences",
      "author" : [ "Gene H. Golub", "Charles F. Van Loan" ],
      "venue" : null,
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 1996
    }, {
      "title" : "Learning convex bodies is hard",
      "author" : [ "Navin Goyal", "Luis Rademacher" ],
      "venue" : "In COLT,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2009
    }, {
      "title" : "Fourier PCA and robust tensor decomposition",
      "author" : [ "Navin Goyal", "Santosh Vempala", "Ying Xiao" ],
      "venue" : "In Symposium on Theory of Computing,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2014
    }, {
      "title" : "The inverse moment problem for convex polytopes",
      "author" : [ "Nick Gravin", "Jean Lasserre", "Dmitrii V. Pasechnik", "Sinai Robins" ],
      "venue" : "Discrete & Computational Geometry,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2012
    }, {
      "title" : "Geometric Algorithms and Combinatorial Optimization",
      "author" : [ "M. Grötschel", "L. Lovász", "A. Schrijver" ],
      "venue" : "Springer,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Invariant integration formulas for the nsimplex by combinatorial methods",
      "author" : [ "A. Grundmann", "H.M. Moeller" ],
      "venue" : "SIAM J. Numer. Anal., 15:282–290,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Lp-norm spherical distribution",
      "author" : [ "A.K. Gupta", "D. Song" ],
      "venue" : "J. Statist. Plann. Inference, 60(2):241–260,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Learning mixtures of spherical Gaussians: moment methods and spectral decompositions",
      "author" : [ "Daniel Hsu", "Sham M. Kakade" ],
      "venue" : "In ITCS,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2013
    }, {
      "title" : "Fast and robust fixed-point algorithms for independent component analysis",
      "author" : [ "Aapo Hyvärinen" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 1999
    }, {
      "title" : "Independent Component Analysis",
      "author" : [ "Aapo Hyvarinen", "Juha Karhunen", "Erkki Oja" ],
      "venue" : null,
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2001
    }, {
      "title" : "Isoperimetric problems for convex bodies and a localization lemma",
      "author" : [ "R. Kannan", "L. Lovász", "M. Simonovits" ],
      "venue" : "Discrete Comput. Geom., 13(3-4):541–559,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Random walks and an O∗(n5) volume algorithm for convex bodies",
      "author" : [ "Ravi Kannan", "László Lovász", "Miklós Simonovits" ],
      "venue" : "Random Structures Algorithms,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 1997
    }, {
      "title" : "Independent component analysis using the spectral measure for alpha-stable distributions",
      "author" : [ "Preben Kidmose" ],
      "venue" : "In Proceedings of IEEE-EURASIP Workshop on Nonlinear Signal and Image Processing,",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2001
    }, {
      "title" : "Alpha-stable distributions in signal processing of audio signals",
      "author" : [ "Preben Kidmose" ],
      "venue" : "In 41st Conference on Simulation and Modelling, Scandinavian Simulation Society,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2000
    }, {
      "title" : "Blind Separation of Heavy Tail Signals",
      "author" : [ "Preben Kidmose" ],
      "venue" : "PhD thesis, Technical University of Denmark,",
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2001
    }, {
      "title" : "Learning geometric concepts via Gaussian surface area",
      "author" : [ "Adam R. Klivans", "Ryan O’Donnell", "Rocco A. Servedio" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2008
    }, {
      "title" : "A lower bound for agnostically learning disjunctions",
      "author" : [ "Adam R. Klivans", "Alexander A. Sherstov" ],
      "venue" : "In COLT,",
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 2007
    }, {
      "title" : "Cryptographic hardness for learning intersections of halfspaces",
      "author" : [ "Adam R. Klivans", "Alexander A. Sherstov" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 2009
    }, {
      "title" : "PAC learning intersections of halfspaces with membership",
      "author" : [ "Stephen Kwek", "Leonard Pitt" ],
      "venue" : "queries. Algorithmica,",
      "citeRegEx" : "68",
      "shortCiteRegEx" : "68",
      "year" : 1998
    }, {
      "title" : "The multi-dimensional version of  ∫ axdx",
      "author" : [ "Jean B. Lasserre", "Konstantin E. Avrachenkov" ],
      "venue" : "American Math. Month.,",
      "citeRegEx" : "69",
      "shortCiteRegEx" : "69",
      "year" : 2001
    }, {
      "title" : "Simulated annealing in convex bodies and an O*(n4) volume algorithm",
      "author" : [ "László Lovász", "Santosh Vempala" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "70",
      "shortCiteRegEx" : "70",
      "year" : 2006
    }, {
      "title" : "On zonotopes",
      "author" : [ "P. McMullen" ],
      "venue" : "Trans. Amer. Math. Soc., 159:91–109,",
      "citeRegEx" : "72",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Isotropic position and inertia ellipsoids and zonoids of the unit ball of a normed n-dimensional space",
      "author" : [ "V.D. Milman", "A. Pajor" ],
      "venue" : "Geometric aspects of functional analysis (1987–88), volume 1376 of Lecture Notes in Math., pages 64–104. Springer, Berlin,",
      "citeRegEx" : "73",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Settling the polynomial learnability of mixtures of Gaussians",
      "author" : [ "A. Moitra", "G. Valiant" ],
      "venue" : "51st Annual IEEE Symposium on Foundations of Computer Science (FOCS 2010),",
      "citeRegEx" : "74",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Noise stability of functions with low influences: Invariance and optimality",
      "author" : [ "Elchanan Mossel", "Ryan O’Donnell", "Krzysztof Oleszkiewicz" ],
      "venue" : "Annals of Math.,",
      "citeRegEx" : "75",
      "shortCiteRegEx" : "75",
      "year" : 2010
    }, {
      "title" : "Learning a parallelepiped: Cryptanalysis of GGH and NTRU signatures",
      "author" : [ "Phong Q. Nguyen", "Oded Regev" ],
      "venue" : "J. Cryptology,",
      "citeRegEx" : "76",
      "shortCiteRegEx" : "76",
      "year" : 2009
    }, {
      "title" : "An Introduction to Integration",
      "author" : [ "Ole A Nielsen" ],
      "venue" : "Theory and Measure Theory. Wiley,",
      "citeRegEx" : "77",
      "shortCiteRegEx" : "77",
      "year" : 1997
    }, {
      "title" : "Stable Distributions - Models for Heavy Tailed Data",
      "author" : [ "J.P. Nolan" ],
      "venue" : "Birkhauser, Boston,",
      "citeRegEx" : "78",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "NP-hardness of largest contained and smallest containing simplices for V- and H-polytopes",
      "author" : [ "Asa Packer" ],
      "venue" : "Discrete & Computational Geometry,",
      "citeRegEx" : "79",
      "shortCiteRegEx" : "79",
      "year" : 2002
    }, {
      "title" : "Concentration of mass on convex bodies",
      "author" : [ "G. Paouris" ],
      "venue" : "Geom. Funct. Anal., 16(5):1021–1049,",
      "citeRegEx" : "80",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Centroid surfaces",
      "author" : [ "C.M. Petty" ],
      "venue" : "Pacific J. Math., 11(4):1535–1547,",
      "citeRegEx" : "81",
      "shortCiteRegEx" : null,
      "year" : 1961
    }, {
      "title" : "Handbook of Heavy Tailed Distributions in Finance, Volume 1: Handbooks in Finance, Book 1",
      "author" : [ "S.T. Rachev" ],
      "venue" : "Elsevier,",
      "citeRegEx" : "82",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Approximate independence of distributions on spheres and their stability properties",
      "author" : [ "S.T. Rachev", "L. Ruschendorf" ],
      "venue" : "The Annals of Probability, 19(3):1311– 1337,",
      "citeRegEx" : "83",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "On Stirling numbers of the second kind",
      "author" : [ "B.C. Rennie", "A.J. Dobson" ],
      "venue" : "Journal of Combinatorial Theory, 7(2):116 – 121,",
      "citeRegEx" : "84",
      "shortCiteRegEx" : null,
      "year" : 1969
    }, {
      "title" : "Sampling inequalities for infinitely smooth functions, with applications to interpolation and machine learning",
      "author" : [ "Christian Rieger", "Barbara Zwicknagl" ],
      "venue" : "Advances in Computational Mathematics,",
      "citeRegEx" : "85",
      "shortCiteRegEx" : "85",
      "year" : 2010
    }, {
      "title" : "Moment recurrence relations for binomial, poisson and hypergeometric frequency distributions",
      "author" : [ "John Riordan" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "86",
      "shortCiteRegEx" : "86",
      "year" : 1937
    }, {
      "title" : "Optimization of conditional value-at-risk",
      "author" : [ "R.T. Rockafellar", "S.P. Uryasev" ],
      "venue" : "The Journal of Risk, (1):21–41,",
      "citeRegEx" : "87",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Random vectors in the isotropic position",
      "author" : [ "M. Rudelson" ],
      "venue" : "J. Funct. Anal., 164(1):60–72,",
      "citeRegEx" : "89",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Smallest singular value of a random rectangular matrix",
      "author" : [ "Mark Rudelson", "Roman Vershynin" ],
      "venue" : "Comm. Pure Appl. Math.,",
      "citeRegEx" : "90",
      "shortCiteRegEx" : "90",
      "year" : 2009
    }, {
      "title" : "Blind source separation of noisy mixtures using a semi-parametric approach with application to heavy-tailed signals",
      "author" : [ "M. Sahmoudi", "K. Abed-Meraim", "M. Lavielle", "E. Kuhn", "Ph. Ciblat" ],
      "venue" : "In Proc. of EUSIPCO",
      "citeRegEx" : "91",
      "shortCiteRegEx" : "91",
      "year" : 2005
    }, {
      "title" : "On the volume of the intersection of two l  p balls",
      "author" : [ "G. Schechtman", "J. Zinn" ],
      "venue" : "Proc. Amer. Math. Soc, volume 110, pages 217–224,",
      "citeRegEx" : "92",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Convex bodies: the Brunn-Minkowski theory, volume 44 of Encyclopedia of Mathematics and its Applications",
      "author" : [ "Rolf Schneider" ],
      "venue" : null,
      "citeRegEx" : "93",
      "shortCiteRegEx" : "93",
      "year" : 1993
    }, {
      "title" : "Estimating the support of a high-dimensional distribution",
      "author" : [ "Bernhard Schölkopf", "John C. Platt", "John Shawe-Taylor", "Alex J. Smola", "Robert C. Williamson" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "94",
      "shortCiteRegEx" : "94",
      "year" : 2001
    }, {
      "title" : "Super-efficiency in blind signal separation of symmetric heavy-tailed sources",
      "author" : [ "Yoav Shereshevski", "Arie Yeredor", "Hagit Messer" ],
      "venue" : "In Statistical Signal Processing,",
      "citeRegEx" : "95",
      "shortCiteRegEx" : "95",
      "year" : 2001
    }, {
      "title" : "Lp-nested symmetric distributions",
      "author" : [ "Fabian Sinz", "Matthias Bethge" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "96",
      "shortCiteRegEx" : "96",
      "year" : 2010
    }, {
      "title" : "The conjoint effect of divisive normalization and orientation selectivity on redundancy reduction",
      "author" : [ "Fabian H. Sinz", "Matthias Bethge" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "97",
      "shortCiteRegEx" : "97",
      "year" : 2008
    }, {
      "title" : "Lp-norm uniform distribution",
      "author" : [ "D. Song", "A.K. Gupta" ],
      "venue" : "Proc. Amer. Math. Soc., 125(2):595–601,",
      "citeRegEx" : "98",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Applications of analytic centers for the numerical solution of semiinfinite, convex programs arising in control theory",
      "author" : [ "G. Sonnevend" ],
      "venue" : "DFG report Nr. 170/1989, Univ. Würzburg, Inst. f. angew. Mathematik,",
      "citeRegEx" : "99",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Covariance estimation for distributions with 2 + moments",
      "author" : [ "Nikhil Srivastava", "Roman Vershynin" ],
      "venue" : null,
      "citeRegEx" : "100",
      "shortCiteRegEx" : "100",
      "year" : 2011
    }, {
      "title" : "Covariance estimation for distributions with 2 + ε moments",
      "author" : [ "Nikhil Srivastava", "Roman Vershynin" ],
      "venue" : "Ann. Probab.,",
      "citeRegEx" : "101",
      "shortCiteRegEx" : "101",
      "year" : 2013
    }, {
      "title" : "Matrix perturbation theory",
      "author" : [ "Gilbert W Stewart", "Ji-guang Sun" ],
      "venue" : null,
      "citeRegEx" : "102",
      "shortCiteRegEx" : "102",
      "year" : 1990
    }, {
      "title" : "A Spectral Algorithm for Learning Mixtures of Distributions",
      "author" : [ "S. Vempala", "G. Wang" ],
      "venue" : "43rd Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "103",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Learning convex concepts from Gaussian distributions with pca",
      "author" : [ "Santosh Vempala" ],
      "venue" : "In FOCS, pages 541–550,",
      "citeRegEx" : "104",
      "shortCiteRegEx" : "104",
      "year" : 2010
    }, {
      "title" : "Learning convex concepts from Gaussian distributions with PCA",
      "author" : [ "Santosh Vempala" ],
      "venue" : "In FOCS, pages 124–130,",
      "citeRegEx" : "105",
      "shortCiteRegEx" : "105",
      "year" : 2010
    }, {
      "title" : "Structure from local optima: Learning subspace juntas via higher order PCA",
      "author" : [ "Santosh S. Vempala", "Ying Xiao" ],
      "venue" : "CoRR, abs/1108.3329,",
      "citeRegEx" : "106",
      "shortCiteRegEx" : "106",
      "year" : 2011
    }, {
      "title" : "How close is the sample covariance matrix to the actual covariance matrix",
      "author" : [ "Roman Vershynin" ],
      "venue" : "J. Theoret. Probab.,",
      "citeRegEx" : "107",
      "shortCiteRegEx" : "107",
      "year" : 2012
    }, {
      "title" : "Ica by maximizing nonstability",
      "author" : [ "Baijie Wang", "Ercan E Kuruoglu", "Junying Zhang" ],
      "venue" : "In Independent Component Analysis and Signal Separation,",
      "citeRegEx" : "108",
      "shortCiteRegEx" : "108",
      "year" : 2009
    }, {
      "title" : "Scattered data approximation, volume 17",
      "author" : [ "Holger Wendland" ],
      "venue" : null,
      "citeRegEx" : "109",
      "shortCiteRegEx" : "109",
      "year" : 2005
    }, {
      "title" : "Moments and Absolute Moments of the Normal Distribution",
      "author" : [ "A. Winkelbauer" ],
      "venue" : "ArXiv e-prints, September",
      "citeRegEx" : "110",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Blind source separation via the second characteristic function",
      "author" : [ "Arie Yeredor" ],
      "venue" : "Signal Processing,",
      "citeRegEx" : "111",
      "shortCiteRegEx" : "111",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 41,
      "context" : "The first such reduction is a solution to the open problem of efficiently learning the intersection of n + 1 halfspaces in R, posed in [43].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 15,
      "context" : "This pre-processing step is a random non-linear scaling inspired by the study of `p balls [16].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 9,
      "context" : "This chapter is based on [10] and [9].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : "This chapter is based on [10] and [9].",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "This chapter is based on work published in [11].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 7,
      "context" : "This chapter is based on [8].",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 30,
      "context" : ", [31, 57, 33].",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 53,
      "context" : ", [31, 57, 33].",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 32,
      "context" : ", [31, 57, 33].",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 38,
      "context" : "algorithms of [39, 43], explicitly require the fourth moment to be finite.",
      "startOffset" : 14,
      "endOffset" : 22
    }, {
      "referenceID" : 41,
      "context" : "algorithms of [39, 43], explicitly require the fourth moment to be finite.",
      "startOffset" : 14,
      "endOffset" : 22
    }, {
      "referenceID" : 103,
      "context" : "Algorithms in [111, 48], which make use of the characteristic function also seem to require at least the fourth moment to be finite: while the characteristic function exists for distributions without moments, the algorithms in these papers use the second or higher derivatives of the (second) characteristic function, and for this to be well-defined one needs the moments of that order to exist.",
      "startOffset" : 14,
      "endOffset" : 23
    }, {
      "referenceID" : 46,
      "context" : "Algorithms in [111, 48], which make use of the characteristic function also seem to require at least the fourth moment to be finite: while the characteristic function exists for distributions without moments, the algorithms in these papers use the second or higher derivatives of the (second) characteristic function, and for this to be well-defined one needs the moments of that order to exist.",
      "startOffset" : 14,
      "endOffset" : 23
    }, {
      "referenceID" : 56,
      "context" : "[62, 64, 95, 28, 29, 91, 108].",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 58,
      "context" : "[62, 64, 95, 28, 29, 91, 108].",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 87,
      "context" : "[62, 64, 95, 28, 29, 91, 108].",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : "[62, 64, 95, 28, 29, 91, 108].",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 28,
      "context" : "[62, 64, 95, 28, 29, 91, 108].",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 83,
      "context" : "[62, 64, 95, 28, 29, 91, 108].",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 100,
      "context" : "[62, 64, 95, 28, 29, 91, 108].",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 71,
      "context" : "Heavy-tailed distributions arise in a wide variety of contexts including signal processing and finance; see [78, 82] for an extensive bibliography.",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 75,
      "context" : "Heavy-tailed distributions arise in a wide variety of contexts including signal processing and finance; see [78, 82] for an extensive bibliography.",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 71,
      "context" : ", [78].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 35,
      "context" : ", [36, 27].",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 26,
      "context" : ", [36, 27].",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 29,
      "context" : ", [30]), heavy tailed distributions are commonly used to model catastrophic but somewhat unlikely scenarios.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 80,
      "context" : "A standard measures of risk in that literature, the so called conditional value at risk [87], is only finite when the first moment is finite.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 46,
      "context" : "The theorem below refers to the algorithm Fourier PCA [48] which solves ICA under the fourth moment assumption.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 66,
      "context" : "is a quadratic form in u and its square root is the support function of a convex body, Legendre’s inertia ellipsoid, up to some scaling factor (see [73] for example).",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 46,
      "context" : "to use an existing ICA algorithm (from [48] in our case) to handle the resulting ICA instance.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 74,
      "context" : "The centroid body of a compact set was first defined in [81].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 48,
      "context" : "The implementation works by first implementing a membership oracle for the polar of the centroid body via sampling and then using it via the ellipsoid method (see [50]) to construct a membership oracle for the centroid body.",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 74,
      "context" : "A fundamental property of the centroid body, for our analysis, is that the centroid body is linearly equivariant, that is, if one applies an invertible linear transformation to a probability measure then the corresponding centroid body transforms in the same way (already observed in [81]).",
      "startOffset" : 284,
      "endOffset" : 288
    }, {
      "referenceID" : 38,
      "context" : ", [39, 43]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 41,
      "context" : ", [39, 43]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 21,
      "context" : "Another related work is [22], on isotropic PCA, affine invariant clustering, and learning mixtures of Gaussians.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 103,
      "context" : "Finally, in [111, 48] a different reweighting, using a “Fourier weight” e T x (here u ∈ R is a fixed vector and x ∈ R is a data point) is used in the computation of the covariance matrix.",
      "startOffset" : 12,
      "endOffset" : 21
    }, {
      "referenceID" : 46,
      "context" : "Finally, in [111, 48] a different reweighting, using a “Fourier weight” e T x (here u ∈ R is a fixed vector and x ∈ R is a data point) is used in the computation of the covariance matrix.",
      "startOffset" : 12,
      "endOffset" : 21
    }, {
      "referenceID" : 48,
      "context" : "This follows from applications of the ellipsoid method from [50].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 48,
      "context" : "The definitions and theorems in this section all come (occasionally with slight rephrasing) from [50] except for the notion of ( , δ)-weak oracle.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 48,
      "context" : "This is done in [50] as they work out in detail the important low level issues of how the numbers in the algorithm are represented as general real numbers cannot be directly handled by computers.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 48,
      "context" : "Definition 1 ([50]).",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "For δ ∈ [0, 1], an ( , δ)-weak membership oracle for K acts as follows: Given a point y ∈ Q, with probability at least 1 − δ it solves the -weak membership problem for y,K, and otherwise its output can be arbitrary.",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 48,
      "context" : "Definition 2 ([50]).",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 48,
      "context" : "2 in [50]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 48,
      "context" : "2 as stated in [50] is stronger than the above statement in that it constructs a weak violation oracle (not defined here) which gives a weak validity oracle which suffices for us.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 48,
      "context" : "1 in [50]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 39,
      "context" : "The result follows from the random walk-based algorithms to generate approximately uniformly random points from a convex body [41, 60, 70].",
      "startOffset" : 126,
      "endOffset" : 138
    }, {
      "referenceID" : 55,
      "context" : "The result follows from the random walk-based algorithms to generate approximately uniformly random points from a convex body [41, 60, 70].",
      "startOffset" : 126,
      "endOffset" : 138
    }, {
      "referenceID" : 64,
      "context" : "The result follows from the random walk-based algorithms to generate approximately uniformly random points from a convex body [41, 60, 70].",
      "startOffset" : 126,
      "endOffset" : 138
    }, {
      "referenceID" : 20,
      "context" : "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].",
      "startOffset" : 83,
      "endOffset" : 116
    }, {
      "referenceID" : 81,
      "context" : "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].",
      "startOffset" : 83,
      "endOffset" : 116
    }, {
      "referenceID" : 43,
      "context" : "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].",
      "startOffset" : 83,
      "endOffset" : 116
    }, {
      "referenceID" : 73,
      "context" : "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].",
      "startOffset" : 83,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].",
      "startOffset" : 83,
      "endOffset" : 116
    }, {
      "referenceID" : 99,
      "context" : "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].",
      "startOffset" : 83,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].",
      "startOffset" : 83,
      "endOffset" : 116
    }, {
      "referenceID" : 93,
      "context" : "There has been a sequence of papers studying the sample complexity of this problem [21, 89, 45, 80, 15, 107, 2, 101].",
      "startOffset" : 83,
      "endOffset" : 116
    }, {
      "referenceID" : 74,
      "context" : ", [81, 73, 44]) convex body associated to (the uniform distribution on) a given convex body.",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 66,
      "context" : ", [81, 73, 44]) convex body associated to (the uniform distribution on) a given convex body.",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 42,
      "context" : ", [81, 73, 44]) convex body associated to (the uniform distribution on) a given convex body.",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 74,
      "context" : "Following [81], consider the function h(u) = E(|〈u,X〉|).",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 74,
      "context" : "It is a slight generalization of statements in [81] and [44, Theorem 9.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 48,
      "context" : "2 of [50] (stated as Theorem 2.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 48,
      "context" : "1 of [50] (Lemma 2 here) gives an algorithm to construct an ( , δ)weak membership oracle WMEMΓX( , δ, 1/r, 1/R) from WVAL(ΓX)◦( 1, δ, R, r).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 48,
      "context" : "1 in [50] shows WMEMΓX( , δ, 1/r, 1/R) calls WVAL(ΓX)◦( 1, δ, R, r) once, with 1 ≥ 1/poly(1/ , ‖y‖, 1/r) (where y is the query point).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 48,
      "context" : "2 from [50] would apply and would give that WVAL(ΓX)◦ outputs an answer as expected.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 39,
      "context" : "2 sampling algorithm such as the one in [41] with c = /(2(n+1) ), r = sm/ √ n, R = sM √ n, and same δ.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 66,
      "context" : "It is known that any n-dimensional isotropic convex body is contained in the ball of radius n + 1 [73, 99],[59, Theorem 4.",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 91,
      "context" : "It is known that any n-dimensional isotropic convex body is contained in the ball of radius n + 1 [73, 99],[59, Theorem 4.",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "Generate z ∼ U [0, 1].",
      "startOffset" : 15,
      "endOffset" : 21
    }, {
      "referenceID" : 46,
      "context" : "2 from [48] in a special case by setting parameters k and ki in that theorem to 4 for i ∈ [n].",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 46,
      "context" : "2 of [48] is called Fourier PCA.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 46,
      "context" : "[48] Let X ∈ R be given by an ICA model X = AS where A ∈ Rn×n is unitary and the Si are mutually independent, E[S i ] ≤M4 for some positive constant M4, and |κ4(Si)| ≥ ∆.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 41,
      "context" : ", using the algorithm in [43]).",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 46,
      "context" : "The algorithm in [48] estimates the second derivative of ψX(u) and computes its eigendecomposition.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 103,
      "context" : "(In [111] and [48], this second derivative is interpreted as a kind of covariance matrix of X but with the twist that a certain “Fourier” weight is used in the expectation computation for the covariance matrix.",
      "startOffset" : 4,
      "endOffset" : 9
    }, {
      "referenceID" : 46,
      "context" : "(In [111] and [48], this second derivative is interpreted as a kind of covariance matrix of X but with the twist that a certain “Fourier” weight is used in the expectation computation for the covariance matrix.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "1 ∈ [0, 1].",
      "startOffset" : 4,
      "endOffset" : 10
    }, {
      "referenceID" : 46,
      "context" : "While this theorem is not stated in [48], it is easy to derive from their proof of Theorem 2.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 46,
      "context" : "1 in [48].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 46,
      "context" : "where we ensured that ‖Ψ̃X̂(u)−ΨX̂(u)‖F < by taking sufficiently many samples of X̂ to get a good estimate with probability at least δ; as in [48], a standard concentration argument shows that poly(n,M4, 1/∆, 1/ , 1/δ) samples suffice for this purpose.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "As noted above, the technique in [10], while being provably efficient and correct, suffers from practical implementation issues.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : "The former, orthogonalization via centroid body scaling, uses the samples already present in the algorithm rather than relying on a random walk to draw samples which are approximately uniform in the algorithm’s approximation of the centroid body (as is done in [10]).",
      "startOffset" : 261,
      "endOffset" : 265
    }, {
      "referenceID" : 9,
      "context" : "In [10], another orthogonalization procedure, namely orthogonalization via the uniform distribution in the centroid body is theoretically proven to work.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 53,
      "context" : "It’s standard to use covariance matrix for whitening when the second moments of all independent components exist [57]: Given samples from the ICA model X = AS, we compute the empirical covariance matrix Σ̃ which tends to the true covariance matrix as we take more samples and set B = Σ̃−1/2.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 27,
      "context" : ", [28]), the empirical covariance matrix was used for whitening in the heavy-tailed regime with good empirical performance; [28] also provided some theoretical analysis to explain this surprising performance.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 27,
      "context" : ", [28]), the empirical covariance matrix was used for whitening in the heavy-tailed regime with good empirical performance; [28] also provided some theoretical analysis to explain this surprising performance.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "The following lemma from [10] says that the empirical average of the absolute value of X converges to the expectation of |X|.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "For δ ∈ [0, 1], an ( , δ)-weak membership oracle for K acts as follows: Given a point y ∈ Q, with probability at least 1− δ it solves the -weak membership problem for y,K, and otherwise its output can be arbitrary.",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 65,
      "context" : "This is proven in [72].",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "the Frobenius norm to measure the error, but all experiments were also performed using the well-known Amari index [6]; the results have similar behavior and are not presented here.",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "As proposed in [10], Gaussian damping is a preprocessing technique that converts data from an ICA model X = AS, where A is unitary (columns are orthogonal with unit l2-norm) to data from a related ICA model XR = ASR, where R > 0 is a parameter to be chosen.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 9,
      "context" : "For more details about the technical requirements for choosing R, see [10].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 25,
      "context" : ", [26] .",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 57,
      "context" : "[63]) that speech data can be modeled by α-stable distributions.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 57,
      "context" : "To estimate whether the data is heavy-tailed, as in [63], we estimate parameter α of a best-fit α-stable distribution.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 45,
      "context" : "For general n, it is known to require 2 √ n) samples [47] (see also [65] for a similar lower bound in a different but related model of learning).",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 59,
      "context" : "For general n, it is known to require 2 √ n) samples [47] (see also [65] for a similar lower bound in a different but related model of learning).",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 45,
      "context" : "As mentioned in [47], it turns out that if the body has few facets (e.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 86,
      "context" : "[94] for the problem of estimating the support of a probability distribution.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 72,
      "context" : ") However, the problem of finding a minimum volume simplex is in general NP-hard [79].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 41,
      "context" : "[43] gave an efficient algorithm for this problem (with some restrictions on the allowed distributions, but also with some weaker requirements than full independence) along with most of the details of a rigorous analysis (a complete analysis",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "of a special case can be found in [13]; see also [106] for a generalization of ICA to subspaces along with a rigorous analysis).",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 98,
      "context" : "of a special case can be found in [13]; see also [106] for a generalization of ICA to subspaces along with a rigorous analysis).",
      "startOffset" : 49,
      "endOffset" : 54
    }, {
      "referenceID" : 41,
      "context" : "[43] asked if one could learn other convex bodies, and in particular simplices, efficiently from uniformly random samples.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 69,
      "context" : "[76] gave a simpler and rigorous algorithm and analysis for the case of learning parallelepipeds with similarities to the popular FastICA algorithm of [56].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 52,
      "context" : "[76] gave a simpler and rigorous algorithm and analysis for the case of learning parallelepipeds with similarities to the popular FastICA algorithm of [56].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 69,
      "context" : "The algorithm in [76] is a first order algorithm unlike Frieze et al.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 41,
      "context" : "The algorithms in both [43, 76] make use of the fourth moment function of the probability distribution.",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 69,
      "context" : "The algorithms in both [43, 76] make use of the fourth moment function of the probability distribution.",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 33,
      "context" : "More information on ICA including historical remarks can be found in [58, 34].",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 41,
      "context" : "Let us note that [43] allow certain kinds of dependencies among the components, however this does not appear to be useful for learning simplices.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 61,
      "context" : "[67] prove that learning intersections of n half-spaces in R (for constant > 0) is hard under standard cryptographic assumptions (PAC-learning is possible, however, if one also has access to a membership oracle in addition to random samples [68]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 62,
      "context" : "[67] prove that learning intersections of n half-spaces in R (for constant > 0) is hard under standard cryptographic assumptions (PAC-learning is possible, however, if one also has access to a membership oracle in addition to random samples [68]).",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 60,
      "context" : "[66, 104, 105] and references therein.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 96,
      "context" : "[66, 104, 105] and references therein.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 97,
      "context" : "[66, 104, 105] and references therein.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 41,
      "context" : "[43] and [47] consider the uniform distribution over the intersection.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 45,
      "context" : "[43] and [47] consider the uniform distribution over the intersection.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 47,
      "context" : "[49] show how to reconstruct a polytope with N vertices in R, given its first O(nN) moments in (n + 1) random directions.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 47,
      "context" : "In our setting, where we have access to only a polynomial number of random samples, it’s not clear how to compute moments of such high orders to the accuracy required for the algorithm of [49] even for simplices.",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 6,
      "context" : "A recent and parallel work of [7] is closely related to ours.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "Similar representations are known for the uniform measure in an n-dimensional `p ball (denoted ` n p ) [16] and the cone measure on the boundary of an `p ball [92, 83, 98] (see Section 3.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 84,
      "context" : "Similar representations are known for the uniform measure in an n-dimensional `p ball (denoted ` n p ) [16] and the cone measure on the boundary of an `p ball [92, 83, 98] (see Section 3.",
      "startOffset" : 159,
      "endOffset" : 171
    }, {
      "referenceID" : 76,
      "context" : "Similar representations are known for the uniform measure in an n-dimensional `p ball (denoted ` n p ) [16] and the cone measure on the boundary of an `p ball [92, 83, 98] (see Section 3.",
      "startOffset" : 159,
      "endOffset" : 171
    }, {
      "referenceID" : 90,
      "context" : "Similar representations are known for the uniform measure in an n-dimensional `p ball (denoted ` n p ) [16] and the cone measure on the boundary of an `p ball [92, 83, 98] (see Section 3.",
      "startOffset" : 159,
      "endOffset" : 171
    }, {
      "referenceID" : 41,
      "context" : ", [43]), this transformation can be determined up to a rotation from the mean and the covariance matrix of the uniform distribution on the given simplex.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 52,
      "context" : "A fixed point-like iteration (inspired by the analysis of FastICA [56] and of gradient descent in [76]) starting from a random point in the unit sphere finds a local maximum efficiently with high probability.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 69,
      "context" : "A fixed point-like iteration (inspired by the analysis of FastICA [56] and of gradient descent in [76]) starting from a random point in the unit sphere finds a local maximum efficiently with high probability.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 69,
      "context" : "By definition of total variation distance, Subroutine 5 succeeds with almost as large probability when given a sample from S (an argument already used in [76]).",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 15,
      "context" : "The cone measure on the surface ∂K of centrally symmetric convex body K in R [16, 92, 83, 98] is defined by",
      "startOffset" : 77,
      "endOffset" : 93
    }, {
      "referenceID" : 84,
      "context" : "The cone measure on the surface ∂K of centrally symmetric convex body K in R [16, 92, 83, 98] is defined by",
      "startOffset" : 77,
      "endOffset" : 93
    }, {
      "referenceID" : 76,
      "context" : "The cone measure on the surface ∂K of centrally symmetric convex body K in R [16, 92, 83, 98] is defined by",
      "startOffset" : 77,
      "endOffset" : 93
    }, {
      "referenceID" : 90,
      "context" : "The cone measure on the surface ∂K of centrally symmetric convex body K in R [16, 92, 83, 98] is defined by",
      "startOffset" : 77,
      "endOffset" : 93
    }, {
      "referenceID" : 84,
      "context" : "From [92] and [83] we have the following representation of the cone measure on ∂B p : 85",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 76,
      "context" : "From [92] and [83] we have the following representation of the cone measure on ∂B p : 85",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 15,
      "context" : "From [16], we also have the following variation, a representation of the uniform distribution in B p :",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 63,
      "context" : "Similar more general formulas appear in [69].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 49,
      "context" : "We will use the following result from [51] for αi ≥ 0: ∫",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 69,
      "context" : "As we noted in the introduction, our algorithm is inspired by the algorithm of [76] for the related problem of learning hypercubes and also by the FastICA algorithm in [56].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 52,
      "context" : "As we noted in the introduction, our algorithm is inspired by the algorithm of [76] for the related problem of learning hypercubes and also by the FastICA algorithm in [56].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 69,
      "context" : "With the right update rule in hand the analysis turns out to be quite similar to the one in [76].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 41,
      "context" : "A natural approach to do this would be to use gradient descent or Newton’s method (this was done in [43]).",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 69,
      "context" : "Our analysis has the same outline as that of [76].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 69,
      "context" : "This is because the iteration that we get is the same as that of [76] except that cubing is replaced by squaring (see below); however some details in our proof are different.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 69,
      "context" : "(A similar argument is made in [76] with different parameters.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 15,
      "context" : "The next lemma complements the main result in [16], Theorem 1 (Theorem 3.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 88,
      "context" : "The use of a non-linear scaling step to turn a distribution into one having independent components has been done before [96, 97], but there it is applied after finding a transformation that makes the distribution axis-aligned.",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 89,
      "context" : "The use of a non-linear scaling step to turn a distribution into one having independent components has been done before [96, 97], but there it is applied after finding a transformation that makes the distribution axis-aligned.",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 88,
      "context" : "[96, 97], without independent components, and therefore the use of ICA is somewhat heuristic.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 89,
      "context" : "[96, 97], without independent components, and therefore the use of ICA is somewhat heuristic.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 36,
      "context" : "This line of work was started in [37] where the first algorithm to recover parameters using a number of samples polynomial in the dimension was presented.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 37,
      "context" : ", [38, 12, 103, 1, 42]).",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 11,
      "context" : ", [38, 12, 103, 1, 42]).",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 95,
      "context" : ", [38, 12, 103, 1, 42]).",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : ", [38, 12, 103, 1, 42]).",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 40,
      "context" : ", [38, 12, 103, 1, 42]).",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 17,
      "context" : "A completion of the attempts to weaken the separation conditions was achieved in [18] and [74], where it was shown that arbitrarily small separation was sufficient for learning a general mixture with a fixed number of components in polynomial time.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 67,
      "context" : "A completion of the attempts to weaken the separation conditions was achieved in [18] and [74], where it was shown that arbitrarily small separation was sufficient for learning a general mixture with a fixed number of components in polynomial time.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 67,
      "context" : "Moreover, a one-dimensional example given in [74] showed that an exponential dependence on the number of components",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 37,
      "context" : "It is worth noting that while quite different in many aspects, all of these papers used a general scheme similar to that in the original work [38] by reducing high-dimensional inference to a small number of low-dimensional problems through appropriate projections.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 51,
      "context" : "However, a surprising result was recently proved in [54].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 51,
      "context" : "The result in [54] is inherently high-dimensional as that condition is never satisfied when the means belong to a lower-dimensional space.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 67,
      "context" : "The one-dimensional example in [74] cannot answer this question as it is a specific worst-case scenario, which can be potentially ruled out by some genericity condition.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "However, we show that for k points uniformly sampled from [0, 1] there are (with high probability) two mixtures of unit Gaussians with means on non-intersecting subsets of these points, whose L distance is O∗(e−k) and which are thus not polynomially identifiable.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 82,
      "context" : "To do that we provide smoothed analysis of the condition number using certain results from [90] and anti-concentration inequalities.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 46,
      "context" : "We combine this with the recent work on efficient algorithms for underdetermined ICA from [48] to obtain the necessary bounds.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 46,
      "context" : "3 of [48]; it should not be difficult, however, to adapt the algorithm to use a method similar to that of [54] to handle the case where m < n.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 51,
      "context" : "3 of [48]; it should not be difficult, however, to adapt the algorithm to use a method similar to that of [54] to handle the case where m < n.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 18,
      "context" : "We point out the simultaneous and independent work of [19], where the authors prove learnability results related to our Theorems 4.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 18,
      "context" : "The results in [19], which are based on tensor decompositions, are stronger in that they can learn mixtures of axis-aligned Gaussians (with non-identical covariance matrices) without requiring to know the covariance matrices in advance.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "Let X be a set of 4k points uniformly sampled from [0, 1].",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 51,
      "context" : "This is in contrast to conjectured computational barriers that arise in related settings based on the noisy parity problem (see [54] for pointers).",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 67,
      "context" : "The only previous information-theoretic lower bound for learning GMMs we are aware of is due to [74] and holds for two specially designed onedimensional mixtures.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 46,
      "context" : "The ICA algorithm from [48] to which we will be reducing learning a GMM relies on the shared tensor structure of the derivatives of the second characteristic function and the higher order multi-variate cumulants.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 46,
      "context" : "1, from [48]) allows us to recover A up to the necessary ambiguities in the noisy ICA setting.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 46,
      "context" : "The theorem establishes guarantees for an algorithm from [48] for noisy underdetermined ICA, UnderdeterminedICA.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 46,
      "context" : "The following theorem (from [48]) allows us to recover A up to the necessary ambiguities in the noisy ICA setting.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 46,
      "context" : "The theorem establishes guarantees for an algorithm from [48] for noisy underdetermined ICA, UnderdeterminedICA.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 46,
      "context" : "1 ([48]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 46,
      "context" : "The algorithm then calculates various internal paremeters: a bound on directional covariances, error parameters to be split between the “Poissonization” process and the call to UnderdeterminedICA, the threshold parameter τ and Poisson parameter λ to be used in Subroutine 5, and values explicitly needed by the proof of UnderdeterminedICA in [48].",
      "startOffset" : 342,
      "endOffset" : 346
    }, {
      "referenceID" : 46,
      "context" : "We show that the reduction results in a model that allows one to use the ICA algorithm UnderdeterminedICA presented in [48] (see Section 2.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 79,
      "context" : "It is known (see [86]) that EY ` = ∑̀",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "15 in [5]),",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 46,
      "context" : "1 We now show that after the reduction is applied, we can use the ICA routine given in [48] to learn the GMM.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 46,
      "context" : ") As in [48], it will be convenient to work with the multilinear part of the Khatri–Rao product: For a column vector Ak ∈ R define A 2 k ∈ R( n 2), a subvector of A 2 k ∈ R 2 , given by (A 2 k )ij := (Ak)i(Ak)j for 1 ≤ i < j ≤ n.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 46,
      "context" : "This can be proved along the lines of a similar result in [48].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "We will apply the anticoncentration inequality of Carbery–Wright [23] to this polynomial to conclude that the distance between the k’th column of (M + N) 2 and the span of the rest of the columns is unlikely to be very small (see Appendix 4.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 67,
      "context" : "This goes beyond the specific example of exponential closeness given in [74] as we demonstrate that such mixtures are ubiquitous as long as there is no lower bound on the separation between the components.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "Specifically, let S be a cube [0, 1] ⊂ R.",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "Let X be any subset of k points in [0, 1] .",
      "startOffset" : 35,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "Let g be any positive function with L2 norm 1 supported on [0, 1] n and let f = Kg.",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 78,
      "context" : "From [85], Corollary 5.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "1 (taking λ = 0) we have that for some A > 0 and h sufficiently small ‖f − fX,k‖L∞([0,1]n) < exp(A log h h )",
      "startOffset" : 83,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "and ‖f − fX,k‖L2([0,1]n) < exp(A log h h )",
      "startOffset" : 17,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Note that the norm is on [0, 1] while we need to control the norm on R.",
      "startOffset" : 25,
      "endOffset" : 31
    }, {
      "referenceID" : 101,
      "context" : "28 of [109]) ‖f − fX,k‖H = 〈f − fX,k, f − fX,k〉H = 〈f − fX,k, f〉H = 〈f − fX,k,Kg〉H = 〈f − fX,k, g〉L2([0,1]n) ≤ ‖f − fX,k‖L2(X)‖g‖L2(X) < exp(A log h h )",
      "startOffset" : 6,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "28 of [109]) ‖f − fX,k‖H = 〈f − fX,k, f − fX,k〉H = 〈f − fX,k, f〉H = 〈f − fX,k,Kg〉H = 〈f − fX,k, g〉L2([0,1]n) ≤ ‖f − fX,k‖L2(X)‖g‖L2(X) < exp(A log h h )",
      "startOffset" : 101,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "Let X and Y be any two subsets of [0, 1] with fill h.",
      "startOffset" : 34,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "By integrating over the interval [0, 1], and since f is strictly positive on the interval, it is easy to see that α > C (and by the same token β > C), where C is some universal constant.",
      "startOffset" : 33,
      "endOffset" : 39
    }, {
      "referenceID" : 24,
      "context" : "The most theoretically justified ICA algorithms have relied on the tensor structure of multivariate cumulants, including the early, popular practical algorithm JADE [25].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 13,
      "context" : "In the fully determined ICA setting in which the number source signals does not exceed the ambient dimension, the papers [14] and [17] demonstrate that ICA with additive Gaussian noise can be solved in polynomial time and using polynomial samples.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "In the fully determined ICA setting in which the number source signals does not exceed the ambient dimension, the papers [14] and [17] demonstrate that ICA with additive Gaussian noise can be solved in polynomial time and using polynomial samples.",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 23,
      "context" : "The tensor structure of the cumulants was (to the best of our knowledge) first exploited in [24] and later in [4] to solve underdetermined ICA.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "The tensor structure of the cumulants was (to the best of our knowledge) first exploited in [24] and later in [4] to solve underdetermined ICA.",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 46,
      "context" : "Finally, [48] provides",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 82,
      "context" : "2 Rudelson-Vershynin subspace bound Lemma 31 (Rudelson–Vershynin [90]).",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 68,
      "context" : "The version of the anticoncentration inequality we use is explicitly given in [75] which in turn follows immediately from [23]:",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : "The version of the anticoncentration inequality we use is explicitly given in [75] which in turn follows immediately from [23]:",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 68,
      "context" : "Lemma 32 ([75]).",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 34,
      "context" : ", [35].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 77,
      "context" : "The following bound comes from [84] Theorem 3.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 102,
      "context" : "For general `, it is known (see [110]) that",
      "startOffset" : 32,
      "endOffset" : 37
    }, {
      "referenceID" : 70,
      "context" : "3 in [77] and Sect.",
      "startOffset" : 5,
      "endOffset" : 9
    } ],
    "year" : 2017,
    "abstractText" : "Data-driven applications are growing. Machine learning and data analysis now finds both scientific and industrial application in biology, chemistry, geology, medicine, and physics. These applications rely on large quantities of data gathered from automated sensors and user input. Furthermore, the dimensionality of many datasets is extreme: more details are being gathered about single user interactions or sensor readings. All of these applications encounter problems with a common theme: use observed data to make inferences about the world. Our work obtains the first provably efficient algorithms for Independent Component Analysis (ICA) in the presence of heavy-tailed data. The main tool in this result is the centroid body (a well-known topic in convex geometry), along with optimization and random walks for sampling from a convex body. This is the first algorithmic use of the centroid body and it is of independent theoretical interest, since it effectively replaces the estimation of covariance from samples, and is more generally accessible. We demonstrate that ICA is itself a powerful geometric primitive. That is, having access to an efficient algorithm for ICA enables us to efficiently solve other important problems in machine learning. The first such reduction is a solution to the open problem of efficiently learning the intersection of n + 1 halfspaces in R, posed in [43]. This reduction relies on a non-linear transformation of samples from such an intersection of halfspaces (i.e. a simplex ) to samples which are approximately from a",
    "creator" : "LaTeX with hyperref package"
  }
}