{
  "name" : "1511.04210.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Quality of the Initial Basin in Overspecified Neural Networks",
    "authors" : [ "Itay Safran", "Ohad Shamir" ],
    "emails" : [ "itay.safran@weizmann.ac.il", "ohad.shamir@weizmann.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this work, we aspire to understand this phenomenon. In particular, we wish to better understand the behavior of the error over the sample as a function of the weights of the network, where we focus mostly on neural nets comprised of 2 layers, although we will also consider single neuron nets and nets of arbitrary depth, investigating properties such as the number of local minima the function has, and the probability of initializing from a basin with a given minimal value, with the goal of finding reasonable conditions under which efficient learning of the network is possible."
    }, {
      "heading" : "1 Introduction",
      "text" : "Inspired by current understanding of humans’ central nervous system, artificial feedforward neural networks are a class of predictors characterized by a directed graph, where each node computes a weighted linear combination of its inputs, followed by a non-linear scalar activation function. Propagating the input forward through the graph, the net returns its value via the output of the final layer in the graph. Artificial neural nets are a known to have broad expressive power [17, 18, 20]. A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].\nThe biggest hurdle in training neural networks is the training time, and indeed most theoretical results in the field are negative results. By reduction to k-coloring, it has been shown that finding the weights that best fit the training set is NP-hard [6]. This theoretical hardness result is contrasted by their good practical performance, hence motivating this research.\nWhen dealing with optimization problems such as minimizing the loss over a sample as a function of the weights of the network, a pivotal property of the objective function is the number and distribution of its local minima. This property can indicate the difficulty of the minimization problem; If a function has exponentially many poor local minima (as a function of the input dimension), then without any additional knowledge regarding the function, stochastic gradient descent methods (which are the methods of choice for training such networks) are prone to failure, and will likely converge to such a poor local minima depending\nar X\niv :1\n51 1.\n04 21\n0v 1\n[ cs\n.L G\n] 1\n3 N\nov 2\non the starting point of the algorithm. It is also known that even neural networks comprised of a single neuron might have exponentially many local minima [2].\nAt the other extreme, if a function has a single local minimum, then stochastic gradient descent methods will generally be quite efficient under “most” random starting points. Consider a huge neural network where the size of the layer before the output layer (henceforth, “the last hidden layer”) is at least the size of the training sample. It was recently shown that for such networks, under mild assumptions, global optima are ubiquitous, and “most” starting points will lead to the global optima upon optimizing the weights of the last layer [17]. Unfortunately, due to the huge size of such a network, it is very prone to overfitting. However, it does suggest that large networks are indeed easier to train, and perhaps in such cases the set of points from which SGD methods converge to a satisfactory configuration is relatively large. This naturally motivates the idea of overspecification: Making the network somewhat larger than what would be needed (in terms of reaching a given training error, given unbounded computational power), but not so large as to cause overfitting, in the hope that it will make the training problem easier. There is some theoretical and empirical evidence that this is indeed the case [3, 7, 17], but our theoretical understanding of this phenomenon is still quite limited. A main goal of our work is to further study the effectiveness of overspecification, and identify cases where it is possible to provide rigorous guarantees.\nWhat we do\nOne way to study the hardness of the training problem has been to consider the number of non-global local minima the objective function has [2]. However, it is possible that good local minima (in terms of objective value) are nevertheless easy to find, either by consisting of a large portion of the local minima, or by being significantly more attractive to first order optimization methods, such as stochastic gradient descent. On the other hand, we do not want our results to depend on the exact details of the optimization method used. Therefore, informally speaking, we mostly focus on the following question: For a given network, assuming we initialize the weights at random, what is the probability that we begin from a basin with a good local minimum?\nWe study this question starting with a single neuron (section 3), and then move to fully connected neural nets of higher depth, with a single output neuron and using a rectified linear unit (ReLU) as the activation function (sections 4-6). More specifically, we provide the following results:\n• We begin by extending the result of [2] on exponentially many local minimum basins for single neurons, by showing that it holds even if there exists a hypothesis which achieves an arbitrarily small positive training error. We then show that this result is in fact surprisingly brittle to overspecification: Using the same dataset, and replacing a single neuron by a small two-layer network, the probability of reaching a global minimum basin converges exponentially to 1 with the network size.\n• We identify and formally study geometric properties of the objective function, when training neural networks with ReLU activation function and the squared loss, which may contribute to its amenability to first-order optimization methods. In particular, we show that for ReLU networks of arbitrary depth, with high probability, there exists a strictly decreasing path from a random initialization point to a global minimum. Although this does not ensure that such a global minimum will be reached, it does mean that “crossing valleys” across the non-convex loss surface is not a must to reach a good solution, and accords with recent empirical evidence to this effect [11]. We also upper bound the objective values of local minima for such general networks. Finally, we prove other useful properties more specific for two-layer networks, such as establishing the piece-wise convex structure of the objective function, with respect to the optimization over the weights of the first layer.\n• Focusing on two-layer ReLU nets and the squared loss, we study conditions under which overspecification and random initialization will land us in a good basin with high probability. We first show this under a realizability assumption (i.e. that there exist a small two-layer net attaining zero loss on the training set). However, the required amount of overspecification can scale exponentially with the dimension (although it can also be the data’s intrinsic dimension). Next, using a different technique, and without requiring realizability nor a harsh dependence on the dimension, we also show a similar result for high-dimensional data, as long as the space dimension d exceeds the data size. In general, since we do not incorporate regularization, this is a regime prone to overfitting. However, we strengthen this result, by showing a similar result which holds even for data whose size exceeds d, as long as it is clustered in k ≤ d clusters in general position (which is often a reasonable assumption).\nWe emphasize that when using first order optimization techniques, initializing from an optimal basin does not guarantee we converge to the global minimum, nor does initializing from a bad basin guarantees converging to a bad minimum, as the gradient step taken in gradient descent might throw our algorithm to a different basin, while in stochastic gradient descent further uncertainty is introduced via the stochastic nature of the algorithm. For this reason, we keep our focus on the geometry of the objective function rather than the probability of a specific method producing a good result.\nFinally, we remark that unlike some other works in the field, we focus on the training error and do not explicitly consider the generalization capabilities of neural nets or their expressive power.\nRelated work\nWe begin by discussing the work done in [17], where the authors show that for any non-linear activation function and architecture large enough such that the last hidden layer is at least as large as the size of the sample, then global minima are ubiquitous. The key observation behind this result is that if we assume that our network is large enough, specifically the size of the last hidden layer is larger than the data size, then we can ignore the weights of the previous layers by fixing them arbitrarily and then solve the corresponding optimization problem w.r.t. the weights of the last layer, which will become tractable due to the non-linear nature of the activation function. They also show empirically how overspecification can aid training using stochastic gradient descent. While we shall also highlight the effect overspecification has on the surface of the objective function, our theoretical result focus on networks with mild overspecification, and of size smaller than the data size. Also, the authors investigate regimes in which efficient learning of nets with polynomial activation functions are possible, whereas we shall only focus on the more common activation functions, such as ReLU or sigmoid.\nOther recent efforts in the field include [1, 3, 12]. In [3], the author studies an approach reducing the finite dimensional problem of training a 2-layer neural net paired with ReLUs with a fixed architecture into a problem of finding a sparse measure on an infinite dimensional space (building on work in [5, 19]. In the above framework, the author demonstrates how the problem of training a net of the aforementioned architecture can be reduced to another NP-Hard problem which allows for a geometric interpretation of the problem with potential approximation algorithms. In [1], the authors show that two-layer neural nets of sufficient size can learn low degree polynomials using gradient descent. In [12], the authors present an algorithm with provable guarantees for training neural nets via tensor decompositions. However, strong assumptions need to be made, such as knowing the underlying distribution over the training instances, as well as derivatives of it. Unlike our work, which mainly investigates the geometry of the objective function without highlighting any specific learning algorithm, the authors of the three aforementioned papers are mainly concerned with the algorithmic aspect of learning and approximating the net properly.\nAnother relevant work is [7], in which the authors investigate the surface area of the loss function of neural nets using ReLUs as activation functions, and make somewhat strong distributional assumptions on the data, such as assuming that the coordinates of the observations in the data are standard normally distributed, and in particular the coordinates are independent of one another, and modeling the connection between neurons as a Bernoulli random variable with constant success probability. In addition to these assumptions, the authors also restrict the set of weights of the class of nets to a sphere. The aforementioned assumptions allow the authors to translate the loss over the data as a function of the weights of a fixed depth H and asymptotically increasing network size to the Hamiltonian of the H-spin spherical spin-glass model, which has been studied extensively in the last few decades. Results on the H-spin glass model imply that for asymptotically large networks all critical values of the loss function whose Hessian has a non-diverging number of negative eigenvalues (which indicates a “small” number of descending directions) must lie in a finite interval, under which critical points are unlikely to form, and above it critical points have a diverging number of negative eigenvalues in the size of the network, and represent saddle points. See also [10] and references therein for related works along similar lines. While this work and ours are similar, in that both investigate the geometry of the objective function and its critical points, we pose milder assumptions on the distribution, and provide non-asymptotic guarantees."
    }, {
      "heading" : "2 Preliminaries and notation",
      "text" : ""
    }, {
      "heading" : "2.1 Neural nets and Machine Learning conventions",
      "text" : "We begin by giving a formal definition of the type of artificial neural network studied in this work. A fully connected feedforward artificial neural network is a directed acyclic graphN = (V,E). The neurons can be decomposed into layers where each neuron is connected to all neurons in the succeeding layer, so no cycles are possible. Each neuron computes a function of the form x 7→ φ ( w>x + b ) where w is a weight vector and b is a bias term specific to that neuron, and φ : R → R is a fixed non-linear activation function, such as a rectified linear unit (ReLU, defined as φ (z) = max {0, z}) or a sigmoid (φ (z) = 1\n1+e−z )). For a vector b = (b1, . . . , bn) and a matrix\nW = . . . w1 . . .... . . . wn . . .  letting φ (Wx + b) := ( φ ( w>1 x + b1 ) , . . . , φ ( w>n x + bn )) , we can define a layer of n neurons as\nx 7→ φ (Wx + b)\nFinally, by denoting the output of the ith layer as Oi for , we can define a network of arbitrary depth recursively by\nOi+1 = φ (Wi+1Oi + bi+1)\nwhere Wi,bi represents the matrix of weights and bias of the ith layer, respectively. We assume for simplicity that for multi-layer networks, the final layer h computes a scalar value, and is a purely linear function with no bias, i.e. Oh = 〈Wh, Oh−1〉. Define the depth of the net as the number of layers h, and denote the number of neurons ni in the ith layer as the size of the layer. We define the width of a net as maxi∈[h] ni.\nWe point out that for simplicity, we do not impose any constraints on the weights of each neuron (i.e. regularization, or having convolutional layers).\nMoreover, to simplify the analysis when discussing networks of depth 1 or 2, we will work with nets without bias (where b = 0 for all neurons, not just the output neuron). This is justified, since one could simulate the additional bias term by incrementing the dimension of the data and mapping an instance in the dataset using x 7→ (x, 1) ∈ Rd+1, in this way the last coordinate of the weight of a neuron will function as a bias term. Having such a fixed coordinate does not affect the validity of our results for 1 and 2 layer nets. The only exception is theorem 2, where it is assumed that each training instance is a singleton with a non-zero value on a different coordinate. However, this result serves merely as an example of how the construction from theorem 1 breaks down for two-layer nets (even without bias). With this sole exception, whenever we do not introduce a bias term in our analysis, then one can increment the dimension of the dataset by 1, and apply the result to an architecture with bias. This is done in order to simplify our analysis. We use S = (xt, yt) m t=1 to denote the training data, where xt ∈ Rd represents the tth training instance of dimension d and yt ∈ R represents the corresponding target value, and where m is used to denote the number of instances in the sample.\nWhen referring to a set of predictors H, we identify each predictor in the class using its set of weights, i.e. when our class is the class of single neuron nets (section 3), each predictor is identified using a set of weights w ∈ Rd, where we use the notation wj to denote its jth coordinate, and when our class is the class of two-layer nets (sections 4-6), each predictor is identified using a set of nd+n weights as explained in the next definition.\nDefinition 1. (Two-layer neural network) To represent the output of a fully connected neural net with 2- layers without bias on the input x ∈ Rd, where the first layer is of size n and W = (w1, . . . ,wn) are its weights, and v ∈ Rn are the weights of the output neuron, we define\nNn (W,v) (x) := n∑ i=1 vi · φ (〈wi,x〉)\nwhere 〈a,b〉 := ∑d\ni=1 aibi represents the dot product of two vectors a,b ∈ Rd, and φ : R → R is a non-linear activation function.\nDefinition 2. (Objective function) When discussing a single neuron and various loss functions, we define the objective function as\nES (w) := 1\nm m∑ t=1 L (φ (〈w,xt〉) , yt)\nwhere L is the loss function. When discussing two-layer networks and the squared loss, we use the following notation\nES (Nn (W,v)) := m∑ t=1 (Nn (W,v) (x)− yt)2\nDefinition 3. (Positive-homogeneity) An activation function φ is said to be positive-homogeneous if ∀c ∈ (0,∞) φ (x) = cφ ( x c ) . A ReLU is one such example.\nDefinition 4. ( -realizability) A sample S is said to be -realizable for some ≥ 0 with respect to a class of predictors H if there exists some h ∈ H s.t. ES (h) ≤ , where ES (h) is the loss of h over the sample S. Since the relevant predictor will always be clear from context, we do not state it explicitly when assuming -realizability."
    }, {
      "heading" : "2.2 Basin Value Distribution",
      "text" : "Definition 5. (Basin) We define a basin of the target function f to optimize over, as a connected set C for which ∀α ∈ R the set {x ∈ C : f (x) ≤ α} is also connected.\nBasins are of utmost importance in optimization as a basin cannot contain more than one minimal surface. Throughout our work, we analyze the surface of the objective function by analyzing the minimal value attained in the basin of initialization. Following standard practice, we generally assume that the weights are initialized randomly, where in each subsection we explicitly specify the distribution of the weights.\nDefinition 6. (Basin Value Distribution) For some > 0, in section 3, when discussing single neurons, we let F ( ) denote the probability of initializing the weights from a basin achieving a minimal value of at most . In sections 4-6, we use the notation Fn ( ) to denote the same probability, but with respect to a two-layer neural net of width n.\nWe note that the precise objective function and the initialization distribution depend on the specific result, and will be specified later."
    }, {
      "heading" : "2.3 Region Partitions",
      "text" : "In sections 4-6, we investigate the surface area of the objective function (in the context of ReLU activation functions) via a partition by hyperplanes that the sample induces on the weight space. The following are the definitions used in that context. In a neural network, each neuron receives its input and then computes a dot product with its weights before computing the output of the ReLU and propagating the result forward to the next layer. Since the ReLU is the zero function on one halfspace and the identity function on the other, we seek to understand the behavior of the objective function on each halfspace separately.\nDefinition 7. (Region) For each sample instance xt ∈ S, t ∈ [m] and b ∈ {−1, 1}, we denote the open halfspace Hbt = { w ∈ Rd : 〈w,xt〉 · b > 0 } . Let b = (b1, . . . , bm) ∈ {−1, 1}m, an open region R is defined as the non-empty intersection of m halfspaces\nRbS = m⋂ t=1 Hbtt 6= ∅\nand a closed region is defined as the topological closure of an open region.\nDefinition 8. (Region Partition) The set of all regions RS = { RbS 6= ∅ : b ∈ {−1, 1} m }\nis referred to as the region partition induced by S. We point out that RS is essentially a partition of Rd viewed as the space of weight vectors, and not of the space from which the training instances come from.\nWhen introducing two-layer nets with first layer of size n, and initializing the weights randomly, the weights of each neuron might be initialized from different regions in the region partition. In this scenario when we wish to analyze the initialization point, we seek to investigate the behavior of the objective function over the combination of n regions initialized from.\nDefinition 9. (Region combination) A region combination is defined by\n(R1, . . . , Rn)\nwhere ∀i ∈ [n] wi, the weights initialized for the ith neuron satisfy wi ∈ Ri, where ∀i ∈ [n] Ri ∈ RS .\nWe note that even though there exist points which belong to no open region (the origin is one such example), these points are of Lebesgue measure 0 with respect to the initialization distribution, and therefore we assume that there will always be a region containing the initialization point when initializing from a continuous distribution."
    }, {
      "heading" : "2.4 Analysis on the sphere",
      "text" : "In sections 5-6, we resort to some analysis on the sphere to obtain results on Fn ( ). Throughout those sections, the following definitions are used.\nDefinition 10. (Unit sphere) Let Sd−1 = { x ∈ Rd : ‖x‖2 = 1 } denote the d− 1 dimensional unit sphere.\nDefinition 11. (Hyper-spherical cap) Let Sd−1 (a, θ) = { b ∈ Sd−1 : 〈a,b〉 ≤ cos θ } denote the d− 1 dimensional hyper-spherical cap of spherical radius θ, centered at a.\nDefinition 12. (Spherical distance) Let s (a,b) := arccos (〈a,b〉) denote the spherical distance (the angle between the two vectors) of two points a,b ∈ Sd−1. When the spherical radius between a,b is taken as the smaller angle θ < π2 , s forms a metric on S d−1.\nDefinition 13. (Unit sphere surface area) Let ωd−1 = σd−1 ( Sd−1 ) denote the surface area of the d − 1 dimensional unit sphere, where σd−1 is the d− 1 dimensional Lebesgue measure.\nDefinition 14. (hyper-spherical cap surface area) Let νd (θ) = σd−1 ( sd−1 (a, θ) ) denote the surface area of an angle θ hyper-spherical cap (independent of a).\nDefinition 15. (Closed ball) Let B̄δ (c) = {x : ‖c− x‖2 ≤ δ} denote the closed ball of radius δ centered at c."
    }, {
      "heading" : "3 Hardness result for single neuron nets",
      "text" : "Considering the complicated nature the surface of the objective function, the most natural starting point is the simplest possible architecture, namely a single neuron without bias: x 7→ φ (〈w,x〉). Although simple to analyze and program, nets comprised of a single neuron prove to be very hard to train in the worst case, as the objective function might contain exponentially many poor local minima as a function of the dimension d [2]. However, the authors in [2] also demonstrate that for the 0-realizable case, under some mild conditions on the loss and activation functions, there exists a single minimal surface of the objective function. Bridging the gap between the two aforementioned results, we first provide a construction demonstrating that even if the realizability assumption is only slightly broken, then an exponential number of local minima could still arise. Furthermore, when assuming that the weight vector of the neuron is initialized uniformly at random from the unit sphere, then the distribution of the minimal value in the basin we initialize from is strongly concentrated around a sub-optimal value as the dimension increases. We point out that in this regime where we consider a single neuron, a region in fact serves as a basin - it partitions our optimization space into sections where the objective function is convex in (see lemma 3 for further detail, and note that in the case of a single neuron, a region combination is merely a single region)."
    }, {
      "heading" : "3.1 Exponentially many local minima for -realizable nets",
      "text" : "Theorem 1. Consider a single neuron neural net with transfer function φ : R → R s.t. φ is strictly monotonically increasing and piecewise differentiable on the positive part of the x axis, ∃δ > 0 s.t. φ is constant on the interval [−δ, 0] 1, a symmetric loss function L : R2 → R+ s.t. L (a, b) = 0 ⇐⇒ a = b, where L is strictly monotonically increasing in |a− b|. Also, we assume that for any y ∈ Im (φ) the loss becomes saturated on the negative part of the x axis, i.e. lim\nz→−∞ L (φ (z) , y) = Ly for some Ly ∈ R+. Then\nthere exists a constant c ∈ R which depends only on φ, L and an -realizable sample S for sufficiently small > 0, such that the error over the sample ES contains 2d strict local minima, and F ( 1 4c ) ≤ e− 1 16 d.\nIn other words, we have exponentially many local minima, where the probability of initializing from a sub-optimal basin converges exponentially fast to 1, yet there exists a solution which obtains an error of .\nProof. Let > 0 be sufficiently small (smaller than a loss and activation - dependent constant which shall become evident later). Denote L′ := L (φ (0) , φ (δ)). Since both φ and L are strictly monotonic, we have L′ > 0 and there exists some δ1 ∈ (0, δ) s.t. L (φ (0) , φ (δ1)) < 2 . Consider the sample\nS = {( x1 , δ, y1 , φ (δ1) ) , ( x2 , −δ, y2 , φ (δ) )} We compute\nL (φ (wx1) , y1) =  ∈ (0, 2 ) , w = −1 ∈ (0, 2 ) , w = 0 0, w = δ1δ\nL (φ (wx2) , y2) =  0, w = −1 L′, w = 0\nL′, w = δ1δ\nThus the total error over the sample is\nES (w) , 1\n2 2∑ i=1 L (φ (wxi) , yi) =  < , w = −1 > L ′ 2 , w = 0 L′\n2 , w = δ1 δ\nBy the monotonicity of L, we have that ∀w ≥ 0 L (φ (wx2) , y2) ≥ L′, so the local minimum ES has in (0,∞) is of value ≥ L′2 , and one must necessarily exist as ES (0) > L′\n2 . On the other hand, S is -realizable as ES (−1) < , so by the monotonicity of φ, L there exists some local minimum of value < . We note that since φ is strictly monotonically increasing on the positive part of the x axis, we have that either the loss on x1 or on x2 is strictly monotonic at any point w ∈ R, so ES has exactly 2 local minima. Furthermore, since the loss on x1 is constant ∀w ∈ [−1, 0] and the loss on x2 is constant ∀w ∈ [0, 1], we have that the objective function contains two basins meeting at zero, so the sign of w determines which of the basins we fall into. To conclude the derivation so far, we showed that there is a one-dimensional sample for which ES has exactly two basins, one “good” (with minimal value at most ) and one “bad” (with minimal value at least L′\n2 ).\n1φ (x) = max (0, x) - ReLU is one example for such a transfer function.\nWe now extend our sample to be d-dimensional in a similar manner as did the authors in [2] as follows: For i = 1, 2 and j ∈ [d], we use the mapping xi,j 7→ (0, . . . , 0, xi, 0, . . . , 0) where the non-zero coordinate is the jth coordinate. It is straightforward to show that the partial derivative ∂∂wj is 0 for xi,k with j 6= k, so the geometry of the surface of the objective function ES is independent for each coordinate separately. Now, every Cartesian product of local minima in the one-dimensional setting form a d-dimensional local minimum. Since we have at least two local minima, a good and another bad one in each coordinate, this combines into 2d local minima, where each minimum’s value would be the average of the one-dimensional minima forming it. Note that the combination of all good minima forms the global minimum with value < , thus the data is -realizable. We stress that an important property of this initialization scheme is that the signs of the coordinates of the initialization point is uniformly distributed on the Boolean cube, as it implies that on each coordinate, independently, we have a probability 0.5 of reaching a bad basin, hence the number of bad basins we initialize from is distributed B (d, 0.5). Letting c := L ′\n2 , we have from Chernoff‘s bound that\nF\n( 1\n4 c\n) ≤ e− 1 16 d\nIn the appendix, we provide a detailed example of this theorem and the construction for the squared loss, and another example for sigmoid activation which demonstrates how a similar construction can be made for activation functions which are not constant over some interval [−δ, 0]. A potential drawback of the general theorem is that the norm of the training instances is not necessarily bounded as a function of 1 . However, in one of the examples we show that the same issues can occur even if the data norm scales only logarithmically with 1 . We also remark that the overwhelming majority of the local minima formed above have an objective value sub-optimal by Ω(c)."
    }, {
      "heading" : "3.2 Learning singleton data sets using ReLUs",
      "text" : "In subsection 3.1, we’ve seen that even when achieving an arbitrarily small error using a single neuron is possible, learning might still be a difficult task. The hardness results above rely on analysis of training sets S of the form\n{((xt, 0, . . . , 0) , yt) , ((0,xt, 0, . . . , 0) , yt) , . . . , ((0, . . . , 0,xt) , yt)}\nAlong each coordinate there are two local minima, only one of which is good. Under the initialization distribution considered, the probabilities of hitting the good basin along each coordinate are independent and strictly less than 1, hence the probability of “hitting” the right basin across all coordinates is exponentially small.\nAs discussed in the introduction, it is natural to study what happens to such a hardness construction under overspecification, which here means replacing a single neuron by a two-layer network of some width n > 1. Surprisingly, it turns out that in this case, the probability of reaching a sub-optimal region decays exponentially in n. Intuitively, this is because for such constructions, for each coordinate it is enough that one of the n neurons in the first layer will initialize at a correct basin. This will happen with overwhelming probability if n is moderately large.\nWe are going to look at a simple situation, where the output neuron is fixed, and we only optimize over the neurons in the first layer. Specifically, we will consider ReLU activation paired with the squared loss, with an initialization distribution where the weight vector of the output neuron is initialized to be in {−1, 1}n\nuniformly at random, and the weights of each neuron in the first layer is drawn uniformly at random from the unit sphere (we note that our results would still hold using other scales – see lemma 1 and lemma 2 below for further details). We then consider optimizing the weights of the first layer only, i.e.\nmin w1,...,wn\n1\nm m∑ t=1 ( n∑ i=1 vi max {0, 〈wi,xt〉} − yt )2 (1)\nNote that for every wi, this function is convex and quadratic in wi inside a region (as defined in definition 7, i.e. the region where the sign of 〈wi,xt〉 remains the same for all t). Moreover, over any such combination of regions (one for each wi), the function above is convex and quadratic, and therefore corresponds to a basin over (w1, . . . ,wn) (we will derive this fact more rigorously in subsection 4.2). As a result, to get a lower bound on the probability of starting at a good basin for Equation 1, it is enough to lower bound the probability of initializing at a region combination, in which there is a low minimal value. Based on this observation, we have the following theorem:\nTheorem 2. Let S ⊆ Rd be a finite sample comprised of singletons (i.e. ∀x ∈ S ∃j ∈ [d] s.t. xj 6= 0 and xi = 0 for any i 6= j). Let α denote the optimal value that can be achieved on S using a single neuron with the ReLU activation function. Then\nFn (α) ≥ 1− 4d ( 3\n4 )n Where Fn is defined with respect to the initialization distribution discussed above, and the objective function in Equation 1.\nProof. Denote Sj = { x ∈ S : xj 6= 0 } , S+j = { x ∈ S : xj > 0 } , S−j = { x ∈ S : xj < 0 } Let wji , i ∈ [n] , j ∈ [d] denote the jth coordinate of the ith neuron in the first layer. Since the order of the neurons in the first layer does not matter, we may assume w.l.o.g. that v, the weights of the second layer, is of the form\nv = 1, . . . , 1︸ ︷︷ ︸ ×k ,−1, . . . ,−1︸ ︷︷ ︸ ×n−k  for some k ∈ {0, . . . , n}. For fixed j we have\nESj (Nn (W,v)) = ES+j (Nn (W,v)) + ES−j (Nn (W,v))\n= ∑ x∈Sj ( k∑ `=1 φ ( xjwj` ) − n∑ `=k+1 φ ( xjwj` ) − y (x) )2\nso for j1 6= j2, ESj1 (Nn (W,v)) and ESj2 (Nn (W,v)) are independent. Thus, in order to minimize ES (Nn (W,v)) = ∑d j=1ESj (Nn (W,v)) it is sufficient to minimize ESj (Nn (W,v)) for each j ∈ [d] separately. Clearly, minimizing ESj is equivalent to minimizing ES+j and ES−j separately, as once more these two are\nindependent of one another due to the nature of the ReLU activation function. Observing that the prediction of Nn (W,v) on any x ∈ S+j is simply a linear combination of the weights satisfying wj` > 0, ` ∈ [n], we have that composing this linear combination with the convex squared loss yields a convex function. We note that the regions of the region partition induced by S+j are just the two halfspaces induced by any x ∈ S+j , so ES+j vanishes on one halfspace, and is convex on the other halfspace, therefore we have thatES+j has a single global minimum on the halfspace it does not vanish in, and the same goes for S−j on the other halfspace. Denote the optimal points minimizing ES+j , ES−j as h + j , h − j respectively, we have that a sufficient condition for an orthant to contain a predictor minimizing ES+j is having w j i1 ,wji2 > 0 for i1, i2 ∈ {1, . . . , k} × {k + 1, . . . , n} since: If h+j > 0 then\nwji =\n{ h+j , i = i1\n0, i 6= i1 achieves the minimum. If h+j < 0 then\nwji = { −h+j , i = i2 0, i 6= i2\nachieves the minimum. Similarly, an orthant containing a predictor with wji1 ,w j i2 < 0 for i1, i2 ∈ {1, . . . , k}× {k + 1, . . . , n} will minimize ES−j . If we initialize a point on the unit sphere, then the probability of having exactly k positive weights on the output neuron is ( n k ) 2−n. The probability of having weights of different sign in the first layer connected to the k positive weights is 1 − 1 2k−1\n, and the probability of having weights of different sign in the first layer connected to the n− k negative weights is 1− 1\n2n−k−1 .\nOverall, the probability of initializing from an orthant which contains a global minimum is\nn−1∑ k=1 ( n k ) 2−n ( 1− 1 2k−1 )( 1− 1 2n−k−1 ) = 1− 4 ( 3\n4\n)n + 6 ( 2\n4\n)n − 4 ( 1\n4 )n ≥ 1− 4 ( 3\n4 )n Assuming n ≥ 5 and using Bernoulli’s inequality, the probability of this happening for all j ∈ [d] is\n≥ ( 1− 4 ( 3\n4 )n)d ≥ 1− 4d ( 3\n4\n)n\nThis result is an interesting example of the power of overspecification, as more neurons in the first layer of the chosen architecture lead to a rapid exponential decay in the values of the local minima of the objective function, and motivates us in further studying overspecification in the following sections."
    }, {
      "heading" : "4 Structural properties of the objective function",
      "text" : "In this section, we state and prove several results regarding the geometry of the objective function, which allow us to gain more insight on its structure, as well as lay the foundations for the results to come in sections 5, 6. From this section and onwards, we always consider ReLU activation functions and the squared loss."
    }, {
      "heading" : "4.1 Existence of a path to the global minimum",
      "text" : "Moving to a general regime where our net is of arbitrary depth and with a single output neuron, theorem 3 below demonstrates that under mild conditions, most of the surface of the objective function overlooks the global minimum. i.e., there exists a monotonically decreasing path from “high” starting points to the global minimum. Moreover, in theorem 4, we demonstrate that with a suitable random initialization, we are overwhelmingly likely to initialize at such a starting point.\nIt should be noted that this monotonic path could be difficult to find using first order optimization techniques. However, this result nevertheless sheds light on the nature of the surface function, demonstrating that it is not completely sporadic in the sense that “crossing valleys” is not a must to reach a good solution, and accords with recent empirical evidence to this effect [11].\nTheorem 3. Let N (~w) ∈ Rm denote the vector of outputs of a ReLU net over a sample S, where we view the set of weights and bias terms of the net as a single vector ~w. And consider the objective function with respect to the squared loss ES (N (~w)) = ∑m t=1 (N (~w) (xt)− yt)\n2. Let winit denote an initialization point satisfying ‖N (winit)− y‖2 > ‖y‖2 and let wopt denote a global minimum. Assume that there exists a continuous path γ (t), t ∈ [0, 1] s.t. γ (0) = winit, γ (1) = wopt and ∀t ∈ [0, 1], there exist an instance x in the training set such that N (γ(t)) (x) 6= 0. Then there exists a strictly decreasing path from winit to wopt.\nIn theorem 4, we show that the requirement of ‖N (winit)− y‖2 > ‖y‖2 indeed holds with high probability under a suitable random initialization. The requirement that N(γ(t)) is not always zero also appears reasonable, as for most neural nets and training sets it is generally possible to continuously change weights from one configuration to another, without making the prediction zero at all data points.\nProof. Noting that the squared loss over the sample for weights w can be represented as 1m ‖N (w)− y‖ 2 2, we begin by establishing the following inequality. We have\n‖N (winit)− y‖2 > ‖y‖2\n⇒ ‖N (winit)‖2 − 2 〈N (winit) ,y〉 > 0 (2)\nDefine Nt := N (γ (t)), we once again use the positively-homogeneous nature of the ReLU to scale the weights of the net by a positive factor at > 0, while stressing that the addition of a bias term does not affect this trick and nevertheless allows us to scale the weights by a constant positive factor, effectively forcing ∀t ∈ [0, 1] the equality\n‖atNt − y‖2 = ‖N0 − y‖2 (1− t) + ‖y‖2 t\nwhich will yield a path from winit to a1wopt with an error over the sample monotonically decreasing to 1 m ‖y‖ 2. Solving the above equality for at\na2t ‖Nt‖ 2 − 2at 〈Nt,y〉+ ‖y‖2 = ( ‖N0‖2 − 2 〈N0,y〉 ) (1− t) + ‖y‖2\n⇒ a2t ‖Nt‖ 2 − 2at 〈Nt,y〉+ ( 2 〈N0,y〉 − ‖N0‖2 ) (1− t) = 0\ntaking the positive root yields\nat =\n2 〈Nt,y〉+ √ 4 〈Nt,y〉2 − 4 ‖Nt‖2 ( 2 〈N0,y〉 − ‖N0‖2 ) (1− t)\n2 ‖Nt‖2\n=\n〈Nt,y〉+ √ 〈Nt,y〉2 − ‖Nt‖2 ( 2 〈N0,y〉 − ‖N0‖2 ) (1− t)\n‖Nt‖2\nwe note that from Equation 2 we have that ‖Nt‖2 (1− t) ( 2 〈N0,y〉 − ‖N0‖2 ) < 0, thus the discriminant is > |〈Nt,y〉|, so at is well defined and positive. If a1 = 1 then γ̃ (t) = atγ (t) is a strictly decreasing path from winit to wopt. Otherwise, since ‖aN1 − y‖2 is convex in a, we have by the optimality assumption of wopt that a = 1 is the global minimum, thus γ′ (t) = (a1 (1− t) + t)wopt results in a strictly decreasing path from a1γ (1) to wopt. Finally, concatenating both γ, γ′ yields a strictly decreasing path from winit to wopt.\nTheorem 3 required that we initialize from a point achieving an error of more than 1m ‖y‖ 2. We now show that this requirement is fulfilled with probability ≥ 14 (1− 2 −nh−1), where nh−1 is the size of the last hidden layer. In the following theorem, we assume that all neurons are initialized uniformly from the unit sphere. Moreover, as in the case of a single neuron, the theorem would work as well with initializing at any other origin-centered sphere (see proof for details).\nTheorem 4. Let N ( W h1 ,b h 1 ) (x) : Rd → R be a fully connected ReLU neural network of depth h with the ReLU activation function, where Wi,bi, i ∈ [h] are the weight matrix and bias vector of the ith layer respectively, and let ni be the number of neurons in the ith layer. Let w denote the weights of the entire network (including bias), and suppose that w is distributed uniformly over the unit sphere, i.e. assume that ‖w‖ = 1 w.p. 1. Then\nPr [ m∑ t=1 ( N ( W h1 ,b h 1 ) (xt)− yt )2 > ‖y‖2 ] ≥ 1 4 ( 1− 2−nh−1 ) Proof. Observe that following our distribution assumption, the signs of the coordinate of each weight are distributed uniformly and independently (in fact, our result would apply for any distribution which induces such a distribution on the signs of the coordinates, and in particular initializing uniformly from any origincentered sphere) which implies that ∀x ∈ Rd, Pr [〈w,x〉+ b > 0] = Pr [〈w,x〉 − b < 0] = 12 , so we have that Pr [φ (〈w,x〉+ b) 6= 0] = 12 . Thus any neuron except the output neuron outputs a non-zero value w.p. 1 2 , regardless of its input x. Thus for the output neuron to receive input 6= 0 we need at least one neuron in the (h− 1)th layer to output a non-zero value, which happens w.p. at least 1−2−nh−1 . Given that the output neuron does receive an input 6= 0, then w.p. 12 its output will be non-zero.\nNow, let p ∈ Rm denote the output vector of the net on S, so the error on the sample is given by 1m ‖p− y‖ 2. Given that p 6= 0, we have ‖p‖2 > 0, and since either one of 〈p,y〉 , 〈−p,y〉 is non-negative we get that\nmax { ‖p− y‖2 , ‖−p− y‖2 } > ‖y‖2\nBy the symmetry of the distribution of the weights, we have w.p. 12 that indeed the error is greater than ‖y‖2, and we have\nPr [ m∑ t=1 ( N ( W h1 ,b h 1 ) (xt)− yt )2 > ‖y‖2 ] ≥ ( 1− 2−nh−1 ) · 1 2 · 1 2\n= 1\n4\n( 1− 2−nh−1 ) concluding the proof of the theorem.\nAnother interesting corollary of theorem 3 is that the objective value of any local minimum cannot be too large:\nCorollary 1. Let N as in 3, and suppose that ∀w s.t. ‖N (w)− y‖2 > ‖y‖2 there exists a path γ as in 3, then there are no local minima with value > 1m ‖y‖ 2.\nProof. Suppose by contradiction that there exists a local minimum w with\n‖N (w)− y‖2 > ‖y‖2\nThen for = ‖N (w)− y‖2 − ‖y‖2 > 0 we have by the continuity of N a δ > 0 and an environment Bδ (w) s.t.\n∀w′ ∈ Bδ (w) ∣∣∣∥∥N (w′)− y∥∥2 − ‖N (w)− y‖2∣∣∣ <\n⇒ ∀w′ ∈ Bδ (w) ∥∥N (w′)− y∥∥2 > ‖y‖2\nbut since ‖N (w)− y‖2 > ‖y‖2 we have from theorem 3 a monotonically decreasing path to wopt, which is a contradiction."
    }, {
      "heading" : "4.2 Partitioning the weight space of the objective function",
      "text" : "We now move to investigating 2-layer ReLU neural nets with respect to the squared loss, where the objective function has the form\n1\nm m∑ t=1 ( n∑ i=1 vi max {0, 〈wi,xt〉} − yt )2 . (3)\nAlthough still relatively simple, they already possess universal approximation properties [8], and encapsulate the challenge of handling a highly non-convex objective function.\nThe goal of this subsection is to capture some important structural properties of such 2-layer networks, which will be used in deriving the results of the following sections. Intuitively, our formal lemmas capture the following two simple but important observations:\n• First, due to the positive homogeneity of the ReLU function, the neural network is invariant to multiplying each wi and dividing the corresponding vi by the same positive factor c. Therefore, in terms of expressiveness, there is no loss of generality by assuming that each vi is in {−1,+1}. In fact, for the results we will show later on (and similar to the results of subsection 3.2), it is enough to just randomly choose and fix each vi ∈ {−1,+1} uniformly at random, and optimize over the weights of the first layer only.\n• Second, the ReLU function is piecewise linear, depending on the sign of 〈w,x〉 so equation 3 is in fact a convex quadratic function of w1, . . . ,wn inside each region where the signs of 〈wi,xt〉 remain the same for all i, t (see the formal definition of a region in section 2). Therefore, equation 3 is piecewise quadratic as a function of w1, . . . ,wn. Moreover, by definition, each such convex quadratic region is a basin. As a result, in a scenario where we optimize over the weights of the first layer only, in order to lower bound the probability of initializing at a basin with a local minima at most , it is enough to lower bound the probability of initializing at a region combination, inside which the minimal value is at most .\nWe begin with the following lemma, which implies that regions (as defined in section 2) are in fact cones. Therefore, the region combination we are in is not affected by rescaling the first layer weights by constant positive factors. As a result, in terms of analyzing which region combination we land on, we can assume w.l.o.g. uniform random initialization over the unit sphere, rather than a sphere of some other radius.\nLemma 1. Let S denote the dataset, let (R1, . . . , Rn), ∀i ∈ [n] Ri ∈ RS be a region combination, wi ∈ Ri, a > 0. Then awi ∈ Ri.\nProof. ∀i ∈ [n], let bi = ( bi1, . . . , b i m ) ∈ {−1, 1}m s.t. Ri = Rb i S , then\nwi ∈ Ri ⇒ ∀t ∈ [m] 〈wi,xt〉 · bit > 0 ⇒ ∀t ∈ [m] a 〈wi,xt〉 · bit > 0 ⇒ ∀t ∈ [m] 〈awi,xt〉 · bit > 0 ⇒ awi ∈ R\nWe continue by reminding the reader a few notational conventions (formally defined in section 2): Let Nn (W, v) denote a net with architecture x 7→ v>φ (Wx), with a first layer of size n where v ∈ Rn,W = (w1, . . . .wn) ∈ Rn×d are the weights. We note that as was previously explained in 2.1, we do not introduce a bias term in the analysis whenever possible. Denote by ES (Nn (W,v)) the average squared loss of Nn on a given sample S = {(xt, yt)}mt=1 with weights W,v. Finally, let φ (x) = max {0, x} be the ReLU function, and recall that it is positive homogeneous in the sense that for any c > 0, φ (x) = cφ (x/c).\nLemma 2. (Reduction of the optimization problem to nets of the form x 7→ a>φ (Wx) where a ∈ {−1, 1}n): Let S = {(xt, yt)}mt=1 be a finite sample, let Nn (W,v) as above. Let Nn (W,a) denote the net x 7→ a>φ (Wx) where a ∈ {−1, 1}n. Denote by ES (Nn (W,v)) the squared loss of Nn on the sample S with weights W,v and let R = (R1, . . . , Rn) be some closed region combination. Then for all W = (w1, . . . ,wn) ∈ R and for all v = (v1, . . . , vn) ∈ Rn there exists W ′ ∈ R,a = (a1, . . . , an) ∈ {−1, 1}n s.t. ∀x ∈ Rd Nn (W,v) (x) = Nn (W,a) (x) and ∀i ∈ [n] vi · ai ≥ 0.\nProof. Clearly, the left-hand side of the equality is not greater than the right-hand side as {−1, 1}n ⊆ Rn. Let R = (R1, . . . , Rn) be some region combination, let(W,v) ∈ R × Rn be some arbitrary point. If vi = 0 for some i ∈ [n], then the output of the i-th neuron is always canceled. So by modifying wi := 0 and vi = 1 we have an equivalent net, so we may assume w.l.o.g ∀i vi 6= 0. Now, by using the positivehomogeneity of the ReLU and taking ci = |vi|, we have an equivalent net with weights (W,a) for all k ∈ N s.t. a ∈ {−1, 1}n. We note that from lemma 1, any rescaling done does not change the region combination W belongs to, so W ′ ∈ R, and since vi is scaled by a positive constant then vi · ai ≥ 0, which concludes the proof of the lemma.\nWhat lemma 2 in fact tells us, is that in terms of the function computed by a two-layer network, the two architectures are equivalent. For this reason, from this point onwards we consider the optimization problem over W and a ∈ {−1, 1}n. Rather than optimizing over the discrete set a ∈ {−1, 1}n, we will just pick those uniformly at random and fix them. Namely, in the context of two-layer networks, we will consider an optimization scenario where we only optimize over W (or equivalently, w1, . . . ,wn in equation 3). While this changes somewhat the optimization problem it would turn out that for the scenarios considered later in the work, it is enough for attaining good basins with high probability, and the analysis is conceptually much simpler.\nWe now turn to characterize the piecewise-quadratic nature of ES , as a function of W :\nLemma 3. (Piecewise quadratic structure of the objective function): Let Nn (W,v) as above. Then ES (Nn (W,v)) where we restrict W to any region combination induced by the sample is quadratic and convex in W .\nProof. We note that Nn (W, v) where W is restricted to some region combination is a linear combination of affine functions, and since the squared loss is convex, their composition is convex and quadratic. Summing over all samples, we obtain a convex and quadratic function.\nA simple implementation of lemma 3 allows us to bound the value of any local minimum which the objective function obtains, as follows:\nCorollary 2. ∀n ∈ N, Fn ( 1 m ∑m t=1 y 2 t ) = 1. i.e. there are no local minima achieving an error of more than 1 m ∑m t=1 y 2 t .\nProof. Let (W, v) ∈ Rn×d × {−1, 1}n be a local minimum point, then W is contained in some region combination. Since the origin ~0 ∈ Rn×d is contained inside the closure of all region combinations, and since the objective function is convex in W in each region combination, we have that ES (Nn (W, v)) ≤ ES (Nn (0, v)). But since\nES (Nn (0, v)) = 1\nm m∑ t=1 y2t\nthe theorem follows.\nWe point out that corollary 2 is a special case of corollary 1, applied to two-layer nets, but nevertheless shows corollary 1 still holds even without the assumption of the existence of a path γ."
    }, {
      "heading" : "5 Successful random initialization paired with overspecification",
      "text" : "In this section we provide results demonstrating that for data which is realizable using two-layer networks, that the probability of initializing from a basin containing a global minimum increases as we add more neurons to the first layer, corresponding to the idea of overspecification. We note that these results hold without significant additional assumptions, but on the flip side, the number of neurons required to guarantee a constant success probability increases exponentially with the dimension d. Thus, the result is only meaningful when the dimension is modest. In the next section, we show results with a much weaker dependence on d, but under additional assumptions. Following the discussion in the previous section, we consider an initialization scheme where the weights of the output neuron is chosen from {−1,+1}n and then fixed, the weights of each neuron in the first layer is chosen uniformly at random from the unit sphere, and Fn( ) (the probability of initializing at a basin with minimal value at most ) is defined with respect to that initialization scheme. Also, we assume that the dataset satisfies ∀t ‖xt‖ ≤ 1. We stress that this assumption is without much loss of generality, as a similar result can be shown to hold for any other bound on the data.\nTheorem 5. Let > 0, let S be a 0-realizable data set w.r.t. two-layer nets of width n, satisfying ∀t ∈ [m] ‖xt‖2 ≤ 1. Let (W ∗,v∗), W ∗ = (w∗1, . . . ,w∗n), w∗i ∈ Rd denote a global minimum achieving ES (Nn (W ∗,v∗)) = 0, let B s.t. ∀i ∈ [n] ‖w∗i ‖ ≤ B, and define for any c > 0\npj ( ) := 1\n2π (d− 1)\n(√ nB · √ 1− 4n2B2 )j Nc := (1 + c) ⌈ n\npd−1 ( ) ⌉ Then\nFN ( ) ≥ max ( 1\n2 , 1− exp\n( − 1\n4pd−1 ( )\nc2\n(1 + c) n )) To proceed, we will need the following two technical lemmas, whose proof may be found in appendix\n8.3.1, 8.3.2.\nLemma 4. Let δ > 0 and let a ∈ Sd−1 ⊆ Rd be a point on the d − 1-dimensional unit sphere. Let b be a point chosen uniformly at random on Sd−1. Then\nPr [‖a− b‖2 ≤ δ] ≥ 1\nπ (d− 1)\n( δ √ 1− δ 2\n4\n)d−1\nLemma 5. Let x = ( x1, . . . , xd ) ∈ Rd, then Nn (w1, . . . ,wn,v) (x) is ‖x‖-Lipschitz in each wi.\nProof. Initializing some W (N) ∈ RN×d uniformly on the unit sphere and some v(N) ∈ {−1, 1}N uniformly, let A denote the event where there exists a subset of n neurons with weights denoted ( W (n),v(n) ) such that\nES ( Nn ( W (n),v(n) )) ≤\nWe now show that the occurrence of A is sufficient for having\nES ( NN ( W (N),v(N) )) ≤\nSuppose A occurs, and assume w.l.o.g. that the n neurons achieving an error of less than are the first n neurons. Then we have that\nES ( NN ( W (n), 0, . . . , 0,v(N) )) = ES ( Nn ( W (n),v(n) )) ≤\nOn the other hand, ( W (N),v(N) ) belongs to some region combination. In particular, we have that(\nW (n), 0, . . . , 0,v(N) )\nbelongs to the closure of the same region combination, as the closure of all regions contains the origin. By lemma 3, we have that ES is convex in W (N) on the given region combination, so the minimum of the region combination satisfies\nES ( NN ( W ′,v′ )) ≤ ES ( NN ( W (n), 0, . . . , 0,v(N) )) where (W ′,v′) denotes the point in the region combination achieving its local minimum. Thus we have that the occurrence of A implies achieving an error ≤ . To finish the proof, we now need to lower bound Pr [A]. For each neuron with weights wi ∈ Rd s.t. ‖wi‖ = 1, we have\nPr [ ‖‖w∗i ‖ ·wi −w∗i ‖ ≤ √\nn\n] = Pr [∥∥∥∥wi − w∗i‖w∗i ‖ ∥∥∥∥ ≤ √ n ‖w∗i ‖ ] lemma 4 ≥ 1\nπ (d− 1)\n( √ n ‖w∗i ‖ · √ 1− 4n2 ‖w∗i ‖ 2 )d−1\n≥ 1 π (d− 1)\n(√ nB · √ 1− 4n2B2 )d−1 = 2pd−1 ( )\nand also Pr [vi = v∗i ] = 1\n2\nand since the two events are independent we have that both occur w.p. pd−1 ( ). Also, this event is independent for each neuron, so we have w.p. at least pd−1 ( ) for each neuron to initialize ‘close‘ enough to (w∗i , v ∗ i ). In this sense, we can lower bound the number of good initializations from below using X ∼ B (N, pd−1 ( )), where B (N, p) is the binomial distribution. The median of a binomially distributed random variable is dNpd−1 ( )e or bNpd−1 ( )c (or both), so either case we have Pr [X ≥ dNpd−1 ( )e] ≥ 12 or Pr [X ≥ dNpd−1 ( )e] ≥ Pr [X ≥ bNpd−1 ( )c] ≥ 12 , therefore we compute\n1 2 ≤ Pr [X ≥ dNpd−1 ( )e]\n= Pr [ X ≥ ⌈⌈ n\npd−1 ( )\n⌉ pd−1 ( ) ⌉] ≤ Pr [ X ≥ ⌈ n\npd−1 ( ) pd−1 ( ) ⌉] = Pr [X ≥ n]\nAlso, by using Chernoff‘s bound we can bound the tail of X as follows\nF ( n; (1 + c) ⌈ n\npd−1 ( )\n⌉ , pd−1 ( ) )\n≤ exp − 1 2pd−1 ( ) ( (1 + c) ⌈ n pd−1( ) ⌉ pd−1 ( )− n )2 (1 + c) ⌈ n\npd−1( )\n⌉ \n≤ exp ( −1\n4\nc2\n(1 + c) n ) thus with probability ≥ max ( 1 2 , 1− exp ( −14 c2 (1+c)n )) we have n neurons s.t. ‖‖w∗i ‖ ·wi −w∗i ‖ ≤ √ n and vi = v ∗ i . Assume w.l.o.g. that these neurons are numbered (wi, vi) for i = 1, . . . , n and let\nWi = ( w∗1, . . . ,w ∗ i , ∥∥w∗i+1∥∥wi+1, . . . , ‖w∗n‖wn)\nfor i = 0, . . . , n. Observing thatWn = W ∗, and thatW0 andW both belong to the same region combination as a result of lemma 1, we can substitute Nn (W,v∗) with Nn (W0,v∗) w.l.o.g., as initializing from either is equivalent. We have\nES (Nn (W0,v ∗)) =\n1\nm m∑ t=1 (Nn (W0,v ∗) (xt)− yt)2\n= 1\nm m∑ t=1 ∣∣∣∣∣ n∑ i=1 (Nn (Wi−1,v ∗) (xt)−Nn (Wi,v∗) (xt)) ∣∣∣∣∣ 2\ntriangle inequality ≤ 1\nm m∑ t=1 ( n∑ i=1 |Nn (Wi−1,v∗) (xt)−Nn (Wi,v∗) (xt)| )2 lemma 5 ≤ 1\nm m∑ t=1 ∣∣∣∣∣‖xt‖ ( n∑ i=1 ‖Wi−1 −Wi‖ )∣∣∣∣∣ 2\n≤ ( n∑ i=1 ‖Wi−1 −Wi‖ )2\n= ( n∑ i=1 ‖‖w∗i ‖ ·wi −w∗i ‖ )2\n≤ ( n∑ i=1 √ n )2 =\nthus concluding the proof of theorem 5.\nTheorem 5 shows that the more neurons we use, the more likely we are to capture a good initialization on a subset of the neurons which will lead to a good minimum, albeit the number of neurons needed to have\na non-negligible success rate is exponential in the dimension. We do note, however, that the result remains essentially the same if the dimension d is replaced by the intrinsic data dimension, or rank of the matrix X which columns are x1, . . . ,xm. Roughly speaking, this is because the region partition corresponds to an embedding in Rd of a partition in the column space ofX , so its structure remains similar. Formally, we have the following theorem:\nTheorem 6. Let > 0, S, (W ∗,v∗) and B as above, and further assume that Rank (X) = k < d where X is the matrix which columns are x1, . . . ,xm , and define for any c > 0\nNc := (1 + c)\n⌈ n\npk−1 ( ) ⌉ Then\nFN ( ) ≥ max ( 1\n2 , 1− exp\n( − 1\n4pk−1 ( )\nc2\n(1 + c) n )) Proof. Let V = span (x1, . . . ,xm), and define T (x) := v‖v‖2 where x = v + v\n⊥ for v ∈ V,v⊥ ∈ V ⊥. First, we observe that for any initialization W = (w1, . . . ,wn), W and T (W ) := (T (w1) , . . . , T (wn)) both belong to the same region combination as ∀i ∈ [n] , ∀t ∈ [m]\n〈xt,wi〉 = 〈 xt, ‖w‖2 · T (wi) + w ⊥ i 〉 = 〈xt, ‖w‖2 · T (wi)〉+ 〈 xt,w ⊥ i\n〉 = 〈xt, ‖w‖2 · T (wi)〉 = ‖w‖2 · 〈xt, T (wi)〉\n⇒ sign (〈xt,wi〉) = sign (〈xt, T (wi)〉)\nthus bothW,T (W ) belong to the same basin achieving the same minimal value. Since any rotation Θ under which V ⊥ is invariant commutes with T , we have for any measurable set A ⊆ V\nσk−1 (A) = σd−1 ( ΘT−1 (A) ) = σd−1 ( T−1 (ΘA) ) So initializing uniformly on Sd−1 is equivalent to initializing uniformly on Sk−1 in the sense of the region combination we initialize from, and we are reduced to the conditions of theorem 5.\n6 Learning m-rank or clustered data\nIn this section, we will show that when learning data of high dimension (specifically, when the dimension satisfiesm ≤ d, wherem is the data size), local minima are not an issue. Building on the previous result, we then show that when our data is clustered into k ≤ d small enough clusters, then once again local minima will not pose a problem. We stress that we follow the same initialization procedure as in section 5, and optimize only over the weights of the first layer."
    }, {
      "heading" : "6.1 Learning Rank m Data",
      "text" : "We now assume that our data matrix X satisfies RankX = m, where m is the number of data points. We note that this immediately implies m ≤ d. Namely, that the number of training examples is not larger than\nthe dimension. Even though this regime might be strongly prone to overfitting, this allows us to investigate the surface area of the objective function effectively, while also serving as a base for the clustered data scenario that we’ll be studying in theorem 8. As in the previous section, Fn is defined with respect to an initialization distribution where the output neuron is fixed at random, and where the optimization is done with respect to the first layer. Our main result in this section is the following theorem, which implies that under the rank assumption, a two-layer network of size O (log (m)) is sufficient to initialize in a basin with 0 error with overwhelming probability.\nTheorem 7. Assume RankX = m, and let Y = (y1, . . . , ym) be arbitrary. Then\nFn (0) ≥ 1−m ( 3\n4 )n Proof. Denote the initialization point as W = (w1, . . . ,wn), and define w′i = ∑m t′=1 ai,t′xt′ where ai,t′ ∈ R are to be determined later. We want to show that for well chosen values of ai,t′ , w′i belongs to the same region as wi and achieves a perfect prediction over a certain subset of S while achieving a prediction of 0 over the rest of the sample, effectively predicting the subset without affecting the prediction over the rest of the sample. By combining enough neurons in this manner, we are able to obtain a perfect prediction over the data, i.e., an error of 0:\nDefine the vector y′i =  y ′ i,1 ...\ny′i,m\n where\ny′i,t = { |yt| 〈wi,xt〉 > 0, vi · yt ≥ 0, ∀j < i 〈wj ,xt〉 ≤ 0 ∧ vj · yt < 0 0 o.w.\nand choose ai = ai,1... ai,m  s.t. the equality XX>ai = y ′ i\nholds. We first stress that by our assumption,\nXX> = 〈x1,x1〉 . . . 〈x1,xt〉... . . . ... 〈xt,x1〉 . . . 〈xt,xt〉  ∈M(m×m) is of rank m, and therefore ai exists and is well-defined. Assuming that for any t ∈ [m] there exists some neuron i s.t. 〈wi,xt〉 > 0, vi · yt ≥ 0 (We will later analyze the probability of this actually happening), we compute the prediction of our net with weights\nW ′ = (w′1, . . . ,w ′ n) on xt:\nNn ( W ′,v ) (xt) = n∑ i=1 viφ (〈 w′i,xt 〉) =\nn∑ i=1 viφ (〈 m∑ t′=1 ai,t′xt′ ,xt 〉)\n= n∑ i=1 viφ ( m∑ t′=1 ai,t′ 〈xt′ ,xt〉 )\n= n∑ i=1 viφ ( y′i,t )\n= n∑ i=1 viφ ( |yt| · 1〈wi,xt〉>0, vi·yt≥0, ∀j<i 〈wj ,xt〉≤0∧vj ·yt<0 ) =\nn∑ i=1 yt · 1〈wi,xt〉>0, vi·yt≥0, ∀j<i 〈wj ,xt〉≤0∧vj ·yt<0\n= yt\nwhere the last equality comes from our assumption that there exists some neuron i s.t. 〈wi,xt〉 > 0, vi ·yt ≥ 0, and from the definition of y′i,t which asserts that at most a single neuron will predict xt. Thus we have\n∀t ∈ [m] Nn ( W ′,v ) (xt) = yt\n⇒ ES ( W ′,v ) = 0\nTo put this result in different words, if xt is positive on the region combination of wi and if vi has the same sign as yt, then w′i predicts xt correctly, given that xt was not previously predicted by a neuron w ′ j where j < i. We now assert that w′i and wi indeed belong to the same region with respect to the region partition induced by S: Note that wi,w′i belong to the same region induced by {xt} ⇐⇒ sign (〈wi,xt〉) · sign (〈w′i,xt〉) ≥ 0. Thus we compute: If 〈wi,xt〉 > 0, vi · yt ≥ 0, ∀j < i 〈wj ,xt〉 ≤ 0 ∧ vj · yt < 0 all hold, then we have\nsign (〈wi,xt〉) = 1\nand\nsign (〈 w′i,xt 〉) = sign (〈 w′i,xt 〉) = sign\n(〈 m∑ t′=1 ai,t′xt′ ,xt 〉)\n= sign ( m∑ t′=1 ai,t′ 〈xt′ ,xt〉 ) = sign ( y′i,t ) = sign (|yt|) ≥ 0\notherwise, we have sign (〈wi,xt〉) ≤ 0\nand\nsign (〈 w′i,xt 〉) = sign ( y′i,t )\n= 0\nTherefore we have that wi,w′i belong to the same region w.r.t. {xt} for any t and thus both belong to the same region w.r.t. the region partition induced by S. Finally, we define the event Ati := 〈wi,xt〉 > 0, vi ·yt ≥ 0, i.e. the ith neuron is able to predict xt correctly. Since vi,wi are independent we have\nPr [ Ati ] = Pr [〈wi,xt〉 > 0] · Pr [vi · yt ≥ 0]\n≥ 1 4\n⇒ Pr [ Ati ] ≤ 3\n4\nsince (wi,wj) , (vi, vj) are independent for i 6= j, we have that Ati, Atj are mutually independent, so\nPr [ n⋂ i=1 Ati ] ≤ ( 3 4 )n using the union bound on ⋂n i=1A t i for t = 1, . . .m we get\nPr [∃t s.t. no neuron predicts xt] ≤ m ( 3\n4 )n Thus, the probability of initializing from a basin achieving a global minimum with error 0 is at least\n1−m ( 3\n4\n)n"
    }, {
      "heading" : "6.2 Learning Clustered Data",
      "text" : "In the previous section, we showed that when training on m ≤ d data points, local minima are not an issue. Typically, when training neural networks, we expect the converse to be true - the dimension of the data is smaller than the size of the sample. To say something meaningful in that regime, we will consider an extension of the previous result, where instead of having fewer data points than dimensions d, we assume that the data is composed of k ≤ d relatively small clusters. Intuitively, if the clusters are sufficiently small, the surface of the objective function will resemble that of having k ≤ d data points, and therefore local minima might still not pose a problem.\nThe difficulty in analyzing the surface of the objective function when m d arises from the somewhat complicated region partition associated with the data. In the clustered case however, we would expect the\nregion partition to look the same as the region partition induced by the centers of the clusters, up to some noise which is created around each cluster center, which stems from the multiple points in each cluster.\nMore specifically, the hyperplane arrangement will be composed of several large regions which resemble the regions if we just had the cluster centers as the data; and many small “noisy” regions close to their boundary, which reflect the fact that the data points don‘t lie exactly in the cluster center (see Figure 2).\nWhile it is unclear as to how does the surface of the objective function looks like in the new regions introduced by the cluster, henceforth the “noisy regions”, we focus on the surface of the objective function in the regions induced by the centers of the clusters, that vary slightly due to the clusterability of the data.\nWe begin by bounding the radii of the clusters as to guarantee that the probability of initializing from noisy regions is negligible. Recall that we assume the weights of each neuron are initialized uniformly at random from the sphere, consider for arbitrary j the closed ball B̄δj (cj) which contains all the points of the jth cluster. The set of points residing in noisy regions is given by\nAj = { x : ‖x‖ = 1, ∃y ∈ Bδj (cj) s.t. 〈x, y〉 = 0 } i.e. the set of points which are perpendicular to some point in the cluster. Let Acj = Sd\\Aj denote the complement of Aj with respect to the unit sphere, we want to lower bound the probability of initializing from Acj , i.e. we want to lower bound the ratio\nσd−1 ( Acj ) ωd−1\nwhere σd−1 denotes the d− 1 dimensional Lebesgue measure.\nLemma 6. Let S (x, θ) denote the open spherical cap of spherical radius θ and center x. Then\nS ( cj , π\n2 − 2 arcsin δj 2 ‖cj‖\n) ∪̇S ( −cj , π\n2 − 2 arcsin δj 2 ‖cj‖\n) ⊆ Acj\nProof. Clearly the two spherical caps are disjoint as they are of spherical radius≤ π2 and the two originate in two diametrically opposite points. Assume x ∈ S ( cj , π 2 − 2 arcsin δj 2‖cj‖ ) , then the projection of B̄δj (cj)\nonto Sd−1, denoted Pj , is a spherical cap of spherical radius θ := 2 arcsin δj\n2‖cj‖ . Since the dot product is a\nbi-linear operation, it suffices to show that ∀y ∈ Pj 〈x̃,y〉 6= 0, where x̃ ∈ Sd−1 is the projection of x onto Sd−1. Let y ∈ Pj , using the fact that s, the spherical distance function, satisfies the triangle inequality we have\ns (x̃,y) ≤ s (x̃, cj) + s (cj ,y)\n< π\n2 − θ + θ\n= π\n2\n⇒ 〈x̃,y〉 6= 0\nwhere the same argument works for x ∈ S ( −cj , π2 − 2 arcsin δj 2‖cj‖ ) and −Pj .\nLemma 7. ∀θ ≥ 0 we have π 2 −θ∫\n0\nsind−2 ξdξ ≥ ωd−1 2ωd−2 − θ\nProof. Consider the function f (θ) = (∫ π 2 −θ\n0 sin d−2 ξdξ ) − ( ωd−1 2ωd−2 − θ )\n, it is monotonically increasing in [0,∞) since\nf ′ (θ) = ∂\n∂θ\n  π 2 −θ∫\n0\nsind−2 ξdξ − ( ωd−1 2ωd−2 − θ )\n= − sind−2 (π 2 − θ ) + 1 ≥ 0\nand since f (0) = 0 we have ∀θ ∈ [0,∞) that ∫ π 2 −θ\n0 sin d−2 ξdξ ≥ ωd−12ωd−2 − θ.\nEquipped with the above lemmas, we now formalize our result for clustered data:\nTheorem 8. Suppose our data is clustered into k ≤ d clusters. Specifically, that the following assumptions hold:\n• ∃c1, . . . , ck and δ1, . . . , δk s.t. ∀x ∈ S ∃j ∈ [k] s.t. x ∈ B̄δj (cj) where B̄δ (c) denotes the closed ball of radius δ and center c.\n• ∀j ∈ [k] δj ≤ δ for some δ ∈ R+. • ∀j ∈ [k] δj‖cj‖ ≤ 2 sin ( √ 2π 16d √ d ) and ∀j ∈ [k] ‖cj‖ ≥ c for some c > 0.\n• ∀t ∈ [m] ‖xt‖ ≤ B for some B ∈ R.\n• Assume that the corresponding values of the x‘s in each cluster is L-Lipschitz, i.e. if x1,x2 belong to the same cluster then their corresponding target values y (x1) , y (x2) satisfy ‖y (x1)− y (x2)‖ ≤ L ‖x1 − x2‖.\nDenote asC the matrix which rows are c1, . . . , ck, and let κ (C) = σmax(C) σmin(C) denote the condition number of C (or the ratio of the largest and smallest singular value of C). Let c (xt) : Rd → Rd, xt 7→ cj where j is the index of the cluster which xt belongs to, and finally, let ŷ = (ŷ1, . . . , ŷm) ∈ Rk denote the target values of the centers c1, . . . ck, which we define as ŷj = mint:c(xt)=cj |yt|+ 2Lδj . Then\nFn\n( δ2 (( 1 + B\nc\n) n κ (C)\nσmin (C) ‖ŷ‖2 + L\n)2) ≥ 1− d ( 7\n8 )n Note that δ measures how tight the clusters are, whereas κ (C) and σmin (C) can be thought of as constants assuming the cluster centers are in general position. So, the theorem implies that for sufficiently tight clusters, then with overwhelming probability, we will initialize from a basin containing a low-error minimum, as long as the network size is O (log (d)).\nProof. We now compute using 6 and 7, and the fact that ∀d ≥ 2 ωd−1ωd ≤ √ d 2π ([15, Lemma 2.3.20])\nσd−1 ( Acj ) ωd−1 ≥ σd−1 ( S ( cj , π 2 − θ )) + σd−1 ( S ( −cj , π2 − θ )) ωd−1\n= 2νd−1\n( π 2 − θ ) ωd−1\n= 2 ωd−2 ωd−1\nπ 2 −θ∫\n0\nsind−2 ξdξ\nLemma 7 ≥ 2ωd−2 ωd−1 ( ωd−1 2ωd−2 − θ )\n= 1− 2ωd−2θ ωd−1\n≥ 1− 4 √ d\n2π · arcsin δj 2 ‖cj‖\n≥ 1− 4 √ d\n2π · arcsin\n( sin ( √ 2π\n16d √ d )) = 1− 1\n4d\nApplying the union bound to the k ≤ d events where we initialize from Aj , we have that we don’t initialize from a noisy region w.p. at least 34 . For a given t ∈ [m], using the union bound again, the probability of initializing from a region in which any internal point w ∈ Rd satisfies 〈xt,w〉 > 0 is at least 14 , and finally, since vi has the correct sign w.p. 1 2 and is independent of where we initialize wi from, we are able to predict xt w.p. at least 78 . Using the union bound once more in the same manner as we did in the previous section gives that we initialize “properly” w.p. at least\n1− k ( 7\n8\n)n ≥ 1− d ( 7\n8\n)n\nWe now turn to showing that when we initialize properly, then we can bound the error obtained in the basin we initialized from. We first stress that initializations made from noisy regions can be ignored using convexity considerations as in the previous sections (Theorem 5). Note thatNn (w1 . . . ,wn,v) (x) is ‖x‖2-Lipschitz in each wi (see the proof of lemma 5 in appendix 8.3.2), it is straightforward showing that\nNn (w1, . . . ,wn,v) (x) is ∑n\ni=1 ‖wi‖-Lipschitz in x using the same technique. Since κ (C) is defined, in particular C is of full-rank and since we initialized properly, by theorem 7 there exists a set of weight W ′ = (w′1, . . . ,w ′ n) s.t. ES′ (W ′,v) = 0 where S′ = (cj , ŷj) k j=1 is the sample comprised of the clusters’ centers. So for the purpose of training a net on the centers, we have that local minima are not an issue. However, even when initializing from non-noisy regions we have that the region combination of the data is affected when adding multiple observations in each cluster. We now show the existence of a set of weights W = (w1, . . . ,wn) in any non-noisy region that performs on the data S′ well, and we then continue to argue that as a result of this W also performs well on the clustered data S, using Lipschitz considerations. For a randomly initialized ŵi ∈ Rd which doesn’t belong to a noisy region, if w′i is inside the region then take w′i = wi. If w′i falls on a noisy region, we take wi closest to w ′ i in the ‖·‖2 norm sense. Denote the origin as O, and denote the point at which the line connecting O and w′i is tangent to Bδi (ci) by Hi, then the vertices O,wi,w\n′ i and O, ci, Hi form similar triangles, and we have∥∥wi −w′i∥∥ = δici ∥∥w′i∥∥2\n≤ δ c ∥∥w′i∥∥2 Recall that w′i is defined as w ′ i := ∑k j=1 ai,jcj = C >ai where ai is the solution to the equation\nCC>ai = y ′ i\nWe derive a bound on ‖w′i‖2 as follows:\nCC>ai = y ′ i ⇒ ai = ( CC> )−1 y′i\n⇒ C>ai = C> ( CC> )−1 y′i\n⇒ ∥∥w′i∥∥2 = ∥∥∥∥C> (CC>)−1 y′i∥∥∥∥\n2 ≤ ∥∥∥C>∥∥∥\nop ∥∥∥∥(CC>)−1∥∥∥∥ op ∥∥y′i∥∥2 = σmax (C) · 1\nσ2min (C) ∥∥y′i∥∥2 ≤ κ (C)\nσmin (C) ‖ŷ‖2\nwhere ‖·‖op denotes the operator norm. We now demonstrate how W ′ predicts S well:∣∣Nn (W ′,v) (xt)− yt∣∣\n= ∣∣Nn (W ′,v) (xt)−Nn (W ′,v) (c (xt)) +Nn (W ′,v) (c (xt))− yt∣∣\n≤ ∣∣Nn (W ′,v) (xt)−Nn (W ′,v) (c (xt))∣∣+ ∣∣Nn (W ′,v) (c (xt))− yt∣∣\n≤ ∥∥∥∥∥ n∑ i=1 w′i ∥∥∥∥∥ · ‖xt − c (xt)‖+ |ŷt − yt| ≤ δn κ (C)\nσmin (C) ‖ŷ‖2 + 2Lδ\nwhere the last inequality comes from yt, ŷt belonging to a ball of diameter at most 2δ and the target values being L-Lipschitz. In a similar manner, we show how W serves as a surrogate for W ′:∣∣Nn (W,v) (xt)−Nn (W ′,v) (xt)∣∣ = n∑\ni=1\n‖xt‖ · ∥∥wi −w′i∥∥\n≤ n∑ i=1 B · δ c ∥∥w′i∥∥2 ≤\nn∑ i=1 B · δ c κ (C) σmin (C) ‖ŷ‖2\n= nB · δ c κ (C) σmin (C) ‖ŷ‖2\nFinally, combining these two inequalities yields\nES (W, v) = 1\nm m∑ t=1 (Nn (W,v) (xt)− ŷt)2\n= 1\nm m∑ t=1 ∣∣Nn (W,v) (xt)−Nn (W ′,v) (xt) +Nn (W ′,v) (xt)− ŷt∣∣2 ≤ 1\nm m∑ t=1 (∣∣Nn (W,v) (xt)−Nn (W ′,v) (xt)∣∣+ ∣∣Nn (W ′,v) (xt)− ŷt∣∣)2 ≤ 1\nm m∑ t=1 ( nB · δ c κ (C) σmin (C) ‖ŷ‖2 + δn κ (C) σmin (C) ‖ŷ‖2 + 2Lδ )2 = δ2 (( 1 + B\nc\n) n κ (C)\nσmin (C) ‖ŷ‖2 + 2L )2 Thus concluding the proof of the theorem."
    }, {
      "heading" : "7 Conclusions and future work",
      "text" : "In this work, we studied the geometry of the surface of the objective function of artificial neural networks, via investigating the distribution of the minimal value of the basin we initialize from. We demonstrated a\nhardness example for single neuron nets which breaks down under overspecification with two-layer nets. We then moved on to characterize various properties of the objective function, which allowed us to further investigate the effects overspecification has on the geometry of the objective function. In particular, we showed that for ReLU networks of arbitrary depth, then with constant probability there exists a strictly monotonic path from the initial weights to a global optimum, hence “valley crossing” is not strictly necessary. In the more particular context of two-layer ReLU networks, where we randomly choose the weights of the output neuron and optimize over the weights of the first layer, we showed that as the network size increases, we will initialize from a good basin with overwhelming probability, under various combinations of assumptions which include low rank, realizability and clusterability.\nWhile the results provided in this work demonstrate how overspecification can help us in the task of training neural nets, they still rely on assumptions which are non-trivial in practice (such as a strong cluster structure, small dimension or more dimensions than data points). Therefore, it would be interesting to provide similar results under weaker assumptions. Furthermore, although quite similar to the architecture used in practice as explained by 2, it is still of great interest to apply similar results without fixing the weights of the output layer. Finally, although possessing universal approximation capabilities, two-layer nets are seldom used in complicated learning problems, as deeper architectures are believed to be crucial for the compact representation of elaborate functions. For this reason, it is of great interest to provide analogous results for deeper architectures, such as the one provided in theorem 3."
    }, {
      "heading" : "8 Appendix",
      "text" : "In this appendix, we illustrate two specific constructions of theorem 1, one for ReLU and one for a sigmoid, both paired with the squared loss. We also prove two technical lemmas used in the proof of theorem 5."
    }, {
      "heading" : "8.1 Exponentially many local minima for -realizable nets with ReLU activation.",
      "text" : "Define φ (x) = max {0, x} , L ( y, y′ ) = ( y − y′ )2\nGiven > 0, consider the following sample:\nS =\n{( 1\n2 , √ 2\n) , (−1, 1) } Define for i = 1, 2\nLi (w) = (φ (wxi)− yi)2\nAnd denote ES (w) = 1\n2 (L1 (w) + L2 (w))\nNote that ES (−1) =\nES\n( 2 √ 2 ) = 1\n2\nare both local minima, and thus S is -realizable. It is straightforward showing thatES is convex in (−∞, 0) and (0,∞), which is also evident in Figure 3, thus if we initialize uniformly on the unit ball then we have a 50% chance to initialize from the bad basin. Extending the sample into a d-dimensional one as we did in theorem 1, we have an -realizable dataset S with 2d local minima. Furthermore, we have that\nF\n( 1\n8\n) ≤ e− d 16"
    }, {
      "heading" : "8.2 Exponentially many local minima for -realizable nets with sigmoid activation.",
      "text" : "The proof of theorem 1 required that the activation function will satisfy ∀z ∈ (−δ, 0) φ (z) = c for some c ∈ R and δ > 0. While this demand includes the commonly used ReLU activation, there are other popular choices for activations, namely sigmoid or tanh, which do not fall under this category. We now provide an explicit construction for the sigmoid activation paired with the squared loss, which results in 2d local\nminima, where the fraction of sub-optimal minima converges exponentially fast to 1. We stress that one can construct such an example for the tanh activation similarly. Define\nφ (x) = 1\n1 + e−x , L\n( y, y′ ) = ( y − y′ )2 Given > 0, consider the following sample:\nS = {(−x1, y1) , (x2, y2)}\nwhere\nx1 , 4 ln\n( 2 ) y1 , 1\n2\nx2 , ln\n( 1 + 2 √ 2\n1− 2 √ 2 ) y2 , φ (−x2) = 1\n2 − √ 2\nDefine for i = 1, 2 Li (w) = (φ (wxi)− yi)2\nThen the loss over the second sample L2 (w) is given by\nL2 (w) =\n( φ ( w ln ( 1 + 2 √ 2\n1− 2 √ 2\n)) − ( 1\n2 − √ 2\n))2\nLemma 8. For any w ∈ (−1, 0]\nlim →0+\nL2 (w)\n2 (1 + w)2 = 1\nProof. Substituting φ (wx2) with its Taylor expansion at = 0, 12 + w √ 2 +O ( 1.5 ) yields:\nlim →0+\n( 1 2 + w √ 2 +O ( 1.5 ) − ( 1 2 − √ 2 ))2\n2 (1 + w)2\n= lim →0+\n(√ 2 (1 + w) +O ( 1.5 ))2\n2 (1 + w)2\n= lim →0+\n2 (1 + w)2 +O ( 2 )\n2 (1 + w)2\n= lim →0+\n1 + O ( 2 )\n= 1\nNow, by lemma 8, and for the purpose of determining the asymptotic behavior of the sample when → 0+, we may w.l.o.g. use L̃2 (w) , 2 (1 + w)2 as a surrogate for L2 (w), since for small enough > 0 the aforementioned surrogate behaves the same up to some constant multiplicative factor, which in turn becomes additive under the effect of the ln function (see Figure 5).\nWe compute\nx1 = 4 ln\n( 2 ) > 2 ln ( 2\n) as →0+ ∼ 2 ln ( 2\n1− √ 1− 2 ) ⇒ −1\n2 x1 < ln\n( 1− √\n1− 2 2 ) ⇒ e− 1 2 x1 < 1− √\n1− 2 2\n⇒ √ 1− 2 < 1− 2e− 1 2 x1\n≤ 1− e − 1 2 x1\n1 + e− 1 2 x1\n⇒ 1 4 − 2 < 1 4 ·\n( 1− e− 1 2 x1\n1 + e− 1 2 x1 )2 = L1 ( −1\n2 ) and since L1 (w) in monotone decreasing in (−∞, 0] we have ∀w ∈ ( −∞,−12 ] L1 (w) > 1−2 4 . Thus\nL1 (w) =  0, w = 0 > 1−2 4 , w = − 1 2\n< 14 , w = −1 and therefore the total error over the sample ES (w) = 12 ( L1 (w) + L̃2 (w) ) satisfies\nES (w) =  , w = 0 > 18 , w = − 1 2\n< 18 , w = −1\nso there exists some bad local minimum of ES (w) with value > 18 in the interval ( −∞,−12 ] , whereas S is\n-realizable due to ES (0) = (see Figure 6). Extending the sample into a d-dimensional one as we previously did, we have an -realizable dataset S where ES contains 2d local minima. Defining a uniform distribution over all local minima, we have that the number of bad local minima is distributed B (d, 0.5), so from Chernoff‘s bound we have that the portion of minima attaining value at most 132 is bounded by e − 1 16 d. Finally, we point out that even though our construction required the data to be unbounded, its growth rate is merely logarithmic in 1 ."
    }, {
      "heading" : "8.3 Technical results from section 5",
      "text" : ""
    }, {
      "heading" : "8.3.1 Proof of lemma 4.",
      "text" : "Proof. For a point a ∈ Sd−1, let Sd−1 (a, θ) := { b ∈ Sd : 〈a,b〉 ≤ cos θ } be the hyper-spherical cap of angle θ ∈ [0, π]. Note that if a,b ∈ Sd form an angle of θ′ ∈ [0, θ] (i.e. b ∈ Sd−1 (a, θ)) then they form an isosceles triangle with apex angle θ′ and equal sides of length 1, so the distance between a and b satisfies\n‖a− b‖ = 2 sin ( θ′\n2 ) ≤ 2 sin ( θ\n2 ) taking δ := 2 sin ( θ 2 ) we have that θ = 2 arcsin ( δ 2 ) , so in order for us to lower bound Pr [‖a− b‖2 ≤ δ], we need to compute the surface area νd−1 (θ) of the hyper-spherical cap of angle θ at point a, and normalize this quantity by the area of the entire hyper-sphere ωd−1. The surface area of a hyper-spherical cap of radius θ is given by the formula: ([16])\nνd−1 (θ) = ωd−2 θ∫ 0 ( sind−2 ξdξ )\nwhere ωd−1 denotes the surface area of Sd−1. Consider the function f (θ) = ∫ θ 0 ( sind−2 ξdξ ) − 1d−1 sin d−1 θ. It is monotonically increasing in [0, π] since\nf ′ (θ) = ∂\n∂θ  θ∫ 0 ( sind−2 ξdξ ) − 1 d− 1 sind−1 θ  = sind−2 θ − sind−2 θ · cos θ = sind−2 θ · (1− cos θ)\n∀θ∈[0,π]\n≥ 0\nand since f (0) = 0 we have ∀θ ∈ [0, π] that ∫ θ 0 ( sind−2 ξdξ ) ≥ 1d−1 sin\nd−1 θ. We compute\nPr [Ad] = ωd−2 ωd−1 θ∫ 0 ( sind−2 ξdξ ) ≥ ωd−2\nωd−1 · sin\nd−1 θ\nd− 1\n= ωd−2 ωd−1\n· sind−1\n( 2 arcsin ( δ 2 )) d− 1\nusing the formulas sin (arcsinx) = x, cos (arcsinx) = √ 1− x2 and sin 2x = 2 sinx · cosx, we have\nsind−1 ( 2 arcsin ( δ\n2\n)) = ( δ √ 1− δ 2\n4 )d−1 finally, ωd−2ωd−1 can be shown to be monotonically increasing for all d ≥ 2, so ωd−2 ωd−1 ≥ ω0ω1 = 1 π , thus yielding\nPr [Ad] ≥ 1\nπ (d− 1)\n( δ √ 1− δ 2\n4 )d−1 which concludes the proof of the lemma.\nBefore moving on to the next lemma, we stress that for a moderately sized dimension d, and in particular for smaller values of δ, we have that the ( 1− δ24 ) d−1 2 term is approximately 1, thus Pr [Ad] ≈ δ d−1\nπ(d−1) . Specifically, it can be readily seen that a more precise computation yields the following bounds for small d\nd Pr [Ad] ≥ 2 δπ 3 δ 2\n2"
    }, {
      "heading" : "8.3.2 Proof of lemma 5.",
      "text" : "Proof. Fix i ∈ [n] and Compute∣∣∣∣∣ ∂∂wji Nn (W,v) (x) ∣∣∣∣∣ = ∣∣g (〈wi,xt〉)xj · vi∣∣ ≤\n∣∣xj∣∣ thus\n‖∇Nn (W,v) (x)‖ = ∥∥∥∥∥ ( ∂\n∂wj1 Nn (W,v) (x) , . . . ,\n∂\n∂wjn Nn (W,v) (x) )∥∥∥∥∥ ≤\n∥∥(x1, . . . , xn)∥∥ = ‖x‖\nsince Nn is differential almost everywhere and has a bounded sub-gradient we have that it is ‖x‖-Lipschitz in wi."
    } ],
    "references" : [ {
      "title" : "Learning polynomials with neural networks",
      "author" : [ "A. Andoni", "R. Panigrahy", "G. Valiant", "L. Zhang" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1908–1916",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Exponentially many local minima for single neurons",
      "author" : [ "P. Auer", "M. Herbster", "M.K. Warmuth" ],
      "venue" : "NIPS",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Breaking the curse of dimensionality with convex neural networks",
      "author" : [ "F. Bach" ],
      "venue" : "arXiv preprint arXiv:1412.8690",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Y. Bengio", "A. Courville", "P. Vincent" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798–1828",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convex neural networks",
      "author" : [ "Y. Bengio", "N.L. Roux", "P. Vincent", "O. Delalleau", "P. Marcotte" ],
      "venue" : "Advances in neural information processing systems, pages 123–130",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Training a 3-node neural network is np-complete",
      "author" : [ "A.L. Blum", "R.L. Rivest" ],
      "venue" : "Neural Networks, 5(1):117–127",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "The loss surface of multilayer networks",
      "author" : [ "A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun" ],
      "venue" : "arXiv preprint arXiv:1412.0233",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Approximation by superpositions of a sigmoidal function",
      "author" : [ "G. Cybenko" ],
      "venue" : "Mathematics of control, signals and systems, 2(4):303–314",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Improving deep neural networks for lvcsr using rectified linear units and dropout",
      "author" : [ "G.E. Dahl", "T.N. Sainath", "G.E. Hinton" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8609–8613. IEEE",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
      "author" : [ "Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio" ],
      "venue" : "NIPS",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Qualitatively characterizing neural network optimization problems",
      "author" : [ "I.J. Goodfellow", "O. Vinyals" ],
      "venue" : "arXiv preprint arXiv:1412.6544",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods",
      "author" : [ "M. Janzamin", "H. Sedghi", "A. Anandkumar" ],
      "venue" : "CoRR abs/1506.08473",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, pages 1097–1105",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Building high-level features using large scale unsupervised learning",
      "author" : [ "Q.V. Le" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8595–8598. IEEE",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Distributing points on the sphere: partitions",
      "author" : [ "P. Leopardi" ],
      "venue" : "separation, quadrature and energy. PhD thesis, University of New South Wales",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Concise formulas for the area and volume of a hyperspherical cap",
      "author" : [ "S. Li" ],
      "venue" : "Asian Journal of Mathematics and Statistics, 4(1):66–70",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On the computational efficiency of training neural networks",
      "author" : [ "R. Livni", "S. Shalev-Shwartz", "O. Shamir" ],
      "venue" : "NIPS, pages 855–863",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Relations among complexity measures",
      "author" : [ "N. Pippenger", "M.J. Fischer" ],
      "venue" : "Journal of the ACM (JACM), 26(2):361–381",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "l1 regularization in infinite dimensional feature spaces",
      "author" : [ "S. Rosset", "G. Swirszcz", "N. Srebro", "J. Zhu" ],
      "venue" : "Learning theory, pages 544–558. Springer",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Learning kernel-based halfspaces with the 0-1 loss",
      "author" : [ "S. Shalev-Shwartz", "O. Shamir", "K. Sridharan" ],
      "venue" : "SIAM Journal on Computing, 40(6):1623–1646",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "Computer Vision–ECCV 2014, pages 818–833. Springer",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Artificial neural nets are a known to have broad expressive power [17, 18, 20].",
      "startOffset" : 66,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "Artificial neural nets are a known to have broad expressive power [17, 18, 20].",
      "startOffset" : 66,
      "endOffset" : 78
    }, {
      "referenceID" : 19,
      "context" : "Artificial neural nets are a known to have broad expressive power [17, 18, 20].",
      "startOffset" : 66,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].",
      "startOffset" : 166,
      "endOffset" : 184
    }, {
      "referenceID" : 8,
      "context" : "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].",
      "startOffset" : 166,
      "endOffset" : 184
    }, {
      "referenceID" : 12,
      "context" : "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].",
      "startOffset" : 166,
      "endOffset" : 184
    }, {
      "referenceID" : 13,
      "context" : "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].",
      "startOffset" : 166,
      "endOffset" : 184
    }, {
      "referenceID" : 20,
      "context" : "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].",
      "startOffset" : 166,
      "endOffset" : 184
    }, {
      "referenceID" : 5,
      "context" : "By reduction to k-coloring, it has been shown that finding the weights that best fit the training set is NP-hard [6].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "It is also known that even neural networks comprised of a single neuron might have exponentially many local minima [2].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : "It was recently shown that for such networks, under mild assumptions, global optima are ubiquitous, and “most” starting points will lead to the global optima upon optimizing the weights of the last layer [17].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 2,
      "context" : "There is some theoretical and empirical evidence that this is indeed the case [3, 7, 17], but our theoretical understanding of this phenomenon is still quite limited.",
      "startOffset" : 78,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "There is some theoretical and empirical evidence that this is indeed the case [3, 7, 17], but our theoretical understanding of this phenomenon is still quite limited.",
      "startOffset" : 78,
      "endOffset" : 88
    }, {
      "referenceID" : 16,
      "context" : "There is some theoretical and empirical evidence that this is indeed the case [3, 7, 17], but our theoretical understanding of this phenomenon is still quite limited.",
      "startOffset" : 78,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "What we do One way to study the hardness of the training problem has been to consider the number of non-global local minima the objective function has [2].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 1,
      "context" : "More specifically, we provide the following results: • We begin by extending the result of [2] on exponentially many local minimum basins for single neurons, by showing that it holds even if there exists a hypothesis which achieves an arbitrarily small positive training error.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "Although this does not ensure that such a global minimum will be reached, it does mean that “crossing valleys” across the non-convex loss surface is not a must to reach a good solution, and accords with recent empirical evidence to this effect [11].",
      "startOffset" : 244,
      "endOffset" : 248
    }, {
      "referenceID" : 16,
      "context" : "Related work We begin by discussing the work done in [17], where the authors show that for any non-linear activation function and architecture large enough such that the last hidden layer is at least as large as the size of the sample, then global minima are ubiquitous.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "Other recent efforts in the field include [1, 3, 12].",
      "startOffset" : 42,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "Other recent efforts in the field include [1, 3, 12].",
      "startOffset" : 42,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : "Other recent efforts in the field include [1, 3, 12].",
      "startOffset" : 42,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "In [3], the author studies an approach reducing the finite dimensional problem of training a 2-layer neural net paired with ReLUs with a fixed architecture into a problem of finding a sparse measure on an infinite dimensional space (building on work in [5, 19].",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "In [3], the author studies an approach reducing the finite dimensional problem of training a 2-layer neural net paired with ReLUs with a fixed architecture into a problem of finding a sparse measure on an infinite dimensional space (building on work in [5, 19].",
      "startOffset" : 253,
      "endOffset" : 260
    }, {
      "referenceID" : 18,
      "context" : "In [3], the author studies an approach reducing the finite dimensional problem of training a 2-layer neural net paired with ReLUs with a fixed architecture into a problem of finding a sparse measure on an infinite dimensional space (building on work in [5, 19].",
      "startOffset" : 253,
      "endOffset" : 260
    }, {
      "referenceID" : 0,
      "context" : "In [1], the authors show that two-layer neural nets of sufficient size can learn low degree polynomials using gradient descent.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 11,
      "context" : "In [12], the authors present an algorithm with provable guarantees for training neural nets via tensor decompositions.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "Another relevant work is [7], in which the authors investigate the surface area of the loss function of neural nets using ReLUs as activation functions, and make somewhat strong distributional assumptions on the data, such as assuming that the coordinates of the observations in the data are standard normally distributed, and in particular the coordinates are independent of one another, and modeling the connection between neurons as a Bernoulli random variable with constant success probability.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "See also [10] and references therein for related works along similar lines.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 1,
      "context" : "Although simple to analyze and program, nets comprised of a single neuron prove to be very hard to train in the worst case, as the objective function might contain exponentially many poor local minima as a function of the dimension d [2].",
      "startOffset" : 234,
      "endOffset" : 237
    }, {
      "referenceID" : 1,
      "context" : "However, the authors in [2] also demonstrate that for the 0-realizable case, under some mild conditions on the loss and activation functions, there exists a single minimal surface of the objective function.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, since the loss on x1 is constant ∀w ∈ [−1, 0] and the loss on x2 is constant ∀w ∈ [0, 1], we have that the objective function contains two basins meeting at zero, so the sign of w determines which of the basins we fall into.",
      "startOffset" : 95,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "We now extend our sample to be d-dimensional in a similar manner as did the authors in [2] as follows: For i = 1, 2 and j ∈ [d], we use the mapping xi,j 7→ (0, .",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "However, this result nevertheless sheds light on the nature of the surface function, demonstrating that it is not completely sporadic in the sense that “crossing valleys” is not a must to reach a good solution, and accords with recent empirical evidence to this effect [11].",
      "startOffset" : 269,
      "endOffset" : 273
    }, {
      "referenceID" : 0,
      "context" : "Assume that there exists a continuous path γ (t), t ∈ [0, 1] s.",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "γ (0) = winit, γ (1) = wopt and ∀t ∈ [0, 1], there exist an instance x in the training set such that N (γ(t)) (x) 6= 0.",
      "startOffset" : 37,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "We have ‖N (winit)− y‖ > ‖y‖ ⇒ ‖N (winit)‖ − 2 〈N (winit) ,y〉 > 0 (2) Define Nt := N (γ (t)), we once again use the positively-homogeneous nature of the ReLU to scale the weights of the net by a positive factor at > 0, while stressing that the addition of a bias term does not affect this trick and nevertheless allows us to scale the weights by a constant positive factor, effectively forcing ∀t ∈ [0, 1] the equality ‖atNt − y‖ = ‖N0 − y‖ (1− t) + ‖y‖ t",
      "startOffset" : 399,
      "endOffset" : 405
    }, {
      "referenceID" : 7,
      "context" : "Although still relatively simple, they already possess universal approximation properties [8], and encapsulate the challenge of handling a highly non-convex objective function.",
      "startOffset" : 90,
      "endOffset" : 93
    } ],
    "year" : 2017,
    "abstractText" : "Over the past few years, artificial neural networks have seen a dramatic resurgence in popularity as a tool for solving hard learning problems in AI applications. While it is widely known that neural networks are computationally hard to train in the worst case, in practice, neural networks are trained efficiently using SGD methods and a variety of techniques which accelerate the learning process. One mechanism which has been suggested to explain this is overspecification, which is the training of a network larger than what would be needed with unbounded computational power. Empirically, despite worst-case NP-hardness results, large networks tend to achieve a smaller error over the training set. In this work, we aspire to understand this phenomenon. In particular, we wish to better understand the behavior of the error over the sample as a function of the weights of the network, where we focus mostly on neural nets comprised of 2 layers, although we will also consider single neuron nets and nets of arbitrary depth, investigating properties such as the number of local minima the function has, and the probability of initializing from a basin with a given minimal value, with the goal of finding reasonable conditions under which efficient learning of the network is possible.",
    "creator" : "LaTeX with hyperref package"
  }
}