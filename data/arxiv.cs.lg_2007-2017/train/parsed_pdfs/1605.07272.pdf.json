{
  "name" : "1605.07272.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Matrix Completion has No Spurious Local Minimum",
    "authors" : [ "Rong Ge", "Jason D. Lee", "Tengyu Ma" ],
    "emails" : [ "rongge@cs.duke.edu.", "jasondlee88@gmail.com.", "tengyu@cs.princeton.edu." ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Matrix completion is the problem of recovering a low rank matrix from partially observed entries. It has been widely used in collaborative filtering and recommender systems [Kor09, RS05], dimension reduction [CLMW11] and multiclass learning [AFSU07]. There has been extensive work on designing efficient algorithms for matrix completion with guarantees. One earlier line of results (see [Rec11, CT10, CR09] and the references therein) rely on convex relaxations. These algorithms achieve strong statistical guarantees, but are quite computationally expensive in practice.\nMore recently, there has been growing interest in analyzing non-convex algorithms for matrix completion [KMO10, JNS13, Har14, HW14, SL15, ZWL15, CW15]. Let M ∈ d×d be the target matrix with rank r d that we aim to recover, and let Ω = {(i, j) : Mi, j is observed} be the set of observed entries. These methods are instantiations of optimization algorithms applied to the objective1,\nf (X) = 1 2 ∑ (i, j)∈Ω [ Mi, j − (XX>)i, j ]2 , (1.1)\nThese algorithms are much faster than the convex relaxation algorithms, which is crucial for their empirical success in large-scale collaborative filtering applications [Kor09].\nAll of the theoretical analysis for the nonconvex procedures require careful initialization schemes: the initial point should already be close to optimum. In fact, Sun and Luo [SL15] showed that after this initialization the problem is effectively strongly-convex, hence many different optimization procedures can be analyzed by standard techniques from convex optimization.\nHowever, in practice people commonly use a random initialization, which still leads to robust and fast convergence. How can these practical algorithms find the optimal solution in spite of the non-convexity? In this work we investigate this question and show that the matrix completion objective has no spurious local minimum. More precisely, we show that any local minimum X of objective function f (·) is also a global minimum with f (X) = 0, and recovers the correct low rank matrix M. ∗Duke University, rongge@cs.duke.edu. †UC Berkeley, jasondlee88@gmail.com. ‡Princeton University, tengyu@cs.princeton.edu. Supported in part by Simons Award in Theoretical Computer Science and IBM PhD Fellowship. 1In this paper we focus on the symmetric case when true M has symmetric decomposition M = ZZT . Some of previous papers work on the asymmetric case when M = ZWT , which is relatively harder than the symmetric case.\nar X\niv :1\n60 5.\n07 27\n2v 1\n[ cs\n.L G\n] 2\n4 M\nay 2\n01 6\nOur characterization of the structure in the objective function implies that (stochastic) gradient descent from arbitrary starting point converge to a global minimum. This is because gradient descent converges to a local minimum [GHJY15, LSJR16], and for our objective function a local minimum is also a global one."
    }, {
      "heading" : "1.1 Main results",
      "text" : "Assume the target matrix M is symmetric and each entry of M is observed with probability p independently 2. We assume M = ZZ> for some matrix Z ∈ d.\nThere are two known issues with matrix completion. First, the choice of Z is not unique since for any orthonormal matrix R ∈ r×r, we have M = (ZR)(ZR)>. Our goal is to find one of these equivalent solutions.\nAnother issue is that matrix completion is impossible when M is “aligned” with standard basis. For example, when M is the identity matrix in its first r × r block, we will very likely be observing only 0 entries. To address this issue, we make the following standard assumption:\nAssumption 1. For any row Zi of Z, we have\n‖Zi‖ 6 µ/ √ d · ‖Z‖F .\nMoreover, Z has a bounded condition number σmax(Z)/σmin(Z) = κ.\nThroughout this paper we think of µ and κ as small constants, and the sample complexity depends polynomially on these two parameters. Also note that this assumption is independent of the choice of Z: all Z such that ZZT = M have the same row norms and Frobenius norm.\nThis assumption is similar to the “incoherence” assumption [CR09]. Our assumption is the same to the ones used in analyzing non-convex algorithms [KMO10, SL15].\nWe enforce X to also satisfy this assumption by a regularizer\nf (X) = 1 2 ∑ (i, j)∈Ω [ Mi, j − (XX>)i, j ]2 + R(X), (1.2)\nwhere R(X) is a function that penalizes X when one of its rows is too large. See Section 3 for the precise definition. Our main result shows that in this setting, the regularized objective function has no spurious local minimum:\nTheorem 1.1. [Informal] All local minimum of the regularized objective (1.1) satisfy XXT = ZZT = M when p > poly(κ, r, µ, log d)/d.\nCombined with the results in [GHJY15, LSJR16] (see more discussions in Section 1.2), we have,\nTheorem 1.2 (Informal). With high probability, stochastic gradient descent on the regularized objective (1.1) will converge to a solution X such that XXT = ZZT = M in polynomial time from any starting point. Gradient descent will converge to such a point with probability 1 from a random starting point.\nOur results are also robust to noise. Even if each entry is corrupted with Gaussian noise of standard deviation µ2‖Z‖2F/d (comparable to the entry itself!), we can still guarantee that all the local minima satisfy ‖XXT − ZZT ‖F 6 ε when p is large enough, see more discussions in Appendix B.\nOur main technique is to show that every point that satisfies the first and second order necessary conditions for optimality must be a desired solution. To achieve this we use new ideas to analyze the effect of the regularizer and show how it is useful in modifying the first and second order conditions to exclude any spurious local minimum.\n2The entries (i, j) and ( j, i) are the same. With probability p we observe both entries and otherwise we observe neither."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "Matrix Completion. The earlier theoretical works on matrix completion analyzed the nuclear norm heuristic [Rec11, CT10, CR09]. This line of work has the cleanest and strongest theoretical guarantees; [CT10, Rec11] showed that if |Ω| & drµ2 log2 d the nuclear norm convex relaxation recovers the exact underlying low rank matrix. The solution can be computed via the solving a convex program in polynomial time. However the primary disadvantage of nuclear norm methods is their computational and memory requirements. The fastest known algorithms have running time O(d3) and require O(d2) memory, which are both prohibitive for moderate to large values of d. These concerns led to the development of the low-rank factorization paradigm of [BM03]; Burer and Monteiro proposed factorizing the optimization variable M̂ = XXT , and optimizing over X ∈ d×r , instead of M̂ ∈ d×d . This approach only requires O(dr) memory, and a single gradient iteration takes time O(r|Ω|), so has much lower memory requirement and computational complexity than the nuclear norm relaxation. On the other hand, the factorization causes the optimization problem to be non-convex in X, which leads to theoretical difficulties in analyzing algorithms. Under incoherence and sufficient sample size assumptions, [KMO10] showed that well-initialized gradient descent recovers M. Similary, [HW14, Har14, JNS13] showed that well-initialized alternating least squares or block coordinate descent converges to M, and [CW15] showed that well-initialized gradient descent converges to M. [SL15, ZWL15] provided a more unified analysis by showing that with careful initialization many algorithms, including gradient descent and alternating least squres, succeed. [SL15] accomplished this by showing an analog of strong convexity in the neighborhood of the solution M.\nNon-convex Optimization. Recently, a line of work analyzes non-convex optimization by separating the problem into two aspects: the geometric aspect which shows the function has no spurious local minimum and the algorithmic aspect which designs efficient algorithms can converge to local minimum that satisfy first and (relaxed versions) of second order necessary conditions.\nOur result is the first that explains the geometry of the matrix completion objective. Similar geometric results are only known for a few problems: phase retrieval/synchronization, tensor decomposition, dictionary learning [GHJY15, SQW15, BBV16]. The matrix completion objective requires different tools due to the sampling of the observed entries, as well as carefully managing the regularizer to restrict the geometry. Parallel to our work Bhojanapalli et al.[BNS16] showed similar results for matrix sensing, which is closely related to matrix completion.\nOn the algorithmic side, it is known that second order algorithms like cubic regularization [NP06] and trust-region [SQW15] algorithms converge to local minima that approximately satisfy first and second order conditions. Gradient descent is also known to converge to local minima [LSJR16] from a random starting point. Stochastic gradient descent can converge to a local minimum in polynomial time from any starting point [Pem90, GHJY15]. All of these results can be applied to our setting, implying various heuristics people use in practice are guaranteed to solve matrix completion."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Notations",
      "text" : "For Ω ⊂ [d] × [d], let PΩ be the operator that maps a matrix A to PΩ(A), where PΩ(A) has the same values as A on Ω, and 0 outside of Ω.\nWe will use the following matrix norms: ‖ · ‖F the frobenius norm, ‖ · ‖ spectral norm, |A|∞ elementwise infinity norm, and |A|p→q = max‖x‖p=1 ‖A‖q. We use the shorthand ‖A‖Ω = ‖PΩA‖F . The trace inner product of two matrices is 〈A, B〉 = tr(A>B), and σmin(X), σmax(X) are the smallest and largest singular values of X. We also use Xi to denote the i-th row of a matrix X."
    }, {
      "heading" : "2.2 Necessary conditions for Optimality",
      "text" : "Given an objective function f (x) : n → , we use ∇ f (x) to denote the gradient of the function, and ∇2 f (x) to denote the Hessian of the function (∇2 f (x) is an n×n matrix where [∇2 f (x)]i, j = ∂ 2\n∂xi∂x j f (x)). It is well known that local minima\nof the function f (x) must satisfy some necessary conditions:\nDefinition 2.1. A point x satisfies the first order necessary condition for optimality (later abbreviated as first order optimality condition) if ∇ f (x) = 0. A point x satisfies the second order necessary condition for optimality (later abbreviated as second order optimality condition)if ∇2 f (x) 0.\nThese conditions are necessary for a local minimum because otherwise it is easy to find a direction where the function value decreases. We will also consider a relaxed second order necessary condition, where we only require the smallest eigenvalue of the Hessian ∇2 f (x) to be not very negative:\nDefinition 2.2. For τ > 0, a point x satisfies the τ-relaxed second order optimality condition, if ∇2 f (x) −τ · I.\nThis relaxation to the second order condition makes the conditions more robust, and allows for efficient algorithms.\nTheorem 2.3. [NP06, SQW15, GHJY15] If every point x that satisfies first order and τ-relaxed second order necessary condition is a global minimum, then many optimization algorithms (cubic regularization, trust-region, stochastic gradient descent) can find the global minimum up to ε error in function value in time poly(1/ε, 1/τ, d)."
    }, {
      "heading" : "3 Warm-up: Rank-1 matrix completion",
      "text" : "In this section we analyze the geometry of the objective function for rank r = 1 case with a simple and clean proof. This case illustrates our main ideas. The rank r analysis follows from the same approach and is shown in the next section.\nIn this case, assume M = zz>, where ‖z‖ = 1, and ‖z‖∞ 6 µ√d . The objective function simplifies to,\nf (x) = 1 2 ‖PΩ(M − xx>)‖2F + λR(x) . (3.1)\nHere we use the the regularization R(x)\nR(x) = d∑\ni=1\nh(xi), and h(t) = (|t| − α)4 t>α .\nThe parameters λ and α will be chosen later as in Theorem 3.2. We will choose α > 10µ/ √\nd so that R(x) = 0 for incoherent x, and thus it only penalizes coherent x. Moreover, we note that R(x) has Lipschitz second order derivative.\nWe first state the optimality conditions, whose proof is deferred to Appendix A.\nProposition 3.1. The first order optimality condition of objective (3.1) is,\n2PΩ(M − xx>)x = λ∇R(x) , (3.2)\nand the second order optimality condition requires:\n∀v ∈ d, ‖PΩ(vx> + xv>)‖2F + λv>∇2R(x)v > 2v>PΩ(M − xx>)v . (3.3)\nMoreover, The τ-relaxed second order optimality condition requires\n∀v ∈ d, ‖PΩ(vx> + xv>)‖2F + λv>∇2R(x)v > 2v>PΩ(M − xx>)v − τ‖v‖2 . (3.4)\nWe give the precise version of Theorem 1.1 for the rank-1 case.\nTheorem 3.2. For p > cµ 6 log1.5 d d where c is a large enough absolute constant, set α = 10µ √\n1/d and λ > µ2 p/α2.Then, with high probability over the randomness of Ω, the only points in d that satisfy first and (or τ-relaxed with τ < −0.1p) second order optimality conditions are z and −z.\nIf we observe every entry of M, Theorem 3.2 follows from simple linear algebra: the first order optimality condition would require Mx = ‖x‖2x, and therefore x has to be an eigenvector of M. The second order optimality condition will make sure x is the eigenvector with largest eigenvalue. When we only observe a small subset of entries, we will use concentration bounds to prove more robust versions of these arguments. In the rest of this section, we will first prove that when x is constrained to be incoherent (and hence the regularizer is 0 and concentration is straightforward) and satisfies the optimality conditions, then x has to be z or −z. Then we go on to explain how the regularizer helps us to change the geometry of those points that are far away from z so that we can rule out them from being local minimum.\nFor simplicity, we will focus on the part that shows a local minimum x must be close enough to z.\nLemma 3.3. In the setting of Theorem 3.2, suppose x satisfies the first-order and second-order optimality condition (3.2) and (3.3). Then when p is defined as in Theorem 3.2,∥∥∥xx> − zz>∥∥∥2F 6 O(ε) . where ε = µ3(pd)−1/2.\nThis turns out to be the main challenge. Once we proved x is close, we can apply the result of Sun and Luo [SL15] (see Lemma C.1), and obtain Theorem 3.2."
    }, {
      "heading" : "3.1 Handling incoherent x",
      "text" : "To demonstrate the key idea, in this section we restrict our attention to the subset of d which contains incoherent x with `2 norm bounded by 1, that is, we consider,\nB = { x : ‖x‖∞ 6 µ √\nd , ‖x‖ 6 1\n} . (3.5)\nNote that the desired solution z is in B, and the regularization R(x) vanishes inside B. The general rational is to assume x satisfies the first and second order optimality condition, and then use these\nconditions to deduce a sequence of properties that x must satisfy\nLemma 3.4. Under the setting of Theorem 3.2 , with high probability over the choice of Ω, for any x ∈ B that satisfies second-order optimality condition (3.3) we have,\n‖x‖2 > 1/4.\nThe same is true if x ∈ B only satisfies τ-relaxed second order optimality condition for τ 6 0.1p.\nProof. We plug in v = z in the second-order optimality condition (3.3), and obtain that∥∥∥PΩ(zx> + xz>)∥∥∥2F > 2z>PΩ(M − xx>)z . (3.6) Intuitively, when restricted to Ω, the squared Frobenius on the LHS and the quadratic form on the RHS should both be approximately a p fraction of the unrestricted case. In fact, both LHS and RHS can be written as the sum of terms of the form 〈PΩ(uvT ), PΩ(stT )〉, because∥∥∥PΩ(zx> + xz>)∥∥∥2F = 2〈PΩ(zxT ), PΩ(zxT )〉 + 2〈PΩ(zxT ), PΩ(xzT )〉\n2z>PΩ(M − xx>)z = 2〈PΩ(zzT ), PΩ(zzT )〉 − 2〈PΩ(xxT ), PΩ(zzT )〉.\nTherefore we can use concentration inequalities (Theorem D.1), and simplify the equation\nLHS of (3.6) = p ∥∥∥zx> + xz>∥∥∥2F ± O( √pd‖x‖2∞‖z‖2∞‖x‖2‖z‖2)\n= 2p‖x‖2‖z‖2 + 2p〈x, z〉2 ± O(pε) , (Since x, z ∈ B)\nwhere ε = O(µ2 √\nlog d pd ). Similarly, by Theorem D.1 again, we have\nRHS of (3.6) = 2 ( 〈PΩ(zz>), PΩ(zz>)〉 − 〈PΩ(xx>), PΩ(zz>)〉 ) (Since M = zz>)\n= 2p‖z‖4 − 2p〈x, z〉2 ± O(pε) (by Theorem D.1 and x, z ∈ B)\n(Note that even we use the τ-relaxed second order optimality condition, the RHS only becomes 1.99p‖z‖4 − 2p〈x, z〉2 ± O(pε) which does not effect the later proofs.)\nTherefore plugging in estimates above back into equation (3.6), we have that\n2p‖x‖2‖z‖2 + 2p〈x, z〉2 ± O(pε) > 2‖z‖4 − 2〈x, z〉2 ± O(pε) ,\nwhich implies that 6p‖x‖2‖z‖2 > 2p‖x‖2‖z‖2 + 4p〈x, z〉2 > 2p‖z‖4 − O(pε). Using ‖z‖2 = 1, and ε being sufficiently small, we complete the proof.\nNext we use first order optimality condition to pin down another property of x – it has to be close to z after scaling. Note that this doesn’t mean directly that x has to be close to z since x = 0 also satisfies first order optimality condition (and therefore the conclusion (3.7) below).\nLemma 3.5. With high probability over the randomness of Ω, for any x ∈ B that satisfies first-order optimality condition (3.2), we have that x also satisfies ∥∥∥〈z, x〉z − ‖x‖2x∥∥∥ 6 O(ε) . (3.7) where ε = Õ(µ3(pd)−1/2).\nProof. Note that since x ∈ B, we have R(x) = 0. Therefore first-order optimality condition says that\nPΩ(M − xx>)x = PΩ(zz>)x − PΩ(xx>)x = 0 . (3.8)\nAgain, intuitively we hope PΩ(zzT ) ≈ pzzT and PΩ(xxT )x ≈ p‖x‖2x. These are made precise by the concentration inequalities Lemma D.4 and Theorem D.2 respectively.\nBy Theorem D.2, we have that with high probability over the choice of Ω, for every x ∈ B,\n‖PΩ(xx>)x − pxx>x‖F 6 pε‖x‖3 6 pε (3.9)\nwhere ε = Õ(µ3(pd)−1/2). Similarly, by Lemma D.4, we have that for with high probability over the choice of Ω,∥∥∥PΩ(zz>) − pzz>∥∥∥ 6 εp . for ε = Õ(µ2(pd)−1/2). Therefore for every x,∥∥∥PΩ(zz>)x − pzz>x∥∥∥ 6 εp‖x‖ 6 εp . (3.10) Plugging in estimates (3.10) and (3.9) into equation (3.8), we complete the proof.\nFinally we combine the two optimality conditions and show (3.7) implies xxT must be close to zzT . Lemma 3.6. Suppose vector x satisfies that ‖x‖2 > 1/4, and that ∥∥∥〈z, x〉z − ‖x‖2x∥∥∥ 6 δ . Then for δ ∈ (0, 0.1),∥∥∥xx> − zz>∥∥∥2F 6 O(δ) .\nProof. We write z = ux + v where u ∈ and v is a vector orthogonal to x. Now we know 〈z, x〉z = u2‖x‖2x + u‖x‖2v, therefore δ > ∥∥∥〈z, x〉z − ‖x‖2x∥∥∥ = ‖x‖2 √u2‖v‖2 + (1 − u2)2.\nIn particular, we know |1−u2| 6 4δ and u‖v‖ 6 4δ. This means |u| ∈ 1± 3δ and ‖v‖ 6 8δ. Now we expand xxT − zzT :\nxxT − zzT = (1 − u2)xxT + uxvT + uvxT + vvT\nIt is clear that all the terms have norm bounded by O(δ), therefore ∥∥∥xx> − zz>∥∥∥2F 6 O(δ)."
    }, {
      "heading" : "3.2 Extension to general x",
      "text" : "We have shown when x is incoherent and satisfies first and second order optimality conditions, then it must be close to z or −z. Now we need to consider more general cases when x may have some very large coordinates. Here the main intuition is that the first order optimality condition with a proper regularizer is enough to guarantee that x cannot have a entry that is too much bigger than µ/ √ d.\nLemma 3.7. With high probability over the choice of Ω, for any x that satisfies first-order order optimality condition (3.2), we have ‖x‖∞ 6 4 max { α, µ √ p/λ } . (3.11)\nHere we recall that α was chosen to be 10µ/ √ d and λ is chosen to be large so that the α dominates the second term µ √ p/λ in the setting of Theorem 3.2.\nProof of Lemma 3.7. Suppose i? = max j |x j|. Without loss of generality, suppose xi? > 0. Suppose i?-th row of Ω consists of entries with index [i]×S i? . If |xi? | 6 2α, we are done. Therefore in the rest of the proof we assume |xi? | > 2α. Note that when p > c(log d)/d for sufficiently large constant c, with high probability over the choice of Ω, we have |S i? | 6 2pd. In the rest of argument we are working with such an Ω with |S i? | 6 2pd.\nWe will compare the i?-th coordinate of LHS and RHS of first-order optimality condition (3.2). For preparation, we have\n∣∣∣(PΩ(M)x)i? ∣∣∣ = ∣∣∣∣(PΩ(zz>)x)i? ∣∣∣∣ = ∣∣∣∣∣∣∣∣ ∑ j∈S i? zi?z jx j ∣∣∣∣∣∣∣∣ 6 |xi? |\n∑ j∈S i? |zi?z j| 6 |xi? | · µ2/d · |S i? | 6 2|xi? |pµ2 (3.12)\nwhere the last step we used the fact that |S i? | 6 2pd. Moreover, we have that (PΩ(xx>)x)i? = ∑ j∈S i? xi? x2j > 0 ,\nand that\n(λ∇R(x))i? = 4λ(|xi? | − α)3 sign(xi? ) > λ\n2 |xi? |3 (Since xi? > 2α)\nNow plugging in the bounds above into the i?-th coordinate of equation (3.2), we obtain\n4|xi? |pµ2 > 2(PΩ(M − xx>)x)i? > (λ∇R(x))i? > λ\n2 |xi? |3 ,\nwhich implies that |xi? | 6 4 √ pµ2/λ.\nSetting λ > µ2 p/α2 and α = 10µ √\n1/d, Lemma 3.7 ensures that any x that satisfies first-order optimality condition is the following ball, B′ = { x ∈ d : ‖x‖∞ 6 4α } .\nThen we would like to continue to use arguments similar to Lemma 3.4 and 3.5. However, things have become more complicated as now we need to consider the contribution of the regularizer.\nLemma 3.8 (Extension of Lemma 3.4). In the setting of Theorem 3.2, with high probability over the choice of Ω, suppose x ∈ B′ satisfies second-order optimality condition (3.3) or τ-relaxed condition for τ 6 0.1p, we have ‖x‖2 > 1/8.\nThe guarantees and proofs are very similar to Lemma 3.4. The main intuition is that we can restrict our attentions to coordinates whose regularizer is equal to 0. See Section A for details.\nWe will now deal with first order optimality condition. We first write out the basic extension of Lemma 3.5, which follows from the same proof except we now include the regularizer term.\nLemma 3.9 (Basic extension of Lemma 3.5). With high probability over the randomness of Ω, for any x ∈ B′ that satisfies first-order optimality condition (3.2), we have that x also satisfies∥∥∥〈z, x〉z − ‖x‖2x − γ · ∇R(x)∥∥∥ 6 O(ε) . (3.13) where ε = Õ(µ6(pd)−1/2) and γ = λ/(2p) > 0.\nNext we will show that we can remove the regularizer term, the main observation here is nonzero entries ∇R(x) all have the same sign as the corresponding entries in x. See Section A for details.\nLemma 3.10. Suppose x ∈ B′ satisfies that ‖x‖2 > 1/8, under the same assumption as Lemma 3.9. we have,∥∥∥〈x, z〉z − ‖x‖2x∥∥∥ 6 O(ε) (3.14) Finally we combine Lemma 3.7, Lemma 3.8 and Lemma 3.10 and prove Lemma 3.6. The argument are also summarized in Figure 1, where we partition d into regions where our lemmas apply."
    }, {
      "heading" : "4 Rank-r case",
      "text" : "In this section we show how to extend the results in Section 3 to recover matrices of rank r.\nRecall that in this case we assume the original matrix M = ZZT , where Z ∈ d×r. We also assume Assumption 1. The objective function is very similar to the rank 1 case\nf (X) = 1 2 ∥∥∥PΩ(M − XX>)∥∥∥2F + λR(X) , (4.1) where R(X) = ∑d i=1 r(‖Xi‖) . and recall that r(t) = (|t| − α)4 t>α. Here α and λ are again parameters that we will determined later. Without loss of generality, we assume that ‖Z‖2F = r in this section. This implies that σmax(Z) > 1 > σmin(Z). Now we shall state the first and second order optimality conditions:\nProposition 4.1. If X is a local optimum of objective function (4.1), its first order optimality condition is,\n2PΩ(M)X = 2PΩ(XX>)X + λ∇R(X) , (4.2)\nand the second order optimality condition is equivalent to\n∀V ∈ d×r, ‖PΩ(VX> + XV>)‖2F + λ〈V>,∇2R(X)V〉 > 2〈PΩ(M − XX>),VV>〉 . (4.3)\nNote that the regularizer now is more complicated than the one dimensional case, but luckily we still have the following nice property.\nProposition 4.2. We have that ∇R(X) = ΓX where Γ ∈ d×d is a diagonal matrix with Γii = 4(‖Xi‖−α) 4\n‖Xi‖ ‖Xi‖>α. As a direct consequence, 〈(∇R(X))i, Xi〉 > 0 for every i ∈ [d].\nNow we are ready to state the precise version of Theorem 1.1:\nTheorem 4.3. Suppose p > C max{µ6κ16r4, µ4κ4r6}d−1 log1.5 d where C is a large enough constant. Let α = 4µκr/ √ d, λ > µ2rp/α2. Then with high probability over the randomness of Ω, any local minimum X of f (·) satisfies that f (X) = 0, and in particular, ZZ> = XX>.\nThe proof of this Theorem follows from a similar path as Theorem 3.2. We first notice that because of the regularizer, any matrix X that satisfies first order optimality condition must be somewhat incoherent (this is analogues to Lemma 3.7):\nLemma 4.4. Suppose |S i| 6 2pd. Then for any X satisfies 1st order optimality (4.2), we have\n‖X‖2→∞ = max i ‖Xi‖ 6 4 max\n{ α, µ √ rp/λ } (4.4)\nProof. Assume i? = argmaxi ‖Xi‖. Suppose the ith row of Ω consists of entries with index [i] × S i. If ‖Xi?‖ 6 2α, then we are done. Therefore in the rest of the proof we assume ‖Xi?‖ > 2α.\nWe will compare the i-th row of LHS and RHS of (4.2). For preparation, we have (PΩ(M)x)i? = ( PΩ(ZZ>)X ) i? = ( PΩ(ZZ>) ) i? X (4.5)\nThen we have that ∥∥∥∥(PΩ(ZZ>))i?∥∥∥∥1 = ∑ j∈S i? |〈Zi? ,Z j〉|\n6 ∑ j∈S i? ‖Zi?‖‖Z j‖ 6 ∑ j∈S i? µ2r/d|S 1| (by incoherence of Z) 6 2µ2rp . (by |S i? | 6 2pd)\nTherefore we can bound the `2 norm of LHS of 1st order optimality condition (4.2) by∥∥∥∥(PΩ(ZZ>)X)i?∥∥∥∥ 6 ∥∥∥∥(PΩ(ZZ>))i?∥∥∥∥1 ∥∥∥X>∥∥∥1→2 6 2µ2rp ‖X‖2→∞ (by ‖X‖2→∞ =\n∥∥∥X>∥∥∥1→2) = 2µ2rp ‖Xi?‖ (4.6)\nNext we lowerbound the norm of the RHS of equation (4.2). We have that (PΩ(XX>)X)i? = ∑ j∈S i? 〈Xi? , X j〉X j = Xi ∑ j∈Xi? X>j X j ,\nwhich implies that\n〈(PΩ(XX>)X)i? , Xi?〉 = Xi? ∑\nj∈Xi? X>j X j\n X>i? > 0 . (4.7)\nUsing Proposition 4.2 we obtain that 〈(PΩ(XX>)X)i? , (∇R(X))i?〉 = ΓiiXi? ∑\nj∈Xi? X>j X j  X>i? > 0 . (4.8) It follows that ∥∥∥(PΩ(XX>)X)i? + (λ∇R(X))i?∥∥∥ > ‖(λ∇R(X))i?‖ (by equation (4.8))\n= 4λ(‖Xi?‖ − α)3 ‖Xi?‖ · ‖Xi?‖ (by Proposition 4.2) > λ\n2 ‖Xi?‖3 (by the assumptino ‖Xi?‖ > 2α)\nTherefore plugging in equation above and equation (4.6) into 1st order optimality condition (4.2). We obtain that ‖Xi?‖ 6 √ 8µ2rp/λ which completes the proof.\nNext, we prove a property implied by first order optimality condition, which is similar to Lemma 3.9.\nLemma 4.5. In the setting of Theorem 4.3, with high probability over the choice of Ω, for any X that satisfies 1st order optimality condition (4.2), we have\n‖X‖2F 6 2rσmax(Z)2 . (4.9)\nMoreover, we have σmax(X) 6 2σmax(Z)r1/6 . (4.10) and ∥∥∥ZZT X − XXT X − γ∇R(X)∥∥∥F 6 O(δ) (4.11) where δ = O(µ3κ3r2 log0.75(d)σmax(Z)−3(dp)−1/2) and γ = λ/(2p) > 0.\nProof. If ‖X‖F 6 √ rσmax(Z)2 we are done. When ‖X‖F > √\nrσmax(Z)2, by Lemma 4.4, we have that max ‖Xi‖ 6 4α = 4µκr/ √ d, and therefore max ‖Xi‖ 6 ν‖X‖F with ν = O(µκ\n√ r/σmax(Z)). Then by Theorem D.2, we have that∥∥∥PΩ(ZZ>)X − pZZ>X∥∥∥F 6 pδ , (4.12)\nand ∥∥∥PΩ(XX>)X − XX>X∥∥∥F 6 pδ , (4.13) where δ = O(µ3κ3r2 log0.75(d)σmax(Z)−3(dp)−1/2). These two imply equation (4.11). Moreover, we have\np ∥∥∥ZZ>X∥∥∥F = ∥∥∥PΩ(ZZ>)X∥∥∥F ± pδ = ∥∥∥PΩ(XX>)X + λR(X)∥∥∥F ± pδ (by equation (4.2))\n> ∥∥∥PΩ(XX>)X∥∥∥F ± pδ (by equation (4.8)) > p\n∥∥∥XX>X∥∥∥F ± 2pδ (4.14) Suppose X has singular value σ1 > . . . > σr. Then we have\n∥∥∥ZZ>X∥∥∥2F 6 ‖ZZ>‖2‖X‖2F 6 σmax(Z)4‖X‖2F = σmax(Z)4(σ21 + · · · + σ2r ). On the other hand,\n∥∥∥XX>X∥∥∥2F = σ61 + · · · + σ6r . Therefore, equation (4.14) implies that (1 + O(δ))σmax(Z)4\nr∑ i=1 σ2i > r∑ i=1 σ6i (4.15)\nThen we have (by Proposition E.1) we complete the proof.\nNow we look at the second order optimality condition, this condition implies the smallest singular value of X is large (similar to Lemma 3.8). Note that this lemma is also true even if x only satisfies relaxed second order optimality condition with τ = 0.01pσmin(Z).\nLemma 4.6. In the setting of Theorem 4.3. With high probability over the choice of Ω, suppose X satisfies equation (4.9), (4.4) the 2nd order optimality condition (4.3). Then,\nσmin(X) > 1 4 σmin(Z) (4.16)\nProof. Let J = {i : ‖Xi‖ 6 α}. Let v ∈ r such that ‖Xv‖ = σmin(X). . Let ZJ be the matrix that has the same i-th row as Z for every i ∈ J and 0 elsewhere. Since ZJ has column rank at most r, by variational characterization of singular values, we have that for there exists unit vector zJ ∈ col-span(ZJ) such that ‖X>zJ‖ 6 σmin(X).\nWe claim that σmin(ZJ) > 12σmin(Z). Let L = [d] − J. Since for any i ∈ L it holds that ‖Xi‖ > α, we have |L|α2 6 ‖X‖2F 6 2rσmax(Z)2 (by equation (4.9)), and it follows that |L| 6 2rσmax(Z)2/α2. Therefore,\nσmin(ZJ) > σmin(Z) − σmax(ZL) > σmin(Z) − ‖ZL‖F > σmin(Z) − √ |L|rµ2/d > σmin(Z) − √ 2r2σmax(Z)2µ2/(α2d)\n> 1 2 σmin(Z) . (by α > rκµ√ d )\nSince zJ ∈ col-span(ZJ) is a unit vector, we have that zJ can be written as zJ = ZJβ where ‖β‖ 6 1σmin(ZJ ) 6 O(1/σmin(Z)). Therefore this in turn implies that ‖zJ‖∞ 6 ‖ZJ‖2→∞‖β‖ 6 O(µ √ r/d/σmin(Z)) 6 O(µκ √ r/d).\nWe will plug in V = zJvT in the 2nd order optimality condition (4.3). Note that since zJ ∈ col-span(ZJ), it is supported on subset J, and therefore ∇2R(X)V = 0. Therefore the term about regularization in (4.3) will vanish. For simplicity, let y = X>zJ , w = Xv We obtain that taking V = zJv> in equation (4.3) will result in∥∥∥PΩ(wz>J + zJw>)∥∥∥2F > 2〈PΩ(ZZ> − XX>), zJz>J 〉 (4.17) Note that we have that ‖w‖∞ 6 ‖X‖2→∞‖v‖ 6 µ √ r/d. Recalling that ‖zJ‖∞ 6 O(µκ √ r/d), by Theorem D.1, we have\nthat p ∥∥∥wz>J + zJw>∥∥∥2F > 2p〈ZZ> − XX>, zJz>J 〉 − δp (4.18)\nwhere δ = O(µ2κr2(pd)−1/2). Then simple algebraic manipulation gives that\n〈w, zJ〉2 + ‖w‖2‖zJ‖2 + ‖X>zJ‖2 > ‖Z>zJ‖2 − δ/2 (4.19)\nNote that 〈w, zJ〉 = 〈v, X>zJ〉 = 〈y, v〉. Recall that ‖zJ‖ = 1 and z ∈ col-span(ZJ), and therefore ‖Z>zJ‖ = ‖Z>J zJ‖ > σ2min(ZJ). Moreover, recall that ‖y‖ = ‖X>zJ‖ 6 σmin(X). Using these with equation (4.19) we obtain that\n〈w, zJ〉2 + ‖w‖2‖zJ‖2 + ‖X>zJ‖2 6 〈y, v〉2 + ‖w‖2 + ‖y‖2\n6 2‖y‖2 + σ2min(X) (by Cauchy-Schwarz and ‖w‖ = σmin(X).) 6 3σ2min(X) (by ‖y‖ 6 σmin(X).)\nTherefore together with equation (4.19) and ‖Z>zJ‖ > σ2min(ZJ) we obtain that\nσmin(X) > (1/2 −Ω(δ))σmin(ZJ) (4.20)\nTherefore combining equation (4.20) and the lower bound on σmin(ZJ) we complete the proof.\nSimilar as before, we show it is possible to remove the regularizer term here, again the intuition is that the regularizer is always in the same direction as X.\nLemma 4.7. Suppose X satisfies equation (4.4) and (4.16) and (4.10), then for any γ > 0,∥∥∥ZZT X − XXT X∥∥∥2F 6 ∥∥∥ZZT X − XXT X − γ∇R(X)∥∥∥2F (4.21) Proof. Let L = {i : ‖Xi‖2 > α}. For i < L, we have that (∇R(X))i = 0. Therefore it suffices to prove that for every i ∈ L,∥∥∥ZiZ>X − XiX>X∥∥∥2 6 ∥∥∥ZiZ>X − XiX>X − (γ∇R(X))i∥∥∥2 It suffices to prov that 〈(∇R(X))i, XiX>X − ZiZ>Z〉 > 0 (4.22) By proposition 4.2, we have ∇R(X))i = ΓXi for Γ > 0. Then\n〈(∇R(X))i, XiX>X〉 = Γ〈Xi, XiX>X〉 > Γ ‖Xi‖2 σmin(XT X)\n> 1 4 Γ ‖Xi‖2 σmin(Z) (by equation 4.16)\nOn the other hand, we have\n〈(∇R(X))i,ZiZ>Z〉 = Γ〈Xi,ZiZ>X〉 6 Γ‖Xi‖‖Zi‖σmax(ZT X) 6 Γ‖Xi‖‖Zi‖σmax(Z)σmax(X) 6 Γ‖Xi‖‖Zi‖σmax(Z)2r1/6 (by equation (4.10))\n6 1\n10 Γ‖Xi‖2σmax(Z)2r−1/3 (by ‖Xi‖ > α > 10\n√ r‖Zi‖)\nTherefore combining two equations above we obtain equation (4.22) which completes the proof.\nFinally we show the form in Equation (4.21) implies ZZT is close to XXT (this is similar to Lemma 3.6).\nLemma 4.8. Suppose X and Z satisfies that σmin(X) > 1/4 · σmin(Z) and that∥∥∥ZZT X − XXT X∥∥∥2F 6 δ2 (4.23) where δ 6 σ3min(Z)/C for a large enough constant C, then\n‖XX> − ZZ>‖2F 6 O(δκ2/σmin(Z)). (4.24)\nProof. The proof is similar to the one-dimensional case, we will separate Z into the directions that are in column span of X and its orthogonal subspace. We will then show the projection of Z in the column span is close to X, and the projection on the orthogonal subspace must be small.\nLet Z = U + V where U = Projspan(X)Z is the projection of Z to the column span of X, and V is the projection to the orthogonal subspace. Then since VT X = 0 we know\nZZT X = (U + V)(U + V)T X = UUT X + VUT X.\nHere columns of the first term UUT X are in the column span of X, and the columns second term VUT X are in the orthogonal subspace. Therefore,\n‖ZZT X − XXT X‖2F = ‖UUT X − XXT X‖2F + ‖VUT X‖2F 6 δ2.\nIn particular, both terms should be bounded by δ2. Therefore ‖UUT − XXT ‖2F 6 δ2/σ2min(X) 6 16δ2/σ2min(Z). Also, we know σmin(UUT X) > σmin(XXT X) − δ > σmin(Z)3/128 if δ 6 σmin(Z)3/128. Therefore σmin(UT X) is at least σmin(Z)3/‖Z‖128. Now ‖V‖2F 6 δ2/σmin(UT X)2 6 O(δ2‖Z‖2/σmin(Z)6). Finally, we can bound ‖UVT ‖F by ‖U‖‖V‖F 6 ‖Z‖‖V‖F (last inequality is because U is a projection of Z), which at least Ω(‖V‖2F) when δ 6 σmin(Z)3/128, therefore\n‖ZZT − XXT ‖F 6 ‖UUT − XXT ‖F + 2‖UVT ‖F + ‖VVT ‖F 6 O(δ‖Z‖2/σmin(Z)3).\nLast thing we need to prove the main theorem is a result from Sun and Luo[SL15], which shows whenever XXT is close to ZZT , the function is essentially strongly convex, and the only points that have 0 gradient are points where XXT = ZZT , this is explained in Lemma C.1. Now we are ready to prove Theorem 4.3:\nProof of Theorem 4.3. Suppose X satisfies 1st and 2nd order optimality condition. Then by Lemma 4.5 and Lemma 4.4, we have that X satisfies equation (4.4), (4.9), (4.10) and (4.11). Then by Lemma 4.6, we obtain that σmin(X) > 1/6 · σmin(Z). Now by Lemma 4.7 and equation (4.11), we have that ∥∥∥ZZT X − XXT X∥∥∥F 6 δ for δ 6 cσmin(Z)3/κ2 for sufficiently small constant c. Then by Lemma 4.8 we obtain that ‖ZZ> − XX>‖F 6 cσmin(Z)2 for sufficiently small constant c. By Lemma C.1, in this region the only points that satisfy the first order optimality condition must satisfy XXT = ZZT .\nHandling Noise. To handle noise, notice that we can only hope to get an approximate solution in presence of noise, and to get that our Lemmas only depend on concentration bounds which still apply in the noisy setting. See Section B for details."
    }, {
      "heading" : "5 Conclusions",
      "text" : "Although the matrix completion objective is non-convex, we showed the objective function has very nice properties that ensures the local minima are also global. This property gives guarantees for many basic optimization algorithms. An important open problem is the robustness of this property under different model assumptions: Can we extend the result to handle asymmetric matrix completion? Is it possible to add weights to different entries (similar to the settings studied in [LLR16])? Can we replace the objective function with a different distance measure rather than Frobenius norm (which is related to works on 1-bit matrix sensing [DPvdBW14])? We hope this framework of analyzing the geometry of objective function can be applied to other problems."
    }, {
      "heading" : "A Omitted Proofs in Section 3",
      "text" : "We first prove the equivalent form of the first and second order optimality conditions:\nLemma A.1 (Proposition 3.1 restated). The first order optimality condition of objective (3.1) is,\n2PΩ(M − xx>)x = λ∇R(x) , (A.1)\nand the second order optimality condition requires:\n∀v ∈ d, ‖PΩ(vx> + xv>)‖2F + λv>∇2R(x)v > 2v>PΩ(M − xx>)v . (A.2)\nMoreover, The τ-relaxed second order optimality condition requires\n∀v ∈ d, ‖PΩ(vx> + xv>)‖2F + λv>∇2R(x)v > 2v>PΩ(M − xx>)v − τ‖v‖2 . (A.3)\nProof. We take the Taylor’s expansion around point x. Let δ be an infinitesimal vector, we have\nf (x + δ) = 1 2 ‖PΩ(M − (x + δ)(x + δ)>)‖2F + λR(x + δ) + o(‖δ‖2)\n= 1 2 ‖PΩ(M − xx> − (xδ> + δx>) − δδ>)‖2F + λ\n( R(x) + 〈∇R(x), δ〉 + 1\n2 δT∇2R(x)δ\n) + o(‖δ‖2)\n= 1 2 ‖M − xx>‖2Ω + λR(x)\n− 〈PΩ(M − xx>), xδ> + δx>〉 + 〈∇R(x), δ〉 + o(‖δ‖2)\n− 〈PΩ(M − xx>), δδ>〉 + 1 2 ‖xδ> + δx>‖2F + 1 2 λδ>∇2R(x)δ + o(‖δ‖2)."
    }, {
      "heading" : "By symmetry 〈PΩ(M−xx>), xδ>〉 = 〈PΩ(M−xx>), δx>〉 = 〈PΩ(M−xx>)x, δ〉, so the first order optimality condition",
      "text" : "is ∀δ, 〈−2PΩ(M − xx>)x + λ∇R(x), δ〉 = 0, which is equivalent to that 2PΩ(M − xx>)x = λ∇R(x).\nThe second order optimality condition says −〈PΩ(M − xx>), δδ>〉 + 12 ‖xδ> + δx>‖2F + 1 2λδ >∇2R(x)δ > 0 for every δ, which is exactly equivalent to Equation (3.3).\nNext we show the full proof for the second order optimality condition:\nLemma A.2 (Lemma 3.8 restated). In the setting of Theorem 3.2, with high probability over the choice of Ω, suppose x ∈ B′ satisfies second-order optimality condition (3.3) or τ-relaxed condition for τ 6 0.1p, we have ‖x‖2 > 1/8.\nProof. If ‖x‖ > 1, then we are done. Therefore in the rest of the proof we assume ‖x‖ 6 1. The proof is very similar to Lemma 3.4. We plug in v = zJ instead into equation (3.3), where J = {i : |xi| 6 α}. Note that ∇R(zJ) vanishes. We plug in v = zJ in the equation (3.3) and obtain that x satisfies that∥∥∥PΩ(zJ x> + xz>J )∥∥∥2F > 2z>J PΩ(M − xx>)zJ . (A.4)\nNote that we assume ‖x‖∞ 6 2α, and in the beginning of the proof we assume wlog ‖x‖ 6 1. Moreover, we have ‖zJ‖ 6 µ√d an, ‖zJ‖ 6 1. Similarly to the derivation in the proof of Lemma 3.4, we apply Theorem D.1 (twice) and obtain that with high probability over the choice of Ω, for every x, for ε = Õ(µ2(pd)−1/2),\nLHS of (A.4) = p ∥∥∥zJ x> + xz>J ∥∥∥2F ± O(pε) = 2p‖x‖2‖zJ‖2 + 2p〈x, zJ〉2 ± O(pε) . RHS of (A.4) = 2 ( 〈PΩ(zz>), PΩ(zJz>J )〉 − 〈PΩ(xx>), PΩ(zJz>J )〉 ) (Since M = zz>)\n= 2‖zJ‖4 − 2〈x, zJ〉2 ± O(pε) . (by Theorem D.1)\n(Again notice that using τ-relaxed second order optimality condition does not effect the RHS by too much, so it does not change later steps.) Therefore plugging the estimates above back into equation (A.4), we have that\np‖x‖2‖zJ‖2 + 2p〈x, zJ〉2 > p‖zJ‖4 ± O(pε) ,\nUsing Cauchy-Schwarz, we have ‖x‖2‖zJ‖2 > 〈x, zJ〉2, and therefore we obtain that ‖zJ‖2‖x‖2 > 13‖zJ‖4 − O(ε). Finally, we claim that ‖zJ‖2 > 1/2, which completes the proof since ‖x‖2 > 13‖zJ‖2 − O(ε) > 1/8. Claim A.3. Suppose α > 4µ√ d\nand x satisfies ‖x‖∞ 6 4α and ‖x‖ 6 2. Let J = {i : |xi| 6 α}. Then we have that ‖zJ‖ > 1/2.\nThe claim can be simply proved as follows: Since ‖x‖2 6 2 we have that |Jc| 6 2/α2 and therefore ‖zJc‖2 6 2µ2/(dα2). This further implies that ‖zJ‖2 = ‖z‖2 − ‖zL‖2 > (1 − 2µ2/(dα2)) > 12 because α > 2µ√ d .\nLemma A.4 (Lemma 3.10 restated). Suppose x ∈ B′ satisfies that ‖x‖2 > 1/8, under the same assumption as Lemma 3.9. we have, ∥∥∥〈x, z〉z − ‖x‖2x∥∥∥ 6 O(ε) (A.5) Proof. We consider the subset of coordinates J = {i : |xi| 6 α} where the regularization g(x) doesn’t have any effect. Let L = {i : |xi| > α} be the complement of J. By Claim A.3, we have that ‖zL‖ > 1/2.\nFor coordinates that are in J, by equation (3.13), we have that∥∥∥〈x, z〉zJ − ‖x‖2xJ∥∥∥ = ∥∥∥∥〈x, z〉zJ − ((‖x‖2xJ + γ∇R(x)))J∥∥∥∥ 6 O(ε) . (A.6) Note by triangle inequality, (A.6) also implies 〈x, z〉 > ‖x‖3 − O(ε) > 1/20. Therefore we can divide both side by\n〈x, z〉, and let β = ‖x‖2/〈x, z〉.\n‖zJ − βxJ‖ 6 O(ε) . (A.7) We now claim β is large:\nClaim A.5. β > 1 − O(ε)\nProof. We first claim ‖x‖ 6 1 + O(ε). This is because 〈∇R(x), x〉 > 0, so ‖‖x‖2x + γ∇R(x)‖ > ‖‖x‖2x‖ = ‖x‖3, and ‖〈x, z〉z‖ 6 ‖x‖. When ‖x‖ > 1 + Cε for large C we have ∥∥∥〈z, x〉z − ‖x‖2x − γ · ∇R(x)∥∥∥ > C′ε for large C′ and that contradicts with (3.13).\nThis implies the norm of xJ is bounded by (1 + O(ε))zJ because for any coordinate i in L, we know |xi| > α > |zi|. Now we use (A.7) and triangle inequality:\n‖βxJ‖ > ‖zJ‖ − O(ε) (A.8) > (1 − O(ε))‖zJ‖ (using ‖zJ‖ > 1/2.) > (1 − O(ε))‖xJ‖\nUsing the Claim, we can now consider the case when i ∈ L. Without loss of generality let’s assume xi > α. Since we have shown that βxi > (1 − O(ε))α, we have that βxi > (1 − O(ε))α > |zi|. Moreover we have λ∇R(x)i > 0 when xi > α. This implies |(βxi + γ∇R(x)i) − zi| > |βxi − zi|. Therefore,\n‖zL − βxL‖ 6 ∥∥∥zL − (βx + γ∇R(x))L∥∥∥\nCombining this with (A.6), we have the result."
    }, {
      "heading" : "B Handling Noise",
      "text" : "Suppose instead of observing the matrix ZZT , we actually observe a noisy version M = ZZT + N, where N is a Gaussian matrix with independent N(0, σ2) entries. In this case we should not hope to exactly recover ZZT (as two close Z’s may generate the same observation). In this Section we show even with fairly large noise our arguments can still hold.\nTheorem B.1. Let µ̂ = max{µ, √ 4σd √\nlog d r }. Suppose p > Cm̂u 6κ12r4d−1ε−2 log1.5 d where C is a large enough\nconstant. Let α = 2µ̂κr/ √\nd, λ > µ̂2rp/α2. Then with high probability over the randomness of Ω, any local minimum X of f (·) satisfies\n‖XXT − ZZT ‖F 6 ε. In fact, a noise level σ √ log d 6 µ2r/d (when the noise is almost as large as the maximum possible entry) does not\nchange the conclusions of Lemmas in this Section.\nProof. There are only three places in the proof where the noise will make a difference. These are: 1. The infinity norm bound of M, used in Lemma 4.4. 2. The LHS of first order optimality condition (Equation (4.2)). 3. The RHS of second order optimality condition (Equation (4.3)).\nWhat we require in these three steps are: 1. |M|∞ should be smaller than µ2r/d. 2. 〈PΩ(N),W〉 should be smaller than |〈PΩ(N), PΩ(W)〉| 6 O(σ|Z|∞dr log d + √ pd2rσ2|W |∞‖W‖F log d). 3. ‖PΩ(N)‖ 6 εp‖ZZT ‖F . When we define the\nµ̂ = max{µ, √ 4σd √\nlog d r }, all of these are satisfied (by Lemma D.5 and D.6).\nNow we can follow the proof and see δ 6 cεσmin(Z)/κ2 for small enough constant c, and By Lemma 4.8 we know ‖XXT − ZZT ‖F 6 ε."
    }, {
      "heading" : "C Finding the Exact Factorization",
      "text" : "In Section 4, we showed that any point that satisfies the first and second order necessary condition must satisfy ‖XXT − ZZT ‖F 6 c for a small enough constant c. In this section we will show that in fact XXT must be exactly equal to ZZT . The proof technique here is mostly based on the work of Sun and Luo[SL15]. However we have to modify their proof because we use slightly different regularizers, and we work in the symmetric case. The main Lemma in [SL15] can be rephrased as follows in our setting:\nLemma C.1 (Analog to Lemma 3.1 in [SL15]). Assuming p is at least Cµ4r6κ4d−1 log d for large enough d, for any X such that ‖XXT − ZZT ‖F 6 ε 6 σmin(Z)2/100, ‖X‖2→∞ is bounded as in Lemma 4.4, there must be a matrix U such that UUT = ZZT and\n〈∇ f (X), X − U〉 > p 4 ‖M − XXT ‖2F .\nAs a corollary, the points in this set that satisfy first order optimality condition must have XXT = ZZT .\nIn order to prove this main lemma, we separate f (X) = g(X) + R(X) where g(X) = 12 ‖PΩ(M − XXT ‖2F , and R(X) is the regularizer. We will first show that the regularizer always has a positive correlation with X − U. Claim C.2. For any U such that UUT = ZZT ,\n〈∇R(X), X − U〉 > 0.\nProof. Since the regularizer is applied to individual rows, we will focus on a row Xi. The key observation here is that for each row Xi, ∇R(Xi) is 0 when ‖Xi‖ 6 2µ/ √ d 6 α. When ‖Xi‖ is larger ∇R(Xi) is always in the same direction as\nXi. Therefore we know either ∇R(Xi) = 0, or ∇R(Xi) = λXi for some λ > 0 and ‖Xi‖ > 2µ/ √\nd > 2‖Zi‖ = 2‖Ui‖. The innerproduct 〈∇R(Xi), Xi − Ui〉 is 0 in the former case, and is at least λ(‖Xi‖2 − ‖Xi‖‖Ui‖) in the latter case.\nNote that this Claim is much more complicated in [SL15] (Claim 4.1) in order to deal with the asymmetric case. Next we will prove the gradient of g(X) has a large correlation with X − U\nClaim C.3. There exists a matrix U where UUT = ZZT , such that\n〈∇R(X), X − U〉 > p 4 ‖M − XXT ‖2F .\nThe proof of this Claim follows from the same strategy as in [SL15]. We will first show that there exists a matrix U such that ‖U − X‖F is small, and then use concentration bounds to show the lowerbound on innerproduct. The concentration bounds are exactly the same as [SL15] (see the proof of Proposition 4.3), the only thing we need to do here is an equivalent version of Proposition 4.1 and 4.2:\nClaim C.4. Suppose ‖XXT − M‖F = ε 6 σmin(Z)2/100, there exists a matrix U such that UUT = M and ‖X − U‖F 6 5ε √ r/σmin(Z)2.\nNote that in this claim we do not need to prove anything about the row norm/Frobenius norm of U, because whenever UUT = M, U must have the same row norm and Frobenius norm as the original low rank component Z. That is why the proof here is again much simpler than [SL15].\nProof. Without loss of generality we assume M is a diagonal matrix with first r diagonal terms being σ1(Z)2, σ2(Z)2, ..., σr(Z)2 (this can be done by a change of basis). We use M′ to denote the first r × r submatrix of M.\nNow we write X in a block form XT = (VT WT ), where V is an r × r matrix and W is an (d − r) × r matrix. Clearly, we want to truncate the W part (because the corresponding part in M is empty), and we need to “fix” the V part so that the first r × r submatrix in XXT is exactly equal to M.\nWe will construct the first r× r block of U as P = VQ = V(VT M−1V)−1/2 (where Q := (VT M−1V)−1/2), all the other entries of U are 0. The difference in U and X is equal to ‖U − X‖F 6 ‖P − V‖F + ‖W‖F .\nSince ‖XXT − M‖F = ε, we know ‖M′ − VVT ‖2F + 2‖VWT ‖2F 6 ε2. In particular both terms are smaller than ε2. First, we bound ‖W‖F . Note that since ‖M′ − VVT ‖F 6 ε 6 σmin(Z)2/100, we know σmin(V)2 > 0.99σmin(Z)2. Therefore σmin(V) > 0.9σmin(Z). Now we know ‖W‖F 6 ‖VWT ‖F/σmin(V) 6 2ε/σmin(Z). Next we bound ‖P − V‖2F . Since ‖M′ − VVT ‖F 6 ε leσmin(Z)2/100, we know (1 − 2ε/σmin(Z)2)VVT M′ (1 + 2ε2/σmin(Z)2)VVT . This implies ‖V‖F 6 1.1‖Z‖F , and (1 − 2ε/σmin(Z)2)I VT M−1V (1 + 2ε/σmin(Z)2)I. Therefore the matrix Q is also very close to identity, in particular, ‖Q− I‖ 6 2ε/σmin(Z)2. Now we know ‖P−V‖F = ‖V‖F‖Q− I‖ 6 3ε‖Z‖F/σmin(Z)2. Using the fact that ‖Z‖F = 1 we know ‖U − X‖F 6 ‖P − V‖F + ‖W‖F 6 5ε √ r/σmin(Z)2."
    }, {
      "heading" : "D Concentration inequality",
      "text" : "In this section we prove the concentration inequalities used in the main part. We first show that the inner-product of two low rank matrices is preserved after restricting to the observed entries. This is mostly used in arguments about the second order necessary conditions.\nTheorem D.1. With high probability over the choice of Ω, for any two rank-r matrices W,Z ∈ d×d, we have\n|〈PΩ(W), PΩ(Z)〉 − p〈W,Z〉| 6 O(|W |∞|Z|∞dr log d + √ pdr|W |∞|Z|∞ ‖W‖F ‖Z‖F log d)\nProof. Since both LHS and RHS are bilinaer in both W and Z, without loss of generality we assume the Frobenius norms of W and Z are all equal to 1. Note that in this case we should expect |W |∞ > 1/d.\nLet δi, j be the indicator variable for Ω, we know 〈PΩ(W,Z〉 = ∑ i, j δi, jWi, jZi, j,\nand in expectation it is equal to p〈W,Z〉. Let Q = ∑i, j(δi, j − p)Wi, jZi, j. We can then view Q as a sum of independent entries (note that δi, j = δ j,i, but we can simply merge the two terms and the variance is at most a factor 2 larger). The expectation [Q] = 0. Each entry in the sum is bounded by |W |∞|Z|∞, and the variance is bounded by\n[Q] 6 p ∑ i, j (Wi, jZi, j)2\n6 p max i, j |Wi, j|2 ∑ i, j Z2i, j\n6 p|W |2∞.\nSimilarly, we also know [Q] 6 p|Z|2∞ and hence [Q] 6 p|W |∞|Z|∞. Now we can apply Bernstein’s inequality, with probability at most η,\n|Q − [Q]| > |W |∞|Z|∞ log 1/η + √\np|W |∞|Z|∞ log(1/η). By Proposition E.2, there is a set Γ of size dO(dr) such that for any rank r matrix X, there is a matrix X̂ ∈ Γ such that ‖X − X̂‖F 6 1/d3. When W and Z come from this set, we can set η = d−Cdr for a large enough constant C. By union bound, with high probability\n|Q − [Q]| 6 O(|W |∞|Z|∞dr log d + √ pdr|W |∞|Z|∞ log d).\nWhen W and Z are not from this set Γ, let Ŵ and Ẑ be the closest matrix in Γ, then we know |〈PΩ(W), PΩ(Z)〉 − p〈W,Z〉 − (〈PΩ(Ŵ), PΩ(Ẑ)〉 − p〈Ŵ, Ẑ〉)| 6 O(1/d3) |W |∞|Z|∞dr log d. Therefore we still have\n|〈PΩ(W), PΩ(Z)〉 − p〈W,Z〉| 6 O(|W |∞|Z|∞dr log d + √ pdr|W |∞|Z|∞ ‖W‖F ‖Z‖F log d).\nNext Theorem shows PΩ(XXT )X is roughly equal to pXXT X, this is one of the major terms in the gradient.\nTheorem D.2. When p > Cν 6r log1.5 d\ndε2 for a large enough constant C, With high probability over the randomness of Ω, for any matrix X ∈ d×r such that ‖Xi‖ 6 ν √ 1 d ‖X‖F , we have\n‖PΩ(XX>)X − pXXT X‖F 6 pε‖X‖3F (D.1)\nProof. Without loss of generality we assume ‖X‖F = 1. Let δi, j be the indicator variable for Ω, we first prove the result when δi, j are independent, then we will use standard techniques to show the same argument works for δi, j = δ j,i.\nNote that [PΩ(XX>)X]i = ∑ j δi, j〈Xi, X j〉X j,\nwhose expectation is equal to [pXXT X]i = p ∑ j 〈Xi, X j〉X j.\nWe know ‖Xi‖ 6 ν √ 1 d , therefore each term is bounded by ν 3(1/d)3/2. Let Zi be a random variable that is equal to ‖PΩ(XX>)X]i − [pXXT X]i‖2, then it is easy to see [Zi] 6 pdν6(r/d)3 = pν6/d2. and the variance [Zi] = [Z2i ] − [Zi]2 6 pdν12(1/d)6 + 2 [Zi]2 6 3 [Zi]2 (as long as p > 1/d). Our goal now is to prove ∑d i=1 Zi 6 p\n2ε2 for all X.\nLet Zi be a truncated version of Zi. That is, Zi = Zi when Zi 6 [2pdν3(1/d)3/2]2, and Zi = [2pdν3(1/d)3/2]2 otherwise. It’s not hard to see Zi has smaller mean and variance compared to Zi. Also, by vector’s Bernstein’s inequality, we know\n[ √ Zi 6 t] 6 d exp − t2pν6 d2 + t ν3 d3/2  . Notice that this is only relevant when t 6 O(pν3d−1/2) (because otherwise the probability is 0) and in that regime\nthe variance term always dominates. Therefore Zi is the square of a subgaussian random variable. By the Bernstein’s inequality, we know the moments of √ Zi − [ √\nZi] are dominated by a Gaussian distribution with variance O( [Zi √ log d).\nNow we can use the concentration bound for quadratics of the subgaussian random variables[HKZ12]: we know that with probability exp(−t),\nd∑ i=1 Zi 6 O( [Z2i ] √ log d(d + 2 √ dt + 2t)).\nthis means with probability exp(−Cdrlogd) with some large constant C, we know ∑di=1 Zi 6 O(pν6r log1.5 d/d). The probability is low enough for us to union bound over all X in a standard ε-net such that every other X is within distance (ε/d)6. Therefore we know with high probability for all X in the ε-net we have ∑d i=1 Zi 6 O(pν 6r log1.5 d/d), which is smaller than p2ε2 when p > Cν 6r log1.5 d\ndε2 for a large enough constant C. For any X̂ that is not in the ε-net, let X be the closest point of X in the net, then ‖X − X̂‖F 6 1/d6, therefore the bound of X̂ clearly follows from the bound of X. Now to convert sum of Zi to sum of Zi, notice that with high probability there are at most 2pd entries in Ω for every\nrow. When that happens Zi is always bounded by [2pdν3(1/d)3/2]2 so Zi = Zi. Let event 1 be ∑d i=1 Zi 6 p 2ε2 for all X, and let event 2 be that there are at most 2pd entries per row, we know with high probability both event happens, and in that case ∑d i=1 Zi 6 p 2ε2 for all X.\nHandling δi, j = δ j,i. First notice that the diagonal entries δi,i’s cannot change the Frobenius norm by more than O(ν3(1/d)3/2 · √ d) 6 pε so we can ignore the diagonal terms. Now for independent terms δi, j, let γ j,i = δi, j, then by union bound both δi, j and γi, j satisfy the equation, and by triangle’s inequality (δi, j + γi, j)/2 also satisfies the inequality. Let τi, j be the true indicator of Ω (hence τi, j = τ j,i), and τ′i, j be an independent copy, we know (τi, j + τ ′ i, j)/2 has the same distribution as (δi, j +γi, j)/2 (for off-diagonal entries), therefore with high probability the equation is true for (τi, j +τ′i, j)/2. The Theorem then follows from the standard Claim below for decoupling (note that sup‖X‖F=1 ‖PΩ(XX\nT )X − pXXT X‖F is a norm for the indicator variables of Ω): Claim D.3 (see e.g. [? ]). Let X,Y be two iid random variables, then\n[‖X‖ > t] 6 3 [‖X + Y‖ > 2t 3 ].\nProof. Let X,Y,Z be iid random variables then,\n[X > t] = [‖(X + Y) + (X + Z) − (Y + Z)‖ > 2t] 6 [‖X + Y‖ > 2t/3] + [‖X + Z‖ > 2t/3] + [‖Y + Z‖ > 2t/3]\n6 3 [‖X + Y‖ > 2t 3 ].\nFinally we argue that random sampling of a matrix gives a nice spectral approximation. This is a standard Lemma that is used in arguing about the PΩ(M)X term in the gradient (PΩ(M − XXT )X).\nLemma D.4. Suppose W ∈ d×d satisfies that |W |∞ 6 νd ‖W‖F , then with high probability (1 − d−10) over the choice of Ω, ‖PΩ(W) − pW‖ 6 εp‖W‖F . where ε = O(ν √ log d/(pd)).\nProof. Without loss of generality we assume ‖W‖F = 1. The proof follows simply from application of Bernstein inequality. We view PΩ(W) as PΩ(W) = ∑\ni, j∈[d]2 si jWi jδi j\nwhere δi j ∈ d×d is the indicator matrix for entry (i, j), and si j ∈ {0, 1} are independent Bernoulli variable with probability p of being 1. Then we have that [PΩ(W)] = pW and ‖si jWi jδi j‖ 6 νd ‖W‖F . Moreover, we compute the variance by ∑\ni, j∈[d]2 [si jW2i jδ > i jδi j] = ∑ i, j∈[d]2 [si jW2i jδ j j]\n= ∑ j∈[d] p ∑ i∈d W2i j  δ j j (D.2) Therefore ∥∥∥∥∥∥∥∥ ∑ i, j∈[d]2 [si jW2i jδ > i jδi j] ∥∥∥∥∥∥∥∥ 6 pν 2 d\nSimilarly we can control ∥∥∥∥∑i, j∈[d]2 [si jW2i jδi jδ>i j]∥∥∥∥ by pν2/d (again notice that although δi, j = δ j,i the bounds here are correct up to constant factors). Then it follows from non-commutative Bernstein inequality [Imb10] that\nΩ\n[‖PΩ(W) − p(W)‖ > εp] 6 d exp(−2ε2 pd/ν2) .\nConcentration Lemmas for Noise Matrix N. Next we will state the concentration lemmas that are necessary when observed matrix is perturbed by Gaussian noise. The proof of these Lemmas are really exactly the same (in fact even simpler) than the corresponding Theorem that we have just proven. The first Lemma is used in the same settings as Theorem D.1.\nLemma D.5. Let N be a random matrix with independent Gaussian entries N(0, σ2). With high probability over the support Ω and the Gaussian N, for any low rank matrix W, we have\n|〈PΩ(N), PΩ(W)〉| 6 O(σ|Z|∞dr log d + √ pd2rσ2|W |∞‖W‖F log d\nProof. The proof is exactly the same as Theorem D.1 as |〈PΩ(N), PΩ(W)〉| is a sum of independent entries that follows from the same Bernstein’s inequality.\nNext we show that random sampling entries of a Gaussian matrix gives a matrix with low spectral norm.\nLemma D.6. Let N be a random Gaussian matrix with independent Gaussian entries N(0, σ2), with high probability over the choice of Ω and N, we have ‖PΩ(N)‖ 6 εpσd, where ε = O( √ log d/pd).\nProof. Again the proof follows from the same argument as Lemma D.4."
    }, {
      "heading" : "E Auxiliary Lemmas",
      "text" : "Proposition E.1. Let a1, . . . , ar > 0, C > 0. Then C4(a21 + · · · + a2r ) > a61 + · · · + a6r implies that a21 + · · · + a2r 6 C2r and that max ai 6 Cr1/6.\nProof. By Cauchy-Schwarz inequality, we have, r∑ i=1 a2i   r∑ i=1 a6i  >  r∑ i=1 a4i 2 > 1r  r∑ i=1 a2i 2  2\nUsing the assumption and equation above we have that a21 + · · · + a2r 6 C2r. This implies with the condition that a61 + · · · + a6r 6 C6r, which implis that max ai 6 Cr1/6.\nProposition E.2. For any ζ ∈ (0, 1), there is a set Γ of rank r d × d matrices, such that for any rank r d × d matrix X with Frobenius norm at most 1, there is a matrix X̂ ∈ Γ with ‖X − X̂‖F 6 ζ. The size of Γ is bounded by (d/ζ)O(dr).\nProof. Standard construction of ε-net shows that there is a set P ⊂ d of size (d/ε)O(d) such that for any ‖u‖ 6 1, there is a û ∈ P such that ‖u − û‖ 6 ε. Such construction can also be applied to matrices and Frobenius norm as that is the same as vectors and `2 norm.\nHere we let ε = 0.1ζ, and construct three sets P1, P2, P3 where P1 is an ε-net for d × r matrices with Frobenius norm at most √ r, P2 is an ε-net for r × r diagonal matrices whose Frobenius norm is bounded by 1, and P3 is an ε-net for r × d matrices with Frobenius norm at most √\nr. Now we define Γ = {ÛD̂V̂ |Û ∈ P1, D̂ ∈ P2, V̂ ∈ P3}. Clearly the size of Γ is as promised. For any rank r d × d matrix X, suppose its Singular Value Decomposition is UDV , we can find Û ∈ P1, D̂ ∈ P2 and V̂ ∈ P3 that are ε close to U,D,V respectively. Therefore ÛD̂V̂ ∈ Γ and it is easy to check\n‖UDV − ÛD̂V̂‖F 6 8ε 6 ζ."
    } ],
    "references" : [ {
      "title" : "Uncovering shared structures in multiclass classification",
      "author" : [ "Yonatan Amit", "Michael Fink", "Nathan Srebro", "Shimon Ullman" ],
      "venue" : "In Proceedings of the 24th international conference on Machine learning,",
      "citeRegEx" : "Amit et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Amit et al\\.",
      "year" : 2007
    }, {
      "title" : "On the low-rank approach for semidefinite programs arising in synchronization and community detection",
      "author" : [ "Afonso S Bandeira", "Nicolas Boumal", "Vladislav Voroninski" ],
      "venue" : "arXiv preprint arXiv:1602.04426,",
      "citeRegEx" : "Bandeira et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bandeira et al\\.",
      "year" : 2016
    }, {
      "title" : "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization",
      "author" : [ "Samuel Burer", "Renato DC Monteiro" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Burer and Monteiro.,? \\Q2003\\E",
      "shortCiteRegEx" : "Burer and Monteiro.",
      "year" : 2003
    }, {
      "title" : "Do we need good initialization for low rank matrix recovery",
      "author" : [ "Srinadh Bhojanapalli", "Behnam Neyshabur", "Nathan Srebro" ],
      "venue" : "Personal Communication,",
      "citeRegEx" : "Bhojanapalli et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bhojanapalli et al\\.",
      "year" : 2016
    }, {
      "title" : "Robust principal component analysis",
      "author" : [ "Emmanuel J Candès", "Xiaodong Li", "Yi Ma", "John Wright" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Candès et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Candès et al\\.",
      "year" : 2011
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel J Candès", "Benjamin Recht" ],
      "venue" : "Foundations of Computational mathematics,",
      "citeRegEx" : "Candès and Recht.,? \\Q2009\\E",
      "shortCiteRegEx" : "Candès and Recht.",
      "year" : 2009
    }, {
      "title" : "The power of convex relaxation: Near-optimal matrix completion",
      "author" : [ "Emmanuel J Candès", "Terence Tao" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Candès and Tao.,? \\Q2010\\E",
      "shortCiteRegEx" : "Candès and Tao.",
      "year" : 2010
    }, {
      "title" : "Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees",
      "author" : [ "Yudong Chen", "Martin J Wainwright" ],
      "venue" : "arXiv preprint arXiv:1509.03025,",
      "citeRegEx" : "Chen and Wainwright.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen and Wainwright.",
      "year" : 2015
    }, {
      "title" : "1-bit matrix completion",
      "author" : [ "Mark A Davenport", "Yaniv Plan", "Ewout van den Berg", "Mary Wootters" ],
      "venue" : "Information and Inference,",
      "citeRegEx" : "Davenport et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Davenport et al\\.",
      "year" : 2014
    }, {
      "title" : "Escaping from saddle points—online stochastic gradient for tensor decomposition",
      "author" : [ "Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan" ],
      "venue" : null,
      "citeRegEx" : "Ge et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2015
    }, {
      "title" : "Understanding alternating minimization for matrix completion",
      "author" : [ "Moritz Hardt" ],
      "venue" : "In FOCS 2014. IEEE,",
      "citeRegEx" : "Hardt.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hardt.",
      "year" : 2014
    }, {
      "title" : "A tail inequality for quadratic forms of subgaussian random vectors",
      "author" : [ "Daniel Hsu", "Sham M Kakade", "Tong Zhang" ],
      "venue" : "Electron. Commun. Probab,",
      "citeRegEx" : "Hsu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2012
    }, {
      "title" : "Fast matrix completion without the condition number",
      "author" : [ "Moritz Hardt", "Mary Wootters" ],
      "venue" : "COLT",
      "citeRegEx" : "Hardt and Wootters.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hardt and Wootters.",
      "year" : 2014
    }, {
      "title" : "Sums of random Hermitian matrices and an inequality by Rudelson",
      "author" : [ "R. Imbuzeiro Oliveira" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Oliveira.,? \\Q2010\\E",
      "shortCiteRegEx" : "Oliveira.",
      "year" : 2010
    }, {
      "title" : "Low-rank matrix completion using alternating minimization",
      "author" : [ "Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi" ],
      "venue" : "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Jain et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2013
    }, {
      "title" : "Matrix completion from a few entries",
      "author" : [ "Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Keshavan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Keshavan et al\\.",
      "year" : 2010
    }, {
      "title" : "The bellkor solution to the netflix grand prize",
      "author" : [ "Yehuda Koren" ],
      "venue" : "Netflix prize documentation,",
      "citeRegEx" : "Koren.,? \\Q2009\\E",
      "shortCiteRegEx" : "Koren.",
      "year" : 2009
    }, {
      "title" : "Recovery guarantee of weighted low-rank approximation via alternating minimization",
      "author" : [ "Yuanzhi Li", "Yingyu Liang", "Andrej Risteski" ],
      "venue" : "arXiv preprint arXiv:1602.02262,",
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Gradient descent converges to minimizers",
      "author" : [ "Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht" ],
      "venue" : "University of California, Berkeley,",
      "citeRegEx" : "Lee et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2016
    }, {
      "title" : "Cubic regularization of Newton method and its global performance",
      "author" : [ "Yurii Nesterov", "Boris T Polyak" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Nesterov and Polyak.,? \\Q2006\\E",
      "shortCiteRegEx" : "Nesterov and Polyak.",
      "year" : 2006
    }, {
      "title" : "Nonconvergence to unstable points in urn models and stochastic approximations",
      "author" : [ "Robin Pemantle" ],
      "venue" : "The Annals of Probability,",
      "citeRegEx" : "Pemantle.,? \\Q1990\\E",
      "shortCiteRegEx" : "Pemantle.",
      "year" : 1990
    }, {
      "title" : "A simpler approach to matrix completion",
      "author" : [ "Benjamin Recht" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Recht.,? \\Q2011\\E",
      "shortCiteRegEx" : "Recht.",
      "year" : 2011
    }, {
      "title" : "Fast maximum margin matrix factorization for collaborative prediction",
      "author" : [ "Jasson DM Rennie", "Nathan Srebro" ],
      "venue" : "In Proceedings of the 22nd international conference on Machine learning,",
      "citeRegEx" : "Rennie and Srebro.,? \\Q2005\\E",
      "shortCiteRegEx" : "Rennie and Srebro.",
      "year" : 2005
    }, {
      "title" : "Guaranteed matrix completion via nonconvex factorization",
      "author" : [ "Ruoyu Sun", "Zhi-Quan Luo" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Sun and Luo.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sun and Luo.",
      "year" : 2015
    }, {
      "title" : "When are nonconvex problems not scary",
      "author" : [ "Ju Sun", "Qing Qu", "John Wright" ],
      "venue" : "arXiv preprint arXiv:1510.06096,",
      "citeRegEx" : "Sun et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2015
    }, {
      "title" : "A nonconvex optimization framework for low rank matrix estimation",
      "author" : [ "Tuo Zhao", "Zhaoran Wang", "Han Liu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization suffices in practice. We prove that the commonly used non-convex objective function for matrix completion has no spurious local minima – all local minima must also be global. Therefore many popular optimization algorithms such as (stochastic) gradient descent can provably solve matrix completion with arbitrary initialization in polynomial time.",
    "creator" : "LaTeX with hyperref package"
  }
}