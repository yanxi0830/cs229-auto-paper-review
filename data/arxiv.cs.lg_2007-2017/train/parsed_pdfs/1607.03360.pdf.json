{
  "name" : "1607.03360.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Approximate maximum entropy principles via Goemans-Williamson with applications to provable variational methods",
    "authors" : [ "Yuanzhi Li", "Andrej Risteski" ],
    "emails" : [ "risteski@cs.princeton.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 7.\n03 36\n0v 1\n[ cs\n.L G\n] 1\n2 Ju\nWe additionally provide surprising applications of the approximate maximum entropy principle to designing provable variational methods for partition function calculations for Ising models without any assumptions on the potentials of the model. More precisely, we show that in every temperature, we can get approximation guarantees for the log-partition function comparable to those in the low-temperature limit, which is the setting of optimization of quadratic forms over the hypercube. (Alon and Naor, 2006)"
    }, {
      "heading" : "1 Introduction",
      "text" : "Maximum entropy principle The maximum entropy principle (Jaynes, 1957) states that given mean parameters, i.e. Eµ[φt(x)] for a family of functionals φt(x), t ∈ [1, T ], where µ is distribution over the hypercube {−1, 1}n, the entropy-maximizing distribution µ is an exponential family distribution, i.e. µ(x) ∝ exp(∑Tt=1 Jtφt(x)) for some potentials Jt, t ∈ [1, T ]. 1 This principle has been one of the reasons for the popularity of graphical models in machine learning: the “maximum entropy” assumption is interpreted as “minimal assumptions” on the distribution other than what is known about it.\nHowever, this principle is problematic from a computational point of view. Due to results of (Bresler et al., 2014; Singh and Vishnoi, 2014), the potentials Jt of the Ising model, in many cases, are impossible to estimate well in polynomial time, unless NP = RP – so merely getting the description of the maximum entropy distribution is already hard. Moreover, in order to extract useful information about this distribution, usually we would also like to at least be able to sample efficiently from this distribution – which is typically NP-hard or even #P-hard.\nIn this paper we address this issue in certain cases. We provide a “bi-criteria” approximation for the special case where the functionals φt(x) are φi,j(x) = xixj , i.e. pairwise moments: we produce an efficiently sampleable distribution over the hypercube which matches these moments up to multiplicative constant factors, and has entropy at most a constant factor smaller from from the entropy of the maximum entropy distribution. 2\n∗Princeton University, Computer Science Department. Email: yuanzhili, risteski@cs.princeton.edu. This work was supported in part by NSF grants CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, Sanjeev Arora’s Simons Investigator Award, and Simons Collaboration Grant.\n1There is a more general way to state this principle over an arbitrary domain, not just the hypercube, but for clarity in this paper we will focus on the hypercube only.\n2In fact, we produce a distribution with entropy Ω(n), which implies the latter claim since the maximum entropy of any distribution of over {−1, 1}n is at most n\nFurthermore, the distribution we consider is very natural: the sign of a multivariate normal variable. This provides theoretical explanation for the phenomenon observed by the computational neuroscience community (Bethge and Berens, 2007) that this distribution (named dichotomized Gaussian there) has near-maximum entropy.\nVariational methods The above results also allow us to get results for a seemingly unrelated problem – approximating the partition function Z = ∑\nx∈{−1,1}n exp( ∑T\nt=1 Jtφt(x)) of a member of an exponential family, which is an important step to calculate marginals.\nOne of the ways to calculate partition function is variational methods: namely, expressing logZ as an optimization problem. While there is a plethora of work on variational methods, of many flavors (mean field, Bethe/Kikuchi relaxations, TRBP, etc; for a survey, see (Wainwright and Jordan, 2008)), they typically come either with no guarantees, or with guarantees in very constrained cases (e.g. loopless graphs; graphs with large girth, etc. (Wainwright et al., 2003; 2005; Weiss, 2000)). While this is a rich area of research, the following extremely basic research question has not been answered:\nWhat is the best approximation guarantee on the partition function in the worst case (with no additional assumptions on the potentials)?\nIn the low-temperature limit, i.e. when |Jt| → ∞, logZ → maxx∈{−1,1}n ∑T\nt=1 Jtφt(x) - i.e. the question reduces to purely to optimization. In this regime, this question has very satisfying answers for many families φt(x). One classical example is when the functionals are φi,j(x) = xixj . In the graphical model community, these are known as Ising models, and in the optimization community this is the problem of optimizing quadratic forms and has been studied by (Charikar and Wirth, 2004; Alon and Naor, 2006; Alon et al., 2006).\nIn the optimization version, the previous papers showed that in the worst case, one can get O(log n) factor multiplicative factor approximation of the log of the partition function, and that unless P = NP, one cannot get better than constant factor approximations of it.\nIn the finite-temperature version, it is known that it is NP-hard to achieve a 1 + ǫ factor approximation to the partition function (i.e. construct a FPRAS) (Sly and Sun, 2012), but nothing is known about coarser approximations. We prove in this paper, informally, that one can get comparable multiplicative guarantees on the log-partition function in the finite temperature case as well – using the tools and insights we develop on the maximum entropy principles.\nOur methods are extremely generic, and likely to apply to many other exponential families, where algorithms based on linear/semidefinite programming relaxations are known to give good guarantees in the optimization regime."
    }, {
      "heading" : "2 Statements of results and prior work",
      "text" : "Approximate maximum entropy The main theorem in this section is the following one.\nTheorem 2.1. For any covariance matrix Σ of a centered distribution µ : {−1, 1}n → R, i.e. Eµ[xixj ] = Σi,j , Eµ[xi] = 0, there is an efficiently sampleable distribution µ̃, which can be sampled as sign(g), where g ∼ N (0,Σ + βI) such that the following holds:\nG 1 + β Σi,j ≤ Eµ̃[XiXj ] ≤ 1 1 + β Σi,j for a fixed constant G, and entropy H(µ̃) ≥\nn 25 (31/4 √ β−1)2√ 3β , for any β ≥ 1 31/2 .\nThere are two prior works on computational issues relating to maximum entropy principles, both proving hardness results.\n(Bresler et al., 2014) considers the “hard-core” model where the functionals φt are such that the distribution µ(x) puts zero mass on configurations x which are not independent sets with respect to some graph G. They show that unless NP = RP, there is no FPRAS for calculating the potentials Jt, given the mean parameters Eµ[φt(x)].\n(Singh and Vishnoi, 2014) prove an equivalence between calculating the mean parameters and calculating partition functions. More precisely, they show that given an oracle that can calculate the mean parameters up to a (1 + ǫ) multiplicative factor in time O(poly(1/ǫ)), one can calculate the partition function of the same exponential family up to (1 + O(poly(ǫ))) multiplicative factor, in time O(poly(1/ǫ)). Note, the ǫ in this work potentially needs to be polynomially small in n (i.e. an oracle that can calculate the mean parameters to a fixed multiplicative constant cannot be used.)\nBoth results prove hardness for fine-grained approximations to the maximum entropy principle, and ask for outputting approximations to the mean parameters. Our result circumvents these hardness results by providing a distribution which is not in the maximum-entropy exponential family, and is allowed to only approximately match the moments as well. To the best of our knowledge, such an approximation, while very natural, has not been considered in the literature.\nProvable variational methods The main theorems in this section will concern the approximation factor that can be achieved by degree-2 pseudo-moment relaxations of the standard variational principle due to Gibbs. (Ellis, 2012) As outlined before, we will be concerned with a particularly popular exponential family: Ising models. We will prove the following three results:\nTheorem 2.2 (Ferromagnetic Ising, informal). There is a convex programming relaxation based on degree-2 pseudomoments that calculates up to multiplicative approximation factor 50 the value of logZ where Z is the partition function of the exponential distribution µ(x) ∝ exp( ∑\ni,j\nJi,jxixj) for Ji,j > 0.\nTheorem 2.3 (Ising model, informal). There is a convex programming relaxation based on degree-2 pseudo-moments that calculates up to multiplicative approximation factor O(log n) the value of logZ where Z is the partition function of the exponential distribution µ(x) ∝ exp( ∑\ni,j\nJi,jxixj).\nTheorem 2.4 (Ising model, informal). There is a convex programming relaxation based on degree-2 pseudo-moments that calculates up to multiplicative approximation factor O(logχ(G)) the value of logZ where Z is the partition function of the exponential distribution µ(x) ∝ exp( ∑\ni,j∈E(G) Ji,jxixj) and G = (V (G), E(G)) is a graph with\nchromatic number χ(G).\nNote Theorem 2.4 is strictly more general than Theorem 2.3, however the proof of Theorem 2.3 uses less heavy machinery and is illuminating enough that we feel merits being presented as a separate result.\nWhile a lot of work is done on variational methods in general (see the survey by (Wainwright and Jordan, 2008) for a detailed overview), to the best of our knowledge nothing is known about the worst-case guarantee that we are interested in here. Moreover, other than a recent paper by (Risteski, 2016), no other work has provided provable bounds for variational methods that proceed via a convex relaxation and a rounding thereof. 3\n(Risteski, 2016) provides guarantees in the case of Ising models that are also based on pseudo-moment relaxations of the variational principle, albeit only in the special case when the graph is “dense” in a suitably defined sense. 4 The results there are very specific to the density assumption and can not be adapted to our worst-case setting.\nFinally, we mention that in the special case of the ferromagnetic Ising models, an algorithm based on MCMC was provided by (Jerrum and Sinclair, 1993), which can give an approximation factor of (1 + ǫ) to the partition function and runs in time O(n11poly(1/ǫ)). In spite of this, the focus of this part of our paper is to provide understanding of variational methods in certain cases, as they continue to be popular in practice for their faster running time compared to MCMC-based methods but are theoretically much more poorly studied."
    }, {
      "heading" : "3 Approximate maximum entropy principles",
      "text" : "Let us recall what the problem we want to solve: Approximate maximum entropy principles We are given a positive-semidefinite matrix Σ ∈ Rn×n with Σi,i = 1, ∀i ∈ [n], which is the covariance matrix of a centered distribution over {−1, 1}n, i.e. Eµ[xixj ] = Σi,j , Eµ[xi] = 0, for a distribution µ : {−1, 1}n → R. We wish to produce a distribution µ̃ : {−1, 1}n → R with pairwise\n3In some sense, it is possible to give provable bounds for Bethe-entropy based relaxations, via analyzing belief propagation directly, which has been done in cases where there is correlation decay and the graph is locally tree-like. (Wainwright and Jordan, 2008) has a detailed overview of such results.\n4More precisely, they prove that in the case when ∀i, j,∆|Ji,j | ≤ ∆n2 ∑ i,j |Ji,j |, one can get an additive ǫ( ∑ i,j Ji,j) approximation to\nlogZ in time n O( ∆ ǫ2 ) .\ncovariances that match the given ones up to constant factors, and entropy within a constant factor of the maximum entropy distribution with covariance Σ. 5\nBefore stating the result formally, it will be useful to define the following constant:\nDefinition 3.1. Define the constant G = mint∈[−1,1] { 2 π arcsin(t)/t } ≈ 0.64.\nWe will prove the following main theorem:\nTheorem 3.1 (Main, approximate entropy principle). For any positive-semidefinite matrix Σ with Σi,i = 1, ∀i, there is an efficiently sampleable distribution µ̃ : {−1, 1}n → R, which can be sampled as sign(g), where g ∼ N (0,Σ+ βI), and satisfies G1+βΣi,j ≤ Eµ̃[xixj ] ≤ 11+βΣi,j and has entropy H(µ̃) ≥ n25 (31/4 √ β−1)2√ 3β , where β ≥ 1 31/2 .\nNote µ̃ is in fact very close to the the one which is classically used to round semidefinite relaxations for solving the MAX-CUT problem. (Goemans and Williamson, 1995) We will prove Theorem 3.1 in two parts – by first lower bounding the entropy of µ̃, and then by bounding the moments of µ̃.\nTheorem 3.2. The entropy of the distribution µ̃ satisfies H(µ̃) ≥ n25 (31/4 √ β−1)2√ 3β when β ≥ 1 31/2 .\nProof. A sample g fromN (0, Σ̃) can be produced by sampling g1 ∼ N (0,Σ), g2 ∼ N (0, βI) and setting g = g1+g2. The sum of two multivariate normals is again a multivariate normal. Furthermore, the mean of g is 0, and since g1, g2 are independent, the covariance of g is Σ+ βI = Σ̃.\nLet’s denote the random variable Y = sign(g1 + g2) which is distributed according to µ̃. We wish to lower bound the entropy of Y. Toward that goal, denote the random variable S := {i ∈ [n] : |(g1)i| ≤ cD} for c,D to be chosen. Then, we have: for γ = c−1c ,\nH(Y) ≥ H(Y|S) = ∑\nS⊆[n] Pr[S = S]H(Y|S = S) ≥\n∑\nS⊆[n],|S|≥γn Pr[S = S]H(Y|S = S)\nwhere the first inequality follows since conditioning doesn’t decrease entropy, and the latter by the non-negativity of entropy. Continue the calculation we can get:\n∑\nS⊆[n],|S|≥γn Pr[S = S]H(Y|S = S) ≥\n∑\nS⊆[n],|S|≥γn Pr[S = S] min S⊆[n],|S|≥γn H(Y|S = S)\n= Pr [|S| ≥ γn] min S⊆[n],|S|≥γn H(Y|S = S)\nWe will lower bound Pr[|S| ≥ γn] first. Notice that E[∑ni=1(g1)2i ] = n, therefore by Markov’s inequality,\nPr\n[\nn ∑\ni=1\n(g1) 2 i ≥ Dn\n]\n≤ 1 D . On the other hand, if ∑n i=1(g1) 2 i ≤ Dn, then |{i : (g1)2i ≥ cD}| ≤ nc , which means\nthat |{i : (g1)2i ≤ cD}| ≥ n− nc = (c−1)n c = γn. Putting things together, this means Pr [|S| ≥ γn] ≥ 1− 1\nD .\nIt remains to lower bound minS⊆[n],|S|≥γnH(Y|S = S). For every S ⊆ [n], |S| ≥ γn, denote by YS the coordinates of Y restricted to S, we get\nH(Y|S = S) ≥ H(YS |S = S) ≥ H∞(YS |S = S) = − log(max yS Pr[YS = yS |S = S])\n(where H∞ is the min-entropy) so we only need to bound maxyS Pr[YS = yS |S = S] We will now, for any yS , upper bound Pr[YS = yS |S = S]. Recall that the event S = S implies that ∀i ∈ S, |(g1)i| ≤ cD. Since g2 is independent of g1, we know that for every fixed g ∈ Rn:\nPr[YS = yS |S = S, g1 = g] = Πi∈S Pr[sign([g]i + [g2]i) = yi] 5Note for a distribution over {−1, 1}n , the maximal entropy a distribution can have is n, which is achieved by the uniform distribution.\nFor a fixed i ∈ [S], consider the term Pr[sign([g]i + [g2]i) = yi]. Without loss of generality, let’s assume [g]i > 0 (the proof is completely symmetric in the other case). Then, since [g]i is positive and g2 has mean 0, we have Pr[[g]i + (g2)i < 0] ≤ 1\n2 .\nMoreover,\nPr [[g]i + [g2]i > 0] = Pr[[g2]i > 0] Pr [[g]i + [g2]i > 0 | [g2]i > 0] +Pr[[g2]i < 0] Pr [[g]i + [g2]i > 0 | [g2]i < 0]\nThe first term is upper bounded by 12 since Pr[[g2]i > 0] ≤ 12 . The second term we will bound using standard Gaussian tail bounds:\nPr [[g]i + [g2]i > 0 | [g2]i < 0] ≤ Pr [|[g2]i| ≤ |[g]i| | [g2]i < 0] = Pr[|[g2]i| ≤ |[g]i|] ≤ Pr[([g2]i)2 ≤ cD] = 1− Pr[([g2]i)2 > cD]\n≤ 1− 2√ 2π exp (−cD/2β)\n\n\n√\nβ cD − ( √ β cD\n)3 \n\nwhich implies\nPr[[g2]i < 0] Pr[[g]i + [g2]i > 0 | [g2]i < 0] ≤ 1\n2\n\n1− 2√ 2π exp (−cD/2β)\n\n\n√\nβ\ncD −\n( √\nβ\ncD\n)3 \n\n\n\nPutting together, we have\nPr[sign((g1)i + (g2)i) = yi] ≤ 1− 1√ 2π exp (−cD/2β)\n\n\n√\nβ cD − ( √ β cD\n)3 \n\nTogether with the fact that |S| ≥ γn we get\nPr[YS = yS |S = s, g1 = g] ≤\n\n1− 1√ 2π exp (−cD/2β)\n\n\n√\nβ cD − ( √ β cD\n)3 \n\n\n\nγn\nwhich implies that\nH(Y) ≥ − ( 1− 1 D ) (c− 1)n c log\n\n1− 1√ 2π exp (−cD/2β)\n\n\n√\nβ cD − ( √ β cD\n)3 \n\n\n\nBy setting c = D = 31/4 √ β and a straightforward (albeit unpleasant) calculation, we can check that H(Y) ≥\nn 25 (31/4 √ β−1)2√ 3β , as we need.\nWe next show that the moments of the distribution are preserved up to a constant G1+β .\nLemma 3.1. The distribution µ̃ has G1+βΣi,j ≤ Eµ̃[XiXj ] ≤ 11+βΣi,j\nProof. Consider the Gram decomposition of Σ̃i,j = 〈vi, vj〉. Then, N (0, Σ̃) is in distribution equal to (sign(〈v1, s〉), . . . , sign(〈vn, s〉)) where s ∼ N (0, I). Similarly as in the analysis of Goemans-Williamson (Goemans and Williamson, 1995), if v̄i = 1‖vi‖vi, we have G〈v̄i, v̄j〉 ≤ Eµ̃[XiXj] = 2 π arcsin(〈v̄i, v̄j〉) ≤ 〈v̄i, v̄j〉.\nHowever, since 〈v̄i, v̄j〉 = 1\n‖vi‖‖vj‖ 〈vi, vj〉 =\n1\n‖vi‖‖vj‖ Σ̃i,j =\n1\n‖vi‖‖vj‖ Σi,j and ‖vi‖ =\n√\nΣ̃i,i = √ 1 + β, ∀i ∈\n[1, n], we get that G\n1 + β Σi,j ≤ Eµ̃[XiXj ] ≤\n1\n1 + β Σi,j as we want.\nLemma 3.2 and 3.1 together imply Theorem 3.1."
    }, {
      "heading" : "4 Provable bounds for variational methods",
      "text" : "We will in this section consider applications of the approximate maximum entropy principles we developed for calculating partition functions of Ising models. Before we dive into the results, we give brief preliminaries on variational methods and pseudo-moment convex relaxations.\nPreliminaries on variational methods and pseudo-moment convex relaxations Recall, variational methods are based on the following simple lemma, which characterizes logZ as the solution of an optimization problem. It essentially dates back to Gibbs (Ellis, 2012), who used it in the context of statistical mechanics, though it has been rediscovered by machine learning researchers (Wainwright and Jordan, 2008):\nLemma 4.1 (Variational characterization of logZ). Let us denote by M the polytope of distributions over {−1, 1}n. Then,\nlogZ = max µ∈M\n{\n∑\nt\nJtEµ[φt(x)] +H(µ)\n}\n(1)\nWhile the above lemma reduces calculating logZ to an optimization problem, optimizing over the polytope M is impossible in polynomial time. We will proceed in a way which is natural for optimization problems – by instead optimizing over a relaxation M′ of that polytope.\nThe relaxation will be associated with the degree-2 Lasserre hierarchy. Intuitively, M′ has as variables tentative pairwise moments of a distribution of {−1, 1}n, and it imposes all constraints on the moments that hold for distributions over {−1, 1}n. To define M′ more precisely we will need the following notion: (for a more in-depth review of moment-based convex hierarchies, the reader can consult (Barak et al., 2014))\nDefinition 4.1. A degree-2 pseudo-moment 6 Ẽν [·] is a linear operator mapping polynomials of degree 2 to R, such that Ẽν [x2i ] = 1, and Ẽν [p(x) 2] ≥ 0 for any polynomial p(x) of degree 1.\nWe will be optimizing over the polytope M′ of all degree-2 pseudo-moments, i.e. we will consider solving\nmax Ẽν [·]∈M′\n{\n∑\nt\nJtẼν [φt(x)] + H̃(Ẽν [·]) }\nwhere H̃ will be a proxy for the entropy we will have to define (since entropy is a global property that depends on all moments, and Ẽν only contains information about second order moments).\nTo see this optimization problem is convex, we show that it can easily be written as a semidefinite program. Namely, note that the pseudo-moment operators are linear, so it suffices to define them over monomials only. Hence, the variables will simply be Ẽν(xS) for all monomials xS of degree at most 2. The constraints Ẽν [x2i ] = 1 then are clearly linear, as is the “energy part” of the objective function. So we only need to worry about the constraint Ẽν [p(x)\n2] ≥ 0 and the entropy functional. We claim the constraint Ẽν [p(x)2] ≥ 0 can be written as a PSD constraint: namely if we define the matrix Q, which is indexed by all the monomials of degree at most 1, and it satisfies Q(xS ,xT ) = Ẽν [xSxT ]. It is easy to see that Ẽν [p(x)2] ≥ 0 ≡ Q 0.\n6The reason Ẽν [·] is called a pseudo-moment, is that it behaves like the moments of a distribution ν : {−1, 1}n → [0, 1], albeit only over polynomials of degree at most 2.\nHence, the final concern is how to write an expression for the entropy in terms of the low-order moments, since entropy is a global property that depends on all moments. There are many candidates for this in machine learning are like Bethe/Kikuchi entropy, tree-reweighted Bethe entropy, log-determinant etc. However, in the worst case – none of them come with any guarantees. We will in fact show that the entropy functional is not an issue when we only care about worst case guarantees – we will relax the entropy trivially to an upper bound of n.\nGiven all of this, the final relaxation we will consider is:\nmax Ẽν [·]∈M′\n{\n∑\nt\nJtẼν [φt(x)] + n\n}\n(2)\nFrom the prior setup it is clear that the solution to (2) is an upper bound to logZ . To prove a claim like Theorem 2.3 or Theorem 2.4, we will then provide a rounding of the solution. In this instance, this will mean producing a distribution µ̃ which has the value of ∑\nt JtEµ̃[φt(x)] + H(µ̃) comparable to the value of the solution. Note this is slightly different than the usual requirement in optimization, where one cares only about producing a single x ∈ {−1, 1}n with comparable value to the solution. Our distribution µ̃ will have entropy Ω(n), and preserves the “energy” portion of the objective ∑\nt JtEµ[φt(x)] up to a comparable factor to what is achievable in the optimization setting.\nWarmup: exponential family analogue of MAX-CUT As a warmup, to illustrate the basic ideas behind the above rounding strategy, before we consider Ising models we consider the exponential family analogue of MAX-CUT. It is defined by the functionals φi,j(x) = (xi −xj)2. Concretely, we wish to approximate the partition function of the\ndistribution µ(x) ∝ exp\n\n\n∑\ni,j\nJi,j(xi − xj)2  . We will prove the following simple observation:\nObservation 4.1. The relaxation (2) provides a factor 2 approximation of logZ . Proof. We proceed as outlined in the previous section, by providing a rounding of (2). We point out again, unlike the standard case in optimization, where typically one needs to produce an assignment of the variables, because of the entropy term here it is crucial that the rounding produces a distribution.\nThe distribution µ̃ we produce here will be especially simple: we will round each xi independently with probability 1 2 . Then, clearly H(µ̃) = n. On the other hand, we similarly have Prµ̃[(xi − xj)2 = 1] = 12 , since xi and xj are rounded independently. Hence, Eµ̃[(xi − xj)2] ≥ 12 . Altogether, this implies ∑\ni,j Ji,jEµ̃[(xi − xj)2] + H(µ̃) ≥ 1 2 ( ∑ i,j Ji,jEν [(xi − xj)2] + n ) as we needed."
    }, {
      "heading" : "4.1 Ising models",
      "text" : "We proceed with the main results of this section on Ising models, which is the case where φi,j(x) = xixj . We will split into the ferromagnetic and general case separately, as outlined in Section 2.\nTo be concrete, we will be given potentials Ji,j , and we wish to calculate the partition function of the Ising model µ(x) ∝ exp(∑i,j Ji,jxixj).\nFerromagnetic case Recall, in the ferromagnetic case of Ising model, we have the conditions that the potentials Ji,j > 0. We will provide a convex relaxation which has a constant factor approximation in this case. First, recall the famous First Griffiths inequality due to Griffiths (Griffiths, 1967) which states that in the ferromagnetic case, Eµ[xixj ] ≥ 0, ∀i, j.\nUsing this inequality, we will look at the following natural strenghtening of the relaxation (2):\nmax Ẽν [·]∈M′;Ẽν [xixj ]≥0,∀i,j\n{\n∑\nt\nJtẼν [φt(x)] + n\n}\n(3)\nWe will prove the following theorem, as a straightforward implication of our claims from Section 3:\nTheorem 4.1. The relaxation (3) provides a factor 50 approximation of logZ .\nProof. Notice, due to Griffiths’ inequality, (3) is in fact a relaxation of the Gibbs variational principle and hence an upper bound)of logZ . Same as before, we will provide a rounding of (3). We will use the distribution µ̃ we designed in Section 3 the sign of a Gaussian with covariance matrix Σ + βI , for a β which we will specify. By Lemma 3.2, we then have H(µ̃) ≥ n25 (31/4 √ β−1)2√ 3β whenever β ≥ 1 31/2 . By Lemma 3.1, on the other hand, we can prove that Eµ̃[xixj ] ≥ G\n1 + β Ẽν [xixj ]\nBy setting β = 21.8202, we get n25 (31/4 √ β−1)2√ 3β ≥ 0.02 and G1+β ≥ 0.02, which implies that\n∑\ni,j\nJi,jEµ̃[xixj ] +H(µ̃) ≥ 0.02\n\n\n∑\ni,j\nJi,jẼν [xixj ] + n\n\n\nwhich implies the claim we want.\nNote that the above proof does not work in the general Ising model case: when Ẽν [xixj ] can be either positive or negative, even if we preserved each Ẽν [xixj ] up to a constant factor, this may not preserve the sum ∑\ni,j Ji,jẼν [xixj ] due to cancellations in that expression.\nGeneral Ising models case Finally, we will tackle the general Ising model case. As noted in the previous section, the straightforward application of the results proven in Section 3 doesn’t work, so we have to consider a different rounding – again inspired by roundings used in optimization.\nThe intuition is the same as in the ferromagnetic case: we wish to design a rounding which preserves the “energy” portion of the objective, while having a high entropy. In the previous section, this was achieved by modifying the Goemans-Williamson rounding so that it produces a high-entropy distribution. We will do a similar thing here, by modifying roundings due to (Charikar and Wirth, 2004) and (Alon et al., 2006).\nThe convex relaxation we will consider will just be the basic one (2), and we will prove the following two theorems:\nTheorem 4.2. The relaxation (2) provides a factor O(log n) approximation to logZ when φi,j(x) = xixj .\nTheorem 4.3. The relaxation (2) provides a factor O(log(χ(G))) approximation to logZ when φi,j(x) = xixj for i, j ∈ E(G) of some graph G = (V (G), E(G)), and χ(G) is the chromatic number of G.\nSince the chromatic number of a graph is bounded by n, the second theorem is in fact strictly stronger than the first, however the proof of the first theorem uses less heavy machinery, and is illuminating enough to be presented on its own.\nBefore delving into the proof of Theorem 4.2, we review the rounding used by (Charikar and Wirth, 2004) in the case of maximizing quadratic forms:\nAlgorithm 1 Quadratic form rounding by (Charikar and Wirth, 2004) 1: Input: A pseudo-moment matrix Σi,j = Eν [xixj ] 2: Output: A sample x from a distribution ρ 3: Sample g from the standard Gaussian N(0, I). 4: Consider the vector h, such that hi = gi/T, T = √ 4 logn\n5: Consider the vector r, such that ri = hi|hi| , if |hi| > 1, and ri = hi otherwise. 6: Produce the rounded vector x ∈ {−1, 1}n, s.t.\nxi =\n{\n+1, with probability 1+ri2 −1, with probability 1−ri2\n}\nAlgorithm 2 Scaled down quadratic form rounding 1: Input: A pseudo-moment matrix Σi,j = Eν [xixj ] 2: Output: A sample x from a distribution µ̃ 3: Sample g from the standard Gaussian N(0, I). 4: Consider the vector h, such that hi = gi/T, T = √ 4 logn\n5: Consider the vector r, such that r′i = 1 2 hi |hi| , if |hi| > 1, and r ′ i = 1 2hi otherwise. 6: Produce the rounded vector x ∈ {−1, 1}n, s.t.\nxi =\n{\n+1, with probability 1+ri2 −1, with probability 1−ri2\n}\nWith that in hand, we can prove Theorem 4.2\nProof of Theorem 4.2. The proof again consists of exhibiting a rounding. Our rounding will essentially be the same as (Charikar and Wirth, 2004), except in step 3, we will produce a vector r′i by scaling down the vector ri by 2 coordinate-wise. For full clarity, the rounding is presented in Algorithm 2.\nWe again, need to analyze the entropy and the moments of the distribution µ̃ that this rounding produces. Let us focus on the entropy first.\nSince conditioning does not decrease entropy, it’s true that H(µ̃) = H(x) ≥ H(x|r), so it suffices to lower bound that quantity. However, note that it holds that ri ≤ 12 , and each xi is rounded independently conditional on ri, so we have:\nH(x|r) = ∑\ni\nH(xi|ri) = ∑\ni\n(\n1 + ri 2 log\n(\n1 + ri 2\n) + 1− ri 2 ( 1− ri 2 )) ≥ ( 2− 3 4 log 3 ) n\nConsider now the moments of the distribution. Let us denote the distribution that the rounding 1 produces by ρ. By Theorem 1 in (Charikar and Wirth, 2004), we\nhave ∑\ni,j\nJi,jEρ[xixj ] ≥ O ( 1\nlogn\n)\n∑\ni,j\nJi,jEν [xixj ]\nAdditional, both our and the (Charikar and Wirth, 2004) roundings are such that Eρ[xixj ] = ErEx|r[xixj ] and Eµ̃[xixj ] = Er′Ex|r′ [xixj ]. Furthermore, as noted in (Charikar and Wirth, 2004), it is easy to check that E[xixj |r′] = r′ir ′ j and obviously r ′ i = 2ri, ∀i in distribution, so we have:\nEµ̃[xixj ] = Er′Ex|r′ [xixj ] = 1\n4 ErEx|r[xixj ] =\n1 4 Eρ[xixj ]\nBut, this directly implies\n∑\ni,j\nJi,jEµ̃[xixj ] = 1\n4\n∑\ni,j\nJi,jEρ[xixj ] ≥ O ( 1\nlogn\n)\n∑\ni,j\nJi,jEν [xixj ]\nas we needed.\nNext, we prove the more general Theorem 4.3. Before proceeding, let’s recall for completeness the following definition of a chromatic number.\nDefinition 4.2 (Chromatic number). The chromatic number χ(G) of a graph G = (V (G), E(G)) is defined as the minimum number of colors in a coloring of the vertices V (G), such that no vertices i, j : (i, j) ∈ E(G) are colored with the same color.\nAlso, let us denote by Sn−1 the set of unit vectors in Rn and L∞[0, 1] the set of (essentially) bounded functions: the functions which are bounded except on a set of measure zero.\nThen, we can recall Theorem 3.3 from (Alon et al., 2006):\nTheorem 4.4 ((Alon et al., 2006)). There exists an absolute constant c such that the following holds: Let G = (V (G), E(G)) be an undirected graph on n vertices without self-loops7, let χ(G) be the chromatic number of G. Then for every function f : V (G) → Sn−1, there exists a function F : V → L∞[0, 1] so that for every i ∈ V (G), ‖F (i)‖∞ ≤ √ cχ(G) and for every (i, j) ∈ E(G),\n〈f(i), f(j)〉 = ∫ 1\n0\nF (i)(t)F (j)(t)dt\nNow, we can prove Theorem 4.3\nProof of Theorem 4.3. The proof is similar, though a little more complicated than the proof of Theorem 4.2. Let Ẽν [·] be the solution of the relaxation. By matrix formulation of the pseudo-moment relaxation in Section 4 , we know that Ẽν [xixj ] = 〈f(i), f(j)〉 for some unit vectors f(i), f(j). Hence, by theorem 4.4, there exists a functionF : V → L∞[0, 1] so that for every i ∈ V (G), ‖F (i)‖∞ ≤ √\ncχ(G) and for every (i, j) ∈ E(G),\nẼν [xixj ] =\n∫ 1\n0\nF (i)(t)F (j)(t)dt\nConsider the following rounding:\n• Pick a t uniformly at random from [0, 1].\n• Consider the function ht : V → R, such that ht(i) = F (i)(t) 2 √ cχ(G)\n• Produce the rounded vector x ∈ {−1, 1}V (G), s.t.\nxi =\n{\n+1, with probability 1+ht(i)2 −1, with probability 1−ht(i)2\n}\nNote importantly that the algorithm does not need to perform this rounding – it is for the analysis of the approximation factor of the relaxation. Therefore, we need not construct it algorithmically.\nLet us denote this distribution as µ̃. We first show that µ̃ has entropy at least ( 2− 34 log 3 )\nn. Note that each xi are round independently conditional on t. Moreover, since ‖F (v)‖∞ ≤ √\ncχ(G), we know that ht(v) ≤ 12 . Therefore, for every fixed t0 ∈ [0, 1]\nH(µ̃ | t = t0) = ∑ i∈V (G) H(xi | t = t0)\n= ∑\ni∈V (G)\n(\n1 + ht0(v)\n2 log\n1 + ht0(v)\n2 + 1− ht0(v) 2 log 1− ht0(v) 2 )\n≥ ( 2− 3 4 log 3 ) n\nIntegrating over t0 we get that H(µ̃) ≥ ( 2− 34 log 3 )\nn. Next, we will show that µ̃ preserves the “energy” part of the objective up to a multiplicative factor O(logχ(G)):\nConsider each edge (i, j) ∈ E(G). We have:\nEµ̃[xixj ] =\n7Meaning no edge connects a vertex with itself\n∫ 1\n0\n(\n(1 + ht(i))(1 + ht(j))\n4 + (1− ht(i))(1 − ht(j)) 4 − (1 + ht(i))(1− ht(j)) 4 − (1− ht(i))(1 + ht(j)) 4 ) dt\n=\n∫ 1\n0\nht(i)ht(j)dt = 1\n4cχ(G)\n∫ 1\n0\nF (i)(t)F (j)(t)dt = 1\n4cχ(G) Ẽν [xixj ]\nThis implies that ∑\ni,j∈E(G) Ji,jEµ̃[xixj ] ≥\n1\n4cχ(G)\n∑\ni,j∈E(G) Ji,jẼν [xixj ]\nTherefore, the relaxation provides a factor O(χ(G)) approximation of logZ , as we wanted."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In summary, we presented computationally efficient approximate versions of the classical max-entropy principle by (Jaynes, 1957): efficiently sampleable distributions which preserve given pairwise moments up to a multiplicative constant factor, while having entropy within a constant factor of the maximum entropy distribution matching those moments. Additionally, we applied our insights to designing provable variational methods for Ising models which provide comparable guarantees for approximating the log-partition function to those in the optimization setting. Our methods are based on convex relaxations of the standard variational principle due to Gibbs, and are extremely generic and we hope they will find applications for other exponential families."
    } ],
    "references" : [ {
      "title" : "Approximating the cut-norm via grothendieck’s inequality",
      "author" : [ "Noga Alon", "Assaf Naor" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Alon and Naor.,? \\Q2006\\E",
      "shortCiteRegEx" : "Alon and Naor.",
      "year" : 2006
    }, {
      "title" : "Rounding sum-of-squares relaxations",
      "author" : [ "Boaz Barak", "Jonathan A Kelner", "David Steurer" ],
      "venue" : "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Barak et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Barak et al\\.",
      "year" : 2014
    }, {
      "title" : "Near-maximum entropy models for binary neural representations of natural images",
      "author" : [ "Matthias Bethge", "Philipp Berens" ],
      "venue" : null,
      "citeRegEx" : "Bethge and Berens.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bethge and Berens.",
      "year" : 2007
    }, {
      "title" : "Hardness of parameter estimation in graphical models",
      "author" : [ "Guy Bresler", "David Gamarnik", "Devavrat Shah" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bresler et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bresler et al\\.",
      "year" : 2014
    }, {
      "title" : "Maximizing quadratic programs: extending grothendieck’s inequality",
      "author" : [ "Moses Charikar", "Anthony Wirth" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "Charikar and Wirth.,? \\Q2004\\E",
      "shortCiteRegEx" : "Charikar and Wirth.",
      "year" : 2004
    }, {
      "title" : "Entropy, large deviations, and statistical mechanics, volume 271",
      "author" : [ "Richard S Ellis" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Ellis.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ellis.",
      "year" : 2012
    }, {
      "title" : "The statistics of curie-weiss models",
      "author" : [ "Richard S Ellis", "Charles M Newman" ],
      "venue" : "Journal of Statistical Physics,",
      "citeRegEx" : "Ellis and Newman.,? \\Q1978\\E",
      "shortCiteRegEx" : "Ellis and Newman.",
      "year" : 1978
    }, {
      "title" : "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming",
      "author" : [ "Michel X Goemans", "David P Williamson" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Goemans and Williamson.,? \\Q1995\\E",
      "shortCiteRegEx" : "Goemans and Williamson.",
      "year" : 1995
    }, {
      "title" : "Correlations in ising ferromagnets",
      "author" : [ "Robert B Griffiths" ],
      "venue" : "i. Journal of Mathematical Physics,",
      "citeRegEx" : "Griffiths.,? \\Q1967\\E",
      "shortCiteRegEx" : "Griffiths.",
      "year" : 1967
    }, {
      "title" : "Information theory and statistical mechanics",
      "author" : [ "Edwin T Jaynes" ],
      "venue" : "Physical review,",
      "citeRegEx" : "Jaynes.,? \\Q1957\\E",
      "shortCiteRegEx" : "Jaynes.",
      "year" : 1957
    }, {
      "title" : "Polynomial-time approximation algorithms for the ising model",
      "author" : [ "Mark Jerrum", "Alistair Sinclair" ],
      "venue" : "SIAM Journal on computing,",
      "citeRegEx" : "Jerrum and Sinclair.,? \\Q1993\\E",
      "shortCiteRegEx" : "Jerrum and Sinclair.",
      "year" : 1993
    }, {
      "title" : "How to compute partition functions using convex programming hierarchies: provable bounds for variational methods",
      "author" : [ "Andrej Risteski" ],
      "venue" : "In Proceedings of the Conference on Learning Theory (COLT),",
      "citeRegEx" : "Risteski.,? \\Q2016\\E",
      "shortCiteRegEx" : "Risteski.",
      "year" : 2016
    }, {
      "title" : "Entropy, optimization and counting",
      "author" : [ "Mohit Singh", "Nisheeth K Vishnoi" ],
      "venue" : "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Singh and Vishnoi.,? \\Q2014\\E",
      "shortCiteRegEx" : "Singh and Vishnoi.",
      "year" : 2014
    }, {
      "title" : "The computational hardness of counting in two-spin models on d-regular graphs",
      "author" : [ "Allan Sly", "Nike Sun" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Sly and Sun.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sly and Sun.",
      "year" : 2012
    }, {
      "title" : "Tree-reweighted belief propagation algorithms and approximate ml estimation by pseudo-moment",
      "author" : [ "Martin J Wainwright", "Tommi S Jaakkola", "Alan S Willsky" ],
      "venue" : null,
      "citeRegEx" : "Wainwright et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Wainwright et al\\.",
      "year" : 2003
    }, {
      "title" : "A new class of upper bounds on the log partition function",
      "author" : [ "Martin J Wainwright", "Tommi S Jaakkola", "Alan S Willsky" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Wainwright et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Wainwright et al\\.",
      "year" : 2005
    }, {
      "title" : "Correctness of local probability propagation in graphical models with loops",
      "author" : [ "Yair Weiss" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Weiss.,? \\Q2000\\E",
      "shortCiteRegEx" : "Weiss.",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Unfortunately, calculating the potentials in the maximum-entropy distribution is intractable (Bresler et al., 2014).",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "(Alon and Naor, 2006)",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "1 Introduction Maximum entropy principle The maximum entropy principle (Jaynes, 1957) states that given mean parameters, i.",
      "startOffset" : 71,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "Due to results of (Bresler et al., 2014; Singh and Vishnoi, 2014), the potentials Jt of the Ising model, in many cases, are impossible to estimate well in polynomial time, unless NP = RP – so merely getting the description of the maximum entropy distribution is already hard.",
      "startOffset" : 18,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "Due to results of (Bresler et al., 2014; Singh and Vishnoi, 2014), the potentials Jt of the Ising model, in many cases, are impossible to estimate well in polynomial time, unless NP = RP – so merely getting the description of the maximum entropy distribution is already hard.",
      "startOffset" : 18,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "This provides theoretical explanation for the phenomenon observed by the computational neuroscience community (Bethge and Berens, 2007) that this distribution (named dichotomized Gaussian there) has near-maximum entropy.",
      "startOffset" : 110,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "(Wainwright et al., 2003; 2005; Weiss, 2000)).",
      "startOffset" : 0,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "(Wainwright et al., 2003; 2005; Weiss, 2000)).",
      "startOffset" : 0,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "In the graphical model community, these are known as Ising models, and in the optimization community this is the problem of optimizing quadratic forms and has been studied by (Charikar and Wirth, 2004; Alon and Naor, 2006; Alon et al., 2006).",
      "startOffset" : 175,
      "endOffset" : 241
    }, {
      "referenceID" : 0,
      "context" : "In the graphical model community, these are known as Ising models, and in the optimization community this is the problem of optimizing quadratic forms and has been studied by (Charikar and Wirth, 2004; Alon and Naor, 2006; Alon et al., 2006).",
      "startOffset" : 175,
      "endOffset" : 241
    }, {
      "referenceID" : 13,
      "context" : "construct a FPRAS) (Sly and Sun, 2012), but nothing is known about coarser approximations.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "(Bresler et al., 2014) considers the “hard-core” model where the functionals φt are such that the distribution μ(x) puts zero mass on configurations x which are not independent sets with respect to some graph G.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : "(Singh and Vishnoi, 2014) prove an equivalence between calculating the mean parameters and calculating partition functions.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "(Ellis, 2012) As outlined before, we will be concerned with a particularly popular exponential family: Ising models.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 11,
      "context" : "Moreover, other than a recent paper by (Risteski, 2016), no other work has provided provable bounds for variational methods that proceed via a convex relaxation and a rounding thereof.",
      "startOffset" : 39,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "3 (Risteski, 2016) provides guarantees in the case of Ising models that are also based on pseudo-moment relaxations of the variational principle, albeit only in the special case when the graph is “dense” in a suitably defined sense.",
      "startOffset" : 2,
      "endOffset" : 18
    }, {
      "referenceID" : 10,
      "context" : "Finally, we mention that in the special case of the ferromagnetic Ising models, an algorithm based on MCMC was provided by (Jerrum and Sinclair, 1993), which can give an approximation factor of (1 + ǫ) to the partition function and runs in time O(npoly(1/ǫ)).",
      "startOffset" : 123,
      "endOffset" : 150
    }, {
      "referenceID" : 7,
      "context" : "(Goemans and Williamson, 1995) We will prove Theorem 3.",
      "startOffset" : 0,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "Similarly as in the analysis of Goemans-Williamson (Goemans and Williamson, 1995), if v̄i = 1 ‖vi‖vi, we have G〈v̄i, v̄j〉 ≤ Eμ̃[XiXj] = 2 π arcsin(〈v̄i, v̄j〉) ≤ 〈v̄i, v̄j〉.",
      "startOffset" : 51,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : "It essentially dates back to Gibbs (Ellis, 2012), who used it in the context of statistical mechanics, though it has been rediscovered by machine learning researchers (Wainwright and Jordan, 2008): Lemma 4.",
      "startOffset" : 35,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "To define M′ more precisely we will need the following notion: (for a more in-depth review of moment-based convex hierarchies, the reader can consult (Barak et al., 2014)) Definition 4.",
      "startOffset" : 150,
      "endOffset" : 170
    }, {
      "referenceID" : 8,
      "context" : "First, recall the famous First Griffiths inequality due to Griffiths (Griffiths, 1967) which states that in the ferromagnetic case, Eμ[xixj ] ≥ 0, ∀i, j.",
      "startOffset" : 69,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "We will do a similar thing here, by modifying roundings due to (Charikar and Wirth, 2004) and (Alon et al.",
      "startOffset" : 63,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "2, we review the rounding used by (Charikar and Wirth, 2004) in the case of maximizing quadratic forms: Algorithm 1 Quadratic form rounding by (Charikar and Wirth, 2004) 1: Input: A pseudo-moment matrix Σi,j = Eν [xixj ] 2: Output: A sample x from a distribution ρ 3: Sample g from the standard Gaussian N(0, I).",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 4,
      "context" : "2, we review the rounding used by (Charikar and Wirth, 2004) in the case of maximizing quadratic forms: Algorithm 1 Quadratic form rounding by (Charikar and Wirth, 2004) 1: Input: A pseudo-moment matrix Σi,j = Eν [xixj ] 2: Output: A sample x from a distribution ρ 3: Sample g from the standard Gaussian N(0, I).",
      "startOffset" : 143,
      "endOffset" : 169
    }, {
      "referenceID" : 4,
      "context" : "Our rounding will essentially be the same as (Charikar and Wirth, 2004), except in step 3, we will produce a vector r′ i by scaling down the vector ri by 2 coordinate-wise.",
      "startOffset" : 45,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "By Theorem 1 in (Charikar and Wirth, 2004), we have",
      "startOffset" : 16,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "i,j Ji,jEν [xixj ] Additional, both our and the (Charikar and Wirth, 2004) roundings are such that Eρ[xixj ] = ErEx|r[xixj ] and Eμ̃[xixj ] = Er′Ex|r′ [xixj ].",
      "startOffset" : 48,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "Furthermore, as noted in (Charikar and Wirth, 2004), it is easy to check that E[xixj |r′] = r′ ir ′ j and obviously r ′ i = 2ri, ∀i in distribution, so we have: Eμ̃[xixj ] = Er′Ex|r′ [xixj ] = 1 4 ErEx|r[xixj ] = 1 4 Eρ[xixj ] But, this directly implies",
      "startOffset" : 25,
      "endOffset" : 51
    } ],
    "year" : 2016,
    "abstractText" : "The well known maximum-entropy principle due to Jaynes, which states that given mean parameters, the maximum entropy distribution matching them is in an exponential family, has been very popular in machine learning due to its “Occam’s razor” interpretation. Unfortunately, calculating the potentials in the maximum-entropy distribution is intractable (Bresler et al., 2014). We provide computationally efficient versions of this principle when the mean parameters are pairwise moments: we design distributions that approximately match given pairwise moments, while having entropy which is comparable to the maximum entropy distribution matching those moments. We additionally provide surprising applications of the approximate maximum entropy principle to designing provable variational methods for partition function calculations for Ising models without any assumptions on the potentials of the model. More precisely, we show that in every temperature, we can get approximation guarantees for the log-partition function comparable to those in the low-temperature limit, which is the setting of optimization of quadratic forms over the hypercube. (Alon and Naor, 2006)",
    "creator" : "LaTeX with hyperref package"
  }
}