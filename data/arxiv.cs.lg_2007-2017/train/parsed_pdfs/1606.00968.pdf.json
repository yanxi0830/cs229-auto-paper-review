{
  "name" : "1606.00968.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Smooth Imitation Learning for Online Sequence Prediction",
    "authors" : [ "Hoang M. Le", "Andrew Kang", "Yisong Yue", "Peter Carr" ],
    "emails" : [ "HMLE@CALTECH.EDU", "AKANG@CALTECH.EDU", "YYUE@CALTECH.EDU", "PETER.CARR@DISNEYRESEARCH.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In many complex planning and control tasks, it can be very challenging to explicitly specify a good policy. For such tasks, the use of machine learning to automatically learn a good policy from observed expert behavior, also known as imitation learning or learning from demonstrations, has proven tremendously useful (Abbeel & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).\nIn this paper, we study the problem of imitation learning for smooth online sequence prediction in a continuous regime.\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nOnline sequence prediction is the problem of making online decisions in response to exogenous input from the environment, and is a special case of reinforcement learning (see Section 2). We are further interested in policies that make smooth predictions in a continuous action space.\nOur motivating example is the problem of learning smooth policies for automated camera planning (Chen et al., 2016): determining where a camera should look given environment information (e.g., noisy person detections) and corresponding demonstrations from a human expert.1 It is widely accepted that a smoothly moving camera is essential for generating aesthetic video (Gaddam et al., 2015). From a problem formulation standpoint, one key difference between smooth imitation learning and conventional imitation learning is the use of a “smooth” policy class (which we formalize in Section 2), and the goal now is to mimic expert demonstrations by choosing the best smooth policy.\nThe conventional supervised learning approach to imitation learning is to train a classifier or regressor to predict the expert’s behavior given training data comprising input/output pairs of contexts and actions taken by the expert. However, the learned policy’s prediction affects (the distribution of) future states during the policy’s actual execution, and so violates the crucial i.i.d. assumption made by most statistical learning approaches. To address this issue, numerous learning reduction approaches have been proposed (Daumé III et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011), which iteratively modify the training distribution in various ways such that any supervised learning guarantees provably lift to the sequential imitation setting (potentially at the cost of statistical or computational efficiency).\nWe present a learning reduction approach to smooth imitation learning for online sequence prediction, which we call SIMILE (Smooth IMItation LEarning). Building\n1Access data at http://www.disneyresearch.com/ publication/smooth-imitation-learning/ and code at http://github.com/hoangminhle/SIMILE.\nar X\niv :1\n60 6.\n00 96\n8v 1\n[ cs\n.L G\n] 3\nJ un\n2 01\nupon learning reductions that employ policy aggregation (Daumé III et al., 2009), we provably lift supervised learning guarantees to the smooth imitation setting and show much faster convergence behavior compared to previous work. Our contributions can be summarized as: • We formalize the problem of smooth imitation learn-\ning for online sequence prediction, and introduce a family of smooth policy classes that is amenable to supervised learning reductions. • We present a principled learning reduction approach, which we call SIMILE. Our approach enjoys several attractive practical properties, including learning a fully deterministic stationary policy (as opposed to SEARN (Daumé III et al., 2009)), and not requiring data aggregation (as opposed to DAgger (Ross et al., 2011)) which can lead to super-linear training time. • We provide performance guarantees that lift the the underlying supervised learning guarantees to the smooth imitation setting. Our guarantees hold in the agnostic setting, i.e., when the supervised learner might not achieve perfect prediction. • We show how to exploit a stability property of our smooth policy class to enable adaptive learning rates that yield provably much faster convergence compared to SEARN (Daumé III et al., 2009). • We empirically evaluate using the setting of smooth camera planning (Chen et al., 2016), and demonstrate the performance gains of our approach."
    }, {
      "heading" : "2. Problem Formulation",
      "text" : "Let X “ tx1, . . . , xT u Ă X T denote a context sequence from the environment X , and A “ ta1, . . . , aT u Ă AT denote an action sequence from some action space A. Context sequence is exogenous, meaning at does not influence future context xt`k for k ě 1. Let Π denote a policy class, where each π P Π generates an action sequence A in response to a context sequence X. Assume X Ă Rm,A Ă Rk are continuous and infinite, with A non-negative and bounded such that ~0 ĺ a ĺ R~1 @a P A. Predicting actions at may depend on recent contexts xt, . . . , xt´p and actions at´1, . . . , at´q . Without loss of generality, we define a state space S as tst “ rxt, at´1su.2 Policies π can thus be viewed as mapping states S “ XˆA to actions A. A roll-out of π given context sequence X “ tx1, . . . , xT u is the action sequence A “ ta1, . . . , aT u:\nat “ πpstq “ πprxt, at´1sq, st`1 “ rxt`1, ats @t P r1, . . . , T s .\nNote that unlike the general reinforcement learning problem, we consider the setting where the state space splits into external and internal components (by definition, at influences subsequent states st`k, but not xt`k). The use\n2We can always concatenate consecutive contexts and actions.\nof exogenous contexts txtu models settings where a policy needs to take online, sequential actions based on external environmental inputs, e.g. smooth self-driving vehicles for obstacle avoidance, helicopter aerobatics in the presence of turbulence, or smart grid management for external energy demand. The technical motivation of this dichotomy is that we will enforce smoothness only on the internal state.\nConsider the example of autonomous camera planning for broadcasting a sport event (Chen et al., 2016). X can correspond to game information such as the locations of the players, the ball, etc., and A can correspond to the pantilt-zoom configuration of the broadcast camera. Manually specifying a good camera policy can be very challenging due to sheer complexity involved with mapping X to A. It is much more natural to train π P Π to mimic observed expert demonstrations. For instance, Π can be the space of neural networks or tree-based ensembles (or both).\nFollowing the basic setup from (Ross et al., 2011), for any policy π P Π, let dπt denote the distribution of states at time t if π is executed for the first t´1 time steps. Furthermore, let dπ “ 1T řT t“1 d π t be the average distribution of states if we follow π for all T steps. The goal of imitation learning is to find a policy π̂ P Π which minimizes the imitation loss under its own induced distribution of states: π̂ “ argmin\nπPΠ `πpπq “ argmin πPΠ Es„dπ r`pπpsqqs , (1)\nwhere the (convex) imitation loss `pπpsqq captures how well π imitates expert demonstrations for state s. One common ` is squared loss between the policy’s decision and the expert demonstration: `pπpsqq “ }πpsq´π˚psq}2 for some norm }.}. Note that computing ` typically requires having access to a training set of expert demonstrations π˚ on some set of context sequences. We also assume an agnostic setting, where the minimizer of (1) does not necessarily achieve 0 loss (i.e. it cannot perfectly imitate the expert)."
    }, {
      "heading" : "2.1. Smooth Imitation Learning & Smooth Policy Class",
      "text" : "In addition to accuracy, a key requirement of many continuous control and planning problems is smoothness (e.g., smooth camera trajectories). Generally, “smoothness” may reflect domain knowledge about stability properties or approximate equilibria of a dynamical system. We thus formalize the problem of smooth imitation learning as minimizing (1) over a smooth policy class Π.\nMost previous work on learning smooth policies focused on simple policy classes such as linear models (Abbeel & Ng, 2004), which can be overly restrictive. We instead define a much more general smooth policy class Π as a regularized space of complex models.\nDefinition 2.1 (Smooth policy class Π). Given a complex model class F and a class of smooth regularizers H, we define smooth policy class Π Ă FˆH as satisfying:\nΠ fi tπ “ pf, hq,f P F , h P H | πpsq is close to both fpx, aq and hpaq @ induced state s “ rx, as P Su\nwhere closeness is controlled by regularization. For instance, F can be the space of neural networks or decision trees and H be the space of smooth analytic functions. Π can thus be viewed as policies that predict close to some f P F but are regularized to be close to some h P H. For sufficiently expressive F , we often have that Π Ă F . Thus optimizing over Π can be viewed as constrained optimization over F (by H), which can be challenging. Our SIMILE approach integrates alternating optimization (between F and H) into the learning reduction. We provide two concrete examples of Π below.\nExample 2.1 (Πλ). Let F be any complex supervised model class, and define the simplest possibleH fi thpaq “ au. Given f P F , the prediction of a policy π can be viewed as regularized optimization over the action space to ensure closeness of π to both f and h:\nπpx, aq “ argmin a1PA\n› ›fpx, aq ´ a1 › › 2 ` λ › ›hpaq ´ a1 › › 2\n“ fpx, aq ` λhpaq 1` λ “ fpx, aq ` λa 1` λ , (2)\nwhere regularization parameter λ trades-off closeness to f and to previous action. For large λ, πpx, aq is encouraged make predictions that stays close to previous action a.\nExample 2.2 (Linear auto-regressor smooth regularizers). Let F be any complex supervised model class, and define H using linear auto-regressors,H fi thpaq “ θJau, which model actions as a linear dynamical system (Wold, 1939). We can define π analogously to (2).\nIn general, SIMILE requires that Π satisfies a smooth property stated below. This property, which is exploited in our theoretical analysis (see Section 5), is motivated by the observation that given a (near) constant stream of context sequence, a stable behavior policy should exhibit a corresponding action sequence with low curvature. The two examples above satisfy this property for sufficiently large λ.\nDefinition 2.2 (H-state-smooth imitation policy). For small constant 0 ă H ! 1, a policy πprx, asq is Hstate-smooth if it is H-smooth w.r.t. a, i.e. for fixed x P X , @a, a1 P A, @i: › ›∇πiprx, asq ´∇πiprx, a1sq › ›\n˚ ď H }a´ a1} where πi indicates the ith component of vector-valued function3 πpsq “ “ π1psq, . . . , πkpsq ‰\nP Rk, and }.} and }.}˚ are some norm and dual norm respectively. For twice differentiable policy π, this is equivalent to having the bound on the Hessian∇2πiprx, asq ĺ HIk @i.\n3This emphasizes the possibility that π is a vector-valued function of a. The gradient and Hessian are viewed as arrays of k gradient vectors and Hessian matrices of 1-d case, since we simply treat action in Rk as an array of k standard functions."
    }, {
      "heading" : "3. Related Work",
      "text" : "The most popular traditional approaches for learning from expert demonstration focused on using approximate policy iteration techniques in the MDP setting (Kakade & Langford, 2002; Bagnell et al., 2003). Most prior approaches operate in discrete and finite action space (He et al., 2012; Ratliff et al., 2009; Abbeel & Ng, 2004; Argall et al., 2009). Some focus on continuous state space (Abbeel & Ng, 2005), but requires a linear model for the system dynamics. In contrast, we focus on learning complex smooth functions within continuous action and state spaces. One natural approach to tackle the more general setting is to reduce imitation learning to a standard supervised learning problem (Syed & Schapire, 2010; Langford & Zadrozny, 2005; Lagoudakis & Parr, 2003). However, standard supervised methods assume i.i.d. training and test examples, thus ignoring the distribution mismatch between training and rolled-out trajectories directly applied to sequential learning problems (Kakade & Langford, 2002). Thus a naive supervised learning approach normally leads to unsatisfactory results (Ross & Bagnell, 2010). Iterative Learning Reductions. State-of-the-art learning reductions for imitation learning typically take an iterative approach, where each training round uses standard supervised learning to learn a policy (Daumé III et al., 2009; Ross et al., 2011). In each round n, the following happens: • Given initial state s0 drawn from the starting distribu-\ntion of states, the learner executes current policy πn, resulting in a sequence of states sn1 , . . . , s n T . • For each snt , a label pant (e.g., expert feedback) is collected indicating what the expert would do given snt , resulting in a new dataset Dn “ tpst,pant qu. • The learner integrates Dn to learn a policy π̂n. The learner updates the current policy to πn`1 based on π̂n and πn.\nThe main challenge is controlling for the cascading errors caused by the changing dynamics of the system, i.e., the distribution of states in each Dn „ dπn . A policy trained using dπn induces a different distribution of states than dπn , and so is no longer being evaluated on the same distribution as during training. A principled reduction should (approximately) preserve the i.i.d. relationship between training and test examples. Furthermore the state distribution dπ should converge to a stationary distribution.\nThe arguably most notable learning reduction approaches for imitation learning are SEARN (Daumé III et al., 2009) and DAgger (Ross et al., 2011). At each round, SEARN learns a new policy π̂n and returns a distribution (or mixture) over previously learned policies: πn`1 “ βπ̂n`p1´ βqπn for β P p0, 1q. For appropriately small choices of β, this stochastic mixing limits the “distribution drift” between πn and πn`1 and can provably guarantee that the\nperformance of πn`1 does not degrage significantly relative to the expert demonstrations.4\nDAgger, on the other hand, achieves stability by aggregating a new dataset at each round to learn a new policy from the combined data set D Ð D Y Dn. This aggregation, however, significantly increases the computational complexity and thus is not practical for large problems that require many iterations of learning (since the training time grows super-linearly w.r.t. the number of iterations).\nBoth SEARN and DAgger showed that only a polynomial number of training rounds is required for convergence to a good policy, but with a dependence on the length of horizon T . In particular, to non-trivially bound the total variation distance }dπnew ´ dπold}1 of the state distributions between old and new policies, a learning rate β ă 1T is required to hold (Lemma 1 of Daumé III, Langford, and Marcu (2009) and Theorem 4.1 of Ross, Gordon, and Bagnell (2011)). As such, systems with very large time horizons might suffer from very slow convergence. Our Contributions. Within the context of previous work, our SIMILE approach can be viewed as extending SEARN to smooth policy classes with the following improvements:\n• We provide a policy improvement bound that does not depend on the time horizon T , and can thus converge much faster. In addition, SIMILE has adaptive learning rate, which can further improve convergence. • For the smooth policy class described in Section 2, we show how to generate simulated or “virtual” expert feedback in order to guarantee stable learning. This alleviates the need to have continuous access to a dynamic oracle / expert that shows the learner what to do when it is off-track. In this regard, the way SIMILE integrates expert feedback subsumes the set-up from SEARN and DAgger. • Unlike SEARN, SIMILE returns fully deterministic policies. Under the continuous setting, deterministic policies are strictly better than stochastic policies as (i) smoothness is critical and (ii) policy sampling requires holding more data during training, which may not be practical for infinite state and action spaces. • Our theoretical analysis reveals a new sequential prediction setting that yields provably fast convergence, in particular for smooth policy classes on finitehorizon problems. Existing settings that enjoy such results are limited to Markovian dynamics with discounted future rewards or linear model classes."
    }, {
      "heading" : "4. Smooth Imitation Learning Algorithm",
      "text" : "Our learning algorithm, called SIMILE (Smooth IMItation LEarning), is described in Algorithm 1. At a high level, the process can be described as:\n4A similar approach was adopted in Conservative Policy Iteration for the MDP setting (Kakade & Langford, 2002).\nAlgorithm 1 SIMILE (Smooth IMItation LEarning) Input: features X “ txtu, human trajectory A˚ “ ta˚t u,\nbase routine Train, smooth regularizers h P H 1: Initialize A0 Ð A˚,S0 Ð t “ xt, a ˚ t´1 ‰ u,\nh0 “ argmin hPH\nT ř\nt“1\n› ›a˚t ´ hpa˚t´1q › ›\n2: Initial policy π0 “ π̂0 Ð TrainpS0,A0| h0q 3: for n “ 1, . . . , N do 4: An “ tant u Ð πn´1pSn´1q //sequential roll-out 5: Sn Ð tsnt “ “ xt, a n t´1 ‰\nu //snt “ rxt:t´p, at´1:t´qs 6: pAn “ tpant u @snt P Sn // collect smooth feedback\n7: hn “ argmin hPH\nT ř\nt“1\n› › pant ´ hppant´1q › › //new regularizer\n8: π̂n Ð TrainpSn, pAn| hnq // train policy 9: β Ð βp`pπ̂nq, `pπn´1qq //adaptively set β 10: πn “ βπ̂n ` p1´ βqπn´1 // update policy 11: end for output Last policy πN\n1. Start with some initial policy π̂0 (Line 2). 2. At iteration n, use πn´1 to build a new state distribu-\ntion Sn and dataset Dn “ tpsnt ,pant qu (Lines 4-6). 3. Train π̂n “ argminπPΠ Es„Sn r`npπpsqqs, where `n\nis the imitation loss (Lines 7-8). Note that `n needs not be the original `, but simply needs to converge to it. 4. Interpolate π̂n and πn´1 to generate a new deterministic policy πn (Lines 9-10). Repeat from Step 2 with nÐ n` 1 until some termination condition is met.\nSupervised Learning Reduction. The actual reduction is in Lines 7-8, where we follow a two-step procedure of first updating the smooth regularize hn, and then training π̂n via supervised learning. In other words, Train finds the best f P F possible for a fixed hn. We discuss how to set the training targets pant below. Policy Update. The new policy πn is a deterministic interpolation between the previous πn´1 and the newly learned π̂n (Line 10). In contrast, for SEARN, πn is a stochastic interploation (Daumé III et al., 2009). Lemma 5.2 and Corollary 5.3 show that deterministic interpolation converges at least as fast as stochastic for smooth policy classes.\nThis interpolation step plays two key roles. First, it is a form of myopic or greedy online learning. Intuitively, rolling out πn leads to incidental exploration on the mistakes of πn, and so each round of training is focused on refining πn. Second, the interpolation in Line 10 ensures a slow drift in the distribution of states from round to round, which preserves an approximate i.i.d. property for the supervised regression subroutine and guarantees convergence.\nHowever this model interpolation creates an inherent tension between maintaining approximate i.i.d. for valid su-\npervised learning and more aggressive exploration (and thus faster convergence). For example, SEARN’s guarantees only apply for small β ă 1{T . SIMILE circumvents much of this tension via a policy improvement bound that allows β to adaptively increase depending on the quality of π̂n (see Theorem 5.6), which thus guarantees a valid learning reduction while substantially speeding up convergence. Feedback Generation. We can generate training targets pant using “virtual” feedback from simulating expert demonstrations, which has two benefits. First, we need not query the expert π˚ at every iteration (as done in DAgger (Ross et al., 2011)). Continuously acquiring expert demonstrations at every round can be seen as a special case and a more expensive strategy. Second, virtual feedback ensures stable learning, i.e., every π̂n is a feasible smooth policy.\n000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053\n054 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107\nCVPR #307\nCVPR #307\nCVPR 2015 Submission #307. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\nLearning Online Smooth Predictors for Realtime Camera Planning\nAnonym u CVPR submission\nPaper ID 307\nAbstract\nData-driven prediction methods are extremely useful in many computer vision applications. However, the estimators are normally learned within a time independent context. When used for online prediction, the results are jittery. Although smoothing can be added after the fact (such as a Kalman filter), the approach is not ideal. Instead, temporal smoothness should be incorporated into the learning process. In this paper, we show how the ‘search and learn’ algorithm (which has been used previously for tagging parts of speech) can be adapted to efficiently learn regressors for temporal signals. We apply our data-driven learning technique to a camera planning problem: given noisy basketball player detection data, we learn where the camera should look based on examples from a human operator. Our experimental results show how a learning algorithm which takes into account temporal consistency of sequential predictions has significantly better performance than time independent estimators.\n1. Introduction In this work, we investigate the problem of determining where a camera should look when broadcasting a basketball game (see Fig. 1). Realtime camera planning shares many similarities with online object tracking: in both cases, the algorithms must constantly revise an estimated target position as new evidence is acquired. Noise and other ambiguities cause non-ideal jittery trajectories: they are are not good representations of how objects actually move, and in camera planning, lead to unaesthetic results. In practice, temporal regularization is employed to minimize jitter. The amount of regularization is a design parameter, and controls a trade-off between precision and smoothness. In contrast to object tracking, smoothness is of paramount importance in camera control: fluid movements which maintain adequate framing are preferable to erratic motions which pursue perfect composition.\nModel-free estimation methods, such as random forests, are very popular because they can be learned directly from\nFigure 1: Camera Planning. The objective is to predict\nan appropriate pan angle for a broadcast camera based on noisy player detection data. Consider two planning algorithms (shown as blue and red curves in the schematic) which both make the same mistake at time A but recover to a good framing by C (the ideal camera trajectory is shown in black). The blue solution quickly corrects by time B using a jerky motion, whereas the red curve conducts a gradual correction. Although the red curve has a larger discrepancy with the ideal motion curve, its velocity characteristics are most similar to the ideal motion path.\ndata. Often, the estimator is learned within a time independent paradigm, and temporal regularization is integrated as a post-processing stage (such as a Kalman filter). However, this two stage approach is not ideal because the data-driven estimator is prevented from learning any temporal patterns. In this paper, we condition the data-driven estimator on previous predictions, which allows it to learn temporal patterns within the data (in addition to any direct feature-based relationships). However, this recursive formulation (similar to reinforcement learning) makes the problem much more difficult to solve. We employ a variant of the ‘search and learn’ (SEARN) algorithm to keep training efficient. Its strategy is to decouple the recursive relationships using an auxiliary reference signal. This allows the predictor to be learned efficiently using supervised techniques, and our experiments demonstrate significant improvements when using this holistic approach.\nProblem Definition In the case of camera planning, we assume there is an underlying function f : X 7! Y which describes the ideal camera work that should occur at the\n1\nConsider Figure 1, where our policy πn (blue/red) made a mistake at location A, and where we have only a single expert demonstration from π˚ (black). Depending on the smoothness requirements of the policy class, we can simulate virtual expert feedback as via ei-\nther the red line (more smooth) or blue (less smooth) as a tr deoff between squared imitation loss and smoothness.\nWhen the roll-out of πn´1 (i.e. An) differs substantially from A˚, especially during early iterati ns, using smoother feedback (red instead of blue) can result in more stable learning. We formalize this notion for Πλ in Proposition 5.8. Intuitively, whenever πn´1 makes a mistake, resulti g in a “bad” state snt , the feedback should recommend a smooth correction pant w.r.t. An to make training “easier” for the learner.5 The virtual feedback pant should converge to the expert’s action a˚t . In practice, we use pant “ σant ` p1´ σqa˚t with σ Ñ 0 s n increases (which satisfies Proposition 5.8)."
    }, {
      "heading" : "5. Theoretical Results",
      "text" : "All proofs re deferred to the supplementary m erial."
    }, {
      "heading" : "5.1. Stability Conditions",
      "text" : "One natural smoothness condition is that πprx, asq should be stable w.r.t. if x is fixed. Consider the camera planning setting: the expert policy π˚ should have very small curvatur , since constant inputs should correspond to constant actions. This motivates Definition 2.2, which requires that Π has low curvature given fixed context. We also show that smooth policies per Definition 2.2 lead to stable actions, in the sense that “n arby” states are m pped to “ne rby” actions. The following helper lemma is useful:\n5A similar idea was proposed (He et al., 2012) for DAggertype algorithm, albeit only for linear model classes.\nLemma 5.1. For a fixed x, define πprx, asq fi ϕpaq. If ϕ is non-negative and H-smooth w.r.t. a., then:\n@a, a1 : ` ϕpaq ´ ϕpa1q ˘2 ď 6H ` ϕpaq ` ϕpa1q ˘ › ›a´ a1 › › 2 .\nWriting π as πprx, asq fi “ π1prx, asq, . . . , πkprx, asq ‰ with each πiprx, asq H-smooth, Lemma 5.1 implies }pπprx, asq ´ πprx, a1sqq} ď ? 12HR }a´ a1} for R upper bounding A. Bounded action space means that a sufficiently small H leads to the following stability conditions:\nCondition 1 (Stability Condition 1). Π satisfies the Stability Condition 1 if for a fixed input feature x, the actions of π in states s “ rx, as and s1 “ rx, a1s satisfy }πpsq ´ πps1q} ď }a´ a1} for all a, a1 P A. Condition 2 (Stability Condition 2). Π satisfies Stability Condition 2 if each π is γ-Lipschitz continuous in the action component a with γ ă 1. That is, for a fixed x the actions of π in states s “ rx, as and s1 “ rx, a1s satisfy }πpsq ´ πps1q} ď γ }a´ a1} for all a, a1 P A. These two conditions directly follow from Lemma 5.1 and assuming sufficiently small H . Condition 2 is mildly stronger than Condition 1, and enables proving much stronger policy improvement compared to previous work."
    }, {
      "heading" : "5.2. Deterministic versus Stochastic",
      "text" : "Given two policies π and π̂, and interpolation parameter β P p0, 1q, consider two ways to combine policies:\n1. stochastic: πstopsq “ π̂psq with probability β, and πstopsq “ πpsq with probability 1´ β 2. deterministic: πdetpsq “ βπ̂psq ` p1´ βqπpsq\nPrevious learning reduction approaches only use stochastic interpolation (Daumé III et al., 2009; Ross et al., 2011), whereas SIMILE uses deterministic. The following result shows that deterministic and stochastic interpolation yield the same expected behavior for smooth policy classes.\nLemma 5.2. Given any starting state s0, sequentially execute πdet and πsto to obtain two separate trajectories A “ tatuTt“1 and Ã “ tãtuTt“1 such that at “ πdetpstq and ãt “ πstops̃tq, where st “ rxt, at´1s and s̃t “ rxt, ãt´1s. Assuming the policies are stable as per Condition 1, we have EÃrãts “ at @t “ 1, . . . , T , where the expectation is taken over all random roll-outs of πsto.\nLemma 5.2 shows that deterministic policy combination (SIMILE) yields unbiased trajectory roll-outs of stochastic policy combination (as done in SEARN & CPI). This represents a major advantage of SIMILE, since the number of stochastic roll-outs of πsto to average to the deterministic trajectory of πdet is polynomial in the time horizon T , leading to much higher computational complexity. Furthermore, for convex imitation loss `πpπq, Lemma 5.2 and Jensen’s inequality yield the following corollary, which states that under convex loss, deterministic policy performs at least no worse than stochastic policy in expectation:\nCorollary 5.3 (Deterministic Policies Perform Better). For deterministic πdet and stochastic πsto interpolations of two policies π and π̂, and convex loss `, we have:\n`πdetpπdetq “ `πstopErπstosq ď E r`πstopπstoqs\nwhere the expectation is over all roll-outs of πsto.\nRemark. We construct a simple example to show that Condition 1 may be necessary for iterative learning reductions to converge. Consider the case where contexts X Ă R are either constant or vary neglibly. Expert demonstrations should be constant π˚prxn, a˚sq “ a˚ for all n. Consider an unstable policy π such that πpsq “ πprx, asq “ ka for fixed k ą 1. The rolled-out trajectory of π diverges π˚ at an exponential rate. Assume optimistically that π̂ learns the correct expert behavior, which is simply π̂psq “ π̂prx, asq “ a. For any β P p0, 1q, the updated policy π1 “ βπ̂`p1´βqπ becomes π1prx, asq “ βa`p1´βqka. Thus the sequential roll-out of the new policy π1 will also yield an exponential gap from the correct policy. By induction, the same will be true in all future iterations."
    }, {
      "heading" : "5.3. Policy Improvement",
      "text" : "Our policy improvement guarantee builds upon the analysis from SEARN (Daumé III et al., 2009), which we extend to using adaptive learning rates β. We first restate the main policy improvement result from Daumé III et al. (2009).\nLemma 5.4 (SEARN’s policy nondegradation - Lemma 1 from Daumé III et al. (2009)). Let `max “ supπ,s `pπpsqq, π1 is defined as πsto in lemma 5.2. Then for β P p0, 1{T q:\n`π1pπ1q ´ `πpπq ď βTEs„dπ r`pπ̂psqqs ` 1\n2 β2T 2`max.\nSEARN guarantees that the new policy π1 does not degrade from the expert π˚ by much only if β ă 1{T . Analyses of SEARN and other previous iterative reduction methods (Ross et al., 2011; Kakade & Langford, 2002; Bagnell et al., 2003; Syed & Schapire, 2010) rely on bounding the variation distance between dπ and dπ1 . Three drawbacks of this approach are: (i) non-trivial variation distance bound typically requires β to be inversely proportional to time horizon T , causing slow convergence; (ii) not easily applicable to the continuous regime; and (iii) except under MDP framework with discounted infinite horizon, previous variation distance bounds do not guarantee monotonic policy improvements (i.e. `π1pπ1q ă `πpπq).\nWe provide two levels of guarantees taking advantage of Stability Conditions 1 and 2 to circumvent these drawbacks. Assuming the Condition 1 and convexity of `, our first result yields a guarantee comparable with SEARN.\nTheorem 5.5 (T-dependent Improvement). Assume ` is convex and L-Lipschitz, and Condition 1 holds. Let “ max s„dπ }π̂psq ´ πpsq}. Then:\n`π1pπ1q ´ `πpπq ď β LT ` β p`πpπ̂q ´ `πpπqq . (3)\nIn particular, choosing β P p0, 1{T q yields: `π1pπ1q ´ `πpπq ď L` β p`πpπ̂q ´ `πpπqq . (4)\nSimilar to SEARN, Theorem 5.5 also requires β P p0, 1{T q to ensure the RHS of (4) stays small. However, note that the reduction term βp`πpπ̂q ´ `πpπqq allows the bound to be strictly negative if the policy π̂ trained on dπ significantly improves on `πpπq (i.e., guaranteed policy improvement). We observe empirically that this often happens, especially in early iterations of training.\nUnder the mildly stronger Condition 2, we remove the dependency on the time horizon T , which represents a much stronger guarantee compared to previous work. Theorem 5.6 (Policy Improvement). Assume ` is convex and L-Lipschitz-continuous, and Condition 2 holds. Let “ max\ns„dπ }π̂psq ´ πpsq}. Then for β P p0, 1q:\n`π1pπ1q ´ `πpπq ď βγ L\np1´ βqp1´ γq ` βp`πpπ̂q ´ `πpπqq. (5)\nCorollary 5.7 (Monotonic Improvement). Following the notation from Theorem 5.6, let ∆ “ `πpπq ´ `πpπ̂q and δ “ γ L1´γ . Then choosing step size β “ ∆´δ 2∆ , we have:\n`π1pπ1q ´ `πpπq ď ´ p∆´ δq2\n2p∆` δq . (6)\nThe terms and `πpπ̂q ´ `πpπq on the RHS of (4) and (5) come from the learning reduction, as they measure the “distance” between π̂ and π on the state distribution induced by π (which forms the dataset to train π̂). In practice, both terms can be empirically estimated from the training round, thus allowing an estimate of β to minimize the bound.\nTheorem 5.6 justifies using an adaptive and more aggressive interpolation parameter β to update policies. In the worst case, setting β close to 0 will ensure the bound from (5) to be close to 0, which is consistent with SEARN’s result. More generally, monotonic policy improvement can be guaranteed for appropriate choice of β, as seen from Corollary 5.7. This strict policy improvement was not possible under previous iterative learning reduction approaches such as SEARN and DAgger, and is enabled in our setting due to exploiting the smoothness conditions."
    }, {
      "heading" : "5.4. Smooth Feedback Analysis",
      "text" : "Smooth Feedback Does Not Hurt: Recall from Section 4 that one way to simulate “virtual” feedback for training a new π̂ is to set the target ât “ σat ` p1 ´ σqa˚t for σ P p0, 1q, where smooth feedback corresponds to σ Ñ 1. To see that simulating smooth “virtual” feedback target does not hurt the training progress, we alternatively view SIMILE as performing gradient descent in a smooth function space (Mason et al., 1999). Define the cost functional C : Π Ñ R over policy space to be the average imitation loss over S as Cpπq “ ş\nS }πpsq ´ π˚psq}2 dP psq. The gra-\ndient (Gâteaux derivative) of Cpπq w.r.t. π is:\n∇Cpπqpsq “ BCpπ ` αδsqBα ˇ ˇ ˇ α“0 “ 2pπpsq ´ π˚psqq,\nwhere δs is Dirac delta function centered at s. By first order approximationCpπ1q “ Cpβπ̂`p1´βqπq “ Cpπ`βpπ̂´ πqq « Cpπq` βx∇Cpπq, π̂´ πy. Like traditional gradient descent, we want to choose π̂ such that the update moves the functional along the direction of negative gradient. In other words, we want to learn π̂ P Π such that x∇Cpπq, π̂´ πy ! 0. We can evaluate this inner product along the states induced by π. We thus have the estimate:\nx∇Cpπq, π̂ ´ πy « 2 T\nT ÿ t“1 pπpstq ´ π˚pstqqpπ̂pstq ´ πpstqq\n“ 2 T\nT ÿ t“1 pat ´ a˚t qpπ̂prxt, at´1sq ´ atq.\nSince we want x∇Cpπq, π̂ ´ πy ă 0, this motivates the construction of new data set D with states trxt, at´1suTt“1 and labels tpatuTt“1 to train a new policy π̂, where we want pat´a˚t qppat´atq ă 0. A sufficient solution is to set target pat “ σat ` p1 ´ σqa˚t (Section 4), as this will point the gradient in negative direction, allowing the learner to make progress.\nSmooth Feedback is Sometimes Necessary: When the current policy performs poorly, smooth virtual feedback may be required to ensure stable learning, i.e. producing a feasible smooth policy at each training round. We formalize this notion of feasibility by considering the smooth policy class Πλ in Example 2.1. Recall that smooth regularization of Πλ via H encourages the next action to be close to the previous action. Thus a natural way to measure smoothness of π P Πλ is via the average first order difference of consecutive actions 1T řT t“1 }at ´ at´1}. In particular, we want to explicitly constrain this difference relative to the expert trajectory 1T řT t“1 }at ´ at´1} ď η at each iteration, where η9 1T řT t“1 › ›a˚t ´ a˚t´1 › ›.\nWhen π performs poorly, i.e. the ”average gap” between current trajectory tatu and ta˚t u is large, the training target for π̂ should be lowered to ensure learning a smooth policy is feasible, as inferred from the following proposition. In practice, we typically employ smooth virtual feedback in early iterations when policies tend to perform worse.\nProposition 5.8. Let ω be the average supervised training error from F , i.e. ω “ min\nfPF Ex„X r}fprx, 0sq ´ a˚}s.\nLet the rolled-out trajectory of current policy π be tatu. If the average gap between π and π˚ is such that Et„Uniformr1:T s r}a˚t ´ at´1}s ě 3ω ` ηp1 ` λq, then using ta˚t u as feedback will cause the trained policy π̂ to be non-smooth, i.e.:\nEt„Uniformr1:T s r}ât ´ ât´1}s ě η, (7) for tâtu the rolled-out trajectory of π̂."
    }, {
      "heading" : "6. Experiments",
      "text" : "Automated Camera Planning. We evaluate SIMILE in a case study of automated camera planning for sport broadcasting (Chen & Carr, 2015; Chen et al., 2016). Given noisy tracking of players as raw input data txtuTt“1, and demonstrated pan camera angles from professional human operator as ta˚t uTt“1, the goal is to learn a policy π that produces trajectory tatuTt“1 that is both smooth and accurate relative to ta˚t uTt“1. Smoothness is critical in camera control: fluid movements which maintain adequate framing are preferable to jittery motions which constantly pursue perfect tracking (Gaddam et al., 2015). In this setting, time horizon T is the duration of the event multiplied by rate of sampling. Thus T tends to be very large. Smooth Policy Class. We use a smooth policy class following Example 2.2: regression tree ensembles F regularized by a class of linear autoregressor functions H (Chen et al., 2016). See Appendix B for more details. Summary of Results. • Using our smooth policy class leads to dramatically\nsmoother trajectories than not regularizing usingH. • Using our adaptive learning rate leads to much faster\nconvergence compared to conservative learning rates from SEARN (Daumé III et al., 2009). • Using smooth feedback ensures stable learning of smooth policies at each iteration. • Deterministic policy interpolation performs better than stochastic interpolation used in SEARN.\nSmooth versus Non-Smooth Policy Classes. Figure 2 shows a comparison of using a smooth policy class versus a non-smooth one (e.g., not using H). We see that our\napproach can reliably learn to predict trajectories that are both smooth and accurate. Adaptive vs. Fixed β: One can, in principle, train using SEARN, which requires a very conservative β in order to guarantee convergence. In contrast, SIMILE adaptively selects β based on relative empirical loss of π and π̂ (Line 9 of Algorithm 1). Let errorpπ̂q and errorpπq denote the mean-squared errors of rolled-out trajectories tâtu, tatu, respectively, w.r.t. ground truth ta˚t u. We can set β as:\nβ̂ “ errorpπq errorpπ̂q ` errorpπq , (8)\nwhich encourages the learner to disregard bad policies when interpolating, thus allowing fast convergence to a good policy (see Theorem 5.6). Figure 3 compares the convergence rate of SIMILE using adaptive β versus conservative fixed values of β commonly used in SEARN (Daumé III et al., 2009). We see that adaptively choosing β enjoys substantially faster convergence. Note that very large fixed β may overshoot and worsen the combined policy after a few initial improvements. Smooth Feedback Generation: We set the target labels to ânt “ σant ` p1 ´ σqa˚t for 0 ă σ ă 1 (Line 6 of Algorithm 1). Larger σ corresponds to smoother (ânt is closer to ant´1) but less accurate target (further from a ˚ t ), as seen in Figure 4. Figure 5 shows the trade-off between\nsmoothness loss (blue line, measured by first order difference in Proposition 5.8) and imitation loss (red line, measured by mean squared distance) for varying σ. We navigate this trade-off by setting\nσ closer to 1 in early iterations, and have σ Ñ 0 as n increases. This “gradual increase” produces more stable policies, especially during early iterations where the learning policy tends to perform poorly (as formalized in Proposition 5.8). In Figure 4, when the initial policy (green trajectory) has poor performance, setting smooth\ntargets (Figure 4b) allows learning a smooth policy in the subsequent round, in contrast to more accurate but less stable performance of “difficult” targets with low σ (Figure 4c-d). Figure 6 visualizes the behavior of the the intermediate policies learned by SIMILE, where we can see that each intermediate policy is a smooth policy. Deterministic vs. Stochastic Interpolation: Finally, we evaluate the benefits of using deterministic policy averaging intead of stochastically combine different policies, as done in SEARN. To control for other factors, we set β to a fixed value of 0.5, and keep the new training dataset Dn the same for each iteration n. The average imitation loss of stochastic policy sampling are evaluated after 50 stochastic roll-outs at each iterations. This average stochastic policy error tends to be higher compared to the empirical error of the deterministic trajectory, as seen from Figure 7, and confirms our finding from Corollary 5.3."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We formalized the problem of smooth imitation learning for online sequence prediction, which is a variant of imitation learning that uses a notion of a smooth policy class. We proposed SIMILE (Smooth IMItation LEarning), which is an iterative learning reduction approach to learning smooth policies from expert demonstrations in a continuous and dynamic environment. SIMILE utilizes an adaptive learning rate that provably allows much faster convergence compared to previous learning reduction approaches, and also enjoys better sample complexity than previous work by being fully deterministic and allowing for virtual simulation of training labels. We validated the efficiency and practicality of our approach on a setting of online camera planning."
    }, {
      "heading" : "A. Detailed Theoretical Analysis and Proofs",
      "text" : "A.1. Proof of lemma 5.1\nLemma Statement. (Lemma 5.1) For a fixed x, define πprx, asq fi ϕpaq. If ϕ is non-negative and H-smooth w.r.t. a., then:\n@a, a1 : ` ϕpaq ´ ϕpa1q ˘2 ď 6H ` ϕpaq ` ϕpa1q ˘ › ›a´ a1 › › 2 .\nThe proof of Lemma 5.1 rests on 2 properties of H-smooth functions (differentiable) in R1, as stated below\nLemma A.1 (Self-bounding property of Lipschitz-smooth functions). Let φ : R Ñ R be an H-smooth non-negative function. Then for all a P R: |∇φpaq| ď a 4Hφpaq\nProof. By mean value theorem, for any a, a1 we have D η P pa, a1q (or pa1, aq) such that φpa1q “ φpaq`∇φpηqpa1´aq. Since φ is non-negative,\n0 ď φpa1q “ φpaq `∇φpaqpa1 ´ aq ` p∇φpηq ´∇φpaqqpa1 ´ aq\nď φpaq `∇φpaqpa1 ´ aq `H|η ´ a||a1 ´ a| ď φpaq `∇φpaqpa1 ´ aq `H|a1 ´ a|2\nChoosing a1 “ a´ ∇φpaq2H proves the lemma.\nLemma A.2 (1-d Case (Srebro et al., 2010)). Let φ : RÑ R be an H-smooth non-negative function. Then for all a, a1 P R:\n` φpaq ´ φpa1q ˘2 ď 6H ` φpaq ` φpa1q ˘ ` a´ a1 ˘2\nProof. As before, Dη P pa, a1q such that φpa1q ´ φpaq “ ∇φpηqpa1 ´ aq. By assumption of φ, we have |∇φpηq ´ ∇φpaq| ď H|η ´ a| ď H|a1 ´ a|. Thus we have:\n|∇φpηq| ď |∇φpaq|`H|a´ a1| (9) Consider two cases:\nCase 1: If |a ´ a1| ď |∇φpaq|5H , then by equation 9 we have |∇φpηq| ď 6{5|∇φpaq|. Thus\n` φpaq ´ φpa1q ˘2 “ p∇φpηqq2 ` a´ a1 ˘2\nď 36 25 p∇φpaqq2 ` a´ a1 ˘2 ď 144 25 Hφpaq ` a´ a1 ˘2\nby lemma A.1. Therefore, pφpaq ´ φpa1qq2 ď 6Hφpaq pa´ a1q2 ď 6H pφpaq ` φpa1qq pa´ a1q2\nCase 2: If |a ´ a1| ą |∇φpaq|5H , then equation 9 gives |∇φpηq| ď 6H|a´ a1|. Once again `\nφpaq ´ φpa1q ˘2 “ ` φpaq ´ φpa1q ˘ ∇φpηq ` a´ a1 ˘\nď | ` φpaq ´ φpa1q ˘ ||∇φpηq|| ` a´ a1 ˘ | ď | ` φpaq ´ φpa1q ˘ | ´ 6H ` a´ a1 ˘2 ¯\nď 6H ` φpaq ` φpa1q ˘ ` a´ a1 ˘2\nProof of Lemma 5.1. The extension to the multidimensional case is straightforward. For any a, a1 P Rk, consider the function φ : R Ñ R such that φptq “ ϕpp1 ´ tqa ` ta1q, then φ is a differentiable, non-negative function and ∇tpφptqq “ x∇ϕpa ` tpa1 ´ aqq, a1 ´ ay. Thus:\n|φ1pt1q ´ φ1pt2q| “ |x∇ϕpa` t1pa1 ´ aqq´ ∇ϕpa` t2pa1 ´ aqq, a1 ´ ay|\nď › ›∇ϕpa` t1pa1 ´ aqq ´∇ϕpa` t2pa1 ´ aqq › ›\n˚\n› ›a1 ´ a › ›\nď H|t1 ´ t2| › ›a´ a1 › › 2\nTherefore φ is an H }a´ a1}2-smooth function in R. Apply lemma A.2 to φ, we have:\npφp1q ´ φp0qq2 ď 6H › ›a´ a1 › › 2 pφp1q ` φp0qq p1´ 0q2\nwhich is the same as pϕpaq ´ ϕpa1qq2 ď 6Hpϕpaq ` ϕpa1qq }a´ a1}2\nA.2. Proof of lemma 5.2\nLemma Statement. (Lemma 5.2) Given any starting state s0, sequentially execute πdet and πsto to obtain two separate trajectories A “ tatuTt“1 and Ã “ tãtuTt“1 such that at “ πdetpstq and ãt “ πstops̃tq, where st “ rxt, at´1s and s̃t “ rxt, ãt´1s. Assuming the policies are stable as per Condition 1, we have EÃrãts “ at @t “ 1, . . . , T , where the expectation is taken over all random roll-outs of πsto.\nProof. Given a starting state s0, we prove by induction that EÃrãts “ at.\nIt is easily seen that the claim is true for t “ 1.\nNow assuming that EÃrãt´1s “ at´1. We have EÃrãts “ EÃrErãt|s̃tss\n“ EÃrβπ̂ps̃tq ` p1´ βqπps̃tqs “ βEÃrπ̂ps̃tqs ` p1´ βqEÃrπps̃tqs\nThus: ›\n›EÃrãts ´ at › › “ › ›EÃrãts ´ βπ̂pstq ´ p1´ βqπpstq › ›\n“ }βEÃrπ̂ps̃tqs ` p1´ βqEÃrπps̃tqs ´ βπ̂pstq ´ p1´ βqπpstq} ď β ›\n›EÃrπ̂ps̃tqs ´ π̂pstq › › ` p1´ βq ›\n›EÃrπps̃tqs ´ πpstq › ›\nď β › ›EÃrãt´1s ´ at´1 › ›\n` p1´ βq › ›EÃrãt´1s ´ at´1 › ›\n“ 0 per inductive hypothesis. Therefore we conclude that EÃrãts “ at @t “ 1, . . . , T\nA.3. Proof of theorem 5.6 and corollary 5.7 - Main policy improvement results\nIn this section, we provide the proof to theorem 5.6 and corollary 5.7.\nTheorem Statement. (theorem 5.6) Assume ` is convex and L-Lipschitz-continuous, and Condition 2 holds. Let “ max s„dπ }π̂psq ´ πpsq}. Then for β P p0, 1q:\n`π1pπ1q ´ `πpπq ď βγ L\np1´ βqp1´ γq ` βp`πpπ̂q ´ `πpπqq.\nProof. First let’s review the notations: let T be the trajectory horizon. For a policy π in the deterministic policy class Π, given a starting state s0, we roll out the full trajectory s0 πÝÑ s1 πÝÑ . . . πÝÑ sT , where st “ rxt, πpst´1qs, with xt encodes the featurized input at current time t, and πpst´1q encodes the dependency on previous predictions. Let `pπpsqq be the loss of taking action πpsq at state s, we can define the trajectory loss of policy π from starting state s0 as\n`pπ|s0q “ 1\nT\nT ÿ t“1 `pπpstqq\nFor a starting state distribution µ, we define policy loss of π as the expected loss along trajectories induced by π: `πpπq “ Es0„µr`pπ|s0qs. Policy loss `πpπq can be understood as\n`πpπq “ ż\ns0„µ\nE xt„X\n1\nT\n«\nT ÿ t“1 `pπpstqq\nff\ndµps0q\nTo prove policy improvement, we skip the subscript of algorithm 1 to consider general policy update rule within\neach iteration:\nπ1 “ πnew “ βπ̂ ` p1´ βqπ (10) where π “ πold is the current policy (combined up until the previous iteration), π̂ is the trained model from calling the base regression routine TrainpS, pA|hq. Learning rate (step-size) β may be adaptively chosen in each iteration. Recall that this update rule reflects deterministic interpolation of two policies.\nWe are interested in quantifying the policy improvement when updating π to π1. Specifically, we want to bound\nΓ “ `π1pπ1q ´ `πpπq where `πpπq (respectively `π1pπ1q) denotes the trajectory loss of π (respectively π1) on the state distribution induced by π (resp. π1)\nWe will bound the loss difference of old and new policies conditioned on a common starting state s0. Based on update rule (10), consider rolling out π1 and π from the same starting state s0 to obtain two separate sequences π1 ÞÝÑ ts0 Ñ s11 . . . Ñ s1T u and π ÞÝÑ ts0 Ñ s1 . . . Ñ sT u corresponding to the same stream of inputs x1, . . . , xT .\nΓps0q “ 1\nT\nT ÿ t“1 `pπ1ps1tqq ´ `pπpstqq\n“ 1 T\nT ÿ t“1 `pπ1ps1tqq ´ `pπ1pstqq ` `pπ1pstqq ´ `pπpstqq\n(11)\nAssume convexity of ` (e.g. sum of square losses):\n`pπ1pstqq “ `pβπ̂pstq ` p1´ βqπpstqq ď β`pπ̂pstqq ` p1´ βq`pπpstqq\nThus we can begin to bound individual components of Γps0q as\n`pπ1ps1tqq ´ `pπpstqq ď `pπ1ps1tqqq ´ `pπ1pstqq ` β r`pπ̂pstqq ´ `pπpstqqs\nSince ` is L-Lipschitz continuous, we have\n`pπ1ps1tqq ´ `pπ1pstqq ď L › ›π1ps1tq ´ π1pstq › ›\nď Lγ › ›s1t ´ st › › (12)\nwhere (12) is due to the smoothness condition [2] of policy class Π. Given a policy class Π with γ ă 1, the following claim can be proved by induction: Claim: }s1t ´ st} ď β p1´βqp1´γq\nProof. For the base case, given the same starting state s0, we have s11 “ rx1, π1ps0qs and s1 “ rx1, πps0qs. Thus }s11 ´ s1} “ }π1ps0q ´ πps0q} “ }βπ̂ps0q ` p1´ βqπps0q ´ πps0q} “ β }π̂ps0q ´ πps0q} ď β ď β p1´βqp1´γq .\nIn the inductive case, assume we have › ›s1t´1 ´ st´1 › › ď\nβ p1´βqp1´γq . Then similar to before, the definition of s 1 t and st leads to › ›s1t ´ st › › “ › › “ xt, π 1ps1t´1q ‰ ´ rxt, πpst´1qs › ›\n“ › ›π1ps1t´1q ´ πpst´1q › › ď ›\n›π1ps1t´1q ´ π1pst´1q › ›` › ›π1pst´1q ´ πpst´1q › ›\nď γ › ›s1t´1 ´ st´1 › ›` β }π̂pst´1q ´ πpst´1q}\nď γ β p1´ βqp1´ γq ` β\nď β p1´ βqp1´ γq\nApplying the claim to equation (12), we have\n`pπ1ps1tqq ´ `pπ1pstqq ď βγ L\np1´ βqp1´ γq which leads to\n`pπ1ps1tq ´ `pπpstqqq ď βγ L\np1´ βqp1´ γq ` βp`pπ̂pstqq ´ `pπpstqqq (13)\nIntegrating (13) over the starting state s0 „ µ and input trajectories txtuTt“1, we arrive at the policy improvement bound:\n`π1pπ1q ´ `πpπq ď βγ L\np1´ βqp1´ γq ` βp`πpπ̂q ´ `πpπqq\nwhere `πpπ̂q is the expected loss of the trained policy π̂ on the state distribution induced by policy π (reduction term, analogous to policy advantage in the traditional MDP terminologies (Kakade & Langford, 2002))\nThis means in the worst case, as we choose β Ñ 0, we have r`π1pπ1q ´ `πpπqs Ñ 0, meaning the new policy does not degrade much for a small choice of β. However if `πpπ̂q ´ `πpπq ! 0, we can choose β to enforce monotonic improvement of the policy by adaptively choosing β that minimizes the right-hand side. In particular, let the reduction term be ∆ “ `πpπq ´ `πpπ̂q ą 0 and let δ “ γ L1´γ , then for β “ ∆´δ2∆ we have the following monotonic policy improvement:\n`π1pπ1q ´ `πpπq ď ´ p∆´ δq2\n2p∆` δq\nA.4. Proof of theorem 5.5 - T -dependent improvement\nTheorem Statement. (theorem 5.5) Assume ` is convex and L-Lipschitz, and Condition 1 holds. Let “ max s„dπ }π̂psq ´ πpsq}. Then:\n`π1pπ1q ´ `πpπq ď β LT ` β p`πpπ̂q ´ `πpπqq .\nIn particular, choosing β P p0, 1{T q yields: `π1pπ1q ´ `πpπq ď L` β p`πpπ̂q ´ `πpπqq .\nProof. The proof of theorem 5.5 largely follows the structute of theorem 5.6, except that we are using the slighty weaker Condition 1 which leads to weaker bound on the policy improvement that depends on the trajectory horizon T . For any state s0 taken from the starting state distribution µ, sequentially roll-out policies π1 and π to receive two separate trajectories π1 : s0 Ñ s11 Ñ . . . Ñ s1T and π1 : s0 Ñ s1 Ñ . . . Ñ sT . Consider a pair of states s1t “ rxt, π1ps1t´1qs and st “ rxt, πpst´1qs corresponding to the same input feature xt, as before we can decompose `pπ1ps1tqq´ `pπpstqq “ `pπ1ps1tqq´ `pπ1pstqq` `pπ1pstqq´ `pπpstqq ď L }π1ps1tq ´ π1pstq} ` βp`pπ̂pstqq ´ `pπpstqqq due to convexity and L-Lipschitz continuity of `.\nCondition 1 further yields: `pπ1ps1tqq ´ `pπpstqq ď L }s1t ´ st} ` βp`pπ̂pstqq ´ `pπpstqqq. By the construction of the states, note that › ›s1t ´ st › › “ › ›π1ps1t´1q ´ πpst´1q › ›\nď › ›π1ps1t´1q ´ π1pst´1q › ›` › ›π1pst´1q ´ πpst´1q › › ď ›\n›s1t´1 ´ st´1 › ›` βp}π̂pst´1q ´ πpst´1q}q ď ›\n›s1t´1 ´ st´1 › ›` β (by condition 1 and definition of ).\nFrom here, one can use this recursive relation to easily show that }s1t ´ st} ď β t for all t P r1, T s.\nAveraging over the T time steps and integrating over the starting state distribution, we have:\n`π1pπ1q ´ `πpπq ď β LpT ` 1q{2` βp`πpπ̂q ´ `πpπqq ď β LT ` βp`πpπ̂q ´ `πpπqq\nIn particular, β P p0, 1{T q yields `π1pπ1q ´ `πpπq ď L ` βp`πpπ̂q ´ `πpπqq.\nA.5. Proof of proposition 5.8 - smooth expert proposition\nProposition Statement. (Proposition 5.8) Let ω be the average supervised training error from F , i.e. ω “ min fPF Ex„X r}fprx, 0sq ´ a˚}s. Let the rolled-out trajectory of current policy π be tatu. If the average gap between π and π˚ is such that Et„Uniformr1:T s r}a˚t ´ at´1}s ě 3ω ` ηp1`λq, then using ta˚t u as feedback will cause the trained policy π̂ to be non-smooth, i.e.:\nEt„Uniformr1:T s r}ât ´ ât´1}s ě η, for tâtu the rolled-out trajectory of π̂.\nProof. Recall that Πλ is formed by regularizing a class of supervised learners F with the singleton class of smooth function H fi thpaq “ au, via a hyper-parameter λ\nthat controls the trade-off between being close to the two classes.\nMinimizing over Πλ can be seen as a regularized optimization problem:\nπ̂px, aq “ argmin πPΠ `pπprx, asqq\n“ argmin fPF,hPH\npfpx, aq ´ a˚q2 ` λpfpx, aq ´ hpaqq2\n“ argmin fPF\npfpx, aq ´ a˚q2 ` λpfpx, aq ´ aq2\n(14)\nwhere hyper-parameter λ trades-off the distance of fpx, aq relative to a (smoothness) and a˚ (imitation accuracy), and a P R1.\nSuch a policy π, at execution time, corresponds to the regularized minimizer of:\nat “ πprx, at´1sq “ argmin\na }a´ fprxt, at´1sq}2 ` λ }a´ at´1}2\n“ fprxt, at´1sq ` λat´1 1` λ (15)\nwhere f P F is the minimizer of equation 14 Thus we enforce smoothness of learning policy from Πλ by encouraging low first order difference of consecutive actions of the executed trajectory tatu. In practice, we may contrain this first order difference relative to the human trajectory 1T řT t“1 }at ´ at´1} ď η, where η9 1T řT t“1 › ›a˚t ´ a˚t´1 › ›.\nConsider any given iteration with the following set-up: we execute old policy π “ πold to get rolled-out trajectory tatuTt“1. Form the new data set as D “ tpst, a˚t quTt“1 with predictors st “ rxt, at´1s and feedback labels simply the human actions a˚t . Use this data set to train a policy π̂ by learning a supervised f̂ P F from D. Similar to π, the execution of π̂ corresponds to ât where:\nât “ π̂prxt, ât´1sq\n“ argmin a\n› › › a´ f̂prxt, ât´1sq › › ›\n2\n` λ }a´ ât´1}2\n“ f̂prxt, ât´1sq ` λât´1 1` λ (16)\nDenote by f0 the ”naive” supervised learner from F . In other words, f0 “ argmin\nfPF\nT ř\nt“1 }fprxt, 0sq ´ a˚t } 2. Let ω be\nthe average gap between human trajectory and the rolledout trajectory of f0, i.e.\nω “ 1 T\nT ÿ t“1 }f0prxt, 0sq ´ a˚t }\nNote that it is reasonable to assume that the average errors\nof f and f̂ are no worse than f0, since in the worst case we can simply discard the extra features at´1 (resp. ât´1) of f (resp. f̂ ) to recover the performance of the naive learner f0:\n1\nT\nT ÿ t“1 }fprxt, at´1sq ´ a˚t } ď ω\n1\nT\nT ÿ\nt“1\n› › › f̂prxt, ât´1sq ´ a˚t › › › ď ω\nAssume that the old policy π “ πold is ”bad” in the sense that the rolled-out trajectory tatuTt“1 differs substantially from human trajectory ta˚t uTt“1. Specifically, denote the gap:\n1\nT\nT ÿ t“1 }a˚t ´ at´1} “ Ω \" ω\nThis means the feedback correction a˚t to st “ rxt, at´1s is not smooth. We will show that the trained policy π̂ from D will not be smooth. From the definition of at and ât from equations 15 and 16, we have for each t:\nat´ât “ λ 1` λ pat´1´ât´1q` fprxt, at´1sq ´ f̂prxt, ât´1sq\n1` λ Applying triangle inequality and summing up over t, we have:\n1\nT\nT ÿ t“1 }at ´ ât} ď 2ω\nFrom here we can provide a lower bound on the smoothness of the new trajectory ât, as defined by the first order difference 1T řT t“1 }ât ´ ât´1}. By definition of ât:\n}ât ´ ât´1} “\n› › › › › f̂prxt, ât´1sq ´ ât´1 1` λ › › › › ›\n“\n› › › › › f̂prxt, ât´1sq ´ a˚t ` a˚t ´ at´1 ` at´1 ´ ât´1 1` λ › › › › ›\ně }a˚t ´ at´1} ´\n› › › f̂prxt, ât´1sq ´ a˚t › › › ´ }at´1 ´ ât´1}\n1` λ Again summing up over t and taking the average, we obtain:\n1\nT\nT ÿ t“1 }ât ´ ât´1} ě Ω´ 3ω 1` λ\nHence for Ω \" ω, meaning the old trajectory is sufficiently far away from the ideal human trajectory, setting the learning target to be the ideal human actions will cause the learned trajectory to be non-smooth.\nB. Imitation Learning for Online Sequence Prediction With Smooth Regression Forests\nB.1. Variant of SIMILE Using Smooth Regression Forest Policy Class\nWe provide a specific instantiation of algorithm 1 that we used for our experiment, based on a policy class Π as a smooth regularized version of the space of treebased ensembles. In particular, F is the space of random forests and H is the space of linear auto-regressors H fi thpat´1:t´τ q “ řτ i“1 ciat´iu. In combination, F andH form a complex tree-based predictor that can predict smooth sequential actions.\nEmpirically, decision tree-based ensembles are among the best performing supervised machine learning method (Caruana & Niculescu-Mizil, 2006; Criminisi et al., 2012). Due to the piece-wise constant nature of decision treebased prediction, the results are inevitably non-smooth. We propose a recurrent extension based on H, where the prediction at the leaf node is not necessarily a constant, but rather is a smooth function of both static leaf node prediction and its previous predictions. By merging the powerful tree-based policy class with a linear auto-regressor, we provide a novel approach to train complex models that can accommodate smooth sequential prediction using modelbased smooth regularizer, at the same time leveraging the expressiveness of complex model-free function class (one can similarly apply the framework to the space of neural networks). Algorithm 2, which is based on SIMILE, describes in more details our training procedure used for the automated camera planning experiment. We first describe the role of the linear autoregressor class H, before discussing how to incorporate H into decision tree training to make smooth prediction (see the next section).\nThe autoregresor hπpa´1, . . . , a´τ q is typically selected from a class of autoregressors H. In our experiments, we use regularized linear autoregressors asH. Consider a generic learning policy π with a rolled-out trajectory A “ tatuTt“1 corresponding to the input sequence X “ txtuTt“1. We form the state sequence S “ tstuTt“1 “ trxt, . . . , xt´τ , at´1, . . . , at´τ suTt“1, where τ indicates the past horizon that is adequate to approximately capture the full state information. We approximate the smoothness of the trajectory A by a linear autoregressor\nhπ ” hπpstq ” τ ÿ\ni“1 ciat´i\nfor a (learned) set of coefficients tciuτi“1 such that at « hπ pstq. Given feedback target pA “ tâtu, the joint loss\nAlgorithm 2 Imitation Learning for Online Sequence Prediction with Smooth Regression Forest Input: Input features X “ txtuTt“1, expert demonstration\nA˚ “ ta˚t uTt“1, base routine Forest, past horizon τ , sequence of σ P p0, 1q\n1: Initialize A0 Ð A˚,S0 Ð t “ xt:t´τ , a ˚ t´1:t´τ ‰ u,\nh0 “ argmin c1,...,cτ\nT ř\nt“1\n` a˚t ´ řτ i“1 cia ˚ t´i\n˘2\n2: Initial policy π0 “ π̂0 ÐForestpS0,A0| h0q 3: for n “ 1, . . . , N do 4: An “ tant u Ð tπn´1p “ xt:t´τ , a n´1 t´1:t´τ ‰\nqu //sequential roll-out old policy\n5: Sn Ð tsnt “ “ xt:t´τ , a n t´1:t´τ ‰ u //Form states in 1d case 6: pAn “ tpant “ σant ` p1´ σqa˚t u @snt P Sn // collect smooth 1-step feedback\n7: hn “ argmin c1,...,cτ\nT ř\nt“1\n` ânt ´ řτ i“1 ciâ n t´i ˘2 //update ci\nvia regularized least square 8: π̂n ÐForestpSn, pAn| hnq // train with smooth decision forests. See section B.2 9: β Ð errorpπqerrorpπ̂q`errorpπq //set β to weighted\nempirical errors 10: πn “ βπ̂n ` p1´ βqπn´1 // update policy 11: end for output Last policy πN\nfunction thus becomes\n`pa, âtq “ `dpa, âtq ` λ`Rpa, stq\n“ pa´ âtq2 ` λpa´ τ ÿ\ni“1 ciat´iq2\nHere λ trades off between smoothness versus absolute imitation accuracy. The autoregressor hπ acts as a smooth linear regularizer, the parameters of which can be updated at each iteration based on feedback target pA according to\nhπ “ argmin hPH\n› › › pA´ hppAq › › ›\n2\n“ argmin c1,...,cτ\np T ÿ\nt“1 pât ´\nτ ÿ i“1 ciât´iq2q, (17)\nIn practice we use a regularized version of equation (17) to learn a new set of coefficients tciuτi“1. The Forest procedure (Line 8 of algorithm 2) would use this updated hπ to train a new policy that optimizes the trade-off between at « ât (feedback) versus smoothness as dictated by at « řτ i“1 ciat´i.\nB.1.1. SMOOTH REGULARIZATION WITH LINEAR AUTOREGRESSORS\nOur application of Algorithm 1 to realtime camera planning proceeds as follows: At each iteration, we form a state se-\nquence S based on the rolled-out trajectory A and tracking input data X such that st “ rxt, . . . , xt´τ , at´1, . . . , at´τ s for appropriate τ that captures the history of the sequential decisions. We generate feedback targets pA based on each st P S following ât “ σat ` p1´ σqa˚t using a parameter σ P p0, 1q depending on the Euclidean distance between A and A˚. Typically, σ gradually decreases to 0 as the rolledout trajectory improves on the training set. After generating the targets, a new linear autoregressor hπ (new set of coefficients tciuτi“1) is learned based on pA using regularized least squares (as described in the previous section). We then train a new model π̂ based on S, pA, and the updated coefficients tciu, using Forest - our recurrent decision tree framework that is capable of generating smooth predictions using autoregressor hπ as a smooth regularizer (see the following section for how to train smooth decision trees). Note that typically this creates a ”chicken-and-egg” problem. As the newly learned policy π̂ is greedily trained with respect to pA, the rolled-out trajectory of π̂ may have a state distribution that is different from what the previously learned hπ would predict. Our approach offers two remedies to this circular problem. First, by allowing feedback signals to vary smoothly relative to the current rolled-out trajectory A, the new policy π̂ should induce a new autoregresor that is similar to previously learned hπ . Second, by interpolating distributions (Line 10 of Algorithm 2) and having pA eventually converge to the original human trajectory A˚, we will have a stable and converging state distribution, leading to a stable and converging hπ .\nThroughout iterations, the linear autoregressor hπ and regularization parameter λ enforces smoothness of the rolledout trajectory, while the recurrent decision tree framework Forest learns increasingly accurate imitation policy. We generally achieve a satisfactory policy after 5-10 iterations in our sport broadcasting data sets. In the following section, we describe the mechanics of our recurrent decision tree training.\nB.2. Smooth Regression Tree Training\nGiven states s as input, a decision tree specifies a partitioning of the input state space. Let D “ tpsm, âmquMm“1 denote a training set of state/target pairs. Conventional regression tree learning aims to learn a partitioning such that each leaf node, node, makes a constant prediction via minimizing the squared loss function:\nānode “ argmin a\nÿ\nps,âqPDnode\n`dpa, âq\n“ argmin a\nÿ\nps,âqPDnode\npâ´ aq2, (18)\nwhereDnode denotes the training data fromD that has partitioned into the leaf node. For squared loss, we have:\nānode “ mean tâ |ps, âq P Dnode u . (19)\nIn the recurrent extension to Forest, we allow the decision tree to branch on the input state s, which includes the previous predictions a´1, . . . , a´τ . To enforce more explicit smoothness requirements, let hπpa´1, . . . , a´τ q denote an autoregressor that captures the temporal dynamics of π over the distribution of input sequences dx, while ignoring the inputs x. At time step t, hπ predicts the behavior at “ πpstq given only at´1, . . . , at´τ .\nOur policy class Π of recurrent decision trees π makes smoothed predictions by regularizing the predictions to be close to its autoregressor hπ . The new loss function incorporates both the squared distance loss `d, as well as a smooth regularization loss such that:\nLDpaq “ ÿ\nps,âqPD `dpa, âq ` λ`Rpa, sq\n“ ÿ\nps,âqPD pa´ âq2 ` λpy ´ hπpsqq2\nwhere λ is a hyper-parameter that controls how much we care about smoothness versus absolute distance loss.\nMaking prediction: For any any tree/policy π, each leaf node is associated with the terminal leaf node value ānode such that prediction ã given input state s is:\nãpsq ” πpsq “ argmin a pa´ ānodepsqq2 ` λpa´ hπpsqq2,\n(20)\n“ ānodepsq ` λhπpsq\n1` λ . (21)\nwhere nodepsq denotes the leaf node of the decision tree that s branches to.\nSetting terminal node value: Given a fixed hπ and decision tree structure, navigating through consecutive binary queries eventually yields a terminal leaf node with associated training data Dnode Ă D.\nOne option is to set the terminal node value ānode to satisfy:\nānode “ argmin a\nÿ\nps,âqPDnode\n`dpãps|aq, âq\n“ argmin a\nÿ\nps,âqPDnode\npãps|aq ´ âq2 (22)\n“ argmin a\nÿ\nps,âqPDnode\nˆ\na` λhπpsq 1` λ ´ â\n˙2\nfor ãps|aq defined as in (21) with a ” ānodepsq. Similar to (19), we can write the closed-form solution of (22) as:\nānode “ mean tp1` λqâ´ λhπpsq |ps, âq P Dnode u . (23) When λ “ 0, (23) reduces to (19).\nNote that (22) only looks at imitation loss `d, but not smoothness loss `R. Alternatively in the case of joint imitation and smoothness loss, the terminal leaf node is set to minimize the joint loss function:\nānode “ argmin a LDnodepãps|aqq\n“ argmin a\nÿ\nps,âqPDnode\n`dpãps|aq, âq ` λ`Rpãps|aq, sq\n“ argmin a\nÿ\nps,âqPDnode\npãps|aq ´ âq2 ` λpãps|aq ´ hπpsqq2\n(24)\n“ argmin a\nÿ\nps,âqPDnode\nˆ\na` λhπpsq 1` λ ´ â\n˙2\n` λ ˆ a` λhπpsq 1` λ ´ hπpsq\n˙2\n“ mean tâ |ps, âq P Dnode u , (25)\nNode splitting mechanism: For a node representing a subset Dnode of the training data, the node impurity is defined as:\nInode “ LDnodepānodeq “ ÿ\nps,âqPDnode\n`dpānode, âq ` λ`Rpānode, sq\n“ ÿ\nps,âqPDnode\npānode ´ âq2 ` λpānode ´ hπpsqq2\nwhere ānode is set according to equation (23) or (25) over ps, âq’s in Dnode. At each possible splitting point where Dnode is partitioned into Dleft and Dright, the impurity of the left and right child of the node is defined similarly. As with normal decision trees, the best splitting point is chosen as one that maximizes the impurity reduction: Inode ´ |Dleft||Dnode|Ileft ´ |Dright| |Dnode| Iright"
    } ],
    "references" : [ {
      "title" : "Apprenticeship learning via inverse reinforcement learning",
      "author" : [ "Abbeel", "Pieter", "Ng", "Andrew Y" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Abbeel et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Abbeel et al\\.",
      "year" : 2004
    }, {
      "title" : "Exploration and apprenticeship learning in reinforcement learning",
      "author" : [ "Abbeel", "Pieter", "Ng", "Andrew Y" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Abbeel et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Abbeel et al\\.",
      "year" : 2005
    }, {
      "title" : "A survey of robot learning from demonstration",
      "author" : [ "Argall", "Brenna D", "Chernova", "Sonia", "Veloso", "Manuela", "Browning", "Brett" ],
      "venue" : "Robotics and autonomous systems,",
      "citeRegEx" : "Argall et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Argall et al\\.",
      "year" : 2009
    }, {
      "title" : "Policy search by dynamic programming",
      "author" : [ "Bagnell", "J Andrew", "Kakade", "Sham M", "Schneider", "Jeff G", "Ng", "Andrew Y" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Bagnell et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bagnell et al\\.",
      "year" : 2003
    }, {
      "title" : "An empirical comparison of supervised learning algorithms",
      "author" : [ "Caruana", "Rich", "Niculescu-Mizil", "Alexandru" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Caruana et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Caruana et al\\.",
      "year" : 2006
    }, {
      "title" : "Mimicking human camera operators",
      "author" : [ "Chen", "Jianhui", "Carr", "Peter" ],
      "venue" : "In IEEE Winter Conference Applications of Computer Vision (WACV),",
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning online smooth predictors for real-time camera planning using recurrent decision trees",
      "author" : [ "Chen", "Jianhui", "Le", "Hoang M", "Carr", "Peter", "Yue", "Yisong", "Little", "James J" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning",
      "author" : [ "Criminisi", "Antonio", "Shotton", "Jamie", "Konukoglu", "Ender" ],
      "venue" : "Foundations and Trends in Computer Graphics and Vision,",
      "citeRegEx" : "Criminisi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Criminisi et al\\.",
      "year" : 2012
    }, {
      "title" : "Search-based structured prediction",
      "author" : [ "Daumé III", "Hal", "Langford", "John", "Marcu", "Daniel" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "III et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "III et al\\.",
      "year" : 2009
    }, {
      "title" : "Imitation learning by coaching",
      "author" : [ "He", "Eisner", "Jason", "Daume", "Hal" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "He et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning trajectory preferences for manipulators via iterative improvement",
      "author" : [ "Jain", "Ashesh", "Wojcik", "Brian", "Joachims", "Thorsten", "Saxena", "Ashutosh" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Jain et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2013
    }, {
      "title" : "Approximately optimal approximate reinforcement learning",
      "author" : [ "Kakade", "Sham", "Langford", "John" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Kakade et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2002
    }, {
      "title" : "Reinforcement learning as classification: Leveraging modern classifiers",
      "author" : [ "Lagoudakis", "Michail", "Parr", "Ronald" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Lagoudakis et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lagoudakis et al\\.",
      "year" : 2003
    }, {
      "title" : "Relating reinforcement learning performance to classification performance",
      "author" : [ "Langford", "John", "Zadrozny", "Bianca" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Langford et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Langford et al\\.",
      "year" : 2005
    }, {
      "title" : "Functional gradient techniques for combining hypotheses",
      "author" : [ "Mason", "Llew", "Baxter", "Jonathan", "Bartlett", "Peter L", "Frean", "Marcus" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Mason et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Mason et al\\.",
      "year" : 1999
    }, {
      "title" : "Learning to search: Functional gradient techniques for imitation learning",
      "author" : [ "Ratliff", "Nathan", "Silver", "David", "Bagnell", "J. Andrew" ],
      "venue" : "Autonomous Robots,",
      "citeRegEx" : "Ratliff et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ratliff et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient reductions for imitation learning",
      "author" : [ "Ross", "Stéphane", "Bagnell", "Drew" ],
      "venue" : "In Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Ross et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2010
    }, {
      "title" : "A reduction of imitation learning and structured prediction to no-regret online learning",
      "author" : [ "Ross", "Stephane", "Gordon", "Geoff", "Bagnell", "J. Andrew" ],
      "venue" : "In Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Ross et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2011
    }, {
      "title" : "Smoothness, low noise and fast rates",
      "author" : [ "Srebro", "Nathan", "Sridharan", "Karthik", "Tewari", "Ambuj" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Srebro et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2010
    }, {
      "title" : "A reduction from apprenticeship learning to classification",
      "author" : [ "Syed", "Umar", "Schapire", "Robert E" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Syed et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Syed et al\\.",
      "year" : 2010
    }, {
      "title" : "A study in the analysis of stationary time series, 1939",
      "author" : [ "Wold", "Herman" ],
      "venue" : null,
      "citeRegEx" : "Wold and Herman.,? \\Q1939\\E",
      "shortCiteRegEx" : "Wold and Herman.",
      "year" : 1939
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "For such tasks, the use of machine learning to automatically learn a good policy from observed expert behavior, also known as imitation learning or learning from demonstrations, has proven tremendously useful (Abbeel & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).",
      "startOffset" : 209,
      "endOffset" : 331
    }, {
      "referenceID" : 2,
      "context" : "For such tasks, the use of machine learning to automatically learn a good policy from observed expert behavior, also known as imitation learning or learning from demonstrations, has proven tremendously useful (Abbeel & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).",
      "startOffset" : 209,
      "endOffset" : 331
    }, {
      "referenceID" : 17,
      "context" : "For such tasks, the use of machine learning to automatically learn a good policy from observed expert behavior, also known as imitation learning or learning from demonstrations, has proven tremendously useful (Abbeel & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).",
      "startOffset" : 209,
      "endOffset" : 331
    }, {
      "referenceID" : 10,
      "context" : "For such tasks, the use of machine learning to automatically learn a good policy from observed expert behavior, also known as imitation learning or learning from demonstrations, has proven tremendously useful (Abbeel & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).",
      "startOffset" : 209,
      "endOffset" : 331
    }, {
      "referenceID" : 6,
      "context" : "Our motivating example is the problem of learning smooth policies for automated camera planning (Chen et al., 2016): determining where a camera should look given environment information (e.",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 17,
      "context" : "To address this issue, numerous learning reduction approaches have been proposed (Daumé III et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011), which iteratively modify the training distribution in various ways such that any supervised learning guarantees provably lift to the sequential imitation setting (potentially at the cost of statistical or computational efficiency).",
      "startOffset" : 81,
      "endOffset" : 146
    }, {
      "referenceID" : 17,
      "context" : ", 2009)), and not requiring data aggregation (as opposed to DAgger (Ross et al., 2011)) which can lead to super-linear training time.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "• We empirically evaluate using the setting of smooth camera planning (Chen et al., 2016), and demonstrate the performance gains of our approach.",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "Consider the example of autonomous camera planning for broadcasting a sport event (Chen et al., 2016).",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "Following the basic setup from (Ross et al., 2011), for any policy π P Π, let dt denote the distribution of states at time t if π is executed for the first t ́1 time steps.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "The most popular traditional approaches for learning from expert demonstration focused on using approximate policy iteration techniques in the MDP setting (Kakade & Langford, 2002; Bagnell et al., 2003).",
      "startOffset" : 155,
      "endOffset" : 202
    }, {
      "referenceID" : 9,
      "context" : "Most prior approaches operate in discrete and finite action space (He et al., 2012; Ratliff et al., 2009; Abbeel & Ng, 2004; Argall et al., 2009).",
      "startOffset" : 66,
      "endOffset" : 145
    }, {
      "referenceID" : 15,
      "context" : "Most prior approaches operate in discrete and finite action space (He et al., 2012; Ratliff et al., 2009; Abbeel & Ng, 2004; Argall et al., 2009).",
      "startOffset" : 66,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "Most prior approaches operate in discrete and finite action space (He et al., 2012; Ratliff et al., 2009; Abbeel & Ng, 2004; Argall et al., 2009).",
      "startOffset" : 66,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : "State-of-the-art learning reductions for imitation learning typically take an iterative approach, where each training round uses standard supervised learning to learn a policy (Daumé III et al., 2009; Ross et al., 2011).",
      "startOffset" : 176,
      "endOffset" : 219
    }, {
      "referenceID" : 17,
      "context" : ", 2009) and DAgger (Ross et al., 2011).",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : "First, we need not query the expert π ̊ at every iteration (as done in DAgger (Ross et al., 2011)).",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "A similar idea was proposed (He et al., 2012) for DAggertype algorithm, albeit only for linear model classes.",
      "startOffset" : 28,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "Previous learning reduction approaches only use stochastic interpolation (Daumé III et al., 2009; Ross et al., 2011), whereas SIMILE uses deterministic.",
      "startOffset" : 73,
      "endOffset" : 116
    }, {
      "referenceID" : 17,
      "context" : "Analyses of SEARN and other previous iterative reduction methods (Ross et al., 2011; Kakade & Langford, 2002; Bagnell et al., 2003; Syed & Schapire, 2010) rely on bounding the variation distance between dπ and dπ1 .",
      "startOffset" : 65,
      "endOffset" : 154
    }, {
      "referenceID" : 3,
      "context" : "Analyses of SEARN and other previous iterative reduction methods (Ross et al., 2011; Kakade & Langford, 2002; Bagnell et al., 2003; Syed & Schapire, 2010) rely on bounding the variation distance between dπ and dπ1 .",
      "startOffset" : 65,
      "endOffset" : 154
    }, {
      "referenceID" : 7,
      "context" : "Policy Improvement Our policy improvement guarantee builds upon the analysis from SEARN (Daumé III et al., 2009), which we extend to using adaptive learning rates β. We first restate the main policy improvement result from Daumé III et al. (2009). Lemma 5.",
      "startOffset" : 95,
      "endOffset" : 247
    }, {
      "referenceID" : 7,
      "context" : "Policy Improvement Our policy improvement guarantee builds upon the analysis from SEARN (Daumé III et al., 2009), which we extend to using adaptive learning rates β. We first restate the main policy improvement result from Daumé III et al. (2009). Lemma 5.4 (SEARN’s policy nondegradation - Lemma 1 from Daumé III et al. (2009)).",
      "startOffset" : 95,
      "endOffset" : 328
    }, {
      "referenceID" : 14,
      "context" : "To see that simulating smooth “virtual” feedback target does not hurt the training progress, we alternatively view SIMILE as performing gradient descent in a smooth function space (Mason et al., 1999).",
      "startOffset" : 180,
      "endOffset" : 200
    }, {
      "referenceID" : 6,
      "context" : "We evaluate SIMILE in a case study of automated camera planning for sport broadcasting (Chen & Carr, 2015; Chen et al., 2016).",
      "startOffset" : 87,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : "2: regression tree ensembles F regularized by a class of linear autoregressor functions H (Chen et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 109
    }, {
      "referenceID" : 18,
      "context" : "2 (1-d Case (Srebro et al., 2010)).",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "Empirically, decision tree-based ensembles are among the best performing supervised machine learning method (Caruana & Niculescu-Mizil, 2006; Criminisi et al., 2012).",
      "startOffset" : 108,
      "endOffset" : 165
    } ],
    "year" : 2016,
    "abstractText" : "We study the problem of smooth imitation learning for online sequence prediction, where the goal is to train a policy that can smoothly imitate demonstrated behavior in a dynamic and continuous environment in response to online, sequential context input. Since the mapping from context to behavior is often complex, we take a learning reduction approach to reduce smooth imitation learning to a regression problem using complex function classes that are regularized to ensure smoothness. We present a learning meta-algorithm that achieves fast and stable convergence to a good policy. Our approach enjoys several attractive properties, including being fully deterministic, employing an adaptive learning rate that can provably yield larger policy improvements compared to previous approaches, and the ability to ensure stable convergence. Our empirical results demonstrate significant performance gains over previous approaches.",
    "creator" : "LaTeX with hyperref package"
  }
}