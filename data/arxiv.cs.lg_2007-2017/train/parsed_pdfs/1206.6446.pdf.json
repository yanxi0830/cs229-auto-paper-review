{
  "name" : "1206.6446.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Agglomerative Bregman Clustering",
    "authors" : [ "Matus Telgarsky", "Sanjoy Dasgupta" ],
    "emails" : [ "mtelgars@cs.ucsd.edu", "dasgupta@cs.ucsd.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Starting with points {xi}mi=1 and a pairwise merge cost ∆(·, ·), classical agglomerative clustering produces a single hierarchical tree as follows (Duda et al., 2001).\n1. Start with m clusters: Ci := {xi} for each i.\n2. While at least two clusters remain:\n(a) Choose {Ci, Cj} with minimal ∆(Ci, Cj). (b) Remove {Ci, Cj}, add in Ci ∪ Cj .\nIn order to build a hierarchy with low k-means cost, one can use the merge cost due to Ward (1963),\n∆w(Ci, Cj) := |Ci||Cj | |Ci|+ |Cj | ‖τ(Ci)− τ(Cj)‖22,\nwhere τ(C) denotes the mean of cluster C.\nThe k-means cost, and thus the Ward merge rule, inherently prefer spherical clusters of common radius. To accommodate other cluster shapes and input domains, the squared Euclidean norm may be replaced with a relaxation sharing many of the same properties, a Bregman divergence.\nThis manuscript develops the theory of agglomerative clustering with Bregman divergences.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s)."
    }, {
      "heading" : "1.1. Bregman clustering",
      "text" : "There is already a rich theory of clustering with Bregman divergences, and in particular the relationship of these divergences with exponential family distributions (Banerjee et al., 2005). The standard development has two shortcomings, the first of which is amplified in the agglomerative setting.\nDegenerate divergences. Many divergences lead to merge costs which are undefined on certain inputs. This scenario is exacerbated with small clusters; for instance, with Gaussian clusters, the corresponding divergence rule is the KL divergence, which demands full rank cluster covariances. This is impossible with ≤ d points, but the agglomerative procedure above starts with singletons.\nMinimal representations. The standard theory of exponential families and its connections to Bregman divergences depend on minimal representations: there is just one way to write down any particular distribution. On the other hand, the natural encoding for many problems — e.g., Ising models, and many other examples listed in the textbook of Wainwright & Jordan (2008, Section 4) — is overcomplete, necessitating potentially tedious conversions to invoke the theory."
    }, {
      "heading" : "1.2. Contribution",
      "text" : "The approach of this manuscript is to carefully build a theory of Bregman divergences constructed from convex, yet nondifferentiable functions. Section 2 will present the basic definition, and verify this generalization satisfies the usual Bregman divergence properties.\nSection 3 will revisit the standard Bregman hard clustering model (Banerjee et al., 2005), and show how it naturally leads to a merge cost ∆. Section 4 then constructs exponential families, demonstrating that nondifferentiable Bregman divergences, while permitting representations which are not minimal, still satisfy all the usual properties. To overcome the aforemen-\ntioned small-sample cases where divergences may not be well-defined, Section 5 presents smoothing procedures which immediately follow from the preceding technical development.\nTo close, Section 6 presents the final algorithm, and Section 7 provides experimental validation both by measuring cluster fit, and the suitability of cluster features in supervised learning tasks.\nThe various appendices contain all proofs, as well as some additional technical material and examples."
    }, {
      "heading" : "1.3. Related work",
      "text" : "A number of works present agglomerative schemes for clustering with exponential families, from the perspective of KL divergences between distributions, or the analogous goal of maximizing model likelihood, or lastly in connection to the information bottleneck method (Iwayama & Tokunaga, 1995; Fraley, 1998; Heller & Ghahramani, 2005; Garcia et al., 2010; Blundell et al., 2010; Slonim & Tishby, 1999). Furthermore, Merugu (2006) studied the same algorithm as the present work, phrased in terms of Bregman divergences. These preceding methods either do not explicitly mention divergence degeneracies, or circumvent them with Bayesian techniques, a connection discussed in Section 5.\nBregman divergences for nondifferentiable functions have been studied in a number of places. Remark 2.4 shows the relationship between the presented version, and one due to Gordon (1999). Furthermore, Kiwiel (1995) presents divergences almost identical to those here, but the manuscripts and focuses differ thereafter.\nThe development here of exponential families and related Bregman properties generalizes results found in a variety of sources (Brown, 1986; Azoury & Warmuth, 2001; Banerjee et al., 2005; Wainwright & Jordan, 2008); further bibliographic remarks will appear throughout, and in Appendix G. Finally, parallel to the completion of this manuscript, another group has developed exponential families under similarly relaxed conditions, but from the perspective of maximum entropy and convex duality (Csiszár & Matúš, 2012)."
    }, {
      "heading" : "1.4. Notation",
      "text" : "The following concepts from convex analysis are used throughout the text; the interested reader is directed to the seminal text of Rockafellar (1970). A set is convex when the line segment between any two of its elements is again within the set. The epigraph of a function f : Rn → R̄, where R̄ = R ∪ {±∞}, is the set of points bounded below by f ; i.e., the set\n{(x, r) : x ∈ Rn, r ≥ f(x)} ⊆ Rn × R̄. A function is convex when its epigraph is convex, and closed when its epigraph is closed. The domain dom(f) of a function f : Rn → R̄ is the subset of inputs not mapping to +∞: dom(f) = {x ∈ Rn : f(x) < ∞}. A function is proper if dom(f) is nonempty, and f never takes on the value −∞. The Bregman divergences in this manuscript will be generated from closed proper convex functions.\nThe conjugate of a function f is the function f∗(φ) := supx 〈φ, x〉 − f(x); when f is closed proper convex, so is f∗, and moreover f∗∗ = f . A subgradient g to a function f at y ∈ dom(f) provides an affine lower bound: for any x ∈ Rn, f(x) ≥ f(y) + 〈g, x− y〉. The set of all subgradients at a point y is denoted by ∂f(y) (which is easily empty). The directional derivative f ′(y; d) of a function f at y in direction d is limt↓0(f(y + td)− f(y))/t.\nThe affine hull of a set S ⊆ Rn is the smallest affine set containing it. If S is translated and rotated so that its affine hull is some Rd ⊆ Rn, then its interior within Rd is its relative interior within Rn. Said another way, the relative interior ri(S) is the interior of S with respect to the Rn topology relativized to the affine hull of S. Although functions in this manuscript will generally be closed, their domains are often (relatively) open.\nConvex functions will be defined over Rn, but it will be useful to treat data as lying in an abstract space X , and a statistic map τ : X → Rn will embed examples in the desired Euclidean space. This map, which will also be overloaded to handle finite subsets of X , will eventually incorporate the smoothing procedure.\nThe cluster cost will be denoted by φ, or φf,τ to make the underlying convex function and statistic map clear; similarly, the merge cost is denoted by ∆ and ∆f,τ ."
    }, {
      "heading" : "2. Bregman divergences",
      "text" : "Given a convex function f : Rn → R̄, the Bregman divergence Bf (·, y) is the gap between f and its linearization at y. Typically, f is differentiable, and so Bf (x, y) = f(x)− f(y)− 〈∇f(y), x− y〉. Definition 2.1. Given a convex function f : Rn → R̄, the corresponding Bregman divergence between x, y ∈ dom(f) is\nBf (x, y) := f(x)− f(y) + f ′(y; y − x). ♦\nUnlike gradients, directional derivatives are welldefined whenever a convex function is finite, although they can be infinite on the relative boundary of dom(f) (Rockafellar, 1970, Theorems 23.1, 23.3, 23.4).\nNoting that f ′(y; y − x) ≥ −f ′(y;x − y) (Rockafellar, 1970, Theorem 23.1), it may seem closer to the original expression to instead use f(x) − f(y) − f ′(y;x − y) (which is thus bounded above by Bf (x, y)); however, it will later be shown that Bf (·, y) is convex, which fails if the directional derivative is flipped. This distinction is depicted in Figure 1.\nExample 2.2. In the case of the differentiable convex function f2 = ‖ · ‖22, Bf2(x, y) = ‖x − y‖22 follows by noting f ′2(y; y − x) = 〈2y, y − x〉. To analyze the case of f1 = ‖ ·‖1, first consider the univariate case h = | · |. Either by drawing a picture or checking h′(·; ·) from definition, it follows that\nBh(x, y) :=\n{ 0 when xy > 0,\n2|x| otherwise.\nThen noting that f ′1(·; ·) decomposes coordinate-wise, it follows that Bf1(x, y) = ∑ i Bh(xi, yi). Said another way, Bf1 is twice the l 1 distance from x to the farthest orthant containing y, which bears a resemblance to the hinge loss. ♦\nBf can also be written in terms of subgradients. Proposition 2.3. Let a proper convex f and y ∈ ri(dom(f)) be given. Then for any x ∈ dom(f):\n• f ′(y; y − x) and Bf (x, y) are finite, and\n• Bf (x, y) := maxg∈∂f(y) f(x)− f(y)− 〈g, x− y〉.\nThe above characterization will be extremely useful in proofs, where the existence of a maximizing subgradient ḡy,x will frequently be invoked.\nRemark 2.4. Given x ∈ dom(f) and a dual element g ∈ Rn, another nondifferentiable generalization of Bregman divergence, due to Gordon (1999), is\nDf (x, g) := f(x) + f ∗(g)− 〈g, x〉 .\nNow suppose there exists y ∈ ri(dom(f)) with g ∈ ∂f(y); the Fenchel-Young inequality (Rockafellar, 1970, Theorem 23.5) grants Df (x, g) = f(x) − f(y) − 〈g, x− y〉. Thus, by Proposition 2.3,\nBf (x, y) := max{Df (x, g) : g ∈ ∂f(y)}. ♦\nTo sanity check Bf , Appendix A states and proves a number of key Bregman divergence properties, generalized to the case of nondifferentiability. The following list summarizes these properties; in general, f is closed proper convex, y ∈ ri(dom(f)), and x ∈ dom(f).\n• Bf (·, y) is convex, proper, nonnegative, and Bf (y, y) = 0.\n• When f is strictly convex, Bf (x, y) = 0 iff x = y.\n• Given gx ∈ ri(dom(f∗)), supx∈∂f∗(gx) Bf (x, y) = supgy∈∂f(y) Bf∗(gy, gx).\n• Under some regularity conditions on f , a generalization of the Pythagorean theorem holds, with Bf replacing squared Euclidean distance.\nOver and over, this section depends on relative interiors. What’s so bad about the relative boundary? The directional derivatives and subgradients break down. If y ∈ relbd(dom(f)) and x ∈ ri(dom(f)), then f ′(y; y − x) =∞ = Bf (x, y), and there exists no maximizing subgradient as in Proposition 2.3; in fact, one can not in general guarantee the existence of any subgradients at all.\nIn just a moment, the cluster model will be developed, where it will be very easy for the second argument argument of Bf to lie on relbd(dom(f)), rendering the divergences infinite and cluster costs meaningless. Worse, it is frequently the case dom(f) is relatively open, meaning the relative boundary is not in dom(f)! The smoothing methods of Section 5 work around these issues. Their approach is simple enough: they just push relative boundary points inside the relative interior."
    }, {
      "heading" : "3. Cluster model",
      "text" : "Let a finite collection C of points {xi}mi=1 in some abstract space X — say, documents or vectors — be given. In order to cluster these with Bregman divergences, the first step is to map them into Rn.\nDefinition 3.1. A statistic map τ is any function from X to Rn. Given a finite set C ⊆ X , overload τ via averages: τ(C) := |C|−1 ∑ x∈C τ(x). ♦\nFor now, it suffices to think of τ as the identity map (with X = Rn), with an added convenience of computing means. Section 4, however, will rely on τ when constructing exponential families.\nDefinition 3.2. Given a statistic map τ : X → Rn and convex function f , the cost of a single cluster C is\nφf,τ (C) := ∑ x∈C Bf (τ(x), τ(C)).\n(This cost was the basis for Bregman hard clustering (Banerjee et al., 2005).) ♦\nExample 3.3 (k-means cost). Choose X = Rn, τ(x) = x, and f = ‖ · ‖22. As discussed in Example 2.2, Bf (x, y) = ‖x − y‖22, and so φf,τ (C) =∑ x∈C ‖x− τ(C)‖22, precisely the k-means cost. ♦\nAs such, τ(C) plays the role of a cluster center. While this may be intuitive for the k-means cost, it requires justification for general Bregman divergences. The following definition and results bridge this gap.\nDefinition 3.4. A convex function f is relatively (Gâteaux) differentiable if, for any y ∈ ri(dom(f)), there exists g (necessarily any subgradient) so that, for any x ∈ dom(f), f ′(y; y − x) = 〈g, y − x〉. ♦\nEvery differentiable function is relatively differentiable (with g = ∇f(y)); fortunately, many relevant convex functions, in particular those used to construct Bregman divergences between exponential family distributions (cf. Proposition 4.5), will be relatively differentiable.\nUnder relative differentiability, Bregman divergences admit a bias-variance style decomposition, which immediately justifies the choice of centroid τ(C).\nLemma 3.5. Let a proper convex relatively differentiable f , points {xi}mi=1 ⊂ Rn, and weights {αi}mi=1 ⊂ R be given, with µ := ∑ i αixi/( ∑ j αj) ∈ ri(dom(f)). Then, given any point y ∈ ri(dom(f)), m∑ i=1 αiBf (xi, y) = m∑ i=1 αiBf (xi, µ) + ( m∑ i=1 αi ) Bf (µ, y).\nCorollary 3.6. Suppose the convex function f is relatively differentiable, let any statistic map τ and any finite cluster C ⊆ X be given. Then φf,τ (C) = infy∈Rn ∑ x∈C B(τ(x), y).\nProof. Use µ := τ(C) = |C|−1 ∑ x∈C τ(x), Lemma 3.5, and Bf ≥ 0.\nContinuing, the stage is set to finally construct the Bregman merge cost.\nDefinition 3.7. Given two finite subsets C1, C2 of X , the cluster merge cost is simply growth in total cost:\n∆f,τ (C1, C2) = φf,τ (C1 ∪ C2)− ∑\nj∈{1,2}\nφf,τ (Ci). ♦\nThe above expression seems to imply that the computational cost of ∆ scales with the number of points. But in fact, one need only look at the relevant centers.\nProposition 3.8. Let a proper convex relatively differentiable f and two finite subsets C1, C2 of X with τ(Ci) ∈ ri(dom(f)) be given. Then\n∆f,τ (C1, C2) = ∑\nj∈{1,2}\n|Cj |Bf (τ(Cj), τ(C1 ∪ C2)).\nExample 3.9 (Ward/k-means merge cost). Continuing with the k-means cost from Example 3.3, note that for j ∈ {1, 2},\n‖τ(Cj)− τ(C1 ∪ C2)‖2 = |C3−j | · ‖τ(C1)− τ(C2)‖2\n|C1|+ |C2| .\nPlugging this into the simplification of ∆f,τ provided by Proposition 3.8,\n∆f,τ (C1, C2) = ∑\nj∈{1,2}\n|Cj ||C3−j |2\n(|C1|+ |C2|)2 ‖τ(C1)− τ(C2)‖22\n= |C1||C2| |C1|+ |C2| ‖τ(C1)− τ(C2)‖22.\nThis is exactly the Ward merge cost. ♦"
    }, {
      "heading" : "4. Exponential families",
      "text" : "So far, this manuscript has developed a mathematical basis to clustering with Bregman divergences. But what does it matter, if examples of meaningful Bregman divergences are few and far between?\nThe primary mechanism for constructing meaningful merge costs is to model the clusters as exponential family distributions. Throughout this section, let ν be any measure over X , and further stipulate the statistic map τ is ν-measurable.\nDefinition 4.1. Given a measurable statistic map τ and a vector θ ∈ Rn of canonical parameters, the corresponding exponential family distribution has density\npθ(x) := exp(〈τ(x), θ〉 − ψ(θ)),\nwhere the normalization term ψ, typically called the cumulant or log partition function, is simply\nψ(θ) = ln ∫ exp(〈τ(x), θ〉)dν(x). ♦\nMany standard distributions have this representation.\nExample 4.2. Choose X = Rd with ν being Lebesgue measure, and n = d(d + 1), i.e. Rn = Rd(d+1). The map τ(x) = (x, xx>) will provide for Gaussian densities. In particular, starting from the familiar form, with mean µ ∈ Rd and positive definite covariance Σ ∈ Rd2 , the density at x, pθ(x), is\nexp(−(x− µ)>Σ−1(x− µ)/2)√ (2π)d|Σ|\n= exp ( 〈 τ(x), (Σ−1µ,−Σ−1/2) 〉 − 1\n2 ln((2π)d|Σ| exp(µ>Σ−1µ))\n) .\nIn other words, θ = (Σ−1µ,−Σ−1/2). Notice that ψ (here expanded as 12 ln(. . .)) and θ do not make sense if Σ is merely positive semi-definite. ♦\nSo far so good, but where’s the convex function, and does the definition of pθ even make sense?\nProposition 4.3. Given a measurable statistic map τ , the function ψ is well-defined, closed, convex, and never takes on the value −∞. Remark 4.4. Notice that Proposition 4.3 did not provide that ψ is proper, only that it is never −∞. Unfortunately, more can not be guaranteed: if ν is Lebesgue measure over R and τ(x) = 0 for all x, then every parameter choice θ ∈ R has ψ(θ) = ∞. It is therefore necessary to check, for any provided τ , whether dom(ψ) is nonempty. ♦\nNot only is ψ closed convex, it is about as well-behaved as any function discussed in this manuscript.\nProposition 4.5. Suppose dom(ψ) is nonempty. Then ψ is relatively differentiable; in fact, given any θ ∈ ri(dom(ψ)), any τ̂ ∈ ∂ψ(θ), and any ξ ∈ dom(ψ),\nψ′(θ; ξ − θ) = 〈τ̂ , ξ − θ〉 = ∫ 〈τ(x), ξ − θ〉 pθ(x)dν(x). If ψ is fully differentiable at θ,then ∇ψ(θ) = ∫ τpθ. Since ψ is closed, given τ̂ ∈ ∂ψ(θ), it follows that θ ∈ ∂ψ∗(τ̂). There is still cause for concern that other subgradients at τ̂ lead to different densities, but as will be shown below, this does not happen.\nNow that a relevant convex function ψ has been identified, the question is whether Bψ (or Bψ∗) provide a reasonable notion of distance amongst densities.\nThis will be answered in two ways. To start, recall the Kullback-Leibler divergence K between densities p, q:\nK(p, q) = ∫ p(x) ln ( p(x)\nq(x)\n) dν(x).\nTheorem 4.6. Let any θ1, θ2 ∈ ri(dom(ψ)) and any τ̂1 ∈ ∂ψ(θ1), τ̂2 ∈ ∂ψ(θ2) be given, where ∂ψ∗(τ̂2) ⊆ ri(dom(ψ)) (for instance, if dom(ψ) is relatively open). Then\nK(pθ1 , pθ2) = Bψ(θ2, θ1) = Bψ∗(τ̂1, τ̂2).\nFurthermore, if θ1 ∈ ∂ψ∗(τ̂2), then pθ1 = pθ2 ν-a.e..\nMotivated by Proposition 4.5 and Theorem 4.6, the choice here is to base the cluster model on Bψ∗ .\nGiven two clusters {Ci}2i=1, set τ̂i := τ(Ci). When working with these clusters, it is entirely sufficient to store only these statistics and the cluster sizes, since τ(C1 ∪ C2) = |C1 ∪ C2|−1(|C1|τ̂1 + |C2|τ̂2). Assuming for interpretability that ψ is differentiable, since ψ is closed, ψ∗∗ = ψ, and thus ∇ψ(θ1) = ∫ τpθ1 = τ̂1; that is to say, these distributions have their (aptly named) mean parameterizations as their means. And as provided by Theorem 4.6, even if differentiability fails, various subgradients of the same mean all effectively represent the same distributions.\nExample 4.7. Suppose X is a finite set, representing a vocabulary with n words, and ν is counting measure over X . The statistic map τ converts word k into the kth basis vector ek. Let τ̂ ∈ Rn++ represent the mean parameters of a multinomial over this set; observe that\npθ(ei) = 〈τ(i), τ̂〉 = exp(〈ei, ln τ̂〉)− ln ∫ exp(〈τ(k), ln τ̂〉)dν(k).\nThat is to say, the canonical parameter vector is θ = ln τ̂ , the coordinate-wise logarithm of the mean parameters. Proposition 4.5 can be verified directly: (∇ψ(θ))i = eθi/ ∑ k e\nθk = τ̂ . Similarly, given another multinomial with mean parameters τ̂ ′ ∈ Rn++ and canonical parameters θ′ = ln τ̂ ′,\nK(pθ, pθ′) = n∑ i=1 τ̂i ln ( τ̂i τ̂ ′i ) .\nThe notation Rn++ means strictly positive coordinates: no word can have zero probability. Without this restriction, it is not possible to map into the canonical parameter space. This is precisely the scenario the smoothing methods of Section 5 will work around: the provided clusters are on the relative boundary of dom(ψ∗), which is either not part of dom(ψ∗) at all, or as is the case here, causes degenerate Bregman divergences (infinite valued, and lacking subgradients). ♦\nRemark 4.8. The multinomials in Example 4.7 have an overcomplete representation: scaling any canonical parameter vector by a constant gives the same\nmean parameter. In general, if two relative interior canonical parameters θ1 6= θ2 have a common subgradient τ̂ ∈ ∂ψ(θ1) ∩ ∂ψ(θ2), then it follows that {θ1, θ2} ⊂ ∂ψ∗(τ̂) (Rockafellar, 1970, Theorem 23.5). That is to say: this scenario leads to mean parameters which have distinct subgradients, and are thus points of nondifferentiability within ri(dom(ψ∗)), which necessitate the generalized development of Bregman divergences in this manuscript. ♦\nA further example of Gaussians appears in Appendix C.\nThe second motivation for ∆ψ∗,τ is an interpretation in terms of model fit.\nTheorem 4.9. Fix some measurable statistic map τ , and let two finite subsets C1, C2 of X be given with mean parameters {τ(C1), τ(C2)} = {τ̂1, τ̂2} ⊆ ri(dom(ψ∗)). Choose any canonical parameters θi ∈ ∂ψ∗(τ̂i), and for convenience set C3 := C1 ∪ C2, with mean parameter τ̂3 and any canonical parameter θ3 ∈ ∂ψ∗(τ̂3). Then\n∆ψ∗,τ (C1, C2) = ∑\ni∈{1,2} ∑ x∈Ci ln pθi(x)− ∑ x∈C3 ln pθ3(x)."
    }, {
      "heading" : "5. Smoothing",
      "text" : "The final piece of the technical puzzle is the smoothing procedure: most of the above properties — for instance, that Bf (τ(C1), τ(C2)) < ∞ — depend on τ(C2) ∈ ri(dom(f)). Relative boundary points lead to degeneracies; for example, this characterizes the Gaussian degeneracy identified in the introduction.\nDefinition 5.1. Given a (nonempty) convex set S, a statistic map τ : X → Rn is a smoothing statistic map for S if, given any finite set C ⊆ X , τ(C) ∈ ri(S). ♦\nIt turns out to be very easy to construct smoothing statistic maps.\nTheorem 5.2. Let a nonempty convex set S be given. Let τ0 : X → Rn be a statistic map satisfying, for any finite C ⊆ X , τ0(C) ∈ cl(S). Let z ∈ ri(S) and α ∈ (0, 1) be arbitrary. Given any finite set C ⊆ X , define the maps\nτ1(C) := (1− α)τ0(C) + αz, τ2(C) := τ0(C) + αz,"
    }, {
      "heading" : "In general, τ1 is a smoothing statistic map for S. If additionally S is a convex cone, then τ2 is also a smoothing statistic map for S.",
      "text" : "The following two examples smooth Gaussians and multinomials via Theorem 5.2. The parameters α and\nz are chosen from data, and moreover satisfy ‖αz‖ ↓ 0 as the total amount of available data grows; that is to say, τ will more and more closely match τ0.\nExample 5.3 (Smoothing multinomials.). The mean parameters to a multinomial lie within the probability simplex, a compact convex set. As discussed in Example 4.7, only the relative interior of the simplex provides canonical parameters. According to Theorem 5.2, all that remains in fixing this problem is to determine αz.\nThe approach here is to interpret the provided multinomial τ0(C) = τ̂ as based on a finite sample of size m, and thus the true parameters lie within some confidence interval around τ̂ ; crucially, this confidence interval intersects the relative interior of the probability simplex. One choice is a Bernstein-based upper confidence estimate τ(C) = τ0(C) + O(1/m +√ p(1− p)/m), where p = 1/n. ♦\nExample 5.4 (Smoothing Gaussians.). In the case of Gaussians, as discussed in Example 4.2, only positive definite covariance matrices are allowed. But this set is a convex cone, so Theorem 5.2 reduces the problem to finding a sensible element to add in.\nConsider the case of singleton clusters. Adding a fullrank covariance matrix in to the observed zero covariance matrix is like replacing this singleton with a constellation of points centered at it. Equivalently, each point is replaced with a tiny Gaussian, which is reminiscent of nonparametric density estimation techniques. Therefore one option is to use bandwidth selection techniques; the experiments of Section 7 use the “normal reference rule” (Bowman & Azzalini, 1997, Section 2.4.2), trying both the approach of estimating a bandwidth for each coordinate (suffix -nd), and computing one bandwidth for every direction uniformly, and simply adding a rescaling of the identity matrix to the sample covariance (suffix -n). ♦\nWhen there is a probabilistic interpretation of the clusters, and in particular τ(C) may be viewed as a maximum likelihood estimate, another approach is to choose some prior over the parameters, and have τ produce a MAP estimate which also lies in the relative interior. As stated, this approach will differ slightly from the one presented here: the weight on the added element will scale with the cluster size, rather than the size of the full data, and the relationship of τ(C1∪C2) to τ(C1) and τ(C2) becomes less clear."
    }, {
      "heading" : "6. Clustering algorithm",
      "text" : "The algorithm appears in Algorithm 1. Letting T∆f,τ denote an upper bound on the time to calculate a\nAlgorithm 1 Agglomerate. Input Merge cost ∆f,τ , points {xi}mi=1 ⊆ X . Output Hierarchical clustering.\nInitialize forest as F := {{xi} : i ∈ [m]}. while |F | > 1 do\nLet {Ci, Cj} ⊆ F be any pair minimizing ∆f,τ (Ci, Cj), as computed by Proposition 3.8. Remove {Ci, Cj} from F , add in Ci ∪ Cj .\nend while return the single tree within F .\nsingle merge cost, a brute-force implementation (over m points) takes space O(m) and time O(m3T∆f,τ ), whereas caching merge cost computations in a minheap requires space O(m2) and time O(m2(lg(m) + T∆f,τ )). Please refer Appendix E for more notes on running times, and a depiction of sample hierarchies over synthetic data.\nIf Proposition 3.8 is used to compute ∆f,τ , then only the sizes and means of clusters need be stored, and computing this merge cost involves just two Bregman divergences calculations. As the new mean is a convex combination of the two old means, computing it takes time O(n). The Bregman cost itself can be more expensive; for instance, as discussed with Gaussians in Appendix C, it is necessary to invert a matrix, meaning O(n3) steps."
    }, {
      "heading" : "7. Empirical results",
      "text" : "Trees generated by Agglomerate are evaluated in two ways. First, cluster compatibility scores are computed via dendrogram purity and initialization quality for EM upon mixtures of Gaussians. Secondly, cluster features are fed into supervised learners.\nThere are two kinds of data: Euclidean (points in some Rn), and text data. There are three Euclidean data sets: UCI’s glass (214 points in R9); 3s and 5s from the mnist digit recognition problem (1984 training digits and 1984 testing digits in R49, scaled down from the original 28x28); UCI’s spambase data (2301 train and 2300 test points in R57). Text data is drawn from the 20 newsgroups data, which has a vocabulary of 61,188 words; a difficult dichotomy (20n-h), the pair alt.atheism/talk.religion.misc (856 train and 569 test documents); an easy dichotomy (20n-e), the pair rec.sport.hockey/sci.crypt (1192 train and 794 test documents). Finally, 20n-b collects these four groups into one corpus.\nThe various trees are labeled as follows. s-link and c-link denote single and complete linkage, where l1\ndistance is used for text, and l2 distance is used for Euclidean data. km is the Ward/k-means merge cost. g-n fits full covariance Gaussians, whereas dg-nd fits diagonal covariance Gaussians; smoothing follows the data-dependent scheme of Example 5.4. multi fits multinomials, with the smoothing scheme of Example 5.3."
    }, {
      "heading" : "7.1. Cluster compatibility",
      "text" : "Table 1 contains cluster purity scores, a standard dendrogram quality measure, defined as follows. For any two points with the same label l, find the smallest cluster C in the tree which contains them both; the purity with respect to these two points is the fraction of C having label l. The purity of the dendrogram is the weighted sum, over all pairs of points sharing labels, of pairwise purities. The glass, spam, and 20newsgroups data appear in Heller & Ghahramani (2005); although a direct comparison is difficult, since those experiments used subsampling and randomized purity, the Euclidean experiments perform similarly, and the text experiments fare slightly better here.\nFor another experiment, now assessing the viability of Agglomerate as an initialization to EM applied to mixtures of Gaussians, please see Appendix F."
    }, {
      "heading" : "7.2. Feature generation",
      "text" : "The final experiment is to use dendrograms, built from training data, to generate features for classification tasks. Given a budget of features k, the top k clusters {Ci}ki=1 of a specified dendrogram are chosen, and for any example x, the ith feature is ∆(Ci, {x}). Statistically, this feature measures the amount by which the model likelihood degrades when Ci is adjusted to accommodate x. The choice of tree was based on training set purity from Table 1. In all tests, the original features are discarded (i.e., only the k generated features are used).\nFigure 2 shows the performance of logistic regression classifiers using tree features, as well as SVD features. The stopping rule used validation set performance."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was graciously supported by the NSF under grant IIS-0713540."
    } ],
    "references" : [ {
      "title" : "Relative loss bounds for on-line density estimation with the exponential family of distributions",
      "author" : [ "Azoury", "Katy S", "Warmuth", "Manfred K" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Azoury et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Azoury et al\\.",
      "year" : 2001
    }, {
      "title" : "Clustering with Bregman divergences",
      "author" : [ "Banerjee", "Arindam", "Merugu", "Srujana", "Dhillon", "Inderjit S", "Ghosh", "Joydeep" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Banerjee et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2005
    }, {
      "title" : "Convex Analysis and Nonlinear Optimization",
      "author" : [ "Borwein", "Jonathan", "Lewis", "Adrian" ],
      "venue" : null,
      "citeRegEx" : "Borwein et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Borwein et al\\.",
      "year" : 2000
    }, {
      "title" : "Fundamentals of Statistical Exponential Families",
      "author" : [ "Brown", "Lawrence D" ],
      "venue" : "Insitute of Mathematical Statistics,",
      "citeRegEx" : "Brown and D.,? \\Q1986\\E",
      "shortCiteRegEx" : "Brown and D.",
      "year" : 1986
    }, {
      "title" : "Generalized minimizers of convex integral functionals, Bregman distance, Pythagorean identities",
      "author" : [ "Csiszár", "Imre", "Matúš", "Frantǐsek" ],
      "venue" : null,
      "citeRegEx" : "Csiszár et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Csiszár et al\\.",
      "year" : 2012
    }, {
      "title" : "Real analysis: modern techniques and their applicatins",
      "author" : [ "Folland", "Gerald B" ],
      "venue" : "Wiley Interscience,",
      "citeRegEx" : "Folland and B.,? \\Q1999\\E",
      "shortCiteRegEx" : "Folland and B.",
      "year" : 1999
    }, {
      "title" : "Algorithms for model-based gaussian hierarchical clustering",
      "author" : [ "C. Fraley" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Fraley,? \\Q1998\\E",
      "shortCiteRegEx" : "Fraley",
      "year" : 1998
    }, {
      "title" : "Hierarchical gaussian mixture model",
      "author" : [ "Garcia", "Vincent", "Nielsen", "Frank", "Nock", "Richard" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "Garcia et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Garcia et al\\.",
      "year" : 2010
    }, {
      "title" : "Approximate Solutions to Markov Decision Processes",
      "author" : [ "Gordon", "Geoff J" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Gordon and J.,? \\Q1999\\E",
      "shortCiteRegEx" : "Gordon and J.",
      "year" : 1999
    }, {
      "title" : "Bayesian hierarchical clustering",
      "author" : [ "Heller", "Katherine A", "Ghahramani", "Zoubin" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Heller et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Heller et al\\.",
      "year" : 2005
    }, {
      "title" : "Fundamentals of Convex Analysis",
      "author" : [ "Hiriart-Urruty", "Jean-Baptiste", "Lemaréchal", "Claude" ],
      "venue" : null,
      "citeRegEx" : "Hiriart.Urruty et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hiriart.Urruty et al\\.",
      "year" : 2001
    }, {
      "title" : "Hierarchical bayesian clustering for automatic text classification",
      "author" : [ "Iwayama", "Makoto", "Tokunaga", "Takenobu" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Iwayama et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Iwayama et al\\.",
      "year" : 1995
    }, {
      "title" : "Proximal minimization methods with generalized Bregman functions",
      "author" : [ "Kiwiel", "Krzysztof C" ],
      "venue" : "SIAM journal on control and optimization,",
      "citeRegEx" : "Kiwiel and C.,? \\Q1995\\E",
      "shortCiteRegEx" : "Kiwiel and C.",
      "year" : 1995
    }, {
      "title" : "Distributed Learning using Generative Models",
      "author" : [ "Merugu", "Srujana" ],
      "venue" : "PhD thesis, University of Texas, Austin,",
      "citeRegEx" : "Merugu and Srujana.,? \\Q2006\\E",
      "shortCiteRegEx" : "Merugu and Srujana.",
      "year" : 2006
    }, {
      "title" : "A Survey of Recent Advances in Hierarchical Clustering Algorithms",
      "author" : [ "Murtagh", "Fionn" ],
      "venue" : "The Computer Journal,",
      "citeRegEx" : "Murtagh and Fionn.,? \\Q1983\\E",
      "shortCiteRegEx" : "Murtagh and Fionn.",
      "year" : 1983
    }, {
      "title" : "Convex Analysis",
      "author" : [ "Rockafellar", "R. Tyrrell" ],
      "venue" : null,
      "citeRegEx" : "Rockafellar and Tyrrell.,? \\Q1970\\E",
      "shortCiteRegEx" : "Rockafellar and Tyrrell.",
      "year" : 1970
    }, {
      "title" : "Agglomerative information bottleneck",
      "author" : [ "Slonim", "Noam", "Tishby", "Naftali" ],
      "venue" : "pp. 617–623,",
      "citeRegEx" : "Slonim et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Slonim et al\\.",
      "year" : 1999
    }, {
      "title" : "Graphical Models, Exponential Families, and Variational Inference",
      "author" : [ "Wainwright", "Martin J", "Jordan", "Michael I" ],
      "venue" : "Now Publishers Inc.,",
      "citeRegEx" : "Wainwright et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Wainwright et al\\.",
      "year" : 2008
    }, {
      "title" : "Hierarchical grouping to optimize an objective function",
      "author" : [ "Ward", "Joe H" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Ward and H.,? \\Q1963\\E",
      "shortCiteRegEx" : "Ward and H.",
      "year" : 1963
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "There is already a rich theory of clustering with Bregman divergences, and in particular the relationship of these divergences with exponential family distributions (Banerjee et al., 2005).",
      "startOffset" : 165,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : "Section 3 will revisit the standard Bregman hard clustering model (Banerjee et al., 2005), and show how it naturally leads to a merge cost ∆.",
      "startOffset" : 66,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "A number of works present agglomerative schemes for clustering with exponential families, from the perspective of KL divergences between distributions, or the analogous goal of maximizing model likelihood, or lastly in connection to the information bottleneck method (Iwayama & Tokunaga, 1995; Fraley, 1998; Heller & Ghahramani, 2005; Garcia et al., 2010; Blundell et al., 2010; Slonim & Tishby, 1999).",
      "startOffset" : 267,
      "endOffset" : 401
    }, {
      "referenceID" : 7,
      "context" : "A number of works present agglomerative schemes for clustering with exponential families, from the perspective of KL divergences between distributions, or the analogous goal of maximizing model likelihood, or lastly in connection to the information bottleneck method (Iwayama & Tokunaga, 1995; Fraley, 1998; Heller & Ghahramani, 2005; Garcia et al., 2010; Blundell et al., 2010; Slonim & Tishby, 1999).",
      "startOffset" : 267,
      "endOffset" : 401
    }, {
      "referenceID" : 6,
      "context" : "A number of works present agglomerative schemes for clustering with exponential families, from the perspective of KL divergences between distributions, or the analogous goal of maximizing model likelihood, or lastly in connection to the information bottleneck method (Iwayama & Tokunaga, 1995; Fraley, 1998; Heller & Ghahramani, 2005; Garcia et al., 2010; Blundell et al., 2010; Slonim & Tishby, 1999). Furthermore, Merugu (2006) studied the same algorithm as the present work, phrased in terms of Bregman divergences.",
      "startOffset" : 294,
      "endOffset" : 430
    }, {
      "referenceID" : 1,
      "context" : "The development here of exponential families and related Bregman properties generalizes results found in a variety of sources (Brown, 1986; Azoury & Warmuth, 2001; Banerjee et al., 2005; Wainwright & Jordan, 2008); further bibliographic remarks will appear throughout, and in Appendix G.",
      "startOffset" : 126,
      "endOffset" : 213
    }, {
      "referenceID" : 1,
      "context" : "(This cost was the basis for Bregman hard clustering (Banerjee et al., 2005).",
      "startOffset" : 53,
      "endOffset" : 76
    } ],
    "year" : 2012,
    "abstractText" : "This manuscript develops the theory of agglomerative clustering with Bregman divergences. Geometric smoothing techniques are developed to deal with degenerate clusters. To allow for cluster models based on exponential families with overcomplete representations, Bregman divergences are developed for nondifferentiable convex functions.",
    "creator" : "pdfsam-console (Ver. 2.0.6e)"
  }
}