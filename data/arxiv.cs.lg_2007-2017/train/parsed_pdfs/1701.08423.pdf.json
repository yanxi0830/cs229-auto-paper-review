{
  "name" : "1701.08423.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 1.\n08 42\n3v 1\n[ cs\n.D S]\n• Distribution Stability (Awasthi, Blum, Sheffet, FOCS 2010) • Spectral Separability (Kumar, Kannan, FOCS 2010) • Perturbation Resilience (Bilu, Linial, ICS 2010) and show that Local Search performs well on the instances with the aforementioned stability properties. Specifically, for the k-means and k-median objective, we show that Local Search exactly recovers the optimal clustering if the dataset is 3+ ε-perturbation resilient, and is a PTAS for distribution stability and spectral separability. This implies the first PTAS for instances satisfying the spectral separability condition. For the distribution stability condition we also go beyond previous work by showing that the clustering output by the algorithm and the optimal clustering are very similar.\nThis is a significant step toward understanding the success of Local Search heuristics in clustering applications and supports the legitimacy of the stability conditions: They characterize some of the structure of real-world instances that make Local Search a popular heuristic.\n∗Supported by Deutsche Forschungsgemeinschaft within the Collaborative Research Center SFB 876, project A2"
    }, {
      "heading" : "1 Introduction",
      "text" : "Clustering is a ubiquitous problem. The aim is to partition data points according to similarity. From a practitioner’s point of view, the appropriateness of a particular objective function depends on the underlying structure. For instance, if the data is generated by a mixture of unit Gaussians, the problem is often modeled by the k-means problem. The appropriateness of a model gives clustering its easy-in-practice, hard-in-theory quality: On the one hand, a benchmark algorithm often yields a good clustering with an appropriate model. On the other hand, many clustering objectives are NP-hard to approximate. To bridge this gap, prior work usually proceeds in two steps: (1) characterize properties of a natural clustering of the underlying data and (2) design an algorithm leveraging such properties, which then bypass traditional hardness results. At this point, there is a wide variety of (1) characterizations of well-behaved instances and of (2) algorithms tuned to those instances.\nIn contrast, we proceed in the reverse order: (1) focus on a single, all-purpose algorithm that is already widely used in practice, and (2) prove that is works well for most models of well-clusterable instances for the k-median and k-means objective functions. The algorithm: a simple Local Search heuristic."
    }, {
      "heading" : "1.1 Our Contribution",
      "text" : "Loosely speaking, we show that:\nDistribution Stability Local Search achieves a (1 + ε)-approximation and “recovers most of the structure of the optimal solution” (see Theorem 1.1) – This is the first algorithm that achieves both a (1+ε)-approximation w.r.t to the cost function and a (1−ε)-approximate classification for most of the clusters.\nSpectral Separability Local Search achieves a (1+ ε)-approximation (see Theorem 1.3) – this is the first PTAS for the problem.\nPerturbation Resilience Any local optimum is a global optimum for α-perturbation-resilient instances with α > 3 (see Theorem 1.2).\nThis yields a unified and simple approach toward stability conditions. There are two possible highlevel interpretations of our results: (1) since Local Search heuristics are widely used by practitioners, our work shows that the three main stability conditions capture some of the structure of practical inputs that make Local Search efficient, giving more legitimacy to the stability conditions and (2) assuming that the stability conditions are legitimate (i.e.: characterize real-world instances), our results make a step toward understanding the success of Local Search heuristics.\nWe now proceed to a more formal exposition of our contribution. The problem we consider in this work is the following slightly more general version of the k-means and k-median problems.\nDefinition 1.1 (k-Clustering). Let A be a set of clients, F a set of centers, both lying a metric space (X ,dist), cost a function A×F → R+, and k a non-negative integer. The k-clustering problem asks for a subset S of F , of cardinality at most k, that minimizes\ncost(S) = ∑\nx∈A min c∈S cost(x, c).\nThe clustering of A induced by S is the partition of A into subsets C = {C1, . . . Ck} such that Ci = {x ∈ A | ci = argmin\nc∈S cost(x, c)} (breaking ties arbitrarily).\nThe well known k-median and k-means problems correspond to the special cases cost(a, c) = dist(a, c) and cost(a, c) = dist(a, c)2 respectively.\nIn this paper we will analyze the performance of the following widely-used Local Search algorithm (Algorithm 1) (see e.g.: [1] or [56]). This algorithm has a polynomial running time (see [9, 33]). In the following we will refer to its parameter ((ε) in the description of Algorithm 1) by the neighborhood size of Local Search.\nAlgorithm 1 Local Search(ε) for k-Median and k-Means\n1: Input: A,F, cost, k 2: S ← Arbitrary subset of F of cardinality at most k. 3: while ∃ S′ s.t. |S′| ≤ k and |S − S′|+ |S′ − S| ≤ 1/ε and cost(S′) ≤ (1− ε/n) cost(S) 4: do 5: S ← S′ 6: end while 7: Output: S\nVarious forms of stability conditions that should be satisfied by well-clusterable instances have been proposed, see Figure 1. The main three incomparable notions are distribution stability, perturbation resilience, and spectral separability.\nWe detail the different notions of stability and the results we obtain for each of them. Throughout the rest of this paper, let OPT denote the value of an optimal solution. We start with the notion of stability due to Awasthi et al. [10], called “distribution stability”.\nDefinition 1.2 (Distribution Stability [10]). Let (A,F, cost, k) be an input for k-clustering and let {C∗1 , . . . , C∗k} denote the optimal k-clustering of A with centers S = {c∗1, . . . c∗k}. Given β > 0, the\ninstance is β-distribution stable if, for any i, ∀x /∈ C∗i ,\ncost(x, c∗i ) ≥ β OPT\n|C∗i | .\nWe show that Local Search is a PTAS for β-distribution stable instances. Moreover, we show that for almost all clusters (i.e.: at least k−O(β−1ε−3)), the algorithm recovers most of the optimal clusters (i.e.: there is a bijection between the optimal clusters and the clusters of the algorithm such that a (1− ε) fraction of the points of each cluster agree). Theorem 1.1. There exists a constant c such that the following holds. Let β > 0 and ε < 1/2. For any β-stable instance, the solution output by Local Search(c1ε\n−3β−1) (Algorithm 1) has cost at most (1 + ε)OPT.\nMoreover, let C∗ = {C∗1 , . . . , C∗k} denote an optimal k-clustering and let L = {L1, . . . , Lk} denote the clustering output by Local Search(c1ε\n−3β−1). There exists a bijection φ : L 7→ C∗ such that for at least m = k − O(β−1ε−3) clusters L′1, . . . , L′m ⊆ L, we have both (1 − c1ε)|φ(L′1)| ≤ |L′1 ∩ φ(L′1)| and (1− c1ε)|L′1| ≤ |L′1 ∩ φ(L′1)|.\nA PTAS for β-distribution stable instances was previously given by Awasthi et al. [10]. Our contribution is to show that (1) Local Search is already a PTAS: no specific algorithm is needed, and (2) Local Search recovers most of the structure of the underlying optimal clustering. Moreover, βdistribution stability is also implied by “cost separation” as defined by Ostrovsky et al. [69], so Local Search is also a PTAS in their setting and also recovers most of the structure of such instances. Furthermore, our results extend to a slightly more general definition of β-distribution stability (where only a (1 − δ) > 1/2 fraction of the points of each cluster has to satisfy the β-distribution stability condition) at the expense of a (1 +O(δ)) factor in the approximation guarantee.\nWe now turn to the second notion of stability, called “perturbation resilience”.\nDefinition 1.3 (Perturbation Resilience [11]). Let (A,F, cost, k) be an input for k-clustering and let {C∗1 , . . . , C∗k} denote the optimal k-clustering of A with centers S = {c∗1, . . . c∗k}. Given α ≥ 1, the instance is α-perturbation-resilient if for any cost function cost′ on A with\n∀ (p, q) ∈ A× F, cost(p, q) ≤ cost′(p, q) ≤ α · cost(p, q),\n{C∗1 , . . . , C∗k} is the unique optimal clustering of the instance (A,F, cost′, k). This notion of stability was historically defined for cost = dist. We show that a local optimum must be the global optimum in that case. More precisely, consider a solution S0 to the k-clustering problem with parameter p. We say that S0 is 1/ε-locally optimal if for any solution S1 such that |S0 − S1|+ |S1 − S0| ≤ 2/ε, cost(S1) ≥ cost(S0). Theorem 1.2. Let α > 3. For any instance of the k-median problem that is α-perturbationresilient, any 2(α − 3)−1-locally optimal solution is the optimal clustering {C∗1 , . . . , C∗k}.\nWe extend this theorem to instances for which cost = distp for some constant p (in particular for p = 2, where the k-clustering instance corresponds to the k-means problem). An optimal algorithm for 2-perturbation resilient clustering for any center-based objective function was very recently given by Bakshi and Chepurko [14]. Our contribution is to show that Local Search is already optimal for 3 + ε-perturbation resilient instances; no specific algorithm is needed.1\nWe now turn to the third stability condition, called “spectral separation”.\n1We do not quite match [14]: One limitation is that Local Search is not necessarily optimal for 2-perturbation resilient instances, see Proposition 3.15\nDefinition 1.4 (Spectral Separation [60]2). Let (A,Rd, || · ||2, k) be an input for k-means clustering in Euclidean space and let {C∗1 , . . . C∗k} denote an optimal clustering of A with centers S = {c∗1, . . . c∗k}. Denote by C an n× d matrix such that the row Ci = argmin\nc∗j∈S ||Ai − c∗j ||2. Denote\nby || · ||2 the spectral norm of a matrix. Then {C∗1 , . . . C∗k} is γ-spectrally separated, if for any pair (i, j) the following condition holds:\n||c∗i − c∗j || ≥ γ ·\n\n 1 √\n|C∗i | +\n1 √\n|C∗j |\n\n ||A− C||2.\nSince this stability is defined over non-finite metric spaces, we require standard preprocessing steps in order to use Local Search, see Algorithm 2. They consist of reducing the number of dimensions and discretizing the space in order to bound the number of candidate centers.\nAlgorithm 2 Project and Local Search\n1: Project points A onto the best rank k/ε subspace 2: Embed points into a random subspace of dimension O(ε−2 log n) 3: Compute candidate centers (Corollary 1) 4: Local Search(Θ(ε−4)) 5: Output clustering\nTheorem 1.3. Let (A,Rd, || · ||2, k) be an instance of Euclidean k-means clustering with optimal clustering C = {C∗1 , . . . C∗k} and centers S = {c∗1, . . . c∗k}. If C is more than 3 √ k-spectrally separated, then Algorithm 2 is a polynomial time approximation scheme.\nIn previous work by Kumar and Kannan [60], an algorithm was given with approximation ratio 1+O(OPTk/OPTk−1), where OPTi denotes the value of an optimal solution using i centers. Assuming that OPTk/OPTk−1 ≤ ε implies that the optimal k-clustering C is Ω( √\nk/ε)-spectrally separated [60]. Thus our assumption in Theorem 1.3 that C is √ k-spectrally separated is weaker (it does not depend on ε) and therefore our result is stronger since the approximation guarantee does not depend on the assumption about instances. We obtain a PTAS. We note that the previous algorithms focused on recovering the target optimal clustering and not optimizing the k-means objective function, though there exists some overlap. In general, a (1 + ε)-approximation does not have to agree with the target clustering on a majority of points, see Remark 4.0.1. There are applications where finding the correct classification is more relevant and applications where minimizing the value of the k-means objective is relevant (in image compression for example, see e.g.: [56]).\nThe main message is that Local Search occupies a sweet spot between practical performance and theoretical guarantees, both with respect to worst case instances and with respect to stable instances for various notions of stability. More boldly, our work indicates that many formal characterizations of practical instances can be, at least to some degree, viewed as “instances for which Local Search works well”. It supports the definition of stability conditions as conditions characterizing real-world inputs since Local Search heuristics are very popular among practitioners.\nWhile the (worst case) running time bounds given in this work might appear too high for realworld applications, we consider this view as possibly too pessimistic, given that there is a trade-off between how “stable” the instances are and the quality of approximation. Hence if instances are\n2The proximity condition of Kumar and Kannan [60] implies the spectral separation condition.\nhighly stable (i.e.: the parameter of the stability condition is high), Local Search does not require a large neighborhood size to output a nearly-optimal solution."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "The problems we study are NP-hard: k-median and k-means are already NP-hard in the Euclidean plane (see Meggido and Supowit [67], Mahajan et al. [64], and Dasgupta and Freud [38]). In terms of hardness of approximation, both problems are APX-hard, even in the Euclidean setting when both k and d are part of the input (see Gua and Khuller [44], Jain et al. [51], Guruswami et al. [47] and Awasthi et al. [12]). On the positive side, constant factor approximations are known in metric space for both k-median and k-means (see for example [63, 52, 68]).\nGiven the hardness results, how can one hope to obtain a (1+ε)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.\nCost Separation In one of the earliest attempts to formalize the notion of a meaningful clustering, Ostrovsky et al. [69] assumed that cost of an optimal clustering with k centers is smaller than an ε2-fraction of the cost of an optimal clustering with k− 1 centers, see also Schulman [72] for an earlier condition for two clusters and the irreducibility condition by Kumar et al. [61]. The condition has several appealing properties. It is robust against small perturbations of the data set and it implies that two low-cost clusterings agree on a large fraction of points. It is also motivated by the commonly used elbow method of determining the correct value of k: run an algorithm for an incrementally increasing number of clusters until the cost drops significantly. The popular D2 sampling technique (also known as k-means++) has an improved performance for cost separated instances compared to the worst-case O(log k)-approximation ratio [7, 28, 53, 69]. A related, slightly weaker condition called α-weakly-deletion stability was introduced by Awasthi et al. [10] where the cost of assigning all the points from one cluster in the optimal k-clustering to another center increases the objective by some factor (1 + α).\nTarget-Based Clustering The notion of finding a target clustering is more prevalent in machine learning than minimizing an objective function. Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73]. Balcan et al. [15, 16] gave a deterministic condition called approximation stability under which a target clustering can be retrieved by via any sufficiently good algorithm for the k-means (or kmedian) objective function. The distance between two clusterings with k clusters is the minimum number of disagreements under all perfect matchings of clusters. Two clustering are ε-close if the “edit distance” is at most ε · n. If the instance satisfies that any clustering with cost within a factor c of the optimum is ε-close to the target clustering, it is called (c, ε)-stable. The condition was extended to account for the presence of noisy data by Balcan et al. [20]. For results using approximation stability, see [16, 17, 71, 3, 10].\nAnother deterministic condition that relates target clustering recovery via the k-means objective was introduced by Kumar and Kannan [60]. Viewing each data point as a row of a matrix A and the rows of the center matrix K containing the centroid of the respective target cluster of A, it imposes a proximity condition on each point Ai when projected onto the line connecting its target centroid cj and some other centroid cℓ. The condition requires that the projection of Ai is closer\nto cj than to any cℓ by a factor of Ω\n( k · (\n1√ |Cj | + 1√ |Cℓ|\n) · ||A−K||2 ) , where ||A − K||2 is the\nspectral norm and Cj and Cℓ are the target clusters. This condition implies spectral separability with γ ∈ Ω(k). For further spectral based approaches, see also [13].\nPerturbation Resilience The notion behind this stability condition is that bounded modifications to the input should not affect the optimum solution. Bilu et al. [26, 25] formalized this as allowing edge weights in a graph to be modified by a factor of at most γ without changing the maxcut. Perturbation resilience has some similarity to smoothed analysis (see Arthur et al. [6, 8] for work on k-means). The main difference is that smoothed analysis takes a worst case instance and applies a random perturbation, while perturbation resilience takes a well-behaved instance and applies an adversarial perturbation. For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].\nLocal Search There exists a large body of bicriteria approximations for k-median and k-means [59, 30, 33, 21]. Arya et al. [9] (see also [46]) gave the first analysis showing that Local Search with a neighborhood size of 1/ε gives a 3 + 2ε approximation to k-median and showed that this bound is tight. Kanungo et al. [56] proved an approximation ratio of 9 + ε for Euclidean k-means clustering by Local Search, currently the best known algorithm with a polynomial running time in metric and Euclidean spaces.3 Recently, Local Search with an appropriate neighborhood was shown to be a PTAS for k-means and k-median in certain restricted metrics including constant dimensional Euclidean space [42, 32]. Due to its simplicity, Local Search is also a popular subroutine for clustering tasks in various computational models [22, 27, 45]. For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74]"
    }, {
      "heading" : "2 Approach and Techniques",
      "text" : ""
    }, {
      "heading" : "2.1 Distribution Stability",
      "text" : "The first important observation is that only a few clusters have more than a 1/ε−3 fraction of the total cost of the solution. For these clusters, Local Search with appropriate an neighborhood size will find the optimal solution. Among the remaining clusters, the centers and points close to the center are far away from each other. Any locally optimal solution cannot err on too many of these clusters. The cost of charging the points of the remaining clusters can then be charged into the overall contribution, allowing us to bound the approximation factor, see Figure 2. Our proof includes a few ingredients from [10] such as the notion of inner-ring (we work with a slightly more general definition) and distinguishing between cheap and expensive clusters. However, our analysis is more general as it allows us to analyze not only the cost of the solution of the algorithm, but also the structure of the clusters.\nEuclidean inputs can be straightforwardly “discretized” by computing an appropriate candidate set of centers, for instance via Matousek’s approximate centroid set [65] and then applying the Johnson-Lindenstrauss lemma, if the dimension is too large.\n3They combined Local Search with techniques from Matousek [65] for k-means clustering in Euclidean spaces. The running time of the algorithm as stated incurs an additional factor of ε−d due to the use of Matousek’s approximate centroid set. Using standard techniques (see e.g. Section B of this paper), a fully polynomial running time in n, d, and k is also possible without sacrificing approximation guarantees."
    }, {
      "heading" : "2.2 Perturbation Resilience",
      "text" : "The tight approximation factor of 3 + ε of Local Search for k-median implies a locality gap4 of 3. The main observation is that perturbation resilience implies that locally optimal solutions for a local neighborhood of appropriate size are equal to the global optimum."
    }, {
      "heading" : "2.3 Spectral Separability",
      "text" : "In Section 4 we study the spectral separability conditions for the Euclidean k-means problem. Nowadays, a standard preprocessing step in Euclidean k-means clustering is to project onto the subspace spanned by the rank k-approximation. Indeed, this is the first step of the algorithm by Kumar and Kannan [60] (see Algorithm 3).\nAlgorithm 3 k-means with spectral initialization [60]\n1: Project points onto the best rank k subspace 2: Compute a clustering C with constant approximation factor on the projection 3: Initialize centroids of each cluster of C as centers in the original space 4: Run Lloyd’s k-means until convergence\nIn general, projecting onto the best rank k subspace and computing a constant approximation on the projection results in a constant approximation in the original space. Kumar and Kannan [60] and later Awasthi and Sheffet [13] gave tighter bounds if the spectral separation is large enough. Our algorithm omits steps 3 and 4. Instead, we project onto slightly more dimensions and subsequently use Local Search as the constant factor approximation in step 2. To utilize Local Search, we further require a candidate set of solutions, which is described in Section B. For pseudocode, we refer to Algorithm 2.\nIt is easy to show that spectral separability implies distribution stability if the dimension is of or-\nder k: (1) the distance between centers is Ω(\n(\n1√ |C∗i | + 1√|C∗j | ) k·||A−C||2) = Ω (( 1√ |C∗i | + 1√|C∗j | )√ OPT ) ,\nand (2) the distance of any point to the “wrong” center is at least 1/2 of this amount, i.e. the cost of assigning a point to the “wrong” cluster is Ω(OPT|Cj | ). Projecting onto sufficiently many dimensions allows us to transform a high dimensional point set into a low dimensional one, see for instance recent work by Cohen et al. [31]. The projection retains the cost and spectral separability of a clustering, however it does not preserve optimality. In particular, the distance of a single point to the centroids of the clusters can be arbitrarily distorted (see Figure 3), which prevents us from using the naive reduction to distribution stability.\nInstead, we locally improve on the optimal clustering by reassigning points (Lemma 4.1). A large contraction of relevant distances can only happen for few points, i.e. the cluster sizes are roughly the same. For the remaining points, we can show that they are guaranteed to have a minimum distance to the wrong center.\n4The locality gap is the maximum ratio between a local optimum and a global optimum."
    }, {
      "heading" : "3 Metric Spaces",
      "text" : ""
    }, {
      "heading" : "3.1 Preliminaries",
      "text" : "In this section, we consider inputs that consist in both a set of clients A and a set of candidate centers F , together with a distance function dist : A ∪ F × A ∪ F → R+5. Let p ≥ 1. We assume that the cost function is defined as cost(a, b) = dist(a, b)p, for any (a, b) ∈ A × F . Observe that this is the case for the k-median and k-means problems with p = 1 and 2 respectively.\nTo give a slightly simpler proof we will assume that p = 1. Applying the two following lemmas at different steps of the proof ensure that the result holds for higher value of p (by introducing a dependency in 1/εO(p) in the neighborhood size of the algorithm).\nLemma 3.1. Let p ≥ 0 and 1/2 > ε > 0. For any a, b, c ∈ A ∪ F , we have cost(a, b) ≤ (1 + ε)pcost(a, c) + cost(c, b)/εp.\nThe following lemma follows from the binomial theorem.\nLemma 3.2. Let p ≥ 0. For any a, b, c ∈ A ∪ F , we have cost(a, b) ≤ 2p(cost(a, c) + cost(c, b))."
    }, {
      "heading" : "3.2 Distribution Stability",
      "text" : "We work with the notion β, δ-distribution stability which generalizes β-distribution stability. This extends our result to datasets such that for each cluster of the optimal solution, most of the points satisfy the β-distribution stability condition.\nDefinition 3.1 ((β, δ)-Distribution Stability). Let (A,F, cost, k) be an instance of k-clustering where A ∪ F lie in a metric space and let C∗ = {C∗1 , . . . , C∗k} denote a partition of A and S∗ = {c∗1, . . . , c∗k} ⊆ F be a set of centers. Further, let β > 0 and 1/2 > δ. Then (A,F, cost, k), (C∗, S∗) is a (β, δ)-distribution stable instance if, for any i, there exists a set ∆i ⊆ C∗i such that |∆i| ≥ (1− δ)|C∗i | and for any x ∈ ∆i, for any j 6= i,\ncost(x, c∗j ) ≥ β OPT\n|C∗j | ,\nwhere cost(x, c∗j ) is the cost of assigning x to c ∗ j .\nFor any (A,F, cost, k), (C∗, S∗) (β, δ)-distribution stable instance, we refer to (C∗, S∗) as a (β, δ)-clustering of the instance. We show the following theorem.\nTheorem 3.3. Let p > 0, there exists a constant c1 such that the following holds. Let β > 0, δ < 1/2. For any ε < 1/2 and (β, δ)-stable instance with (β, δ) clustering (C∗, S∗), the cost of the solution output by Local Search(2ε−3β−1) (Algorithm 1) is at most (1 + c1(ε+ δ))cost(C).\nMoreover, let L = {L1, . . . , Lk} denote the clusters of the solution output by Local Search(2ε−3β−1). There exists a bijection φ : L 7→ C∗ such that for at least m = k−O(β−1ε−3) clusters L′1, . . . , L′m ⊆ L, we have (1− c1(ε+ δ))|φ(L′1)|, (1 − c1(ε+ δ))|L′1| ≤ |L′1 ∩ φ(L′1)|.\nNote that Theorem 1.1 is an immediate corollary of Theorem 3.3 by taking δ = 0. For ease of exposition, we give the proof of Theorem 3.3 for δ = 0 (see Section A for the general proof). Throughout this section we consider a set of centers S∗ = {c∗1, . . . , c∗k} whose induced clustering is C∗ = {C∗1 , . . . , C∗k} and such that the instance is (β, δ)-stable with respect\n5This distance function is the metric completion of the dist function described in the introduction: ∀a, b ∈ A,dist(a, b) = minc∈F dist(a, c) + dist(c, f) and analogously for elements of F .\nto this clustering. We denote by clusters the parts of a partition C∗ = {C∗1 , . . . , C∗k}. Let cost(C∗) =\n∑k i=1 ∑ x∈C∗i cost(x, c ∗ i ). Moreover, for any cluster C ∗ i , for any client x ∈ C∗i , de-\nnote by gx the cost of client x in solution C ∗: gx = cost(x, c∗i ) = dist(x, c ∗ i ) since p = 1. Let L denote the output of LocalSearch(β−1ε−3) and l(x) the cost induced by client x in solution L, namely lx = minℓ∈L cost(x, ℓ). The following definition is a generalization of the inner-ring definition of [10].\nDefinition 3.2. For any ε0, we define the inner ring of cluster i, IR ε0 i , as the set of x ∈ A ∪ F such that dist(x, c∗i ) ≤ ε0βOPT/|C∗i |.\nWe say that cluster i is cheap if ∑ x∈C∗i gx ≤ ε 3βOPT, and expensive otherwise. We aim at\nproving the following structural lemma.\nLemma 3.4. There exists a set of clusters Z∗ ⊆ C∗ of size at most (ε−3 + 160ε−1)β−1 such that for any cluster C∗i ∈ C∗ − Z∗, we have the following properties\n1. C∗i is cheap.\n2. At least a (1− ε) fraction of IRε2i are served by a unique center L(i) in solution L.\n3. The total number of clients p ∈ ⋃j 6=i∆j, that are served by L(i) in L is at most ε|IRε 2 i |.\nSee Fig 2 for a typical cluster of C∗ −Z∗. We start with the following lemma which generalizes Fact 4.1 in [10].\nLemma 3.5. Let C∗i be a cheap cluster. For any ε0, we have |IRε0i | > (1− ε3/ε0)|C∗i |.\nWe then prove that the inner rings of cheap clusters are disjoint.\nLemma 3.6. Let ε0 < 1/3. If C ∗ i 6= C∗j are cheap clusters, then IRε0i ∩ IRε0j = ∅.\nThe following observation follows directly from the definition of cheap clusters.\nLemma 3.7. Let Z1 ⊆ C∗ be the set of clusters of C∗ that are not cheap. Then |Z1| ≤ ε−3β−1.\nFor each cheap cluster C∗i , let L(i) denote a center of L that belongs to IRεi if there is one (and remain undefined otherwise). By Lemma 3.6, L(i) 6= L(j) for i 6= j.\nLemma 3.8. Let C∗−Z2 denote the set of clusters C∗i that are cheap, such that L(i) is defined, and such that at least (1−ε)|IRε2i | clients of IRε 2 i are served in L by L(i). Then |Z2| ≤ (ε−3+120ε−1)β−1.\nProof. We distinguish five types of clusters in C∗: expensive clusters (k1), cheap clusters with L(i) undefined (k2), cheap clusters with exactly one center of L in IRεi − IR (1−ε)ε i (k3), cheap clusters with exactly one center of L in IR(1−ε)εi (k4), and cheap clusters with at least two centers of L in IRεi (k5). Since L and C∗ both have k clusters and the inner rings of cheap clusters are disjoint (Lemma 3.6), we have k5 ≤ k1 + k2. By Lemma 3.7, we have k1 ≤ ε−3β−1.\nWe now bound the number k2 of cheap clusters such that L(i) is undefined. Consider a cheap cluster C∗i ⊆ C∗ − Z1 such that at least a (1 − ε) fraction of the clients of IRε 2\ni are served in L by some centers that are either in IRεi − IR(1−ε)εi or not in IRεi . By the triangular inequality, the cost for any client c in IRε 2\ni is at least ((1 − ε)ε − ε2)βOPT/|C∗i |. Since ε ≤ 1/2, it is at least εβOPT/(4|C∗i |). Since at least (1 − ε)|IRε 2 i | clients of IRε 2 i are served by centers that are not in IRεi , the total cost in L induced by those clients is at least (1− ε)|IRε 2\ni |εβOPT/(2|C∗i |). By Lemma 3.5, substituting |IRε2i | yields,\n(1− ε)|IRε2i |εβ OPT 2|C∗i | ≥ (1− ε)(1 − ε)|C∗i |εβ OPT 2|C∗i | ≥ εβOPT 8\nsince ε ≤ 1/2. Now, observe that by [9], the cost of L is at most a 5 approximation to the cost of OPT in the worst case. Thus, k2 ≤ 40(ε−1β−1).\nBy a similar argument, we can bound the number of clusters k4 such that a (1 − ε) fraction the clients in IRε 2\ni are served by a center not in IR (1−ε)ε i . We have k4 ≤ 80(ε−1β−1) since ε < 1/2.\nFor the remaining clusters, we have that there is a unique center located in IR (1−ε)ε i and that IRεi − IR (1−ε)ε i does not contain any center. Additionally, at least (1 − ε)|IRε 2 i | clients in IRε 2 i are served by a center in IRεi and so in IR (1−ε)ε i . Thus, by the triangular inequality we have that at least (1 − ε)|IRε2i | clients in IRε 2 i are served by the unique center in IR ε i , namely L(i). It follows that |Z2| ≤ (ε−3 + 120ε−1)β−1.\nWe continue with the following lemma, whose proof relies on similar arguments.\nLemma 3.9. There exists a set Z3 ⊆ C∗ − Z2 of size at most (ε−3 + 40ε−1)β−1 such that for any cluster C∗j ∈ C∗ − Z3, the total number of clients x ∈ ⋃\ni 6=j ∆i, that are served by L(j) in L is at most ε|IRε2i |.\nTherefore, the proof of Lemma 3.4 follows from combining Lemmas 3.7,3.8 and 3.9. We now turn to the analysis of the cost of L. Let C(Z∗) = ⋃C∗i ∈Z∗ C ∗ i . For any cluster\nC∗i ∈ C∗ − Z∗, let L(i) be the unique center of L that serves a set of clients Ai ⊆ IRε 2 i such that\n|Ai| ≥ (1 − ε)|IRε 2 i |. Let L̂ = ⋃ C∗i ∈C∗−Z∗ L(i) and L̄ = L − L̂. Define Ā and Â to be the set of clients that are served in solution L by centers of L̄ and L̂ respectively. Finally, let A(L(i)) be the set of clients that are served by L(i) in solution L. Observe that the A(L(i)) partition Â. Lemma 3.10. We have\n−εcost(L)/n+ ∑\nx∈Ā∪C(Z∗) lx ≤\n∑\nx∈Ā∪C(Z∗) gx + ε(cost(L) + cost(C∗))/(1 − ε)2.\nProof. Consider the following mixed solution M = L̂∪{c∗i ∈ Z∗ | C∗i ∈ Z∗}. We start by bounding the cost of M. For any client x /∈ Ā ∪ C(Z∗), the center that serves it in L belongs to M. Thus its cost is at most lx. Now, for any client x ∈ C̄(Z∗), the center that serves it in Z∗ is in M, so its cost is at most gx.\nFinally, we evaluate the cost of the clients in Ā−C(Z∗). Consider such a client x and let C∗i be the cluster it belongs to in solution C∗. By definition of Ā we have that C∗i /∈ Z∗. Therefore, L(i) is defined and so we have L(i) ∈ L̂ ⊆ M. Hence, the cost of x in M is at most cost(x,L(i)). Observe that by the triangular inequality dist(x,L(i)) ≤ dist(x, c∗i ) + dist(c∗i ,L(i)) = gx + dist(c∗i ,L(i)).\nNow consider a client x′ ∈ C∗i ∩Ai. By the triangular inequality, we have that cost(c∗i ,L(i)) ≤ cost(c∗i , x ′) + cost(x′,L(i)) = gx′ + lx′ . Hence,\ncost(c∗i ,L(i)) ≤ 1 |C∗i ∩Ai| ∑\nx′∈C∗i ∩Ai (gx′ + lx′).\nIt follows that assigning the clients of Ā ∩ C∗i to L(i) induces a cost of at most ∑\nx∈Ā∩C∗i\ngx + |C∗i ∩ Ā| |C∗i ∩Ai| ∑\nx′∈C∗i ∩Ai (gx′ + lx′).\nBy Lemma 3.8 and the definition of Z∗, we have that |C∗i ∩ Ā|/|C∗i ∩ Ai| ≤ ε/(1 − ε)2. Summing over all clusters C∗i /∈ Z∗, we obtain that the cost in M for the clients in Ā ∩ C∗i is at most\n∑\nc∈Ā−C(Z∗) gx +\nε\n(1− ε)2 (cost(C ∗) + cost(L)).\nBy Lemmas 3.8,3.9, we have that |M − L| + |L − M| ≤ 3(ε−3 + 40ε−1)β−1. Thus, by local optimality (1− ε/n)cost(L) ≤ cost(M). Therefore, combining the above observations, we have\n(1− ε n )cost(L) ≤\n∑\nx/∈Ā∪C(Z∗) lx +\n∑\nx∈Ā∪C(Z∗) gx +\nε\n(1− ε)2 (cost(C ∗) + cost(L))\n− ε n cost(L) +\n∑\nx/∈Ā∪C(Z∗) lx +\n∑\nx∈Ā∪C(Z∗) lx ≤\n∑\nx/∈Ā∪C(Z∗) lx +\n∑\nx∈Ā∪C(Z∗) gx +\nε\n(1− ε)2 (cost(C ∗) + cost(L)).\n− ε n cost(L) +\n∑\nx∈Ā∪C(Z∗) lx ≤\n∑\nx∈Ā∪C(Z∗) gx +\nε\n(1− ε)2 (cost(L) + cost(C ∗)).\nWe now turn to evaluate the cost for the clients that are not in Ā ∪ C(Z∗), namely the clients in Â−C(Z∗). For any cluster C∗i , for any x ∈ C∗i −∆i define Reassign(x) to be the distance from x to the center in L that is the closest to c∗i . Before going deeper in the analysis, we need the following lemma.\nLemma 3.11. For any δ < 1/2, we have for any C∗i ,\n∑\nx∈C∗i −∆i Reassign(x) ≤\n∑\nx∈C∗i −∆i gx +\nδ\n(1− δ) ∑\nx∈C∗i\n(lx + gx).\nProof. Consider a client x ∈ C∗i − ∆i. Let ℓ′ be the center that serves at least one client of ∆i that is the closest to c∗i . Since δ < 1, ℓ\n′ is well defined. By the triangular inequality we have that Reassign(x) ≤ cost(x, ℓ′) ≤ cost(x, c∗i ) + cost(c∗i , ℓ′) = gx + cost(c∗i , ℓ′). Then,\n∑\nx∈C∗i −∆i Reassign(x) ≤\n∑\nx∈C∗i −∆i gx + |C∗i −∆i| · cost(c∗i , ℓ′).\nNow, since ℓ′ is the center that serves at least one client of ∆i that is the closest to c∗i we have that for any x ∈ ∆i, by the triangular inequality cost(c∗i , ℓ′) ≤ cost(c∗i , x) + cost(x, ℓ′) ≤ gx + lx. Therefore,\ncost(c∗i , ℓ ′) ≤ 1|∆i| ∑\nx∈∆i (gx + lx).\nCombining, we obtain\n∑\nx∈C∗i −∆i Reassign(x) ≤\n∑\nx∈C∗i −∆i gx +\n|C∗i −∆i| |∆i| ∑\nx∈∆i (gx + lx)\n= ∑\nx∈C∗i −∆i gx +\nδ\n(1− δ) ∑\nx∈∆i (gx + lx),\nby definition of (β, δ)-stability.\nWe now partition the clients of cluster C∗i ∈ C∗ − Z∗. For any i, let ∆̄i be the set of clients of C∗i that are served in solution L by a center L(j) for some j 6= i and C∗j ∈ C∗ − Z∗. Moreover, let ∆̃i = (A(L(i)) ∩ ( ⋃ j 6=i ∆̄j))− C(Z∗). Finally, define C̃∗i = C∗i − (Ā ∪ ⋃ j 6=i ∆̃j). Lemma 3.12. Let C∗i be a cluster in C ∗ − Z∗. Define the solution Mi = L − {L(i)} ∪ {c∗i } and denote by mic the cost of client c in solution Mi. Then ∑\nx∈A mix ≤\n∑\nx/∈A(L(i))∪C̃∗i\nlc + ∑\nx∈C̃∗i\ngx + ∑\nx∈∆̃i\nReassign(x) + ∑\nx∈A(L(i))− (C̃∗i ∪∆̃i)\nlx + ε (1− ε) ( ∑\nx∈C̃∗i\ngx + lx).\nProof. Assume towards contradiction that this is not true. Consider a client x ∈ IRε0i ∩ IRε0j . Without loss of generality assume |C∗i | ≥ |C∗j |. By the triangular inequality we have cost(c∗j , c∗i ) ≤ cost(c∗j , x) + cost(x, c ∗ i ) ≤ 2ε0βOPT/|C∗j |. Since δ < 1/2, there exists a client c′ ∈ IRε0i ∩∆i. Thus, we have cost(c′, c∗j ) ≤ 3ε0βOPT/|C∗j |. This is a contradiction to the assumption that the instance is (β, δ)-distribution stable with respect to (C∗, S∗) and c′ being in ∆i for any ε0 < 1/3.\nWe can thus prove the following lemma, which concludes the proof.\nLemma 3.13. There exists a constant η such that\n−ε · cost(L) + ∑\nx∈Â−C(Z∗)\nlx ≤ ∑\nx∈Â−C(Z∗)\ngx + (η(ε+ δ 1− δ ))(cost(L) + cost(C ∗)).\nThe proof of Theorem 3.3 follows from (1) observing that Ā∪Â = A and summing the equations from Lemmas 3.10 and 3.13 and (2) Lemma 3.4."
    }, {
      "heading" : "3.3 α-Perturbation-Resilient Instances",
      "text" : "We consider the standard definition of α-perturbation-resilient instances.\nDefinition 3.3. Let I = (A,F, cost, k) be an instance for the k, p-clustering problem. For α ≥ 1, I is α-perturbation-resilient if there exists a unique optimal clustering {C∗1 , . . . , C∗k} and for any instance I ′ = (A,F, cost′, k, p), such that\n∀ p, q ∈ P, cost(p, q) ≤ cost′(p, q) ≤ αdist(p, q),\nthe unique optimal clustering is {C∗1 , . . . , C∗k}.\nObserve that cost′ in Definition 3.3 needs not be a metric. In the following, we assume that cost(a, b) = dist(a, b)p for some fixed p and some distance function dist defined over A∪F . Consider a solution S0 to the k-clustering problem with parameter p. We say that S0 is 1/ε-locally optimal if any solution S1 such that |S0 − S1|+ |S1 − S0| ≤ 2/ε has cost at least cost(S0). Theorem 1.2. Let α > 3. For any instance of the k-median problem that is α-perturbationresilient, any 2(α− 3)−1-locally optimal solution is the optimal clustering {C∗1 , . . . , C∗k}.\nFor ease of exposition, we give the proof for the k-median problem, when p = 1. Applying Lemma 3.1 in the proof of Lemma C.1 yields the results for general p with α growing exponentially with p. Moreover, define lc to be the cost for client c in solution L and gc to be its cost in the optimal solution C∗. Finally, for any sets of centers S and S0 ⊂ S, define NS(S0) to be the set of clients served by a center of S0 in solution S, i.e.: NS(S0) = {x | ∃s ∈ S0,dist(x, s) = mins′∈S dist(x, s′)}.\nThe proof of Theorem 1.2 relies on the following theorem of particular interest.\nTheorem 3.14 (Local-Approximation Theorem.). Let L be a 1/ε-locally optimal solution and C∗ be any solution. Define S = L ∩ C∗ and L̃ = L − S and C̃∗ = C∗ − S. Then\n∑\nc∈NC∗(C̃∗)∪NL(L̃)\nlc ≤ (3 + 2ε) ∑\nc∈NC∗ (C̃∗)∪NL(L̃)\ngc.\nAdditionally, we show that the analysis is tight:\nProposition 3.15. There exists an infinite family of 3-perturbation-resilient instances such that for any constant ε > 0, there exists a ε−1-locally optimal solution that has cost at least 3OPT.\nThis relies on the example from [9]. It is straightforward to see that the instance they provide is 3-perturbation-resilient."
    }, {
      "heading" : "4 Spectral Separability",
      "text" : "In this Section we will study the spectral separability condition for the Euclidean k-means problem. Our main result will be a proof of Theorem 1.3.\nWe first recall the basic notions and definitions for Euclidean k-means. Let A ∈ Rn×d be a set of points in d-dimensional Euclidean space, where the row Ai contains the coordinates of the ith point. The singular value decomposition is defined as A = UΣV T , where U ∈ Rn×d and V ∈ Rd×d are orthogonal and Σ ∈ Rd×d is a diagonal matrix containing the singular values where per convention the singular values are given in descending order, i.e. Σ1,1 = σ1 ≥ Σ2,2 = σ2 ≥ . . .Σd,d = σd. Denote the Euclidean norm of a d-dimensional vector x by ||x|| = √\n∑d i=1 x 2 i . The spectral norm\nand Frobenius norm are defined as ||A||2 = σ1 and ||A||F = √ ∑d i=1 σ 2 i , respectively.\nThe best rank k approximation min rank(X)=k\n||A − X||F is given via Ak = UkΣV T = UΣkV T =\nUΣV Tk , where Uk, Σk and V T k consist of the first k columns of U , Σ and V T , respectively, and are zero otherwise. The best rank k approximation also minimizes the spectral norm, that is ||A − Ak||2 = σk+1 is minimal among all matrices of rank k. The following fact is well known throughout k-means literature and will be used frequently throughout this section.\nFact 1. Let A be a set of points in Euclidean space and denote by c(A) = 1|A| ∑\nx∈A x the centroid of A. Then the 1-means cost of any candidate center c can be decomposed via\n∑ x∈A ||x− c||2 = ∑ x∈A ||x− c(A)||2 + |A| · ||c(A) − c||2\nand ∑\nx∈A ||x− c(A)||2 = 1 2 · |A| ∑ x∈A ∑ y∈A ||x− y||2.\nNote that the centroid is the optimal 1-means center of A. For a clustering C = {C1, . . . Ck} of A with centers S = {c1, . . . ck}, the cost is then ∑k i=1 ∑ p∈Ci ||p−ci||2. Further, if ci = 1 |Ci| ∑\np∈Ci p, we can rewrite the objective function in matrix form by associating the ith point with the ith row of\nsome matrix A and using the cluster matrix X ∈ Rn×k with Xi,j =\n\n\n\n1√ |C∗j | if Ai ∈ C∗j 0 else to denote\nmembership. Note that XTX = I, i.e. X is an orthogonal projection and that ||A−XXTA||2F is the cost of the optimal k-means clustering. k-means is therefore a constrained rank k-approximation problem.\nWe first restate the separation condition.\nDefinition 4.1 (Spectral Separation). Let A be a set of points and let {C1, . . . Ck} be a clustering of A with centers {c1, . . . ck}. Denote by C an n × d matrix such that Ci = argmin\nj∈{1,...,k} ||Ai − cj ||2.\nThen {C1, . . . Ck} is γ spectrally separated, if for any pair of centers ci and cj the following condition holds:\n||ci − cj || ≥ γ · ( 1 √\n|Ci| +\n1 √\n|Cj|\n)\n||A− C||2.\nThe following crucial lemma relates spectral separation and distribution stability.\nLemma 4.1. For a point set A, let C = {C1, . . . , Ck} be an optimal clustering with centers S = {c1, . . . , ck} associated clustering matrix X that is at least γ · √ k spectrally separated, where γ > 3. For ε > 0, let Am be the best rank m = k/ε approximation of A. Then there exists a clustering K = {C ′1, . . . C ′2} and a set of centers Sk, such that\n1. the cost of clustering Am with centers Sk via the assignment of K is less than ||Am − XXTAm||2F and\n2. (K,Sk) is Ω((γ − 3)2 · ε)-distribution stable.\nWe note that this lemma would also allow us to use the PTAS of Awasthi et al. [10]. Before giving the proof, we outline how Lemma 4.1 helps us prove Theorem 1.3. We first notice that if the rank of A is of order k, then elementary bounds on matrix norm show that spectral separability implies distribution stability. We aim to combine this observation with the following theorem due\nto Cohen et al. [31]. Informally, it states that for every rank k approximation, (an in particular for every constrained rank k approximation such as k-means clustering), projecting to the best rank k/ε subspace is cost-preserving.\nTheorem 4.2 (Theorem 7 of [31]). For any A ∈ Rn×d, let A′ be the rank ⌈k/ε⌉-approximation of A. Then there exists some positive number c such that for any rank k orthogonal projection P ,\n||A− PA||2F ≤ ||A′ − PA′||2F + c ≤ (1 + ε)||A − PA||2F .\nThe combination of the low rank case and this theorem is not trivial as points may be closer to a wrong center after projecting, see also Figure 3. Lemma 4.1 determines the existence of a clustering whose cost for the projected points Am is at most the cost of C\n∗. Moreover, this clustering has constant distribution stability as well which, combined with the results from Section B, allows us to use Local Search. Given that we can find a clustering with cost at most (1+ε) · ||Am−XXTAm||2F , Theorem 4.2 implies that we will have a (1 + ε)2-approximation overall.\nTo prove the lemma, we will require the following steps:\n• A lower bound on the distance of the projected centers ||ciVmV Tm − cjVmV Tm || ≈ ||ci − cj ||.\n• Find a clustering K with centers S∗m = {c1VmV Tm , . . . , c∗kVmV Tm} of Am with cost less than ||Am −XXTAm||2F .\n• Show that in a well-defined sense, K and C∗ agree on a large fraction of points.\n• For any point x ∈ Ki, show that the distance of x to any center not associated with Ki is large.\nWe first require a technical statement.\nLemma 4.3. For a point set A, let C = {C1, . . . Ck} be a clustering with associated clustering matrix X and let A′ and A′′ be optimal low rank approximations where without loss of generality k ≤ rank(A′) < rank(A′′). Then for each cluster Ci\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 |Ci| ∑\nj∈Ci\n( A′′j −A′j )\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣\n2\n≤ √ k\n|Ci| · ||A−XXTA||2.\nProof. By Fact 1 |Ci| · || 1|Ci| ∑ j∈Ci(A ′′ i −A′i)||22 is, for a set of point indexes Ci, the cost of moving the centroid of the cluster computed on A′′ to the centroid of the cluster computed on A′. For a clustering matrix X, ||XXTA′−XXTA′||2F is the sum of squared distances of moving the centroids computed on the point set A′′ to the centroids computed on A′. We then have\n|Ci|·\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 |Ci| ∑ j∈Ci (A′′j −A′j) ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 2\n2\n≤ ||XXTA′′−XXTA′||2F ≤ ||X||2F ·||A′′−A′||22 ≤ k·σ2k+1 ≤ k·||A−XXTA||22.\nProof of Lemma 4.1. For any point p associated with some row of A, let pm = pVmV T m be the corresponding row in Am. Similarly, for some cluster Ci, denote the center in A by ci and the center in Am by c m i . Extend these notion analogously for projections p\nk and cki to the span of the best rank k approximation Ak.\nWe have for any m ≥ k i 6= j\n||cmi − cmj || ≥ ||ci − cj || − ||ci − cmi || − ||cj − cmj ||\n≥ γ · (\n1√ Ci + 1 √ |Cj |\n)\n√ k||A−XXTA||2\n− 1√ Ci\n√ k||A−XXTA||2 − 1 √ |Cj | √ k||A−XXTA||2\n= (γ − 1) · (\n1√ Ci + 1 √ |Cj|\n)\n√ k||A−XXTA||2, (1)\nwhere the second inequality follows from Lemma 4.3.\nIn the following, let ∆i = √ k√ |Ci|\n||A − XXTA||2. We will now construct our target clustering K. Note that we require this clustering (and its properties) only for the analysis. We distinguish between the following three cases.\nCase 1: p ∈ Ci and cmi = argmin j∈{1,...,k} ||pm − cj||:\nThese points remain assigned to cmi . The distance between pm and a different center c m j is at least 12 ||cmi − cmj || ≥ γ−1 2 ε(∆i +∆j) due to Equation 1.\nCase 2: p ∈ Ci, cmi 6= argmin j∈{1,...,k} ||pm − cj ||, and cki 6= argmin j∈{1,...,k} ||pk − ckj ||:\nThese points will get reassigned to their closest center.\nThe distance between pm and a different center c m j is at least 1 2 ||cmi − cmj || ≥ γ−1 2 ε(∆i +∆j) due to Equation 1.\nCase 3: p ∈ Ci, cmi 6= argmin j∈{1,...,k} ||pm − cmj ||, and cki = argmin j∈{1,...,k} ||pk − ckj ||:\nWe assign pm to cmi at the cost of a slightly weaker movement bound on the distance between pm and cmj . Due to orthogonality of V , we have for m > k, (Vm−Vk)TVk = V Tk (Vm−Vk) = 0. Hence VmV T mVk = VmV T k Vk +Vm(Vm−Vk)TVk = VkV Tk Vk +(Vm −Vk)V Tk Vk = VkV Tk Vk = Vk. Then pk = pVkV T k = pVmV T mVkV T k = pmVkV T k . Further, ||pk − ckj || ≥ 12 ||ckj − cki || ≥ γ−1 2 (∆i + ∆j) due to Equation 1. Then the distance between pm and a different center c m j\n||pm − cmj || ≥ ||pm − ckj || − ||cmj − ckj || = √ ||pm − pk||2 + ||pk − ckj ||2 − ||cmj − ckj ||\n≥ ||pk − ckj || −∆j ≥ γ − 3 2 (∆i +∆j),\nwhere the equality follows from orthogonality and the second to last inequality follows from Lemma 4.3.\nNow, given the centers {cm1 , . . . cmk }, we obtain a center matrix MK where the ith row of MK is the center according to the assignment of above. Since both clusterings use the same centers but K improves locally on the assignments, we have ||Am −MK ||2F ≤ ||Am − XXTAm||2F , which\nproves the first statement of the lemma. Additionally, due to the fact that Am−XXTAm has rank m = k/ε, we have\n||Am −MK ||2F ≤ ||Am −XXTAm||2F ≤ m · ||Am −XXTAm||22 ≤ k/ε · ||A−XXTA||2F (2)\nTo ensure stability, we will show that for each element of K there exists an element of C, such that both clusters agree on a large fraction of points. This can be proven by using techniques from Awasthi and Sheffet [13] (Theorem 3.1) and Kumar and Kannan [60] (Theorem 5.4), which we repeat for completeness.\nLemma 4.4. Let K = {C ′1, . . . C ′k} and C = {C1, . . . Ck} be defined as above. Then there exists a bijection b : C → K such that for any i ∈ {i, . . . , k}\n(\n1− 32 (γ − 1)2\n) |Ci| ≤ b(|Ci|) ≤ ( 1 + 32 (γ − 1)2 ) |Ci|.\nProof. Denote by Ti→j the set of points from Ci such that ||cki − pk|| > ||ckj − pk||. We first note that ||Ak −XXTA||2F ≤ 2k · ||Ak − XXTA||22 ≤ 2k · ( ||A−Ak||2 + ||A−XXTA||2 )2 ≤ 8k · ||A − XXTA||22 ≤ 8 · |Ci| · ∆2i for any i ∈ {1, . . . , k}. The distance ||pk − cki || ≥ 12 ||cki − ckj || ≥ γ−1 2 · ( 1√ Ci + 1√ |Cj | )√ k||A − XXTA||22. Assigning these points to cki , we can bound the total number of points added to and subtracted from cluster Cj by observing\n∆2j ∑ i 6=j |Ti→j | ≤ ∑ i 6=j |Ti→j | ·\n(\nγ − 1 2\n)2\n· (∆i +∆j)2 ≤ ||Ak −XXTA||2F ≤ 8 · |Cj | ·∆2j\n∆2j ∑ i 6=j |Tj→i| ≤ ∑ j 6=i |Tj→i| ·\n(\nγ − 1 2\n)2\n· (∆i +∆j)2 ≤ ||Ak −XXTA||2F ≤ 8 · |Cj | ·∆2j .\nTherefore, the cluster sizes are up to some multiplicative factor of (\n1± 32 (γ−1)2\n)\nidentical.\nWe now have for each point pm ∈ C ′i a minimum cost of\n||pm − cmj ||2 ≥ ( γ − 3 2 · ( 1 √\n|Ci| +\n1 √\n|Cj |\n)\n· √ k · ||A−XXTA||2\n)2\n≥\n\n  γ − 3 2 ·\n\n \n√ √ √ √ 1 (\n1 + 32 (γ−1)2\n) · |C ′i| +\n√ √ √ √ 1 (\n1 + 32 (γ−1)2\n)\n· |C ′j |\n\n  · √ k · ||A−XXTA||2\n\n \n2\n≥ 4 · (γ − 3) 2 81 · ε ||Am −MK || 2 F\n|C ′j |\nwhere the first inequality holds due to Case 3, the second inequality holds due to Lemma 4.4 and the last inequality follows from γ > 3 and Equation 2. This ensures that the distribution stability condition is satisfied.\n4.0.1 Proof of Theorem 1.3\nProof. Given the optimal clustering C∗ of A with clustering matrix X, Lemma 4.1 guarantees the existence of a clustering K with center matrix MK such that ||Am−MK ||2F ≤ ||Am−XXTAm|| and that C has constant distribution stability. If ||Am−MK ||2F is not a constant factor approximation, we are already done, as Local Search is guaranteed to find a constant factor approximation. Otherwise due to Corollary 1 (Section B in the appendix), there exists a discretization (Am, F, || · ||2, k) of (Am,R\nd, || · ||2, k) such that the clustering C of the first instance has at most (1 + ε) times the cost of C in the second instance and such that C has constant distribution stability. By Theorem 1.1, Local Search with appropriate (but constant) neighborhood size will find a clustering C ′ with cost at most (1 + ε) times the cost of K in (Am, F, || · ||2, k). Let Y be the clustering matrix of C ′. We then have ||Am − Y Y TAm||2F + ||A − Am||2F ≤ (1 + ε)2||Am −MK ||2F + ||A − Am||2F ≤ (1+ ε)2||Am −XXTAm||2F + ||A−Am||2F ≤ (1+ ε)3||A−XXTA||2F due to Theorem 4.2. Rescaling ε completes the proof.\nRemark. Any (1 + ε)-approximation will not in general agree with a target clustering. To see this consider two clusters: (1) with mean on the origin and (2) with mean δ on the the first axis and 0 on all other coordinates. We generate points via a multivariate Gaussian distribution with an identity covariance matrix centered on the mean of each cluster. If we generate enough points, the instance will have constant spectral separability. However, if δ is small and the dimension large enough, an optimal 1-clustering will approximate the k-means objective."
    }, {
      "heading" : "5 Acknowledgments",
      "text" : "The authors thank their dedicated advisor for this project: Claire Mathieu. Without her, this collaboration would not have been possible.\nAppendix\nA (β, δ)-Stability\nLemma 3.5. Let C∗i be a cheap cluster. For any ε0, we have |IRε0i | > (1− ε3/ε0)|C∗i |.\nProof. Observe that each client that is not in IRε0i is at distance at least ε0βOPT/|C∗i | from c∗i . Since i is cheap, the total cost of the clients in C∗i is at most ε\n3βOPT and in particular, the total cost of the clients that are not in IRε0i does not exceed ε\n3βOPT. Therefore, the total number of such clients is at most ε3|C∗i |/ε0.\nLemma 3.6. Let ε0 < 1/3. If C ∗ i 6= C∗j are cheap clusters, then IRε0i ∩ IRε0j = ∅.\nProof. Assume towards contradiction that this is not true. Consider a client x ∈ IRε0i ∩ IRε0j . Without loss of generality assume |C∗i | ≥ |C∗j |. By the triangular inequality we have cost(c∗j , c∗i ) ≤ cost(c∗j , x) + cost(x, c ∗ i ) ≤ 2ε0βOPT/|C∗j |. Since δ < 1/2, there exists a client c′ ∈ IRε0i ∩∆i. Thus, we have cost(c′, c∗j ) ≤ 3ε0βOPT/|C∗j |. This is a contradiction to the assumption that the instance is (β, δ)-distribution stable with respect to (C∗, S∗) and c′ being in ∆i for any ε0 < 1/3.\nLemma 3.9. There exists a set Z3 ⊆ C∗ − Z2 of size at most (ε−3 + 40ε−1)β−1 such that for any cluster C∗j ∈ C∗ − Z3, the total number of clients x ∈ ⋃\ni 6=j ∆i, that are served by L(j) in L is at most ε|IRε2i |.\nProof. Consider a cheap cluster C∗j ∈ C∗−Z2 such that the total number of clients x ∈ ∆i, for j 6= i, that are served by L(j) in L is greater than ε|IRε2j |. By the triangular inequality and the definition of (β, δ)-stability, the total cost for each x ∈ ∆i, j 6= i served by L(j) is at least (1− ε)βOPT/|C∗j |. Since there are at least ε|IRε2j | such clients, their total cost is at least ε|IRε 2\nj |(1− ε)βOPT/|C∗j |. By Lemma 3.5, this is at least\nε(1− ε)2|C∗j |(1− ε)β OPT |C∗j | ≥ εβOPT 8 ,\nsince ε ≤ 1/2. Recall that by [9], L is a 5-approximation and so there exist at most 40ε−1β−1 such clusters. By Lemma 3.7, the total number of not cheap clusters is at most ε−3β−1 and so, there exists a set Z3 of size at most (ε −3 + 40ε−1)β−1 satisfying the lemma.\nLemma 3.11. For any δ < 1/2, we have for any C∗i ,\n∑\nx∈C∗i −∆i Reassign(x) ≤\n∑\nx∈C∗i −∆i gx +\nδ\n(1− δ) ∑\nx∈C∗i\n(lx + gx).\nProof. Consider a client x ∈ C∗i − ∆i. Let ℓ′ be the center that serves at least one client of ∆i that is the closest to c∗i . Since δ < 1, ℓ\n′ is well defined. By the triangular inequality we have that Reassign(x) ≤ cost(x, ℓ′) ≤ cost(x, c∗i ) + cost(c∗i , ℓ′) = gx + cost(c∗i , ℓ′). Then,\n∑\nx∈C∗i −∆i Reassign(x) ≤\n∑\nx∈C∗i −∆i gx + |C∗i −∆i| · cost(c∗i , ℓ′).\nNow, since ℓ′ is the center that serves at least one client of ∆i that is the closest to c∗i we have that for any x ∈ ∆i, by the triangular inequality cost(c∗i , ℓ′) ≤ cost(c∗i , x) + cost(x, ℓ′) ≤ gx + lx. Therefore,\ncost(c∗i , ℓ ′) ≤ 1|∆i| ∑\nx∈∆i (gx + lx).\nCombining, we obtain\n∑\nx∈C∗i −∆i Reassign(x) ≤\n∑\nx∈C∗i −∆i gx +\n|C∗i −∆i| |∆i| ∑\nx∈∆i (gx + lx)\n= ∑\nx∈C∗i −∆i gx +\nδ\n(1− δ) ∑\nx∈∆i (gx + lx),\nby definition of (β, δ)-stability.\nLemma 3.12. Let C∗i be a cluster in C ∗ − Z∗. Define the solution Mi = L − {L(i)} ∪ {c∗i } and denote by mic the cost of client c in solution Mi. Then ∑\nx∈A mix ≤\n∑\nx/∈A(L(i))∪C̃∗i\nlc + ∑\nx∈C̃∗i\ngx + ∑\nx∈∆̃i\nReassign(x) + ∑\nx∈A(L(i))− (C̃∗i ∪∆̃i)\nlx + ε (1− ε) ( ∑\nx∈C̃∗i\ngx + lx).\nProof. For any client x /∈ A(L(i))∪ C̃∗i , the center that serves it in L belongs to Mi. Thus its cost is at most lx. Moreover, observe that any client x ∈ C̃∗i can now be served by c∗i , and so its cost is at most gx. For each client x ∈ ∆̃i, since all the centers of L except for L(i) are in Mi, we bound its cost by Reassign(x).\nNow, we bound the cost of a client x ∈ A(L(i)) − (C̃∗i ∪ ∆̃i). The closest center in Mi for a client x ∈ A(L(i)) − (C̃∗i ∪ ∆̃i) is not farther than c∗i . By the triangular inequality we have that the cost of such a client x is at most cost(x, c∗i ) ≤ cost(x,L(i))+ cost(L(i), c∗i ) = lx+cost(L(i), c∗i ), and so\n∑\nx∈A(L(i))− (C̃∗i ∪∆̃i)\nmix ≤ |A(L(i)) − (C̃∗i ∪ ∆̃i)|cost(L(i), c∗i ) + ∑\nx∈A(L(i))− (C̃∗i ∪∆̃i)\nlx. (3)\nNow, observe that, for any client x ∈ |A(L(i)) ∩ C̃∗i |, by the triangular inequality we have that cost(L(i), c∗i ) ≤ cost(x,L(i)) + cost(x, c∗i ) = lx + gx. Therefore,\ncost(L(i), c∗i ) ≤ 1 |A(L(i)) ∩ C̃∗i | ∑\nx∈A(L(i))∩C̃∗i\n(lx + gx). (4)\nCombining Equations 3 and 4, we have that\n∑\nx∈A(L(i))− (C̃∗i ∪∆̃i)\nmic ≤ ∑\nx∈A(L(i))− (C̃∗i ∪∆̃i)\nlx + |A(L(i)) − C̃∗i | |A(L(i)) ∩ C̃∗i |\n∑\nx∈A(L(i))∩C̃∗i\n(lx + gx). (5)\nWe now remark that since C̃∗i is not in Z ∗, we have by Lemmas 3.8 and 3.9, |A(L(i))−C̃∗i | ≤ ε|IRε 2 i | and (1− ε)|IRε2i | ≤ |A(L(i)) ∩ C̃∗i |. Thus, combining with Equation 5 yields the lemma.\nLemma 3.13. There exists a constant η such that\n−ε · cost(L) + ∑\nx∈Â−C(Z∗)\nlx ≤ ∑\nx∈Â−C(Z∗)\ngx + (η(ε+ δ 1− δ ))(cost(L) + cost(C ∗)).\nProof. We consider a cluster C∗i in C ∗ − Z∗. and the solution Mi = L − {L(i)} ∪ {c∗i }. Observe that Mi and L only differs by L(i) and c∗i . Therefore, by local optimality and Lemma 3.12, we have that (1− εn)cost(Li) ≤ cost(Mi). Then, (1− ε n )cost(Li) ≤ ∑\nx/∈A(L(i))∪C̃∗i\nlx + ∑\nx∈C̃∗i\ngx + ∑\nx∈A(L(i))− (C̃∗i ∪∆̃i)\nlx + ∑\nx∈∆̃i\nReassign(x) + ε (1− ε) ∑\nx∈C∗i\n(gx + lx)\nand so, simplifying ε\nn cost(Li) +\n∑\nx∈C̃∗ i\nlx + ∑\nx∈∆̃i\nlx ≤ ∑\nx∈C̃∗ i\ngx + ∑\nx∈∆̃i\nReassign(x) + ε (1− ε) ∑\nx∈C∗i\n(gx + lx)\nWe now apply this analysis to each cluster C̃∗i ∈ C∗−Z∗. Summing over all clusters C̃∗i ∈ C∗−Z∗, we obtain,\n−k ε n cost(L)+\n|C∗−Z∗| ∑\ni=1\n\n\n∑\nx∈C̃∗i\nlx + ∑\nx∈∆̃i\nlx\n\n ≤\n|C∗−Z∗| ∑\ni=1\n\n\n∑\nx∈C̃∗i\ngx + ∑\nx∈∆̃i\nReassign(c)\n\n + ε (1− ε) (cost(L) + cost(C ∗))\nBy Lemma 3.11 and the definition of C̃∗i ,\n−k ε n cost(L) +\n|C∗−Z∗| ∑\ni=1\n∑\nx∈C∗i ∩Â\nlx ≤ |C∗−Z∗| ∑\ni=1\n∑\nx∈C∗i ∩Â\ngx + ( ε (1− ε) + δ (1− δ) )(cost(L) + cost(C ∗)).\nTherefore, −εcost(L) + ∑\nc∈Â−C(Z∗)\nlx ≤ ∑\nx∈Â−C(Z∗)\ngx + (η(ε+ δ 1− δ ))(cost(L) + cost(C ∗)), for some\nconstant η and any δ < 1/2."
    }, {
      "heading" : "B Euclidean Distribution Stability",
      "text" : "In this section we show how to reduce the Euclidean problem to the discrete version. Our analysis is focused on the k-means problem, however we note that the discretization works for all values of cost = distp, where the dependency on p grows exponentially. For constant p, we obtain polynomial sized candidate solution sets in polynomial time. For k-means itself, we could alternatively combine Matousek’s approximate centroid set [65] with the Johnson Lindenstrauss lemma and avoid the following construction; however this would only work for optimal distribution stable clusterings and the proof Theorem 1.3 requires it to hold for non-optimal clusterings as well.\nFirst, we describe a discretization procedure. It will be important to us that the candidate solution preserves (1) the cost of any given set of centers and (2) distribution stability.\nFor a set of points P , a set of points Nε is an ε-net of P if for every point x ∈ P there exists some point y ∈ Nε with ||x− y|| ≤ ε. It is well known that for unit Euclidean ball of dimension d, there exists an ε-net of cardinality (1 + 2/ε)d, see for instance Pisier [70]. We will use such ε-nets in our discretization.\nLemma B.1. Let A be a set of n points in d-dimensional Euclidean space and let β, ε > 0 with min(β, ε, √ 7− 4 √ 2−1) > 2η > 0 be constants. Suppose there exists a clustering C = {C1, . . . , Ck} with centers S = {c1, . . . ck} such that\n1. cost(C,S) = ∑k\ni=1\n∑\nx∈Ci ||x − ci||2 is a constant approximation to the optimum clustering and\n2. C is β-distribution stable.\nThen there exists a discretization D of the solution space such that there exists a subset S′ = {c′1, . . . c′k} ⊂ D of size k with\n1. ∑k\ni=1\n∑ x∈Ci ||x− c′i||2 ≤ (1 + ε) · cost(C,S) and\n2. C with centers S′ is β/2-distribution stable.\nThe discretization consists of O(n · log n · ηd+2) many points.\nProof. Let OPT being the cost of an optimal k-means clustering. Define an exponential sequence to the base of (1 + η) starting at (η · OPTn ) and ending at (n · OPT). It is easy to see that the sequence contains t = log1+η(n\n2/η) ∈ O(η−1(log n + log(1/η)) many elements. For each point p ∈ A, define B(p, ℓi) as the d-dimensional ball centered at p with radius √\n(1 + η)i · η · OPTn . We cover the ball B(p, ℓi) with an η/8 · ℓi net Nε/8(p, ℓi). As the set of candidate centers, we let D = ∪p∈A ∪ti=0 Nη/8(p, ℓi). Clearly, |D| ∈ O(n · log n · (1 + 16/η)d+2).\nNow for each ci ∈ S, set c′i = argmin q∈D ||q − ci||. We will show that S′ = {c′1, . . . c′k} satisfies the two conditions of the lemma.\nFor (1), we first consider the points p with ||p − ci|| ≤ √\nε · OPTn . Then there exists a c′i such that ||p − c′i||2 ≤ η2/64 · εOPTn and summing up over all such points, we have a total contribution to the objective value of at most η2 · ε/64 ·OPT ≤ η3/64 ·OPT.\nNow consider the remaining points. Since the cost(C,S) is a constant approximation, the\ncenter ci of each point p satisfies √ (1 + η)i · η · OPTn ≤ ||ci−p|| ≤ √\n(1 + η)i+1 · η · OPTn for some i ∈ {0, . . . t}. Then there exists some point q ∈ Nη/8(p, ℓi+1) with ||q−ci|| ≤ η/· √\n(1 + η)i+1 · η · OPTn ≤ η/8 ·√1 + η||p−ci|| ≤ η/4||p−ci||. We then have ||p−c′i||2 ≤ (1+η/4)2 ||p−ci||2. Summing up over both cases, we have a total cost of at most η3/64·OPT+(1+η/4)2 ·cost(C,S′) ≤ (1+η)·cost(C,S′) ≤ (1 + ε) · cost(C,S′).\nTo show (2), let us consider some point p /∈ Cj with ||p−cj ||2 > β ·OPT|Cj | . Since β · OPT |Cj | ≥ 2η · OPT n ,\nthere exists a point q and an i ∈ {0, . . . t} such that √\nβ 1+η · OPTn ≤ ||ci−q|| ≤\n√\nβ · (1 + η) · ε · OPTn .\nThen ||c′j − cj|| ≤ η · (1+η) √\nβ · (1 + η) · OPTn . Similarly to above, the point c′j satisfies ||p− c′j||2 ≥ (||p − cj|| − ||cj − c′j ||)2 ≥ ( √ β · OPT|Cj | − √ β · η(1 + η) · OPTn )2 ≥ (1 − η(1 + η))β · OPT|Cj | > β · OPT |Cj | where the last inequality holds for any η < 12 · ( √ 7− 4 √ 2− 1).\nTo reduce the dependency on the dimension, we combine this statement with the seminal theorem originally due to Johnson and Lindenstrauss [54].\nLemma B.2 (Johnson-Lindenstrauss lemma). For any set of n points N in d-dimensional Euclidean space and any 0 < ε < 1/2, there exists a distribution F over linear maps f : ℓd2 → ℓm2 with m ∈ O(ε−2 log n) such that\nPf∼F [∀x, y ∈ N, (1− ε)||x− y|| ≤ ||f(x)− f(y)|| ≤ (1 + ε)||x− y||] ≥ 2\n3 .\nIt is easy to see that Johnson-Lindenstrauss type embeddings preserve the Euclidean k-means cost of any clustering, as the cost of any clustering can be written in terms of pairwise distances (see also Fact 1 in Section 4). Since the distribution over linear maps F can be chosen obliviously with respect to the points, this extends to distribution stability of a set of k candidate centers as well.\nCombining Lemmas B.2 and B.1 gives us the following corollary.\nCorollary 1. Let A be a set of points in d-dimensional Euclidean space with a clustering C = {C1, . . . Ck} and centers S = {c1, . . . ck} such that C is β-perturbation stable. Then there exists a (A,F, || · ||2, k)-clustering instance with clients A, npoly(ε−1) centers F and a subset S′ ⊂ F ∪ A of k centers such that C and S′ is O(β) stable and the cost of clustering A with S′ is at most (1 + ε) times the cost of clustering A with S.\nRemark. This procedure can be adapted to work for general powers of cost functions. For Lemma B.1, we simply rescale η. The Johnson-Lindenstrauss lemma can also be applied in these settings, at a slightly worse target dimension of O((p + 1)2 log((p + 1)/ε)ε−3 log n), see Kerber and Raghvendra [57]."
    }, {
      "heading" : "C α-Perturbation Resilience",
      "text" : "Throughout the rest of this section, we let L denote the centers that form a local optimum. We also define C∗ to be the optimal centers induced by the clustering {C∗1 , . . . , C∗k}.\nWe first show how Theorem 3.14 allows us to prove Theorem 1.2.\nProof of Theorem 1.2. Given an instance (A,F, cost, k), we define the following instance I ′ = (A,F, cost′, k), where cost(a, b) = dist(a, b) for some distance function defined over A ∪ F . For each client c ∈ NC∗(C̃∗) ∪ NL(L̃), let ℓi be the center of L that serves it in L, for any point p 6= ℓi, we define cost′(c, p) = αcost(c, p) and cost′(c, ℓi) = cost(c, ℓi). For the other clients we set cost′ = cost. Observe that by local optimality, the clustering induced by L is {C∗1 , . . . , C∗k} if and only if L = C∗. Therefore, the cost of C∗ in instance I ′ is equal to\nα ∑\nc∈NC∗(C̃∗)∪NL(L̃)\ngc + ∑\nc/∈NC∗ (C̃∗)∪NL(L̃)\ngc.\nOn the other hand, the cost of L in I ′ is the same than in I, by Theorem 3.14 ∑\nc∈NC∗(C̃∗)∪NL(L̃)\nlc ≤ ∑\nc∈NC∗ (C̃∗)∪NL(L̃)\nlc ≤ (3 + 2(α− 3)\n2 )\n∑\nc∈NC∗ (C̃∗)∪NL(L̃)\ngc\nand by definition ∑\nc/∈NC∗ (C̃∗)∪NL(L̃)\nlc ≤ ∑\nc/∈NC∗ (C̃∗)∪NL(L̃)\ngc.\nHence the cost of L in I ′ is at most\nα ∑\nc∈NC∗(C̃∗)∪NL(L̃)\ngc + ∑\nc/∈NC∗ (C̃∗)∪NL(L̃)\ngc.\nBy definition of α-perturbation-resilience, we have that the clustering {C∗1 , . . . , C∗k} is the unique optimal solution in I ′. Therefore L = C∗ and the Theorem follows.\nWe now turn to the proof of Theorem 3.14 We first introduce some definitions, following the terminology of [9, 46].\nConsider the following bipartite graph Γ = (L̃ ∪ C̃∗, E) where E is defined as follows. For any center f ∈ C̃∗, we have (f, ℓ) ∈ E where ℓ is the center of L̃ that is the closest to f . Denote NΓ(ℓ) the neighbors of the point corresponding to center ℓ in Γ.\nFor each edge (f, ℓ) ∈ E , for any client c ∈ NC∗(f)−NL(ℓ), we define Reassignc as the cost of reassigning client c to ℓ. We derive the following lemma.\nLemma C.1. For any client c, Reassignc ≤ lc + 2gc.\nProof. By definition we have Reassignc = dist(c, ℓ). By the triangular inequality dist(c, ℓ) ≤ dist(c, f)+dist(f, ℓ). Since f serves c in C∗ we have dist(c, f) = gc, hence dist(c, ℓ) ≤ gc+dist(f, ℓ). We now bound dist(f, ℓ). Consider the center ℓ′ that serves c in solution L. By the triangular inequality we have dist(f, ℓ′) ≤ dist(f, c) + dist(c, ℓ′) = gc + lc. Finally, since ℓ is the closest center of f in L, we have dist(f, ℓ) ≤ dist(f, ℓ′) ≤ gc + lc and the lemma follows.\nWe partition the centers of L̃ as follows. Let L̃0 be the set of centers of L̃ that have degree 0 in Γ. Let L̃≤ε−1 be the set of centers of L̃ that have degree at least one and at most 1/ε in Γ. Let L̃>ε−1 be the set of centers of L̃ that have degree greater than 1/ε in Γ.\nWe now partition the centers of L̃ and C̃∗ using the neighborhoods of the vertices of L̃ in Γ. We start by iteratively constructing two set of pairs S≤ε−1 and S>ε−1 . For each center ℓ ∈ L̃≤ε−1∪L̃>ε−1 , we pick a set Aℓ of |NΓ(ℓ)| − 1 centers of L̃0 and define a pair ({ℓ} ∪ Aℓ, NΓ(ℓ)). We then remove Aℓ from L̃0 and repeat. Let S≤ε−1 be the pairs that contain a center of L̃≤ε−1 and let S>ε−1 be the remaining pairs.\nThe following lemma follows from the definition of the pairs.\nLemma C.2. Let (RL̃, RC̃ ∗ ) be a pair in S≤p∪S>p. If ℓ ∈ RL̃, then for any f such that (f, ℓ) ∈ E, f ∈ RC̃∗. Lemma C.3. For any pair (RL̃, RC̃ ∗\n) ∈ S≤p we have that ∑\nc∈NC∗(RC̃∗ )\nlc ≤ ∑\nc∈NC∗(RC̃∗ )\ngc + 2 ∑\nNL(RL̃)−NC∗ (RC̃∗ )\ngc.\nProof. Consider the mixed solution M = L − RL̃ ∪ RC̃∗ . For each point c, let mc denote the cost of c in solution M . We have\nmc =\n\n \n \ngc if c ∈ NC∗(RC̃∗). Reassignc if c ∈ NL(RL̃)−NC∗(RC̃\n∗ ∪ S) and by Lemma C.2. lc Otherwise.\nNow, observe that the solution M differs from L by at most 2/ε centers. Thus, by 1/ε-local optimality we have cost(L) ≤ cost(C∗). Summing over all clients and simplifying, we obtain\n∑\nc∈NC∗ (RC̃∗ )∪NL(RL̃)\nlc ≤ ∑\nc∈NC∗(RC̃∗ )\ngc + ∑\nNL(RL̃)−NC∗ (RC̃∗ )\nReassignc.\nThe lemma follows by combining with Lemma C.1.\nWe now analyze the cost of the clients served by a center of L that has degree greater than ε−1 in Γ.\nLemma C.4. For any pair (RL̃, RC̃ ∗ ) ∈ S>ε−1 we have that ∑\nc∈NC∗(RC̃∗ )\nlc ≤ ∑\nc∈NC∗ (RC̃∗ )\ngc + 2(1 + ε) ∑\nNL(RL̃)−NC∗ (RC̃∗ )\ngc.\nProof. Consider the center ℓ̂ ∈ RL̃ that has in-degree greater than ε−1. Let L̂ = RL̃ − {ℓ̂}. For each ℓ ∈ L̂, we associate a center f(ℓ) in RC̃∗ in such a way that each f(ℓ) 6= f(ℓ′), for ℓ 6= ℓ′. Note that this is possible since |L̂| = |RC̃∗ | − 1. Let f̃ be the center of RC̃∗ that is not associated with any center of L̂.\nNow, for each center ℓ of L̂ we consider the mixed solution M ℓ = L − {ℓ} ∪ {f(ℓ)}. For each client c, we bound its cost mℓc in solution M ℓ. We have\nmℓc =\n\n \n  gc if c ∈ NC∗(f(ℓ)). Reassignc if c ∈ NL(ℓ)−NC∗(f(ℓ)) and by Lemma C.2. lc Otherwise.\nSumming over all center ℓ ∈ L̂ and all the clients in NC∗(f(ℓ)) ∪ NL(ℓ), we have by ε−1-local optimality\n∑\nc∈NC∗(RC̃∗−f̃)∪NL(L̂)\nlc ≤ ∑\nc∈NC∗ (RC̃∗−f̃)\ngc + ∑\nNL(L̂)−NC∗ (RC̃∗−f̃)\nReassignc. (6)\nWe now complete the proof of the lemma by analyzing the cost of the clients in NC∗(f̃). We consider the center ℓ∗ ∈ L̂ that minimizes the reassignment cost of its clients. Namely, the center ℓ∗ such that ∑\nc∈NL(ℓ∗)Reassignc is minimized. We then consider the solution M (ℓ∗,f̃) = L−{ℓ∗}∪{f̃}.\nFor each client c, we bound its cost m (ℓ∗,f̃) c in solution M (ℓ ∗,f̃). We have\nm(ℓ ∗,f̃)\nc =\n\n \n  gc if c ∈ NC∗(f̃). Reassignc if c ∈ NL(ℓ∗)−NC∗(f̃) and by Lemma C.2. lc Otherwise.\nThus, summing over all clients c, we have by local optimality\n∑\nc∈NC∗ (f̃)∪NL(ℓ∗)\nlc ≤ ∑\nc∈NC∗ (f̃)\ngc + ∑\nNL(ℓ∗)−NC∗ (f̃)\nReassignc. (7)\nBy Lemma C.1, combining Equations 6 and 7 and averaging over all centers of L̂ we have\n∑\nc∈NC∗(RC̃∗ )\nlc ≤ ∑\nc∈NC∗ (RC̃∗ )\ngc + 2(1 + ε) ∑\nNL(RL̃)−NC∗ (RC̃∗ )\ngc.\nWe now turn to the proof of Theorem 3.14.\nProof of Theorem 3.14. Observe first that for any c ∈ NL(L̃) − NC∗(C̃∗), we have lc ≤ gc. This follows from the fact that the center that serves c in C∗ is in S and so in L and thus, we have lc ≤ gc. Therefore\n∑\nc∈NL(L̃)−NC∗ (C̃∗)\nlc ≤ ∑\nc∈NL(L̃)−NC∗ (C̃∗)\ngc. (8)\nWe now sum the equations of Lemmas C.3 and C.4 over all pairs and obtain\n∑\n(RL̃,RC̃∗ )\n∑\nc∈NC∗(RC̃∗ )\nlc ≤ ∑\n(RL̃,RC̃∗ )\n\n \n∑\nc∈NC∗(RC̃∗ )\ngc + (2 + 2ε) ∑\nNL(RL̃)−NC∗ (RC̃∗ )\ngc\n\n \n∑\nc∈NC∗(C̃∗)\nlc ≤ ∑\nc∈NC∗(C̃∗)\ngc + (2 + 2ε) ∑\nc∈NL(L̃)−NC∗ (C̃∗)\ngc\n∑\nc∈NC∗(C̃∗)∪NL(L̃)\nlc ≤ (3 + 2ε) ∑\nc∈NC∗ (C̃∗)∪NL(L̃)\ngc By Eq. 8."
    } ],
    "references" : [ {
      "title" : "Local Search in Combinatorial Optimization",
      "author" : [ "Emile Aarts", "Jan K. Lenstra", "editors" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1997
    }, {
      "title" : "On spectral learning of mixtures of distributions",
      "author" : [ "Dimitris Achlioptas", "Frank McSherry" ],
      "venue" : "In Learning Theory, 18th Annual Conference on Learning Theory, COLT 2005, Bertinoro,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "k-means++ under approximation stability",
      "author" : [ "Manu Agarwal", "Ragesh Jaiswal", "Arindam Pal" ],
      "venue" : "Theor. Comput. Sci.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Learning mixtures of arbitrary gaussians",
      "author" : [ "Sanjeev Arora", "Ravi Kannan" ],
      "venue" : "In Proceedings on 33rd Annual ACM Symposium on Theory of Computing, July 6-8,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2001
    }, {
      "title" : "Approximation schemes for Euclidean k -medians and related problems",
      "author" : [ "Sanjeev Arora", "Prabhakar Raghavan", "Satish Rao" ],
      "venue" : "In Proceedings of the Thirtieth Annual ACM Symposium on the Theory of Computing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1998
    }, {
      "title" : "Smoothed analysis of the k-means method",
      "author" : [ "David Arthur", "Bodo Manthey", "Heiko Röglin" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "k-means++: the advantages of careful seeding",
      "author" : [ "David Arthur", "Sergei Vassilvitskii" ],
      "venue" : "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Worst-case and smoothed analysis of the ICP algorithm, with an application to the k-means method",
      "author" : [ "David Arthur", "Sergei Vassilvitskii" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Local search heuristics for k-median and facility location problems",
      "author" : [ "Vijay Arya", "Naveen Garg", "Rohit Khandekar", "Adam Meyerson", "Kamesh Munagala", "Vinayaka Pandit" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2004
    }, {
      "title" : "Stability yields a PTAS for k-median and k-means clustering",
      "author" : [ "Pranjal Awasthi", "Avrim Blum", "Or Sheffet" ],
      "venue" : "In 51th Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Center-based clustering under perturbation stability",
      "author" : [ "Pranjal Awasthi", "Avrim Blum", "Or Sheffet" ],
      "venue" : "Inf. Process. Lett.,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "The hardness of approximation of Euclidean k-means",
      "author" : [ "Pranjal Awasthi", "Moses Charikar", "Ravishankar Krishnaswamy", "Ali Kemal Sinop" ],
      "venue" : "In 31st International Symposium on Computational Geometry, SoCG 2015, June 22-25,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Improved spectral-norm bounds for clustering. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques - 15th International Workshop",
      "author" : [ "Pranjal Awasthi", "Or Sheffet" ],
      "venue" : "APPROX 2012, and 16th International Workshop,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Polynomial time algorithm for 2-stable clustering",
      "author" : [ "Ainesh Bakshi", "Nadiia Chepurko" ],
      "venue" : "instances. CoRR,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Approximate clustering without the approximation",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta" ],
      "venue" : "In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Clustering under approximation stability",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Finding low error clusterings",
      "author" : [ "Maria-Florina Balcan", "Mark Braverman" ],
      "venue" : "In COLT 2009 - The 22nd Conference on Learning Theory,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "k-center clustering under perturbation resilience",
      "author" : [ "Maria-Florina Balcan", "Nika Haghtalab", "Colin White" ],
      "venue" : "In 43rd International Colloquium on Automata, Languages, and Programming,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "Clustering under perturbation resilience",
      "author" : [ "Maria-Florina Balcan", "Yingyu Liang" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Agnostic clustering",
      "author" : [ "Maria-Florina Balcan", "Heiko Röglin", "Shang-Hua Teng" ],
      "venue" : "In Algorithmic Learning Theory, 20th International Conference,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "On variants of k-means clustering",
      "author" : [ "Sayan Bandyapadhyay", "Kasturi R. Varadarajan" ],
      "venue" : "CoRR, abs/1512.02985,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Distributed balanced clustering via mapping coresets",
      "author" : [ "MohammadHossein Bateni", "Aditya Bhaskara", "Silvio Lattanzi", "Vahab S. Mirrokni" ],
      "venue" : "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Polynomial learning of distribution families",
      "author" : [ "Mikhail Belkin", "Kaushik Sinha" ],
      "venue" : "In 51th Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Data stability in clustering: A closer look",
      "author" : [ "Shalev Ben-David", "Lev Reyzin" ],
      "venue" : "Theor. Comput. Sci.,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "On the practically interesting instances of MAXCUT",
      "author" : [ "Yonatan Bilu", "Amit Daniely", "Nati Linial", "Michael E. Saks" ],
      "venue" : "In 30th International Symposium on Theoretical Aspects of Computer Science,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Are stable instances easy? Combinatorics",
      "author" : [ "Yonatan Bilu", "Nathan Linial" ],
      "venue" : "Probability & Computing,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "Parallel approximation algorithms for facilitylocation problems",
      "author" : [ "Guy E. Blelloch", "Kanat Tangwongsan" ],
      "venue" : "SPAA",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2010
    }, {
      "title" : "Streaming k-means on well-clusterable data",
      "author" : [ "Vladimir Braverman", "Adam Meyerson", "Rafail Ostrovsky", "Alan Roytman", "Michael Shindler", "Brian Tagiku" ],
      "venue" : "In Proceedings of the Twenty- Second Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2011
    }, {
      "title" : "Isotropic PCA and affine-invariant clustering",
      "author" : [ "S. Charles Brubaker", "Santosh Vempala" ],
      "venue" : "Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2008
    }, {
      "title" : "Improved combinatorial algorithms for facility location problems",
      "author" : [ "Moses Charikar", "Sudipto Guha" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2005
    }, {
      "title" : "Dimensionality reduction for k-means clustering and low rank approximation",
      "author" : [ "Michael B. Cohen", "Sam Elder", "Cameron Musco", "Christopher Musco", "Madalina Persu" ],
      "venue" : "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "The power of Local Search for clustering",
      "author" : [ "Vincent Cohen-Addad", "Philip N. Klein", "Claire Mathieu" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2016
    }, {
      "title" : "Effectiveness of local search for geometric optimization",
      "author" : [ "Vincent Cohen-Addad", "Claire Mathieu" ],
      "venue" : "In 31st International Symposium on Computational Geometry, SoCG 2015, June 22-25,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    }, {
      "title" : "Variational shape approximation",
      "author" : [ "David Cohen-Steiner", "Pierre Alliez", "Mathieu Desbrun" ],
      "venue" : "ACM Trans. Graph.,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2004
    }, {
      "title" : "Graph partitioning via adaptive spectral techniques",
      "author" : [ "Amin Coja-Oghlan" ],
      "venue" : "Combinatorics, Probability & Computing,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2010
    }, {
      "title" : "Spectral clustering with limited independence",
      "author" : [ "Anirban Dasgupta", "John E. Hopcroft", "Ravi Kannan", "Pradipta Prometheus Mitra" ],
      "venue" : "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2007
    }, {
      "title" : "Learning mixtures of gaussians",
      "author" : [ "Sanjoy Dasgupta" ],
      "venue" : "Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1999
    }, {
      "title" : "Random projection trees for vector quantization",
      "author" : [ "Sanjoy Dasgupta", "Yoav Freund" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2009
    }, {
      "title" : "A probabilistic analysis of EM for mixtures of separated, spherical gaussians",
      "author" : [ "Sanjoy Dasgupta", "Leonard J. Schulman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2007
    }, {
      "title" : "Iterative clustering of high dimensional text data augmented by local search",
      "author" : [ "Inderjit S. Dhillon", "Yuqiang Guan", "Jacob Kogan" ],
      "venue" : "In Proceedings of the 2002 IEEE International Conference on Data Mining (ICDM 2002),",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2002
    }, {
      "title" : "A unified framework for approximating and clustering data",
      "author" : [ "Dan Feldman", "Michael Langberg" ],
      "venue" : "In Proceedings of the 43rd ACM Symposium on Theory of Computing,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2011
    }, {
      "title" : "Local search yields a PTAS for k-means in doubling metrics",
      "author" : [ "Zachary Friggstad", "Mohsen Rezapour", "Mohammad R. Salavatipour" ],
      "venue" : "CoRR, abs/1603.08976,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2016
    }, {
      "title" : "Tight analysis of a multiple-swap heurstic for budgeted red-blue median",
      "author" : [ "Zachary Friggstad", "Yifeng Zhang" ],
      "venue" : "In 43rd International Colloquium on Automata, Languages, and Programming,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2016
    }, {
      "title" : "Greedy strikes back: Improved facility location algorithms",
      "author" : [ "Sudipto Guha", "Samir Khuller" ],
      "venue" : "J. Algorithms,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 1999
    }, {
      "title" : "Clustering data streams: Theory and practice",
      "author" : [ "Sudipto Guha", "Adam Meyerson", "Nina Mishra", "Rajeev Motwani", "Liadan O’Callaghan" ],
      "venue" : "IEEE Trans. Knowl. Data Eng.,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2003
    }, {
      "title" : "Simpler analyses of local search algorithms for facility location",
      "author" : [ "Anupam Gupta", "Kanat Tangwongsan" ],
      "venue" : "CoRR, abs/0809.2554,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2008
    }, {
      "title" : "Embeddings and non-approximability of geometric problems",
      "author" : [ "Venkatesan Guruswami", "Piotr Indyk" ],
      "venue" : "In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, January 12-14,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2003
    }, {
      "title" : "J-means: a new local search heuristic for minimum sum of squares clustering",
      "author" : [ "Pierre Hansen", "Nenad Mladenovic" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2001
    }, {
      "title" : "Smaller coresets for k-median and k-means clustering",
      "author" : [ "Sariel Har-Peled", "Akash Kushal" ],
      "venue" : "Discrete & Computational Geometry,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2007
    }, {
      "title" : "On coresets for k-means and k-median clustering",
      "author" : [ "Sariel Har-Peled", "Soham Mazumdar" ],
      "venue" : "In Proceedings of the 36th Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2004
    }, {
      "title" : "A new greedy approach for facility location problems",
      "author" : [ "Kamal Jain", "Mohammad Mahdian", "Amin Saberi" ],
      "venue" : "In Proceedings on 34th Annual ACM Symposium on Theory of Computing, May 19-21,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2002
    }, {
      "title" : "Approximation algorithms for metric facility location and k -median problems using the primal-dual schema and Lagrangian relaxation",
      "author" : [ "Kamal Jain", "Vijay V. Vazirani" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2001
    }, {
      "title" : "Analysis of k-means++ for separable data. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques - 15th International Workshop",
      "author" : [ "Ragesh Jaiswal", "Nitin Garg" ],
      "venue" : "APPROX 2012, and 16th International Workshop,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2012
    }, {
      "title" : "Extensions of Lipschitz mapping into Hilbert space. In Conf. in modern analysis and probability, volume 26 of Contemporary Mathematics, pages 189–206",
      "author" : [ "W.B. Johnson", "J. Lindenstrauss" ],
      "venue" : "American Mathematical Society,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 1984
    }, {
      "title" : "The spectral method for general mixture models",
      "author" : [ "Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2008
    }, {
      "title" : "A local search approximation algorithm for k-means clustering",
      "author" : [ "Tapas Kanungo", "David M. Mount", "Nathan S. Netanyahu", "Christine D. Piatko", "Ruth Silverman", "Angela Y. Wu" ],
      "venue" : "Comput. Geom.,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2004
    }, {
      "title" : "Approximation and streaming algorithms for projective clustering via random projections",
      "author" : [ "Michael Kerber", "Sharath Raghvendra" ],
      "venue" : "In Proceedings of the 27th Canadian Conference on Computational Geometry,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2015
    }, {
      "title" : "A nearly linear-time approximation scheme for the euclidean k-median problem",
      "author" : [ "Stavros G. Kolliopoulos", "Satish Rao" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2007
    }, {
      "title" : "Analysis of a local search heuristic for facility location problems",
      "author" : [ "Madhukar R. Korupolu", "C. Greg Plaxton", "Rajmohan Rajaraman" ],
      "venue" : "J. Algorithms,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 2000
    }, {
      "title" : "Clustering with spectral norm and the k-means algorithm",
      "author" : [ "Amit Kumar", "Ravindran Kannan" ],
      "venue" : "In 51th Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2010
    }, {
      "title" : "Linear-time approximation schemes for clustering problems in any dimensions",
      "author" : [ "Amit Kumar", "Yogish Sabharwal", "Sandeep Sen" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 2010
    }, {
      "title" : "Finding meaningful cluster structure amidst background noise",
      "author" : [ "Shrinu Kushagra", "Samira Samadi", "Shai Ben-David" ],
      "venue" : "In Algorithmic Learning Theory - 27th International Conference,",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2016
    }, {
      "title" : "Approximating k-median via pseudo-approximation",
      "author" : [ "Shi Li", "Ola Svensson" ],
      "venue" : "In Symposium on Theory of Computing Conference,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2013
    }, {
      "title" : "The planar k-means problem is NP-hard",
      "author" : [ "Meena Mahajan", "Prajakta Nimbhorkar", "Kasturi R. Varadarajan" ],
      "venue" : "Theor. Comput. Sci.,",
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2012
    }, {
      "title" : "On approximate geometric k-clustering",
      "author" : [ "Jiŕı Matousek" ],
      "venue" : "Discrete & Computational Geometry,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2000
    }, {
      "title" : "Spectral partitioning of random graphs",
      "author" : [ "Frank McSherry" ],
      "venue" : "In 42nd Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 2001
    }, {
      "title" : "On the complexity of some common geometric location problems",
      "author" : [ "Nimrod Megiddo", "Kenneth J. Supowit" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 1984
    }, {
      "title" : "The online median problem",
      "author" : [ "Ramgopal R. Mettu", "C. Greg Plaxton" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "68",
      "shortCiteRegEx" : "68",
      "year" : 2003
    }, {
      "title" : "The effectiveness of Lloyd-type methods for the k-means problem",
      "author" : [ "Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "69",
      "shortCiteRegEx" : "69",
      "year" : 2012
    }, {
      "title" : "The volume of convex bodies and Banach space geometry",
      "author" : [ "Gilles Pisier" ],
      "venue" : "Cambridge Tracts in Mathematics",
      "citeRegEx" : "70",
      "shortCiteRegEx" : "70",
      "year" : 1999
    }, {
      "title" : "Clustering with or without the approximation",
      "author" : [ "Frans Schalekamp", "Michael Yu", "Anke van Zuylen" ],
      "venue" : "J. Comb. Optim.,",
      "citeRegEx" : "71",
      "shortCiteRegEx" : "71",
      "year" : 2013
    }, {
      "title" : "Clustering for edge-cost minimization (extended abstract)",
      "author" : [ "Leonard J. Schulman" ],
      "venue" : "In Proceedings of the Thirty-Second Annual ACM Symposium on Theory of Computing, May 21-23,",
      "citeRegEx" : "72",
      "shortCiteRegEx" : "72",
      "year" : 2000
    }, {
      "title" : "A spectral algorithm for learning mixture models",
      "author" : [ "Santosh Vempala", "Grant Wang" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "73",
      "shortCiteRegEx" : "73",
      "year" : 2004
    }, {
      "title" : "Towards event source unobservability with minimum network traffic in sensor networks",
      "author" : [ "Yi Yang", "Min Shao", "Sencun Zhu", "Bhuvan Urgaonkar", "Guohong Cao" ],
      "venue" : "In Proceedings of the First ACM Conference on Wireless Network Security,",
      "citeRegEx" : "74",
      "shortCiteRegEx" : "74",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ": [1] or [56]).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 55,
      "context" : ": [1] or [56]).",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "This algorithm has a polynomial running time (see [9, 33]).",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 32,
      "context" : "This algorithm has a polynomial running time (see [9, 33]).",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : "Distribution Stability Awasthi, Blum, Sheffet [10]",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "Approximation Stability Balcan, Blum, Gupta [15, 16] Cost Separation Ostrovsky, Rabani, Schulman, Swamy [69] Jaiswal, Garg [53]",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 15,
      "context" : "Approximation Stability Balcan, Blum, Gupta [15, 16] Cost Separation Ostrovsky, Rabani, Schulman, Swamy [69] Jaiswal, Garg [53]",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 68,
      "context" : "Approximation Stability Balcan, Blum, Gupta [15, 16] Cost Separation Ostrovsky, Rabani, Schulman, Swamy [69] Jaiswal, Garg [53]",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 52,
      "context" : "Approximation Stability Balcan, Blum, Gupta [15, 16] Cost Separation Ostrovsky, Rabani, Schulman, Swamy [69] Jaiswal, Garg [53]",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 59,
      "context" : "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 24,
      "context" : "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 25,
      "context" : "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 10,
      "context" : "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 18,
      "context" : "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 10,
      "context" : "Center Proximity Awasthi, Blum, Sheffet [11] Balcan, Liang [19]",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 18,
      "context" : "Center Proximity Awasthi, Blum, Sheffet [11] Balcan, Liang [19]",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "For example, if an instance is cost-separated then it is distribution-stable; therefore the algorithm by Awasthi, Blum and Sheffet [10] also works for cost-separated instances.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "[10], called “distribution stability”.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "2 (Distribution Stability [10]).",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "[10].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 68,
      "context" : "[69], so Local Search is also a PTAS in their setting and also recovers most of the structure of such instances.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "3 (Perturbation Resilience [11]).",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : "An optimal algorithm for 2-perturbation resilient clustering for any center-based objective function was very recently given by Bakshi and Chepurko [14].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 13,
      "context" : "We do not quite match [14]: One limitation is that Local Search is not necessarily optimal for 2-perturbation resilient instances, see Proposition 3.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 59,
      "context" : "4 (Spectral Separation [60]2).",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 59,
      "context" : "In previous work by Kumar and Kannan [60], an algorithm was given with approximation ratio 1+O(OPTk/OPTk−1), where OPTi denotes the value of an optimal solution using i centers.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 59,
      "context" : "k/ε)-spectrally separated [60].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 55,
      "context" : ": [56]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 59,
      "context" : "Hence if instances are The proximity condition of Kumar and Kannan [60] implies the spectral separation condition.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 66,
      "context" : "2 Related Work The problems we study are NP-hard: k-median and k-means are already NP-hard in the Euclidean plane (see Meggido and Supowit [67], Mahajan et al.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 63,
      "context" : "[64], and Dasgupta and Freud [38]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "[64], and Dasgupta and Freud [38]).",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 43,
      "context" : "In terms of hardness of approximation, both problems are APX-hard, even in the Euclidean setting when both k and d are part of the input (see Gua and Khuller [44], Jain et al.",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 50,
      "context" : "[51], Guruswami et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 46,
      "context" : "[47] and Awasthi et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 62,
      "context" : "On the positive side, constant factor approximations are known in metric space for both k-median and k-means (see for example [63, 52, 68]).",
      "startOffset" : 126,
      "endOffset" : 138
    }, {
      "referenceID" : 51,
      "context" : "On the positive side, constant factor approximations are known in metric space for both k-median and k-means (see for example [63, 52, 68]).",
      "startOffset" : 126,
      "endOffset" : 138
    }, {
      "referenceID" : 67,
      "context" : "On the positive side, constant factor approximations are known in metric space for both k-median and k-means (see for example [63, 52, 68]).",
      "startOffset" : 126,
      "endOffset" : 138
    }, {
      "referenceID" : 40,
      "context" : "Given the hardness results, how can one hope to obtain a (1+ε)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.",
      "startOffset" : 175,
      "endOffset" : 183
    }, {
      "referenceID" : 60,
      "context" : "Given the hardness results, how can one hope to obtain a (1+ε)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.",
      "startOffset" : 175,
      "endOffset" : 183
    }, {
      "referenceID" : 4,
      "context" : "Given the hardness results, how can one hope to obtain a (1+ε)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.",
      "startOffset" : 203,
      "endOffset" : 226
    }, {
      "referenceID" : 31,
      "context" : "Given the hardness results, how can one hope to obtain a (1+ε)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.",
      "startOffset" : 203,
      "endOffset" : 226
    }, {
      "referenceID" : 41,
      "context" : "Given the hardness results, how can one hope to obtain a (1+ε)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.",
      "startOffset" : 203,
      "endOffset" : 226
    }, {
      "referenceID" : 57,
      "context" : "Given the hardness results, how can one hope to obtain a (1+ε)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.",
      "startOffset" : 203,
      "endOffset" : 226
    }, {
      "referenceID" : 48,
      "context" : "Given the hardness results, how can one hope to obtain a (1+ε)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.",
      "startOffset" : 203,
      "endOffset" : 226
    }, {
      "referenceID" : 49,
      "context" : "Given the hardness results, how can one hope to obtain a (1+ε)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.",
      "startOffset" : 203,
      "endOffset" : 226
    }, {
      "referenceID" : 68,
      "context" : "[69] assumed that cost of an optimal clustering with k centers is smaller than an ε2-fraction of the cost of an optimal clustering with k− 1 centers, see also Schulman [72] for an earlier condition for two clusters and the irreducibility condition by Kumar et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 71,
      "context" : "[69] assumed that cost of an optimal clustering with k centers is smaller than an ε2-fraction of the cost of an optimal clustering with k− 1 centers, see also Schulman [72] for an earlier condition for two clusters and the irreducibility condition by Kumar et al.",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 60,
      "context" : "[61].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "The popular D2 sampling technique (also known as k-means++) has an improved performance for cost separated instances compared to the worst-case O(log k)-approximation ratio [7, 28, 53, 69].",
      "startOffset" : 173,
      "endOffset" : 188
    }, {
      "referenceID" : 27,
      "context" : "The popular D2 sampling technique (also known as k-means++) has an improved performance for cost separated instances compared to the worst-case O(log k)-approximation ratio [7, 28, 53, 69].",
      "startOffset" : 173,
      "endOffset" : 188
    }, {
      "referenceID" : 52,
      "context" : "The popular D2 sampling technique (also known as k-means++) has an improved performance for cost separated instances compared to the worst-case O(log k)-approximation ratio [7, 28, 53, 69].",
      "startOffset" : 173,
      "endOffset" : 188
    }, {
      "referenceID" : 68,
      "context" : "The popular D2 sampling technique (also known as k-means++) has an improved performance for cost separated instances compared to the worst-case O(log k)-approximation ratio [7, 28, 53, 69].",
      "startOffset" : 173,
      "endOffset" : 188
    }, {
      "referenceID" : 9,
      "context" : "[10] where the cost of assigning all the points from one cluster in the optimal k-clustering to another center increases the objective by some factor (1 + α).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 3,
      "context" : "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 22,
      "context" : "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 28,
      "context" : "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 34,
      "context" : "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 35,
      "context" : "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 36,
      "context" : "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 38,
      "context" : "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 54,
      "context" : "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 65,
      "context" : "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 72,
      "context" : "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 14,
      "context" : "[15, 16] gave a deterministic condition called approximation stability under which a target clustering can be retrieved by via any sufficiently good algorithm for the k-means (or kmedian) objective function.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 15,
      "context" : "[15, 16] gave a deterministic condition called approximation stability under which a target clustering can be retrieved by via any sufficiently good algorithm for the k-means (or kmedian) objective function.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 19,
      "context" : "[20].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "For results using approximation stability, see [16, 17, 71, 3, 10].",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 16,
      "context" : "For results using approximation stability, see [16, 17, 71, 3, 10].",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 70,
      "context" : "For results using approximation stability, see [16, 17, 71, 3, 10].",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 2,
      "context" : "For results using approximation stability, see [16, 17, 71, 3, 10].",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 9,
      "context" : "For results using approximation stability, see [16, 17, 71, 3, 10].",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 59,
      "context" : "Another deterministic condition that relates target clustering recovery via the k-means objective was introduced by Kumar and Kannan [60].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 12,
      "context" : "For further spectral based approaches, see also [13].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 25,
      "context" : "[26, 25] formalized this as allowing edge weights in a graph to be modified by a factor of at most γ without changing the maxcut.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 24,
      "context" : "[26, 25] formalized this as allowing edge weights in a graph to be modified by a factor of at most γ without changing the maxcut.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : "[6, 8] for work on k-means).",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "[6, 8] for work on k-means).",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 10,
      "context" : "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].",
      "startOffset" : 47,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].",
      "startOffset" : 47,
      "endOffset" : 71
    }, {
      "referenceID" : 17,
      "context" : "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].",
      "startOffset" : 47,
      "endOffset" : 71
    }, {
      "referenceID" : 18,
      "context" : "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].",
      "startOffset" : 47,
      "endOffset" : 71
    }, {
      "referenceID" : 23,
      "context" : "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].",
      "startOffset" : 47,
      "endOffset" : 71
    }, {
      "referenceID" : 61,
      "context" : "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].",
      "startOffset" : 47,
      "endOffset" : 71
    }, {
      "referenceID" : 58,
      "context" : "Local Search There exists a large body of bicriteria approximations for k-median and k-means [59, 30, 33, 21].",
      "startOffset" : 93,
      "endOffset" : 109
    }, {
      "referenceID" : 29,
      "context" : "Local Search There exists a large body of bicriteria approximations for k-median and k-means [59, 30, 33, 21].",
      "startOffset" : 93,
      "endOffset" : 109
    }, {
      "referenceID" : 32,
      "context" : "Local Search There exists a large body of bicriteria approximations for k-median and k-means [59, 30, 33, 21].",
      "startOffset" : 93,
      "endOffset" : 109
    }, {
      "referenceID" : 20,
      "context" : "Local Search There exists a large body of bicriteria approximations for k-median and k-means [59, 30, 33, 21].",
      "startOffset" : 93,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : "[9] (see also [46]) gave the first analysis showing that Local Search with a neighborhood size of 1/ε gives a 3 + 2ε approximation to k-median and showed that this bound is tight.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 45,
      "context" : "[9] (see also [46]) gave the first analysis showing that Local Search with a neighborhood size of 1/ε gives a 3 + 2ε approximation to k-median and showed that this bound is tight.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 55,
      "context" : "[56] proved an approximation ratio of 9 + ε for Euclidean k-means clustering by Local Search, currently the best known algorithm with a polynomial running time in metric and Euclidean spaces.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 41,
      "context" : "3 Recently, Local Search with an appropriate neighborhood was shown to be a PTAS for k-means and k-median in certain restricted metrics including constant dimensional Euclidean space [42, 32].",
      "startOffset" : 183,
      "endOffset" : 191
    }, {
      "referenceID" : 31,
      "context" : "3 Recently, Local Search with an appropriate neighborhood was shown to be a PTAS for k-means and k-median in certain restricted metrics including constant dimensional Euclidean space [42, 32].",
      "startOffset" : 183,
      "endOffset" : 191
    }, {
      "referenceID" : 21,
      "context" : "Due to its simplicity, Local Search is also a popular subroutine for clustering tasks in various computational models [22, 27, 45].",
      "startOffset" : 118,
      "endOffset" : 130
    }, {
      "referenceID" : 26,
      "context" : "Due to its simplicity, Local Search is also a popular subroutine for clustering tasks in various computational models [22, 27, 45].",
      "startOffset" : 118,
      "endOffset" : 130
    }, {
      "referenceID" : 44,
      "context" : "Due to its simplicity, Local Search is also a popular subroutine for clustering tasks in various computational models [22, 27, 45].",
      "startOffset" : 118,
      "endOffset" : 130
    }, {
      "referenceID" : 39,
      "context" : "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 42,
      "context" : "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 47,
      "context" : "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 33,
      "context" : "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 73,
      "context" : "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 9,
      "context" : "Our proof includes a few ingredients from [10] such as the notion of inner-ring (we work with a slightly more general definition) and distinguishing between cheap and expensive clusters.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 64,
      "context" : "Euclidean inputs can be straightforwardly “discretized” by computing an appropriate candidate set of centers, for instance via Matousek’s approximate centroid set [65] and then applying the Johnson-Lindenstrauss lemma, if the dimension is too large.",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 64,
      "context" : "They combined Local Search with techniques from Matousek [65] for k-means clustering in Euclidean spaces.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 59,
      "context" : "Indeed, this is the first step of the algorithm by Kumar and Kannan [60] (see Algorithm 3).",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 59,
      "context" : "Algorithm 3 k-means with spectral initialization [60] 1: Project points onto the best rank k subspace 2: Compute a clustering C with constant approximation factor on the projection 3: Initialize centroids of each cluster of C as centers in the original space 4: Run Lloyd’s k-means until convergence In general, projecting onto the best rank k subspace and computing a constant approximation on the projection results in a constant approximation in the original space.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 59,
      "context" : "Kumar and Kannan [60] and later Awasthi and Sheffet [13] gave tighter bounds if the spectral separation is large enough.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "Kumar and Kannan [60] and later Awasthi and Sheffet [13] gave tighter bounds if the spectral separation is large enough.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 30,
      "context" : "[31].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "The following definition is a generalization of the inner-ring definition of [10].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : "1 in [10].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 8,
      "context" : "Now, observe that by [9], the cost of L is at most a 5 approximation to the cost of OPT in the worst case.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 8,
      "context" : "This relies on the example from [9].",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "[10].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[31].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "2 (Theorem 7 of [31]).",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 12,
      "context" : "This can be proven by using techniques from Awasthi and Sheffet [13] (Theorem 3.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 59,
      "context" : "1) and Kumar and Kannan [60] (Theorem 5.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "Recall that by [9], L is a 5-approximation and so there exist at most 40ε−1β−1 such clusters.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 64,
      "context" : "For k-means itself, we could alternatively combine Matousek’s approximate centroid set [65] with the Johnson Lindenstrauss lemma and avoid the following construction; however this would only work for optimal distribution stable clusterings and the proof Theorem 1.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 69,
      "context" : "It is well known that for unit Euclidean ball of dimension d, there exists an ε-net of cardinality (1 + 2/ε)d, see for instance Pisier [70].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 53,
      "context" : "To reduce the dependency on the dimension, we combine this statement with the seminal theorem originally due to Johnson and Lindenstrauss [54].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 56,
      "context" : "The Johnson-Lindenstrauss lemma can also be applied in these settings, at a slightly worse target dimension of O((p + 1)2 log((p + 1)/ε)ε−3 log n), see Kerber and Raghvendra [57].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 8,
      "context" : "14 We first introduce some definitions, following the terminology of [9, 46].",
      "startOffset" : 69,
      "endOffset" : 76
    }, {
      "referenceID" : 45,
      "context" : "14 We first introduce some definitions, following the terminology of [9, 46].",
      "startOffset" : 69,
      "endOffset" : 76
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we analyze the performance of a simple and standard Local Search algorithm for clustering on well behaved data. Since the seminal paper by Ostrovsky, Rabani, Schulman and Swamy [FOCS 2006], much progress has been made to characterize real-world instances. We distinguish the three main definitions • Distribution Stability (Awasthi, Blum, Sheffet, FOCS 2010) • Spectral Separability (Kumar, Kannan, FOCS 2010) • Perturbation Resilience (Bilu, Linial, ICS 2010) and show that Local Search performs well on the instances with the aforementioned stability properties. Specifically, for the k-means and k-median objective, we show that Local Search exactly recovers the optimal clustering if the dataset is 3+ ε-perturbation resilient, and is a PTAS for distribution stability and spectral separability. This implies the first PTAS for instances satisfying the spectral separability condition. For the distribution stability condition we also go beyond previous work by showing that the clustering output by the algorithm and the optimal clustering are very similar. This is a significant step toward understanding the success of Local Search heuristics in clustering applications and supports the legitimacy of the stability conditions: They characterize some of the structure of real-world instances that make Local Search a popular heuristic. Supported by Deutsche Forschungsgemeinschaft within the Collaborative Research Center SFB 876, project A2",
    "creator" : "LaTeX with hyperref package"
  }
}