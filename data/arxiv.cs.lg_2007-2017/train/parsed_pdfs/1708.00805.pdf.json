{
  "name" : "1708.00805.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Variational Generative Stochastic Networks with Collaborative Shaping",
    "authors" : [ "Philip Bachman", "Doina Precup" ],
    "emails" : [ "PHIL.BACHMAN@GMAIL.COM", "DPRECUP@CS.MCGILL.CA" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Significant research effort has been directed towards developing models capable of effectively synthesizing samples from complicated distributions. We propose an approach to this problem whose goal is two-fold. We want to learn a distribution which is practically indistinguishable from the target distribution and we also want training, inference and sampling to be efficient. Our approach can be viewed as a class of Generative Stochastic Networks (GSNs) (Bengio et al., 2014). We show that any model trained with the walkback procedure (Bengio et al., 2013) is encompassed by our approach. Instead of using denoising autoencoders, we leverage recent variational methods for deep, directed generative models, e.g. (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014), and build our approach starting from variational auto-encoders. By feeding the output of such an auto-encoder back into it-\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\nself, we construct a Markov chain whose stationary distribution provably (in the non-parametric, infinite-data limit) converges to the target distribution.\nAs an alternative to the walkback procedure for training GSNs, we propose an approach based on recent work in Approximate Bayesian Computation. We partner a generative model with a function approximator that estimates the log-density ratio between the model-generated distribution and the target distribution, in what can be seen as a collaborative alternative to the adversarial approaches in (Gutmann et al., 2014a;b; Goodfellow et al., 2014). We show that the global minimizer of the resulting objective (in the non-parametric, infinite-data limit) is achievable only when the model distribution matches the target distribution.\nTo control the model complexity, we introduce a regularization term close in spirit to reinforcement learning methods such as relative entropy policy search (Peters et al., 2010) and other approaches which depend on a notion of “natural system dynamics”, e.g. (Todorov, 2009). Specifically, we re-weigh the standard KL(posterior || prior) term that appears in the variational free-energy. For a generative model p(x) = ∑ z p(x|z)p(z), with latent variables z ∈ Z , this approach provides a direct mechanism for trading complexity in p(x) against the ability to exactly reproduce the training distribution whenever most of the complexity in p(x) is captured by the latent variables. E.g. if p(x|z) is an isotropic Gaussian whose mean varies with z, but whose variance is the same for all z, restricting KL(p(z|x) || p(z)) to be small for all x forces p(x) to be approximately an isotropic Gaussian.\nOur models permit efficient generation of independent samples, efficient generation of sequences of samples representing “locally-coherent” random walks along the data manifold, and efficient evaluation of a variational lowerbound on the log-likelihood assigned to arbitrary inputs. We show that our approach produces models which significantly outperform the GSNs in (Bengio et al., 2014) and the adversarial networks in (Goodfellow et al., 2014) in terms of test-set log-likelihood and qualitative behavior.\nar X\niv :1\n70 8.\n00 80\n5v 1\n[ cs\n.L G\n] 2\nA ug\n2 01"
    }, {
      "heading" : "2. Background",
      "text" : "This section provides a summary of prior work on denoising auto-encoders and Generative Stochastic Networks, which constitute the basis of our model."
    }, {
      "heading" : "2.1. Generalized Denoising Auto-encoders",
      "text" : "In the Generalized Denoising Auto-encoder (DAE) framework (Bengio et al., 2013), one trains a reconstruction distribution pθ(x|x̃) to match the conditional distribution P(x|x̃) implicit in an infinite set of pairs {(x1, x̃1), ..., (xn, x̃n)} generated by drawing each xi ∈ X from the target distribution D and then generating each x̃i ∈ X by applying some stochastic corruption process qφ(x̃|x) to xi. Given qφ and pθ, where θ and φ denote parameters of the two distributions, one can construct a Markov chain over x ∈ X by iteratively sampling a point xt from pθ(xt|x̃t−1) and then sampling a point x̃t from qφ(x̃t|xt). The chain is initialized at t = 0 by sampling x0 directly fromD and its transition operator Tθ(xt|xt−1) can be computed by marginalizing over x̃t−1.\nGiven a few small assumptions on the forms of qφ and pθ, and the larger assumption that pθ(x|x̃) provides a consistent estimator of P(x|x̃) as the number of training samples x ∼ D goes to infinity, it was shown in (Bengio et al., 2013) that the Markov chain constructed from the iterative process described above will be ergodic and have a stationary distribution πθ which matches D (i.e. ∀x, πθ(x) = D(x)). All of the discussion in (Bengio et al., 2013) assumed that both xi and x̃i for each training pair (xi, x̃i) were from the same space X , although this was not required for their proofs. The Generative Stochastic Network (GSN) framework (Bengio et al., 2014) thus made the jump of assuming a corruption process qφ(zt|xt−1, zt−1). This extends the Generalized DAE framework by introducing a latent space Z 6= X , and by allowing the current latent state zt to depend on the previous latent state zt−1 (in addition to its dependence on the previous observable state xt−1). Figures 2 (a) and (b) illustrate the graphical models corresponding to Generalized DAEs and GSNs."
    }, {
      "heading" : "2.2. Training with Walkback",
      "text" : "A method called walkback training was proposed for Generalized DAEs in (Bengio et al., 2013) and used again for GSNs in (Bengio et al., 2014). The motivation for walkback training was to mitigate difficulties encountered in practical, finite-data settings, where many values for the latent variables z ∈ Z that were rarely (if ever) visited during training would appear when sampling from the resulting Markov chain. The difficulties stem primarily from a desire to make the corruption process qφ induce a conditional distribution P(x|z) which is roughly unimodal over x given\nAlgorithm 1 Walkback for a General GSN Input: data sample x, corruptor qφ, reconstructor pθ Initialize an empty training pair list Pxz = { } Set ẑ to some initial vector in Z . for i = 1 to kburn−in do\nSample ž from qφ(z|x, ẑ) then set ẑ to ž. end for Set x̂ to x. for i = 1 to kroll−out do\nSample ž from qφ(z|x̂, ẑ) then set ẑ to ž. Sample x̌ from pθ(x|ẑ) then set x̂ to x̌. Add pair (x, ẑ) to Pxz .\nend for Return: Pxz .\nany particular z (because this makes it easier to model with pθ(x|z)), which fights against the possibility that D contains multiple well-separated modes (which would necessitate a relatively non-local corruption process, able to “carve out” reliable paths between these modes during training).\nThe walkback procedure can be interpreted as a “wrapper” function which takes the corruption process qφ and the reconstruction distribution pθ, and then samples from a process W(z|x; qφ, pθ) which procedurally generates a distribution over z ∈ Z given any x ∈ X , as shown in Alg. 1. For example, in the original GSN paper (Bengio et al., 2014), the reconstruction distribution pθ(x|z) for a GSN which emulates Gibbs sampling in a Deep Boltzmann Machine was trained on pairs (x, z) sampled from the walkback process described in Alg 1. The returned set of pairs Pxz can be viewed as containing data (x, ẑ) ∼ W(z|x; qφ, pθ), where W is specified procedurally rather than directly. The reconstruction distribution pθ(x|z) is then trained to approximate the conditional Pxz(x|z) implicit in the pairs generated via Alg. 1."
    }, {
      "heading" : "3. Simple Generative Stochastic Networks",
      "text" : "We define a “Simple GSN” as any GSN in which the corruption process renders zt independent of zt−1 given xt−1. Simple GSNs thus represent the minimal, direct extension of Generalized DAEs to corruption processes that may produce outputs in a different space from their inputs. Fig. 1(c) shows the structure of a simple GSN based on iteratively sampling from a walkback process W(z|x; qφ, pθ) and a reconstruction distribution pθ(x|z). The Simple GSN model is in fact quite general, and covers all GSNs trained with a walkback procedure. We now give versions of the theorems from (Bengio et al., 2013) modified for Simple GSNs, which show that training with enough data and with sufficiently powerful function approximators pθ/qφ produces a Markov chain whose asymp-\ntotic distribution exists and matches the target distribution.\nTheorem 1. If pθ(x|z) is a consistent estimator of the true conditional distribution P(x|z) and the transition operator Tθ(xt+1|xt) that samples zt from qφ(zt|xt) and xt+1 from pθ(xt+1|zt) defines an ergodic Markov chain, then as the number of examples used to train pθ(x|z) goes to infinity (i.e. as pθ(x|z) converges to P(x|z)), the asymptotic distribution of the Markov chain defined by Tθ converges to the target distribution D.\nThe proof is a direct translation of the proof for Theorem 1 in (Bengio et al., 2013), but with zs replacing x̃s. Briefly, drawing an initial x from D and then sampling alternately from qφ(z|x) and P(x|z) is equivalent to sampling from a Gibbs chain for the joint distribution over (xi, zi) generated by repeatedly sampling xi ∼ D and zi ∼ qφ(z|xi). Ergodicity guarantees the existence of an asymptotic distribution for the Markov chain. Since pθ(x|z) converges to the true P(x|z), the asymptotic distribution of the chain converges to the marginal distribution of x in the Gibbs chain, which is just D(x). Corollary 2. Let X be a set in which every pair of points is connected by a finite-length path contained in X . Suppose that for each x ∈ X there exists a “shell” set Sx ⊆ X such that all paths between x and any point in X \\ Sx pass through some point in Sx whose shortest path to x has length > 0. Suppose that ∀x′ ∈ Sx ∪ {x},∃zxx′ such that qφ(zxx′ |x) > 0 and pθ(x′|zxx′) > 0. Then, the Markov chain with transition operator Tθ(xt+1|xt) =∑ z pθ(xt+1|z)qφ(z|xt) is ergodic.\nProof. The chain is aperiodic because the assumptions imply that ∀x,∃zxx such that qφ(zxx|x) > 0 and pθ(x|zxx) > 0, so Tθ(xt+1 = x|xt = x) > 0. To show that the chain is irreducible, note that by assumption, ∀x′ ∈ Sx, T (xt+1 = x′|xt = x) > 0. For any x′ 6∈ Sx, consider the short-\nest path from x to x′ and note that ∃y ∈ Sx on this path such that the shortest path between x and y is > 0 and Tθ(xt+1 = y|xt = x) > 0. Hence, T (xt+1 = x′|xt = x) > 0, as the path x → x′ can be decomposed into a finite sequence of finite-length segments, each with nonzero transition probability. Because the chain is over a finite state space it is also positive recurrent. As the chain is aperiodic, irreducible, and positive recurrent, it is also ergodic.\nThe restricted dependency structure of Simple GSNs lets us avoid some complications faced by the proofs in (Bengio et al., 2014). Our proof of Corollary 2 also avoids reliance on an ball, which does not work correctly in discrete or discontinuous spaces, in which paths starting at x with length > may contain no “segments” overlapping with the set of all paths starting at x with length ≤ . Training pθ(x|z) for any GSN using samples generated by walkback as described in Algorithm 1 corresponds to training a Simple GSN built around the reconstruction distribution pθ and corruption processW(z|x; qφ, pθ). The key observation here is that the samples in Pxz generated by Algorithm 1 are obtained by relating multiple sampled latent variables ẑ back to the single observable variable x given as input. In order to train pθ to be consistent with the joint distribution over (x, z) pairs generated by the Markov chain constructed from pθ and qφ(zt|xt−1, zt−1), it would actually be necessary to train pθ on pairs (xt−1, zt) generated by explicitly unrolling the chain. In Sec. 5 we present a mechanism based on Approximate Bayesian Computation that allows training pθ directly on the pairs (xt−1, zt) generated by unrolling a GSN’s Markov chain and applying backpropagation through time (BPTT).\nWalkback can be viewed as an effective way to construct a more dispersed distribution over the latent space Z than\nwould be provided by the original corruption process qφ. Though not explicitly stated in the existing work on GSNs, it seems that balancing between maximizing dispersion of the corruption process and the ease of modeling the reconstruction distribution pθ plays a role for GSNs analogous to balancing between minimizing the KL divergence KL(qφ(z|x)||p(z)) and maximizing the expected conditional log-likelihood Ez∼qφ(z|x) log pθ(x|z) when training a generative model pθ(x) with variational methods, or balancing between following the “natural dynamics” of the system and optimizing reward in policy search (Peters et al., 2010; Todorov, 2009). The next section expands on this relation."
    }, {
      "heading" : "4. Variational Simple GSNs",
      "text" : "We now develop a Simple GSN which can efficiently generate “locally-coherent” random walks along the manifold described by the target distribution D, efficiently generate independent (approximate) samples from the target distributionD, and efficiently evaluate a lower-bound on the loglikelihood assigned by the model to arbitrary inputs. We do this by replacing the denoising auto-encoders in existing examples of Generalized DAEs and GSNs with variational auto-encoders (Kingma & Welling, 2014; Rezende et al., 2014), while reinterpreting the two competing terms in the variational free-energy F (see Eq. 2) as representing an explicit trade-off between the dispersion of qφ(z|x) and the ease of modeling pθ(x|z). Suppose that, in addition to pθ(x|z) and qφ(z|x), we also have access to a distribution p∗(z) over Z (which could be learned or fixed a priori). Given these three distributions, we can define the derived distribution pθ(x; p∗) such that pθ(x; p∗) = ∑ z pθ(x|z)p∗(z), and the variational freeenergyF(x; qφ, pθ, p∗), which provides an upper-bound on the negative log-likelihood of x ∈ X under pθ(x; p∗): F(x; qφ, pθ, p∗) = (1) = −\n∑ z [qφ(z|x) log pθ(x|z)] + KL(qφ(z|x)||p∗(z))\n≥ − log pθ(x; p∗) (2) A step-by-step derivation is provided in the Appendix.\nGiven F(x; qφ, pθ, p∗), we can maximize a lower-bound on the expected log-likelihood of samples x ∼ D under the model pθ(x; p∗) by minimizing:\nEx∼D F(x; qφ, pθ, p∗) = (3) Ex∼D [\nE z∼qφ(z|x)\n[− log pθ(x|z)] + KL(qφ(z|x)||p∗(z)) ]\nIt is useful to compare this to the objective for Generalized DAEs, i.e. Eq. 4 in (Bengio et al., 2013):\nL(θ) = E x∼D,x̃∼qφ(z̃|z) [− log pθ(x|x̃) + λΩ(θ, x, x̃)] (4)\nin which Ω(θ, x, x̃) is a regularization term for controlling the capacity of pθ when the number of available samples x ∼ D is finite. The basic training process for both Eq. 3 and 4 can be described as follows: draw a sample x ∼ D, apply a random corruption to get z/x̃ ∼ qφ(·|x), then adjust the parameters to reduce − log pθ(x|·) and an added regularization term. Training with walkback simply changes the x̃ ∼ qφ(z̃|z) in Eq. 4 to x̃ ∼ W(x̃|x; pθ, qφ). We consider the objective in Eq. 3 from a GSN perspective and treat it as comprising two terms: a reconstruction error− log pθ(x|·) and a dispersion maximization term KL(qφ(·|x) || p∗). Based on a desire to keep qφ welldispersed for all x ∈ X , and to keep the degree of the dispersion relatively consistent across x ∈ X , we replace the basic KL term in Eq. 3 with the following:\nλ ( KL(qφ(·|x)||p∗) + γ([KL(qφ(·|x)||p∗)− K̄]+)2 ) , (5) in which K̄ = Ex∼X KL(qφ(·|x)||p∗) and [·]+ indicates positive rectification, i.e. clamping all negative values to 0. This penalizes both the magnitude and variance of the per-example KL, while avoiding any pressure to increase KL. We make this modification to allow tighter control over the trade-off between reconstruction fidelity and dispersion of the corruption process. Although re-weighting the KL term in Eq. 3 may seem odd to those already familiar with variational methods, we emphasize that the freeenergy F(x; qφ, pθ, p∗) from Eq. 2 still provides a valid upper-bound on − log pθ(x; p∗). The appendix provides further discussion of this variational free-energy."
    }, {
      "heading" : "5. Collaborative Generative Networks",
      "text" : "In this section we take a step back and present a general method for shaping the distribution G produced by a generative model gθ to be practically indistinguishable from a target distribution D. In Section 6 we combine this method with variational Simple GSNs to directly train unrolled Markov chains. The general approach of estimating the parameters of gθ to minimize some computable measure of dissimilarity between G and D is called Approximate Bayesian Computation. Examples of Approximate Bayesian Computation include spectral methods based on the “method of moments”, which learn the parameters of gθ so as to match some of the statistical moments of G to the corresponding moments observed in an empirical sample from D, and more recent approaches based on minimizing the ability of some classifier to distinguish between G and D (Gutmann et al., 2014a;b; Goodfellow et al., 2014). Motivated by the recent empirical success of this latter approach in training deep generative models (Goodfellow et al., 2014), we develop a related approach which offers improved stability and a simpler proof of correctness.\nWe define an objective for jointly optimizing a generator function gθ and a guide function fψ which shapes the distribution G produced by gθ to match a target distribution D. Our method can be interpreted as a collaboration between gθ and fψ , in contrast with the adversarial approach presented in (Goodfellow et al., 2014). It is based on optimizing a term which encourages fψ to approximate the log-density-ratio log D(x)G(x) for x ∈ X while also using fψ as a guidance signal for redistributing the mass emitted by gθ. Our objective comprises two parts: one optimized by fψ and the other optimized by gθ. We show that gθ and fψ can simultaneously minimize their respective objectives if and only if ∀x, G(x) = D(x) and fψ(x) = log D(x)G(x) = 0. The objective Lf for fψ is the basic logistic regression loss for a binary classifier which assumes equal prior probability for the positive class D and the negative class G, i.e.:\nLf = E x∼D [b(fψ(x))] + E x∼G [b(−fψ(x))] (6)\nwhere b(f) = log(exp(−f) + 1) is the binomial deviance loss. The objective Lg for gθ is based on, e.g., a one-sided absolute value loss:\nLg = E x∼G [|fψ(x)| · I[fψ(x) < 0]] (7)\nin which I[·] is a binary indicator function. Theorem 3. The objectives Lf and Lg are simultaneously optimized with respect to fψ and gθ if and only if ∀x ∈ X ,G(x) = D(x) and fψ(x) = log D(x)G(x) = 0.\nProof. By definition of Lf , it is minimized w.r.t. fψ if and only if ∀x, fψ(x) = log D(x)G(x) (Hastie et al., 2008). If Lf is minimized w.r.t. fψ then we know furthermore that either ∀x, fψ(x) = log D(x)G(x) = 0 or ∃x s.t. fψ(x) = log D(x) G(x) > 0 and ∃x′ s.t. fψ(x′) = log D(x ′)\nG(x′) < 0. The former situation results in Lg = 0. The latter situation results in Lg > 0, because |fψ(x)| · I[fψ(x) < 0] ≥ 0 with equality only when fψ(x) ≥ 0. Thus, whenever Lf is optimized w.r.t. fψ , Lg can obtain its minimum possible value of 0 if and only if ∀x, D(x) = G(x).\nNote that this proof works for any Lg which involves an expectation (w.r.t. x ∼ G) over a quantity which is everywhere non-negative and equal to 0 if and only if fψ(x) ≥ 0. We leave a detailed investigation of the relative merits of the various possible Lg for future work. The key characteristic that distinguishes our objective from (Goodfellow et al., 2014) is that, given a fixed guide function fψ , our objective for the generator gθ pushes the mass in over-dense regions of G towards zero-contours of fθ while leaving the mass in under-dense regions unmoved.\nIn contrast, the adversarial objective in (Goodfellow et al., 2014) drives all mass emitted by gθ towards local maxima of fψ . These local maxima will typically correspond to points in X , while the zero-contours sought by our objective will correspond to regions in X . We can also incorporate additional terms in Lg , under the restriction that the added terms are minimized when ∀x, D(x) = G(x). E.g. moment matching terms for matching the mean and covariance of x ∼ G with those of x ∼ D can be included to help avoid the occasional empirical “collapses” of G that were described in (Goodfellow et al., 2014)."
    }, {
      "heading" : "6. Generating Random Walks on Manifolds",
      "text" : "We now combine the variational Simple GSNs from Sec. 4 with the collaborative mechanism from Sec. 5. Our goal is to directly train the Markov chain constructed by unrolling the variational Simple GSN to produce locally-contiguous walks along the manifold of the target distributionD, and to have the asymptotic distribution of the chain approximate D. The collaborative mechanism described in Sec. 5 pairs a generator gθ with a guide function fψ . For the generator, we propose using a variational Simple GSN, which we unroll into a Markov chain by initializing with a sample x0 ∼ D and then repeatedly generating {x1, ..., xt, ..., xn} by sampling zt ∼ qφ(z|xt) xt+1 ∼ pθ(x|zt). In other words, we self-loop a variational auto-encoder by piping its output back into its input. The unrolled joint system is depicted in Fig. 2. In practice, we implement pθ, qφ and the guide function fψ using neural networks, whose specific architectures are described further in Sec. 7."
    }, {
      "heading" : "7. Experiments",
      "text" : "We now present tests examining the behavior of our models on the MNIST and TFD datasets. We chose these datasets\nto allow direct comparison with (Bengio et al., 2014) and (Goodfellow et al., 2014).\nOur first tests with MNIST data examined the benefits of training using the unrolled collaborative mechanism in Fig. 2. We represented qφ and pθ using neural networks with two hidden layers of 1500 rectified-linear units and we set the latent space Z to R64. We used a Gaussian with identity covariance for the prior distribution p∗(x). The output layer of qφ produced two vectors in R64, one representing the mean of a Gaussian distribution over Z and the other representing the element-wise log-variances of the distribution. Given this qφ/p∗, KL(qφ(z|x)||p∗) was easy to compute analytically, and its gradients with respect to φ were readily available. We interpreted pθ(x|z) as a factored Bernoulli distribution, with Rao-Blackwellisation over the possible binarizations of each x. I.e., the output layer of pθ produced a vector in R784, which was then passed through a sigmoid to get x̂. We minimized − log pθ(x|z) = −sum(x log x̂+ (1−x) log(1− x̂)), where denotes element-wise multiplication and we sum the vector entries. The gradients of − log pθ(x|z) w.r.t. θ were directly available, and we backpropped through sampling z ∼ qφ(z|x) to get gradients w.r.t. φ using the techniques in (Kingma & Welling, 2014; Rezende et al., 2014).\nWe used a Maxout network (Goodfellow et al., 2013) with two hidden layers of 1200 units in 300 groups of 4 for the guide function fψ . For Lg in Eq. 7, we used a halfrectified elastic-net (Zou & Hastie, 2005), with the linear and quadratic parts weighted equally. We unrolled our chains for 6 steps. The distributions D and G for training fψ according to Eq. 6 were given by the MNIST training set and the xi emitted by the unrolled chains. We passed gradients through the unrolled computation graph via BPTT.\nWe performed model updates using gradients computed from mini-batches of 100 distinct examples from the training set, each of which was passed through the model for 5 samples from qφ(z|x). We trained using plain SGD. We pre-trained pθ and qφ as a variational autoencoder (VAE) for 100k updates by running the model in Fig. 1(c) out to x1. After 100k VAE updates we “forked” the model into a “multi-step guided” model and a “one-step VAE” model and performed a further 100k updates to each of the now-independent models. We implemented our models in Python using the THEANO library (Bergstra et al., 2010). Code implementing the models described in this paper is available online at: github.com/Philip-Bachman/ICML-2015. The code provides full details on learning rates and other hyperparameters.\nFig. 3 illustrates the significantly improved long-run sampling behavior of chains trained with unrolling. The chains in Fig. 3 start at the top left and run from left-to-right and top-to-bottom. The true samples from D used to initialize the chains are in the top-left corners of (a)/(b). We downsampled the samples emitted by these chains 5x for this figure. Training with unrolling and collaborative guidance allows the chain to continually generate clean samples while exhibiting rapid mixing between the modes of the target distribution. Without explicit unrolling during training, the chain can only perform a few steps before degrading into samples that are well-separated from the target distribution. Qualitatively, the samples generated by the model trained with collaboratively-guided unrolling compare favourably with those presented for GSNs in (Bengio et al., 2014).\nFor our second tests with the MNIST images, we used networks with roughly the same structure as in our first tests but we set Z to R50, we used 1000 units in each of the hidden layers in qφ/pθ, and we used a Gaussian reconstruction distribution pθ(x|z). To effect this final change, we interpreted the vector produced by the output layer of pθ as the mean of a Gaussian distribution over X and we shared a single “bias” parameter across all z to model the elementwise log-variance of pθ(x|z). We thus modeled the target distribution using an infinite mixture of isotropic Gaussians, with the mixture weights of each Gaussian fixed a priori and with their individual locations and shared scale adjusted to match the training data.\nFor Fig. 4(a)-(d), we trained a pair of models. We trained the first model with λ = 4 and γ = 0.1 in Eq. 5. We refer to this model as ORK, for over-regularized KL. We trained the second model to minimize the basic free energy in Eq. 2. We refer to this model as VAR. As in our first MNIST experiments, we initialized each model with pretraining by running the model in Fig. 1(c) a single step. We performed 80k pre-training updates using mini-batches\nand SGD updates as described for our first experiments. We then performed another 120k updates by unrolling the chains for 6 steps following the graph in Fig. 2. The guide function fψ was trained as in our first experiments.\nFig. 4 illustrates interesting behaviors of the ORK and VAR models. We found that the ORK model with strong regularization on KL(qφ(z|x)||p∗(z)) was able to significantly out-perform the VAR model according to the Gaussian Parzen density estimator (GPDE) test described in (Breuleux et al., 2011)1. On the test set, the best ORK model scored 265, which compares favorably to the 214 in (Bengio et al., 2014) and the 225 in (Goodfellow et al., 2014). The best VAR model scored 220. The peak performance by this metric occurred much earlier in training for the VAR model than for the ORK model. Using the same network architecture, but with λ in Eq. 5 increased to 24, our approach achieved a score of 330 on the GPDE test.\nInterestingly, the GPDE log-likelihood bound began to decrease rapidly beyond a certain point in training. However, the variational bound continued to increase. Qualitatively, this behavior is clearly reflected in the samples shown in Fig. 4(d), which we drew directly from the models by sampling from their priors p∗(x). Samples generated from the ORK model remain reasonable throughout training, but eventually suffer on the GPDE bound due to excess “sharpness”. Samples drawn from the VAE model after 150k updates, when the VAE model significantly outperforms the ORK model in terms of the variational bound, are hardly recognizable as handwritten digits. In effect, the model is concentrating its posterior mass, as given by qφ(z|x), on increasingly small regions of Z , in exchange for significant reductions in the reconstruction cost − log pθ(x|z). This seems to lead to most of the mass of p∗(z) falling on zs which have little or no mass under any of the qφ(z|x). By forcing the qφ(z|x) to be more dispersed over Z , our added KL terms in Eq. 5 help mitigate this issue, albeit at the cost of less precise reconstruction of any particular digit. The scatter plots in Fig. 4 illustrate the evolution of the GPDE/variational bounds over the course of training and the trade-off between reconstruction cost and posterior KL that is obtained by the ORK and VAR models.\nWe performed analogous tests with the TFD dataset, which comprises 48x48 grayscale images of frontal faces in various expressions. We made a few changes from the second set of MNIST tests. We expanded the hidden layers of the networks representing qφ/pθ to 2000 rectified-linear units each and we expanded the latent space Z to R100. We pre-\n1Briefly, this test approximates the distribution of a generative model by drawing 10k samples from the model and then using those samples as the mixture means for a uniformly-weighted mixture of 10k Gaussians, with a shared isotropic variance selected for the mixture components based on a validation set\nprocessed the images to have pixel intensities in the range [0...1], as in (Bengio et al., 2014; Goodfellow et al., 2014). We extended the pre-training phase to 150k updates and the unrolled, collaboratively-guided phase was reduced to 60k updates. Interestingly, in these tests the ORK model did not suffer significantly in terms of the variational bound, while achieving dramatically improved performance on the GPDE bound. For comparison, best previous results on the GPDE bound for this dataset are 2050 (Goodfellow et al., 2014) and 2110 (Bengio et al., 2013). Our ORK model scored 2060 on the test set using a GPDE variance selected on the validation set. When we increased λ for the ORK model from 5 to 30, the GPDE score increased to 2130."
    }, {
      "heading" : "8. Discussion",
      "text" : "We presented an approach for learning generative models belonging to a simple subset of GSNs, using variational auto-encoders as building blocks. We generated Markov chains by looping the output of these auto encoders back into the input, and trained them to generate random walks along a target manifold, based on feedback from a guide function trained to discriminate between samples emitted by the chain and samples drawn from the data manifold. A key conceptual contribution of our approach is that we run the generative process as an unrolled Markov chain according to its a natural dynamics, i.e. the same way we want to run it “in the wild”, and then correct differences between exhibited and desired behavior by providing direct feedback about their magnitude and location (instead of trying to force the behavior in some way during the generation process). The experimental evaluation demonstrates that this direct approach is beneficial in practice.\nIn the long run, we believe it will be interesting to focus on interpreting our method from a reinforcement learning point of view. In this case, the “ease of modeling” the posterior distribution p(x|z) can be viewed as a reward to be maximized, and may be easily replaced or augmented with other sources of reward. The current approach of using back propagation through time for the training could also be replaced by more efficient methods based on eligibility traces. While we only considered generating random walks over manifolds in this paper, in future work we would like to apply our approach to modeling distributions over observed trajectories on manifolds, e.g., as seen in speech, video, motion capture, and other sequential data."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Funding for this work was provided by NSERC. The authors would also like to thank the anonymous reviewers for providing helpful feedback."
    } ],
    "references" : [ {
      "title" : "Generalized denoising auto-encoders as generative models",
      "author" : [ "Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "A cpu and gpu math expression compiler",
      "author" : [ "J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Bengio", "Y. Theano" ],
      "venue" : "In Python for Scientific Computing Conference (SciPy),",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2010
    }, {
      "title" : "Quickly generating representative samples from an rbmderived process",
      "author" : [ "Breuleux", "Olivier", "Bengio", "Yoshua", "Vincent", "Pascal" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Breuleux et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Breuleux et al\\.",
      "year" : 2011
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Goodfellow", "Ian J", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Classifier abc",
      "author" : [ "Gutmann", "Michael U", "Dutta", "Ritabrata", "Kaski", "Samuel", "Corander", "Jukka" ],
      "venue" : "In MCMSki IV (posters),",
      "citeRegEx" : "Gutmann et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gutmann et al\\.",
      "year" : 2014
    }, {
      "title" : "Likelihood-free inference via classification",
      "author" : [ "Gutmann", "Michael U", "Dutta", "Ritabrata", "Kaski", "Samuel", "Corander", "Jukka" ],
      "venue" : "In arXiv:1407.4981v1 [stat.CO],",
      "citeRegEx" : "Gutmann et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gutmann et al\\.",
      "year" : 2014
    }, {
      "title" : "Elements of Statistical Learning II",
      "author" : [ "Hastie", "Trevor", "Friedman", "Jerome", "Tibshirani", "Robert" ],
      "venue" : null,
      "citeRegEx" : "Hastie et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2008
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Kingma", "Diederik P", "Welling", "Max" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Neural variational inference and learning",
      "author" : [ "Mnih", "Andriy", "Gregor", "Karol" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Mnih et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2014
    }, {
      "title" : "Relative entropy policy search",
      "author" : [ "Peters", "Jan", "Mulling", "Karen", "Altun", "Yasemin" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Peters et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2010
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Rezende", "Danilo", "Mohamed", "Shakir", "Wierstra", "Daan" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient computation of optimal action",
      "author" : [ "Todorov", "Emanuel" ],
      "venue" : "Proceedings of the National Academy of Science (PNAS),",
      "citeRegEx" : "Todorov and Emanuel.,? \\Q2009\\E",
      "shortCiteRegEx" : "Todorov and Emanuel.",
      "year" : 2009
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "Zou", "Hui", "Hastie", "Trevor" ],
      "venue" : "Journal of the Royal Statistical Society, B.,",
      "citeRegEx" : "Zou et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We show that any model trained with the walkback procedure (Bengio et al., 2013) is encompassed by our approach.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "(Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014), and build our approach starting from variational auto-encoders.",
      "startOffset" : 0,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "We partner a generative model with a function approximator that estimates the log-density ratio between the model-generated distribution and the target distribution, in what can be seen as a collaborative alternative to the adversarial approaches in (Gutmann et al., 2014a;b; Goodfellow et al., 2014).",
      "startOffset" : 250,
      "endOffset" : 300
    }, {
      "referenceID" : 9,
      "context" : "To control the model complexity, we introduce a regularization term close in spirit to reinforcement learning methods such as relative entropy policy search (Peters et al., 2010) and other approaches which depend on a notion of “natural system dynamics”, e.",
      "startOffset" : 157,
      "endOffset" : 178
    }, {
      "referenceID" : 3,
      "context" : ", 2014) and the adversarial networks in (Goodfellow et al., 2014) in terms of test-set log-likelihood and qualitative behavior.",
      "startOffset" : 40,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "In the Generalized Denoising Auto-encoder (DAE) framework (Bengio et al., 2013), one trains a reconstruc-",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "Given a few small assumptions on the forms of qφ and pθ, and the larger assumption that pθ(x|x̃) provides a consistent estimator of P(x|x̃) as the number of training samples x ∼ D goes to infinity, it was shown in (Bengio et al., 2013) that the Markov chain constructed from the iterative process described above will be ergodic and have a stationary distribution πθ which matches D (i.",
      "startOffset" : 214,
      "endOffset" : 235
    }, {
      "referenceID" : 0,
      "context" : "All of the discussion in (Bengio et al., 2013) assumed that both xi and x̃i for each training pair (xi, x̃i) were from the same space X , although this was not required for their proofs.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "A method called walkback training was proposed for Generalized DAEs in (Bengio et al., 2013) and used again for GSNs in (Bengio et al.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "We now give versions of the theorems from (Bengio et al., 2013) modified for Simple GSNs, which show that training with enough data and with sufficiently powerful function approximators pθ/qφ produces a Markov chain whose asymp-",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "The proof is a direct translation of the proof for Theorem 1 in (Bengio et al., 2013), but with zs replacing x̃s.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "Though not explicitly stated in the existing work on GSNs, it seems that balancing between maximizing dispersion of the corruption process and the ease of modeling the reconstruction distribution pθ plays a role for GSNs analogous to balancing between minimizing the KL divergence KL(qφ(z|x)||p(z)) and maximizing the expected conditional log-likelihood Ez∼qφ(z|x) log pθ(x|z) when training a generative model pθ(x) with variational methods, or balancing between following the “natural dynamics” of the system and optimizing reward in policy search (Peters et al., 2010; Todorov, 2009).",
      "startOffset" : 549,
      "endOffset" : 585
    }, {
      "referenceID" : 10,
      "context" : "We do this by replacing the denoising auto-encoders in existing examples of Generalized DAEs and GSNs with variational auto-encoders (Kingma & Welling, 2014; Rezende et al., 2014), while reinterpreting the two competing terms in the variational free-energy F (see Eq.",
      "startOffset" : 133,
      "endOffset" : 179
    }, {
      "referenceID" : 0,
      "context" : "4 in (Bengio et al., 2013):",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "Examples of Approximate Bayesian Computation include spectral methods based on the “method of moments”, which learn the parameters of gθ so as to match some of the statistical moments of G to the corresponding moments observed in an empirical sample from D, and more recent approaches based on minimizing the ability of some classifier to distinguish between G and D (Gutmann et al., 2014a;b; Goodfellow et al., 2014).",
      "startOffset" : 367,
      "endOffset" : 417
    }, {
      "referenceID" : 3,
      "context" : "Motivated by the recent empirical success of this latter approach in training deep generative models (Goodfellow et al., 2014), we develop a related approach which offers improved stability and a simpler proof of correctness.",
      "startOffset" : 101,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "Our method can be interpreted as a collaboration between gθ and fψ , in contrast with the adversarial approach presented in (Goodfellow et al., 2014).",
      "startOffset" : 124,
      "endOffset" : 149
    }, {
      "referenceID" : 6,
      "context" : "fψ if and only if ∀x, fψ(x) = log D(x) G(x) (Hastie et al., 2008).",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "The key characteristic that distinguishes our objective from (Goodfellow et al., 2014) is that, given a fixed guide function fψ , our objective for the generator gθ pushes the mass in over-dense regions of G towards zero-contours of fθ while leaving the mass in under-dense regions unmoved.",
      "startOffset" : 61,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "In contrast, the adversarial objective in (Goodfellow et al., 2014) drives all mass emitted by gθ towards local maxima of fψ .",
      "startOffset" : 42,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "moment matching terms for matching the mean and covariance of x ∼ G with those of x ∼ D can be included to help avoid the occasional empirical “collapses” of G that were described in (Goodfellow et al., 2014).",
      "startOffset" : 183,
      "endOffset" : 208
    }, {
      "referenceID" : 3,
      "context" : ", 2014) and (Goodfellow et al., 2014).",
      "startOffset" : 12,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "φ using the techniques in (Kingma & Welling, 2014; Rezende et al., 2014).",
      "startOffset" : 26,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "We implemented our models in Python using the THEANO library (Bergstra et al., 2010).",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "on the validation set log-likelihood provided by the Gaussian Parzen density estimator described in (Breuleux et al., 2011) and the",
      "startOffset" : 100,
      "endOffset" : 123
    }, {
      "referenceID" : 2,
      "context" : "We found that the ORK model with strong regularization on KL(qφ(z|x)||p∗(z)) was able to significantly out-perform the VAR model according to the Gaussian Parzen density estimator (GPDE) test described in (Breuleux et al., 2011)1.",
      "startOffset" : 205,
      "endOffset" : 228
    }, {
      "referenceID" : 3,
      "context" : ", 2014) and the 225 in (Goodfellow et al., 2014).",
      "startOffset" : 23,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : "1], as in (Bengio et al., 2014; Goodfellow et al., 2014).",
      "startOffset" : 10,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "For comparison, best previous results on the GPDE bound for this dataset are 2050 (Goodfellow et al., 2014) and 2110 (Bengio et al.",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : ", 2014) and 2110 (Bengio et al., 2013).",
      "startOffset" : 17,
      "endOffset" : 38
    } ],
    "year" : 2017,
    "abstractText" : "We develop an approach to training generative models based on unrolling a variational autoencoder into a Markov chain, and shaping the chain’s trajectories using a technique inspired by recent work in Approximate Bayesian computation. We show that the global minimizer of the resulting objective is achieved when the generative model reproduces the target distribution. To allow finer control over the behavior of the models, we add a regularization term inspired by techniques used for regularizing certain types of policy search in reinforcement learning. We present empirical results on the MNIST and TFD datasets which show that our approach offers state-of-the-art performance, both quantitatively and from a qualitative point of view.",
    "creator" : "LaTeX with hyperref package"
  }
}