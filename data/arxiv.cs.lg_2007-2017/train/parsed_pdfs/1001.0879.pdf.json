{
  "name" : "1001.0879.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Linear Probability Forecasting",
    "authors" : [ "Fedor Zhdanov", "Yuri Kalnishkan" ],
    "emails" : [ "fedor@cs.rhul.ac.uk", "yura@cs.rhul.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 1.\n08 79\nv1 [\ncs .L\nG ]\nMulti-class classification is one of the most important tasks in machine learning. In this paper we consider two online multi-class classification problems: classification by a linear model and by a kernelized model. The quality of predictions is measured by the Brier loss function. We suggest two computationally efficient algorithms to work with these problems and prove theoretical guarantees on their losses. We kernelize one of the algorithms and prove theoretical guarantees on its loss. We perform experiments and compare our algorithms with logistic regression."
    }, {
      "heading" : "1 Introduction",
      "text" : "Online prediction is a wide area of machine learning (see Cesa-Bianchi and Lugosi, 2006). Its algorithms can be applied to different data mining problems (see for example Freund and Schapire, 1997). Online prediction provides efficient algorithms which adapt to a predicted process “on fly”. In online regression framework we assume the existence of some input at each step and try to predict an outcome on this input. This process is repeated step by step. We consider multi-dimensional Brier game where outcomes and predictions come from a simplex and can be thought of as probability distributions on the vertices of the simplex. If the outcomes are identified with vertices of the simplex this problem can be thought of as the multi-class classification problem of the given input.\nIn the simple case the dependence between the input and its outcome is assumed to be linear; linear regression minimising the expected loss is studied in statistics. As opposite to the traditional statistical setting, the learner in online prediction does not make any statistical assumptions about the data generating process. Its goal is to predict as well as the best linear function on input. Instead of looking for the best linear function, our learner considers all linear functions and makes his prediction by mixing them in a certain way at each prediction step. We prove theoretical bounds on the cumulative loss of the learner in comparison with the cumulative loss of the best linear function (we\nsay the learner competes with these functions). We consider the square loss: mean square error is one of the benchmark measures for classification algorithms (see Brier, 1950).\nWe use Vovk’s Aggregating Algorithm (a generalization of the Bayesian mixture) to mix functions (as in Aggregating Algorithm Regression, AAR: see Vovk, 2001). This method has previously been applied to the case when possible outcomes lie in a segment of the real line, and so the prediction was one-dimensional. We develop two algorithms to solve the problem of multi-dimensional prediction. The first algorithm applies a variant of AAR to predict each coordinate of the outcome separately, and then combines these predictions in a certain way to get probability prediction. The other algorithm is designed to give probability predictions directly; these are first computationally efficient online regression algorithm designed to solve linear and non-linear multi-class classification problems. We derive theoretical bounds on the losses of both algorithms. We come to an unexpected conclusion that the component-wise algorithm is better than the second one asymptotically, but worse in the beginning of the prediction process. Their performance on benchmark data sets is very similar.\nOne component of the prediction of the second algorithm has the meaning of a remainder. In practice this situation is quite common. For example, in a football match either one team wins or the other, and the remainder is a draw (see Vovk and Zhdanov (2008) for online prediction experiments in football). When we analyse a precious metal alloy we may look for a description of the following kind: the alloy has 40% of gold, 35% of silver, and some addition (e.g., copper and palladium). It is common for financial applications to predict the direction of the price: the price can go up, down, or stay close to the current value. We perform classification experiments with linear algorithms and compare them with logistic regression.\nA description of the framework can be found in Section 2, description of the algorithms can be found in Section 3, and derivation of the theoretical bounds can be found in Section 4.\nWe look for a way to extend the class of experts using the kernel trick. We kernelize the second algorithm and prove a theoretical bound on its loss. The cumulative loss of the kernelized algorithm is compared with the cumulative loss of any finite set of functions from the RKHS given by a kernel parameter it uses. Kernelization process is described in Section 5. Our experiments are shown in Section 6. Section 7 makes the conclusions and shows some possibilities for prospective work."
    }, {
      "heading" : "2 Framework",
      "text" : "A game of prediction contains three components: a space Ω of outcomes, a decision space Γ, and a loss function λ : Ω × Γ → R. We are interested in the generalisation of the Brier game from Brier (1950) where the space of outcomes Ω = P(Σ) is the set of all probability measures on a finite set Σ with d elements, Γ := {(γ1, . . . , γd) : ∑d i=1 γi = 1, γi ∈ R} is a hyperplane in d-dimensional space\ncontaining all the outcomes, and for any y ∈ Ω we define the loss\nλ(y, γ) = ∑\nσ∈Σ\n(γ{σ} − y{σ})2 .\nFor example, if Ω = {1, 2, 3}, ω = 1, γ{1} = 1/2, γ{2} = 1/4, and γ{3} = 1/4, λ(ω, γ) = (1/2−1)2+(1/4−0)2+(1/4−0)2 = 3/8. Brier loss is one of the most important loss functions used to assess the quality of classification algorithms. The game of prediction is being played repeatedly by a learner receiving some input vectors xt ∈ X ⊆ Rn, and follows prediction protocol 1.\nProtocol 1 Protocol of forecasting game\nL0 := 0. for t = 1, 2, . . . do Reality announces a signal xt ∈ X ⊆ Rn. Learner announces γt ∈ Γ ⊆ Rd. Reality announces yt ∈ Ω ⊆ Rd. Lt := Lt−1 + λ(yt, γt). end for\nWe find an algorithm which is capable of competing with all linear functions (we call them experts) ξt = (ξ 1 t , . . . , ξ d t ) ′ on x:\nξ1t = 1/d+ α ′ 1xt\n. . .\nξd−1t = 1/d+ α ′ d−1xt (1)\nξdt = 1− ξ1 − · · · − ξd−1 = 1/d− ( d−1∑\ni=1\nαi )′ xt,\nwhere αi = (α 1 i , . . . , α n i ) ′, i = 1, . . . , d− 1. In the model (1) the prediction for the last component of an outcome is calculated from the predictions for other components. Denote α = (α′1, . . . , α ′ d−1)\n′ ∈ Θ = Rn(d−1). Then any expert can be presented as ξt = ξt(α). Let also LT (α) = ∑T t=1 λ(yt, ξt(α)) be the cumulative loss of an expert α over T trials."
    }, {
      "heading" : "3 Derivation of the algorithms",
      "text" : "In this section we describe how we apply the Aggregating Algorithm (AA) proposed in Vovk (1990) to mix experts and make predictions. The algorithm keeps weights Pt−1(dα) for the experts at each prediction step t, and updates them by the exponential weighting scheme after the actual outcomes is announced:\nPt(dα) = β λ(yt,ξt(α))Pt−1(dα), β ∈ (0, 1). (2)\nHere β = e−η, where η ∈ (0,∞) is a learning rate parameter. This weight update ensures that the experts which predict badly at the step t receive less weight. The weights are then normalized P ∗t (dα) = Pt(dα) Pt(Θ) .\nThe prediction of the algorithm is a combination of the experts’ predictions. It is suggested in Kivinen and Warmuth (1999) that the prediction is simply the weighted average of the experts’ predictions with weights Pt(dα). The Aggregating Algorithm uses more sophisticated prediction scheme, and sometimes achieves better theoretical performance. It first defines a generalised prediction at any step t as a function gt : Ω → R such that\ngt(y) = logβ\n∫\nΘ\nβλ(y,ξt(α))P ∗t−1(dα) (3)\nfor all y ∈ Ω. It is a weighted average (in a general sense) of the experts’ losses for each possible outcome. It then predicts any γt such that\nλ(y, γt) ≤ gt(y) (4)\nfor all possible y ∈ Ω. If such prediction can be found for any weights distribution on experts the game is called perfectly mixable. Perfectly mixable games and other types of games are analyzed in Vovk (1998). It is also shown there that for countable (and thus finite) number of experts the AA achieves the best possible theoretical guarantees."
    }, {
      "heading" : "3.1 Proof of mixability",
      "text" : "In this section we prove that our game is perfectly mixable and show a function that can be used to give predictions satisfying (4).\nIt is shown in Theorem 1 Vovk and Zhdanov (2008) that the Brier game with finite number of outcomes is perfectly mixable iff η ∈ (0, 1]. The two authors of that paper consider the outcome space of d probability measures concentrated in points of Σ. We denote this space by R(Σ). They consider experts giving predictions from all probability measures P(Σ). We need to prove that the inequality (4) holds for our experts (1) (who can give predictions outside of the probability simplex) and our outcome space Ω (the whole probability simplex, not only its vertices). Lemma 2 describes the first part, but first we need to state an additional statement. The following lemma shows that any vector from R d can be projected into simplex without increasing the Brier loss.\nLemma 1. For any ξ = (ξ1, . . . , ξd) ∈ Rd there exists θ = (θ1, . . . , θd) ∈ P(Σ) such that for any y ∈ Ω we have λ(y, θ) ≤ λ(y, ξ).\nProof. The Brier loss of a prediction γ is a square Euclidean distance between γ and the actual outcome y in a d-dimensional space. The proof follows from the fact that Ω is a convex and closed set in Rd.\nLemma 2. Let P (dα) be any probability distribution on Θ. Then for any η ∈ (0, 1] there exists γ ∈ Γ such that for any y ∈ R(Σ) we have\nλ(y, γ) ≤ logβ ∫\nΘ\nβλ(y,ξ(α))P (dα).\nProof. By Lemma 1 for any ξ(α) we can find θ(α) ∈ P(Σ) such that the loss of experts decreases: λ(y, θ(α)) ≤ λ(y, ξ(α)) for any y ∈ R(Σ). Thus we have\nlogβ\n∫\nΘ\nβλ(y,θ(α))P (dα) ≤ logβ ∫\nΘ\nβλ(y,ξ(α))P (dα)\nfor any y ∈ R(Σ). We can take the same prediction γ ∈ Γ that satisfies the necessary inequality with θ instead of ξ. By Theorem 1 in Vovk and Zhdanov (2008) such prediction exists for any η ∈ (0, 1] (β ∈ [e−1, 1)).\nA way to convert the generalised prediction into the prediction of AA is called a substitution function. We prove that we can use the same substitution function and the same learning rate parameter η as for the case of finite number of possible outcomes. Such a function is proposed in Vovk and Zhdanov (2008). This is an extension of Lemma 4.1 from Haussler et al. (1998).\nLemma 3. Let P (dα) be a probability distribution on Θ and put\nf(y) = logβ\n∫\nΘ\nβλ(y,ξ(α))P (dα)\nfor every y ∈ Ω. Then if γ is such a prediction that λ(z, γ) ≤ f(z) for any z ∈ R(Σ) then λ(y, γ) ≤ f(y) for any y ∈ Ω.\nProof. For the typographical reasons we will write ξ instead of ξ(α). It is easy to ensure that λ(y, γ)− λ(y, ξ) = ∑σ∈Σ y{σ}[λ(zσ, γ)− λ(zσ, ξ)] for zσ{ρ} = 0 if σ 6= ρ and zσ{ρ} = 1 if σ = ρ. We also have that λ(y, γ) − f(y) ≤ 0 is equivalent to ∫ Θ β\nλ(y,ξ)−λ(y,γ)P (dα) ≤ 1. Thus due to the convexity of the exponent function ∫ Γ β ∑ σ∈Σ y{σ}[λ(zσ,ξ)−λ(zσ,γ)]P (dα) ≤ ∑σ∈Σ y{σ} = 1.\nLet us denote the i-th possible outcome from R(Σ) by y{i}, i = 1, . . . , d. We use the substitution function defined by the following proposition:\nProposition 1. Let ri = g(y{i}), and x+ = max(x, 0). Define s ∈ R by the requirement\nd∑\ni=1\n(s− ri)+ = 2.\nIf the prediction of the Aggregating Algorithm is given by\nγi = (s− ri)+\n2 , i = 1, . . . , d\nthen (4) holds.\nThis function allows us to avoid weights normalization in calculating the generalized prediction at each step (avoid ∗ in the weights distribution), which would be computationally inefficient. Suppose we can get only r = gt(y) + C instead of gt(y), where C is the same for all y. Then predictions γt defined by the substitution function from Proposition 1 will be the same as if we calculated the generalized prediction with weights normalization."
    }, {
      "heading" : "3.2 Algorithm for multidimensional outcomes",
      "text" : "We set the prior weights distribution P0 over the set Θ = R n(d−1) of experts α to have the Gaussian density with a parameter a > 0:\n(aη/π)n(d−1)/2e−aη‖α‖ 2 dα.\nInstead of taking the integral in (3) we get a shifted generalised prediction r by calculating ri = gT (y{i}) − gT (y{d}) (we omit the index T in r for brevity). Each component of r = (r1, . . . , rd) corresponds to one of the possible outcomes, so rd = 0. Other components, i = 1, . . . , d− 1:\nri = logβ βgT (y{i})+\n∑T−1 t=1 gt(yt)\nβgT (y{d})+ ∑T−1 t=1 gt(yt) = logβ\n∫ Θ e\n−ηQ(α,y{i})dα∫ Θ e−ηQ(α,y{d})dα\nwhere by Q(α, y) we denote the quadratic form:\nQ(α, y) =\nT∑\nt=1\nd∑\ni=1\n((yit − ξi(xt))2.\nHere yt = (y 1 t , . . . , y d t ) are the outcomes on the steps before T and yT = (y1T , . . . , y d T ) is a possible outcome on the step T .\nLet C = ∑T\nt=1 xtx ′ t be n× n matrix. The quadratic form Q can be divided\ninto a quadratic part, a linear part, and a remainder: Q = Q1 +Q2 +Q3. Here\nQ1(α, y) = α ′Aα\nis a quadratic part of Q(α, y). Here A is a square matrix with n(d − 1) rows (see the expression for A in the algorithm below). The linear part is equal to\nQ2(α, y) = h ′α− 2\nd−1∑\ni=1\n(yiT − ydT )α′ixT ,\nwhere hi = −2 ∑T−1 t=1 (y i t − ydt )xt, i = 1, . . . , d − 1 make up a big vector h = (h′1, . . . , h ′ d−1) ′. The remainder is equal to\nQ3(α, y) =\nT−1∑\nt=1\nd∑\ni=1\n(yit − 1/d)2 + d∑\ni=1\n(yiT − 1/d)2.\nRatio for ri can be calculated using the following lemmas. The integral evaluates as follows:\nLemma 4. Let Q(α) = α′Aα+ b′α+ c, where α, b ∈ Rn, c is a scalar and A is a symmetric positive definite n× n matrix. Then\n∫\nRn\ne−Q(α)dα = e−Q0 πn/2√ detA ,\nwhere Q0 = minα∈Rn Q(α).\nThe proof of this lemma can be found in Harville (1997, Theorem 15.12.1). Following this lemma, we can rewrite ri as ri = F (A, bi, zi), i = 1, . . . , d − 1, where\nF (A, bi, zi) = min α∈Θ Q(α, yi)−min α∈Θ Q(α, yd).\nVariables bi, zi and the precise formula for F are defined by the following lemma\nLemma 5. Let\nF (A, b, z) = min α∈Rn (α′Aα + b′α+ z′α)− min α∈Rn (α′Aα+ b′α− z′α),\nwhere b, z ∈ Rn and A is a symmetric positive definite n × n matrix. Then F (A, b, z) = −b′A−1z.\nProof. This lemma is proven by taking the derivative of the quadratic forms in F by α and calculating the minimum: minα∈Rn(α ′Aα + c′α) = − (A −1c)′\n4 c for any c ∈ Rn (see Harville, 1997, Theorem 19.1.1).\nWe can see that bi = h+(x ′ T , . . . , x ′ T ,0, x ′ T , . . . , x ′ T ) ′ ∈ Rn(d−1), where 0 is a zero-vector from Rn. We also have zi = (−x′T , . . . ,−x′T ,−2x′T ,−x′T , . . . ,−x′T )′. Thus we can calculate d − 1 differences ri, assign rd = 0, and then apply the substitution function from proposition 1 to get predictions. The resulting algorithm is Algorithm 1. We will further call it mAAR (multi-dimensional Aggregating Algorithm for Regression)."
    }, {
      "heading" : "3.3 Component-wise algorithm",
      "text" : "In this section we derive the component-wise algorithm. It gives predictions for each component of the outcome separately, and then combines them in a special way.\nFirst we explain why we should not directly use the algorithm and the theoretical bound proposed in Vovk (2001). Vovk’s experts do not allow us to take advantage of the fact that only one outcome is possible to happen at each moment. They are more suitable for the case when each input vector x can belong to many classes simultaneously in case of classification. In other words, they are centered around the center 1/2 of the prediction interval [0, 1]: ξi = 1/2 + αix. Assume that the number of outcomes is very large and the distribution on experts is normal N(0, σ2) with small σ. Then the average experts’ prediction is (1/2, . . . , 1/2, 1−(d−1)/2)), and the average loss of the experts on trials with the same outcome y = y{i} (we can take y = (1, 0, . . . , 0)) is (d−1)/22+(d−1)2/22.\nAlgorithm 1 mAAR for the Brier game\nFix n, a > 0. C = 0, h = 0. for t = 1, 2, . . . do Read new xt ∈ X.\nC = C + xtx ′ t, A = aI +   2C · · · C ... . . . ...\nC · · · 2C\n \nSet bi = h + (x ′ t, . . . , x ′ t, 0, x ′ t, . . . , x ′ t) ′, where 0 is a zero-vector from Rn is placed at i-th position, i = 1, . . . , d− 1. Set zi = (−x′t, . . . ,−x′t,−2x′t,−x′t, . . . ,−x′t)′, where −2x′t is placed at i-th position, i = 1, . . . , d− 1. Calculate ri := −b′iA−1zi, rd := 0, i = 1, . . . , d− 1. Solve ∑d i=1(s− ri)+ = 2 in s ∈ R. Set γit := (s− ri)+/2, ω ∈ Ω, i = 1, . . . , d. Output prediction γt ∈ P(Ω). Read observation yt. hi = hi − 2(yit − ydt )xt, h = (h′1, . . . , h′d−1)′.\nend for\nComponents of experts (1) concentrate around the point 1/d, and so experts have the average loss (d− 1)/d2 + (1− 1/d)2. This loss is smaller than the loss of Vovk’s experts for large values of d.\nOur component-wise experts are expressed by\nξit = 1/d+ α ′ ixt, i = 1, . . . , d. (5)\nThe derivation of the component-wise algorithm (further cAAR stands for componentwise Aggregating Algorithm Regression) is similar to the derivation of Algorithm 1 for two outcomes. The initial distribution on each component of experts (5) is given by\n(aη̃/π)n/2e−aη̃‖αi‖ 2\ndαi.\nNote that the value for η̃ here will be different from 1 since the loss function by each component is half of the Brier loss λ(y, γ) = (y − γ)2 + (1− y − (1− γ))2. We will further see that η̃ = 2. The loss of expert ξ(αi) over the first T trials is\nT∑\nt=1\n(yit−1/d−α′ixt)2 = α′i\n( T∑\nt=1\nxtx ′ t ) αi−2α′i ( T∑\nt=1\n(yit − 1/d)xt ) + T∑\nt=1\n(yit−1/d)2.\nInstead of the substitution function from Proposition 1 we use the substitution function suggested in Vovk (2001) for the one-dimensional game:\nγiT = 1\n2 + gT (0)− gT (1) 2\nTherefore, the substitution function can be represented as\nγiT = 1\n2 +\n1 2 logβ̃\nβ̃gT (0)\nβ̃gT (1)\n= 1\n2 +\n1 2 logβ̃\n∫ Rn e−η̃α ′ iBαi+2η̃α ′ i(E+(0−1/d)xT )−η̃(W+1/d\n2)dαi∫ Rn e−η̃α ′ i Bαi+2η̃α′i(E+(1−1/d)xT )−η̃(W+(1−1/d) 2)dαi\n= 1\nd +\n1 2 F\n( B,−2E − d− 2\nd xT , xT\n)\n= 1\nd +\n( T−1∑\nt=1\n(yit − 1/d)x′t + d− 2 2d x′T\n)( aI + T∑\nt=1\nxtx ′ t )−1 xT (6)\nfor i = 1, . . . , d. Here B = aI + ∑T\nt=1 xtx ′ t, E = ∑T−1 t=1 (y\ni t − 1/d)xt, W =∑T−1\nt=1 (y i t − 1/d)2, β̃ = e−η̃. The transitions are justified using Lemma 4 and Lemma 5. Then this method projects its prediction onto the prediction simplex such that the loss does not increase. We use the projection algorithm suggested in Michelot (1986).\nAlgorithm 2 Projection of a point from Rn onto probability simplex.\nInitialize I = ∅, x = 1 ∈ Rd. Let γT be the prediction vector and |I| is the dimension of the set I. while 1 do γT = γT − ∑d i=1 γiT−1\nd−|I| ;\nγiT = 0, ∀i ∈ I; If γiT ≥ 0 for all i = 1, . . . , d then break; I = I\n⋃{i : γiT < 0}; If γiT < 0 for some i then γ i T = 0;\nend while"
    }, {
      "heading" : "4 Theoretical bound",
      "text" : "We derive the theoretical bounds for the losses of Algorithm 1 and of a naive component-wise algorithm predicting in the same framework."
    }, {
      "heading" : "4.1 Component-wise algorithm",
      "text" : "We prove here the theoretical bound for the loss of cAAR. The following lemma is the main tool helping us to prove our theorems. It is easy to prove the following statement (Lemma 1 from Vovk (2001)):\nLemma 6. If the learner follows the Aggregating Algorithm in a perfectly mixable game, then for every positive integer T , every sequence of outcomes of the\nlength T , and any initial weights distribution on experts P0(dα) it suffers loss satisfying for any α ∈ Θ\nLT (AA(η, P0)) ≤ logβ ∫\nΘ\nβLT (α)P0(dα). (7)\nProof. We proceed by induction in T : for T = 0 the inequality is obvious, and for T > 0 we have:\nLT (AA(η, P0)) ≤ LT−1(AA(η, P0)) + gT (ωT )\n= logβ\n∫\nΘ\nβL θ T−1P0(dθ) + logβ\n∫\nΘ\nβλ(ωT ,ξ θ t )\nβL θ T−1\n∫ Θ βL θ T−1P0(dθ) P0(dθ)\n= logβ\n∫\nΘ\nβL θ TP0(dθ) .\nHere the second equality follows from the inductive assumption, the definition (3) of gT , and (2).\nThe loss of the component-wise algorithm by one component is bounded as in the following theorem.\nTheorem 1. Let the outcome space in the prediction game be [A,B], A,B ∈ R. Assume experts’ predictions at each step are ξt = C + α\n′xt, where α ∈ Rn, C ∈ R is the same for all the experts α, and ‖xt‖∞ ≤ X, ∀t. There exists a prediction algorithm producing γi ∈ R, i = 1, . . . , d such that for any a > 0, every positive integer T , every sequence of input vectors and outcomes of the length T and any α ∈ Rn we have\nT∑\nt=1\n(γt − yt)2 ≤ T∑\nt=1\n(ξt − yt)2 + a‖α‖22 + n(B −A)2\n4 ln\n( TX2\na + 1\n) . (8)\nProof. We need to prove that the game is perfectly mixable (see (4)) and find the optimal parameter η for the algorithm. Implications similar to the ones in the proof of Lemma 2 from Vovk (2001) lead to the inequality η ≤ 2(B−A)2 . Clearly, Lemma 6 holds for our case, so we need only to calculate the difference between the right-hand side of (7)\nlogβ\n∫\nRn\ndα(aη/π)n/2 exp [ −ηα′ ( aI + T∑\nt=1\nxtx ′ t\n) α\n+ η 2α′\n( T∑\nt=1\n(yt − C)xt ) − η T∑\nt=1\n(yt − C)2 ] .\nand the loss of the best expert α′0\n( aI + ∑T t=1 xtx ′ t ) α0−2α′0 (∑T t=1(yt − C)xt ) +\n∑T t=1(yt −C)2. Here α0 is the point where the minimum of the quadratic form\nis attained. Then due to Lemma 4 this difference will be equal to\n1\n2η ln det\n( I + 1\na\nT∑\nt=1\nxtx ′ t\n) ≤ n(B −A) 2\n4 ln\n( TX2\na + 1\n) .\nWe bound the determinant of a symmetric positive definite matrix by the product of its diagonal elements (see Beckenbach and Bellman (1961), Chapter 2, Theorem 7) and use η = 2(B−A)2 .\nInterestingly, the theoretical bound for the regression algorithm depends only on the size of the prediction interval but not on the location of it. It also does not depend on the concentration point of experts. We use the component-wise algorithm to predict each component separately.\nTheorem 2. If ‖xt‖∞ ≤ X, ∀t, then for any a > 0, every positive integer T , every sequence of outcomes of the length T , and any α ∈ Rn(d−1) the loss LT of the component-wise algorithm satisfies\nLT ≤ LT (α) + da‖α‖22 + nd\n4 ln\n( TX2\na + 1\n) . (9)\nProof. We extend the class of experts in (1) in (5). The algorithm predicts each component of the outcome separately. Summing theoretical bounds (8) for d components of the outcome, taking αd = − ∑d−1 i=1 α ′ i, and using the Cauchy inequality ‖∑d−1i=1 αi‖22 ≤ (d− 1) ∑d−1\ni=1 ‖αi‖22 we get the bound. To give probability forecasts we can project prediction points on the prediction simplex using Algorithm 2. The bound will then hold by Lemma 1."
    }, {
      "heading" : "4.2 Linear forecasting",
      "text" : "The theoretical bound for the loss of the Algorithm 1 is\nTheorem 3. If ‖xt‖∞ ≤ X, ∀t, then for any a > 0, every positive integer T , every sequence of outcomes of the length T , and any α ∈ Rn(d−1) mAAR(2a) satisfies\nLT (mAAR(2a)) ≤ LT (α) + 2a‖α‖22 + n(d− 1)\n2 ln\n( TX2\na + 1\n) . (10)\nProof. We apply mAAR with the parameter b = 2a. Recall that C = ∑T\nt=1 xtx ′ t.\nFollowing the line of the proof of Theorem 1 with η = 1 we get the theoretical bound.\nWe can derive a slightly better theoretical bound: in the determinant of A one should subtract the second block raw from the first one and then add the first block column to the second one, then repeat this d− 2 times.\nProposition 2. In the conditions of Theorem 3 mAAR(a) satisfies\nLT (mAAR(a)) ≤ LT (α) + a‖α‖22\n+ n(d− 2)\n2 ln\n( TX2\na + 1\n) + n\n2 ln\n( TX2d\na + 1\n) . (11)\nThe theoretical bound (10) is worse asymptotically by d than the bound (9) of the component-wise algorithm, but it is better in the beginning, especially when the norm of the best expert ‖α‖ is large. This can happen in the important case when the dimension of the input vector is larger than the size of the prediction set: n >> T ."
    }, {
      "heading" : "5 Kernelization",
      "text" : "In some cases the linear model can be considered not rich enough to describe data well, and a more complicated model is needed. We use a popular in computer learning kernel trick, firstly applied to the AAR in Gammerman et al. (2004). We derive an algorithm competing with all sets of functions from an RKHS with d− 1 elements."
    }, {
      "heading" : "5.1 Derivation of the algorithm",
      "text" : "Definition 1. Let us take x1, . . . , xn ∈ X. A kernel function is a nonnegative function K : Rn × Rn → R satisfying ∑ni,j=1 K(xi, xj)ξiξj ≥ 0 for all positive integers n, all x1, . . . , xn ∈ X, and ξ1, . . . , ξn ∈ R.\nAn RKHS contains all linear regressors 〈Φ(·), h〉H defined by means of a feature map (for all the definitions see Schölkopf and Smola, 2002). It can also be defined in a different equivalent way as a functional Hilbert space with continuous evaluation functional ϕ : f ∈ F 7→ f(x) for each x ∈ X. We will use the notation cF (x) for the norm of this functional: cF(x) := supf :‖f‖F≤1 |f(x)| and for the embedding constant cF := supx∈X cF(x) and assume cF < ∞.\nOur algorithm competes with the following experts:\nξ1t = 1/d+ f1(xt)\n. . .\nξd−1t = 1/d+ fd−1(xt) (12)\nξdt = 1− ξ1 − · · · − ξd−1. Here f1, . . . , fd−1 ∈ F are any functions from some RKHS F . We start by rewriting mAAR in the dual form. Denote\nỸi = −2(yi1 − yd1 , . . . , yiT−1 − ydT−1,−1/2), Y i = −2(yi1 − yd1 , . . . , yiT−1 − ydT−1, 0)\nk̃(xT ) = (x ′ 1xT , . . . , x ′ TxT ) ′,\nK̃ = (x′s, xt)s,t is the matrix of scalar products\nfor i = 1, . . . , d− 1, s, t = 1, . . . , T . We show that the predictions of mAAR can be represented in terms of variables defined above. We will need the following matrix property.\nProposition 3. Let B,C be matrices such that the number of rows in B equals to the number of columns in C, and identity matrices I. If aI+CB and aI+BC are nonsingular then\nB(aI + CB)−1 = (aI +BC)−1B. (13)\nProof. This is equivalent to (aI + BC)B = B(aI + CB). That is true because of distributivity of matrix multiplication.\nLet us set A =  aI +   2K̃ · · · K̃ ... . . . ...\nK̃ · · · 2K̃\n   .\nLemma 7. On trial T values ri for i = 1, . . . , d−1 in mAAR can be represented as\nri = ( Ỹ1 · · · Y i · · · Ỹd−1 )\n·A−1 ( k̃(xT ) ′ · · · 2k̃(xT )′ · · · k̃(xT )′ )′ . (14)\nProof. By M = (x1, . . . , xT ) denote a matrix n × T of column input vectors. Let us set\nB =   2M · · · M ... . . . ...\nM · · · 2M\n  , C =   M ′ · · · 0 ... . . . ...\n0 · · · M ′\n  .\nThen hi from the algorithm mAAR equals hi = MY i ∈ Rn. Decompose b′i =( Ỹ1 · · · Y i · · · Ỹd−1 ) C, where only the i-th block uses Y i. The matrix A is equal A = aI +BC. Using proposition 3\nri = −b′iA−1zi = − ( Ỹ1 · · · Y i · · · Ỹd−1 )\n· (aI + CB)−1C ( −x′T · · · −2x′T · · · − x′T )′ .\nNote that K̃ = M ′M and k̃(xT ) = M ′xT , thus (14) holds.\nIf instead of dot product in K̃, k̃(xT ) we can choose a different kernel (clas-\nsical examples of kernels are Gaussian (RBF): K(xi, xj) = e −\n‖xi−xj‖ 2\n2σ2 , Vapnik’s polynomial K(xi, xj) = (xi · xj + 1)d, etc.). To get predictions one can use the same substitution function from Proposition 1. We call this algorithm mKAAR (K for Kernelized)."
    }, {
      "heading" : "5.2 Theoretical bound for the kernelized algorithm",
      "text" : "To derive a theoretical bound for the loss of mKAAR we will use the following matrix determinant identity lemma.\nLemma 8 (Matrix determinant identity). Let B,C are as in Proposition 3, and a is a real number. Then det(aI +BC) = det(aI + CB).\nProof. The proof is by considering a block matrix identity.\nThe main theorem follows from the property of RKHS called Representer theorem (see Schölkopf and Smola, 2002, Theorem 4.2).\nTheorem 4 (Representer theorem). Denote by g : [0,∞) → R a strictly monotonic increasing function. Assume X is an arbitrary set, and F is a Reproducing Kernel Hilbert Space of functions on X with the given kernel K : X2 → R. Assume we also have a positive integer T and an arbitrary loss function c : (X× R2)T → R⋃{∞}. Then each minimizer f ∈ F of\nc ((x1, y1, f(x1)), . . . , (xT , yT , f(xT ))) + g(‖f‖F)\nadmits a representation of the form f(x) = ∑T\ni=1 αiK(xi, x) for any x ∈ X and reals αi, i = 1, . . . , T .\nThe theoretical bound for the loss of mKAAR is proven in the following theorem.\nTheorem 5. Assume X is an arbitrary set of inputs and F is a Reproducing Kernel Hilbert Space of functions on X with the given kernel K : X2 → R. Then for any a > 0, any f1, . . . , fd−1 ∈ F , any positive integer T , and any sequence of inputs and outputs (x1, y1), . . . , (xT , yT )\nLT (mKAAR) ≤ LT (f) + a d−1∑\ni=1\n‖fi‖2F + 1\n2 ln detA (15)\nHere the matrix K̃ is a matrix of kernel values K(xi, xj), i, j = 1, . . . , T .\nProof. The bound follows from Theorem 3 for mAAR and the Representer theorem. Let us first consider the case with scalar product kernel. Denote C = ∑T t=1 xtx ′ t. By Lemma 8 and calculations similar to ones in the proof of Lemma 7 we have the equality of determinants. So we can use any other kernel instead of scalar product to get the term with the determinant. The Representer theorem assures that the minimum of the expression LT (f) + a ∑d−1 i=1 ‖fi‖2F by f -s is reached on a linear regressor.\nWe can represent the bound (15) in another form which is more familiar from the on-line prediction literature:\nCorollary 1. Under assumptions of Theorem 5 and if we know the number of steps T in advance and are given F > 0, the mKAAR reaches the performance\nLT (mKAAR) ≤ LT (f) + 2cFF √ (d− 1)T , (16)\nfor any f1, . . . , fd−1 ∈ F : ∑d−1\ni=1 ‖fi‖2F ≤ F . Proof. Bounding the logarithm of the determinant we have ln detA ≤ (d − 1)T ln ( 1 +\n2c2F a\n) .We can choose the value for a where the minimum is achieved:\na = cF √ (d−1)T\nF ."
    }, {
      "heading" : "6 Experiments",
      "text" : "We run our algorithms on six real world time-series data sets. In the time series we consider there are no signals attached to the outcomes. However we can take vectors consisting of previous observations (we shall take ten of those) and use them as signals. Data set DEC-PKT1 contains an hours worth of all wide-area traffic between Digital Equipment Corporation and the rest of the world. Data set LBL-PKT-41 consists of observations of another hour of traffic between the Lawrence Berkeley Laboratory and the rest of the world. We transformed both the data sets in such a way that each observation is the number of packets in the corresponding network during a fixed time interval of one second. The other four datasets2 (C4,C9,E5,E8) relate to transportation data. Two of them (C9,C11) contain low-frequency monthly traffic measures. Two of them (E5,E8) contain high-frequency day traffic measures. On each of these data sets the following operations were performed: subtraction of the mean value and division by the maximum absolute value. The resulting time series are shown in Figure 1.\nWe used ten previous observations as an input vector for tested algorithms at each prediction step. We are solving the 3-class classification problem: we predict whether the next value in a time series will be more than the previous value plus a precision parameter ǫ, less than that value, or lies in the 2ǫ tube around the previous value. The precision ǫ is chosen to be the median of all the changes in a data set. In order to assess the quality of predictions, we calculate the cumulative square loss at the last two thirds of each time series (test set) and divide it by the number of examples (MSE). Since we are considering the online setting, we could calculate the cumulative loss from the beginning of each time series. However our approach is not sensitive to starting effects, it allows us to choose the ridge parameter a fairly on the training set, and it allows us to compare the performance of our algorithms with batch algorithms, which would be normally used to solve this problem.\nThe square loss on the test set takes into account the quality of an algorithm only at the very end of the prediction process, and does not consider the quality during the process. We introduce another quality measure: at each step in the\n1Data sets can be found http://ita.ee.lbl.gov/html/traces.html. 2Data sets can be found http://www.neural-forecasting-competition.com/index.htm.\ntest set we calculate MSE of an algorithm until this step. After all the steps we average these MSEs (AMSE). Clearly, if one algorithm is better than another on the whole test set (its total MSE is smaller) but was often worse on many parts of the test set (total MSEs of many parts of the set is larger), this measure takes it into account.\nWe compare the performance of our algorithms with the multinomial logistic regression (mLog), because it is a standard classification algorithm which gives probability predictions:\nγimLog = eθ\nix\n∑d i=1 e θix\nfor all the components of the outcome i = 1, . . . , d. In our case d = 3. Here parameters θ1, . . . , θd are estimated from the training set. We apply this algorithm in two regimes: batch regime, where the algorithm learns only on the training set and is tested on the test set (and thus θ is not updated on the test set); and in the online regime, where at each step new parameters θ are found, and only one next outcome is predicted. The second regime is more fair to compare with online algorithms, but the first one is standard and faster. In both regimes logistic regression does not have theoretical guarantees on the square loss.\nWe also compare our algorithms with the simple predictor predicting the average of the ten previous outcomes (and thus it always gives probability predictions).\nWe are not aware of other efficient algorithms for online probability prediction, and thus logistic regression and simple predictor as the only baselines. Component-wise algorithms which could be used for online prediction (e.g., Gradient Descent, Kivinen and Warmuth 1997, Ridge Regression, Hoerl and Kennard 2000), have to use normalization by Algorithm 2. Thus they have to be applied in a different way than they are described in the corresponding papers, and can not be fairly compared with our algorithms.\nThe ridge for our algorithms is chosen to achieve the best MSE on the training set: the first third of each series. The results are shown in Table 1. We highlight the most precise algorithms for different data sets. We also show time needed to make predictions on the whole data set. The algorithms were implemented in Matlab R2007b and run on the laptop with 2Gb RAM and processor Intel Core 2, T7200, 2.00GHz.\nAs we can see from the table, online methods perform better than the batch method. Online logistic regression performs well, but is very slow. Our algorithms perform similar to each other and comparable to the online logistic regression, but are much faster."
    }, {
      "heading" : "7 Discussion",
      "text" : "We consider an important generalization of the online classification problem. We presented new algorithms which give probability predictions in the Brier game. Both algorithms do not involve any numerical integration, and can be easily computed. Both algorithms have theoretical guarantees on their cumulative losses. One of the algorithms is kernelized and a theoretical bound is proven for the kernelized algorithm. We performed experiments with linear algorithms and showed that they perform relatively well. We compared them with the logistic regression: the benchmark algorithm giving probability predictions.\nCompeting with linear experts in the case where possible outcomes lie in a more than 2-dimensional simplex was not widely considered by other researchers, so the comparison of theoretical bounds can not be performed. Kivinen and Warmuth’s work Kivinen and Warmuth (2001) includes the case when the possible outcomes lie in a more than 2-dimensional simplex and their algorithm competes with all logistic regression functions. They use the relative entropy loss function L and get a regret term of the order O( √ LT (α)) which is upper unbounded in the worst case. Their prediction algorithm is not computationally efficient and it is not clear how to extend their results for the case when the predictors lie in an RKHS.\nWe can prove lower bounds for the regret term of the order O(d−1d lnT ) for the case of the linear model (1) using methods similar to ones described in Vovk (2001), and lower bounds for the regret term of the order O( √ T ) for the case of RKHS. Thus we can say that the order of our bounds by time step is optimal. Multiplicative constants may possibly be improved though."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Authors are grateful for useful comments and discussions to Alexey Chernov, Vladimir Vovk, and Alex Gammerman. This work has been supported by EPSRC grant EP/F002998/1 and ASPIDA grant from the Cyprus Research Promotion Foundation."
    } ],
    "references" : [ {
      "title" : "Verification of forecasts expressed in terms of probability",
      "author" : [ "Glenn W. Brier" ],
      "venue" : "Monthly Weather Review,",
      "citeRegEx" : "Brier.,? \\Q1950\\E",
      "shortCiteRegEx" : "Brier.",
      "year" : 1950
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "J. Comput. System Sci.,",
      "citeRegEx" : "Freund and Schapire.,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund and Schapire.",
      "year" : 1997
    }, {
      "title" : "On-line prediction with kernels and the complexity approximation principle",
      "author" : [ "Alexander Gammerman", "Yuri Kalnishkan", "Vladimir Vovk" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Gammerman et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Gammerman et al\\.",
      "year" : 2004
    }, {
      "title" : "Matrix algebra from a statistician’s perspective",
      "author" : [ "David A. Harville" ],
      "venue" : null,
      "citeRegEx" : "Harville.,? \\Q1997\\E",
      "shortCiteRegEx" : "Harville.",
      "year" : 1997
    }, {
      "title" : "Sequential prediction of individual sequences under general loss functions",
      "author" : [ "David Haussler", "Jyrki Kivinen", "Manfred K. Warmuth" ],
      "venue" : "IEEE Trans. Inform. Theory,",
      "citeRegEx" : "Haussler et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Haussler et al\\.",
      "year" : 1998
    }, {
      "title" : "Ridge regression: Biased estimation for nonorthogonal problems",
      "author" : [ "Arthur E. Hoerl", "Robert W. Kennard" ],
      "venue" : null,
      "citeRegEx" : "Hoerl and Kennard.,? \\Q2000\\E",
      "shortCiteRegEx" : "Hoerl and Kennard.",
      "year" : 2000
    }, {
      "title" : "Exponentiated gradient versus gradient descent for linear predictors",
      "author" : [ "Jyrki Kivinen", "Manfred K. Warmuth" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "Kivinen and Warmuth.,? \\Q1997\\E",
      "shortCiteRegEx" : "Kivinen and Warmuth.",
      "year" : 1997
    }, {
      "title" : "Averaging expert predictions",
      "author" : [ "Jyrki Kivinen", "Manfred K. Warmuth" ],
      "venue" : "In Computational learning theory (Nordkirchen,",
      "citeRegEx" : "Kivinen and Warmuth.,? \\Q1999\\E",
      "shortCiteRegEx" : "Kivinen and Warmuth.",
      "year" : 1999
    }, {
      "title" : "Relative loss bounds for multidimensional regression problems",
      "author" : [ "Jyrki Kivinen", "Manfred K. Warmuth" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Kivinen and Warmuth.,? \\Q2001\\E",
      "shortCiteRegEx" : "Kivinen and Warmuth.",
      "year" : 2001
    }, {
      "title" : "A finite algorithm for finding the projection of a point onto the canonical simplex of rn",
      "author" : [ "C Michelot" ],
      "venue" : "J. Optim. Theory Appl.,",
      "citeRegEx" : "Michelot.,? \\Q1986\\E",
      "shortCiteRegEx" : "Michelot.",
      "year" : 1986
    }, {
      "title" : "Learning with kernels: Support Vector Machines, regularization, optimization, and beyond",
      "author" : [ "Bernhard Schölkopf", "Alexander J. Smola" ],
      "venue" : null,
      "citeRegEx" : "Schölkopf and Smola.,? \\Q2002\\E",
      "shortCiteRegEx" : "Schölkopf and Smola.",
      "year" : 2002
    }, {
      "title" : "Aggregating strategies",
      "author" : [ "Vladimir Vovk" ],
      "venue" : "In Proceedings of the Third Annual Workshop on Computational Learning Theory,",
      "citeRegEx" : "Vovk.,? \\Q1990\\E",
      "shortCiteRegEx" : "Vovk.",
      "year" : 1990
    }, {
      "title" : "A game of prediction with expert advice",
      "author" : [ "Vladimir Vovk" ],
      "venue" : "J. Comput. System Sci.,",
      "citeRegEx" : "Vovk.,? \\Q1998\\E",
      "shortCiteRegEx" : "Vovk.",
      "year" : 1998
    }, {
      "title" : "Competitive on-line statistics",
      "author" : [ "Vladimir Vovk" ],
      "venue" : "International Statistical Review,",
      "citeRegEx" : "Vovk.,? \\Q2001\\E",
      "shortCiteRegEx" : "Vovk.",
      "year" : 2001
    }, {
      "title" : "Prediction with expert advice for the Brier game",
      "author" : [ "Vladimir Vovk", "Fedor Zhdanov" ],
      "venue" : "Proceedings of the 25th International Conference on Machine Learning,",
      "citeRegEx" : "Vovk and Zhdanov.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vovk and Zhdanov.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We consider the square loss: mean square error is one of the benchmark measures for classification algorithms (see Brier, 1950). We use Vovk’s Aggregating Algorithm (a generalization of the Bayesian mixture) to mix functions (as in Aggregating Algorithm Regression, AAR: see Vovk, 2001). This method has previously been applied to the case when possible outcomes lie in a segment of the real line, and so the prediction was one-dimensional. We develop two algorithms to solve the problem of multi-dimensional prediction. The first algorithm applies a variant of AAR to predict each coordinate of the outcome separately, and then combines these predictions in a certain way to get probability prediction. The other algorithm is designed to give probability predictions directly; these are first computationally efficient online regression algorithm designed to solve linear and non-linear multi-class classification problems. We derive theoretical bounds on the losses of both algorithms. We come to an unexpected conclusion that the component-wise algorithm is better than the second one asymptotically, but worse in the beginning of the prediction process. Their performance on benchmark data sets is very similar. One component of the prediction of the second algorithm has the meaning of a remainder. In practice this situation is quite common. For example, in a football match either one team wins or the other, and the remainder is a draw (see Vovk and Zhdanov (2008) for online prediction experiments in football).",
      "startOffset" : 115,
      "endOffset" : 1473
    }, {
      "referenceID" : 0,
      "context" : "We are interested in the generalisation of the Brier game from Brier (1950) where the space of outcomes Ω = P(Σ) is the set of all probability measures on a finite set Σ with d elements, Γ := {(γ1, .",
      "startOffset" : 47,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : "In this section we describe how we apply the Aggregating Algorithm (AA) proposed in Vovk (1990) to mix experts and make predictions.",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "It is suggested in Kivinen and Warmuth (1999) that the prediction is simply the weighted average of the experts’ predictions with weights Pt(dα).",
      "startOffset" : 19,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : "Perfectly mixable games and other types of games are analyzed in Vovk (1998). It is also shown there that for countable (and thus finite) number of experts the AA achieves the best possible theoretical guarantees.",
      "startOffset" : 65,
      "endOffset" : 77
    }, {
      "referenceID" : 11,
      "context" : "It is shown in Theorem 1 Vovk and Zhdanov (2008) that the Brier game with finite number of outcomes is perfectly mixable iff η ∈ (0, 1].",
      "startOffset" : 25,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "By Theorem 1 in Vovk and Zhdanov (2008) such prediction exists for any η ∈ (0, 1] (β ∈ [e, 1)).",
      "startOffset" : 16,
      "endOffset" : 40
    }, {
      "referenceID" : 11,
      "context" : "Such a function is proposed in Vovk and Zhdanov (2008). This is an extension of Lemma 4.",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "1 from Haussler et al. (1998). Lemma 3.",
      "startOffset" : 7,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "First we explain why we should not directly use the algorithm and the theoretical bound proposed in Vovk (2001). Vovk’s experts do not allow us to take advantage of the fact that only one outcome is possible to happen at each moment.",
      "startOffset" : 100,
      "endOffset" : 112
    }, {
      "referenceID" : 12,
      "context" : "Instead of the substitution function from Proposition 1 we use the substitution function suggested in Vovk (2001) for the one-dimensional game:",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "We use the projection algorithm suggested in Michelot (1986).",
      "startOffset" : 45,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "It is easy to prove the following statement (Lemma 1 from Vovk (2001)):",
      "startOffset" : 58,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "Implications similar to the ones in the proof of Lemma 2 from Vovk (2001) lead to the inequality η ≤ 2 (B−A) .",
      "startOffset" : 62,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "We use a popular in computer learning kernel trick, firstly applied to the AAR in Gammerman et al. (2004). We derive an algorithm competing with all sets of functions from an RKHS with d− 1 elements.",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "We presented new algorithms which give probability predictions in the Brier game. Both algorithms do not involve any numerical integration, and can be easily computed. Both algorithms have theoretical guarantees on their cumulative losses. One of the algorithms is kernelized and a theoretical bound is proven for the kernelized algorithm. We performed experiments with linear algorithms and showed that they perform relatively well. We compared them with the logistic regression: the benchmark algorithm giving probability predictions. Competing with linear experts in the case where possible outcomes lie in a more than 2-dimensional simplex was not widely considered by other researchers, so the comparison of theoretical bounds can not be performed. Kivinen and Warmuth’s work Kivinen and Warmuth (2001) includes the case when the possible outcomes lie in a more than 2-dimensional simplex and their algorithm competes with all logistic regression functions.",
      "startOffset" : 70,
      "endOffset" : 808
    }, {
      "referenceID" : 0,
      "context" : "We presented new algorithms which give probability predictions in the Brier game. Both algorithms do not involve any numerical integration, and can be easily computed. Both algorithms have theoretical guarantees on their cumulative losses. One of the algorithms is kernelized and a theoretical bound is proven for the kernelized algorithm. We performed experiments with linear algorithms and showed that they perform relatively well. We compared them with the logistic regression: the benchmark algorithm giving probability predictions. Competing with linear experts in the case where possible outcomes lie in a more than 2-dimensional simplex was not widely considered by other researchers, so the comparison of theoretical bounds can not be performed. Kivinen and Warmuth’s work Kivinen and Warmuth (2001) includes the case when the possible outcomes lie in a more than 2-dimensional simplex and their algorithm competes with all logistic regression functions. They use the relative entropy loss function L and get a regret term of the order O( √ LT (α)) which is upper unbounded in the worst case. Their prediction algorithm is not computationally efficient and it is not clear how to extend their results for the case when the predictors lie in an RKHS. We can prove lower bounds for the regret term of the order O( d lnT ) for the case of the linear model (1) using methods similar to ones described in Vovk (2001), and lower bounds for the regret term of the order O( √ T ) for the case of RKHS.",
      "startOffset" : 70,
      "endOffset" : 1420
    } ],
    "year" : 2010,
    "abstractText" : "Multi-class classification is one of the most important tasks in machine learning. In this paper we consider two online multi-class classification problems: classification by a linear model and by a kernelized model. The quality of predictions is measured by the Brier loss function. We suggest two computationally efficient algorithms to work with these problems and prove theoretical guarantees on their losses. We kernelize one of the algorithms and prove theoretical guarantees on its loss. We perform experiments and compare our algorithms with logistic regression.",
    "creator" : "LaTeX with hyperref package"
  }
}