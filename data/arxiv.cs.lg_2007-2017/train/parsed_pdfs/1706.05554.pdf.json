{
  "name" : "1706.05554.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Coresets for Vector Summarization with Applications to Network Graphs",
    "authors" : [ "Dan Feldman", "Sedat Ozer", "Daniela Rus" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We provide a deterministic data summarization algorithm that approximates the mean p̄ = 1\nn ∑ p∈P p of a set P of n vectors in R\nd, by a weighted mean p̃ of a subset of O(1/ε) vectors, i.e., independent of both n and d. We prove that the squared Euclidean distance between p̄ and p̃ is at most ε multiplied by the variance of P . We use this algorithm to maintain an approximated sum of vectors from an unbounded stream, using memory that is independent of d, and logarithmic in the n vectors seen so far. Our main application is to extract and represent in a compact way friend groups and activity summaries of users from underlying data exchanges. For example, in the case of mobile networks, we can use GPS traces to identify meetings; in the case of social networks, we can use information exchange to identify friend groups. Our algorithm provably identifies the Heavy Hitter entries in a proximity (adjacency) matrix. The Heavy Hitters can be used to extract and represent in a compact way friend groups and activity summaries of users from underlying data exchanges. We evaluate the algorithm on several large data sets."
    }, {
      "heading" : "1 Introduction",
      "text" : "The wide-spread use of smart phones, wearable devices, and social media creates a vast space of digital footprints for people, which include location information from GPS traces, phone call history, social media postings, etc. This is an ever-growing wealth of data that can be used to identify social structures and predict activity patterns. We wish to extract the underlying social network of a group of mobile users given data available about them (e.g. GPS traces, phone call history, news articles, etc.) in order to identify and predict their various activities such as meetings, friend groups, gathering places, collective activity patterns, etc. There are several key challenges to achieve these capabilities. First, the data is huge so we need efficient methods for processing and representing the data. Second, the data is multi-modal heterogeneous. This presents challenges in data processing and representation, but also opportunities to extract correlations that may not be visible in a single data source. Third, the data is often noisy.\nar X\niv :1\n70 6.\n05 55\n4v 1\n[ cs\n.L G\n] 1\n7 Ju\nWe propose an approach based on coresets to extract underlying connectivity information while performing data summarization for a given a large data set. We focus our intuition examples and evaluations on social networks because of their intuitive nature and access to data sets, although the method is general and applies to networks of information in general. Our approach works on streaming datasets to represent the data in a compact (sparse) way. Our coreset algorithm gets a stream of vectors and approximates their sum using small memory. Essentially, a coreset C is a significantly smaller portion (a scaled subset) of the original and large set D of vectors. Given D and the algorithm A, where running algorithm A on D is intractable due to lack of memory, the task-specific coreset algorithm efficiently reduces the data set D to a coreset C so that running the algorithm A on C requires a low amount of memory and the result is provable approximately the same as running the algorithm on D. Coreset captures all the important vectors in D for a given algorithm A. The challenges are computing C fast and proving that C is the right scaled subset, i.e., running the algorithm on C gives approximately the same result as running the algorithm on D.\nMore specifically, the goal of this paper is to suggest a way to maintain a sparse representation of an n × d matrix, by maintaining a sparse approximation of each of its rows. For example, in a proximity matrix associated with a social network, instead of storing the average proximity to each of n users, we would like to store only the N n largest entries in each row (known as “Heavy Hitters”), which correspond to the people seen by the user most often. Given an unbounded stream of movements, it is hard to tell which are the people the user met most, without maintaining a counter to each person. For example, consider the special case N = 1. We are given a stream of pairs (i, val) where i ∈ {1, · · · , n} is an index of a counter, and val is a real number that represents a score for this counter. Our goal is to identify which counter has the maximum average score till now, and approximate that score. While that is easy to do by maintaining the sum of n scores, our goal is to maintain only a constant set of numbers (memory words). In general, we wish to have a provable approximation to each of the n accumulated scores (row in a matrix), using, say, only O(1) memory. Hence, maintaining an n × n matrix would take O(n) instead of O(n2) memory. For millions of users or rows, this means 1 Gigabytes of memory can be stored in RAM for real-time updates, compared to millions of Gigabytes. Such data reduction will make it practical to keep hundreds of matrices for different types of similarities (sms, phone calls, locations), different types of users, and different time frames (average proximity in each day, year, etc). This paper contributes the following: (1) A compact representation for streaming proximity data for a group of many users; (2) A coreset construction algorithm for maintaining the social network with error guarantees; (3) An evaluation of the algorithm on several data sets.\nTheoretical Contribution: These results are based on an algorithm that computes an ε-coresetC of size |C| = O(1/ε) for the mean of a given set, and every error parameter ε ∈ (0, 1) as defined in Section 2. Unlike previous results, this algorithm is deterministic, maintains a weighted subset of the input vectors (which keeps their sparsity), can be applied on a set of vectors whose both cardinality n and dimension d is arbitrarily large or unbounded. Unlike existing randomized sketching algorithms for summing item frequencies (1-sparse binary vectors), this coreset can be used to approximate the sum of arbitrary real vectors, including negative entries (for decreasing counters), dense\nvectors (for fast updates), fractions (weighted counter) and with error that is based on the variance of the vectors (sum of squared distances to their mean) which might be arbitrarily smaller than existing errors: sum/max of squared/non-squared distances to the origin (`2/`1/`∞)."
    }, {
      "heading" : "1.1 Solution Overview",
      "text" : "We implemented a system that demonstrates the use and performance of our suggested algorithm. The system constructs a sparse social graph from the GPS locations of real moving smart-phone users and maintains the graph in a streaming fashion as follows.\nInput stream: The input to our system is an unbounded stream of (real-time) GPS points, where each point is represented in the vector format of (time, userID, longitude, latitude). We maintain an approximation of the average proximity of each user to all the other n − 1 users seen so far, by using space (memory) that is only logarithmic in n. The overall memory would then be near-linear in n, in contrast to the quadratic O(n2) memory that is needed to store the exact average proximity vector for each of the n users. We maintain a dynamic array of the n user IDs seen so far and assume, without loss of generality, that the user IDs are distinct and increasing integers from 1 to n. Otherwise we use a hash table from user IDs to such integers. In general, the system is designed to handle any type of streaming records in the format (streamID, v) where v is a d-dimensional vector of reals, to support the other applications. Here, the goal is to maintain a sparse approximation to the sum of the vectors v that were assigned to each stream ID.\nProximity matrix: We also maintain an (non-sparse) array pos of length n that stores the current location of each of the n users seen so far. That array forms the current n2 pairs of proximities prox(u, v) between every pair of users and their current locations u and v. These pairs correspond to what we call the proximity matrix at time t, which is a symmetric adjacency n×nmatrix of a social graph, where the edge weight of a pair of users (an entry in this matrix) is their proximity at the current time t. We are interested in maintaining the sum of these proximity matrices over time, which, after division by n, will give the average proximity between every two users over time. This is the average proximity matrix. Since the average proximity matrix and each proximity matrix at a given time require O(n2) memory, we cannot keep them all in memory. Our goal is to maintain a sparse approximation version of each of row in the average proximity matrix which will use only O(log n) memory. Hence, the required memory by the system will be O(n log n) instead of O(n2).\nAverage proximity vector: The average proximity vector is a row vector of length n in the average proximity matrix for each of the n users. We maintain only a sparse approximation vector for each user, thus, only the non-zeroes entries are kept in memory as a set of pairs of type (index, value). We define the proximity between the current location vectors u, v ∈ R3 of two users as: prox(u, v) := e−dist(u,v).\nCoreset for a streamed average: Whenever a new record (time, userID, longitude, latitude) is inserted to the stream, we update the entries for that user as his/her current position array pos is changed. Next, we compute that n proximities from that user to each of the other users. Note that in our case, the proximity from a user to himself is always e0 = 1. Each proximity proxj where 1 ≤ j ≤ n is converted to a sparse vector\n(0, · · · , 0, proxj , 0, · · · , 0) with one non-zero entry. This vector should be added to the average proximity vector of user j, to update the jth entry. Since maintaining the exact average proximity vector will take O(n) memory for each of the n users, we instead add this sparse vector to an object (“coreset”) that maintains an approximation to the average proximity vector of user j.\nOur problem is then reduced to the problem of maintaining a sparse average of a stream of sparse vectors in Rn, using O(log n) memory. Maintaining such a stream for each of the n users seen so far, will take overall memory of O(n log n) as desired, compared to the exact solution that requires O(n2) memory. We generalize and formalize this problem, as well as the approximation error and its practical meaning, in Section 2."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "As mobile applications become location-aware, the representation and analysis of locationbased data sets become more important and useful in various domains Wasserman (1980); Hogan (2008); Carrington et al. (2005); Dinh et al. (2010); Nguyen et al. (2011); Lancichinetti & Fortunato (2009). An interesting application is to extract the relationships between mobile users (in other words, their social network) from their location data Liao (2006); Zheng (2011); Dinh et al. (2013). Therefore, in this paper, we use coresets to represent and approximate (streaming) GPS-based location data for the extraction of the social graphs. The problem in social network extraction from GPS data is closely related to the frequency moment problem. Frequency approximation is considered the main motivation for streaming in the seminal work of Alon et al. (1996a), known as the “AMS paper”, which introduced the streaming model.\nCoresets have been used in many related applications. The most relevant are coresets for k-means; see Barger & Feldman (2015) and reference therein. Our result is related to coreset for 1-mean that approximates the mean of a set of points. A coreset as defined in this paper can be easily obtained by uniform sampling ofO(log d log(1/δ))/ε2 or O(1/(δε)) points from the input, where δ ∈ (0, 1) is the probability of failure. However, for sufficiently large stream the probability of failure during the stream approaches 1. In addition, we assume that d may be arbitrarily large. In Barger & Feldman (2015) such a coreset was suggested but its size is exponential in 1/ε. We aim for a deterministic construction of size independent of d and linear in 1/ε.\nA special case of such coreset for the case that the mean is the origin (zero) was suggested in Feldman et al. (2016), based on Frank-Wolfe, with applications to coresets for PCA/SVD. In this paper we show that the generalization to any center is not trivial and requires a non-trivial embedding of the input points to a higher dimensional space.\nEach of the above-mentioned prior techniques has at least one of the following disadvantages: (1) It holds only for positive entries. Our algorithm supports any real vector. Negative values may be used for deletion or decreasing of counters, and fraction may represent weights. (2) It is randomized, and thus will always fail on unbounded stream. Our algorithm is deterministic. (3) It supports only s = 1 non-zero entries. Our algorithm supports arbitrary number of non-zeroes entries with only linear dependency of the required memory on s. (4) It projects the input vectors on a random subspace, which diminishes the sparsity of these vectors. Our algorithm maintains a\nsmall weighted subset of the vectors. This subset keeps the sparsity of the input and thus saves memory, but also allows us to learn the representative indices of points (time stamps in our systems) that are most important in this sense.\nThe most important difference and the main contribution of our paper is the error guarantee. Our error function in (1) is similar to ‖p̄− p̂‖` ≤ ε ‖p̄‖q for ` = 2 on the left hand side. Nevertheless, the error on the right hand side might be significantly smaller: instead of taking the sum of squared distances to the origin (norm of the average vector), we use the variance, which is the sum of squared distances to the mean. The later one is always smaller, since the mean of a set of vectors minimized their sum of squared distances."
    }, {
      "heading" : "2 Problem Statement",
      "text" : "The input is an unbounded stream vectors p1, p2, · · · in Rd. Here, we assume that each vector has one non-zero entry. In the social network example, d is the number of users and each vector is in the form (0, · · · , 0, proxj , 0, · · · , 0), where proxj is the proximity between the selected user and user j. Note that for each user, we independently maintain an input stream of proximities to each of the other users, and the approximation of its average. In addition, we get another input: the error parameter N , which is related to the memory used by the system. Roughly, the required memory for an input stream will be O(N log n) and the approximation error will be ε := 1/N . That is, our algorithms are efficient when N is a constant that is much smaller than the number of vectors that were read from stream, 2 < N n. For example, to get roughly 1 percents of error, we have ε = 0.01, and the memory is about 100n, compared to n2 for the exact average proximity. The output p̂ is an N -sparse approximation to the average vector in the stream over the n vectors p1, · · · , pn seen so far. That is, an approximation to the centroid, or center of mass, p̄ = 1n ∑ i pi. Here and in what follows, the sum is over i ∈ {1, · · · , n}. Note that even if s = 1, the average p̄ might have n N non-zero entries, as in the case where pi = (0, · · · , 0, 1, 0, · · · , 0) is the ith row of the identity matrix. The sparse approximation p̂ of the average vector p̄ has the following properties: (1) The vector p̂ has at most N non-zero entries. (2) The vector p̂ approximates the vector of average proximities p̄ in the sense that the (Euclidean) distance between the two vectors is var/N where var is the variance of all the vectors seen so far in the stream. More formally, ‖p̄− p̂‖2 ≤ εvar, where ε = 1/N is the error, p̄ = 1 n ∑n i=1 pi is the average vector in Rd for the n input vectors, and the variance is the sum of squared distances to the average vector.\nDistributed and parallel computation. Our system supports distributed and streaming input simultaneously in a “embarrassingly parallel” fashion. E.g., this allows multiple users to send their streaming smart-phone data to the cloud simultaneously in real-time. There is no assumption regarding the order of the data in user ID. Using M nodes, each node will have to use only 1/M fraction of the memory to (log n)O(1)/M that is used by one node for the same problem, and the average insertion time for a new point will be reduced by a factor of M to (log n)O(1)/M .\nParallel coreset computation of unbounded streams of distributed data was suggested in Feldman & Tassa (2015), as an extension to the classic merge-and-reduce\nframework in Bentley & Saxe (1980); Har-Peled (2006). We apply this framework on our off-line algorithm to handle streaming and distributed data (see Section 3).\nGeneralizations. Above we assume that each vector in the stream has a single non-zero entry. To generalize that, we now assume that each vector has at most s nonzeroes entries and that these vectors are weighted (e.g. by their importance). Under these assumptions, we wish to approximate the weighted mean p̄ = ∑n i=1 uipi where u = (u1, · · · , un) is a weight vector that represents distribution, i.e., ui ≥ 0 for every i ∈ [n]. Then, our problem is formalized as follows.\nProblem 1 Consider an unbounded stream of real vectors p1, p2, · · · , where each vector is represented only by its non-zero entries, i.e., pairs (entryIndex, value) ∈ {1, 2, 3, . . .} × R. Maintain a subset of N n input vectors, and a corresponding vector of positive reals (weights) w1, w2, · · · , wN , where the sum p̂ := ∑N i=1 wipi\napproximates the sum p̄ = ∑n i=1 pi of the n vectors seen so far in the stream up to\na provably small error that depends on its variance var(p) := ∑n i=1 ‖pi − p̄‖ 2 2. Formally, for an error parameter ε that may depend on N ,\n‖p̄− p̂‖ ≤ εvar(p). (1) We provide a solution for this problem mainly by proving Theorem 1 for off-line data, and turn it into algorithms for streaming and distributed data as explained in Section 3."
    }, {
      "heading" : "3 New Coreset Algorithms",
      "text" : "In this section we first describe an algorithm for approximating the sum of n streaming vectors using one pass. The algorithm calls its off-line version as a sub-procedure. We then explain how to run the algorithm on distributed and unbounded streaming data using M machines or parallel threads. The size of the weighted subset of vectors that are maintained in memory, and the insertion time per new vector in the stream are logarithmic on the number n of vectors in the stream. Using M machines, the memory and running time per machine is reduced by a factor of M ."
    }, {
      "heading" : "3.1 Streaming and Distributed Data",
      "text" : "Overview. The input to Algorithm 1 is a stream provided as a pointer to a device that sends the next input vectors in a stream that consists of n vectors, upon request, For example, a hard drive, a communication socket, or a web-service that collects information from the users. The second parameter ε defines the approximation error. The required memory grows linearly with respect to 1/ε.\nAlgorithm 1 maintains a binary tree whose leaves are the input vectors, and each inner node is a coreset, as in the left or right hand side of Fig. 1. However, at most one coreset in each level of the tree is actually stored in memory. In Line 1, we initialize the current height of this tree. Using log(n)/ε vectors in memory our algorithm returns an O(ε)-coreset, but to get exactly ε-coreset, we increase it in Line 2 by a constant factor α that can be find in the proof of Theorem 1. In Lines 3-14, we read batches (sets) of O(log(n)/ε) vectors from the stream and compress them. The last batch may be smaller. Line 4 defines the next batch P . Unlike the coresets in the nodes of the tree, we assume that the input vectors are unweighted, so, Line 5 defines a weight 1 for each input vector. Line 6 reduce the set P by half to the weighted coreset (S,w) using Algorithm 2 (the off-line coreset construction.) Theorem 1 guarantees that such a compression is possible.\nIn Lines 8–12 we add the new coreset (S,w) to the lowest level ` of the binary tree, if it is not assigned to a coreset already, i.e., S` is not empty. Otherwise, we merge the new coreset (S,w) with the level’s coreset S`, mark the level as empty from coresets (S` ← ∅), and continue to the next higher level of the tree until we reach a level ` that is not assigned to a coreset , i.e., S` = ∅. Line 13 handle the case where we reach to the root of the tree, and a new (top) level is created. In this case, only the new root of the tree contains a coreset.\nWhen the streaming is over, in Lines 15–16 we collect the active coreset in each of the O(log n) tree levels (if it has one) and return the union of these coresets.\nParallel computation on distributed data. In this model each machine has its own stream of data, and computes its coreset independently and in parallel to the other machines, as described above. Whenever we wish to get the coreset for the union of streaming vectors, we collect the current coreset from each machine on a single machine or a server. Since each machine sends only a coreset, the communication is also logarithmic in n. For M ≥ 2 machines and a single input stream, we send every ith point in the stream to the ith machine, for every i between 1 and M . For example, if M = 2, the odd vectors will be sent to the first machine, and every second (even) vector will be sent to the second machine. Then, each machine will compute a coreset for its own unbounded stream; See Fig 1."
    }, {
      "heading" : "3.2 Off-line data reduction",
      "text" : "Algorithm 2 is called by Algorithm 1 and its variant in the last sub-section for compressing a given small set P of vectors in memory by computing its coreset. As in Line 10 of Algorithm 1, the input itself might be a coreset of another set, thus we assume that the input has a corresponding weight vector u. Otherwise, we assign a weight 1 for each input vector, i.e., u = (1, · · · , 1). The output of Algorithm 1 is an\nε-coreset (S,w) of size |S| = O(1/ε) for a given ε ∈ (0, 1). While the number n of input vectors can be of an arbitrary size, Algorithm 2 always passes an input set of n = 2|S| points to get output that is smaller by half.\nOverview of Algorithm 2: In Line 1 the desired mean Eu that we wish to approximate is computed. Lines 2–4 are used for adding an extra dimension for each input vector later. In Lines 4-6 we normalize the augmented input, by constructing a set q1, · · · , qn of unit vectors with a new set of weights s1, · · · , sn whose mean is∑ i siqi = (0, · · · , 0, x/v). We then translate this mean to the origin by defining the new set H in Line 7. The main coreset construction is computed in Lines 9–13 on the normalized set H whose mean is the origin and its vectors are on the unit ball. This is a greedy, gradient descent method, based on the Frank-Wolfe framework Feldman et al. (2016). In iteration i = 1, we begin with an arbitrary input point c1 in H . Since c1 is a unit vector, its distance from the mean of H (origin) is 1. In Line 11– 12 we compute the farthest point h2 from c1, and approximates the mean using only c1 and the new point h2. This is done, by projecting the origin on the line segment through c1 and h2, to get an improved approximation c2 that is closer to the origin. We continue to add input points in this manner, where in the ith iteration another input point is selected for the coreset, and the new center is a convex combination of i points. In the proof of Theorem 1 it is shown that the distance to the origin in the ith iteration is α/i, which yields an ε-approximation after β = O(α/ε) iterations.\nThe resulting center cβ is spanned by β input vectors. In Line 11 we compute their new weights based on their distances from cβ . Lines 14–16 are used to convert the weights of the vectors in the normalized H back to the original input set of vectors. The algorithm then returns the small subset of β input vectors with their weights vector w."
    }, {
      "heading" : "4 Correctness",
      "text" : "In this section we show that Algorithm 1 computes correctly the coreset for the average vector in Theorem 1.\nLet Dn denote all the possible distributions over n items, i.e., D is the unit simplex Dn = {(u1, · · · , un) ∈ Rn | ui ≥ 0 and ∑n i=1 ui = 1} .\nGiven a set P = {p1, · · · , pn} of vectors, the mean of P is 1n ∑ i=1 pi. This is also the expectation of a random vector that is chosen uniformly at random from P . The sum of variances of this vector is the sum of squared distances to the mean. More generally, for a distribution u ∈ S over the vectors of P , the (weighted) mean is∑n i=1 uipi, which is the expected value of a vector chosen randomly using the distribution u. The variance varu is the sum of weighted squared distances to the mean. By letting N = 1/ε in the following theorem, we conclude that there is always a sparse distribution w of at most 1/ε non-zeroes entries, that yields an approximation to its weighted mean, up to an ε-fraction of the variance. Theorem 1 (Coreset for the average vector) Let u ∈ Dn be a distribution over a set P = {p1, · · · , pn} of n vectors in Rd, and let N ≥ 1. Let (S,w) denote the output of a call to CORESET(P, u, 1/N); see Algorithm 1. Then w ∈ Dn consists O(N) non-zero entries, such that the sum p̄ = ∑ i=1 uipi deviates from the sum p̂ =\nAlgorithm 1: STREAMING-CORESET(stream, ε) Input: An input stream of n vectors in Rd.\nan error parameter ε ∈ (0, 1) Output: An ε-coreset (S,w) for the set of n vectors;\nsee Theorem 1. 1 Set max← 0 2 Set α to be a sufficiently large constant that can be derived from the proof of\nTheorem 1. 3 while stream is not empty do 4 Set P ← next d2α ln(n)/εe input vectors in stream 5 Set u← (1, · · · , 1) where u has |P | entries. 6 Set (S,w)← CORESET(P, u, ε/(α ln(n)) 7 Set `← 1 8 while S` 6= ∅ and ` ≤ max do 9 Set S` ← S ∪ S`\n10 Set (S,w)← CORESET(S`, w`, ε) 11 Set S` ← ∅ 12 Set `← `+ 1 13 if ` > max then Set max← ` 14 Set (S`, w`)← (S,w) 15 Set S ← ⋃max i=1 Si and w ← (w1, w2, · · · , wmax) 16 return (S,w)\n∑n i=1 wipi by at most a (1/N)-fraction of the variance varu = ∑n i=1 ui ‖pi − p̄‖ 2 2, i.e., ‖p̄− p̂‖22 ≤ varu N .\nBy the O(·) notation above, it suffices to prove that there is a constant α > 0 such that N ≥ α and\n‖Eu − Ew‖22 ≤ αvaru N , (2)\nwhere Eu = p̄ and Ew = p̂. The proof is constructive and thus immediately implies Algorithm 1. Indeed, let\nx = ∑ j uj ‖pj − Eu‖ , and v = ∑ j uj ‖(pj − Eu, x)‖.\nHere and in what follows, ‖·‖ = ‖·‖2 and all the sums are over [n] = {1, · · · , n}. For every i ∈ [n] let\nqi = (pi−Eu,x) ‖(pi−Eu,x)‖ , and si = ui‖(pi−Eu,x)‖ v .\nHence, ∑ i siqi = 1 v ∑ i ui(pi − Eu, x)\n= 1\nv (∑ i uipi − ∑ m umEu, ∑ k ukx )\n= 1\nv ∑ i uipi − ∑ j ujpj , x  = (0, · · · , 0, x v ) .\n(3)\nAlgorithm 2: CORESET(P, u, ε) Input: A set P of vectors in Rd,\na positive weight vector u = (u1, · · · , un), an error parameter ε ∈ (0, 1)\nOutput: An ε-coreset (S,w) for (P, u) 1 Set Eu ← ∑n i=1 uipi\n2 Set x← ∑n j=1 uj ‖pj − Eu‖\n3 Set v ← ∑n j=1 uj ‖(pj − Eu, x)‖ 4 for i← 1 to n do 5 Set qi ← (pi − Eu, x) ‖(pi − Eu, x)‖ 6 Set si ← ui ‖(pi − Eu, x)‖ v 7 Set H ← {qi − (0, · · · , 0, x/v) | i ∈ [n]} 8 Set α← a sufficiently large constant that can be derived from the proof of\nTheorem 1. 9 Set c1 ← an arbitrary vector in H\n10 for i← 1 to β := dα/εe do 11 hi+1 ← farthest point from ci in H 12 ci+1 ← the projection of the origin on the segment ci, hi+1 13 Compute w′ = (w′1, · · · , w′β) ∈ Sβ such that cβ = ∑β i=1 w ′ ihi+1 14 for i← 1 to β do 15 w′′i← vw ′ i\n(pi − Eu, x)\n16 wi ← w′′i∑β j=1 w ′′ j\n17 w ← (w1, · · · , wβ) 18 S ← {v1, · · · , wβ} 19 return (S,w)\nSince (s1, · · · , sn) ∈ Dn we have that the point p = ∑ i siqi is in the convex hull of Q = {q1, · · · , qn}. By applying the Frank-Wolfe algorithm as described in Clarkson (2005) for the function f(s) = ‖As‖, where each row of A corresponds to a vector in Q, we conclude that there is w′ = (w′1, · · · , w′n) ∈ Dn that has at most N non-zero entries such that\n‖ ∑ i(si − w′i)qi‖ 2 = ∥∥∥∑i siqi −∑j w′jqj∥∥∥2 = ‖p− q‖2 ≤ 1N . (4)\nFor every i ∈ [n], define\nw′′i = vw′i\n‖(pi − Eu, x)‖ and wi = w′′i∑ j w ′′ j .\nWe thus have:\n‖Eu − Ew‖2 = ∥∥∥∑i uipi −∑j wjpj∥∥∥2 = ∥∥∑i(ui − wi)pi∥∥2 (5)\n= ∥∥∑ i(ui − wi)(pi − Eu, x) ∥∥2 (6)\n= v2 ∥∥∥∑i (ui‖(pi−Eu,x)‖v − wi‖(pi−Eu,x)‖v ) qi∥∥∥2 (7)\n= v2 ∥∥∥∑i (si − (w′′i /∑j w′′j )·‖(pi−Eu,x)‖v ) qi∥∥∥2 (8)\n= v2 ∥∥∥∑i (si − w′i∑j w′′j ) qi∥∥∥2 , (9)\nwhere (5) is by the definitions ofEu andEw, (6) follows since ∑ i ui = ∑ i wi = 1\nand thus ∑ i uiy = ∑ j ujy for every vector y, (8) follows by the definitions of wi and qi, and (9) by the definition of w′i. Next, we bound (9). Since for every two reals y, z,\n2yz ≤ y2 + z2 (10)\nby letting y = ‖a‖ and z = ‖b‖ for a, b ∈ Rd,\n‖a+ b‖2 ≤ ‖a‖2 + ‖b‖2 + 2 ‖a‖ ‖b‖\n≤ ‖a‖2 + ‖b‖2 + (‖a‖2 + ‖b‖2) = 2 ‖a‖2 + 2 ‖b‖2 . (11)\nBy substituting a = ∑ i(si − w′i)qi and b = ∑ i(w ′ i − w′i∑ j w ′′ j\n)qi in (11), we obtain∥∥∥∥∥∑ i ( si − w′i∑ j w ′′ j ) qi ∥∥∥∥∥ 2 ≤ 2 ∥∥∥∥∥∑ i (si − w′i)qi ∥∥∥∥∥ 2 (12)\n+ 2 ∥∥∥∥∥∑ i ( w′i − w′i∑ j w ′′ j ) qi ∥∥∥∥∥ 2 . (13)\nBound on (13): Observe that∥∥∥∥∥∑ i ( w′i − w′i∑ j w ′′ j ) qi ∥∥∥∥∥ 2 = ∥∥∥∥∥∑ i w′i ( 1− 1∑ j w ′′ j )∥∥∥∥∥ 2\n= ( 1− 1∑\nj w ′′ j\n)2 · ∥∥∥∥∥∑ i w′iqi ∥∥∥∥∥ 2 . (14)\nLet τ = v√ Nx . By the triangle inequality\nv = ∑ j uj ‖(pj − Eu, x)‖ ≤ ∑ j uj ‖pj − Eu‖+ x = 2x. (15)\nBy choosing c > 16 in (2) we have N ≥ 16, so\nτ ≤ 2√ N ≤ 12 . (16)\nSubstituting a = − ∑ i siqi and b = ∑ i(si−w′j)qj in (11) bounds the right expression of (14) by ∥∥∥∥∥∑ i w′iqi ∥∥∥∥∥ 2 ≤ 2 ∥∥∥∥∥∑ i siqi ∥∥∥∥∥ 2 + 2 ∥∥∥∥∥∑ i (si − w′j)qj ∥∥∥∥∥ 2\n≤ 2x 2\nv2 +\n2 N =\n2(1 + τ2)\nτ2N ,\n(17)\nwhere the last inequality follows from (3) and (4). For bounding the left expression of (14), note that\n(1− ∑ j w′′j ) 2 =\n( 1−\n∑ j w′j · v ‖(pj − Eu, x)‖\n)2 (18)\n≤ ∥∥∥∥∥(0, · · · , 0, 1)−∑ j w′j · ( v x (pj − Eu), v) ‖(pj − Eu, x)‖ ∥∥∥∥∥ 2\n= v2\nx2 ∥∥∥∥∥(0, · · · , 0, xv )−∑ j w′j · (pj − Eu, x) ‖(pj − Eu, x)‖ ∥∥∥∥∥ 2\n= v2\nx2 ∥∥∥∥∥∑ i (si − w′i)qi ∥∥∥∥∥ 2 ≤ v 2 Nx2 = τ2,\nwhere (18) follows since ‖b‖2 ≤ ‖(a, b)‖2 for every pair a, b of vectors, and the last inequality is by (3) and (4). Hence, ∑ j w ′′ j ≥ 1− τ , and(\n1− 1∑ j w ′′ j\n)2 = ( 1− ∑ i w ′′ i∑\nj w ′′ j\n)2 ≤ τ 2\n(1−τ)2 .\nCombining (14) and (17) bounds (13) by\n2 ∥∥∥∥∥∑ i ( w′i − w′i∑ j w ′′ j ) qi ∥∥∥∥∥ 2 = 2 ( 1− 1∑ j w ′′ j )2 · ∥∥∥∥∥∑ i w′iqi ∥∥∥∥∥ 2\n≤ 2τ 2 (1− τ)2 · 2(1 + τ2) τ2N = 4(1 + τ2) N(1− τ)2 .\nBound on (12): Plugging the last inequality, (4) and (12) in (9) yields ‖Eu − Ew‖2 = v2 ∥∥∥∥∥∑ i ( si − w′i∑ j w ′′ j ) qi ∥∥∥∥∥ 2\n≤ 2v2 (∥∥∥∥∥∑\ni\n(si − w′i)qi ∥∥∥∥∥ 2 + ∥∥∥∥∥∑ i ( w′i − w′i∑ j w ′′ j ) qi ∥∥∥∥∥ 2 )\n≤ 2v 2\nN\n( 1 + 2(1 + τ2)\n(1− τ)2\n) ≤ αv 2\nN ,\n(19)\nfor a sufficiently large constant α, e.g. α = 3, where in the last inequality we used (16). Since v ≤ 2x by (15) we have\nv ≤ 2x = 2 ∑ j uj ‖pj − Eu‖\n= ∑ j 2 · √√ varu √ uj · √ uj ‖pj − Eu‖√√ varu\n≤ ∑ j ( √ varuuj + uj ‖pj − Eu‖2√ varu ) = √ varu + 1√\nvaru ∑ j uj ‖pj − Eu‖2 = 2 √ varu,\nwhere in the second inequality we used (10). Plugging this in (19) and replacing N by 4αN = O(N) in the proof above, yields the desired bound\n‖Eu − Ew‖2 ≤ αv 2 N ≤ 4α·varu N ."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "We implemented the coreset algorithms in 1 and 2. We also implemented a brute force method for determining the social network that considers the entire data. We used this method to derive the ground truth for the social network for small scale data. Our system’s overview is given in Figure 3 and explained in Section 1.1. The coreset algorithm computes the heavy hitters by approximating the sum of the columns of the proximity matrix as explained in Section 1.1. In this section, we have used different data sets from three different sources: In our first experiment, we compared our coreset algorithm’s error with other sketch algorithms Charikar et al. (2004); Cormode & Muthukrishnan (2005a). The second dataset is the New York City Cab dataset1 and the third data set is from Stanford2 and includes six different graph-based data sets. In all the figures shown in this section, the x axis shows the coreset size (N) and the y axis represents the normalized error value (Error ∗mean(var(pi)2)/mean(norm(pi))), where Error = ‖pi − p̄‖2. In our experiments, we ran N iterations and wrote down the empirical error .\nComparison to sketch algorithms: Since the algorithms in Charikar et al. (2004); Cormode & Muthukrishnan (2005a) focused on selecting the entries at scalar level (i.e., individual entries from a vector), in this experiment, we generated a small scale synthetic data (standard Gaussian distribution) and compared the error made by our\n1https://publish.illinois.edu/dbwork/open-data/ 2https://snap.stanford.edu/data/\ncoreset implementation to four other sketch implementations. These sketch algorithms are: Count Sketch, Count Min, Count Median, BJKST and F2-Sketch (see Fig. 2). For the sketch algorithms, we used the code available at3. We plot the results in Fig. 2 where our Coresets algorithm showed better approximation than all other well known sketch techniques for all N values.\nApplication on NYC data: Here we applied our algorithm on the NYC data. The data contains the location information of 13249 taxi cabs with 14776616 entries. The goal here is showing how the error on Coreset approximation would change on real data with respect to N (we expect that the error would reduce with respect to N as the theory suggests). This can be seen in Fig. 4, where x axis is the coreset size (N) and y axis is the normalized error.\nApplication on Stanford Data Sets: Here we apply our algorithm on six different data sets from Stanford: Amazon, Youtube, DBLP, LiveJournal, Orkut and Wikitalk data sets. We run the Coreset algorithm to approximate the total number of connectivities each node has. We computed the error for each of the seven different N values from [100, 200, 300, 400, 500, 600, 900] for each data set. We used the first 50000 entries from the Orkut, Live Journal, Youtube and Wiki data sets and the first 5000 entries from Amazon and DBLP data set. The results are shown in Figure 5. In the figures, y axis represents the normalized error. The results demonstrate the utility of our proposed method for summarization.\n3https://github.com/jiecchen/StreamLib/"
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper we proposed a new coreset algorithm for streaming data sets with applications to summarizing large networks to identify the ”heavy hitters”. The algorithm takes a stream of vectors as input and maintains their sum using small memory. Our presented algorithm shows better performance at even lower values of non-zero entries (i.e., at higher sparsity rates) when compared to the other existing sketch techniques. We demonstrated that our algorithm can catch the heavy hitters efficiently in social networks from the GPS-based location data and in several graph data sets from the Stanford data repository."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Support for this research has been provided in part by Ping An Insurance and NSFSaTCBSF CNC 1526815. We are grateful for this support."
    } ],
    "references" : [ {
      "title" : "The space complexity of approximating the frequency moments",
      "author" : [ "Alon", "Noga", "Matias", "Yossi", "Szegedy", "Mario" ],
      "venue" : "In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Alon et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 1996
    }, {
      "title" : "The space complexity of approximating the frequency moments",
      "author" : [ "Alon", "Noga", "Matias", "Yossi", "Szegedy", "Mario" ],
      "venue" : "In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Alon et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 1996
    }, {
      "title" : "Counting distinct elements in a data stream",
      "author" : [ "Bar-Yossef", "Ziv", "TS Jayram", "Kumar", "Ravi", "D Sivakumar", "Trevisan", "Luca" ],
      "venue" : "In International Workshop on Randomization and Approximation Techniques in Computer Science,",
      "citeRegEx" : "Bar.Yossef et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bar.Yossef et al\\.",
      "year" : 2002
    }, {
      "title" : "k-means for streaming and distributed big sparse data",
      "author" : [ "Barger", "Artem", "Feldman", "Dan" ],
      "venue" : "SDM’16 and arXiv preprint arXiv:1511.08990,",
      "citeRegEx" : "Barger et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Barger et al\\.",
      "year" : 2015
    }, {
      "title" : "Decomposable searching problems i. static-todynamic transformation",
      "author" : [ "Bentley", "Jon Louis", "Saxe", "James B" ],
      "venue" : "Journal of Algorithms,",
      "citeRegEx" : "Bentley et al\\.,? \\Q1980\\E",
      "shortCiteRegEx" : "Bentley et al\\.",
      "year" : 1980
    }, {
      "title" : "Models and methods in social network analysis, volume 28",
      "author" : [ "Carrington", "Peter J", "Scott", "John", "Wasserman", "Stanley" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "Carrington et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Carrington et al\\.",
      "year" : 2005
    }, {
      "title" : "Finding frequent items in data streams",
      "author" : [ "Charikar", "Moses", "Chen", "Kevin", "Farach-Colton", "Martin" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Charikar et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Charikar et al\\.",
      "year" : 2004
    }, {
      "title" : "Subgradient and sampling algorithms for l1-regression",
      "author" : [ "K.L. Clarkson" ],
      "venue" : "In Proc. 16th Annu. ACM-SIAM Symp. on Discrete algorithms (SODA), pp",
      "citeRegEx" : "Clarkson,? \\Q2005\\E",
      "shortCiteRegEx" : "Clarkson",
      "year" : 2005
    }, {
      "title" : "An improved data stream summary: the count-min sketch and its applications",
      "author" : [ "Cormode", "Graham", "S. Muthukrishnan" ],
      "venue" : "Journal of Algorithms,",
      "citeRegEx" : "Cormode et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Cormode et al\\.",
      "year" : 2005
    }, {
      "title" : "An improved data stream summary: the count-min sketch and its applications",
      "author" : [ "Cormode", "Graham", "S. Muthukrishnan" ],
      "venue" : "Journal of Algorithms,",
      "citeRegEx" : "Cormode et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Cormode et al\\.",
      "year" : 2005
    }, {
      "title" : "On approximation of new optimization methods for assessing network vulnerability",
      "author" : [ "Dinh", "Thang N", "Xuan", "Ying", "Thai", "My T", "EK Park", "Znati", "Taieb" ],
      "venue" : "In INFOCOM,",
      "citeRegEx" : "Dinh et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Dinh et al\\.",
      "year" : 2010
    }, {
      "title" : "An adaptive approximation algorithm for community detection in dynamic scale-free networks",
      "author" : [ "Dinh", "Thang N", "Nguyen", "Nam P", "Thai", "My T" ],
      "venue" : "In INFOCOM,",
      "citeRegEx" : "Dinh et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dinh et al\\.",
      "year" : 2013
    }, {
      "title" : "More constraints, smaller coresets: constrained matrix approximation of sparse big data",
      "author" : [ "Feldman", "Dan", "Tassa", "Tamir" ],
      "venue" : "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’15),",
      "citeRegEx" : "Feldman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2015
    }, {
      "title" : "Dimensionality reduction of massive sparse datasets using coresets",
      "author" : [ "Feldman", "Dan", "Volkov", "Mikhail", "Rus", "Daniela" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Feldman et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2016
    }, {
      "title" : "Coresets for discrete integration and clustering",
      "author" : [ "Har-Peled", "Sariel" ],
      "venue" : "In FSTTCS 2006: Foundations of Software Technology and Theoretical Computer Science,",
      "citeRegEx" : "Har.Peled and Sariel.,? \\Q2006\\E",
      "shortCiteRegEx" : "Har.Peled and Sariel.",
      "year" : 2006
    }, {
      "title" : "Analyzing social networks. The Sage handbook of online research",
      "author" : [ "Hogan", "Bernie" ],
      "venue" : null,
      "citeRegEx" : "Hogan and Bernie.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hogan and Bernie.",
      "year" : 2008
    }, {
      "title" : "Community detection algorithms: a comparative analysis",
      "author" : [ "Lancichinetti", "Andrea", "Fortunato", "Santo" ],
      "venue" : "Physical review E,",
      "citeRegEx" : "Lancichinetti et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lancichinetti et al\\.",
      "year" : 2009
    }, {
      "title" : "Location-based activity recognition",
      "author" : [ "Liao", "Lin" ],
      "venue" : "PhD thesis, University of Washington,",
      "citeRegEx" : "Liao and Lin.,? \\Q2006\\E",
      "shortCiteRegEx" : "Liao and Lin.",
      "year" : 2006
    }, {
      "title" : "Adaptive algorithms for detecting community structure in dynamic social networks",
      "author" : [ "Nguyen", "Nam P", "Dinh", "Thang N", "Xuan", "Ying", "Thai", "My T" ],
      "venue" : "In INFOCOM,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2011
    }, {
      "title" : "Analyzing social networks as stochastic processes",
      "author" : [ "Wasserman", "Stanley" ],
      "venue" : "Journal of the American statistical association,",
      "citeRegEx" : "Wasserman and Stanley.,? \\Q1980\\E",
      "shortCiteRegEx" : "Wasserman and Stanley.",
      "year" : 1980
    }, {
      "title" : "Location-based social networks: Users",
      "author" : [ "Zheng", "Yu" ],
      "venue" : "In Computing with spatial trajectories,",
      "citeRegEx" : "Zheng and Yu.,? \\Q2011\\E",
      "shortCiteRegEx" : "Zheng and Yu.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "As mobile applications become location-aware, the representation and analysis of locationbased data sets become more important and useful in various domains Wasserman (1980); Hogan (2008); Carrington et al. (2005); Dinh et al.",
      "startOffset" : 189,
      "endOffset" : 214
    }, {
      "referenceID" : 3,
      "context" : "As mobile applications become location-aware, the representation and analysis of locationbased data sets become more important and useful in various domains Wasserman (1980); Hogan (2008); Carrington et al. (2005); Dinh et al. (2010); Nguyen et al.",
      "startOffset" : 189,
      "endOffset" : 234
    }, {
      "referenceID" : 3,
      "context" : "As mobile applications become location-aware, the representation and analysis of locationbased data sets become more important and useful in various domains Wasserman (1980); Hogan (2008); Carrington et al. (2005); Dinh et al. (2010); Nguyen et al. (2011); Lancichinetti & Fortunato (2009).",
      "startOffset" : 189,
      "endOffset" : 256
    }, {
      "referenceID" : 3,
      "context" : "As mobile applications become location-aware, the representation and analysis of locationbased data sets become more important and useful in various domains Wasserman (1980); Hogan (2008); Carrington et al. (2005); Dinh et al. (2010); Nguyen et al. (2011); Lancichinetti & Fortunato (2009). An interesting application is to extract the relationships between mobile users (in other words, their social network) from their location data Liao (2006); Zheng (2011); Dinh et al.",
      "startOffset" : 189,
      "endOffset" : 290
    }, {
      "referenceID" : 3,
      "context" : "As mobile applications become location-aware, the representation and analysis of locationbased data sets become more important and useful in various domains Wasserman (1980); Hogan (2008); Carrington et al. (2005); Dinh et al. (2010); Nguyen et al. (2011); Lancichinetti & Fortunato (2009). An interesting application is to extract the relationships between mobile users (in other words, their social network) from their location data Liao (2006); Zheng (2011); Dinh et al.",
      "startOffset" : 189,
      "endOffset" : 447
    }, {
      "referenceID" : 3,
      "context" : "As mobile applications become location-aware, the representation and analysis of locationbased data sets become more important and useful in various domains Wasserman (1980); Hogan (2008); Carrington et al. (2005); Dinh et al. (2010); Nguyen et al. (2011); Lancichinetti & Fortunato (2009). An interesting application is to extract the relationships between mobile users (in other words, their social network) from their location data Liao (2006); Zheng (2011); Dinh et al.",
      "startOffset" : 189,
      "endOffset" : 461
    }, {
      "referenceID" : 3,
      "context" : "As mobile applications become location-aware, the representation and analysis of locationbased data sets become more important and useful in various domains Wasserman (1980); Hogan (2008); Carrington et al. (2005); Dinh et al. (2010); Nguyen et al. (2011); Lancichinetti & Fortunato (2009). An interesting application is to extract the relationships between mobile users (in other words, their social network) from their location data Liao (2006); Zheng (2011); Dinh et al. (2013). Therefore, in this paper, we use coresets to represent and approximate (streaming) GPS-based location data for the extraction of the social graphs.",
      "startOffset" : 189,
      "endOffset" : 481
    }, {
      "referenceID" : 0,
      "context" : "Frequency approximation is considered the main motivation for streaming in the seminal work of Alon et al. (1996a), known as the “AMS paper”, which introduced the streaming model.",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "Frequency approximation is considered the main motivation for streaming in the seminal work of Alon et al. (1996a), known as the “AMS paper”, which introduced the streaming model. Coresets have been used in many related applications. The most relevant are coresets for k-means; see Barger & Feldman (2015) and reference therein.",
      "startOffset" : 95,
      "endOffset" : 306
    }, {
      "referenceID" : 0,
      "context" : "Frequency approximation is considered the main motivation for streaming in the seminal work of Alon et al. (1996a), known as the “AMS paper”, which introduced the streaming model. Coresets have been used in many related applications. The most relevant are coresets for k-means; see Barger & Feldman (2015) and reference therein. Our result is related to coreset for 1-mean that approximates the mean of a set of points. A coreset as defined in this paper can be easily obtained by uniform sampling ofO(log d log(1/δ))/ε or O(1/(δε)) points from the input, where δ ∈ (0, 1) is the probability of failure. However, for sufficiently large stream the probability of failure during the stream approaches 1. In addition, we assume that d may be arbitrarily large. In Barger & Feldman (2015) such a coreset was suggested but its size is exponential in 1/ε.",
      "startOffset" : 95,
      "endOffset" : 785
    }, {
      "referenceID" : 0,
      "context" : "Frequency approximation is considered the main motivation for streaming in the seminal work of Alon et al. (1996a), known as the “AMS paper”, which introduced the streaming model. Coresets have been used in many related applications. The most relevant are coresets for k-means; see Barger & Feldman (2015) and reference therein. Our result is related to coreset for 1-mean that approximates the mean of a set of points. A coreset as defined in this paper can be easily obtained by uniform sampling ofO(log d log(1/δ))/ε or O(1/(δε)) points from the input, where δ ∈ (0, 1) is the probability of failure. However, for sufficiently large stream the probability of failure during the stream approaches 1. In addition, we assume that d may be arbitrarily large. In Barger & Feldman (2015) such a coreset was suggested but its size is exponential in 1/ε. We aim for a deterministic construction of size independent of d and linear in 1/ε. A special case of such coreset for the case that the mean is the origin (zero) was suggested in Feldman et al. (2016), based on Frank-Wolfe, with applications to coresets for PCA/SVD.",
      "startOffset" : 95,
      "endOffset" : 1052
    }, {
      "referenceID" : 12,
      "context" : "This is a greedy, gradient descent method, based on the Frank-Wolfe framework Feldman et al. (2016). In iteration i = 1, we begin with an arbitrary input point c1 in H .",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "By applying the Frank-Wolfe algorithm as described in Clarkson (2005) for the function f(s) = ‖As‖, where each row of A corresponds to a vector in Q, we conclude that there is w′ = (w′ 1, · · · , w′ n) ∈ D that has at most N non-zero entries such that",
      "startOffset" : 54,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : "Figure 2: Given a set of vectors from a standard Gaussian distribution, the graph shows the `2 error (y-axis) between their sum and their approximated sum using only N samples (the x-axis) based on Count Sketch Charikar et al. (2004), Count Min Cormode & Muthukrishnan (2005a), Count Median Cormode & Muthukrishnan (2005b), BJKST Bar-Yossef et al.",
      "startOffset" : 211,
      "endOffset" : 234
    }, {
      "referenceID" : 3,
      "context" : "Figure 2: Given a set of vectors from a standard Gaussian distribution, the graph shows the `2 error (y-axis) between their sum and their approximated sum using only N samples (the x-axis) based on Count Sketch Charikar et al. (2004), Count Min Cormode & Muthukrishnan (2005a), Count Median Cormode & Muthukrishnan (2005b), BJKST Bar-Yossef et al.",
      "startOffset" : 211,
      "endOffset" : 277
    }, {
      "referenceID" : 3,
      "context" : "Figure 2: Given a set of vectors from a standard Gaussian distribution, the graph shows the `2 error (y-axis) between their sum and their approximated sum using only N samples (the x-axis) based on Count Sketch Charikar et al. (2004), Count Min Cormode & Muthukrishnan (2005a), Count Median Cormode & Muthukrishnan (2005b), BJKST Bar-Yossef et al.",
      "startOffset" : 211,
      "endOffset" : 323
    }, {
      "referenceID" : 0,
      "context" : "(2004), Count Min Cormode & Muthukrishnan (2005a), Count Median Cormode & Muthukrishnan (2005b), BJKST Bar-Yossef et al. (2002), F2-Sketch Alon et al.",
      "startOffset" : 103,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "(2002), F2-Sketch Alon et al. (1996b), and our coreset.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "In this section, we have used different data sets from three different sources: In our first experiment, we compared our coreset algorithm’s error with other sketch algorithms Charikar et al. (2004); Cormode & Muthukrishnan (2005a).",
      "startOffset" : 176,
      "endOffset" : 199
    }, {
      "referenceID" : 6,
      "context" : "In this section, we have used different data sets from three different sources: In our first experiment, we compared our coreset algorithm’s error with other sketch algorithms Charikar et al. (2004); Cormode & Muthukrishnan (2005a). The second dataset is the New York City Cab dataset1 and the third data set is from Stanford2 and includes six different graph-based data sets.",
      "startOffset" : 176,
      "endOffset" : 232
    }, {
      "referenceID" : 6,
      "context" : "In this section, we have used different data sets from three different sources: In our first experiment, we compared our coreset algorithm’s error with other sketch algorithms Charikar et al. (2004); Cormode & Muthukrishnan (2005a). The second dataset is the New York City Cab dataset1 and the third data set is from Stanford2 and includes six different graph-based data sets. In all the figures shown in this section, the x axis shows the coreset size (N) and the y axis represents the normalized error value (Error ∗mean(var(pi))/mean(norm(pi))), where Error = ‖pi − p̄‖2. In our experiments, we ran N iterations and wrote down the empirical error . Comparison to sketch algorithms: Since the algorithms in Charikar et al. (2004); Cormode & Muthukrishnan (2005a) focused on selecting the entries at scalar level (i.",
      "startOffset" : 176,
      "endOffset" : 732
    }, {
      "referenceID" : 6,
      "context" : "In this section, we have used different data sets from three different sources: In our first experiment, we compared our coreset algorithm’s error with other sketch algorithms Charikar et al. (2004); Cormode & Muthukrishnan (2005a). The second dataset is the New York City Cab dataset1 and the third data set is from Stanford2 and includes six different graph-based data sets. In all the figures shown in this section, the x axis shows the coreset size (N) and the y axis represents the normalized error value (Error ∗mean(var(pi))/mean(norm(pi))), where Error = ‖pi − p̄‖2. In our experiments, we ran N iterations and wrote down the empirical error . Comparison to sketch algorithms: Since the algorithms in Charikar et al. (2004); Cormode & Muthukrishnan (2005a) focused on selecting the entries at scalar level (i.",
      "startOffset" : 176,
      "endOffset" : 765
    } ],
    "year" : 2017,
    "abstractText" : "We provide a deterministic data summarization algorithm that approximates the mean p̄ = 1 n ∑ p∈P p of a set P of n vectors in R , by a weighted mean p̃ of a subset of O(1/ε) vectors, i.e., independent of both n and d. We prove that the squared Euclidean distance between p̄ and p̃ is at most ε multiplied by the variance of P . We use this algorithm to maintain an approximated sum of vectors from an unbounded stream, using memory that is independent of d, and logarithmic in the n vectors seen so far. Our main application is to extract and represent in a compact way friend groups and activity summaries of users from underlying data exchanges. For example, in the case of mobile networks, we can use GPS traces to identify meetings; in the case of social networks, we can use information exchange to identify friend groups. Our algorithm provably identifies the Heavy Hitter entries in a proximity (adjacency) matrix. The Heavy Hitters can be used to extract and represent in a compact way friend groups and activity summaries of users from underlying data exchanges. We evaluate the algorithm on several large data sets.",
    "creator" : "LaTeX with hyperref package"
  }
}