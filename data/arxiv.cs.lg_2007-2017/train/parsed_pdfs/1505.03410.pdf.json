{
  "name" : "1505.03410.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Mind the duality gap: safer rules for the Lasso",
    "authors" : [ "Olivier Fercoq", "Alexandre Gramfort", "Joseph Salmon" ],
    "emails" : [ "OLIVIER.FERCOQ@TELECOM-PARISTECH.FR", "ALEXANDRE.GRAMFORT@TELECOM-PARISTECH.FR", "JOSEPH.SALMON@TELECOM-PARISTECH.FR" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Since the mid 1990’s, high dimensional statistics has attracted considerable attention, especially in the context of linear regression with more explanatory variables than observations: the so-called p ą n case. In such a context, the least squares with `1 regularization, referred to as the Lasso (Tibshirani, 1996) in statistics, or Basis Pursuit (Chen et al., 1998) in signal processing, has been one of the most popular tools. It enjoys theoretical guarantees (Bickel et al., 2009), as well as practical benefits: it provides sparse solutions and fast convex solvers are available. This has made the Lasso a popular method in modern data-science toolkits. Among successful fields where it has been applied,\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\none can mention dictionary learning (Mairal, 2010), biostatistics (Haury et al., 2012) and medical imaging (Lustig et al., 2007; Gramfort et al., 2012) to name a few.\nMany algorithms exist to approximate Lasso solutions, but it is still a burning issue to accelerate solvers in high dimensions. Indeed, although some other variable selection and prediction methods exist (Fan & Lv, 2008), the best performing methods usually rely on the Lasso. For stability selection methods (Meinshausen & Bühlmann, 2010; Bach, 2008; Varoquaux et al., 2012), hundreds of Lasso problems need to be solved. For non-convex approaches such as SCAD (Fan & Li, 2001) or MCP (Zhang, 2010), solving the Lasso is often a required preliminary step (Zou, 2006; Zhang & Zhang, 2012; Candès et al., 2008).\nAmong possible algorithmic candidates for solving the Lasso, one can mention homotopy methods (Osborne et al., 2000), LARS (Efron et al., 2004), and approximate homotopy (Mairal & Yu, 2012), that provide solutions for the full Lasso path, i.e., for all possible choices of tuning parameter λ. More recently, particularly for p ą n, coordinate descent approaches (Friedman et al., 2007) have proved to be among the best methods to tackle large scale problems.\nFollowing the seminal work by El Ghaoui et al. (2012), screening techniques have emerged as a way to exploit the known sparsity of the solution by discarding features prior to starting a Lasso solver. Such techniques are coined safe rules when they screen out coefficients guaranteed to be zero in the targeted optimal solution. Zeroing those coefficients allows to focus more precisely on the non-zero ones (likely to represent signal) and helps reducing the computational burden. We refer to (Xiang et al., 2014) for a concise introduction on safe rules. Other alternatives have tried to screen the Lasso relaxing the “safety”. Potentially, some variables are wrongly disregarded and post-processing is needed to recover them. This is for instance the strategy adopted for the strong rules (Tibshirani et al., 2012).\nThe original basic safe rules operate as follows: one\nar X\niv :1\n50 5.\n03 41\n0v 1\n[ st\nat .M\nL ]\n1 3\nM ay\nchooses a fixed tuning parameter λ, and before launching any solver, tests whether a coordinate can be zeroed or not (equivalently if the corresponding variable can be disregarded or not). We will refer to such safe rules as static safe rules. Note that the test is performed according to a safe region, i.e., a region containing a dual optimal solution of the Lasso problem. In the static case, the screening is performed only once, prior any optimization iteration. Two directions have emerged to improve on static strategies.\n• The first direction is oriented towards the resolution of the Lasso for a large number of tuning parameters. Indeed, practitioners commonly compute the Lasso over a grid of parameters and select the best one in a data-driven manner, e.g., by cross-validation. As two consecutive λ1s in the grid lead to similar solutions, knowing the first solution may help improve screening for the second one. We call sequential safe rules such strategies, also referred to as recursive safe rules in (El Ghaoui et al., 2012). This road has been pursued in (Wang et al., 2013; Xu & Ramadge, 2013; Xiang et al., 2014), and can be thought of as a “warm start” of the screening (in addition to the warm start of the solution itself). When performing sequential safe rules, one should keep in mind that generally, only an approximation of the previous dual solution is computed. Though, the safety of the rule is guaranteed only if one uses the exact solution. Neglecting this issue, leads to “unsafe” rules: relevant variables might be wrongly disregarded.\n• The second direction aims at improving the screening by interlacing it throughout the optimization algorithm itself: although screening might be useless at the beginning of the algorithm, it might become (more) efficient as the algorithm proceeds towards the optimal solution. We call these strategies dynamic safe rules following (Bonnefoy et al., 2014a;b).\nBased on convex optimization arguments, we leverage duality gap computations to propose a simple strategy unifying both sequential and dynamic safe rules. We coined GAP SAFE rules such safe rules.\nThe main contributions of this paper are 1) the introduction of new safe rules which demonstrate a clear practical improvement compared to prior strategies 2) the definition of a theoretical framework for comparing safe rules by looking at the convergence of their associated safe regions.\nIn Section 2, we present the framework and the basic concepts which guarantee the soundness of static and dynamic screening rules. Then, in Section 3, we introduce the new concept of converging safe rules. Such rules identify in\nfinite time the active variables of the optimal solution (or equivalently the inactive variables), and the tests become more and more precise as the optimization algorithm proceeds. We also show that our new GAP SAFE rules, built on dual gap computations, are converging safe rules since their associated safe regions have a diameter converging to zero. We also explain how our GAP SAFE tests are sequential by nature. Application of our GAP SAFE rules with a coordinate descent solver for the Lasso problem is proposed in Section 4. Using standard data-sets, we report the time improvement compared to prior safe rules."
    }, {
      "heading" : "1.1. Model and notation",
      "text" : "We denote by rds the set t1, . . . , du for any integer d P N. Our observation vector is y P Rn and the design matrix X “ rx1, ¨ ¨ ¨ , xps P Rnˆp has p explanatory variables (or features) column-wise. We aim at approximating y as a linear combination of few variables xj’s, hence expressing y as Xβ where β P Rp is a sparse vector. The standard Euclidean norm is written } ¨ }, the `1 norm } ¨ }1, the `8 norm } ¨ }8, and the matrix transposition of a matrix Q is denoted by QJ. We denote ptq` “ maxp0, tq.\nFor such a task, the Lasso is often considered (see Bühlmann & van de Geer (2011) for an introduction). For a tuning parameter λ ą 0, controlling the trade-off between data fidelity and sparsity of the solutions, a Lasso estimator β̂pλq is any solution of the primal optimization problem\nβ̂pλq P arg min βPRp\n1 2 ‖Xβ ´ y‖22 ` λ ‖β‖1 looooooooooooomooooooooooooon\n“Pλpβq\n. (1)\nDenoting ∆X “ θ P Rn : ∣∣xJj θ∣∣ ď 1,@j P rps( the dual feasible set, a dual formulation of the Lasso reads (see for instance Kim et al. (2007) or Xiang et al. (2014)):\nθ̂pλq “ arg max θP∆XĂRn\n1 2 ‖y‖22 ´\nλ2\n2 ∥∥∥θ ´ y λ ∥∥∥2 2\nlooooooooooooomooooooooooooon\n“Dλpθq\n. (2)\nWe can reinterpret Eq. (2) as θ̂pλq “ Π∆X py{λq, where ΠC refers to the projection onto a closed convex set C. In particular, this ensures that the dual solution θ̂pλq is always unique, contrarily to the primal β̂pλq."
    }, {
      "heading" : "1.2. A KKT detour",
      "text" : "For the Lasso problem, a primal solution β̂pλq P Rp and the dual solution θ̂pλq P Rn are linked through the relation:\ny “ Xβ̂pλq ` λθ̂pλq . (3)\nThe Karush-Khun-Tucker (KKT) conditions state:\n@j P rps, xJj θ̂pλq P # tsignpβ̂pλqj qu if β̂ pλq j ‰ 0,\nr´1, 1s if β̂pλqj “ 0. (4)\nSee for instance (Xiang et al., 2014) for more details. The KKT conditions lead to the fact that for λ ě λmax “ }XJy}8, 0 P Rp is a primal solution. It can be considered as the mother of all safe screening rules. So from now on, we assume that λ ď λmax for all the considered λ’s."
    }, {
      "heading" : "2. Safe rules",
      "text" : "Safe rules exploit the KKT condition (4). This equation implies that β̂pλqj “ 0 as soon as |xJj θ̂pλq| ă 1. The main challenge is that the dual optimal solution is unknown. Hence, a safe rule aims at constructing a set C Ă Rn containing θ̂pλq. We call such a set C a safe region. Safe regions are all the more helpful that for many j’s, µCpxjq :“ supθPC |xJj θ| ă 1, hence for many j’s, β̂pλqj “ 0.\nPractical benefits are obtained if one can construct a region C for which it is easy to compute its support function, denoted by σC and defined for any x P Rn by:\nσCpxq “ max θPC\nxJθ . (5)\nCast differently, for any safe region C, any j P rps, and any primal optimal solution β̂pλq, the following holds true:"
    }, {
      "heading" : "If µCpxjq “ maxpσCpxjq, σCp´xjqq ă 1 then β̂pλqj “ 0.",
      "text" : "(6) We call safe test or safe rule, a test associated to C and screening out explanatory variables thanks to Eq. (6). Remark 1. Reminding that the support function of a set is the same as the support function of its closed convex hull (Hiriart-Urruty & Lemaréchal, 1993)[Proposition V.2.2.1], we restrict our search to closed convex safe regions.\nBased on a safe region C one can partition the explanatory variables into a safe active set AλpCq and a safe zero set ZλpCq where:\nApλqpCq “ tj P rps : µCpxjq ě 1u, (7) ZpλqpCq “ tj P rps : µCpxjq ă 1u. (8)\nNote that for nested safe regions C1 Ă C2 then ApλqpC1q Ă ApλqpC2q. Consequently, a natural goal is to find safe regions as small as possible: narrowing safe regions can only increase the number of screened out variables.\nRemark 2. If C “ tθ̂pλqu, the safe active set is the equicorrelation set ApλqpCq “ Eλ :“ tj P rps : |xJj θ̂pλq| “ 1u (in most cases (Tibshirani, 2013) it is exactly the active set of β̂pλq). If the Lasso has a unique solution, its support is exactly the equicorrelation set. If it is not unique, the equicorrelation set contains all the solutions’ supports and there exists a Lasso solution whose support is exactly this set (Tibshirani, 2013)[Lemma 12]. The other extreme case is when C “ ∆X , and ApλqpCq “ rps. Here, no variable is screened out: ZpλqpCq “ H and the screening is useless.\nWe now consider common safe regions whose support functions are easy to obtain in closed form. For simplicity we focus only on balls and domes, though more complicated regions could be investigated (Xiang et al., 2014)."
    }, {
      "heading" : "2.1. Sphere tests",
      "text" : "Following previous work on safe rules, we call sphere tests, tests relying on balls as safe regions. For a sphere test, one chooses a ball containing θ̂pλq with center c and radius r, i.e., C “ Bpc, rq. Due to their simplicity, safe spheres have been the most commonly investigated safe regions (see for instance Table 1 for a brief review). The corresponding test is defined as follows:\nIf µBpc,rqpxjq “ |xJj c| ` r}xj} ă 1, then β̂ pλq j “ 0. (9)\nNote that for a fixed center, the smaller the radius, the better the safe screening strategy.\nExample 1. The first introduced sphere test (El Ghaoui et al., 2012) consists in using the center c “ y{λ and radius r “ |1{λ ´ 1{λmax|}y}. Given that θ̂pλq “ Π∆X py{λq, this is a safe region since y{λmax P ∆X and }y{λmax ´ Π∆X py{λq} ď }y}|1{λ ´ 1{λmax|. However, one can check that this static safe rule is useless as soon as\nλ\nλmax ď min jPrps\n˜\n1` |xJj y|{p}xj}}y}q 1` λmax{p}xj}}y}q\n¸\n. (10)"
    }, {
      "heading" : "2.2. Dome tests",
      "text" : "Other popular safe regions are domes, the intersection between a ball and a half-space. This kind of safe region has\nbeen considered for instance in (El Ghaoui et al., 2012; Xiang & Ramadge, 2012; Xiang et al., 2014; Bonnefoy et al., 2014b). We denote Dpc, r, α, wq the dome with ball center c, ball radius r, oriented hyperplane with unit normal vector w and parameter α such that c ´ αrw is the projection of c on the hyperplane (see Figure 1 for an illustration in the interesting case α ą 0). Remark 3. The dome is non-trivial whenever α P r´1, 1s. When α “ 0, one gets simply a hemisphere.\nFor the dome test one needs to compute the support function for C “ Dpc, r, α, wq. Interestingly, as for balls, it can be obtained in a closed form. Due to its length though, the formula is deferred to the Appendix (see also (Xiang et al., 2014)[Lemma 3] for more details)."
    }, {
      "heading" : "2.3. Dynamic safe rules",
      "text" : "For approximating a solution β̂pλq of the Lasso primal problem Pλ, iterative algorithms are commonly used. We denote βk P Rp the current estimate after k iterations of any iterative algorithm (see Section 4 for a specific study on coordinate descent). Dynamic safe rules aim at discovering safe regions that become narrower as k increases. To do so, one first needs dual feasible points: θk P ∆X . Following El Ghaoui et al. (2012) (see also (Bonnefoy et al., 2014a)), this can be achieved by a simple transformation of the current residuals ρk “ y ´Xβk, defining θk as\n#\nθk“αkρk, αk“min ” max ´\nyJρk λ‖ρk‖2 , ´1‖XJρk‖8\n¯\n, 1‖XJρk‖8\nı\n.\n(11) Such dual feasible θk is proportional to ρk, and is the closest point (for the norm } ¨ }) to y{λ in ∆X with such a property, i.e., θk “ Π∆XXSpanpρkqpy{λq. A reason for choosing this dual point is that the dual optimal solution θ̂pλq is the projection of y{λ on the dual feasible set ∆X , and the optimal θ̂pλq is proportional to y ´Xβ̂pλq, cf. Equation (3). Remark 4. Note that if limkÑ`8 βk “ β̂pλq (convergence\nof the primal) then with the previous display and (3), we can show that limkÑ`8 θk “ θ̂pλq. Moreover, the convergence of the primal is unaltered by safe rules: screening out unnecessary coefficients of βk, can only decrease the distance between βk and β̂pλq.\nExample 2. Note that any dual feasible point θ P ∆X immediately provides a ball that contains θ̂pλq since∥∥∥θ̂pλq ´ y\nλ ∥∥∥ “ min θ1P∆X ∥∥∥θ1 ´ y λ ∥∥∥ ď ∥∥∥θ ´ y λ ∥∥∥ :“ qRλpθq. (12)\nThe ball B ` y{λ, qRλpθkq ˘\ncorresponds to the simplest safe region introduced in (Bonnefoy et al., 2014a;b) (cf. Figure 2 for more insights). When the algorithm proceeds, one expects that θk gets closer to θ̂pλq, so }θk ´ y{λ} should get closer to }θ̂pλq ´ y{λ}. Similarly to Example 1, this dynamic rule becomes useless once λ is too small. More precisely, this occurs as soon as\nλ\nλmax ď min jPrps\n˜\n1` |xJj y|{p}xj}}y}q λmax}θ̂pλq}{}y} ` λmax{p}xj}}y}q\n¸\n.\n(13) Noticing that }θ̂pλq} ď }y{λ} (since Π∆X is a contraction and 0 P ∆X ) and proceeding as for (10), one can show that this dynamic safe rule is inefficient when:\nλ\nλmax ď min jPrps\n˜\n|xJj y| λmax\n¸\n. (14)\nThis is a critical threshold, yet the screening might stop even at a larger λ thanks to Eq. (13). In practice the bound in Eq. (13) cannot be evaluated a priori due to the term }θ̂pλq}). Note also that the bound in Eq. (14) is close to the one in Eq. (10), explaining the similar behavior observed in our experiments (see Figure 3 for instance)."
    }, {
      "heading" : "3. New contributions on safe rules",
      "text" : ""
    }, {
      "heading" : "3.1. Support discovery in finite time",
      "text" : "Let us first introduce the notions of converging safe regions and converging safe tests.\nDefinition 1. Let pCkqkPN be a sequence of closed convex sets in Rn containing θ̂pλq. It is a converging sequence of safe regions for the Lasso with parameter λ if the diameters of the sets converge to zero. The associated safe screening rules are referred to as converging safe tests.\nNot only converging safe regions are crucial to speed up computation, but they are also helpful to reach exact active set identification in a finite number of steps. More precisely, we prove that one recovers the equicorrelation set of the Lasso (cf. Remark 2) in finite time with any converging strategy: after a finite number of steps, the equicorrelation set Eλ is exactly identified. Such a property is\nsometimes referred to as finite identification of the support (Liang et al., 2014). This is summarized in the following.\nTheorem 1. Let pCkqkPN be a sequence of converging safe regions. The estimated support provided by Ck, ApλqpCkq “ tj P rps : maxθPCk |θJxj | ě 1u, satisfies limkÑ8A\npλqpCkq “ Eλ, and there exists k0 P N such that @k ě k0 one gets ApλqpCkq “ Eλ.\nProof. The main idea of the proof is to use that limkÑ8 Ck “ tθ̂pλqu, limkÑ8 µCkpxq “ µtθ̂pλqupxq “ |xJθ̂pλq| and that the set ApλqpCkq is discrete. Details are delayed to the Appendix.\nRemark 5. A more general result is proved for a specific algorithm (Forward-Backward) in Liang et al. (2014). Interestingly, our scheme is independent of the algorithm considered (e.g., Forward-Backward (Beck & Teboulle, 2009), Primal Dual (Chambolle & Pock, 2011), coordinatedescent (Tseng, 2001; Friedman et al., 2007)) and relies only on the convergence of a sequence of safe regions."
    }, {
      "heading" : "3.2. GAP SAFE regions: leveraging the duality gap",
      "text" : "In this section, we provide new dynamic safe rules built on converging safe regions.\nTheorem 2. Let us take any pβ, θq P Rp ˆ ∆X . Denote pRλpβq :“ 1λ ` ‖y‖2´‖Xβ ´ y‖2´2λ ‖β‖1 ˘1{2 ` ,\nqRλpθq :“ ‖θ ´ y{λ‖ , θ̂pλq the dual optimal Lasso solution and r̃λpβ, θq :“ b qRλpθq2 ´ pRλpβq2, then\nθ̂pλq P B ´ θ, r̃λpβ, θq ¯ . (15)\nProof. The construction of the ball Bpθ, r̃λpβ, θqq is based on the weak duality theorem (cf. (Rockafellar & Wets,\n1998) for a reminder on weak and strong duality). Fix θ P ∆X and β P Rp, then it holds that\n1 2 ‖y‖2 ´ λ\n2\n2 ∥∥∥θ ´ y λ ∥∥∥2 ď 1 2 ‖Xβ ´ y‖2 ` λ ‖β‖1 .\nHence,\n∥∥∥θ ´ y λ ∥∥∥ ě c ´ ‖y‖2 ´ ‖Xβ ´ y‖2 ´ 2λ ‖β‖1 ¯ ` λ . (16)\nIn particular, this provides }θ̂pλq ´ y{λ} ě pRλpβq. Combining (12) and (16), asserts that θ̂pλq belongs to the annulus Apy{λ, qRλpθq, pRλpβqq :“ tz P Rn : pRλpβq ď }z ´ y{λ} ď qRλpθqu (the light blue zone in Figure 2).\nRemind that the dual feasible set ∆X is convex, hence ∆X X Bpy{λ, qRλpθqq is also convex. Thanks to (16), ∆XXBpy{λ, qRλpθqq “ ∆XXApy{λ, qRλpθq, pRλpβqq, and then ∆X X Apy{λ, qRλpθq, pRλpβqq is convex too. Hence, θ̂pλq is inside the annulus Apy{λ, qRλpθq, pRλpβqq and so is rθ, θ̂pλqs Ď Apy{λ, qRλpθq, pRλpβqq by convexity (see Figure 2,(a) and Figure 2,(b)). Moreover, θ̂pλq is the point of rθ, θ̂pλqs which is closest to y{λ. The farthest where θ̂pλq can be according to this information would be if rθ, θ̂pλqs were tangent to the inner ball Bpy{λ, pRλpβqq and }θ̂pλq ´ y{λ} “ pRλpβq. Let us denote θint such a point. The tangency property reads ‖θint ´ y{λ‖ “ pRλpβq and pθ ´ θintqJpy{λ ´ θintq “ 0. Hence, with the later and the definition of qRλpθq, ‖θ ´ y{λ‖2 “ ‖θ ´ θint‖2 ` ‖θint ´ y{λ‖2and ‖θ ´ θint‖2 “ qRλpθq2 ´ pRλpβq2.\nSince by construction θ̂pλq cannot be further away from θ than θint (again, insights can be gleaned from Figure 2), we conclude that θ̂pλq P B ` θ, p qRλpθq2 ´ pRλpβq2q1{2 ˘ .\nRemark 6. Choosing β “ 0 and θ “ y{λmax, then one recovers the static safe rule given in Example 1.\nWith the definition of the primal (resp. dual) objective for Pλpβq, (resp. Dλpθq), the duality gap reads as Gλpβ, θq “ Pλpβq´Dλ pθq. Remind that ifGλpβ, θq ď , then one has Pλpβq ´ Pλpβ̂pλqq ď , which is a standard stopping criterion for Lasso solvers. The next proposition establishes a connection between the radius rλpβ, θq and the duality gap Gλpβ, θq. Proposition 1. For any pβ, θq P Rp ˆ ∆X , the following holds\nr̃λpβ, θq2 ď rλpβ, θq2 :“ 2\nλ2 Gλpβ, θq. (17)\nProof. Use the fact that qRλpθq2 “ ‖θ ´ y{λ‖2 and pRλpβq2 ě p‖y‖2 ´ ‖Xβ ´ y‖2 ´ 2λ ‖β‖1q{λ2.\nIf we could choose the “oracle” θ “ θ̂pλq and β “ β̂pλq in (15) then we would obtain a zero radius. Since those quantities are unknown, we rather pick dynamically the current available estimates given by an optimization algorithm: β “ βk and θ “ θk as in Eq. (11). Introducing GAP SAFE spheres and domes as below, Proposition 2 ensures that they are converging safe regions.\nGAP SAFE sphere:\nCk “ B pθk, rλpβ, θqq . (18)\nGAP SAFE dome:\nCk “ D\n¨\n˝\ny λ ` θk 2 , qRλpθkq 2 , 2\n˜\npRλpβkq qRλpθkq\n¸2\n´ 1, θk ´ yλ }θk ´ yλ}\n˛\n‚.\n(19)\nProposition 2. For any converging primal sequence pβkqkPN, and dual sequence pθkqkPN defined as in Eq. (11), then the GAP SAFE sphere and the GAP SAFE dome are converging safe regions.\nProof. For the GAP SAFE sphere the result follows from strong duality, Remark 4 and Proposition 1 yield limkÑ8 rλpβk, θkq “ 0, since limkÑ8 θk “ θ̂pλq and limkÑ8 βk “ β̂pλq. For the GAP SAFE dome, one can check that it is included in the GAP SAFE sphere, therefore inherits the convergence (see also Figure 2,(c) and (d)).\nRemark 7. The radius rλpβk, θkq can be compared with the radius considered for the Dynamic Safe rule and Dynamic ST3 (Bonnefoy et al., 2014a) respectively: qRλpθkq “ }θk ´ y{λ}2 and p qRλpθkq2 ´ δ2q1{2, where δ “ pλmax{λ ´ 1q{ ‖xj‹‖. We have proved that limkÑ8 rλpβk, θkq “ 0, but a weaker property is satisfied by the two other radius: limkÑ8 qRλpθkq “ qRλpθ̂pλqq “ } qRλpθ̂pλqq ´ y{λ}2 and limkÑ8p qRλpθkq2 ´ δ2q1{2 “ p qRλpθ̂pλqq2 ´ δ2q1{2 ą 0."
    }, {
      "heading" : "3.3. GAP SAFE rules : sequential for free",
      "text" : "As a byproduct, our dynamic screening tests provide a warm start strategy for the safe regions, making our GAP SAFE rules inherently sequential. The next proposition shows their efficiency when attacking a new tuning parameter, after having solved the Lasso for a previous λ, even only approximately. Handling approximate solutions is a critical issue to produce safe sequential strategies: without taking into account the approximation error, the screening might disregard relevant variables, especially the one near the safe regions boundaries. Except for λmax, it is unrealistic to assume that one can dispose of exact solutions.\nConsider λ0 “ λmax and a non-increasing sequence of T ´ 1 tuning parameters pλtqtPrT´1s in p0, λmaxq. In practice, we choose the common grid (Bühlmann & van de Geer, 2011)[2.12.1]): λt “ λ010´δt{pT´1q (for instance in Figure 3, we considered δ “ 3). The next result controls how the duality gap, or equivalently, the diameter of our GAP SAFE regions, evolves from λt´1 to λt.\nProposition 3. Suppose that t ě 1 and pβ, θq P Rpˆ∆X . Reminding r2λtpβ, θq “ 2Gλtpβ, θq{λ 2 t , the following holds\nr2λtpβ, θq “ ˆ λt´1 λt ˙ r2λt´1pβ, θq (20)\n` p1´ λt λt´1 q ∥∥∥∥Xβ ´ yλt ∥∥∥∥2 ´ pλt´1λt ´ 1q ‖θ‖2 . Proof. Details are given in the Appendix.\nThis proposition motivates to screen sequentially as follows: having pβ, θq P Rpˆ∆X such that Gλt´1pβ, θq ď , then, we can screen using the GAP SAFE sphere with center θ and radius rλpβ, θq. The adaptation to the GAP SAFE dome is straightforward and consists in replacing θk, βk, λ by θ, β, λt in the GAP SAFE dome definition.\nRemark 8. The basic sphere test of (Wang et al., 2013) requires the exact dual solution θ “ θ̂pλt´1q for center, and has radius |1{λt´1{λt´1| ‖y‖, which is strictly larger than ours. Indeed, if one has access to dual and primal optimal solutions at λt´1, i.e., pθ, βq “ pθ̂pλt´1q, β̂pλt´1qq, then r2λt´1pβ, θq “ 0, θ “ py ´Xβq{λt´1 and\nr2λtpβ, θq “ ˆ λ2t´1 λ2t p1´ λt λt´1 q ´ pλt´1 λt ´ 1q ˙ ‖θ‖2 ,\nď ˆ 1 λt ´ 1 λt´1\n˙2\n‖y‖2 ,\nsince }θ} ď }y}{λt´1 for θ “ θ̂pλt´1q.\nNote that contrarily to former sequential rules (Wang et al., 2013), our introduced GAP SAFE rules still work when one has only access to approximations of θ̂pλt´1q."
    }, {
      "heading" : "4. Experiments",
      "text" : ""
    }, {
      "heading" : "4.1. Coordinate Descent",
      "text" : "Screening procedures can be used with any optimization algorithm. We chose coordinate descent because it is well suited for machine learning tasks, especially with sparse and/or unstructured design matrix X . Coordinate descent requires to extract efficiently columns of X which is typically not easy in signal processing applications where X is commonly an implicit operator (e.g. Fourier or wavelets).\nAlgorithm 1 Coordinate descent with GAP SAFE rules input X, y, ,K, f, pλtqtPrT´1s\nInitialization: λ0 “ λmax βλ0 “ 0 for t P rT ´ 1s do β Ð βλt´1 (previous -solution) for k P rKs do\nif k mod f “ 1 then Compute θ and C thanks to (11) and (18) or (19) Get AλtpCq “ tj P rps : µCpxjq ě 1u as in (7) if Gλtpβ, θq ď then βλt Ð β break for j P AλtpCq do βj Ð ST `\nλt ‖xj‖2\n, βj ´ xJj pXβ´yq\n‖xj‖2 ˘\n# STpu, xq “ signpxq p|x| ´ uq` (softthreshold)\noutput pβλtqtPrT´1s\nWe implemented the screening rules of Table 1 based on the coordinate descent in Scikit-learn (Pedregosa et al., 2011). This code is written in Python and Cython to generate low level C code, offering high performance. A low level language is necessary for this algorithm to scale. Two implementations were written to work efficiently with both dense data stored as Fortran ordered arrays and sparse data stored in the compressed sparse column (CSC) format. Our pseudo-code is presented in Algorithm 1. In practice, we perform the dynamic screening tests every f “ 10 passes through the entire (active) variables. Iterations are stopped when the duality gap is smaller than the target accuracy.\nThe naive computation of θk in (11) involves the computation of ∥∥XJρk∥∥8 (ρk being the current residual), which costs Opnpq operations. This can be avoided as one knows when using a safe rule that the index achieving the maximum for this norm is in AλtpCq. Indeed, by construction arg maxjPAλt pCq |xJj θk| “ arg maxjPrps |xJj θk| “ arg maxjPrps |xJj ρk|. In practice the evaluation of the dual gap is therefore not a Opnpq but Opnqq where q is the size of AλtpCq. In other words, using screening also speeds up\nthe evaluation of the stopping criterion.\nWe did not compare our method against the strong rules of Tibshirani et al. (2012) because they are not safe and therefore need complex post-processing with parameters to tune. Also we did not compare against the sequential rule of Wang et al. (2013) (e.g., EDDP) because it requires the exact dual optimal solution of the previous Lasso problem, which is not available in practice and can prevent the solver from actually converging: this is a phenomenon we always observed on our experiments."
    }, {
      "heading" : "4.2. Number of screened variables",
      "text" : "Figure 3 presents the proportion of variables screened by several safe rules on the standard Leukemia dataset. The screening proportion is presented as a function of the number of iterations K. As the SAFE screening rule of El Ghaoui et al. (2012) is sequential but not dynamic, for a given λ the proportion of screened variables does not depend on K. The rules of Bonnefoy et al. (2014a) are more efficient on this dataset but they do not benefit much from the dynamic framework. Our proposed GAP SAFE tests screen much more variables, especially when the tuning parameter λ gets small, which is particularly relevant in practice. Moreover, even for very small λ’s (notice the logarithmic scale) where no variable is screened at the beginning of the optimization procedure, the GAP SAFE rules manage to screen more variables, especially when K increases. Finally, the figure demonstrates that the GAP SAFE dome test only brings marginal improvement over the sphere."
    }, {
      "heading" : "4.3. Gains in the computation of Lasso paths",
      "text" : "The main interest of variable screening is to reduce computation costs. Indeed, the time to compute the screening itself should not be larger than the gains it provides. Hence, we compared the time needed to compute Lasso paths to prescribed accuracy for different safe rules. Figures 4, 5 and 6 illustrate results on three datasets. Figure 4 presents results on the dense, small scale, Leukemia dataset. Figure 5 presents results on a medium scale sparse dataset obtained with bag of words features extracted from the 20newsgroup dataset (comp.graphics vs. talk.religion.misc with TF-IDF removing English stop words and words occurring only once or more than 95% of the time). Text feature extraction was done using Scikit-Learn. Figure 6 focuses on the large scale sparse RCV1 (Reuters Corpus Volume 1) dataset, cf. (Schmidt et al., 2013).\nIn all cases, Lasso paths are computed as required to estimate optimal regularization parameters in practice (when using cross-validation one path is computed for each fold). For each Lasso path, solutions are obtained for T “ 100 values of λ’s, as detailed in Section 3.3. Remark that the grid used is the default one in both Scikit-Learn and the glmnet R package. With our proposed GAP SAFE screening we obtain on all datasets substantial gains in computational time. We can already get an up to 3x speedup when we require a duality gap smaller than 10´4. The interest of the screening is even clearer for higher accuracies: GAP SAFE sphere is 11x faster than its competitors on the Leukemia dataset, at accuracy 10´8. One can observe that with the parameter grid used here, the larger is p compared to n, the higher is the gain in computation time.\nIn our experiments, the other safe screening rules did not show much speed-up. As one can see on Figure 3, those screening rules keep all the active variables for a wide range of λ’s. The algorithm is thus faster for large λ’s but slower afterwards, since we still compute the screening tests. Even if one can avoid some of these useless computations thanks to formulas like (14) or (10), the corresponding speed-up\nwould not be significant."
    }, {
      "heading" : "5. Conclusion",
      "text" : "We have presented new results on safe rules for accelerating algorithms solving the Lasso problem (see Appendix for extension to the Elastic Net). First, we have introduced the framework of converging safe rules, a key concept independent of the implementation chosen. Our second contribution was to leverage duality gap computations to create two safer rules satisfying the aforementioned convergence properties. Finally, we demonstrated the important practical benefits of those new rules by applying them to standard dense and sparse datasets using a coordinate descent solver. Future works will extend our framework to generalized linear model and group-Lasso."
    }, {
      "heading" : "Acknowledgment",
      "text" : "We acknowledge the support from Chair Machine Learning for Big Data at Télécom ParisTech and from the Orange/Télécom ParisTech think tank phi-TAB. This work benefited from the support of the ”FMJH Program Gaspard Monge in optimization and operation research”, and from the support to this program from EDF."
    }, {
      "heading" : "A. Supplementary materials",
      "text" : "We provided in this Appendix some more details on the theoretical results given in the main part.\nA.1. Dome test\nLet us consider the case where the safe region C is the dome Dpc, r, α, wq, with parameters: center c, radius r, relative distance ratio α and unit normal vector w.\nThe computation of the dome test formula proceeds as follows:\nσCpxjq “ # cJxj ` r}xj} if wJxj ă ´α}xj}, cJxj ´ rαwJxj ` r a p}xj}2 ´ |wJxj |2qp1´ α2q otherwise. (21)\nand so\nσCp´xjq “ # ´cJxj ` r}xj} if ´ wJxj ă ´α}xj}, ´cJxj ` rαwJxj ` r a p}xj}2 ´ |wJxj |2qp1´ α2q otherwise. (22)\nWith the previous display we can now compute µCpxjq :“ maxpσCpxjq, σCp´xjqq. Thanks to the Eq. (6), we express our dome test as:\nIf Mmin ă cJxj ăMmax, then β̂pλqj “ 0. (23)\nUsing the former notation:\nMmax “ # 1´ r}xj} if wJxj ă ´α}xj}, 1` rαwJxj ´ r a p}xj}2 ´ |wJxj |2qp1´ α2q otherwise. (24)\nMmin “ # ´1` r}xj} if ´ wJxj ă ´α}xj}, ´1` rαwJxj ` r a p}xj}2 ´ |wJxj |2qp1´ α2q otherwise. (25)\nLet us introduce the following dome parameters, for any θ P ∆X :\n• Center: c “ py{λ` θq{2.\n• Radius: r “ qRλpθq{2.\n• Ratio: α “ ´1` 2 pRλpθq2{ qRλpθq2.\n• Normal vector: w “ py{λ´ θq{ qRλpθq.\nReminding that the support function of a set is the same as the support function of its closed convex hull (Hiriart-Urruty & Lemaréchal, 1993)[Proposition V.2.2.1] means that we only need to optimize over the dome introduced. Therefore, one cannot improve our previous result by optimizing the problem on the intersection of the ball of radius qRλpθq and the complement of the ball of radius pRλpβq (i.e., the blue region in Figure 2).\nA.2. Proof of Theorem 1\nProof. Define maxjREλ |xJj θ̂pλq| “ t ă 1. Fix ą 0 such that ă p1´tq{pmaxjREλ }xj}q. As Ck is a converging sequence containing θ̂pλq, its diameter is converging to zero, and there exists k0 P N such that @k ě k0,@θ P Ck, }θ ´ θ̂pλq} ď . Hence, for any j R Eλ and any θ P Ck, |xJj pθ´ θ̂pλqq| ď pmaxjREλ }xj}q}θ´ θ̂pλq} ď pmaxjREλ }xj}q . Using the triangle inequality, one gets\n|xJj θ| ďpmax jREλ }xj}q `max jREλ |xJj θ̂pλq|\nďpmax jREλ }xj}q ` t ă 1,\nprovided that ă p1´ tq{pmaxjREλ }xj}q. Thus, for any k ě k0, Ecλ Ă ZpλqpCkq “ ApλqpCkqc and ApλqpCkq Ă Eλ.\nFor the reverse inclusion take j P Eλ, i.e., |xJj θ̂pλq| “ 1. Since for all k P N, θ̂pλq P Ck, then j P ApλqpCkq “ tj P rps : maxθPCk |xJj θ| ě 1u and the result holds.\nA.3. Proof of Proposition 3\nWe detail here the proof of Proposition 3.\nProof. We first use the fact that\nGλt´1pβ, θq “ 1\n2 ‖Xβ ´ y‖22 ` λt´1 ‖β‖1 ´\n1 2 ‖y‖22 ` λ2t´1 2 ∥∥∥∥θ ´ yλt´1 ∥∥∥∥2\n2\n,\nto obtain\n‖β‖1 “ 1\nλt´1\n´1\n2 ‖y‖22 ´ ‖Xβ ´ y‖ 2 2 ´ λ2t´1 2 ∥∥∥∥θ ´ yλt´1 ∥∥∥∥2\n2\n`Gλt´1pβ, θq ¯ .\nThen,\nGλtpβ,θq “ 1\n2 ‖Xβ ´ y‖22 ` λt λt´1 ´1 2 ‖y‖22 ´ 1 2 ‖Xβ ´ y‖22 ´ λ2t´1 2 ∥∥∥∥θ ´ yλt´1 ∥∥∥∥2\n2\n`Gλt´1pβ, θq ¯\n´ 1 2 ‖y‖22 ` λ2t 2 ∥∥∥∥θ ´ yλt ∥∥∥∥2\n2\n“1 2 p λt λt´1 ´ 1q ‖y‖22 ` 1 2 p1´ λt λt´1 q ‖Xβ ´ y‖22 ` λt λt´1 Gλt´1pβ, θq ` 1 2 ` ‖λtθ ´ y‖22 ´ λt λt´1 ‖λt´1θ ´ y‖22 ˘\n“1 2 p λt λt´1 ´ 1q ‖y‖22 ` 1 2 p1´ λt λt´1 q ‖Xβ ´ y‖22 ` λt λt´1 Gλt´1pβ, θq\n` 1 2 ´ ‖λtθ ´ y‖22 ´ λt λt´1 ` ‖λtθ ´ y‖22 ` ‖pλt´1 ´ λtqθ‖ 2 2 ` 2pλtθ ´ yq Jpλt´1 ´ λtqθ ˘ ¯ .\nWe deal with the dot product as\n2λtpλt´1 ´ λtqpθ ´ y\nλt qJθ “ λtpλt´1 ´ λtq ` ‖θ‖22 ` ∥∥∥∥θ ´ yλt ∥∥∥∥2 2 ´ ∥∥∥∥ yλt ∥∥∥∥2 2 ˘ .\nHence,\nGλtpβ, θq “ 1 2 p λt λt´1 ´ 1` 1 λt´1 pλt´1 ´ λtqq ‖y‖22 ` 1 2 p1´ λt λt´1 q ‖Xβ ´ y‖22\n´ λt 2 pλt´1 ´ λtqq ‖θ‖22 ` 1 2 p1´ λt λt´1 ´ 1 λt´1 pλt´1 ´ λtqq ‖λtθ ´ y‖22 ` λt λt´1 Gλt´1pβ, θq\n“1 2\nˆ\n1´ λt λt´1\n˙\n‖Xβ ´ y‖22 ´ λt 2 pλt´1 ´ λtqq}θ}2 ` λt λt´1 Gλt´1pβ, θq.\nWe observe in the end that\n2\nλ2t Gλtpβ, θq “\nˆ\n1´ λt λt´1 ˙ ∥∥∥∥Xβ ´ yλt ∥∥∥∥2 2 ´ ˆ λt´1 λt ´ 1 ˙ ‖θ‖22 ` 2 λt´1λt Gλt´1pβ, θq.\nA.4. Elastic-Net\nThe previously proposed tests can be adapted straightforwardly to the Elastic-Net estimator (Zou & Hastie, 2005). We provide here some more details for the interested reader.\nmin βPRp\n1 2 ‖Xβ ´ y‖22 ` λα ‖β‖1 ` λ 2 p1´ αq ‖β‖22 . (26)\nOne can reformulate this problem as a Lasso problem\nmin βPRp\n1\n2 ∥∥∥X̃β ´ ỹ∥∥∥2 2 ` λα ‖β‖1 , (27)\nwhere X̃ “ ˆ X a\np1´ αqλIp\n˙ P Rn`p,p and ỹ “ ˆ\ny 0\n˙\nP Rn`p. With this modification all the tests introduced for the\nLasso can be adapted for the Elastic-Net."
    } ],
    "references" : [ {
      "title" : "Bolasso: model consistent Lasso estimation through the bootstrap",
      "author" : [ "F. Bach" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Bach,? \\Q2008\\E",
      "shortCiteRegEx" : "Bach",
      "year" : 2008
    }, {
      "title" : "A fast iterative shrinkagethresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM J. Imaging Sci.,",
      "citeRegEx" : "Beck and Teboulle,? \\Q2009\\E",
      "shortCiteRegEx" : "Beck and Teboulle",
      "year" : 2009
    }, {
      "title" : "Simultaneous analysis of Lasso and Dantzig selector",
      "author" : [ "P.J. Bickel", "Y. Ritov", "A.B. Tsybakov" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "Bickel et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bickel et al\\.",
      "year" : 2009
    }, {
      "title" : "A dynamic screening principle for the lasso",
      "author" : [ "A. Bonnefoy", "V. Emiya", "L. Ralaivola", "R. Gribonval" ],
      "venue" : "In EUSIPCO,",
      "citeRegEx" : "Bonnefoy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bonnefoy et al\\.",
      "year" : 2014
    }, {
      "title" : "Dynamic Screening: Accelerating First-Order Algorithms for the Lasso and Group-Lasso",
      "author" : [ "A. Bonnefoy", "V. Emiya", "L. Ralaivola", "R. Gribonval" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Bonnefoy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bonnefoy et al\\.",
      "year" : 2014
    }, {
      "title" : "Statistics for highdimensional data",
      "author" : [ "P. Bühlmann", "S. van de Geer" ],
      "venue" : null,
      "citeRegEx" : "Bühlmann and Geer,? \\Q2011\\E",
      "shortCiteRegEx" : "Bühlmann and Geer",
      "year" : 2011
    }, {
      "title" : "Enhancing sparsity by reweighted l1 minimization",
      "author" : [ "E.J. Candès", "M.B. Wakin", "S.P. Boyd" ],
      "venue" : "J. Fourier Anal. Applicat.,",
      "citeRegEx" : "Candès et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Candès et al\\.",
      "year" : 2008
    }, {
      "title" : "A first-order primal-dual algorithm for convex problems with applications to imaging",
      "author" : [ "A. Chambolle", "T. Pock" ],
      "venue" : "J. Math. Imaging Vis.,",
      "citeRegEx" : "Chambolle and Pock,? \\Q2011\\E",
      "shortCiteRegEx" : "Chambolle and Pock",
      "year" : 2011
    }, {
      "title" : "Atomic decomposition by basis pursuit",
      "author" : [ "S.S. Chen", "D.L. Donoho", "M.A. Saunders" ],
      "venue" : "SIAM J. Sci. Comput.,",
      "citeRegEx" : "Chen et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 1998
    }, {
      "title" : "Least angle regression",
      "author" : [ "B. Efron", "T. Hastie", "I.M. Johnstone", "R. Tibshirani" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "Efron et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Efron et al\\.",
      "year" : 2004
    }, {
      "title" : "Safe feature elimination in sparse supervised learning",
      "author" : [ "L. El Ghaoui", "V. Viallon", "T. Rabbani" ],
      "venue" : "J. Pacific Optim.,",
      "citeRegEx" : "Ghaoui et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ghaoui et al\\.",
      "year" : 2012
    }, {
      "title" : "Variable selection via nonconcave penalized likelihood and its oracle properties",
      "author" : [ "J. Fan", "R. Li" ],
      "venue" : "J. Amer. Statist. Assoc.,",
      "citeRegEx" : "Fan and Li,? \\Q2001\\E",
      "shortCiteRegEx" : "Fan and Li",
      "year" : 2001
    }, {
      "title" : "Sure independence screening for ultrahigh dimensional feature space",
      "author" : [ "J. Fan", "J. Lv" ],
      "venue" : "J. Roy. Statist. Soc. Ser. B,",
      "citeRegEx" : "Fan and Lv,? \\Q2008\\E",
      "shortCiteRegEx" : "Fan and Lv",
      "year" : 2008
    }, {
      "title" : "Pathwise coordinate optimization",
      "author" : [ "J. Friedman", "T. Hastie", "H. Höfling", "R. Tibshirani" ],
      "venue" : "Ann. Appl. Stat.,",
      "citeRegEx" : "Friedman et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2007
    }, {
      "title" : "Mixednorm estimates for the M/EEG inverse problem using accelerated gradient methods",
      "author" : [ "A. Gramfort", "M. Kowalski", "M. Hämäläinen" ],
      "venue" : "Physics in Medicine and Biology,",
      "citeRegEx" : "Gramfort et al\\.,? \\Q1937\\E",
      "shortCiteRegEx" : "Gramfort et al\\.",
      "year" : 1937
    }, {
      "title" : "TIGRESS: Trustful Inference of Gene REgulation using Stability Selection",
      "author" : [ "Haury", "A.-C", "F. Mordelet", "P. Vera-Licona", "J.P. Vert" ],
      "venue" : "BMC systems biology,",
      "citeRegEx" : "Haury et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Haury et al\\.",
      "year" : 2012
    }, {
      "title" : "Convex analysis and minimization algorithms. I, volume 305",
      "author" : [ "Hiriart-Urruty", "J.-B", "C. Lemaréchal" ],
      "venue" : "SpringerVerlag, Berlin,",
      "citeRegEx" : "Hiriart.Urruty et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Hiriart.Urruty et al\\.",
      "year" : 1993
    }, {
      "title" : "An interior-point method for large-scale l1regularized least squares",
      "author" : [ "Kim", "S.-J", "K. Koh", "M. Lustig", "S. Boyd", "D. Gorinevsky" ],
      "venue" : "IEEE J. Sel. Topics Signal Process.,",
      "citeRegEx" : "Kim et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2007
    }, {
      "title" : "Local linear convergence of forward–backward under partial smoothness",
      "author" : [ "J. Liang", "J. Fadili", "G. Peyré" ],
      "venue" : "In NIPS, pp. 1970–1978,",
      "citeRegEx" : "Liang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2014
    }, {
      "title" : "Sparse MRI: The application of compressed sensing for rapid MR imaging",
      "author" : [ "M. Lustig", "D.L. Donoho", "J.M. Pauly" ],
      "venue" : "Magnetic Resonance in Medicine,",
      "citeRegEx" : "Lustig et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Lustig et al\\.",
      "year" : 2007
    }, {
      "title" : "Sparse coding for machine learning, image processing and computer vision",
      "author" : [ "J. Mairal" ],
      "venue" : "PhD thesis, École normale supérieure de Cachan,",
      "citeRegEx" : "Mairal,? \\Q2010\\E",
      "shortCiteRegEx" : "Mairal",
      "year" : 2010
    }, {
      "title" : "Complexity analysis of the lasso regularization path",
      "author" : [ "J. Mairal", "B. Yu" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Mairal and Yu,? \\Q2012\\E",
      "shortCiteRegEx" : "Mairal and Yu",
      "year" : 2012
    }, {
      "title" : "A new approach to variable selection in least squares problems",
      "author" : [ "M.R. Osborne", "B. Presnell", "B.A. Turlach" ],
      "venue" : "IMA J. Numer. Anal.,",
      "citeRegEx" : "Osborne et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Osborne et al\\.",
      "year" : 2000
    }, {
      "title" : "Variational analysis, volume 317 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences",
      "author" : [ "R.T. Rockafellar", "Wets", "R.J.-B" ],
      "venue" : null,
      "citeRegEx" : "Rockafellar et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Rockafellar et al\\.",
      "year" : 1998
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "M. Schmidt", "N. Le Roux", "F. Bach" ],
      "venue" : "arXiv preprint arXiv:1309.2388,",
      "citeRegEx" : "Schmidt et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 2013
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "J. Roy. Statist. Soc. Ser. B,",
      "citeRegEx" : "Tibshirani,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani",
      "year" : 1996
    }, {
      "title" : "Strong rules for discarding predictors in lasso-type problems",
      "author" : [ "R. Tibshirani", "J. Bien", "J. Friedman", "T. Hastie", "N. Simon", "J. Taylor", "R.J. Tibshirani" ],
      "venue" : "J. Roy. Statist. Soc. Ser. B,",
      "citeRegEx" : "Tibshirani et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tibshirani et al\\.",
      "year" : 2012
    }, {
      "title" : "The lasso problem and uniqueness",
      "author" : [ "R.J. Tibshirani" ],
      "venue" : "Electron. J. Stat.,",
      "citeRegEx" : "Tibshirani,? \\Q2013\\E",
      "shortCiteRegEx" : "Tibshirani",
      "year" : 2013
    }, {
      "title" : "Convergence of a block coordinate descent method for nondifferentiable minimization",
      "author" : [ "P. Tseng" ],
      "venue" : "J. Optim. Theory Appl.,",
      "citeRegEx" : "Tseng,? \\Q2001\\E",
      "shortCiteRegEx" : "Tseng",
      "year" : 2001
    }, {
      "title" : "Smallsample brain mapping: sparse recovery on spatially correlated designs with randomization and clustering",
      "author" : [ "G. Varoquaux", "A. Gramfort", "B. Thirion" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Varoquaux et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Varoquaux et al\\.",
      "year" : 2012
    }, {
      "title" : "Lasso screening rules via dual polytope projection",
      "author" : [ "J. Wang", "J. Zhou", "P. Wonka", "J. Ye" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Wang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Fast lasso screening tests based on correlations",
      "author" : [ "Z.J. Xiang", "P.J. Ramadge" ],
      "venue" : "In ICASSP, pp",
      "citeRegEx" : "Xiang and Ramadge,? \\Q2012\\E",
      "shortCiteRegEx" : "Xiang and Ramadge",
      "year" : 2012
    }, {
      "title" : "Learning sparse representations of high dimensional data on large scale dictionaries",
      "author" : [ "Z.J. Xiang", "H. Xu", "P.J. Ramadge" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Xiang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Xiang et al\\.",
      "year" : 2011
    }, {
      "title" : "Screening tests for lasso problems",
      "author" : [ "Z.J. Xiang", "Y. Wang", "P.J. Ramadge" ],
      "venue" : "arXiv preprint arXiv:1405.4897,",
      "citeRegEx" : "Xiang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Xiang et al\\.",
      "year" : 2014
    }, {
      "title" : "Three structural results on the lasso problem",
      "author" : [ "P. Xu", "P.J. Ramadge" ],
      "venue" : "In ICASSP, pp",
      "citeRegEx" : "Xu and Ramadge,? \\Q2013\\E",
      "shortCiteRegEx" : "Xu and Ramadge",
      "year" : 2013
    }, {
      "title" : "Nearly unbiased variable selection under minimax concave penalty",
      "author" : [ "Zhang", "C.-H" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "Zhang and C..H.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhang and C..H.",
      "year" : 2010
    }, {
      "title" : "A general theory of concave regularization for high-dimensional sparse estimation problems",
      "author" : [ "Zhang", "C.-H", "T. Zhang" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2012
    }, {
      "title" : "The adaptive lasso and its oracle properties",
      "author" : [ "H. Zou" ],
      "venue" : "J. Am. Statist. Assoc.,",
      "citeRegEx" : "Zou,? \\Q2006\\E",
      "shortCiteRegEx" : "Zou",
      "year" : 2006
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "H. Zou", "T. Hastie" ],
      "venue" : "J. Roy. Statist. Soc. Ser. B,",
      "citeRegEx" : "Zou and Hastie,? \\Q2005\\E",
      "shortCiteRegEx" : "Zou and Hastie",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "In such a context, the least squares with `1 regularization, referred to as the Lasso (Tibshirani, 1996) in statistics, or Basis Pursuit (Chen et al.",
      "startOffset" : 86,
      "endOffset" : 104
    }, {
      "referenceID" : 8,
      "context" : "In such a context, the least squares with `1 regularization, referred to as the Lasso (Tibshirani, 1996) in statistics, or Basis Pursuit (Chen et al., 1998) in signal processing, has been one of the most popular tools.",
      "startOffset" : 137,
      "endOffset" : 156
    }, {
      "referenceID" : 2,
      "context" : "It enjoys theoretical guarantees (Bickel et al., 2009), as well as practical benefits: it provides sparse solutions and fast convex solvers are available.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 20,
      "context" : "one can mention dictionary learning (Mairal, 2010), biostatistics (Haury et al.",
      "startOffset" : 36,
      "endOffset" : 50
    }, {
      "referenceID" : 15,
      "context" : "one can mention dictionary learning (Mairal, 2010), biostatistics (Haury et al., 2012) and medical imaging (Lustig et al.",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 19,
      "context" : ", 2012) and medical imaging (Lustig et al., 2007; Gramfort et al., 2012) to name a few.",
      "startOffset" : 28,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "For stability selection methods (Meinshausen & Bühlmann, 2010; Bach, 2008; Varoquaux et al., 2012), hundreds of Lasso problems need to be solved.",
      "startOffset" : 32,
      "endOffset" : 98
    }, {
      "referenceID" : 29,
      "context" : "For stability selection methods (Meinshausen & Bühlmann, 2010; Bach, 2008; Varoquaux et al., 2012), hundreds of Lasso problems need to be solved.",
      "startOffset" : 32,
      "endOffset" : 98
    }, {
      "referenceID" : 37,
      "context" : "For non-convex approaches such as SCAD (Fan & Li, 2001) or MCP (Zhang, 2010), solving the Lasso is often a required preliminary step (Zou, 2006; Zhang & Zhang, 2012; Candès et al., 2008).",
      "startOffset" : 133,
      "endOffset" : 186
    }, {
      "referenceID" : 6,
      "context" : "For non-convex approaches such as SCAD (Fan & Li, 2001) or MCP (Zhang, 2010), solving the Lasso is often a required preliminary step (Zou, 2006; Zhang & Zhang, 2012; Candès et al., 2008).",
      "startOffset" : 133,
      "endOffset" : 186
    }, {
      "referenceID" : 22,
      "context" : "Among possible algorithmic candidates for solving the Lasso, one can mention homotopy methods (Osborne et al., 2000), LARS (Efron et al.",
      "startOffset" : 94,
      "endOffset" : 116
    }, {
      "referenceID" : 9,
      "context" : ", 2000), LARS (Efron et al., 2004), and approximate homotopy (Mairal & Yu, 2012), that provide solutions for the full Lasso path, i.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 13,
      "context" : "More recently, particularly for p ą n, coordinate descent approaches (Friedman et al., 2007) have proved to be among the best methods to tackle large scale problems.",
      "startOffset" : 69,
      "endOffset" : 92
    }, {
      "referenceID" : 33,
      "context" : "We refer to (Xiang et al., 2014) for a concise introduction on safe rules.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 26,
      "context" : "This is for instance the strategy adopted for the strong rules (Tibshirani et al., 2012).",
      "startOffset" : 63,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "For stability selection methods (Meinshausen & Bühlmann, 2010; Bach, 2008; Varoquaux et al., 2012), hundreds of Lasso problems need to be solved. For non-convex approaches such as SCAD (Fan & Li, 2001) or MCP (Zhang, 2010), solving the Lasso is often a required preliminary step (Zou, 2006; Zhang & Zhang, 2012; Candès et al., 2008). Among possible algorithmic candidates for solving the Lasso, one can mention homotopy methods (Osborne et al., 2000), LARS (Efron et al., 2004), and approximate homotopy (Mairal & Yu, 2012), that provide solutions for the full Lasso path, i.e., for all possible choices of tuning parameter λ. More recently, particularly for p ą n, coordinate descent approaches (Friedman et al., 2007) have proved to be among the best methods to tackle large scale problems. Following the seminal work by El Ghaoui et al. (2012), screening techniques have emerged as a way to exploit the known sparsity of the solution by discarding features prior to starting a Lasso solver.",
      "startOffset" : 63,
      "endOffset" : 847
    }, {
      "referenceID" : 30,
      "context" : "This road has been pursued in (Wang et al., 2013; Xu & Ramadge, 2013; Xiang et al., 2014), and can be thought of as a “warm start” of the screening (in addition to the warm start of the solution itself).",
      "startOffset" : 30,
      "endOffset" : 89
    }, {
      "referenceID" : 33,
      "context" : "This road has been pursued in (Wang et al., 2013; Xu & Ramadge, 2013; Xiang et al., 2014), and can be thought of as a “warm start” of the screening (in addition to the warm start of the solution itself).",
      "startOffset" : 30,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : "Denoting ∆X “ θ P R : ∣∣xJj θ∣∣ ď 1,@j P rps( the dual feasible set, a dual formulation of the Lasso reads (see for instance Kim et al. (2007) or Xiang et al.",
      "startOffset" : 125,
      "endOffset" : 143
    }, {
      "referenceID" : 17,
      "context" : "Denoting ∆X “ θ P R : ∣∣xJj θ∣∣ ď 1,@j P rps( the dual feasible set, a dual formulation of the Lasso reads (see for instance Kim et al. (2007) or Xiang et al. (2014)):",
      "startOffset" : 125,
      "endOffset" : 166
    }, {
      "referenceID" : 32,
      "context" : ", 2012) y{λ q Rλp y λmax q λmax “ }X y}8“|xj‹y| Dynamic ST3 (Xiang et al., 2011) y{λ ́ δxj‹ p q Rλpθkq  ́ δq 1 2 δ “ ` λmax λ ́ 1 ̆",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 30,
      "context" : ", as in (11) ) Sequential (Wang et al., 2013) θ̂pλt ́1q ˇ",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 33,
      "context" : "See for instance (Xiang et al., 2014) for more details.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 27,
      "context" : "If C “ tθ̂pλqu, the safe active set is the equicorrelation set ApλqpCq “ Eλ :“ tj P rps : |xj θ̂pλq| “ 1u (in most cases (Tibshirani, 2013) it is exactly the active set of β̂pλq).",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 27,
      "context" : "If it is not unique, the equicorrelation set contains all the solutions’ supports and there exists a Lasso solution whose support is exactly this set (Tibshirani, 2013)[Lemma 12].",
      "startOffset" : 150,
      "endOffset" : 168
    }, {
      "referenceID" : 33,
      "context" : "For simplicity we focus only on balls and domes, though more complicated regions could be investigated (Xiang et al., 2014).",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 33,
      "context" : "been considered for instance in (El Ghaoui et al., 2012; Xiang & Ramadge, 2012; Xiang et al., 2014; Bonnefoy et al., 2014b).",
      "startOffset" : 32,
      "endOffset" : 123
    }, {
      "referenceID" : 33,
      "context" : "Due to its length though, the formula is deferred to the Appendix (see also (Xiang et al., 2014)[Lemma 3] for more details).",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "Following El Ghaoui et al. (2012) (see also (Bonnefoy et al.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 18,
      "context" : "sometimes referred to as finite identification of the support (Liang et al., 2014).",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 28,
      "context" : ", Forward-Backward (Beck & Teboulle, 2009), Primal Dual (Chambolle & Pock, 2011), coordinatedescent (Tseng, 2001; Friedman et al., 2007)) and relies only on the convergence of a sequence of safe regions.",
      "startOffset" : 100,
      "endOffset" : 136
    }, {
      "referenceID" : 13,
      "context" : ", Forward-Backward (Beck & Teboulle, 2009), Primal Dual (Chambolle & Pock, 2011), coordinatedescent (Tseng, 2001; Friedman et al., 2007)) and relies only on the convergence of a sequence of safe regions.",
      "startOffset" : 100,
      "endOffset" : 136
    }, {
      "referenceID" : 17,
      "context" : "A more general result is proved for a specific algorithm (Forward-Backward) in Liang et al. (2014). Interestingly, our scheme is independent of the algorithm considered (e.",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 30,
      "context" : "The basic sphere test of (Wang et al., 2013) requires the exact dual solution θ “ θ̂pλt ́1q for center, and has radius |1{λt ́1{λt ́1| ‖y‖, which is strictly larger than ours.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 30,
      "context" : "Note that contrarily to former sequential rules (Wang et al., 2013), our introduced GAP SAFE rules still work when one has only access to approximations of θ̂pλt ́1q.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 25,
      "context" : "We did not compare our method against the strong rules of Tibshirani et al. (2012) because they are not safe and therefore need complex post-processing with parameters to tune.",
      "startOffset" : 58,
      "endOffset" : 83
    }, {
      "referenceID" : 25,
      "context" : "We did not compare our method against the strong rules of Tibshirani et al. (2012) because they are not safe and therefore need complex post-processing with parameters to tune. Also we did not compare against the sequential rule of Wang et al. (2013) (e.",
      "startOffset" : 58,
      "endOffset" : 251
    }, {
      "referenceID" : 8,
      "context" : "As the SAFE screening rule of El Ghaoui et al. (2012) is sequential but not dynamic, for a given λ the proportion of screened variables does not depend on K.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "The rules of Bonnefoy et al. (2014a) are more efficient on this dataset but they do not benefit much from the dynamic framework.",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 24,
      "context" : "(Schmidt et al., 2013).",
      "startOffset" : 0,
      "endOffset" : 22
    } ],
    "year" : 2015,
    "abstractText" : "Screening rules allow to early discard irrelevant variables from the optimization in Lasso problems, or its derivatives, making solvers faster. In this paper, we propose new versions of the socalled safe rules for the Lasso. Based on duality gap considerations, our new rules create safe test regions whose diameters converge to zero, provided that one relies on a converging solver. This property helps screening out more variables, for a wider range of regularization parameter values. In addition to faster convergence, we prove that we correctly identify the active sets (supports) of the solutions in finite time. While our proposed strategy can cope with any solver, its performance is demonstrated using a coordinate descent algorithm particularly adapted to machine learning use cases. Significant computing time reductions are obtained with respect to previous safe rules.",
    "creator" : "LaTeX with hyperref package"
  }
}