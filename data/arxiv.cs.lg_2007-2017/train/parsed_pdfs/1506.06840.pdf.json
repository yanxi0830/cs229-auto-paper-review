{
  "name" : "1506.06840.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants",
    "authors" : [ "Sashank J. Reddi", "Ahmed Hefny" ],
    "emails" : [ "sjakkamr@cs.cmu.edu", "ahefny@cs.cmu.edu", "suvrit@mit.edu", "bapoczos@cs.cmu.edu", "alex@smola.org" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "There has been a steep rise in recent work [5, 6, 8–11, 23, 25, 27] on “variance reduced” stochastic gradient algorithms for convex problems of the finite-sum form:\nmin x∈Rd\nf(x) := 1n ∑n i=1 fi(x). (1.1)\nUnder strong convexity assumptions such variance reduced (VR) stochastic algorithms attain better convergence rates (in expectation) than stochastic gradient descent (SGD) [17, 22], both in theory and practice.1 The key property of these VR algorithms is that by exploiting problem structure and by making suitable space-time tradeoffs, they reduce the variance incurred due to stochastic gradients. This variance reduction has powerful consequences: it bestows VR stochastic methods with linear convergence rates, and thereby circumvents slowdowns that usually hit SGD.\n1Though we should note that SGD also applies to the harder stochastic optimization problem min F (x) = E[f(x; ξ)], which need not be a finite-sum.\nar X\niv :1\n50 6.\n06 84\n0v 1\n[ cs\n.L G\nAlthough these advances have great value in general, for large-scale problems we still require parallel or distributed processing. And in this setting, asynchronous variants of SGD remain indispensable [2, 7, 12, 20, 26, 28]. Therefore, a key question is how to extend the synchronous finite-sum VR algorithms to asynchronous parallel and distributed settings.\nWe answer one part of this question by developing new asynchronous parallel stochastic gradient methods that provably converge at a linear rate for smooth strongly convex finite-sum problems. Our methods are inspired by the influential SVRG [9], S2GD [11], SAG [23] and SAGA [5] family of algorithms. We list our contributions more precisely below.\nContributions. Our paper has two core components: (i) a formal general framework for variance reduced stochastic methods based on discussions in [5]; and (ii) asynchronous parallel VR algorithms within the framework. The general framework presents a formal unifying view of several VR methods (e.g., it includes SAGA and SVRG as special cases) while expressing key algorithmic and practical tradeoffs concisely. Thus, it yields a broader understanding of VR methods, which helps us obtain asynchronous parallel variants of VR methods. Under settings common to machine learning problems, our parallel algorithms attain speedups that scale near linearly with the number of processors.\nAs a concrete illustration, we present a specialization to an asynchronous SVRG-like method. We compare this specialization with non-variance reduced asynchronous SGD methods, and observe strong empirical speedups that agree with the theory.\nRelated work. As already mentioned, our work is closest to (and generalizes) SAG [23], SAGA [5], SVRG [9] and S2GD [11], which are primal methods. Also closely related are dual methods such as SDCA [25] and Finito [6], and in its convex incarnation MISO [15]; a more precise relation between these dual methods and VR stochastic methods is described in Defazio’s thesis [4]. By their algorithmic structure, these VR methods trace back to classical non-stochastic incremental gradient algorithms [3], but by now it is well-recognized that randomization helps obtain much sharper convergence results (in expectation). Proximal [27] and accelerated VR methods have also been proposed [19, 24]; we leave a study of such variants of our framework as future work. Finally, there is recent work on lower-bounds for finite-sum problems [1].\nWithin asynchronous SGD algorithms, both parallel [20] and distributed [2, 16] variants are known. In this paper, we focus our attention on the parallel setting. A different line of methods is that of (primal) coordinate descent methods, and their parallel and distributed variants [13, 14, 18, 21]. Our asynchronous methods share some structural assumptions with these methods. Finally, the recent work [10] generalizes S2GD to the mini-batch setting, thereby also permitting parallel processing, albeit with more synchronization and allowing only small mini-batches."
    }, {
      "heading" : "2 A General Framework for VR Stochastic Methods",
      "text" : "We focus on instances of (1.1) where the cost function f(x) has an L-Lipschitz gradient, so that ‖∇f(x)−∇f(y)‖ ≤ L‖x− y‖, and it is λ-strongly convex, i.e., for all x, y ∈ Rd,\nf(x) ≥ f(y) + 〈∇f(y), x− y〉+ λ2 ‖x− y‖ 2. (2.1)\nWhile our analysis focuses on strongly convex functions, we can extend it to just smooth convex functions along the lines of [27].\nInspired by the discussion on a general view of variance reduced techniques in [5], we now describe a formal general framework for variance reduction in stochastic gradient descent. We denote the collection {fi}ni=1 of functions that make up f in (1.1) by F . For our algorithm, we maintain an additional parameter αti ∈ Rd for each fi ∈ F . We use At to denote {αti}ni=1. The general iterative framework for updating the parameters is presented as Algorithm 1. Observe that the algorithm is still abstract, since it does not specify the subroutine SCHEDULEUPDATE. This subroutine determines the crucial update mechanism of {αti} (and thereby ofAt). As we will see different schedules give rise to different fast first-order methods proposed in the literature. The part of the update based on At is the key for these approaches and is responsible for variance reduction.\nNext, we provide different instantiations of the framework and construct a new algorithm derived from it. In particular, we consider incremental methods SAG [23], SVRG [9] and SAGA [5], and classic gradient descent GRADIENTDESCENT for demonstrating our framework.\nALGORITHM 1: GENERIC STOCHASTIC VARIANCE REDUCTION ALGORITHM Data: x0 ∈ Rd, α0i = x0 ∀i ∈ [n] , {1, . . . , n}, step size η > 0 Randomly pick a IT = {i0, . . . , iT } where it ∈ {1, . . . , n} ∀ t ∈ {0, . . . , T} ; for t = 0 to T do\nUpdate iterate as xt+1 ← xt − η ( ∇fit(xt)−∇fit(αtit) + 1 n ∑ i fi(α t i) )\n; At+1 = SCHEDULEUPDATE({xi}t+1i=0, At, t, IT ) ;\nend return xT\nFigure 1 shows the schedules for the aforementioned algorithms. In case of SVRG, SCHEDULEUPDATE is triggered every m iterations (here m denotes precisely the number of inner iterations used in [9]); so At remains unchanged for the m iterations and all αti are updated to the current iterate at the mth iteration. For SAGA, unlike SVRG, At changes at the tth iteration for all t ∈ [T ]. This change is only to a single element of At, and is determined by the index it (the function chosen at iteration t). The update of SAG is similar to SAGA insofar that only one of the αi is updated at each iteration. However, the update for At+1 is based on it+1 rather than it. This results in a biased estimate of the gradient, unlike SVRG and SAGA. Finally, the schedule for gradient descent is similar to SAG, except that all the αi’s are updated at each iteration. Due to the full update we end up with the exact gradient at each iteration. This discussion highlights how the scheduler determines the resulting gradient method.\nTo motivate the design of another schedule, let us consider the computational and storage costs of each of these algorithms. For SVRG, since we update At after every m iterations, it is enough to store a full gradient, and hence, the storage cost is O(d). However, the running time is O(nd) at each epoch and O(d) at each iteration since we have to calculate the full gradient at end of each epoch. In contrast, both SAG and SAGA have high storage costs of O(nd) and running time of O(d) per iteration. Finally, GRADIENTDESCENT has low storage cost since it needs to store the gradient at O(d) cost, but very high computational costs of O(nd) at each iteration.\nIt is interesting to note that when m is high (say greater than n), SVRG has low computational cost per iteration. However, as we will later see, this comes at the expense of slower convergence to the optimal solution. SAG and SAGA can converge faster by allowing us to update At more frequently, but at the cost of additional storage. The tradeoffs between the epoch size m, additional storage, frequency of updates, and the convergence to the optimal solution are still not completely resolved.\nA straightforward approach to design a new scheduler is to combine the schedules of the above algorithms. We call this schedule hybrid stochastic average gradient (HSAG). Specifically, we use the schedules of SVRG and SAGA to develop HSAG. Consider S ⊆ [n], the indices that follow SAGA schedule. We assume that rest of the indices follow an SVRG-like schedule with schedule frequency si for all i ∈ S , [n] \\S. Figure 2 shows the corresponding update schedule of HSAG. If\nThis algorithm assumes access to the index set S and the schedule frequency vector s.\nS = [n] then HSAG is equivalent to SAGA, while at the other extreme, for S = ∅ and si = m for all i ∈ [n], it corresponds to SVRG. HSAG exhibits interesting storage, computational and convergence trade-offs that depend on S. In general, while large cardinality of S likely incurs high storage costs, the computational cost per iteration is relatively low. On the other hand, when cardinality of S is small and si’s are large, storage costs are low but the convergence typically slows down.\nBefore concluding our discussion on the general framework, we would like to draw the reader’s attention to the advantages of studying Algorithm 1. First, note that Algorithm 1 provides a unifying framework for many incremental/stochastic gradient methods proposed in the literature. Second, and more importantly, it provides a generic platform for analyzing this class of algorithms. As we will see in Section 3, this helps us develop and analyze asynchronous versions for different finite-sum algorithms under a common umbrella. Finally, it provides a mechanism to derive new algorithms by designing more sophisticated schedules; as noted above, one such construction gives rise to HSAG."
    }, {
      "heading" : "2.1 Convergence Analysis",
      "text" : "In this section, we provide convergence analysis for Algorithm 1 with HSAG schedules. As observed earlier, SVRG and SAGA are special cases of this setup. Our analysis assumes unbiasedness of the gradient estimates at each iteration, so it does not encompass SAG. For ease of exposition, we assume that all si = m for all i ∈ [n]. Since HSAG is epoch-based, our analysis focuses on the iterates obtained after each epoch. Similar to [9] (see Option II of SVRG in [9]), our analysis will be for the case where the iterate at the end of (k + 1)st epoch, xkm+m, is replaced with an element chosen randomly from {xkm, . . . , xkm+m−1} with probability {p1, · · · , pm}. For brevity, we use x̃k to denote the iterate chosen at the kth epoch. We also need the following quantity for our analysis:\nG̃k , 1\nn ∑ i∈S ( fi(α km i )− fi(x∗)− 〈∇fi(x∗), αkmi − x∗〉 ) .\nTheorem 1. For any positive parameters c, β, κ > 1, step size η and epoch size m, we define the following quantities:\nγ = κ [ 1− ( 1− 1\nκ\n)m]( 2cη(1− Lη(1 + β))− 1\nn − 2c κλ ) θ = max {[ 2c\nγλ\n( 1− 1\nκ\n)m + 2Lcη2\nγ\n( 1 + 1\nβ\n) κ [ 1− ( 1− 1\nκ\n)m]] , ( 1− 1\nκ\n)m} ."
    }, {
      "heading" : "Suppose the probabilities pi ∝ (1− 1κ )",
      "text" : "m−i, and that c, β, κ, step size η and epoch sizem are chosen such that the following conditions are satisfied:\n1 κ + 2Lcη2\n( 1 + 1\nβ ) ≤ 1 n , γ > 0, θ < 1.\nThen, for iterates of Algorithm 1 under the HSAG schedule, we have E [ f(x̃k+1)− f(x∗) + 1\nγ G̃k+1\n] ≤ θ E [ f(x̃k)− f(x∗) + 1\nγ G̃k\n] .\nAs a corollary, we immediately obtain an expected linear rate of convergence for HSAG.\nCorollary 1. Note that G̃k ≥ 0 and therefore, under the conditions specified in Theorem 1 and θ̄ = θ (1 + 1/γ) < 1 we have\nE [ f(x̃k)− f(x∗) ] ≤ θ̄k [ f(x0)− f(x∗) ] .\nWe emphasize that there exist values of the parameters for which the conditions in Theorem 1 and Corollary 1 are easily satisfied. For instance, setting η = 1/16(λn + L), κ = 4/λη, β = (2λn + L)/L and c = 2/ηn, the conditions in Theorem 1 are satisfied for sufficiently large m. Additionally, in the high condition number regime of L/λ = n, we can obtain constant θ < 1 (say 0.5) with m = O(n) epoch size (similar to [5, 9]). This leads to accuracy in the objective function after n log(1/ ) number of iterations."
    }, {
      "heading" : "3 Asynchronous Stochastic Variance Reduction",
      "text" : "We are now ready to present asynchronous versions of the algorithms captured by our general framework. We first describe our setup before delving into the details of these algorithms. Our model of computation is similar to the ones used in Hogwild! [20] and AsySCD [14]. We assume a multicore architecture where each core makes stochastic gradient updates to a centrally stored vector x in an asynchronous manner. There are four key components in our asynchronous algorithm; these are briefly described below.\n1. Read: Read the iterate x and compute the gradient∇fit(x) for a randomly chosen it. 2. Read schedule iterate: Read the schedule iterate A and compute the gradients required\nfor update in Algorithm 1 3. Update: Update the iterate x with the computed incremental update in Algorithm 1. 4. Schedule Update: Run a scheduler update for updating A.\nEach processor repeatedly runs these procedures concurrently, without any synchronization. Hence, x may change in between Step 1 and Step 3. Similarly, A may change in between Steps 2 and 4. In fact, the states of iterates x and A can correspond to different time-stamps. We maintain a global counter t to track the number of updates successfully executed. We use D(t) ∈ [t] and D′(t) ∈ [t] to denote the particular x-iterate and A-iterate used for evaluating the update at the tth iteration. We assume that the delay in between the time of evaluation and updating is bounded by a non-negative integer τ , i.e., t−D(t) ≤ τ and t−D′(t) ≤ τ . The bound on the staleness captures the degree of parallelism in the method: such parameters are typical in asynchronous systems (see e.g., [14, 20]).\nWe assume a read consistent model for our analysis. In particular, our analysis requires that the vector x used for evaluation of gradients be a valid iterate that existed at some point in time. However, like Hogwild! our implementation is lock-free. We will revisit this point in Section 4."
    }, {
      "heading" : "3.1 Convergence Analysis",
      "text" : "The key ingredients to the success of asynchronous algorithms for multicore stochastic gradient descent are sparsity and “disjointness” of the data matrix [20]. We also exploit these properties of the data for our convergence analysis. More formally, let ‖x‖i denote the norm of x with respect to non-zero coordinates of function fi; then, the convergence depends on ∆, the smallest constant such that Ei[‖x‖2i ] ≤ ∆‖x‖2. Intuitively, ∆ denotes the average frequency with which a feature appears in the data matrix. We are interested in situations where ∆ 1. As a warm up, let us first discuss convergence analysis for asynchronous SVRG. The general case is similar, but much more involved. Hence, it is instructive to first go through the analysis of asynchronous SVRG. Theorem 2. Suppose step size η, epoch size m are chosen such that the following condition holds:\n0 < θs :=\n( 1 ληm + 4L ( η+L∆τ2η2 1−2L2∆η2τ2 )) (\n1− 4L ( η+L∆τ2η2\n1−2L2∆η2τ2 )) < 1. Then, for the iterates of an asynchronous variant of Algorithm 1 with SVRG schedule and probabilities pi = 1/m for all i ∈ [m], we have\nE[f(x̃k+1)− f(x∗)] ≤ θs E[f(x̃k)− f(x∗)].\nThe bound obtained in Theorem 2 is useful when ∆ is small. To see this, as earlier, consider the indicative case where L/λ = n. The synchronous version of SVRG obtains a convergence rate of θ = 0.5 for step size η = 0.1/L and epoch size m = O(n). For the asynchronous variant of SVRG, by setting η = 0.1/2(max{1,∆1/2τ}L), we obtain a similar rate with m = O(n + ∆1/2τn). To obtain this, set η = ρ/L where ρ = 0.1/2(max{1,∆1/2τ}) and θs = 0.5. Then, a simple calculation gives the following:\nm n = 2 ρ\n( 1− 2∆τ2ρ2\n1− 12ρ− 14∆τ2ρ2\n) ≤ c′max{1,∆1/2τ},\nwhere c′ is some constant. This follows from the fact that ρ = 0.1/2(max{1,∆1/2τ}). Suppose τ < 1/∆1/2. Then we can achieve nearly the same guarantees as the synchronous version, but τ times faster since we are running the algorithm asynchronously. For example, consider the sparse setting where ∆ = o(1/n); then it is possible to get near linear speedup when τ = o(n1/2). On the other hand, when ∆1/2τ > 1, we can obtain a theoretical speedup of 1/∆1/2.\nWe finally provide the convergence result for the asynchronous algorithm in the general case. The proof is complicated by the fact that set A, unlike in SVRG, changes during the epoch. The key idea is that only a single element of A changes at each iteration. Furthermore, it can only change to one of the iterates in the epoch. This control provides a handle on the error obtained due to the staleness. Due to space constraints, the proof is relegated to the appendix. Theorem 3. For any positive parameters c, β, κ > 1, step size η and epoch size m, we define the following quantities:\nζ = ( cη2 + ( 1− 1\nκ\n)−τ cL∆τ2η3 ) ,\nγa = κ\n[ 1− ( 1− 1\nκ\n)m][ 2cη − 8ζL(1 + β)− 2c\nκλ − 96ζLτ n\n( 1− 1\nκ )−τ − 1 n ] ,\nθa = max   2c γaλ ( 1− 1 κ )m + 8ζL ( 1 + 1β ) γa κ [ 1− ( 1− 1 κ )m] ,(1− 1 κ )m ."
    }, {
      "heading" : "Suppose probabilities pi ∝ (1− 1κ )",
      "text" : "m−i, parameters β, κ, step-size η, and epoch size m are chosen such that the following conditions are satisfied:\n1 κ + 8ζL\n( 1 + 1\nβ\n) + 96ζLτ\nn\n( 1− 1\nκ )−τ ≤ 1 n , η2 ≤ ( 1− 1 κ )m−1 1 12L2∆τ2 , γa > 0, θa < 1.\nThen, for the iterates of asynchronous variant of Algorithm 1 with HSAG schedule we have E [ f(x̃k+1)− f(x∗) + 1\nγa G̃k+1\n] ≤ θaE [ f(x̃k)− f(x∗) + 1\nγa G̃k\n] .\nCorollary 2. Note that G̃k ≥ 0 and therefore, under the conditions specified in Theorem 3 and θ̄a = θa (1 + 1/γa) < 1, we have\nE [ f(x̃k)− f(x∗) ] ≤ θ̄ka [ f(x0)− f(x∗) ] .\nBy using step size normalized by ∆1/2τ (similar to Theorem 2) and parameters similar to the ones specified after Theorem 1 we can show speedups similar to the ones obtained in Theorem 2.\nBefore ending our discussion on the theoretical analysis, we would like to highlight an important point. Our emphasis throughout the paper was on generality. While Theorem 1 and Theorem 3 are presented here in full generality, one can obtain stronger results in specific cases. For example, in the case of SAGA, one can obtain per iteration convergence guarantees (see [5]) rather than those corresponding to per epoch presented in the paper. However, there is no qualitative difference in these guarantees accumulated over the epoch. Furthermore, in this case, our analysis for both synchronous and asynchronous cases can be easily modified to obtain convergence properties similar to the ones obtained in [5]."
    }, {
      "heading" : "4 Experiments",
      "text" : "We present our empirical results in this section. For our experiments, we study the problem of binary classification via l2-regularized logistic regression. More formally, we are interested in the following optimization problem:\nmin x\n1\nn n∑ i=1 ( log(1 + exp(yiz > i x)) + λ‖x‖2 ) , (4.1)\nwhere zi ∈ Rd and yi is the corresponding label for each i ∈ [n]. In all our experiments, we set λ = 1/n. Note that such a choice leads to high condition number.\nSince we are interested in sparse datasets, simply taking fi(x) = log(1+exp(yiz>i x))+λ‖x‖2 is not efficient as it requires updating the whole vector x at each iteration. This is due to the regularization term in each of the fi’s. Instead, similar to [20], we rewrite problem in (4.1) as follows:\nmin x\n1\nn n∑ i=1 log(1 + exp(yiz>i x)) + λ ∑ j∈nz(zi) ‖xj‖2 dj  , where nz(z) represents the non-zero components of vector z, and dj = ∑ i 1(j ∈ nz(zi)). While this leads to sparse gradients at each iteration, updates in SVRG are still dense due to the part of the update that contains ∑ i∇fi(αi)/n. This problem can be circumvented by using a ‘just-in-time’\nupdate scheme similar to the one mentioned in [23]. First, recall that for SVRG, ∑ i∇fi(αi)/n does not change during an epoch (see Figure 1). Therefore, during the (k + 1)st epoch we have the following relationship:\nxt = x̃k − η t−1∑ j=km (fij (x j)− fij (x̃k)) − [η(t− km) n n∑ i=1 fi(x̃ k) ] .\nWe maintain each bracketed term separately. The updates to the first term in the above equation are sparse while those to the second term are just a simple scalar addition, since we already maintain the average gradient ∑n i=1 fi(x̃ k)/n. When the gradient of fit at x t is needed, we only calculate components of xt required for fit by aggregating these two terms. Hence, each step of this update procedure can be implemented in a way that respects sparsity of the data.\nWe evaluate the following algorithms for our experiments:\n• Lock-Free SVRG: This is the lock-free asynchronous variant of Algorithm 1 using SVRG schedule; all threads can read and update the parameters with any synchronization. Parameter updates are performed through atomic compare-and-swap instruction facilitated by modern processors [20]. A constant step size that gives the best convergence is chosen for the dataset.\n• Locked SVRG: This is the locked version of the asynchronous variant of Algorithm 1 using SVRG schedule. In particular, we use a concurrent read exclusive write locking model, where all threads can read the parameters but only one threads can update the parameters at a given time. The step size is chosen similar to Lock-Free SVRG.\n• Lock-Free SGD: This is the lock-free asynchronous variant of the SGD algorithm (see [20]). We compare two different versions of this algorithm: (i) SGD with constant step size (referred to as CSGD). (ii) SGD with decaying step size η0 √ σ0/(t+ σ0) (referred to as DSGD), where\nconstants η0 and σ0 specify the scale and speed of decay. For each of these versions, step size is tuned for each dataset to give the best convergence progress.\nAll the algorithms were implemented in C++. The linear algebra operations are mainly performed using eigen32. We run our experiments on datasets from LIBSVM website3. Similar to [27], we normalize each example in the dataset so that ‖zi‖2 = 1 for all i ∈ [n]. Such a normalization leads to an upper bound of 0.25 on the Lipschitz constant of the gradient of fi. The epoch size m is chosen as 2n (as recommended in [9]) in all our experiments. In the first experiment, we compare the speedup achieved by our asynchronous algorithm. To this end, for each dataset we first measure the time required for the algorithm to each an accuracy of 10−10 (i.e., f(x)− f(x∗) < 10−10). The speedup with P threads is defined as the ratio of the runtime with a single thread to the runtime with P threads. Results in Figure 3 show the speedup on various datasets. As seen in the figure, we achieve significant speedups for all the datasets. Not surprisingly, the speedup achieved by Lock-free SVRG is much higher than ones obtained by locking. Furthermore, the lowest speedup is achieved for rcv1 dataset. Similar speedup behavior was reported for this dataset in [20]. It should be noted that this dataset is not sparse and hence, is a bad case for the algorithm (similar to [20]).\nFor the second set of experiments we compare the performance of Lock-Free SVRG with stochastic gradient descent. In particular, we compare with the variants of stochastic gradient descent, DSGD and CSGD, described earlier in this section. It is well established that the performance of variance reduced stochastic methods is better than that of SGD. We would like to empirically verify that such benefits carry over to the asynchronous variants of these algorithms. Figure 4 shows the performance of Lock-Free SVRG, DSGD and CSGD. Since the computation complexity of each epoch of these algorithms is different, we directly plot the objective value versus the runtime for each of these algorithms. We use 10 cores for comparing the algorithms in this experiment. As seen in the figure, Lock-Free SVRG outperforms both DSGD and CSGD. The performance gains are qualitatively similar to those reported in [9] for the synchronous versions of these algorithms. It can also be seen that the DSGD, not surprisingly, outperforms CSGD in all the cases. In our experiments, we observed that Lock-Free SVRG, in comparison to SGD, is relatively much less sensitive to the step size and more robust to increasing threads."
    }, {
      "heading" : "5 Discussion & Future Work",
      "text" : "In this paper, we presented a unifying framework based on [5], that captures many popular variance reduction techniques for stochastic gradient descent. We use this framework to develop a simple hybrid variance reduction method. The primary purpose of the framework, however, was to provide a common platform to analyze various variance reduction techniques. To this end, we provided convergence analysis for the framework under certain conditions. More importantly, we propose an asynchronous algorithm for the framework with provable convergence guarantees. The key consequence of our approach is that we obtain asynchronous variants of several algorithms like SVRG,\n2http://eigen.tuxfamily.org/ 3http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/binary.html\nSAGA and S2GD. Our asynchronous algorithms exploits sparsity in the data to obtain near linear speedup in settings that are typically encountered in machine learning.\nFor future work, it would be interesting to perform an empirical comparison of various schedules. In particular, it would be worth exploring the space-time-accuracy tradeoffs of these schedules. We would also like to analyze the effect of these tradeoffs on the asynchronous variants."
    }, {
      "heading" : "A Appendix",
      "text" : "Notation: We use Df to denote the Bregman divergence (defined below) for function f .\nDf (x, y) = f(x)− f(y)− 〈∇f(y), x− y〉.\nFor ease of exposition, we use E[X] to denote the expectation the random variable X with respect to indices {i1, . . . , it} when X depends on just these indices up to step t. This dependence will be clear from the context. We use 1 to denote the indicator function."
    }, {
      "heading" : "Proof of Theorem 1",
      "text" : "Proof. We expand function f as f(x) = g(x) + h(x) where g(x) = 1n ∑ i∈S fi(x) and g(x) = 1 n ∑ i/∈S fi(x). Let the present epoch be k + 1. We define the following:\nvt = 1\nη (xt+1 − xt) = −\n[ ∇fit(xt)−∇fit(αtit) + 1\nn ∑ i fi(α t i)\n]\nGt = 1\nn ∑ i∈S ( fi(α t i)− fi(x∗)− 〈∇fi(x∗), αti − x∗〉 ) Rt = E [ c‖xt − x∗‖2 +Gt ] .\nWe first observe that E[vt] = −∇f(xt). This follows from the unbiasedness of the gradient at each iteration. Using this observation, we have the following:\nE[Rt+1] = E[c‖xt+1 − x∗‖2 +Gt+1] = E[c‖xt + ηvt − x∗‖2 +Gt+1] = cE [ ‖xt − x∗‖2 ] + cη2E [ ‖vt‖2 ] + 2cηE [ 〈xt − x∗, vt〉 ] + E[Gt+1]\n≤ cE [ ‖xt − x∗‖2 ] + cη2E [ ‖vt‖2 ] − 2cηE [ f(xt)− f(x∗) ] + E[Gt+1]. (A.1)\nThe last step follows from convexity of f and the unbiasedness of vt. We have the following relationship between Gt+1 and Gt.\nE[Gt+1] = (\n1− 1 n\n) E [Gt] + 1\nn E\n[ 1\nn ∑ i∈S ( fi(x\nt)− fi(x∗)− 〈∇fi(x∗), xt − x∗〉 )]\n= ( 1− 1\nn\n) E [Gt] + 1\nn E[Dg(xt, x∗)]. (A.2)\nThis follows from the definition of the schedule of HSAG for indices in S. Substituting the above relationship in Equation (A.1) we get the following.\nRt+1 ≤ Rt + cη2E [ ‖vt‖2 ] − 2cηE [ f(xt)− f(x∗) ] − 1 n E[Gt] + 1 n E[Dg(xt, x∗)]\n≤ (\n1− 1 κ\n) Rt + c\nk E[‖xt − x∗‖2] + cη2E\n[ ‖vt‖2 ] − 2cηE [ f(xt)− f(x∗) ] + ( 1\nκ − 1 n\n) E[Gt] + 1\nn E[Dg(xt, x∗)]\n:= ( 1− 1\nκ\n) Rt + bt.\nWe describe the bounds for bt (defined below).\nbt = c κ E[‖xt − x∗‖2]︸ ︷︷ ︸\nT1\n+cη2 E [ ‖vt‖2 ]︸ ︷︷ ︸ T2 −2cηE [ f(xt)− f(x∗) ] + ( 1\nκ − 1 n\n) E[Gt] + 1\nn E[Dg(xt, x∗)].\nThe terms T1 and T2 can be bounded in the following fashion:\nT1 = E[‖xt − x∗‖2] ≤ 2\nλ E[f(xt)− f(x∗)]\nT2 = E [ ‖vt‖2 ] ≤ ( 1 + 1\nβ\n) E [ ‖∇fit(αtit)−∇fit(x ∗)‖2 ] + (1 + β)E [ ‖∇fit(xt)−∇fit(x∗)‖2 ] ≤ 2L\nn\n( 1 + 1\nβ ) E ∑ i [ fi(α t i)− f(x∗)− 〈 ∇fi(x∗), αti − x∗ 〉] + 2L\nn (1 + β)E ∑ i [ fi(x t)− f(x∗) ]\n≤ 2L ( 1 + 1\nβ\n)[ Gt +Dh(x̃ k, x∗) ] + 2L(1 + β)E[f(xt)− f(x∗)].\nThe bound on T1 is due to strong convexity nature of function f . The first inequality and second inequalities on T2 directly follows from Lemma 3 of [5] and simple application of Lemma 1 respectively. The third inequality follows from the definition of Gt and the fact that αti = x̃\nk for all i /∈ S and t ∈ {km, . . . , km+m}. Substituting these bounds T1 and T2 in bt, we get\nbt ≤ − [ 2cη − 2cLη2(1 + β)− 2c\nκλ\n] E [ f(xt)− f(x∗) ] + ( 1\nκ + 2cLη2\n( 1 + 1\nβ ) − 1 n ) E[Gt] + 1 n E[Dg(xt, x∗)]\n+ 2cLη2 ( 1 + 1\nβ\n)[ Dh(x̃ k, x∗) ]\n≤ − [ 2cη − 2cLη2(1 + β)− 1\nn − 2c κλ\n] E [ f(xt)− f(x∗) ] + ( 1\nκ + 2cLη2\n( 1 + 1\nβ ) − 1 n ) E[Gt] + 2cLη2 ( 1 + 1 β )[ Dh(x̃ k, x∗) ]\n≤ − [ 2cη − 2cLη2(1 + β)− 1\nn − 2c κλ\n] E [ f(xt)− f(x∗) ] + 2cLη2 ( 1 + 1\nβ\n)[ Dh(x̃ k, x∗) ] .\n(A.3)\nThe second inequality follows from Lemma 2. In particular, we use the fact that f(x) − f(x∗) = Df (x, x\n∗) and Df (x, x∗) = Dg(x, x∗) + Dh(x, x∗) ≥ Dg(x, x∗). The third inequality follows from the following for the choice of our parameters:\n1 κ + 2Lcη2\n( 1 + 1\nβ ) ≤ 1 n .\nApplying the recursive relationship on Rt+1 for m iterations, we get\nRkm+m ≤ (\n1− 1 κ\n)m R̃k + m−1∑ j=0 ( 1− 1 κ )m−1−j bk+j\nwhere\nR̃k = E [ c‖x̃k − x∗‖2 + G̃k ] .\nSubstituting the bound on bt from Equation (A.3) in the above equation we get the following inequality:\nRkm+m ≤ (\n1− 1 κ\n)m R̃k+\n+ m−1∑ j=0 ( 2cη(1− Lη(1 + β))− 1 n − 2c κλ )( 1− 1 κ )m−1−j E [ f(xk+j)− f(x∗) ] +\nm−1∑ j=0 ( 1− 1 κ )m−1−j 2Lcη2 ( 1 + 1 β ) E [ h(x̃k)− h(x∗)− 〈∇h(x∗), x̃k − x∗〉 ] .\nWe now use the fact that x̃k+1 is chosen randomly from {xkm, . . . , xkm+m−1} with probabilities proportional to {(1− 1/κ)m−1, . . . , 1} we have the following consequence of the above inequality.\nRkm+m + κ\n[ 1− ( 1− 1\nκ\n)m]( 2cη(1− Lη(1 + β))− 1\nn − 2c κλ\n) E [ f(x̃k+1)− f(x∗) ] ≤ 2c\nλ\n( 1− 1\nκ\n)m E [ f(x̃k)− f(x∗) ] + ( 1− 1\nκ\n)m E [ G̃k ] + 2Lcη2κ [ 1− ( 1− 1\nκ\n)m]( 1 + 1\nβ\n) E [ Dh(x̃ k, x∗) ] .\nFor obtaining the above inequality, we used the strongly convex nature of function f . Again, using the Bregman divergence based inequality (see Lemma 2)\nf(x)− f(x∗) = Df (x, x∗) = Dg(x, x∗) +Dh(x, x∗) ≥ Dh(x, x∗), we have the following inequality\nRkm+m + κ\n[ 1− ( 1− 1\nκ\n)m]( 2cη(1− Lη(1 + β))− 1\nn − 2c κλ\n) E [ f(x̃k+1)− f(x∗) ] ≤ [ 2c\nλ\n( 1− 1\nκ\n)m + 2Lcη2κ [ 1− ( 1− 1\nκ\n)m]] [ f(x̃k)− f(x∗) ] + ( 1− 1\nκ\n)m E [ G̃k ] .\n(A.4)\nWe use the following notation:\nγ = κ [ 1− ( 1− 1\nκ\n)m]( 2cη(1− Lη(1 + β))− 1\nn − 2c κλ ) θ = max {[ 2c\nγλ\n( 1− 1\nκ\n)m + 2Lcη2\nγ\n( 1 + 1\nβ\n) κ [ 1− ( 1− 1\nκ\n)m]] , ( 1− 1\nκ\n)m} .\nUsing the above notation, we have the following inequality from Equation (A.4). E [ f(x̃k+1)− f(x∗) + 1\nγ G̃k+1\n] ≤ θ E [ f(x̃k)− f(x∗) + 1\nγ G̃k\n] ,\nwhere θ < 1 is a constant that depends on the parameters used in the algorithm."
    }, {
      "heading" : "Proof of Theorem 2",
      "text" : "Proof. Let the present epoch be k + 1. Recall that D(t) denotes the iterate used in the tth iteration of the algorithm. We define the following:\nut = − [ ∇fit(xD(t))−∇fit(x̃k) +∇f(x̃k) ] vt = − [ ∇fit(xt)−∇fit(x̃k) +∇f(x̃k) ] .\nWe have the following: E‖xt+1 − x∗‖2 = E‖xt + ηut − x∗‖2 = E [ ‖xt − x∗‖2 + η2‖ut‖2 + 2η〈xt − x∗, ut〉 ] . (A.5)\nWe first bound the last term of the above inequality. We expand the term in the following manner: E〈xt − x∗, ut〉 = E [ 〈x∗ − xt,∇fit(xD(t))〉 ] = E [ 〈x∗ − xD(t),∇fit(xD(t))〉\n] ︸ ︷︷ ︸\nT3\n+ t−1∑ d=D(t) E [ 〈xd − xd+1,∇fit(xd)〉 ] ︸ ︷︷ ︸\nT4\n+ t−1∑ d=D(t) E [ 〈xd − xd+1,∇fit(xD(t))−∇fit(xd)〉 ] ︸ ︷︷ ︸\nT5\n.\n(A.6) The first equality directly follows from the definition of ut and its property of unbiasedness. The second step follows from simple algebraic calculations. Terms T3 and T4 can be bounded in the following way:\nT3 ≤ E[fit(x∗)− fit(xD(t))]. (A.7) This bound directly follows from convexity of function fit .\nT4 = t−1∑ d=D(t) E [ 〈xd − xd+1,∇fit(xd)〉 ] ≤\nt−1∑ d=D(t) E [ fit(x d)− fit(xd+1) + L 2 ‖xd+1 − xd‖2it ]\n≤ E [ fit(x D(t))− fit(xt) ] + L∆\n2 t−1∑ d=D(t) E [ ‖xd+1 − xd‖2 ] . (A.8)\nThe first inequality follows from lipschitz continuous nature of the gradient of function fit . The second inequality follows from the definition of ∆. The last term T5 can be bounded in the following manner.\nT5 = E  t−1∑ d=D(t) 〈xd − xd+1,∇fit(xD(t))−∇fit(xd)〉  ≤ E\n t−1∑ d=D(t) ‖xd+1 − xd‖it‖∇fit(xD(t))−∇fit(xd)‖  ≤ E\n t−1∑ d=D(t) ‖xd+1 − xd‖it d−1∑ j=D(t) ‖∇fit(xj+1)−∇fit(xj)‖  ≤ E\n t−1∑ d=D(t) d−1∑ j=D(t) L 2 ( ‖xd+1 − xd‖2it + ‖x j+1 − xj‖2it )\n≤ L∆(τ − 1) 2\nE t−1∑\nd=D(t)\n‖xd+1 − xd‖2. (A.9)\nThe first inequality follows from Cauchy-Schwartz inequality. The second inequality follows from repeated application of triangle inequality. The third step is a simple application of AM-GM inequality and the fact that gradient of the function fit is lipschitz continuous. Finally, the last step can be obtained from the fact that the staleness in gradient is at most τ and the definition of ∆.\nBy combining the bounds on T3, T4 and T5 in Equations (A.7), (A.8) and (A.9) respectively and substituting the sum in Equation (A.6), we get\nE〈xt − x∗, ut〉 ≤ E f(x∗)− f(xt) + L∆τ 2 t−1∑ d=D(t) ‖xd+1 − xd‖2  . (A.10)\nBy substituting the above inequality in Equation (A.5), we get\nE [ ‖xt+1 − x∗‖2 ] ≤ E [ ‖xt − x∗‖2 + η2‖ut‖2 − 2η(f(xt)− f(x∗)) + L∆τη3\nt−1∑ d=D(t)\n‖ud‖2 ] .\n(A.11) We next bound the term E[‖ut‖2] in terms of E [ ‖vt‖2 ] in the following way:\nE [ ‖ut‖2 ] ≤ 2E [ ‖ut − vt‖2 + ‖vt‖2 ] ≤ 2E [ ‖∇fit(xt)−∇fit(xD(t))‖2 ] + 2E\n∥∥vt‖2] ≤ 2L2E [ ‖xd+1 − xd‖2it ] + 2E [ ‖vt‖2\n] ≤ 2L2τ\nt−1∑ d=D(t) E [ ‖xt − xD(t)‖2it ] + 2E [ ‖vt‖2 ] ≤ 2L2∆η2τ\nt−1∑ d=D(t) E [ ‖ud‖2 ] + 2E [ ‖vt‖2 ] .\nThe first step follows from AM-GM inequality. The second inequality follows from the lipschitz continuous nature of the gradient. The third step follows from simple application of CauchySchwartz inequality. Adding the above inequalities from t = km to t = km+m− 1, we get\nkm+m−1∑ t=km E [ ‖ut‖2 ] ≤ km+m−1∑ t=km 2L2∆η2τ t−1∑ d=D(t) E [ ‖ud‖2 ] + 2E [ ‖vt‖2 ] ≤ 2L2∆η2τ2\nkm+m−1∑ t=km E [ ‖ut‖2 ] + 2 km+m−1∑ t=km E [ ‖vt‖2 ] .\nHere we again used the fact that the delay in the gradients is at most τ . From the above inequality, we get\nkm+m−1∑ t=km E [ ‖ut‖2 ] ≤ 2 (1− 2L2∆η2τ2) km+m−1∑ t=km E [ ‖vt‖2 ] . (A.12)\nAdding Equation (A.11) from t = km to t = km + m − 1 and substituting Equation (A.12) in the resultant, we get E [ ‖xkm+m − x∗‖2 ] ≤ E [ ‖x̃k − x∗‖2 + (η2 + L∆τ2η3)\nkm+m−1∑ t=km ‖ut‖2 − km+m−1∑ t=km 2η(f(xt)− f(x∗))\n]\n≤ E [ ‖x̃k − x∗‖2 + 2 ( η2 + L∆τ2η3\n1− 2L2∆η2τ2 ) km+m−1∑ t=km ‖vt‖2 − km+m−1∑ t=km 2η(f(xt)− f(x∗)) ] .\nThe first step follows from telescopy sum and the definition of x̃k. From Lemma 3 of [5] (also see [9]), we have E[‖vt‖2] ≤ 4LE [ f(xt)− f(x∗) + f(x̃k)− f(x∗) ] .\nSubstituting this in the inequality above, we get the following bound:( 2η − 8L ( η2 + L∆τ2η3\n1− 2L2∆η2τ2\n)) mE[f(x̃k+1)− f(x∗)]\n≤ ( 2\nµ + 8L\n( η2 + L∆τ2η3\n1− 2L2∆η2τ2\n) m ) E[f(x̃k)− f(x∗)]."
    }, {
      "heading" : "Proof of Theorem 3",
      "text" : "Proof. Let the present epoch be k+1. For simplicity, we assume that the iterates x andA used in the each iteration are from the same time step (index) i.e., D(t) = D′(t) for all t ∈ T . Recall that D(t) and D′(t) denote the index used in the tth iteration of the algorithm. Our analysis can be extended to the case ofD(t) 6= D′(t) in a straightforward manner. We expand function f as f(x) = g(x)+h(x) where g(x) = 1n ∑ i∈S fi(x) and g(x) = 1 n ∑ i/∈S fi(x). We define the following:\nut = 1\nη (xt+1 − xt) = −\n[ ∇fit(xD(t))−∇fit(α D(t) it ) + 1\nn ∑ i fi(α D(t) i )\n]\nvt = 1\nη (xt+1 − xt) = −\n[ ∇fit(xt)−∇fit(αtit) + 1\nn ∑ i fi(α t i)\n] .\nWe use the same Lyapunov function used in Theorem 1. We recall the following definitions:\nGt = 1\nn ∑ i∈S ( fi(α t i)− fi(x∗)− 〈∇fi(x∗), αti − x∗〉 ) Rt = E [ c‖xt − x∗‖2 +Gt ] .\nUsing unbiasedness of the gradient we have E[ut] = −∇f(xD(t)) and E[vt] = −∇f(xt). Using this observation, we have the following:\ncE[‖xt+1 − x∗‖2] = cE[‖xt + ηut − x∗‖2] = cE [ ‖xt − x∗‖2 ] + cη2 E [ ‖ut‖2 ]︸ ︷︷ ︸ T6 +2cη E [ 〈xt − x∗, ut〉 ]︸ ︷︷ ︸ T7 . (A.13)\nThe last step follows from convexity of f and the unbiasedness of vt. We further bound term T6 in the following manner:\nT6 = E [ ‖ut‖2 ] ≤ 2E [ ‖ut − vt‖2 ] + 2E[‖vt‖2]. (A.14)\nThe first term can be bounded in the following manner: E [ ‖ut − vt‖2 ] ≤ E [∥∥∥(∇fit(xt)−∇fit(xD(t)))− (∇fit(αD(t)it )−∇fit(αtit)) + 1\nn ∑ i (∇fi(αti)−∇fi(α D(t) i )) ∥∥∥2] ≤ 3E [∥∥∥∇fit(xt)−∇fit(xD(t))∥∥∥2]+ 3E [∥∥∥∇fit(αD(t)it )−∇fit(αtit)∥∥∥2]\n+ 3E ∥∥∥∥∥ 1n∑ i (∇fi(αti)−∇fi(α D(t) i )) ∥∥∥∥∥ 2 \n≤ 3E [∥∥∥∇fit(xt)−∇fit(xD(t))∥∥∥2]+ 3E [∥∥∥∇fit(αD(t)it )−∇fit(αtit)∥∥∥2]\n+ 3\nn ∑ i E [∥∥∥∇fi(αti)−∇fi(αD(t)i )∥∥∥2] . (A.15)\nThe second step follows from the inequality (a+ b+ c)2 ≤ 3(a2 + b2 + c2). The last step follows from simple application of Jensen’s inequality. The first term can be bounded easily in the following manner:\nE [ ‖∇fit(xt)−∇fit(xD(t))‖2 ] ≤ L2τ t−1∑ d=D(t) E [ ‖xd+1 − xd‖2it ] ≤ L2∆η2τ\nt−1∑ d=D(t) E [ ‖ud‖2 ] .\nThe second and third terms need more delicate analysis. The key insight for our analysis is that at most τ αi’s differ from time step D(t) to t. This is due to the fact that the delay is bounded by τ and at most one αi changes at each iteration. Furthermore, whenever there is a change in αi, it changes to one of the iterates xj for some j = {t− τ, . . . , t}. With this intuition we bound the second term in the following fashion.\nE [∥∥∥∇fit(αD(t)i )−∇fit(αtit)∥∥∥2] ≤ 1n t−1∑ j=D(t) ∑ i∈S E [ 1(i = ij) ∥∥∥∇fi(xj)−∇fi(αD(t)i )∥∥∥2]\n≤ 2 n t−1∑ j=D(t) ∑ i∈S E [ 1(i = ij) (∥∥∇fi(xj)−∇fi(x∗)∥∥2 + ∥∥∥∇fi(αD(t)i )−∇fi(x∗)∥∥∥2)]\n≤ 2 n2 t−1∑ j=D(t) ∑ i∈S E [∥∥∇fi(xj)−∇fi(x∗)∥∥2]+ 2 n2 t−1∑ j=D(t) ∑ i∈S E [∥∥∥∇fi(αD(t)i )−∇fi(x∗)∥∥∥2]\n≤ 4L n t−1∑ j=D(t) E\n[ 1\nn ∑ i∈S fi(x j)− fi(x∗)− 〈∇fi(x∗), xj − x∗〉)\n]\n+ 4Lτ\nn E\n[ 1\nn ∑ i∈S fi(α D(t) i )− fi(x ∗)− 〈∇fi(x∗), αD(t)i − x ∗〉\n] .\nThe second inequality follows from the fact that ‖a + b‖2 ≤ 2‖a‖2 + 2‖b‖2. The last step directly follows from Lemma 1. Note that sum is over indices in S since αi’s for i /∈ S do not change during the epoch.\nThe third term in Equation A.15 can be bounded by exactly the same technique we used for the second term. The bound, in fact, turns out to identical to second term since it is chosen uniformly random. Combining all the terms we have\nT6 ≤ 2E[‖vt‖2] + 6L2∆η2τ t−1∑\nd=D(t)\nE [ ‖ud‖2 ] + 48L\nn t−1∑ j=D(t) E [ Dg(x j , x∗) ] + 48Lτ n E [ GD(t) ] .\nThe term T7 can be bounded in a manner similar to one in Theorem 2 to obtain the following (see proof of Theorem 2 for details):\nE〈xt − x∗, ut〉 ≤ E f(x∗)− f(xt) + L∆τη2 2 t−1∑ d=D(t) ‖ud‖2  . (A.16)\nWe need the following bound for our analysis: m−1∑ j=0 ( 1− 1 κ )m−1−j E[‖ukm+j‖2] ≤ 2 m−1∑ j=0 ( 1− 1 κ )m−1−j E[‖vkm+j‖2]\n+ km+m−1∑ t=km 6L2∆η2τ t−1∑ d=D(t) E [ ‖ud‖2 ] +\nkm+m−1∑ t=km 48L n t−1∑ j=D(t) E [ Dg(x j , x∗) ]\n+ km+m−1∑ t=km 48Lτ n E [ GD(t) ] .\nThe above inequality follows directly from the bound on T6. Under the condition η2 ≤ (\n1− 1 κ\n)m−1 1\n12L2∆τ2 .\nwe have the following inequality m−1∑ j=0 ( 1− 1 κ )m−1−j E[‖ukm+j‖2] ≤ 4 m−1∑ j=0 ( 1− 1 κ )m−1−j E[‖vkm+j‖2]\n+ km+m−1∑ t=km 96L n t−1∑ j=D(t) E [ Dg(x j , x∗) ]\n+ km+m−1∑ t=km 96Lτ n E [ GD(t) ] . (A.17)\nThis follows from that fact that km+m−1∑ t=km 6L2∆η2τ t−1∑ d=D(t) E [ ‖ud‖2 ] ≤ 1 2 m−1∑ j=0 ( 1− 1 κ )m−1−j E[‖ukm+j‖2].\nunder the condition mentioned above. We have the following: Rt+1 = cE [ ‖xt − x∗‖2 ] + cη2E [ ‖ut‖2 ] + 2cηE [ 〈xt − x∗, ut〉 ] + E [Gt+1]\n:= ( 1− 1\nκ\n) Rt + et. (A.18)\nWe bound et in the following manner:\net = c κ ‖xt − x∗‖2 + 1 κ E[Gt] + cη2E\n[ ‖ut‖2 ] + 2cηE [ 〈xt − x∗, ut〉 ] + E [Gt+1]\n= c\nκ ‖xt − x∗‖2 +\n( 1\nκ − 1 n\n) E[Gt] + cη2E [ ‖ut‖2 ] + 2cηE [ 〈xt − x∗, ut〉 ] + 1\nn E[Dg(xt, x∗)]\n≤ − (\n2cη − 2c κλ\n) E [ f(xt)− f(x∗) ] + ( 1\nκ − 1 n\n) E[Gt] + cη2E[‖ut‖2]\n+ cL∆τη3 t−1∑\nd=D(t)\nE [ ‖ud‖2 ] + 1\nn E[Dg(xt, x∗)].\nThe second equality follows from the definition of Gt+1 (see Equation (A.2)). E[Gt+1] = (\n1− 1 n\n) E [Gt] + 1\nn E[Dg(xt, x∗)].\nApplying the recurrence relationship in Equation (A.18) with the derived bound on et, we have Rkm+m ≤ (\n1− 1 κ\n)m R̃k + m−1∑ j=0 ( 1− 1 κ )m−1−j ekm+j\n≤ (\n1− 1 κ\n)m R̃k + m−1∑ j=0 ( 1− 1 κ )m−1−j ekm+j\n≤ (\n1− 1 κ\n)m R̃k + m−1∑ j=0 ( 1− 1 κ )m−1−j e′km+j ,\nwhere e′t is defined as follows R̃k = E [ c‖x̃k − x∗‖2 + G̃k ] e′t = − ( 2cη − 2c\nκλ\n) E [ f(xt)− f(x∗) ] + ( 1\nκ − 1 n\n) E[Gt]\n+ ( cη2 + ( 1− 1\nκ\n)−τ cL∆τ2η3 ) E[‖ut‖2] + 1\nn E[Dg(xt, x∗)].\nWe use the following notation for ease of exposition:\nζ = ( cη2 + ( 1− 1\nκ\n)−τ cL∆τ2η3 ) .\nThis last inequality follows from that fact that the delay is at most τ . In particular, each index j ∈ {D(t) . . . , t} for at most τ times. Substituting the bound in Equation (A.17), we get the following:\nRkm+m ≤ (\n1− 1 κ\n)m R̃k − ( 2cη − 2c\nκλ )m−1∑ j=0 ( 1− 1 κ )m−1−j E [ f(xkm+j)− f(x∗) ] + 4ζ\nm−1∑ j=0 ( 1− 1 κ )m−1−j E[‖vkm+j‖2]\n+\n[ 96ζLτ\nn\n( 1− 1\nκ\n)−τ + 1\nn ] m−1∑ j=0 ( 1− 1 κ )m−1−j E [ Dg(x km+j , x∗) ]\n+\n[ 1\nκ +\n96ζLτ\nn\n( 1− 1\nκ )−τ − 1 n ] m−1∑ j=0 ( 1− 1 κ )m−1−j E [ GD(km+j) ] . (A.19)\nWe now use the following previously used bound on vt (see bound T2 in the proof of Theorem 1). E[‖vt‖2] ≤ 2L ( 1 + 1\nβ\n)[ Gt +Dh(x̃ k, x∗) ] + 2L(1 + β)E[f(xt)− f(x∗)].\nSubstituting the above bound on vt in Equation (A.19), we get the following: Rkm+m ≤ (\n1− 1 κ\n)m R̃k\n− [ 2cη − 8ζL(1 + β)− 2c\nκλ − 96ζLτ n\n( 1− 1\nκ )−τ − 1 n ] ×\nm−1∑ j=0 ( 1− 1 κ )m−1−j E [ f(xkm+j)− f(x∗) ] + [ 1\nκ + 8ζL\n( 1 + 1\nβ\n) + 96ζLτ\nn\n( 1− 1\nκ )−τ − 1 n ] ×\nm−1∑ j=0 ( 1− 1 κ )m−1−j E [ GD(km+j) ] + 8ζL ( 1 + 1\nβ )m−1∑ j=0 ( 1− 1 κ )m−1−j E [ Dh(x̃ k, x∗) ]\n≤ 2c λ\n( 1− 1\nκ\n)m E [ f(x̃k)− f(x∗) ] + ( 1− 1\nκ\n)m E [ G̃k ] − [ 2cη − 8ζL(1 + β)− 2c\nκλ − 96ζLτ n\n( 1− 1\nκ )−τ − 1 n ] ×\nm−1∑ j=0 ( 1− 1 κ )m−1−j E [ f(xkm+j)− f(x∗) ] + 8ζL ( 1 + 1\nβ\n) κ [ 1− ( 1− 1\nκ\n)m] E [ Dh(x̃ k, x∗) ] . (A.20)\nThe first step is due to the Bregman divergence based inequality Df (x, x∗) ≥ Dg(x, x∗). The second step follows from the expanding R̃k and using the strong convexity of function f . We now use the fact that x̃k+1 is chosen randomly from {xkm, . . . , xkm+m−1} with probabilities proportional to {(1 − 1/κ)m−1, . . . , 1} we have the following consequence of the above inequality. We use the following notation:\nγa = κ\n[ 1− ( 1− 1\nκ\n)m][ 2cη − 8ζL(1 + β)− 2c\nκλ − 96ζLτ n\n( 1− 1\nκ )−τ − 1 n ]\nθa = max   2c γaλ ( 1− 1 κ )m + 8ζL ( 1 + 1β ) γa κ [ 1− ( 1− 1 κ )m] ,(1− 1 κ )m . Using the above notation, we have the following inequality from Equation (A.4).\nE [ f(x̃k+1)− f(x∗) + 1\nγa G̃k+1\n] ≤ θa E [ f(x̃k)− f(x∗) + 1\nγa G̃k\n] .\nwhere θ < 1 is a constant that depends on the parameters used in the algorithm."
    }, {
      "heading" : "Other Lemmatta",
      "text" : "Lemma 1. [9] For any αi ∈ Rd where i ∈ [n] and x∗, we have\nE [ ‖∇fit(αit)−∇fit(x∗)‖2 ] ≤ 2L\nn ∑ i [fi(αi)− f(x∗)− 〈∇fi(x∗), αi − x∗〉] .\nLemma 2. Suppose f : Rd → R and f = g+ h where f, g and h are convex and differentiable. x∗ is the optimal solution to arg minx f(x) then we have the following\nDf (x, x ∗) = f(x)− f(x∗) Df (x, x ∗) = Dg(x, x ∗) +Dh(x, x ∗) Df (x, x ∗) ≥ Dg(x, x∗).\nProof. The proof follows trivially from the fact that x∗ is the optimal solution and linearity and non-negative properties of Bregman divergence."
    } ],
    "references" : [ {
      "title" : "A lower bound for the optimization of finite sums",
      "author" : [ "Alekh Agarwal", "Leon Bottou" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Distributed delayed stochastic optimization",
      "author" : [ "Alekh Agarwal", "John C Duchi" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Incremental gradient, subgradient, and proximal methods for convex optimization: A survey",
      "author" : [ "Dimitri P Bertsekas" ],
      "venue" : "Optimization for Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "New Optimization Methods for Machine Learning",
      "author" : [ "Aaron Defazio" ],
      "venue" : "PhD thesis, Australian National University,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien" ],
      "venue" : "In NIPS",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Finito: A faster, permutable incremental gradient method for big data problems",
      "author" : [ "Aaron J Defazio", "Tibério S Caetano", "Justin Domke" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Optimal distributed online prediction using mini-batches",
      "author" : [ "Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "A globally convergent incremental Newton method",
      "author" : [ "M. Gürbüzbalaban", "A. Ozdaglar", "P. Parrilo" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In NIPS",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting",
      "author" : [ "Jakub Konečný", "Jie Liu", "Peter Richtárik", "Martin Takáč" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Communication Efficient Distributed Machine Learning with the Parameter Server",
      "author" : [ "Mu Li", "David G Andersen", "Alex J Smola", "Kai Yu" ],
      "venue" : "In NIPS",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Asynchronous stochastic coordinate descent: Parallelism and convergence properties",
      "author" : [ "Ji Liu", "Stephen J. Wright" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "An asynchronous parallel stochastic coordinate descent algorithm",
      "author" : [ "Ji Liu", "Steve Wright", "Christopher Ré", "Victor Bittorf", "Srikrishna Sridhar" ],
      "venue" : "ICML",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Optimization with first-order surrogate functions",
      "author" : [ "Julien Mairal" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Distributed asynchronous incremental subgradient methods",
      "author" : [ "A Nedić", "Dimitri P Bertsekas", "Vivek S Borkar" ],
      "venue" : "Studies in Computational Mathematics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2001
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization problems",
      "author" : [ "Yu Nesterov" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Stochastic Proximal Gradient Descent with Acceleration Techniques",
      "author" : [ "Atsushi Nitanda" ],
      "venue" : "In NIPS",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent",
      "author" : [ "Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
      "author" : [ "Peter Richtárik", "Martin Takáč" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "H. Robbins", "S. Monro" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1951
    }, {
      "title" : "Minimizing Finite Sums with the Stochastic Average Gradient",
      "author" : [ "Mark W. Schmidt", "Nicolas Le Roux", "Francis R. Bach" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "Accelerated mini-batch stochastic dual coordinate ascent",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "In NIPS",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "On distributed stochastic optimization and learning",
      "author" : [ "Ohad Shamir", "Nathan Srebro" ],
      "venue" : "In Proceedings of the 52nd Annual Allerton Conference on Communication, Control, and Computing,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "Lin Xiao", "Tong Zhang" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "There has been a steep rise in recent work [5, 6, 8–11, 23, 25, 27] on “variance reduced” stochastic gradient algorithms for convex problems of the finite-sum form:",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 5,
      "context" : "There has been a steep rise in recent work [5, 6, 8–11, 23, 25, 27] on “variance reduced” stochastic gradient algorithms for convex problems of the finite-sum form:",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "There has been a steep rise in recent work [5, 6, 8–11, 23, 25, 27] on “variance reduced” stochastic gradient algorithms for convex problems of the finite-sum form:",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 8,
      "context" : "There has been a steep rise in recent work [5, 6, 8–11, 23, 25, 27] on “variance reduced” stochastic gradient algorithms for convex problems of the finite-sum form:",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "There has been a steep rise in recent work [5, 6, 8–11, 23, 25, 27] on “variance reduced” stochastic gradient algorithms for convex problems of the finite-sum form:",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "There has been a steep rise in recent work [5, 6, 8–11, 23, 25, 27] on “variance reduced” stochastic gradient algorithms for convex problems of the finite-sum form:",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 23,
      "context" : "There has been a steep rise in recent work [5, 6, 8–11, 23, 25, 27] on “variance reduced” stochastic gradient algorithms for convex problems of the finite-sum form:",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 25,
      "context" : "There has been a steep rise in recent work [5, 6, 8–11, 23, 25, 27] on “variance reduced” stochastic gradient algorithms for convex problems of the finite-sum form:",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "Under strong convexity assumptions such variance reduced (VR) stochastic algorithms attain better convergence rates (in expectation) than stochastic gradient descent (SGD) [17, 22], both in theory and practice.",
      "startOffset" : 172,
      "endOffset" : 180
    }, {
      "referenceID" : 20,
      "context" : "Under strong convexity assumptions such variance reduced (VR) stochastic algorithms attain better convergence rates (in expectation) than stochastic gradient descent (SGD) [17, 22], both in theory and practice.",
      "startOffset" : 172,
      "endOffset" : 180
    }, {
      "referenceID" : 1,
      "context" : "And in this setting, asynchronous variants of SGD remain indispensable [2, 7, 12, 20, 26, 28].",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "And in this setting, asynchronous variants of SGD remain indispensable [2, 7, 12, 20, 26, 28].",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "And in this setting, asynchronous variants of SGD remain indispensable [2, 7, 12, 20, 26, 28].",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 18,
      "context" : "And in this setting, asynchronous variants of SGD remain indispensable [2, 7, 12, 20, 26, 28].",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 24,
      "context" : "And in this setting, asynchronous variants of SGD remain indispensable [2, 7, 12, 20, 26, 28].",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "Our methods are inspired by the influential SVRG [9], S2GD [11], SAG [23] and SAGA [5] family of algorithms.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 21,
      "context" : "Our methods are inspired by the influential SVRG [9], S2GD [11], SAG [23] and SAGA [5] family of algorithms.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "Our methods are inspired by the influential SVRG [9], S2GD [11], SAG [23] and SAGA [5] family of algorithms.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "Our paper has two core components: (i) a formal general framework for variance reduced stochastic methods based on discussions in [5]; and (ii) asynchronous parallel VR algorithms within the framework.",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 21,
      "context" : "As already mentioned, our work is closest to (and generalizes) SAG [23], SAGA [5], SVRG [9] and S2GD [11], which are primal methods.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "As already mentioned, our work is closest to (and generalizes) SAG [23], SAGA [5], SVRG [9] and S2GD [11], which are primal methods.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "As already mentioned, our work is closest to (and generalizes) SAG [23], SAGA [5], SVRG [9] and S2GD [11], which are primal methods.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 23,
      "context" : "Also closely related are dual methods such as SDCA [25] and Finito [6], and in its convex incarnation MISO [15]; a more precise relation between these dual methods and VR stochastic methods is described in Defazio’s thesis [4].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "Also closely related are dual methods such as SDCA [25] and Finito [6], and in its convex incarnation MISO [15]; a more precise relation between these dual methods and VR stochastic methods is described in Defazio’s thesis [4].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "Also closely related are dual methods such as SDCA [25] and Finito [6], and in its convex incarnation MISO [15]; a more precise relation between these dual methods and VR stochastic methods is described in Defazio’s thesis [4].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "Also closely related are dual methods such as SDCA [25] and Finito [6], and in its convex incarnation MISO [15]; a more precise relation between these dual methods and VR stochastic methods is described in Defazio’s thesis [4].",
      "startOffset" : 223,
      "endOffset" : 226
    }, {
      "referenceID" : 2,
      "context" : "By their algorithmic structure, these VR methods trace back to classical non-stochastic incremental gradient algorithms [3], but by now it is well-recognized that randomization helps obtain much sharper convergence results (in expectation).",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 25,
      "context" : "Proximal [27] and accelerated VR methods have also been proposed [19, 24]; we leave a study of such variants of our framework as future work.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 17,
      "context" : "Proximal [27] and accelerated VR methods have also been proposed [19, 24]; we leave a study of such variants of our framework as future work.",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 22,
      "context" : "Proximal [27] and accelerated VR methods have also been proposed [19, 24]; we leave a study of such variants of our framework as future work.",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "Finally, there is recent work on lower-bounds for finite-sum problems [1].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 18,
      "context" : "Within asynchronous SGD algorithms, both parallel [20] and distributed [2, 16] variants are known.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "Within asynchronous SGD algorithms, both parallel [20] and distributed [2, 16] variants are known.",
      "startOffset" : 71,
      "endOffset" : 78
    }, {
      "referenceID" : 14,
      "context" : "Within asynchronous SGD algorithms, both parallel [20] and distributed [2, 16] variants are known.",
      "startOffset" : 71,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "A different line of methods is that of (primal) coordinate descent methods, and their parallel and distributed variants [13, 14, 18, 21].",
      "startOffset" : 120,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "A different line of methods is that of (primal) coordinate descent methods, and their parallel and distributed variants [13, 14, 18, 21].",
      "startOffset" : 120,
      "endOffset" : 136
    }, {
      "referenceID" : 16,
      "context" : "A different line of methods is that of (primal) coordinate descent methods, and their parallel and distributed variants [13, 14, 18, 21].",
      "startOffset" : 120,
      "endOffset" : 136
    }, {
      "referenceID" : 19,
      "context" : "A different line of methods is that of (primal) coordinate descent methods, and their parallel and distributed variants [13, 14, 18, 21].",
      "startOffset" : 120,
      "endOffset" : 136
    }, {
      "referenceID" : 9,
      "context" : "Finally, the recent work [10] generalizes S2GD to the mini-batch setting, thereby also permitting parallel processing, albeit with more synchronization and allowing only small mini-batches.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 25,
      "context" : "1) While our analysis focuses on strongly convex functions, we can extend it to just smooth convex functions along the lines of [27].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "Inspired by the discussion on a general view of variance reduced techniques in [5], we now describe a formal general framework for variance reduction in stochastic gradient descent.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 21,
      "context" : "In particular, we consider incremental methods SAG [23], SVRG [9] and SAGA [5], and classic gradient descent GRADIENTDESCENT for demonstrating our framework.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "In particular, we consider incremental methods SAG [23], SVRG [9] and SAGA [5], and classic gradient descent GRADIENTDESCENT for demonstrating our framework.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "In particular, we consider incremental methods SAG [23], SVRG [9] and SAGA [5], and classic gradient descent GRADIENTDESCENT for demonstrating our framework.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : "In case of SVRG, SCHEDULEUPDATE is triggered every m iterations (here m denotes precisely the number of inner iterations used in [9]); so A remains unchanged for the m iterations and all α i are updated to the current iterate at the mth iteration.",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "Similar to [9] (see Option II of SVRG in [9]), our analysis will be for the case where the iterate at the end of (k + 1)st epoch, x, is replaced with an element chosen randomly from {x, .",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 8,
      "context" : "Similar to [9] (see Option II of SVRG in [9]), our analysis will be for the case where the iterate at the end of (k + 1)st epoch, x, is replaced with an element chosen randomly from {x, .",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "5) with m = O(n) epoch size (similar to [5, 9]).",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "5) with m = O(n) epoch size (similar to [5, 9]).",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 18,
      "context" : "Our model of computation is similar to the ones used in Hogwild! [20] and AsySCD [14].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 12,
      "context" : "Our model of computation is similar to the ones used in Hogwild! [20] and AsySCD [14].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : ", [14, 20]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 18,
      "context" : ", [14, 20]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 18,
      "context" : "1 Convergence Analysis The key ingredients to the success of asynchronous algorithms for multicore stochastic gradient descent are sparsity and “disjointness” of the data matrix [20].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 4,
      "context" : "For example, in the case of SAGA, one can obtain per iteration convergence guarantees (see [5]) rather than those corresponding to per epoch presented in the paper.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "Furthermore, in this case, our analysis for both synchronous and asynchronous cases can be easily modified to obtain convergence properties similar to the ones obtained in [5].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 18,
      "context" : "Instead, similar to [20], we rewrite problem in (4.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 21,
      "context" : "This problem can be circumvented by using a ‘just-in-time’ update scheme similar to the one mentioned in [23].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 18,
      "context" : "Parameter updates are performed through atomic compare-and-swap instruction facilitated by modern processors [20].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 18,
      "context" : "• Lock-Free SGD: This is the lock-free asynchronous variant of the SGD algorithm (see [20]).",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 25,
      "context" : "Similar to [27], we normalize each example in the dataset so that ‖zi‖2 = 1 for all i ∈ [n].",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 8,
      "context" : "The epoch size m is chosen as 2n (as recommended in [9]) in all our experiments.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 18,
      "context" : "Similar speedup behavior was reported for this dataset in [20].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "It should be noted that this dataset is not sparse and hence, is a bad case for the algorithm (similar to [20]).",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 8,
      "context" : "The performance gains are qualitatively similar to those reported in [9] for the synchronous versions of these algorithms.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "In this paper, we presented a unifying framework based on [5], that captures many popular variance reduction techniques for stochastic gradient descent.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "Bibliography [1] Alekh Agarwal and Leon Bottou.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 1,
      "context" : "[2] Alekh Agarwal and John C Duchi.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Dimitri P Bertsekas.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Aaron Defazio.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Aaron J Defazio, Tibério S Caetano, and Justin Domke.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Rie Johnson and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Jakub Konečný, Jie Liu, Peter Richtárik, and Martin Takáč.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[12] Mu Li, David G Andersen, Alex J Smola, and Kai Yu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[13] Ji Liu and Stephen J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[14] Ji Liu, Steve Wright, Christopher Ré, Victor Bittorf, and Srikrishna Sridhar.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[15] Julien Mairal.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[16] A Nedić, Dimitri P Bertsekas, and Vivek S Borkar.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[17] A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[18] Yu Nesterov.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[19] Atsushi Nitanda.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[20] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[21] Peter Richtárik and Martin Takáč.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[22] H.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[23] Mark W.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[24] Shai Shalev-Shwartz and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[25] Shai Shalev-Shwartz and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[26] Ohad Shamir and Nathan Srebro.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[27] Lin Xiao and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "The first inequality and second inequalities on T2 directly follows from Lemma 3 of [5] and simple application of Lemma 1 respectively.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "From Lemma 3 of [5] (also see [9]), we have E[‖v‖] ≤ 4LE [ f(x)− f(x∗) + f(x̃)− f(x∗) ] .",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "From Lemma 3 of [5] (also see [9]), we have E[‖v‖] ≤ 4LE [ f(x)− f(x∗) + f(x̃)− f(x∗) ] .",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : "[9] For any αi ∈ R where i ∈ [n] and x∗, we have E [ ‖∇fit(αit)−∇fit(x)‖ ] ≤ 2L n ∑",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2015,
    "abstractText" : "We study optimization algorithms based on variance reduction for stochastic gradient descent (SGD). Remarkable recent progress has been made in this direction through development of algorithms like SAG, SVRG, SAGA. These algorithms have been shown to outperform SGD, both theoretically and empirically. However, asynchronous versions of these algorithms—a crucial requirement for modern large-scale applications—have not been studied. We bridge this gap by presenting a unifying framework for many variance reduction techniques. Subsequently, we propose an asynchronous algorithm grounded in our framework, and prove its fast convergence. An important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as SVRG and SAGA as a byproduct. Our method achieves near linear speedup in sparse settings common to machine learning. We demonstrate the empirical performance of our method through a concrete realization of asynchronous SVRG.",
    "creator" : "LaTeX with hyperref package"
  }
}