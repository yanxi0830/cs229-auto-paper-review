{
  "name" : "1206.6402.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Parallelizing Exploration–Exploitation Tradeoffs with Gaussian Process Bandit Optimization",
    "authors" : [ "Thomas Desautels", "Andreas Krause" ],
    "emails" : [ "tadesaut@caltech.edu", "krausea@ethz.ch", "jwb@robotics.caltech.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Many applications, from recommender systems to optimal control to experimental design, require solving exploration–exploitation tradeoffs: one needs to make\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\na sequence of decisions with uncertain outcomes and thus, based on noisy feedback, one wishes to simultaneously learn a model and use that model to maximize the reward obtained.\nOften, the set of possible decisions is large or infinite, and therefore we must be able to generalize from partial observations to predict the likely reward associated with unexplored decisions. A second crucial challenge is that we wish to explore many possible decisions in parallel: in information retrieval, it may not be possible to update the predictive model in real-time, but perhaps once per day, taking into account all the feedback collected; in experimental design, we may wish to design batches of simultaneously running experiments, only incorporating feedback once all experiments terminate; and in complex control tasks, performance feedback may become available only after a delay.\nThis paper tackles these two central challenges arising when solving large-scale exploration–exploitation tradeoffs. We model the problem as a stochastic multiarmed bandit problem, where the unknown mean payoff function is modeled as a Gaussian process (GP, Rasmussen & Williams (2006)). As nonparametric statistical models, GPs can flexibly incorporate a variety of assumptions about regularity of the payoff function via its covariance (or kernel) function. We design an efficient algorithm, GP-BUCB, that is able to handle both the parallel exploration problem (where we propose batches of B experiments executed concurrently) and delayed feedback (where each decision can only use feedback up to B rounds ago). Our approach generalizes the GP-UCB approach (Srinivas et al., 2010) to the parallel setting. We prove bounds on the cumulative regret incurred by GP-BUCB. We show that, perhaps surprisingly, near-linear speedup is possible\nfor many commonly used kernel functions: as long as the batch size B grows at most polylogarithmically in the number of rounds T , the GP-BUCB regret bounds only increase by a constant factor independent of B as compared to the known bounds for the sequential algorithm. We also demonstrate how the GP-BUCB algorithm can be drastically accelerated by using lazy evaluations. We evaluate our approach on several synthetic benchmark optimization tasks, as well as two real data sets, respectively related to automated vaccine design and therapeutic spinal cord stimulation.\nRelated Work Classical work on multi-armed bandit problems has focused on the case of a finite number of decisions (Robbins, 1952). Optimistic allocation according to upper-confidence bounds (UCB) on the payoffs has proven to be particularly effective (Auer et al., 2002). Recently, approaches for coping with large (or infinite) sets of decisions have been developed. In these cases, dependence between the payoffs associated with different decisions must be modeled and exploited. Examples include bandits with linear (Dani et al., 2008; Abernethy et al., 2008) or Lipschitz-continous payoffs (Kleinberg et al., 2008), or bandits on trees (Kocsis & Szepesvári, 2006; Bubeck et al., 2008). The exploration-exploitation tradeoff has also been studied in Bayesian global optimization and response surface modeling, where Gaussian process models are often used due to their flexibility in incorporating prior assumptions about the payoff function (Brochu et al., 2009). Several heuristics, such as Maximum Expected Improvement (Jones et al., 1998), Maximum Probability of Improvement (Mockus, 1989), and upperconfidence based methods (Cox & John, 1997), have been developed to balance exploration with exploitation and successfully applied in learning problems (Lizotte et al., 2007). Recently, Srinivas et al. (2010) analyzed GP-UCB, an upper-confidence bound sampling based algorithm for this setting, and proved bounds on its cumulative regret, and thus convergence rates for Bayesian global optimization. We build on this foundation and generalize it to the parallel setting.\nTo enable parallel selection, one must account for the lag between decisions and observations. Most existing approaches that can deal with such delay result in a multiplicative increase in the cumulative regret as the delay grows. Only recently, Dudik et al. (2011) demonstrated that it is possible to obtain regret bounds that only increase additively with the delay (i.e., the penalty becomes negligible for large numbers of decisions). However, the approach of Dudik et al. only applies to contextual bandit problems with finite decision sets, and thus not to settings with complex (even nonparametric) payoff functions. In contrast, there has\nbeen heuristic work in parallel Bayesian global optimization using GPs, e.g. by Ginsbourger et al. (2010). The state of the art is the simulation matching algorithm of Azimi et al. (2010). To our knowledge, no theoretical results regarding the regret of this algorithm exist. We compare with this approach in Section 5."
    }, {
      "heading" : "2. Problem Statement and Background",
      "text" : "We wish to make a sequence of decisions x1,x2, . . . ,xT ∈ D, where D is called the decision set, which is often (but not necessarily) a compact subset of Rd. For each decision, we observe noisy scalar reward y1, y2, . . . , yT , where for any t, yt = f(xt) + εt and where f : D → R is in turn an unknown function modeling the expected payoff f(x) for each decision x. For now we assume that the noise variables εt are i.i.d. Gaussian with known variance σ2n, i.e., εt ∼ N (0, σ2n). We will relax this assumption later. In the strictly sequential setting, we allow xt to depend on observations y1:t−1 associated with x1, . . . ,xt−1. Below, we will formalize the main problem tackled in this paper: the challenging setting where xt may only depend on y1:t′ , for some t ′ < t−1. We wish to maximize the cumulative reward∑T t=1 f(xt), or equivalently minimize the cumulative\nregret RT = ∑T t=1 rt, where rt = [f(x ∗)− f(xt)] and x∗ ∈ argmaxx∈D f(x) is an optimum decision (assumed to exist, but not necessarily to be unique). In experimental design, D might be the set of possible stimuli that can be applied, and f(x) corresponds to the response to stimulus x ∈ D. By minimizing the regret, we ensure progress towards the most effective stimulus uniformly over T . In fact, the average regret, RT /T , is a natural upper bound on the suboptimality of the best stimulus considered so far, i.e., RT /T ≥ mint [f(x∗)− f(xt)] (often called the simple regret, Bubeck et al. (2009)).\nThe Problem: Parallel / Delayed Selection In many applications, we wish to select batches of decisions x1, ...,xB to be evaluated in parallel. One natural application is the design of high-throughput experiments, where we perform several experiments in parallel, but only receive feedback after the experiments have concluded. In other settings, we may only receive feedback after a delay. In both situations, decisions are selected sequentially, but when making the decision xt in round t, we can only make use of the feedback obtained in rounds 1, . . . , t′, for some t′ ≤ t−1. Formally, we assume there is some mapping fb : N → N0 such that fb[t] ≤ t−1, ∀t ∈ N, and when taking decision at time t, we can use feedback up to and including round fb[t]. If fb[t] = 0, no information is available.\nThis framework can model a variety of realistic scenarios. Setting B = 1 corresponds to the non-delayed, strictly sequential setting. If the feedback is delayed by exactly B rounds, we can simply set fb[t] = max{t− B, 0}. To select batches of size B, we can simply set fb[t] = b(t − 1)/BcB, i.e., fb[1] = . . . fb[B] = 0, fb[B + 1] = . . . fb[2B] = B, . . . . We may also be interested in executing several experiments in parallel, but the duration of an experiment may be variable, and we can start a new experiment as soon as one finishes. In this case, fb[t] may be a more complex mapping. Here, B is the bound on the duration of any single experiment. In the following, we only assume that t− fb[t] ≤ B for some known constant B.\nModeling f via Gaussian Processes (GPs) If we do not make any assumptions about the payoff function f , for large (possibly infinite) decision setsD there is no hope to do well, i.e., incur little regret or even simply converge to an optimal decision. One effective formalism is to model f as a sample from a Gaussian process (GP) prior. A GP is a probability distribution across a class of – typically smooth – functions, which is parameterized by a kernel function k(x,x′), which characterizes the smoothness of f , and a mean function µ(x), which we assume to be µ(x) = 0 w.l.o.g. We write f ∼ GP(µ, k) to denote that we model f as sampled from such a GP. If we assume that the noise is i.i.d. Gaussian and we condition on a set of observations y1:t−1 = [y1, ..., yt−1] corresponding to X = {x1, ...,xt−1}, at any x ∈ D, we obtain a Gaussian posterior f(x)|y1:t−1 ∼ N (µt−1(x), σ2t−1(x)), where\nµt−1(x) = k[K + σ 2I]−1y1:t−1 and (1) σ2t−1(x) = k(x,x)− k[K + σ2nI]−1kT , (2)\nwhere k = k(x, X) is the row vector of kernel evaluations between x and X and K = K(X,X) is the matrix of kernel evaluations between past observations.\nThe GP-UCB approach Modeling f as a sample from a GP has the major advantage that the predictive uncertainty can be used to guide exploration and exploitation. Recently, Srinivas et al. (2010) analyzed the Gaussian process Upper Confidence Bound (GPUCB) selection rule\nxt = argmax x∈D\n[ µt−1(x) + α 1/2 t σt−1(x) ] . (3)\nThis decision rule uses αt, a domain-specific timevarying parameter, to trade off exploitation (sampling x with high mean) and exploration (sampling x with high standard deviation) by changing the relative weighting of the posterior mean and standard deviation, respectively µt−1(x) and σt−1(x) from Equations (1) and (2). Srinivas et al. (2010) showed that,\nAlgorithm 1 GP-BUCB\nInput: Decision set D, GP prior µ0, σ0, kernel function k(·, ·) for t = 1, 2, . . . , T do\nChoose xt = argmaxx∈D[µfb[t](x) + β 1/2 t σt−1(x)] Compute σt(·) if t = fb[t+ 1] then\nObtain yt′ = f(xt′) + εt′ for t ′ ∈ {fb[t], . . . , t}\nPerform Bayesian inference to obtain µt(·) end if\nend for\nwith proper choice of αt, the cumulative regret of GPUCB grows sublinearly for many commonly used kernel functions, providing the first regret bounds and convergence rates for GP optimization.\nMotivated by the strong theoretical and empirical performance of GP-UCB, we explore generalizations to batch / parallel selection (i.e., B > 1). One näıve approach would be to update the GP-UCB score (3) only once new feedback becomes available, but this algorithm would simply select the same observation up to B times, leading to limited exploration. To encourage more exploration, one may require that no decision is selected twice (i.e., simply rank decisions according to the GP-UCB score, and pick decisions in order of decreasing score, until new feedback is available). However, since f often varies smoothly, so does the GPUCB score; this modification would also suffer from limited exploration. In the following, we introduce the Gaussian process - Batch Upper Confidence Bound (GP-BUCB) algorithm, which encourages diversity in exploration, and prove strong performance guarantees.\n3. The GP-BUCB Algorithm A key property of GPs is that the predictive variance (2) only depends on where the observations are made, but not which values were actually observed. Thus, it is possible to compute the posterior variance used in the sequential GP-UCB score, even while previous observations are not yet available. A natural approach towards parallel exploration is therefore to alter (3) to sequentially choose decisions within the batch as\nxt = argmax x∈D\n[ µfb[t](x) + β 1/2 t σt−1(x) ] . (4)\nHere, the role of βt is analogous to that of αt in the GP-UCB algorithm. This approach naturally encourages diversity in exploration by taking into account the change in predictive variance: since the payoffs of “similar” decisions have similar predictive distributions, exploring one decision will automatically reduce the predictive variance of similar decisions.\nt (x) will contain C\nseq\nfb[t](x). (b): Due to the observations that\nGP-BUCB “hallucinates” (stars), the outer posterior confidence intervals Cbatcht (x) shrink from their values at the start of the batch (black dashed), but still contain Cseqfb[t](x), as desired. (c): Upon selection of the last decision of the batch, the feedback for all decisions is obtained, and new confidence intervals Cseqfb[t′](x) and corresponding C batch fb[t′] (x) are computed.\nThe disadvantage of taking this approach, however, is that the decision selection late in the batch is predicated on having information from the early decisions in the batch, but we do not in fact currently have that information; we are being “overconfident” about our knowledge of the function at those locations. This overconfidence requires us to compensate in a principled manner. One conceptual approach to doing so is to increase the width of the confidence intervals (through proper choice of βt), such that the confidence intervals used by GP-BUCB are conservative, i.e., contain the true function f(x) with high probability. Figure 1 illustrates this idea. In Section 4, we show how it is indeed possible to properly choose βt so that the regret only mildly increases, providing strong theoretical evidence about the potential for parallelizing GP optimization.\nLazy Variance Calculation One major computational bottleneck of applying GP-BUCB is calculating the posterior mean µt(x) and variance σ 2 t (x) for the candidate decisions. The mean is updated only whenever feedback is obtained, and – upon computation of the Cholesky factorization of K(X,X) + σ2nI (which only needs to be done once whenever new feedback arrives) – predicting µt(x) takes O(t) additions and multiplications. On the other hand, σ2t must be recomputed for every x in D after every single round, and requires solving backsubstitution, which requires O(t2) computations. Therefore, the variance computation dominates the computational cost of GP-BUCB.\nFortunately, for any fixed decision x, σ2t (x) is monotonically decreasing in t. This fact can be exploited to dramatically improve the running time of GP-BUCB, at least for finite (or when using discretizations of the) decision sets D. The key idea is that instead of recomputing σt−1(x) for all decisions x in every round t, we can maintain an upper bound σ̂t−1(x), initial-\nized to σ̂0(x) = ∞. In every round, we lazily apply the GP-BUCB rule with this upper bound, to identify\nxt = argmax x∈D\n[ µfb[t](x) + β 1/2 t σ̂t−1(x) ] . (5)\nWe then recompute σ̂t−1(xt) ← σt−1(xt). If xt still lies in the argmax of (5), we have identified the next decision to make, and set σ̂t(x) = σ̂t−1(x) for all remaining decisions x. This idea generalizes to the bandit setting a technique proposed by Minoux (1978), which concerns calculating the greedy action for submodular maximization and leads to dramatically improved empirical computational speed, discussed in Section 5."
    }, {
      "heading" : "4. Regret Bounds",
      "text" : "Srinivas et al. (2010) prove that the cumulative regret of the strictly sequential GP-UCB can be bounded (up to logarithmic factors) as RT = O ∗( √ TαT γT ), where\nγT = max |A|≤T I(f ; yA) (6)\nis the maximum mutual information\nI(f ; yA)=H(yA)−H(yA |f)= 1 2 log ∣∣I+σ−2n K(A,A)∣∣\nobtained through observations yA of any set A ⊆ D of T decisions evaluated. For many kernel functions commonly used in practice, they show that γT grows sublinearly and αT only needs to grow polylogarithmically in T . Thus, RT /T → 0, i.e., GP-UCB is a no-regret algorithm.\nThe analysis of GP-UCB (and upper-confidence index policies in general) rests upon three major pillars: (1) The constructed confidence intervals\nCseqt (x) = [ µt−1(x)± α1/2t σt−1(x) ] (7)\ncontain the true payoff f(x) with high probability; (2) The width of the confidence interval at the selected decision bounds the instantaneous regret rt (i.e.,\nrt ≤ wt, where wt = 2α1/2t σt−1(xt)); and (3) the widths w1, . . . , wT shrink sufficiently quickly to ensure sublinear regret.\nOur strategy for choosing βt in the GP-BUCB rule rests on a generalization of this analysis. We will choose βt such that the confidence intervals\nCbatcht (x) = [ µfb[t](x)± β 1/2 t σt−1(x) ] (8)\nstill contain the true expected payoff f(x) with high probability. Under this condition, a straightforward generalization of the arguments of Srinivas et al. (2010) leads to regret bounds of the form O∗( √ TβT γT ) (formal statement is given below).\nAvoiding Overconfidence. We seek to derive sufficient conditions on βt to ensure that the confidence intervals employed by GP-BUCB contain f with high probability. As we will see, a crucial role is played by the conditional mutual information, which for observations yA and yS of two finite sets A,S ⊆ D is defined as\nI(f ;yA | yS) = H(yA | yS)−H(yA | f).\nLemma 1 is the key technical result, which allows us to infer how much the confidence intervals must be enlarged to avoid overconfidence.\nLemma 1. For f sampled from a known GP prior with known noise variance σ2n, the ratio of σfb[t](x) to σt−1(x) is bounded as\nσfb[t](x) σt−1(x) ≤ exp{I(f ;yfb[t]+1:t−1 | y1:fb[t])}. (9)\nTherefore, the relative amount by which the confidence intervals can shrink w.r.t. decision x is bounded by the worst-case (greatest) mutual information I(f ;yfb[t]+1:t−1 | y1:fb[t]) obtained during selection of xfb[t]+1:t−1, those decisions for which feedback is not available. Thus, if we have a constant bound C on the maximum conditional mutual information that can be accrued within a batch, we can use it to guide our choice of βt to ensure that the algorithm is not overconfident. We can then leverage the machinery of Srinivas et al. (2010) to derive our regret bound below.\nRegret Bounds Our main result bounds the regret of GP-BUCB in terms of a bound C on the maximum conditional mutual information. It holds under any of three different assumptions about the payoff function f , which may all be of practical interest. In particular, it holds even if the assumption that f is sampled from a GP is replaced by the assumption that f has low norm in the Reproducing Kernel Hilbert Space (RKHS) associated with the kernel function.\nTheorem 1. Let δ ∈ (0, 1). Suppose one of the following assumptions holds:\n1. D is finite, f is sampled from a known GP prior with known noise variance σ2n, and αt = 2 log(|D|t2π2/6δ). 2. D ⊆ [0, l]d is compact and convex, d ∈ N, l > 0. f is sampled from a known GP prior with known noise variance σ2n, and k(x,x\n′) satisfies the following bound w.h.p. on the derivatives of GP sample paths f : for some constants a, b > 0,\nPr { sup x∈D |∂f/∂xj | > L } ≤ ae−(L/b) 2 , j = 1, . . . , d.\nChoose αt = 2 log(t 22π2/(3δ)) + 2d log ( t2dbl √ log(4da/δ) ) .\n3. D is arbitrary; f has RKHS norm ||f ||k ≤M . The noise εt form an arbitrary martingale difference sequence (meaning that E[εt | ε1, . . . , εt−1] = 0 for all t ∈ N), uniformly bounded by σn. Further define αt = 2M 2 + 300γt ln 3(t/δ).\nFurther suppose we have bound C > 0 s.t., for all t,\nmax A⊆D,|A|≤B−1 I(f ;yA | y1:fb[t]) ≤ C. (10)\nThen, the cumulative regret of GP-BUCB, using βt = exp(2C)αfb[t], is bounded by O∗( √ TγT exp(2C)αT ) w.h.p. Precisely,\nPr { RT ≤ √ C1T exp(2C)αT γT + 2 ∀T ≥ 1 } ≥ 1−δ\nwhere C1 = 8/ log(1 + σ −2 n ).\nThe key quantity that controls the regret in Theorem 1 is the bound C on the maximum conditional mutual information obtainable within a batch (10). In particular, the cumulative regret bound of GP-BUCB is a factor exp(C) larger than the regret bound for the sequential (B = 1) GP-UCB algorithm. Intuitively, one expects that C must grow monotonically with B: with greater delay, there is more potential for exploration (and thus to gain more information). An easy upper bound is obtained as follows: Due to the “information never hurts” bound (Cover & Thomas, 1991), the conditional mutual information I(f ;yA | yS) is monotonically decreasing in S (i.e., as elements are added to set S). Therefore, I(f ;yA | yS) ≤ I(f ;yA) ≤ γB−1, whenever |A| ≤ B − 1. However, the choice C = γB−1 is not satisfying; usually, γB−1 grows at least as Ω(logB), suggesting that exp(C) would have to grow at least linearly inB. In the following, we show that it is possible to slightly modify the GP-BUCB algorithm so that a constant choice of C independent of B suffices.\nBetter Bounds Through “Initialization” The key idea that allows us to obtain regret bounds independent of B is again to exploit monotonicity properties of the conditional mutual information. Suppose that instead of GP-BUCB, we use a two-stage procedure, that first nonadaptively (i.e., without any feedback) selects an initialization set Dinit of size |Dinit| = T init. The algorithm then obtains feedback yinit for all decisions Dinit = {xinit1 , . . . ,xinitT init}. In a second stage, it then applies GP-BUCB on the posterior Gaussian process distribution, conditioned on yinit.\nNotice that if we define\nγinitT = max A⊆D,|A|≤T I(f ;yA | yinit),\nthen, under the assumptions of Theorem 1, using C = γinitB−1, the regret of the two-stage algorithm is bounded by RT = O(T init + √ TγinitT αT exp 2C). In the following, we show that it is indeed possible to construct an initialization set Dinit such that the size T init is dominated by √ TγinitT αT exp(2C), and – crucially – that C = γinitB−1 can be bounded independently of the batch size B.\nWe will construct Dinit via uncertainty sampling: we start with Dinit0 = {}, and for each t = 1, . . . , T init greedily add the most uncertain decision\nxinitt = argmax x∈D σ2t−1(x),\nand set Dinitt = D init t−1 ∩ xinitt . We have the following key result about the residual information gain γinit:\nLemma 2. Suppose we use uncertainty sampling to generate an initialization set Dinit of size T init. Then\nγinitB−1 ≤ B − 1 T init γT init .\nWhenever γT is sublinear (i.e., γT = o(T )), then for any constant C > 0, we can choose T init as a function of B such that γinitB−1 < C. In order to derive bounds on T init, we in turn need a concrete analytical bound on γT . Fortunately, Srinivas et al. (2010) prove bounds on how the information gain γT grows for some of the most commonly used kernels. Table 1 provides sufficient conditions for how quickly T init must grow as a function of the batch size B. Finally, note that uncertainty sampling is a special case of the GP-BUCB algorithm with a constant prior mean of 0 and the require-\nment that for all 1 ≤ t ≤ T init, fb[t] = 0, i.e., no feedback is taken into account for the first T init iterations.\nWe summarize our analysis in the following theorem. For sake of notation, define RseqT to be the regret bound of Srinivas et al. (2010) associated with the sequential GP-UCB algorithm (i.e., Theorem 1 with B = 1).\nTheorem 2. Suppose one of the conditions of Theorem 1 is satisfied. Further suppose the kernel and T init are as listed in Table 1. Fix δ > 0. Let RT be the regret of GP-BUCB, which ignores feedback for the first T init rounds. Then there exists a constant C ′ independent of B such that for any T ≥ 0, it holds with probability at least 1− δ that\nRT ≤ C ′RseqT + 2||f ||∞T init,\nwhere C ′ takes the value shown in Table 1.\nNotice that, whenever B = O(polylog(T )), T init = O(polylog(T )). Further note RseqT = Ω( √ T ). Thus, as long as the batch size does not grow too quickly, the term O(T init) is dominated by C ′RseqT and thus the regret bounds of GP-BUCB are only a constant factor independently of B worse than those of GP-UCB."
    }, {
      "heading" : "5. Experiments",
      "text" : "We empirically evaluate GP-BUCB on several synthetic benchmark problems as well as two real applications. We compare it with four alternatives: (1) The strictly sequential GP-UCB algorithm (B = 1); (2) NRB-UCB, an approach that simply picks the maximizer of the GP-UCB score B times; (3) NTB-UCB, an approach that picks the top B scores according to the GP-UCB criterion; (4) A state of the art algorithm for Batch Bayesian optimization proposed by Azimi et al. (2010), which can use either a UCB or Maximum Expected Improvement (MEI) decision rule, herein SM-UCB and SM-MEI respectively. All batch selection algorithms pick batches of B = 10 points and all experiments were repeated for 100 trials with independent observation noise for each trial.\nSynthetic Benchmark Problems We first test GP-BUCB in conditions where the true prior is known. A set of 100 example functions was drawn from a zeromean GP with Matérn kernel over the interval [0, 1].\nThe kernel, its parameters, and the noise variance were known to each algorithm. The decision set D was the discretization of [0, 1] into 1000 evenly spaced points. Figures 2(a) and 2(e) present the results of this experiment. GP-BUCB performed slightly better than SM-UCB and SM-MEI in terms of both average regret and minimum regret. GP-BUCB, SM-UCB, and SM-MEI were outperformed by GP-UCB early on, but after they received their first observations at the end of batch 1 (query 10), performance was comparable to GP-UCB. As expected, both of the näıve algorithms performed quite poorly. Figure 2(d) compares the algorithms in terms of their running time; lazy variance calculations led to dramatic running time improvements. We also performed experiments on other synthetic benchmark domains, with qualitatively similar results (presented in the supplemental material).\nAutomated Vaccine Design We also tested GPBUCB on a database of Widmer et al. (2010), which describes the binding affinity of various peptides with a Major Histocompatibility Complex (MHC) Class I molecule, of importance when designing vaccines to exploit peptide binding properties. Each of the peptides is described by a set of chemical features in R45. The binding affinity of each peptide, which is treated as the reward or payoff, is described as an offset IC50 value. The experiments used a linear ARD kernel fitted on a different MHC molecule from the same data set. Figures 2(b) and 2(f) present this experiment’s results. GP-BUCB performs competitively with SM-MEI and SM-UCB, both in terms of average and minimum regret, and converges to the performance of GP-UCB.\nSpinal Cord Therapy Lastly, we compare the algorithms on a data set of leg muscle activity triggered by therapeutic spinal electrostimulation in spinal cord injured rats. The experimental objective is to choose the stimulus electrodes which maximize the resulting activity in lower limb muscles, as measured by electromyography (EMG), in order to improve spinal reflex and locomotor function. We sought to maximize the peak-to-peak amplitude of the recorded EMG waveforms from the right medial Gastrocnemius muscle in a time window corresponding to a single interneuronal delay. This objective function measures to what degree the selected stimulus activates the interneurons in the spinal gray matter which control reflex activity. Electrode configurations were represented in R4 by the cathode and anode locations on the array. A squaredexponential ARD kernel was fitted for this space using experimental data from 12 days post-injury. Algorithm testing was done on data from 116 electrode pairs tested on the 14th day post-injury. Experimental results are presented in Figures 2(c) and 2(g). This problem setting was quite challenging for all algorithms, as the data was highly multi-modal. Consequently, GP-UCB often failed to find the optimum in the number of queries examined; out of 100 runs, only 18 had converged to the optimum, and out of the remainder, none had ever visited the optimum in 200 queries. Interestingly, the GP-BUCB, SM-UCB and SM-MEI algorithms were more robust to these difficulties; their superior initialization, born of the exploratory behavior forced on them by their initial ignorance, resulted in convergence likelihoods on the or-\nder of 40% for each. The superior performance of GPBUCB to SM-UCB and SM-MEI with respect to average regret and the comparable likelihoods of convergence within the practical experimental window considered indicate that GP-BUCB is at least as effective as the current state of the art in this challenging experimental setting. Lazy variance calculations again led to dramatic running time improvements, presented in Figure 2(h)."
    }, {
      "heading" : "6. Conclusions",
      "text" : "We have developed the GP-BUCB algorithm for parallelizing exploration and exploitation tradeoffs in Gaussian process bandit optimization. We showed how the regret of GP-BUCB can be bounded in terms of an intuitive conditional mutual information quantity. Using this analysis, we prove that GP-BUCB can be “initialized” to obtain regret bounds which only additively depend on the batch size for many kernel functions commonly used. We further show how “lazy” variance evaluation can yield order-of-magnitude improvements in running time. In our experiments, GP-BUCB compares favorably to the state of the art in parallel Bayesian optimization, which is not equipped with theoretical guarantees. We believe that our results provide an important step towards solving complex, large-scale exploration-exploitation tradeoffs.\nAcknowledgments The authors thank Daniel Golovin for helpful discussions. This work was partially supported by NIH project R01 NS062009, SNSF grant 200021 137971, NSF IIS-0953413, DARPA MSEE FA8650-11-1-7156 and the ThinkSwiss Research Scholarship."
    } ],
    "references" : [ {
      "title" : "Competing in the dark: An efficient algorithm for bandit linear optimization",
      "author" : [ "J. Abernethy", "E. Hazan", "A. Rakhlin" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Abernethy et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Abernethy et al\\.",
      "year" : 2008
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Mach. Learn.,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Batch bayesian optimization via simulation matching",
      "author" : [ "J. Azimi", "A. Fern", "X.Fern" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Azimi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Azimi et al\\.",
      "year" : 2010
    }, {
      "title" : "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
      "author" : [ "E. Brochu", "M. Cora", "N. de Freitas" ],
      "venue" : "In TR-2009-23,",
      "citeRegEx" : "Brochu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Brochu et al\\.",
      "year" : 2009
    }, {
      "title" : "Online optimization in X-armed bandits",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz", "C. Szepesvári" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2008
    }, {
      "title" : "Pure exploration in multi-armed bandits problems",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "In ALT,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2009
    }, {
      "title" : "Sdo: A statistical method for global optimization",
      "author" : [ "D.D. Cox", "S. John" ],
      "venue" : "Multidisciplinary Design Optimization: State of the Art,",
      "citeRegEx" : "Cox and John,? \\Q1997\\E",
      "shortCiteRegEx" : "Cox and John",
      "year" : 1997
    }, {
      "title" : "Stochastic linear optimization under bandit feedback",
      "author" : [ "V. Dani", "T.P. Hayes", "S.M. Kakade" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Dani et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2008
    }, {
      "title" : "Efficient optimal learning for contextual bandits",
      "author" : [ "M. Dudik", "D. Hsu", "S. Kale", "N. Karampatziakis", "J. Langford", "L. Reyzin", "T. Zhang" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Dudik et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dudik et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient global optimization of expensive black-box functions",
      "author" : [ "D.R. Jones", "M. Schonlau", "W.J. Welch" ],
      "venue" : "J Glob. Opti.,",
      "citeRegEx" : "Jones et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 1998
    }, {
      "title" : "Multi-armed bandits in metric spaces",
      "author" : [ "R. Kleinberg", "A. Slivkins", "E. Upfal" ],
      "venue" : "In STOC, pp",
      "citeRegEx" : "Kleinberg et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kleinberg et al\\.",
      "year" : 2008
    }, {
      "title" : "Bandit based montecarlo planning",
      "author" : [ "L. Kocsis", "C. Szepesvári" ],
      "venue" : "In ECML,",
      "citeRegEx" : "Kocsis and Szepesvári,? \\Q2006\\E",
      "shortCiteRegEx" : "Kocsis and Szepesvári",
      "year" : 2006
    }, {
      "title" : "Automatic gait optimization with Gaussian process regression",
      "author" : [ "D. Lizotte", "T. Wang", "M. Bowling", "D. Schuurmans" ],
      "venue" : "In IJCAI, pp",
      "citeRegEx" : "Lizotte et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Lizotte et al\\.",
      "year" : 2007
    }, {
      "title" : "Accelerated greedy algorithms for maximizing submodular set functions",
      "author" : [ "M. Minoux" ],
      "venue" : "Optimization Techniques,",
      "citeRegEx" : "Minoux,? \\Q1978\\E",
      "shortCiteRegEx" : "Minoux",
      "year" : 1978
    }, {
      "title" : "Bayesian Approach to Global Optimization",
      "author" : [ "J. Mockus" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "Mockus,? \\Q1989\\E",
      "shortCiteRegEx" : "Mockus",
      "year" : 1989
    }, {
      "title" : "Gaussian Processes for Machine Learning",
      "author" : [ "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : null,
      "citeRegEx" : "Rasmussen and Williams,? \\Q2006\\E",
      "shortCiteRegEx" : "Rasmussen and Williams",
      "year" : 2006
    }, {
      "title" : "Some aspects of the sequential design of experiments",
      "author" : [ "H. Robbins" ],
      "venue" : "Bul. Am. Math. Soc.,",
      "citeRegEx" : "Robbins,? \\Q1952\\E",
      "shortCiteRegEx" : "Robbins",
      "year" : 1952
    }, {
      "title" : "Gaussian process optimization in the bandit setting: No regret and experimental design",
      "author" : [ "N. Srinivas", "A. Krause", "S. Kakade", "M. Seeger" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Srinivas et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Srinivas et al\\.",
      "year" : 2010
    }, {
      "title" : "Inferring latent task structure for multitask learning by multiple kernel learning",
      "author" : [ "C. Widmer", "N. Toussaint", "Y. Altun", "G. Rätsch" ],
      "venue" : "BMC Bioinformatics,",
      "citeRegEx" : "Widmer et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Widmer et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Our approach generalizes the GP-UCB approach (Srinivas et al., 2010) to the parallel setting.",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 16,
      "context" : "Related Work Classical work on multi-armed bandit problems has focused on the case of a finite number of decisions (Robbins, 1952).",
      "startOffset" : 115,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : "Optimistic allocation according to upper-confidence bounds (UCB) on the payoffs has proven to be particularly effective (Auer et al., 2002).",
      "startOffset" : 120,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : "Examples include bandits with linear (Dani et al., 2008; Abernethy et al., 2008) or Lipschitz-continous payoffs (Kleinberg et al.",
      "startOffset" : 37,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "Examples include bandits with linear (Dani et al., 2008; Abernethy et al., 2008) or Lipschitz-continous payoffs (Kleinberg et al.",
      "startOffset" : 37,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : ", 2008) or Lipschitz-continous payoffs (Kleinberg et al., 2008), or bandits on trees (Kocsis & Szepesvári, 2006; Bubeck et al.",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : ", 2008), or bandits on trees (Kocsis & Szepesvári, 2006; Bubeck et al., 2008).",
      "startOffset" : 29,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : "The exploration-exploitation tradeoff has also been studied in Bayesian global optimization and response surface modeling, where Gaussian process models are often used due to their flexibility in incorporating prior assumptions about the payoff function (Brochu et al., 2009).",
      "startOffset" : 254,
      "endOffset" : 275
    }, {
      "referenceID" : 9,
      "context" : "Several heuristics, such as Maximum Expected Improvement (Jones et al., 1998), Maximum Probability of Improvement (Mockus, 1989), and upperconfidence based methods (Cox & John, 1997), have been developed to balance exploration with exploitation and successfully applied in learning problems (Lizotte et al.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 14,
      "context" : ", 1998), Maximum Probability of Improvement (Mockus, 1989), and upperconfidence based methods (Cox & John, 1997), have been developed to balance exploration with exploitation and successfully applied in learning problems (Lizotte et al.",
      "startOffset" : 44,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : ", 1998), Maximum Probability of Improvement (Mockus, 1989), and upperconfidence based methods (Cox & John, 1997), have been developed to balance exploration with exploitation and successfully applied in learning problems (Lizotte et al., 2007).",
      "startOffset" : 221,
      "endOffset" : 243
    }, {
      "referenceID" : 0,
      "context" : ", 2008; Abernethy et al., 2008) or Lipschitz-continous payoffs (Kleinberg et al., 2008), or bandits on trees (Kocsis & Szepesvári, 2006; Bubeck et al., 2008). The exploration-exploitation tradeoff has also been studied in Bayesian global optimization and response surface modeling, where Gaussian process models are often used due to their flexibility in incorporating prior assumptions about the payoff function (Brochu et al., 2009). Several heuristics, such as Maximum Expected Improvement (Jones et al., 1998), Maximum Probability of Improvement (Mockus, 1989), and upperconfidence based methods (Cox & John, 1997), have been developed to balance exploration with exploitation and successfully applied in learning problems (Lizotte et al., 2007). Recently, Srinivas et al. (2010) analyzed GP-UCB, an upper-confidence bound sampling based algorithm for this setting, and proved bounds on its cumulative regret, and thus convergence rates for Bayesian global optimization.",
      "startOffset" : 8,
      "endOffset" : 784
    }, {
      "referenceID" : 7,
      "context" : "Only recently, Dudik et al. (2011) demonstrated that it is possible to obtain regret bounds that only increase additively with the delay (i.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "Only recently, Dudik et al. (2011) demonstrated that it is possible to obtain regret bounds that only increase additively with the delay (i.e., the penalty becomes negligible for large numbers of decisions). However, the approach of Dudik et al. only applies to contextual bandit problems with finite decision sets, and thus not to settings with complex (even nonparametric) payoff functions. In contrast, there has been heuristic work in parallel Bayesian global optimization using GPs, e.g. by Ginsbourger et al. (2010). The state of the art is the simulation matching algorithm of Azimi et al.",
      "startOffset" : 15,
      "endOffset" : 522
    }, {
      "referenceID" : 2,
      "context" : "The state of the art is the simulation matching algorithm of Azimi et al. (2010). To our knowledge, no theoretical results regarding the regret of this algorithm exist.",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : ", RT /T ≥ mint [f(x∗)− f(xt)] (often called the simple regret, Bubeck et al. (2009)).",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : "Recently, Srinivas et al. (2010) analyzed the Gaussian process Upper Confidence Bound (GPUCB) selection rule",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 17,
      "context" : "Srinivas et al. (2010) showed that, Algorithm 1 GP-BUCB",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "This idea generalizes to the bandit setting a technique proposed by Minoux (1978), which concerns calculating the greedy action for submodular maximization and leads to dramatically improved empirical computational speed, discussed in Section 5.",
      "startOffset" : 68,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : "Under this condition, a straightforward generalization of the arguments of Srinivas et al. (2010) leads to regret bounds of the form O∗( √ TβT γT ) (formal statement is given below).",
      "startOffset" : 75,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "We can then leverage the machinery of Srinivas et al. (2010) to derive our regret bound below.",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : "Fortunately, Srinivas et al. (2010) prove bounds on how the information gain γT grows for some of the most commonly used kernels.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 17,
      "context" : "For sake of notation, define R T to be the regret bound of Srinivas et al. (2010) associated with the sequential GP-UCB algorithm (i.",
      "startOffset" : 59,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "We compare it with four alternatives: (1) The strictly sequential GP-UCB algorithm (B = 1); (2) NRB-UCB, an approach that simply picks the maximizer of the GP-UCB score B times; (3) NTB-UCB, an approach that picks the top B scores according to the GP-UCB criterion; (4) A state of the art algorithm for Batch Bayesian optimization proposed by Azimi et al. (2010), which can use either a UCB or Maximum Expected Improvement (MEI) decision rule, herein SM-UCB and SM-MEI respectively.",
      "startOffset" : 343,
      "endOffset" : 363
    }, {
      "referenceID" : 18,
      "context" : "Automated Vaccine Design We also tested GPBUCB on a database of Widmer et al. (2010), which describes the binding affinity of various peptides with a Major Histocompatibility Complex (MHC) Class I molecule, of importance when designing vaccines to exploit peptide binding properties.",
      "startOffset" : 64,
      "endOffset" : 85
    } ],
    "year" : 2012,
    "abstractText" : "Can one parallelize complex exploration– exploitation tradeoffs? As an example, consider the problem of optimal highthroughput experimental design, where we wish to sequentially design batches of experiments in order to simultaneously learn a surrogate function mapping stimulus to response and identify the maximum of the function. We formalize the task as a multiarmed bandit problem, where the unknown payoff function is sampled from a Gaussian process (GP), and instead of a single arm, in each round we pull a batch of several arms in parallel. We develop GP-BUCB, a principled algorithm for choosing batches, based on the GP-UCB algorithm for sequential GP optimization. We prove a surprising result; as compared to the sequential approach, the cumulative regret of the parallel algorithm only increases by a constant factor independent of the batch size B. Our results provide rigorous theoretical support for exploiting parallelism in Bayesian global optimization. We demonstrate the effectiveness of our approach on two real-world applications.",
    "creator" : "LaTeX with hyperref package"
  }
}