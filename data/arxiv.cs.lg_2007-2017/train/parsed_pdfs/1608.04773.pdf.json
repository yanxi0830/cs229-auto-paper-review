{
  "name" : "1608.04773.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Faster Principal Component Regression and Stable Matrix Chebyshev Approximation",
    "authors" : [ "Zeyuan Allen-Zhu", "Yuanzhi Li" ],
    "emails" : [ "zeyuan@csail.mit.edu", "yuanzhil@cs.princeton.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We solve principal component regression (PCR), up to a multiplicative accuracy 1 + γ, by reducing the problem to Õ(γ−1) black-box calls of ridge regression. Therefore, our algorithm does not require any explicit construction of the top principal components, and is suitable for large-scale PCR instances. In contrast, previous result requires Õ(γ−2) such black-box calls.\nWe obtain this result by developing a general stable recurrence formula for matrix Chebyshev polynomials, and a degree-optimal polynomial approximation to the matrix sign function. Our techniques may be of independent interests, especially when designing iterative methods."
    }, {
      "heading" : "1 Introduction",
      "text" : "In machine learning and statistics, it is often desirable to represent a large-scale dataset in a more tractable, lower-dimensional form, without losing too much information. One of the most robust ways to achieve this goal is through principal component projection (PCP):\nPCP: project vectors onto the span of the top principal components of the a matrix.\nIt is well-known that PCP decreases noise and increases efficiency in downstream tasks. One of the main applications is principal component regression (PCR):\nPCR: linear regression but restricted to the subspace of top principal components.\nClassical algorithms for PCP or PCR rely on a principal component analysis (PCA) solver to recover the top principal components first; with these components available, the tasks of PCP and PCR become trivial because the projection matrix can be constructed explicitly.\nUnfortunately, PCA solvers demand a running time that at least linearly scales with the number of top principal components chosen for the projection. For instance, to project a vector onto the top 1000 principal components of a high-dimensional dataset, even the most efficient Krylov-based [18] or Lanczos-based [4] methods require a running time that is proportional to 1000 × 40 = 4 × 104 times the input matrix sparsity, if the Krylov or Lanczos method is executed for 40 iterations. This is usually computationally intractable.\nar X\niv :1\n60 8.\n04 77\n3v 2\n[ st\nat .M\nL ]\n2 4\nA pr"
    }, {
      "heading" : "1.1 Approximating PCP Without PCA",
      "text" : "In this paper, we propose the following notion of PCP approximation. Given a data matrix A ∈ Rd′×d (with singular values no greater than 1) and a threshold λ > 0, we say that an algorithm solves (γ, ε)-approximate PCP if —informally speaking and up to a multiplicative 1± ε error— it projects (see Def. 3.1 for a formal definition)\n1. any eigenvector ν of A>A with value in [ λ(1 + γ), 1 ] to ν, 2. any eigenvector ν of A>A with value in [ 0, λ(1− γ) ] to ~0, 3. any eigenvector ν of A>A with value in [ λ(1− γ), λ(1 + γ) ] to “anywhere between ~0 and ν.”\nSuch a definition also extends to (γ, ε)-approximate PCR (see Def. 3.2). It was first noticed by Frostig et al. [13] that approximate PCP and PCR be solved with a running time independent of the number of principal components above threshold λ. More specifically, they reduced (γ, ε)-approximate PCP and PCR to\nO ( γ−2 log(1/ε) ) black-box calls of any ridge regression subroutine\nwhere each call computes (A>A + λI)−1u for some vector u.1 Our main focus of this paper is to quadratically improve this performance and reduce PCP and PCR to\nO ( γ−1 log(1/γε) ) black-box calls of any ridge regression subroutine\nwhere each call again computes (A>A + λI)−1u.\nRemark 1.1. Frostig et al. only showed their algorithm satisfies the properties 1 and 2 of (γ, ε)approximation (but not the property 3), and thus their proof was only for matrix A with no singular value in the range [ √ λ(1− γ), √ λ(1 + γ)]. This is known as the eigengap assumption, which is rarely satisfied in practice [18]. In this paper, we prove our result both with and without such eigengap assumption. Since our techniques also imply the algorithm of Frostig et al. satisfies property 3, throughout the paper, we say Frostig et al. solve (γ, ε)-approximate PCP and PCR."
    }, {
      "heading" : "1.2 From PCP to Polynomial Approximation",
      "text" : "The main technique of Frostig et al. is to construct a polynomial to approximate the sign function sgn(x) : [−1, 1]→ {±1}:\nsgn(x) def = { +1, x ≥ 0; −1, x < 0.\nIn particular, given any polynomial g(x) satisfying ∣∣g(x)− sgn(x)\n∣∣ ≤ ε ∀x ∈ [−1,−γ] ∪ [γ, 1] , and (1.1)∣∣g(x) ∣∣ ≤ 1 ∀x ∈ [−γ, γ] , (1.2)\nthe problem of (γ, ε)-approximate PCP can be reduced to computing the matrix polynomial g(S) for S def = (A>A + λI)−1(A>A− λI) (cf. Fact 7.1). In other words,\n• to project any vector χ ∈ Rd to top principal components, we can compute g(S)χ instead; and 1Ridge regression is often considered as an easy-to-solve machine learning problem: using for instance SVRG [17],\none can usually solve ridge regression to an 10−8 accuracy with at most 40 passes of the data.\n• to compute g(S)χ, we can reduce it to ridge regression for each evaluation of Su for some vector u.\nRemark 1.2. Since the transformation from A>A to S is not linear, the final approximation to the PCP is a rational function (as opposed to a polynomial) over A>A. We restrict to polynomial choices of g(·) because in this way, the final rational function has all the denominators being A>A + λI, thus reduces to ridge regressions. Remark 1.3. The transformation from A>A to S ensures that all the eigenvalues of A>A in the range (1± γ)λ roughly map to the eigenvalues of S in the range [−γ, γ].\nMain Challenges. There are two main challenges regarding the design of polynomial g(x).\n• Efficiency. We wish to minimize the degree n = deg(g(x)) because the computation of g(S)χ usually requires n calls of ridge regression.\n• Stability. We wish g(x) to be stable; that is, g(S)χ must be given by a recursive formula where if we make ε′ error in each recursion (due to error incurred from ridge regression), the final error of g(S)χ must be at most ε′ × poly(d).\nRemark 1.4. Efficient routines such as SVRG [17] solve ridge regression and thus compute Su for any u ∈ Rd, with running times only logarithmically in 1/ε′. Therefore, by setting ε′ = ε/poly(d), one can blow up the running time by a small factor O(log(d)) in order to obtain an ε-accurate solution for g(S)χ.\nThe polynomial g(x) constructed by Frostig et al. comes from truncated Taylor expansion. It has degree O ( γ−2 log(1/ε) ) and is stable. This γ−2 dependency limits the practical performance of their proposed PCP and PCR algorithms, especially in a high accuracy regime. At the same time,\n• the optimal degree for a polynomial to satisfy even only (1.1) is Θ ( γ−1 log(1/ε) ) [9, 10].\nFrostig et al. were unable to find a stable polynomial matching this optimal degree and left it as open question.2"
    }, {
      "heading" : "1.3 Our Results and Main Ideas",
      "text" : "We provide an efficient and stable polynomial approximation to the matrix sign function that has a near-optimal degree O(γ−1 log(1/γε)). At a high level, we construct a polynomial q(x) that approximately equals (\n1+κ−x 2\n)−1/2 for some κ = Θ(γ2); then we set g(x) def = x · q(1 +κ− 2x2) which\napproximates sgn(x).\nTo construct q(x), we first note that (\n1+κ−x 2\n)−1/2 has no singular point on [−1, 1] so we can\napply Chebyshev approximation theory to obtain some q(x) of degree O(γ−1 log(1/γε)) satisfying ∣∣∣q(x)−\n(1 + κ− x 2 )−1/2∣∣∣ ≤ ε for every x ∈ [−1, 1] .\nThis can be shown to imply ∣∣g(x)− sgn(x) ∣∣ ≤ ε for every x ∈ [−1,−γ] ∪ [γ, 1], so (1.1) is satisfied. In order to prove (1.2) (i.e., ∣∣g(x) ∣∣ ≤ 1 for every x ∈ [−γ, γ]) , we prove a separate lemma:3\nq(x) ≤ (1 + κ− x\n2\n)−1/2 for every x ∈ [1, 1 + κ] .\n2Using degree reduction, Frostig et al. found an explicit polynomial g(x) of degree O ( γ−1 log(1/γε) ) satisfying (1.1). However, that polynomial is unstable because it is constructed monomial by monomial and has exponentially large coefficients in front of each monomial. Furthermore, it is not clear if their polynomial satisfies the (1.2).\n3We proved a general lemma which holds for any function whose all orders of derivatives are non-negative at x = 0.\nNote that this does not follow from standard Chebyshev theory because Chebyshev approximation guarantees are only with respect to x ∈ [−1, 1] and do not extend to singular point x = 1 + κ.\nThis proves the “efficiency” part of the main challenges discussed earlier. As for the “stability” part, we prove a general theorem regarding any weighted sum of Chebyshev polynomials applied to matrices. We provide a backward recurrence algorithm and show that it is stable under noisy computations. This may be of independent interest.\nFor interested readers, we compare our polynomial q(x) with that of Frostig et al. in Figure 1."
    }, {
      "heading" : "1.4 Related Work",
      "text" : "There are a few attempts to reduce the cost of PCA when solving PCR, by for instance approximating the matrix APλ where Pλ is the PCP projection matrix [6, 7]. However, they cost a running time that linearly scales with the number of principal components above λ.\nA significant number of papers have focused on the low-rank case of PCA [2, 4, 18] and its online variant [3]. Unfortunately, all of these methods require a running time that scales at least linearly with respect to the number of top principal components.\nMore related to this paper is work on matrix sign function, which plays an important role in control theory and quantum chromodynamics. Several results have addressed Krylov methods for applying the sign function in the so-called Krylov subspace, without explicitly constructing any approximate polynomial [21, 24]. However, Krylov methods are not (γ, ε)-approximate PCP solvers, and there is no supporting stability theory behind them.4 Other iterative methods have also been proposed, see Section 5 of textbook [16]. For instance, Schur’s method is a slow one and also requires the matrix to be explicitly given. The Newton’s iteration and its numerous variants (e.g. [19]) provide rational approximations to the matrix sign function as opposed to polynomial approximations. Our result and Frostig et al. [13] differ from these cited works, because we have only accessed an approximate ridge regression oracle, so ensuring a polynomial approximation to the sign function and ensuring its stability are crucial.\nUsing matrix Chebyshev polynomials to approximate matrix functions is not new. Perhaps the most celebrated example is to approximate S−1 using polynomials on S, used in the analysis of conjugate gradient [22]. Independent from this paper,5 Han et al. [15] used Chebyshev polynomials to approximate the trace of the matrix sign function, i.e., Tr(sgn(S)), which is similar but a different problem.6 Also, they did not study the case when the matrix-vector multiplication oracle is only approximate (like we do in this paper), or the case when S has eigenvalues in the range [−γ, γ].\n4We anyways have included Krylov method in our empirical evaluation section and shall discuss its performance there, see for instance Remark 8.1.\n5Their paper appeared online two months before us, and we became aware of their work in March 2017. 6In particular, their degree of the Chebyshev polynomial is O ( γ−1(log2(1/γ) + log(1/γ) log(1/ε)) ) in the language\nof this paper; in contrast, we have degree O ( γ−1 log(1/γε) ) .\nRoadmap.\n• In Section 2, we provide notions for this paper and basics for Chebyshev polynomials • In Section 3, we put forward our formal definitions for approximate PCP and PCR, and show\na reduction from approximate PCR to approximate PCP.\n• In Section 4, we prove a general lemma regarding Chebyshev approximations outside [−1, 1]. • In Section 5, we design our polynomial approximation to sgn(x). • In Section 6, we show how to stably compute any weighted sum of Chebyshev polynomials. • In Section 7, we provide pseudocode and prove our main theorems regarding PCP and PCR. • In Section 8, we provide empirical evaluations of our theory."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We denote by 1[e] ∈ {0, 1} the indicator function for event e, by ‖v‖ or ‖v‖2 the Euclidean norm of a vector v, by M† the Moore-Penrose pseudo-inverse of a symmetric matrix M, and by ‖M‖2 its spectral norm. We sometimes use ~v to emphasize that v is a vector.\nGiven a symmetric d × d matrix M and any f : R → R, f(M) is the matrix function applied to M, which is equal to Udiag{f(D1), . . . , f(Dd)}U> if M = Udiag{D1, . . . , Dd}U> is its eigendecomposition.\nThroughout the paper, matrix A is of dimension d′ × d. We denote by σmax(A) the largest singular value of A. Following the tradition of [13] and keeping the notations light, we assume without loss of generality that σmax(A) ≤ 1. We are interested in PCP and PCR problems with an eigenvalue threshold λ ∈ (0, 1).\nThroughout the paper, we denote by λ1 ≥ · · · ≥ λd ≥ 0 the eigenvalues of A>A, and by ν1, . . . , νd ∈ Rd the eigenvectors of A>A corresponding to λ1, . . . , λd. We denote by Pλ the projection matrix Pλ def = (ν1, . . . , νj)(ν1, . . . , νj)\n> where j is the largest index satisfying λj ≥ λ. In other words, Pλ is a projection matrix to the eigenvectors of A >A with eigenvalues ≥ λ.\nDefinition 2.1. The principal component projection (PCP) of χ ∈ Rd at threshold λ is ξ∗ = Pλχ. Definition 2.2. The principal component regression (PCR) of regressand b ∈ Rd′ at threshold λ is x∗ = arg min\ny∈Rd ‖APλy − b‖2 or equivalently x∗ = (A>A)†Pλ(A>b) ."
    }, {
      "heading" : "2.1 Ridge Regression",
      "text" : "Definition 2.3. A black-box algorithm ApxRidge(A, λ, u) is an ε-approximate ridge regression solver, if for every u ∈ Rd, it satisfies ‖ApxRidge(A, λ, u)− (A>A + λI)−1u‖ ≤ ε‖u‖.7\nRidge regression is equivalent to solving well-conditioned linear systems, or minimizing strongly convex and smooth objectives f(y)\ndef = 12y >(A>A + λI)y − u>y. Remark 2.4. There is huge literature on efficient algorithms solving ridge regression. Most notably,\n(1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods;\n(2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and\n(3) NUACDM [5] gives the fastest coordinate-descent method.\n7In fact, throughout the paper, we only need ApxRidge to satisfy this property with high probability for each u.\nThe running time of (1) is O(nnz(A)λ−1/2 log(1/ε)) where nnz(A) is time to multiply A to any vector. The running times of (2) and (3) depend on structural properties of A and are always faster than (1).\nBecause the best complexity of ridge regression depends on the structural properties of A, following Frostig et al., we only compute our running time in terms of the “number of black-box calls” to a ridge regression solver."
    }, {
      "heading" : "2.2 Chebyshev Polynomials",
      "text" : "Definition 2.5. Chebyshev polynomials of 1st and 2nd kind are {Tn(x)}n≥0 and {Un(x)}n≥0 where\nT0(x) def= 1, T1(x) def= x, Tn+1(x) def= 2x · Tn(x)− Tn−1(x) U0(x) def= 1, U1(x) def= 2x, Un+1(x) def= 2x · Un(x)− Un−1(x)\nFact 2.6 ([23]). It satisfies ddxTn(x) = nUn−1(x) for n ≥ 1 and\n∀n ≥ 0: Tn(x) =    cos(n arccos(x)), if |x| ≤ 1; cosh(n arccosh(x)), if x ≥ 1; (−1)n cosh(n arccosh(−x)), if x ≤ −1.\nIn particular, when x ≥ 1, Tn(x) = 12 [( x− √ x2 − 1 )n + ( x+ √ x2 − 1 )n] and Un(x) = 12√x2−1 [( x+\n√ x2 − 1 )n+1 − ( x− √ x2 − 1 )n+1] .\nTn(x) = 1\n2\n[( x− √ x2 − 1 )n + ( x+ √ x2 − 1 )n]\nUn(x) = 1\n2 √ x2 − 1\n[( x+ √ x2 − 1 )n+1 − ( x− √ x2 − 1 )n+1]\nDefinition 2.7. For function f(x) whose domain contains [−1, 1], its degree-n Chebyshev truncated series and degree-n Chebyshev interpolation are respectively\npn(x) def =\nn∑\nk=0\nakTk(x) and qn(x) def= n∑\nk=0\nckTk(x) ,\nwhere ak def = 2− 1[k = 0] π\n∫ 1\n−1 f(x)Tk(x)√ 1− x2 dx and ck def = 2− 1[k = 0] n+ 1\nn∑\nj=0\nf ( xj ) Tk ( xj ) .\nAbove, xj def = cos ( (j+0.5)π n+1 ) ∈ [−1, 1] is the j-th Chebyshev point of order n.\nThe following lemma is known as the aliasing formula for Chebyshev coefficients:\nLemma 2.8 (cf. Theorem 4.2 of [23]). Let f be Lipschitz continuous on [−1, 1] and {ak}, {ck} be defined in Def. 2.7, then\nc0 = a0 + a2n + a4n + ... , cn = an + a3n + a5n + ... , and\n∀k ∈ {1, 2, . . . , n− 1} : ck = ak + (ak+2n + ak+4n + ...) + (a−k+2n + a−k+4n + ...)\nDefinition 2.9. For every ρ > 0, let Eρ be the ellipse E of foci ±1 with major radius 1 + ρ. (This is also known as Bernstein ellipse with parameter 1 + ρ+ √ 2ρ+ ρ2.)\nThe following lemma is the main theory regarding Chebyshev approximation:\nLemma 2.10 (cf. Theorem 8.1 and 8.2 of [23]). Suppose f(z) is analytic on Eρ and |f(z)| ≤M on Eρ. Let pn(x) and qn(x) be the degree-n Chebyshev truncated series and Chebyshev interpolation of f(x) on [−1, 1]. Then, • maxx∈[−1,1] |f(x)− pn(x)| ≤ 2M\nρ+ √ 2ρ+ρ2\n( 1 + ρ+ √ 2ρ+ ρ2 )−n ;\n• maxx∈[−1,1] |f(x)− qn(x)| ≤ 4M ρ+ √ 2ρ+ρ2\n( 1 + ρ+ √ 2ρ+ ρ2 )−n .\n• |a0| ≤M and |ak| ≤ 2M ( 1 + ρ+ √ 2ρ+ ρ2 )−k for k ≥ 1."
    }, {
      "heading" : "3 Approximate PCP and PCR",
      "text" : "We formalize our notions of approximation for PCP and PCR, and provide a reduction from PCR to PCP."
    }, {
      "heading" : "3.1 Our Notions of Approximation",
      "text" : "Recall that Frostig et al. [13] work only with matrices A that satisfy the eigengap assumption, that is, A has no singular value in the range [ √ λ(1− γ), √ λ(1 + γ)]. Their approximation guarantees are very straightforward:\n• an output ξ is ε-approximate for PCP on vector χ if ‖ξ − ξ∗‖ ≤ ε‖χ‖; • an output x is ε-approximate for PCR with regressand b if ‖x− x∗‖ ≤ ε‖b‖.\nUnfortunately, these notions are too strong and impossible to satisfy for matrices that do not have a large eigengap around the projection threshold λ.\nIn this paper we propose the following more general (but yet very meaningful) approximation notions.\nDefinition 3.1. An algorithm B(χ) is (γ, ε)-approximate PCP for threshold λ, if for every χ ∈ Rd\n1. ∥∥P(1+γ)λ ( B(χ)− χ )∥∥ ≤ ε‖χ‖. 2. ∥∥(I−P(1−γ)λ)B(χ)\n∥∥ ≤ ε‖χ‖. 3. ∀i such that λi ∈ [ (1− γ)λ, (1 + γ)λ ] , it satisfies |〈νi,B(χ)− χ〉| ≤ |〈νi, χ〉|+ ε‖χ‖.\nIntuitively, the first property above states that, if projected to the eigenspace with eigenvalues above (1 + γ)λ, then B(χ) and χ are almost identical; the second property states that, if projected to the eigenspace with eigenvalues below (1−γ)λ, then B(χ) is almost zero; and the third property states that, for each eigenvector νi with eigenvalue in the range [(1− γ)λ, (1 + γ)λ], the projection 〈νi,B(χ)〉 must be between 0 and 〈νi, χ〉 (but up to an error ε‖χ‖).\nNaturally, Pλ(χ) itself is a (0, 0)-approximate PCP. We propose the following notion for approximate PCR:\nDefinition 3.2. An algorithm C(b) is (γ, ε)-approximate PCR for threshold λ, if for every b ∈ Rd′\n1. ∥∥(I−P(1−γ)λ)C(b) ∥∥ ≤ ε‖b‖. 2. ‖AC(b)− b‖ ≤ ‖Ax∗ − b‖+ ε‖b‖.\nwhere x∗ = (A>A)†P(1+γ)λA>b is the exact PCR solution for threshold (1 + γ)λ.\nThe first notion states that the output x = C(b) has nearly no correlation with eigenvectors below threshold (1− γ)λ; and the second states that the regression error should be nearly optimal with respect to the exact PCR solution but at a different threshold (1 + γ)λ.\nRelationship to Frostig et al. Under eigengap assumption, our notions are equivalent to Frostig et al.: Fact 3.3. If A has no singular value in [ √ λ(1− γ), √ λ(1 + γ)], then\n• Def. 3.1 is equivalent to ‖B(χ)−Pλ(χ)‖ ≤ O(ε)‖χ‖. • Def. 3.2 implies ‖C(χ)− x∗‖ ≤ O(ε/λ)‖b‖ and ‖C(χ)− x∗‖ ≤ O(ε)‖b‖ implies Def. 3.2.\nAbove, x∗ = (A>A)†PλA>b is the exact PCR solution."
    }, {
      "heading" : "3.2 Reductions from PCR to PCP",
      "text" : "If the PCP solution ξ = Pλ(A >b) is computed exactly, then by definition one can compute (A>A)†ξ which gives a solution to PCR by solving a linear system. However, as pointed by Frostig et al. [13], this computation is problematic if ξ is only approximate. The following approach has been proposed to improve its accuracy by Frostig et al.\n• “compute p((A>A + λI)−1)ξ where p(x) is a polynomial that approximates function x1−λx .” This is a good approximation to (A>A)†ξ because the composition of functions x1−λx and 1 1+λx is\nexactly x−1. Frostig et al. picked p(x) = pm(x) = ∑m t=1 λ t−1xt which is a truncated Taylor series, and used the following procedure to compute sm ≈ pm((A>A + λI)−1)ξ:\ns0 = B(A>b), s1 = ApxRidge(A, λ, s0), ∀k ≥ 1: sk+1 = s1 + λ · ApxRidge(A, λ, sk) . (3.1) Above, B is an approximate PCP solver and ApxRidge is an approximate ridge regression solver.\nUnder the eigengap assumption, Frostig et al. [13] showed that\nLemma 3.4 (PCR-to-PCP). For fixed λ, γ, ε ∈ (0, 1), let A be a matrix whose singular values lie in [ 0, √ (1− γ)λ ] ∪ [√ (1− γ)λ, 1 ] . Let ApxRidge be any O( ε\nm2 )-approximate ridge regression\nsolver, and let B be any (γ,O( ελ m2 ))-approximate PCP solver8. Then, procedure (3.1) satisfies\n‖sm − (A>A)†PλA>b‖ ≤ ε‖b‖ if m = Θ(log(1/εγ)) .\nUnfortunately, the above lemma does not hold without eigengap assumption. In this paper, we fix this issue by proving the following analogous lemma:\nLemma 3.5 (gap free PCR-to-PCP). For fixed λ, ε ∈ (0, 1) and γ ∈ (0, 2/3], let A be a matrix whose singular values are no more than 1. Let ApxRidge be any O( ε\nm2 )-approximate ridge regres-\nsion solver, and B be any (γ,O( ελ m2\n))-approximate PCP solver. Then, procedure (3.1) satisfies, {\n‖(I−P(1−γ)λ)sm‖ ≤ ε‖b‖ , and ‖Asm − b‖ ≤ ‖A(A>A)†P(1+γ)λA>b− b‖+ ε‖b‖\n} if m = Θ(log(1/εγ))\nNote that the conclusion of this lemma exactly corresponds to the two properties in our Def. 3.2. The proof of Lemma 3.5 is not hard, but requires a very careful case analysis by decomposing vectors b and each sk into three components, each corresponding to eigenvalues of A\n>A in the range [0, (1− γ)λ], [(1− γ)λ, (1 + γ)λ] and [(1 + γ)λ, 1]. We defer the details to Appendix A.\n8Recall from Fact 3.3 that this requirement is equivalent to saying that ‖B(χ)−Pλχ‖ ≤ O( ε √ λ\nm2 )‖χ‖."
    }, {
      "heading" : "4 Property of Chebyshev Approximation Outside [−1, 1]",
      "text" : "Classical Chebyshev approximation theory (such as Lemma 2.10) only talks about the behaviors of pn(x) or gn(x) on interval [−1, 1]. However, for the purpose of this paper, we must also bound its value for x > 1. We prove the following general lemma in Appendix B, and believe it could be of independent interest: (we denote by f (k)(x) the k-th derivative of f at x)\nLemma 4.1. Suppose f(z) is analytic on Eρ and for every k ≥ 0, f (k)(0) ≥ 0. Then, for every n ∈ N, letting pn(x) and qn(x) be be the degree-n Chebyshev truncated series and Chebyshev interpolation of f(x), we have\n∀y ∈ [0, ρ] : 0 ≤ pn(1 + y), qn(1 + y) ≤ f(1 + y) .\n5 Our Polynomial Approximation of sgn(x) For fixed κ ∈ (0, 1], we consider the degree-n Chebyshev interpolation qn(x) = ∑n\nk=0 ckTk(x) of the function f(x) = ( 1+κ−x\n2\n)−1/2 on [−1, 1]. Def. 2.7 tells us that\nck def = 2− 1[k = 0] n+ 1\nn∑\nj=0\n(√ 2 cos (k(j + 0.5)π n+ 1 ))( 1 + κ− cos ((j + 0.5)π n+ 1 ))−1/2 .\nOur final polynomial to approximate sgn(x) is therefore\ngn(x) = x · qn(1 + κ− 2x2) and deg(gn(x)) = 2n+ 1 . We prove the following theorem in this section:\nTheorem 5.1. For every α ∈ (0, 1], ε ∈ (0, 1/2), choosing κ = 2α2, our function gn(x) def= x · qn(1 + κ− 2x2) satisfies that as long as n ≥ 1√2α log 3 εα2 , then (see also Figure 1)\n• |gn(x)− sgn(x)| ≤ ε for every x ∈ [−1, α] ∪ [α, 1]. • gn(x) ∈ [0, 1] for every x ∈ [0, α] and gn(x) ∈ [−1, 0] for every x ∈ [−α, 0].\nNote that our degree n = O ( α−1 log(1/αε) ) is near-optimal, because the minimum degree for a polynomial to satisfy even only the first item is Θ ( α−1 log(1/ε) ) [9, 10]. However, the results of [9, 10] are not constructive, and thus may not lead to stable matrix polynomials. We prove Theorem 5.1 by first establishing two simple lemmas. The following lemma is a consequence of Lemma 2.10:\nLemma 5.2. For every ε ∈ (0, 1/2) and κ ∈ (0, 1], if n ≥ 1√ κ ( log 1κ + log 4 ε ) then\n∀x ∈ [−1, 1], |f(x)− qn(x)| ≤ ε .\nProof of Lemma 5.2. Denoting by f(z) = (\n1+κ−z 2 )−0.5 , we know that f(z) is analytic on ellipse Eρ\nwith ρ = κ/2, and it satisfies |f(z)| ≤ √\n2/κ in Eρ. Applying Lemma 2.10, we know that when n ≥ 1√\nκ ( log 1κ + log 4 ε ) it satisfies |f(x)− qn(x)| ≤ ε.\nThe next lemma an immediate consequence of our Lemma 4.1 with f(z) = (\n1+κ−z 2\n)−0.5 :\nLemma 5.3. For every ε ∈ (0, 1/2), κ ∈ (0, 1], n ∈ N, and x ∈ [0, κ], we have\n0 ≤ qn(1 + x) ≤ (κ− x\n2\n)−1/2 .\nProof of Theorem 5.1. We are now ready to prove Theorem 5.1.\n• When x ∈ [−1, α]∪ [α, 1], it satisfies 1 + κ− 2x2 ∈ [−1, 1]. Therefore, applying Lemma 5.2 we have whenever n ≥ 1√\nκ log 6εκ = 1√ 2α log 3 εα2 it satisfies |f(1 +κ− 2x2)− qn(1 +κ− 2x2)|∞ ≤ ε. This further implies\n|gn(x)−sgn(x)| = |xqn(1+κ−2x2)−xf(1+κ−2x2)| ≤ |x||f(1+κ−2x2)−qn(1+κ−2x2)| ≤ ε .\n• When |x| ≤ α, it satisfies 1 + κ− 2x2 ∈ [1, 1 + κ]. Applying Lemma 5.3 we have ∀x ∈ [0, α] : 0 ≤ gn(x) = x · qn(1 + κ− 2x2) ≤ x · (x2)−1/2 = 1\nand similarly for x ∈ [−α, 0] it satisfies 0 ≥ gn(x) ≥ −1.\nA Bound on Chebyshev Coefficients. We also give an upper bound to the coefficients of polynomial qn(x). Its proof can be found in Appendix C, and this upper bound shall be used in our final stability analysis. Lemma 5.4 (coefficients of qn). Let qn(x) = ∑n\nk=0 ckTk(x) be the degree-n Chebyshev interpolation of f(x) = ( 1+κ−x\n2\n)−1/2 on [−1, 1]. Then, ∀i ∈ {0, 1, . . . , n} : |ci| ≤ e √ 32(i+ 1)\nκ\n( 1 + κ+ √ 2κ+ κ2 )−i"
    }, {
      "heading" : "6 Stable Computation of Matrix Chebyshev Polynomials",
      "text" : "In this section we show that any polynomial that is a weighted summation of Chebyshev polynomials with bounded coefficients, can be stably computed when applied to matrices with approximate computations. We achieve so by first generalizing Clenshaw’s backward method to matrix case in Section 6.1 in order to compute a matrix variant of Chebyshev sum, and then analyze its stability in Section 6.2 with the help from Elloit’s forward-backward transformation [8].\nRemark 6.1. We wish to point out that although Chebyshev polynomials are known to be stable under error when computed on scalars [14], it is not immediately clear why it holds also for matrices. Recall that Chebyshev polynomials satisfy Tn+1(x) = 2xTn(x) − Tn−1(x). In the matrix case, we have Tn+1(M)χ = 2MTn(M)χ− Tn−1(M)χ where χ ∈ Rd is a vector. If we analyzed this formula coordinate by coordinate, error could blow up by a factor d per iteration.\nIn addition, we need to ensure that the stability theorem holds for matrices M with eigenvalues that can exceed 1. This is not standard because Chebyshev polynomials are typically analyzed only on domain [−1, 1]."
    }, {
      "heading" : "6.1 Clenshaw’s Method in Matrix Form",
      "text" : "In the scalar case, Clenshaw’s method (sometimes referred to as backward recurrence) is one of the most widely used implementations for Chebyshev polynomials. We now generalize it to matrices.\nConsider any computation of the form\n~sN def =\nN∑\nk=0\nTk(M)~ck ∈ Rd where M ∈ Rd×d is symmetric and each ~ck is in Rd . (6.1)\n(Note that for PCP and PCR purposes, we it suffices to consider ~ck = c ′ kχ where c ′ k ∈ R is a scalar and χ ∈ Rd is a fixed vector for all k. However, we need to work on this more general form for our stability analysis.)\nVector sN can be computed using the following procedure:\nLemma 6.2 (backward recurrence). ~sN = ~b0 −M~b1 where ~bN+1 def = ~0, ~bN def = ~cN , and ∀r ∈ {N − 1, . . . , 0} : ~br def= 2M~br+1 −~br+2 + ~cr ∈ Rd ."
    }, {
      "heading" : "6.2 Inexact Clenshaw’s Method in Matrix Form",
      "text" : "We show that, if implemented using the backward recurrence formula, the Chebyshev sum of (6.1) can be stably computed. We define the following model to capture the error with respect to matrix-vector multiplications.\nDefinition 6.3 (inexact backward recurrence). Let M be an approximate algorithm that satisfies ‖M(u)−Mu‖2 ≤ ε‖u‖2 for every u ∈ Rd. Then, define inexact backward recurrence to be\nb̂N+1 def = 0, b̂N def = ~cN , and ∀r ∈ {N − 1, . . . , 0} : b̂r def= 2M ( b̂r+1 ) − b̂r+2 + ~cr ∈ Rd ,\nand define the output as ŝN def = b̂0 −M(̂b1).\nThe following theorem gives an error analysis to our inexact backward recurrence. We prove it in Appendix D.1, and the main idea of our proof is to convert each error vector of a recursion of the backward procedure into an error vector corresponding to some original ~ck.\nTheorem 6.4 (stable Chebyshev sum). For every N ∈ N∗, suppose the eigenvalues of M are in [a, b] and suppose there are parameters CU ≥ 1, CT ≥ 1, ρ ≥ 1, Cc ≥ 0 satisfying\n∀k ∈ {0, 1, . . . , N} : { ρk‖~ck‖ ≤ Cc ∧ ∀x ∈ [a, b] : |Tk(x)| ≤ CTρk and |Uk(x)| ≤ CUρk } .\nThen, if the inexact backward recurrence in Def. 6.3 is applied with ε ≤ 14NCU , we have ‖ŝN − ~sN‖ ≤ ε · 2(1 + 2NCT )NCUCc ."
    }, {
      "heading" : "7 Algorithms and Main Theorems for PCP and PCR",
      "text" : "We are now ready to state our main theorems for PCP and PCR. We first note a simple fact:\nFact 7.1. (Pλ)χ = I+sgn(S) 2 where S def = 2(A>A + λI)−1A>A− I = (A>A + λI)−1(A>A− λI).\nIn other words, for every vector χ ∈ Rd, the exact PCP solution Pλ(χ) is the same as computing (Pλ)χ = I+sgn(S) 2 χ. Thus, we can use our polynomial gn(x) introduced in Section 5 and compute gn(S)χ ≈ sgn(S)χ. Finally, in order to compute gn(S), we need to multiply S to deg(gn) vectors; whenever we do so, we call perform ridge regression once."
    }, {
      "heading" : "7.1 Our Pseudo Codes",
      "text" : "First of all, we can approximately compute Sχ for an arbitrary χ ∈ Rd. This simply uses one oracle call to ridge regression, see Algorithm 1.\nNext, since we are interested in (γ, ε)-approximate PCP, we want gn(x) to be close to sgn(x) on all eigenvalues of A>A that are outside [(1− γ)λ, (1 + γ)λ], or equivalently all eigenvalues of S outside the range\n[ − (1 + γ)− 1\n1 + (1 + γ) , 1− (1− γ) 1 + (1− γ)\n] .\nAlgorithm 1 MultS(A, λ, χ) Input: A ∈ Rd′×d; λ > 0; χ ∈ Rd. Output: a vector that approximately equals Sχ = (A>A + λI)−1(A>A− λI)χ\n1: return ApxRidge(A, λ,A>Aχ− λχ).\nSince this new interval contains [−α, α] for α def= γ/(2+γ) = γ/2−O(γ2), we can apply Theorem 5.1, which gives us a polynomial gn(x) = x · qn(1 + κ − 2x2) where κ = 2α2 = 2(γ/(2 + γ))2. We use (inexact) backward recurrence —see Lemma 6.2— to compute the Chebyshev interpolation polynomial u← qn ( (1+κ)I−2S2 ) χ. Our final output for approximate PCP is simply Su+χ2 because Pλ ≈ Sgn((1+κ)−2S 2)+I\n2 . We summarize this algorithm as QuickPCP(A, χ, λ, γ, n) in Algorithm 2.\nAlgorithm 2 QuickPCP(A, χ, λ, γ, n) Input: A ∈ Rd′×d data matrix satisfying σmax(A) ≤ 1; χ ∈ Rd, vector to project; λ > 0, eigenvalue threshold for PCP; γ ∈ (0, 2/3], PCP approximation ratio. n, number of iterations one can also ignore γ and set γ = 0, see Remark 7.5 Output: a vector ξ ∈ Rd satisfying ξ ≈ Pλ(χ). 1: γ ← max{γ, log(n)n } if γ to small, work in a γ-free regime, see Remark 7.5 2: κ← 2 ( γ/(2 + γ)\n)2 recall κ = 2α2 = 2(γ/(2 + γ))2 in our analysis 3: Define ck def = 2−1[k=0]n+1 ∑n j=0 (√ 2 cos (k(j+0.5)π n+1 ))( 1 + κ− cos ( (j+0.5)π n+1 ))−1/2 coefficients for qn(x) 4: bn+1 ← ~0, bn ← cn · χ 5: for r ← n− 1 to 0 do 6: w ← (1 + κ)br+1 − 2 · MultS(A, λ, MultS(A, λ, br+1)); w ≈ ((1 + κ)I− S2)br+1 7: br ← 2w − br+2 + cr · χ 8: end for 9: u← MultS(A, λ, b0 − w); u ≈ S(gn((1 + κ)I− S2))χ ≈ sgn(S)χ 10: return 12u+ 1 2χ output ≈ sgn(S)+I2 χ\nFinally, we apply the PCR-to-PCP reduction (see Section 3) to derive a solution for PCR from an approximate solution for PCP. See QuickPCR(A, b, λ, γ, n,m) in Algorithm 3.\nAlgorithm 3 QuickPCR(A, b, λ, γ, n,m) Input: A, λ, γ, n the same as QuickPCP; b ∈ Rd′ is the regressand vector; m is the number of iterations for PCR. choosing m = 10 it sufficient for practical purposes Output: a vector x ∈ Rd that solves approximate PCR. 1: v ← QuickPCP(A,A>b, λ, γ, n), s← v, s1 ← ApxRidge(A, λ, v); 2: for r ← 1 to m do 3: s← λ · ApxRidge(A, λ, s) + s1; 4: return s\nFact 7.2. QuickPCP calls ridge regression 2n+ 1 times and QuickPCR calls it 2n+m+ 2 times."
    }, {
      "heading" : "7.2 Our Main Theorems",
      "text" : "We first state our main theorem under the eigengap assumption, in order to provide a direct comparison to that of Frostig et al. [13].\nTheorem 7.3 (eigengap assumption). Given A ∈ Rd′×d and λ, γ ∈ (0, 1), assume that the singular values of A are in the range [0, √ (1− γ)λ] ∪ [ √ (1 + γ)λ, 1]. Given χ ∈ Rd and b ∈ Rd′, denote by ξ∗ = Pλχ and x∗ = (A>A)−1PλA>b the exact PCP and PCR solutions. If ApxRidge is an ε′-approximate ridge regression solver, then\nthe output ξ ← QuickPCP(A, χ, λ, γ, n) satisfies ‖ξ∗ − ξ‖ ≤ ε‖χ‖ if n = Θ ( γ−1 log 1γε ) and log(1/ε′) = Θ ( log 1γε ) ; the output x← QuickPCR(A, b, λ, γ, n,m) satisfies ‖x− x∗‖ ≤ ε‖b‖ if n = Θ ( γ−1 log 1γλε ) , m = Θ ( log 1γε ) and log(1/ε′) = Θ ( log 1γλε ) .\nIn contrast, the number of ridge-regression oracle calls was Θ(γ−2 log 1γε) for PCP and Θ(γ −2 log 1γλε) for PCR in [13]. We include the proof of Theorem 7.3 in Appendix E.1. Next we state our stronger theorem without the eigengap assumption.\nTheorem 7.4 (gap-free). Given A ∈ Rd′×d, λ ∈ (0, 1), and γ ∈ (0, 2/3], assume that ‖A‖2 ≤ 1. Given χ ∈ Rd and b ∈ Rd′, and suppose ApxRidge is an ε′-approximate ridge regression solver, then\n• QuickPCP outputs ξ that is (γ, ε)-approximate PCP with O ( γ−1 log 1γε ) oracle calls to ApxRidge\nas long as log(1/ε′) = Θ ( log 1γε ) .\n• QuickPCR outputs x that is (γ, ε)-approximate PCR with O ( γ−1 log 1γλε ) oracle calls to ApxRidge\nas long as elog(1/ε′) = Θ ( log 1γλε ) .\nWe make a final remark here regarding the practical usage of QuickPCP and QuickPCR.\nRemark 7.5. Since our theory is for (γ, ε)-approximations that have two parameters, the user in principle has to feed in both γ and n (in addition to other default inputs such as A, b and λ). In practice, however, it is usually sufficient to obtain (ε, ε)-approximate PCP and PCR. Therefore, our pseudocodes allow users to set γ = 0 and thus ignore this parameter γ; in such a case, we shall use γ = log(n)/n which is equivalent to setting γ = Θ(ε) because n = Θ(γ−1 log(1/γε))."
    }, {
      "heading" : "8 Experiments",
      "text" : "In the same way as [13], we conclude this paper with an empirical evaluation to demonstrate our theorems.\nDatasets. We consider synthetic and real-life datasets.\n• We generate the synthetic dataset in the same way as [13]. That is, we form a 3000 × 2000 dimensional matrix A via the SVD A = UΣV> where U and V are random orthonormal matrices and Σ contains random singular values. Among the 2000 singular values, we let half of them be randomly chosen from [0, √ 0.1(1 − a)] and the other half randomly chosen from\n[ √\n0.1(1+a), 1]. We generate vector b by adding noise to the response Ax of a random “true” x that correlates with A’s top principal components. We consider eigenvalue threshold λ = 0.1, and use a = 0, 0.01, 0.02, 0.1 in our experiments. We call these datasets random-a.\n• As for the real-life dataset, we use mnist [11]. After scaling its largest singular value to one,9 we choose the eigenvalue threshold λ = 0.0025 (or equivalently singular value threshold√ λ = 0.05). The closest singular values to this threshold are respectively 0.05027 and 0.04958.\nAlgorithms. We implemented our algorithm and Frostig et al. [13] (which we call FMMS for short) and minimized the number of calls to ridge regression in our implementations. For instance, if using our pseudocode QuickPCP, the number of ridge regression calls is 2n + 1; if using our pseudocode QuickPCR, the number of extra ridge regression calls is m + 1. We choose m = 10 in all of our experiments because the theoretical prediction of m is only a small logarithmic quantity (see Lemma 3.4 and Lemma 3.5).\nWe also implemented a practical heuristic using Krylov subspace that were found on the website [12]. We call this algorithm Krylov method for short. Krylov method transforms the covariance matrix AA> into a lower-dimensional Krylov subspace and performs exact PCP and PCR there. Similar to this paper, Krylov method also reduces PCP and PCR to multiple calls of ridge regressions.10\nWe emphasize that Krylov method has no supporting theory behind it. Since we find it performs much faster than FMMS in practice, we include it in our experiments for a stronger comparison.\nRemark 8.1. There are two main issues behind the missing theory of Krylov method.\n• Stability. If matrix-vector multiplications are only approximate, Krylov-based methods are usually unstable so one needs to replace it with other stable variants. Our polynomial approximation gn(x) can be viewed as one such stable variant. • Accuracy. To the best of our knowledge, even with exact computations, if there is no eigengap around threshold λ —which is usually the case in real life— it is unlikely that Krylov method can achieve a log(1/ε) convergence with respect to the ε-parameter in (γ, ε)-approximate PCP or PCR.11 Our experiments later (namely Figure 3(c) and 3(f)) shall also confirm on this."
    }, {
      "heading" : "8.1 Evaluation 1: With Eigengap Assumption",
      "text" : "In the first evaluation we consider matrices that satisfy the eigengap assumption. To simulate an eigengap, we use random datasets random-a with a = 0.01, 0.02, 0.1 and present our findings in Figure 2 in terms of the following three performance measures:\n• Regression Error: ‖x − x∗‖2/‖x∗‖2; where x is the output of a PCR algorithm and x∗ = (A>A)†PλA>b is the exact PCR solution. • Projection Error: ‖ξ − ξ∗‖2/‖ξ∗‖2; where ξ is the output of a PCP algorithm and ξ∗ = PλA >b is the exact PCP solution. • Denoising Error: ‖(I−Pλ)ξ‖2/‖ξ‖2; where ξ is the output of a PCP algorithm. The x-axis of these plots represent the number of calls to ridge regression, and in Figure 2 we use exact implementations of ridge regression similar to the experiments in [13]. Note that the horizontal axis starts with 0 for projection performances (second and third column) and with 10\n9This is a cheap procedure and for instance can be done by power method [13]. 10The original code [12], when working with Krylov subspace of dimension k, requires 2k calls of ridge regression. In our experiments, we improved this implementation and reduced it from 2k calls to k calls for a stronger comparison. 11This is so because Krylov method works in a smaller dimension whose so-called “Ritz values” approximate the original eigenvalues of A>A. However, this approximation cannot be “exponentially close” because there are only very few Ritz values as compared to the original eigenvalues of A>A.\nfor regression performance (first column). This is so because in order to reduce PCR to PCP one needs m+1 calls to ridge regression in QuickPCR and in our experiments we simply choose m = 10.\nWe make some important observations from these results\n• We significantly outperform FMMS for our choices of a. • Our performance degrades as a (and thus γ) decreases; this is consistent to our theory. • The performance of Krylov method fluctuates partly due to the missing theory behind it.\nThis limits the practicality of Krylov method, because it is hardly possible for the algorithm to determine when is the best time to stop the algorithm.12\n• If the fluctuation of Krylov method is ignored, it matches the performance of QuickPCP and QuickPCR. This is an interesting phenomenon and might even be a first evidence towards a theoretical proof for Krylov method."
    }, {
      "heading" : "8.2 Evaluation 2: Without Eigengap Assumption",
      "text" : "In our second evaluation we consider scenarios when there is no significant eigengap around the projection threshold λ. We consider dataset random-a for a = 0 as well as dataset mnist. This\n12Of course, if the true projection matrix Pλ is given explicitly, we can determine a good iteration to stop. However, the entire PCP problem is regarding how to compute Pλ without explicitly constructing it.\ntime, we also consider three performance measures. The first two are the same as the previous subsection, as for the third measure, we replace it with\n• denoising error (small): ‖(I−P0.81λ)ξ‖2/‖ξ‖2. We emphasize here that in gap-free scenarios, regression error, projection error, or even the quantity ‖(I − Pλ)ξ‖2 can all be very large — in the extreme case if there is an eigenvector that has exactly eigenvalue λ, then these quantities do not converge to zero. This is why our gap-free approximation definitions do not account for such quantities (see Def. 3.1 and Def. 3.2).\nIn contrast, by focusing only on eigenvectors that are less than threshold (1 − γ)λ for some γ > 0, and looking at ‖(I − P(1−γ)λ)ξ‖2, this quantity can indeed converge to ε > 0 with a speed that is O(γ−1 log(1/ε)) if our algorithm is used (see Theorem 7.4). Note that this speed was only O(γ−2 log(1/ε)) for FMMS.\nWe present our findings in Figure 3 and make some important conclusions here:\n• Our method still significantly outperforms FMMS. • In terms of denoising error, our method significantly outperforms Krylov method. This is so\nbecause, according to Remark 8.1, Krylov method cannot achieve a log(1/ε) convergence rate with respect to the ε-parameter in (γ, ε)-approximate PCP or PCR. Threfore, our method is clearly the best for denoising purposes."
    }, {
      "heading" : "8.3 Evaluation 3: Stability Test",
      "text" : "In our third evaluation, we verify that our method continues to work well even if ridge regressions are computed with moderate error. We consider two types of errors in our experiments:\nRemark. Although it seems our method is more affected by error than FMMS, we emphasize that this is because FMMS is too slow and still works in a very low-accuracy regime in the plots. (For instance, as a stable algorithm, FMMS should not be affected by error of magnitude around 10−6 when the desired accuracy is above 10−4.)\n• ridge-SVRG: we run the SVRG [17] method for 50 passes to solve each ridge regression.13 • ridge-10−k: we run exact ridge regression but randomly add noise [−10−k, 10−k] per coordinate. We present our findings in Figure 4. For cleanness, we compare only the denoising error and only\non datasets mnist, random-0 and random-0.1.14 We make the following conclusions and remarks:\n• Even with inexact ridge regression, our method still works very well. We continue to outperform FMMS significantly.\n• Compared with Krylov method, we continue to outperform it significantly in gap-free scenarios. • Although it seems our method is more affected by error than FMMS, we emphasize that this\nis because FMMS is too slow and still works in a very low-accuracy regime in the plots. (For\n13We choose the epoch length of SVRG to be 2n, and therefore full gradients are computed every 2n stochastic iterations. Each n stochastic iterations is counted as one “pass” of the data, and each full gradient computation is counted as one “pass” of the data.\n14Since mnist and random-0 are datasets without significant eigengap, we present “denoising error (small)” as defined in Section 8.2.\ninstance, as a stable algorithm, FMMS should not be affected by error of magnitude around 10−6 when the desired accuracy is above 10−4.)"
    }, {
      "heading" : "9 Conclusion",
      "text" : "We summarize our contributions. • We put forward approximate notions for PCP and PCR that do not rely on any eigengap\nassumption. Our notions reduce to standard ones under the eigengap assumption. • We design near-optimal polynomial approximation g(x) to sgn(x) satisfying (1.1) and (1.2). • We develop general stable recurrence formula for matrix Chebyshev polynomials; as a corollary,\nour g(x) can be applied to matrices in a stable manner. • We obtain faster, provable PCA-free algorithms for PCP and PCR than known results."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Yin Tat Lee for suggesting us the new title, and anonymous referees for useful suggestions. Z. Allen-Zhu is partially supported an NSF Grant, no. CCF-1412958, and a Microsoft Research Grant, no. 0518584. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of NSF or Microsoft.\nAppendix"
    }, {
      "heading" : "A Proof of Lemma 3.5",
      "text" : "Lemma 3.5. For fixed λ, ε ∈ (0, 1) and γ ∈ (0, 2/3], let A be a matrix whose singular values are no more than 1. Let ApxRidge be any O( ε\nm2 )-approximate ridge regression solver, and B be any\n(γ,O( ελ m2\n))-approximate PCP solver. Then, procedure (3.1) satisfies, {\n‖(I−P(1−γ)λ)sm‖ ≤ ε‖b‖ , and ‖Asm − b‖ ≤ ‖A(A>A)†P(1+γ)λA>b− b‖+ ε‖b‖\n} if m = Θ(log(1/εγ))\nProof of Lemma 3.5. We first notice that the approximation guarantee of B implies ‖s0‖ = ‖B(A>b)‖ ≤ ‖A>b‖+O ( ελ/m2 ) ‖b‖ ≤ 2‖b‖ . Let us consider a new exact sequence {s∗k}k≥0 where s∗0 = P(1−γ)λs0, s ∗ 1 = (A\n>A + λI)−1s∗0, ∀k ≥ 1: s∗k+1 = s∗1 + λ · (A>A + λI)−1s∗k . Step I. We first bound the error between sk and s ∗ k. We have ‖s∗k+1‖ ≤ ‖s∗1‖+λ‖A>A+λI‖2‖s∗k‖ ≤ ‖s∗1‖+ ‖s∗k‖ which implies ‖s∗k‖ ≤ k‖s∗1‖ ≤ kλ‖s∗0‖ ≤ 2kλ ‖b‖. Therefore, ‖s∗k+1 − sk+1‖ ≤ ‖s∗1 − s1‖+ λ‖(A>A + λI)−1s∗k − ApxRidge(A, λ, sk)‖\n≤ ‖s∗1 − s1‖+ λ‖(A>A + λI)−1(s∗k − sk)‖+ λ‖(A>A + λI)−1sk − ApxRidge(A, λ, sk)‖ ≤ ‖s∗1 − s1‖+ ‖s∗k − sk‖+O ( λε/m2 ) ‖sk‖ . (A.1)\nSince ‖s∗k‖ ≤ 2kλ ‖b‖ ≤ 2mλ ‖b‖ and since ‖s∗0 − s0‖ ≤ O( ελm2 )‖b‖, we can conclude from (A.1) (by telescoping sum over k = 1, . . . , k′) that\n∀k′ ∈ {0, 1, . . . ,m} : ‖s∗k′ − sk′‖ ≤ ε‖b‖ .\nStep II. We next focus on s∗k and decompose s ∗ k into three parts: for every k ≥ 0, define v1,k = P(1+γ)λs ∗ k =: P1s ∗ k, v2,k = (I−P(1−γ)λ)s∗k =: P2s∗k, v3,k = (P(1−γ)λ−P(1+γ)λ)s∗k =: P3s∗k . The update rule of s∗k tells us that\n∀i ∈ [3], k ≥ 1: vi,k = 1\nλ\nk∑\nt=1\n( λ(A>A + λI)−1Pi )t s∗0 .\nIn particular, since v2,0 = P2s ∗ 0 = 0 we always have v2,m = 0.\nAs for v1,m and v3,m, we first notice that if we denote by pk(x) def = ∑k t=1 λ t−1xt, then vi,k =\npk((A >A + λI)−1)Pis∗0. But since limk→∞ pk(x) = x 1−λx =: p(x), we have\nlim k→∞\nvi,k = p((A >A + λI)−1)Pis∗0 = (A >A)†Pis∗0 = (A >A)†vi,0 .\nAt the same time, note that the spectral norms ‖λ · (A>A+λI)−1P1‖2 and ‖λ · (A>A+λI)−1P3‖2 are both no more than 34 . (This is so because for every eigenvalue λj of A\n>A that is below λ(1−γ) we have λλ+λj ≤ λ λ+(1/3)λ = 3 4 .) Therefore, for both i = 1 and i = 3, we have\n∥∥vi,m − lim k→∞ vi,k ∥∥ ≤ 1 λ\n∞∑\nt=m+1\n‖λ · (A>A + λI)−1Pi‖t2 · ‖s∗0‖ ≤ (3/4)m ·O ( ‖b‖/λ ) .\nIn other words, choosing m = Θ(log(1/ελ)), we have\n‖v1,m − (A>A)†v1,0‖ ≤ ε‖b‖ and ‖v3,m − (A>A)†v3,0‖ ≤ ε‖b‖ . (A.2)\nStep III. We now take into account the error of the PCP solver B. For v1,m, we have: ‖v1,m − (A>A)†P1A>b‖ ≤ ‖v1,m − (A>A)†v1,0‖+ ‖(A>A)†P1(B(A>b)−A>b)‖\n≤ ε‖b‖+ 1 λ ‖P1\n( B(A>b)−A>b ) ‖ ≤ 2ε‖b‖ , (A.3)\nwhere the first inequality uses triangle inequality, the second uses (A.2), and the third uses Def. 3.1 and ‖A>b‖ ≤ ‖b‖.\nAs for v3,m, we let A = UΣV > be the SVD of A and let Σ† be the same matrix Σ except all\nnon-zero elements get inverted. We have\n‖A(v3,m − (A>A)†P3A>b)‖ ¬ ≤ ∥∥∥A ( (A>A)†v3,0 − (A>A)†P3A>b )∥∥∥+ ‖Av3,m −A(A>A)†v3,0‖  ≤ ∥∥∥A ( (A>A)†v3,0 − (A>A)†P3A>b )∥∥∥+ ε‖b‖\n= ∥∥∥UΣ†V>P3 ( B(A>b)−A>b )∥∥∥+ ε‖b‖ ® ≤ ∥∥∥Σ†V>P3 ( B(A>b)−A>b )∥∥∥+ ε‖b‖ = ∑\ni:λi∈[(1−γ)λ,(1+γ)λ]\n1√ λi |〈vi,B(A>b)−A>b〉|+ ε‖b‖\n¯ ≤\n∑\ni:λi∈[(1−γ)λ,(1+γ)λ]\n1√ λi |〈vi,A>b〉|+ 2ε‖b‖\n= ‖(A>A)†P3A>b‖+ 2ε‖b‖ . (A.4) Above, ¬ uses triangle inequality,  uses (A.2) and the fact ‖A‖2 ≤ 1, ® uses ‖U‖2 ≤ 1, ¯ uses Def. 3.1 and ‖A>b‖ ≤ ‖b‖.\nStep IV. Finally we put everything together and bound the regression error. Denote by opt = ‖A(A>A)†P(1+γ)λA>b− b‖. If we decompose b as\nb = ( 3∑\ni=1\nA(A>A)†PiA>b ) + (b−A(A>A)†A>b) , (A.5)\nthen the four vectors in (A.5) are orthogonal to each other, which gives us\nopt = ‖A(A>A)†P1A>b− b‖ = ‖A(A>A)†P2A>b‖+ ‖A(A>A)†P3A>b‖+ ‖A(A>A)†A>b− b‖ . (A.6)\nNow we compute the regression error with respect to s∗m:\n‖As∗m − b‖ ¬ = ‖A(v1,m + v3,m)− b‖  =\n∥∥∥∥∥A(v1,m + v3,m)− 3∑\ni=1\nA(A>A)†PiA>b+ (b−A(A>A)†A>b) ∥∥∥∥∥\n® ≤ ∥∥∥A(v1,m − (A>A)†P1A>b) ∥∥∥+ ∥∥∥A(v3,m − (A>A)†P3A>b) ∥∥∥\n+‖A(A>A)†P2A>b‖+ ‖A(A>A)†A>b− b‖ ¯ ≤ 4ε‖b‖+ ‖A(A>A)†P2A>b‖+ ‖A(A>A)†P3A>b‖+ ‖A(A>A)†A>b− b‖ ° = opt + 4ε‖b‖ .\nAbove, ¬ is because v2,m = 0;  uses (A.5); ® uses triangle inequality; ¯ uses (A.3) and (A.4); ° uses (A.6).\nFinally, using ‖s∗m− sm‖ ≤ ε‖b‖ we complete the proof that ‖Asm− b‖ ≤ opt+ 5ε‖b‖. We also have ‖P2sm‖ ≤ ε‖b‖+ ‖P2s∗m‖ = ε‖b‖ because P2s∗m = v2,m = 0."
    }, {
      "heading" : "B Appendix for Section 4",
      "text" : "Lemma 4.1. Suppose f(z) is analytic on Eρ and for every k ≥ 0, f (k)(0) ≥ 0. Then, for every n ∈ N, letting pn(x) and qn(x) be be the degree-n Chebyshev truncated series and Chebyshev interpolation of f(x), we have\n∀y ∈ [0, ρ] : 0 ≤ pn(1 + y), qn(1 + y) ≤ f(1 + y) . To show Lemma 4.1 we first need an auxiliary lemma, which can be proved by some careful\ncase analysis (see Appendix B.1). Lemma B.1. Let m,n ∈ N be two integers, then am,n = ∫ 1 −1 xm√ 1−x2Tn(x)dx ≥ 0.\nLemma B.1 essentially says that the Chebyshev coefficients of any function xm must be all non-negative. We also recall the following lemma regarding high-order derivatives of Chebyshev truncated series:\nLemma B.2 (cf. Theorem 21.1 of [23]). Suppose f(z) is analytic on Eρ with ρ > 0, and let pn(x) be the degree-n Chebyshev truncated series of f(x). Then, for every k ≥ 0,\nlim n→+∞ max x∈[−1,1]\n{ |f (k)(x)− p(k)n (x)| } = 0 .\nWe are now ready to prove Lemma 4.1. The main idea is to expand f into its Taylor series, and then deal with monomials xm one by one:\nProof of Lemma 4.1. Since f (k)(0) ≥ 0 for all k ≥ 0, and since f(z) is analytic, we can write f as f(z) = ∑∞ k=0 rkz\nk where each rk is a nonnegative real. Consider the i-th coefficient of Chebyshev series:\nai = 2− 1[i = 0]\nπ\n∫ 1\n−1 f(x)√ 1− x2 Ti(x)dx = 2− 1[i = 0] π\n∞∑\nk=0\nrk\n∫ 1\n−1 xk√ 1− x2 Ti(x) ≥ 0\nwhere the last inequality is due to Lemma B.1, and the integral and infinite Taylor sum are interchangeable.15 This implies we can write pn(x) = ∑n i=0 aiTi(x) where each ai ≥ 0. Since each Ti(1+y) is a polynomial of degree i, it exactly equals to its degree-i Taylor expansion∑i k=0 yk k! T (k) i (1). Thus, we have (recall y ∈ [0, ρ])\npn(1 + y) =\nn∑\ni=0\naiTi(1 + y) = n∑\ni=0\ni∑\nk=0\nai k! T (k)i (1)yk =\nn∑\nk=0\n1\nk!\n( n∑\ni=k\naiT (k)i (1) ) yk .\nDenote by bk,n = (∑n i=k aiT (k) i (1) ) . Since for every i, k ≥ 0 it satisfies T (k)i (1) ≥ 0 (which is a factual property of Chebyshev polynomial) and ai ≥ 0, we know bk,n ≥ 0 and moreover bk,n is monotonically non-decreasing in n for each k ≥ 0. On the other hand, Lemma B.2 implies\nlim n→∞\n∣∣p(k)n (1)− f (k)(1) ∣∣ = lim\nn→∞\n∣∣bk,n − f (k)(1) ∣∣ = 0 ,\nso we must have 0 ≤ bk,n ≤ f (k)(1) for every n ∈ N (because bk,n is non-decreasing in n). Therefore, for every y ∈ [0, ρ]:\n0 ≤ pn(1 + y) = n∑\nk=0\n1 k! bk,ny\nk ≤ ∞∑\nk=0\n1 k! bk,ny\nk ≤ ∞∑\nk=0\n1 k! f (k)(1)yk = f(1 + y) . (B.1)\nFinally, since qn(x) def = ∑n\nk=0 ckTk(x) is a degree-n Chebyshev interpolation polynomial, the aliasing Lemma 2.8 tells us ci ≥ 0 for every i = 0, 1, . . . , n. Furthermore, applying the aliasing Lemma 2.8 again we have ci ≥ ai for i = 0, 1, . . . , n but ∑n i=0 ci = ∑∞ i=0 ai. Therefore, using the fact that T (k)i (1) is a monotone increasing function in i (for every fixed k), we have\n0 ≤ n∑\ni=0\nciT (k)i (1) ≤ ∞∑\ni=0\naiT (k)i (1) = limn→∞ bk,n = f (k)(1) .\nFinally, an analogous proof as (B.1) also shows 0 ≤ qn(1 + y) ≤ f(1 + y) for every y ∈ [0, ρ].\nB.1 Proof of Lemma B.1 Lemma B.1. Let m,n ∈ N be two integers, then am,n = ∫ 1 −1 xm√ 1−x2Tn(x)dx ≥ 0.\n15The interchangeability and be verified as follows. Denoting by fm(x) def = ∑m k=0 rmx\nm, we have fm(x) uniformly converges to f(x) on x ∈ [−1, 1] because the Taylor expansion of any analytical function has local uniform convergence, but [−1, 1] is a compact, closed interval so local uniform convergence becomes global uniform convergence.\nFor every ε > 0, let M be the integer so that for every m ≥ M it satisfies maxx∈[−1,1] |fm(x) − f(x)| ≤ ε. We compute that ∣∣∣ ∫ 1 −1 f(x)√ 1−x2 Ti(x)dx − ∑m k=0 rk ∫ 1 −1 xk√ 1−x2 Ti(x)dx ∣∣∣ = ∣∣∣ ∫ 1 −1 f(x)−fm(x)√ 1−x2 Ti(x)dx ∣∣∣ ≤ ∫ 1 −1 ε√ 1−x2 dx = επ. Therefore, the left hand side converges to zero so the integral and the infinite Taylor sum are interchangeable.\nProof of Lemma B.1. Recall that Tn(−x) = (−1)nTn(x). Therefore,\nam,n =\n∫ −1\n1\n(−x)m√ 1− x2\nTn(−x)d(−x) = (−1)m+n ∫ 1\n−1 xm√ 1− x2 Tn(x)dx = (−1)m+nam,n ,\nwhich implies that when m+ n is odd it satisfies am,n = 0. We next focus on the case when m+ n is even. We first consider two base cases:\n• n = 0,m = 2k: we have x2k ≥ 0 for all x ∈ [−1, 1] so am,n = a2k,0 = ∫ 1 −1 x2k√ 1−x2dx ≥ 0. • n = 1,m = 2k+ 1: we have x2k+2 ≥ 0 for all x ∈ [−1, 1] so am,n = a2k+1,1 = ∫ 1 −1 x2k+2√ 1−x2dx ≥ 0.\nAs for general n ≥ 2, we integrate by parts and have:\nam,n =\n∫ 0\n−π\ncosm(θ)\nsin θ cos(nθ)d(cos θ) =\n∫ π\n0 cosm(θ) cos(nθ)dθ\n= 1\nn cosm(θ) sin(nθ)\n∣∣∣∣ π\n0\n− 1 n\n∫ π\n0 (−m sin(θ) cosm−1(θ)) sin(nθ)dθ\n= m\nn\n∫ π\n0 sin(θ) cosm−1(θ) sin(nθ)dθ\n= m n2 sin(θ) cosm−1(θ)(− cos(nθ)) ∣∣∣∣ π\n0\n− m n2\n∫ π\n0 (−(m− 1) cosm−2 θ +m cosm θ)(− cos(nθ))dθ\n= −m(m− 1) n2 am−2,n + m2 n2 am,n\n=⇒ (m2 − n2)am,n = m(m− 1)am−2,n . (B.2) In particular, choosing m = n in (B.2) we have an−2,n = 0, and this implies\n∀m ≤ n : am−2,n = m2 − n2 m(m− 1)am,n = 0 .\nAs for an,n for n ≥ 1, we have\nan,n =\n∫ 1\n−1 xn√ 1− x2\nTn(x)dx = ∫ 1\n−1 xn√ 1− x2 Tn+1(x) + Tn−1(x) 2x dx = 1 2 (an−1,n+1 + an−1,n−1) = 1 2 an−1,n−1\nand thus by induction we have an,n ≥ 0. Using (B.2) again we conclude that\n∀m ≥ n+ 2: am,n = m(m− 1) m2 − n2 am−2,n ≥ 0 ."
    }, {
      "heading" : "C Proof of Lemma 5.4",
      "text" : "We first note the following lemma which follows from Lemma 2.10 together with the aliasing Lemma 2.8: Lemma C.1. Suppose f(z) is analytic on Eρ and |f(z)| ≤ M on Eρ. Let qn(x) = ∑n\ni=0 ciTi(x) be the degree-n Chebyshev interpolation of f , then\n∀i ∈ {0, 1, . . . , n} : |ci| ≤ 2M\nρ+ √ 2ρ+ ρ2\n( 1 + ρ+ √ 2ρ+ ρ2 )−i .\nApplying Lemma C.1 on f(z) = (\n1+κ−z 2\n)−1/2 , we have\nLemma 5.4. Let qn(x) = ∑n k=0 ckTk(x) be the degree-n Chebyshev interpolation of f(x) = ( 1+κ−x 2 )−1/2 on [−1, 1]. Then,\n∀i ∈ {0, 1, . . . , n} : |ci| ≤ e √ 32(i+ 1)\nκ\n( 1 + κ+ √ 2κ+ κ2 )−i\nProof of Lemma 5.4. For each i ∈ {0, 1, . . . , n}, consider a value ρ ∈ [κ/2, κ) to be chosen later. We know that f(z) is analytic and satisfies |f(z)| ≤ √ 2\nκ−ρ on Eρ. Using Lemma C.1 we have:\n|ci| ≤\n√ 8\nκ−ρ\n( 1 + ρ+ √ 2ρ+ ρ2 )−i\nρ+ √ 2ρ+ ρ2 ≤ 1√ κ\n√ 8\nκ− ρ ( 1 + ρ+ √ 2ρ+ ρ2 )−i , (C.1)\nwhere we used κ ≤ 1 in the second inequality. If we take ρ = κ− κ4(i+1) , we have: (\n1 + κ+ √ 2κ+ κ2 1 + ρ+ √ 2ρ+ ρ2 )i = ( 1 + κ− ρ+ √ 2κ+ κ2 − √ 2ρ+ ρ2 1 + ρ+ √ 2ρ+ ρ2 )i\n≤ ( 1 + (κ− ρ) ( 1 + 2 + κ+ ρ√\n2κ\n))i\n≤ (\n1 + (κ− ρ) 4√ κ\n)i ≤ ( 1 + 1\ni+ 1\n)i ≤ e .\nPutting this back to (C.1), we have: |ci| ≤ √ 32(i+ 1)\nκ\n( 1 + κ+ √ 2κ+ κ2\n1 + ρ+ √ 2ρ+ ρ2\n)i ( 1 + κ+ √ 2κ+ κ2 )−i ≤ e √ 32(i+ 1)\nκ\n( 1 + κ+ √ 2κ+ κ2 )−i ."
    }, {
      "heading" : "D Appendix for Section 6",
      "text" : "Lemma 6.2. ~sN = ~b0 −M~b1 where ~bN+1 def = ~0, ~bN def = ~cN , and ∀r ∈ {N − 1, . . . , 0} : ~br def= 2M~br+1 −~br+2 + ~cr ∈ Rd .\nProof. We write ~sN = t >c where t = (T0(M), . . . , TN (M))> and c = (~c0, . . . ,~cN )>. Recall that the recursive formula of Chebyshev polynomial tells us\nNt def =   I −2M I I −2M I . . . . . . . . .\nI −2M I\n    T0(M) T1(M) T2(M)\n... TN (M)\n  =   I −M\n0 ... 0\n  def = w .\nIn addition, it is easy to verify that the ~br sequence satisfies N >b = c if we denote by b def= (~b0, . . . ,~bN ) >. Therefore, we have ~sN = t>c = t>N>b = w>b = ~b0 −M~b1 as desired. Fact D.1. ~br = ∑N k=r Uk−r(M)~ck for every r ∈ {0, 1, . . . , N + 1}.\nProof. This can be deduced directly from the recursive formula of Chebyshev polynomials of the second kind. See for instance Equation (3.120) of [14].\nD.1 Proof of Theorem 6.4\nTheorem 6.4. For every N ∈ N∗, suppose the eigenvalues of M are in [a, b] and suppose there are parameters CU ≥ 1, CT ≥ 1, ρ ≥ 1, Cc ≥ 0 satisfying ∀k ∈ {0, 1, . . . , N} : { ρk‖~ck‖ ≤ Cc ∧ ∀x ∈ [a, b] : |Tk(x)| ≤ CTρk and |Uk(x)| ≤ CUρk } .\nThen, if the inexact backward recurrence in Def. 6.3 is applied with ε ≤ 14NCU , we have ‖ŝN − ~sN‖ ≤ ε · 2(1 + 2NCT )NCUCc . Proof of Theorem 6.4. We first note that according to ~bn = ∑N\nk=n Uk−n(M)~ck from Fact D.1, we have\n∀n ∈ {0, 1, . . . , N} : ‖~bn‖ ≤ N∑\nk=n\n∥∥Uk−n(M) ∥∥ 2 ‖~ck‖ ≤ (N − n+ 1) · ρ−n · CUCc . (D.1)\nDenoting by ~ηr def = M ( b̂r ) −Mb̂r, we have\n∀r ∈ {N − 1, . . . , 0} : b̂r = 2Mb̂r+1 − b̂r+2 + ~cr + 2~ηr+1 and ŝN = b̂0 + Mb̂1 + ~η1 , and therefore if we denote by (δ~b)r = b̂r −~br, we have\n(δ~b)N+1 = 0, (δ~b)N = 0, and ∀r ∈ {N − 1, . . . , 0} : (δ~b)r = 2M(δ~b)r+1 − (δ~b)r+2 + 2~ηr+1 . In other words, the {(δ~b)r}r sequence also satisfies the recursive formula in Lemma 6.2 where ~ck is replaced with 2~ηk+1. This implies, according to Lemma 6.2,\n(δ~b)0 −M(δ~b)1 = 2 N−1∑\nk=0\nTk(M)~ηk+1\nand therefore\nŝN − ~sN = (δ~b)0 −M(δ~b)1 + ~η1 = ~η1 + 2 N−1∑\nk=0\nTk(M)~ηk+1 (D.2)\nAt the same time, applying Fact D.1 on sequence {(δ~b)r}r, we have\n(δ~b)r = 2 N−1∑\nk=r\nUk−r(M)~ηk+1 (D.3)\nNow we are ready to prove that, as long as ε ≤ 14NCU , it satisfies\n∀k ∈ [N ] : ‖~ηk‖ ≤ ε · ρ−k(2NCUCc) and ‖(δ~b)k−1‖ ≤ ε · ρ−k(4N2C2UCc) We prove this by reverse double induction.\n• In the base case, ‖~ηN‖ ≤ ε‖b̂N‖ = ε‖~bN‖ ≤ ερ−NCUCc where the first inequality uses our assumption on M and the second uses (D.1). • Suppose the upper bound ‖~ηk‖ ≤ ε · ρ−k(2NCUCc) holds for every k ≥ k0, then\n‖(δ~b)k0−1‖ ≤ 2 N−1∑\nk=k0−1\n∥∥∥Uk−k0+1(M)~ηk+1 ∥∥∥ ≤ 2CUρ−k0 · N−1∑\nk=k0−1 ‖ρk+1~ηk+1‖\n≤ 2NCUρ−k0 · (ε · 2NCUCc) = ε · ρ−k0(4N2C2UCc) . Above, the first inequality is by (D.3) and triangle inequality, the second is by the definition of CU , the third is by inductive assumption.\n• Suppose the upper bound ‖(δ~b)k−1‖ ≤ ε · ρ−k(4N2C2UCc) holds for k = k0 + 1, then ‖~ηk0‖ ≤ ε‖b̂k0‖ ≤ ε ( ‖~bk0‖+ ‖(δ~b)k0‖ ) ≤ ερ−k0 ( NCUCc + 4ερ −1N2C2UCc )\n= ερ−k0NCUCc(1 + 4ρ−1εNCU ) ≤ 2ρ−k0NCUCc . Above, the first inequality is by our assumption on M, the second is by triangle inequality, the third is by (D.1) and our inductive assumption, and the last is by our assumption on ε.\nFinally, using (D.2), we have\n‖ŝN − ~sN‖ ≤ ‖~η1‖+ 2 N−1∑\nk=0\n∥∥Tk(M)‖2‖~ηk+1‖ ≤ ε(2NCUCc) + 2NεCT (2NCUCc)\n≤ ε · 2(1 + 2NCT )NCUCc ."
    }, {
      "heading" : "E Appendix for Section 7",
      "text" : "Fact 7.1. (Pλ)χ = I+sgn(S) 2 where S def = 2(A>A + λI)−1A>A− I = (A>A + λI)−1(A>A− λI). Proof. This is so because S shares the same eigenspace as A>A and maps all the eigenvalues of A>A above threshold λ to eigenvalues of S between 0 and 1, and all the eigenvalues below λ to eigenvalues of S between −1 and 0. Therefore, if applied to function sgn(x)+12 , we have that sgn(S)+I\n2 zeros out all the eigenvalues of S between −1 and 0, and thus equivalently zeros out all the eigenvalues of A>A below threshold λ. This is exactly the same as the projection matrix Pλ.\nE.1 Proof of Theorem 7.3\nTheorem 7.3 (restated). Given A ∈ Rd′×d and λ, γ ∈ (0, 1), assume that the singular values of A are in the range [0, √ (1− γ)λ]∪[ √ (1 + γ)λ, 1]. Given χ ∈ Rd and b ∈ Rd′, denote by ξ∗ = Pλχ and x∗ = (A>A)−1PλA>b the exact PCP and PCR solutions. If ApxRidge is an ε′-approximate ridge regression solver, then\nthe output ξ ← QuickPCP(A, χ, λ, γ, n) satisfies ‖ξ∗ − ξ‖ ≤ ε‖χ‖ if n = Θ ( γ−1 log 1γε ) and log(1/ε′) = Θ ( log 1γε ) ; the output x← QuickPCR(A, b, λ, γ, n,m) satisfies ‖x− x∗‖ ≤ ε‖b‖ if n = Θ ( γ−1 log 1γλε ) , m = Θ ( log 1γε ) and log(1/ε′) = Θ ( log 1γλε ) .\nProof of Theorem 7.3. The eigenvalues of S def = (A>A + λI)−1(A>A− λI) are in the range\n[ − 1,−(1 + γ)− 1 1 + (1− γ) ] ∪ [1− (1− γ) 1 + (1 + γ) , 1− λ 1 + λ ] ⊆ [ − 1,−α ] ∪ [ α, 1 ] .\nbecause α = γ/(2+γ). Therefore, according to Theorem 5.1, gn(S)χ satisfies ‖gn(S)χ−sgn(S)χ‖ ≤ ε‖χ‖ for every χ ∈ Rd which in turns implies ‖12(gn(S) + I)χ−Pλχ‖ ≤ ε‖χ‖.\nWe now analyze stability. Denote by M = (1 + κ)I − 2S2 and recall that gn(S) = Sqn(M) = Sqn ( (1 + κ)I − 2S2 ) where κ = 2α2. We wish to apply Theorem 6.4 to show that qn(M)χ can be computed in a stable manner and therefore gn(S)χ as well. We verify the assumptions of Theorem 6.4 below:\n• Since ApxRidge is ε′-approximate (see Def. 2.3), we have that Line 6 of QuickPCP corresponds to an approximate algorithm\nM(χ) = (1 + κ)χ− 2MultS(A, λ, MultS(A, λ, χ)) satisfying ‖Mχ−M(χ)‖ ≤ O(ε′)‖χ‖ for every vector χ. • Recall that qn(·) is a Chebyshev sum with coefficients at most O(1/ √ κ) = O(1/α) = O(1/γ)\naccording to Def. 2.7. Thus, we can choose ρ = 1 and Cc = O(1/γ) in Theorem 6.4.\n• Since the eigenvalues of M are in [−1, 1] and |Tk(x)| ≤ 1 and |Uk(x)| ≤ n + 1 for every x ∈ [−1, 1] (see Fact 2.6), we can choose CT = 1 and CU = n+ 1 in Theorem 6.4.\nThe conclusion of Theorem 6.4 tells us that our approximate backward recurrence in QuickPCP computes gn(S)χ up to an accuracy O(ε\n′γ−1n3) · ‖χ‖. In other words, as long as log(1/ε′) ≤ O(log nεγ ), we can approximately compute 1 2(gn(S) + I)χ within accuracy O(ε) · ‖χ‖.\nCombining everything above, we conclude that choosing n = Θ(γ−1 log(1/γε)) and log(1/ε′) = Θ(log nεγ ) = Θ(log 1 εγ ), we can satisfy ‖ξ∗ − ξ‖ ≤ ε‖χ‖.\nAs for the PCR guarantee, we simply replace ε with ε · √ λ/m2 and χ with A>b in the above analysis. Then we apply Lemma 3.4, and conclude that choosing n = Θ(γ−1 log(1/γλε)), m = Θ(log(1/εγ)), and log(1/ε′) = Θ(log 1εγ ), it satisfies ‖x∗ − x‖ ≤ ε‖b‖.\nE.2 Proof of Theorem 7.4\nTheorem 7.4 (restated). Given A ∈ Rd′×d, λ ∈ (0, 1), and γ ∈ (0, 2/3], assume that the singular values of A are no more than 1. Given χ ∈ Rd and b ∈ Rd′, and suppose ApxRidge is an ε′-approximate ridge regression solver, then\nthe output ξ ← QuickPCP(A, χ, λ, γ, n) is (γ, ε)-approximate PCP if n = Θ ( γ−1 log 1γε ) and log(1/ε′) = Θ ( log 1γε ) ; the output x← QuickPCR(A, b, λ, γ, n,m) is (γ, ε)-approximate PCR if n = Θ ( γ−1 log 1γλε ) , m = Θ ( log 1γε ) and log(1/ε′) = Θ ( log 1γλε ) .\nProof of Theorem 7.4. Consider the same S = (A>A + λI)−1(A>A − λI), α = γ/(2 + γ), and κ = 2α2 as before. We observe that A>A and S share the same eigenspace. Furthermore, the eigenvalues of A>A in the range\n(1) : [(1 + γ)λ, 1] (2) : [0, (1− γ)λ] (3) : ( (1− γ)λ, (1 + γ)λ ) (E.1)\nrespectively map to the eigenvalues of S in the range16\n(1) : [α, 1] (2) : [−1,−α] (3) : (−1, 1)\nLet us now write χ = ∑3\ni=1 ∑ k∈Λi βkνk where Λi ⊆ [d] consists of the indices k where λk is in\nthe i-th interval in (E.1), and βk ∈ R is the weight. We thus have\nξ′ def= gn(S) + I\n2 χ =\n3∑\ni=1\n∑\nk∈Λi\ngn(λk) + λk 2 βkνk .\n16More precisely, given transformation f : x 7→ x−λ x+λ\n, it satisfies (1) f ( [(1 + γ)λ, 1] ) ⊆ [α, 1], (2) f ( [0, (1 − γ)λ] ) ⊆\n[−1,−α], and (3) f ( ((1− γ)λ, (1 + γ)λ) ) ⊆ (−1, 1).\nSince for every k ∈ Λ1 ∪Λ2 it satisfies λk ∈ [−1,−α]∪ [α, 1], we can apply Theorem 5.1 (and using n ≥ 1√\n2α log 3 εα2 ):\n1. ‖P(1+γ)λ(ξ′ − χ)‖ = ‖ ∑ k∈Λ1 (gn(λk)+λk 2 − 1 ) βkνk‖ ≤ ε‖χ‖. 2. ‖(I−P(1−γ)λ)ξ′‖ = ‖ ∑ k∈Λ2 (gn(λk)+λk 2 ) βkνk‖ ≤ ε‖χ‖. 3. ∀k ∈ Λ3, |〈νi, ξ′ − χ〉| = ∣∣gn(λk)+λk 2 − 1 ∣∣ · |βk| ≤ |βk| = |〈νi, χ〉|. (Here, the last inequality is\nbecause if λk ≥ 0 then gn(λk) +λk ∈ [λk, 1 +λk] and if λk < 0 then gn(λk) +λk ∈ [λk−1, λk].) Note that these two guarantees correspond to the three properties for approximate PCP (see Def. 3.1), and thus we are left to deal with stability by applying Theorem 6.4. In other words, denoting by M = (1+κ)I−2S2 and recalling that gn(S) = Sqn(M), we wish to apply Theorem 6.4 to show that qn(M)χ can be computed in a stable manner and therefore qn(S)+I 2 χ as well. We verify the assumptions of Theorem 6.4 below:\n• As before, Line 6 of QuickPCP corresponds to an approximate algorithm M(χ) satisfying ‖Mχ−M(χ)‖ ≤ O(ε′)‖χ‖ for every vector χ. • qn(·) is a Chebyshev sum with coefficients satisfying |ci| ≤ O( √ i/κ) ( 1 + κ + √ 2κ+ κ2\n)−i according to Lemma 5.4. Therefore, we can choose ρ = 1 + κ+ √ 2κ+ κ2 and Cc = O(n/κ) = O(n/γ2) in Theorem 6.4.\n• Since the eigenvalues of M are in [−1, 1 + κ], we have for every x ∈ [−1, 1 + κ], it satisfies |Tk(x)| ≤ (1 + κ + √ 2κ+ κ2)k and |Uk(x)| ≤ 12√2κ+κ2 (1 + κ + √ 2κ+ κ2)n+1. Therefore, we\ncan choose CT = 1 and CU = O( 1 κ) in Theorem 6.4.\nFinally, the conclusion of Theorem 6.4 tells us that our approximate backward recurrence in QuickPCP computes qn(S)χ up to an accuracy O(ε\n′γ−4n3) · ‖χ‖. In other words, as long as log(1/ε′) ≤ O(log nεγ ), we can approximately compute ξ′ = 12(gn(S) + I)χ within accuracy ε · ‖χ‖, or equivalently ‖ξ − ξ′‖ ≤ ε‖χ‖. Together with our analysis at the beginning of the proof, we have\n1. ‖P(1+γ)λ(ξ − χ)‖ ≤ 2ε‖χ‖. 2. ‖(I−P(1−γ)λ)ξ‖ ≤ 2ε‖χ‖. 3. ∀k ∈ Λ3, |〈νi, ξ − χ〉| ≤ |βk| = |〈νi, χ〉|+ ε‖χ‖.\nThis finishes proving that ξ is an (γ,O(ε))-approximate PCP solution when n = Θ ( γ−1 log 1γε ) and\nlog(1/ε′) = Θ ( log 1γε ) .\nAs for the PCR guarantee, we simply replace ε with ε · λ/m2 and χ with A>b in the above analysis. Then we apply Lemma 3.5, and conclude that choosing n = Θ(γ−1 log(1/ελγ)), m = Θ(log(1/εγ)), and log(1/ε′) = Θ(log 1εγ ), it satisfies that x is a (γ, ε)-approximate PCR solution."
    } ],
    "references" : [ {
      "title" : "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods",
      "author" : [ "Zeyuan Allen-Zhu" ],
      "venue" : "In STOC,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2017
    }, {
      "title" : "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition",
      "author" : [ "Zeyuan Allen-Zhu", "Yuanzhi Li" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "First Efficient Convergence for Streaming k-PCA: a Global, Gap- Free, and Near-Optimal Rate",
      "author" : [ "Zeyuan Allen-Zhu", "Yuanzhi Li" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain",
      "author" : [ "Zeyuan Allen-Zhu", "Yuanzhi Li" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Even faster accelerated coordinate descent using non-uniform sampling",
      "author" : [ "Zeyuan Allen-Zhu", "Peter Richtárik", "Zheng Qu", "Yang Yuan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "Faster SVD-truncated regularized least-squares",
      "author" : [ "Christos Boutsidis", "Malik Magdon-Ismail" ],
      "venue" : "IEEE International Symposium on Information Theory,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Computing truncated singular value decomposition least squares solutions by rank revealing QR-factorizations",
      "author" : [ "Tony F Chan", "Per Christian Hansen" ],
      "venue" : "SIAM Journal on Scientific and Statistical Computing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1990
    }, {
      "title" : "Error analysis of an algorithm for summing certain finite series",
      "author" : [ "David Elliott" ],
      "venue" : "Journal of the Australian Mathematical Society,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1968
    }, {
      "title" : "Uniform approximation of sgn x by polynomials and entire functions",
      "author" : [ "Alexandre Eremenko", "Peter Yuditskii" ],
      "venue" : "Journal d’Analyse Mathématique,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "Polynomials of the best uniform approximation to sgn (x) on two intervals",
      "author" : [ "Alexandre Eremenko", "Peter Yuditskii" ],
      "venue" : "Journal d’Analyse Mathématique,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "LIBSVM Data: Classification, Regression and Multi-label",
      "author" : [ "Rong-En Fan", "Chih-Jen Lin" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Principal Component Projection Without Principal Component Analysis",
      "author" : [ "Roy Frostig", "Cameron Musco", "Christopher Musco", "Aaron Sidford" ],
      "venue" : "In ICML,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Numerical Methods for Special Functions. Society for Industrial and Applied Mathematics, jan",
      "author" : [ "Amparo Gil", "Javier Segura", "Nico M. Temme" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "Approximating the spectral sums of large-scale matrices using chebyshev approximations",
      "author" : [ "Insu Han", "Dmitry Malioutov", "Haim Avron", "Jinwoo Shin" ],
      "venue" : "arXiv preprint arXiv:1606.00942,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Functions of Matrices",
      "author" : [ "N. Higham" ],
      "venue" : "Society for Industrial and Applied Mathematics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Randomized block krylov methods for stronger and faster approximate singular value decomposition",
      "author" : [ "Cameron Musco", "Christopher Musco" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Computing fundamental matrix decompositions accurately via the matrix sign function in two iterations: The power of zolotarev’s functions",
      "author" : [ "Yuji Nakatsukasa", "Roland W Freund" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Introductory Lectures on Convex Programming Volume: A Basic course, volume I",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Model order reduction: theory, research aspects and applications, volume",
      "author" : [ "Wilhelmus H.A. Schilders", "Henk A. Van der Vorst", "Joost Rommes" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "An introduction to the conjugate gradient method without the agonizing pain",
      "author" : [ "Jonathan Richard Shewchuk" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1994
    }, {
      "title" : "Approximation Theory and Approximation Practice",
      "author" : [ "Lloyd N. Trefethen" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "Numerical methods for the qcdd overlap operator",
      "author" : [ "Jasper van den Eshof", "Andreas Frommer", "Th Lippert", "Klaus Schilling", "Henk A. van der Vorst" ],
      "venue" : "i. sign-function and error bounds. Computer Physics Communications,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "For instance, to project a vector onto the top 1000 principal components of a high-dimensional dataset, even the most efficient Krylov-based [18] or Lanczos-based [4] methods require a running time that is proportional to 1000 × 40 = 4 × 104 times the input matrix sparsity, if the Krylov or Lanczos method is executed for 40 iterations.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "For instance, to project a vector onto the top 1000 principal components of a high-dimensional dataset, even the most efficient Krylov-based [18] or Lanczos-based [4] methods require a running time that is proportional to 1000 × 40 = 4 × 104 times the input matrix sparsity, if the Krylov or Lanczos method is executed for 40 iterations.",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 11,
      "context" : "[13] that approximate PCP and PCR be solved with a running time independent of the number of principal components above threshold λ.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "This is known as the eigengap assumption, which is rarely satisfied in practice [18].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "In other words, • to project any vector χ ∈ Rd to top principal components, we can compute g(S)χ instead; and Ridge regression is often considered as an easy-to-solve machine learning problem: using for instance SVRG [17], one can usually solve ridge regression to an 10−8 accuracy with at most 40 passes of the data.",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 15,
      "context" : "Efficient routines such as SVRG [17] solve ridge regression and thus compute Su for any u ∈ Rd, with running times only logarithmically in 1/ε′.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 8,
      "context" : "1) is Θ ( γ−1 log(1/ε) ) [9, 10].",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "1) is Θ ( γ−1 log(1/ε) ) [9, 10].",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "There are a few attempts to reduce the cost of PCA when solving PCR, by for instance approximating the matrix APλ where Pλ is the PCP projection matrix [6, 7].",
      "startOffset" : 152,
      "endOffset" : 158
    }, {
      "referenceID" : 6,
      "context" : "There are a few attempts to reduce the cost of PCA when solving PCR, by for instance approximating the matrix APλ where Pλ is the PCP projection matrix [6, 7].",
      "startOffset" : 152,
      "endOffset" : 158
    }, {
      "referenceID" : 1,
      "context" : "A significant number of papers have focused on the low-rank case of PCA [2, 4, 18] and its online variant [3].",
      "startOffset" : 72,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "A significant number of papers have focused on the low-rank case of PCA [2, 4, 18] and its online variant [3].",
      "startOffset" : 72,
      "endOffset" : 82
    }, {
      "referenceID" : 16,
      "context" : "A significant number of papers have focused on the low-rank case of PCA [2, 4, 18] and its online variant [3].",
      "startOffset" : 72,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "A significant number of papers have focused on the low-rank case of PCA [2, 4, 18] and its online variant [3].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 19,
      "context" : "Several results have addressed Krylov methods for applying the sign function in the so-called Krylov subspace, without explicitly constructing any approximate polynomial [21, 24].",
      "startOffset" : 170,
      "endOffset" : 178
    }, {
      "referenceID" : 22,
      "context" : "Several results have addressed Krylov methods for applying the sign function in the so-called Krylov subspace, without explicitly constructing any approximate polynomial [21, 24].",
      "startOffset" : 170,
      "endOffset" : 178
    }, {
      "referenceID" : 14,
      "context" : "4 Other iterative methods have also been proposed, see Section 5 of textbook [16].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "[19]) provide rational approximations to the matrix sign function as opposed to polynomial approximations.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[13] differ from these cited works, because we have only accessed an approximate ridge regression oracle, so ensuring a polynomial approximation to the sign function and ensuring its stability are crucial.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "Perhaps the most celebrated example is to approximate S−1 using polynomials on S, used in the analysis of conjugate gradient [22].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 13,
      "context" : "[15] used Chebyshev polynomials to approximate the trace of the matrix sign function, i.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "Following the tradition of [13] and keeping the notations light, we assume without loss of generality that σmax(A) ≤ 1.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 20,
      "context" : "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 18,
      "context" : "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 15,
      "context" : "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.",
      "startOffset" : 224,
      "endOffset" : 227
    }, {
      "referenceID" : 21,
      "context" : "6 ([23]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 21,
      "context" : "2 of [23]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 21,
      "context" : "2 of [23]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 11,
      "context" : "[13] work only with matrices A that satisfy the eigengap assumption, that is, A has no singular value in the range [ √ λ(1− γ), √ λ(1 + γ)].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[13], this computation is problematic if ξ is only approximate.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[13] showed that Lemma 3.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "• gn(x) ∈ [0, 1] for every x ∈ [0, α] and gn(x) ∈ [−1, 0] for every x ∈ [−α, 0].",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : "Note that our degree n = O ( α−1 log(1/αε) ) is near-optimal, because the minimum degree for a polynomial to satisfy even only the first item is Θ ( α−1 log(1/ε) ) [9, 10].",
      "startOffset" : 164,
      "endOffset" : 171
    }, {
      "referenceID" : 9,
      "context" : "Note that our degree n = O ( α−1 log(1/αε) ) is near-optimal, because the minimum degree for a polynomial to satisfy even only the first item is Θ ( α−1 log(1/ε) ) [9, 10].",
      "startOffset" : 164,
      "endOffset" : 171
    }, {
      "referenceID" : 8,
      "context" : "However, the results of [9, 10] are not constructive, and thus may not lead to stable matrix polynomials.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "However, the results of [9, 10] are not constructive, and thus may not lead to stable matrix polynomials.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 7,
      "context" : "2 with the help from Elloit’s forward-backward transformation [8].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "We wish to point out that although Chebyshev polynomials are known to be stable under error when computed on scalars [14], it is not immediately clear why it holds also for matrices.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 11,
      "context" : "[13].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "In contrast, the number of ridge-regression oracle calls was Θ(γ−2 log 1 γε) for PCP and Θ(γ −2 log 1 γλε) for PCR in [13].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "In the same way as [13], we conclude this paper with an empirical evaluation to demonstrate our theorems.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 11,
      "context" : "• We generate the synthetic dataset in the same way as [13].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "• As for the real-life dataset, we use mnist [11].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "[13] (which we call FMMS for short) and minimized the number of calls to ridge regression in our implementations.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "The x-axis of these plots represent the number of calls to ridge regression, and in Figure 2 we use exact implementations of ridge regression similar to the experiments in [13].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 11,
      "context" : "Note that the horizontal axis starts with 0 for projection performances (second and third column) and with 10 This is a cheap procedure and for instance can be done by power method [13].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 15,
      "context" : "• ridge-SVRG: we run the SVRG [17] method for 50 passes to solve each ridge regression.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "The update rule of sk tells us that ∀i ∈ [3], k ≥ 1: vi,k = 1 λ k ∑",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 21,
      "context" : "1 of [23]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 12,
      "context" : "120) of [14].",
      "startOffset" : 8,
      "endOffset" : 12
    } ],
    "year" : 2017,
    "abstractText" : "We solve principal component regression (PCR), up to a multiplicative accuracy 1 + γ, by reducing the problem to Õ(γ−1) black-box calls of ridge regression. Therefore, our algorithm does not require any explicit construction of the top principal components, and is suitable for large-scale PCR instances. In contrast, previous result requires Õ(γ−2) such black-box calls. We obtain this result by developing a general stable recurrence formula for matrix Chebyshev polynomials, and a degree-optimal polynomial approximation to the matrix sign function. Our techniques may be of independent interests, especially when designing iterative methods.",
    "creator" : "LaTeX with hyperref package"
  }
}