{
  "name" : "1702.06081.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Non-Discriminatory Predictors",
    "authors" : [ "Blake Woodworth", "Suriya Gunasekar", "Mesrob I. Ohannessian" ],
    "emails" : [ "blake@ttic.edu", "suriya@ttic.edu", "mesrob@ttic.edu", "nati@ttic.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Machine learning algorithms are increasingly deployed in important decision making tasks that affect people’s lives significantly. These tools already appear in domains such as lending, policing, criminal sentencing, and targeted service offerings. In many of these domains, it is morally and legally undesirable to discriminate based on certain “protected attributes” such as race and gender. Even in seemingly innocent applications, such as ad placement and product recommendations, such discrimination might be illegal or detrimental. Consequently, there has been abundant public, academic and technical interest in notions of non-discrimination and fairness, and achieving “equal opportunity by design” is a major United States national Big Data challenge, [White House, 2016].\nWe consider non-discrimination in supervised learning where the goal is to learn a (potentially randomized) predictor h(X) or1 h(X,A) for a target quantity Y ∈ Y using features X ∈ X and a protected attribute A ∈ A, while ensuring non-discrimination with respect to A. As an illustrative example, consider a financial institution that wants to predict whether a particular individual will pay back a loan or not, corresponding to Y = 1 or Y = 0. The features X could include financial as well as other information, e.g. about education, driving, and housing history, languages spoken, and the number of members in the household, all of which have a potential of being used inappropriately as a surrogate for the protected attribute, A, such as gender or race. It is important that the predictor for loan repayment not be even implicitly “discriminatory” with respect to A.\nThe particular notion of non-discrimination we consider here is “equalized odds”, recently presented and studied by Hardt et al. [2016]:\nDefinition 1 (Equalized Odds). A possibly randomized predictor Ŷ = h(X,A) for target quantity Y is non-discriminatory with respect to a protected attribute A if Ŷ is independent of A conditioned on Y .\nInformally, we require that even if the correct label Y provides information about the protected attribute A, if we already know Y , the prediction does not provide any additional information about\n1See Hardt et al. [2016] for a discussion on why it might be necessary for a non-discriminatory predictor to use A\nar X\niv :1\n70 2.\n06 08\n1v 1\n[ cs\n.L G\n] 2\n0 Fe\nb 20\n17\nA. The definition can also be motivated in terms incentive structure and of moving the burden of uncertainty from the protected population to the decision maker. See Hardt et al. [2016] for further discussion of the definition, its implications, and comparisons to alternative notions.\nIn a binary prediction task when Ŷ , A, Y ∈ {0, 1}, Definition 1 can be interpreted in terms of true and false positive rates. Denote the group-conditional true and false positive rates as,\nγya(Ŷ ) := P(Ŷ = 1 | Y = y,A = a), (1)\nThen Definition 1 is equivalent to requiring that the class conditional true and false positive rates agree on different “groups” (different values of A):\nγ00(Ŷ ) = γ01(Ŷ ) and γ10(Ŷ ) = γ11(Ŷ ) (2)\nReturning to the loan example, this definition requires that the percentage of men who are wrongly denied loans even though they would have paid it back must be the same as the corresponding percentage for women, and that the percentage of men who are wrongly given loans that they will not pay back must match the percentage of women. This does not however require that the same percentage of male and female applicants will receive loans. For instance, if women pay back loans with truly higher frequency than men, then the predictor would be allowed to deny loans to men more often than women.\nWhile Hardt et al. focused on the notion itself and how it behaves on the population, here we tackled the problem of how to learn a good non-discriminatory predictor (i.e. satisfying the equalized odds Definition 1) from a finite training set. We examine this both from a statistical perspective, of how to best obtain a predictor from finite data that would be as good and non-discriminatory as possible on the population, and from a computational perspective.\nOne possible approach to learning a non-discriminative predictor is post hoc correction [Hardt et al., 2016]: first learn a good predictor ignoring non-discrimination, i.e. a possibly discriminatory predictor. Afterwards, this predictor is “corrected” by taking into account A in order to make the predictor non-discriminatory. When Y is binary and the predictors Ŷ are real-valued, they show that the unconstrained Bayes optimal least-square regressor can be post hoc corrected to the optimal predictor with respect to the 0-1 loss. In Section 2, we consider more carefully the limitations of such a post hoc procedure. In particular, we show that this approach can fail for the 0-1 and hinge losses, even if the Bayes optimal predictor with respect to those losses is learned in the first step. We also show that even when minimizing the squared loss, the approach can fail once we limit ourselves to a specific hypothesis class, as is essential when learning from finite data. From this, we conclude that post hoc correction is not sufficient, and that it is necessary to directly incorporate non-discrimination into the learning process.\nTurning to learning from finite data, we cannot hope to ensure exact non-discrimination on the population. To this end, in Section 3 we define a notion of approximate non-discrimination, motivate it, and explore its limits by analyzing the statistical problem of detecting whether or not a predictor is at least α-discriminatory.\nWe then turn to the main statistical question: given a finite training set, how can we best learn a predictor that is ensured to be as non-discriminatory as possible (on the population) and competes (in terms of its population loss) with the best non-discriminatory predictor in some given hypothesis class (this is essentially an extension of the notion of agnostic PAC learning with a non-discrimination constraint). In Section 4 we show that an ERM-type procedure, minimizing the training error subject to an empirical non-discrimination constraint, is statistically sub-optimal, and instead we present a statistically optimal (up to constant factors) two-step learning procedure for non-discriminatory binary classification.\nUnfortunately, learning a non-discriminatory binary classifier is computationally hard, which we prove in Section 5. In order to allow tractable training, in Section 6, we present a relaxation of equalized odds, based only on a second-moment condition instead of full conditional independence. We show that under this second moment notion of non-discrimination it is computationally tractable to learn a nearly optimal non-discriminatory linear predictor with respect to a convex loss."
    }, {
      "heading" : "2 Sub-optimality of Post-training Correction",
      "text" : "When the protected attribute A and the target Y are both binary, the post hoc correction algorithm proposed by Hardt et al. [2016] can be applied to a binary or real-valued predictor Ŷ ∈ H, deriving a randomized binary predictor that is non-discriminatory. The algorithm is convenient because it requires access only to the joint distribution over (Ŷ , A, Y ) and does not use the features X, thus it can be applied retroactively to an already trained predictor. Such predictors are formulated using the notion of a derived predictor:\nDefinition 2. [Definition 4.1 Hardt et al. [2016]] A predictor Ỹ is derived from a random variable R and protected attribute A if it is a possibly randomized function of (R,A) alone. In particular, Ỹ is independent of X conditioned on (R,A).\nFor binary classification problems, the optimal post hoc correction Ỹ for a binary or real valued predictor Ŷ ∈ R is a straightforward ternary optimization problem: it is simply the nondiscriminatory, derived, binary predictor that minimizes the expectation of loss ` (Hardt et al. [2016]):\nỸ = argmin f :R×{0,1}7→{0,1}\nE ` ( f(Ŷ , A), Y ) s.t. γy0(f) = γy1(f) ∀y = {0, 1}\n(3)\nTwo notable features of the corrected predictor Ỹ are that a) it is not constrained to any particular hypothesis class, and b) it may be a random function of Ŷ and A; indeed for many distributions and hypothesis classes there may not even exist a non-constant, deterministic, non-discriminatory predictor. Nevertheless, Ỹ does indirectly depend on the hypothesis class from which Ŷ was learned and the loss function used to train Ŷ .\nWe are interested in comparing the optimality of Ỹ from post hoc correction to the following Y ∗\nwhich is the optimal non-discriminatory predictor in the hypothesis class H under consideration:\nY ∗ = argmin h∈H E ` (h(X,A), Y ) s.t. γy0(h) = γy1(h) ∀y = {0, 1} (4)\nIdeally, the expected loss of Ỹ would compare favorably against that of Y ∗. Indeed, Hardt et al. [2016] show that when the target Y is binary, if we can first find a predictor R that is exactly or nearly Bayes optimal for the squared loss over an unconstrained hypothesis class, then applying the post hoc correction (3) using the 0-1 loss (i.e. with ` = `01 in (3)) to R will yield a predictor Ỹ that is non-discriminatory and has loss no worse than Y ∗. This statement can be extended to the case of first finding the optimal unconstrained predictor with respect to any strictly convex loss, and then using the post hoc correction with the 0-1 loss.\nNevertheless, from a practical perspective this approach is very unsatisfying. First, for general distributions, it is impossible to learn the Bayes optimal predictor from finite samples of data. Also, as we will show, the post hoc correction of the optimal predictor with respect to the 0-1 or hinge losses can have much worse performance than the best non-discriminatory predictor, even when\nthe hypothesis class in unconstrained. Moreover, if the hypothesis class is restricted there can also be a gap between the post hoc correction of the optimal predictor in the hypothesis class and the best non-discriminatory predictor, even when optimizing a strictly convex loss function.\nIn the following example, we see that when the loss function is not strictly convex, the post hoc correction of even the unconstrained Bayes optimal predictor can have poor accuracy:\nExample 1. When the hypothesis class is unconstrained, for any ∈ (0, 1/4) there exists a distribution D such that a) the optimal non-discriminatory predictor with respect to the 0-1 loss has loss at most 2 but b) the post hoc correction of the unrestricted Bayes optimal predictor has loss at least 0.5.\nA similar statement can also be made about hinge loss. For an unconstrained hypothesis class, for any ∈ (0, 1/4) and the same distribution D , a) the optimal non-discriminatory predictor with respect to the hinge loss has loss at most 4 but b) the post hoc correction of the Bayes optimal unrestricted predictor has loss at least 1.\nWe construct D as follows:\nX\nY\nA X,A, Y ∈ {0, 1} PD (Y = 1) = 1\n2 PD (A = y | Y = y) = 1− PD (X = y | Y = y) = 1− 2\n(5)\nBoth X and A are highly predictive of Y , but A is slightly more so. Therefore, minimizing either the 0-1 or the hinge loss requires returning A and ignoring X entirely. Consequently, γy1 = 1 = 1− γy0 so the optimal predictor is 1-discriminatory, and the post hoc correction is forced to return a terrible predictor even though returning X would be accurate and non-discriminatory. A more detailed proof is included in Appendix A.1\nIn the second example, we show that when the hypothesis class is restricted, the correction of the optimal regressor in the class can yield a suboptimal classifier, even with squared loss.\nExample 2. Let H be the class of linear predictors with L1 norm at most 12 − 2 , for some ∈ (2/25, 1/4). Then there exists a distribution D such that a) the optimal non-discriminatory predictor in H with respect to the squared loss has square loss at most 116 + 3 2 + 3\n2, but b) the post hoc correction of the Bayes optimal square loss regressor in H returns a constant predictor which has (trivial) square loss of 1/4.\nSimilarly, for the class H of sparse linear predictors, for any ∈ (0, 1/4), there exists a distribution D such that a) the optimal non-discriminatory predictor in H with respect to the squared loss has square loss at most 2 − 4 2, but b) the post hoc correction of the Bayes optimal squared loss regressor in H again returns a constant predictor which has (trivial) square loss of 1/4.\nThe distribution D is the same as was defined in (5). Again, A is slightly more predictive of Y than X, and since the sparsity or the sparsity surrogate L1 norm of the predictor is constrained by the hypothesis class, the Bayes optimal predictor chooses to use just the feature A and ignore X. Consequently, the predictor is extremely discriminatory, and the post hoc correction algorithm will return a highly sub-optimal constant predictor which performs no better than chance. Details of the proof are deferred to Appendix A.2\nFrom these examples, it is clear that simply finding the optimal predictor with respect to a particular loss function and hypothesis class and correcting it post hoc can perfom very poorly. We conclude that in order to learn a predictor that is simultaneously accurate and non-discriminatory in the general case, it is essential to account for non-discrimination during the learning process."
    }, {
      "heading" : "3 Detecting Discrimination in Binary Predictors",
      "text" : "In the following sections, we look at tools for integrating non-discrimination into the supervised learning framework. In formulating algorithms for learning accurate and non-discriminatory predictors, it is important to address the issues that arise from dealing with finite data.\nTo begin with, using finite samples, we can never hope to ensure or even verify if a predictor Ŷ satisfies the non-discrimination criterion in Definition 1. This necessitates defining a notion of approximate non-discrimination which generalizes to the equalized odds criteria in the population, but can also be computed using finite samples.\nLet us consider the task of binary classification, where both A, Y ∈ {0, 1} and the predictors Ŷ output values in {0, 1}. Recall the definition of the population class-conditional true and false positive rates γya(Ŷ ) = P(Ŷ = 1 | Y = y,A = a) and the fact that non-discrimination is equivalent to the condition that γ00 = γ01 and γ10 = γ11. For a set of of n i.i.d. samples, S = {(xi, ai, yi)}ni=1 ∼ Pn(X,A, Y ), the sample analogue of γya is defined as follows,\nγSya(Ŷ ) = 1\nnSya n∑ i=1 Ŷ (xi, ai)1(yi = y, ai = a), , where n S ya = n∑ i=1 1(yi = y, ai = a). (6)\nTo ensuring non-discrimination, we could possibly require γSy0 = γ S y1 on a large enough sample S, however this not sufficient for two reasons. First, in general it is not feasible to match γSy0 = γ S y1\nexactly for nSy0 6= nSy1, e.g. if nSy0 = 2 and nSy1 = 3, then γSy0 ∈ { 0, 12 , 1 } but γSy1 ∈ { 0, 13 , 2 3 , 1 }\n, thus only predictors which are constant conditioned on Y = y, i.e. either the constant or the perfect predictor, would be non-discriminatory. Second, even when γSy0 = γ S y1 on S, this almost certainly does not ensure that γy0 = γy1 on the population. For this same reason, it is impossible to be certain that a given predictor is non-discriminatory on the population.\nFor these reasons, we define a notion of approximate non-discrimination, which is possible to ensure on a sample and, when it holds on a sample, generalizes to the population.\nDefinition 3. A possibly randomized binary predictor Ŷ is said to be α-discriminatory with respect to a binary protected attribute A on the population or a sample S if\nΓ(Ŷ ) := max y∈{0,1} ∣∣∣γy0(Ŷ )− γy1(Ŷ )∣∣∣ ≤ α or ΓS(Ŷ ) := max y∈{0,1} ∣∣∣γSy0(Ŷ )− γSy1(Ŷ )∣∣∣ ≤ α. (7) The decision to define approximate non-discrimination in terms of conditional rather than joint probabilities is important, particularly in the case that the a, y pairs occur with widely varying frequencies. For example, if approximate non-discrimination were defined in terms of the joint probabilities P (Ŷ = ŷ, A = a, Y = y) and if P (A = 0, Y = 1) = α/10, then a predictor could be “α-discriminatory” all while being arbitrarily unfair towards the A = 0, Y = 1 population. This issue does not arise when using Definition 3. Nevertheless, with this definition, approximate non-discrimination becomes more difficult to ensure as the least frequent group becomes rarer.\nGiven this definition, we propose a simple statistical test to test the hypothesis that a given predictor Ŷ is at most α-discriminatory on the population for some α > 0. Let S = {(xi, ai, yi)}ni=1 ∼ Pn(X,Y,A) denote a set of n i.i.d. samples, and for y, a ∈ {0, 1}, let Pya = P(Y = y,A = a). We propose the following test for detecting α-discrimination:\nT ( Ŷ , S, α ) = 1 ( ΓS(Ŷ ) > α\n2\n) (8)\nLemma 1. Given n i.i.d. samples S, ∀α, δ > 0, if n > 16 maxya log 16/δPyaα2 , then with probability greater than 1− δ, T satisfies,\nT ( Ŷ , S, α ) =\n{ 0 if Ŷ is 0-discriminatory on population\n1 if Ŷ is at least α-discriminatory on population.\nThe proof is based on concentration of ΓS , and is provided in Appendix B.1."
    }, {
      "heading" : "4 Integrated Learning of Non-Discriminatory Binary Predictors",
      "text" : "In Example 1, we saw that even though an almost perfect non-discriminatory predictor exists within the hypothesis class, if we ignore non-discrimination in training with 0-1 loss, then optimal post hoc correction using (3) yields a poor predictor with no better than chance accuracy. Thus, to find a predictor that both is nearly non-discriminatory and has nearly optimal loss for general hypothesis classes, it is necessary to incorporate non-discrimination criteria into the learning process. For a hypothesis class H, we would ideally find the optimal non-discriminatory predictor:\nY ∗ = argmin h∈H E` (h(X,A), Y ) s.t. γy0(h) = γy1(h) ∀y = {0, 1} . (9)\nHowever, as motivated in Section 3, it is impossible to learn 0-discriminatory predictors from finite samples. In this section, we address following question: given access to n i.i.d samples, what level of approximate non-discrimination and accuracy is it possible to ensure in a learned predictor?\nWe propose a two step framework for learning a non-discriminatory binary predictor that minimizes the expected 0-1 loss L(Ŷ ) = E `01(Ŷ , Y ) = E1(Ŷ 6= Y ) over a binary hypothesis class H = {h : X → {0, 1}}. Broadly, the two-step framework is as follows:\n1. Non-discrimination in training: Estimate an almost non-discriminatory empirical risk minimizer Ŷ by incorporating approximate non-discrimination constraints on the samples. 2. Post-training correction: With additional samples from (Ŷ , Y, A), derive a randomized predictor Ỹ to further reduce discrimination."
    }, {
      "heading" : "4.1 Two step framework for binary predictors",
      "text" : "We partition the training data consisting of n independent samples S = {(xi, ai, yi) ∼ P(X,A, Y )}Ni=1 into two subsets S1 and S2 to be used in Step 1 and Step 2, respectively\n2. For a predictor h ∈ H and y, a ∈ {0, 1}, recall notation for the population and sample groupconditional true and false positive ratesγya(h) and γ S ya(h) from (1) and (6), respectively. Additionally, for S1 and S2, let n Sk ya = ∑ i∈Sk 1(yi = y, ai = a). In general the subsets need not be of equal size, but for simplicity, let |S1| = |S2| = n/2.\nStep 1: Non-discrimination in training\nFor the first step, we estimate an empirical risk minimizing predictor Ŷ ∈ H, subject to the constraint that Ŷ be αn-discriminatory on S1, where αn is a tunable hyperparameter:\nŶ = argmin h∈H\n2\nn ∑ i∈S1 `01(h(xi), yi)\ns.t. ΓS1(h) = max y∈{0,1} ∣∣∣γS1y0 (h)− γS1y1 (h)∣∣∣ < αn. (10) 2We occasionally overload the S to also denote the indices [n], e.g. i ∈ S to denote the ith sample (xi, ai, yi) ∈ S.\nStep 2: Post-training correction\nAs Step 2, we propose a post hoc correction to Ŷ estimated in Step 1 for improved non-discrimination. Using samples in S2, which are independent of S1, we estimate the best randomized predictor Ỹ derived from (Ŷ , A) (Definition 2). Let P(Ŷ ) denote the set of randomized binary predictors that can be derived solely from P(Ŷ , A, Y ) and let α̃n be a tunable hyperparameter, then Ỹ is given by,\nỸ = argmin Ỹ ∈P(Ŷ )\n2\nn ∑ i∈S2 E Ỹ `01(Ỹ (xi), yi)\ns.t. ΓS2(Ỹ ) = max y∈{0,1} ∣∣∣γS2y0 (Ỹ )− γS2y1 (Ỹ )∣∣∣ < α̃n, (11) where for a randomized predictor Ỹ , the group-conditional probabilities on a sample is defined to be γS2ya (Ỹ ) = 1\nn S2 ya\n∑ i∈S2 EỸ 1(Ỹi = 1, Yi = y,Ai = a). The above optimization problem is a finite\nsample adaptation of the post hoc correction in (3) proposed by Hardt et al. [2016]. As with the post hoc correction on the population (3), estimating a predictor Ỹ ∈ P(Ŷ ) derived from (Ŷ , A) is simply optimization over the following four parameters that completely specify Ỹ ,\np̃ŷa := p̃ŷa(Ỹ ) = P(Ỹ = 1 | Ŷ = ŷ, A = a) for ŷ, a ∈ {0, 1}. (12)\nIn Section 4.2, we discuss how the post hoc correction step offers statistical advantages over the one-shot approach of using all of the training data for Step 1. Besides these statistical advantages, the addition of the post hoc correction is also motivated from other practical considerations: (a) The derived predictors only need access to samples from P(Ŷ , Y, A) and can be deployed without explicit access to predictive features X, and (b) the proposed correction step (11) can be easily optimized using ternary search and can be repeated multiple times as more and more samples from P(Ŷ , Y, A) are seen by the system, without having to retrain the classifier from scratch."
    }, {
      "heading" : "4.2 Statistical guarantees",
      "text" : "In this section, we discuss the statistical properties of the estimators Ŷ and Ỹ from Step 1 and Step 2, respectively. We define the following notation for succinctly describing the quality of a predictor:\nDefinition 4. Q(L, ) = {h : X → {0, 1} : Γ(h) ≤ ,L(h) ≤ L} denotes the set of -discriminatory binary predictors with loss L.\nThe following theorem shows the statistical learnability of hypothesis classes H with respect to the best non-discriminatory predictor in H using the two-step framework. In the following results, recall the notation for Γ(Ŷ ) from Definition 3.\nFor y, a ∈ {0, 1}, let Pya = P(Y = y,A = a) denote the group-outcome probabilities, and let V C(H) denote the Vapnik-Chervonenkis dimension of a hypothesis class H.\nTheorem 1. Let n=Ω ( maxya\nlog 1/δ Pya\n) and the hyperparameters satisfy αn, α̃n=Θ ( maxya √ log 1/δ nPya ) .\nFor a binary hypothesis class H, any distribution P(X,Y,A), and any δ ∈ (0, 1/2), if Y ∗ ∈ H is a non-discriminatory predictor, then with probability greater than 1− δ, the output of the two step procedure Ỹ satisfies the following for absolute constants C1 and C2,\nL(Ỹ ) ≤ L(Y ∗) + C1 max ya\n√ V C(H) + log 1/δ\nnPya , and Γ(Ỹ ) ≤ C2 max ya\n√ log 1/δ\nnPya .\nThus, with n = Ω (\nmaxya V C(H) Pya 2 + maxya 1 Pyaα2\n) samples, and an appropriate choice of αn, α̃n,\nthe two step framework returns Ỹ ∈ Q(L(Y ∗) + , α) with high probability.\nThe proof is based on the following two Lemmas. The first is a statistical guarantee on the the loss and non-discrimination after training in Step 1:\nLemma 2. Under the conditions in Theorem 1, w.p. greater than 1− δ, Ŷ from Step 1 satisfies\nL(Ŷ ) ≤ L(Y ∗) + C1\n√ V C(H) + log 1/δ\nn , and Γ(Ŷ ) ≤ C2 max ya\n√ V C(H) + log 1/δ\nnPya .\nNotice that the non-discrimination we can guarantee for Ŷ also scales with the complexity of the hypothesis class V C(H). In the second step, we search over a much more restricted space of derived predictors, essentially the convex hull of |A| conditional predictors of Ŷ , which allows us to obtain a guarantee on non-discrimination that does not scale with V C(H). The following lemma ensures that if Ŷ from Step 1 is approximately non-discriminatory, the correction in the second step does not incur significant additional loss:\nLemma 3. If h is an α-discriminatory binary predictor h ∈ Q(L(h), α), then the optimal 0- discriminatory derived predictor Ỹ ∗(h) from (3) using 0-1 loss satisfies Ỹ ∗(h) ∈ Q(L(h) + α, 0).\nThis lemma, along with the examples in Section 2 further motivates an integrated leaning step, such as Step 1, where non-discrimination is explicitly encouraged in training.\nThe following theorem shows that the level of non-discrimination that it is possible to ensure using just the first step of the procedure can grow with the complexity of the hypothesis class.\nTheorem 2. There exists a finite, binary hypothesis class H and a data distribution D such that with probability at least 1/2, the classifier Ŷ learned from Step 1 using n samples from D is at least maxy,a 3 log |H|−1 5\n4nPya -discriminatory on the population.\nTheorem 2 suggests that the non-discrimination guarantee provided by Lemma 2 has the correct dependence on the problem parameters. The intermediate predictor Ŷ from Step 1, without the post hoc correction, can only be guaranteed to be non-discriminatory up to a tolerance term that grows with the complexity of the hypothesis class. On the other hand, when using the two step procedure, the sample complexity of ensuring that the final predictor Ỹ is at most α-discriminatory is Ω(maxya α\n−2P−1ya ) (Theorem 1) which does not depend on the complexity of the hypothesis class, and also matches the sample complexity of merely detecting α-discrimination from Lemma 1.\nFinally, we note that the sample complexity’s dependence on minya Pya in Theorem 1 is unavoidable for our definition of approximate non-discrimination. If there is a rare group or group-outcome combination, we still need enough samples from that group to ensure that the loss and γSya generalize to the popluation for every A = a, Y = y in order to be non-discriminatory. This is the same reason why the dependence on Pya arises in Lemma 1. On the positive side, this bottleneck provides further incentive to actively seek samples and target labels for minority populations, which might otherwise be disregarded if non-discrimination were not a consideration."
    }, {
      "heading" : "5 Computational Intractability",
      "text" : "The proposed procedure for learning non-discriminatory predictors from a sample is statistically optimal, but it is clearly computationally intractable for almost any interesting hypothesis class\nsince the first step (10) involves minimizing the 0-1 loss. As is typically done with intractable learning problems, we therefore look to alternative loss functions and hypothesis classes in order to find a computationally feasible procedure.\nA natural choice is the hypothesis class of linear predictors with a convex loss function. In this case, we would like to have an efficient algorithm for finding a non-discriminatory predictor that has convex loss that is approximately as good as the loss of the best non-discriminatory linear predictor. However, even in the case of binary A and Y and without the difficulty of optimizing the 0-1 loss, the non-discrimination constraint is extremely strong for real-valued predictors, requiring that the cumulative distribution functions of the predictor conditioned on A = 0 and A = 1 match at every threshold. In fact, the mere existence of a non-trivial (i.e. non-constant) linear predictor that is non-discriminatory requires a relatively special distribution. This is the case even when considering a real-valued analogue of α-approximate non-discrimination.\nOne could relax the problem one step further with a less restrictive non-discrimination requirement. Consider the class of linear predictors with a convex loss where only the sign of the predictor need be non-discriminatory. Unfortunately, using a result by Daniely [2015] even this is computationally intractable:\nTheorem 3. Let L∗ be the loss of the optimal linear predictor whose sign is non-discriminatory. Subject to the assumption that refuting random K-XOR formulas is computationally hard,3 the learning problem of finding a possibly randomized function f such that Lhinge(f) ≤ L∗+ such that sign(f) is α-discriminatory requires exponential time in the worst case for < 18 and α < 1 8 .\nThe proof goes through a reduction from the hardness of improper, agnostic PAC learning of Halfspaces. Given a distribution D over (X,Y ) and the knowledge that there is a linear predictor which achieves 0-1 loss L∗ on D, we construct a new distribution D̃ over (X̃, Ã, Ỹ ) such that an approximately non-discriminatory predictor with small hinge loss can be used to make accurate predictions on D, even if it is not a linear function. The distribution D̃ is identical to the original distribution D when conditioned on Ã = 1, and is supported on only two points conditioned on Ã = 0. The probabilities of the two points are constructed so that satisfying non-discrimination requires making accurate predictions on the Ã = 1 population, and thus on D. In particular, for parameters , α < 18 , the predictor will have 0-1 loss at most 15 16L\n∗ + 47128 on D, which is bounded away from 12 when L\n∗ < 110 . Since Daniely [2015] prove that finding a predictor with accuracy bounded away from 12 is hard in general, we conclude that the learning problem is computationally hard. See Appendix D for a complete proof.\nTo summarize, learning a non-discriminatory binary predictor with the 0-1 loss is hard; learning a real-valued linear predictor with respect to a convex loss function is problematic due to the potential inexistence of a non-trivial non-discriminatory linear predictor; and even requiring only that the sign of the linear predictor be non-discriminatory is computationally hard. A more significant relaxation of non-discrimination is therefore required to arrive at a computationally tractable learning problem."
    }, {
      "heading" : "6 Relaxing non-discrimination",
      "text" : "Motivated by the hardness result in Section 5, we now proceed to relax the criterion of equalized odds. Previous work by Zafar et al. [2016] addresses a notion of non-discrimination which amounts to relaxing the equalized odds constraint that P(Ŷ = ŷ | Y = y,A = 0) = P(Ŷ = ŷ | Y = y,A = 1) to the constraint that E[Ŷ | Y = y,A = 0] = E[Ŷ | Y = y,A = 1], i.e. the first moments of Ŷ must\n3See Daniely [2015] for a description of the problem.\nmatch. They propose optimizing the hinge loss subject to an approximation of this first moment constraint. Their work is primarily applied and gives no learning or non-discrimination guarantees for their learning rule, both of which we address in this section for a different relaxation.\nHere, we seek to identify a more tractable notion of non-discrimination based on second order moments. In particular, we propose the following condition, which under minor conditions is implied by equalized odds. Equalized correlations is generally a weaker condition than equalized odds, but when considering the squared loss and X,A, Y are jointly Gaussian, it is in fact equivalent.\nDefinition 5 (Equalized correlations). We say that a score R satisfies equalized correlations or is second-moment non-discriminatory with respect to the protected attribute A and target Y , if R, A, and Y satisfy the following:\nσRAσ 2 Y = σRY σY A, (13)\nwhere σαβ = E [(α− E[α])(β − E[β])] is the covariance between two random variables α and β.\nTheorem 4 (Gaussian non-discrimination). Let X, A, and Y be jointly Gaussian. Then the squared loss optimal (equalized odds) non-discriminatory predictor is linear. Furthermore, for all linear predictors in this setting, non-discrimination is equivalent to satisfying Equation (13).\nTheorem 4 means in particular that in the Gaussian setting under the squared loss, the optimal predictor that is non-discriminatory in the sense of Definition 1 is the same as the optimal predictor that is non-discriminatory in the more relaxed sense of Definition 5. We stress that this is generally not the case for non-Gaussian scenarios and with losses other than the squared loss that may result in non-linear optimal predictors.\nOne may also consider intermediate notions of non-discrimination between equalized odds and equalized correlations. One such option is to require that the conditional covariance σRA|Y vanishes. We do not elaborate further, except to note that it is immediate that equalized odds implies this to be the case and this in turn generally implies equalized correlations.\nSince linear prediction can be thought of as the hallmark of Gaussian processes, one could therefore justify the relaxation of Definition 5 as being the appropriate notion of non-discrimination when we restrict our predictors to be linear. Linear predictors, especially under kernel transformations, are used in a wide array of applications. They thus form a practically relevant family of predictors where one would like to achieve non-discrimination. Therefore, in the remainder of this section, we develop a theory for non-discriminating linear predictors.\nA particularly attractive feature of equalized correlations is that, for linear predictors, Equation (13) amounts to a linear constraint. With any convex loss `(·, ·), finding the optimal second-moment non-discriminatory predictor can be written as:\nmin w\nE [ ` ( Y,wT [ X A ])] s.t. wT ( Σ[X\nA\n] ,A σ2Y − Σ[X\nA\n] ,Y σY A\n) = 0\nThis is a convex optimization problem with a single linear constraint, and is thus generally tractable. In what follows, we take X to be a real-valued vector, A to be a scalar, and the target to be binary Y ∈ {±1}, unless otherwise noted. We also commit to the squared loss (R−Y )2 throughout. Without loss of generality, we assume that X, A, and Y all have zero mean. Recall the definition of the optimal linear predictor :\nR̂ = arg min r(x,a)=wT [xa ]\nE[(Y − r(X,A))2] = r̂(X,A), (14)\nwhere one can determine that r̂(x, a) = ŵT [ xa ] with ŵ = E [[ X A ] [XT AT ] ]−1 E [[ X A ] Y ]\n= Σ−1[ X A ] , [ X A ] Σ[X A ] ,Y .\nThe optimal linear predictor may, in general, be discriminatory, so we define the optimal secondmoment non-discriminatory linear predictor as follows:\nR? = arg min r(x,a)=wT [xa ] E[(Y − r(X,A))2] = r?(X,A) s.t. σr(X,A),Aσ2Y = σr(X,A),Y σY A (15)\nthus the predictor is constrained to be linear and to satisfy (13), which is a single linear constraint. We can give a closed-form expression for R? (see Section E.2 in Appendix E for details). We have that r?(x, a) = w?T [ xa ], where\nw? = Σ−1[ X A ] , [ X A ] (Σ[X A ] ,Y − αv ) where v is a vector encoding the non-discrimination constraint and α is a scalar defined as\nv = Σ[X A ] ,A − Σ[X A ] ,Y σY A/σ 2 Y and α =\nvTΣ−1[ X A ] , [ X A ]Σ[X A ] ,Y\nvTΣ−1[ X A ] , [ X A ]v Written as such, R? is a function of X and A. Nevertheless, it turns out that this optimal non-discriminatory linear predictor can be derived, in the sense of Definition 2, from the optimal (possibly discriminatory) linear predictor R̂ of Equation 14 and without access to X individually.\nTheorem 5 (Derived). The second-moment non-discriminatory linear predictor minimizing the squared loss can be derived from the optimal least squares linear predictor R? and the joint (second moment) statistics of (R?, A, Y ). Specifically,\nR? = R̂− α ( A+ R̂σY A/σ 2 Y ) ,\nwith\nα = σY A + σR̂,Y σY A/σ\n2 Y σ2A − (σY A)2/σ2Y + ( σ R̂,A − σ R̂,Y σY A/σ2Y ) σY A/σ2Y .\nTheorem 5 shows that, as far as the equalized correlation criterion is concerned, there is no penalty for first finding an optimal linear predictor and then correcting it. Consequently, this criterion easily enforceable on existing predictors. Intuitively, one must simply “subtract” any potential correlation one could derive about protected attributes from the prediction score and the outcome, and this can be entirely determined from the statistics of the optimal linear predictor R̂, the protected attribute A, and the outcome Y , without the need to know the extended set of attributes X. We emphasize that this result does not rely on any Gaussian assumptions, but simply on the fact that we have limited ourselves to linear predictors and the relaxed notion of non-discrimination. Finally, it is worth mentioning that a two-step procedure, as in Section 4, could be developed also for learning second-moment (approximately) non-discriminatory linear predictors from samples."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work we took the first steps toward a statistical and computational theory of learning non-discriminatory (equalized odds) predictors. We saw that post hoc correction might not be optimal and devised a statistically optimal two-step procedure, after observing that a straightforward\nERM-type approach is not sufficient. Computationally, working with binary non-discrimination is essentially has hard as agnostically learning binary predictors, and so we should expect to have to resort to relaxations. We took the first step to this end in Section 6 where we considered a second moment relaxation of non-discrimination which leads to tractable learning. We hope this will not be the final word on learning non-discriminatory predictors and that this work will spur interest in further understanding our relaxation, suggesting other relaxations, and studying other computationally efficient procedures with provable guarantees."
    }, {
      "heading" : "A Deferred Proofs From Section 2",
      "text" : ""
    }, {
      "heading" : "A.1 Proof of Example 1",
      "text" : "We restate the example for convenience:\nExample 1. When the hypothesis class is unconstrained, for any ∈ (0, 1/4) there exists a distribution D such that a) the optimal non-discriminatory predictor with respect to the 0-1 loss has loss at most 2 but b) the post hoc correction of the unrestricted Bayes optimal predictor has loss at least 0.5.\nA similar statement can also be made about hinge loss. For an unconstrained hypothesis class, for any ∈ (0, 1/4) and the same distribution D , a) the optimal non-discriminatory predictor with respect to the hinge loss has loss at most 4 but b) the post hoc correction of the Bayes optimal unrestricted predictor has loss at least 1.\nConsider the unconstrained hypothesis class of all (possibly randomized) functions from (X,A) to {0, 1}. Let D be the following distribution over (X,A, Y ), with X,A, Y ∈ {0, 1}:\nP(Y = 1) = 0.5 P(A = y | Y = y) = 1− P(X = y | Y = y) = 1− 2 (16)\nThe graphical model representing this distribution is\nX Y A\nClearly, X ⊥ A | Y , so Y ∗ = X is non-discriminatory and achieves a 0-1 loss of 2 . This same predictor achieves hinge loss 4 . This predictor, being non-discriminatory, upper bounds the loss of the optimal non-discriminatory predictor with respect to the 0-1 and hinge losses.\nThe optimal predictor with respect to the 0-1 loss, which might be discriminatory, is in the convex hull (i.e. it might be randomized combination) of the sized mappings from {0, 1}×{0, 1} 7→ {0, 1}. The Bayes optimal predictor with respect to the 0-1 loss is the hypothesis\nĥ(x, a) = argmax y∈{0,1}\nP(Y = y | X = x,A = a) (17)\nGiven a ∈ {0, 1}, note that since < 1/4\nP(Y = a | X = a,A = a) = 1 1 + P(A=a | Y=1−a)P(X=a | Y=1−a)P(A=a | Y=a)P(X=a | Y=a)\n= 1\n1 + ( )(2 )(1− )(1−2 )\n> 1\n2 .\n(18)\nSimilarly\nP(Y = a | X = 1− a,A = a) = 1 1 + P(A=a | Y=1−a)P(X=1−a | Y=1−a)P(A=a | Y=a)P(X=1−a | Y=a)\n= 1\n1 + (1−2 )(1− )(2 )\n> 1\n2 .\n(19)\nTherefore, the Bayes optimal predictor is ĥ(X,A) = A, which is 1-discriminatory, as\nP(ĥ(X,A) = 1 | Y = y,A = 0) = 0 but P(ĥ(X,A) = 1 | Y = y,A = 1) = 1 (20)\nConsider now the post-hoc correction h̃ of ĥ. The best non-discriminatory predictor Ỹ derived from the joint distribution (ĥ, A, Y ) ≡ (A,A, Y ) is given by the following optimization problem\nỸ = argmin h\nL(h)\ns.t. γy0(h) = γy1(h) for y = 0, 1[ γ0a(h) γ1a(h) ] ∈ ConvHull ([ 0 0 ] , [ γ0a(ĥ) γ1a(ĥ) ] , [ γ0a(1− ĥ) γ1a(1− ĥ) ] , [ 1 1 ]) for a = 0, 1.\n(21)\nThe first constraint requires that the resultant predictor be non-discriminatory. The second requires that the class conditional true positive and false positive rates of the predictor be in the convex hull of the constant 0 predictor, the constant 1 predictor, the predictor ĥ and its negative. This constraint is equivalent to requiring that h̃ be derived from ĥ. Because γy0(ĥ) = 0 and γy1(ĥ) = 1, the second constraint requires the predictor have equal true and false positive rates. As P(Y = 1) = 0.5, 0.5 is optimal true and false positive rate and L(h̃) = 0.5.\nUsing the same distribution, but with X,A, Y ∈ {−1, 1} instead of {0, 1}, the optimal nondiscriminatory predictor with respect to the hinge loss is no worse than the predictor which returns X, achieving hinge loss 4 . However, the Bayes optimal predictor with respect to the hinge loss is again that predictor which returns\nĥ(x, a) = argmax y∈{0,1}\nP(Y = y | X = x,A = a) = a.\nBy the same line of reasoning, the post hoc correction of ĥ, which must have identical statistics for A = 0 and A = 1 is forced to have equal true and false positive rates, and thus the best derived predictor is identically 0 which has hinge loss 1."
    }, {
      "heading" : "A.2 Proof of Example 2",
      "text" : "We restate the example for convenience:\nExample 2. Let H be the class of linear predictors with L1 norm at most 12 − 2 , for some ∈ (2/25, 1/4). Then there exists a distribution D such that a) the optimal non-discriminatory predictor in H with respect to the squared loss has square loss at most 116 + 3 2 + 3\n2, but b) the post hoc correction of the Bayes optimal square loss regressor in H returns a constant predictor which has (trivial) square loss of 1/4.\nSimilarly, for the class H of sparse linear predictors, for any ∈ (0, 1/4), there exists a distribution D such that a) the optimal non-discriminatory predictor in H with respect to the squared loss has square loss at most 2 − 4 2, but b) the post hoc correction of the Bayes optimal squared loss regressor in H again returns a constant predictor which has (trivial) square loss of 1/4.\nIn this example, we consider the squared loss and the hypothesis class of linear predictors with L1 norm at most 12 − 2 for some ∈ (2/25, 1/4):\nH = { w1X + w2A+ b : |w1|+ |w2| ≤ 1\n2 − 2\n} .\nWith this parameter , we use the same distribution as in the proof of Theorem 1:\nP(Y = 1) = 0.5 P(A = y | Y = y) = 1− P(X = y | Y = y) = 1− 2\nSince X ⊥ A | Y , any linear function of X only will be non-discriminatory and h(X) = (\n1 2 − 2\n) X+\n1 4 + has the required L 1 norm and achieves squared loss 116 + 3 2 + 3 2. This predictor, being nondiscriminatory, upper bounds the squared loss of the optimal non-discriminatory predictor.\nTo derive the optimal potentially discriminatory predictor, it will be useful to begin by calculating the covariances between each of the variables:\nE [X] = E [A] = E [Y ] = 1\n2 (22) E [ X2 ] = E [ A2 ] = E [ Y 2 ] = 1\n2 (23)\nE [XA] = 1 2 − 3 2 + 2 2 (24) E [XY ] = 1− 2\n2 (25)\nE [AY ] = 1−\n2 (26)\nThe optimal predictor optimizes\nĥ = argmin w1,w2,b\nE [ (w1X + w2A+ b− y)2 ] s.t. |w1|+ |w2| ≤ 1\n2 − 2 .\n(27)\nForming the Lagrangian:\nL(w1, w2, b, λ) = E [ (w1X + w2A+ b− y)2 ] + λ ( |w1|+ |w2| − 1\n2 − 2 ) = w21E [ X2 ] + w22E [ A2 ] + b2 + E [ Y 2 ] + 2w1w2E [XA] + 2w1bE [X]\n− 2w1E [XY ] + 2w2bE [A]− 2w2E [AY ]− 2bE [Y ] + λ ( |w1|+ |w2| − 1\n2 − 2 ) = w21 + w 2 2 + 1\n2 + b2 + w1w2(1− 3 + 4 2) + w1b− w1(1− 2 ) + w2b − w2(1− )− b+ λ ( |w1|+ |w2| − 1\n2 − 2\n) . (28)\nAt the following values:\nw1 = 0 w2 = 1 2 − 2 b = 1 4 + λ = 1 4 (29)\nthe subdifferential of L contains 0 for any ∈ (2/25, 1/4). Looking term by term we have:\n∂`\n∂w1 = w1 + w2(1− 3 + 4 2) + b− (1− 2 ) + λsign(w1) (30)\n=\n( 1\n2 − 2\n) (1− 3 + 4 2) + 1\n4 + − (1− 2 ) + 1 4 sign(0) (31)\n= 1\n4 sign(0)− 8 3 + 8 2 − 2 − 1 4 , (32)\nwhere sign(0) is an arbitrary value in [−1, 1] which is the subdifferential of f(z) = |z| at 0. For any ∈ (2/25, 1/4) ∣∣∣∣−8 3 + 8 2 − 2 − 14 ∣∣∣∣ < 14 =⇒ 0 ∈ ∂`∂w1 (33)\nFurthermore,\n∂`\n∂w2 = w1(1− 3 + 4 2) + w2 + b− (1− ) + λsign(w2)\n= 1 2 − 2 + 1 4 + − (1− ) + 1 4 = 0,\n(34)\nand\n∂` ∂b = 2b+ w1 + w2 − 1\n= 1\n2 + 2 +\n1 2 − 2 − 1 = 0.\n(35)\nThis proves that ĥ(X,A) = (\n1 2 − 2 ) A+ 14 + is the Bayes optimal predictor in H with respect\nto the squared loss. Furthermore, the random variable is supported on only two points: 14 + when A = 0 and 34 − when A = 1. It is clear that ĥ is not independent of A conditioned on Y . Since ĥ is a deterministic function of A, the post hoc correction h̃, which must be independent of A conditioned on Y , is forced to be independent of A, and consequently Y . Thus, h̃ ≡ 0.5 is the best possible derived predictor, achieving square loss 1/4.\nConsidering the class of 1-sparse linear predictors, the predictor h(X,A) = (1 − 4 )X + 2 , being conditionally independent of A is non-discriminatory and achieves squared loss 2 − 4 2, upper bounding the loss of the optimal non-discriminatory 1-sparse linear predictor. Without regard for non-discrimination, the optimal hypothesis in the class with respect to the squared loss is ĥ(X,A) = (1− 2 )A+ . This post hoc correction of this predictor suffers from the same issue as in the bounded norm case, resulting in a predictor that can have squared loss no better than 1/4."
    }, {
      "heading" : "B Deferred Proofs From Section 3",
      "text" : "Recall the notation Γya(h) = maxy |γy0(h) − γy1(h)| and ΓSya(h) = maxy |γSy0(h) − γSy1(h)| from Definition 3. To avoid clutter, we sometimes drop the dependence on h for γya when h is evident from the context. Also, recall that S = {(xi, yi, ai) : i ∈ [n]} ∼ Pn(X,Y,A) and Pya = P(Y = y,A = a).\nThe following lemma on the concentration for ΓS is used in multiple proofs. The lemma is proved in B.2.\nLemma 4. For δ ∈ (0, 1/2) and a binary predictor h, if nPya > 2 log 16/δ, the following hold,\nP ( |Γ(h)− ΓS(h)| > 2 max\nya\n√ log 16/δ\nnPya\n) ≤ δ. (36)"
    }, {
      "heading" : "B.1 Proof of Lemma 1",
      "text" : "Lemma 1 Given n i.i.d. samples S, ∀α, δ > 0, if n > 16 maxya log 16/δPyaα2 , then with probability greater than 1− δ, T satisfies,\nT ( Ŷ , S, α ) =\n{ 0 if Ŷ is 0-discriminatory on population\n1 if Ŷ is at least α-discriminatory on population.\nProof. Recall that T (Ŷ , S, αn) = 1 ( ΓS(Ŷ ) > αn ) . Let αn be chosen to satisfy,\n2 max ya\n√ log 16/δ\nnPya < αn < α− 2 max ya\n√ log 16/δ\nnPya .\nThen the following results readily follow from Lemma 4, 1. If Ŷ is non-discriminatory, i.e. Γ(Ŷ ) = 0 .\nP ( T (Ŷ , S, αn) = 1 ) = P(ΓS(Ŷ ) > αn) ≤ P ( ΓS(Ŷ ) > Γ(Ŷ ) + 2 max\nya\n√ log 16/δ\nnPya\n) ≤ δ.\n2. Similarly, suppose a binary predictor is Ŷ is at least α-discriminatory, i.e. Γ(Ŷ ) ≥ α, then\nP ( T (Ŷ , S, αn) = 0 ) = P ( ΓS(Ŷ ) ≤ αn ) ≤ P ( ΓS(Ŷ ) ≤ Γ(Ŷ )− 2 max\nya\n√ log 16/δ\nnPya\n) ≤ δ.\nThus, if n > 16 maxya log 16/δ Pyaα2 , then T (Ŷ , S, α2 ) satisfies the conditions of the test in Lemma 1."
    }, {
      "heading" : "B.2 Proof of Lemma 4",
      "text" : "Proof of Lemma 4. Recall that Pya = P(Y = y,A = a), S = {(Xi, Yi, Ai) : i ∈ [n]} ∼ Pn(X,Y,A) and nSya = ∑ i 1(Yi = y,Ai = a). With slight abuse of notation, we define random variables Sya = {i : Yi = y,Ai = a}. We then have γSya(h)|Sya = ∑ j∈Sya h(xj)\nnSya ∼ 1 nSya Binomial(γya, n S ya) with E[γSya|Sya] = γya.\nP(|γSya − γya| > t) = ∑ Sya P(|γSya − γya| > t|Sya)P(Sya) (a) ≤ ∑ Sya 2 exp (−2t2nSya)P(Sya)\n≤ 2P(nSya ≤ nPya\n2 ) + 2 exp (−t2nPya)P(nSya ≥ nPya 2 )\n(b) ≤ 2 exp (−nPya/2) + 2 exp (−t2nPya) (c) ≤ δ 8 + 2 exp (−t2nPya) (37)\nwhere (a) and (b) follow from Hoeffding’s inequality on γSya|Sya ∼ Binomial(γya, nSya) and nSya ∼ Binomial(n,Pya), respectively, and (c) follows from conditions on nPya in Lemma 4.\nFinally, for y ∈ {0, 1}, using a series of triangle inequality,∣∣|γSy0 − γSy1| − |γy0 − γy1|∣∣ ≤ |γSy0 − γSy1 − γy0 + γy1| ≤ |γSy0 − γSy0|+ |γSy1 − γy1|, and PS (∣∣|γSy0 − γSy1| − |γy0 − γy1|∣∣ > 2t) ≤ PS(|γSy0 − γy0|+ |γSy1 − γy1| > 2t)\n(a) ≤ PS ( |γSy0 − γy0| > t ) + PS ( |γSy1 − γy1| > t ) (b) ≤ δ\n2 , (38)\nwhere (a) follows from union bound, and (b) follows from (37) using t = maxya √ log 16/δ nPya . The lemma follows from collecting the failure probabilities for y = 0, 1."
    }, {
      "heading" : "C Deferred Proofs From Section 4",
      "text" : "We use the notation A ≤δ B to denote that A ≤ B holds with probability greater than 1 − δ. Recall the notation Γya(h) = maxy |γy0(h) − γy1(h)| and ΓSya(h) = maxy |γSy0(h) − γSy1(h)| from Definition 3. To avoid clutter, we sometimes drop the dependence on h for γya when h is evident from the context. Finally, in this section C,C1 and C2 denote absolute constants that are not necessarily the same at each occurrence."
    }, {
      "heading" : "C.1 Proof of Theorem 1",
      "text" : "Theorem 1. Let n=Ω ( maxya\nlog 1/δ Pya\n) and the hyperparameters satisfy αn, α̃n=Θ ( maxya √ log 1/δ nPya ) .\nFor a binary hypothesis class H, any distribution P(X,Y,A), and any δ ∈ (0, 1/2), if Y ∗ ∈ H is a non-discriminatory predictor, then with probability greater than 1− δ, the output of the two step procedure Ỹ satisfies the following for absolute constants C1 and C2,\nL(Ỹ ) ≤ L(Y ∗) + C1 max ya\n√ V C(H) + log 1/δ\nnPya , and Γ(Ỹ ) ≤ C2 max ya\n√ log 1/δ\nnPya .\nThus, with n = Ω (\nmaxya V C(H) Pya 2 + maxya 1 Pyaα2\n) samples, and an appropriate choice of αn, α̃n,\nthe two step framework returns Ỹ ∈ Q(L(Y ∗) + , α) with high probability.\nProof. We begin by stating the following result from Lemma 6 (stated and proved in C.3) which shows the concentration of loss and discrimination in derived predictors: if nPya > 4 log 32/δ, for any randomized predictor h̃ derived from Ŷ , A, i.e. h̃ ∈ P(Ŷ ), satisfies the following:\n∣∣L(h̃)− LS2(h̃)∣∣ ≤δ/4 √ log 8/δn , and |Γ(h̃)− ΓS2(h̃)| ≤δ/2 2 maxya √ 2 log 32/δ nPya . (39)\nWe also recall from Lemma 3 that if h is an α-discriminatory binary predictor, the optimum nondiscriminatory derived predictor Ỹ ∗(h) given by (3) satisfies, Ỹ ∗(h) ∈ Q(L(h)+α, 0). Additionally, using Lemma 2, we have that Γ(Ŷ ) ≤δ C maxya √ V C(H)+log 1/δ\nnPya . Thus, using Lemma 2 and Lemma 3, ∃Ỹ ∗ ∈ P(Ŷ ), such that Ỹ ∗ ∈ Q ( C maxya √ V C(H)+log 1/δ nPya , 0 ) , and using using the concentration in (39), with high probability\nLS2(Ỹ ) ≤ LS2(Ỹ ∗) ≤ L(Ỹ ∗) + C √ log 8/δ\n2n ≤ C max ya\n√ V C(H) + log 1/δ\nnPya . (40)\nFinally, consider a finite hypothesis class that includes all mappings from (Ŷ , A) to binary values, if Ŷ and A are both binary. There are 42 = 16 such mappings denoted by Hderived = {h : {0, 1} × {0, 1} → {0, 1}}. Also, recall that the feasible set of (randomized) binary predictors derived solely from P(h, Y,A) is denoted by P(h) and any Ỹ ∈ P(h) is completely specified by {p̃ ĥ,a\n(Ỹ ) = P(Ỹ = 1|h(X) = ĥ, A = a) : ĥ ∈ {0, 1}, a ∈ {0, 1}}. Any such randomized derived predictor Ỹ ∈ P(Ŷ ) derived from (Ŷ ,A) is in the convex hull of Hderived.\nThus, V C(P(Ŷ )) = V C(conv(Hderived)) = log 16 which is a constant. Using this in standard VC dimension uniform bound [Bousquet et al., 2004], we have the following with high probability,\nL(Ỹ ) ≤ LS2(Ỹ ) + C1\n√ log 1/δ\nn\n(a) ≤ C max ya\n√ V C(H) + log 1/δ\nnPya ,\nΓ(Ỹ ) (b)\n≤ α̃n + C2 max ya\n√ log 1/δ\nnPya , (41)\nwhere (a) follows from (40), and (b) from (39) and using the fact that V C(P(Ŷ ) is constant."
    }, {
      "heading" : "C.2 Proof of Lemma 2 and Lemma 3",
      "text" : "Lemma 2 Under the conditions in Theorem 1, w.p. greater than 1− δ, Ŷ from Step 1 satisfies\nL(Ŷ ) ≤ L(Y ∗) + C1\n√ V C(H) + log 1/δ\nn , and Γ(Ŷ ) ≤ C2 max ya\n√ V C(H) + log 1/δ\nnPya .\nProof of Lemma 2. Recall that the training data for Step 1 are denoted by S1 = {(xi, ai, yi) : i ∈ [n/2]} ∼ Pn/2(X,A, Y ) and Pya = P(Y = y,A = a). From Using Hoeffding’s inequality on the empirical 0-1 loss LS1(h), and using concentration results for ΓS1 from Lemma 4, the following holds for δ ∈ (0, 1/2) and ∀(y, a), nPya > 4 log 32/δ.\n∣∣L(h)− LS1(h)∣∣ ≤δ/4 √ log 8/δn , and |Γ(h)− ΓS1(h)| ≤δ/2 2 maxya √ 2 log 32/δ nPya . (42)\nUsing (42) and the standard VC dimension uniform bound [Bousquet et al., 2004], the following holds with high probability for absolute constants C1 and C2,\n|L(Ŷ )− LS1(Ŷ )| ≤δ/4 C1\n√ V C(H) + log 1/δ\nn , and\n|Γ(Ŷ )− ΓS1(Ŷ )| ≤δ/2 C2 max ya\n√ V C(H) + log 1/δ\nnPya .\n(43)\nFinally, from Lemma 5 (stated and proved in C.3), with probability greater than 1 − δ/4, any 0-discriminatory Y ∗ ∈ H is in the feasible set for Step 1 in (10), and thus by optimality of Ŷ , LS1(Ŷ ) ≤ LS1(Y ∗).\nL(Ŷ ) ≤δ/4 LS1(Ŷ ) + C1\n√ V C(H) + log 1/δ\nn ≤δ/4 L(Y ∗) + 2C1\n√ V C(H) + log 1/δ\nn ,\nΓ(Ŷ ) ≤δ/2 αn + C2 max ya\n√ V C(H) + log 1/δ\nnPya . (44)\nThe lemma follows from combining the failure probabilities.\nLemma 3 If h is an α-discriminatory binary predictor h ∈ Q(L(h), α), then the optimal 0- discriminatory derived predictor Ỹ ∗(h) from (3) using 0-1 loss satisfies Ỹ ∗(h) ∈ Q(L(h) + α, 0).\nProof of Lemma 3. The intuition is to conservatively bound the true and false positive rates of the non-discriminatory derived predictor using the class conditional rates for h. In the case of binary predictors, Ỹ being derived from h is equivalent to requiring that\n] ( γ0a(Ỹ (h)), γ1a(Ỹ (h)) ) ∈ Conv ((0, 0), (1, 1), (γ0a(h), γ1a(h)), (1− γ0a(h), 1− γ1a(h)))\nIn the figure below, Ỹ ∗(h) is the actual optimal derived non-discriminatory predictor, but we estimate it conservatively using the worse of the class conditional true and false positive rates of h:\nWithout loss of generality, assume γ1a(h) ≥ 0.5 and γ0a(h) ≤ 0.5 for all a (the hypothesis class is at least as good as chance). Consider the predictor Ỹ such that ∀a ∈ {0, 1},(\nγ0a(Ỹ ), γ1a(Ỹ ) ) = (max(γ00, γ01),min(γ10, γ11)) ∈ Conv ((0, 0), (1, 1), (γ0a(h), γ1a(h)))\nthat is, Ỹ has the greater of the two false positive rates and lesser of the two true positive rates for both classes A = 1 and A = 0. Clearly, this choice of Ỹ is both non-discriminatory and derived, thus it is a feasible point for (3). Let γ̃y := γy1(Ỹ ) = γy0(Ỹ ), we then have\nE[`01(Ỹ ∗(h))] ≤ E[`01(Ỹ )] = P(Y = 0)γ̃0 + P(Y = 1)(1− γ̃1) = ∑\na∈{0,1}\nP(Y = 0, A = a)γ0a(Ỹ ) + ∑\na∈{0,1}\nP(Y = 1, A = a)(1− γ1a(Ỹ )) (45)\nSince h is α-discriminatory, for each a γ1a(h)−mina′ γ1a′(h) = γ1a(h)−γ1a(Ỹ ) < α and maxa′ γ0a′(h)− γ0a(h) = γ0a(Ỹ )− γ0a(h) < α, thus (45) can be upper bounded with∑\na∈{0,1}\nP(Y = 0, A = a) (γ0a(h) + α) + ∑\na∈{0,1}\nP(Y = 1, A = a)(1− γ1a(h) + α)\n≤ α+ ∑\na∈{0,1}\nP(Y = 0, A = a)γ0a(h) + ∑\na∈{0,1}\nP(Y = 1, A = a)(1− γ1a(h))\n= α+ E[`01(h)]\n(46)"
    }, {
      "heading" : "C.3 Supporting Lemmas for Proof of Theorem 1",
      "text" : "Lemma 5. Let HS1αn = {h ∈ H : Γ S1(h) ≤ αn}. If ∀(y, a), nPya > 4 log 32/δ and αn in (10) satisfies αn ≥ 2 maxya √\n2 log 64/δ nPya , then with probability greater than 1 − δ4 , for any Y ∗ ∈ H ∩ Q(L(Y ∗), 0),\nY ∗ ∈ HSαn.\nProof of Lemma 5. Given Y ∗ ∈ H ∩Q(L∗, 0),\nP(Y ∗ /∈ HS1αn) = P(Γ S1(Y ∗) > αn)\n(a) ≤ P (\nΓS1(Y ∗) > 2 max ya\n√ 2 log 64/δ\nnPya ) ≤ P ( ΓS1(Y ∗) > Γ(Y ∗) + 2 max\nya\n√ 2 log 64/δ\nnPya\n) (47)\n(b) ≤ δ 4 , (48)\nwhere (a) follows from the condition on αn and (b) from Lemma 4.\nLemma 6. For δ ∈ (0, 1/2) and h ∈ H, if n2Pya > 4 log 32/δ, For any randomized predictor Ỹ derived from Ŷ , A, i.e.Ỹ ∈ P(Ŷ ), satisfies the following:\n∣∣L(Ỹ )− LS2(Ỹ )∣∣ ≤δ/4 √ log 8/δn , and |Γ(Ỹ )− ΓS2(Ỹ )| ≤δ/2 2 maxya √ 2 log 32/δ nPya . (49)\nProof of Lemma 6. The proof essentially follows the same arguments that were used for (42) and Lemma 4.\nFor a randomized predictor Ỹ ,\nLS(Ỹ )− L(Ỹ ) = 2 n ∑ i∈S2 E Ỹ `(Ỹ (ŷi, ai), yi)− EX,A,Y EỸ `(Ỹ , Y )\nHere LS(Ỹ )−L(Ỹ ) is merely a sum of n/2, [0, 1] bounded random variables and Hoeffdings bound can be applied to get the required concentration in (39). Similarly, for any randomized predictor Ỹ , the conditional random variable γS2ya (Ỹ )|S2ya =∑ j∈S2ya E Ỹ Ỹ (ŷj ,aj)\nnSya is a sum of [0, 1] bounded random variables with mean E[γS2ya (Ỹ )|S2ya ] = γya(Ỹ ),\nthe proof of Lemma 4 can be repeated verbatim for the randomized prediction where instead of the Hoeffdings’ bound on Binomial random variables, we use the identical Hoeffdings’ bound for [0, 1] bounded random variables."
    }, {
      "heading" : "C.4 Proof of Theorem 2",
      "text" : "Let the marginal distribution over (A, Y ) be given with p = mina,y P(A = a, Y = y). Since the definiton of fairness is invariant to re-labelling of A, Y , assume without loss of generality that p corresponds to A = 1, Y = 1. For α ∈ (0, 1/2), the distribution D over (X,A, Y ) ∈ {0, 1}n ×\n{0, 1} × {0, 1} is described by\nP(X1 = y | Y = y) = 1− α P(Xi = 0 | Y = 0, A = 0) = 1 for i = 2, 3, ..., n P(Xi = 1 | Y = 1, A = 0) = 1 for i = 2, 3, ..., n P(Xi = 0 | Y = 0, A = 1) = 1 for i = 2, 3, ..., n P(Xi = 1 | Y = 1, A = 1) = 1− α for i = 2, 3, ..., n\n(50)\nConsider the following hypothesis class H = {hi}ni=1 with hi(X,A) = Xi. The hypothesis h1 has 0-1 loss L01(h1) = P(X1 6= Y ) = α and is exactly non-discriminatory since X1 ⊥ A | Y by construction. For every other i = 2, 3, ..., n, the 0-1 loss of hi is the same:\nL01(hi) = ∑ y ∑ a P(Xi = 1− y | Y = y,A = a)P(Y = y,A = a) = pα (51)\nhowever, for these hypotheses |P(hi = 1|Y = 1, A = 1)− P(hi = 1|Y = 1, A = 0)| = α so hi is αdiscriminatory.\nWe will now show that on a sample S of size m, the empirical risk minimizer subject to an approximate non-discrimination constraint, ĥ, will be hi for i 6= 1 with probability 0.5. Hence, the first step alone cannot assure with probability better than 0.5 a classifier that is better than α-discriminatory.\nFirst, we note that the predictions of hi and hj are independent for i 6= j since Xi and Xj are independent. Therefore, the number of errors made by each classifier hi on the sample S are independent and\nP(LS01(h1) = 0) = (1− α)m P(LS01(hi) = 0) = (1− pα)m for i = 2, 3, ..., n (52)\nSince a classifier that makes zero errors on S is automatically non-discriminatory on S, if h1 makes at least 1 mistake on S and some hi does not make any errors, then h1 will be the optimum of (10). This event occurs with probability:\nP ( LS01(h1) > 0 ∧ ∃i LS01(hi) = 0 ) = (1− (1− α)m) ( 1− P ( ∀i > 1 LS01(hi) > 0 )) (53)\n= (1− (1− α)m) ( 1−\nn∏ i=2 (1− (1− pα)m)\n) (54)\n= (1− (1− α)m) ( 1− (1− (1− pα)m)n−1 )\n(55)\nFrom here, we use that\n∀k ∈ N ∀x ∈ [0, 1] (1− x)k ≤ 1 1 + kx\n(56)\nThus, since 1− pα, α ∈ [0, 1]:\nP ( LS01(h1) > 0 ∧ ∃i LS01(hi) = 0 ) = (1− (1− α)m) ( 1− (1− (1− pα)m)n−1 ) (57)\n≥ (\n1− 1 1 +mα\n)( 1− 1\n1 + (n− 1)(1− pα)m\n) (58)\n= mα 1 +mα − ( 1− 1 1 +mα ) 1 1 + (n− 1)(1− pα)m (59)\nThis expression is is greater than 1/2 if the first term is at least 2/3 and the second term is at most 1/6. Thus\nmα 1 +mα ≥ 2 3 ⇐⇒ α ≥ 2 m (60)\nand ( 1− 1\n1 +mα\n) 1\n1 + (n− 1)(1− pα)m ≤ 1 6\n⇐= 1 1 + (n− 1)(1− pα)m ≤ 1 6 ⇐⇒ log 1 1− pα ≤ log n−15 m\n(61)\nSince − log(1− x) < x1−x for x ∈ (0, 1), and p ≤ 1 4 , the expression (59) is at least 1/2 when\n2\nm ≤ α ≤ 3 log n5 4pm\n(62)\nTherefore, when α = 3 log n\n5 4pm , with probability 0.5 h1 has non-zero error on S and a different\npredictor has zero error. We conclude that there exists a distribution and hypothesis class such that with probability 0.5, the hypothesis returned by the first step is 3 log n\n5 4pm -discriminatory."
    }, {
      "heading" : "D Proof of Theorem 3",
      "text" : "Let A be an algorithm that takes as inputs a hypothesis class H, a distribution D̃ over (X̃, Ã, Ỹ ) with Ã ∈ {0, 1} and Ỹ ∈ {−1,+1}, an accuracy parameter > 0, and a non-discrimination parameter α > 0 and returns a predictor f = A(D̃, , α) such that with probability 1− ζ\nLhinge D̃ (f) ≤ min h∈H0-disc(D̃) Lhinge D̃ (h) + (63)∣∣∣PD̃ (f ≥ 0 ∣∣∣ Ỹ = y, Ã = 0)− PD̃ (f ≥ 0 ∣∣∣ Ỹ = y, Ã = 1)∣∣∣ ≤ α for y = −1,+1 The possibly randomized predictor f need not be in the hypothesis classH, but it is being compared against the best predictor in H whose sign is non-discriminatory.\nWe will show that such an algorithm can be used to improperly weakly learn Halfspace which, subject to the complexity assumption that refuting random K-XOR formulas is hard, was shown to be computationally hard by Daniely [2015]. We conclude that A must be computationally hard to compute.\nThe Halfspace problem is to take a distribution D over (X,Y ) withX ∈ Rd and Y ∈ {−1,+1}, and find the linear predictor\nh∗(x) = sign(w∗Tx) where w∗ = argmin w∈Rd E x,y∼D\n[ sign(wTx) 6= y ] (64)\nThe proof of hardenss of the Halfspace problem was shown using a distribution over the unit hypercube in d-dimensions, thus we will assume that D is a bounded distribution. We assume access to the distribution D, knowledge of L01D (h∗), and for now, access to the joint distribution of (h∗(X), Y ):\nη−− = PD(h∗(X) = −1, Y = −1) η−+ = PD(h∗(X) = −1, Y = +1) η+− = PD(h∗(X) = +1, Y = −1) η++ = PD(h∗(X) = +1, Y = +1)\n(65)\nhowever, we will show later that it is not necessary to know the η’s. Since it is always possible to get 0-1 loss at most 12 with a Halfspace predictor, we assume that η++ + η−− ≥ η+− + η−+ = L01D (h∗). From the distribution D we construct a new distribution D̃ over (X̃, Ã, Ỹ ) with X̃ ∈ Rd+1, Ã ∈ {0, 1}, and Ỹ ∈ {−1,+1} in the following manner:\nPD̃(Ã = 0) = 1− δ PD̃(Ã = 1) = δ PD̃(X̃ = −e1, Ỹ = −1 | Ã = 0) = η−− PD̃(X̃ = −e1, Ỹ = +1 | Ã = 0) = η−+ PD̃(X̃ = e1, Ỹ = −1 | Ã = 0) = η+− PD̃(X̃ = e1, Ỹ = +1 | Ã = 0) = η++ PD̃(X̃ = [0, x], Ỹ = y | Ã = 1) = PD(X = x, Y = y) ∀x, y\n(66)\nwhere e1 is the first standard basis vector in Rd+1. In other words, when Ã = 1 the distribution D̃ is identical to D besides a zero appended to the beginning of X. When Ã = 0, D̃ is supported on two points −e1 and e1.\nWe will apply the algorithm A to the distribution D̃ with parameters and α to be determined later. In this case H is the class of linear predictors so the hinge loss of f on D̃ must be competitve with the hinge loss of the best linear predictor whose sign is non-discriminatory.\nUsing the following lemmas, we show that the output of A must have small hinge loss on D̃, that this output can then be modified so that it has small 0-1 loss on D̃, and finally that it can be further modified to achieve small 0-1 loss on D. The proofs are deferred to the end of this discussion.\nLemma 7. There exists a linear predictor h whose sign is 0-discrminatory such that\nLhinge D̃ (h) = 2(1− δ)L01D (h∗) + 2δ\nBy Lemma 7 and the defintion of f from (63),\nLhinge D̃ (f) ≤ min h∈H0-disc(D̃) Lhinge D̃ (h) + ≤ 2(1− δ)L01D (h∗) + 2δ + (67)\nand the sign of f is α-discriminatory. Next,\nLemma 8. The predictor f can be efficiently modified to yield a new predictor f ′ whose sign is is α-discriminatory such that\nL01D̃ (f ′) ≤ (1− δ)L01D (h∗) + 2δ +\nFinally,\nLemma 9. The predictor f ′′(x) = f ′([0, x], 1) achieves L01D (f ′′) ≤ L01D̃ (f ′) + α(1− δ).\nThe predictor f ′′ described in Lemma 9 thus has 0-1 loss on D\nL01D (f ′′) ≤ (1− δ)(L01D (h∗) + α) + 2δ + (68)\nTheorem 1.3 from Daniely [2015] proves that there is no algorithm running in time polynomial in the dimension d that can return a predictor achieving 0-1 error ≤ 12 − d −c with high probability\nfor a constant c > 0 for an arbitrary distribution, even with the knowledge that L01D (h∗) ≤ L∗ for L∗ < 1/2. Thus, A(D̃, , α) cannot run in time polynomial in the dimension d for any parameters L01D (h∗), , α, and δ such that (68) is greater than 1 2 − d\n−c for any c > 0. For , α < 18 , and setting δ = 116 , (68) shows that\nL01D (f ′′) ≤ 15 16 L01D (h∗) + 47 128 (69)\nFor any L∗ < 110 this is at most 1 2 − 1 40 and does not depend on the dimension.\nIn this proof we assumed knowledge of the parameters η which describe the conditional error rates of h∗. If a polynomial time algorithm for A existed, then it would be possible to perform two-dimensional grid search over the η. Calls made to A with the incorrect values of η might result in very inaccurate or discriminatory predictors, but using an estimate of η up to O(α) accuracy is sufficient to approximate the Halfspaces solution using A. Thus at most O(log2(1/α)) calls to the polynomial time algorithm would be needed. Therefore, in order for A to guarantee for an arbitrary D̃ that its output would have excess hinge loss at most 18 and its sign would be at most 1 8 -discriminatory, it must run in time super-polynomial in the dimension in the worst case."
    }, {
      "heading" : "D.1 Deferred proofs",
      "text" : "Proof of Lemma 7. Define h(X,A) = sign ([ 1 w∗ ]T X ) . Then h(−e1, 0) = −1, h(e1, 0) = 1, and h([0, x], 1) = h∗(x), where h∗ is as defined in (64). Since the sign function is invariant to scaling, L01D (h∗) = L01D (ch∗) for any c > 0.\nTheorem 1.3 in Daniely [2015] involves a distribution D that is supported on the unit hypercube in Rd, thus the predictor h∗‖w∗‖2 √ d ∈ [−1, 1] with probability 1, and has the same 0-1 loss as h∗.\nBy the definition of D̃, η+−, and η−+:\nPD̃(h ≥ 0 | Ỹ = −1, Ã = 0) = η+−\nPD̃(Ỹ = −1 | Ã = 0)\n= η+−\nPD(Y = −1) = PD(h∗ ≥ 0 | Y = −1)\n= PD̃(h ≥ 0 | Ỹ = −1, Ã = 1)\n(70)\nPD̃(h < 0 | Ỹ = 1, Ã = 0) = η−+\nPD̃(Ỹ = 1 | Ã = 0)\n= η−+\nPD(Y = 1) = PD(h∗ < 0 | Y = 1)\n= PD̃(h < 0 | Ỹ = 1, Ã = 1)\n(71)\nTherefore, h is 0-discriminatory at threshold 0. Also\nLhinge D̃ (h) = [1 + h(x1)]+PD̃(X̃ = x1, Ã = 0, Ỹ = −1) (72)\n+[1− h(x0)]+PD̃(X̃ = x0, Ã = 0, Ỹ = 1)\n+ ∫ X [1 + h(x)]+PD̃(X̃ = x, Ã = 1, Ỹ = −1)dx\n+ ∫ X [1− h(x)]+PD̃(X̃ = x, Ã = 1, Ỹ = 1)dx\n≤ 2η+−PD̃(Ã = 0) + 2η−+PD̃(Ã = 0) (73)\n+2 ∫ X ( PD̃(X̃ = x, Ã = 1, Ỹ = −1) + PD̃(X̃ = x, Ã = 1, Ỹ = 1) ) dx\n= 2PD̃(Ã = 0)(η+− + η−+) + 2PD̃(Ã = 1) (74) = 2(1− δ)(η−+ + η+−) + 2δ (75)\nProof of Lemma 8. Since only the sign of f is required to be α-discriminatory, we can modify the magnitude of its predictions without affecting its level of non-discrimination. Therefore, we first truncate the output of f to lie in the range [−1, 1], which can only reduce the hinge loss.\nIgnoring for the moment that the sign of f must be α-discriminatory, we would like to define f ′ so that f ′(−e1, 0) = −1 and f ′(e1, 0) = 1 with probability 1. In this case, the hinge loss when Ã = 0 is exactly 2(η+− + η−+). With that being said, any modification to f that changes the distribution of the sign of the predictor risks rendering it more than α-discriminatory. With this in mind, we construct f ′ such that\nf ′(−e1, 0) = { −1 w.p. P(f(−e1, 0) < 0) 0 w.p. P(f(−e1, 0) ≥ 0)\nf ′(e1, 0) = { 1 w.p. P(f(e1, 0) ≥ 0) −0 w.p. P(f(e1, 0) < 0)\nf ′([0, x], 1) = f([0, x], 1)\n(76)\nwhere −0 is a negative number of arbitrarily small magnitude. Constructed this way, the distribution of the sign of f ′ conditioned on Ã is identical to that of f , meaning that the sign of f ′ is α-discriminatory.\nThe hinge loss of f ′ is an upper bound on the 0-1 loss, and in order to show that f ′ achieves small 0-1 loss, we will show that the hinge loss is a loose upper bound. The construction of D̃ and the predictions of f ′ conditioned on Ã = 0 creates a substantial gap between the losses.\nNotice that when f ′ makes a prediction of magnitude 1 that has the correct sign, both the hinge loss and the 0-1 loss evaluate to 0. Similarly, when f ′ makes a prediction of magnitude 0 with the incorrect sign, both losses are 1. Thus in each of these cases, the hinge loss is equivalent to the 0-1 loss.\nHowever, if f ′ makes a prediction of magnitude 1 with the incorrect sign, the hinge loss is 2 but the 0-1 loss is only 1, and when f ′ makes a prediction of magnitude 0 with the correct sign, the hinge loss is 1 but the 0-1 loss is 0. Consequently, in each of these cases there is a gap of 1 between the hinge and 0-1 losses. Thus,\nE [ `hinge(f ′)− `01(f ′) ∣∣∣ Ã = 0] = P(|f ′| = 1, sign(f ′) 6= Ỹ ∣∣∣ Ã = 0) (77) + P ( |f ′| = 0, sign(f ′) = Ỹ\n∣∣∣ Ã = 0) Considering each term separately:\nP ( |f ′| = 1, sign(f ′) 6= Ỹ ∣∣∣ Ã = 0) = P ( f ′(−e1, 0) = −1, Ỹ = 1\n∣∣∣ X̃ = −e1, Ã = 0)P(X̃ = −e1 ∣∣∣ Ã = 0) (78) + P ( f ′(e1, 0) = 1, Ỹ = −1\n∣∣∣ X̃ = e1, Ã = 0)P(X̃ = e1 ∣∣∣ Ã = 0) = P ( f ′(−e1, 0) = −1 ) P ( X̃ = −e1, Ỹ = 1\n∣∣∣ Ã = 0) (79) + P ( f ′(e1, 0) = 1 ) P ( X̃ = e1, Ỹ = −1\n∣∣∣ Ã = 0) = P (f(−e1, 0) < 0) η−+ + P (f(e1, 0) ≥ 0) η+− (80)\nWith (79) following from the fact that conditioned on X̃ and Ã = 0, f ′(X̃, 0) is a random variable\nthat is independent of the value of Ỹ . Similarly, P ( |f ′(X̃, 0)| = 0, sign(f ′(X̃, 0)) = Ỹ ∣∣∣ Ã = 0) = P ( f ′(−e1, 0) = 0, Ỹ = 1\n∣∣∣ X̃ = −e1, Ã = 0)P(X̃ = −e1 ∣∣∣ Ã = 0) (81) + P ( f ′(e1, 0) = −0, Ỹ = −1\n∣∣∣ X̃ = e1, Ã = 0)P(X̃ = e1 ∣∣∣ Ã = 0) = P ( f ′(−e1, 0) = 0 ) P ( X̃ = −e1, Ỹ = 1\n∣∣∣ Ã = 0) (82) + P ( f ′(e1, 0) = −0 ) P ( X̃ = e1, Ỹ = −1\n∣∣∣ Ã = 0) = P (f(−e1, 0) ≥ 0) η−+ + P (f(e1, 0) < 0) η+− (83)\nCombining (77) with (80) and (83), we see that E [ `hinge(f ′)− `01(f ′) ∣∣∣ Ã = 0] = P (f(−e1, 0) < 0) η−+ + P (f(e1, 0) ≥ 0) η+− (84) + P (f(−e1, 0) ≥ 0) η−+ + P (f(e1, 0) < 0) η+−\n= η−+ + η+− (85)\nThe 0-1 loss of f ′ can be decomposed as\nL01D̃ (f ′) = P(Ã = 0)E\n[ `01(f) ∣∣∣ Ã = 0]+ P(Ã = 1)E [`01(f) ∣∣∣ Ã = 1] (86) By (85),\nP(Ã = 0)E [ `01(f) ∣∣∣ Ã = 0] = (1− δ)(E [`hinge(f) ∣∣∣ Ã = 0]− η−+ − η+−) (87) and since the hinge loss is always an upper bound on the 0-1 loss\nP(Ã = 1)E [ `01(f) ∣∣∣ Ã = 1] ≤ δE [`hinge(f) ∣∣∣ Ã = 1] (88) From Lemma 7, the hinge loss of f ′ (which is at most the hinge loss of f) is upper bounded by 2(1− δ)(η−+ + η+−) + 2δ + . Thus,\nL01D̃ (f ′) ≤ (1− δ)\n( E [ `hinge(f) ∣∣∣ Ã = 0]− η−+ − η+−)+ δE [`hinge(f) ∣∣∣ Ã = 1] (89) = Lhinge\nD̃ (f ′)− (1− δ)(η−+ + η+−) (90) ≤ (1− δ)(η−+ + η+−) + 2δ + (91)\nProof of Lemma 9. Because f ′ is α-discriminatory at threshold 0∣∣∣PD̃(f ′ ≥ 0 | Ỹ = −1, Ã = 0)− PD̃(f ′ ≥ 0 | Ỹ = −1, Ã = 1)∣∣∣ ≤ α∣∣∣PD̃(f ′ < 0 | Ỹ = 1, Ã = 0)− PD̃(f ′ < 0 | Ỹ = 1, Ã = 1)∣∣∣ ≤ α (92)\nLet f ′′(x) = f ′([0, x], 1), then\nL01D̃ (h) = PD̃(Ỹ = −1, Ã = 0)PD̃(f ′ ≥ 0 | Ỹ = −1, Ã = 0) (93)\n+ PD̃(Ỹ = 1, Ã = 0)PD̃(f ′ < 0 | Ỹ = 1, Ã = 0) + PD̃(Ỹ = −1, Ã = 1)PD̃(f ′ ≥ 0 | Ỹ = −1, Ã = 1) + PD̃(Ỹ = 1, Ã = 1)PD̃(f ′ < 0 | Ỹ = 1, Ã = 1)\n≥ PD̃(Ỹ = −1, Ã = 0) ( PD̃(f ′ ≥ 0 | Ỹ = −1, Ã = 1)− α )\n(94) + PD̃(Ỹ = 1, Ã = 0) ( PD̃(f ′ < 0 | Ỹ = 1, Ã = 1)− α ) + PD̃(Ỹ = −1, Ã = 1)PD̃(f ′ ≥ 0 | Ỹ = −1, Ã = 1) + PD̃(Ỹ = 1, Ã = 1)PD̃(f ′ < 0 | Ỹ = 1, Ã = 1)\n= ( PD̃(Ỹ = −1, Ã = 0) + PD̃(Ỹ = −1, Ã = 1) ) PD̃(f ′ ≥ 0 | Ỹ = −1, Ã = 1) (95)\n+ ( PD̃(Ỹ = 1, Ã = 0) + PD̃(Ỹ = 1, Ã = 1) ) PD̃(f ′ < 0 | Ỹ = 1, Ã = 1)\n+ ( PD̃(Ỹ = −1, Ã = 0) + PD̃(Ỹ = 1, Ã = 0) ) (−α)\n= PD̃(Ỹ = −1)PD̃(f ′ ≥ 0 | Ỹ = −1, Ã = 1) (96)\n+ PD̃(Ỹ = 1) PD̃(f ′ < 0 | Ỹ = 1, Ã = 1)− αPD̃(Ã = 0) (97) = PD(Y = −1)PD(f ′′ ≥ 0 | Y = −1) (98) + PD(Y = 1)PD(f ′′ < 0 | Y = 1)− α(1− δ) = L01D (f ′′)− α(1− δ) (99)\nThe lemma follows immediately."
    }, {
      "heading" : "E Proofs for Section 6 - Relaxing non-discrimination",
      "text" : "We use Σ· to denote covariances involving a vector and we reserve σ· for scalar covariances."
    }, {
      "heading" : "E.1 Proof of Theorem 4",
      "text" : "First, let us show the second claim. Let R be any linear predictor. By linearity, it follows that (R,A, Y ) are jointly Gaussian. By the conditional covariance formula, we have:\nσRA|Y = σRA − σRY σY A/σ2Y .\nIf R satisfies equalized correlations, then the right-hand side here is 0. It follows that R and A are uncorrelated conditionally on Y . But since they are jointly Gaussian, they are also independent conditionally on Y . Therefore R also satisfies equalized odds non-discrimination. The converse also holds: if R satisfies equalized odds, then R and A are uncorrelated given Y , and therefore equalized correlations is satified.\nNow let us move back to the main claim. Assume, without loss of generality, that all variables are centered. Let us first find the optimal (a priori not necessarily linear) predictor that satisfies the relaxed second-moment non-discrimination criterion. In particular, the Lagrangian to minimize may be written as:\nE[(R− Y )2]− λ ( σ2Y E[RA]− σAY E[RY ] ) ,\nBut just like in the unconstrained least squares problem, we may apply the law of total expectatons to condition the loss and the R- terms in the second-moment non-discrimination constraint to be conditioned on X and A. Thus the optimum is achieved for each X,A by minimizing the following Lagrangian:\nE[(R− Y )2|X,A]− λ ( σ2YAR− σAY E[Y |X,A]R ) ,\nor equivalently R2 − 2E[Y |X,A]R− λ ( σ2YAR− σAY E[Y |X,A]R ) . It follows that the optimal R is a linear function of A, E[Y |X,A] and λ. λ is determined over the statistics of the problem, and therefore it is a constant that does not depend on specific values of X and A, and E[Y |X,A] in the Gaussian setting is linear. It thus follows that the optimum, let’s call it R◦, is a linear function of X and A. It minimizes the expected square loss subject to a relaxed non-discrimination criterion, therefore it is not larger than the optimizer under the stricter constraint. Conversely, by linearity it does also satisfy the stricter constraint, and is thus no smaller than the optimizer under that constraint (recall that we didn’t start out by imposing linearity). Therefore R◦ is precisely the squared loss optimal equalized odds non-discriminatory predictor.\nE.2 Optimal equalized correlations linear predictor\nWe write the proofs more generally for a vector-valued protected attribute A, and the scalar case follows directly. In this case v is a matrix and α is a vector mixing the columns of v. First note that condition (13) translates into a linear constraint on w in the least-squares problem of Equation (15). Using the bilinearity of the covariance, this constraint is equivalent to:\nwTΣ[X A ] ,A − wTΣ[X A ] ,Y ΣY,A/σ 2 Y ≡ wTv = 0.\nWe can now write the cost function with a vector of Lagrange multipliers: J(w, λ) = E[(Y − wT [ X A ] )2] + wTvλ,\nwhose gradient is ∇wJ = 2Σ[X\nA ] , [ X A ]w − 2Σ[X A ] ,Y + vλ.\nSetting this to zero, the optimality conditions give us the claimed functional form. The vector α can then be obtained by enforcing the constraint:\nwTv = ( Σ Y, [ X A ] − αT vT ) Σ−1[ X A ] , [ X A\n]v = 0 and thus\nvTΣ−1[ X A ] , [ X A ]vα = vTΣ−1[ X A ] , [ X A ]Σ[X A ] ,Y ."
    }, {
      "heading" : "E.3 Proof of Theorem 5",
      "text" : "First, let us rewrite R? as: R? = w?T [ X A ] = ŵT [ X A ] − αTvTΣ−1[\nX A ] , [ X A\n] [X A ] = R̂− αTvTΣ−1[\nX A ] , [ X A\n] [X A ]\nNext, recall that v = Σ[X\nA\n] ,A − Σ[X\nA\n] ,Y ΣY,A/σ 2 Y ,\nand since\nΣ[X A ] , [ X A\n] = [Σ X, [ X A\n] Σ A, [ X A\n] ] ⇒ Σ\nA, [ X A ]Σ−1[ X A ] , [ X A ] = [ 0dim(A)×dim(X) Idim(A) ] , we have\nvTΣ−1[ X A ] , [ X A ] = [ 0 I ] + ΣA,Y ŵT /σ2Y . Therefore\nvTΣ−1[ X A ] , [ X A\n] [X A ] = A+ R̂\nσ2Y ΣA,Y\nand can be derived from (R̂, A, Y ). Then multiplying from the right by v and using the bilinearity of the covariance, we get the terms in α:\nvTΣ−1[ X A ] , [ X A ]v = ΣA,A − 1σ2Y ΣA,Y ΣY,A + 1σ2Y ΣA,Y ( Σ R̂,A − 1 σ2Y Σ R̂,Y ΣY,A ) ,\nand vTΣ−1[ X A ] , [ X A ]Σ[X A ] ,Y = ΣA,Y + 1 σ2Y ΣA,Y ΣR̂,Y .\nThis shows that α also derives from (R̂, A, Y ) as stated, completing the proof."
    } ],
    "references" : [ {
      "title" : "Introduction to statistical learning theory",
      "author" : [ "Olivier Bousquet", "Stéphane Boucheron", "Gábor Lugosi" ],
      "venue" : "In Advanced lectures on machine learning,",
      "citeRegEx" : "Bousquet et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bousquet et al\\.",
      "year" : 2004
    }, {
      "title" : "Complexity theoretic limitations on learning halfspaces",
      "author" : [ "Amit Daniely" ],
      "venue" : "arXiv preprint arXiv:1505.05800,",
      "citeRegEx" : "Daniely.,? \\Q2015\\E",
      "shortCiteRegEx" : "Daniely.",
      "year" : 2015
    }, {
      "title" : "From average case complexity to improper learning complexity",
      "author" : [ "Amit Daniely", "Nati Linial", "Shai Shalev-Shwartz" ],
      "venue" : "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Daniely et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Daniely et al\\.",
      "year" : 2014
    }, {
      "title" : "Equality of opportunity in supervised learning",
      "author" : [ "Moritz Hardt", "Eric Price", "Nathan Srebro" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Hardt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hardt et al\\.",
      "year" : 2016
    }, {
      "title" : "Big data: A report on algorithmic systems, opportunity, and civil rights. 2016",
      "author" : [ "White House" ],
      "venue" : "URL https://obamawhitehouse.archives.gov/sites/default/files/microsites/",
      "citeRegEx" : "House.,? \\Q2016\\E",
      "shortCiteRegEx" : "House.",
      "year" : 2016
    }, {
      "title" : "Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment",
      "author" : [ "Muhammad Bilal Zafar", "Isabel Valera", "Manuel Gomez-Rodriguez", "Krishna P. Gummadi" ],
      "venue" : "CoRR, abs/1610.08452,",
      "citeRegEx" : "Zafar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zafar et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "We consider learning a predictor which is non-discriminatory with respect to a “protected attribute” according to the notion of “equalized odds” proposed by Hardt et al. [2016]. We study the problem of learning such a non-discriminatory predictor from a finite training set, both statistically and computationally.",
      "startOffset" : 157,
      "endOffset" : 177
    }, {
      "referenceID" : 3,
      "context" : "The particular notion of non-discrimination we consider here is “equalized odds”, recently presented and studied by Hardt et al. [2016]:",
      "startOffset" : 116,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "Informally, we require that even if the correct label Y provides information about the protected attribute A, if we already know Y , the prediction does not provide any additional information about See Hardt et al. [2016] for a discussion on why it might be necessary for a non-discriminatory predictor to use A",
      "startOffset" : 202,
      "endOffset" : 222
    }, {
      "referenceID" : 3,
      "context" : "See Hardt et al. [2016] for further discussion of the definition, its implications, and comparisons to alternative notions.",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "One possible approach to learning a non-discriminative predictor is post hoc correction [Hardt et al., 2016]: first learn a good predictor ignoring non-discrimination, i.",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 3,
      "context" : "When the protected attribute A and the target Y are both binary, the post hoc correction algorithm proposed by Hardt et al. [2016] can be applied to a binary or real-valued predictor Ŷ ∈ H, deriving a randomized binary predictor that is non-discriminatory.",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "1 Hardt et al. [2016]] A predictor Ỹ is derived from a random variable R and protected attribute A if it is a possibly randomized function of (R,A) alone.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "For binary classification problems, the optimal post hoc correction Ỹ for a binary or real valued predictor Ŷ ∈ R is a straightforward ternary optimization problem: it is simply the nondiscriminatory, derived, binary predictor that minimizes the expectation of loss ` (Hardt et al. [2016]): Ỹ = argmin f :R×{0,1}7→{0,1} E ` ( f(Ŷ , A), Y )",
      "startOffset" : 269,
      "endOffset" : 289
    }, {
      "referenceID" : 3,
      "context" : "Indeed, Hardt et al. [2016] show that when the target Y is binary, if we can first find a predictor R that is exactly or nearly Bayes optimal for the squared loss over an unconstrained hypothesis class, then applying the post hoc correction (3) using the 0-1 loss (i.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "The above optimization problem is a finite sample adaptation of the post hoc correction in (3) proposed by Hardt et al. [2016]. As with the post hoc correction on the population (3), estimating a predictor Ỹ ∈ P(Ŷ ) derived from (Ŷ , A) is simply optimization over the following four parameters that completely specify Ỹ ,",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "Unfortunately, using a result by Daniely [2015] even this is computationally intractable: Theorem 3.",
      "startOffset" : 33,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "Unfortunately, using a result by Daniely [2015] even this is computationally intractable: Theorem 3. Let L∗ be the loss of the optimal linear predictor whose sign is non-discriminatory. Subject to the assumption that refuting random K-XOR formulas is computationally hard,3 the learning problem of finding a possibly randomized function f such that Lhinge(f) ≤ L∗+ such that sign(f) is α-discriminatory requires exponential time in the worst case for < 1 8 and α < 1 8 . The proof goes through a reduction from the hardness of improper, agnostic PAC learning of Halfspaces. Given a distribution D over (X,Y ) and the knowledge that there is a linear predictor which achieves 0-1 loss L∗ on D, we construct a new distribution D̃ over (X̃, Ã, Ỹ ) such that an approximately non-discriminatory predictor with small hinge loss can be used to make accurate predictions on D, even if it is not a linear function. The distribution D̃ is identical to the original distribution D when conditioned on Ã = 1, and is supported on only two points conditioned on Ã = 0. The probabilities of the two points are constructed so that satisfying non-discrimination requires making accurate predictions on the Ã = 1 population, and thus on D. In particular, for parameters , α < 1 8 , the predictor will have 0-1 loss at most 15 16L ∗ + 47 128 on D, which is bounded away from 12 when L ∗ < 1 10 . Since Daniely [2015] prove that finding a predictor with accuracy bounded away from 1 2 is hard in general, we conclude that the learning problem is computationally hard.",
      "startOffset" : 33,
      "endOffset" : 1399
    }, {
      "referenceID" : 4,
      "context" : "Previous work by Zafar et al. [2016] addresses a notion of non-discrimination which amounts to relaxing the equalized odds constraint that P(Ŷ = ŷ | Y = y,A = 0) = P(Ŷ = ŷ | Y = y,A = 1) to the constraint that E[Ŷ | Y = y,A = 0] = E[Ŷ | Y = y,A = 1], i.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "the first moments of Ŷ must See Daniely [2015] for a description of the problem.",
      "startOffset" : 32,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "Using this in standard VC dimension uniform bound [Bousquet et al., 2004], we have the following with high probability,",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "Using (42) and the standard VC dimension uniform bound [Bousquet et al., 2004], the following holds with high probability for absolute constants C1 and C2,",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "We will show that such an algorithm can be used to improperly weakly learn Halfspace which, subject to the complexity assumption that refuting random K-XOR formulas is hard, was shown to be computationally hard by Daniely [2015]. We conclude that A must be computationally hard to compute.",
      "startOffset" : 214,
      "endOffset" : 229
    }, {
      "referenceID" : 1,
      "context" : "3 from Daniely [2015] proves that there is no algorithm running in time polynomial in the dimension d that can return a predictor achieving 0-1 error ≤ 12 − d −c with high probability",
      "startOffset" : 7,
      "endOffset" : 22
    }, {
      "referenceID" : 1,
      "context" : "3 in Daniely [2015] involves a distribution D that is supported on the unit hypercube in Rd, thus the predictor h∗ ‖w∗‖2 √ d ∈ [−1, 1] with probability 1, and has the same 0-1 loss as h∗.",
      "startOffset" : 5,
      "endOffset" : 20
    } ],
    "year" : 2017,
    "abstractText" : "We consider learning a predictor which is non-discriminatory with respect to a “protected attribute” according to the notion of “equalized odds” proposed by Hardt et al. [2016]. We study the problem of learning such a non-discriminatory predictor from a finite training set, both statistically and computationally. We show that a post-hoc correction approach, as suggested by Hardt et al, can be highly suboptimal, present a nearly-optimal statistical procedure, argue that the associated computational problem is intractable, and suggest a second moment relaxation of the non-discrimination definition for which learning is tractable.",
    "creator" : "LaTeX with hyperref package"
  }
}