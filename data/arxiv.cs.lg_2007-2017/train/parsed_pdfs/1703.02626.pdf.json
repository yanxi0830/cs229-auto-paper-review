{
  "name" : "1703.02626.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Horde of Bandits using Gaussian Markov Random Fields",
    "authors" : [ "Sharan Vaswani", "Mark Schmidt", "Laks V.S. Lakshmanan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "The gang of bandits (GOB) model [7] is a recent contextual bandits framework that shares information between a set of bandit problems, related by a known (possibly noisy) graph. This model is useful in problems like recommender systems where the large number of users makes it vital to transfer information between users. Despite its effectiveness, the existing GOB model can only be applied to small problems due to its quadratic timedependence on the number of nodes. Existing solutions to combat the scalability issue require an often-unrealistic clustering assumption. By exploiting a connection to Gaussian Markov random fields (GMRFs), we show that the GOB model can be made to scale to much larger graphs without additional assumptions. In addition, we propose a Thompson sampling algorithm which uses the recent GMRF sampling-by-perturbation technique, allowing it to scale to even larger problems (leading to a “horde” of bandits). We give regret bounds and experimental results for GOB with Thompson sampling and epoch-greedy algorithms, indicating that these methods are as good as or significantly better than ignoring the graph or adopting a clustering-based approach. Finally, when an existing graph is not available, we propose a heuristic for learning it on the fly and show promising results."
    }, {
      "heading" : "1 Introduction",
      "text" : "Consider a newly established recommender system (RS) which has little or no information about the users’ preferences or any available rating data. The unavailability of rating data implies that we can not use traditional collaborative filtering based methods [41]. Furthermore,\nProceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS) 2017, Fort Lauderdale, Florida, USA. JMLR: W&CP volume 54. Copyright 2017 by the author(s).\nin the scenario of personalized news recommendation or for recommending trending Facebook posts, the set of available items is not fixed but instead changes continuously. This new RS can recommend items to the users and observe their ratings to learn their preferences from this feedback (“exploration”). However, in order to retain its users, at the same time it should recommend “relevant” items that will be liked by and elicit higher ratings from users (“exploitation”). Assuming each item can be described by its content (like tags describing a news article or video), the contextual bandits framework [29] offers a popular approach for addressing this exploration-exploitation trade-off.\nHowever, this framework assumes that users interact with the RS in an isolated manner, when in fact a RS might have an associated social component. In particular, given the large number of users on such systems, we may be able to learn their preferences more quickly by leveraging the relations between them. One way to use a social network of users to improve recommendations is with the recent gang of bandits (GOB) model [7]. In particular, the GOB model exploits the homophily effect [35] that suggests users with similar preferences are more likely to form links in a social network. In other words, user preferences vary smoothly across the social graph and tend to be similar for users connected with each other. This allows us to transfer information between users; we can learn about a user from his or her friends’ ratings. However, the existing recommendation algorithm in the GOB framework has a quadratic time-dependence on the number of nodes (users) and thus can only be used for a small number of users. Several recent works have tried to improve the scaling of the GOB model by clustering the users into groups [17, 36], but this limits the flexibility of the model and loses the ability to model individual users’ preferences.\nIn this paper, we cast the GOB model in the framework of Gaussian Markov random fields (GMRFs) and show how to exploit this connection to scale it to much larger graphs. Specifically, we interpret the GOB model as the optimization of a Gaussian likelihood on the users’ observed ratings and interpret the user-user graph as the\nar X\niv :1\n70 3.\n02 62\n6v 1\n[ cs\n.L G\n] 7\nM ar\n2 01\n7\nprior inverse-covariance matrix of a GMRF. From this perspective, we can efficiently estimate the users’ preferences by performing MAP estimation in a GMRF. In addition, we propose a Thompson sampling GOB variant that exploits the recent sampling-by-perturbation idea from the GMRF literature [37] to scale to even larger problems. This idea is fairly general and might be of independent interest in the efficient implementation of other Thompson sampling methods. We establish regret bounds (Section 4) and provide experimental results (Section 5) for Thompson sampling as well as an epoch-greedy strategy. These experiments indicate that our methods are as good as or significantly better than approaches which ignore the graph or that cluster the nodes. Finally, when the graph of users is not available, we propose a heuristic for learning the graph and user preferences simultaneously in an alternating minimization framework (Appendix A)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Social Regularization: Using social information to improve recommendations was first introduced by Ma et al. [31]. They used matrix factorization to fit existing rating data but constrained a user’s latent vector to be similar to their friends in the social network. Other methods based on collaborative filtering followed [38, 13], but these works assume that we already have rating data available. Thus, these methods do not address the exploration-exploitation trade-off faced by a new RS that we consider.\nBandits: The multi-armed bandit problem is a classic approach for trading off exploration and exploitation as we collect data [26]. When features (context) for the “arms” are available and changing, it is referred to as the contextual bandit problem [4, 29, 9]. The contextual bandit framework is important for the scenario we consider where the set of items available is constantly changing, since the features allow us to make predictions about items we have never seen before. Algorithms for the contextual bandits problem include epoch-greedy methods [27], those based on upper confidence bounds (UCB) [9, 1], and Thompson sampling methods [2]. Note that these standard contextual bandit methods do not model the user-user dependencies that we want to exploit.\nSeveral graph-based methods to model dependencies between the users have been explored in the (noncontextual) multi-armed bandit framework [6, 33, 3, 32], but the GOB model of Cesa-Bianchi et al. [7] is the first to exploit the network between users in the contextual bandit framework. They proposed a UCB-style algorithm and showed that using the graph leads to lower regret from both a theoretical and practical standpoint. However, their algorithm has a time complexity that is quadratic in the number of users. This makes it\ninfeasible for typical RS that have tens of thousands (or even millions) of users.\nTo scale up the GOB model, several recent works propose to cluster the users and assume that users in the same cluster have the same preferences [17, 36]. But this solution loses the ability to model individual users’ preferences, and indeed our experiments indicate that in some applications clustering significantly hurts performance. In contrast, we want to scale up the original GOB model that learns more fine-grained information in the form of a preference-vector specific to each user.\nAnother interesting approach to relax the clustering assumption is to cluster both items and users [30], but this only applies if we have a fixed set of items. Some works consider item-item similarities to improve recommendations [42, 23], but this again requires a fixed set of items while we are interested in RS where the set of items may constantly be changing. There has also been work on solving a single bandit problem in a distributed fashion [24], but this differs from our approach where we are solving an individual bandit problem on each of the n nodes. Finally, we note that all of the existing graph-based works consider relatively small RS datasets (∼ 1k users), while our proposed algorithms can scale to much larger RS."
    }, {
      "heading" : "3 Scaling up Gang of Bandits",
      "text" : "In this section we first describe the general GOB framework, then discuss the relationship to GMRFs, and finally show how this leads to more scalable method. In this paper Tr(A) denotes the trace of matrix A, A ⊗ B denotes the Kronecker product of matrices A and B, Id is used for the d-dimensional identity matrix, and vec(A) is the stacking of the columns of a matrix A into a vector."
    }, {
      "heading" : "3.1 Gang of Bandits Framework",
      "text" : "The contextual bandits framework proceeds in rounds. In each round t, a set of items Ct becomes available. These items could be movies released in a particular week, news articles published on a particular day, or trending stories on Facebook. We assume that |Ct| = K for all t. We assume that each item j can be described by a context (feature) vector xj ∈ Rd. We use n as the number of users, and denote the (unknown) ground-truth preference vector for user i as w∗i ∈ Rd. Throughout the paper, we assume there is only a single target user per round. It is straightforward extend our results to multiple target users.\nGiven a target user it, our task is to recommend an available item jt ∈ Ct to them. User it then provides feedback on the recommended item jt in the form of a rating rit,jt . Based on this feedback, the estimated preference vector for user it is updated. The recommendation algorithm must trade-off between exploration\n(learning about the users’ preferences) and exploitation (obtaining high ratings). We evaluate performance using the notion of regret, which is the loss in recommendation performance due to lack of knowledge of user preferences. In particular, the regret R(T ) after T rounds is given by:\nR(T ) = T∑ t=1 [ max j∈Ct (w∗Tit xj)−w ∗T it xjt ] . (1)\nIn our analysis we make the following assumptions:\nAssumption 1. The `2-norms of the true preference vectors and item feature vectors are bounded from above. Without loss of generality we’ll assume ||xj ||2 ≤ 1 for all j and ||w∗i ||2 ≤ 1 for all i. Also without loss of generality, we assume that the ratings are in the range [0, 1].\nAssumption 2. The true ratings can be given by a linear model [29], meaning that ri,j = (w∗i )Txj + ηi,j,t for some noise term ηi,j,t.\nThese are standard assumptions in the literature. We denote the history of observations until round t as Ht−1 = {(iτ , jτ , riτ ,jτ )}τ=1,2···t−1 and the union of the set of available items until round t along with their corresponding features as Ct−1.\nAssumption 3. The noise ηi,j,t is conditionally subGaussian [2][7] with zero mean and bounded variance, meaning that E[ηi,j,t | Ct−1,Ht−1] = 0 and that there exists a σ > 0 such that for all γ ∈ R, we have E[exp(γηi,j,t) | Ht−1,Ct−1] ≤ exp(γ 2σ2 2 ).\nThis assumption implies that for all i and j, the conditional mean is given by E[ri,j |Ct−1,Ht−1] = w∗Ti xj and that the conditional variance satisfies V[ri,j |Ct−1,Ht−1] ≤ σ2.\nIn the GOB framework, we assume access to a (fixed) graphG = (V, E) of users in the form of a social network (or “trust graph”). Here, the nodes V correspond to users, whereas the edges E correspond to friendships or trust relationships. The homophily effect implies that the true user preferences vary smoothly across the graph, so we expect the preferences of users connected in the graph to be close to each other. Specifically,\nAssumption 4. The true user preferences vary smoothly according to the given graph, in the sense that we have a small value of∑\n(i1,i2)∈E\n||w∗i1 −w ∗ i2 || 2.\nHence, we assume that the graph acts as a correctlyspecified prior on the users’ true preferences. Note\nthat this assumption implies that nodes in dense subgraphs will have a higher similarity than those in sparse subgraphs (since they will have a larger number of neighbours).\nThis assumption is violated in some datasets. For example, in our experiments we consider one dataset in which the available graph is imperfect, in that user preferences do not seem to vary smoothly across all graph edges. Intuitively, we might think that the GOB model might be harmful in this case (compared to ignoring the graph structure). However, in our experiments, we observe that even in these cases, the GOB approach still lead to results as good as ignoring the graph.\nThe GOB model [7] solves a contextual bandit problem for each user, where the mean vectors in the different problems are related according to the Laplacian L1 of the graph G. Let wi,t be the preference vector estimate for user i at round t. Let wt and w∗ ∈ Rdn (respectively) be the concatenation of the vectors wi,t and w∗i across all users. The GOB model solves the following regression problem to find the mean preference vector estimate at round t,\nwt = argminw [ n∑ i=1 ∑ k∈Mi,t (wTi xk − ri,k)2\n+λwT (L⊗ Id)w ] , (2)\nwhere Mi,t is the set of items rated by user i up to round t. The first term is a data-fitting term and models the observed ratings. The second term is the Laplacian regularization and equal to ∑ (i,j)∈E λ||wi,t− wj,t||22. This term models smoothness across the graph with λ > 0 giving the strength of this regularization. Note that the same objective function has also been explored for graph-regularized multi-task learning [14]."
    }, {
      "heading" : "3.2 Connection to GMRFs",
      "text" : "Unfortunately, the approach of Cesa-Bianchi [7] for solving (2) has a computational complexity of O(d2n2). To solve (2) more efficiently, we now show that it can be interpreted as performing MAP estimation in a GMRF. This will allow us to apply the GOB model to much larger datasets, and lead to an even more scalable algorithm based on Thompson sampling (Section 4).\nConsider the following generative model for the ratings ri,j and the user preference vectors wi,\nri,j ∼ N (wTi xj , σ2), w ∼ N (0, (λL⊗ Id)−1).\nThis GMRF model assumes that the ratings ri,j are independent given wi and xj , which is the standard\n1To ensure invertibility, we set L = LG + In where LG is the normalized graph Laplacian.\nregression assumption. Under this independence assumption the first term in (2) is equal up the negative log-likelihood for all of the observed ratings rt at time t, log p(rt | w,xt, σ), up to an additive constant and assuming σ = 1. Similarly, the negative log-prior p(w | λ, L) in this model gives the second term in (2) (again, up to an additive constant that does not depend on w). Thus, by Bayes rule minimizing (2) is equivalent to maximizing the posterior in this GMRF model.\nTo characterize the posterior, it is helpful to introduce the notation φi,j ∈ Rdn to represent the “global” feature vector corresponding to recommending item j to user i. In particular, let φi,j be the concatenation of n d-dimensional vectors where the ith vector is equal to xj and the others are zero. The rows of the t×dn dimensional matrix Φt correspond to these “global” features for all the recommendations made until time t. Under this notation, the posterior p(w | rt,w,Φt) is given by a N (ŵt,Σ−1t ) distribution with Σt = 1σ2 Φ T t Φt+λ(L⊗Id) and ŵt = 1σ2 Σ −1 t bt with bt = ΦTt rt. We can view the approach in [7] as explicitly constructing the dense dn × dn matrix Σ−1t , leading to an O(d2n2) memory requirement. A new recommendation at round t is thus equivalent to a rank-1 update to Σt, and even with the Sherman-Morrison formula this leads to an O(d2n2) time requirement for each iteration."
    }, {
      "heading" : "3.3 Scalability",
      "text" : "Rather than treating Σt as a general matrix, we propose to exploit its structure to scale up the GOB framework to problems where n is very large. In particular, solving (2) corresponds to finding the mean vector of the GMRF, which corresponds to solving the linear system Σtw = bt. Since Σt is positivedefinite, the linear system can be solved using conjugate gradient [20]. Conjugate gradient notably does not require Σ−1t , but instead uses matrix-vector products Σtv = (ΦTt Φt)v + λ(L ⊗ Id)v for vectors v ∈ Rdn. Note that ΦTt Φt is block diagonal and has only O(nd2) non-zeroes. Hence, ΦTt Φtv can be computed in O(nd2) time. For computing (L ⊗ Id)v, we use that (BT ⊗A)v = vec(AV B), where V is an n× d matrix such that vec(V ) = v. This implies (L⊗ Id)v can be written as V LT which can be computed in O(d · nnz(L)) time, where nnz(L) is the number of non-zeroes in L. This approach thus has a memory requirement of O(nd2 + nnz(L)) and a time complexity of O(κ(nd2 +d ·nnz(L))) per mean estimation. Here, κ is the number of conjugate gradient iterations which depends on the condition number of the matrix (we used warm-starting by the solution in the previous round for our experiments, which meant that κ = 5 was enough for convergence). Thus, the algorithm scales linearly in n and in the number of edges of the network (which\ntends to be linear in n due to the sparsity of social relationships). This enables us to scale to large networks, of the order of 50K nodes and millions of edges."
    }, {
      "heading" : "4 Alternative Bandit Algorithms",
      "text" : "The above structure can be used to speed up the mean estimation for any algorithm in the GOB framework. However, the LINUCB-like algorithm in [7] needs to estimate the confidence intervals √ φTi,jΣ−1t φi,j for each available item j ∈ Ct. Using the GMRF connection, estimating these requires O(|Ct|κ(nd2 + d · nnz(L))) time since we need solve the linear system with |Ct| right-hand sides, one for each available item. But this becomes impractical when the number of available items in each round is large.\nWe propose two approaches for mitigating this: first, in this section we adapt the epoch-greedy [27] algorithm to the GOB framework. Epoch-greedy doesn’t require confidence intervals and is thus very scalable, but unfortunately it doesn’t achieve the optimal regret of Õ( √ T ). To achieve the optimal regret, we also propose a GOB variant of Thompson sampling [29]. In this section we further exploit the connection to GMRFs to scale Thompson sampling to even larger problems by using the recent sampling-by-perturbation trick [37]. This GMRF connection and scalability trick might be of independent interest for Thompson sampling in other large-scale problems."
    }, {
      "heading" : "4.1 Epoch-Greedy",
      "text" : "Epoch-greedy [27] is a variant of the popular -greedy algorithm that explicitly differentiates between exploration and exploitation rounds. An “exploration” round consists of recommending a random item from Ct to the target user it. The feedback from these exploration rounds is used to learn w∗. An “exploitation” round consists of choosing the available item ĵt which maximizes the expected rating, ĵt = argmaxj∈Ct ŵ T t φit,j . Epoch-greedy proceeds in epochs, where each epoch q consists of 1 exploration round and sq exploitation rounds.\nScalability: The time complexity for Epoch-Greedy is dominated by the exploitation rounds that require computing the mean and estimating the expected rating for all the available items. Given the mean vector, this estimation takes O(d|Ct|) time. The overall time complexity per exploitation round is thus O(κ(nd2 + d · nnz(L)) + d|Ct|).\nRegret: We assume that we incur a maximum regret of 1 in an exploration round, whereas the regret incurred in an exploitation round depends on how well we have learned w∗. The attainable regret is thus proportional to the generalization error for the class of hypothesis functions mapping the context vector to an expected rating [27]. In our case, the class of hypotheses is a set\nof linear functions (one for each user) with Laplacian regularization. We characterize the generalization error in the GOB framework in terms of its Rademacher complexity [34], and use this to bound the expected regret leading to the result below. For ease of exposition in the regret bounds, we suppress the factors that don’t depend on either n, L, λ or T . The complete bound is stated in the supplementary material (Appendix B).\nTheorem 1. Under the additional assumption that ||wt||2 ≤ 1 for all rounds t, the expected regret obtained by epoch-greedy in the GOB framework is given as:\nR(T ) = Õ ( n1/3 ( Tr(L−1) λn ) 1 3 T 2 3 )\nProof Sketch. Let H be the class of valid hypotheses of linear functions coupled with Laplacian regularization. Let Err(q,H) be the generalization error for H after obtaining q unbiased samples in the exploration rounds. We adapt Corollary 3.1 from [27] to our context:\nLemma 1. If sq = ⌊\n1 Err(q,H)\n⌋ and QT is the smallest\nQ such that Q+ ∑Q q=1 sq ≥ T , the regret obtained by Epoch-Greedy can be bounded as R(T ) ≤ 2QT .\nWe use [34] to bound the generalization error of our class of hypotheses in terms of its empirical Rademacher complexity R̂nq (H). With probability 1− δ,\nErr(q,H) ≤ R̂nq (H) +\n√ 9 ln(2/δ)\n2q . (3)\nUsing Theorem 2 in [34] and Theorem 12 from [5], we obtain\nR̂nq (H) ≤ 2 √ q\n√ 12Tr(L−1)\nλ . (4)\nUsing (3) and (4) we obtain\nErr(q,H) ≤\n[ 2 √ 12Tr(L−1)/λ+ √\n9 ln(2/δ) 2 ] √ q . (5)\nThe theorem follows from (5) along with Lemma 1.\nThe effect of the graph on this regret bound is reflected through the term Tr(L−1). For a connected graph, we have the following upper-bound Tr(L\n−1) n ≤\n(1−1/n) ν2 + 1n [34]. Here, ν2 is the second smallest eigenvalue of the Laplacian. The value ν2 represents the algebraic connectivity of the graph [15]. For a more connected graph, ν2 is higher, the value of Tr(L −1) n is\nlower, resulting in a smaller regret. Note that although this result leads to a sub-optimal dependence on T (T 23 instead of T 12 ), our experiments incorporate a small modification that gives similar performance to the more-expensive LINUCB."
    }, {
      "heading" : "4.2 Thompson sampling",
      "text" : "A common alternative to LINUCB and Epoch-Greedy is Thompson sampling (TS). At each iteration TS uses a sample w̃t from the posterior distribution at round t, w̃t ∼ N (wt,Σ−1t ). It then selects the item jt based on the obtained sample, jt = argmaxj∈Ct w̃ T t φit,j . We show below that the GMRF connection makes TS scalable, but unlike Epoch-Greedy it also achieves the optimal regret.\nScalability: The conventional approach for sampling from a multivariate Gaussian posterior involves forming the Cholesky factorization of the posterior covariance matrix. But in the GOB model the posterior covariance matrix is a dn-dimensional matrix where the fill-in from the Cholesky factorization can lead to a computational complexity of O(d2n2). In order to implement Thompson sampling for large values of n, we adapt the recent sampling-by-perturbation approach [37] to our setting, and this allows us to sample from a Gaussian prior and then solve a linear system to sample from the posterior.\nLet w̃0 be a sample from the prior distribution and let r̃t be the perturbed (with standard normal noise) rating vector at round t, meaning that r̃t = rt + yt for yt ∼ N (0, It). In order to obtain a sample w̃t from the posterior, we can solve the linear system\nΣtw̃t = (L⊗ Id)w̃0 + ΦTt r̃t. (6)\nLet S be the Cholesky factor of L so that L = SST . Note that L⊗ Id = (S⊗ Id)(S⊗ Id)T . If z ∼ N (0, Idn), we can obtain a sample from the prior by solving (S ⊗ Id)w̃0 = z. Since S tends to be sparse (using for example [12, 25]), this equation can be solved efficiently using conjugate gradient. We can pre-compute and store S and thus obtain a sample from the prior in time O(d · nnz(L)). Using that ΦTt r̃t = bt + ΦTt yt in (6) and simplifying we obtain\nΣtw̃t = (L⊗ Id)w̃0 + bt + ΦTt yt (7)\nAs before, this system can be solved efficiently using conjugate gradient. Note that solving (7) results in an exact sample from the dn-dimensional posterior. Computing ΦTt yt has a time complexity of O(dt). Thus, this approach is faster than the original GOB framework whenever t < dn2. Since we focus on the case of large graphs, this condition will tend to hold in our setting.\nWe now describe an alternative method of constructing the right side of (7) that doesn’t depend on t. Observe\nthat computing ΦTt yt is equivalent to sampling from the distribution N (0,ΦTt Φt). To sample from this distribution, we maintain the Cholesky factor Pt of ΦTt Φt. Recall that the matrix ΦTt Φt is block diagonal (one block for every user) for all rounds t. Hence, its Cholesky factor Pt also has a block diagonal structure and requires O(nd2) storage. In each round, we make a recommendation to a single user and thus make a rank-1 update to only one d × d block of Pt. This is an order O(d2) operation. Once we have an updated Pt, sampling from N (0,ΦTt Φt) and constructing the right side of (7) is an O(nd2) operation. The per-round computational complexity for our TS approach is thus O(min{nd2, dt}+ d ·nnz(L)) for forming the right side in (7), O(nd2 +d ·nnz(L)) for solving the linear system in (7) as well as for computing the mean, and O(d · |Ct|) for selecting the item. Thus, our proposed approach has a complexity linear in the number of nodes and edges and can scale to large networks.\nRegret: To analyze the regret with TS, observe that TS in the GOB framework is equivalent to solving a single dn-dimensional contextual bandit problem, but with a modified prior covariance equal to (λL⊗ Id)−1 instead of Idn. We obtain the result below by following a similar argument to Theorem 1 in [2]. The main challenge in the proof is to make use of the available graph to bound the variance of the arms. We first state the result and then sketch the main differences from the original proof.\nTheorem 2. Under the following additional technical assumptions: (a) log(K) < (dn− 1) ln(2), (b) λ < dn, and (c) log ( 3+T/λdn\nδ\n) ≤ log(KT ) log(T/δ), with prob-\nability 1−δ, the regret obtained by Thompson Sampling in the GOB framework is given as:\nR(T ) = Õ ( dn √ T√ λ √ log ( 3 Tr(L−1) n + Tr(L −1)T λdn2σ2 ))\nProof Sketch. To make the notation cleaner, for the round t and target user it under consideration, we use j to index the available items. Let the index of the optimal item at round t be j∗t whereas the index of the item chosen by our algorithm is denoted jt. Let st(j) be the standard deviation in the estimated rating of item j at round t. It is given as st(j) = √ φTj Σ −1 t−1φj .\nFurther, let lt = √ dn log ( 3+t/λdn\nδ\n) + √\n3λ. Let Eµ(t) be the event such that for all j,\nEµ(t) : |〈wt,φj〉 − 〈w ∗,φj〉| ≤ ltst(j)\nWe prove that, for δ ∈ (0, 1), Pr(Eµ(t)) ≥ 1− δ. Define gt = √ 4 log(tK)ρt + lt, where ρt = √ 9d log ( t δ ) . Let\nγ = 14e√π . Given that the event E µ(t) holds with high probability, we follow an argument similar to Lemma 4 of [2] and obtain the following bound:\nR(T ) ≤ 3gT γ T∑ t=1 st(jt) + 2gT γ T∑ t=1 1 t2\n+6gT γ\n√ 2T ln 2/δ (8)\nTo bound the variance of the selected items,∑T t=1 st(jt), we extend the analysis in [11, 43] to include the prior covariance term. We thus obtain the following inequality:\nT∑ t=1 st(jt) ≤ √ dnT\n× √ C log ( Tr(L−1)\nn\n) + log ( 3 + T\nλdnσ2 ) (9)\nwhere C = 1 λ log(1+ 1 λσ2 ) . Substituting this into (8) completes the proof.\nNote that since n is large in our case, assumption (a) for the above theorem is reasonable. Assumptions (b) and (c) define the upper and lower bounds on the regularization parameter λ. Similar to epoch-greedy, transferring information across the graph reduces the regret by a factor dependent on Tr(L−1). Note that compared to epoch-greedy, the regret bound for Thompson sampling has a worse dependence on n, but its Õ( √ T ) dependence on T is optimal. If L = Idn, we match the Õ(dn √ T ) regret bound for a dn-dimensional contextual bandit problem [1]. Note that we have a dependence on d and n similar to the original GOB paper [7] and that this method performs similarly in practice in terms of regret. However, as will see, our algorithm is much faster."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "Data: We first test the scalability of various algorithms using synthetic data and then evaluate their regret performance on two real datasets. For synthetic data we generate random d-dimensional context vectors and ground-truth user preferences, and generate the ratings according to the linear model. We generated a random Kronecker graph with sparsity 0.005 (which is approximately equal to the sparsity of our real datasets). It is well known that such graphs capture many properties of real-world social networks [28].\nFor the real data, we use the Last.fm and Delicious datasets which are available as part of the HetRec 2011 workshop. Last.fm is a music streaming website where each item corresponds to a music artist and the dataset consists of the set of artists each user has listened to. The associated social network consists of 1.8K users (nodes) and 12.7K friendship relations (edges). Delicious is a social bookmarking website, where an item corresponds to a particular URL and the dataset consists of the set of websites bookmarked by each user. Its corresponding social network consists of 1.8K users and 7.6K user-user relations. Similar to [7], we use the set of associated tags to construct the TF-IDF vector for each item and reduce the dimension of these vectors to d = 25. An artist (or URL) that a user has listened to (or has bookmarked) is said to be “liked” by the user. In each round, we select a target user uniformly at random and make the set Ct consist of 25 randomly chosen items such that there is at least 1 item liked by the target user. An item liked by the target user is assigned a reward of 1 whereas other items are assigned a zero reward. We use a total of T = 50 thousand recommendation rounds and average our results across 3 runs.\nAlgorithms: We denote our graph-based epochgreedy and Thompson sampling algorithms as G-EG and G-TS, respectively. For epoch-greedy, although the theory suggests that we update the preference estimates only in the exploration rounds, we observed better performance by updating the preference vectors in all rounds (we use this variant in our experiments). We use 10% of the total number of rounds for exploration, and we “exploit\" in the remaining rounds. Similar to [17], all hyper-parameters are set using an initial validation set of 5 thousand rounds. The best validation performance was observed for λ = 0.01 and σ = 1. To control the amount of exploration for Thompson sampling, we the use posterior reshaping trick [8] which reduces the variance of the posterior by a factor of 0.01.\nBaselines: We consider two variants of graph-based UCB-style algorithms: GOBLIN is the method proposed in the original GOB paper [7] while we use GOBLIN++ to refer to a variant that exploits the fast mean estimation strategy we develop in Section 3.3. Similar to [7], for both variants we discount the confidence bound term by a factor of α = 0.01.\nWe also include baselines which ignore the graph structure and make recommendations by solving independent linear contextual bandit problems for each user. We consider 3 variants of this baseline: the LINUCBIND proposed in [29], an epoch-greedy variant of this approach (EG-IND), and a Thompson sampling variant (TS-IND). We also compared to a baseline that does no personalization and simply considers a single bandit\nproblem across all users (LINUCB-SIN). Finally, we compared against the state-of-the-art online clusteringbased approach proposed in [17], denoted CLUB. This method starts with a fully connected graph and iteratively deletes edges from the graph based on UCB estimates. CLUB considers each connected component of this graph as a cluster and maintains one preference vector for all the users belonging to a cluster. Following the original work, we make CLUB scalable by generating a random Erdos-Renyi graph Gn,p with p = 3lognn . 2 In all, we compare our proposed algorithms G-EG and G-TS with 7 reasonable baseline methods."
    }, {
      "heading" : "5.2 Results",
      "text" : "Scalability: We first evaluate the scalability of the various algorithms with respect to the number of network nodes n. Figure 1(a) shows the runtime in seconds/iteration when we fix d = 25 and vary the size of the network from 16 thousand to 33 thousand nodes. Compared to GOBLIN, our proposed GOBLIN++ is more efficient in terms of both time (almost 2 orders of magnitude faster) and memory. Indeed, the existing GOBLIN method runs out of memory even on very small networks and thus we do not plot it for larger networks. Further, our proposed G-EG and G-TS methods scale even more gracefully in the number of nodes and are much faster than GOBLIN++ (although not as fast as the clustering-based CLUB or methods that ignore the graph).\nWe next consider scalability with respect to d. Figure 1(b) fixes n = 1024 and varies d from 10 to 500. In this figure it is again clear that our proposed GOBLIN++ scales much better than the original GOBLIN algorithm. The EG and TS variants are again even faster, and other key findings from this experiment are (i) it was not faster to ignore the graph and (ii) our proposed G-EG and G-TS methods scale better with d than CLUB.\nRegret Minimization: We follow [17] in evaluating recommendation performance by plotting the ratio of cumulative regret incurred by the algorithm divided by the regret incurred by a random selection policy. Figure 2(a) plots this measure for the Last.fm dataset. In this dataset we see that treating the users independently (LINUCB-IND) takes a long time to drive down the regret (we do not plot EG-IND and TS-IND as they had similar performance) while simply aggregating across users (LINUCB-SIN) performs well initially (but eventually stops making progress). We see that the approaches exploiting the graph help learn the user\n2We reimplemented CLUB. Note that one of the datasets from our experiments was also used in that work and we obtain similar performance to that reported in the original paper.\n(a) (b)Figure 1: Synthetic network: Runtime (in seconds/iteration) vs (a) Number of nodes (b) Dimension\n(a) Last.fm (b) DeliciousFigure 2: Regret Minimization\npreferences faster than the independent approach and we note that on this dataset our proposed G-TS method performed similar to or slightly better than the state of the art CLUB algorithm.\nFigure 2(b) shows performance on the Delicious dataset. On this dataset personalization is more important and we see that the independent method (LINUCB-IND) outperforms the non-personalized (LINUCB-SIN) approach. The need for personalization in this dataset also leads to worse performance of the clustering-based CLUB method, which is outperformed by all methods that model individual users. On this dataset the advantage of using the graph is less clear, as the graph-based methods perform similar to the independent method. Thus, these two experiments suggest that (i) the scalable graph-based methods do no worse than ignoring the graph in cases where the graph is not helpful and (ii) the scalable graph-based methods can do significantly better on datasets where the graph is helpful. Similarly, when user preferences naturally form clusters our proposed methods perform similarly to CLUB, whereas on datasets where individual preferences are\nimportant our methods are significantly better."
    }, {
      "heading" : "6 Discussion",
      "text" : "This work draws a connection between the GOB framework and GMRFs, and uses this to scale up the existing GOB model to much larger graphs. We also proposed and analyzed Thompson sampling and epochgreedy variants. Our experiments on recommender systems datasets indicate that the Thompson sampling approach in particular is much more scalable than existing GOB methods, obtains theoretically optimal regret, and performs similar to or better than other existing scalable approaches.\nIn many practical scenarios we do not have an explicit graph structure available. In the supplementary material we consider a variant of the GOB model where we use L1-regularization to learn the graph on the fly. Our experiments there show that this approach works similarly to or much better than approaches which use the fixed graph structure. It would be interesting to explore the theoretical properties of this approach."
    }, {
      "heading" : "A Learning the Graph",
      "text" : "In the main paper, we assumed that the graph is known, but in practice such a user-user graph may not be available. In such a case, we explore a heuristic to learn the graph on the fly. The computational gains described in the main paper make it possible to simultaneously learn the user-preferences and infer the graph between users in an efficient manner. Our approach for learning the graph is related to methods proposed for multitask and multilabel learning in the batch setting [19, 18] and multitask learning in the online setting [40]. However, prior works that learn the graph in related settings only tackle problem with tens or hundreds of tasks/labels while we\nlearn the graph and preferences across thousands of users.\nLet Vt ∈ Rn×n be the inverse covariance matrix corresponding to the graph inferred between users at round t. Since zeroes in the inverse covariance matrix correspond to conditional independences between the corresponding nodes (users) [39], we use L1 regularization on Vt for encouraging sparsity in the inferred graph. We use an additional regularization term ∆(Vt||Vt−1) to encourage the graph to change smoothly across rounds. This encourages Vt to be close to Vt−1 according to a distance metric ∆. Following [40], we choose ∆ to be the log-determinant Bregman divergence given by ∆(X||Y ) = Tr(XY −1)− log |XY −1| − dn. If Wt ∈ Rd×n = [w1w2 . . .wn] corresponds to the matrix of user preference estimates, the combined objective can\nbe written as:\n[wt, Vt] = argmin w,V\n||rt − Φtw||22 + Tr ( V (λWTW + V −1t−1) ) + λ2||V ||1 − (dn+ 1) ln |V | (10)\nThe first term in (10) is the data fitting term. The second term imposes the smoothness constraint across the graph and ensures that the changes in Vt are smooth. The third term ensures that the learnt precision matrix is sparse, whereas the last term penalizes the complexity of the precision matrix. This function is independently convex in both w and V (but not jointly convex), and we alternate between solving for wt and Vt in each round. With a fixed Vt, the w sub-problem is the same as the MAP estimation in the main paper and can be done\nefficiently. For a fixed wt, the V sub-problem is given by\nVt = argmin V\nTr ( (V [λWTt W t + V −1t−1) ) + λ2||V ||1 − (dn+ 1) ln |V | (11)\nHere W t refers to the mean subtracted (for each dimension) matrix of user preferences. This problem can be written as a graphical lasso problem [16], minX Tr(SX) + λ2||X||1 − log |X|, where the empirical covariance\nmatrix S is equal to λWTt W t + V −1t−1. We use the highly-scalable second order methods described in [21, 22] to solve (11). Thus, both sub-problems in the alternating minimization framework at each round can be solved\nefficiently.\nFor our preliminary experiments in this direction, we use the most scalable epoch-greedy algorithm for learning the graph on the fly and denote this version as L-EG. We also consider another variant, U-EG in which we start from the Laplacian matrix L corresponding to the given graph and allow it to change by re-estimating the graph according to (11). Since U-EG has the flexibility to infer a better graph than the one given, such a variant is important for cases where the prior is meaningful but somewhat misspecified (the given graph accurately reflects some but not all of the user similarities). Similar to [40], we start off with an empty graph and start learning the\ngraph only after the preference vectors have become stable, which happens in this case after each user has received 10 recommendations. We update the graph every 1K rounds. For both datasets, we allow the learnt\ngraph to contain at most 100K edges and tune λ2 to achieve a sparsity level equal to 0.05 in both cases.\nTo avoid clutter, we plot all the variants of the EG algorithm, L-EG and U-EG, and use EG-IND, G-EG, EG-SIN as baselines. We also plot CLUB as a baseline. For the Last.fm dataset (Figure 3(b)(a)), U-EG performs slightly better than G-EG, which already performed well. The regret for L-EG is lower compared to LINUCB-IND indicating that learning the graph helps, but is worse as compared to both CLUB and LINUCB-SIN. On the other hand, for Delicious (Figure 3(b)(b)), L-EG and U-EG are the best performing methods. L-EG slightly outperforms EG-IND, underscoring the importance of learning the user-user graph and transferring information between users. It also outperforms G-EG, which implies that it is able to learn a graph which reflects user\nsimilarities better than the existing social network between users. For both datasets, U-EG is among the top performing methods, which implies that allowing modifications to a good (in that it reflects user similarities\n(a) Last.fm (b) DeliciousFigure 3: Regret Minimization while learning the graph\nreasonably well) initial graph to model the obtained data might be a good method to overcome prior misspecification. From a scalability point of view, for Delicious the running time for L-EG is 0.1083\nseconds/iteration (averaged across T ) as compared to 0.04 seconds/iteration for G-EG. This shows that even in the absence of an explicit user-user graph, it is possible to achieve a low regret in an efficient manner."
    }, {
      "heading" : "B Regret bound for Epoch-Greedy",
      "text" : "Theorem 1. Under the additional assumption that ||wt||2 ≤ 1 for all rounds t, the expected regret obtained by epoch-greedy in the GOB framework is given as:\nR(T ) = Õ ( n1/3 ( Tr(L−1) λn ) 1 3 T 2 3 ) (12)\nProof. LetH be the class of hypotheses of linear functions (one for each user) coupled with Laplacian regularization. Let µ(H, q, s) represent the regret or cost of performing s exploitation steps in epoch q. Let the number of exploitation steps in epoch q be sq.\nLemma 2 (Corollary 3.1 from [27]). If sq = b 1µ(H,q,1)c and QT is the minimum Q such that Q+ ∑Q q=1 sq ≥ T , then the regret obtained by Epoch Greedy is bounded by R(T ) ≤ 2QT .\nWe now bound the quantity µ(H, q, 1). Let Err(q,H) be the generalization error for H after obtaining q unbiased samples in the exploration rounds. Clearly,\nµ(H, q, s) = s · Err(q,H). (13)\nLet `LS be the least squares loss. Let the number of unbiased samples per user be equal to p. The empirical Rademacher complexity for our hypotheses class H under `LS can be given as R̂np (`LS ◦ H). The generalization error for H can be bounded as follows:\nLemma 3 (Theorem 1 from [34]). With probability 1− δ,\nErr(q,H) ≤ R̂np (`LS ◦ H) +\n√ 9 ln(2/δ)\n2pn (14)\nAssume that the target user is chosen uniformly at random. This implies that the expected number of samples per user is at least p = b qnc. For simplicity, assume q is exactly divisible by n so that p = q n (this only affects the bound by a constant factor). Substituting p in (14), we obtain\nErr(q,H) ≤ R̂np (`LS ◦ H) +\n√ 9 ln(2/δ)\n2q . (15)\nThe Rademacher complexity can be bounded using Lemma 4 (see below) as follows:\nR̂np (`LS ◦ H) ≤ 1 √ p\n√ 48 Tr(L−1)\nλn = 1√ q\n√ 48 Tr(L−1)\nλ (16)\nSubstituting this into (15) we obtain\nErr(q,H) ≤ 1√ q\n[√ 48 Tr(L−1) λ + √ 9 ln(2/δ) 2 ] . (17)\nWe set sq = 1Err(q,H) . Denoting [√ 48 Tr(L−1) λ + √ 9 ln(2/δ) 2 ] as C, sq = √ q C .\nRecall that from Lemma 2, we need to determine QT such that\nQT + QT∑ q=1 sq ≥ T =⇒ QT∑ q=1 (1 + sq) ≥ T\nSince sq ≥ 1, this implies that ∑QT q=1 2sq ≥ T . Substituting the value of sq and observing that for all q, sq+1 ≥ sq, we obtain the following:\n2QT sQT ≥ T =⇒ 2 Q\n3/2 T C ≥ T =⇒ QT ≥\n( CT\n2\n) 2 3\nQT = [√\n12 Tr(L−1) λ\n+ √\n9 ln(2/δ) 8\n] 2 3\nT 2 3 (18)\nUsing the above equation with Lemma 2, we can bound the regret as\nR(T ) ≤ 2 [√\n12 Tr(L−1) λ\n+ √\n9 ln(2/δ) 8\n] 2 3\nT 2 3 (19)\nTo simplify this expression, we suppress the term √\n9 ln(2/δ) 8 in the Õ notation, implying that\nR(T ) = Õ ( 2 [\n12 Tr(L−1) λ\n] 1 3\nT 2 3 ) (20)\nTo present and interpret the result, we keep only the factors which are dependent on n, λ, L and T . We then obtain\nR(T ) = Õ ( n1/3 ( Tr(L−1) λn ) 1 3 T 2 3 ) (21)\nThis proves Theorem 1. We now prove Lemma 4, which was used to bound the Rademacher complexity.\nLemma 4. The empirical Rademacher complexity for H under `LS on observing p unbiased samples for each of the n users can be given as:\nR̂np (`LS ◦ H) ≤ 1 √ p\n√ 48 Tr(L−1)\nλn (22)\nProof. The Rademacher complexity for a class of linear predictors with graph regularization for a 0/1 loss function `0,1 can be bounded using Theorem 2 of [34]. Specifically,\nR̂np (`0,1 ◦ H) ≤ 2M √ p\n√ Tr((λL)−1)\nn (23)\nwhere M is the upper bound on the value of ||L 1 2 W∗||2√ n\nand W ∗ is the d× n matrix corresponding to the true user preferences.\n(24)\nWe now upper bound ||L 1 2 W∗||2√ n .\n||L 12W ∗||2 ≤ ||L 1 2 ||2||W ∗||2\n||W ∗||2 ≤ ||W ∗||F = √√√√ n∑ i=1 ||w∗i ||22 ||W ∗||2 ≤ √ n (Using assumption 1: For all i, ||w∗i ||2 ≤ 1) ||L 12 || ≤ νmax(L 1 2 ) = √ νmax(L) ≤ √ 3\n(The maximum eigenvalue of any normalized Laplacian LG is 2 [10] and recall that L = LG + In)\n=⇒ ||L 1 2W ∗||2√ n ≤ √ 3 =⇒ M = √ 3 (25)\nSince we perform regression using a least squares loss function instead of classification, the Rademacher complexity in our case can be bounded using Theorem 12 from [5]. Specifically, if ρ is the Lipschitz constant of the least squares problem,\nR̂np (`LS ◦ H) ≤ 2ρ · Rnp (`0,1 ◦ H) (26)\nSince the estimates wi,t are bounded from above by 1 (additional assumption in the theorem), ρ = 1. From Equations 24, 26 and the bound on M , we obtain that\nR̂np (`LS ◦ H) ≤ 4 √ p\n√ 3 Tr(L−1)\nλn (27)\nwhich proves the lemma. Theorem 2. Under the following additional technical assumptions: (a) log(K) < (dn− 1) ln(2) (b) λ < dn (c) log (\n3+T/λdn δ\n) ≤ log(KT ) log(T/δ), with probability 1− δ, the regret obtained by Thompson Sampling in the GOB\nframework is given as:\nR(T ) = Õ ( dn√ λ √ T √ log ( Tr(L−1) n ) + log ( 3 + T λdnσ2 )) (28)\nProof. We can interpret graph-based TS as being equivalent to solving a single dn-dimensional contextual bandit problem, but with a modified prior covariance ((L⊗ Id)−1 instead of Idn). Our argument closely follows the proof structure in [2], but is modified to include the prior covariance. For ease of exposition, assume that the target user at each round is implicit. We use j to index the available items. Let the index of the optimal item at round t be j∗t , whereas the index of the item chosen by our algorithm is denoted jt.\nLet r̂t(j) be the estimated rating of item j at round t. Then, for all j,\nr̂t(j) ∼ N (〈wt,φj〉, st(j)) (29)\nHere, st(j) is the standard deviation in the estimated rating for item j at round t. Recall that Σt−1 is the covariance matrix at round t. st(j) is given as:\nst(j) = √ φTj Σ −1 t−1φj (30)\nWe drop the argument in st(jt) to denote the standard deviation and estimated rating for the selected item jt i.e. st = st(jt) and r̂t = r̂t(jt).\nLet ∆t measure the immediate regret at round t incurred by selecting item jt instead of the optimal item j∗t . The immediate regret is given by:\n∆t = 〈w∗,φj∗t 〉 − 〈w ∗,φjt〉 (31)\nDefine Eµ(t) as the event such that for all j,\nEµ(t) : |〈wt,φj〉 − 〈w ∗,φj〉| ≤ ltst(j) (32)\nHere lt = √ dn log ( 3+t/λdn\nδ\n) + √\n3λ. If the event Eµ(t) holds, it implies that the expected rating at round t is close to the true rating with high probability.\nRecall that |Ct| = K and that w̃t is a sample drawn from the posterior distribution at round t. Define ρt = √ 9dn log ( t δ ) and gt = min{ √ 4dn ln(t), √ 4 log(tK)}ρt + lt. Define Eθ(t) as the event such that for all j,\nEθ(t) : |〈w̃t,φj〉 − 〈wt,φj〉| ≤ min{ √ 4dn ln(t), √ 4 log(tK)}ρtst(j) (33)\nIf the event Eθ(t) holds, it implies that the estimated rating using the sample w̃t is close to the expected rating at round t.\n(34)\nIn lemma 7, we prove that the event Eµ(t) holds with high probability. Formally, for δ ∈ (0, 1),\nPr(Eµ(t)) ≥ 1− δ (35)\nTo show that the event Eθ(t) holds with high probability, we use the following lemma from [2].\nLemma 5 (Lemma 2 of [2]).\nPr(Eθ(t))|Ft−1) ≥ 1− 1 t2\n(36)\nNext, we use the following lemma to bound the immediate regret at round t.\nLemma 6 (Lemma 4 in [2]). Let γ = 14e√π . If the events E µ(t) and Eθ(t) are true, then for any filtration Ft−1, the following inequality holds:\nE[∆t|Ft−1] ≤ 3gt γ E[st|Ft−1] + 2gt γt2\n(37)\nDefine I(E) to be the indicator function for an event E . Let regret(t) = ∆t · I(Eµ(t)). We use Lemma 8 (proof is given later) which states that with probability at least 1− δ2 ,\nT∑ t=1 regret(t) ≤ T∑ t=1 3gt γ st + T∑ t=1 2gt γt2 + √√√√2 T∑ t=1 36g2t γ2 ln(2/δ) (38)\nFrom Lemma 7, we know that event Eµ(t) holds for all t with probability at least 1− δ2 . This implies that, with probability 1− δ2 , for all t\nregret(t) = ∆t (39)\nFrom Equations 38 and 39, we have that with probability 1− δ,\nR(T ) = T∑ t=1 ∆t ≤ T∑ t=1 3gt γ st + T∑ t=1 2gt γt2 + √√√√2 T∑ t=1 36g2t γ2 ln(2/δ)\nNote that gt increases with t i.e. for all t, gt ≤ gT\nR(T ) ≤ 3gT γ T∑ t=1 st + 2gT γ T∑ t=1 1 t2 + 6gT γ √ 2T ln(2/δ) (40)\nUsing Lemma 9 (proof given later), we have the following bound on ∑T t=1 st, the variance of the selected items:\nT∑ t=1 st ≤ √ dnT\n√ C log ( Tr(L−1)\nn\n) + log ( 3 + T\nλdnσ2\n) (41)\nwhere C = 1 λ log(1+ 1 λσ2 ) .\n(42)\nSubstituting this into Equation 40, we get\nR(T ) ≤ 3gT γ √ dnT\n√ C log ( Tr(L−1)\nn\n) + log ( 3 + T\nλdnσ2\n) + 2gT\nγ T∑ t=1 1 t2 + 6gT γ √ 2T ln(2/δ)\nUsing the fact that ∑T t=1 1 t2 < π2 6\nR(T ) ≤ 3gT γ √ dnT\n√ C log ( Tr(L−1)\nn\n) + log ( 3 + T\nλdnσ2\n) + π\n2gT 3γ + 6gT γ\n√ 2T ln(2/δ) (43)\nWe now upper bound gT . By our assumption on K, log(K) < (dn − 1) ln(2). Hence for all t ≥ 2, min{ √ 4dn ln(t), √ 4 log(tK)} = √ 4 log(tK). Hence,\ngT = 6 √ dn log(KT ) log(T/δ) + lT\n= 6 √ dn log(KT ) log(T/δ) + √ dn log ( 3 + T/λdn\nδ\n) + √ 3λ\nBy our assumption on λ, λ < dn. Hence,\ngT ≤ 8 √ dn log(KT ) log(T/δ) + √ dn log ( 3 + T/λdn\nδ\n)\nUsing our assumption that log (\n3+T/λdn δ\n) ≤ log(KT ) log(T/δ),\ngT ≤ 9 √ dn log(KT ) log(T/δ)\n(44)\nSubstituting the value of gT into Equation 43, we obtain the following:\nR(T ) ≤ 27dn γ √ T\n√ C log ( Tr(L−1)\nn\n) + log ( 3 + T\nλdnσ2 ) + 3π2 √ dn ln(T/δ) ln(KT )\nγ +\n54 √ dn ln(T/δ) ln(KT ) √ 2T ln(2/δ)\nγ\nFor ease of exposition, we keep the just leading terms on d, n and T . This gives the following bound on R(T ).\nR(T ) = Õ (\n27dn γ √ T\n√ C log ( Tr(L−1)\nn\n) + log ( 3 + T\nλdnσ2 )) Rewriting the bound to keep only the terms dependent on d, n, λ, T and L. We thus obtain the following equation.\nR(T ) = Õ ( dn√ λ √ T √ log ( Tr(L−1) n ) + log ( 3 + T λdnσ2 )) (45)\nThis proves the theorem.\nWe now prove the the auxiliary lemmas used in the above proof.\nIn the following lemma, we prove that Eµ(t) holds with high probability, i.e., the expected rating at round t is close to the true rating with high probability.\nLemma 7.\nThe following statement is true for all δ ∈ (0, 1):\nPr(Eµ(t)) ≥ 1− δ (46)\nProof.\nRecall that rt = 〈w∗, φjt〉+ ηt (Assumption 2) and that Σtwt = bt σ2 . Define St−1 = ∑t−1 l=1 ηlφjl .\nSt−1 = t−1∑ l=1 (rl − 〈w∗, φjl〉) φjl = t−1∑ l=1 ( rlφjl − φjlφ T jl w∗ )\nSt−1 = bt−1 − t−1∑ l=1 ( φjlφ T jl ) w∗ = bt−1 − σ2(Σt−1 − Σ0)w∗ = σ2(Σt−1wt − Σt−1w∗ + Σ0w∗)\nŵt −w∗ = Σ−1t−1 (\nSt−1 σ2 − Σ0w∗\n)\nThe following holds for all j:\n|〈wt,φj〉 − 〈w ∗,φj〉| = |〈φj ,wt −w ∗〉| ≤ ∣∣∣∣φTj Σ−1t−1(St−1σ2 − Σ0w∗ ) ∣∣∣∣ ≤ ||φj ||Σ−1\nt−1 (∣∣∣∣∣∣∣∣St−1σ2 − Σ0w∗ ∣∣∣∣∣∣∣∣\nΣ−1 t−1\n) (Since Σ−1t−1 is positive definite)\nBy triangle inequality,\n|〈wt,φj〉 − 〈w ∗,φj〉| ≤ ||φj ||Σ−1\nt−1 (∣∣∣∣∣∣∣∣St−1σ2 ∣∣∣∣∣∣∣∣\nΣ−1 t−1\n+ ||Σ0w∗||Σ−1 t−1\n) (47)\nWe now bound the term ||Σ0w∗||Σ−1 t−1\n||Σ0w∗||Σ−1 t−1 ≤ ||Σ0w∗||Σ−10 =\n√ w∗TΣT0 Σ −1 0 Σ0w∗ (Since φjtφ T jt is positive definite for all t)\n= √ w∗TΣ0w∗ (Since Σ0 is symmetric)\n≤ √ νmax(Σ0)||w∗||2\n≤ √ νmax(λL⊗ Id) (||w∗||2 ≤ 1)\n= √ νmax(λL) (νmax(A⊗B) = νmax(A) · νmax(B))\n≤ √ λ · νmax(L)\n||Σ0w∗||Σ−1 t−1 ≤ √\n3λ (The maximum eigenvalue of any normalized Laplacian is 2 [10] and recall that L = LG + In)\nFor bounding ||φj ||Σ−1 t−1 , note that\n||φj ||Σ−1 t−1\n= √\nφTj Σ −1 t−1φj = st(j)\nUsing the above relations, Equation 47 can thus be rewritten as:\n|〈wt,φj〉 − 〈w ∗,φj〉| ≤ st(j) ( 1 σ ||St−1||Σ−1 t−1 + √ 3λ )\n(48)\nTo bound ||St−1||Σ−1 t−1 , we use Theorem 1 from [1] which we restate in our context. Note that using this theorem with the prior covariance equal to Idn gives Lemma 8 of [2].\nTheorem 2 (Theorem 1 of [1]). For any δ > 0, t ≥ 1, with probability at least 1− δ,\n||St−1||2Σ−1 t−1 ≤ 2σ2 log\n( det(Σt)1/2 det(Σ0)−1/2\nδ ) ||St−1||2Σ−1 t−1 ≤ 2σ2 ( log ( det(Σt)1/2 ) + log ( det(Σ−10 )1/2 ) − log(δ) )\nRewriting the above equation,\n||St−1||2Σ−1 t−1 ≤ σ2\n( log (det(Σt)) + log ( det(Σ−10 ) ) − 2 log(δ) )\nWe now use the trace-determinant inequality. For any n× n matrix A, det(A) ≤ ( Tr(A) n )n which implies that\nlog(det(A)) ≤ n log ( Tr(A) n ) . Using this for both Σt and Σ−10 , we obtain:\n||St−1||Σ−1 t−1 ≤ dnσ2\n( log ((\nTr(Σt) dn\n)) + log (( Tr(Σ−10 ) dn )) − 2 dn log(δ) )\n(49)\nNext, we use the fact that\nΣt = Σ0 + t∑ l=1 φjlφ T jl =⇒ Tr(Σt) ≤ Tr(Σ0) + t (Since ||φjl ||2 ≤ 1)\nNote that Tr(A⊗B) = Tr(A) · Tr(B). Since Σ0 = λL⊗ Id, it implies that Tr(Σ0) = λd · Tr(L). Also note that Tr(Σ−10 ) = Tr((λL)−1 ⊗ Id) = dλ Tr(L −1). Using these relations in Equation 49,\n||St−1||2Σ−1 t−1 ≤ dnσ2\n( log ( λdTr(L) + t\ndn\n) + log ( Tr(L−1) λn ) − 2 dn log(δ) )\n≤ dnσ2 ( log (\nTr(L) Tr(L−1) n2 + tTr(L −1) λdn2\n) − log(δ 2dn ) ) (log(a) + log(b) = log(ab))\n= dnσ2 log (\nTr(L) Tr(L−1) n2δ + tTr(L −1) λdn2δ\n) (Redefining δ as δ 2dn )\nIf L = In, Tr(L) = Tr(L−1) = n, we recover the bound in [2] i.e.\n||St−1||2Σ−1 t−1 ≤ dnσ2 log\n( 1 + t/λdn\nδ\n) (50)\nThe upper bound for Tr(L) is 3n, whereas the upper bound on Tr(L−1) is n. We thus obtain the following relation.\n||St−1||2Σ−1 t−1 ≤ dnσ2 log ( 3 δ + t λdnδ ) ||St−1||Σ−1\nt−1 ≤ σ\n√ dn log ( 3 + t/λdn\nδ\n) (51)\nCombining Equations 48 and 51, we have with probability 1− δ,\n|〈wt,φj〉 − 〈w ∗,φj〉| ≤ st(k)\n(√ dn log ( 3 + t/λdn\nδ\n) + √ 3λ )\n|〈wt,φj〉 − 〈w ∗,φj〉| ≤ st(k)lt\nwhere lt = √ dn log ( 3+t/λdn\nδ\n) + √ 3λ. This completes the proof.\n(52)\nLemma 8. With probability 1− δ,\nT∑ t=1 regret(t) ≤ T∑ t=1 3gt γ st + T∑ t=1 2gt γt2 + √√√√2 T∑ t=1 36g2t γ2 ln 2 δ\n(53)\nProof.\nLet Zl and Yt be defined as follows:\nZl = regret(l)− 3gl γ sl − 2gl γl2\nYt = t∑ l=1 Zl (54) E[Yt − Yt−1|Ft−1] = E[Xt] = E[regret(t)|Ft−1]− 3gt γ st − 2gt γt2 E[regret(t)|Ft−1] ≤ E[∆t|Ft−1] ≤ 3gt γ st − 2gt γt2 (Definition of regret(t) and using lemma 6)\nE[Yt − Yt−1|Ft−1] ≤ 0\nHence, Yt is a super-martingale process. We now state and use the Azuma-Hoeffding inequality for Yt\n(55)\nInequality 1 (Azuma-Hoeffding). If a super-martingale Yt (with t ≥ 0) and its the corresponding filtration Ft−1, satisfies |Yt − Yt−1| ≤ ct for some constant ct, for all t = 1, . . . T , then for any a ≥ 0,\nPr(YT − Y0 ≥ a) ≤ exp ( −a2\n2 ∑T t=1 c 2 t\n) (56)\nWe define Y0 = 0. Note that |Yt − Yt−1| = |Zl| is bounded by 1 + 3glγ − 2gl γl2 . Hence, ct = 6gt γ . Setting a = √ 2 ln(2/δ) ∑T t=1 c 2 t in the above inequality, we obtain that with probability 1− δ2 ,\nYT ≤ √√√√2 T∑ t=1 36g2t γ2 ln(2/δ) T∑ t=1 ( regret(t)− 3gt γ st − 2gt γt2 ) ≤ √√√√2 T∑ t=1 36g2t γ2 ln(2/δ) (57) T∑ t=1 regret(t) ≤ T∑ t=1 3gt γ st + T∑ t=1 2gt γt2 + √√√√2 T∑ t=1 36g2t γ2 ln(2/δ) (58)\nLemma 9.\nT∑ t=1 st ≤ √ dnT\n√ C log ( Tr(L−1)\nn\n) + log ( 3 + T\nλdnσ2\n) (59)\nProof.\nFollowing the proof in [11, 43],\ndet [Σt] ≥ det [ Σt−1 +\n1 σ2 φjtφ T jt ] = det [ Σ 1 2 t−1 ( I + 1\nσ2 Σ− 1 2 t−1φjtφ T jt Σ− 1 2 t−1\n) Σ 1 2 t−1 ] = det [Σt−1] det [ I + 1\nσ2 Σ− 1 2 t−1φjtφ T jt Σ− 1 2 t−1 ] det [Σt] = det [Σt−1] ( 1 + 1\nσ2 φTjt Σ−1t−1φjt\n) = det [Σt−1] ( 1 + s 2 t\nσ2 ) log (det [Σt]) ≥ log (det [Σt−1]) + log ( 1 + s 2 t\nσ2 ) log (det [ΣT ]) ≥ log (det [Σ0]) +\nT∑ t=1 log ( 1 + s 2 t σ2 ) (60)\nIf A is an n× n matrix, and B is an d× d matrix, then det[A⊗B] = det[A]d det[B]n. Hence,\ndet[Σ0] = det[λL⊗ Id] = det[λL]d\ndet[Σ0] = [λn det(L)]d = λdn[det(L)]d\nlog (det[Σ0]) = dn log (λ) + d log (det[L]) (61)\nFrom Equations 60 and 61,\nlog (det [ΣT ]) ≥ (dn log (λ) + d log (det[L])) + T∑ t=1 log ( 1 + s 2 t σ2 ) (62)\nWe now bound the trace of Tr(ΣT+1).\nTr(Σt+1) = Tr(Σt) + 1 σ2 φjtφ T jt =⇒ Tr(Σt+1) ≤ Tr(Σt) + 1 σ2\n(Since ||φjt || ≤ 1)\nTr(ΣT ) ≤ Tr(Σ0) + T σ2\nSince Tr(A⊗B) = Tr(A) · Tr(B)\nTr(ΣT ) ≤ Tr (λ(L⊗ Id)) + T\nσ2 =⇒ Tr(ΣT ) ≤ λdTr(L) +\nT σ2 (63)\nUsing the determinant-trace inequality, we have the following relation:( 1 dn Tr(ΣT ) )dn ≥ (det[ΣT ])\ndn log (\n1 dn\nTr(ΣT ) ) ≥ log (det[ΣT ]) (64)\nUsing Equations 62, 63 and 64, we obtain the following relation.\ndn log ( λdTr(L) + Tσ2\ndn\n) ≥ (dn log (λ) + d log (det[L])) +\nT∑ t=1 log ( 1 + s 2 t σ2 )\nT∑ t=1 log ( 1 + s 2 t σ2 ) ≤ dn log ( λdTr(L) + Tσ2 dn ) − dn log (λ)− d log (det[L])\n≤ dn log ( λdTr(L) + Tσ2\ndn\n) − dn log (λ) + d log ( det[L−1] ) (det[L−1] = 1/det[L])\n≤ dn log ( λdTr(L) + Tσ2\ndn\n) − dn log (λ) + dn log ( 1 n Tr(L−1) )\n(Using the determinant-trace inequality for log(det[L−1])) ≤ dn log ( λdTr(L) Tr(L−1) + Tr(L −1)T σ2\nλdn2\n) (log(a) + log(b) = log(ab))\n≤ dn log (\nTr(L) Tr(L−1) n2 + Tr(L −1)T λdn2σ2 ) The maximum eigenvalue of any Laplacian is 2. Hence Tr(L) is upper-bounded by 3n.\nT∑ t=1 log ( 1 + s 2 t σ2 ) ≤ dn log ( 3 Tr(L−1) n + Tr(L −1)T λdn2σ2 ) (65)\n(66)\ns2t = φ T j Σ −1 t φj ≤ φ T j Σ −1 0 φj (Since we are making positive definite updates at each round t)\n≤ ‖φj‖ 2νmax(Σ−10 ) = ‖φj‖ 2 1 νmin(λL⊗ Id) = ‖φj‖ 2 1 νmin(λL)\n(νmin(A⊗B) = νmin(A)νmin(B))\n≤ 1 λ · 1 νmin(L)\n(||φj ||2 ≤ 1)\ns2t ≤ 1 λ\n(Minimum eigenvalue of a normalized Laplacian LG is 0. L = LG + In)\nMoreover, for all y ∈ [0, 1/λ], we have log ( 1 + yσ2 ) ≥ λ log ( 1 + 1λσ2 ) y based on the concavity of log(·). To see this, consider the following function:\nh(y) = log ( 1 + yσ2 ) λ log ( 1 + 1λσ2\n) − y (67) Clearly, h(y) is concave. Also note that, h(0) = h(1/λ) = 0. Hence for all y ∈ [0, 1/λ], the function h(y) ≥ 0. This implies that log ( 1 + yσ2 ) ≥ λ log ( 1 + 1λσ2 ) y. We use this result by setting y = s2t .\nlog ( 1 + s 2 t\nσ2\n) ≥ λ log ( 1 + 1\nλσ2\n) s2t\ns2t ≤ 1 λ log ( 1 + 1λσ2 ) log(1 + s2t σ2 ) (68)\nHence,\nT∑ t=1 s2t ≤ 1 λ log ( 1 + 1λσ2 ) T∑ t=1 log ( 1 + s 2 t σ2 ) (69)\nBy Cauchy Schwartz,\nT∑ t=1 st ≤ √ T √√√√ T∑ t=1 s2t (70)\nFrom Equations 69 and 70,\nT∑ t=1 st ≤ √ T √√√√ 1 λ log ( 1 + 1λσ2 ) T∑ t=1 log ( 1 + s 2 t σ2 ) T∑ t=1 st ≤ √ T √√√√C T∑ t=1 log ( 1 + s 2 t σ2 ) (71)\nwhere C = 1 λ log(1+ 1 λσ2 ) . Using Equations 65 and 71,\nT∑ t=1 st ≤ √ dnT\n√ C log ( 3 Tr(L−1)\nn + Tr(L −1)T λdn2σ2 ) T∑ t=1 st ≤ √ dnT √ C log ( Tr(L−1) n ) + log ( 3 + T λdnσ2 ) (72)"
    } ],
    "references" : [ {
      "title" : "Improved algorithms for linear stochastic bandits",
      "author" : [ "Yasin Abbasi-Yadkori", "Dávid Pál", "Csaba Szepesvári" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Thompson sampling for contextual bandits with linear payoffs",
      "author" : [ "Shipra Agrawal", "Navin Goyal" ],
      "venue" : "arXiv preprint arXiv:1209.3352,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Nonstochastic multi-armed bandits with graph-structured feedback",
      "author" : [ "Noga Alon", "Nicolo Cesa-Bianchi", "Claudio Gentile", "Shie Mannor", "Yishay Mansour", "Ohad Shamir" ],
      "venue" : "arXiv preprint arXiv:1409.8428,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2002
    }, {
      "title" : "Rademacher and gaussian complexities: Risk bounds and structural results",
      "author" : [ "Peter L Bartlett", "Shahar Mendelson" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "Leveraging side observations in stochastic bandits",
      "author" : [ "Stéphane Caron", "Branislav Kveton", "Marc Lelarge", "Smriti Bhagat" ],
      "venue" : "In Proceedings of the Twenty- Eighth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "A gang of bandits",
      "author" : [ "Nicolo Cesa-Bianchi", "Claudio Gentile", "Giovanni Zappella" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "An empirical evaluation of thompson sampling",
      "author" : [ "Olivier Chapelle", "Lihong Li" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Contextual bandits with linear payoff functions",
      "author" : [ "Wei Chu", "Lihong Li", "Lev Reyzin", "Robert E Schapire" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Stochastic linear optimization under bandit feedback",
      "author" : [ "Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade" ],
      "venue" : "In 21st Annual Conference on Learning Theory - COLT",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Algorithm 849: A concise sparse cholesky factorization package",
      "author" : [ "Timothy A Davis" ],
      "venue" : "ACM Transactions on Mathematical Software (TOMS),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    }, {
      "title" : "Socially enabled preference learning from implicit feedback data",
      "author" : [ "Julien Delporte", "Alexandros Karatzoglou", "Tomasz Matuszczyk", "Stéphane Canu" ],
      "venue" : "In Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Regularized multi–task learning",
      "author" : [ "Theodoros Evgeniou", "Massimiliano Pontil" ],
      "venue" : "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2004
    }, {
      "title" : "Algebraic connectivity of graphs",
      "author" : [ "Miroslav Fiedler" ],
      "venue" : "Czechoslovak mathematical journal,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1973
    }, {
      "title" : "Sparse inverse covariance estimation with the graphical lasso",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Robert Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    }, {
      "title" : "Online clustering of bandits",
      "author" : [ "Claudio Gentile", "Shuai Li", "Giovanni Zappella" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Multi-task sparse structure learning",
      "author" : [ "Andre R Goncalves", "Puja Das", "Soumyadeep Chatterjee", "Vidyashankar Sivakumar", "Fernando J Von Zuben", "Arindam Banerjee" ],
      "venue" : "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Multi-label structure learning with ising model selection",
      "author" : [ "André R Gonçalves", "Fernando J Von Zuben", "Arindam Banerjee" ],
      "venue" : "In Proceedings of the 24th International Conference on Artificial Intelligence,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Methods of conjugate gradients for solving linear systems, volume",
      "author" : [ "Magnus Rudolph Hestenes", "Eduard Stiefel" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1952
    }, {
      "title" : "Sparse inverse covariance matrix estimation using quadratic approximation",
      "author" : [ "Cho-Jui Hsieh", "Inderjit S Dhillon", "Pradeep K Ravikumar", "Mátyás A Sustik" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Big & quic: Sparse inverse covariance estimation for a million variables",
      "author" : [ "Cho-Jui Hsieh", "Mátyás A Sustik", "Inderjit S Dhillon", "Pradeep K Ravikumar", "Russell Poldrack" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Spectral thompson sampling",
      "author" : [ "Tomáš Kocák", "Michal Valko", "Rémi Munos", "Shipra Agrawal" ],
      "venue" : "Horde of Bandits using Gaussian Markov Random Fields Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Distributed clustering of linear bandits in peer to peer networks",
      "author" : [ "Nathan Korda", "Balázs Szörényi", "Shuai Li" ],
      "venue" : "In Proceedings of the 33nd International Conference on Machine Learning,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "Approximate gaussian elimination for laplacians-fast, sparse, and simple",
      "author" : [ "Rasmus Kyng", "Sushant Sachdeva" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "Tze Leung Lai", "Herbert Robbins" ],
      "venue" : "Advances in applied mathematics,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1985
    }, {
      "title" : "The epoch-greedy algorithm for multi-armed bandits with side information",
      "author" : [ "John Langford", "Tong Zhang" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2008
    }, {
      "title" : "Kronecker graphs: An approach to modeling networks",
      "author" : [ "Jure Leskovec", "Deepayan Chakrabarti", "Jon Kleinberg", "Christos Faloutsos", "Zoubin Ghahramani" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2010
    }, {
      "title" : "A contextual-bandit approach to personalized news article recommendation",
      "author" : [ "Lihong Li", "Wei Chu", "John Langford", "Robert E Schapire" ],
      "venue" : "In Proceedings of the 19th international conference on World wide web,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2010
    }, {
      "title" : "Collaborative filtering bandits",
      "author" : [ "Shuai Li", "Alexandros Karatzoglou", "Claudio Gentile" ],
      "venue" : "In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "Recommender systems with social regularization",
      "author" : [ "Hao Ma", "Dengyong Zhou", "Chao Liu", "Michael R Lyu", "Irwin King" ],
      "venue" : "In Proceedings of the fourth ACM international conference on Web search and data mining,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2011
    }, {
      "title" : "Latent bandits",
      "author" : [ "Odalric-Ambrym Maillard", "Shie Mannor" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2014
    }, {
      "title" : "From bandits to experts: On the value of side-observations",
      "author" : [ "Shie Mannor", "Ohad Shamir" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2011
    }, {
      "title" : "The rademacher complexity of linear transformation classes. In Learning Theory, pages 65–78",
      "author" : [ "Andreas Maurer" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2006
    }, {
      "title" : "Birds of a feather: Homophily in social networks",
      "author" : [ "Miller McPherson", "Lynn Smith-Lovin", "James M Cook" ],
      "venue" : "Annual review of sociology,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2001
    }, {
      "title" : "Dynamic clustering of contextual multi-armed bandits",
      "author" : [ "Trong T Nguyen", "Hady W Lauw" ],
      "venue" : "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2014
    }, {
      "title" : "Gaussian sampling by local perturbations",
      "author" : [ "George Papandreou", "Alan L Yuille" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2010
    }, {
      "title" : "Collaborative filtering with graph information: Consistency and scalable methods",
      "author" : [ "Nikhil Rao", "Hsiang-Fu Yu", "Pradeep K Ravikumar", "Inderjit S Dhillon" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2015
    }, {
      "title" : "Gaussian Markov random fields: theory and applications",
      "author" : [ "Havard Rue", "Leonhard Held" ],
      "venue" : "CRC Press,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2005
    }, {
      "title" : "Online learning of multiple tasks and their relationships",
      "author" : [ "Avishek Saha", "Piyush Rai", "Suresh Venkatasubramanian", "Hal Daume" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2011
    }, {
      "title" : "A survey of collaborative filtering techniques",
      "author" : [ "Xiaoyuan Su", "Taghi M Khoshgoftaar" ],
      "venue" : "Advances in artificial intelligence,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2009
    }, {
      "title" : "Spectral bandits for smooth graph functions",
      "author" : [ "Michal Valko", "Rémi Munos", "Branislav Kveton", "Tomáš Kocák" ],
      "venue" : "In 31th International Conference on Machine Learning,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "The gang of bandits (GOB) model [7] is a recent contextual bandits framework that shares information between a set of bandit problems, related by a known (possibly noisy) graph.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 39,
      "context" : "The unavailability of rating data implies that we can not use traditional collaborative filtering based methods [41].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 27,
      "context" : "Assuming each item can be described by its content (like tags describing a news article or video), the contextual bandits framework [29] offers a popular approach for addressing this exploration-exploitation trade-off.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "One way to use a social network of users to improve recommendations is with the recent gang of bandits (GOB) model [7].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 33,
      "context" : "In particular, the GOB model exploits the homophily effect [35] that suggests users with similar preferences are more likely to form links in a social network.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "Several recent works have tried to improve the scaling of the GOB model by clustering the users into groups [17, 36], but this limits the flexibility of the model and loses the ability to model individual users’ preferences.",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 34,
      "context" : "Several recent works have tried to improve the scaling of the GOB model by clustering the users into groups [17, 36], but this limits the flexibility of the model and loses the ability to model individual users’ preferences.",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 35,
      "context" : "In addition, we propose a Thompson sampling GOB variant that exploits the recent sampling-by-perturbation idea from the GMRF literature [37] to scale to even larger problems.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 29,
      "context" : "[31].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "Other methods based on collaborative filtering followed [38, 13], but these works assume that we already have rating data available.",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "Other methods based on collaborative filtering followed [38, 13], but these works assume that we already have rating data available.",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 24,
      "context" : "Bandits: The multi-armed bandit problem is a classic approach for trading off exploration and exploitation as we collect data [26].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "When features (context) for the “arms” are available and changing, it is referred to as the contextual bandit problem [4, 29, 9].",
      "startOffset" : 118,
      "endOffset" : 128
    }, {
      "referenceID" : 27,
      "context" : "When features (context) for the “arms” are available and changing, it is referred to as the contextual bandit problem [4, 29, 9].",
      "startOffset" : 118,
      "endOffset" : 128
    }, {
      "referenceID" : 8,
      "context" : "When features (context) for the “arms” are available and changing, it is referred to as the contextual bandit problem [4, 29, 9].",
      "startOffset" : 118,
      "endOffset" : 128
    }, {
      "referenceID" : 25,
      "context" : "Algorithms for the contextual bandits problem include epoch-greedy methods [27], those based on upper confidence bounds (UCB) [9, 1], and Thompson sampling methods [2].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "Algorithms for the contextual bandits problem include epoch-greedy methods [27], those based on upper confidence bounds (UCB) [9, 1], and Thompson sampling methods [2].",
      "startOffset" : 126,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "Algorithms for the contextual bandits problem include epoch-greedy methods [27], those based on upper confidence bounds (UCB) [9, 1], and Thompson sampling methods [2].",
      "startOffset" : 126,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "Algorithms for the contextual bandits problem include epoch-greedy methods [27], those based on upper confidence bounds (UCB) [9, 1], and Thompson sampling methods [2].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 5,
      "context" : "Several graph-based methods to model dependencies between the users have been explored in the (noncontextual) multi-armed bandit framework [6, 33, 3, 32], but the GOB model of Cesa-Bianchi et al.",
      "startOffset" : 139,
      "endOffset" : 153
    }, {
      "referenceID" : 31,
      "context" : "Several graph-based methods to model dependencies between the users have been explored in the (noncontextual) multi-armed bandit framework [6, 33, 3, 32], but the GOB model of Cesa-Bianchi et al.",
      "startOffset" : 139,
      "endOffset" : 153
    }, {
      "referenceID" : 2,
      "context" : "Several graph-based methods to model dependencies between the users have been explored in the (noncontextual) multi-armed bandit framework [6, 33, 3, 32], but the GOB model of Cesa-Bianchi et al.",
      "startOffset" : 139,
      "endOffset" : 153
    }, {
      "referenceID" : 30,
      "context" : "Several graph-based methods to model dependencies between the users have been explored in the (noncontextual) multi-armed bandit framework [6, 33, 3, 32], but the GOB model of Cesa-Bianchi et al.",
      "startOffset" : 139,
      "endOffset" : 153
    }, {
      "referenceID" : 6,
      "context" : "[7] is the first to exploit the network between users in the contextual bandit framework.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "To scale up the GOB model, several recent works propose to cluster the users and assume that users in the same cluster have the same preferences [17, 36].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 34,
      "context" : "To scale up the GOB model, several recent works propose to cluster the users and assume that users in the same cluster have the same preferences [17, 36].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 28,
      "context" : "Another interesting approach to relax the clustering assumption is to cluster both items and users [30], but this only applies if we have a fixed set of items.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 40,
      "context" : "Some works consider item-item similarities to improve recommendations [42, 23], but this again requires a fixed set of items while we are interested in RS where the set of items may constantly be changing.",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 21,
      "context" : "Some works consider item-item similarities to improve recommendations [42, 23], but this again requires a fixed set of items while we are interested in RS where the set of items may constantly be changing.",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 22,
      "context" : "There has also been work on solving a single bandit problem in a distributed fashion [24], but this differs from our approach where we are solving an individual bandit problem on each of the n nodes.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "Also without loss of generality, we assume that the ratings are in the range [0, 1].",
      "startOffset" : 77,
      "endOffset" : 83
    }, {
      "referenceID" : 27,
      "context" : "The true ratings can be given by a linear model [29], meaning that ri,j = (wi )xj + ηi,j,t for some noise term ηi,j,t.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "The noise ηi,j,t is conditionally subGaussian [2][7] with zero mean and bounded variance, meaning that E[ηi,j,t | Ct−1,Ht−1] = 0 and that there exists a σ > 0 such that for all γ ∈ R, we have E[exp(γηi,j,t) | Ht−1,Ct−1] ≤ exp( 2σ2 2 ).",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "The noise ηi,j,t is conditionally subGaussian [2][7] with zero mean and bounded variance, meaning that E[ηi,j,t | Ct−1,Ht−1] = 0 and that there exists a σ > 0 such that for all γ ∈ R, we have E[exp(γηi,j,t) | Ht−1,Ct−1] ≤ exp( 2σ2 2 ).",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : "The GOB model [7] solves a contextual bandit problem for each user, where the mean vectors in the different problems are related according to the Laplacian L1 of the graph G.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 12,
      "context" : "Note that the same objective function has also been explored for graph-regularized multi-task learning [14].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "2 Connection to GMRFs Unfortunately, the approach of Cesa-Bianchi [7] for solving (2) has a computational complexity of O(d2n2).",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "We can view the approach in [7] as explicitly constructing the dense dn × dn matrix Σ−1 t , leading to an O(d2n2) memory requirement.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 18,
      "context" : "Since Σt is positivedefinite, the linear system can be solved using conjugate gradient [20].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : "However, the LINUCB-like algorithm in [7] needs to estimate the confidence intervals √ φi,jΣ t φi,j for each available item j ∈ Ct.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 25,
      "context" : "We propose two approaches for mitigating this: first, in this section we adapt the epoch-greedy [27] algorithm to the GOB framework.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 27,
      "context" : "To achieve the optimal regret, we also propose a GOB variant of Thompson sampling [29].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 35,
      "context" : "In this section we further exploit the connection to GMRFs to scale Thompson sampling to even larger problems by using the recent sampling-by-perturbation trick [37].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 25,
      "context" : "1 Epoch-Greedy Epoch-greedy [27] is a variant of the popular -greedy algorithm that explicitly differentiates between exploration and exploitation rounds.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 25,
      "context" : "The attainable regret is thus proportional to the generalization error for the class of hypothesis functions mapping the context vector to an expected rating [27].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 32,
      "context" : "We characterize the generalization error in the GOB framework in terms of its Rademacher complexity [34], and use this to bound the expected regret leading to the result below.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 25,
      "context" : "1 from [27] to our context:",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 32,
      "context" : "We use [34] to bound the generalization error of our class of hypotheses in terms of its empirical Rademacher complexity R̂q (H).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 32,
      "context" : "Using Theorem 2 in [34] and Theorem 12 from [5], we obtain",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "Using Theorem 2 in [34] and Theorem 12 from [5], we obtain",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 32,
      "context" : "For a connected graph, we have the following upper-bound Tr(L −1) n ≤ (1−1/n) ν2 + 1 n [34].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 13,
      "context" : "The value ν2 represents the algebraic connectivity of the graph [15].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 35,
      "context" : "In order to implement Thompson sampling for large values of n, we adapt the recent sampling-by-perturbation approach [37] to our setting, and this allows us to sample from a Gaussian prior and then solve a linear system to sample from the posterior.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 10,
      "context" : "Since S tends to be sparse (using for example [12, 25]), this equation can be solved efficiently using conjugate gradient.",
      "startOffset" : 46,
      "endOffset" : 54
    }, {
      "referenceID" : 23,
      "context" : "Since S tends to be sparse (using for example [12, 25]), this equation can be solved efficiently using conjugate gradient.",
      "startOffset" : 46,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "We obtain the result below by following a similar argument to Theorem 1 in [2].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "Given that the event E (t) holds with high probability, we follow an argument similar to Lemma 4 of [2] and obtain the following bound:",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 9,
      "context" : "To bound the variance of the selected items, ∑T t=1 st(jt), we extend the analysis in [11, 43] to include the prior covariance term.",
      "startOffset" : 86,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : "If L = Idn, we match the Õ(dn √ T ) regret bound for a dn-dimensional contextual bandit problem [1].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "Note that we have a dependence on d and n similar to the original GOB paper [7] and that this method performs similarly in practice in terms of regret.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 26,
      "context" : "It is well known that such graphs capture many properties of real-world social networks [28].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : "Similar to [7], we use the set of associated tags to construct the TF-IDF vector for each item and reduce the dimension of these vectors to d = 25.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "Similar to [17], all hyper-parameters are set using an initial validation set of 5 thousand rounds.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 7,
      "context" : "To control the amount of exploration for Thompson sampling, we the use posterior reshaping trick [8] which reduces the variance of the posterior by a factor of 0.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "Baselines: We consider two variants of graph-based UCB-style algorithms: GOBLIN is the method proposed in the original GOB paper [7] while we use GOBLIN++ to refer to a variant that exploits the fast mean estimation strategy we develop in Section 3.",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 6,
      "context" : "Similar to [7], for both variants we discount the confidence bound term by a factor of α = 0.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 27,
      "context" : "We consider 3 variants of this baseline: the LINUCBIND proposed in [29], an epoch-greedy variant of this approach (EG-IND), and a Thompson sampling variant (TS-IND).",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "Finally, we compared against the state-of-the-art online clusteringbased approach proposed in [17], denoted CLUB.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 15,
      "context" : "Regret Minimization: We follow [17] in evaluating recommendation performance by plotting the ratio of cumulative regret incurred by the algorithm divided by the regret incurred by a random selection policy.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] Shipra Agrawal and Navin Goyal.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Noga Alon, Nicolo Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Peter L Bartlett and Shahar Mendelson.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Stéphane Caron, Branislav Kveton, Marc Lelarge, and Smriti Bhagat.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Nicolo Cesa-Bianchi, Claudio Gentile, and Giovanni Zappella.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Olivier Chapelle and Lihong Li.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Wei Chu, Lihong Li, Lev Reyzin, and Robert E Schapire.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[11] Varsha Dani, Thomas P.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[12] Timothy A Davis.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[13] Julien Delporte, Alexandros Karatzoglou, Tomasz Matuszczyk, and Stéphane Canu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[14] Theodoros Evgeniou and Massimiliano Pontil.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[15] Miroslav Fiedler.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[16] Jerome Friedman, Trevor Hastie, and Robert Tibshirani.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[17] Claudio Gentile, Shuai Li, and Giovanni Zappella.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[18] Andre R Goncalves, Puja Das, Soumyadeep Chatterjee, Vidyashankar Sivakumar, Fernando J Von Zuben, and Arindam Banerjee.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[19] André R Gonçalves, Fernando J Von Zuben, and Arindam Banerjee.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[20] Magnus Rudolph Hestenes and Eduard Stiefel.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[21] Cho-Jui Hsieh, Inderjit S Dhillon, Pradeep K Ravikumar, and Mátyás A Sustik.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[22] Cho-Jui Hsieh, Mátyás A Sustik, Inderjit S Dhillon, Pradeep K Ravikumar, and Russell Poldrack.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[23] Tomáš Kocák, Michal Valko, Rémi Munos, and Shipra Agrawal.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[24] Nathan Korda, Balázs Szörényi, and Shuai Li.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[25] Rasmus Kyng and Sushant Sachdeva.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[26] Tze Leung Lai and Herbert Robbins.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[27] John Langford and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[28] Jure Leskovec, Deepayan Chakrabarti, Jon Kleinberg, Christos Faloutsos, and Zoubin Ghahramani.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[29] Lihong Li, Wei Chu, John Langford, and Robert E Schapire.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[30] Shuai Li, Alexandros Karatzoglou, and Claudio Gentile.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[31] Hao Ma, Dengyong Zhou, Chao Liu, Michael R Lyu, and Irwin King.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[32] Odalric-Ambrym Maillard and Shie Mannor.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[33] Shie Mannor and Ohad Shamir.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[34] Andreas Maurer.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[35] Miller McPherson, Lynn Smith-Lovin, and James M Cook.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "[36] Trong T Nguyen and Hady W Lauw.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[37] George Papandreou and Alan L Yuille.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[38] Nikhil Rao, Hsiang-Fu Yu, Pradeep K Ravikumar, and Inderjit S Dhillon.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "[39] Havard Rue and Leonhard Held.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "[40] Avishek Saha, Piyush Rai, Suresh Venkatasubramanian, and Hal Daume.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "[41] Xiaoyuan Su and Taghi M Khoshgoftaar.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 40,
      "context" : "[42] Michal Valko, Rémi Munos, Branislav Kveton, and Tomáš Kocák.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "Our approach for learning the graph is related to methods proposed for multitask and multilabel learning in the batch setting [19, 18] and multitask learning in the online setting [40].",
      "startOffset" : 126,
      "endOffset" : 134
    }, {
      "referenceID" : 16,
      "context" : "Our approach for learning the graph is related to methods proposed for multitask and multilabel learning in the batch setting [19, 18] and multitask learning in the online setting [40].",
      "startOffset" : 126,
      "endOffset" : 134
    }, {
      "referenceID" : 38,
      "context" : "Our approach for learning the graph is related to methods proposed for multitask and multilabel learning in the batch setting [19, 18] and multitask learning in the online setting [40].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 37,
      "context" : "Since zeroes in the inverse covariance matrix correspond to conditional independences between the corresponding nodes (users) [39], we use L1 regularization on Vt for encouraging sparsity in the inferred graph.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 38,
      "context" : "Following [40], we choose ∆ to be the log-determinant Bregman divergence given by ∆(X||Y ) = Tr(XY −1)− log |XY −1| − dn.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 14,
      "context" : "This problem can be written as a graphical lasso problem [16], minX Tr(SX) + λ2||X||1 − log |X|, where the empirical covariance matrix S is equal to λWTt W t + V −1 t−1.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 19,
      "context" : "We use the highly-scalable second order methods described in [21, 22] to solve (11).",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 20,
      "context" : "We use the highly-scalable second order methods described in [21, 22] to solve (11).",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 38,
      "context" : "Similar to [40], we start off with an empty graph and start learning the graph only after the preference vectors have become stable, which happens in this case after each user has received 10 recommendations.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 25,
      "context" : "1 from [27]).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 32,
      "context" : "The generalization error for H can be bounded as follows: Lemma 3 (Theorem 1 from [34]).",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 32,
      "context" : "The Rademacher complexity for a class of linear predictors with graph regularization for a 0/1 loss function `0,1 can be bounded using Theorem 2 of [34].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 4,
      "context" : "Since we perform regression using a least squares loss function instead of classification, the Rademacher complexity in our case can be bounded using Theorem 12 from [5].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "Our argument closely follows the proof structure in [2], but is modified to include the prior covariance.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "To show that the event E(t) holds with high probability, we use the following lemma from [2].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "Lemma 5 (Lemma 2 of [2]).",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "Lemma 6 (Lemma 4 in [2]).",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "To bound ||St−1||Σ−1 t−1 , we use Theorem 1 from [1] which we restate in our context.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "Note that using this theorem with the prior covariance equal to Idn gives Lemma 8 of [2].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "Theorem 2 (Theorem 1 of [1]).",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "If L = In, Tr(L) = Tr(L−1) = n, we recover the bound in [2] i.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : "Following the proof in [11, 43],",
      "startOffset" : 23,
      "endOffset" : 31
    } ],
    "year" : 2017,
    "abstractText" : "The gang of bandits (GOB) model [7] is a recent contextual bandits framework that shares information between a set of bandit problems, related by a known (possibly noisy) graph. This model is useful in problems like recommender systems where the large number of users makes it vital to transfer information between users. Despite its effectiveness, the existing GOB model can only be applied to small problems due to its quadratic timedependence on the number of nodes. Existing solutions to combat the scalability issue require an often-unrealistic clustering assumption. By exploiting a connection to Gaussian Markov random fields (GMRFs), we show that the GOB model can be made to scale to much larger graphs without additional assumptions. In addition, we propose a Thompson sampling algorithm which uses the recent GMRF sampling-by-perturbation technique, allowing it to scale to even larger problems (leading to a “horde” of bandits). We give regret bounds and experimental results for GOB with Thompson sampling and epoch-greedy algorithms, indicating that these methods are as good as or significantly better than ignoring the graph or adopting a clustering-based approach. Finally, when an existing graph is not available, we propose a heuristic for learning it on the fly and show promising results.",
    "creator" : "LaTeX with hyperref package"
  }
}