{
  "name" : "1703.03389.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Faster Greedy MAP Inference for Determinantal Point Processes",
    "authors" : [ "Insu Han", "Prabhanjan Kambadur", "Kyoungsoo Park", "Jinwoo Shin" ],
    "emails" : [ "hawki17@kaist.ac.kr", "badur@gmail.com", "soo@kaist.ac.kr", "woos@kaist.ac.kr" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Determinantal point processes (DPPs) are elegant probabilistic models, first introduced by [24], who called them ‘fermion processes’. Since then, DPPs have been extensively studied in the fields of quantum physics and random matrices, giving rise to a beautiful theory [5]. The characteristic of a DPP is to repel, which makes DPPs useful for modeling diversity. Recently, they have been applied in many machine learning tasks such as summarization [9], human pose detection [19], clustering [15] and tweet time-line generation [35]. In particular, their computational advantage compared to other probabilistic models is that many important inference tasks are computationally tractable. For example, conditioning, sampling [15] and marginalization of DPPs admit polynomialtime/efficient algorithms, while those on popular graphical models [14] do not, i.e., they are NPhard. One exception is the MAP inference (finding the most likely configuration), which is our main interest; this is known to be NP-hard even for DPPs [19]. ∗School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Korea. Emails: hawki17@kaist.ac.kr †Business Analytics and Mathematical Sciences, Natural Language Processing, Bloomberg LP. Email: prabhanjankambadur@gmail.com ‡School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Korea. Email: kyoungsoo@kaist.ac.kr §School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Korea. Email: jinwoos@kaist.ac.kr\nar X\niv :1\n70 3.\n03 38\n9v 1\n[ cs\n.D M\n] 9\nM ar\nThe distribution of diverse sets under DPPs is characterized by determinants of a kernel matrix based on their features, and the corresponding MAP inference reduces to finding a sub-matrix that maximizes its determinant. It is well known that the matrix log-determinant is a submodular function; that is, the MAP inference of DPPs is a special instance of submodular maximization. Greedy algorithms have been shown to have the best worst-case approximation guarantees for many instances of submodular maximization; for example, (1− 1/e)-approximation for monotone functions. Furthermore, it has been often empirically observed that greedy algorithms provide near optimal solutions [17]. Hence, greedy algorithms have been also applied for the DPP task [19, 35, 36]. Known implementations of greedy selection on DPP require computation of log-determinants, matrix inversions [19] or solving linear systems [22]. Consequently, they run in Ω(d4) time where d is the total number of items (see Section 2.3). In this paper, we propose faster greedy implementations that run in O(d3) time."
    }, {
      "heading" : "1.1 Contribution",
      "text" : "Our high-level idea is to amortize greedy operations by utilizing log-determinant approximation schemes. A greedy selection requires computation of marginal gains of log-determinants; we consider their first-order (linear) approximations. We observe that the computation of multiple marginal gains can be amortized into a single run of a linear solver, in addition to multiple vector inner products. We choose the popular conjugate gradient descent (CG) [32] as a linear solver. In addition, for improving the quality of first-order approximations, we partition remaining items into p ≥ 1 sets (via some clustering algorithm), and apply the first-order approximations in each partition. The resulting approximate computation of multiple marginal gains at each greedy selection requires 2p runs of CG under the Schur complement, and the overall running time of the proposed greedy algorithm becomes O(d3) under the choice of p = O(1) (see Section 3).\nNext, we develop an even faster greedy algorithm using a batch strategy for larger-scale DPPs. In addition to using the first-order approximations of log-determinants under a partitioning scheme, we add k > 1 elements instead of a single element to the current set, where we sample some candidates among all possible k elements to relax the expensive cost of computing all marginal gains. Intuitively, the batch selection makes the algorithm k times faster, while potentially reducing the approximation quality. Now, we suggest running the recent fast log-determinant approximation scheme (LDAS) [11] p times, instead of running CG pk times under the Schur complement, where LDAS utilizes high-order, i.e., polynomial, approximations to the scalar log function with stochastic trace estimators. Since the complexities of running LDAS and CG are comparable, running the former p times is faster than running the latter pk times if k > 1. Finally, we discovered a novel scheme for boosting the approximation quality by sharing random vectors among many runs of LDAS, and also establish theoretical justification why this helps.\nIn our experiments on both synthetic and real-world datasets, the proposed algorithms are orders of magnitude faster than competitors, while losing only 0.01-approximation ratio."
    }, {
      "heading" : "1.2 Related work",
      "text" : "To the best of our knowledge, this is the first work that aims for developing faster greedy algorithms specialized for the MAP inference of DPP, while there has been several efforts on those for general submodular maximization. An accelerated greedy algorithm was first proposed by [26] which\nmaintains the upper bounds on the marginal gains instead of recomputing exact values. In each iteration, only the elements with the maximal bound compute the exact gain, which still bounds on the exact value due to submodularity. However, we found that the approximation quality of this algorithm is extremely bad for the DPP case (see Section 5). Another natural approach is on stochastic greedy selections computing marginal gains of randomly selected elements. Its worst-case approximation guarantee was also studied [27], under the standard, non-batch, greedy algorithm. The idea of stochastic selections can be also applied to our algorithms, where we indeed apply it for designing our faster batch greedy algorithm as mentioned earlier. Recently, [4] proposed a ‘one-pass’ greedy algorithm where each greedy selection requires computing only a single marginal gain, i.e., the number of marginal gains necessary to compute can be significantly reduced. However, this algorithm is attractive only for the case when evaluating a marginal gain does not increase with respect to the size of the current set, which does not hold for the DPP case. As reported in Section 5, it performs significantly worse than our algorithms in both their approximation qualities and running times. There have been also several efforts to design parallel/distributed implementations of greedy algorithms: [30] use parallel strategies for the above one-pass greedy algorithm and [20] adapt a MapReduce paradigm for implementing greedy algorithms in distributed settings. One can also parallelize our algorithms easily since they require independent runs of matrix-vector (or vector inner) products, but we do not explore this aspect in this paper. Finally, we remark that a non-greedy algorithm was studied in [8] for better MAP qualities of DPP, but it is much slower than ours as reported in Section 5."
    }, {
      "heading" : "1.3 Organization",
      "text" : "We introduce the necessary background in Section 2, and present the proposed algorithms in Section 3 and Section 4. Experimental results are reported in Section 5."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We start by defining a necessary notation. Our algorithms for determinantal point processes (DPPs) select elements from the ground set of d items Y = [d] := {1, 2, . . . , d} and denote the set of all subsets of Y by 2Y . For any positive semidefinite matrix L ∈ Rd×d, we denote λmin and λmax to be the smallest and the largest eigenvalues of L. Given subset X,Y ⊆ Y , we use LX,Y to denote the submatrix of L obtained by entries in rows and columns indexed by X and Y , respectively. For notational simplicity, we let LX,X = LX and LX,{i} = LX,i for i ∈ Y . In addition, LX is defined as the average of LX∪{i} for i ∈ Y \\ X . Finally, 〈·, ·〉 means the matrix/vector inner product or element-wise product sum.\nIn Section 2.1, we introduce the maximum a posteriori (MAP) inference of DPP, then the standard greedy optimization scheme and its naı̈ve implementations are described in Section 2.2 and Section 2.3, respectively."
    }, {
      "heading" : "2.1 Determinantal Point Processes",
      "text" : "DPPs are probabilistic models for subset selection of a finite ground set Y = [d] that captures both quality and diversity. Formally, it defines the following distribution on 2Y : for X ⊆ Y ,\nPr [X ⊆ Y] ∝ det (LX) ,\nwhere L ∈ Rd×d is a positive definite matrix called an L-ensemble kernel. Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9]. In particular, we are interested in the MAP inference, i.e., finding the most diverse subset Y of Y that achieves the highest probability, i.e., arg maxY⊆Y det(LY ), possibly under some constraints on Y . Unlike other inference tasks on DPP, it is known that MAP is a NP-hard problem [19]."
    }, {
      "heading" : "2.2 Greedy Submodular Maximization",
      "text" : "A set function f : 2Y → R is submodular if its marginal gains are decreasing, i.e.,\nf(X ∪ {i})− f(X) ≥ f(Y ∪ {i})− f(Y ),\nfor every X ⊆ Y ⊂ Y and every i ∈ Y \\ Y . We say f is monotone if f(X) ≤ f(Y ) for every X ⊆ Y . It is well known that DPP has the submodular structure, i.e., f = log det is submodular.\nThe submodular maximization task is to find a subset maximizing a submodular function f , which corresponds to the MAP inference task in the DPP case. Hence, it is NP-hard and a popular approximate scheme is the following greedy procedure [28]: initially, X ← ∅ and iteratively update X ← X ∪ {imax} for\nimax = argmax i∈Y\\X\nf(X ∪ {i})− f(X), (1)\nas long as f(X ∪ {imax}) > f(X). For the monotone case, it guarantees (1− 1/e)-approximation [28]. Under some modifications of the standard greedy procedure, 2/5-approximation can be guaranteed even for non-monotone functions [7]. Irrespectively of such theoretical guarantees, it has been empirically observed that greedy selection (1) provides near optimal solutions in practice [17, 33, 35, 36]."
    }, {
      "heading" : "2.3 Naı̈ve Implementations of Greedy Algorithm",
      "text" : "Log-determinant or related computations, which are at the heart of greedy algorithms for MAP inference of DPPs, are critical to compute the marginal gain log detLX∪{i} − log detLX . Since exact computations of log-determinants might be slow, requiringO(d3) time even for d-dimensional sparse matrices, we introduce recent efficient log-determinant approximation schemes (LDAS). The log-determinant of a symmetric positive definite matrix A can be approximated by combining (a) Chebyshev polynomial expansion of the scalar log function and (b) matrix trace estimators via Monte Carlo methods:\nlog detA = tr (logA) (a) ≈ tr (pn(A)) (b) ≈ 1 m m∑ t=1 v(t)>pn(A)v (t).\nHere, pn(x) is a polynomial expansion of degree n approximating log x and v(1), . . . ,v(m) are random vectors used for estimating the trace of pn(A). In literature, several polynomial expansions, including Taylor [3], Chebyshev [11] and Legendre [31] have been used. For trace estimation, several random vectors have been also studied [1], e.g., the Hutchinson method [13] chooses elements\nof v as i.i.d. random numbers in {−1,+1} so that E [ v>Av ] = tr (A). In this paper, we use LDAS using the Chebyshev polynomial and Hutchinson method [11], but one can also use other alternatives.\nLog-determinant Approximation Scheme (LDAS) [11]\nInput: symmetric matrix A ∈ Rd×d with eigenvalues in [δ, 1 − δ], sampling number m and polynomial degree n Initialize: Γ← 0 cj ← j-th coefficient of Chebyshev expansion of log x on [δ, 1− δ] for 0 ≤ j ≤ n. for i = 1 to m do\nDraw a random vector v(i) ∈ {−1,+1}d whose entries are uniformly distributed. w\n(i) 0 ← v(i) and w (i) 1 ← 21−2δAv (i) − 11−2δv (i)\nu← c0w(i)0 + c1w (i) 1 for j = 2 to n do w\n(i) 2 ← 41−2δAw (i) 1 − 21−2δw (i) 1 −w (i) 0\nu← u + cj w(i)2 w\n(i) 0 ← w (i) 1 and w (i) 1 ← w (i) 2\nend for Γ← Γ + v(i)>u/m\nend for Output: Γ\nObserve that LDAS only requires matrix-vector multiplications and its running time is Θ ( d2 )\nfor constants m,n = O(1). One can directly use LDAS for computing (1) and the resulting greedy algorithm runs in Θ(d · T 3GR) time if the number of greedy updates on the current set X is TGR. For example, if TGR = Ω(d), the complexity is simply Θ(d4). An alternative way to achieve the same complexity is to use the Schur complement [29]:\nlog detLX∪{i} − log detLX = log ( Li,i − Li,XL−1X LX,i ) . (2)\nThis requires a linear solver to compute L−1X LX,i; conjugate gradient descent (CG) [10] is a popular choice in practice. Hence, if one applies CG to compute the max-marginal gain (1), the resulting greedy algorithm runs in Θ(d · T 3GR · TCG) time, where TCG denotes the number of iterations of each CG run. In the worst case, CG converges to the exact solution when TCG grows with the matrix dimension, but for practical purposes, it typically provides a very accurate solution in few iterations, i.e., TCG = O(1). Recently, Gauss quadrature via Lanczos iteration is used for efficient computing of Li,XL −1 X LX,i [22]. Although it guarantees rigorous upper/lower bounds, CG is faster and accurate enough for practical purposes.\nIn summary, the greedy MAP inference of DPP can be implemented efficiently via LDAS or CG. The faster implementations proposed in this paper smartly employ both of them as key components utilizing their complementary benefits."
    }, {
      "heading" : "3 Faster Greedy DPP Inference",
      "text" : "In this section, we provide a faster greedy submodular maximization scheme for the MAP inference of DPP. We explain our key ideas in Section 3.1 and then, provide the formal algorithm description\nin Section 3.2."
    }, {
      "heading" : "3.1 Key Ideas",
      "text" : "First-order approximation of log-determinant. The main computational bottleneck of a greedy algorithm is to evaluate the marginal gain (1) for every element not in the current set. To reduce the time complexity, we consider the following first-order, i.e., linear, approximation of log-determinant as:1\nargmax i∈Y\\X\nlog detLX∪{i} − log detLX\n= argmax i∈Y\\X\nlog detLX∪{i} − log detLX\n≈ argmax i∈Y\\X\n〈 L −1 X , LX∪{i} − LX 〉 , (3)\nwhere recall that LX is the average of LX∪{i}. Observe that computing (3) requires the vector inner product of a single column (or row) of L −1 X and LX∪{i}−LX because LX∪{i} and LX share almost all entries except a single row and a column.\nTo obtain a single column of L −1 X , one can solve a linear system using the CG algorithm. More importantly, it suffices to run CG once for computing (3), while the naı̈ve greedy implementation described in Section 2.3 has to run CG |Y \\X| times. As we mentioned earlier, after obtaining the single column of L −1 X using CG, one has to perform |Y \\ X| vector inner products in (3), but it is much cheaper than |Y \\X| CG runs requiring matrix-vector multiplications.\nPartitioning. In order to further improve the quality of first-order approximation (3), we partition Y \\X into p distinct subsets so that\n‖LX∪{i} − LX‖F ‖LX∪{i} − L (j) X ‖F ,\nwhere an element i is in the partition j ∈ [p], L(j)X is the average of LX∪{i} for i in the partition j, and ‖·‖F is the Frobenius norm. Since LX∪{i} becomes closer to the average L (j)\nX , one can expect that the first-order approximation quality in (3) is improved. But, we now need a more expensive procedure to approximate the marginal gain:\nlog detLX∪{i} − log detLX = ( log detLX∪{i} − log detL (j) X ) + ( log detL (j) X − log detLX )\n≈ 〈( L (j)\nX )−1 , LX∪{i} − L (j) X 〉 ︸ ︷︷ ︸\n(a)\n+ ( log detL (j) X − log detLX )\n︸ ︷︷ ︸ (b) .\nThe first term (a) can be computed efficiently as we explained earlier, but we have to run CG p times for computing single columns of L (1)\nX , . . . , L (p)\nX . The second term (b) can be also computed using CG similarly to (2) under the Schur complement. Hence, one has to run CG 2p times in total. If p is\n1 ∇X log detX = ( X−1 )>\nlarge, the overall complexity becomes larger, but the approximation quality improves as well. We also note that one can try various clustering algorithms, e.g., k-means or Gaussian mixture. Instead, we use a simple random partitioning scheme because it is not only the fastest method but it also works well in our experiments."
    }, {
      "heading" : "3.2 Algorithm Description and Guarantee",
      "text" : "The formal description of the proposed algorithm is described in Algorithm 1.\nAlgorithm 1 Faster Greedy DPP Inference\n1: Input: kernel matrix L ∈ Rd×d and number of partitions p 2: Initialize: X ← ∅ 3: while Y \\X 6= ∅ do 4: Partition Y \\X randomly into p subsets. 5: for j = 1 to p do 6: L (j)\nX ← average of LX∪{i} for i in the partition j 7: z(j) ← (|X|+ 1)-th column of ( L (j)\nX )−1 8: δj ← log detL (j)\nX − log detLX 9: end for\n10: for i ∈ Y \\X do 11: ∆i ← 〈 LX∪{i} − L (j) X , Mat ( z(j) )〉 2 + δj where element i is included in partition j. 12: end for 13: imax ← argmaxi∈Y\\X ∆i 14: if log detLX∪{imax} − log detLX < 0 then 15: return X 16: end if 17: X ← X ∪ {imax} 18: end while\nAs we explained in Section 3.1, the lines 7, 8 require to run CG. Hence, the overall complexity becomes Θ(T 3GR · TCG · p + d · T 2GR) = Θ(T 3GR + d · T 2GR), where we choose p, TCG = O(1), TGR is the number of greedy updates on the current set X and TCG is the number of iterations of each CG run. For example, if TGR = Ω(d), it is simply Θ(d3). It is much better than the complexity Θ(d4) of the naı̈ve implementations described in Section 2.3. In particular, if kernel matrix L is sparse, i.e., number of non-zeros of each column/row is O(1), ours has the complexity Θ(T 2GR + d · TGR) while the naı̈ve approaches are still worse having the complexity Θ(d · T 2GR).\nWe also provide the following approximation guarantee of Algorithm 1 for the monotone case. Theorem 1. Suppose the smallest eigenvalue of L is greater than 1. Theh, it holds that\nlog detLX ≥ (1− 1/e) max Z⊆Y,|Z|=|X| log detLZ − 2|X|ε.\n2For Z ∈ Rd×k , Mat(Z) ∈ Rd×d is defined whose the last k columns and rows are equal to Z and Z>, respectively, and other entries set to 0.\nwhere\nε = max X⊆Y,i∈Y\\X\nj∈[p]\n∣∣∣∣∣log detLX∪{i}detL(j)X − 〈( L (j) X )−1 , LX∪{i} − L (j) X 〉∣∣∣∣∣ and X is the output of Algorithm 1.\nThe above theorem captures the relation between the first-order approximation error ε in (3) and the worst-case approximation ratio of the algorithm."
    }, {
      "heading" : "4 Faster Batch-Greedy DPP Inference",
      "text" : "In this section, we present an even faster greedy algorithm for the MAP inference task of DPP, in particular for large-scale tasks. On top of ideas described in Section 3.1, we use a batch strategy, i.e., add k elements instead of a single element to the current set, where LDAS in Section 2.3 is now used as a key component. The batch strategy accelerates our algorithm. We first provide the formal description of the batch greedy algorithm in Section 4.1. In Section 4.2, we describe additional ideas on applying LDAS as a subroutine of the proposed batch algorithm."
    }, {
      "heading" : "4.1 Algorithm Description",
      "text" : "Algorithm 2 Faster Batch-Greedy DPP Inference\n1: Input: kernel matrix L ∈ Rd×d, number of partitions p, batch size k and the number of batch samples s 2: Initialize: X ← ∅ 3: while Y \\X is not empty do 4: Ii ← Randomly draw a batch of size k for i ∈ [s]. 5: Partition [s] randomly into p subsets. 6: for j = 1 to p do 7: L (j)\nX ← average of LX∪Ii for i in the partition j 8: Z(j) ← (|X|+ 1) to (|X|+ k)-th columns of ( L (j)\nX )−1 9: δj ← log detL (j)\nX using LDAS. 10: end for 11: for i = 1 to s do 12: ∆Batchi ← 〈 LX∪Ii − L (j) X , Mat ( Z(j) )〉 2 + δj\nwhere a batch index i is included in j-th partition. 13: end for 14: imax ← argmaxi∈[s] ∆Batchi 15: if log detLX∪Iimax − log detLX < 0 then 16: return X 17: end if 18: X ← X ∪ Iimax 19: end while\nThe formal description of the proposed algorithm is described in Algorithm 2. Similar to the line 7 in Algorithm 1, the line 8 of Algorithm 2 can be solved by the CG algorithms. However, in the line 9 of Algorithm 2, we use LDAS and be reminded that it runs in Θ(d2) time. In addition, the line 12 requires the vector inner products k · s times. Thus, the total complexity becomes Θ ( T 3GR · ( TCG + mn k ) · p+ s · T 2GR ) = Θ(T 3GR) where TGR is the number of greedy updates on the current set X and we choose all parameters p, TCG, k, s,m, n = O(1). We note that Algorithm 2 is expected to perform faster than Algorithm 1 when TGR, d are large. This is primarily because the size of the current set X increases by k > 1 for each greedy iteration. A larger choice of k speeds up the algorithm up to k times, but it might hurt its output quality. We explain more details of key components of the batch algorithm below.\nBatch selection. The essence of Algorithm 2 is adding k > 1 elements, called batch, simultaneously to the current set with an improved marginal gain. Formally, it starts from the empty set and recursively updates X ← X ∪ Imax for\nImax = argmax I⊆Y\\X,|I|=k log detLX∪I . (4)\nuntil no gain is attained. The non-batch greedy procedure (1) corresponds to k = 1. Such batch greedy algorithms have been also studied for submodular maximization [28, 12] and recently, [23] studied their theoretical guarantees showing that they can be better than their non-batch counterparts under some conditions. The main drawback of the standard batch greedy algorithms is that finding the optimal batch of size k requires computing too many marginal gains of (|Y\\X| k ) subsets.\nTo address the issue, we sample s (|Y\\X|\nk\n) bunches of batch subsets randomly and compute\napproximate batch marginal gains using them. [27] first propose an uniformly random sampling to the standard non-batch greedy algorithm. The authors show that it guarantees (1 − 1/e − O(e−s)) approximation ratio in expectation and report that it performs well in many applications. In our experiments, we choose s = 500 batch samples.\nHigh-order approximation of log-determinant. Recall that for Algorithm 1, we suggest using the CG algorithm under the Schur complement for computing\nlog detL (j)\nX − log detLX . (5)\nOne can apply the same strategy for Algorithm 2, which requires running the CG algorithm k times for (5). Instead, we suggest running LDAS (using polynomial/high-order approximations of the scalar log function) only once, i.e., the line 9, which is much faster if k is large. We remind that the asymptotic complexities of both CG and LDAS are the same as Θ(d2)."
    }, {
      "heading" : "4.2 Sharing Randomness in Trace Estimators",
      "text" : "To improve the approximation quality of Algorithm 2, we further suggest running LDAS using the same random vectors v(1), . . . ,v(m) across j ∈ [p]. This is because we are interested in relative values log detL (j)\nX for j ∈ [p] instead of their absolute ones. Our intuition is that different random vectors have different bias, which hurt the comparison task. Figure 1 demonstrates an experiment on the estimation of log detL (j)\nX when random vectors are shared and independent, respectively. This implies that sharing random vectors might be worse for estimating the absolute values of logdeterminants, but better for comparing them.\nWe also formally justify the idea of sharing random vectors as stated in the follows theorem.\nTheorem 2. Suppose A,B are positive definite matrices whose eigenvalues are in [δ, 1− δ] for δ > 0. Let ΓA,ΓB be the estimations of log detA, log detB by LDAS using the same random vectors v(1), . . . ,v(m) for both. Then, it holds that\nVar [ΓA − ΓB ] ≤ 32M2ρ2 (ρ+ 1)\n2\nm (ρ− 1)6 (1− 2δ)2 ‖A−B‖2F\nwhere M = 5 log (2/δ) and ρ = 1 + 2√ 2/δ−1−1 .\nWithout sharing random vectors, the variance should grow linearly with respect to ‖A‖2F + ‖B‖ 2 F . In our case, matrices A and B correspond to some of L (j)\nX , and ‖A − B‖2F is significantly smaller than ‖A‖2F + ‖B‖ 2 F . We believe that our idea of sharing randomness might be of broader interest in many applications of LDAS or its variants, requiring multiple log-determinant computations."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "In this section, we evaluate our proposed algorithms for the MAP inference on synthetic and realworld DPP instances.\nSetups. The experiments are performed using a machine with a hexa-core Intel CPU (Core i75930K, 3.5 GHz) and 32 GB RAM. We compare our algorithms with following competitors: the standard greedy algorithm (GREEDY) [28], stochastic greedy algorithm (STOCH) [27], double greedy algorithm (DOUBLE) [4] and softmax extension (SOFTMAX) [8].3 We implement GREEDY using DPP marginalization requiring matrix inversion [19], which is a bit faster (preserving the same accuracy) than its naı̈ive implementation described in Section 2.3. We use 100 samples in STOCH, regardless of matrix dimension, where a larger number of samples makes it more accurate, but slower. Unless stated otherwise, we choose parameters of p = 10, k = 10, s = 500, TCG = 30, m = 30 and n = 20, regardless matrix dimension, for our algorithms.\nAdditional tricks for boosting accuracy. For boosting approximation qualities of our algorithms, we use the simple trick in our experiments: recompute top ` marginal gains exactly (using CG)\n3We also run the accelerated greedy algorithm [26] for general submodular maximization, but do not report its performance since its approximation quality is extremely bad for the DPP case.\nwhere they are selected based on estimated marginal gains, i.e., ∆i for Algorithm 1 and ∆Batchi for Algorithm 2. Then, our algorithms choose the best element among ` candidates, based on their exact marginal gains. Since we choose small ` = 20 in our experiments, this additional process increases the running times of our algorithms marginally, but makes them more accurate. In fact, the trick is inspired by [26] where the authors also recompute the exact marginal gain of a single element. In addition, for boosting further approximation qualities of Algorithm 2, we also run Algorithm 1 in parallel and choose the largest one among {∆i,∆Batchi } given the current set. Hence, at most iterations, the batch with the maximal ∆Batchi is chosen and increases the current set size by k (i.e., making speed-up) as like Algorithm 2, and the non-batch with the maximal ∆i is chosen at very last iterations, which fine-tunes the solution quality. We still call the synthesized algorithm by Algorithm 2 in this section.\nPerformance metrics. For the performance measure on approximation qualities of algorithms, we use the following ratio of log-probabilities:\nlog detLX/log detLXGREEDY .\nwhere X and XGREEDY are the outputs of an algorithm and GREEDY, respectively. Namely, we compare outputs of algorithms with that of GREEDY since the exact optimum is hard to compute. Similarly, we report the running time speedup of each algorithm over GREEDY."
    }, {
      "heading" : "5.1 Synthetic Dataset",
      "text" : "In this section, we use synthetic DPP datasets generated as follows. As [18, 19] proposed, a kernel matrix L for DPP can be re-parameterized as\nLi,j = qiφ > i φjqj ,\nwhere qi ∈ R+ is considered as the quality of item i and φi ∈ Rd is the normalized feature vector of item i so that φ>i φj measures the similarity between i and j. We use qi = exp (βxi) for the quality measurement xi ∈ R and choose β = 0.1. We choose each entry of φi and xi drawn from the normal distribution N (0, 1) for all i ∈ [d], and then normalize φi so that ‖φi‖2 = 1.\nWe first show how much the number of clusters p and the batch size k are sensitive for Algorithm 1 and Algorithm 2, respectively. Figure 3(a) shows the accuracy of Algorithm 1 with different numbers of clusters. It indeed confirms that a larger cluster improves its accuracy since it makes first-order approximations tighter. Figure 3(b) shows the performance trend of Algorithm 2 as the batch size k increases, which shows that a larger batch might hurt its accuracy. Based on these experiments, we choose p = k = 10 in order to target 0.01 approximation ratio loss compared to GREEDY.\nWe generate synthetic kernel matrices with varying dimension d up to 5, 000, and the performances of tested algorithms are reported in Figure 2(a). One can observe that GREEDY seems to be nearoptimal, where only SOFTMAX often provides marginally larger log-probabilities than GREEDY under small dimensions. Interestingly, we found that DOUBLE has the strong theoretical guarantee for general submodular maximization [4], but its practical performance for DPP is bad. It is faster than GREEDY, but much slower than ours. STOCH is the fastest one among our competitors, but ours outperform it with respect to running time given accuracy. In overall, our algorithms are at orders of magnitude faster than GREEDY, STOCH and SOFTMAX, while loosing 0.01-approximation ratio. For example, Algorithm 2 is 140 times faster than GREEDY for d = 5, 000, and the gap should increase for larger dimension d."
    }, {
      "heading" : "5.2 Real Dataset",
      "text" : "We use real-world datasets of the following two tasks of matched and video summarizations.\nMatched summarization. We evaluate our proposed algorithms for matched summarization that is first proposed by [8]. This task gives useful information for comparing the texts addressed at different times by the same speaker. Suppose we have two different documents and each one consists of several statements. The goal is to apply DPP for finding statement pairs that are similar to each other, while they summarize (i.e., diverse) well the two documents. We use transcripts of debates in 2016 US Republican party presidential primaries speeched by following 8 participates: Bush, Carson, Christie, Kasich, Paul, Trump, Cruz and Rubio.4\nWe follow similar pre-processing steps of [8]. First, every sentence is parsed and only nouns except the stopwords are extracted via NLTK [2]. Then, we remove the ‘rare’ words occurring less than 10% of the whole debates, and then ignore each statement which contains more ‘rare’ words than ’frequent’ ones in it. This gives us a dataset containing 3, 406 distinct ‘frequent’ words and 1, 157 statements. For each statement pair (i, j), feature vector φ(i,j) = wi + wj ∈ R3406 where wi is generated as a frequency of words in the statement i. Then, we normalize φ(i,j). The match quality x(i,j) is measured as the cosine similarity between two statements i and j, i.e., x(i,j) = w>i wj , and we remove statement pairs (i, j) such that its match quailty x(i,j) is smaller than 15% of the maximum one. Finally, by choosing q(i,j) = exp ( 0.1 · x(i,j) ) , we obtain ( 8 2 ) = 28 kernel matrices of dimension d from 516 to 4, 000.\nFigure 4 reports log-probability ratios and speedups of Algorithm 2 under the 28 kernels. We observe that Algorithm 2 looses 0.01-approximation ratio on average, compared to GREEDY, under the real-world kernels. Interestingly, SOFTMAX runs much slower than even GREEDY, while our algorithm runs significantly faster than GREEDY for large dimension, e.g., 147 times faster for d = 4, 000 corresponding to transcripts of Bush and Rubio.\nVideo summarization. We evaluate our proposed algorithms video summarization. We use 39 videos from a Youtube dataset [6], and the trained DPP kernels from [9]. Under the kernels, we found that the numbers of selected elements from algorithms are typically small (less than 10), and\n4Details of the primaries are provided in http://www.presidency.ucsb.edu/debates.php.\nhence we use Algorithm 1 instead of its batch version Algorithm 2. For performance evaluation, we use an F-score based on five sets of user summaries where it measures the quality across two summaries.\nFigure 5(a) illustrates F-score for GREEDY and Algorithm 1 and Figure 5(b) reports its speedup. Our algorithm achieves over 30 times speedup in this case, while it produces F-scores that are very similar to those of GREEDY. For some video, it achieves even better F-score, as illustrated in 5(c)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have presented fast algorithms for the MAP inference task of large-scale DPPs. Our main idea is to amortize common determinant computations via linear algebraic techniques and recent logdeterminant approximation methods. Although we primarily focus on a special matrix optimization, we expect that several ideas developed in this paper would be useful for other related matrix computational problems, in particular, involving multiple determinant computations."
    }, {
      "heading" : "A Proof of Theorem 1",
      "text" : "For given X ⊆ Y , we denote that the true marginal gain Λi and the approximated gain ∆i (used in Algorithm 1) as\nΛi := log detLX∪{i} − log detLX\n∆i :=\n〈( L (j)\nX )−1 , LX∪{i} − L (j) X 〉 + ( log detL (j) X − log detLX )\nwhere an item i ∈ Y \\ X is in the partition j. We also use iOPT = argmaxi Λi and imax = argmaxi ∆i. Then, we have\nΛimax≥∆imax − ε ≥∆iOPT − ε ≥ ΛiOPT − 2ε\nwhere the first and third inequalities are from the definition of ε, i.e., |Λi −∆i| ≤ ε, and the second inequality holds by the optimality of imax. In addition, when the smallest eigenvalue of L is greater than 1, log detLX is monotone and non-negative [33]. To complete the proof, we introduce following approximation guarantee of the greedy algorithm with a ‘noise’ during the selection [34]. Theorem. (Noisy greedy algorithm) Suppose a submodular function f defined on ground set Y is monotone and non-negative. Let X0 = ∅ and Xk = Xk−1 ∪ {imax} such that\nf(Xk−1 ∪ {imax})− f(Xk−1) ≥ max i∈Y\\Xk−1 (f(Xk−1 ∪ {i})− f(Xk−1))− εk\nfor some εk ≥ 0. Then,\nf(Xk) ≥ (1− 1/e) max X⊆Y,|X|≤k f(X)− k∑ i=1 εi\nTheorem 1 is straightforward by substituting 2ε into εk. This completes the proof of Theorem 1."
    }, {
      "heading" : "B Proof of Theorem 2",
      "text" : "As we explained in Section 2.3, Chebyshev expansion of log x in [δ, 1− δ] with degree n is defined as pn (x). This can be written as\npn (x) = n∑ k=0 ckTk ( 2 1− 2δ x− 1 1− 2δ ) (6)\nwhere the coefficient ck and the k-th Chebyshev polynomial Tk(x) are defined as\nck =  1 n+ 1 n∑ j=0 f ( 1− 2δ 2 xj + 1 2 ) T0(xj) if k = 0 2\nn+ 1 n∑ j=0 f ( 1− 2δ 2 xj + 1 2 ) Tk(xj) otherwise\n(7) Tk+1(x) = 2xTk(x)− Tk−1(x) for k ≥ 1 (8)\nwhere xj = cos ( π(j+1/2) n+1 ) for j = 0, 1, . . . , n and T0(x) = 1, T1(x) = x [25]. For simplicity, we now use H := pn (A) − pn (B) and denote Ã = 21−2δA − 1\n1−2δ I where I is identity matrix with same dimension of A and same for B̃.\nWe estimate the log-determinant difference while random vectors are shared, i.e.,\nlog detA− log detB ≈ 1 m m∑ i=1 v(i)>Hv(i).\nTo show that the variance of v(i)>Hv(i) is small as ‖A−B‖F , we provide that\nVar\n[ 1\nm m∑ i=1 v(i)>Hv(i)\n] = 1\nm Var\n[ v>Hv ] ≤ 2 m ‖H‖2F = 2 m ‖pn (A)− pn (B)‖2F\n≤ 2 m ( n∑ k=0 |ck| ∥∥∥Tk (Ã)− Tk (B̃)∥∥∥ F )2 where the first inequality holds from [1] and the second is from combining (6) with the triangle inequality. To complete the proof, we use following two lemmas. Lemma 3. Let Tk (·) be Chebyshev polynomial with k-degree and symmetric matricesB,E satisfied with ‖B‖2 ≤ 1, ‖B + E‖2 ≤ 1. Then, for k ≥ 0,\n‖Tk (B + E)− Tk (B)‖F ≤ k 2 ‖E‖F .\nLemma 4. Let ck be the k-th coefficient of Chebyshev expansion for f (x). Suppose f is analytic with |f (z)| ≤ M in the region bounded by the ellipse with foci ±1 and the length of major and minor semiaxis summing to ρ > 1. Then,\nn∑ k=0 k2 |ck| ≤ 2Mρ (ρ+ 1) (ρ− 1)3 .\nIn order to apply Lemma 4, we should consider f(x) = log ( 1−2δ\n2 x+ 1 2\n) . Then it can be easily\nobtained M = 5 log (2/δ) and ρ = 1 + 2√ 2/δ−1−1 as provided in [11].\nUsing Lemma 3 and 4, we can write\nVar\n[ 1\nm m∑ i=1 v(i)>Hv(i) ] ≤ 2 m ( n∑ k=0 |ck| ∥∥∥Tk (Ã)− Tk (B̃)∥∥∥ F )2\n≤ 2 m ( n∑ k=0 |ck| k2 ∥∥∥Ã− B̃∥∥∥ F )2\n≤ 2 m\n( 2Mρ (ρ+ 1)\n(ρ− 1)3\n)2( 2\n1− 2δ ‖A−B‖F )2 = 32M2ρ2 (ρ+ 1) 2\nm (ρ− 1)6 (1− 2δ)2 ‖A−B‖2F\nwhere the second inequality holds from Lemma 3 and the thrid is from Lemma 4. This completes the proof of Theorem 2.\nB.1 Proof of Lemma 3\nDenote Rk := Tk (B + E) − Tk (B). From the recurrence of Chebyshev polynomial (8), Rk has following\nRk+1 = 2 (B + E)Rk −Rk−1 + 2E Tk (B) (9)\nfor k ≥ 1 where R1 = E, R0 = 0 where 0 is defined as zero matrix with the same dimension of B. Solving this, we obtain that\nRk+1 = gk+1 (B + E)E + k∑ i=0 hi (B + E)E Tk+1−i (B) (10)\nfor k ≥ 1 where both gk (·) and hk (·) are polynomials with degree k and they have following recurrences\ngk+1 (x) = 2xgk (x)− gk−1 (x) , g1 (x) = 1, g0 (x) = 0, hk+1 (x) = 2xhk (x)− hk−1 (x) , h1 (x) = 2, h0 (x) = 0.\nIn addition, we can easily verify that\n2 max x∈[−1,1] |gk (x)| = max x∈[−1,1] |hk (x)| = 2k.\nPutting all together, we conclude that\n‖Rk+1‖F ≤ ‖gk+1 (B + E)E‖F + ∥∥∥∥∥ k∑ i=0 hi (B + E)E Tk+1−i (B) ∥∥∥∥∥ F\n≤ ‖gk+1 (B + E)‖2 ‖E‖F + k∑ i=0 ‖hi (B + E)‖2 ‖E‖F ‖ Tk+1−i (B)‖2\n≤ ( ‖gk+1 (B + E)‖2 + k∑ i=0 ‖hi (B + E)‖2 ) ‖E‖F\n≤ ( k + 1 +\nk∑ i=0 2i ) ‖E‖F\n= (k + 1) 2 ‖E‖F\nwhere the second inequality holds from ‖Y X‖F = ‖XY ‖F ≤ ‖X‖2 ‖Y ‖F for matrix X,Y and the third inequality uses that |Tk (x)| ≤ 1 for all k ≥ 0. This completes the proof of Lemma 3.\nB.2 Proof of Lemma 4\nFor general analytic function f , Chebyshev series of f is defined as\nf (x) = a0 2 + ∞∑ k=1 akTk (x) , ak = 2 π ∫ 1 −1 f (x)Tk (x)√ 1− x2 dx.\nand from [25] it is known that\nck − ak = ∞∑ j=1 (−1)j ( a2j(n+1)−k + a2j(n+1)+k ) and |ak| ≤ 2Mρk for 0 ≤ k ≤ n. We remind that ck is defined in (7). Using this facts, we get\nk2 |ck| ≤ k2 |ak|+ ∞∑\nj=1\n∣∣a2j(n+1)−k∣∣+ ∣∣a2j(n+1)+k∣∣ \n≤ k2 |ak|+ ∞∑ j=1 k2 ∣∣a2j(n+1)−k∣∣+ k2 ∣∣a2j(n+1)+k∣∣\n≤ k2 |ak|+ ∞∑ j=1 (2j(n+ 1)− k)2 ∣∣a2j(n+1)−k∣∣+ (2j(n+ 1) + k)2 ∣∣a2j(n+1)+k∣∣\nTherefore, we have n∑ k=0 k2 |ck| ≤ n∑ k=0 k2 |ak|+ ∞∑ k=n+1 k2 |ak|\n≤ ∞∑ k=0 k2 |ak| ≤ ∞∑ k=0 k2 2M ρk = 2Mρ (ρ+ 1) (ρ− 1)3\nThis completes the proof of Lemma 4."
    } ],
    "references" : [ {
      "title" : "Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix",
      "author" : [ "H. Avron", "S. Toledo" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Nltk: the natural language toolkit",
      "author" : [ "S. Bird" ],
      "venue" : "In Proceedings of the COLING/ACL on Interactive presentation sessions,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "A randomized algorithm for approximating the log determinant of a symmetric positive definite matrix. arXiv preprint arXiv:1503.00374",
      "author" : [ "C. Boutsidis", "P. Drineas", "P. Kambadur", "A. Zouzias" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "A tight linear time (1/2)approximation for unconstrained submodular maximization",
      "author" : [ "N. Buchbinder", "M. Feldman", "J. Seffi", "R. Schwartz" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "An introduction to the theory of point processes: volume II: general theory and structure",
      "author" : [ "D.J. Daley", "D. Vere-Jones" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2007
    }, {
      "title" : "Vsumm: A mechanism designed to produce static video summaries and a novel evaluation method",
      "author" : [ "S.E.F. De Avila", "A.P.B. Lopes", "A. da Luz", "A. de Albuquerque Araújo" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Maximizing non-monotone submodular functions",
      "author" : [ "U. Feige", "V.S. Mirrokni", "J. Vondrak" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Near-optimal map inference for determinantal point processes",
      "author" : [ "J. Gillenwater", "A. Kulesza", "B. Taskar" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Diverse sequential subset selection for supervised video summarization",
      "author" : [ "B. Gong", "Chao", "W.-L", "K. Grauman", "F. Sha" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Iterative methods for solving linear systems",
      "author" : [ "A. Greenbaum" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1997
    }, {
      "title" : "Large-scale log-determinant computation through stochastic chebyshev expansions",
      "author" : [ "I. Han", "D. Malioutov", "J. Shin" ],
      "venue" : "In ICML,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Worst case analysis of greedy type algorithms for independence systems",
      "author" : [ "D. Hausmann", "B. Korte", "T. Jenkyns" ],
      "venue" : "In Combinatorial Optimization,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1980
    }, {
      "title" : "A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines",
      "author" : [ "M.F. Hutchinson" ],
      "venue" : "Communications in Statistics-Simulation and Computation,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1990
    }, {
      "title" : "Learning in graphical models, volume 89",
      "author" : [ "M.I. Jordan" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1998
    }, {
      "title" : "Fast determinantal point process sampling with application to clustering",
      "author" : [ "B. Kang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "On sampling and greedy map inference of constrained determinantal point processes",
      "author" : [ "T. Kathuria", "A. Deshpande" ],
      "venue" : "arXiv preprint arXiv:1607.01551",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies",
      "author" : [ "A. Krause", "A. Singh", "C. Guestrin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Learning determinantal point processes",
      "author" : [ "A. Kulesza", "B. Taskar" ],
      "venue" : "Proceedings of UAI. Citeseer",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Determinantal point processes for machine learning",
      "author" : [ "A. Kulesza", "B Taskar" ],
      "venue" : "Foundations and Trends R  © in Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Fast greedy algorithms in mapreduce and streaming",
      "author" : [ "R. Kumar", "B. Moseley", "S. Vassilvitskii", "A. Vattani" ],
      "venue" : "ACM Transactions on Parallel Computing,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Efficient sampling for k-determinantal point processes",
      "author" : [ "C. Li", "S. Jegelka", "S. Sra" ],
      "venue" : "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Gaussian quadrature for matrix inverse forms with applications",
      "author" : [ "C. Li", "S. Sra", "S. Jegelka" ],
      "venue" : "In Proceedings of The 33rd International Conference on Machine Learning,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Performance bounds for the kbatch greedy strategy in optimization problems with curvature",
      "author" : [ "Y. Liu", "Z. Zhang", "E.K. Chong", "A. Pezeshki" ],
      "venue" : "In American Control Conference (ACC),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "The coincidence approach to stochastic point processes",
      "author" : [ "O. Macchi" ],
      "venue" : "Advances in Applied Probability,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1975
    }, {
      "title" : "Accelerated greedy algorithms for maximizing submodular set functions",
      "author" : [ "M. Minoux" ],
      "venue" : "In Optimization Techniques,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1978
    }, {
      "title" : "Lazier than lazy greedy",
      "author" : [ "B. Mirzasoleiman", "A. Badanidiyuru", "A. Karbasi", "J. Vondrák", "A. Krause" ],
      "venue" : "arXiv preprint arXiv:1409.7938",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2014
    }, {
      "title" : "An analysis of approximations for maximizing submodular set functionsi",
      "author" : [ "G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1978
    }, {
      "title" : "Schur complements and statistics",
      "author" : [ "D.V. Ouellette" ],
      "venue" : "Linear Algebra and its Applications,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1981
    }, {
      "title" : "Parallel double greedy submodular maximization",
      "author" : [ "X. Pan", "S. Jegelka", "J.E. Gonzalez", "J.K. Bradley", "M.I. Jordan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "Large-scale log-determinant computation via weighted l 2 polynomial approximation with prior distribution of eigenvalues",
      "author" : [ "W. Peng", "H. Wang" ],
      "venue" : "In International Conference on High Performance Computing and Applications,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "Iterative methods for sparse linear systems",
      "author" : [ "Y. Saad" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2003
    }, {
      "title" : "On greedy maximization of entropy",
      "author" : [ "D. Sharma", "A. Kapoor", "A. Deshpande" ],
      "venue" : "In ICML, pages 1330–1338",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    }, {
      "title" : "An online algorithm for maximizing submodular functions",
      "author" : [ "M. Streeter", "D. Golovin" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2009
    }, {
      "title" : "Tweet timeline generation with determinantal point processes",
      "author" : [ "Yao", "J.-g", "F. Fan", "W.X. Zhao", "X. Wan", "E. Chang", "J. Xiao" ],
      "venue" : "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Determinantal point processes (DPPs) are elegant probabilistic models, first introduced by [24], who called them ‘fermion processes’.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "Since then, DPPs have been extensively studied in the fields of quantum physics and random matrices, giving rise to a beautiful theory [5].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "Recently, they have been applied in many machine learning tasks such as summarization [9], human pose detection [19], clustering [15] and tweet time-line generation [35].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 18,
      "context" : "Recently, they have been applied in many machine learning tasks such as summarization [9], human pose detection [19], clustering [15] and tweet time-line generation [35].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : "Recently, they have been applied in many machine learning tasks such as summarization [9], human pose detection [19], clustering [15] and tweet time-line generation [35].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 33,
      "context" : "Recently, they have been applied in many machine learning tasks such as summarization [9], human pose detection [19], clustering [15] and tweet time-line generation [35].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 14,
      "context" : "For example, conditioning, sampling [15] and marginalization of DPPs admit polynomialtime/efficient algorithms, while those on popular graphical models [14] do not, i.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "For example, conditioning, sampling [15] and marginalization of DPPs admit polynomialtime/efficient algorithms, while those on popular graphical models [14] do not, i.",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 18,
      "context" : "One exception is the MAP inference (finding the most likely configuration), which is our main interest; this is known to be NP-hard even for DPPs [19].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 16,
      "context" : "Furthermore, it has been often empirically observed that greedy algorithms provide near optimal solutions [17].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 18,
      "context" : "Hence, greedy algorithms have been also applied for the DPP task [19, 35, 36].",
      "startOffset" : 65,
      "endOffset" : 77
    }, {
      "referenceID" : 33,
      "context" : "Hence, greedy algorithms have been also applied for the DPP task [19, 35, 36].",
      "startOffset" : 65,
      "endOffset" : 77
    }, {
      "referenceID" : 18,
      "context" : "Known implementations of greedy selection on DPP require computation of log-determinants, matrix inversions [19] or solving linear systems [22].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : "Known implementations of greedy selection on DPP require computation of log-determinants, matrix inversions [19] or solving linear systems [22].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 30,
      "context" : "We choose the popular conjugate gradient descent (CG) [32] as a linear solver.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 10,
      "context" : "Now, we suggest running the recent fast log-determinant approximation scheme (LDAS) [11] p times, instead of running CG pk times under the Schur complement, where LDAS utilizes high-order, i.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 24,
      "context" : "An accelerated greedy algorithm was first proposed by [26] which",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 25,
      "context" : "Its worst-case approximation guarantee was also studied [27], under the standard, non-batch, greedy algorithm.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "Recently, [4] proposed a ‘one-pass’ greedy algorithm where each greedy selection requires computing only a single marginal gain, i.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 28,
      "context" : "There have been also several efforts to design parallel/distributed implementations of greedy algorithms: [30] use parallel strategies for the above one-pass greedy algorithm and [20] adapt a MapReduce paradigm for implementing greedy algorithms in distributed settings.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 19,
      "context" : "There have been also several efforts to design parallel/distributed implementations of greedy algorithms: [30] use parallel strategies for the above one-pass greedy algorithm and [20] adapt a MapReduce paradigm for implementing greedy algorithms in distributed settings.",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 7,
      "context" : "Finally, we remark that a non-greedy algorithm was studied in [8] for better MAP qualities of DPP, but it is much slower than ours as reported in Section 5.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9].",
      "startOffset" : 118,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9].",
      "startOffset" : 118,
      "endOffset" : 128
    }, {
      "referenceID" : 33,
      "context" : "Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9].",
      "startOffset" : 118,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9].",
      "startOffset" : 139,
      "endOffset" : 151
    }, {
      "referenceID" : 14,
      "context" : "Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9].",
      "startOffset" : 139,
      "endOffset" : 151
    }, {
      "referenceID" : 20,
      "context" : "Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9].",
      "startOffset" : 139,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "Under the distribution, several probabilistic inference tasks are required for real-world applications, including MAP [9, 8, 35], sampling [16, 15, 21], marginalization and conditioning [9].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 18,
      "context" : "Unlike other inference tasks on DPP, it is known that MAP is a NP-hard problem [19].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 26,
      "context" : "Hence, it is NP-hard and a popular approximate scheme is the following greedy procedure [28]: initially, X ← ∅ and iteratively update X ← X ∪ {imax} for imax = argmax i∈Y\\X f(X ∪ {i})− f(X), (1)",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 26,
      "context" : "For the monotone case, it guarantees (1− 1/e)-approximation [28].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "Under some modifications of the standard greedy procedure, 2/5-approximation can be guaranteed even for non-monotone functions [7].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 16,
      "context" : "Irrespectively of such theoretical guarantees, it has been empirically observed that greedy selection (1) provides near optimal solutions in practice [17, 33, 35, 36].",
      "startOffset" : 150,
      "endOffset" : 166
    }, {
      "referenceID" : 31,
      "context" : "Irrespectively of such theoretical guarantees, it has been empirically observed that greedy selection (1) provides near optimal solutions in practice [17, 33, 35, 36].",
      "startOffset" : 150,
      "endOffset" : 166
    }, {
      "referenceID" : 33,
      "context" : "Irrespectively of such theoretical guarantees, it has been empirically observed that greedy selection (1) provides near optimal solutions in practice [17, 33, 35, 36].",
      "startOffset" : 150,
      "endOffset" : 166
    }, {
      "referenceID" : 2,
      "context" : "In literature, several polynomial expansions, including Taylor [3], Chebyshev [11] and Legendre [31] have been used.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 10,
      "context" : "In literature, several polynomial expansions, including Taylor [3], Chebyshev [11] and Legendre [31] have been used.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 29,
      "context" : "In literature, several polynomial expansions, including Taylor [3], Chebyshev [11] and Legendre [31] have been used.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "For trace estimation, several random vectors have been also studied [1], e.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : ", the Hutchinson method [13] chooses elements",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "In this paper, we use LDAS using the Chebyshev polynomial and Hutchinson method [11], but one can also use other alternatives.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "Log-determinant Approximation Scheme (LDAS) [11] Input: symmetric matrix A ∈ Rd×d with eigenvalues in [δ, 1 − δ], sampling number m and polynomial degree n Initialize: Γ← 0 cj ← j-th coefficient of Chebyshev expansion of log x on [δ, 1− δ] for 0 ≤ j ≤ n.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 27,
      "context" : "An alternative way to achieve the same complexity is to use the Schur complement [29]: log detLX∪{i} − log detLX = log ( Li,i − Li,XL X LX,i ) .",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "(2) This requires a linear solver to compute L−1 X LX,i; conjugate gradient descent (CG) [10] is a popular choice in practice.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 21,
      "context" : "Recently, Gauss quadrature via Lanczos iteration is used for efficient computing of Li,XL −1 X LX,i [22].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 26,
      "context" : "Such batch greedy algorithms have been also studied for submodular maximization [28, 12] and recently, [23] studied their theoretical guarantees showing that they can be better than their non-batch counterparts under some conditions.",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "Such batch greedy algorithms have been also studied for submodular maximization [28, 12] and recently, [23] studied their theoretical guarantees showing that they can be better than their non-batch counterparts under some conditions.",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 22,
      "context" : "Such batch greedy algorithms have been also studied for submodular maximization [28, 12] and recently, [23] studied their theoretical guarantees showing that they can be better than their non-batch counterparts under some conditions.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 25,
      "context" : "[27] first propose an uniformly random sampling to the standard non-batch greedy algorithm.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "We compare our algorithms with following competitors: the standard greedy algorithm (GREEDY) [28], stochastic greedy algorithm (STOCH) [27], double greedy algorithm (DOUBLE) [4] and softmax extension (SOFTMAX) [8].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 25,
      "context" : "We compare our algorithms with following competitors: the standard greedy algorithm (GREEDY) [28], stochastic greedy algorithm (STOCH) [27], double greedy algorithm (DOUBLE) [4] and softmax extension (SOFTMAX) [8].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : "We compare our algorithms with following competitors: the standard greedy algorithm (GREEDY) [28], stochastic greedy algorithm (STOCH) [27], double greedy algorithm (DOUBLE) [4] and softmax extension (SOFTMAX) [8].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 7,
      "context" : "We compare our algorithms with following competitors: the standard greedy algorithm (GREEDY) [28], stochastic greedy algorithm (STOCH) [27], double greedy algorithm (DOUBLE) [4] and softmax extension (SOFTMAX) [8].",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 18,
      "context" : "3 We implement GREEDY using DPP marginalization requiring matrix inversion [19], which is a bit faster (preserving the same accuracy) than its naı̈ive implementation described in Section 2.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 24,
      "context" : "For boosting approximation qualities of our algorithms, we use the simple trick in our experiments: recompute top ` marginal gains exactly (using CG) 3We also run the accelerated greedy algorithm [26] for general submodular maximization, but do not report its performance since its approximation quality is extremely bad for the DPP case.",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 24,
      "context" : "In fact, the trick is inspired by [26] where the authors also recompute the exact marginal gain of a single element.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : "As [18, 19] proposed, a kernel matrix L for DPP can be re-parameterized as Li,j = qiφ > i φjqj , where qi ∈ R is considered as the quality of item i and φi ∈ R is the normalized feature vector of item i so that φi φj measures the similarity between i and j.",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 18,
      "context" : "As [18, 19] proposed, a kernel matrix L for DPP can be re-parameterized as Li,j = qiφ > i φjqj , where qi ∈ R is considered as the quality of item i and φi ∈ R is the normalized feature vector of item i so that φi φj measures the similarity between i and j.",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 3,
      "context" : "Interestingly, we found that DOUBLE has the strong theoretical guarantee for general submodular maximization [4], but its practical performance for DPP is bad.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : "We evaluate our proposed algorithms for matched summarization that is first proposed by [8].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "4 We follow similar pre-processing steps of [8].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "First, every sentence is parsed and only nouns except the stopwords are extracted via NLTK [2].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "We use 39 videos from a Youtube dataset [6], and the trained DPP kernels from [9].",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "We use 39 videos from a Youtube dataset [6], and the trained DPP kernels from [9].",
      "startOffset" : 78,
      "endOffset" : 81
    } ],
    "year" : 2017,
    "abstractText" : "Determinantal point processes (DPPs) are popular probabilistic models that arise in many machine learning tasks, where distributions of diverse sets are characterized by matrix determinants. In this paper, we develop fast algorithms to find the most likely configuration (MAP) of large-scale DPPs, which is NP-hard in general. Due to the submodular nature of the MAP objective, greedy algorithms have been used with empirical success. Greedy implementations require computation of log-determinants, matrix inverses or solving linear systems at each iteration. We present faster implementations of the greedy algorithms by utilizing the complementary benefits of two log-determinant approximation schemes: (a) first-order expansions to the matrix log-determinant function and (b) high-order expansions to the scalar log function with stochastic trace estimators. In our experiments, our algorithms are orders of magnitude faster than their competitors, while sacrificing marginal accuracy.",
    "creator" : "LaTeX with hyperref package"
  }
}