{
  "name" : "1602.07726.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adaptive Learning with Robust Generalization Guarantees",
    "authors" : [ "Rachel Cummings", "Katrina Ligett", "Kobbi Nissim", "Aaron Roth", "Zhiwei Steven Wu" ],
    "emails" : [ "rachelc@caltech.edu", "katrina@caltech.edu", "kobbi@cs.bgu.ac.il", "aaroth@cis.upenn.edu", "wuzhiwei@cis.upenn.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 2.\n07 72\n6v 1\n[ cs\n.D S]\n2 4\nFe b\nWe call the weakest such notion Robust Generalization. A second, intermediate, notion is the stability guarantee known as differential privacy. The strongest guarantee we consider we call Perfect Generalization. We prove that every hypothesis class that is PAC learnable is also PAC learnable in a robustly generalizing fashion, albeit with an exponential blowup in sample complexity. We conjecture that a stronger version of this theorem also holds that avoids any blowup in sample complexity (and, in fact, it would, subject to a longstanding conjecture [LW86, War03]). It was previously known that differentially private algorithms satisfy robust generalization. In this paper, we show that robust generalization is a strictly weaker concept, and that there is a learning task that can be carried out subject to robust generalization guarantees, yet cannot be carried out subject to differential privacy, answering an open question of [DFH+15a]. We also show that perfect generalization is a strictly stronger guarantee than differential privacy, but that, nevertheless, many learning tasks can be carried out subject to the guarantees of perfect generalization."
    }, {
      "heading" : "1 Introduction",
      "text" : "Generalization, informally, is the ability of a learner to reflect not just its training data, but properties of the underlying distribution from which the data are drawn. When paired with empirical risk minimization, it is the fundamental goal of learning. Typically, we say that a learning algorithm generalizes if, given access to some training set drawn i.i.d. from an underlying data distri-\n∗Dept. of Computing and Mathematical Sciences, California Institute of Technology. rachelc@caltech.edu †Dept. of Computing and Mathematical Sciences, California Institute of Technology and Benin School of Computer Science and Engineering, Hebrew University of Jerusalem. katrina@caltech.edu ‡Dept. of Computer Science, Ben-Gurion University and Center for Research in Computation and Society, Harvard University. kobbi@cs.bgu.ac.il §Dept. of Computer and Information Sciences, University of Pennsylvania. aaroth@cis.upenn.edu ¶Dept. of Computer and Information Sciences, University of Pennsylvania. wuzhiwei@cis.upenn.edu\nbution, it returns a hypothesis whose empirical error (on the training data) is close to its true error (on the underlying distribution).\nThis is, however, a surprisingly brittle notion—even if the output of a learning algorithm generalizes, one may be able to extract additional hypotheses by performing further computations on the output hypothesis—i.e., by postprocessing—that do not themselves generalize. As an example, notice that the standard notion of generalization does not prevent a learner from encoding the entire training set in the hypothesis that it outputs, which in turn allows a data analyst to generate a hypothesis that over-fits to an arbitrary degree. In this sense, traditional generalization is not robust to misinterpretation by subsequent analyses (postprocessing) (either malicious or naive).\nMisinterpretation of learning results is only one face of the threat—the problem is much more alarming. Suppose the output of a (generalizing) learning algorithm influences, directly or indirectly, the choice of future learning tasks. For example, suppose a scientist chooses a scientific hypothesis to explore on some data, on the basis of previously (generalizingly!) learned correlations in that data set. Or suppose a data scientist repeatedly iterates a model selection procedure while validating it on the same holdout set, attempting to optimize his empirical error. These approaches are very natural, but also can lead to false discovery in the first case, and disastrous overfitting to the holdout set in the second [DFH+15c], because traditional generalization is not robust to adaptive composition.\nIn this paper, we study two refined notions of generalization—robust generalization and perfect generalization, each of which is preserved under post-processing (we discuss their adaptive composition guarantees more below). Viewed in relation to these two notions, differential privacy can also be cast as a third, intermediate generalization guarantee. It was previously known that differentially private algorithms were also robustly generalizing [DFH+15b, BNS+16]. As we show in this paper, however, differential privacy is a strictly stronger guarantee—there are proper learning problems that can be solved subject to robust generalization that cannot be solved subject to differential privacy (or with any other method previously known to guarantee robust generalization). Moreover, we show that every PAC learnable class (even over infinite data domains) is learnable subject to robust generalization, albeit with an exponential blowup in sample complexity (a comparable statement is not known for differentially private learning). We conjecture that in fact learning under robust generalization does not require more than a constant blowup in sample complexity. We also show that, in a sense, differential privacy is a strictly weaker guarantee than perfect generalization. We provide a number of generic techniques for learning under these notions of generalization, prove useful properties for each, and explore intriguing open problems raised by our investigation. As we will discuss, perfect generalization also can be interpreted as a privacy guarantee, and thus may also be of interest to the privacy community."
    }, {
      "heading" : "1.1 Our Results",
      "text" : "Informally, we say that a learning algorithm has a guarantee of robust generalization if it is not only guaranteed to output a hypothesis whose empirical error is close to the true error (and near optimal), but if no adversary taking the output hypothesis as input can find another hypothesis whose empirical error differs substantially from its true error. (In particular, robustly generalizing algorithms are inherently robust to post-processing, and hence can be used to generate other test statistics in arbitrary ways without worry of overfitting). We say that a learning algorithm has the stronger guarantee of perfect generalization if its output reveals almost nothing about the training data that could not have been learned via only direct oracle access to the underlying data\ndistribution. It was previously known [DFH+15b, DFH+15a, BNS+16] that both differential privacy and bounded description length outputs are sufficient conditions to guarantee that a learning algorithm satisfies robust generalization. However, prior to this work, it was possible that differential privacy was equivalent to robust generalization in the sense that any learning problem that could be solved subject to the guarantees of robust generalization could also be solved via a differentially private algorithm.1 Indeed, this was one of the open questions stated in [DFH+15a]. We resolve this question (Section 3.3) by showing a simple proper learning task (learning threshold functions over the real line) that can be solved with guarantees of robust generalization (indeed, with the optimal sample complexity) but that cannot be non-trivially properly learned by any differentially private algorithm (or any algorithm with bounded description length outputs). We do so (Theorem 3.6) by showing that generalization guarantees that follow from compression schemes [LW86] carry over to give guarantees of robust generalization (thus giving a third technique, beyond differential privacy and description length arguments, for establishing robust generalization). In addition to threshold learning, important learning procedures like SVMs have compression schemes, and so satisfy robust generalization without modification. We also show (Theorem 3.7) that compression schemes satisfy an adaptive composition theorem, and so can be used for adaptive data analysis while guaranteeing robust generalization. Note that, somewhat subtly, robustly generalizing algorithms derived by other means need not necessarily maintain their robust generalization guarantees under adaptive composition (a sequence of computations in which later computations have access not only to the training data, but also to the outputs of previous computations). Using a recent result of [MY15], we show (Theorem 3.14) that any PAC learnable hypothesis class (even over an infinite domain) is also learnable with robust generalization, subject to an exponential blowup in sample complexity. We further conjecture (Conjecture 3.13) that every problem that can be PAC learned can also be PAC learnedwith guarantees of robust generalization, without any sample-complexity overhead. This would be the case, assuming the conjecture [LW86,War03] that every VC-class of dimension d has a compression scheme of size d.\nWe then show (Theorem 4.6) that perfectly generalizing algorithms can be compiled into differentially private algorithms (in a black box way) with little loss in their parameters, and that (Theorem 5.4) differentially private algorithms are perfectly generalizing, but with a loss of a factor of √ n in the generalization parameter. Moreover, we show (Theorem 5.5) that this √ n loss is necessary. Nevertheless, we show (Section 4.1) that any finite hypothesis class can be PAC learned subject to perfect generalization, and that (Theorem 2.13) perfectly generalizing algorithms satisfy an adaptive “advanced composition theorem” similar to differentially private algorithms [DRV10]. Hence, like differentially private algorithms, perfectly generalizing algorithms can be used in a black box manner for adaptive data analysis."
    }, {
      "heading" : "1.2 Related work",
      "text" : "Classically, machine learning has been concerned only with the basic generalization guarantee that the empirical error of the learned hypothesis be close to the true error. There are three main approaches to proving standard generalization guarantees of this sort. The first is by bounding\n1More precisely, it was known that algorithms with bounded description length could give robust generalization guarantees for the computation of high sensitivity statistics that could not be achieved via differential privacy [DFH+15a]. However, for low-sensitivity statistics (like the empirical error of a classifier, and hence for the problem of learning), there was no known separation.\nvarious notions of complexity of the range of the algorithm—most notably, the VC-dimension (see, e.g., [KV94] for a textbook introduction). These guarantees are not robust to post-processing or adaptive composition. The second follows from an important line of work [BE02, PRMN04, SSSSS10] that establishes connections between the stability of a learning algorithm and its ability to generalize. Most of these classic stability notions are defined over some metric on the output space (rather than on the distribution over outputs), and for these reasons are also brittle to post-processing and adaptive composition. The third is the compression-scheme method first introduced by [LW86] (see, e.g., [SSBD14] for a textbook introduction). As we show in this paper, the generalization guarantees that follow from compression schemes are robust to post-processing and adaptive composition. A longstanding conjecture [War03] states that VC-classes of dimension d have compression schemes of size d, but the best result known to date is that they have compression schemes of size exponential in d [MY15].\nA recent line of work [DFH+15b, DFH+15a, BNS+16, RZ16] has studied algorithmic conditions that guarantee the sort of robust generalization guarantees we study in this paper, suitable for adaptive data analysis. [DFH+15b] show that differential privacy (a stability guarantee on the output distribution of an algorithm) is sufficient to give robust generalization guarantees, and [DFH+15a] show that description length bounds on the algorithm’s output are also sufficient.\nDifferential privacy was introduced by [DMNS06] (see [DR14] for a textbook introduction), and private learning has been a central object of study since [KLN+11]. The key results we use here are the upper bounds for private learning proven by [KLN+11] using the exponential mechanism of [MT07], and the lower bounds for private proper threshold learning due to [BNSV15]. A measure similar to, but distinct from the notion of perfect generalization that we introduce here was briefly studied as a privacy solution concept in [BLR08] under the name “distributional privacy.”"
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Learning Theory Background",
      "text" : "LetX denote a domain, which contains all possible examples. A hypothesis h : X → {0,1} is a boolean mapping that labels examples by {0,1}, with h(x) = 1 indicating that x is a positive instance and h(x) = 0 indicating that x is a negative instance. A hypothesis class is a set of hypotheses. Throughout the paper, we elide dependencies on the dimension of the domain.\nWe will sometimes writeXL for X×{0,1}, i.e., labelled examples. LetDL ∈ ∆XL be a distribution over labeled examples; we will refer to it as the underlying distribution. We write SL ∼i.i.d. DnL to denote a sample of n labeled examples drawn i.i.d. from DL. A learning algorithm takes such a sample SL (also known as a training set) as input, and outputs a hypothesis. Note that we use subscript-L to denote labeling of examples in the domain, in samples, and in distributions. When DL is well-defined, we also sometimes write D for the marginal distribution of DL over X .\nTypically, the goal when selecting a hypothesis is to minimize the true error (also known as the expected error) of the selected hypothesis on the underlying distribution:\nerr(DL,h) = Pr (x,y)∼DL [h(x) , y].\nThis is in contrast to the empirical error (also known as the training error), which is the error of\nthe selected hypothesis h on the sample SL:\nerrE(SL,h) ≡ 1 |SL| ∑\n(xi ,yi)∈SL 1[h(xi) , yi].\nIn order to minimize true error, learning algorithms typically seek to minimize their empirical error, and to combine this with a generalization guarantee, which serves to translate low empirical error into a guarantee of low true error.\nFor any set S ∈ X n, let ES denote the empirical distribution that assigns weight 1/n on every observation in S . For any hypothesis h : X → {0,1}, we will write h(D) to denote Ex∼D [h(x)] and h(S) to denote h(ES ) = Ex∼ES [h(x)] = 1/n ∑ xi∈S h(xi ). We say that a hypothesis h : X → {0,1} αoverfits to the sample S taken from D if |h(S)− h(D)| ≥ α. Traditional generalization requires that a mechanism output a hypothesis that does not overfit to the sample.\nDefinition 2.1 ((Traditional)Generalization). Let Y be an arbitrary domain. AmechanismM : Yn → (Y → {0,1}) is (α,β)-generalizing if for all distributions C over Y , given a sample T ∼i.i.d. Cn,\nPr[M(T ) outputs h : Y → {0,1} such that |h(T )− h(C)| ≤ α] ≥ 1− β,\nwhere the probability is over the choice of the sample T and the randomness of M. Note that (traditional) generalization does not preventM from encoding its input sample T in the hypothesis h that it outputs. We initially use notation for the domain, distribution, and sample in our generalization definitions that is distinct from the analogous notation in learning settings, in order to emphasize that the generalization notions are meaningful both for general, unlabelled domains, and also in the context of learning (where the domain is over labelled examples).\nIn this paper, we consider two differentmodels of learning. Note that throughout the paper, we focus only on proper learning, wherein the learner is required to return a hypothesis from the class it is learning, rather than from, e.g., some superset of that class. For simplicity, we frequently omit the word “proper.” In the setting of PAC learning, we assume that the examples in the support of the underlying distribution are labeled consistently with some target hypothesis h∗ from a known hypothesis classH. In this case, we could write err(h) = Prx∼D[h(x) , h∗(x)]. Definition 2.2 (PAC Learning). A hypothesis classH over domain X is PAC learnable if there exists a polynomial nH : R2 → R and a learning algorithm A such that for all hypotheses h∗ ∈ Hd , all α,γ ∈ (0,1/2), and all distributionsD over X , given inputs α,γ and a sample SL = (z1, . . . , zn), where n ≥ nH(1/α, log(1/γ )), zi = (xi ,h(xi )) and the xi ’s are drawn i.i.d. from D, the algorithm A outputs a hypothesis h ∈ H with the following guarantee:\nPr[err(h) ≤ α] ≥ 1−γ.\nThe probability is taken over both the randomness of the examples and the internal randomness of A. We will call a learning algorithm with such a guarantee (α,γ )-accurate.\nIn the setting of agnostic learning, we do not assume that the labels of the underlying data distribution are consistentwith some hypothesis inH. The goal then becomes finding a hypothesis whose true error is almost optimal within the hypothesis class H. Definition 2.3 (Agnostic Learning). Agnostically learnable is defined identically to PAC learnable with two exceptions:\n1. the data are drawn and labelled from an arbitrary distributionDL over X × {0,1}\n2. the output hypothesis h satisfies the following\nPr[err(h) ≤OPT+α] ≥ 1−γ,\nwhere OPT = minf ∈H{err(f )} and the probability is taken over both the randomness of the data and the internal randomness of the algorithm.\nWe will call a learning algorithm with such a guarantee (α,γ )-accurate.\nIt is known that (in the binary classification setting we study), a hypothesis class is learnable if and only if its VC-dimension is polynomially bounded:\nDefinition 2.4 (VC Dimension [VC71]). A set S ⊆ X is shattered by a hypothesis class H if H restricted to S contains all 2|S | possible functions from S to {0,1}. The VC dimension of H denoted VCDIM(H), is the cardinality of a largest set S shattered byH."
    }, {
      "heading" : "2.2 Notions of Generalization",
      "text" : "In this section we introduce the three notions of generalization that are studied throughout this paper. We say that a mechanism M robustly generalizes if the mechanism does not provide information that helps overfit the sample it is given as input. Formally:\nDefinition 2.5 (Robust Generalization). Let R be an arbitrary range and Y an arbitrary domain. A mechanism M : Yn → R is (α,β)-robustly generalizing if for all distributions C over Y and any adversary A, with probability 1− ζ over the choice of sample T ∼i.i.d. Cn,\nPr[A(M(T )) outputs h : Y → {0,1} such that |h(T )− h(C)| ≤ α] ≥ 1−γ,\nfor some ζ,γ such that β = ζ +γ , where the probability is over the randomness of M and A.\nFor our other notions of generalization we require the following definition of distributional closeness.\nDefinition 2.6 ((ε,δ)-Closeness). Let R be an arbitrary range, and let ∆R denote the set of all probability distributions over R. We say that distributions D1,D2 ∈ ∆R are (ε,δ)-close and write D1 ≈ε,δ D2 if for all O ⊆R,\nPr y∼D1 [y ∈ O] ≤ exp(ε) Pr y∼D2 [y ∈ O] + δ and Pr y∼D2 [y ∈ O] ≤ exp(ε) Pr y∼D1 [y ∈ O] + δ.\nGiven an arbitrary domainY , we say samples T ,T ′ ∈ Yn are neighboring if they differ on exactly one element, i.e., if their symmetric difference is 1. A mechanism M is differentially private if the distributions of its outputs are close on neighboring samples.\nDefinition 2.7 (Differential Privacy, [DMNS06]). A mechanism M : Yn → R is (ε,δ)-differentially private if for every pair of neighboring samples T ,T ′ ∈ Yn,M(T ) ≈ε,δ M(T ′).\nLet Y be an arbitrary domain and R be an arbitrary range, and let ∆Y denote the set of all probability distributions over Y . A simulator Sim : ∆Y → R is a (randomized) mechanism that takes a probability distribution over Y as input, and outputs an outcome in the range R. For any fixed distribution C ∈ ∆Y , we sometimes write SimC to denote the output distribution Sim(C).\nWe say that a mechanism M perfectly generalizes if the distribution of its output when run on a sample is close to that of a simulator that did not have access to the sample.\nDefinition 2.8 (Perfect Generalization). Let R be an arbitrary range and Y an arbitrary domain. Let 0 ≤ β < 1, ε ≥ 0, and 0 ≤ δ < 1. A mechanismM : Yn →R is (β,ε,δ)-perfectly generalizing if for every distribution C over Y there exists a simulator SimC such that with probability at least 1 − β over the choice of sample T ∼i.i.d. Cn, M(T ) ≈ε,δ SimC .\nDiscussion of the generalization notions We will see that all three of the above generalization notions are robust to postprocessing and compatible with adaptive composition,2 making each of them much more appealing than traditional generalization for learning contexts. Perfect generalization also has an intuitive interpretation as a privacy solution concept that guarantees privacy not just to the individuals in a data sample, but to the sample as a whole (one can think of this as providing privacy to a data provider such as a school or a hospital, when each provider’s data comes from the same underlying distribution). Despite the very strong guarantee it gives, we will see that many tasks are achievable under perfect generalization."
    }, {
      "heading" : "2.3 Basic Properties of the Generalization Notions",
      "text" : "Here we state several basic properties of the generalization notions defined above. Proofs are deferred to Appendix A.\nThe following lemma is a useful tool for bounding the closeness parameters between two distributions via an intermediate distribution, such as that of the simulator. It allows us to say (Corollary 2.10) that for any perfectly generalizing mechanism, any two “typical” samples will induce similar output distributions.\nLemma 2.9. LetD1,D2,D3 be distributions over an abstract domain X . That is,D1,D2,D3 ∈ ∆X . If D1 ≈ε,δ D2 and D2 ≈ε′,δ′ D3 where ε,ε′ < ln2 then D1 ≈ε+ε′ ,2(δ+δ′) D3. If δ = δ′, then D1 ≈ε+ε′ ,3δ D3.\nCorollary 2.10. Suppose that a mechanism M : Yn → R is (β,ε,δ)-perfectly generalizing, where ε < ln2. Let T1,T2 ∼i.i.d. Cn be two independent samples. Then with probability at least 1−2β over the random draws of T1 and T2, the following holds\nM(T1) ≈2ε,3δ M(T2).\nWe can show that both robust generalization and perfect generalization are robust to postprocessing, i.e., arbitrary interpretation. It is known that differential privacy is also robust to postprocessing [DR14].\n2Specifically, differentially private and perfectly generalizing algorithms can be composed in a black box manner. Robustly generalizing algorithms cannot, but many particular methods for guaranteeing robust generalization, including those that are differentially private, compose adaptively.\nLemma 2.11 (Robustness to Postprocessing). Given any (α,β)-robustly generalizing (resp. (β,ε,δ)perfectly generalizing) mechanism M : Yn → R and any post-processing procedure A : R → R′, the compositionA◦M : Yn →R is also (α,β)-robustly generalizing (resp. (β,ε,δ)-perfectly generalizing).\nNext we show that the class of perfectly generalizing mechanisms is closed under adaptive composition, and that the parameters degrade smoothly as more mechanisms are composed. Variants of the following results were previously known for differentially private mechanisms; we show they also hold for perfectly generalizing mechanisms.\nTheorem 2.12 says that the composition of multiple (β,ε,0)-perfectly generalizingmechanisms is also perfectly generalizing, where the β and ε parameters “add up”.\nTheorem 2.12 (Basic composition). Let Mi : Yn → Ri be (βi , εi ,0)-perfectly generalizing for i = 1, . . . , k. The composition M[k] : Yn → R1 × · · · × Rk , defined as M[k](T ) = (M1(T ), . . .Mk(T )) is ( ∑k i=1βi , ∑k i=1 εi ,0)-perfectly generalizing.\nTheorem 2.12 required that when running k perfectly generalizing mechanisms, the ε parameter must decay linearly in k. Theorem 2.13 requires the ε parameter to decay only as √ k, at the expense of a small loss in the δ parameter; relative to Theorem 2.12, this allows for quadratically more mechanisms to be composed with nearly the same generalization guarantee. Further, the guarantees of Theorem 2.13 continue to hold even when the mechanisms to be composed can be chosen adaptively. That is, the ith mechanism can chosen based on the outputs of the first i − 1 mechanisms. This is the analogue of the composition theorem for differential privacy proven in [DRV10]. Note also that a stronger variant of Theorem 2.12 also holds, wherein the δ’s are nonzero, and also add up, and wherein the algorithms are adaptively chosen.\nTheorem 2.13 (Advanced composition). Let M[k] : Yn → R be a mechanism with arbitrary kdimensional range R such that the output of M[k] can be written as M[k](T ) = (y1, . . . ,yk), where y1 =M1(T ), y2 =M2(T ;y1), . . ., yk =Mk(T ;y1, . . . ,yk−1) andwhere eachMi is (β,ε,δ)-perfectly generalizing (for all values of parameters y1, . . . ,yk). ThenM[k] is (kβ,ε′ , kδ+δ′)-perfectly generalizing for:\nε′ = √ 2k ln(1/δ′)ε + kε(eε − 1)."
    }, {
      "heading" : "3 Robust Generalization via Compression Schemes",
      "text" : "In this section, we present a new technique, based on the idea of compression bounds, for designing learning algorithms with the robust generalization guarantee. Recent work [DFH+15b, DFH+15a, BNS+16] gives two other techniques for obtaining robust generalizing mechanisms. As we will see, our new technique allows one to learn hypothesis classes under robust generalization for which the two previous techniques do not apply. These results lead us to pose an intriguing open question: is every PAC learnable hypothesis also PAC learnable under the constraint of robust generalization? This question is closely connected to a longstanding open problem [LW86,War03].\nWe first give a definition for what it means to learn a hypothesis under robust generalization.\nDefinition 3.1 (RG PAC/Agnostic Learning). A hypothesis classH over domainX isPAC/agnostically learnable under robust generalization (RG-PAC/agnostically learnable) if there exists a polynomial nH : R4 → R and a learning algorithm A such that for all α,β,ε,δ ∈ (0,1/2), given inputs α,β,ε,δ\nand a sample SL ∈ X nL where n ≥ nH(1/α,1/ε, log(1/β), log(1/δ)), the algorithm A is an (α,β)accurate PAC/agnostic learner, and is (ε,δ)-robustly generalizing."
    }, {
      "heading" : "3.1 Compression Learners",
      "text" : "A hypothesis class has a compression scheme of size k if any arbitrary set SL of labeled examples can be mapped to a size-k sequence of the input examples, from which it is possible to compute an empirical risk minimizer for SL.\nDefinition 3.2 (Compression Scheme). Let H be a hypothesis class and let k ∈ N. We say that H has a compression scheme of size k if for all n ∈N, there exists a compression algorithm A : X nL →X kL and an encoding algorithm B : X kL → H such that for any arbitrary set SL of n labelled examples, A will select a sequence of examples A(SL) = (zi1 , zi2 , . . . , zik ) ∈ SkL , and B will output a hypothesis h′ = B(A(SL)) that is an empirical risk minimizer; i.e. errE(SL,h′) ≤ errE(SL,h) for all h ∈ H. We call L = (B ◦A) the compression learner of size k.\nIt is known that the existence of compression schemes implies the learnability of a hypothesis class:\nTheorem 3.3 ([LW86]). Let H be a hypothesis class with a compression scheme of size k. Then H is PAC and agnostically learnable under robust generalization with a sample complexity of k ·poly(1/α, log(1/β)), where α,β are accuracy parameters in Definition 3.1.\nNext, we want to show that any such compression learner also satisfies robust generalization. As an intermediate step, we recall the following result, which follows from a standard application of a concentration bound.\nLemma 3.4 (see, e.g., [SSBD14] Theorem 30.2). Let n,k ∈ N such that n ≥ 2k. Let A : X nL → X kL be an algorithm that takes a sample SL of n labelled examples as input, and selects a sequence of labeled examples A(SL) = (zi1 , zi2 , . . . , zik ) ∈ SkL of length k. Let B : X kL → (X → {0,1}) take a sequence of k labelled examples and return a hypothesis.\nFor any random sample SL ∼i.i.d. DnL , let VL = {z | z < A(SL)} be set examples not selected by A, and write V for the unlabelled version of VL. Let h = B(A(SL)) be the hypothesis output by B. Then, with probability of at least 1− δ over the random draws of SL, we have\n|h(V )− h(D)| ≤ √ h(V ) 4k log(2n/δ)\nn + 8k log(2n/δ) n\nRecall that h(D) = Ex∼D [h(x)] denotes the expected value of h, and h(V ) = 1n−k ∑\nx∈V h(x) is the average value of h over the examples in V .\nThis theorem is useful in analyzing the guarantees of a compression learner. If we interpret A as a compression algorithm, and B as an encoding algorithm that outputs a hypothesis h, Theorem 3.4 says that the empirical error of h over the remaining subset V is close to its true error.\nHowever, we can also interpret algorithm B as an adversary who is trying to overfit a hypothesis to the input sample SL. Since the hypothesis output by a compression algorithm is uniquely determined by the sequence of examples output by the compression algorithm A, we could think of the adversary post-processing the size-k sequence of examples that defines the output hypothesis. Therefore, it suffices to show that the compression algorithm A is robustly generalizing.\nWe will establish this by showing that any algorithm that outputs a small sequence of the input sample is robustly generalizing:\nLemma 3.5. Let n,k be integers let ε,δ > 0, and let A : X nL →X kL be an algorithm that takes any set SL ∈ X nL as input and outputs a sequence T ∈ SkL of size k. Then A is (ε,δ)-robustly generalizing for\nε =O   √ k log(n/δ)\nn\n  ,\nas long as n ≥ 8k log(2n/δ). Proof. We will appeal to Theorem 3.4. Let F : X kL → {X → {0,1}} be a deterministic mapping from samples of size k to hypotheses. Let SL ∼i.i.d. Dn be a random sample of size n, T = A(SL) be the sequence output by the compression algorithm, V be the examples (without labels) not selected by A, and f = F(T ) be the function output by the adversary. By the result of Lemma 3.4, we know that with probability at least 1− δ over the random draws of SL, the following holds,\n|f (V )− f (D)| ≤ √ 4k log(2n/δ)\nn + 8k log(2n/δ) n ≡ C\nLet S be the examples in SL but without labels. By triangle inequality we have\n|f (SL)− f (D)| ≤ 1\nn ∣∣∣∣∣∣∣∣ ∑ z∈SL (f (z)− f (D)) ∣∣∣∣∣∣∣∣\n= 1\nn ∣∣∣∣∣∣∣ ∑ z∈V (f (z)− f (D)) + ∑ z<V (f (z)− f (D)) ∣∣∣∣∣∣∣\n≤ 1 n ∣∣∣∣∣∣∣ ∑ z∈V (f (z)− f (D)) ∣∣∣∣∣∣∣ + 1 n ∣∣∣∣∣∣∣ ∑ z<V (f (z)− f (D)) ∣∣∣∣∣∣∣ ≤ Cn n + k n = C + k n (1)\nFor n ≥ 8k log(2n/δ), the right hand side of Equation (1) is bounded by O( k log(n/δ)n ), which recovers our stated bound.\nTo extend the analysis to any randomized function F ′, observe that any randomized mapping from X kL to hypotheses X → {0,1} is just a distribution over a collection of deterministic mappings F (defined above). It follows that F ′(T ) is just a distribution over a collection of functions f that satisfy Equation (1). Therefore, the robust generalization guarantee holds with probability 1 over the randomness of F ′.\nTheorem 3.6. Suppose that a hypothesis class H has a compression scheme of size k. Then H is both PAC and agnostically learnable under robust generalization by the compression learner.\nProof. Let α,β be target accuracy parameters and ε,δ be the target generalization parameters (as in Definition 3.1). By our assumption, we know that there exists a compression algorithm A and encoding algorithm B such that the compression learner L = (B ◦ A). We know that L can PAC/agnostically learnH with a sample complexity of k ·poly(1/α, log(1/β)).\nTo show that L satisfies robust generalization, observe that given any sample SL of size n, the hypothesis h output by L is just a postprocessing function of the sequence A(SL). By the postprocessing guarantee property in Lemma 2.11, it suffices to show that the compression learner A is robustly generalizing. By Lemma 3.5, we know that the compression learner is (ε,δ)-robustly generalizing as long as n ≥ Õ ( k log(1/δ)\nε2\n) .\nTherefore, the compression learner is both an (α,β)-accurate PAC/agnostic learner and satis-\nfies (ε,δ)-robust generalization with a sample complexity of k ·poly ( 1 α , 1 ε , log ( 1 β ) , log ( 1 δ )) .\nWe can also show that compression learners continue to give robust generalizationunder adaptive composition.\nTheorem 3.7 (Adaptive Composition for Compression Learners). LetM[m] : X n →Hm be an adaptive composition of compression schemes such that for any S ∈ X n, M[m](S) = (h1, . . . ,hm), where h1 = M1(S), h2 = M2(S ;h1), . . . ,hm = M(S ;h1, . . . ,hm−1), where Mi(·;h1, . . . ,hi−1) is a compression learner of size ki for all choices of h1, . . . ,hi−1. Let k = ∑m i=1 ki . ThenM[m] is (ε,δ)-robustly generalizing, where\nε =O   √ k log(n/β)\nn\n  ,\nas long as n ≥ 8k log(2n/β).\nProof. For each Mi , we can write it as Mi(·;h1, . . . ,hi−1) = (Bi ◦Ai), where Ai is the compression algorithm and Bi is the encoding algorithm. Note that the sequence of output hypotheses is just a postprocessing of the sequence of examples output by the compression algorithms—that is, given the sequence of examples output by the compression algorithms, we can uniquely determine the set of output hypotheses. So it suffices to prove that the adaptive composition of the compression algorithms satisfies robust generalization. Note that the composition compression algorithms can be viewed as a single compression algorithm that releases a sequence of examples of length k. By directly applying Lemma 3.5, we recover the stated bound."
    }, {
      "heading" : "3.2 Robust Generalization via Differential Privacy and Description Length",
      "text" : "We briefly review two existing techniques for obtaining algorithms with robust generalization guarantee, from the recent line of work starting with [DFH+15b], and followed by [DFH+15a, BH15, BNS+16]. Here we will rephrase their results in terms of robust generalization (this terminology is new to the present paper).\nFirst, it is known that differential privacy implies robust generalization.\nTheorem 3.8 ([BNS+16]). Let M : X n → R be a (ε,δ)-differentially private mechanism for n ≥ O(ln(1/δ)/ε2). ThenM also satisfies (O(ε),O(δ/ε))-robust generalization.\nAlgorithms with a small output range (i.e., each output can be described using a small number of bits) also enjoy robust generalization.\nTheorem 3.9 ([DFH+15a]). Let M : X n → R be a mechanism such that |R| is bounded. Then M satisfies (α,β)-robust generalization, with α = √ ln(|R|/β)\n2n ."
    }, {
      "heading" : "3.3 Case Study: Proper Threshold Learning",
      "text" : "Next, we consider the problem of properly learning thresholds in the PAC setting. We will first note that when the domain size is infinite, there is no proper PAC learner that is differentially private or has finite output range. In contrast to these impossibility results, we show that the class of threshold functions admits a simple compression scheme, and hence a PAC learning algorithm that satisfies robust generalization. This result, in particular, gives a separation between the power of learning under robust generalization and that of learning under differential privacy.\nConsider the hypothesis class of threshold functions {hx}x∈X over a totally ordered domain X , where hx(y) = 1 if y ≤ x and hx(y) = 0 if y > x. We will first recall an impossibility result for privately learning thresholds.\nTheorem 3.10 ([BNSV15] Theorem 6.2). Let α > 0 be the accuracy parameter (as in Definition 2.2). For every n ∈N, and δ ≤ 1/(1500n2), any (1/2,δ)-differentially private and (α,1/8)-accurate (proper) PAC learner for threshold functions requires sample complexity n =Ω (log∗ |X |/α).\nIn particular, the result of Theorem 3.10 implies that there is no private proper PAC learner for threshold functions over an infinite domain. Similarly, we can show that there is no proper PAC learner for thresholds that has a finite outcome range. Lemma 3.11. For any n ∈ N and any learner M : X n → H′ such that the output hypothesis class H′ is a subset of H and has bounded cardinality, there exits a distribution D ∈ ∆X such that the output hypothesis has true error err(h) ≥ 1/2. Proof. Let |H′ | = m. We can write H′ = {hx1 ,hx2 , . . . ,hxm } such that x1 < x2 < . . . < xm. Let y,z be points such that x1 < y < z < x2. Let D be a distribution over X that puts half of the probability mass on y and the other half on z. Suppose our target hypothesis is c = hy . Then c(y) = 1 and c(z) = 0. Note that for each h ∈ H′, it must be case that h(y) = h(z), and thus its true error must be at least 1/2.\nNow we will show that the class of threshold functions can be properly PAC learned under the constraint of robust generalization even when the domain size is infinite.\nTheorem 3.12. For any α,ε,δ,γ > 0, there exists a compression learner that is an (ε,δ)-accurate (proper) PAC learner for thresholds, and satisfies (α,γ )-robust generalization, as long as the sample size satisfies\nn ≥ Õ ( max { 1\nα , 1 ε2\n} ln ( 1\nδγ\n)) .\nProof. Consider the compression function A, that, given a sample, outputs the largest positive example s+ in the sample. Then consider the encoding function B that, given any example s+, returns the threshold function hs+ . Such a threshold function will correctly label all the examples in the sample. This gives us a compression scheme of size 1 for the class of threshold functions. By Theorem 3.6, we know that this learner is (ε,δ)-robust generalizing as long as n ≥ O ( 1 ε2 log(n/δ) ) . By Theorem 3.3 and the sample complexity bound of [LW86], this learner is an (α,β)-accurate PAC learner if\nn ≥max { 2\nα ln\n( 1\nβ\n) , 4\nα ln\n( 4\nα\n) +2 } .\nTherefore, if n satisfies the stated bound, the compression learner is both (α,β)-accurate and satisfies (ε,δ)-robust generalization."
    }, {
      "heading" : "3.4 A Conjecture on Robust Generalizing Learning",
      "text" : "The result of Theorem 3.6 gives us a novel tool to design robustly generalizing learners for several hypothesis classes. For example, we can learn axis aligned rectangles, half-spaces, and separating polynomials based on the compression schemes for these hypothesis classes. This leads us to propose the following conjecture:\nConjecture 3.13. Every PAC learnable hypothesis class is also PAC learnable under robust generalization.\nWe will first show that any PAC learnable hypothesis class (i.e., any hypothesis class with polynomial VC-dimension) is learnable with a guarantee of robust generalization, but not necessarily with a polynomial sample size.\nTheorem 3.14. Let H be a hypothesis class with VC dimension d, and let α,β,ε,δ ∈ (0,1/2). Then there exists a (α,β)-accurate PAC learning algorithm forH that satisfies (ε,δ)-robust generalization with a sample size of Õ ( 2O(d)max { 1 α , 1 ε2 } ln ( 1 δγ )) .\nProof. We rely on a recent result of [MY15], which gives a compression scheme of size 2O(d) for any hypothesis class with VC dimension d. By Theorem 3.6 and the sample complexity bound in [LW86], there is (α,β)-accurate PAC learner that satisfies (ε,δ)-robust generalization as long as the sample size satisfies the stated bound.\nEven though the sample complexity bound has an exponential dependence on the VC dimension, this result gives us a generic robustly generalizing learner for any learnable class. Previously, such a generic robustly generalizing learner was only known in the case when either the hypothesis class or the domain had bounded size; such results derive from a differentially private algorithm in [KLN+11].\nIf one could improve the exponential dependence 2O(d) in Theorem 3.14 to poly(d), our conjecture would hold. Our conjecture is also related to an longstanding conjecture regarding compression bounds and learnability.\nConjecture 3.15 ([LW86, War03]). For any hypothesis class of VC dimension d, there is a compression scheme of size O(d).\nNote that subject to Conjecture 3.15, our Conjecture 3.13 is true.\nLemma 3.16. Suppose that Conjecture 3.15 holds. Then any hypothesis class that is PAC learnable is also PAC learnable under robust generalization.\nNote, however, that it may be possible to resolve our Conjecture 3.13 without resolving Conjecture 3.15. We will have a more in-depth discussion in Section 6."
    }, {
      "heading" : "4 Learning under Perfect Generalization",
      "text" : "In this section, we will focus on the problem of agnostic learning under the constraint of perfect generalization. Our main result gives a perfectly generalizing generic learner in the settings where the domain X or the hypothesis class H has bounded size. The sample complexity will depend\nlogarithmically on these two quantities. Furthermore, we give a reduction from any perfectly generalizing learner to a differentially private learner that preserves the sample complexity bounds (up to constant factors). This allows us to carry over lower bounds for differentially private learning to learning under perfect generalization. In particular, we will show that proper threshold learning with unbounded domain size is impossible under perfect generalization.\nWe will first define what it means to learn a hypothesis under perfect generalization.\nDefinition 4.1 (PG PAC/Agnostic Learning). Ahypothesis classH over domainX is PAC/agnostically learnable under perfect generalization (PG-PAC/agnostically learnable) if there exists a polynomial nH : R5 → R and a learning algorithm A such that for all α,γ,β,ε,δ ∈ (0,1/2), given inputs α,γ,β,ε,δ and a sample SL ∈ X nL where n ≥ nH(1/α,1/ε, log(1/γ ), log(1/β), log(1/δ)), the algorithm A is an (α,γ )-accurate PAC/agnostic learner, and is (ε,δ)-perfectly generalizing."
    }, {
      "heading" : "4.1 Generic PG Agnostic Learner",
      "text" : "Nowwe present a generic perfectly generalizing agnostic learner, which is based on the exponential mechanism of [MT07] and analogous to the generic learner of [KLN+11].\nOur learner, formally presented in Algorithm 1, takes generalization parameters ε,β, a sample of n labeled examples SL ∼i.i.d. DnL , and a hypothesis class H as input, and samples a random hypothesis with probability that is exponentially biased towards hypotheses with small empirical error. We show that this algorithm is perfectly generalizing.\nAlgorithm 1 Generic Agnostic Learner A(β, ε, SL, H)\nOutput h ∈ H with probability proportional to exp ( − √ |SL|·ε·errE (SL,h)√ 2ln(2|H|/β) )\nLemma 4.2. Given any ε,β > 0 and finite hypothesis class H, the learning algorithm A(β,ε, ·, ·) is (β,ε,0)-perfectly generalizing.\nProof. Let SL ∼i.i.d. DnL be a labeled random sample of size n. Note that since each (xi ,yi ) in SL is drawn from the underlying distribution DL, we know that for each h ∈ H,\nE SL∼i.i.d.DnL\n[errE(SL,h)] = err(h).\nFix any h ∈ H. Then by a Chernoff-Hoeffding bound, we know that with probability at least 1− β/ |H|, the following holds:\n|errE(SL,h)− err(h)| ≤ √ 1\n2n ln ( 2|H| β ) . (2)\nApplying a union bound, we know that the above holds for all h ∈ H with probability at least 1− β. We will condition on this event for the remainder of the proof. Now consider the following randomized simulator:\nSim(DL) : output h ∈ H with probability proportional to exp   −ε · √n · err(h) √ 2ln(2|H|/β)   .\nWe want to show that the output distributions satisfyA(β,ε,SL) ≈ε,0 Sim(DL), where SL ∼i.i.d. DnL is a labeled random sample of size n. LetZ = ∑ h∈H exp ( −ε√n·errE(SL ,h)√\n2ln(2|H|/β)\n) andZ ′ = ∑ h∈H exp ( −ε·√n·err(h)√ 2ln(2|H|/β) ) .\nFor each h ∈ H,\nPr[A(β,ε,SL,H) = h] Pr[Sim(DL) = h] =\nexp ( −ε·√n·errE(SL,h)√\n2ln(2|H|/β)\n) /Z\nexp ( −ε·√n·err(h)√ 2ln(2|H|/β) ) /Z ′\n= exp   ε · √n (err(h)− errE(SL,h))√\n2ln(2|H|/β)\n  · Z ′\nZ\n≤ exp ( ε\n2\n) exp ( ε\n2 ) · Z Z\n= exp(ε).\nA symmetric argument would also show that Pr[Sim(DL)=h]\nPr[A(β,ε,SL ,H)=h] ≤ exp(ε). Therefore, A(β,ε, ·, ·) is (β,ε,0)-perfectly generalizing.\nTheorem 4.3. LetH be a finite hypothesis class and α,γ > 0. Then the generic learner Algorithm 1 instantiated as A(γ,ε, ·,H) is (α,γ )-accurate as long as the sample size\nn ≥ 6 ε2α2 (ln(2|H|) + ln(1/γ ))3 .\nProof. Let SL ∼i.i.d. DnL, and let the algorithm A(γ,ε,SL,H) be the Generic Agnostic Learner of Algorithm 1. Consider the event E = {A(γ,ε,SL,H) = h | err(h) > α +OPT}, where α is our target accuracy parameter. We want to show that Pr[E] ≤ γ as long as the sample size n satisfies the stated bound.\nBy a Chernoff-Hoeffding bound (similar to Equation (2)), we have that with probability at least 1−γ/2, the following condition holds for each h ∈ H:\n|errE(SL,h)− err(h)| ≤ √ 1\n2n ln ( 4|H| γ ) ≡ B(n).\nWe will condition on the event above. Let h∗ = argminh′∈H err(h′) and let OPT = err(h∗), then\nmin h′∈H\nerrE(SL,h ′) ≤ errE(SL,h∗) ≤ err(h∗) +B(n) = OPT+B(n)\nRecall that for each h ∈ H, the probability that the hypothesis output by A(γ,ε,SL,H) is h is, exp ( −ε√n · errE(SL,h)/ √ 2ln(2|H|/γ ) )\n∑ h′∈H exp ( −ε√n · errE(SL,h′)/ √ 2ln(2|H|/γ ) )\n≤ exp\n( −ε√n · errE(SL,h)/ √ 2ln(2|H|/γ ) )\nmaxh′∈H exp ( −ε√n · errE(SL,h′)/ √ 2ln(2|H|/γ ) )\n= exp ( −ε √ n · (errE(SL,h)−min\nh′∈H errE(SL,h\n′))/ √ 2ln(2|H|/γ )\n)\n≤ exp ( −ε √ n · (errE(SL,h)−OPT−B(n))/ √ 2ln(2|H|/γ ) ) .\nTaking union bound, we know that the probability that A(γ,ε,SL,H) outputs a hypothesis h with empirical error errE(SL,h) ≥OPT+2B(n) is at most |H|exp ( −ε√nB(n)/ √ 2ln(2|H|/γ ) ) .\nLet’s set B(n) = α/3, and the event E = {A(γ,ε,SL,Hd ) = h | err(h) > α +OPT} implies\nerrE(SL,h) ≥OPT+2α/3 = OPT+2B(n) and |errE(SL,h)− err(h)| ≥ α/3 = B(n).\nIt is sufficient to set n large enough to bound the probabilities of these two events. Further if we a sample size n ≥ 6\nε2α2 (ln(2|H|/γ ))3, both probabilities are bounded by γ/2, which means we must\nhave Pr[E] ≤ γ ."
    }, {
      "heading" : "4.2 PG Learning with VC Dimension Sample Bounds",
      "text" : "We can also extend the sample complexity bound in Theorem 4.3 by one that is dependent on the VC dimension of the hypothesis class H, but resulting bound will have a logarithmic dependence on the size of the domain |X |.\nCorollary 4.4. Every hypothesis class H with finite VC dimension is PG agnostically learnable with a sample size of n =O ( (VCDIM(H) · ln |X |+ ln 1β )3 · 1ε2α2 ) .\nProof. By Sauer’s lemma (see e.g., [KV94]), we know that there are at mostO(|X |VCDIM(H)) different labelings of the domain X by the hypotheses in H. We can run the exponential mechanism over such a hypothesis class H′ with cardinality |H′ | = O ( |X |VCDIM(H) ) . The complexity bound follows from Theorem 4.3 directly."
    }, {
      "heading" : "4.3 Limitations of PG learning",
      "text" : "We have so far given a generic agnostic learner with perfect generalization in the cases where either |X | or |H| is finite. We now show that such finiteness condition is necessary by revisiting the threshold learning problem in Section 3.3. In particular, we will show that when the domain size is infinite, properly learning thresholds under perfect generalization is impossible. Our result crucially relies on a reduction from a perfectly generalizing learner to a differentially private learner, which allows us to apply lower bound results of differentially private learning(such as Theorem 3.10) to PG agnostic learning.\nFirst, let’s consider the reduction in Algorithm 2, which is a black-box mechanism that takes as input a perfectly generalizingmechanismM : X nL →R and a labeled sample SL ∈ X nL , and outputs an element of R. We show that this new mechanismM′(M, ·) is differentially private.\nAlgorithm 2M′(M : X nL →R, SL ∈ X nL ) Let ESL be the empirical distribution that assigns weight 1/n to each of the data points in SL Sample TL ∼i.i.d. (ESL)n OutputM(TL) ∈ R\nTheorem 4.5. Let β < 1/2e and ε ≤ ln(2), and M be a (β,ε,δ)-perfectly generalizing mechanism, then the mechanismM′(M, ·) of Algorithm 2 is (4ε,16δ+2β)-differentially private.\nProof. Let SL,S ′ L ∈ X n be neighboring databases that differ on the ith entry, and let ESL and ES ′L denote their corresponding empirical distributions. Since M is (β,ε,δ)-perfectly generalizing, there exists a simulator Sim such that with probability at least 1− β over choosing TL ∼i.i.d. (ESL )n,\nM(TL) ≈ε,δ Sim. (3)\nSimilarly, there exists a simulator Sim′ such that with probability at least 1 − β over choosing T ′L ∼i.i.d. (ES ′L)n,\nM(T ′L) ≈ε,δ Sim′ . (4) We get from equations (3), (4) that with probability at least 1−2β over the choice of TL and T ′L,\nM(TL) ≈ε,δ Sim and M(T ′L) ≈ε,δ Sim′ . (5)\nLet {(xi ,yi )} = SL \\ S ′L. Then for a (1 − 1/n)n ≈ 1/e fraction of the samples TL ∼i.i.d. (ESL )n, we have (xi ,yi) < TL and thus TL also lies in the support of (ES ′L )n. Since 1/e − 2β > 0, there exists a T ∗ L ∈ X nL such that (xi ,yi ) < T ∗ L , so T ∗ L lies in the support of both (ESL )n and (ES ′L)n. Since M is perfectly generalizing, we have that,\nM(T ∗L) ≈ε,δ Sim and M(T ∗L) ≈ε,δ Sim′ . (6)\nCombining equations (5) and (6), we have with probability at least 1− 2β,\nM(TL) ≈ε,δ Sim ≈ε,δ M(T ∗L) ≈ε,δ Sim′ ≈ε,δ M(T ′L).\nBy Lemma 2.9, with probability at least 1− 2β,\nM′(SL) =M(TL) ≈4ε,16δ M(T ′L) =M′(S ′L).\nTherefore,M′ is (4ε,16δ+2β)-differentially private.\nTheorem 4.6. Let H be a hypothesis class with VC dimension d < ∞. Suppose that H admits an agnostic learner M : X nL → H that is (α,γ )-accurate and (β,ε,δ)-perfectly generalizing. Then algorithmM′(M, ·) defined as in Algorithm 2 is (4ε,16δ+2β)-differentially private, and is also an (O(α),O(γ ))-accurate agnostic learner for H.\nWe will rely on the following result on the uniform convergence properties of any hypothesis class with finite VC dimension.\nTheorem 4.7 (see e.g. [SSBD14] Theorem 6.8). LetH be a hypothesis class of VC dimension d <∞. Then there are constants C1 and C2 such that the following holds:\n1. Fix any α,γ > 0. Let SL ∼i.i.d. DnL , then with probability at least 1−γ , |errT (SL,h)− err(h)| ≤ α for all h ∈ H, as long as\nn ≥ C1 d + log(1/γ )\nα2\n2. Any agnostic learner that is (α,γ )-accurate requires a sample of size at least\nC2 d + log(1/γ )\nα2\nTheorem 4.6. Let SL ∼i.i.d. DnL be a random sample of size n. By Part 2 of Theorem 4.7 and our assumption that M is an (α,γ )-accurate agnostic learner, we know that n ≥ C2 (d+log(1/γ ))α2 .\nLet ĥ =M′(M,SL). First, we can view ESL as some distribution over the labeled examples. Since M is an (α,γ )-accurate learner, we have with probability at least 1−γ , for each h ∈ H,\nerrE(SL, ĥ) ≤ errE(SL,h) +α. (7)\nLet’s condition on this event. By Part 1 of Theorem 4.7, we have with probability at least 1−γ over the random draws of SL, for each h ∈ H,\n|errE(SL,h)− err(h)| ≤O(α). (8)\nWe will condition on this event as well. Let h∗ = argminh∈H err(h). Then by combining Equations (7) and (8), we get\nerr(ĥ) ≤ errE(SL, ĥ) +O(α) ≤ errE(SL,h∗) +O(α) ≤ err(h∗) +O(α)\nwhich recovers the stated utility guarantee. By Theorem 4.5, know that the mechanism M′(M, ·) is also (4ε,16δ+2β)-differentially private.\nThe result of Theorem 4.6 implies that the existence of a perfectly generalizing agnostic learner would imply the existence of a differentially private one. Moreover, the lower bound results for private learning would also apply to perfectly generalizing learner as well. In particular, based on the result of [BNSV15], we can show that there is no proper threshold learner that satisfies perfect generalization when the domain size is infinite.\nTheorem 4.8. Let α > 0 be the accuracy parameter. For every n ∈ N, and δ,β ≤ 1/(10000n2), any (β,1/8,δ)-perfectly generalizing and (α,1/32)-accurate proper agnostic learner for threshold function requires sample complexity n =Ω (log∗ |X |/α)."
    }, {
      "heading" : "5 Relationship between Perfect Generalization and Other Generaliza-",
      "text" : "tion Notions\nIn this section, we explore the relationship between perfect generalization and both robust generalization and differential privacy. Section 5.1 shows that any perfectly generalizing mechanism is also robustly generalizing, but there exists robustly generalizing mechanisms that are neither differentially private nor perfectly generalizing for any reasonable parameters. Section 5.2 shows that all differentially private mechanisms are perfectly generalizing with some necessary loss in generalization parameters, but there exist perfectly generalizing mechanisms which are not differentially private for any reasonable parameters.\nIn this way, perfect generalization is stronger than robust generalization in a qualitative sense: perfect generalization implies robust generalization, but the converse is not true. Perfect generalization is stronger than differential privacy only in a quantitative sense: differential privacy does imply perfect generalization, but with generalization parameters that are necessarily worse."
    }, {
      "heading" : "5.1 Separation between Perfect and Robust Generalization",
      "text" : "In this section we show that perfect generalization is a stronger requirement than robust generalization. Lemma 5.1 shows one direction of this, by showing that every perfectly generalizing mechanism also satisfies robust generalization with only a constant degradation in the generalization parameters.\nLemma 5.1. For any β,ε,δ ∈ (0,1), suppose that a mechanismM : X n →R with arbitrary rangeR is (β,ε,δ)-perfectly generalizing. Then M is also (α,2(β + δ))-robustly generalizing, where\nα =\n√ 2\nn ln\n( 2(2ε+1)\nβ + δ\n) .\nProof. Let A : R → (X → {0,1}) be function that takes in the output of M(S) and produces a hypothesis h : X → {0,1}. Our goal is to show that h will not overfit on the original sample S .\nBy Lemma 2.11, the composition of A◦M : X n → (X → {0,1}) is also (β,ε,δ)-perfectly generalizing. This means there exits simulator Sim : ∆X →R such that with probability over a random sample S , Sim(D) ≈ε,δ (A ◦M)(S). Define the event E = {S ∈ X n | [ Sim(D) ≈ε,δ (A◦M)(S)\n]}. By perfect generalization, PrS∼i.i.d.Dn[E] ≥ 1− β.\nAlso by a Chernoff-Hoeffding bound, for any fixed h ∈ H and any α > 0,\nPr S∼i.i.d.Dn\n[|h(S)− h(D)| ≥ α] ≤ 2exp ( −2α2n ) .\nThe following bounds the probability that the hypothesis h output by (A ◦M)(S) overfits on the sample S , where ∧ denotes the logical AND.\nPr S∼i.i.d.Dn\n[h← (A◦M(S))∧ |h(S)− h(D)| ≥ α]\n= ∑\nS∈X n Pr[S]Pr[h← (A◦M(S))∧ |h(S)− h(D)| ≥ α | S]\n≤(1−Pr[E]) + ∑\nS∈E Pr[S]Pr[h← (A◦M(S))∧ |h(S)− h(D)| ≥ α | S]\n≤(1−Pr[E]) + ∑\nS∈E Pr[S] (Pr[h← Sim(D)∧ |h(S)− h(D)| ≥ α | S] · exp(ε) + δ)\n≤(1−Pr[E]) + ∑\nS∈X n Pr[S] (Pr[h← Sim(D)∧ |h(S)− h(D)| ≥ α | S] · exp(ε) + δ)\n=(1−Pr[E]) + δ+ exp(ε) Pr S∼i.i.d.Dn [h← Sim(D)∧ |h(S)− h(D)| ≥ α] ≤(1−Pr[E]) + δ+2exp(ε) · exp(−2α2n) ≤β + δ +2exp(ε) · exp(−2α2n)\nSetting α = √\n2 n ln ( 2(2ε+1) β+δ ) also gives exp(−2α2n) = β+δ 2(2ε+1) . Plugging this into the above equa-\ntions, we see that the probability that (A◦M)(S) overfits to S by more than our choice of α is at most:\nPr S∼i.i.d.Dn [h← (A◦M(S))∧ |h(S)− h(D)| ≥ α] ≤ β + δ +2exp(ε) β + δ 2(1 + 2ε) = 2(β + δ).\nThus M is (α,2(β + δ))-robustly generalizing for our specified value of α.\nOur next result, Lemma 5.2 shows that there exist robustly generalizing mechanisms that are neither differentially private nor perfectly generalizing for any reasonable parameters. Lemma 5.2. For any γ > 0 and n ∈N, there exists a mechanismM : X n → {0,1} that is ( √ ln(2/γ )/2n,γ )- robustly generalizing, but is not (ε,δ)-differentially private for any bounded ε and δ < 1, and is not (β,ε′,δ′)-perfectly generalizing for any β < 1/2− 1/√n, bounded ε′, and δ′ < 1/2. Proof. Consider the domain X = {0,1}, and the following deterministic mechanism M : X n → {0,1}: given a sample S , output 1 if more than ⌊n/2⌋ of the elements in S is 1, and output 0 otherwise. Note M has a small output space, so by Theorem 3.9, M is ( √ ln(2/γ )/2n,γ )-robustly generalizing for any γ > 0. Consider two neighboring samples S1 and S2 such that S1 has ⌊n/2⌋+ 1 number of 1’s, and S2 has ⌊n/2⌋ number of 1’s. Then Pr[M(S1) = 1] = 1 and Pr[M(S2) = 1] = 0. Therefore, the mechanism is not (ε,δ)-differentially private for any bounded ε and δ < 1.\nTo show that M is not perfectly generalizing, consider the distribution D that is uniform over X = {0,1}. That is, Prx∼D[x = 1] = Prx∼D[x = 0] = 1/2. Suppose that M is (β,ε′,δ′)-perfectly generalizing with β < 1/2 − 1/√n. In particular, this implies that β < 1/2 − (\nn ⌊n/2⌋) 2n . Let Sim be the\nassociated simulator, and let p = Pr[Sim(D) = 1]. Since each the events of (M(S) = 0) and (M(S) = 1) will occur with probability (over the randomdraws of S) greater than β, then there exists samples S1 and S2 such that bothM(S1),M(S2) ≈ε′,δ′ Sim(D), and furthermore M(S1) = 1 and M(S2) = 0 deterministically. This means, p ≤ exp(ε′) ·Pr[M(S2) = 1] + δ′ = δ′ and, (1− p) ≤ exp(ε′) ·Pr[M(S1) = 0] + δ′ = δ′ . It follows from above that δ′ ≥ 1/2. Thus, M is not (β,ε′ ,δ′) for any β < 1/2 − 1/√n, bounded ε′, and δ′ < 1/2."
    }, {
      "heading" : "5.2 Perfect Generalization and Differential Privacy",
      "text" : "We now focus on the relationship between differential privacy and perfect generalization to show that perfect generalization is a strictly stronger definition in the sense that problems that can be solved subject to perfect generalization can also be solved subject to differential privacy with little loss in the parameters, whereas in the reverse direction, parameters necessarily degrade. Recall that we have already shown that any perfectly generalizing mechanism can be “compiled” into a differentially private mechanism with only a constant factor loss in parameters (Theorem 4.5). We here note however that this compilation is necessary – that perfectly generalizing algorithms are not necessarily themselves differentially private. In the reverse direction, we show that every differentially private algorithm is strongly generalizing, with some necessary degradation in the generalization parameters.\nWe first give an example of a perfectly generalizing mechanism that does not satisfy differential privacy for any reasonable parameters. The intuition behind this result is that perfect generalization requires a mechanism to behave similarly only on a (1− β)-fraction of samples, while differential privacy requires a mechanism to behave similarly on all neighboring samples. The mechanism of Theorem 5.3 exploits this difference to find a pair of unlikely neighboring samples which are treated very differently.\nTheorem 5.3. For any β > 0 and any n ≥ log(1/β), there exists a mechanism M : X n →R which is (β,0,0)-perfectly generalizing but is not (ε,δ)-differentially private for any bounded ε and δ < 1.\nProof. Consider the domain X = {0,1} and the following simple mechanism M: given a sample S = {s1, . . . , sn} of size n, it will output “Strange” if the sample S satisfies:\ns1 = s2 = . . . = s⌊n/2⌋ = 1 and, s⌊n/2⌋+1 = s⌊n/2⌋+2 = . . . = sn = 0,\nand output “Normal” otherwise. We first show thatM is ((1/2)n,0,0)-perfectly generalizing. Consider the following deterministic simulator Sim that simply outputs “Normal” no matter what the input distribution over the domain is.\nSuppose that the distribution D over the domain satisfies Prx∼D[x = 1] = p for some p ∈ [0,1]. Note that the probability (over the random draws of S) of outputting “Strange” is\nPr[M(S) = “Strange”] = p⌊n/2⌋(1− p)⌈n/2⌉ =≤ (1/2)n.\nThis means, with probability at least 1− (1/2)n over the random draws of S , M will output “Normal,” and also\nPr[M(S) = “Normal”]\nPr[Sim(D) = “Normal”] = 1 ≤ exp(0).\nTherefore,M is ((1/2)n,0,0)-perfectly generalizing. Now consider the sample T = {t1, . . . , tn} such that\nt1 = t2 = . . . = t⌊n/2⌋ = 1 and, t⌊n/2⌋+1 = t⌊n/2⌋+2 = . . . = tn = 0.\nLet T ′ be any neighboring sample of T such that |T∆T ′ | = 1. We know thatM(T ′) = “Normal”, so, Pr[M(T ′) = “Normal”] Pr[M(T ) = “Normal”] = 1 0 =∞.\nTherefore, the mechanismM is not (ε,δ)-differentially private for any bounded ε and δ < 1.\nNow we show the other direction of the relationship between these two definitions: any differentially private mechanism is also perfectly generalizing. We begin with Theorem 5.4, which proves that every (ǫ,0)-differentially private mechanism is also (β,O( √ n ln(1/β)ε),0)-perfectly generalizing. We will later show that this dependence on n and β is tight.\nTheorem 5.4. Let M : X n → R be an (ε,0)-differentially private mechanism, where R is an arbitrary finite range. Then M is also (β, √ 2n ln(2|R|/β)ε,0)-perfectly generalizing.\nProof. Given an (ε,0)-differentially private mechanism M, consider the following log-likelihood function q : X n ×R→R, such that for any sample S ∈ X n and outcome r ∈ R, we have\nq(S,r) def = log(Pr[M(S) = r]) .\nSince M is (ε,0)-differentially private, we know that for all neighboring S,S ′ ∈ X n, the function q satisfies,\nmax r∈R\n∣∣∣q(S,r)− q(S ′ , r) ∣∣∣ =max\nr∈R ∣∣∣∣∣∣ln ( Pr[M(S) = r] Pr[M(S ′) = r] )∣∣∣∣∣∣ ≤ ε.\nFor any distribution D ∈ ∆X , the sample S = (s1, . . . , sn) ∼i.i.d. Dn is now a random variable, rather than a fixed input. By an application of McDiarmid’s inequality to the variables s1, . . . sn, we have that for any r ∈ R,\nPr S∼i.i.d.Dn\n[∣∣∣∣q(S,r)−E [q(S,r)] ∣∣∣∣ ≥ t ] ≤ 2exp (−2t2 nε2 ) . (9)\nInstantiating Equation (9) with t = ε √ (n/2) ln(2|R|/β) and taking a union bound, we have that with probability at least 1− β, it holds for all r ∈ R that, ∣∣∣∣q(S,r)−E [q(S,r)] ∣∣∣∣ ≤ ε √ (n/2) ln(2|R|/β). (10)\nWe condition on this event for the remainder of the proof. Define the simulator Sim(D) for mechanismM on distributionD as follow for all r ∈ R, output the r with probability proportional to exp ( ES∼i.i.d.Dn [q(S,r)] ) . Let Z = ∑ r∈R exp ( ES∼i.i.d.Dn [q(S,r)] ) be the normalization factor, and by Equation (10),\nexp ( ε √ (n/2) ln(2|R|/β) ) ≤ Z ≤ exp ( ε √ (n/2) ln(2|R|/β) )\nFor any r ∈ R, Pr[M(S) = r] Pr[Sim(D) = r] =\nexp(q(S,r))\nexp ( ES ′∼i.i.d.Dn[q(S ′ , r)] ) /Z\n= exp ( q(S,r)− E\nS ′∼i.i.d.Dn [q(S ′ , r)]\n) ·Z\n≤ exp ( ε √ 2n ln(2|R|/β) ) ,\nwhere the last inequality is due to Equation (10). For any O ⊆R and for ε′ = ε √ 2n ln(2|R|/β),\nPr[M(S) ∈ O] = ∑\nr∈O Pr[M(S) = r]\n≤ ∑\nr∈O eε\n′ Pr[Sim(D) = r]\n= eε ′ Pr[Sim(D) ∈ O].\nBy symmetry, Pr[Sim(D) ∈ O] Pr[M(S) ∈ O] ≤ exp ( ε √ 2n ln(2|R|/β) ) .\nThus for any distributionD ∈ ∆X , with probability at least 1− β over the choice of S ∼i.i.d. Dn, we have that M(S) ≈ε′ ,0 Sim(D), for ε′ = ε √ 2n ln(2|R|/β), so M is (β,ε √ 2n ln(2|R|/β),0)-perfectly generalizing.\nThe following result proves that the degradation of ε in Theorem 5.4 is necessary, and the dependence on n and β is asymptotically tight.\nTheorem 5.5. For any ε > 0, β ∈ (0,1) and n ∈ N, there exists a mechanism M : X n → R that is (ε,0)-differentially private, but not (β,ε′ ,0)-perfectly generalizing for any ε′ = o(ε √ n ln(1/β)).\nProof. Consider the domain X = {0,1} and the distribution D over X such that Prx∼D[x = 1] = Prx∼D[x = 0] = 1/2. Consider followingmechanismM : X n → {0,1}Given a sample S = {s1, . . . , sn} ∈ X n,M will do the following:\n1. first compute the sample average s = 1n ∑n i=1 si ;\n2. then compute a noisy estimate ŝ = s+Lap( 1nε ) by adding Laplace noise with parameter 1/nε;\n3. if ŝ ≤ 1/2, output 0; otherwise, output 1.\nIn words, the mechanism tries to identify the majority in the sample based on the noisy estimate ŝ. Note that the average value s is (1/n)-sensitive statistic — that is, changing a single sample point si in S will change the value of s by at most 1/n. Also observe that M is the Laplace mechanism of [DMNS06] composed with a (data independent) post-processing step, so we know M is (ε,0)differentially private.\nNow suppose that M is (β,ε′ ,0,n)-strongly generalizing for some β ∈ (0,1). Using a standard tail bound for Binomial distribution, we know that for any S ∼i.i.d. Dn and k ≤ 1/8, the sample average s satisfies\nPr[s ≤ n/2− k] = Pr[s ≥ n/2+ k] ≥ 1 15\nexp ( −16nk2 ) .\nIn other words, for any γ ∈ (0,1), we have both Pr[s ≤ 1/2−K] ≥ γ and Pr[s ≥ 1/2+K] ≥ γ , where K = √ ln(1/(15γ ))\n4 √ n\n. For the remainder of the proof, we will set γ = 2 √ β.\nLet S1,S2 ∼i.i.d. Dn be two random samples with sample averages s1 and s2. By Corollary 2.10, we know that Pr[M(S1) 02ε′,0 M(S2)] ≤ 2β. Since γ2 > 2β, it follows that with strictly positive probability over the random draws over S1 and S2, all of the events that s1 ≤ n/2−K , s2 ≥ n/2+K , and M(S1) ≈2ε′,0 M(S2) occur simultaneously. For the remainder of the proof, we condition on samples S1 and S2 satisfying these conditions, which will happen with probability greater than 2β.\nIf we apply our mechanism M to both samples, we will first obtain noisy estimates ŝ1 and ŝ2, and by the property of the Laplace distribution, we know for any t > 0\nPr[|ŝ1 − s1| < K] = 1− exp(−Knε) and Pr[|ŝ2 − s2| < K] = 1− exp(−Knε)\nNote that the event |ŝ1 − s1| < K implies that M(S1) = 0, and the event |ŝ2 − s2| < K implies that M(S2) = 1. The condition of M(S1) ≈2ε′,0 M(S2) implies that\nexp(2ε′) ≥ Pr[M(S1) = 0] Pr[M(S2) = 0] = Pr[M(S1) = 0] 1−Pr[M(S2) = 1] ≥ 1− exp(−Knε) exp(−Knε) = exp(Knε)− 1\nIt follows that we must have\nε′ ≥ 1 2 (Knε − 1) =Ω\n( ε √ n ln(1/β) ) ,\nwhich recovers the stated bound.\nTheorems 5.4 and Theorem 5.5 only show a relationship between (ε,0)-differential privacy and strong generalization. To show such a relationship when δ > 0, we appeal to group privacy, first studied by [DKM+06], which says that if M is (ε,δ)-differentially private and two samples S,S ′ differ on k entries, then M(S) ≈kε,ke(k−1)εδ M(S ′). Using simulator SimD =M(S∗) for any fixed sample S∗ ∼i.i.d. Dn and by the fact that any sample S can differ from S∗ in an most n samples, we see that M is (0,nε,ne(n−1)εδ)-perfectly generalizing.\nUnfortunately, this blowup in parameters is generally unacceptable for most tasks. We\nsuspect that the necessary blowup in the ε parameter is closer to Θ (√ n ln(1/β) ) as with (ε,0)differential privacy, but leave a formal proof as an open question for future work. On the positive side, most known (ε,δ)-differentially privatemechanisms are designed by composing several (ε′ ,0)-differentially private mechanisms, where the δ > 0 is an artifact of the composition (see e.g. Theorem 3.20 of [DR14] for more details). Since perfect generalization also composes, we can show that such mechanisms are perfectly generalizing by first seeing that each of the (ε′ ,0)-differentially private mechanisms is perfectly generalizing by Theorem 5.4, and then composing these building blocks as in the original mechanism using Theorem 2.13. This will give better generalization parameters than a direct reduction via group privacy."
    }, {
      "heading" : "6 Conjectures and Open Problems",
      "text" : "We have proven in Theorem 3.14 that any PAC learnable hypothesis class (i.e. any hypothesis class with polynomial VC-dimension) is learnable with a guarantee of robust generalization, albeit with a possible exponential blowup in sample complexity. This connection relies on the recent work of [MY15] that shows that any VC-class of dimension d has a compression scheme of size exponential in d. We note that a corresponding result is not known for differentially private learners – in fact there is a hypothesis class of VC-dimension 1 (one dimensional threshold functions) for which there is no known private learner.\nWe conjecture something much stronger – that every PAC learnable problem is PAC learnable with the guarantee of robust generalization, with at most a constant blowup in sample complexity. Our conjecture is implied by Warmuth’s conjecture [War03] that every VC-class of dimension d has a compression scheme of size d. There may, however, also be other avenues towards proving our conjecture. Since compression schemes also satisfy adaptive composition guarantees, our conjecture would imply that it is possible to do arbitrary adaptive learning without resorting to approximation (as is necessary with differential privacy, and often to obtain bounded description outputs), and with little overhead in sample complexity.\nOn the technical side, we have proven in Theorem 5.4 that any (ε,0)-differentially private mechanism is also (approximately) (β, √ nε,0)-perfectly generalizing. We do not have a similarly tight qualitative reduction for (ε,δ)-differentially private mechanisms, but we suspect a result like Theorem 5.4 holds for this class of mechanisms as well."
    }, {
      "heading" : "A Missing Proofs in Section 2",
      "text" : "Proof of Lemma 2.9. In the following, we will use (a∧ b) to denote min{a,b}. For all O ⊆R,\nPr y∼D1 [y ∈ O] ≤ (exp(ε) Pr y∼D2 [y ∈ O] + δ)∧ 1\n≤ (exp(ε) Pr y∼D2 [y ∈ O])∧ 1+ δ ≤ exp(ε) ( exp(ε′) Pr\ny∼D3 [y ∈ O] + δ′\n) + δ\n= exp(ε + ε′) Pr y∼D3 [y ∈ O] + 2δ′ + δ.\nA similar argument gives Pry∼D3 [y ∈ O] ≤ exp(ε + ε′)Pry∼D1 [y ∈ O] + 2δ + δ′.\nProof of Corollary 2.10. By a union bound, with probability 1−2β over the draws of T1,T2 ∼i.i.d. Cn,\nM(T1) ≈ε,δ SimC ≈ε,δ M(T2).\nThe result then follows from Lemma 2.9.\nProof of Lemma 2.11. The result for robustly generalizing mechanisms follows immediately from the definition: Assume by way of contradiction that there exists an (α,β)-robustly generalizing mechanism M : Yn → R and a post-processing procedure A : R → R′ such that A ◦M is not (α,β)-robustly generalizing. Then there exists an adversary A′ such that A′(A(M(T ))) outputs a hypothesis h that violates the robust generalization condition. However, this would imply thatA′◦ A is an adversary that violates the robust generalization condition, contradicting the assumption that M is (α,β)-robustly generalizing.\nLet M : Yn → R be (β,ε,δ)-perfectly generalizing, and let A : R → R′ be a post-processing procedure. Fix any distribution C, and let SimC denote the simulator such that M(T ) ≈ε,δ SimC with probability 1− β when T ∼i.i.d. Cn. We will show that with probability at least 1− β over the sample T ∼i.i.d. Cn,\nA(M(T )) ≈ε,δ A(SimC). First note that any randomized mapping can be decomposed into a convex combination of\ndeterministic mappings. Let\nA = ∑\ni=1\nγiAi s.t. ∑\ni=1\nγi = 1 and 0 < γi ≤ 1 ∀i,\nwhere each Ai : R → R′ is deterministic. For the remainder of the proof, we will assume that M(T ) ≈ε,δ SimC , which will be the case with probability 1− β.\nFix an arbitrary O′ ⊆R′ and define Oi = {r ∈ R | Ai(r) ∈ O′} for i ∈ [k].\nPr[A(M(T )) ∈ O′] = ∑\ni=1\nγi Pr[Ai(M(T )) ∈ O′]\n= ∑\ni=1\nγi Pr[M(T ) ∈ Oi]\n≤ ∑\ni=1\nγi (e εPr[SimC ∈ Oi] + δ)\n= ∑\ni=1\nγi (e εPr[Ai(SimC) ∈ O′] + δ)\n= eεPr[A(SimC) ∈ O′] + δ.\nA symmetric argument shows that\nPr[A(SimC) ∈ O′] ≤ eεPr[A(M(T )) ∈ O′] + δ.\nThus with probability at least 1−β, A(M(T )) ≈ε,δ A(SimC). The mapping A(SimC) : Yn →R′ is simply a new simulator, so A◦M is (β,ε,δ)-perfectly generalizing.\nProof of Theorem 2.12. Fix any distribution C, and for all i ∈ [k] let Simi(C) denote the simulator such that Mi(T ) ≈ε,δ Simi(C) with probability 1 − βi when T ∼i.i.d. Cn. Define Sim[k](C) = (Sim1(C), . . . ,Simk(C)). For the remainder of the proof, we will assume that Mi(T ) ≈ε,δ Simi(C) for all i ∈ [k], which will be the case with probability at least 1−∑ki=1βi over the choice of the sample.\nFix any (r1, . . . , rk) ∈ R1 × · · · ×Rk :\nPr[M[k](T ) = (r1, . . . , rk)] = k∏\ni=1\nPr[Mi(T ) = ri]\n≤ k∏\ni=1\neεi Pr[Simi(C) = ri]\n= e ∑k\ni=1 εi Pr[Sim[k](C) = (r1, . . . , rk)]\nFor any O ⊆R1 × · · · ×Rk , Pr[M[k](T ) ∈ O] = ∫\no∈O Pr[M[k](T ) = o]do ≤\n∫\no∈O e ∑k i=1 εi Pr[Sim[k](C) = o]do = e ∑k i=1 εi Pr[Sim[k](C) ∈ O].\nA symmetric argument would show that Pr[Sim[k](C) ∈ O] ≤ e ∑k\ni=1 εi Pr[M[k](T ) ∈ O]. The mapping Sim[k](C) serves as a simulator forM[k](T ), soM[k] is ( ∑k i=1βi , ∑k i=1 εi ,0)-perfectly\ngeneralizing.\nBefore proving Theorem 2.13, we present four helpful lemmas. The first says that for a pair of distributions which are (ε,δ)-close, one can construct an intermediate distribution which is (ε,0)-close to the first distribution, and (0,δ)-close to the second.\nLemma A.1. If Y,Z ∈ ∆R for some abstract rangeR and Y ≈ε,δ Z, then there exists aW ∈ ∆R such that W ≈ε,0 Z andW ≈0,δ Y . Proof. We will construct such a W from the distributions of Y and Z. For each r ∈ R, let Y (r) = Pr[Y = r] and Z(r) = Pr[Z = r]. Define sets S and T :\nS = {r | Y (r) > eεZ(r)} and T = {r | Z(r) > eεY (r)}.\nAlso define functions α,β : R→R:\nα(r) = min { eε, δ +Y (r)\nZ(r)\n} and β(r) = max { e−ε,\nY (r)− δ Z(r)\n} .\nDefine functionW ′ : R→ R as follows:\nW ′(r) =  Y (r) if r < S ∪T α(r)Z(r) if r ∈ S β(r)Z(r) if r ∈ T .\nAlthough we are not yet guaranteed that W ′(r) forms a probability distribution over R, it does satisfy our other desired properties. By construction for all r ∈ R,\nW ′(r) ≤ eεZ(r), Z(r) ≤ eεW ′(r), and |Y (r)−W ′(r)| ≤ δ. (11)\nWe will now construct a distribution W ∈ ∆R from W ′ by either decreasing the weight assigned to r ∈ S or increasing the weight assigned to r ∈ T . Note that such changes will maintain the properties of (11). Let AS = ∫ r∈S α(r)Z(r)dr and AT = ∫ r∈T β(r)Z(r)dr. Also let AW ′ = ∫ r∈RW\n′(r)dr. If AW ′ > 1, we scale down the weight of r ∈ S and define W ∈ ∆R as:\nPr[W = r] =  W ′(r) if r < S( 1− AW ′−1AS ) W ′(r) if r ∈ S , ∀r ∈ R.\nIf AW ′ < 1, we similarly scale up the weight of r ∈ T and defineW ∈ ∆R as:\nPr[W = r] =  W ′(r) if r < T( 1+ 1−AW ′AT ) W ′(r) if r ∈ T , ∀r ∈ R.\nThenW ∈ ∆R andW ≈ε,0 Z andW ≈0,δ Y .\nThe following lemma shows that a product distribution of (0,δ)-close distributions maintains the closeness property. This allows for the construction of vectors which are (0,δ′)-close.\nLemma A.2. Let Y,Z ∈ ∆R1 and Y ′ ,Z ′ ∈ ∆R2 for arbitrary ranges R1 and R2 such that Y and Y ′ are both independent from Z and Z ′ . Then if Y ≈0,δ1 Y ′ and Z ≈0,δ2 Z ′ , then (Y,Z) ≈0,δ1+δ2 (Y ′ ,Z ′).\nProof. By our independence assumption,\n(Y,Z) ≈0,δ1 (Y ′ ,Z) ≈0,δ2 (Y ′ ,Z ′).\nThen,\nmax O⊆R1×R2\n∣∣∣Pr[(Y,Z) ∈ O]−Pr[(Y ′ ,Z ′) ∈ O] ∣∣∣\n= max O⊆R1×R2\n∣∣∣Pr[(Y,Z) ∈ O]−Pr[(Y ′,Z) ∈ O] + Pr[(Y ′,Z) ∈ O]−Pr[(Y ′ ,Z ′) ∈ O] ∣∣∣\n≤ max O⊆R1×R2\n∣∣∣Pr[(Y,Z) ∈ O]−Pr[(Y ′ ,Z) ∈ O] ∣∣∣+ max\nO⊆R1×R2\n∣∣∣Pr[(Y ′,Z) ∈ O]−Pr[(Y ′,Z ′) ∈ O] ∣∣∣\n≤ δ1 + δ2\nThus (Y,Z) ≈0,δ1+δ2 (Y ′ ,Z ′).\nThe next lemma translates the worst-case guarantee of (ε,0)-closeness into an average case guarantee that is asymptotically smaller than ε. This lemma can be interpreted as bounding the KL-divergence between two distributions that are (ε,0)-close.\nLemma A.3 ([DRV10]). If Y,Z ∈ ∆R and Y ≈ε,0 Z, then,\nE y∼Y\n[ ln Pr[Y = y]\nPr[Z = y]\n] ≤ ε(eε − 1).\nThe final lemma used in the proof of Theorem 2.13 is Azuma’s Inequality, a well-known concentration inequality for random variables that are not independent of each other.\nLemma A.4 (Azuma’s Inequality). Let C1, . . . ,Ck be real-valued random variables such that for all i ∈ [k], Pr[|Ci | ≤ α] = 1, and for every (c1, . . . , ci−1) ∈ Supp(C1, . . . ,Ci−1), we have,\nE[Ci | C1 = c1, . . . ,Ci−1 = ci−1] ≤ β. Then for every z > 0,\nPr   k∑\ni=1\nCi > kβ + z √ kα   ≤ e −z2/2.\nProof of Theorem 2.13. Define the simulator forM[k] on distribution C to be Sim[k](C) = (Sim1(C), . . . ,Simk(C)), where Simi is the simulator for mechanism Mi . Let T ∼i.i.d. Cn. Because each Mi is (β,ε,δ)perfectly generalizing for all input parameters (y1, . . . ,yi−1), then with probability at least 1 − β over the choice of T , Mi(T ;y1, . . . ,yi−1) ≈ε,δ Simi(C). Taking a union bound, all k mechanisms will be (ε,δ) close to their simulators with probability at least 1 − kβ. We will assume this is the case for the remainder of the proof.\nBy Lemma A.1, there existsW = (W1, . . . ,Wk) such that each Wi satisfies,\nWi |W1=y1,...,Wi−1=yi−1 ≈0,δ Mi(T ;y1, . . . ,yi−1) and Wi |W1=y1,...,Wi−1=yi−1 ≈ε,0 Simi(C)|y1,...,yi−1 , for all i ∈ [k] and all y1, . . . ,yk . By a repeated application of Lemma A.2, M[k](T ) ≈0,kδ W . Let y = (y1, . . . ,yk) denote an element in the range of M[k], and define the set,\nB = { y | Pr[W = y] > eε′ Pr[Sim[k](C) = y] } ,\nfor ε′ = √ 2k ln(1/δ′)ε + kε(eε − 1).\nWe will show that Pr[W ∈ B] ≤ δ′, and thus for every O ⊆R, Pr[M[k](T ) ∈ O] ≤ kδ +Pr[W ∈ O]\n≤ kδ +Pr[W ∈ B] + Pr[W ∈ O \\B] ≤ kδ + δ′ + eε′ Pr[Sim[k](C) ∈ O].\nIt remains to be shown that Pr[W ∈ B] ≤ δ′. For any fixed y = (y1, . . . ,yk),\nln\n( Pr[W = y]\nPr[Sim[k](C) = y]\n)\n= ln   k∏\ni=1\nPr[Wi = yi |W1 = y1, . . . ,Wi−1 = yi−1] Pr[Simi(C) = yi | y1, . . . ,yi−1]  \n=\nk∑\ni=1\nln\n( Pr[Wi = yi |W1 = y1, . . . ,Wi−1 = yi−1]\nPr[Simi(C) = yi | y1, . . . ,yi−1]\n)\ndef =\nk∑\ni=1\nci(y1, . . . ,yi ).\nFor each i ∈ [k], we analyze the random variable ci(y1, . . . ,yi−1,Wi ) that is conditioned on previous outputs (y1, . . . ,yi−1). Since Wi |W1=y1,...,Wi−1=yi−1 ≈ε,0 Simi(C)|y1,...,yi−1 , then for any fixed yi ,∣∣∣ci(y1, . . . ,yi−1,yi )\n∣∣∣ ≤ ε. By Lemma A.3, E[ci(y1, . . . ,yi−1,Wi )] ≤ ε(eε − 1).\nFinally, we can apply Azuma’s Inequality to the random variables Ci = ci(y1, . . . ,yi−1,Wi ) with α = ε, β = ε(eε − 1) and z = √ 2ln(1/δ′) to see that,\nPr[W ∈ B] = Pr   k∑\ni=1\n> ε′   < e −z2/2 = δ′ ,\nfor our previous choice of ε′ = √ 2k ln(1/δ′)ε + kε(eε − 1).\nWe have shown that for all O ⊆R,\nPr[M[k](T ) ∈ O] ≤ kδ + δ′ + eε ′ Pr[Sim[k](C) ∈ O].\nSwapping the roles of M[k] and Sim[k] would yield a symmetric argument that,\nPr[Sim[k](C) ∈ O] ≤ kδ + δ′ + eε ′ Pr[M[k](T ) ∈ O].\nThus M[k](T ) ≈ε′ ,kδ+δ′ Sim[k](C). Recalling our failure probability of kβ,M[k] is (kβ,ε′ , kδ+δ′)-perfectly generalizing for ε′ = √ 2k ln(1/δ′)ε+ kε(eε − 1)."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "The traditional notion of generalization—i.e., learning a hypothesis whose empirical error<lb>is close to its true error—is surprisingly brittle. As has recently been noted [DFH+15b], even<lb>if several algorithms have this guarantee in isolation, the guarantee need not hold if the al-<lb>gorithms are composed adaptively. In this paper, we study three notions of generalization—<lb>increasing in strength—that are robust to postprocessing and amenable to adaptive composi-<lb>tion, and examine the relationships between them.<lb>We call the weakest such notion Robust Generalization. A second, intermediate, notion is<lb>the stability guarantee known as differential privacy. The strongest guarantee we consider we<lb>call Perfect Generalization. We prove that every hypothesis class that is PAC learnable is also<lb>PAC learnable in a robustly generalizing fashion, albeit with an exponential blowup in sam-<lb>ple complexity. We conjecture that a stronger version of this theorem also holds that avoids<lb>any blowup in sample complexity (and, in fact, it would, subject to a longstanding conjec-<lb>ture [LW86, War03]). It was previously known that differentially private algorithms satisfy<lb>robust generalization. In this paper, we show that robust generalization is a strictly weaker<lb>concept, and that there is a learning task that can be carried out subject to robust generaliza-<lb>tion guarantees, yet cannot be carried out subject to differential privacy, answering an open<lb>question of [DFH+15a]. We also show that perfect generalization is a strictly stronger guar-<lb>antee than differential privacy, but that, nevertheless, many learning tasks can be carried out<lb>subject to the guarantees of perfect generalization.",
    "creator" : "LaTeX with hyperref package"
  }
}