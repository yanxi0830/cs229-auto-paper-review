{
  "name" : "1703.05449.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Minimax Regret Bounds for Reinforcement Learning",
    "authors" : [ "Mohammad Gheshlaghi Azar", "Ian Osband" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n05 44\n9v 1\n[ st\nat .M\nL ]\n1 6\nM ar\nWe consider the problem of efficient exploration in finite horizon MDPs. We show that an optimistic modification to model-\nbased value iteration, can achieve a regret bound Õ( √ HSAT+H2S2A+H √ T )whereH is the time horizon, S the number of states, A the number of actions and T the time elapsed. This result improves over the best previous known bound Õ(HS √ AT ) achieved by the UCRL2 algorithm Jaksch et al. (2010a). The key significance of our new results is that when T ≥ H3S3A and SA ≥ H , it leads to a regret of Õ( √ HSAT ) that matches the established lower bounds of Ω( √ HSAT ) up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in S), and we use \"exploration bonuses\" based on Bernstein’s inequality, together with using a recursive -Bellman-type- Law of Total Variance (to improve scaling in H)."
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the reinforcement learning (RL) problem of an agent interacting with an environment in order to maximize its cumulative rewards through time (Burnetas and Katehakis, 1997; Sutton and Barto, 1998). We model the environment as a Markov decision process (MDP), but the agent is uncertain of the true dynamics of the MDP. As the agent interacts with the environment it observes the states, actions and rewards generated by the system dynamics. This leads to a fundamental trade off: should the agent explore poorly-understood states and actions in order to improve future performance, or exploit its existing knowledge to better optimize short-run rewards.\nThe most common approach to this learning problem is to separate the process of estimation and optimization. In this paradigm, point estimates of the unknown quantities are used in place of the unknown parameters and a plan is made with respect to these estimates. Naive optimization with respect to these point estimates can lead to premature and sub-optimal exploitation and so may never learn the optimal policy. Dithering approaches to exploration (such as ǫ-greedy) address this failing through additional random action selection. However, as this exploration is not directed the resultant algorithms may take exponentially long to learn (Kearns and Singh, 2002).\nIn order to learn efficiently it is necessary that the agent prioritizes potentially informative states and actions. To do this, it is important that the agent maintains some notion of its own uncertainty. In some sense, given any prior belief, the optimal solution to this exploration/exploitation dilemma is given by the dynamic programming in the extended Bayesian belief state (Bertsekas, 2007). However, the computational demands of this method become intractable for even small problems (Guez et al., 2013) while finite approximations can be arbitrarily poor (Munos, 2014).\nTo combat these failings, the majority of provably efficient learning algorithms employ a heuristic principle known as optimism in the face of uncertainty (OFU). In these algorithms, each state and action is afforded some “optimism” such that its imagined value is as high as statistically plausible. The agent then chooses a policy under this optimistic view of the world. This allows for efficient exploration since poorly-understood states and actions are afforded higher optimistic bonus. As the agent resolves its uncertainty, the effects of optimism will reduce and the agent’s policy will approach optimality. Almost all reinforcement learning algorithms with polynomial bounds on sample complexity employ optimism to guide exploration (Kearns and Singh, 2002; Brafman and Tennenholtz, 2002; Strehl et al., 2006; Jaksch et al., 2010a).\nAn alternative principle motivated by the Thompson sampling heuristic (Thompson, 1933) has emerged as a practical competitor to optimism. The algorithm posterior sampling for reinforcement learning (PSRL) maintains a posterior distribution for MDPs and, at each episode of interaction, follows a policy which is optimal for a single random sample (Strens, 2000).\nRecent work has established Bayesian regret bounds for PSRL and even argues for the potential benefits of sampling-based methods over existing optimistic approaches (Osband et al., 2013; Osband and Van Roy, 2016b). In this paper we incorporate several of these insights and “fire back” to defend the optimistic principle for tabular reinforcement learning with a new and improved algorithm.\nWe present a conceptually simple and computationally efficient approach to optimistic reinforcement learning in MDPs with finite episode length and without generalization. Our algorithm, upper confidence bound value iteration (UCBVI) is similar to model-based interval estimation (MBIE-EB) (Strehl and Littman, 2005) with a delicate alteration to the form of the “exploration bonus”. In particular UCBVI replaces the constant β in MBIE-EB, which scales the bonus, with the empirical variance of the value function of each state-action pair. Importantly, this algorithm addresses some of the shortcomings of existing approaches to optimistic exploration (Jaksch et al., 2010a; Dann and Brunskill, 2015) that have been highlighted relative to PSRL (Osband and Van Roy, 2016b).\nOur key contribution is to establish a high probability regret bound Õ( √ HSAT +H2S2A+H √ T ) where S is the number\nof states, A is the number of actions,H is the episode length and T is the time elapsed and where Õ ignores logarithmic terms. Importantly, for T > H3S3A and SA ≥ H this bound is Õ( √ HSAT ), which matches the lower bounds for this problem up to logarithmic factors (Jaksch et al., 2010a). This positive result is the first of its kind and helps to address an ongoing question about where the fundamental lower bounds lie for reinforcement learning in finite horizon MDPs (Bartlett and Tewari, 2009; Dann and Brunskill, 2015; Osband and Van Roy, 2016a). Our analysis contains two key insights that may be useful for future work in this area.\n1. Careful application of the Bernstein and Freedman inequalities (Bernstein, 1927; Freedman, 1975) to the concentration\nof the optimal value function directly, rather than building confidence sets for the transitions probabilities and rewards, like in UCRL2 (Jaksch et al., 2010a) and UCFH (Dann and Brunskill, 2015). 2. The use of exploration bonuses based on Bernstein’s inequality, and a recursive Bellman-type Law of Total Variance\n(LTV) to prove tight bounds on the expected sum of the variances of the value estimates, in a similar spirit to the analysis from Azar et al. (2013); Lattimore and Hutter (2012).\nAt a high level, this work addresses the noted shortcomings of existing algorithms for optimistic RL (Osband and Van Roy, 2016b) and demonstrates (contrary to previous assertions) that it is possible to design a simple and computationally efficient optimistic algorithm that does not suffer from these flaws when T is sufficiently large. We use these insights to alter the algorithm to simultaneously address both the loose scaling in S and the loose scaling inH to obtain the first regret bounds that match the Ω( √ HSAT ) lower bounds as T becomes large.\nWe should be careful to mention the current limitations of our work, each of which may provide fruitful ground for future research. First, we study the setting of episodic, finite horizon MDPs and not the more general setting of weakly communicating systems (Bartlett and Tewari, 2009; Jaksch et al., 2010a). Further, our bounds only improve over previous scalings\nÕ(HS √ AT ) for every T > H3S3A (see Jaksch et al., 2010a, for the derivations).\nWe hope that this work will serve to elucidate several of the existing shortcomings of efficient exploration in the tabular\nsetting and, hopefully, help further the direction of research towards provably optimal exploration in reinforcement learning."
    }, {
      "heading" : "2 Problem formulation",
      "text" : "In this section, we briefly review some notation, as well as some standard concepts and definitions from the theory of Markov decision processes (MDPs).\nMarkovDecision ProblemsWe consider the problem of undiscounted episodic reinforcement learning (RL) (Bertsekas and Tsitsiklis,\n1996), where an RL agent interacts with a stochastic environment and this interaction is modeled as a discrete-time MDP. An MDP is a quintuple 〈S,A, P,R,H〉, where S and A are the set of states and actions, P is the state transition distribution, The function R : S × A → ℜ is a real-valued function on the state-action space and the horizon H is the length of episode. We denote by P (·|x, a) and R(x, a) the probability distribution over the next state and the immediate reward of taking action a at state x, respectively. The agent interacts with the environment in a sequence of episodes. The interaction between the agent and environment at every episode1 k ∈ [K] is as follows: starting from xk,1 ∈ S which is chosen by the environment, the agent interacts with environment forH steps by following a sequence of actions chosen in A and observes a sequence of next-states and rewards until the end of episode. The initial state xk,1 may change arbitrary from one episode to another one. We also use the notation ‖ · ‖1 for the ℓ1 norm throughout this paper.\n1we write [n] for {i ∈ N | 1 ≤ i ≤ n}.\nAssumption 1 (MDP Regularity). We assume S andA are finite sets with cardinalities S, A, respectively. We also assume that the immediate reward R(x, a) is deterministic and belongs to the interval [0, 1].2\nIn this paper we focus on the setting where the reward function R is known, but extending our algorithm to unknown stochastic rewards poses no real difficulty.\nThe policy during an episode is expressed as a mappings π : S × [H ] → A. The value V πh : S → R denotes the value function at every step h = 1, 2, . . . , H and state x ∈ S such that V πh (x) corresponds to the expected sum of H − h rewards received under policy π, starting from xh = x ∈ S. Under assumption 1 there exists always a policy π∗ which attains the best possible values, V ∗h (x) def = supπ V π h (x) ∀x ∈ S and h ≥ 1. The function V ∗h is called the optimal value function. The policy π at every step h defines the state transition kernel P πh and the reward function r π h as P π h (y|s) def = P (y|x, π(x, h)) and rπh(x) def = R(x, π(x, h)) for all x ∈ S. For every V : S → R the right-linear operators P · and P πh · are also defined as (PV )(x, a) def = ∑ y∈SP (y|x, a)V (y) for all (x, a) ∈ S ×A and (P πh V )(x) def = ∑ y∈S P π h (y|s)V (y) for all x ∈ S, respectively.\nThe Bellman operator for the policy π, at every step h > 0, is defined, for all x ∈ S, as\n(T πh V )(x) def = rπh(x) + (P π h V )(x).\nWe also define the state-action Bellman operator as follows: ∀(x, a) ∈ S ×A,\n(T V )(x, a) def= R(x, a) + (PV )(x, a), and the optimality Bellman operator: ∀(x, a) ∈ S ×A,\n(T ∗V )(x) def= max a∈A (T V )(x, a).\nFor ease of exposition, we remove the dependence on x and (x, a), e.g., writing PV for (PV )(x, a) and V for V (x), when there is no possible confusion.\nWe measure the performance of the learner over T = KH steps by the regret Regret(K), defined as\nRegret(K) def =\nK∑\nk=1\nV ∗1 (xk,1)− V πk1 (xk,1),\nwhere πk is the control policy followed by the learner at episode k. Thus the regret measures the expected loss of following the policy produced by the learner instead of the optimal policy. So the goal of learner is to follow a sequence π1, π2, . . . , πK such that Regret(K) is as small as possible."
    }, {
      "heading" : "3 Upper confidence bound value iteration",
      "text" : "In this section we introduce the two variants of the algorithm that we investigate in this paper. We call the algorithm upper confidence bound value iteration (UCBVI). UCBVI is an extension of value iteration which guarantee than the resultant value function is a (high-probability) upper confidence bound (UCB) on the optimal value function. This algorithm is very related to the model based interval estimation (MBIE-EB) algorithms (Strehl and Littman, 2005). Our key contribution is the precise design of the upper confidence sets, and the analysis which lead to tight regret bounds.\nAlgorithm 1 UCB-VI\nInitialize dataH = ∅ for episode k = 1, 2, .. do\nQk,h = UCB− Q− values(H) for step h = 1, .., H do\nTake action ak,h = argmaxa Qk,h(xk,h, a) UpdateH = H ∪ (xk,h, ak,h, xk,h+1)\nend for\nend for\n2Our results also hold if the rewards are taken from some interval [Rmin, Rmax] instead of [0, 1], in which case the bounds scale with the factor Rmax − Rmin.\nAlgorithm 2 UCB-Q-values Require: Bonus algorithm gen_bonus, DataH Compute, for all (x,a,y)∈S×A×S, Nk(x,a,y)= ∑ (x′,a′,y′)∈H1{x′=x,a′=a,y′=y}\nNk(x,a)= ∑\ny∈SNk(x,a,y) N ′k,h(x,a)= ∑ (xi,h,ai,h,xi,h+1)∈H1{xi,h=x,ai,h=a} Let K={(x,a)∈S ×A, Nk(x,a)>0} Estimate P̂k(y|x,a)= Nk(x,a,y)Nk(x,a) for all (x,a)∈K Initialize Qk,H+1(x,a)=0 for all (x,a)∈S×A for h=H,H−1,..,1 do\nfor (x,a)∈S×A do if (x,a)∈K then\nbk,h(x,a)=bonus(P̂k(x,a),Vk,h+1,Nk,N ′ k,h)\nQk,h(x,a)=min ( Qk−1,h(x,a),H,\nR(x,a)+(P̂kVk,h+1)(x,a)+bk,h(x,a) )\nelse\nQk,h(x,a)=H end if\nend for\nend for return Q-valuesQk,h\nUCB-VI described in Algorithm 1 calls UCB-Q-values (Algorithm 2) which returns UCBs on the Q-values computed by value iteration using an empirical Bellman operator to which is added a confidence bonus bonus. We consider two variants of UCBVI depending on the structure of bonus, which we present in Algorithms 3 and 4.\nThe first of these, UCBVI_1 considers UCBVI with bonus = bonus_1. bonus_1 is a very simple bound based upon Chernoff-Hoeffding’s concentration inequality for values bounded in [0, H ]. We will see in Theorem 1 that this very simple algorithm can already achieve a regret bound of Õ(H √ SAT ), thus improving the best previously known regret bounds from\na S to a √ S dependence. The intuition for this improved S-dependence is that our algorithm (as well as our analysis) does not consider confidence sets on the transition dynamics P (y|x, a) like UCRL2 and UCFH do, but instead directly maintains confidence intervals on the optimal value function. This is crucial as, for any given (x, a), the transition dynamics are Sdimensional whereas the Q-value function is one-dimensional.\nAlgorithm 3 bonus_1\nRequire: P̂k(x, a), Nk(x, a)\nb(x, a) = 7HL √\n1 Nk(x,a) where L = log(5SAT/δ),\nreturn b\nHowever, the loose form of UCB given by UCBVI_1 does not look at the value function of the next state, and just consider it as being bounded in [0, H ]. However, much better bounds can be obtained by looking at the variance of the next state values. Our main result relies upon UCBVI with bonus = bonus_2, which we refer to as UCBVI_2 . UCBVI_2 builds upon the intuition for UCBVI_1 , but also incorporates a variance-dependent exploration bonus. This leads to tighter exploration\nbonuses and an improved regret bound of Õ( √ HSAT ).\nThe intuition for this is relatively simple. If the agent had knowledge of the true value V ∗, we could use the variance (over one transition) of the value function V ∗ in place of the loose bound ofH . Of course, as the agent learns it does not have access to the true value. Instead, in UCBVI_2 , we use a bonus based upon the empirical variance of the estimated next value, given the data gathered so far and with some additional bonus to account for the estimation error we ensure that we have an upper bound on the variance of V ∗.\nAlgorithm 4 bonus_2 Require: P̂k(x, a), Vk,h+1, Nk, N ′ k,h\nb(x,a)=\n√ 8LVarY∼P̂k(·|x,a)(Vk,h+1(Y ))\nNk(x,a) +\n14L\n3Nk(x,a) +\n√√√√8 ∑ yP̂k(y|x,a) [ min ( 652H3S2AL2\nN ′ k,h+1\n(y) ,H 2 )]\nNk(x,a) ,\nwhere L=log(5SAT/δ), return b\nAs more data is gathered, this variance estimate will converge to the variance of V ∗ and the effects of the bonus are subdominant. Now, using an iterative -Bellman-type- Law of Total Variance, we have that the sum of the next-state variances of V ∗ (overH time steps) (which is related to the sum of the exploration bonuses overH steps) is bounded by the variance of the H-steps return. Thus the size of the bonuses built by UCBVI_2 are constrained over the H steps. And we prove that the sum of those bonuses do not grow linearly in H but in √ H only. This is the key for our improved dependence fromH to √ H ."
    }, {
      "heading" : "4 Main results",
      "text" : "In this section we present the main results of the paper, which are upper bounds on the regret of UCBVI_1 and UCBVI_2 algorithms. We assume Assumption 1 holds.\nTheorem 1 (Regret bound for UCBVI_1 ). Consider a parameter δ > 0. Then the regret of UCBVI_1 is bounded w.p. at least 1− δ, by\nRegret(T ) ≤ 20HL √ SAT + 100H2S2AL2, (1)\nwhere L = log(5HSAT/δ).\nTheorem 1 is significant in that, for large T , it improves the regret dependence from S to √ S, compared to the best known bound of Jaksch et al. (2010b). The main intuition for this improved S-dependence is that we bound the estimation error of the next-state value function directly, instead of the transition probabilities. More precisely, instead of bounding the estimation error (P̂ πkk −P πk)Vk,h+1 by ‖P̂ πkk −P πk‖1‖Vk,h+1‖∞ (as is done in Jaksch et al. (2010b) for example), we bound (P̂ πkk − P πk)V ∗h+1 instead (for which a bound with no dependence on S can be achieved since V ∗ is deterministic) and handle carefully the correction term (P̂ πkk − P πk)(Vk,h+1 − V ∗h+1).\nOur second result, Theorem 2, demonstrates that we can improve upon the H-dependence by using a more refined, Bernstein-type, exploration bonus.\nTheorem 2 (Regret bound for UCBVI_2 ). Consider a parameter δ > 0. Then the regret of UCBVI_2 is bounded w.p. 1− δ, by\nRegret(T ) ≤ 28L √ HSAT + 1225H2S2AL2 + 4H √ TL, (2)\nwhere L = log(5HSAT/δ).\nThis result is particularly significant since, for T large enough (i.e., T ≥ H3S3A), our bound is Õ( √ HSAT ) which\nmatches the established lower bound Ω( √ HSAT ) of (Jaksch et al., 2010a; Osband and Van Roy, 2016a) up to a logarithmic factor.\nThe key insight is to apply concentration inequalities to bound the estimation errors and the exploration bonuses in terms of the variance of V ∗ at the next state. We then use the fact that the sum of these variances is bounded by the variance of the return (in a spirit similar to (Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012)), which proves that the\nestimation errors accumulate as √ H instead of linearly inH , thus implying the improvedH-dependence."
    }, {
      "heading" : "4.1 Computational efficiency",
      "text" : "The results of Theorems 1 and 2 provide performance guarantees on the statistical efficiency of the respective UCBVI algorithms. Importantly, both UCBVI_1 and UCBVI_2 result in computationally tractable algorithms. Each episode, both algorithms perform an optimistic value iteration at computational cost on the same order as solving a known MDP. In fact the computational cost of this algorithm is of the same order as that of standard model-based value iteration, which is of Õ(HSAmin(T, S)) at every update (Kearns and Singh, 1999). This amounts to the total computational cost of Õ(SAT min(T, S))\nas we updateK = T/H times after T steps. In fact it is possible to match the Õ-scalings of both Theorems 1 and 2 with variants of UCBVI that only selectively recompute optimistic value functions after sufficiently many extra visits to any state and action. This technique is common to the literature involves extending the period of fixed optimistic estimates as more data is gathered Jaksch et al. (2010b); Dann and Brunskill (2015). The computational cost of this variant of UCBVI then amounts to Õ(S2A2 min(T, S)) as it only needs to update the model Õ(SA) times (Jaksch et al., 2010a). We can use similar techniques to further improve the computational complexity of both UCBVI_1 and UCBVI_2 but omit these modifications for clarity in an (already complicated) analysis.\nIn comparison with UCRL2 both variants of our algorithms are up to S-times less computationally expensive than the previous state of the art , despite the improvement in regret scaling for large T . The reason for this improved computational efficiency comes from the structure of UCBVI. Since both algorithms design confidence sets upon the optimal value function directly, rather than the underlying estimates for rewards and transitions, they avoid the need for the computationally intensive extended value iteration Jaksch et al. (2010b)."
    }, {
      "heading" : "5 Proof sketch",
      "text" : "Here provide the sketch proof of our results. The full proof is deferred to the appendix."
    }, {
      "heading" : "5.1 Sketch Proof of Theorem 1",
      "text" : "LetΩ = {Vk,h ≥ V ∗h , ∀k, h} be the event under which all computed Vk,h values are upper bounds on the optimal value function. Using backward induction on h (and standard concentration inequalities) we can prove that Ω holds with high probability. To simplify notations in this sketch of proof we will not make explicit the numerical constant, denoting by a numerical constant which can vary from line to line. The exact values of these constants are provided in the full proof. We will also make use of simplified notations, such as using L to represent the logarithmic term L = log( SAT/δ).\nThe cumulative regret at time T is Regret(k) def = ∑ k V ∗(xk,1) − V πk(xk,1). Define R̃egret(k) def= ∑\nk Vk,h(xk,1) − V πk(xk,1). Under Ω we have Regret(k) ≤ R̃egret(k), so we now bound R̃egret(k). Define ∆k,h def= V ∗ − V πk and ∆̃k,h def= Vk,h − V πk . Thus\n∆k,h ≤ ∆̃k,h = P̂ πkk Vk,h+1 + bk,h − P πkV πkh+1 = (P̂ πkk − P πk)Vk,h+1 + P πk∆̃k,h+1 + bk,h.\nThe difficulty in bounding (P̂ πkk − P πk)Vk,h+1 is that both Vk,h+1 and P̂ πkk are random vectors and are not independent (the value function Vk,h+1 computed at h + 1 may depend on the samples collected from state xh,k), thus a straightforward application of Chernoff-Hoeffding (CH) inequality does not work here. In Jaksch et al. (2010b), this issue is addressed by\nbounding it as ‖P̂ πkk − P πk‖1‖Vk,h+1‖∞ at the price of an additional √ S.\nThe main contribution of our Õ(H √ SAT ) bound (which removes a √ S factor compared to the previous bound of Jaksch et al. (2010b)) is to handle this term more properly. Instead of directly bounding (P̂ πkk − P πk)Vk,h+1, we bound (P̂ πkk − P πk)V ∗h+1, using straightforward application of CH (which removes the √ S factor since V ∗h+1 is deterministic), and deal with the correction term (P̂ πkk − P πk)(Vk,h+1 − V ∗h+1). We have\n∆̃k,h = (P̂ πk k − P πk)(Vk,h+1 − V ∗h+1) + P πk∆̃k,h+1 + bk,h + ek,h,\nwhere ek,h def = (P̂ πkk − P πk)V ∗h+1(xk,h) is the estimation error of the optimal value function at the next state. Defining δ̃k,h def = ∆̃k,h(xk,h), we have\nδ̃k,h ≤ (P̂ πkk − P πk)∆k,h+1(xk,h)+δ̃k,h+1+ǫk,h+bk,h+ek,h,\nwhere ǫk,h def = P πk∆k,h+1(xk,h)−∆k,h+1(xk,h+1).\nStep 1: bound on the correction term (P̂ πkk − P πk)∆k,h+1(xk,h). Using Bernstein’s inequality (B), this term is bounded by\n∑\ny\n(P̂ πk(y|xk,h)− P πk(y|xk,h))∆k,h+1(y) (B) ≤ ∑\ny\nP πk(y|xk,h) √\nL\nP πk(y|xk,h)nk,h ∆k,h+1(y) +\nSHL nk,h ,\nwhere nk,h def = Nk(xk,h, πk(xk,h)). Now considering only the y such thatP πk(y|xk,h)nk,h ≥ H2L, and since 0 ≤ ∆k,h+1 ≤ ∆̃k,h+1, then (P̂ πk k − P πk)∆k,h+1(xk,h) is bounded by\nǭk,h +\n√ L\nP πk(xk,h+1|xk,h)nk,h δ̃k,h+1 +\nSHL nk,h ≤ǭk,h + 1 H δ̃k,h+1 + SHL nk,h ,\nwhere ǭk,h def = √ L nk,h (∑ y P πk(y|xk,h) ∆̃k,h+1(y)√ Pπk (y|xk,h) − δ̃k,h+1√ Pπk (xk,h+1|xk,h) ) .\nThe sum over the neglected y such that P πk(y|xk,h)nk,h < H2L contributes to an additional term\n∑\ny\n√ P πk(y|xk,h)nk,hL\nn2k,h ∆k,h+1(y) ≤ SH2L/nk,h.\nNeglecting this term (and the smaller order term SHL/nk,h) for now (by the pigeon-hole principle we can prove that these terms contribute to the final regret by a constant at most S2AH2L2), we have\nδ̃k,h ≤ ( 1 + 1\nH\n) δ̃k,h+1 + ǫk,h + ǭk,h + bk,h + ek,h\n≤ ( 1 + 1\nH\n)H\n︸ ︷︷ ︸ ≤e\nH−1∑\ni=h\n( ǫk,i + ǭk,i + bk,i + ck,i ) .\nThe regret is thus bounded by\nR̃egret(k) ≤ ∑\nk,h\n(ǫk,h + ǭk,h + bk,h + ek,h). (3)\nWe now bound those 4 terms. It is easy to check that ∑ k,h ǫk,h and ∑ k,h ǭk,h are sums of martingale differences, which are bounded using Azuma’s inequality, and lead to a regret of Õ(H √ T ) without dependence on the size of state and action space. The leading terms in the regret bound comes from the sum of the exploration bonuses ∑\nk,h bk,h and the estimation errors ek,h.\nStep 2: Bounding the martingales ∑ k,h ǫk,h and ∑ k,h ǭk,h. Using Azuma’s inequality we deduce\n∑\nk,h\nǫk,h (Az) ≤ H √ TL, and\n∑\nk,h\nǭk,h (Az) ≤ √ TL. (4)\nStep 3: Bounding the exploration bonuses ∑\nk,h bk,h: Using the pigeon-hole principle, we have\n∑\nk,h\nbk,h = HL ∑\nk,h\n√ 1\nnk,h\n= HL ∑\nx,a\nNT (x,a)∑\nn=1\n√ 1\nn\n≤ HL √ SAT . (5)\nStep 4: Bounding on the estimation errors ∑\nk,h ek,h. Using CH, w.h.p. we have ek,h = (P̂ πk k − P πk)V ∗h+1\n(CH)\n≤ H √ L\nnk,h . Thus this bound on the estimation errors are of the same order as the exploration bonuses (which is the reason we\nchoose those bonuses...).\nPutting everything together: Plugging (4) and (5) into (3) (and adding the smaller order term) we deduce\nRegret(k) ≤ R̃egret(k) ≤ ( HL √ SAT +H2S2AL2 ) ."
    }, {
      "heading" : "5.2 Sketch Proof of Theorem 2",
      "text" : "The proof of Theorem 1 relied on proving by a straightforward induction over h that Ω = {Vk,h ≥ V ∗h , ∀k, h} hold with high probability. In the case of exploration defined by the exploration bonuses:\nbk,h(x, a) =\n√ LVY∼P̂πk\nk (·|x,a)\n( Vk,h+1(Y ) )\nNk(x, a)︸ ︷︷ ︸ empirical Bernstein\n+ L\nNk(x, a)︸ ︷︷ ︸ empirical Bernstein\n+\n√√√√min ( H3S2AL2 ∑ y P̂k(y|x,a) N ′\nk,h+1 (y) , H\n2 )\nNk(x, a)︸ ︷︷ ︸ additional bonus\n.\nThe backward induction over h is not straightforward. Indeed, if the Vk,h+1 are upper bounds on V ∗ h+1, it is not necessarily the case that the empirical variance of Vk,h+1 are upper bound on the empirical variance of V ∗ h+1. However we can prove by (backward) induction over h that Vk,h+1 is sufficiently close to V ∗ h+1 to guarantee that the variance of those terms are sufficiently close to each other so that the additional bonus (additional term in the exploration bonus (6)) will make sure that Vk,h is still a bound on V ∗h . More precisely, define the set of indices:\n[k, h]hist def = {(i, j), s.t.(1 ≤ i ≤ k and 1 ≤ j ≤ H) or (i = k and h < j ≤ H)},\nand the event Ωk,h def = {Vi,j ≥ V ∗h , (i, j) ∈ [k, h]hist}. Our induction is the following: • Assume that Ωk,h holds. Then we prove that (Vk,h+1 − V ∗h+1)(y) ≤ H √ SAL N ′\nk,h+1 (y) .\n• Deduce that V Y ∼P̂k(·|x,a)\n( Vk,h+1(Y ) ) + H3S2AL2 ∑ y P̂k(y|x,a) N′\nk,h+1 (y)\n≥V Y∼P̂k(·|x,a)\n( V\n∗ h+1(Y )\n) , so the additional bonus com-\npensates for the possible variance difference. Thus Vk,h ≥ V ∗h and Ωk,h−1 holds.\nSo in order to prove that all values computed by the algorithm are upper bounding V ∗, we just need to prove that under Ωk,h,\nwe have (Vk,h+1 − V ∗h+1)(y) ≤ min( H1.5SL √ A N ′\nk,h+1 (y) , H), which is obtained by deriving the following regret bound on\nR̃k,h(y) def =\n∑ i≤k (Vi,h+1 − V πih+1)(xi,h+1)I{xi,h+1 = y}\n≤ HL √ SAN ′k,h+1(y), (6)\n(indeed, since {Vi,h}i is a decreasing sequence in i, we have that\n(Vk,h+1 − V ∗h+1)(y) ≤ R̃k,h+1(y)/N ′k,h+1(y)\n≤ HL √ SA/N ′k,h+1(y)).\nOnce we have proven that w.h.p., all computed values are upper bounds on V ∗ (i.e. event Ω), then we prove that under Ω, the following regret bound holds:\nRegret(k) ≤ R̃egret(k) ≤ (L √ HSAT +H2S2AL2). (7)\nThe proof of (6) relies on the same derivations as those used for proving (7). The only two difference being are that (i) T is replaced by N ′k,h+1(y), the number of times a state y was reached at time h + 1, up to episode k, and (ii) the additional √ H factor which comes from the fact that at any episode, N ′k,h+1(y) can only tick once, whereas the total number of transitions from y during any episode can be as large as H . The full proof of (6) will be given in detail in the appendix. We now give a proof sketch of (7) under Ω.\nSimilar steps used for proving Theorem 1 apply. The main difference compared to Theorem 1 is the bound on the sum of the exploration bonuses and the estimation errors (which we consider in Steps 3’ and 4’ below). This is where we can remove\nthe √ H factor. The use of the Bernstein inequality makes it possible to bound both of those terms in terms of the expected sum of variances (under the current policy πk at any episode k) of the next-state values (for that policy), and then using recursively the Law of Total Variance to conclude that this quantity is nothing else that the variance of the returns. This step is detailed now. For simplicity of the exposition of this sketch proof we neglect second order terms.\nStep 3’: Bounding the sum of exploration bonuses bk,h. We have\n∑\nk,h\nbk,h = √ L ∑\nk,h\n√ VY ∼P̂πk\nh (·|xk,h)\n( Vk,h+1(Y ) )\nnk,h ︸ ︷︷ ︸\nmain term\n+\n√√√√min ( H3S2AL2 ∑ y P̂k(y|x,a) N ′\nk,h+1 (y) , H\n2 )\nNk(x, a)︸ ︷︷ ︸ second order term\n+ ∑\nk,h\nL\nNk(x, a) ︸ ︷︷ ︸\nsecond order term\n.\nBy Cauchy-Schwarz, the main term is bounded by (∑ k,h V̂k,h+1 ∑ k,h 1\nnk,h\n)1/2 , where V̂k,h+1 def = VY∼P̂πk\nh (·|xk,h)\n( Vk,h+1(Y ) ) .\nSince ∑\nk,h 1 nk,h ≤ SA log(T ) by the pigeon-hole principle, we now focus on the term∑k,h V̂k,h+1.\nWe now prove that V̂k,h+1 is close to V πk k,h+1 def = VY∼Pπk\nh (·|xk,h)\n( V πkh+1(Y ) ) by bounding the following quantity:\nV̂k,h+1 − Vπkk,h+1 (i)\n≤ P̂ πkV 2k,h+1 − P πk(V πkh+1)2 (8) = (P̂ πk − P πk)V 2k,h︸ ︷︷ ︸\n(ak,h)\n+P πk(V 2k,h − (V πkh )2)︸ ︷︷ ︸ (a′\nk,h )\n,\nwhere (i) holds since under Ω, Vk,h+1 ≥ V ∗h+1 ≥ V πkh+1.\nStep 3’-a: bounding ∑\nk,h V̂k,h+1 − Vπkk,h+1. Using similar argument as those used in Jaksch et al. (2010b), we have that ak,h ≤ H2‖P̂ πk − P πk‖1 ≤ H2 √ SL/nk,h (where nk,h\ndef = Nk(xk,h, πk(xk,h))). Thus from the pigeon-hole principle,∑\nk,h ak,h ≤ H2S √ ATL.\nNow a′k,h is be bounded as\na′k,h ≤ 2HP πk(Vk,h − V πkh ) = 2HP πk∆̂k,h.\nThus using Azuma’s inequality,\n∑\nk,h\na′k,h (Az) ≤ 2H ∑\nk,h\nδ̂k,h+1 + H 2 √ TL\n≤ 2H2U + H2 √ TL,\nwhere U is defined an upper-bound on the pseudo regret: U def = ∑ k,h(bk,h + ek,h) + H √ T (an upper bound on the r.h.s. of (3)).\nStep 3’-b: bounding ∑\nk,h V πk k,h+1. This is the dominant term!\nFor any episode k, E[ ∑\nh V πk k,h+1|Hk] is the expected sum of variances of the value function V πk (y) at the next state\ny ∼ P πk(·|xk,h) under the true transition model for the current policy. A recursive application of the law of total variance (see e.g., Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012) shows that this quantity is nothing else than the variance of the return (sum of h rewards) under policy πk: V (∑ h r(xk,h, πk(xk,h)) ) , which is thus bounded by H2. Finally,\nusing Freedman’s (Fr) inequality to bound ∑\nk,h V πk k,h+1 by its expectation (see the exact derivation in the appendix), we deduce\n∑\nk,h\nVπkk,h+1\n(Fr) ≤ ∑\nk\nE [∑\nh\nVπkk,h+1|Hk ] + H2 √ TL\n≤ TH + H2 √ TL. (9)\nThus, using (9), (8) and the bounds on ∑ ak,h and ∑ a′k,h, we deduce that\n∑\nk,h\nbk,h ≤ L √ (TH +H2U)SA.\nStep 4’: Bounding the sum of estimation errors ∑\nk,h ek,h. We now use Bernstein inequality to bound the estimation errors\n∑\nk,h\nek,h = ∑\nk,h\n(P̂ πkk − P πk)V ∗h+1(xk,h)\n≤ √\nV∗k,h+1 nk,h + HL nk,h ,\nwhere V∗k,h+1 def = VY∼Pπk (·|xk,h) ( V ∗h+1(Y ) ) . Now, in a very similar way as in Step 3’ above, we relate V∗k,h+1 to V πk k,h+1 and\nuse the Law of total variance to bound ∑\nk,h V πk k,h+1 byHT and deduce that\n∑\nk,h\nek,h ≤ L √ (TH +H2U)SA.\nFinally we deduce from (3) that U ≤ L √ (TH +H2U)SA thus U ≤ (L √ HSAT + H2SAL2), which is the main\nterm of the regret, from which we deduce (7). So the reason we are able to remove the √ H factor from the regret bound comes from the fact that the sum, overH steps, of the variances of the next state values (which define the amplitude of the confidence intervals) is at most bounded by the variance of the return. Intuitively this means that the size of the confidence intervals do not add up linearly over H steps but grows as √ H only. Although the sequence of estimation errors are not independent over time, we are able to demonstrate a concentration of measure phenomenon that shows that those estimation errors concentrate as if they were independent."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper we present a new variant of the familiar concept of optimism in the face of uncertainty. Our key contribution is the design and analysis of the algorithm UCBVI_2 , which addresses two key shortcomings in existing algorithms for optimistic exploration in finite MDPs. First we apply a concentration to the value as a whole, rather than the transition estimates, this\nleads to a reduction from S to √ S. Next we apply a recursive law of total variance to couple estimates across an episode, rather\nthan at each time step individually, this leads to a reduction fromH to √ H .\nTheorem 2 provides the first regret bounds which, for sufficiently large T , match the lower bounds for the problem Õ( √ HSAT ) up to logarithmic factors. It remains an open problem whether for small values of T we can match the lower bound using this approach. In fact we believe that the higher order term can be improved from Õ(H2S2A) to Õ(HS2A) by a more careful analysis, i.e., a more extensive use Freedman-Bernstein inequalities. The same applies to the term of orderH √ T\nwhich can be improved to √ HT .\nThese results are particularly significant because they help to settle an ongoing debate over whether such bounds were even attainable, or whether the true lower bound should be Ω(H √ SAT ) Bartlett and Tewari (2009); Osband and Van Roy (2016a).\nMoving from this big-picture insight we have stressed in this paper to an analytically rigorous bound is non-trivial. Although we push many of the technical details to the appendix, our paper also makes several contributions in terms of analytical tools that may be useful in subsequent work."
    }, {
      "heading" : "Appendices",
      "text" : "We begin by introducing some notation in Sect. A. We then provide the full analysis of UCBVI in Sect. B."
    }, {
      "heading" : "A Notation",
      "text" : "Let denote the total number of times that we visit state x while taking action a at step h of all episodes up to episode k by N ′k,h(x, a). We also use the notation N ′ k,h(x) = ∑ a∈A N ′ k,h(x, a) for the total number of visits to state x at time step h up to episode k. Also define the empirical variance V̂k,h, the variance of optimal value function V ∗ h(x, a) the empirical variance of optimal value function V̂∗k,h(x, a) and the variance of V π h as\nV̂k,h(x, a) def = Vary∼P̂k(·|x,a)(Vk,h+1(y)),\nV∗h(x, a) def = Vary∼P (·|x,a)(V ∗ h (y)),\nV̂∗k,h(x, a) def = Vary∼P̂k(·|x,a)(V ∗ h (y)),\nVπh(x, a) def = Vary∼P (·|x,a)(V π h (y)).\nfor every (x, a) ∈ S × A and k ∈ [K] and h ∈ [H ]. We also introduce some short-hand notation: we use the lower case to denote the functions evaluated at the current state-action pair, e.g., we write nk,h for Nk(xk,h, πk(xk,h, h)) and vk,h for Vk,h(xk,h). Let also denote V ∗ k,h = V ∗ k,h(xk,h, πk(xk,h, h)) and V πk k,h = V πk k,h(xk,h, πk(xk,h, h)) for every k ∈ [K] and\nh ∈ [H ]. Also define b′i,j(x) = min ( 652S2H2AL2\nN ′ i,j+1\n(x) , H 2 ) for every x ∈ S"
    }, {
      "heading" : "A.1 “Typical” state-actions and steps",
      "text" : "In our analysis we split the episodes into 2 sets: the set of “typical” episodes in which the number of visits to the encountered state-actions are large and the rest of the episodes. We then prove a tight regret bound for the typical episodes. As the total count of other episodes is bounded this technique provides us with the desired result. The set of typical state-actions pairs for every episode k is defined as follows\n[(x, a)]k def = {(x, a) : (x, a) ∈ S ×A, Nh(x, a) ≥ H,N ′k,h(x) ≥ H}.\nBased on the definition of [(x, a)]typ and [k]typ we define the set of typical episodes and the set of typical state-dependent episodes as follow\n[k]typ def = {i : i ∈ [k], ∀h ∈ [H ], (xi,h, πi(xi,h, h)) ∈ [(x, a)]k, i ≥ 75HS2AL},\n[k]typ,x def = {i : i ∈ [k], ∀h ∈ [H ], (xi,h, πi(xi,h, h)) ∈ [(x, a)]k, N ′k,h(x) ≥ 75HS2AL}.\nAlso for every (x, a) ∈ S ×A the set of typical next states at every episode k is defined as follows:\n[y]k,x,a def = {y : y ∈ S, Nk(x, a)P (y|x, a) ≥ 2H2L}.\nAlso let denote [y]k,h = [y]k,xk,h,πk(xxk,h ) for every k ∈ [K] and h ∈ [H ]."
    }, {
      "heading" : "A.2 Surrogate regrets",
      "text" : "Our ultimate goal is to prove bound on the regret Regret(k). However in our analysis we mostly focus on bounding the surrogate regrets. Let ∆̃k,h(x) def = Vk,h(x) − V πkh (x) for every x ∈ S, h ∈ [H ] and k ∈ [K]. Then the upper-bound regret R̃egret defined as follows\nR̃egret(k) def =\nk∑\ni=1\nδ̃i,1.\nR̃egret(k) is useful in our analysis since it provides an upperbound on the true regret Regret(k). So one can bound\nR̃egret(k) as a surrogate for Regret(k). We also define the corresponding per state-step regret and upper-bound regret for every state x ∈ X and step h ∈ [H ], respectively, as follows\nRegret(k, x, h) def =\nk∑\ni=1\nI(xi,h = x)δi,h,\nR̃egret(k, x, h) def =\nk∑\ni=1\nI(xi,h = x)δ̃i,h."
    }, {
      "heading" : "A.3 Martingale difference sequences",
      "text" : "In our analysis we rely heavily on the theory of martingale sequences to prove bound on the regret incurred due to encountering a random sequence of states. We now provide some definitions and notation in that regard.\nWe define the following martingale operator for every k ∈ [K], h ∈ [H ] and F : S → ℜ. Also let t = (k − 1)H + h. denote the time stamp at step h of episode k then\nMtF def= P πkh F − F (xk,h+1).\nLet Ht be the history of all random events up to (and including) step h of episode k then we have that E(MtF |Ht) = 0. Thus MtF is a martingale difference w.r.t. Ht. Also let G be a real-value function depends on Ht+s for some integer s > 0. Then we generalize our definition of operatorMt as\nMtG def= E (G(Ht+s)| Ht)−G(Ht+s),\nwhere E is over the randomization of the sequence of states generated by the sequence of policies πk, πk+1, . . . . Here also MtG is a martingale difference w.r.t. Ht.\nLet define∆typ,k,h : S → ℜ as follows for every k ∈ [K] and h ∈ [H ] and y ∈ S:\n∆̃typ,k,h+1(y) def =\n√ Ik,h(y)\nnk,hpk,h(y) ∆̃k,h+1(y),\nwhere the function pk,h : S → [0, 1] is defined as pk,h(y) = P πkh (y|xk,h) and Ik,h(y) writes for Ik,h(y) = I(y ∈ [y]k,h) for every y ∈ X . We also define the following martingale differences which we use frequently:\nεk,h def = Mt∆̃k,h+1, ε̄k,h def = Mt∆̃typ,k,h+1."
    }, {
      "heading" : "A.4 High probability events",
      "text" : "We now introduce the high probability events E and Ωk,h under which the regret is small.\nLet use the shorthand notation L def = log\n( 5SAT\nδ\n) . Also for every v > 0, p ∈ [0, 1] and n > 0 let define the confidence\nintervals c1, c2 and c3, respectively, as follow:\nc1(v, n) def = 2\n√ vL\nn +\n14HL\n3n ,\nc2(p, n) def = 2\n√ p(1− p)L\nn +\n2L 3n ,\nc3(n) def = 2\n√ SL\nn .\nLet P be the set of all probability distributions on S. Define the following confidence set for every k = 1, . . . ,K , n > 0 and (x, a) ∈ S ×A:\nP(k, h, n, x, a, y) def= { P̃ (·|x, a) ∈ P : |(P̃ − P )V ∗h (x, a)| ≤ min ( c1 (V ∗ h(x, a), n) , c1 ( V̂∗k,h(x, a), n ))\n|P̃ (y|x, a)− P (y|x, a)| ≤ c2 (P (y|x, a), n) , ‖P̃ (·|x, a) − P (·|x, a)‖1 ≤ c3(n) } .\nWe now define the random event EP̂ as follows\nEP̂ def = { P̂k(·|x, a) ∈ P(k, h,Nk(x, a), x, a, y), ∀k ∈ [K], ∀h ∈ [H ], ∀(y, x, a) ∈ S × S ×A } .\nLet t be a positive integer. Let F = {fs}s∈[t] be a set of real-value functions on Ht+s, for some integer s > 0. We now define the following random events for every w̄ > 0 and ū > 0 and c̄ > 0:\nEaz(F , ū, c̄) def= { t∑\ns=1\nMsfs ≤ 2 √ tū2c̄ } ,\nEfr(F , w̄, ū, c̄) def= { t∑\ns=1\nMsfs ≤ 4 √ w̄c+ 14ūc̄\n3\n} .\nWe also use the short-hand notation Eaz(F , ū) and Efr(F , w̄, ū) for Eaz(F , ū, L) and Efr(F , w̄, L), respectively. Now let define the following sets of functions for every k ∈ [K] and h ∈ [H ]:\nF∆̃,k,h def = { ∆̃i,j : i ∈ [k], h < j ∈ [H − 1] } ,\nF ′ ∆̃,k,h def =\n{ ∆̃typ,i,j : i ∈ [k], h < j ∈ [H ] } ,\nF∆̃,k,h,x def = { ∆̃i,jI(xi,h = x) : i ∈ [k], h < j ∈ [H ] } ,\nF ′ ∆̃,k,h,x def =\n{ ∆̃typ,i,jI(xi,h = x) : i ∈ [k], h < j ∈ [H ] } ,\nGV,k,h def=    H∑\nj=h+1\nVπij : i ∈ [k], h < j ∈ [H ]    ,\nGV,k,h,x def=    H∑\nj=h+1\nVπij I(xi,h = x) : i ∈ [k], h < j ∈ [H ]    ,\nFb′,k,h def= { b′i,j : i ∈ [k], h < j ∈ [H − 1] } ,\nFb′,k,h,x def= { b′i,jI(xi,h = x) : i ∈ [k], h < j ∈ [H ] } .\nWe now define the high probability event E as follows\nE def= EP̂ ⋂ ⋂\nk∈[K] h∈[H] x∈S\n[ Eaz(F∆̃,k,h, H) ⋂ Eaz(F ′∆̃,k,h, 1/ √ L) ⋂ Eaz(F∆̃,k,h,x, H) ⋂ Eaz(F ′∆̃,k,x,h, 1/ √ L)\n⋂ Efr(GV,k,h, H4T,H3) ⋂ Eaz(GV,k,h,x, H5N ′k,h(x), H3) ⋂ Eaz(Fb′,k,h, H2) ⋂ Eaz(Fb′,k,h,x, H2) ] .\nThe following lemma shows that the event E holds with high probability: Lemma 1. Let δ > 0 be a real scalar. Then the event E holds w.p. at least 1− δ. Proof. To prove this result we need to show that a set of concentration inequalities with regard to the empirical model P̂k holds simultaneously. For every h ∈ [H ] the Bernstein inequality combined with a union bound argument, to take into account that Nk(x, a) ∈ [T ] is a random number, leads to the following inequality w.p. 1 − δ (see, e.g., Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012, for the statement of the Bernstein inequality and the application of the union bound in similar cases, respectively.)\n∣∣∣ [ (P − P̂k)V ∗h ] (x, a) ∣∣∣ ≤ √ 2V∗h(x, a) log ( 2T δ )\nNk(x, a) +\n2H log ( 2T δ )\n3Nk(x, a) , (10)\nwhere we rely on the fact that V ∗h is uniformly bounded by H . Using the same argument but this time with the Empirical Bernstein inequality (see, e.g., Maurer and Pontil, 2009), for Nk(x., a) > 1, leads to\n∣∣∣ [ (P − P̂k)V ∗h ] (x, a) ∣∣∣ ≤\n√ 2V̂∗k,h(x, a) log ( 2T δ )\nNk(x, a) +\n7H log ( 2T δ )\n3Nk(x, a) . (11)\nThe Bernstein inequality combined with a union bound argument on Nk(x, a) also implies the following bound w.p. 1− δ\n|Nk(y, x, a)−Nk(x, a)P (y|x, a)| ≤ √ 2Nk(x, a)Varz∼P (·|x,a)(1(z = y)) log ( 2T\nδ\n) + 2 log ( 2T δ )\n3 ,\nwhich implies the following bound w.p. 1− δ:\n∣∣∣P̂k(y|x, a)− P (y|x, a) ∣∣∣ ≤\n√ P (y|x, a)(1 − P (y|x, a)) log ( 2T δ )\nNk(x, a) +\n2 log ( 2T δ ) 3Nk(x, a) . (12)\nThe following result also holds on ℓ1-normed estimation error of the transition distribution (Weissman et al., 2003), combined with a union bound on Nk(x, a) ∈ [T ] implies w.p. 1− δ\n∥∥∥P̂k(·|x, a) − P (·|x, a) ∥∥∥ 1\n≤ √ 2S log ( 2T δ )\nNk(x, a) . (13)\nWe now focus on bounding the sequence of martingales. Let n > 0 be an integer and u, δ > 0 be some real scalars. Let the sequence of random variables {X1, X2, . . . , Xn} be a sequence of martingale differences w.r.t. to some filtration Fn. Let this sequence be uniformly bounded from above and below by u. Then the Azuma’s inequality (Cesa-Bianchi and Lugosi, 2006, see, e.g.,) implies that w.p. 1− δ\nn∑\ni=1\nXi ≤ √ 2nu log ( 1\nδ\n) . (14)\nWhen the sum of the variances ∑n\ni=1 Var(Xi|Fi) ≤ w then the following sharper bound due to Freedman (1975) holds w.p. 1− δ\nn∑\ni=1\nXi ≤ √ 2w log ( 1\nδ\n) + 2u log ( 1 δ )\n3 . (15)\nLet k ∈ [K], h ∈ [H ] and x ∈ X . then the inequality of Eq. 14 immediately implies that the following events holds w.p. 1− δ:\nEaz ( F∆̃,k,h, H, log (1/δ) ) , (16) Eaz ( F ′\n∆̃,k,h , 1/\n√ L, log (1/δ) ) , (17)\nEaz ( Fb′,k,h, H2, log (1/δ) ) . (18)\nAlso Eq. 14 combined with a union bound argument over all N ′k,h(x) ∈ [T ] (see, e.g., Bubeck et al., 2011, for the full description of the application of union bound argument in the case martigale process with random stopping time) implies that the following events hold w.p. 1− δ\nEaz ( F∆̃,k,h,x, H, log (T/δ) ) , (19) Eaz ( F ′\n∆̃,k,h,x , 1/\n√ L, log (T/δ) ) , (20)\nEaz ( Fb′,k,h,x, H2, log (T/δ) ) . (21)\nSimilarly the inequality of Eq. 15 leads to the following events w.p. 1− δ\nEfr ( GV,k,h, w̄k,h, , H3, log (T/δ) ) , (22) Efr ( GV,k,h,x, w̄k,h,x, , H3, log (1/δ) ) , (23)\nwhere w̄k,h and w̄k,h,x are upper bounds onWk,h andWk,h,x, respectively, defined as\nWk,h def =\nk∑\ni=1\nVar\n  H−1∑\nj=h\nVπi,j+1 ∣∣∣∣∣∣ Hi,1   , (24)\nWk,h,x def =\nk∑\ni=1\nI(xi,h = x)E\n  H−1∑\nj=h\nVπi,j+1 ∣∣∣∣∣∣ Hi,1   . (25)\nSo to establish a value for w̄k,h and w̄k,h,x we need to prove bound Wk,h and Wk,h,x. Here we only prove this bound for Wk,h as the proof technique to boundWk,h,x is identical to the way we boundWk,h.\nWk,h ≤ k∑\ni=1\nE\n  H−1∑\nj=h\nVπki,j+1 ∣∣∣∣∣∣ Hk   2 ≤ H3 k∑\ni=1\nE\n  H−1∑\nj=h\nVπki,j+1 ∣∣∣∣∣∣ Hk   . (26)\nNow let the sequence {x1, x2, . . . , xH} be the sequence of states encountered by following some policy π throughout an episode k. Then the recursive application of LTV leads to (see e.g., Munos and Moore, 1999; Lattimore and Hutter, 2012, for the proof.)\nE\n  H−1∑\nj=h\nVπ(xh, π(xh, h))   = Var ( H∑\nh=1\nrπ(xh) ) . (27)\nBy combining Eq. 27 into Eq. 26 we deduce\nWk,h ≤ H3 k∑\ni=1\nVar\n  H−1∑\nj=h\nrk,h ∣∣∣∣∣∣ Hk   ≤ H5k = H4Tk. (28)\nSimilarly the following bound holds onWk,h,x\nWk,h,x ≤ H5Nk,h(x). (29)\nPlugging the bounds of Eq. 28 and Eq. 29 in to the bounds of Eq. 22 and Eq. 23 and a union bound over all Nk,h(x) ∈ [T ] leads to the following bounds w.p. 1− δ:\nEfr ( GV,k,h, H4T,H3, log (1/δ) ) , (30) Efr ( GV,k,h,x, H5Nk,h(x), H3, log (T/δ) ) . (31)\nCombining the results of Eq. 10, Eq. 11, Eq. 12, Eq. 13, Eq. 16 Eq. 17 Eq. 18, Eq. 19, Eq. 20, Eq. 21, Eq. 30 and Eq. 31\nand taking a union bound over these random events as well as all possible k ∈ [K], h ∈ [H ], (s, a) ∈ S ×A proves the result."
    }, {
      "heading" : "A.4.1 UCB Events",
      "text" : "Let k ∈ [K] and h ∈ [H ]. Denote the set of steps for which the value functions are obtained before Vk,h as\n[k, h]hist = {(i, j) : i ∈ [K], j ∈ [H ], i < k ∨ (i = k ∧ j > h)}.\nLet Ωk,h = {Vi,j ≥ V ∗h , ∀(i, j) ∈ [k, h]hist} be the event under whic Vi,j prior to Vk,h computation are upper bounds on the optimal value functions. Using backward induction on h (and standard concentration inequalities) we will prove that Ωk,h holds under the event E (see Lem. 19)."
    }, {
      "heading" : "A.5 Other useful notation",
      "text" : "Here we define some other notation that we use throughout the proof. We denote the total count of steps upto episode k ∈ [K] by Tk def = H(k − 1). We first define zk,h, for every h ∈ [H ] and k ∈ [K], as follow\nc4,k,h = 4H2SAL\nnk,h .\nfor every k ∈ [K] , h ∈ [H ] and x ∈ [x] we also introduce the following notation which we use later when we sum up the regret:\nCk,h def =\nk∑\ni=1\nI(i ∈ [k]typ) H−1∑\nj=h\nc1,i,j ,\nBk,h def =\nk∑\ni=1\nI(i ∈ [k]typ) H−1∑\nj=h\nbi,j ,\nCk,h,x def =\nk∑\ni=1\nI(i ∈ [k]typ,x, xk,h = x) H−1∑\nj=h\nc1,i,j ,\nBk,h,x def =\nk∑\ni=1\nI(i ∈ [k]typ,x, xk,h = x) H−1∑\nj=h\nbi,j,\nwhere c1,k,h is the shorthand-notation for c1(v ∗ k,h, nk,h). We also define the upper bound Uk,h and Uk,h,x for every k ∈ [K] , h ∈ [H ] and x ∈ S as follows, respectively\nUk,h def = e\nk∑\ni=1\nH−1∑\nj=h\n[bi,j + c1,i,j + c4,i,j ] + (H + 1) √ TkL,\nUk,h,x def = e\nk∑\ni=1\nH−1∑\nj=h\n[bi,j + c1,i,j + c4,i,j ] + (H + 1) 3/2 √ N ′k,h(x)L,"
    }, {
      "heading" : "B Proof of the Regret Bounds",
      "text" : "Before we start the main analysis we state the following useful lemma that will be used frequently in the analysis:\nLemma 2. letX ∈ R and Y ∈ R be two random variables. Then following bound holds for their variances\nVar(X) ≤ 2 [Var(Y ) + Var(X − Y )] . Proof. The following sequence of inequalities hold\nVar(X) = E(X − Y − E(X − Y ) + Y − E(Y ))2 ≤ 2E(X − Y − E(X − Y ))2 + 2E(Y − E(Y ))2. The result follows from the definition of variance.\nWe proceed by proving the following key lemma which shows that proves bound on∆k,h under the assumption that Vk,h is UCB w.r.t. V ∗h . Lemma 3. Let k ∈ [K] and h ∈ [H ]. Let the event E and Ωk,h hold. Then the following bound holds on δk,h and δ̃k,h:\nδk,h ≤ δ̃k,h ≤ e H−1∑\ni=h\n[ εk,i + 2 √ Lε̄k,i + c1,k,i + bk,i + c4,k,i ] . (32)\nProof. For the ease of exposition we abuse the notation and drop the dependencies on k, e.g., we write x1, π and V1 for xk,1, πk and Vk,1, respectively. We proceed by bounding δ̃h under the event E at every step 0 < h < H :\nδ̃h = ThVh+1(xh)− T πh V πh+1(xh) = [P̂ πh Vh+1](xh) + bh − [P πh V πh+1](xh) = bh + [(P̂ π h − P πh )V ∗h+1](xh) + [(P̂ πh − P πh )(Vh+1 − V ∗h+1)](xh) + [P πh (Vh+1 − V πh+1)](xh)\n≤ δ̃h+1 + εh + bh + c1,h + [(P̂ πh − P πh )(Vh+1 − V ∗h+1)](xh)︸ ︷︷ ︸ (a) , (33)\nwhere the last inequality follows from the fact that under the event E we have that [(P̂ πh − P πh )V ∗h+1](xh) ≤ c1,h. We now bound (a):\n(a) = ∑\ny∈S (P̂ πh (y|xh)− P πh (y|xh))(Vh+1(y)− V ∗h+1(y))\n(I) ≤ ∑\ny∈S\n 2 √ ph(y)(1− ph(y))L\nnh +\n4L\n3nh\n ∆h+1(y)\n≤ 2 √ L ∑\ny∈S\n√ ph(y)\nnh ∆̃h+1(y)\n︸ ︷︷ ︸ (b)\n+ 4SHL\n3nh ,\nwhere (I) holds under the event E . We proceed by bounding (b):\n(b) = ∑\ny∈[y]h\n√ ph(y)\nnh ∆̃h+1(y)\n︸ ︷︷ ︸ (c)\n+ ∑\ny/∈[y]h\n√ ph(y)\nnh ∆̃h+1(y)\n︸ ︷︷ ︸ (d)\n. (34)\nThe term (c) can be bounded as follows\n(c) = ∑\ny∈[y]h\nP πh (y|xh) √\n1\nnhph(y) ∆̃h+1(y) = ε̄h +\n√ 1\nnhph(xh+1) I(xh+1 ∈ [y]h)δ̃h+1\n≤ ε̄h + √ 1\n4LH2 δ̃h+1, (35)\nwhere in the last line we rely on the definition of [y]h. We now bound (d):\n(d) = ∑\ny/∈[y]h\n√ ph(y)nh\nn2h ∆̃h+1(y) ≤\nSH √ 4LH2\nnh . (36)\nBy combining Eq. 35 and Eq. 36 into Eq. 34 we deduce\n(b) ≤ SH √ 4LH2\nnh +\n√ 1\n4LH2 δ̃h+1 + ε̄h. (37)\nBy combining Eq. 37 and Eq. 34 into Eq. B we deduce\nδ̃h ≤ εh + 2 √ Lε̄h + bh + c1,h + c4,h + ( 1 + 1\nH\n) δ̃h+1.\nLet denote γh = (1 + 1/H) h. The previous bound combined with an induction argument implies that\nδ̃h ≤ ∑H−1 i=h γi−h [ εi + 2 √ Lε̄i + c1,i + c4,i + bi ] .\nThe inequality log(1 + x) ≤ x for every x > −1 leads to γh ≤ γH ≤ e for every h ∈ [H ]. This combined with the assumption that vh ≥ v∗h under the event Ωh completes the proof.\nLemma 4. Let k ∈ [k] and h ∈ [H ]. Let the events E and Ωk,h hold. Then\nk−1∑\ni=1\nδi,h ≤ k−1∑\ni=1\nδ̃i,h ≤ e k−1∑\ni=1\nH−1∑\nj=h\n[ εi,j + 2 √ Lε̄i,j + bi,j + c1,i,j + c4,i,j ] .\nProof. The proof follows by summing up the bounds of Lem. 3 and taking into acoount the fact if Ωk,h holds then Ωi,j for all (i, j) ∈ [k, h]hist hold.\nTo simplify the bound of Lem. 4 we prove bound on sum of the martingales εk,h and ε̄k,h\nLemma 5. Let k ∈ [k] and h ∈ [H ]. Let the event E and Ωk,h hold. Then the following bound holds\nk∑\ni=1\nH−1∑\nj=h\nεi,j ≤ H √ (H − h)kL ≤ H √ TkL, (38)\nk∑\ni=1\nH−1∑\nj=h\nε̄i,j ≤ √ (H − h)k ≤ √ Tk. (39)\nAlso the following bounds holds for every x ∈ X and h ∈ H:\nk∑\ni=1\nI(xi,h = x) H−1∑\nj=h\nεi,j ≤ H √ (H − h)N ′k,h(x)L, (40)\nk∑\ni=1\nI(xi,h = x)\nH−1∑\nj=h\nε̄i,j ≤ √ (H − h)N ′k,h(x). (41)\nProof. The fact that the event E holds implies that the events Eaz(F∆̃,k,h, H), Eaz(F ′∆̃,k,h, 1√ L ) , Eaz(F∆̃,k,h,x, H) and Eaz(F ′∆̃,x,k,h, 1√ L ) hold. Under these events the inequalities of the statement hold. This combined with the fact that (H − h)k ≤ Tk completes the proof.\nWe now bound the sum of δs in terms of the upper-boundU :\nLemma 6. Let k ∈ [K] and h ∈ [H ]. Let the event E and Ωk,h holds. Then the following bounds hold for every h ∈ [H ] x ∈ S\nk∑\ni=1\nδi,h ≤ k∑\ni=1\nδ̃i,h ≤ Uk,h ≤ Uk,1,\nk∑\ni=1\nI(xi,h = x)δi,h ≤ k∑\ni=1\nI(xi,h = x)δ̃i,h ≤ Uk,h,x. ≤ Uk,1,x.\nProof. The proof follows by incorporating the result of Lem. 5 into Lem. 4 and taking into account that for every h ∈ [H ] the term Uk,h (Uk,h,x) is a summation of non-negative terms which are also contained in Uk,1 (U1,h,x).\nLemma 7. Let k ∈ [K] and h ∈ [H ]. Let the event E and Ωk,h holds. Then the following bounds hold for every x ∈ S\nk∑\ni=1\nH∑\nj=h\nδi,j ≤ k∑\ni=1\nH∑\nj=h\nδ̃i,j ≤ HUk,1,\nk∑\ni=1\nI(xi,h = x)\nH∑\nj=h\nδi,j ≤ k∑\ni=1\nI(xi,h = x)\nH∑\nj=h\nδ̃i,j ≤ HUk,1,x.\nProof. The proof follows by summing up the bounds of Lem. 6.\nWe now focus on bounding the terms Ck,h (Ck,h,x) and Bk,h (Bk,h,x) in Lem. 11 and Lem. 12, respectively. Before we proceed with the proof of Lem. 11 and Lem. 12. we prove the following key result which bounds sum of the variances of V πk,h using an LTV argument:\nLemma 8. Let k ∈ [K] and h ∈ [H ]. Then under the events E and Ωk,h the following hold for every x ∈ S\nk∑\ni=1\nH−1∑\nj=h\nVπi,j+1 ≤ TkH + 2 √ H4TkL+ 4H3L\n3 ,\nk∑\ni=1\nI(xi,h = x)\nH−1∑\nj=h\nVπi,j+1 ≤ N ′k,h(x)H2 + 2 √ H5N ′k,h(x)L+ 4H3L\n3 .\nProof. Under E the events Efr(GV,k,h, H4Tk, H3) and Efr(GV,k,h,x, H5Nk,h(x), H3) hold which then imply:\nk∑\ni=1\nH−1∑\nj=h\nVπi,j+1 ≤ k∑\ni=1\nE\n  H−1∑\nj=h\nVπi,j+1 ∣∣∣∣∣∣ Hk,h  + 2 √ H4TkL+ 4H3L 3 , (42)\nk∑\ni=1\nI(xi,h = x) H−1∑\nj=h\nVπi,j+1 ≤ k∑\ni=1\nI(xi,h = x)E\n  H−1∑\nj=h\nVπi,j+1 ∣∣∣∣∣∣ Hk,h  + 2 √ H5N ′k,hL+ 4H3L 3 . (43)\nThe LTV argument of Eq. 27 then leads to\nk∑\ni=1\nE\n  H−1∑\nj=h\nVπi,j+1 ∣∣∣∣∣∣ Hi,h   = k∑\ni=1\nVar\n  H∑\nj=h+1\nrπi,j\n  ≤ KH2 = TH (44)\nk∑\ni=1\nI(xi,h = x)E\n  H−1∑\nj=h\nVπi,j+1 ∣∣∣∣∣∣ Hi,h   = k∑\ni=1\nI(xi,h = x)Var\n  H∑\nj=h+1\nrπi,j   ≤ N ′k,h(x)H2. (45)\nEq. 42 and Eq. 43 combined with Eq. 44 and Eq. 45, respectively, complete the proof.\nLemma 9. Let k ∈ [K] and h ∈ [H ]. Then under the events E and Ωk,h the following hold for every x ∈ S\nk∑\ni=1\nH−1∑\nj=h\n( V∗i,j+1 − Vπi,j+1 ) ≤ 2H2Uk,h + 4H2 √ TkL, (46)\nk∑\ni=1\nI(xi,h = x) H−1∑\nj=h\n( V∗i,j+1 − Vπi,j+1 ) ≤ 2H2Uk,h,x + √ HN ′k,h(x, a)L. (47)\nProof. We begin by the following sequence of inequalities:\nk∑\ni=1\nH−1∑\nj=h\nV∗i,j+1 − Vπi,j+1 (I) ≤ k∑\ni=1\nH−1∑\nj=h\nEy∼pi,j [( V ∗i,j+1(y) )2 − ( V πii,j+1(y) )2]\n= k∑\ni=1\nH−1∑\nj=h\nEy∼pi,j [ (V ∗j+1(y)− V πij+1(y)(V ∗j+1(y) + V πij+1(y)) ]\n≤ 2H k∑\ni=1\nH−1∑\nj=h\nEy∼pi,j ( V ∗j+1(y)− V πij+1(y) )\n︸ ︷︷ ︸ (a)\n, (48)\nwhere (I) is obtained from the definition of the variance as well as the fact that V ∗i,j ≥ V πk,h. The last line also follows from the fact that V πk ≤ V ∗h ≤ H .\nUsing an identical argument we can also prove the following bound for state-dependent difference:\nk∑\ni=1\nI(xi,h = x)\nH−1∑\nj=h\nV∗i,j+1 − Vπi,j+1 ≤ 2H k∑\ni=1\nI(xi,h = x)\nH−1∑\nj=h\nEy∼pi,j ( V ∗j+1(y)− V πij+1(y) )\n︸ ︷︷ ︸ (b)\n, (49)\nTo bound (a) we use the fact that under the event E the event Eaz(F∆̃,k,h, H) also holds. This combined with the fact that under the event Ωk,h the inequality δk,h ≤ δ̃k,h holds implies that\n(a) ≤ k∑\ni=1\nH−1∑\nj=h\nδ̃i,j+1 + 2H √ TkL\n≤ HU1,h + 2H √ TkL, (50)\nwhere in the last line we rely on the result of Lem. 7. Similarly we can prove the following bound for (b) under the events Ωk,h and Eaz(F∆̃,k,h,x, H):\n(b) ≤ k∑\ni=1\nI(xi,h = x)\nH−1∑\nj=h\n∆̃i,j+1 + 2H 1.5 √ N ′k,h(x)L\n≤ HUk,h,x + 2H1.5 √ N ′k,h(x)L. (51)\nThe result then follows by incorporating the results of Eq. 50 and Eq. 51 into Eq. 48 and Eq. 49, respectively.\nLemma 10. Let k ∈ [K] and h ∈ [H ]. Then under the events E and Ωk,h the following hold for every x ∈ S\nk∑\ni=1\nH−1∑\nj=h\nV̂i,j+1 − Vπi,j+1 ≤ 2H2Uk,1 + 7H2S √ ATkL, (52)\nk∑\ni=1\nI(xi,h = x) H−1∑\nj=h\nV̂i,j+1 − Vπi,j+1 ≤ 2H2Uk,h,x + 7H2S √ HAN ′k,h(x)L. (53)\nProof. Here we only prove the bound on Eq. 52. The proof for the bound of Eq. 53 can be done in a very similar manner, as it is shown in the previous lemmas (the only difference is that HN ′k,h(x) and Uk,h,x replace Tk and Uk,1, respectively). The following sequence of inequalities hold:\nk∑\ni=1\nH−1∑\nj=h\nV̂i,j+1 − Vπi,j+1 (I) ≤ k∑\ni=1\nH−1∑\nj=h\nEy∼p̂i,j (Vi,j+1(y)) 2 − Ey∼pi,j ( V πij+1(y) )2\n= k∑\ni=1\nH−1∑\nj=h\nEy∼p̂i,j (Vi,j+1(y)) 2 −\nH−1∑\nj=1\nEy∼pi,j (Vi,j+1(y)) 2\n︸ ︷︷ ︸ (a)\n+ k∑\ni=1\nH−1∑\nj=h\nEy∼pi,j [ (Vi,j+1(y)) 2 − ( V πij+1(y) )2]\n︸ ︷︷ ︸ (b)\n, (54)\nwhere to prove (I) we rely on the definition of the variance as well as the assumption that Vi,j ≥ V πij due to the event Ωk,h. We now bound (a):\n(a) (I) ≤ k∑\ni=1\nH−1∑\nj=h\n2H2\n√ SL\nnk,h\n(II) ≤ 3H2S √ ATkL,\nwhere (I) holds under the event E and (II) holds due to the pigeon-hole argument (see, e.g., Jaksch et al., 2010a, for the proof).\nUsing an identical analysis to the one in Lem. 10 and taking into account that Vi,j ≥ V ∗j under the eventΩk,h and E we can bound (b)\n(b) (I) ≤ 2H   k∑\ni=1\nH−1∑\nj=h\nδ̃i,j+1 + 2H √ TkL)   ≤ 2H2 ( Uk,1 + 4 √ TkL ) ,\nwhere (I) holds since under the event E the event Eaz(F∆̃,k,h, H) holds. Combining (a) and (b) to bound Eq. 54 proves the result.\nWe now bound Ck,h and Ck,h,x:\nLemma 11. Let k ∈ [K] and h ∈ [H ]. Then under the events E and Ωk,h the following hold for every x ∈ S\nCk,h ≤ 4 √ HSATk + 4 √ H2Uk,1SAL2, (55)\nCk,h,x ≤ 4 √ H2SANk,h(x) + 4 √ H2Uk,h,xSAL2. (56)\nProof. Here we only prove the bound on Eq. 55. The proof for the bound of Eq. 56 can be done in a very similar manner, as it is shown in the previous lemmas (the only difference is that HN ′k,h(x) and Uk,h,x replace Tk and Uk,1, respectively). The Cauchy–Schwarz inequality leads to the following sequence of inequalities:\nCk,h =\nk∑\ni=1\nI(j ∈ [k]typ)\n  H−1∑\nj=h\n2\n√ V∗i,j+1L\nni,j +\n4HL\n3ni,j\n \n≤ 2 √ L √√√√√√√ k∑ i=1 H−1∑ j=h V∗i,j+1\n︸ ︷︷ ︸ (a)\n√√√√√√√ k∑ i=1 I(i ∈ [k]typ) H−1∑ j=h 1\nni,j ︸ ︷︷ ︸\n(b)\n+\nk∑\ni=1\nI(i ∈ [k]typ) H−1∑\nh=j\n4HL 3ni,j (57)\nWe now prove bounds on (a) and (b) respectively\n(a) =\nk∑\ni=1\nH−1∑\nj=h\nVπi,j+1\n︸ ︷︷ ︸ (c)\n+\nk∑\ni=1\nH−1∑\nj=h V∗i,j+1 − Vπi,j+1 ︸ ︷︷ ︸\n(d)\n. (58)\n(c) and (d) can be bounded under the event E and Ωk,h using the results of Lem. 8 and Lem.9. We then deduce\n(a) ≤ HTk + 2H2Uk,1 + 6H2 √ TkL+ 4H2L\n3\n≤ 2HTk + 2H2Uk,1, where the last line follows by the fact that for the typical episodes Tk ≥ 75H2S2AL2. Thus if Tk ≤ H2L the term Ck,h trivially equals to 0 otherwise the higher order terms are bounded by O(HTk).\nWe now bound (b) using a pigeon-hole argument\n(b) ≤ 2 ∑\n(x,a)∈S×A\nNk(x,a)∑\nn=1\n1 n ≤ 2SA\nT∑\nn=1\n1 n ≤ 2SA log(3T ).\nPlugging the bound on (a) and (b) into Eq. 57 and taking in to account that for the typical episodes [k]typ we have that T ≥ H2L completes the proof.\nWe now boundBk,h:\nLemma 12. Let k ∈ [K] and h ∈ [H ]. Let the bonus is defined according to Algo. 4. Then under the events E and Ωk,h the following hold for every x ∈ S,\nBk,h ≤ 10L √ TkHSA+ 7 √ H2SAL2Uk,1 + 390H 2S2AL2, (59)\nBk,h,x ≤ 10L √ N ′k,h(x)HSA+ 7 √ H2SAL2Uk,h,x + 390H 2S2AL2, (60)\nProof. Here we only prove the bound on Eq. 59. The proof for the bound of Eq. 60 can be done in a very similar manner, as it is shown in the previous lemmas (the only difference is that HN ′k,h(x) and Uk,h,x replace Tk and Uk,1, respectively). We first notice that the following holds:\nBk,h ≤ k∑\ni=1\nI(i ∈ [k]typ) H−1∑\nj=h\n√ 8V̂i,j+1L\nni,j ︸ ︷︷ ︸\n(a)\n+L\nk∑\ni=1\nI(i ∈ [k]typ) H−1∑\nj=h\n  √√√√ 8\nni,j\n∑ y∈S p̂i,j(y)min\n( 652S2H2AL2\nN ′i,j+1(y) , H2\n) \n︸ ︷︷ ︸ (b)\n.\nWe first note that the bound on Bk,h is similar to the bound on Ck,h. The main difference (beside the difference in H.O.Ts)\nis that here V∗h+1 is replaced by V̂i,j+1. So in our proof we first focus on dealing with this difference. The Cauchy–Schwarz inequality leads to:\n(a) ≤ √ 8L √√√√√√√ k∑ i=1 H−1∑ j=h V̂i,j+1(x, a)\n︸ ︷︷ ︸ (c)\n√√√√√√√ k∑ i=1 I(k ∈ [k]typ) H−1∑ j=h 1\nni,j ︸ ︷︷ ︸\n(d)\n,\nThe bound on (d) is identical to the corresponding bound in Lem. 11. So we only focus on bounding (c):\n(c) =\nk∑\ni=1\nH−1∑\nj=h\nVπi,j+1\n︸ ︷︷ ︸ (e)\n+\nk∑\ni=1\nH−1∑\nj=h V̂i,j+1 − Vπi,j+1 ︸ ︷︷ ︸\n(f)\n. (61)\n(e) and (f) can be bounded in high probability using the results of Lem. 8 and Lem.10. This implies\n(c) ≤ HTk + 3H2Uk,1 + 9H2S √ ATkL+ 4H2L\n3\n≤ 2HTk + 3H2Uk,1,\nwhere the last line follows by the fact that for the typical episodes Tk ≥ 100H2S2AL. Thus if Tk ≤ 100H2S2L then Bk,h trivially equals to 0 otherwise the higher order terms are bounded by O(HT ). Combining the bound on (b) and (c) leads to the following bound on (a):\n(a) ≤ 6L √ HSATk + 7HL √ SAUk,1.\nTo bound (b) we make use of Cauchy-Schwarz inequality again.\n(b) ≤ √√√√√√√ k∑ i=1 I(i ∈ [k]typ) H−1∑ j=h ∑ y∈S p̂i,j(y)b ′ i,j+1(y)\n︸ ︷︷ ︸ (g)\nk∑\ni=1\nH−1∑\nj=h\nI(i ∈ [k]typ) ni,j\n︸ ︷︷ ︸ (h)\n.\nThe term (h) bounded by 2SAL using a pigeon-hole argument (see Lem. 11). We proceed by bounding (g):\n(g) ≤ k∑\ni=1\nH−1∑\nj=h (p̂i,j − pi,j)b′i,j+1 ︸ ︷︷ ︸\n(i)\n+ k∑\ni=1\nH−1∑\nj=h\n(pi,jb ′ V − b′i,j+1(xi,j+1))\n︸ ︷︷ ︸ (j)\n+ k∑\ni=1\nI(i ∈ [k]typ) H−1∑\nj=h\nb′i,j+1(xi,j+1)\n︸ ︷︷ ︸ k\n.\nGiven that the event E holds the term (i) bounded by 2 √ 2H2S √ ALTk by using the pigeon-hole argument. Under the event\nE the event Eaz(Fb′,k,h, H2) holds. This implies that the term (j) is also bounded by 2H2 √ TkL as it is sum of the martingale differences. The term (k) is also bounded by 20000H3S3A3L3 using the pigeon-hole argument. Combining all these bounds together leads to the following bound on (b)\n(b) ≤ √ 16 √ 2H2S2 √ TkAL3 + 16H2SA √ TkL3 + 136000S4H4A2L3.\nCombining this with the bound on (a) and taking into account the fact that we only bound the Bk,h for the typical episodes, in which Tk ≥ 100H2S2AL2, completes the proof.\nLemma 13. Let the bonus is defined according to Algo. 4. Then under the events E and ΩK,1 the following hold\nRegret(K) ≤ R̃egret(K) ≤ UK,1 ≤ 14L √ HSAT + 11HL √ SAUk,1 + 490H 2S2A2L+ 2H √ TL. (62)\nProof. We first notice that Regret(K) and Regret(K) are bounded by Uk,1 due to Lem.6. To bound Uk,1 we sum up the regret due to Bk,h and Ck,h from Lem. 11 and Lem. 12. We also bound the sum ∑K\nk=1 ∑H h=1 c4,k,h by 2HSAL using a pigeon hole\nargument. We also note that Bk,h and Ck,h only account for the regret of typical episodes in which T ≥ H2S2A2L. The regret of those episodes which do not belong to the typical set [k]typ, can be bounded by O(H 2S2A2L2), trivially.\nThe following lemma establishes an explicit bound on the regret:\nLemma 14. Let the bonus is defined according to Algo. 4. Then under the events E and ΩK,1 the following hold\nRegret(K) ≤ R̃egret(K) ≤ UK,1 ≤ 28L √ HSAT + 1225H2S2AL2 + 4H √ TL. (63)\nProof. The proof follows by solving the bound of Lem. 13 in terms of Uk,1. which only contributes to the additional regret of O(H2L2SA)\nLemma 15. Let the bonus is defined according to Algo. 3. Then under the events E and ΩK,1 the following holds\nRegret(K) ≤ R̃egret(K) ≤ UK,1 ≤ 20H √ SATL+ 100H2S2AL2. (64)\nProof. The proof up to Lem. 11 is identical to the proof of Lem. 14. The main difference is to prove bound on Ck,h and Bk,h\nhere we use a loose bound ofO(H √\nSAL nk,h )for both exploration bonus bk,h and the confidence interval c1,k,h and then sum these\nterms using a pigeon-hole argument (The proof is provided in Jaksch et al., 2010a) which leads to a bound of O(H √ SATL) on both BK,1 and CK,1. Plugging these results into the bound of Lem. 7 combined with the regret of non-typical episodes complete the proof\nLemma 16. Let the bonus is defined according to Algo. 4. Let k ∈ [K] and h ∈ [H ]. Then under the events E and Ωk,h the following hold for every x ∈ S,\nRegret(k, x, h) ≤ R̃egret(k, x, h) ≤ 28HL √ SAN ′k,h(x) + 1225H 2S2AL2 + 4H1.5 √ N ′k,h(x)L\n≤ 65H1.5SL √ AN ′k,h(s).\nProof. The proof is similar to the proof of total regret. Here also we use Lem. 12, Lem. 11 and a pigeon-hole argument to bound the regrets due to Bk,h, Ck,h and c4,k,h. We then incorporate these terms into Lem.6 to bound the regret in terms of Uk,h,x. The result follows by solving the bound w.r.t. the upper bound Uk,h,x.\nLemma 17. Let the bonus b is defined according to Algo. 4. Let k ∈ [K] and h ∈ [H ]. Then under the events E and Ωk,h the following hold for every x ∈ S\nVk,h(x)− V ∗h (x) ≤ 65 √ H3S2AL2\nN ′k,h(s) .\nProof. From Lem. 16 we have that\n65H1.5SL √ AN ′k,h(s)\n≥ k∑\ni=1\nI(xi,h = x)(Vi,h(x)− V πih (x))\n≥ (Vk,h(x) − V ∗h (x)) k∑\ni=1\nI(xi,h = x) = N ′ k,h(x)(Vk,h(x) − V ∗h (x)),\nwhere the last inequality holds due to the fact that Vk,h by definition is monotonically non-increasing in k. The proof then follows by collecting terms.\nLemma 18. Let the bonus b is defined according to Algo. 3. Then under the event E the set of events {Ωk,h}k∈[K],h∈H hold.\nProof. We prove this result by induction. First we notice that for h = H by definition Vk,h = V ∗ h thus the inequality Vk,h ≥ V ∗h trivially holds. Thus to prove this result for h < H we only need to show that if the inequality Vk,h ≥ V ∗h holds for h it also holds for h− 1 for every h < H :\nVk,h(x)− V ∗h (x) = TkVk,h−1(x)− T V ∗(x) ≥ bk(x, π∗h(x)) + P̂ π ∗ k,hVk,h+1(x) − P π ∗ h Vk,h+1(x)\n= bk(x, π ∗ h(x)) + P̂ π∗ k,h(Vk,h+1 − V ∗h+1)(x) + (P̂ π ∗ k,h − P π ∗ h )V ∗ h+1(x)\n≥ bk(x, π∗h(x)) + (P̂ π ∗ k,h − P π ∗ h )V ∗ h+1(x),\nwhere the last line follows by the induction condition that Vk,h+1 ≥ V ∗h+1. The fact that the event E hols implies that (P π ∗ h − P̂ π ∗\nk,h)V ∗ h+1(x) ≤ c1(Nk(x, π∗h(x))) ≤ bk(x, π∗h(x)), which completes the proof.\nLemma 19. Let the bonus b is defined according to Algo. 4. Then under the event E the set of events {Ωk,h}k∈[K],h∈H hold.\nProof. We prove this result by induction. We first notice that in the case of the first episode V1,h = H ≥ V ∗h . To prove this result by induction in the case of 1 < k ∈ [K] we need to show that in the case of h ∈ [H − 1] if Ωk,h+1 holds then Ωk,h also holds. If Ωk,h−1 holds then Vi,j ≥ V ∗j for every (i, j) ∈ [k, h]hist. We can then invoke the result of Lem. 17 which implies\nVk,h+1(x)− V ∗h+1(x) ≤ 65 H1.5SL √ A√\nN ′k,h+1(x) .\nUsing this result which guarantees that Vk,h+1 is close to V ∗ h+1 we prove that Vk,h − V ∗h ≥ 0, that is the event Ωk,h holds.\nVk,h − V ∗h = min(Vk−1,h, Tk,hVi,j+1, H)− V ∗h\nIf Vk−1,h ≤ Tk,hVi,j+1 the result Vk,h − V ∗h = Vk−1,h − V ∗h ≥ 0 holds trivially. Also if Vk−1,h ≥ H the result trivially holds. So we only need to consider the case that Tk,hVi,j+1 ≤ Vk−1,h ≤ H in that case we have w\nVk,h(x) − V ∗h (x) ≥ Tk,hVi,j+1(x)− T V ∗h+1(x) (I)\n≥ bk,h(x, π∗(x, h)) + P̂ π ∗ h Vi,j+1(x)− P π∗h V ∗h+1(x) = bk,h(x, π ∗(x, h)) + (P̂ π ∗ h − P π ∗ h )V ∗ h+1(x) + P̂ π∗ h (Vi,j+1 − V ∗h+1)(x)\n(II)\n≥ bk,h(x, π∗(x, h)) + (P̂ π ∗ h − P π ∗ h )V ∗ h+1(x),\nwhere in (I) we rely on the fact that πk,h is the greedy policy w.r.t. Vk,h. Thus\nbk,h(x, π ∗(x, h)) + P̂ π ∗ h Vi,j+1(x) ≤ bk,h(x, πk(x, h)) + P̂ πkh Vi,j+1(x).\nAlso (II) follows from the induction assumption. Under the event E we have\nVk,h − V ∗h ≥ bk,h − c1(V̂∗h, Nk)\n≥\n√ 8V̂k,hL\nNk − 2\n√ V̂∗hL\nNk︸ ︷︷ ︸ (a)\n− 14L 3Nk\n+\n√√√√ P̂k [ 8min ( 652H3S2A2L2\nN ′ k,h+1\n, H2 )]\nNk +\n14L 3Nk .\nWe now prove a lower bound on (a):\n(a) ≥    − √ 4V̂∗k,h − 8V̂k,h Nk V̂k,h ≤ V∗,\n0 otherwise.\nWe proceed by bounding V̂∗k,h in terms of V̂k,h from above:\nV̂∗k,h (I)\n≤ 2V̂k,h + 2Vary∼P̂k(Vk,h+1(y)− V ∗ h+1(y)) ≤ 2V̂k,h + 2 P̂k(Vk,h+1 − V ∗h+1)2︸ ︷︷ ︸\n(b)\n,\nwhere (I) is an application of Lem. 2. We now bound (b). Combining this result with the result of Eq. 65 leads to the following bound on (a)\n(a) ≥    − √√√√8P̂k [ min ( 652H3S2AL2 N ′ k,h+1 , H2 )] Nk V̂k,h ≤ V̂∗,\n0 otherwise,\nwhere the last inequality holds under the event E . The proof is completed by plugging (a) and (b) into Eq. 65 which proves that Vk,h ≥ V ∗h thus the event Ωk,hholds."
    }, {
      "heading" : "B.1 Proof of Thm. 1",
      "text" : "The result is a direct consequence of Lem. 18 and Lem. 15 and the fact that the high probability event E holds w.p. 1− δ."
    }, {
      "heading" : "B.2 Proof of Thm. 2",
      "text" : "The result is a direct consequence of Lem. 19 and Lem. 14 and the fact that the high probability event E holds w.p. 1− δ."
    } ],
    "references" : [ {
      "title" : "Minimax pac bounds on the sample complexity of reinforcement learning",
      "author" : [ "M.G. Azar", "R. Munos", "H.J. Kappen" ],
      "venue" : null,
      "citeRegEx" : "Azar et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Azar et al\\.",
      "year" : 2013
    }, {
      "title" : "REGAL: A regularization based algorithm for reinforcement learning in weakly commu",
      "author" : [ "P.L. Bartlett", "A. Tewari" ],
      "venue" : null,
      "citeRegEx" : "Bartlett and Tewari,? \\Q2009\\E",
      "shortCiteRegEx" : "Bartlett and Tewari",
      "year" : 2009
    }, {
      "title" : "R-max - a general polynomial time algorithm for near-optimal reinforcement",
      "author" : [ "D.P. edition. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Neuro-Dynamic Programming",
      "citeRegEx" : "Bertsekas and Tsitsiklis,? \\Q1996\\E",
      "shortCiteRegEx" : "Bertsekas and Tsitsiklis",
      "year" : 1996
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi",
      "year" : 2012
    }, {
      "title" : "Optimal adaptive policies for markov decision processes",
      "author" : [ "A.N. 12:1587–1627. Burnetas", "M.N. Katehakis" ],
      "venue" : null,
      "citeRegEx" : "Burnetas and Katehakis,? \\Q1997\\E",
      "shortCiteRegEx" : "Burnetas and Katehakis",
      "year" : 1997
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Cesa.Bianchi and Lugosi,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi",
      "year" : 2006
    }, {
      "title" : "On tail probabilities for martingales",
      "author" : [ "D.A. TBA. Freedman" ],
      "venue" : "Information Processing Systems,",
      "citeRegEx" : "Freedman,? \\Q1975\\E",
      "shortCiteRegEx" : "Freedman",
      "year" : 1975
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "T. Jaksch", "R. Ortner", "P. Auer" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Jaksch et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jaksch et al\\.",
      "year" : 2010
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "T. Jaksch", "R. Ortner", "P. Auer" ],
      "venue" : "Learning Research,",
      "citeRegEx" : "Jaksch et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jaksch et al\\.",
      "year" : 2010
    }, {
      "title" : "Finite-sample convergence rates for Q-learning and indirect algorithms",
      "author" : [ "M. Kearns", "S. Singh" ],
      "venue" : "Learning Research,",
      "citeRegEx" : "Kearns and Singh,? \\Q1999\\E",
      "shortCiteRegEx" : "Kearns and Singh",
      "year" : 1999
    }, {
      "title" : "PAC bounds for discounted MDPs",
      "author" : [ "T. 3):209–232. Lattimore", "M. Hutter" ],
      "venue" : null,
      "citeRegEx" : "Lattimore and Hutter,? \\Q2012\\E",
      "shortCiteRegEx" : "Lattimore and Hutter",
      "year" : 2012
    }, {
      "title" : "more) efficient reinforcement learning via posterior sampling",
      "author" : [ "I. Osband", "D. Russo", "B. Van Roy" ],
      "venue" : null,
      "citeRegEx" : "Osband et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Osband et al\\.",
      "year" : 2013
    }, {
      "title" : "On lower bounds for regret in reinforcement learning. stat, 1050:9",
      "author" : [ "I. Osband", "B. Van Roy" ],
      "venue" : "Neural Information Processing Systems,",
      "citeRegEx" : "Osband and Roy,? \\Q2016\\E",
      "shortCiteRegEx" : "Osband and Roy",
      "year" : 2016
    }, {
      "title" : "PAC model-free reinforcement learning",
      "author" : [ "A.L. Strehl", "L. Li", "E. Wiewiora", "J. Langford", "M.L. Littman" ],
      "venue" : null,
      "citeRegEx" : "Strehl et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Strehl et al\\.",
      "year" : 2006
    }, {
      "title" : "A theoretical analysis of model-based interval estimation",
      "author" : [ "A.L. Strehl", "M.L. Littman" ],
      "venue" : null,
      "citeRegEx" : "Strehl and Littman,? \\Q2005\\E",
      "shortCiteRegEx" : "Strehl and Littman",
      "year" : 2005
    }, {
      "title" : "A Bayesian framework for reinforcement learning",
      "author" : [ "M.J.A. ACM. Strens" ],
      "venue" : "international conference on Machine learning,",
      "citeRegEx" : "Strens,? \\Q2000\\E",
      "shortCiteRegEx" : "Strens",
      "year" : 2000
    }, {
      "title" : "Inequalities for the l1 deviation",
      "author" : [ "T. Weissman", "E. Ordentlich", "G. Seroussi", "S. Verdu", "M.J. Weinberger" ],
      "venue" : null,
      "citeRegEx" : "Weissman et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Weissman et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "This result improves over the best previous known bound Õ(HS √ AT ) achieved by the UCRL2 algorithm Jaksch et al. (2010a). The key significance of our new results is that when T ≥ HSA and SA ≥ H , it leads to a regret of Õ( √ HSAT ) that matches the established lower bounds of Ω( √ HSAT ) up to a logarithmic factor.",
      "startOffset" : 100,
      "endOffset" : 122
    }, {
      "referenceID" : 4,
      "context" : "1 Introduction We consider the reinforcement learning (RL) problem of an agent interacting with an environment in order to maximize its cumulative rewards through time (Burnetas and Katehakis, 1997; Sutton and Barto, 1998).",
      "startOffset" : 168,
      "endOffset" : 222
    }, {
      "referenceID" : 13,
      "context" : "Almost all reinforcement learning algorithms with polynomial bounds on sample complexity employ optimism to guide exploration (Kearns and Singh, 2002; Brafman and Tennenholtz, 2002; Strehl et al., 2006; Jaksch et al., 2010a).",
      "startOffset" : 126,
      "endOffset" : 224
    }, {
      "referenceID" : 15,
      "context" : "The algorithm posterior sampling for reinforcement learning (PSRL) maintains a posterior distribution for MDPs and, at each episode of interaction, follows a policy which is optimal for a single random sample (Strens, 2000).",
      "startOffset" : 209,
      "endOffset" : 223
    }, {
      "referenceID" : 11,
      "context" : "Recent work has established Bayesian regret bounds for PSRL and even argues for the potential benefits of sampling-based methods over existing optimistic approaches (Osband et al., 2013; Osband and Van Roy, 2016b).",
      "startOffset" : 165,
      "endOffset" : 213
    }, {
      "referenceID" : 14,
      "context" : "Our algorithm, upper confidence bound value iteration (UCBVI) is similar to model-based interval estimation (MBIE-EB) (Strehl and Littman, 2005) with a delicate alteration to the form of the “exploration bonus”.",
      "startOffset" : 118,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "This positive result is the first of its kind and helps to address an ongoing question about where the fundamental lower bounds lie for reinforcement learning in finite horizon MDPs (Bartlett and Tewari, 2009; Dann and Brunskill, 2015; Osband and Van Roy, 2016a).",
      "startOffset" : 182,
      "endOffset" : 262
    }, {
      "referenceID" : 6,
      "context" : "Careful application of the Bernstein and Freedman inequalities (Bernstein, 1927; Freedman, 1975) to the concentration of the optimal value function directly, rather than building confidence sets for the transitions probabilities and rewards, like in UCRL2 (Jaksch et al.",
      "startOffset" : 63,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "First, we study the setting of episodic, finite horizon MDPs and not the more general setting of weakly communicating systems (Bartlett and Tewari, 2009; Jaksch et al., 2010a).",
      "startOffset" : 126,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "The use of exploration bonuses based on Bernstein’s inequality, and a recursive Bellman-type Law of Total Variance (LTV) to prove tight bounds on the expected sum of the variances of the value estimates, in a similar spirit to the analysis from Azar et al. (2013); Lattimore and Hutter (2012).",
      "startOffset" : 245,
      "endOffset" : 264
    }, {
      "referenceID" : 0,
      "context" : "The use of exploration bonuses based on Bernstein’s inequality, and a recursive Bellman-type Law of Total Variance (LTV) to prove tight bounds on the expected sum of the variances of the value estimates, in a similar spirit to the analysis from Azar et al. (2013); Lattimore and Hutter (2012). At a high level, this work addresses the noted shortcomings of existing algorithms for optimistic RL (Osband and Van Roy, 2016b) and demonstrates (contrary to previous assertions) that it is possible to design a simple and computationally efficient optimistic algorithm that does not suffer from these flaws when T is sufficiently large.",
      "startOffset" : 245,
      "endOffset" : 293
    }, {
      "referenceID" : 2,
      "context" : "MarkovDecision ProblemsWe consider the problem of undiscounted episodic reinforcement learning (RL) (Bertsekas and Tsitsiklis, 1996), where an RL agent interacts with a stochastic environment and this interaction is modeled as a discrete-time MDP.",
      "startOffset" : 100,
      "endOffset" : 132
    }, {
      "referenceID" : 14,
      "context" : "This algorithm is very related to the model based interval estimation (MBIE-EB) algorithms (Strehl and Littman, 2005).",
      "startOffset" : 91,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "We then use the fact that the sum of these variances is bounded by the variance of the return (in a spirit similar to (Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012)), which proves that the estimation errors accumulate as √ H instead of linearly inH , thus implying the improvedH-dependence.",
      "startOffset" : 118,
      "endOffset" : 188
    }, {
      "referenceID" : 10,
      "context" : "We then use the fact that the sum of these variances is bounded by the variance of the return (in a spirit similar to (Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012)), which proves that the estimation errors accumulate as √ H instead of linearly inH , thus implying the improvedH-dependence.",
      "startOffset" : 118,
      "endOffset" : 188
    }, {
      "referenceID" : 6,
      "context" : "Theorem 1 is significant in that, for large T , it improves the regret dependence from S to √ S, compared to the best known bound of Jaksch et al. (2010b). The main intuition for this improved S-dependence is that we bound the estimation error of the next-state value function directly, instead of the transition probabilities.",
      "startOffset" : 133,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "Theorem 1 is significant in that, for large T , it improves the regret dependence from S to √ S, compared to the best known bound of Jaksch et al. (2010b). The main intuition for this improved S-dependence is that we bound the estimation error of the next-state value function directly, instead of the transition probabilities. More precisely, instead of bounding the estimation error (P̂ πk k −P k)Vk,h+1 by ‖P̂ πk k −P k‖1‖Vk,h+1‖∞ (as is done in Jaksch et al. (2010b) for example), we bound (P̂ πk k − P k)V ∗ h+1 instead (for which a bound with no dependence on S can be achieved since V ∗ is deterministic) and handle carefully the correction term (P̂ πk k − P k)(Vk,h+1 − V ∗ h+1).",
      "startOffset" : 133,
      "endOffset" : 471
    }, {
      "referenceID" : 9,
      "context" : "In fact the computational cost of this algorithm is of the same order as that of standard model-based value iteration, which is of Õ(HSAmin(T, S)) at every update (Kearns and Singh, 1999).",
      "startOffset" : 163,
      "endOffset" : 187
    }, {
      "referenceID" : 7,
      "context" : "This technique is common to the literature involves extending the period of fixed optimistic estimates as more data is gathered Jaksch et al. (2010b); Dann and Brunskill (2015).",
      "startOffset" : 128,
      "endOffset" : 150
    }, {
      "referenceID" : 7,
      "context" : "This technique is common to the literature involves extending the period of fixed optimistic estimates as more data is gathered Jaksch et al. (2010b); Dann and Brunskill (2015). The computational cost of this variant of UCBVI then amounts to Õ(SA min(T, S)) as it only needs to update the model Õ(SA) times (Jaksch et al.",
      "startOffset" : 128,
      "endOffset" : 177
    }, {
      "referenceID" : 7,
      "context" : "This technique is common to the literature involves extending the period of fixed optimistic estimates as more data is gathered Jaksch et al. (2010b); Dann and Brunskill (2015). The computational cost of this variant of UCBVI then amounts to Õ(SA min(T, S)) as it only needs to update the model Õ(SA) times (Jaksch et al., 2010a). We can use similar techniques to further improve the computational complexity of both UCBVI_1 and UCBVI_2 but omit these modifications for clarity in an (already complicated) analysis. In comparison with UCRL2 both variants of our algorithms are up to S-times less computationally expensive than the previous state of the art , despite the improvement in regret scaling for large T . The reason for this improved computational efficiency comes from the structure of UCBVI. Since both algorithms design confidence sets upon the optimal value function directly, rather than the underlying estimates for rewards and transitions, they avoid the need for the computationally intensive extended value iteration Jaksch et al. (2010b).",
      "startOffset" : 128,
      "endOffset" : 1058
    }, {
      "referenceID" : 7,
      "context" : "In Jaksch et al. (2010b), this issue is addressed by bounding it as ‖P̂ πk k − P k‖1‖Vk,h+1‖∞ at the price of an additional √ S.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 7,
      "context" : "In Jaksch et al. (2010b), this issue is addressed by bounding it as ‖P̂ πk k − P k‖1‖Vk,h+1‖∞ at the price of an additional √ S. The main contribution of our Õ(H √ SAT ) bound (which removes a √ S factor compared to the previous bound of Jaksch et al. (2010b)) is to handle this term more properly.",
      "startOffset" : 3,
      "endOffset" : 260
    }, {
      "referenceID" : 7,
      "context" : "Using similar argument as those used in Jaksch et al. (2010b), we have that ak,h ≤ H2‖P̂ πk − P k‖1 ≤ H √ SL/nk,h (where nk,h def = Nk(xk,h, πk(xk,h))).",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "A recursive application of the law of total variance (see e.g., Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012) shows that this quantity is nothing else than the variance of the return (sum of h rewards) under policy πk: V (∑ h r(xk,h, πk(xk,h)) ) , which is thus bounded by H.",
      "startOffset" : 53,
      "endOffset" : 133
    }, {
      "referenceID" : 10,
      "context" : "A recursive application of the law of total variance (see e.g., Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012) shows that this quantity is nothing else than the variance of the return (sum of h rewards) under policy πk: V (∑ h r(xk,h, πk(xk,h)) ) , which is thus bounded by H.",
      "startOffset" : 53,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "These results are particularly significant because they help to settle an ongoing debate over whether such bounds were even attainable, or whether the true lower bound should be Ω(H √ SAT ) Bartlett and Tewari (2009); Osband and Van Roy (2016a).",
      "startOffset" : 190,
      "endOffset" : 217
    }, {
      "referenceID" : 1,
      "context" : "These results are particularly significant because they help to settle an ongoing debate over whether such bounds were even attainable, or whether the true lower bound should be Ω(H √ SAT ) Bartlett and Tewari (2009); Osband and Van Roy (2016a).",
      "startOffset" : 190,
      "endOffset" : 245
    }, {
      "referenceID" : 16,
      "context" : "The following result also holds on l1-normed estimation error of the transition distribution (Weissman et al., 2003), combined with a union bound on Nk(x, a) ∈ [T ] implies w.",
      "startOffset" : 93,
      "endOffset" : 116
    }, {
      "referenceID" : 6,
      "context" : "When the sum of the variances ∑n i=1 Var(Xi|Fi) ≤ w then the following sharper bound due to Freedman (1975) holds w.",
      "startOffset" : 92,
      "endOffset" : 108
    } ],
    "year" : 2017,
    "abstractText" : "We consider the problem of efficient exploration in finite horizon MDPs. We show that an optimistic modification to modelbased value iteration, can achieve a regret bound Õ( √ HSAT+HSA+H √ T )whereH is the time horizon, S the number of states, A the number of actions and T the time elapsed. This result improves over the best previous known bound Õ(HS √ AT ) achieved by the UCRL2 algorithm Jaksch et al. (2010a). The key significance of our new results is that when T ≥ HSA and SA ≥ H , it leads to a regret of Õ( √ HSAT ) that matches the established lower bounds of Ω( √ HSAT ) up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in S), and we use \"exploration bonuses\" based on Bernstein’s inequality, together with using a recursive -Bellman-typeLaw of Total Variance (to improve scaling in H).",
    "creator" : "LaTeX with hyperref package"
  }
}