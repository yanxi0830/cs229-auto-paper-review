{
  "name" : "1505.05114.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems",
    "authors" : [ "Yuxin Chen", "Emmanuel J. Candès" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : ""
    }, {
      "heading" : "1.1 Problem formulation",
      "text" : "Imagine we are given a set of m quadratic equations taking the form\nyi = |〈ai,x〉|2 , i = 1, · · · ,m, (1)\nwhere the data y = [yi]1≤i≤m and design vectors ai ∈ Rn/Cn are known whereas x ∈ Rn/Cn is unknown. Having information about |〈ai,x〉|2—or, equivalently, |〈ai,x〉|—means that we a priori know nothing about the phases or signs of the linear products 〈ai,x〉. The problem is this: can we hope to identify a solution, if any, compatible with this nonlinear system of equations?\nThis problem is combinatorial in nature as one can alternatively pose it as recovering the missing signs of 〈ai,x〉 from magnitude-only observations. As is well known, many classical combinatorial problems with Boolean variables may be cast as special instances of (1). As an example, consider the NP-complete stone problem in which we have n stones each of weight wi > 0 (1 ≤ i ≤ n), which we would like to divide into two groups of equal sum weight. Letting xi ∈ {−1, 1} indicate which of the two groups the ith stone belongs to, one can formulate this problem as solving the quadratic system{\nx2i = 1, i = 1, · · · , n, (w1x1 + · · ·+ wnxn)2 = 0.\n(2)\n∗Department of Statistics, Stanford University, Stanford, CA 94305, U.S.A. †Department of Mathematics, Stanford University, Stanford, CA 94305, U.S.A.\nar X\niv :1\n50 5.\n05 11\n4v 1\n[ cs\n.I T\n] 1\n9 M\nay 2\n01 5\nHowever simple this formulation may seem, even checking whether a solution to (2) exists or not is known to be NP complete [7].\nMoving from combinatorial optimization to the physical sciences, one application of paramount importance is the phase retrieval [20, 21] problem, which permeates through a wide spectrum of techniques including X-ray crystallography, diffraction imaging, and microscopy. In a nutshell, the problem of phase retrieval arises due to the physical limitation of optical sensors, which are often only able to record the intensities of the diffracted waves scattered by an object under study. Notably, upon illuminating an object x, the diffraction pattern is of the form of Ax; however, it is only possible to obtain intensity measurements y = |Ax|2 leading to the quadratic system (1).1 In the Fraunhofer regime where data is collected in the far-field zone, A is given by the spatial Fourier transform. We refer to [37] for in-depth reviews of this subject.\nContinuing this motivating line of thought, in any real-world application recorded intensities are always corrupted by at least a small amount of noise so that observed data are only about |〈ai,x〉|2; i.e.\nyi ≈ |〈ai,x〉|2 , i = 1, · · · ,m. (3)\nAlthough we present results for arbitrary noise distributions—even for non-stochastic noise—we shall pay a particular attention to the Poisson data model, which assumes\nyi ind.∼ Poisson ( |〈ai,x〉|2 ) , i = 1, · · · ,m. (4)\nThe reason why this statistical model is of special interest is that it naturally describes the variation in the number of photons detected by an optical sensor in various imaging applications."
    }, {
      "heading" : "1.2 Nonconvex optimization",
      "text" : "Under a stochastic noise model with independent samples, a first impulse for solving (3) is to seek the maximum likelihood estimate (MLE), namely,\nminimizez − ∑m\ni=1 ` (z; yi) , (5)\nwhere ` (z; yi) denotes the log-likelihood of a candidate solution z given the outcome yi. For instance, under the Poisson noise model (4) one can write\n`(z; yi) = yi log(|a∗i z|2)− |a∗i z|2 (6)\nmodulo some constant offset. Unfortunately, the log-likelihood is usually not concave, thus making the problem of finding an MLE NP complete in general.\nTo alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45]. The basic idea is to introduce a rank-one matrix X = xx∗ to linearize the quadratic constraints, and then relax the rank-one constraint. Suppose we have Poisson data, then this strategy converts the problem into a convenient convex program:\nminimizeX ∑m i=1(µ 2 i − yi logµi) + λTr(X) subject to µi = a>i Xai, 1 ≤ i ≤ m, X 0.\nNote that the likelihood function is augmented by the trace functional whose role is to promote low-rank solutions. While such convex relaxation schemes enjoy intriguing performance guarantees in many aspects in the sense that they require near-optimal sample complexity and achieve near-optimal error bounds for certain noise models, the computational cost typically far exceeds the order of n3. This limits applicability to large-dimensional data.\nThis paper follows another route: rather than lifting the problem into higher dimensions by introducing matrix variables, this paradigm maintains its iterates within the vector domain and optimize the nonconvex\n1Here and below, for z ∈ Cn, |z| (resp. |z|2) is the vector of magnitudes (resp. squared magnitudes).\nobjective directly (e.g. [11, 19, 21, 30, 32, 35, 36]). One promising approach along this line is the recently proposed two-stage algorithm called Wirtinger Flow (WF) [11]. Simply put, WF starts by computing a suitable initial guess z(0) using a spectral method, and then successively refines the estimate via an update rule that bears a strong resemblance to a gradient descent scheme, namely,\nz(t+1) = z(t) + µt m ∑m i=1 ∇`(z(t); yi),\nwhere z(t) denotes the tth iterate of the algorithm, and µt is the step size (or learning rate). Here, ∇`(z; yi) stands for the Wirtinger derivative w.r.t. z, which in the real-valued case reduces to the ordinary gradient. The main results of [11] demonstrate that WF is surprisingly accurate for both real-valued and complexvalued Gaussian sampling models. Specifically, when ai ∼ N (0, I) or ai ∼ N (0, I) + jN (0, I):\n1. WF achieves exact recovery from m = O (n log n) quadratic equations when there is no noise;2\n2. WF attains -accuracy—in a relative sense—within O(mn2 log 1/ ) time (or flops);\n3. In the presence of Gaussian noise, WF is stable and converges to the MLE as shown in [39].\nWhile these results formalize the advantages of WF, the computational cost of WF is still larger than the best one can hope for. Moreover, the statistical guarantee in terms of the sample complexity is weaker than that achievable by convex relaxations.3"
    }, {
      "heading" : "1.3 This paper: Truncated Wirtinger Flow",
      "text" : "This paper develops a novel linear-time algorithm, which also enjoys near-perfect statistical guarantees. Following the spirit of WF, we propose a novel procedure called Truncated Wirtinger Flow (TWF) adopting a more subtle gradient flow. Informally, TWF proceeds in two stages:\n1. Initialization: compute an initial guess z(0) by means of a spectral method applied to a subset T0 of the observations {yi};\n2. Loop: for 0 ≤ t < T , z(t+1) = z(t) +\nµt m ∑ i∈Tt+1 ∇`(z(t); yi) (7)\nfor some index set Tt+1 ⊆ {1, · · · ,m} determined by z(t).\nThree remarks are in order.\n• Firstly, the index set Tt+1 is data-dependent and iteration-varying, which is a distinguishing feature of TWF in comparison to WF. In words, Tt+1 corresponds to those equations whose resulting gradient components (i.e. ∇`(z(t); yi)) are in some sense not excessively large; see Sections 2 and 3. As we shall see later, the main point is that this truncation gives us a better search direction.\n• Secondly, we recommend that the step size µt is either taken as some appropriate constant or determined by a backtracking line search. For instance, under appropriate conditions, we can take µt = 0.2.\n• Finally, the most expensive part of the gradient stage consists in computing ∇`(z; yi), 1 ≤ i ≤ m, which can often be performed in an efficient manner. More concretely, under the real-valued Poisson data model (4) one has\n∇`(z; yi) = 2 {\nyi |a>i z|2 aia > i z − aia>i z\n} = 2 ( yi − |a>i z|2\na>i z\n) ai, 1 ≤ i ≤ m.\n2 The standard notation f(n) = O (g(n)) or f(n) . g(n) (resp. f(n) = Ω (g(n)) or f(n) & g(n)) means that there exists a constant c > 0 such that |f(n)| ≤ c|g(n)| (resp. |f(n)| ≥ c |g(n)|). f(n) g(n) means that there exist constants c1, c2 > 0 such that c1|g(n)| ≤ |f(n)| ≤ c2|g(n)|.\n3 M. Soltanolkotabi recently informed us that the sample complexity of WF may be improved if one employs a better initialization procedure.\nThus, calculating {∇`(z; yi)} essentially amounts to two matrix-vector products. LettingA := [a1, · · · ,am]> as before, we have\n∑ i∈Tt+1\n∇`(z(t); yi) = A>v, vi = { 2 yi−|a>i z| 2 a>i z , i ∈ Tt+1,\n0, otherwise.\nHence, Az gives v and A>v the desired truncated gradient.\nA detailed specification of the algorithm is deferred to Section 2."
    }, {
      "heading" : "1.4 Numerical surprises",
      "text" : "To give the readers a sense of the practical power of TWF, we present here three illustrative numerical examples. Since it is impossible to recover the global sign—i.e. we cannot distinguish x from −x—we will evaluate our solutions to the quadratic equations through the distance measure put forth in [11] representing the Euclidean distance modulo a global sign: for complex signals,\ndist (z,x) := minϕ:∈[0,2π) ‖e−jϕz − x‖. (8)\nwhile it is simply min ‖z±x‖ in the real case. We shall use dist(x̂,x)/‖x‖ throughout to denote the relative erorr of an estimate x̂. In the sequel, TWF proceeds assuming a Poisson log-likelihood (6). Standalone Matlab implementations of TWF are available at http://web.stanford.edu/~yxchen/TWF/ (see [40] for straight WF implementations).\nThe first numerical example concerns the following two problems under noise-free real-valued data:\n(a) find x ∈ Rn s.t. bi = a>i x, 1 ≤ i ≤ m; (b) find x ∈ Rn s.t. bi = |a>i x|, 1 ≤ i ≤ m.\nApparently, (a) is tantamount to solving a linear least-squares problem, while (b) involves solving a set of quadratic equations. Arguably the most popular method for solving large-scale least squares problems is the conjugate gradient (CG) method [34] applied to the normal equations. We are going to compare the computational efficiency between CG (for solving least squares) and TWF with a step size µt ≡ 0.2 (for solving a quadratic system). Set m = 8n and generate x ∼ N (0, I) and ai ∼ N (0, I), 1 ≤ i ≤ m, independently. This gives a matrix A>A with clustered eigenvalues and a low condition number equal to about (1 + √ 1/8)2/(1− √ 1/8)2 ≈ 4.38 by the Marchenko-Pastur law. Therefore, this is an ideal setting for CG as it converges extremely rapidly [42, Theorem 38.5]. Fig. 1 shows the relative estimation error of each method as a function of the iteration count, where TWF is seeded through 10 power iterations. For ease of comparison, we illustrate the iteration counts in different scales so that 4 TWF iterations are equivalent to 1 CG iteration.\nRecognizing that each iteration of CG and TWF involves two matrix vector products Az and A>v, for such a design we reach a suprising observation:\nEven when all phase information is missing, TWF is capable of solving a quadratic system of equations only about 4 times slower than solving a least squares problem of the same size!\nTo illustrate the applicability of TWF on real images, we turn to testing our algorithm on a 320× 1280 digital photograph of Stanford main quad. We consider a type of measurements that falls under the category of coded diffraction patterns (CDP) [10] and set\ny(l) = |FD(l)x|2, 1 ≤ l ≤ L. (9)\nHere, F stands for a discrete Fourier transform (DFT) matrix, and D(l) is a diagonal matrix whose diagonal entries are independently and uniformly drawn from {1,−1, j,−j} (phase delays). In phase retrieval, each D(l) represents a random mask placed after the object so as to modulate the illumination patterns. When L masks are employed, the total number of quadratic measurements is m = nL. In this example, L = 12 random coded patterns are generated to measure each color band (i.e. red, green, or blue) separately. The\nexperiment is carried out on a MacBook Pro equipped with a 3 GHz Intel Core i7 and 16GB of memory. We run 50 iterations of the truncated power method for initialization, and 50 truncated gradient iterations, which in total costs 43.9 seconds or 2400 FFTs for each color band. The relative errors after spectral initialization and after 50 TWF iterations are 0.4773 and 2.16×10−5, respectively, with the recovered images displayed in Fig. 2. In comparison, WF with 50 power iterations and 100 gradient iterations (which takes 54.5 seconds or 3600 FFTs) returns an image of relative error 1.309, still extremely far from the truth.\nWhile the above experiments concern noiseless data, the numerical surprise extends to the noisy realm. Suppose the data are drawn according to the Poisson noise model (4), with ai ∼ N (0, I) independently generated. Fig. 3 displays the empirical relative mean-square error (MSE) of TWF as a function of the signal-to-noise ratio (SNR), where the relative MSE for an estimate x̂ and the SNR are defined as4\nMSE := dist2(x̂,x) ‖x‖2 , and SNR := 3‖x‖ 2. (10)\nBoth SNR and MSE are displayed on a dB scale (i.e. the values of 10 log10(SNR) and 10 log10(rel. MSE) are plotted). To evaluate the accuracy of the TWF solutions, we consider the performance achieved by MLE applied to an ideal problem in which the true phases are revealed. In this ideal scenario, in addition to the data {yi} we are further given exact phase information {ϕi = sign(a>i x)}. Such precious information gives away the phase retrieval problem and makes the MLE efficiently solvable since the MLE problem with side information\nminimizez∈Rn − ∑m i=1 yi log ( |a>i z|2 ) + (a>i z) 2 subject to ϕi = sign(a>i z)\ncan be cast as a convex program\nminimizez∈Rn − ∑m\ni=1 2yi log\n( ϕia > i z ) + (a>i z) 2.\nFig. 3 illustrates the empirical performance for this ideal problem. The plots demonstrate that even when all phases are erased, TWF yields a solution of nearly the best possible quality, since it only incurs an extra 1.5 dB loss compared to ideal MLE computed with all true phases revealed. This phenomenon arises regardless of the SNR!\nFor experiments with noisy complex-valued data and (untruncated) WF, please see [39]. 4To justify the definition of SNR, note that the signals and noise are captured by µi = (a>i x) 2 and yi − µi (1 ≤ i ≤ m),\nrespectively. The ratio of the signal power to the noise power is therefore ∑m i=1 µ 2 i∑m i=1 Var[yi] = ∑m i=1 |a > i x| 4∑m i=1 |a > i x| 2 ≈ 3m‖x‖4 m‖x‖2 = 3‖x‖ 2."
    }, {
      "heading" : "1.5 Main results",
      "text" : "The preceding numerical discoveries unveil promising features of TWF in three aspects: (1) exponentially fast convergence; (2) exact recovery from noiseless data with sample complexity O (n); (3) nearly minimal mean-square loss in the presence of noise. This paper offers a formal substantiation of all these findings. To this end, we assume a tractable model in which the design vectors ai’s are independent Gaussian:\nai ∼ N (0, In) . (11)\nFor concreteness, our results are concerned with TWF based on the Poisson log-likelihood function\n`i(z) := ` (z; yi) = yi log ( |a>i z|2 ) − |a>i z|2, (12)\nwhere we shall use `i(z) as a shorthand for `(z; yi) from now on. We begin with the performance guarantees of TWF in the absence of noise.\nTheorem 1 (Exact recovery). Consider the noiseless case (1) with an arbitrary signal x ∈ Rn. Suppose that the step size µt is either taken to be a positive constant µt ≡ µ or chosen via a backtracking line search.\nThen there exist some universal constants 0 < ρ, ν < 1 and µ0, c0, c1, c2 > 0 such that with probability exceeding 1− c1 exp (−c2m), the truncated Wirtinger Flow estimates (Algorithm 1 with parameters specified in Table 1) obey\ndist(z(t),x) ≤ ν(1− ρ)t‖x‖, ∀t ∈ N, (13)\nprovided that m ≥ c0n and µ ≤ µ0.\nAs explained below, we can often take µ0 ≈ 0.3.\nRemark 1. As will be made precise in Section 5 (and in particular Proposition 1), one can take\nµ0 = 0.994− ζ1 − ζ2 −\n√ 2/(9π)α−1h\n2 (1.02 + 0.665/αh)\nfor some small quantities ζ1, ζ2 and some truncation threshold αh that is usually taken to be αh ≥ 5. Under appropriate conditions, one can treat µ0 as\nµ0 ≈ 0.3. (14) Theorem 1 justifies at least two appealing features of TWF: (i) minimal sample complexity and (ii) linear-time complexity. Specifically, TWF allows exact recovery from O(n) quadratic equations, which is optimal since one needs at least n measurements to have a well posed problem. Also, because of the geometric convergence rate, TWF achieves -accuracy (i.e. dist(z(t),x) ≤ ‖x‖) within at most O (log 1/ ) iterations. The total computational cost is therefore O (mn log 1/ ), which is linear in the problem size. These outperform the provable guarantees of WF [11], which requires O(n log n) sample complexity and runs in O(mn2 log 1/ ) time.\nWe emphasize that enhanced performance vis-à-vis WF is not the result of a sharper analysis, but rather, the result of key algorithmic changes. In both the initialization and iterative refinement stages, TWF proceeds in a more prudent manner by means of proper truncation. In a nutshell, TWF operates only upon a subset of data whose contributions can be well controlled, thus guaranteeing better initial guess and descent directions. With these in place, we take the step size in a far more liberal fashion—which is bounded away from 0—compared to a step size which is inversely propotional to n as explained in [11]. In fact, what enables the movement to be more aggressive is exactly the cautious choice of Tt, which precludes adverse effects from atypical samples.\nTo be broadly applicable, the proposed algorithm must guarantee reasonably faithful estimates in the presence of noise. Suppose that yi = |〈ai,x〉|2 + ηi, 1 ≤ i ≤ m, (15) where ηi represents an error term. We claim that TWF is stable against additive noise, as demonstrated in the theorem below.\nTheorem 2 (Stability). Consider the noisy case (15). Suppose that the step size µt is either taken to be a positive constant µt ≡ µ or chosen via a backtracking line search. If\nm ≥ c0n, µ ≤ µ0, and ‖η‖∞ ≤ c1 ‖x‖ 2 , (16)\nthen with probability at least 1 − c2 exp (−c3m), the truncated Wirtinger Flow estimates (Algorithm 1 with parameters specified in Table 1) satisfy\ndist(z(t),x) . ‖η‖√ m‖x‖ + (1− ρ) t‖x‖, ∀t ∈ N (17)\nsimultanesouly for all x ∈ Rn. Here, 0 < ρ < 1 and µ0, c0, c1, c2, c3 > 0 are some universal constants. Under the Poisson noise model (4), there exists an an event of probability at least 1 − c2 exp(−c3m) on which\nP { dist(z(t),x) . 1 + (1− ρ)t‖x‖, ∀t ∈ N ∣∣∣ {ai}1≤i≤m}→ 1. (18)\nholds for all x ∈ Rn satisfying ‖x‖ ≥ log1.5m.\nRemark 2. In the main text, we will prove Theorem 2 only for the case where x is fixed and independent of the design vectors {ai}. Interested readers are referred to the supplemental materials [13] for the proof of the universal theory (i.e. the case simultaneously accommodating all x ∈ Rn). Note that when there is no noise (η = 0), this stronger result guarantees the universality of the noiseless recovery. Remark 3. [39] proves similar stability estimates using the WF approach under Gaussian noise. (M. Soltanolkotabi has also informed us that his results extend to other noise models.) There, the sample and computational complexities are still on the order of n log n and mn2 respectively whereas the computational complexity in Theorem 2 is linear, i.e. on the order of mn.\nTheorem 2 essentially reveals that the estimation error of TWF rapidly shrinks to O ( ‖η‖/ √ m\n‖x‖\n) within\nlogarithmic iterations. Put another way, since the SNR for the model (15) is captured by SNR := ∑m i=1 |〈ai,x〉|4 ‖η‖2 ≈ 3m‖x‖4 ‖η‖2 , (19)\nwe immediately arrive at an alternative form of performance guarantee:\ndist(z(t),x) . 1√ SNR ‖x‖+ (1− ρ)t‖x‖, ∀t ∈ N, (20)\nrevealing the stability of TWF as a function of SNR. We emphasize that this estimate holds for any error term η—i.e. any noise structure, even deterministic. This being said, specializing this estimate to the Poisson noise model (4) with ‖x‖ & log1.5m gives an estimation error that will eventually approach a numerical constant, independent of n and m.\nEncouragingly, this is already the best statistical guarantee any algorithm can achieve. We formalize this claim by deriving a fundamental lower bound on the minimax estimation error.\nTheorem 3 (Lower bound on the minimax risk). Suppose that ai ∼ N (0, I), m = κn for some fixed κ independent of n, and n is sufficiently large. For any K ≥ log1.5m, define5\nΥ(K) := {x ∈ Rn | ‖x‖ ∈ (1± 0.1)K}. 5Here, 0.1 can be replaced by any positive constant within (0, 1/2).\nWith probability approaching one, the minimax risk under the Poisson model (4) obeys\ninf x̂ sup x∈Υ(K)\nE [ dist (x̂,x) ∣∣ {ai}1≤i≤m] ≥ ε1√ κ , (21)\nwhere the infimum is over all estimator x̂. Here, ε1 > 0 is a numerical constant independent of n and m.\nWhen the number m of measurements is proportional to n and the energy of the planted solution exceeds log3m, Theorem 3 asserts that there exists absolutely no estimator that can achieve an estimation error that vanishes as n increases. This lower limit matches the estimation error of TWF, which corroborates the optimality of TWF under noisy data.\nCareful readers will naturally wonder whether the regime ‖x‖ ≥ log1.5m—or ‖x‖ ≥ √n log1.5m if we normalize ai so that ‖ai‖ ≈ 1—is of practical importance. Note that in the optical imaging applications, the Poisson noise model employs x and y to describe the numbers of photons diffracted by the specimen and detected by the optical sensor, respectively. Each specimen under study needs to be sufficiently illuminated in order for the receiver to sense the diffracted light; this means that the number of photons hitting each pixel must be large on the average (typically much larger than log3m). The regime ‖x‖ ≤ log1.5m is thus of little practical interest as it basically corresponds to a black object.\nIt is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few. While these paradigms enjoy favorable empirical behavior, most of them fall short of theoretical support, except for a version of alternating minimization (called AltMinPhase) [32] that requires fresh samples for each iteration. In comparison, AltMinPhase attains -accuracy only when the sample complexity exceeds the order of n log3 n+ n log2 n log 1/ , which is at least a factor of log3 n from optimal. Furthermore, none of these algorithms come with provable stability guarantees, which are particularly important in most realistic scenarios. Interesting readers are referred to [11] for a comparison of these non-convex schemes, and [10] for a discussion of other alternative approaches (e.g. [1, 4]) and performance lower bounds (e.g. [5, 18]). On the other hand, the family of two-stage nonconvex procedures—spectral initialization followed by iterative refinement—has proved efficient for other problems that involve latent variables, which leads to theoretical guarantees for general EM algorithms [3] and sparse coding schemes [2]. The truncation idea proposed herein might promise improved performance for these problems as well."
    }, {
      "heading" : "2 Algorithm: Truncated Wirtinger Flow",
      "text" : "This section describes the two stages of truncated Wirtinger flow in details, presented in a reverse order. For each stage, we start with some algorithmic issues encountered by WF, which is then used to motivate and explain the basic principles of TWF. Here and throughout, we let A : Rn×n 7→ Rm be the linear map\nM ∈ Rn×n 7→ A (M) := { a>i Mai } 1≤i≤m\nand A the design matrix A := [a1, · · · ,am]>."
    }, {
      "heading" : "2.1 Truncated gradient stage",
      "text" : "For independent samples, the gradient of the Poisson log-likelihood for any z ∈ Rn with a>i z 6= 0 for all i, obeys\nm∑ i=1 ∇`i(z) = m∑ i=1 2 yi − |a>i z|2\na>i z︸ ︷︷ ︸ :=νi\nai, (22)\nwhere νi represents the weight assigned to each ai. This forms the descent direction of WF updates.\nUnfortunately, WF moving along the preceding direction does not come close to a meaningful solution under real-valued Poisson data. To see this, it is helpful to consider any fixed vector z ∈ Rn independent of the design vectors. The typical size of min1≤i≤m |a>i z| is about on the order of 1m‖z‖, introducing some unreasonably large weights νi, which can be as large as m‖x‖2/‖z‖. (For complex-valued data where ai ∼ N (0, I) + jN (0, I), this issue is less severe since min1≤i≤m |a>i z| 1√m‖z‖.) Consequently, the iterative updates based on (1.2) often overshoot, and this arises starting from the very initial stage.\nFig. 4 illustrates this phenomenon by showing the locus of −∇`i (z) when ai varies. Examination of the figure seems to suggest that most of the gradient components ∇`i (z) are more or less pointing towards the ground-truth solution x and forming reasonable descent directions. This intuition is corroborated by numerical experiments under real-valued data. As illustrated in Fig. 5, the solutions returned by the untruncated WF (designed for a Poisson log-likelihood) are very far from the ground truth.\nHence, to remedy the aforementioned stability issue, it would be natural to separate the small fraction of abnormal gradient components by regularizing the weights νi, possibly via appropriate truncation. This\ngives rise to the update rule of TWF:\nz(t+1) = z(t) + µt m ∇`tr(z(t)), ∀t ∈ N, (23)\nwhere ∇`tr (·) denotes the truncated gradient given by 6\n∇`tr (z) := m∑ i=1 2 yi − |a>i z|2 a>i z ai1Ei1(z)∩Ei2(z). (24)\nfor some truncation criteria specified by E i1 (·) and E i2 (·). In our algorithm, we take E i1 (z) and E i2 (z) to be two collections of events given by\nE i1(z) := { αlbz ≤ ∣∣a>i z∣∣ ‖z‖ ≤ α ub z } ; (25)\nE i2(z) := { |yi − |a>i z|2| ≤\nαh m ∥∥y −A (zz>)∥∥ 1 ∣∣a>i z∣∣ ‖z‖ } , (26)\nwhere αlbz , αubz , αz are predetermined truncation thresholds. To keep notation light, we shall use E i1 and E i2 rather than E i1 (z) and E i2 (z) whenever it is clear from context.\nWe emphasize that the above truncation simply throws away those weights νi’s that fall outside some confidence range as to remove the contribution of abnormal components. To achieve this, we regularize both the numerator and denominator of νi by enforcing separate truncation rules. Recognize that for any fixed z, the denominator obeys\nE [∣∣a>i z∣∣] = √2/π‖z‖,\nwhich leads up to the rule (25). Regarding the numerator, by the law of large numbers one would expect\nE [∣∣yi − |a>i z|2∣∣] ≈ 1m ∥∥y −A (zz>)∥∥1 ,\nand hence it is natural to regularize the numerator by ensuring∣∣yi − |a>i z|2∣∣ . 1m ∥∥y −A (zz>)∥∥1 . As a remark, we include an extra term |a\n> i z| ‖z‖ in (26) to sharpen the theory, but all our results continue\nto hold (up to some modification of constants) if we drop this term in the truncation rule (26). Detailed procedures are summarized in Algorithm 1 7."
    }, {
      "heading" : "2.2 Truncated spectral initialization",
      "text" : "In order for the truncated gradient stage to converge rapidly, we need to seed it with a suitable initialization. One natural alternative is the spectral method adopted in [11,32], which amounts to computing the leading eigenvector of Ỹ := 1m ∑m i=1 yiaia > i . This arises from the observation that when ai ∼ N (0, I) and ‖x‖ = 1,\nE[Ỹ ] = I + 2xx>,\nwhose leading eigenvector is exactly x with an eigenvalue of 3. Unfortunately, this spectral technique converges to a good initial point only when m & n log n, due to the fact that (a>i x)2aia>i is heavy-tailed, a random quantity which does not have a moment generating 6 In the complex-valued case, the truncation is enforced upon the Wirtinger derivative, which reads ∇`tr (z) :=∑m i=1 2 yi−|z∗ai|2 z∗ai ai1Ei1(z)∩E i 2(z) .\n7Careful readers might note that we include some extra factor √ n\n‖ai‖ (which is approximately 1 in the Gaussian model) in\nAlgorithm 1. This occurs since we present Algorithm 1 in a more general fashion that applies beyond the model ai ∼ N (0, I), but all results / proofs continue to hold in the presence of this extra term.\nAlgorithm 1 Truncated Wirtinger Flow. Input: Measurements {yi | 1 ≤ i ≤ m} and sampling vectors {ai | 1 ≤ i ≤ m}; truncation thresholds αlbz , αubz , αh, and αy (see default values in Table 1).\nInitialize z(0) to be √\nmn∑m i=1‖ai‖\n2λ0z̃, where λ0 = √ 1 m ∑m i=1 yi and z̃ is the leading eigenvector of\nY = 1\nm m∑ i=1 yiaia ∗ i 1{|yi|≤α2yλ20}. (27)\nLoop: for t = 0 : T do\nz(t+1) = z(t) + 2µt m m∑ i=1 yi − ∣∣a∗i z(t)∣∣2 z(t)∗ai ai1Ei1∩Ei2 , (28)\nwhere E i1 := { αlbz ≤ √ n\n‖ai‖ |a∗i z(t)| ‖z(t)‖ ≤ α ub z\n} , E i2 := { |yi − |a∗i z(t)|2| ≤ αhKt √ n\n‖ai‖ |a∗i z(t)| ‖z(t)‖\n} , (29)\nand Kt := 1\nm m∑ l=1 ∣∣yl − |a∗l z(t)|2∣∣. Output zT .\nfunction. To be more precise, consider the noiseless case yi = |a>i x|2 and recall that maxi yi ≈ 2 logm. Letting k = arg maxi yi, one can calculate(\nak ‖ak‖\n)> Ỹ\nak ‖ak‖ ≥ ( ak ‖ak‖ )>( 1 m aka > k )( a>k x )2( ak ‖ak‖ ) ≈ 2n logm m ,\nwhich is much larger than x>Ỹ x = 3 unless m/n is very large. This tells us that in the regime where m n, there exists some unit vector ak/‖ak‖ that is closer to the leading eigenvector of Ỹ than x. This phenomenon prevents the spectral method from returning a meaningful initial guess.\nTo address this issue, we propose to discard those observations yi that are several times larger than the mean during spectral initialization. Specifically, the initial estimate is obtained by computing the leading eigenvector z̃ of\nY := 1\nm m∑ i=1 yiaia > i 1{|yi|≤α2y( 1m ∑ml=1 yl)} (30)\nfor some truncation threshold αy, and then rescaling z̃ so as to have roughly the same norm as x (which is estimated to be 1m ∑m l=1 yl); see Algorithm 1 for the detailed procedure.\nThe above drawback of the spectral method is not merely a theoretical concern but rather a substantial practical issue. We have seen this in Fig. 2 (main quad example) showing the enormous advantage of the truncation scheme. This is also further illustrated in Fig. 6, which compares the empirical efficiency of both methods with αy = 3 set to be the truncation threshold. For both Gaussian designs and CDP models, the empirical loss incurred by the original spectral method increases as n grows, which is in stark constrast to the truncated spectral method that achieves almost identical accuracy over the same range of n."
    }, {
      "heading" : "2.3 Choice of algorithmic parameters",
      "text" : "One implementation detail to specify is the step size µt at each iteration t. There are two alternatives that work well in both theory and practice:\n1. Fixed step size. Take µt ≡ µ (∀t ∈ N) for some constant µ > 0. As long as µ is not too large, our main results state that this strategy always works—although the convergence rate depends on µ. Under appropriate conditions, our theorems hold for any constant 0 < µ < 0.28.\nn: signal dimension 1000 2000 3000 4000 5000\nRe lat\nive e\nrro r\n0.6\n0.7\n0.8\n0.9\n1 spectral method truncated spectral method\nn : signal dimension (105) 0.5 1 1.5 2 2.5 3 3.5 4\nRe lat\nive e\nrro r\n0.4\n0.6\n0.8\n1\n1.2\n1.4 spectral method truncated spectral method\n(a) (b)\nAnother set of important algorithmic parameters to determine is the truncation thresholds αh, αlbz , αubz , αy, and αp (for a backtracking line search only). The present paper isolates the set of (αh, αlbz , αubz , αy)\nobeying (31) as given in Table 1 when a fixed step size is employed. More concretely, this range subsumes as special cases all parameters obeying the following constraints:\n0 < αlbz ≤ 0.5, αubz ≥ 5, αh ≥ 5, and αy ≥ 3. (35)\nWhen a backtracking line search is adopted, an extra parameter αp is needed, which we take to be αp ≥ 5. In all theory presented herein, we assume that the parameters fall within the range singled out in Table 1."
    }, {
      "heading" : "3 Why TWF works?",
      "text" : "Before proceeding, it is best to develop an intuitive understanding of the TWF iterations. We start with a notation representing the (unrecoverable) global phase [11] for real-valued data\nφ (z) := { 0, if ‖z − x‖ ≤ ‖z + x‖ , π, else.\n(36)\nIt is self-evident that (−z) + µ\nm ∇tr`\n( − z ) = − { z + µ\nm ∇tr`(z)\n} ,\nand hence (cf. Definition (8))\ndist (\n(−z) + µ m ∇tr`(−z),x\n) = dist ( z + µ\nm ∇tr` (z) ,x ) despite the global phase uncertainty. For simplicity of presentation, we shall drop the phase term by letting z be e−jφ(z)z and setting h = z − x, whenever it is clear from context.\nThe first object to consider is the descent direction. To this end, we find it convenient to work with a fixed z independent of the design vectors ai, which is of course heuristic but helpful in developing some intuition. Rewrite\n∇`i(z) = 2 (a>i x) 2 − (a>i z)2 a>i z ai (i) = − 2(a > i h)(2a > i z − a>i h) a>i z ai\n= −4(a>i h)ai + 2 (a>i h) 2\na>i z ai︸ ︷︷ ︸\n:=ri\n, (37)\nwhere (i) follows from the identity a2 − b2 = (a+ b)(a− b). The first component of (37), which on average gives −4h, makes a good search direction when averaged over all the observations i = 1, . . . ,m. The issue is that the other term ri—which is in general non-integrable—could be devastating. The reason is that a>i z could be arbitrarily small, thus resulting in an unbounded ri. As a consequence, a non-negligible portion of the ri’s may exert a very strong influence on the descent direction in an undesired manner.\nSuch an issue can be prevented if one can detect and separate those gradient components bearing abnormal ri’s. Since we cannot observe the individual components of the decomposition (37), we cannot reject indices with large values of ri directly. Instead, we examine each gradient component as a whole and discard it if its size is not absolutely controlled. Fortunately, such a strategy is sufficient to ensure that most of the contribution from the truncated gradient comes from the first component of (37), namely, −4(a>i h)ai. As will be made precise in Proposition 2 and Lemma 7, the truncated gradient obeys\n− 〈 1 m ∇`tr(z),h 〉 ≥ (4− )‖h‖2 −O (‖h‖3 ‖z‖ ) (38)\nand ∥∥∥ 1 m ∇`tr(z) ∥∥∥ . ‖h‖. (39) Here, one has (4− )‖h‖2 in (38) instead of 4‖h‖2 to account for the bias introduced by truncation, where is small as long as we only throw away a small fraction of data. Looking at (38) and (39) we see that the search direction is sufficiently aligned with the deviation −h = x− z of the current iterate; i.e. they form a reasonably good angle that is bounded away from 90◦. Consequently, z is expected to be dragged towards x provided that the step size is appropriately chosen.\nThe observations (38) and (39) are reminiscent of a (local) regularity condition given in [11], which has been shown to be a fundamental criterion that dictates rapid convergence of iterative procedures (including WF and other gradient descent schemes). When specialized to TWF, we say that − 1m∇`tr (·) satisfies the regularity condition, denoted by RC (µ, λ, ), if〈\nh,− 1 m ∇`tr(z)\n〉 ≥ µ\n2 ∥∥∥ 1 m ∇`tr (z) ∥∥∥2 + λ 2 ‖h‖2 (40)\nholds for all z obeying ‖z − x‖ ≤ ‖x‖, where 0 < < 1 is some constant. Such an -ball around x is sometimes referred to as a basin of attraction. Formally, under RC (µ, λ, ), a little algebra gives\ndist2 ( z + µ\nm ∇`tr (z) ,x\n) ≤ ∥∥∥z + µ m ∇`tr (z)− x ∥∥∥2 = ‖h‖2 + ∥∥∥ µ m ∇`tr (z) ∥∥∥2 + 2µ〈h, 1 m ∇`tr (z)\n〉 ≤ ‖h‖2 + ∥∥∥ µ m ∇`tr (z) ∥∥∥2 − µ2 ∥∥∥∥ 1m∇`tr (z) ∥∥∥∥2 − µλ ‖h‖2\n= (1− µλ) dist2 (z,x) (41)\nfor any z with ‖z − x‖ ≤ . In words, the TWF update rule is locally contractive around the planted solution, provided that RC (µ, λ, ) holds for some nonzero µ and λ. Apparently, Conditions (38) and (39) already imply the validity of RC for some constants µ, λ 1 when ‖h‖/‖z‖ is reasonably small, which in turn allows us to take a constant step size µ and enables a constant contraction rate 1− µλ.\nFinally, caution must be exercised when connecting RC with strong convexity, since the former does not necessarily guarantee the latter within the basin of attraction. As an illustration, Fig. 7 plots the graph of a non-convex function obeying RC. The distinction stems from the fact that RC is stated only for those pairs z and h = z−x with x being a fixed component, rather than simultaneously accommodating all possible z and h = z − z̃ with z̃ being an arbitrary vector. In contrast, RC says that the only stationary point of the truncated objective in a neighborhood of x is x, which often suffices for a gradient-descent type scheme to succeed.\n0.2 0.4 0.6 0.8 1\n0.9\n1 1.1\nt\nf(t)/ p 1 + t2"
    }, {
      "heading" : "4 Numerical experiments",
      "text" : "In this section, we report additional numerical results to verify the practical applicability of TWF. In all numerical experiments conducted in the current paper, we set\nαlbz = 0.3, α ub z = 5, αh = 5, and αy = 3. (42)\nThis is a concrete combination of parameters satisfying our condition (31). Unless otherwise noted, we employ 50 power iterations for initialization, adopt a fixed step size µt ≡ 0.2 when updating TWF iterates, and set the maximum number of iterations to be T = 1000 for the iterative refinement stage.\nThe first series of experiments concerns exact recovery from noise-free data. Set n = 1000 and generate a real-valued signal x at random. Then for m varying between 2n and 6n, generate m design vectors ai independently drawn from N (0, I). An experiment is claimed to succeed if the returned estimate x̂ satisfies dist (x̂,x) / ‖x‖ ≤ 10−5. Fig. 8 illustrates the empirical success rate of TWF (over 100 Monte Carlo trials for each m) revealing that exact recovery is practially guaranteed from fewer than 1000 iterations when the number of quadratic constraints is about 5 times the ambient dimension.\nTo see how special the real-valued Gaussian designs are to our theoretical finding, we perform experiments on two other types of measurement models. In the first, TWF is applied to complex-valued data by generating ai ∼ N ( 0, 12I ) + jN ( 0, 12I ) . The other is the model of coded diffraction patterns described in (9). Fig. 9 depicts the average success rate for both types of measurements over 100 Monte Carlo trials, indicating that m > 4.5n and m ≥ 6n are often sufficient under complex-valued Gaussian and CDP models, respectively.\nFor the sake of comparison, we also report the empirical performance of WF in all the above settings, where the step size is set to be the default choice of [11], that is, µt = min{1− e−t/330, 0.2}. As can be seen, the empirical success rates of TWF outperform WF when T = 1000 under Gaussian models, suggesting that TWF either converges faster or exhibits better phase transition behavior.\nAnother series of experiments has been carried out to demonstrate the stability of TWF when the number m of quadratic equations varies. We consider the case where n = 1000, and vary the SNR (cf. (10)) from 15 dB to 55dB. The design vectors are real-valued independent Gaussian ai ∼ N (0, I), while the measurements yi are generated according to the Poisson noise model (4). Fig. 10 shows the relative mean square error— in the dB scale—as a function of SNR, when averaged over 100 independent runs. For all choices of m, the numerical experiments demonstrate that the relative MSE scales inversely proportional to SNR, which matches our stability guarantees in Theorem 2 (since we observe that on the dB scale, the slope is about -1 as predicted by the theory (20))."
    }, {
      "heading" : "5 Exact recovery from noiseless data",
      "text" : "This section proves the theoretical guarantees of TWF in the absence of noise (i.e. Theorem 1). We separate the noiseless case mainly out of pedagogical reasons, as most of the steps carry over to the noisy case with slight modification.\nThe analysis for the truncated spectral method follows similar argument as in [11, Section 7.8], which we defer to Appendix C. In short, for any fixed δ > 0 and x ∈ Rn, the initial point z(0) returned by the truncated spectral method obeys dist(z(0),x) ≤ δ‖x‖ with high probability, provided that m/n exceeds some large constant. With this in place, it suffices to show that the TWF update rule is locally contractive, as stated in the following proposition.\nProposition 1 (Local error contraction). Consider the noiseless case (1). Under the condition (31), there exist some universal constants 0 < ρ0 < 1 and c0, c1, c2 > 0 such that with probability exceeding 1− c1 exp (−c2m),\ndist2 ( z + µ\nm ∇`tr (z) ,x\n) ≤ (1− ρ0) dist2 (z,x) (43)\nholds simultaneously for all x, z ∈ Rn obeying\ndist (z,x) ‖z‖ ≤ min { 1 11 , αlbz 3αh , αlbz 6 , 5.7 ( αlbz )2 2αubz + α lb z } , (44)\nprovided that m ≥ c0n and that µ is some constant obeying 0 < µ ≤ µ0 := 0.994−ζ1−ζ2− √ 2/(9π)α−1h 2(1.02+0.665/αh) .\nProposition 1 reveals the monotonicity of the estimation error: once entering a neighborhood around x of a reasonably small size, the iterative updates will remain within this neighborhood all the time and be attracted towards x at a geometric rate.\nAs shown in Section 3, under the hypothesis RC (µ, λ, ) one can conclude dist2 ( z + µ\nm ∇`tr(z),x\n) ≤ (1− µλ)dist2(z,x), ∀(z,x) with dist(z,x) ≤ . (45)\nThus, everything now boils down to showing RC (µ, λ, ) for some constants µ, λ, > 0. This occupies the rest of this section.\n5.1 Preliminary facts about {E i1} and {E i2} Before proceeding, we gather a few properties of the events E i1 and E i2, which will prove crucial in establishing RC (µ, λ, ). To begin with, recall that the truncation level given in E i2 depends on 1m ∥∥A (xx> − zz>)∥∥ 1 . Instead of working with this random variable directly, we use deterministic quantities that are more amenable to analysis. Specifically, we claim that 1m ∥∥A (xx> − zz>)∥∥ 1 offers a uniform and orderwise tight estimate on ‖h‖ ‖z‖, which can be seen from the following two facts.\nLemma 1. Fix ζ ∈ (0, 1). If m > c0nζ−2 log 1ζ , then with probability at least 1− C exp(−c1ζ2m),\n0.9 (1− ζ) ‖M‖F ≤ 1\nm ‖A (M)‖1 ≤ (1 + ζ)\n√ 2 ‖M‖F (46)\nholds for all symmetric rank-2 matrices M ∈ Rn×n. Here, c0, c1, C > 0 are some universal constants.\nProof. Since [12, Lemma 3.1] already establishes the upper bound, it suffices to prove the lower tail bound. Consider all symmetric rank-2 matrices M with eigenvalues 1 and −t for some −1 ≤ t ≤ 1. When t ∈ [0, 1], it has been shown in the proof of [12, Lemma 3.2] that with high probability,\n1 m ‖A (M)‖1 ≥ (1− ζ) f (t) , (47)\nx\nz\n`(z)\n1\nfor all such rank-2 matrices M , where f (t) := 2π { 2 √ t+ (1− t) ( π/2− 2arc tan( √ t) )}\n. The lower bound in this case can then be justified by recognizing that f (t) / √ 1 + t2 ≥ 0.9 for all t ∈ [0, 1], as illustrated in Fig. 11. The case where t ∈ [−1, 0] is an immediate consequence from [12, Lemma 3.1].\nLemma 2. Consider any x, z ∈ Rn obeying ‖z − x‖ ≤ δ ‖z‖ for some δ < 12 . Then one has √ 2− 4δ ‖z − x‖ ‖z‖ ≤ ∥∥xx> − zz>∥∥\nF ≤ (2 + δ) ‖z − x‖ ‖z‖ . (48)\nProof. Take h = z − x and write∥∥xx> − zz>∥∥2 F = ∥∥− hz> − zh> + hh>∥∥2 F\n= ∥∥hz> + zh>∥∥2\nF + ‖h‖4 − 2〈hz> + zh>,hh>〉\n= 2 ‖z‖2 ‖h‖2 + 2|h>z|2 + ‖h‖4 − 2‖h‖2(h>z + z>h).\nWhen ‖h‖ < 12‖z‖, the Cauchy–Schwartz inequality gives\n2 ‖z‖2 ‖h‖2 − 4 ‖z‖ ‖h‖3 ≤ ∥∥xx> − zz>∥∥2\nF ≤ 4 ‖z‖2 ‖h‖2 + 4 ‖h‖3 ‖z‖+ ‖h‖4 , (49)\n⇒ √ (2 ‖z‖ − 4 ‖h‖) ‖z‖ · ‖h‖ ≤ ∥∥xx> − zz>∥∥\nF ≤ (2 ‖z‖+ ‖h‖) · ‖h‖ (50)\nas claimed.\nTaken together the above two facts demonstrate that with probability 1− exp (−Ω (m)),\n1.15 ‖z − x‖ ‖z‖ ≤ 1 m ∥∥A (xx> − zz>)∥∥ 1 ≤ 3 ‖z − x‖ ‖z‖ (51)\nholds simultaneously for all z and x satisfying ‖h‖ ≤ 111 ‖z‖. Conditional on (51), the inclusion\nE i3 ⊆ E i2 ⊆ E i4 (52)\nholds with respect to the following events E i3 : = {∣∣|a>i x|2 − |a>i z|2∣∣ ≤ 1.15αh ‖h‖ · ∣∣a>i z∣∣} , (53)\nE i4 : = {∣∣|a>i x|2 − |a>i z|2∣∣ ≤ 3αh ‖h‖ · ∣∣a>i z∣∣} . (54)\nThe point of introducing these new events is that E i3’s (resp. E i4’s) are statistically independent for any fixed x and z and are, therefore, easier to work with.\nNote that each E i3 (resp. E i4) is specified by a quadratic inequality. A closer inspection reveals that in order to satisfy these quadratic inequalities, the quantity a>i h must fall within two intervals centered around 0 and 2a>i z, respectively. One can thus facilitate analysis by decoupling each quadratic inequality of interest into two simple linear inequalities, as stated in the following lemma.\nLemma 3. For any γ > 0, define Diγ := {∣∣|a>i x|2 − |a>i z|2∣∣ ≤ γ ‖h‖ ∣∣a>i z∣∣} , (55)\nDi,1γ := { |a>i h| ‖h‖ ≤ γ } , (56)\nand Di,2γ := {∣∣∣∣a>i h‖h‖ − 2a>i z‖h‖ ∣∣∣∣ ≤ γ} . (57) Thus, Di,1γ and Di,2γ represent the two intervals on a>i h centered around 0 and 2a>i z. If ‖h‖‖z‖ ≤ αlbz γ , then the following inclusion holds( Di,1γ\n1+ √ 2\n∩ E i1 ) ∪ ( Di,2γ\n1+ √ 2\n∩ E i1 ) ⊆ Diγ ∩ E i1 ⊆ ( Di,1γ ∩ E i1 ) ∪ ( Di,2γ ∩ E i1 ) . (58)"
    }, {
      "heading" : "5.2 Proof of the regularity condition",
      "text" : "By definition, one step towards proving the regularity condition (40) is to control the norm of the truncated gradient. In fact, a crude argument already reveals that ‖ 1m∇`tr(z)‖ . ‖h‖. To see this, introduce v = [vi]1≤i≤m with vi := 2 |a>i x|2−|a>i z|2 a>i z 1Ei1∩Ei2 . It comes from the truncation rule E i 1 as well as the inclusion property (52) that∣∣a>i z∣∣ & ‖z‖ and ∣∣∣yi − ∣∣a>i z∣∣2∣∣∣ . 1m‖A(xx> − zz>)‖1 ‖h‖ ‖z‖ , implying |vi| . ‖h‖ and hence ‖v‖ . √ m‖h‖. The Marchenko–Pastur law gives ‖A‖ . √m, whence\n1 m ‖∇`tr(z)‖ = 1 m ‖A>v‖ ≤ 1 m ‖A‖ · ‖v‖ . ‖h‖. (59)\nA more refined estimate will be provided in Lemma 7. The above argument essentially tells us that to establish RC, it suffices to verify a uniform lower bound of the form − 〈 h, 1\nm ∇`tr (z)\n〉 & ‖h‖2 , (60)\nas formally derived in the following proposition.\nProposition 2. Consider the noise-free measurements yi = |a>i x|2 and any fixed constant > 0. Under the condition (31), if m > c1n, then with probability exceeding 1− C exp (−c0m),\n− 〈 h, 1\nm ∇`tr (z)\n〉 ≥ 2 { 1.99− 2 (ζ1 + ζ2)− √ 8/(9π)α−1h − } ‖h‖2 (61)\nholds uniformly over all x, z ∈ Rn obeying\n‖h‖ ‖z‖ ≤ min\n{ 1\n11 , αlbz 3αh , αlbz 6 ,\n5.7 ( αlbz )2\n2αubz + α lb z\n} . (62)\nHere, c0, c1, C > 0 are some universal constants, and ζ1 and ζ2 are defined in (31).\nThe basic starting point is the observation that (a>i z)− (a>i x)2 = (a>i h)(2a>i z − a>i h) and hence\n− 1 2m ∇`tr (z) = 1 m m∑ i=1 (a>i z) 2 − (a>i x)2 a>i z ai1Ei1∩Ei2\n= 1\nm m∑ i=1 2(a>i h)ai1Ei1∩Ei2 − 1 m m∑ i=1 (a>i h) 2 a>i z ai1Ei1∩Ei2 . (63)\nOne would expect the contribution of the second term of (63) (which is a second-order quantity) to be small as ‖h‖ / ‖z‖ decreases.\nTo facilitate analysis, we rewrite (63) in terms of the more convenient events Di,1γ and Di,2γ . Specifically, the inclusion property (52) together with Lemma 3 reveals that\nDi,1γ3 ∩ E i1 ⊆ E i3 ∩ E i1 ⊆ E i2 ∩ E i1 ⊆ E i4 ∩ E i1 ⊆ ( Di,1γ4 ∪ Di,2γ4 ) ∩ E i1, (64)\nwhere the parameters γ3, γ4 are given by\nγ3 := 0.476αh, and γ4 := 3αh. (65)\nThis taken collectively with the identity (63) leads to a lower estimate\n− 〈 1\n2m ∇`tr(z),h 〉 ≥ 2 m m∑ i=1 ( a>i h )2 1Ei1∩D i,1 γ3 − 1 m m∑ i=1 ∣∣a>i h∣∣3∣∣a>i z∣∣ 1Ei1∩Di,1γ4 − 1m m∑ i=1 ∣∣a>i h∣∣3∣∣a>i z∣∣ 1Ei1∩Di,2γ4 , (66) leaving us with three quantities in the right-hand side to deal with. We pause here to explain and compare the influences of these three terms.\nTo begin with, as long as the truncation step does not discard too many samples, the first term should be close to 2m ∑ i |a>i h|2, which approximately gives 2‖h‖2 from the law of large numbers. This term turns out to be dominant in the right-hand side of (66) as long as ‖h‖/‖z‖ is reasonably small. To see this, please recognize that the second term in the right-hand side is O(‖h‖3/‖z‖), simply because both a>i h and a>i z are absolutely controlled on Di,1γ4 ∩ E i1. However, Di,2γ4 does not share such a desired feature. By the very definition of Di,2γ4 , each nonzero summand of the last term of (66) must obey\n∣∣a>i h∣∣ ≈ 2 ∣∣a>i z∣∣ and, therefore, |a\n> i h|3 |a>i z| 1Ei1∩Di,2γ4 is roughly of the order of ‖z‖ 2; this could be much larger than our target level\n‖h‖2. Fortunately, Di,2γ4 is a rare event, thus precluding a noticable influence upon the descent direction. All of this is made rigorous in Lemma 4 (first term), Lemma 5 (second term) and Lemma 6 (third term) together with subsequent analysis.\nLemma 4. Fix γ > 0, and let E i1 and Di,1γ be defined in (25) and (56), respectively. Set ζ1 := 1−min { E [ ξ21{√1.01αlbz ≤|ξ|≤ √ 0.99αubz } ] ,E [ 1{√1.01αlbz ≤|ξ|≤ √ 0.99αubz } ]} (67)\nand ζ2 := E [ ξ21{|ξ|>√0.99γ} ] , (68)\nwhere ξ ∼ N (0, 1). For any > 0, if m > c1n −2 log −1, then with probability at least 1− C exp(−c0 2m),\n1\nm m∑ i=1 ∣∣a>i h∣∣2 1Ei1∩Di,1γ ≥ (1− ζ1 − ζ2 − ) ‖h‖2 (69) holds for all non-zero vectors h, z ∈ Rn. Here, c0, c1, C > 0 are some universal constants.\nWe now move on to the second term in the right-hand side of (66). For any fixed γ > 0, the definition of E i1 gives rise to an upper estimate\n1\nm m∑ i=1 ∣∣a>i h∣∣3∣∣a>i z∣∣ 1Ei1∩Di,1γ ≤ 1αlbz ‖z‖ · 1m m∑ i=1 ∣∣a>i h∣∣3 1Di,1γ ≤ (1 + ) √ 8/π ‖h‖3 αlbz ‖z‖ , (70)\nwhere √\n8/π ‖h‖3 is exactly the untruncated moment E[|a>i h|3]. The second inequality is a consequence of the lemma below, which arises by observing that the summands |a>i h|31Di,1γ are independent sub-Gaussian random variables.\nLemma 5. For any constant γ > 0, if m/n ≥ c0 · −2 log −1, then\n1\nm m∑ i=1 ∣∣a>i h∣∣3 1Di,1γ ≤ (1 + )√8/π ‖h‖3 , ∀h ∈ Rn (71) with probability at least 1− C exp(−c1 2m) for some universal constants c0, c1, C > 0.\nIt remains to control the last term of (66). As mentioned above, the influence of this term is small since the set of ai’s satisfying Di,2γ accounts for a small fraction of measurements. Put formally, the number of equations satisfying ∣∣a>i h∣∣ ≥ γ ‖h‖ decays rapidly for large γ (at least at a quadratic rate), as stated below. Lemma 6. For any 0 < < 1, there exist some universal constants c0, c1, C > 0 such that\n1\nm m∑ i=1 1{|a>i h|≥ γ‖h‖} ≤ 1 0.49γ exp ( −0.485γ2 ) + γ2 , ∀h ∈ Rn\\{0} and γ ≥ 2 (72)\nwith probability at least 1− C exp ( −c0 2m ) . This holds with the proviso m/n ≥ c1 · −2 log −1.\nTo connect this lemma with the last term of (66), we recognize that when γ ≤ α lb z ‖z‖ ‖h‖ , one has\n1Ei1∩D i,2 γ ≤ 1{|a>i h|≥αlbz ‖z‖}. (73)\nThe constraint ∣∣∣a>i h‖h‖ − 2a>i z‖h‖ ∣∣∣ ≤ γ of Di,2γ necessarily requires∣∣a>i h∣∣\n‖h‖ ≥ 2 ∣∣a>i z∣∣ ‖h‖ − γ ≥ 2αlbz ‖z‖ ‖h‖ − γ ≥ αlbz ‖z‖ ‖h‖ , (74)\nwhere the last inequality comes from our assumption on γ. With Lemma 6 in place, (73) immediately gives\nm∑ i=1 1Ei1∩D i,2 γ ≤ ‖h‖ 0.49αlbz ‖z‖ exp\n( −0.485 ( αlbz ‖z‖ ‖h‖ )2) +\n‖h‖2\n(αlbz ) 2 ‖z‖2\n≤ 1 9800 ( ‖h‖ αlbz ‖z‖ )4 + (αlbz ) 2 (‖h‖ ‖z‖ )2 (75)\nas long as ‖h‖‖z‖ ≤ αlbz 6 , where the last inequality uses the majorization 1 20000x4 ≥ 1x exp\n( −0.485x2 ) holding for\nany x ≥ 6. In addition, on E i1 ∩ Di,2γ , the amplitude of each summand can be bounded in such a way that∣∣a>i h∣∣3∣∣a>i z∣∣ ≤\n∣∣2a>i z∣∣+ γ ‖h‖∣∣a>i z∣∣ (2αubz ‖z‖+ γ ‖h‖)2 (76) ≤ ( 2 + γ\nαlbz\n‖h‖ ‖z‖\n)( 2αubz + γ\n‖h‖ ‖z‖\n)2 ‖z‖2 , (77)\nwhere both inequalities are immediate consequences from the definitions of Di,2γ and E i1 (see (57) and (25)). Taking this together with the cardinality bound (75) and picking appropriately, we get\n1\nm m∑ i=1 ∣∣a>i h∣∣3∣∣a>i z∣∣ 1Ei1∩Di,2γ ≤  ( 2 + γ αlbz ‖h‖ ‖z‖ )( 2αubz + γ ‖h‖ ‖z‖ )2 9800 (αlbz )\n4︸ ︷︷ ︸ ϑ1\n‖h‖2 ‖z‖2 +  ‖h‖ 2 . (78)\nFurthermore, under the condition that\nγ ≤ αlbz ‖z‖ ‖h‖ and ‖h‖ ‖z‖ ≤\n√ 98 ( αlbz )2 √ 3 (2αubz + α lb z ) ,\none can simplify (78) by observing that ϑ1 ≤ 1100 , which results in\n1\nm m∑ i=1 ∣∣a>i h∣∣3∣∣a>i z∣∣ 1Ei1∩Di,2γ ≤ ( 1 100 + ) ‖h‖2 . (79)\nPutting all preceding results in this subsection together reveals that with probability exceeding 1 − exp (−Ω (m)),\n− 〈 h, 1\n2m ∇`tr (z)\n〉 ≥ { 1.99− 2 (ζ1 + ζ2)− √ 8/π\n‖h‖ αlbz ‖z‖\n− 3 } ‖h‖2\n≥ { 1.99− 2 (ζ1 + ζ2)− √ 8/π(3αh) −1 − 3 } ‖h‖2 (80)\nholds simultaneously over all x and z satisfying\n‖h‖ ‖z‖ ≤ min { αlbz 3αh , αlbz 6 , √ 98/3 ( αlbz )2 2αubz + α lb z , 1 11 } (81)\nas claimed in Proposition 2. To conclude this section, we provide a tighter estimate about the norm of the truncated gradient.\nLemma 7. Fix δ > 0, and assume that yi = (a>i x)2. Suppose that m ≥ c0n for some large constant c0 > 0. There exist some universal constants c, C > 0 such that with probability at least 1− C exp (−cm),\n1\nm ∥∥∇`tr (z)∥∥ ≤ (1 + δ) · 4√1.02 + 0.665/αh ‖h‖ (82) holds simultaneously for all x, z ∈ Rn satisfying ‖h‖‖z‖ ≤ min { αlbz 3αh , αlbz 6 , √ 98/3(αlbz ) 2 2αubz +α lb z , 111 } .\nLemma 7 complements the preceding arguments by allowing us to identify a concrete plausible range for the step size. Specifically, putting Lemma 7 and Proposition 2 together suggests that\n− 〈 h, 1\nm ∇`tr (z)\n〉 ≥\n2 { 1.99− 2 (ζ1 + ζ2)− √ 8/(9π)α−1h − }\n(1 + δ) 2 · 16 (1.02 + 0.665/αh)\n∥∥∥∥ 1m∇`tr (z) ∥∥∥∥2 . (83)\nTaking and δ to be sufficiently small we arrive at a feasible range (cf. Definition (40)) µ ≤ 0.994− ζ1 − ζ2 − √\n2/(9π)α−1h 2 (1.02 + 0.665/αh) := µ0. (84)\nThis establishes Proposition 1 and in turn Theorem 1 when µt is taken to be a fixed constant. To justify the contraction under backtracking line search, it suffices to prove that the resulting step size falls within this range (84), which we defer to Appendix D."
    }, {
      "heading" : "6 Stability",
      "text" : "This section goes in the direction of establishing stability guarantees of TWF. We concentrate on the iterative gradient stage, and defer the analysis for the initialization stage to Appendix C.\nBefore continuing, we collect two bounds that we shall use several times. The first is the observation that\n1 m ‖y −A(zz>)‖1 ≤ 1 m ‖A(xx> − zz>)‖1 + 1 m ‖η‖1 . ‖h‖‖z‖+ 1 m ‖η‖1 . ‖h‖‖z‖+ 1√ m ‖η‖, (85)\nwhere the last inequality follows from Cauchy-Schwarz. Setting\nvi := 2 yi − |a>i z|2\na>i z 1Ei1∩Ei2\nas usual, this inequality together with the truncation rules E i1 and E21 give\n|vi| . ‖h‖+ ‖η‖√m‖z‖ =⇒ ∥∥ 1 m∇`tr(z) ∥∥ = 1m‖A>v‖ ≤ ∥∥∥ 1√mA∥∥∥ 1√m‖v‖ (i). 1√m‖v‖ . ‖h‖+ ‖η‖√m‖z‖ , (86)\nwhere (i) arises from [44, Corollary 5.35]. As discussed in Section 3, the estimation error is contractive if − 1m∇`tr (z) satisfies the regularity condition. With (86) in place, RC reduces to\n− 1 m 〈∇`tr (z) ,h〉 & ‖h‖2. (87)\nUnfortunately, (87) does not hold for all z within the neighborhood of x due to the existence of noise. Instead we establish the following:\n• The condition (87) holds for all h obeying\nc3 ‖η‖ /√m ‖z‖ ≤ ‖h‖ ≤ c4‖x‖ (88)\nfor some constants c3, c4 > 0 (we shall call it Regime 1); this will be proved later. In this regime, the reasoning in Section 3 gives\ndist ( z + µ\nm ∇`tr(z), x\n) ≤ (1− ρ)dist(z,x) (89)\nfor some appropriate constants µ, ρ > 0 and, hence, error contraction occurs as in the noiseless setting.\n• However, once the iterate enters Regime 2 where\n‖h‖ ≤ c3 ‖η‖√ m ‖z‖ (90)\nthe estimation error might no longer be contractive. Fortunately, in this regime each move by µm∇`tr (z) is of size at most O( ‖η‖√\nm‖z‖ ), compare (86). As a result, at each iteration the estimation error cannot\nincrease by more than a numerical constant times ‖η‖√ m‖z‖ before possibly jumping out (of this regime). Therefore,\ndist ( z + µ\nm ∇`tr(z), x\n) ≤ c5\n‖η‖√ m‖x‖ (91)\nfor some constant c5 > 0. Moreover, as long as ‖η‖∞/‖x‖2 is sufficiently small, one can guarantee that c5 ‖η‖√ m‖x‖ ≤ c5 ‖η‖∞ ‖x‖ ≤ c4‖x‖. In other words, if the iterate jumps out of Regime 2, it will still fall within Regime 1.\nTo summarize, suppose the initial guess z(0) obeys dist(z(0),x) ≤ c4‖x‖. Then the estimation error will shrink at a geometric rate 1 − ρ before it enters Regime 2. Afterwards, z(t) will either stay within Regime 2 or jump back and forth between Regimes 1 and 2. Because of the bounds (91) and (89), the estimation errors will never exceed the order of ‖η‖√\nm‖x‖ from then on. Putting these together establishes (17), namely, the first part of the theorem.\nBelow we justify the condition (87) for Regime 1, for which we start by gathering additional properties of the truncation rules. By Cauchy-Schwartz, 1m ‖η‖1 ≤ 1√m ‖η‖ ≤ 1 c3 ‖h‖ ‖z‖. When c3 is sufficiently large, applying Lemmas 1 and 2 gives\n1 m ∑m l=1 ∣∣∣yl − ∣∣a>l z∣∣2∣∣∣ ≤ 1m ∥∥A (xx> − zz>)∥∥1 + 1m ‖η‖1 ≤ 2.98‖h‖‖z‖; 1 m ∑m l=1\n∣∣∣yl − ∣∣a>l z∣∣2∣∣∣ ≥ 1m ∥∥A (xx> − zz>)∥∥1 − 1m ‖η‖1 ≥ 1.151‖h‖‖z‖. (92) From now on, we shall denote Ẽ i2 :=\n{ ∣∣|a>i x|2 − |a>i z|2∣∣ ≤ αhm ∥∥y −A (zz>)∥∥1 |a>i z|‖z‖ } to differentiate from E i2. For any small constant > 0, we introduce the index set G := {i : |ηi| ≤ C ‖η‖ / √ m} that satisfies |G| = (1− )m. Note that C must be bounded as n scales, since\n‖η‖2 ≥ ∑\ni/∈G η2i ≥ (m− |G|) · C2 ‖η‖2/m ≥ C2 ‖η‖2 ⇒ C ≤ 1/\n√ . (93)\nWe are now ready to analyze the truncated gradient, which we separate into several components as follows\n∇tr` (z) = 2 ∑ i∈G ∣∣a>i x∣∣2 − ∣∣a>i z∣∣2 a>i z ai1Ei1∩Ei2 + 2 ∑ i/∈G ∣∣a>i x∣∣2 − ∣∣a>i z∣∣2 a>i z\nai1Ei1∩Ẽi2︸ ︷︷ ︸ :=∇cleantr `(z)\n+ 2 ∑ i∈G ηi a>i z\nai1Ei1∩Ei2︸ ︷︷ ︸ :=∇noisetr `(z)\n+ 2 ∑ i/∈G\n( yi − ∣∣a>i z∣∣2 a>i z 1Ei1∩Ei2 − ∣∣a>i x∣∣2 − ∣∣a>i z∣∣2 a>i z 1Ei1∩Ẽi2 ) ai︸ ︷︷ ︸\n:=∇extratr `(z)\n. (94)\n• For each index i ∈ G, the inclusion property (52) (i.e. E i3 ⊆ E i2 ⊆ E i4) holds. To see this, observe that∣∣yi − |a>i z|2∣∣ ∈ ∣∣|a>i x|2 − |a>i z|2∣∣± |ηi|. Since |ηi| ≤ C ‖η‖/ √ m ‖h‖‖z‖ when c3 is sufficiently large, one can derive the inclusion (52)\nimmediately from (92). As a result, all the proof arguments for Proposition 2 carry over to ∇cleantr ` (z), suggesting that\n− 〈 h, 1\nm ∇cleantr ` (z)\n〉 ≥ 2 { 1.99− 2 (ζ1 + ζ2)− √ 8/(9π)α−1h − } ‖h‖2. (95)\n• Next, letting wi = 2ηia>i z1Ei1∩Ei21{i∈G}, we see that for any constant δ > 0, the noise component obeys∥∥∥∥ 1m∇noisetr `(z) ∥∥∥∥ = ∥∥∥∥ 1mA>w ∥∥∥∥ ≤ ∥∥∥∥ 1√mA ∥∥∥∥ ∥∥∥∥ 1√mw ∥∥∥∥ (ii)≤ 1 + δ√m ‖w‖ ≤ (1 + δ)2‖η‖/ √ m αlbz ‖z‖ , (96)\nwhen m/n is sufficiently large. Here, (ii) arises from [44, Corollary 5.35], and the last inequality is a consequence of the upper estimate\n‖w‖2 ≤ 4 m∑ i=1 |ηi|2 (a>i z) 2 1Ei1∩Ei2 ≤ 4 m∑ i=1 |ηi|2 (αlbz ‖z‖)2 = 4 ‖η‖2 (αlbz ‖z‖)2 . (97)\nIn turn, this immediately gives∣∣∣∣〈h, 1m∇noisetr ` (z)〉 ∣∣∣∣ ≤ ‖h‖∥∥∥∥ 1m∇noisetr ` (z) ∥∥∥∥ ≤ 2 (1 + δ)αlbz ‖η‖√m‖z‖‖h‖. (98) • We now turn to the last term∇extratr ` (z). According to the definition of E i2 and Ẽ i2 as well as the property\n(92), the weight qi := 2 ( yi−|a>i z|2 a>i z 1Ei1∩Ei2 − |a>i x|2−|a>i z|2 a>i z 1Ei1∩Ẽi2 ) 1{i/∈G} is bounded in magnitude by 6‖h‖. This gives ‖q‖ ≤ √ m− |G| · 6‖h‖ ≤ 6√ m‖h‖,\n⇒ ∣∣∣〈 1 m ∇extratr ` (z) ,h 〉∣∣∣ ≤ ‖h‖ · ∥∥ 1 m ∇extratr ` (z) ∥∥ = 1 m ‖h‖ · ∥∥A>q∥∥ ≤ 6 (1 + δ)√ ‖h‖2. (99) Taking the above bounds together yields\n− 1 m 〈∇`tr (z) ,h〉 ≥ 2\n{ 1.99− 2 (ζ1 + ζ2)− √ 8\n9π\n1 αh − 6(1 + δ)√ −\n} ‖h‖2 − 2 (1 + δ)\nαlbz\n‖η‖√ m ‖z‖‖h‖.\nSince ‖h‖ ≥ c3 ‖η‖√m‖z‖ for some large constant c3 > 0, setting to be small one obtains\n− 1 m 〈∇`tr (z) ,h〉 ≥ 2\n{ 1.95− 2 (ζ1 + ζ2)− √ 8/(9π)α−1h } ‖h‖2 (100)\nfor all h obeying\nc3‖η‖/ √ m ‖z‖ ≤ ‖h‖ ≤ min { 1 11 , αlbz 3αh , αlbz 6 , √ 98/3 ( αlbz )2 2αubz + α lb z } ‖z‖,\nwhich finishes the proof of Theorem 2 for general η. Up until now, we have established the theorem for general η, and it remains to specialize it to the Poisson model. Standard concentration results, which we omit, give\n1 m ‖η‖2 ≈ 1 m m∑ i=1 E [ η2i ] = 1 m m∑ i=1 ( a>i x )2 ≈ ‖x‖2. (101) Substitution into (17) completes the proof."
    }, {
      "heading" : "7 Minimax lower bound",
      "text" : "The goal of this section is to establish the minimax lower bound given in Theorem 3. For notational simplicity, we denote by P (y | w) the likelihood of yi ind.∼ Poisson(|a>i w|2), 1 ≤ i ≤ m conditional on {ai}. For any two probability measures P and Q, we denote by KL (P‖Q) the Kullback–Leibler (KL) divergence between them:\nKL (P‖Q) := ˆ log\n( dP\ndQ\n) dP, (102)\nThe basic idea is to adopt the general reduction scheme discussed in [43, Section 2.2], which amounts to finding a finite collection of hypotheses that are minimally separated. Below we gather one result useful for constructing and analyzing such hypotheses.\nLemma 8. Suppose that ai ∼ N (0, In), n is sufficiently large, and m = κn for some sufficiently large constant κ > 0. Consider any x ∈ Rn\\{0}. On an event B of probability approaching one, there exists a collectionM of M = exp (n/30) distinct vectors obeying the following properties: (i) x ∈M; (ii) for all w(l),w(j) ∈M,\n1/ √ 8− (2n)−1/2 ≤ ∥∥w(l) −w(j)∥∥ ≤ 3/2 + n−1/2; (103)\n(iii) for all w ∈M, |a>i (w − x) |2 |a>i x|2 ≤ ‖w − x‖ 2 ‖x‖2 {2 + 17 log 3m}, 1 ≤ i ≤ m. (104)\nIn words, Lemma 8 constructs a set M of exponentially many vectors/hypotheses scattered around x and yet well separated. From (ii) we see that each pair of hypotheses in M is separated by a distance roughly on the order of 1, and all hypotheses reside within a spherical ball centered at x of radius 3/2+o(1). When ‖x‖ ≥ log1.5m, every hypothesis w ∈ M satisfies ‖w‖ ≈ ‖x‖ 1. In addition, (iii) says that the quantities |a>i (w − x) |/|a>i x| are all very well controlled (modulo some logarithmic factor). In particular, when ‖x‖ ≥ log1.5m, one must have\n|a>i (w − x) |2 |a>i x|2 . ‖w − x‖2 ‖x‖2 log 3m . 1 log3m log3m . 1. (105)\nIn the Poisson model, such a quantity turns out to be crucial in controlling the information divergence between two hypotheses, as demonstrated in the following lemma.\nLemma 9. Fix a family of design vectors {ai}. Then for any w and r ∈ Rn,\nKL ( P (y | w + r) ‖ P (y | w) ) ≤ ∑m\ni=1 |a>i r|2\n( 8 +\n2|a>i r|2 |a>i w|2\n) . (106)\nLemma 9 and (105) taken collectively suggest that on the event B∩C (B is in Lemma 8 and C := {‖A‖ ≤√ 2m}), the conditional KL divergence (we condition on the ai’s) obeys\nKL ( P (y | w) ‖ P (y | x) ) ≤ c3 ∑m i=1 ∣∣a>i (w − x)∣∣2 ≤ 2c3m ‖w − x‖2 , ∀w ∈M; (107) here, the inequality holds for some constant c3 > 0 provided that ‖x‖ ≥ log1.5m, and the last inequality is a result of C (which occurs with high probability). We now use hypotheses as in Lemma 8 but rescaled in such a way that ‖w − x‖ δ, and ‖w − w̃‖ δ, ∀w, w̃ ∈M with w 6= w̃. (108) for some 0 < δ < 1. This is achieved via the substitution w ←− x+δ(w−x); with a slight abuse of notation, M denotes the new set.\nThe hardness of a minimax estimation problem is known to be dictated by information divergence inequalities such as (107). Indeed, suppose that\n1 M − 1 ∑ w∈M\\{x} KL ( P (y | w) ‖ P (y | x) ) ≤ 1 10 log (M − 1) (109)\nholds, then the Fano-type minimax lower bound [43, Theorem 2.7] asserts that\ninf x̂ sup x∈M\nE [ ‖x̂− x‖ ∣∣ {ai}] & min w,w̃∈M,w 6=w̃ ‖w − w̃‖. (110)\nSince M = exp(n/30), (109) would follow from\n2c3‖w − x‖2 ≤ n/(300m). w ∈M. (111)\nHence, we just need to select δ to be a small multiple of √ n/m. This in turn gives\ninf x̂ sup x∈M\nE [ ‖x̂− x‖ ∣∣ {ai}] & min w,w̃∈M,w 6=w̃ ‖w − w̃‖ & √ n/m. (112)\nFinally, it remains to connect ‖x̂ − x‖ with dist (x̂,x). Since all the w ∈ M are clustered around x and are at a mutual distance about δ that is much smaller than ‖x‖, we can see that for any reasonable estimator, dist(x̂,x) = ‖x̂− x‖. This finishes the proof."
    }, {
      "heading" : "8 Discussion",
      "text" : "To keep our treatment concise, this paper does not strive to explore all possible generalizations of the theory. There are nevertheless a few extensions worth pointing out.\n• More general objective functions. For concreteness, we restrict our analysis to the Poisson loglikelihood function, but the analysis framework we laid out easily carries over to a broad class of (nonconvex) objective functions. For instance, all results continue to hold if we replace the Poisson likelihood by the Gaussian likelihood; that is, the polynomial function −∑mi=1(yi − |a>i z|2)2 studied in [11]. A general guideline is to first check whether the expected regularity condition\nE [ − 〈\n1 m∇`tr (z) ,h\n〉] & ‖h‖2\nholds for any fixed z within a neighborhood around x. If so, then often times RC holds uniformly within this neighborhood due to sharp concentration of measure ensured by the truncation procedure.\n• Sub-Gaussian measurements. The theory extends to the situation where the ai’s are i.i.d. subGaussian random vectors, although the truncation threshold might need to be tweaked based on the sub-Gaussian norm of ai. A more challenging scenario, however, is the case where the ai’s are generated according to the CDP model, since there is much less randomness to exploit in the mathematical analysis. We leave this to future research.\nHaving demonstrated the power of TWF in recovering a rank-one matrix xx∗ from quadratic equations, we remark on the potential of TWF towards recovering low-rank matrices from rank-one measurements. Imagine that we wish to estimate a rank-r matrix X 0 and that all we know about X is\nyi = a > i Xai, 1 ≤ i ≤ m.\nIt is known that this problem can be efficiently solved by using more computational-intensive semidefinite programs [8,14]. With the hope of developing a linear-time algorithm, one might consider a modified TWF scheme, which would maintain a rank-r matrix variable and operate as follows: perform truncated spectral initialization, and then successively update the current guess via a truncated gradient descent rule applied to a presumed log-likelihood function.\nMoving away from i.i.d. sub-Gaussian measurements, there is a proliferation of problems that involve completion of a low-rank matrix X from partial entries, where the rank is known a priori. It is selfevident that such entry-wise observations can also be cast as rank-one measurements of X. Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33]. A concrete application of this flavor is a simple form of the fundamental alignment/matching problem [6, 15, 25]. Imagine a collection of n instances, each representing an image of the same physical object but with different shift ri ∈ {0, · · · ,M − 1}. The goal is to align all these instances from observations on the relative shift between pairs of them. Denoting by Xi the cyclic shift by an amount ri of IM , one sees that the collection matrix X := [X>i Xj ]1≤i,j≤k is a rank-M matrix, and the relative shift observations can be treated as rank-one measurements of X. Running TWF over this problem instance might result in a statistically and computationally efficient solution. This would be of great practical interest."
    }, {
      "heading" : "A Proofs for Section 5",
      "text" : "A.1 Proof of Lemma 3 First, we make the observation that (a>i z)2− (a>i x)2 = ( 2a>i z − a>i h ) a>i h is a quadratic function in a>i h. If we assume γ ≤ α lb z ‖z‖ ‖h‖ , then on the event E i1 one has\n(a>i z) 2 ≥ αlbz ‖z‖ · |a>i z| ≥ γ ‖h‖ ∣∣a>i z∣∣ . (113) Solving the quadratic inequality that specifies Diγ gives\na>i h ∈ [ a>i z − √( a>i z )2 + γ ‖h‖ ∣∣a>i z∣∣, a>i z −√(a>i z)2 − γ ‖h‖ ∣∣a>i z∣∣] , or a>i h ∈ [ a>i z + √( a>i z\n)2 − γ ‖h‖ ∣∣a>i z∣∣, a>i z +√(a>i z)2 + γ ‖h‖ ∣∣a>i z∣∣] , which we will simplify in the sequel.\nSuppose for the moment that a>i z ≥ 0, then the preceding two intervals are respectively equivalent to\na>i h ∈  − γ ‖h‖ ∣∣a>i z∣∣ a>i z + √( a>i z )2 + γ ‖h‖ ∣∣a>i z∣∣ , γ ‖h‖ ∣∣a>i z∣∣ a>i z + √( a>i z )2 − γ ‖h‖ ∣∣a>i z∣∣  := I1;\na>i h− 2a>i z ∈  − γ ‖h‖ ∣∣a>i z∣∣ a>i z + √( a>i z )2 − γ ‖h‖ ∣∣a>i z∣∣ , γ ‖h‖ ∣∣a>i z∣∣ a>i z + √( a>i z )2 + γ ‖h‖ ∣∣a>i z∣∣  := I2.\nAssuming (113) and making use of the observations γ ‖h‖ ∣∣a>i z∣∣\na>i z + √( a>i z )2 − γ ‖h‖ ∣∣a>i z∣∣ ≤ γ ‖h‖ ∣∣a>i z∣∣ a>i z = γ ‖h‖\nand γ ‖h‖ ∣∣a>i z∣∣ a>i z + √( a>i z )2 + γ ‖h‖ ∣∣a>i z∣∣ ≥ γ ‖h‖ ∣∣a>i z∣∣( 1 + √ 2 ) ∣∣a>i z∣∣ = γ1 +√2 ‖h‖ ,\nwe obtain the inner and outer bounds[ ± ( 1 + √ 2 )−1 γ ‖h‖ ] ⊆ I1, I2 ⊆ [ ± γ ‖h‖ ] .\nSetting γ1 := γ1+√2 gives( Di,1γ1 ∩ Ei,1 ) ∪ ( Di,2γ1 ∩ Ei,1 ) ⊆ Dγ ∩ Ei,1 ⊆ ( Di,1γ ∩ Ei,1 ) ∪ ( Di,2γ ∩ Ei,1 ) .\nProceeding with the same argument, we can derive exactly the same inner and outer bounds in the regime where a>i z < 0, concluding the proof.\nA.2 Proof of Lemma 4 By homogeneity, it suffices to establish the claim for the case where both h and z are unit vectors.\nSuppose for the moment that h and z are statistically independent from {ai}. We introduce two auxiliary Lipschitz functions approximating indicator functions:\nχz (τ) :=  1, if |τ | ∈ [√ 1.01αlbz , √ 0.99αubz ] ; −100 ( αubz )−2 τ2 + 100, if |τ | ∈ [√ 0.99αubz , α ub z ] ; 100 ( αlbz )−2 τ2 − 100, if |τ | ∈ [ αlbz , √ 1.01αlbz ] ;\n0, else.\n(114)\nχh (τ) :=  1, if |τ | ∈ [ 0, √ 0.99γ ] ; − 100γ2 τ2 + 100, if |τ | ∈ [√ 0.99γ, γ ] ;\n0, else. (115)\nSince h and z are assumed to be unit vectors, these two functions obey\n0 ≤ χz ( a>i z ) ≤ 1Ei1 , and 0 ≤ χh ( a>i h ) ≤ 1Di,1γ (116)\nand thus,\n1\nm m∑ i=1 ( a>i h )2 1Ei1∩D i,1 γ ≥ 1 m m∑ i=1 (a>i h) 2χz(a > i z)χh(a > i h). (117)\nWe proceed to lower bound 1m ∑m i=1 ( a>i h )2 χz ( a>i z ) χh ( a>i h ) .\nFirstly, to compute the mean of (a>i h)2χz(a>i z)χh(a>i h), we introduce an auxiliary orthonormal matrix\nUz =\n[ z>/ ‖z‖\n...\n] (118)\nwhose first row is along the direction of z, and set\nh̃ := Uzh, and ãi := Uzai. (119)\nAlso, denote by ãi,1 (resp. h̃1) the first entry of ãi (resp. h̃), and ãi,\\1 (resp. h̃\\1) the remaining entries of ãi (resp. h̃), and let ξ ∼ N (0, 1). We have\nE [ ( a>i h )2 χz ( a>i z ) χh ( a>i h ) ] ≥ E [ (a>i h) 2χz ( a>i z ) ] − E [( a>i h )2 ( 1− χh ( a>i h ))] ≥ E [( ãi,1h̃1 )2 χz ( a>i z )] + E [( ã>i,\\1h̃\\1\n)2]E [χz (a>i z)]− ‖h‖2 E [ξ21{|ξ|>√0.99γ}] ≥ |h̃1|2(1− ζ1) + ‖h̃\\1‖2(1− ζ1)− ζ2‖h‖2 (120) ≥ (1− ζ1 − ζ2) ‖h‖2 ,\nwhere the identity (120) arises from (67) and (68). Since ( a>i h )2 χz ( a>i z ) χh ( a>i h ) is bounded in magnitude by γ2 ‖h‖2, it is a sub-Gaussian random variable with sub-Gaussian norm O(γ2 ‖h‖2). Apply the Hoeffdingtype inequality [44, Proposition 5.10] to deduce that for any > 0,\n1\nm m∑ i=1 ( a>i h )2 χz ( a>i z ) χh ( a>i h ) ≥ E [( a>i h )2 χz ( a>i z ) χh ( a>i h )] − ‖h‖2 (121)\n≥ (1− ζ1 − ζ2 − ) ‖h‖2 (122)\nwith probability at least 1− exp(−Ω( 2m)). The next step is to obtain uniform control over all unit vectors, for which we adopt a basic version of an\n-net argument. Specifically, we construct an -net N with cardinality |N | ≤ (1 + 2/ )2n (cf. [44]) such that for any (h, z) with ‖h‖ = ‖z‖ = 1, there exists a pair h0, z0 ∈ N satisfying ‖h− h0‖ ≤ and ‖z − z0‖ ≤ . Now that we have discretized the unit spheres using a finite set, taking the union bound gives\n1\nm m∑ i=1 ( a>i h0 )2 χz ( a>i z0 ) χh ( a>i h0 ) ≥ (1− ζ1 − ζ2 − ) ‖h0‖2 , ∀h0, z0 ∈ N (123)\nwith probability at least 1− (1 + 2/ )2n exp(−Ω( 2m)). Define f1(·) and f2(·) such that f1(τ) := τχh( √ τ) and f2(τ) := χz( √ τ), which are both bounded functions with Lipschitz constant O(1). This guarantees that for each unit vector pair h and z,∣∣∣(a>i h)2 χz (a>i z)χh (a>i h)− (a>i h0)2 χz (a>i z0)χh (a>i h0)∣∣∣ ≤ |χh ( a>i z ) | · | ( a>i h )2 χh ( a>i h ) − ( a>i h0 )2 χh ( a>i h0 ) |+ |(a>i h0)2χh ( a>i h0 ) | · |χh ( a>i z ) − χh ( a>i z0 ) |\n≤ |χh ( a>i z ) | · ∣∣f1(|a>i h|2)− f1(|a>i h0|2)∣∣+ ∣∣(a>i h0)2χh (a>i h0) ∣∣ · ∣∣f2 (|a>i z|2)− f2 (|a>i z0|2) ∣∣\n. ∣∣(a>i h)2 − (a>i h0)2∣∣+ (a>i z)2 − (a>i z0)2∣∣.\nConsequently, there exists some universal constant c3 > 0 such that∣∣∣ 1 m m∑ i=1 ( a>i h )2 χz ( a>i z ) χh ( a>i h ) − 1 m m∑ i=1 ( a>i h0 )2 χz ( a>i z0 ) χh ( a>i h0 ) ∣∣∣ . 1\nm ∥∥∥A(hh> − h0h>0 )∥∥∥ 1 + 1 m ∥∥A(zz> − z0z>0 )∥∥1 (i) ≤ c3\n{∥∥hh> − h0h>0 ∥∥F + ∥∥zz> − z0z>0 ∥∥F} (ii) ≤ 2.5c3\n{∥∥h− h0∥∥ · ∥∥h∥∥+ ∥∥z − z0∥∥ · ∥∥z∥∥} ≤ 5c3 , where (i) results from Lemma 1, and (ii) arises from Lemma 2 whenever < 1/2.\nWith the assertion (123) in place, we see that with high probability,\n1\nm m∑ i=1 ( a>i h )2 χz ( a>i z ) χh ( a>i h ) ≥ (1− ζ1 − ζ2 − (5c3 + 1) ) ‖h‖2\nfor all unit vectors h and z. Since can be arbitrary, putting this and (117) together completes the proof.\nA.3 Proof of Lemma 5 The proof makes use of standard concentration of measure and covering arguments, and it suffices to restrict our attention to unit vectors h. We find it convenient to work with an auxiliary function\nχ2 (τ) =  |τ | 32 , if |τ | ≤ γ2, −γ ( |τ | − γ2 ) + γ3, if γ2 < |τ | ≤ 2γ2,\n0, else.\nApparently, χ2 (τ) is a Lipschitz function of τ with Lipschitz norm O (γ). Recalling the definition of Di,1γ , we see that each summand is bounded above by\n|a>i h|3 1Di,1γ ≤ χ2 ( |a>i h|2 ) .\nFor each fixed h and > 0, applying the Bernstein inequality [44, Proposition 5.16] gives\n1\nm m∑ i=1 ∣∣a>i h∣∣3 1Di,1γ ≤ 1m m∑ i=1 χ2 (∣∣a>i h∣∣2) ≤ E [χ2 (∣∣a>i h∣∣2)]+ ≤ E\n[ ∣∣a>i h∣∣3 ]+ = √8/π + with probability exceeding 1− exp ( −Ω ( 2m )) .\nFrom [44, Lemma 5.2], there exists an -net N of the unit sphere with cardinality |N | ≤ ( 1 + 2 )n. For each h, suppose that ‖h0 − h‖ ≤ for some h0 ∈ N . The Lipschitz property of χ2 implies\n1\nm m∑ i=1 { χ2 (∣∣a>i h∣∣2)− χ2 (∣∣a>i h0∣∣2)} . 1m m∑ i=1 ∣∣∣∣∣a>i h∣∣2 − ∣∣a>i h0∣∣2∣∣∣ (i) ‖h− h0‖ ‖h‖ , where (i) arises by combining Lemmas 1 and 2. This demonstrates that with high probability,\n1\nm m∑ i=1 ∣∣a>i h∣∣3 1Di,1γ ≤ 1m m∑ i=1 χ2 ( |a>i h|2 ) ≤ √ 8/π +O ( )\nfor all unit vectors h, as claimed.\nA.4 Proof of Lemma 6 Without loss of generality, the proof focuses on the case where ‖h‖ = 1. Fix an arbitrary small constant δ > 0. One can eliminate the difficulty of handling the discontinuous indicator functions by working with the following auxiliary function\nχ3 (τ, γ) :=  1, if √ τ ≥ ψlb (γ) ; 100τ ψ2lb(γ) − 99, if √τ ∈ [√ 0.99ψlb (γ) , ψlb (γ) ] ;\n0, else. (124)\nHere, ψlb (·) is a piecewise constant function defined as\nψlb (γ) := (1 + δ) b log γlog(1+δ)c ,\nwhich clearly satisfy γ1+δ ≤ ψlb (γ) ≤ γ. Such a function is useful for our purpose since for any 0 < δ ≤ 0.005,\n1{|a>i h|≥γ} ≤ χ3 ( ∣∣a>i h∣∣2 , γ) ≤ 1{|a>i h|≥√0.99ψlb(γ)} ≤ 1{|a>i h|≥0.99γ}. (125)\nFor any fixed unit vector h, the above argument leads to an upper tail estimate: for any 0 < t ≤ 1,\nP { χ3 ( ∣∣a>i h∣∣2 , γ) ≥ t} ≤ P{1{|a>i h|≥0.99γ} ≥ t} = P{1{|a>i h|≥0.99γ} = 1}\n= 2 ˆ ∞ 0.99γ φ (x) dx ≤ 2 0.99γ φ (0.99γ) , (126)\nwhere φ(x) is the density of a standard normal, and (126) follows from the tail bound ´∞ x φ(x)dx ≤ 1xφ (x)\nfor all x > 0. This implies that when γ ≥ 2, both χ3 ( |a>i h|2, γ ) and 1{|a>i h|≥0.99γ} are sub-exponential with sub-exponential norm O(γ−2) (cf. [44, Definition 5.13]). We apply the Bernstein-type inequality for the sum\nof sub-exponential random variables [44, Corollary 5.17], which indicates that for any fixed h and γ as well as any sufficiently small ∈ (0, 1),\n1\nm m∑ i=1 χ3 ( ∣∣a>i h∣∣2 , γ) ≤ 1m m∑ i=1 1{|a>i h|≥0.99γ} ≤ E [ 1{|a>i h|≥0.99γ} ] + 1 γ2\n≤ 2 0.99γ\nexp ( −0.49γ2 ) + 1\nγ2 holds with probability exceeding 1− exp ( −Ω( 2m) ) .\nWe now proceed to obtain uniform control over all h and 2 ≤ γ ≤ 2n. To begin with, we consider all 2 ≤ γ ≤ m and construct an -net N over the unit sphere such that: (i) |N | ≤ ( 1 + 2 )n; (ii) for any h with ‖h‖ = 1, there exists a unit vector h0 ∈ N obeying ‖h− h0‖ ≤ . Taking the union bound gives the following: with probability at least 1− logmlog(1+δ) ( 1 + 2 )n exp(−Ω( 2m)),\n1\nm m∑ i=1 χ3 ( ∣∣a>i h0∣∣2 , γ0) ≤ (0.495γ0)−1 exp (−0.49γ20)+ γ−20\nholds simultaneously for all h0 ∈ N and γ0 ∈ { (1 + δ) k | 1 ≤ k ≤ logmlog(1+δ) } .\nNote that χ3 (τ, γ0) is a Lipschitz function in τ with the Lipschitz constant bounded above by 100ψ2lb(γ0) .\nWith this in mind, for any (h, γ) with ‖h‖ = 1 and γ0 := (1 + δ)k ≤ γ < (1 + δ)k+1, one has∣∣∣χ3( ∣∣a>i h0∣∣2 , γ0)− χ3( ∣∣a>i h∣∣2 , γ)∣∣∣ = ∣∣∣χ3( ∣∣a>i h0∣∣2 , γ0)− χ3( ∣∣a>i h∣∣2 , γ0)∣∣∣ ≤ 100\nψ2lb (γ0) ∣∣∣∣∣a>i h∣∣2 − ∣∣a>i h0∣∣2∣∣∣ . It then follows from Lemmas 1-2 that\n1\nm ∣∣∣∣∣ m∑ i=1 χ3 (∣∣a>i h0∣∣2 , γ0)− m∑ i=1 χ3 (∣∣a>i h∣∣2 , γ) ∣∣∣∣∣ ≤ 100ψ2lb (γ0) 1m ∥∥∥A(hh> − h0h>0 )∥∥∥ 1\n≤ 250 (1 + δ) 2\nγ2 ‖h− h0‖‖h‖ ≤\n250(1 + δ)2\nγ2 .\nPutting the above results together gives that for all 2 ≤ γ ≤ (1 + δ) logm log(1+δ) = m,\n1\nm m∑ i=1 χ3 (∣∣a>i h∣∣2 , γ) ≤ 1m m∑ i=1 χ3 (∣∣a>i h0∣∣2 , γ0)+ 250 (1 + δ)2γ2 ≤ 1\n0.495γ0 exp\n( −0.49γ20 ) + 251 (1 + δ) 2\nγ2\n≤ 1 0.49γ\nexp ( −0.485γ2 ) + 251 (1 + δ) 2\nγ2\nwith probability exceeding 1− logmlog(1+δ) ( 1 + 2 )n exp ( −c 2m ) . This establishes (72) for all 2 ≤ γ ≤ m.\nIt remains to deal with the case where γ > m. To this end, we rely on the following observation:\n1\nm m∑ i=1 1{|a>i h|≥m} ≤ 1 m m∑ i=1 ∣∣a>i h∣∣2 m2 (i) ≤ 1 + δ m2 ‖h‖2 1 m , ∀h with ‖h‖ = 1,\nwhere (i) comes from [12, Lemmas 3.1]. This basically tells us that with high probability, none of the indicator variables can be equal to 1. Consequently, 1m ∑m i=1 1{|a>i h|≥m} = 0, which proves the claim.\nA.5 Proof of Lemma 7 Fix δ > 0. Recalling the notation vi := 2 {\n2a>i h− |a>i h|2 a>i z } 1Ei1∩Ei2 , we see from the expansion (63) that∥∥∥ 1\nm ∇tr`(z) ∥∥∥ = ∥∥∥ 1 m A>v ∥∥∥ ≤ 1 m ‖A‖ · ‖v‖ ≤ (1 + δ) ‖v‖√ m (127)\nas soon as m ≥ c1n for some sufficiently large c1 > 0. Here, the norm estimate ‖A‖ ≤ √ m (1 + δ) arises from standard random matrix results [44, Corollary 5.35]. Everything then comes down to controlling ‖v‖. To this end, making use of the inclusion (64) yields\n1 4m ‖v‖2 = 1 m m∑ i=1 ( 2a>i h− |a>i h|2 a>i z )2 1Ei1∩Ei2 ≤ 1 m m∑ i=1 ( 2 ∣∣a>i h∣∣+ |a>i h|2|a>i z| )2 1Ei1∩(D i,1 γ4 ∪Di,2γ4 )\n≤ 1 m m∑ i=1 { 4(a>i h) 2 + ( 4|a>i h|3 |a>i z| + |a>i h|4 |a>i z|2 ) 1Ei1∩(D i,1 γ4 ∪Di,2γ4 ) }\n= 1\nm m∑ i=1 { 4 ( a>i h )2 + ( 4 + |a>i h| |a>i z| ) |a>i h|3 |a>i z| ( 1Ei1∩D i,1 γ4 + 1Ei1∩D i,2 γ4 )} .\nThe first term is controlled by [12, Lemma 3.1] in such a way that with probability 1− exp(−Ω(m)), 1\nm m∑ i=1 4 ( a>i h )2 ≤ 4 (1 + δ) ‖h‖2 . Turning to the remaining terms, we see from the definition of Di,1γ and Di,2γ that∣∣a>i h∣∣∣∣a>i z∣∣ ≤ { γ‖h‖ αlbz ‖z‖ , on E i1 ∩ Di,1γ 2 + γ‖h‖\nαlbz ‖z‖ , on E i1 ∩ Di,2γ\n≤ {\n1, on E i1 ∩ Di,1γ 3, on E i1 ∩ Di,2γ\nas long as γ ≤ α lb z ‖z‖ ‖h‖ . Consequently, one can bound\n1\nm m∑ i=1 ( 4 + |a>i h| |a>i z| ) |a>i h|3 |a>i z| ( 1Ei1∩D i,1 γ + 1Ei1∩D i,2 γ ) ≤ 5\nm m∑ i=1 |a>i h|3 |a>i z| 1Ei1∩D i,1 γ + 7 m m∑ i=1 |a>i h|3 |a>i z| 1Ei1∩D i,2 γ\n≤ 5 (1 + δ) √\n8/π‖h‖3 αlbz ‖z‖ + 7 100 (1 + δ) ‖h‖2 ,\nwhere the last inequality follows from (70) and (79). Recall that γ4 = 3αh. Taken together all these bounds lead to the upper bound\n1\n4m ‖v‖2 ≤ (1 + δ)\n{ 4 + 5 √\n8/π ‖h‖ αlbz ‖z‖ + 7 100\n} ‖h‖2 ≤ (1 + δ) { 4 + 5 √ 8/π\n3αh +\n7\n100\n} ‖h‖2\nwhenever ‖h‖‖z‖ ≤ min { αlbz 3αh , αlbz 6 , √ 98/3(αlbz ) 2 2αubz +α lb z , 111 } . Substituting this into (127) completes the proof."
    }, {
      "heading" : "B Proofs for Section 7",
      "text" : "B.1 Proof of Lemma 8 Firstly, we collect a few results on the magnitudes of a>i x (1 ≤ i ≤ m) that will be useful in constructing the hypotheses. Observe that for any given x and any sufficiently large m,\nP {\nmin 1≤i≤m ∣∣a>i x∣∣ ≥ 1m logm ‖x‖ } = ( P { |a>i x| ≥\n1\nm logm ‖x‖\n})m ≥ (\n1− 2√ 2π\n1\nm logm\n)m ≥ 1− o(1).\nBesides, since E [ 1{|a>i x|≤ ‖x‖5 logm} ] ≤ 1√ 2π 2 5 logm ≤ 15 logm , applying Hoeffding’s inequality yields\nP {∑m\ni=1 1{|a>i x|≤ ‖x‖5 logm} >\nm\n4 logm } = P { 1\nm ∑m i=1 ( 1{|a>i x|≤ ‖x‖5 logm} − E [ 1{|a>i x|≤ ‖x‖5 logm} ]) >\n1\n20 logm\n} ≤ exp ( −Ω ( m\nlog2m\n)) .\nTo summarize, with probability 1− o(1), one has\nmin1≤i≤m ∣∣a>i x∣∣ ≥ 1m logm‖x‖; (128)∑m\ni=1 1{|a>i x|≤ ‖x‖logm} ≤\nm\n4 logm := k. (129)\nIn the sequel, we will first produce a setM1 of exponentially many vectors surrounding x in such a way that every pair is separated by about the same distance, and then verify that a non-trivial fraction of M1 obeys (104). Without loss of generality, we assume that x takes the form x = [b, 0, · · · , 0]> for some b > 0.\nThe construction of M1 follows a standard random packing argument. Let w = [w1, · · · , wn]> be a random vector with\nwi = xi + 1√ 2n zi, 1 ≤ i ≤ n,\nwhere zi ind.∼ N (0, 1). The collectionM1 is then obtained by generating M1 = exp ( n 20 ) independent copies w(l) (1 ≤ l < M1) of w. For any w(l),w(j) ∈M1, the concentration inequality [44, Corollary 5.35] gives\nP { 0.5 √ n− 1 ≤ √n ∥∥w(l) −w(j)∥∥ ≤ 1.5√n+ 1} ≥ 1− 2 exp (−n/8) ; P { 0.5 √ n− 1 ≤ √ 2n ∥∥w(l) − x∥∥ ≤ 1.5√n+ 1} ≥ 1− 2 exp (−n/8) .\nTaking the union bound over all ( M1 2 ) pairs we obtain\n0.5− n−1/2 ≤ ∥∥w(l) −w(j)∥∥ ≤ 1.5 + n−1/2, ∀l 6= j\n1/ √ 8− (2n)−1/2 ≤ ∥∥w(l) − x∥∥ ≤√9/8 + (2n)−1/2, 1 ≤ l ≤M1 (130)\nwith probability exceeding 1− 2M21 exp ( −n8 ) ≥ 1− 2 exp ( − n40 ) .\nThe next step is to show that many vectors inM1 satisfy (104). For any given w with r := w − x, by letting ai,⊥ := [ai,2, · · · , ai,n]>, r‖ := r1, and r⊥ := [r2, · · · , rn]>, we derive\n|a>i r|2 |a>i x|2 ≤ 2|ai,1r‖|2 + 2|a>i,⊥r⊥|2 |ai,1|2 ‖x‖2 ≤ 2|r‖| 2 ‖x‖2 + 2|a>i,⊥r⊥|2 |ai,1|2 ‖x‖2 ≤ 2‖r‖ 2 ‖x‖2 + 2|a>i,⊥r⊥|2 |ai,1|2 ‖x‖2 . (131)\nIt then boils down to developing an upper bound on |a > i,⊥r⊥|2 |ai,1|2\n. This ratio is convenient to work with since the numerator and denominator are stochastically independent. To simplify presentation, we reorder {ai} in a way that\n(m logm)−1 ‖x‖ ≤ ∣∣a>1 x∣∣ ≤ ∣∣a>2 x∣∣ ≤ · · · ≤ ∣∣a>mx∣∣ ;\nthis will not affect our subsequent analysis concerning a>i,⊥r⊥ since it is independent of a > i x.\nTo proceed, we let r(l)⊥ consist of all but the first entry of w (l) −x, and introduce the indicator variables\nξli :=  1{∣∣∣a>i,⊥r(l)⊥ ∣∣∣≤ 1m√n−12n }, 1 ≤ i ≤ k, 1{∣∣∣a>i,⊥r(l)⊥ ∣∣∣≤√ 2(n−1) lognn }, i > k, (132)\nwhere k = m4 logm as before. In words, we divide a > i,⊥r (l) ⊥ , 1 ≤ i ≤ m into two groups, with the first group enforcing far more stringent control than the second group. These indicator variables are useful since any\nw(l) obeying ∏m i=1 ξ l i = 1 will satisfy (104) when n is sufficiently large. To see this, note that for the first group of indices, ξli = 1 requires∣∣∣a>i,⊥r(l)⊥ ∣∣∣ ≤ 1m √ n− 1 2n ≤ 2 m √ n− 1√ n− 2 ∥∥r(l)∥∥ ≤ 3 m\n∥∥r(l)∥∥, 1 ≤ i ≤ k, (133) where the second inequality follows from (130). This taken collectively with (128) and (131) yields∣∣a>i r(l)∣∣2∣∣a>i x∣∣2 ≤ 2‖r(l)‖2 ‖x‖2 + 9 m2 ∥∥r(l)∥∥2 1 m2 log2m ‖x‖2 ≤ (2 + 9 log2m) ∥∥r(l)∥∥2 ‖x‖2 , 1 ≤ i ≤ k.\nRegarding the second group of indices, ξli = 1 gives∣∣∣a>i,⊥r(l)⊥ ∣∣∣ ≤ √ 2 (n− 1) log n n ≤ √ 17 log n ∥∥r(l)∥∥, i = k + 1, · · · ,m, (134)\nwhere the last inequality again follows from (130). Plugging (134) and (129) into (131) gives∣∣a>i r(l)∣∣2∣∣a>i x∣∣2 ≤ 2‖r(l)‖2 ‖x‖2 + 17 ∥∥r(l)∥∥2 log n ‖x‖2 / log2m ≤ (2 + 17 log 3m) ∥∥r(l)∥∥2 ‖x‖2 , i ≥ k + 1.\nConsequently, (104) is satified for all 1 ≤ i ≤ m. It then suffices to guarantee the existence of exponentially many vectors obeying ∏m i=1 ξ l i = 1.\nNote that the first group of indicator variables are quite stringent, namely, for each i only a fraction O(1/m) of the equations could satisfy ξli = 1. Fortunately, M1 is exponentially large, and hence even M1/m k is exponentially large. Put formally, we claim that the first group satisfies\nM1∑ l=1 k∏ i=1 ξli ≥ 1 2\nM1\n(2π) k/2 (1 + 4 √ k/n)k/2\n( 1√\n2πm\n)k := M̃1 (135)\nwith probability exceeding 1− exp (−Ω (k))− exp(−M̃1/4). With this claim in place (which will be proved later), one has\nM1∑ l=1 k∏ i=1 ξli ≥ 1 2 M1 1 (e2m) k = 1 2 exp (( 1 20 − k (2 + logm) n ) n ) ≥ 1 2 exp ( 1 25 n )\nwhen n and m/n are sufficiently large. In light of this, we will let M2 be a collection comprising all w(l) obeying ∏k i=1 ξ l i = 1, which has size M2 ≥ 12 exp ( 1 25n ) based on the preceding argument. For notational simplicity, it will be assumed that the vectors inM2 are exactly w(j) (1 ≤ j ≤M2). We now move on to the second group by examining how many vectors w(j) in M2 further satisfy∏m\ni=k+1 ξ j i = 1. Notably, the above construction of M2 relies only on {ai}1≤i≤k and is independent of the remaining vectors {ai}i>k. In what follows the argument proceeds conditional on M2 and {ai}1≤i≤k. Applying the union bound gives\nE [∑M2\nj=1\n( 1− ∏m i=k+1 ξji )] = ∑M2 j=1 P { ∃i (k < i ≤ m) : ∣∣∣a>i,⊥r(l)⊥ ∣∣∣ > √ 2 (n− 1) log n n } ≤\nM2∑ j=1 m∑ i=k+1 P {∣∣∣a>i,⊥r(l)⊥ ∣∣∣ > √ 2 (n− 1) log n n } ≤ M2m 1 n2 .\nThis combined with Markov’s inequality gives∑M2 j=1 ( 1− ∏m i=k+1 ξji ) ≤ m logm n2 ·M2\nwith probability 1 − o(1). Putting the above inequalities together suggests that with probability 1 − o(1), there exist at least (\n1− m logm n2\n) M2 ≥ 1\n2\n( 1− m logm\nn2\n) exp ( 1\n25 n\n) ≥ exp ( n 30 ) vectors in M2 satisfying ∏m l=k+1 ξ l i = 1. We then choose M to be the set consisting of all these vectors, which forms a valid collection satisfying the properties of Lemma 8. Finally, the only remaining step is to establish the claim (135). To start with, consider an n× k matrix\nB := [b1, · · · , bk] of i.i.d. standard normal entries, and let u ∼ N ( 0, 1nIn ) . Conditional on the {bi’s,\nbu =  b1,u... bk,u  :=  b > 1 u ... b>k u  ∼ N (0, 1 n B>B ) .\nFor sufficiently large m, one has k = m4 logm ≤ 14n. Using [44, Corollary 5.35] we get∥∥∥ 1 n B>B − I ∥∥∥ ≤ 4√k/n (136) with probability 1−exp (−Ω(k)). Thus, for any constant 0 < < 12 , conditional on {bi} and (136) we obtain\nP { k⋂ i=1 { |b>i u| ≤ 1 m }} ≥ (2π)− k2 det− 12 ( 1 n B>B )ˆ bu∈Υ exp ( − 1 2 b>u ( 1 n B>B )−1 bu ) dbu\n≥ (2π)− k2 ( 1 + 4 √ k/n )− k2 ˆ bu∈Υ exp ( − 1 2 ( 1− 4 √ k/n )−1 k∑ i=1 b2i,u ) dbu (137)\n≥ (2π)− k2 ( 1 + 4 √ k/n )− k2 (√2πm)−k, (138) where Υ := {b̃ | |b̃i| ≤ m−1, 1 ≤ i ≤ k} and (137) is a direct consequence from (136).\nWhen it comes to our quantity of interest, the above lower bound (138) indicates that on an event (defined via {ai}) of probability approaching 1, we have\nE [∑M1\nl=1 ∏k i=1 ξli ] ≥ M1 (2π)− k 2 ( 1 + 4 √ k/n )− k2 (√ 2πm )−k . (139)\nSince conditional on {ai}, ∏k i=1 ξ l i are independent across l, applying the Chernoff-type bound [31, Theorem 4.5] gives ∑M1 l=1 ∏k i=1 ξli ≥ M1 2 (2π) − k2 ( 1 + 4 √ k/n )− k2 (√ 2πm\n)−k with probability exceeding 1− exp ( − 18 M1(2π)k/2(1+4√k/n)k/2 ( 1√ 2πm )k ) . This concludes the proof.\nB.2 Proof of Lemma 9 Before proceeding, we introduce the χ2-divergence between two probability measures P and Q as\nχ2 (P‖Q) := ˆ ( dP\ndQ\n)2 dQ− 1. (140)\nIt is well known (e.g. [43, Lemma 2.7]) that\nKL (P‖Q) ≤ log(1 + χ2 (P‖Q)), (141)\nand hence it suffices to develop an upper bound on the χ2 divergence.\nUnder independence, for any w0,w1 ∈ Rn, the decoupling identity of the χ2 divergence [43, Page 96] gives\nχ2 (P (y | w1) ‖ P (y | w0)) = ∏m\ni=1\n( 1 + χ2 (P (yi | w1) ‖ P (yi | w0)) ) − 1\n= exp (∑m i=1 ( |a>i w1|2 − |a>i w0|2 )2 |a>i w0|2 ) − 1. (142)\nThe preceding identity (142) arises from the following computation: by definition of χ2(·‖·),\nχ2 (Poisson (λ1) ‖ Poisson (λ0)) = {∑∞\nk=0\n( λk1 exp (−λ1) )2 λk0 exp (−λ0) k! } − 1\n= exp ( λ0 − 2λ1 +\nλ21 λ0 ){∑∞ k=0 ( λ21/λ0 )k k! exp ( − λ 2 1 λ0 )} − 1 = exp ( (λ1 − λ0)2 λ0 ) − 1.\nSet r := w1 −w0. To summarize,\nKL (P (y | w1) ‖ P (y | w0)) ≤ m∑ i=1\n( |a>i w1|2 − |a>i w0|2 )2 |a>i w0|2\n(143)\n≤ m∑ i=1 ∣∣a>i r∣∣2 (2 ∣∣a>i w0∣∣+ ∣∣a>i r∣∣)2 |a>i w0|2\n= m∑ i=1 |a>i r|2 ( 8|a>i w0|2 + 2|a>i r|2 |a>i w0|2 ) . (144)\nC Initialization via truncated spectral Method This section demonstrates that the truncated spectral method works whenm n, as stated in the proposition below.\nProposition 3. Fix δ > 0 and x ∈ Rn. Consider the model where yi = a>i x+ηi and ai ind.∼ N (0, I). Suppose that ‖η‖∞ ≤ ε ‖x‖ 2 for some sufficiently small constant ε > 0. With probability exceeding 1− exp (−Ω (m)), the solution z(0) returned by the truncated spectral method obeys\ndist(z(0),x) ≤ δ‖x‖, (145)\nprovided that m > c0n for some constant c0 > 0.\nProof. By homogeneity, it suffices to consider the case where ‖x‖ = 1. Recall from [12, Lemma 3.1] that 1 m ∑m i=1(a > i x) 2 ∈ [1± ε]‖x‖2. Under the hypothesis ‖η‖∞ ≤ ε‖x‖2, one has 1m ‖η‖1 ≤ ε‖x‖2, which yields\n1\nm m∑ l=1 yl = 1 m m∑ l=1 ( a>l x )2 + 1 m m∑ l=1 ηl ∈ [1± 2ε]‖x‖2\nwith probability 1− exp(−Ω(m)). This in turn implies that\n1{|(a>i x)2+ηi|≤α2y( 1m ∑l yl)} ≤ 1{|a>i x|2≤α2y( 1m ∑l yl)+|ηi|} ≤ 1{|a>i x|2≤(1+2ε)α2y+ε} 1{|(a>i x)2+ηi|≤α2y( 1m ∑l yl)} ≥ 1{|a>i x|2≤α2y( 1m ∑l yl)−|ηi|} ≥ 1{|a>i x|2≤(1−2ε)α2y−ε}\nand, hence,\n1\nm m∑ i=1 aia > i ( a>i x )2 1{|a>i x|≤ √ (1−2ε)α2y−ε}︸ ︷︷ ︸\n:=Y 2\nY 1 m m∑ i=1 aia > i ( a>i x )2 1{|a>i x|≤ √ (1+2ε)α2y+ε}︸ ︷︷ ︸\n:=Y 1\n. (146)\nLetting ξ ∼ N (0, 1), one can compute\nE [Y 1] = β1xx> + β2I, and E [Y 2] = β3xx> + β4I, (147)\nwhere β1 := E [ ξ41{|ξ|≤ √ (1+2ε)α2y+ε} ] − E [ ξ21{|ξ|≤ √ (1+2ε)α2y+ε} ] , β2 := E [ ξ21{|ξ|≤ √ (1+2ε)α2y+ε} ] , β3 :=\nE [ ξ41{|ξ|≤ √ (1−2ε)α2y−ε} ] − E [ ξ21{|ξ|≤ √ (1−2ε)α2y−ε} ] and β4 := E [ ξ21{|ξ|≤ √ (1−2ε)α2y−ε} ] . Recognizing that aia > i ( a>i x )2 1{|a>i x)|≤c} can be rewritten as bib > i for some sub-Gaussian vector bi := ai ( a>i x ) 1{|a>i x)|≤c}, we apply standard results on random matrices with non-isotropic sub-Gaussian rows [44, Equation (5.26)] to deduce ‖Y 1 − E [Y 1]‖ ≤ δ, ‖Y 2 − E [Y 2]‖ ≤ δ (148) with probability 1 − exp (−Ω (m)), provided that m/n exceeds some large constant. Besides, when ε is sufficiently small, one further has ‖E [Y 1]− E [Y 2] ‖ ≤ δ. These taken together with (146) give\n‖Y − β1xx> − β2I‖ ≤ 3δ. (149)\nFix δ̃ > 0. With (149) in place, repeating the same proof arguments as in [11, Section 7.8] (which we omit in the current paper) and taking δ, ε to be sufficiently small, we obtain\ndist(z(0),x) ≤ δ̃ (150)\nas long as m/n is sufficiently large, as claimed.\nWe now justify that the Poisson model (4) satisfies the condition ‖η‖ ≤ ε‖x‖2 whenever ‖x‖ ≥ log1.5m. Suppose that µi = (a>i x)2 and hence yi ∼ Poisson(µi). It follows from the Chernoff bound that\nP (yi − µi ≥ τ) ≤ E [etyi ]\nexp (t(µi + τ)) =\nexp (µi (e t − 1))\nexp (t(µi + τ)) = exp\n( µi ( et − t− 1 ) − tτ ) , ∀t ≥ 0.\nTaking τ = 2ε̃µi and t = ε̃ for any 0 ≤ ε̃ ≤ 1 gives\nP (yi − µi ≥ 2ε̃µi) ≤ exp ( µi ( et − t− 1− 2ε̃t )) (i) ≤ exp ( µi ( t2 − 2ε̃t )) = exp ( −µiε̃2 ) ,\nwhere (i) follows since et ≤ 1 + t+ t2 (0 ≤ t ≤ 1). Letting κi = µi/‖x‖2 and setting ε̃ = ε/2κi, we obtain\nP ( yi − µi ≥ ε‖x‖2 ) = P (yi − µi ≥ 2ε̃µi) ≤ exp ( −κi‖x‖2ε̃2 ) = exp ( − ε\n2‖x‖2 4κi\n) .\nIn addition, standard results on Gaussian measures indicate that max1≤i≤m κi . log n. As a consequence, if ‖x‖2 & log3m, then ‖x‖ 2\nκi & log2m (1 ≤ i ≤ m), which further gives\nP ( ∀i : ηi ≥ ε‖x‖2 ) = P ( ∀i : yi − µi ≥ ε‖x‖2 ) ≤ m exp ( − Ω ( ε2 log2m ) ) from the union bound. Similarly, applying the same argument on −yi we get ηi ≥ −ε‖x‖2 for all i, which together with (151) establish that ‖η‖∞ ≤ ε‖x‖2 (151) with high probability. In conclusion, the claim (145) applies to the Poisson model."
    }, {
      "heading" : "D Local error contraction with backtracking line search",
      "text" : "In this section, we verify the effectiveness of a backtracking line search strategy by showing local error contraction. To keep it concise, we only sketch the proof for the noiseless case, but the proof extends to the noisy case without much difficulty. Also we do not strive to obtain an optimized constant. For concreteness, we prove the following proposition.\nProposition 4. The claim in Proposition 1 continues to hold if αh ≥ 6, αubz ≥ 5, αlbz ≤ 0.1, αp ≥ 5, and\n‖h‖/‖z‖ ≤ tr (152)\nfor some constant tr > 0 independent of n and m.\nNote that if αh ≥ 6, αubz ≥ 5 and αlbz ≤ 0.1, then the boundary step size µ0 given in Proposition 1 satisfies 0.994− ζ1 − ζ2 − √\n2/(9π)α−1h 2 ( 1.02 + 0.665α−1h ) ≥ 0.384. Thus, it suffices to show that the step size obtained by a backtracking line search lies within (0,0.384). For notational convenience, we will set\np := m−1∇`tr (z) and E i3 := {∣∣a>i z∣∣ ≥ αlbz ‖z‖ and ∣∣a>i p∣∣ ≤ αp ‖p‖}\nthroughout the rest of the proof. We also impose the assumption\n‖p‖ / ‖z‖ ≤ (153)\nfor some sufficiently small constant > 0, so that ∣∣a>i p∣∣ / ∣∣a>i z∣∣ is small for all non-truncated terms. It is self-evident from (80) that in the regime under study, one has\n‖p‖ ≥ 2 { 1.99− 2 (ζ1 + ζ2)− √ 8/π(3αh) −1 − o (1) } ‖h‖ ≥ 3.64 ‖h‖ . (154)\nTo start with, consider three scalars h, b, and δ. Setting bδ := (b+δ)2−b2\nb2 , we get\n(b+ h) 2 log (b+ δ)\n2\nb2 − (b+ δ)2 + b2 = (b+ h)2 log (1 + bδ)− b2bδ\n(i) ≤ (b+ h)2 { bδ − 0.4875b2δ } − b2bδ = ((b+ h)2 − b2)bδ − 0.4875 (b+ h)2 b2δ = hδ (2 + h/b) (2 + δ/b)− 0.4875 (1 + h/b)2 |δ (2 + δ/b)|2\n= 4hδ + 2h2δ\nb +\n2hδ2 b + h2δ2 b2 − 0.4875δ2\n( 1 + h\nb\n)2( 2 + δ\nb\n)2 , (155)\nwhere (i) follows from the inequality log (1 + x) ≤ x− 0.4875x2 for sufficiently small x. To further simplify the bound, observe that\nδ2 ( 1 + h\nb\n)2( 2 + δ\nb\n)2 ≥ 4δ2 ( 1 + h\nb\n)2 + δ2 ( 1 + h\nb\n)2 4δ\nb and\n2hδ2 b + h2δ2 b2 =\n(( 1 + h\nb\n)2 − 1 ) δ2.\nPlugging these two identities into (155) yields\n(155) ≤ 4hδ + 2h 2δ b − ( 0.95 ( 1 + h b )2 + 1 ) δ2 − 0.4875δ2 ( 1 + h b )2 4δ b\n≤ 4hδ − 1.95δ2 + 2h 2 |δ| |b| + 1.9|h| |b| δ 2 + 1.95 ∣∣δ3∣∣ |b| ( 1 + h b )2 .\nReplacing respectively b, δ, and h with a>i z, τa>i p, and −a>i h, one sees that the log-likelihood `i (z) =\nyi log(|a>i z|2)− |a>i z|2 obeys\n`i (z + τp)− `i (z) = yi log ∣∣a>i (z + τp)∣∣2∣∣a>i z∣∣2 − ∣∣a>i (z + τp)∣∣2 + ∣∣a>i z∣∣2 ≤ −4τ ( a>i h ) ( a>i p\n)︸ ︷︷ ︸ :=I1,i − 1.95τ2 ( a>i p )2︸ ︷︷ ︸ :=I2,i\n+ 2τ ( a>i h )2 ∣∣a>i p∣∣∣∣a>i z∣∣︸ ︷︷ ︸ :=I3,i + 1.9τ2 ∣∣a>i h∣∣∣∣a>i z∣∣ (a>i p)2︸ ︷︷ ︸ :=I4,i\n+ 1.95τ3 ∣∣a>i p∣∣3∣∣a>i z∣∣ ( 1− a > i h a>i z )2 ︸ ︷︷ ︸\n:=I5,i\n.\nThe next step is then to bound each of these terms separately. Most of the following bounds are straightforward consequences from [12, Lemma 3.1] combined with the truncation rule. For the first term, applying the AM-GM inequality we get\n1\nm m∑ i=1 I1,i1Ei3 ≤ 4τ 3.64m m∑ i=1 { 3.642 2 ( a>i h )2 + 1 2 ( a>i p )2} ≤ 4τ (1 + δ) 3.64 { 3.642 2 ‖h‖2 + 1 2 ‖p‖2 } .\nSecondly, it follows from Lemma 4 that\n1\nm m∑ i=1 I2,i1Ei3 = −1.95τ 2 1 m m∑ i=1 ( a>i p )2 1Ei3 ≤ −1.95 ( 1− ζ̃1 − ζ̃2 ) τ2 ‖p‖2 ,\nwhere ζ̃1 := max{E [ ξ21{|ξ|≤√1.01αlbz } ] ,E [ 1{|ξ|≤√1.01αlbz } ] } and ζ̃2 := E [ ξ21{|ξ|>√0.99αh} ] . The third term is controlled by 1\nm m∑ i=1 I3,i1Ei3 ≤ 2τ αp ‖p‖ αlbz ‖z‖\n{ 1\nm m∑ i=1 ( a>i h\n)2} . τ ‖h‖2 .\nFourthly, it arises from the AM-GM inequality that\n1\nm m∑ i=1 I4,i1Ei3 ≤ 1.9τ2αp ‖p‖ αlbz ‖z‖ 1 m m∑ i=1 ∣∣a>i h∣∣ ∣∣a>i p∣∣ . τ2 1m m∑ i=1 { 2 ∣∣a>i h∣∣2 + 18 ∣∣a>i p∣∣2 } . τ2 ‖p‖2 .\nFinally, the last term is bounded by\n1\nm m∑ i=1 I5,i1Ei3 ≤ 1 m m∑ i=1 1.95τ3 ∣∣a>i p∣∣3∣∣a>i z∣∣ ( a>i x a>i z )2 ≤ 1.95τ 3α3p ‖p‖3 (αlbz ) 3 ‖z‖3 1 m m∑ i=1 ( a>i x )2 . τ3 ‖x‖2 ‖z‖2 ‖p‖2 .\nUnder the hypothesis (154), we can further derive 1m ∑m i=1 I1,i1Ei3 ≤ τ (1.1 + δ) ‖p‖\n2. Putting all the above bounds together yields that the truncated objective function is majorized by\n1\nm m∑ i=1 {`i (z + τp)− `i (z)}1Ei3 ≤ 1 m m∑ i=1 (I1,i + I2,i + I3,i + I4,i + I5,i)1Ei3\n≤ τ (1.1 + δ) ‖p‖2 − 1.95 ( 1− ζ̃1 − ζ̃2 ) τ2 ‖p‖2 + τ ̃ ‖p‖2\n= { τ (1.1 + δ)− 1.95 ( 1− ζ̃1 − ζ̃2 ) τ2 + τ ̃ } ‖p‖2 (156)\nfor some constant ̃ > 0 that is linear in . Note that the backtracking line search seeks a point satisfying 1m ∑m i=1 {`i (z + τp)− `i (z)}1Ei3 ≥ 1 2τ ‖p‖ 2. Given the above majorization (156), this search criterion is satisfied only if\nτ/2 ≤ τ (1.1 + δ)− 1.95(1− ζ̃1 − ζ̃2)τ2 + τ ̃\nor, equivalently,\nτ ≤ 0.6 + δ + ̃ 1.95(1− ζ̃1 − ζ̃2) := τub.\nTaking δ and ̃ to be sufficiently small, we see that τ ≤ τub ≤ 0.384, provided that αlbz ≤ 0.1, αubz ≥ 5, αh ≥ 6, and αp ≥ 5.\nUsing very similar arguments, one can also show that 1m ∑m i=1 {`i (z + τp)− `i (z)}1Ei3 is minorized by a\nsimilar quadratic function, which combined with the stopping criterion 1m ∑m i=1 {`i (z + τp)− `i (z)}1Ei3 ≥ 1 2τ ‖p‖ 2 suggests that τ is bounded away from 0. We omit this part for conciseness."
    }, {
      "heading" : "Acknowledgements",
      "text" : "E. C. is partially supported by NSF under grant CCF-0963835 and by the Math + X Award from the Simons Foundation. Y. C. is supported by the same NSF grant. We thank Carlos Sing-Long and Weijie Su for helpful comments about an early version of the manuscript. E. C. is grateful to Xiaodong Li and Mahdi Soltanolkotabi for many discussions about Wirtinger flows."
    } ],
    "references" : [ {
      "title" : "Phase retrieval with polarization",
      "author" : [ "B. Alexeev", "A.S. Bandeira", "M. Fickus", "D.G. Mixon" ],
      "venue" : "SIAM Journal on Imaging Sciences, 7(1):35–66",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Simple",
      "author" : [ "S. Arora", "R. Ge", "T. Ma", "A. Moitra" ],
      "venue" : "efficient, and neural algorithms for sparse coding. arXiv:1503.00778",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Statistical guarantees for the EM algorithm: From population to sample-based analysis",
      "author" : [ "S. Balakrishnan", "M.J. Wainwright", "B. Yu" ],
      "venue" : "arXiv preprint arXiv:1408.2156",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Painless reconstruction from magnitudes of frame coefficients",
      "author" : [ "R. Balan", "B. Bodmann", "P. Casazza", "D. Edidin" ],
      "venue" : "Journal of Fourier Analysis and Applications, 15(4):488–501",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Saving phase: Injectivity and stability for phase retrieval",
      "author" : [ "A.S. Bandeira", "J. Cahill", "D.G. Mixon", "A.A. Nelson" ],
      "venue" : "Applied and Computational Harmonic Analysis, 37(1):106–125",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multireference alignment using semidefinite programming",
      "author" : [ "A.S. Bandeira", "M. Charikar", "A. Singer", "A. Zhu" ],
      "venue" : "Conference on Innovations in theoretical computer science, pages 459–470",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Lectures on modern convex optimization",
      "author" : [ "A. Ben-Tal", "A. Nemirovski" ],
      "venue" : "volume 2. SIAM",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "ROP: Matrix recovery via rank-one projections",
      "author" : [ "T. Cai", "A. Zhang" ],
      "venue" : "The Annals of Statistics, 43(1):102–138",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Solving quadratic equations via PhaseLift when there are about as many equations as unknowns",
      "author" : [ "E.J. Candès", "X. Li" ],
      "venue" : "Foundations of Computational Mathematics, 14(5):1017–1026",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Phase retrieval from coded diffraction patterns",
      "author" : [ "E.J. Candès", "X. Li", "M. Soltanolkotabi" ],
      "venue" : "to appear in Applied and Computational Harmonic Analysis",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Phase retrieval via Wirtinger flow: Theory and algorithms",
      "author" : [ "E.J. Candès", "X. Li", "M. Soltanolkotabi" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming",
      "author" : [ "E.J. Candès", "T. Strohmer", "V. Voroninski" ],
      "venue" : "Communications on Pure and Applied Mathematics, 66(8):1017–1026",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Supplemental materials for: “solving random quadratic systems of equations is nearly as easy as solving linear systems",
      "author" : [ "Y. Chen", "E.J. Candès" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Exact and stable covariance estimation from quadratic sampling via convex programming",
      "author" : [ "Y. Chen", "Y. Chi", "A.J. Goldsmith" ],
      "venue" : "to appear, IEEE Transactions on Information Theory",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Near-optimal joint optimal matching via convex relaxation",
      "author" : [ "Y. Chen", "L.J. Guibas", "Q. Huang" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "A convex formulation for mixed regression with two components: Minimax optimal rates",
      "author" : [ "Yudong Chen", "Xinyang Yi", "Constantine Caramanis" ],
      "venue" : "In Conf. on Learning Theory,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Stable optimizationless recovery from phaseless linear measurements",
      "author" : [ "L. Demanet", "P. Hand" ],
      "venue" : "Journal of Fourier Analysis and Applications, 20(1):199–221",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Phase retrieval: Stability and recovery guarantees",
      "author" : [ "Y.C. Eldar", "S. Mendelson" ],
      "venue" : "Applied and Computational Harmonic Analysis, 36(3):473–494",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Phase retrieval by iterated projections",
      "author" : [ "V. Elser" ],
      "venue" : "JOSA. A, 20(1):40–55",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Phase retrieval algorithms: a comparison",
      "author" : [ "J.R. Fienup" ],
      "venue" : "Applied optics, 21:2758–2769",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "A practical algorithm for the determination of phase from image and diffraction plane pictures",
      "author" : [ "R.W. Gerchberg" ],
      "venue" : "Optik, 35:237",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "Improved recovery guarantees for phase retrieval from coded diffraction patterns",
      "author" : [ "D. Gross", "F. Krahmer", "R. Kueng" ],
      "venue" : "arXiv preprint arXiv:1402.6286",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A partial derandomization of phaselift using spherical designs",
      "author" : [ "D. Gross", "F. Krahmer", "R. Kueng" ],
      "venue" : "Journal of Fourier Analysis and Applications, 21(2):229–266",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fast matrix completion without the condition number",
      "author" : [ "Moritz Hardt", "Mary Wootters" ],
      "venue" : "Conference on Learning Theory, pages",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Consistent shape maps via semidefinite programming",
      "author" : [ "Q. Huang", "L. Guibas" ],
      "venue" : "Computer Graphics Forum, 32(5):177–186",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Recovery of sparse 1-D signals from the magnitudes of their Fourier transform",
      "author" : [ "K. Jaganathan", "S. Oymak", "B. Hassibi" ],
      "venue" : "IEEE ISIT, pages 1473–1477",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Low-rank matrix completion using alternating minimization",
      "author" : [ "P. Jain", "P. Netrapalli", "S. Sanghavi" ],
      "venue" : "ACM symposium on Theory of computing, pages 665–674",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Matrix completion from a few entries",
      "author" : [ "R.H. Keshavan", "A. Montanari", "S. Oh" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2010
    }, {
      "title" : "Sparse signal recovery from quadratic measurements via convex programming",
      "author" : [ "X. Li", "V. Voroninski" ],
      "venue" : "SIAM Journal on Mathematical Analysis, 45(5):3019–3033",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Alternating projection",
      "author" : [ "S. Marchesin", "Y. Tu", "H. Wu" ],
      "venue" : "ptychographic imaging and phase synchronization. arXiv:1402.0550",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Probability and computing",
      "author" : [ "M. Mitzenmacher", "E. Upfal" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Phase retrieval using alternating minimization",
      "author" : [ "P. Netrapalli", "P. Jain", "S. Sanghavi" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Non-convex robust PCA",
      "author" : [ "P. Netrapalli", "U. Niranjan", "S. Sanghavi", "A. Anandkumar", "P. Jain" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1107–1115",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Numerical Optimization (2nd edition)",
      "author" : [ "J. Nocedal", "S.J. Wright" ],
      "venue" : "Springer",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Compressive phase retrieval via generalized approximate message passing",
      "author" : [ "P. Schniter", "S. Rangan" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2015
    }, {
      "title" : "GESPAR: Efficient phase retrieval of sparse signals",
      "author" : [ "Y. Shechtman", "A. Beck", "Y.C. Eldar" ],
      "venue" : "IEEE Transactions on Signal Processing, 62(4):928–938",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Phase retrieval with application to optical imaging",
      "author" : [ "Y. Shechtman", "Y.C. Eldar", "O. Cohen", "H.N. Chapman", "J. Miao", "M. Segev" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2015
    }, {
      "title" : "Sparsity based sub-wavelength imaging with partially incoherent light via quadratic compressed sensing",
      "author" : [ "Y. Shechtman", "Y.C. Eldar", "A. Szameit", "M. Segev" ],
      "venue" : "Optics express, 19(16)",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Algorithms and Theory for Clustering and Nonconvex Quadratic Programming",
      "author" : [ "M. Soltanolkotabi" ],
      "venue" : "PhD thesis, Stanford University",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Guaranteed matrix completion via non-convex factorization",
      "author" : [ "R. Sun", "Z. Luo" ],
      "venue" : "arXiv:1411.8003",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Numerical linear algebra",
      "author" : [ "L.N. Trefethen", "D. Bau III" ],
      "venue" : "volume 50. SIAM",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Introduction to nonparametric estimation",
      "author" : [ "A.B. Tsybakov", "V. Zaiats" ],
      "venue" : "volume 11. Springer",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "R. Vershynin" ],
      "venue" : "Compressed Sensing, Theory and Applications, pages 210 – 268",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A",
      "author" : [ "I. Waldspurger" ],
      "venue" : "d’Aspremont, and S. Mallat. Phase recovery, maxcut and complex semidefinite programming. Mathematical Programming, 149(1-2):47–81",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "We propose a novel method, which starting with an initial guess computed by means of a spectral method, proceeds by minimizing a nonconvex functional as in the Wirtinger flow approach [11].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 6,
      "context" : "However simple this formulation may seem, even checking whether a solution to (2) exists or not is known to be NP complete [7].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 19,
      "context" : "Moving from combinatorial optimization to the physical sciences, one application of paramount importance is the phase retrieval [20, 21] problem, which permeates through a wide spectrum of techniques including X-ray crystallography, diffraction imaging, and microscopy.",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 20,
      "context" : "Moving from combinatorial optimization to the physical sciences, one application of paramount importance is the phase retrieval [20, 21] problem, which permeates through a wide spectrum of techniques including X-ray crystallography, diffraction imaging, and microscopy.",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 36,
      "context" : "We refer to [37] for in-depth reviews of this subject.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 7,
      "context" : "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].",
      "startOffset" : 169,
      "endOffset" : 215
    }, {
      "referenceID" : 8,
      "context" : "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].",
      "startOffset" : 169,
      "endOffset" : 215
    }, {
      "referenceID" : 11,
      "context" : "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].",
      "startOffset" : 169,
      "endOffset" : 215
    }, {
      "referenceID" : 13,
      "context" : "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].",
      "startOffset" : 169,
      "endOffset" : 215
    }, {
      "referenceID" : 15,
      "context" : "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].",
      "startOffset" : 169,
      "endOffset" : 215
    }, {
      "referenceID" : 16,
      "context" : "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].",
      "startOffset" : 169,
      "endOffset" : 215
    }, {
      "referenceID" : 21,
      "context" : "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].",
      "startOffset" : 169,
      "endOffset" : 215
    }, {
      "referenceID" : 22,
      "context" : "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].",
      "startOffset" : 169,
      "endOffset" : 215
    }, {
      "referenceID" : 25,
      "context" : "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].",
      "startOffset" : 169,
      "endOffset" : 215
    }, {
      "referenceID" : 28,
      "context" : "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].",
      "startOffset" : 169,
      "endOffset" : 215
    }, {
      "referenceID" : 37,
      "context" : "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].",
      "startOffset" : 169,
      "endOffset" : 215
    }, {
      "referenceID" : 43,
      "context" : "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].",
      "startOffset" : 169,
      "endOffset" : 215
    }, {
      "referenceID" : 10,
      "context" : "[11, 19, 21, 30, 32, 35, 36]).",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "[11, 19, 21, 30, 32, 35, 36]).",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 20,
      "context" : "[11, 19, 21, 30, 32, 35, 36]).",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 29,
      "context" : "[11, 19, 21, 30, 32, 35, 36]).",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 31,
      "context" : "[11, 19, 21, 30, 32, 35, 36]).",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 34,
      "context" : "[11, 19, 21, 30, 32, 35, 36]).",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 35,
      "context" : "[11, 19, 21, 30, 32, 35, 36]).",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "One promising approach along this line is the recently proposed two-stage algorithm called Wirtinger Flow (WF) [11].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 10,
      "context" : "The main results of [11] demonstrate that WF is surprisingly accurate for both real-valued and complexvalued Gaussian sampling models.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 38,
      "context" : "In the presence of Gaussian noise, WF is stable and converges to the MLE as shown in [39].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "we cannot distinguish x from −x—we will evaluate our solutions to the quadratic equations through the distance measure put forth in [11] representing the Euclidean distance modulo a global sign: for complex signals, dist (z,x) := minφ:∈[0,2π) ‖e−jφz − x‖.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 33,
      "context" : "Arguably the most popular method for solving large-scale least squares problems is the conjugate gradient (CG) method [34] applied to the normal equations.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "We consider a type of measurements that falls under the category of coded diffraction patterns (CDP) [10] and set y = |FDx|, 1 ≤ l ≤ L.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 38,
      "context" : "This phenomenon arises regardless of the SNR! For experiments with noisy complex-valued data and (untruncated) WF, please see [39].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : "These outperform the provable guarantees of WF [11], which requires O(n log n) sample complexity and runs in O(mn2 log 1/ ) time.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : "With these in place, we take the step size in a far more liberal fashion—which is bounded away from 0—compared to a step size which is inversely propotional to n as explained in [11].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 12,
      "context" : "Interested readers are referred to the supplemental materials [13] for the proof of the universal theory (i.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 38,
      "context" : "[39] proves similar stability estimates using the WF approach under Gaussian noise.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.",
      "startOffset" : 203,
      "endOffset" : 210
    }, {
      "referenceID" : 20,
      "context" : "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.",
      "startOffset" : 203,
      "endOffset" : 210
    }, {
      "referenceID" : 18,
      "context" : "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 31,
      "context" : "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.",
      "startOffset" : 264,
      "endOffset" : 268
    }, {
      "referenceID" : 34,
      "context" : "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.",
      "startOffset" : 310,
      "endOffset" : 314
    }, {
      "referenceID" : 35,
      "context" : "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.",
      "startOffset" : 379,
      "endOffset" : 383
    }, {
      "referenceID" : 31,
      "context" : "While these paradigms enjoy favorable empirical behavior, most of them fall short of theoretical support, except for a version of alternating minimization (called AltMinPhase) [32] that requires fresh samples for each iteration.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 10,
      "context" : "Interesting readers are referred to [11] for a comparison of these non-convex schemes, and [10] for a discussion of other alternative approaches (e.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "Interesting readers are referred to [11] for a comparison of these non-convex schemes, and [10] for a discussion of other alternative approaches (e.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "[1, 4]) and performance lower bounds (e.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "[1, 4]) and performance lower bounds (e.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "[5, 18]).",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "[5, 18]).",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 2,
      "context" : "On the other hand, the family of two-stage nonconvex procedures—spectral initialization followed by iterative refinement—has proved efficient for other problems that involve latent variables, which leads to theoretical guarantees for general EM algorithms [3] and sparse coding schemes [2].",
      "startOffset" : 256,
      "endOffset" : 259
    }, {
      "referenceID" : 1,
      "context" : "On the other hand, the family of two-stage nonconvex procedures—spectral initialization followed by iterative refinement—has proved efficient for other problems that involve latent variables, which leads to theoretical guarantees for general EM algorithms [3] and sparse coding schemes [2].",
      "startOffset" : 286,
      "endOffset" : 289
    }, {
      "referenceID" : 10,
      "context" : "One natural alternative is the spectral method adopted in [11,32], which amounts to computing the leading eigenvector of Ỹ := 1 m ∑m i=1 yiaia > i .",
      "startOffset" : 58,
      "endOffset" : 65
    }, {
      "referenceID" : 31,
      "context" : "One natural alternative is the spectral method adopted in [11,32], which amounts to computing the leading eigenvector of Ỹ := 1 m ∑m i=1 yiaia > i .",
      "startOffset" : 58,
      "endOffset" : 65
    }, {
      "referenceID" : 10,
      "context" : "We start with a notation representing the (unrecoverable) global phase [11] for real-valued data",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "The observations (38) and (39) are reminiscent of a (local) regularity condition given in [11], which has been shown to be a fundamental criterion that dictates rapid convergence of iterative procedures (including WF and other gradient descent schemes).",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "For the sake of comparison, we also report the empirical performance of WF in all the above settings, where the step size is set to be the default choice of [11], that is, μt = min{1− e−t/330, 0.",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "When t ∈ [0, 1], it has been shown in the proof of [12, Lemma 3.",
      "startOffset" : 9,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "9 for all t ∈ [0, 1], as illustrated in Fig.",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "For instance, all results continue to hold if we replace the Poisson likelihood by the Gaussian likelihood; that is, the polynomial function −∑mi=1(yi − |ai z|2)2 studied in [11].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 7,
      "context" : "It is known that this problem can be efficiently solved by using more computational-intensive semidefinite programs [8,14].",
      "startOffset" : 116,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : "It is known that this problem can be efficiently solved by using more computational-intensive semidefinite programs [8,14].",
      "startOffset" : 116,
      "endOffset" : 122
    }, {
      "referenceID" : 23,
      "context" : "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].",
      "startOffset" : 129,
      "endOffset" : 142
    }, {
      "referenceID" : 26,
      "context" : "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].",
      "startOffset" : 129,
      "endOffset" : 142
    }, {
      "referenceID" : 27,
      "context" : "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].",
      "startOffset" : 129,
      "endOffset" : 142
    }, {
      "referenceID" : 39,
      "context" : "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].",
      "startOffset" : 129,
      "endOffset" : 142
    }, {
      "referenceID" : 32,
      "context" : "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 5,
      "context" : "A concrete application of this flavor is a simple form of the fundamental alignment/matching problem [6, 15, 25].",
      "startOffset" : 101,
      "endOffset" : 112
    }, {
      "referenceID" : 14,
      "context" : "A concrete application of this flavor is a simple form of the fundamental alignment/matching problem [6, 15, 25].",
      "startOffset" : 101,
      "endOffset" : 112
    }, {
      "referenceID" : 24,
      "context" : "A concrete application of this flavor is a simple form of the fundamental alignment/matching problem [6, 15, 25].",
      "startOffset" : 101,
      "endOffset" : 112
    }, {
      "referenceID" : 42,
      "context" : "[44]) such that for any (h, z) with ‖h‖ = ‖z‖ = 1, there exists a pair h0, z0 ∈ N satisfying ‖h− h0‖ ≤ and ‖z − z0‖ ≤ .",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "We consider the fundamental problem of solving quadratic systems of equations in n variables, where yi = |〈ai,x〉|, i = 1, . . . ,m and x ∈ R is unknown. We propose a novel method, which starting with an initial guess computed by means of a spectral method, proceeds by minimizing a nonconvex functional as in the Wirtinger flow approach [11]. There are several key distinguishing features, most notably, a distinct objective functional and novel update rules, which operate in an adaptive fashion and drop terms bearing too much influence on the search direction. These careful selection rules provide a tighter initial guess, better descent directions, and thus enhanced practical performance. On the theoretical side, we prove that for certain unstructured models of quadratic systems, our algorithms return the correct solution in linear time, i.e. in time proportional to reading the data {ai} and {yi} as soon as the ratio m/n between the number of equations and unknowns exceeds a fixed numerical constant. We extend the theory to deal with noisy systems in which we only have yi ≈ |〈ai,x〉| and prove that our algorithms achieve a statistical accuracy, which is nearly un-improvable. We complement our theoretical study with numerical examples showing that solving random quadratic systems is both computationally and statistically not much harder than solving linear systems of the same size—hence the title of this paper. For instance, we demonstrate empirically that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size.",
    "creator" : "LaTeX with hyperref package"
  }
}