{
  "name" : "1611.08230.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Fast Sparsifying Transforms",
    "authors" : [ "Cristian Rusu", "John Thompson" ],
    "emails" : [ "john.thompson}@ed.ac.uk." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 1.\n08 23\n0v 2\n[ cs\n.L G\n] 2\n8 M\nay 2\n01 7\nI. INTRODUCTION\nDictionary learning methods [1] represent a well-known class of algorithms that have seen many applications in signal processing [2], image processing [3], wireless communications [4] and machine learning [5]. The key idea of this approach is not to use an off-the-shelf transform like the Fourier, Hadamard or wavelet but to learn a new transform, often called an overcomplete dictionary, for a particular task (like coding and classification) from the data itself. While the dictionary learning problem is NP-hard [6] in general, it has been extensively studied and several good algorithms to tackle it exist. Alternating minimization methods like the method of optimal directions (MOD) [7], K–SVD [8], [9] and direct optimization [10] have been shown to work well in practice and also enjoy some theoretical performance guarantees. While learning a dictionary we need to construct two objects: the dictionary and the representation of the data in the dictionary.\nOne problem that arises in general when using learned dictionaries is the fact that they lack any structure. This is to be compared with the previously mentioned off-the-shelf transforms that have a rich structure. This is reflected in their low computational complexity, i.e., they can be applied\nThe authors are with the Institute for Digital Communications, School of Engineering, The University of Edinburgh, Scotland. Email: {c.rusu, john.thompson}@ed.ac.uk. Demo source code available online at https://udrc.eng.ed.ac.uk/sites/udrc.eng.ed.ac.uk/files/attachments/demo.zip. This work was supported by the Engineering and Physical Sciences Research Council (EPSRC) Grant number EP/K014277/1 and the MOD University Defence Research Collaboration (UDRC) in Signal Processing.\ndirectly using O(n log n) computations for example [11]. Our goal in this paper is to provide a solution to the problem of constructing fast transforms, based upon the structure of Givens rotations, learned from training data.\nWe first choose to study orthogonal structures since sparse reconstruction is computationally cheaper in such a dictionary: we project the data onto the column space of the dictionary and keep the largest s coefficients in magnitude to obtain the provable best s-term approximation. Working in an n dimensional feature space, this operation has complexity O(n2). In a general non-orthogonal (and even overcomplete) dictionary, special non-linear reconstruction methods such as ℓ1 minimization [12], greedy approaches like orthogonal matching pursuit (OMP) [13] or variational Bayesian algorithms like approximate message passing (AMP) [14] need to be applied. Aside from the fact that in general these methods cannot guarantee to produce best s-term approximations they are also computationally expensive. For example, the classical OMP has complexityO(sn2) [15] and, assuming that we are looking for sparse approximations with s ≪ n, it is in general computationally cheaper than ℓ1 optimization. Therefore, considering a square orthogonal dictionary is a first step in the direction of constructing a fast transform. For the analysis dictionary, recent work based on transform learning [16], [17] has been proposed. Still, notice that computing sparse representations in such a dictionary has complexity O(n2) and therefore, our goal of constructing a fast transform cannot be reached with just a general orthogonal dictionary. We make the case that our fundamental goal is to actually build a structured orthogonal dictionary such that matrix-vector multiplications with this dictionary can be achieved with less than O(n2) operations, preferably O(n log n). This connects our paper to previous work on approximating orthogonal (and symmetric) matrices [18] such that matrix-vector multiplications are computationally efficient.\nWhen we talk about “learning fast sparsifying transforms” we do not refer to the efficient learning procedures (although the proposed learning methods have polynomial complexity) but we refer to the transforms themselves, i.e., once we have the transform, the computational complexity of using it is low, preferably O(n log n) to perform matrix-vector multiplication. Previous work [19], [20], [21], [22], [23], [24], [25] in the literature has already proposed various structured dictionaries to cope with the high computational complexity of learned transforms. Previous work also dealt with the construction of structured orthogonal dictionaries. Specifically, [26] proposed to build an orthogonal dictionary composed of a product of a few Householder reflectors. In this fashion, the computational complexity of the dictionary is controlled and a trade-off between representation performance and computational complexity is shown.\n2 Learned dictionaries with low computational complexity can bridge the gap between the classical transforms that are preferred especially in power limited hardware (or battery operated devices) and the overcomplete, computationally cumbersome, learned dictionaries that provide state-of-the-art performance in many machine learning tasks. The contribution of this paper is two fold.\nFirst, we consider the problem of constructing an orthogonal dictionary as a product of a given number of generalized Givens rotations. We start by showing the optimum solution to the dictionary learning problem when the dictionary is a single generalized Givens rotation and then move to expand on this result and propose an algorithm that sequentially builds a product of generalized Givens rotations to act as a dictionary for sparse representations. Each step of the algorithm solves exactly the proposed optimization problem and therefore we can guarantee that it monotonically converges to a local minimum. We show numerically that the fast dictionaries proposed in this paper outperform those based on Householder reflectors [26] in terms of representation error, for the same computational complexity.\nSecond, based on a structure similar to the generalized Givens rotation we then propose a learning method that constructs square, non-orthogonal, computationally efficient dictionaries. In order to construct the dictionary we again solve exactly a series of optimization problems. Unfortunately we cannot prove the monotonic convergence of the algorithm since the sparse reconstruction step, based in this paper on OMP, cannot guarantee in general a monotonic reduction in our objective function. Still, we are able to show that these fast non-orthogonal transforms perform very well, better than their orthogonal counterparts.\nIn the results section we compare the proposed methods among each other and to previously proposed dictionary learning methods in the literature. We show that the methods proposed in this paper provide a clear trade-off between representation performance and computational complexity. Interestingly, we are able to provide numerical examples where the proposed fast orthogonal dictionaries have higher computational efficiency and provide better representation performance than the well-known discrete cosine transform (DCT), the transform at the heart of the jpeg compression standard [27]."
    }, {
      "heading" : "II. A BRIEF DESCRIPTION OF DICTIONARY LEARNING OPTIMIZATION PROBLEMS",
      "text" : "Given a real dataset Y ∈ Rn×N and sparsity level s, the general dictionary learning problem is to produce the factorization Y ≈ DX given by the optimization problem:\nminimize D, X\n‖Y −DX‖2F\nsubject to ‖xi‖0 ≤ s, 1 ≤ i ≤ N\n‖dj‖2 = 1, 1 ≤ j ≤ n,\n(1)\nwhere the objective function describes the Frobenius norm representation error achieved by the square dictionary D ∈ Rn×n with the sparse representations X ∈ Rn×N whose columns are subject to the ℓ0 pseudo-norm ‖xi‖0 (the number of nonzero elements of columns xi). To avoid trivial solutions, the\ndimensions obey s ≪ n ≪ N . Several algorithms that work very well in practice exist [7] [8] [15] to solve this factorization problem. Their approach, and the one we also adopt in this paper, is to keep the dictionary fixed and update the representations and then reverse the roles by updating the dictionary with the representations fixed. This alternating minimization approach proves to work very well experimentally [7], [8] and allows some theoretical insights [28].\nIn this paper we also consider the dictionary learning problem (1) with an orthogonal dictionary Q ∈ Rn×n [29] [30] [31] [32]. The orthogonal dictionary learning problem (which we call in this paper Q–DLA) [33] is formulated as:\nminimize Q, X; QQT=QTQ=I\n‖Y −QX‖2F\nsubject to ‖xi‖0 ≤ s, 1 ≤ i ≤ N. (2)\nSince the dictionary Q is orthogonal, the construction of X no longer involves ℓ1 [12], OMP [13] or AMP [14] approaches as in (1), but reduces to X = Ts(Q\nTY), where Ts() is an operator that given an input vector zeros all entries except the largest s in magnitude and given an input matrix applies the same operation on each column in turn. To solve (2) for variable Q and fixed X, a problem also known as the orthogonal Procrustes problem [34], a closed form solution Q = UVT is given by the singular value decomposition of YXT = UΣVT ."
    }, {
      "heading" : "III. A BUILDING BLOCK FOR FAST TRANSFORMS",
      "text" : "For indices (i, j), j > i and variables p, q, r, t ∈ R let us define the basic transform, which we call an R-transform:\nRij =\n\n    \nIi−1 p r\nIj−i−1 q t\nIn−j\n\n     ∈ Rn×n, (3)\nwhere we have denoted Ii as the identity matrix of size i. For simplicity, we denote the non-zero part of Rij as\nR̃ij =\n[\np r q t\n]\n∈ R2×2. (4)\nA right side multiplication between a R-transform and a matrix X ∈ Rn×N operates only rows i and j as\nRijX = [x1 . . . pxi + rxj . . .\n. . . qxi + txj . . . xn] T ,\n(5)\nwhere xTi is the i th row ofX. The number of operations needed for this task is only 6N . Left and right multiplications with a R-transform (or its transpose) are therefore computationally efficient. We use this matrix structure as a basic building block for the transforms learned in this paper. Remark 1. Every matrix D ∈ Rn×n can be written as a product of at most ⌈n2 − n2 ⌉ R-transforms. Therefore, we can consider the R-transforms as fundamental building blocks for all square transforms D. Proof. Consider the singular value decomposition D = UΣVT . Each U and V can be factored as a product of (\nn 2\n)\nGivens rotations [35] which are all in fact constrained\n3 R-transforms (with p = t = c and r = −q = d for some given c and d such that c2 + d2 = 1) and a diagonal matrix containing only {±1} entries. While the diagonal Σ can be factored as a product of ⌈n2 ⌉ diagonal R-transforms. In this paper we will be interested to useRij in least squares problems with the objective function as:\n‖Y −RijX‖ 2 F = ‖Y‖ 2 F + ‖X‖ 2 F − 2tr(Z)− ∥ ∥ ∥ ∥ [\nyTi yTj\n]∥\n∥ ∥ ∥\n2\nF\n−\n∥ ∥ ∥ ∥ [ xTi xTj ]∥ ∥ ∥ ∥\n2\nF\n+2tr(Z{i,j})+\n∥ ∥ ∥ ∥ [ yTi yTj ] −R̃ij [ xTi xTj ]∥ ∥ ∥ ∥\n2\nF\n.\n(6)\nFor simplicity of exposition we have defined\nZ = YXT ,Z{i,j} =\n[\nZii Zij Zji Zjj\n]\n∈ R2×2, Zij = y T i xj , (7)\nwhere yTi and x T i are the i th rows of Y and X, respectively.\nWe now introduce learning methods to create computation-\nally efficient orthogonal and non-orthogonal dictionaries."
    }, {
      "heading" : "IV. A METHOD FOR DESIGNING FAST ORTHOGONAL",
      "text" : "TRANSFORMS: Gm–DLA\nIn this section we propose a method called Gm–DLA to learn orthogonal dictionaries that are factorized as a product of m G-transforms (constrained R-transforms)."
    }, {
      "heading" : "A. An overview of G-transforms",
      "text" : "We call Gij a G-transform, an orthogonal constrained Rtransform (3) parameterized only by c, d ∈ R with c2+d2 = 1, and the indices (i, j), i 6= j such that the non-zero part ofGij , corresponding to (4), is given by\nG̃ij ∈\n{[\nc d −d c\n]\n,\n[\nc d d −c\n]}\n. (8)\nClassically, a Givens rotation is a matrix as in (3) with\nG̃ij =\n[\nc d −d c\n]\nsuch that det(Gij) = 1, i.e., proper rotation\nmatrices are orthogonal matrices with determinant one. These rotations are important since any orthogonal dictionary of size n× n can be factorized in a product of (\nn 2\n)\nGivens rotations\n[35]. In this paper, since we are interested in the computational complexity of these structures, we allow both options in (8) that fully characterize all 2×2 real orthogonal matrices – these structures are discussed in [36, Chapter 2.1]. With G̃ij = [\nc d d −c\n]\nthe G-transform in (3) is in fact a Householder\nreflector Gij = I − 2gijg T ij where gij ∈ R n, ‖gij‖2 = 1, has all entries equal to zero except for the ith and j th entries that are √ 0.5(1− c) and −sign(d) √\n0.5(1 + c), respectively – one might call this a “Givens reflector” to highlight its distinguishing sparse structure. Givens rotations have been previously used in matrix factorization applications [37], [38]."
    }, {
      "heading" : "B. One G-transform as a dictionary",
      "text" : "Consider now the dictionary learning problem in (2). Let us keep the sparse representations X fixed and consider a single G-transform as a dictionary. We reach the following\nminimize (i,j), G̃ij\n‖Y −GijX‖ 2 F . (9)\nWhen indices (i, j) are fixed, the problem reduces to constructing G̃ij , a constrained two dimensional optimization problem. To select the indices (i, j), among the (\nn 2\n)\npossi-\nbilities, an appropriate strategy needs to be defined. We detail next how to deal with these two problems to provide an overall solution for (9). To solve (9) for the fixed coordinates (i, j) we reach the optimization problem\nminimize G̃ij; G̃TijG̃ij=G̃ijG̃ T ij =I\n∥ ∥ ∥ ∥ [ yTi yTj ] − G̃ij [ xTi xTj ]∥ ∥ ∥ ∥\n2\nF\n. (10)\nThis is a two dimensional Procrustes problem [34] whose optimum solution is G̃ij = UV T where Z{i,j} = UΣV T . It has been shown in [26] that the reduction in the objective function of (10) when considering an orthogonal dictionary Gij given by the Procrustes solution is\n∥ ∥ ∥ ∥ [ yTi yTj ] − G̃ij [ xTi xTj ]∥ ∥ ∥ ∥\n2\nF\n=\n∥ ∥ ∥ ∥ [ yTi yTj ]∥ ∥ ∥ ∥\n2\nF\n+\n∥ ∥ ∥ ∥ [ xTi xTj ]∥ ∥ ∥ ∥\n2\nF\n−2tr\n(\nGTij\n[\nyTi yTj\n] [\nxTi xTj\n]T )\n=\n∥ ∥ ∥ ∥ [ yTi yTj ] ∥ ∥ ∥ ∥\n2\nF\n+\n∥ ∥ ∥ ∥ [ xTi xTj ] ∥ ∥ ∥ ∥\n2\nF\n− 2‖Z{i,j}‖∗,\n(11)\nwhere ‖Z{i,j}‖∗ is the nuclear norm of Z{i,j}, i.e., the sum of its singular values. Choosing (i, j) in (9) requires a closer look at its objective function (6) for R̃ij = G̃ij , the constrained G-transform structure. Using (11) we can state a result in the special case of a G-transform. We need both because for any indices (i, j) the reduction in the objective function invokes the nuclear norm, while for the other indices the reduction invokes the trace. We can analyze the two objective function values separately because the Frobenius norm is elementwise and as such also blockwise. Therefore, the objective of (9) is\n‖Y −GijX‖ 2 F = ‖Y‖ 2 F + ‖X‖ 2 F − 2tr(Z)− 2Cij , where Cij = ‖Z{i,j}‖∗ − tr(Z{i,j}). (12)\nSince we want to minimize this quantity, the choice of indices needs to be made as follows\n(i⋆, j⋆) = argmax (i,j), j>i Cij , (13)\nand then solve a Procrustes problem [34] to construct G̃i⋆j⋆ .\nThese (i⋆, j⋆) values are the optimum indices that lead to the maximal reduction in the objective function of (9). The expression in (13) is computationally cheap given that Z{i,j} is a 2×2 real matrix. Its trace is trivial to compute tr(Z{i,j}) = Zii + Zjj (one addition operation) while the singular values of Z{i,j} can be explicitly computed as\nσ1,2=\n√\n1\n2\n(\n‖Z{i,j}‖ 2 F ±\n√\n‖Z{i,j}‖ 4 F−4 det(Z{i,j}) 2\n)\n. (14)\nTherefore, the full singular value decomposition can be avoided and the sum of the singular values from (14) can be computed in only 23 operations (three of which are taking square roots). The cost of computing Cij for all indices\n4 (i, j), j > i, is 25n(n−1)2 operations. The computational burden is still dominated by constructing Z = YXT which takes 2snN operations.\nRemark 2. Notice that Cij ≥ 0 always. In general, this is because the sum of the singular values of any matrix Z of size n × n is always greater than the sum of its eigenvalues. To see this, use the singular value decomposition of Z = UΣVT ,Σ = diag(σ), and develop:\ntr(Z) = tr(ΣVTU) = n ∑\nk=1\nσk∆kk ≤ n ∑\nk=1\nσk = ‖Z‖∗, (15)\nwhere we have use the circular property of the trace and ∆ = VTU where ∆kk are its diagonal entries which obey |∆kk| ≤ 1 since bothU andV are orthogonal and their entries are sub-unitary in magnitude. Therefore, in our particular case, we have that Cij = 0 when Z{i,j} is symmetric and positive semidefinite (we have that ∆ = I in (15) and therefore tr(Z{i,j}) = ‖Z{i,j}‖∗). If we have that Cij = 0 for all i and j then no G-transform can reduce the objective function in (9) and therefore the solution is Gij = I. Remark 3.We can extend the G-transform to multiple indices. For example, if we consider three coordinates then Gijk ∈ R n×n has the non-zero orthogonal block G̃ijk ∈ R 3×3. For a transform over q indices there are ( n\nq\n)\nsuch blocks and its\nmatrix-vector multiplication takes (2q − 1)q operations.\nRemark 4. There are some connections between the Householder [26] and the G-transform approaches. As previously\nexplained, when G̃ij =\n[\nc d d −c\n]\nthe G-transform can also\nbe viewed as a Householder reflector, i.e., Gij = I− 2gijg T ij where gij ∈ R n is a 2-sparse vector. Following results from [26] we can also write\n‖Y −GijX‖ 2 F =‖Y‖ 2 F + ‖X‖ 2 F − 2tr(YX T )\n+ 2gTij(YX T +XYT )gij ,\n(16)\nwhich, together with (12), leads to gTij(YX T +XYT )gij = −Cij . This means that choosing to maximize Cij in (12) is equivalent to computing an eigenvector of YXT +XYT of sparsity two associated with a negative eigenvalue.\nThere are also some differences between the two approaches. For example, matrix-vector multiplication with a Gtransform Gijx takes 6 operations but when using the Householder structureGijx = (I−2gijg T ij)x = x−2(g T ijx)gij takes 8 operations (4 operations to compute the constant C = 2gTijx, 2 operations to compute the 2-sparse vector z = Cgij and 2 operations to compute the final result x − z). Therefore, the G-transform structure is computationally preferable to the Householder structure. Each Householder reflector has n− 1 (because of the orthogonality constraint) degrees of freedom while each G-transform has only 1 (the angle θ ∈ [0, 2π] for which c = cos θ and d = sin θ) plus 1 bit (the choice of the rotation or reflector in (8)).\nThis concludes our discussion for the single G-transform case. Notice that the solution outlined in this section solves (9) exactly, i.e., it finds the optimum G-transform."
    }, {
      "heading" : "C. A method for designing fast orthogonal transforms: Gm– DLA",
      "text" : "In this paper we propose to construct an orthogonal trans-\nform U ∈ Rn×n with the following structure:\nU = Gimjm . . .Gi2j2Gi1j1 . (17)\nThe value of m is a choice of the user. For example, if we choose m to be O(n log n) the transform U can be computed in O(n logn) computational complexity – similar to the classical fast transforms. The goal of this section is to propose a learning method that constructs such a transform.\nWe fix the representations X and all G-transforms in (17) except for the kth, denoted asGikjk . To optimize the dictionary U only for this transform we reach the objective function\n‖Y −UX‖2F = ‖Y −Gimjm . . .Gi1j1X‖ 2 F\n=‖GTik+1jk+1 . . .G T imjm Y −Gikjk . . .Gi1j1X‖ 2 F =‖Yk −GikjkXk‖ 2 F , (18)\nwhere we have used the fact that multiplication by any orthogonal transform preserves the Frobenius norm. For simplicity we have denoted Yk and Xk the known quantities in (18) and therefore Zk = YkX T k .\nNotice that we have reduced the problem to the formulation in (9) whose full solution is outlined in the previous section. We can apply this procedure for all G-transforms in the product of U and therefore a full update procedure presents itself: we will sequentially update each transform and then the sparse representations until convergence. The full procedure we propose, called Gm–DLA, is detailed in Algorithm 1. The initialization of Gm–DLA uses a known construction. It has been shown experimentally in the past [39], that a good initial orthogonal dictionary is to choose U from the singular value decomposition of the dataset Y = UΣVT . We can also provide a theoretical argument for this choice. Consider that\nX = Ts(U TY) = Ts(U TUΣVT ) = Ts(ΣV T ). (19)\nA sub-optimal choice is to assume that the operator Ts keeps only the first s rows of ΣVT , i.e., X = ΣsV\nT where Σs is the Σ matrix where we keep only the leading principal submatrix of size s × s and set to zero everything else. This is a good choice since the positive diagonal elements of Σ are sorted in decreasing order of their values and therefore we expect X to keep entries with large magnitude. In fact, ‖X‖2F = ∑s k=1 σ 2 k, where the σk’s are the diagonal elements of Σ, due to the fact that the rows of VT have unit magnitude. Furthermore, with the same X = ΣsV T we have ‖Y −UX‖2F = ‖U(Σ−Σs)V T ‖2F = ∑n k=s+1 σ 2 k. We expect this error term to be relatively small since we sum over the smallest squared singular values of Y. Therefore, with this choice of U and the optimal X = Ts(U TY) we have that ‖Y−UX‖2F ≤ ∑n k=s+1 σ 2 k , i.e., the representation error is always smaller than the error given by the best s-rank approximation of Y.\nIn Gm–DLA, with the sparse representations X = Ts(U\nTY) we proceed to iteratively construct each Gtransform. At step k, the problem to be solved is similar to (18)\n5 Algorithm 1 – Gm–DLA. Fast Orthonormal Transform Learning. Input: The dataset Y ∈ Rn×N , the number of G-transforms m, the target sparsity s and the number of iterations K . Output: The sparsifying square orthogonal transform U = Gimjm . . .Gi2j2Gi1j1 and sparse representations X such that ‖Y −UX‖2F is reduced.\nInitialization:\n1) Perform the economy size singular value decompo-\nsition of the dataset Y = UΣVT . 2) Compute sparse representations X = Ts(U\nTY). 3) For k = 1, . . . ,m: with X and all previous k − 1 G-transforms fixed and Gitjt = I, t = k + 1, . . . ,m, construct the new Gikjk where indices (ik, jk) are given by (13) and G̃ikjk = UV T by the singular value decomposition Jk{ik,jk} = UΣV T such that we minimize\n‖Y−GikjkGik−1jk−1 . . .Gi1j1X‖ 2 F = ‖Y−GikjkXk‖ 2 F ,\nas in (18) for Yk = Y and Jk = YX T k .\nIterations 1, . . . ,K:\n1) For k = 1, . . . ,m: two-step update of Gikjk , with X and all other transforms Gitjt , t 6= k fixed, such that (18) is minimized:\na) Update best indices (ik, jk) by (13). b) With new indices, update the transform G̃ikjk = UV T by the singular value decomposition Zk{ik,jk} = UΣV T , where Zk = YkX T k as in (18). 2) Compute sparse representations X = Ts(U TY),\nwhere U is given by (17).\nbut all transforms indexed above k are currently the identity (not initialized) and will be computed in the following steps.\nNotice that Z = YXT , necessary to compute all the values Cij , is computed fully only once before the iterative process. At each iteration of the algorithms only two columns of Z need to be recomputed. Therefore, the update of Z is trivial since it involves the linear combinations of two columns according to a G-transform multiplication (5).\nThe iterations of Gm–DLA update each G-transform sequentially, keeping all other constant, in order to minimize the current error term.\nThe algorithm is fast since the matrices involved in all computations can be updated from previous iterations. For example, at step k+1, notice from (18) that Yk+1 = Gik+1jk+1Yk and Xk+1 = GikjkXk . The same observation holds for Zk+1 = Gik+1jk+1YkX T kG T ikjk = Gik+1jk+1ZkG T ikjk . Of course, Y1 = G T i2j2\n. . .GTimjmY and X1 = X. We always need to construct Z1 from scratch since X has been fully updated in the sparse reconstruction step.\nAfter all transforms are computed, the dictionaryU is never explicitly constructed. We always remember its factorization (17) and apply it (directly or inversely) by sequentially applying the G-transforms in its composition. The total computational complexity of applying this dictionary for UTY is O(mN) which is O(n log(n)N) for sufficiently large m (of order n logn). This is to be compared with the O(n2N) of a\ngeneral orthogonal dictionary. Additionally, when consecutive G-transforms operate on different indices they can be applied in parallel, reducing the running time of Gm–DLA and that of manipulating the resulting dictionary.\nThe number of transforms m could be decided during the runtime of Gm–DLA based on the magnitude of the largest value Cij . Since this magnitude decides the reduction in the objective function of our problem, a threshold can be introduced to decide on the fly if a new transform is worth adding to the factorization.\nThese observations are important from a computational perspective since the number of transforms is relatively high, O(n logn), and therefore their manipulation should be performed efficiently when learning the dictionary to keep the running time of Gm–DLA low.\nSince each G-transform computed in our method maximally reduces the objective function and because the sparse reconstruction step is exact when using an orthogonal dictionary, we can guarantee that the proposed method monotonically converges to a local optimum point. Remark 5. At each iteration of the proposed algorithm we update a single G-transform according to the maximum value Cij . We have in fact the opportunity to update a maximum of ⌊n/2⌋ transforms simultaneously. We could for example partition the set {1, 2, . . . , n} in pairs of two and construct the corresponding G-transforms such that the sum of their Cij is maximized. With such a strategy fewer iterations are necessary but the problem of partitioning the indices such that the error is maximally reduced can be computationally demanding (all possible unique combinations of indices associations need to be generated). We expect Gm–DLA, as it is, to produce better results (lower representation error) due to the one-by-one transform update mechanism. Compared to the Householder approach [26] we again expect Gm–DLA to performs better since the optimization is made over two coordinates at a time.\nEven so, there are several options regarding the ordering. We can process the G-transforms in the order of their indices or in a random order for example in an effort to try to avoid local minimum points. Remark 6. After indices (i, j) are selected we have that Cij = 0 and therefore this pair cannot be selected again until either index i or j participates in the construction of a future G-transform. This is because after constructing Gij to minimize (10) we have that Z{i,j} is updated to Z{i,j}G̃ T ij = UΣV TVUT = UΣUT which is symmetric and positive definite due to the solution G̃ij = UV T . Remark 7. As previously discussed, the Procrustes solutionQ is the best orthogonal minimizer of (11). It has been shown in [26] that with this Q we have that T = YXTQT = UΣUT is symmetric positive semidefinite. Since Q is the global minimizer, there cannot be a G-transform Gij such that GijQ further reduces the error. This means that all symmetric\n2 × 2 sub-matrices T{i,j} =\n[\nTii Tij Tij Tjj\n]\nof T are positive\nsemidefinite, i.e., Tii + Tjj ≥ 0 and TiiTjj ≥ T 2 ij for all pairs (i, j). This observation needs to hold for any symmetric positive definite matrix T. Unfortunately, the converse is not true in general.\n6 This means that even with an appropriately large m ∼ O(n2), Gm–DLA might not always be able to match the performance of Q–DLA. This is not a major concern since in this paper we explore fast transforms and thereforem ≪ n2. This concludes the presentation of the proposed Gm–DLA method. Based on similar principles next we provide a learning method for fast square but non-orthogonal dictionaries."
    }, {
      "heading" : "V. A METHOD FOR DESIGNING FAST, GENERAL,",
      "text" : "NON-ORTHOGONAL TRANSFORMS: Rm–DLA\nIn the case of orthogonal dictionaries, the fundamental building blocks like Householder reflectors and Givens rotations are readily available. This is not the case for general dictionaries. In this section we propose a building block for non-orthogonal structures in subsection A and then show how this can be used in a similar fashion to the G-transform to learn computationally efficient square non-orthogonal dictionaries by deriving the Rm–DLA method in subsection B."
    }, {
      "heading" : "A. A building block for fast non-orthogonal transforms",
      "text" : "We assume no constraints on the variables p, q, r, t (these are four degrees of freedom) and therefore Rij from (3) is no longer orthogonal in general. We propose to solve the following optimization problem\nminimize (i,j), R̃ij\n‖Y −RijX‖ 2 F . (20)\nAs in the G-transform case, we proceed with analyzing how indices (i, j) are selected and then how to solve the optimization problem (20), with the indices fixed. We define\nZ = YXT , W = XXT , (21)\nwith entries Zij and Wij respectively. Solving (20) for fixed (i, j) leads to a least squares optimization problem as\nminimize R̃ij\n∥ ∥ ∥ ∥ [ yTi yTj ] − R̃ij [ xTi xTj ]∥ ∥ ∥ ∥\n2\nF\n, (22)\nwhere yTi ,x T i are i th rows of Y and X respectively and whose\nsolution is R̃ij =\n[\nZii Zij Zji Zjj\n] [\nWii Wij Wji Wjj\n]−1\n.\nChoosing (i, j) in (20) depends on the objective function value in (22) given by the least squares solution from above:\n∥ ∥ ∥ ∥ [ yTi yTj ] − R̃ij [ xTi xTj ] ∥ ∥ ∥ ∥\n2\nF\n=\n∥ ∥ ∥ ∥ [ yTi yTj ] ∥ ∥ ∥ ∥\n2\nF\n− tr\n(\n[\nZii Zij Zji Zjj\n]T [\nZii Zij Zji Zjj\n] [\nWii Wij Wji Wjj\n]−1 )\n.\n(23)\nThis, together with the development in (6), leads to\n‖Y −RijX‖ 2 F = ‖Y‖ 2 F + ‖X‖ 2 F − 2tr(Z)− Cij ,\nwith Cij =\n∥ ∥ ∥ ∥ [ xTi xTj ]∥ ∥ ∥ ∥\n2\nF\n− 2tr\n([\nZii Zij Zji Zjj\n])\n+ tr\n(\n[\nZii Zij Zji Zjj\n]T [\nZii Zij Zji Zjj\n] [\nWii Wij Wji Wjj\n]−1 )\n.\n(24)\nSince the matrices involved in the computation of Cij are 2×2 we can use the trace formula and the inversion of a 2×2 matrix formula to explicitly calculate\nCij = Wii +Wjj − 2(Zii + Zjj)\n+ Wii(Z\n2 ij + Z 2 jj) +Wjj(Z 2 ii + Z 2 ji)\nWiiWjj −WijWji\n− (ZiiZij + ZjiZjj)(Wij +Wji)\nWiiWjj −WijWji .\n(25)\nFinally, to solve (20) we select the indices as\n(i⋆, j⋆) = argmax j>i Cij , (26)\nand then solve a least square problem to construct R̃i⋆j⋆ . The Cij are computed only when WiiWjj − WijWji 6= 0, otherwise Cij = −∞. To compute each Cij in (25) we need 24 operations and there are n(n−1)\n2 such Cij . The computational\nburden is dominated by constructing Z = YXT ,W = XXT which take 2snN and snN operations, respectively. Remark 8. A necessary condition for a dictionary D ∈ Rn×n to be a local minimum point for the dictionary learning problem is that all Cij = 0 for Z = YX TDT ,W = DXXTDT .\nThis concludes our discussion for one transformRij . Notice that just like in the case of one G-transform, the solution given here finds the optimum Rij to minimize (20)."
    }, {
      "heading" : "B. A method for designing fast general transforms: Rm–DLA",
      "text" : "Similarly to Gm–DLA, we now propose to construct a\ngeneral dictionary D ∈ Rn×n with the following structure:\nD = Rimjm . . .Ri2j2Ri1j1∆. (27)\nThe value of m is a choice of the user. For example, if we choose m to be O(n log n) the dictionary D can be applied in O(n logn) computational complexity – similar to the classical fast transforms. The goal of this section is to propose a learning method that constructs such a general dictionary. As the transformations Rij are general, the diagonal matrix ∆ ∈ Rn×n is there to ensure that all columns dj of D are normalized ‖dj‖2 = 1 (as in the formulation (1)). This normalization does not affect the performance of the method since DX is equivalent to (D∆)(∆−1X). We fix the representations X and all transforms in (27) except for the kth transform Rikjk . Moreover, all transforms Rik+1jk+1 , . . . ,Rimjm are set to I. Because the transforms Rij are not orthogonal we cannot access directly any transform Rikjk in (27), but only the left most one Rimjm . In this case, to optimize the dictionary D only for this Rikjk transform we reach the objective\n‖Y −Rikjk . . .Ri2j2Ri1j1X‖ 2 F = ‖Y −RikjkXk‖ 2 F . (28)\nTherefore, our goal is to solve\nminimize Rikjk\n‖Y −RikjkXk‖ 2 F . (29)\nNotice that we have reduced the problem to the formulation in (20) whose full solution is outlined in the previous section. We can apply this procedure for all G-transforms in the\n7 Algorithm 2 – Rm–DLA. Fast Non-orthogonal Transform Learning. Input: The datasetY ∈ Rn×N , the number ofRij transforms m, the target sparsity s and the number of iterations K . Output: The sparsifying square non-orthogonal transform D = Rimjm . . .Ri2j2Ri1j1∆ and sparse representations X such that ‖Y −DX‖2F is reduced.\nInitialization:\n1) Perform the economy size singular value decompo-\nsition of the dataset Y = UΣVT . 2) Compute sparse representations X = Ts(U TY).\nIterations 1, . . . ,K:\n1) For k = 1, . . . ,m: with X and all previous k − 1 R-transforms fixed and Ritjt = I, t = k + 1, . . . ,m, construct the new Rikjk where indices (ik, jk) are given by (26) and R̃ikjk is given by the least squares solution that minimizes (28).\n2) Compute ∆ in (27) such that ‖dj‖2 = 1. 3) Compute sparse representations X=OMP(D,Y, s)\nwhere D is given in (27).\nIterations 1, . . . ,K:\n1) For k = 1, . . . ,m: with X, indices (ik, jk) and all transforms except the kth fixed, update only the non-zero part ofRikjk , denoted R̃ikjk , such that (30) is minimized.\n2) Compute ∆ in (27) such that ‖dj‖2 = 1. 3) Compute sparse representations X=OMP(D,Y, s)\nwhere D is given in (27).\nproduct of D and therefore a full update procedure presents itself: we will sequentially update each transform in (27), from the right to the left, and then the sparse representations until convergence or for a total number of iterations K . Once these iterations terminate we can refine the result. As previously mentioned, we cannot arbitrarily update a transform R̃ikjk because this transform is not orthogonal. But we can update its non-zero part R̃ikjk . Consider the development:\n‖Y−Rimjm . . .Rik+1jk+1RikjkRik−1jk−1 . . .Ri1j1X‖ 2 F\n=‖Y −BkRikjkXk‖ 2 F =‖vec(Y) − (XTk ⊗Bk)vec(Rikjk)‖ 2 F\n=\n∥ ∥ ∥ ∥ ∥ ∥ vec(Y)− ∑\nt∈{1,...,n}\\{ik,jk}\n((XTk )t ⊗ (Bk)t)−Cx\n∥ ∥ ∥ ∥ ∥ ∥ 2\nF\n=‖w−Cx‖2F , (30) where x = vec(R̃ikjk) ∈ R 4 and C = [(XTk )ik ⊗ (Bk)ik (X T k )ik ⊗ (Bk)jk (X T k )jk ⊗ (Bk)ik (X T k )jk ⊗ (Bk)jk ] ∈ R nN×4. We have denoted by (Bk)ik the i th k column of Bk and ⊗ is the Kronecker product. To develop (30) we have used the fact that the Frobenius norm is an elementwise operator, the structure of Rikjk and the fact that\nvec(BkRikjkXk) = (X T k ⊗Bk)vec(Rikjk). (31)\nThe x that minimizes (30) is given by the least squares solution x = (CTC)−1CTw. Therefore, once the product of the m\ntransforms is constructed we can update the non-zero part of any transform to further reduce the objective function. What we cannot do is update the indices (ik, jk) on which the calculation takes place, these stay the same.\nTherefore, we propose a learning procedure that has two sets of iterations: the first constructs the transforms Rikjk in a rigid manner, ordered from right to left most, and the second only updates the non-zero parts R̃ikjk of all the transforms without changing the coordinates (ik, jk). The full procedure we propose, called Rm–DLA, is detailed in Algorithm 2. This algorithm has two main parts which we will now describe.\nThe initialization of Rm–DLA has the goal to construct the sparse representation matrix X ∈ Rn×N . We have several options in this step. We can construct X in the same way as for Gm–DLA from the singular value decomposition of the dataset or by running another dictionary learning algorithm (like the K–SVD [8] for example) and use the X it constructs.\nThe iterations of Rm–DLA are divided into two sets. The goal of the first set of iterations is to decide upon all the indices (ik, jk) while the second set of iterations optimizes over the non-zero components of all the transforms in the factorization with the fixed indices previously decided.\nThe proposed Rm–DLA is can be itself efficiently implemented. When iteratively solving problems as (28) we have that Xk+1 = RikjkXk with X1 = X while when iteratively solving problems as (30) we have that Xk+1 = RikjkXk and Bk+1 = BkR\n−1 ikjk with X1 = X and B1 = Rimjm . . .Ri2j2 .\nThe explicit inverse R−1ikjk is not computed, instead the equivalent linear system for 2 variables can be efficiently solved.\nThe updates of all the transforms Rikjk monotonically decrease our objective function since we solve exactly the optimization problems in these variables. Unfortunately, normalizing to unit ℓ2 norm the columns of the transform and constructing the sparse approximations via OMP, which is not an exact optimization step, may cause increases in the objective function. For these reasons, monotonic convergence of Rm–DLA to a local minimum point cannot be guaranteed. For this reason, at all times we keep track of the best solution pair (D,X) and return it at the end of each iterative process. This concludes our discussion of Rm–DLA. We now move to discuss the computational complexity of the transforms created by the proposed methods and to show experimentally their representation capabilities."
    }, {
      "heading" : "VI. THE COMPUTATIONAL COMPLEXITY OF USING LEARNED TRANSFORMS",
      "text" : "In this section we look at the computational complexity of using the learned dictionaries to create the sparse representations on a dataset Y of size n×N . We are in a computational regime where we assume dimensions obey\ns ≪ n ≪ N. (32)\nThe computational complexity of using a general nonorthogonal dictionary A of size n × n in sparse recovery problems with Batch–OMP [15] is\nNA ≈ (2n 2 + s2n+ 3sn+ s3)N + n3. (33)\n8 The cost of n3 is associated with the construction of the Gram matrix of the dictionary and it does not depend on the number of samples N in the data. The total number of operations is dominated by constructing the projections in the dictionary column space which takes 2n2 operations per sample. The other operations dependent on the sparsity s and express the cost of iteratively finding the support of the sparse approximation.\nThe computational complexity of using an orthogonal dic-\ntionary Q designed via Q–DLA is\nNQ ≈ (2n 2 + ns)N. (34)\nAs in the general case, the cost is dominated by constructing the projections QTY which takes 2n2 operations for each of theN columns inY. The cost of ns expresses the approximate work done to identify the largest s entries in magnitude in the representation of each data sample. This can be performed in an efficient manner by keeping the s largest components in magnitude while the projections are computed for each data sample. Compared with (33), the iterative steps of constructing the support of the OMP solution for each sample and the construction of the Gram matrix (which is the identity matrix in this case) is no longer needed.\nThe same operation with a dictionary U as (17) computed\nvia Gm1–DLA takes\nNU ≈ (6m1 + ns)N. (35)\nThe result is similar to (34) but now the cost of constructing the projections UTY takes now only 6m1 operations per data sample. Here is where the G-transform factorization is used explicitly to reduce the computational complexity.\nFinally, with a dictionary D as (27) computed via Rm2– DLA the sparse approximation step via Batch–OMP [15] takes\nND ≈ (6m2 + n+ s 2n+ 3sn+ s3)N + 6m2n. (36)\nIn this case, the cost of building the projections DTY takes 6m2 operations to apply each Rij transform and then n operations to apply the scaling of the diagonal ∆. Simplifications occur also for the construction of the symmetric Gram matrix DTD which now takes 6m2n operations, instead of the regular n3 operations. This later simplification might not be significant since it is not dependent on the size of the dataset N . A dictionary U designed via Gm1–DLA has approximately the same computationally complexity as a general orthogonal dictionary Q designed via Q–DLA when\nm1 =\n⌊\nn2\n3\n⌋\n. (37)\nBecause any orthogonal matrix can be factorized as a product of n(n−1)\n2 G-transforms and because of the upper limit im-\nposed in (37) it is clear that we cannot express any orthogonal dictionary as an efficient transform for sparse representations. In some cases, the full orthogonal dictionaryQ might be more efficient than its factorization with G-transforms. In general, the representation error achieved by general orthogonal dictionaries designed via Q–DLA is a performance limit for Gtransform based dictionaries.\nA similar comparison can be made between the computational complexity of a general dictionary A and that of a dictionary D composed of m2 transformations Rij . Their complexities approximately match when\nm2 =\n⌊\n(2n2 + s2n+ 3sn+ s3)N + n3\n6(N + n)\n⌋\nN→∞ ≈\n⌊\nn2\n3\n⌋\n. (38)\nThis shows that for both Gm–DLA and Rm–DLA the computationally efficient regimes are when m ∼ O(n) or in general m ≪ n2.\nA last comment regards the comparison between dictionaries created with Gm1–DLA and Rm2–DLA. When m1 = m2 we expect Rm2–DLA to perform better but at a higher computational cost. Assuming large datasets N → ∞ and low sparsity s ≪ n, computational complexities approximately match when\nm1 ≈\n⌊\nm2 + (s2 + 3s+ 1)n\n6\n⌋\n. (39)\nDue to the use of the OMP procedure for non-orthogonal dictionaries to create the sparse approximations, dictionaries designed via Rm–DLA are much more computationally complex than the orthogonal dictionaries designed via Gm–DLA. Otherwise, as depicted in (39), for the same representation performance the orthogonal dictionaries may contain many more G-transforms in their factorization than Rij transforms contained in the factorization of a non-orthogonal dictionary. As a consequence, it may be that orthogonal dictionaries are always more computationally efficient than general dictionaries for approximately equal representation capabilities. A definite advantage of Rm–DLA is that it has the potential to create dictionaries that go below representation errors given by orthogonal dictionaries designed via Q–DLA, the performance limit of Gm–DLA.\nUsing these approximate complexities, we discuss in the results section the representation performance versus the computational complexity trade-off that the dictionaries constructed via the proposed methods display."
    }, {
      "heading" : "VII. EXPERIMENTAL RESULTS",
      "text" : "In this section we provide experimental results that show how transforms designed via the proposed methods Gm–DLA and Rm–DLA behave on image data.\nThe input data that we consider are taken from popular test images from the image processing literature (pirate, peppers, boat etc.). The test dataset Y ∈ Rn×N consists of 8× 8 nonoverlapping patches with their means removed and normalized Y ← Y/255. We choose to compare the proposed methods on image data since in this setting fast transforms that perform very well, like the Discrete Cosine Transform (DCT) [41] for example, are available. Our goal is to provide dictionaries based on factorizations like (17) and (27) that perform well in terms of representation error with a small number m of basic transforms in their composition. All algorithms run for K = 150 iterations and there are N = 12288 image patches in the dataset Y each of size n = 64.\n9 50 100 150 200 250 20 40 60 80 100 120 140 160 180\nNumber of G−transforms (initialization of G 256 −DLA)\nR ep\nre se\nnt at\nio n\ner ro\nr (%\n)\ns = 4 s = 8 s = 12\nFig. 1. For the proposed G256–DLA we show the relative representation error (40) in the initialization steps for the dataset Y created from the patches of the images couple, peppers and boat with sparsity s ∈ {4, 8, 12}. Notice that in general the representation error can surpass 100%, for example, for orthogonal dictionaries, the maximum value ǫ = 4 is achieved when X = −Y and D = I in (40).\n20 40 60 80 100 120 140\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\nIterations\nR ep\nre se\nnt at\nio n\ner ro\nr (%\n)\ns = 4 s = 8 s = 12\nFig. 2. For the same experimental setup as in Figure 1, we show the representation error for the K = 150 regular iterations of G256–DLA.\nTo measure the quality of a dictionary D we choose to\nevaluate the relative representation error\nǫ = ‖Y −DX‖2F‖Y‖ −2 F (%). (40)\nFigures 1 and 2 show the evolution of G256–DLA for K = 150 iterations (including the initialization procedure, i.e., the first 256 steps of the algorithm). The figures show how effective the initialization is in reducing the representation error for any sparsity level. Notice that the initialization procedure provides similar results regardless of the sparsity level s. The K = 150 iterations of G256–DLA further lower the representation error providing better results with larger sparsity level. As previously discussed, each step of the algorithm monotonically decreases the objective function of the dictionary learning problem until convergence.\nFigure 3 shows how Gm–DLA evolves with the number of transforms m and the sparsity s. As expected, increasing the number of transforms Gij and Rij in the factorization always lowers the representation error but with diminishing returns as m increases. This figure helps choose the number of transforms m while balancing between the computational complexity and representation performance. Large decreases in the representation error are seen up to m = 96 or m = 128 while thereafter increasingm brings smaller benefits. Also, with higher sparsity levels the number of transforms m becomes less relevant. We notice that with s = 12 the\n32 64 96 128 160 192 224 256\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nNumber of transforms m\nR ep\nre se\nnt at\nio n\ner ro\nr (%\n)\nG m −DLA, s = 4 R m −DLA, s = 4 G m −DLA, s = 8 R m −DLA, s = 8 G m −DLA, s = 12 R m −DLA, s = 12\nFig. 3. Performance of Gm–DLA and Rm–DLA in terms of the relative representation error (40) for different sparsity levels s ∈ {4, 8, 12}.\n42 85 128 170 256 341 682\n10\n15\n20\n25\n30\n35\n40\n45\n50\nNumber of transforms m\nR ep\nre se\nnt at\nio n\ner ro\nr (%\n)\nH 1 −DLA\nH 2 −DLA\nH 3 −DLA H 4 −DLA H 6 −DLA H 8 −DLA H 16 −DLA\nDCT G\nm −DLA Q−DLA SK−SVD\nFig. 4. Comparisons, in terms of relative representation errors (40), of Gm– DLA against the DCT, Q–DLA [33], SK–SVD [40] and Householder based orthogonal dictionaries [26] denoted here Hp–DLA where p is the number of reflectors in the factorization of the dictionary. The number of transforms m is chosen so that computational complexity comparisons against Hp–DLA is possible. Computational complexity approximately match between: H1–DLA and G42–DLA, H2–DLA and G85–DLA, H3–DLA and G128–DLA, H4– DLA and G170–DLA, H6–DLA and G256–DLA, H8–DLA and G341–DLA, H16–DLA and G682–DLA. The sparsity level is set to s = 4 for all methods. We use the SK–SVD to build a square, non-orthogonal, dictionary.\nrepresentation performance hits a plateau after m ≥ 128 transforms.\nAn interesting point of comparison is between the dictionaries constructed via Gm–DLA and Hp–DLA [26]. Figure 4 provides a detailed comparison between the two. A matrixvector multiplication takes 4n operations for a reflector and only 6 operations for a G-transform. If we compare the computational complexities of the dictionaries constructed by the two methods we find approximate equality between Hp–DLA and G⌊ 23np⌋ –DLA. Notice from this figure that for a lowm the G-transform approach provides better results than the Householder approach while also enjoying lower computational complexity. As the complexity of the dictionaries increases (larger number of G-transforms or reflectors) the gap between the two approaches decreases. The most complex dictionaries are designed via H16–DLA and G682–DLA and they closely match the performance of the general orthogonal dictionary learning approach Q–DLA while still keeping a computational advantage. In this case, the Householder approach keeps a slight edge in representation performance. Since the proposed approach updates the G-transforms sequentially the probability of getting stuck in local minimum points is more likely with large m. The difficulties that Gm–DLA encounters for large m are also discussed in Remark 6.\n10\nIt is also interesting to see that the representation performance of the DCT is matched by H3–DLA and G85–DLA. The computational complexity of H3–DLA approximately matches that of the DCT [41] (based on the FFT) while G85–DLA is actually computationally simpler than the DCT. In fact, any dictionary constructed by Gm–DLA for 85 ≤ m ≤ 128 is faster and provides better representations than the DCT.\nFigure 5 compares the Gm–DLA against the Rm–DLA for the same number of transforms m in their factorizations. Rm–DLA always outperforms Gm–DLA since the Gij is a constrained version of Rij . Unfortunately, the non-orthogonal transforms also have much higher computational complexity than their orthogonal counterparts in the sparse approximation step. For example, the computational complexity is approximately equivalent between dictionaries designed via R42–DLA and G351–DLA. The main benefit of non-orthogonal transforms is that ultimately, for large enoughm, their performance surpasses that of general (computationally inefficient) orthogonal transforms designed via Q–DLA. In our case this happens for m ≥ 256. The performance of the DCT is approximately matched by R50–DLA. Surprisingly, less than n factors in the product of the transform suffice to match the performance of the classical DCT transform for sparse recovery. This highlights the way dictionaries designed via Rm–DLA balance the computationally efficiency and representation performance trade-off, i.e., one R-transform gives 4 degrees of freedom for the cost of 6 operations.\nFigure 6 shows the performance of Rm–DLA in a regime close to the results of the SK–SVD dictionary learning method [40]. We use the SK–SVD to construct a square dictionary, i.e.,\na dictionary with n atoms. The complexity of the dictionary designed via R1364–DLA matches that of the dictionary designed via SK–SVD while there is a small performance gap between the two. We notice experimentally that the iterative procedure of Rm–DLA improves performance always when increasing m but the probability of getting stuck in local minimum points increases. Therefore, just as Gm–DLA has some trouble matching the performance of Q–DLA, Rm–DLA has trouble exactly matching the performance of SK–SVD.\nIn the last experimental setup we compare our Rm–DLA with the previously proposed Sparse K–SVD approach [42]. We use the Sparse K–SVD to build a square dictionary D = ΦS ∈ Rn×n where Φ is a well-known classic transform (in our case the DCT) and S ∈ Rn×n is matrix with only p non-zero entries per column. In this fashion, matrix-vector multiplication like DTy = STΦTy takes 2pn+C operations, where C is the cost of applying the DCT (in our case, this is the same as using a transform designed via G128–DLA or R128–DLA). Rm–DLA performs consistently better than the Sparse K–SVD for very fast transforms while the performance gap closes for very low representation errors. The Sparse K– SVD suffers from the fact that the fast transformΦ is fixed and therefore the optimization takes place over only pn degreed of freedom. We restrict ourselves to square transforms and avoid the comparison with overcomplete dictionaries designed via the K–SVD or the Sparse K–SVD. Experimental insights into how the representation performance scales with the number of atoms in the dictionary are given in [40], [43].\nWhen designing a very fast orthogonal transform (whose complexity let us say is order n or n logn) then Gm–DLA provides very good results while achieving the lowest computational complexity. For improved performance, more complex orthogonal transforms perform better when designed via Hp– DLA. If representation capabilities is the only performance metric then the non-orthogonal transforms designed by Rm– DLA are the weapon of choice. For large m both Gm– DLA and Rm–DLA can suffer from long running times. For example, G128–DLA takes several minutes to terminate while R128–DLA’s running time is close to ten minutes on a modern Intel i7 computer system. We note that the algorithms are\n11\nimplemented in Matlab R©. A careful implementation in a lower level compiled programming language will drive these running times much lower and reduce the memory footprint."
    }, {
      "heading" : "VIII. CONCLUSIONS",
      "text" : "In this paper we present practical procedures to learn square orthogonal and non-orthogonal dictionaries already factored into a fixed number of computationally efficient blocks. We show how effective the dictionaries constructed via the proposed methods are on image data where we compare against the fast cosine transform on one side and general nonorthogonal and orthogonal dictionaries on the other. We also show comparisons with a recently proposed method that constructs Householder based orthogonal dictionaries. We show empirically that the proposed methods construct transforms that provide an improved trade-off between computational complexity and representation performance among the methods we consider. We are able to construct transforms that exhibit lower computational efficiency and lower representation error than the fast cosine transform for image data. We expect the current work to extend the use of learned transforms in time critical scenarios and to devices where, due to power limitations, only low complexity algorithms can be deployed."
    } ],
    "references" : [ {
      "title" : "Dictionary learning",
      "author" : [ "I. Tosic", "P. Frossard" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 28, no. 2, pp. 27–38, 2011.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "From sparse solutions of systems of equations to sparse modeling of signals and images",
      "author" : [ "A.M. Bruckstein", "D.L. Donoho", "M. Elad" ],
      "venue" : "SIAM Review, vol. 51, pp. 34–81, 2009.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Image denoising via sparse and redundant representations over learned dictionaries",
      "author" : [ "M. Elad", "M. Aharon" ],
      "venue" : "IEEE Trans. Image Proc., vol. 15, no. 12, pp. 3736–3745, 2006.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Low complexity hybrid sparse precoding and combining in millimeter wave MIMO systems",
      "author" : [ "C. Rusu", "R. Mendez-Rial", "N. Gonzalez-Prelcic", "R.W. Heath" ],
      "venue" : "Proc. IEEE ICC, 2015.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Task-driven dictionary learning",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 4, pp. 791–804, 2012.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the computational intractability of exact and approximate dictionary learning",
      "author" : [ "A.M. Tillmann" ],
      "venue" : "IEEE Signal Processing Letters, vol. 22, no. 1, pp. 45–49, 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Method of optimal directions for frame design",
      "author" : [ "K. Engan", "S.O. Aase", "J.H. Husøy" ],
      "venue" : "Proc. IEEE ICASSP, 1999, pp. 2443–2446.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation",
      "author" : [ "M. Aharon", "M. Elad", "A.M. Bruckstein" ],
      "venue" : "IEEE Trans. Sig. Proc., vol. 54, no. 11, pp. 4311–4322, 2006.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "On the uniqueness of overcomplete dictionaries, and a practical way to retrieve them",
      "author" : [ "——" ],
      "venue" : "Linear Algebra and Applications, vol. 416, pp. 48–67, 2006.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Direct optimization of the dictionary learning problem",
      "author" : [ "A. Rakotomamonjy" ],
      "venue" : "IEEE Trans. Sig. Proc., vol. 61, no. 22, pp. 5495–5506, 2013.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "An algorithm for the machine calculation of complex Fourier series",
      "author" : [ "J. Cooley", "J. Tukey" ],
      "venue" : "Mathematics of Computation, vol. 19, no. 90, pp. 297–301, 1965.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "Just relax: Convex programming methods for subset selection and sparse approximation",
      "author" : [ "J.A. Tropp" ],
      "venue" : "IEEE Trans. Inf. Theory, vol. 52, pp. 1030–1051, 2006.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Greed is good: Algorithmic results for sparse approximation",
      "author" : [ "——" ],
      "venue" : "IEEE Trans. Inf. Theory, vol. 50, pp. 2231–2242, 2004.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Message-passing algorithms for compressed sensing",
      "author" : [ "D.L. Donoho", "A. Maleki", "A. Montanari" ],
      "venue" : "Proceedings of the National Academy of Sciences, vol. 106, no. 45, pp. 18 914–18 919, 2009.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit",
      "author" : [ "R. Rubinstein", "M. Zibulevsky", "M. Elad" ],
      "venue" : "CS Technion, 2008.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning sparsifying transforms",
      "author" : [ "S. Ravishankar", "Y. Bresler" ],
      "venue" : "IEEE Trans. Signal Process., vol. 61, no. 5, pp. 1072–1086, 2013.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "l0 sparsifying transform learning with efficient optimal updates and convergence guarantees",
      "author" : [ "——" ],
      "venue" : "IEEE Trans. Signal Process., vol. 63, no. 9, pp. 2389–2404, 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fast approximation of rotations and Hessians matrices",
      "author" : [ "M. Mathieu", "Y. LeCun" ],
      "venue" : "arXiv:1404.7195, 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Double sparsity: learning sparse dictionaries for sparse signal approximation",
      "author" : [ "R. Rubinstein", "M. Zibulevsky", "M. Elad" ],
      "venue" : "IEEE Trans. Sig. Proc., vol. 58, no. 3, pp. 1553–1564, 2010.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Chasing butterflies: In search of efficient dictionaries",
      "author" : [ "L.L. Magoarou", "R. Gribonval" ],
      "venue" : "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), April 2015, pp. 3287–3291.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Toward fast transform learning",
      "author" : [ "O. Chabiron", "F. Malgouyres", "J.-Y. Tourneret", "N. Dobigeon" ],
      "venue" : "Technical report, 2013.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Explicit shift-invariant dictionary learning",
      "author" : [ "C. Rusu", "B. Dumitrescu", "S.A. Tsaftaris" ],
      "venue" : "IEEE Signal Proc. Let., vol. 21, no. 1, pp. 6–9, 2014.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Block orthonormal overcomplete dictionary learning",
      "author" : [ "C. Rusu", "B. Dumitrescu" ],
      "venue" : "21st European Sig. Proc. Conf., 2013, pp. 1–5.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning doubly sparse transforms for images",
      "author" : [ "S. Ravishankar", "Y. Bresler" ],
      "venue" : "IEEE Trans. Image Process., vol. 22, no. 12, pp. 4598–4612, 2013.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Fast orthonormal sparsifying transforms based on Householder reflectors",
      "author" : [ "C. Rusu", "N. Gonzalez-Prelcic", "R. Heath" ],
      "venue" : "IEEE Trans. Sig. Process., vol. 64, no. 24, pp. 6589–6599, 2016.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The JPEG still picture compression standard",
      "author" : [ "G.K. Wallace" ],
      "venue" : "IEEE Trans. Consumer Electronics, vol. 38, no. 1, 1992.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Learning sparsely used overcomplete dictionaries",
      "author" : [ "A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli", "R. Tandon" ],
      "venue" : "JMLR Workshop and Conference Proceedings, vol. 35, 2014, pp. 1–15.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sparse orthonormal transforms for image compression",
      "author" : [ "O.G. Sezer", "O. Harmanci", "O.G. Guleryuz" ],
      "venue" : "Proc. IEEE ICIP, 2008, pp. 149– 152.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Approximation and compression with sparse orthonormal transforms",
      "author" : [ "O.G. Sezer", "O.G. Guleryuz", "Y. Altunbasak" ],
      "venue" : "IEEE Trans. Image Proc., vol. 24, no. 8, pp. 2328–2343, 2015.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An EM-algorithm approach for the design of orthonormal bases adapted to sparse representations",
      "author" : [ "A. Dremeau", "C. Herzet" ],
      "venue" : "Proc. IEEE ICASSP, 2010, pp. 2046–2049.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning efficient data representations with orthogonal sparse coding",
      "author" : [ "H. Schutze", "E. Barth", "T. Martinetz" ],
      "venue" : "IEEE Trans. Comp. Imaging, vol. 2, no. 3, pp. 177–189, 2016.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Learning unions of orthonormal bases with thresholded singular value decompositon",
      "author" : [ "S. Lesage", "R. Gribonval", "F. Bimbot", "L. Benaroya" ],
      "venue" : "Proc. IEEE ICASSP, 2005, pp. 293–296.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A generalized solution of the orthogonal Procrustes problem",
      "author" : [ "P. Schonemann" ],
      "venue" : "Psychometrika, vol. 31, no. 1, pp. 1–10, 1966.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1966
    }, {
      "title" : "Matrix Computations",
      "author" : [ "G.H. Golub", "C.F. Van Loan" ],
      "venue" : null,
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1996
    }, {
      "title" : "The sparse matrix transform for covariance estimation and analysis of high dimensional signals",
      "author" : [ "C. Guangzhi", "L.R. Bachega", "C.A. Bouman" ],
      "venue" : "IEEE Trans. Image Processing, vol. 20, no. 3, pp. 625–640, 2011.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Multiresolution matrix factorization",
      "author" : [ "R. Kondor", "N. Teneva", "V. Garg" ],
      "venue" : "Proc. of the 31st International Conference on Machine Learning, 2014, pp. 1620–1628.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "An initialization strategy for the dictionary learning problem",
      "author" : [ "C. Rusu", "B. Dumitrescu" ],
      "venue" : "Proc. IEEE ICASSP, 2014, pp. 6731–6735.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Stagewise K-SVD to design efficient dictionaries for sparse representations",
      "author" : [ "——" ],
      "venue" : "IEEE Signal Processing Letters, vol. 19, no. 10, pp. 631–634, 2012.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A fast computational algorithm for the discrete cosine transform",
      "author" : [ "W.-H. Chen", "C.H. Smith", "S.C. Fralick" ],
      "venue" : "IEEE Trans. Communications, vol. 25, no. 9, pp. 1004–1009, 1977.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Double sparsity: Learning sparse dictionaries for sparse signal approximation",
      "author" : [ "R. Rubinstein", "M. Zibulevsky", "M. Elad" ],
      "venue" : "IEEE Trans. Sig. Process., vol. 58, no. 3, pp. 1553–1564, 2010.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Fast design of efficient dictionaries for sparse representations",
      "author" : [ "C. Rusu" ],
      "venue" : "Proc. IEEE Machine Learning for Signal Processing, 2012, pp. 1–5.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "INTRODUCTION Dictionary learning methods [1] represent a well-known class of algorithms that have seen many applications in signal processing [2], image processing [3], wireless communications [4] and machine learning [5].",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "INTRODUCTION Dictionary learning methods [1] represent a well-known class of algorithms that have seen many applications in signal processing [2], image processing [3], wireless communications [4] and machine learning [5].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "INTRODUCTION Dictionary learning methods [1] represent a well-known class of algorithms that have seen many applications in signal processing [2], image processing [3], wireless communications [4] and machine learning [5].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 3,
      "context" : "INTRODUCTION Dictionary learning methods [1] represent a well-known class of algorithms that have seen many applications in signal processing [2], image processing [3], wireless communications [4] and machine learning [5].",
      "startOffset" : 193,
      "endOffset" : 196
    }, {
      "referenceID" : 4,
      "context" : "INTRODUCTION Dictionary learning methods [1] represent a well-known class of algorithms that have seen many applications in signal processing [2], image processing [3], wireless communications [4] and machine learning [5].",
      "startOffset" : 218,
      "endOffset" : 221
    }, {
      "referenceID" : 5,
      "context" : "While the dictionary learning problem is NP-hard [6] in general, it has been extensively studied and several good algorithms to tackle it exist.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : "Alternating minimization methods like the method of optimal directions (MOD) [7], K–SVD [8], [9] and direct optimization [10] have been shown to work well in practice and also enjoy some theoretical performance guarantees.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "Alternating minimization methods like the method of optimal directions (MOD) [7], K–SVD [8], [9] and direct optimization [10] have been shown to work well in practice and also enjoy some theoretical performance guarantees.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "Alternating minimization methods like the method of optimal directions (MOD) [7], K–SVD [8], [9] and direct optimization [10] have been shown to work well in practice and also enjoy some theoretical performance guarantees.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 9,
      "context" : "Alternating minimization methods like the method of optimal directions (MOD) [7], K–SVD [8], [9] and direct optimization [10] have been shown to work well in practice and also enjoy some theoretical performance guarantees.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 10,
      "context" : "directly using O(n log n) computations for example [11].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "In a general non-orthogonal (and even overcomplete) dictionary, special non-linear reconstruction methods such as l1 minimization [12], greedy approaches like orthogonal matching pursuit (OMP) [13] or variational Bayesian algorithms like approximate message passing (AMP) [14] need to be applied.",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 12,
      "context" : "In a general non-orthogonal (and even overcomplete) dictionary, special non-linear reconstruction methods such as l1 minimization [12], greedy approaches like orthogonal matching pursuit (OMP) [13] or variational Bayesian algorithms like approximate message passing (AMP) [14] need to be applied.",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 13,
      "context" : "In a general non-orthogonal (and even overcomplete) dictionary, special non-linear reconstruction methods such as l1 minimization [12], greedy approaches like orthogonal matching pursuit (OMP) [13] or variational Bayesian algorithms like approximate message passing (AMP) [14] need to be applied.",
      "startOffset" : 272,
      "endOffset" : 276
    }, {
      "referenceID" : 14,
      "context" : "For example, the classical OMP has complexityO(sn) [15] and, assuming that we are looking for sparse approximations with s ≪ n, it is in general computationally cheaper than l1 optimization.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "For the analysis dictionary, recent work based on transform learning [16], [17] has been proposed.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : "For the analysis dictionary, recent work based on transform learning [16], [17] has been proposed.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 17,
      "context" : "This connects our paper to previous work on approximating orthogonal (and symmetric) matrices [18] such that matrix-vector multiplications are computationally efficient.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 18,
      "context" : "Previous work [19], [20], [21], [22], [23], [24], [25] in the literature has already proposed various structured dictionaries to cope with the high computational complexity of learned transforms.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 19,
      "context" : "Previous work [19], [20], [21], [22], [23], [24], [25] in the literature has already proposed various structured dictionaries to cope with the high computational complexity of learned transforms.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 20,
      "context" : "Previous work [19], [20], [21], [22], [23], [24], [25] in the literature has already proposed various structured dictionaries to cope with the high computational complexity of learned transforms.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 21,
      "context" : "Previous work [19], [20], [21], [22], [23], [24], [25] in the literature has already proposed various structured dictionaries to cope with the high computational complexity of learned transforms.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 22,
      "context" : "Previous work [19], [20], [21], [22], [23], [24], [25] in the literature has already proposed various structured dictionaries to cope with the high computational complexity of learned transforms.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 23,
      "context" : "Previous work [19], [20], [21], [22], [23], [24], [25] in the literature has already proposed various structured dictionaries to cope with the high computational complexity of learned transforms.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 24,
      "context" : "Specifically, [26] proposed to build an orthogonal dictionary composed of a product of a few Householder reflectors.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 24,
      "context" : "We show numerically that the fast dictionaries proposed in this paper outperform those based on Householder reflectors [26] in terms of representation error, for the same computational complexity.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 25,
      "context" : "Interestingly, we are able to provide numerical examples where the proposed fast orthogonal dictionaries have higher computational efficiency and provide better representation performance than the well-known discrete cosine transform (DCT), the transform at the heart of the jpeg compression standard [27].",
      "startOffset" : 301,
      "endOffset" : 305
    }, {
      "referenceID" : 6,
      "context" : "Several algorithms that work very well in practice exist [7] [8] [15] to solve this factorization problem.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "Several algorithms that work very well in practice exist [7] [8] [15] to solve this factorization problem.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "Several algorithms that work very well in practice exist [7] [8] [15] to solve this factorization problem.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "This alternating minimization approach proves to work very well experimentally [7], [8] and allows some theoretical insights [28].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "This alternating minimization approach proves to work very well experimentally [7], [8] and allows some theoretical insights [28].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 26,
      "context" : "This alternating minimization approach proves to work very well experimentally [7], [8] and allows some theoretical insights [28].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 27,
      "context" : "In this paper we also consider the dictionary learning problem (1) with an orthogonal dictionary Q ∈ R [29] [30] [31] [32].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 28,
      "context" : "In this paper we also consider the dictionary learning problem (1) with an orthogonal dictionary Q ∈ R [29] [30] [31] [32].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 29,
      "context" : "In this paper we also consider the dictionary learning problem (1) with an orthogonal dictionary Q ∈ R [29] [30] [31] [32].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 30,
      "context" : "In this paper we also consider the dictionary learning problem (1) with an orthogonal dictionary Q ∈ R [29] [30] [31] [32].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 31,
      "context" : "The orthogonal dictionary learning problem (which we call in this paper Q–DLA) [33] is formulated as:",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "Since the dictionary Q is orthogonal, the construction of X no longer involves l1 [12], OMP [13] or AMP [14] approaches as in (1), but reduces to X = Ts(Q Y), where Ts() is an operator that given an input vector zeros all entries except the largest s in magnitude and given an input matrix applies the same operation on each column in turn.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "Since the dictionary Q is orthogonal, the construction of X no longer involves l1 [12], OMP [13] or AMP [14] approaches as in (1), but reduces to X = Ts(Q Y), where Ts() is an operator that given an input vector zeros all entries except the largest s in magnitude and given an input matrix applies the same operation on each column in turn.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : "Since the dictionary Q is orthogonal, the construction of X no longer involves l1 [12], OMP [13] or AMP [14] approaches as in (1), but reduces to X = Ts(Q Y), where Ts() is an operator that given an input vector zeros all entries except the largest s in magnitude and given an input matrix applies the same operation on each column in turn.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 32,
      "context" : "To solve (2) for variable Q and fixed X, a problem also known as the orthogonal Procrustes problem [34], a closed form solution Q = UV is given by the singular value decomposition of YX = UΣV .",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 33,
      "context" : "Givens rotations [35] which are all in fact constrained",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 33,
      "context" : "Givens rotations [35].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 34,
      "context" : "Givens rotations have been previously used in matrix factorization applications [37], [38].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 35,
      "context" : "Givens rotations have been previously used in matrix factorization applications [37], [38].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 32,
      "context" : "This is a two dimensional Procrustes problem [34] whose optimum solution is G̃ij = UV T where Z{i,j} = UΣV T .",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 24,
      "context" : "It has been shown in [26] that the reduction in the objective function of (10) when considering an orthogonal dictionary Gij given by the Procrustes solution is",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 32,
      "context" : "and then solve a Procrustes problem [34] to construct G̃i⋆j⋆ .",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 24,
      "context" : "There are some connections between the Householder [26] and the G-transform approaches.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 24,
      "context" : "Following results from [26] we can also write",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 36,
      "context" : "It has been shown experimentally in the past [39], that a good initial orthogonal dictionary is to choose U from the singular value decomposition of the dataset Y = UΣV .",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 24,
      "context" : "Compared to the Householder approach [26] we again expect Gm–DLA to performs better since the optimization is made over two coordinates at a time.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 24,
      "context" : "It has been shown in [26] that with this Q we have that T = YXQT = UΣU is symmetric positive semidefinite.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 7,
      "context" : "We can construct X in the same way as for Gm–DLA from the singular value decomposition of the dataset or by running another dictionary learning algorithm (like the K–SVD [8] for example) and use the X it constructs.",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 14,
      "context" : "The computational complexity of using a general nonorthogonal dictionary A of size n × n in sparse recovery problems with Batch–OMP [15] is",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : "Finally, with a dictionary D as (27) computed via Rm2– DLA the sparse approximation step via Batch–OMP [15] takes",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 38,
      "context" : "We choose to compare the proposed methods on image data since in this setting fast transforms that perform very well, like the Discrete Cosine Transform (DCT) [41] for example, are available.",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 31,
      "context" : "Comparisons, in terms of relative representation errors (40), of Gm– DLA against the DCT, Q–DLA [33], SK–SVD [40] and Householder based orthogonal dictionaries [26] denoted here Hp–DLA where p is the number of reflectors in the factorization of the dictionary.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 37,
      "context" : "Comparisons, in terms of relative representation errors (40), of Gm– DLA against the DCT, Q–DLA [33], SK–SVD [40] and Householder based orthogonal dictionaries [26] denoted here Hp–DLA where p is the number of reflectors in the factorization of the dictionary.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 24,
      "context" : "Comparisons, in terms of relative representation errors (40), of Gm– DLA against the DCT, Q–DLA [33], SK–SVD [40] and Householder based orthogonal dictionaries [26] denoted here Hp–DLA where p is the number of reflectors in the factorization of the dictionary.",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 24,
      "context" : "An interesting point of comparison is between the dictionaries constructed via Gm–DLA and Hp–DLA [26].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 38,
      "context" : "The computational complexity of H3–DLA approximately matches that of the DCT [41] (based on the FFT) while G85–DLA is actually computationally simpler than the DCT.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 37,
      "context" : "Figure 6 shows the performance of Rm–DLA in a regime close to the results of the SK–SVD dictionary learning method [40].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 39,
      "context" : "Pareto curves for Rm–DLA and the Sparse K–SVD approach [42].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 39,
      "context" : "In the last experimental setup we compare our Rm–DLA with the previously proposed Sparse K–SVD approach [42].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 37,
      "context" : "Experimental insights into how the representation performance scales with the number of atoms in the dictionary are given in [40], [43].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 40,
      "context" : "Experimental insights into how the representation performance scales with the number of atoms in the dictionary are given in [40], [43].",
      "startOffset" : 131,
      "endOffset" : 135
    } ],
    "year" : 2017,
    "abstractText" : "Given a dataset, the task of learning a transform that allows sparse representations of the data bears the name of dictionary learning. In many applications, these learned dictionaries represent the data much better than the static well-known transforms (Fourier, Hadamard etc.). The main downside of learned transforms is that they lack structure and therefore they are not computationally efficient, unlike their classical counterparts. These posse several difficulties especially when using power limited hardware such as mobile devices, therefore discouraging the application of sparsity techniques in such scenarios. In this paper we construct orthogonal and nonorthogonal dictionaries that are factorized as a product of a few basic transformations. In the orthogonal case, we solve exactly the dictionary update problem for one basic transformation, which can be viewed as a generalized Givens rotation, and then propose to construct orthogonal dictionaries that are a product of these transformations, guaranteeing their fast manipulation. We also propose a method to construct fast square but nonorthogonal dictionaries that are factorized as a product of few transforms that can be viewed as a further generalization of Givens rotations to the non-orthogonal setting. We show how the proposed transforms can balance very well data representation performance and computational complexity. We also compare with classical fast and learned general and orthogonal transforms.",
    "creator" : "LaTeX with hyperref package"
  }
}