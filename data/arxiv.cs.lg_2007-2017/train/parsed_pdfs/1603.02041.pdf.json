{
  "name" : "1603.02041.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Shared Representations for Value Functions in Multi-task Reinforcement Learning",
    "authors" : [ "Diana Borsa", "Thore Graepel", "John Shawe-Taylor" ],
    "emails" : [ "DIANA.BORSA@GMAIL.COM", "THORE@GOOGLE.COM", "J.SHAWE-TAYLOR@CS.UCL.AC.UK" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Reinforcement learning (RL) has gained a lot of popularity and has seen remarkable successes in the last years, exploiting and benefiting greatly from the recent developments in general functional approximators, such as neural networks (Mnih et al., 2015). At least part of this success seems to be linked to the ability of these universal functional approximators to distill meaningfully representations (Bengio, 2009), from high-dimensional input states.\nThese enabled RL to scale up to more complex environments and scenarios that were previously prohibited or required a great amount of feature engineering as shown in (Mnih et al., 2015), (Silver et al., 2016). Thus, learning a good abstraction of a given environment and the agent’s role in it, seems to be a key component in developing complex and optimal control mechanisms.\nWhile a lot of progress has been made in improving learning on individual single tasks, there seems to have been a lot less work in trying to re-use or efficiently transfer information from one task to the another (Taylor & Stone, 2009b). Nevertheless, it is natural to assume that the different tasks an agent needs to learn during its life, share a lot of structure and in-build redundancy. And potentially this could be leverage to speed-up learning. In this work we will propose a way to address this aspect, by learning robust, transferable abstractions of the environment that generalize over a set of tasks.\nValue functions are a central ideas in reinforcement learning (Sutton & Barto, 1998) and have been successfully used in conjunction with functional approximators to generalize over large state-action spaces. They are a concise way to readily assess the ”goodness” of a state and can be learnt efficiently even in an off-policy fashion. This enables us to decouple the data gathering and the learning process, but most importantly this allows us to re-use past experiences collected under arbitrary or exploratory policies (Sutton & Barto, 1998). More recently, value functions have been shown to exhibit a very nice compositional structure with respect to the state space and goal states (Schaul et al., 2015). This is consistent with earlier studies in (Sutton et al., 2011) that suggest value functions can capture and represent knowledge beyond their current goal that can be leveraged or re-used. Similar structures have been identified in the hierarchical reinforcement learning literature (Dietterich, 2000) or (Sutton et al., 1999). These all motivated our choice of explicitly modelling the presence of this shared structure in the state-action value space. ar X iv :1\n60 3.\n02 04\n1v 1\n[ cs\n.A I]\n7 M\nar 2\n01 6\nUsing a multi-task RL formulation and following the recent work done in (Calandriello et al., 2014), we firstly outline two general ways of learning RL tasks jointly and sharing knowledge across them by extending two of the most popular procedures for learning value function, Fitted Q-Iteration (Ernst et al., 2005) and Fitted Policy Iteration (Antos et al., 2007), to accommodate this shared structure assumption. Furthermore, taking advantage of the multitask methods developed in supervised settings, we extend the work in (Calandriello et al., 2014) to account for taskspecific components.\nWe will also show empirically that these lead to an overall improvement on the policies inferred, as well as a decrease in the number of samples per task needed to achieve good performance. We explore the nature of the representation learnt and its potential transferability to new, but related tasks. We show this learning is able infer a compressed structure that nevertheless captures a lot of transferable knowledge, similar to option-like transition models (Sutton et al., 1999) – without us ever specifying a partition of desirable states or subgoals. Finally we will argue that this way of learning, leads to more robust and refined representations which are deemed crucial for learning and planning in complex environments."
    }, {
      "heading" : "2. Proposed Model",
      "text" : ""
    }, {
      "heading" : "2.1. Background and Notation",
      "text" : "We define a Markov Decision Process (MDP) as a tuple M = (S,A,P,R, γ), where S is the set of states,A is the set of actions1, P : S × (S × A) → [0, 1] is the transition dynamics P(s′|s, a) which provides a probability over next state s′, R : S × A → R is a reward signal, which is assumed to be bounded (∃Rmax, s.t. R(s, a) ≤ Rmax,∀s ∈ S, a ∈ A) and γ ∈ [0, 1] is a discount factor.\nGiven an MDP and any policy π : S × A → [0, 1], we define the (state-action) value function, Qπ(s, a) as the discounted cumulative reward an agent is expected to collect when starting from state s ∈ S , taking action a ∈ A and then act accordingly to policy π:\nQπ(s, a) = Eπ,P [ ∞∑ t=0 γtrt|s = s0, a = a0 ] (1)\nThe expectation is over all trajectories starting in (s, a) and obtained by interacting with the environment (P) while following behaviour policy π.\nOur goal is to learn an optimal behaviour with respect to this expected cumulative reward. Thus we are looking for π∗ s.t.\nπ∗(s, a) = arg max πQ π(s, a) (2)\n1in this work this will be a finite set\nWe will denote this optimal value function as Q∗ = Qπ ∗ . And note that findingQ∗, automatically gives us an optimal policy π∗ by acting greedily with respect to these values. In the following, we denote this greedy operation by π∗ = GQ∗."
    }, {
      "heading" : "2.2. Problem formulation",
      "text" : "We will consider the scenario in which an agent resides (or is placed) into an environment in which it needs to perform a series of tasks. The overall goal is to learn how to succeed at all these tasks. The environment is described by a state-action space S × A and a transition kernel P(s′|s, a) and the tasks can be specified by different rewards signals Rt(s, a), one for each task t = 1, T . This formally gives rise to T MDPsMt = (S,A,P,Rt, γ) which share a lot of structure. Thus, if we can find a way to leverage this structure, we expect this to aid the learning process and lead to better generalization. (Taylor & Stone, 2009a)"
    }, {
      "heading" : "2.3. A shared (value function) representation",
      "text" : "We propose to model the shared structured found in above defined MDP-s as a shared embedding of the state-action space φ : S ×A → Rd on which we can build the individual optimal value functions {Q∗t }t=1,T for all considered tasks and potentially new ones. Thus in this paper we are interested in learning this shared embedding as well as ultimately the optimal behaviour for each of the tasks considered. In the following, we will present how one can extend two of the most popular paradigms of learning value functions, Fitted Q-Iteration and Fitted Policy Iteration, to incorporate this shared structure assumption. This will come down to employing a multi-task learning procedure in the target-fitting step of Q-Iteration and in the policy evaluation step of Policy-Iteration."
    }, {
      "heading" : "3. Multi-task (Fitted) Value Iteration",
      "text" : "In this section we outline a general framework of using approximate value iteration to infer the optimal Q-values (and optimal policies) for a set of tasks, in a given environment following the MT-RL setup previously introduced. The proposed algorithm is an extension of Fitted Q-Iteration (FQI) that allows for joint learning and transfer across tasks. Following the recipe of FQI, at each step in the iteration loop and for each sample in our experience set D = {(s, a, r, s′)|s′ ∼ P(.|s, a)}, we compute the onestep TD target based on our current estimate of the value function. Then, treating these estimates as ground truth, we obtain a regression problem from the state-action space onto the TD targets (which are really place-holders for the true value function). In the case of MT-RL, we will obtain such a regression problem for each task t. Now we could, in principle, solve all these regression problems in-\ndependently for each task, which would amount to applying FQI individually to each task. But our assumption is that there is shared structure between tasks and we would like to make use of this common ground to aid the learning process and arrive at more robust abstractions of the input space. Thus we propose solving the regression problems jointly, accounting for and building upon a common representation. A detailed description of the proposed procedure is outlined in Algorithm 1.\nAlgorithm 1 Multi-task Fitted Q-Iteration Require: D = ∪Tt=1Dt ∼ µ,P - set of experi-\nences/episodes for each task t\nInitialize Θ = Θ0, k = 0 (parameters)\nwhile not converged (dΘ < ||k < MaxIter) do Compute Targets: Y(k+1)t = {y (k+1) t (s, a) = Rt(s, a) +\nγmaxa′ Q (k)(s′, a′)|(s, a, s′) ∈ Dt},∀t = 1, T Multi-task Learning: Θ(k+1) = MTL(D = ∪Tt=1Dt,Y(k) = ∪Tt=1Y (k) t )\ndΘ = ‖Θ(k+1) −Θ(k)‖, k = k + 1 end while\nReturn: Θ = {θt}Tt=1 (⇒ Qt(s, a) = fθt(s, a),∀s, a ∈ S ×A)\nNote that, in the spirit of generalization, we do not specify a particular algorithm for the multi-task learning step(MTL\nin Algorithm 1). There is extensive literature of how to deal with multi-task inference and exploit shared structure between tasks in purely supervised settings, and we will take a look at some instantiations of this step throughout this work."
    }, {
      "heading" : "4. Multi-task (Fitted) Policy Iteration",
      "text" : "By a similar argument as the one presented in the last section for MT-FQI, we can extend the framework of general policy iteration to the MT-RL scenario. Policy Iteration algorithms rely on an alternating procedure between a policy evaluation step and a policy improvement step. We can extend this framework to the multi-task case, by defining a (current) set of policies Π = {πt}Tt=1, one for each task, and then we evolve this set of policies jointly at each iteration, k. Please find an outline of the proposed procedure in Algorithm 2. For now, we implement the policy improvement step by acting greedily with respect to our current estimates of the value function. This step is done individually for each task.\nOn the other hand, we allow joint learning and sharing of knowledge in the policy evaluation step. This gives rise to a general procedure we will call Multi-task Policy Evaluation (MT-PE)- see Algorithm 3. In MT-PE, we are given a set of policies Π = {πt}Tt=1, one for each task, and a collection of experiences D = ∪Tt=1Dt. Then the aim of the algorithm is to approximate the corresponding value functions Qπtt associated with acting out policy πt for task t.\nAlgorithm 2 Multi-Task Policy Iteration (MT-PI) Require: D = ∪Tt=1Dt, set of experiences for each task t\nInitialize Θ = Θ0, k = 0\nwhile convergence not reached (dΘ < ) do Policy Evaluation: Compute Θ(k) = MT-PE(D,Π(k)) via Algorithm 3 where Π(k) = [π1, · · · , πt, · · · , πT ] and Θ(k) = [θπ11 , · · · , θ πt t , · · · , θ πT T ]\nPolicy Improvement π\n(k+1) j (a|s) = arg maxaQ π (k) t t (s, a) =\narg maxa fθπtt (s, a) end while\nReturn: Θ = {θt}Tt=1 and policies Π = {πt}Tt=1 ≈ Π∗\nNote that, in general, this step (policy evaluation) requires on-policy data, for each policy πt and for each task t. This could be quite demanding and inefficient data-wise, as the numbers of tasks grow, not to mention that this is just the inner loop of another iterative algorithm (MT-PI). In this work, we opt for an implementation of the policy evaluation step that circumvents this problem. Making use of the Bellman Expectation Equation, we can compute regression targets for approximating {Qπtt } by only using experience of the form (s, a, rt, s′) previously collected, as we did in the case of Fitted-Q Iteration.\n∀(s, a, rt, s′) ∈ Dt → y(i)t = rt(s, a) + γQ (i) t (s ′, πt(s ′)) (3)\nTherefore, we have now reduced the original problem to a set of regression problems that can be solved jointly, under a shared input space representation. This is very similar to the multi-task learning step employed in MT-FQI, but now the shared structure is learnt to model the input set of policies Π = {πt}Tt=1, rather than the optimal ones. Nevertheless, by constantly improving the set of policies that are presented to the MT-PE step, we should eventually be able to convergence to the optimal policies and thus at this point, the policy evaluation step should be able to recover the shared structure amongst optimal value functions."
    }, {
      "heading" : "5. Multi-task and Representation Learning",
      "text" : "In this section will we look at a couple of methods we can plug into the above algorithms in the MTL step. For this we will assume a linear parametrization of the state-action value space -i.e. we assume ∃Φ = {φk : S × A → R} s.t. all value function of interest Qt can be well approximated by this linear combination of this set of features. In the case of fitted value iteration we want this set of features to fit well the intermediate targets Y(i), but ultimately we are\nAlgorithm 3 Multi-Task Policy Evaluation (MT-PE) Require: D = ∪Tt=1Dt, set of experiences for each task t\nA set of policies Π = {πt}Tt=1, for each task t that need to be evaluated\nInitialize Θ = Θ0, i = 0\nwhile convergence not reached (dΘ < ) do Compute Targets: Y(i+1)t = {y (i+1) t (s, a) = Rt(s, a) +\nγQ (i) t (s ′, πt(s ′))|(s, a, s′) ∈ Dt},∀t = 1, T Multi-task Learning: Θ(i+1) = MTL(D = ∪Tt=1Dt,Y(i) = ∪Tt=1Y (i) t )\ndΘ = ‖Θ(i+1) −Θ(i)‖, i = i+ 1 end while\nReturn: Θ = {θt}Tt=1( ⇒ Q̂πtt (s, a) = fθt(s, a) ≈ Q πt t (s, a),∀s, a ∈ S ×A )\ninterested in a set of features that fit well the optimal value functions Q∗t and we will see that this turns out to be very small subspace of the original feature space.\nQ∗t (s, a) = 〈Φ∗(s, a), wt〉,∀t = 1, T (4)\nIn the case of policy iteration, at each evaluation step we are interested in having a feature space that well approximates the value function corresponding to our current policies Π(k). Thus we are looking for ΦPE s.t. Q π (k) t t ≈ 〈ΦPE(s, a), w π (k) t t 〉,∀t = 1, T . As policies improve, we will end up fitting optimal or near-optimal value functions. Certainly if the regression step can be done perfectly (no approximation error), policy iteration will continue to improve the policies and in the limit will converge to the optimal value functions. Thus, the representation that will come out of this learning procedure should be similar to the ones learned by value-iteration procedures. Consequently, ultimately what we want in terms of representation is a (low-dimensional) features space that spans the optimal value functions of interest."
    }, {
      "heading" : "5.1. Multi-task feature learning",
      "text" : "In terms of planning, the joint problem we are trying to solve can be formalized as inferring {wt}Tt=1 = arg minW [∑ t L ( wTt Φ, Q̂t ) +H(W ) ] where H(W ) is a regularizer on the weight vectors, that encourages feature sharing. At the same time, we wish to learn a more compact abstraction of the state-action space, that will be shared among tasks. To make this a bit more formal, let Qt,w : S × A → R, Qt,w(s, a) := 〈Φ(s, a), wt〉, then our\nassumption can be expressed as: ∃ a small set of features {ψi}i=1,Nψ such that ∀t, Qt(s, a) = ∑ i αtiψi(s, a) = 〈αt, UTΦ(s, a)〉 where ψi-s form a basis for the relevant low-dim subspace.\nThus for each task t, we are trying to solve jointly the following optimization problem:\narg min f=〈αt,UTΦ〉 [∑ t LDt ( ft(s, a), y (k) t ) + ‖αt‖21 ] (5)\nIn (Argyriou et al., 2008) this was shown to be equivalent to Eq. 6 and can be solved efficiently by an alternating minimization procedure:\narg min A,U [∑ t LDt ( 〈αt, UTΦ(s, a)〉, y(k)t ) +H(A) ] (6) whereA = [α1, · · · , αt, · · · , αT ] and this is assumed to be sparse, takeH(A) = γ||A||22,1."
    }, {
      "heading" : "5.2. Allowing for task specificity",
      "text" : "The above procedure can be used to construct very informative shared features – as shown in (Calandriello et al., 2014) and in our experimental section. However, in a lot of scenarios tasks can benefit from having a small and sparse set of features that represent the particularities of each individual task on top of a low-dimensional shared subspace. This is definitely the case in many practical applications and had been observed in purely supervised settings as well – it is simply too restricted to constrain all tasks to be using a single shared structure. Thus researchers have come up with various ways of incorporating task-specific components — see (Zhou et al., 2011), (Jalali et al., 2010), (Chen et al., 2012) and reference therein – and showed that modelling these explicitly can improve both the learning (accuracy and speed) and interpretability of the resulting representations. In this work, we choose just one of these formulations, introduced in (Ando & Zhang, 2005), where we learn a low-dimensional shared representationUΦ(s, a) as before, as well as a task specific vector wt, on which we place a strong sparsity constraint to encourage common features to still be identified and shared.\nQt,θ(s, a) = (w T t + v T t U)Φ(s, a) (7)\nNote that if U is a zero matrix, then we will be treating the task as completely independent and on the other hand if wt is zero for all tasks, we recover the previous formulation. Furthermore, we can place an orthogonality condition on the set of shared features inferred, by enforcing UUT = I .\nThe resulting optimization problem has the form:\narg min U,{vt,wt}Tt=1 [∑ t L ( (wTt + v T t U)Φ, Q̂t ) +H(W ) ] (8) and can be solved by Alternating Structure Optimization (ASO) – see (Ando & Zhang, 2005)."
    }, {
      "heading" : "6. Experiments",
      "text" : "We assess the performance and behaviour of our proposed model and learning procedures in the 4-room navigation task (Sutton et al., 1999). The state space S is described by all valid positions an agent might take – any position in the grid, but the wall and the agent has access to four actions A = {→,←, ↑, ↓}. We consider a deterministic dynamics in those directions and all walls are considered elastic – bumping into walls has no effect on your state. Tasks are specified as target locations in the environment the agent need to navigate to. These will be sampled at random from the valid states in the environment. We do not specify a starting state – agents need to learn to navigate to the selected goal position from any part of the environment. When the agent transitions to the goal states, it collects a positive reward. No other reward signal is provided.\nSince all of the proposed methods can be run off-policy, thus decoupling the experience gathering and the learning, we sample a modest amount of experience up front for each of the considered tasks Dt. This can be done, in principle, by any behaviour policy, but in all our data-gathering we employ uniformly random exploration.\nOnce data is gathered or provided, we can proceed with the learning. All experiments were conducted under a restrictive sample budget |Dt| ∈ {500, 750, 1000}. Firstly, we would like to compare the proposed joint-representation learning with its single tasks counterparts of FQI and FPI to see what effects, if any, enforcing and learning a shared representation would have. We assess the quality of the inferred greedy policies by the amount of reward they are able to produce under random starts – this is a proxy for the real value function V πt , where πt = GQ̂t.\nV πtemp ≈ Es0∼µ [ ∞∑ k=0 γkrk|s0, πt ] ≈ V πt\nDepending on the selection of starting states, the difficulty of the tasks and thus amount of reward achievable may vary. To ease interpretation, we report the normalized value of the above estimate, with respect to the optimal value function at the starting states. Example results for the first 10 training tasks are displayed in Figure 2. These were obtained by training on 30 randomly sampled tasks on 500 samples of experience per task. We can see that\nthe joint-learning procedures manage to learn good policies, quite close to optimal ones that substantially outperform the single-task learning. Please note that our proposed extension to allow task-specific features, in most cases, improves performance, even when considering a very small set of common features (dshared = 5) - which also gives us a much faster convergence in the shared subspace. Indeed, this behaviour seems to be consistent to lower samples sizes, although it is worth mentioning that divergence does occur more often in these extrem conditions (very few samples) and regularization parameters that might ensure convergence (Calandriello et al., 2014) provide a solution that is often worse than even the single task. Outside, those extreme cases, policy and value iteration methods perform very similarly and as we can see from Figure 3 - 4, that they tend to converge to the same solution.\nTo get a better idea of the average task performance we obtain and how that changes during training, we can look at the average distance between our estimate of the value functions at iteration k and the optimal ones Q∗t . For this small environment, these can be computed analytically. Results for 500 and 1000 samples budgets are displayed in Figure 3. We observe quite a big difference between the single-task and multi-task procedures in terms of recovering the true optimal value functions. Convergence to a better MSE happens much faster and we get even asymptotic superior solution. Nevertheless, closeness to the optimal value functions in Euclidean space may not necessarily imply the same relation in policy space. A plot of the quality of policies as a function of value/policy iterations\nis available in Figure 4. Here, we report the normalized average regret 1T ∑ t ||V ∗t − V πt emp,t||/||V ∗t ||. We can see that the policies in general will converge much faster than the value functions, when comparing with the Q-value convergence in Figure 3. Please also note that the multi-task Fitted Policy-Iteration procedures inherit the same speedy convergence present in the single-task counterpart."
    }, {
      "heading" : "6.1. Learnt shared representations",
      "text" : "Probably the most interesting phenomenon encountered in learning these shared representations is the nature of the low dimensional representations inferred. We visualize the inferred set of shared features (Figure 5) and their respective weights in the value-function (Figure 6).\nThese were produced via MT-FQI with ASO, with the constraint that the shared subspace has at most 5-dimensions. And even this seem to be too permissive, as we actually obtain strong activations only for the top 3 features inferred ψ1:3 – presented in Figure 5. Thus the learnt representation is very low dimensional, but at the same time expressive enough to effectively approximate optimal value functions."
    }, {
      "heading" : "6.2. Transferring knowledge to new tasks",
      "text" : "The learnt representations resemble option-like features (Sutton et al., 1999) that essentially inform the agent, across tasks, how to navigate efficiently between rooms and negotiate the narrow hallways. These are indeed easily transferable ’skills’ that can be use in learning a new task. We test this hypothesis by augmenting the represen-\nψ 1 (s, ↓ )\n2 4 6 8 10\n2 4 6 8 10\nψ 1 (s, →)\n2 4 6 8 10\n2 4 6 8 10\nψ 1 (s, ↑ )\n2 4 6 8 10\n2 4 6 8 10\nψ 1 (s, ←)\n2 4 6 8 10\n2 4 6 8 10\nψ 2 (s, ↓ )\n2 4 6 8 10\n2 4 6 8 10\nψ 2 (s, →)\n2 4 6 8 10\n2 4 6 8 10\nψ 2 (s, ↑ )\n2 4 6 8 10\n2 4 6 8 10\nψ 2 (s, ←)\n2 4 6 8 10\n2 4 6 8 10\nψ 3 (s, ↓ )\n2 4 6 8 10\n2 4 6 8 10\nψ 3 (s, →)\n2 4 6 8 10\n2 4 6 8 10\nψ 3 (s, ↑ )\n2 4 6 8 10\n2 4 6 8 10\nψ 3 (s, ←)\n2 4 6 8 10\n2 4 6 8 10\nTask ID (t)\nCoefficients α t (read columns)\nTask ID (t)\n5 10 15 20 25 30W eig\nht s f\nor (N\nEW ) f\nea tu\nre s I\nD (\nΨ 2:3\n)\n0.5\n1\n1.5\n2\n2.5\n-40 -30\n-20 -10\n0 10\n20 30\n40\nFigure 6. Weighting Coefficients α1:3,t and α2:3,t for the above three most prominent shared features. We can see from these values that the first feature clearly dominated in all tasks. Bottom: Rescaled version of α2:3,t such that we can see the activation of the other two prominent features. Blue corresponds to negative activation and red to positive ones. Given the nature of the features one can readily read out, just by looking at the sign of the weight, which room that task’s goal state is. For instance, if we look at second task: negative activation for both ψ2 ⇒ north-side of the environment and ψ3 ⇒ west-side of the environment. The goal G2 is indeed located at position (2, 1) in the top-left room.\ntation for the new task, with this shared subspace. We investigate the benefits of having learnt a shared subspace over a set of training tasks in terms of transferring that knowledge when optimizing for a new task. We augment the feature space for the new task, with the learnt features ψs and then we assess the effect this modification has on learning the new task. In Figure 7, we present an empirical evaluation of the cumulative regret the agent will incur on the inferred (greedy) policy, when trained on the original representation φ, versus the augmented representation {ψs, φ}, after seeing a varying amount of samples N = 50, 200, 300, 500, 700, 1000, 2000. We can see that the augmented representation is able to produce a good performance under smaller sample sizes. In general, the learning based on the transferred representation is able to produce a policy that is equivalent to the ones we could learn without transfer under twice as much data. This behaviour is consistent until convergence."
    }, {
      "heading" : "6.3. Connection to Options",
      "text" : "As previously, the learnt shared representation seems account for the general topology and dynamics of the environment in the value functions. They nicely partition the environment into relevant regions to facilitate the global navigation to a local neighbourhood of the goal. Some of those features are characteristic of options (Sutton et al., 1999), skills (Konidaris & Barto, 2007), macro-actions literature (Dietterich, 2000) and are hve the potential to drastically improve the efficiency and scalability of RL methods (Barto & Mahadevan, 2003), (Hengst, 2002). In the\nfollowing we would like to investigate this connection further.\nFollowing the formulation in (Sutton et al., 1999), an option o = 〈I, µ, β〉, is a generalization of primitive actions a ∈ A to a temporally extended course of action. I is the initiation set I ⊆ S from which the option is available, µ is the policy we are going to follow once the options is triggered and β : S → [0, 1] is the probability of termination. In this case, the value function take the form:\nQπtt (s, o) = E  K∑ k=0\nγkrt,k︸ ︷︷ ︸ rt(s,o)\n+γK+1 ∑ s′ P (s′|s, o)V πt (s′)  where s′ is in the termination state of options o. We denote P oss′ = ∑∞ k po(s\n′, k)γk, where po(s′, k) is the probability that options owill terminate in state s′ after exactly k steps. Note that this term accounts for the transition dynamics, the policy of the option and its termination criteria, all of which, for us, are task-invariant. Moreover note that for us, rt(s, o) is generally 0 unless the option happens to hit the goal. Thus the above equation, simplifies to:\nQπtt (s, o) = ∑ s′  P oss′︸︷︷︸ task independent(t,πt) · V πt(s′)︸ ︷︷ ︸ task dependent(t,πt)  This is a linear combination between the option transition models P oss′ = φ\nµo(s, s′) in the termination set (subgoals of the option) – which is independent of task t and πt, it only depends on µo – weighted by the value function of the termination states for each of the tasks – which incorporates the dependency on task and the individual policy employed after the option has terminated. This is very similar to the parametrization we assumed in Eq. 4. This suggest that the learnt representation is able to capture and represent efficiently some option-like transition models without specifying any subgoals, policies nor initial states. We hypothesize that the learnt shared space is actually a compressed basis for these option-transition models. In order to test this hypothesis, we consider an intuitive set of options O (like navigation to a particular room) and test if this learnt basis can span P oss′ for some option o ∈ O and can successfully represent an option-policy.\nWe define an option o1 to be navigating to a specific room, say room 1 (NW). The initialization set Io1 is the set of states outside the room and termination set is any state in the desired room. We also can define an MDP that maintains the same transition dynamics, state and action space, but now the reward signal is zero outside the target room and a constant positive reward in any of the desired termination states s′ ∈ room 1. Note that the value func-\ntion corresponding to this newly defined semi-MDP (Sutton et al., 1999) is given by: Qπ(s, o) = ∑ s′ P o ss′ , as V π(s′) = const 2. In this semi-MDP we run FQI and indeed see that we are able to construct a value function, based solely on the learnt 5-dim feature space ψs, that successfully completes the specified task. Results for all such navigation options are available in Figure 8.\n1 2 3 4 5 6 7 8 9 10\n1 2 3 4 5 6 7 8 9 10\nPlease note that the above defined options are quite extended ones. Simpler ones would include making your way outside a particular room – these are along the lines of the options defined in (Sutton et al., 1999) and (Stolle & Precup, 2002) – and these can be easily recovered as well. Actually for these simpler options we require very few samples to obtain the desired behaviour (10-30 samples), although they might not be optimal – please consult Supplementary material for details. The fact that we are able to express a whole variety of such intuitively defined options – much more than the dimensionality of the common subspace on which we are building on – is a clear indication of the expressiveness of this shared representation and its potential transferability in aiding learning of new tasks within\n2This is actually true, only under a mild assumption that the agent under π(s′) will not leave the room, which is where all the reward is.\nthe same environment."
    }, {
      "heading" : "7. Related work",
      "text" : "There is a good collection of methods that tackle various aspect of multi-task reinforcement learning (Lazaric, 2012) and (Taylor & Stone, 2009a). As with our approach, these methods try to learn jointly either value functions or policies over a set of tasks (Lazaric & Ghavamzadeh, 2010), (Dimitrakakis & Rothkopf, 2011) but under different structure and environment assumptions. A more recent study in (Konidaris et al., 2012), also employs the idea of a shared feature space, but both the learning procedure and the proposed way of transferring this tuned knowledge is very different from ours. The main novel idea this work introduces is modelling explicitly a shared abstraction of the state-action space that can be refined throughout the learning process while optimizing for the value functions. The ability to change the representation throughout the learning process to model the improving set of policies is crucial. This is the only way option-like features could emerges – these already incorporate both the transition model and good policies that generalize over tasks, as shown in the previous section. One of the methods investigated in (Calandriello et al., 2014) in a study on sparsity in multi-task, is very closely related to our learning procedure and this work can be seen as a generalization of that method, although the focus and model assumptions are quite different. Perhaps the most relevant prior work that shares our vision and some of the modelling assumption is the approach in (Schaul et al., 2015) which model a shared staterepresentation between goals and assume a linear factorization between this state embbeding and task/goal embbeding."
    }, {
      "heading" : "8. Conclusion and Future work",
      "text" : "In this work, we investigated the problem of representation learning in multi-task/multi-goal reinforcement learning. We introduced the multi-task RL paradigm and showed how two of the most popular classes of planning algorithms, fitted Q-Iteration and approximate Policy Iteration, can be extended to learn from multiple tasks jointly. Focusing on linear parametrization of theQ-function, we showed at least two ways in which one can harness the power of well-established multi-task learning and transfer algorithms developed in supervised settings and apply them to inferring a joint structure over optimal value functions, and implicitly over policies. As argued before and shown in these preliminary experiments, RL can benefit a lot from integrating joint treatment of goals and exploiting commonality between tasks. This ought to lead to more efficient learning and better generalization. Although these are very encouraging results, this paradigm does need more\ninvestigation to assess convergence behaviour, scalability to more complex tasks, employing other multi-task learning or representation learning procedures and we hope this work will serve as staring point."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "We investigate a paradigm in multi-task reinforcement learning (MT-RL) in which an agent is placed in an environment and needs to learn to perform a series of tasks, within this space. Since the environment does not change, there is potentially a lot of common ground amongst tasks and learning to solve them individually seems extremely wasteful. In this paper, we explicitly model and learn this shared structure as it arises in the state-action value space. We will show how one can jointly learn optimal value-functions by modifying the popular valueiteration and policy-iteration procedures to accommodate this shared representation assumption and leverage the power of multi-task supervised learning. Finally, we demonstrate that the proposed model and training procedures, are able to infer good value functions, even under low samples regimes. In addition to data efficiency, we will show in our analysis, that learning abstractions of the state space jointly across tasks leads to more robust, transferable representations with the potential for better generalization.",
    "creator" : "LaTeX with hyperref package"
  }
}