{
  "name" : "1605.08636.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "PAC-Bayesian Theory Meets Bayesian Inference",
    "authors" : [ "Pascal Germain", "Francis Bach", "Alexandre Lacoste", "Simon Lacoste-Julien" ],
    "emails" : [ "firstname.lastname@inria.fr", "allac@google.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Since its early beginning [Shawe-Taylor and Williamson, 1997], the PAC-Bayesian theory claims to provide “PAC guarantees to Bayesian algorithms” [McAllester, 1999]. However, despite the amount of work dedicated to this statistical learning theory—many authors improved the initial results1 and/or generalized them for various machine learning setups2—it is mostly used as a frequentist method. That is, under the assumptions that the learning samples are i.i.d.-generated by a data-distribution, this theory expresses probably approximately correct (PAC) bounds on the generalization risk. In other words, with probability 1−δ, the generalization risk is at most ε away from the training risk. The Bayesian side of PAC-Bayes comes mostly from the fact that these bounds are expressed on the averaging/aggregation/ensemble of multiple predictors (weighted by a posterior distribution) and incorporate prior knowledge. Although it is still sometimes referred as a theory that bridges the Bayesian and frequentist approach [e.g., Guyon et al., 2010], it has been merely used to explicitly justify Bayesian methods until now.3\nIn this work, we provide (up to our knowledge) the first direct connection between Bayesian inference techniques [summarized by Ghahramani, 2015] and PAC-Bayesian theory in a general setup. Our study is based on a simple but insightful connection between the Bayesian marginal likelihood and PAC-Bayesian bounds, that we obtain by considering the negative log-likelihood loss function (Section 3). By doing so, we provide an alternative explanation for the Bayesian Occam’s razor criteria [Jeffreys and Berger, 1992, MacKay, 1992] in the context of model selection, explained as the complexity-accuracy trade-off appearing in most PAC-Bayesian results. In Section 4, we extend PAC-Bayes theorems to regression problems with unbounded loss, adapted to the negative log likelihood loss function. Finally, we study the Bayesian model selection from a PAC-Bayesian perspective (Section 5), and illustrate our finding on classical Bayesian regression tasks (Section 6).\n1Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc. 2Langford and Shawe-Taylor [2002], Seldin and Tishby [2010], Seldin et al. [2011, 2012], Bégin et al. [2014], Pentina and Lampert [2014], etc. 3Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Grünwald and Langford [2007], Lacoste-Julien et al. [2011] for other studies drawing links between frequentist statistics and Bayesian inference.\nar X\niv :1\n60 5.\n08 63\n6v 1\n[ st\nat .M\nL ]\n2 7\nM ay\n2 01"
    }, {
      "heading" : "2 PAC-Bayesian Theory",
      "text" : "We denote the learning sample (X,Y )={(xi, yi)}ni=1∈(X×Y)n, that contains n input-output pairs. The main assumption of frequentist learning theories—including PAC-Bayes—is that (X,Y ) is randomly sampled from a data generating distribution that we denoteD. Thus, we denote (X,Y )∼Dn the i.i.d. observation of n elements. From a frequentist perspective, we consider in this work loss functions ` : F×X×Y → R, where F is a (discrete or continuous) set of predictors f : X → Y , and we write the empirical risk on the sample (X,Y ) and the generalization error on distribution D as\nL̂ `X,Y (f) = 1\nn n∑ i=1 `(f, xi, yi) ; L `D(f) = E (x,y)∼D `(f, x, y) .\nThe PAC-Bayesian theory [McAllester, 1999, 2003] studies an averaging of the above losses according to a posterior distribution ρ̂ overF . That is, it provides probably approximately correct generalization bounds on the (unknown) quantity Ef∼ρ̂ L `D(f) = Ef∼ρ̂ E(x,y)∼D `(f, x, y) , given the empirical estimate Ef∼ρ̂ L̂ `X,Y (f) and some other parameters. Among these, most PAC-Bayesian theorems rely on the Kullback-Leibler divergence KL(ρ̂‖π) = Ef∼ρ̂ ln[ρ̂(f)/π(f)] between a prior distribution π over F—specified before seeing the learning sample X,Y—and the posterior ρ̂—typically obtained by feeding a learning process with X,Y .\nTwo appealing aspects of PAC-Bayesian theorems are that they provide data-driven generalization bounds that are computed on the training sample (i.e., they do not rely on a testing sample) and that are uniformly valid for all ρ̂ over F . This explains why many works study them as model selection criteria or as an inspiration for learning algorithm conception. Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al. [2013], Noy and Crammer [2014]. Theorem 1 (Catoni, 2007). Given a distribution D over X × Y , a hypothesis set F , a loss function `′ : F × X × Y → [0, 1], a prior distribution π over F , a δ ∈ (0, 1], and a real number β > 0, with probability at least 1− δ over the choice of (X,Y ) ∼ Dn, we have\n∀ρ̂ on F : E f∼ρ̂ L ` ′ D (f) ≤ 1 1− e−β\n[ 1− e−β Ef∼ρ̂ L̂ `′ X,Y (f)− 1 n ( KL(ρ̂‖π)+ ln 1δ )] . (1)\nTheorem 1 is limited to loss functions mapping to the range [0, 1]. Through a straightforward rescaling we can extend it to any bounded loss, i.e., ` : F ×X ×Y → [a, b], where [a, b] ⊂ R. This is done by using β := b− a and with the rescaled loss function `′(f, x, y) := (`(f, x, y)−a)/(b−a) ∈ [0, 1] . After few arithmetic manipulations, we can rewrite Equation (1) as\n∀ρ̂ on F : E f∼ρ̂ L `D(f) ≤ a+ b−a1−ea−b\n[ 1− exp ( −E f∼ρ̂ L̂ `X,Y (f)+a− 1n ( KL(ρ̂‖π)+ ln 1δ ))] . (2)\nFrom an algorithm design perspective, Equation (2) suggests optimizing a trade-off between the empirical expected loss and the Kullback-Leibler divergence. Indeed, for fixed π, X , Y , n, and δ, minimizing Equation (2) is equivalent to find the distribution ρ̂ that minimizes\nn E f∼ρ̂ L̂ `X,Y (f) + KL(ρ̂‖π) . (3)\nAs mentioned by Zhang [2006], Catoni [2007], Germain et al. [2009], Lever et al. [2013], Alquier et al. [2015], the optimal Gibbs posterior ρ̂∗ is given by\nρ̂∗(f) = 1ZX,Y π(f) e −n L̂ `X,Y (f) , (4)\nwhere ZX,Y is a normalization term. Notice that the constant β is now absorbed in the loss function as the rescaling factor setting the trade-off between the expected empirical loss and KL(ρ̂‖π)."
    }, {
      "heading" : "3 Bridging Bayes and PAC-Bayes",
      "text" : "In this section, we show that by choosing the negative-log-likelihood loss function, minimizing the PAC-Bayes bound is equivalent to maximizing the Bayesian marginal likelihood. To obtain this\nresult, we first consider the Bayesian approach that starts by defining a prior p(θ) over the set of possible model parameters Θ. This induces a set of probabilistic estimators fθ ∈ F , mapping x to a probability distribution over Y . Then, we can estimate the likelihood of observing y given x and θ, i.e., p(y|x, θ) ≡ fθ(y|x).4 Using Bayes’ rule, we obtain the posterior p(θ|X,Y ):\np(θ|X,Y ) = p(θ) p(Y |X, θ) p(Y |X) ∝ p(θ) p(Y |X, θ) , (5)\nwhere p(Y |X, θ) = ∏n i=1 p(yi|xi, θ) and p(Y |X) = Eθ∼p(θ) p(Y |X, θ).\nTo bridge the Bayesian approach with the PAC-Bayesian framework, we consider the negative log-likelihood loss function [see Banerjee, 2006], denoted `nll and defined by\n`nll(fθ, x, y) ≡ − ln p(y|x, θ) . (6)\nThen, we can relate the empirical loss L̂ `X,Y of a predictor to its likelihood:\nL̂ `nllX,Y (θ) = 1\nn n∑ i=1 `nll(θ, xi, yi) = − 1 n n∑ i=1 ln p(yi|xi, θ) = − 1 n ln p(Y |X, θ) ,\nor, the other way around, p(Y |X, θ) = e−nL̂ `nll X,Y (θ). (7)\nUnfortunately, existing PAC-Bayesian theorems work with bounded loss functions or in very specific context [as Zhang, 2006, Dalalyan and Tsybakov, 2008], and `nll spans the whole real axis in its general form. To this end, in Section 4, we explore PAC-Bayes bounds for unbounded losses. Meanwhile, we consider priors with bounded likelihood. This can be done by assigning a prior of zero to any θ yielding − ln p(y|x, θ) /∈ [a, b]. Now, using Equation (7) in the optimal posterior (Equation 4) simplifies to:\nρ̂∗(θ) = π(θ) e−n L̂\n`nll X,Y (θ)\nZX,Y = p(θ) p(Y |X, θ) p(Y |X) = p(θ|X,Y ) , (8)\nwhere the normalization constant ZX,Y corresponds to the Bayesian marginal likelihood: ZX,Y ≡ p(Y |X) = ∫\nΘ\nπ(θ) e−n L̂ `nll X,Y (θ)dθ . (9)\nThis shows that the optimal PAC-Bayes posterior given by the generalization bound of Theorem 1 coincides with the Bayesian posterior, when one chooses `nll as loss function and β := b−a (as in Equation 2). Moreover, using the posterior of Equation (8) inside Equation (3), we obtain\nn E θ∼ρ̂∗\nL̂ `nllX,Y (θ) + KL(ρ̂ ∗‖π) (10)\n= n ∫ Θ π(θ) e −n L̂ `nll X,Y (θ) ZX,Y L̂ `nllX,Y (θ) dθ + ∫ Θ π(θ) e −n L̂ `nll X,Y (θ) ZX,Y ln [ π(θ) e −n L̂ `nll X,Y (θ) π(θ)ZX,Y ] dθ\n= ∫ Θ π(θ) e −n L̂ `nll X,Y (θ) ZX,Y [ ln 1ZX,Y ] dθ = ZX,Y ZX,Y ln 1ZX,Y = − lnZX,Y .\nIn other words, minimizing the PAC-Bayes bound is equivalent to maximizing the marginal likelihood. Thus, from the PAC-Bayesian standpoint, the latter encodes a trade-off between the averaged negative log-likelihood loss function and the prior-posterior Kullback-Leibler divergence.5 Although it appears in essence a very different problem, we note that the relation derived in Equation (10) is similar to the one used by variational Bayesian methods, which approximate a hardly computable Bayesian posterior by the “closest” distribution belonging to a parametrized family.\nWe conclude this section by proposing a compact form of Theorems 1 by expressing it in terms of the marginal likelihood, as a direct consequence of Equation (10).\n4To stay aligned with the PAC-Bayesian setup, we only consider the discriminative case in this paper. One can extend to the generative setup by considering the likelihood of the form p(y, x|θ) instead.\n5To our knowledge, this is the first time this has been reported in a general setup. The thesis of Seeger [2003, Section 3.2] foreseeing this by noticing that “the log marginal likelihood incorporates a similar trade-off as the PAC-Bayesian theorem”, but using another variant of the PAC-Bayes bound and in the context of classification.\nCorollary 2. Given a data distribution D, a parameter set Θ, a prior distribution π over Θ, a δ ∈ (0, 1], if `nll lies in [a, b], we have, with probability at least 1− δ over the choice of (X,Y ) ∼ Dn,\nE θ∼ρ̂∗\nL`nllD (θ) ≤ a+ b−a 1−ea−b\n[ 1− ea n √ ZX,Y δ ] , (11)\nwhere ρ̂∗ is the Gibbs optimal posterior (Eq. 8) and ZX,Y is the marginal likelihood (Eq. 9).\nIn Section 5, we exploit the link between PAC-Bayesian bounds and Bayesian marginal likelihood to expose similarities between both frameworks in the context of model selection. Beforehand, next Section 4 extends the PAC-Bayesian generalization guarantees to unbounded loss function. This is mandatory to make our study fully valid, as the negative log-likelihood loss function is in general unbounded (as well as other common regression losses)."
    }, {
      "heading" : "4 PAC-Bayesian Bounds for Regression",
      "text" : "This section aims to extend the PAC-Bayesian results of Section 3 to real valued unbounded loss. These result are used in forthcoming sections to study `nll, but they are valid for broader classes of loss functions. Importantly, our new results are focused on regression problems, as opposed to the usual PAC-Bayesian classification framework.\nThe new bounds are obtained through a recent theorem of Alquier et al. [2015], stated below (we provide a proof in Appendix A.1 for completeness). Theorem 3 (Alquier et al. [2015]). Given a distribution D over X × Y , a hypothesis set F , a loss function ` : F × X × Y → R, a prior distribution π over F , a δ ∈ (0, 1], and a real number λ > 0, with probability at least 1−δ over the choice of (X,Y ) ∼ Dn, we have\n∀ρ̂ on F : E f∼ρ̂ L `D(f) ≤ E f∼ρ̂ L̂ `X,Y (f) +\n1\nλ\n[ KL(ρ̂‖π) + ln 1\nδ + Ψ`,π,D(λ, n)\n] , (12)\nwhere Ψ`,π,D(λ, n) = ln E f∼π E X′,Y ′∼Dn\nexp [ λ ( L `D(f)− L̂ `X′,Y ′(f) )] . (13)\nAlquier et al. [2015] used Theorem 3 to design a learning algorithm for {0, 1}-valued classification losses. Indeed, a bounded loss function ` : F × X × Y → [a, b] can be used along with Theorem 3 by applying the Hoeffding’s lemma to Equation (13), that gives Ψ`,π,D(λ, n) ≤ λ2(b−a)2/(2n). More specifically, with λ := n, we obtain the following bound\n∀ρ̂ on F : E f∼ρ̂ L `D(f) ≤ E f∼ρ̂ L̂ `X,Y (f) + 1n\n[ KL(ρ̂‖π) + ln 1δ ] + 12 (b− a) 2. (14)\nNote that the latter bound leads to the same trade-off as Theorem 1 (expressed by Equation 3). However, the choice λ := n has the inconvenience that the bound value is at least 12 (b− a)\n2, even at the limit n→∞. Note that another choice that makes the bound converge is λ := √ n:\n∀ρ̂ on F : E f∼ρ̂ L `D(f) ≤ E f∼ρ̂ L̂ `X,Y (f) + 1√n\n[ KL(ρ̂‖π) + ln 1δ + 1 2 (b− a) 2 ] . (15)\nA similar result to Equation (15) leads to long-life learning algorithms in Pentina and Lampert [2014].\nSub-Gaussian losses. In a regression context, it may be restrictive to consider strictly bounded loss functions. Therefore, we extend Theorem 3 to sub-Gaussian loss functions. We say that an unbounded loss function ` is sub-Gaussian with a variance factor s2 under a prior π and a data-distribution D if it can be described by a sub-Gaussian random variable V , i.e., its moment generating function is upper bounded by the one of a normal distribution of variance s2 [see Boucheron et al., 2013, Section 2.3]:\nψ V\n(λ) = ln E exp [ λ(V −EV ) ] ≤ λ 2s2\n2 , ∀λ ∈ R , (16)\nThe above sub-Gaussian assumption corresponds to the Hoeffding assumption of Alquier et al. [2015], and allows to obtain the following result. Corollary 4. Given D, F , `, π and δ defined in Theorem 3 statement, if the loss is sub-Gaussian with variance factor s2, we have, with probability at least 1−δ over the choice of (X,Y ) ∼ Dn,\n∀ρ̂ on F : E f∼ρ̂ L `D(f) ≤ E f∼ρ̂ L̂ `X,Y (f) + 1n\n[ KL(ρ̂‖π) + ln 1δ ] + 12 s 2.\nProof. For i = 1 . . . n, we denote `i a i.i.d. realization of the random variable `(f, x, y)−L̂ `X′,Y ′(f). Ψ`,π,D(λ, n) = ln E exp [ λ n ∑n i=1 `i ] = ln ∏n i=1 E exp [ λ n`i ] = ∑n i=1 ψ`i( λ n ) ≤ n λ2s2 2n2 = λ2s2 2n ,\nwhere the inequality comes from the sub-Gaussian loss assumption (Equation 16). The result is then obtained from Theorem 3, with λ := n.\nSub-Gamma losses. We say that an unbounded loss function ` is sub-Gamma with a variance factor s2 and scale parameter c, under a prior π and a data-distribution D, if it can be described by a re-centered sub-Gamma random variable V−EV [see Boucheron et al., 2013, Section 2.4], that is\nψ V\n(λ) ≤ s 2 c2 (− ln(1−λc)− λc) ≤ λ2s2 2(1−cλ) , ∀λ ∈ (0, 1 c ) .\nUnder this sub-Gamma assumption, we obtain the following new result, which is necessary to study the linear regression in next sections. Corollary 5. Given D, F , `, π and δ defined in Theorem 3 statement, if the loss is sub-Gamma with variance factor s2 and scale c < 1, we have, with probability at least 1−δ over (X,Y ) ∼ Dn,\n∀ρ̂ on F : E f∼ρ̂ L `D(f) ≤ E f∼ρ̂ L̂ `X,Y (f) + 1n\n[ KL(ρ̂‖π) + ln 1δ ] + 12(1−c) s 2 . (17)\nAs a special case, with ` := `nll and ρ̂ := ρ̂∗ (Eq. 8), we have E θ∼ρ̂∗ L`nllD (θ) ≤ s2 2(1−c) − 1 n ln (ZX,Y δ) .\nProof. Following the same path as in Corollary 4 proof (with λ := n), we have Ψ`,π,D(n, n) = ln E exp [ ∑n i=1 `i] = ln ∏n i=1 E exp [`i] = ∑n i=1 ψ`i(1) ≤ n s2 2(1−c) = n s2 2(1−c) ,\nwhere the inequality comes from the sub-Gamma loss assumption, with 1 ∈ (0, 1c ).\nSquared loss. The parameters s2 and c of Corollary 5 rely on the chosen loss function and prior, and the assumptions concerning the data distribution. As an example, consider a regression problem where X ×Y = Rd ×R, a family of linear predictors fw(x) = w · x, with w ∈ Rd, and a Gaussian prior π ∼ N (0, σ2π). Let assume that the input examples lie inside a ball of radius γ and the label of x is given by y = w∗ · x + , where ∼ N (0, σ2 ) is a Gaussian noise. Under the squared loss function `sqr(fw,x, y) = (w · x − y)2, we show in Appendix A.3 that Corollary 5 is valid with s2 ≥ 2‖w∗‖2γ2 and c ≤ 2(σ2 + σ2πγ2)2. The latter term tells us that the bound degrades when the noise increases, as expected. Empirical values of the bound under the squared loss are computed in Section 6.\nRegression versus classification. The classical PAC-Bayesian theorems are stated in a classification context and bound the generalization error/loss of the stochastic Gibbs predictor Gρ̂. In order to predict the label of an example x ∈ X , the Gibbs predictor first draws a hypothesis h ∈ F according to ρ̂, and then returns h(x). Maurer [2004] shows that we can generalize PAC-Bayesian bounds on the generalization risk of the Gibbs classifier to any loss function with output between zero and one. Provided that y ∈ {−1, 1} and h(x) ∈ [−1, 1], a common choice is to use the linear loss function `′01(h, x, y) = 1 2 − 1 2y h(x). The Gibbs generalization loss is then given by RD(Gρ̂) = E(x,y)∼D Eh∼ρ̂ ` ′ 01(h, x, y) . Many PAC-Bayesian works use RD(Gρ̂) as a surrogate loss to study the zero-one classification loss of the majority vote classifier RD(Bρ̂):\nRD(Bρ̂) = Pr (x,y)∼D ( y E h∼ρ̂ h(x) < 0 ) = E (x,y)∼D I [ y E h∼ρ̂ h(x) < 0 ] , (18)\nwhere I[·] being the indicator function. Given a distribution ρ̂, an upper bound on the Gibbs risk is converted on an upper bound on the majority vote risk byRD(Bρ̂) ≤ 2RD(Gρ̂) [Langford and ShaweTaylor, 2002]. In some situations, this factor of two may be reached, i.e., RD(Bρ̂) ' 2RD(Gρ̂). In other situations, we may have RD(Gρ̂) = 0 even if RD(Gρ̂) = 12− (see Germain et al. [2015] for an extensive study). Indeed, these bounds obtained via the Gibbs risk are exposed to be loose and/or unrepresentative of the majority vote generalization error.6\n6It is noteworthy that the best PAC-Bayesian empirical bound values are so far obtained by considering a majority vote of linear classifiers, where prior and posterior are Gaussian [Langford and Shawe-Taylor, 2002, Ambroladze et al., 2006, Germain et al., 2009], similarly to the Bayesian linear regression analyzed in Section 6.\nIn the current work, we study regression losses instead of classification ones. That is, the provided results express upper bounds on Ef∼ρ̂ L `D(f) for any (bounded, sub-Gaussian, or sub-Gamma) losses. Of course, one may want to bound the regression loss of the averaged regressor Fρ̂(x) = Ef∼ρ̂ f(x). In this case, if the loss function ` is convex (as the squared loss), Jensen’s inequality gives L `D(Fρ̂) ≤ Ef∼ρ̂ L `D(f) . Note that a strict inequality replaces the factor two mentioned above for the classification case, due to the non-convex indicator function of Equation (18).\nNow that we state generalization bounds for real-valued loss functions, we can continue our study linking PAC-Bayesian results to Bayesian inference. In next section, we focus on model selection."
    }, {
      "heading" : "5 Analysis of Model Selection",
      "text" : "Let consider L distinct models {Mi}Li=1, each one defined by a set of parameters Θi. The PACBayesian theorems naturally suggest selecting the model that is best adapted for the given task by evaluating the bound for each model {Mi}Li=1 and selecting the one with the lowest bound [McAllester, 2003, Ambroladze et al., 2006, Zhang, 2006]. This is closely linked with the Bayesian model selection procedure, as we showed in Section 3 that minimizing the PAC-Bayes bound amounts to maximizing the marginal likelihood. Indeed, given a collection of L optimal Gibbs posteriors—one for each mode—given by Equation (8),\np(θ|X,Y,Mi) ≡ ρ̂∗i (θ) = 1ZX,Y,iπi(θ) e n L̂ `nllX,Y (θ), for θ ∈ Θi , (19)\nthe Bayesian Occam’s razor criteria [Jeffreys and Berger, 1992, MacKay, 1992] chooses the one with the higher model evidence\np(Y |X,Mi) ≡ ZX,Y,i = ∫\nΘi\nπi(θ) e −nL̂ `X,Y (θ) dθ . (20)\nCorollary 6 below formally links the PAC-Bayesian and the Bayesian model selection. To obtain this result, we simply use the bound of Corollary 5 L times, together with `nll and Equation (10). From the union bound (a.k.a. Bonferroni inequality), it is mandatory to compute each bound with a confidence parameter of δ/L, to ensure that the final conclusion is valid with probability at least 1−δ. Corollary 6. Given a data distribution D, a family of model parameters {Θi}Li=1 and associated priors {πi}Li=1—where πi is defined over Θi— , a δ ∈ (0, 1], if the loss is sub-Gamma with parameters s2 and c < 1, then, with probability at least 1− δ over (X,Y ) ∼ Dn,\n∀i ∈ {1, . . . , L} : E θ∼ρ̂∗i L`nllD (θ) ≤ 1 2(1−c) s 2 − 1n ln\n( ZX,Y,i δ L ) .\nwhere ρ̂∗i is the Gibbs optimal posterior (Eq. 19) and ZX,Y,i is the marginal likelihood (Eq. 20).\nHence, under the uniform prior over the L models, choosing the one with the best model evidence is equivalent to choosing the one with the lowest PAC-Bayesian bound.\nHierarchical Bayes. To perform proper inference of hyperparameters, we have to rely on the Hierarchical Bayes approach. This is done by considering an hyperprior p(η) over the set of hyperparameters H. Then, the prior p(θ|η) can be conditioned on a choice of hyperparameter η. The Bayesian rule of Equation (5) becomes p(θ, η|X,Y ) = p(η) p(θ|η) p(Y |X,θ)p(Y |X) .\nUnder the negative log-likelihood loss function, we can rewrite the results of Corollary 2 as a generalization bounds on Eη∼ρ̂0 Eθ∼ρ̂∗η L `nll D (θ), where ρ̂0(η) ∝ π0(η)ZX,Y,η is the hyperposterior on H and π0 the hyperprior. Indeed, Equation (11) becomes\nE θ∼ρ̂∗ L`nllD (θ) = E η∼ρ̂∗0 E θ∼ρ̂∗η L`nllD (θ) ≤ 1 2(1−c) s 2 − 1n ln\n( E\nη∼π0 ZX,Y,η δ\n) . (21)\nTo relate to the bound obtained in Corollary 6, we consider the case of a discrete hyperparameter set, H = {ηi}Li=1, with a uniform prior. Then, Equation (21) becomes\nE θ∼ρ̂∗ L`nllD (θ) = E η∼ρ̂∗0 E θ∼ρ̂∗η L`nllD (θ) ≤ 1 2(1−c) s 2 − 1n ln\n(∑L i=1 ZX,Y,ηi δ L ) .\nThis bound is now function of ∑L i=1 ZX,Y,ηi instead maxi ZX,Y,ηi as in Corollary 6. This yields a tighter bound, corroborating the Bayesian wisdom that model averaging performs best.\nWhen selecting a single hyperparameter η∗, the hierarchical representation is equivalent to choosing a deterministic hyperposterior, satisfying ρ̂0(η∗) = 1 and 0 for every other values. We then have\nKL(ρ̂||π) = KL(ρ̂0||π0) + E η∼ρ̂0 KL(ρ̂η||πη) = ln(L) + KL(ρ̂η∗ ||πη∗) .\nWith the optimal posterior for the selected η∗, we have\nn E θ∼ρ̂ L̂ `nllX,Y (θ) + KL(ρ̂||π) = n E θ∼ρ̂∗η L̂ `nllX,Y (θ) + KL(ρ̂ ∗ η∗ ||πη∗) + ln(L) = − ln(ZX,Y,η∗) + ln(L) = − ln ( ZX,Y,η∗\nL\n) .\nInserting this result into Equation (17), we fall back on the bound obtained in Corollary 6. Hence, by comparing the values of the bounds one can get an estimate on the consequence of performing model selection instead of model averaging."
    }, {
      "heading" : "6 Linear Regression",
      "text" : "In this section, we perform Bayesian linear regression using the parameterization of Bishop [2006]. The output space is Y := R and, for an arbitrary input spaceX , we use a mapping functionφ :X→Rd.\nThe model. Given (x, y) ∈ X × Y and model parameters θ := 〈w, σ〉 ∈ Rd × R+, we consider the likelihood p(y|x, 〈w, σ〉) = N (y|w ·φ(x), σ2). Thus, the negative log-likelihood loss is\n`nll(〈w, σ 〉, x, y) = − ln p(y|x, 〈w, σ 〉) = 12 ln(2πσ 2) + 12σ2 (y −w ·φ(x)) 2 (22) For a fixed σ, minimizing Equation (22) is equivalent to minimizing the square-loss function `sqr(w, x, y) = (y − w · φ(x))2. We also consider an isotropic Gaussian prior of mean 0 and variance σ2π: p(w|σπ) = N (w|0, σ2π). For the sake of simplicity, we consider a fixed noise parameter σ2 and a fixed prior variance σ2π. The Gibbs optimal posterior (see Equation 8) is then given by\nρ̂∗(w) ≡ p(w|σ, σπ) = p(w|σ,σπ) p(Y |X,w,σ,σπ)p(Y |X,σ,σπ) = N (w | ŵ, A −1) , (23)\nwhere A := 1σ2 Φ T Φ + 1σ2π I ; ŵ := 1σ2A −1ΦTy ; Φ is a n×d matrix such that the ith line is φ(xi) ; y := [y1, . . . yn] is the labels-vector ; and the negative log marginal likelihood is\n− ln ( ZD(σ, σπ) ) = 12σ2 ‖y −Φŵ‖ 2 + n2 ln(2πσ 2) + 12σ2π ‖ŵ‖2 + 12 log |A|+ d lnσπ\n= n L̂ `nllX,Y (ŵ) + 1 2σ2 tr(Φ TΦA−1)︸ ︷︷ ︸\nnEw∼ρ̂∗ L̂ `nll X,Y (w)\n+ 12σ2π tr(A−1)− d2 + 1 2σ2π ‖ŵ‖2 + 12 log |A|+ d lnσπ︸ ︷︷ ︸\nKL ( N (ŵ,A−1) ‖N (0,σ2πI) ) . Last equality comes from 12σ2 tr(Φ TΦA−1)+ 1σ2π tr(A−1)= tr( 12σ2 Φ TΦA−1+ 1σ2π A−1)= tr(A−1A) =d (see Appendix A.4 for complete calculations). This exhibits how the Bayesian regression optimization problem can be express by the minimization of a PAC-Bayesian bound, expressed by a trade-off between Ew∼ρ̂∗ L̂ `nllX,Y (w) and KL ( N (ŵ, A−1) ‖N (0, σ2π I) ) .\nModel selection experiment. To produce Figures 1a and 1b, we reimplemented the toy experiment of Bishop [2006, Subsection 3.5.1]. That is, we generated a learning sample of 15 data points according to y = sin(x) + , where x is uniformly sampled in the interval [0, 2π] and ∼ N (0, 14 ) is a Gaussian noise. We then learn seven different polynomial models with the regression given by Equation (23). More precisely, for a polynomial model of degree d, we map input x ∈ R to a vector φ(x) = [1, x1, x2, . . . , xd] ∈ Rd+1, and we fix parameters σ2π = 10.005 and σ\n2 = 12 . Figure 1a illustrates the seven learned models. Figure 1b shows the marginal likelihood computed for each polynomial model, and is designed to reproduce Bishop [2006, Figure 3.14], where it is explained that the marginal likelihood correctly indicates that the polynomial model of degree d = 3 is “the simplest model which gives a good explanation for the observed data”. We show that this claim is well quantified by the trade-off intrinsic to our PAC-Bayesian approach: the complexity KL term keeps increasing with the parameter d ∈ {1, 2, . . . , 7}, while the empirical risk drastically decreases from d = 2 to d = 3, and only slightly afterward. Moreover, we show the generalization risk (computed on a test sample of size 1000) tends to increase with complex models (for d ≥ 4).\nEmpirical comparison of bound values. Figure 1c compares the values of the PAC-Bayesian bounds presented in this paper on a synthetic dataset where the inputs points are randomly generated to a Gaussian x ∼ N (0, I) in R20, and the outputs are given by y = w∗ · x + , with ‖w∗‖=1 and ∼ N (0, 13 ). We perform Bayesian linear regression in the input space, i.e., φ(x)=x, fixing σ2π= 1 100 and σ\n2=2. That is, we compute the optimal posterior of Equation (23) for training samples of sizes from 10 to 106. For each learned model, we compute empirical negative loss likelihood of Equation (22), and the three PAC-Bayes bounds, with confidence parameter of δ= 120 . We estimate the bounds parameters a, b, s, c from observed samples.\nFor small and medium sized training samples (n . 104), the bound of Corollary 5, that we have developed for (unbounded) sub-Gamma losses, gives as far the tighter guarantees than the two other results for [a, b]-bounded losses. However, our new bound always maintains a gap of s2/2(1−c) between its value and the expected loss. The result of Corollary 2 [adapted from Catoni, 2007] from bounded losses suffer from a similar gap, while having higher values that our sub-Gaussian result. Finally, the result of Theorem 3 [Alquier et al., 2015], combined with λ = 1/ √ n (Eq 15), converges to the expected loss, but it provides good guarantees only for large training sample (n & 105). Note that the latter bound is not directly minimized by our “optimal posterior”, as opposed to the one with λ = 1/n (Eq 14), for which we observe values in [19.2, 19.8] (not displayed on Figure 1c)."
    }, {
      "heading" : "7 Conclusion",
      "text" : "The first contribution of this paper is to bridge the concepts underlying the Bayesian and the PACBayesian approaches. This was done by showing that, under proper parametrization, the minimization of the PAC-Bayesian bound minimizes the marginal likelihood. This study, that relies on the realvalued negative log-likelihood loss function, motivates the second contribution of this paper, which is to prove PAC-Bayesian generalization bounds for regression with unbounded sub-Gamma loss functions, that provides generalization guarantees for the squared loss in regression tasks.\nIn this work, we studied model selection techniques. On a broader perspective, we would like to suggest both Bayesian and PAC-Bayesian frameworks may have more to learn from each other than what has been done lately. As future work, we plan to study other Bayesian techniques through the light of PAC-Bayesian tools, such as variational Bayes and empirical Bayes methods."
    }, {
      "heading" : "A Supplementary material",
      "text" : "A.1 Proof of Theorem 3 Proof. From Donsker-Varadhan’s change of measure, with φ(f) := λ ( L `D(f)− L̂ `X,Y (f) ) , we have ∀ ρ̂ on F :\nλ (\nE f∼ρ̂ L `D(f)− E f∼ρ̂ L̂ `X,Y (f)\n) = E f∼ρ̂ λ ( L `D(f)− L̂ `X,Y (f) ) ≤ KL(ρ̂‖π) + ln ( E f∼π eλ ( L `D(f)−L̂ ` X,Y (f) )) .\nNow, we apply apply Markov’s inequality on the random variable Ef∼π e λ ( L `D(f)−L̂ ` X,Y (f) ) :\nPr X,Y∼Dn\n( ζπ(X,Y ) ≤ 1\nδ E X′,Y ′∼Dn ζπ(X\n′, Y ′) ) ≥ 1− δ .\nThis implies that with probability at least 1−δ over the choice of X,Y ∼ Dn, we have ∀ ρ̂ on F :\nE f∼ρ̂ L `D(f) ≤ E f∼ρ̂ L̂ `X,Y (f) +\n1\nλ KL(ρ̂‖π) + ln EX′,Y ′∼Dn Ψ`,π,D(λ, n) δ  .\nA.2 Proof of Equation (14) and (15)\nProof. For i = 1 . . . n, we denote `i a realization of the random variable `(f, x, y) − L̂ `X′,Y ′(f). Each `i is i.i.d., zero-mean, and bounded by a− b and b− a, as `(f, x, y) ∈ [a, b]. Thus,\nE f∼π E X′,Y ′∼Dn\nexp [ λ ( L `D(f)− L̂ `X′,Y ′(f) )] = E exp\n[ λ\nn n∑ i=1 `i\n]\n= n∏ i=1 E exp [ λ n `i ]\n≤ n∏ i=1 exp [ λ2(a− b− (b− a))2 8n2 ]\n= n∏ i=1 exp [ λ2(b− a)2 2n2 ] = exp [ λ2(b− a)2\n2n\n] ,\nwhere the Inequality comes from Hoeffding’s lemma.\nWith λ := n, Equation (12) becomes Equation (14) :\nE f∼ρ̂ L `D(f) ≤ E f∼ρ̂ L̂ `X,Y (f) +\n1\nn\n[ KL(ρ̂‖π) + ln 1\nδ + n2(b− a)2 2n ] = E\nf∼ρ̂ L̂ `X,Y (f) +\n1\nn\n[ KL(ρ̂‖π) + ln 1\nδ +\n] + 1\n2 (b− a)2 .\nSimilarly, with λ := √ n, Equation (12) becomes Equation (15) .\nA.3 Study of the Squared Loss\nAssume that:\n• w ∼ N (0, σ2π I) , (w ∈ Rd)\n• ∼ N (0, σ2 ) • y = w∗ · x + • ∀x ∈ X , γ ≥ sup ‖x‖\nWe have y|x ∼ N (x ·w∗, σ2 ). Thus, z|x = (y −w · x)|x ∼ N (x ·w∗, σ2 + σ2π‖x‖2)\neψ = E x E y|x E w exp(λ[(y −w · x)2 −E x E y|x E w (y −w · x)2])\n= E x E z|x exp(λ[z2 −E x E z|x z2])\n≤ E x E z|x exp(λ[z2])\n( ) = E x 1√ 1− 2λ(σ2 + σ2π‖x‖2)2 exp\n( λ(w∗ · x)2\n1− 2λ(σ2 + σ2π‖x‖2)2 ) ≤ E\nx 1√ 1− 2λ(σ2 + σ2πγ2)2 exp\n( λ(w∗ · x)2\n1− 2λ(σ2 + σ2πγ2)2 ) ≤ 1√\n1− 2λ(σ2 + σ2πγ2)2 exp\n( λ‖w∗‖2γ2\n1− 2λ(σ2 + σ2πγ2)2\n) .\nψ ≤ λ‖w ∗‖2γ2\n1− 2λ(σ2 + σ2πγ2)2 − 1 2 ln ( 1− 2λ(σ2 + σ2πγ2)2 ) ≤ λ‖w ∗‖2γ2\n1− 2λ(σ2 + σ2πγ2)2\n= 2λ‖w∗‖2γ2\n2(1− 2λ(σ2 + σ2πγ2)2)\n≤ 2λ 2s2\n2(1− cλ) ,\nwith s2 ≥ 2‖w ∗‖2γ2 λ and c ≤ 2(σ 2 + σ 2 πγ 2)2. Note that the Equality ( ) is only valid for λ < 1c .\nA.4 Linear Regression\nWe defined A := 1σ2 Φ T Φ + 1σ2π I ; ŵ := 1σ2A −1ΦTy ; Φ as a n×d matrix such that the ith line is φ(xi) ; y := [y1, . . . yn] as the labels-vector ; and we decompose of the marginal likelihood into the PAC-Bayesian trade-off:\n− ln ( ZD(σ, σπ) ) = 12σ2 ‖y −Φŵ‖ 2 + n2 ln(2πσ 2) + 12σ2π ‖ŵ‖2 + 12 log |A|+ d lnσπ\n= n L̂ `nllX,Y (ŵ) + 1 2σ2 tr(Φ TΦA−1)︸ ︷︷ ︸\nnEw∼ρ̂∗ L̂ `nll X,Y (w)\n+ 12σ2π tr(A−1)− d2 + 1 2σ2π ‖ŵ‖2 + 12 log |A|+ d lnσπ︸ ︷︷ ︸\nKL ( N (ŵ,A−1) ‖N (0,σ2πI) ) , which is based on the following three equalities:\nn E w∼ρ̂ L̂ `nllX,Y (w) = E w∼ρ̂ ∑ i − ln p(yi|xi,w)\n= E w∼ρ̂\n( n\n2 ln(2πσ2) +\n1\n2σ2 ∑ i\n(yi −w ·φ(xi))2 )\n= n\n2 ln(2πσ2) +\n1\n2σ2 E w∼ρ̂ ‖y −Φw‖2\n= n\n2 ln(2πσ2) +\n1\n2σ2 E w∼ρ̂\n( ‖y‖2 − 2yΦw + wTΦTΦw ) = n\n2 ln(2πσ2) +\n1\n2σ2\n( ‖y‖2 − 2yΦŵ + E\nw∼ρ̂ wTΦTΦw ) = n\n2 ln(2πσ2) +\n1 2σ2 ( ‖y‖2 − 2yΦŵ + tr ( ΦTΦA−1 ) + ŵTΦTΦŵ ) = n\n2 ln(2πσ2) +\n1 2σ2 ‖y −Φŵ‖2 + 1 2σ2 tr ( ΦTΦA−1 ) = n L̂ `nllX,Y (ŵ) + 1 2σ2 tr ( ΦTΦA−1 )\nKL ( N (ŵ, A−1) ‖N (0, σ2πI) ) = 1\n2\n( tr ( (σ2πI) −1A−1 ) + 1\nσ2π ‖ŵ‖2 − d+ log |σ 2 πI| |A| ) = 1\n2\n( 1\nσ2π tr ( A−1 ) + 1 σ2π ‖ŵ‖2 − d+ log |A−1|+ d lnσ2π\n)\n1 2σ2 tr\n( ΦTΦA−1 ) + 1σ2π tr ( A−1 ) = tr ( 1 2σ2 Φ TΦA−1 + 1σ2π A−1 )\n= tr ( A−1( 12σ2 Φ TΦ + 1σ2π I) )\n= tr ( A−1A ) = d ."
    } ],
    "references" : [ {
      "title" : "On the properties of variational approximations of Gibbs posteriors",
      "author" : [ "Pierre Alquier", "James Ridgway", "Nicolas Chopin" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Alquier et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Alquier et al\\.",
      "year" : 2015
    }, {
      "title" : "Tighter PAC-Bayes bounds",
      "author" : [ "A. Ambroladze", "E. Parrado-Hernández", "J. Shawe-Taylor" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ambroladze et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Ambroladze et al\\.",
      "year" : 2006
    }, {
      "title" : "On Bayesian bounds",
      "author" : [ "Arindam Banerjee" ],
      "venue" : "In ICML, pages",
      "citeRegEx" : "Banerjee.,? \\Q2006\\E",
      "shortCiteRegEx" : "Banerjee.",
      "year" : 2006
    }, {
      "title" : "PAC-Bayesian theory for transductive learning",
      "author" : [ "Luc Bégin", "Pascal Germain", "François Laviolette", "Jean-Francis Roy" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Bégin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bégin et al\\.",
      "year" : 2014
    }, {
      "title" : "Pattern Recognition and Machine Learning (Information Science and Statistics)",
      "author" : [ "Christopher M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "Bishop.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 2006
    }, {
      "title" : "A general framework for updating belief distributions",
      "author" : [ "P.G. Bissiri", "C.C. Holmes", "S.G. Walker" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Bissiri et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bissiri et al\\.",
      "year" : 2016
    }, {
      "title" : "Concentration inequalities : a nonasymptotic theory of independence",
      "author" : [ "Stéphane Boucheron", "Gábor Lugosi", "Pascal Massart" ],
      "venue" : "Oxford university press,",
      "citeRegEx" : "Boucheron et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Boucheron et al\\.",
      "year" : 2013
    }, {
      "title" : "PAC-Bayesian supervised classification: the thermodynamics of statistical learning, volume 56",
      "author" : [ "Olivier Catoni" ],
      "venue" : "Inst. of Mathematical Statistic,",
      "citeRegEx" : "Catoni.,? \\Q2007\\E",
      "shortCiteRegEx" : "Catoni.",
      "year" : 2007
    }, {
      "title" : "Aggregation by exponential weighting, sharp PAC-Bayesian bounds and sparsity",
      "author" : [ "Arnak S. Dalalyan", "Alexandre B. Tsybakov" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Dalalyan and Tsybakov.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dalalyan and Tsybakov.",
      "year" : 2008
    }, {
      "title" : "PAC-Bayesian learning of linear classifiers",
      "author" : [ "Pascal Germain", "Alexandre Lacasse", "François Laviolette", "Mario Marchand" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Germain et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Germain et al\\.",
      "year" : 2009
    }, {
      "title" : "Risk bounds for the majority vote: From a PAC-Bayesian analysis to a learning",
      "author" : [ "Pascal Germain", "Alexandre Lacasse", "Francois Laviolette", "Mario Marchand", "Jean-Francis Roy" ],
      "venue" : null,
      "citeRegEx" : "Germain et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Germain et al\\.",
      "year" : 2015
    }, {
      "title" : "Probabilistic machine learning and artificial intelligence",
      "author" : [ "Zoubin Ghahramani" ],
      "venue" : "Nature, 521:452–459,",
      "citeRegEx" : "Ghahramani.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ghahramani.",
      "year" : 2015
    }, {
      "title" : "Suboptimal behavior of Bayes and MDL in classification under misspecification",
      "author" : [ "Peter Grünwald", "John Langford" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Grünwald and Langford.,? \\Q2007\\E",
      "shortCiteRegEx" : "Grünwald and Langford.",
      "year" : 2007
    }, {
      "title" : "Learning efficient random maximum a-posteriori predictors with non-decomposable loss functions",
      "author" : [ "Tamir Hazan", "Subhransu Maji", "Joseph Keshet", "Tommi S. Jaakkola" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2013
    }, {
      "title" : "Ockham’s razor and Bayesian analysis",
      "author" : [ "William H. Jeffreys", "James O. Berger" ],
      "venue" : "American Scientist,",
      "citeRegEx" : "Jeffreys and Berger.,? \\Q1992\\E",
      "shortCiteRegEx" : "Jeffreys and Berger.",
      "year" : 1992
    }, {
      "title" : "Agnostic Bayes",
      "author" : [ "Alexandre Lacoste" ],
      "venue" : "PhD thesis, Université Laval,",
      "citeRegEx" : "Lacoste.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lacoste.",
      "year" : 2015
    }, {
      "title" : "Approximate inference for the loss-calibrated Bayesian",
      "author" : [ "Simon Lacoste-Julien", "Ferenc Huszar", "Zoubin Ghahramani" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Lacoste.Julien et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lacoste.Julien et al\\.",
      "year" : 2011
    }, {
      "title" : "PAC-Bayes & margins",
      "author" : [ "John Langford", "John Shawe-Taylor" ],
      "venue" : "In NIPS, pages 423–430,",
      "citeRegEx" : "Langford and Shawe.Taylor.,? \\Q2002\\E",
      "shortCiteRegEx" : "Langford and Shawe.Taylor.",
      "year" : 2002
    }, {
      "title" : "Tighter PAC-Bayes bounds through distributiondependent priors",
      "author" : [ "Guy Lever", "François Laviolette", "John Shawe-Taylor" ],
      "venue" : "Theor. Comput. Sci.,",
      "citeRegEx" : "Lever et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lever et al\\.",
      "year" : 2013
    }, {
      "title" : "A note on the PAC-Bayesian theorem",
      "author" : [ "Andreas Maurer" ],
      "venue" : "CoRR, cs.LG/0411099,",
      "citeRegEx" : "Maurer.,? \\Q2004\\E",
      "shortCiteRegEx" : "Maurer.",
      "year" : 2004
    }, {
      "title" : "Some PAC-Bayesian theorems",
      "author" : [ "David McAllester" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "McAllester.,? \\Q1999\\E",
      "shortCiteRegEx" : "McAllester.",
      "year" : 1999
    }, {
      "title" : "PAC-Bayesian stochastic model selection",
      "author" : [ "David McAllester" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "McAllester.,? \\Q2003\\E",
      "shortCiteRegEx" : "McAllester.",
      "year" : 2003
    }, {
      "title" : "Generalization bounds and consistency for latent structural probit and ramp loss",
      "author" : [ "David A. McAllester", "Joseph Keshet" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "McAllester and Keshet.,? \\Q2011\\E",
      "shortCiteRegEx" : "McAllester and Keshet.",
      "year" : 2011
    }, {
      "title" : "Generalization error bounds for Bayesian mixture algorithms",
      "author" : [ "Ron Meir", "Tong Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Meir and Zhang.,? \\Q2003\\E",
      "shortCiteRegEx" : "Meir and Zhang.",
      "year" : 2003
    }, {
      "title" : "On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes",
      "author" : [ "Andrew Y. Ng", "Michael I. Jordan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ng and Jordan.,? \\Q2001\\E",
      "shortCiteRegEx" : "Ng and Jordan.",
      "year" : 2001
    }, {
      "title" : "Robust forward algorithms via PAC-Bayes and Laplace distributions",
      "author" : [ "Asf Noy", "Koby Crammer" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Noy and Crammer.,? \\Q2014\\E",
      "shortCiteRegEx" : "Noy and Crammer.",
      "year" : 2014
    }, {
      "title" : "A PAC-Bayesian bound for lifelong learning",
      "author" : [ "Anastasia Pentina", "Christoph H. Lampert" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Pentina and Lampert.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pentina and Lampert.",
      "year" : 2014
    }, {
      "title" : "PAC-Bayesian generalization bounds for gaussian processes",
      "author" : [ "Matthias Seeger" ],
      "venue" : "JMLR, 3:233–269,",
      "citeRegEx" : "Seeger.,? \\Q2002\\E",
      "shortCiteRegEx" : "Seeger.",
      "year" : 2002
    }, {
      "title" : "Bayesian Gaussian Process Models: PAC-Bayesian Generalisation Error Bounds and Sparse Approximations",
      "author" : [ "Matthias Seeger" ],
      "venue" : "PhD thesis, University of Edinburgh,",
      "citeRegEx" : "Seeger.,? \\Q2003\\E",
      "shortCiteRegEx" : "Seeger.",
      "year" : 2003
    }, {
      "title" : "PAC-Bayesian analysis of co-clustering and beyond",
      "author" : [ "Yevgeny Seldin", "Naftali Tishby" ],
      "venue" : null,
      "citeRegEx" : "Seldin and Tishby.,? \\Q2010\\E",
      "shortCiteRegEx" : "Seldin and Tishby.",
      "year" : 2010
    }, {
      "title" : "PAC-Bayesian analysis of contextual bandits",
      "author" : [ "Yevgeny Seldin", "Peter Auer", "François Laviolette", "John Shawe-Taylor", "Ronald Ortner" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Seldin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Seldin et al\\.",
      "year" : 2011
    }, {
      "title" : "PAC-Bayesian inequalities for martingales",
      "author" : [ "Yevgeny Seldin", "François Laviolette", "Nicolò Cesa-Bianchi", "John Shawe-Taylor", "Peter Auer" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Seldin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Seldin et al\\.",
      "year" : 2012
    }, {
      "title" : "A PAC analysis of a Bayesian estimator",
      "author" : [ "John Shawe-Taylor", "Robert C. Williamson" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Shawe.Taylor and Williamson.,? \\Q1997\\E",
      "shortCiteRegEx" : "Shawe.Taylor and Williamson.",
      "year" : 1997
    }, {
      "title" : "Information-theoretic upper and lower bounds for statistical estimation",
      "author" : [ "Tong Zhang" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "Zhang.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 32,
      "context" : "Since its early beginning [Shawe-Taylor and Williamson, 1997], the PAC-Bayesian theory claims to provide “PAC guarantees to Bayesian algorithms” [McAllester, 1999].",
      "startOffset" : 26,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "Since its early beginning [Shawe-Taylor and Williamson, 1997], the PAC-Bayesian theory claims to provide “PAC guarantees to Bayesian algorithms” [McAllester, 1999].",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 11,
      "context" : "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al.",
      "startOffset" : 34,
      "endOffset" : 48
    }, {
      "referenceID" : 4,
      "context" : "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc.",
      "startOffset" : 34,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc.",
      "startOffset" : 34,
      "endOffset" : 99
    }, {
      "referenceID" : 4,
      "context" : "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc. Langford and Shawe-Taylor [2002], Seldin and Tishby [2010], Seldin et al.",
      "startOffset" : 34,
      "endOffset" : 138
    }, {
      "referenceID" : 4,
      "context" : "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc. Langford and Shawe-Taylor [2002], Seldin and Tishby [2010], Seldin et al.",
      "startOffset" : 34,
      "endOffset" : 164
    }, {
      "referenceID" : 2,
      "context" : "[2011, 2012], Bégin et al. [2014], Pentina and Lampert [2014], etc.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "[2011, 2012], Bégin et al. [2014], Pentina and Lampert [2014], etc.",
      "startOffset" : 14,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "[2011, 2012], Bégin et al. [2014], Pentina and Lampert [2014], etc. Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al.",
      "startOffset" : 14,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al.",
      "startOffset" : 57,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al.",
      "startOffset" : 57,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al.",
      "startOffset" : 57,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Grünwald and Langford [2007], Lacoste-Julien et al.",
      "startOffset" : 57,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Grünwald and Langford [2007], Lacoste-Julien et al.",
      "startOffset" : 57,
      "endOffset" : 157
    }, {
      "referenceID" : 2,
      "context" : "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Grünwald and Langford [2007], Lacoste-Julien et al.",
      "startOffset" : 57,
      "endOffset" : 180
    }, {
      "referenceID" : 2,
      "context" : "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Grünwald and Langford [2007], Lacoste-Julien et al.",
      "startOffset" : 57,
      "endOffset" : 210
    }, {
      "referenceID" : 2,
      "context" : "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Grünwald and Langford [2007], Lacoste-Julien et al. [2011] for other studies drawing links between frequentist statistics and Bayesian inference.",
      "startOffset" : 57,
      "endOffset" : 240
    }, {
      "referenceID" : 7,
      "context" : "Theorem 1 (Catoni, 2007).",
      "startOffset" : 10,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al.",
      "startOffset" : 18,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al.",
      "startOffset" : 18,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al.",
      "startOffset" : 18,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al. [2013], Noy and Crammer [2014].",
      "startOffset" : 18,
      "endOffset" : 162
    }, {
      "referenceID" : 7,
      "context" : "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al. [2013], Noy and Crammer [2014]. Theorem 1 (Catoni, 2007).",
      "startOffset" : 18,
      "endOffset" : 186
    }, {
      "referenceID" : 28,
      "context" : "As mentioned by Zhang [2006], Catoni [2007], Germain et al.",
      "startOffset" : 16,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "As mentioned by Zhang [2006], Catoni [2007], Germain et al.",
      "startOffset" : 30,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "As mentioned by Zhang [2006], Catoni [2007], Germain et al. [2009], Lever et al.",
      "startOffset" : 30,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "As mentioned by Zhang [2006], Catoni [2007], Germain et al. [2009], Lever et al. [2013], Alquier et al.",
      "startOffset" : 30,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "[2013], Alquier et al. [2015], the optimal Gibbs posterior ρ̂∗ is given by ρ̂∗(f) = 1 ZX,Y π(f) e −n L̂ ` X,Y (f) , (4)",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "The new bounds are obtained through a recent theorem of Alquier et al. [2015], stated below (we provide a proof in Appendix A.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "The new bounds are obtained through a recent theorem of Alquier et al. [2015], stated below (we provide a proof in Appendix A.1 for completeness). Theorem 3 (Alquier et al. [2015]).",
      "startOffset" : 56,
      "endOffset" : 180
    }, {
      "referenceID" : 26,
      "context" : "A similar result to Equation (15) leads to long-life learning algorithms in Pentina and Lampert [2014].",
      "startOffset" : 76,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "The above sub-Gaussian assumption corresponds to the Hoeffding assumption of Alquier et al. [2015], and allows to obtain the following result.",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "Maurer [2004] shows that we can generalize PAC-Bayesian bounds on the generalization risk of the Gibbs classifier to any loss function with output between zero and one.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 8,
      "context" : "In other situations, we may have RD(Gρ̂) = 0 even if RD(Gρ̂) = 12− (see Germain et al. [2015] for an extensive study).",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "In this section, we perform Bayesian linear regression using the parameterization of Bishop [2006]. The output space is Y := R and, for an arbitrary input spaceX , we use a mapping functionφ :X→R.",
      "startOffset" : 85,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "Finally, the result of Theorem 3 [Alquier et al., 2015], combined with λ = 1/ √ n (Eq 15), converges to the expected loss, but it provides good guarantees only for large training sample (n & 10).",
      "startOffset" : 33,
      "endOffset" : 55
    } ],
    "year" : 2017,
    "abstractText" : "We exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam’s razor criteria, under the assumption that the data is generated by a i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-Gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}