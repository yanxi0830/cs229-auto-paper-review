{
  "name" : "1703.09260.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Goal-Driven Dynamics Learning via Bayesian Optimization",
    "authors" : [ "Somil Bansal", "Roberto Calandra", "Ted Xiao", "Sergey Levine", "Claire J. Tomlin" ],
    "emails" : [ "tomlin}@eecs.berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nGiven the system dynamics, optimal control schemes such as LQR, MPC, and feedback linearization can efficiently design a controller that maximizes a performance criterion. However, depending on the system complexity, it can be quite challenging to model its true dynamics. However, for a given task, a globally accurate dynamics model is not always necessary to design a controller. Often, partial knowledge of the dynamics is sufficient, e.g., for trajectory tracking purposes a local linearization of a non-linear system is often sufficient. In this paper we argue that, for complex systems, it might be preferable to adapt the controller design process for the specific task, using a learned system dynamics model sufficient to achieve the desired performance.\nWe propose Dynamics Optimization via Bayesian Optimization (aDOBO), a Bayesian Optimization (BO) based active learning framework to learn the dynamics model that achieves the best performance for a given task based on the performance observed in experiments on the physical system. This active learning framework takes into account all past experiments and suggests the next experiment in order to learn most about the relationship between the performance criterion and the model parameters. Particularly important for robotic systems is the use of data-efficient approaches, where only few experiments are needed to obtain improved performance. Hence, we employ BO, an optimization method often used to optimize a performance criterion while keeping the number of evaluations of the physical system small [1]. Specifically, we use BO to optimize the dynamics model\nAll authors are with the Department of Electrical Engineering and Computer Sciences, University of California, Berkeley. {somil, roberto.calandra, t.xiao, svlevine, tomlin}@eecs.berkeley.edu\n∗This research is supported by ONR under the Embedded Humans MURI (N00014-16-1-2206).\nwith respect to the desired task, where the dynamics model is updated after every experiment so as to maximize the performance on the physical system. A flow diagram of our framework is shown in Figure 1. The current locally linear dynamics model, together with the cost function (also referred to as performance criterion), are used to design a controller with an appropriate optimal control scheme. The cost (or performance) of the controller is evaluated in closed-loop operation with the actual (unknown) physical plant. BO uses this performance information to iteratively update the dynamics model to improve the performance. This procedure corresponds to optimizing the (locally linear) system dynamics with the purpose of maximizing the performance of the final controller. Hence, unlike traditional system identification approaches, it does not necessarily correspond to finding the most accurate dynamics model, but rather the model yielding the best controller performance when provided to the optimal control method used.\nTraditional system identification approaches are divided into two stages: 1) creating a dynamics model by minimizing some prediction error (e.g., using least squares) 2) using this dynamics model to generate an appropriate controller. In this approach, modeling the dynamics can be considered an offline process as there is no information flow between the two design stages. In online methods, the dynamics model is instead iteratively updated using new data collected by evaluating the controller [2]. Our approach is an online method. Both for the online and the offline cases, creating a dynamics model based only on minimizing the prediction error can introduce sufficient inaccuracies to lead to suboptimal control performance [3]. Using machine learning techniques, such as Gaussian processes, does not alleviate this issue [4]. Instead, authors in [3] proposed to optimize the dynamics model directly with respect to the controller performance, but since the dynamics model is optimized offline, the resultant model is not necessarily optimal for the actual system. We instead explicitly find the dynamics model that produces the best ar X\niv :1\n70 3.\n09 26\n0v 1\n[ cs\n.S Y\n] 2\n7 M\nar 2\n01 7\ncontrol performance for the actual system. Previous studies addressed the problem of optimizing a controller using BO. In [5]–[7] authors tuned the penalty matrices in an LQR problem for performance optimization. Parameters of a linear feedback controller are learned in [8] using BO. Although interesting results emerge from these studies, it is not clear how these methods perform for non-quadratic cost functions. Moreover, when an accurate system model is not available, tuning penalty matrices may not achieve the desired performance. Our approach overcomes these challenges as it does not rely on an accurate system dynamics model or impose any linear structure on the controller. In fact, aDOBO can easily design non-linear controllers as well (see Sec. IV). The problem of updating a system model to improve control performance is also related to adaptive control, where the model parameters are identified from sensor data, and subsequently the updated model is used to design a controller (see [9]–[13]). However, in adaptive control, the model parameters are generally updated to get a good prediction model and not necessarily to maximize the controller performance. In contrast, we explicitly take into account the observed performance and search for the model that achieves the highest performance.\nTo the best of our knowledge, this is the first method that optimize a dynamics model to maximize the control performance on the actual system. Our approach does not require the prior knowledge of an accurate dynamics model, nor of its parameterized form. Instead, the dynamics model is optimized, in an active learning setting, directly with respect to the desired cost function using data-efficient BO."
    }, {
      "heading" : "II. PROBLEM FORMULATION",
      "text" : "Consider an unknown, stable, discrete-time, potentially\nnon-linear, dynamical system\nzk+1 = f(zk, uk), k ∈ {0, 1, . . . , N − 1} , (1) where zk ∈ Rnx and uk ∈ Rnu denote the system state and the control input at time k respectively. Given an initial state z0, the objective is to design a controller that minimizes the cost function J subject to the dynamics in (1), i.e.,\nJ∗0 = min uN−10 J0(z N 0 ,u N−1 0 ) sub. to zk+1 = f(zk, uk) , (2)\nJ0(z N 0 ,u N−1 0 ) = N−1∑ i=0 l(zi, ui) + g(zN , uN ) , (3) where zNi := (zi, zi+1, . . . , zN ). u N−1 i is similarly defined. One of the key challenges in designing such a controller is the modeling of the unknown system dynamics in (1). In this work, we model (1) as a linear time-invariant (LTI) system with system matrices (Aθ , Bθ ). The system matrices are parameterized by θ ∈M ⊆ Rd, which is to be varied during the learning procedure. For a given θ and the current system state zk, let πk(zk, θ) denote the optimal control sequence for the linear system (Aθ , Bθ ) for the horizon {k, k+1, . . . , N}\nπk(zk, θ) := ū N−1 k = arg min\nuN−1k\nJk(z N k ,u N−1 k ) ,\nsubject to zj+1 =Aθzj +Bθuj . (4)\nThe key difference between (2) and (4) is that the controller is designed for the parameterized linear system as opposed to the true system. As θ is varied, different matrix pairs (Aθ , Bθ ) are obtained, which result in different controllers π(·, θ). Our aim is to find, among all linear models, the linear model (Aθ∗ , Bθ∗) whose controller π(·, θ∗) minimizes J0 (ideally achieves J∗0 ) for the actual system, i.e.,\nθ∗ = arg min θ∈M J0(z N 0 ,u N−1 0 ) ,\nsubject to zk+1 = f(zk, uk) , uk = π1k(zk, θ), (5)\nwhere π1k(zk, θ) denotes the 1st control in the sequence πk(zk, θ). To make the dependence on θ explicit, we refer to J0 in (5) as J(θ) here on. Note that (Aθ∗ , Bθ∗) in (5) may not correspond to an actual linearization of the system, but simply to the linear model that gives the best performance on the actual system when its optimal controller is applied in a closed-loop fashion on the actual physical plant.\nWe choose LTI modeling to reduce the number of parameters used to represent the system, and make the dynamics learning process data efficient. Linear modeling also allows to efficiently design the controller in (4) for general cost functions (e.g., using MPC for any convex cost J). In general, the effectiveness of linear modeling depends on both the system and the control objective. If f is linear, a linear model is trivially sufficient for any control objective. If f is non-linear, a linear model may not be sufficient for all control tasks; however, for regulation and trajectory tracking tasks, a linear model is often adequate (see Sec. V). A linear parameterization is also used in adaptive control for similar reasons [13]. Nevertheless, the proposed framework can handle more general model classes as long as the optimal control problem in (4) can be solved for that class.\nSince f is unknown, the shape of the cost function, J(θ), in (5) is unknown. The cost is thus evaluated empirically in each experiment, which is often expensive as it involves conducting an experiment. Thus, the goal is to solve the optimization problem in (5) with as few evaluations as possible. In this paper, we do so via BO."
    }, {
      "heading" : "III. BACKGROUND",
      "text" : "In order to optimize (Aθ , Bθ ), we use BO. In this section, we briefly introduce Gaussian processes and BO."
    }, {
      "heading" : "A. Gaussian Process (GP)",
      "text" : "Since the function J(θ) in (5) is unknown a priori, we use nonparametric GP models to approximate it over its domain M. GPs are a popular choice for probabilistic nonparametric regression, where the goal is to find a nonlinear map, J(θ) : M → R, from an input vector θ ∈ M to the function value J(θ). Hence, we assume that function values J(θ), associated with different values of θ, are random variables and that any finite number of these random variables have a joint Gaussian distribution dependent on the values of θ [14]. For GPs, we define a prior mean function and a covariance function, k(θi, θj), which defines the covariance (or kernel) of any two function values, J(θi)\nand J(θj). In this work, the mean is assumed to be zero without loss of generality. The choice of kernel is problemdependent and encodes general assumptions such as smoothness of the unknown function. In the experimental section, we employ the 5/2 Matèrn kernel where the hyperparameters are optimized by maximizing the marginal likelihood [14]. This kernel function implies that the underlying function J is differentiable and takes values within the 2σ confidence interval with high probability.\nThe GP framework can be used to predict the distribution of the performance function J(θ∗) at an arbitrary input θ∗ based on the past observations, D = {θi, J(θi)}ni=1. Conditioned on D, the mean and variance of the prediction are\nµ(θ∗) = kK−1J ; σ2(θ∗) = k(θ∗, θ∗)− kK−1kT , (6)\nwhere K is the kernel matrix with Kij = k(θi, θj), k = [k(θ1, θ ∗), . . . , k(θn, θ ∗)] and J = [J(θ1), . . . , J(θn)]. Thus, the GP provides both the expected value of the performance function at any arbitrary point θ∗ as well as a notion of the uncertainty of this estimate."
    }, {
      "heading" : "B. Bayesian Optimization (BO)",
      "text" : "Bayesian optimization aims to find the global minimum of an unknown function [1], [15], [16]. BO is particularly suitable for the scenarios where evaluating the unknown function is expensive, which fits our problem in Sec. II. At each iteration, BO uses the past observations D to model the objective function, and uses this model to determine informative sample locations. A common model used in BO for the underlying objective, and the one that we consider, are Gaussian processes (see Sec. III-A). Using the mean and variance predictions of the GP from (6), BO computes the next sample location by optimizing the so-called acquisition function, α (·). Different acquisition functions are used in literature to trade off between exploration and exploitation during the optimization process [1]. For example, the next evaluation for expected improvement (EI) acquisition function [17] is given by θ∗ = arg minθ α (θ) where\nα (θ) = σ(θ)[uΦ(u) + φ(u)]; u = (µ(θ)− T )/σ(θ). (7)\nΦ(·) and φ(·) in (7), respectively, are the standard normal cumulative distribution and probability density functions. The target value T is the minimum of all explored data. Intuitively, EI selects the next parameter point where the expected improvement over T is maximal. Repeatedly evaluating the system at points given by (7) thus improves the observed performance. Note that optimizing α (θ) in (7) does not require physical interactions with the system, but only evaluation of the GP model. When a new set of optimal parameters θ∗ is determined, they are finally evaluated on the real objective function J (i.e., the system).\nIV. DYNAMICS OPTIMIZATION VIA BO (aDOBO)\nThis section presents the technical details of aDOBO, a novel framework for optimizing dynamics model for maximizing the resultant controller performance. In this work, θ ∈ Rnx(nx+nu), i.e., each element in θ corresponds to an entry\nAlgorithm 1: aDOBO algorithm 1 D ←− if available: {θ, J (θ)} 2 Prior ←− if available: Prior of the GP hyperparameters 3 Initialize GP with D 4 while optimize do 5 Find θ∗ = arg minθ α (θ); θ\n′ ←− θ∗ 6 for i = 0 : N − 1 do 7 Given zi and (Aθ′ , Bθ′ ), compute πi(zi, θ ′ ) 8 Apply π1i (zi, θ ′ ) on the real system and measure zi+1 9 zN0 ←− (zN0 , zi+1)\n10 uN−10 ←− (u N−1 0 , π 1 i (zi, θ ′ )) 11 Evaluate J(θ ′ ) := J0(z N 0 ,u N−1 0 ) using (3) 12 Update GP and D with {θ′ , J(θ′)}\nof the Aθ or Bθ matrices. This parameterization is chosen for simplicity, but other parameterizations can easily be used.\nGiven an initial state of the system z0 and the current system dynamics model (Aθ′ , Bθ′ ), we design an optimal control sequence π0(z0, θ ′ ) that minimizes the cost function J0(z N 0 ,u N−1 0 ), i.e., we solve the optimal control problem in (4). The first control of this control sequence is applied to the actual system and the next state z1 is measured. We then similarly compute π1(z1, θ ′ ) starting at z1, apply the first control in the obtained control sequence, measure z2, and so on until we get zN . Once zN0 and u N−1 0 are obtained, we compute the true performance of uN−10 on the actual system by analytically computing J0(zN0 ,u N−1 0 ) using (3). We denote this cost by J(θ ′ ) for simplicity. We next update the GP based on the collected data sample {θ′ , J(θ′)}. Finally, we compute θ∗ that minimizes the corresponding acquisition function α (θ) and repeat the process for (Aθ∗ , Bθ∗). Our approach is illustrated in Figure 1 and summarized in Algorithm 1. Intuitively, aDOBO directly learns the shape of the cost function J(θ) as a function of linearizations (Aθ , Bθ ). Instead of learning the global shape of this function through random queries, it analyzes the performance of all the past evaluations and by optimizing the acquisition function, generates the next query that provides the maximum information about the minima of the cost function. This direct minima-seeking behavior based on the actual observed performance ensures that our approach is data-efficient. Thus, in the space of all linearizations, we efficiently and directly search for the linearization whose corresponding controller minimizes J on the actual system.\nSince the problem in (4) is an optimal control problem for the linear system (Aθ′ , Bθ′ ), depending on the form of the cost function J , different optimal control schemes can be used. For example, if J is quadratic, the optimal controller is a linear feedback controller given by the solution of a Riccati equation. If J is a general convex function, the optimal control problem is solved through a general convex MPC solver, and the resultant controller could be non-linear. Thus, depending on the form of J , the controller designed\nby aDOBO can be linear or non-linear. This property causes aDOBO to perform well in the scenarios where a linear controller is not sufficient, as shown in Sec. VI-B. More generally, the proposed framework is modular and other control schemes can be used that are more suitable for the given cost function, which allows us to capture a richer controller space.\nNote that the GP in our algorithm can be initialized with dynamics models whose controllers are known to perform well on the actual system. This generally leads to a faster convergence. For example, when a good linearization of the system is known, it can be used to initialize D. When no information is known about the system a priori, the initial models are queried randomly."
    }, {
      "heading" : "V. NUMERICAL SIMULATIONS",
      "text" : "In this section, we present some simulation results on the\nperformance of the proposed method for controller design."
    }, {
      "heading" : "A. Dubins Car System",
      "text" : "For the first simulation, we consider a three dimensional non-linear Dubins car whose dynamics are given as\nẋ = v cosφ, ẏ = v sinφ, φ̇ = ω , (8)\nwhere z := (x, y, φ) is the state of system, p = (x, y) is the position, φ is the heading, v is the speed, and ω is the turn rate. The input (control) to the system is u := (v, ω). For simulation purposes, we discretize the dynamics at a frequency of 10Hz. Our goal is to design a controller that steers the system to the equilibrium point z∗ = 0, u∗ = 0 starting from the state z0 := (1.5, 1, π/2). In particular, we want to minimize the cost function\nJ0(z N 0 ,u N−1 0 ) = N−1∑ k=0 ( zTk Qzk + u T kRuk ) +zTNQfzN . (9)\nWe choose N = 30. Q, Qf and R are all chosen as identity matrices of appropriate sizes. We also assume that the dynamics are not known; hence, we cannot directly design a controller to steer the system to the desired equilibrium. Instead, we use aDOBO to find a linearization of dynamics in (8) that minimizes the cost function in (9), directly from the experimental data. In particular, we represent the system in (8) by a parameterized linear system zk+1 = Aθzk +Bθuk, design a controller for this system and apply it on the actual\nsystem. Based on the observed performance, BO suggests a new linearization and the process is repeated. Since the cost function is quadratic in this case, the optimal control problem for a particular θ is an LQR problem, and can be solved efficiently. For further details of LQR method, we refer interested readers to [18]. For BO, we use the MATLAB library BayesOpt [19]. Since there are 3 states and 2 inputs, we learn 15 parameters in total, one corresponding to each entry of the Aθ and Bθ matrices. The bounds on the parameters are chosen randomly as M = [−2, 2]15. As acquisition function, we use EI (see eq. (7)). Since no information is assumed to be known about the system, the GP was initialized with a random θ. We also warp the cost function J using the log function before passing it to BO. Warping makes the cost function smoother while maintaining its monotonic properties, which makes the sampling process in BO more efficient and leads to a faster convergence.\nFor comparison, we solve the true optimal controller that minimizes (9) subject to the dynamics in (8) using the nonlinear solver fmincon in MATLAB to get the minimum achievable cost J∗0 across all controllers. We use the percentage error between the true optimal cost J∗0 and the cost achieved by aDOBO as our comparison metric in this work\nηn = 100× (J∗0 − J(θn))/J∗0 , (10)\nwhere J(θn) is the best cost achieved by aDOBO by iteration n. In Fig. 2, we plot ηn for Dubins car. As learning progresses, aDOBO gathers more and more information about the minimum of J0 and reaches within 6% of J∗0 in 200 iterations, demonstrating its effectiveness in designing a controller for an unknown system just from the experimental data. Fig. 2 also highlights the effect of warping in BO. A well warped function converges faster to the optimal performance. We also compared the control and state trajectories obtained from the learned controller with the optimal control and state trajectories. As shown in Fig. 3, the learned system matrices not only achieve the optimal cost, but also follow the optimal state and control trajectories very closely. Even though the trajectories are very close to each other for the true system and its learned linearization, this linearization\nmay not correspond to any actual linearization of the system. The next simulation illustrates this property more clearly."
    }, {
      "heading" : "B. A Simple 1D Linear System",
      "text" : "For this simulation, we consider a simple 1D linear system\nzk+1 = zk + uk , (11)\nwhere zk and uk are the state and the input of the system at time k. Although the dynamics model is very simple, it illustrates some key insights about the proposed method. Our goal is to design a controller that minimizes (9) starting from the state z0 = 1. We choose N = 30 and R = Q = Qf = 1. Since the dynamics are assumed to be unknown, we use aDOBO to learn the dynamics. Here θ := (θ1, θ2) ∈ R2 are the parameters to be learned.\nThe learning process converges in 45 iterations to the true optimal performance (J∗0 = 1.61), which is computed using LQR on the real system. The converged parameters are θ1 = 1.69 and θ2 = 2.45, which are vastly different from the true parameters θ1 = 1 and θ2 = 1, even though the actual system is a linear system. To understand this, we plot the cost obtained on the true system J0 as a function of linearization parameters (θ1, θ2) in Fig. 4. Since the performances of the two sets of parameters are\nvery close to each other, a direct performance based learning process (e.g., aDOBO) cannot distinguish between them and both sets are equally optimal for it. More generally, a wide range of parameters lead to similar performance on the actual system. Hence, we expect the proposed approach to recover the optimal controller and the actual state trajectories, but not necessarily the true dynamics or its true linearization. This simulation also suggests that the true dynamics of the system may not even be required as far as the control performance for a given objective is concerned."
    }, {
      "heading" : "C. Cart-pole System",
      "text" : "We next apply aDOBO to a cart-pole system\n(M +m)ẍ−mlψ̈ cosψ +mlψ̇2 sinψ = F , lψ̈ − g sinψ = ẍ cosψ ,\n(12)\nwhere x denotes the position of the cart with mass M , ψ denotes the pendulum angle, and F is a force that serves as the control input. The massless pendulum is of length l with a mass m attached at its end. Define the system state as z := (x, ẋ, ψ, ψ̇) and the input as u := F . Starting from the\nstate (0, 0, π6 , 0), the goal is to keep the pendulum straight up, while keeping the state within given lower and upper bounds. In particular, we want to minimize the cost\nJ0(z N 0 ,u N−1 0 ) = N−1∑ k=0 ( zTk Qzk + u T kRuk ) + zTNQfzN\n+ λ N∑ i=0 max(0, z − zi, zi − z),\n(13) where λ penalizes the deviation of state zi below z and above z. We assume that the dynamics are unknown and use aDOBO to optimize the dynamics. For simulation, we discretize the dynamics at a frequency of 10Hz. We choose N = 30, M = 1.5Kg, m = 0.175Kg, λ = 100 and l = 0.28m. The Q = Qf = diag([0.1, 1, 100, 1]) and R = 0.1 matrices are chosen to penalize the angular deviation significantly. We use z = [−2,−∞,−0.1,−∞] and z = [2,∞,∞,∞], i.e., we are interested in controlling the pendulum while keeping the cart position within [−2, 2], and limiting the pendulum overshoot to 0.1. The optimal control problem for a particular linearization is a convex MPC problem and solved using YALMIP [20]. The true J∗0 is computed using fmincon.\nAs shown in Fig. 5, aDOBO reaches within 20% of the optimal performance in 250 iterations and continue to make progress towards finding the optimal controller. This simulation demonstrates that the proposed method (a) is applicable to highly non-linear systems, (b) can handle general convex cost functions that are not necessarily quadratic, and (c) different optimal control schemes can be used within the proposed framework. Since an MPC controller can in general be non-linear, this also implies that the proposed method can design non-linear controllers as well."
    }, {
      "heading" : "VI. COMPARISON WITH OTHER METHODS",
      "text" : "In this section, we compare our approach with some other online learning schemes for controller design.\nA. Tuning (Q,R) vs aDOBO\nIn this section, we consider the case in which the cost function J0 is quadratic (see Eq. (9)). Suppose that the actual linearization of the system around z∗ = 0 and u∗ = 0 is known and given by (A∗, B∗). To design a controller for the actual system in such a case, it is a common practice to use\nan LQR controller for the linearized dynamics. However, the resultant controller may be sub-optimal for the actual nonlinear system. To overcome this problem, authors in [5], [6] propose to optimize the controller by tuning penalty matrices Q and R in (9). In particular, the authors solve\nθ∗ = arg min θ∈M J0(z N 0 ,u N−1 0 ) ,\nsub. to zk+1 = f(zk, uk), uk = K(θ)zk , K(θ) = LQR(A∗, B∗,WQ(θ),WR(θ), Qf ) ,\n(14)\nwhere K(θ) denotes the LQR feedback matrix obtained for the system matrices (A∗, B∗) with WQ and WR as state and input penalty matrices. The difference between optimization problems (5) and (14) is that now we parameterize penalty matrices WQ and WR instead of system dynamics. The optimization problem in (14) is solved using BO in a similar fashion as we solve (5) [5]. The parameter θ, in this case, can be initialized by the actual penalty matrices Q and R, instead of a random query, which generally leads to a much faster convergence. An alternative approach is to use aDOBO, except that now we can use (A∗, B∗) as initializations for the system matrices A and B.\nWhen (A∗, B∗) are known to a good accuracy, (Q,R) tuning method is expected to converge quickly to the optimal performance compared to aDOBO as it needs to learn fewer parameters, i.e., (nx+nu) (assuming diagonal penalty matrices) compared to nx(nx+nu) parameters for aDOBO. However, when there is error in (A∗, B∗) (or more generally if dynamics are unknown), the performance of the (Q,R) tuning method can degrade significantly as it relies on an accurate linearization of the system dynamics, rendering the method impractical for control design purposes. To compare the two methods we use the Dubins car model in Eq. (8). The rest of the simulation parameters are same as Section V-A. We compute the linearization of Dubins car around z∗ = 0 and u∗ = 0 using (8) and add random matrices (Ar, Br) to them to generate A′ = (1 − α)A∗ + αAr and B′ = (1−α)B∗+αBr. We then initialize both methods with (A′, B′) for different αs. As shown in Fig. 6, the (Q,R) tuning method outperforms aDOBO, when there is no noise in (A∗, B∗). But as α increases, its performance deteriorates significantly. In contrast, aDOBO is fairly indifferent to the noise level, as it does not assume any prior knowledge of system dynamics. The only information assumed to be known is penalty matrices (Q,R), which are generally designed by the user and hence are known a priori. The another limitation of tuning (Q,R) is that, by design, it can only be used for a quadratic cost function J0, whereas aDOBO can be used for more general cost functions as shown in Sec. V-C.\nB. Learning K vs aDOBO\nWhen the cost function is quadratic, another potential approach is to directly parameterize and optimize the feedback matrix K ∈ Rnxnu in (14) as [8]\nθ∗ = arg min θ∈M J0(z N 0 ,u N−1 0 ) ,\nsub. to zk+1 = f(zk, uk), uk = Kθzk . (15)\nThe advantage of this approach is that only nxnu parameters are learned compared to nx(nx + nu) parameters in aDOBO, which is also evident from Fig. 7a, wherein the learning process for K converges much faster than that for aDOBO. However, a linear controller might not be sufficient for general cost functions, and non-linear controllers are required to achieve a desired performance. As shown in Sec. V-C, aDOBO is not limited to linear controllers; hence, it outperforms the K learning method in such scenarios. Consider, for example, the linear system\nxk+1 = xk + yk, yk+1 = yk + uk , (16)\nand the cost function in Eq. (13) with state zk = (xk, yk), N = 30, z = [0.5,−0.4] and z = [∞,∞]. Q, Qf and R are all identity matrices of appropriate sizes, and λ = 100.\nAs evident from Fig. 7b, directly learning a feedback matrix performs poorly with an error as high as 80% from the optimal cost. Since the cost is not quadratic, the optimal controller is not necessarily linear; however, since the controller in (15) is restricted to a linear space, it performs rather poorly in this case. In contrast, aDOBO continues to improve performance and reaches within 20% of the optimal cost within few iterations, because we implicitly parameterize a much richer controller space via learning A and B. In this example, we capture non-linear controllers by using a linear dynamics model with a convex MPC solver. Since the underlying system is linear, the true optimal controller is also in our search space. Our algorithm makes sure that we make a steady progress towards finding that controller.\nHowever, we are not restricted to learning a linear controller K. One can also directly learn the actual control sequence to be applied to the system (which also captures the optimal controller). This approach, although viable, may not be data-efficient compared to aDOBO as the control sequence space can be very large depending on the problem horizon, and will require a large number of experiments. As shown in Table I, its performance error is more than 250% even after 600 iterations, rendering the method impractical for real systems.\nC. Adaptive Control vs aDOBO\nIn this work, we aim to directly find the best linearization based on the observed performance. Another approach is to learn a true linearization of the system based on the observed state and input trajectory during the experiments. The underlying hypothesis is that as more and more data is collected, a better linearization is obtained, eventually leading to an improved control performance. This approach is in-line with the traditional model identification and the adaptive control frameworks. Let (jzN0 , ju N−1 0 ) denotes the state and input trajectories for experiment j. We also let Di = ∪ij=1(jzN0 , ju N−1 0 ). After experiment i, we fit an LTI model of the form zk+1 = Aizk+Biuk using least squares on data in Di and then use this model to obtain a new controller for experiment i+1. We apply this approach on the linear system in (16) and the non-linear system in (8) with the cost function in (9). For the linear system, the approach converges to the true system dynamics in 5 iterations. However, this approach performs rather poorly on the non-linear system, as shown in Table II. When the underlying system is non-linear, all state and input trajectories may not contribute to the performance improvement. A good linearization should be obtained from the state and input trajectories in the region of interest, which depends on the task. For example, here we want to regulate the system to the equilibrium (0, 0) so a linearization of the system around (0, 0) should be obtained. Thus, it is desirable to use the system trajectories that are close to this equilibrium point. However, a naive prediction error based approach has no means to select these “good” trajectories from the pool of trajectories and hence can lead to a poor performance. In contrast, aDOBO does not suffer from these limitations, as it explicitly utilizes a performance based optimization. A summary of the advantages and limitations of the four methods is provided in Table III."
    }, {
      "heading" : "VII. QUADROTOR POSITION TRACKING EXPERIMENTS",
      "text" : "We now present the results of our experiments on Crazyflie 2.0, which is an open source nano quadrotor platform developed by Bitcraze. Its small size, low cost, and robustness make it an ideal platform for testing\nnew control paradigms. Recently, it has been extensively used to demonstrate aggressive flights [21], [22].\nFor small yaw, the quadrotor system is modeled as a rigid body with a ten dimensional state vector s := [ p, v, ζ, ω ] , which includes the position p = (x, y, z) in an inertial frame I , linear velocities v = (vx, vy, vz) in I , attitude (orientation) represented by Euler angles ζ, and angular velocities ω. The system is controlled via three inputs u := [ u1, u2, u3 ] , where u1 is the thrust along the z-axis, and u2 and u3 are rolling, pitching moments respectively. The full non-linear dynamics of Crazyflie can be obtained from [23] using the physical parameters computed in [21]. In this experiment, our goal is to track a desired position p∗ starting from the initial position p0 = [0, 0, 1]. Formally, we minimize\nJ0(̄s N 0 ,u N−1 0 ) = N−1∑ k=0 ( s̄TkQs̄+ u T kRuk ) + s̄TNQf s̄ , (17)\nwhere s̄ := [ p− p∗, v, ζ, ω ] . Given the dynamics in [23], the desired optimal control problem can be solved using LQR; however, the resultant controller may not be optimal for the actual system since (a) true underlying system is nonlinear (b) the actual system may not follow the dynamics in [23] due to several unmodeled effects, as illustrated in our results. Hence, we assume that the dynamics of vx and vy are unknown, and model them as[\nfvx fvy\n] = Aθ [ φ ψ ] +Bθu1 , (18)\nwhere A and B are parameterized through θ. Our goal is to learn the parameter θ∗ that minimizes the cost in (17) for the actual Crazyflie using aDOBO. We use N = 400; the penalty matrix Q is chosen to penalize the position deviation. In our experiments, Crazyflie was flown in presence of a VICON motion capture system, which along with on-board sensors provides the full state information at 100Hz. The optimal control problem for a particular linearization in (18) is solved using LQR. For comparison, we compute the nominal optimal controller using the full dynamics in [23]. Figure 9 shows the performance of the controller from aDOBO compared with the nominal controller during the learning process. The nominal controller outperforms the learned controller initially, but within a few iterations, aDOBO performs better than the controller derived from the known dynamics model of Crazyflie. This is because aDOBO optimizes controller based on the performance of the actual system and hence can account for unmodeled effects. In 45 iterations, the learned controller outperforms the nominal controller by 12%, demonstrating the performance potential of aDOBO on real systems."
    }, {
      "heading" : "VIII. CONCLUSIONS AND FUTURE WORK",
      "text" : "In this work, we introduce aDOBO, an active learning framework to optimize the system dynamics with the intent of maximizing the controller performance. Through simulations and real-world experiments, we demonstrate that aDOBO achieve the optimal control performance even when no prior information is known about the system dynamics. For future work, it will be interesting to generalize aDOBO to optimize the dynamics for a class of cost functions. Leveraging the state and input trajectory data, along with the observed performance, to further increase the data-efficiency of the learning process is another promising direction. Finally, it will be interesting to see how aDOBO can scale to more complex non-linear dynamics models."
    } ],
    "references" : [ {
      "title" : "Taking the human out of the loop: A review of Bayesian optimization",
      "author" : [ "B. Shahriari", "K. Swersky", "Z. Wang", "R.P. Adams", "N. de Freitas" ],
      "venue" : "Proceedings of the IEEE, vol. 104, no. 1, pp. 148–175, 2016.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Gaussian processes for data-efficient learning in robotics and control",
      "author" : [ "M.P. Deisenroth", "D. Fox", "C.E. Rasmussen" ],
      "venue" : "Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2015.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reinforcement learning with misspecified model classes",
      "author" : [ "J. Joseph", "A. Geramifard", "J.W. Roberts", "J.P. How", "N. Roy" ],
      "venue" : "International Conference on Robotics and Automation, 2013, pp. 939–946.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Model learning for robot control: a survey",
      "author" : [ "D. Nguyen-Tuong", "J. Peters" ],
      "venue" : "Cognitive Processing, vol. 12, no. 4, pp. 319–340, 2011.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Automatic LQR tuning based on Gaussian process global optimization",
      "author" : [ "A. Marco", "P. Hennig", "J. Bohg", "S. Schaal", "S. Trimpe" ],
      "venue" : "International Conference on Robotics and Automation, 2016.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A selftuning LQR approach demonstrated on an inverted pendulum",
      "author" : [ "S. Trimpe", "A. Millane", "S. Doessegger", "R. D’Andrea" ],
      "venue" : "IFAC Proceedings Volumes, vol. 47, no. 3, pp. 11 281–11 287, 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Feedback controller parameterizations for reinforcement learning",
      "author" : [ "J.W. Roberts", "I.R. Manchester", "R. Tedrake" ],
      "venue" : "Symposium on Adaptive Dynamic Programming And Reinforcement Learning (ADPRL), 2011, pp. 310–317.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Bayesian optimization for learning gaits under uncertainty",
      "author" : [ "R. Calandra", "A. Seyfarth", "J. Peters", "M.P. Deisenroth" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence, vol. 76, no. 1, pp. 5–23, 2015.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Implicit and explicit LQG self-tuning controllers",
      "author" : [ "M. Grimble" ],
      "venue" : "Automatica, vol. 20, no. 5, pp. 661–669, 1984.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "A generalized LQG approach to self-tuning control part i. aspects of design",
      "author" : [ "D. Clarke", "P. Kanjilal", "C. Mohtadi" ],
      "venue" : "International Journal of Control, vol. 41, no. 6, pp. 1509–1523, 1985.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Nonlinear adaptive control using nonparametric Gaussian process prior models",
      "author" : [ "R. Murray-Smith", "D. Sbarbaro" ],
      "venue" : "IFAC Proceedings Volumes, vol. 35, no. 1, pp. 325–330, 2002.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Adaptive control: stability, convergence and robustness",
      "author" : [ "S. Sastry", "M. Bodson" ],
      "venue" : "Courier Corporation,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Gaussian Processes for Machine Learning",
      "author" : [ "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise",
      "author" : [ "H.J. Kushner" ],
      "venue" : "Journal of Basic Engineering, vol. 86, p. 97, 1964.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1964
    }, {
      "title" : "Gaussian processes for global optimization",
      "author" : [ "M.A. Osborne", "R. Garnett", "S.J. Roberts" ],
      "venue" : "Learning and Intelligent Optimization (LION3), 2009, pp. 1–15.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On bayesian methods for seeking the extremum",
      "author" : [ "J. Močkus" ],
      "venue" : "Optimization Techniques IFIP Technical Conference, 1975.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "The linear-quadratic optimal regulator for descriptor systems: discrete-time case",
      "author" : [ "D.J. Bender", "A.J. Laub" ],
      "venue" : "Automatica, 1987.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "BayesOpt: a bayesian optimization library for nonlinear optimization, experimental design and bandits.",
      "author" : [ "R. Martinez-Cantin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "YALMIP: A toolbox for modeling and optimization in MATLAB",
      "author" : [ "J. Lofberg" ],
      "venue" : "International Symposium on Computer Aided Control Systems Design, 2005, pp. 284–289.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Planning and control for quadrotor flight through cluttered environments",
      "author" : [ "B. Landry" ],
      "venue" : "Master’s thesis, MIT, 2015.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning quadrotor dynamics using neural network for flight control",
      "author" : [ "S. Bansal", "A.K. Akametalu", "F.J. Jiang", "F. Laine", "C.J. Tomlin" ],
      "venue" : "Conference on Decision and Control, 2016, pp. 4653–4660.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Parameter identification of an autonomous quadrotor",
      "author" : [ "N. Abas", "A. Legowo", "R. Akmeliawati" ],
      "venue" : "International Conference On Mechatronics, 2011, pp. 1–8.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Hence, we employ BO, an optimization method often used to optimize a performance criterion while keeping the number of evaluations of the physical system small [1].",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 1,
      "context" : "In online methods, the dynamics model is instead iteratively updated using new data collected by evaluating the controller [2].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "introduce sufficient inaccuracies to lead to suboptimal control performance [3].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : "Using machine learning techniques, such as Gaussian processes, does not alleviate this issue [4].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "Instead, authors in [3] proposed to optimize the dynamics model directly with respect to the controller performance, but since",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "In [5]–[7] authors tuned the penalty",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "In [5]–[7] authors tuned the penalty",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 7,
      "context" : "Parameters of a linear feedback controller are learned in [8] using BO.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 11,
      "context" : "The problem of updating a system model to improve control performance is also related to adaptive control, where the model parameters are identified from sensor data, and subsequently the updated model is used to design a controller (see [9]–[13]).",
      "startOffset" : 242,
      "endOffset" : 246
    }, {
      "referenceID" : 11,
      "context" : "A linear parameterization is also used in adaptive control for similar reasons [13].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 12,
      "context" : "variables have a joint Gaussian distribution dependent on the values of θ [14].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : "In the experimental section, we employ the 5/2 Matèrn kernel where the hyperparameters are optimized by maximizing the marginal likelihood [14].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "Bayesian optimization aims to find the global minimum of an unknown function [1], [15], [16].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 13,
      "context" : "Bayesian optimization aims to find the global minimum of an unknown function [1], [15], [16].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "Bayesian optimization aims to find the global minimum of an unknown function [1], [15], [16].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "Different acquisition functions are used in literature to trade off between exploration and exploitation during the optimization process [1].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 15,
      "context" : "For example, the next evaluation for expected improvement (EI) acquisition function [17] is given by θ∗ = arg minθ α (θ) where",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 16,
      "context" : "For further details of LQR method, we refer interested readers to [18].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 17,
      "context" : "For BO, we use the MATLAB library BayesOpt [19].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : "The optimal control problem for a particular linearization is a convex MPC problem and solved using YALMIP [20].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "To overcome this problem, authors in [5], [6] propose to optimize the controller by tuning penalty matrices Q and R in (9).",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "To overcome this problem, authors in [5], [6] propose to optimize the controller by tuning penalty matrices Q and R in (9).",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : "The optimization problem in (14) is solved using BO in a similar fashion as we solve (5) [5].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 7,
      "context" : "proach is to directly parameterize and optimize the feedback matrix K ∈ Rxu in (14) as [8]",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "6: Dubins car: Comparison between (Q,R) tuning [5] (dashed curves), and aDOBO (solid curves) for different noise levels in (A∗, B∗).",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 7,
      "context" : "7: Mean and standard deviation of η obtained via directly learning K [8] and aDOBO for different cost functions.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 19,
      "context" : "Recently, it has been extensively used to demonstrate aggressive flights [21], [22].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 20,
      "context" : "Recently, it has been extensively used to demonstrate aggressive flights [21], [22].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 21,
      "context" : "The full non-linear dynamics of Crazyflie can be obtained from [23] using the physical parameters computed in [21].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 19,
      "context" : "The full non-linear dynamics of Crazyflie can be obtained from [23] using the physical parameters computed in [21].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "In this experiment, our goal is to track a desired position p∗ starting from the initial position p0 = [0, 0, 1].",
      "startOffset" : 103,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : "Given the dynamics in [23], the desired optimal control problem can be solved using LQR; however, the resultant controller may not be optimal for the actual system since (a) true underlying system is non-",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 21,
      "context" : "linear (b) the actual system may not follow the dynamics in [23] due to several unmodeled effects, as illustrated in our results.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "Method Advantages Limitations (Q,R) learning [5] Only (nx + nu) parameters are to be learned so learning will be faster.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "F learning [8] Only nxnu parameters are to be learned so learning will be faster.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 21,
      "context" : "For comparison, we compute the nominal optimal controller using the full dynamics in [23].",
      "startOffset" : 85,
      "endOffset" : 89
    } ],
    "year" : 2017,
    "abstractText" : "Real-world robots are becoming increasingly complex and commonly act in poorly understood environments where it is extremely challenging to model or learn their true dynamics. Therefore, it might be desirable to take a taskspecific approach, wherein the focus is on explicitly learning the dynamics model which achieves the best control performance for the task at hand, rather than learning the true dynamics. In this work, we use Bayesian optimization in an active learning framework where a locally linear dynamics model is learned with the intent of maximizing the control performance, and used in conjunction with optimal control schemes to efficiently design a controller for a given task. This model is updated directly based on the performance observed in experiments on the physical system in an iterative manner until a desired performance is achieved. We demonstrate the efficacy of the proposed approach through simulations and real experiments on a quadrotor testbed.",
    "creator" : "LaTeX with hyperref package"
  }
}