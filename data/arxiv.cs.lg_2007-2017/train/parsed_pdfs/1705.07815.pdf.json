{
  "name" : "1705.07815.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Minimax Statistical Learning and Domain Adaptation with Wasserstein Distances∗",
    "authors" : [ "Jaeho Lee", "Maxim Raginsky" ],
    "emails" : [ "jlee620@illinois.edu", "maxim@illinois.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 5.\n07 81\n5v 1\n[ cs\n.L G\n] 2\n2 M\nay 2"
    }, {
      "heading" : "1 Introduction and problem set-up",
      "text" : "In the traditional paradigm of statistical learning [1], we have a class P of probability measures on a measurable instance space Z and a class F of measurable functions f : Z → R+. Each f ∈ F quantifies the loss of some decision rule or a hypothesis applied to instances z ∈ Z, so, with a slight abuse of terminology, we will refer to F as the hypothesis space. The (expected) risk of a hypothesis f on instances generated according to P is given by\nR(P, f) := EP [f(Z)] =\n∫\nZ f(z)P (dz).\nGiven an n-tuple Z1, . . . , Zn of i.i.d. training examples drawn from an unknown P ∈ P, the objective is to find a hypothesis f̂ ∈ F whose risk R(P, f̂) is close to the minimum risk\nR∗(P,F) := inf f∈F R(P, f) (1.1)\nwith high probability. Under suitable regularity assumptions, this objective can be accomplished via Empirical Risk Minimization (ERM) [1, 2]:\nR(Pn, f) = 1\nn\nn∑\ni=1\nf(Zi) −→ min, f ∈ F (1.2)\n∗This work was supported in part by the NSF grant nos. CIF-1527388 and CIF-1302438, and in part by the NSF\nCAREER award 1254041. †jlee620@illinois.edu ‡maxim@illinois.edu\nwhere Pn := 1 n ∑n i=1 δZi is the empirical distribution of the training examples.\nRecently, however, an alternative viewpoint has emerged, inspired by ideas from robust statistics and robust stochastic optimization. In this distributionally robust framework, instead of solving the ERM problem (1.2), one aims to solve the minimax problem\nsup Q∈A(Pn)\nR(Q, f) −→ min, f ∈ F (1.3)\nwhere A(Pn) is an ambiguity set containing the empirical distribution Pn and, possibly, the unknown probability law P either with high probability or almost surely. The ambiguity sets serve as a mechanism for compensating for the uncertainty about P that inherently arises due to having only a finite sample to work with, and they can be constructed in a variety of ways, e.g., via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7]. However, with the exception of the recent work by Farnia and Tse [3], the minimizer of (1.3) is still evaluated under the standard statistical risk minimization paradigm. In this work, we instead propose replacing the statistical risk minimization criterion (1.1) with the local minimax risk\ninf f∈F sup Q∈A(P ) R(Q, f)\nat P , where the ambiguity set A(P ) is taken to be a Wasserstein ball centered at P . (Recently, this modification was also proposed by Farnia and Tse [3] for ambiguity sets defined by a finite number of moment constraints.) As we will argue below, this change of perspective is natural when there is a possibility of domain drift, i.e., when the learned hypothesis is evaluated on a distribution Q which may be different from the distribution P that was used to generate the training data."
    }, {
      "heading" : "1.1 Wasserstein ambiguity sets and local minimax risk",
      "text" : "We assume that the instance space Z is a Polish space (i.e., a complete separable metric space) with metric dZ . We denote by P(Z) the space of all Borel probability measures on Z, and by Pp(Z) with p ≥ 1 the space of all P ∈ P(Z) with finite pth moments:\nPp(Z) := { P ∈ P(Z) : EP [dpZ(Z, z0)] < ∞ for some z0 ∈ Z } .\nThe metric structure of Z can be used to define a family of metrics on the spaces Pp(Z) [8]:\nDefinition 1.1. For p ≥ 1, the p-Wasserstein distance between P,Q ∈ Pp(Z) is\nWp(P,Q) := inf M(·×Z)=P M(Z×·)=Q\n( EM [d p Z(Z,Z ′)] )1/p , (1.4)\nwhere the infimum is over all couplings of P and Q, i.e., probability measures M on the product space Z × Z with the given marginals P and Q.\nRemark 1.1. Wasserstein metrics arise in the problem of optimal transport : for any coupling M of P and Q, the conditional distribution MZ′|Z can be viewed as a randomized policy for ‘transporting’ a unit quantity of some material from a random location Z to another location Z ′, while satisfying the marginal constraint Z ′ ∼ Q. If the cost of transporting a unit of material from z ∈ Z to z′ ∈ Z is given by dpZ(z, z ′), then W pp (P,Q) is the minimum expected transport cost.\nWe now consider a learning problem (P,F) with P = Pp(Z) for some p ≥ 1. Following [5, 6, 7], we let the ambiguity set A(P ) be the p-Wasserstein ball of radius ε ≥ 0 centered at P :\nA(P ) = BWε,p(P ) := { Q ∈ Pp(Z) : Wp(P,Q) ≤ ε } ,\nwhere the radius ε > 0 is a tunable parameter. We then define the local worst-case risk of f at P ,\nRε,p(P, f) := sup Q∈BWε,p(P ) R(Q, f),\nand the local minimax risk at P :\nR∗ε,p(P,F) := inf f∈F Rε,p(P, f).\nSome inequalities relating the local worst-case risk Rε,p(Q, f) and the statistical risk R(Q, f) are presented in Section 2.\nThe basic problem of interest can now be stated as follows: given an n-tuple Z1, . . . , Zn of i.i.d. training examples drawn from an unknown P ∈ Pp(Z), find a hypothesis f̂ ∈ F , such that\nRε,p(P, f̂) ≈ R∗ε,p(P,F) with high probability.\nIn Section 3, we show that, under some regularity assumptions, this goal can be achieved via a natural Empirical Risk Minimization (ERM) procedure. In particular, Theorem 3.1 provides a high-probability bound on the excess risk Rε,p(P, f̂)−R∗ε,p(P,F)."
    }, {
      "heading" : "1.2 Motivating problem: domain adaptation",
      "text" : "One of the attractive features of ambiguity sets based on Wasserstein distances is that, because of their intimate connection to the metric geometry of the instance space, they provide a natural mechanism for handling uncertainty due to (possibly randomized) transformations acting on the problem instances. To illustrate this point, we briefly discuss a motivating example of domain adaptation.\nIn contrast to the standard statistical learning framework, where the risk of the learned hypothesis is evaluated on the same unknown distribution that was used for generating the training examples, the problem of domain adaptation [9] arises when the training data are generated according to an unknown distribution P , but the learned hypothesis is evaluated on another unknown distribution Q. However, it is assumed that these distributions (commonly referred to as problem domains) are somehow related, and some partial information about Q is also available at training time. In the context of supervised learning, the instances are random couples Z = (X,Y ) consisting of features X and labels Y , and the training data consist of n labeled examples Z1 = (X1, Y1), . . . , Zn = (Xn, Yn) drawn from the source domain P and m unlabeled features X ′1, . . . ,X ′ m drawn from the target domain Q. The goal is to learn a hypothesis that would perform well on the target domain Q. In a recent paper, Courty et al. [10] have introduced an algorithmic framework for domain adaptation based on optimal transport. Their approach revolves around a particular generative model for the drift between the source and the target domains. Let us disintegrate the source domain distribution as P = µ⊗ PY |X , where µ ∈ P(X ) is the marginal distribution of the features and PY |X is the conditional distribution of the labels given the features. Then there exists an unknown deterministic transformation T : X → X of the feature space, such that a sample (X ′, Y ′)\nfrom the target domain distribution Q = ν ⊗ QY |X can be generated using the following two-step procedure:\nX T−−−−→ X ′ PY |X−−−−−→ Y ′,\nwhere the input X is drawn from µ. In other words, the domain drift is due solely to an unknown deterministic transformation of the features. If we further assume that T is the optimal transport map from µ to ν, i.e., that W pp (µ, ν) = Eµ[d p X (X,T (X))] under some metric dX on X , and if Wp(P,Q) = Wp(µ, ν), then it is natural to cast the problem of domain adaptation as that of learning a hypothesis f ∈ F that would approximately achieve the local minimax risk Rε,p(P,F), where ε is some estimate of Wp(µ, ν) from source and target training data. In Section 4, we present an algorithm based on this idea and provide a quantitative analysis of its performance. In particular, Theorem 4.1 gives a high-probability bound on the excess risk of the learned classifier with respect to the target domain distribution Q. We note that, in contrast to the original methodology of Courty et al. [10], our approach completely bypasses the problem of estimating the transport map T ."
    }, {
      "heading" : "2 Local worst-case risk vs. statistical risk",
      "text" : "In some situations (see, e.g., Section 4), it is of interest to convert back and forth between local worst-case (or local minimax) risks and the usual statistical risks. In this section, we give a couple of inequalities relating these quantities. The first one is a simple consequence of the Kantorovich duality theorem from the theory of optimal transport [8]:\nProposition 2.1. Suppose that f is L-Lipschitz, i.e., |f(z) − f(z′)| ≤ LdZ(z, z′) for all z, z′ ∈ Z. Then, for any Q ∈ BWε,p(P ),\nR(Q, f) ≤ Rε,p(P, f) ≤ R(Q, f) + 2Lε.\nAs an example, consider the problem of binary classification with hinge loss: Z = X ×Y, where X is an arbitrary feature space, Y = {−1,+1}, and the hypothesis space F consists of all functions of the form f(z) = f(x, y) = max{0, 1− yf0(x)}, where f0 : X → R is a candidate predictor. Then, since the function u 7→ max{0, 1 − u} is Lipschitz-continuous with constant 1, we can write\n|f(x, y)− f(x′, y′)| ≤ |yf0(x)− y′f0(x′)| ≤ 2‖f0‖X1{y 6= y′}+ |f0(x)− f0(x′)|,\nwhere ‖f0‖X := supx∈X |f0(x)|. If ‖f0‖X < ∞ and if f0 is L0-Lipschitz with respect to some metric dX on X , then it follows that f is Lipschitz with constant max{2‖f0‖X , L0} with respect to the product metric\ndZ(z, z ′) = dZ((x, y), (x ′, y′)) := dX (x, x ′) + 1{y 6= y′}.\nNext we consider the case when the function f is smooth but not Lipschitz-continuous. Since we are working with general metric spaces that may lack an obvious differentiable structure, we need to first introduce some concepts from metric geometry [11]. A metric space (Z,dZ) is a geodesic space if for every two points z, z′ ∈ Z there exists a path γ : [0, 1] → Z, such that γ(0) = z, γ(1) = z′, and dZ(γ(s), γ(t)) = (t− s) · dZ(γ(0), γ(1)) for all 0 ≤ s ≤ t ≤ 1 (such a path is called a constant-speed\ngeodesic). A functional F : Z → R is geodesically convex if for any pair of points z, z′ ∈ Z there is a constant-speed geodesic γ, so that\nF (γ(t)) ≤ (1− t)F (γ(0)) + tF (γ(1)) = (1− t)F (z) + tF (z′), ∀t ∈ [0, 1].\nAn upper gradient of a Borel function f : Z → R is a functional Gf : Z → R+, such that for any pair of points z, z′ ∈ Z there exists a constant-speed geodesic γ obeying\n|f(z′)− f(z)| ≤ ∫ 1\n0 Gf (γ(t))dt · dZ(z, z′). (2.1)\nWith these definitions at hand, we have the following:\nProposition 2.2. Suppose that f has a geodesically convex upper gradient Gf . Then\nR(Q, f) ≤ Rε,p(P, f) ≤ R(Q, f) + 2ε sup Q∈BWε,p(P ) ‖Gf (Z)‖Lq(Q),\nwhere 1/p + 1/q = 1, and ‖ · ‖Lq(Q) := ( EQ| · |q )1/q .\nAs a simple example, consider the setting of regression with quadratic loss: let X be a convex subset of Rd, let Y = [−B,B] for some 0 < B < ∞, and equip Z = X × Y with the Euclidean metric\ndZ(z, z ′) = √ ‖x− x′‖22 + |y − y′|2, z = (x, y), z′ = (x′, y′).\nSuppose that the functions f ∈ F are of the form f(z) = f(x, y) = (y−f0(x))2 with f0 ∈ C1(Rd,R), such that ‖f0‖X ≤ M < ∞ and ‖∇f0(x)‖2 ≤ L‖x‖2 for some 0 < L < ∞. Then Proposition 2.2 leads to the following:\nProposition 2.3.\nR(Q, f) ≤ Rε,2(P, f) ≤ R(Q, f) + 4ε(B +M) ( 1 + L sup\nQ∈BWε,2(P )\nσQ,X\n) ,\nwhere σQ,X := EQ‖X‖2 for Z = (X,Y ) ∼ Q."
    }, {
      "heading" : "3 Guarantees for empirical risk minimization",
      "text" : "Let Z1, . . . , Zn be an n-tuple of i.i.d. training examples drawn from P . In this section, we analyze the performance of the ERM procedure\nf̂ := argmin f∈F\nR∗ε,p(Pn, f). (3.1)\nThe following strong duality result due to Gao and Kleywegt [7] will be instrumental:\nProposition 3.1. For any upper semicontinuous function f : Z → R and for any Q ∈ Pp(Z),\nRε,p(Q, f) = min λ≥0\n{ λεp +EQ[ϕλ,f (Z)] } ,\nwhere ϕλ,f (z) := supz′∈Z { f(z′)− λ · dpZ(z, z′) } .\nWe begin by imposing some regularity assumptions:\nAssumption 3.1. The instance space Z is bounded: diam(Z) := supz,z′∈Z dZ(z, z′) < ∞. Assumption 3.2. The functions in F are upper semicontinuous and uniformly bounded: 0 ≤ f(z) ≤ M < ∞ for all f ∈ F and z ∈ Z. Assumption 3.3. There exists a hypothesis f0 ∈ F , such that, for all z ∈ Z, f0(z) ≤ C0dpZ(z, z0) for some C0 ≥ 0 and z0 ∈ Z.\nWe can now give a performance guarantee for the ERM procedure (3.1):\nTheorem 3.1. If Assumptions 3.1–3.3 are satisfied, then, with probability at least 1− δ,\nRε,p(P, f̂)−R∗ε,p(P,F) ≤ 24√ n\n∫ ∞\n0\n√ logN (F , ‖ · ‖∞, u/2)du\n+ 24C0(2 diam(Z))p√\nn\n( 1 + ( diam(Z)\nε\n)p) + 3M √ log(2/δ)\n2n , (3.2)\nwhere N (F , ‖ · ‖∞, ·) are the covering numbers of F with respect to the sup norm. Remark 3.1. The first term on the right-hand side of (3.1) is the Dudley entropy integral [12]. We conjecture that this term can be replaced by the expected Rademacher average of the hypothesis class F , but have been unable to prove it. Remark 3.2. The second term on the right-hand side of (3.1) increases as ε → 0. The excess risk bound of Farnia and Tse [3] has the same behavior, where in that case ε is the slack in the moment constraints defining the ambiguity set. This (1/ε)p scaling can be eliminated if more refined bounds on the optimum dual λ are available.\nProof. Let f∗ ∈ F be any achiever of the local minimax risk R∗ε,p(P,F). We start by decomposing the excess risk:\nRε,p(P, f̂)−R∗ε,p(P,F) = Rε,p(P, f̂)−R∗ε,p(P, f∗) ≤ Rε,p(P, f̂)−Rε,p(Pn, f̂) +Rε,p(Pn, f∗)−Rε,p(P, f∗),\nwhere the last step follows from the definition of f̂ . Define\nλ̂ := argmin λ≥0\n{ λεp +EPn [ϕλ,f̂ (Z)] } , λ∗ := argmin\nλ≥0\n{ λεp +EP [ϕλ,f∗(Z)] } .\nThen, using Proposition 3.1, we can write\nRε,p(P, f̂)−Rε,p(Pn, f̂) = min λ≥0\n{ λεp + ∫\nZ ϕ λ,f̂ (z)P (dz)\n} − ( λ̂εp + ∫\nZ ϕ λ̂,f̂ (z)Pn(dz)\n)\n≤ ∫\nZ ϕ λ̂,f̂\n(z)(P − Pn)(dz)\nand, following similar logic,\nRε,p(Pn, f ∗)−Rε,p(P, f∗) ≤\n∫\nZ ϕλ∗,f∗(z)(Pn − P )(dz). (3.3)\nBy Lemma 3.1 given in Section 3.2, λ̂ ∈ Λ := [0, C02p−1(1 + (diam(Z)/ε)p)]. Hence, defining the function class Φ := { ϕλ,f : λ ∈ Λ, f ∈ F } , we have\nRε,p(P, f̂)−Rε,p(Pn, f̂) ≤ sup ϕ∈Φ\n[∫\nZ ϕd(P − Pn)\n] . (3.4)\nSince all f ∈ F take values in [0,M ], the same holds for all ϕ ∈ Φ. Therefore, by a standard symmetrization argument,\nRε,p(P, f̂)−R∗ε,p(P, f̂) ≤ 2Rn(Φ) +M √ 2 log(2/δ)\nn\nwith probability at least 1− δ/2, where\nRn(Φ) := E  sup ϕ∈Φ 1 n n∑\ni=1\nσiϕ(Zi)\n \nis the expected Rademacher average of Φ, with i.i.d. Rademacher random variables σ1, . . . , σn independent of Z1, . . . , Zn. Moreover, from (3.3) and from Hoeffding’s inequality it follows that\nRε,p(Pn, f ∗)−Rε,p(P, f∗) ≤ M\n√ log(2/δ)\n2n\nwith probability at least 1− δ/2. Consequently,\nRε,p(P, f̂)−R∗ε,p(P,F) ≤ 2Rn(Φ) + 3M √ log(2/δ)\n2n (3.5)\nwith probability at least 1− δ. Using the bound of Lemma 3.2 from Section 3.2 in (3.5), we obtain the statement of the theorem."
    }, {
      "heading" : "3.1 Example bounds",
      "text" : "In this subsection, we illustrate the use of Theorem 3.1 when (upper bounds on) the covering numbers for the hypothesis class F are available. Throughout this section, we let Z = X ×Y, where the feature space X = {x ∈ Rd : ‖x‖2 ≤ r0} is a ball of radius r0 in Rd with center at the origin, the label space Y ⊆ [−B,+B] for some 0 < B < ∞. We equip Z with the Euclidean metric\ndZ(z, z ′) = dZ((x, y), (x ′, y′)) := √ ‖x− x′‖22 + |y − y′|2\nand take p = 2. We first consider a simple neural network class F consisting of functions of the form f(z) = f(x, y) = (y− s(fT0 x))2, where s : R → R is a bounded smooth nonlinearity with s(0) = 0 and with bounded first derivative, and where f0 takes values in the unit ball in R d.\nCorollary 3.1. For any P ∈ P(Z), with probability at least 1− δ,\nRε,2(P, f̂)−R∗ε,2(P,F)\n≤ C1√ n +\n384(r0 +B) 2\n√ n\n( 1 + 4(r20 +B 2)\nε2\n) + 6(‖s‖2∞ +B2) √\nlog(2/δ)√ 2n\nwhere C1 is a constant dependent only on d, r0, s:\nC1 = 288 √ dr0(B + ‖s‖∞)‖s′‖∞.\nWe also consider the case of a massive nonparametric class. Let (HK , ‖ · ‖K) be the Gaussian reproducing kernel Hilbert space (RKHS) with the kernel K(x1, x2) = exp { −‖x1 − x2‖22/σ2 } for\nsome σ > 0, and let Br := { h ∈ HK : ‖h‖K ≤ r } be the radius-r ball in HK . Let F be the class of all functions of the form f(z) = f(x, y) = (y − f0(x))2, where the predictors f0 : X → R belong to IK(Br), an embedding of Br into the space C(X ) of continuous real-valued functions on X equipped with the sup norm ‖f‖X := supx∈X |f(x)|.\nIn order to apply Theorem 3.1, we need to control the covering numbers N (F , ‖ · ‖∞, ·). To that end, we need the following estimate due to Cucker and Zhou [13, Thm 5.1] (which was later shown by Kühn [14] to be asymptotically exact up to the double logarithmic factor):\nProposition 3.2. For compact X ⊆ Rd,\nlogN (IK(Br), ‖ · ‖X , u) ≤ d ( 32 +\n640d(diam(X ))2 σ2\n)d+1 ( log r\nu\n)d+1\nholds for all 0 < u ≤ r/2. Using Proposition 3.2, we can prove the following generalization bound for Gaussian RKHS.\nCorollary 3.2. With probability at least 1− δ, for any P ∈ P(Z), Rε,2(P, f̂)−R∗ε,2(P,F) ≤ C1√ n (r2 +Br) + 384(r0 +B) 2 √ n ( 1 + 4(r20 +B 2) ε2 ) + 6(r2 +B2) √ log(2/δ)√ 2n\nwhere C1 is a constant dependent only on d, r0, σ:\nC1 = 48 √ d ( 2Γ ( d+ 3\n2 , log 2\n) + (log 2) d+1 2 )( 32 +\n2560dr20 σ2\n) d+1 2\n,\nand Γ(s, v) := ∫∞ v u s−1e−udu is the incomplete gamma function."
    }, {
      "heading" : "3.2 Technical lemmas for the proof of Theorem 3.1",
      "text" : "Lemma 3.1. Fix some Q ∈ Pp(Z). Define f̃ ∈ F and λ̃ ≥ 0 via\nf̃ := argmin f∈F Rε,p(Q, f) and λ̃ := argmin λ≥0\n{ λεp +EQ[ϕλ,f̃ (Z)] } .\nThen\nλ̃ ≤ C02p−1 ( 1 + ( diam(Z)\nε\n)p) . (3.6)\nLemma 3.2. The expected Rademacher complexity of the function class Φ satisfies\nRn(Φ) ≤ 12√ n\n∫ ∞\n0\n√ logN (F , ‖ · ‖∞, u/2)du+ 12C0(2 diam(Z))p√ n\n( 1 + ( diam(Z)\nε\n)p) ."
    }, {
      "heading" : "4 Application to domain adaptation",
      "text" : "As discussed in Section 1.2, the problem of domain adaptation arises when we want to transfer the data or knowledge from a source domain P ∈ P(Z) to a different but related target domain Q ∈ P(Z) [9]. Suppose that it is possible to estimate the Wasserstein distance Wp(P,Q) between the two domain distributions. Then, as we show below, we can provide a generalization bound for the target domain by combining estimation guarantees for Wp(P,Q) with risk inequalities of Section 2.\nWe work in the setting considered by Courty et al. [10]: Let Z = X × Y, where (X , dX ) is the feature space and (Y, dY ) is the label space. We endow Z with the ℓp product metric\ndZ(z, z ′) = dZ((x, y), (x ′, y′)) := ( dpX (x, x ′) + dpY(y, y ′) )1/p .\nLet P = µ⊗PY |X and Q = ν⊗QY |X be the source and the target domain distributions, respectively. We assume that domain drift is due to an unknown (possibly nonlinear) transformation T : X → X of the feature space that preserves the conditional distribution of the labels given the features. That is, ν = T#µ, the pushforward of µ by T , and for any x ∈ X and any measurable set B ⊆ Y\nPY |X(B|x) = QY |X(B|T (x)). (4.1)\nThis assumption leads to the following lemma, which enables us to estimate Wp(P,Q) only from unlabeled source domain data and unlabeled target domain data:\nLemma 4.1. Suppose there exists a deterministic and invertible optimal transport map T : X → X such that ν = T#µ, i.e., W p p (µ, ν) = Eµ[d p X (X,T (X))]. Then\nWp(P,Q) = Wp(µ, ν). (4.2)\nRemark 4.1. If X is a convex subset of Rd endowed with the ℓp metric dX (x, x′) = ‖x − x′‖p for p ≥ 2, then, under the assumption that µ and ν have positive densities with respect to the Lebesgue measure, the (unique) optimal transport map from µ to ν is deterministic and a.e. invertible – in fact, its inverse is equal to the optimal transport map from ν to µ [8].\nNow suppose that we have n labeled examples (X1, Y1), . . . , (Xn, Yn) from P and m unlabeled examples X ′1, . . . ,X ′ m from ν. Define the empirical distributions\nµn = 1\nn\nn∑\ni=1\nδXi , νm = 1\nm\nm∑\nj=1\nδX′j .\nNotice that, by the triangle inequality, we have\nWp(µ, ν) ≤ Wp(µ, µn) +Wp(µn, νm) +Wp(ν, νm). (4.3)\nHere, Wp(µn, νm) can be computed from unlabeled data by solving a finite-dimensional linear program [8], and the following convergence result of Fournier and Guillin [15] implies that, with high probability, both Wp(µ, µn) and Wp(ν, νm) rapidly converge to zero as n,m → ∞:\nProposition 4.1. Let µ be a probability distribution on a bounded set X ⊂ Rd, where d > 2p. Let µn denote the empirical distribution of X1, . . . ,Xn i.i.d.∼ µ. Then, for any r ∈ (0,∞),\nP(Wp(µn, µ) ≥ r) ≤ Ca exp(−Cbnrd/p) (4.4)\nwhere Ca, Cb are constants depending on p, d, diam(X ) only.\nBased on these considerations, we propose the following domain adaptation scheme:\n1. Compute the p-Wasserstein distance Wp(µn, νm) between the empirical distributions of the features in the labeled training set from the source domain P and the unlabeled training set from the target domain Q.\n2. Set the desired confidence parameter δ ∈ (0, 1) and the radius\nε̂(δ) := Wp(µn, νm) +\n( log(4Ca/δ)\nCbn\n)p/d + ( log(4Ca/δ)\nCbm\n)p/d . (4.5)\n3. Compute the empirical risk minimizer\nf̂ = argmin f∈F Rε̂(δ),p(Pn, f), (4.6)\nwhere Pn is the empirical distribution of the n labeled samples from P .\nWe can give the following target domain generalization bound for the hypothesis generated according to (4.6):\nTheorem 4.1. Suppose that the feature space X is a bounded subset of Rd with d > 2p, take dX (x, x\n′) = ‖x− x′‖p, and let F be a family of hypotheses with Lipschitz constant at most L. Then with probability at least 1− δ, the empirical risk minimizer f̂ from (4.6) satisfies\nR(Q, f̂)−R(Q,F) ≤ 2Lε̂(δ) + C1√ n + C2√ n\n( 1 + ( diam(Z) ε̂(δ) )p) + 3M √ log(4/δ)√ 2n ,\nwhere\nC1 = 24\n∫ ∞\n0\n√ logN (F , ‖ · ‖∞, u/2)du and C2 = 24C0(2diam(Z))p.\nProof. For simplicity, we assume that there exists a hypothesis f∗ ∈ F that achieves R(Q,F). Then, for any ε > 0 such that Wp(P,Q) ≤ ε, Proposition 2.1 implies that\nR(Q, f̂)−R(Q, f∗) ≤ Rε,p(P, f̂)−Rε,p(P, f∗) + 2Lε ≤ Rε,p(P, f̂)−Rε,p(P,F) + 2Lε.\nFrom Theorem 3.1, we know that\nRε,p(P, f̂)−Rε,p(P,F) ≤ C1√ n + C2√ n\n( 1 + ( diam(Z)\nε\n)p) + 3M √\nlog(4/δ)√ 2n\nholds with probability at least 1−δ/2. Thus, it remains to find the right ε, such that thatWp(P,Q) ≤ ε holds with high probability. From Proposition 4.1, we see that each of the following two statements holds with probability at least 1− δ/4:\nWp(µn, µ) ≤ ( log(4Ca/δ)\nCbn\n)p/d , Wp(νm, ν) ≤ ( log(4Ca/δ)\nCbm\n)p/d .\nSince Wp(P,Q) = Wp(µ, ν) by Lemma 4.1, we see that Wp(P,Q) ≤ ε̂(δ) with probability at least 1−δ/2, where ε̂(δ) is given by Eq. (4.5). The claim of the theorem follows from the union bound."
    }, {
      "heading" : "A Proofs",
      "text" : "A.1 Proofs for Section 2\nProof of Proposition 2.1. For p = 1, the result follows immediately from the Kantorovich dual representation of W1(·, ·) [8]:\nW1(Q,Q ′) = sup    |EQF −EQ′F | : sup\nz,z′∈Z z 6=z′\n|F (z) − F (z′)| dZ(z, z′) ≤ 1   \nand from the fact that, for Q,Q′ ∈ BWε,1(P ), W1(Q,Q′) ≤ 2ε by the triangle inequality. For p > 1, the result follows from the fact that W1(Q,Q ′) ≤ Wp(Q,Q′) for all Q,Q′ ∈ Pp(Z).\nProof of Proposition 2.2. Fix some Q,Q′ ∈ BWε,p(P ) and let M ∈ P(Z × Z) achieve the infimum in (1.4) for Wp(Q,Q ′). Then for (Z,Z ′) ∼ M we have\nf(Z ′)− f(Z) ≤ ∫ 1\n0 Gf (γ(t))dt · dZ(Z,Z ′)\n≤ 1 2\n( Gf (Z) +Gf (Z ′) ) dZ(Z,Z ′),\nwhere the first inequality is from (2.1) and the second one is by the assumed geodesic convexity of Gf . Taking expectations of both sides with respect to M and using Hölder’s inequality, we obtain\nR(Q′, f)−R(Q, f) ≤ 1 2\n( EM ∣∣Gf (Z) +Gf (Z ′) ∣∣q )1/q ( EMd p Z(Z,Z ′) )1/p\n= 1\n2\n∥∥Gf (Z) +Gf (Z ′) ∥∥ Lq(M) Wp(Q,Q ′),\nwhere we have used the p-Wasserstein optimality of M for Q and Q′. By the triangle inequality, and since Z ∼ Q and Z ′ ∼ Q,\n‖Gf (Z) +Gf (Z ′)‖Lq(M) ≤ ‖Gf (Z)‖Lq(Q) + ‖Gf (Z)‖Lq(Q′) ≤ 2 sup\nQ∈BWε,p(P )\n‖Gf (Z)‖Lq(Q).\nInterchanging the roles of Q and Q′ and proceeding with the same argument, we obtain the estimate\nsup Q,Q′∈BWε,p(P ) |R(Q, f)−R(Q′, f)| ≤ 2ε sup Q∈BWε,p(P ) ‖Gf (Z)‖Lq(Q),\nfrom which it follows that\nR(Q, f) ≤ Rε,p(P, f) = sup\nQ′∈BWε,p(P )\n[R(Q′, f)−R(Q, f) +R(Q, f)]\n≤ R(Q, f) + 2ε sup Q∈BWε,p(P ) ‖Gf (Z)‖Lq(Q).\nProof of Proposition 2.3. As a subset of Rd+1, Z is a geodesic space: for any pair z, z′ ∈ Z there is a unique constant-speed geodesic γ(t) = (1 − t)z + tz′. We claim that Gf (z) = Gf (x, y) = 2(B+M)(1+L‖∇f0(x)‖2) is a geodesically convex upper gradient for f(z) = f(x, y) = (y−f0(x))2. In this flat Euclidean setting, geodesic convexity coincides with the usual definition of convexity, and the map z 7→ Gf (z) is evidently convex:\nGf ((1 − t)z + tz′) ≤ (1− t)Gf (z) + tGf (z′).\nNext, by the mean-value theorem,\nf(z′)− f(z) = ∫ 1\n0 〈z′ − z,∇f((1− t)z + t′z)〉dt\n≤ ∫ 1\n0 ‖∇f((1− t)z + tz′)‖2 dt · ‖z − z′‖2\n=\n∫ 1\n0 ‖∇f((1− t)z + tz′)‖2 dt · dZ(z, z′),\nand a simple calculation shows that\n‖∇f(z)‖22 = ‖∇f(x, y)‖22 = 4f(z) ( 1 + ‖∇f0(z)‖22 )\n≤ 4(B +M)2(1 + L2‖x‖22).\nTherefore, ‖∇f(z)‖2 ≤ Gf (z) for z = (x, y), as claimed. Thus, by Proposition (2.2),\nR(Q, f) ≤ R∗ε,2(P, f) ≤ R(Q, f) + 2 sup\nQ∈BWε,2(P )\n‖Gf (Z)‖L2(Q)ε\n= R(Q, f) + 4(B +M) ( 1 + L sup\nQ∈BWε,2(P )\nEQ‖X‖2 ) ε\n= R(Q, f) + 4(B +M) ( 1 + L sup\nQ∈BWε,2(P )\nσQ,X\n) ε.\nA.2 Proofs for Section 3\nProof of Corollary 3.1. We first verify the regularity assumptions. Assumption 3.1 is evidently satisfied since diam(Z)2 = diam(X )2 + diam(Y)2 ≤ 4r20 + 4B2. Each f ∈ F is continuous, and Assumption 3.2 holds with M = 2(‖s‖2∞ +B2). To verify Assumption 3.3, take f0 = 0 and pick an arbitrary x0 ∈ X . Then\nf(x, y) = (y − s(0))2 = y2 ≤ ‖x− x0‖2 + y2 = d2Z(z, z0)\nwith z0 = (x0, 0). Thus, Assumption 3.3 holds with C0 = 1 and z0 = 0.\nTo evaluate the Dudley entropy integral in (3.1), we need to estimate the covering numbers N (F , ‖ · ‖∞, ·). First observe that, for any two f, g ∈ F corresponding to f0, g0 ∈ Rd, we have\nsup x∈X sup y∈Y |f(x, y)− g(x, y)| = sup x∈X sup y∈Y\n∣∣∣∣ ( y − s(fT0 x) )2 − ( y − s(gT0 x) )2∣∣∣∣\n≤ 2B sup x∈X |s(fT0 x)− s(gT0 x)|+ sup x∈X |s2(fT0 x)− s2(gT0 x)| ≤ 2r0 ( B + ‖s‖∞\n) ‖s′‖∞︸ ︷︷ ︸\n:=D\n‖f0 − g0‖2.\nSince f0, g0 belong to the unit ball in R d,\nN (F , ‖ · ‖∞, u/2) ≤ ( 6D\nu\n)d\nfor 0 < u < 2D, and N (F , ‖ · ‖∞, u/2) = 1 for u ≥ 2D, which gives ∫ ∞\n0\n√ logN (F , ‖ · ‖∞, u/2)du ≤\n∫ 2D\n0\n√ d log ( 6D\nu\n) du\n= 6D √ d\n∫ 1/3\n0\n√ log ( 1\nu\n) du\n= 6cD √ d,\nwhere c = 16 ( 2 √ log 3 + 3 √ πerfc( √ log 3) ) < 1. Substituting this into the bound (3.1), we get the desired estimate.\nProof of Corollary 3.2. We start by presenting the following technical lemma.\nLemma A.1. For any f, g ∈ F we have: ‖f‖∞ ≤ 2(r2 +B2) ‖f − g‖∞ ≤ 2(r +B)‖f0 − g0‖X , where f0, g0 ∈ IK(Br) are the predictors satisfying f(x, y) = (y− f0(x))2 and g(x, y) = (y− g0(x))2. Proof. First note that for Gaussian kernels, supx∈X √ K(x, x) ≤ 1 by definition. Then, we have the first claim by\n|f(z)| = (f0(x)− y)2 ≤ 2f20 (x) + 2y2 ≤ 2r2 + 2B2, where the first inequality is due to convexity of square function and the second inequality is due to the reproducing kernel property of K and the Cauchy-Schwarz inequality in HK . Second claim is established similarly:\n∣∣f(z)− g(z) ∣∣ = ∣∣(f0(x)− y)2 − (g0(x)− y)2 ∣∣\n= ∣∣f0(x) + g0(x)− 2y ∣∣∣∣f0(x)− g0(x) ∣∣ ≤ ( 2 sup ‖h‖K≤r |h(x)| + 2|y| )∣∣f0(x)− g0(x) ∣∣\n≤ 2(r +B)‖f0 − g0‖X , where the last inequality is due to Cauchy-Schwarz inequality again.\nWe now return to the proof of Corollary 3.2. Assumption 3.1 holds since diam(Z)2 = diam(X )2+ diam(Y)2 ≤ 4r20 + 4B2. The functions in F are continuous, and Assumption 3.2 holds with M = 2(r2+B2) by virtue of the first estimate of Lemma A.1. To verify Assumption 3.3, take f0 = 0 and pick an arbitrary x0 ∈ X . Then\nf(x, y) = y2 ≤ ‖x− x0‖2 + y2 = d2Z(z, z0)\nwith z0 = (x0, 0). Thus, Assumption 3.3 holds with C0 = 1 and z0 = 0. Now we proceed to upper-bound the Dudley entropy integral for F :\n∫ ∞\n0\n√ logN ( F , ‖ · ‖∞, u\n2\n) du\n≤ ∫ ∞\n0\n√ logN ( IK(Br), ‖ · ‖X ,\nu\n4(r +B)\n) du\n=\n∫ 4(r+B)r\n0\n√ logN ( IK(Br), ‖ · ‖X ,\nu\n4(r +B)\n) du\n≤ ∫ 2(r2+Br)\n0\n√ logN ( IK(Br), ‖ · ‖X ,\nu\n4(r +B)\n) du\n︸ ︷︷ ︸ :=T1\n+\n∫ 4(r2+Br)\n2(r2+Br)\n√ logN ( IK(Br), ‖ · ‖X , r\n2\n) du\n︸ ︷︷ ︸ :=T2\nwhere we used the second claim of Lemma A.1 for the first inequality and the monotonicity of covering numbers for the second inequality. Plugging in the estimate from Proposition 3.2,\nT1 ≤ √ d ( 32 +\n2560dr20 σ2\n) d+1 2 ∫ 2(r2+Br)\n0\n( log 4(r2 +Br)\nu\n) d+1 2\ndu\n= 4 √ d ( 32 +\n2560dr20 σ2\n) d+1 2\n(r2 +Br)\n∫ +∞\nlog 2 e−uu\nd+1 2 du\n= 4 √ d ( 32 +\n2560dr20 σ2\n) d+1 2\n(r2 +Br)Γ\n( d+ 3\n2 , log 2\n) .\nT2 ≤ 2(r2 +Br) · √ d ( 32 +\n2560dr20 σ2\n) d+1 2\n(log 2) d+1 2 ,\nand hence T1 + T2 ≤ C148 (r2 +Br).\nProof of Lemma 3.1. Since ϕλ,f ≥ 0 for all λ, f , we arrive at\nλ̃ ≤ Rε,p(Q,F) εp . (A.1)\nWe proceed to upper-bound the local minimax risk Rε,p(Q,F):\nRε,p(Q,F) = inf f∈F min λ≥0\n{ λεp + ∫\nZ sup z′∈Z\n[ f(z′)− λdpZ(z, z′) ] Q(dz′)\n}\n≤ min λ≥0\n{ λεp + ∫\nZ sup z′∈Z\n[ f0(z ′)− λdpZ(z, z′) ] Q(dz′)\n}\n≤ min λ≥0\n{ λεp + ∫\nZ sup z′∈Z\n[ C0d p Z(z ′, z0)− λdpZ(z, z′) ] Q(dz′) } .\nFor λ ≥ C02p−1, the integrand can be upper-bounded as follows:\nsup z′∈Z\n[ C0d p Z(z ′, z0)− λdpZ(z, z′) ] ≤ sup\nz′∈Z\n[ C02 p−1dpZ(z, z0) + (C02 p−1 − λ)dpZ(z, z′) ]\n≤ C02p−1dpZ(z, z0).\nTherefore,\nRε,p(Q,F) ≤ min λ≥C02p−1\n{ λεp +C02 p−1 ∫\nZ dpZ(z, z0)Q(dz)\n}\n≤ C02p−1 ( εp + (diam(Z))p ) .\nSubstituting this estimate into (A.1), we obtain (3.6)\nProof of Lemma 3.2. Define the Φ-indexed process X = (Xϕ)ϕ∈Φ via\nXϕ := 1√ n\nn∑\ni=1\nσiϕ(Zi),\nwhich is clearly zero-mean: E[Xϕ] = 0 for all ϕ ∈ Φ. To upper-bound the Rademacher average Rn(Φ), we first show that X is a subgaussian process with respect to a suitable pseudometric. For ϕ = ϕλ,f and ϕ ′ = ϕλ′,f ′ , define\ndΦ(ϕ,ϕ ′) := ‖f − f ′‖∞ + (diam(Z))p|λ− λ′|,\nand it is not hard to show that ‖ϕ − ϕ′‖∞ ≤ dΦ(ϕ,ϕ′). Then, for any t ∈ R, using Hoeffding’s lemma and the fact that (σi, Zi) are i.i.d., we arrive at\nE [ exp(t(Xϕ −Xϕ′)) ] = E  exp   t√\nn\nn∑\ni=1\nσi(ϕ(Zi)− ϕ′(Zi))\n   \n=  E [ exp\n( t√ n σ1 ( ϕ(Z1)− ϕ′(Z1)\n)) ]  n\n≤ exp ( t2d2Φ(ϕ,ϕ ′)\n2\n) .\nHence, X is subgaussian with respect to dΦ, and therefore the Rademacher average Rn(Φ) can be upper-bounded by the Dudley entropy integral [12]:\nRn(Φ) ≤ 12√ n\n∫ ∞\n0\n√ logN (Φ, dΦ, u)du,\nwhere N (Φ, dΦ, ·) are the covering numbers of (Φ, dΦ). From the definition of dΦ, it follows that\nN (Φ, dΦ, u) ≤ N (F , ‖ · ‖∞, u/2) · N (Λ, | · |, u/2(diam(Z))p),\nand therefore\nRn(Φ) ≤ 12√ n\n(∫ ∞\n0\n√ logN (F , ‖ · ‖∞, u/2)du+\n∫ ∞\n0\n√ logN (Λ, | · |, u/2(diam(Z))p)du ) .\nSince Λ is a compact interval, it is straightforward to upper-bound the second integral:\n∫ ∞\n0\n√ logN (Λ, | · |, u/2(diam(Z))p)du ≤ 2|Λ|(diam(Z))p\n∫ 1/2\n0\n√ log(1/u)du\n= 2c|Λ|(diam(Z))p,\nwhere |Λ| = C02p−1(1 + (diam(Z)/ε)p) is the length of Λ and c = 12 (√ log 2 + √ πerfc( √ log 2) ) < 1. Consequently,\nRn(Φ) ≤ 12√ n\n(∫ ∞\n0\n√ logN (F , ‖ · ‖∞, u/2)du+ 2|Λ|(diam(Z))p\n)\n≤ 12√ n\n∫ ∞\n0\n√ logN (F , ‖ · ‖∞, u/2)du+ 12C0(2 diam(Z))p√ n\n( 1 + ( diam(Z)\nε\n)p) .\nA.3 Proofs for Section 4\nProof of Lemma 4.1. First we prove that Wp(P,Q) ≤ Wp(µ, ν). Define the mapping T̃ : Z → Z by T̃ := T ⊗ idZ , i.e., T̃ (z) = T̃ (x, y) = (T (x), y), and let Q̃ = T̃#P , the pushforward of P by T̃ . We claim that Q̃ ≡ Q. Indeed, for any measurable sets A ⊆ X and B ⊆ Y,\nQ̃(A×B) = T̃#P (A×B) = P (T−1(A) ×B)\n=\n∫\nT−1(A) µ(dx)PY |X(B|x)\n=\n∫\nA T#µ(dx)PY |X(B|T (x))\n=\n∫\nA ν(dx)QY |X(B|x),\nwhere we have used the relation (4.1) and the invertibility of T . Thus,\nW pp (P,Q) ≤ EP [dpZ(Z, T̃ (Z)))] = EP [d p X (X,T (X))] = W p p (µ, ν).\nFor the reverse inequality, let M ∈ P(Z × Z) be the optimal coupling of P and Q. Then, for Z = (X,Y ) and Z ′ = (X ′, Y ′) with (Z,Z ′) ∼ M , the marginal MXX′ is evidently a coupling of the marginals µ and ν, and therefore\nW pp (P,Q) = EM [d p Z(Z,Z ′)]\n= EM [d p X (X,X ′)] +EM [d p Y(Y, Y ′)] ≥ EM [dpX (X,X ′)] ≥ W pp (µ, ν)."
    } ],
    "references" : [ {
      "title" : "Statistical Learning Theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : "Wiley",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems",
      "author" : [ "V. Koltchinskii" ],
      "venue" : "volume 2033 of Lecture Notes in Mathematics. Springer",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A minimax approach to supervised learning",
      "author" : [ "F. Farnia", "D. Tse" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Statistics of robust optimization: a generalized empirical likelihood approach",
      "author" : [ "J.C. Duchi", "P.W. Glynn", "H. Namkoong" ],
      "venue" : "arXiv preprint 1610.03425",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations",
      "author" : [ "P. Mohajerin Esfahani", "D. Kuhn" ],
      "venue" : "arXiv preprint 1505.05116",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Distributionally robust logistic regression",
      "author" : [ "S. Shafieezadeh-Abadeh", "P. Mohajerin Esfahani", "D. Kuhn" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Distributionally robust stochastic optimization with Wasserstein distance",
      "author" : [ "R. Gao", "A.J. Kleywegt" ],
      "venue" : "arXiv preprint 1604.02199",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Topics in optimal transportation",
      "author" : [ "C. Villani" ],
      "venue" : "American Mathematics Society, Providence, RI",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A theory of learning from different domains",
      "author" : [ "S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan" ],
      "venue" : "Machine Learning, 79:151–175",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Optimal transport for domain adaptation",
      "author" : [ "N. Courty", "R. Flamary", "D. Tuia", "A. Rakotomamonjy" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Gradient Flows in Metric Spaces and in the Space of Probability Measures",
      "author" : [ "L. Ambrosio", "N. Gigli", "G. Savaré" ],
      "venue" : "Birkhäuser",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Upper and Lower Bounds for Stochastic Processes: Modern Methods and Classical Problems",
      "author" : [ "M. Talagrand" ],
      "venue" : "Springer",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning theory: an approximation theory viewpoint",
      "author" : [ "F. Cucker", "D. Zhou" ],
      "venue" : "Cambridge University Press, Cambridge, MA",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Covering numbers of gaussian reproducing kernel hilbert spaces",
      "author" : [ "T. Kühn" ],
      "venue" : "Journal of Complexity, pages 489–499",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On the rate of convergence in wasserstein distance of the empirical measure",
      "author" : [ "N. Fournier", "A. Guillin" ],
      "venue" : "Probability Theory and Related Fields, pages 1–32",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Introduction and problem set-up In the traditional paradigm of statistical learning [1], we have a class P of probability measures on a measurable instance space Z and a class F of measurable functions f : Z → R+.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "Under suitable regularity assumptions, this objective can be accomplished via Empirical Risk Minimization (ERM) [1, 2]:",
      "startOffset" : 112,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "Under suitable regularity assumptions, this objective can be accomplished via Empirical Risk Minimization (ERM) [1, 2]:",
      "startOffset" : 112,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].",
      "startOffset" : 77,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].",
      "startOffset" : 77,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].",
      "startOffset" : 77,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "However, with the exception of the recent work by Farnia and Tse [3], the minimizer of (1.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "(Recently, this modification was also proposed by Farnia and Tse [3] for ambiguity sets defined by a finite number of moment constraints.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "The metric structure of Z can be used to define a family of metrics on the spaces Pp(Z) [8]: Definition 1.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "Following [5, 6, 7], we let the ambiguity set A(P ) be the p-Wasserstein ball of radius ε ≥ 0 centered at P : A(P ) = B ε,p(P ) := { Q ∈ Pp(Z) : Wp(P,Q) ≤ ε } ,",
      "startOffset" : 10,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "Following [5, 6, 7], we let the ambiguity set A(P ) be the p-Wasserstein ball of radius ε ≥ 0 centered at P : A(P ) = B ε,p(P ) := { Q ∈ Pp(Z) : Wp(P,Q) ≤ ε } ,",
      "startOffset" : 10,
      "endOffset" : 19
    }, {
      "referenceID" : 6,
      "context" : "Following [5, 6, 7], we let the ambiguity set A(P ) be the p-Wasserstein ball of radius ε ≥ 0 centered at P : A(P ) = B ε,p(P ) := { Q ∈ Pp(Z) : Wp(P,Q) ≤ ε } ,",
      "startOffset" : 10,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "In contrast to the standard statistical learning framework, where the risk of the learned hypothesis is evaluated on the same unknown distribution that was used for generating the training examples, the problem of domain adaptation [9] arises when the training data are generated according to an unknown distribution P , but the learned hypothesis is evaluated on another unknown distribution Q.",
      "startOffset" : 232,
      "endOffset" : 235
    }, {
      "referenceID" : 9,
      "context" : "[10] have introduced an algorithmic framework for domain adaptation based on optimal transport.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[10], our approach completely bypasses the problem of estimating the transport map T .",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "The first one is a simple consequence of the Kantorovich duality theorem from the theory of optimal transport [8]: Proposition 2.",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 10,
      "context" : "Since we are working with general metric spaces that may lack an obvious differentiable structure, we need to first introduce some concepts from metric geometry [11].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "A metric space (Z,dZ) is a geodesic space if for every two points z, z′ ∈ Z there exists a path γ : [0, 1] → Z, such that γ(0) = z, γ(1) = z′, and dZ(γ(s), γ(t)) = (t− s) · dZ(γ(0), γ(1)) for all 0 ≤ s ≤ t ≤ 1 (such a path is called a constant-speed",
      "startOffset" : 100,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "A functional F : Z → R is geodesically convex if for any pair of points z, z′ ∈ Z there is a constant-speed geodesic γ, so that F (γ(t)) ≤ (1− t)F (γ(0)) + tF (γ(1)) = (1− t)F (z) + tF (z), ∀t ∈ [0, 1].",
      "startOffset" : 195,
      "endOffset" : 201
    }, {
      "referenceID" : 6,
      "context" : "The following strong duality result due to Gao and Kleywegt [7] will be instrumental: Proposition 3.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 11,
      "context" : "1) is the Dudley entropy integral [12].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "The excess risk bound of Farnia and Tse [3] has the same behavior, where in that case ε is the slack in the moment constraints defining the ambiguity set.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "1] (which was later shown by Kühn [14] to be asymptotically exact up to the double logarithmic factor): Proposition 3.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "2, the problem of domain adaptation arises when we want to transfer the data or knowledge from a source domain P ∈ P(Z) to a different but related target domain Q ∈ P(Z) [9].",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : "[10]: Let Z = X × Y, where (X , dX ) is the feature space and (Y, dY ) is the label space.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "invertible – in fact, its inverse is equal to the optimal transport map from ν to μ [8].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : "Here, Wp(μn, νm) can be computed from unlabeled data by solving a finite-dimensional linear program [8], and the following convergence result of Fournier and Guillin [15] implies that, with high probability, both Wp(μ, μn) and Wp(ν, νm) rapidly converge to zero as n,m → ∞:",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 14,
      "context" : "Here, Wp(μn, νm) can be computed from unlabeled data by solving a finite-dimensional linear program [8], and the following convergence result of Fournier and Guillin [15] implies that, with high probability, both Wp(μ, μn) and Wp(ν, νm) rapidly converge to zero as n,m → ∞:",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 7,
      "context" : "For p = 1, the result follows immediately from the Kantorovich dual representation of W1(·, ·) [8]: W1(Q,Q ) = sup    |EQF −EQ′F | : sup z,z′∈Z z 6=z′ |F (z) − F (z′)| dZ(z, z′) ≤ 1    and from the fact that, for Q,Q′ ∈ BW ε,1(P ), W1(Q,Q) ≤ 2ε by the triangle inequality.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : "Hence, X is subgaussian with respect to dΦ, and therefore the Rademacher average Rn(Φ) can be upper-bounded by the Dudley entropy integral [12]: Rn(Φ) ≤ 12 √ n ∫ ∞",
      "startOffset" : 139,
      "endOffset" : 143
    } ],
    "year" : 2017,
    "abstractText" : "As opposed to standard empirical risk minimization (ERM), distributionally robust optimization aims to minimize the worst-case risk over a larger ambiguity set containing the original empirical distribution of the training data. In this work, we describe a minimax framework for statistical learning with ambiguity sets given by balls in Wasserstein space. In particular, we prove a generalization bound that involves the covering number properties of the original ERM problem. As an illustrative example, we provide generalization guarantees for domain adaptation problems where the Wasserstein distance between the source and target domain distributions can be reliably estimated from unlabeled samples.",
    "creator" : "LaTeX with hyperref package"
  }
}