{
  "name" : "1508.03398.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "End-to-end Learning of Latent Dirichlet Allocation by Mirror-Descent Back Propagation",
    "authors" : [ "Jianshu Chen", "Ji He", "Yelong Shen", "Lin Xiao", "Xiaodong He", "Jianfeng Gao", "Xinying Song", "Li Deng" ],
    "emails" : [ "jianshuc@microsoft.com", "yeshen@microsoft.com", "lin.xiao@microsoft.com", "xiaohe@microsoft.com", "jfgao@microsoft.com", "xinson@microsoft.com", "deng@microsoft.com", "jvking@uw.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Latent Dirichlet Allocation (LDA) [4], among various forms of topic models, is an important probabilistic generative model for analyzing large collections of text corpora. In LDA, each document is modeled as a collection of words, where each word is assumed to be generated from a certain topic drawn from a topic distribution. The topic distribution can be viewed as a latent representation of the document, which can be used as a feature for prediction purpose (e.g., sentiment analysis). In particular, the inferred topic distribution is fed into a separate classifier or regression model (e.g., logistic regression or linear regression) to perform prediction. Such a separate learning structure usually significantly restricts the performance of the algorithm. For this purpose, various supervised topic models have been proposed to model the documents jointly with the label information. In [3], variational methods was applied to learn a supervised LDA (sLDA) model by maximizing the lower bound of the joint probability of the input data and the labels. The DiscLDA method developed in [11] learns the transformation matrix from the latent topic representation to the output in a discriminative manner, while learning the topic to word distribution in a generative manner similar to the standard LDA. In [21, 22], max margin supervised topic models are developed for classification and regression, which are trained by optimizing the sum of the variational bound for the log marginal likelihood and an additional term that characterizes the prediction margin. These methods successfully incorporate the information from both the input data and the labels, and showed better performance in prediction compared to the vanilla LDA model.\nOne challenge in LDA is that the exact inference is intractable, i.e., the posterior distribution of the topics given the input document cannot be evaluated explicitly. For this reason, various approximate inference methods are proposed, such as variational learning [3,4,21,22] and Gibbs sampling [7,23], for computing the approximate posterior distribution of the topics. In this paper, we will show that,\nar X\niv :1\n50 8.\n03 39\n8v 1\n[ cs\n.L G\n] 1\n4 A\nalthough the full posterior probability of the topic distribution is difficult, its maximum a posteriori (MAP) inference, as a simplified problem, is a convex optimization problem when the Dirichlet parameter satisfies certain conditions, which can be solved efficiently by the mirror descent algorithm (MDA) [1,15,18]. Indeed, Sontag and Roy [16] pointed out that the MAP inference problem of LDA in this situation is polynomial-time and can be solved by an exponentiated gradient method, which shares a same form as our mirror-descent algorithm with constant step-size. Nevertheless, different from [16], which studied the inference problem alone, our focus in this paper is to integrate back propagation with mirror-descent algorithm to perform fully discriminative training of supervised topic models, as we proceed to explain below.\nAmong the aforementioned methods, one training objective of the supervised LDA model is to maximize the joint likelihood of the input and the output variables [3]. Another variant is to maximize the sum of the log likelihood (or its variable bound) and a prediction margin [21–23]. Moreover, the DiscLDA optimizes part of the model parameters by maximizing the marginal likelihood of the input variables, and optimizes the other part of the model parameters by maximizing the conditional likelihood. For this reason, DiscLDA is not a fully discriminative training of all the model parameters. In this paper, we propose a fully discriminative training of all the model parameters by maximizing the posterior probability of the output given the input document. We will show that the discriminative training can be performed in a principled manner by naturally integrating the back-propagation with the MDA-based exact MAP inference. Discriminative training of generative model is widely used and usually outperforms standard generative training in prediction tasks [2, 6, 9, 10, 12, 20]. As pointed out in [2,12], discriminative training increases the robustness against the mismatch between the generative model and the real data. To our best knowledge, this paper is the first work to perform a fully end-to-end discriminative training of LDA. Experimental results on two real-world tasks also show the superior performance of discriminative training of LDA models on text analysis.\nIn addition to the aforementioned related studies on unsupervised and supervised LDA models [3,11, 21–23], there have been another stream of work that applied empirical risk minimization to graphical models such as Markov Random Field (MRF) and nonnegative matrix factorization (NMF) [8, 17]. Specifically, in [17], an approximate inference algorithm, belief propagation, is used to compute the belief of the output variables, which is further fed into a decoder to produce the prediction. The approximate inference and the decoder are treated as an entire black-box decision rule, which is tuned jointly via back propagation. Our work is different from the above studies in that we use an exact MAP inference based on convex optimization theory motivate the discriminative training from a principled probabilistic framework."
    }, {
      "heading" : "2 Smoothed Supervised LDA Model",
      "text" : "We consider the smoothed supervised LDA model in Figure 1. Let K be the number of topics, N be the number of words in each document, V be the vocabulary size, and D be the number of documents in the corpus. The generative process of the model in Figure 1 can be described as:\n1. For each document d, choose the topic proportions according to a Dirichlet distribution: θd ∼ p(θd|α) = Dir(α), where α is a K× 1 vector consisting of nonnegative components.\n2. Draw each column φk of a V ×K matrix Φ independently from an exchangeable Dirichlet distribution: φk ∼ Dir(β) (i.e., Φ ∼ p(Φ|β)), where β > 0 is the smoothing parameter.\n3. To generate each word wd,n:\n(a) Choose a topic zd,n ∼ p(zd,n|θd) = Multinomial(θd). 1 (b) Choose a word wd,n ∼ p(wd,n|zd,n,Φ) = Multinomial(φzd,n).\n4. Choose the C × 1 response vector: yd ∼ p(yd|θ, U, γ). (a) In regression, p(yd|θd, U, γ) = N(Uθd, γ−1), where U is a C ×K matrix consisting\nof regression coefficients. (b) In multi-class classification, p(yd|θd, U, γ) = Multinomial ( σ(γUθd) ) , where σ :\nRC → RC is a softmax function defined as σ(x)c = e xc∑C\nc′=1 e x c′\n, c = 1, . . . , C.\nTherefore, the entire model can be described by the following joint probability\np(Φ|β) D∏ d=1 [ p(yd|θd, U, γ) · p(θd|α) · p(wd,1:N |zd,1:N ,Φ) · p(zd,1:N |θd)︸ ︷︷ ︸\n,p(yd,θd,wd,1:N ,zd,1:N |Φ,U,α,γ)\n] (1)\nwhere wd,1:N and zd,1:N denotes all the words and the associated topics, respectively, in the d-th document. Note that the model in Figure 1 is slightly different from the one proposed in [3], where, in addition to the Dirichlet smoothing part on φk, the response variable yd in Figure 1 is coupled with θd instead of zd,1:N as in [3]. Blei and Mcauliffe also pointed out this choice as an alternative in [3]. We will show that this modification will enable us to develop an end-to-end discriminative training with superior prediction performance.\nTo develop a fully discriminative training method for the model parameters Φ and U , we follow the argument in [2,12], which states that the discriminative training is also equivalent to maximizing the joint likelihood of a new model family with an additional set of parameters:\narg max Φ,U,Φ̃ p(Φ|β)p(Φ̃|β) D∏ d=1 p(yd|wd,1:N ,Φ, U, α, γ) D∏ d=1 p(wd,1:N |Φ̃, α) (2)\nwhere p(wd,1:N |Φ̃, α) is obtained by marginalizing p(yd, θd, wd,1:N , zd,1:N |Φ, U, α, γ) in (1) and replace Φ with Φ̃. The above problem (2) decouples into\narg max Φ,U\n[ ln p(Φ|β) + D∑ d=1 ln p(yd|wd,1:N ,Φ, U, α, γ) ]\n(3)\narg max Φ̃\n[ ln p(Φ̃|β) + D∑ d=1 ln p(wd,1:N |Φ̃, α) ]\n(4)\nwhich are the discriminative learning problem of supervised LDA (Eq. (3)), and the unsupervised learning problem of LDA (Eq. (4)), respectively. It was pointed out in [2] that the discriminative training (3) improves performance by compensating for model mismatch, i.e., the differences between the true distribution of the data and the distribution specified by the model. We will show that both problems can be solved in a unified manner using a new MAP inference and back propagation."
    }, {
      "heading" : "3 Exact MAP Inference",
      "text" : "We first consider the inference problem in the smoothed LDA model. For the supervised case, the main objective is to infer yd given the words wd,1:N in each document d, i.e., computing\np(yd|wd,1:N ,Φ, U, α, γ) = ∫ θd p(yd|θd, U, γ)p(θd|wd,1:N ,Φ, α)dθd (5)\nwhere the probability p(yd|θd, U, γ) is known (e.g., multinomial or Gaussian for classification and regression problems — see Section 2). The main challenge is to evaluate p(θd|wd,1:N ,Φ, α), i.e., infer the topic proportion given each document, which is also the important inference problem in\n1We will represent all the multinomial variables by a one-hot vector that has a single component equal to one and all other components being zero, where the position of the one is determined by the value of the multinomial variable.\nthe unsupervised LDA model. However, it is well known that the exact evaluation of the posterior probability p(θd|wd,1:N ,Φ, α) is intractable [3,4,7,11,21–23]. For this reason, various approximate inference methods, such as variational inference [3, 4, 11, 21, 22] and Gibbs sampling [7, 23], have been proposed to compute the approximate posterior probability. In this paper, we take an alternative approach for inference; given each document d, we only seek a point (maximum a posterior) estimate of θd, instead of its full (approximate) posterior probability. The major motivation is that, although the full posterior probability of θd is difficult, its MAP inference, as a simplified problem, is a convex optimization problem (Section 3.1) and is thus tractable. Furthermore, having the MAP estimate of θd, we can efficiently infer the prediction variable yd according to the following approximation of p(yd|wd,1:N ,Φ, U, α, γ) from (5):\np(yd|wd,1:N ,Φ, U, α, γ) = Eθd|wd,1:N [p(yd|θd, U, γ)] ≈ p(yd|θ̂d|wd,1:N , U, γ) (6) where Eθd|wd,1:N [·] denotes the conditional expectation with respect to θd given wd,1:N , and the expectation is sampled by the MAP estimate, θ̂d|wd,1:N , of θd given wd,1:N , defined as\nθ̂d|wd,1:N = arg max θd\np(θd|wd,1:N ,Φ, α, β) (7)\nThe approximation gets more precise when p(θd|wd,1:N ,Φ, α, β) becomes more concentrated around θ̂d|wd,1;N . Experimental results on several real datasets (Section 5) show that the approximation (6) provides excellent prediction performance."
    }, {
      "heading" : "3.1 MAP Inference as a Convex Optimization Problem",
      "text" : "Using the Bayesian rule p(θd|wd,1:N ,Φ, α) = p(θd|α)p(wd,1:N |θd,Φ)/p(wd,1:N |Φ, α) and the fact that p(wd,1:N |Φ, α) is independent of θd, we obtain the equivalent form of (7) as\nθ̂d|wd,1:N = arg max θd∈PK\n[ ln p(θd|α) + ln p(wd,1:N |θd,Φ) ] (8)\nwhere PK = {θ ∈ RK : θj ≥ 0, ∑K j=1 θj = 1} denotes the (K − 1)-dimensional probability simplex vector, p(θd|α) is the Dirichlet distribution described earlier, and p(wd,1:N |θd,Φ) can be computed by integrating p(wd,1:N , zd,1:N |θd,Φ) = ∏N n=1 p(wd,n|zd,n,Φ)p(zd,n|θd) over zd,1:N , which leads to (derived in Appendix A)\np(wd,1:N |θd,Φ) = V∏ v=1 ( K∑ j=1 θd,jΦvj )xd,v = p(xd|θd,Φ) (9)\nwhere xd,v denotes the term frequency of the v-th word (in vocabulary) inside the d-th document, and xd denotes the V -dimensional bag-of-words (BoW) vector of the d-th document. Note that p(wd,1:N |θd,Φ) depends on wd,1:N only via the BoW vector xd, which is the sufficient statistics. Therefore, we use p(xd|θd,Φ) and p(wd,1:N |θd,Φ) interchangeably from now on. Substituting the expression of Dirichlet distribution and (9) into (8), we get\nθ̂d|wd,1:N = arg max θd∈PK\n[ xTd ln(Φθd) + (α− 1)T ln θd ] = arg min\nθd∈PK\n[ − xTd ln(Φθd)− (α− 1)T ln θd ] (10)\nwhere we dropped the terms independent of θd, and 1 denotes an all-one vector. Note that when each element of α is greater than or equal to one, the objective function in (10) is convex and the problem is convex. When α is strictly greater than one, the objective function is strictly convex and has a unique solution. In this paper, we will only focus on the regime of α being greater than one."
    }, {
      "heading" : "3.2 Mirror Descent Algorithm for MAP Inference",
      "text" : "An efficient approach to solving the constrained optimization problem (10) is the mirror descent algorithm (MDA) with Bregman divergence chosen to be generalized Kullback-Leibler divergence [1, 15, 18]. Specifically, let f(θd) denote the cost function in (10), then the MDA updates the MAP estimate of θd iteratively according to:\nθd,` = arg min θd∈PK\n[ f(θd,`−1) + [∇θdf(θd,`−1)]T (θd − θd,`−1) + 1\nTd,` Ψ(θd, θd,`−1)\n] (11)\nθd,` denotes the estimate of θd,` at the `-th iteration, Td,` denotes the step-size of MDA, and Ψ(x, y) is the Bregman divergence chosen to be Ψ(x, y) = xT ln(x/y) − 1Tx + 1T y. The argmin in (11) can be solved in closed-form (see Appendix B) as\nθd,` = 1\nCθ · θd,`−1 exp\n( Td,` [ ΦT\nxd Φθd,`−1 + α− 1 θd,`−1\n]) , ` = 1, . . . , L, θd,0 = 1\nK 1 (12)\nwhere Cθ is a normalization factor such that θd,` adds up to one, denotes Hadamard product, L is the number of MDA iterations, and the divisions in (12) are element-wise operations. Note that the recursion (12) naturally enforces each θd,` to be on the probability simplex. The MDA step-size Td,` can be either constant, i.e., Td,` = T , or adaptive over iterations and samples, determined by line search (see Appendix C). The computation complexity in (12) is low since most computations are sparse matrix operations. For example, although by itself Φθd,`−1 in (12) is a dense matrix multiplication, we only need to evaluate the elements of Φθd,`−1 at the positions where the corresponding elements of xd are nonzero, because all other elements of xd/Φθd,`−1 is known to be zero. Overall, the computation complexity in each iteration of (12) is O(nTok ·K), where nTok denotes the number of unique tokens in the document. In practice, we only use a small number of iterations, L, in (12) and use θd,L to approximate θ̂d|wd,1:N so that (6) becomes\np(yd|wd,1:N ,Φ, U, α, γ) ≈ p(yd|θd,L, U, γ) (13) In summary, the inference of θd and yd can be implemented by the layered architecture in Figure 2, where the top layer infers yd using (13) and the MDA layers infer θd iteratively using (12). Figure 2 also implies that the the MDA layers act as a feature extractor by generating the MAP estimate θd,L for the output layer. Our end-to-end learning strategy developed in the next section jointly learns the model parameter U at the output layer and the model parameter Φ at the feature extractor layers to maximize the posterior of the prediction variable given the input document."
    }, {
      "heading" : "4 Learning by Mirror-Descent Back Propagation",
      "text" : "We now consider the supervised learning problem (3) and the unsupervised learning problem (4), respectively, using the developed MDA-based MAP inference. We first consider the supervised learning problem. With (13), the discriminative learning problem (3) can be approximated by\narg min Φ,U\n[ − ln p(Φ|β)−\nD∑ d=1\nln p(yd|θd,L, U, γ) ]\n(14)\nwhich can be solved by stochastic gradient descent (SGD). Note that the cost function in (14) depends on U explicitly through p(yd|θd,L, U, γ), which can be computed directly from its definition in Sec. 2. On the other hand, the cost function in (14) depends on Φ implicitly through θd,L. From Figure 2, we observe that θd,L not only depends on Φ explicitly (as indicated in the MDA block on the right-hand side of Figure 2) but also depends on Φ implicitly via θd,L−1, which in turn depends on Φ both explicitly and implicitly (through θd,L−2) and so on. That is, the dependency of\nthe cost function on Φ is in a layered manner. Therefore, we devise a back propagation procedure to efficiently compute its gradient with respect to Φ according to the mirror-descent graph in Figure 2, which back propagate the error signal through the MDA blocks at different layers. The gradient formula and the implementation details of the learning algorithm can be found in Appendices C–D.\nFor the unsupervised learning problem (4), the gradient of ln p(Φ̃|β) with respect to Φ̃ assumes the same form as that of ln p(Φ|β). Moreover, it can be shown that the gradient of ln p(wd,1:N |Φ̃, α, γ) with respect Φ̃ can be expressed as (see Appendix E):\n∂ ln p(wd,1:N |Φ̃, α) ∂Φ̃ = Eθd|xd\n{ ∂\n∂Φ̃\n[ ln p(xd|θd, Φ̃) + ln p(θd|α) ]} (15)\nwhere p(xd|θd, Φ̃) assumes the same form as (9) except Φ is replaced by Φ̃. The conditional expectation is evaluated with respect to the posterior probability p(θd|wd,1:N , Φ̃, α), which can be sampled by the MAP estimate of θd:\n∂ ln p(wd,1:N |Φ̃, α) ∂Φ̃ ≈ ∂ ∂Φ̃\n[ ln p(xd|θd,L, Φ̃) + ln p(θd,L|α) ] (16)\nwhere θd,L is an approximation of θ̂d|wd,1:N computed via (12) and Figure 2."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Description of Datasets and Baselines",
      "text" : "We evaluated our proposed supervised learning (denoted as BP-sLDA) and unsupervised learning (denoted as BP-LDA) methods on two real-world datasets. The first dataset we use is a large-scale dataset built on Amazon movie reviews (AMR) [13]. The data set consists of 7.9 million movie reviews (1.48 billion words) from Amazon, written by 889,176 users, on a total of 253,059 movies. For text preprocessing we removed punctuations and lowercasing capital letters. A vocabulary of size 5,000 is built by selecting the most frequent words. Same as [19], we shifted the review scores so that they have zero mean. The task is formulated as a regression problem, where we seek to predict the rating score using the text of the review.\nSecond, we demonstrate the effectiveness of our algorithm on a multi-domain sentiment (MultiSent) classification task. Sentiment classification system has gained popularity due to their application to multiple text genres, including financial news and product reviews. We use the dataset provided by [5], which contains a total 342,104 product reviews consisting of 25 types of product reviews, such as apparel, electronics, kitchen and housewares. The task is formulated as a binary classification problem to predict the polarity (positive or negative) of each review. Similar to AMR task, we preprocessed the text by removing punctuations and lowercasing capital letters. A vocabulary of size 1,000 is built from the most frequent words.\nWe examined our proposed methods (BP-sLDA and BP-LDA) as well as baselines on both tasks. For BP-sLDA, p(yd|θd, U, γ) is chosen to be Gaussion on the AMR regression task and multinormial on the MultiSent classification task (see Sec. 2). For BP-LDA, we first train the models in an unsupervised manner, and then generate per-document topic proportion θd as their features in the inference steps, on top of which we train a linear regression model in AMR regression task and train a logistic regression model in the MultiSent classification task, respectively. The baseline algorithms are implemented either in C++ or Java and our proposed algorithms are implemented in C#.2 We compared our methods to the unsupervised LDA learned by Gibbs sampling (GibbsLDA) [14], logistic/linear regression using raw bag-of-words (BoW), supervised-LDA (sLDA) [3], and MedLDA [21, 22]. Similar to BP-LDA, a separate linear/logistic regression is trained on the features generated by Gibbs-LDA. All the experiments are conducted with 5-fold cross validation."
    }, {
      "heading" : "5.2 Prediction Performance",
      "text" : "We first evaluate the prediction performance of different models on the AMR regression task. We use the predictive R2 to measure the prediction performance, defined as: pR2 = 1 − (∑d(yod −\n2The code will be released soon.\nyd) 2)/( ∑ d(y o d − ȳod)2), where yod denotes the label of the d-th document in the heldout (out-offold) set during the 5-fold cross validation, ȳod is the mean of all y o d in the heldout set, and yd is the predicted value. We first created a subset by randomly sampling 79K documents (reviews) from the 7.9 million reviews. The pR2 scores of different models with varying number of topics are shown in Figure 3(a). Note that the BP-sLDA model outperforms the other baselines with large margin. Moreover, the unsupervised BP-LDA model outperforms the unsupervised LDA model trained by Gibbs sampling (Gibbs-LDA). We further train our BP-sLDA model on the full 7.9M dataset with 5- fold cross validation and list the pR2 scores in Table 1. We can see that pR2 improves significantly compared to the best results on the 79K dataset shown in Figure 3(a). Moreover, the results in Table 1 also significantly outperform the pR2 scores of Gibbs-sLDA [23], Spectral-sLDA [19], and the Hybrid method (Gibbs-sLDA initialized with Spectral-sLDA) reported in [19], whose pR2 scores are between 0.1 and 0.2 for 5 ∼ 10 topics (and deteriorate when further increasing the topic number). The results therein are obtained on the same full AMR data with same setting as this paper. To further demonstrate the superior performance of BP-sLDA on the large vocabulary scenario, we trained BP-sLDA on the full 7.9M AMR dataset with full vocabulary (701K) and obtain the pR2 scores in Table 2. Note that the results are even significantly better than our results in Table 1.\nNext, we evaluate the performance of our algorithms on the binary classification task of multidomain sentiment analysis. We use the area-under-the-curve (AUC) of the operating curve of probability of correct positive versus probability of false positive as our performance metric. In Figure 3(b), we show the AUC of our methods and the baselines, which also shows that BP-sLDA outperforms other methods and that BP-LDA outperforms the unsupervised Gibbs-LDA model.\nFrom Figure 3, we note that the BP-sLDA model also consistently outperforms the linear regression or logistic regression model on the raw bag-of-words features. In Fig.3b (MultiSent), logistic regression achieves AUC of 90.4%, while BP-sLDA achieves the best AUC of 91.4% with 20 topics, which is about 10% relative improvement over logistic regression. And BP-sLDA significantly outperforms prior-art topic models, which have AUCs less than 80%. This means that our proposed discriminative training method and MDA-based MAP inference together are able to extract useful features from the raw BoW inputs for prediction purpose.\nTable 2: pR2 on full AMR data (7.9M documents and 701K vocabulary size).\nNumber of topics 5 10 20 50 100 Linear regression 0.403 BP-sLDA 0.633 0.677 0.672 0.682 0.684\n2 4 6 8 10 0\n0.1\n0.2\n0.3\n0.4\n0.5\nL\npR 2\nK=5 K=20 K=100\n(a) Number of MDA iterations L\n−4 −3 −2 −1 0 0\n0.1\n0.2\n0.3\n0.4\n0.5\nlog10(α−1)\npR 2\nK=5 K=20 K=100\n(b) Dirichlet parameter α\n−5 −4 −3 −2 −1 0\n0.1\n0.2\n0.3\n0.4\n0.5\nlog10(β−1)\npR 2\nK=5 K=20 K=100\n(c) Smoothing parameter β\nFigure 4: Sensitivity of hyper parameters: pR2 score for different L, α, and β.\nIn addition, we also conducted a new binary text classification experiment with highly promising results on a large-scale proprietary dataset for business-centric applications (1.2M documents and vocabulary size of 128K). In this new task, BP-sLDA (200 topics) achieves AUC of 92.2% and error rate of 15.2%, while LR has AUC of 90.5% and error rate of 17.1% (11% relative error rate cut). The gain is consistent with what was observed in the other two tasks."
    }, {
      "heading" : "5.3 Analysis and Discussion",
      "text" : "We now analyze the influence of different hyper parameters on the prediction performance. Note from Figure 3(a) that, when we increase the number of topics, the pR2 score of BP-sLDA first improves and then slightly deteriorates after it goes beyond 20 topics. This is most likely to be caused by overfitting on the small dataset (79K documents), because the BP-sLDA models trained on the full 7.9M dataset produce much higher pR2 scores (Table 1) than that on the 79K dataset and keep improving as the model size (number of topics) increases. Another interesting observation from Figure 3 is that, with limited amount of labeled data, the unsupervised LDA models (BP-LDA and Gibbs-LDA) are less prone to overfitting. Since unlabeled data are widely available, one future work is to combine the supervised and unsupervised parts together to have a semi-supervised LDA models. The framework suggested by [2, 12] could be one potential approach to integrate these two parts together.\nTo further understand the influence of the other hyper-parameters, we plot in Figure 4 the pR2 scores of BP-sLDA on the 79K AMR dataset for different values of L, α, and β. The performance is not very sensitive to the number of MDA inference steps L. One explanation for this phenomena is that the mirror-descent back propagation, as an end-to-end training of the prediction output, compensates the imperfection caused by the limited number of inference steps. And, we observe that, by properly tuning the Dirichlet parameter α and the smoothing parameter β, we could further improve the prediction performance of the model. Moreover, although we mainly focus on convex inference under α > 1, our algorithm could also handle α < 1 case except that, in this case, the inference is no longer convex and hence no global optimal MAP inference is guaranteed as for the methods prior to this work. Table 3 shows the corresponding pR2 scores of BP-sLDA on the 7.9M AMR dataset. Although the results is not as good as the α > 1 case in Table 1, they still significantly outperform the baselines."
    }, {
      "heading" : "5.4 Efficiency in Computation Time",
      "text" : "To compare the efficiency of the algorithms, we show the training time (in hours) of different models on the AMR dataset (79K and 7.9M) in Figure 5, which shows that our algorithm scales well when\nTable 3: pR2 for α < 1 case on full AMR data (7.9M documents and 5K vocabulary size).\nNumber of topics 5 10 20 50 100 BP-sLDA (α = 0.5) 0.488 0.548 0.575 0.571 0.574 BP-sLDA (α = 0.1) 0.441 0.558 0.572 0.569 0.570\n0 20 40 60 80 100 0\n20\n40\n60\n80\n100\n120\nNumber of topics\nTr ai\nni ng\nti m\ne in\nh ou\nrs\nsLDA (79K) BP−sLDA (79K) MedLDA (79K) BP−sLDA (7.9M)\nFigure 5: Training time of different methods (in hours) on the AMR dataset.\nwe increase the model size (number of topics). In addition, it also scales well on the large-scale (7.9M) dataset, which can be completed within reasonable amount of time."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have developed novel learning approaches for both supervised and unsupervised LDA models, using exact MAP inference with mirror descent algorithm and back propagation. In particular, the supervised LDA model is trained in an end-to-end fully discriminative manner by maximizing the posterior probability of the prediction variable given input documents. We evaluate the prediction performance of the models on two real-world regression and classification tasks. The results show that the discriminative training approach significantly improves the performance of the supervised LDA model relative to previous learning methods. Moreover, the newly developed inference and learning techniques also improve the performance of the unsupervised LDA model by providing better features for prediction. Future works include (i) exploring other optimization algorithms for the MAP inference problem, such as accelerated mirror descent, and (ii) developing semi-supervised learning of LDA based on the framework suggested by [2, 12]. More importantly, note that the layered architecture in Figure 2 could also be viewed as a deep feedforward neural network with special structures designed from the topic model in Figure 1. This opens up a new direction of combining the strength of both (generative) topic models and neural networks to develop new deep learning models that are scalable, interpretable and having high prediction performance."
    }, {
      "heading" : "B Derivation of the Recursion for Mirror Descent Algorithm",
      "text" : "First, we rewrite the optimization problem (11) as\nmin θd\n[∇θdf(θd,`−1)]T (θd − θd,`−1) + 1\nTd,` Ψ(θd, θd,`−1) (19)\ns.t. 1T θd = 1, θd 0 (20) where θd 0 denotes that each element of the vector θd is greater than or equal to zero. Using the fact that Ψ(x, y) = xT ln(x/y)− 1Tx + 1T y, the constrained optimization problem (19)–(20) becomes\nmin θd\n[∇θdf(θd,`−1)]T (θd − θd,`−1) + 1\nTd,`\n[ θTd ln\nθd θd,`−1\n− 1T θd + 1T θd,`−1 ]\n(21)\ns.t. 1T θd = 1, θd 0 (22) Dropping the terms independent of θd, we can write (21)–(22) as\nmin θd\n[∇θdf(θd,`−1)]T θd + 1\nTd,`\n[ θTd ln\nθd θd,`−1\n− 1T θd ]\n(23)\ns.t. 1T θd = 1, θd 0 (24)\nTo solve (23)–(24), we write its Lagrangian as\nL = [∇θdf(θd,`−1)]T θd + 1\nTd,`\n[ θTd ln\nθd θd,`−1\n− 1T θd ] + λ(1T θd − 1) (25)\nwhere we relaxed the nonnegative constraint in the above Lagrange multiplier. However, we will show that the solution obtained will automatically be nonnegative mainly because of the logarithm term in the cost function. Taking the derivative of L with respect to θd and λ and setting them to zero, we have, respectively,\n∂L ∂θd = ∇θdf(θd,`−1) + 1 Td,`\n[ ln\nθd θd,`−1\n] + λ1 = 0\n∂L ∂λ = 1T θd − 1 = 0\nwhich leads to\nθd = 1\nλ θd,`−1 exp (−Td,` · ∇θdf(θd,`−1))\n1T θd = 1\nSolving the above two equations together, we obtain\nθd = 1\nCθ θd,`−1 exp (−Td,` · ∇θdf(θd,`−1)) (26)\nwhere Cθ is a normalization factor such that θd,` adds up to one. Note that the above recursion can always guarantee non-negativity of the entries in the vector θd,` since we will always initialize the vector in the feasible region. Recall that f(θd) is the cost function on the right-hand side of (10), which is given by\nf(θd) = −xTd ln(Φθd)− (α− 1)T ln θd Therefore, the gradient of f(θd) can be computed as\n∇θdf(θd) = − xd Φθd − α− 1 θd (27)\nSubstituting the above gradient formula into (26), we obtain the desired result in (12).\nC Implementation Details of the BP-sLDA\nIn this section, we describe the implementation details of the mirror-descent back propagation for the end-to-end learning of the supervised LDA model. Specifically, we will describe the details of the inference algorithm, and the model parameter estimation algorithm.\nC.1 Inference algorithm: Mirror Descent\nLet f(θd) denote the objective function in (12). As we discussed in the paper, we use recursion (12) to iteratively find the MAP estimate of θd given wd,1:N , which we repeat below:\nθd,` = 1\nCθ · θd,`−1 exp\n( Td,` [ ΦT\nxd Φθd,`−1 + α− 1 θd,`−1\n]) , ` = 1, . . . , L, θd,0 = 1\nK 1 (28)\nThe step-size Td,` in mirror descent can be chosen to be either constant, i.e., Td,` = T , or adaptive over iterations ` and documents d. To adaptively determine the step-size, we can use line search procedure. A simple line search can be implemented as follows. For each document d:\n• Initialization: Td,0 = Td−1,L/η, where 0 < η < 1 (e.g., η = 0.5). • Repeat:\n– Update θd,` by (28) .\n– Break if following condition holds:\nf(θd,`) ≤ f(θd,`−1) + [∇θdf(θd,`−1)]T (θd,` − θd,`−1) + 1\n2Td,` Ψ(θd,`, θd,`−1)\n(29)\nelse: Td,` ← η · Td,` Moreover, Ψ(θd,`, θd,`−1) can also be replaced by the squared vector 1-norm:\nf(θd,`) ≤ f(θd,`−1) + [∇θdf(θd,`−1)]T (θd,` − θd,`−1) + 1\n2Td,` ‖θd,` − θd,`−1‖21 (30)\nThe line search approach determines the step-sizes adaptively, automatically stabilizing the algorithm and making inference converge faster.\nC.2 Parameter Estimation: Stochastic Gradient Descent with Back Propagation\nWe first rewrite the training cost (14) as\nJ(U,Φ) = D∑ d=1 Qd(U,Φ) (31)\nwhere Qd(·) denotes the loss function at the d-th document, defined as\nQd(U,Φ) , − 1\nD ln p(Φ|β)− ln p(yd|θd,L, U, γ) (32)\nNote that, we do not have constraint on the model parameter U . Therefore, to update U , we can directly use the standard mini-batch stochastic gradient descent algorithm. We randomly sample a mini-batch of documents, and then perform MAP inference of θd for each document in the minibatch. And then, we compute the stochastic gradient of the loss function for each document, and use the averaged stochastic gradient to update U .\nOn the other hand, each column of the model parameter Φ is constrained to be on a (V − 1)- dimension probability simplex, i.e, each element of Φ has to be nonnegative and each column sum up to one (i.e., Φ is a left-stochastic matrix). For this reason, we need to enforce the constraint on Φ. Recalling the definition of the Dirichlet smoothing p(Φ|β), we have\n− 1 D ln p(Φ|β) = − 1 D ln (Γ(V β) Γ(β)V )K K∏ j=1 V∏ v=1 Φβ−1vj  = − 1\nD K∑ j=1 V∑ v=1 (β − 1) ln Φvj + C (33)\nObserve that expression (33) provides a natural log barrier for each element of Φ to enforce it to be nonnegative. Therefore, we can relax the nonnegative constraint on the elements of Φ and focus on enforcing the left-stochastic constraint. Let φj be the j-th column of Φ, we use the following algorithm to update the estimate of φj :\n• Sample a mini-batch of documents. • Perform MAP inference of yd and θd using (12) for each document in the mini-batch. • Compute the gradient ∂Qd/∂φj for each document d in the mini-batch and average them:\n∆φj = 1\nDb ∑ d∈Db ∂Qd ∂φj , j = 1, . . . ,K\nwhere ∂Qd/∂φj is the j-th column of ∂Qd/∂Φ, which can be computed according to the formula in Sec. D of this Appendix, Db denotes the set of the documents in the minibatch, and Db is the number of documents in the mini-batch. The gradients are evaluated at φj,t−1, the previous estimate of φj at time t− 1.\n• Set initial learning rate: µφj = µ0, j = 1, . . . ,K. • For each j = 1, . . . ,K, repeat until all the elements of φj,t are nonnegative:\n– Update φj :\nφj,t = Π{φ:1Tφ=1} ( φj,t−1 − µφj ·∆φj ) (34)\nwhere Π{φ:1Tφ=1}(·) is the Euclidean projection operator onto the affine space: {φ : 1Tφ = 1}, which can be evaluated efficiently in closed-form:\nΠ{φ: 1Tφ=1}(x) =\n( I − 11 T\nK\n) x+ 1\nK 1\n– Shrink the learning rate: µφj = ηµφj , where 0 < η < 1 (e.g., η = 0.5).\nIn the above iteration (34), we do not need to recompute the gradient ∆φj but just iteratively shrink the learning rates until there is no violation of the nonnegativity constraint. This line search is only used to avoid the stochastic gradient descent from randomly moving φj into the nonnegative regime, where the log-barrier cannot push φj back to the positive region. Furthermore, we are allowing different columns of Φ to have different learning rates, which, from our observation in experiments, makes the training algorithm converge much faster than the uniform learning rate over all columns."
    }, {
      "heading" : "D Gradient Formula of BP-sLDA",
      "text" : "In this section, we give the gradient formula for the supervised learning of BP-sLDA. To this end, we first rewrite the training cost (14) as\nJ(U,Φ) = D∑ d=1 Qd(U,Φ) (35)\nwhere Qd(·) denotes the loss function at the d-th document, defined as\nQd(U,Φ) , − 1\nD ln p(Φ|β)− ln p(yd|θd,L, U, γ) (36)\nThe expressions for the two terms in (36) are given by\n− 1 D ln p(Φ|β) = − 1 D ln (Γ(V β) Γ(β)V )K K∏ j=1 V∏ v=1 Φβ−1vj  = − 1\nD K∑ j=1 V∑ v=1 (β − 1) ln Φvj + C (37)\n− ln p(yd|θd,L, U, γ) =  − V∑ j=1 yd,j ln exp(γ · po,d,j)∑V m=1 exp(γ · po,d,m) classification 1\n2γ ‖yd − po,d‖22 + C regression\n=  − V∑ j=1 yd,jγ · po,d,j + ln V∑ m=1 exp(γ · po,d,m) classification 1\n2γ ‖yd − po,d‖22 + C regression\n(38)\nwhere C in the above expressions denotes a constant term that is independent of U and Φ, and\npo,d , Uθd,L (39)\nIn order to apply stochastic gradient descent to minimize (35), it suffices to evaluate the gradient of Qd(U,Φ) with respect to U and Φ, which we now proceed to derive. Note that the choice of p(yd|θd,L, U, γ) is not restricted to the above two options in our framework. Other forms could also be used and the corresponding gradient formula could also be derived. However, in this paper, we only list the gradient formula for these two classical choices.\nD.1 Gradient with respect to U\nFirst, we derive the gradient of Qd(·) with respect U . Note that the only term in (36) depending on U is ln p(yd|θd,L, U, γ). Therefore, we have ∂Qd/∂U = −∂ ln p(yd|θd,L, U, γ)/∂U . Taking the gradient of (38) with respect to U and after some simple algebra, we get\n∂Qd ∂U = { −γ · (yd − ŷd)θTd,L classification − 1γ · (yd − ŷd)θTd,L regression\n(40)\nwhere ŷd is defined as\nŷd = { σ(γ · po,d) classification po,d regression\nwhere σ(·) is the soft-max function:\nσ(x) , x∑V\nm=1 exp(xm)\nD.2 Gradient with respect to Φ\nIn this subsection, we give the final expression for the gradient of Qd with respect to Φ. The derivation can be found in Sec. D.3 of this Appendix.\n∂Qd ∂Φ = − 1 D · β − 1 Φ + L∑ `=1 ∂Qd ∂Φ`\n(41)\nwhere ∂Qd/∂Φ` is defined as\n∂Qd ∂Φ`\n= Td,` · {\nxd Φθd,`−1\n(θd,` ξd,`)T − [ Φ(θd,` ξd,`)\nxd (Φθd,`−1)2\n] θTd,`−1 } (42)\nand ξd,` is an intermediate error vector computed from the following backward recursion: ξd,`−1 = (I−1θTd,`−1) { θd,` ξd,` θd,`−1 −Td,` · [ ΦTdiag ( xd (Φθd,`−1)2 ) Φ+diag ( α− 1 θ2d,`−1 )] (θd,` ξd,`) } (43)\nwhich is initialized at\nξL,t = −(I − 1θTd,L) · UT · γ(yd − ŷd) (44) In the above back propagation formula, xd and yd are the input bag-of-words vector and the label. The quantities θd,` and ŷd are obtained from the inference step, the MDA step-size Td,` is either set to be a constant (as a hyper-parameters) or determined by line-search in the inference step.\nSimilar to the inference iteration (12), the above gradients can be computed efficiently by exploiting the sparsity of the vector xd. For example, only the elements at the nonzero positions of xd need to be computed for Φθd,`−1 and Φ(θd,` ξd,`) since xdΦθd,`−1 and xd (Φθd,`−1)2\nare known to be zero at these positions. Moreover, although (β−1)/Φ is a dense matrix operation, it is the same within one mini-batch and can therefore be computed only once over each mini-batch, which can significantly reduce the amount of computation.\nD.3 Derivation of the gradient with respect to Φ\nIn this subsection, we derive the gradient formula for Φ. Note from (36) that, there are two terms that depend on Φ, and\n∂Qd ∂Φ = ∂ ∂Φ ( − 1 D ln p(Φ|β) ) + ∂ ∂Φ ( − ln p(yd|θd,L, U, γ) ) (45)\nThe first term depends on Φ explicitly and its gradient can be evaluated direct as\n∂\n∂Φ ( − 1 D ln p(Φ|β) ) = ∂ ∂Φ − 1 D K∑ j=1 V∑ v=1 (β − 1) ln Φvj  = − 1\nD · β − 1 Φ (46)\nThe second term, however, depends on Φ implicitly through θd,L. From Figure 2, we observe that θd,L not only depends on Φ explicitly (as indicated in the MDA block on the right-hand side of Figure 2) but also depends on Φ implicitly via θd,L−1, which in turn depends on Φ both explicitly and implicitly (through θd,L−2) and so on. That is, the dependency of the cost function on Φ is in a layered manner. For this reason, we need to apply chain rule to derive the its full gradient with respect to Φ, which we describe below.\nFirst, as we discussed above, each MDA block in Figure 2 contains Φ, and Qd(U,Φ) depends on the Φ appeared at different layers through θd,L, . . . , θd,1. If we denote these Φ at different layers as ΦL, . . . ,Φ1, and introduce an auxiliary function Qd(U,Φ1, . . . ,ΦL) to represent an artificial function, − ln p(yd|θd,L, U, γ), with this “untied” Φ across layers in Figure 2, then the original − ln p(yd|θd,L, U, γ) with “tied” Φ across layers can be written in the form of Qd(U,Φ1, . . . ,ΦL) as\n− ln p(yd|θd,L, U, γ) = Qd(U,Φ, . . . ,Φ) (47) For this reason, we can express the gradient of − ln p(yd|θd,L, U, γ) with respect to Φ as\n∂\n∂Φ\n( − ln p(yd|θd,L, U, γ) ) = L∑ `=1 ∂Qd ∂Φ`\n(48)\nwhere ∂Qd/∂Φ` denotes the gradient of Q(U,Φ1, . . . ,ΦL) with respect to Φ` evaluated at Φ1 = Φ2 = · · · = ΦL = Φ. Therefore, we only need to compute the gradient ∂Qd/∂Φ`. For simplicity of notation, we drop the subscript of d in θd,` and define the following intermediate quantities:\nz` = Td,` · [ ΦT\nxd Φθ`−1 + α− 1 θ`−1 ] p` = θ`−1 exp(z`)\nThen the MDA inference recursion (12) can be written in the following equivalent form: z` = Td,` · [ ΦT\nxd Φθ`−1 + α− 1 θ`−1\n] (49)\np` = θ`−1 exp(z`) (50) θ` =\np` 1T p`\n(51)\nTo derive the gradient ∂Qd/∂Φ`, it suffices to derive ∂Q∂Φ`,ji . Note that\n∂Qd ∂Φ`,ji = ∂pT` ∂Φ`,ji · ∂Qd ∂p` = ∂pT` ∂Φ`,ji · δ` (52)\nwhere\nδ` , ∂Qd ∂p`\n(53)\nis an intermediate quantities which follows a backward recursion to be derived later. To proceed, we need to derive ∂pT` /∂Φ`,ji:\n∂pT` ∂Φ`,ji = θT`−1 ∂ exp(zT` ) ∂Φ`,ji\n= θT`−1 [ ∂zT` ∂Φ`,ji · diag ( exp(z`) )]\n= θT`−1 [ ∂zT` ∂Φ`,ji 1 exp(zT` ) ]\n= θT`−1 exp(zT` ) ∂zT` ∂Φ`,ji\n= pT` ∂zT` ∂Φ`,ji\n(54)\nThen, we need to derive the expression for ∂z T l\n∂Φ`,ji :\n∂zT` ∂Φ`,ji\n= Td,` · { ∂\n∂Φ`,ji\n( xTd\nθT`−1Φ T `\n) · Φ` +\nxTd θT`−1Φ T ` · Φ` Φ`,ji\n}\n= Td,` · { ∂\n∂Φ`,ji\n( xTd\nθT`−1Φ T `\n) · Φ` +\nxTd θT`−1Φ T `\n· Eji }\n= Td,` · { −∂θ T `−1Φ T `\n∂Φ`,ji · diag\n( xd\n(Φ`θ`−1)2\n) · Φ` +\nxTd θT`−1Φ T l\n· Eji }\n= Td,` · { −θT`−1Eij · diag ( xd\n(Φ`θ`−1)2\n) · Φ` +\nxTd θT`−1Φ T `\n· Eji }\n= Td,` · { −[θ`−1]i [ xd\n(Φ`θ`−1)2 ] j eTj Φ` + [ xd Φ`θ`−1 ] j eTi } (55)\nwhere ei denotes a one-hot vector with the i-th element being one and all other element equal to zero, and Eji denotes a matrix whose (j, i)-th element is one and all other elements are zero. Substituting the above expression into (54), we obtain\n∂pT` ∂Φ`,ji = pT` ∂zT` ∂Φ`,ji\n= Td,` · pT` { −[θ`−1]i [ xd\n(Φ`θ`−1)2 ] j eTj Φ` + [ xd Φ`θ`−1 ] j eTi } (56)\nTherefore,\n∂Qd ∂Φ`,ji = ∂pT` ∂Φ`,ji · δ`\n= Td,` · p` { −[θ`−1]i [ xd\n(Φ`θ`−1)2 ] j eTj Φ` + [ xd Φ`θ`−1 ] j eTi } δ`\n= Td,` · { −[θ`−1]i [ xd\n(Φ`θ`−1)2 ] j ( p` eTj Φ` ) δ` + [ xd Φ`θ`−1 ] j (p` eTi )δ` }\n= Td,` · { −[θ`−1]i [ xd\n(Φ`θ`−1)2 ] j ( p` eTj Φ` ) δ` + [ xd Φ`θ`−1 ] j [p`]i · [δ`]i }\n= Td,` · { −[θ`−1]i [ xd\n(Φ`θ`−1)2 ] j ( eTj Φ`diag(p`) ) δ` + [ xd Φ`θ`−1 ] j [p`]i · [δ`]i }\n= Td,` · { −[θl]i [ xd\n(Φ`θ`−1)2 ] j eTj Φ`(pl−1 δ`) + [ xd Φ`θ`−1 ] j [p`]i · [δ`]i }\n= Td,` · { −[θ`−1]i [ xd\n(Φ`θ`−1)2 ] j [Φ`(p` δ`)]j + [ xd Φ`θ`−1 ] j [p`]i · [δ`]i }\n(57)\nWriting the above expressions into matrix form (derivative with respect Φ`), we obtain:\n∂Qd ∂Φ`\n= Td,` · {\nxd Φ`θ`−1\n(p` δ`)T − [ Φ`(p` δ`)\nxd (Φ`θ`−1)2\n] θT`−1 } (58)\nNow we need to derive the recursion for computing δ`. By the definition of δ` in (53), we have\nδ`−1 , ∂Qd ∂p`−1\n= ∂θT`−1 ∂p`−1 · ∂p T ` ∂θ`−1 · ∂Qd ∂p`\n= ∂θT`−1 ∂p`−1 · ∂p T ` ∂θ`−1 · δ` (59)\nTo continue, we have to evaluate ∂θ T `−1\n∂p`−1 and ∂p\nT `\n∂θ`−1 . By (49)–(51), we have\n∂pT` ∂θ`−1 = ∂θT`−1 ∂θ`−1 1 exp(zT` ) + 1θT`−1 ∂ exp(zT` ) ∂θ`−1\n= I [1 exp(zT` )] + 1θT`−1 [ ∂zT` ∂θ`−1 · ∂e T ` ∂z` ] = diag ( exp(z`) ) + 1θT`−1 [ ∂zT` ∂θ`−1 · diag ( exp(z`) )]\n= diag ( exp(z`) ) + 1θT`−1 [ ∂zT` ∂θ`−1 1 exp(zT` ) ]\n= diag ( exp(z`) ) + 1 [ θT`−1 exp(zT` ) ] ∂z T `\n∂θ`−1 = diag ( exp(z`) )\n+ 1pT` ∂zT` ∂θ`−1\n(60)\nTo proceed, we need to derive the expression for ∂z T `\n∂θ`−1 :\n∂zT` ∂θ`−1\n= Td,` · { ∂\n∂θ`−1\n( xTd\nθT`−1Φ T `\n) Φ` + ∂\n∂θ`−1 ( α− 1 θ`−1 )T}\n= Td,` · { −∂θ T `−1Φ T `\n∂θ`−1 · diag\n( xd\n(ΦT` θ`−1) 2\n) Φ` − diag ( α− 1 θ2`−1 )}\n= Td,` · { −ΦT` diag ( xd\n(ΦT` θ`−1) 2\n) Φ` − diag ( α− 1 θ2`−1 )} = −Td,` · { ΦT` diag ( xd\n(ΦT` θ`−1) 2\n) Φ` + diag ( α− 1 θ2`−1 )} (61)\nSubstituting the above expression into (60), we get the expression for ∂p T `\n∂θ`−1 :\n∂pT` ∂θ`−1 = diag\n{ exp ( Td,` [ ΦT`\nxd Φ`θ`−1 + α− 1 θ`−1 ])} − Td,` · (1pT` ) [ ΦT` diag ( xd\n(Φ`θ`−1)2\n) Φ` + diag ( α− 1 θ2`−1 )] = diag\n( p` θ`−1 ) − Td,` · (1pT` ) [ ΦT` diag ( xd (Φ`θ`−1)2 ) Φ` + diag ( α− 1 θ2`−1 )] = { diag ( 1\nθ`−1\n) − Td,` · [ ΦT` diag ( xd\n(Φ`θ`−1)2\n) Φ` + diag ( α− 1 θ2`−1 )]} diag(p`)\n(62)\nTo complete the derivation of the recursion (59), we need to derive ∂θ T `−1\n∂p`−1,t , which is given by\n∂θT`−1 ∂p`−1 = ∂pT`−1 ∂p`−1 · 1 1T p`−1 + ∂ ∂p`−1\n( 1\n1T p`−1\n) pT`−1 =\nI − 1θT`−1 1T p`−1\n(63)\nExpressions (59), (62) and (63) provide the complete backward recursion for δ`, which starts from ` = L and ends at ` = 2. Finally, to initialize this backward recursion, we need to derive the expression for δL. By its definition, we have\nδL , ∂Qd ∂pL\n= ∂θTL ∂pL · ∂pTo,d ∂θL · ∂Qd ∂po,d\n= ∂θTL ∂pL · UT · ∂Qd ∂po,d\n= 1\n1T pL (I − 1θTL) · UT · ∂Qd ∂po,d\n(64)\nwhere in the last step we substituted (63). By (47) and(38), we have\n∂Qd ∂po,d = ∂ ∂po,d\n( − ln p(yd|θd,L, U, γ) ) =\n{ −γ · (yd − ŷd) classification − 1γ · (yd − ŷd) regression\n(65)\nTherefore,\nδL =  − 1 1T pL (I − 1θTL) · UT · γ · (yd − ŷd) classification\n− 1 1T pL (I − 1θTL) · UT · 1 γ · (yd − ŷd) regression\n(66)\nAs a final remark, we found in practical implementation that p` could be very large while δ` could be small, which leads to potential numerical instability. To address this issue, we introduce the following new variable:\nξd,` , 1 T p` · δ` (67)\nThen, the quantities p` and δ` can be replaced with one variable ξd,`, and the backward recursion of δ` can also be replaced with the backward recursion of ξd,`. With some simple algebra, we obtain the final gradient expression for Φ in Appendix D.2."
    }, {
      "heading" : "E Gradient Formula of BP-LDA",
      "text" : "The unsupervised learning problem (4) can be rewritten, equivalently, as minimizing the following cost function:\nJ(Φ̃) = D∑ d=1 Qd(Φ̃) (68)\nwhere Qd(Φ̃) is the loss function defined as\nQd(Φ̃) = − 1\nD ln p(Φ̃|β)− ln p(wd,1:N |Φ̃, α) (69)\nTaking the gradient of both sides of (69), we obtain\n∂Qd\n∂Φ̃ =\n∂\n∂Φ̃\n( − 1 D ln p(Φ̃|β) ) + ∂\n∂Φ̃\n( − ln p(wd,1:N |Φ̃, α) ) (70)\nThe first term in (70) has already been derived in (46):\n∂ ∂Φ̃ ln p(Φ̃|β) = β − 1 Φ̃ (71)\nwhere β−1 Φ̃ denotes elementwise division of the scalar β − 1 by the matrix Φ̃. We now proceed to derive the second term in (70).\n∂\n∂Φ̃ ln p(wd,1:N |Φ̃, α) =\n1 p(wd,1:N |Φ̃, α) · ∂ ∂Φ̃ p(wd,1:N |Φ̃, α)\n= 1 p(wd,1:N |Φ̃, α) · ∫ θd [ ∂ ∂Φ̃ p(wd,1:N , θd|Φ̃, α) ] dθd\n= 1 p(wd,1:N |Φ̃, α) · ∫ θd [ ∂ ∂Φ̃ ln p(wd,1:N , θd|Φ̃, α) ] · p(wd,1:N , θd|Φ̃, α)dθd\n= ∫ θd [ ∂ ∂Φ̃ ln p(wd,1:N , θd|Φ̃, α) ] · p(wd,1:N , θd|Φ̃, α) p(wd,1:N |Φ̃, α) dθd\n= ∫ θd [ ∂ ∂Φ̃ ln p(wd,1:N , θd|Φ̃, α) ] · p(θd|wd,1:N , Φ̃, α)dθd\n= Eθd|wd,1:N\n[ ∂\n∂Φ̃ ln p(wd,1:N , θd|Φ̃, α)\n] (72)\nTo continue, we now derive the expression for the gradient inside the expectation term in (72). By (9) and the definition of Dirichlet distribution p(θd|α), we can write ln p(wd,1:N , θd|Φ̃, α) as\nln p(wd,1:N , θd|Φ̃, α) = ln p(wd,1:N , θd|Φ̃, α) = ln p(wd,1:N |θd, Φ̃) + ln p(θd|α)\n= K∑ j=1 (αj − 1) ln θd,j + ln Γ ( K∑ j=1 αj ) − K∑ j=1 ln Γ(αj)\n+ V∑ v=1 xd,v ln ( K∑ j=1 θd,jΦ̃vj ) = xTd ln(Φ̃θd) + (α− 1)T ln θd + ln(1Tα)− 1T ln Γ(α) (73)\nTaking the gradient of the above expression with respect to Φ̃, we obtain the gradient formula."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "<lb>We develop a fully discriminative learning approach for supervised Latent Dirich-<lb>let Allocation (LDA) model, which maximizes the posterior probability of the pre-<lb>diction variable given the input document. Different from traditional variational<lb>learning or Gibbs sampling approaches, the proposed learning method applies<lb>(i) the mirror descent algorithm for exact maximum a posterior inference and (ii)<lb>back propagation with stochastic gradient descent for model parameter estimation,<lb>leading to scalable learning of the model in an end-to-end discriminative manner.<lb>As a byproduct, we also apply this technique to develop a new learning method for<lb>the traditional unsupervised LDA model. Experimental results on two real-world<lb>regression and classification tasks show that the proposed methods significantly<lb>outperform the previous supervised/unsupervised LDA learning methods.",
    "creator" : "LaTeX with hyperref package"
  }
}