{
  "name" : "1703.07823.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Fake News Mitigation via Point Process Based Intervention ",
    "authors" : [ "Mehrdad Farajtabar", "Jiachen Yang", "Xiaojing Ye", "Huan Xu", "Rakshit Trivedi", "Elias Khalil", "Shuang Li", "Le Song", "Hongyuan Zha" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The recent proliferation of malicious fake news in social media has been a source of widespread concern. Given that more than 62% of U.S. adults turn to social media for news, with 18% doing so often, fake news can have potential realworld consequences on a large scale (Gottfried & Shearer, 2016). For example, within the final three months of the 2016 U.S. presidential election, news stories that favored either of the two nominees–later proved to be fake–were shared over 37 million times on Facebook alone, and over half of those who recalled seeing fake news stories believed them (Allcott & Gentzkow, 2017). An analysis by Buzzfeed News shows that the top 20 false election stories from hoax websites generated nearly 1.5 million more user engagement activities on Facebook than the top 20 stories from reputable major news outlets (Silverman, 2016). Therefore, there is an urgent call to develop effective rectifying strategies to mitigate the impact of fake news.\nPolicies to counter fake news can be categorized by the level of manual oversight and the aggressiveness of action required. Aggressively acting on fake news has various drawbacks. For example, Facebook’s strategy allows users to report stories as potential fake news, sends these stories to fact-checking organizations, and flags them as disputed in users’ newsfeed (Mosseri, 2016). Such direct action on the offending news requires a high degree of human oversight, which can be costly and slow for large social networks and also may violate civil rights. The report-and-flag mechanism is also open to abuse by adversaries who maliciously report real news. Given these disadvantages, we consider an alternative strategy: optimizing the performance of real news propagation over the network. Intuitively, we want people’s exposure to real news to match their exposure to fake news.\nWe face several key modeling and computational issues. For example, how to quantify the uncertainty of user activities and news propagation within the network? How to measure the effect of mitigation incentives and activities? Is it possible to steer the spontaneous user mitigation activities by an intervention strategy? To address these questions, we model the temporal randomness of fake news and mitigation events (“valid news”) as multivariate point processes with self and mutual excitations, in which the control incentivizes more spontaneous mitigation events by contributing to the exogenous activity of campaigner nodes. The influence of fake news and mitigation activities is quantified using event exposure counts (i.e. the number of times that a user is exposed to fake or real news posts from other users whom she follows).\n1School of Computational Science and Engineering, Georgia Tech. 2Department of Mathematics and Statistics, Georgia State University. 3School of Industrial and Systems Engineering, Georgia Tech.. Correspondence to: Mehrdad Farajtabar <mehrdad@gatech.edu>.\nar X\niv :1\n70 3.\n07 82\n3v 1\n[ cs\n.L G\n] 2\n2 M\nar 2\n01 7\nOur key contributions are as follows. We present the first formulation of fake news mitigation as the problem of optimal point process intervention in a network. The goal is to optimize the activity policy of a set of campaigner nodes to mitigate a fake news process stemming from another set of nodes. It creates opportunities for designing a variety of objectives, e.g. minimizing the number of users who see fake news but were not reached by real news. We give the first derivation of second-order statistics of random exposure counts in the non-stationary case, which is essential in policy evaluation and improvement. By defining a state space for the network, formulating actions as exogenous intensity, and defining reward functions, we map the fake news mitigation problem to an optimal policy problem in a Markov decision process (MDP), which will be solved by model-based least-squares temporal difference learning (LSTD) specific to the context of multivariate point processes. Furthermore, to the best of our knowledge, we are the first to conduct a real-time point process intervention experiment.\nRelated work. The emergence of social media as a prominent news source in the past few years raises concomitant concerns about the quality, truthfulness, and credibility of information presented (Mitra et al., 2017). To reduce the amount of labor-intensive manual fact-checking, there have been research efforts devoted to building classifiers to detect factuality of information, predicting credibility level of posts, and detecting controversial information from inquiry phrases (Mitra et al., 2017; Zeng et al., 2016; Zhao et al., 2015). These works mainly focused on extracting linguistic features from texts to determine the credibility of news and posts. Our focus in this paper, however, is to design an incentive strategy so that users can spontaneously take action to mitigate a real-world fake news epidemic.\nPoint process models have been recently used to model activities in social networks (Farajtabar et al., 2015; Parikh et al., 2012; Hosseini et al., 2017; Karimi et al., 2016). More especially Hawkes process (Hawkes, 1971) is a class of selfand mutually exciting point processes that has been applied to variety of problems in social networks including cascade modeling (Zarezade et al., 2015), reliability of crowd generated data (Tabibian et al., 2016), social media popularity (Rizoiu et al.), community detection (Tran et al., 2015), causal inference (Xu et al., 2016), linguistic influence (Guo et al., 2015), and change point detection in social networks (Li et al., 2016).\nSteering user activities by adding external incentives to the exogenous intensity of Hawkes processes was first considered in (Farajtabar et al., 2014). In (Farajtabar et al., 2016), a multistage campaigning method to optimally distribute incentive resources based on dynamic programming was developed. In these previous works, objective functions were designed using expected values of exposure counts rather than the stochastic exposure process, which may reduce the accuracy of\nsolutions. Furthermore, it faced the demanding problem of computing the cost-to-go using the Hawkes model, while we address this using linear function approximation. For stationary Hawkes processes, second order statistics was derived in (Bacry & Muzy, 2014a;b); however, it is essential to compute both first and second order statistics for Hawkes processes in the non-stationary stages due to time sensitivity of the fake news mitigation task, and we derive it for the first time in this paper. Recent work has also applied methods in stochastic differential equations to the context of point processes, to find the best intensity for information guiding (Wang et al., 2016) and achieving highest visibility (Zarezade et al., 2017). While these works consider networks with only a single process, our work focuses on optimizing a mitigation process with respect to a second competing process.\nReinforcement learning tackles the problem of finding good policies for actions to take in MDP where exact solutions are intractable, either due to size or lack of complete knowledge. Large-scale policy evaluation and iteration problems can be tackled by function approximation, which reduces the solution dimension using feature vector basis (Sutton & Barto, 1998). By adding control terms to a multivariate Hawkes process model of random network activities, fake news mitigation can be formulated as a policy optimization problem in an MDP. To address the randomness of Hawkes processes, batch reinforcement learning using samples collected from the trajectory of a fixed behavior policy can be applied (Antos et al., 2007). In particular, linear Least Squares Temporal Difference (LSTD) uses a batch of samples to learn a linear approximation of the value function under a policy with provable convergence (Bradtke & Barto, 1996). This policy evaluation step alternates with a model-based policy improvement step in a policy iteration algorithm to arrive at successively improved policies."
    }, {
      "heading" : "2. Preliminaries and Problem Statement",
      "text" : "Multivariate Hawkes processes. Hawkes process is a doubly stochastic point process with self-excitations, meaning that past events increase the chance of arrivals of new events (Hawkes, 1971), and has been extensively used to model activities in social networks (Farajtabar et al., 2015; Linderman & Adams, 2014; He et al., 2015; Rizoiu et al.; Lee et al., 2016). Let t` be the time of the `-th event, then the Hawkes process can be represented by the counting process N(t) =∑ t`≤t h(t − t`) that tracks the number of events up to time t, where h(t) is the standard Heaviside function such that h(t) = 1 if t ≥ 0 and = 0 if t < 0. The key characteristic of a point process is its conditional intensity function, defined as the probability of observing an event in an infinitesimal window given the history. For Hawkes process it is given by\nλ(t) = µ+ ∑ t`<t φ(t− t`).\nHere, µ ≥ 0 is the exogenous (base) intensity and φ(t) is the Hawkes kernel that describes how fast the excitement of a past event decays. In this paper, we employ the standard (stationary) exponential Hawkes kernel, i.e., φ(t) = αe−ωth(t) with ω > α > 0. In an n-dimensional multivariate Hawkes process (MHP), there are n such processes N1(t), . . . , Nn(t) that can also mutually excite one another, and the conditional intensity λ(t) := ( λ1(t), . . . , λn(t)\n)> ∈ Rn+ is given by λ(t) = µ+\n∫ t 0 Φ(t− s) dN(s).\nHere, N(t) := ( N1(t), . . . , Nn(t) )> ∈ Nn0 , µ := (µ1, . . . , µn)> ∈ Rn+, and [Φ(t)]ij = φij(t) := αije−ωth(t). We let H(t) denote the filtration of N(t), generated by the σ-algebra of history { (t`, i`)|t` ≤ t } of this point process, where i` ∈ {1, . . . , n} is the identity (node) of the `-th event.\nNetwork activities. We model the activities of both fake news and mitigation events as MHP in the network. Basically, MHP is a networked point process model with dependent dimensions (nodes), and can capture the underlying dynamics of social networks. Define F (t) = ( F1(t), . . . , Fn(t) )> ∈ Nn0 , where Fi(t) counts the number of times user i shares a piece of news from the fake campaign up to time t. Similarly, define M(t) = ( M1(t), . . . ,Mn(t)\n)> ∈ Nn0 for the mitigation process. Correspondingly, we have two intensity functions: λM (t) = (λM1 (t), . . . , λ M n (t))\n> and λF (t) = (λF1 (t), . . . , λ F n (t)) > and two sets of exogenous intensities µM and µF .\nGoal. Given that both F (t) and M(t) are modeled by the Hawkes processes, our goal is to find the optimal mitigation campaign by imposing interventions to users such that the mitigation effect (rigorously defined in sec. 3.1) can be maximized or equivalently the fake news be rectified under budget constraints. To this end, we measure the influence of fake\nnews and mitigation activities using event exposures, describe the mechanism of mitigation interventions, and quantify the effect of interventions mathematically.\nEvent exposure. Event exposure is a quantitative measure of campaign influence, and is represented as a counting process, E(t) = ( E1(t), . . . , En(t) )> . Here, Ei(t) records the number of times user i is exposed (she or one of her neighbors performs an activity) to a campaign N(t) by time t. Let B be the adjacency matrix of the user network, i.e., bij = 1 if user i follows user j. Assume bii = 1 for all i. Then the exposure process is given by E(t) = BN(t). We define F(t) = BF (t) andM(t) = BM(t) as the fake news and mitigation processes, respectively.\nIntervention. Suppose we can perform intervention by incentivizing a subset of users in the k-th stage during time [τk, τk+1) for k = 0, 1, . . . . For simplicity we consider uniform time duration τk+1 − τk = ∆T for all k, since generalization to nonuniform time durations is trivial. In order to steer the mitigation activities to counter the fake news (criteria given below) at these stages, we impose an additional constant intervention uki ≥ 0 to the exogenous intensity µi during time [τk, τk+1) for each stage k = 0, 1, . . . . The mitigation activity intensity at the k-th stage is\nλM (t) = µ+ uk + ∫ t 0 Φ(t− s) dM(s)\nfor t ∈ [τk, τk+1). Note that the intervention itself exhibits a stochastic nature: adding uki to µi is equivalent to incentivizing user i to increase her activity rate but it is still uncertain when she will perform an activity, which appropriately mimics the randomness in the real world.\nReward function. For each stage k, xk (defined later) is the state of the whole MDP that encodes all the information from previous stages and uk is the current control imposed at this stage. LetMki (t;xk, uk) := ∑ j bij ∫ t τk\ndMj(s) be the number of times user i is exposed to the mitigation campaign by time t ∈ [τk, τk+1) within stage k, then the goal is to steer the expected total number of exposure Mki (t;xk, uk) using uk, s.t. the sum of reward functions R(xk, uk) (rigorously defined in sec. 3.1) is maximized.\nProblem statement. By observing the counting process in previous stages (summarized in a sequence of xk) and taking the future uncertainty into account, the control problem is to design a policy π such that the controls uk = π(xk) can maximize the total discounted objective E[ ∑∞ k=0 γ\nkRk], where γ ∈ (0, 1] is the discount rate and Rk is the observed reward at stage k. In addition, we may have constraints on the amount of control, such as a budget constraint on the sum of all interventions to users at each stage, or a cap over the amount of intensity a user can handle. A feasible set or an action space over which we find the best intervention is represented as\nUk := { u ∈ Rn|u>ck ≤ Ck, 0 ≤ u ≤ αk } .\nHere, cki is the price per unit increase of exogenous intensity of user i and Ck ∈ R+ is the total budget at stage k. Also, αki is the cap on the amount of activities of the user i."
    }, {
      "heading" : "3. Proposed Method",
      "text" : "In this section, we present the formulation of reward functions in terms of event exposures of fake news and mitigation activities. Then we derive the key statistics of the MHP required for reward function evaluation, followed by the policy iteration scheme to find the optimal intervention."
    }, {
      "heading" : "3.1. Fake news mitigation",
      "text" : "As we discussed above, the total reward of policy π is defined by the value function\nV π(x0) = E [ ∞∑ k=0 γkRk ∣∣∣∣x0] (1)\nfor the initial state x0 of fake and mitigation processes, where the observed reward R quantifies the effect of mitigation activities M(t) in each stage and γ ∈ (0, 1] is the discount rate. In this paper, we consider two types of reward functions R(x, u):\n1) Correlation Maximization: One possible way is to require correlation between mitigation exposures and fake news exposures: people exposed more to fake news should also be exposed more to the true news, to counter the fake news campaign. Therefore, we can form the reward function R in stage k as follows:\nR(xk, uk) = 1\nn Mk(τk+1;xk, uk)>Fk(τk+1;xk, uk).\n2) Difference Minimization: Suppose the goal is to minimize the number of unmitigated fake news events, then we can form a reward function R in stage k as the least squares of unmitigated numbers:\nR(xk, uk) = − 1 n ∥∥∥Mk(τk+1;xk, uk)−Fk(τk+1;xk, uk)∥∥∥2 . These are two sample realizations of the MHP-MDP based intervention one can formulate, among many others. To solve the policy optimization problem argmaxπ V\nπ(x0) for V π defined in (1), we need to evaluate the value function V π for any given policy π, which requires the first and second order statistics (moments) of any multivariate Hawkes processes N(t), as we derive next."
    }, {
      "heading" : "3.2. Second order statistics of non-stationary MHP",
      "text" : "For an n-dim MHPN(t) with standard exponential kernel Φ(t), the following proposition provides closed-form solution of the mean intensity η(t) := E[λ(t)] for both constant and time-varying exogenous intensity µ(t): Proposition 1 (Theorem 3 (Farajtabar et al., 2014; 2016)). Let N(t) be an n-dimensional MHP defined in sec. 2 with exogenous intensity µ and Hawkes kernel Φ(t) = Ae−ωth(t), then the mean intensity η(t) is given by\nη(t) = [ I +A(A− ωI)−1 ( e(A−ωI)t − I )] µ. (2)\nLet Λ(t) = ∫ t\n0 λ(s) ds be the compensator of N(t), then by Doob-Meyer’s decomposition theorem N(t) − Λ(t) is a\nzero mean martingale. This implies that the first order statistics E[N(t)] can be obtained by\nE[N(t)] = E[Λ(t)] = E[ ∫ t\n0\nλ(s) ds] = ∫ t 0 E[λ(s)] ds = ∫ t 0 η(s) ds\nusing eq. (2).\nTo evaluate the reward function R defined previously, we need to derive second order statistics of multivariate Hawkes process N(t) in its non-stationary stage. The following theorem states the key ingredients for the second order statistics. The proof is provided in the appendix.\nTheorem 2. LetN(t) be an n-dim MHP with exogenous intensity µ and Hawkes kernel Φ defined in sec. 2, then the second order statistics of N(t) for t, t′ ≥ 0 is given by\nE [ dN(t) dN(t′)> ] = G(t′, t)>Σ(t′) dtdt′+\nδ(t− t′)Σ(t′) dtdt′ + η(t)η(t′)> dtdt′ (3)\nwhere η(t) = E[λ(t)] is given in (2), Σ(t) = diag([ηi(t)]) is diagonal, and G is the unique solution of\nG(t′, t) = G(t′, t) ∗ Φ(t) + Φ(t− t′)− δ(t− t′)I. (4)\nMoreover G(t′, t)>Σ(t′) = Σ(t)G(t, t′) for all t, t′ ≥ 0.\nBased on Theorem 2, we can compute second order statistics such as E[Ni(t)Nj(t′)] for all i, j and t, t′ ≥ 0."
    }, {
      "heading" : "3.3. State Representation",
      "text" : "Hawkes process is non-Markovian and one needs complete knowledge of the history to characterize the entire process. However, when the standard exponential kernel Φ(t, s) = Ae−ω(t−s)h(t− s) is employed, the effect of history up to time τk on the future t > τk can be cleverly summarized by one scalar per dimension (Farajtabar et al., 2016). For 1 ≤ i ≤ n,\nAlgorithm 1 LSTD policy iteration in point processes Input: set of samples S, feature ψ(·), discount γ repeat\nInitialize Aπ = 0 and bπ = 0. for each state x ∈ S do Aπ ← Aπ + ψ(x)(ψ(x)− γψ(x′))> bπ ← bπ + ψ(x)rπ end for wπ ← (Aπ)−1bπ for each state x ∈ S do π(x)← argmax\nu {E[R(x, u)] + γE[V π(x′)|u,wπ]}\nend for until ‖∆wπ‖ < 0.1 return wπ\ndefine yki := λ k−1 i (τk)− u k−1 i − µi, (and yi0 = 0 by convention), then the intensity due to events of all previous k stages can be written as ∫ τk\n0 Ae−ω(t−s) dN(s) = yke−ω(t−τk). In other words, yk is sufficient to encode the information of\nactivities in the past k stages that are relevant to future. Note that we have two separate ykM and y k F to track the dynamics of both mitigation and fake processes.\nAlso, in order to tackle objectives over multiple stages, we add aggregated number of events at L previous ∆f -time intervals over all dimensions. Define a vector zk ∈ RnL where zk(l−1)n+i = ∫ τk−(l−1)∆f τk−l∆f dNi(s) for 1 ≤ i ≤ n and 1 ≤ l ≤ L. In other words, zk(l−1)n+i records the number of events of i-th dimension in the l-th interval of length ∆f prior to time τk. For example, choosing ∆f = ∆T and setting L = 2 means that events from the two most recent stages are counted. Similarly, we have two separate zkM and z k F corresponding to the two processes. Now, the state vector xk ∈ R2nL+2n is the concatenation of the above four vectors xk = [ykM ; ykF ; zkM ; zkF ]."
    }, {
      "heading" : "3.4. Least Squares Temporal Difference",
      "text" : "The optimal value function satisfies the Bellman equation:\nV π(x) = E[R(x, π(x))] + γE[V π(x′)], (5)\nwhere x′ is the next state after taking action based on policy π at state x. Least squares temporal difference learning (LSTD) is a sample-efficient procedure for policy evaluation, which subsequently facilitates policy improvement. The value function is approximated by V̂ π(x) = ∑D d=1 w π dψd(x), where ψd is the d-th feature of state x and w π d is its coefficient for policy π. This can be compactly represented as V̂ π(x) = ψ(x)>wπ , where ψ(x) = (ψ1(x), . . . , ψD(x))>. The following presents our choice of features, and explains the policy evaluation and improvement steps inspired by LSTD(0) (Sutton & Barto, 1998).\nFeatures. The number of events in a few recent consecutive intervals of point processes have been used as a reliable feature to parameterize point processes (Parikh et al., 2012; Qin & Shelton, 2015; Lian et al., 2015). Following their work we take L prior intervals of length ∆f for each dimension of the fake news process and record the number of events in that period as one feature. ψk(l−1)n+i = z k (l−1)n+i for 1 ≤ i ≤ n and 1 ≤ l ≤ L. This will count for nL features. Similarly we take nL features from the mitigation process. Finally, we add a last feature ψk2nL+1 = 1 as the bias term. Therefore, ψk = [zkM ; z k F ; 1] and the feature space has dimension D = 2nL+ 1.\nPolicy Evaluation. Substituting the approximation into the Bellman equation, we have:\nψ(x)>wπ = E[R(x, π(x))] + γE[ψ(x′)>]wπ. (6)\nTo find the best fit of wπ we have to consider all possible x; however, since the state space is infinite-dimensional, enumerating all states is impossible and we utilize a set S of samples S = {x1, . . . , xS}.\nLet ψ(xs) = ψs ∈ RD, E[ψ(x′s)] = ψ′s ∈ RD, and rπs = E[R(xs, π(xs))] ∈ R. Then define matrices of current features Ψ = [ψ>1 ; . . . ;ψ > S ] > ∈ RS×D and next features Ψ′ = [ψ′>1 ; . . . ;ψ′ > S ] > ∈ RS×D, the rewards rπ = [rπ1 , . . . , rπS ]> ∈ RS ,\nAlgorithm 2 Real-time fake news mitigation Input: network A, learned wπ , feature ψ(·), discount γ repeat\nObserve state x of the network activities u = argmaxa{E[R(x, a)] + γE[V π(x′)|a,wπ]} Add u to base exogenous intensity µ and generate mitigation event times {ti} using point process model Create posts at times {ti} using campaigner accounts\nuntil end of campaign\nand the sample value functions as vπ = [V π(x1), . . . , V π(xS)]> ∈ RS . Appendix C presents how we leverage the first and second order statistics of Hawkes process to find E[R(x, u)] and E[V π(x′)]. Given the above definition, the Bellman optimality of eq. (6) can be written in matrix format:\nvπ = Ψwπ = rπ + γΨ′wπ , Tπvπ, (7)\nwhere Tπ is the Bellman optimality operator. A way to find a good estimate is to force the approximate value function to be a fixed point of the optimality equation under the Bellman operator, i.e., Tπ v̂π ≈ v̂π. (Lagoudakis & Parr, 2003). For that, the fixed point has to lie in the space of approximate value functions, spanned by the basis functions Ψ. v̂π lies in that space by definition, but Tπ v̂π may have an orthogonal component and must be projected. This is achieved by the orthogonal projection operator (Ψ(Ψ>Ψ)−1Ψ>). Therefore the approximate value function v̂π must be invariant under one application of the Bellman operator Tπ followed by orthogonal projection:\nv̂π = Ψ(Ψ>Ψ)−1Ψ>(Tπ v̂π). (8)\nBy substituting the linear approximation Ψwπ = vπ into the above equation and some manipulations, we get a D × D linear systems of equations Aπωπ = bπ , where Aπ = Ψ>(Ψ − γΨ′) and bπ = Ψ>rπ , and whose solution is the fitted coefficients wπ . It has been shown that the estimated wπ converges to the best w∗ as the available number of samples tends to infinity (Bradtke & Barto, 1996). Appendix B presents a detailed derivation.\nPolicy Improvement. The second part of the algorithm implements policy improvement, i.e., getting an improved policy π′ via one-step look-ahead as follows:\nπ′(x) = argmax u E[R(x, u) + γV π(x′)]. (9)\nLSTD(0) alternates between the policy improvement and policy evaluation iteratively until wπ converges (Bradtke & Barto, 1996). Alg. 1 summarizes this procedure.\nLSTD in Hawkes context. LSTD is particularly suitable to the problem we are interested in. It learns the value function V π(x), and as such, policy improvement can be challenging without knowing the model. Because of this, methods that aim to learn the Q-function Qπ(x, u), such as LSPI (Lagoudakis & Parr, 2003), are widely applied. The downside of Q-function based methods is that they typically require more samples than learning the value function. Yet, in our setup, learning the value function is sufficient, by writing the action-value function as Qπ(x, u) = E[R(x, u) + V π(x′)], and observing that the learned model of the multivariate Hawkes process enables analytical computation of the expectation (see Appendix C for details):\nE[V π(x′)] = n∑ i=1 L−1∑ l=1 wπln+iz k−1 M,(l−1)n+i + w π nL+ln+iz k−1 F,(l−1)n+i\n+ n∑ i=1 wiE[zkM,i] + wnL+iE[zkF,i] + wπ2nL+1,\nE[R(x, u)] = 1\nn E[zkM ]>B>B E[zkF ], % correlation\nE[R(x, u)] = − 1 n E[zkM > B>B zkM ]− 1 n E[zkF > B>B zkF ]\n+ 2\nn E[zkM ]>B>B E[zkF ]. % difference\nWe require much fewer samples to learn V π(x) compared to learning an approximate Qπ(x, u), and in particular compared to LSPI we avoid explicitly discretizing the continuous action space from which the action u is chosen.\nWe further remark that the policy improvement step finds the optimal action u at any state x by computing argmaxu E[R(x, u) + V π(x′)], where the action u to be optimized appears in the calculation of both the expected current reward and the expected value at the next state. This optimization problem is convex under our choice of reward functions and the form of the Hawkes conditional intensity.\nAfter learning the optimal policy (implicit by wπ of the linearly-approximated value function) we start at the real-time intervention part. By observing the state we find the optimal intervention intensity by simply solving eq. (9). Alg. 2 summarizes the real-time mitigation procedure."
    }, {
      "heading" : "4. Experiments",
      "text" : "We evaluate our fake news mitigation framework by both simulated and real-time real-world experiments and show our approach significantly outperforms several state-of-the-art methods and alternatives"
    }, {
      "heading" : "4.1. Baselines",
      "text" : "We compared our Least-squares Temporal Difference (LTD) intervention procedure with the followings:\n1) CEC (Farajtabar et al., 2016): This is a recent network intervention algorithm based on point processes. It formulates a dynamic programming problem,\nV k(x) = max uk E[R(xk, uk) + γV π(xk+1)], (10)\nand uses approximate look-ahead dynamic programming implemented via Certainly Equivalence Control (CEC) (Bertsekas, 1995) to find the optimum intervention.\n2) OPL (Farajtabar et al., 2014): An open loop dynamic programming control based on convex optimization that finds the best intervention for all stages in one shot:\nmax u1,u2,...,uK E[ ∑ k γkR(xk, uk)]. (11)\nOpen Loop (OPL) is an important baseline, because comparing it against closed-loop strategies like CEC and LTD indicates how much feedback information helps improve future decisions. It quantifies the value of information in the context of dynamic programming and optimal control.\n3) CLS: For each node i belonging to the mitigation campaign, we compute its closeness centrality centi = 1∑ j dis(i,j)\n, where dis is the shortest distance from i to j. Then, we assign the budget such that ui ∝ centi, meaning that budget is distributed to mitigation nodes based on their proximity to nodes according to network structure. Closeness Centrality (CLS) has been widely used in the literature as a baseline for finding influential nodes (Chen et al., 2012; De Arruda et al., 2014; Gao et al., 2015).\n4) EXP: The CLS baseline is only structural and does not use the fake news infection data. EXP augments it by computing an Exposure-based Closeness Centrality, centki = ∑ j ∑L l=1 F k−l j\ndis(i,j) , where the numerator is the total number of times node j has been exposed to the fake news campaign in the L intervals before stage k. The more times node j has been exposed to the fake news, the more important it is for the mitigation campaign to reach it. EXP assigns the budget according to uki ∝ centki .\n5) RND: This policy assigns a random solution in the convex space of feasible interventions. It serves as a baseline and improvement over this random policy makes comparison feasible across different settings."
    }, {
      "heading" : "4.2. Empirical validation of second order statistics",
      "text" : "In this section, we empirically study the theoretical results of section 3.2. The mean and standard deviation of the empirical second order statistics averaged over 100 simulations was compared to the theoretical mean. This experiment helps to verify that it can be used in simulations to evaluate the merits of our proposed algorithm and versus the baselines. Fig. 2 demonstrates the second order correlation profile of 4 random pairs of users simulated 100 times. We see that the empirical average almost matches the theoretical average. Furthermore, it is interesting to see that the standard deviation increases with time. This is due to the aggregation of more random elements as time passes. Therefore, one should be careful with using the empirical mean without a sufficient number of random runs when the time interval is large."
    }, {
      "heading" : "4.3. Linear approximation accuracy",
      "text" : "In our LSTD algorithm we used a linear approximation for the value-function. One might ask how accurately can linear features approximate the value-function. To this end, we take a sample state x as a state with no prior activity and intensity ψ(x) = (0, . . . , 0, 1). This is the the initial state assumed in our experiment runs. First we empirically found V π(x) under the learned policy by simulating the process 100 times each with 10 stages. We compare the empirical average and the standard deviation of the total reward with the one estimated by the linear approximation ψ>wπ . Fig. 3 shows the results for correlation maximization and difference minimization. In both cases, by increasing the number of samples (used in LSTD), the estimated w leads to better estimation of V π(x). First, the figure shows that we can achieve a reasonable accuracy with a fair amount of samples. Secondly, although it appears that the approximation is converging to the empirical value, we notice that increasing the number of samples beyond 4000 does not improve the error, which maintains a constant distance from 0. We believe this is because the optimal value function does not lie in the linear span of the feature space. Employing more complex features, such as polynomial features and deep neural network based representations, remain as interesting avenues for future work."
    }, {
      "heading" : "4.4. Synthetic Experiments",
      "text" : "Setup. For all except the experiment over network size, the networks were generated synthetically with n = 300 nodes. Endogenous intensity coefficients were set as aij ∼ U [0, 0.5]. To mimic real world networks, sparsity was set to 0.02, i.e., each edge was kept with probability 0.02. The influence matrix was scaled appropriately such that the spectral radius is a random number smaller than one to ensure the stability of the process. The Hawkes kernel parameter was set to ω = 1, which means loosing roughly 63 % of influence after 1 unit of time (minutes, hours, etc). Both fake news and mitigation\nprocesses obey these network settings. Among n nodes, we assume 20 nodes create fake news and another 20 nodes can be incentivized (via the exogenous intensity) to spread true news. Each stage has length of ∆T = 1. The discount factor was set to γ = 0.7. For determining features, we set L = 2 and we choose ∆f = ∆T for simplicity. The upper bound for the intervention intensity was chosen by αi ∼ U [0, 0.5]. The price of each person was cki = 1, and the total budget at stage k was randomly generated as Ck ∼ (n×U [0, 0.5]). 1000 randomly sampled states were used for the LSTD algorithm. To evaluate a policy (learnt by an algorithm) we simulated the network under that policy 50 times and take the discounted total reward averaged over these 50 runs as an empirical valuation of the policy. Furthermore, each single run was simulated for 10 consecutive stages; from the eleventh stage onward, the objectives contribute 0.02 of the total reward and can be safely discarded. For all experiments, the above settings are assumed unless it is explicitly mentioned otherwise.\nIntervention results. Fig. 4 demonstrates the performance of different methods. Performance of a policy is quantified as the ratio of the total reward achieved by running the policy, over the total reward achieved by the random policy (RND). This allows us to compare the effectiveness of the algorithms over a variety of settings. All the results reported are averages over 10 runs with random networks generated according to the above setup. Overall, it is clear that LTD is almost consistently the best. It improves over the random policy by roughly 20 percent. CEC is the second best and shows the effectiveness of multi-stage and closed loop intervention. This validates our intuition that although CEC computes the reward from both fake news and mitigation processes, the lack of explicit features corresponding to previous events in its value function prevents it from learning the reason for the reward. Roughly, OPL is the third best algorithm, due to its negligence of the state and the actual events that occurred. Next, comes the EXP algorithm followed by the CLS. The poor performance of these (compared to others) shows that structural properties are not sufficient to tackle the fake news mitigation problem. EXP is roughly better than CEC because it heuristically takes into account the fake news exposure.\nFig. 4-a shows the performance with respect to increasing network size. The difference between alternative methods and the gap between LTD and others increase with the network size. Furthermore, the performance of all methods show an increase over random policy when the problem size gets larger. This illustrates the fact that efficient distribution of budget matters more when confronted with problems of increasing complexity and size.\nFig. 4-b shows the performance with respect to increasing the mitigation campaign size. Larger campaigns imply greater flexibility of intervention, which can be exploited by clever algorithms to achieve higher performance.\nFig. 4-c shows the performance with respect to increasing sparsity of the network. Interestingly, the performance of all the algorithms move towards to the random policy as the network becomes denser. This can be understood by considering a complete graph, so that no matter how and to whom we distribute the mitigation budget, all the nodes are exposed to the mitigation campaign almost equally. However, since real social networks are usually sparse, the effectiveness of the proposed method stands out.\nFinally, Fig. 4-d shows the performance with respect to the length of an stage. Longer stage lengths increase the potential for a good policy to attain higher reward than a random policy, and this is reflected by the sharp increase and larger performance gap between LTD and others for longer lengths. We observe the same patterns for the distance minimization in Fig. 5 problem and avoid repeating them."
    }, {
      "heading" : "4.5. Real experiments",
      "text" : "In this section we explain our real-time intervention results. To the best of our knowledge, we are the first to employ a real-time experiment to evaluate a point process based social network intervention strategy.\nSetup. Using five Twitter accounts, each of which made five posts on machine learning topics at random times per day for a span of two months (Nov.-Dec. 2016), we accumulated a network of 1894 real users with 23407 directed edges in total. We used this historical data to learn the network parameters {αij , µi} using maximum likelihood (similar to related work (Zhou et al., 2013; Farajtabar et al., 2014)) with one hour as the time resolution and the kernel decay parameter ω set to 0.1. As illustrated in Fig.1 the optimal policy was learned using LSTD and policy improvement. Then the real-time experiment starts: Two of the accounts, interpreted as the source of fake news, continued to behave using the same randomized policy as they did in the data collection stage, while the posting times of the other three accounts were generated from (u1, u2, u3)T , produced by our LTD strategy or a competitor strategy. Each policy was run for 10 stages of length 12 hours. Therefore, DeltaT = ∆f = 12. Since both fake news and mitigation accounts were tweeting random posts on machine learning, we assume negligible bias in the content that can confound the performance. At the end of each stage, all retweets–by users within the network–of the posts made during the two most recent stages were used to construct the feature vector and compute the value function, which was used to find the optimal intervention for the next stage. The methods CEC and OPL belong to the same category, and it has been shown that CEC outperforms OPL in (Farajtabar et al., 2016). Furthermore, EXP and CLS also belong to similar families and our synthetic experiments confirm the superiority of the former. So, to save time in real interventions, we only test CEC from the first and EXP from the second pair, and compare them with the random policy (RND) and with our algorithm (LTD).\nReal-time intervention results. Fig. 6 shows the performance of our results compared to competitors. The results show that our approach outperforms the other three baselines by a reasonable margin. As expected CEC is the second best algorithm with a margin of 5 for the correlation maximization objective. It translates to increase in amount of correlation equal to 5, which is a noticeable amount. Furthermore, in the difference minimization task, our approach reached around 7 in difference. This means that we decreased the difference in exposure to the two processes to less 2.6 per user, which is considerable improvement. For both tasks, LTD made more mitigation posts over all daytime phases than it did over all nighttime phases, whereas the competitor strategies did the opposite. This could be a reason for its better performance. One surprising fact is that the number of retweets by users outside the network, which was not used for our features, can exceed the number of retweets by users within the network. This is because the “hashtag” feature on Twitter allows posts\nto be seen by a much larger set of users, who do not necessarily follow the source accounts. In addition to retweets, users can also “like” a post, indicating that they were exposed to fake or real news; while we measured this, we did not include it in the reward. Future experiments can use these two observations to widen the experimental scope and more accurately measure the effectiveness of a mitigation strategy. Despite having these limitations, our experiment serves as a proof-ofconcept for the applicability of point process based intervention in networks, and–to the best of our knowledge–is the first to verify the superiority of a method in a real-time, real-world intervention setting.\nPrediction evaluation results. The previous part describes the more interesting evaluation scheme of real-time intervention in a social media platform. In this part, we used historical real data to mimic this procedure. We extracted 12 full 10-stage trajectory of events from the 2-month random policy historical data. For any of these 10 pairs, the methods were evaluated according to how well they predict the relative ordering among these 12 trajectories (with respect to the objective function). To evaluate each method, we created a sorted list of these 12 trajectories according to increasing objective, and created a second list sorted by increasing closeness to the intervention method. This closeness is the mean squared error between the prescribed intervention and actual intensity, which we inferred using maximum likelihood. Then, by computing the rank correlation of the two sorted lists, and repeating for each of the five methods, we can find out how well they perform on the prediction task. A better predictor is expected to be a better mitigation strategy. Fig. 7 shows the performance."
    }, {
      "heading" : "5. Discussion",
      "text" : "The use of point process based intervention comes with a tradeoff: on one hand, the stochastic nature of multivariate point processes allows it to model the uncertainty of event occurrences in real-world networks; however, by adopting this stochastic model, the intervention policy can only set the optimal conditional intensity, rather than the precise best times, of fake news mitigation events. This can be improved in future experiments by choosing shorter time intervals for stages in the mitigation campaign. An assumption made in the real-world experiment is that all events by user u are seen by user v if v follows u, meaning that fake or real news events at u are seen by v. While it is true for Twitter that all tweets by u will appear on the home timeline of v who follows u (Twitter, 2016), it is not necessarily true that a follower v will see these tweets (suppose they did not access Twitter that day). Therefore, future experiments can improve the accuracy of reward and performance measurements by estimating the probability of users being online during certain time intervals and seeing tweets from accounts they follow.\nFor future work we would like to incorporate more complex features, such as qaudratic and nonlinear features. Utilizing deep neural nets is also an interesting future work for modeling complex feature set."
    }, {
      "heading" : "A. Proof of Theorem 2",
      "text" : "Proof. Fix node index j and t′ ≥ 0, define gji(t′, t) for all node i and t such that\ngji(t ′, t) dt = E [ dNi(t)|dNj(t′) = 1 ] − δijδ(t− t′) dt− ηi(t) dt (12)\nSince the conditional intensity of Ni(t) is λi(t), we have\ngji(t ′, t) dt = E [ dNi(t)|dNj(t′) = 1 ] − δijδ(t− t′) dt− ηi(t) dt\n= E [ λi(t)|dNj(t′) = 1 ] dt− δijδ(t− t′) dt− ηi(t) dt\nFurthermore, we have λi(t) = µi(t) + ∑n k=1 ∫ t 0 φki(t− s) dNk(s) and hence\nE [ λi(t)|dNj(t′) = 1 ] = µi(t) + n∑ k=1 ∫ t 0 φki(t− s)E[dNk(s)|dNj(t′) = 1]\n= µi(t) + n∑ k=1 ∫ t 0 φki(t− s) [ gjk(t ′, s) ds+ δkjδ(s− t′) ds+ ηk(s) ds ]\n= µi(t) + n∑ k=1 ∫ t 0 φki(t− s)gjk(t′, s) ds+ φji(t− t′) + n∑ k=1 ∫ t 0 φki(t− s)ηk(s) ds\nwhere we applied the definition of gjk in (12) to obtain the second equality. Combining the two equations above and using the fact that ηi(t) = µi(t) + ∑n k=1 ∫ t 0 φki(t− s)ηk(s) ds, we obtain that\ngji(t ′, t) = n∑ k=1 ∫ t 0 φki(t− s)gjk(t′, s) ds+ φji(t− t′)− δijδ(t− t′)\nSince j and t′ are arbitrary, we let G(t′, t) be the matrix such that the (j, i)-th entry of G(t′, t) is gji(t′, t), then we have\nG(t′, t) = G(t′, t) ∗ Φ(t) + Φ(t− t′)− δ(t− t′)I (13)\nNote that the Wiener-Hopf equation (13) determines the unique solution G(t′, t) for all t ≥ t′. Moreover, since MHP is simple and that dNi(t) = 0 or 1 a.s. for all i, we have\nE[dNi(t) dNj(t′)] = Pr ( dNi(t) = 1,dNj(t ′) = 1 )\n= Pr ( dNi(t)|dNj(t′) = 1 ) Pr ( dNj(t ′) = 1 ) = E[dNi(t)|dNj(t) = 1]E[dNj(t′)] (14) = E[dNi(t)|dNj(t) = 1]E[λj(t′)] dt′ = E[dNi(t)|dNj(t) = 1]ηj(t′) dt′ = gji(t ′, t)ηj(t ′) dtdt′ + δijδ(t− t′)ηj(t′) dtdt′ + ηi(t)ηj(t′) dtdt′\nSimilarly, we can switch i and j, and t and t′ to obtain\nE[dNi(t) dNj(t′)] = gij(t, t′)ηi(t) dtdt′ + δijδ(t− t′)ηi(t) dtdt′ + ηi(t)ηj(t′) dtdt′ (15)\nCombining (14) and (15) we have that gji(t ′, t)ηj(t ′) = gij(t, t ′)ηi(t)\ni.e., G(t′, t)>Σ(t′) = Σ(t)G(t, t′), from which G(t′, t) for t < t′ is also uniquely determined. We therefore have\nE [ dN(t) dN(t′)> ] = G(t′, t)>Σ(t′) dtdt′ + δ(t− t′)Σ(t′) dtdt′ + η(t)η(t′)> dtdt′ (16)\nThis completes the proof."
    }, {
      "heading" : "B. Details of Policy Evaluation",
      "text" : "We seek an approximate value function v̂π that is invariant under one application of the Bellman operator Tπ followed by orthogonal projection:\nv̂π = Ψ(Ψ>Ψ)−1Ψ>(Tπ v̂π) (17)\nBy replacing the linear approximation, Ψwπ = vπ , and some manipulations we get:\nΨ(Ψ>Ψ)−1Ψ>(rπ + γΨ′wπ) = Ψwπ Ψ ( (Ψ>Ψ)−1Ψ>(rπ + γΨ′wπ)− wπ ) = 0\n(Ψ>Ψ)−1Ψ>(rπ + γΨ′wπ)− wπ = 0 (Ψ>Ψ)−1Ψ>(rπ + γΨ′wπ) = wπ\nΨ>(rπ + γΨ′wπ) = (Ψ>Ψ)wπ Ψ>(Ψ− γΨ′)︸ ︷︷ ︸ D×D wπ = Ψ>rπ︸ ︷︷ ︸ D×1\nDefining Aπ = Ψ>(Ψ − γΨ′) and bπ = Ψ>rπ the estimated coefficients are the solution of a D × D linear systems of equation: Aπωπ = bπ ."
    }, {
      "heading" : "C. Details of Policy Improvement",
      "text" : "Assume we are at the beginning of stage k.The expected feature vector for the next state x′ is comprised of L intervals per process, out of which L− 1 are observed. Only the most recent interval is not observed and needs to be re-evaluated in expectation sense. To compute E[V π(x′)] have:\nE[V π(x′)] =E[ D∑ d=1 wπdψd(x ′)] (18)\n=E[ ∑\ni=1...n,l=1...L\nwπ(l−1)n+iz k M,(l−1)n+i + w π nL+(l−1)n+iz k F,(l−1)n+i + w π 2nL+1] (19)\n= ∑\ni=1...n,l=1...L−1\nwπln+iz k−1 M,(l−1)n+i + w π nL+ln+iz k−1 F,(l−1)n+i + ∑ i=1...n wiE[zkM,i] + wnL+iE[zkF,i] (20)\n+ wπ2nL+1 (21)\nThen, following (Farajtabar et al., 2016), we obtain\nE[zkM ] = Γ (µM + uk) + Υ ykM (22) E[zkF ] = ΓµF + Υ ykM (23)\nwhere\nΥ = (A− ωI)−1(e(A−ωI)∆ − I) (24) Γ = Υ + (A− ωI)−1(Υ− I∆)/ω; (25)\nTo find E[R(x, u)] for the two different reward functions we have defined\n• Correlation Maximization\nE[R(xk, uk)] = 1\nn E[Mk(τk+1;xk, uk)>Fk(τk+1;xk, uk)] =\n1 n E[zkM > B>B zkF ] (26)\n= 1\nn E[zkM ]>B>B E[zkF ] =\n1\nn\n( Γ(µM + uk) + ΥykM )> B>B ( ΓµF + ΥykF ) (27)\nHere the second line is due to the fact that mitigation campaign and fake news process are independent of each other (given the network model). Note the linear dependence of the the objective on our intervention uk which combined with linear constraints result in a convex optimization problem.\n• Difference Minimization E[R(xk, uk)] =− 1 n E[ ∥∥∥Mk(τk+1;xk, uk)−Fk(τk+1;xk, uk)∥∥∥2 ] (28)\n=− 1 n E[(BzkM −B zkF )>(BzkM −B zkF )] (29) =− 1 n E[zkM > B>B zkM ]︸ ︷︷ ︸\nSecond order moments\n+ 2 n E[zkM ]>B>B E[zkF ]︸ ︷︷ ︸\nFirst order moments\n− 1 n E[zkF > B>B zkF ]︸ ︷︷ ︸\nSecond order moments\n(30)\nThe first and second order moments are computed by (Farajtabar et al., 2014) and Theorem 2, respectively."
    } ],
    "references" : [ {
      "title" : "Social media and fake news in the 2016 election",
      "author" : [ "Allcott", "Hunt", "Gentzkow", "Matthew" ],
      "venue" : "Technical report, National Bureau of Economic Research,",
      "citeRegEx" : "Allcott et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Allcott et al\\.",
      "year" : 2017
    }, {
      "title" : "Value-iteration based fitted policy iteration: learning with a single trajectory",
      "author" : [ "Antos", "András", "Szepesvári", "Csaba", "Munos", "Rémi" ],
      "venue" : "In Approximate Dynamic Programming and Reinforcement Learning,",
      "citeRegEx" : "Antos et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Antos et al\\.",
      "year" : 2007
    }, {
      "title" : "Hawkes model for price and trades high-frequency dynamics",
      "author" : [ "Bacry", "Emmanuel", "Muzy", "Jean-François" ],
      "venue" : "Quantitative Finance,",
      "citeRegEx" : "Bacry et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bacry et al\\.",
      "year" : 2014
    }, {
      "title" : "Second order statistics characterization of hawkes processes and nonparametric estimation",
      "author" : [ "Bacry", "Emmanuel", "Muzy", "Jean-Francois" ],
      "venue" : "arXiv preprint arXiv:1401.0903,",
      "citeRegEx" : "Bacry et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bacry et al\\.",
      "year" : 2014
    }, {
      "title" : "Dynamic programming and optimal control, volume",
      "author" : [ "Bertsekas", "Dimitri P" ],
      "venue" : null,
      "citeRegEx" : "Bertsekas and P.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bertsekas and P.",
      "year" : 1995
    }, {
      "title" : "Linear least-squares algorithms for temporal difference learning",
      "author" : [ "Bradtke", "Steven J", "Barto", "Andrew G" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Bradtke et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Bradtke et al\\.",
      "year" : 1996
    }, {
      "title" : "Identifying influential nodes in complex networks",
      "author" : [ "Chen", "Duanbing", "Lü", "Linyuan", "Shang", "Ming-Sheng", "Zhang", "Yi-Cheng", "Zhou", "Tao" ],
      "venue" : "Physica a: Statistical mechanics and its applications,",
      "citeRegEx" : "Chen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2012
    }, {
      "title" : "Role of centrality for the identification of influential spreaders in complex networks",
      "author" : [ "De Arruda", "Guilherme Ferraz", "Barbieri", "André Luiz", "Rodrı́guez", "Pablo Martı́n", "Rodrigues", "Francisco A", "Moreno", "Yamir", "da Fontoura Costa", "Luciano" ],
      "venue" : "Physical Review E,",
      "citeRegEx" : "Arruda et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Arruda et al\\.",
      "year" : 2014
    }, {
      "title" : "Shaping social activity by incentivizing users",
      "author" : [ "Farajtabar", "Mehrdad", "Du", "Nan", "Gomez-Rodriguez", "Manuel", "Valera", "Isabel", "Zha", "Hongyuan", "Song", "Le" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Farajtabar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Farajtabar et al\\.",
      "year" : 2014
    }, {
      "title" : "Coevolve: A joint point process model for information diffusion and network co-evolution",
      "author" : [ "Farajtabar", "Mehrdad", "Wang", "Yichen", "Rodriguez", "Manuel Gomez", "Li", "Shuang", "Zha", "Hongyuan", "Song", "Le" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Farajtabar et al\\.,? \\Q1954\\E",
      "shortCiteRegEx" : "Farajtabar et al\\.",
      "year" : 1954
    }, {
      "title" : "Multistage campaigning in social networks",
      "author" : [ "Farajtabar", "Mehrdad", "Ye", "Xiaojing", "Harati", "Sahar", "Song", "Le", "Zha", "Hongyuan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Farajtabar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Farajtabar et al\\.",
      "year" : 2016
    }, {
      "title" : "Identifying influential nodes for efficient routing in opportunistic networks",
      "author" : [ "Gao", "Zhenxiang", "Shi", "Yan", "Chen", "Shanzhi" ],
      "venue" : "Journal of Communications,",
      "citeRegEx" : "Gao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2015
    }, {
      "title" : "News Use Across Social Media Platforms 2016",
      "author" : [ "Gottfried", "Jeffrey", "Shearer", "Elisa" ],
      "venue" : "Pew Research Center,",
      "citeRegEx" : "Gottfried et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gottfried et al\\.",
      "year" : 2016
    }, {
      "title" : "Spectra of some self-exciting and mutually exciting point processes",
      "author" : [ "Hawkes", "Alan G" ],
      "venue" : null,
      "citeRegEx" : "Hawkes and G.,? \\Q1971\\E",
      "shortCiteRegEx" : "Hawkes and G.",
      "year" : 1971
    }, {
      "title" : "Hawkestopic: A joint model for network inference and topic modeling from text-based cascades",
      "author" : [ "He", "Xinran", "Rekatsinas", "Theodoros", "Foulds", "James", "Getoor", "Lise", "Liu", "Yan" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent poisson factorization for temporal recommendation",
      "author" : [ "Hosseini", "Seyed Abbas", "Alizadeh", "Keivan", "Khodadadi", "Ali", "Arabzadeh", "Farajtabar", "Mehrdad", "Zha", "Hongyuan", "Rabiee", "Hamid R" ],
      "venue" : "arXiv preprint arXiv:1703.01442,",
      "citeRegEx" : "Hosseini et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Hosseini et al\\.",
      "year" : 2017
    }, {
      "title" : "Smart broadcasting: Do you want to be seen",
      "author" : [ "Karimi", "Mohammad Reza", "Tavakoli", "Erfan", "Farajtabar", "Mehrdad", "Song", "Le", "Gomez-Rodriguez", "Manuel" ],
      "venue" : "arXiv preprint arXiv:1605.06855,",
      "citeRegEx" : "Karimi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Karimi et al\\.",
      "year" : 2016
    }, {
      "title" : "Least-squares policy iteration",
      "author" : [ "Lagoudakis", "Michail G", "Parr", "Ronald" ],
      "venue" : "Journal of machine learning research,",
      "citeRegEx" : "Lagoudakis et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lagoudakis et al\\.",
      "year" : 2003
    }, {
      "title" : "Hawkes processes with stochastic excitations",
      "author" : [ "Lee", "Young", "Lim", "Kar Wai", "Ong", "Cheng Soon" ],
      "venue" : "In Proceedings of The 33rd International Conference on Machine Learning,",
      "citeRegEx" : "Lee et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2016
    }, {
      "title" : "Detecting weak changes in dynamic events over networks",
      "author" : [ "Li", "Shuang", "Xie", "Yao", "Farajtabar", "Mehrdad", "Verma", "Apurv", "Song", "Le" ],
      "venue" : "arXiv preprint arXiv:1603.08981,",
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "A multitask point process predictive model",
      "author" : [ "Lian", "Wenzhao", "Henao", "Ricardo", "Rao", "Vinayak", "Lucas", "Joseph E", "Carin", "Lawrence" ],
      "venue" : "In ICML, pp. 2030–2038,",
      "citeRegEx" : "Lian et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lian et al\\.",
      "year" : 2015
    }, {
      "title" : "Discovering latent network structure in point process data",
      "author" : [ "Linderman", "Scott W", "Adams", "Ryan P" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Linderman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Linderman et al\\.",
      "year" : 2014
    }, {
      "title" : "A parsimonious language model of social media credibility across disparate events",
      "author" : [ "Mitra", "Tanushree", "G Wright", "Gilbert", "Eric" ],
      "venue" : "In Proc. CSCW,",
      "citeRegEx" : "Mitra et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Mitra et al\\.",
      "year" : 2017
    }, {
      "title" : "Addressing hoaxes and fake news, 2016",
      "author" : [ "Mosseri", "Adam" ],
      "venue" : "URL http://newsroom.fb.com/news/2016/12/ news-feed-fyi-addressing-hoaxes-and-fake-news/",
      "citeRegEx" : "Mosseri and Adam.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mosseri and Adam.",
      "year" : 2016
    }, {
      "title" : "Conjoint modeling of temporal dependencies in event streams",
      "author" : [ "Parikh", "Ankur P", "Gunawardana", "Asela", "Meek", "Christopher" ],
      "venue" : "BMAW-12 Preface,",
      "citeRegEx" : "Parikh et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2012
    }, {
      "title" : "Auxiliary gibbs sampling for inference in piecewise-constant conditional intensity models",
      "author" : [ "Qin", "Zhen", "Shelton", "Christian R" ],
      "venue" : "In UAI, pp",
      "citeRegEx" : "Qin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2015
    }, {
      "title" : "This analysis shows how viral fake election news stories outperformed real news on facebook, 2016",
      "author" : [ "Silverman", "Craig" ],
      "venue" : null,
      "citeRegEx" : "Silverman and Craig.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silverman and Craig.",
      "year" : 2016
    }, {
      "title" : "Introduction to Reinforcement Learning",
      "author" : [ "Sutton", "Richard S", "Barto", "Andrew G" ],
      "venue" : null,
      "citeRegEx" : "Sutton et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1998
    }, {
      "title" : "Distilling information reliability and source trustworthiness from digital traces",
      "author" : [ "Tabibian", "Behzdad", "Valera", "Isabel", "Farajtabar", "Mehrdad", "Song", "Le", "Schölkopf", "Bernhard", "Gomez-Rodriguez", "Manuel" ],
      "venue" : "arXiv preprint arXiv:1610.07472,",
      "citeRegEx" : "Tabibian et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tabibian et al\\.",
      "year" : 2016
    }, {
      "title" : "Netcodec: Community detection from individual activities",
      "author" : [ "Tran", "Long", "Farajtabar", "Mehrdad", "Song", "Le", "Zha", "Hongyuan" ],
      "venue" : "In Proceedings of the 2015 SIAM International Conference on Data Mining,",
      "citeRegEx" : "Tran et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2015
    }, {
      "title" : "Steering opinion dynamics in information diffusion",
      "author" : [ "Wang", "Yichen", "Theodorou", "Evangelos", "Verma", "Apurv", "Song", "Le" ],
      "venue" : "networks. CoRR,",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning granger causality for hawkes processes",
      "author" : [ "Xu", "Hongteng", "Farajtabar", "Mehrdad", "Zha", "Hongyuan" ],
      "venue" : "In Proceedings of The 33rd International Conference on Machine Learning,",
      "citeRegEx" : "Xu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Redqueen: An online algorithm for smart broadcasting in social networks",
      "author" : [ "A. Zarezade", "U. Upadhyay", "H. Rabiee", "M. Gomez-Rodriguez" ],
      "venue" : null,
      "citeRegEx" : "Zarezade et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zarezade et al\\.",
      "year" : 2017
    }, {
      "title" : "Correlated cascades: Compete or cooperate",
      "author" : [ "Zarezade", "Ali", "Khodadadi", "Farajtabar", "Mehrdad", "Rabiee", "Hamid R", "Zha", "Hongyuan" ],
      "venue" : "arXiv preprint arXiv:1510.00936,",
      "citeRegEx" : "Zarezade et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zarezade et al\\.",
      "year" : 2015
    }, {
      "title" : "unconfirmed: Classifying rumor stance in crisis-related social media messages",
      "author" : [ "Zeng", "Li", "Starbird", "Kate", "Spiro", "Emma S" ],
      "venue" : "In Tenth International AAAI Conference on Web and Social Media,",
      "citeRegEx" : "Zeng et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2016
    }, {
      "title" : "Enquiring minds: Early detection of rumors in social media from enquiry posts",
      "author" : [ "Zhao", "Zhe", "Resnick", "Paul", "Mei", "Qiaozhu" ],
      "venue" : "In WWW,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning social infectivity in sparse low-rank networks using multi-dimensional hawkes processes",
      "author" : [ "Zhou", "Ke", "Zha", "Hongyuan", "Song", "Le" ],
      "venue" : "In AISTATS, pp",
      "citeRegEx" : "Zhou et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "The emergence of social media as a prominent news source in the past few years raises concomitant concerns about the quality, truthfulness, and credibility of information presented (Mitra et al., 2017).",
      "startOffset" : 181,
      "endOffset" : 201
    }, {
      "referenceID" : 22,
      "context" : "To reduce the amount of labor-intensive manual fact-checking, there have been research efforts devoted to building classifiers to detect factuality of information, predicting credibility level of posts, and detecting controversial information from inquiry phrases (Mitra et al., 2017; Zeng et al., 2016; Zhao et al., 2015).",
      "startOffset" : 264,
      "endOffset" : 322
    }, {
      "referenceID" : 34,
      "context" : "To reduce the amount of labor-intensive manual fact-checking, there have been research efforts devoted to building classifiers to detect factuality of information, predicting credibility level of posts, and detecting controversial information from inquiry phrases (Mitra et al., 2017; Zeng et al., 2016; Zhao et al., 2015).",
      "startOffset" : 264,
      "endOffset" : 322
    }, {
      "referenceID" : 35,
      "context" : "To reduce the amount of labor-intensive manual fact-checking, there have been research efforts devoted to building classifiers to detect factuality of information, predicting credibility level of posts, and detecting controversial information from inquiry phrases (Mitra et al., 2017; Zeng et al., 2016; Zhao et al., 2015).",
      "startOffset" : 264,
      "endOffset" : 322
    }, {
      "referenceID" : 24,
      "context" : "Point process models have been recently used to model activities in social networks (Farajtabar et al., 2015; Parikh et al., 2012; Hosseini et al., 2017; Karimi et al., 2016).",
      "startOffset" : 84,
      "endOffset" : 174
    }, {
      "referenceID" : 15,
      "context" : "Point process models have been recently used to model activities in social networks (Farajtabar et al., 2015; Parikh et al., 2012; Hosseini et al., 2017; Karimi et al., 2016).",
      "startOffset" : 84,
      "endOffset" : 174
    }, {
      "referenceID" : 16,
      "context" : "Point process models have been recently used to model activities in social networks (Farajtabar et al., 2015; Parikh et al., 2012; Hosseini et al., 2017; Karimi et al., 2016).",
      "startOffset" : 84,
      "endOffset" : 174
    }, {
      "referenceID" : 33,
      "context" : "More especially Hawkes process (Hawkes, 1971) is a class of selfand mutually exciting point processes that has been applied to variety of problems in social networks including cascade modeling (Zarezade et al., 2015), reliability of crowd generated data (Tabibian et al.",
      "startOffset" : 193,
      "endOffset" : 216
    }, {
      "referenceID" : 28,
      "context" : ", 2015), reliability of crowd generated data (Tabibian et al., 2016), social media popularity (Rizoiu et al.",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 29,
      "context" : "), community detection (Tran et al., 2015), causal inference (Xu et al.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 31,
      "context" : ", 2015), causal inference (Xu et al., 2016), linguistic influence (Guo et al.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 19,
      "context" : ", 2015), and change point detection in social networks (Li et al., 2016).",
      "startOffset" : 55,
      "endOffset" : 72
    }, {
      "referenceID" : 8,
      "context" : "Steering user activities by adding external incentives to the exogenous intensity of Hawkes processes was first considered in (Farajtabar et al., 2014).",
      "startOffset" : 126,
      "endOffset" : 151
    }, {
      "referenceID" : 10,
      "context" : "In (Farajtabar et al., 2016), a multistage campaigning method to optimally distribute incentive resources based on dynamic programming was developed.",
      "startOffset" : 3,
      "endOffset" : 28
    }, {
      "referenceID" : 30,
      "context" : "Recent work has also applied methods in stochastic differential equations to the context of point processes, to find the best intensity for information guiding (Wang et al., 2016) and achieving highest visibility (Zarezade et al.",
      "startOffset" : 160,
      "endOffset" : 179
    }, {
      "referenceID" : 32,
      "context" : ", 2016) and achieving highest visibility (Zarezade et al., 2017).",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "To address the randomness of Hawkes processes, batch reinforcement learning using samples collected from the trajectory of a fixed behavior policy can be applied (Antos et al., 2007).",
      "startOffset" : 162,
      "endOffset" : 182
    }, {
      "referenceID" : 14,
      "context" : "Hawkes process is a doubly stochastic point process with self-excitations, meaning that past events increase the chance of arrivals of new events (Hawkes, 1971), and has been extensively used to model activities in social networks (Farajtabar et al., 2015; Linderman & Adams, 2014; He et al., 2015; Rizoiu et al.; Lee et al., 2016).",
      "startOffset" : 231,
      "endOffset" : 331
    }, {
      "referenceID" : 18,
      "context" : "Hawkes process is a doubly stochastic point process with self-excitations, meaning that past events increase the chance of arrivals of new events (Hawkes, 1971), and has been extensively used to model activities in social networks (Farajtabar et al., 2015; Linderman & Adams, 2014; He et al., 2015; Rizoiu et al.; Lee et al., 2016).",
      "startOffset" : 231,
      "endOffset" : 331
    }, {
      "referenceID" : 8,
      "context" : "Second order statistics of non-stationary MHP For an n-dim MHPN(t) with standard exponential kernel Φ(t), the following proposition provides closed-form solution of the mean intensity η(t) := E[λ(t)] for both constant and time-varying exogenous intensity μ(t): Proposition 1 (Theorem 3 (Farajtabar et al., 2014; 2016)).",
      "startOffset" : 286,
      "endOffset" : 317
    }, {
      "referenceID" : 10,
      "context" : "However, when the standard exponential kernel Φ(t, s) = Ae−ω(t−s)h(t− s) is employed, the effect of history up to time τk on the future t > τk can be cleverly summarized by one scalar per dimension (Farajtabar et al., 2016).",
      "startOffset" : 198,
      "endOffset" : 223
    }, {
      "referenceID" : 24,
      "context" : "The number of events in a few recent consecutive intervals of point processes have been used as a reliable feature to parameterize point processes (Parikh et al., 2012; Qin & Shelton, 2015; Lian et al., 2015).",
      "startOffset" : 147,
      "endOffset" : 208
    }, {
      "referenceID" : 20,
      "context" : "The number of events in a few recent consecutive intervals of point processes have been used as a reliable feature to parameterize point processes (Parikh et al., 2012; Qin & Shelton, 2015; Lian et al., 2015).",
      "startOffset" : 147,
      "endOffset" : 208
    }, {
      "referenceID" : 10,
      "context" : "Baselines We compared our Least-squares Temporal Difference (LTD) intervention procedure with the followings: 1) CEC (Farajtabar et al., 2016): This is a recent network intervention algorithm based on point processes.",
      "startOffset" : 117,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : "2) OPL (Farajtabar et al., 2014): An open loop dynamic programming control based on convex optimization that finds the best intervention for all stages in one shot:",
      "startOffset" : 7,
      "endOffset" : 32
    }, {
      "referenceID" : 6,
      "context" : "Closeness Centrality (CLS) has been widely used in the literature as a baseline for finding influential nodes (Chen et al., 2012; De Arruda et al., 2014; Gao et al., 2015).",
      "startOffset" : 110,
      "endOffset" : 171
    }, {
      "referenceID" : 11,
      "context" : "Closeness Centrality (CLS) has been widely used in the literature as a baseline for finding influential nodes (Chen et al., 2012; De Arruda et al., 2014; Gao et al., 2015).",
      "startOffset" : 110,
      "endOffset" : 171
    }, {
      "referenceID" : 36,
      "context" : "We used this historical data to learn the network parameters {αij , μ} using maximum likelihood (similar to related work (Zhou et al., 2013; Farajtabar et al., 2014)) with one hour as the time resolution and the kernel decay parameter ω set to 0.",
      "startOffset" : 121,
      "endOffset" : 165
    }, {
      "referenceID" : 8,
      "context" : "We used this historical data to learn the network parameters {αij , μ} using maximum likelihood (similar to related work (Zhou et al., 2013; Farajtabar et al., 2014)) with one hour as the time resolution and the kernel decay parameter ω set to 0.",
      "startOffset" : 121,
      "endOffset" : 165
    }, {
      "referenceID" : 10,
      "context" : "The methods CEC and OPL belong to the same category, and it has been shown that CEC outperforms OPL in (Farajtabar et al., 2016).",
      "startOffset" : 103,
      "endOffset" : 128
    } ],
    "year" : 2017,
    "abstractText" : "We propose the first multistage intervention framework that tackles fake news in social networks by combining reinforcement learning with a point process network activity model. The spread of fake news and mitigation events within the network is modeled by a multivariate Hawkes process with additional exogenous control terms. By choosing a feature representation of states, defining mitigation actions and constructing reward functions to measure the effectiveness of mitigation activities, we map the problem of fake news mitigation into the reinforcement learning framework. We develop a policy iteration method unique to the multivariate networked point process, with the goal of optimizing the actions for maximal total reward under budget constraints. Our method shows promising performance in real-time intervention experiments on a Twitter network to mitigate a surrogate fake news campaign, and outperforms alternatives on synthetic datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}