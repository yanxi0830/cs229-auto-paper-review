{
  "name" : "1609.08281.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Efficient Method for Robust Projection Matrix Design",
    "authors" : [ "Tao Honga", "Zhihui Zhub" ],
    "emails" : [ "hongtao@cs.technion.ac.il", "zzhu@mines.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Our objective is to efficiently design a robust projection matrix Φ for the Compressive Sensing (CS) systems when applied to the signals that are not exactly sparse. The optimal projection matrix is obtained by mainly minimizing the average coherence of the equivalent dictionary. In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a novel penalty function independent of a particular SRE matrix. Without requiring of training data, we can efficiently design the robust projection matrix and apply it for most of CS systems, like a CS system for image processing with a conventional wavelet dictionary in which the SRE matrix is generally not available. Simulation results demonstrate the efficiency and effectiveness of the proposed approach compared with the state-of-the-art methods. In addition, we experimentally demonstrate with natural images that under similar compression rate, a CS system with a learned dictionary in high dimensions outperforms the one in low dimensions in terms of reconstruction accuracy. This together with the fact that our proposed method can efficiently work in high dimension suggests that a CS system can be potentially implemented beyond the small patches in sparsity-based image processing.\nKeywords: Robust projection matrix, sparse representation error (SRE), high dimensional dictionary, mutual coherence."
    }, {
      "heading" : "1. Introduction",
      "text" : "Since the beginning of this century, Compressive Sensing or Compressed Sensing (CS) has received a great deal of attention [1] - [6]. Generally speaking, CS is a mathematical framework that addresses accurate recovery of a signal vector x ∈ℜN from a set of linear measurements\ny = Φx ∈ℜM (1)\nwhere M N and Φ ∈ ℜM×N is referred to as the projection or sensing matrix. CS has found many applications in the areas such as image processing, machine learning, pattern recognition, signal detection/classification etc. We refer the reader to [5] [6] and the references therein to find the related topics mentioned above.\nSparsity and coherence are two important concepts in CS theory. We say a signal x of interest approximately sparse (in some basis or dictionary) if we can approximately express it as a linear combination of few columns (also called atoms) from a well-chosen dictionary:\nx = Ψθ+ e (2)\nwhere Ψ ∈ ℜN×L is the given or determined dictionary, θ ∈ ℜL is a sparse coefficient vector with few non-zero elements, and e ∈ℜN stands for the sparse representation error (SRE). In particular, the vector x is called (purely or exactly) K-sparse in\nEmail addresses: hongtao@cs.technion.ac.il (Tao Hong), zzhu@mines.edu (Zhihui Zhu)\nΨ if ‖θ‖0 = K and e = 0 and approximately K-sparse in Ψ if ‖θ‖0 = K and e has relatively small energy. Here, ‖θ‖0 denotes the number of non-zero elements in θ and 0 represents a vector whose entries are equivalent to 0 . Through this paper, we say θ is K-sparse if ‖θ‖0 = K regardless whether e = 0.\nSubstituting the sparse model (2) of x into (1) gives\ny = ΦΨθ+Φe , Dθ+Φe (3)\nwhere the matrix D =ΦΨ is referred to as the equivalent dictionary of the CS system and ε , Φe denotes the projection noise caused by SRE. The goal of a CS system is to retrieve θ (and hence x) from the measurements y. Due to the fact that M L, solving y ≈Dθ for θ is an undetermined problem which has an infinite number of solutions. By utilizing the priori knowledge that θ is sparse, a CS system typically attempts to recover θ by solving the following problem:\nθ = argmin θ̃ ‖θ̃‖0, s.t. ‖y−Dθ̃‖2 ≤ ‖ε‖2 (4)\nwhich can be solved by many efficient numerical algorithms including basis pursuit (BP), orthogonal matching pursuit (OMP), least absolute shrinkage and selection operator (LASSO) etc. All of the methods can be found in [5] [7] and the references therein.\nTo ensure exact recovery of θ through (4), we need certain conditions on the equivalent dictionary D. One of such conditions is related to the concept of mutual coherence. The mutual coherence of a matrix D ∈ℜM×L is denoted by\nµ(D), max 1≤i, j≤L |Ḡ(i, j)| (5)\nPreprint submitted to Signal Processing September 7, 2017\nar X\niv :1\n60 9.\n08 28\n1v 3\n[ cs\n.L G\n] 6\nS ep\n2 01\n7\nwhere Ḡ = D̄T D̄ is called the Gram matrix of D̄ = DSsc with Ssc a diagonal scaling matrix such that each column of D̄ is of unit length. Here T represents the transpose operator. It is known that µ(D) is lower bounded by the Welch bound µ(D) = √ L−M M(L−1) , i.e., µ(D) ∈ [√ L−M M(L−1) ,1 ] . The mutual coherence µ(D) measures the worst-case coherence between any two columns of D and is one of the fundamental quantities associated with the CS theory. As shown in [5], when there is no projection noise (i.e., ε = 0), any K-sparse signal θ can be exactly recovered by solving the linear system (4) as long as\nK < 1 2\n[ 1+\n1 µ(D)\n] (6)\nwhich indicates that a smaller µ(D) ensures a CS system to recover the signal with a larger K. Thus, [8] [9] proposed methods to design a dictionary with small mutual coherence. For a given dictionary Ψ, the mutual coherence of the equivalent dictionary is actually determined or controlled by the projection matrix Φ. So it would be of great interest to design Φ such that µ(D) is minimized. Another similar indicator used to evaluate the average performance of a CS system is named average mutual coherence µav. The definition of µav is given as follows:\nµav(D), ∑∀(i, j)∈Sav |Ḡ(i, j)|\nNav\nwhere Sav , {(i, j) : µ̄ ≤ |Ḡ(i, j)|} with 0 ≤ µ̄ < 1 as a prescribed parameter and Nav is the number of components in the index set Sav.\nThere has been much effort [10] - [14] devoted to designing an optimal Φ that outperforms the widely used random matrix in terms of signal recovery accuracy (SRA). However, all these methods are based on the assumption that the signal is exactly sparse under a given dictionary, which is not true for practical applications. It is experimentally observed that the sensing matrix designed by [10] - [14] based on mutual coherence results in inferior performance for real images (which are generally approximately but not exactly sparse under a well-chosen dictionary). To address this issue, the recent work in [15] [16] proposed novel methods to design a robust projection matrix when the SRE exists.1 Through this paper, similar to what is used in [15] [16], a robust projection (or sensing) matrix means it is designed with consideration of possible SRE and hence the corresponding CS system yields superior performance when the SRE e in (2) is not nil. However, the approaches in [15] [16] need the explicit value of the SRE on the training dataset, making them inefficient in several aspects. First, many practical CS systems with predefined analytical dictionaries (e.g., the wavelet dictionary, and the modulated discrete prolate spheroidal sequences (DPSS) dictionary for sampled multiband signals [17]) actually do not involve any training dataset and hence no SRE available.\n1We note that the approaches considered in [15] [16] share the same framework. The difference is that in [16] the authors utilized an efficient iterative algorithm giving an approximate solution, while a closed form solution is derived in [15].\nIn order to design the robust projection matrix for these CS systems using the framework presented in [15] [16], one has to first construct plenty of extra representative dataset for the explicit SRE with the given dictionary, which limits the range of applications. Second, even for the CS system with a dictionary learned typically on a large-scale dataset, we need a lot of memories and computations to store and compute with the huge dataset as well its corresponding SRE for designing a robust sensing matrix. Moreover, if the CS system is applied to a dynamic dataset, e.g., video stream, it is practically impossible to store all the data and compute its corresponding SRE. Therefore, the requirement of the explicit value of SRE for the training dataset makes the methods in [15] [16] limited and inefficient for all the cases discussed above.\nIn this paper, to drop the requirement of the training dataset as well as its SRE, we propose a novel robust projection matrix framework only involving a predefined dictionary. With this new framework, we can efficiently design projection matrices for the CS systems mentioned above. We stress that by efficient method for robust projection matrix design (which is the title of this paper), we are not providing an efficient method for solving the problems in [15] [16]; instead we provide a new framework in which the training dataset and its corresponding SRE are not required any more. Experiments on synthetic data and real images demonstrate the proposed sensing matrix yields a comparable performance in terms of SRA compared with the ones obtained by [15] [16].\nBefore proceeding, we first briefly introduce some notation used throughout the paper. MATLAB notations are adopted in this paper. In this connection, for a vector, v(k) denotes the k-th component of v. For a matrix, Q(i, j) means the (i, j)-th element of matrix Q, while Q(k, :) and Q(:,k) indicate the k-th row and column vector of Q, respectively. We use I and IL to denote an identity matrix with arbitrary and L×L dimension, respectively. The k-th column of Q is also denoted by qk. trace(Q) denotes the calculation of the trace of Q. The Frobenius norm of\na given matrix Q is ‖Q‖F = √ ∑i, j ‖Q(i, j)‖2 = √\ntrace(QT Q) where T represents the transpose operator. The definition of lp\nnorm for a vector v ∈ℜN is ‖v‖p , ( N ∑\nk=1 |v(k)|p\n) 1 p\n, p≥ 1.\nThe remainder is arranged as follows. Some preliminaries are given in Section 2 to state the motivation of developing such a novel model. The proposed model which does not need the SRE is shown in Section 3 and the corresponding optimal sensing problem is solved in this section. The synthetic and real data experiments are carried out in Section 4 to demonstrate the efficiency and effectiveness of the proposed method. Some conclusions are given in Section 5 to end this paper."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "A sparsifying dictionary Ψ for a given dataset {xk}Pk=1 is usually obtained by considering the following problem\n{Ψ,θk}= argmin Ψ̃,θ̃k\nP\n∑ k=1 ‖xk− Ψ̃θ̃k‖22 s.t. ‖θ̃k‖0 ≤ K (7)\nwhich can be addressed by some practical algorithms [18], among which the popularly utilized are the K-singular value decomposition (K-SVD) algorithm [19] and the method of optimal direction (MOD) [20]. As stated in the previous section, the SRE ek = xk−Ψθk is generally not nil. We concatenate all the SRE {ek} into an N×P matrix:\nE , X −ΨΘ\nwhich is referred to as the SRE matrix corresponding to the training dataset {xk} and the learned dictionary Ψ.\nThe recent work in [15] [16] attempted to design a robust projection matrix with consideration of the SRE matrix E and proposed to solve\nΦ = argmin Φ̃ ‖IL−ΨT Φ̃ T Φ̃Ψ‖2F +λ‖Φ̃E‖2F (8)\nor Φ = arg min\nΦ̃,G∈Hξ ‖G−ΨT Φ̃T Φ̃Ψ‖2F +λ‖Φ̃E‖2F (9)\nwhere Hξ is the set of relaxed equiangular tight frames (ETFs):\nHξ = {G|G = GT , G(i, i) = 1,∀ i,max i, j |G(i, j)| ≤ ξ}.\nCompared to (8) which requires the Gram matrix of the equivalent dictionary close to an identity matrix, (9) relaxes the requirement of coherence between the equivalent dictionary but is much harder to solve. See [15] [16] for more discussions on this issue.\nWe remark that to ensure the designed sensing matrix by (8) or (9) be robust to the SRE for all the signals of interest, the SRE matrix E should be well representative, i.e., we need sufficient number of training signals xk. As stated in [15] [16], these methods ((8) and (9)) can be applied naturally when the dictionary is learned by algorithms like K-SVD with plenty of training data {xk}, since the SRE E is available without any additional effort. However, this could be prohibitive when the CS system with an analytic dictionary is applied to some arbitrary signals (but still they are approximately sparse in this dictionary), since there are no sufficient number of data available to obtain the SRE matrix E . For example, one may only want to apply the CS system to an arbitrary image with the wavelet dictionary. Also, these methods are prohibitive for a dictionary trained on large datasets with millions of training samples and in a dynamic CS system for streaming signals. To train such a dictionary, we have to conduct online algorithms [21] - [23] which typically apply stochastic gradient method where in each iteration a randomly selected tiny part of the training signals called mini-batch instead of the whole data is utilized for computing the expected gradient. In these cases, additional efforts are needed to obtain the SRE matrix E and it is usually prohibitive to compute and store E for all the training dataset. All of these situations make the approach proposed in [15] [16] become limited.\nAiming to obtain a neural network well expressing the signals of interest, an empirical strategy widely used by deep learning community is to utilize a huge training dataset so that the\nnetwork can extract more important features and avoid overfitting. Similar to this phenomenon, a dictionary trained on a huge dataset is also expected to contain more features of the represented signal. Simulation results shown in Section 4 demonstrate that a CS system with such a dictionary and designing a projection matrix designed on this dictionary yields a higher reconstruction accuracy on natural images than the one with a dictionary obtained from a small dataset. The recent work in [26] [27] stated that a dictionary learned with larger patches (e.g., 64×64) on a huge dataset can better capture features in natural images.2 In this paper, we also attempt to experimentally investigate the performance of designing a projection matrix on a high dimensional dictionary. These also motivate us to develop an efficient method for designing a robust sensing matrix without the requirement of the SRE matrix E as it is not easy to obtain for the above two situations.\nIn the next section, we provide a novel framework to efficiently design a robust sensing matrix, and more importantly, it can be applied to the situation when the SRE matrix E is not available."
    }, {
      "heading" : "3. A Novel Approach to Projection Matrix Design",
      "text" : "In this section, we provide an efficient robust sensing matrix design approach which drops the requirement of training signals and their corresponding SRE matrix E . Our proposed framework is actually inspired by (8) and (9) from the following two aspects."
    }, {
      "heading" : "3.1. A Novel Framework for Robust Projection Matrix Design",
      "text" : "First note that the energy of SRE ‖E‖2F is usually very small since the learned sparsifying dictionary is assumed to sparsely represent a signal well as in (2). Otherwise if ‖E‖2F is very large, it indicates that the dictionary is not well designed for this class of signals and it is possible that this class of signal can not be recovered from the compressive measurements no matter what projection matrix is utilized. It follows from the norm consistent property that\n‖ΦE‖F ≤ ‖Φ‖F‖E‖F (10)\nwhich implies informally that a smaller sensing matrix ‖Φ‖F yields a smaller projected SRE ‖ΦE‖F .\nAlso as illustrated before that the amount of training data should be sufficient so that they can represent the class of targeted signals, the energy in the corresponding SRE matrix E should spread out in every elements. In other words, one can view the expected SRE as an additive Guassian white noise. In this situation, we have the following result.\n2The dimension of a dictionary in such a case becomes high compared with the moderate dictionary size shown in [19]. In fact, the name of a high dimensional dictionary in this paper means the dictionary obtained by training on a larger size of represented signal.\nLemma 1. Suppose E (:,k)= ek,∀k = 1, · · ·P are i.i.d Gaussian random vectors with each of zero mean and covariance σ2I . Then for any Φ ∈ RM×N , we have\nE [ ‖ΦE‖2F ] = Pσ2‖Φ‖2F . (11)\nwhere E denotes the expectation operator. Moreover, when the number of training samples P approaches to ∞, we have ‖ΦE‖ 2 F\nP converges in probability and almost surely to σ2‖Φ‖2F . In particular,\n√ p ( ‖ΦE‖2F\nP −σ2‖Φ‖2F\n) d−→N (0,2σ2‖ΦΦT ‖2F) (12)\nwhere N (µ,ς) denotes the Gaussian distribution of mean µ and variance ς, and d−→ means convergence in distribution.\nProof. For each k, we first define dk = Φek. Since ek ∼ N (0,σ2I), we have dk ∼ N (0,σ2ΦΦT ). Let ΦΦT = QΛQT be an eigendecomposition of ΦΦT , where Λ is an M×M diagonal matrix with the non-negative eigenvalues λ1, . . . ,λM along its diagonal. We have\n‖dk‖22 = ‖QT dk‖22\nand QT dk ∼N (0,σ2Λ).\nFor convenience, we define new random variables c = QT dk and b1 = 1λ1σ2 c 2(1),b2 = 1λ1σ2 c 2(2), . . . ,bM = 1λ1σ2 c\n2(M). It is clear that b1, b2, . . . , bM are independent random variables of χ21 distribution, the chi-squared distribution with 1 degree of freedom.\nNow we compute the mean of ‖Φek‖2F :\nE[‖Φek‖2F ] = E[‖dk‖2F ] = E[‖c‖2F ]\n= M\n∑ i=1\nλiσ2E[bi] = M\n∑ i=1 λiσ2\n= trace(σ2ΦΦT ) = σ2‖Φ‖2F\n(13)\nwhere the second line we utilize E[χ21] = 1. The variance of ‖Φek‖2F is given by:\nVar[‖Φek‖2F ] = Var[‖dk‖2F ] = Var[‖c‖2F ]\n= M\n∑ i=1 λ2i σ 4Var[bi] = 2\nM\n∑ i=1 λ2i σ 4\n= 2trace(σ4ΦΦT ΦΦT )\n= 2σ4‖ΦΦT ‖2F\n(14)\nwhere the second line we utilize Var[χ21] = 2, and the third line follows because\nΦΦT ΦΦT = QΛ2QT .\nThus, we obtain (11) by noting that\nE[‖ΦE‖2F ] = E\n[ P\n∑ k=1 ‖Φek‖22\n] = Pσ2‖Φ‖2F\nIt follows from (13) and (14) that { ‖Φe1‖22, . . . ,‖ΦeP‖22 } is a sequence of independent and identically distributed random variable drawn from distributions of expected values given by σ2‖Φ‖2F and variances given by 2σ4‖ΦΦ\nT ‖2F . Thus, by the law of large numbers [29], the average ‖ΦE‖ 2 F\nP converges in probability and almost surely to the expected value σ2‖Φ‖2F as P→ ∞. Finally, the central limit theorem [29] establishes that as P approaches infinity, the random variables √ P( ‖ΦE‖ 2 F\nP −σ 2‖Φ‖2F)\nconverges in distribution to a normal N (0,2σ2‖ΦΦT ‖2F).\nIn words, Lemma 1 indicates that when the number of training samples approaches to infinity, ‖ΦE‖2F is proportional to ‖Φ‖2F . Inspired by (10)-(12), it is expected that without any training signals and their corresponding SRE matrix E , a robust projection matrix can be obtained by solving the following problem\nΦ = argmin Φ̃\nf (Φ̃)≡ ‖IL−ΨT Φ̃ T Φ̃Ψ‖2F +λ‖Φ̃‖2F (15)\nor\nΦ = arg min Φ̃,G∈Hξ\nf (Φ̃,G)≡ ‖G−ΨT Φ̃T Φ̃Ψ‖2F +λ‖Φ̃‖2F (16)\nHere, with abuse of notation, we use both f to denote the objective function in (15) and (16). However, it should be clear from the context as we always use f (Φ̃) to represent the one in (15) and f (Φ̃,G) to represent the one in (16). Since the more training samples can better represent the signals of interest and the SRE, (12) indicates that the sensing matrices obtained by (15) and (16) are more robust to SRE than the ones obtained by (8) and (9). This is demonstrated by experiments in Section 4. The numerical algorithms is presented to solve (15) and (16) in the following section.\n3.2. Efficient Algorithms for Solving (15) and (16) Note that f (Φ̃) is a special case of f (Φ̃,G) with G = IL. Thus, we first consider solving\nmin Φ̃\nf (Φ̃,G) = ‖G−ΨT Φ̃T Φ̃Ψ‖2F +λ‖Φ̃‖2F (17)\nwith an arbitrary G. To that end, we introduce a low-rank minimization problem\nmin A\ng(A,G)≡ ‖G−ΨT AΨ‖2F +λtrace(A)\ns.t. rank(A)≤M,A 0 (18)\nBy eigendecomposition of A, it is clear that (17) is equivalent to (18). The problem (17) is often referred to as the factor problem of (18). Also note that g(A,G) is a convex function of A for any fixed G, though the problem (18) is nonconvex because of the rank constraint. The recent work [24] has shown that a number of iterative algorithms (including gradient descent) can provably solve the factored problem (i.e., (17)) for a set of low-rank matrix optimizations (i.e., (18)). Thus, in this paper,\nthe Conjugate-Gradient (CG) [30] method is utilized to solve (17).3 The gradient of f (Φ̃,G) in terms of Φ̃ is given as follows:\n∇Φ̃ f (Φ̃,G) = 2λΦ̃−4Φ̃ΨGΨ T +4Φ̃ΨΨT Φ̃T Φ̃ΨΨT (19)\nAfter obtaining the gradient of f (Φ̃,G), the toolbox minFunc4 [25] is utilized to solve (17) with CG method. We note that the gradient-based method only involves simple matrix multiplication in (19), without requiring performing SVD and matrix inversion. Hence it is also suitable for designing the projection matrix for a CS system working on high dimensional signals.\nWe now turn to solve (16) which has two variables Φ̃ and G ∈ Hξ. A widely used strategy for such problems is the alternating minimization [10] [11] [15] [16]. The main idea behind alternating minimization for (16) is that we keep one variable constant (say Φ̃), and optimize over the other variable (say G). Once G is fixed, as we explained before, we utilize CG method to solve (17). On the other hand, the solution to minG f (Φ̃,G) can be simply obtained by projecting the Gram matrix of the equivalent dictionary onto the set Hξ when we fix Φ̃. The main steps of the algorithm are outlined in Algorithm 1.\nAlgorithm 1\nInitialization: Set k = 1, Φ0 as a random one and the number of iterations Iter.\nStep I: Set G̃k = ΨT ΦTk−1Φk−1Ψ and then project it onto the set Hξ:\nGk(i, j) =  1, i = j, G̃k(i, j), i , j, |G̃k(i, j)| ≤ ξ, ξ · sign(G̃k(i, j)), i , j, |G̃k(i, j)|> ξ\nwhere sign(·) is a sign function.\nStep II: Solve Φk = argminΦ̃ f (Φ̃,Gk) with CG. If k < Iter, set k = k+1 and go to Step I. Otherwise, terminate the algorithm and output ΦIter.\nRemarks:\n• It is clear that this approach is independent of training data and can be utilized for most of CS systems as long as the sparsifying dictionary Ψ is given.\n• Even in the case where the SRE matrix E is available, it is much easier and more efficient to solve (15) than (8) since typically the number of columns in E is dramatically greater than the size of Ψ and Φ, i.e., P M,N,L.\n3We note that both of the methods shown in [15] [16] for solving (17) need to calculate the inversion of ΨΨT . However, in practice, the learned dictionary sometimes is ill-conditioned, which may cause numerical instable problem if directly applying their methods. Thus, as global convergence of many local search algorithms for solving similar low-rank optimizations is guaranteed in [24], CG is chosen to solve (17). Obviously, if the aforementioned problem does not happen in practical cases, the method in [15] [16] can be used to address (15). Moreover, we will show that CG and the methods shown in [15] [16] yield a similar solution in the following experiments.\n4We note that minFunc is a stable toolbox that can be efficiently applied with millions of variables.\n• Simulation results with synthetic data and natural images (where the SRE matrix E is available) show that the proposed method also yields a comparable performance to or outperforms the methods in [15] [16] in terms of SRA. Moreover, the experiments on natural images show that designing a projection matrix on a given dictionary which is learned with large dataset or high-dimensional training signals can improve SRA significantly with the same compression rate MN . However, it requires a great deal of memories to store the SRE matrix E for either large dateset or high-dimensional training data."
    }, {
      "heading" : "4. Simulation Results",
      "text" : "In this section, we perform a set of experiments on synthetic data and natural images to demonstrate the performance of the CS system with projection matrix designed by the proposed methods. For convenience, the corresponding CS systems are denoted by CSMT with Φ̃ obtained via (15) and CSMT−ET F with Φ̃ obtained via (16), and are compared with the following CS systems: CSrandn with a random projection matrix, CSLH with the sensing matrix obtained via (8) [15], CSLH−ET F with the sensing matrix obtained via (9) [15], and CSDCS [28]. It was first proposed in [28] that simultaneously optimizing Φ and Ψ for a CS system results in better performance in terms of SRA. In the sequel, we also examine this strategy in natural images and the corresponding CS system is denoted by CSS−DCS.5 For simplicity, the parameter ξ in Hξ is set to Welch bound in the following experiments.\nThe SRA is evaluated in terms of the peak signal-to-noise ratio (PSNR) [5]\nρpsnr , 10× log10 [ (2r−1)2\nρmse\n] dB\nwith r = 8 bits per pixel. We also utilized the measure ρmse:\nρmse , 1\nN×P\nP\n∑ k=1 ‖x̃k− xk‖22\nwhere xk is the original signal, x̃k = Ψθ̃k stands for the reconstructed signal with θ̃k the solution of (4), and P is the number of patches in an image or the testing data."
    }, {
      "heading" : "A. Synthetic Data Experiments",
      "text" : "An N×L dictionary Ψ is generated with normally distributed entries and then is normalized so that each column has unit l2 norm. We also generate a random M×N matrix Φ0 (where each entry has Gaussian distribution of zero-mean and variance 1) as the initial condition for all of the aforementioned projection matrices. Φ0 is also utilized as the sensing matrix in CSrandn.\n5In our experiment, the coupling factor utilized in CSS−DCS is set to 0.5 which is the best value in our setting. Generally speaking, CSS−DCS should have a best performance in terms of SRA because it optimizes projection matrix and dictionary simultaneously. Thus, the performance of CSS−DCS serves as the indicator of the best performance can be achieved by other CS systems that only consider optimizing the projection matrix.\nThe synthetic data for training and testing is obtained as follows. A set of 2P K-sparse vectors {θk ∈ ℜL} is generated as the sparse coefficients where each non-zero elements of {θk} is randomly positioned with a Gaussian distribution of zero-mean and unit variance. The set of signal vectors {xk} is produced with xk = Ψsk + ek , x (0) k + ek, ∀k, where Ψ is the given dictionary and ek is the random noise with Gaussian distribution of zero-mean and variance σ2e to yield different signal-to-noise ration (SNR) (in dB) of the signals. Clearly, x(0)k is exactly Ksparse in Ψ, while xk is approximately K-sparse in Ψ.\nDenote X = X (0)+∆ as the signal matrix of dimension N× 2P, where X (0)(:,k) = x(0)k and ∆(:,k) = ek. We use the SRE matrix E = ∆(:,1 : P) in (8) and (9) whose solutions are used for CSLH and CSLH−ET F , respectively. The data X (:,P+ 1 : 2P) is utilized for testing the CS systems. The measurements {yk} are obtained by yk = ΦX (:,P+k), ∀k ∈ (0,P] where Φ is the projection matrix of the CS systems. For simplicity, OMP is chosen to solve the sparse coding problem throughout the experiments.\nWith the synthetic data, we conduct three set of experiments to demonstrate the performance of our proposed framework for robust projection matrix design, i.e., (15) and (16). In these three set of experiments, we respectively show the convergence of CG method, the effect of λ and the signal recovery accuracy of the proposed projection matrices CSMT and CSMT−ET F versus different SNR of the signals.\n1) Convergence Analysis: Let M = 20, N = 60, L = 100 and K = 4.We utilize CG to solve (15). We note that a random dictionary with well-conditioned is chosen and thus we also compute the closed-form solution shown in [15] for (15). The objective value obtained by the closed-form solution is denoted by f ∗ and is compared with the CG method. The evolution of f (Φ) for different λ is shown in Figure 1. We note that different λ results in different functions f (Φ) and hence different f ∗. We observe global convergence of CG method for solving (15) with all the choices of λ.\n2) The Choice of λ: With M = 20, N = 60, L = 80, K = 4 and SNR = 15 dB, we check the effect of the trade-off parameter λ in terms of ρmse for CSMT and CSMT−ET F . The λ is chosen from 0 to 2 with step size 0.01. The evaluation of ρmse versus different λ is depicted in Figure 2. Remark 1:\n• As seen from Figure 2, different λ yields different performance in terms of ρmse for this practical situation where the SNR is 15dB. It is clear that a proper choice of λ results in significantly better performance than other values, especially for CSMT−ET F . Clearly, the advantage of the proposed method is shown by comparing the cases for λ = 0 and other values of λ as the former corresponds to the traditional approaches which do not take the SRE into account. In the sequel, we simplicity search the best λ (with which the CS systems attain the minimal ρmse for the test data) within (0,1] for each experiment setting.\n• According to this experiment, if λ is well chosen, CSMT−ET F has better performance than CSMT in terms of\nρmse. However, the performance of CSMT−ET F is more sensitive than with λ than CSMT . We will show in the next experiment that the performance of CSMT−ET F outperforms CSMT in synthetic data when the SNR is not too small. However, for natural images which have relatively large SRE, CSMT always has better performance than CSMT−ET F . This phenomenon is also observed for CSLH and CSLH−ET F in [15] [16]. Thus, we only consider the performance of CSMT and CSLH for the natural images in next section.\n3) Signal Recovery Accuracy Evaluation: With M = 20, N =\n60, L = 80, K = 4 and P = 1000, we compare our CS systems CSMT and CSMT−ET F with other CS systems for SNR varying from 5 to 45 dB. Figure 3 displays signal reconstruction error ρmse versus SNR for all six CS systems.\nRemark 2:\n• It is clear that the sensing matrices obtained via (15) and (16) have at least similar performance to the ones obtained via (8) and (9) [15, 16], though our proposed framework does not utilize the SRE matrix E . We also observe that CSMT−ET F outperforms CSLH−ET F when SNR is larger than 15 dB. This demonstrates the effectiveness of our proposed framework in Section (3) and verifies our argument in Lemma 1 that the sparse representation error is not explicitly required.\n• As seen from Figure 3, CSMT has slightly better performance than CSMT−ET F when the SNR is smaller than 15 dB. In other words, we recommend to utilize CSMT (with the sensing matrix obtained via (15)) when the sparse representation error is relatively large, e.g., natural images, which meets our claims in Remark 1."
    }, {
      "heading" : "B. Natural Images Experiments",
      "text" : "In this section, three set of experiments are conducted on natural images. Through these experiments, we verify the effectiveness of the proposed framework for robust sensing matrix design in Section 3 and demonstrate the reason for dropping the requirement on the SRE matrix E . As we explained before, since the SRE is relatively large for natural images, CSMT and CSLH are respectively superior to CSMT−ET F and CSLH−ET F . Thus, we only show the results for CSMT and CSLH .\nIn the first set of experiments, we compare the performance of CSMT and CSLH when a set of training signals and the corresponding SRE matrix E are available. In the second set of experiments, we design the projection matrix with a dictionary\nlearned on a much larger training dataset. The performance of CS systems with a higher dimensional dictionary is given in the Experiment C. We observe that a CS system with a higher dimensional dictionary and a projection matrix designed by our proposed algorithm yields better SRA under the same compression rate. Both training and testing datasets used in these three set of experiments are extracted as follows from the LabelMe database [31]. Note that Data I is extracted with small patches and Data II is obtained by sample larger patches for the third experiment.\nTraining Data I: A set of 8×8 non-overlapping patches is obtained by randomly extracting 400 patches from each image in the whole LabelMe training dataset. We arrange each patch of 8×8 as a vector of 64×1. A set of 400×2920 = 1.168×106 training samples is obtained to train the sparsifying dictionary.\nTesting Data I: A set of 8× 8 non-overlapping patches is obtained by randomly extracting 15 patches from 400 images in LableMe testing dataset as the testing data.\nTraining Data II: The training data contains a set of 16× 16 non-overlapping patches which are obtained by randomly extracting 400 patches from the whole images in the LabelMe training dataset. Each 16× 16 patch is then arranged as a length-256 vector. A set of 1.168×106 training samples is utilized.\nTesting Data II: The testing data is extracted in the same way for the training data but from the LabelMe testing dataset. We randomly extract 8000 testing samples from 400 images with each sample an 16×16 non-overlapping patch.\nExperiment A: small dataset and low dimensional dictionary\nWe perform the same experiment as in [15] to demonstrate the effectiveness of the proposed CS system CSMT without using the SRE E . The training data is obtain by randomly chosen 6000 samples from Training Data I and the K-SVD algorithm is used to train the dictionary Ψ.\nSimilar to [15], the parameters M, N, L and K are set to 20, 64, 100 and 4, respectively. The trade-off parameter λ in CSLH is set to 0.1 to yield a highest ρpsnr for Testing Data I. We also set λ = 0.1 for the proposed CS system CSMT .\nThe behavior of the five projection matrices in terms of mutual coherence and projection noise is examined and shown in Table 1. In order to illustrate the effectiveness of the proposed projection matrix, ten natural images are conducted to check its performance in terms of PSNR. The results are shown in Table 2.\nRemark 3:\n• As seen from Table 1 , the results are self-explanatory. It shows that CSMT has small ‖Φ‖F and also small projection noise ‖ΦE‖F . This supports the proposed idea of using ‖Φ‖F as a surrogate of ‖ΦE‖F to design the robust projection matrix.\n• As shown in Table 2, we observe that CSMT outperforms CSLH in terms of ρpsnr for most of the tested images.\nWe note that as long as an image can be approximately sparsely represented by the learned dictionary Ψ, it is expected that the CS system CSMT yields reasonable performance for this image since the sensing matrix utilized in CSMT considers almost all the patterns of the SRE rather than a fixed one (as indicated by (12)) and thus is robust to SRE.\nWe also observe that CSS−DCS has highest ρpsnr; this is because CSS−DCS simultaneously optimizes the projection matrix and the sparsifying dictionary. It is of interest to note that CSS−DCS also has small ‖Φ‖F and ‖ΦE‖F (as shown in Table 1). This again indicates that it is reasonable to minimize ‖Φ‖F to get small projection noise ‖ΦE‖F .\nExperiment B: large dataset and low dimensional dictionary In this set of experiments, we first learn a dictionary on largescale training samples, i.e., Training Data I, and then design the projection matrices with the learned dictionary. As discussed in the previous section, the large-scale training dataset makes it inefficient or even impossible to compute the SRE matrix E . Therefore, it is inefficient to utilize the methods in [15] [16] as they require the SRE matrix E . Similar reason holds for CSS−DCS. Fortunately, the following results show that the proposed CS system CSMT performs comparably to CSLH .\nThe online dictionary learning algorithm in [22] [23] is chosen to train the sparsifying dictionary on the whole Training Data I. For a fair comparison, we calculate the SRE E off-line for CSLH in this experiment.6 The same M, N, L, K in Experiment A are used in this experiment. λ = 0.9 and λ = 1e− 3 are selected for CSMT and CSLH , respectively. We note that the choice of λ for CSLH is very sensitive to E . This is because the two terms ‖IL−ΨT Φ̃\nT Φ̃Ψ‖2F and ‖Φ̃E‖2F in (8) for CSLH have different physical meanings and more importantly, the second term ‖Φ̃E‖2F increases when we have more number of training data, while the first term is independent of the training data. Thus, we need to decrease λ for CSLH when we increase the number of training data. Remark 4:\n• As shown in Table 3, benefiting from large-scale training samples, the performance of both CSLH and CSMT has been improved compared with the one in Table 2. Moreover, we also observe that CSMT performs similarly to CSLH . It is also of interest to note that the PSNR for CSMT in Table 3 is higher than the one for CSS−DCS in Table 2 for most of the tested images. This suggests that if the dictionary and the projection matrix are simultaneously optimized by online algorithm with large dataset, the performance of the corresponding CS system can be further improved since joint optimization (CSS−DCS) is expected to have better performance than only optimizing projection matrix with a given dictionary (CSMT ) under the same\n6In order to compare with CSLH , we still compute the SRE matrix E for the training data though it requires abundant of extra storage and computation resources.\nsettings. We note that the proposed framework for projection matrix design can be utilized for online simultaneous optimization of the dictionary and the projection matrix. Investigation along this direction is on-going.\n• We compare the computational complexity of our proposed method with the one in [15] [16]. The later mainly consists of two more steps: the calculations of the SRE matrix E and EE T . Calculating E involves the OMP algorithm [32] with computational complexity of O ( PKNL(KL logL+K3) ) , where we repeat that\nP, N, L and K denote the number of samples, the dimension of signal, the number of atoms in dictionary and the sparsity level, respectively. The complexity for calculating EE T is O ( PN2 ) . Thus, compared with CSMT , CSLH needs at least more computational time of O ( PNK2L2 logL+PNLK4 +PN2 ) . In the set of next experiments, we will show the advantage of designing the projection matrix on a high dimensional dictionary. With N and L increasing, the efficiency of the proposed method CSMT becomes more distinct.\nExperiment C: large dataset and high dimensional dictionary\nInspired by the work in [26], we attempt to design the projection matrix on a high dimensional dictionary in this set of experiments. The reason to utilize a high dimensional dictionary is as follows. The sparse representation of a natural image X can be written as,\nX = X̃ +E X̃ , ΨΘ\nwhere E is the sparse representation error.7 We first recover Θ by solving a set of (3) and then take X̃ = ΨΘ as the recovered image. It is clear that no matter what projection matrix is utilized, the best we can obtain is X̃ instead of X . Thus, with a dictionary which can capture more information of the training dataset and better represent X with X̃ , the corresponding CS system is excepted to yield a higher SRA. As stated in [26], training the dictionary with larger patches results in smaller sparse representation errors for natural images. However, training dictionary on larger patches, we have to train on a large-scale dataset to better represent the signals of interest. This demonstrates the efficiency of the proposed method for designing a robust projection matrix on a high dimensional dictionary as this method drops the requirement of the SRE matrix E which is not only in high dimension, but also large-scale.\nThe parameters M, N, L, K and λ are set to 80, 256, 800, 16 and 0.5, respectively. Due to the fact that CSMT has a similar performance with CSLH and the choice of λ for CSLH is very sensitive to E , we omit the performance of CSLH in this experiment. The simulation results are presented in Table 4. In order to demonstrate the visual effect clearly, two images Lena\n7Since OMP is used to conduct the sparse coding mission in this paper, each column of Θ is exactly K-sparse.\nand Mandrill are shown in Figs. 4 and 5, respectively. For a clear comparison, We choose the projection matrices and corresponding dictionary which yields the highest average ρpsnr from Table 2 to Table 4 in Figs. 4 and 5. Remark 5:\n• We observe that for the CS systems CSrandn and CSDCS, designing the projection matrix on a high dimensional dictionary results in similar performance to what is shown in Experiments A and B with the same compression rate MN . Moreover, CSDCS has lower ρpsnr in Experiments B and C than Experiment A. However, the proposed CS system CSMT has increasing ρpsnr from Experiment A to Experiment C. This indicates the effectiveness of the proposed method for a CS system with a higher dimensional dictionary.\n• We also investigate the influence of the parameters M, L , K to the above mentioned CS systems. The simulation results on Testing Data II are given in Fig.s 6 to 8. As can\nbe observed, the proposed CS system CSMT has highest ρpsnr among the three CS systems.\n• The recent work in [33] states that it is possible to train the dictionary on millions of training signals whose dimension is also more than one million. The proposed method can be utilized to design a robust projection matrix on such high dimensional dictionaries since it gets rid of the requirement of the SRE matrix E . Note that in this case, more efforts for efficiently solving (15) are needed. A full investigation regarding this direction belongs to a future work.\nThree sets of experiments on natural images are conducted to illustrate the effectiveness and efficiency of the proposed framework in Section 3. A dictionary trained on a larger dataset can better represent the signal and the corresponding CS system yields better performance in terms of SRA. Additionally, a high dimensional dictionary has more freedom to represent the signals of interest. The CS system with a high dimensional dictionary and a projection matrix obtained by the proposed\nmethod results in higher ρpsnr. However, both cases need to train the dictionary on a large-scale training dataset, making it inefficient or even impossible for computing the SRE matrix E . One of the main contributions in this paper is proposing a new framework that is independent of the SRE matrix E ."
    }, {
      "heading" : "5. Conclusion",
      "text" : "This paper considers the problem of designing a robust projection matrix for the signals that are not exactly sparse. A novel cost function is proposed to decrease the influence of SRE for the measurements and at the same time is independent of training data and the corresponding SRE matrix (the independence of training data saves computations for practical designing). As shown in Lemma 1, we state that discarding the\nSRE matrix in designing procedure is reasonable as it is equivalent to the case when we have infinite number of training samples. We thus utilize ‖Φ‖2F as an surrogate to the projected SRE ‖ΦE‖2F to design the sensing matrices. The performance of designing projection matrices with dictionaries either learned on large-scale training dataset or of high dimension is experimentally examined. The simulation results on synthetic data and natural images demonstrate the effectiveness and efficiency of the proposed approach. It is of interest to note that the proposed method yields better performance when we increase the dimension of the dictionary, which surprisingly is not true for the other methods.\nOur proposed framework for designing robust sensing matrices—which shares similar structure to that in [15] [16]— simultaneously minimizes the surrogate of sparse representation error (SRE) and the mutual coherence of the CS systems. Thus we need extract effort (like Figure 2) to find an optimal λ that well balances these two terms. An ongoing research is to come up with a new framework without requiring balancing the tradeoff between minimizing the mutual coherence and decreasing the projected SRE."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This research is supported in part by ERC Grant agreement no. 320649, and in part by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI). The code in this paper to represent the experiments can be downloaded through the link https://github.com/happyhongt/"
    } ],
    "references" : [ {
      "title" : "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information,",
      "author" : [ "E.J. Candès", "J. Romberg", "T. Tao" ],
      "venue" : "IEEE Trans. Inf. Theory,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Near optimal signal recovery from random projections: Universal encoding strategies,",
      "author" : [ "E.J. Candès", "T. Tao" ],
      "venue" : "IEEE Trans. Inf. Theory,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Compressed sensing,",
      "author" : [ "D.L. Donoho" ],
      "venue" : "IEEE Trans. Inf. Theory,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "An introduction to compressive samping,",
      "author" : [ "E.J. Candès", "M.B. Wakin" ],
      "venue" : "IEEE Signal Process. Mag.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Sparse and Redundant Representations: from theory to applications in signal and image processing",
      "author" : [ "M. Elad" ],
      "venue" : "Springer Science & Business Media",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "`1-`2 Optimization in Signal and Image Processing,",
      "author" : [ "M. Zibulevsky", "M. Elad" ],
      "venue" : "IEEE Signal Process. Mag",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Learning incoherent dictionaries for sparse approximation using iterative projections and rotations,",
      "author" : [ "D. Barchiesi", "M.D. Plumbley" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "and A",
      "author" : [ "G. Li", "Z. Zhu", "H. Bai" ],
      "venue" : "Yu, “A new framework for designing incoherent sparsifying dictionaries,” in IEEE Conf. Acous., Speech, Signal Process.(ICASSP), pp. 4416-4420",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Optimized projections for compressed sensing,",
      "author" : [ "M. Elad" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "A gradient-based alternating minimzation approach for optimization of the measurement matrix in compressive sensing,",
      "author" : [ "V. Abolghasemi", "S. Ferdowsi", "S. Sanei" ],
      "venue" : "Signal Process.,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Design of projection matrix for compressive sensing by nonsmooth optimization,",
      "author" : [ "W.-S. Lu", "T. Hinamoto" ],
      "venue" : "IEEE International Symposium Circuits and Systems (ISCAS),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Projection matrix optimization for block-sparse compressive sensing,",
      "author" : [ "S. Li", "Z. Zhu", "G. Li", "L. Chang", "Q. Li" ],
      "venue" : "IEEE Conf. Signal Process., Communicaton and Computation (ICSPCC),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "On projection matrix optimization for compressive sensing systems,",
      "author" : [ "G. Li", "Z.H. Zhu", "D.H. Yang", "L.P. Chang", "H. Bai" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Designint robust sensing matrix for image compression,",
      "author" : [ "G. Li", "X. Li", "S. Li", "H. Bai", "Q. Jiang", "X. He" ],
      "venue" : "IEEE Trans. Image Process.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "An efficient algorithm for designing projection matrix in compressive sensing based on alternating optimization,",
      "author" : [ "T. Hong", "H. Bai", "S. Li", "Z. Zhu" ],
      "venue" : "Signal Process.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Approximating sampled sinusoids and multiband signals using multiband modulated DPSS dictionaries,",
      "author" : [ "Z. Zhu", "M.B. Wakin" ],
      "venue" : "J. Fourier Analysis Appl.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Dictionary Learning,",
      "author" : [ "I. Tosic", "P. Frossard" ],
      "venue" : "IEEE Signal Process. Mag.,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation,",
      "author" : [ "M. Aharon", "M. Elad", "A. Bruckstein" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Hakon-housoy, “Method of optimal direction for frame design,",
      "author" : [ "K. Engan", "J.H.S.O. Aase" ],
      "venue" : "Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.(ICASSP),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1999
    }, {
      "title" : "Online algorithms and stochastic approximations,",
      "author" : [ "L. Botou" ],
      "venue" : "Online Learning and Neural Networks,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1998
    }, {
      "title" : "J",
      "author" : [ "J. Mairal", "F. Bach" ],
      "venue" : "Ponce and G. Sapiro, “Online dictionary learning for sparse coding,” Proceedings of the 26th annual international conference on machine learning ACM (ICML), pp. 689-696",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Online learning for matrix factorization and sparse coding,",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro" ],
      "venue" : "Journal of Machine Learning,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "and M",
      "author" : [ "Z. Zhu", "Q. Li", "G. Tang" ],
      "venue" : "B. Wakin, “Global Optimality in Lowrank Matrix Optimization,” arXiv preprint, arXiv:1702.07945",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "minFunc: unconstrained differentiale multivariate optimization in Matlab.",
      "author" : [ "M. Schmidt" ],
      "venue" : "https://www.cs.ubc.ca/ ̃schmidtm/ Software/minFunc.html,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2005
    }, {
      "title" : "Trainlets: dictionary learning in high dimensions,",
      "author" : [ "J. Sulam", "B. Ophir", "M. Zibulevsky", "M. Elad" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "Large inpainting of face images with trainlets,",
      "author" : [ "J. Sulam", "M. Elad" ],
      "venue" : "IEEE Signal Processing Letter,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "Learning to sense sparse sig-  nals: simultaneous sensing matrix ans sparsifying dictionary optimization,",
      "author" : [ "J.M. Duarte-Carvajalino", "G. Sapiro" ],
      "venue" : "IEEE Trans. Image Process.,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2009
    }, {
      "title" : "Statistical Inference",
      "author" : [ "G. Casella", "L.B. Roger" ],
      "venue" : "Vol. 2. Pacific Grove, CA: Duxbury",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Numerical Optimization",
      "author" : [ "J. Nocedal", "S. Wright" ],
      "venue" : "Springer",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "LabelMe: A Database and Web-Based Tool for Image Annotation,",
      "author" : [ "B.C. Russell", "A. Torralba", "K.P. Murphy", "W.T. Freeman" ],
      "venue" : "International Journal of Computation Vision,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2008
    }, {
      "title" : "M",
      "author" : [ "R. Rubinstein" ],
      "venue" : "Zibulevsky and M. Elad, “Efficient implementation of the K-SVD algorithm and the Batch-OMP method,” Department of Computer Science, Technion, Israel, Tech. Rep.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "B",
      "author" : [ "A. Mensch", "J. Mairal" ],
      "venue" : "Thirion and G. Varoquaux, “Dictionary learning for massive matrix factorization,” Proceedings of the 33th annual international conference on machine learning ACM (ICML)",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a novel penalty function independent of a particular SRE matrix.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 14,
      "context" : "In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a novel penalty function independent of a particular SRE matrix.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : "Since the beginning of this century, Compressive Sensing or Compressed Sensing (CS) has received a great deal of attention [1] - [6].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "We refer the reader to [5] [6] and the references therein to find the related topics mentioned above.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "All of the methods can be found in [5] [7] and the references therein.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "All of the methods can be found in [5] [7] and the references therein.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "As shown in [5], when there is no projection noise (i.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 6,
      "context" : "Thus, [8] [9] proposed methods to design a dictionary with small mutual coherence.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 7,
      "context" : "Thus, [8] [9] proposed methods to design a dictionary with small mutual coherence.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "There has been much effort [10] - [14] devoted to designing an optimal Φ that outperforms the widely used random matrix in terms of signal recovery accuracy (SRA).",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 12,
      "context" : "There has been much effort [10] - [14] devoted to designing an optimal Φ that outperforms the widely used random matrix in terms of signal recovery accuracy (SRA).",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "It is experimentally observed that the sensing matrix designed by [10] - [14] based on mutual coherence results",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "It is experimentally observed that the sensing matrix designed by [10] - [14] based on mutual coherence results",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : "To address this issue, the recent work in [15] [16] proposed novel methods to design a robust projection matrix when the SRE exists.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "To address this issue, the recent work in [15] [16] proposed novel methods to design a robust projection matrix when the SRE exists.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "1 Through this paper, similar to what is used in [15] [16], a robust projection (or sensing) matrix means it is designed with consideration of possible SRE and hence the corresponding CS system yields superior performance when the SRE e in (2) is not nil.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "1 Through this paper, similar to what is used in [15] [16], a robust projection (or sensing) matrix means it is designed with consideration of possible SRE and hence the corresponding CS system yields superior performance when the SRE e in (2) is not nil.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 13,
      "context" : "However, the approaches in [15] [16] need the explicit value of the SRE on the training dataset, making them inefficient in several aspects.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : "However, the approaches in [15] [16] need the explicit value of the SRE on the training dataset, making them inefficient in several aspects.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : ", the wavelet dictionary, and the modulated discrete prolate spheroidal sequences (DPSS) dictionary for sampled multiband signals [17]) actually do not involve any training dataset and hence no SRE available.",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 13,
      "context" : "1We note that the approaches considered in [15] [16] share the same framework.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "1We note that the approaches considered in [15] [16] share the same framework.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 14,
      "context" : "The difference is that in [16] the authors utilized an efficient iterative algorithm giving an approximate solution, while a closed form solution is derived in [15].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : "The difference is that in [16] the authors utilized an efficient iterative algorithm giving an approximate solution, while a closed form solution is derived in [15].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : "In order to design the robust projection matrix for these CS systems using the framework presented in [15] [16], one has to first construct plenty of extra representative dataset for the explicit SRE with the given dictionary, which limits the range of applications.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "In order to design the robust projection matrix for these CS systems using the framework presented in [15] [16], one has to first construct plenty of extra representative dataset for the explicit SRE with the given dictionary, which limits the range of applications.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "Therefore, the requirement of the explicit value of SRE for the training dataset makes the methods in [15] [16] limited and inefficient for all the cases discussed above.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "Therefore, the requirement of the explicit value of SRE for the training dataset makes the methods in [15] [16] limited and inefficient for all the cases discussed above.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "We stress that by efficient method for robust projection matrix design (which is the title of this paper), we are not providing an efficient method for solving the problems in [15] [16]; instead we provide a new framework in which the training dataset and its corresponding SRE are not required any more.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 14,
      "context" : "We stress that by efficient method for robust projection matrix design (which is the title of this paper), we are not providing an efficient method for solving the problems in [15] [16]; instead we provide a new framework in which the training dataset and its corresponding SRE are not required any more.",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 13,
      "context" : "Experiments on synthetic data and real images demonstrate the proposed sensing matrix yields a comparable performance in terms of SRA compared with the ones obtained by [15] [16].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 14,
      "context" : "Experiments on synthetic data and real images demonstrate the proposed sensing matrix yields a comparable performance in terms of SRA compared with the ones obtained by [15] [16].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 16,
      "context" : "which can be addressed by some practical algorithms [18], among which the popularly utilized are the K-singular value decomposition (K-SVD) algorithm [19] and the method of optimal direction (MOD) [20].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "which can be addressed by some practical algorithms [18], among which the popularly utilized are the K-singular value decomposition (K-SVD) algorithm [19] and the method of optimal direction (MOD) [20].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 18,
      "context" : "which can be addressed by some practical algorithms [18], among which the popularly utilized are the K-singular value decomposition (K-SVD) algorithm [19] and the method of optimal direction (MOD) [20].",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 13,
      "context" : "The recent work in [15] [16] attempted to design a robust projection matrix with consideration of the SRE matrix E and proposed to solve",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "The recent work in [15] [16] attempted to design a robust projection matrix with consideration of the SRE matrix E and proposed to solve",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 13,
      "context" : "See [15] [16] for more discussions on this issue.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 14,
      "context" : "See [15] [16] for more discussions on this issue.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 13,
      "context" : "As stated in [15] [16], these methods ((8) and (9)) can be applied naturally when the dictionary is learned by algorithms like K-SVD with plenty of training data {xk}, since the SRE E is available without any additional effort.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 14,
      "context" : "As stated in [15] [16], these methods ((8) and (9)) can be applied naturally when the dictionary is learned by algorithms like K-SVD with plenty of training data {xk}, since the SRE E is available without any additional effort.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "To train such a dictionary, we have to conduct online algorithms [21] - [23] which typically apply stochastic gradient method where in each iteration a randomly selected tiny part of the training signals called mini-batch instead of the whole data is utilized for computing the expected gradient.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 21,
      "context" : "To train such a dictionary, we have to conduct online algorithms [21] - [23] which typically apply stochastic gradient method where in each iteration a randomly selected tiny part of the training signals called mini-batch instead of the whole data is utilized for computing the expected gradient.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "All of these situations make the approach proposed in [15] [16] become limited.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "All of these situations make the approach proposed in [15] [16] become limited.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 24,
      "context" : "The recent work in [26] [27] stated that a dictionary learned with larger patches (e.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 25,
      "context" : "The recent work in [26] [27] stated that a dictionary learned with larger patches (e.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "2The dimension of a dictionary in such a case becomes high compared with the moderate dictionary size shown in [19].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 27,
      "context" : "Thus, by the law of large numbers [29], the average ‖ΦE‖ 2 F P converges in probability and almost surely to the expected value σ‖Φ‖F as P→ ∞.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : "Finally, the central limit theorem [29] establishes that as P ap-",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 22,
      "context" : "The recent work [24] has shown that a number of iterative algorithms (including gradient descent) can provably solve the factored problem (i.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 28,
      "context" : "the Conjugate-Gradient (CG) [30] method is utilized to solve (17).",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 23,
      "context" : "∇Φ̃ f (Φ̃,G) = 2λΦ̃−4Φ̃ΨGΨ T +4Φ̃ΨΨT Φ̃ Φ̃ΨΨT (19) After obtaining the gradient of f (Φ̃,G), the toolbox minFunc4 [25] is utilized to solve (17) with CG method.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "A widely used strategy for such problems is the alternating minimization [10] [11] [15] [16].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "A widely used strategy for such problems is the alternating minimization [10] [11] [15] [16].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 13,
      "context" : "A widely used strategy for such problems is the alternating minimization [10] [11] [15] [16].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 14,
      "context" : "A widely used strategy for such problems is the alternating minimization [10] [11] [15] [16].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "3We note that both of the methods shown in [15] [16] for solving (17) need to calculate the inversion of ΨΨT .",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "3We note that both of the methods shown in [15] [16] for solving (17) need to calculate the inversion of ΨΨT .",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 22,
      "context" : "Thus, as global convergence of many local search algorithms for solving similar low-rank optimizations is guaranteed in [24], CG is chosen to solve (17).",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : "Obviously, if the aforementioned problem does not happen in practical cases, the method in [15] [16] can be used to address (15).",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "Obviously, if the aforementioned problem does not happen in practical cases, the method in [15] [16] can be used to address (15).",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 13,
      "context" : "Moreover, we will show that CG and the methods shown in [15] [16] yield a similar solution in the following experiments.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 14,
      "context" : "Moreover, we will show that CG and the methods shown in [15] [16] yield a similar solution in the following experiments.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : "• Simulation results with synthetic data and natural images (where the SRE matrix E is available) show that the proposed method also yields a comparable performance to or outperforms the methods in [15] [16] in terms of SRA.",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 14,
      "context" : "• Simulation results with synthetic data and natural images (where the SRE matrix E is available) show that the proposed method also yields a comparable performance to or outperforms the methods in [15] [16] in terms of SRA.",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 13,
      "context" : "For convenience, the corresponding CS systems are denoted by CSMT with Φ̃ obtained via (15) and CSMT−ET F with Φ̃ obtained via (16), and are compared with the following CS systems: CSrandn with a random projection matrix, CSLH with the sensing matrix obtained via (8) [15], CSLH−ET F with the sensing matrix obtained via (9) [15], and CSDCS [28].",
      "startOffset" : 268,
      "endOffset" : 272
    }, {
      "referenceID" : 13,
      "context" : "For convenience, the corresponding CS systems are denoted by CSMT with Φ̃ obtained via (15) and CSMT−ET F with Φ̃ obtained via (16), and are compared with the following CS systems: CSrandn with a random projection matrix, CSLH with the sensing matrix obtained via (8) [15], CSLH−ET F with the sensing matrix obtained via (9) [15], and CSDCS [28].",
      "startOffset" : 325,
      "endOffset" : 329
    }, {
      "referenceID" : 26,
      "context" : "For convenience, the corresponding CS systems are denoted by CSMT with Φ̃ obtained via (15) and CSMT−ET F with Φ̃ obtained via (16), and are compared with the following CS systems: CSrandn with a random projection matrix, CSLH with the sensing matrix obtained via (8) [15], CSLH−ET F with the sensing matrix obtained via (9) [15], and CSDCS [28].",
      "startOffset" : 341,
      "endOffset" : 345
    }, {
      "referenceID" : 26,
      "context" : "It was first proposed in [28] that simultaneously optimizing Φ and Ψ for a CS system results in better performance in terms of SRA.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "The SRA is evaluated in terms of the peak signal-to-noise ratio (PSNR) [5]",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "We note that a random dictionary with well-conditioned is chosen and thus we also compute the closed-form solution shown in [15] for (15).",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "This phenomenon is also observed for CSLH and CSLH−ET F in [15] [16].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "This phenomenon is also observed for CSLH and CSLH−ET F in [15] [16].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "• It is clear that the sensing matrices obtained via (15) and (16) have at least similar performance to the ones obtained via (8) and (9) [15, 16], though our proposed framework does not utilize the SRE matrix E .",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 14,
      "context" : "• It is clear that the sensing matrices obtained via (15) and (16) have at least similar performance to the ones obtained via (8) and (9) [15, 16], though our proposed framework does not utilize the SRE matrix E .",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 29,
      "context" : "Both training and testing datasets used in these three set of experiments are extracted as follows from the LabelMe database [31].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 13,
      "context" : "We perform the same experiment as in [15] to demonstrate the effectiveness of the proposed CS system CSMT without using the SRE E .",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "Similar to [15], the parameters M, N, L and K are set to 20, 64, 100 and 4, respectively.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 13,
      "context" : "Therefore, it is inefficient to utilize the methods in [15] [16] as they require the SRE matrix E .",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 14,
      "context" : "Therefore, it is inefficient to utilize the methods in [15] [16] as they require the SRE matrix E .",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 20,
      "context" : "The online dictionary learning algorithm in [22] [23] is chosen to train the sparsifying dictionary on the whole Training Data I.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 21,
      "context" : "The online dictionary learning algorithm in [22] [23] is chosen to train the sparsifying dictionary on the whole Training Data I.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "• We compare the computational complexity of our proposed method with the one in [15] [16].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : "• We compare the computational complexity of our proposed method with the one in [15] [16].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 30,
      "context" : "Calculating E involves the OMP algorithm [32] with computational complexity of O ( PKNL(KL logL+K3) ) , where we repeat that P, N, L and K denote the number of samples, the dimension of signal, the number of atoms in dictionary and the sparsity level, respectively.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 24,
      "context" : "Inspired by the work in [26], we attempt to design the projection matrix on a high dimensional dictionary in this set of experiments.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 24,
      "context" : "As stated in [26], training the dictionary with larger patches results in smaller sparse representation errors for natural images.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 31,
      "context" : "• The recent work in [33] states that it is possible to train the dictionary on millions of training signals whose dimension is also more than one million.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : "Our proposed framework for designing robust sensing matrices—which shares similar structure to that in [15] [16]— simultaneously minimizes the surrogate of sparse representation error (SRE) and the mutual coherence of the CS systems.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "Our proposed framework for designing robust sensing matrices—which shares similar structure to that in [15] [16]— simultaneously minimizes the surrogate of sparse representation error (SRE) and the mutual coherence of the CS systems.",
      "startOffset" : 108,
      "endOffset" : 112
    } ],
    "year" : 2017,
    "abstractText" : "Our objective is to efficiently design a robust projection matrix Φ for the Compressive Sensing (CS) systems when applied to the signals that are not exactly sparse. The optimal projection matrix is obtained by mainly minimizing the average coherence of the equivalent dictionary. In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a novel penalty function independent of a particular SRE matrix. Without requiring of training data, we can efficiently design the robust projection matrix and apply it for most of CS systems, like a CS system for image processing with a conventional wavelet dictionary in which the SRE matrix is generally not available. Simulation results demonstrate the efficiency and effectiveness of the proposed approach compared with the state-of-the-art methods. In addition, we experimentally demonstrate with natural images that under similar compression rate, a CS system with a learned dictionary in high dimensions outperforms the one in low dimensions in terms of reconstruction accuracy. This together with the fact that our proposed method can efficiently work in high dimension suggests that a CS system can be potentially implemented beyond the small patches in sparsity-based image processing.",
    "creator" : "LaTeX with hyperref package"
  }
}