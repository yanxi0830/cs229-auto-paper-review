{
  "name" : "1609.09525.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Q. Barthélemy", "C. Gouy-Pailler", "M. Sebag", "J. Atif" ],
    "emails" : [ "isaac.yoann@gmail.com", "q.barthelemy@gmail.com", "cedric.gouy-pailler@cea.fr", "michele.sebag@lri.fr", "jamal.atif@dauphine.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "This paper addresses the structurally-constrained sparse decomposition of multidimensional signals onto overcomplete families of vectors, called dictionaries. The contribution of the paper is threefold. Firstly, a generic spatio-temporal regularization term is designed and used together with the standard `1 regularization term to enforce a sparse decomposition preserving the spatio-temporal structure of the signal. Secondly, an optimization algorithm based on the split Bregman approach is proposed to handle the associated optimization problem, and its convergence is analyzed. Our well-founded approach yields same accuracy as the other algorithms at the state-of-the-art, with significant gains in terms of convergence speed. Thirdly, the empirical validation of the approach on artificial and real-world problems demonstrates the generality and effectiveness of the method. On artificial problems, the proposed regularization subsumes the Total Variation minimization and recovers the expected decomposition. On the real-world problem of electro-encephalography brainwave decomposition, the approach outperforms similar approaches in terms of P300 evoked potentials detection, using structured spatial priors to guide the decomposition.\nKeywords: Structured sparsity, overcomplete representations, analysis prior, split Bregman, fused-LASSO, EEG denoising."
    }, {
      "heading" : "1. Introduction",
      "text" : "In the last two decades, dictionary-based representations have been applied with success on a number of tasks, e.g. robust transmission with compressed\nIThe work presented in this paper has been partially funded by DIGITEO under the grant 2011-053D.\nEmail addresses: isaac.yoann@gmail.com (Y. Isaac), q.barthelemy@gmail.com (Q. Barthélemy), cedric.gouy-pailler@cea.fr (C. Gouy-Pailler), michele.sebag@lri.fr (M. Sebag), jamal.atif@dauphine.fr (J. Atif)\nPreprint submitted to Elsevier October 3, 2016\nar X\niv :1\n60 9.\n09 52\n5v 1\n[ cs\n.D S]\n2 9\nSe p\nsensing [14], image restoration [35], blind source separation [33] or classification [54] to name a few. Dictionary-based representations proceed by approximating a signal with a linear combination of elements, referred to as dictionary atoms, where the dictionary is either given based on the domain knowledge, or learned from a signal database [48]. The state-of-the-art mostly considers overcomplete dictionaries; in such cases the signal decomposition is not unique, requiring to select a decomposition with specific constraints or properties. One such property is the sparsity, where the signal decomposition involves but a few atoms; the trade-off between the number of such atoms and the approximation error is controlled via a weighted penalization term. Despite their good properties, sparse decompositions are sensitive w.r.t. the data noise [15], particularly so when the dictionary atoms are highly correlated.\nThis paper focuses on preserving the structure of the signal through the dictionary-based decomposition, thereby expectedly decreasing its sensitivity w.r.t. noise. The proposed approach is motivated by, and illustrated on, spatiotemporal multi-dimensional signals, referred to as multi-channel signals. The decomposition of multi-channel signals involves: i) decomposing each channel into the dictionary (see [41] for a survey); ii) ensuring that the structure of the multi-channel data is preserved in the multi-dimensional decomposition.\nFollowing and extending previous work [27], this paper focuses on structured dictionary-based decomposition, where the dictionary-based representation preserves the structure of the signal, as follows. Assuming that each one of the mono-dimensional signals is structured (e.g. being recorded in consecutive time samples and being continuous w.r.t. time), the structured decomposition property aims at preserving the signal structure in the dictionary-based representation (e.g. requiring that the approximation of signals in consecutive time steps is expressed on same atoms with “close” weights). Formally, the structured decomposition property is enforced via considering a specific regularization term besides the data fitting term (minimization of the approximation error) and the standard `1 term (maximizing the decomposition sparsity). The main contribution of this paper is to propose an efficient optimization scheme based on the split Bregman iterations for performing this structured decomposition of multi-dimensional signals. Implementation details including an efficient heuristic for hyper-parameters tuning is provided to accelerate the proposed algorithm. The proposed Multi-dimensional Sparse Structured Signal Approximation (Multi-SSSA) is assessed on synthetic signals for the fusedLASSO regularization obtained when the analysis term is a TV penalty. The approach is assessed in terms of i) its computational cost compared with the state-of-the-art smooth proximal gradient [9] approach; ii) its ability to recover the sparse structure of the initial signals compared with standard sparsity constraints and fused LASSO (`0, `2,0, `1, `2,1 and `1 + `2,1) even though the true structure of the signal is used to define the fused-LASSO regularization. Finally, Multi-SSSA is applied on electroencephalographic (EEG) signals to detect P300 evoked potentials. Using a data-driven prior, the structured decomposition approach is shown to effectively denoise the signal, resulting in a better\nclassification accuracy compared to the other regularizations. The paper is organized as follows. Section 2 introduces the formal background. Related work is discussed in Section 3. Section 4 gives an overview of the proposed Multi-SSSA algorithm and the optimization strategy. The experimental validation of the approach on artificial data in terms of computational cost and structure recovery is presented and discussed respectively in Sections 5.2 and 5.3. Section 6 presents the denoising application on EEG signals. The paper concludes with a general discussion and points out some perspectives in Section 7.\nNotations.. In the following, the j-th column of a matrix X is written X(j), the i-th row XT (i), and the i-th element of j-th column X(i, j). In stands for the identity matrix of size n. The `p matrix norm is defined as ‖X‖p = ( ∑ i ∑ j |X(i, j)|p) 1 p , with p = 2 corresponding to the classical Frobenius norm,\nand the `p,q mixed norm is defined as ‖X‖p,q = ( ∑ i( ∑ j |X(i, j)|p) q p ) 1 q ."
    }, {
      "heading" : "2. Sparse structured decomposition problem",
      "text" : ""
    }, {
      "heading" : "2.1. General problem",
      "text" : "Let Y = [Y (1), . . . , Y (T )] ∈ RC×T be a matrix of T ordered (e.g. corresponding to consecutive samples) C-dimensional signals, and Φ ∈ RC×NΦ an overcomplete dictionary of NΦ normalized atoms (NΦ C). We consider the following linear model:\nY (t) = ΦX(t) + E(t), t ∈ {1, . . . , T} , Y = ΦX + E , (1)\nin which X = [X(1), · · · , X(T )] ∈ RNΦ×T is the decomposition matrix and E = [E(1), · · · , E(T )] ∈ RC×T stands for a Gaussian noise matrix.\nThe sparse structured approximation problem consists in decomposing each signal Y (t), t ∈ {1, · · · , T} onto the dictionary such that the joint decomposition X reflects the underlying structure of Y . This decomposition aims at removing the Gaussian noise present in these signals and at separating theirs components into target and non-target ones (depending on the considered application). The structured decomposition problem is formalized as the minimization of the objective function:\nmin X∈RNΦ×T\n‖Y − ΦX‖22 + λ1‖X‖1 + λ2‖XP‖1 , (2)\nwith λ1, λ2 the regularization coefficients and P ∈ RT×NP a matrix encoding the prior knowledge about the signal structure. The use of the ‖XP‖1 regularization term can be interpreted in terms of sparse analysis [17]. Classically, the sparse analysis decomposition problem is formalized as follows:\nmin X∈RC×T\n‖Y −X‖22 + λ1‖ΨX‖1 , (3)\nwhereas the synthesis problem writes as minX ‖Y − ΦX‖22 + λ1‖X‖1. In the analysis setting, atoms in the dictionary Ψ are viewed as filters, on which the projection of the decomposed signals are required to be sparse. Along this line, the P matrix can be viewed as a set of linear filters, on which the projections of the decomposed signals are required to be sparse, thereby preserving the regularities given from prior knowledge about the application domain. The matrix P can be learned from a set of signals [39, 42].\nThe present paper considers a synthesis formulation of the problem, regularized by an analysis term. In order to illustrate the interest of such decomposition, the particular case of the multi-dimensional fused-LASSO is described below."
    }, {
      "heading" : "2.2. Multi-dimensional fused-LASSO problem",
      "text" : "In the particular case of a piecewise constant prior, a block-wise decomposition is expected. Formally this structure encoded by coefficients is defined as follows:\n∀n ∈ {1, · · · , NΦ}, XT (n) = Mn∑ m=1 αnm1κnm , (4)\nwith {κnm, ∀m ∈ {1, · · · ,Mn}} a partition of {1, · · · , T} corresponding to the Mn blocks of the coefficients associated with the n-th atom of X, and {αnm ∈ R, ∀m ∈ {1, · · · ,Mn}} the coefficients associated with the blocks. In the studied model, adding an `1 and a multi-dimensional TV regularization terms allows to enforce such prior. The TV term leads to a sparse decomposition gradient, thus preserving signal singularities and data edges, and supporting the detection of abrupt changes [43]. The analysis term is then written:\n‖XPTV‖1 = ∑T t=2 ‖X(t)−X(t− 1)‖1, where\nPTV =  −1 1 −1 1 . . . . . . −1\n1\n ∈ RT×T−1 .\nThe Figure 1 depicts an example of time-series coefficients obtained in this case, i.e. with sparse variations, for a given atom of the dictionary (in grey). This combination of regularization terms is known as the fused-LASSO and has been chosen for the evaluation of the proposed algorithm on synthetic data because of its interest for various applications: frequency hopping [1], geophysical studies [19], multi-task learning [58], trend analysis [32], covariance estimation [12], analysis of association between genetic markers and traits [31] or change point detection [6], among others.\nIf the fused-LASSO is a well-known model, it is important to notice that the introduced Multi-SSSA is able to solve the general problem of Eq. (2) for any matrix P , with an analytic or a data-driven content."
    }, {
      "heading" : "3. Related work",
      "text" : "Quite a few methods have been designed in the last decade to achieve the sparse approximation [11] of multi-dimensional signals while the monodimensional case has been intensively studied. Two main approaches have been considered: greedy methods trying to approximate the solution of the `0 regularized problem [50] and convex optimization solvers working on the `1 relaxed problem [49, 23, 41]. The concept of structured sparsity then emerged from the integration of prior knowledge encoded as regularizations into these approximation problems [26, 29]. The proposed Multi-SSSA approach falls in the latter category with a regularization combining a classical sparsity term and an analysis one. Some analysis regularizations have been extensively studied like the TV one which has been introduced in the ROF model [43, 13] for image denoising, nonetheless in the context of dictionary-based decomposition, their study has only begun recently [44, 51, 36]. Their interest for retrieving the underlying sparse structure of signals has been shown in [8]. To the best of our knowledge, the combination of these regularization terms had not been studied except for the particular case of the fused-LASSO introduced in [46].\nDespite the convexity of the associated minimization problem, the two `1 non-differentiable terms make it difficult to solve (by classical gradient approaches). Various approaches have been developed for solving the fused-LASSO problem. In their seminal work, Tibshirani et al. [46] transformed the problem to a quadratic one and used standard optimization tools. While this approach proved computationally feasible for small-sized problems since it relies on increasing the dimension of the search space, it does not scale efficiently with problem size. Path algorithms have then been developed: Hoefling proposed in\n[24] a method solving this problem in the particular case of the fused-LASSO signal approximator (Φ = IC) and Tibshirani et al. [47] designed a path method for the generalized LASSO problem. More recently, scalable approaches based on proximal sub-gradient methods [34], ADMM1[53] and split Bregman iterations [56] have been successfully applied to the mono-dimensional generalized fused-LASSO. Concerning the multi-dimensional fused-LASSO, an efficient method has been proposed recently in [9] for multi-task regression. A proximal method [37] is applied to a smooth approximation of the fused-LASSO regularization terms in this study and can be considered for different analysis regularizations. This approach is the most comparable with the present work.\nTo conclude this state-of-the-art, an illustration of the different regularizations is plotted in Figure 2: `2 (top left), `1 [45] (top right), `2,1 [57] (middle left), `1 + `2,1 [22] (middle right), TV [43] (bottom right) and `1 + TV (bottom right).\n1Alternating Direction Method of Multipliers.\nThis figure also illustrates the interest of structurally constrained regularizations in various noisy situations. Many concrete applications entail extracting specific activities (signal space or target activities) from the original signals, therefore getting rid of various kind of noise, defined as spurious/non target activities. While target and non-target activities often share a few properties (statistical distribution, spatial and temporal structures), the introduction of structurally constrained regularizations aims at drastically limiting the resulting space to signals exhibiting the desired properties."
    }, {
      "heading" : "4. Optimization strategy",
      "text" : "The split Bregman approach has been shown particularly well suited to `1 minimization problems [20] because of its ability to early detect zero and nonzero coefficients, which insures it a fast convergence on these problems. Thus, this optimization scheme has been chosen to solve the studied decomposition of Eq. (2)."
    }, {
      "heading" : "4.1. Optimization Scheme",
      "text" : "The above minimization problem is written as follows,\nmin X∈RNΦ×T\n‖Y − ΦX‖22 + λ1‖X‖1 + λ2‖XP‖1 .\nTo setup the optimization scheme, let us first restate it as:\nmin X∈RNΦ×T\nA∈RNΦ×T ,B∈RNΦ×NP\n‖Y − ΦX‖22 + λ1‖A‖1 + λ2‖B‖1\ns.t. A = X and B = XP . (5)\nThis reformulation is a key step of the split Bregman method. It decouples the three terms and allows to optimize them separately within the iterations. To setup this iteration scheme, Eq. (5) is rewritten as an unconstrained problem:\nmin X∈RNΦ×T\nA∈RNΦ×T ,B∈RNΦ×NP\n‖Y − ΦX‖22 + λ1‖A‖1 + λ2‖B‖1\n+ µ1 2 ‖X −A‖22 + µ2 2 ‖XP −B‖22 .\nDenoting i the current iteration, the split Bregman scheme [20] is then written:\n(Xi+1, Ai+1, Bi+1) = argmin X∈RNΦ×T\nA∈RNΦ×T ,B∈RNΦ×NP\n‖Y − ΦX‖22 + λ1‖A‖1 + λ2‖B‖1\n+ µ1 2 ‖X −A+DiA‖22 + µ2 2 ‖XP −B +DiB‖22\n(6)\nDi+1A = D i A + (X i+1 −Ai+1) Di+1B = D i B + (X i+1P −Bi+1) .\nThis scheme is equivalent to the one obtained with the augmented Lagrangian method (or ADMM) when the constraints are linear [55]. Each iteration requires the solving of the primal problem before updating the dual variables. Thanks to the split of the three terms, the minimization of the primal problem Eq. (6) can be performed iteratively by alternatively updating the following variables:\nXi+1 = argmin X∈RNΦ×T ‖Y − ΦX‖22 + µ1 2 ‖X −Ai +DiA‖22\n+ µ2 2 ‖XP −Bi +DiB‖22 (7)\nAi+1 = argmin A∈RNΦ×T\nλ1‖A‖1 + µ1 2 ‖Xi+1 −A+DiA‖22 (8)\nBi+1 = argmin B∈RNΦ×NP λ2‖B‖1 + µ2 2 ‖Xi+1P −B +DiB‖22 . (9)\nEmpirically, it has been noted that only few iterations of this system are necessary for convergence [20]. In our implementation, this update is only performed once at each iteration of the global optimization algorithm.\nEq. (8) and Eq. (9) can be solved with the soft-thresholding operator [10]:\nAi+1 = SoftThreshold λ1 µ1 (Xi+1 +DiA), (10) Bi+1 = SoftThreshold λ2 µ2 (Xi+1P +DiB) (11)\nwith\n(SoftThreshold λ (X))(i, j) = max\n( 0, 1− λ\n|X(i, j)|\n) X(i, j).\nSolving Eq. (7) requires the minimization of a convex differentiable function which can be performed via classical optimization methods. We propose here to solve it deterministically which is an original contribution of this work. Let us define H from Eq. (7) such as:\nXi+1 = argmin X∈RNΦ×T H(X) . (12)\nDifferentiating this expression with respect to X yields:\nd\ndX H(X) = (2ΦTΦ + µ1INΦ)X +X(µ2PP T ) (13)\n− 2ΦTY + µ1(DiA −Ai) + µ2(DiB −Bi)PT ,\nThe minimum X̂ = Xi+1 of Eq. (7) is obtained by solving ddXH(X̂) = 0, which is known as a Sylvester equation:\nWX̂ + X̂Z = M i , (14)\nwith W = 2ΦTΦ + µ1INΦ ∈ RNΦ×NΦ , Z = µ2PPT ∈ RT×T and M i = 2ΦTY + µ1(A\ni −DiA) + µ2(Bi −DiB)PT ∈ RNΦ×T . In a general setting, solving efficiently a Sylvester equation can be timeconsuming when the dimension of these matrices are large. A closed form solution derived from the vectorized formulation can be computed but it requires heavy calculations. One of the most used methods to deal with this issue, is the Bartels–Stewart algorithm [2] whose time complexity is O(N3Φ). In our particular case, the structures of the involved matrices ease this update.\nIndeed, W and Z being real symmetric matrices, solving for X̂ the Sylvester equation WX̂ + X̂Z = M i is equivalent to solve for X̂ ′ the following diagonal system\nDwX̂ ′ + X̂ ′Dz = M i′ , (15)\nwhere\nW = FDwF T , Z = GDzG T , (16) X̂ ′ = FT X̂G, M i′ = FTM iG .\nW and Z can be diagonalized in orthogonal bases (F and G) which allow to rewrite the Sylvester equation as follows:\nFDwF T X̂ + X̂GDzG T = M i ,\nand by applying the orthogonal properties of F and G the above expression is obtained. The solution X̂ ′ of the problem Eq.(15) is then computed as follows:\n∀t ∈ {1, · · · , T} X̂ ′(t) = (Dw +Dz(t, t)INΦ)−1M i′(t) ,\nwhich is equivalent to\n∀n ∈ {1, · · · , NΦ}, ∀t ∈ {1, · · · , T} , (Dw(n, n) +Dz(t, t)) X̂ ′(n, t) = M i′(n, t) , (17)\nwhich can be computed with\nX̂ ′ = M i′ O , (18)\nwhere corresponds to an element-wise division and\nO(n, t) = Dw(n, n) +Dz(t, t) . (19)\nX̂ is then calculated as X̂ = FX̂ ′GT . This last update can be numerically unstable when the elements of O are close to 0. As a consequence, µ1 and µ2 should be chosen carefully to avoid numerical instabilities. In addition, some terms can be precomputed to speed up the computations performed during each iteration. These implementation details and the choice of the penalty parameters are discussed in Section 4.3, and the full algorithm is summarized in Section 4.4."
    }, {
      "heading" : "4.2. Convergence",
      "text" : "Thanks to the exact solving of the primal subproblem presented in the previous section, the convergence of the above scheme can be derived by drawing inspiration from the steps described in [7].\nTheorem. Assume that λ1 ≥ 0, λ2 ≥ 0, µ1 > 0 and µ2 > 0, then the following holds\nlim i→∞ ‖Y − ΦXi‖22 + λ1‖Xi‖1 + λ2‖XiP‖1\n=‖Y − ΦX̂‖22 + λ1‖X̂‖1 + λ2‖X̂P‖1 (20)\nwhere X̂ denotes the solution of our problem. In addition, if our problem has a unique solution, from the convexity of E(X) = ‖Y − ΦX‖22 + λ1‖X‖1 + λ2‖XP‖1 and Eq. (20), we have\nlim i→∞\nXi = X̂ . (21)\nComplete proof is available in Appendix A, demonstrating that split Bregman scheme converges to a solution of the convex problem. The uniqueness of the solution depend on the choice of P ."
    }, {
      "heading" : "4.3. Implementation details and penalty parameters tuning",
      "text" : "The terms W and Z in Eq. (14) are independent of the considered iteration i. Their diagonalizations can then be performed only once and for all as well as the computation of O. These diagonalizations are derived easily from those of 2ΦTΦ and PPT and can be realized off-line (and then pre-computed)\n2ΦTΦ = F∆wF T , PPT = G∆zG T (22)\nDw = ∆w + µ1INΦ and Dz = µ2∆z. (23)\nThus, the update in Eq. (12) does not require heavy computations, even when the penalty parameters µ1 and µ2 change during the iterations (see below).\nYΦ = 2F TΦTY G and PG = P TG can also be pre-computed to avoid useless calculations. Besides, one can notice that the computational cost of each iteration depends on chain multiplications of three matrices (e.g FXtempGT ). The computation time of these products depends on the order in which the multiplications are performed. The computational costs of the three chains appearing in the previously described scheme for both multiplication orders are presented in Figure 3. The costs of the first two chains do not depend on the order of computation but the last one does. Hence, if T ≥ NP then FT ((DiB − Btemp)PG) is computed, otherwise (FT (DiB −Btemp))PG is computed.\nThe choice of µ1 and µ2 has a crucial impact on the rate of convergence. On the one hand, they should be chosen to get a great conditioning of the primal problem. On the other hand, these parameters can be updated during the\niterations to improve this rate as it is classically done in augmented Lagrangian methods [4].\nA poor conditioning of the primal update appears when these parameters are too small. The subproblem written in Eq. (7) becomes numerically unstable when the elements of the matrix O(n, t) = Dw(n, n) + Dz(t, t) are close to 0. The eigenvalues of W = 2ΦTΦ + µ1INΦ and Z = µ2PP\nT are non-negative and their smallest values depend on the penalty parameters. Let {λw(n),∀n ∈ {1, · · · , NΦ}} and {λz(t),∀t ∈ {1, · · · , T}} be respectively the eigenvalues of 2ΦTΦ and PPT , we have:\nmin n,t O(n, t) = min n λw(n) + µ1 + µ2 min t λz(t) . (24)\nWhen Φ is an overcomplete dictionary, minn λw(n) = 0 since this is the minimal singular value of Φ. Thus, depending on the eigenvalues of PPT , µ1 and µ2 should be chosen carefully to avoid numerical instability.\nAn update of these parameters during the iterations can speed up the convergence. As seen before, the split Bregman scheme is equivalent to the augmented Lagrangian one when the constraints are linear. Thus, a common strategy used in the augmented Lagrangian scheme has been chosen here to update the penalty parameters: when the loss associated with a constraint does not decrease enough between two iterations, the corresponding parameter is increased. Formally, let h1(X,A) = ‖X−A‖2 and h2(X,B) = ‖XP −B‖2 be the constraints losses, the parameters are updated as follows,\nµi1 =  µ i−1 1 if h1(X i, Ai) < r1h1(X i−1, Ai−1)\nρ1µ i−1 1 otherwise\n(25)\nand\nµi2 =  µ i−1 2 if h2(X i, Bi) < r2h2(X i−1, Bi−1)\nρ2µ i−1 2 otherwise\n(26)\nwhere r1, r2 are the threshold parameters and ρ1, ρ2 are the ratios of the geometric progressions µ1, µ2.\nTo achieve a fast convergence, the initialization of these parameters should not enforce the constraints too strictly in the first iterations but these parameters must not be setup with a value too small in order to not affect the resolution of the primal problem. A heuristic is described here to ease the initialization of µ1 and µ2. This procedure has been shown to be empirically efficient:\n1. define a search grid g for µ1 and µ2, 2. perform the first iteration of the optimization scheme for each couple of\nparameters [g(j), g(l)], 3. evaluate t1(j, l) = µ1 2 h1(X(j, l) 1, A(j, l)1)2 and t2(j, l) = µ2 2 h2(X(j, l) 1, B(j, l)1)2\nfor each couple, 4. initialize µ01 and µ 0 2 with:\nµ01 = g(argmax j ∑ l t1(j, l)) ,\nµ02 = g(argmax l ∑ j t2(j, l)) ."
    }, {
      "heading" : "4.4. Full Multi-SSSA algorithm",
      "text" : "The Multi-SSSA is summarized below as a pseudo-code procedure. To ease the understanding of the method, only the crucial steps are highlighted. The initialization and performance optimization steps just point out their corresponding equations. Note that only one subiteration is needed to solve the primal subproblem. Parameters: λ1, λ2, µ 0 1, µ 0 2, , iterMax, kMax, r1, r2, ρ1, ρ2\nprocedure Multi-SSSA(Y , Φ, P ) Initialize D0A, D 0 B , X 0 and set B0 = X0P , A0 = X0\nDiagonalize 2ΦT Φ and PPT to get ∆w, ∆z, F and G . Eq. (22) Compute Dw, Dz, from ∆w, ∆z, µ 0 1 and µ 0 2) . Eq. (23) Calculate O from Dw and Dz . Eq. (19) Precompute YΦ = 2F T ΦTY G and PG = P TG i = 0 while i ≤ iterMax and ‖X\ni−Xi−1‖2 ‖Xi‖2\n≥ do . stopping criteria M ′ = YΦ − FT (µi1(DiA −Ai)G\n+ µi2(D i B −Bi)PG)\nXi+1 = M ′ O Xi+1 = FXi+1GT . X primal update Ai+1 = SoftThresholdλ1\nµi1\n(Xi+1 +DiA) . A primal update\nBi+1 = SoftThresholdλ2 µi2 (Xi+1P +DiB) . B primal update Di+1A = D i A + (X\ni+1 −Ai+1) . DA dual update Di+1B = D i B + (X\ni+1P −Bi+1) . DB dual update Compute µi+11 and µ i+1 2 . Eq. (25, 26) Update Dw, Dz and O . Eq. (23, 19) i = i+ 1\nend while return Xi\nend procedure"
    }, {
      "heading" : "5. Experiments on synthetic data",
      "text" : "To evaluate the proposed method, two experiments have been performed on synthetic data. The computational time of the algorithm is first evaluated w.r.t. the state-of-the-art. Then, its ability to recover the underlying structure of signals is compared with other classical decomposition approaches. Both experiments have been carried out in the particular case of the fused-LASSO regularization (cf. Eq. (2) with P = PTV) on synthetic piecewise constant signals."
    }, {
      "heading" : "5.1. Data generation",
      "text" : "Each of these piecewise constant signals Y k, k ∈ {1, · · · ,K} has been synthesized from a built decomposition matrix Xk and a dictionary Φ with Y k = ΦXk. The atoms of Φ have been drawn independently from a Gaussian distribution to create a random overcomplete dictionary, which consequently has a low coherence:\nmax i,j ∈{1,··· ,NΦ}, i 6=j\nΦ(i)TΦ(j) ≈ 0.35 .\nEach block-wise decomposition matrix X has been built as a linear combination of specific activities generated as follows:\nΘn,t,d(i, j) =  0 if i 6= n H(j − (t− d×T2 ))\n−H(j − (t+ d×T2 )) if i = n\nwhere Θ ∈ RNΦ×T , H is the Heaviside function, n ∈ {1, · · · , NΦ} the index of an atom, t the center of the activity and d its duration. Each decomposition matrix X could then be written:\nX = M∑ m=1 αmΘnm,tm,dm ,\nwhere M is the number of activities appearing in one signal and the αm stand for the activation weights. An example of generated signal is given in Figure 4."
    }, {
      "heading" : "5.2. Experimental assessment of computational time",
      "text" : "The computational time of the Multi-SSSA algorithm is first evaluated. As noticed earlier, the most efficient method proposed for solving the multidimensional fused-LASSO is a smooth proximal-gradient method [9]. The TV analysis term is approached by a smooth penalty and an accelerated gradient descent is considered for minimizing the cost function. QP and SOCP formulations could also be considered but have been shown to be much slower than the proximal method [9]. Hence, the proposed optimization scheme is only compared here to this approach on the particular case of the multi-dimensional fused-LASSO."
    }, {
      "heading" : "5.2.1. Experiments setup",
      "text" : "Tests. To fairly compare these methods, the unique solution of our convex problem is calculated precisely beforehand. To compute this solution, we used the Multi-SSSA which is stopped when the relative change of the cost between two iterations is under 10−10. Then, both methods are executed and stopped only when the relative difference between the current cost and the value of the loss at the optimum is under a fixed precision. Three experiments have been performed to assess the computational time of both methods when different dimensions of our problem are varying. Their respective setups are described in Figure 5. In addition, each test is realized for different values of the precision defined above: 10−4, 10−5 and 10−6.\nRegularization parameters. The regularization parameter λ1 has been fixed such that ‖Y −ΦX̂‖2/‖Y ‖2 ≈ 0.1 where X̂ is the output of the optimization method and λ2 determined by cross-validation on the distance between the decomposition matrices used to build the signals and those obtained as outputs.\nImplementation details. Both methods have been implemented using MATLAB (64 bits). The experiments have been performed on a PC with 16GB RAM and a 8-core processor. Concerning the proximal approach, the minimization is performed by the classical FISTA method as in [9]. More precisely, the variant named ”FISTA with backtracking” in [3] has been chosen. The Lipschitzien coefficient L of the smooth term’s gradient is approximated then with a variable following a geometrical progression: Li = ρkLi−1 with ρ = 1.05 and L0 = 1. Besides, the parameter µ balancing the compromise between the analysis (TV) penalty and its smooth version is chosen such that the precision desired on the solution could be reached. A tight bound of the distance between the strict cost and the smooth one is known theoretically [9] and is proportional to µ. Consequently, to obtain finalLoss× precision ≥ gapBound = Kgapµ where finalLoss is the minimum value of the cost function and Kgap = 1 2NΦ(T +NP ), µ is defined as follows:\nµ = 0.95× lossF inal × precision Kgap . (27)\nAs for the introduced scheme, since the initial value of penalty parameters (obtained with the method presented in Section 4.3) have been observed to be stable (for the considered signals), they are computed off-line for each point of our tests on a 20 × 20 logarithmic grid. The update of these parameters is performed with ρ1 = ρ2 = 1.05 and r1 = r2 = 0.95. The diagonalizations of 2ΦTΦ and PPT are performed off-line."
    }, {
      "heading" : "5.2.2. Results and discussion",
      "text" : "The results are presented in Figure 6. The execution times are displayed in a logarithmic scale. As expected, for both methods, the computation time is almost not affected by the number of channels. The observed increase of computation time is due to the computation of the stopping criteria at each iteration. Two more observations can be derived from these curves. Firstly, the split Bregman method is faster than the proximal gradient one in all cases presented here (for all precisions: 10−4, 10−5 and 10−6) and the curves have the same shapes. Secondly, the differences in speed between these approaches become more important when the desired precision becomes smaller. This last observation can be understood by noticing that the gradient’s Lipschitzien coefficient of the smooth TV penalty presented in [9] is inversely proportional to the parameter µ. As this coefficient corresponds to the inverse of the FISTA gradient step coefficient, when µ becomes smaller (insuring a small gap) the gradient step becomes smaller and the method is then slower."
    }, {
      "heading" : "5.3. Experimental evaluation of sparse recovery on synthetic data",
      "text" : "The interest of the studied regularization is now studied in a dictionary-based representation context. The performance of the described model to recover the underlying block-wise structures of artificial signals is assessed w.r.t. classical regularizations."
    }, {
      "heading" : "5.3.1. Compared regularizations",
      "text" : "Multi-SSSA is compared both with algorithms coding each signal separately with the `0 and `1 regularization terms and to methods performing the decomposition simultaneously with the `2,0, `2,1 and `2,1 +`1 regularization terms. Regarding the `0 and `2,0 constraints, the solutions are respectively given by the orthogonal matching pursuit (OMP) [38] and the simultaneous OMP (SOMP) [50]. The `1 solutions are obtained by the LARS method [16] and a proximal approach (FISTA [3]) has been chosen to deal with the `2,1 and `1 + `2,1 regularizations [22]. The approximation problems with the `1 and `2,1 regularization terms are respectively referred to as the LASSO [45] and group-LASSO [57] problems, where group-LASSO is used defining only one group by atom.\nAmong the compared algorithms, we have implemented the OMP and the SOMP, whereas the SPAMS2 [30] toolbox has been used for the other methods."
    }, {
      "heading" : "5.3.2. Experimental settings",
      "text" : "The goal of this experiment is to assess the ability of each model to retrieve the underlying structures of the designed piecewise constant signals. Since these performances vary with the number of activities M composing the signals and their duration d, this experience is performed for each point of the following grid of parameters:\n• M ∈ {20, 30, · · · , 110} ,\n• d ∼ U(dmin, dmax) (dmin, dmax) ∈ {(0.05, 0.15), (0.15, 0.25), · · · , (0.95, 1)} .\nFor each model and each point in the grid, the evaluation is carried out as follows:\n• The set of built signals is split to create a training set allowing to determine the best regularization parameters and a test set designed to evaluate the performance with these parameters.\n• For various regularization parameters, each signal Y of the training set is decomposed and the estimated decomposition matrix X̂ is compared with the built one X using the following distance: ε(X, X̂) = ‖X− X̂‖2/‖X‖2. The parameters giving the best performances are chosen.\n• Each signal of the test set is decomposed with the optimal parameters and the same distance is computed between the estimated decomposition matrix and the built one.\nOther parameters of the signals construction are presented in Figure 7."
    }, {
      "heading" : "5.3.3. Results",
      "text" : "For each point in the grid of parameters, the mean (among test signals) of the distance ε has been computed for each method and compared to the mean\n2http://spams-devel.gforge.inria.fr/\nobtained by the Multi-SSSA algorithm. First, an analysis of variance shows that significant differences exist between the methods (p ≤ 0.05). Paired t-tests (with Bonferroni corrections) have been performed to assess the significant differences between the couples of methods. The results are displayed in Figure 8. In the ordinate axis, the number of patterns M increases from the top to the bottom and in the abscissa axis, the duration d grows from left to right. The top-left image displays the mean distances obtained by the Multi-SSSA algorithm, i.e. `1 + TV. Unsurprisingly, the difficulty of finding the true decomposition increases with the number of patterns and their durations. The other figures present its performances compared to other methods by displaying the differences of mean distances in gray scale. These differences are calculated such that negative values (darker blocks) correspond to parameters where the introduced method outperforms the other one. The white diamonds correspond to non-significant differences of mean distances. Results of the OMP and the LASSO solver (`1) are very similar as well as those of the SOMP and the group-LASSO solver (`2,1): they obtain the same pattern of performances on our grid of parameters. So, we only display here the matrices comparing the fused-LASSO regularization to those of the LASSO and group-LASSO models as well as with the regularization `1 + `2,1."
    }, {
      "heading" : "5.3.4. Discussions",
      "text" : "First, concerning the comparison between the `1 (and `0) regularization terms and the `1 + TV one, it can be noted that similar results are obtained when only few atoms are active at the same time. It happens in our artificial signals when only few patterns have been used to create decomposition matrices and/or when the pattern durations are small. On the contrary, when many atoms are active simultaneously, the multi-dimensional fused-LASSO outperforms the LASSO model, allowing better retrieval of the block-wise structures of signals by using inter-signals prior information. Concerning the `2,1 (and `2,0) regularization terms, results depend more on the duration of patterns. When patterns are longer, their performances are similar to the fused-LASSO one. On the contrary, when patterns have short/medium durations the group-LASSO model is outperformed. This is not surprising since these regularization terms select atoms for the entire duration of the signals. As expected the `1 +`2,1 model combines the advantages of the `1 and `2,1 regularization terms, having same performances as the fused-LASSO for both small number of patterns or long ones. In addition, its efficiency is better than the\nother studied regularizations in the middle of our grid even if it is still outperformed by the fused-LASSO which better detects the signals abrupt changes. Finally, this experiment illustrates the ability of the method to discriminate activities based on their statistical properties. Interferent signals, which share similar properties with a desired signal, will be split among distinct components if their spatial location of their temporal structure differ."
    }, {
      "heading" : "6. Application on EEG signals for P300 single-trial classification",
      "text" : "The general model described in Eq. (2) of Section 2 can be applied to various contexts, with a non-analytic matrix P . This section is devoted to present the application of the studied regularization for the (unsupervised) denoising of real EEG data in a classification task: the detection of P300 evoked potentials."
    }, {
      "heading" : "6.1. Detection of P300 evoked potentials",
      "text" : "The P300 is one of the most popular evoked potential used within Brain Computer Interfaces (BCI) systems. The P300 speller introduced in [18] allows for instance to spell words (letter by letter) by detecting such potentials after the presentation of visual stimuli. These potentials are usually elicited by presenting a rare target stimulus among common non-target ones (oddball approach). The P300 wave appears between 250 and 450 ms after the target stimulus and is mainly located in the parietal and the occipital lobes. Its latency and amplitude depend on various factors like the target-to-target intervals (see [40] for a review). The following experiment focuses on the single-trial detection of such brain activity. Each EEG measurement is a spatio-temporal signal which can be studied as a sum of electrical activities emitted by different neural assemblies. These activities are characterized by specific time course and spatial patterns which depend on the locations of their sources and the orientations of the associated electrical activities. In order to obtain plausible decompositions of such signals, the extracted components should respect the properties of these activities as the smoothness induced by the diffusion of electrical waves in the skull. The hypothesis in this experiment is that such plausible representations can be obtained via the studied regularization. The main components of these decompositions could then lead to a better identification of brain activities and in particular the components of the P300 potentials. Two types of noise must be removed: the sensors noise and the EEG background activities not related to the target activity. Since the components of the P300 potential are not known precisely, in order to evaluate the introduced regularization, the main components of the decompositions are used to reconstruct the signals. Then these denoised signals are used to identify P300 potentials."
    }, {
      "heading" : "6.2. Decomposition model",
      "text" : "The previously studied model is considered with EEG signals Y ∈ RC×T recorded on T electrodes over C time samples3. These signals are decomposed thanks to the Multi-SSSA algorithm on a Gabor time-frequency dictionary Φ ∈ RC×NΦ which is able to efficiently represent both transitory and oscilatory EEG components [52]. The analysis regularization matrix P ∈ RT×NP is chosen as the dual of a spatial dictionary Φs ∈ RNP×T composed ofNP atoms. This dictionary is composed of realistic EEG topographies, horizontally concatenated since the regularization is applied with the dual on the lines of X (spatial dimension). An EEG topography corresponds to the values of the electrical potential of all electrodes at a given instant. The dual of this dictionary is approximated by the Moore–Penrose pseudo-inverse of Φs [17]. The following steps have been carried out to construct this spatial dictionary:\n• a realistic head model has been built from MRI data and has been divided into voxels,\n• for each voxel and different orientations of voxels’ electrical activities, the associated EEG topographies have been computed by solving the EEG direct problem (with the software OpenMEEG [21]) and regrouped in a large spatial dictionary (≈ 5000 elements),\n• a subset of this large dictionary has been finally selected with a greedy approach such that its coherence does not exceed 0.9 (NP = 350 elements).\nSince the retrieval of the sources of these components is not the objective here, this latter step has been carried out to improve the conditioning of the decompositions and preserve a reasonable computational time (decomposition time of a signal around 2 seconds). Some topographies of Φs are presented in Figure 9."
    }, {
      "heading" : "6.3. Experimental settings",
      "text" : "In this experiment, the studied regularization (denoted MSSSA) is compared to three others: `1, `2,1 and `2,1 + `1 to evaluate their ability to denoise P300 signals and ease their detection.\nP300 data. The dataset IIb of the BCI Competition II [5] has been chosen to evaluate our model. These signals have been recorded on T = 64 electrodes at 240 Hz in several sessions. Trials have been extracted between 150 and 450 ms after each stimulus and band-filtered between 0.1 and 20 Hz with a fourth order Butterworth filter. The dataset is composed of three sessions and only the two first sessions have been used in this experiment.\n3Model in Eq. (2) applies a structured prior along the time dimension whereas, in this section, the prior is applied along the spatial dimension. Thus, notations for spatial and temporal dimensions are inversed.\nProtocol. To evaluate the efficiency of each regularization, each trial Y k is firstly decomposed on the dictionary Φ to obtain its decomposition matrix X̂k, then, a classification algorithm is applied to the set of the reconstructed signals {Ŷ k = ΦX̂k, k ∈ {1, . . . ,K}}. The BLDA (Bayesian Linear Discriminant Analysis) algorithm [25] has been chosen to perform the classification step. It has been shown particularly efficient for the detection of P300 evoked potentials. The optimal regularization parameters of each model are learned on the second session of the dataset with a n-fold cross-validation, and the evaluation is then assessed on the first session of the dataset. This validation is performed for several values of n: 2 (500 signals in each fold), 5 (200 signals in each fold) and 10 (100 signals in each fold) in order to evaluate the regularizations efficiency for various sizes of the training set. Each cross-validation is performed 25 times, with folds selected randomly. For this experiment, the MSSSA has been stopped with the following criterion: ‖Xi −Xi−1‖2/‖Xi‖2 ≤ eps (with eps = 10−6)."
    }, {
      "heading" : "6.4. Results and discussion",
      "text" : "The results of this experiment are presented in Figure 10. As expected, classification scores rise w.r.t. the number of folds and thus the size of the training set. Concerning the comparison between the regularizations, paired Wilcoxon signed-rank test have been performed between results obtained for each term, given the following results:\n• the `1 regularization does not improve the scores obtained on raw signals (p ≥ 0.05),\n• `2,1 and `2,1 + `1 regularizations got the same results and improve significantly (p ≤ 0.005) the scores obtained on raw signals.\n• the introduced MSSSA improve significantly (p ≤ 0.005) the scores obtained with the `2,1 regularization.\nOnly the regularizations enforcing a spatial structure in the decomposition coefficients (`2,1 and MSSSA) allow here to improve the classification results by extracting plausible EEG components. The `2,1 constraints all the channels to be decomposed on the same atoms, enforcing the choice of time-frequency atoms allowing to represent the time courses of all channels. The efficiency of this terms could be explained by noting that brain electrical activities are diffused by the skull and then affect a majority of the sensors. The proposed regularisation improves even more these classification scores by considering a data-driven spatial priors encoded in the analysis matrix P which guide the decomposition to more plausible P300 components. Removing the noise Ek = Y k − Ŷ k, this decomposition can then be considered as a denoising step before P300 signals classification and more generally before EEG signals classification. In addition, the results could probably be improved by building the spatial dictionary from a head model of the subject on which the EEG are measured."
    }, {
      "heading" : "7. Conclusion and perspectives",
      "text" : "The studied approach enforces structural properties of the signal decomposition on the dictionary through the regularization term ‖XP‖1. The overall optimization problem is efficiently and scalably handled through the split Bregman method.\nRegarding the structural regularization term, a proof of principle of the approach is obtained in the particular case of the TV norm, conducive to the discovery of block-wise structures in the decomposition on overcomplete dictionaries: as shown in Section 5.3, the recovery of the block-wise structure of the input signals is significantly improved when using the `1 + TV regularization compared to other regularizations. In addition, the EEG experiment (Section 6) illustrates how the analysis term can be used to enforce structural properties on the decomposition in a data-driven way. It allows in this last case to denoise EEG signals and improve the single-trial detection of P300. The decompositions are guided to plausible components thank to an analysis matrix constructed from a dictionary of realistic EEG topographies.\nThe second contribution of the paper is the original split-Bregman method handling the underlying optimization problem, and its scalability compared with the state-of-the-art. As shown in Section 5.2, Multi-SSSA outperforms the smooth proximal gradient in terms of speed when the problem dimensions increase, and it is less sensitive w.r.t. the wanted precision. Furthermore, an empirically efficient heuristic procedure is proposed to adjust the penalty hyperparameters and thus preserve the efficiency of the algorithm. This efficiency is explained from the split-Bregman ability to early detect zero coefficients (see [20], Appendix), thereby easily accommodating `1 regularization. The main scalability limitation of the proposed scheme comes from the diagonalization of the matrix in Eq. (22), with cubic complexity in its size; it therefore requires the dictionary size NΦ and/or T to remain in the hundreds. A second limitation regards the memory complexity, and the storage of variables involved in the method. To overcome these limitations, a method solving approximatively the primal subproblem Eq. (7) can be considered. Even if the exact solving of this subproblem allows to ensure the convergence of the proposed scheme, the split Bregman iterations have been shown empirically to converge even when the primal problem is not solved exactly. When NΦ or NP become really large, this option can significantly reduce the computation time.\nFurther perspectives and on-going work are primarily concerned with carrying out a full evaluation of the presented EEG application, in particular to examine the influence of the dictionaries choices (spatial and time-frequency) on the denoising efficiency. In addition, an extension of the microstates EEG model will be studied with a TV regularization matrix [28]. Another direction of research is concerned with approximating the solution of the subproblem in Eq. (12), instead of solving it exactly. A mid-term research perspective is to learn both the dictionary Φ and the regularities P from the data. While not rigorously shown in this paper, we believe that the proposed frame-\nwork provides with a rich panel of possibilities to filter signal of interest from noise and interference in various context and applications."
    }, {
      "heading" : "Appendix A. Optimization scheme convergence",
      "text" : "The convergence theorem (Section 4.2) of the introduced optimization scheme is here demonstrated by applying the convergence analysis of Osher et al. [7]. This theorem hold for λ1 ≥ 0, λ2 ≥ 0, µ1 > 0 and µ2 > 0.\nThe studied iterative algorithm consider at each iteration three convex subproblems Eq. (7), (8), (9). The first order optimality condition of these problems gives:\n0 = (2ΦTΦ + µ1I)X i+1 + µ2X i+1PPT\n− 2ΦTY + µ1(DiA −Ai) + µ2(DiB −Bi)PT , 0 = λ1Q i+1 A − µ1(D i A −Ai+1 +Xi+1) , 0 = λ2Q i+1 B − µ2(D i B −Bi+1 +Xi+1P ) , (A.1)\nDi+1A = D i A + (X i+1 −Ai+1) , Di+1B = D i B + (X i+1P −Bi+1) ,\nwhere Qi+1A ∈ ∂‖Ai+1‖1 et Q i+1 B ∈ ∂‖Bi+1‖1.\nIn addition, the convexity of the main problem Eq. (5) insures the existence of an unique solution which respect the KKT conditions. The Lagrangian L of the problem could thus be written:\nL = ‖Y − ΦX‖2F + λ1‖X‖1 + λ2‖XP‖1,\nand ∃X̂ such that\n0 =− 2ΦT (Y − ΦX̂) + λ1Q̂A + λ2Q̂BPT , (A.2) Â = X̂, and B̂ = X̂P ,\nwhere Q̂A ∈ ∂‖Â‖1 et Q̂B ∈ ∂‖B̂‖1. This solution is a fixed point of the optimization scheme and verifies:\n0 = (2ΦTΦ + µ1I)X̂ + µ2X̂PP T − 2ΦTY + µ1(D̂A − Â) + µ2(D̂B − B̂)PT ,\n0 = λ1Q̂A − µ1(D̂A − Â+ X̂) , 0 = λ2Q̂B − µ2(D̂B − B̂ + X̂P ) , (A.3)\nD̂A = D̂A + (X̂ − Â) , D̂B = D̂B + (X̂P − B̂) .\nSubstracting Eq. (A.3) from Eq. (A.1) we obtained the same system with\nthe errors variables:\nX̃i = Xi − X̂, Ãi = Ai − Â, B̃i = Bi − B̂ , D̃iA = D i A − D̂B , D̃iB = DiB − D̂B\nQ̃iA = Q i A − Q̂A, Q̃iB = QiB − Q̂B .\nPerforming the scalar product of the first line by X̃i+1, the scalar product of the second line by Ãi+1, the scalar product of the third line by B̃i+1 and taking the square Frobenius norm of the last lines, we obtained the following system:\n0 = 2‖ΦX̃i+1‖2F + µ1‖X̃i+1‖2F + µ2〈X̃i+1, X̃i+1PPT 〉 + µ1(〈X̃i+1, D̃iA〉 − 〈X̃i+1, Ãi〉) + µ2(〈X̃i+1, D̃iBPT 〉 − 〈X̃i+1, B̃iPT 〉) , 0 = λ1〈Ãi+1Q̃i+1A 〉 − µ1(〈Ã i+1D̃iA〉 − ‖Ãi+1‖2F + 〈Ãi+1, X̃i+1〉) , 0 = λ2〈B̃i+1, Q̃i+1B 〉 − µ2(〈B̃ i+1, D̃iB〉 − ‖B̃i+1‖2F + 〈B̃i+1, X̃i+1P 〉) ,\n‖D̃i+1A ‖ 2 F = ‖D̃iA‖2F + (‖X̃i+1‖2F + ‖Ãi+1‖2F − 2〈X̃i+1, Ãi+1〉)− 2〈D̃iA, X̃i+1 − Ãi+1〉 , ‖D̃i+1B ‖ 2 F = ‖D̃iB‖2F + (‖X̃i+1P‖2F + ‖B̃i+1‖2F\n− 2〈X̃i+1P, B̃i+1〉)− 2〈D̃iB , X̃i+1P − B̃i+1〉 .\nSumming the 3 first equations and slightly modifying the others, gives:\n0 = 2‖ΦX̃i+1‖2F + µ1‖X̃i+1‖2F + µ2〈X̃i+1, X̃i+1PPT 〉 + λ1〈Ãi+1, Q̃i+1A 〉+ λ2〈B̃\ni+1, Q̃i+1B 〉 + µ1(〈X̃i+1, D̃iA〉 − 〈X̃i+1, Ãi〉 − 〈Ãi+1, D̃iA〉+ ‖Ãi+1‖2F − 〈Ãi+1, X̃i+1〉) + µ2(〈X̃i+1, D̃iBPT 〉 − 〈X̃i+1, B̃iPT 〉 − 〈B̃i+1, D̃iB〉+ ‖B̃i+1‖2F − 〈B̃i+1, X̃i+1P 〉) ,\n〈D̃iA,X̃i+1 − Ãi+1〉 = 1\n2 (‖D̃i+1A ‖ 2 F − ‖D̃iA‖2F − ‖X̃i+1 − Ãi+1‖2F ) ,\n〈D̃iB ,X̃i+1P − B̃i+1〉 = 1\n2 (‖D̃i+1B ‖ 2 F − ‖D̃iB‖2F − ‖X̃i+1P − B̃i+1‖2F ) .\nand combining these equations and summing between i = 1 and i = I:\nµ1 2 (‖D̃1A‖2F − ‖D̃SA‖2F ) + µ2 2 (‖D̃1B‖2F − ‖D̃SB‖2F )\n= 2 S∑ i=1 ‖ΦX̃i‖2F + S∑ i=1 λ1〈Ãi+1, Q̃i+1A 〉+ λ2〈B̃ i+1, Q̃i+1B 〉\n+ µ1 2 (−‖Ã1‖2F + S∑ i=1 ‖X̃i+1 − Ãi+1‖2F + ‖X̃i+1 − Ãi‖2F + ‖AS‖2F )\n+ µ2 2 (−‖B̃1‖2F + S∑ i=1 ‖X̃i+1P − B̃i+1‖2F + ‖X̃i+1P − B̃i‖2F + ‖BS‖2F ) .\nThe convexity of ‖.‖1 imply that the terms < Ãi, Q̃A i > and < B̃i, Q̃B i > are positives (∀i). µ1, µ2, λ1 and λ2 being non-negative, all terms of the above equations are non-negative and we have:\nµ1 2 (‖D̃1A‖2F + ‖Ã1‖2F ) + µ2 2 (‖D̃1B‖2F + ‖B̃1‖2F )\n≥ 2 S∑ i=1 ‖ΦX̃i‖2F + S∑ i=1 λ1〈Ãi+1, Q̃i+1A 〉+ λ2〈B̃ i+1, Q̃i+1B 〉 + µ1 2 ‖X̃i+1 − Ãi‖2F + µ2 2 ‖X̃i+1P − B̃i‖2F .\nFrom this last equation we can derive:\n∞∑ i=1 ‖ΦX̃i‖2F <∞ , (A.4)\n∞∑ i=1 〈Ãi+1, Q̃i+1A 〉 <∞, ∞∑ i=1 〈B̃i+1, Q̃i+1B 〉 <∞ , (A.5)\n∞∑ i=1 ‖X̃i+1 − Ãi‖2F <∞, ∞∑ i=1 ‖X̃i+1P − B̃i‖2F <∞ , (A.6)\nwhich leads to the convergence theorem enunciated in Section 4.2. From Eq. (A.5) and the properties of the Bregman distance (cf. Eq. (3.6) in [7]):\nlim i→∞\n‖Ai‖1 − ‖Â‖1 − 〈Ai − Â, Q̂A〉 = 0 ,\nlim i→∞\n‖Bi‖1 − ‖B̂‖1 − 〈Bi − B̂, Q̂B〉 = 0 ,\nwhich, combined with Eq. (A.6), gives:\nlim i→∞\n‖Xi‖1 − ‖X̂‖1 − 〈Xi − X̂, Q̂A〉 = 0 , (A.7)\nlim i→∞\n‖XiP‖1 − ‖X̂P‖1 − 〈Xi − X̂, Q̂BPT 〉 = 0 , (A.8)\nand finally, by taking λ1 Eq. (A.7) + λ2 Eq. (A.8) and Eq. (A.2), we have:\nlim i→∞ ‖Xi‖1 − ‖X̂‖1 + ‖XiP‖1 − ‖X̂P‖1\n− 〈Xi − X̂, 2ΦT (Y − ΦX̂)〉 = 0 . (A.9)\nIn addition, ‖ΦX̃i‖22 =< ∇f(Xi)−∇f(X̂), Xi−X̂ > for f(X) = ‖Y −ΦX‖22. F is convex and from Eq. (A.4) and the same Bregman distance’s property used before we can write:\nlim i→∞ ‖Y − ΦXi‖2F − ‖Y − ΦX̂‖2F\n− 〈Xi − X̂,−2ΦT (Y − ΦX̂)〉 ,\nwhich provide with Eq. (A.9) the first result of the theorem:\nlim i→∞ ‖Y − ΦXi‖2F + λ1‖Xi‖1 + λ2‖XiP‖1\n=‖Y − ΦX̂‖2F + λ1‖X̂‖1 + λ2‖X̂P‖1 = 0 .\nThe second part is obtained by noting that the fonction g(X) = ‖Y −ΦX‖22+ λ1‖X‖1 + λ2‖XP‖1 is continuous, strictly convex and then has an unique minimizer. So, limi→∞ g(X i) = g(X̂)⇒ limi→∞Xi = X̂ (see Osher et al. [7])."
    } ],
    "references" : [ {
      "title" : "Multiple frequencyhopping signal estimation via sparse regression",
      "author" : [ "D. Angelosante", "G. Giannakis", "N. Sidiropoulos" ],
      "venue" : "Acoustics Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Solution of the matrix equation AX+ XB= C [F4",
      "author" : [ "R. Bartels", "G. Stewart" ],
      "venue" : "Communications of the ACM",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1972
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM Journal on Imaging Sciences",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Constrained optimization and lagrange multiplier methods",
      "author" : [ "D. Bertsekas" ],
      "venue" : "Computer Science and Applied Mathematics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1982
    }, {
      "title" : "The BCI Competition 2003: progress and perspectives in detection and discrimination of EEG single trials",
      "author" : [ "B. Blankertz", "Müller", "K.-R", "G. Curio", "T. Vaughan", "G. Schalk", "J. Wolpaw", "A. Schlögl", "C. Neuper", "G. Pfurtscheller", "T. Hinterberger", "M. Schröder", "N. Birbaumer" ],
      "venue" : "IEEE Trans. on Biomedical Engineering",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "The group fused Lasso for multiple changepoint detection",
      "author" : [ "K. Bleakley", "Vert", "J.-P" ],
      "venue" : "arXiv preprint arXiv:1106.4199",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Split Bregman methods and frame based image restoration",
      "author" : [ "J. Cai", "S. Osher", "Z. Shen" ],
      "venue" : "Multiscale modeling and simulation",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Compressed sensing with coherent and redundant dictionaries",
      "author" : [ "E.J. Candes", "Y.C. Eldar", "D. Needell", "P. Randall" ],
      "venue" : "Applied and Computational Harmonic Analysis",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Smoothing proximal gradient method for general structured sparse regression",
      "author" : [ "X. Chen", "Q. Lin", "S. Kim", "J. Carbonell", "E. Xing" ],
      "venue" : "The Annals of Applied Statistics",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Signal recovery by proximal forwardbackward splitting",
      "author" : [ "P.L. Combettes", "V.R. Wajs" ],
      "venue" : "Multiscale Modeling & Simulation",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2005
    }, {
      "title" : "Sparse solutions to linear inverse problems with multiple measurement vectors",
      "author" : [ "S. Cotter", "B. Rao", "K. Engan", "K. Kreutz-Delgado" ],
      "venue" : "IEEE Trans. on Signal Processing",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "The joint graphical Lasso for inverse covariance estimation across multiple classes",
      "author" : [ "P. Danaher", "P. Wang", "D.M. Witten" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology)",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "A fast and exact algorithm for total variation minimization. In: Pattern recognition and image analysis",
      "author" : [ "J. Darbon", "M. Sigelle" ],
      "venue" : "Vol. 3522 of Lecture Notes in Computer Science",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2005
    }, {
      "title" : "Compressed sensing",
      "author" : [ "D. Donoho" ],
      "venue" : "IEEE Trans. on Information Theory",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "Stable recovery of sparse overcomplete representations in the presence of noise",
      "author" : [ "D. Donoho", "M. Elad", "V. Temlyakov" ],
      "venue" : "IEEE Trans. on Information Theory",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "Least angle regression",
      "author" : [ "B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani" ],
      "venue" : "The Annals of statistics",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2004
    }, {
      "title" : "Analysis versus synthesis in signal priors",
      "author" : [ "M. Elad", "P. Milanfar", "R. Rubinstein" ],
      "venue" : "Inverse problems",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "Talking off the top of your head: toward a mental prosthesis utilizing event-related brain potentials",
      "author" : [ "L.A. Farwell", "E. Donchin" ],
      "venue" : "Electroencephalography and clinical Neurophysiology",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1988
    }, {
      "title" : "Regularization of linear and non-linear geophysical ill-posed problems with joint sparsity constraints",
      "author" : [ "A. Gholami", "H. Siahkoohi" ],
      "venue" : "Geophysical Journal International",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2010
    }, {
      "title" : "The split Bregman method for `1 regularized problems",
      "author" : [ "T. Goldstein", "S. Osher" ],
      "venue" : "SIAM Journal on Imaging Sciences",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "Open- MEEG: opensource software for quasistatic bioelectromagnetics",
      "author" : [ "A. Gramfort", "T. Papadopoulo", "E. Olivi", "M Clerc" ],
      "venue" : "Biomed. Eng. Online",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "Time-frequency mixed-norm estimates: sparse M/EEG imaging with non-stationary source activations",
      "author" : [ "A. Gramfort", "D. Strohmeier", "J. Haueisen", "M.S. Hämäläinen", "M. Kowalski" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Atoms of all channels, unite! Average case analysis of multi-channel sparse recovery using greedy algorithms",
      "author" : [ "R. Gribonval", "H. Rauhut", "K. Schnass", "P. Vandergheynst" ],
      "venue" : "Journal of Fourier analysis and Applications",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "A path algorithm for the fused Lasso signal approximator",
      "author" : [ "H. Hoefling" ],
      "venue" : "Journal of Computational and Graphical Statistics",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "An efficient P300-based brain–computer interface for disabled subjects",
      "author" : [ "U. Hoffmann", "Vesin", "J.-M", "T. Ebrahimi", "K. Diserens" ],
      "venue" : "Journal of Neuroscience methods",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2008
    }, {
      "title" : "Learning with structured sparsity",
      "author" : [ "J. Huang", "T. Zhang", "D. Metaxas" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "Multi-dimensional sparse structured signal approximation using split Bregman iterations",
      "author" : [ "Y. Isaac", "Q. Barthélemy", "J. Atif", "C. Gouy-Pailler", "M. Sebag" ],
      "venue" : "In: Acoustics Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Généralisation des micro-états EEG par apprentissage régularisé temporellement de dictionnaires topographiques",
      "author" : [ "Y. Isaac", "Q. Barthélemy", "C. Gouy-Pailler", "J. Atif", "M. Sebag" ],
      "venue" : "XXV Colloque GRETSI - Traitement du Signal et des Images",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Structured variable selection with sparsity-inducing norms",
      "author" : [ "R. Jenatton", "J. Audibert", "F. Bach" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2011
    }, {
      "title" : "Proximal methods for sparse hierarchical dictionary learning",
      "author" : [ "R. Jenatton", "J. Mairal", "F. Bach", "G. Obozinski" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2010
    }, {
      "title" : "Statistical estimation of correlated genome associations to a quantitative trait network. PLoS genetics",
      "author" : [ "S. Kim", "E. Xing" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2009
    }, {
      "title" : "Blind source separation of more sources than mixtures using overcomplete representations",
      "author" : [ "Lee", "T.-W", "M. Lewicki", "M. Girolami", "T. Sejnowski" ],
      "venue" : "IEEE Signal Processing Letters",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1999
    }, {
      "title" : "An efficient algorithm for a class of fused Lasso problems",
      "author" : [ "J. Liu", "L. Yuan", "J. Ye" ],
      "venue" : "Proc. 16th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2010
    }, {
      "title" : "Sparse representation for color image restoration",
      "author" : [ "J. Mairal", "M. Elad", "G. Sapiro" ],
      "venue" : "IEEE Trans. on Image Processing",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2008
    }, {
      "title" : "Synthesis and analysis prior algorithms for joint-sparse recovery",
      "author" : [ "A. Majumdar", "R.K. Ward" ],
      "venue" : "In: Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2012
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Mathematical Programming",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2005
    }, {
      "title" : "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition",
      "author" : [ "Y. Pati", "R. Rezaiifar", "P. Krishnaprasad" ],
      "venue" : "Signals, Systems and Computers,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 1993
    }, {
      "title" : "Learning analysis sparsity priors. In: Sampta’11",
      "author" : [ "G. Peyré", "J. Fadili" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2011
    }, {
      "title" : "Updating P300: an integrative theory of P3a and P3b",
      "author" : [ "J. Polich" ],
      "venue" : "Clinical neurophysiology",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2007
    }, {
      "title" : "Surveying and comparing simultaneous sparse approximation (or group-Lasso) algorithms",
      "author" : [ "A. Rakotomamonjy" ],
      "venue" : "Signal Processing",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2011
    }, {
      "title" : "K-SVD dictionary-learning for the analysis sparse model",
      "author" : [ "R. Rubinstein", "T. Faktor", "M. Elad" ],
      "venue" : "In: Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2012
    }, {
      "title" : "Nonlinear total variation based noise removal algorithms",
      "author" : [ "L. Rudin", "S. Osher", "E. Fatemi" ],
      "venue" : "Physica D: Nonlinear Phenomena",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 1992
    }, {
      "title" : "Signal restoration with overcomplete wavelet transforms: comparison of analysis and synthesis priors",
      "author" : [ "I.W. Selesnick", "M.A. Figueiredo" ],
      "venue" : "SPIE Optical Engineering+ Applications. International Society for Optics and Photonics,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2009
    }, {
      "title" : "Regression shrinkage and selection via the Lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological)",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 1996
    }, {
      "title" : "Sparsity and smoothness via the fused Lasso",
      "author" : [ "R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology)",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2005
    }, {
      "title" : "The solution path of the generalized Lasso",
      "author" : [ "R. Tibshirani", "J. Taylor" ],
      "venue" : "The Annals of Statistics",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2011
    }, {
      "title" : "Dictionary learning",
      "author" : [ "I. Tošić", "P. Frossard" ],
      "venue" : "IEEE Signal Processing Magazine",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2011
    }, {
      "title" : "Algorithms for simultaneous sparse approximation",
      "author" : [ "J. Tropp" ],
      "venue" : "Part II: Convex relaxation. Signal Processing",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2006
    }, {
      "title" : "Algorithms for simultaneous sparse approximation",
      "author" : [ "J. Tropp", "A. Gilbert", "M. Strauss" ],
      "venue" : "Part I: Greedy pursuit. Signal Processing",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2006
    }, {
      "title" : "Robust sparse analysis regularization",
      "author" : [ "S. Vaiter", "G. Peyré", "C. Dossal", "J. Fadili" ],
      "venue" : "IEEE Trans. on Information Theory",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2013
    }, {
      "title" : "Frequency domain models of the EEG",
      "author" : [ "P. Valdés", "J. Bosch", "R. Grave", "J. Hernandez", "J. Riera", "R. Pascual", "R. Biscay" ],
      "venue" : "Brain topography",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 1992
    }, {
      "title" : "An ADMM algorithm for a class of total variation regularized estimation problems",
      "author" : [ "B. Wahlberg", "S. Boyd", "M. Annergren", "Y. Wang" ],
      "venue" : "IFAC Symp. Syst. Ident",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2012
    }, {
      "title" : "Robust face recognition via sparse representation",
      "author" : [ "J. Wright", "A. Yang", "A. Ganesh", "S. Sastry", "Y. Ma" ],
      "venue" : "IEEE Trans. on Pattern Analysis and Machine Intelligence",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2009
    }, {
      "title" : "Augmented Lagrangian method, dual methods, and split Bregman iteration for ROF, vectorial TV, and high order models",
      "author" : [ "C. Wu", "X. Tai" ],
      "venue" : "SIAM Journal on Imaging Sciences",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2010
    }, {
      "title" : "Split Bregman method for large scale fused Lasso",
      "author" : [ "G. Ye", "X. Xie" ],
      "venue" : "Computational Statistics & Data Analysis",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2011
    }, {
      "title" : "Model selection and estimation in regression with grouped variables",
      "author" : [ "M. Yuan", "Y. Lin" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology)",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2006
    }, {
      "title" : "Modeling disease progression via fused sparse group",
      "author" : [ "J. Zhou", "J. Liu", "V. Narayan", "J. Ye" ],
      "venue" : "Lasso. In: Proc. 18th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining. ACM,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "sensing [14], image restoration [35], blind source separation [33] or classification [54] to name a few.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 33,
      "context" : "sensing [14], image restoration [35], blind source separation [33] or classification [54] to name a few.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 31,
      "context" : "sensing [14], image restoration [35], blind source separation [33] or classification [54] to name a few.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 52,
      "context" : "sensing [14], image restoration [35], blind source separation [33] or classification [54] to name a few.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 46,
      "context" : "Dictionary-based representations proceed by approximating a signal with a linear combination of elements, referred to as dictionary atoms, where the dictionary is either given based on the domain knowledge, or learned from a signal database [48].",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 14,
      "context" : "the data noise [15], particularly so when the dictionary atoms are highly correlated.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 39,
      "context" : "The decomposition of multi-channel signals involves: i) decomposing each channel into the dictionary (see [41] for a survey); ii) ensuring that the structure of the multi-channel data is preserved in the multi-dimensional decomposition.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 26,
      "context" : "Following and extending previous work [27], this paper focuses on structured dictionary-based decomposition, where the dictionary-based representation preserves the structure of the signal, as follows.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 8,
      "context" : "The approach is assessed in terms of i) its computational cost compared with the state-of-the-art smooth proximal gradient [9] approach; ii) its ability to recover the sparse structure of the initial signals compared with standard sparsity constraints and fused LASSO (`0, `2,0, `1, `2,1 and `1 + `2,1) even though the true structure of the signal is used to define the fused-LASSO regularization.",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 16,
      "context" : "The use of the ‖XP‖1 regularization term can be interpreted in terms of sparse analysis [17].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 37,
      "context" : "The matrix P can be learned from a set of signals [39, 42].",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 40,
      "context" : "The matrix P can be learned from a set of signals [39, 42].",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 41,
      "context" : "The TV term leads to a sparse decomposition gradient, thus preserving signal singularities and data edges, and supporting the detection of abrupt changes [43].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 0,
      "context" : "This combination of regularization terms is known as the fused-LASSO and has been chosen for the evaluation of the proposed algorithm on synthetic data because of its interest for various applications: frequency hopping [1], geophysical studies [19], multi-task learning [58], trend analysis [32], covariance estimation [12], analysis of association between genetic markers and traits [31] or change point detection [6], among others.",
      "startOffset" : 220,
      "endOffset" : 223
    }, {
      "referenceID" : 18,
      "context" : "This combination of regularization terms is known as the fused-LASSO and has been chosen for the evaluation of the proposed algorithm on synthetic data because of its interest for various applications: frequency hopping [1], geophysical studies [19], multi-task learning [58], trend analysis [32], covariance estimation [12], analysis of association between genetic markers and traits [31] or change point detection [6], among others.",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 56,
      "context" : "This combination of regularization terms is known as the fused-LASSO and has been chosen for the evaluation of the proposed algorithm on synthetic data because of its interest for various applications: frequency hopping [1], geophysical studies [19], multi-task learning [58], trend analysis [32], covariance estimation [12], analysis of association between genetic markers and traits [31] or change point detection [6], among others.",
      "startOffset" : 271,
      "endOffset" : 275
    }, {
      "referenceID" : 11,
      "context" : "This combination of regularization terms is known as the fused-LASSO and has been chosen for the evaluation of the proposed algorithm on synthetic data because of its interest for various applications: frequency hopping [1], geophysical studies [19], multi-task learning [58], trend analysis [32], covariance estimation [12], analysis of association between genetic markers and traits [31] or change point detection [6], among others.",
      "startOffset" : 320,
      "endOffset" : 324
    }, {
      "referenceID" : 30,
      "context" : "This combination of regularization terms is known as the fused-LASSO and has been chosen for the evaluation of the proposed algorithm on synthetic data because of its interest for various applications: frequency hopping [1], geophysical studies [19], multi-task learning [58], trend analysis [32], covariance estimation [12], analysis of association between genetic markers and traits [31] or change point detection [6], among others.",
      "startOffset" : 385,
      "endOffset" : 389
    }, {
      "referenceID" : 5,
      "context" : "This combination of regularization terms is known as the fused-LASSO and has been chosen for the evaluation of the proposed algorithm on synthetic data because of its interest for various applications: frequency hopping [1], geophysical studies [19], multi-task learning [58], trend analysis [32], covariance estimation [12], analysis of association between genetic markers and traits [31] or change point detection [6], among others.",
      "startOffset" : 416,
      "endOffset" : 419
    }, {
      "referenceID" : 10,
      "context" : "Quite a few methods have been designed in the last decade to achieve the sparse approximation [11] of multi-dimensional signals while the monodimensional case has been intensively studied.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 48,
      "context" : "Two main approaches have been considered: greedy methods trying to approximate the solution of the `0 regularized problem [50] and convex optimization solvers working on the `1 relaxed problem [49, 23, 41].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 47,
      "context" : "Two main approaches have been considered: greedy methods trying to approximate the solution of the `0 regularized problem [50] and convex optimization solvers working on the `1 relaxed problem [49, 23, 41].",
      "startOffset" : 193,
      "endOffset" : 205
    }, {
      "referenceID" : 22,
      "context" : "Two main approaches have been considered: greedy methods trying to approximate the solution of the `0 regularized problem [50] and convex optimization solvers working on the `1 relaxed problem [49, 23, 41].",
      "startOffset" : 193,
      "endOffset" : 205
    }, {
      "referenceID" : 39,
      "context" : "Two main approaches have been considered: greedy methods trying to approximate the solution of the `0 regularized problem [50] and convex optimization solvers working on the `1 relaxed problem [49, 23, 41].",
      "startOffset" : 193,
      "endOffset" : 205
    }, {
      "referenceID" : 25,
      "context" : "The concept of structured sparsity then emerged from the integration of prior knowledge encoded as regularizations into these approximation problems [26, 29].",
      "startOffset" : 149,
      "endOffset" : 157
    }, {
      "referenceID" : 28,
      "context" : "The concept of structured sparsity then emerged from the integration of prior knowledge encoded as regularizations into these approximation problems [26, 29].",
      "startOffset" : 149,
      "endOffset" : 157
    }, {
      "referenceID" : 41,
      "context" : "Some analysis regularizations have been extensively studied like the TV one which has been introduced in the ROF model [43, 13] for image denoising, nonetheless in the context of dictionary-based decomposition, their study has only begun recently [44, 51, 36].",
      "startOffset" : 119,
      "endOffset" : 127
    }, {
      "referenceID" : 12,
      "context" : "Some analysis regularizations have been extensively studied like the TV one which has been introduced in the ROF model [43, 13] for image denoising, nonetheless in the context of dictionary-based decomposition, their study has only begun recently [44, 51, 36].",
      "startOffset" : 119,
      "endOffset" : 127
    }, {
      "referenceID" : 42,
      "context" : "Some analysis regularizations have been extensively studied like the TV one which has been introduced in the ROF model [43, 13] for image denoising, nonetheless in the context of dictionary-based decomposition, their study has only begun recently [44, 51, 36].",
      "startOffset" : 247,
      "endOffset" : 259
    }, {
      "referenceID" : 49,
      "context" : "Some analysis regularizations have been extensively studied like the TV one which has been introduced in the ROF model [43, 13] for image denoising, nonetheless in the context of dictionary-based decomposition, their study has only begun recently [44, 51, 36].",
      "startOffset" : 247,
      "endOffset" : 259
    }, {
      "referenceID" : 34,
      "context" : "Some analysis regularizations have been extensively studied like the TV one which has been introduced in the ROF model [43, 13] for image denoising, nonetheless in the context of dictionary-based decomposition, their study has only begun recently [44, 51, 36].",
      "startOffset" : 247,
      "endOffset" : 259
    }, {
      "referenceID" : 7,
      "context" : "Their interest for retrieving the underlying sparse structure of signals has been shown in [8].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 44,
      "context" : "To the best of our knowledge, the combination of these regularization terms had not been studied except for the particular case of the fused-LASSO introduced in [46].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 44,
      "context" : "[46] transformed the problem to a quadratic one and used standard optimization tools.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] a method solving this problem in the particular case of the fused-LASSO signal approximator (Φ = IC) and Tibshirani et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 45,
      "context" : "[47] designed a path method for the generalized LASSO problem.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "More recently, scalable approaches based on proximal sub-gradient methods [34], ADMM[53] and split Bregman iterations [56] have been successfully applied to the mono-dimensional generalized fused-LASSO.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 51,
      "context" : "More recently, scalable approaches based on proximal sub-gradient methods [34], ADMM[53] and split Bregman iterations [56] have been successfully applied to the mono-dimensional generalized fused-LASSO.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 54,
      "context" : "More recently, scalable approaches based on proximal sub-gradient methods [34], ADMM[53] and split Bregman iterations [56] have been successfully applied to the mono-dimensional generalized fused-LASSO.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 8,
      "context" : "Concerning the multi-dimensional fused-LASSO, an efficient method has been proposed recently in [9] for multi-task regression.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 35,
      "context" : "A proximal method [37] is applied to a smooth approximation of the fused-LASSO regularization terms in this study and can be considered for different analysis regularizations.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 43,
      "context" : "To conclude this state-of-the-art, an illustration of the different regularizations is plotted in Figure 2: `2 (top left), `1 [45] (top right), `2,1 [57] (middle left), `1 + `2,1 [22] (middle right), TV [43] (bottom right) and `1 + TV (bottom right).",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 55,
      "context" : "To conclude this state-of-the-art, an illustration of the different regularizations is plotted in Figure 2: `2 (top left), `1 [45] (top right), `2,1 [57] (middle left), `1 + `2,1 [22] (middle right), TV [43] (bottom right) and `1 + TV (bottom right).",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 21,
      "context" : "To conclude this state-of-the-art, an illustration of the different regularizations is plotted in Figure 2: `2 (top left), `1 [45] (top right), `2,1 [57] (middle left), `1 + `2,1 [22] (middle right), TV [43] (bottom right) and `1 + TV (bottom right).",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 41,
      "context" : "To conclude this state-of-the-art, an illustration of the different regularizations is plotted in Figure 2: `2 (top left), `1 [45] (top right), `2,1 [57] (middle left), `1 + `2,1 [22] (middle right), TV [43] (bottom right) and `1 + TV (bottom right).",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 19,
      "context" : "The split Bregman approach has been shown particularly well suited to `1 minimization problems [20] because of its ability to early detect zero and nonzero coefficients, which insures it a fast convergence on these problems.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "Denoting i the current iteration, the split Bregman scheme [20] is then written: (X, A, B) = argmin X∈RNΦ×T A∈RNΦ×T ,B∈RNΦ×NP ‖Y − ΦX‖2 + λ1‖A‖1 + λ2‖B‖1",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 53,
      "context" : "This scheme is equivalent to the one obtained with the augmented Lagrangian method (or ADMM) when the constraints are linear [55].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 19,
      "context" : "Empirically, it has been noted that only few iterations of this system are necessary for convergence [20].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 9,
      "context" : "(9) can be solved with the soft-thresholding operator [10]: A = SoftThreshold λ1 μ1 (X +D A), (10)",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "One of the most used methods to deal with this issue, is the Bartels–Stewart algorithm [2] whose time complexity is O(N Φ).",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "Convergence Thanks to the exact solving of the primal subproblem presented in the previous section, the convergence of the above scheme can be derived by drawing inspiration from the steps described in [7].",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 3,
      "context" : "iterations to improve this rate as it is classically done in augmented Lagrangian methods [4].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "As noticed earlier, the most efficient method proposed for solving the multidimensional fused-LASSO is a smooth proximal-gradient method [9].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 8,
      "context" : "QP and SOCP formulations could also be considered but have been shown to be much slower than the proximal method [9].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "Concerning the proximal approach, the minimization is performed by the classical FISTA method as in [9].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "More precisely, the variant named ”FISTA with backtracking” in [3] has been chosen.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "A tight bound of the distance between the strict cost and the smooth one is known theoretically [9] and is proportional to μ.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "This last observation can be understood by noticing that the gradient’s Lipschitzien coefficient of the smooth TV penalty presented in [9] is inversely proportional to the parameter μ.",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 36,
      "context" : "Regarding the `0 and `2,0 constraints, the solutions are respectively given by the orthogonal matching pursuit (OMP) [38] and the simultaneous OMP (SOMP) [50].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 48,
      "context" : "Regarding the `0 and `2,0 constraints, the solutions are respectively given by the orthogonal matching pursuit (OMP) [38] and the simultaneous OMP (SOMP) [50].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 15,
      "context" : "The `1 solutions are obtained by the LARS method [16] and a proximal approach (FISTA [3]) has been chosen to deal with the `2,1 and `1 + `2,1 regularizations [22].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "The `1 solutions are obtained by the LARS method [16] and a proximal approach (FISTA [3]) has been chosen to deal with the `2,1 and `1 + `2,1 regularizations [22].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 21,
      "context" : "The `1 solutions are obtained by the LARS method [16] and a proximal approach (FISTA [3]) has been chosen to deal with the `2,1 and `1 + `2,1 regularizations [22].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 43,
      "context" : "The approximation problems with the `1 and `2,1 regularization terms are respectively referred to as the LASSO [45] and group-LASSO [57] problems, where group-LASSO is used defining only one group by atom.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 55,
      "context" : "The approximation problems with the `1 and `2,1 regularization terms are respectively referred to as the LASSO [45] and group-LASSO [57] problems, where group-LASSO is used defining only one group by atom.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 29,
      "context" : "Among the compared algorithms, we have implemented the OMP and the SOMP, whereas the SPAMS [30] toolbox has been used for the other methods.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "The P300 speller introduced in [18] allows for instance to spell words (letter by letter) by detecting such potentials after the presentation of visual stimuli.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 38,
      "context" : "Its latency and amplitude depend on various factors like the target-to-target intervals (see [40] for a review).",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 50,
      "context" : "These signals are decomposed thanks to the Multi-SSSA algorithm on a Gabor time-frequency dictionary Φ ∈ RC×NΦ which is able to efficiently represent both transitory and oscilatory EEG components [52].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 16,
      "context" : "The dual of this dictionary is approximated by the Moore–Penrose pseudo-inverse of Φs [17].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 20,
      "context" : "The following steps have been carried out to construct this spatial dictionary: • a realistic head model has been built from MRI data and has been divided into voxels, • for each voxel and different orientations of voxels’ electrical activities, the associated EEG topographies have been computed by solving the EEG direct problem (with the software OpenMEEG [21]) and regrouped in a large spatial dictionary (≈ 5000 elements), • a subset of this large dictionary has been finally selected with a greedy approach such that its coherence does not exceed 0.",
      "startOffset" : 359,
      "endOffset" : 363
    }, {
      "referenceID" : 4,
      "context" : "The dataset IIb of the BCI Competition II [5] has been chosen to evaluate our model.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 24,
      "context" : "The BLDA (Bayesian Linear Discriminant Analysis) algorithm [25] has been chosen to perform the classification step.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 19,
      "context" : "This efficiency is explained from the split-Bregman ability to early detect zero coefficients (see [20], Appendix), thereby easily accommodating `1 regularization.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 27,
      "context" : "In addition, an extension of the microstates EEG model will be studied with a TV regularization matrix [28].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "[7].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "6) in [7]):",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 6,
      "context" : "[7]).",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2016,
    "abstractText" : "This paper addresses the structurally-constrained sparse decomposition of multidimensional signals onto overcomplete families of vectors, called dictionaries. The contribution of the paper is threefold. Firstly, a generic spatio-temporal regularization term is designed and used together with the standard `1 regularization term to enforce a sparse decomposition preserving the spatio-temporal structure of the signal. Secondly, an optimization algorithm based on the split Bregman approach is proposed to handle the associated optimization problem, and its convergence is analyzed. Our well-founded approach yields same accuracy as the other algorithms at the state-of-the-art, with significant gains in terms of convergence speed. Thirdly, the empirical validation of the approach on artificial and real-world problems demonstrates the generality and effectiveness of the method. On artificial problems, the proposed regularization subsumes the Total Variation minimization and recovers the expected decomposition. On the real-world problem of electro-encephalography brainwave decomposition, the approach outperforms similar approaches in terms of P300 evoked potentials detection, using structured spatial priors to guide the decomposition.",
    "creator" : "LaTeX with hyperref package"
  }
}