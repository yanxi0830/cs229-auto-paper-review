{
  "name" : "1606.01275.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Predicting with Distributions",
    "authors" : [ "Michael Kearns", "Zhiwei Steven Wu" ],
    "emails" : [ "mkearns@cis.upenn.edu", "wuzhiwei@cis.upenn.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 6.\n01 27\n5v 3\n[ cs\n.D S]\n9 J\nun 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider a new variant of the Probably Approximately Correct (PAC) learning framework. In our model, a joint distribution over vector pairs (x,y) is determined by an unknown target function c(x) that maps input vectors x not to individual outputs, but to entire distributions over output vectors y in some large space. This model generalizes settings such as learning with classification noise or errors, probablistic concepts (where y is a probabilistic but scalar function of x), multiclass learning (where y is a multi- or vector-valued but deterministic function of x), and settings in which the output space associated with a classification may be large and complex. It is an instance of a more general framework in which the distribution of multiple hidden variables—with unknown but parametric structural dependencies on observable inputs — determines the distribution of observable outputs. For the special case of a single binary hidden variable, we provide the first formal learning guarantees in a PAC framework.\nAs in the standard PAC model, we begin with an unknown binary function or concept c chosen from a known class C,1 whose inputs x are distributed according to an unknown and arbitrary distribution. Now, however, the value c(x) determines which of two unknown probability distributions Pc(x) govern the distribution of y, where P0 and P1 are chosen from a known class of distributions P . Thus y is distributed according to a mixture model, but the mixture component is given by a hidden classifier c. The learner does not see explicit labels c(x), but only the resulting (x,y) pairs. The goal is to learn a hypothesis model that consists of a hypothesis h that is a {0,1}-valued function, and two probability distributions P̂0 and P̂1 from the class P . Given any input x, the model will predict the vector y to be drawn from the distribution P̂h(x) (and hence\n*Dept. of Computer and Information Sciences, University of Pennsylvania. Email: mkearns@cis.upenn.edu †Dept. of Computer and Information Sciences, University of Pennsylvania. Email: wuzhiwei@cis.upenn.edu 1We leave the consideration of multi- or real-valued functions c(x) to future work.\npredict with distribution P̂h(x)). Our objective is to minimize the conditional Kullback-Leibler (KL) divergence Ex [ KL(Pc(x)||P̂h(x)) ]\n, rather than simply the KL divergence to the mixture. We thus refer to our model as Predicting with Distributions (PwD).\nOne of our primary motivations is composition and reducibility across different learning models — in this case, models for classification and models for distribution learning. Within the standard PAC (classification) model, there is a rich theory of reducibility between specific learning problems (Pitt and Warmuth, 1990; Kearns and Valiant, 1994), between classes of learning problems (Schapire, 1990; Kearns, 1998), as well as composition theorems allowing the creation of more complex learning algorithm from simpler ones (Kearns et al., 1994). Less common are results allowing one to assemble algorithms with provable performance guarantees from constituents that are solving different types of learning problems. A natural starting point for such an investigation is with the standard PAC supervised learning model, and its distributional analogue (Kearns et al., 1994), since these models are each already populated with a number of algorithms with strong theoretical guarantees.\nOur main technical interest is thus in conditions permitting computationally efficient learning algorithms composed of extant classification and distribution learning algorithms. Informally, our results imply that for every concept class C known to be PAC learnable with classification noise (Angluin and Laird, 1987), and almost every class P known to be PAC learnable in the distributional sense of Kearns et al. (1994), PwD problems given by (C,P ) are learnable in our framework."
    }, {
      "heading" : "1.1 Our Results and Techniques",
      "text" : "Our results take the form of reductions from ourmodel to algorithms for PAC learning the concept class C and the distribution class P separately.2 The primary conceptual step is in identifying the natural technical conditions that connect these two different classes of learning problems. The centerpiece in this “bridge” is the notion of a distinguishing event for two probability distributions P0,P1 ∈ P , which is an event whose probability is “signficantly” (inverse polynomially) different under P0 and P1, provided these distributions are themselves sufficiently different.\nOur first result shows that a distinguishing event can be used, via a particular randomized mapping, to turn the observed y into a noisy binary label for the unknown concept c. This will serve as a building block for us to combine efficient PAC learners from classification and distribution learning.\nWe then use distinguishing events to provide two different reductions of our model to PAC classification and distribution learning algorithms. In the “forward” reduction, we assume the distribution class P admits a small set of candidate distinguishing events. We show that such candidate events exist and can be efficiently constructed for the class of spherical Gaussians and product distributions over any discrete domain. By searching and verifying this set for such an event, we first PAC learn c from noisy examples, then use the resulting hypothesis to “separate” P0 and P1 for a distributional PAC algorithm for the class P . This gives:\n2Throughout the paper, all PAC learning algorithms (for both concept class C and distribution class P ) in our reduction runs in polynomial time, since we are primarily concerned with computational efficiency (as opposed to sample complexity).\nTheorem 1 (Informal Statement, Forward Reduction). Suppose that the concept class C is PAC learnable under classification noise, and the distribution class P is PAC learnable and admits a polynomial-sized set of distinguishing events. Then the joint class (C,P ) is PwD-learnable.\nIn the “reverse” reduction, we instead first separate the distributions, then use their approximations to learn c. Here we need a stronger distribution-learning assumption, but no assumption on distinguishing events. More precisely, we assume that mixtures of two distributions from P (which is exactly what the unconditioned y is) are PAC learnable. Once we have identified the (approximate) mixture components, we show they can be used to explicitly construct a specialized distinguishing event, which in turn lets us create a noisy label for c. This leads our result in the reverse reduction:\nTheorem 2 (Informal Statement, Reverse Reduction). Suppose that the concept class C is PAC learnable under classification noise, and any mixture of two distributions from P is PAC learnable. Then the joint class (C,P ) is PwD-learnable.\nIn both reductions, we make central use of Le Cam’s method to show that any PAC concept or distribution learning algorithm must have a certain “robustness” to corrupted data. Thus in both the forward and reverse directions, by controlling the accuracy of the model learned in the first step, we ensure the second step of learning will succeed.\nSince practically every C known to be PAC learnable can also be learned with classification noise (either directly or via the statistical query framework (Kearns, 1998), with parity-based constructions being the only known exceptions), and the distribution classes P known to be PAC learnable have small sets of distinguishing events (such as product distributions), and/or have mixture learning algorithms (such as Gaussians), our results yield efficient PwD algorithms for almost all combinations of PAC classification and distribution learning algorithms known to date."
    }, {
      "heading" : "1.2 Related Works",
      "text" : "At the highest level, our model falls under the framework of Haussler (1992), which gives a decision-theoretic treatment of PAC-style learning (Valiant, 1984) for very general loss functions; our model can be viewed as a special case in which the loss function is conditional log-loss given the value of a classifier. Whereas Haussler (1992) is primarily concerned with sample complexity, our focus here is on computational complexity and composition of learning models.\nAt a more technical level, our results nicely connect two well-studied models under the PAC learning literature. First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al. (2006) that established the equivalence of learning under (standard) classification noise (CN) and under class-conditional classification noise (CCCN). Our work also relies on the PAC model for distribution learning (Kearns et al., 1994), including a long line of works on learning mixtures of distributions (see e.g. Dasgupta (1999); Arora and Kannan (2001); Vempala and Wang (2004); Feldman et al. (2008)). Our newmodel of PwD learning, in particular, can be viewed as a composition of these two models.\nOur model is also technically related to the one of co-training (Blum and Mitchell, 1998) in that the input x and the output y give two different views on the data, and they are conditionally independent given the unknown label z = c(x), which is also a crucial assumption for co-training (as well as various other latent variable models for inference and learning). However, our model\nis also fundamentally different from co-training in two ways. First, in our model, there is not a natural target Boolean function that maps y to the label z. For example, any outcome y can be generated from both distributions P0 and P1. In other words, just using y is not sufficient for identifying the label z. Second, our learning goal is to predict what distribution the outcome y is drawn from given the input x, as opposed to predicting the unknown label z."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Model: PwD-Learning",
      "text" : "Let X denote the space of all possible contexts, and Y denote the space of all possible outcomes. We assume that all contexts x ∈ X are of some common length n, and all outcomes y ∈ Y are of some common length k. Here the lengths are typically measured by the dimension; the most common examples for X are the boolean hypercube {0,1}n and subsets of Rn ({0,1}k and Rk for Y ).\nLet C be a class of {0,1}-valued functions (also called concepts) over the context space X , and P be a class of probability distributions over the outcome space Y . We assume an underlying distribution D over X , a target concept c ∈ C, and target distributions P0 and P1 in P . Together, we will call the tuple (c,P0,P1) the target model.\nGiven any target model (c,P0,P1) and underlying distributionD, our learning algorithm is then given sample access to the following generative example oracle Gen(D, c,P0,P1) (or simply Gen). On each call, the oracle does the following (see Figure 1 for an illustration):\n1. Draws a context x randomly according to D;\n2. Evaluates the concept c on x, and draws an outcome y randomly from Pc(x);\n3. Returns the context-outcome pair (x,y).\nA hypothesis model is a triple T = (h, P̂0, P̂1) that consists of a hypothesis h ∈ C and two hypothesis distributions P̂0 and P̂1 ∈ P . Given any context x, the hypothesis model predicts the outcome y to be drawn from the distribution P̂h(x) (or simply predicts with distribution P̂h(x)). The goal of our learning algorithm is to output a hypothesis model with high accuracy with respect to the target model, and the error of any model T is defined as\nerr(T ) = E x∼D\n[ KL(Pc(x)||P̂h(x)) ]\nwhere KL denotes Kullback-Leibler divergence (KL divergence). Our model of Predicting with Distributions learning (PwD-learning) is thus defined as follows.\nDefinition 1 (PwD-Learnable). Let C be a concept class over X , and P be a class of distributions over Y . We say that the joint class (C,P ) is PwD-learnable if there exists an algorithm L such that for any target concept c ∈ C, any distribution D over X , and target distributions P0,P1 ∈ P over Y , and for any ε > 0 and 0 < δ ≤ 1, the following holds: if L is given inputs ε,δ as inputs and sample access from Gen(D, c,P0,P1), then L will halt in time bounded by poly(1/ε,1/δ,n,k) and output a triple T = (h, P̂0, P̂1) ∈ C ×P ×P that with probability at least 1− δ satisfies err(T ) ≤ ε.\nObserve that the unconditional distribution over y is a mixture of the target distributions P0 and P1. In our model, it is not enough to learn the mixture distribution (which is a standard\nproblem in learningmixtures of distributions). Our learning objective is to minimize the expected conditional KL divergence, which is more demanding and in general requires a good approximation to the target concept c over X .\nAlso note that we have stated the definition for the “proper” learning case in which the hypothesis models lie in the target classes C and P . However, all of our results hold for the more general case in which they lie in potentially richer classes C′ and P ′."
    }, {
      "heading" : "2.2 Related Learning Models",
      "text" : "We now discuss two learning models related to our setting (see the appendix for formal definitions).\nCNLearning Wefirst introduce PAC learning under classification noise (CN) (Angluin and Laird, 1987). For any noise rate 0 ≤ η < 1/2, consider the example oracle EXηCN(c,D) that on each call draws an example (x,c(x)) randomly according to D, then with probability 1−η returns the uncorrupted example (x,c(x)), and with probability η returns the erroneous example (x,¬c(x)). The concept class C is CN learnable if there exists a polynomial-time algorithm that given sample access to EX\nη CN finds a hypothesis h ∈ C that approximately minimizes the classification error:\nerr(h) = Prx∼D[c(x) , h(x)].\nCCCNLearning In amore general noisemodel calledClass-Conditional Classification Noise (CCCN) proposed by Ralaivola et al. (2006), the example oracle EX\nη CCCN has class-dependent noise rates—\nthat is, the noise rate η0 for the negative examples (c(x) = 0) and the noise rate η1 for the positive examples (c(x) = 1) may be different, and both below 1/2. Moreover, Ralaivola et al. (2006) show that any class that is learnable under CN is also learnable under CCCN. (See the appendix for a formal statement).\nDistribution Learning We also make use of results from for PAC learning probability distributions (Kearns et al., 1994). A distribution class P is efficiently learnable if there exists a polynomialtime algorithm that, given sample access to an unknown target distribution P, outputs an accurate distribution P̂ such that KL(P ||P̂) ≤ ε for some target accuracy ε. For any distribution P ∈ P and any point y ∈ Y , we assume that we can evaluate the probability (density) of y assigned by P (referred to as learning with an evaluator in Kearns et al. (1994); see the appendix for the formal\ndefinition). We will write P(y) to denote the probability (or density) of point y, and write P(E) to denote Pry∼P [y ∈ E] for any measurable set E ⊂ Y .\nTo simplify our analysis, for the remainder of the paper wewill make the following assumption on the class P to ensure that the log-likelihood loss (or log-loss) is bounded in the domain Y . While this condition may not hold for some natural classes of distributions (e.g. Gaussians), it can be obtained using standard procedures (for instance, by truncating, or mixing with a small amount of the uniform distribution; see Feldman et al. (2006) for an example).\nAssumption 1 (Boundedness Assumption). There exists a quantity M that is upper bounded by poly(k) such that for any distribution P ∈ P and any point y ∈ Y , we have log(1/P(y)) ≤M ."
    }, {
      "heading" : "3 CN Learning with Identified Distinguishing Events",
      "text" : "In this section, we will introduce a central concept to our framework—distinguishing events. Informally, an event E ⊂ Y is distinguishing for distributions P0 and P1 if it occurs with different probabilities under the measures of P0 and P1. As a consequence, these events are informative about target concept c that determines which distribution the outcome y is drawn from. We will rely on such events to create a CCCN learning instance for the target concept c. Thus, whenever the class C is learnable under CN (and hence learnable under CCCN by Ralaivola et al. (2006)), we can learn the target concept c under the PwD model using a distinguishing event.\nDefinition 2 (Distinguishing Event). Let P and Q be distributions over the outcome space Y , and let ξ > 0. An event E ⊆ Y is ξ-distinguishing for distributions P and Q if |P(E)−Q(E)| ≥ ξ. We will call ξ the separation parameter for such an event.\nWe will now show that the knowledge of a distinguishing event between P0 and P1 allows us to simulate an example oracle EX\nη CCCN, and therefore we can learn the concept c with a CCCN\nlearner. The main technical problem here is to assign noisy labels based on the distinguishing event so that noise rates η0 and η1 of the oracle are strictly less than 1/2.\nOur solution is to construct a randomized mapping from the event to the labels.3 Let us first introduce some parameters. Let E ⊆ Y be a ξ-distinguishing event for the distributions P0 and P1 for some ξ ∈ (0,1]. We will write p = P0(E) and q = P1(E). Consider the following algorithm Lab(p̂, q̂,ξ) that takes parameters p̂, q̂ that are estimates for p and q, and the separation parameter ξ as inputs, and randomly creates noisy labels for (x,y) pair drawn from Gen:\n• Draw an example (x,y) from the oracle Gen.\n• If y ∈ E, assign label ℓ = 1 with probability a1 and ℓ = 0 with probability a0 = 1− a1; Otherwise, assign label ℓ = 1 with probability b1 and ℓ = 0 with probability b0 = 1− b1, where\na0 = 1/2+ ξ(p̂ + q̂ − 2) 4(q̂ − p̂) and b0 = 1/2+ ξ(p̂ + q̂) 4(q̂ − p̂) (1)\n• Output the labeled example (x,ℓ).\n3In the work of Blum and Mitchell (1998), the authors showed that any CN learnable class is also learnable when the class-conditional noise rates satisfy η0 + η1 < 1. Our construction here will imply a more general result—the class remains learnable when the noise rates satisfy η0 + η1 , 1.\nIt’s easy to check that both vectors (a0,a1) and (b0,b1) form valid probabilities over {0,1} (see the appendix for a proof).\nAs mentioned, we need to ensure the class-conditional noise rates to be below 1/2. As a first step, we work out the noise rates of Lab in terms of the true probabilities p and q, and show that the “estimated” noise rates based on p̂ and q̂ are below (1/2− ξ/4). Lemma 1. Given a fixed ξ-distinguishing event E, the class-conditional noise rates of Lab are\nη1 = Pr[ℓ = 0 | c(x) = 1] = qa0 + (1− q)b0 and η0 = Pr[ℓ = 1 | c(x) = 0] = pa1 + (1− p)b1. Moreover, given any input estimates (p̂, q̂) for (p,q), the parameters a0,a1,b0 and b1 satisfy:\nq̂a0 + (1− q̂)b0 = p̂a1 + (1− p̂)b1 ≤ 1/2− ξ/4. By Lemma 1, we know that as long as the input estimates p̂ and q̂ are sufficiently close to p and q, the noise rates will be less than 1/2. To obtain such estimates, we will guess the values of p and q on a grid of size ⌈1/∆⌉2 in the range of [0,1]2, where ∆ ∈ [0,1] is some discretization parameter. Note that for some pair of values i, j ∈ [⌈1/∆⌉] and i , j such that the guesses (p̂, q̂) = (i∆, j∆) satisfies p̂ ∈ [p −∆,p +∆] and q̂ ∈ [q −∆,q +∆] Given such accurate guesses p̂ and q̂, we can then guarantee low noise rates as derived below:\nLemma 2. Fix any ∆ ∈ [0,1]. Suppose that the estimates p̂ and q̂ satisfy |p− p̂| ≤ ∆ and |q− q̂| ≤ ∆, then the class-conditional noise rates η0 and η1 for Lab(p̂, q̂,ξ) are upper bounded by 1/2− ξ/4+∆.\nThus, if we choose the discretization parameter ∆ to be below ξ/4, then the algorithm Lab(p̂, q̂) is a valid example oracle EX\nη CCCN for some pair of guess estimates. Furthermore, if we apply the\ncorresponding CCCN learning algorithm to the instantiations of Lab(p̂, q̂) over all guesses (p̂, q̂), the output list of hypotheses is then guaranteed to contain an accurate one.\nLemma 3. Let ε,δ ∈ (0,1). Suppose that the concept class C is CN learnable, and there exists an identified ξ-distinguishing event E for the two target distributions P0 and P1. Then there exists an algorithm L1 such that when given ε,δ,ξ and E as inputs, it will halt in time bounded by poly(1/ε,1/δ,1/ξ,n), and with probability at least 1− δ, output a list of hypotheses that contains some h such that err(h) ≤ ε.\nIn the next two sections, we will use the algorithm in Lemma 3 as a subroutine for learning the target concept c in the PwD framework."
    }, {
      "heading" : "4 Forward Reduction",
      "text" : "Now we will give our forward algorithmic reduction: first use a CN learner to approximate the target concept c sufficiently well to separate the distributions P0 and P1, then learn each distribution using a distribution learner.4 We will rely on the result in Section 3 to learn c with a CCCN learner, but we do not assume the learner has a priori identified a distinguishing event. Instead, we will assume that the distribution class P admits a parametric class of distinguishing events of polynomial size, which allows us to distinguish any two distributions in P with large KL-divergence.\n4We use the term “forward” to indicate that the reduction decomposes the learning process into the steps suggested by the generative model depicted in Figure 1.\nAssumption 2 (Parametric Class of Distinguishing Events). There exists a parametric class of events E(·) for the distribution class P such that for any γ > 0 and for any two probability distributions P and Q in P with KL(P ||Q) ≥ γ , the class of events E(γ ) contains a ξ-distinguishing event E for P and Q, where ξ ≥ 1/ poly(k,1/γ ). Furthermore, E(γ ) can be computed in time poly(k,1/γ ) and the cardinality |E(γ )| ≤ poly(k,1/γ ).\nTo illustrate the intuition of how to construct such class of distinguishing events, we will give a simple example here. In the appendix, we will extend the construction to work for the class of spherical Gaussian distributions and product distributions over discrete domains.\nSimple Example Consider the outcome space Y = {0,1}k and the class of full-support product distributions P over Y . Let P,Q ∈ P be two distribution such that KL(P ||Q) ≥ γ . Under the boundedness condition in Assumption 1, it can be shown that there exists some coordinate l such that |P l −Ql | ≥ 1/poly(k,1/γ ), where P l = Pry∼P [yl = 1] and Ql = Pry∼Q[yl = 1]. Therefore, for each coordinate l, the event that the coordinate yj is 1 is a candidate distinguishing event, so the class of events is simply E = {1[yl = 1] | l ∈ [k]}.\nHere is our main result in the forward reduction.\nTheorem 3 ((Formal version of Theorem 1)). Under the Assumption 2 that P admits a parametric class of events E , the joint class (C,P ) is PwD-learnable as long as the concept class C is CN learnable, and the distribution class P is efficiently learnable.\nWe will present our reduction in three key steps.\n1. First, as a simple extension to Section 3, we can learn a hypothesis h with sufficiently small error assuming the class of events E contains a distinguishing event for the distributions P0 and P1.\n2. Suppose we have learned an accurate hypothesis h from the first step, we can then use h to separate outcomes y drawn from P0 and P1, and apply the distribution learner to learn accurate distributions P̂0 and P̂1. This creates an accurate hypothesis model T̂ = (h, P̂0, P̂1).\n3. Finally, we need to handle the case where the distributions P0 and P1 are arbitrarily close, and there is no distinguishing event for us to learn the concept c. We will show in this case it is not necessary to learn the target concept, and we can directly learn the distributions without relying on an accurate hypothesis h.\nThe main technical challenge lies in the second and third steps, where we will apply the distribution learner (for single distributions in P ) on samples drawn from a mixture of P0 and P1. To tackle this issue, we will prove a robustness result for any distribution learner — as long as the input distribution is sufficiently close to the target distribution, the output distribution by the learner remains accurate. 5"
    }, {
      "heading" : "4.1 CN Learning with a Class of Events",
      "text" : "As a first step in our reduction, we will simply extend Lemma 3: for each event E in the event class E , run the CCCN learner using E as a candidate distinguishing event. If the two target\n5Our result actually extends to any PAC learning algorithm, and we omit the simple details.\ndistributions P0 and P1 have large KL divergence, then one of the output hypotheses h will be accurate with respect to c:\nLemma 4. Let ε,δ ∈ (0,1) and γ > 0. Suppose that the class C is CN learnable, the class P admits a parametric class of events E (as in Assumption 2). If the two distributions P0 and P1 satisfy max{KL(P0||P1),KL(P1||P0)} ≥ γ , then there exists an algorithm L2 that given sample access to Gen and ε,δ,γ as inputs, runs in time poly(1/ε,1/δ,1/γ,n), and with probability at least 1− δ outputs a list of hypotheses H that contains a hypothesis h with error err(h) ≤ ε."
    }, {
      "heading" : "4.2 Robustness of Distribution Learner",
      "text" : "Before we proceed to the next two steps of the reduction, we will briefly digress to give a useful robustness result showing that the class P remains efficiently learnable even if the input distribution is slightly perturbed. Our result relies on the well-known Le Cam’s method, which is a powerful tool for giving lower bounds in hypothesis testing. We state the following version for our purpose.6\nLemma 5. [Le Cam’s method (see e.g. Le Cam (1986); Yu (1997))] Let Q0 and Q1 be two probability distributions over Y , and let A : Ym → {0,1} be a mapping from m observations in Y to either 0 or 1. Then\nPr A,Ym∼Qm0 [A(Ym) , 0] + Pr A,Ym∼Qm1 [A(Ym) , 1] ≥ 1− √ mKL(Q0||Q1)/2\nwhere Ym ∼Qmθ denotes an i.i.d. sample of size m drawn from the distribution Qθ . The lemma above shows that any statistical procedure that determines whether the underlying distribution is Q0 or Q1 based on m independent observations must have high error if the two distributions are too close. In particular, if their KL divergence satisfies KL(Q0||Q1) ≤ 1/m, then the procedure has at least constant error probability under measureQ0 or Q1. Now let’s construct such a procedure A using any distribution learner L for the class P . Suppose the learner is εaccurate with high probability when given sample of sizem, and the distributionQ0 is in the class P . Consider the following procedure A:\n• Run the learning algorithm L on sample S of size m. If the algorithm fails to output a hypothesis distribution, output 1. Otherwise, let Q̂ be the output distribution by L.\n• If KL(Q0||Q̂) ≤ ε, output 0; otherwise output 1. Note that if the sample S is drawn from the distribution Q0, then A will correctly output 0 with high probability based on the accuracy guarantee of L. This means the procedure has to err when S is drawn from the slightly perturbed distributionQ1, and so the learner will with constant probability output an accurate distribution Q̂ such that KL(Q0||Q̂) ≤ ε. More formally: Lemma 6. Let ε > 0, δ ∈ (0,1/2) and m ∈ N. Suppose there exists a distribution learner L such that for any unknown target distribution P ∈ P , when L inputs m random draws from P , it with probability at least 1− δ outputs a distribution P̂ such that KL(P ||P̂) ≤ ε. Then for any Q0 ∈ P and any distribution Q1 over the same range Y , if the learner L inputs a sample of size m drawn independently from Q1, it will with probability at least 1 − δ′ output a distribution Q̂ such that KL(Q0||Q̂) ≤ ε, where δ′ = δ + √\nmKL(Q0||Q1)/2. 6In the usual statement of Le Cam’s method, the right-hand side of the inequality is in fact 1− ‖Qm0 −Qm1 ‖tv , where\n‖ · ‖tv denotes total variation distance. We obtain the current bound by a simple application of Pinsker inequality.\nProof. Consider the procedure A constructed above that uses the learner L as a subroutine. By the guarantee of the algorithm, we know that PrL,Ym∼Qm0 [KL(Q0||Q̂) ≤ ε] ≥ 1− δ. This means\nPr A,Ym∼Qm0\n[A(Ym) ,Q0] ≤ δ.\nBy Lemma 5, we have\nPr A,Ym∼Qm1\n[A(Ym) ,Q1] ≥ 1− √ m\n2 KL(Q0||Q1)− δ.\nThis in turn implies that with probability at least (1−δ− √ m 2KL(Q0||Q1)) over the draws of Ym ∼Qm1 and the internal randomness of L, the output distribution Q̂ satisfies KL(P ||Q̂) ≤ ε.\nTherefore, if the KL divergence between the target distribution and the input distribution is smaller than inverse of the (polynomial) sample size, the output distribution by the learner is accurate with constant probability. By using a standard amplification technique, we can guarantee the accuracy with high probability:\nLemma 7. Suppose that the distribution class P is PAC learnable. There exist an algorithm L2 and a polynomial mP (·, ·, ·) such that that for any target unknown distribution P, when given any ε > 0 and 0 < δ ≤ 1/4 as inputs and sample access from a distributionQ such thatKL(P ||Q) ≤ 1/(2mP (1/ε,1/δ,k)), runs in time poly(1/ε,1/δ,k) and outputs a list of distributions P ′ that with probability at least 1 − δ contains some P̂ ∈ P ′ with KL(P ||P̂) ≤ ε.\nAs a consequence, even when input sample distribution is slightly “polluted”, we can still learn the target distribution accurately with a small blow-up in the computational and sample complexity."
    }, {
      "heading" : "4.3 Learning the Distributions with an Accurate Hypothesis",
      "text" : "Now we will return to the second step of our reduction: use an accurate hypothesis h and distribution learner for P to learn the two distributions P0 and P1. For any observation (x,y) drawn from the example oracle Gen, we can use the hypothesis h to determine whether the outcome y is drawn from P0 or P1, which allows us to create independent samples from both distributions. However, because of the small error of h with respect to the target concept c, the input sample is in fact drawn from a mixture between P0 and P1. To remedy this problem, we will choose a sufficiently small error rate for hypothesis h (but still an inverse polynomial in the learning parameters), which guarantees that the mixture is close enough to either one of single target distributions. We can then apply the result in Lemma 7 to learn each distribution, which together gives us a hypothesis model (h, P̂0, P̂1).\nLemma 8. Suppose that the distribution class P is efficiently learnable. Let ε > 0,0 < δ ≤ 1 and h ∈ C be an hypothesis. Then there exists an algorithm L3 and a polynomial r(·, ·, ·) such that when given ε, δ and h as inputs, L3 runs in time bounded by poly(1/ε,1/δ,k), and outputs a list of probability models T such that with probability at least 1 − δ there exists some T̂ ∈ T such that err(T̂ ) ≤ ε, as long as the hypothesis h satisfies err(h) ≤ 1/r(1/ε,1/δ,k)."
    }, {
      "heading" : "4.4 Directly Applying the Distribution Learner",
      "text" : "In the last step of our forward reduction, we will consider the case where the two target distributions P0 and P1 are too close to admit a distinguishing event, and so we will not be able to learn the target concept c as in the first step. We show that in this case learning c is not necessary for obtaining an accurate probability model — we can simply run the robust distribution learner developed in Lemma 7 over the samples drawn from the mixture to learn single distribution.\nWe will first define the following notion of healthy mixture, which captures the mixture distributions with non-trivial weights on two sufficiently different components. This will also facilitate our discussion in the reverse reduction.\nDefinition 3 (Healthy Mixture). Let Q be mixture of two distributions Q0 and Q1 from the class P , and let w0 and w1 be the weights on the two components respectively. Then Q is an η-healthy mixture if both min{w0,w1} ≥ η and max{KL(P0||P1),KL(P1||P0)} ≥ η hold. If one of the two conditions does not hold, we will call Q an η-unhealthy mixture.\nWe now show that whenever the mixture distribution P is unhealthy, we can use the robust learner in Lemma 7 to directly learn a distribution P̂ for our prediction purpose (simply always predict with P̂ regardless of the context x). Note that this not only includes the case where P0 and P1 are arbitrarily close, but also the one where the weight on one component is close to 0, which will be useful in Section 5.\nLemma 9. Suppose that the distribution class P is PAC learnable. Let P be the unconditional mixture distribution over the outcomes Y under the distribution Gen. Let ε > 0 and δ ∈ (0,1). Then there exists an algorithm L4 and a polynomial g(·, ·, ·) such that when L4 is given sample access to Gen and ε,δ as inputs, it runs in time bounded by poly(1/ε,1/δ,k) and it will with probability at least 1 − δ, output a list of distributions P ′ that contains P̂ with Ex∼D [ KL(Pc(x)||P̂) ]\n≤ ε, as long as P is an η-unhealthy mixture for some η ≤ 1/g(k,1/ε,1/δ).\nWe will now combine the all the tools to provide a proof sketch for Theorem 3 (see the appendix for details).\nProof Sketch for Theorem 3. Our algorithm for PwD learning the joint class (C,P ) is roughly the following. First, we will make use of Assumption 2 and obtain a set of candidate distinguishing events for the target distributions P0 and P1. We will run the CCCN learner to learn c using each candidate event E to generate noisy labels. This generates a list of hypotheses. We will use the hypotheses h to separate the two distributions P0 and P1 and apply the algorithm in Lemma 8 to learn each distribution individually. This will give polynomially many hypothesis models T̂ = (h, P̂0, P̂1). By Lemma 4 and Lemma 8, we know at least one of the models is accurate when P0 and P1 are sufficiently different.\nTo cover the case where the two distributions are too close, we will use the algorithm in Lemma 9 to learn a list of distributions over Y . In particular, the model (h′ , P̂ , P̂) is accurate for at least one of the output distribution P̂.\nTogether, the two procedures above will give a list of polynomially many hypothesis models, at least one of which is guaranteed to be accurate. We will use the standard maximum likehood method to output the model that minimizes empirical log-loss, and with high probability, this will be an accurate model.7\n7See the appendix for the details and analysis of the maximum likelihood method in the PwD model.\nWe previously gave examples (such as product distributions and special cases of multivariate Gaussians) that admit small classes of distinguishing events, and to which Theorem 3 can be applied. There are other important cases — such as general multivariate Gaussians — for which we do not know such classes.8 However, we now describe a different, “reverse” reduction that instead assumes learnability of mixtures, and thus is applicable to more general Gaussians via known mixture learning algorithms (Dasgupta, 1999; Arora and Kannan, 2001; Feldman et al., 2006)."
    }, {
      "heading" : "5 Reverse Reduction",
      "text" : "In our reverse reduction, our strategy is to first learn the two distributions P0 and P1 sufficiently well, and then construct a specialized distinguishing event to learn the target concept c with a CCCN learner.9 We will make a stronger learnability assumption on the distribution class P — we assume a parametrically correct learner for any healthy mixture of two distributions in P .\nAssumption 3 (Parametrically Correct Mixture Learning). There exists a mixture learner LM and a polynomial ρ such that for any ε > 0,0 < δ ≤ 1, and for any Z that is an η-healthy mixture of two distributions Y0 and Y1 from P , the following holds: if LM is given sample access to Z and ε,δ > 0 as inputs, LM runs in time poly(k,1/ε,1/δ) and with probability at least 1 − δ, outputs a mixture Ẑ of distributions Ŷ0 and Ŷ1 such that max{KL(Y0||Ŷ0),KL(Y1||Ŷ1)} ≤ ε.\nWe remark that the assumption of parametric correctness is a mild condition, and is satisfied by almost all mixture learning algorithms in the literature (see e.g. Dasgupta (1999); Feldman et al. (2006, 2008); Hsu and Kakade (2013)). Also note that we only require this condition when the healthy mixture condition in Definition 3 is met. If the two either the two distributions Y0 and Y1 are arbitrarily close or the mixture is extremely unbalanced, we are not supposed to learn both components correctly.\nTheorem 4 (Formal Version of Theorem 2). Suppose the class C is CN learnable, the distribution class P is efficiently learnable and satisfies the parametrically correct mixture learning assumption (Assumption 3). Then the joint class (C,P ) is PwD-learnable.\nWith the tools we develop for the forward reduction, the proof for reverse reduction is straightforward. There are essentially two cases we need to deal with. In the first case where the mixture distribution over Y is healthy, we can use the parametrically correct mixture learner to learn the two target distributions, we can then use the accurate approximations P̂0 and P̂1 to find a distinguishing event for P0 and P1, which allows us to learn the concept c with a CCCN learner. In the case where the mixture distribution is unhealthy and we cannot learn the components accurately, we can again appeal to the robustness result we show using Le Cam’s method — we can directly apply the learner for single distributions and learn P0 or P1.\n8We conjecture that Gaussians do indeed have a small set of distinguishing events, but have not been able to prove it.\n9We use the term “reverse” to indicate that the reduction decomposes the learning process into the steps suggested by the inverted generative model depicted in Figure 2."
    }, {
      "heading" : "5.1 CN Learning with a Mixture Learner",
      "text" : "Given any two distributions P , Q over Y and a parameter τ, consider the event (or subset)\nE(P,Q,τ) = {y ∈ Y | P(y) ≥ 2τQ(y)}\nWe will first show that such subset is a distinguishing event for the input distributions P and Q as long as the distributions P and Q are sufficiently different.\nLemma 10. Fix any γ ∈ (0,1]. Suppose thatKL(P ||Q) ≥ γ , then E(P,Q,γ/2) is a (γ2/(8M))-distinguishing event for the distributions P and Q.\nNext, we show that even if we only have access to the approximate distributions P̂ and Q̂, we can still identify a distinguishing event for P and Q, as long as the approximations are accurate.\nLemma 11. Suppose that the distributions P, P̂,Q,Q̂ over Y satisfy that KL(P ||P̂) ≤ α, KL(Q||Q̂) ≤ α, and KL(P ||Q) ≥ γ for some α,γ ∈ (0,1]. Then the event E(P̂, Q̂, (γ2/(8M)− √ 2α)2) is a ξ-distinguishing\nevent with ξ ≥ 1/ poly(1/γ,1/α,k) as long as γ > 8M( √ 2α + (8M2α)1/8).\nGiven these structural lemmas, we now know a way to construct a distinguishing event based on approximations to the target distributions P0 and P1. We can then create a and use the algorithm in Lemma 3 to learn the concept c, and in turn compute a list of hypothesis models, one of which is guaranteed to be accurate when the mixture distribution is healthy.\nLemma 12. Suppose the class P satisfies the parametric mixture learning assumption (Assumption 3), the class C is CN learnable, and mixture distribution over Y is γ-healthy for some γ > 0. Then there exists an algorithm L that given ε,δ and γ as inputs and sample access from Gen, halts in time bounded by poly(1/ε,1/δ,1/γ,n,k), and with probability at least 1−δ, outputs a list of probability models T that contains some T̂ with err(T̂ ) ≤ ε.\nFinally, to wrap up and prove Theorem 4, we also need to handle the case where healthy mixture condition in Definition 3 does not hold. We will again appeal to the robust distribution learner in Lemma 9 to learn the distributions directly, and construct hypothesis models based on the output distributions. To guarantee that the output hypothesis model is accurate, we will again use the maximum likelihood method to select the model with the minimum empirical log-loss (formal proof deferred to the appendix)."
    }, {
      "heading" : "6 Future Work",
      "text" : "Despite the generality of our results and reductions, there remain some appealing directions for further research. These include allowing the conditioning event to be richer than a simple binary function c(x), for instance multi- or even real-valued. This might first entail the development of theories for noisy learning in such models, which is well-understood primarily in the binary setting.\nWe also note that our study has suggested an interesting problem in pure probability theory, namely whether general Gaussians permit a small class of distinguishing events.\nAcknowledgments We thank We thank Akshay Krishnamurthy and Shahin Jabbari for helpful discussions."
    }, {
      "heading" : "A Missing Details and Proofs",
      "text" : "A.1 Missing Details in Section 2\nDefinition 4 (CN Learnability (Angluin and Laird, 1987)). Let C be a concept class over X . We say that C is efficiently learnable with noise (CN learnable) if there exists a learning algorithm L such that for any c ∈ C, any distribution D over X , any noise rate 0 ≤ η < 1/2, and for any 0 < ε ≤ 1 and 0 < δ ≤ 1, the following holds: if L is given inputs ηb (where 1/2 > ηb ≥ η), ε,δ,n, and is given access to EX\nη CN(c,D), then L will halt in time bounded by poly(1/(1− 2ηb),1/ε,1/δ,n) and output a hypothesis\nh ∈ C that with probability at least 1− δ satisfies err(h) ≤ ε.\nLemma 13 (CN = CCCN (Ralaivola et al., 2006)). Suppose that the concept class C is CN learnable. Then there exists an algorithm LC and a polynomial mC(·, ·, ·, ·) such that for every target concept c ∈ C, any ε,δ ∈ (0,1], for any noise rates η0,η1 ≤ ηb < 1/2, if L is given inputs ε,δ,ηb and access to EX\nη CCCN(c,D), then L will halt in time bounded by mC(1/(1 − 2ηb),1/ε,1/δ,n), and output with probability at least 1 − δ a hypothesis h with error err(h) ≤ ε. We will say that LC is an (efficient) CCCN learner for C with sample complexity mC .\nDefinition 5 (Evaluator (Kearns et al., 1994)). Let P be a class of distributions over the outcome space Y . We say that P has a efficient evaluator if there exists a polynomial p such that for any n ≥ 1, and for any distribution P ∈ P , there exists an algorithm EP with runtime bounded by poly(k) that given an input y ∈ Y outputs the probability (density) assigned to y by P. Thus, if y ∈ Y , then EP (y) is the weight of y under P . We call EP an evaluator for P .\nA.2 Missing Proofs in Section 3\nClaim 1. The values of a0 and b0 satisfy a0,b0 ∈ [0,1].\nProof. Without loss of generality, let’s assume that q ≥ p + ξ. Since p + q ∈ [0,2], we know that a0 ≤ 1/2 and we can write\na0 = 1/2+ ξ(p + q − 2) 4(q − p) ≥ 1/2−\nξ\n2(q − p) ≥ 1/2− 1/2 ≥ 0\nSimilarly, we know that b0 ≥ 1/2 and we can write\nb0 = 1/2+ ξ(p + q) 4(q − p) ≤ 1/2+ ξ/2 ξ = 1\nThis proves our claim.\nLemma 1. Given a fixed ξ-distinguishing event E, the class-conditional noise rates of Lab are\nη1 = Pr[ℓ = 0 | c(x) = 1] = qa0 + (1− q)b0 and η0 = Pr[ℓ = 1 | c(x) = 0] = pa1 + (1− p)b1.\nMoreover, given any input estimates (p̂, q̂) for (p,q), the parameters a0,a1,b0 and b1 satisfy:\nq̂a0 + (1− q̂)b0 = p̂a1 + (1− p̂)b1 ≤ 1/2− ξ/4.\nProof. We can derive the probabilities as follows\nPr[ℓ = 0 | c(x) = 1] = Pr[(ℓ = 0)∧ (y ∈ E) | c(x) = 1] +Pr[(ℓ = 0)∧ (y < E) | c(x) = 1] = Pr\nGen [y ∈ E | c(x) = 1] Pr Lab [ℓ = 0 | (y ∈ E)∧ (c(x) = 1)]\n+ Pr Gen [y < E | c(x) = 1] Pr Lab [ℓ = 0 | (y < E)∧ (c(x) = 1)]\n= Pr Gen [y ∈ E | c(x) = 1]a0 + Pr Gen [y < E | c(x) = 1]b0 = qa0 + (1− q)b0\nSimilarly, we can also show that Pr[ℓ = 1 | c(x) = 0] = pa1 + (1 − p)b1. For the second part of the statement, we can show\nq̂a0 + (1− q̂)b0 = q̂ 2 + ξ(p̂ + q̂ − 2)q̂ 4(q̂ − p̂) + (1− q̂) 2 + ξ(p̂ + q̂)(1− q̂) 4(q̂ − p̂) = 1/2− ξ/4\np̂a1 + (1− p̂)b1 = p̂ 2 − ξ(p̂ + q̂ − 2)p̂ 4(q̂ − p̂) + (1− p̂) 2 − ξ(p̂ + q̂)(1− p̂) 4(q̂ − p̂) = 1/2− ξ/4\nwhich recovers our claim.\nLemma 2. Fix any ∆ ∈ [0,1]. Suppose that the estimates p̂ and q̂ satisfy |p− p̂| ≤ ∆ and |q− q̂| ≤ ∆, then the class-conditional noise rates η0 and η1 for Lab(p̂, q̂,ξ) are upper bounded by 1/2− ξ/4+∆. Proof. Since a0,a1,b0,b1 ∈ [0,1], and by our assumption on the accuracy of p̂ and q̂, we have\nη1 − (q̂a0 + (1− q̂)b0) = (qa0 + (1− q)b0)− (q̂a0 + (1− q̂)b0) = (q − q̂)(a0 − b0) ≤ ∆ η0 − (q̂a1 + (1− q̂)b1) = (qa1 + (1− q)b1)− (q̂a1 + (1− q̂)b1) = (q − q̂)(a1 − b1) ≤ ∆\nThe result of Lemma 1 tells us that\nq̂a0 + (1− q̂)b0 = p̂a1 + (1− p̂)b1 ≤ 1/2− ξ/4 Therefore, we must also have η0,η1 ≤ 1/2− ξ/4+∆.\nLemma 3. Let ε,δ ∈ (0,1). Suppose that the concept class C is CN learnable, and there exists an identified ξ-distinguishing event E for the two target distributions P0 and P1. Then there exists an algorithm L1 such that when given ε,δ,ξ and E as inputs, it will halt in time bounded by poly(1/ε,1/δ,1/ξ,n), and with probability at least 1− δ, output a list of hypotheses that contains some h such that err(h) ≤ ε. Proof. Since the concept class C is CN learnable, by the result of Ralaivola et al. (2006) we know there exists an efficient algorithm A that when given access to some example oracle EXηCCCN with η0,η1 ≤ 1/2−ξ/8, outputs a hypothesis h with error bounded ε with probability at least 1−δ, halts in time poly(1/ε,1/δ,1/ξ,n).\nNow let parameter ∆ = ξ/8, and consider the algorithm: for each pair of values (p̂, q̂) = (i∆, j∆) such that i, j ∈ [⌈1/∆⌉] and i , j, use the Lab(p̂, q̂,ξ) to generate labeled examples, and run the algorithmA with sample access to Lab; if the algorithm halts in time p and outputs an hypothesis ĥ, store the hypothesis in a the list H. In the end, output the hypothesis list.\nBy Lemma 2, we know for some guessed values of p′ and q′, the algorithm Lab(p′ ,q′ ,ξ) is an CCCN oracle with noise rates η0,η1 ≤ 1/2− ξ/8. Then by the guarantee of the learning algorithm, we know with probability at least 1− δ, the algorithm will output an ε-accurate hypothesis under these guesses.\nA.3 Missing Proofs in Section 4\nLemma 4. Let ε,δ ∈ (0,1) and γ > 0. Suppose that the class C is CN learnable, the class P admits a parametric class of events E (as in Assumption 2). If the two distributions P0 and P1 satisfy max{KL(P0||P1),KL(P1||P0)} ≥ γ , then there exists an algorithm L2 that given sample access to Gen and ε,δ,γ as inputs, runs in time poly(1/ε,1/δ,1/γ,n), and with probability at least 1− δ outputs a list of hypotheses H that contains a hypothesis h with error err(h) ≤ ε. Proof. Consider the following algorithm. We will first use the oracle E with input parameter γ to obtain a class of events E(γ ) that contains a ξ-distinguishing event E∗ with ξ ≥ poly(γ,1/n). Then for each event E ∈ E(γ ), we will run the algorithm A in Lemma 3 with accuracy parameters ε, δ, separation parameter ξ, and E as an hypothetical distinguishing event as input. For each event, the instantiation of algorithm A will halt in polynomial time. Furthermore, when the input event is E∗ it will with probability at least 1−δ outputs a list of hypotheses H that contains a hypothesis h such that err(h) ≤ ε by the guarantee of Lemma 3.\nLemma 7. Suppose that the distribution class P is PAC learnable. There exist an algorithm L2 and a polynomial mP (·, ·, ·) such that that for any target unknown distribution P, when given any ε > 0 and 0 < δ ≤ 1/4 as inputs and sample access from a distributionQ such thatKL(P ||Q) ≤ 1/(2mP (1/ε,1/δ,k)), runs in time poly(1/ε,1/δ,k) and outputs a list of distributions P ′ that with probability at least 1 − δ contains some P̂ ∈ P ′ with KL(P ||P̂) ≤ ε. Proof. Let L be a distribution learner that given a independent sample of size m drawn from the unknown target distribution P, runs in time bounded by poly(1/ε,1/δ,n) with probability at least 1−δ, outputs a distribution P ′ such that KL(P ||P ′) ≤ ε. By Lemma 6, we know that with probability at least (1/2− δ) ≥ 1/4, the algorithm can also output a distribution P ′′ such that KL(P ||P ′′) ≤ ε if the algorithm is given a sample of size m drawn from the distribution Q.\nLet r = log3/4(1/δ). Now we will run the algorithm r times on r independent samples, each of size m. Let P ′ be the list of output hypothesis distributions in these runs. We know that with probability at least 1−(1−1/4)r = 1−δ, there exists a distribution P̂ ∈ P ′ such that KL(P ||P̂) ≤ ε.\nThe following is a technical lemma that allows us to bound the KL divergence between between a mixture distribution and one of its component.\nLemma 14. Let P and Q be two distributions over Y and R be a mixture of P and Q with weights wp and wq respectively. Then we have KL(P ||R) ≤ wqKL(P ||Q). Proof. Let wp and wq be the weights associated with P and Q respectively in the mixture R.\nKL(P ||R) = ∫\ny P(y) log\n(\nP(y)\nR(y)\n)\ndy\n=\n∫\ny (wpP(y) +wqP(y)) log\n(\nwpP(y) +wqP(y)\nwpP(y) +wqQ(y)\n)\ndy\n(by the log-sum inequality) ≤ ∫\ny\n(\nwpP(y) log\n(\nwpP(y)\nwpP(y)\n))\ndy +\n∫\ny\n(\nwqP(y) log\n(\nwqP(y)\nwqQ(y)\n))\ndy\n=wqKL(P ||Q)\nwhich proves our claim.\nLemma 8. Suppose that the distribution class P is efficiently learnable. Let ε > 0,0 < δ ≤ 1 and h ∈ C be an hypothesis. Then there exists an algorithm L3 and a polynomial r(·, ·, ·) such that when given ε, δ and h as inputs, L3 runs in time bounded by poly(1/ε,1/δ,k), and outputs a list of probability models T such that with probability at least 1 − δ there exists some T̂ ∈ T such that err(T̂ ) ≤ ε, as long as the hypothesis h satisfies err(h) ≤ 1/r(1/ε,1/δ,k).\nProof. Our algorithm will first call the oracle Gen for N = Cm2(2/ε,4/δ,k) ( M2\nε2 log(1/δ)\n)\ntimes,\nwhere C is some constant (to be determined in the following analysis) and m2 is the polynomial upper bound for the runtime of the algorithm defined in Lemma 7. Then the algorithm will separate these data points (x,y)’s into two samples, one for h(x) = 0 and the other for h(x) = 1. For each sample corresponding to h(x) = j, if the sample size is at least m = m2(2/ε,4/δ), the run the learning algorithm L2 in Lemma 7 to the sample with target accuracy ε/2 and failure probability δ/4 and obtain a polynomial list of distributions Pj ; otherwise, simply output a singleton list containing any arbitrary distribution in P .\nLet j ∈ {0,1} and πj = Prx∼D[h(x) = j]. Let us first consider the case where πj ≥ ε/(2M). In order to invoke Lemma 14, we will upper bound the quantity wjKL(Pj ||P̂j ), where wj = Prx∼D[c(x) = j]. We know that for some large enough constantC, we can guarantee with probability at least 1−δ/4, we will collect at least m observations with h(x) = j. Let εh = err(h), note that when we instantiate the learner L2 on the sample with h(x) = j, the input distribution Ij is a (εh,1− εh)-mixture of the distributions P1−j and Pj . Then there exists a polynomial r such that if err(h) ≤ 1/r(1/ε,1/δ,k), we can have the following based on Lemma 14\nKL(Pj ||Ij ) ≤ εhKL(P ||Q) ≤ 1/mP (2/ε,4/δ,k)\nwheremP is the polynomial defined in Lemma 7. This means, the learning algorithm L2 will with probability at least 1−δ/4, returns some distribution P̂j in the output list such that KL(Pj ||P̂j ) ≤ ε/2, which implies that wjKL(Pj ||P̂j ) ≤ ε/2.\nSuppose that πj < ε/(2M), then we know that no matter what the distribution P̂j is, we have wjKL(Pj ||P̂j ) ≤ ε2M M = ε/2 by Assumption 1. Finally, our algorithm will output a list of probability models T = {(h, P̂0, P̂1) | P̂0 ∈ P0, P̂1 ∈ P1}, such that with probability at least 1− δ, there exists some model T̂ = (h, P̂0, P̂1) ∈ T such that\nerr(T ) = w0KL(P0||P̂0) +w1KL(P1||P̂1) ≤ ε,\nwhich recovers our claim.\nLemma 9. Suppose that the distribution class P is PAC learnable. Let P be the unconditional mixture distribution over the outcomes Y under the distribution Gen. Let ε > 0 and δ ∈ (0,1). Then there exists an algorithm L4 and a polynomial g(·, ·, ·) such that when L4 is given sample access to Gen and ε,δ as inputs, it runs in time bounded by poly(1/ε,1/δ,k) and it will with probability at least 1 − δ, output a list of distributions P ′ that contains P̂ with Ex∼D [ KL(Pc(x)||P̂) ]\n≤ ε, as long as P is an η-unhealthy mixture for some η ≤ 1/g(k,1/ε,1/δ). Proof. We first consider the case where the weight on one component is small, and without loss of generality assume that w1 ≤ ε/(4Mm). By Lemma 14 and Assumption 1, we know that\nKL(P0||R) ≤ w1KL(P0||P1) ≤ ε\n2Mm M ≤ 1/(2m).\nBy instantiating the algorithm in Lemma 7 with parameters (ε/2,δ), we know with probability 1−δ, there exists a hypothesis distribution P̂ in the output list such that KL(P0||P̂) ≤ ε/2. Again by our Assumption 1, we know KL(P1||P̂) ≤M , so it follows that\nE x∼D\n[ KL(Pc(x)||P̂) ] = w0KL(P0||P̂) +w1KL(P1||P̂) ≤ ε 2 + εKL(P1||P̂) 2Mm ≤ ε.\nNext suppose that we are in the second case where KL(P0||P1),KL(P1||P0) ≤ 1/(2m). We know from Lemma 14 that\nKL(P0||R) ≤ w1KL(P0||P1) ≤ 1/(2m) and, KL(P1||R) ≤ w0KL(P1||P0) ≤ 1/(2m)\nWe will also apply the algorithm in Lemma 7 which guarantees with probability at least 1− δ that there exists a hypothesis distribution P̂ in the output list P ′ such that KL(P0||P̂),KL(P1||P̂) ≤ ε/2, which implies that\nE x∼D\n[ KL(Pc(x)||P̂) ] = w0KL(P0||P̂) +w1KL(P1||P̂) ≤ ε.\nTherefore, there exists a distribution P̂ in the output list that satisfies our claim as long as we choose the polynomial g such that g(1/ε,1/δ,k) ≥max{2Mm/ε,2m} for all ε,δ and m.\nProof of Theorem 3 We will now combine the all the tools to prove Theorem 3. First, consider the class of events E(γ ) with γ = 1/g(1/ε,1/δ,k) (specified in Lemma 9). Then we will apply the CN algorithm L2 in Lemma 4 to obtain a list H of polynomially many hypotheses. For each h ∈ H, run the algorithm L3 with h as a candidate hypothesis. This will generate a list of a list of probability models T . If max{KL(P0||P1),KL(P1||P0)} ≥ γ , then T is guaranteed to contain an ε-accurate model with high probability (based on Lemma 4 and Lemma 8). Next, apply the distribution learner in Lemma 9 over the mixture distribution over Y . If the algorithm outputs a distribution P̂, create a model T ′ = (h0, P̂ , P̂), where hypothesis h0 labels every example as negative. If max{KL(P0||P1),KL(P1||P0)} < γ , we know T ′ is ε-accurate with high probability (based on Lemma 9). Finally, apply the maximum likelihood method to the list of models T ∪ {T ′}: draw a sample of polynomial size from Gen, then for each model T ∈ T ∪{T ′}, compute the empirical logloss over the sample, and output the model with the minimum log loss. By standard argument, we can show that the output model is accurate with high probability.\nA.4 Missing Proofs in Section 5\nLemma 10. Fix any γ ∈ (0,1]. Suppose thatKL(P ||Q) ≥ γ , then E(P,Q,γ/2) is a (γ2/(8M))-distinguishing event for the distributions P and Q.\nProof. Note that for any y ∈ E such that P(E) > 0, we have log P(y) Q(y) ≤M by Assumption 1, and for any y < E, we also have log (\nP(y) Q(y)\n)\n< γ/2.\nKL(P ||Q) = ∫\ny∈Y P(y) log\nP(y) Q(y) dy\n=\n∫\ny∈E P(y) log\nP(y) Q(y) dy +\n∫\ny<E P(y) log\nP(y) Q(y) dy\n< P(E)M + (1−P(E))γ 2 = γ\n2 + (M −γ/2)P(E) < γ 2 +MP(E)\nSince we know that KL(P ||Q) ≥ γ , it follows that P(E) > γ2M . Furthermore,\nP(E)−Q(E) = P(E) (\n1− Q(E) P(E)\n)\n≥ P(E)    \n  1− sup y∈E\nQ(y)\nP(y)\n     \n≥ P(E) ( 1− 2−γ/2 ) ≥ γ P(E) 4\nwhere the last step follows from the fact that 1− 2−a ≥ a/2 for any a ∈ [0,1]. It follows that\nP(E)−Q(E) > γ P(E) 4 > γ 2M γ 4 =\nγ2\n8M ,\nwhich proves our statement.\nLemma 11. Suppose that the distributions P, P̂,Q,Q̂ over Y satisfy that KL(P ||P̂) ≤ α, KL(Q||Q̂) ≤ α, and KL(P ||Q) ≥ γ for some α,γ ∈ (0,1]. Then the event E(P̂, Q̂, (γ2/(8M)− √ 2α)2) is a ξ-distinguishing\nevent with ξ ≥ 1/ poly(1/γ,1/α,k) as long as γ > 8M( √ 2α + (8M2α)1/8).\nProof. Since we have both KL(P ||P̂),KL(Q||Q̂) ≤ α, by Pinsker’s inequality, we can bound the total variation distances\n‖P − P̂‖tv ≤ √ α/2 and, ‖Q − Q̂‖tv ≤ √ α/2.\nBy Lemma 10 and the definition of total variation distance, we know that\n‖P −Q‖tv = sup E⊂Y |P(E)−Q(E)| ≥ γ2/(8M)\nBy triangle inequality, the above implies\n‖P̂ − Q̂‖tv ≥ γ2 8M − √ 2α ≡ b\nBy Pinsker’s inequality, we know that ‖P̂ − Q̂‖tv ≤ √\nKL(P̂ ||Q̂)/2. It follows that KL(P̂ ||Q̂) ≥ 2b2. Consider the event E = E(P̂, Q̂,b2). We know by Lemma 10 that E is a (b4/(2M))-distinguishing event for distributions P̂ and Q̂. Since both KL(P ||P̂),KL(Q||Q̂) ≤ α, we have\n|P(E)− P̂(E)| ≤ ‖P(E′)− P̂(E′)‖tv ≤ √ α/2 and, |Q(E)− Q̂(E)| ≤ ‖Q(E′)− P̂(E′)‖tv ≤ √ α/2.\nSince E is a (b4/(2M))-distinguishing event for the distributions P̂ and Q̂, this means |P̂(E)−Q̂(E)| ≥ (b4/(2M)), and by triangle inequality, we have\n|P(E)−Q(E)| = |(P(E)− P̂(E)) + (P̂(E)− Q̂(E)) + (Q̂(E)−Q(E))| ≥ |P̂(E)− Q̂(E)| − |P(E)− P̂(E)| − |Q̂(E)−Q(E)| ≥ (b4/(2M))− √ 2α\nNote that if we have γ > 8M( √ 2α + (8M2α)1/8), then we can guarantee both b > 0 and (b4/(2M))−√\n2α > 0.\nLemma 12. Suppose the class P satisfies the parametric mixture learning assumption (Assumption 3), the class C is CN learnable, and mixture distribution over Y is γ-healthy for some γ > 0. Then there exists an algorithm L that given ε,δ and γ as inputs and sample access from Gen, halts in time bounded by poly(1/ε,1/δ,1/γ,n,k), and with probability at least 1−δ, outputs a list of probability models T that contains some T̂ with err(T̂ ) ≤ ε. Proof. We will first invoke the algorithm LM in Assumption 3 so that with probability at least 1−δ/2, the output approximations for the two components satisfy KL(P0||P̂0) ≤ α andKL(P1||P̂1) ≤ α for someα that satisfies γ > 8M( √ 2α+(8M2α)1/8). This process will halt in time poly(1/α,1/δ,1/γ,k).\nBy Lemma 10, we know that the either event E(P̂0, P̂1,γ/2) is a ξ-distinguishing event for P0 and P1 for some ξ ≥ 1/ poly(1/γ,n,k). Then we can use the CN learning algorithm L1 in Lemma 3 with the distinguishing event E to learn a list of hypotheses H under polynomial time, and there exists some h ∈ H that is ε1 accurate, with ε1 = 1/r(1/ε,1/δ,k) (specified in Lemma 8). For each hypothesis h′ ∈H, run the algorithm L3 with h′ as the candidate hypothesis and ε as the target accuracy parameter. By Lemma 8, this will halt in polynomial time, and outputs a list of probability models T such that one of which has error err(T̂ ) ≤ ε.\nProof of Theorem 4 The algorithm consists of three steps. First, we will run the algorithm in Lemma 12 by setting γ = 1/g(1/ε,δ,k) (specified in Lemma 8) and other parameters in a way to guarantee that whenever max{KL(P0||P̂0),KL(P1||P̂1)} ≥ γ and min{w0,w1} ≥ γ both hold, the output list of models T contains some T that has error at most ε. Next, we will directly apply the distribution learner in Lemma 9 so that when the healthy mixture condition is not met, the algorithm outputs a distribution P̂ such that Ex∼D [ KL(Pc(x)||P̂) ]\n. Lastly, similar to the final step in the forward reduction, we run the maximum likelihood algorithm to output the model in T ∪ {(h0, P̂ , P̂)} with the smallest empirical log-loss."
    }, {
      "heading" : "B Maximum Likelihood Algorithm",
      "text" : "In this section, we will formally define the maximum likelihood algorithm, which is a useful subroutine to select an accurate probability model from a list of candidate models. First, to give some intuition, we show that the objective of minimizing Ex∼D [ KL(Pc(x)||P̂h(x)) ] is equivalent to minimizing the expected log-losses. For any distribution P̂ over Y and a point r ∈ Y , the log likelihood loss (or simply log-loss) is defined as loss(y, P̂) = − log P̂(y). The entropy of a distribution P over range Y , denoted H(P), is defined as\nH(P) =\n∫\ny∈Y P(y) log\n1\nP(y) dy\nFor any two distributions P and P̂ over Y , we could write KL-divergence as\nKL(P ||P̂) = ∫\ny∈Y P(y) log\n1\nP̂(y) dy −H(P) = E y∼P\n[ − log P̂(y) ] −H(P) (2)\nwhich will be useful for proving the next lemma.\nLemma 15. Given any hypothesis h : X → {0,1}, and hypothesis distributions P̂0 and P̂1, we have\nE x∼D\n[ KL(Pc(x)||P̂h(x)) ]\n= E x∼D\n[ H(Pc(x)) ] − E (x,y)∼Gen [ log(P̂h(x)(y)) ]\nProof. We can write the following\nE x∼D\n[ KL(Pc(x)||Ph(x)) ] = Pr D [c(x) = 1,h(x) = 1]KL(P1||P̂1) + PrD [c(x) = 1,h(x) = 0]KL(P1||P̂0)\n+ Pr D [c(x) = 0,h(x) = 1]KL(P0||P̂1) + PrD [c(x) = 0,h(x) = 0]KL(P0||P̂0)\n(apply Equation (2)) = E x∼D\n[ H(Pc(x)) ] − ∑\n(i,j)∈{0,1}2 Pr D [c(x) = i,h(x) = j] E y∼Pi\n[ log(P̂j(y)) ]\n= E x∼D\n[ H(Pc(x)) ] − E (x,y)∼Gen [ log(P̂h(x)(y)) ]\nwhich proves our claim.\nTherefore, we could write err(T ) = Ex∼D [ H(Pc(x)) ] −E(x,y)∼Gen [ log(P̂h(x)(y)) ] for any model T =\n(h, P̂0, P̂1). Observe that Ex∼D [ H(Pc(x)) ] is independent of the choices of (h, P̂0, P̂1), so our goal can also be formulated as minimizing the expected log-loss E(x,y)∼Gen [ log(P̂h(x)(y)) ]\n. To do that, we will use the following maximum likelihood algorithm: given a list of probability models T as input, draw a set of S of samples (x,y)’s from Gen, and for each T = (h, P̂0, P̂1) ∈ T , compute the log-loss on the sample\nloss(S,T ) = ∑\n(x,y)∈S loss(y,Ph(x)),\nand lastly output the probability model T̂ ∈ T with the smallest loss(S,T ). Our goal is to show that if the list of models T contains an accurate model T , the maximum likelihood algorithm will then output an accurate model with high probability.\nTheorem 5. Let ε > 0. Let T be a set of probability models such that at least one model T ∗ ∈ T has error err(T ∗) ≤ ε. Suppose that the class P also satisfies bounded assumption (in Assumption 1).\nIf we run the maximum likelihood algorithm on the list T using a set S of independent samples drawn from Gen. Then, with probability at least 1 − δ, the algorithm outputs some model T̂ ∈ T such that err(T̂ ) ≤ 4ε with\nδ ≤ (|T |+1)exp (−2mε2\nM2\n)\n.\nTo prove this result, we rely on the Hoeffding concentration bound.\nTheorem 6. Let x1, . . . ,xn be independent bounded random variables such that each xi falls into the interval [a,b] almost surely. Let X = ∑\ni xi . Then for any t > 0 we have\nPr[X −E [X] ≥ t] ≤ exp ( −2t2 n(b − a)2 ) and Pr[X −E [X] ≤ −t] ≤ exp ( −2t2 n(b − a)2 )\nProof. Our proof essentially follows from the same analysis of Feldman et al. (2008) (Theorem 17). We say that a probability model T is good if err(T ) ≤ 4ε, and bad otherwise. We know that T is guaranteed to contain at least one good model. In the following, we will writeH(Gen) to denote Ex∼D [ H(Pc(x)) ]\n. The probability δ that the algorithm fails to output some good model is at most the probability the best model T ∗ has loss(S,T ) ≥ m (H(Gen) + 2ε) or some bad model T ′ has loss(S,T ′) ≤ m (H(Gen) + 3ε). Applying union bound, we get\nδ ≤ |T | Pr[loss(S,T ′) ≤m (H(Gen) + 3ε) | err(T ) ≥ 4ε] + Pr[loss(S,T ∗) ≥m (H(Gen) + 2ε)]\nFor each bad model T ′ with err(T ′) > 4ε, we can write\nPr[loss(S,T ′) ≤m(H(Gen) + 3ε)] = Pr[loss(S,T ′) ≤m(H(Gen) + 4ε)− εm] (because err(T ′) ≥ 0) ≤ Pr[loss(S,T ′) ≤m(H(Gen) + err(T ′))− εm]\n= Pr[loss(S,T ′) ≤ E S∼Genm [ loss(S,T ′)− ε]] ≤ exp (−2mε2\nM2\n)\nwhere the last step follows from Theorem 6. Similarly, for the best model T ∗ with err(T ∗) ≤ ε, we have the following derivation:\nPr[loss(S,T ∗) ≥m (H(Gen) + 2ε)] = Pr[loss(S,T ∗) ≥m (H(Gen) + ε) +mε] ≤ Pr[loss(S,T ∗) ≥m (H(Gen) + err(T ∗) +mε)] = Pr[loss(S,T ∗) ≥ E\nS∼Genm [loss(S,T ∗)] +mε]\n≤ exp (−2mε2\nM2\n)\nCombining these two probabilities recovers the stated bound.\nIn other words, as long as we have an ε-accurate model in the list, we can guarantee with probability at least 1−δ that the output model has error O(ε) using a sample of size no more than poly(k/ε) · log(1/δ)."
    }, {
      "heading" : "C Examples of Distinguishing Events",
      "text" : "In this section, we give two distribution classes that admit distinguishing event class of polynomial size.\nC.1 Spherical Gaussian\nWe consider the class of spherical Gaussian in Rk with fixed covariance and bounded means. In particular, let P = {N (µ,Σ) | µ ∈ [0,1]k} where Σ is some diagonal covariance matrix in Rk×k such that the variance in each coordinate satisfy 0 < σ2j ≤ σ2 for some constant σ > 1. Theorem 7. There exists a parametric class of events E(·) for the distribution class P of k-dimensional Spherical Gaussian such that for any γ > 0 and for any two probability distributions P and Q in the class P such that KL(P ||Q) ≥ γ , the class of events E(γ ) contains an event E that is an ξ-distinguishing event, where max{1/ξ, |E(γ )|} ≤ poly(k,1/γ ). Proof. Recall that the KL divergence of two multivariate Gaussian distributions P and Q with means µ,µ′ and covariance matrices Σp ,Σq can be written as\nKL(P ||Q) = 1 2\n(\ntr(Σ−1q Σp) + (µ ′ −µ)⊺Σq(µ′ −µ)− k + log\n(\ndetΣq\ndetΣp\n))\n.\nFor any two distributions P and Q in our class P , we can simplify the KL divergence as\nKL(P ||Q) ≤ σ 2\n2 ‖µ−µ′‖22.\nThen KL(P ||Q) ≥ γ implies that there exists some coordinate j ∈ [k] such that |µj −µ′j | ≥ √ 2γ/(kσ2). Note that the marginal distributions of Pj and Qj over the j-the coordinate are N (µj ,σ2j ) and N (µ′j ,σ2j ) respectively. Without loss of generality, assume that µ′j < µj . Then for any value t ∈ [µ′j ,µj ], we have\nPj [y ≥ t]−Qj [y ≥ t] ≥ Pj [y ∈ [t,µj ]]. (3) Let ∆ = √\n2γ/(kσ2), and consider the discretized set L(γ ) = {0,∆, . . . ,⌊1/∆⌋∆}. Then we know there exists a value t′ ∈ L such that t′ ∈ L(γ ) such that t′ ∈ [µ′j ,µj ] and µj − t′ ≥ ∆. By Equation (3), we can write\nPj [y ≥ t′]−Qj [y ≥ t′] ≥ 1\n2 erf(∆/(\n√ 2σj )) ≥ 1\n2 erf(∆/(\n√ 2σ))\nwhere erf denotes the Gauss error function with erf(x) = 2√ π\n∫ x 0 e−a 2 da for every x ∈ R. The Taylor\nexpansion of the function is\nerf(x) = 2√ π\n∞ ∑\ni=0\n(−1)ix2i+1 n!(2i +1) = 2√ π\n(\nx − x 3 3 + x5 10 − x 7 42 . . .\n)\nTherefore, for any x ∈ [0,1), there exists a constant C such that erf(x/( √ 2σ))/2 ≥ C x. It follows that Pj [y ≥ t′]−Qj [y ≥ t′] ≥ C∆. This means that the event of (yj ≥ t′) is a (C∆)-distinguishing event for the two distributions P and Q. Therefore, for any γ > 0, we can construct the following class of distinguishing events E(γ ) = {1[yj ≥ t′] | j ∈ [k], t′ ∈ L(γ )}. Note that both 1/(C∆) and |E(γ )| is upper bounded by poly(1/γ,k), which recovers our claim.\nC.2 Product Distributions over Discrete Domains\nConsider the space of b-ary cube Y = {0, . . . ,b − 1}k , and the class of full-support product distributions P over Y : distributions whose k coordinates are mutually independent distributions over {0, . . . ,b − 1}. In particular, we assume that there exists some quantity M ≤ poly(k,b) such that for each P ∈ P and each coordinate j and yj ∈ {0,1, . . . b−1}, we have log(1/Pj(yj )) ≤M . Now let’s show that this class of distributions admits a small class of distinguishing events as well.\nTheorem 8. There exists a parametric class of events E(·) for the production distribution class over the b-ary cube such that for any γ > 0 and for any two probability distributions P and Q in the class P such that KL(P ||Q) ≥ γ , the class of events E(γ ) contains an event E that is an ξ-distinguishing event, where max{1/ξ, |E(γ )|} ≤ poly(k,b,1/γ ).\nProof. In the following, we will write P = P1 × . . .×Pk and Q =Q1 × . . .×Qk . Note that\nKL(P ||Q) = ∑\nj ′∈[k] KL(Pj ′ ||Qj ′ ).\nTherefore KL(P ||Q) ≥ γ implies that there exists some coordinate j such that KL(Pj ||Qj ) ≥ γ/k. This means\n∑\ny′j∈{0,...,b−1} Pj(y\n′ j ) log\n      Pj(y ′ j )\nQj (y ′ j )\n      ≥ γ/k.\nThis means there exists some t ∈ {0, . . . ,b − 1} such that Pj (t) log(Pj (t)/Qj(t)) ≥ γ/(kb). Recall that log ( Pj(t)/Qj (t) )\n≤ M , then we must have Pj(t) ≥ γ/(kbM). Furthermore, since Pj (t) ≤ 1, we must also have log(Pj(t)/Qj (t)) ≥ γ/(kb). It follows that\nPj(t)−Qj(t) ≥ Pj (t) ( 1− Qj (t)\nPj(t)\n)\n≥ γ kbM ( 1− 2−γ/(kb) ) ≥ γ kbM γ 2kb =\nγ2\n2(kb)2M\nwhere the last inequality follows from the fact that 1 − 2−z ≥ z/2 for any z ∈ [0,1]. Therefore, for any γ > 0, the following class of events\nE(γ ) = {1[yj = t] | t ∈ {0,1, . . . ,b − 1}, j ∈ [k]}\nwould contain a ξ-distinguishing event, and max{1/ξ, |E(γ )|} ≤ poly(k,b,1/γ )."
    } ],
    "references" : [ {
      "title" : "Learning from noisy examples",
      "author" : [ "D. Angluin", "P.D. Laird" ],
      "venue" : "Machine Learning 2, 4, 343– 370.",
      "citeRegEx" : "Angluin and Laird,? 1987",
      "shortCiteRegEx" : "Angluin and Laird",
      "year" : 1987
    }, {
      "title" : "Learning mixtures of arbitrary gaussians",
      "author" : [ "S. Arora", "R. Kannan" ],
      "venue" : "Proceedings of the Thirty-third Annual ACM Symposium on Theory of Computing. STOC ’01. ACM, New York, NY, USA, 247–257.",
      "citeRegEx" : "Arora and Kannan,? 2001",
      "shortCiteRegEx" : "Arora and Kannan",
      "year" : 2001
    }, {
      "title" : "Combining labeled and unlabeled data with co-training",
      "author" : [ "A. Blum", "T.M. Mitchell" ],
      "venue" : "Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT 1998, Madison, Wisconsin, USA, July 24-26, 1998. 92–100.",
      "citeRegEx" : "Blum and Mitchell,? 1998",
      "shortCiteRegEx" : "Blum and Mitchell",
      "year" : 1998
    }, {
      "title" : "Learning mixtures of gaussians",
      "author" : [ "S. Dasgupta" ],
      "venue" : "40th Annual Symposium on Foundations of Computer Science, FOCS ’99, 17-18 October, 1999, New York, NY, USA. 634–644.",
      "citeRegEx" : "Dasgupta,? 1999",
      "shortCiteRegEx" : "Dasgupta",
      "year" : 1999
    }, {
      "title" : "PAC learning with constant-partition classification noise and applications to decision tree induction",
      "author" : [ "S.E. Decatur" ],
      "venue" : "Proceedings of the Fourteenth International Conference on Machine Learning. ICML ’97. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 83–91.",
      "citeRegEx" : "Decatur,? 1997",
      "shortCiteRegEx" : "Decatur",
      "year" : 1997
    }, {
      "title" : "Learningmixtures of product distributions over discrete domains",
      "author" : [ "J. Feldman", "R. O’Donnell", "R.A. Servedio" ],
      "venue" : "SIAM J. Comput",
      "citeRegEx" : "Feldman et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2008
    }, {
      "title" : "PAC learning axis-aligned mixtures of Gaussians with no separation assumption",
      "author" : [ "J. Feldman", "R.A. Servedio", "R. O’Donnell" ],
      "venue" : "In Learning Theory, 19th Annual Conference on Learning Theory, COLT",
      "citeRegEx" : "Feldman et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2006
    }, {
      "title" : "Decision theoretic generalizations of the PAC model for neural net and other learning applications",
      "author" : [ "D. Haussler" ],
      "venue" : "Inf. Comput. 100, 1, 78–150.",
      "citeRegEx" : "Haussler,? 1992",
      "shortCiteRegEx" : "Haussler",
      "year" : 1992
    }, {
      "title" : "Learning mixtures of spherical gaussians: moment methods and spectral decompositions",
      "author" : [ "D.J. Hsu", "S.M. Kakade" ],
      "venue" : "Innovations in Theoretical Computer Science, ITCS ’13, Berkeley, CA, USA, January 9-12, 2013. 11–20.",
      "citeRegEx" : "Hsu and Kakade,? 2013",
      "shortCiteRegEx" : "Hsu and Kakade",
      "year" : 2013
    }, {
      "title" : "Learning boolean formulas",
      "author" : [ "M. Kearns", "M. Li", "L. Valiant" ],
      "venue" : "J. ACM 41, 6 (Nov.), 1298– 1328.",
      "citeRegEx" : "Kearns et al\\.,? 1994",
      "shortCiteRegEx" : "Kearns et al\\.",
      "year" : 1994
    }, {
      "title" : "Cryptographic limitations on learning boolean formulae and finite automata",
      "author" : [ "M. Kearns", "L. Valiant" ],
      "venue" : "J. ACM 41, 1 (Jan.), 67–95.",
      "citeRegEx" : "Kearns and Valiant,? 1994",
      "shortCiteRegEx" : "Kearns and Valiant",
      "year" : 1994
    }, {
      "title" : "Efficient noise-tolerant learning from statistical queries",
      "author" : [ "M.J. Kearns" ],
      "venue" : "J. ACM 45, 6, 983– 1006.",
      "citeRegEx" : "Kearns,? 1998",
      "shortCiteRegEx" : "Kearns",
      "year" : 1998
    }, {
      "title" : "On the learnability of discrete distributions",
      "author" : [ "M.J. Kearns", "Y. Mansour", "D. Ron", "R. Rubinfeld", "R.E. Schapire", "L. Sellie" ],
      "venue" : "Proceedings of the Twenty-Sixth Annual ACM Symposium on Theory of Computing, 23-25 May 1994, Montréal, Québec, Canada. 273–282.",
      "citeRegEx" : "Kearns et al\\.,? 1994",
      "shortCiteRegEx" : "Kearns et al\\.",
      "year" : 1994
    }, {
      "title" : "Asymptotic methods in statistical decision theory",
      "author" : [ "L.M. Le Cam" ],
      "venue" : "Springer series in statistics. Springer-Verlag, New York.",
      "citeRegEx" : "Cam,? 1986",
      "shortCiteRegEx" : "Cam",
      "year" : 1986
    }, {
      "title" : "Prediction-preserving reducibility",
      "author" : [ "L. Pitt", "M.K. Warmuth" ],
      "venue" : "J. Comput. Syst. Sci. 41, 3 (Dec.), 430–467.",
      "citeRegEx" : "Pitt and Warmuth,? 1990",
      "shortCiteRegEx" : "Pitt and Warmuth",
      "year" : 1990
    }, {
      "title" : "CN = CPCN",
      "author" : [ "L. Ralaivola", "F. Denis", "C.N. Magnan" ],
      "venue" : "InMachine Learning, Proceedings of the Twenty-Third International Conference (ICML 2006), Pittsburgh, Pennsylvania, USA, June 25-29, 2006. 721–728.",
      "citeRegEx" : "Ralaivola et al\\.,? 2006",
      "shortCiteRegEx" : "Ralaivola et al\\.",
      "year" : 2006
    }, {
      "title" : "The strength of weak learnability",
      "author" : [ "R.E. Schapire" ],
      "venue" : "Mach. Learn. 5, 2 (July), 197–227.",
      "citeRegEx" : "Schapire,? 1990",
      "shortCiteRegEx" : "Schapire",
      "year" : 1990
    }, {
      "title" : "A theory of the learnable",
      "author" : [ "L.G. Valiant" ],
      "venue" : "Proceedings of the 16th Annual ACM Symposium on Theory of Computing, April 30 - May 2, 1984, Washington, DC, USA. 436–445.",
      "citeRegEx" : "Valiant,? 1984",
      "shortCiteRegEx" : "Valiant",
      "year" : 1984
    }, {
      "title" : "A spectral algorithm for learning mixture models",
      "author" : [ "S. Vempala", "G. Wang" ],
      "venue" : "J. Comput. Syst. Sci. 68, 4, 841–860.",
      "citeRegEx" : "Vempala and Wang,? 2004",
      "shortCiteRegEx" : "Vempala and Wang",
      "year" : 2004
    }, {
      "title" : "Assouad, fano, and le cam",
      "author" : [ "B. Yu" ],
      "venue" : "Festschrift for Lucien Le Cam. Springer New York, 423–435. 15",
      "citeRegEx" : "Yu,? 1997",
      "shortCiteRegEx" : "Yu",
      "year" : 1997
    }, {
      "title" : "We say that a probability model T is good if err(T ) ≤ 4ε, and bad otherwise. We know that T is guaranteed to contain at least one good model",
      "author" : [ "Feldman" ],
      "venue" : null,
      "citeRegEx" : "Feldman,? \\Q2008\\E",
      "shortCiteRegEx" : "Feldman",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Within the standard PAC (classification) model, there is a rich theory of reducibility between specific learning problems (Pitt and Warmuth, 1990; Kearns and Valiant, 1994), between classes of learning problems (Schapire, 1990; Kearns, 1998), as well as composition theorems allowing the creation of more complex learning algorithm from simpler ones (Kearns et al.",
      "startOffset" : 122,
      "endOffset" : 172
    }, {
      "referenceID" : 10,
      "context" : "Within the standard PAC (classification) model, there is a rich theory of reducibility between specific learning problems (Pitt and Warmuth, 1990; Kearns and Valiant, 1994), between classes of learning problems (Schapire, 1990; Kearns, 1998), as well as composition theorems allowing the creation of more complex learning algorithm from simpler ones (Kearns et al.",
      "startOffset" : 122,
      "endOffset" : 172
    }, {
      "referenceID" : 16,
      "context" : "Within the standard PAC (classification) model, there is a rich theory of reducibility between specific learning problems (Pitt and Warmuth, 1990; Kearns and Valiant, 1994), between classes of learning problems (Schapire, 1990; Kearns, 1998), as well as composition theorems allowing the creation of more complex learning algorithm from simpler ones (Kearns et al.",
      "startOffset" : 211,
      "endOffset" : 241
    }, {
      "referenceID" : 11,
      "context" : "Within the standard PAC (classification) model, there is a rich theory of reducibility between specific learning problems (Pitt and Warmuth, 1990; Kearns and Valiant, 1994), between classes of learning problems (Schapire, 1990; Kearns, 1998), as well as composition theorems allowing the creation of more complex learning algorithm from simpler ones (Kearns et al.",
      "startOffset" : 211,
      "endOffset" : 241
    }, {
      "referenceID" : 9,
      "context" : "Within the standard PAC (classification) model, there is a rich theory of reducibility between specific learning problems (Pitt and Warmuth, 1990; Kearns and Valiant, 1994), between classes of learning problems (Schapire, 1990; Kearns, 1998), as well as composition theorems allowing the creation of more complex learning algorithm from simpler ones (Kearns et al., 1994).",
      "startOffset" : 350,
      "endOffset" : 371
    }, {
      "referenceID" : 9,
      "context" : "A natural starting point for such an investigation is with the standard PAC supervised learning model, and its distributional analogue (Kearns et al., 1994), since these models are each already populated with a number of algorithms with strong theoretical guarantees.",
      "startOffset" : 135,
      "endOffset" : 156
    }, {
      "referenceID" : 0,
      "context" : "Informally, our results imply that for every concept class C known to be PAC learnable with classification noise (Angluin and Laird, 1987), and almost every class P known to be PAC learnable in the distributional sense of Kearns et al.",
      "startOffset" : 113,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "Informally, our results imply that for every concept class C known to be PAC learnable with classification noise (Angluin and Laird, 1987), and almost every class P known to be PAC learnable in the distributional sense of Kearns et al. (1994), PwD problems given by (C,P ) are learnable in our framework.",
      "startOffset" : 114,
      "endOffset" : 243
    }, {
      "referenceID" : 11,
      "context" : "Since practically every C known to be PAC learnable can also be learned with classification noise (either directly or via the statistical query framework (Kearns, 1998), with parity-based constructions being the only known exceptions), and the distribution classes P known to be PAC learnable have small sets of distinguishing events (such as product distributions), and/or have mixture learning algorithms (such as Gaussians), our results yield efficient PwD algorithms for almost all combinations of PAC classification and distribution learning algorithms known to date.",
      "startOffset" : 154,
      "endOffset" : 168
    }, {
      "referenceID" : 17,
      "context" : "At the highest level, our model falls under the framework of Haussler (1992), which gives a decision-theoretic treatment of PAC-style learning (Valiant, 1984) for very general loss functions; our model can be viewed as a special case in which the loss function is conditional log-loss given the value of a classifier.",
      "startOffset" : 143,
      "endOffset" : 158
    }, {
      "referenceID" : 0,
      "context" : "First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al.",
      "startOffset" : 85,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : "First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al.",
      "startOffset" : 85,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : "First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al.",
      "startOffset" : 85,
      "endOffset" : 139
    }, {
      "referenceID" : 9,
      "context" : "Our work also relies on the PAC model for distribution learning (Kearns et al., 1994), including a long line of works on learning mixtures of distributions (see e.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "Our model is also technically related to the one of co-training (Blum and Mitchell, 1998) in that the input x and the output y give two different views on the data, and they are conditionally independent given the unknown label z = c(x), which is also a crucial assumption for co-training (as well as various other latent variable models for inference and learning).",
      "startOffset" : 64,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "At the highest level, our model falls under the framework of Haussler (1992), which gives a decision-theoretic treatment of PAC-style learning (Valiant, 1984) for very general loss functions; our model can be viewed as a special case in which the loss function is conditional log-loss given the value of a classifier.",
      "startOffset" : 61,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "At the highest level, our model falls under the framework of Haussler (1992), which gives a decision-theoretic treatment of PAC-style learning (Valiant, 1984) for very general loss functions; our model can be viewed as a special case in which the loss function is conditional log-loss given the value of a classifier. Whereas Haussler (1992) is primarily concerned with sample complexity, our focus here is on computational complexity and composition of learning models.",
      "startOffset" : 61,
      "endOffset" : 342
    }, {
      "referenceID" : 0,
      "context" : "First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al. (2006) that established the equivalence of learning under (standard) classification noise (CN) and under class-conditional classification noise (CCCN).",
      "startOffset" : 86,
      "endOffset" : 194
    }, {
      "referenceID" : 0,
      "context" : "First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al. (2006) that established the equivalence of learning under (standard) classification noise (CN) and under class-conditional classification noise (CCCN). Our work also relies on the PAC model for distribution learning (Kearns et al., 1994), including a long line of works on learning mixtures of distributions (see e.g. Dasgupta (1999); Arora and Kannan (2001); Vempala and Wang (2004); Feldman et al.",
      "startOffset" : 86,
      "endOffset" : 521
    }, {
      "referenceID" : 0,
      "context" : "First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al. (2006) that established the equivalence of learning under (standard) classification noise (CN) and under class-conditional classification noise (CCCN). Our work also relies on the PAC model for distribution learning (Kearns et al., 1994), including a long line of works on learning mixtures of distributions (see e.g. Dasgupta (1999); Arora and Kannan (2001); Vempala and Wang (2004); Feldman et al.",
      "startOffset" : 86,
      "endOffset" : 546
    }, {
      "referenceID" : 0,
      "context" : "First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al. (2006) that established the equivalence of learning under (standard) classification noise (CN) and under class-conditional classification noise (CCCN). Our work also relies on the PAC model for distribution learning (Kearns et al., 1994), including a long line of works on learning mixtures of distributions (see e.g. Dasgupta (1999); Arora and Kannan (2001); Vempala and Wang (2004); Feldman et al.",
      "startOffset" : 86,
      "endOffset" : 571
    }, {
      "referenceID" : 0,
      "context" : "First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al. (2006) that established the equivalence of learning under (standard) classification noise (CN) and under class-conditional classification noise (CCCN). Our work also relies on the PAC model for distribution learning (Kearns et al., 1994), including a long line of works on learning mixtures of distributions (see e.g. Dasgupta (1999); Arora and Kannan (2001); Vempala and Wang (2004); Feldman et al. (2008)).",
      "startOffset" : 86,
      "endOffset" : 594
    }, {
      "referenceID" : 0,
      "context" : "CNLearning Wefirst introduce PAC learning under classification noise (CN) (Angluin and Laird, 1987).",
      "startOffset" : 74,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "CCCNLearning In amore general noisemodel calledClass-Conditional Classification Noise (CCCN) proposed by Ralaivola et al. (2006), the example oracle EX η CCCN has class-dependent noise rates— that is, the noise rate η0 for the negative examples (c(x) = 0) and the noise rate η1 for the positive examples (c(x) = 1) may be different, and both below 1/2.",
      "startOffset" : 105,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "CCCNLearning In amore general noisemodel calledClass-Conditional Classification Noise (CCCN) proposed by Ralaivola et al. (2006), the example oracle EX η CCCN has class-dependent noise rates— that is, the noise rate η0 for the negative examples (c(x) = 0) and the noise rate η1 for the positive examples (c(x) = 1) may be different, and both below 1/2. Moreover, Ralaivola et al. (2006) show that any class that is learnable under CN is also learnable under CCCN.",
      "startOffset" : 105,
      "endOffset" : 387
    }, {
      "referenceID" : 9,
      "context" : "Distribution Learning We also make use of results from for PAC learning probability distributions (Kearns et al., 1994).",
      "startOffset" : 98,
      "endOffset" : 119
    }, {
      "referenceID" : 9,
      "context" : "Distribution Learning We also make use of results from for PAC learning probability distributions (Kearns et al., 1994). A distribution class P is efficiently learnable if there exists a polynomialtime algorithm that, given sample access to an unknown target distribution P, outputs an accurate distribution P̂ such that KL(P ||P̂) ≤ ε for some target accuracy ε. For any distribution P ∈ P and any point y ∈ Y , we assume that we can evaluate the probability (density) of y assigned by P (referred to as learning with an evaluator in Kearns et al. (1994); see the appendix for the formal",
      "startOffset" : 99,
      "endOffset" : 556
    }, {
      "referenceID" : 5,
      "context" : "Gaussians), it can be obtained using standard procedures (for instance, by truncating, or mixing with a small amount of the uniform distribution; see Feldman et al. (2006) for an example).",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 15,
      "context" : "Thus, whenever the class C is learnable under CN (and hence learnable under CCCN by Ralaivola et al. (2006)), we can learn the target concept c under the PwD model using a distinguishing event.",
      "startOffset" : 84,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "3In the work of Blum and Mitchell (1998), the authors showed that any CN learnable class is also learnable when the class-conditional noise rates satisfy η0 + η1 < 1.",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "Our result relies on the well-known Le Cam’s method, which is a powerful tool for giving lower bounds in hypothesis testing. We state the following version for our purpose.6 Lemma 5. [Le Cam’s method (see e.g. Le Cam (1986); Yu (1997))] Let Q0 and Q1 be two probability distributions over Y , and let A : Ym → {0,1} be a mapping from m observations in Y to either 0 or 1.",
      "startOffset" : 39,
      "endOffset" : 224
    }, {
      "referenceID" : 13,
      "context" : "Our result relies on the well-known Le Cam’s method, which is a powerful tool for giving lower bounds in hypothesis testing. We state the following version for our purpose.6 Lemma 5. [Le Cam’s method (see e.g. Le Cam (1986); Yu (1997))] Let Q0 and Q1 be two probability distributions over Y , and let A : Ym → {0,1} be a mapping from m observations in Y to either 0 or 1.",
      "startOffset" : 39,
      "endOffset" : 235
    }, {
      "referenceID" : 3,
      "context" : "8 However, we now describe a different, “reverse” reduction that instead assumes learnability of mixtures, and thus is applicable to more general Gaussians via known mixture learning algorithms (Dasgupta, 1999; Arora and Kannan, 2001; Feldman et al., 2006).",
      "startOffset" : 194,
      "endOffset" : 256
    }, {
      "referenceID" : 1,
      "context" : "8 However, we now describe a different, “reverse” reduction that instead assumes learnability of mixtures, and thus is applicable to more general Gaussians via known mixture learning algorithms (Dasgupta, 1999; Arora and Kannan, 2001; Feldman et al., 2006).",
      "startOffset" : 194,
      "endOffset" : 256
    }, {
      "referenceID" : 6,
      "context" : "8 However, we now describe a different, “reverse” reduction that instead assumes learnability of mixtures, and thus is applicable to more general Gaussians via known mixture learning algorithms (Dasgupta, 1999; Arora and Kannan, 2001; Feldman et al., 2006).",
      "startOffset" : 194,
      "endOffset" : 256
    }, {
      "referenceID" : 3,
      "context" : "Dasgupta (1999); Feldman et al.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 3,
      "context" : "Dasgupta (1999); Feldman et al. (2006, 2008); Hsu and Kakade (2013)).",
      "startOffset" : 0,
      "endOffset" : 68
    } ],
    "year" : 2017,
    "abstractText" : "We consider a new learning model in which a joint distribution over vector pairs (x,y) is determined by an unknown function c(x) that maps input vectors x not to individual outputs, but to entire distributions over output vectors y. Our main results take the form of rather general reductions from our model to algorithms for PAC learning the function class and the distribution class separately, and show that virtually every such combination yields an efficient algorithm in our model. Our methods include a randomized reduction to classification noise and an application of Le Cam’s method to obtain robust learning algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}