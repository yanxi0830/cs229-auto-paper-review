{
  "name" : "1206.6404.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Policy Gradients with Variance Related Risk Criteria",
    "authors" : [ "Aviv Tamar", "Dotan Di Castro", "Shie Mannor" ],
    "emails" : [ "avivt@tx.technion.ac.il", "dot@tx.technion.ac.il", "shie@ee.technion.ac.il" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In both Reinforcement Learning (RL; Bertsekas & Tsitsiklis, 1996) and planning in Markov Decision Processes (MDPs; Puterman, 1994), the typical objective is to maximize the cumulative (possibly discounted) expected reward, denoted by J . When the model’s parameters are known, several well-established and efficient optimization algorithms are known. When the model parameters are not known, learning is needed and there are several algorithmic frameworks that solve the learning problem efficiently, at least when the model is finite. In many applications, however, the decision maker is also interested in minimizing some form of risk of the policy. By risk, we mean reward\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ncriteria that take into account not only the expected reward, but also some additional statistics of the total reward such as its variance, its Value at Risk, etc. (Luenberger, 1998). Risk can be measured with respect to two types of uncertainties. The first type, termed parametric uncertainty is related to the imperfect knowledge of the problem parameters. The second type, termed inherent uncertainty is related to the stochastic nature of the system (random reward and transition function). Both types of uncertainties can be important, depending on the application at hand.\nRisk aware decision making is important both in planning and in learning. Two prominent examples are in finance and process control. In financial decision making, a popular performance criterion is the Sharpe Ratio (SR; Sharpe, 1966) – the ratio between the expected profit and its standard deviation. This measure is so popular that it is one of the reported metrics each mutual fund reports annually. When deciding how to allocate a portfolio both types of uncertainties are important: the decision maker does not know the model parameters or the actual realization of the market behavior. In process control as well, both uncertainties are essential and a robust optimization framework (Nilim & El Ghaoui, 2005) is often adopted to overcome imperfect knowledge of the parameters and uncertainty in the transitions. In this paper we focus on inherent uncertainty and use learning to mitigate parametric uncertainty.\nThe topic of risk-aware decision making has been of interest for quite a long time, and several frameworks for incorporating risk into decision making have been suggested. In the context of MDPs and addressing inherent uncertainty, Howard & Matheson (1972) proposed to use an exponential utility function, where the factor of the exponent controls the risk sensitivity. Another approach considers the percentile performance criterion (Filar et al., 1995), in which the average reward has to exceed some value with a given probability. Addressing parameter uncertainty has been done within\nthe Bayesian framework (where a prior is assumed on the unknown parameters, see Poupart et al., 2006) or within the robust MDP framework (where a worstcase approach is taken over the parameters inside an uncertainty set). Much less work has been done on risk sensitive criteria within the RL framework, with a notable exception of Borkar & Meyn (2002) who considered exponential utility functions and of Geibel & Wysotzki (2005) who considered models where some states are “error states,” representing a bad or even catastrophic outcome.\nIn this work we consider an RL setup and focus on risk measures that involve the variance of the cumulative reward, denoted by V . Typical performance criteria that fall under this definition include\n(a) Maximize J s.t. V ≤ c\n(b) Minimize V s.t. J ≥ c (c) Maximize the Sharpe Ratio: J/ √ V (d) Maximize J − c √ V\nThe rationale behind our choice of risk measure is that these performance criteria, such as the SR mentioned above, are being used in practice. Moreover, it seems that human decision makers understand how to use variance well, and that exponential utility functions require determining the exponent coefficient which is non-intuitive.\nVariance-based risk criteria, however, are computationally demanding. It has long been recognized (Sobel, 1982) that optimization problems such as (a) are not amenable to standard dynamic programming techniques. Furthermore, Mannor & Tsitsiklis have shown that even when the MDP’s parameters are known, many of these problems are computationally intractable, and some are not even approximable. This is not surprising given that other risk related criteria such as percentile optimization are also known to be hard except in special cases.\nDespite these somewhat discouraging results, in this work we show that this important problem may be tackled successfully, by considering policy gradient type algorithms that optimize the problem locally. We present a framework for dealing with performance criteria that include the variance of the cumulative reward. Our approach is based on a new fundamental result for the variance of episodic tasks. Previous work by Sobel, 1982 presented similar equations for the infinite horizon discounted case, however, the importance of our result is that the episodic setup allows us to derive policy gradient type algorithms. We\npresent both model-based and model-free algorithms for solving problems (a) and (c), and prove that they converge. Extension of our algorithms to other performance criteria such as (b) and (d) listed above is immediate. The effectiveness of our approach is further demonstrated numerically in a risk sensitive portfolio management problem."
    }, {
      "heading" : "2. Framework and Background",
      "text" : "In this section we present the framework considered in this work and explain the difficulty in mean-variance optimization."
    }, {
      "heading" : "2.1. Definitions and Framework",
      "text" : "We consider an agent interacting with an unknown environment that is modeled by an MDP in discrete time with a finite state set X , {1, . . . , n} and finite action set U , {1, . . . ,m}. Each selected action u ∈ U at a state x ∈ X determines a stochastic transition to the next state y ∈ X with a probability Pu(y|x).\nFor each state x the agent receives a corresponding reward r(x) that is bounded and depends only on the current state1. The agent maintains a parameterized policy function that is in general a probabilistic function, denoted by µθ(u|x), mapping a state x ∈ X into a probability distribution over the controls U . The parameter θ ∈ RKθ is a tunable parameter, and we assume that µθ(u|x) is a differentiable function w.r.t. θ. Note that for different values of θ, different probability distributions over U are associated for each x ∈ X. We denote by x0, u0, r0, x1, u1, r1, . . . a state-actionreward trajectory where the subindex specifies time. For notational easiness, we define xki , u k i , and r k i to be xi . . . , xk, ui . . . , uk, and ri . . . , rk, respectively, and Rki to be the cumulative reward along the trajectory\nRki = ∑k j=i rj .\nUnder each policy induced by µθ(u|x), the environment and the agent induce together a Markovian transition function, denoted by Pθ(y|x), satisfying Pθ(y|x) = ∑ u µθ(u|x)Pu(y|x). The following assumption will be valid throughout the rest of the paper.\nAssumption 2.1. Under all policies, the induced Markov chain Pθ is ergodic, i.e., aperiodic, recurrent, and irreducible.\nUnder assumption 2.1 the Markovian transition function Pθ(y|x) induces a stationary distribution over the state space X, denoted by πθ. We denote by Eθ[·] and\n1Generalizing the results presented here to the case where the reward depends on the state and the action rewards is straightforward.\nVarθ[·] to be the expectation and variance operators w.r.t. the measure Pθ(y|x).\nThere are several performance criteria investigated in the RL literature that differ mainly on their time horizon and the treatment of future rewards (Bertsekas & Tsitsiklis, 1996). One popular criterion is the average reward defined by ηθ = ∑ x πθ(x)r(x). Under this criterion, the agent’s goal is to find the parameter θ that maximizes ηθ. One appealing property of this criterion is the possibility of obtaining estimates of ∇ηθ from simulated trajectories efficiently, which leads to a class of stochastic gradient type algorithms known as policy gradient algorithms. In this work, we also follow the policy gradient approach, but focus on the mean-variance tradeoff. While one can consider the tradeoff between ηθ and Varπ[r(x)], defined as the variance w.r.t the measure πθ, these expressions are not sensitive to the trajectory but only to the induced stationary distribution, and represent the per-round variability.\nConsequently, we focus on the finite horizon case, also known as the episodic case, that is important in many applications. Assume (without lost of generality) that x∗ is some recurrent state for all policies and let τ , min{k > 0|xk = x∗} denote the first passage time to x∗.\nLet the random variable B denote the accumulated reward along the trajectory terminating at the recurrent state x∗\nB , τ−1∑ k=0 r(xk). (1)\nClearly, it is desirable to choose a policy for which B is large in some sense 2. In this work, we are interested in the mean-variance tradeoff in B.\nWe define the value function as\nJ(x) , Eθ [B|x0 = x] , , x = 1, . . . , n, (2)\nand the trajectory variance function as\nV (x) , Varθ [B|x0 = x] , , x = 1, . . . , n.\nNote that the dependence of J(x) and V (x) on θ is suppressed in notation.\nThe questions explored in this work are the following stochastic optimization problems:\n(a) The constrained trajectory-variance problem:\nmax θ J(x∗) s.t. V (x∗) ≤ b, (3)\n2Note that finite horizon MDPs can be formulated as a special case of (1).\nwhere b is some positive value.\n(b) The maximal SR problem:\nmax θ S(x∗) , J(x∗)√ V (x∗) . (4)\nIn order for these problems to be well defined, we make the following assumption:\nAssumption 2.2. Under all policies J(x∗) and V (x∗) are bounded.\nFor the SR problem we also require the following:\nAssumption 2.3. We have V (x∗) > for some > 0.\nIn the next subsection we discuss the challenges involved in solving problems (3) and (4), which motivate our gradient based approach."
    }, {
      "heading" : "2.2. The Challenges of Trajectory-Variance Problems",
      "text" : "As was already recognized by Sobel (1982), optimizing the mean-variance tradeoff in MDPs cannot be solved using traditional dynamic programming methods such as policy iteration. Mannor & Tsitsiklis showed that for the case of a finite horizon T , in general, solving problem (3) is hard and is equivalent to solving the subset-sum problem. Since our case can be seen as a generalization of a finite horizon problem, (3) is a hard problem as well. One reason for the hardness of the problem is that, as suggested by Mannor & Tsitsiklis, the underlying optimization problem is not necessarily convex. In the following, we give an example where the set of all (J(x∗), V (x∗)) pairs spanned by all possible policies is not convex.\nConsider the following symmetric deterministic MDP with 8 states X = {x∗, x1a, x1b, x2a, x2b, x2c, x2d, t}, and two actions U = {u1, u2}. The reward is equal to 1 or −1 when action u1 or u2 are chosen, respectively. The MDP is sketched in Figure 1, left pane. We consider a set of random policies parameterized by θ1 ∈ [0, 1] and θ2 ∈ [0, 1], such that µ(u1|x∗) = θ1 and µ(u1|x1a) = µ(u1|x1b) = θ2.\nNow, we can achieve J(x∗) ∈ {−2, 0, 2} with zero variance if we choose (θ1, θ2) ∈ {(0, 0), (0, 1), (1, 0), (1, 1)}, i.e., only with the deterministic policies. Any −2 < J(x∗) < 2, J(x∗) 6= 0, can be achieved but only with a random policy, i.e., with some variance. Thus, the region is not convex. The achievable (J(x∗), V (x∗)) pairs are depicted in the right pane of Figure 1."
    }, {
      "heading" : "3. Formulae for the Trajectory Variance and its Gradient",
      "text" : "In this section we present formulae for the mean and variance of the cumulated reward between visits to the recurrent state. The key point in our approach is the following observation. By definition (1), a transition to x∗ always terminates the accumulation in B and does not change its value. Therefore, the following Bellman like equation can be written for the value function\nJ(x) = r(x) + ∑ y 6=x∗ Pθ(y|x)J(y) x = 1, . . . , n. (5)\nSimilar equations can be written for the trajectory variance, and in the following lemma we show that these equations are solvable, yielding expressions for J and V .\nProposition 3.1. Let P be a stochastic matrix corresponding to a policy satisfying Assumption 2.1, where its (i, j)-th entry is the transition from state i to state j. Define P ′ to be a matrix equal to P except that the column corresponding to state x∗ is zeroed (i.e., P ′(i, x∗) = 0 for i = 1, . . . , n). Then,\n(a) the matrix I − P ′ is invertible;\n(b) J = (I − P ′)−1r;\n(c) V = (I − P ′)−1ρ,\nwhere ρ ∈ Rn and\nρ(x) = ∑ y P ′(y|x)J(y)2 − (∑ y P ′(y|x)J(y) )2 .\nProof. (a) Consider an equivalent Stochastic Shortest Path (SSP) problem where x∗ is the termination state.\nThe corresponding transition matrix Pssp is defined by Pssp(i, j) = P (i, j) for i 6= x∗, Pssp(x∗, j) = 0 for j 6= x∗, and Pssp(x∗, x∗) = 1. Furthermore, let P ∗ ∈ Rn−1×n−1 denote the matrix P with the x∗’th row and column removed, which is also the transition matrix of the SSP problem without the terminal state. By the irreducibility of P in Assumption 2.1 Pssp is proper, and by proposition 2.2.1 in (Bertsekas, 2006) we have that I − P ∗ is invertible. Finally, observe that by the definition of P ′ we have\ndet(I − P ′) = det(I − P ∗),\nthus, det(I − P ′) 6= 0.\n(b) Choose x ∈ {1, . . . , n}. Then, J(x) = r(x) + ∑ y 6=x∗ P (y|x)J(y),\nwhere we excluded the recurrent state from the sum since after reaching the recurrent state there is no further rewards by definition (2). In vectorial form, J = r + P ′J where using (a) we conclude that J = (I − P ′)−1r.\n(c) Choose x ∈ {1, . . . , n}. Then,\nV (x) =E (τ−1∑ k=0 r(xk) )2 ∣∣∣∣x0 = x − J(x)2\n=r(x)2 + 2r(x) ∑ y 6=x∗ P (y|x)E [ τ−1∑ k=1 r(xk) ∣∣∣∣x1 = y ] +\n∑ y 6=x∗ P (y|x)E (τ−1∑ k=1 r(xk) )2 ∣∣∣∣x1 = y − J(x)2,\n=r(x)2 + 2r(x) ∑ y 6=x∗ P (y|x)J(y) + ∑ y 6=x∗ P (y|x)V (y)\n+ ∑ y 6=x∗ P (y|x)J(y)2 − J(x)2,\nwhere in the second equality we took the first term out of the summation, and in the third equality we used the definition of J and V . Next, we show that r(x)2 + 2r(x) ∑ y 6=x∗ P (y|x)J(y)+ ∑ y 6=x∗ P (y|x)J(y)2−J(x)2 is equal to ρ(x):\nr(x)2 + 2r(x) ∑ y 6=x∗ P (y|x)J(y)− J(x)2 + ∑ y 6=x∗ P (y|x)J(y)2\n= (r(x) + J(x)) (r(x)− J(x)) + 2r(x) ∑ y 6=x∗ P (y|x)J(y) + ∑ y 6=x∗ P (y|x)J(y)2\n= (r(x) + J(x)) −∑ y 6=x∗ P (y|x)J(y) \nPolicy Gradients with Variance Related Risk Criteria + 2r(x) ∑ y 6=x∗ P (y|x)J(y) + ∑ y 6=x∗ P (y|x)J(y)2\n= (r(x)− J(x)) ∑ y 6=x∗ P (y|x)J(y) + ∑ y 6=x∗ P (y|x)J(y)2 = ∑ y 6=x∗ P (y|x)J(y)2 − ∑ y 6=x∗ P (y|x)J(y)\n2 . Proposition 3.1 can be used to derive expressions for the gradients w.r.t. θ of J and V . Let A ◦ B denote the element-wise product between vectors A and B. The gradient expressions are presented in the following lemma.\nLemma 3.2. We have\n∇J = (I − P ′)−1∇P ′J, (6)\nand ∇V = (I − P ′)−1 (∇ρ+∇P ′V ) , (7)\nwhere\n∇ρ = ∇P ′J2+2P ′ (J ◦ ∇J)−2P ′J ◦(∇P ′J + P ′∇J) . (8)\nThe proof is a straightforward differentiation of the expressions in Lemma 3.1, and is described in Section A of the supplementary material 3.\nWe remark that similar equations for the infinite horizon discounted return case were presented by Sobel (1982), in which I−P ′ is replaced with I−βP , where β < 1 is the discount factor. The analysis in (Sobel, 1982) makes use of the fact that I − βP is invertible, therefore an extension of their results to the undiscounted case is not immediate."
    }, {
      "heading" : "4. Gradient Based Algorithms",
      "text" : "In this section we derive gradient based algorithms for solving problems (3) and (4). We present both exact algorithms, which may be practical for small problems, and simulation based algorithms for larger problems. Our algorithms deal with the constraint based on the penalty method, which is described in the following subsection."
    }, {
      "heading" : "4.1. Penalty methods",
      "text" : "One approach for solving constrained optimization problems (COPs) such as (3) is to transform the COP to an equivalent unconstrained problem, which can be solved using standard unconstrained optimization techniques. These methods, generally known as\n3http://tx.technion.ac.il/~avivt/icml12supp.pdf\npenalty methods, add to the objective a penalty term for infeasibility, thereby making infeasible solutions suboptimal. Formally, given a COP\nmax f(x), s.t. c(x) ≤ 0, (9)\nwe define an unconstrained problem\nmax f(x)− λg (c(x)) , (10)\nwhere g(x) is the penalty function, typically taken as g(x) = (max (0, x)) 2 , and λ > 0 is the penalty coefficient. As λ increases, the solution of (10) converges to the solution of (9), suggesting an iterative procedure for solving (9): solve (10) for some λ, then increase λ and solve (10) using the previous solution as an initial starting point.\nIn this work we use the penalty method to solve the COP in (3). An alternative approach, which is deferred to future work, is to use barrier methods, in which a different penalty term is added to the objective that forces the iterates to remain within the feasible set (Boyd & Vandenberghe, 2004)."
    }, {
      "heading" : "4.2. Exact Gradient Algorithm",
      "text" : "When the MDP transitions are known, the expressions for the gradients in Lemma 3.2 can be immediately plugged into a gradient ascent algorithm for the following penalized objective function of problem (3)\nfλ = J(x ∗)− λg (V (x∗)− b) .\nLet αk denote a sequence of positive step sizes. Then, a gradient ascent algorithm for maximizing fλ is\nθk+1 = θk + αk (∇J(x∗)− λg′ (V (x∗)− b)∇V (x∗)) . (11)\nLet us make the following assumption on the smoothness of the objective function and on the set of its local optima. 4\nAssumption 4.1. For all θ ∈ RKθ and λ > 0, the objective function fλ has bounded second derivatives. Furthermore, the set of local optima of fλ is countable.\nThen, under Assumption 4.1, and suitable conditions on the step sizes, the gradient ascent algorithm (11) can be shown to converge to a locally optimal point of fλ.\nFor the SR optimization problem (4), using the quotient derivative rule for calculating the gradient of S,\n4Note that the smoothness of J(x∗) and V (x∗) may be satisfied by choosing a suitable policy function such as the softmax function.\nwe obtain the following algorithm\nθk+1 = θk + αk√ V (x∗)\n( ∇J(x∗)− J(x ∗)\n2V (x∗) ∇V (x∗)\n) ,\n(12)\nwhich can be shown to converge under similar conditions to a locally optimal point of (4). When the state space is large, or when the model is not known, computation of the gradients using equations (6) and (7) is not feasible. In these cases, we can use simulation to obtain unbiased estimates of the gradients, as we describe in the next section, and perform a stochastic gradient ascent."
    }, {
      "heading" : "4.3. Simulation based optimization",
      "text" : "When a simulator of the MDP dynamics is available, it is possible to obtain unbiased estimates of the gradients ∇J and ∇V from a sample trajectory between visits to the recurrent state. The technique is called the likelihood ratio method, and it underlies all policy gradient algorithms (Baxter & Bartlett, 2001; Marbach & Tsitsiklis, 1998). The following lemma gives the necessary gradient estimates for our case.\nLemma 4.2. We have\n∇J(x) = E[Rτ−10 ∇ logP ( xτ−10 ) |x0 = x],\nand ∇V (x)=E[ ( Rτ−10 )2∇ logP (xτ−10 ) |x0 = x]−2J(x)∇J(x), where the expectation is over trajectories.\nThe proof is given in Section B of the supplementary material.\nGiven an observed trajectory xτ−10 , u τ−1 0 , r τ−1 0 , and using Lemma 4.2 we devise the estimator ∇̂J(x∗) , Rτ−10 ∇ logP ( xτ−10 ) which is an unbiased estimator of ∇J(x∗). Furthermore, using the Markov property of the state transition and the fact that the only dependance on θ is in the policy µθ, the term ∇ logP ( xτ−10\n) can be reduced to\n∇ logP ( xτ−10 ) = τ−1∑ k=0 ∇ logµθ (uk|xk) ,\nmaking the computation of ∇̂J(x∗) from an observed trajectory straightforward. Assume for the moment that we know J(x∗) and V (x∗). Then ∇̂V (x∗) , (Rτ−10 ) 2∇ logP ( xτ−10 ) −2J(x∗)∇̂J(x∗) is an unbiased estimate of ∇V (x∗), and plugging ∇̂V and ∇̂J in (11) gives a proper stochastic gradient ascent algorithm.\nUnfortunately, we cannot calculate J(x∗) exactly without knowing the model, and obtaining an unbiased estimate of J(x)∇J(x) from a single trajectory is impossible (for a similar reason that the variance of a random variable cannot be estimated from a single sample of it). We overcome this difficulty by using a two timescale algorithm, where estimates of J and V are calculated on the fast time scale, and θ is updated on a slower time scale.\nThe algorithm updates the parameters every episode, upon visits to the recurrent state x∗. Let τk where k = 0, 1, 2, . . . denote the times of these visits. To ease notation, we also define xk = (xτk−1 , . . . , xτk−1)\nand Rk = ∑τk−1 t=τk−1\nrt to be the trajectories and accumulated rewards observed between visits, and denote zk , ∇ logP (xk) to be the likelihood ratio derivative. The simulation based algorithm for the constrained optimization problem (3) is\nJ̃k+1 = J̃k + αk ( Rk − J̃k ) Ṽk+1 = Ṽk + αk ( (Rk)2 − J̃2k − Ṽk\n) θk+1 = θk + βk ( Rk − λg′ ( Ṽk − b )( (Rk)2 − 2J̃k )) zk,\n(13)\nwhere αk and βk are positive step sizes. Similarly, for optimizing the SR (4), we change the update rule for θ to\nθk+1 = θk + βk√ Ṽk\n( Rk − J̃k(R\nk)2 − 2RkJ̃2k 2Ṽk\n) zk.\n(14)\nIn the next theorem we prove that algorithm (13) converges almost surely to a locally optimal point of the corresponding objective function. The proof for Algorithm (14) is essentially the same and thus omitted. For notational clarity, throughout the remainder of this section, the dependence of J(x∗) and V (x∗) on θ is made explicit using a subscript.\nTheorem 4.3. Consider algorithm (13), and let Assumptions 2.1, 2.2, and 4.1 hold. If the step size sequences satisfy ∑ k αk = ∑ k βk = ∞,∑\nk α 2 k, ∑ k β 2 k < ∞, and limk→∞ βk αk\n= 0, then almost surely\nlim k→∞\n∇ (Jθk(x∗)− λg (Vθk(x∗)− b)) = 0. (15)\nProof. (sketch) The proof relies on representing Equation (13) as a stochastic approximation with two timescales (Borkar, 1997), where J̃k and Ṽk are updated on a fast schedule while θk is updated on a slow schedule. Thus, θk may be seen as quasi-static w.r.t. J̃k and Ṽk ,\nsuggesting that J̃k and Ṽk may be associated with the following ordinary differential equations (ODE)\nJ̇ = Eθ[B|x0 = x∗]− J, V̇ = Eθ[B2|x0 = x∗]− J2 − V.\n(16)\nFor each θ, the ODE (16) can be solved analytically to yield J(t) = J∞+c1e\n−t and V (t) = V∞−2J∞c1te−t+ c21e −2t + c2e\n−t, where c1 and c2 are constants, and {J∞, V∞} is a globally asymptotically stable fixed point which satisfies\nJ∞ = Jθ(x ∗), V∞ = Vθ(x ∗). (17)\nIn turn, due to the timescale difference, J̃k and Ṽk in the iteration for θk may be replaced with their stationary limit points J∞ and V∞, suggesting the following ODE for θ\nθ̇ = ∇ (Jθ(x∗)− λg (Vθ(x∗)− b)) . (18)\nUnder Assumption 4.1, the set of stable fixed point of (18) is just the set of locally optimal points of the objective function fλ. Let Z denote this set, which by Assumption 4.1 is countable. Then, by Theorem 5 in Leslie & Collins, 2002 (which is extension of Theorem 1.1 in Borkar, 1997), θk converges to a point in Z almost surely."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section we apply the simulation based algorithms of Section 4 to a portfolio management problem, where the available investment options include both liquid and non-liquid assets. In the interest of understanding the performance of the different algorithms, we consider a rather simplistic model of the corresponding financial problem. We emphasize that dealing with richer models requires no change in the algorithms.\nWe consider a portfolio that is composed of two types of assets. A liquid asset (e.g., short term T-bills), which has a fixed interest rate rl but may be sold at every time step t = 1, . . . , T , and a non-liquid asset (e.g., low liquidity bonds or options) that has a time dependent interest rate rnl(t), yet may be sold only after a maturity period of N steps. In addition, the non-liquid asset has some risk of not being paid (i.e., a default) with a probability prisk. A common investment strategy in this setup is laddering–splitting the investment in the non-liquid assets to chunks that are reinvested in regular intervals, such that a regular cash flow is maintained. In our model, at each time step the investor may change his portfolio by investing a fixed fraction α of his total available cash in a non-liquid asset. Of course, he can only do that when he has at\nleast α invested in liquid assets, otherwise he has to wait until enough non-liquid assets mature. In addition, we assume that at each t the interest rate rnl(t) takes one of two values - rhighnl or r low nl , and the transitions between these values occur stochastically with switching probability pswitch. The state of the model at each time step is represented by a vector x(t) ∈ RN+2, where x1 ∈ [0, 1] is the fraction of the investment in liquid assets, x2, . . . , xN+1 ∈ [0, 1] is the fraction in nonliquid assets with time to maturity of 1, . . . , N time steps, respectively, and xN+2(t) = rnl(t) − E[rnl(t)]. At time t = 0 we assume that all investments are in liquid assets, and we denote x∗ = x(t = 0). The binary action at each step is determined by a stochastic policy, with probability µθ(x) = + (1 − 2 )/ ( 1 + e−θx\n) of investing in a non-liquid asset. Note that this ‘ - constrained’ softmax policy comes to satisfy Assumption 2.3. Our reward is just the logarithm of the return from the investment (which is additive at each step). The dynamics of the investment chunks are illustrated in Figure 2.\nWe optimized the policy parameters using the simulation based algorithms of Section 4 with three different performance criteria: (a) Average reward: max J(x∗), (b) Variance constrained reward max J(x∗) s.t. V (x∗) ≤ b, and (c) the SR max J(x∗) √ V (x∗). Figure 3 shows the distribution of the accumulated reward. As anticipated, the policy for criterion (a) was risky, and yielded higher gain than the policy for the variance constrained criterion (b). Interestingly, maximizing the SR resulted in a very conservative policy, that almost never invested in the non-liquid asset. The parameters for the experiments are detailed in the supplementary material, Section C."
    }, {
      "heading" : "6. Conclusion",
      "text" : "This work presented a novel algorithmic approach for RL with variance related risk criteria, a subject that while being important for many applications, has been\nnotoriously known to pose significant algorithmic challenges. Since getting to an optimal solution seems hard even when the model is known, we adopted a gradient based approach that achieves local optimality.\nA few issues are in need of further investigation. First, we note a possible extension to other risk measures such as the percentile criterion (Delage & Mannor, 2010). This will require a result reminiscent to Proposition 3.1 that would allow us to drive the optimization. Second, we could consider variance in the optimization process to improve convergence time in the style of control variates. Policy gradient algorithms are known to suffer from high variance when the recurrent state in not visited frequently. One technique for dealing with this difficulty is by using control variates (Greensmith et al., 2004). Imposing a variance constraint as described in this work also acts along this direction, and may in fact improve performance of such algorithms even if variance is not part of the criterion we are optimizing. Third, policy gradients are just one family of algorithms we can consider. It would be interesting to see if a temporal-difference style algorithm can be developed for the risk measures considered here. Lastly, we note that experimentally, maximizing the SR resulted in a very risk averse behavior. This interesting phenomenon deserves more research. It suggests that it might be more prudent to consider other risk measures instead of the SR."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The research leading to these results has received funding from the European Unions Seventh Framework Programme (FP7/2007-2013) under PASCAL2 (PUMP PRIMING) grant agreement no. 216886 and under a Marie Curie Reintegration Fellowship (IRG) grant agreement no. 249254."
    } ],
    "references" : [ {
      "title" : "Dynamic Programming and Optimal Control, Vol II",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Athena Scientific, third edition,",
      "citeRegEx" : "Bertsekas,? \\Q2006\\E",
      "shortCiteRegEx" : "Bertsekas",
      "year" : 2006
    }, {
      "title" : "Stochastic approximation with two time scales",
      "author" : [ "V.S. Borkar" ],
      "venue" : "Systems & Control Letters,",
      "citeRegEx" : "Borkar,? \\Q1997\\E",
      "shortCiteRegEx" : "Borkar",
      "year" : 1997
    }, {
      "title" : "Risk-sensitive optimal control for markov decision processes with monotone cost",
      "author" : [ "V.S. Borkar", "S.P. Meyn" ],
      "venue" : "Math. Oper. Res.,",
      "citeRegEx" : "Borkar and Meyn,? \\Q2002\\E",
      "shortCiteRegEx" : "Borkar and Meyn",
      "year" : 2002
    }, {
      "title" : "Percentile optimization for Markov decision processes with parameter uncertainty",
      "author" : [ "E. Delage", "S. Mannor" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Delage and Mannor,? \\Q2010\\E",
      "shortCiteRegEx" : "Delage and Mannor",
      "year" : 2010
    }, {
      "title" : "Percentile performance criteria for limiting average markov decision processes",
      "author" : [ "J.A. Filar", "D. Krass", "K.W. Ross" ],
      "venue" : "IEEE Trans. Auto. Control,",
      "citeRegEx" : "Filar et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Filar et al\\.",
      "year" : 1995
    }, {
      "title" : "Risk-sensitive reinforcement learning applied to control under",
      "author" : [ "P. Geibel", "F. Wysotzki" ],
      "venue" : "constraints. JAIR,",
      "citeRegEx" : "Geibel and Wysotzki,? \\Q2005\\E",
      "shortCiteRegEx" : "Geibel and Wysotzki",
      "year" : 2005
    }, {
      "title" : "Variance reduction techniques for gradient estimates in reinforcement learning",
      "author" : [ "E. Greensmith", "P.L. Bartlett", "J. Baxter" ],
      "venue" : "JMLR, 5:1471–1530,",
      "citeRegEx" : "Greensmith et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Greensmith et al\\.",
      "year" : 2004
    }, {
      "title" : "Risk-sensitive markov decision processes",
      "author" : [ "R.A. Howard", "J.E. Matheson" ],
      "venue" : "Management Science,",
      "citeRegEx" : "Howard and Matheson,? \\Q1972\\E",
      "shortCiteRegEx" : "Howard and Matheson",
      "year" : 1972
    }, {
      "title" : "Convergent multipletimescales reinforcement learning algorithms in normal form games",
      "author" : [ "D.S. Leslie", "E.J. Collins" ],
      "venue" : "Annals of App. Prob.,",
      "citeRegEx" : "Leslie and Collins,? \\Q2002\\E",
      "shortCiteRegEx" : "Leslie and Collins",
      "year" : 2002
    }, {
      "title" : "Investment Science",
      "author" : [ "D. Luenberger" ],
      "venue" : null,
      "citeRegEx" : "Luenberger,? \\Q1998\\E",
      "shortCiteRegEx" : "Luenberger",
      "year" : 1998
    }, {
      "title" : "Simulation-based optimization of markov reward processes",
      "author" : [ "P. Marbach", "J.N. Tsitsiklis" ],
      "venue" : "IEEE Trans. Auto. Control,",
      "citeRegEx" : "Marbach and Tsitsiklis,? \\Q1998\\E",
      "shortCiteRegEx" : "Marbach and Tsitsiklis",
      "year" : 1998
    }, {
      "title" : "Robust control of Markov decision processes with uncertain transition matrices",
      "author" : [ "A. Nilim", "L. El Ghaoui" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Nilim and Ghaoui,? \\Q2005\\E",
      "shortCiteRegEx" : "Nilim and Ghaoui",
      "year" : 2005
    }, {
      "title" : "An analytic solution to discrete bayesian reinforcement learning",
      "author" : [ "P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Poupart et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Poupart et al\\.",
      "year" : 2006
    }, {
      "title" : "Markov decision processes: discrete stochastic dynamic programming",
      "author" : [ "M.L. Puterman" ],
      "venue" : null,
      "citeRegEx" : "Puterman,? \\Q1994\\E",
      "shortCiteRegEx" : "Puterman",
      "year" : 1994
    }, {
      "title" : "Mutual fund performance",
      "author" : [ "W.F. Sharpe" ],
      "venue" : "The Journal of Business,",
      "citeRegEx" : "Sharpe,? \\Q1966\\E",
      "shortCiteRegEx" : "Sharpe",
      "year" : 1966
    }, {
      "title" : "The variance of discounted markov decision processes",
      "author" : [ "M.J. Sobel" ],
      "venue" : "J. Applied Probability,",
      "citeRegEx" : "Sobel,? \\Q1982\\E",
      "shortCiteRegEx" : "Sobel",
      "year" : 1982
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "In both Reinforcement Learning (RL; Bertsekas & Tsitsiklis, 1996) and planning in Markov Decision Processes (MDPs; Puterman, 1994), the typical objective is to maximize the cumulative (possibly discounted) expected reward, denoted by J .",
      "startOffset" : 108,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "(Luenberger, 1998).",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 14,
      "context" : "In financial decision making, a popular performance criterion is the Sharpe Ratio (SR; Sharpe, 1966) – the ratio between the expected profit and its standard deviation.",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "Another approach considers the percentile performance criterion (Filar et al., 1995), in which the average reward has to exceed some value with a given probability.",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "Much less work has been done on risk sensitive criteria within the RL framework, with a notable exception of Borkar & Meyn (2002) who considered exponential utility functions and of Geibel & Wysotzki (2005) who considered models where some states are “error states,” representing a bad or even catastrophic outcome.",
      "startOffset" : 109,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : "Much less work has been done on risk sensitive criteria within the RL framework, with a notable exception of Borkar & Meyn (2002) who considered exponential utility functions and of Geibel & Wysotzki (2005) who considered models where some states are “error states,” representing a bad or even catastrophic outcome.",
      "startOffset" : 109,
      "endOffset" : 207
    }, {
      "referenceID" : 15,
      "context" : "It has long been recognized (Sobel, 1982) that optimization problems such as (a) are not amenable to standard dynamic programming techniques.",
      "startOffset" : 28,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "As was already recognized by Sobel (1982), optimizing the mean-variance tradeoff in MDPs cannot be solved using traditional dynamic programming methods such as policy iteration.",
      "startOffset" : 29,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "1 in (Bertsekas, 2006) we have that I − P ∗ is invertible.",
      "startOffset" : 5,
      "endOffset" : 22
    }, {
      "referenceID" : 15,
      "context" : "The analysis in (Sobel, 1982) makes use of the fact that I − βP is invertible, therefore an extension of their results to the undiscounted case is not immediate.",
      "startOffset" : 16,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : "We remark that similar equations for the infinite horizon discounted return case were presented by Sobel (1982), in which I−P ′ is replaced with I−βP , where β < 1 is the discount factor.",
      "startOffset" : 99,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "(sketch) The proof relies on representing Equation (13) as a stochastic approximation with two timescales (Borkar, 1997), where J̃k and Ṽk are updated on a fast schedule while θk is updated on a slow schedule.",
      "startOffset" : 106,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "One technique for dealing with this difficulty is by using control variates (Greensmith et al., 2004).",
      "startOffset" : 76,
      "endOffset" : 101
    } ],
    "year" : 2012,
    "abstractText" : "Managing risk in dynamic decision problems is of cardinal importance in many fields such as finance and process control. The most common approach to defining risk is through various variance related criteria such as the Sharpe Ratio or the standard deviation adjusted reward. It is known that optimizing many of the variance related risk criteria is NP-hard. In this paper we devise a framework for local policy gradient style algorithms for reinforcement learning for variance related criteria. Our starting point is a new formula for the variance of the cost-togo in episodic tasks. Using this formula we develop policy gradient algorithms for criteria that involve both the expected cost and the variance of the cost. We prove the convergence of these algorithms to local minima and demonstrate their applicability in a portfolio planning problem.",
    "creator" : "LaTeX with hyperref package"
  }
}