{
  "name" : "1702.04837.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging",
    "authors" : [ "Shusen Wang", "Alex Gittens", "Michael W. Mahoney" ],
    "emails" : [ "shusen@berkeley.edu", "gittea@rpi.edu", "mmahoney@stat.berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the biases and variances of sketched MRR; these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions.\nWe establish theoretically and empirically that model averaging greatly decreases this gap. Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the statistical risks incurred by sketching.\nKeywords: randomized linear algebra, matrix sketching, ridge regression\nar X\niv :1\n70 2.\n04 83\n7v 3"
    }, {
      "heading" : "1. Introduction",
      "text" : "Regression is one of the most fundamental problems in machine learning. The simplest and most thoroughly studied regression model is least squares regression (LSR). Given features X = [xT1 ; . . . ,x T n ] ∈ Rn×d and responses y = [y1, . . . , yn]T ∈ Rn, the LSR problem minw ‖Xw − y‖22 can be solved in O(nd2) time using the QR decomposition or in O(ndt) time using accelerated gradient descent algorithms. Here, t is the number of iterations, which depends on the initialization, the condition number of X, and the stopping criterion.\nThis paper considers the n d problem, where there is much redundancy in X. Matrix sketching, as used within Randomized Linear Algebra (RLA) (Mahoney, 2011, Woodruff, 2014), works by reducing the size of X without losing too much information; this operation can be modeled as taking actual rows or linear combinations of the rows of X with a sketching matrix S to form the sketch STX. Here S ∈ Rn×s satisfies d < s n so that STX generically has the same rank but much fewer rows as X. Sketching has been used to speed up LSR (Drineas et al., 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguyên, 2013) by solving the sketched LSR problem minw ‖STXw − STy‖22 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013).\nThere has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein.\nThe concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w̃ be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value ‖Xw̃ − y ∥∥2 2 is at most times worse than ‖Xw? − y ∥∥2 2 . These works also bounded ‖w̃ − w?‖22 in terms of the difference in the objective function values and the condition number of XTX.\nA more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al. (2016d) considered statistical properties of sketched LSR like the bias and variance. In particular, Pilanci and Wainwright (2015) showed that sketched LSR has much higher variance than the optimal solution.\nBoth of these perspectives are important and of practical interest. The optimization perspective is relevant when the data can be taken as deterministic values. The statistical perspective is relevant in machine learning and statistics applications where the data are random, and the regression coefficients are therefore themselves random variables.\nIn practice, regularized regression, e.g., ridge regression and LASSO, exhibit more attractive bias-variance trade-offs and generalization errors than vanilla LSR. Furthermore,\n1. The condition number of XTSSTX is very close to that of XTX, and thus the number of iterations t is almost unchanged.\nthe matrix generalization of LSR, where multiple responses are to be predicted, is often more useful than LSR. However, the properties of sketched regularized matrix regression are largely unknown. Hence, the question: how, if at all, does our understanding of the optimization and statistical properties of sketched LSR generalize to sketched regularized regression? We answer this question for sketched matrix ridge regression (MRR).\nRecall that X is n× d. Let Y ∈ Rn×m denote the matrix of corresponding responses. We study the MRR problem\nmin W\n{ f(W) , 1n ∥∥XW −Y∥∥2 F + γ‖W‖2F } , (1)\nwhich has optimal solution\nW? = (XTX + nγId) †XTY. (2)\nHere, (·)† denotes the Moore-Penrose inversion operation. LSR is a special case of MRR, with m = 1 and γ = 0. The optimal solution W? can be obtained in O(nd2 + nmd) time using a QR decomposition of X. Sketching can be applied to MRR in two ways:\nWc = (XTSSTX + nγId) †(XTSSTY), (3) Wh = (XTSSTX + nγId) †XTY. (4)\nFollowing the convention of Pilanci and Wainwright (2015), Wang et al. (2016a), we call Wc classical sketch and Wh Hessian sketch. Table 1 lists the time costs of the three solutions to MRR."
    }, {
      "heading" : "1.1 Main Results and Contributions",
      "text" : "We summarize all of our upper bounds in Table 2. Our optimization analysis bounds the objective function values, while our statistical analysis guarantees the bias and variance.\nWe first study classical and Hessian sketches from the optimization perspective. Theorems 1 and 2 show that\n• Classical sketch achieves relative error in the objective value. With sketch size s = Õ(d/ ), the objective satisfies f(Wc) ≤ (1 + )f(W?).\n• Hessian sketch does not achieve relative error in the objective value. In particular, if 1 n‖Y‖ 2 F is much larger than f(W ?), then f(Wh) can be far larger than f(W?).\nWe then study classical sketch and Hessian sketch from the statistical perspective, by modeling Y = XW0 +Ξ as the sum of a true linear model and random noise, decomposing the risk R(W) = E‖XW−XW0‖2F into bias and variance terms, and bounding these terms. The risk R(W) determines how well the trained model W generalizes to the test samples; see the discussions in Appendix A. We draw the following conclusions (see Theorems 4, 5, 7 for the details):\n• The bias of classical sketch can be nearly as small as that of the optimal solution. The variance is Θ ( n s ) times that of the optimal solution; this bound is optimal. Therefore\nover-regularization2 should be used to supress the variance. (As γ increases, the bias increases, and the variance decreases.)\n• Since Y is not sketched with Hessian sketch, the variance of Hessian sketch can be close to the optimal solution. However, Hessian sketch has high bias, especially when nγ is small compared to ‖X‖22. This indicates that over-regularization is necessary for Hessian sketch to have low bias.\nOur empirical evaluations bear out these theoretical results. In particular, in Section 4, we show in Figure 3 that even when the regularization parameter γ is fine-tuned, the risks of classical sketch and Hessian sketch are worse than that of the optimal solution by an order of magnitude. This is an empirical demonstration of the fact that the near-optimal properties\n2. By over-regularization, we mean choosing a larger value of the regularization parameter γ than what we would optimally choose for the unsketched problem.”\nof sketching from the optimization perspective are much less relevant in a statistical setting than its sub-optimal statistical properties.\nWe propose to use model averaging, which averages the solutions of g sketched MRR problems, to attain lower optimization and statistical errors. Without ambiguity, we denote classical and Hessian sketches with model averaging by Wc and Wh, respectively. Theorems 8, 9, 11, 13 establish the following results:\n• Classical Sketch. Model averaging improves the objective function value and the variance and does not increase the bias. Specifically, with the same sketch size s, model averaging makes f(W\nc)−f(W?) f(W?) and var(Wc) var(W?) respectively decrease to almost 1 g of\nthose of classical sketch without model averaging, provided that s d. See Table 2 for the details.\n• Hessian Sketch. Model averaging decreases the objective function value and the bias and does not increase the variance.\nNote that classical sketch with uniform sampling and model averaging is essentially bagging (synonym bootstrap aggregating) (Breiman, 1996) (or a variant called pasting (Breiman, 1999)) for ridge regression. Our work lends strong guarantees to bagging for ridge regression. Different from bagging, our approach is not limited to uniform sampling.\nClassical sketch with model averaging has three immediate applications. In the singlemachine setting,\n• Classical sketch with model averaging offers a way to improve the statistical performance in the presence of heavy noise. Assume the sketch size is s = Õ( √ nd). As\ng grows larger than ns , the variance of the averaged solution can be even lower than the optimal solution. See Corollary 12 for further discussions. Using sketching methods other than uniform sampling, the performance is independent of matrix coherence, which is an improvement over the traditional bagging (Breiman, 1996).\nIn the distributed setting, the feature-response pairs (x1,y1), · · · , (xn,yn) ∈ Rd × Rm are divided among g machines. Assuming that the data have been shuffled randomly, each machine contains a sketch constructed by uniformly sampled rows from the dataset without replacement. We illustrate this procedure in Figure 1. In this setting, the model averaging procedure will communicate the g local models only once to return the final estimate; this process has very low communication and latency costs, and it suggests two further applications of classical sketch with model averaging:\n• Model Averaging for Machine Learning. If a low-precision solution is acceptable, the averaged solution can be used in lieu of distributed numerical optimization algorithms requiring multiple rounds of communication. If ng is big enough compared to d and the row coherence of X is small, then “one-shot” model averaging has bias and variance comparable to the optimal solution.\n• Model Averaging for Optimization. If a high-precision solution to MRR is required, then an iterative numerical optimization algorithm must be used. The cost of such\nnumerical optimization algorithms heavily depends on the quality of the initialization.3 A good initialization saves lots of iterations. The averaged model is provably close to the optimal solution, so model averaging provides a high-quality initialization for more expensive algorithms."
    }, {
      "heading" : "1.2 Prior Work",
      "text" : "The body of work on sketched LSR mentioned earlier (Drineas et al., 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguyên, 2013) shares many similarities with our results. However, the theories of sketched LSR developed from the optimization perspective do not obviously extend to MRR, and the statistical analysis of LSR and MRR differ: among other differences, LSR is unbiased while MRR has a nontrivial bias and therefore has a bias-variance tradeoff that must be considered.\nLu et al. (2013) has considered a different application of sketching to ridge regression: they assume d n, reduce the number of features in X using sketching, and conduct statistical analysis. Our setting differs in that we consider n d, reduce the number of samples by sketching, and allow for multiple responses.\nThe model averaging analyzed in this paper is similar in spirit to the AvgM algorithm of (Zhang et al., 2013). When classical sketch is used with uniform row sampling without replacement, our model averaging procedure is a special case of AvgM. However, our results do not follow from those of (Zhang et al., 2013). First, we make no assumption on the data, whereas they assumed x1, · · · ,xn are i.i.d. from an unknown distribution. Second, the objectives are different: we study the distances ‖XWc −XW?‖2F and E‖XWc −XW0‖2F , where Wc is the averaged classical sketches, W0 is the unknown ground truth, and W\n? is the optimal solution based on the observed data; they essentially studied E‖Wc −W?‖2F ; our expectation is taken w.r.t. the random noise in the responses, while their expectation is w.r.t. the distribution of x1, · · · ,xn. Third, our results apply to many other sketching\n3. For example, the conjugate gradient method satisfies ‖W(t)−W?‖2F ‖W(0)−W?‖2\nF\n≤ θt1; the stochastic block coordinate\ndescent (Tu et al., 2016) satisfies Ef(W (t))−f(W?)\nf(W(0))−f(W?) ≤ θ t 2. Here W (t) is the output of the t-th iteration;\nθ1, θ2 ∈ (0, 1) depend on the condition number of XTX + nγId and some other factors.\nensembles than uniform sampling without replacement. Our results clearly indicate that the performance critically depends on the row coherence of X; this dependence is not captured in (Zhang et al., 2013). For similar reasons, our work is different from the divide-and-conquer kernel ridge regression algorithm of (Zhang et al., 2015).\nIterative Hessian sketch has been studied by Pilanci and Wainwright (2015), Wang et al. (2016a). By way of comparison, all the algorithms in this paper are “one-shot” rather than iterative. Upon completion of this paper, we noticed the contemporary works (Avron et al., 2016, Thanei et al., 2017). Avron et al. (2016) studied classical sketch from the optimization perspective, and Thanei et al. (2017) studied LSR with model averaging."
    }, {
      "heading" : "1.3 Paper Organization",
      "text" : "Section 2 defines our notation and introduces the sketching schemes we consider. Section 3 presents our theoretical results. Sections 4 and 5 conduct experiments to verify our theories and demonstrates the usefulness of model averaging. Section 6 shows the sketch of proof. The proofs of the theorems are in the appendix."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "Throughout, we take In to be the n × n identity matrix and 0 to be a vector or matrix of all zeroes of the appropriate size. Given a matrix A = [aij ], the i-th row is denoted by ai:, and the j-th column is by a:j . The Frobenius and spectral norms of A are written as, respectively, ‖A‖F and ‖A‖2. The set {1, 2, · · · , n} is written [n]. Let O, Ω, and Θ be the standard asymptotic notation. Let Õ conceal logarithm factors.\nThroughout, we fix X ∈ Rn×d as our matrix of features. We set ρ = rank(X) and write the SVD of X as X = UΣVT , where U, Σ, V are respectively n × ρ, ρ × ρ, and d × ρ matrices. We let σ1 ≥ · · · ≥ σρ > 0 be the singular values of X. The Moore-Penrose inverse of X is defined by X† = VΣ−1UT . The row leverage scores of X are li = ‖u:i‖22 for i ∈ [n]. The row coherence of X is µ(X) = nρ maxi ‖u:i‖ 2 2. Throughout, we let µ be shorthand for µ(X). The notation defined in Table 3 are used throughout this paper.\nMatrix sketching turns big matrices into smaller ones without losing too much information useful in tasks like linear regression. We denote the process of sketching a matrix X ∈ Rn×d by X′ = STX. Here, S ∈ Rn×s is called a sketching matrix and X′ ∈ Rs×d is called a sketch of X. In practice, except for Gaussian projection (where the entries of S are i.i.d. sampled from N (0, 1/s)), the sketching matrix S is not formed explicitly.\nMatrix sketching can be accomplished by random selection or random projection. Random sampling corresponds to sampling rows of X i.i.d. with replacement according to given row sampling probabilities p1, · · · , pm ∈ (0, 1). The corresponding (random) sketching matrix S ∈ Rn×s has exactly one non-zero entry, whose position indicates the index of the selected row; in practice, this S is not explicitly formed. Uniform sampling fixes p1 = · · · = pn = 1n . Leverage score sampling sets pi proportional to the (exact or approximate (Drineas et al., 2012)) row leverage scores li of X. In practice shrinked leverage score sampling can be a better choice than leverage score sampling (Ma et al.,\n2015). The sampling probabilities of shrinked leverage score sampling are defined by pi = 1 2 ( li∑n j=1 lj + 1n ) .4\nThe exact leverage scores are unnecessary in practice; constant-factor approximation to the leverage scores is sufficient. Leverage scores can be efficiently approximated by the algorithms of (Drineas et al., 2012). Let l1, · · · , ln be the true leverage scores. We denote the approximate leverages by l̃1, · · · , l̃n such that\nl̃q ∈ [lq, τ lq] for all q ∈ [n], (5)\nwhere τ ≥ 1 indicates the quality of approximation. We can use pq = l̃q/ ∑\nj l̃j as the sampling probability. For the shrinked leverage score sampling, we define the sampling probabilities\npi = 1\n2 ( l̃i∑n j=1 l̃j + 1 n ) for i = 1, . . . , n. (6)\nUsing the approximate leverage scores to replace the exact ones, we only need to make the sketch size τ times larger. As long as τ is a small constant, the orders of the sketch sizes of the exact and approximate leverage score sampling are the same. Thus we do not distinguish the exact and approximate leverage scores in this paper.\nGaussian projection is also well-known as the prototypical Johnson-Lindenstrauss transform (Johnson and Lindenstrauss, 1984). Let G ∈ Rm×s be a standard Gaussian matrix, i.e., each entry is sampled independently from N (0, 1). The matrix S = 1√\ns G is a\nGaussian projection matrix. It takes O(nds) time to apply S ∈ Rn×s to any n × d dense matrix, which makes Gaussian projection inefficient relative to other forms of sketching.\nSubsampled randomized Hadamard transform (SRHT) (Drineas et al., 2011, Lu et al., 2013, Tropp, 2011) is a more efficient alternative to Gaussian projection. Let\n4. In fact, pi can be any convex combination of li∑n j=1 lj and 1 n (Ma et al., 2015). We use the weight 1 2 for\nconvenience; our conclusions extend in a straightforward manner to other weightings.\nHn ∈ Rn×n be the Walsh-Hadamard matrix with +1 and −1 entries, D ∈ Rn×n be a diagonal matrix with diagonal entries sampled uniformly from {+1,−1}, and P ∈ Rn×s be the uniform row sampling matrix defined above. The matrix S = 1√\nn DHnP ∈ Rn×s is an\nSRHT matrix, and can be applied to any n × d matrix in O(nd log s) time. In practice, the subsampled randomized Fourier transform (SRFT) (Woolfe et al., 2008) is often used in lieu of the SRHT, because the SRFT exists for all values of n, whereas Hn exists only for some values of n. Their performance and theoretical analyses are very similar.\nCountSketch can be applied to any X ∈ Rn×d in O(nd) time (Charikar et al., 2004, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguyên, 2013, Pham and Pagh, 2013, Weinberger et al., 2009). Though more efficient to apply, CountSketch requires a bigger sketch size than Gaussian projections, SRHT, and leverage score sampling to attain the same theoretical guarantees. The readers can refer to (Woodruff, 2014) for a detailed description of CountSketch. CountSketch may not be applicable to model averaging as a theoretically sound approach like the other sketching methods. See Remark 20 for the reasons."
    }, {
      "heading" : "3. Main Results",
      "text" : "Sections 3.1 and 3.2 analyze sketched MRR from, respectively, optimization and statistical perspectives. Sections 3.3 and 3.4 capture the impacts of model averaging on, respectively, the optimization and statistical properties of sketched MRR.\nWe described six sketching methods in Section 2. For simplicity, in this section, we refer to leverage score sampling, shrinked leverage score sampling, Gaussian projection, and SRHT as the four sketching methods; and we will mention explicitly uniform sampling and CountSketch. Throughout, let µ be the row coherence of X and β = ‖X‖22 ‖X‖22+nγ ≤ 1."
    }, {
      "heading" : "3.1 Sketched MRR: Optimization Perspective",
      "text" : "Theorem 1 shows that f(Wc), the objective value of classical sketch, is very close to the optimal objective value f(W?). The approximation quality improves as γ increases.\nTheorem 1 (Classical Sketch) Let β = ‖X‖22 ‖X‖22+nγ ≤ 1. For the four sketching methods with s = Õ (βd ) , uniform sampling with s = O ( µβd log d ) , and CountSketch with s = O (βd2 ) , the inequality\nf(Wc)− f(W?) ≤ f(W?)\nholds with probability at least 0.9.\nThe corresponding guarantee for the performance of Hessian sketch is given in Theorem 2. It is weaker than the guarantee for classical sketch, especially when 1n‖Y‖ 2 F is far larger than f(W?). If Y is nearly noiseless—Y is well-explained by a linear combination of the columns of X—and γ is small, then f(W?) is close to zero, and consequently f(W?) can be far smaller than 1n‖Y‖ 2 F . Therefore, in this case which is ideal for MRR, f(W\nh) is not close to f(W?) and our theory suggests Hessian sketch does not perform as well as classical sketch. This is verified by our experiments (see Figure 2), which show that unless\nγ is big or a large portion of Y is outside the column space of X, the ratio f(W h)\nf(W?) can be large.\nTheorem 2 (Hessian Sketch) Let β = ‖X‖22 ‖X‖22+nγ ≤ 1. For the four sketching methods with s = Õ (β2d ) , uniform sampling with s = O (µβ2d log d ) , and CountSketch with s = O(β 2d2 ), the inequality\nf(Wh)− f(W?) ≤ ( ‖Y‖2F n − f(W ?) ) .\nholds with probability at least 0.9.\nThese two results imply that f(Wc) and f(Wh) can be close to f(W?). When this is the case, curvature of the objective function ensures that the sketched solutions Wc and Wh are close to the optimal solution W?. Lemma 3 studies the Mahalanobis distance ‖M(W −W?)‖2F . Here M is any non-singular matrix; in particular, it can be the identity matrix or (XTX)1/2. Lemma 3 is a consequence of Lemma 31.\nLemma 3 Let f be the objective function of MRR defined in (1), W ∈ Rd×m be arbitrary, and W? be the optimal solution defined in (2). For any non-singular matrix M, the Mahalanobis distance satisfies\n1 n ∥∥M(W −W?)∥∥2 F ≤ f(W)− f(W ?) σ2min [ (XTSSTX + nγId)1/2M−1\n] . By choosing M = (XTX)1/2, we can bound 1n‖XW−XW\n?‖2F in terms of the difference in the objective values:\n1 n ∥∥XW −XW?∥∥2 F ≤ β [ f(W)− f(W?) ] ,\nwhere β = ‖X‖22 ‖X‖22+nγ\n≤ 1. With Lemma 3, we can directly apply Theorems 1 or 2 to bound 1 n‖XW c −XW?‖2F or 1 n‖XW h −XW?‖2F ."
    }, {
      "heading" : "3.2 Sketched MRR: Statistical Perspective",
      "text" : "We consider the following fixed design model. Let X ∈ Rn×d be the observed feature matrix, W0 ∈ Rd×m be the true and unknown model, Ξ ∈ Rn×m contain unknown random noise, and\nY = XW0 + Ξ (7)\nbe the observed responses. We make the following standard weak assumptions on the noise:\nE[Ξ] = 0 and E[ΞΞT ] = ξ2In.\nWe observe X and Y and seek to estimate W0. We can evaluate the quality of the estimate by the risk:\nR(W) = 1nE ∥∥XW −XW0∥∥2F , (8)\nwhere the expectation is taken w.r.t. the noise Ξ. In Appendix A we explain that R(W) determines how well W generalize to test data. We study the risk functions R(W?), R(Wc), and R(Wh) in the following.\nTheorem 4 (Bias-Variance Decomposition) We consider the data model described in this subsection. Let W be W?, Wc, or Wh, as defined in (2), (3), (4), respectively; then the risk function can be decomposed as\nR(W) = bias2(W) + var(W).\nRecall the SVD of X defined in Section 2: X = UΣVT . The bias and variance terms can be written as\nbias ( W? ) = γ √ n ∥∥∥(Σ2 + nγIρ)−1ΣVTW0∥∥∥\nF ,\nvar ( W? ) = ξ 2\nn ∥∥∥(Iρ + nγΣ−2)−1∥∥∥2 F ,\nbias ( Wc ) = γ √ n ∥∥∥(UTSSTU + nγΣ−2)†Σ−1VTW0∥∥∥\nF ,\nvar ( Wc ) = ξ 2\nn ∥∥∥(UTSSTU + nγΣ−2)†UTSST∥∥∥2 F ,\nbias ( Wh ) = γ √ n ∥∥∥(Σ−2 + UTSSTU−Iρnγ )(UTSSTU + nγΣ−2)†ΣVTW0∥∥∥\nF ,\nvar ( Wh ) = ξ 2\nn ∥∥∥(UTSSTU + nγΣ−2)†∥∥∥2 F .\nThroughout this paper, we compare the bias and variance of classical sketch and Hessian sketch to those of the optimal solution W?. We first study the bias, variance, and risk of W?, which will help us understand the subsequent comparisons. Here we can assume that Σ2 = VTXTXV is linear with n, which is reasonable because XTX = ∑n i=1 xix T i and V is orthogonal matrix.\n• Bias. The bias of W? is independent of n and is increasing with γ. The bias is a price paid for supressing the variance; for least squares regression, γ is zero, and the bias equals to zero.\n• Variance. The variance of W? is inversely proportional to n. As n grows, the variance decreases to zero. Thus it is highly interesting to compare the variance of approximate solutions to var(W?).\n• Risk. Note that W? is not the minimizer of R(·); W0 is the minimizer because R(W0) = 0. Nevertheless, because W0 is unknown, W\n? with fine-tuned γ is a standard choice in practice. It is thus highly interesting to find solutions with risks comparable to or even better than R(W?).\nTheorem 5 provides upper and lower bounds on the bias and variance of classical sketch. In particular, we see that that bias(Wc) is within a factor of (1± ) of bias(W?). However, var(Wc) can be Θ(ns ) times worse than var(W ?).\nTheorem 5 (Classical Sketch) For Gaussian projection and SRHT sketching with s = Õ( d 2 ), uniform sampling with s = O(µd log d 2 ), or CountSketch with s = O(d2 2\n), the inequalities\n1− ≤ bias(W c)\nbias(W?) ≤ 1 + ,\n(1− )n s ≤ var(W\nc)\nvar(W?) ≤ (1 + )n s\nhold with probability at least 0.9. For shrinked leverage score sampling with s = O(d log d\n2 ), theses inequalities, except for\nthe lower bound on the variance, hold with probability at least 0.9.\nRemark 6 To establish upper (lower) bound on the variance, we need the upper (lower) bound on ‖S‖22. For leverage score sampling, there is neither nontrivial upper nor lower bound on ‖S‖22, which is why the variance of leverage score sampling cannot be bounded. For shrinked leverage score sampling, we only have the upper bound ‖S‖22 ≤ 2ns ; but ‖S‖ 2 2 does not have a nontrivial lower bound, which is why shrinked leverage score sampling lacks lower bound on variance. In Remark 18, we explain why for (shrinked) leverage score sampling, ‖S‖22 does not have upper and (or) lower bound.\nTheorem 7 establishes similar upper and lower bounds on the bias and variance of Hessian sketch. The situation is the reverse of that with classical sketch: the variance of Wh is close to that of W? if s is large enough, but as the regularization parameter γ goes to zero, bias(Wh) becomes much larger than bias(W?).\nTheorem 7 (Hessian Sketch) For the four sketching methods with s = Õ( d 2 ), uniform sampling with s = O(µd log d 2 ), and CountSketch with s = O(d2 2 ), the inequalities\nbias(Wh) bias(W?) ≤ (1 + )\n( 1 +\n‖X‖22 nγ\n) ,\n1− ≤ var(W h)\nvar(W?) ≤ 1 +\nhold with probability at least 0.9. Further assume that σ2ρ ≥ nγ . Then\nbias(Wh)\nbias(W?) ≥ 1 1 + ( σ2ρ nγ − 1 )\nholds with probability at least 0.9.\nThe lower bound on the bias shows that Hessian sketch can suffer from a much higher bias than the optimal solution. The gap between bias(Wh) and bias(W?) can be lessened by increasing the regularization parameter γ, but such over-regularization increases the baseline bias(W?) itself. It is also worth mentioning that unlike bias(W?) and bias(Wc), bias(Wh) is not monotonically increasing with γ, as is empirically verified in Figure 3.\nIn sum, our theories show that the classical and Hessian sketches are not statistically comparable to the optimal solutions: classical sketch has too high a variance, and Hessian sketch has too high a bias for reasonable amounts of regularization. In practice, the regularization parameter γ should be tuned to optimize the prediction accuracy. Our experiments in Figure 3 show that even with fine-tuned γ, the risks of classical and Hessian sketches can be higher than the risk of the optimal solution by an order of magnitude. Formally speaking, minγ R(W\nc) minγ R(W?) and minγ R(Wh) minγ R(W?) hold in practice.\nOur empirical study in Figure 3 suggests classical and Hessian sketches both require overregularization, i.e., setting γ larger than what is best for the optimal solution W?. Formally\nspeaking, argminγ R(W c) > argminγ R(W ?) and argminγ R(W h) > argminγ R(W ?). Although this is the case for both types of sketches, the underlying explanations are different. Classical sketch has a high variance, so a large γ is required to supress the variance (its variance is non-increasing with γ). Hessian sketch has very high bias when γ is small, so a reasonably large γ is necessary to lower its bias."
    }, {
      "heading" : "3.3 Model Averaging: Optimization Perspective",
      "text" : "We consider model averaging as an approach to increasing the accuracy of sketched MRR solutions. The model averaging procedure is straightforward: one independently draws g sketching matrices S1, · · · ,Sg ∈ Rn×s, uses these to form g sketched MRR solutions, denoted by {Wci} g i=1 or {Whi } g i=1, and averages these solutions to obtain the final estimate\nWc = 1g ∑g i=1 W c i or W h = 1g ∑g i=1 W h i . Practical applications of model averaging are enumerated in Section 1.1.\nTheorems 8 and 9 present guarantees on the optimization accuracy of using model averaging to combine the classical/Hessian sketch solutions. We can contrast these with the guarantees provided for sketched MRR in Theorems 1 and 2. For classical sketch with model averaging, we see that when ≤ 1g , the bound on f(W\nh) − f(W?) is proportional to /g. From Lemma 3 we can see that the distances from Wc to W? also decreases accordingly.\nTheorem 8 (Classical Sketch with Model Averaging) Let β = ‖X‖22 ‖X‖22+nγ\n≤ 1. For the four methods, let s = Õ (βd ) ; for uniform sampling, let s = O (µβd log d ) . Then the inequality\nf(Wc)− f(W?) ≤ ( g + β 2 2 ) f(W?)\nholds with probability at least 0.8.\nFor Hessian sketch with model averaging, if β2 ≤ 1 g2 , then the bound on f(Wh)−f(W?)\nis proportional to g2 .\nTheorem 9 (Hessian Sketch with Model Averaging) Let β = ‖X‖22 ‖X‖22+nγ ≤ 1. For the four methods let s = Õ (β2d ) , and for uniform sampling let s = O (µβ2d log d ) , then the inequality\nf(Wh)− f(W?) ≤ ( g2 + 2 β2 )(‖Y‖2F n − f(W?) ) .\nholds with probability at least 0.8."
    }, {
      "heading" : "3.4 Model Averaging: Statistical Perspective",
      "text" : "Model averaging also has the salutatory property of reducing the risks of the classical and Hessian sketches. Our first result conducts a bias-variance decomposition for the averaged solution of sketched MRR.\nTheorem 10 (Bias-Variance Decomposition) We consider the fixed design model (7). The risk function defined in (8) can be decomposed as\nR(W) = bias2(W) + var(W).\nThe bias and variance terms are\nbias ( Wc ) = γ √ n ∥∥∥∥1g g∑ i=1 ( UTSiS T i U + nγΣ −2)†Σ−1VTW0∥∥∥∥ F ,\nvar ( Wc ) = ξ2\nn ∥∥∥∥1g g∑ i=1 ( UTSiS T i U + nγΣ −2)†UTSiSTi ∥∥∥∥2 F ,\nbias ( Wh ) = γ √ n ∥∥∥∥1g g∑ i=1 ( Σ−2 + UTSiS T i U−Iρ nγ )( UTSiS T i U + nγΣ −2)†ΣVTW0∥∥∥∥ F ,\nvar ( Wh ) = ξ2\nn ∥∥∥∥1g g∑ i=1 ( UTSiS T i U + nγΣ −2)†∥∥∥∥2 F .\nTheorems 11 and 13 provide upper bounds on the bias and variance of model-averaged sketched MRR for, respectively, classical sketch and Hessian sketch. We can contrast them with Theorems 5 and 7 to see the statistical benefits of model averaging.\nTheorem 11 (Classical Sketch with Model Averaging) For shrinked leverage score sampling, Gaussian projection, and SRHT with s = Õ ( d 2 ) , or uniform sampling with s =\nO (µd log d\n2\n) , the inequalities\nbias(Wc) bias(W?) ≤ 1 + ,\nvar(Wc)\nvar(W?) ≤ n s (√ 1+ /g g + )2 hold with probability at least 0.8.\nBagging (Breiman, 1996) works in the following way: first, randomly generates g datasets by uniform sampling with or without replacement; second, solves the matrix regression/classification problem independently on each generated dataset; last, aggregates the solutions in some way, e.g., model averaging. We show in Corollary 12 to show bagging indeed achieves better variance than the optimal solution W?. Note that our approaches are more general than the traditional bagging: the g datasets can be formed by any sketching and not limited to uniform sampling.\nCorollary 12 (Bagging) Let g = O ( n s ) ; for shrinked leverage score sampling, Gaussian\nprojection, and SRHT with s = Õ (√ nd ) , or uniform sampling with s = O (√ µnd log d ) , it holds with probability 0.8 that var(Wc) ≤ var(W?).\nFurthermore, for shrinked leverage score sampling, Gaussian projection, and SRHT with s = Õ ( dg ) ≤ n, or uniform sampling with s = O ( µgd log d ) , it holds with probability 0.8 that var(Wc) = O ( 1 g var(W ?) ) .\nTheorem 13 shows that model averaging decreases the bias of Hessian sketch without increasing the variance. For Hessian sketch without model averaging, recall that bias(Wh) is larger than bias(W?) by a factor of O(‖X‖22/(nγ)). Theorem 13 shows that model averaging reduces this ratio by a factor of g when ≤ 1 g .\nTheorem 13 (Hessian Sketch with Model Averaging) For the four methods with s = Õ ( d 2 ) , or uniform sampling with s = O (µd log d 2 ) , the inequalities\nbias(Wh) bias(W?) ≤ 1 + + ( g + 2 )‖X‖22 nγ ,\nvar(Wh) var(W?) ≤ 1 +\nhold with probability at least 0.8."
    }, {
      "heading" : "4. Experiments on Synthetic Data",
      "text" : "We conduct experiments on synthetic data to verify our main Theorems. Section 4.1 describes the data model and experiment settings. Sections 4.2 and 4.3 study classical and Hessian sketches from the optimization and statistical perspectives, respectively, to verify Theorems 1, 2, 5, 7. Sections 4.4 and 4.5 study the model averaging from the optimization and statistical perspectives, respectively, to corroborate Theorems 8, 9, 11, 13."
    }, {
      "heading" : "4.1 Settings",
      "text" : "Following (Ma et al., 2015, Yang et al., 2016), we construct X = Udiag(σ)VT ∈ Rn×d and y = Xw0 + ε ∈ Rn in the following way.\n• Let the rows of A ∈ Rn×d be i.i.d. sampled from multivariate t-distribution with covariance matrix C and v = 2 degree of freedom, where the (i, j)-th entry of C ∈ Rm×n is 2 × 0.5|i−j|. A has high row coherence. Let U be the orthonormal bases of A.\n• Let the entries of b ∈ Rd be equally paced between 0 and −6. Let σi = 10bi for all i ∈ [d].\n• Let V ∈ Rd×d be the orthonormal bases of a d× d standard Gaussian matrix.\n• Let w0 = [10.2d; 0.110.6d; 10.2d].\n• The entries of ε ∈ Rn are i.i.d. sampled from N (0, ξ2).\nIn this way, X has high row coherence; its condition number is κ(XTX) = 1012. Let S ∈ Rn×s be any of the six sketching methods considered in this paper. We fix n = 105, d = 500, and s = 5, 000. Since the sketching methods are randomized, we always repeat sketching 10 times and report the averaged results.\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch\nOptimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\nFigure 20: .\n.\nξ = 10−3 ξ = 10−2 ξ = 10−1\nWc\nγ 10-12 10-10 10-8 10-6 10-4 10-2\nO bj\nec tiv\ne F\nun ct\nio n\n10-7\n10-6\n10-5\n10-4\n10-3\n10-2\nγ 10-12 10-10 10-8 10-6 10-4 10-2\nO bj\nec tiv\ne F\nun ct\nio n\n10-4\n10-3\n10-2\nγ 10-12 10-10 10-8 10-6 10-4 10-2\nO bj\nec tiv\ne F\nun ct\nio n\n0.005\n0.01\n0.02\n0.05\nWh\nγ 10-12 10-10 10-8 10-6 10-4 10-2\nO bj\nec tiv\ne F\nun ct\nio n\n10-7\n10-6\n10-5\n10-4\n10-3\n10-2\nγ 10-12 10-10 10-8 10-6 10-4 10-2\nO bj\nec tiv\ne F\nun ct\nio n\n10-4\n10-3\n10-2\nγ 10-12 10-10 10-8 10-6 10-4 10-2\nO bj\nec tiv\ne F\nun ct\nio n\n0.005\n0.01\n0.02\n0.05"
    }, {
      "heading" : "4.2 Sketched MRR: Optimization Perspective",
      "text" : "We seek to verify Theorems 1 and 2 which study classical and Hessian sketches, respective, from the optimization perspective. In Figure 2, we plot the objective function value f(w) = 1 n‖Xw − y‖ 2 2 + γ‖w‖22 against γ, under different settings of noise intensity ξ. The black curves correspond to the optimal solution w?; the color curves are classical or Hessian sketch with different sketching methods. The results verify our theory: classical sketch wc is always close to optimal; Hessian sketch wh is much worse than the optimal when γ is small and y is mostly in the column space of X."
    }, {
      "heading" : "4.3 Sketched MRR: Statistical Perspective",
      "text" : "In Figure 3, we plot the analytical expressions for the squared bias, variance, and risk stated in Theorem 4 against the regularization parameter γ. Because the analytical expressions involve the random sketching matrix S, we randomly generate S, repeat this procedure 10 times, and report the average of the computed squared biases, variances, and risks. We fix ξ = 0.1. The results of this experiment match our theory: classical sketch magnified the variance, and Hessian sketch increased the bias. Even when γ is fine-tuned, the risks of classical and Hessian sketches can be much higher than those of the optimal solution. Our experiment also indicates that classical and Hessian sketches require setting γ larger than the best regularization parameter for the optimal solution W?.\n16\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch\nOptimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\nFigure 20: .\n.\nBias2 Var Risk = Bias2 + Var\nWc\nγ 10-12 10-10 10-8 10-6 10-4 10-2\nB ia s2 10-12 10-11 10-10 10-9 10-8 10-7 10-6 10-5 10-4 10-3 10-2\nγ 10-12 10-10 10-8 10-6 10-4 10-2\nV ar ia nc e 10-12 10-11 10-10 10-9 10-8 10-7 10-6 10-5 10-4 10-3 10-2\nγ 10-12 10-10 10-8 10-6 10-4 10-2\nR is k 10-12 10-11 10-10 10-9 10-8 10-7 10-6 10-5 10-4 10-3 10-2 the min risk of the classical sketch the min risk of the optimal solution optimal γ for the optimal solution optimal γ for the classical sketch\nWh\nγ 10-12 10-10 10-8 10-6 10-4 10-2\nB ia s2 10-12 10-11 10-10 10-9 10-8 10-7 10-6 10-5 10-4 10-3 10-2\nγ 10-12 10-10 10-8 10-6 10-4 10-2\nV ar ia nc e 10-12 10-11 10-10 10-9 10-8 10-7 10-6 10-5 10-4 10-3 10-2\nγ 10-12 10-10 10-8 10-6 10-4 10-2\nR is k 10-12 10-11 10-10 10-9 10-8 10-7 10-6 10-5 10-4 10-3 10-2 the min risk of the Hessian sketch the min risk of the optimal solution optimal γ for the optimal solution optimal γ for the Hessian sketch\nClassical sketch and Hessian sketch do not outperform each other in terms of the risk. When variance dominates bias, Hessian sketch is better in terms of the risk; when bias dominates variance, classical sketch is better. In the experiment yielding Figure 3, Hessian sketch had lower risk than classical sketch. This is not generally true: if we used a smaller ξ, so that the variance is dominated by bias, then classical sketch results in lower risks than Hessian sketch."
    }, {
      "heading" : "4.4 Model Averaging: Optimization Objective",
      "text" : "We use different intensity of noise—we set ξ = 10−2 or 10−1, where ξ defined in Section 4.1 indicates the intensity of the noise in the response vector y. We calculate the objective function values f(wc[g]) and f(w h [g]) under different settings of g, γ. We use different matrix sketching but fix the sketch size s = 5, 000. Theorem 8 shows that for large s, e.g., Gaussian projection with s = Õ (βd ) , then\nf ( wc[g] ) − f ( w? ) ≤ ( g + β 2 2 ) f(w?), (9)\nwhere β = ‖X‖22 ‖X‖22+nγ ≤ 1. In Figure 4(a) we plot g against the ratio\nf(wc[1])− f(w ?) f(wc[g])− f(w?) . (10)\n17\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch\nOptimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\nFigure 22: .\n23\n⇠ = 10 2 ⇠ = 10 1\n= 10 12\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n= 10 6\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\nFigure 1: avg obj nb classical\nh\n2\n(a) Classical sketch with model averaging.\n⇠ = 10 2 ⇠ = 10 1\n= 10 12\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n= 10 6\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\nFigure 2: avg obj nb hessian\nh\n3\n(b) Hessian sketch with model averaging.\nFigure 4: Empirical study of model averaging from optimization perspective. The x-axis is g, i.e. the number of samples over which model averaging averages. In 4(a), the y-axis is the ratio (log-scale) defined in (10). In 4(b), the y-axis is the ratio (log-scale) defined in (11).\nRapid growth of the ratio indicates high effectiveness of the model averaging. The results in Figure 4(a) indicate model averaging significantly improves the accuracy in terms of the objective function value. For the three random projection methods, the growth rate is almost linear in g. In Figure 4(a), we observe that the regularization parameter γ affects the ratio (10). The ratio grows faster when γ = 10−12 than when γ = 10−6. However, this phenomenon cannot be explained by our theory.\nTheorem 9 shows that for large sketch size s, e.g., Gaussian projection with s = Õ (β2d ) ,\nthen\nf(wh)− f(w?) ≤ ( g2 + 2 β2 )( ‖y‖22 n − f(w ?) ) ,\nwhere β = ‖X‖22 ‖X‖22+nγ ≤ 1. In Figure 4(b), we plot g against the ratio\nf(wh[1])− f(w ?) f(wh[g])− f(w?) . (11)\nRapid growth of the ratio indicates high effectiveness of the model averaging. If β2 ≤ 1 g2 , equivalently s is at least Õ(dg2), the ratio (11) should grow nearly quadratically with g. However, the requirement on the sketch size s can be hardly satisfied, unless s is big and g is small. The empirical results reflects that the growth rate is moderately rapid for very small g and very slow for large g. This is in accordance with the theory.\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch\nOptimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\n00.5110 -12\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\nFigure 21: .\n22\ns = 1, 000 s = 5, 000\n= 10 12\n0 10 20 30 40 50 10−5\n10−4\n10−3\n10−2\n10−1\ng\nVa ria nc e\n0 10 20 30 40 50 10−5\n10−4\n10−3\n10−2\ng\nVa ria nc e\n= 10 6\n0 10 20 30 40 50 10−6\n10−5\n10−4\n10−3\ng\nVa ria nc e\n0 10 20 30 40 50 10−6\n10−5\n10−4\ng\nVa ria nc e\nFigure 3: avg var nb classical\nh\n4\n(a) The variance var(wc[g]).\ns = 1, 000 s = 5, 000\n= 10 12\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n= 10 6\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\n0 10 20 30 40 50 0\n10\n20\n30\n40\n50\ng\nR at io\nFigure 4: avg varratio nb classical\nh\n5\n(b) The ratio var(wc[1])\nvar(wc [g]\n) ."
    }, {
      "heading" : "4.5 Model Averaging: Statistical Perspective",
      "text" : "We study the model averaging from the statistical perspective. We calculate bias(w?), var(w?) (optimal solution) according to Theorem 4 and bias(wc[g]), var(w c [g]) (classical sketch) and bias(wh[g]), var(w h [g]) (Hessian sketch) according to Theorem 10."
    }, {
      "heading" : "4.5.1 Classical Sketch",
      "text" : "Theorem 11 shows that for large enough s, e.g., Gaussian projection with s = Õ ( d 2 ) , then\nbias(wc[g])\nbias(w?) ≤ 1 + and\nvar(wc[g])\nvar(w?) ≤ n s (√ 1+ /g g + )2 hold with high probability. The theorem indicates that model averaging does not make the bias worse and that it improves the variance. We conduct experiments to verify this point.\nIn Figure 5(a) we plot g agains the variance var(wc[g]); the variance of the optimal solution w? is employed for comparison. Clearly, the variance drops as g grows. In particular, when s is big (s = 5, 000) and g exceeds ns (= 100,000 5,000 = 20), var(w c [g]) can be even lower than var(w?). This verifies Corollary 12. This has an important implication: if y is corrupted by intense noise, we can use the classical sketch with model averaging to obtain a solution which has lower variance than the optimal solution w?.\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch\nOptimal Solution\ng 050 Va ria\nnc e\n10-610-510 -4\nUniform Sampling Leverage Sampling Shrinked Lev. Sampling Gaussian Projection SRFT Count Sketch Optimal Solution\nFigure 22: .\n23\n1 10 20 30 40 50 1\n1.5\n2\n2.5\n3\n3.5\n4\ng\nR at io\n(a) s = 1, 000\n1 10 20 30 40 50 1\n1.5\n2\n2.5\n3\n3.5\n4\ng\nR at io\n(b) s = 2, 000\n1 10 20 30 40 50 1\n1.5\n2\n2.5\n3\n3.5\n4\ng\nR at io\n(c) s = 5, 000\nFigure 5: avg biasratio nb hessian\nh\nFigure 6: Empirical study of the bias of Hessian sketch with model averaging. The x-axis is g (number of samples over which model averaging averages); the y-axis is the ratio (12).\nTo make the decrease of var(wc[g]) more clear, in Figure 5(b) we plot g against the ratio var(wc\n[1] )\nvar(wc [g] ) . According to Theorem 11, this ratio grows linearly with g if s is at least Õ(dg). Otherwise, the ratio is sublinear with g. The theory is verified by the empirical results in Figure 5(b).\nWe plotted g against bias(wc[g]) in the same way as Figures 5(a) and 5(b). All the curves of g against the bias are almost horizontal, indicating that the increase of g does make the bias better or worse. We do not show the plots in the paper because these nearly horizontal curves are not interesting."
    }, {
      "heading" : "4.5.2 Hessian Sketch",
      "text" : "Theorem 13 shows that for large enough s, e.g., Gaussian projection with s = Õ ( d 2 ) , the inequalities\nbias(wh[g])\nbias(w?) ≤ 1 + + ( g + 2 )‖X‖22 nγ\nand var(wh[g])\nvar(w?) ≤ 1 +\nhold with high probability. Theoretically speaking, model averaging improves the bias without making the variance worse. The bound\nbias(wh[g])− bias(w ?)\nbias(w?) ≤ + ( g + 2 )‖X‖22 nγ\nindicates that if (1) nγ is much smaller than ‖X‖22 and (2) ≤ 1g , equivalently s is at least Õ(dg2), then the ratio is proportional to g .\nTo verify Theorem 13, we set γ very small—γ = 10−12—and vary s and g. In Figure 6 we plot the ratio\nbias(wh[1])− bias(w ?) bias(wh[g])− bias(w?) , (12)\n20\nby fixing γ = 10−12 and vary s and g. Ideally, for large sketch size s = Õ(dg2), the ratio should grow nearly linearly with g. Figure 6 shows only for large s and very small g, the growth can be near linear with g; this verifies our theory.\nWe have also plotted g against var ( wh[g] ) . We observe that var ( wh[g] ) remains nearly\nunaffected as g grows from 1 to 50. Since the curves of g against var ( wh[g] ) are almost horizontal lines, we do not show the plot in the paper."
    }, {
      "heading" : "5. Model Averaging Experiments on Real-World Data",
      "text" : "In Section 1 we mentioned that in the distributed setting where the feature-response pairs (x1,y1), · · · , (xn,yn) ∈ Rd×m are stored across g machines, classical sketch with model averaging requires only one round of communication, and is therefore a communicationefficient algorithm that can be used to: (1) obtain an approximate solution of the MRR problem with risk comparable to a batch solution, and (2) obtain a low-precision solution of the MRR optimization problem that can be used as an initializer for more communicationintensive optimization algorithms. In this section, we demonstrate both applications.\nWe use the Million Song Year Prediction Dataset, which has 463, 715 training samples and 51, 630 test samples with 90 features and one response. We normalize the data by shifting the responses to have zero mean and scaling the range of each feature to [−1, 1]. We randomly partition the training data into g parts, which amounts to uniform row selection with sketch size s = ng ."
    }, {
      "heading" : "5.1 Prediction Error",
      "text" : "We tested the prediction performance of sketched ridge regression by implementing classical sketch with model averaging in PySpark (Zaharia et al., 2010).5 We ran our experiments using PySpark in local mode; the experiments had three steps: (1) use five-fold crossvalidation to determine the regularization parameter γ; (2) learn the model w using\n5. The code is available at https://github.com/wangshusen/SketchedRidgeRegression.git\n‖w?‖2 .\nthe selected γ; and (3) use w to predict on the test set and record the mean squared errors (MSEs). These steps map nicely onto the Map-Reduce programming model used by PySpark.\nWe plot g = ns against the MSE in Figure 7. As g grows, the sketch size s = n g decreases, so the performance of classical sketching deteriorates. However classical sketch with model averaging always has MSE comparable to the optimal solution."
    }, {
      "heading" : "5.2 Optimization Error",
      "text" : "We mentioned earlier that classical sketch with or without model averaging can be used to initialize optimization algorithms for solving MRR. If w is initialized with zero-mean random variables or deterministically with zeros, then E‖w − w?‖2/‖w?‖2 ≥ 1. Any w with the above ratio substantially smaller than 1 provides a better initialization. We implemented classical sketch with and without model averaging in Python and calculated the above ratio on the training set of the Year Prediction dataset; to estimate the expectation, we repeated the procedure 100 times and report the average of the ratios.\nIn Figure 8 we plot g against the average of the ratio ‖w−w ?‖2\n‖w?‖2 at different settings of the regularization parameter γ. Clearly, classical sketch does not give a very good initialization unless g is small (equivalently, the sketch size s = ng is large). In contrast, the averaged solution is always close to w?."
    }, {
      "heading" : "6. Sketch of Proof",
      "text" : "In this section, we provide an outline of the proofs of our main results. Detailed proofs can be found in the Appendix. Section 6.1 shows some key properties of matrix sketching. Section 6.2 considers taking average of sketched matrices; the results will be applied to analyze sketched MRR with model averaging. Sections 6.3 to 6.6 establish the structural results of sketched MRR with or without model averaging.\nOur main results in Section 3 (Theorems 1, 2, 5, 7, 8, 9, 11, 13) directly follow from the key properties of matrix sketching and the structural results of sketched MRR. Table 4 summarizes the dependency relationship among the theorems. For example, Theorem 1, which studies classical sketch from optimization perspective, is one of our main theorems and can be proved using Theorems 16 and 21."
    }, {
      "heading" : "6.1 Properties of Matrix Sketching",
      "text" : "Our analysis of the sketched MRR uses some of the three key properties defined in Assumption 1. Theorem 16 establishes that the six sketching methods consider in this paper under different settings enjoy the three key properties. Finally, we show in Theorem 17 the lower bounds of ‖S‖22; the theorem will be used to prove the lower bounds of variance in Theorem 5.\nIn Assumption 1, the subspace embedding property is that sketching preserves the product of a row orthogonal matrix and its transpose. Equivalently, it ensures that all the singular values of any sketched column orthogonal matrix are around one. The matrix multiplication property states that sketching preserves the multiplication of a row orthogonal matrix and an arbitrary matrix. The bounded spectral norm property is that the spectral norm of S ∈ Rn×s is around ns .\nAssumption 1 Let η, ∈ (0, 1) be any fixed parameters. Let B be any matrix of proper size, ρ = rank(X), and U ∈ Rn×ρ be the orthogonal bases of X. Let S ∈ Rn×s be a sketching matrix where s depends on η and/or . Assume S satisfies\n1.1 ∥∥UTSSTU− Iρ∥∥2 ≤ η (Subspace Embedding Property);\n1.2 ∥∥UTSSTB−UTB∥∥2\nF ≤ ‖B‖2F (Matrix Multiplication Property);\n1.3 ‖S‖22 ≤ θns for some constant θ (Bounded Spectral Norm Property).\nRemark 14 Note that the first two assumptions were identified in (Mahoney, 2011) and are the relevant structural conditions needed to be satisfied to obtain strong results from the optimization perspective. The third condition is new, but Ma et al. (2015), Raskutti and Mahoney (2016) demonstrated that some sort of additional condition is needed to obtain strong results from the statistical perspective.\nRemark 15 We note that UTU = Iρ, and thus Assumption 1.1 can actually be expressed in the form of an approximate matrix multiplication bound (Drineas et al., 2006a). We call it the Subspace Embedding Property since, as first highlighted in Drineas et al. (2006b), this subspace embedding property is the key result to obtain high-quality sketching algorithms for regression and related problems.\nTheorem 16 shows that the six sketching methods with sufficiently large s satisfy the three properties. We prove Theorem 16 in Appendix B. Theorem 16 shows that for all the sketching methods except leverage score sampling,6 ‖S‖22 has nontrivial upper bound. That is why Theorems 5 and 11 are not applicable to leverage score sampling. That is also the motivation of using the shrinked leverage score sampling.\nTheorem 16 Fix failure probability δ and error parameters η and ; set the sketch size s as Table 5. Assumption 1.1 is satisfied with probability at least 1 − δ1. Assumption 1.2 is satisfied with probability at least 1 − δ2. Assumption 1.3 is satisfied either surely or with probability close to one; the parameter θ is shown in Table 5.\nTheorem 17 establishes lower bound on ‖S‖22. The results will be applied to prove the lower bound of the variance of classical sketch. From Table 6 we can see that the bound of (shrinked) leverage score sampling is not interesting, because µ can be very large. That is why in Theorems 5, the shrinked leverage score sampling lacks nontrivial lower bound. We prove Theorem 17 in Appendix B.\nTheorem 17 (Lower Bound on the Spectral Norm of Sketching Matrix) The sketching methods and ϑ are described in Table 6. Then STS ϑns Is holds either surely or with probability close to one.\n6. If one leverage score approaches zero, then the corresponding sampling probability pi goes to zero. By the definition of S, the scaling 1√\nspi goes to infinity, which makes ‖S‖22 unbounded.\nRemark 18 Let p1, · · · , pn be the sampling probabilities. By the definition of S, the nonzero entries of S can be any of 1√spi , for i ∈ [n].\nFor leverage score sampling, ‖S‖22 has neither nontrivial upper nor lower bound.7 It is because mini pi can be close to zero and maxi pi can be large (close to one).\nFor shinked leverage score sampling, because mini pi is at least 1 2n , ‖S‖ 2 2 has nontrivial upper bound; unfortunately, similar to leverage score sampling, maxi pi can be large, and thereby ‖S‖22 does not have nontrivial lower bound."
    }, {
      "heading" : "6.2 Matrix Sketching with Averaging",
      "text" : "We have shown that sketching can be applied to approximate matrix multiplication; see Assumptions 1.1 and 1.2. What will happen if we independently draw g sketches, approximately compute the multiplications, and average the g products? Intuitively, the averaged product should better approximate the true product.\nLet us justify the intuition formally. Let S1, · · · ,Sg ∈ Rn×s be some sketching matrices and A and B are arbitrary fixed matrices with proper size. It is not hard to show that\n1\ng g∑ i=1 ATSiS T i B = A TSSTB,\nwhere S = 1√g [S1, · · · ,Sg] ∈ R n×gs. Clearly S is a sketching matrix larger than any of S1, · · · ,Sg. If S1, · · · ,Sg are column selection, SRHT, or Gaussian projection matrices, then S is the same type of sketching matrix.8\nTo analyze the sketched MRR with model averaging, we make the following assumptions. Here Assumption 2.1 is the subspace embedding property; Assumption 2.2 is the matrix multiplication property; Assumption 2.3 is the bounded spectral norm property.\nAssumption 2 Let η, ∈ (0, 1) be any fixed parameters. Let B be any matrix of proper size, ρ = rank(X), and U ∈ Rn×ρ be the orthogonal bases of X. Let S1, · · · ,Sg ∈ Rn×s be\n7. In our application, nontrivial bound means ‖S‖22 is of order ns . 8. The CountSketch does not enjoy such property. If Si ∈ Rn×s is CountSketch matrix, then it has only\none non-zero entry in each row. In contrast, S ∈ Rn×gs has g non-zero entries in each row; thus S is not CountSketch matrix.\ncertain sketching matrices and S = 1√g [S1, · · · ,Sg] ∈ R n×gs; here s depends on η and/or ."
    }, {
      "heading" : "Assume Si and S satisfy",
      "text" : "2.1 ∥∥UTSiSTi − Iρ∥∥2 ≤ η for all i ∈ [g] and ∥∥UTSSTU− Iρ∥∥2 ≤ ηg ;\n2.2 ( 1 g ∑g i=1 ∥∥UTSiSTi B−UTB∥∥F )2 ≤ ‖B‖2F and ∥∥UTSSTB−UTB∥∥2F ≤ g‖B‖2F ; 2.3 For some constant θ, ‖Si‖22 ≤ θns for all i ∈ [g], and ‖S‖ 2 2 ≤ θngs .\nTheorem 19 shows that random column selection, SRHT, and Gaussian projection matrices satisfy Assumptions 2.1, 2.2, 2.3. We prove Theorem 19 in Appendix B.\nTheorem 19 Let S1, · · · ,Sg ∈ Rn×s be the same type of random sketching matrices, which can be independently drawn random column selection, SRHT, or Gaussian projection matrices. Fix failure probability δ and error parameters η and ; set the sketch size s as Table 5.\nThen Assumption 2.1 holds with probability at least 1− gδ1 − δ1. Assumption 2.2 holds with probability at least 1 − 2δ2. Assumption 2.3 satisfied either surely or with probability close to one; the parameter θ is defined in Table 5.\nIn Theorem 16, Assumption 1.1 fails with probability at most δ1. In contrast, in Theorem 19, the counterpart assumption fails with probability at most (g+ 1)δ1. However, this makes little different, because δ1 is in the logarithm and can be set very small (recall Table 5).\nRemark 20 We do not know whether CountSketch enjoys the properties in Assumption 2. This problem is difficult for two reasons. First, as aforemented, the concatenation of multiple CountSketch matrices is not a CountSketch matrix. Second, the failure probability of the subspace embedding property of CountSketch is constant, rather than exponentially small."
    }, {
      "heading" : "6.3 Sketched MRR: Optimization Perspective",
      "text" : "Theorem 21 holds under the subspace embedding property and the matrix multiplication property (Assumptions 1.1 and 1.2). We prove Theorems 21 in Appendix C.\nTheorem 21 (Classical Sketch) Let Assumptions 1.1 and 1.2 hold for the sketching matrix S ∈ Rn×s. Let η and be defined in Assumption 1. Let α = 2max{ ,η 2}\n1−η and\nβ = ‖X‖22 ‖X‖22+nγ . Then\nf(Wc)− f(W?) ≤ αβf(W?).\nTheorem 22 holds under the subspace embedding property (Assumption 1.1). We prove Theorems 22 in Appendix C.\nTheorem 22 (Hessian Sketch) Let Assumption 1.1 hold for the sketching matrix S ∈ Rn×s. Let η be defined in Assumption 1 and β = ‖X‖ 2 2\n‖X‖22+nγ . Then\nf(Wh)− f(W?) ≤ η 2β2\n(1− η)2 ( ‖Y‖2F n − f(W?) ) ."
    }, {
      "heading" : "6.4 Sketched MRR: Statistical Perspective",
      "text" : "Theorem 23 holds under the subspace embedding property (Assumption 1.1) and the bounded spectral norm property (Assumption 1.3). The theorem shows that for classical sketch, the bias is close to the optimal solution, but the bound on variance is very weak. We prove Theorems 23 in Appendix D.\nTheorem 23 (Classical Sketch) Let η and θ be defined in Assumption 1. Under Assumption 1.1, it holds that\n1 1 + η ≤ bias(W\nc)\nbias(W?) ≤ 1 1− η .\nUnder Assumptions 1.1 and 1.3, it holds that\nvar(Wc)\nvar(W?) ≤ (1 + η) (1− η)2 θn s .\nTheorem 24 establishes a lower bound on the variance of classical sketch. We prove Theorems 24 in Appendix D.\nTheorem 24 (Lower Bound on the Variance) Under Assumption 1.1 and the additional assumption that STS ϑns Is, it holds that\nvar(Wc) var(W?) ≥ 1− η (1 + η)2 ϑn s .\nTheorem 25 holds under the subspace embedding property (Assumption 1.1). In the theorem we define ρ = rank(X) and σ1 ≥ · · · ≥ σρ as the singular values of X. We prove Theorems 25 in Appendix D.\nTheorem 25 (Hessian Sketch) Let η be defined in Assumption 1. Under Assumption 1.1, it holds that\nbias(Wh)\nbias(W?) ≤ 1 1− η\n( 1 +\nησ21 nγ\n) ,\n1 1 + η ≤ var(W\nh)\nvar(W?) ≤ 1 1− η .\nFurther assume that σ2ρ ≥ nγ η . Then\nbias(Wh)\nbias(W?) ≥ 1 1 + η (ησ2ρ nγ − 1 ) ."
    }, {
      "heading" : "6.5 Model Averaging: Optimization Perspective",
      "text" : "Theorem 26 holds under the subspace embedding property (Assumption 2.1) and the matrix multiplication property (Assumption 2.2). We prove Theorems 26 in Appendix E.\nTheorem 26 (Classical Sketch with Model Averaging) Let η and be defined in Assumption 2. Let α = 2 [ max {√\ng , η g\n} + 2β max { η √ , η2 }]2 and β =\n‖X‖22 ‖X‖22+nγ\n≤ 1. Under Assumption 2.1 and 2.2, we have that\nf(Wc)− f(W?) ≤ αβf(W?).\nTheorem 27 holds under the subspace embedding property (Assumption 2.1). We prove Theorems 27 in Appendix E.\nTheorem 27 (Hessian Sketch with Model Averaging) Let η be defined in Assumption 2. Let α = (η g + η2 1−η ) and β = ‖X‖22 ‖X‖22+nγ ≤ 1. Under Assumption 2.1, we have that\nf(Wh)− f(W?) ≤ α2β2 ( 1 n ‖Y‖2F − f(W?) ) ."
    }, {
      "heading" : "6.6 Model Averaging: Statistical Perspective",
      "text" : "Theorem 28 requires the subspace embedding property (Assumption 2.1). In addition, to bound the variance, the spectral norms of S1, · · · ,Sg and S = 1√g [S1, · · · ,Sg] must be bounded (Assumption 2.3). The theorem shows that model averaing improves the variance without making the bias worse. We prove Theorems 28 in Appendix F.\nTheorem 28 (Classical Sketch with Model Averaging) Under Assumption 2.1, it holds that\nbias(Wc)\nbias(W?) ≤ 1 1− η .\nUnder Assumptions 2.1 and 2.3, it holds that\nvar(Wc)\nvar(W?) ≤ θn s (√ 1 + η/g √ g + η √ 1 + η 1− η )2 ."
    }, {
      "heading" : "Here η and θ are defined in Assumption 2.",
      "text" : "Theorem 29 requires the subspace embedding property (Assumption 2.1). It shows that model averaging improves the bias without making the variance worse. We prove Theorems 29 in Appendix F.\nTheorem 29 (Hessian Sketch with Model Averaging) Under Assumption 2.1, it holds that:\nbias(Wh)\nbias(W?) ≤ 1 1− η + (η g + η2 1− η )‖X‖22 nγ , var(Wh)\nvar(W?) ≤ 1 1− η .\nHere η is defined in Assumption 2."
    }, {
      "heading" : "7. Conclusions",
      "text" : "We studied sketched matrix ridge regression (MRR) from optimization and statistical perspectives. Using classical sketch, by taking a large enough sketch, one can obtain an -accurate approximate solution. Counterintuitively and in contrast to classical sketch, the relative error of Hessian sketch increases as the responses Y are better approximated by linear combinations of the columns of X. Both classical and Hessian sketches can have statistical risks that are worse than the risk of the optimal solution by an order of magnitude.\nWe proposed the use of model averaging to attain better optimization and statistical properties. We have shown that model averaging leads to substantial improvements in the theoretical error bounds, suggesting applications in distributed optimization and machine learning. We also empirically verified its practical benefits."
    }, {
      "heading" : "Appendix A. Risk of Fixed Design Model",
      "text" : "Let the risk R(W) be defined in (8). The risk R(W) determines how well the learned W generalize to test data, which is the reason why we care about the risk. We formally explain this in the following.\nUnder the fixed design model, a test sample x is uniformly drawn from the set {x1, · · · ,xn} ⊂ Rd. The corresponding response is y = 〈W0,x〉+ ξ′ ∈ Rm, where ξ′ ∈ Rm captures random noise; assume that E[ξ′] = 0 and that ξ′ is independ of W, W0, and x. The test mean squared error (MSE) is\nEx,ξ ∥∥WTx− y∥∥2\n2 = Ex,ξ ∥∥WTx−WT0 x + W0x− y∥∥22 = Ex,ξ\n∥∥WTx−WT0 x− ξ′∥∥22 = Ex,ξ\n[∥∥WTx−WT0 x∥∥22 + ∥∥ξ′∥∥22] = 1\nn n∑ i=1 ∥∥WTxi −WT0 xi∥∥22 + E∥∥ξ′∥∥22 = R(W) + E ∥∥ξ′∥∥2 2 .\nTo this end, it is clear that the test MSE equals to the (training) risk R(W ) plus the “intensity” of the noise in test response."
    }, {
      "heading" : "Appendix B. Properties of Matrix Sketching: Proofs",
      "text" : "In Section B.1 we prove Theorem 16. In Section B.2, we prove Theorem 17. In Section B.3 we prove Theorem 19."
    }, {
      "heading" : "B.1 Proof of Theorem 16",
      "text" : "We prove the six sketching methods considered in this paper all satisfy the three key properties. In Section B.1.1 we show the six sketching methods satisfy Assumptions 1.1 and 1.2. In section B.1.2 we show the six sketching methods satisfy Assumption 1.3."
    }, {
      "heading" : "B.1.1 Proof of Assumptions 1.1 and 1.2",
      "text" : "For uniform sampling, leverage score sampling, Gaussian projection, SRHT, and CountSketch, the subspace embedding property and matrix multiplication property have been established by the previous work (Drineas et al., 2008, 2011, Meng and Mahoney, 2013, Nelson and Nguyên, 2013, Tropp, 2011, Woodruff, 2014). See also (Wang et al., 2016c) for the summary.\nIn the following we prove only the shrinked leverage score sampling. We cite the following lemma from (Wang et al., 2016b); the lemma was firstly proved by the work (Drineas et al., 2008, Gittens, 2011, Woodruff, 2014).\nLemma 30 (Wang et al. (2016b)) Let U ∈ Rn×ρ be any fixed matrix with orthonormal columns. The column selection matrix S ∈ Rn×s samples s columns according to arbitrary probabilities p1, p2, · · · , pn. Assume α ≥ ρ and\nmax i∈[n] ‖ui:‖22 pi ≤ α.\nWhen s ≥ α6+2η 3η2 log(ρ/δ1), it holds that\nP {∥∥Iρ −UTSSTU∥∥2 ≥ η} ≤ δ1."
    }, {
      "heading" : "When s ≥ α δ2 , it holds that",
      "text" : "E ∥∥UB−UTSSTB∥∥2\nF ≤ δ2 ‖B‖2F ;\nas a consequence of Markov’s inequality, it holds that\nP {∥∥UB−UTSSTB∥∥2\nF ≥ ‖B‖2F\n} ≤ δ2.\nHere the expectation and probability are all w.r.t. the randomness in S.\nNow we apply the above lemma to analyze shrinked leverage score sampling. For the approximate shrinked leverage scores defined in (5), the sampling probabilities satisfy\npi = 1\n2 ( 1 n + l̃i∑n q=1 l̃q ) ≥ ‖ui:‖ 2 2 2τρ .\nHere l̃i and τ are defined in (5). Thus for all i ∈ [n], ‖ui:‖22 ρ < 2τρ. We can then apply Lemma 30 to show that Assumption 1.1 holds with probability at least 1 − δ1 when s ≥ 2τρ6+2η\n3η2 log(ρ/δ1) and that Assumption 1.2 holds with probability at least 1 − δ2 when\ns ≥ 2τρ δ2 ."
    }, {
      "heading" : "B.1.2 Proof of Assumption 1.3",
      "text" : "For Uniform Sampling and SRHT, it is easy to show that STS = ns Is. Thus ‖S‖ 2 2 = n s .\nLet {psi} and {pui } be the sampling probabilites of the leverage score sampling and uniform sampling, respectively. Obviously psi ≥ 12p u i . Thus for the shrinked leverage score sampling, ‖S‖22 ≤ 2ns . Vershynin (2010) showed that the greatest singular value of the standard Gaussian matrix G ∈ Rn×s is at most √ n + √ s + t with probability at least 1 − 2e−t2/2. Thus for Gaussian projection matrix S,\n‖S‖22 = 1\ns ‖G‖22 ≤\n( √ n+ √ s+ t)2\ns\nholds with probability at least 1− 2e−t2/2. If S is the CountSketch matrix, then each row of S has exactly one nonzero entry, either 1 or −1. Because the columns of S are orthogonal to each other, it holds that\n‖S‖22 = max i∈[s] ‖s:i‖22 = max i∈[s] nnz(s:i).\nThe problem of bounding nnz(s:i) is equivalent to assigning n balls into s bins uniformly at random and bounding the number of balls in the bins. Patrascu and Thorup (2012) showed that the maximal number of balls in any bin is at most n/s+O (√ n/s logc n ) with probability at least 1− 1n , where c = O(1). Thus\n‖S‖22 = max i∈[s]\nnnz(s:i) ≤ n\ns +O\n(√ n logc n√\ns\n) = n\ns\n( 1 + o(1) ) holds with probability at least 1− 1n ."
    }, {
      "heading" : "B.2 Proof of Theorem 17",
      "text" : "For uniform sampling and SRHT, it holds that STS = ns Is. For non-uniform sampling with probabilities p1, · · · , pn ( ∑ i pi = 1), let pmax = maxi pi. The smallest entry in S is 1√spmax , and thus S TS 1spmax Is. For the leverage score sampling, pmax = µ n . For the shrinked leverage score sampling, pmax = 1+µ 2n . The lower bound on ‖S‖ 2 2 is established. Vershynin (2010) showed that the smallest singular value of any n×s standard Gaussian matrix G is at least √ n− √ s − t with probability at least 1 − 2e−t2/2. If S = 1√\ns G is the\nGaussian projection matrix, the smallest eigenvalue of STS is (1− o(1))ns with probability very close to one.\nIf S is the CountSketch matrix, then each row of S has exactly one nonzero entry, either 1 or −1. Because the columns of S are orthogonal to each other, it holds that\nσ2min(S) = min i∈[s] ‖s:i‖22 = min i∈[s] nnz(s:i).\nThe problem of bounding nnz(s:i) is equivalent to assigning n balls into s bins uniformly at random and bounding the number of balls in the bins. Standard concentration argument can show that each bin has at least ns (1 − o(1)) balls w.h.p. Hence σ 2 min(S) ≥ ns (1 − o(1)) w.h.p."
    }, {
      "heading" : "B.3 Proof of Theorem 19",
      "text" : "Assumption 2.1. By Theorem 16 and the union bound, we have that ∥∥UTSiSTi −Iρ∥∥2 ≤ η hold simultaneously for all i ∈ [g] with probability at least 1 − gδ1. Because S ∈ Rn×gs is the same type of sketching matrix, it follows from Theorem 16 that\n∥∥UTSSTU− Iρ∥∥2 ≤ ηg holds with probability at least 1− δ1.\nAssumption 2.2. By the same proof of Theorem 16, we can easily show that\nE ∥∥UTB−UTSiSTi B∥∥2F ≤ δ2 ‖B‖2F ,\nwhere B is any fixed matrix and the expectation is taken w.r.t. S. It follows from Jensen’s inequality that\n( E ∥∥UTSiSTi B−UTB∥∥F)2 ≤ E∥∥UTSiSTi B−UTB∥∥2F ≤ δ2 ∥∥B∥∥2F .\nIt follows that\n1\ng g∑ i=1 E ∥∥UTSiSTi B−UTB∥∥F ≤ √δ2 ∥∥B∥∥F ,\nand thus\n(1 g g∑ i=1 E ∥∥UTSiSTi B−UTB∥∥F)2 ≤ δ2 ∥∥B∥∥2F .\nIt follows from Markov’s bound that\nP {(1\ng g∑ i=1 ∥∥UTSiSTi B−UTB∥∥F)2 ≤ ∥∥B∥∥2F} ≥ 1− δ2. Because S ∈ Rn×gs is the same type of sketching matrix, it follows from Theorem 16 that∥∥UTSSTB−UTB∥∥2\nF ≤ g‖B‖ 2 F holds with probability at least 1− δ2.\nAssumption 2.3. Theorem 16 shows that ‖Si‖22 can be bounded either surely or with probability very close to 1 (assume n is big enough). Because g n, ‖Si‖22 can be bounded simultaneously for all i ∈ [g] either surely or with probability close to 1. Because S ∈ Rn×gs is the same type of sketching matrix, it follows from Theorem 16 that ‖S‖22 ≤ ngs holds either surely or with probability very close to 1."
    }, {
      "heading" : "Appendix C. Sketched MRR from Optimization Perspective: Proofs",
      "text" : "In Section C.1 we establish a key lemma. In Section C.2 we prove Theorem 21. In Section C.3 we prove Theorem 22."
    }, {
      "heading" : "C.1 Key Lemma",
      "text" : "Recall that objective function of the matrix ridge regression (MRR)is\nf(W) , 1\nn\n∥∥XW −Y∥∥2 F + γ‖W‖2F .\nThe optimal solution is W? = argminW f(W). The following is the key lemma for decomposing difference between any W and W?.\nLemma 31 For any matrix W and any nonsingular matrix M of proper size, it holds that\nf(W) = 1 n tr [ YTY − (2W? −W)T (XTX + nγIn)W ] ,\nf(W?) = 1\nn\n[∥∥Y⊥∥∥2 F + nγ ∥∥(Σ2 + nγIρ)−1/2UTY∥∥2F ],\nf(W)− f(W?) = 1 n ∥∥∥(XTX + nγId)1/2(W −W?)∥∥∥2 F ,∥∥∥M−1(W −W?)∥∥∥2\nF ≤ σ−2min\n[ (XTX + nγId) 1/2M ] ∥∥∥(XTX + nγId)1/2(W −W?)∥∥∥2\nF .\nHere X = UΣVT is the SVD; Y⊥ = Y −XX†Y.\nProof Let U be the left singular vectors of X. The objective function f(W) can be written as\nf(W) = 1\nn\n∥∥XW −Y∥∥2 F + γ ∥∥W∥∥2 F\n= 1 n tr [ YTY − (2W? −W)T (XTX + nγIn)W ] .\nThen\nf(W?) = 1 n tr [ YT ( In −X(XTX + nγId)−1XT ) Y ]\n= 1 n tr [ YT ( In −U(Iρ + nγΣ−2)−1UT ) Y ] = 1 n tr [ YTY −YTUUTY + YTUUTY −YTU(Iρ + nγΣ−2)−1UTY\n] = 1\nn\n{ tr [ YT (In −UUT )Y ] + nγ · tr [ YTU ( Σ2 + nγIρ )−1 UTY ]} = 1\nn\n[∥∥Y⊥∥∥2 F + nγ ∥∥(Σ2 + nγIρ)−1/2UTY∥∥2F ].\nThe difference in the objective function value is\nf(W)− f(W?) = 1 n\ntr [ (W −W?)T (XTX + nγId)(W −W?) ] = 1\nn ∥∥∥(XTX + nγId)1/2(W −W?)∥∥∥2 F .\nBecause σmin(A)‖B‖F ≤ ‖AB‖F holds for any nonsingular A and any B, it holds for any nonsingular matrix M that\nσ2min [ (XTX + nγId) 1/2M ]∥∥∥M−1(W −W?)∥∥∥2\nF ≤ ∥∥∥(XTX + nγId)1/2MM−1(W −W?)∥∥∥2 F\n= ∥∥∥(XTX + nγId)1/2(W −W?)∥∥∥2\nF .\nThe last claim in the lemma follows from the above inequality."
    }, {
      "heading" : "C.2 Proof of Theorem 21",
      "text" : "Proof Let ρ = rank(X), U ∈ Rn×ρ be the left singular vectors of X, and Y⊥ = Y − XX†Y = Y −UUTY. It follows from the definition of W? and Wc that\nWc −W? = (XTSSTX + nγId)−1XTSSTY − (XTX + nγId)−1XTY.\nIt follows that\n(XTSSTX + nγId)(W c −W?)\n= XTSSTY⊥ + XTSSTXX†Y − (XTSSTX + nγId)(XTX + nγId)−1XTY = XTSSTY⊥ − nγX†Y + (XTSSTX + nγId) [ X† − (XTX + nγId)−1XT ] Y = XTSSTY⊥ − nγX†Y + nγ(XTSSTX + nγId)(XTX + nγId)−1X†Y = XTSSTY⊥ + nγ(XTSSTX−XTX)(XTX + nγId)−1X†Y.\nIt follows that\n(XTX + nγId) −1/2(XTSSTX + nγId)(W c −W?) = A + B, (13)\nwhere\nA = [ (XTX + nγId) 1/2 ]† XTSSTY⊥ = V(Σ2 + nγIρ)−1/2ΣUSSTY⊥,\nB = nγ [ (XTX + nγId) 1/2 ]†\n(XTSSTX−XTX)(XTX + nγId)†X†Y = nγV(Σ2 + nγIρ)\n−1/2Σ(UTSSTU− Iρ)Σ(Σ2 + nγIρ)−1Σ−1UTY = nγVΣ(Σ2 + nγIρ) −1/2(UTSSTU− Iρ)(Σ2 + nγIρ)−1UTY.\nIt follows from (13) that\n(XTX + nγId) 1/2 ( Wc −W? ) = [ (XTX + nγId) −1/2(XTSSTX + nγId)(X TX + nγId)\n−1/2]†(A + B) By Assumption 1.1, we have that\n(1− η)(XTX + nγId) (XTSSTX + nγId) (1 + η)(XTX + nγId)\nIt follows that∥∥∥[(XTX + nγId)−1/2(XTSSTX + nγId)(XTX + nγId)−1/2]†∥∥∥ 2 ≤ 1 1− η . Thus∥∥∥(XTX + nγId)1/2(Wc −W?)∥∥∥2 F ≤ 1 1− η ∥∥∥A + B∥∥∥2 F ≤ 2 1− η (∥∥A∥∥2 F + ∥∥B∥∥2 F ) .\nLemma 31 shows f ( Wc ) − f ( W? ) = 1\nn ∥∥∥(XTX + nγId)1/2(Wc −W?)∥∥∥2 F ≤ 2 n(1− η) (∥∥A∥∥2 F + ∥∥B∥∥2 F ) . (14)\nWe respectively bound ‖A‖2F and ‖B‖2F in the following. It follows from Assumption 1.2 and UTY⊥ = 0 that\n‖A‖2F = ∥∥∥V(Σ2 + nγIρ)−1/2ΣUSSTY⊥∥∥∥2\nF ≤ ∥∥(Σ2 + nγIρ)−1/2Σ∥∥22 ∥∥UTSSTY⊥ −UTY⊥∥∥2F\n≤ ∥∥(Σ2 + nγIρ)−1/2Σ∥∥22 ∥∥Y⊥∥∥2F .\nBy the definition of B, we have ‖B‖2F ≤ n2γ2 ∥∥Σ(Σ2 + nγIρ)−1/2(UTSSTU− Iρ)(Σ2 + nγIρ)−1UTY∥∥2F\n≤ n2γ2 ∥∥Σ(Σ2 + nγIρ)−1/2(UTSSTU− Iρ)(Σ2 + nγIρ)−1/2∥∥22∥∥(Σ2 + nγIρ)−1/2UTY∥∥2F\n= n2γ2 ∥∥ΣN∥∥2\n2 ∥∥(Σ2 + nγIρ)−1/2UTY∥∥2F , where we define N = (Σ2 +nγIρ)\n−1/2(UTSSTU− Iρ)(Σ2 +nγIρ)−1/2. By Assumption 1.1, we have −η(Σ2 + nγIρ)−1 N η(Σ2 + nγIρ)−1. It follows that\n‖B‖2F ≤ n2γ2 ∥∥ΣN2Σ∥∥\n2 ∥∥(Σ2 + nγIρ)−1/2UTY∥∥2F ≤ η2n2γ2\n∥∥Σ(Σ2 + nγIρ)−2Σ∥∥2∥∥(Σ2 + nγIρ)−1/2UTY∥∥2F = η2n2γ2\n∥∥(Σ2 + nγIρ)−1Σ∥∥22∥∥(Σ2 + nγIρ)−1/2UTY∥∥2F = η2nγ\n∥∥(Σ2 + nγIρ)−1/2Σ∥∥22∥∥(Σ2 + nγIρ)−1/2UTY∥∥2F . The last equality follows from that ‖(Σ2 + nγIρ)−1/2‖2 ≤ nγ. It follows that\n‖A‖2F + ‖B‖2F ≤ max { , η2 }∥∥∥(Σ2 + nγId)−1Σ∥∥∥ 2 [∥∥Y⊥∥∥2 F + nγ ∥∥(Σ2 + nγId)−1/2UTY∥∥2F ]\n≤ max { , η2 } σ2max σ2max + nγ [∥∥Y⊥∥∥2 F + nγ ∥∥(Σ2 + nγId)−1/2UTY∥∥2F ]\n≤ max { , η2 } βnf(W?). (15)\nHere the last inequality follows from Lemma 31. Then the theorem follows from (15) and (14)."
    }, {
      "heading" : "C.3 Proof of Theorem 22",
      "text" : "Proof By the definition of Wh and W?, we have (XTX + nγId) 1/2 ( Wh −W? ) = (XTX + nγId) 1/2 [ (XTSSTX + nγId) † − (XTX + nγId)† ] XTY\n= V(Σ2 + nγIρ) 1/2 [ (ΣUTSSTUΣ + nγIρ) † − (Σ2 + nγIρ)−1 ] ΣUTY.\nIt follows from Assumption 1.1 that UTSSTU has full rank, and thus (XTX + nγId) 1/2 ( Wh −W? ) = V(Σ2 + nγIρ) 1/2 [ (ΣUTSSTUΣ + nγIρ) −1 − (Σ2 + nγIρ)−1 ] ΣUTY\n= V(Σ2 + nγIρ) 1/2(Σ2 + nγIρ) −1(Σ2 −ΣUTSSTUΣ)(ΣUTSSTUΣ + nγIρ)−1ΣUTY = V(Σ2 + nγIρ) −1/2Σ(Iρ −UTSSTU)Σ(ΣUTSSTUΣ + nγIρ)−1ΣUTY.\nwhere the second equality follow from M−1 −N−1 = N−1(N−M)M−1. We define (XTX + nγId) 1/2 ( Wh −W? ) = VABC,\nwhere\nA = (Σ2 + nγIρ) −1/2Σ(Iρ −UTSSTU)Σ(Σ2 + nγIρ)−1/2, B = (Σ2 + nγIρ) 1/2(ΣUTSSTUΣ + nγIρ) −1(Σ2 + nγIρ)1/2, C = (Σ2 + nγIρ) −1/2ΣUTY.\nIt follows from Assumption 1.1 that ‖A‖2 ≤ η ∥∥∥(Σ2 + nγIρ)−1/2Σ2(Σ2 + nγIρ)−1/2∥∥∥\n2 ≤ ηβ,\n‖B‖2 ≤ (1− η)−1.\nIt holds that∥∥C∥∥2 F ≤ ∥∥∥(Σ2 + nγIρ)−1/2ΣUTY∥∥∥2 F\n= [ tr ( YTUUTY ) − nγ tr ( YTU(Σ2 + nγId) −1UTY )]\n= [ − tr ( YT (Id −UUT )Y ) − nγ tr ( YTU(Σ2 + nγId) †UTY ) + tr ( YTY )] = ( − nf(W?) + ∥∥Y∥∥2 F ) ,\nwhere the last equality follows from Lemma 31. It follows from Lemma 31 that\nf(Wh)− f(W?) = 1 n ∥∥(XTX + nγId)1/2(Wh −W?)∥∥2F = 1\nn\n∥∥ABC∥∥2 F ≤ η 2β2 (1− η)2 ( 1 n ∥∥Y∥∥2 F − f(W?) ) ."
    }, {
      "heading" : "Appendix D. Sketched MRR from Statistical Perspective: Proofs",
      "text" : "In Section D.1 we prove Theorem 4. In Section D.2 we prove Theorem 23. In Section D.3 we prove Theorem 24. In Section B.2 we prove Theorem 17. In Section D.4 we prove Theorem 25. Recall that the fixed design model is Y = XW0 + Ξ where Ξ is random, EΞ = 0, and E[ΞΞT ] = ξ2In."
    }, {
      "heading" : "D.1 Proofs of Theorem 4",
      "text" : "We prove Theorem 4 in the following. In the proof we exploit several identities. The Frobenius norm and matrix trace satisfies that for any matrix A and B,\n‖A−B‖2F = tr [ (A−B)(A−B)T ) ] = tr(AAT ) + tr(BBT )− 2tr(ABT ).\nThe trace is linear function, and thus for any fixed A and B and random matrix Ψ of proper size,\nE [ tr(AΨB) ] = tr [ A(EΨ)B ] where the expectation is taken w.r.t. Ψ. Proof It follows from the definition of the optimal solution W? in (2) that\nXW? = X(XTX + nγId) †XT (XW0 + Ξ)\n= U(Σ2 + nγIρ) −1Σ3VTW0 + U(Σ2 + nγIρ)−1Σ2UTΞ = U [ Iρ − nγ(Σ2 + nγIρ)−1 ] ΣVTW0 + U(Σ 2 + nγIρ) −1Σ2UTΞ\n= XW0 − nγU(Σ2 + nγIρ)−1ΣVTW0 + U(Σ2 + nγIρ)−1Σ2UTΞ.\nSince E[Ξ] = 0 and E[ΞΞT ] = ξ2In, it holds that\nR(W?) = 1 n E ∥∥XW? −XW0∥∥2F\n= 1\nn ∥∥∥− nγ(Σ2 + nγIρ)−1ΣVTW0 + (Σ2 + nγIρ)−1Σ2UTΞ∥∥∥2 F\n= nγ2 ∥∥∥(Σ2 + nγIρ)−1ΣVTW0∥∥∥2\nF + ξ2 n ∥∥∥(Σ2 + nγIρ)−1Σ2∥∥∥2 F .\nThis shows the bias and variance of the optimal solution W?. We then decompose the risk function R ( Wc ) . It follows from the definition of Wc in (3) that\nXWc = X(XTSSTX + nγId) †XTSST (XW0 + Ξ) = UΣ ( ΣUTSSTUΣ + nγId )† Σ ( UTSSTUΣVTW0 + U TSSTΞ )\n= U(UTSSTU + nγΣ−2)−1 [ (UTSSTU + nγΣ−2)ΣVTW0 − nγΣ−1VTW0 + UTSSTΞ ] = XW0 + U(U TSSTU + nγΣ−2)−1 ( − nγΣ−1VTW0 + UTSSTΞ ) .\nSince E[Ξ] = 0 and E[ΞΞT ] = ξ2In, it follows that\nR ( Wc ) = 1 n E ∥∥XWc −XW0∥∥2F\n= 1\nn ∥∥∥− nγ(UTSSTU + nγΣ−2)−1Σ−1VTW0 + (UTSSTU + nγΣ−2)−1UTSSTΞ∥∥∥2 F\n= nγ2 ∥∥∥(UTSSTU + nγΣ−2)−1Σ−1VTW0∥∥∥2\nF + ξ2 n ∥∥∥(UTSSTU + nγΣ−2)−1UTSST∥∥∥2 F .\nThis shows the bias and variance of the approximate solution Wc. We then decompose the risk function R ( Wh ) . It follows from the definition of Wh in (4) that\nXWh −XW0 = X(XTSSTX + nγIn)†XT (XW0 + Ξ)−XW0 = X(XTSSTX + nγId)\n†XTXW0 −XW0 + X(XTSSTX + nγId)†XTΞ = U [ (UTSSTU + nγΣ−2)−1 − I−1ρ ] UTXW0 + U(U TSSTUT + nγΣ−2)†UTΞ\n= U ( Iρ −UTSSTU− nγΣ−2 )( UTSSTU + nγΣ−2 )−1 ΣVTW0\n+ U(UTSSTUT + nγΣ−2)†UTΞ,\nwhere the last equality follows from that A−1−B−1 = B−1(B−A)A−1 for any nonsingular matrices A and B of proper size. Since E[Ξ] = 0 and E[ΞΞT ] = ξ2In, it follows that\nR ( Wh ) = bias2 ( Wh ) + var ( Wh ) where\nbias2 ( Wh ) = 1\nn ∥∥∥(nγΣ−2 + UTSSTU− Iρ)(UTSSTU + nγΣ−2)−1ΣVTW0∥∥∥2 F ,\nvar ( Wh ) = ξ2\nn ∥∥∥(UTSSTU + nγΣ−2)−1∥∥∥2 F .\nThis shows the bias and variance of Wh."
    }, {
      "heading" : "D.2 Proof of Theorem 23",
      "text" : "Proof Assumption 1.1 ensures that (1− η)Iρ UTSSTU (1 + η)Iρ. It follows that (1− η) ( Iρ + nγΣ −2) UTSSTU + nγΣ−2 (1 + η)(Iρ + nγΣ−2). The bias term can be written as\nbias2 ( Wc ) = nγ2 ∥∥(UTSSTU + nγΣ−2)†Σ−1VTW0∥∥2F = nγ2 tr ( WT0 VΣ\n−1[(UTSSTU + nγΣ−2)†]2Σ−1VTW0) ≤ nγ 2 (1−η)2 ∥∥(Iρ + nγΣ−2)−1Σ−1VTW0∥∥2F = nγ 2 (1−η)2 ∥∥(Σ2 + nγIρ)−1ΣVTW0∥∥2F = 1 (1−η)2 bias 2(W?).\nWe can analogously show bias2(Wc) ≥ 1 (1+η)2 bias2(W?). Let B = ( UTSSTU + nγΣ−2 )† UTS ∈ Rρ×s. By Assumption 1.1, it holds that\n(1− η) [( UTSSTU + nγΣ−2 )2]† BBT (1 + η)[(UTSSTU + nγΣ−2)2]†.\nApplying Assumption 1.1 again, we obtain\n(1− η)2 ( Iρ + nγΣ −2)2 (UTSSTU + nγΣ−2)2 (1 + η)2(Iρ + nγΣ−2)2 Note that both sides are nonsingular. Combining the above two equations, we have\n1− η (1 + η)2\n( Iρ + nγΣ −2)−2 BBT 1 + η (1− η)2 ( Iρ + nηΣ −2)−2. Taking the trace of all the terms, we obtain\n1− η (1 + η)2\n≤ ∥∥B∥∥2\nF∥∥(Iρ + nγΣ−2)−1∥∥2F ≤ 1 + η (1− η)2\nThe variance term can be written as\nvar ( Wc ) = ξ2\nn\n∥∥BST∥∥2 F ≤ ξ 2\nn\n∥∥B∥∥2 F ∥∥S∥∥2 2\n≤ ξ 2(1 + η) n(1− η)2 ∥∥(Iρ + nγΣ−2)−1∥∥2F ∥∥S∥∥22 = (1 + η)‖S‖22\n(1− η)2 var(W?).\nThe upper bound the the variance follows from Assumption 1.3."
    }, {
      "heading" : "D.3 Proof of Theorem 24",
      "text" : "Proof Let B = ( UTSSTU + nγΣ−2 )† UTS ∈ Rρ×s. In the proof of Theorem 5 we show that\nvar ( Wc ) = ξ2\nn\n∥∥BST∥∥2 F .\nIf STS ϑns Is, then it holds that\nvar ( Wc ) = ξ2\nn\n∥∥BST∥∥2 F ≥ ϑn\ns\nξ2\nn\n∥∥B∥∥2 F ≥ ϑn\ns 1− η (1 + η)2 var(W?).\nThis established the lower bounds on the variance."
    }, {
      "heading" : "D.4 Proof of Theorem 25",
      "text" : "Proof Theorem 4 shows that\nbias ( Wh ) = γ √ n ∥∥∥∥(Σ−2 + UTSSTU− Iρnγ )(UTSSTU + nγΣ−2)†ΣVTW0 ∥∥∥∥ F\n= γ √ n ∥∥AΣ2B∥∥ F ≤ γ √ n ∥∥AΣ2∥∥ 2 ∥∥B∥∥ F ,\nvar ( Wh ) = ξ2\nn ∥∥∥(UTSSTU + nγΣ−2)†∥∥∥2 F .\nwhere we define\nA = Σ−2 + UTSSTU− Iρ\nnγ , B = Σ−2 ( UTSSTU + nγΣ−2 )† ΣVTW0.\nWe first analyze then bias. It follows from Assumption 1.1 that Σ−2 ( Iρ − η nγ Σ2 ) A Σ−2 ( Iρ + η nγ Σ2 ) . (16)\nSince ( Iρ − ηnγΣ 2 )2 (Iρ + ηnγΣ2)2 (1 + ησ21nγ )2Iρ, it follows that\nA2 Σ−4 ( Iρ + η nγ Σ2 )2 ( 1 + ησ21 nγ )2 Σ−4.\nThus ∥∥AΣ2∥∥2 2 = ∥∥ΣTA2Σ2∥∥ 2 ≤ ( 1 + ησ21 nγ )2 .\nIt follows from Assumption 1.1 that (1 + η)−1 ( Iρ + nγΣ −2)−1 ((1 + η)Iρ + nγΣ−2)−1 ( UTSSTU + nγΣ−2\n)† ((1− η)Iρ + nγΣ−2)−1 (1− η)−1(Iρ + nγΣ−2)−1. Thus\nBTB = WT0 VΣ 3 ( Σ−2(UTSSTU + nγΣ−2)†Σ−2 )2 Σ3VTW0 (17)\n(1− η)−2WT0 VΣ3 ( Σ−2(Iρ + nγΣ−2)−1Σ−2 )2 Σ3VTW0\n= (1− η)−2WT0 VΣ ( Σ2 + nγIρ )−2 ΣVTW0.\nIt follows that\n‖B‖2F = tr ( BTB ) ≤ (1− η)−2 ∥∥(Σ−2 + nγIρ)−1ΣVTW0∥∥2F = bias2(W?)nγ2(1− η)2 . where the last equality follows from the definition of bias(W?). By the definition of A and B, we have\nbias2 ( Wh ) ≤ γ2n ∥∥AΣ2∥∥2 2 ∥∥B∥∥2 F = 1 (1− η)2 ( 1 + ησ21 nγ )2 bias2 ( W? ) .\nTo this end, the upper bound on bias ( Wh ) is established.\nBy the same definition of A and B, we can also show that\nbias ( Wh ) = γ √ n ∥∥AΣ2B∥∥\nF ≥ γ √ n σmin\n( AΣ2 ) ∥∥B∥∥ F .\nAssume that σ2ρ ≥ nγ η . It follows from (16) that\nA2 (ησ2ρ nγ − 1 )2 Σ−4.\nThus\nσ2min ( AΣ2 ) = σmin(Σ 2A2Σ2) ≥ (ησ2ρ nγ − 1 )2 .\nIt follows from (17) that\nBTB (1 + η)−2WT0 VΣ ( Σ2 + nγIρ )−2 ΣVTW0.\nThus\n‖B‖2F = tr ( BTB ) ≥ (1 + η)−2 ∥∥(Σ−2 + nγIρ)−1ΣVTW0∥∥2F = bias2(W?)nγ2(1 + η)2 . In sum, we obtain\nbias2 ( Wh ) ≥ γ2n σ2min ( AΣ2 ) ∥∥B∥∥2 F = (1 + η)−2 (ησ2ρ nγ − 1 )2 bias2 ( W? ) .\nTo this end the lower bound on bias ( Wh ) is established.\nIt follows from Assumption 1.1 that\n(1 + η)−1 ( Iρ + nγΣ −2)−1 (UTSSTU + nγΣ−2)−1 (1− η)−1 (Iρ + nγΣ−2)−1 It follows from Theorem 4 that\nvar ( Wh ) = ξ2\nn ∥∥∥(UTSSTU + nγΣ−2)−1∥∥∥2 F\n∈ 1 1∓ η\nξ2\nn ∥∥∥(Iρ + nγΣ−2)−1∥∥∥2 F\n= 1 1∓ η var ( W? ) .\nThis concludes the proof."
    }, {
      "heading" : "Appendix E. Model Averaging from Optimization Perspective: Proofs",
      "text" : "In Section E.1 we prove Theorem 26. In Section E.2 we prove Theorem 27."
    }, {
      "heading" : "E.1 Proof of Theorem 26",
      "text" : "Proof By Lemma 31, we only need to show ‖(XTX+nγId)1/2(Wc−W?)‖2F ≤ nαβf(W?). In the proof, we define ρ = rank(X) and σ1 ≥ · · · ≥ σρ be the singular values of X.\nIn the proof of Theorem 21 we show that\n(XTX + nγId) 1/2 ( Wci −W? ) = [ (XTX + nγId) −1/2(XTSiSTi X + nγId)(X TX + nγId)\n−1/2]†(Ai + Bi) = C†i ( Ai + Bi ) ,\nwhere\nAi = V(Σ 2 + nγIρ) −1/2ΣUSiSTi Y ⊥, Bi = nγVΣ(Σ 2 + nγIρ) −1/2(UTSiSTi U− Iρ)(Σ2 + nγIρ)−1UTY\nCi = [ (XTX + nγId) 1/2 ]†( XTSiS T i X + nγId )[ (XTX + nγId) 1/2 ]†\n= V(Iρ + nγΣ −2)−1/2(UTSiSTi U + nγΣ −2)(Iρ + nγΣ−2)−1/2VT = VVT + V(Iρ + nγΣ −2)−1/2(UTSiSTi U− Iρ)(Iρ + nγΣ−2)−1/2VT .\nBy Assumption 2.1, we have that Ci ( 1− η σ 2 max\nσ2max+nγ\n) VVT . Since η ≤ 1/2, it follows that\nC†i ( 1+ 2η σ 2 max\nσ2max+nγ\n) VVT . Let C†i = VV T +V∆iV T . It holds that ∆i 2η σ 2 max σ2max+nγ VVT\n2ηβVVT . By definition, Wc = 1g ∑g i=1 W c i . It follows that\n∥∥∥(XTX + nγId)1/2(Wci −W?)∥∥∥ F = ∥∥∥1 g g∑ i=1 C†i (Ai + Bi) ∥∥∥ F\n≤ ∥∥∥1 g g∑ i=1 (Ai + Bi) ∥∥∥ F + ∥∥∥1 g g∑ i=1 V∆iV T (Ai + Bi) ∥∥∥ F\n≤ ∥∥∥1 g g∑ i=1 Ai ∥∥∥ F + ∥∥∥1 g g∑ i=1 Bi ∥∥∥ F + 1 g g∑ i=1 ∥∥∆i∥∥2(∥∥Ai∥∥F + ∥∥Bi∥∥F) ≤ ∥∥∥1 g g∑ i=1 Ai ∥∥∥ F + ∥∥∥1 g g∑ i=1 Bi ∥∥∥ F + 2ηβ 1 g g∑ i=1\n(∥∥Ai∥∥F + ∥∥Bi∥∥F). (18) By Assumption 2.3, we have that\n1\ng g∑ i=1 ∥∥Ai∥∥F = ∥∥(Σ2 + nγId)−1/2Σ∥∥2 · 1g g∑ i=1\n∥∥UTSiSTi Y⊥∥∥F ≤ √\nσ2max σ2max + nγ\n∥∥Y⊥∥∥ F .\nWe apply Assumption 2.1 and follow the proof of Theorem 21 to show that\n∥∥Bi∥∥2F ≤ η2nγ σ2maxσ2max + nγ ∥∥∥(Σ2 + nγId)−1/2UTY∥∥∥2 F .\nIt follows that\n1\ng g∑ i=1 (∥∥Ai∥∥F + ∥∥Bi∥∥F) ≤ max {√ , η }√ σ2max\nσ2max + nγ\n(∥∥Y⊥∥∥ F + √ nγ ∥∥(Σ2 + nγId)−1/2UTY∥∥F)\n≤ max {√ , η }√ β √ 2 ∥∥Y⊥∥∥2\nF + 2nγ ∥∥(Σ2 + nγId)−1/2UTY∥∥2F = max {√ , η }√ β √ 2n f(W?). (19)\nHere the equality follows from Lemma 31. Let S = 1g [S1, · · · ,Sg] ∈ R n×sg. We have that\n1\ng g∑ i=1 Ai = V(Σ 2 + nγId) −1/2ΣUTSSTY⊥\n1\ng g∑ i=1 Bi = nγVΣ(Σ 2 + nγId) −1/2(UTSSTU− Iρ)(Σ2 + nγIρ)−1UTY.\nFollowing the proof Theorem 21 and applying Assumptions 2.1 and 2.2, we have that\n∥∥∥1 g g∑ i=1 Ai ∥∥∥ F + ∥∥∥1 g g∑ i=1 Bi ∥∥∥ F ≤ √√√√2∥∥∥1 g g∑ i=1 Ai ∥∥∥2 F + 2 ∥∥∥1 g g∑ i=1 Bi ∥∥∥2 F\n≤ max {√\ng , η g }√ σ2max σ2max + nγ √ 2n f(W?) = max {√ g , η g }√ β √ 2n f(W?). (20)\nIt follows from (18), (19), and (20) that∥∥∥(XTX + nγId)1/2(Wci −W?)∥∥∥ F\n≤ ∥∥∥1 g g∑ i=1 Ai ∥∥∥ F + ∥∥∥1 g g∑ i=1 Bi ∥∥∥ F + 2ηβ 1 g g∑ i=1 (∥∥Ai∥∥F + ∥∥Bi∥∥F) ≤ [ max {√\ng , η g\n} + 2β ·max { η √ , η2 }]√ β √ 2n f(W?)\n= √ αβn f(W?).\nThis concludes our proof."
    }, {
      "heading" : "E.2 Proof of Theorem 27",
      "text" : "Proof By Lemma 31, we only need to show ∥∥(XTX + nγId)1/2(Wh −W?)∥∥2F ≤ α2β2(−\nnf(W?) + ‖Y‖2F ) .\nIn the proof of Theorem 2 we show that (XTX + nγId) 1/2 ( Whi −W? ) = VAiBiC,\nwhere\nAi = (Σ 2 + nγIρ) −1/2Σ(Iρ −UTSiSTi U)Σ(Σ2 + nγIρ)−1/2, Bi = (Σ 2 + nγIρ) 1/2(ΣUTSiS T i UΣ + nγIρ) −1(Σ2 + nγIρ)1/2,\nC = (Σ2 + nγIρ) −1/2ΣUTY.\nIt follows from Assumption 2.1 that for all i ∈ [g], 1\n1+η (Σ 2 + nγIρ) −1 (ΣUTSiSTi UΣ + nγIρ)−1 11−η (Σ 2 + nγIρ) −1.\nWe let Bi = Iρ + ∆i. Thus − η1+η Iρ ∆i η 1−η Iρ. It follows that\n(XTX + nγId) 1/2 ( Wh −W? ) = 1\ng g∑ i=1 (XTX + nγId) 1/2 ( Whi −W? ) = 1\ng g∑ i=1 VAi(Iρ + ∆i)C = 1 g g∑ i=1 VAiC + 1 g g∑ i=1 VAi∆iC."
    }, {
      "heading" : "It follows that∥∥∥(XTX + nγId)1/2(Wh −W?)∥∥∥",
      "text" : "F ≤ ∥∥∥1 g g∑ i=1 Ai ∥∥∥ 2 ∥∥∥C∥∥∥ F + 1 g g∑ i=1 ∥∥Ai∥∥2∥∥∆i∥∥2∥∥C∥∥F ≤ ∥∥∥1 g g∑ i=1 Ai ∥∥∥ 2 ∥∥∥C∥∥∥ F + η 1− η (1 g g∑ i=1\n∥∥Ai∥∥2)∥∥C∥∥F . (21) Let S = 1g [S1, · · · ,Sg] ∈ R\nn×gs. It follows from the definition of Ai that∥∥Ai∥∥2 = ∥∥∥(Σ2 + nγIρ)−1/2Σ(Iρ −UTSiSTi U)Σ(Σ2 + nγIρ)−1/2∥∥∥2 ≤ η ∥∥∥(Σ2 + nγIρ)−1/2ΣΣ(Σ2 + nγIρ)−1/2∥∥∥ 2 = η σ2max σ2max + nγ = ηβ,\n∥∥∥1 g g∑ i=1 Ai ∥∥∥ 2 = ∥∥∥(Σ2 + nγIρ)−1/2Σ(Iρ −UTSSTU)Σ(Σ2 + nγIρ)−1/2∥∥∥ 2\n≤ η g ∥∥∥(Σ2 + nγIρ)−1/2ΣΣ(Σ2 + nγIρ)−1/2∥∥∥ 2 = η g σ2max σ2max + nγ = ηβ g .\nIt follows from (21) that ∥∥∥(XTX + nγId)1/2(Wh −W?)∥∥∥ F\n≤ (η g + η2 1− η ) β ∥∥C∥∥ F\n≤ (η g + η2 1− η ) β √ −nf(W?) + ‖Y‖2F ,\nwhere the latter inequality follows from the proof of Theorem 22. This concludes the proof."
    }, {
      "heading" : "Appendix F. Model Averaging from Statistical Perspective: Proofs",
      "text" : "In Section F.1 we prove Theorem 28. In Section F.2 we prove Theorem 29"
    }, {
      "heading" : "F.1 Proof of Theorem 28",
      "text" : "Proof The bound on bias ( Wc ) can be shown in the same way as the proof of Theorem 23.\nWe prove the bound on var ( Wc ) in the following. It follows from Assumption 2.1 that\n(1 + η)−1(Iρ + nγΣ−2)−1 (UTSiSTi U + nγΣ−2)† (1− η)−1(Iρ + nγΣ−2)−1\nLet\n(UTSiS T i U + nγΣ −2)† = (Iρ + nγΣ−2)−1/2(Iρ + ∆i)(Iρ + nγΣ−2)−1/2.\nIt holds that\n− η 1 + η Iρ ∆i η 1− η Iρ.\nBy the definition of var(Wc) in Theorem 10, we have that√ var ( Wc\n) =\nξ√ n ∥∥∥∥1g g∑ i=1 (Iρ + nγΣ −2)−1UTSiS T i + 1 g g∑ i=1 (Iρ + nγΣ −2)−1/2∆i(Iρ + nγΣ −2)−1/2UTSiS T i ∥∥∥∥ F\n≤ ξ√ n (∥∥∥(Iρ + nγΣ−2)−1UTSST∥∥∥ F + 1 g g∑ i=1 ∥∥∥(Iρ + nγΣ−2)−1/2∆i(Iρ + nγΣ−2)−1/2UTSiSTi ∥∥∥ F ) ≤ ξ√\nn ∥∥(Iρ + nγΣ−2)−1∥∥F(∥∥UTS∥∥2∥∥S∥∥2 + η1− η 1g g∑ i=1 ∥∥UTSi∥∥2∥∥Si∥∥2) = √ var ( W? )(∥∥UTS∥∥ 2 ∥∥S∥∥ 2 + η 1− η 1 g g∑ i=1\n∥∥UTSi∥∥2∥∥Si∥∥2). Under Assumption 2.1, we have that ‖STi U‖22 ≤ 1 +η and ‖STU‖22 ≤ 1 +\nη g . It follows that√\nvar ( Wc ) var ( W? ) ≤ √1 + η g ∥∥S∥∥ 2 + η √ 1 + η 1− η 1 g g∑ i=1 ∥∥Si∥∥2. Then the theorem follows from Assumption 2.3."
    }, {
      "heading" : "F.2 Proof of Theorem 29",
      "text" : "Proof The bound on var ( Wh ) can be established in the same way as Theorem 25.\nWe prove the bound on bias ( Wh ) in the following. Let\n(UTSiS T i U + nγΣ −2)† = (Iρ + nγΣ−2)−1/2(Iρ + ∆i)(Iρ + nγΣ−2)−1/2.\nUnder Assumption 2.1, we have that ∆i η1−η Iρ. It follows from Theorem 10 that\nbias ( Wh ) = γ √ n ∥∥∥∥1g g∑ i=1 ( Σ−2 + UTSiS T i U− Iρ nγ ) (UTSiS T i U + nγΣ −2)†ΣVTW0 ∥∥∥∥ F\n≤ γ √ n ∥∥∥∥1g g∑ i=1 ( Σ−2 + UTSiS T i U− Iρ nγ ) (Iρ + nγΣ −2)−1ΣVTW0 ∥∥∥∥ F\n+ γ √ n ∥∥∥∥1g g∑ i=1 ( Σ−2 + UTSiS T i U− Iρ nγ ) (Iρ + nγΣ −2)−1/2∆i(Iρ + nγΣ −2)−1/2ΣVTW0 ∥∥∥∥ F\n, γ √ n ( A+B ) ,\nwhere\nA = ∥∥∥1 g g∑ i=1 ( Σ−2 + UTSiS T i U− Iρ nγ )( Iρ + nγΣ −2)−1ΣVTW0∥∥∥ F\n= ∥∥∥(Σ−2 + UTSSTU− Iρ\nnγ\n)( Iρ + nγΣ −2)−1ΣVTW0∥∥∥ F ,\nB = ∥∥∥1 g g∑ i=1 ( Σ−2 + UTSiS T i U− Iρ nγ )( Iρ + nγΣ −2)−1/2∆i(Iρ + nγΣ−2)−1/2ΣVTW0∥∥∥ F\n≤ 1 g g∑ i=1 ∥∥∥(Σ−2 + UTSiSTi U− Iρ nγ )( Iρ + nγΣ −2)−1/2∆i(Iρ + nγΣ−2)−1/2ΣVTW0∥∥∥ F .\nIt follows from Assumption 2.1 that( 1− ησ 2 max\ngnγ\n) Σ−2 Σ−2 + U\nTSSTU− Iρ nγ\n(\n1 + ησ2max gnγ\n) Σ−2,\nand thus ( Σ−2 +\nUTSSTU− Iρ nγ\n)2 (\n1 + ησ2max gnγ\n)2 Σ−4.\nIt follows that A = ∥∥∥(Σ−2 + UTSSTU− Iρ\nnγ\n)( Iρ + nγΣ −2)−1ΣVTW0∥∥∥ F\n≤ (\n1 + ησ2max gnγ )∥∥∥(Σ2 + nγIρ)−1ΣVTW0∥∥∥ F .\nIt follows from Assumption 2.1 that( 1− ησ 2 max\nnγ\n)2 Σ−4\n( Σ−2 + η\nnγ Iρ\n)2 (\n1 + ησ2max nγ\n)2 Σ−4.\nMoreover, Σ−2 ( Iρ + nγΣ −2)−1/2∆i(Iρ + nγΣ−2)−1/2Σ−2 η 1− η Σ−2 ( Iρ + nγΣ −2)−1Σ−2\nIt follows that B ≤ (\n1 + ησ2max nγ ) · 1 g g∑ i=1 ∥∥∥Σ−2(Iρ + nγΣ−2)−1/2∆i(Iρ + nγΣ−2)−1/2Σ−2Σ3VTW0∥∥∥ F\n≤ η 1− η\n( 1 +\nησ2max nγ\n) · ∥∥∥(Σ2 + nγIρ)−1ΣVTW0∥∥∥\nF .\nHence\nbias ( Wh ) ≤ γ √ n ( A+B ) ≤ [ 1 1− η + (η g + η2 1− η )σ2max nγ ] γ √ n ∥∥∥(Σ2 + nγIρ)−1ΣVTW0∥∥∥ F\n= [ 1 1− η + (η g + η2 1− η )σ2max nγ ] bias ( W? ) .\nHere the equality follows from Theorem 4."
    } ],
    "references" : [ {
      "title" : "Sharper bounds for regression and low-rank approximation with regularization",
      "author" : [ "Haim Avron", "Kenneth L. Clarkson", "David P. Woodruff" ],
      "venue" : "arXiv preprint arXiv:1611.03225,",
      "citeRegEx" : "Avron et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Avron et al\\.",
      "year" : 2016
    }, {
      "title" : "Pasting small votes for classification in large databases and on-line",
      "author" : [ "Leo Breiman" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Breiman.,? \\Q1999\\E",
      "shortCiteRegEx" : "Breiman.",
      "year" : 1999
    }, {
      "title" : "Finding frequent items in data streams",
      "author" : [ "Moses Charikar", "Kevin Chen", "Martin Farach-Colton" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Charikar et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Charikar et al\\.",
      "year" : 2004
    }, {
      "title" : "Low rank approximation and regression in input sparsity time",
      "author" : [ "Kenneth L. Clarkson", "David P. Woodruff" ],
      "venue" : "In Annual ACM Symposium on theory of computing (STOC),",
      "citeRegEx" : "Clarkson and Woodruff.,? \\Q2013\\E",
      "shortCiteRegEx" : "Clarkson and Woodruff.",
      "year" : 2013
    }, {
      "title" : "Fast Monte Carlo algorithms for matrices I: Approximating matrix multiplication",
      "author" : [ "Petros Drineas", "Ravi Kannan", "Michael W. Mahoney" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2006
    }, {
      "title" : "Sampling algorithms for `2 regression and applications",
      "author" : [ "Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan" ],
      "venue" : "In Annual ACM-SIAM Symposium on Discrete Algorithm (SODA),",
      "citeRegEx" : "Drineas et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2006
    }, {
      "title" : "Relative-error CUR matrix decompositions",
      "author" : [ "Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2008
    }, {
      "title" : "Faster least squares approximation",
      "author" : [ "Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan", "Tamás Sarlós" ],
      "venue" : "Numerische Mathematik,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2011
    }, {
      "title" : "Fast approximation of matrix coherence and statistical leverage",
      "author" : [ "Petros Drineas", "Malik Magdon-Ismail", "Michael W. Mahoney", "David P. Woodruff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2012
    }, {
      "title" : "The spectral norm error of the naive Nyström extension",
      "author" : [ "Alex Gittens" ],
      "venue" : "arXiv preprint arXiv:1110.5305,",
      "citeRegEx" : "Gittens.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gittens.",
      "year" : 2011
    }, {
      "title" : "Extensions of Lipschitz mappings into a Hilbert space",
      "author" : [ "William B. Johnson", "Joram Lindenstrauss" ],
      "venue" : "Contemporary mathematics,",
      "citeRegEx" : "Johnson and Lindenstrauss.,? \\Q1984\\E",
      "shortCiteRegEx" : "Johnson and Lindenstrauss.",
      "year" : 1984
    }, {
      "title" : "Faster ridge regression via the subsampled randomized Hadamard transform",
      "author" : [ "Yichao Lu", "Paramveer Dhillon", "Dean P Foster", "Lyle Ungar" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Lu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2013
    }, {
      "title" : "A statistical perspective on algorithmic leveraging",
      "author" : [ "Ping Ma", "Michael W Mahoney", "Bin Yu" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Ma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2015
    }, {
      "title" : "Randomized algorithms for matrices and data",
      "author" : [ "Michael W. Mahoney" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Mahoney.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mahoney.",
      "year" : 2011
    }, {
      "title" : "Low-distortion subspace embeddings in inputsparsity time and applications to robust linear regression",
      "author" : [ "Xiangrui Meng", "Michael W Mahoney" ],
      "venue" : "In Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Meng and Mahoney.,? \\Q2013\\E",
      "shortCiteRegEx" : "Meng and Mahoney.",
      "year" : 2013
    }, {
      "title" : "Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings",
      "author" : [ "John Nelson", "Huy L Nguyên" ],
      "venue" : "In IEEE Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Nelson and Nguyên.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nelson and Nguyên.",
      "year" : 2013
    }, {
      "title" : "The power of simple tabulation-based hashing",
      "author" : [ "Mihai Patrascu", "Mikkel Thorup" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Patrascu and Thorup.,? \\Q2012\\E",
      "shortCiteRegEx" : "Patrascu and Thorup.",
      "year" : 2012
    }, {
      "title" : "Fast and scalable polynomial kernels via explicit feature maps",
      "author" : [ "Ninh Pham", "Rasmus Pagh" ],
      "venue" : "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),",
      "citeRegEx" : "Pham and Pagh.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pham and Pagh.",
      "year" : 2013
    }, {
      "title" : "Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares",
      "author" : [ "Mert Pilanci", "Martin J Wainwright" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Pilanci and Wainwright.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pilanci and Wainwright.",
      "year" : 2015
    }, {
      "title" : "A statistical perspective on randomized sketching for ordinary least-squares",
      "author" : [ "Garvesh Raskutti", "Michael W Mahoney" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raskutti and Mahoney.,? \\Q2016\\E",
      "shortCiteRegEx" : "Raskutti and Mahoney.",
      "year" : 2016
    }, {
      "title" : "Random projections for large-scale regression",
      "author" : [ "Gian-Andrea Thanei", "Christina Heinze", "Nicolai Meinshausen" ],
      "venue" : "arXiv preprint arXiv:1701.05325,",
      "citeRegEx" : "Thanei et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Thanei et al\\.",
      "year" : 2017
    }, {
      "title" : "Improved analysis of the subsampled randomized Hadamard transform",
      "author" : [ "Joel A Tropp" ],
      "venue" : "Advances in Adaptive Data Analysis,",
      "citeRegEx" : "Tropp.,? \\Q2011\\E",
      "shortCiteRegEx" : "Tropp.",
      "year" : 2011
    }, {
      "title" : "Large scale kernel learning using block coordinate descent",
      "author" : [ "Stephen Tu", "Rebecca Roelofs", "Shivaram Venkataraman", "Benjamin Recht" ],
      "venue" : "arXiv preprint arXiv:1602.05310,",
      "citeRegEx" : "Tu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2016
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "Roman Vershynin" ],
      "venue" : "arXiv preprint arXiv:1011.3027,",
      "citeRegEx" : "Vershynin.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vershynin.",
      "year" : 2010
    }, {
      "title" : "Sketching meets random projection in the dual: A provable recovery algorithm for big and highdimensional data",
      "author" : [ "Jialei Wang", "Jason D Lee", "Mehrdad Mahdavi", "Mladen Kolar", "Nathan Srebro" ],
      "venue" : "arXiv preprint arXiv:1610.03045,",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "SPSD matrix approximation vis column selection: Theories, algorithms, and extensions",
      "author" : [ "Shusen Wang", "Luo Luo", "Zhihua Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards more efficient SPSD matrix approximation and CUR matrix decomposition",
      "author" : [ "Shusen Wang", "Zhihua Zhang", "Tong Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Computationally feasible near-optimal subset selection for linear regression under measurement constraints",
      "author" : [ "Yining Wang", "Adams Wei Yu", "Aarti Singh" ],
      "venue" : "arXiv preprint arXiv:1601.02068,",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Feature hashing for large scale multitask learning",
      "author" : [ "Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Weinberger et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Weinberger et al\\.",
      "year" : 2009
    }, {
      "title" : "A fast randomized algorithm for the approximation of matrices",
      "author" : [ "Franco Woolfe", "Edo Liberty", "Vladimir Rokhlin", "Mark Tygert" ],
      "venue" : "Applied and Computational Harmonic Analysis,",
      "citeRegEx" : "Woolfe et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Woolfe et al\\.",
      "year" : 2008
    }, {
      "title" : "Implementing randomized matrix algorithms in parallel and distributed environments",
      "author" : [ "Jiyan Yang", "Xiangrui Meng", "Michael W Mahoney" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Yang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Spark: Cluster computing with working",
      "author" : [ "Matei Zaharia", "Mosharaf Chowdhury", "Michael J Franklin", "Scott Shenker", "Ion Stoica" ],
      "venue" : "sets. HotCloud,",
      "citeRegEx" : "Zaharia et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zaharia et al\\.",
      "year" : 2010
    }, {
      "title" : "Communication-efficient algorithms for statistical optimization",
      "author" : [ "Yuchen Zhang", "John C. Duchi", "Martin J. Wainwright" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    }, {
      "title" : "Divide and conquer kernel ridge regression: a distributed algorithm with minimax optimal rates",
      "author" : [ "Yuchen Zhang", "John Duchi", "Martin Wainwright" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguyên, 2013) by solving the sketched LSR problem minw ‖STXw − Sy‖2 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013).",
      "startOffset" : 15,
      "endOffset" : 507
    }, {
      "referenceID" : 3,
      "context" : ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguyên, 2013) by solving the sketched LSR problem minw ‖STXw − Sy‖2 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein.",
      "startOffset" : 15,
      "endOffset" : 583
    }, {
      "referenceID" : 3,
      "context" : ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguyên, 2013) by solving the sketched LSR problem minw ‖STXw − Sy‖2 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein.",
      "startOffset" : 15,
      "endOffset" : 747
    }, {
      "referenceID" : 3,
      "context" : ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguyên, 2013) by solving the sketched LSR problem minw ‖STXw − Sy‖2 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein.",
      "startOffset" : 15,
      "endOffset" : 764
    }, {
      "referenceID" : 3,
      "context" : ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguyên, 2013) by solving the sketched LSR problem minw ‖STXw − Sy‖2 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w̃ be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value ‖Xw̃ − y ∥∥2 2 is at most times worse than ‖Xw? − y ∥∥2 2 . These works also bounded ‖w̃ − w‖2 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al.",
      "startOffset" : 15,
      "endOffset" : 1452
    }, {
      "referenceID" : 3,
      "context" : ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguyên, 2013) by solving the sketched LSR problem minw ‖STXw − Sy‖2 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w̃ be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value ‖Xw̃ − y ∥∥2 2 is at most times worse than ‖Xw? − y ∥∥2 2 . These works also bounded ‖w̃ − w‖2 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al.",
      "startOffset" : 15,
      "endOffset" : 1481
    }, {
      "referenceID" : 3,
      "context" : ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguyên, 2013) by solving the sketched LSR problem minw ‖STXw − Sy‖2 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w̃ be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value ‖Xw̃ − y ∥∥2 2 is at most times worse than ‖Xw? − y ∥∥2 2 . These works also bounded ‖w̃ − w‖2 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al.",
      "startOffset" : 15,
      "endOffset" : 1512
    }, {
      "referenceID" : 3,
      "context" : ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguyên, 2013) by solving the sketched LSR problem minw ‖STXw − Sy‖2 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w̃ be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value ‖Xw̃ − y ∥∥2 2 is at most times worse than ‖Xw? − y ∥∥2 2 . These works also bounded ‖w̃ − w‖2 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al. (2016d) considered statistical properties of sketched LSR like the bias and variance.",
      "startOffset" : 15,
      "endOffset" : 1533
    }, {
      "referenceID" : 3,
      "context" : ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguyên, 2013) by solving the sketched LSR problem minw ‖STXw − Sy‖2 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w̃ be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value ‖Xw̃ − y ∥∥2 2 is at most times worse than ‖Xw? − y ∥∥2 2 . These works also bounded ‖w̃ − w‖2 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al. (2016d) considered statistical properties of sketched LSR like the bias and variance. In particular, Pilanci and Wainwright (2015) showed that sketched LSR has much higher variance than the optimal solution.",
      "startOffset" : 15,
      "endOffset" : 1656
    }, {
      "referenceID" : 18,
      "context" : "Following the convention of Pilanci and Wainwright (2015), Wang et al.",
      "startOffset" : 28,
      "endOffset" : 58
    }, {
      "referenceID" : 18,
      "context" : "Following the convention of Pilanci and Wainwright (2015), Wang et al. (2016a), we call Wc classical sketch and Wh Hessian sketch.",
      "startOffset" : 28,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "Note that classical sketch with uniform sampling and model averaging is essentially bagging (synonym bootstrap aggregating) (Breiman, 1996) (or a variant called pasting (Breiman, 1999)) for ridge regression.",
      "startOffset" : 169,
      "endOffset" : 184
    }, {
      "referenceID" : 32,
      "context" : "The model averaging analyzed in this paper is similar in spirit to the AvgM algorithm of (Zhang et al., 2013).",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 32,
      "context" : "However, our results do not follow from those of (Zhang et al., 2013).",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 22,
      "context" : "For example, the conjugate gradient method satisfies ‖W−W‖F ‖W(0)−W?‖2 F ≤ θ 1; the stochastic block coordinate descent (Tu et al., 2016) satisfies Ef(W )−f(W) f(W(0))−f(W?) ≤ θ t 2.",
      "startOffset" : 120,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguyên, 2013) shares many similarities with our results. However, the theories of sketched LSR developed from the optimization perspective do not obviously extend to MRR, and the statistical analysis of LSR and MRR differ: among other differences, LSR is unbiased while MRR has a nontrivial bias and therefore has a bias-variance tradeoff that must be considered. Lu et al. (2013) has considered a different application of sketching to ridge regression: they assume d n, reduce the number of features in X using sketching, and conduct statistical analysis.",
      "startOffset" : 15,
      "endOffset" : 460
    }, {
      "referenceID" : 32,
      "context" : "Our results clearly indicate that the performance critically depends on the row coherence of X; this dependence is not captured in (Zhang et al., 2013).",
      "startOffset" : 131,
      "endOffset" : 151
    }, {
      "referenceID" : 33,
      "context" : "For similar reasons, our work is different from the divide-and-conquer kernel ridge regression algorithm of (Zhang et al., 2015).",
      "startOffset" : 108,
      "endOffset" : 128
    }, {
      "referenceID" : 17,
      "context" : "Iterative Hessian sketch has been studied by Pilanci and Wainwright (2015), Wang et al.",
      "startOffset" : 45,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "Iterative Hessian sketch has been studied by Pilanci and Wainwright (2015), Wang et al. (2016a). By way of comparison, all the algorithms in this paper are “one-shot” rather than iterative.",
      "startOffset" : 45,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "Upon completion of this paper, we noticed the contemporary works (Avron et al., 2016, Thanei et al., 2017). Avron et al. (2016) studied classical sketch from the optimization perspective, and Thanei et al.",
      "startOffset" : 66,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "Upon completion of this paper, we noticed the contemporary works (Avron et al., 2016, Thanei et al., 2017). Avron et al. (2016) studied classical sketch from the optimization perspective, and Thanei et al. (2017) studied LSR with model averaging.",
      "startOffset" : 66,
      "endOffset" : 213
    }, {
      "referenceID" : 8,
      "context" : "Leverage score sampling sets pi proportional to the (exact or approximate (Drineas et al., 2012)) row leverage scores li of X.",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "Leverage scores can be efficiently approximated by the algorithms of (Drineas et al., 2012).",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "Gaussian projection is also well-known as the prototypical Johnson-Lindenstrauss transform (Johnson and Lindenstrauss, 1984).",
      "startOffset" : 91,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : "In fact, pi can be any convex combination of li ∑n j=1 lj and 1 n (Ma et al., 2015).",
      "startOffset" : 66,
      "endOffset" : 83
    }, {
      "referenceID" : 29,
      "context" : "In practice, the subsampled randomized Fourier transform (SRFT) (Woolfe et al., 2008) is often used in lieu of the SRHT, because the SRFT exists for all values of n, whereas Hn exists only for some values of n.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 31,
      "context" : "1 Prediction Error We tested the prediction performance of sketched ridge regression by implementing classical sketch with model averaging in PySpark (Zaharia et al., 2010).",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "Remark 14 Note that the first two assumptions were identified in (Mahoney, 2011) and are the relevant structural conditions needed to be satisfied to obtain strong results from the optimization perspective.",
      "startOffset" : 65,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "The third condition is new, but Ma et al. (2015), Raskutti and Mahoney (2016) demonstrated that some sort of additional condition is needed to obtain strong results from the statistical perspective.",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "The third condition is new, but Ma et al. (2015), Raskutti and Mahoney (2016) demonstrated that some sort of additional condition is needed to obtain strong results from the statistical perspective.",
      "startOffset" : 32,
      "endOffset" : 78
    }, {
      "referenceID" : 4,
      "context" : "1 can actually be expressed in the form of an approximate matrix multiplication bound (Drineas et al., 2006a). We call it the Subspace Embedding Property since, as first highlighted in Drineas et al. (2006b), this subspace embedding property is the key result to obtain high-quality sketching algorithms for regression and related problems.",
      "startOffset" : 87,
      "endOffset" : 208
    }, {
      "referenceID" : 24,
      "context" : "Lemma 30 (Wang et al. (2016b)) Let U ∈ Rn×ρ be any fixed matrix with orthonormal columns.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 23,
      "context" : "Vershynin (2010) showed that the greatest singular value of the standard Gaussian matrix G ∈ Rn×s is at most √ n + √ s + t with probability at least 1 − 2e−t/2.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 16,
      "context" : "Patrascu and Thorup (2012) showed that the maximal number of balls in any bin is at most n/s+O (√ n/s log n ) with probability at least 1− 1 n , where c = O(1).",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 23,
      "context" : "Vershynin (2010) showed that the smallest singular value of any n×s standard Gaussian matrix G is at least √ n− √ s − t with probability at least 1 − 2e−t/2.",
      "startOffset" : 0,
      "endOffset" : 17
    } ],
    "year" : 2017,
    "abstractText" : "We address the statistical and optimization impacts of using classical sketch versus Hessian sketch to solve approximately the Matrix Ridge Regression (MRR) problem. Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR—namely, it recovers nearly optimal solutions. In contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the “mass” in the responses and the optimal objective value. For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the biases and variances of sketched MRR; these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases this gap. Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the statistical risks incurred by sketching.",
    "creator" : "LaTeX with hyperref package"
  }
}