{
  "name" : "1608.06203.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Computational and Statistical Tradeoffs in Learning to Rank",
    "authors" : [ "Ashish Khetan", "Sewoong Oh" ],
    "emails" : [ "khetan2@illinois.edu", "swoh@illinois.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In classical statistical inference, we are typically interested in characterizing how more data points improve the accuracy, with little restrictions or considerations on computational aspects of solving the inference problem. However, with massive growths of the amount of data available and also the complexity and heterogeneity of the collected data, computational resources, such as time and memory, are major bottlenecks in many modern applications. As a solution, recent advances in learning theory introduce hierarchies of algorithmic solutions, ordered by the respective computational complexity, for several fundamental machine learning applications in (Bousquet and Bottou, 2008; Shalev-Shwartz and Srebro, 2008; Chandrasekaran and Jordan, 2013; Agarwal et al., 2012; Lucic et al., 2015). Guided by sharp analyses on the sample complexity, these approaches provide theoretically sound guidelines that allow the analyst the flexibility to fall back to simpler algorithms to enjoy the full merit of the improved run-time.\nInspired by these advances, we study the time-data tradeoff in learning to rank. In many applications such as election, policy making, polling, and recommendation systems, we want to aggregate individual preferences to produce a global ranking that best represents the collective social preference. Learning to rank is a rank aggregation approach, which assumes that the data comes from a parametric family of choice models, and learns the parameters that determine the global ranking. Traditionally, each revealed preference is assumed to have one of the following three structures. Pairwise comparison, where one item is preferred over another, is common in sports and chess matches. Best-out-of-κ comparison, where one is chosen among a set of κ alternatives, is common in historical purchase data. κ-way comparison, where we observe a linear ordering of a set of κ candidates, is used in some elections and surveys. We will refer to such structures as traditional in\nar X\niv :1\n60 8.\n06 20\n3v 1\n[ cs\n.L G\n] 2\n2 A\nug 2\ncomparisons to modern datasets with non-traditional structures whose behavior change drastically. For such traditional preferences, efficient schemes for learning to rank have been proposed, such as Ford Jr. (1957); Hunter (2004); Hajek et al. (2014); Chen and Suh (2015), which we explain in detail in Section 1.1. However, modern datasets are unstructured and heterogeneous. As Khetan and Oh (2016) show, this can lead to significant increase in the computational complexity, requiring exponential run-time in the size of the problem in the worst case.\nTo alleviate this computational challenge, we propose a hierarchy of estimators which we call generalized rank-breaking, ordered in increasing computational complexity and achieving increasing accuracy. The key idea is to break down the heterogeneous revealed preferences into simpler pieces of ordinal relations, and apply an estimator tailored for those simple structures treating each piece as independent. Several aspects of rank-breaking makes this problem interesting and challenging. A priori, it is not clear which choices of the simple ordinal relations are rich enough to be statistically efficient and yet lead to tractable estimators. Even if we identify which ordinal relations to extract, the ignored correlations among those pieces can lead to an inconsistent estimate, unless we choose carefully which pieces to include and which to omit in the estimation. We further want sharp analysis on the sample complexity, which reveals how computational and statistical efficiencies trade off. We would like to address all these challenges in providing generalized rank-breaking methods.\nProblem formulation. We study the problem of aggregating ordinal data based on users’ preferences that are expressed in the form of partially ordered sets (poset). A poset is a collection of ordinal relations among items. For example, consider a poset {(i6 ≺ {i5, i4}), (i5 ≺ i3), ({i3, i4} ≺ {i1, i2})} over items {i1, . . . , i6}, where (i6 ≺ {i5, i4}) indicates that item i5 and i4 are both preferred over item i6. Such a relation is extracted from, for example, the user giving a 2-star rating to i5 and i4 and a 1-star to i6. Assuming that the revealed preference is consistent, a poset can be represented as a directed acyclic graph (DAG) Gj as below.\nWe assume that each user j is presented with a subset of items Sj , and independently provides her ordinal preference in the form of a poset, where the ordering is drawn from the Plackett-Luce (PL) model. The PL model is a popular choice model from operations research and psychology, used to model how people make choices under uncertainty. It is a special case of random utility models, where each item i is parametrized by a latent true utility θi ∈ R. When offered with Sj , the user samples the perceived utility Ui for each item independently according to Ui = θi + Zi, where Zi’s are i.i.d. noise. In particular, the PL model assumes Zi’s follow the standard Gumbel distribution. The observed poset is a partial observation of the ordering according to this perceived utilities. We discuss possible extensions to general class of random utility models in Section 1.1.\nThe particular choice of the Gumbel distribution has several merits, largely stemming from the fact that the Gumbel distribution has a log-concave pdf and is inherently memoryless. In our analyses, we use the log-concavity to show that our proposed algorithm is a concave maximization (Remark 2.1) and the memoryless property forms the basis of our rank-breaking idea. Precisely, the PL model is statistically equivalent to the following procedure. Consider a ranking as a mapping from a position in the rank to an item, i.e. σj : [|Sj |] → Sj . It can be shown that the PL model is generated by first independently assigning each item i ∈ Sj an unobserved value Yi, exponentially distributed with mean e−θi , and the resulting ranking σj is inversely ordered in Yi’s so that Yσj(1) ≤ Yσj(2) ≤ · · · ≤ Yσj(|Sj |).\nThis inherits the memoryless property of exponential variables, such that P(Y1 < Y2 < Y3) = P(Y1 < {Y2, Y3})P(Y2 < Y3), leading to a simple interpretation of the PL model as sequential choices:\nP(i3 ≺ i2 ≺ i1) = P({i3, i2} ≺ i1)P(i3 ≺ i2) = eθi1 eθi1 + eθi2 + eθi3 × e θi2 eθi2 + eθi3 .\nIn general, we have P[σj ] = ∏|Sj |−1 i=1 (e θ∗ σj(i))/( ∑|Sj | i′=i e θ∗ σj(i ′)). We assume that the true utility θ∗ ∈ Ωb where\nΩb = { θ ∈ Rd ∣∣ ∑ i∈[d] θi = 0, |θi| ≤ b for all i ∈ [d] } . (1)\nNotice that centering of θ ensures its uniqueness as PL model is invariant under shifting of θ. The bound b on θi is written explicitly to capture the dependence in our main results.\nWe denote a set of n users by [n] = {1, . . . , n} and the set of d items by [d]. Let Gj denote the DAG representation of the poset provided by the user j over Sj ⊆ [d] according to the PL model with weights θ∗. The maximum likelihood estimate (MLE) maximizes the sum of all possible rankings that are consistent with the observed Gj for each j:\nθ̂ ∈ arg max θ∈Ωb { n∑ j=1 log ( ∑ σ∈Gj Pθ[σ] )} , (2)\nwhere we slightly abuse the notation Gj to denote the set of all rankings σ that are consistent with the observation. When Gj has a traditional structure as explained earlier in this section, then the optimization is a simple multinomial logit regression, that can be solved efficiently with off-the-shelf convex optimization tools. Hajek et al. (2014) provides full analysis of the statistical complexity of this MLE under traditional structures. For general posets, it can be shown that the above optimization is a concave maximization, using similar techniques as Remark 2.1. However, the summation over rankings in Gj can involve number of terms super exponential in the size |Sj |, in the worst case. This renders MLE intractable and impractical.\nPairwise rank-breaking. A common remedy to this computational blow-up is to use rankbreaking. Rank-breaking traditionally refers to pairwise rank-breaking, where a bag of all the pairwise comparisons is extracted from observations {Gj}j∈[n] and is applied to estimators that are tailored for pairwise comparisons, treating each paired outcome as independent. This is one of the motivations behind the algorithmic advances in the popular topic of learning from pairwise\ncomparisons in (Ford Jr., 1957; Hunter, 2004; Negahban et al., 2014; Shah et al., 2015a; Maystre and Grossglauser, 2015).\nIt is computationally efficient to apply maximum likelihood estimator assuming independent pairwise comparisons, which takes O(d2) operations to evaluate. However, this computational gain comes at the cost of statistical efficiency. Azari Soufiani et al. (2014) showed that if we include all paired comparisons, then the resulting estimate can be statistically inconsistent due to the ignored correlations among the paired orderings, even with infinite samples. In the example from Figure 1, there are 12 paired relations implied by the DAG: (i6 ≺ i5), (i6 ≺ i4), (i6 ≺ i3), . . . , (i3 ≺ i1), (i4 ≺ i1). In order to get a consistent estimate, Azari Soufiani et al. (2014) provide a rule for choosing which pairs to include, and Khetan and Oh (2016) provide an estimator that optimizes how to weigh each of those chosen pairs to get the best finite sample complexity bound. However, such a consistent pairwise rank-breaking results in throwing away many of the ordered relations, resulting in significant loss in accuracy. For example, Including a paired relation from Gj in the example results in a biased estimator. None of the pairwise orderings can be used from Gj , without making the estimator inconsistent as shown in Azari Soufiani et al. (2013). Whether we include all paired comparisons or only a subset of consistent ones, there is a significant loss in accuracy as illustrated in Figure 3. For the precise condition for consistent rank-breaking we refer to (Azari Soufiani et al., 2013, 2014; Khetan and Oh, 2016).\nThe state-of-the-art approaches operate on either one of the two extreme points on the computational and statistical trade-off. The MLE in (2) requires O( ∑ j∈[n] |Sj |!) summations to just evaluate the objective function, in the worst case. On the other hand, the pairwise rank-breaking requires only O(d2) summations, but suffers from significant loss in the sample complexity. Ideally, we would like to give the analyst the flexibility to choose a target computational complexity she is willing to tolerate, and provide an algorithm that achieves the optimal trade-off at the chosen operating point.\nContribution. We introduce a novel generalized rank-breaking that bridges the gap between MLE and pairwise rank-breaking. Our approach allows the user the freedom to choose the level of computational resources to be used, and provides an estimator tailored for the desired complexity. We prove that the proposed estimator is tractable and consistent, and provide an upper bound on the error rate in the finite sample regime. The analysis explicitly characterizes the dependence on the topology of the data. This in turn provides a guideline for designing surveys and experiments in practice, in order to maximize the sample efficiency. We provide numerical experiments confirming the theoretical guarantees."
    }, {
      "heading" : "1.1 Related work",
      "text" : "In classical statistics, one is interested in the tradeoff between the sample size and the accuracy, with little considerations to the computational complexity or time. As more computations are typically required with increasing availability of data, the computational resources are often the bottleneck. Recently, a novel idea known as “algorithmic weakening” has been investigated to overcome such a bottleneck, in which a hierarchy of algorithms is proposed to allow for faster algorithms at the expense of decreased accuracy. When guided by sound theoretical analyses, this idea allows the statistician to achieve the same level of accuracy and save time when more data is available. This is radically different from classical setting where processing more data typically requires more computational time.\nDepending on the application, several algorithmic weakenings have been studied. In the application of supervised learning, Bousquet and Bottou (2008) proposed the idea that weaker approximate optimization algorithms are sufficient for learning when more data is available. Various gradient based algorithms are analyzed that show the time-accuracy-sample tradeoff. In a similar context, Shalev-Shwartz and Srebro (2008) analyze a particular implementation of support vector machine and show that the target accuracy can be achieved faster when more data is available, by running the iterative algorithm for shorter amount of time. In the application of de-noising, Chandrasekaran and Jordan (2013) provide a hierarchy of convex relaxations where constraints are defined by convex geometry with increasing complexity. For unsupervised learning, Lucic et al. (2015) introduce a hierarchy of data representations that provide more representative elements when more data is available at no additional computation. Standard clustering algorithms can be applied to thus generated summary of the data, requiring less computational complexity.\nIn the application of learning to rank, we follow the principle of algorithmic weakening and propose a novel rank-breaking to allow the practitioner to navigate gracefully the time-sample trade off as shown in the figure below. We propose a hierarchy of estimators indexed by M ∈ Z+ indicating how complex the estimator is (defined formally in Section 2). Figure 2 shows the result of a experiment on synthetic datasets on how much time (in seconds) and how many samples are required to achieve a target accuracy. If we are given more samples, then it is possible to achieve the target accuracy, which in this example is MSE≤ 0.3d2 × 10−6, with fewer operations by using a simpler estimator with smaller M . The details of the experiment is explained in Figure 3.\nLearning to rank from the PL model has been studied extensively under the traditional scenario dating back to Zermelo (1929) who first introduced the PL model for pairwise comparisons. Various approaches for estimating the PL weights from traditional samples have been proposed. The problem can be formulated as a convex optimization that can be solved efficiently using the off-the-shelf solvers. However, tailored algorithms for finding the optimal solution have been pro-\nposed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition. Negahban et al. (2014) introduce Rank Centrality, a novel spectral ranking algorithm which formulates a random walk from the given data, and show that the stationary distribution provides accurate estimates of the PL weights. Maystre and Grossglauser (2015) provide a connection between those previous approaches, and give a unified random walk approach that finds the fixed point of the KKT conditions.\nOn the theoretical side, when samples consist of pairwise comparisons, Simons and Yao (1999) first established consistency and asymptotic normality of the maximum likelihood estimate when all teams play against each other. For a broader class of scenarios where we allow for sparse observations, where the number of total comparisons grow linearly in the number of teams, Negahban et al. (2014) show that Rank Centrality achieves optimal sample complexity by comparing it to a lower bound on the minimax rate. For a more general class of traditional observations, including pairwise comparisons, Hajek et al. (2014) provide similar optimal guarantee for the maximum likelihood estimator. Chen and Suh (2015) introduced Spectral MLE that applies Rank Centrality followed by MLE, and showed that the resulting estimate is optimal in L∞ error as well as the previously analyzed L2 error. Shah et al. (2015a) study a new measure of the error induced by the Laplacian of the comparisons graph and prove a sharper upper and lower bounds that match up to a constant factor.\nHowever, in modern applications, the computational complexity of the existing approaches blowup due to the heterogeneity of modern datasets. Although, statistical and computational tradeoffs have been investigated under other popular choice models such as the Mallows models by Betzler et al. (2014) or stochastically transitive models by Shah et al. (2015b), the algorithmic solutions do not apply to random utility models and the analysis techniques do not extend. We provide a novel rank-breaking algorithms and provide finite sample complexity analysis under the PL model. This approach readily generalizes to some RUMs such as the flipped Gumbel distribution. However, it is also known from Azari Soufiani et al. (2014), that for general RUMs there is no consistent rank-breaking, and the proposed approach does not generalize."
    }, {
      "heading" : "2. Generalized rank-breaking",
      "text" : "Given Gj ’s representing the users’ preferences, generalized rank-breaking extracts a set of ordered relations and applies an estimator treating each ordered relation as independent. Concretely, for each Gj , we first extract a maximal ordered partition Pj of Sj that is consistent with Gj . An ordered partition is a partition with a linear ordering among the subsets, e.g. Pj = ({i6} ≺ {i5, i4, i3} ≺ {i2, i1}) for Gj from Figure 1. This is maximal, since we cannot further partition any of the subsets without creating artificial ordered relations that are not present in the original Gj .\nThe extracted ordered partition is represented by a directed hypergraph Gj(Sj , Ej), which we call a rank-breaking graph. Each edge e = (B(e), T (e)) ∈ Ej is a directed hyper edge from a subset of nodes B(e) ⊆ Sj to another subset T (e) ⊆ Sj . The number of edges in Ej is |Pj |−1 where |Pj | is the number of subsets in the partition. For each subset in Pj except for the least preferred subset, there is a corresponding edge whose top-set T (e) is the subset, and the bottom-set B(e) is the set of all items less preferred than T (e). For the example in Figure 1, we have Ej = {e1, e2} where e1 = (B(e1), T (e1)) = ({i6, i5, i4, i3}, {i2, i1}) and e2 = (B(e2), T (e2) = ({i6}, {i5, i4, i3}) extracted\nfrom Gj . Denote the probability that T (e) is preferred over B(e) when T (e) ∪B(e) is offered as\nPθ(e) = Pθ ( B(e) ≺ T (e) ) = ∑ σ∈ΛT (e)\nexp (∑|T (e)|\nc=1 θσ(c) ) ∏|T (e)| u=1 (∑|T (e)| c′=u exp ( θσ(c′) ) + ∑ i∈B(e) exp (θi) ) (3)\nwhich follows from the definition of the PL model, where ΛT (e) is the set of all rankings over T (e). The computational complexity of evaluating this probability is determined by the size of the top-set |T (e)|, as it involves (|T (e)|!) summations.\nWe let the analyst choose the order M ∈ Z+ depending on how much computational resource is available, and only include those edges with |T (e)| ≤M in the following step. We apply the MLE for comparisons over paired subsets, assuming all rank-breaking graphs are independently drawn. Precisely, we propose order-M rank-breaking estimate, which is the solution that maximizes the log-likelihood under the independence assumption:\nθ̂ ∈ arg max θ∈Ωb LRB(θ) , where LRB(θ) = ∑ j∈[n] ∑ e∈Ej :|T (e)|≤M logPθ(e) . (4)\nIn a special case when M = 1, this can be transformed into the traditional pairwise rank-breaking, where (i) this is a concave maximization; (ii) the estimate is (asymptotically) unbiased and consistent as shown in Azari Soufiani et al. (2013, 2014); and (iii) and the finite sample complexity have been analyzed in Khetan and Oh (2016). Although, this order-1 rank-breaking provides a significant gain in computational efficiency, the information contained in higher-order edges are unused, resulting in a significant loss in accuracy.\nWe provide the analyst the freedom to choose the computational complexity he/she is willing to tolerate. However, for general M , it has not been known if the optimization in (4) is tractable and/or if the solution is consistent. Since Pθ(B(e) ≺ T (e)) as explicitly written in (3) is a sum of log-concave functions, it is not clear if the sum is also log-concave. Due to the ignored dependency in the formulation (4), it is not clear if the resulting estimate is consistent. We first establish that it is a concave maximization in Remark 2.1, then prove consistency in Remark 2.2, and provide a sharp analysis of the performance in the finite sample regime, characterizing the trade-off between computation and sample size in Section 4. We use the Random Utility Model (RUM) interpretation of the PL model to prove concavity. We refer to Section 5.1 for a proof.\nRemark 2.1. LRB(θ) is concave in θ ∈ Rd.\nIn order to discuss consistency of the proposed approach, we need to specify how we sample the set of items to be offered Sj and also which partial ordering over Sj is to be observed. Here, we consider a simple but canonical scenario for sampling ordered relations, and show the proposed method is consistent for all non-degenerate cases. However, we study a more general sampling scenario, when we analyze the order-M estimator in the finite sample regime in Section 4.\nFollowing is the canonical sampling scenario. There is a set of ˜̀ integers (m̃1, . . . , m̃˜̀) whose sum is strictly less than d. A new arriving user is presented with all d items and is asked to provide her top m̃1 items as an unordered set, and then the next m̃2 items, and so on. This is sampling from the PL model and observing an ordered partition with (˜̀+ 1) subsets of sizes m̃a’s, and the last subset includes all remaining items. We apply the generalized rank-breaking to get rank-breaking graphs {Gj} with ˜̀edges each, and order-M estimate is computed. We show that this is consistent,\ni.e. asymptotically unbiased in the limit of the number of users n. A proof is provided in Section 5.2.\nRemark 2.2. Under the PL model and the above sampling scenario, the order-M rank-breaking estimate θ̂ in (4) is consistent for all choices of M ≥ mina∈˜̀m̃a.\nFigure 3 (left panel) shows the accuracy-sample tradeoff for increasing computation M on the same data. As predicted by the anlaysis, generalized rank-breaking (GRB) is consistent (Remark 2.2) and the error decays at rate (1/n) (Theorem 4.1), and decreases with increase in M , order of GRB. For comparison, we also plot the error achieved by pairwise rank-breaking (PRB) approach where we include all paired relations derived from data, which we call inconsistent PRB. As predicted by Azari Soufiani et al. (2014), this results in an inconsistent estimate, whose error does not vanish as we increase the sample size. Notice that including all paired comparisons increases bias, but also decreases variance of the estimate. Hence, when sample size is limited and variance is dominating the bias, it is actually beneficial to include those biased paired relations to gain in variance at the cost of increased bias. Theoretical analysis of such a bias-variance tradeoff is outside the scope of this paper, but proposes an interesting research direction. We fix d = 256, ˜̀= 5, m̃a = a for a ∈ {1, 2, 3, 4, 5}, and sample posets from the canonical scenario, except that each user is presented κ = 32 random items. The PL weights are chosen i.i.d. U [−2, 2]. On the right panel, we let m̃a = 3 for all a ∈ [˜̀] and vary ˜̀∈ {1, 2, 4, 8, 16}. We are providing more observations as ˜̀ where |Ej | = ˜̀. The proposed GRB with M = 3 makes the full use of the given observations and achieve decreasing error, whereas for PRB the increased bias dominates the error. For comparisons, we provide the error achieved by an oracle estimator who knows the exact ordering among those items belonging to the top-sets and runs MLE. For example, if ˜̀= 2, the GRB observes an ordering ({i1, i2, i4, i5, . . .} ≺ {i17, i3, i6} ≺ {i9, i2, i11}) whereas the oracle estimator has extra information on the ordering among those top sets, i.e. ({i1, i2, i4, i5, . . .} ≺ i17 ≺ i3 ≺ i6 ≺ i9 ≺ i2 ≺ i11}).\nPerhaps surprisingly, GRB is able to achieve a similar performance without this significant this extra information, unless |Ej | is large. The performance degradation in large |Ej | regime is precisely captured in our main analysis in Theorem 4.1.\nNotations. We use n to denote the number of users providing partial rankings, indexed by j ∈ [n] where [n] = {1, 2, . . . , n}. We use d to denote the number of items, indexed by i ∈ [d]. Given rank-breaking graphs {Gj(Sj , Ej)}j∈[n] extracted from the posets {Gj}, we first define the order M rank-breaking graphs {G(M)j (Sj , E (M) j )}, where E (M) j is a subset of Ej that includes only those edges ej ∈ Ej with |T (ej)| ≤M . This represents those edges that are included in the estimation for a choice of M . For finite sample analysis, the following quantities capture how the error depends on the topology of the data collected. Let κj ≡ |Sj | and `j ≡ |E(M)j |. We index each edge ej in E\n(M) j by a ∈ [`j ] and define mj,a ≡ |T (ej,a)| for the a-th edge of the j-th rank-breaking graph and rj,a ≡ |T (ej,a)|+ |B(ej,a)|. Note that, we use tilde in subscript with mj,a and `j when M is equal to Sj . That is ˜̀j is the number of edges in Ej and m̃j,a is the size of the top-sets in those edges. We\nlet pj ≡ ∑ a∈[`j ]mj,a denote the effective sample size for the observation G (M) j , such that the total\neffective sample size is ∑\nj∈[n] pj . Notice that although we do not explicitly write the dependence on M , all of the above quantities implicitly depend on the choice of M ."
    }, {
      "heading" : "3. Comparison graph",
      "text" : "The analysis of the optimization in (4) shows that, with high probability, LRB(θ) is strictly concave with λ2(H(θ)) ≤ −Cbγ1γ2γ3λ2(L) < 0 for all θ ∈ Ωb (Lemma 5.4), and the gradient is also bounded with ‖∇LRB(θ∗)‖ ≤ C ′bγ −1/2 2 ( ∑ j pj log d)\n1/2 (Lemma 5.3). the quantities γ1, γ2, γ3, and λ2(L), to be defined shortly, represent the topology of the data. This leads to Theorem 4.1:\n‖θ̂ − θ∗‖2 ≤ 2‖∇LRB(θ∗)‖ −λ2(H(θ)) ≤ C ′′b\n√∑ j pj log d\nγ1γ 3/2 2 γ3λ2(L)\n, (5)\nwhere Cb, C ′ b, and C ′′ b are constants that only depend on b, and λ2(H(θ)) is the second largest eigenvalue of a negative semidefinite Hessian matrix H(θ) of LRB(θ). Recall that θ>1 = 0 since we restrict our search in Ωb. Hence, the error depends on λ2(H(θ)) instead of λ1(H(θ)) whose corresponding eigen vector is the all-ones vector. We define a comparison graph H([d], E) as a weighted undirected graph with weights Aii′ = ∑ j∈[n]:i,i′∈Sj pj/(κj(κj − 1)). The corresponding graph Laplacian is defined as:\nL ≡ n∑ j=1 pj κj(κj − 1) ∑ i<i′∈Sj (ei − ei′)(ei − ei′)> . (6)\nIt is immediate that λ1(L) = 0 with 1 as the eigenvector. There are remaining d−1 eigenvalues that sum to Tr(L) = ∑ j pj . The rescaled λ2(L) and λd(L) capture the dependency on the topology:\nα ≡ λ2(L)(d− 1) Tr(L) , β ≡ Tr(L) λd(L)(d− 1) . (7)\nIn an ideal case where the graph is well connected, then the spectral gap of the Laplacian is large. This ensures all eigenvalues are of the same order and α = β = Θ(1), resulting in a smaller error\nrate. The concavity of LRB(θ) also depends on the following quantities. We discuss the role of the topology in Section 4. Note that the quantities defined in this section implicitly depend on the choice of M , which controls the necessary computational power, via the definition of the rankbreaking {Gj,a}. We define the following quantities that control our upper bound. γ1 incorporates asymmetry in probabilities of items being ranked at different positions depending upon their weight θ∗i . It is 1 for b = 0 that is when all the items have same weight, and decreases exponentially with increase in b. γ2 controls the range of the size of the top-set with respect to the size of the bottomset for which the error decays with the rate of 1/(size of the top-set). The dependence in γ3 and ν are due to weakness in the analysis, and ensures that the Hessian matrix is strictly negative definite.\nγ1 ≡ min j,a\n{( rj,a −mj,a\nκj\n)2e2b−2} , γ2 ≡ min\nj,a\n{( rj,a −mj,a\nrj,a\n)2} , and (8)\nγ3 ≡ 1−max j,a\n{ 4e16b\nγ1\nm2j,ar 2 j,aκ 2 j\n(rj,a −mj,a)5\n} , ν ≡ max\nj,a\n{ mj,aκ 2 j\n(rj,a −mj,a)2\n} . (9)"
    }, {
      "heading" : "4. Main Results",
      "text" : "We present main theoretical analyses and numerical simulations confirming the theoretical predictions."
    }, {
      "heading" : "4.1 Upper bound on the achievable error",
      "text" : "We provide an upper bound on the error for the order-M rank-breaking approach, showing the explicit dependence on the topology of the data. We assume each user provides a partial ranking according to his/her ordered partitions. Precisely, we assume that the set of offerings Sj , the number of subsets (˜̀j + 1), and their respective sizes (m̃j,1, . . . , m̃j,˜̀j ) are predetermined. Each user randomly draws a ranking of items from the PL model, and provides the partial ranking of the form ({i6} ≺ {i5, i4, i3} ≺ {i2, i1}) in the example in Figure 1. For a choice of M , the order-M rank-breaking graph is extracted from this data. The following theorem provides an upper bound on the achieved error, and a proof is provided in Section 5.\nTheorem 4.1. Suppose there are n users, d items parametrized by θ∗ ∈ Ωb, and each user j ∈ [n] is presented with a set of offerings Sj ⊆ [d] and provides a partial ordering under the PL model. For a choice of M ∈ Z+, if γ3 > 0 and the effective sample size ∑n j=1 pj is large enough such that\nn∑ j=1 pj ≥ 214e20bν2 (αγ1γ2γ3)2β pmax κmin d log d , (10)\nwhere b ≡ maxi |θ∗i | is the dynamic range, pmax = maxj∈[n] pj, κmin = minj∈[n] κj, α is the (rescaled) spectral gap, β is the (rescaled) spectral radius in (7), and γ1, γ2, γ3, and ν are defined in (8) and (9), then the generalized rank-breaking estimator in (4) achieves\n1√ d ‖θ̂ − θ∗‖ ≤ 40e\n7b\nαγ1γ 3/2 2 γ3\n√ d log d∑n\nj=1 ∑`j a=1mj,a , (11)\nwith probability at least 1 − 3e3d−3. Moreover, for M ≤ 3 the above bound holds with γ3 replaced by one, giving a tighter result.\nNote that the dependence on the choice of M is not explicit in the bound, but rather is implicit in the construction of the comparison graph and the number of effective samples N = ∑ j ∑ a∈[`j ]mj,a. In an ideal case, b = O(1) and mj,a = O(r 1/2 j,a ) for all (j, a) such that γ1, γ2 are finite. further, if the spectral gap is large such that α > 0 and β > 0, then Equation (11) implies that we need the effective sample size to scale as O(d log d), which is only a logarithmic factor larger than the number of parameters. In this ideal case, there exist universal constants C1, C2 such that if mj,a < C1 √ rj,a and rj,a > C2κj for all {j, a}, then the condition γ3 > 0 is met. Further, when rj,a = O(κj,a), maxκj,a/κj′,a′ = O(1), and max pj,a/pj′,a′ = O(1), then condition on the effective sample size is met with ∑ j pj = O(d log d). We believe that dependence in γ3 is weakness of our analysis and there is no dependence as long as mj,a < rj,a."
    }, {
      "heading" : "4.2 Lower bound on computationally unbounded estimators",
      "text" : "Recall that ˜̀j ≡ |Ej |, m̃j,a = |T (ea)| and r̃j,a = |T (ea) ∪ B(ea)| when M = Sj . We prove a fundamental lower bound on the achievable error rate that holds for any unbiased estimator even with no restrictions on the computational complexity. For each (j, a), define ηj,a as\nηj,a = m̃j,a−1∑ u=0 ( 1 r̃j,a − u + u(m̃j,a − u) m̃j,a(r̃j,a − u)2 ) + ∑ u<u′∈[m̃j,a−1]\n2u m̃j,a(r̃j,a − u) m̃j,a − u′ r̃j,a − u′ (12)\n= m̃2j,a/(3r̃j,a) +O(m̃ 3 j,a/r̃ 2 j,a) . (13)\nTheorem 4.2. Let U denote the set of all unbiased estimators of θ∗ that are centered such that θ̂1 = 0, and let µ = maxj∈[n],a∈[˜̀j ]{m̃j,a − ηj,a}. For all b > 0,\ninf θ̂∈U sup θ∗∈Ωb\nE[‖θ̂ − θ∗‖2] ≥ max  (d− 1)2∑n j=1 ∑˜̀ j a=1(m̃j,a − ηj,a) , 1 µ d∑ i=2 1 λi(L)  . (14) The proof relies on the Cramer-Rao bound and is provided in Section 5. Since ηj,a’s are non-\nnegative, the mean squared error is lower bounded by (d−1)2/N , where N = ∑\nj ∑ a∈˜̀j m̃j,a is the\neffective sample size. Comparing it to the upper bound in (11), this is tight up to a logarithmic factor when (a) the topology of the data is well-behaved such that all respective quantities are finite; and (b) there is no limit on the computational power and M can be made as large as we need. The bound in Eq. (14) further gives a tighter lower bound, capturing the dependency in ηj,a’s and λi(L)’s. Considering the first term, ηj,a is larger when m̃j,a is close to r̃j,a, giving a tighter bound. The second term in (14) implies we get a tighter bound when λ2(L) is smaller.\nIn Figure 4 left and middle panel, we compare performance of our algorithm with pairwise breaking, Cramer Rao lower bound and oracle MLE lower bound. We fix d = 512, n = 105, θ∗ chosen i.i.d. uniformly over [−2, 2]. Oracle MLE knows relative ordering of items in all the top-sets T (e) and hence is strictly better than the GRB. We fix ˜̀ = ` = 1 that is r = κ, and vary m . In the left panel, we fix κ = 32 and in the middle panel, we fix κ = 16. Perhaps surprisingly, GRB matches with the oracle MLE which means relative ordering of top-m items among themselves is\nstatistically insignificant when m is sufficiently small in comparison to κ. For κ = 16, as m gets large, the error starts to increase as predicted by our analysis. The reason is that the quantities γ1 and γ2 gets smaller as m increases, and the upper bound increases consequently. In the right panel, we fix m = 4. When κ is small, γ2 is small, and hence error is large; when b is large γ1 is exponentially small, and hence error is significantly large. This is different from learning Mallows models in Ali and Meilă (2012) where peaked distributions are easier to learn, and is related to the fact that we are not only interested in recovering the (ordinal) ranking but also the (cardinal) weight."
    }, {
      "heading" : "4.3 Computational and statistical tradeoff",
      "text" : "For estimators with limited computational power, however, the above lower bound fails to capture the dependency on the allowed computational power. Understanding such fundamental trade-offs is a challenging problem, which has been studied only in a few special cases, e.g. planted clique problem (Deshpande and Montanari, 2015; Meka et al., 2015). This is outside the scope of this paper, and we instead investigate the trade-off achieved by the proposed rank-breaking approach. When we are limited on computational power, Theorem 4.1 implicitly captures this dependence when order-M rank-breaking is used. The dependence is captured indirectly via the resulting rankbreaking {Gj,a}j∈[n],a∈[`j ] and the topology of it. We make this trade-off explicit by considering a simple but canonical example. Suppose θ∗ ∈ Ωb with b = O(1). Each user gives an i.i.d. partial ranking, where all items are offered and the partial ranking is based on an ordered partition with ˜̀ j = b √ 2cd1/4c subsets. The top subset has size m̃j,1 = 1, and the a-th subset has size m̃j,a = a, up\nto a < ˜̀j , in order to ensure that they sum at most to c √ d for sufficiently small positive constant c and the condition on γ3 > 0 is satisfied. The last subset includes all the remaining items in the bottom, ensuring m̃j,˜̀j ≥ d/2 and γ1, γ2 and ν are all finite.\nComputation. For a choice of M such that M ≤ `j − 1, we consider the computational complexity in evaluating the gradient of LRB, which scales as TM = ∑ j∈[n] ∑ a∈[M ](mj,a!)rj,a = O(M ! × dn). Note that we find the MLE by solving a convex optimization problem using first order methods, and detailed analysis of the convergence rate and the complexity of solving general convex optimizations is outside the scope of this paper.\nSample. Under the canonical setting, for M ≤ `j − 1, we have L = M(M + 1)/(2d(d− 1)) ( I−\n11> ) . This complete graph has the largest possible spectral gap, and hence α > 0 and β > 0. Since\nthe effective samples size is ∑\nj,a m̃j,aI{m̃j,a ≤ M} = nM(M + 1)/2, it follows from Theorem 4.1\nthat the (rescaled) root mean squared error is O( √\n(d log d)/(nM2)). In order to achieve a target error rate of ε, we need to choose M = Ω((1/ε) √ (d log d)/n). The resulting trade-off between\nrun-time and sample to achieve root mean squared error ε is T (n) ∝ (d(1/ε) √\n(d log d)/ne)!dn. We show numerical experiment under this canonical setting in Figure 3 (left) with d = 256 and M ∈ {1, 2, 3, 4, 5}, illustrating the trade-off in practice."
    }, {
      "heading" : "4.4 Real-world datasets",
      "text" : "On sushi preferences (Kamishima, 2003) and jester dataset (Goldberg et al., 2001), we improve over pairwise breaking and achieves same performance as the oracle MLE. Full rankings over κ = 10 types of sushi are randomly chosen from d = 100 types of sushi are provided by n = 5000 individuals. As the ground truth θ∗, we use the ML estimate of PL weights over the entire data. In Figure 5, left panel, for each m ∈ {3, 4, 5, 6, 7}, we remove the known ordering among the top-m and bottom-(10 −m) sushi in each set, and run our estimator with one breaking edge between top-m and bottom-(10−m) items. We compare our algorithm with inconsistent pairwise breaking (using optimal choice of parameters from Khetan and Oh (2016)) and the oracle MLE. For m ≤ 6, the proposed rank-breaking performs as well as an oracle who knows the hidden ranking among the top m items. Jester dataset consists of continuous ratings between −10 to +10 of 100 jokes on sets of size κ, 36 ≤ κ ≤ 100, by 24, 983 users. We convert ratings into full rankings. The ground truth θ∗ is computed similarly. For m ∈ {2, 3, 4, 5}, we convert each full ranking into a poset that has ` = bκ/mc partitions of size m, by removing known relative ordering from each partition. Figure 5 compares the three algorithms using all samples (middle panel), and by varying the sample size (right panel) for fixed m = 4. All figures are averaged over 50 instances."
    }, {
      "heading" : "5. Proofs",
      "text" : "We provide the proofs of the main results."
    }, {
      "heading" : "5.1 Proof of Remark 2.1",
      "text" : "Recall that Pθ(B(e) ≺ T (e)) is the probability that an agent ranks the collection of items T (e) above B(e) when offered S = B(e) ∪ T (e). We want to show that Pθ(B(e) ≺ T (e)) is log-concave under the PL model. We prove a slightly general result which works for a family of RUMs in the location\nfamily. Random Utility Models (RUM) are defined as a probabilistic model where there is a realvalued utility parameter θi associated with each items i ∈ S, and an agent independently samples random utilities {Ui}i∈S for each item i with conditional distribution µi(·|θi). Then the ranking is obtained by sorting the items in decreasing order as per the observed random utilities Ui’s. Location family is a subset of RUMs where the shapes of µi’s are fixed and the only parameters are the means of the distributions. For location family, the noisy utilities can be written as Ui = θi + Zi for i.i.d. random variable Zi’s. In particular, it is PL model when Zi’s follow the independent standard Gumbel distribution. We will show that for the location family if the probability density function for each Zi’s is log-concave then logPθ(B(e) ≺ T (e)) is concave. The desired claim follows as the pdf of standard Gumbel distribution is log-concave. We use the following Theorem from Prékopa (1980). A similar technique was used to prove concavity when |T (e)| = 1 in Azari Soufiani et al. (2012).\nLemma 5.1 (Theorem 9 in Prékopa (1980)). Suppose g(θ, Z) is a concave function in R2r, where θ ∈ Rr is fixed and Z is a r−component random vector whose probability distribution is logarithmic concave in Rr, then the function\nh(θ) = P[g(θ, Z) ≥ 0], for θ ∈ Rr (15)\nis logarithmic concave on Rr.\nTo apply the above lemma to get our result, let r = |S|, g(θ, Z) = mini∈T (e){θi + Zi} − maxi′∈B(e){θi′ + Zi′}, and observe that Pθ(B(e) ≺ T (e)) = P(g(θ, Z) ≥ 0) and g(θ, Z) is concave."
    }, {
      "heading" : "5.2 Proof of Remark 2.2",
      "text" : "Define event E(e) ≡ {T (e)∪B(e) items are ranked in bottom r positions when the offer set is [d]}. Define Pθ,[d](B(e) ≺ T (e)|E(e)) be the conditional probability of T (e) items being ranked higher than B(e) items when the offer set is [d], conditioned on the event E(e). Observe that Pθ,[d](B(e) ≺ T (e)|E(e)) is the probability of observing the event B(e) ≺ T (e) under the proposed rank-breaking. First we show that Pθ(e) = Pθ,[d](B(e) ≺ T (e)|E(e)), where Pθ(e) is the probability that T (e) ≺ B(e) when the offer set is {T (e) ∪ B(e)} as defined in (3). This follows from the fact that under PL model for any disjoint set of items {Ci}i∈[`] such that ∪`i=1Ci = [d],\nP ( C` ≺ C`−1 ≺ · · · ≺ C1 ) = P ( C` ≺ C`−1 ) P ( {C`, C`−1} ≺ C`−2 ) · · ·P ( {C`, C`−1, · · · , C2} ≺ C1 ) , (16)\nwhere P(Ci1 ≺ Ci2) is the probability that Ci2 items are ranked higher than Ci1 items when the offer set is S = {Ci1 ∪ Ci2}. Under the given sampling scenario, the comparison graph H([d], E) as defined in section 3 is connected and hence the estimate θ̂, (4) is unique. Therefore, it follows that maximum likelihood estimate θ̂ is consistent. Further, for a general sampling scenario, Theorem 4.1 proves that the estimator is consistent as the error goes to zero in the limit as n increases."
    }, {
      "heading" : "5.3 Proof of Theorem 4.1",
      "text" : "We define few additional notations. p ≡ (1/n) ∑n\nj=1 pj . V (ej,a) ≡ T (ej,a) ∪ B(ej,a) for all j ∈ [n] and a ∈ [`j ]. Note that by definition of rank-breaking edge ej,a, V (ej,a) is a random set of items that are ranked in bottom rj,a positions in a set of Sj items by the user j.\nThe proof sketch is inspired from Khetan and Oh (2016). The main difference and technical challenge is in showing the strict concavity of LRB(θ) when restricted to Ωb. We want to prove an upper bound on ∆ = θ̂− θ∗, where θ̂ is the sample dependent solution of the optimization (4) and θ∗ is the true utility parameter from which the samples are drawn. Since θ̂, θ∗ ∈ Ωb, it follows that ∆1 = 0. Since θ̂ is the maximizer of LRB(θ), we have the following inequality,\nLRB(θ̂)− LRB(θ∗)− 〈∇LRB(θ∗),∆〉 ≥ −〈∇LRB(θ∗),∆〉 ≥ −‖∇LRB(θ∗)‖2‖∆‖2, (17)\nwhere the last inequality uses the Cauchy-Schwartz inequality. By the mean value theorem, there exists a θ = cθ̂ + (1− c)θ∗ for some c ∈ [0, 1] such that θ ∈ Ωb and\nLRB(θ̂)− LRB(θ∗)− 〈∇LRB(θ∗),∆〉 = 1 2 ∆>H(θ)∆ ≤ −1 2 λ2(−H(θ))‖∆‖22, (18)\nwhere λ2(−H(θ)) is the second smallest eigen value of −H(θ). We will show in Lemma 5.4 that −H(θ) is positive semi definite with one eigen value at zero with a corresponding eigen vector 1 = [1, . . . , 1]>. The last inequality follows since ∆>1 = 0. Combining Equations (17) and (18),\n‖∆‖2 ≤ 2‖∇LRB(θ∗)‖2 λ2(−H(θ)) , (19)\nwhere we used the fact that λ2(−H(θ)) > 0 from Lemma 5.4. The following technical lemmas prove that the norm of the gradient is upper bounded by γ\n−1/2 2 e\nb √\n6np log d with high probability and the second smallest eigen value is lower bounded by (1/8) e−6bαγ1γ2γ3(np/(d − 1)). This finishes the proof of Theorem 4.1.\nThe (random) gradient of the log likelihood in (4) can be written as the following, where the randomness is in which items ended up in the top set T (ej,a) and the bottom set B(ej,a):\n∇iLRB(θ) = n∑ j=1 `j∑ a=1 ∑ C⊆Sj ,\n|C|=rj,a−1\nI { V (ej,a) = {C, i} }∂ logPθ(ej,a) ∂θi . (20)\nNote that we are intentionally decomposing each summand as a summation over all C of size rj,a−1, such that we can separate the analysis of the expectation in the following lemma. The random variable I{{C, i} = V (ej,a)} indicates that we only include one term for any given instance of the sample. Note that the event I{{C, i} = V (ej,a)} is equivalent to the event that the {C, i} items are ranked in bottom rj,a positions in the set Sj , that is V (ej,a) items are ranked in bottom rj,a positions in the set Sj .\nLemma 5.2. If the j-th poset is drawn from the PL model with weights θ∗ then for any given C′ ⊆ Sj with |C′| = rj,a,\nE [ I { C′ = V (ej,a) }∂ logPθ∗(ej,a) ∂θ∗i ∣∣∣∣{ej,a′}a′<a] = 0 . (21) First, this lemma implies that E [ I { C′ = V (ej,a) }∂ log Pθ∗ (ej,a) ∂θ∗i ] = 0. Secondly, the above lemma allows us to construct a vector-valued martingale and apply a generalization of Azuma-Hoeffding’s tail bound on the norm to prove the following concentration of measure. This proves the desired bound on the gradient.\nLemma 5.3. If n posets are independently drawn over d items from the PL model with weights θ∗ then with probability at least 1− 2e3d−3,\n‖∇LRB(θ∗)‖ ≤ γ−1/22 e b √ 6np log d , (22)\nwhere γ2 depend on the choice of the rank-breaking and are defined in Section 3.\nWe will prove in (30) that the Hessian matrix H(θ) ∈ Sd with Hii′(θ) = ∂ 2LRB(θ) ∂θi∂θi′ can be\nexpressed as\n−H(θ) = n∑ j=1 `j∑ a=1 ∑ i<i′∈Sj I{(i, i′) ⊆ V (ej,a)} ( ∂2 logPθ(ej,a) ∂θi∂θi′ (ei − ei′)(ei − ei′)> ) . (23)\nIt is easy to see that H(θ)1 = 0. The following lemma proves a lower bound on the second smallest eigenvalue λ2(−H(θ)) in terms of re-scaled spectral gap α of the comparison graph H defined in Section 3.\nLemma 5.4. Under the hypothesis of Theorem 4.1, if the assumptions in Equation (10) are satisfied then with probability at least 1− d−3, the following holds for any θ ∈ Ωb:\nλ2(−H(θ)) ≥ e−6bαγ1γ2γ3\n8\nnp\n(d− 1) , (24)\nand λ1(−H(θ)) = 0 with corresponding eigen vector 1.\nThis finishes the proof of the desired claim."
    }, {
      "heading" : "5.4 Proof of Lemma 5.2",
      "text" : "Recall that ej,a is a random event where randomness is in which items ended up in the top-set T (ej,a) and the bottom-set B(ej,a), and Pθ∗(ej,a) = Pθ∗ [B(ej,a) ≺ T (ej,a)] that is the probability of observing B(ej,a) ≺ T (ej,a) when the offer set is B(ej,a) ∪ T (ej,a) as defined in (3). Define, Pθ∗,Sj [ej,a|V (ej,a) = C′] to be the conditional probability of observing B(ej,a) ≺ T (ej,a), when the offer set is Sj , conditioned on the event that V (ej,a) = C′. Note that we have put subscript Sj in Pθ∗ to specify that the offer set is Sj . Observe that for any set C′ ⊆ Sj , the event {C′ = V (ej,a)} is equivalent to C′ items being ranked in bottom rj,a positions when the offer set is Sj . In other words, it is conditioned on the event that the subset V (ej,a) items are ranked in bottom rj,a positions when the offer set is Sj . It is easy to check that under PL model\nPθ∗,Sj [ej,a|V (ej,a) = C ′] = Pθ∗ [ej,a],\n(see Remark 2.2). Also, by conditioning on any outcome of {ej,a′}a′<a it can be checked that\nPθ∗,Sj [ej,a|V (ej,a) = C ′, {ej,a′}a′<a] = Pθ∗,Sj [ej,a|V (ej,a) = C ′].\nTherefore, we have E [ ∂ logPθ∗ [ ej,a ]\n∂θ∗i ∣∣∣∣V (ej,a) = C′, {ej,a′}a′<a] = E [ ∂ logPθ∗,Sj [ ej,a|V (ej,a) = C′, {ej,a′}a′<a\n] ∂θ∗i ∣∣∣∣V (ej,a) = C′, {ej,a′}a′<a] =\n∑ ej,a:V (ej,a)=C′ {ej,a′}a′<a Pθ∗,Sj [ ej,a ∣∣V (ej,a) = C′, {ej,a′}a′<a] ∂ ∂θ∗i logPθ∗,Sj [ ej,a ∣∣V (ej,a) = C′, {ej,a′}a′<a]\n= ∂\n∂θ∗i ∑ ej,a:V (ej,a)=C′ Pθ∗,Sj [ ej,a ∣∣V (ej,a) = C′] = ∂ ∂θ∗i 1 = 0 ,\nwhere we used {ej,a : V (ej,a) = C′} = {ej,a : V (ej,a) = C′, {ej,a′}a′<a} which follows from the definition of rank-breaking edges ej,a. This proves the desired claim."
    }, {
      "heading" : "5.5 Proof of Lemma 5.3",
      "text" : "We view ∇LRB(θ∗) as the final value of a discrete time vector-valued martingale with values in Rd. Define ∇L(ej,a)RB ∈ Rd as the gradient vector arising out of each rank-breaking edge {ej,a}j∈[n],a∈[`j ] as\n∇iL (ej,a) RB (θ ∗) ≡ ∑ C⊆Sj I { V (ej,a) = {C, i} } ∇i logPθ∗(ej,a) , (25)\nsuch that ∇LRB(θ∗) = ∑ j∈[n] ∑ a∈[`j ]∇L (ej,a) RB . We take ∇L (ej,a) RB as the incremental random vector\nin a martingale of ∑n\nj=1 `j time steps. Let Hj,a denote (the sigma algebra of) the history up to ej,a and define a sequence of random vectors in Rd:\nZj,a ≡ E[∇L (ej,a) RB (θ ∗)|Hj,a] ,\nwith the convention that Z1,1 = E[∇L (ej,a) RB (θ ∗)] = 0 as proved in Lemma 5.2. It also follows from Lemma 5.2 that E[Zj,a+1|Zj,a] = Zj,a for a < `j . Also, from the independence of samples, it follows that E[Zj+1,1|Zj,`j ] = Zj,`j . Applying a generalized version of the vector Azuma-Hoeffding inequality which readily follows from [Theorem 1.8, Hayes (2005)], we have\nP [ ‖∇LRB(θ∗)‖ ≥ δ ] ≤ 2e3 exp ( − δ\n2∑n j=1 ∑`j a=1mj,a2γ −1 2 e 2b\n) , (26)\nwhere we used ‖∇L(ej,a)RB ‖2 ≤ mj,a2γ −1 2 e 2b. Choosing δ = γ−12 e b √ 6np log d gives the desired bound.\nNow we are left to show that ‖∇L(ej,a)RB ‖2 ≤ 2mj,aγ −1 2 e 2b for any θ ∈ Ωb. Recall that σ ∈ ΛT (ej,a) is the set of all full rankings over T (ej,a) items. In rest of the proof, with a slight abuse of notations, we extend each of these ranking σ over T (ej,a) ∪ B(ej,a) items in the following way. Consider any full ranking σ̃ over B(ej,a) items. Then for each σ ∈ ΛT (ej,a), the extension is such that σ(|T (ej,a)| + c) = σ̃(c) for 1 ≤ c ≤ |B(ej,a)|. The choice of ranking σ̃ will have no impact on\nany of the following mathematical expressions. From the definition of Pθ(ej,a) (3), we have, for any i ∈ V (ej,a),\n∂Pθ(ej,a) ∂θi = I{i ∈ T (ej,a)}Pθ(ej,a) (27)\n− ∑\nσ∈ΛT (ej,a)\nexp (∑mj,a c=1 θσ(c) )∏mj,a\nu=1 (∑rj,a c′=u exp ( θσ(c′) ))︸ ︷︷ ︸ ≡Aσ\n(mj,a∑ u′=1 I{σ−1(i) ≥ u′} exp(θi)∑rj,a c′=u′ exp ( θσ(c′) ) )︸ ︷︷ ︸ ≡Bσ,i︸ ︷︷ ︸\n≡Ei\n.\nNote that Aσ, Bσ,i and Ei depend on ej,a. Observe that for any 1 ≤ u′ ≤ mj,a and any σ ∈ ΛT (ej,a),\n∑ i∈V (ej,a) I{σ−1(i) ≥ u′} exp(θi) = rj,a∑ c′=u′ exp ( θσ(c′) ) . (28)\nTherefore, ∑\ni∈V (ej,a)Bσ,i = mj,a. It follows that∑ i∈V (ej,a) Ei = ∑\nσ∈ΛT (ej,a)\nAσ ( ∑ i∈V (ej,a) Bσ,i ) = mj,a ∑ σ∈ΛT (ej,a) Aσ = mj,aPθ(ej,a) , (29)\nwhere the last equality follows from the definition of Pθ(ej,a) (4). Also, since for any i, i′, e(θi−θi′ ) ≤ e2b; for any i, Bσ,i ≤ e2b ∑rj,a k=rj,a−mj,a+1(1/k) ≤ e\n2b(1 + log(rj,a/(rj,a −mj,a + 1))) ≤ γ−12 e2b, where the last inequality follows from the definition of γ2 (8) and the fact that x ≤ √ 1 + log x for all\nx ≥ 1. Therefore, Ei ≤ γ−12 e2b ∑\nσ∈ΛT (ej,a) Aσ = γ\n−1 2 e 2bPθ(ej,a). We have ∂ logPθ(ej,a)/∂θi =\n(1/Pθ(ej,a))∂Pθ(ej,a)/∂θi = I{i ∈ T (ej,a)} − Ei/Pθ(ej,a). Since |T (ej,a)| = mj,a, ‖∇L (ej,a) RB ‖2 ≤ mj,a + ∑ i∈V (ej,a)(Ei/Pθ(ej,a)) 2 ≤ 2mj,aγ−12 e2b, where we used (29) and the fact that γ −1 2 ≥ 1."
    }, {
      "heading" : "5.5.1 Proof of Lemma 5.4",
      "text" : "First, we prove (23). For brevity, remove {j, a} from Pθ(ej,a). From Equations (27) and (29), and |T (ej,a)| = mj,a, we have ∑ i∈V (ej,a) ∂ ∂θi Pθ(e) = mj,aPθ(e)−mj,aPθ(e) = 0. It follows that\n∑ i∈V (ej,a) ( ∂2 logPθ(e) ∂θi′∂θi ) =\n1 Pθ(e) ∂ ∂θi′ ( ∑ i∈V (ej,a) ( ∂Pθ(e) ∂θi )) − 1 (Pθ(e))2 ∂Pθ(e) ∂θi′ ( ∑ i∈V (ej,a) ( ∂Pθ(e) ∂θi )) = 0 . (30)\nSince by definition LRB(θ) = ∑n\nj=1 ∑`j a=1 logPθ(ej,a), and Hii′(θ) = ∂2LRB(θ) ∂θi∂θi′ which is a symmetric\nmatrix, Equation (30) implies that it can be expressed as given in Equation (23). It follows that all-ones is an eigenvector of H(−θ) with the corresponding eigenvalue being zero.\nTo get a lower bound on λ2(−H(θ)), we apply Weyl’s inequality\nλ2(−H(θ)) ≥ λ2(E[−H(θ)])− ‖H(θ)− E[H(θ)]‖ . (31)\nWe will show in (34) that λ2(E[−H(θ)]) ≥ e−6bαγ1γ2γ3(np/(4(d − 1))) and in (51) that ‖H(θ) − E[H(θ)]‖ ≤ 16e4bν √ pmax κmin np β(d−1) log d. Putting these together,\nλ2(−H(θ)) ≥ e−6bαγ1γ2γ3 np\n4(d− 1) − 16e4bν √ pmax κmin\nnp\nβ(d− 1) log d (32)\n≥ e −6bαγ1γ2γ3\n8\nnp\n(d− 1) , (33)\nwhere the last inequality follows from the assumption on nκmin given in (10). To prove a lower bound on λ2(E[−H(θ)]), we claim that for θ ∈ Ωb,\nE [ −H(θ) ] e−6bγ1γ2γ3 n∑ j=1 pj 4κj(κj − 1) ∑ i<i′∈Sj (ei − ei′)(ei − ei′)> (34)\n= e−6bγ1γ2γ3\n4 L ,\nwhere L ∈ Sd is defined in (6). Using λ2(L) = npα/(d − 1) from (7), we have λ2(−H(θ)) ≥ e−6bαγ1γ2γ3(np/(4(d− 1))). To prove (34), notice that\nE[−H(θ)ii′ ] = E [ ∑ j∈[n] ∑ a∈[`j ] I { (i, i′) ⊆ V (ej,a) } ∂2 logPθ(ej,a) ∂θi∂θi′ ] , (35)\nwhen i 6= i′. We will show that for any i 6= i′ ∈ V (ej,a),\n∂2 logPθ(ej,a) ∂θi∂θi′ ≥  e−2bmj,a r2j,a\nif i, i′ ∈ B(ej,a)\n− e 4bm2j,a\n(rj,a−mj,a+1)2 otherwise .\n(36)\nWe need to bound the probability of two items appearing in the bottom-set B(ej,a) and in the top-set T (ej,a).\nLemma 5.5. Consider a ranking σ over a set S ⊆ [d] such that |S| = κ. For any two items i, i′ ∈ S, θ ∈ Ωb, and 1 ≤ `, `1, `2 ≤ κ− 1,\nPθ [ σ−1(i), σ−1(i′) > ` ] ≥ e\n−4b(κ− `)(κ− `− 1) κ(κ− 1)\n( 1− `\nκ\n)2e2b−2 , (37)\nPθ [ σ−1(i) = ` ] ≤ e 6b\nκ− ` , (38)\nPθ [ σ−1(i) = `1, σ −1(i′) = `2 ] ≤ e 10b\n(κ− `1 − 1)(κ− `2) . (39)\nwhere the probability Pθ is with respect to the sampled ranking resulting from PL weights θ ∈ Ωb.\nSubstituting ` = κj − rj,a + mj,a in (37), and `, `1, `2 ≤ κj − rj,a + mj,a in (38) and (39), we have,\nPθ [ (i, i′) ⊆ B(ej,a) ] ≥ e −4b(rj,a −mj,a)2\n4κj(κj − 1)\n(rj,a −mj,a κj )2e2b−2 , (40)\nPθ [ i ∈ T (ej,a), i′ ∈ B(ej,a) ] ≤ mj,a max\n`∈[κj−rj,a+mj,a] P(σ−1(i) = `)\n≤ e 6bmj,a\nrj,a −mj,a , (41) Pθ [ (i, i′) ⊆ T (ej,a) ] ≤ m2j,a max\n`1,`2∈[κj−rj,a+mj,a] P(σ−1(i) = `1, σ−1(i′) = `2)\n≤ e10bm2j,a\n2 (rj,a −mj,a − 1)(rj,a −mj,a) , (42)\nwhere (40) uses rj,a − mj,a − 1 ≥ (rj,a − mj,a)/4, (41) uses Pθ[i ∈ T (ej,a), i′ ∈ B(ej,a)] ≤ Pθ[i ∈ T (ej,a)], and (41)-(42) uses counting on the possible choices. The bound in (42) is smaller than the one in (41) as per our assumption that γ3 > 0.\nUsing Equations (35)-(36) and (40)-(42), and the definitions of γ1, γ2, γ3 from Section 3, we get\nE[−H(θ)ii′ ] ≥∑ j∈[n] ∑ a∈[`j ] {(rj,a −mj,a κj )2e2b−2 ︸ ︷︷ ︸\n≥γ1\n(rj,a −mj,a rj,a )2 ︸ ︷︷ ︸\n≥γ2\ne−6bmj,a 4κj(κj − 1) − e 6bmj,a rj,a −mj,a e4bm2j,a (rj,a −mj,a + 1)2 }\n≥ ∑ j,a γ1γ2e −6bmj,a 4κj(κj − 1) ( 1 − 4e 16b γ1 m2j,ar 2 j,aκ 2 j (rj,a −mj,a)5 )\n︸ ︷︷ ︸ ≥γ3 . (43)\nThis combined with (23) proves the desired claim (34). Further, in Appendix 5.8, we show that if mj,a ≤ 3 for all {j, a} then ∂ 2 log Pθ(ej,a) ∂θi∂θi′\nis non-negative even for i 6= i′ ∈ T (ej,a), and i ∈ T (ej,a), i′ ∈ B(ej,a) as opposed to a negative lower-bound given in (36). Therefore, bound on E[−H(θ)] in (34) can be tightened by a factor of γ3.\nTo prove claim (36), define the following for σ ∈ ΛT (ej,a),\nAσ ≡ exp\n(∑mj,a c=1 θσ(c) )∏mj,a u=1 (∑rj,a c′=u exp ( θσ(c′) )) , Bσ ≡ mj,a∑ u′=1 1∑rj,a c′=u′ exp ( θσ(c′)\n) , Bσ,i ≡\nmj,a∑ u′=1 I{σ−1(i) ≥ u′}∑rj,a c′=u′ exp ( θσ(c′) ) , Cσ ≡ mj,a∑ u′=1 1(∑rj,a c′=u′ exp ( θσ(c′)\n))2 , Cσ,i ≡\nmj,a∑ u′=1 I{σ−1(i) ≥ u′}(∑rj,a c′=u′ exp ( θσ(c′) ))2 , Cσ,i,i′ ≡ mj,a∑ u′=1 I{σ−1(i), σ−1(i′) ≥ u′}(∑rj,a c′=u′ exp ( θσ(c′)\n))2 . (44) First, a few observations about the expression of Aσ. For any σ ∈ ΛT (ej,a) and any i ∈ V (ej,a), θi is in the numerator if and only if i ∈ T (ej,a), since in all the rankings that are consistent with\nthe observation ej,a, T (ej,a) items are ranked in top mj,a positions. For any σ ∈ ΛT (ej,a) and any i ∈ B(ej,a), θi is in all the product terms ∏mj,a u=1 (·) of the denominator, since in all the consistent rankings these items are ranked below mj,a position. For any i ∈ T (ej,a), θi appears in product term corresponding to index u if and only if item i is ranked at position u or lower than u in the ranking σ ∈ ΛT (ej,a). Now, observe that Bσ is defined such that the partial derivative of Aσ with respect to any i ∈ B(ej,a) is −AσBσeθi , and Bσ,i is defined such that the partial derivative of Aσ with respect to any i ∈ T (ej,a) is Aσ − AσBσeθi . Further, observe that −Cσeθi is the partial derivative of Bσ with respect to i ∈ B(ej,a), −Cσ,ieθi is the partial derivative of Bσ,i with respect to i ∈ T (ej,a), and −Cσ,ieθi′ is the partial derivative of Bσ,i with respect to i′ ∈ B(ej,a). −Cσ,i,i′eθi′ is the partial derivative of Bσ,i with respect to i\n′ 6= i ∈ T (ej,a). For ease of notation, we omit subscript (j, a) whenever it is clear from the context. Also, we use∑ σ to denote ∑ σ∈ΛT (ej,a) . With the above defined notations, from (4), we have, Pθ(e) = ∑ σ Aσ. With the above given observations for the notations in (44), first partial derivative of Pθ(e) can be expressed as following:\n∂Pθ(e) ∂θi =\n{∑ σ ( Aσ −AσBσ,ieθi ) if i ∈ T (ej,a)∑\nσ\n( −AσBσeθi ) if i ∈ B(ej,a) .\n(45)\nIt follows that for i 6= i′ ∈ V (ej,a),\n∂2Pθ(e) ∂θi∂θi′\n=  ∑ σ ( (Aσ(Bσ) 2 +AσCσ)e (θi+θi′ ) ) if i, i′ ∈ B(ej,a)∑ σ ( Aσ −AσBσ,i′eθi′ + (AσBσ,iBσ,i′ +AσCσ,i,i′)e(θi+θi′ ) −AσBσ,ieθi ) if i, i′ ∈ T (ej,a)∑\nσ\n( (AσBσBσ,i +AσCσ,i)e (θi+θi′ ) −AσBσeθi′ )\notherwise .\n(46)\nUsing ∂ 2 log Pθ(e) ∂θi∂θi′ = 1Pθ(e) ∂2Pθ(e) ∂θi∂θi′ − 1 (Pθ(e))2 ∂Pθ(e) ∂θi ∂Pθ(e) ∂θi′ , with above derived first and second derivatives, and after following some algebra, we have\n(Pθ(e))2 e(θi+θi′ ) ∂2 logPθ(e) ∂θi∂θi′\n=  ( ∑ σ Aσ)( ∑ σ Aσ(Bσ) 2)− ( ∑ σ AσBσ) 2 + ( ∑ σ Aσ)( ∑ σ AσCσ) if i, i ′ ∈ B(ej,a) ( ∑ σ Aσ)( ∑ σ AσBσ,iBσ,i′ +AσCσ,i,i′)− ( ∑ σ AσBσ,i)( ∑ σ AσBσ,i′) if i, i ′ ∈ T (ej,a) ( ∑ σ Aσ)( ∑ σ AσBσBσ,i +AσCσ,i)− ( ∑ σ AσBσ)( ∑ σ AσBσ,i) otherwise . (47)\nObserve that from Cauchy-Schwartz inequality ( ∑ σ Aσ)( ∑ σ Aσ(Bσ) 2) − ( ∑ σ AσBσ)\n2 ≥ 0. Also, we have e(θi+θi′ )Cσ ≥ e−2b(m/r2) and eθiBσ,i ≤ eθiBσ ≤ e2b(m/(r −m + 1)) for any i ∈ V (ej,a). This proves the desired claim (36). Next we need to upper bound deviation of −H(θ) from its expectation. From (47), we have,∣∣∂2 log Pθ(ej,a) ∂θi∂θi′ ∣∣ ≤ 3e4bm2j,a/(rj,a − mj,a + 1)2 ≤ 3e4bνmj,a/(κj(κj − 1)), where the last inequality\nfollows from the definition of ν (9). Therefore,\n−H(θ) 3e4bν n∑ j=1 `j∑ a=1 ∑ i<i′∈Sj I{(i, i′) ⊆ V (ej,a)} mj,a κj(κj − 1) (ei − ei′)(ei − ei′)> (48)\n3e4bν n∑ j=1 ∑ i<i′∈Sj ∑`j a=1mj,a κj(κj − 1) (ei − ei′)(ei − ei′)> ≡ n∑ j=1 yjLj , (49)\nwhere yj = (3e 4bνpj)/(κj(κj − 1)) and Lj = ∑ i<i′∈Sj (ei − ei′)(ei − ei′)\n> = κjdiag(eSj ) − eSje>Sj for eSj = ∑ i∈Sj ei. Observe that ‖yjLj‖ ≤ (3e 4bνpmax)/κmin. Moreover, L 2 j κjLj , and it follows that\nn∑ j=1 y2jL 2 j 9e8bν2 n∑ j=1 p2j κ2j (κj − 1)2 κjLj 9e8bν2pmax κmin L , (50)\nwhere we used the fact that L = (pj/(κj(κj − 1))) ∑n j=1 Lj , for L defined in (6). Using λd(L) =\nnp/(β(d−1)) from (7), it follows that ‖ ∑n\nj=1 Eθ[y2jY 2j ]‖ ≤ 9e8bν2pmax\nκmin\nnp β(d−1) . By the matrix Bernstien\ninequality, with probability at least 1− d−3,\n‖H(θ)− E[H(θ)]‖ ≤ 12e4bν √ pmax κmin\nnp\nβ(d− 1) log d+\n8e4bνpmax log d\nκmin ≤ 16e4bν √ pmax κmin np β(d− 1) log d , (51)\nwhere the last inequality follows from the assumption on nκmin given in (10)."
    }, {
      "heading" : "5.6 Proof of Lemma 5.5",
      "text" : "Claim (37): Since providing a lower bound on Pθ [ σ−1(i), σ−1(i′) > ` ] for arbitrary θ is challenging, we construct a new set of parameters {θ̃j}j∈[d] from the original θ. These new parameters are constructed such that it is both easy to compute the probability and also provides a lower bound on the original distribution. Define α̃i,i′,`,θ as\nα̃i,i′,`,θ ≡ max `′∈[`] max Ω⊆S\\{i,i′} :|Ω|=κ−`′\n{ exp(θi) + exp(θi′)(∑\nj∈Ω exp(θj) ) /|Ω|\n} , (52)\nand αi,i′,`,θ = ⌈ α̃i,i′,`,θ ⌉ . For ease of notation we remove the subscript from α and α̃. We denote\nthe sum of the weights by W ≡ ∑\nj∈S exp(θj). We define a new set of parameters {θ̃j}j∈S :\nθ̃j =\n{ log(α̃/2) for j = i or i′ ,\n0 otherwise . (53)\nSimilarly define W̃ ≡ ∑\nj∈S exp(θ̃j) = κ− 2 + α̃. We have,\nPθ [ σ−1(i), σ−1(i′) > ` ] =\n∑ j1∈S j1 6=i,i′\n( exp(θj1)\nW\n∑ j2∈S\nj2 6=i,i′,j1\n( exp(θj2)\nW − exp(θj1) · · · ( ∑ j`∈S j` 6=i,i′, j1,··· ,j`−1\nexp(θj`) W − ∑j`−1\nk=j1 exp(θk)\n) · · · ))\n= ∑ j1∈S j1 6=i,i′\n( exp(θj1)\nW − exp(θj1) · · · ∑ j`−1∈S j`−1 6=i,i′, j1,··· ,j`−2\n( exp(θj`−1)\nW − ∑j`−1\nk=j1 exp(θk) ∑ j`∈S j` 6=i,i′, j1,··· ,j`−1\n( exp(θj`)\nW\n) · · · ))\n(54)\nConsider the second-last summation term in the above equation and let Ω` = S \\{i, i′, j1, . . . , j`−2}. Observe that, |Ω`| = κ− ` and from equation (52),\nexp(θi)+exp(θi′ )∑ j∈Ω` exp(θj) ≤ α̃κ−` . We have,\n∑ j`−1∈Ω`\nexp(θj`−1) W − ∑j`−1\nk=j1 exp(θk) = ∑\nj`−1∈Ω`\nexp(θj`−1) W − ∑j`−2\nk=j1 exp(θk)− exp(θj`−1) ≥ ∑ j`−1∈Ω` exp(θj`−1)\nW − ∑j`−2\nk=j1 exp(θk)− (∑ j`−1∈Ω` exp(θj`−1) ) /|Ω`|\n(55)\n=\n∑ j`−1∈Ω` exp(θj`−1)\nexp(θi) + exp(θi′) + ∑ j`−1∈Ω` exp(θj`−1)− (∑ j`−1∈Ω` exp(θj`−1) ) /|Ω`|\n= ( exp(θi) + exp(θi′)∑ j`−1∈Ω` exp(θj`−1) + 1− 1 κ− ` )−1\n≥\n( α̃\nκ− ` + 1− 1 κ− `\n)−1 (56)\n= κ− `\nα̃+ κ− `− 1 = ∑ j`−1∈Ω`\nexp(θ̃j`−1) W̃ − ∑j`−1\nk=j1 exp(θ̃k)\n, (57)\nwhere (55) follows from the Jensen’s inequality and the fact that for any c > 0, 0 < x < c, xc−x is convex in x. Equation (56) follows from the definition of α̃i,i′,`,θ, (52), and the fact that |Ω`| = κ−`. Equation (57) uses the definition of {θ̃j}j∈S .\nConsider {Ω˜̀}2≤˜̀≤`−1, |Ω˜̀| = κ− ˜̀, corresponding to the subsequent summation terms in (54). Observe that\nexp(θi)+exp(θi′ )∑ j∈Ω˜̀ exp(θj) ≤ α/|Ω˜̀|. Therefore, each summation term in equation (54) can be\nlower bounded by the corresponding term where {θj}j∈S is replaced by {θ̃j}j∈S . Hence, we have\nPθ [ σ−1(i), σ−1(i′) > ` ] ≥\n∑ j1∈S j1 6=i,i′\n( exp(θ̃j1)\nW̃ − exp(θ̃j1) · · · ∑ j`−1∈S j`−1 6=i,i′, j1,··· ,j`−2\n( exp(θ̃j`−1)\nW̃ − ∑j`−1\nk=j1 exp(θ̃k) ∑ j`∈S j` 6=i,i′, j1,··· ,j`−1\n( exp(θj`)\nW\n) · · · ))\n≥ e−4b ∑ j1∈S j1 6=i,i′\n( exp(θ̃j1)\nW̃ − exp(θ̃j1) · · · ∑ j`−1∈S j`−1 6=i,i′, j1,··· ,j`−2\n( exp(θ̃j`−1)\nW̃ − ∑j`−1\nk=j1 exp(θ̃k) ∑ j`∈S j` 6=i,i′, j1,··· ,j`−1\n( exp(θ̃j`)\nW̃\n) · · · ))\n= ( e−4b ) P θ̃ [ σ−1(i), σ−1(i′) > ` ] . (58)\nThe second inequality uses exp(θi)W ≥ e −2b/κ and exp(θ̃i) W̃ ≤ e2b/κ. Observe that exp(θ̃j) = 1 for all j 6= i, i′ and exp(θ̃i) + exp(θ̃i′) = α̃ ≤ dα̃e = α ≥ 1. Therefore, we have\nP θ̃\n[ σ−1(i), σ−1(i′) > ` ] = ( κ− 2 ` ) ` ! (κ− 2 + α̃)(κ− 2 + α̃− 1) · · · (κ− 2 + α̃− (`− 1))\n≥ (κ− 2)! (κ− `− 2)!\n1\n(κ+ α− 2)(κ+ α− 3) · · · (κ+ α− (`+ 1))\n≥ (κ− `+ α− 2)(κ− `+ α− 3) · · · (κ− `− 1) (κ+ α− 2)(κ+ α− 3) · · · (κ− 1)\n≥ (κ− `)(κ− `− 1) κ(κ− 1)\n( 1− `\nκ+ 1\n)α−2 . (59)\nClaim (37) follows by combining Equations (58) and (59) and using the fact that α ≤ 2e2b. Claim (38): Define,\nα̃`,θ ≡ min i∈S min `′∈[`] min Ω∈S\\{i}\n:|Ω|=κ−`′+1\n{ exp(θi)(∑\nj∈Ω exp(θj) ) /|Ω|\n} . (60)\nAlso, define α`,θ ≡ bα̃`,θc. Note that α`,θ ≥ 0 and α̃`,θ ≤ e2b. We denote the sum of the weights by W ≡ ∑ j∈S exp(θj). Analogous to the proof of claim (37), we define the new set of parameters {θ̃j}j∈S :\nθ̃j =\n{ log(α̃`,θ) for j = i ,\n0 otherwise . (61)\nSimilarly define W̃ ≡ ∑\nj∈S exp(θ̃j) = κ − 1 + α̃`,θ. Using the techniques similar to the ones used in proof of claim (37), we have,\nPθ [ σ−1(i) = ` ] ≤ e4bP\nθ̃\n[ σ−1(i) = ` ] . (62)\nObserve that exp(θ̃j) = 1 for all j 6= i and exp(θ̃i) = α̃`,θ ≥ bα̃`,θc = α`,θ ≥ 0. Therefore, we have\nP θ̃\n[ σ−1(i) = ` ] = ( κ− 1 `− 1 ) α̃`,θ(`− 1)! (κ− 1 + α̃`,θ)(κ− 2 + α̃`,θ) · · · (κ− `+ α̃`,θ)\n≤ (κ− 1)! (κ− `)!\ne2b\n(κ− 1 + α`,θ)(κ− 2 + α`,θ) · · · (κ− `+ α`,θ)\n≤ e 2b\nκ\n( 1− `\nκ+ α`,θ\n)α`,θ−1 ≤ e 2b\nκ− ` . (63)\nClaim 38 follows by combining Equations (62) and (63). Claim (39): Again, we construct a new set of parameters {θ̃j}j∈[d] from the original θ using α̃`,θ defined in (60):\nθ̃j =\n{ log(α̃`,θ) for j ∈ {i, i′} ,\n0 otherwise . (64)\nSimilarly define W̃ ≡ ∑\nj∈S exp(θ̃j) = κ− 2 + 2α̃`,θ. Using the techniques similar to the ones used in proof of claim (37), we have,\nPθ [ σ−1(i) = `1, σ −1(i′) = `2 ] ≤ e8bP\nθ̃\n[ σ−1(i) = `1, σ −1(i′) = `2 ] (65)\nObserve that exp(θ̃j) = 1 for all j 6= i, i′ and exp(θ̃i) = exp(θ̃i′) = α̃`,θ ≥ bα̃c`,θ = α`,θ ≥ 0. Therefore, we have\n= P θ̃\n[ σ−1(i) = `1, σ −1(i′) = `2 ] = ( ( κ−2 `2−2 ) α̃2`,θ(`2 − 2)!\n(κ− 2 + 2α̃`,θ)(κ− 1 + 2α̃`,θ) · · · (κ− 2 + 2α̃`,θ − (`1 − 1))\n1\n(κ− 2 + α̃`,θ − (`1 − 1)) · · · (κ− 2 + α̃`,θ − (`2 − 2))\n)\n≤ (κ− 2)! (κ− `2)!\ne4b\n(κ− 2)(κ− 1) · · · (κ− `1 − 1)(κ− `1 − 1) · · · (κ− `2)\n≤ e 4b\n(κ− `1 − 1)(κ− `2) . (66)\nClaim 39 follows by combining Equations (65) and (66)."
    }, {
      "heading" : "5.7 Proof of Theorem 4.2",
      "text" : "Let H(θ) ∈ Sd be Hessian matrix such that Hii′(θ) = ∂ 2LRB(θ) ∂θi∂θi′ . The Fisher information matrix is defined as I(θ) = −Eθ[H(θ)]. From lemma 2.1, LRB(θ) is concave. This implies that I(θ) is positivesemidefinite and from (23) its smallest eigenvalue is zero with all-ones being the corresponding eigenvector. Fix any unbiased estimator θ̂ of θ ∈ Ωb. Since, θ̂ ∈ U , θ̂ − θ is orthogonal to 1. The\nCramer-Rao lower bound then implies that E[‖θ̂ − θ∗‖2] ≥ ∑d\ni=2 1 λi(I(θ)) . Taking supremum over\nboth sides gives\nsup θ E[‖θ̂ − θ∗‖2] ≥ sup θ d∑ i=2\n1\nλi(I(θ)) ≥ d∑ i=2\n1\nλi(I(0)) . (67)\nIn the following, we will show that\nI(0) = −Eθ[H(0)] n∑ j=1 `j∑ a=1 mj,a − ηj,a κj(κj − 1) ∑ i<i′∈Sj (ei − ei′)(ei − ei′)> (68)\nmax j,a\n{ mj,a − ηj,a } L . (69)\nUsing Jensen’s inequality, we have ∑d\ni=2 1 λi(I(0)) ≥ (d−1) 2∑d i=2 λi(I(0)) = (d−1) 2 Tr(I(0)) . From (68), we have\nTr(I(0)) ≤ ∑ j,a(mj,a−ηj,a). From (69), we have ∑d i=2 1/λi(I(0)) ≥ (1/max{mj,a−ηj,a}) ∑d\ni=1 1/λi(L) . This proves the desired claim.\nNow we are left to show claim (68). Consider a rank-breaking edge ej,a. Using notations defined in lemma 5.4, in particular Equation (44), and omitting subscript {j, a} whenever it is clear from the context, we have, for any i ∈ V (ej,a),\n∂2Pθ(ej,a) ∂2θi =\n{∑ σ ( −AσBσeθi +Aσ(Bσ)2e2θi +AσCσeθi ) if i ∈ B(ej,a)∑\nσ\n( Aσ − 3AσBσ,ieθi +AσCσ,i)e2θi +Aσ(Bσ,i)2e2θi ) if i ∈ T (ej,a) ,\n(70)\nand using (45), we have\n∂2 logPθ(ej,a) ∂2θi ∣∣∣ θ=0 = {( (Cσ −Bσ) ) θ=0 if i ∈ B(ej,a)( 1\nmj,a!\n∑ σ ( Cσ,i −Bσ,i + (Bσ,i)2 ) − (∑ σ Bσ,i mj,a! )2) θ=0 if i ∈ T (ej,a) , (71)\nwhere σ ∈ ΛT (ej,a) and the subscript θ = 0 indicates the the respective quantities are evaluated at θ = 0. From the definitions given in (44), for θ = 0, we have Bσ − Cσ = ∑m−1 u=0 (r−u−1) (r−u)2\nand, ∑\nσ(Bσ,i − Cσ,i)/(m!) = 1 m ∑m−1 u=0 (m−u)(r−u−1) (r−u)2 . Also, ∑ σ Bσ,i/(m!) = 1 m ∑m−1 u=0\nm−u r−u and∑\nσ(Bσ,i) 2/(m!) = 1m ∑m−1 u=0 (∑u u′=0 1 r−u′ )2 . Combining all these and, using Pθ=0[i ∈ T (ej,a)] = m/κ and Pθ=0[i ∈ B(ej,a)] = (r −m)/κ, and after following some algebra, we have for any i ∈ Sj ,\n−E [ ∂2 logPθ(ej,a)\n∂2θi\n∣∣∣ θ=0 ] = 1\nκ\n( m− m−1∑ u=0 1 r − u − 1 m m−1∑ u=0 u(m− u) (r − u)2 − 1 m m−2∑ u=0 2u r − u (m−1∑ u′>u m− u′ r − u′ ))\n= mj,a − ηj,a\nκj , (72)\nwhere ηj,a is defined in (12). Since row-sums of H(θ) are zeroes, (23), and for θ = 0, all the items are exchangeable, we have for any i 6= i′ ∈ Sj ,\nE [ ∂2 logPθ(ej,a)\n∂θi∂θi′\n∣∣∣ θ=0 ] = mj,a − ηj,a κj(κj − 1) , (73)\nThe claim (68) follows from the expression of H(θ), Equation (23). To verify (72), observe that (r −m)(Bσ − Cσ) +m( ∑ σ Bσ,i/(m!)) = m− ∑m−1 u=0 1 r−u . And,\n1\nm (m−1∑ u=0 m− u r − u )2 − m−1∑ u=0 ( u∑ u′=0 1 r − u′ )2\n= m−1∑ u=0 ( (m− u)2 m(r − u)2 − m− u (r − u)2 ) + ∑ 0≤u<u′≤m−1 ( 2(m− u)(m− u′) m(r − u)(r − u′) − 2(m− u ′) (r − u)(r − u′) )\n= m−1∑ u=0 −u(m− u) m(r − u)2 + ∑ 0≤u<u′≤m−1 −2u(m− u′) m(r − u)(r − u′) ."
    }, {
      "heading" : "5.8 Tightening of Lemma 5.4",
      "text" : "Recall that Pθ(ej,a) is same as probability of Pθ[T (ej,a) B(ej,a)] that is the probability that an agent ranks T (ej,a) items above B(ej,a) items when provided with a set comprising V (ej,a) items. As earlier, for brevity of notations, we omit subscript {j, a} whenever it is clear from the context. For m = 1 or 2, it is easy to check that all off-diagonal elements in hessian matrix of logPθ(e) are non-negative. However, since number of terms in summation in Pθ(e) grows as m!, for m ≥ 3 the straight-forward approach becomes too complex. Below, we derive expressions for cross-derivatives in hessian, for general m, using alternate definition (sorting of independent exponential r.v.’s in increasing order) of PL model, where the number of terms grow only as 2m. However, we are unable to analytically prove that the cross-derivatives are non-negative for m > 2. Feeding these expressions in MATLAB and using symbolic computation, for m = 3, we can simplify these expressions and it turns out that they are sum of only positive numbers. For m = 4, with limited computational power it becomes intractable. We believe that it should hold for any value of m < r. Using (36), we need to check only for cross-derivatives for the case when i 6= i′ ∈ T (ej,a) or i ∈ T (ej,a), i′ ∈ B(ej,a). Since, minimum of exponential random variables is exponential, we can assume that |B(ej,a)| = 1 that is r = m + 1. Define λi ≡ eθi . Without loss of generality, assume T (ej,a) = {2, · · · ,m + 1} and B(ej,a) = {1}. Define Cx = ∏m+1 i=3 (1 − e−λix). Then, using\nthe alternate definition of the PL model, we have, Pθ(e) = ∫∞ 0 Cx(1− e −λ2x)λ1e −λ1xdx. Following some algebra, ∂ 2 log Pθ(e) ∂θ1∂θ2\n≥ 0 is equivalent to A1 ≥ 0, where A1 ≡(∫ Cx ( xe−λ1x − xe−λx ) dx )(∫ Cxxe −λxdx ) − (∫ Cx(e λ1x − e−λx)dx )(∫ Cxx 2e−λxdx ) ,\nwhere all integrals are from 0 to ∞ and, λ ≡ λ1 + λ2. Consider A1 as a function of λ1. Since A1(λ1) = 0 for λ1 = λ, showing ∂A1/∂λ1 ≤ 0 for 0 ≤ λ1 ≤ λ would suffice. Following some algebra, and using λ1 ≤ λ, ∂A1/∂λ1 ≤ 0 is equivalent to A2(λ1) ≡ ( ∫∞ 0 Cxxe −λ1x ) / ( ∫∞ 0 Cxx 2e−λ1x ) being monotonically non-decreasing in λ1. To further simplify the condition, define f (0)(y) = 1/y2, g(0)(y) = 1/y3 and, f (1)(y) = f (0)(y) − f (0)(y + λ3), and recursively f (m−1)(y) = f (m−2)(y) − f (m−2)(y + λm+1). Similarly define g (0), · · · , g(m−1). Using these recursively defined functions,\n2A2(λ1) = f (m−1)(λ1)\ng(m−1)(λ1) ,\nfor m = 3, 2A2(λ1) = λ−21 − (λ1 + λ3)−2 − (λ1 + λ4)−2 + (λ1 + λ3 + λ4)−2\nλ−31 − (λ1 + λ3)−3 − (λ1 + λ4)−3 + (λ1 + λ3 + λ4)−3 .\nTherefore, we need to show that A2(λ1) is monotonically non-decreasing in λ1 ≥ 0 for any nonnegative λ3, · · · , λm, and that would suffice to prove that the cross-derivatives arising from i ∈ T (ej,a), i\n′ ∈ B(ej,a) are non-negative. For cross-derivatives arising from i 6= i′ ∈ T (ej,a), defineBx = ∏m+1 i=4 (1−eλix)e−λ1x. ∂2 log Pθ(e) ∂θ2∂θ3\n≥ 0 is equivalent to A3 ≥ 0, where A3 ≡(∫\nBx(1− e−λ2x)(1− e−λ3x)dx )(∫ Bxx 2e−(λ2+λ3)xdx ) − (∫ Bx(1− e−λ2x)xe−λ3xdx )(∫ Bx(1− e−λ3x)xe−λ2xdx ) ,\nwhere all integrals are from 0 to ∞. For m = 3, using MATLAB we can show that both types of cross-derivatives are non-negative."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by NSF SaTC award CNS-1527754, and NSF CISE award CCF-1553452."
    } ],
    "references" : [ {
      "title" : "Oracle inequalities for computationally adaptive model selection",
      "author" : [ "A. Agarwal", "P.L. Bartlett", "J.C. Duchi" ],
      "venue" : "arXiv preprint arXiv:1208.0129,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2012
    }, {
      "title" : "Meilă. Experiments with kemeny ranking: What works when",
      "author" : [ "M.A. Ali" ],
      "venue" : "Mathematical Social Sciences,",
      "citeRegEx" : "Ali,? \\Q2012\\E",
      "shortCiteRegEx" : "Ali",
      "year" : 2012
    }, {
      "title" : "Random utility theory for social choice",
      "author" : [ "H. Azari Soufiani", "D.C. Parkes", "L. Xia" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Soufiani et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Soufiani et al\\.",
      "year" : 2012
    }, {
      "title" : "Generalized method-of-moments for rank aggregation",
      "author" : [ "H. Azari Soufiani", "W. Chen", "D. C Parkes", "L. Xia" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Soufiani et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Soufiani et al\\.",
      "year" : 2013
    }, {
      "title" : "Computing parametric ranking models via rank-breaking",
      "author" : [ "H. Azari Soufiani", "D. Parkes", "L. Xia" ],
      "venue" : "In Proceedings of The 31st International Conference on Machine Learning,",
      "citeRegEx" : "Soufiani et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Soufiani et al\\.",
      "year" : 2014
    }, {
      "title" : "Theoretical and empirical evaluation of data reduction for exact kemeny rank aggregation",
      "author" : [ "N. Betzler", "R. Bredereck", "R. Niedermeier" ],
      "venue" : "Autonomous Agents and Multi-Agent Systems,",
      "citeRegEx" : "Betzler et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Betzler et al\\.",
      "year" : 2014
    }, {
      "title" : "The tradeoffs of large scale learning",
      "author" : [ "O. Bousquet", "L. Bottou" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Bousquet and Bottou.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bousquet and Bottou.",
      "year" : 2008
    }, {
      "title" : "Computational and statistical tradeoffs via convex relaxation",
      "author" : [ "V. Chandrasekaran", "M.I. Jordan" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Chandrasekaran and Jordan.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chandrasekaran and Jordan.",
      "year" : 2013
    }, {
      "title" : "Spectral mle: Top-k rank aggregation from pairwise comparisons",
      "author" : [ "Y. Chen", "C. Suh" ],
      "venue" : null,
      "citeRegEx" : "Chen and Suh.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen and Suh.",
      "year" : 2015
    }, {
      "title" : "Improved sum-of-squares lower bounds for hidden clique and hidden submatrix problems",
      "author" : [ "Y. Deshpande", "A. Montanari" ],
      "venue" : "arXiv preprint arXiv:1502.06590,",
      "citeRegEx" : "Deshpande and Montanari.,? \\Q2015\\E",
      "shortCiteRegEx" : "Deshpande and Montanari.",
      "year" : 2015
    }, {
      "title" : "Solution of a ranking problem from binary comparisons",
      "author" : [ "L.R. Ford Jr." ],
      "venue" : "The American Mathematical Monthly,",
      "citeRegEx" : "Jr.,? \\Q1957\\E",
      "shortCiteRegEx" : "Jr.",
      "year" : 1957
    }, {
      "title" : "Eigentaste: A constant time collaborative filtering algorithm",
      "author" : [ "K. Goldberg", "T. Roeder", "D. Gupta", "C. Perkins" ],
      "venue" : "Information Retrieval,",
      "citeRegEx" : "Goldberg et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Goldberg et al\\.",
      "year" : 2001
    }, {
      "title" : "Minimax-optimal inference from partial rankings",
      "author" : [ "B. Hajek", "S. Oh", "J. Xu" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Hajek et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hajek et al\\.",
      "year" : 2014
    }, {
      "title" : "A large-deviation inequality for vector-valued martingales",
      "author" : [ "T.P. Hayes" ],
      "venue" : "Combinatorics, Probability and Computing,",
      "citeRegEx" : "Hayes.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hayes.",
      "year" : 2005
    }, {
      "title" : "Mm algorithms for generalized bradley-terry models",
      "author" : [ "D.R. Hunter" ],
      "venue" : "Ann. of Stat., pages 384–406,",
      "citeRegEx" : "Hunter.,? \\Q2004\\E",
      "shortCiteRegEx" : "Hunter.",
      "year" : 2004
    }, {
      "title" : "Nantonac collaborative filtering: recommendation based on order responses",
      "author" : [ "T. Kamishima" ],
      "venue" : "In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Kamishima.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kamishima.",
      "year" : 2003
    }, {
      "title" : "Data-driven rank breaking for efficient rank aggregation",
      "author" : [ "A. Khetan", "S. Oh" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Khetan and Oh.,? \\Q2016\\E",
      "shortCiteRegEx" : "Khetan and Oh.",
      "year" : 2016
    }, {
      "title" : "Tradeoffs for space, time, data and risk in unsupervised learning",
      "author" : [ "M. Lucic", "M.I. Ohannessian", "A. Karbasi", "A. Krause" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Lucic et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lucic et al\\.",
      "year" : 2015
    }, {
      "title" : "Fast and accurate inference of plackett-luce models",
      "author" : [ "L. Maystre", "M. Grossglauser" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Maystre and Grossglauser.,? \\Q2015\\E",
      "shortCiteRegEx" : "Maystre and Grossglauser.",
      "year" : 2015
    }, {
      "title" : "Sum-of-squares lower bounds for planted clique",
      "author" : [ "R. Meka", "A. Potechin", "A. Wigderson" ],
      "venue" : "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,",
      "citeRegEx" : "Meka et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Meka et al\\.",
      "year" : 2015
    }, {
      "title" : "Rank centrality: Ranking from pair-wise comparisons",
      "author" : [ "S. Negahban", "S. Oh", "D. Shah" ],
      "venue" : null,
      "citeRegEx" : "Negahban et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Negahban et al\\.",
      "year" : 2014
    }, {
      "title" : "Logarithmic concave measures and related topics",
      "author" : [ "A. Prékopa" ],
      "venue" : "In Stochastic programming,",
      "citeRegEx" : "Prékopa.,? \\Q1980\\E",
      "shortCiteRegEx" : "Prékopa.",
      "year" : 1980
    }, {
      "title" : "Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence",
      "author" : [ "N.B. Shah", "S. Balakrishnan", "J. Bradley", "A. Parekh", "K. Ramchandran", "M.J. Wainwright" ],
      "venue" : null,
      "citeRegEx" : "Shah et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2015
    }, {
      "title" : "Stochastically transitive models for pairwise comparisons: Statistical and computational issues",
      "author" : [ "N.B. Shah", "S. Balakrishnan", "A. Guntuboyina", "M.J. Wainright" ],
      "venue" : "arXiv preprint arXiv:1510.05610,",
      "citeRegEx" : "Shah et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2015
    }, {
      "title" : "Svm optimization: inverse dependence on training set size",
      "author" : [ "S. Shalev-Shwartz", "N. Srebro" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Shalev.Shwartz and Srebro.,? \\Q2008\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Srebro.",
      "year" : 2008
    }, {
      "title" : "Asymptotics when the number of parameters tends to infinity in the bradley-terry model for paired comparisons",
      "author" : [ "G. Simons", "Y. Yao" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Simons and Yao.,? \\Q1999\\E",
      "shortCiteRegEx" : "Simons and Yao.",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "As a solution, recent advances in learning theory introduce hierarchies of algorithmic solutions, ordered by the respective computational complexity, for several fundamental machine learning applications in (Bousquet and Bottou, 2008; Shalev-Shwartz and Srebro, 2008; Chandrasekaran and Jordan, 2013; Agarwal et al., 2012; Lucic et al., 2015).",
      "startOffset" : 207,
      "endOffset" : 342
    }, {
      "referenceID" : 24,
      "context" : "As a solution, recent advances in learning theory introduce hierarchies of algorithmic solutions, ordered by the respective computational complexity, for several fundamental machine learning applications in (Bousquet and Bottou, 2008; Shalev-Shwartz and Srebro, 2008; Chandrasekaran and Jordan, 2013; Agarwal et al., 2012; Lucic et al., 2015).",
      "startOffset" : 207,
      "endOffset" : 342
    }, {
      "referenceID" : 7,
      "context" : "As a solution, recent advances in learning theory introduce hierarchies of algorithmic solutions, ordered by the respective computational complexity, for several fundamental machine learning applications in (Bousquet and Bottou, 2008; Shalev-Shwartz and Srebro, 2008; Chandrasekaran and Jordan, 2013; Agarwal et al., 2012; Lucic et al., 2015).",
      "startOffset" : 207,
      "endOffset" : 342
    }, {
      "referenceID" : 0,
      "context" : "As a solution, recent advances in learning theory introduce hierarchies of algorithmic solutions, ordered by the respective computational complexity, for several fundamental machine learning applications in (Bousquet and Bottou, 2008; Shalev-Shwartz and Srebro, 2008; Chandrasekaran and Jordan, 2013; Agarwal et al., 2012; Lucic et al., 2015).",
      "startOffset" : 207,
      "endOffset" : 342
    }, {
      "referenceID" : 17,
      "context" : "As a solution, recent advances in learning theory introduce hierarchies of algorithmic solutions, ordered by the respective computational complexity, for several fundamental machine learning applications in (Bousquet and Bottou, 2008; Shalev-Shwartz and Srebro, 2008; Chandrasekaran and Jordan, 2013; Agarwal et al., 2012; Lucic et al., 2015).",
      "startOffset" : 207,
      "endOffset" : 342
    }, {
      "referenceID" : 9,
      "context" : "For such traditional preferences, efficient schemes for learning to rank have been proposed, such as Ford Jr. (1957); Hunter (2004); Hajek et al.",
      "startOffset" : 106,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "For such traditional preferences, efficient schemes for learning to rank have been proposed, such as Ford Jr. (1957); Hunter (2004); Hajek et al.",
      "startOffset" : 106,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "For such traditional preferences, efficient schemes for learning to rank have been proposed, such as Ford Jr. (1957); Hunter (2004); Hajek et al. (2014); Chen and Suh (2015), which we explain in detail in Section 1.",
      "startOffset" : 106,
      "endOffset" : 153
    }, {
      "referenceID" : 8,
      "context" : "(2014); Chen and Suh (2015), which we explain in detail in Section 1.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "(2014); Chen and Suh (2015), which we explain in detail in Section 1.1. However, modern datasets are unstructured and heterogeneous. As Khetan and Oh (2016) show, this can lead to significant increase in the computational complexity, requiring exponential run-time in the size of the problem in the worst case.",
      "startOffset" : 8,
      "endOffset" : 157
    }, {
      "referenceID" : 12,
      "context" : "Hajek et al. (2014) provides full analysis of the statistical complexity of this MLE under traditional structures.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 14,
      "context" : "comparisons in (Ford Jr., 1957; Hunter, 2004; Negahban et al., 2014; Shah et al., 2015a; Maystre and Grossglauser, 2015).",
      "startOffset" : 15,
      "endOffset" : 120
    }, {
      "referenceID" : 20,
      "context" : "comparisons in (Ford Jr., 1957; Hunter, 2004; Negahban et al., 2014; Shah et al., 2015a; Maystre and Grossglauser, 2015).",
      "startOffset" : 15,
      "endOffset" : 120
    }, {
      "referenceID" : 18,
      "context" : "comparisons in (Ford Jr., 1957; Hunter, 2004; Negahban et al., 2014; Shah et al., 2015a; Maystre and Grossglauser, 2015).",
      "startOffset" : 15,
      "endOffset" : 120
    }, {
      "referenceID" : 16,
      "context" : "For the precise condition for consistent rank-breaking we refer to (Azari Soufiani et al., 2013, 2014; Khetan and Oh, 2016).",
      "startOffset" : 67,
      "endOffset" : 123
    }, {
      "referenceID" : 2,
      "context" : "Azari Soufiani et al. (2014) showed that if we include all paired comparisons, then the resulting estimate can be statistically inconsistent due to the ignored correlations among the paired orderings, even with infinite samples.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "Azari Soufiani et al. (2014) showed that if we include all paired comparisons, then the resulting estimate can be statistically inconsistent due to the ignored correlations among the paired orderings, even with infinite samples. In the example from Figure 1, there are 12 paired relations implied by the DAG: (i6 ≺ i5), (i6 ≺ i4), (i6 ≺ i3), . . . , (i3 ≺ i1), (i4 ≺ i1). In order to get a consistent estimate, Azari Soufiani et al. (2014) provide a rule for choosing which pairs to include, and Khetan and Oh (2016) provide an estimator that optimizes how to weigh each of those chosen pairs to get the best finite sample complexity bound.",
      "startOffset" : 6,
      "endOffset" : 440
    }, {
      "referenceID" : 2,
      "context" : "Azari Soufiani et al. (2014) showed that if we include all paired comparisons, then the resulting estimate can be statistically inconsistent due to the ignored correlations among the paired orderings, even with infinite samples. In the example from Figure 1, there are 12 paired relations implied by the DAG: (i6 ≺ i5), (i6 ≺ i4), (i6 ≺ i3), . . . , (i3 ≺ i1), (i4 ≺ i1). In order to get a consistent estimate, Azari Soufiani et al. (2014) provide a rule for choosing which pairs to include, and Khetan and Oh (2016) provide an estimator that optimizes how to weigh each of those chosen pairs to get the best finite sample complexity bound.",
      "startOffset" : 6,
      "endOffset" : 517
    }, {
      "referenceID" : 2,
      "context" : "Azari Soufiani et al. (2014) showed that if we include all paired comparisons, then the resulting estimate can be statistically inconsistent due to the ignored correlations among the paired orderings, even with infinite samples. In the example from Figure 1, there are 12 paired relations implied by the DAG: (i6 ≺ i5), (i6 ≺ i4), (i6 ≺ i3), . . . , (i3 ≺ i1), (i4 ≺ i1). In order to get a consistent estimate, Azari Soufiani et al. (2014) provide a rule for choosing which pairs to include, and Khetan and Oh (2016) provide an estimator that optimizes how to weigh each of those chosen pairs to get the best finite sample complexity bound. However, such a consistent pairwise rank-breaking results in throwing away many of the ordered relations, resulting in significant loss in accuracy. For example, Including a paired relation from Gj in the example results in a biased estimator. None of the pairwise orderings can be used from Gj , without making the estimator inconsistent as shown in Azari Soufiani et al. (2013). Whether we include all paired comparisons or only a subset of consistent ones, there is a significant loss in accuracy as illustrated in Figure 3.",
      "startOffset" : 6,
      "endOffset" : 1021
    }, {
      "referenceID" : 6,
      "context" : "In the application of supervised learning, Bousquet and Bottou (2008) proposed the idea that weaker approximate optimization algorithms are sufficient for learning when more data is available.",
      "startOffset" : 43,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "In the application of supervised learning, Bousquet and Bottou (2008) proposed the idea that weaker approximate optimization algorithms are sufficient for learning when more data is available. Various gradient based algorithms are analyzed that show the time-accuracy-sample tradeoff. In a similar context, Shalev-Shwartz and Srebro (2008) analyze a particular implementation of support vector machine and show that the target accuracy can be achieved faster when more data is available, by running the iterative algorithm for shorter amount of time.",
      "startOffset" : 43,
      "endOffset" : 340
    }, {
      "referenceID" : 6,
      "context" : "In the application of supervised learning, Bousquet and Bottou (2008) proposed the idea that weaker approximate optimization algorithms are sufficient for learning when more data is available. Various gradient based algorithms are analyzed that show the time-accuracy-sample tradeoff. In a similar context, Shalev-Shwartz and Srebro (2008) analyze a particular implementation of support vector machine and show that the target accuracy can be achieved faster when more data is available, by running the iterative algorithm for shorter amount of time. In the application of de-noising, Chandrasekaran and Jordan (2013) provide a hierarchy of convex relaxations where constraints are defined by convex geometry with increasing complexity.",
      "startOffset" : 43,
      "endOffset" : 618
    }, {
      "referenceID" : 6,
      "context" : "In the application of supervised learning, Bousquet and Bottou (2008) proposed the idea that weaker approximate optimization algorithms are sufficient for learning when more data is available. Various gradient based algorithms are analyzed that show the time-accuracy-sample tradeoff. In a similar context, Shalev-Shwartz and Srebro (2008) analyze a particular implementation of support vector machine and show that the target accuracy can be achieved faster when more data is available, by running the iterative algorithm for shorter amount of time. In the application of de-noising, Chandrasekaran and Jordan (2013) provide a hierarchy of convex relaxations where constraints are defined by convex geometry with increasing complexity. For unsupervised learning, Lucic et al. (2015) introduce a hierarchy of data representations that provide more representative elements when more data is available at no additional computation.",
      "startOffset" : 43,
      "endOffset" : 784
    }, {
      "referenceID" : 5,
      "context" : "posed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition.",
      "startOffset" : 14,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "posed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition.",
      "startOffset" : 14,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "posed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition. Negahban et al. (2014) introduce Rank Centrality, a novel spectral ranking algorithm which formulates a random walk from the given data, and show that the stationary distribution provides accurate estimates of the PL weights.",
      "startOffset" : 14,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "posed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition. Negahban et al. (2014) introduce Rank Centrality, a novel spectral ranking algorithm which formulates a random walk from the given data, and show that the stationary distribution provides accurate estimates of the PL weights. Maystre and Grossglauser (2015) provide a connection between those previous approaches, and give a unified random walk approach that finds the fixed point of the KKT conditions.",
      "startOffset" : 14,
      "endOffset" : 364
    }, {
      "referenceID" : 5,
      "context" : "posed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition. Negahban et al. (2014) introduce Rank Centrality, a novel spectral ranking algorithm which formulates a random walk from the given data, and show that the stationary distribution provides accurate estimates of the PL weights. Maystre and Grossglauser (2015) provide a connection between those previous approaches, and give a unified random walk approach that finds the fixed point of the KKT conditions. On the theoretical side, when samples consist of pairwise comparisons, Simons and Yao (1999) first established consistency and asymptotic normality of the maximum likelihood estimate when all teams play against each other.",
      "startOffset" : 14,
      "endOffset" : 603
    }, {
      "referenceID" : 5,
      "context" : "posed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition. Negahban et al. (2014) introduce Rank Centrality, a novel spectral ranking algorithm which formulates a random walk from the given data, and show that the stationary distribution provides accurate estimates of the PL weights. Maystre and Grossglauser (2015) provide a connection between those previous approaches, and give a unified random walk approach that finds the fixed point of the KKT conditions. On the theoretical side, when samples consist of pairwise comparisons, Simons and Yao (1999) first established consistency and asymptotic normality of the maximum likelihood estimate when all teams play against each other. For a broader class of scenarios where we allow for sparse observations, where the number of total comparisons grow linearly in the number of teams, Negahban et al. (2014) show that Rank Centrality achieves optimal sample complexity by comparing it to a lower bound on the minimax rate.",
      "startOffset" : 14,
      "endOffset" : 905
    }, {
      "referenceID" : 5,
      "context" : "posed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition. Negahban et al. (2014) introduce Rank Centrality, a novel spectral ranking algorithm which formulates a random walk from the given data, and show that the stationary distribution provides accurate estimates of the PL weights. Maystre and Grossglauser (2015) provide a connection between those previous approaches, and give a unified random walk approach that finds the fixed point of the KKT conditions. On the theoretical side, when samples consist of pairwise comparisons, Simons and Yao (1999) first established consistency and asymptotic normality of the maximum likelihood estimate when all teams play against each other. For a broader class of scenarios where we allow for sparse observations, where the number of total comparisons grow linearly in the number of teams, Negahban et al. (2014) show that Rank Centrality achieves optimal sample complexity by comparing it to a lower bound on the minimax rate. For a more general class of traditional observations, including pairwise comparisons, Hajek et al. (2014) provide similar optimal guarantee for the maximum likelihood estimator.",
      "startOffset" : 14,
      "endOffset" : 1126
    }, {
      "referenceID" : 4,
      "context" : "Chen and Suh (2015) introduced Spectral MLE that applies Rank Centrality followed by MLE, and showed that the resulting estimate is optimal in L∞ error as well as the previously analyzed L2 error.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "Chen and Suh (2015) introduced Spectral MLE that applies Rank Centrality followed by MLE, and showed that the resulting estimate is optimal in L∞ error as well as the previously analyzed L2 error. Shah et al. (2015a) study a new measure of the error induced by the Laplacian of the comparisons graph and prove a sharper upper and lower bounds that match up to a constant factor.",
      "startOffset" : 0,
      "endOffset" : 217
    }, {
      "referenceID" : 2,
      "context" : "Although, statistical and computational tradeoffs have been investigated under other popular choice models such as the Mallows models by Betzler et al. (2014) or stochastically transitive models by Shah et al.",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "Although, statistical and computational tradeoffs have been investigated under other popular choice models such as the Mallows models by Betzler et al. (2014) or stochastically transitive models by Shah et al. (2015b), the algorithmic solutions do not apply to random utility models and the analysis techniques do not extend.",
      "startOffset" : 137,
      "endOffset" : 218
    }, {
      "referenceID" : 2,
      "context" : "However, it is also known from Azari Soufiani et al. (2014), that for general RUMs there is no consistent rank-breaking, and the proposed approach does not generalize.",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "In a special case when M = 1, this can be transformed into the traditional pairwise rank-breaking, where (i) this is a concave maximization; (ii) the estimate is (asymptotically) unbiased and consistent as shown in Azari Soufiani et al. (2013, 2014); and (iii) and the finite sample complexity have been analyzed in Khetan and Oh (2016). Although, this order-1 rank-breaking provides a significant gain in computational efficiency, the information contained in higher-order edges are unused, resulting in a significant loss in accuracy.",
      "startOffset" : 221,
      "endOffset" : 337
    }, {
      "referenceID" : 2,
      "context" : "As predicted by Azari Soufiani et al. (2014), this results in an inconsistent estimate, whose error does not vanish as we increase the sample size.",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "This is different from learning Mallows models in Ali and Meilă (2012) where peaked distributions are easier to learn, and is related to the fact that we are not only interested in recovering the (ordinal) ranking but also the (cardinal) weight.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : "planted clique problem (Deshpande and Montanari, 2015; Meka et al., 2015).",
      "startOffset" : 23,
      "endOffset" : 73
    }, {
      "referenceID" : 19,
      "context" : "planted clique problem (Deshpande and Montanari, 2015; Meka et al., 2015).",
      "startOffset" : 23,
      "endOffset" : 73
    }, {
      "referenceID" : 15,
      "context" : "On sushi preferences (Kamishima, 2003) and jester dataset (Goldberg et al.",
      "startOffset" : 21,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : "On sushi preferences (Kamishima, 2003) and jester dataset (Goldberg et al., 2001), we improve over pairwise breaking and achieves same performance as the oracle MLE.",
      "startOffset" : 58,
      "endOffset" : 81
    }, {
      "referenceID" : 11,
      "context" : "On sushi preferences (Kamishima, 2003) and jester dataset (Goldberg et al., 2001), we improve over pairwise breaking and achieves same performance as the oracle MLE. Full rankings over κ = 10 types of sushi are randomly chosen from d = 100 types of sushi are provided by n = 5000 individuals. As the ground truth θ∗, we use the ML estimate of PL weights over the entire data. In Figure 5, left panel, for each m ∈ {3, 4, 5, 6, 7}, we remove the known ordering among the top-m and bottom-(10 −m) sushi in each set, and run our estimator with one breaking edge between top-m and bottom-(10−m) items. We compare our algorithm with inconsistent pairwise breaking (using optimal choice of parameters from Khetan and Oh (2016)) and the oracle MLE.",
      "startOffset" : 59,
      "endOffset" : 721
    }, {
      "referenceID" : 18,
      "context" : "We use the following Theorem from Prékopa (1980). A similar technique was used to prove concavity when |T (e)| = 1 in Azari Soufiani et al.",
      "startOffset" : 34,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "A similar technique was used to prove concavity when |T (e)| = 1 in Azari Soufiani et al. (2012).",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 21,
      "context" : "1 (Theorem 9 in Prékopa (1980)).",
      "startOffset" : 16,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "The proof sketch is inspired from Khetan and Oh (2016). The main difference and technical challenge is in showing the strict concavity of LRB(θ) when restricted to Ωb.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "8, Hayes (2005)], we have",
      "startOffset" : 3,
      "endOffset" : 16
    } ],
    "year" : 2016,
    "abstractText" : "For massive and heterogeneous modern datasets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited. In the application of learning to rank, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data. This allows the number of data points collected to be gracefully traded off against computational resources available, while guaranteeing the desired level of accuracy. Theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data.",
    "creator" : "LaTeX with hyperref package"
  }
}