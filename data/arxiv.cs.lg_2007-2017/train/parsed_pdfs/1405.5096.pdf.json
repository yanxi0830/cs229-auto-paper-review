{
  "name" : "1405.5096.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms",
    "authors" : [ "Richard Combes", "Alexandre Proutiere" ],
    "emails" : [ "RCOMBES@KTH.SE", "ALEPRO@KTH.SE" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n50 96\nv1 [\ncs .L\nG ]\n2 0\nM ay\nWe consider stochastic multi-armed bandits where the expected reward is a unimodal function over partially ordered arms. This important class of problems has been recently investigated in (Cope, 2009; Yu & Mannor, 2011). The set of arms is either discrete, in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards, or continuous, in which case arms belong to a bounded interval. For discrete unimodal bandits, we derive asymptotic lower bounds for the regret achieved under any algorithm, and propose OSUB, an algorithm whose regret matches this lower bound. Our algorithm optimally exploits the unimodal structure of the problem, and surprisingly, its asymptotic regret does not depend on the number of arms. We also provide a regret upper bound for OSUB in nonstationary environments where the expected rewards smoothly evolve over time. The analytical results are supported by numerical experiments showing that OSUB performs significantly better than the state-of-the-art algorithms. For continuous sets of arms, we provide a brief discussion. We show that combining an appropriate discretization of the set of arms with the UCB algorithm yields an order-optimal regret, and in practice, outperforms recently proposed algorithms designed to exploit the unimodal structure.\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s)."
    }, {
      "heading" : "1. Introduction",
      "text" : "Stochastic Multi-Armed Bandits (MAB) (Robbins, 1952; Gittins, 1989) constitute the most fundamental sequential decision problems with an exploration vs. exploitation trade-off. In such problems, the decision maker selects an arm in each round, and observes a realization of the corresponding unknown reward distribution. Each decision is based on past decisions and observed rewards. The objective is to maximize the expected cumulative reward over some time horizon by balancing exploitation (arms with higher observed rewards should be selected often) and exploration (all arms should be explored to learn their average rewards). Equivalently, the performance of a decision rule or algorithm can be measured through its expected regret, defined as the gap between the expected reward achieved by the algorithm and that achieved by an oracle algorithm always selecting the best arm. MAB problems have found many fields of application, including sequential clinical trials, communication systems, economics, see e.g. (Cesa-Bianchi & Lugosi, 2006; Bubeck & Cesa-Bianchi, 2012).\nIn their seminal paper (Lai & Robbins, 1985), Lai and Robbins solve MAB problems where the successive rewards of a given arm are i.i.d., and where the expected rewards of the various arms are not related. They derive an asymptotic (when the time horizon grows large) lower bound of the regret satisfied by any algorithm, and present an algorithm whose regret matches this lower bound. This initial algorithm was quite involved, and many researchers have tried to devise simpler and yet efficient algorithms. The most popular of these algorithms are UCB (Auer et al., 2002) and its extensions, e.g. KL-UCB (Garivier & Cappé, 2011; Cappé et al., 2013) (note that KL-UCB algorithm was initially proposed in (Lai, 1987), see (2.6)). When the expected rewards of the various arms are not related (Lai & Robbins, 1985), the regret of the best algorithm is essentially of the order O(K log(T )) where K denotes the number of arms, and T is the time horizon. When\nK is very large or even infinite, MAB problems become more challenging. Fortunately, in such scenarios, the expected rewards often exhibit some structural properties that the decision maker can exploit to design efficient algorithms. Various structures have been investigated in the literature, e.g., Lipschitz (Agrawal, 1995; Kleinberg et al., 2008; Bubeck et al., 2008), linear (Dani et al., 2008), convex (Flaxman et al., 2005).\nWe consider bandit problems where the expected reward is a unimodal function over partially ordered arms as in (Yu & Mannor, 2011). The set of arms is either discrete, in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards, or continuous, in which case arms belong to a bounded interval. This unimodal structure occurs naturally in many practical decision problems, such as sequential pricing (Yu & Mannor, 2011) and bidding in online sponsored search auctions (B., 2005).\nOur contributions. We mainly investigate unimodal bandits with finite sets of arms, and are primarily interested in cases where the time horizon T is much larger than the number of arms K .\n(a) For these problems, we derive an asymptotic regret lower bound satisfied by any algorithm. This lower bound does not depend on the structure of the graph, nor on its size: it actually corresponds to the regret lower bound in a classical bandit problem (Lai & Robbins, 1985), where the set of arms is just a neighborhood of the best arm in the graph.\n(b) We propose OSUB (Optimal Sampling for Unimodal Bandits), a simple algorithm whose regret matches our lower bound, i.e., it optimally exploits the unimodal structure. The asymptotic regret of OSUB does not depend on the number of arms. This contrasts with LSE (Line Search Elimination), the algorithm proposed in (Yu & Mannor, 2011) whose regret scales as O(γD log(T )) where γ is the maximum degree of vertices in the graph and D is its diameter. We present a finite-time analysis of OSUB, and derive a regret upper bound that scales as O(γ log(T )+K). Hence OSUB offers better performance guarantees than LSE as soon as the time horizon satisfies T ≥ exp(K/γD). Although this is not explicitly mentioned in (Yu & Mannor, 2011), we believe that LSE was meant to address bandits where the number of arms is not negligible compared to the time horizon.\n(c) We further investigate OSUB performance in nonstationary environments where the expected rewards smoothly evolve over time but keep their unimodal structure.\n(d) We conduct numerical experiments and show that OSUB significantly outperforms LSE and other classi-\ncal bandit algorithms when the number of arms is much smaller than the time horizon.\n(e) Finally, we briefly discuss systems with a continuous set of arms. We show that using a simple discretization of the set of arms, UCB-like algorithms are order-optimal, and actually outperform more advanced algorithms such as those proposed in (Yu & Mannor, 2011). This result suggests that in discrete unimodal bandits with a very large number of arms, it is wise to first prune the set of arms, so as to reduce its size to a number of the order of √ T/ log(T )."
    }, {
      "heading" : "2. Related work",
      "text" : "Unimodal bandits have received relatively little attention in the literature. They are specific instances of bandits in metric spaces (Kleinberg, 2004; Kleinberg et al., 2008; Bubeck et al., 2008). In this paper, we add unimodality and show how this structure can be optimally exploited. Unimodal bandits have been specifically addressed in (Cope, 2009; Yu & Mannor, 2011). In (Cope, 2009), bandits with a continuous set of arms are studied, and the author shows that the Kiefer-Wolfowitz stochastic approximation algorithm achieves a regret of the order of O( √ T ) under some strong regularity assumptions on the reward function. In (Yu & Mannor, 2011), for the same problem, the authors present LSE, an algorithm whose regret scales as O( √ T log(T )) without the need for a strong regularity assumption. The LSE algorithm is based on Kiefer’s golden section search algorithm. It iteratively eliminates subsets of arms based on PAC-bounds derived after appropriate sampling. By design, under LSE, the sequence of parameters used for the PAC bounds is pre-defined, and in particular does not depend of the observed rewards. As a consequence, LSE may explore too much sub-optimal parts of the set of arms. For bandits with a continuum set of arms, we actually show that combining an appropriate discretization of the decision space (i.e., reducing the number of arms to √ T/ log(T ) arms) and the UCB algorithm can outperform LSE in practice (this is due to the adaptive nature of UCB). Note that the parameters used in LSE to get a regret of the order O( √ T log(T )) depend on the time horizon T .\nIn (Yu & Mannor, 2011), the authors also present an extension of the LSE algorithm to problems with discrete sets of arms, and provide regret upper bounds of this algorithm. These bounds depends on the structure of the graph defining unimodal structure, and on the number of arms as mentioned previously. LSE performs better than classical bandit algorithms only when the number of arms is very large, and actually becomes comparable to the time horizon. Here we are interested in bandits with relatively small number of arms.\nNon-stationary bandits have been studied in\n(Hartland et al., 2007; Garivier & Moulines, 2008; Slivkins & Upfal, 2008; Yu & Mannor, 2011). Except for (Slivkins & Upfal, 2008), these papers deal with environments where the expected rewards and the best arm change abruptly. This ensures that arms are always well separated, and in turn, simplifies the analysis. In (Slivkins & Upfal, 2008), the expected rewards evolve according to independent brownian motions. We consider a different, but more general class of dynamic environments: here the rewards smoothly evolve over time. The challenge for such environments stems from the fact that, at some time instants, arms can have expected rewards arbitrarily close to each other.\nFinally, we should mention that bandit problems with structural properties such as those we address here can often be seen as specific instances of problems in the control of Markov chains, see (Graves & Lai, 1997). We leverage this observation to derive regret lower bounds. However, algorithms developed for the control of generic Markov chains are often too complex to implement in practice. Our algorithm, OSUB, is optimal and straightforward to implement."
    }, {
      "heading" : "3. Model and Objectives",
      "text" : "We consider a stochastic multi-armed bandit problem with K ≥ 2 arms. We discuss problems where the set of arms is continuous in Section 6. Time proceeds in rounds indexed by n = 1, 2, . . .. Let Xk(n) be the reward obtained at time n if arm k is selected. For any k, the sequence of rewards (Xk(n))n≥1 is i.i.d. with distribution and expectation denoted by νk and µk respectively. Rewards are independent across arms. Let µ = (µ1, . . . , µK) represent the expected rewards of the various arms. At each round, a decision rule or algorithm selects an arm depending on the arms chosen in earlier rounds and their observed rewards. We denote by kπ(n) the arm selected under π in round n. The set Π of all possible decision rules consists of policies π satisfying: for any n ≥ 1, if Fπn is the σ-algebra generated by (kπ(t), Xkπ(t)(t))1≤t≤n, then kπ(n+1) is Fπn -measurable."
    }, {
      "heading" : "3.1. Unimodal Structure",
      "text" : "The expected rewards exhibit a unimodal structure, similar to that considered in (Yu & Mannor, 2011). More precisely, there exists an undirected graph G = (V,E) whose vertices correspond to arms, i.e., V = {1, . . . ,K}, and whose edges characterize a partial order (initially unknown to the decision maker) among expected rewards. We assume that there exists a unique arm k⋆ with maximum expected reward µ⋆, and that from any sub-optimal arm k 6= k⋆, there exists a path p = (k1 = k, . . . , km = k⋆) of length m (depending on k) such that for all i = 1, . . . ,m− 1, (ki, ki+1) ∈ E and µki < µki+1 . We denote by UG the set of vectors µ satisfying this unimodal structure.\nThis notion of unimodality is quite general, and includes, as a special case, classical unimodality (where G is just a line). Note that we assume that the decision maker knows the graph G, but ignores the best arm, and hence the partial order induced by the edges of G."
    }, {
      "heading" : "3.2. Stationary and non-stationary environments",
      "text" : "The model presented above concerns stationary environments, where the expected rewards for the various arms do not evolve over time. In this paper, we also consider non-stationary environments where these expected rewards could evolve over time according to some deterministic dynamics. In such scenarios, we denote by µk(n) the expected reward of arm k at time n, i.e., E[Xk(n)] = µk(n), and (Xk(n))n≥1 constitutes a sequence of independent random variables with evolving mean. In non-stationary environments, the sequences of rewards are still assumed to be independent across arms. Moreover, at any time n, µ(n) = (µ1(n), . . . µK(n)) is unimodal with respect to some fixed graph G, i.e., µ(n) ∈ UG (note however that the partial order satisfied by the expected rewards may evolve over time)."
    }, {
      "heading" : "3.3. Regrets",
      "text" : "The performance of an algorithm π ∈ Π is characterized by its regret up to time T (where T is typically large). The way regret is defined differs depending on the type of environment.\nStationary Environments. In such environments, the regret Rπ(T ) of algorithm π ∈ Π is simply defined through the number of times tπk (T ) = ∑\n1≤n≤T 1{kπ(n) = k} that arm k has been selected up to time T : Rπ(T ) = ∑K\nk=1(µ ⋆−µk)E[tπk (T )]. Our objectives are (1) to identify an asymptotic (when T → ∞) regret lower bound satisfied by any algorithm in Π, and (2) to devise an algorithm that achieves this lower bound.\nNon-stationary Environments. In such environments, the regret of an algorithm π ∈ Π quantifies how well π tracks the best arm over time. Let k⋆(n) denote the optimal arm with expected reward µ⋆(n) at time n. The regret of π up to time T is hence defined as: Rπ(T ) = ∑T\nn=1\n( µ⋆(n)− E[µkπ(n)(n)] ) ."
    }, {
      "heading" : "4. Stationary environments",
      "text" : "In this section, we consider unimodal bandit problems in stationary environments. We derive an asymptotic lower bound of regret when the reward distributions belong to a parametrized family of distributions, and propose OSUB, an algorithm whose regret matches this lower bound."
    }, {
      "heading" : "4.1. Lower bound on regret",
      "text" : "To simplify the presentation, we assume here that the reward distributions belong to a parametrized family of distributions. More precisely, we define a set of distributions V = {ν(θ)}θ∈[0,1] parametrized by θ ∈ [0, 1]. The expectation of ν(θ) is denoted by µ(θ) for any θ ∈ [0, 1]. ν(θ) is absolutely continuous with respect to some positive measure m on R, and we denote by p(x, θ) its density. The Kullback-Leibler (KL) divergence number between ν(θ) and ν(θ′) is: KL(θ, θ′) = ∫\nR log(p(x, θ)/p(x, θ′))p(x, θ)m(dx). We\ndenote by θ⋆ a parameter (it might not be unique) such that µ(θ⋆) = µ⋆, and we define the minimal divergence number between ν(θ) and ν(θ⋆) as: Imin(θ, θ⋆) = infθ∈[0,1]:µ(θ′)≥µ⋆ KL(θ, θ ′).\nFinally, we say that arm k has parameter θk if νk = ν(θk), and we denote by ΘG the set of all parameters θ = (θ1, . . . , θK) ∈ [0, 1]K such that the corresponding expected rewards are unimodal with respect to graph G: µ = (µ1, . . . , µK) ∈ UG. Of particular interest is the family of Bernoulli distributions: the support of m is {0, 1}, µ(θ) = θ, and Imin(θ, θ⋆) = I(θ, θ⋆) where I(θ, θ⋆) = θ log( θθ⋆ ) + (1 − θ) log( 1−θ1−θ⋆ ) is KL divergence number between Bernoulli distributions of respective means θ and θ⋆.\nWe are now ready to derive an asymptotic regret lower in parametrized unimodal bandit problems as defined above. Without loss of generality, we restrict our attention to so-called uniformly good algorithms, as defined in (Lai & Robbins, 1985) (uniformly good algorithms exist as shown later on). We say that π ∈ Π is uniformly good if for all θ ∈ ΘG, we have that Rπ(T ) = o(T a) for all a > 0.\nTheorem 4.1 Let π ∈ Π be a uniformly good algorithm, and assume that νk = ν(θk) ∈ V for all k. Then for any θ ∈ ΘG,\nlim inf T→+∞\nRπ(T ) log(T ) ≥ c(θ) = ∑\n(k,k∗)∈E\nµ⋆ − µk Imin(θk, θ⋆) . (1)\nThe above theorem is a consequence of results in optimal control of Markov chains (Graves & Lai, 1997). All proofs are presented in appendix. As in classical discrete bandit problems, the regret scales at least logarithmically with time (the regret lower bound derived in (Lai & Robbins, 1985) is obtained from Theorem 4.1 assuming that G is the complete graph). We also observe that the unimodal structure, if optimally exploited, can bring significant performance improvements: the regret lower bound does not depend on the size K of the decision space. Indeed c(θ) includes only terms corresponding to arms that are neighbors in G of the optimal arm (as if one could learn without regret that all other arms are sub-optimal).\nIn the case of Bernoulli rewards, the lower regret bound becomes log(T ) ∑\n(k,k∗)∈E µ⋆−µk I(θk,θ⋆)\n. Note that LSE and GLSE, the algorithms proposed in (Yu & Mannor, 2011), have performance guarantees that do not match our lower bound: when G is a line, LSE achieves a regret bounded by 41/∆2 log(T ), whereas in the general case, GLSE incurs a regret of the order of O(γD log(T )) where γ is the maximal degree of vertices in G, and D is its diameter. The performance of LSE critically depends on the graph structure, and the number of arms. Hence there is an important gap between the performance of existing algorithms and the lower bound derived in Theorem 4.1. In the next section, we close this gap and propose an asymptotically optimal algorithm."
    }, {
      "heading" : "4.2. The OSUB Algorithm",
      "text" : "We now describe OSUB, a simple algorithm whose regret matches the lower bound derived in Theorem of 4.1 for Bernoulli rewards, i.e., OSUB is asymptotically optimal. The algorithm is based on KL-UCB proposed in (Lai, 1987; Cappé et al., 2013), and uses KL-divergence upper confidence bounds to define an index for each arm. OSUB can be readily extended to systems where reward distributions are within one-parameter exponential families by simply modifying the definition of arm indices as done in (Cappé et al., 2013). In OSUB, each arm is attached an index that resembles the KL-UCB index, but the arm selected at a given time is the arm with maximal index within the neighborhood in G of the arm that yielded the highest empirical reward. Note that since the sequential choices of arms are restricted to some neighborhoods in the graph, OSUB is not an index policy. To formally describe OSUB, we need the following notation. For p ∈ [0, 1], s ∈ N, and n ∈ N, we define:\nF (p, s, n) = sup{q ≥ p : sI(p, q) ≤ log(n) + c log(log(n))}, (2)\nwith the convention that F (p, 0, n) = 1, and F (1, s, n) = 1, and where c > 0 is a constant. Let k(n) be the arm selected under OSUB at time n, and let tk(n) denote the number of times arm k has been selected up to time n. The empirical reward of arm k at time n is µ̂k(n) =\n1 tk(n) ∑n t=1 1{k(t) = k}Xk(t), if tk(n) > 0 and µ̂k(n) = 0 otherwise. We denote by L(n) = argmax1≤k≤K µ̂k(n) the index of the arm with the highest empirical reward (ties are broken arbitrarily). Arm L(n) is referred to as the leader at time n. Further define lk(n) = ∑n t=1 1{L(t) = k} the number of times arm k has been the leader up to time n. Now the index of arm k at time n is defined as:\nbk(n) = F (µ̂k(n), tk(n), lk(L(n))).\nFinally for any k, let N(k) = {k′ : (k′, k) ∈ E} ∪ {k} be the neighborhood of k in G. The pseudo-code of OSUB is\npresented below.\nAlgorithm OSUB\nInput: graph G = (V,E) For n ≥ 1, select the arm k(n) where:\nk(n) =\n\n\n\nL(n) if lL(n)(n)−1 γ+1 ∈ N, arg max\nk∈N(L(n)) bk(n) otherwise,\nwhere γ is the maximal degree of nodes in G and ties are broken arbitrarily.\nNote that OSUB forces us to select the current leader often: L(n) is chosen when lL(n)(n) − 1 is a multiple of γ + 1. This ensures that the number of times an arm has been selected is at least proportional to the number of times this arm has been the leader. This property significantly simplifies the regret analysis, but it could be removed."
    }, {
      "heading" : "4.3. Finite-time analysis of OSUB",
      "text" : "Next we provide a finite time analysis of the regret achieved under OSUB. Let ∆ denote the minimal separation between an arm and its best adjacent arm: ∆ = min1≤k≤K maxk′:(k,k′)∈E µk′ − µk. Note that ∆ is not known a priori.\nTheorem 4.2 Assume that the rewards lie in [0,1] (i.e., the support of νk is included in [0, 1], for all k), and that (µ1, . . . , µK) ∈ UG. The number of times suboptimal arm k is selected under OSUB satisfies: for all ǫ > 0 and all T ≥ 3,\nE[tk(T )] ≤\n\n \n \n(1 + ǫ) log(T )+c log(log(T ))I(µk,µ∗) if (k, k ⋆) ∈ E,\n+C1 log log(T ) + C2\nTβ(ǫ) C3 ∆2 otherwise,\nwhere β(ǫ) > 0, and 0 < C1 < 7, C2 > 0, C3 > 0 are constants.\nTo prove this upper bound, we analyze the regret accumulated (i) when the best arm k⋆ is the leader, and (ii) when the leader is arm k 6= k⋆. (i) When k⋆ is the leader, the algorithm behaves like KL-UCB restricted to the arms around k⋆, and the regret at these rounds can be analyzed as in (Cappé et al., 2013). (ii) Bounding the number of rounds where k 6= k⋆ is not the leader is more involved. To do this, we decompose this set of rounds into further subsets (such as the time instants where k is the leader and its mean is not well estimated), and control their expected cardinalities using concentration inequalities. Along the way, we establish Lemma 4.3, a new concentration inequality of independent interest.\nLemma 4.3 Let {Zt}t∈Z be a sequence of independent random variables with values in [0, B]. Define Fn the σ-algebra generated by {Zt}t≤n and the filtration F = (Fn)n∈Z. Consider s ∈ N, n0 ∈ Z and T ≥ n0. We define Sn =\n∑n t=n0 Bt(Zt − E[Zt]), where Bt ∈ {0, 1} is a Ft−1-measurable random variable. Further define tn =\n∑n t=n0 Bt. Define φ ∈ {n0, . . . , T+1} a F -stopping time such that either tφ ≥ s or φ = T + 1. Then we have that: P[Sφ ≥ tφδ , φ ≤ T ] ≤ exp(−2sδ2B−2). As a consequence: P[|Sφ| ≥ tφδ , φ ≤ T ] ≤ 2 exp(−2sδ2B−2).\nLemma 4.3 concerns the sum of products of i.i.d. random variables and of a previsible sequence, evaluated at a stopping time (for the natural filtration). We believe that concentration results for such sums can be instrumental in bandit problems, where typically, we need information about the empirical rewards at some specific random time epochs (that often are stopping times). Refer to the appendix for a proof. A direct consequence of Theorem 4.2 is the asymptotic optimality of OSUB in the case of Bernoulli rewards:\nCorollary 4.4 Assume that rewards distributions are Bernoulli (i.e for any k, νk ∼ Bernoulli(θk)), and that θ ∈ ΘG. Then the regret achieved under π=OSUB satisfies: lim supT→+∞ R π(T )/ log(T ) ≤ c(θ)."
    }, {
      "heading" : "5. Non-stationary environments",
      "text" : "We now consider time-varying environments. We assume that the expected reward of each arm varies smoothly over time, i.e., it is Lipschitz continuous: for all n, n′ ≥ 1 and 1 ≤ k ≤ K: |µk(n)− µk(n′)| ≤ σ|n− n′|. We further assume that the unimodal structure is preserved (with respect to the same graph G): for all n ≥ 1, µ(n) ∈ UG. Considering smoothly varying rewards is more challenging than scenarios where the environment is abruptly changing. The difficulty stems from the fact that the rewards of two or more arms may become arbitrarily close to each other (this happens each time the optimal arm changes), and in such situations, regret is difficult to control. To get a chance to design an algorithm that efficiently tracks the best arm, we need to make some assumption to limit the proportion of time when the separation of arms becomes too small. Define for T ∈ N, and ∆ > 0:\nH(∆, T ) =\nT ∑\nn=1\n∑\n(k,k′)∈E\n1{|µk(n)− µk′(n)| < ∆}.\nAssumption 1 There exists a function Φ and ∆0 such that for all ∆ < ∆0: lim supT→+∞ H(∆, T )/T ≤ Φ(K)∆."
    }, {
      "heading" : "5.1. OSUB with a Sliding Window",
      "text" : "To cope with the changing environment, we modify the OSUB algorithm, so that decisions are based on past choices and observations over a time-window of fixed duration equal to τ + 1 rounds. The idea of adding a sliding window to algorithms initially designed for stationary environments is not novel (Garivier & Moulines, 2008); but here, the unimodal structure and the smooth evolution of rewards make the regret analysis more challenging.\nDefine: tτk(n) = ∑n t=n−τ 1{k(t) = k}; µ̂τk(n) = (1/tτk(n)) ∑n t=n−τ 1{k(t) = k}Xk(t) if tτk(n) > 0 and µ̂τk(n) = 0 otherwise; L τ (n) = argmax1≤k≤K µ̂ τ k(n); lτk(n) = ∑n\nt=n−τ 1{Lτ(t) = k}. The index of arm k at time n then becomes: bτk(n) = F (µ̂τk(n), t τ k(n), l τ k(L\nτ (n))). The pseudo-code of SWOSUB is presented below.\nAlgorithm SW-OSUB Input: graph G = (V,E), window size τ + 1 For n ≥ 1, select the arm k(n) where:\nk(n) =\n\n\n\nLτ (n) if lτLτ (n)(n)−1\nγ+1 ∈ N, arg max\nk∈N(Lτ (n)) bτk(n) otherwise."
    }, {
      "heading" : "5.2. Regret Analysis",
      "text" : "In non-stationary environments, achieving sublinear regrets is often not possible. In (Garivier & Moulines, 2008), the environment is subject to abrupt changes or breakpoints. It is shown that if the density of breakpoints is strictly positive, which typically holds in practice, then the regret of any algorithm has to scale linearly with time. We are interested in similar scenarios, and consider smoothly varying environments where the number of times the optimal arm changes has a positive density. The next theorem provides an upper bound of the regret per unit of time achieved under SW-OSUB. This bound holds for any non-stationary environment with σ-Lipschitz rewards.\nTheorem 5.1 Let ∆: 2τσ < ∆ < ∆0. Assume that for any n ≥ 1, µ(n) ∈ UG and µ⋆(n) ∈ [a, 1 − a] for some a > 0. Further suppose that µk(·) is σ-Lipschitz for any k. The regret per unit time under π =SW-OSUB with a sliding window of size τ + 1 satisfies: if a > στ , then for any T ≥ 1, Rπ(T )\nT ≤ H(∆, T ) T (1 + ∆) + C1K log(τ) τ(∆− 4τσ)2\n+ γ (\n1 + g −1/2 0 ) log(τ) + c log(log(τ)) + C2 2τ(∆− 2τσ)2 ,\nwhere C1, C2 are positive constants and g0 = (a−στ)(1− a+ στ)/2.\nCorollary 5.2 Assume that for any n ≥ 1, µ(n) ∈ UG and µ⋆(n) ∈ [a, 1 − a] for some a > 0, and that µk(·) is σLipschitz for any k. Set τ = σ−3/4 log(1/σ)/8. The regret per unit of time of π =SW-OSUB with window size τ + 1 satisfies:\nlim sup T→∞\nRπ(T )\nT ≤ CΦ(K)σ 14 log\n(\n1\nσ\n)\n(1 +Kj(σ)),\nfor some constant C > 0, and some function j such that limσ→0+ j(σ) = 0.\nThese results state that the regret per unit of time achieved under SW-OSUB decreases and actually vanishes when the speed at which expected rewards evolve decreases to 0. Also observe that the dependence of this regret bound in the number of arms is typically mild (in many practical scenarios, Φ(K) may actually not depend on K).\nThe proof of Theorem 5.1 relies on the same types of arguments as those used in stationary environments. To establish the regret upper bound, we need to evaluate the performance of the KL-UCB algorithm in non-stationary environments (the result and the corresponding analysis are presented in appendix)."
    }, {
      "heading" : "6. Continuous Set of Arms",
      "text" : "In this section, we briefly discuss the case where the decision space is continuous. The set of arms is [0, 1], and the expected reward function µ : [0, 1] → R is assumed to be Lipschitz continuous, and unimodal: there exists x⋆ ∈ [0, 1] such that µ(x′) ≥ µ(x) if x′ ∈ [x, x⋆] or x′ ∈ [x⋆, x]. Let µ⋆ = µ(x⋆) denote the highest expected reward. A decision rule selects at any round n ≥ 1 an arm x and observes the corresponding reward X(x, n). For any x ∈ [0, 1], (X(x, n))n≥1 is an i.i.d. sequence. We make the following additional assumption on function µ.\nAssumption 2 There exists δ0 > 0 such that (i) for all x, y in [x⋆, x⋆+ δ0] (or in [x⋆− δ0, x⋆]), C1|x− y|α ≤ |µ(x)− µ(y)|; (ii) for δ ≤ δ0, if |x−x∗| ≤ δ, then |µ(x∗)−µ(x)| ≤ C2δ α.\nThis assumption is more general than that used in (Yu & Mannor, 2011). In particular it holds for functions with a plateau and a peak: µ(x) = max(1−|x−x⋆|/ǫ, 0). Now as for the case of a discrete set of arms, we denote by Π the set of possible decision rules, and the regret achieved under rule π ∈ Π up to time T is: Rπ(T ) = Tµ⋆−∑Tn=1 E[µ(xπ(n))], where xπ(n) is the arm selected under π at time n.\nThere is no known precise asymptotic lower bound for continuous bandits. However, we know that for our problem, the regret must be at least of the order of O( √ T ) up to\nlogarithmic factor. In (Yu & Mannor, 2011), the authors show that the LSE algorithm achieves a regret scaling as O( √ T log(T )), under more restrictive assumptions. We show that combining discretization and the UCB algorithm as initially proposed in (Kleinberg, 2004) yields lower regrets than LSE in practice (see Section 7), and is orderoptimal, i.e., the regret grows as O( √ T log(T )).\nFor δ > 0, we define a discrete bandit problem with K = ⌈1/δ⌉ arms, and where the rewards of k-th arm are distributed as X((k− 1)/δ, n). The expected reward of the k-th arm is µk = µ((k− 1)/δ). Let π be an algorithm running on this discrete bandit problem. The regret of π for the initial continuous bandit problem is at time T : Rπ(T ) = Tµ⋆ − ∑⌈1/δ⌉k=1 µkE[tπk (T )]. We denote by UCB(δ) the UCB algorithm (Auer et al., 2002) applied to the discretized bandit. In the following proposition, we show that when δ = (log(T )/ √ T )1/α, UCB(δ) is orderoptimal. In practice, one may not know the time horizon T in advance. In this case, using the “doubling trick” (see e.g. (Cesa-Bianchi & Lugosi, 2006)) would incur an additional logarithmic multiplicative factor in the regret.\nProposition 1 Consider a unimodal bandit on [0, 1] with rewards in [0, 1] and satisfying Assumption 2. Set δ = (log(T )/ √ T )1/α. The regret under UCB(δ) satisfies:\nlim sup T→∞ Rπ(T )√ T log(T ) ≤ C23α + 16/C1."
    }, {
      "heading" : "7. Numerical experiments",
      "text" : ""
    }, {
      "heading" : "7.1. Discrete bandits",
      "text" : "We compare the performance of our algorithm to that of KL-UCB (Cappé et al., 2013), LSE (Yu & Mannor, 2011), UCB (Auer et al., 2002), and UCB-U. The latter algorithm is obtained by applying UCB restricted to the arms which are adjacent to the current leader as in OSUB. We add the prefix ”SW” to refer to Sliding Window versions of these algorithms.\nStationary environments. In our first experiment, we consider K = 17 arms with Bernoulli rewards of respective averages µ = (0.1, 0.2, ...., 0.9, 0.8, . . . , 0.1). The rewards are unimodal (the graph G is simply a line). The regret achieved under the various algorithms is presented in Figure 1 and Table 1. The parameters in LSE algorithm are chosen as suggested in Proposition 4.5 (Yu & Mannor, 2011). Regrets are calculated averaging over 50 independent runs. OSUB significantly outperforms all other algorithms. The regret achieved under LSE is not presented in Figure 1, because it is typically much larger than that of other algorithms. This poor performance can be explained by the non-adaptive nature of LSE, as already discussed earlier. LSE can beat UCB when the number of arms is\nnot negligible compared to the time horizon (e.g. in Figure 4 in (Yu & Mannor, 2011), K = 250.000 and the time horizon is less than 3K): in such scenarios, UCB-like algorithms perform poorly because of their initialization phase (all arms have to be tested once).\nIn Figure 2, the number of arms is 129, and the expected rewards form a triangular shape as in the previous example, with minimum and maximum equal to 0.1 and 0.9, respectively. Similar observations as in the case of 17 arms can be made. We deliberately restrict the plot to small time horizons: this corresponds to scenarios where LSE can perform well.\nNon-stationary environments. We now investigate the per-\nformance of SW-OSUB in a slowly varying environment. There are K = 10 arms whose expected rewards form a moving triangle: for k = 1, . . . ,K , µk(n) = (K−1)/K− |w(n)−k|/K , where w(n) = 1+(K−1)(1+sin(nσ))/2. Figure 3 presents the regret as a function of time under various algorithms when the speed at which the environment evolves is σ = 10−3. The window size are set as follows for the various algorithms: τ = σ−4/5 for SW-UCB and SW-KL-UCB (the rationale for this choice is explained in appendix), τ = σ−3/4 log(1/σ)/8 for SW-UCB-U and OSUB. In Figure 4, we show how the speed σ impacts the regret per time unit. SW-OSUB provides the most efficient way of tracking the optimal arm."
    }, {
      "heading" : "7.2. Continuous bandits",
      "text" : "In Figure 5, we compare the performance of the LSE and UCB(δ) algorithms when the set of arms is continuous. The expected rewards form a triangle: µ(x) = 1/2− |x− 1/2| so that µ⋆ = 1/2 and x⋆ = 1/2. The parameters used in LSE are those given in (Yu & Mannor, 2011), whereas the discretization parameter δ in UCB(δ) is set to δ = log(T )/ √ T . UCB(δ) significantly outperforms LSE at any time: an appropriate discretization of continuous bandit problems might actually be more efficient than other meth-\nods based on ideas taken from classical optimization theory.\nFigure 6 compares the regret of the discrete version of LSE (with optimized parameters), and of OSUB as the number of arms K grows large, T = 50, 000. The average rewards of arms are extracted from the triangle used in the continuous bandit, and we also provide the regret achieved under UCB(δ). OSUB outperforms UCB(δ) even if the number of arms gets as large as 7500! OSUB also beats LSE unless the number of arms gets bigger than 0.6× T ."
    }, {
      "heading" : "8. Conclusion",
      "text" : "In this paper, we address stochastic bandit problems with a unimodal structure, and a finite set of arms. We provide asymptotic regret lower bounds for these problems and design an algorithm that asymptotically achieves the lowest regret possible. Hence our algorithm optimally exploits the unimodal structure of the problem. Our preliminary analysis of the continuous version of this bandit problem suggests that when the number of arms become very large and comparable to the time horizon, it might be wiser to prune the set of arms before actually running any algorithm."
    }, {
      "heading" : "A. Proof of Theorem 4.1",
      "text" : "We derive here a regret lower bound for the unimodal bandit problem. To this aim, we apply the techniques used by Graves and Lai (Graves & Lai, 1997) to investigate efficient adaptive decision rules in controlled Markov chains. We recall here their general framework. Consider a controlled Markov chain (Xt)t≥0 on a finite state space S with a control set U . The transition probabilities given control u ∈ U are parametrized by θ taking values in a compact metric space Θ: the probability to move from state x to state y given the control u and the parameter θ is p(x, y;u, θ). The parameter θ is not known. The decision maker is provided with a finite set of stationary control laws G = {g1, . . . , gK} where each control law gj is a mapping from S to U : when control law gj is applied in state x, the applied control is u = gj(x). It is assumed that if the decision maker always selects the same control law g the Markov chain is then irreducible with stationary distribution πgθ . Now the expected reward obtained when applying control u in state x is denoted by r(x, u), so that the expected reward achieved under control law g is: µθ(g) = ∑ x r(x, g(x))π g θ (x). There is an optimal control law given θ whose expected reward is denoted µ⋆θ ∈ argmaxg∈G µθ(g). Now the objective of the decision maker is to sequentially select control laws so as to maximize the expected reward up to a given time horizon T . As for MAB problems, the performance of a decision scheme can be quantified through the notion of regret which compares the expected reward to that obtained by always applying the optimal control law.\nWe now apply the above framework to our unimodal bandit problem, and we consider θ ∈ ΘG. The Markov chain has values in S = R. The set of control laws is G = {1, . . . ,K}. These laws are constant, in the sense that the control applied by control law k does not depend on the state of the Markov chain, and corresponds to selecting arm k. The transition probabilities are: p(x, y; k, θ) = p(y, θk). Finally, the reward r(x, k) does not depend on the state and is equal to µ(θk), which is also the expected reward obtained by always using control law k.\nWe now fix θ ∈ ΘG. Define KLk(θ, λ) = KL(θk, λk) for\nany k. Further define the set B(θ) consisting of all bad parameters λ ∈ ΘG such that k⋆ is not optimal under parameter λ, but which are statistically indistinguishable from θ:\nB(θ) = {λ ∈ ΘG : λk⋆ = θk⋆ and max k µ(λk) > µ(λk⋆)},\nB(θ) can be written as the union of sets Bk(θ), (k, k⋆) ∈ E defined as:\nBk(θ) = {λ ∈ B(θ) : µ(λk) > µ(λk⋆)}.\nWe have that B(θ) = ∪(k,k⋆)∈EBk(θ), because if µ(λk⋆) < maxk µ(λk), then there must exist k such that (k, k∗) ∈ E, and µ(λk) > µ(λk⋆ ). By applying Theorem 1 in (Graves & Lai, 1997), we know that c(θ) is the minimal value of the following LP:\nmin ∑ k ck(µ(θk⋆)− µ(θk)) (3) s.t. infλ∈Bk(θ) ∑ l 6=k⋆ clKL l(θ, λ) ≥ 1, (k, k⋆) ∈ E(4)\nck ≥ 0, ∀k. (5)\nNext we show that the constraints (4) on the ck’s are equivalent to:\nmin (k,k⋆)∈E\nckImin(θk, µ(θk⋆)) ≥ 1. (6)\nConsider k fixed with (k, k⋆) ∈ E. We prove that:\ninf λ∈Bk(θ)\n∑\nl 6=k⋆\nclKL l(θ, λ) = ckImin(θk, µ(θk⋆)). (7)\nThis is simply due to the following two observations:\n• Since µ(λk) > µ(θk⋆) and the KL divergence is positive:\n∑\nl 6=k⋆\nclKL l(θ, λ) ≥ ckKLk(θ, λ)\n≥ ckImin(θk, µ(θk⋆)).\n• For ǫ > 0, define λǫ as follows: µ(λk) > µ(θk⋆) and KL(θk, λk) ≤ Imin(θk, µ(θk⋆)) + ǫ and λl = θl for l 6= k. By construction, λǫ ∈ Bk(θ), and\nlim ǫ→0\n∑\nl 6=k⋆\nclKL l(θ, λǫ) = ckImin(θk, µ(θk⋆)).\nFrom (7), we deduce that constraints (4) are equivalent to (6) (indeed, for (k, k⋆) ∈ E, (4) is equivalent to ckImin(θk, µ(θk⋆)) ≥ 1). With the constraints (6), the optimization problem becomes straightforward to solve, and its solution yields:\nc(θ) = ∑\n(k,k⋆)∈E\nµ(θk⋆)− µ(θk) Imin(θk, µ(θk⋆)) ."
    }, {
      "heading" : "B. Concentration inequalities and Preliminaries",
      "text" : "B.1. Proof of Lemma 4.3\nWe prove Lemma 4.3, a new concentration inequality which extends Hoeffding’s inequality, and is used for the regret analysis in subsequent sections. We believe that Lemma 4.3 could be useful in a variety of bandit problems, where an upper bound on the deviation of the empirical mean sampled at a stopping time is needed. An example would be the probability that the empirical reward of the k-th arm deviates from its expectation, when it is sampled for the s-th time.\nProof. Let λ > 0, and define Gn = exp(λ(Sn − δtn))1{n ≤ T }. We have that:\nP[Sφ ≥ tφδ , φ ≤ T ] = P[exp(λ(Sφ − δtφ))1{φ ≤ T } ≥ 1] = P[Gφ ≥ 1] ≤ E[Gφ].\nNext we provide an upper bound for E[Gφ]. We define the following quantities:\nYt = Bt[λ(Zt − E[Zt])− λ2B2/8]\nG̃n = exp\n(\nn ∑\nt=n0\nYt\n)\n1{n ≤ T }.\nSo that G can be written:\nGn = G̃n exp(−tn(λδ − λ2B2/8)).\nSetting λ = 4δ/B2:\nGn = G̃n exp(−2tnδ2/B2).\nUsing the fact that tφ ≥ s if φ ≤ T , we can upper bound Gφ by:\nGφ = G̃φ exp(−2tφδ2/B2) ≤ G̃φ exp(−2sδ2/B2).\nIt is noted that the above inequality holds even when φ = T + 1, since GT+1 = G̃T+1 = 0. Hence:\nE[Gφ] ≤ E[G̃φ] exp(−2sδ2/B2).\nWe prove that (G̃n)n is a super-martingale. We have that E[G̃T+1|FT ] = 0 ≤ G̃T . For n ≤ T − 1, since Bn+1 is Fn measurable:\nE[G̃n+1|Fn] = G̃n((1−Bn+1) +Bn+1E[exp(Yn+1)]).\nAs proven by Hoeffding (Hoeffding, 1963)[eq. 4.16] since Zn+1 ∈ [0, B]:\nE[exp(λ(Zn+1 − E[Zn+1]))] ≤ exp(λ2B2/8),\nso E[exp(Yn+1)] ≤ 1 and (G̃n)n is indeed a supermartingale: E[G̃n+1|Fn] ≤ G̃n. Since φ ≤ T + 1 almost surely, and (G̃n)n is a supermartingale, Doob’s optional stopping theorem yields: E[G̃φ] ≤ E[G̃n0−1] = 1, and so\nP[Sφ ≥ tφδ, φ ≤ T ] ≤ E[Gφ] ≤ E[G̃φ] exp(−2sδ2/B2) ≤ exp(−2sδ2/B2).\nwhich concludes the proof. The second inequality is obtained by symmetry.\nB.2. Preliminary results\nLemma B.1 states that if a set of instants Λ can be decomposed into a family of subsets (Λ(s))s≥1 of instants (each subset has at most one instant) where k is tried sufficiently many times (tk(n) ≥ ǫs, for n ∈ Λ(s)), then the expected number of instants in Λ at which the average reward of k is badly estimated is finite.\nLemma B.1 Let k ∈ {1, . . . ,K}, and ǫ > 0. Define Fn the σ-algebra generated by (Xk(t))1≤t≤n,1≤k≤K . Let Λ ⊂ N be a (random) set of instants. Assume that there exists a sequence of (random) sets (Λ(s))s≥1 such that (i) Λ ⊂ ∪s≥1Λ(s), (ii) for all s ≥ 1 and all n ∈ Λ(s), tk(n) ≥ ǫs, (iii) |Λ(s)| ≤ 1, and (iv) the event n ∈ Λ(s) is Fnmeasurable. Then for all δ > 0:\nE[ ∑\nn≥1\n1{n ∈ Λ, |µ̂k(n)− E[µ̂k(n)]| > δ}] ≤ 1\nǫδ2 . (8)\nProof. Let T ≥ 1. For all s ≥ 1, since Λ(s) has at most one element, define φs = T + 1 if Λ(s) ∩ {1, . . . , T } is empty and {φs} = Λ(s) otherwise. Since Λ ⊂ ∪s≥1Λ(s), we have:\nT ∑\nn=1\n1{n ∈ Λ, |µ̂k(n)− E[µ̂k(n)]| > δ}\n≤ ∑\ns≥1\n1{|µ̂k(φs)− E[µ̂k(φs)]| > δ, φs ≤ T }.\nTaking expectations:\nE[\nT ∑\nn=1\n1{n ∈ Λ, |µ̂k(n)− E[µ̂k(n)]| > δ}]\n≤ ∑\ns≥1\nP[|µ̂k(φs)− E[µ̂k(φs)]| > δ, φs ≤ T ].\nSince φs is a stopping time upper bounded by T + 1, and that tk(φs) ≥ ǫs we can apply Lemma 4.3 to obtain:\nE[\nT ∑\nn=1\n1{n ∈ Λ, |µ̂k(n)− E[µ̂k(n)]| > δ}]\n≤ ∑\ns≥1\n2 exp ( −2sǫδ2 ) ≤ 1 ǫδ2 .\nWe have used the inequality: ∑ s≥1 e −sw ≤ ∫ +∞\n0 e −uwdu = 1/w. Since the above reasoning is\nvalid for all T , we obtain the claim (8).\nA useful corollary of Lemma B.1 is obtained by choosing δ = ∆k,k′/2, when arms k and k′ are separated by at least ∆k,k′ .\nLemma B.2 Let k, k′ ∈ {1, . . . ,K} with k 6= k′ and ǫ > 0. Define Fn the σ-algebra generated by (Xk(t))1≤t≤n,1≤k≤K . Let Λ ⊂ N be a (random) set of instants. Assume that there exists a sequence of (random) sets (Λ(s))s≥1 such that (i) Λ ⊂ ∪s≥1Λ(s), (ii) for all s ≥ 1 and all n ∈ Λ(s), tk(n) ≥ ǫs and tk′(n) ≥ ǫs, (iii) for all s we have |Λ(s)| ≤ 1 almost surely and (iv) for all n ∈ Λ, we have E[µ̂k(n)] ≤ E[µ̂k′(n)] −∆k,k′ (v) the event n ∈ Λ(s) is Fn-measurable. Then:\nE[ ∑\nn≥1\n1{n ∈ Λ, µ̂k(n) > µ̂k′ (n)}] ≤ 8\nǫ∆2k,k′ . (9)\nLemma B.3 is straightforward from (Garivier & Cappé, 2011)[Theorem 10]. It should be observed that this result is not a direct application of Sanov’s theorem; Lemma B.3 provides sharper bounds in certain cases, and it is also valid for non-Bernoulli distributed random variables.\nLemma B.3 For 1 ≤ tk(n) ≤ τ and δ > 0, if {Xk(i)}1≤i≤τ are independent random variables with mean µk, we have that:\nP\n\ntk(n)I\n\n\n1\ntk(n)\ntk(n) ∑\ni=1\nXk(i), µk\n\n ≥ δ\n\n\n≤ 2e⌈δ log(τ)⌉ exp(−δ).\nWe present results related to the KL divergence that will be instrumental when manipulating indexes bk(n). Lemma B.4 gives an upper and a lower bound for the KL divergence. The lower bound is Pinsker’s inequality. The upper bound is due to the fact that I(p, q) is convex in its second argument.\nLemma B.4 For all p, q ∈ [0, 1]2, p ≤ q:\n2(p− q)2 ≤ I(p, q) ≤ (p− q) 2\nq(1− q) . (10)\nand\nI(p, q) ∼ (p− q) 2\nq(1− q) , q → p + (11)\nProof. The lower bound is Pinsker’s inequality. For the upper bound, we have:\n∂I ∂q (p, q) = q − p q(1 − q) .\nSince q 7→ ∂I∂q (p, q) is increasing, the fundamental theorem of calculus gives the announced result:\nI(p, q) ≤ ∫ q\np\n∂I ∂u (p, u) du ≤ (p− q) 2 q(1− q) .\nThe equivalence comes from a Taylor development of q → I(p, q) at p, since:\n∂I ∂q (p, q)|q=p = 0,\n∂2I ∂q2 (p, q)|q=p =\n1\nq(1− q) .\nWe prove a deviation bound similar to that of Lemma B.1 for non-stationary environments.\nLemma B.5 Let k ∈ {1, . . . ,K}, n0 ∈ N and ǫ > 0. Let Λ ⊂ N be a (random) set of instants. Assume that there exists a sequence of (random) sets (Λ(s))s≥1 such that (i) Λ ⊂ ∪s≥1Λ(s), (ii) for all s ≥ 1 and all n ∈ Λ(s), tk(n) ≥ ǫs, and (iii) for all s ≥ 1 |Λ(s)∩[n0, n0+τ ]| ≤ 1. Then for all δ > 0:\nE[\nn0+τ ∑\nn=n0\n1{n ∈ Λ, |µ̂k(n)−E[µ̂k(n)]| > δ}] ≤ log(τ)\n2ǫδ2 +2.\nProof. Fix s0 ≥ 1. We use the following decomposition, depending on the value of s with respect to s0:\n{n ∈ Λ, |µ̂k(n)− E[µ̂k(n)]| > δ} ⊂ A ∪B,\nwhere\nA = {n0, . . . , n0 + τ} ∩ (∪1≤s≤s0Λ(s)), B = {n0, . . . , n0 + τ}\n∩ {n ∈ ∪s≥s0Λ(s) : |µ̂k(n)− E[µ̂k(n)]| > δ}.\nSince for all s, |Λ(s) ∩ {n0, . . . , n0 + τ}| ≤ 1, we have |A| ≤ s0. The expected size of B is upper bounded by:\nE[|B|] ≤ n0+τ ∑\nn=n0\nP[n ∈ ∪s≥s0Λ(s), |µ̂k(n)− E[µ̂k(n)]| > δ]\n≤ n0+τ ∑\nn=n0\nP[|µ̂k(n)− E[µ̂k(n)]| > δ, tk(n) ≥ ǫs0].\nFor a given n, we apply Lemma 4.3 with n− τ in place of n0, and φ = n if tk(n) ≥ ǫs0 and φ = T + 1 otherwise. It is noted that φ is indeed a stopping time. We get:\nP[|µ̂k(n)− E[µ̂k(n)]| > δ, tk(n) ≥ ǫs0] ≤ 2 exp ( −2s0ǫδ2 ) .\nTherefore, setting s0 = log(τ)/(2ǫδ2),\nE[|B|] ≤ 2τ exp ( −2s0ǫδ2 ) = 2.\nFinally we obtain the announced result:\nE[\nn0+τ ∑\nn=n0\n1{n ∈ Λ, |µ̂k(n)−E[µ̂k(n)]| > δ}] ≤ log(τ)\n2ǫδ2 +2.\nLemma B.6 Consider k, k′ ∈ {1, . . . ,K}, n0 ∈ N and ǫ > 0. Let Λ ⊂ N be a (random) set of instants. Assume that there exists a sequence of (random) sets (Λ(s))s≥1 such that (i) Λ ⊂ ∪s≥1Λ(s), and (ii) for all s ≥ 1 and all n ∈ Λ(s), tk(n) ≥ ǫs, tk′(n) ≥ ǫs and (iii) for all s ≥ 1 |Λ(s) ∩ [n0, n0 + τ ]| ≤ 1 and (iv) for all n ∈ Λ, we have E[µ̂k(n)] ≤ E[µ̂k′ (n)]−∆k,k′ . Then for all δ > 0:\nE[\nn0+τ ∑\nn=n0\n1{n ∈ Λ, µ̂k(n) > µ̂k′ (n)}] ≤ 4 log(τ)\nǫ∆2k,k′ + 4."
    }, {
      "heading" : "C. Proofs for stationary environments",
      "text" : "C.1. Proof of Theorem 4.2\nNotations. Throughout the proof, by a slight abuse of notation, we omit the floor/ceiling functions when it does not create ambiguity. Consider a suboptimal arm k 6= k⋆. Define the difference between the average reward of k and k′ : ∆k,k′ = |µk′ − µk| > 0. We use the notation:\ntk,k′ (n) = n ∑\nt=1\n1{L(t) = k, k(t) = k′}.\ntk,k′ (n) is the number of times up time n that k′ has been selected given that k was the leader.\nProof. Let T > 0. The regret ROSUB(T ) of OSUB algorithm up to time T is:\nROSUB(T ) = ∑\nk 6=k⋆\n(µk⋆ − µk)E[ T ∑\nn=1\n1{k(n) = k}].\nWe use the following decomposition:\n1{k(n) = k} = 1{L(n) = k⋆, k(n) = k} +1{L(n) 6= k⋆, k(n) = k}.\nNow\n∑\nk 6=k⋆\n(µk⋆ − µk)E[ T ∑\nn=1\n1{L(n) 6= k⋆, k(n) = k}]\n≤ ∑\nk 6=k⋆\nE[\nT ∑\nn=1\n1{L(n) 6= k⋆, k(n) = k}]\n≤ ∑\nk 6=k⋆\nE[lk(T )].\nObserving that when L(n) = k⋆, the algorithm selects a decision (k, k⋆) ∈ E, we deduce that:\nROSUB(T ) ≤ ∑\nk 6=k⋆\nE[lk(T )]\n+ ∑\n(k,k⋆)∈E\n(µk⋆ − µk)E[ T ∑\nn=1\n1{L(n) = k⋆, k(n) = k}]\nThen we analyze the two terms in the r.h.s. in the above inequality. The first term corresponds to the average number of times where k⋆ is not the leader, while the second term represents the accumulated regret when the leader is k⋆. The following result states that the first term is O(log(log(T ))):\nTheorem C.1 For k 6= k⋆, E[lk(T )] = O(log(log(T ))).\nFrom the above theorem, we conclude that the leader is k⋆ except for a negligible number of instants (in expectation). When k⋆ is the leader, OSUB behaves as KL-UCB restricted to the set N(k⋆) of possible decisions. Following the same analysis as in (Garivier & Cappé, 2011) (the analysis of KL-UCB), we can show that for all ǫ > 0 there are constants C1 ≤ 7 , C2(ǫ) and β(ǫ) > 0 such that:\nE[\nT ∑\nn=1\n1{L(n) = k⋆, k(n) = k}]\n≤ E[ T ∑\nn=1\n1{bk(n) ≥ bk⋆(n)}]\n≤ (1 + ǫ) log(T ) I(µk, µk⋆) + C1 log(log(T )) + C2(ǫ) T β(ǫ) .\n(12)\nCombining the above bound with Theorem C.1, we get:\nROSUB(T ) ≤ (1 + ǫ)c(θ) log(T ) +O(log(log(T ))), (13)\nwhich concludes the proof of Theorem 4.2.\nIt remains to show that Theorem C.1 holds, which is done in the next section. The proof of Theorem C.1 is technical, and requires the concentration inequalities presented in section B. The theorem itself is proved in C.2.\nC.2. Proof of Theorem C.1\nLet k be the index of a suboptimal arm. Let δ > 0, ǫ > 0 small enough (we provide a more precise definition later on). We define k2 = argmaxk′:(k,k′)∈E µk′ the best neighbor of k. To derive an upper bound of E[lk(T )], we decompose the set of times where k is the leader into the following sets: {n ≤ T : L(n) = k} ⊂ Aǫ ∪BTǫ , where\nAǫ = {n : L(n) = k, tk2(n) ≥ ǫlk(n)} BTǫ = {n ≤ T : L(n) = k, tk2(n) ≤ ǫlk(n)}.\nHence we have:\nE[lk(T )] ≤ E [ |Aǫ|+ |BTǫ | ] ,\nNext we provide upper bounds of E[|Aǫ|] and E[|BTǫ |].\nBound on E|Aǫ|. Let n ∈ Aǫ and assume that lk(n) = s. By design of the algorithm, tk(n) ≥ s/(γ + 1). Also tk2(n) ≥ ǫlk(n) = ǫs. We apply Lemma B.2 with Λ(s) = {n ∈ Aǫ, lk(n) = s}, Λ = ∪s≥1Λ(s). Of course, for any s, |Λ(s)| ≤ 1. We have: Aǫ = {n ∈ Λ : µ̂k(n) ≥ µ̂k2(n)}, since when n ∈ Aǫ, k is the leader. Lemma B.2 can be applied with k′ = k2. We get: E|Aǫ| < ∞.\nBound on E|BTǫ |. We introduce the following sets:\n• Cδ is the set of instants at which the average reward of the leader k is badly estimated:\nCδ = {n : L(n) = k, |µ̂k(n)− µk| > δ}.\n• Dδ = ∪k′∈N(k)\\{k2}Dδ,k′ where Dδ,k′ = {n : L(n) = k, k(n) = k′, |µ̂k′ (n) − µk′ | > δ} is the set of instants at which k is the leader, k′ is selected and the average reward of k′ is badly estimated.\n• ET = {n ≤ T : L(n) = k, bk2(n) ≤ µk2}, is the set of instants at which k is the leader, and the upper confidence index bk2(n) underestimates the average reward µk2 .\nWe first prove that |BTǫ | ≤ 2γ(1+γ)(|Cδ|+|Dδ|+|ET |)+ O(1) as T grows large, and then provide upper bounds on E|Cδ|, E|Dδ|, and E|ET |. Let n ∈ BTǫ . When k is the leader, the selected decision is in N(k):\nlk(n) = tk,k2 (n) + ∑\nk′∈N(k)\\{k2}\ntk,k′ (n).\nWe recall that tk,k′ (n) denotes the number of times up to time n when k is the leader and k′ is selected. Since n ∈ BTǫ , tk,k2(n) ≤ ǫlk(n), from which we deduce that:\n(1− ǫ)lk(n) ≤ ∑\nk′∈N(k)\\{k2}\ntk,k′ (n).\nChoose ǫ < 1/(2(γ + 1)). With this choice, from the previous inequality, we must have that either (a) there exists k1 ∈ N(k) \\ {k, k2}, tk,k1 (n) ≥ lk(n)/(γ + 1) or (b) tk,k(n) ≥ (3/2)lk(n)/(γ + 1) + 1.\n(a) Assume that tk,k1(n) ≥ lk(n)/(γ + 1). Since tk,k1 (n) is only incremented when k1 is selected and k is the leader, and since n 7→ lk(n) is increasing, there exists a unique φ(n) < n such that L(φ(n)) = k, k(φ(n)) = k1, tk,k1(φ(n)) = ⌊lk(n)/(2(γ + 1))⌋. φ(n) is indeed unique because tk,k1(φ(n)) is incremented at time φ(n).\nNext we prove by contradiction that for lk(n) ≥ l0 large enough and δ small enough, we must have φ(n) ∈ Cδ ∪ Dδ ∪ ET . Assume that φ(n) /∈ Cδ ∪ Dδ ∪ ET . Then bk2(φ(n)) ≥ µk2 , µ̂k1(φ(n)) ≤ µk1 + δ. Using Pinsker’s inequality and the fact that tk1(φ(n)) ≥ tk,k1 (φ(n)):\nbk1(φ(n)) ≤ µ̂k1(φ(n))\n+\n√\nlog(lk(φ(n))) + c log(log(lk(φ(n))))\n2tk1(φ(n))\n≤ µk1 + δ + √ log(lk(n)) + c log(log(lk(n)))\n2⌊lk(n)/(2(γ + 1))⌋ .\nNow select δ < (µk2 − µk)/2 and l0 such that √\n(log(l0) + c log(log(l0)))/2⌊l0/(2(γ + 1))⌋ ≤ δ. If lk(n) ≥ l0:\nbk1(φ(n)) ≤ µk1 + 2δ < µk2 ≤ bk2(φ(n)),\nwhich implies that k1 cannot be selected at time φ(n) (because bk1(φ(n)) < bk2(φ(n))), a contradiction.\n(b) Assume that tk,k(n) ≥ (3/2)lk(n)/(γ + 1) + 1 = lk(n)/(γ + 1) + lk(n)/(2(γ + 1)) + 1. There are at least lk(n)/(2(γ + 1)) + 1 instants ñ such that lk(ñ) − 1 is not a multiple of 1/(γ + 1), L(ñ) = k and k(ñ) = k. By the same reasoning as in (a) there exists a unique φ(n) < n such that L(φ(n)) = k, k(φ(n)) = k , tk,k(φ(n)) = ⌊lk(n)/(2(γ + 1))⌋ and (lk(φ(n)) − 1) is not a multiple of 1/(γ + 1). So bk(φ(n)) ≥ bk2(φ(n)). The same reasoning as that applied in (a) (replacing k1 by k) yields φ(n) ∈ Cδ ∪Dδ ∪ ET .\nWe define BTǫ,l0 = {n : n ∈ BTǫ , lk(n) ≥ l0}, and we have that |BTǫ | ≤ l0 + |BTǫ,l0 |. We have defined a mapping φ from BTǫ,l0 to Cδ ∪ Dδ ∪ ET . To bound the size of BTǫ,l0 , we use the following decomposition:\n{n : n ∈ BTǫ,l0 , lk(n) ≥ l0} ⊂ ∪n′∈Cδ∪Dδ∪ET {n : n ∈ BTǫ,l0 , φ(n) = n′}.\nLet us fix n′. If n ∈ BTǫ,l0 and φ(n) = n′, then ⌊lk(n)/(2(γ + 1))⌋ ∈ ∪k′∈N(k)\\{k2}{tk,k′(n′)} and lk(n)\nis incremented at time n because L(n) = k. Therefore:\n|{n : n ∈ BTǫ,l0 , φ(n) = n′}| ≤ 2γ(γ + 1).\nUsing union bound, we obtain the desired result:\n|BTǫ | ≤ l0+|BTǫ,l0 | ≤ O(1)+2γ(γ+1)(|Cδ |+|Dδ|+|ET |).\nBound on E|Cδ|. We apply Lemma B.1 with Λ(s) = {n : L(n) = k, lk(n) = s}, and Λ = ∪s≥1Λ(s). Then of course, |Λ(s)| ≤ 1 for all s. Moreover by design, tk(n) ≥ s/(γ + 1) when n ∈ Λ(s), so we can choose any ǫ < 1/(γ + 1) in Lemma B.1. Now Cδ = {n ∈ Λ : |µ̂k(n)− µk| > δ}. From (8), we get E|Cδ| < ∞.\nBound on E|Dδ|. Let k′ ∈ N(k) \\ {k2}. Define for any s, Λ(s) = {n : L(n) = k, k(n) = k′, tk′(n) = s}, and Λ = ∪s≥1Λ(s). We have |Λ(s)| ≤ 1, and for any n ∈ Λ(s), tk′(n) = s ≥ ǫs for any ǫ < 1. We can now apply Lemma B.1 (where k is replaced by k′). Note that Dδ,k′ = {n ∈ Λ : |µ̂k′ (n) − µk′ | > δ}, and hence (8) leads to E|Dδ,k′ | < ∞, and thus E|Dδ| < ∞.\nBound on E|ET |. We can show as in (Garivier & Cappé, 2011) (the analysis of KL-UCB) that E|ET | = O(log(log(T ))) (more precisely, this result is a simple application of Theorem 10 in (Garivier & Cappé, 2011)).\nWe have shown that E|BTǫ | = O(log(log(T ))), and hence E[lk(T )] = O(log(log(T ))), which concludes the proof of Theorem C.1."
    }, {
      "heading" : "D. Proofs for non-stationary environments",
      "text" : "To simplify the notation, we remove the superscript τ throughout the proofs, e.g tτk(n) and l τ k(n) are denoted by tk(n) and lk(n).\nD.1. A lemma for sums over a sliding window\nWe will use Lemma D.1 repeatedly to bound the number of times some events occur over a sliding window of size τ .\nLemma D.1 Let A ⊂ N, and τ ∈ N fixed. Define a(n) = ∑n−1\nt=n−τ 1{t ∈ A}. Then for all T ∈ N and s ∈ N we have the inequality:\nT ∑\nn=1\n1{n ∈ A, a(n) ≤ s} ≤ s⌈T/τ⌉. (14)\nAs a consequence, for all k ∈ {1, . . . ,K}, we have: T ∑\nn=1\n1{k(n) = k, tk(n) ≤ s} ≤ s⌈T/τ⌉, (15)\nT ∑\nn=1\n1{L(n) = k, lk(n) ≤ s} ≤ s⌈T/τ⌉.\nThese inequalities are obtained by choosing A = {n : k(n) = k} and A = {n : L(n) = k} in (14).\nProof. We decompose {1, . . . , T } into intervals of size τ : {1, . . . , τ} , {τ + 1, . . . , 2τ} etc. We have:\nT ∑\nn=1\n1{n ∈ A, a(n) ≤ s}\n≤ ⌈T/τ⌉−1 ∑\ni=0\nτ ∑\nn=1\n1{n+ iτ ∈ A, a(n+ iτ) ≤ s}. (16)\nFix i and assume that ∑τ n=1 1{n + iτ ∈ A, a(n + iτ) ≤ s} > s. Then there must exist n′ < τ such that n′ ∈ A and ∑n′\nn=1 1{n + iτ ∈ A, a(n + iτ) ≤ s} = s. Since a(n′+iτ) ≥ ∑n ′\nn=1 1{n+iτ ∈ A, a(n+iτ) ≤ s}, we have a(n′ + iτ) ≥ s. As n′ ∈ A, we must have a(n′′ + iτ) ≥ (s+ 1) for all n′′ > n′ such that n′′ ∈ A. So\nτ ∑\nn=1\n1{n+ iτ ∈ A, a(n+ iτ) ≤ s}\n=\nn′ ∑\nn=1\n1{n+ iτ ∈ A, a(n+ iτ) ≤ s} = s,\nwhich is a contradiction. Hence, for all i:\nτ ∑\nn=1\n1{n+ iτ ∈ A, a(n+ iτ) ≤ s} ≤ s,\nand substituting in (16) gives the desired result:\nT ∑\nn=1\n1{n ∈ A, a(n) ≤ s} ≤ ⌈T/τ⌉−1 ∑\ni=0\ns = s⌈T/τ⌉.\nD.2. Regret of SW-KL-UCB\nIn order to analyze the regret of SW-OSUB , we first have to analyze the regret SW-KL-UCB on which SW-OSUB is based.\nTheorem D.2 Let ∆: 2τσ < ∆ < ∆0. Assume that for any n ≥ 1, µ⋆(n) ∈ [a, 1 − a] for some a > 0. Further suppose that µk(·) is σ-Lipschitz for any k. The regret per\nunit time under π =SW-KL-UCB with a sliding window of size τ satisfies: if a > στ , then for any T ≥ 1,\nRπ(T )\nT ≤ H(∆, T ) T ∆\n+K (\n1 + g −1/2 0 ) log(τ) + c log(log(τ)) + C1 2τ(∆− 2τσ)2 ,\nwhere C1 is a positive constant and g0 = (a−στ)(1−a+ στ)/2.\nRecall that due to the changing environment and the use of a sliding window, the empirical reward is a biased estimator of the average reward, and that its bias is upper bounded by στ .\nTo ease the regret analysis, we first provide bounds on the empirical reward. Unlike in the stationary case, the empirical reward µ̂k(n) is not a sum of tk(n) i.i.d. variables. We define Xk(n′, n) = Xk(n′)+(µk(n)+σ|n′−n|−µk(n′)) , Xk(n ′, n) = Xk(n ′)+(µk(n)−σ|n′−n|−µk(n′)) and:\nµ̂ k (n) =\n1\ntk(n)\nn ∑\nn′=n−τ\nXk(n ′, n)1{k(n′) = k},\nµ̂k(n) = 1\ntk(n)\nn ∑\nn′=n−τ\nXk(n ′, n)1{k(n′) = k}.\nThen of course, µ̂ k (n) ≤ µ̂k(n) ≤ µ̂k(n).\nNow the regret under π=SW-OSUB is given by:\nRπ(T ) = T ∑\nn=1\nK ∑\nk=1\n(µk⋆(n)− µk(n))P[k(n) = k].\nWe define Imin = 2(∆ − 2τσ)2. Let ǫ > 0 and Kτ = (1 + ǫ) log(τ)+c log(log(τ))Imin . We introduce the following sets of events:\n(i) A = ∪Kk=1Ak, where\nAk = {1 ≤ n ≤ T : k(n) = k, |µk(n)− µk⋆(n)| < ∆},\nAk is the set of times at which k is chosen, and k is ”close” to the optimal decision. Note that, by definition, |A| ≤ H(∆, T ).\n(ii) B = {1 ≤ n ≤ T : bk⋆(n) ≤ µk⋆(n) − τσ}. B is the set of times at which the index bk⋆(n) underestimates the average reward of the optimal decision (with an error greater than the bias τσ).\n(iii) C = ∪Kk=1Ck , Ck = {1 ≤ n ≤ T : k(n) = k, tk(n) ≤ Kτ}. Ck is the set of times at which k is selected and it has been tried less than Kτ times.\n(iv) D = ∪Kk=1Dk, Dk = {1 ≤ n ≤ T : k(n) = k, n /∈ (A∪B ∪C)}. Dk is the set of times where (a) k is chosen, (b) k has been tried more than Kτ times, (c) k is not close to the optimal decision, and (d) the average reward of the optimal decision is not underestimated.\nWe will show that: ∑\nn∈A\n(µ∗(n)− µk(n)(n)) ≤ ∆H(∆, T ). (17)\nand the following inequalities\nE[|B|] ≤ O(T/τ), E[|Ck|] ≤ Kτ ⌈T/τ⌉,\nE[|Dk]] ≤ T\n(τ log(τ)c)g0ǫ2 .\nWe deduce that:\nRπ(T ) ≤ ∆H(∆, T ) +O(T/τ)\n+KKτ ⌊T/τ⌋+ KT (τ log(τ)c)g0ǫ2 ,\nwhich proves Theorem D.2.\nProof of (17). Let n ∈ Ak. If n ∈ Ak, by definition we have |µk⋆(n) − µk(n)| < ∆. Then if k(n) = k, we have that µ∗(n)− µk(n)(n) ≤ ∆ so that:\n∑\nn∈A\n(µ∗(n)− µk(n)(n)) ≤ ∆|A| ≤ ∆H(∆, T ),\nwhich completes the proof of (17).\nBound on E[|B|]. Let n ∈ B. Note that µ̂ k⋆ (n) ≤ µ̂k⋆(n) ≤ bk⋆(n). Since bk⋆(n) ≤ µk⋆(n)−στ , we deduce that: µ̂\nk⋆ (n) ≤ µk⋆(n)− στ . Now we have:\nP[n ∈ B] = P[bk⋆(n) ≤ µk⋆(n)− στ ] = P[tk⋆(n)I (µ̂k⋆(n), µk⋆(n)− στ)\n≥ log(τ) + c log(log(τ))] (a) ≤ P[tk⋆(n)I (\nµ̂ k⋆ (n), µk⋆(n)− στ\n)\n≥ log(τ) + c log(log(τ))] (b) ≤ 2e τ(log(τ))c−2 ,\nwhere (a) is due to the fact that µ̂ k⋆ (n) ≤ µ̂k⋆(n), and (b) is obtained applying Lemma B.3. Hence: E[|B|] ≤ O(T/τ).\nBound on E[|Ck|]. Using Lemma D.1, we get |Ck| ≤ Kτ ⌈T/τ⌉, and hence |C| ≤ KKτ ⌊T/τ⌋.\nBound on E[|Dk|]. We will prove that n ∈ Dk implies that µ̂k(n) deviates from its expectation by at least f(ǫ, Imin) > 0 so that:\nP[n ∈ Dk] ≤ P [ µ̂k(n)− E[µ̂k(n)] > f(ǫ, Imin) ] .\nLet n ∈ Dk. Since k(n) = k and bk⋆(n) ≥ µk⋆(n) − στ , we have bk(n) ≥ µk⋆(n) − στ . We decompose Dk as follows:\nDk = Dk,1 ∪Dk,2 Dk,1 = {n ∈ Dk : µ̂k(n) ≥ µk⋆(n)− στ} Dk,2 = {n ∈ Dk : µ̂k(n) ≤ µk⋆(n)− στ}\nIf n ∈ Dk,1, µ̂k(n)−E[µ̂k(n)] ≥ µk⋆(n)−µk(n)−2στ > 0 so that µ̂k(n) indeed deviates from its expectation. Now let n ∈ Dk,2. We have:\nP[n ∈ Dk,2] ≤ P[bk(n) ≥ µk⋆(n)− στ, n ∈ Dk,2] = P[tk(n)I (µ̂k(n), µk⋆(n)− στ)\n≤ log(τ) + c log(log(τ)), n ∈ Dk,2] (a) ≤ P[KτI (\nµ̂k(n), µk⋆(n)− στ )\n≤ log(τ) + c log(log(τ)), tk(n) ≥ Kτ ]\n= P\n[\nI ( µ̂k(n), µk⋆(n)− στ ) ≤ Imin 1 + ǫ , tk(n) ≥ Kτ ] ,\nwhere in (a), we used the facts that: µ̂k(n) ≤ µk⋆(n)−στ , µ̂k(n) ≥ µ̂k(n), and tk(n) ≥ Kτ (n /∈ C). It is noted that since n /∈ Ak, by Pinkser’s inequality we have that: I(µk(n) + τσ, µk⋆ (n) − τσ) ≥ 2(µk⋆(n) − µk(n) − 2τσ)2 ≥ 2(∆ − 2τσ)2 = Imin. By continuity and monotonicity of the KL divergence, there exists a unique positive function f such that:\nI (µk(n) + στ + f(ǫ, Imin), µk⋆(n)− στ) = Imin 1 + ǫ ,\nµk(n) + στ + f(ǫ, Imin) ≤ µk⋆(n)− στ.\nWe are interested in the asymptotic behavior of f when ǫ , Imin both tend to 0 . Define µ′ , µ′′ and µ0 such that\nµk(n) + στ ≤ µ′ ≤ µ′′ ≤ µ0 = µk⋆(n)− στ.\nand\nI(µ′, µ0) = Imin , I(µ ′′, µ0) = Imin 1 + ǫ .\nUsing the equivalent (11) given in Lemma B.4, there exists a function a such that:\n(µ0 − µ′)2 µ0(1− µ0) (1 + a(µ0 − µ′)) = Imin,\n(µ0 − µ′′)2 µ0(1 − µ0) (1 + a(µ0 − µ′′)) = Imin 1 + ǫ .\nwith a(δ) → 0 when δ → 0+. It is noted that 0 ≤ µ0 − µ′′ ≤ µ0 − µ′ = o(1) when Imin → 0+ by continuity of the KL divergence. Hence:\nµ′′ − µ′ = ( ǫ\n2 + o(1)\n)\n√\nµ0(1− µ0)Imin.\nUsing the inequality\nf(ǫ, Imin) = µ ′′ − (µk(n) + στ)\n≥ µ′′ − µ′ = ǫ 2 √ µ0(1 − µ0)Imin,\nwe have proved that:\n2f(ǫ, Imin) 2 ≥ ǫ2g0Imin + o(ǫ2)\nwith g0 = (a− στ)(1 − a+ στ)/2.\nTherefore, since E[µ̂k(n)] ≤ µk(n) + στ , as claimed, we have\nP[n ∈ Dk] ≤ P [\nµ̂k(n)− E[µ̂k(n)] ≥ f(ǫ, Imin) , tk(n) ≥ Kτ ] .\nWe now apply Lemma 4.3 with n − τ in place of n0, Kτ in place of s and φ = n if tk(n) ≥ Kτ and φ = T + 1 otherwise. We obtain, for all n:\nP[n ∈ Dk] ≤ P [\nµ̂k(n)− E[µ̂k(n)] ≥ f(ǫ, Imin), tk(n) ≥ Kτ ]\n≤ exp ( −2Kτf(ǫ, Imin)2 ) ≤ 1 (τ log(τ)c)g0ǫ2 ,\nand we get the desired bound by summing over n:\nE[|Dk|] = T ∑\nn=1\nP[n ∈ Dk] ≤ T\n(τ log(τ)c)g0ǫ2 .\nD.3. Proof of Theorem 5.1\nWe first introduce some notations. For any set A of instants, we use the notation: A[n0, n] = A∩{n0, . . . , n0+ τ}. Let n0 ≤ n. We define tk(n0, n) the number of times k has been chosen during interval {n0, . . . , n0 + τ}, lk(n0, n) the number of times k has been the leader, and tk,k′ (n0, n) the number of times k′ has been chosen while k was the leader:\ntk(n0, n) = n ∑\nn′=n0\n1{k(n′) = k},\nlk(n0, n) = n ∑\nn′=n0\n1{L(n′) = k},\ntk,k′ (n0, n) = n ∑\nn′=n0\n1{L(n′) = k, k(n′) = k′}.\nNote that lk(n − τ, n) = lk(n), tk(n − τ, n) = tk(n) and tk,k′ (n − τ, n) = tk,k′ (n). Given ∆ > 0, we define the\nset of instants at which the average reward of k is separated from the average reward of its neighbours by at least ∆:\nNk(∆) = ∩(k′,k)∈E{n : |µk(n)− µk′(n)| > ∆}.\nWe further define the amount of time that k is suboptimal, k is the leader, and it is well separated from its neighbors:\nLk(∆) = {n : L(n) = k 6= k⋆(n), n ∈ Nk(∆)}.\nBy definition of the regret under π =SW-OSUB :\nRπ(T ) =\nT ∑\nn=1\n∑\nk 6=k⋆(n)\n(µk⋆(n)− µk(n))P[k(n) = k].\nTo bound the regret, as in the stationary case, we split the regret into two components: the regret accumulated when the leader is the optimal arm, and the regret generated when the leader is not the optimal arm. The regret when the leader is suboptimal satisfies:\nT ∑\nn=1\n∑\nk 6=k⋆(n)\n(µk⋆(n)− µk)1{k(n) = k, L(n) 6= k⋆(n)}\n≤ T ∑\nn=1\n1{L(n) 6= k⋆(n)}\n≤ T ∑\nn=1\n∑\nk 6=k⋆(n)\n1{L(n) = k 6= k⋆(n)}\n≤ T ∑\nn=1\n∑\nk 6=k⋆(n)\n1{n ∈ Lk(∆)}\n+ 1{∃k′ : (k, k⋆) ∈ E : |µk(n)− µk′(n)| ≤ ∆} ≤ ( K ∑\nk=1\n|Lk(∆)[0, T ]|+H(∆, T ) ) .\nTherefore the regret satisfies:\nRπ(T ) ≤ ( H(∆, T ) + K ∑\nk=1\nE[|Lk(∆)[0, T ]|] )\n+\nT ∑\nn=1\n∑\n(k,k⋆(n))∈E\n(µk⋆(n)− µk(n))P[k(n) = k].\n(18)\nThe second term of the r.h.s in (18) is the regret of SWOSUB when k⋆(n) is the leader. This term can be analyzed using the same techniques as those used for the analysis of SW-KL-UCB and is upper bounded by the regret of SWKL-UCB. It remains to bound the first term of the r.h.s in (18).\nTheorem D.3 Consider ∆ > 4τσ. Then for all k:\nE[|Lk(∆)[0, T ]|] ≤ C1 × T log(τ)\nτ(∆ − 4τσ)2 , (19)\nwhere C1 > 0 does not depend on T , τ , σ and ∆.\nSubstituting (19) in (18), we obtain the announced result.\nD.4. Proof of Theorem D.3\nIt remains to prove Theorem D.3. Define δ = (∆−4τσ)/2. We can decompose {1, . . . , T } into at most ⌈T/τ⌉ intervals of size τ . Therefore, to prove the theorem, it is sufficient to prove that for all n0 ∈ Lk(∆) we have:\nE[|Lk(∆)[n0, n0 + τ ]|] ≤ O ( log(τ)\nδ2\n)\n.\nIn the remaining of the proof, we consider an interval {n0, . . . , n0 + τ}, with n0 ∈ Lk(∆) fixed. It is noted that the best neighbour of k changes with time. We define k2(n) the best neighbor of k at time n. From the Lipschitz assumption and the fact that ∆ > 4τσ, we have that for all n ∈ {n0, . . . , n0 + τ}, k2(n) = k2(n0). Indeed for all n ∈ {n0, . . . , n0 + τ}:\nµk2(n0)(n)− µk(n) ≥ µk2(n0)(n0)− µk(n0)− 2(n− n0)σ ≥ ∆− 2τσ ≥ 2τσ > 0.\nWe write k2 = k2(n0) = k2(n) when this does not create ambiguity. We will use the fact that, for all n ∈ {n0, . . . , n0 + τ}:\nE[µ̂k2(n)]− E[µ̂k(n)] ≥ µk2(n)− µk(n)− 2τσ, ≥ µk2(n0)− µk(n0)− 4τσ, ≥ ∆− 4τσ = 2δ > 0.\nWe decompose Lk(∆)[n0, n0 + τ ] = An0ǫ ∪Bn0ǫ , with:\nAn0ǫ = {n ∈ Lk(∆)[n0, n0 + τ ], tk2(n) ≥ ǫlk(n0, n)} the set of times where k is the leader, k is not the optimal arm, and its best neighbor k2 has been tried sufficiently many times during interval {n0, . . . , n0 + τ},\nBn0ǫ = {n ∈ Lk(∆)[n0, n0 + τ ], tk2(n) ≤ ǫlk(n0, n)} the set of times where k is the leader, k is not the optimal arm, and its best neighbor k2 has been little tried during interval {n0, . . . , n0 + τ}.\nBound on E[An0ǫ ]. Let n ∈ An0ǫ . We recall that E[µ̂k2(n)] − E[µ̂k(n)] ≥ 2δ, so that the reward of k or k2 must be badly estimated at time n:\nP[n ∈ An0ǫ ] ≤ P[|µ̂k(n)− E[µ̂k(n)]| > δ] + P[|µ̂k2(n)− E[µ̂k2(n)]| > δ].\nWe apply Lemma B.6, with k′ = k2, ∆k,k′ = 2δ, Λ(s) = {n ∈ An0ǫ , lk(n0, n) = s}, tk2(n) ≥ ǫlk(n0, n) = ǫs. By design of SW-OSUB : tk(n) ≥ lk(n0, n)/(γ + 1) = s/(γ+1). Using the fact that |Λ(s)| ≤ 1 for all s, we have that:\nE[An0ǫ ] ≤ O ( log(τ)\nǫδ2\n)\n.\nBound on E[Bn0ǫ ]. Define l0 such that\n√\nlog(l0) + c log(log(l0))\n2⌊l0/(2(γ + 1))⌋ ≤ δ.\nIn particular we can choose l0 = 2(γ + 1)(log(1/δ)/δ2). Indeed, with such a choice we have that\n√\nlog(l0) + c log(log(l0))\n2⌊l0/(2(γ + 1))⌋ ∼ δ/2 , δ → 0+.\nLet ǫ < 1/(2(γ + 1)), and define the following sets:\nCn0δ is the set of instants at which the average reward of the leader k is badly estimated:\nCn0δ = {n ∈ {n0, . . . , n0 + τ} : L(n) = k 6= k⋆(n), |µ̂k(n)− E[µ̂k(n)]| > δ};\nDn0δ = ∪k′∈N(k)\\{k2}Dn0δ,k′ where Dn0δ,k′ = {n : L(n) = k 6= k⋆(n), k(n) = k′, |µ̂k′ (n) − E[µ̂k′ (n)]| > δ}. Dn0δ is the set of instants at which k is the leader, k′ is selected and the average reward of k′ is badly estimated.\nEn0 = {n ≤ T : L(n) = k 6= k⋆(n), bk2(n) ≤ E[µ̂k2(n)]} is the set of instants at which k is the leader, and the upper confidence index bk2(n) underestimates the average reward E[µ̂k2 (n)].\nLet n ∈ Bn0ǫ . Write s = lk(n0, n), and we assume that s ≥ l0. Since tk2(n0, n) ≤ ǫlk(n0, n) and the fact that lk(n0, n) = tk2(n0, n) + ∑ k′∈N(k)\\{k2} tk′(n0, n), we must have (a) there exists k1 ∈ N(k) \\ {k, k2} such that tk1(n0, n) ≥ s/(γ + 1) or (b) tk1(n0, n) ≥ (3/2)s/(γ + 1) + 1. Since tk,k(n) and tk,k2(n) are incremented only at times when k(n) = k and k(n) = k2 respectively,\nthere must exist a unique index φ(n) ∈ {n0, . . . , n0 + τ} such that either: (a) tk,k1(φ(n)) = ⌊s/(2(γ + 1))⌋ and k(φ(n)) = k1; or (b) tk,k2(φ(n)) = ⌊(3/2)s/(γ + 1)⌋ and k(n) = k and lk(φ(n)) is not a multiple of 3. In both cases, as in the proof of theorem C.1, we must have that φ(n) ∈ Cn0δ ∪Dn0δ ∪En0 . We now upper bound the number of instants n which are associated to the same φ(n). Let n, n′ ∈ Bn0ǫ and s = lk(n0, n). We see that φ(n′) = φ(n) implies either ⌊lk(n0, n′)/(2(γ + 1))⌋ = ⌊lk(n0, n)/(2(γ + 1))⌋ or ⌊(3/2)lk(n0, n′)/(γ + 1)⌋ = ⌊(3/2)lk(n0, n)/(γ + 1)⌋. Furthermore, n′ 7→ lk(n0, n′) is incremented at time n′. Hence for all n ∈ Bn0ǫ :\n|n′ ∈ Bn0ǫ , φ(n′) = φ(n)| ≤ 2γ(γ + 1). We have established that:\n|Bn0ǫ | ≤ l0 + 2γ(γ + 1)(|Cn0δ |+ |Dn0δ |+ |En0 |) = 2(γ + 1) log(1/δ)/δ2\n+ 2γ(γ + 1)(|Cn0δ |+ |Dn0δ |+ |En0 |). We complete the proof by providing bounds of the expected sizes of sets Cn0δ , D n0 δ and E n0 .\nBound of E[Cn0δ ]: Using Lemma B.5 with Λ(s) = {n ∈ Cn0δ , lk(n0, n) = s}, and by design of SW-OSUB : tk(n) ≥ lk(n0, n)/(γ + 1) = s/(γ + 1). Since |Λ(s)| ≤ 1 for all s, we have that:\nE[|Cn0δ |] ≤ O ( log(τ)\nδ2\n)\n.\nBound of E[Dn0δ ]: Using Lemma B.5 with Λ(s) = {n ∈ Dn0δ , tk,k′(n0, n) = s}, and |Λ(s)| ≤ 1 for all s, we have that:\nE[|Dn0δ,k′ |] ≤ O ( log(τ)\nδ2\n)\n.\nBound of E[En0 ]: By Lemma B.3 since lk(n) ≤ τ :\nP[n ∈ En0 ] ≤ 2e⌈log(τ)(log(τ) + c log(log(τ)))⌉ exp(− log(τ) + c log(log(τ)))\n≤ 4e τ log(τ)c−2 .\nThus\nE[|En0 |] ≤ 4e (log τ)c−2 .\nPutting the various bounds all together, we have:\nE[|Lk(∆)[n0, n0 + τ ]|] ≤ O ( log(τ)\nδ2\n)\n,\nfor all n0 ∈ Lk(∆), uniformly in δ, which concludes the proof."
    }, {
      "heading" : "E. Proof of Proposition 1",
      "text" : "The regret of UCB(δ) is defined as:\nRπ(T ) ≤ ⌈1/δ⌉ ∑\nk=1\nE[tk(T )](µ ∗ − µk).\nWe separate the arms into three different sets. {1, . . . , ⌈1/δ⌉} = A∪B∪C, with: A = {k∗−1, k∗, k∗+1} the optimal arm and its neighbors, B = {k : k /∈ A, (k − 1)δ ∈ [x∗ − δ0, x∗ + δ0]} the arms which are not neighbors of the optimal arm, but are in [x∗ − δ0, x∗ + δ0], and C = {k : (k − 1)δ /∈ [x∗ − δ0, x∗ + δ0]} the rest of the arms.\nWe consider δ < δ0/3, so that A ⊂ [x∗ − δ0, x∗ + δ0]. By our assumption on the reward function, if k ∈ A, |x∗ − δ(k − 1)| ≤ 2δ then |µ∗ − µk| ≤ C2(2δ)α. The regret is upper bounded by:\nRπ(T ) ≤ TC2(2δ)α + ∑\nk∈B∪C\nE[tk(T )](µ ∗ − µk).\nUsing the fact that µ∗ − µk∗ ≤ C2δα and ∑⌈1/δ⌉\nk=1 E[tk(T )] ≤ T , the bound becomes:\nRπ(T ) ≤ TC2(3δ)α + ∑\nk∈B∪C\nE[tk(T )](µk∗ − µk).\nBy (Auer et al., 2002) (the analysis of UCB), for all k, E[tk(T )] ≤ 8 log(T )/(µk∗ − µk)2. Replacing in the regret upper bound:\nRπ(T ) ≤ TC2(3δ)α + ∑\nk∈B∪C\n8 log(T )/(µk∗ − µk).\nIf k ∈ B, |δ(k∗ − 1) − δ(k − 1)| ≥ δ(|k∗ − k| − 1), so µk∗ − µk ≥ C1δα(|k∗ − k| − 1)α. If k ∈ C , then |δ(k∗ − 1)− δ(k− 1)| ≥ δ0/2, so µk∗ −µk ≥ C1(δ0/2)α. So the regret for arms in B ∪ C reduces to:\nRπ(T ) ≤ TC2(3δ)α + 8 log(T )⌈1/δ⌉ C1(δ0/2)α +2\n⌈1/δ⌉ ∑\nk=1\n8 log(T ) C1(δk)α .\nUsing a sum-integral comparison: ∑⌈1/δ⌉ k=1 k −α ≤ ∑⌈1/δ⌉ k=1 k\n−1 ≤ 1 + log(⌈1/δ⌉), so that: Rπ(T ) ≤ TC2(3δ)α\n+ 8 log(T ) ( ⌈1/δ⌉ C1(δ0/2)α + 2(1 + log(⌈1/δ⌉)) C1δα ) .\nSetting δ = (log(T )/ √ T )1/α, the regret becomes:\nRπ(T ) ≤ TC2(3α)(log(T )/ √ T )+\n8 log(T )\n( ⌈( √ T/ log(T ))1/α⌉ C1(δ0/2)α + 2(1 + log(T )) C1 log(T )/ √ T ) .\nwe have used the fact that ⌈1/δ⌉ ≤ T .\nRπ(T ) ≤ C2(3α) log(T ) √ T\n+8\n( √ T + 1\nC1(δ0/2)α +\n2 √ T (1 + log(T ))\nC1\n)\nLetting T → ∞ gives the result:\nlim sup T\nRπ(T )/( √ T log(T )) ≤ C23α + 16/C1."
    } ],
    "references" : [ {
      "title" : "The continuum-armed bandit problem",
      "author" : [ "R. Agrawal" ],
      "venue" : "SIAM J. Control and Optimization,",
      "citeRegEx" : "Agrawal,? \\Q1995\\E",
      "shortCiteRegEx" : "Agrawal",
      "year" : 1995
    }, {
      "title" : "Finite time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Strategic bidder behavior in sponsored search auctions",
      "author" : [ "Edelman" ],
      "venue" : "In Proc. of Workshop on Sponsored Search Auctions, ACM Electronic Commerce,",
      "citeRegEx" : "B. and Edelman.,? \\Q2005\\E",
      "shortCiteRegEx" : "B. and Edelman.",
      "year" : 2005
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi",
      "year" : 2012
    }, {
      "title" : "Online optimization in x-armed bandits",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz", "C. Szepesvári" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2008
    }, {
      "title" : "Kullback-leibler upper confidence bounds for optimal sequential allocation",
      "author" : [ "O. Cappé", "A. Garivier", "O. Maillard", "R. Munos", "G. Stoltz" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Cappé et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cappé et al\\.",
      "year" : 2013
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi",
      "year" : 2006
    }, {
      "title" : "Regret and convergence bounds for a class of continuum-armed bandit problems",
      "author" : [ "E.W. Cope" ],
      "venue" : "IEEE Trans. Automat. Contr.,",
      "citeRegEx" : "Cope,? \\Q2009\\E",
      "shortCiteRegEx" : "Cope",
      "year" : 2009
    }, {
      "title" : "Stochastic linear optimization under bandit feedback",
      "author" : [ "V. Dani", "T.P. Hayes", "S.M. Kakade" ],
      "venue" : "In Proc. of Conference On Learning Theory (COLT),",
      "citeRegEx" : "Dani et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2008
    }, {
      "title" : "Online convex optimization in the bandit setting: gradient descent without a gradient",
      "author" : [ "A. Flaxman", "A.T. Kalai", "H.B. McMahan" ],
      "venue" : "In Proc. of ACM/SIAM symposium on Discrete Algorithms (SODA), pp",
      "citeRegEx" : "Flaxman et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Flaxman et al\\.",
      "year" : 2005
    }, {
      "title" : "The KL-UCB algorithm for bounded stochastic bandits and beyond",
      "author" : [ "A. Garivier", "O. Cappé" ],
      "venue" : "In Proc. of Conference On Learning Theory (COLT),",
      "citeRegEx" : "Garivier and Cappé,? \\Q2011\\E",
      "shortCiteRegEx" : "Garivier and Cappé",
      "year" : 2011
    }, {
      "title" : "On upper-confidence bound policies for non-stationary bandit problems",
      "author" : [ "A. Garivier", "E. Moulines" ],
      "venue" : "In Proc. of Algorithmic Learning Theory (ALT),",
      "citeRegEx" : "Garivier and Moulines,? \\Q2008\\E",
      "shortCiteRegEx" : "Garivier and Moulines",
      "year" : 2008
    }, {
      "title" : "Bandit Processes and Dynamic Allocation Indices",
      "author" : [ "J.C. Gittins" ],
      "venue" : "John Wiley,",
      "citeRegEx" : "Gittins,? \\Q1989\\E",
      "shortCiteRegEx" : "Gittins",
      "year" : 1989
    }, {
      "title" : "Asymptotically efficient adaptive choice of control laws in controlled markov chains",
      "author" : [ "T.L. Graves", "T.L. Lai" ],
      "venue" : "SIAM J. Control and Optimization,",
      "citeRegEx" : "Graves and Lai,? \\Q1997\\E",
      "shortCiteRegEx" : "Graves and Lai",
      "year" : 1997
    }, {
      "title" : "Change point detection and meta-bandits for online learning in dynamic environments",
      "author" : [ "C. Hartland", "N. Baskiotis", "S. Gelly", "O. Teytaud", "M. Sebag" ],
      "venue" : "In Proc. of conférence francophone sur l’apprentissage automatique (CAp07),",
      "citeRegEx" : "Hartland et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hartland et al\\.",
      "year" : 2007
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "W. Hoeffding" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Hoeffding,? \\Q1963\\E",
      "shortCiteRegEx" : "Hoeffding",
      "year" : 1963
    }, {
      "title" : "Multi-armed bandits in metric spaces",
      "author" : [ "R. Kleinberg", "A. Slivkins", "E. Upfal" ],
      "venue" : "In Proc. of the 40th annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Kleinberg et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kleinberg et al\\.",
      "year" : 2008
    }, {
      "title" : "Nearly tight bounds for the continuumarmed bandit problem",
      "author" : [ "R.D. Kleinberg" ],
      "venue" : "In Proc. of the conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Kleinberg,? \\Q2004\\E",
      "shortCiteRegEx" : "Kleinberg",
      "year" : 2004
    }, {
      "title" : "Adaptive treatment allocation and the multiarmed bandit problem",
      "author" : [ "T.L. Lai" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Lai,? \\Q1987\\E",
      "shortCiteRegEx" : "Lai",
      "year" : 1987
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Lai and Robbins,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai and Robbins",
      "year" : 1985
    }, {
      "title" : "Some aspects of the sequential design of experiments",
      "author" : [ "H. Robbins" ],
      "venue" : "Bulletin of the American Mathematical Society,",
      "citeRegEx" : "Robbins,? \\Q1952\\E",
      "shortCiteRegEx" : "Robbins",
      "year" : 1952
    }, {
      "title" : "Adapting to a changing environment: the brownian restless bandits",
      "author" : [ "A. Slivkins", "E. Upfal" ],
      "venue" : "In Proc. of Conference On Learning Theory (COLT),",
      "citeRegEx" : "Slivkins and Upfal,? \\Q2008\\E",
      "shortCiteRegEx" : "Slivkins and Upfal",
      "year" : 2008
    }, {
      "title" : "Bound on E|ET |. We can show as in (Garivier & Cappé, 2011) (the analysis of KL-UCB) that E|ET | = O(log(log(T ))) (more precisely, this result is a simple application of Theorem 10 in (Garivier",
      "author" : [ "E|Dδ" ],
      "venue" : null,
      "citeRegEx" : "∞.,? \\Q2011\\E",
      "shortCiteRegEx" : "∞.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "This important class of problems has been recently investigated in (Cope, 2009; Yu & Mannor, 2011).",
      "startOffset" : 67,
      "endOffset" : 98
    }, {
      "referenceID" : 20,
      "context" : "Introduction Stochastic Multi-Armed Bandits (MAB) (Robbins, 1952; Gittins, 1989) constitute the most fundamental sequential decision problems with an exploration vs.",
      "startOffset" : 50,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "Introduction Stochastic Multi-Armed Bandits (MAB) (Robbins, 1952; Gittins, 1989) constitute the most fundamental sequential decision problems with an exploration vs.",
      "startOffset" : 50,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "The most popular of these algorithms are UCB (Auer et al., 2002) and its extensions, e.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "KL-UCB (Garivier & Cappé, 2011; Cappé et al., 2013) (note that KL-UCB algorithm was initially proposed in (Lai, 1987), see (2.",
      "startOffset" : 7,
      "endOffset" : 51
    }, {
      "referenceID" : 18,
      "context" : ", 2013) (note that KL-UCB algorithm was initially proposed in (Lai, 1987), see (2.",
      "startOffset" : 62,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz (Agrawal, 1995; Kleinberg et al., 2008; Bubeck et al., 2008), linear (Dani et al.",
      "startOffset" : 12,
      "endOffset" : 72
    }, {
      "referenceID" : 16,
      "context" : ", Lipschitz (Agrawal, 1995; Kleinberg et al., 2008; Bubeck et al., 2008), linear (Dani et al.",
      "startOffset" : 12,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : ", Lipschitz (Agrawal, 1995; Kleinberg et al., 2008; Bubeck et al., 2008), linear (Dani et al.",
      "startOffset" : 12,
      "endOffset" : 72
    }, {
      "referenceID" : 8,
      "context" : ", 2008), linear (Dani et al., 2008), convex (Flaxman et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : ", 2008), convex (Flaxman et al., 2005).",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : "They are specific instances of bandits in metric spaces (Kleinberg, 2004; Kleinberg et al., 2008; Bubeck et al., 2008).",
      "startOffset" : 56,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : "They are specific instances of bandits in metric spaces (Kleinberg, 2004; Kleinberg et al., 2008; Bubeck et al., 2008).",
      "startOffset" : 56,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "They are specific instances of bandits in metric spaces (Kleinberg, 2004; Kleinberg et al., 2008; Bubeck et al., 2008).",
      "startOffset" : 56,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "Unimodal bandits have been specifically addressed in (Cope, 2009; Yu & Mannor, 2011).",
      "startOffset" : 53,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "In (Cope, 2009), bandits with a continuous set of arms are studied, and the author shows that the Kiefer-Wolfowitz stochastic approximation algorithm achieves a regret of the order of O( √ T ) under some strong regularity assumptions on the reward function.",
      "startOffset" : 3,
      "endOffset" : 15
    }, {
      "referenceID" : 14,
      "context" : "(Hartland et al., 2007; Garivier & Moulines, 2008; Slivkins & Upfal, 2008; Yu & Mannor, 2011).",
      "startOffset" : 0,
      "endOffset" : 93
    }, {
      "referenceID" : 18,
      "context" : "The algorithm is based on KL-UCB proposed in (Lai, 1987; Cappé et al., 2013), and uses KL-divergence upper confidence bounds to define an index for each arm.",
      "startOffset" : 45,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "The algorithm is based on KL-UCB proposed in (Lai, 1987; Cappé et al., 2013), and uses KL-divergence upper confidence bounds to define an index for each arm.",
      "startOffset" : 45,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "OSUB can be readily extended to systems where reward distributions are within one-parameter exponential families by simply modifying the definition of arm indices as done in (Cappé et al., 2013).",
      "startOffset" : 174,
      "endOffset" : 194
    }, {
      "referenceID" : 5,
      "context" : "(i) When k is the leader, the algorithm behaves like KL-UCB restricted to the arms around k, and the regret at these rounds can be analyzed as in (Cappé et al., 2013).",
      "startOffset" : 146,
      "endOffset" : 166
    }, {
      "referenceID" : 17,
      "context" : "We show that combining discretization and the UCB algorithm as initially proposed in (Kleinberg, 2004) yields lower regrets than LSE in practice (see Section 7), and is orderoptimal, i.",
      "startOffset" : 85,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "We denote by UCB(δ) the UCB algorithm (Auer et al., 2002) applied to the discretized bandit.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "Discrete bandits We compare the performance of our algorithm to that of KL-UCB (Cappé et al., 2013), LSE (Yu & Mannor, 2011), UCB (Auer et al.",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 1,
      "context" : ", 2013), LSE (Yu & Mannor, 2011), UCB (Auer et al., 2002), and UCB-U.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 15,
      "context" : "As proven by Hoeffding (Hoeffding, 1963)[eq.",
      "startOffset" : 23,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "By (Auer et al., 2002) (the analysis of UCB), for all k, E[tk(T )] ≤ 8 log(T )/(μk∗ − μk).",
      "startOffset" : 3,
      "endOffset" : 22
    } ],
    "year" : 2014,
    "abstractText" : "We consider stochastic multi-armed bandits where the expected reward is a unimodal function over partially ordered arms. This important class of problems has been recently investigated in (Cope, 2009; Yu & Mannor, 2011). The set of arms is either discrete, in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards, or continuous, in which case arms belong to a bounded interval. For discrete unimodal bandits, we derive asymptotic lower bounds for the regret achieved under any algorithm, and propose OSUB, an algorithm whose regret matches this lower bound. Our algorithm optimally exploits the unimodal structure of the problem, and surprisingly, its asymptotic regret does not depend on the number of arms. We also provide a regret upper bound for OSUB in nonstationary environments where the expected rewards smoothly evolve over time. The analytical results are supported by numerical experiments showing that OSUB performs significantly better than the state-of-the-art algorithms. For continuous sets of arms, we provide a brief discussion. We show that combining an appropriate discretization of the set of arms with the UCB algorithm yields an order-optimal regret, and in practice, outperforms recently proposed algorithms designed to exploit the unimodal structure. Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).",
    "creator" : "LaTeX with hyperref package"
  }
}