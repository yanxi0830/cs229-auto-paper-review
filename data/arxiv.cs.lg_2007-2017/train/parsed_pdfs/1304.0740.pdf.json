{
  "name" : "1304.0740.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "O(logT ) Projections for Stochastic Optimization of Smooth and Strongly Convex Functions",
    "authors" : [ "Lijun Zhang", "Tianbao Yang", "Rong Jin", "Xiaofei He", "Zhang Yang", "Jin He" ],
    "emails" : [ "zhanglij@msu.edu", "tyang@ge.com", "rongjin@cse.msu.edu", "xiaofeihe@cad.zju.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 4.\n07 40\nv1 [\nKeywords: Epoch gradient descent, extra-gradient descent, mini-batch, strongly convex, smooth"
    }, {
      "heading" : "1. Introduction",
      "text" : "The goal of stochastic optimization is to solve the optimization problem\nmin w∈D F (w),\nusing only the stochastic gradients of F (w). In particular, we assume there exists a gradient oracle, which for any point w ∈ D, returns a random vector ĝ(w) that gives an unbiased estimate of the subgradient of F (·) at w. A special case of stochastic optimization is the risk minimization problem, whose objective function is given by\nF (w) = E(x,y) [ℓ(w; (x, y))] ,\nc© 2013 L. Zhang, T. Yang, R. Jin & X. He.\nwhere (x, y) is an instance-label pair, ℓ is a convex loss function that measures the prediction error, and the expectation is taken oven the unknown joint distribution of (x, y) (Zhang, 2004; Shalev-Shwartz et al., 2009; Hu et al., 2009). The performance of stochastic optimization algorithms is typically characterized by the excess risk\nF (wT )− min w∈D F (w),\nwhere T is the number of iterations and wT is the solution obtained after making T calls to the gradient oracle.\nFor general Lipschitz continuous convex functions, stochastic gradient descent exhibits the unimprovable O(1/ √ T ) rate of convergence (Nemirovski and Yudin, 1983; Nemirovski et al., 2009). For strongly-convex functions, the algorithms proposed in very recent works (Juditsky and Nesterov, 2010; Hazan and Kale, 2011; Rakhlin et al., 2012; Chen et al., 2012) achieve the optimal O(1/T ) rate (Agarwal et al., 2012). Although these convergence rates are significantly worse than the results in deterministic optimization, stochastic optimization is appealing due to its low per-iteration complexity. However, this is not the case when the domain D is complex. This is because most stochastic optimization algorithms require projecting the solution at each iteration into domain D to ensure its feasibility, an expensive operation when the domain is complex. In this paper, we show that if the objective function is smooth and strongly convex, it is possible to reduce the number of projections dramatically without affecting the convergence rate.\nOur work is motivated by the difference in convergence rates between stochastic and deterministic optimization. When the objective function is smooth and convex, under the first-order oracle assumption, Nesterov’s accelerated gradient method enjoys the optimal O(1/T 2) rate (Nesterov, 2004, 2005). Thus, for deterministic optimization of smooth and convex functions, we can achieve an O(1/ √ T ) rate by only performing O(T 1/4) updating. When the objective function is smooth and strongly convex, the optimal rate for first-order algorithms is O(1/αk), for some constant α > 1 (Nesterov, 2004, 2007). In other words, for deterministic optimization of smooth and strongly convex functions, we can achieve an O(1/T ) rate by only performing O(log T ) updating. The above observations inspire us to consider the following questions.\n1. For Stochastic Optimization of Smooth and Convex functions (SOSC), is it possible to maintain the optimal O(1/ √ T ) rate by performing O(T 1/4) projections? 2. For Stochastic Optimization of Smooth and Strongly Convex functions (SOS2C), is it possible to maintain the optimal O(1/T ) rate by performing O(log T ) projections?\nFor the 1st question, we have found a positive answer from literature. By combining mini-batches (Roux et al., 2008) with the accelerated stochastic approximation (Lan, 2012), we can achieve the optimal O(1/ √ T ) rate by performing O(T 1/4) projections (Cotter et al., 2011). However, a naive application of mini-batches does not lead to the desired O(log T ) complexity for SOS2C. The main contribution of this paper is a novel stochastic optimization algorithm that answers the 2nd question positively. Our theoretical analysis reveals, both in expectation and with a high probability, that the proposed algorithm achieves the optimal O(1/T ) rate by only performing O(log T ) projections."
    }, {
      "heading" : "2. Related Work",
      "text" : "In this section, we provide a brief review of the existing approaches for avoiding projections."
    }, {
      "heading" : "2.1. Mini-batch based algorithms",
      "text" : "Instead of updating the solution after each call to the gradient oracle, mini-batch based algorithms use the average gradient over multiple calls to update the solution (Roux et al., 2008; Shalev-Shwartz et al., 2011; Dekel et al., 2011). For a fixed batch size B, the number of updates (and projections) is reduced from O(T ) to O(T/B), and the variance of the stochastic gradient is reduced from σ to σ/ √ B. By appropriately balancing between the loss cased by a smaller number of updates and the reduction in the variance of stochastic gradients, it is able to maintain the optimal rate of convergence.\nThe idea of mini-batches can be incorporated into any stochastic optimization algorithm that uses gradient-based updating rules. When the objective function is smooth and convex, combining mini-batches with the accelerated stochastic approximation (Lan, 2012) leads to\nO\n(\nB2 T 2 + 1√ T\n)\nrate of convergence (Cotter et al., 2011). By setting B = T 3/4, we achieve the optimal O(1/ √ T ) rate with only O(T 1/4) projections. When the target function is smooth and strongly convex, we can apply mini-batches to the optimal algorithms for strongly convex functions (Hu et al., 2009; Ghadimi and Lan, 2012), leading to\nO\n(\nB2 T 2 + 1 T\n)\nrate of convergence (Dekel et al., 2012). In order to maintain the optimal O(1/T ) rate, the value of B cannot be larger than √ T , implying at least O( √ T ) projections are required. In contrast, the algorithm proposed in this paper achieves an O(1/T ) rate with only O(log T ) projections."
    }, {
      "heading" : "2.2. Projection free algorithms",
      "text" : "Due to the low iteration cost, Frank-Wolfe algorithm (Frank and Wolfe, 1956) or conditional gradient method (Levitin and Polyak, 1966) has seen a recent surge of interest in machine learning (Hazan, 2008; Clarkson, 2010; Lacoste-Julien et al., 2013). At each iteration of the Frank-Wolfe algorithm, instead of performing a projection that requires solving a constrained quadratic programming problem, it solves a constrained linear programming problem. For many domains of interest, including the positive semidefinite cone and the trace norm ball, the constrained linear problem can be solved more efficiently than a projection problem (Jaggi, 2013), making this kind of methods attractive for large-scale optimization.\nIn a recent work (Hazan and Kale, 2012), an online variant of the Frank-Wolfe algorithm is proposed. Although the online Frank-Wolfe algorithm exhibits an O(1/ √ T ) convergence rate for smooth functions, it is unable to achieve the optimal O(1/T ) rate for strongly convex functions. Besides, the memory complexity of this algorithm is O(T ), making it\nunsuitable for large-scale optimization problems. Another related work is the stochastic gradient descent with only one projection (Mahdavi et al., 2012). This algorithm is built upon the assumption that the solution domain can be characterized by an inequality constraint g(w) ≤ 0 and the gradient of g(·) can be evaluated efficiently. Unfortunately, this assumption does not hold for some commonly used domain (e.g., the trace norm ball). Compared to the projection free algorithms, our proposed method is more general because it make no assumption about the solution domain."
    }, {
      "heading" : "3. Stochastic Optimization of Smooth and Strongly Convex Functions",
      "text" : ""
    }, {
      "heading" : "3.1. Preliminaries",
      "text" : "We first define smoothness and strongly convexity.\nDefinition 1 A function f : D → R is L-smooth w.r.t. a norm ‖ · ‖ if f is everywhere differentiable and\n‖∇f(w)−∇f(w′)‖∗ ≤ L‖w −w′‖, ∀w,w′ ∈ D.\nwhere ‖ · ‖∗ is the dual norm.\nDefinition 2 A smooth function f : D → R is λ-strongly convex w.r.t. a norm ‖ · ‖, if f is everywhere differentiable and\n‖∇f(w)−∇f(w′)‖∗ ≥ λ‖w −w′‖, ∀w,w′ ∈ D.\nTo simplify our analysis, we assume that both ‖ · ‖ and ‖ · ‖∗ are the vector ℓ2 norm in the following discussion.\nFollowing (Hazan and Kale, 2011), we make the following assumptions about the gradient oracle.\n• There is a gradient oracle, which, for a given input point w returns a stochastic gradient ĝ(w) whose expectation is the gradient of F (w) at w, i.e.,\nE[ĝ(w)] = ∇F (w).\nWe further assume the stochastic gradients obtained by calling the oracle are independent.\n• The gradient oracle is G-bounded, i.e.,\n‖ĝ(w)‖ ≤ G, ∀w ∈ D.\nWe note that this assumption may be relaxed by assuming the orlicz norm of ĝ(w) to be bounded (Lan, 2012), i.e., E[exp(‖ĝ(w)‖2/G2)] ≤ exp(1). Although our theoretical result holds even under the assumption of bounded orlicz norm, we choose the Gbounded gradient for simplicity. Define w∗ as the optimal solution that minimizes F (w), i.e., w∗ = argminw∈D F (w). Using the strongly convexity of F (w), we have (Hazan and Kale, 2011)\nλ 2 ‖w −w∗‖2 ≤ F (w)− F (w∗) ≤\n2G2\nλ ,∀ w ∈ D. (1)"
    }, {
      "heading" : "3.2. The Algorithm",
      "text" : "Algorithm 1 shows the proposed method for Stochastic Optimization of Smooth and Strongly Convex functions (SOS2C), that achieves the optimal O(1/T ) rate of convergence by performing O(log T ) projections. The inputs of the algorithm are: (1) η, the step size, (2) M , the fixed number of updates per epoch/stage, (3) B1, the initial batch size, and (4) T , the total number of calls to the gradient oracle. With a slight abuse of notation, we use ĝ(w, i) to denote the stochastic gradient at w obtained after making the i-th call to the oracle. We denote the projection of w onto the domain D by ΠD(w).\nSimilar to the epoch gradient descent algorithm (Hazan and Kale, 2011), the proposed algorithm consists of two layers of loops. It uses the outer (while) loop to divide the learning process into a sequence of epochs (Step 5 to Step 12). Similar to (Hazan and Kale, 2011), the number of calls to the gradient oracle made by Algorithm 1 increases exponentially over the epoches, a key that allows us to achieve the optimal O(1/T ) convergence rate for strongly convex functions. We note that other techniques, such as the α-suffix averaging (Rakhlin et al., 2012), can also be used as an alternative.\nIn the inner (for) loop of each epoch, we combine the idea of mini-batches (Dekel et al., 2011) with extra-gradient descent (Nemirovski, 2005; Juditsky et al., 2011). We choose extra-gradient descent because it allows us to replace in the excess risk bound E[‖ĝ(w)‖2] with E[‖ĝ(w)− E[ĝ(w)]‖2], the variance of the stochastic gradient ĝ(w), thus opening the door to fully exploring the capacity of mini-batches in variance reduction.\nTo be more specific, in the k-th epoch, we maintain two sequences of solutions {wkt }Mt=1 and {zkt }Mt=1, where zkt is an auxiliary solution that allows us to effectively explore the smoothness of the loss function. At each iteration t of the k-th epoch, we calculate the average gradients ḡkt and f̄ k t by calling the gradient oracle B\nk times (Steps 6 and 8), and update the solutions wkt and z k t using the average gradients (Steps 7 and 9). The batch size Bk is fixed inside each epoch but doubles from epoch to epoch (Step 11). This is in contrast to most mini-batch based algorithms that have a fixed batch size. This difference is critical for achieving O(1/T ) convergence rate with only O(log T ) updates."
    }, {
      "heading" : "3.3. The main results",
      "text" : "The following theorem bounds the expected excess risk of the solution return by Algorithm 1 and the number of projections.\nTheorem 1 Set the parameters in Algorithm 1 as\nη = 1√ 6L , M = 4 ηλ and B1 = 12ηλ.\nThe final point wk1 returned by Algorithm 1 makes at most T calls to the gradient oracle, and has its excess risk bounded by\nE[F (wk1)− F (w∗)] ≤ 384G2\nλT = O\n(\n1\nT\n)\n,\nand the total number of projections bounded by\n8 √ 6L\nλ\n⌊\nlog2\n(\nT 96 + 1\n)⌋\n= O (log T ) .\nAlgorithm 1 log T Projections for SOS2C\n1: Input: parameters η, M , B1 and T 2: Initialize w11 ∈ D arbitrarily 3: Set k = 1 4: while 2M\n∑k i=1 B i ≤ T do 5: for t = 1 to M do 6: Compute the average gradient at wkt over B k calls to the gradient oracle\nḡkt = 1\nBk\nBk ∑\ni=1\nĝ(wkt , i)\n7: Update\nzkt = ΠD\n( wkt − ηḡkt )\n8: Compute the average gradient at zkt over B k calls to the gradient oracle\nf̄kt = 1\nBk\nBk ∑\ni=1\nĝ(zkt , i)\n9: Update\nwkt+1 = ΠD\n( wkt − ηf̄kt )\n10: end for 11: wk+11 = 1 M ∑M t=1 z k t , and B k+1 = 2Bk 12: k = k + 1 13: end while 14: Return: wk1\nTheorem 1 shows that in expectation, Algorithm 1 achieve an O(1/T ) convergence with O(log T ) updates. The following theorem gives a high probability bound of the excess risk for Algorithm 1.\nTheorem 2 Set the parameters in Algorithm 1 as\nη = 1√ 6L , M = 4 ηλ and B1 = αηλ,\nwhere α is defined below. For any 0 < δ < 1, let\nδ̃ = δ\nk† ,\nk† =\n⌊\nlog2\n(\nT\n8α + 1\n)⌋\n= O(log T ), (2)\nα =max\n{\n400 log2 8M\nδ̃ , 1 + 64 log2\n8M\nδ̃\n(\nlog 4N\nδ̃ +\n4 9 log2 4N\nδ̃\n)}\n(3)\n=O\n[\n(\nlog log T + log 1\nδ\n)4 ]\n,\nN =\n⌈\nlog2 4MT\nηλ\n⌉\n= O(log T ). (4)\nThe final point wk1 returned by Algorithm 1 makes at most T calls to the gradient oracles, performs\n8 √ 6L\nλ\n⌊\nlog2\n(\nT\n8α + 1\n)⌋\n= O (log T )\nprojections, and with a probability at least 1− δ, has its excess risk bounded by\nF (wk1)− F (w∗) ≤ 32αG2\nλT = O\n(\n(log log T + log 1/δ)4\nT\n)\n.\nRemark: It is worth noting that we achieve the high probability bound without making any modifications to Algorithm 1. This is in contrast to the epoch gradient descent algorithm (Hazan and Kale, 2011) that needs to shrink the domain size in order to obtain the desirable high probability bound, which could potentially lead to an additional computational cost in performing projection. We remove the shrinking step by effectively exploring the peeling technique (Bartlett et al., 2005).\nThe number of projections required by Algorithm 1, according to Theorem 2, exhibits a linear dependence on the conditional number L/λ, which can be very large when dealing with ill-conditioned optimization problems. In the deterministic setting, the convergence rate only depends on the square root of the conditional number (Nesterov, 2004, 2007). Thus, we conjecture that it may be possible to improve the dependence on the conditional number to its square root in the stochastic setting, a problem that will be examined in the future."
    }, {
      "heading" : "4. Analysis",
      "text" : "We here present the proofs of main theorems. The omitted proofs are provided in the supplementary material."
    }, {
      "heading" : "4.1. Proof of Theorem 1",
      "text" : "Since we make use of the the multi-stage learning strategy, the proof provided below is similar to the proof in (Hazan and Kale, 2011). We begin by analyzing the property of the inner loop in Algorithm 1, which is a combination of mini-batches and the extra-gradient descent. To this end, we have the following lemma.\nLemma 3 Let η = 1/[ √ 6L] in Algorithm 1. Then, we have\nF\n(\n1\nM\nM ∑\nt=1\nzkt\n)\n− F (w∗) ≤ ‖wk1 −w∗‖2 2Mη − λ 2M\nM ∑\nt=1\n‖zkt −w∗‖2\n+ 3η\nM\n(\nM ∑\nt=1\n‖ḡkt − gkt ‖2 + M ∑\nt=1\n‖f̄kt − fkt ‖2 )\n(5)\n+ 1\nM\nM ∑\nt=1\n〈fkt − f̄kt , zkt −w∗〉 (6)\nwhere gkt = ∇F (wkt ) and fkt = ∇F (zkt ).\nTaking the conditional expectation of the inequality, we have\nEk−1\n[\nF\n(\n1\nM\nM ∑\nt=1\nzkt\n)]\n− F (w∗) ≤ ‖wk1 −w∗‖2\n2Mη +\n6ηG2\nBk .\nwhere Ek−1[·] denotes the expectation conditioned on all the randomness up to epoch k − 1.\nThe quantity in (5) illustrates the advantage of the extra-gradient descent, i.e., it is able to produce variance-dependent upper bound when applied to stochastic optimization. Because of mini-batches, the expectations of ‖ḡkt − gkt ‖2 and ‖f̄kt − fkt ‖2 are smaller than G2/Bk, which leads to the tight upper bound in the second inequality.\nBased on Lemma 3, we get the following lemma that bounds the expected excess risk in each epoch.\nLemma 4 Define ∆k = F (w k 1)− F (w∗). Set the parameters η = 1/[ √ 6L], M = 4/[ηλ] and B1 = 12ηλ in Algorithm 1. For any k, we have\nE[∆k] ≤ Vk = G2\nλ2k−2 .\nProof It is straightforward to check that\nBk = 12ηλ2k−1 = 24ηG2\nVk . (7)\nWe prove this lemma by induction on k. When k = 1, we know that\n∆1 = F (w 1 1)− F (w∗)\n(1) ≤ 2G 2\nλ =\nG2\nλ21−2 = V1.\nAssume that E[∆k] ≤ Vk for some k ≥ 1, and we prove the inequality for k + 1. From Lemma 3, we have\nEk−1\n[ F (\nwk+11\n)] − F (w∗) ≤ ‖wk1 −w∗‖2\n2Mη +\n6ηG2\nBk .\nThus\nE [ F (\nwk+11\n)]\n− F (w∗)\n≤ E[‖w k 1 −w∗‖2] 2Mη + 6ηG2 Bk\n(1) ≤ E[2(F (w k 1)− F (w∗))/λ] 2Mη + 6ηG2 Bk\n(7) =\nE[∆k] Mηλ + Vk 4 ≤ Vk 4 + Vk 4 = Vk+1.\nWe are now at the position to prove Theorem 1. Proof [Proof of Theorem 1] From the stopping criterion of the outer loop in Algorithm 1, we know that the number of the epochs is given by the largest value of k such that\n2M\nk ∑\ni=1\nBi ≤ T.\nSince\n2M\nk ∑\ni=1\nBi = 24Mηλ\nk ∑\ni=1\n2i−1 = 96(2k − 1),\nthe final epoch is given by\nk† =\n⌊\nlog2\n(\nT 96 + 1\n)⌋\n,\nand the final output is wk †+1\n1 . From Lemma 4, we have\nE[F (wk †+1 1 )]− F (w∗) ≤ Vk†+1 = G2 2k†−1λ ≤ 384G 2 λT ,\nwhere we use the fact\n2k † ≥ 1\n2\n(\nT 96 + 1\n)\n≥ T 192 .\nThe total number of projections is\n2Mk† = 8 √ 6L\nλ\n⌊\nlog2\n(\nT 96 + 1\n)⌋\n."
    }, {
      "heading" : "4.2. Proof of Theorem 2",
      "text" : "Compared to the proof of Theorem 1, the main difference here is that we need a high probability version of Lemma 3. Specifically, we need to provide high probability bounds for the quantities in (5) and (6).\nTo bound the variances given in (5), we need the following norm concentration inequality in Hilbert Space (Smale and Zhou, 2009).\nLemma 5 Let H be a Hilbert Space and let ξ be a random variable on (Z, ρ) with values in H. Assume ‖ξ‖ ≤ B < ∞ almost surely. Let {ξi}mi=1 be independent random drawers of ρ. For any 0 < δ < 1, with a probability at least 1− δ,\n∥ ∥ ∥ ∥ ∥ 1 m m ∑\ni=1\n(ξi − E[ξi]) ∥ ∥ ∥ ∥\n∥ ≤ 4B√ m log 2 δ .\nBased on Lemma 5, it is straightforward to prove the following lemma.\nLemma 6 With a probability at least 1− δ̃/2, we have\n‖ḡkt − gkt ‖ ≤ 4G√ Bk log 4M δ̃ , ∀ t = 1, . . . ,M. (8)\nSimilarly, with a probability at least 1− δ̃/4, we have\n‖f̄kt − fkt ‖ ≤ 4G√ Bk log 8M δ̃ , ∀ t = 1, . . . ,M. (9)\nWe define the Martingale difference sequence:\nZkt = 〈fkt − f̄kt , zkt −w∗〉.\nIn order to bound the summation of Zkt in (6), we make use of the Berstein inequality for martingales (Cesa-Bianchi and Lugosi, 2006) and the peeling technique described in (Bartlett et al., 2005), leading to the following Lemma.\nLemma 7 We use E1 to denote the event that all the inequalities in (9) hold. On event E1, with a probability at least 1− δ̃/4, we have\nM ∑\nt=1\nZkt ≤ 4G2ηM\nBk log2\n8M\nδ̃ +\nG2\nλBk\n[\n1 + 64 log2 8M\nδ̃\n(\nlog 4n\nδ̃ +\n4 9 log2 4n\nδ̃\n)]\n+ λ\n2\nM ∑\nt=1\n‖zkt −w∗‖2,\nwhere\nn =\n⌈\nlog2 4MBk\nηλ\n⌉\n. (10)\nSubstituting the results in Lemmas 6 and 7 into Lemma 3, we obtain the lemma below.\nLemma 8 For any 0 < δ̃ < 1, with a probability at least 1− δ̃, we have\nF\n(\n1\nM\nM ∑\nt=1\nzkt\n)\n− F (w∗) ≤ ‖wk1 −w∗‖2\n2Mη +\n100G2η\nBk log2\n8M\nδ̃\n+ G2\nλBkM\n[\n1 + 64 log2 8M\nδ̃\n(\nlog 4n\nδ̃ +\n4 9 log2 4n\nδ̃\n)]\n,\nwhere n is given in (10).\nBased on Lemma 8, we provide a high probability version of Lemma 4, that bounds the excess risk in each epoch with a high probability. Lemma 9 Set the parameters η = 1/[ √ 6L], M = 4/[ηλ] and B1 = αηλ in Algorithm 1, where α is defined in (3). For any k, with a probability at least (1− δ̃)k−1, we have\n∆k = F (w k 1)− F (w∗) ≤ Vk =\nG2\nλ2k−2 .\nProof We follow the logic used in the proof of Lemma 4. It is straightforward to check that\nBk = αηλ2k−1 = 2αηG2\nVk .\nWhen k = 1, with a probability (1− δ̃)1−1 = 1, we have\n∆1 = F (w 1 1)− F (w∗)\n(1) ≤ 2G 2\nλ =\nG2\nλ21−2 = V1.\nAssume that with a probability at least (1− δ̃)k−1, ∆k ≤ Vk for some k ≥ 1. We now prove the case for k + 1. Notice that N defined in (4) is larger than n defined in (10). From Lemma 8, with a probability at least 1− δ̃, we have\n∆k+1 = F (w k+1 1 )− F (w∗)\n≤‖w k 1 −w∗‖2 2Mη + 100G2η Bk log2 8M δ̃ + G2 λBkM\n[\n1 + 64 log2 8M\nδ̃\n(\nlog 4N\nδ̃ +\n4 9 log2 4N\nδ̃\n)]\n≤∆k 4 + 400 α log2 8M\nδ̃\nVk 8 + 1 α\n[\n1 + 64 log2 8M\nδ̃\n(\nlog 4N\nδ̃ +\n4 9 log2 4N\nδ̃\n)]\nVk 8 .\nUsing the definition of α in (3), with a probability at least (1− δ̃)k we have,\n∆k+1 ≤ 1\n4 Vk +\n1 8 Vk + 1 8 Vk = 1 2 Vk = Vk+1.\nNow, we provide the proof of Theorem 2.\nProof [Proof of Theorem 2] The number of epochs made is given by the largest value of k satisfying 2M\n∑k i=1B i ≤ T . Since\n2M k ∑\ni=1\nBi = 2Mαλη k ∑\ni=1\n2i−1 = 8α(2k − 1),\nk† defined in (2) is the value of the final epoch, and the final output is wk †+1\n1 . From\nLemma 9, we have with a probability at least (1− δ̃)k†\nF (wk †+1 1 )− F (w∗) = ∆k†+1 ≤ Vk†+1 = G2\n2k†−1λ =\n2G2\n2k†λ ≤ 32αG\n2\nλT ,\nwhere we use the fact\n2k † ≥ 1\n2\n(\nT\n8α + 1\n)\n≥ T 16α .\nWe complete the proof by using the property that (1 − 1x)x is an increasing function when x > 1, which implies\n(1− δ̃)k† = ( 1− δ k†\n)k†\n=\n(\n(\n1− 1 k†/δ\n)k†/δ )δ ≥ ( (\n1− 1 1/δ\n)1/δ )δ\n= 1− δ."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section, we present numerical experiments to support our theoretical analysis. We studied the following algorithms:\n1. log T : the proposed algorithm that is optimal for SOS2C but only needs log(T ) projections; 2. EP GD: the epoch gradient descent developed in (Hazan and Kale, 2011), which is also optimal for SOS2C but needs O(T ) projections; 3. SGD: the stochastic gradient descent with step size ηt = 1/(λt) (Shalev-Shwartz et al., 2011), which achieves O(log T/T ) rate of convergence for general SOS2C and needs O(T ) projections. We first consider the a simple stochastic optimization problem adapted from (Rakhlin et al., 2012), which is both smooth and strongly convex. The objective function is F (W ) = 12‖W‖2F and the domain is the 5 × 5 dimensional positive semidefinite (PSD) cone. The stochastic gradient oracle, given a pointW , returns the stochastic gradientW+Z where Z is uniformly distributed in [−1, 1]5×5. Because of the noise matrix Z, all the immediate solutions are not PDS and we need to project them back to the PSD cone. To ensure the eigendecomposition only involving real numbers, we further require Z to be symmetric. Notice that for this problem we know W∗ = argminW o F (W ) = 0 5×5. Since the gradient of W∗ is 0 5×5, it can be shown that SGD also achieves the optimal O(1/T ) rate of convergence on this problem (Rakhlin et al., 2012).\nLet WT be the solution returned after making T calls to the gradient oracle. To verify if the proposed algorithm achieves an O(1/T ) convergence, we measure (F (WT )− F (W∗))× T versus T , which is given in Fig. 1(a). We observe that when T is sufficiently large, quantity (F (WT ) − F (W∗)) × T essentially becomes a constant for all three algorithms, implying O(1/T ) convergence rates for all the algorithms. We also observe that the constant achieved by the proposed algorithm is slightly larger than the two competitors, which can be attributed to the term (log log T )4 in our bound in Theorem 2. To demonstrate the advantage of our algorithm, we plot the value of the objective function versus the number of projections P in Fig. 1(b). We observe that using our algorithm, the objective function is reduced significantly faster than other algorithms w.r.t. the number of projections.\nIn the second experiment, we apply our algorithm to the regularized distance metric learning (Jin et al., 2009). The goal is to solve the following problem\nmin W 0\nE(xi,yi),(xj ,yj)[ℓ(yij(1− ‖xi − xj‖2M ))] + λ\n2 ‖W‖2F ,\nwhere xi is the instance, and yi is xi’s label, yij is derived from labels yi and yj (i.e., yij = 1 if yi = yj and −1 otherwise), ‖x‖2M = x⊤Mx, and ℓ(z) = log(1 + exp(−z)) is the logit loss. During the optimization process, the call to the gradient oracle corresponds to generate a training pair {(xi, yi), (xj , yj)} randomly. To estimate the value of objective function, we evaluate the average empirical loss on 104 testing pairs, which are also generated randomly. Fig. 2 shows the value of the objective function versus the number of projections P . Again, this result validates that the proposed algorithm log T is able to reduce the number of projections dramatically without hurting the performance."
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this paper, we study the problem of reducing the number of projections in stochastic optimization by exploring the property of smoothness. When the target function is smooth\nand strongly convex, we propose a novel algorithm that achieves the optimal O(1/T ) rate of convergence by only performing O(log T ) projections.\nAn open question is how to extend our results to stochastic composite optimization (Lan, 2012), where the objective function is a combination of non-smooth and smooth stochastic components. We plan to explore the composite gradient mapping technique, introduced in (Nesterov, 2007), to see if we can achieve an O(1/T ) convergence rate with only O(log T ) projections."
    }, {
      "heading" : "Appendix A. Proof of Lemma 3",
      "text" : "We need the following lemma that characterizes the property of the extra-gradient descent.\nLemma 10 (Lemma 3.1 in (Nemirovski, 2005)) Let Z be a convex compact set in Euclidean space E with inner product 〈·, ·〉, let ‖ · ‖ be a norm on E and ‖ · ‖∗ be its dual norm, and let ω(z) : Z 7→ R be a α-strongly convex function with respect to ‖ · ‖. The Bregman distance associated with ω for points z,w ∈ Z is defined as\nBω(z,w) = ω(z)− ω(w)− 〈z−w,∇ω(w)〉.\nLet U be a convex and closed subset of Z, and let z− ∈ Z, let ξ,η ∈ E, and let γ > 0. Consider the points\nw = argmin y∈U\n{〈γξ −∇ω(z−),y〉 + ω(y)},\nz+ = argmin y∈U\n{〈γη −∇ω(z−),y〉 + ω(y)}.\nThen for all z ∈ U one has\n〈w − z, γη〉 ≤ Bω(z, z−)−Bω(z, z+) + γ2\nα ‖η − ξ‖2∗ −\nα 2 {‖w − z−‖2 + ‖z+ −w‖2}.\nProof [Proof of Lemma 3] We first state the inner loop in Algorithm 1 below.\nfor t = 1 to M do Compute the average gradient at wkt over B k calls to the gradient oracle\nḡkt = 1\nBk\nBk ∑\ni=1\nĝ(wkt , i)\nUpdate\nzkt = ΠD\n( wkt − ηḡkt )\nCompute the average gradient at zkt over B k calls to the gradient oracle\nf̄kt = 1\nBk\nBk ∑\ni=1\nĝ(zkt , i)\nUpdate\nwkt+1 = ΠD\n( wkt − ηf̄kt )\nend for\nTo simplify the notation, we define\ngkt = ∇F (wkt ) and fkt = ∇F (zkt ).\nLet the two norms ‖ · ‖ and ‖ · ‖∗ in Lemma 10 be the vector ℓ2 norm. Each iteration in the inner loop satisfies the conditions in Lemma 10 by doing the mappings below:\nU = Z = E ← D, ω(z) ← 1 2 ‖z‖2, α ← 1, γ ← η,\nz− ← wkt , ξ ← ḡkt , η ← f̄kt , w ← zkt , z+ ← wkt+1, z ← w∗.\nFollowing Lemma 10, we have\n〈zkt −w∗, ηf̄kt 〉\n≤‖w k t −w∗‖2 2 − ‖w k t+1 −w∗‖2 2 + η2‖ḡkt − f̄kt ‖2 − 1 2 ‖wkt − zkt ‖2 ≤‖w k t −w∗‖2\n2 − ‖w k t+1 −w∗‖2 2 + 3η2 ( ‖ḡkt − gkt ‖2 + ‖f̄kt − fkt ‖2 + ‖gkt − fkt ‖2 )\n− 1 2 ‖wkt − zkt ‖2\n≤‖w k t −w∗‖2 2 − ‖w k t+1 −w∗‖2 2 + 3η2 ( ‖ḡkt − gkt ‖2 + ‖f̄kt − fkt ‖2 )\n+ 3η2‖gkt − fkt ‖2 − 1\n2 ‖wkt − zkt ‖2\n≤‖w k t −w∗‖2 2 − ‖w k t+1 −w∗‖2 2 + 3η2 ( ‖ḡkt − gkt ‖2 + ‖f̄kt − fkt ‖2 )\n+ 3η2L2‖wkt − zkt ‖2 − 1\n2 ‖wkt − zkt ‖2\n≤‖w k t −w∗‖2 2 − ‖w k t+1 −w∗‖2 2 + 3η2 ( ‖ḡkt − gkt ‖2 + ‖f̄kt − fkt ‖2 ) ,\n(11)\nwhere in the fifth line we use the smoothness assumption\n‖gkt − fkt ‖ = ‖∇F (wkt )−∇F (zkt )‖ ≤ L‖wkt − zkt ‖.\nFrom the property of λ-strongly convex function and (11), we obtain\nF (zkt )− F (w∗)\n≤〈fkt , zkt −w∗〉 − λ\n2 ‖zkt −w∗‖2\n=〈f̄kt , zkt −w∗〉+ 〈fkt − f̄kt , zkt −w∗〉 − λ\n2 ‖zkt −w∗‖2\n≤‖w k t −w∗‖2 2η − ‖w k t+1 −w∗‖2 2η + 3η ( ‖ḡkt − gkt ‖2 + ‖f̄kt − fkt ‖2 )\n+ 〈fkt − f̄kt , zkt −w∗〉 − λ\n2 ‖zkt −w∗‖2.\nSumming up over all t = 1, 2, . . . ,M , we have\nM ∑\nt=1\nF (zkt )−MF (w∗)\n≤‖w k 1 −w∗‖2 2η + 3η\n(\nM ∑\nt=1\n‖ḡkt − gkt ‖2 + M ∑\nt=1\n‖f̄kt − fkt ‖2 )\n+\nM ∑\nt=1\n〈fkt − f̄kt , zkt −w∗〉 − λ\n2\nM ∑\nt=1\n‖zkt −w∗‖2.\nDividing both sides by M and following Jensen’s inequality, we have\nF\n(\n1\nM\nM ∑\nt=1\nzkt\n)\n− F (w∗)\n≤ 1 M\nM ∑\nt=1\nF (zkt )− F (w∗)\n≤‖w k 1 −w∗‖2 2Mη + 3η M\n(\nM ∑\nt=1\n‖ḡkt − gkt ‖2 + M ∑\nt=1\n‖f̄kt − fkt ‖2 ) +\n1\nM\nM ∑\nt=1\n〈fkt − f̄kt , zkt −w∗〉 − λ\n2M\nM ∑\nt=1\n‖zkt −w∗‖2.\n(12)\nwhich gives the first inequality in Lemma 3. Let Ek−1[·] denote the expectation conditioned on all the randomness up to epoch k− 1 and Et−1k [·] denote the expectation conditioned on all the randomness up to the t − 1-th iteration in the k-th epoch. Taking the conditional expectation of (12), we have\nEk−1\n[\nF\n(\n1\nM\nM ∑\nt=1\nzkt\n)]\n− F (w∗)\n≤‖w k 1 −w∗‖2 2Mη + 3η M\n(\nM ∑\nt=1\nEk−1\n[ ‖ḡkt − gkt ‖2 ] + M ∑\nt=1\nEk−1\n[ ‖f̄kt − fkt ‖2 ]\n)\n+ 1\nM\nM ∑\nt=1\nEk−1\n[ 〈fkt − f̄kt , zkt −w∗〉 ] ,\n(13)\nwhere we drop the last term, since it is negative. To bound Ek−1 [ ‖ḡkt − gkt ‖2 ] , we have\nEk−1\n[ ‖ḡkt − gkt ‖2 ] = Ek−1\n\n\n∥ ∥ ∥ ∥ ∥ ∥ 1 Bk Bk ∑\ni=1\nĝ(wkt , i)− gkt\n∥ ∥ ∥ ∥ ∥ ∥ 2 \n=Ek−1\n\n\n∥ ∥ ∥ ∥ ∥ ∥ 1 Bk Bk ∑\ni=1\n( ĝ(wkt , i)− gkt )\n∥ ∥ ∥ ∥ ∥ ∥ 2 \n= 1\n[Bk]2\nBk ∑\ni=1\nEk−1\n[\n∥ ∥ ∥ĝ(wkt , i)− gkt ∥ ∥ ∥\n2 ]\n+ 1\n[Bk]2 Ek−1\n\n\n∑\ni 6=j\n〈\nEt−1k\n[ ĝ(wkt , i)− gkt ] ,Et−1k [ ĝ(wkt , j)− gkt ]〉\n\n\n= 1\n[Bk]2\n\n\nBk ∑\ni=1\nEk−1\n[\n∥ ∥ ∥ ĝ(wkt , i) − gkt ∥ ∥ ∥\n2 ]\n  ≤ G 2\nBk ,\n(14)\nwhere we make use of the facts ĝ(wkt , i) and ĝ(w k t , j) are independent when i 6= j, and\nEt−1k\n[ ĝ(wkt , i) − gkt ] = 0, Et−1k [ ‖ĝ(wkt , i)− gkt ‖2 ] ≤ Et−1k [ ‖ĝ(wkt , i)‖2 ] ≤ G2, ∀i = 1, . . . , Bk.\nSimilarly, we also have\nEk−1\n[ ‖f̄kt − fkt ‖2 ] ≤ G 2\nBk . (15)\nNotice that f̄kt is an unbiased estimate of f k t , thus\nEk−1\n[ 〈fkt − f̄kt , zkt −w∗〉 ] = Ek−1 [ 〈Et−1k [ fkt − f̄kt ] , zkt −w∗〉 ] = 0. (16)\nSubstituting (14), (15), and (16) into (13), we get the second inequality in Lemma 3."
    }, {
      "heading" : "Appendix B. Proof of Lemma 6",
      "text" : "Proof Recall that ḡkt = 1 Bk\n∑Bk\ni=1 ĝ(w k t , i), thus\n‖ḡkt − gkt ‖ =\n∥ ∥ ∥ ∥ ∥ ∥ 1 Bk Bk ∑\ni=1\nĝ(wkt , i)− gkt\n∥ ∥ ∥ ∥ ∥ ∥ .\nSince ‖ĝ(wkt , i)‖ ≤ G, and E[ĝ(wkt , i)] = gkt , we have with a probability at least 1− δ\n‖ḡkt − gkt ‖ ≤ 4G√ Bk log 2 δ .\nWe obtain (8) by the union bound and setting δ̃/2 = Mδ. The inequality in (9) can be proved in the same way."
    }, {
      "heading" : "Appendix C. Proof of Lemma 7",
      "text" : "We first state the Berstein inequality for martingales (Cesa-Bianchi and Lugosi, 2006), which is used in the proof below.\nTheorem 3 (Bernstein’s inequality for martingales). Let X1, . . . ,Xn be a bounded martingale difference sequence with respect to the filtration F = (Fi)1≤i≤n and with |Xi| ≤ K. Let\nSi =\ni ∑\nj=1\nXj\nbe the associated martingale. Denote the sum of the conditional variances by\nΣ2n =\nn ∑\nt=1\nE [ X2t |Ft−1 ] .\nThen for all constants t, ν > 0,\nPr\n[\nmax i=1,...,n\nSi > t and Σ 2 n ≤ ν\n] ≤ exp ( − t 2\n2(ν +Kt/3)\n)\n,\nand therefore,\nPr\n[\nmax i=1,...,n\nSi > √ 2νt+ 2\n3 Kt and Σ2n ≤ ν\n]\n≤ e−t.\nTo simplify the notation, we define\nA =\nM ∑\ni=1\n‖zkt −w∗‖2 ≤ 4MG2\nλ2 ,\nC = 4G√ Bk log 8M δ̃ .\nIn the analysis below, we consider two different scenarios, i.e., A ≤ ηG2/[λBk] and A > ηG2/[λBk].\nC.1. A ≤ ηG2/[λBk] On event E1, we can bound\nZkt ≤ ‖fkt − f̄kt ‖‖zkt −w∗‖ ≤ η\n4 ‖fkt − f̄kt ‖2 +\n1 η ‖zkt −w∗‖2 ≤ η 4 C2 + 1 η ‖zkt −w∗‖2.\nSumming up over all t = 1, 2, . . . ,M ,\nM ∑\nt=1\nZkt ≤ ηMC2\n4 +\n1\nη\nM ∑\nt=1\n‖zkt −w∗‖2 ≤ ηMC2\n4 +\nG2\nλBk . (17)\nC.2. A > ηG2/[λBk]\nSimilar to the above proof, on event E1, we bound\n|Zkt | ≤ ‖fkt − f̄kt ‖‖zkt −w∗‖ ≤ 1\nθ ‖fkt − f̄kt ‖2 +\nθ 4 ‖zkt −w∗‖2 ≤\nC2\nθ +\nθA\n4 ,\nwhere θ can be any nonnegative real number. Denote the sum of conditional variances by\nΣ2M =\nM ∑\nt=1\nEt−1k\n[ [Zkt ] 2 ] ≤ C2 M ∑\nt=1\n‖zt −w∗‖2 = C2A,\nwhere Et−1k [·] denote the expectation conditioned on all the randomness up to the t− 1-th iteration in the k-th epoch.\nNotice that A in the upper bound for |Zkt | and Σ2M is a random variable, thus we cannot directly apply Theorem 3. To address this challenge, we make use of the peeling technique described in (Bartlett et al., 2005), and have\nPr\n(\nM ∑\nt=1\nZkt ≥ 2 √ C2Aτ + 4\n3\n(\nC2\nθ +\nθA\n4\n)\nτ\n)\n=Pr\n(\nM ∑\nt=1\nZkt ≥ 2 √ C2Aτ + 4\n3\n(\nC2\nθ +\nθA\n4\n)\nτ, ηG2 λBk < A ≤ 4MG 2 λ2\n)\n=Pr\n(\nM ∑\nt=1\nZkt ≥ 2 √ C2Aτ + 4\n3\n(\nC2\nθ +\nθA\n4\n)\nτ,\nmax t\n|Zkt | ≤ C2\nθ +\nθA\n4 ,Σ2M ≤ C2A,\nηG2\nλBk < A ≤ 4MG\n2\nλ2\n)\n≤ n ∑\ni=1\nPr\n(\nM ∑\nt=1\nZkt ≥ 2 √ C2Aτ + 4\n3\n(\nC2\nθ +\nθA\n4\n)\nτ,\nmax t\n|Zkt | ≤ C2\nθ +\nθA\n4 ,Σ2M ≤ C2A,\nηG2\nλBk 2i−1 < A ≤ ηG\n2 λBk 2i )\n≤ n ∑\ni=1\nPr\n(\nM ∑\nt=1\nZkt ≥ 2 √ ( C2 ηG2\nλBk 2i−1\n)\nτ + 4\n3\n(\nC2\nθ +\nθ\n4\nηG2 λBk 2i−1\n)\nτ,\nmax t\n|Zkt | ≤ C2\nθ +\nθ\n4\nηG2 λBk 2i,Σ2M ≤ C2 ηG2 λBk 2i )\n≤ n ∑\ni=1\nPr\n(\nM ∑\nt=1\nZkt ≥ √ 2 ( C2 ηG2 λBk 2i ) τ + 2 3 ( C2 θ + θ 4 ηG2 λBk 2i ) τ,\nmax t\n|Zkt | ≤ C2\nθ +\nθ\n4\nηG2 λBk 2i,Σ2M ≤ C2 ηG2 λBk 2i )\n≤ne−τ , where\nn =\n⌈\nlog2 4MBk\nηλ\n⌉\n,\nand the last step follows the Bernstein inequality for martingales in Theorem 3. Setting\nθ = 3λ\n4τ , and τ = log\n4n\nδ̃ ,\nwith a probability at least 1− δ̃/4 we have\nM ∑\nt=1\nZkt\n≤2 √ C2Aτ + 4\n3\n(\nC2\nθ +\nθA\n4\n) τ = 2 √ C2Aτ + 16C2\n9λ τ2 +\nλA\n4\n≤ 4 λ C2τ + λA 4 +\n16C2\n9λ τ2 +\nλA\n4 =\n4C2\nλ\n(\nlog 4n\nδ̃ +\n4 9 log2 4n\nδ̃\n)\n+ λA\n2 .\n(18)\nWe complete the proof by combining (17) and (18)."
    } ],
    "references" : [ {
      "title" : "Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization",
      "author" : [ "Alekh Agarwal", "Peter L. Bartlett", "Pradeep Ravikumar", "Martin J. Wainwright" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2012
    }, {
      "title" : "Local rademacher complexities",
      "author" : [ "Peter L. Bartlett", "Olivier Bousquet", "Shahar Mendelson" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Bartlett et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bartlett et al\\.",
      "year" : 2005
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Nicolo Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "Optimal regularized dual averaging methods for stochastic optimization",
      "author" : [ "Xi Chen", "Qihang Lin", "Javier Pena" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Chen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2012
    }, {
      "title" : "Coresets, sparse greedy approximation, and the frank-wolfe algorithm",
      "author" : [ "Kenneth L. Clarkson" ],
      "venue" : "ACM Transactions on Algorithms,",
      "citeRegEx" : "Clarkson.,? \\Q2010\\E",
      "shortCiteRegEx" : "Clarkson.",
      "year" : 2010
    }, {
      "title" : "Better mini-batch algorithms via accelerated gradient methods",
      "author" : [ "Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Cotter et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cotter et al\\.",
      "year" : 2011
    }, {
      "title" : "Optimal distributed online prediction",
      "author" : [ "Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning,",
      "citeRegEx" : "Dekel et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dekel et al\\.",
      "year" : 2011
    }, {
      "title" : "Optimal distributed online prediction using mini-batches",
      "author" : [ "Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Dekel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dekel et al\\.",
      "year" : 2012
    }, {
      "title" : "An algorithm for quadratic programming",
      "author" : [ "Marguerite Frank", "Philip Wolfe" ],
      "venue" : "Naval Research Logistics Quarterly,",
      "citeRegEx" : "Frank and Wolfe.,? \\Q1956\\E",
      "shortCiteRegEx" : "Frank and Wolfe.",
      "year" : 1956
    }, {
      "title" : "Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework",
      "author" : [ "Saeed Ghadimi", "Guanghui Lan" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Ghadimi and Lan.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ghadimi and Lan.",
      "year" : 2012
    }, {
      "title" : "Sparse approximate solutions to semidefinite programs",
      "author" : [ "Elad Hazan" ],
      "venue" : "In Proceedings of the 8th Latin American conference on Theoretical informatics,",
      "citeRegEx" : "Hazan.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hazan.",
      "year" : 2008
    }, {
      "title" : "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : "In Proceedings of the 24th Annual Conference on Learning Theory,",
      "citeRegEx" : "Hazan and Kale.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2011
    }, {
      "title" : "Projection-free online learning",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "Hazan and Kale.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2012
    }, {
      "title" : "Accelerated gradient methods for stochastic optimization and online learning",
      "author" : [ "Chonghai Hu", "James Kwok", "Weike Pan" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Hu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2009
    }, {
      "title" : "Revisiting frank-wolfe: Projection-free sparse convex optimization",
      "author" : [ "Martin Jaggi" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "Jaggi.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jaggi.",
      "year" : 2013
    }, {
      "title" : "Regularized distance metric learning: Theory and algorithm",
      "author" : [ "Rong Jin", "Shijun Wang", "Yang Zhou" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Jin et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2009
    }, {
      "title" : "Primal-dual subgradient methods for minimizing uniformly convex functions",
      "author" : [ "Anatoli Juditsky", "Yuri Nesterov" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Juditsky and Nesterov.,? \\Q2010\\E",
      "shortCiteRegEx" : "Juditsky and Nesterov.",
      "year" : 2010
    }, {
      "title" : "Solving variational inequalities with stochastic mirror-prox algorithm",
      "author" : [ "Anatoli Juditsky", "Arkadi Nemirovski", "Claire Tauvel" ],
      "venue" : "Stochastic Systems,",
      "citeRegEx" : "Juditsky et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Juditsky et al\\.",
      "year" : 2011
    }, {
      "title" : "Block-coordinate frank-wolfe optimization for structural svm",
      "author" : [ "Simon Lacoste-Julien", "Martin Jaggi", "Mark Schmidt", "Patrick Pletscher" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "Lacoste.Julien et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lacoste.Julien et al\\.",
      "year" : 2013
    }, {
      "title" : "An optimal method for stochastic composite optimization",
      "author" : [ "Guanghui Lan" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Lan.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lan.",
      "year" : 2012
    }, {
      "title" : "Constrained minimization methods",
      "author" : [ "Evgenij S Levitin", "Boris T Polyak" ],
      "venue" : "USSR Computational Mathematics and Mathematical Physics,",
      "citeRegEx" : "Levitin and Polyak.,? \\Q1966\\E",
      "shortCiteRegEx" : "Levitin and Polyak.",
      "year" : 1966
    }, {
      "title" : "Stochastic gradient descent with only one projection",
      "author" : [ "Mehrdad Mahdavi", "Tianbao Yang", "Rong Jin", "Shenghuo Zhu", "Jinfeng Yi" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mahdavi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mahdavi et al\\.",
      "year" : 2012
    }, {
      "title" : "Problem complexity and method efficiency in optimization",
      "author" : [ "A. Nemirovski", "D.B. Yudin" ],
      "venue" : null,
      "citeRegEx" : "Nemirovski and Yudin.,? \\Q1983\\E",
      "shortCiteRegEx" : "Nemirovski and Yudin.",
      "year" : 1983
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Nemirovski et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nemirovski et al\\.",
      "year" : 2009
    }, {
      "title" : "Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems",
      "author" : [ "Arkadi Nemirovski" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Nemirovski.,? \\Q2005\\E",
      "shortCiteRegEx" : "Nemirovski.",
      "year" : 2005
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Yu. Nesterov" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Nesterov.,? \\Q2005\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2005
    }, {
      "title" : "Introductory lectures on convex optimization: a basic course, volume 87 of Applied optimization",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "Nesterov.,? \\Q2004\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2004
    }, {
      "title" : "Gradient methods for minimizing composite objective function",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Core discussion papers,",
      "citeRegEx" : "Nesterov.,? \\Q2007\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2007
    }, {
      "title" : "Making gradient descent optimal for strongly convex stochastic optimization",
      "author" : [ "Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "Rakhlin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rakhlin et al\\.",
      "year" : 2012
    }, {
      "title" : "Topmoumoute online natural gradient algorithm",
      "author" : [ "Nicolas Le Roux", "Pierre-Antoine Manzagol", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Roux et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Roux et al\\.",
      "year" : 2008
    }, {
      "title" : "Stochastic convex optimization",
      "author" : [ "Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan" ],
      "venue" : "In Proceedings of the 22nd Annual Conference on Learning Theory,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2009
    }, {
      "title" : "Pegasos: primal estimated sub-gradient solver for svm",
      "author" : [ "Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro", "Andrew Cotter" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2011
    }, {
      "title" : "Geometry on probability spaces",
      "author" : [ "Steve Smale", "Ding-Xuan Zhou" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "Smale and Zhou.,? \\Q2009\\E",
      "shortCiteRegEx" : "Smale and Zhou.",
      "year" : 2009
    }, {
      "title" : "Solving large scale linear prediction problems using stochastic gradient descent algorithms",
      "author" : [ "Tong Zhang" ],
      "venue" : "In Proceedings of the 21st International Conference on Machine Learning,",
      "citeRegEx" : "Zhang.,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "where (x, y) is an instance-label pair, l is a convex loss function that measures the prediction error, and the expectation is taken oven the unknown joint distribution of (x, y) (Zhang, 2004; Shalev-Shwartz et al., 2009; Hu et al., 2009).",
      "startOffset" : 179,
      "endOffset" : 238
    }, {
      "referenceID" : 30,
      "context" : "where (x, y) is an instance-label pair, l is a convex loss function that measures the prediction error, and the expectation is taken oven the unknown joint distribution of (x, y) (Zhang, 2004; Shalev-Shwartz et al., 2009; Hu et al., 2009).",
      "startOffset" : 179,
      "endOffset" : 238
    }, {
      "referenceID" : 13,
      "context" : "where (x, y) is an instance-label pair, l is a convex loss function that measures the prediction error, and the expectation is taken oven the unknown joint distribution of (x, y) (Zhang, 2004; Shalev-Shwartz et al., 2009; Hu et al., 2009).",
      "startOffset" : 179,
      "endOffset" : 238
    }, {
      "referenceID" : 22,
      "context" : "For general Lipschitz continuous convex functions, stochastic gradient descent exhibits the unimprovable O(1/ √ T ) rate of convergence (Nemirovski and Yudin, 1983; Nemirovski et al., 2009).",
      "startOffset" : 136,
      "endOffset" : 189
    }, {
      "referenceID" : 23,
      "context" : "For general Lipschitz continuous convex functions, stochastic gradient descent exhibits the unimprovable O(1/ √ T ) rate of convergence (Nemirovski and Yudin, 1983; Nemirovski et al., 2009).",
      "startOffset" : 136,
      "endOffset" : 189
    }, {
      "referenceID" : 16,
      "context" : "For strongly-convex functions, the algorithms proposed in very recent works (Juditsky and Nesterov, 2010; Hazan and Kale, 2011; Rakhlin et al., 2012; Chen et al., 2012) achieve the optimal O(1/T ) rate (Agarwal et al.",
      "startOffset" : 76,
      "endOffset" : 168
    }, {
      "referenceID" : 11,
      "context" : "For strongly-convex functions, the algorithms proposed in very recent works (Juditsky and Nesterov, 2010; Hazan and Kale, 2011; Rakhlin et al., 2012; Chen et al., 2012) achieve the optimal O(1/T ) rate (Agarwal et al.",
      "startOffset" : 76,
      "endOffset" : 168
    }, {
      "referenceID" : 28,
      "context" : "For strongly-convex functions, the algorithms proposed in very recent works (Juditsky and Nesterov, 2010; Hazan and Kale, 2011; Rakhlin et al., 2012; Chen et al., 2012) achieve the optimal O(1/T ) rate (Agarwal et al.",
      "startOffset" : 76,
      "endOffset" : 168
    }, {
      "referenceID" : 3,
      "context" : "For strongly-convex functions, the algorithms proposed in very recent works (Juditsky and Nesterov, 2010; Hazan and Kale, 2011; Rakhlin et al., 2012; Chen et al., 2012) achieve the optimal O(1/T ) rate (Agarwal et al.",
      "startOffset" : 76,
      "endOffset" : 168
    }, {
      "referenceID" : 0,
      "context" : ", 2012) achieve the optimal O(1/T ) rate (Agarwal et al., 2012).",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 29,
      "context" : "By combining mini-batches (Roux et al., 2008) with the accelerated stochastic approximation (Lan, 2012), we can achieve the optimal O(1/ √ T ) rate by performing O(T 1/4) projections (Cotter et al.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 19,
      "context" : ", 2008) with the accelerated stochastic approximation (Lan, 2012), we can achieve the optimal O(1/ √ T ) rate by performing O(T 1/4) projections (Cotter et al.",
      "startOffset" : 54,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : ", 2008) with the accelerated stochastic approximation (Lan, 2012), we can achieve the optimal O(1/ √ T ) rate by performing O(T 1/4) projections (Cotter et al., 2011).",
      "startOffset" : 145,
      "endOffset" : 166
    }, {
      "referenceID" : 29,
      "context" : "Mini-batch based algorithms Instead of updating the solution after each call to the gradient oracle, mini-batch based algorithms use the average gradient over multiple calls to update the solution (Roux et al., 2008; Shalev-Shwartz et al., 2011; Dekel et al., 2011).",
      "startOffset" : 197,
      "endOffset" : 265
    }, {
      "referenceID" : 31,
      "context" : "Mini-batch based algorithms Instead of updating the solution after each call to the gradient oracle, mini-batch based algorithms use the average gradient over multiple calls to update the solution (Roux et al., 2008; Shalev-Shwartz et al., 2011; Dekel et al., 2011).",
      "startOffset" : 197,
      "endOffset" : 265
    }, {
      "referenceID" : 6,
      "context" : "Mini-batch based algorithms Instead of updating the solution after each call to the gradient oracle, mini-batch based algorithms use the average gradient over multiple calls to update the solution (Roux et al., 2008; Shalev-Shwartz et al., 2011; Dekel et al., 2011).",
      "startOffset" : 197,
      "endOffset" : 265
    }, {
      "referenceID" : 19,
      "context" : "When the objective function is smooth and convex, combining mini-batches with the accelerated stochastic approximation (Lan, 2012) leads to O ( B2 T 2 + 1 √ T )",
      "startOffset" : 119,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "rate of convergence (Cotter et al., 2011).",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "When the target function is smooth and strongly convex, we can apply mini-batches to the optimal algorithms for strongly convex functions (Hu et al., 2009; Ghadimi and Lan, 2012), leading to O ( B2 T 2 + 1 T )",
      "startOffset" : 138,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "When the target function is smooth and strongly convex, we can apply mini-batches to the optimal algorithms for strongly convex functions (Hu et al., 2009; Ghadimi and Lan, 2012), leading to O ( B2 T 2 + 1 T )",
      "startOffset" : 138,
      "endOffset" : 178
    }, {
      "referenceID" : 7,
      "context" : "rate of convergence (Dekel et al., 2012).",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 8,
      "context" : "Projection free algorithms Due to the low iteration cost, Frank-Wolfe algorithm (Frank and Wolfe, 1956) or conditional gradient method (Levitin and Polyak, 1966) has seen a recent surge of interest in machine learning (Hazan, 2008; Clarkson, 2010; Lacoste-Julien et al.",
      "startOffset" : 80,
      "endOffset" : 103
    }, {
      "referenceID" : 20,
      "context" : "Projection free algorithms Due to the low iteration cost, Frank-Wolfe algorithm (Frank and Wolfe, 1956) or conditional gradient method (Levitin and Polyak, 1966) has seen a recent surge of interest in machine learning (Hazan, 2008; Clarkson, 2010; Lacoste-Julien et al.",
      "startOffset" : 135,
      "endOffset" : 161
    }, {
      "referenceID" : 10,
      "context" : "Projection free algorithms Due to the low iteration cost, Frank-Wolfe algorithm (Frank and Wolfe, 1956) or conditional gradient method (Levitin and Polyak, 1966) has seen a recent surge of interest in machine learning (Hazan, 2008; Clarkson, 2010; Lacoste-Julien et al., 2013).",
      "startOffset" : 218,
      "endOffset" : 276
    }, {
      "referenceID" : 4,
      "context" : "Projection free algorithms Due to the low iteration cost, Frank-Wolfe algorithm (Frank and Wolfe, 1956) or conditional gradient method (Levitin and Polyak, 1966) has seen a recent surge of interest in machine learning (Hazan, 2008; Clarkson, 2010; Lacoste-Julien et al., 2013).",
      "startOffset" : 218,
      "endOffset" : 276
    }, {
      "referenceID" : 18,
      "context" : "Projection free algorithms Due to the low iteration cost, Frank-Wolfe algorithm (Frank and Wolfe, 1956) or conditional gradient method (Levitin and Polyak, 1966) has seen a recent surge of interest in machine learning (Hazan, 2008; Clarkson, 2010; Lacoste-Julien et al., 2013).",
      "startOffset" : 218,
      "endOffset" : 276
    }, {
      "referenceID" : 14,
      "context" : "For many domains of interest, including the positive semidefinite cone and the trace norm ball, the constrained linear problem can be solved more efficiently than a projection problem (Jaggi, 2013), making this kind of methods attractive for large-scale optimization.",
      "startOffset" : 184,
      "endOffset" : 197
    }, {
      "referenceID" : 12,
      "context" : "In a recent work (Hazan and Kale, 2012), an online variant of the Frank-Wolfe algorithm is proposed.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 21,
      "context" : "Another related work is the stochastic gradient descent with only one projection (Mahdavi et al., 2012).",
      "startOffset" : 81,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "Following (Hazan and Kale, 2011), we make the following assumptions about the gradient oracle.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 19,
      "context" : "We note that this assumption may be relaxed by assuming the orlicz norm of ĝ(w) to be bounded (Lan, 2012), i.",
      "startOffset" : 94,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "Using the strongly convexity of F (w), we have (Hazan and Kale, 2011) λ 2 ‖w −w∗‖ ≤ F (w)− F (w∗) ≤ 2G2 λ ,∀ w ∈ D.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : "Similar to the epoch gradient descent algorithm (Hazan and Kale, 2011), the proposed algorithm consists of two layers of loops.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 11,
      "context" : "Similar to (Hazan and Kale, 2011), the number of calls to the gradient oracle made by Algorithm 1 increases exponentially over the epoches, a key that allows us to achieve the optimal O(1/T ) convergence rate for strongly convex functions.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 28,
      "context" : "We note that other techniques, such as the α-suffix averaging (Rakhlin et al., 2012), can also be used as an alternative.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : "In the inner (for) loop of each epoch, we combine the idea of mini-batches (Dekel et al., 2011) with extra-gradient descent (Nemirovski, 2005; Juditsky et al.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 24,
      "context" : ", 2011) with extra-gradient descent (Nemirovski, 2005; Juditsky et al., 2011).",
      "startOffset" : 36,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : ", 2011) with extra-gradient descent (Nemirovski, 2005; Juditsky et al., 2011).",
      "startOffset" : 36,
      "endOffset" : 77
    }, {
      "referenceID" : 11,
      "context" : "This is in contrast to the epoch gradient descent algorithm (Hazan and Kale, 2011) that needs to shrink the domain size in order to obtain the desirable high probability bound, which could potentially lead to an additional computational cost in performing projection.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : "We remove the shrinking step by effectively exploring the peeling technique (Bartlett et al., 2005).",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : "Proof of Theorem 1 Since we make use of the the multi-stage learning strategy, the proof provided below is similar to the proof in (Hazan and Kale, 2011).",
      "startOffset" : 131,
      "endOffset" : 153
    }, {
      "referenceID" : 32,
      "context" : "To bound the variances given in (5), we need the following norm concentration inequality in Hilbert Space (Smale and Zhou, 2009).",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "In order to bound the summation of Zk t in (6), we make use of the Berstein inequality for martingales (Cesa-Bianchi and Lugosi, 2006) and the peeling technique described in (Bartlett et al.",
      "startOffset" : 103,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "In order to bound the summation of Zk t in (6), we make use of the Berstein inequality for martingales (Cesa-Bianchi and Lugosi, 2006) and the peeling technique described in (Bartlett et al., 2005), leading to the following Lemma.",
      "startOffset" : 174,
      "endOffset" : 197
    }, {
      "referenceID" : 11,
      "context" : "EP GD: the epoch gradient descent developed in (Hazan and Kale, 2011), which is also optimal for SOS2C but needs O(T ) projections; 3.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 31,
      "context" : "SGD: the stochastic gradient descent with step size ηt = 1/(λt) (Shalev-Shwartz et al., 2011), which achieves O(log T/T ) rate of convergence for general SOS2C and needs O(T ) projections.",
      "startOffset" : 64,
      "endOffset" : 93
    }, {
      "referenceID" : 28,
      "context" : "We first consider the a simple stochastic optimization problem adapted from (Rakhlin et al., 2012), which is both smooth and strongly convex.",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 28,
      "context" : "Since the gradient of W∗ is 0 5×5, it can be shown that SGD also achieves the optimal O(1/T ) rate of convergence on this problem (Rakhlin et al., 2012).",
      "startOffset" : 130,
      "endOffset" : 152
    }, {
      "referenceID" : 15,
      "context" : "In the second experiment, we apply our algorithm to the regularized distance metric learning (Jin et al., 2009).",
      "startOffset" : 93,
      "endOffset" : 111
    } ],
    "year" : 2013,
    "abstractText" : "Traditional algorithms for stochastic optimization require projecting the solution at each iteration into a given domain to ensure its feasibility. When facing complex domains, such as positive semi-definite cones, the projection operation can be expensive, leading to a high computational cost per iteration. In this paper, we present a novel algorithm that aims to reduce the number of projections for stochastic optimization. The proposed algorithm combines the strength of several recent developments in stochastic optimization, including mini-batch, extra-gradient, and epoch gradient descent, in order to effectively explore the smoothness and strong convexity. We show, both in expectation and with a high probability, that when the objective function is both smooth and strongly convex, the proposed algorithm achieves the optimal O(1/T ) rate of convergence with only O(log T ) projections. Our empirical study verifies the theoretical result.",
    "creator" : "LaTeX with hyperref package"
  }
}