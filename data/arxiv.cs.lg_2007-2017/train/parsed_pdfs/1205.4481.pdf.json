{
  "name" : "1205.4481.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by Exploiting Structure",
    "authors" : [ "Hua Ouyang", "Alexander Gray" ],
    "emails" : [ "agray}@cc.gatech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 5.\n44 81\nv4 [\ncs .L\nG ]\n1 O\nct 2\n01 2"
    }, {
      "heading" : "1. Introduction",
      "text" : "Nonsmoothness is a central issue in machine learning computation, as many important methods minimize nonsmooth convex functions. For example, using the nonsmooth hinge loss yields sparse support vector machines; regressors can be made robust to outliers by using the nonsmooth absolute loss other than the squared loss; the l1-norm is widely used in sparse reconstructions. In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning).\nSmoothness and strong-convexity are typically certificates of the existence of fast global solvers. Nesterov’s deterministic smoothing method Nesterov (2005b) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods Nesterov (2004) can be applied. It converges as f(xt)−minx f(x) ≤ O(1/t) after t iterations. If a nonsmooth function is strongly convex, this rate can be improved to O(1/t2) using the excessive gap technique Nesterov (2005a).\nIn this paper, we extend Nesterov’s smoothing method to the stochastic setting by proposing a stochastic smoothing method for nonsmooth functions. Combining this with a stochastic version of the optimal gradient descent method, we introduce and analyze a new algorithm named Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), for a class of functions that include the popular ML methods of interest.\nTo our knowledge ANSGD is the first stochastic first-order algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions without Polyak’s averaging Polyak and Juditsky (1992). In comparison, the classic SGD converges in O(ln t/t) for\n1. A short version of this paper appears in International Conference of Machine Learning (ICML) 2012.\nnonsmooth strongly convex functions Shalev-Shwartz et al. (2007), and is usually not robust Nemirovski et al. (2009). Even with Polyak’s averaging Bach and Moulines (2011); Xu (2011), there are cases where SGD’s convergence rate still can not be faster than O(ln t/t) Shamir (2011). Numerical experiments on real-world datasets also indicate that ANSGD converges much faster in comparing with these state-of-the-art algorithms.\nA perturbation-based smoothing method is recently proposed for stochastic nonsmooth minimization Duchi et al. (2011). This work achieves similar iteration complexities as ours, in a parallel computation scenario. In serial settings, ANSGD enjoys better and optimal bounds.\nIn machine learning, many problems can be cast as minimizing a composition of a loss function and a regularization term. Before proceeding to the algorithm, we first describe a different setting of “composite minimizations” that we will pursue in this paper, along with our notations and assumptions."
    }, {
      "heading" : "1.1 A Different “Composite Setting”",
      "text" : "In the classic black-box setting of first-order stochastic algorithms Nemirovski et al. (2009), the structure of the objective function minx{f(x) = Eξf(x, ξ) : ξ ∼ P} is unknown. In each iteration t, an algorithm can only access the first-order stochastic oracle and obtain a subgradient f ′(x, ξt). The basic assumption is that f\n′(x) = Eξf ′(x, ξ) for any x, where the random vector ξ is from a fixed distribution P .\nThe composite setting (also known as splitting Lions and Mercier (1979)) is an extension of the black-box model. It was proposed to exploit the structure of objective functions. Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al. (2009); Xiao (2010). A stochastic composite function Φ(x) := f(x) + g(x) is the sum of a smooth stochastic convex function f(x) = Eξf(x, ξ) and a nonsmooth (but simple and deterministic) function g(). To minimize Φ, previous work construct the following model iteratively:\n〈∇f(xt, ξt),x − xt〉+ 1\nηt D(x,xt) + g(x), (1)\nwhere ∇f(xt, ξt) is a gradient, D(·, ·) is a proximal function (typically a Bregman divergence) and ηt is a stepsize.\nA successful application of the composite idea typically relies on the assumption that model (1) is easy to minimize. If g() is very simple, e.g. ‖x‖1 or the nuclear norm, it is straightforward to obtain the minimum in analytic forms. However, this assumption does not hold for many other applications in machine learning, where many loss functions (not the regularization term, here the nonsmooth g() becomes the nonsmooth loss function) are nonsmooth, and do not enjoy separability properties Wright et al. (2009). This includes important examples such as hinge loss, absolute loss, and ǫ-insensitive loss.\nIn this paper, we tackle this problem by studying a new stochastic composite setting: minxΦ(x) = f(x) + g(x), where loss function f() is convex and nonsmooth, while g() is\nconvex and Lg-Lipschitz smooth:\ng(x) ≤ g(y) + 〈∇g(y),x − y〉+ Lg 2 ‖x− y‖2. (2)\nFor clarity, in this paper we focus on unconstrained minimizations. Without loss of generality, we assume that both f() and g() are stochastic: f(x) = Eξf(x, ξ) and g(x) = Eξg(x, ξ), where ξ has distribution P . If either one is deterministic, its ξ is then dropped. To make our algorithm and analysis more general, we assume that g() is µ-strongly convex: ∀x,y,\ng(x) ≥ g(y) + 〈∇g(y),x − y〉+ µ 2 ‖x− y‖2. (3)\nIf it is not strongly convex, one can simply take µ = 0.\nThe main idea of our algorithm again stems from exploiting the structures of f() and g(). In Section 2 we propose to form a smooth stochastic approximation of f(), such that the optimal methods Nesterov (2004) can be applied to attain optimal convergence rates. The convergence of our proposed algorithm is analyzed in Section 3, and a batch-to-online conversion is also proposed. Two popular machine learning problems are chosen as our examples in Section 4, and numerical evaluations are presented in Section 5. All proofs in this paper are provided in the appendix."
    }, {
      "heading" : "2. Approach",
      "text" : ""
    }, {
      "heading" : "2.1 Stochastic Smoothing Method",
      "text" : "An important breakthrough in nonsmooth minimization was made by Nesterov in a series of works Nesterov (2005b,a, 2007b). By exploiting function structures, Nesterov shows that in many applications, minimizing a well-structured nonsmooth function f(x) can be formulated as an equivalent saddle-point form\nmin x∈X f(x) = min x∈X max u∈U\n[ 〈Ax,u〉 −Q(u) ] , (4)\nwhere u ∈ Rm, U ⊆ Rm is a convex set, A is a linear operator mapping RD → Rm and Q(u) is a continuous convex function. Inserting a non-negative ζ-strongly convex function ω(u) in (4) one obtains a smooth approximation of the original nonsmooth function\nf̂(x, γ) := max u∈U\n[ 〈Ax,u〉 −Q(u)− γω(u) ] , (5)\nwhere γ > 0 is a fixed smoothness parameter which is crucial in the convergence analysis. The key property of this approximation is:\nLemma 1 Nesterov (2005b)(Theorem 1) Function f̂(x, γ) is convex and continuously differentiable, and its gradient is Lipschitz continuous with constant Lf̂ := ‖A‖2 γζ , where\n‖A‖ := max x,u {〈Ax,u〉 : ‖x‖ = 1, ‖u‖ = 1}. (6)\nNesterov’s smoothing method was originally proposed for deterministic optimization. A major drawback of this method is that the number of iterations N must be known beforehand, such that the algorithm can set a proper smoothness parameter γ = O\n(2‖A‖ N+1 )\nto ensure convergence. This makes it unsuitable for algorithms that runs forever, or whose number of iterations is not known. Following his work we propose to extend this smoothing method to stochastic optimization. Our stochastic smoothing differs from the deterministic one in the operator A and smoothness parameter γ, where both will be time-varying.\nWe assume that the nonsmooth part f(x, ξ) of the stochastic composite function Φ() is well structured, i.e. for a specific realization ξt, it has an equivalent form like the max function in (4):\nf(x, ξt) = max u∈U\n[ 〈Aξtx,u〉 −Q(u) ] , (7)\nwhere Aξt is a stochastic linear operator associated with ξt. We construct a smooth approximation of this function as:\nf̂(x, ξt, γt) := max u∈U\n[ 〈Aξtx,u〉 −Q(u)− γtω(u) ] , (8)\nwhere γt is a time-varying smoothness parameter only associated with iteration index t, and is independent of ξt. Function ω() is non-negative and ζ-strongly convex. Due to Lemma 1, f̂(x, ξt, γt) is ‖Aξt‖2 γtζ -Lipschitz smooth. It follows that\nLemma 2 ∀x,y, t, Eξf̂(x, ξ, γt) ≤ Eξf̂(y, ξ, γt)+Eξ〈∇f̂(y, ξ, γt),x−y〉+ Eξ‖Aξ‖ 2\nγtζ ‖x−y‖2.\nWe have the following observation about our composite objective Φ(), which relates the reduction of the original and approximated function values.\nLemma 3 For any x,xt, t,\nΦ(xt)− Φ(x) ≤ Eξ [ f̂(xt, ξ, γt) + g(xt, ξ) ] − Eξ [ f̂(x, ξ, γt) + g(x, ξ) ] + γtDU , (9)\nwhere DU := maxu∈U ω(u).\n2.2 Accelerated Nonsmooth SGD (ANSGD)\nWe are now ready to present our algorithm ANSGD (Algorithm 1). This stochastic algorithm is obtained by applying Nesterov’s optimal method to our smooth surrogate function, and thus has a similar form to that of his original deterministic method Nesterov (2004)(p.78). However, our convergence analysis is more straightforward, and does not rely on the concept of estimate sequences. Hence it is easier to identify proper series γt, ηt, αt and θt that are crucial in achieving fast rates of convergence. These series will be determined in our main results (Thm.6 and 7)."
    }, {
      "heading" : "3. Convergence Analysis",
      "text" : "To clarify our presentation, we use Table 1 to list some notations that will be used throughout the paper.\nAlgorithm 1 Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD)\nINPUT: series γt, ηt, θt ≥ 0 and 0 ≤ αt ≤ 1; OUTPUT: xt+1; [0.] Initialize x0 and v0; for t = 0, 1, 2, . . . do\n[1.] yt ← (1−αt)(µ+θt)xt+αtθtvtµ(1−αt)+θt [2.] f̂t+1(x) ← max\nu∈U\n[ 〈Aξt+1x,u〉 −Q(u)− γt+1ω(u) ]\n[3.] xt+1 ← yt − ηt [ ∇f̂t+1(yt) +∇gt+1(yt) ]\n[4.] vt+1 ← θtvt+µyt−[∇f̂t+1(yt)+∇gt+1(yt)]\nµ+θt end for\nOur convergence rates are based on the following main lemma, which bounds the progressive reduction ∆t of the smoothed function value. Actually Line 1, 3, and 4 of Alg.1 are also derived from the proof of this lemma.\nLemma 4 Let γt be monotonically decreasing. Applying algorithm ANSGD to nonsmooth composite function Φ(), we have ∀x and ∀t ≥ 0,\n∆t+1 ≤ (1− αt)∆t + (1− αt)(γt − γt+1)DU+\nΓt+1 + αt 2\n[ θt‖x− vt‖2 − (µ+ θt)‖x− vt+1‖2 ] +\nηtpq +\n[\nαt 2(µ+ θt) + Lt+1 2 η2t − ηt ] q2\n(10)\nwhere p := ‖σt+1(yt)‖ and q := ‖∇f̂t+1(yt) +∇gt+1(yt)‖."
    }, {
      "heading" : "3.1 How to Choose Stepsizes ηt",
      "text" : "In the RHS of (10), nonnegative scalars p, q ≥ 0 are data-dependent, and could be arbitrarily large. Hence we need to set proper stepsizes ηt such that the last two terms in (10) are\nnon-positive. One might conjecture that: there exist a series ct ≥ 0 such that\nηtpq +\n[\nαt 2(µ + θt) + Lt+1 2 η2t − ηt ] q2 ≤ ctp2. (11)\nIt is easy to verify that if we take ηt = αt µ+θt and any series ct ≥ αt2(µ+θt−αtLt+1) ≥ 0, then (11) is satisfied. To retain a tight bound, we take\nct = αt\n2(µ + θt − αtLt+1) . (12)\nTaking expectation on both sides of (10) and noticing that Eξt+1|ξ[t]Γt+1 = 0, Eξt+1ct ≤ αt 2(µ+θt−αtEξt+1Lt+1) due to Jensen’s inequality, we have Lemma 5 ∀x and ∀t ≥ 0, E∆t+1 ≤ (1− αt)E∆t + αtθtD2t − αt(µ+ θt)D2t+1 +\nαt 2(µ + θt − αtELt+1) σ2 + (1− αt)(γt − γt+1)DU , (13)\nThe optimal convergence rates of our algorithm differs according to the fact of µ (positive or not). They are presented separately in the following two subsections, where the choices of γt, θt, αt will also be determined."
    }, {
      "heading" : "3.2 Optimal Rates for Composite Minimizations when µ = 0",
      "text" : "When µ = 0, g() is only convex and Lg-Lipschitz smooth, but not assumed to be strongly convex.\nTheorem 6 Take αt = 2 t+2 , γt+1 = αt, θt = Lgαt + Ω√ αt\n+ E‖Aξ‖2\nζ and ηt = αt θt in Alg.1,\nwhere Ω is a constant. We have ∀x and ∀t ≥ 0,\nE [Φ(xt+1)− Φ(x)] ≤ 4LgD\n2\n(t+ 2)2 + 2E‖Aξ‖2D2/ζ + 4DU t+ 2 +\n√ 2(ΩD2 + σ2/Ω)√\nt+ 2 , (14)\nwhere D2 := maxiD 2 i .\nIn this result, the variance bound is optimal up to a constant factor Agarwal et al. (2012). The dominating factor is still due to the stochasticity, but not affected by the nonsmoothness of f(). Taking the parameter Ω = σ/D, this last term becomes 2 √ 2Dσ√ t+2 . This bound is better than that of stochastic gradient descent or stochastic dual averaging Dekel et al. (2010) for minimizing L-Lipschitz smooth functions, whose rate is O (\nLD20 t + D20+σ 2 √ t\n)\n; without the\nsmooth function g(), our bound is of the same order as it, keeping in mind that our rate is for nonsmooth minimizations. This fact underscores the potential of using stochastic optimal methods for nonsmooth functions.\nThe diminishing smoothness parameter γt = 2\nt+2 indicates that initially a smoother approximation is preferred, such that the solution does not change wildly due to the nonsmoothness and stochasticity. Eventually the approximated function should be closer and closer to the original nonsmooth function, such that the optimality can be reached. Some concrete examples are given in Fig.1.\nThe E‖Aξ‖2 in our bound is a theoretical constant. In Sec.4 we demonstrate a sampling method, and it turns out to work quite well in estimating E‖Aξ‖2."
    }, {
      "heading" : "3.3 Nearly Optimal Rates for Strongly Convex Minimizations",
      "text" : "When µ > 0, g() is strongly convex, and the convergence rate of ANSGD can be improved to O(1/t).\nTheorem 7 Take αt = 2 t+1 , γt+1 = αt, θt = Lgαt + µ 2αt\n+ E‖Aξ‖2\nζ − µ and ηt = αtµ+θt in Alg.1. Denote\nC := max\n{\n4E‖Aξ‖2 ζµ , 2 ( Lg µ\n)1/3 }\n. (15)\nWe have ∀x and ∀t ≥ 0,\nE [Φ(xt+1)− Φ(x)] ≤ 6.58LgD̃\n2\nt(t+ 1) + B + 4DU t+ 1 +\nσ2\nµ(t+ 1) , (16)\nwhere\nB :=\n\n\n\n2E‖Aξ‖2D̃2/ζ t+1 if 0 ≤ t < C, 2(C−2)E‖Aξ‖2D̃2/ζ t(t+1) if t ≥ C,\n(17)\nand D̃2 := max0≤i≤min{t,C} D 2 i .\nNote that C is the smallest iteration index for which one can retain 1/t2 rates for the E‖Aξ‖2 part (B). Without any knowledge about Lg, µ and E‖Aξ‖2, one can set a parameter Ω and take θt = Lgαt+\nµ 2αt\n+ E‖Aξ‖2\nΩζ −µ in the algorithm. In our experiments, we observe that one can take Ω fairly large (of O(E‖Aξ‖2)), meaning that C can be very small (O(1)), and B is O( 1t2 ) for all t. In this sense, strongly convex ANSGD is almost parameter-free. Without the O(1/t) rate of DU , all terms in our bound are optimal. This is why our rate is called “nearly” optimal. In practice, DU is usually small, and it will be dominated by the last term σ 2\nµ(t+1) ."
    }, {
      "heading" : "3.4 Batch-to-Online Conversion",
      "text" : "The performance of an online learning (online convex minimization) algorithm is typically measured by regret, which can be expressed as\nR(t) :=\nt−1 ∑\ni=0\n[ Φ(xi, ξi+1)− Φ(x∗t , ξi+1) ] , (18)\nwhere x∗t := argminx ∑t−1 i=0 [ Φ(x, ξi+1) ]\n. In the learning theory literature, many approaches are proposed which use online learning algorithms for batch learning (stochastic optimization), called “online-to-batch” (O-to-B) conversions. For convex functions, many of these approaches employ an “averaged” solution as the final solution.\nOn the contrary, we show that stochastic optimization algorithms can also be used directly for online learning. This “batch-to-online” (B-to-O) conversion is almost free of any additional effort: under i.i.d. assumptions of data, one can use any stochastic optimization algorithm for online learning.\nProposition 8 For any t ≥ 0, Eξ[t]R(t) ≤\nt−1 ∑\ni=0\nEξ[i] [Φ(xi)− Φ(x∗)] + Eξ[t]\nt−1 ∑\ni=0\n[ Φ(x∗t )− Φ(x∗t , ξi+1) ]\n(19)\nwhere x∗ := argminxΦ(x) and x∗t := argminx ∑t−1 i=0 [ Φ(x, ξi+1) ] .\nWhen Φ() is convex, the second term in (19) can be bounded by applying standard results in uniform convergence (e.g. Boucheron et al. (2005)):\n∑t−1 i=1 Φ(x ∗ t )−Φ(x∗t , ξi+1) = O(\n√ t).\nTogether with summing up the RHS of (14), we can obtain an O( √ t) regret bound. When Φ() is strongly convex, the second term in (19) can be bounded using Shalev-Shwartz et al. (2009):\n∑t−1 i=1 Φ(x ∗ t ) − Φ(x∗t , ξi+1) = O(ln t). Together with summing up the RHS of (16),\nan O(ln t) regret bound is achieved. The O( √ t) and O(ln t) regret bounds are known\nUsing our proposed ANSGD for online learning by B-to-O achieves the same (optimal) regret bounds as state-of-the-art algorithms designated for online learning. However, using O-to-B, one can only retain an O(ln t/t) rate of convergence for stochastic strongly convex optimization. From this perspective, O-to-B is inferior to B-to-O. The sub-optimality of O-to-B is also discussed in Hazan and Kale (2011)."
    }, {
      "heading" : "4. Examples",
      "text" : "In this section, two nonsmooth functions are given as examples. We will show how these functions can be stochastically approximated, and how to calculate parameters used in our algorithm."
    }, {
      "heading" : "4.1 Hinge Loss SVM Classification",
      "text" : "Hinge loss is a convex surrogate of the 0 − 1 loss. Denote a sample-label pair as ξ := {s, l} ∼ P , where s ∈ RD and l ∈ R. Hinge loss can be expressed as fhinge(x) := max{0, 1− lsTx}. It has been widely used for SVM classifiers where the objective is minΦ(x) = minEξfhinge(x) + λ 2‖x‖2. Note that the regularization term g(x) = λ2 ‖x‖2 is λ-strongly convex, hence according to Thm.7, ANSGD enjoys O(1/(λt)) rates. Taking ω(u) = 12‖u‖2 in (8), it is easy to check that the smooth stochastic approximation of hinge loss is\nf̂hinge(x, ξt, γt) = max 0≤u≤1\n{\nu ( 1− ltsTt x ) − γt u2\n2\n}\n. (20)\nThis maximization is simple enough such that we can obtain an equivalent smooth representation:\nf̂hinge(x, ξt, γt) =\n\n \n \n0 if lts T t x ≥ 1, (1−ltsTt x)2 2γt\nif 1− γt ≤ ltsTt x < 1, 1− ltsTt x− γt2 if ltsTt x < 1− γt.\n(21)\nSeveral examples of f̂hinge with varying γt are plotted in Fig.1(left) in comparing with the hinge loss.\nHere u is a scalar, hence it is straightforward to calculate E‖Aξ‖2\nζ , which will be used to generate sequences θt. In binary classification, suppose l ∈ {1,−1}. Using definition (6),\none only needs to calculate E(max‖x‖=1 s T t x) 2. Practically one can take a small subset of k random samples si (e.g. k = 100), and calculate the sample average of the squared norms 1 k ∑k i=1 ‖si‖2. This yields 1k ∑k i=1(max‖x‖=1 s T i x) 2, an estimate of E‖Aξ‖2."
    }, {
      "heading" : "4.2 Absolute Loss Robust Regression",
      "text" : "Absolute loss is an alternative to the popular squared loss for robust regressions Hastie et al. (2009). Using same notations as Sec.4.1 it can be expressed as fabs(x) := |l− sTx|. Taking ω(u) = 12‖u‖2 in (8), its smooth stochastic approximation can be expressed as\nf̂abs(x, ξt, γt) = max−1≤u≤1\n{\nu(lt − sTt x)− γt u2\n2\n}\n. (22)\nSolving this maximization wrt u we obtain an equivalent form:\nf̂abs(x, ξt, γt) =\n\n \n \nlt − sTt x− γt2 if lt − sTt x ≥ γt, (lt−sTt x)2\n2γt if − γt ≤ lt − sTt x < γt, −(lt − sTt x)− γt2 if lt − sTt x < −γt. (23)\nThis approximation looks similar to the well-studied Huber loss Huber (1964), though they are different. Actually they share the same form only when γt = 0.5 (green curve in Fig.1 Right).\nThe parameter E‖Aξ‖2 can be estimated in a similar way as discussed in Sec.4.1."
    }, {
      "heading" : "5. Experimental Results",
      "text" : "In this section, five publicly available datasets from various application domains will be used to evaluate the efficiency of ANSGD. Datasets “svmguide1”, “real-sim”, “rcv1” and “alpha” are for binary classifications, and “abalone” is for robust regressions.1\nFollowing our examples in Sec.4, we will evaluate our algorithm using approximated hinge loss for classifications, and approximated absolute loss for regressions. Exact hinge and absolute losses will be used for subgradient descent algorithms that we will compare with, as described in the following section. All losses are squared-l2-norm-regularized. The regularization parameter λ is shown on each figure. When assuming strong-convexity, we take µ = λ."
    }, {
      "heading" : "5.1 Algorithms for Comparison and Parameters",
      "text" : "We compare ANSGD with three state-of-the-art algorithms. Each algorithm has a datadependent tuning parameter, denoted by Ω (although they have different physical meanings). The best values of Ω are found based on a tuning subset of samples. Note that when assuming strong-convexity, our ANSGD is almost parameter-free. As discussed after Thm.7, our experiments indicate that the optimal Ω is taken such that E‖Aξ‖2\nΩζ ≈ 1, meaning that one can simply take θt = Lgαt +\nµ 2αt + 1− µ. SGD. The classic stochastic approximation Robbins and Monro (1951) is adopted: xt+1 ←\nxt − ηtf ′(xt), where f ′(xt) is the subgradient. When only assuming convexity (µ = 0), we use stepsize ηt =\nΩ√ t . When assuming strong-convexity, we follow the stepsize used in SGD2\nBottou: ηt = 1\nµ(t+Ω) .\nAveraged SGD. This is algorithmically the same as SGD, except that the averaged result x̄ := 1t ∑t i=1 xi is used for testing. We follow the stepsizes suggested by the recent work on the non-asymptotic analysis of SGD Bach and Moulines (2011); Xu (2011), where it is argued that Polyak’s averaging combining with proper stepsizes yield optimal rates. When only assuming convexity, we use stepsizes ηt =\nΩ√ t Bach and Moulines (2011). When\nassuming strong convexity, the stepsize is taken as ηt = 1\nΩ(1+µt/Ω)3/4 Xu (2011).\nAC-SA. This approach Lan (2010); Lan and Ghadimi (2011) is interesting to compare because like ANSGD, it is another way of obtaining a stochastic algorithm based on Nesterov’s optimal method, begging the question of whether it has similar behavior. Theoretically, according to Prop.8 and 9 in Lan and Ghadimi (2011), the bound for the nonsmooth part is of O(1/ √ t) for µ = 0 and O(1/t) for µ > 0. In comparison, our nonsmooth part converges in O(1/t) for µ = 0 and O(1/t2) for µ > 0. Numerically we observe that directly applying AC-SA to nonsmooth functions results in inferior performances.\n1. Dataset “alpha” is obtained from ftp://largescale.ml.tu-berlin.de/largescale/, and the other four datasets can be accessed via http://www.csie.ntu.edu.tw/~cjlin/libsvmtools. Dataset “rcv1” comes with 20, 242 training samples and 677, 399 testing samples. For “svmguide1” and “real-sim”, we randomly take 60% of the samples for training and 40% for testing. For “alpha” and “abalone”, 80% are used for training, and the rest 20% are used for testing."
    }, {
      "heading" : "5.2 Results",
      "text" : "Due to the stochasticity of all the algorithms, for each setting of the experiments, we run the program for 10 times, and plot the mean and standard deviation of the results using error bars.\nIn the first set of experiments, we compare ANSGD with two subgradient-based algorithms SGD and Averaged SGD. Classification results are shown in Fig.2, 3, 4 and 5, and regression results are shown in Fig.6. In each figure, the left column is for algorithms without strongly convex assumptions, while in the right column the algorithms assume strong-convexity and take µ = λ. For classification results, we plot function values over the testing set in the first row, and plot testing accuracies in the second row.\nIt is clear that in all these experiments, ANSGD’s function values converges consistently faster than the other two SGD algorithms. In non-strongly convex experiments, it converges significantly faster than SGD and its averaged version. In strongly convex experiments, it still out performs, and is more robust than strongly convex SGD. Averaged SGD performs well in strongly convex settings, in terms of prediction accuracies, although its errors are still higher than ANSGD in the first three datasets. The only exception is in “alpha” (Fig.5), where Averaged SGD retains higher function values than ANSGD, but its accuracies are contradictorily higher in early stages. The reason might be that the inexact solution serves as an additional regularization factor, which cannot be predicted by the analysis of convergence rates.\nIn the second set of experiments, we compare ANSGD with AC-SA and its strongly convex version. Results are in Fig.7, 8, 9 and 10. In all experiments our ANSGD significantly outperforms AC-SA, and is much more stable. These experiments confirm the theoretically better rates discussed in Sec.5.1."
    }, {
      "heading" : "6. Conclusions and Future Work",
      "text" : "We introduce a different composite setting for nonsmooth functions. Under this setting we propose a stochastic smoothing method and a novel stochastic algorithm ANSGD. Convergence analysis show that it achieves (nearly) optimal rates under both convex and strongly\nconvex assumptions. We also propose a “Batch-to-Online” conversion for online learning, and show that optimal regrets can be obtained.\nWe will extend our method to constrained minimizations, as well as cases when the approximated function f̂() is not easily obtained by maximizing u. Nesterov’s excessive gap technique has the “true” optimal 1/t2 bound, and we will investigate the possibility of integrating it in our algorithm. Exploiting links with statistical learning theories may also be promising."
    }, {
      "heading" : "Appendix A. Proof of Lemma 3",
      "text" : "Proof\nΦ(xt)− Φ(x) = [f(xt)− f(x)] + [g(xt)− g(x)] = Eξ [f(xt, ξ)] + Eξ [−f(x, ξ) + g(xt, ξ)− g(x, ξ)]\n= Eξ max u∈U\n{\n[ 〈Aξxt,u〉 −Q(u)− γtω(u) ] + γtω(u)\n}\n+ Eξ [−f(x, ξ) + g(xt, ξ)− g(x, ξ)]\n≤ Eξ max u∈U\n[ 〈Aξxt,u〉 −Q(u)− γtω(u) ]\n+max u∈U\n[ γtω(u) ] + Eξ [−f(x, ξ) + g(xt, ξ)− g(x, ξ)]\n= Eξ\n[ f̂(xt, ξ, γt) ] + γtDU + Eξ [−f(x, ξ) + g(xt, ξ)− g(x, ξ)]\n≤ Eξ [ f̂(xt, ξ, γt)− f̂(x, ξ, γt) ]\n+ Eξ [g(xt, ξ)− g(x, ξ)] + γtDU . (24)\nThe last inequality is due to the non-negativity of ω() and definitions of f (7) and f̂ (8)."
    }, {
      "heading" : "Appendix B. Proof of Lemma 4",
      "text" : "Before proceeding to the proof of this lemma, we present two auxiliary results. For clarity, in the following lemmas and proofs we use the following notations to denote the smoothly approximated composite function and its expectation:\nFt(x, γt) := f̂t(x) + gt(x) = f̂(x, ξt, γt) + g(x, ξt) (25)\nand F (x, γt) := EξtFt(x, γt). (26)\nThe first lemma is on the smoothly approximated function and the smoothness parameter γt.\nLemma 9 If γt is monotonically decreasing with t, for any x and t ≥ 0,\nF (x, γt) ≤ F (x, γt+1) ≤ F (x, γt) + (γt − γt+1)DU , (27)\nwhere DU := maxu∈U ω(u).\nProof The left inequality is obvious, since γt ≥ γt+1 and ω(u) is nonnegative. For the right inequality,\nF (x, γt+1)− F (x, γt) = Eξf̂(x, ξ, γt+1)− Eξf̂(x, ξ, γt) = max\nu∈U [〈EξAξx,u〉 −Q(u)− γt+1ω(u)]−max u∈U [〈EξAξx,u〉 −Q(u)− γtω(u)]\n≤ max u∈U\n{\n[ 〈EξAξx,u〉 −Q(u)− γt+1ω(u) ] − [ 〈EξAξx,u〉 −Q(u)− γtω(u) ]\n}\n= max u∈U [(γt − γt+1)ω(u)] . (28)\nThe second lemma is about proximal methods using Bregman divergence as proxfunctions, which is a direct result of optimality conditions. It appeared in Lan and Ghadimi (2011)(Lemma 2), and is an extension of the “3-point identity” Chen and Teboulle (1993)(Lemma 3.1).\nLemma 10 Lan and Ghadimi (2011) Let l(x) be a convex function. Let scalars s1, s2 ≥ 0. For any vectors u and v, denote their Bregman divergence as D(u,v). If ∀x,u,v\nx∗ = argmin x l(x) + s1D(u,x) + s2D(v,x), (29)\nthen\nl(x) + s1D(u,x) + s2D(v,x) ≥ l(x∗) + s1D(u,x∗) + s2D(v,x∗) + (s1 + s2)D(x∗,x). (30)\nWe are now ready to prove Lemma 4. Proof [Proof of Lemma 4] Due to Lemma 2 and Lipschitz-smoothness of g(x), F (x, γt+1) has a Lipschitz smooth constant LFt+1 := Eξ‖Aξ‖2 γt+1ζ + Lg. It follows that\nF (xt+1, γt+1)\n≤ F (yt, γt+1) + 〈∇F (yt, γt+1),xt+1 − yt〉+ LFt+1 2 ‖xt+1 − yt‖2 = (1− αt)F (yt, γt+1) + αtF (yt, γt+1) + 〈∇F (yt, γt+1),xt+1 − yt〉+ LFt+1 2 ‖xt+1 − yt‖2 = (1− αt)F (yt, γt+1) + 〈∇F (yt, γt+1), (1 − αt)(xt − yt)〉+\nαtF (yt, γt+1) + 〈∇F (yt, γt+1),xt+1 − yt − (1− αt)(xt − yt)〉+ LFt+1 2 ‖xt+1 − yt‖2\n≤ (1− αt)F (xt, γt+1) + αtF (yt, γt+1) + 〈∇F (yt, γt+1),xt+1 − yt − (1− αt)(xt − yt)〉+ LFt+1 2\n‖xt+1 − yt‖2, (31)\nwhere the last inequality is due to the convexity of F (). Subtracting F (x, γt+1) from both sides of the above inequality we have:\nF (xt+1, γt+1)− F (x, γt+1) ≤ (1− αt)F (xt, γt+1)− F (x, γt+1)\n+ αtF (yt, γt+1) + 〈∇F (yt, γt+1),xt+1 − yt − (1− αt)(xt − yt)〉+ LFt+1 2 ‖xt+1 − yt‖2 ≤ (1− αt) [ F (xt, γt) + (γt − γt+1)DU ] − F (x, γt+1) + αtF (yt, γt+1) + 〈∇F (yt, γt+1),xt+1 − yt − (1− αt)(xt − yt)〉+ LFt+1 2 ‖xt+1 − yt‖2 ≤ (1− αt) [ F (xt, γt)− F (x, γt) ] − αtF (x, γt+1) + (1− αt)(γt − γt+1)DU + αtF (yt, γt+1) + 〈∇F (yt, γt+1),xt+1 − yt − (1− αt)(xt − yt)〉+ LFt+1 2\n‖xt+1 − yt‖2, (32)\nwhere the last two inequalities are due to Lemma 9. Denoting ∆t := F (xt, γt)−F (x, γt) and σt(x) := ∇Ft(x, γt)−∇F (x, γt) we can rewrite (32) as:\n∆t+1 − (1− αt)∆t − (1− αt)(γt − γt+1)DU\n≤ αtF (yt, γt+1)− αtF (x, γt+1) + 〈∇F (yt, γt+1),xt+1 − yt − (1− αt)(xt − yt)〉+ LFt+1 2 ‖xt+1 − yt‖2 (3) ≤ αtF (yt, γt+1)− αt [ F (yt, γt+1) + 〈∇F (yt, γt+1),x− yt〉+ µ\n2 ‖x− yt‖2\n]\n+\n〈∇F (yt, γt+1),xt+1 − yt − (1− αt)(xt − yt)〉+ LFt+1 2 ‖xt+1 − yt‖2\n= −αt [ 〈∇Ft+1(yt, γt+1)− σt+1(yt),x− yt〉+ µ\n2 ‖x− yt‖2\n]\n+\n〈∇F (yt, γt+1),xt+1 − yt − (1− αt)(xt − yt)〉+ LFt+1 2 ‖xt+1 − yt‖2\n= −αt [ 〈∇Ft+1(yt, γt+1),x− yt〉+ µ\n2 ‖x− yt‖2 + θt 2 ‖x− vt‖2\n]\n+ αtθt 2 ‖x− vt‖2+\n〈∇F (yt, γt+1),xt+1 − yt − (1− αt)(xt − yt)〉+ LFt+1 2 ‖xt+1 − yt‖2 + 〈σt+1(yt), αt(x− yt)〉\n≤ −αt [ 〈∇Ft+1(yt, γt+1),vt+1 − yt〉+ µ\n2 ‖vt+1 − yt‖2 + θt 2 ‖vt+1 − vt‖2 + µ+ θt 2 ‖x− vt+1‖2 ] +\nαtθt 2 ‖x− vt‖2 + 〈∇F (yt, γt+1),xt+1 − yt − (1− αt)(xt − yt)〉+ LFt+1 2 ‖xt+1 − yt‖2+ 〈σt+1(yt), αt(x− yt)〉, (33)\nwhere the last inequality is due to Lemma 10 (takingD(u,v) = 12‖u−v‖2) and the definition of vt+1:\nvt+1 := argmin x\n〈∇Ft+1(yt, γt+1),x− yt〉+ µ\n2 ‖x− yt‖2 + θt 2 ‖x− vt‖2. (34)\nMinimizing the above directly leads to Line 4 of Alg.1:\nvt+1 = θtvt + µyt −∇Ft+1(yt, γt+1)\nµ+ θt . (35)\nBase on this updating rule, it is easy to verify the following inequality:\n− αt [ µ\n2 ‖vt+1 − yt‖2 + θt 2 ‖vt+1 − vt‖2\n]\n≤ −αt 2\n[\nµθt µ+ θt ‖vt − yt‖2 + 1 µ+ θt ‖∇Ft+1(yt, γt+1)‖2\n]\n≤ −αt 2 (µ+ θt) ‖∇Ft+1(yt, γt+1)‖2.\n(36)\nTo set xt+1 (Line 3 of Alg.1), we follow the classic stochastic gradient descent, such that ‖xt+1−yt‖2 can be bounded in terms of ‖∇Ft+1(yt, γt+1)‖2: xt+1 = yt−ηt∇Ft+1(yt, γt+1).\nHence\n‖xt+1 − yt‖2 = η2t ‖∇Ft+1(yt, γt+1)‖2, (37)\nand\n〈∇F (yt, γt+1),xt+1 − yt〉 = 〈∇Ft+1(yt, γt+1)− σt+1(yt),xt+1 − yt〉 ≤ −ηt‖∇Ft+1(yt, γt+1)‖2 + ηt‖σt+1(yt)‖ · ‖∇Ft+1(yt, γt+1)‖.\n(38)\nInserting (35,36,37 and 38) into (33) we have\n∆t+1 ≤ (1 − αt)∆t + (1− αt)(γt − γt+1)DU+ αt 2 [ θt‖x− vt‖2 − (µ + θt)‖x− vt+1‖2 ] + 〈σt+1(yt), αt(x− yt) + (1− αt)(xt − yt)〉+ ηt‖σt+1(yt)‖ · ‖∇Ft+1(yt, γt+1)‖+ [\nαt 2(µ + θt) + Lt+1 2 η2t − ηt ]\n‖∇Ft+1(yt, γt+1)‖2+ 〈\n∇Ft+1(yt, γt+1), −αtθt(vt − yt)\nµ+ θt − (1− αt)(xt − yt)\n〉\n.\n(39)\nTaking the last term −αtθt(vt−yt)µ+θt − (1 − αt)(xt − yt) = 0 recovers the updating rule of yt (Line 1 of Alg.1). Hence our result follows."
    }, {
      "heading" : "Appendix C. Proof of Theorem 6",
      "text" : "Proof It is easy to verify that by taking αt = 2 t+2 , γt+1 = αt and θt = Lgαt+ E‖Aξ‖2 ζ + Ω√ αt , we have ∀t > 1: (1− αt−1)(γt−1 − γt) ≤ γt − γt+1, (40)\nand\n(1− αt) αt−1 2(θt−1 − αt−1ELt) ≤ αt 2(θt − αtELt+1) . (41)\nNext we define and bound weighted sums of D2t that will be used later.\nΨ(t) := [αtθt − (1− αt)αt−1θt−1]D2t + (1− αt) [αt−1θt−1 − (1− αt−1)αt−2θt−2]D2t−1+ (1− αt)(1− αt−1) [αt−2θt−2 − (1− αt−2)αt−3θt−3]D2t−2 + · · · ,\n(42)\nwhere replacing αt and θt by their definitions we have ∀t:\nαtθt− (1−αt)αt−1θt−1 = 4Lg\n(t+ 1)2(t+ 2)2 + 2E‖Aξ‖2/ζ (t+ 1)(t+ 2) +\n√ 2 [ (t+ 1) √ t+ 2− t √ t+ 1 ] Ω\n(t+ 1)(t+ 2) (43)\nSubstituting (43) into (42) and using invoking the definition of D2 we have ∀t:\nΨ(t) ≤ 4LgD2 [\n1\n(t+ 1)2(t+ 2)2 +\nt(t+ 1)\n(t+ 1)(t+ 2)\n1\nt2(t+ 1)2 + (t− 1)t (t+ 1)(t+ 2)\n1 (t− 1)2t2 + · · · ]\n+ 2E‖Aξ‖2D2\nζ\n[\n1\n(t+ 1)(t+ 2) +\nt(t+ 1)\n(t+ 1)(t+ 2)\n1\nt(t+ 1) + (t− 1)t (t+ 1)(t+ 2)\n1 (t− 1)t + · · · ]\n+ √ 2ΩD2 [\n(t+ 1) √ t+ 2− t √ t+ 1\n(t+ 1)(t+ 2) +\nt(t+ 1)\n(t+ 1)(t+ 2)\nt √ t+ 1− (t− 1) √ t\nt(t+ 1) +\n(t− 1)t (t+ 1)(t + 2)\n(t− 1) √ t− (t− 2) √ t− 1\n(t− 1)t + · · · ]\n= 4LgD\n2\n(t+ 1)(t + 2)\n[(\n1 t+ 1 − 1 t+ 2\n)\n+\n(\n1 t − 1 t+ 1\n)\n+\n(\n1 t− 1 − 1 t\n) + · · · ]\n+ 2E‖Aξ‖2D2\nζ\n[\n1\n(t+ 1)(t+ 2) +\n1\n(t+ 1)(t+ 2) +\n1\n(t+ 1)(t + 2) + · · ·\n]\n+\n√ 2ΩD2\n(t+ 1)(t + 2)\n[ (t+ 1) √ t+ 2− t √ t+ 1 + t √ t+ 1− (t− 1) √ t+ (t− 1) √ t− (t− 2) √ t− 1 + · · · ]\n≤ αtθtD2. (44)\nSince µ = 0, by recursively applying (13) and 1− α0 = 0 we have\nE∆t+1 ≤ (1− αt)E∆t + αtθt ( D2t −D2t+1 )\n+ αt\n2(θt − αtELt+1) σ2 + (1− αt)(γt − γt+1)DU\n≤ (1− αt)(1− αt−1)E∆t−1 + αtθt ( D2t −D2t+1 ) + (1− αt)αt−1θt−1 ( D2t−1 −D2t ) +\n2αt 2(θt − αtELt+1) σ2 + 2(1 − αt)(γt − γt+1)DU\n≤ · · · (42) ≤ t ∏\ni=0\n(1− αi)∆0 +Ψ(t) + (t+ 1)αt\n2(θt − αtELt+1) σ2 + (t+ 1)(1 − αt)(γt − γt+1)DU\n(44) ≤ αtθtD2 +\nσ2\nθt − αtELt+1 + 2DU t+ 2\n= [ α2tELt+1 +Ω √ αt ] D2 +\n√ αtσ 2\nΩ + 2DU t+ 2 .\n(45)\nCombining with Lemma 3 we have ∀x\nE [Φ(xt+1)− Φ(x)] ≤ [ α2tELt+1 +Ω √ αt ] D2 +\n√ αtσ 2\nΩ + 2DU t+ 2 + γt+1DU\n≤ α2tLgD2 + ( γt+1 + 2\nt+ 2\n)\nDU + α 2 t E‖Aξ‖2 γt+1ζ D2 + √ αt ( ΩD2 + σ2 Ω ) .\n(46)\nTaking γt+1 = αt = 2\nt+2 our result follows."
    }, {
      "heading" : "Appendix D. Proof of Theorem 7",
      "text" : "Proof It is easy to verify that by taking αt = 2 t+1 , we have ∀t ≥ 1\n(1− αt−1)(γt−1 − γt) ≤ γt − γt+1. (47)\nand\n(1− αt)α2t−1 ≤ α2t (48)\nDenote\nSt := αtθt − (1− αt)(αt−1θt−1 + µαt−1). (49)\nTaking θt = Lgαt + µ 2αt\n+ E‖Aξ‖2\nζ − µ it is easy to verify that ∀t ≥ 1:\nSt = 4Lg 1\n(t+ 1)2t2 + 2E‖Aξ‖2 ζ [ 1 t − 1 t+ 1 ] − µ t+ 1 . (50)\nWe want to find the smallest iteration index C such that: when t ≥ C, St ≤ 0. Without any knowledge about Lg and E‖Aξ‖2, minimizing St w.r.t t does not yield an analytic form of C. Hence we simply let\n4Lg 1 (t+ 1)2t2 ≤ µ 2(t+ 1) , (51)\nand\n2E‖Aξ‖2 ζ [ 1 t − 1 t+ 1 ] ≤ µ 2(t+ 1) . (52)\nInequality (51) is satisfied when\nt ≥ 2 ( Lg µ )1/3 , (53)\nand (52) is satisfied when\nt ≥ 4E‖Aξ‖ 2\nζµ . (54)\nCombining these two we reach the definition of C in (15). Next we proceed to prove the bound.\nAs defined in the theorem, we denote D̃2 = max0≤i≤min(t,C)D 2 i . By recursively applying\n(13) for 0 ≤ i ≤ t and noticing that St ≤ 0 ∀t ≥ C, 1− α1 = 0 we have\nE∆t+1 (47) ≤\nt ∏\ni=0 (1− αi)∆0 + (t+ 1)(1 − αt)(γt − γt+1)DU+ [\n(αtθt)D 2 t − (αtθt + µαt)D2t+1\n]\n+\n(1− αt) [ (αt−1θt−1)D 2 t−1 − (αt−1θt−1 + µαt−1)D2t ] + (1− αt)(1 − αt−1) [ (αt−2θt−2)D 2 t−2 − (αt−2θt−2 + µαt−2)D2t−1 ] + · · ·+ t ∏\ni=1\n(1− αi) [ (α0θ0)D 2 0 − (α0θ0 + µα0)D21 ] +\nσ2\nµ\n[\nα2t + (1− αt)α2t−1 + · · ·+ t ∏\ni=1\n(1− αi)α20\n]\n(48) ≤ 2DU\nt+ 1 + D̃2\nt ∏\ni=C−1 (1− αi) [αC−2θC−2 − (1− αC−2)(αC−3θC−3 + µαC−3)] +\nD̃2 t ∏\ni=C−2 (1− αi) [αC−3θC−3 − (1− αC−3)(αC−4θC−4 + µαC−4)] +\n· · · + D̃2 t ∏\ni=2\n(1− αi) [α1θ1 − (1− α1)(α0θ0 + µα0)] + tα2tσ 2\nµ\n(55)\nApplying (50) by ignoring the − µt+1 term to the above inequality we can bound the coefficients of Lg and E‖Aξ‖2 ζ parts separately as follows.\nWhen t ≥ C, for the Lg part: ∏t\ni=C−1(1− αi) (C − 1)2(C − 2)2 + ∏t i=C−2(1− αi) (C − 2)2(C − 3)2 + ∏t i=C−3(1− αi) (C − 3)2(C − 4)2 + · · ·+ ∏t i=2(1− αi) 22 · 12\n= 1\n(t+ 1)t\n[\n1\n(C + 2)(C + 1) +\n1\n(C + 1)C +\n1 C(C − 1)) + · · ·+ 1 2 · 1\n]\n≤ 1 (t+ 1)t\nC+1 ∑\ni=1\n1 i2 ≤ π\n2\n6t(t+ 1)\n(56)\nFor the E‖Aξ‖2\nζ part:\nΠti=C−1(1− αi) ( 1 C − 2 − 1 C − 1 ) +Πti=C−2(1− αi) ( 1 C − 3 − 1 C − 2 ) + · · · + t ∏\ni=2\n(1− αi) ( 1− 1 2 )\n= C − 1 (t+ 1)t − C − 2 (t+ 1)t + C − 2 (t+ 1)t − C − 3 (t+ 1)t + · · ·+ 2 (t+ 1)t − 1 (t+ 1)t = C − 1 (t+ 1)t − 1 (t+ 1)t = C − 2 t(t+ 1) .\n(57)\nCombining with Lemma 3 and taking γt+1 = αt = 2 t+1 we have ∀x:\nE [Φ(xt+1)− Φ(x)] ≤ 2DU t+ 1 + 2π2LgD̃ 2 3t(t+ 1) + 2(C − 2)E‖Aξ‖2D̃2/ζ t(t+ 1) + σ2 µ(t+ 1) + γt+1DU\n= 2π2LgD̃ 2\n3t(t+ 1) + 2(C − 2)E‖Aξ‖2D̃2/ζ t(t+ 1) + 4DU t+ 1 + σ2 µ(t+ 1) .\n(58)\nWhen 0 ≤ t ≤ C, one can simply put C = t in the above, and this completes our proof."
    }, {
      "heading" : "Appendix E. Proof of Proposition 8",
      "text" : "Proof\nEξ[t] R(t) = Eξ[t]\nt−1 ∑\ni=0\n[ Φ(xi, ξi+1)− Φ(x∗t , ξi+1) ]\n= Eξ[t]\nt−1 ∑\ni=0\n{\n[ Φ(xi, ξi+1)− Φ(x∗) ] + [ Φ(x∗)− Φ(x∗t , ξi+1) ]\n}\n=\nt−1 ∑\ni=0\nEξ[i+1]\n[ Φ(xi, ξi+1)− Φ(x∗) ] + Eξ[t]\nt−1 ∑\ni=0\n[Φ(x∗)− Φ(x∗t )] + Eξ[t] t−1 ∑\ni=0\n[ Φ(x∗t )− Φ(x∗t , ξi+1) ]\n≤ t−1 ∑\ni=0\nEξ[i+1]\n[ Φ(xi, ξi+1)− Φ(x∗) ] + Eξ[t]\nt−1 ∑\ni=0\n[ Φ(x∗t )− Φ(x∗t , ξi+1) ]\n= t−1 ∑\ni=0\nEξ[i] [Φ(xi)− Φ(x∗)] + Eξ[t]\nt−1 ∑\ni=0\n[ Φ(x∗t )− Φ(x∗t , ξi+1) ] ."
    } ],
    "references" : [ {
      "title" : "Theory of classification: A survey of some recent advances",
      "author" : [ "Stéphane Boucheron", "Olivier Bousquet", "Gábor Lugosi" ],
      "venue" : "ESAIM: Probability and Statistics,",
      "citeRegEx" : "Boucheron et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Boucheron et al\\.",
      "year" : 2005
    }, {
      "title" : "Convergence analysis of a proximal-like minimization algorithm using bregman functions",
      "author" : [ "Gong Chen", "Marc Teboulle" ],
      "venue" : "SIAM J. on Optimization,",
      "citeRegEx" : "Chen and Teboulle.,? \\Q1993\\E",
      "shortCiteRegEx" : "Chen and Teboulle.",
      "year" : 1993
    }, {
      "title" : "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint",
      "author" : [ "I. Daubechies", "M. Defrise", "C. De Mol" ],
      "venue" : "Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "Daubechies et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Daubechies et al\\.",
      "year" : 2004
    }, {
      "title" : "Optimal distributed online prediction using mini-batches",
      "author" : [ "Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao" ],
      "venue" : null,
      "citeRegEx" : "Dekel et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Dekel et al\\.",
      "year" : 2010
    }, {
      "title" : "Efficient online and batch learning using forward backward splitting",
      "author" : [ "John Duchi", "Yoram Singer" ],
      "venue" : "JMLR, (10):2899–2934,",
      "citeRegEx" : "Duchi and Singer.,? \\Q2009\\E",
      "shortCiteRegEx" : "Duchi and Singer.",
      "year" : 2009
    }, {
      "title" : "Randomized smoothing for stochastic optimization",
      "author" : [ "John Duchi", "Peter L. Bartlett", "Martin J. Wainwright" ],
      "venue" : null,
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
      "author" : [ "Trevor Hastie", "Robert Tibshirani", "Jerome Friedman" ],
      "venue" : null,
      "citeRegEx" : "Hastie et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2009
    }, {
      "title" : "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Hazan and Kale.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2011
    }, {
      "title" : "Accelerated gradient methods for stochastic optimization and online learning",
      "author" : [ "Chonghai Hu", "James T. Kwok", "Weike Pan" ],
      "venue" : "In NIPS 22,",
      "citeRegEx" : "Hu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2009
    }, {
      "title" : "Robust estimation of a location parameter",
      "author" : [ "Peter J. Huber" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "Huber.,? \\Q1964\\E",
      "shortCiteRegEx" : "Huber.",
      "year" : 1964
    }, {
      "title" : "Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, i: a generic algorithmic framework",
      "author" : [ "G. Lan", "S. Ghadimi" ],
      "venue" : "SIAM J. on Optimization,",
      "citeRegEx" : "Lan and Ghadimi.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lan and Ghadimi.",
      "year" : 2011
    }, {
      "title" : "An optimal method for stochastic composite optimization",
      "author" : [ "Guanghui Lan" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Lan.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lan.",
      "year" : 2010
    }, {
      "title" : "Splitting algorithms for the sum of two nonlinear operators",
      "author" : [ "P.L. Lions", "B. Mercier" ],
      "venue" : "SIAM J. on Numerical Analysis,",
      "citeRegEx" : "Lions and Mercier.,? \\Q1979\\E",
      "shortCiteRegEx" : "Lions and Mercier.",
      "year" : 1979
    }, {
      "title" : "Problem Complexity and Method Efficiency in Optimization",
      "author" : [ "A. Nemirovski", "D. Yudin" ],
      "venue" : null,
      "citeRegEx" : "Nemirovski and Yudin.,? \\Q1983\\E",
      "shortCiteRegEx" : "Nemirovski and Yudin.",
      "year" : 1983
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM J. on Optimization,",
      "citeRegEx" : "Nemirovski et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nemirovski et al\\.",
      "year" : 2009
    }, {
      "title" : "Introductory Lectures on Convex Optimization, A Basic Course",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "Nesterov.,? \\Q2004\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning).",
      "startOffset" : 126,
      "endOffset" : 154
    }, {
      "referenceID" : 13,
      "context" : "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning). Smoothness and strong-convexity are typically certificates of the existence of fast global solvers. Nesterov’s deterministic smoothing method Nesterov (2005b) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods Nesterov (2004) can be applied.",
      "startOffset" : 126,
      "endOffset" : 506
    }, {
      "referenceID" : 13,
      "context" : "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning). Smoothness and strong-convexity are typically certificates of the existence of fast global solvers. Nesterov’s deterministic smoothing method Nesterov (2005b) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods Nesterov (2004) can be applied.",
      "startOffset" : 126,
      "endOffset" : 642
    }, {
      "referenceID" : 13,
      "context" : "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning). Smoothness and strong-convexity are typically certificates of the existence of fast global solvers. Nesterov’s deterministic smoothing method Nesterov (2005b) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods Nesterov (2004) can be applied. It converges as f(xt)−minx f(x) ≤ O(1/t) after t iterations. If a nonsmooth function is strongly convex, this rate can be improved to O(1/t2) using the excessive gap technique Nesterov (2005a). In this paper, we extend Nesterov’s smoothing method to the stochastic setting by proposing a stochastic smoothing method for nonsmooth functions.",
      "startOffset" : 126,
      "endOffset" : 851
    }, {
      "referenceID" : 13,
      "context" : "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning). Smoothness and strong-convexity are typically certificates of the existence of fast global solvers. Nesterov’s deterministic smoothing method Nesterov (2005b) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods Nesterov (2004) can be applied. It converges as f(xt)−minx f(x) ≤ O(1/t) after t iterations. If a nonsmooth function is strongly convex, this rate can be improved to O(1/t2) using the excessive gap technique Nesterov (2005a). In this paper, we extend Nesterov’s smoothing method to the stochastic setting by proposing a stochastic smoothing method for nonsmooth functions. Combining this with a stochastic version of the optimal gradient descent method, we introduce and analyze a new algorithm named Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), for a class of functions that include the popular ML methods of interest. To our knowledge ANSGD is the first stochastic first-order algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions without Polyak’s averaging Polyak and Juditsky (1992). In comparison, the classic SGD converges in O(ln t/t) for 1.",
      "startOffset" : 126,
      "endOffset" : 1464
    }, {
      "referenceID" : 7,
      "context" : "(2007), and is usually not robust Nemirovski et al. (2009). Even with Polyak’s averaging Bach and Moulines (2011); Xu (2011), there are cases where SGD’s convergence rate still can not be faster than O(ln t/t) Shamir (2011).",
      "startOffset" : 34,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "(2007), and is usually not robust Nemirovski et al. (2009). Even with Polyak’s averaging Bach and Moulines (2011); Xu (2011), there are cases where SGD’s convergence rate still can not be faster than O(ln t/t) Shamir (2011).",
      "startOffset" : 34,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : "(2007), and is usually not robust Nemirovski et al. (2009). Even with Polyak’s averaging Bach and Moulines (2011); Xu (2011), there are cases where SGD’s convergence rate still can not be faster than O(ln t/t) Shamir (2011).",
      "startOffset" : 34,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "(2007), and is usually not robust Nemirovski et al. (2009). Even with Polyak’s averaging Bach and Moulines (2011); Xu (2011), there are cases where SGD’s convergence rate still can not be faster than O(ln t/t) Shamir (2011). Numerical experiments on real-world datasets also indicate that ANSGD converges much faster in comparing with these state-of-the-art algorithms.",
      "startOffset" : 34,
      "endOffset" : 224
    }, {
      "referenceID" : 3,
      "context" : "A perturbation-based smoothing method is recently proposed for stochastic nonsmooth minimization Duchi et al. (2011). This work achieves similar iteration complexities as ours, in a parallel computation scenario.",
      "startOffset" : 97,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "A perturbation-based smoothing method is recently proposed for stochastic nonsmooth minimization Duchi et al. (2011). This work achieves similar iteration complexities as ours, in a parallel computation scenario. In serial settings, ANSGD enjoys better and optimal bounds. In machine learning, many problems can be cast as minimizing a composition of a loss function and a regularization term. Before proceeding to the algorithm, we first describe a different setting of “composite minimizations” that we will pursue in this paper, along with our notations and assumptions. 1.1 A Different “Composite Setting” In the classic black-box setting of first-order stochastic algorithms Nemirovski et al. (2009), the structure of the objective function minx{f(x) = Eξf(x, ξ) : ξ ∼ P} is unknown.",
      "startOffset" : 97,
      "endOffset" : 705
    }, {
      "referenceID" : 3,
      "context" : "A perturbation-based smoothing method is recently proposed for stochastic nonsmooth minimization Duchi et al. (2011). This work achieves similar iteration complexities as ours, in a parallel computation scenario. In serial settings, ANSGD enjoys better and optimal bounds. In machine learning, many problems can be cast as minimizing a composition of a loss function and a regularization term. Before proceeding to the algorithm, we first describe a different setting of “composite minimizations” that we will pursue in this paper, along with our notations and assumptions. 1.1 A Different “Composite Setting” In the classic black-box setting of first-order stochastic algorithms Nemirovski et al. (2009), the structure of the objective function minx{f(x) = Eξf(x, ξ) : ξ ∼ P} is unknown. In each iteration t, an algorithm can only access the first-order stochastic oracle and obtain a subgradient f ′(x, ξt). The basic assumption is that f ′(x) = Eξf ′(x, ξ) for any x, where the random vector ξ is from a fixed distribution P . The composite setting (also known as splitting Lions and Mercier (1979)) is an extension of the black-box model.",
      "startOffset" : 97,
      "endOffset" : 1102
    }, {
      "referenceID" : 2,
      "context" : "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a).",
      "startOffset" : 118,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a).",
      "startOffset" : 118,
      "endOffset" : 169
    }, {
      "referenceID" : 2,
      "context" : "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al.",
      "startOffset" : 118,
      "endOffset" : 187
    }, {
      "referenceID" : 2,
      "context" : "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al.",
      "startOffset" : 118,
      "endOffset" : 252
    }, {
      "referenceID" : 2,
      "context" : "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al.",
      "startOffset" : 118,
      "endOffset" : 276
    }, {
      "referenceID" : 2,
      "context" : "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al.",
      "startOffset" : 118,
      "endOffset" : 301
    }, {
      "referenceID" : 2,
      "context" : "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al. (2009); Xiao (2010).",
      "startOffset" : 118,
      "endOffset" : 319
    }, {
      "referenceID" : 2,
      "context" : "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al. (2009); Xiao (2010). A stochastic composite function Φ(x) := f(x) + g(x) is the sum of a smooth stochastic convex function f(x) = Eξf(x, ξ) and a nonsmooth (but simple and deterministic) function g().",
      "startOffset" : 118,
      "endOffset" : 332
    }, {
      "referenceID" : 2,
      "context" : "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al. (2009); Xiao (2010). A stochastic composite function Φ(x) := f(x) + g(x) is the sum of a smooth stochastic convex function f(x) = Eξf(x, ξ) and a nonsmooth (but simple and deterministic) function g(). To minimize Φ, previous work construct the following model iteratively: 〈∇f(xt, ξt),x − xt〉+ 1 ηt D(x,xt) + g(x), (1) where ∇f(xt, ξt) is a gradient, D(·, ·) is a proximal function (typically a Bregman divergence) and ηt is a stepsize. A successful application of the composite idea typically relies on the assumption that model (1) is easy to minimize. If g() is very simple, e.g. ‖x‖1 or the nuclear norm, it is straightforward to obtain the minimum in analytic forms. However, this assumption does not hold for many other applications in machine learning, where many loss functions (not the regularization term, here the nonsmooth g() becomes the nonsmooth loss function) are nonsmooth, and do not enjoy separability properties Wright et al. (2009). This includes important examples such as hinge loss, absolute loss, and ǫ-insensitive loss.",
      "startOffset" : 118,
      "endOffset" : 1265
    }, {
      "referenceID" : 15,
      "context" : "In Section 2 we propose to form a smooth stochastic approximation of f(), such that the optimal methods Nesterov (2004) can be applied to attain optimal convergence rates.",
      "startOffset" : 104,
      "endOffset" : 120
    }, {
      "referenceID" : 15,
      "context" : "The key property of this approximation is: Lemma 1 Nesterov (2005b)(Theorem 1) Function f̂(x, γ) is convex and continuously differentiable, and its gradient is Lipschitz continuous with constant Lf̂ := ‖A‖2 γζ , where ‖A‖ := max x,u {〈Ax,u〉 : ‖x‖ = 1, ‖u‖ = 1}.",
      "startOffset" : 51,
      "endOffset" : 68
    }, {
      "referenceID" : 15,
      "context" : "This stochastic algorithm is obtained by applying Nesterov’s optimal method to our smooth surrogate function, and thus has a similar form to that of his original deterministic method Nesterov (2004)(p.",
      "startOffset" : 50,
      "endOffset" : 199
    }, {
      "referenceID" : 3,
      "context" : "This bound is better than that of stochastic gradient descent or stochastic dual averaging Dekel et al. (2010) for minimizing L-Lipschitz smooth functions, whose rate is O ( LD2 0 t + D2 0+σ 2 √ t )",
      "startOffset" : 91,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "Boucheron et al. (2005)): t−1 i=1 Φ(x ∗ t )−Φ(xt , ξi+1) = O( √ t).",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "Boucheron et al. (2005)): t−1 i=1 Φ(x ∗ t )−Φ(xt , ξi+1) = O( √ t). Together with summing up the RHS of (14), we can obtain an O( √ t) regret bound. When Φ() is strongly convex, the second term in (19) can be bounded using Shalev-Shwartz et al. (2009): t−1 i=1 Φ(x ∗ t ) − Φ(xt , ξi+1) = O(ln t).",
      "startOffset" : 0,
      "endOffset" : 252
    }, {
      "referenceID" : 0,
      "context" : "Boucheron et al. (2005)): t−1 i=1 Φ(x ∗ t )−Φ(xt , ξi+1) = O( √ t). Together with summing up the RHS of (14), we can obtain an O( √ t) regret bound. When Φ() is strongly convex, the second term in (19) can be bounded using Shalev-Shwartz et al. (2009): t−1 i=1 Φ(x ∗ t ) − Φ(xt , ξi+1) = O(ln t). Together with summing up the RHS of (16), an O(ln t) regret bound is achieved. The O( √ t) and O(ln t) regret bounds are known Using our proposed ANSGD for online learning by B-to-O achieves the same (optimal) regret bounds as state-of-the-art algorithms designated for online learning. However, using O-to-B, one can only retain an O(ln t/t) rate of convergence for stochastic strongly convex optimization. From this perspective, O-to-B is inferior to B-to-O. The sub-optimality of O-to-B is also discussed in Hazan and Kale (2011). 4.",
      "startOffset" : 0,
      "endOffset" : 830
    }, {
      "referenceID" : 6,
      "context" : "2 Absolute Loss Robust Regression Absolute loss is an alternative to the popular squared loss for robust regressions Hastie et al. (2009). Using same notations as Sec.",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 9,
      "context" : "This approximation looks similar to the well-studied Huber loss Huber (1964), though they are different.",
      "startOffset" : 53,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "This approach Lan (2010); Lan and Ghadimi (2011) is interesting to compare because like ANSGD, it is another way of obtaining a stochastic algorithm based on Nesterov’s optimal method, begging the question of whether it has similar behavior.",
      "startOffset" : 14,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : "This approach Lan (2010); Lan and Ghadimi (2011) is interesting to compare because like ANSGD, it is another way of obtaining a stochastic algorithm based on Nesterov’s optimal method, begging the question of whether it has similar behavior.",
      "startOffset" : 26,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "This approach Lan (2010); Lan and Ghadimi (2011) is interesting to compare because like ANSGD, it is another way of obtaining a stochastic algorithm based on Nesterov’s optimal method, begging the question of whether it has similar behavior. Theoretically, according to Prop.8 and 9 in Lan and Ghadimi (2011), the bound for the nonsmooth part is of O(1/ √ t) for μ = 0 and O(1/t) for μ > 0.",
      "startOffset" : 26,
      "endOffset" : 309
    }, {
      "referenceID" : 9,
      "context" : "It appeared in Lan and Ghadimi (2011)(Lemma 2), and is an extension of the “3-point identity” Chen and Teboulle (1993)(Lemma 3.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "It appeared in Lan and Ghadimi (2011)(Lemma 2), and is an extension of the “3-point identity” Chen and Teboulle (1993)(Lemma 3.",
      "startOffset" : 94,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : "It appeared in Lan and Ghadimi (2011)(Lemma 2), and is an extension of the “3-point identity” Chen and Teboulle (1993)(Lemma 3.1). Lemma 10 Lan and Ghadimi (2011) Let l(x) be a convex function.",
      "startOffset" : 94,
      "endOffset" : 163
    } ],
    "year" : 2012,
    "abstractText" : "In this work we consider the stochastic minimization of nonsmooth convex loss functions, a central problem in machine learning. We propose a novel algorithm called Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which exploits the structure of common nonsmooth loss functions to achieve optimal convergence rates for a class of problems including SVMs. It is the first stochastic algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions (with strong convexity). The fast rates are confirmed by empirical comparisons, in which ANSGD significantly outperforms previous subgradient descent algorithms including SGD.",
    "creator" : "LaTeX with hyperref package"
  }
}