{
  "name" : "1503.00600.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Weiran Wang" ],
    "emails" : [ "weiranwang@ttic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 3.\n00 60\n0v 1\n[ cs\n.L G\n] 2\nM ar\n1 The problem\nIn this report, we consider the following optimization problem:\nmin x\n1 2 ‖x− y‖ 2 +\nn ∑\ni=1\ndi |xi| , (1)\ns.t. x⊤1 = 1,\nwhere y = [y1, . . . , yn] ⊤ ∈ Rn, di ≥ 0, i = 1, . . . , n, and 1 is the n-dimensional vector consisting of all 1’s. This is a quadratic program and the objective function is strictly convex (even though it is non-smooth), so there is a unique solution which we denote by x = [x1, . . . , xn]\n⊤ with a slight abuse of notation. Notice if d1 = · · · = dn and the constraint were absent, the problem has a closed form solution known as the soft-shrinkage operator (see, e.g., Beck and Teboulle, 2009), which is widely used for solving ℓ1regularized problem in learning sparse representations. But our problem (1) is more involved due to the constraint that couples all dimensions of x. Nonetheless, we give an efficient algorithm with time complexity O(n logn) for this problem using only the KKT theorem.\nRemark 1.1. Our motivation for (1) also comes from sparse coding. Yu et al. (2009) propose the local coordinate coding (LCC) algorithm for learning sparse representations induced by locality. Given a data sample u ∈ Rn and a set of landmark points {vj} C j=1 where vj ∈ R\nn, j = 1, . . . , C, the LCC algorithm reconstructs u from the landmark points while enforcing the faraway landmark points to contribute less than nearby landmark points (or to have smaller reconstruction coefficients). Let the reconstruction coefficient of vj be wj , j = 1, . . . , C. Then the optimization problem for these coefficients in LCC is\nmin w\n∥ ∥ ∥ ∥ ∥ ∥ u− C ∑\nj=1\nwjvj\n∥ ∥ ∥ ∥ ∥ ∥ 2 + λ C ∑\nj=1\n‖u− vj‖ 2 |wj | (2)\ns.t.\nC ∑\nj=1\nwj = 1,\nwhere λ > 0 is some trade-off parameter. The constraint in (2) ensures that the representation is translation invariant. There are different ways of solving this problem, e.g., Elhamifar and Vidal (2011) have a similar optimization problem which they solve with Alternating Direction Method of Multipliers (Boyd et al., 2011).\nOne simple way of solving (2) is to use the gradient proximal algorithm and its Nesterov’s acceleration scheme (see Beck and Teboulle, 2009 and the reference therein), where one iteratively takes a short gradient step for the smooth quadratic term and projects the new estimate with the weighted ℓ1 regularization term subject to the sum constraint, where the projection operator solves exactly (1).\n2 The solution\nWe solve the problem (1) using only the KKT theorem (Nocedal and Wright, 2006), which states the necessary and sufficient condition1 satisfied by the solution x. The Lagrangian of (1) is\nL(x, α) = 1\n2 ‖x− y‖\n2 +\nn ∑\ni=1\ndi |xi|+ α(x ⊤1− 1), (3)\nwhere α is the Lagrange multipliers associated with the constraint. And the KKT system of this problem is\nxi − yi + di + α = 0, if xi > 0, (4a)\nxi − yi − di + α = 0, if xi < 0, (4b)\n−di ≤ −yi + α ≤ di, if xi = 0, (4c) n ∑\ni=1\nxi = 1, (4d)\nwhere we have used the fact that the sub-differential of |x| is [−1, 1] at x = 0 to obtain (4c). Denote y−i = yi−di, y +\ni = yi+di, i = 1, . . . , n, which can be computed beforehand. We can then rewrite (4) in terms of α:\nα < y−i ⇐⇒ xi > 0, (5a) α > y+i ⇐⇒ xi < 0, (5b)\ny−i ≤ α ≤ y +\ni ⇐⇒ xi = 0, (5c) ∑\ni: xi>0\n(y−i − α) + ∑\ni: xi<0\n(y+i − α) = 1. (5d)\nObviously, the Lagrange multiplier α is the key to our problem. Once the value of α is determined, we can easily obtain the optimal solution by setting\nxi = y − i − α if y − i > α, (6a) xi = y + i − α if y + i < α, (6b) xi = 0 otherwise. (6c)\nWe can sort all dimensions of y−i and y + i together (a total of 2N scalars) into an ascending z-sequence:\nz1 ≤ z2 ≤ · · · ≤ z2N . (7)\nAn important observation is that the z-sequence partitions the real axis into 4N + 1 disjoint sets, each being either a single point set {zj}, j = 1, . . . , 2N or an open interval of the form (−∞, z1), (zj , zj+1), j = 1, . . . , 2N − 1, or (z2N ,∞) and the Lagrange multiplier α for the solution must lie in one of them.\nWe then test each of the 4N + 1 sets as follows. Assuming that α lies in one set, we can use (5a)–(5c) to conjecture the positive, negative, and zero dimensions of a possible solution x̂. After that, we use (5d) to compute a hypothesized value α̂ for the Lagrange multiplier, i.e.,\nα =\n∑\ni: x̂i>0\ny−i + ∑\ni: x̂i<0\ny+i − 1\n∑\ni: x̂i>0\n1 + ∑\ni: x̂i<0\n1 . (8)\n1Strictly speaking, our objective is convex and non-smooth, so the condition is that the zero vector 0 lies in the sub-differential\nat the solution x.\nIf the computed α̂ indeed lies in the assumed set (a point or an open interval), we have a KKT point and thus the solution.\nSince the problem (1) is strictly convex and there exists a unique global optimum, this procedure will find the exact solution with no more than 4N+1 tests. We can do this efficiently by sorting y−i and y +\ni separately (O(n log n) operations) and gradually merging the two sorted sequences (an O(n) operation). Therefore the total cost of our procedure for solving (1) is of order O(n log n).\nAlgorithm 1 gives the detailed pseudocode for solving (1), whose MATLAB and C++ implementation can be downloaded at https://eng.ucmerced.edu/people/wwang5.\nReferences\nA. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Imaging Sciences, 2(1):183–202, 2009.\nS. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1): 1–122, 2011.\nE. Elhamifar and R. Vidal. Sparse manifold clustering and embedding. In J. Shawe-Taylor, R. S. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems (NIPS), volume 24, pages 55–63. MIT Press, Cambridge, MA, 2011.\nJ. Nocedal and S. J. Wright. Numerical Optimization. Springer Series in Operations Research and Financial Engineering. Springer-Verlag, New York, second edition, 2006.\nK. Yu, T. Zhang, and Y. Gong. Nonlinear learning using local coordinate coding. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems (NIPS), volume 22. MIT Press, Cambridge, MA, 2009.\nAlgorithm 1 Pseudo-code of our projection operator for (1). Input: y ∈ Rn and d = [d1, . . . , dn] where di ≥ 0, i = 1, . . . , n. Sort y − d into y−: y−1 ≤ y − 2 ≤ · · · ≤ y − n . And sort y + d into y +: y+1 ≤ y + 2 ≤ · · · ≤ y + n .\ni← 1, j ← 1 % i/j index of the dimension of y−/y+ that will be merged next. % s1/s2 stores the sum of dimensions of y\n−/y+ that are strictly greater/smaller than the current estimate of α. s1 ← ∑n i=1\ny−i , s2 ← 0, t← n % t is the number of nonzero dimensions of the hypothesized x. if (s1 + s2) < t · y−1 then\nα← (s1 + s2)/t; return % α < y−1 , all dimensions of x are positive. end if while true do\n% Test a single point set. if y−i < y + j then\nk ← i % y−i is the next value in the z-sequence. while (y−k = y −\ni ) && (k ≤ n) do s1 ← s1 − y −\nk , t← t− 1, k ← k + 1 % Skip the contiguous block of identical dimensions in y −.\nend while if (s1 + s2 − 1) = t · y −\ni then\nα← y−i ; return % α happens to lie in a single point set. else\nleft← y−i , i← k % Otherwise, α lies in a open interval with left boundary left. end if\nelse\nif y−i > y + j then\n% y+j is the next value in the z-sequence. if (s1 + s2 − 1) = t · y +\nj then\nα← y+j ; return % α happens to lie in a single point set. else\nleft← y+j % Otherwise, α lies in a open interval with left boundary left. while (y+j = left) && (j ≤ n) do\ns2 ← s2 + y + j , t← t+ 1, j ← j + 1 % Skip the contiguous block of identical entries in y +.\nend while\nend if\nelse\nk ← i % y−i = y + j is the next value in the z-sequence. while (y−k = y −\ni ) && (k ≤ n) do s1 ← s1 − y −\nk , t← t− 1, k ← k + 1 end while if (s1 + s2 − 1) = t · y −\ni then\nα← y−i ; return else\nleft← y−i , i← k while (y+j = left) && (j ≤ n) do\ns2 ← s2 + y +\nj , t← t+ 1, j ← j + 1 end while\nend if\nend if\nend if % Find the right boundary of the open interval and test if it contains α. if y−i < y + j then\nright← y−i else\nright← y+j end if if t · left < (s1 + s2 − 1) && t · right > (s1 + s2 − 1) then α← (s1 + s2 − 1)/t; return % α lies in the open interval (left, right). end if\nend while\nOutput: α is the Lagrange multiplier of the problem (1), use (6) to obtain x."
    } ],
    "references" : [ {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM J. Imaging Sciences,",
      "citeRegEx" : "Beck and Teboulle.,? \\Q2009\\E",
      "shortCiteRegEx" : "Beck and Teboulle.",
      "year" : 2009
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Boyd et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Boyd et al\\.",
      "year" : 2011
    }, {
      "title" : "Sparse manifold clustering and embedding",
      "author" : [ "E. Elhamifar", "R. Vidal" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Elhamifar and Vidal.,? \\Q2011\\E",
      "shortCiteRegEx" : "Elhamifar and Vidal.",
      "year" : 2011
    }, {
      "title" : "Numerical Optimization. Springer Series in Operations Research and Financial Engineering",
      "author" : [ "J. Nocedal", "S.J. Wright" ],
      "venue" : null,
      "citeRegEx" : "Nocedal and Wright.,? \\Q2006\\E",
      "shortCiteRegEx" : "Nocedal and Wright.",
      "year" : 2006
    }, {
      "title" : "Nonlinear learning using local coordinate coding",
      "author" : [ "K. Yu", "T. Zhang", "Y. Gong" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Yu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", Beck and Teboulle, 2009), which is widely used for solving l1regularized problem in learning sparse representations. But our problem (1) is more involved due to the constraint that couples all dimensions of x. Nonetheless, we give an efficient algorithm with time complexity O(n logn) for this problem using only the KKT theorem. Remark 1.1. Our motivation for (1) also comes from sparse coding. Yu et al. (2009) propose the local coordinate coding (LCC) algorithm for learning sparse representations induced by locality.",
      "startOffset" : 2,
      "endOffset" : 415
    }, {
      "referenceID" : 1,
      "context" : ", Elhamifar and Vidal (2011) have a similar optimization problem which they solve with Alternating Direction Method of Multipliers (Boyd et al., 2011).",
      "startOffset" : 131,
      "endOffset" : 150
    }, {
      "referenceID" : 1,
      "context" : ", Elhamifar and Vidal (2011) have a similar optimization problem which they solve with Alternating Direction Method of Multipliers (Boyd et al.",
      "startOffset" : 2,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : "2 The solution We solve the problem (1) using only the KKT theorem (Nocedal and Wright, 2006), which states the necessary and sufficient condition satisfied by the solution x.",
      "startOffset" : 67,
      "endOffset" : 93
    } ],
    "year" : 2015,
    "abstractText" : "We provide a simple and efficient algorithm for the projection operator for weighted l1-norm regularization subject to a sum constraint, together with an elementary proof. The implementation of the proposed algorithm can be downloaded from the author’s homepage. 1 The problem In this report, we consider the following optimization problem: min x 1 2 ‖x− y‖ 2 + n",
    "creator" : "LaTeX with hyperref package"
  }
}