{
  "name" : "1705.07312.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Lower Bound On the Computational Complexity of Discounted Markov Decision Problems",
    "authors" : [ "Yichen Chen", "Mengdi Wang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "that the complexity lower bound reduces to Ω ( |S||A| ) . These results reveal a surprising observation\nthat the computational complexity of the MDP depends on the data structure of input."
    }, {
      "heading" : "1 Introduction",
      "text" : "The Markov Decision Problem (MDP) arises from stochastic control processes in which a planner aims to make a sequence of decisions as the state of the process evolves. It provides a basic mathematical framework for dynamic programming and reinforcement learning. It finds wide applications in engineering systems, operations research, artificial intelligence and computer games.\nConsider the infinite-horizon discounted-reward MDP with finitely many states and finitely many actions. An instance of such MDP can be described by a tuple M = (S,A, P, r, γ) where S is a finite state space, A is a finite action space , γ ∈ (0, 1) is the discount factor , r : S × A 7→ [0, 1] is the reward function and P = (Pa)a∈A is the collection of matrices of transition probabilities. The size of a MDP model is O(|S|2|A|).\nThe MDP tuple M specifies a controlled random walk on the state space S. When action a ∈ A is selected at state i ∈ S, the system transitions to a state j ∈ S with probability Pa(i, j) and receives the reward ria. A (stationary) deterministic policy π : S 7→ A is a mapping from states to actions. A randomized policy can be viewed as a collection of probability distributions for choosing actions at each state. The value vector vπ ∈ R|S| under a policy π is defined as\nvπ(i) = Eπ\n[ ∞∑\nk=0\nγkrikπ(ik) | i0 = i ] , (1)\nwhere (i0, i1, . . . ) is the path of state transitions generated by the Markov chain under policy π and the expectation is taken over the path. It is known [Bel57] that there exists an optimal deterministic policy that maximizes the value vector for all states simultaneously, yielding the optimal value vector v∗ = maxπ vπ. Our objective is to find a (stationary and possibly randomized) policy that performs nearly well as the optimal policy. We say that a policy π is -optimal if ‖v∗ − vπ‖∞ ≤ .\nIn this paper, we study the computational complexity of finding an -optimal policy of the MDP. We focus on the running-time complexity, which is the sum of the number of arithmetic operations (including\n∗Yichen Chen is with Department of Computer Science, Princeton University, Princeton 08544, USA. †Mengdi Wang is with Department of Operations Research and Financial Engineering, Princeton University, Princeton\n08544, USA.\nar X\niv :1\n70 5.\n07 31\n2v 1\n[ cs\n.C C\n] 2\n0 M\nay 2\naddition, subtraction, multiplication and division) and the number of queries to input data (each query asks for one specified entry of the input). Although the MDP is a classical problem that has been studied for years, there has been few results on its computational complexity lower bound. We have not been able to find comparable existing work. To our best knowledge, this is the first work on lower bounds of the computational complexity for finite-state finite-action MDP.\nOur main results are summarized as follows:\n1. We show that for the standard discounted MDP, the running time needed to compute an -optimal policy with probability at least 9/10 is\nΩ(|S|2|A|). as long as < γ4(1−γ) . This result asserts that the running-time complexity of MDP is at least linear with respect to the input size O(|S|2|A|).\n2. We also consider two variants of the discounted MDP where the input is given in specific data structures: arrays of cumulative sums or binary trees. In these two cases, we show that the running time needed to compute an -optimal policy with probability at least 9/10 is\nΩ\n( γ\n1− γ · |S||A|\n) .\nThis result suggests that the complexity of MDP reduces when the input data is given in convenient data structures. It leaves open the possibility for obtaining approximate solutions in sublinear running time.\nSimilar phenomenon has been observed in a recent paper that studies the running-time complexity upper bounds of discounted MDP [Wan17]. In [Wan17], an upperbound Õ ( |S|3|A| ) was established for the standard MDP using a randomized primal-dual algorithm. The upper bound was shown to reduce to Õ ( |S||A| 2 ) under\nsome additional assumption on the ergodicity when the input is given in convenient data structures (i.e., sorted arrays, cumulative probabilities, and binary trees). The main advantage of these data structures is that they enable immediate simulation of the Markov process in the algorithm, requiring zero preprocessing time.\nLet us compare our lower bound results with the upper bounds mentioned above. Although there remains a gap between the upper and lower bounds, there seems to be a sharp contrast between the case with standard input and the case where the input data is given in convenient data structures. This contrast leads us to a counter intuitive conjecture:\nThe computational complexity for solving the MDP depends on the data structure of the input.\nMore importantly, the nice data structures that lead to a lower computational complexity happen to be those that enable immediate sampling of the Markov state transitions. This observation draws an intriguing connection between the running-time complexity for solving the MDP and the sample complexity for estimating the optimal policy. Yet this connection is not fully understood. We believe that this work on complexity lower bounds for the MDP will provide critical insights into the complexity of sequential decision-making problems and reinforcement learning. We hope these results will motivate sharper analysis and better algorithms."
    }, {
      "heading" : "2 Related Works",
      "text" : "Complexity analysis for the MDP has a long history started by Bellman [Bel57]. The MDP was known to be solvable in polynomial time by dynamic programming [PT87]. Yet the complexity’s dependence on the size of the state space and the action space is not clear.\nThere are three major approaches for solving the MDP: the value iteration method, the policy iteration method, and the linear programming method. Most of the existing works focus on establishing the upper bound of the computational complexity for these methods; see e.g., [How60, Tse90, LDK95, MS99, Ye05,\nYe11, Sch13, FH14]. The best known complexity results for these methods are superlinear with respect to the input size O(|S|2|A|). Recently, [Wan17] developed a randomized primal-dual method that achieves a sublinear running time O ( |S||A| 2 ) under additional assumptions on the ergodicity and the format of the\nMDP input. In contrast to the large volumes of upper bound results, there are fewer results on the complexity lower bound. Papadimitriou and Tsitsiklis [PT87] showed that the MDP is complete for P and its partial information variant is PSPACE-complete. Chow and Tsitsiklis [CT89] established a complexity lower bound of Ω( 1 2n+m ) for continuous-state Markov decision process under some smoothness conditions, where n, m are the dimensions of the continuous state and action spaces respectively. Blondel and Tsitsiklis [BT00] surveyed basic results on the complexity of the MDP and more general stochastic control problems. Friedman, Hansen and Zwich [FHZ11] established subexponential lower bound for the simplex method applied to MDP using randomized pivoting rules. Hansen and Zwich [HZ10] showed that Howard’s policy iteration algorithms require at least Ω(|S|2) iterations for the deterministic MDP with average cost. Yet most of these results apply to specific algorithms. We are interested in developing a lower bound that works for arbitrary algorithms. To our best knowledge, such a result is missing in the literature.\nDespite the lack of existing results on the computational complexity, a related notion called sample complexity has been widely studied in the setting of reinforcement learning; e.g., [EDMM02, MT04, SLW+06, SLL09, AMK12, LH12, DB15]. Sample complexity is the number of state transitions one needs to observe in order to estimate the optimal policy. In existing works on sample complexity, although the settings and assumptions vary from one to another, the sample complexity results are typically Ω( |S||A| 2 ). Apparently the notion of sample complexity is fundamentally different from the notion of running-time complexity. Thus the corresponding results cannot be directly compared.\nOur proof of the lower bound for computational complexity is inspired by works in theoretical computer science. For example, Clarkson, Hazan and Woodruff [CHW12] shows that in order to achieve -optimality for linear classification problem, the algorithm needs to query at least Ω( −2(m + n)) entries of the m by n input matrix. Our analysis involves constructing a tree program, which shares a similar spirit as [BFK+81, BFMadH+87, Yao94, Raz16]. These works study the time-space tradeoff for computation problems such as sorting and parity learning, while our work focuses on the time complexity for the MDP and assumes adequate space. Our analysis is also related to the relational adversary method proposed in [Amb00, Aar06]."
    }, {
      "heading" : "3 Main Theorems",
      "text" : "In this section, we establish the lower bound of computational complexity for any algorithm that takes the specification of an instance of MDP as input and outputs a policy. We will show that the format and data structure of the input is of vital importance. We consider three distinct cases: (1) the standard case where the transition probabilities P are given as arrays; (2) the case where the cumulative probabilities are given instead of transition probabilities; (3) the case where the transition probabilities are given in the format of binary trees. We will show that the first case is more difficult than the latter two cases. The proofs are deferred to Sections 4, 5 and Appendix.\nFirst we consider the standard case and define Standard MDP as follows.\nDefinition 1 (Standard MDP). The input of Standard MDP includes:\n• Transition probability matrices Pa of dimension |S| × |S|, for all a ∈ A. • A reward vector r of dimension |S| × |A| and > 0. This is the standard format of the input that is widely used for solving the MDP [Ber95, BT95, Put14, Ber13]. For Standard MDP, we establish the following lower bound on the running time of any (possibly randomized) algorithm.\nTheorem 1. For ≤ γ4(1−γ) , any randomized algorithm for Standard MDP needs a running time at least Ω(|S|2|A|) to compute an -optimal policy with probability at least 9/10.\nIn Theorem 1, the condition ≤ γ4(1−γ) ≈ 14(1−γ) essentially indicates a constant multiplicative approximation ratio that is close to 1/4, because vπ is typically on the order of 11−γ . It says that, in order to get a reasonable approximate policy, the running time is at least linear with respect to the input size O(|S|2|A|). This lower bound might seems quite intuitive, however, the formal proof requires the use of Yao’s minimax principal and careful analysis of the tree program associated with any algorithm. The result of Theorem 1 excludes the possibility of the existence of any sublinear running-time algorithm for standard MDP.\nSecond we consider the case where transition probabilities of MDP are implicitly given in the format of cumulative probabilities (assuming that the states are ordered in some way).We define CDP MDP as follows.\nDefinition 2 (MDP Specified Using Cumulative Probabilities). The input of CDP MDP includes:\n• Matrices of cumulative probabilities Ca of dimension |S| × |S|, for all a ∈ A, where Ca(i, j) =∑j k=1 Pa(i, k) for all i ∈ S, a ∈ A and j ∈ S.\n• A reward vector r of dimension |S| × |A| and > 0.\nThe following theorems concerns the lower bound for solving CDP MDP .\nTheorem 2. For ≥ 3/|S|, any algorithm for CDP MDP needs at least Ω ( |S||A| ) running time to compute an γ4(1−γ) -optimal policy with probability at least 9/10.\nAccording to Theorem 2, the running time needed to compute an -optimal policy with probability at least 9/10 is Ω ( |S||A|γ (1−γ) ) as long as ≥ 3γ4|S|(1−γ) . Note that this is a weaker lower bound than that of Theorem 1. It does not exclude the existence of sublinear-time approximation algorithms. Third, we consider the case when the transition probabilities are given explicitly but are in the format of binary trees rather than arrays. We define the computation problem as follows.\nDefinition 3 (MDP Specified Using Binary Trees). The input of Binary Tree MDP includes:\n• Transition probability distributions Pa(i, ·) that are encoded in binary trees. There are |S||A| trees, one for each state-action pair (i, a) ∈ S × A. Each binary tree has log |A| layers and |S| leaves that store the values of Pa(i, j), j ∈ S. Each inner node of the tree stores the sum of its two children.\n• A reward vector r of dimension |S| × |A| and > 0.\nWe have the following computational complexity lower bound for solving Binary Tree MDP.\nTheorem 3. Any algorithm for Binary Tree MDP needs at least Ω ( |S||A| ) running time to produce an\nγ 4(1−γ) -optimal policy with probability at least 9/10 for larger than 3/|S|.\nThe results of Theorems 2, 3 are similar. They suggest that the MDP becomes easier to solve when the input is given in specific formats. They provide a hint that one might be able to develop faster algorithms to exploit the structures of the input. One may wonder what is common to CDP MDP and Binary Tree MDP? The answer is quite interesting: both the input of CDP MDP and Binary Tree MDP allow immediate simulation of the Markov decision process without preprocessing [WE80].\nIn contrast, given the input of Standard MDP, one needs O(|S|2|A|) running time to preprocess the arrays of transition probabilities in order to sample state-to-state transitions. This observation is consistent with the results of [Wan17] in which the upper bound reduces if the MDP input is given in a nice format that allows immediate sampling of state transitions. We conjecture that, once the Markov chain can be simulated, the MDP becomes easier to solve. This further implies a close connection between the running-time complexity of the MDP and the sample complexity of the associated reinforcement learning problem. This conjecture awaits further research."
    }, {
      "heading" : "4 Family of Hard Instances of MDP",
      "text" : "In this section, we take a substantial step towards establishing the lower bound of computational complexity for the MDP. Let > 0 be an arbitrary value. We will construct two disjoint sets of MDP instances that are close to each other. For any algorithm, the failure to distinguish one set from the other would result in a computation error at least .\n4.1 Hard Instances of Standard MDP\nLet the state space S consist of four parts, S = SU ∪ SG ∩ SB ∪ {sN}, where |SU| = |SG| = |SB| = |S|−13 and sN is a single state. Let the action space be A = AU ∪ {aN} where |AU | = |A| − 1 and aN is a single action. We construct two sets of MDP instances M1 and M2 that are hard to distinguish, which are given below:\n• LetM1 be the set of instances satisfying the following. If we select any action a ∈ A in state s ∈ SG,SB or sN, the state s transitions to itself with a reward 1, 0 or 1/2. Intuitively, SG, SB and sN are the good, bad and neutral states with high, low and median rewards. Given a ∈ AU and s ∈ SU, the system transitions to some s′ ∈ SB with probability 1 and reward 0. Given aN and s ∈ SU, the state transitions to sN with reward 0. For M1 ∈ M1, we have v∗M1(s) = γ 2(1−γ) for all s ∈ SU. The\ncardinality of M1 is |SB||SU×AU |. • LetM2 be the set of instances that differ from those inM1 at one state-action pair, which we denote\nby (s̄, ā) ∈ SU ×AU . Given (s̄, ā), the system transitions to some good state s̄′ ∈ SG with probability 1 and reward 0. For M2 ∈ M2, we have v∗M2(s̄) = γ 1−γ and v ∗ M2\n(s) = γ2(1−γ) for all s ∈ SU/{s̄}. The cardinality of M2 is |SU ×AU | × |SB||SU×AU |.\nFigure 1 shows a pair of M1 ∈M1 and M2 ∈M2 which differ in two entries. We see that ‖v∗M1 − v∗M2‖∞ = γ 2(1−γ) . Suppose that an algorithm can output a γ\n4(1−γ) -optimal policy with high probability for all instances of the MDP. The algorithm must be able to differentiate M1 from M2 with high probability. Informally speaking, the algorithm needs to search for two discrepancies in an array of size |SU×AU ×S| = Ω(|S|2|A|). Intuitively, it needs to query a significant portion of the entire array to succeed with high probability, which requires a running time of Ω(|S|2|A|).\n4.2 Hard Instances of CDP MDP and Binary Tree MDP\nLet > 3/|S| be arbitrary such that 1/ is an integer and let k = (|S|−1)/3. We arrange the good states and the bad states in an alternating order, i.e., sB,1, sG,1, . . . , sB,k, sG,k. Let us construct two sets of instances\nM3 and M4 that are hard to distinguish: • Let M3 consist of a single instance M3 satisfying the follows. If we select action a ∈ AU in state s ∈ SU, the system transitions to sB,1, . . . , sB,1/ each with probability and reward 0. From any s ∈ SG,SB, sN, the system transitions to itself with a reward of 1, 0, /2 respectively, regardless of the action. If we select action aN at state s ∈ SU, it transitions to sN with probability 1 and reward 0. We have v∗M3(s) = γ/(2− 2γ) for s ∈ SU.\n• We let M4 be the set of instances that differ from M3 by one entry. Let there be some state-action pair (s̄, ā) ∈ SU×AU that transitions to one of the first 1/ good state with probability and a reward of 0. For simplicity, we assume that 1/ is an integer. Let all other transition probabilities be identical to M3. There are |SU × AU | × 1/ such instances. For M4 ∈ M4, we have v∗M4(s̄) = γ/(1 − γ) and v∗M4(s) = γ/(2− 2γ) for all s ∈ SU/{s̄}.\nWe see that ‖v∗M3−v∗M4‖∞ = γ 2(1−γ) . For any algorithm that outputs an γ/(4−4γ)-optimal policy, it should be able to differentiate M4 from M3. In the case of CDP MDP , differentiating M4 from M3 requires one to search for one single discrepancy in an array of size |SU ×AU × 1 | = Ω(|S||A|/ ); see Figure 2. In the case of BinaryTreeMDP, differentiating M4 from M3 requires one to search for two discrepancies from Ω(|S||A|/ ) leaves; see Figure 3."
    }, {
      "heading" : "5 Proof of Main Theorems",
      "text" : "In this section, we begin by introducing a sub-problem about differentiating matrices and establish its complexity lower bound. Then we develop the proofs of the main theorems by using Yao’s minimax principal."
    }, {
      "heading" : "5.1 A Sub-Problem",
      "text" : "To facilitate our proof, we first introduce a sub-problem and give a lemma that is vital to the proof of Theorem 1.\nDefinition 4 (Matrix Differentiation). Let A and B be two m by n matrices taking values of 0 or 1. For every row of A, there is an entry of 1 and the other entries are 0. B is similar to A except that there is a row with all zeros. We say that (A,B) is a “pair of matrices” if A and B differ by exactly one entry. Note that there are mnm pairs of matrices in total. We focus on deterministic algorithms that take the m × n binary matrix as the input. For any pair (A,B), we say that a deterministic algorithm is able to distinguish A from B if it queries the entry (i, j) where Aij 6= Bij within a given number of steps when either A or B is given as the input.\nLemma 1. For any deterministic algorithm that applies to Matrix Differentiation, it needs Ω(mn) queries to distinguish 3mnm/5 pairs of matrices.\nIn order to prove Lemma 1, we model any deterministic algorithm for the matrix differentiation problem as a tree program (which was used in [BFK+81] to study the complexity of sorting). As the algorithm makes a new query to the input matrix, the state of the algorithm branches according to the response until the maximal number of queries is reached. The proof is given in the appendix."
    }, {
      "heading" : "5.2 Proof of Theorem 1",
      "text" : "Let us summarize the main arguments of the proof: Suppose that some randomized algorithm can find an -optimal policy to the MDP with high probability within a time limit. Then by using Yao’s minimax principal, we obtain that there exists a deterministic algorithm that is able to successfully distinguish a large number of hard instances inM1 from those inM2. It follows that one can construct an algorithm for Matrix Differentiation and apply Lemma 1 to establish the lower bound. The proof has a similar spirit as that of the relational adversary method [Amb00, Aar06]. Proof of Theorem 1. Given the state space S, the action space A and the discount factor γ, we denote M0 to be the set of the MDP instances where the entries of P and r are multiples of 0.01. Note that M1 and M2 are two subsets of M0. Let ΠT be the set of all the deterministic algorithms that takes M0 ∈ M0 as input and run at most T steps (each step is an query of an entry). Let µ be a distribution on ΠT and let D be a distribution on M0. Recall that a randomized algorithm running at most T steps is a distribution on ΠT . Theorem 1 can be equivalently stated as: If\nmax µ∈P(ΠT ) min M0∈M0 Pπ∼µ(M0) ( max s∈S ∣∣vπM0(s)− v∗M0(s) ∣∣ ≤ ) ≥ 9/10, (2)\nthen T = Ω(|S|2|A|) where P(ΠT ) is the set of all probability measures on ΠT and π is the policy computed by the randomized algorithm µ on input M0. If (2) holds, we apply Yao’s minimax principle [Yao77] and get\nmin D∈P(M0) max µ∈ΠT PM0∼D ( max s∈S ∣∣vπM0(s)− v∗M0(s) ∣∣ ≤ ) ≥ 9/10, (3)\nwhere π is the policy computed by µ on input M0. The advantage of using Yao’s minimax principle is that we have converted the complexity of randomized algorithms into the complexity of deterministic algorithms on randomized input.\nNow let D1 and D2 be the uniform distributions over M1 and M2 respectively. Let D be an equal mixture of D1 and D2. Assume that there exists a deterministic algorithm µ which outputs an -optimal policy with probability 9/10 when the MDP instance is drawn from D. As a result, µ must succeed with probability at least 4/5 if the instance is drawn from either D1 or D2 alone. Let C1 ⊂M1 and C2 ⊂M2 be the sets of instances on which µ succeeds. Let m = |SU ×AU | and n = |SB|. By the discussion in Section 4, we have\n|C1| ≥ 4\n5 |M1| =\n4 5 nm, |C2| ≥ 4 5 |M2| = 4 5 mnm.\nFor instances M1 ∈ M1 and M2 ∈ M2, we say that (M1,M2) is a pair of MDP instances if the following two conditions are satisfied: (i) The transition probabilities of M1 and M2 differ at only two entries, (s̄, ā, s\n′) and (s̄, ā, s̄′). (ii) s′ and s̄′ have the same index within SB and SG (e.g., s′ = sB,1 and s̄′ = sG,1).\nConsider the graph where nodes are MDP instances and there is an edge between every “pair”. We can see that each M1 ∈M1 has degree m and each M2 ∈M2 has degree 1. Let e(·, ·) be the indicator function where e(M1,M2) = 1 if (M1,M2) is a pair (so there is an edge) and 0 otherwise. The total number of pairs between C1 and C2 is\n∑\nM1∈C1,M2∈C2 e(M1,M2)\n≥ ∑\nM1∈C1,M2∈M2 e(M1,M2)−\n∑\nM1∈M1,M2∈M2\\C2 e(M1,M2)\n≥4 5 nm ×m− 1 5 mnm = 3 5 mnm.\nIt means that the deterministic algorithm µ must be able to distinguish at least 3mnm/5 pairs of MDP instances.\nIn what follows, we use µ to construct an algorithm that applies to Matrix Differentiation. We first construct an auxiliary algorithm µ′ that mimics the work flow of µ with a slight difference. Suppose that µ queries an entry (s, a, s′). If s′ is a bad states, µ′ will query (s, a, s′) and branch in the same way as if µ queries the entry (s, a, s′) and branches. If s′ is a good state, µ′ will also query (s, a, s′), but regardless of the value of (s, a, s′), µ′ will branch in the way that µ branches after reading a value 0 at (s, a, s′). The algorithm µ′ outputs all the actual values of queries it read along the computation path.\nWe claim that µ′ is able to differentiate all the pairs that µ differentiate. Let (M1,M2) be a pair of MDP instances which can be differentiated by µ. Suppose that we run both µ and µ′ on M1 (or M2). Let (s̄, ā, s̄′) be the entry on which M1 and M2 differ - this entry is eventually queried by µ. Let (s, a, s\n′) be any previous query made by µ. If s′ is a good state, then the value of (s, a, s′) for both M1 and M2 must be 0 (according to the construction of M1,M2 in Section 4.1). Therefore µ\n′ will always mimic the moves of µ until it queries the right entry to differentiate M1 from M2. As a result, µ\n′ can distinguish all the pairs that µ distinguishes in T steps. Now let us we construct an algorithm based on µ′ that applies to Matrix Differentiation, which is given in Algorithm 1.\nAlgorithm 1 The algorithm constructed from µ′ that applies to Matrix Differentiation\n1: Input: An m× n matrix A. 2: Set |SU|, |SB|, |SG| and |AU | such that m = |SU ×AU | and n = |SG| = |SB|. 3: while µ′ hasn’t terminated do 4: Let (s, a, s′) be the entry to be queried by µ′ 5: Let i be the index of (s, a) in SU ×AU 6: if s′ ∈ SB then 7: Query Aij where j is the index of s\n′ in SB 8: else if s′ ∈ SG then 9: Query Aij where j is the index of s\n′ in SG 10: end if 11: Return Aij to answer the query of µ ′ 12: end while\nWe claim that Algorithm 1 can distinguish at least 3mnm/5 pairs of matrices for Matrix Differentiation. Given a pair (M1,M2) ∈ M1 ×M2, we denote the subarray of transition probabilities corresponding to (SU × A) × SB as the matrix A and the matrix B. Note that A differs from B at a single entry, so (A,B) is a pair of Matrix Differentiation. Due to the definition for pairs of MDP instances (condition (ii)), we can verify that there is a one-to-one correspondence between the set of pairs of matrices for Matrix\nDifferentiation and the set of pairs of MDP instances. For each pair (M1,M2) that µ ′ distinguishes, µ′ queries an entry (s, a, s′) on which M1 and M2 differ. Because of condition(ii), Algorithm 1 will query the corresponding entry (i, j) on which A and B differ. Since µ′ differentiate 3mnm/5 pairs of MDP instances in running time T , Algorithm 1 must be able to differentiate 3mnm/5 pairs of matrices for Matrix Differentiation using T queries. Finally, we apply Lemma 1 and obtain T = Ω(|S|2|A|)."
    }, {
      "heading" : "5.3 Proofs of Theorem 2 and 3",
      "text" : "The proofs of Theorems 2 and 3 are significantly simpler than that of Theorem 1. This is because M3 and M4 differs in one single entry. The following proofs do not rely on Lemma 1. Proof of Theorem 2. Using similar notations as in Section 5.2, we restate Theorem 2 as: If\nmax µ∈D(ΠT ) min M0∈M0 Pπ∼µ(M0) ( max s∈S ∣∣vπM0(s)− v∗M0(s) ∣∣ ≤ ) ≥ 9/10,\nthen T = Ω(|S||A|/ ), where ΠT is the set of all the deterministic algorithms for CDP MDP . We apply the minimax principal similarly as in Section 5.2.\nLet D3,D4 be the uniform distributions overM3,M4 respectively. Let D be an equal mixture of D3 and D4. We say that M3 ∈ M3 and M4 ∈ M4 is a pair if M3 and M4 differs by exactly one entry. Note that |M3| = 1 and there are |S||A|/ pairs in total.\nSuppose that there is a deterministic algorithm µ that finds an γ4−4γ -optimal policy with probability at least 9/10, it must succeed (finds the optimal action) on M3 and on 4/5 of all instances in M4. It means that the algorithm µ must be able to distinguish at least 4/5×|S||A|/ pairs. When µ queries a new entry, it distinguishes one more pair of (M3,M4) that happen to only differ at this entry. The goal is to to distinguish at least 4/5 · |S||A|/ pairs. Therefore, µ makes at least Ω(|S||A|/ ) queries and its running time is lower bounded by Ω(|S||A|/ ). Proof of Theorem 3. The proof of Theorem 3 is very similar to the proof of Theorem 2. The intuition is that for any pair (M3,M4), the binary tree representations of M3 and M4 are exactly the same except for two leaves. In order to find these two leaves or to be certain that such leaves do not exist, we need to query at least Ω(|S||A|/ ) leaves. We omit the details for brevity."
    }, {
      "heading" : "A Proof of Lemma 1",
      "text" : "Computational Model: We model the deterministic algorithm solving Matrix Differentiation by a tree program. A tree program of length n is a directed tree with n + 1 layers, where each layer represents a time step. Each node of the tree represents a state of the algorithm that leads to a query of one entry of the input, whose edges correspond to the possible responses to the query. Given an input, the query at the root is first tested, and the state of the algorithm moves following the edge given by the response to the query. This leads to the second query, the position of which is decided by the deterministic algroithm and the response of the first query. The query continues until it reaches the leaf of the tree.\nFor Matrix Differentiation, the entries of the inputs only take value of 0 or 1. Thus, every node has exactly two outgoing edges to the next layer. We assume that the edge to the left child corresponds to the response of 0 and the edge to the right child corresponds to the response of 1. Each inner node in the tree is associated with a tuple (i, j), which is the entry to be queried at that node.\nBefore giving the proof, we give a few more definitions. For any input, the corresponding computation path is the sequence of nodes visited by the algorithm until termination. We define the i-th slice of the tree program to be the set of all the nodes to which the path has i − 1 edges labelled by 1. Recall that input matrices A and B are a pair if and only if A and B differs by exactly one entry. We say that a node distinguishes the pair (A,B) if the node is on the computation paths of both A and B and Aij 6= Bij where (i, j) is the entry to be queried at that node.\nWe are ready to introduce the auxillary lemmas that establish the properties of the tree programs for any deterministic algorithm solving the Matrix Differentiation. Without loss of generality, we assume that the algorithm won’t query the same entry two times in a single computation path. We have the following lemmas.\nLemma 2. Any single node at the k-th slice can distinguish at most nm−k pairs.\nProof. Suppose that a node at the k-th slice distinguishes a pair (A,B). It means that the node is on the computation paths of both A and B, and the responses to the previous queries on the path are the same given either A or B as the input. Observe that the responses of k− 1 of previous queries are 1, which means that the algorithm knows at least k − 1 fixed rows of A and B. Now if Aij 6= Bij where (i, j) is the query of the current node, the i-th row of A and B is also fixed and the positions of unknown ones are only in the remaining m − k rows. The total number of possible configurations of these rows is at most nm−k. As a result, any single node at the k-th slice can distinguish at most nm−k pairs.\nLemma 3. The nodes at the k-th slice can distinguish at most nm pairs in total.\nProof. Let (A,B) be any pair of the input where all the rows of A have a 1. We want to count the number of pairs involving A that is distinguished at the k-th slice. We claim that if the computation path of A doesn’t reach the (k + 1)-th slice, then none of the pairs involving A will be distinguished at the k-th slice. Suppose that the computation path doesn’t reach the k-th slice. It is obvious that none of such pairs will be distinguished at k-th slice. If the computation path reaches the k-th slice and stays on it, then all the responses at the k-th slice are 0. Yet, if the tree program distinguishes a pair (A,B) at the k-th slice, it must query an entry (i, j) where Aij = 1 and Bij = 0. As a result, if there exists a pair involving A that is distinguished at the k-th slice, the computation path of A must reach the (k + 1)-th slice.\nAssume that the computation path of A reaches the (k+ 1)-th slice. We can see that the path will reach (k + 1)-th slice from some node. Denote (i, j) to be the entry queried at that node. Observe that the node can distinguish only one pair (A,B) of all the m pairs involving A as there is only one possible B such that Aij 6= Bij . It means that for every possible A, the tree program distinguish at most one pair involving A at the k-th slice. As there are at most nm configurations of A, the tree program can distinguish at most nm pairs at the k-th slice.\nThe intuition of Lemma 3 is that whenever we observe one more “1” in a query, the algorithm can distinguish at most nm more pairs. Now, we give our third lemma.\nLemma 4. Let L ≥ m be the number of layers of the tree program. Then the total number of nodes at the k-th slice is less than or equal to ( L k ) .\nProof. Each node at the k-th slice is uniquely identified by the path from the root to the node, which can be represented by a sequence of numbers (a1, a2, . . . , ak). Here, ai is the number of nodes on the path which belongs to the i-th slice. The number of possible paths is at most the number of integer solution to the equation ∑k i=1 ai ≤ L. We introduce a slack variable α so that α + ∑k i=1 ai = L + 1 where\nα ≥ 1, a1 ≥ 1, . . . , ak ≥ 1. It is easy to see that the number of solutions is ( L k ) .\nThe above three lemmas establish the properties of the tree program for the Matrix Differentiation. We are now ready to prove the main lemma.\nProof of Lemma 1. Assume that we have a deterministic algorithm that distinguishes at least 3mn5/5 pairs in nk time steps, where k is an integer smaller than m/5. We first construct the tree program of the deterministic algorithm. We divide the tree into two parts, where the first part is the first 5k slices and the second part is the remaining m − 5k slices. By Lemma 3, the tree program can distinguish at most 5knm pairs in the first 5k slices. We then bound the number of pairs which the tree can distinguish in the remaining slices. In the i-th slice, there are at most ( nk i ) nodes by Lemma 4. By Lemma 2, each node can\ndistinguish at most nm−i pairs. As a result, the tree program can distinguish at most ( nk i ) nm−i pairs at i-th slice for i > 5k. The total number of pairs we can distinguish in the remaining slices is then\nm∑\ni=5k+1\n( nk\ni\n) nm−i ≤ m∑\ni=5k\n( nk\ni\n) nm−i ≤ m∑\ni=5k\n(nk)i\ni! nm−i = nm\nm∑\ni=5k\nki i! .\nBy Taylor’s remainder theorem, we have\nm∑\ni=5k\nki\ni! ≤ f\n(5k)(k) (5k)! k5k = ekk5k (5k)! ,\nwhere f(k) = ek. By using Stirling’s formula where (5k)! ≥ √ 2π(5k)5k+1/2e−5k, we have the following\nnm m∑\ni=5k\nki\ni! ≤ n mekk5k√ 2π(5k)5k+1/2e−5k\n≤ nm ( e6/5k\n5k\n)5k ≤ nm.\nAs a result, the total number of pairs we can distinguish is smaller than (5k + 1)nm when the layer of the tree program is nk. In order to distinguish at least 3mn5/5 pairs, the number of layers of the tree program should be Ω(mn)."
    } ],
    "references" : [ {
      "title" : "Lower bounds for local search by quantum arguments",
      "author" : [ "Scott Aaronson" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Aaronson.,? \\Q2006\\E",
      "shortCiteRegEx" : "Aaronson.",
      "year" : 2006
    }, {
      "title" : "Quantum lower bounds by quantum arguments",
      "author" : [ "Andris Ambainis" ],
      "venue" : "In Proceedings of the thirty-second annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Ambainis.,? \\Q2000\\E",
      "shortCiteRegEx" : "Ambainis.",
      "year" : 2000
    }, {
      "title" : "On the sample complexity of reinforcement learning with a generative model",
      "author" : [ "Mohammad Gheshlaghi Azar", "Rémi Munos", "Bert Kappen" ],
      "venue" : "arXiv preprint arXiv:1206.6461,",
      "citeRegEx" : "Azar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Azar et al\\.",
      "year" : 2012
    }, {
      "title" : "Dynamic Programming",
      "author" : [ "Richard Bellman" ],
      "venue" : null,
      "citeRegEx" : "Bellman.,? \\Q1957\\E",
      "shortCiteRegEx" : "Bellman.",
      "year" : 1957
    }, {
      "title" : "Dynamic programming and optimal control, volume 1",
      "author" : [ "Dimitri P Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "Bertsekas.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 1995
    }, {
      "title" : "Abstract dynamic programming",
      "author" : [ "Dimitri P Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "Bertsekas.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 2013
    }, {
      "title" : "A time-space tradeoff for sorting on non-oblivious machines",
      "author" : [ "Allan Borodin", "Michael J Fischer", "David G Kirkpatrick", "Nancy A Lynch", "Martin Tompa" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Borodin et al\\.,? \\Q1981\\E",
      "shortCiteRegEx" : "Borodin et al\\.",
      "year" : 1981
    }, {
      "title" : "A time-space tradeoff for element distinctness",
      "author" : [ "Allan Borodin", "Faith Fich", "Friedhelm Meyer auf der Heide", "Eli Upfal", "Avi Wigderson" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Borodin et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Borodin et al\\.",
      "year" : 1987
    }, {
      "title" : "Neuro-dynamic programming: an overview",
      "author" : [ "Dimitri P Bertsekas", "John N Tsitsiklis" ],
      "venue" : "In Proceedings of the 34th IEEE Conference on Decision and Control,",
      "citeRegEx" : "Bertsekas and Tsitsiklis.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bertsekas and Tsitsiklis.",
      "year" : 1995
    }, {
      "title" : "A survey of computational complexity results in systems and control",
      "author" : [ "Vincent D Blondel", "John N Tsitsiklis" ],
      "venue" : null,
      "citeRegEx" : "Blondel and Tsitsiklis.,? \\Q2000\\E",
      "shortCiteRegEx" : "Blondel and Tsitsiklis.",
      "year" : 2000
    }, {
      "title" : "Sublinear optimization for machine learning",
      "author" : [ "Kenneth L Clarkson", "Elad Hazan", "David P Woodruff" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Clarkson et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Clarkson et al\\.",
      "year" : 2012
    }, {
      "title" : "The complexity of dynamic programming",
      "author" : [ "Chef-Seng Chow", "John N Tsitsiklis" ],
      "venue" : "Journal of complexity,",
      "citeRegEx" : "Chow and Tsitsiklis.,? \\Q1989\\E",
      "shortCiteRegEx" : "Chow and Tsitsiklis.",
      "year" : 1989
    }, {
      "title" : "Sample complexity of episodic fixed-horizon reinforcement learning",
      "author" : [ "Christoph Dann", "Emma Brunskill" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dann and Brunskill.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dann and Brunskill.",
      "year" : 2015
    }, {
      "title" : "Pac bounds for multi-armed bandit and Markov decision processes",
      "author" : [ "Eyal Even-Dar", "Shie Mannor", "Yishay Mansour" ],
      "venue" : "In International Conference on Computational Learning Theory,",
      "citeRegEx" : "Even.Dar et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Even.Dar et al\\.",
      "year" : 2002
    }, {
      "title" : "The value iteration algorithm is not strongly polynomial for discounted dynamic programming",
      "author" : [ "Eugene A Feinberg", "Jefferson Huang" ],
      "venue" : "Operations Research Letters,",
      "citeRegEx" : "Feinberg and Huang.,? \\Q2014\\E",
      "shortCiteRegEx" : "Feinberg and Huang.",
      "year" : 2014
    }, {
      "title" : "Subexponential lower bounds for randomized pivoting rules for the simplex algorithm",
      "author" : [ "Oliver Friedmann", "Thomas Dueholm Hansen", "Uri Zwick" ],
      "venue" : "In Proceedings of the forty-third annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Friedmann et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Friedmann et al\\.",
      "year" : 2011
    }, {
      "title" : "Dynamic programming and Markov processes",
      "author" : [ "Ronald A. Howard" ],
      "venue" : "The MIT press,",
      "citeRegEx" : "Howard.,? \\Q1960\\E",
      "shortCiteRegEx" : "Howard.",
      "year" : 1960
    }, {
      "title" : "Lower bounds for Howards algorithm for finding minimum mean-cost cycles",
      "author" : [ "Thomas Hansen", "Uri Zwick" ],
      "venue" : "Algorithms and Computation,",
      "citeRegEx" : "Hansen and Zwick.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hansen and Zwick.",
      "year" : 2010
    }, {
      "title" : "On the complexity of solving Markov decision problems",
      "author" : [ "Michael L Littman", "Thomas L Dean", "Leslie Pack Kaelbling" ],
      "venue" : "In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "Littman et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Littman et al\\.",
      "year" : 1995
    }, {
      "title" : "PAC bounds for discounted MDPs",
      "author" : [ "Tor Lattimore", "Marcus Hutter" ],
      "venue" : "In International Conference on Algorithmic Learning Theory,",
      "citeRegEx" : "Lattimore and Hutter.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lattimore and Hutter.",
      "year" : 2012
    }, {
      "title" : "On the complexity of policy iteration",
      "author" : [ "Yishay Mansour", "Satinder Singh" ],
      "venue" : "In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "Mansour and Singh.,? \\Q1999\\E",
      "shortCiteRegEx" : "Mansour and Singh.",
      "year" : 1999
    }, {
      "title" : "The sample complexity of exploration in the multi-armed bandit problem",
      "author" : [ "Shie Mannor", "John N Tsitsiklis" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Mannor and Tsitsiklis.,? \\Q2004\\E",
      "shortCiteRegEx" : "Mannor and Tsitsiklis.",
      "year" : 2004
    }, {
      "title" : "The complexity of Markov decision processes",
      "author" : [ "Christos H Papadimitriou", "John N Tsitsiklis" ],
      "venue" : "Mathematics of operations research,",
      "citeRegEx" : "Papadimitriou and Tsitsiklis.,? \\Q1987\\E",
      "shortCiteRegEx" : "Papadimitriou and Tsitsiklis.",
      "year" : 1987
    }, {
      "title" : "Markov decision processes: discrete stochastic dynamic programming",
      "author" : [ "Martin L Puterman" ],
      "venue" : null,
      "citeRegEx" : "Puterman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Puterman.",
      "year" : 2014
    }, {
      "title" : "Fast learning requires good memory: A time-space lower bound for parity learning",
      "author" : [ "Ran Raz" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Raz.,? \\Q2016\\E",
      "shortCiteRegEx" : "Raz.",
      "year" : 2016
    }, {
      "title" : "Improved and generalized upper bounds on the complexity of policy iteration",
      "author" : [ "Bruno Scherrer" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Scherrer.,? \\Q2013\\E",
      "shortCiteRegEx" : "Scherrer.",
      "year" : 2013
    }, {
      "title" : "Reinforcement learning in finite mdps: Pac analysis",
      "author" : [ "Alexander L Strehl", "Lihong Li", "Michael L Littman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Strehl et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Strehl et al\\.",
      "year" : 2009
    }, {
      "title" : "Pac model-free reinforcement learning",
      "author" : [ "Alexander L Strehl", "Lihong Li", "Eric Wiewiora", "John Langford", "Michael L Littman" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning,",
      "citeRegEx" : "Strehl et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Strehl et al\\.",
      "year" : 2006
    }, {
      "title" : "Solving h-horizon, stationary markov decision problems in time proportional to log (h)",
      "author" : [ "Paul Tseng" ],
      "venue" : "Operations Research Letters,",
      "citeRegEx" : "Tseng.,? \\Q1990\\E",
      "shortCiteRegEx" : "Tseng.",
      "year" : 1990
    }, {
      "title" : "Randomized linear programming solves the discounted Markov decision problem in nearly-linear running time",
      "author" : [ "Mengdi Wang" ],
      "venue" : "arXiv preprint arXiv:1704.01869,",
      "citeRegEx" : "Wang.,? \\Q2017\\E",
      "shortCiteRegEx" : "Wang.",
      "year" : 2017
    }, {
      "title" : "An efficient method for weighted sampling without replacement",
      "author" : [ "Chak-Kuen Wong", "Malcolm C. Easton" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Wong and Easton.,? \\Q1980\\E",
      "shortCiteRegEx" : "Wong and Easton.",
      "year" : 1980
    }, {
      "title" : "Probabilistic computations: Toward a unified measure of complexity",
      "author" : [ "Andrew Chi-Chin Yao" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Yao.,? \\Q1977\\E",
      "shortCiteRegEx" : "Yao.",
      "year" : 1977
    }, {
      "title" : "Near-optimal time-space tradeoff for element distinctness",
      "author" : [ "Andrew Chi-Chih Yao" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Yao.,? \\Q1994\\E",
      "shortCiteRegEx" : "Yao.",
      "year" : 1994
    }, {
      "title" : "A new complexity result on solving the Markov decision problem",
      "author" : [ "Yinyu Ye" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Ye.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ye.",
      "year" : 2005
    }, {
      "title" : "The simplex and policy-iteration methods are strongly polynomial for the Markov decision problem with a fixed discount rate",
      "author" : [ "Yinyu Ye" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Ye.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ye.",
      "year" : 2011
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We study the computational complexity of the infinite-horizon discounted-reward Markov Decision Problem (MDP) with a finite state space S and a finite action space A. We show that any randomized algorithm needs a running time at least Ω(|S||A|) to compute an -optimal policy with high probability. We consider two variants of the MDP where the input is given in specific data structures, including arrays of cumulative probabilities and binary trees of transition probabilities. For these cases, we show that the complexity lower bound reduces to Ω ( |S||A| ) . These results reveal a surprising observation that the computational complexity of the MDP depends on the data structure of input.",
    "creator" : "LaTeX with hyperref package"
  }
}