{
  "name" : "1311.2495.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Noisy Power Method: A Meta Algorithm with Applications",
    "authors" : [ "Moritz Hardt", "Eric Price" ],
    "emails" : [ "mhardt@us.ibm.com", "ecprice@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Streaming PCA. A recent work of Mitliagkas et al. (NIPS 2013) gives a space-efficient algorithm for PCA in a streaming model where samples are drawn from a gaussian spiked covariance model. We give a simpler and more general analysis that applies to arbitrary distributions confirming experimental evidence of Mitliagkas et al. Moreover, even in the spiked covariance model our result gives quantitative improvements in a natural parameter regime. It is also notably simpler and follows easily from our general convergence analysis of the noisy power method together with a matrix Chernoff bound.\nPrivate PCA. We provide the first nearly-linear time algorithm for the problem of differentially private principal component analysis that achieves nearly tight worst-case error bounds. Complementing our worst-case bounds, we show that the error dependence of our algorithm on the matrix dimension can be replaced by an essentially tight dependence on the coherence of the matrix. This result resolves the main problem left open by Hardt and Roth (STOC 2013). The coherence is always bounded by the matrix dimension but often substantially smaller thus leading to strong average-case improvements over the optimal worst-case bound."
    }, {
      "heading" : "1 Introduction",
      "text" : "Computing the dominant singular vectors of a matrix is one of the most important algorithmic tasks underlying many applications including low-rank approximation, PCA, spectral clustering, dimensionality reduction, matrix completion and topic modeling. The classical problem is well-understood, but many recent applications in machine learning face the fundamental problem of approximately finding singular vectors in the presence of noise. Noise\n∗IBM Research Almaden. Email: mhardt@us.ibm.com †IBM Research Almaden. Email: ecprice@mit.edu\nar X\niv :1\n31 1.\n24 95\nv4 [\ncs .D\nS] 3\nF eb\ncan enter the computation through a variety of sources including sampling error, missing entries, adversarial corruptions and privacy constraints. It is desirable to have one robust method for handling a variety of cases without the need for ad-hoc analyses. In this paper we consider the noisy power method, a fast general purpose method for computing the dominant singular vectors of a matrix when the target matrix can only be accessed through inaccurate matrix-vector products.\nFigure 1 describes the method when the target matrix A is a symmetric d × d matrix—a generalization to asymmetric matrices is straightforward. The algorithm starts from an initial matrix X0 ∈ Rd×p and iteratively attempts to perform the update rule X` → AX`. However, each such matrix product is followed by a possibly adversarially and adaptively chosen perturbation G` leading to the update rule X`→ AX` +G`. It will be convenient though not necessary to maintain that X` has orthonormal columns which can be achieved through a QR-factorization after each update.\nThe noisy power method is a meta algorithm that when instantiated with different settings of G` and X0 adapts to a variety of applications. In fact, there have been a number of recent surprising applications of the noisy power method:\n1. Jain et al. [JNS13, Har14] observe that the update rule of the well-known alternating least squares heuristic for matrix completion can be considered as an instance of NPM. This lead to the first provable convergence bounds for this important heuristic.\n2. Mitgliakas et al. [MCJ13] observe that NPM applies to a streaming model of principal component analysis (PCA) where it leads to a space-efficient and practical algorithm for PCA in settings where the covariance matrix is too large to process directly.\n3. Hardt and Roth [HR13] consider the power method in the context of privacy-preserving PCA where noise is added to achieve differential privacy.\nIn each setting there has so far only been an ad-hoc analysis of the noisy power method. In the first setting, only local convergence is argued, that is, X0 has to be cleverly chosen. In the second setting, the analysis only holds for the spiked covariance model of PCA. In the third application, only the case p = 1 was considered.\nIn this work we give a completely general analysis of the noisy power method that overcomes limitations of previous analyses. Our result characterizes the global convergence properties of the algorithm in terms of the noise G` and the initial subspace X0. We then consider the important case where X0 is a randomly chosen orthonormal basis. This case is rather delicate since the initial correlation between a random matrix X0 and the target subspace is vanishing in the dimension d for small p. Another important feature of the analysis\nis that it shows how X` converges towards the first k 6 p singular vectors. Choosing p to be larger than the target dimension leads to a quantitatively stronger result. Theorem 2.4 formally states our convergence bound. Here we highlight one useful corollary to illustrate our more general result.\nCorollary 1.1. Let k 6 p. Let U ∈Rd×k represent the top k singular vectors of A and let σ1 > · · · > σn > 0 denote its singular values. Suppose X0 is an orthonormal basis of a random p-dimensional subspace. Further suppose that at every step of NPM we have\n5‖G`‖ 6 ε(σk − σk+1) and 5‖U>G`‖ 6 (σk − σk+1) √ p− √ k−1\nτ √ d\nfor some fixed parameter τ and ε < 1/2. Then with all but τ−Ω(p+1−k) + e−Ω(d) probability, there exists an L =O( σkσk−σk+1 log(dτ/ε)) so that after L steps we have that ∥∥∥(I −XLX>L )U∥∥∥ 6 ε. The corollary shows that the algorithm converges in the strong sense that the entire spectral norm of U up to an ε error is contained in the space spanned by XL. To achieve this the result places two assumptions on the magnitude of the noise. The total spectral norm of G` must be bounded by ε times the separation between σk and σk+1. This dependence on the singular value separation arises even in the classical perturbation theory of Davis-Kahan [DK70]. The second condition is specific to the power method and requires that the noise term is proportionally smaller when projected onto the space spanned by the top k singular vectors. This condition ensures that the correlation between X` and U that is initially very small is not destroyed by the noise addition step. If the noise term has some spherical properties (e.g. a Gaussian matrix), we expect the projection onto U to be smaller by a factor of √ k/d, since the space U is k-dimensional. In the case where p = k +Ω(k) this is precisely what the condition requires. When p = k the requirement is stronger by a factor of k. This phenomenon stems from the fact that the smallest singular value of a random p × k gaussian matrix behaves differently in the square and the rectangular case.\nWe demonstrate the usefulness of our convergence bound with several novel results in some of the aforementioned applications."
    }, {
      "heading" : "1.1 Application to memory-efficient streaming PCA",
      "text" : "In the streaming PCA setting we receive a stream of samples z1, z2, . . . zn ∈Rd drawn i.i.d. from an unknown distribution D over Rd . Our goal is to compute the dominant k eigenvectors of the covariance matrix A = Ez∼D zz>. The challenge is to do this in space linear in the output size, namely O(kd). Recently, Mitgliakas et al. [MCJ13] gave an algorithm for this problem based on the noisy power method. We analyze the same algorithm, which we restate here and call SPM:\nThe algorithm can be executed in space O(pd) since the update step can compute the d × p matrix A`X`−1 incrementally without explicitly computing A`. The algorithm maps to our setting by defining G` = (A` −A)X`−1. With this notation Y` = AX`−1 +G`. We can apply Corollary 1.1 directly once we have suitable bounds on ‖G`‖ and ‖U>G`‖.\nThe result of [MCJ13] is specific to the spiked covariance model. The spiked covariance model is defined by an orthonormal basis U ∈ Rd×k and a diagonal matrix Λ ∈ Rk×k with diagonal entries λ1 > λ2 > · · · > λk > 0. The distribution D(U,Λ) is defined as the normal distribution N(0, (UΛ2U> + σ2Idd×d)). Without loss of generality we can scale the examples\nsuch that λ1 = 1. One corollary of our result shows that the algorithm outputs XL such that∥∥∥(I −XLX>L )U∥∥∥ 6 ε with probability 9/10 provided p = k +Ω(k) and the number of samples satisfies\nn = Θ σ6 + 1ε2λ6k kd  .\nPreviously, the same bound1 was known with a quadratic dependence on k in the case where p = k. Here we can strengthen the bound by increasing p slightly.\nWhile we can get some improvements even in the spiked covariance model, our result is substantially more general and applies to any distribution. The sample complexity bound we get varies according to a technical parameter of the distribution. Roughly speaking, we get a near linear sample complexity if the distribution is either “round” (as in the spiked covariance setting) or is very well approximated by a k dimensional subspace. To illustrate the latter condition, we have the following result without making any assumptions other than scaling the distribution:\nCorollary 1.2. Let D be any distribution scaled so that Pr {‖z‖ > t} 6 exp(−t) for every t > 1. Let U represent the top k eigenvectors of the covariance matrix Ezz> and σ1 > · · · > σd > 0 its eigenvalues. Then, SPM invoked with p = k + Ω(k) outputs a matrix XL such with probability 9/10 we have∥∥∥(I −XLX>L )U∥∥∥ 6 ε provided SPM receives n samples where n satisfies n = Õ ( σkε2k(σk−σk+1)3 · d) .\nThe corollary establishes a sample complexity that’s linear in d provided that the spectrum decays quickly, as is common in applications. For example, if the spectrum follows a power law so that σj ≈ j−c for a constant c > 1/2, the bound becomes n = Õ(k2c+2d/ε2)."
    }, {
      "heading" : "1.2 Application to privacy-preserving spectral analysis",
      "text" : "Many applications of singular vector computation are plagued by the fact that the underlying matrix contains sensitive information about individuals. A successful paradigm in privacypreserving data analysis rests on the notion of differential privacy which requires all access to the data set to be randomized in such a way that the presence or absence of a single data item is hidden. The notion of data item varies and could either refer to a single entry, a single row, or a rank-1 matrix of bounded norm. More formally, Differential Privacy requires that the output distribution of the algorithm changes only slightly with the addition or deletion of\n1That the bound stated in [MCJ13] has a σ6 dependence is not completely obvious. There is a O(σ4) in the numerator and log((σ2 + 0.75λ2k )/(σ 2 + 0.5λ2k )) in the denominator which simplifies to O(1/σ 2) for constant λk and σ2 > 1.\na single data item. This requirement often necessitates the introduction of significant levels of noise that make the computation of various objectives challenging. Differentially private singular vector computation has been studied actively in recent years [BDMN05, MM09, BBDS12, CSS12, KT13, HR12, HR13, DTTZ14]. There are two main objectives. The first is computational efficiency. The second objective is to minimize the amount of error that the algorithm introduces.\nIn this work, we give a fast algorithm for differentially private singular vector computation based on the noisy power method that leads to nearly optimal bounds in a number of settings that were considered in previous work. The algorithm is described in Figure 3. It’s a simple instance of NPM in which each noise matrix G` is a gaussian random matrix scaled so that the algorithm achieves (ε,δ)-differential privacy (as formally defined in Definition 4.1). It is easy to see that the algorithm can be implemented in time nearly linear in the number of nonzero entries of the input matrix (input sparsity). This will later lead to strong improvements in running time compared with several previous works.\nWe first state a general purpose analysis of PPM that follows from Corollary 1.1.\nTheorem 1.3. Let k 6 p. Let U ∈ Rd×k represent the top k singular vectors of A and let σ1 > · · · > σd > 0 denote its singular values. Then, PPM satisfies (ε,δ)-differential privacy and after L =O( σkσk−σk+1 log(d)) iterations we have with probability 9/10 that∥∥∥(I −XLX>L )U∥∥∥ 6Oσmax‖X`‖∞√d logLσk − σk+1 · √ p √ p − √ k − 1\n . When p = k + Ω(k) the trailing factor becomes a constant. If p = k it creates a factor k overhead. In the worst-case we can always bound ‖X`‖∞ by 1 since X` is an orthonormal basis. However, in principle we could hope that a much better bound holds provided that the target subspace U has small coordinates. Hardt and Roth [HR12, HR13] suggested a way to accomplish a stronger bound by considering a notion of coherence of A, denoted as µ(A). Informally, the coherence is a well-studied parameter that varies between 1 and n, but is often observed to be small. Intuitively, the coherence measures the correlation between the singular vectors of the matrix with the standard basis. Low coherence means that the singular vectors have small coordinates in the standard basis. Many results on matrix completion and robust PCA crucially rely on the assumption that the underlying matrix has low coherence [CR09, CT10, CLMW11] (though the notion of coherence here will be somewhat different).\nTheorem 1.4. Under the assumptions of Theorem 1.3, we have the conclusion\n∥∥∥(I −XLX>L )U∥∥∥ 6Oσ√µ(A) logd logLσk − σk+1 · √ p √ p − √ k − 1  . Hardt and Roth proved this result for the case where p = 1. The extension to p > 1 lost a factor of √ d in general and therefore gave no improvement over Theorem 1.3. Our result resolves the main problem left open in their work. The strength of Theorem 1.4 is that the bound is essentially dimension-free under a natural assumption on the matrix and never worse than our worst-case result. It is also known that in general the dependence on d achieved in Theorem 1.3 is best possible in the worst case (see discussion in [HR13]) so that further progress requires making stronger assumptions. Coherence is a natural such assumption. The proof of Theorem 1.4 proceeds by showing that each iterate X` satisfies ‖X`‖∞ 6O( √ µ(A) log(d)/d) and applying Theorem 1.3. To do this we exploit a non-trivial symmetry of the algorithm that we discuss in Section 4.3.\nOther objective functions and variants differential privacy. An important recent work by Dwork, Talwar, Thakurta and Zhang analyzes the mechanism of adding Gaussian noise to the covariance matrix and computing a truncated singular value decomposition of the noisy covariance matrix [DTTZ14]. Their objective function is a natural measure of how much variance of the data is captured by the resulting subspace. Our results are formally incomparable due to a different choice of objective function. We also do not know how to analyze the performance of the power method under their objective function. Indeed, this is an interesting question related to the content of Conjecture 1.6 that we will state shortly.\nOur discussion above applied to (ε,δ)-differential privacy under changing a single entry of the matrix. Several works consider other variants of differential privacy. It is generally easy to adapt the power method to these settings by changing the noise distribution or its scaling. To illustrate this aspect, we consider the problem of privacy-preserving principal component analysis as recently studied by [CSS12, KT13]. Both works consider an algorithm called exponential mechanism. The first work gives a heuristic implementation that may not converge, while the second work gives a provably polynomial time algorithm though the running time is more than cubic. Our algorithm gives strong improvements in running time while giving nearly optimal accuracy guarantees as it matches a lower bound of [KT13] up to a Õ( √ k) factor. We also improve the error dependence on k by polynomial factors compared to previous work. Moreover, we get an accuracy improvement of O( √ d) for the case of (ε,δ)-differential privacy, while these previous works only apply to (ε,0)-differential privacy. Section 4.2 provides formal statements."
    }, {
      "heading" : "1.3 Related Work",
      "text" : "Numerical Analysis. One might expect that a suitable analysis of the noisy power method would have appeared in the numerical analysis literature. However, we are not aware of a reference and there are a number of points to consider. First, our noise model is adaptive thus setting it apart from the classical perturbation theory of the singular vector decomposition [DK70]. Second, we think of the perturbation at each step as large making it conceptually different from floating point errors. Third, research in numerical analysis over the past decades\nhas largely focused on faster Krylov subspace methods. There is some theory of inexact Krylov methods, e.g., [SS07] that captures the effect of noisy matrix-vector products in this context. Related to our work are also results on the perturbation stability of the QR-factorization since those could be used to obtain convergence bounds for subspace iteration. Such bounds, however, must depend on the condition number of the matrix that the QR-factorization is applied to. See Chapter 19.9 in [Hig02] and the references therein for background. Our proof strategy avoids this particular dependence on the condition number.\nStreaming PCA. PCA in the streaming model is related to a host of well-studied problems that we cannot survey completely here. We refer to [ACLS12, MCJ13] for a thorough discussion of prior work. Not mentioned therein is a recent work on incremental PCA [BDF13] that leads to space efficient algorithms computing the top singular vector; however, it’s not clear how to extend their results to computing multiple singular vectors.\nPrivacy. There has been much work on differentially private spectral analysis starting with Blum et al. [BDMN05] who used an algorithm sometimes called Randomized Response, which adds a single noise matrix N either to the input matrix A or the covariance matrix AA>. This approach was used by McSherry and Mironov [MM09] for the purpose of a differentially private recommender system. Most recently, as discussed earlier, Dwork, Talwar, Thakurta and Zhang [DTTZ14] revisit (a variant of) the this algorithm and give matching upper and lower bounds under a natural objective function. While often suitable when AA> fits into memory, the approach can be difficult to apply when the dimension of AA> is huge as it requires computing a dense noise matrix N. The power method can be applied more easily to large sparse matrices, as well as in a streaming setting as shown by [MCJ13].\nChaudhuri et al. [CSS12] and Kapralov-Talwar [KT13] use the so-called exponential mechanism to sample approximate eigenvectors of the matrix. The sampling is done using a heuristic approach without convergence polynomial time convergence guarantees in the first case and using a polynomial time algorithm in the second. Both papers achieve a tight dependence on the matrix dimension d (though the dependence on k is suboptimal in general). Most closely related to our work are the results of Hardt and Roth [HR13, HR12] that introduced matrix coherence as a way to circumvent existing worst-case lower bounds on the error. They also analyzed a natural noisy variant of power iteration for the case of computing the dominant eigenvector of A. When multiple eigenvectors are needed, their algorithm uses the well-known deflation technique. However, this step loses control of the coherence of the original matrix and hence results in suboptimal bounds. In fact, a √ rank(A) factor is lost."
    }, {
      "heading" : "1.4 Open Questions",
      "text" : "We believe Corollary 1.1 to be a fairly precise characterization of the convergence of the noisy power method to the top k singular vectors when p = k. The main flaw is that the noise tolerance depends on the eigengap σk − σk+1, which could be very small. We have some conjectures for results that do not depend on this eigengap.\nFirst, when p > k, we think that Corollary 1.1 might hold using the gap σk − σp+1 instead of σk − σk+1. Unfortunately, our proof technique relies on the principal angle decreasing at each step, which does not necessarily hold with the larger level of noise. Nevertheless we expect the\nprincipal angle to decrease fairly fast on average, so that XL will contain a subspace very close to U . We are actually unaware of this sort of result even in the noiseless setting.\nConjecture 1.5. Let X0 be a random p-dimensional basis for p > k. Suppose at every step we have\n100‖G`‖ 6 ε(σk − σp+1) and 100‖UTG`‖ 6 √ p − √ k − 1\n√ d\nThen with high probability, after L =O( σkσk−σp+1 log(d/ε)) iterations we have\n‖(I −XLX>L )U‖ 6 ε.\nThe second way of dealing with a small eigengap would be to relax our goal. Corollary 1.1 is quite stringent in that it requires XL to approximate the top k singular vectors U , which gets harder when the eigengap approaches zero and the kth through p+ 1st singular vectors are nearly indistinguishable. A relaxed goal would be for XL to spectrally approximate A, that is\n‖(I −XLX>L )A‖ 6 σk+1 + ε. (1)\nThis weaker goal is known to be achievable in the noiseless setting without any eigengap at all. In particular, [HMT11] shows that (1) happens after L =O(σk+1ε logn) steps in the noiseless setting. A plausible extension to the noisy setting would be:\nConjecture 1.6. Let X0 be a random 2k-dimensional basis. Suppose at every step we have\n‖G`‖ 6 ε and ‖UTG`‖ 6 ε √ k/d\nThen with high probability, after L =O(σk+1ε logd) iterations we have that\n‖(I −XLX>L )A‖ 6 σk+1 +O(ε)."
    }, {
      "heading" : "2 Convergence of the noisy power method",
      "text" : "Figure 1 presents our basic algorithm that we analyze in this section. An important tool in our analysis are principal angles, which are useful in analyzing the convergence behavior of numerical eigenvalue methods. Roughly speaking, we will show that the tangent of the k-th principal angle between X and the top k eigenvectors of A decreases as σk+1/σk in each iteration of the noisy power method.\nDefinition 2.1 (Principal angles). Let X and Y be subspaces of Rd of dimension at least k. The principal angles 0 6 θ1 6 · · · 6 θk between X and Y and associated principal vectors x1, . . . ,xk and y1, . . . , yk are defined recursively via\nθi(X ,Y ) = min { arccos ( 〈x,y〉 ‖x‖2‖y‖2 ) : x ∈ X , y ∈ Y ,x ⊥ xj , y ⊥ yj for all j < i } and xi , yi are the x and y that give this value. For matrices X and Y , we use θk(X,Y ) to denote the kth principal angle between their ranges."
    }, {
      "heading" : "2.1 Convergence argument",
      "text" : "We will make use of a non-recursive expression for the principal angles, defined in terms of the set Pk of p × p projection matrices Π from p dimensions to k dimensional subspaces:\nClaim 2.2. Let U ∈Rd×k have orthonormal columns and X ∈Rd×p have independent columns, for p > k. Then\ncosθk(U,X) = max Π∈Pk min x∈range(XΠ) ‖x‖2=1 ‖U>x‖ = max Π∈Pk min ‖w‖2=1 Πw=w\n‖U>Xw‖ ‖Xw‖ .\nFor V =U⊥, we have\ntanθk(U,X) = min Π∈Pk max x∈range(XΠ) ‖V >x‖ ‖U>x‖ = min Π∈Pk\nmax ‖w‖2=1 Πw=w\n‖V >Xw‖ ‖U>Xw‖ .\nFix parameters 1 6 k 6 p 6 d. In this section we consider a symmetric d × d matrix A with singular values σ1 > σ2 > · · · > σd . We let U ∈ Rd×k contain the first k eigenvectors of A. Our main lemma shows that tanθk(U,X) decreases multiplicatively in each step.\nLemma 2.3. Let U contain the largest k eigenvectors of a symmetric matrix A ∈ Rd×d , and let X ∈Rd×p with XtransX = Id for some p > k. Let G ∈Rd×p satisfy\n4‖U>G‖ 6 (σk − σk+1)cosθk(U,X) 4‖G‖ 6 (σk − σk+1)ε.\nfor some ε < 1. Then\ntanθk(U,AX +G) 6max ε,maxε,(σk+1σk )1/4 tanθk(U,X) .\nProof. Let Π∗ be the matrix projecting onto the smallest k principal angles of X, so that\ntanθk(U,X) = max ‖w‖2=1 Π∗w=w\n‖V >Xw‖ ‖U>Xw‖ .\nWe have that\ntanθk(U,AX +G) = min Π∈Pk max ‖w‖2=1 Πw=w\n‖V >(AX +G)w‖ ‖U>(AX +G)w‖\n6 max ‖w‖2=1 Π∗w=w\n‖V >AXw‖+ ‖V >Gw‖ ‖U>AXw‖ − ‖U>Gw‖\n6 max ‖w‖2=1 Π∗w=w\n1 ‖U>Xw‖ · σk+1‖V >Xw‖+ ‖V >Gw‖ σk − ‖U>Gw‖/‖U>Xw‖ (2)\nDefine ∆ = (σk − σk+1)/4. By the assumption on G,\nmax ‖w‖2=1 Π∗w=w\n‖U>Gw‖ ‖U>Xw‖ 6 ‖U>G‖/ cosθk(U,X) 6 (σk − σk+1)/4 = ∆.\nSimilarly, and using that 1/ cosθ 6 1 + tanθ for any angle θ,\nmax ‖w‖2=1 Π∗w=w\n‖V >Gw‖ ‖U>Xw‖ 6 ‖G‖/ cosθk(U,X) 6 ε∆(1 + tanθk(U,X)).\nPlugging back into (2) and using σk = σk+1 + 4∆,\ntanθk(U,AX +G) 6 max ‖w‖2=1 Π∗w=w\n‖V >Xw‖ ‖U>Xw‖ · σk+1 σk+1 + 3∆ + ε∆(1 + tanθk(U,X)) σk+1 + 3∆ .\n= σk+1 + ε∆ σk+1 + 3∆ tanθk(U,X) + ε∆ σk+1 + 3∆ = (1− ∆ σk+1 + 3∆ ) σk+1 + ε∆ σk+1 + 2∆ tanθk(U,X) + ∆ σk+1 + 3∆ ε 6max(ε, σk+1 + ε∆ σk+1 + 2∆ tanθk(U,X))\nwhere the last inequality uses that the weighted mean of two terms is less than their maximum. Finally, we have that\nσk+1 + ε∆ σk+1 + 2∆ 6max( σk+1 σk+1 +∆ , ε)\nbecause the left hand side is a weighted mean of the components on the right. Since σk+1σk+1+∆ 6 ( σk+1σk+1+4∆ ) 1/4 = (σk+1/σk)1/4, this gives the result.\nWe can inductively apply the previous lemma to get the following general convergence result.\nTheorem 2.4. Let U represent the top k eigenvectors of the matrix A and γ = 1− σk+1/σk . Suppose that the initial subspace X0 and noise G` is such that\n5‖U>G`‖ 6 (σk − σk+1)cosθk(U,X0) 5‖G`‖ 6 ε(σk − σk+1)\nat every stage `, for some ε < 1/2. Then there exists an L . 1γ log ( tanθk(U,X0) ε ) such that for all ` > L we have tanθ(U,XL) 6 ε.\nProof of Theorem 2.4. We will see that at every stage ` of the algorithm,\ntanθk(U,X`) 6max(ε, tanθk(U,X0))\nwhich implies for ε 6 1/2 that\ncosθk(U,X`) >min(1− ε2/2,cosθk(U,X0)) > 7 8 cosθk(U,X0)\nso Lemma 2.3 applies at every stage. This means that\ntanθk(U,X`+1) = tanθk(U,AX` +G) 6max(ε,δ tanθk(U,X`))\nfor δ = max(ε, (σk+1/σk)1/4). After\nL = log1/δ tanθk(U,X0)\nε\niterations the tangent will reach ε and remain there. Observing that\nlog(1/δ) &min(log(1/ε), log(σk/σk+1)) >min(1, log 1\n1−γ ) >min(1,γ) = γ\ngives the result."
    }, {
      "heading" : "2.2 Random initialization",
      "text" : "The next lemma essentially follows from bounds on the smallest singular value of gaussian random matrices [RV09].\nLemma 2.5. For an arbitrary orthonormal U and random subspace X, we have\ntanθk(U,X) 6 τ\n√ d\n√ p − √ k − 1\nwith all but τ−Ω(p+1−k) + e−Ω(d) probability.\nProof. Consider the singular value decomposition U>X = AΣB> of U>X. Setting Π to be matrix projecting onto the first k columns of B, we have that\ntanθk(U,X) 6 max ‖w‖2=1 Πw=w\n‖V >Xw‖ ‖U>Xw‖\n6 ‖V >X‖ max ‖w‖2=1 Πw=w 1 ‖ΣB>w‖ = ‖V >X‖ max ‖w‖2=1 supp(w)∈[k] 1 ‖Σw‖ = ‖V >X‖ σk(U>X) .\nLet X ∼N (0, Id×p) represent the random subspace. Then Y :=U>X ∼N (0, Ik×p). By [RV09], for any ε, the smallest singular value of Y is at least ( √ p − √ k − 1)/τ with all but τ−Ω(p+1−k) + e−Ω(p) probability. On the other hand, ‖X‖ . √ d with all but e−Ω(d) probability. Hence\ntanθk(U,X) . τ\n√ d\n√ p − √ k − 1\nwith the desired probability. Rescaling τ gets the result.\nWith this lemma we can prove the corollary that we stated in the introduction.\nProof of Corollary 1.1. By Claim 2.5, with the desired probability we have tanθk(U,X0) 6 τ √ d√\np− √ k−1\n. Hence cosθk(U,X0) > 1/(1 + tanθk(U,X0)) > √ p− √ k−1\n2·τ √ d . Rescale τ and apply Theo-\nrem 2.4 to get that tanθk(U,XL) 6 ε. Then ‖(I −XLX>L )U‖ = sinθk(U,XL) 6 tanθk(U,XL) 6 ε."
    }, {
      "heading" : "3 Memory efficient streaming PCA",
      "text" : "In the streaming PCA setting we receive a stream of samples z1, z2, · · · ∈ Rd . Each sample is drawn i.i.d. from an unknown distribution D over Rd . Our goal is to compute the dominant k eigenvectors of the covariance matrix A = Ez∼D zz>. The challenge is to do this with small space, so we cannot store the d2 entries of the sample covariance matrix. We would like to use O(dk) space, which is necessary even to store the output.\nThe streaming power method (Figure 2, introduced by [MCJ13]) is a natural algorithm that performs streaming PCA with O(dk) space. The question that arises is how many samples it requires to achieve a given level of accuracy, for various distributions D. Using our general analysis of the noisy power method, we show that the streaming power method requires fewer samples and applies to more distributions than was previously known.\nWe analyze a broad class of distributions:\nDefinition 3.1. A distribution D over Rd is (B,p)-round if for every p-dimensional projection P and all t > 1 we have Prz∼D {‖z‖ > t} 6 exp(−t) and Prz∼D { ‖P z‖ > t · √ Bp/d } 6 exp(−t) .\nThe first condition just corresponds to a normalization of the samples drawn from D. Assuming the first condition holds, the second condition always holds with B = d/p. For this reason our analysis in principle applies to any distribution, but the sample complexity will depend quadratically on B.\nLet us illustrate this definition through the example of the spiked covariance model studied by [MCJ13]. The spiked covariance model is defined by an orthonormal basis U ∈Rd×k and a diagonal matrix Λ ∈Rk×k with diagonal entries λ1 > λ2 > · · · > λk > 0. The distribution D(U,Λ) is defined as the normal distribution N(0, (UΛ2U> + σ2Idd×d)/D) where D = Θ(dσ2 + ∑ i λ 2 i ) is a normalization factor chosen so that the distribution satisfies the norm bound. Note that the the i-th eigenvalue of the covariance matrix is σi = (λ 2 i + σ\n2)/D for 1 6 i 6 k and σi = σ2/D for i > k. We show in Lemma 3.6 that the spiked covariance model D(U,Λ) is indeed (B,p)-round for B =O( λ 2 1+σ 2\ntr(Λ)/d+σ2 ), which is constant for σ & λ1. We have the following main theorem.\nTheorem 3.2. Let D be a (B,p)-round distribution over Rd with covariance matrix A whose eigenvalues are σ1 > σ2 > · · · > σd > 0. Let U ∈ Rd×k be an orthonormal basis for the eigenvectors corresponding to the first k eigenvalues of A. Then, the streaming power method SPM returns an orthonormal basis X ∈ Rd×p such that tanθ(U,X) 6 ε with probability 9/10 provided that SPM receives n samples from D for some n satisfying\nn 6 Õ\n( B2σkk log 2d\nε2(σk − σk+1)3d ) if p = k +Θ(k). More generally, for all p > k one can get the slightly stronger result\nn 6 Õ Bpσkmax{1/ε2,Bp/(√p − √ k − 1)2} log2d\n(σk − σk+1)3d  . Instantiating with the spiked covariance model gives the following:\nCorollary 3.3. In the spiked covariance model D(U,Λ) the conclusion of Theorem 3.2 holds for p = 2k with\nn = Õ  (λ21 + σ2)2(λ2k + σ2)ε2λ6k dk  .\nWhen λ1 =O(1) and λk = Ω(1) this becomes n = Õ ( σ6+1 ε2 · dk ) .\nWe can apply Theorem 3.2 to all distributions that have exponentially concentrated norm by setting B = d/p. This gives the following result.\nCorollary 3.4. Let D be any distribution scaled such that Prz∼D[‖z‖ > t] 6 exp(−t) for all t > 1. Then the conclusion of Theorem 3.2 holds for p = 2k with\nn = Õ (\nσk ε2k(σk − σk+1)3\n· d ) .\nIf the eigenvalues follow a power law, σj ≈ j−c for a constant c > 1/2, this gives an n = Õ(k2c+2d/ε2) bound on the sample complexity."
    }, {
      "heading" : "3.1 Error term analysis",
      "text" : "Fix an orthonormal basis X ∈ Rd×k . Let z1, . . . , zn ∼ D be samples from a distribution D with covariance matrix A and consider the matrix\nG = ( A− Â ) X ,\nwhere Â = 1n ∑n i=1 ziz > i is the empirical covariance matrix on n samples. Then, we have that ÂX = AX +G. In other words, one update step of the power method executed on Â can be expressed as an update step on A with noise matrix G. This simple observation allows us to apply our analysis of the noisy power method to this setting after obtaining suitable bounds on ‖G‖ and ‖U>G‖.\nLemma 3.5. Let D be a (B,p)-round distribution with covariance matrix M. Then with all but O(1/n2) probability,\n‖G‖ .\n√ Bp log4n logd\ndn + 1 n2 and ‖U>G‖ .\n√ B2p2 log4n logd\nd2n + 1 n2\nProof. We will use a matrix Chernoff bound to show that 1. Pr { ‖G‖ > Ct log(n)2 √ Bp/d +O(1/n2) } 6 d exp(−t2n) + 1/n2\n2. Pr { ‖U>G‖ > Ct log(n)2Bp/d +O(1/n2) } 6 d exp(−t2n) + 1/n2\nsetting t = √\n2 n logd gives the result. However, matrix Chernoff inequality requires the distri-\nbution to satisfy a norm bound with probability 1. We will therefore create a closely related distribution D̃ that satisfies such a norm constraint and is statistically indistinguishable up to small error on n samples. We can then work with D̃ instead of D. This truncation step is standard and works because of the concentration properties of D.\nIndeed, let D̃ be the distribution obtained from D be replacing a sample z with 0 if\n‖z‖ > C log(n) or ‖U>z‖ > C log(n) √ Bp/d or ‖z>X‖ > C log(n) √ Bp/d .\nFor sufficiently large constant C, it follows from the definition of (B,p)-round that the probability that one or more of n samples from D get zeroed out is at most 1/n2. In particular, the two product distributions D(n) and D̃(n) have total variation distance at most 1/n2. Furthermore, we claim that the covariance matrices of the two distributions are at most O(1/n2) apart in spectral norm. Formally,∥∥∥∥∥ Ez∼Dzz> − Ez̃∼D̃ z̃z̃> ∥∥∥∥∥ 6 1n2 ·O (∫ t>1 C2t2 log2(n)exp(−t)dt ) 6O(1/n2) .\nIn the first inequality we use the fact that z only gets zeroed out with probability 1/n2. Conditional on this event, the norm of z is larger than tC log(n) with probability at most n2 exp(−12 tC logn) 6 exp(−t).Assuming the norm is at most tC log(n) we have ‖zz\n>‖ 6 t2C2 log2(n) and this bounds the contribution to the spectral norm of the difference.\nNow let G̃ be the error matrix defined as G except that we replace the samples z1, . . . , zn by n samples z̃1, . . . , z̃n from the truncated distribution D̃. By our preceding discussion, it now suffices to show that\n1. Pr { ‖G̃‖ > Ct log2(n) √ Bp/d } 6 d exp(−t2n)\n2. Pr { ‖U>G̃‖ > Ct log2(n)Bp/d } 6 d exp(−t2n)\nTo see this, let Si = z̃i z̃ > i X. We have\n‖Si‖ 6 ‖z̃i‖ · ∥∥∥z̃>i X∥∥∥ 6 C2 log2(n) ·√Bp/d\nSimilarly, ∥∥∥U>Si∥∥∥ 6 ‖U>z̃i‖ · ∥∥∥z̃>i X∥∥∥ 6 C2 log2(n) · Bpd . The claims now follow directly from the matrix Chernoff bound stated in Lemma A.4."
    }, {
      "heading" : "3.2 Proof of Theorem 3.2",
      "text" : "Given Lemma 3.5 we will choose n such that the error term in each iteration satisfies the assumptions of Theorem 2.4. Let G` denote the instance of the error term G arising in the `-th iteration of the algorithm. We can find an n satisfying\nn\nlog(n)4 =O Bpmax { 1/ε2,Bp/( √ p − √ k − 1)2 } logd\n(σk − σk+1)2d  such that by Lemma 3.5 we have that with probability 1−O(1/n2),\n‖G`‖ 6 ε(σk − σk+1)\n5 and ‖U>G`‖ 6 σk − σk+1 5\n√ p − √ k − 1\n√ d\n.\nHere we used that by definition 1/n ε and 1/n σk−σk+1 and so the 1/n2 term in Lemma 3.5 is of lower order.\nWith this bound, it follows from Theorem 2.4 that after L = O(log(d/ε)/(1 − σk+1/σk)) iterations we have with probability 1−max{1,L/n2} that tanθ(U,XL) 6 ε. The over all sample complexity is therefore\nLn = Õ Bpσkmax { 1/ε2,Bp/( √ p − √ k − 1)2 } log2d\n(σk − σk+1)3d  . Here we used that 1− σk+1/σk = (σk − σk+1)/σk . This concludes the proof of Theorem 3.2."
    }, {
      "heading" : "3.3 Proof of Lemma 3.6 and Corollary 3.4",
      "text" : "Lemma 3.6. The spiked covariance model D(U,Λ) is (B,k)-round for B =O( λ 2 1+σ 2\ntr(Λ)/d+σ2 ).\nProof. Note that an example z ∼ D(U,Λ) is distributed as UΛg + g ′ where g ∼N(0,1/D)k is a standard gaussian and g ′ ∼ N(0,σ2/D)d . is a noise term. Recall, that D is the normalization term. Let P be any projection operator onto a k-dimensional space. Then,\n‖P z‖ = ‖PUΛg + P g ′‖ 6 ‖PUΛg‖+ ‖P g ′‖ 6 ‖Λg‖+ ‖P g ′‖ 6 λ1‖g‖+ ‖P g ′‖ .\nBy rotational invariance of g ′, we may assume that P is the projection onto the first k coordinates. Hence, ‖P g ′‖ is distributed like the norm of N(0,σ2/D)k . Using standard tail bounds for the norm of a gaussian random variables, we can see that ‖P z‖2 =O(t(kλ21 + kσ2)/D) with probability 1− exp(−t). On the other hand, D = Θ( ∑k i=1λ 2 i + dσ\n2). We can now solve for B by setting\nΘ( kλ21 + kσ 2∑k i=1λ 2 i + dσ 2 ) = Bk d\n⇔ B = Θ( λ21 + σ 2\n1 d ∑k i=1λ 2 i + σ 2 ) .\nCorollary 3.4 follows by plugging in the bound on B and the eigenvalues of the covariance matrix into our main theorem.\nProof of Corollary 3.4. In the spiked covariance model D(U,Λ) we have\nB = λ21 + σ 2\nD , σk =\nλ2k + σ 2\nD , σk+1 =\nσ2 D , D =O(tr(Λ2) + dσ2) .\nHence, B2σk\n(σk − σk+1)3d =\n(λ21 + σ 2)2(λ2k + σ 2)\nλ6kd 6\n(λ21 + σ 2)3\nλ6kd\nPlugging this bound into Theorem 3.2 gives Corollary 3.4."
    }, {
      "heading" : "4 Privacy-preserving singular vector computation",
      "text" : "In this section we prove our results about privacy-preserving singular vector computation. We begin with a standard definition of differential privacy, sometimes referred to as entry-level differential privacy, as it hides the presence or absence of a single entry.\nDefinition 4.1 (Differential Privacy). A randomized algorithmM : Rd×d ′ → R (where R is some arbitrary abstract range) is (ε,δ)-differentially private if for all pairs of matrices A,A′ ∈ Rd×d′ differing in only one entry by at most 1 in absolute value, we have that for all subsets of the range S ⊆ R, the algorithm satisfies: Pr {M(A) ∈ S} 6 exp(ε)Pr {M(A′) ∈ S}+ δ .\nThe definition is most meaningful when A has entries in [0,1] so that the above definition allows for a single entry to change arbitrarily within this range. However, this is not a requirement for us. The privacy guarantee can be strengthened by decreasing ε > 0.\nFor our choice of σ in Figure 3 the algorithm satisfies (ε,δ)-differential privacy as follows easily from properties of the Gaussian distribution. See, for example, [HR13] for a proof.\nClaim 4.2. PPM satisfies (ε,δ)-differential privacy.\nIt is straightforward to prove Theorem 1.3 by invoking our convergence analysis of the noisy power method together with suitable error bounds. The error bounds are readily available as the noise term is just gaussian.\nProof of Theorem 1.3. Let m = max‖X`‖∞. By Lemma A.2 the following bounds hold with probability 99/100:\n1. maxL`=1 ‖G`‖ . σm √ d logL\n2. maxL`=1 ‖U >G`‖ . σm\n√ k logL\nLet\nε′ = σm\n√ d logL\nσk − σk+1 & 5maxL`=1 ‖G`‖ σk − σk+1 .\nBy Corollary 1.1, if we also have that maxL`=1 ‖U >G`‖ 6 (σk − σk+1)\n√ p− √ k−1\nτ √ d for a sufficiently large constant τ , then we will have that\n‖(I −XLX>L )U‖ 6 ε ′ 6\nσm √ d logL\nσk − σk+1\nafter the desired number of iterations, giving the theorem. Otherwise,\n(σk − σk+1) √ p − √ k − 1\nτ √ d\n6 L max `=1 ‖U>G`‖ . ε′(σk − σk+1)\n√ k/d,\nso it is trivially true that σm √ d logL\nσk − σk+1\n√ p\n√ p − √ k − 1\n> ε′ √ k\n√ p − √ k − 1\n& 1 > ‖(I −XLX>L )U‖."
    }, {
      "heading" : "4.1 Low-rank approximation",
      "text" : "Our results readily imply that we can compute accurate differentially private low-rank approximations. The main observation is that, assuming XL and U have the same dimension, tanθ(U,XL) 6 α implies that the matrix XL also leads to a good low-rank approximation for A in the spectral norm. In particular\n‖(I −XLX>L )A‖ 6 σk+1 +ασ1 . (3)\nMoreover the projection step of computing XLX > L A can be carried out easily in a privacypreserving manner. It is again the `∞-norm of the columns of XL that determine the magnitude of noise that is needed. Since A is symmetric, we have X>A = (AX)>. Hence, to obtain a good low-rank approximation it suffices to compute the product AXL privately as AXL +GL. This leads to the following corollary.\nCorollary 4.3. Let A ∈ Rd×d be a symmetric matrix with singular values σ1 > . . . > σd and let γ = 1−σk+1/σk . There is an (ε,δ)-differentially private algorithm that given A and k, outputs a rank 2k matrix B such that with probability 9/10,\n‖A−B‖ 6 σk+1 + Õ σ1√(k/γ)d logd log(1/δ)ε(σk − σk+1)  . The Õ-notation hides the factor O (√ log(log(d)/γ) ) .\nProof. Apply Theorem 1.3 with p = 2k and run the algorithm for L + 1 steps with L = O(γ−1 logd). This gives the bound\nα = ‖(I −XLX>L )A‖ 6O √(k/γ)d logd log(log(d)/γ) log(1/δ)ε(σk − σk+1)  . Moreover, the algorithm has computed YL+1 = AXL +GL and we have B = XLY > L+1 = XLX > L A+ XLG > L . Therefore\n‖A−B‖ 6 σk+1 +ασ1 + ∥∥∥XLG>L ∥∥∥\nwhere ∥∥∥XLG>L ∥∥∥ 6 ‖GL‖ . By definition of the algorithm and Lemma A.2, we have\n‖GL‖ 6O (√ σ2d ) =O (1 ε √ (k/γ)d log(d) log(1/δ) ) .\nGiven that the α-term gets multiplied by σ1, this bound on ‖GL‖ is of lower order and the corollary follows."
    }, {
      "heading" : "4.2 Principal Component Analysis",
      "text" : "Here we illustrate that our bounds directly imply results for the privacy notion studied by Kapralov and Talwar [KT13]. The notion is particularly relevant in a setting where we think of A as a sum of rank 1 matrices each of bounded spectral norm.\nDefinition 4.4. A randomized algorithm M : Rd×d ′ → R (where R is some arbitrary abstract range) is (ε,δ)-differentially private under unit spectral norm changes if for all pairs of matrices A,A′ ∈ Rd×d′ satisfying ‖A − A′‖2 6 1, we have that for all subsets of the range S ⊆ R, the algorithm satisfies: Pr {M(A) ∈ S} 6 exp(ε)Pr {M(A′) ∈ S}+ δ .\nLemma 4.5. If PPM is executed with each G` sampled independently as G` ∼ N (0,σ2)d×p with σ = ε−1 √ 4pL log(1/δ), then PPM satisfies (ε,δ)-differential privacy under unit spectral norm changes. If G` is sampled with i.i.d. Laplacian entries G` ∼ Lap(0,λ)n×k where λ = 10ε−1pL √ d, then PPM satisfies (ε,0)-differential privacy under unit spectral norm changes.\nProof. The first claim follows from the privacy proof in [HR12]. We sketch the argument here for completeness. LetD be any matrix with ‖D‖2 6 1 (thought of as A−A′ in Definition 4.4) and let ‖x‖ = 1 be any unit vector which we think of as one of the columns of X = X`−1. Then, we have ‖Dx‖ 6 ‖D‖·‖x‖ 6 1, by definition of the spectral norm. This shows that the “`2-sensitivity” of one matrix-vector multiplication in our algorithm is bounded by 1. It is well-known that it suffices to add Gaussian noise scaled to the `2-sensitivity of the matrix-vector product in order to achieve differential privacy. Since there are kL matrix-vector multiplications in total we need to scale the noise by a factor of √ kL.\nThe second claim follows analogously. Here however we need to scale the noise magnitude to the “`1-sensitivity” of the matrix-vector product which be bound by √ n using CauchySchwarz. The claim then follows using standard properties of the Laplacian mechanism.\nGiven the previous lemma it is straightforward to derive the following corollaries.\nCorollary 4.6. Let A ∈ Rd×d be a symmetric matrix with singular values σ1 > . . . > σd and let γ = 1− σk+1/σk . There is an algorithm that given a A and parameter k, preserves (ε,δ)-differentially privacy under unit spectral norm changes and outputs a rank 2k matrix B such that with probability 9/10,\n‖A−B‖ 6 σk+1 + Õ σ1√(k/γ)d logd log(1/δ)ε(σk − σk+1)  . The Õ-notation hides the factor O (√ log(log(d)/γ) ) .\nProof. The proof is analogous to the proof of Corollary 4.3.\nA similar corollary applies to (ε,0)-differential privacy.\nCorollary 4.7. Let A ∈ Rd×d be a symmetric matrix with singular values σ1 > . . . > σd and let γ = 1− σk+1/σk . There is an algorithm that given a A and parameter k, preserves (ε,δ)-differentially privacy under unit spectral norm changes and outputs a rank 2k matrix B such that with probability 9/10,\n‖A−B‖ 6 σk+1 + Õ ( σ1k\n1.5d log(d) log(d/γ) εγ(σk − σk+1)\n) .\nProof. We invoke PPM with p = 2k and Laplacian noise with the scaling given by Lemma 4.5 so that the algorithm satisfies (ε,0)-differential privacy. Specifically, G` ∼ Lap(0,λ)d×p where λ = 10ε−1pL √ d. Lemma A.3. Indeed, with probability 99/100, we have\n1. maxL`=1 ‖G`‖ 6O ( λ √ kd log(kdL) ) =O ( (1/εγ)k1.5d log(d) log(kdL) ) 2. maxL`=1 ‖U >G`‖ 6O (λk log(kL)) =O ( (1/εγ)k2 √ d log(d) log(kL) )\nWe can now plug these error bounds into Corollary 1.1 to obtain the bound∥∥∥(I −XLX>L )U∥∥∥ 6O (k1.5d log(d) log(d/γ)εγ(σk − σk+1) )\nRepeating the argument from the proof of Corollary 4.3 gives the stated guarantee for low-rank approximation.\nThe bound above matches a lower bound shown by Kapralov and Talwar [KT13] up to a factor of Õ( √ k). We believe that this factor can be eliminated from our bounds by using a quantitatively stronger version of Lemma A.3. Compared to the upper bound of [KT13] our algorithm is faster by a more than a quadratic factor in d. Moreover, previously only bounds for (ε,0)-differential privacy were known for the spectral norm privacy notion, whereas our bounds strongly improve when going to (ε,δ)-differential privacy."
    }, {
      "heading" : "4.3 Dimension-free bounds for incoherent matrices",
      "text" : "The guarantee in Theorem 1.3 depends on the quantity ‖X`‖∞ which could in principle be as small as √ 1/d. Yet, in the above theorems, we use the trivial upper bound 1. This in turn resulted in a dependence on the dimensions of A in our theorems. Here, we show that the dependence on the dimension can be replaced by an essentially tight dependence on the coherence of the input matrix. In doing so, we resolve the main open problem left open by Hardt and Roth [HR13]. The definition of coherence that we will use is formally defined as follows.\nDefinition 4.8 (Matrix Coherence). We say that a matrix A ∈Rd×d′ with singular value decomposition A =UΣV > has coherence\nµ(A) def= { d‖U‖2∞,d′‖V ‖2∞ } .\nHere ‖U‖∞ = maxij |Uij | denotes the largest entry of U in absolute value.\nOur goal is to show that the `∞-norm of the vectors arising in PPM is closely related to the coherence of the input matrix. We obtain a nearly tight connection between the coherence of the matrix and the `∞-norm of the vectors that PPM computes.\nTheorem 4.9. Let A ∈Rd×d be symmetric. Suppose NPM is invoked on A, and L 6 n, with each G` sampled from N (0,σ2` ) d×p for some σ` > 0. Then, with probability 1− 1/n,\nL max `=1 ‖X`‖2∞ 6O\n( µ(A) log(d)\nd\n) .\nProof. Fix ` ∈ [L]. Let A = ∑n i=1σiuiu > i be given in its eigendecomposition. Note that\nB = d\nmax i=1 ‖ui‖∞ 6 √ µ(A) d .\nWe may write any column x of X` as x = ∑d i=1 siαiui where αi are non-negative scalars such\nthat ∑d i=1α 2 i = 1, and si ∈ {−1,1} where si = sign(〈x,ui〉). Hence, by Lemma 4.13 (shown below),\nthe signs (s1, . . . , sd) are distributed uniformly at random in {−1,1}d . Hence, by Lemma 4.14 (shown below), it follows that Pr { ‖x‖∞ > 4B √ logd } 6 1/n3 . By a union bound over all p 6 d\ncolumns it follows that Pr { ‖X`‖∞ > 4B √ logd } 6 1/d2 . Another union bound over all L 6 d steps completes the proof.\nThe previous theorem states that no matter what the scaling of the Gaussian noise is in each step of the algorithm, so long as it is Gaussian the algorithm will maintain that X` has small coordinates. We cannot hope to have coordinates smaller than √ µ(A)/d, since eventually the algorithm will ideally converge to U. This result directly implies the theorem we stated in the introduction.\nProof of Theorem 1.4. The claim follows directly from Theorem 1.3 after applying Theorem 4.9 which shows that with probability 1− 1/n,\nL max `=1 ‖X`‖2∞ 6O\n( µ(A) log(d)\nd\n) ."
    }, {
      "heading" : "4.4 Proofs of supporting lemmas",
      "text" : "We will now establish Lemma 4.13 and Lemma 4.14 that were needed in the proof of the previous theorem. For that purpose we need some basic symmetry properties of the QR-factorization. To establish these properties we recall the Gram-Schmidt algorithm for computing the QRfactorization.\nDefinition 4.10 (Gram-Schmidt). The Gram-Schmidt orthonormalization algorithm, denoted GS, is given an input matrix V ∈ Rd×p with columns v1, . . . , vp and outputs an orthonormal matrixQ ∈Rd×p with the same range as V . The columns q1, . . . , qp ofQ are computed as follows: For i = 1 to p do:\n– rii ← ‖vi‖ – qi ← vi/rii – For j = i + 1 to p do:\n– rij ← 〈qi ,vj〉 – vj ← vj − rijqi\nThe first states that the Gram-Schmidt operation commutes with an orthonormal transformation of the input.\nLemma 4.11. Let V ∈ Rd×p and let O ∈ Rd×d be an orthonormal matrix. Then, GS(OV ) = O ×GS(V ).\nProof. Let {rij}ij∈[p] denote the scalars computed by the Gram-Schmidt algorithm as specified in Definition 4.10. Notice that each of the numbers {rij}ij∈[p] is invariant under an orthonormal transformation of the vectors v1, . . . , vp. This is because ‖Ovi‖ = ‖vi‖ and 〈Ovi ,Ovj〉 = 〈vi ,vj〉. Moreover, The output Q of Gram-Schmidt on input of V satisfies Q = VR, where R is an upper right triangular matrix which only depends on the numbers {rij}i,j∈[p]. Hence, the matrix R is identical when the input is OV . Thus, GS(OV ) =OVR =O ×GS(V ).\nGiven i.i.d. Gaussian matrices G0,G1, . . . ,GL ∼N (0,1)d×p, we can describe the behavior of our algorithm by a deterministic function f (G0,G1, . . . ,GL) which executes subspace iteration starting with G0 and then suitably scales G` in each step. The next lemma shows that this function is distributive with respect to orthonormal transformations.\nLemma 4.12. Let f : (Rd×p)L→Rn×p denote the output of PPM on input of a matrix A ∈Rn×n as a function of the noise matrices used by the algorithm as described above. Let O be an orthonormal matrix with the same eigenbasis as A. Then,\nf (OG0,OG1, . . . ,OGL) =O × f (G0, . . . ,GL) . (4)\nProof. For ease of notation we will denote by X0, . . . ,XL the iterates of the algorithm when the noise matrices are G0, . . . ,GL, and we denote by Y0, . . . ,YL the iterates of the algorithm when the noise matrices are OG0, . . . ,OGL. In this notation, our goal is to show that YL =OXL.\nWe will prove the claim by induction on L. For L = 0, the base case follows from Lemma 4.11. Indeed,\nY0 = GS(OG0) =O ×GS(G0) =OX0 .\nLet ` > 1. We assume the claim holds for ` − 1 and show that it holds for `. We have,\nY` = GS(AY`−1 +OG`)\n= GS(AOX`−1 +OG`) (by induction hypothesis)\n= GS(O(AX`−1 +G`)) (A and O commute) =O ×GS(AX`−1 +G`) (Lemma 4.11) =OX` .\nNote that A and O commute, since they share the same eigenbasis by the assumption of the lemma. This is what we needed to prove.\nThe previous lemmas lead to the following result characterizing the distribution of signs of inner products between the columns of X` and the eigenvectors of A.\nLemma 4.13 (Sign Symmetry). Let A be a symmetric matrix given in its eigendecomposition as A = ∑d i=1λiuiu > i . Let ` > 0 and let x be any column of X`, where X` is the iterate of PPM on input of A. Put Si = sign(〈ui ,x〉) for i ∈ [d]. Then (S1, . . . ,Sd) is uniformly distributed in {−1,1}d .\nProof. Let (z1, . . . , zd) ∈ {−1,1}d be a uniformly random sign vector. Let O = ∑d i=1 ziuiu > i . Note that O is an orthonormal transformation. Clearly, any column Ox of OX` satisfies the conclusion of the lemma, since 〈ui ,Ox〉 = zi〈ui ,x〉. Since the Gaussian distribution is rotationally invariant, we have that OG` and G` follow the same distribution. In particular, denoting by Y` the matrix computed by the algorithm if OG0, . . . ,OG` were chosen, we have that Y` and X` are identically distributed. Finally, by Lemma 4.12, we have that Y` =OX`. By our previous observation this means that Y` satisfies the conclusion of the lemma. As Y` and X` are identically distributed, the claim also holds for X`.\nWe will use the previous lemma to bound the `∞-norm of the intermediate matrices X` arising in power iteration in terms of the coherence of the input matrix. We need the following large deviation bound.\nLemma 4.14. Let α1, . . . ,αd be scalars such that ∑d i=1α 2 i = 1 and u1, . . . ,ud are unit vectors in R n. Put B = maxdi=1 ‖ui‖∞. Further let (s1, . . . , sd) be chosen uniformly at random in {−1,1} d . Then,\nPr  ∥∥∥∥∥∥∥ d∑ i=1 siαiui ∥∥∥∥∥∥∥ ∞ > 4B √ logd  6 1/d3 . Proof. Let X = ∑d i=1Xi where Xi = siαiui . We will bound the deviation of X in each entry and\nthen take a union bound over all entries. Consider Z = ∑d i=1Zi where Zi is the first entry of Xi .\nThe argument is identical for all other entries of X. We have EZ = 0 and EZ2 = ∑d i=1EZ 2 i 6\nB2 ∑d i=1α 2 i = B 2. Hence, by Theorem A.1 (Chernoff bound),\nPr { |Z | > 4B √ log(d) } 6 exp ( −16B\n2 log(d) 4B2\n) 6 exp(−4log(d)) = 1\nd4 .\nThe claim follows by taking a union bound over all d entries of X."
    }, {
      "heading" : "A Deferred Concentration Inequalities",
      "text" : "Theorem A.1 (Chernoff bound). Let the random variables X1, . . . ,Xm be independent random variables such that for every i, Xi ∈ [−1,1] almost surely. Let X = ∑m i=1Xi and let σ\n2 = V X. Then, for any t > 0, Pr {|X −EX | > t} 6 exp ( − t24σ2 ) .\nThe next lemma follows from standard concentration properties of the Gaussian distribution.\nLemma A.2. Let U ∈ Rd×k be a matrix with orthonormal columns. Let G1, . . . ,GL ∼ N (0,σ2)d×p with k 6 p 6 d and assume that L 6 d. Then, with probability 1− 10−4,\nmax `∈[L] ‖U>G`‖ 6O\n( σ √ p+ logL ) .\nProof. By rotational invariance of G` the spectral norm ‖U>G`‖ is distributed like largest singular value of a random draw from k×p gaussian matrix N(0,σ2)k×p. Since p > k, the largest singular value strongly concentrates around O(σ √ p) with a gaussian tail. By the gaussian concentration of Lipschitz functions of gaussians, taking the maximum over L gaussian matrices introduces an additive O(σ √ logL) term.\nWe also have an analogue of the previous lemma for the Laplacian distribution.\nLemma A.3. Let U ∈Rn×k be a matrix with orthonormal columns. Let G1, . . . ,GL ∼ Lap(0,λ)d×p with k 6 p 6 d and assume that L 6 d. Then, with probability 1− 10−4,\nmax `∈[L] ‖U>G`‖ 6O\n( λ √ pk log(Lpk) ) .\nProof. We claim that with probability 1−10−4 for every ` ∈ [L], every entry ofU>G` is bounded by O(λ log(Lpk)) in absolute value. This follows because each entry has variance λ2 and is a weighted sum of n independent Laplacian random variables Lap(0,λ). Assuming this event occurs, we have\nmax `∈[L] ‖U>G`‖ 6max `∈[L] ‖U>G`‖F 6O\n( λ √ pk log(Lpk) ) .\nLemma A.4 (Matrix Chernoff). Let X1, . . . ,Xn ∼ X be i.i.d. random matrices of maximum dimension d and mean µ, uniformly bounded by ‖X‖ 6 R. Then for all t 6 1,\nPr {∥∥∥1n∑iXi −EX1∥∥∥ > tR} 6 de−Ω(mt2)"
    }, {
      "heading" : "B Reduction to symmetric matrices",
      "text" : "For all our purposes it suffices to consider symmetric n×n matrices. Given a non-symmetric m × n matrix B we may always consider the (m + n) × (m + n) matrix A = [0B |B>0]. This transformation preserves all the parameters that we are interested in as was argued in [HR13] more formally. This allows us to discuss symmetric eigendecompositions rather than singular vector decompositions and therefore simplify our presentation below."
    } ],
    "references" : [ {
      "title" : "Stochastic optimization for pca and pls",
      "author" : [ "Raman Arora", "Andrew Cotter", "Karen Livescu", "Nathan Srebro" ],
      "venue" : "In Communication, Control, and Computing (Allerton),",
      "citeRegEx" : "Arora et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "The JohnsonLindenstrauss transform itself preserves differential privacy",
      "author" : [ "Jeremiah Blocki", "Avrim Blum", "Anupam Datta", "Or Sheffet" ],
      "venue" : "In Proc. 53rd Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Blocki et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Blocki et al\\.",
      "year" : 2012
    }, {
      "title" : "The fast convergence of incremental PCA",
      "author" : [ "Akshay Balsubramani", "Sanjoy Dasgupta", "Yoav Freund" ],
      "venue" : "In Proc. 27th Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Balsubramani et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Balsubramani et al\\.",
      "year" : 2013
    }, {
      "title" : "Practical privacy: the SuLQ framework",
      "author" : [ "Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim" ],
      "venue" : "In Proc. 24th PODS,",
      "citeRegEx" : "Blum et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Blum et al\\.",
      "year" : 2005
    }, {
      "title" : "Robust principal component analysis",
      "author" : [ "Emmanuel J. Candès", "Xiaodong Li", "Yi Ma", "John Wright" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Candès et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Candès et al\\.",
      "year" : 2011
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel J. Candès", "Benjamin Recht" ],
      "venue" : "Foundations of Computional Mathematics,",
      "citeRegEx" : "Candès and Recht.,? \\Q2009\\E",
      "shortCiteRegEx" : "Candès and Recht.",
      "year" : 2009
    }, {
      "title" : "Near-optimal differentially private principal components",
      "author" : [ "Kamalika Chaudhuri", "Anand Sarwate", "Kaushik Sinha" ],
      "venue" : "In Proc. 26th Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2012
    }, {
      "title" : "The power of convex relaxation: nearoptimal matrix completion",
      "author" : [ "Emmanuel J. Candès", "Terence Tao" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Candès and Tao.,? \\Q2010\\E",
      "shortCiteRegEx" : "Candès and Tao.",
      "year" : 2010
    }, {
      "title" : "The rotation of eigenvectors by a perturbation",
      "author" : [ "Chandler Davis", "W.M. Kahan" ],
      "venue" : "iii. SIAM J. Numer. Anal.,",
      "citeRegEx" : "Davis and Kahan.,? \\Q1970\\E",
      "shortCiteRegEx" : "Davis and Kahan.",
      "year" : 1970
    }, {
      "title" : "Analyze Gauss: optimal bounds for privacy-preserving principal component analysis",
      "author" : [ "Cynthia Dwork", "Kunal Talwar", "Abhradeep Thakurta", "Li Zhang" ],
      "venue" : "In Proc. 46th Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Dwork et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2014
    }, {
      "title" : "Understanding alternating minimization for matrix completion",
      "author" : [ "Moritz Hardt" ],
      "venue" : "In Proc. 55th Foundations of Computer Science (FOCS). IEEE,",
      "citeRegEx" : "Hardt.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hardt.",
      "year" : 2014
    }, {
      "title" : "Accuracy and Stability of Numerical Algorithms",
      "author" : [ "Nicholas J. Higham" ],
      "venue" : "Society for Industrial and Applied Mathematics,",
      "citeRegEx" : "Higham.,? \\Q2002\\E",
      "shortCiteRegEx" : "Higham.",
      "year" : 2002
    }, {
      "title" : "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "author" : [ "Nathan Halko", "Per-Gunnar Martinsson", "Joel A. Tropp" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Halko et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Halko et al\\.",
      "year" : 2011
    }, {
      "title" : "Beating randomized response on incoherent matrices",
      "author" : [ "Moritz Hardt", "Aaron Roth" ],
      "venue" : "In Proc. 44th Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Hardt and Roth.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hardt and Roth.",
      "year" : 2012
    }, {
      "title" : "Beyond worst-case analysis in private singular vector computation",
      "author" : [ "Moritz Hardt", "Aaron Roth" ],
      "venue" : "In Proc. 45th Symposium on Theory of Computing (STOC). ACM,",
      "citeRegEx" : "Hardt and Roth.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hardt and Roth.",
      "year" : 2013
    }, {
      "title" : "Low-rank matrix completion using alternating minimization",
      "author" : [ "Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi" ],
      "venue" : "In Proc. 45th Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Jain et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2013
    }, {
      "title" : "On differentially private low rank approximation",
      "author" : [ "Michael Kapralov", "Kunal Talwar" ],
      "venue" : "In Proc. 24rd Symposium on Discrete Algorithms (SODA). ACM-SIAM,",
      "citeRegEx" : "Kapralov and Talwar.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kapralov and Talwar.",
      "year" : 2013
    }, {
      "title" : "Memory limited, streaming PCA",
      "author" : [ "Ioannis Mitliagkas", "Constantine Caramanis", "Prateek Jain" ],
      "venue" : "In Proc. 27th Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Mitliagkas et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mitliagkas et al\\.",
      "year" : 2013
    }, {
      "title" : "Differentially private recommender systems: building privacy into the net",
      "author" : [ "Frank McSherry", "Ilya Mironov" ],
      "venue" : "In Proc. 15th KDD,",
      "citeRegEx" : "McSherry and Mironov.,? \\Q2009\\E",
      "shortCiteRegEx" : "McSherry and Mironov.",
      "year" : 2009
    }, {
      "title" : "Smallest singular value of a random rectangular matrix",
      "author" : [ "Mark Rudelson", "Roman Vershynin" ],
      "venue" : "Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "Rudelson and Vershynin.,? \\Q2009\\E",
      "shortCiteRegEx" : "Rudelson and Vershynin.",
      "year" : 2009
    }, {
      "title" : "Recent computational developments in krylov subspace methods for linear systems",
      "author" : [ "Valeria Simoncini", "Daniel B. Szyld" ],
      "venue" : "Numerical Linear Algebra With Applications,",
      "citeRegEx" : "Simoncini and Szyld.,? \\Q2007\\E",
      "shortCiteRegEx" : "Simoncini and Szyld.",
      "year" : 2007
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "We provide a new robust convergence analysis of the well-known power method for computing the dominant singular vectors of a matrix that we call the noisy power method. Our result characterizes the convergence behavior of the algorithm when a significant amount noise is introduced after each matrix-vector multiplication. The noisy power method can be seen as a meta-algorithm that has recently found a number of important applications in a broad range of machine learning problems including alternating minimization for matrix completion, streaming principal component analysis (PCA), and privacy-preserving spectral analysis. Our general analysis subsumes several existing adhoc convergence bounds and resolves a number of open problems in multiple applications: Streaming PCA. A recent work of Mitliagkas et al. (NIPS 2013) gives a space-efficient algorithm for PCA in a streaming model where samples are drawn from a gaussian spiked covariance model. We give a simpler and more general analysis that applies to arbitrary distributions confirming experimental evidence of Mitliagkas et al. Moreover, even in the spiked covariance model our result gives quantitative improvements in a natural parameter regime. It is also notably simpler and follows easily from our general convergence analysis of the noisy power method together with a matrix Chernoff bound. Private PCA. We provide the first nearly-linear time algorithm for the problem of differentially private principal component analysis that achieves nearly tight worst-case error bounds. Complementing our worst-case bounds, we show that the error dependence of our algorithm on the matrix dimension can be replaced by an essentially tight dependence on the coherence of the matrix. This result resolves the main problem left open by Hardt and Roth (STOC 2013). The coherence is always bounded by the matrix dimension but often substantially smaller thus leading to strong average-case improvements over the optimal worst-case bound.",
    "creator" : "LaTeX with hyperref package"
  }
}