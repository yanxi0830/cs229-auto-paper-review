{
  "name" : "1406.1822.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Logarithmic Time Online Multiclass prediction",
    "authors" : [ "Anna Choromanska", "John Langford" ],
    "emails" : [ "aec2163@columbia.edu", "jcl@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The central problem of this paper is computational complexity in a setting where the number of classes k for multiclass prediction is very large. Such problems occur in natural language (Which translation is best?), search (What result is best?), and detection (Who is that?) tasks. Almost all machine learning algorithms (with the notable exception of decision trees) have running times for multiclass classification which are O(k) with a canonical example being one-against-all classifiers [1]. In this setting, the most efficient imaginable approach has a running time of O(log k) per example for both training and testing, while effectively using online learning algorithms to minimize passes over the data. Our central goal is improving the quality of most-efficient solutions so as to grow the frontier of tasks solvable via machine learning.\nThe goal of logarithmic (in k) complexity naturally motivates approaches that construct a logarithmic depth hierarchy over the labels, with one label per leaf. While this hierarchy is sometimes available through prior knowledge, in many scenarios it needs to be learned as well. This naturally leads to a partition problem which arises at each node in the hierarchy. The partition problem is finding a classifier: c : X → {−1, 1} which divides examples into two subsets with a purer set of labels than the original set. Definitions of purity vary, but canonical examples are the number of labels remaining in each subset, or softer notions such as the average Shannon entropy of the class labels. Despite resulting in a classifier, this problem is fundamentally different from standard binary classification. To see this, note that replacing c(x) with −c(x) is very bad for binary classification, but has no impact on the quality of a partition1. The partition problem (just like most natural clustering objectives) is fundamentally non-convex for symmetric classes since the average c(x)−c(x)2 of c(x) and −c(x) creates a very poor partition.\n∗A part of this work was done when A.C. was at Microsoft Research New York. 1The problem bears parallels to clustering in this regard.\nar X\niv :1\n40 6.\n18 22\nv1 [\ncs .L\nG ]\n6 J\nun 2\n01 4\nThe choice of partition matters in problem dependent ways. For example, consider examples on a line with label i at position i and threshold classifiers. In this case, trying to partition class labels {1, 3} from class label 2 results in poor performance.\nThe partition problem is typically solved for decision tree learning via an enumerate-and-test approach amongst a small set of possible classifiers. In the multiclass setting, it is desirable to achieve substantial error reduction for each node in the tree which motivates using a richer set of classifiers in the nodes to minimize the number of nodes, and thereby decrease the computational complexity. The main theoretical contribution of this work is to establish a boosting algorithm for learning trees withO(k) nodes andO(log k) depth, thereby addressing the goal of logarithmic time train and test complexity. In Section 2.3 this result generalizes a binary boosting-by-decision-tree theorem [2] to\nmulticlass boosting. As in all boosting results, performance is critically dependent on the quality of the weak learner, supporting intuition that we need sufficiently rich partitioners at nodes. The approach uses a new objective for decision tree learning, which we optimize at each node of the tree. The objective and its theoretical properties are presented in Section 2.\nA complete system with multiple partitions could be constructed top down (as the boosting theorem) or bottom up (as Filter Tree [3]). A bottom up partition process appears impossible with representational constraints as shown in appendix section 6 so we focus on top-down tree creation.\nWhenever there are representational constraints on partitions (such as linear classifiers), finding a strong partition function requires an efficient search over this set of classifiers. Efficient searches over large function classes are routinely performed via gradient descent techniques for supervised learning, so they seem like a natural candidate. In existing literature, examples for doing this exist when the problem is indeed binary, or when there is a pre-specified hierarchy over the labels and we just need to find partitioners aligned with that hierarchy. Neither of these cases applies—we have multiple labels and want to dynamically create the choice of partition, rather than assuming that one was handed to us. Does there exist a purity criterion amenable to a gradient descent approach? The precise objective studied in theory fails this test due to its discrete nature, and even natural approximations are challenging to tractably optimize under computational constraints. As a result, we use the theoretical objective as a motivation and construct a new Logarithmic Online Multiclass Tree (LOMTree) algorithm for empirical evaluation.\nCreating a tree in an online fashion creates a new class of problems. What if some node is initially created but eventually proves useless because no examples go to it? At best this results in a wasteful solution, while in practice it starves other parts of the tree which need representational complexity. To deal with this, we design an efficient process for recycling orphan nodes into locations where they are needed, and prove that the number of times a node is recycled is at most logarithmic in the number of examples. The algorithm is described in Section 3 and we analyze the swapping bound in Section 3.1.\nAnd is it effective? Given the inherent non-convexity of the partition problem this is unavoidably an empirical question which we answer on a range of datasets varying from 26 to 105K classes in Section 4. We find that under constrained training times, this approach is quite effective compared to all baselines while dominating other O(log k) train time approaches.\nWhat’s new? To the best of our knowledge, the splitting criterion, the boosting statement, the LOMTree algorithm, the swapping guarantee, and the experimental results are all new here."
    }, {
      "heading" : "1.1 Prior Work",
      "text" : "Only a few authors have previously addressed logarithmic time training. The Filter Tree [3] addresses consistent (and robust) multiclass classification, showing that it is possible in the statistical limit. The Filter Tree does not address the partition problem as we do here. In our experiments we compare with the Filter Tree empirically and find that addressing the partition problem is often helpful. The partition finding problem is addressed in the conditional probability tree [4], but that paper addresses conditional probability estimation. Conditional probability estimation can be converted into multiclass prediction, but doing so is not a logarithmic time operation.\nQuite a few authors have addressed logarithmic testing time while allowing training time to be O(k) or worse. While these approaches are intractable on our larger scale problems, we describe them here for context. The partition problem can be addressed by recursively applying spectral clustering on a confusion graph [5]. Empirically, this approach has been found to sometimes lead to badly imbalanced splits [6]. In the context of ranking, another approach uses k-means hierarchical clustering to recover the label sets for a given partition [7].\nThe more recent work [8] on the multiclass classification problem addresses it via sparse output coding by tuning high-cardinality multiclass categorization into a bit-by-bit decoding problem, where for the sake of scalability to large-scale problems, the authors decouple the learning processes of coding matrix and bit predictors and use probabilistic decoding to decode the optimal class label.\nDecision trees are naturally structured to allow logarithmic time prediction. Traditional decision trees often have difficulties with a large number of classes because their splitting criteria are not wellsuited to the large class setting. However, newer approaches [9] have addressed this effectively at significant scales in the context of multilabel classification (multilabel learning, with missing labels, is also addressed in [10] where the authors focus on the empirical risk minimization framework with a low-rank constraint). Additionally, a well-known problem with hierarchical classification is that the performance significantly deteriorates lower in the hierarchy [11] which some authors solve by biasing the training distribution to reduce error propagation while simultaneously combining bottom-up and top-down approaches during training [12]."
    }, {
      "heading" : "2 Framework and theoretical analysis",
      "text" : "In this section we describe the essential elements of the approach, and outline the theoretical properties of the resulting framework. We begin by presenting the high-level ideas."
    }, {
      "heading" : "2.1 Setting",
      "text" : "We employ a hierarchical approach for learning a multiclass decision tree structure, training this structure in a top-down fashion. We assume that we receive examples x ∈ X ⊆ Rd, with labels y ∈ {1, 2, . . . , k}. We also assume access to a hypothesis class H where each h ∈ H is a binary classifier, h : X 7→ {−1, 1}. The overall objective is to learn a tree of depth O(log k), where each node in the tree consists of a classifier h ∈ H. The classifiers are trained in such a way that hn(x) = 1 means that the example x is sent to the right subtree of node n, while h(x) = −1 sends x to the left subtree. When we reach a leaf node, we just predict according to the label with the highest frequency amongst the examples reaching that leaf.\nIn the interest of computational complexity, we want to encourage the number of examples going to the left and right to be fairly balanced. For good statistical accuracy, we want to send examples of class i almost exclusively to either the left or the right subtree, thereby refining the purity of the class distributions at subsequent levels in the tree. An objective balancing these competing criteria, and resulting theoretical properties is illustrated in the following sections. A key consideration in picking this objective is that we want to effectively optimize it over hypotheses h ∈ H, while streaming over examples in an online fashion. This seems unsuitable with some of the more standard decision tree objectives such as Shannon or Gini entropy, which leads us to design a new objective. At the same time, we show in Section 2.3 that under suitable assumptions, optimizing the objective also leads to effective reduction of the average Shannon entropy over the entire tree."
    }, {
      "heading" : "2.2 An objective and analysis of resulting partitions",
      "text" : "We now define a criterion to measure the quality of a hypothesis h ∈ H in creating partitions at a fixed node n in the tree. Let πi denote the proportion of label i amongst the examples reaching this node. Let P (h(x) > 0) and P (h(x) > 0|i) denote the fraction of examples reaching n for which h(x) > 0, marginally and conditional on class i respectively. Then we define the objective:\nJ(h) = 2 k∑ i=1 πi |P (h(x) > 0)− P (h(x) > 0|i)| , (1)\nwhere we aim to maximize the objective J(h) to obtain high quality partitions. Intuitively, the objective encourages the fraction of examples going to the left from class i to be substantially different from the background fraction for each class i. As a concrete simple scenario, if P (h(x) > 0) = 0.5 for some hypothesis h, then the objective prefers P (h(x) > 0|i) to be as close to 0 or 1 as possible for each class i, leading to pure partitions. We now make these intuitions more formal. Definition 1 (Purity). The hypothesis h ∈ H induces a pure split if\nα := k∑ i=1 πi min(Pr(h(x) > 0|i), P r(h(x) < 0|i)) ≤ δ,\nwhere δ ∈ [0, 0.5), and α is called the purity factor.\nIn particular, a partition is called maximally pure if α = 0, meaning that each class is sent exclusively to the left or the right. It is easily seen that for a hypothesis h which induces maximally pure splits, J(h) = 2P (h(x) > 0). We now define a similar definition for the balancedness of a split. Definition 2 (Balancedness). The hypothesis h ∈ H induces a balanced split if\nc ≤ Pr(h(x) > 0)︸ ︷︷ ︸ =β ≤ 1− c,\nwhere c ∈ [0, 0.5), and β is called the balancing factor.\nA partition is called maximally balanced if β = 0.5, meaning that an equal number of examples are sent to the left and right children of the partition. The balancing factor and the purity factor are related as shown in Lemma 1 (most of the proofs are deferred to the Supplementary material). Lemma 1. For any hypothesis h, and any distribution over examples (x, y), the purity factor α and the balancing factor β satisfy\nα ≤ min {\n2− J(h) 4β\n− β, 0.5 } . (2)\nA partition is called maximally pure and balanced if it satisfies both α = 0 and β = 0.5. We see that J(h) = 1 for a hypothesis h inducing a maximally pure and balanced partition as captured in the next lemma. Of course we do not expect to have hypotheses producing maximally pure and balanced splits in practice. Lemma 2. For any hypothesis h : X 7→ {−1, 1}, the objective J(h) satisfies J(h) ∈ [0, 1]. Furthermore, if h induces a maximally pure and balanced partition then J(h) = 1 ."
    }, {
      "heading" : "2.3 Quality of the entire tree",
      "text" : "The above section helps us understand the quality of an individual split produced by effectively maximizing J(h). However, in a hierarchical setting, we must reason about the quality of the entire tree as we add more and more nodes. Does each new node ensure sufficient improvement in the statistical quality? Can we obtain high quality predictions with shallow trees? While this may not hold in general, we adopt a boosting-style approach to this question here. We measure the quality of trees using the average entropy over all the leaves in the tree, and track the decrease of this entropy as a function of the number of nodes. In doing so, we borrow from the theoretical analysis of decision\ntree algorithms in Kearns and Mansour [2] originally developed to show the boosting properties of the decision trees for binary classification problems. We generalize their theoretical analysis to the multiclass classification setting.\nGiven a tree T , we consider the entropy function G as the measure of the quality of tree:\nG(T ) = ∑ n∈L wn k∑ i=1 −πni ln(πni)\nwhere πni’s are the probabilities that a randomly chosen x drawn from the underlying target distribution P has label i given that x reaches leaf n and wn is the weight of leaf n defined as the probability of randomly chosen x drawn from P to reach leaf n (note that ∑ n∈L wn = 1).\nThe analysis studies a tree construction algorithm where we recursively find the leaf node with the highest weight, and choose to split it into two children. Consider the tree T constructed over t steps. Since each step takes one leaf node and splits it into two, we have a total of t leaf nodes after t steps. Let n be the heaviest node at time t and its weight wn be denoted by w for brevity. Consider splitting this leaf to two children n0 and n1. For the ease of notation let w0 = wn0 and w1 = wn1 . Also for the ease of notation let β = P (hn(x) > 0) and Pi = P (hn(x) > 0|i). Let πi be the shorthand for πni. Recall that β = ∑k i=1 πiPi and ∑k i=1 πi = 1. Also notice that w0 = w(1 − β) and w1 = wβ. Let π be the k-element vector with ith entrance equal to πi. Furthermore let G(π) = ∑k i=1−πi ln(πi).\nLetGt be a shorthand forG(T ), the entropy of tree T with t leaves. Before the split the contribution of node n to Gt was wG(π1, π2, . . . , πk). Let πi(n0) = πi(1−Pi) 1−β and πi(n1) = πiPi β be the probabilities that a randomly chosen x drawn from P has label i given that x reaches nodes n0 and n1 respectively. Furthermore let π(n0) be the k-element vector with ith entry equal to πi(n0) and let π(n1) be the k-element vector with ith entry equal to πi(n1). Notice that π = (1 − β)π(n0) + βπ(n1). After the split the contribution of the same, now internal, node n changes to w((1− β)G(π(n0)) + βG(π(n1)). We denote the difference between them as ∆t and thus\n∆t := Gt+1 −Gt = w [G(π)− (1− β)G(π(n0))− βG(π(n1))] . (3) We aim to lower-bound ∆t. The entropy reduction of Equation 3 [2] corresponds to a gap in the Jensen’s inequality applied to the concave function G(π). One can show that ∆t ≥ J 2Gt 8β(1−β)t ln k , thus the larger the objective J(h) is at time t, the larger the entropy reduction ends up being, which further reinforces intuitions to maximize J . In general, it might not be possible to find any hypothesis with a large enough objective J(h) to guarantee sufficient progress at this point so we appeal to a weak learning assumption described next.\nThe assumption posits that each non-leaf node of the tree T has a hypothesis h in its hypothesis class H which guarantees a “weak” purity of the split on any distribution P over X . Under this assumption, one can use the new decision tree approach to drive the error below any threshold. Definition 3 (Weak Hypothesis Assumption). Let γ ∈ (0,min(βn, 1−βn)]. Let for any distribution P over X at each non-leaf node n of the tree T there exists a hypothesis h ∈ H such that J(h)/2 =∑k i=1 πi|Pni − βn| ≥ γ.\nThe weak hypothesis assumption can be used to further lower-bound ∆t as ∆t > η 2Gt 16t which directly leads to Theorem 1. Theorem 1. Under the Weak Hypothesis Assumption, for any α ∈ [0, 1], to obtainGt ≤ α it suffices to make\nt ≥ ( 1\nα\n) 4(1−γ)2 ln k γ2\nsplits."
    }, {
      "heading" : "3 The LOMTree Algorithm",
      "text" : "The objective function of Section 2 has another convenient equivalent form which yields a simple online algorithm for tree construction and training. Notice that the objective function defined in\nEquation 1 can be equivalently written as\nJ(h) = 2|Ex[1(h(x) > 0)]− Ex,i[1(h(x) > 0|i)]|\nMaximizing this objective is a discrete optimization problem that can be relaxed, by dropping the indicator operator, to the following form\nJ(h) = 2|Ex[h(x)]− Ex,i[h(x)|i]|,\nwhere Ex,i[h(x)|i] is the expected signed margin of class i. The empirical estimates of the expectations can be easily stored and updated online in every node of the tree. The decision whether to send an example x reaching a node to its left or right child node is based on the sign of the difference between the two expectations: Ex[h(x)] and Ex,y[h(x)|y] where y is a label of the the data point x. During training, the algorithm assigns a unique label to each node of the tree which is currently a leaf. This is the label with the highest frequency amongst the examples reaching that leaf. While testing, a test example is pushed down the tree along the path starting from the root and ending at the leaf, where in each non-leaf node of the path its regressor directs the example either to the left or right child node. The test example is then labeled with the label assigned to the leaf that this example descended to.\nAlgorithm 1 Online tree training Input: regression algorithm R, max number of tree non-leaf nodes T , swap resistance RS Subroutine SetupNode (v)\nmv = zeros(k, 1) (sum of the margins per class) lv = zeros(k, 1) (# of data points per class reaching v) nv = zeros(k, 1) (# of data points which are used to train regressor in v) ev = zeros(k, 1) (expected margin per class) Cv = 0 (the size of the smallest leaf in the subtree with root v)\nSubroutine UpdateC (v) While (v 6= r AND CPARENT(v) 6= Cv) {v = PARENT(v); Cv = min(CLEFT(v), CRIGHT(v))}\nSubroutine Swap (v) Find a leaf s for which (Cs = Cr) sPA = PARENT(s), sGPA = GRANDPARENT(s), sSIB = SIBLING(s) If (sPA = LEFT(sGPA)) {LEFT(sGPA) = sSIB} Else {RIGHT(sGPA) = sSIB} UpdateC (sSIB); SetupNode (s); LEFT(v) = s; SetupNode (sPA); RIGHT(v) = sPA Create the root node r = 0: SetupNode (r); N = 1 For each example (x, y) do {\nSet j = r While j is not a leaf do { If (∑k 1=1 m v(y)∑k\ni=1 n v(y)\n> ej(y) ) {c = −1} Else {c = 1}\nTrain f j with example (x, c) lj(y)++; mjc(y) += f j(x); njc(y) ++; e j c(y) = m j c(y)/n j c(y) Set j to the child of j corresponding to f j If (j is a leaf) {\nlj(y)++ If (lj has at least two non-zero entries\nAND (N < T OR Cj −maxi=1,2,...,k lj(i) > RS(Cr + 1)) { If (N < T ) {SetupNode (LEFT(j)); SetupNode (RIGHT(j)); N++} Else {Swap(j)} CLEFT(j) = ⌊ Cj/2 ⌋ ; CRIGHT(j) = Cj − CLEFT(j)\nUpdateC (LEFT(j)) } } Cj++\nThe training phase, while the tree is being constructed and trained, is captured in Algorithm 1. The stopping criterion for expanding the tree is when the number of non-leaf nodes of the tree reaches the pre-specified threshold T ."
    }, {
      "heading" : "3.1 Swapping",
      "text" : "Consider a scenario where the current training example descends to leaf j. The leaf can split (create two children) if the examples that reached it in the past were coming from at least two different classes. However, if the number of non-leaf nodes of the tree reaches threshold T , no more nodes can be expanded and thus j cannot create children. Since the tree construction is done online, some nodes created at early stages of training may end up useless because no examples reach them later on. This prevents potentially useful splits such as at leaf j. This problem can be solved by recycling orphan nodes (subroutine Swap in Algorithm 1). The general idea behind node recycling is to allow nodes to split if a certain condition is met. In particular, node j splits if the following holds:\nCj − max i=1,2,...,k lj(i) > RS(C r + 1), (4)\nwhere Cj is the size of the smallest leaf in the subtree with root j (r denotes the root of the entire tree), where the smallest leaf is the one with the smallest total number of data points that reached it in the past, lj is a k-dimensional vector of non-negative integers where the ith entrance is the count of the number of data points reaching leaf j in the past, and finally RS is the pre-specified input parameter that we call ’swap resistance’. RS should be set to at least 4. The subtraction of maxi=1,2,...,k l\nj(i) in the above condition ensures that a pure node will not be recycled. It turns out that the condition captured in Inequality 4 allows us to prove (in Section 3.1) that the number of times a node is recycled is upper-bounded by the logarithm of the number of examples. If the condition is satisfied, the swap of the nodes is performed where an orphan leaf s, which was reached by the smallest number of examples in the past, and its parent sPA are detached from the tree and become children of node j whereas the old sibling sSIB of an orphan node s becomes a direct child of the old grandparent sGPA of s. The swapping procedure is illustrated in Figure 2.\nLemma 3. The number of times Algorithm 1 recycles a node is upper-bounded by the logarithm of the number of data points.\nHere we bound the number of swaps that any node makes. Consider RS = 4. As before, let j be the node that is about to split and s be the orphan node that will be recycled (thus Cr = Cs). The condition in Equation 4 implies that the swap is done if Cj > 4(Cr + 1) = 4(Cs + 1). Algorithm 1 makes s a child of j during the swap and sets its counter to Csnew = ⌊ Cj/2 ⌋ ≥ 2(Cr + 1) = 2(Cs + 1). The algorithm then updates Cr. Since the value of Csnew at least doubles after a swap and the maximum value of any counter is at most the number of examples, the node can be involved in at most log2 n swaps where n is the number of examples."
    }, {
      "heading" : "4 Experiments",
      "text" : "We conducted experiments on a variety of benchmark multiclass datasets: Isolet, Sector, Aloi, ImageNet (ImNet) and ODP2. The details of the datasets are provided in Table 1. The datasets were divided into training (90% and testing 10%). Furthermore, 10% of the training dataset was used as a validation set. We compared LOMtree with a balanced random tree of logarithmic depth (Rtree), Filter tree [13] and a one-against-all classifier (OAA). All methods were implemented in the open source learning system Vowpal Wabbit [14] and have similar levels of optimization. The regressors in the\n2The details of the source of each dataset are provided in the Supplementary material.\ntree nodes for LOMtree, Rtree and Filter tree as well as the OAA regressors were trained by online gradient descent for which we explored step sizes chosen from the set {0.25, 0.5, 0.75, 1, 2, 4, 8}. For each method we investigated training with up to 20 passes through the data and we selected the best setting of the parameters (step size and number of passes) as the one minimizing the holdout error. Additionally, for the LOMtree we investigated different settings of the stopping criterion for the tree expansion: T = {k − 1, 2k − 1, 4k − 1, 8k − 1, 16k − 1, 32k − 1, 64k − 1}, and swap resistance RS = {4, 8, 16, 32, 64, 128, 256}.\nTable 2: Test error (%) on multiclass classification problems.\nIsolet Sector Aloi ImNet ODP LOMtree 6.36 16.19 16.50 90.17 93.46\nRtree 16.92 15.77 83.74 96.99 98.85 Filter tree 15.10 17.70 80.50 92.12 93.76\nOAA 3.56 9.17 13.78 NA NA\nTable 4: Per-example test time on multiclass classification problems.\nIsolet Sector Aloi ImNet ODP LOMtree 0.14ms 0.13ms 0.06ms 0.52ms 0.26ms\nOAA 0.16 ms 0.24ms 0.33ms 0.21s 1.05s\nWe report test error (Table 2), train time (Table 3) and per-example test time (Table 4). Training time and test error are not reported for OAA on ImageNet and ODP due to the intractability of this method on such extreme multiclass classification problems. Note that, as expected, time-wise LOMtree significantly outperforms OAA due to building only close-to logarithmic depth trees. The improvement in the training time increases with the number of classes in the classification problem. For instance on Aloi training with LOMtree is 12.8 times faster than with OAA. The same can be said about the test time, where the per-example test time for Aloi, ImageNet and ODP are respectively 5.5, 403.8 and 4038.5 times faster for LOMtree than for OAA.\n6 8 10 12 14 16\n2\n4\n6\n8\n10\n12\nlog 2 (number of classes)\nlo g\n2( ti\nm e\nra ti\no )\nLOMtree vs one−against−all"
    }, {
      "heading" : "5 Conlusion and Future Work",
      "text" : "The LOMTree algorithm reduces the multiclass problem to a set of binary problems organized in a tree structure where recovering the split in every tree node is done by optimizing a carefully crafted partition criterion. The criterion guarantees pure (inducing small error) and balanced splits leading to logarithmic training and testing time for the tree classifier. We provide theoretical justification via a boosting statement as well as an empirical evaluation on multiple multiclass datasets. Empirically, we find that this is the best available logarithmic time approach with performance approaching oneagainst all on smaller datasets where that is a viable baseline."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Alekh Agarwal, Dean Foster, Robert Schapire and Matus Telgarsky for valuable discussions."
    }, {
      "heading" : "6 Bottom-up partitions do not work",
      "text" : "Here we show that the most natural bottom-up construction for creating partitions is not viable with an example.\nBottom-up construction techniques start by pairing labels, either randomly or arbitrarily, and then building a predictor of whether the class label is left or right conditioned on the class label being one of the paired labels. In order to construct a full tree, this operation must compose, pairing trees with size 2 to create trees of size 4. Here, we show that the straightforward approach to composition fails.\nSuppose we have a one dimensional feature space with examples of class label i having feature value i and we work with threshold predictors. Suppose we have 4 classes 1, 2, 3, 4, and we happen to pair (1, 3) and (2, 4). It is easy to build a linear predictor for each of these splits. The next step is building a predictor for (1, 3) vs (2, 4) which is impossible because all thresholds in (−∞, 1), (2, 3), and (4,∞) err on two labels while thresholds on (1, 2) and (3, 4) err on one label."
    }, {
      "heading" : "7 Proof of Lemma 1",
      "text" : "We start from deriving an upper-bound on J(h). Again for the ease of notation let Pi = Pr(h(x) > 0|i). Thus\nJ(h) = 2 k∑ i=1 πi |P (h(x) > 0|i)− P (h(x) > 0)| = 2 k∑ i=1 πi ∣∣∣∣∣∣Pi − k∑ j=1 πjPj ∣∣∣∣∣∣ , where ∀i={1,2,...,k}0 ≤ Pi ≤ 1. Let αi = min(Pi, 1 − Pi) and recall the purity factor α =∑k i=1 πiαi and the balancing factor β = P (h(x) > 0). Without loss of generality let β ≤ 1 2 . Furthermore, let\nL1 = {i : i ∈ {1, 2, . . . , k}, Pi ≥ 1\n2 }, L2 = {i : i ∈ {1, 2, . . . , k}, Pi ∈ [β,\n1 2 )}\nand L3 = {i : i ∈ {1, 2, . . . , k}, Pi < β}. First notice that\nβ = k∑ i=1 πiPi = ∑ i∈L1 πi(1− αi) + ∑ i∈L2∪L3 πiαi = ∑ i∈L1 πi − 2 ∑ i∈L1 πiαi + α (5)\nTherefore\nJ(h)\n2 = k∑ i=1 πi |Pi − β| = ∑ i∈L1 πi(1− αi − β) + ∑ i∈L2 πi(αi − β) + ∑ i∈L3 πi(β − αi)\n= ∑ i∈L1 πi(1− β)− ∑ i∈L1 πiαi + ∑ i∈L2 πiαi − ∑ i∈L2 πiβ + ∑ i∈L3 πiβ − ∑ i∈L3 πiαi\nNote that ∑ i∈L3 πi = 1− ∑ i∈L1 πi − ∑ i∈L2 πi and therefore\nJ(h)\n2 = ∑ i∈L1 πi(1−β)− ∑ i∈L1 πiαi+ ∑ i∈L2 πiαi− ∑ i∈L2 πiβ + β(1− ∑ i∈L1 πi− ∑ i∈L2 πi)− ∑ i∈L3 πiαi\n= ∑ i∈L1 πi(1− 2β)− ∑ i∈L1 πiαi + ∑ i∈L2 πiαi + β(1− 2 ∑ i∈L2 πi)− ∑ i∈L3 πiαi\nFurthermore, since − ∑ i∈L1 πiαi + ∑ i∈L2 πiαi − ∑ i∈L3 πiαi = −α+ 2 ∑ i∈L2 πiαi we further write that\nJ(h)\n2 = ∑ i∈L1 πi(1− 2β) + β(1− 2 ∑ i∈L2 πi)− α+ 2 ∑ i∈L2 πiαi\nBy Equation 5, it can be further rewritten as\nJ(h)\n2 = (1− 2β)(β + 2 ∑ i∈L1 πiαi − α) + β(1− 2 ∑ i∈L2 πi)− α+ 2 ∑ i∈L2 πiαi\n= 2(1− β)(β − α) + 2(1− 2β) ∑ i∈L1 πiαi + 2 ∑ i∈L2 πi(αi − β)\nSince αi’s are bounded by 0.5 we obtain\nJ(h)\n2 ≤ 2(1− β)(β − α) + 2(1− 2β) ∑ i∈L1 πiαi + 2 ∑ i∈L2 πi( 1 2 − β)\n≤ 2(1− β)(β − α) + 2(1− 2β)α+ 1− 2β = 2β(1− β)− 2α(1− β) + 2α(1− 2β) + 1− 2β = 1− 2β2 − 2βα\nThus:\nα ≤ 2− J(h) 4β − β."
    }, {
      "heading" : "8 Proof of Lemma 2",
      "text" : "Proof. We first show that J(h) ∈ [0, 1]. We start from deriving an upper-bound on J(h), where h ∈ H is some hypothesis in the hypothesis class. For the ease of notation let Pi = Pr(h(x) > 0|i). Thus\nJ(h) = 2 k∑ i=1 πi |P (h(x) > 0|i)− P (h(x) > 0)| (6)\n= 2 k∑ i=1 πi ∣∣∣∣∣∣Pi − k∑ j=1 πjPj ∣∣∣∣∣∣ , where ∀i={1,2,...,k}0 ≤ Pi ≤ 1. The objective J(h) is certainly maximized on the extremes of the [0, 1] interval. The upper-bound on J(h) can be thus obtained by setting some of the Pi’s to 1’s and remaining ones to 0’s. To be more precise, let\nL1 = {i : i ∈ {1, 2, . . . , k}, Pi = 1} and L2 = {i : i ∈ {1, 2, . . . , k}, Pi = 0}.\nTherefore it follows that\nJ(h) ≤ 2 ∑ i∈L1 πi(1− ∑ j∈L1 πj) + ∑ i∈L2 πi ∑ j∈L1 πj  = 2\n[∑ i∈L1 πi − ( ∑ i∈L1 πi) 2 + (1− ∑ i∈L1 πi) ∑ i∈L1 πi ]\n= 4 [∑ i∈L1 πi − ( ∑ i∈L1 πi) 2 ]\nLet b = ∑ i∈L1 πi thus\nJ(h) ≤ 4b(1− b) = −4b2 + 4b (7)\nSince b ∈ [0, 1], it is straightforward that −4b2 + 4b ∈ [0, 1] and thus J(h) ∈ [0, 1]. We now proceed to prove the main statement of Lemma 2, if h induces a maximally pure and balanced partition then J(h) = 1. Since h is maximally balanced, P (h(x) > 0) = 0.5. Simultaneously, since h is maximally pure ∀i={1,2,...,k}(P (h(x) > 0|i) = 0 or P (h(x) > 0|i) = 1). Substituting that into Equation 7 yields that J(h) = 1.\n9 Proof of the lower-bound ∆t ≥ J 2Gt\n8β(1−β)t lnk\nProof. First, without loss of generality assume that P1 ≤ P2 ≤ · · · ≤ Pk. For the ease of notation let J = J(hn). As observed in Kearns and Mansour [2], the entropy reduction of Equation 3 corresponds to a gap in the Jensen’s inequality, applied to the concave function G(π). Recall that Shannon entropy is strongly concave with respect to `1-norm (see e.g., Example 2.5 in ShalevShwartz [15]). As a specific consequence (see e.g. Theorem 2.1.9 in Nesterov [16]) we obtain\n∆t ≥ wβ(1− β)‖π(n0)− π(n1)‖21 = wβ(1− β) ( k∑ i=1 |πi(n0)− πi(n1)| )2\n= wβ(1− β) ( k∑ i=1 πi ∣∣∣∣Piβ − 1− Pi1− β ∣∣∣∣ )2 = wβ(1− β) ( k∑ i=1 πi ∣∣∣∣ Pi − ββ(1− β) ∣∣∣∣ )2\n= w\nβ(1− β) ( k∑ i=1 |πi(Pi − β)| )2 =\nwJ2\n4β(1− β) ,\nwhere the last equality uses the definition of J(h) = 2 ∑k i=1 πi|Pi − β|.\nFollowing the argument of Kearns and Mansour [2], notice that at round t there must be a leaf n such that wn ≥ Gt2t ln k and this leaf is selected to the currently considered split (we split the leaf node with the highest weight), where Gt = ∑ n∈L wn ∑k i=1−πni ln(πni). That is because\nGt = ∑ n∈L wn k∑ i=1 −πni ln(πni) ≤ ∑ n∈L wn ln k ≤ 2twmax ln k,\nwhere wmax = maxn wn. Thus wmax ≥ Gt2t ln k . Thus\n∆t ≥ J2Gt\n8β(1− β)t ln k ."
    }, {
      "heading" : "10 Proof of Theorem 1",
      "text" : "Proof. Note that the condition γ ∈ (0,min(βn, 1 − βn)] implies that γ ≤ 12 . From the weak hypothesis assumption it follows that for any n, βn cannot be too near 0 or 1 since 1− γ ≥ βn ≥ γ. In Section 9 we showed that\n∆t ≥ J2Gt\n8β(1− β)t ln k .\nWe now proceed to further lower-bound ∆t. Note that the weak hypothesis assumption guarantees J(h) ≥ 2γ, which further yields\n∆t ≥ γ2Gt\n2(1− γ)2t ln k . Let η = √\n8 (1−γ)2 ln kγ. Then\n∆t > η2Gt 16t .\nThus we obtain the recurrence inequality\nGt+1 ≤ Gt −∆t < Gt − η2Gt 16t = Gt\n[ 1− η 2\n16t ] One can now compute the minimum number of splits required to reduce Gt below α, where α ∈ [0, 1]. We use the result from [2] (see the proof of Theorem 10) and obtain the final statement of the theorem."
    }, {
      "heading" : "11 Experiments - dataset details",
      "text" : "Below we provide the details of the datasets that we were using for the experiments in Section 4:\n• Isolet: downloaded from http://www.cs.huji.ac.il/˜shais/datasets/ ClassificationDatasets.html\n• Sector and Aloi: downloaded from http://www.csie.ntu.edu.tw/˜cjlin/ libsvmtools/datasets/multiclass.html\n• ImageNet [17]: features extracted according to http://www.di.ens.fr/willow/ research/cnn/ from the authors.\n• ODP [12]: obtained from Paul Bennett. Our version has signficantly more classes than reported in that paper because we use the entire dataaset."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We study the problem of multiclass classification with an extremely large number<lb>of classes, with the goal of obtaining train and test time complexity logarithmic<lb>in the number of classes. We develop top-down tree construction approaches for<lb>constructing logarithmic depth trees. On the theoretical front, we formulate a new<lb>objective function, which is optimized at each node of the tree and creates dy-<lb>namic partitions of the data which are both pure (in terms of class labels) and<lb>balanced. We demonstrate that under favorable conditions, we can construct loga-<lb>rithmic depth trees that have leaves with low label entropy. However, the objective<lb>function at the nodes is challenging to optimize computationally. We address the<lb>empirical problem with a new online decision tree construction procedure. Exper-<lb>iments demonstrate that this online algorithm quickly achieves small error rates<lb>relative to more common O(k) approaches and simultaneously achieves signif-<lb>icant improvement in test error compared to other logarithmic training time ap-<lb>proaches.",
    "creator" : "LaTeX with hyperref package"
  }
}