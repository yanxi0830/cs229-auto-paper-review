{
  "name" : "1703.01678.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Data-Dependent Stability of Stochastic Gradient Descent",
    "authors" : [ "Ilja Kuzborskij", "Christoph H. Lampert" ],
    "emails" : [ "ilja.kuzborskij@idiap.ch", "chl@ist.ac.at" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Stochastic gradient descent (SGD) has become one of the workhorses of modern machine learning. In particular, it is the optimization method of choice for training highly complex and non-convex models, such as neural networks. When it was observed that these models generalize better (suffer less from overfitting) than classical machine learning theory suggests, a large theoretical interest emerged to explain this phenomenon. Given that SGD at best finds a local minimum of the non-convex objective function, it has been argued that all such minima might be equally good. However, at the same time, a large body of empirical work and tricks of trade, such as early stopping, suggests that in practice one might not even reach a minimum, yet nevertheless observes excellent performance.\nIn this work we follow an alternative route that aims to directly analyze the generalization ability of SGD by studying how sensitive it is to small perturbations in the training set. This is known as algorithmic stability approach [5] and was used recently [16] to establish generalization bounds for both convex and non-convex learning settings. To do so they employed a rather restrictive notion of stability that does not depend on the data, but captures only intrinsic characteristics of the learning algorithm and global properties of the objective function. Consequently, their analysis results in worst-case guarantees that in some cases tend to be too pessimistic. As recently pointed out in [38], deep learning might indeed be such a case, as this notion of stability is insufficient to give deeper theoretical insights, and a less restrictive one is desirable.\nAs our main contribution in this work we establish that a data-dependent notion of algorithmic stability, very similar to the On-Average Stability [34], holds for SGD when applied to convex as well as non-convex learning problems. As a consequence we obtain new generalization bounds that depend on the data-generating distribution and the initialization point of an algorithm. For convex loss functions, the bound on the generalization error is multiplicative in the risk at the initialization point. For the non-convex loss functions, besides the risk, it is also critically controlled by the expected second-order information about the objective function at the initialization point. We further corroborate our findings empirically\nar X\niv :1\n70 3.\n01 67\n8v 3\n[ cs\n.L G\n] 2\n6 M\nay 2\n01 7\nand show that, indeed, the data-dependent generalization bound is tighter than the worst-case counterpart on non-convex objective functions. Finally, the nature of the data-dependent bounds allows us to state optimistic bounds that switch to the faster rate of convergence subject to the vanishing empirical risk.\nIn particular, our findings justify the intuition that SGD is more stable in less curved areas of the objective function and link it to the generalization ability. This also backs up numerous empirical findings in the deep learning literature that solutions with low generalization error occur in less curved regions. At the same time, in pessimistic scenarios, our bounds are no worse than those of [16].\nFinally, we exemplify an application of our bounds, and propose a simple yet principled transfer learning scheme for the convex and non-convex case, which is guaranteed to transfer from the best source of information. In addition, this approach can also be used to select a good initialization given a number of random starting positions. This is a theoretically sound alternative to the purely random commonly used in non-convex learning.\nThe rest of the paper is organized as follows. We revisit the connection between stability and generalization of SGD in Section 3 and introduce a data-dependent notion of stability in Section 4. We state the main results in Section 5, in particular, Theorem 3 for the convex case, and Theorem 5 for the non-convex one. Next we demonstrate empirically that the bound shown in Theorem 5 is tighter than the worst-case one in Section 5.2.1. Finally, we suggest application of these bounds by showcasing principled transfer learning approaches in Section 5.3, and we conclude in Section 6."
    }, {
      "heading" : "2 Related Work",
      "text" : "Algorithmic stability has been a topic of interest in learning theory for a long time, however, the modern approach on the relationship between stability and generalization goes back to the milestone work of [5]. They analyzed several notions of stability, which fall into two categories: distribution-free and distributiondependent ones. The first category is usually called uniform stability and focuses on the intrinsic stability properties of an algorithm without regard to the data-generating distribution. Uniform stability was used to analyze many algorithms, including regularized Empirical Risk Minimization (ERM) [5], randomized aggregation schemes [10], and recently SGD by [16, 24], and [31]. Despite the fact that uniform stability has been shown to be sufficient to guarantee learnability, it can be too pessimistic, resulting in worst-case rates.\nIn this work we are interested in the data-dependent behavior of SGD, thus the emphasis will fall on the distribution-dependent notion of stability, known as on-average stability, explored throughly in [34]. The attractive quality of this less restrictive stability type is that the resulting bounds are controlled by how stable the algorithm is under the data-generating distribution. For instance, in [5] and [9], the on-average stability is related to the variance of an estimator. In [33, Sec. 13], the authors show risk bounds that depend on the expected empirical risk of a solution to the regularized ERM. In turn, one can exploit this fact to state improved optimistic risk bounds, for instance, ones that exhibit fast-rate regimes [20, 14], or even to design enhanced algorithms that minimize these bounds in a data-driven way, e.g. by exploiting side information as in transfer [21, 3] and metric learning [30]. Here, we mainly focus on the later direction in the context of SGD: how stable is SGD under the data-generating distribution given an initialization point? We also touch the former direction by taking advantage of our data-driven analysis and show optimistic bounds as a corollary.\nWe will study the on-average stability of SGD for both convex and non-convex loss functions. In the convex setting, we will relate stability to the risk at the initialization point, while previous data-driven stability arguments usually consider minimizers of convex ERM rather than a stochastic approximation [33, 20]. Beside convex problems, our work also covers the generalization ability of SGD on non-convex problems. Here, we borrow techniques of [16] and extend them to the distribution-dependent setting. That said, while bounds of [16] are stated in terms of worst-case quantities, ours reveal new connections to the data-dependent second-order information. These new insights also partially justify empirical observations in deep learning about the link between the curvature and the generalization error [17, 19, 6]. At the same time, our work is an alternative to the theoretical studies of neural network objective functions [7, 18], as\nwe focus on the direct connection between the generalization and the curvature. In this light, our work is also related to non-convex optimization by SGD. Literature on this subject typically studies rates of convergence to the stationary points [13, 1, 32], and ways to avoid saddles [12, 23]. However, unlike these works, and similarly to [16], we are interested in the generalization ability of SGD, and thanks to the stability approach, involvement of stationary points in our analysis is not necessary.\nFinally, we propose an example application of our findings in Transfer Learning (TL). For instance, by controlling the stability bound in a data-driven way, one can choose an initialization that leads to improved generalization. This is related to TL where one transfers from pre-trained models [22, 36, 29, 3], especially popular in deep learning due to its data-demanding nature [11]. Theoretical literature on this topic is mostly focused on the ERM setting and PAC-bounds, while our analysis of SGD yields such guarantees as a corollary."
    }, {
      "heading" : "3 Stability of Stochastic Gradient Descent",
      "text" : "First, we introduce definitions used in the rest of the paper."
    }, {
      "heading" : "3.1 Definitions",
      "text" : "We will denote with small and capital bold letters respectively column vectors and matrices, e.g., a “ ra1, a2, . . . , adsT P Rd andA P Rd1ˆd2 , }a} is understood as a Euclidean norm and }A}2 as the spectral norm. We denote enumeration by rns “ t1, . . . , nu for n P N.\nWe indicate an example space by Z and its member by z P Z . For instance, in a supervised setting Z “ X ˆ Y , such that X is the input and Y is the output space of a learning problem. We assume that training and testing examples are drawn iid from a probability distribution D over Z . In particular, we will denote the training set as S “ tziumi“1 „ Dm.\nFor a parameter space H, we define a learning algorithm as a map A : Zm ÞÑ H and for brevity we will use the notation AS “ ApSq. In the following we assume that H Ď Rd. To measure the accuracy of a learning algorithm A, we have a loss function fpw, zq, which measures the cost incurred by predicting with parameters w P H on an example z. The risk of w, with respect to the distribution D, and the empirical risk given a training set S are then defined as\nRpwq :“ E z„D\nrfpw, zqs, and pRSpwq :“ 1\nm\nm ÿ i“1 fpw, ziq ."
    }, {
      "heading" : "3.2 Uniform Stability and Generalization",
      "text" : "On an intuitive level, a learning algorithm is said to be stable whenever a small perturbation in the training set does not affect its outcome too much. Of course, there is a number of ways to formalize the perturbation and the extent of the change in the outcome, and we will discuss some of them below. The most important consequence of a stable algorithm is that it generalizes from the training set to the unseen data sampled from the same distribution. In other words, the difference between the risk RpASq and the empirical risk pRSpASq of the algorithm’s output is controlled by the quantity that captures how stable the algorithm is. So, to observe good performance, or a decreasing true risk, we must have a stable algorithm and decreasing empirical risk (training error), which usually comes by design of the algorithm. In this work we focus on the stability of the Stochastic Gradient Descent (SGD) algorithm, and thus, as a consequence, we study its generalization ability.\nRecently, [16] used a stability argument to prove generalization bounds for learning with SGD. Specifically, the authors extended the notion of the uniform stability originally proposed by [5], to accommodate randomized algorithms.\nDefinition 1 (Uniform stability). A randomized algorithm A is -uniformly stable if for all datasets S, Spiq P Zm such that S and Spiq differ in the i-th example, we have\nsup zPZ,iPrms\n\"\nE A rfpAS , zq ´ fpASpiq , zqs\n*\nď .\nSince SGD is a randomized algorithm, we have to cope with two sources of randomness: the data-generating process and the randomization of the algorithm A itself, hence we have statements in expectation. The following theorem of [16] shows that the uniform stability implies generalization in expectation.\nTheorem 1. Let A be -uniformly stable. Then, ˇ\nˇ ˇ ˇ E S,A\n” pRSpASq ´RpASq ı\nˇ ˇ ˇ ˇ ď .\nThus it suffices to characterize the uniform stability of an algorithm to state a generalization bound. In particular, [16] showed generalization bounds for SGD under different assumptions on the loss function f . Despite that these results hold in expectation, other forms of generalization bounds, such as highprobability ones, can be derived from the above [34].\nApart from SGD, uniform stability has been used before to prove generalization bounds for many learning algorithms [5]. However, these bounds typically suggest worst-case generalization rates, and rather reflect intrinsic stability properties of an algorithm. In other words, uniform stability is oblivious to the data-generating process and any other side information, which might reveal scenarios where generalization occurs at a faster rate. In turn, these insights could motivate the design of improved learning algorithms. In the following we address some limitations of analysis through uniform stability by using a less restrictive notion of stability. We extend the setting of [16] by proving data-dependent stability bounds for convex and non-convex loss functions. In addition, we also take into account the initialization point of an algorithm as a form of supplementary information, and we dedicate special attention to its interplay with the data-generating distribution. Finally, we discuss situations where one can explicitly control the stability of SGD in a data-dependent way."
    }, {
      "heading" : "4 Data-dependent Stability Bounds for SGD",
      "text" : "In this section we describe a notion of data-dependent algorithmic stability, that allows us to state generalization bounds which depend not only on the properties of the learning algorithm, but also on the additional parameters of the algorithm. We indicate such additional parameters by θ, and therefore we denote stability as a function pθq. In particular, in the following we will be interested in scenarios where θ describes the data-generating distribution and the initialization point of SGD.\nDefinition 2 (On-Average stability). A randomized algorithm A is pθq-on-average stable if it is true that\nsup iPrms\n\"\nE A E S,z rfpAS , zq ´ fpASpiq , zqs\n*\nď pθq ,\nwhere S iid„Dm and Spiq is its copy with i-th example replaced by z iid„D.\nOur definition of on-average stability resembles the notion introduced by [34]. The difference lies in the fact that we take supremum over index of replaced example. A similar notion was also used by [5] and later by [10] for analysis of a randomized aggregation schemes, however their definition involves absolute difference of losses. The dependence on θ also bears similarity to recent work of [24], however, there, it is used in the context of uniform stability. The following theorem shows that on-average - stable random algorithm is guaranteed to generalize in expectation.\nTheorem 2. Let an algorithm A be pθq-on-average stable. Then,\nE S E A\n” RpASq ´ pRSpASq ı ď pθq .\nProof (sketch). For any S “ tziumi“1 iid„Dm, let Spiq be its copy with i-th example replaced by z iid„D. We relate expected empirical risk and expected risk by\nE S E A rRpASqs “ E S E A r pRSpASqs ` δ , where δ “\n1\nm\nm ÿ i“1 E S,z E A rfpAS , zq ´ fpASpiq , zqs .\nWe further get that\nδ ď sup iPrms\n\"\nE S,z E A rfpAS , zq ´ fpASpiq , zqs\n*\nď pθq .\nThe theorem follows as by definition, the r.h.s. is bounded by pθq."
    }, {
      "heading" : "5 Main Results",
      "text" : "Before presenting our main results in this section, we discuss algorithmic details and assumptions. We will study the following variant of SGD: given a training set S “ tziumi“1\niid„ Dm, step sizes tαtuTt“1, random indices I “ tjtuTt“1, and an initialization point w1, perform updates\nwt`1 “ wt ´ αt∇fpwt, zjtq\nfor T ď m steps. We assume that the indices in I are sampled from the uniform distribution over rms without replacement, and that this is the only source of randomness for SGD. In practice this corresponds to permuting the training set before making a pass through it, as it is commonly done in practical applications. Next, we introduce statements about the loss functions f used in the following.\nDefinition 3 (Lipschitz f ). A loss function f is L-Lipschitz if }∇fpw, zq} ď L, @w P H and @z P Z . Note that this also implies that |fpw, zq ´ fpv, zq| ď L}w ´ v} .\nDefinition 4 (Smooth f ). A loss function is β-smooth if @w,v P H and @z P Z , }∇fpw, zq ´ ∇fpv, zq} ď β}w ´ v} , which also implies fpw, zq ´ fpv, zq ď ∇fpv, zqJpw ´ vq ` β2 }w ´ v} 2 .\nDefinition 5 (Lipschitz Hessians). A loss function f has a ρ-Lipschitz Hessian if @w,v P H and @z P Z , }∇2fpw, zq ´∇2fpv, zq}2 ď ρ}w ´ v} .\nThe last condition is occasionally used in analysis of SGD [12] and holds whenever f has a bounded third derivative. All presented theorems assume that the loss function used by SGD is non-negative, Lipschitz, and β-smooth. Examples of such commonly used loss functions are the logistic/softmax losses and neural networks with sigmoid activations. Convexity of loss functions or Lipschitzness of Hessians will only be required for some results, and we will denote it explicitly when necessary. Proofs for all the statements in this section are given in the supplementary material."
    }, {
      "heading" : "5.1 Convex Losses",
      "text" : "First, we present a new and data-dependent stability result for convex losses.\nTheorem 3. Assume that f is convex, and that SGD’s step sizes satisfy αt ď 2β , @t P rT s. Then SGD is pD,w1q-on-average stable with\npD,w1q ď 2L\na\n2βRpw1q m\nT ÿ t“1 αt .\nUnder the same assumptions, [16] showed a uniform stability bound ď 2L2m řT t“1 αt. Our bound\ndiffers since it involves a multiplicative risk at the initialization point, that is a\nRpw1q, in place of a Lipschitz constant. Thus, our bound corroborates the intuition that whenever we start at a good location of the objective function, the algorithm is more stable and thus generalizes better. In the extreme case of Rpw1q “ 0, the theorem confirms that SGD, in expectation, does not need to make any updates and is therefore perfectly stable. Note, that a result of this type cannot be obtained through the more restrictive uniform stability, precisely because such bounds on the stability must hold even for a worst-case choice of data distribution and initialization. In contrast, the notion of stability we employ depends on the data-generating distribution, which allowed us to introduce dependency on the risk.\nFurthermore, consider that we start at arbitrary locationw1: assuming that the loss function is bounded for a concrete H and Z , the rate of our bound up to a constant is no worse than that of [16]. Finally, one can always tighten this result by taking the minimum of two stability bounds.\nData-dependent argument, very similar to the one used in the proof of Theorem 3, can be also applied to prove the following optimistic bound for learning on convex problems with SGD.\nTheorem 4. Assume that f is convex, and that SGD’s step sizes satisfy αt “ ct ď 2 β , @t P rT s. Then the output of SGD obeys\nE S,A\n” RpASq ´ pRSpASq ı ď 4 4 a\nβRpw1q ? cT\nm\nc\nE S,A\n” pRSpASq ı ` 16 a βRpw1qcT m2 . (1)\nThe bound of Theorem 4 is usually called optimistic because for a vanishing expected empirical risk, it manifests the fast decay of the generalization error. In particular, in our case, the fast rate is Op a\nRpw1qT {m2q. For the common choice of m “ OpT q, this expression reduces to the more familiar looking Op a\nRpw1q{mq. Optimistic bounds for convex learning were extensively studied in recent years in PAC and stochastic optimization settings. PAC literature approached such bounds through relative VC bounds [37], local Rademacher complexity [2], and Rademacher bounds for smooth loss classes [35]. Stochastic optimization literature usually studied optimistic bounds constructively, e.g. for stochastic mirror descent [35] when learning with smooth losses, and stochastic online Newton step [25] for exp-concave loss functions. Here we focus on comparison to [35], since their results assume only smoothness of the loss function, while others impose stronger assumptions. In particular, we consider Corollary 3 of [35], showing the bound on the excess risk ESrRpASqss´arg minwPHRpwq for stochastic optimization. In other words, their bound characterizes consistency of an algorithm w.r.t. the minimizer in the class, and therefore it is not directly comparable to ours. However, their proof technique also allows to obtain the bound on the generalization error of a shape similar to the consistency one (similarly as in [35, Theorem 1]). The main difference of our bound (1) from [35] is a novel multiplicative dependency on the risk at the initialization point Rpw1q, and thus our bound suggests improvement over previous one in warm-start scenarios, especially where initialization point is close to the optimal."
    }, {
      "heading" : "5.2 Non-convex Losses",
      "text" : "Now we state a new stability result for non-convex losses.\nTheorem 5. Assume that fp¨, zq P r0, 1s and has a ρ-Lipschitz Hessian, and that step sizes of a form αt “ ct satisfy c ď min ! 1 β , 1 4p2β lnpT qq2 ) . Then SGD is pD,w1q-on-average stable with\npD,w1q ď 1` 1cγ m ` 2cL2 ˘ 1 1`cγ ˆ E S,A rRpASqs ¨ T ˙ cγ 1`cγ , where (2)\nγ :“ min !\nβ, E z\n“ › ›∇2fpw1, zq › ›\n2\n‰ ` cρp1` lnpT qq a 2βRpw1q ) . (3)\nIn particular, γ characterizes how the curvature at the initialization point affects stability, and hence the generalization error of SGD. Since γ heavily affects the rate of convergence in (2), and in most situations\nsmaller γ yields higher stability, we now look at a few cases of its behavior. Consider a regime such that γ is of the order Θ̃ ´ Er}∇2fpw1, zq}2s ` a Rpw1q ¯\n, or in other words, that stability is controlled by the curvature and the risk of the initialization point w1. This suggests that starting from a point in a less curved region with low risk should yield higher stability, and therefore as predicted by our theory, allow for faster generalization. In addition, we observe that the considered stability regime offers a principled way to pre-screen a good initialization point in practice, by choosing the one that minimizes spectral norm of the Hessian and the risk.\nNext, we focus on a more specific case. Suppose that we choose a step size αt “ ct such that γ ď Θ̃ ` Er}∇2fpw1, zq}2s ˘\n, yet not too small, so that the empirical risk can still be decreased. Then, stability is dominated by the curvature around w1. Indeed, lower generalization errors on non-convex problems, such as training deep neural networks, have been observed empirically when SGD is actively guided [17, 15, 6] or converges to solutions with low curvature [19]. However, to the best of our knowledge, Theorem 5 is the first to establish a theoretical link between the curvature of the loss function and the generalization ability of SGD in a data-dependent sense.\nTheorem 5 allows us to show the following statement that further reinforces the effect of the initialization point on the generalization error.\nCorollary 1. Under conditions of Theorem 5 we have that SGD is pD,w1q-on-average stable with\npD,w1q ď O ˜ 1` 1cγ m pRpw1q ¨ T q cγ 1`cγ ¸ . (4)\nWe take a moment to discuss the role of the risk term in pRpw1q ¨ T q cγ\n1`cγ . Observe that pD,w1q Ñ 0 as Rpw1q Ñ 0, in other words, the generalization error approaches zero as the risk of the initialization point vanishes. This is an intuitive behavior, however, uniform stability does not capture this due to its distribution-free nature. Finally, we note that [16, Theorem 3.8] showed a bound similar to (2), however, in place of γ their bound has a Lipschitz constant of the gradient. The crucial difference lies in term γ which is now not merely a Lipschitz constant, but rather depends on the data-generating distribution and initialization point of SGD. We compare to their bound by considering the worst case scenario, namely, that SGD is initialized in a point with high curvature, or altogether, that the objective function is highly curved everywhere. Then, at least our bound is no worse than the one of [16], since γ ď β.\nTheorem 5 also allows us to prove an optimistic generalization bound for learning with SGD on non-convex objectives.\nCorollary 2. Under conditions of Theorem 5 we have that the output of SGD obeys\nE S,A\n” RpASq ´ pRSpASq ı ď O ˜ 1` 1cγ m ¨max # ˆ E S,A ” pRSpASq ı ¨ T ˙ cγ 1`cγ , ˆ T m ˙cγ +¸ .\nAn important consequence of Corollary 2, is that for a vanishing expected empirical risk, in particular for ES,Ar pRSpASqs “ O ` T cγ\nm1`cγ\n˘ , the generalization error behaves as O ` T cγ\nm1`cγ\n˘\n. Considering the full pass, that is m “ OpT q, we have an optimistic generalization error of order O p1{mq instead of Opm´ 1 1`cγ q. We note that PAC bounds with similar optimistic message (although not directly comparable), but without curvature information can also be obtained through empirical Bernstein bounds as in [26]. However, a PAC bound does not suggest a way to minimize non-convex empirical risk in general, where, on the other hand, SGD is known to work reasonably well."
    }, {
      "heading" : "5.2.1 Tightness of Non-convex Bounds",
      "text" : "Next we empirically assess the tightness of our non-convex generalization bounds on real data. In the following experiment we train a neural network with three convolutional layers interlaced with maxpooling, followed by the fully connected layer with 16 units, on the MNIST dataset. This totals in a model\nwith 18K parameters. Figure 1 compares our data-dependent bound (2) to the distribution-free one of [16, Theorem 3.8]. As as a reference we also include an empirical estimate of the generalization error taken as an absolute difference of the validation and training average losses. Since our bound also depends on the initialization point, we plot (2) for multiple “warm-starts”, ie.with SGD initialized from a pre-trained position. We consider 7 such warm-starts at every 200 steps, and report data-dependent quantities used to compute (2) just beneath the graph. Our first observation is that, clearly, the data-dependent bound gives tighter estimate, by roughly one order of magnitude. Second, simulating start from a pre-trained position suggests even tighter estimates: we suspect that this is due to decreasing validation error which is used as an empirical estimate for Rpw1q which effects heavily bound (2).\nFigure 1: Empirical tightness of datadependent and uniform generalization bounds evaluated by training a convolutional neural network.\nWe compute an empirical estimate of the expected Hessian spectral norm by the power iteration method using an efficient Hessian-vector multiplication method [28]. Since bounds depend on constants L, β, and ρ, we estimate them heuristically by tracking maximal values of the gradient and Hessian norms throughout optimization. We compute bounds with estimates pL “ 78.72, pβ “ 1692.28, pρ “ 3823.73, and c “ 10´3. Note that actual constants can only be larger than estimated ones, and thus, discrepancy between the worst-case and the data-dependent bound can be even larger."
    }, {
      "heading" : "5.3 Application to Transfer Learning",
      "text" : "One example application of data-dependent bounds presented before lies in Transfer Learning (TL), where we are interested in achieving faster generalization on a target task by exploiting side information that originates from different but related source tasks. The literature on TL explored many ways to do so, and here we will focus on the one that is most compatible with our bounds. More formally, suppose that the target task at hand is characterized by a joint probability distribution D, and as before we have a training set S iid„Dm. Some TL approaches also assume access to the data sampled from the distributions associated with the source tasks. Here we follow a conservative approach – instead of the source data, we receive a set of source hypotheses\nwsrck (K k“1 Ă H, trained on the source tasks. The goal of a learner is to come up with a target hypothesis, which in the optimistic scenario generalizes better by relying on source hypotheses. In the TL literature this is known as Hypothesis Transfer Learning (HTL) [22], that is, we transfer from the source hypotheses which act as a proxy to the source tasks and the risk Rpwsrck q quantifies how much source and target tasks are related. In the following we will consider SGD for HTL, where the source hypotheses act as initialization points. First, consider learning with convex losses: Theorem 3 depends on Rpw1q, thus it immediately quantifies the relatedness of source and target tasks. So it is enough to pick the point that minimizes the stability bound to transfer from the most related source. Then, bounding Rpwsrck q by pRSpwsrck q through Hoeffding bound along with union bound gives with high probability that\nmin kPrKs pD,wsrck q ď min kPrKs O\n˜\npRSpwsrck q ` c logpKq m\n¸\n.\nHence, the most related source is the one that simply minimizes empirical risk. Similar conclusions where drawn in HTL literature, albeit in the context of ERM. Matters are slightly more complicated in the non-convex case. We take a similar approach, however, now we minimize stability bound (4), and for the\nsake of simplicity assume that we make a full pass over the data, so T “ m. Minimizing the following empirical upper bound select the best source.\nProposition 1. Let pγ˘k “ 1 m řm i“1 }∇2fpwsrck , ziq}2 ` λ\nb\npRSpwsrck q ˘ O ´ 4 a logpKq{m ¯\n, where λ “ cρp1` lnpT qq ? 2β. Then with high probability we have that\nmin kPrKs pD,wsrck q ď min kPrKs O\n¨\n˝\nˆ\n1` 1 cpγ´k\n˙ pRSpwsrck q cpγ` k 1`cpγ` k ¨ a logpKq\nm 1 1`cpγ` k\n˛\n‚ .\nWe also note that there is no restriction on the origin of the source hypotheses wsrck . In general, these can even be random guesses, in which case we would be pre-screening a good starting position. Finally, pγk involves estimation of the spectral norm of the Hessian, which is computationally cheaper to evaluate compared to the complete Hessian matrix [28]. This is particularly relevant for deep learning, where computation of the Hessian matrix can be prohibitively expensive."
    }, {
      "heading" : "6 Conclusions and Future Work",
      "text" : "In this work we proved data-dependent stability bounds for SGD and revisited its generalization ability. We presented novel bounds for convex and non-convex smooth loss functions, partially controlled by data-dependent quantities, while previous stability bounds for SGD were derived through the worst-case analysis. In particular, for non-convex learning, we demonstrated theoretically that generalization of SGD is heavily affected by the expected curvature around the initialization point. We demonstrated empirically that our bound is indeed tighter compared to the uniform one. In addition, our data-dependent analysis also allowed us to show optimistic bounds on the generalization error of SGD, which exhibit fast rates subject to the vanishing empirical risk of the algorithm’s output.\nIn future work we further intend to explore our theoretical findings experimentally and evaluate the feasibility of the transfer learning based on the second-order information. Another direction lies in making our bounds adaptive. So far we have presented bounds that have data-dependent components, however the step size cannot be adjusted depending on the data, e.g. as in [39]. This was partially addressed by [24], albeit in the context of uniform stability, and we plan to extend this idea to the context of data-dependent stability."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was in parts funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement no 637076).\nThis work was in parts funded by the European Research Council under the European Union’s Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement no 308036."
    }, {
      "heading" : "A Proofs",
      "text" : "In this section we present proofs of all the statements.\nProof of Theorem 2. Indicate by S “ tziumi“1 and S1 “ tz1iumi“1 independent training sets sampled i.i.d. from D, and let Spiq “ tz1, . . . , zi´1, z1i, zi`1, . . . , zmu, such that z1i\niid„D. We relate expected empirical risk and expected risk by\nE S E A\n” pRSpASq ı\n“E S E A\n«\n1\nm\nm ÿ i“1 fpAS , ziq\nff\n“ E S,S1 E A\n«\n1\nm\nm ÿ i“1 fpASpiq , z1iq\nff\n“ E S,S1 E A\n«\n1\nm\nm ÿ i“1 fpAS , z1iq\nff\n´ δ\n“E S E A rRpASqs ´ δ ,\nwhere\nδ “ E S,S1 E A\n«\n1\nm\nm ÿ\ni“1\n` fpAS , z1iq ´ fpASpiq , z1iq ˘\nff\n“ 1 m\nm ÿ i“1 E S,z1i E A “ fpAS , z1iq ´ fpASpiq , z1iq ‰ .\nRenaming z1i as z and taking sup over i we get that\nδ ď sup iPrms\n\"\nE S,z E A rfpAS , zq ´ fpASpiq , zqs\n*\n.\nThis completes the proof.\nA.1 Preliminaries\nWe say that the SGD gradient update rule is an operator Gt : H ÞÑ H, such that\nGtpwq :“ w ´ αt∇fpw, zitq ,\nand it is also a function of the training set S and a random index set I . Then,wt`1 “ Gtpwtq, throughout t “ 1, . . . , T . Moreover we will use the notation wS,t to indicate the output of SGD ran on a training set S, at step t, and define\nδtpS, zq :“ }wS,t ´wSpiq,t} .\nNext, we summarize a few instrumental facts about Gt and few statements about the loss functions used in our proofs.\nDefinition 6 (Expansiveness). A gradient update rule is η-expansive if for all w,v,\n}Gtpwq ´Gtpvq} ď η}w ´ v} .\nThe following lemma characterizes expansiveness for the gradient update rule under different assumptions on f .\nLemma 1 (Lemma 3.6 in [16]). Assume that f is β-smooth. Then, we have that:\n1) Gt is p1` αtβq-expansive,\n2) If f in addition is convex, then, for any αt ď 2β , the gradient update rule Gt is 1-expansive.\nAn important consequence of β-smoothness of f is self-boundedness [33], which we will use on many occasions.\nLemma 2 (Self-boundedness). For β-smooth non-negative function f we have that\n}∇fpw, zq} ď a 2βfpw, zq .\nSelf-boundedness in turn implies the following boundedness of a gradient update rule.\nCorollary 3. Assume that f is β-smooth and non-negative. Then,\n}w ´Gtpwq} “ αt}∇fpw, zjtq} ď αt min ! b 2βfpw, zjtq, L ) .\nProof. By Lemma 2\n}αt∇fpw, zjtq} ď αt b 2βfpw, zjtq ,\nand also by Lipschitzness of f , }αt∇fpw, zjtq} ď αtL.\nNext we introduce a bound that relates the risk of the output at step t to the risk of the initialization pointw1 through the variance of the gradient. Given an appropriate choice of step size, this bound will be crucial at stating stability bounds that depend on the risk at w1. The proof idea is similar to the one of [13]. In particular, it does not require convexity of the loss function.\nLemma 3. Assume that the loss function f is β-smooth. Then, for wS,t we have that\nE S rfpwS,t, zjtq ´ fpw1, zjtqs ď\nt´1 ÿ k“1 αk ˆ αkβ 2 ´ 1 ˙ E S “ }∇fpwS,k, zjkq} 2 ‰ .\nProof. For brevity let wk “ wS,k. The β-smoothness of f for any zjt we have\nfpwk`1, zjtq ´ fpwk, zjtq ď ∇fpwk, zjtqJpwk`1 ´wkq ` β\n2 }wk`1 ´wk}2 .\nConsidering SGD update wk`1 “ wk ´ αk∇fpwk, zjkq, where zjk iid„D, and summing both sides from 1 to t´ 1 we get\nfpwt, zjtq ´ fpw1, zjtq ď ´ t´1 ÿ\nk“1 αk∇fpwk, zjtqJ∇fpwk, zjkq `\nβ\n2\nt´1 ÿ k“1 α2k}∇fpwk, zjkq} 2 .\nTaking expectation w.r.t. S and z on both sides, using the fact thatwS,k does not depend on zjk nor on zjt , and that zjk , zjt iid„D, we have that\nE S,z rfpwS,t, zjtq ´ fpw1, zjtqs ď ´ t´1 ÿ\nk“1 αk E S,z\n“ ∇fpwS,k, zjtqJ∇fpwS,k, zjkq ‰ ` β 2\nt´1 ÿ k“1 α2k E S,z “ }∇fpwS,k, zjkq} 2 ‰\n“ αk ˆ αkβ\n2 ´ 1\n˙ t ÿ\nk“1 α2k E S,z\n“ }∇fpwS,k, zjkq} 2 ‰ .\nThe following lemma is a consequence of Lemma 3 and self-boundedness.\nLemma 4. Assume that the loss function f is β-smooth and non-negative, and that step sizes obey αt ď 2β . Then @t P rT s we have that\nE S,z r}∇fpwS,t, zjtq}s ď\na\n2βRpw1q .\nProof. By Lemma 2, }∇fpwS,t, zjtq} ď a 2βfpwS,t, zjtq. Now, we invoke Lemma 3 assuming that the step size is set such that αt ď 2β to get that\nE S,z r}∇fpwS,t, zjtq}s ď\na\n2β E S\n” b\nfpwS,t, zjtq ı\nď b\n2β E S rfpwS,t, zjtqs (By Jensen’s inequality.)\nď b\n2β E S rfpw1, zjtqs “\na\n2βRpw1q . (By Lemma 3.)\nThe following lemma is similar to Lemma 3.11 of [16], and is instrumental in bounding the stability of SGD. However, we make an adjustment and state it in expectation over the data. Note that it does not require convexity of the loss function.\nLemma 5. Assume that the loss function fp¨, zq P r0, 1s is L-Lipschitz for all z. Then, for every t0 P t0, 1, 2, . . .mu we have that,\nE S,z E A\n“ fpwS,T , zq ´ fpwSpiq,T , zq ‰\n(5)\nď L E S,z\n„\nE A rδT pS, zq | δt0pS, zq “ 0s\n\n` E S,A rRpASqs t0 m . (6)\nProof. We proceed with elementary decomposition, Lipschitzness of f , and using the fact that f is non-negative to have that\nfpwS,T , zq ´ fpwSpiq,T , zq (7) “ `\nfpwS,T , zq ´ fpwSpiq,T , zq ˘ I tδt0pS, zq “ 0u ` `\nfpwS,T , zq ´ fpwSpiq,T , zq ˘ I tδt0pS, zq ‰ 0u ď LδT pS, zqI tδt0pS, zq “ 0u ` fpwS,T , zqI tδt0pS, zq ‰ 0u . (8)\nTaking expectation w.r.t. algorithm randomization, we get that\nE A\n“ fpwS,T , zq ´ fpwSpiq,T , zq ‰\n(9)\nď LE A rδT pS, zqI tδt0pS, zq “ 0us ` E A rfpwS,T , zqI tδt0pS, zq ‰ 0us . (10)\nRecall that i P rms is the index where S and Spiq differ, and introduce a random variable τA taking on the index of the first time step where SGD uses the example zi or a replacement z. Note also that τA does not depend on the data. When τA ą t0, then it must be that δt0pS, zq “ 0, because updates on both S and Spiq are identical until t0. A consequence of this is that I tδt0pS, zq ‰ 0u ď I tτA ď t0u. Thus the rightmost term in (10) is bounded as\nE A rfpwS,T , zqI tδt0pS, zq ‰ 0us ď E A rfpwS,T , zqI tτA ď t0us .\nNow, focus on the r.h.s. above. Recall that we assume randomization by sampling from the uniform distribution over rms without replacement, and denote a realization by tjiumi“1. Then, we can always express our randomization as permutation function πApSq “ tzjiu m i“1. In addition, introduce an algorithm\nGD : Zm ÞÑ H, which is identical to A, except that it passes over the training set S sequentially without randomization. That said, we have that\nE A rfpwS,T , zqI tτA ď t0us “ E A\n“ fpGDπApSq, zqI tτA ď t0u ‰ ,\nand taking expectation over the data,\nE S,z\n„\nE A rfpwS,T , zqI tτA ď t0us\n\n“ E A\n„\nE S,z\n“ fpGDπApSq, zq ‰ I tτA ď t0u  .\nNow observe that for any realization of A, ES,z “ fpGDπApSq, zq ‰ “ EA ES,z rfpAS , zqs because expectation w.r.t. S and z does not change under our randomization 1. Thus, we have that\nE A\n„\nE S,z\n“ fpGDπApSq, zq ‰ I tτA ď t0u \n“ E S,A rRpASqsPpτA ď t0q .\nNow assuming that τA is uniformly distributed over rms we have that\nP pτA ď t0q “ t0 m .\nPutting this together with (7) and (8), we finally get that\nE S,z E A\n“ fpwS,T , zq ´ fpwSpiq,T , zq ‰\nď L E S,z\n„\nE A rδT pS, zqI tδt0pS, zq “ 0us\n\n` E S,A rRpASqs t0 m\nď L E S,z\n„\nE A rδT pS, zq | δt0pS, zq “ 0s\n\n` E S,A rRpASqs t0 m .\nThis completes the proof.\nWe spend a moment to highlight the role of conditional expectation in (6). Observe that we could naively bound (5) by the Lipschitzness of f , but Lemma 5 follows a more careful argument. First note that t0 is a free parameter. The expected distance in (6) between SGD outputs wS,t and wSpiq,t is conditioned on the fact that at step t0 outputs of SGD are still the same. This means that the perturbed point is encountered after t0. Then, the conditional expectation should be a decreasing function of t0: the later the perturbation occurs, the smaller deviation between wS,t and wSpiq,t we should expect. Later we use this fact to minimize the bound (6) over t0.\nA.2 Convex Losses\nIn this section we prove on-average stability for loss functions that are non-negative, β-smooth, and convex.\nTheorem 6. Assume that f is convex, and that SGD’s is ran with step sizes tαtuTt“1. Then, for every t0 P t0, 1, 2, . . .mu, SGD is pD,w1q-on-average stable with\npD,w1q ď 2\nm\nT ÿ\nt“t0`1 αt E S,z r}∇fpwt, zjtq}s ` E S,A rRpASqs\nt0 m .\n1Strictly speaking we could omit EAr¨s and consider any randomization by reshuffling, but we keep expectation for the sake of clarity.\nProof. For brevity denote ∆tpS, zq :“ EA rδtpS, zq | δt0pS, zq “ 0s. We start by applying Lemma 5:\nE S,z E A\n“ fpwS,T , zq ´ fpwSpiq,T , zq ‰ ď L E S,z r∆T pS, zqs ` E S,A rRpASqs t0 m . (11)\nOur goal is to bound the first term on the r.h.s. as a decreasing function of t0, so that eventually we can minimize the bound w.r.t. t0. At this point we focus on the first term, and the proof partially follows the outline of the proof of Theorem 3.7 in [16]. The strategy will be to establish the bound on ∆T pS, zq by using a recursive argument. In fact we will state the bound on ∆t`1pS, zq in terms of ∆tpS, zq and then unravel the recursion. Finally, we will take expectation w.r.t. the data after we obtain the bound by recursion.\nTo do so, we distinguish two cases: 1) SGD encounters a perturbed point at step t, that is t “ i, and 2) the current point is the same in S and Spiq, so t ‰ i. For the first case, we will use data-dependent boundedness of the gradient update rule, Corollary 3, that is\n}GtpwS,tq ´GtpwSpiq,tq} ď δtpS, zq ` 2αt}∇fpwS,t, zjtq} .\nTo handle the second case, we will use the expansiveness of the gradient update rule, Lemma 1, which states that for convex loss functions, the gradient update rule is 1-expansive, so δt`1pS, zq ď δtpS, zq. Considering both cases of example selection, and noting that SGD encounters the perturbation w.p. 1m , we write EA for a step t as\n∆t`1pS, zq ď ˆ 1´ 1 m ˙ ∆tpS, zq ` 1 m p∆tpS, zq ` 2αt}∇fpwS,t, zjtq}q\n“∆tpS, zq ` 2αt}∇fpwS,t, zjtq}\nm .\nUnraveling the recursion from T to t0 and plugging the above into (11) yields\nE A E S,z rδT pS, zqs ď\n2\nm\nT ÿ\nt“t0`1 αt E S,z r}∇fpwt, zjtq}s ` E S,A rRpASqs\nt0 m .\nThis completes the proof.\nNext corollary is a simple consequence of Theorem 6 and Lemma 4.\nProof of Theorem 3. Consider Theorem 6 and set t0 “ 0. Then we have that\npD,w1q ď 2\nm\nT ÿ t“1 αt}∇fpwt, zjtq}\nď 2 a 2βRpw1q m T ÿ\nt“1 αt ,\nwhere the last inequality comes from Lemma 4 assuming that αt ď 2β .\nProof of Theorem 4. For brevity denote r “ ES,A rRpASqs. Consider Theorem 6 and assume that the step size obeys αt “ ct ď 2 β . We have\npD,w1q ď 2c\nm\nT ÿ\nt“t0`1\nES,z r}∇fpwt, zjtq}s t ` r t0 m\nď 2c a 2βRpw1q m ln ˆ T\nt0\n˙\n` r t0 m\n(12)\nď 2c a 2βRpw1q m T t0 ` r t0 m ,\nwhere in (12) we used Lemma 4 to bound expectation of norm and bounded the sum of the step sizes by the logarithm. Now, setting t0 “ b 2 ? 2βRpw1qcT r minimizes the bound above, and plugging it back we get that\npD,w1q ď 2 b 2 a\n2βRpw1qcrT m .\nBy Theorem 2 we then have that\nr ´ E S,A\n” pRSpASq ı ď 4 4 a\nβRpw1q ¨ ? crT\nm (13)\nNow using a simple fact that for any non-negative A,B,C,\nA ď B ` C ? Añ A ď B ` C2 ` ? BC ,\nwe get from (13) that\nr ´ E S,A\n” pRSpASq ı ď 4 4 a\nβRpw1q ? cT\nm\nc\nE S,A\n” pRSpASq ı ` 16 a βRpw1qcT m2 .\nThis completes the proof.\nA.3 Non-convex Losses\nOur proof of a stability bound for non-convex loss functions, Theorem 5, follows a general outline of [16, Theorem 3.8]. Namely, the outputs of SGD run on a training set S and its perturbed version Spiq will not differ too much, because by the time a perturbation is encountered, the step size has already decayed enough. So, on the one hand, stabilization is enforced by the diminishing the step size, and on the other hand, by how much updates expand the distance between the gradients after the perturbation. Since [16] work with uniform stability, they capture the expansiveness of post-perturbation update by the Lipschitzness of the gradient. In combination with a recursive argument, their bound has exponential dependency on the Lipschitz constant of the gradient. We argue that the Lipschitz continuity of the gradient can be too pessimistic in general. Instead, we rely on a local data-driven argument: considering that we initialize SGD at point w1, how much do updates expand the gradient under the distribution of interest? The following crucial lemma characterizes such behavior in terms of the curvature at w1.\nLemma 6. Assume that the loss function fp¨, zq is β-smooth and that its Hessian is ρ-Lipschitz. Then, ›\n›GtpwS,tq ´GtpwSpiq,tq › › ď p1` αtξtpS, zqq δtpS, zq\nwhere\nξtpS, zq :“ › ›∇2fpw1, ztq › › 2 ` ρ\nc\nβ\n2\nT ÿ k“1 αk ´ b fpwS,k, zjkq ` b fpwSpiq,k, z1jkq ¯ .\nFurthermore, for any t P rT s,\nE S,z rξtpS, zqs ď E z\n“› ›∇2fpw1, zq › ›\n2\n‰ ` cρp1` lnpT qq a 2βRpw1q .\nProof. Recall that the randomness of the algorithm is realized through sampling without replacement from the uniform distribution over rms. Apart from that we will not be concerned with the randomness of the algorithm, and given the set of random variables tjiumi“1, for brevity we will use indexing notation z1, z2, . . . , zm to indicate zj1 , zj2 , . . . , zjm . Next, let S piq “ tz1iu m i“1, and introduce a shorthand notation fkpwq “ fpw, zkq and fk1pwq “ fpw, z1kq. We start by applying triangle inequality to get ›\n›GtpwS,tq ´GtpwSpiq,tq › › ď }wS,t ´wSpiq,t} ` αt › ›∇ftpwS,tq ´∇ftpwSpiq,tq › › .\nIn the following we will focus on the second term of r.h.s. above. Given SGD outputs wS,t and wSpiq,t with t ą i, our goal here is to establish how much do gradients grow apart with every new update. This behavior can be characterized assuming that gradient is Lipschitz continuous, however, we conduct a local analysis. Specifically, we observe how much do updates expand gradients, given that we start at some pointw1 under the data-generating distribution. So, instead of the Lipschitz constant, expansiveness rather depends on the curvature aroundw1. On the other hand, we are dealing with outputs at an arbitrary time step t, and therefore we first have to relate them to the initialization pointw1. We do so by using the gradient update rule and telescopic sums, and conclude that this relationship is controlled by the sum of gradient norms along the update path. We further establish that this sum is controlled by the risk of w1, through self-bounding property of the loss function and Lemma 3. Thus, the proof consists of two parts: 1) Decomposition into curvature and gradients along the update path, and 2) bounding those gradients.\n1) Decomposition. Introduce δt :“ wSpiq,t ´wS,t. By Taylor theorem we get that\n∇ftpwS,tq ´∇ftpwSpiq,tq “ ż 1\n0\n´ ∇2ftpwS,t ` τδtq ´∇2ftpw1q ¯ dτδt `∇2ftpw1qδt .\nTaking norm on both sides, applying triangle inequality, Cauchy-Schwartz inequality, and assuming that Hessians are ρ-Lipschitz we obtain\n}∇ftpwS,tq ´∇ftpwSpiq,tq} (14)\nď ż 1\n0\n› ›∇2ftpwS,t ` τδtq ´∇2ftpw1q › ›dτ}δt} ` › ›∇2ftpw1q › › }δt}\nď ρ ż 1\n0 }wS,t ´w1 ` τδt}dτ}δt} `\n› ›∇2ftpw1q › › }δt} . (15)\n2) Bounding gradients. Using telescoping sums and SGD update rule we get that\nwS,t ´w1 ` τδt “ wS,t ´w1 ` τ ` wSpiq,t ´w1 `w1 ´wS,t ˘\n“ t´1 ÿ\nk“1 pwS,k`1 ´wS,kq ` τ\nt´1 ÿ\nk“1\n` wSpiq,k`1 ´wSpiq,k ˘\n´ τ t´1 ÿ\nk“1 pwS,k`1 ´wS,kq\n“ pτ ´ 1q t´1 ÿ\nk“1 αk∇fkpwS,kq ´ τ\nt´1 ÿ k“1 αk∇fk1pwSpiq,kq .\nPlugging above into the integral of (15) we have\nż 1\n0\n› › › › › t´1 ÿ\nk“1 αk\n` pτ ´ 1q∇fkpwS,kq ´ τ∇fk1pwSpiq,kq ˘\n› › › › › dτ\nď 1 2\n› › › › › t´1 ÿ\nk“1 αk∇fkpwS,kq\n› › › › › ` 1 2 › › › › › t´1 ÿ\nk“1 αk∇fk1pwSpiq,kq\n› › › › ›\nď c β\n2\nt´1 ÿ k“1 αk ´ b fkpwS,kq ` b fk1pwSpiq,kq ¯ ,\nwhere the last inequality comes from the self-bounding property of β-smooth functions, Lemma 2. Plugging this result back into (15) completes the proof of the first statement.\nBounding ES,zrξtpS, zqs. Now we briefly focus on the expectation of ξtpS, zq, and relate it to the risk of w1 and expectation Hessian. By definition of ξtpS, zq\nE S,z rξtpS, zqs ď ρ\nc\nβ\n2\nT ÿ k“1 αk ˆ E S,z ” b fpwS,k, zjkq ı ` E S,z ”b fpwSpiq,k, z1jkq ı ˙ ` E S,z “ › ›∇2fpw1, ztq › › 2 ‰ .\nBy Jensen’s inequality and applying Lemma 3 assuming that αt ď 2β we have,\nE ” b fpwS,k, zjkq ı ď b E rfpwS,k, zjkqs ď a Rpw1q .\nWe arrive at the same bound for the perturbed term by renaming z1jk into zjk , using the fact that wSpiq,k does not depend on z1jk under the randomization of SGD. Finally putting things together,\nE S,z rξtpS, zqs ď ρ\na 2βRpw1q T ÿ\nk“1 αk ` E z\n“ }∇2fpw1, zq} ‰ ,\nand upper bounding řT k“1 αk ď c p1` lnpT qq proves the second statement.\nNext, we need the following statement to prove our stability bound.\nProposition 2 (Bernstein-type inequality). Let Z be a zero-mean real-valued r.v., such that |Z| ď b and ErZ2s ď σ2. Then for all |c| ď 12b , we have that E “ ecZ ‰ ď ec2σ2 .\nProof. Stated inequality is a consequence of a Bernstein-type inequality for moment generating functions, Theorem 2.10 in [4]. Observe that zero-centered r.v. Z bounded by b satisfies Bernstein’s condition, that is\n|ErpZ ´ ErZsqqs| ď q! 2 σ2bk´2 for all integers q ě 3 .\nThis in turn satisfies condition for Bernstein-type inequality stating that\nE rexp pcpZ ´ ErZsqqs ď exp ˆ c2σ2{2 1´ b|c| ˙ .\nChoosing |c| ď 12b verifies the statement.\nNow we are ready to prove Theorem 5, which bounds the pD,w1q-on-average stability of SGD.\nProof of Theorem 5. For brevity denote r :“ ES,A rRpASqs and\n∆tpS, zq :“ E A rδtpS, zq | δt0pS, zq “ 0s .\nBy Lemma 5, for all t0 P rms,\nE S,z E A\n“ fpwS,T , zq ´ fpwSpiq,T , zq ‰ ď L E S,z r∆T pS, zqs ` r t0 m . (16)\nMost of the proof is dedicated to bounding the first term in (16). We deal with this similarly as in [16]. Specifically, we state the bound on ∆T pS, zq by using a recursion. In our case, however, we also have an expectation w.r.t. the data, and to avoid complications with dependencies, we first unroll the recursion for the random quantities, and only then take the expectation. At this point the proof crucially relies on the product of exponentials arising from the recursion, and all relevant random quantities end up inside of them. We alleviate this by Proposition 2. Finally, we conclude by minimizing (16) w.r.t. t0. Thus we have three steps: 1) recursion, 2) bounding Erexpp¨ ¨ ¨ qs, and 3) tuning of t0.\n1) Recursion. We begin by stating the bound on ∆T pS, zq by recursion. Thus we will first state the bound on ∆t`1pS, zq in terms of ∆tpS, zq, and other relevant quantities and then unravel the recursion. As in the convex case, we distinguish two cases: 1) SGD encounters the perturbed point at step t, that is t “ i, and 2) the current point is the same in S and Spiq, so t ‰ i. For the first case, we will use worst-case boundedness of Gt, Corollary 3, that is, }GtpwS,tq ´ GtpwSpiq,tq} ď δtpS, zq ` 2αtL . To handle the second case we will use Lemma 6, namely,\n› ›GtpwS,tq ´GtpwSpiq,tq › › ď p1` αtξtpS, zqq δtpS, zq .\nIn addition, as a safety measure we will also take into account that the gradient update rule is at most p1 ` αtβq-expansive by Lemma 1. So we will work with the function ψtpS, zq :“ min tξtpS, zq, βu instead of ξtpS, zq. and decompose the expectation w.r.t. A for a step t. Noting that SGD encounters the perturbed example with probability 1m ,\n∆t`1pS, zq ď ˆ 1´ 1 m ˙ p1` αtψtpS, zqq∆tpS, zq ` 1 m p2αtL`∆tpS, zqq\n“ ˆ 1` ˆ 1´ 1 m ˙ αtψtpS, zq ˙ ∆tpS, zq ` 2αtL m\nď exp pαtψtpS, zqq∆tpS, zq ` 2αtL\nm , (17)\nwhere the last inequality follows from 1` x ď exppxq. This inequality is not overly loose for x P r0, 1s, and, in our case it becomes instrumental in handling the recursion.\nNow, observe that relation xt`1 ď atxt ` bt with xt0 “ 0 unwinds from T to t0 as xT ď řT t“t0`1 bt śT k“t`1 ak. Consequently, having ∆t0pS, zq “ 0, we unwind (17) to get\n∆T pS, zq ď T ÿ\nt“t0`1\n˜\nT ź\nk“t`1 exp\nˆ\ncψkpS, zq k\n˙\n¸\n2cL\nmt\n“ T ÿ\nt“t0`1 exp\n˜\nc T ÿ\nk“t`1\nψkpS, zq k\n¸\n2cL\nmt . (18)\n2) Bounding Erexpp¨ ¨ ¨ qs. We take expectation w.r.t. S and z on both sides and focus on the expectation of the exponential in (18). First, introduce µk :“ ES,zrψkpS, zqs, and proceed as\nE S,z\n«\nexp\n˜\nc T ÿ\nk“t`1\nψkpS, zq k\n¸ff\n“ E S,z\n«\nexp\n˜\nc T ÿ\nk“t`1\nψkpS, zq ´ µk k\n¸ff\nexp\n˜\nc T ÿ\nk“t`1\nµk k\n¸\n. (19)\nObserve that zero-mean version of ψkpS, zq is bounded as\nT ÿ\nk“t`1\n|ψkpS, zq ´ µk| k ď 2β lnpT q ,\nand assume the setting of c as c ď 1 2p2β lnpT qq2 . By Proposition 2, we have\nE\n«\nexp\n˜\nc T ÿ\nk“t`1\nψkpS, zq ´ µk k\n¸ff\nď exp\n¨\n˝c2 E\n»\n–\n˜\nT ÿ\nk“t`1\nψkpS, zq ´ µk k\n¸2 fi\nfl\n˛\n‚\n“ exp\n¨\n˝\nc 2 E\n»\n–\n˜\n1\n2β lnpT q\nT ÿ\nk“t`1\nψkpS, zq ´ µk k\n¸2 fi\nfl\n˛\n‚\nď exp ˜ c\n2 E\n« ˇ\nˇ ˇ ˇ ˇ\nT ÿ\nk“t`1\nψkpS, zq ´ µk k\nˇ ˇ ˇ ˇ ˇ ff¸\nď exp ˜ c\n2\nT ÿ\nk“t`1\nE r|ψkpS, zq ´ µk|s k\n¸\nď exp ˜ c T ÿ\nk“t`1\nµk k\n¸\n.\nGetting back to (19) we conclude that\nE S,z\n«\nexp\n˜\nc T ÿ\nk“t`1\nψkpS, zq k\n¸ff ď exp ˜ c T ÿ\nk“t`1\n2µk k\n¸\n. (20)\nNext, we give an upper-bound on µk, that is µk ď min tβ,ES,zrξkpS, zqsu. Finally, we bound ES,zrξkpS, zqs using the second result of Lemma 6, which holds for any k P rT s, to get that µk ď γ, with γ defined in (3).\n3) Tuning of t0. Now we turn our attention back to (18). Considering that we took an expectation w.r.t. the data, we use (20) and the fact that µk ď γ to get that\nE S,z r∆T pS, zqs ď\nT ÿ\nt“t0`1 exp\n˜\n2cγ T ÿ\nk“t`1\n1\nk\n¸\n2cL\nmt\nď T ÿ\nt“t0`1 exp\nˆ\n2cγ ln\nˆ\nT\nt\n˙˙\n2cL\nmt\n“ 2cL m ` T 2cγ ˘\nT ÿ\nt“t0`1 t´2cγ´1\nď 1 2cγ 2cL m\nˆ\nT\nt0\n˙2cγ\n.\nPlug the above into (16) to get\nE S,z E A\n“ fpwS,T , zq ´ fpwSpiq,T , zq ‰\nď L 2\nγm\nˆ\nT\nt0\n˙2cγ\n` r t0 m . (21)\nLet q “ 2cγ. Then, setting\nt0 “ ˆ 2cL2\nr\n˙ 1 1`q\nT q 1`q\nminimizes (21). Plugging t0 back we get that (21) equals to\n1` 1q m ` 2cL2 ˘ 1 1`q prT q q 1`q .\nThis completes the proof.\nThis theorem implies the following result that is further controlled by the initialization point.\nProof of Corollary 1. Consider statement of Theorem 5. Assuming that step size αt ď 2β , Lemma 3 implies that ES,A rRpASqs ď Rpw1q, which completes the proof.\nA.3.1 Optimistic Rates for Learning with Non-convex Loss Functions\nNext we will prove an optimistic bound based on Theorem 5, in other words, the bound that demonstrates fast convergence rate subject to the vanishing empirical risk. First we will need the following technical statement.\nLemma 7. [8, Lemma 7.2] Let c1, c2, . . . , cl ą 0 and s ą q1 ą q2 ą . . . ą ql´1 ą 0. Then the equation\nxs ´ c1xq1 ´ c2xq2 ´ ¨ ¨ ¨ ´ cl´1xql´1 ´ cl “ 0\nhas a unique positive solution x‹. In addition,\nx‹ ď max \" plc1q 1 s´q1 , plc2q 1 s´q2 , ¨ ¨ ¨ , plcl´1q 1 s´ql´1 , plclq 1 s * .\nNext we prove a useful technical lemma similarly as in [27, Lemma 7].\nLemma 8. Let a, c ą 0 and 0 ă α ă 1. Then the inequality\nx´ axα ´ c ď 0\nimplies x ď max ! 2 α 1´αa 1 1´α , p2cqα a ) ` c .\nProof. Consider a function hpxq “ x´ axα ´ c. Applying Lemma 7 with s “ 1, l “ 2, c1 “ a, c2 “ c, and q1 “ α we get that hpxq “ 0 has a unique positive solution x‹ and\nx‹ ď max ! p2aq 1 1´α , 2c ) . (22)\nMoreover, the inequality hpxq ď 0 is verified for x “ 0, and limxÑ`8 hpxq “ `8, so we have that hpxq ď 0 implies x ď x‹. Now, using this fact and the fact that hpx‹q “ 0, we have that\nx ď x‹ “ a px‹qα ` c ,\nand upper-bounding x‹ by (22) we finally have\nx ď amax ! p2aq α 1´α , p2cqα ) ` c ,\nwhich completes the proof.\nProof of Corollary 2. Consider Theorem 5 and observe that it verifies condition of Lemma 8 with x “ ES,A rRpASqs, c “ ES,A ” pRSpASq ı , α “ cγ1`cγ , and\na “ 1` 1cγ m ` 2cL2 ˘ 1 1`cγ T cγ 1`cγ .\nNote that α{p1´ αq “ cγ and 1{p1´ αq “ 1` cγ. Then, we obtain that\nE S,A\n” RpASq ´ pRSpASq ı ď max # 2cγ ˜ 1` 1cγ m ¸1`cγ ` 2cL2 ˘ T cγ ,\nˆ\n2 E S,A\n” pRSpASq ı\n˙ cγ 1`cγ ˜ 1` 1cγ m ` 2cL2 ˘ 1 1`cγ T cγ 1`cγ ¸+\n“ max # ˆ\n2` 2 cγ\n˙1`cγ `\ncL2 ˘\nˆ\nT cγ\nm1`cγ\n˙\n,\n1` 1cγ m ` 2cL2 ˘ 1 1`cγ ˆ 2 E S,A ” pRSpASq ı ¨ T ˙ cγ 1`cγ + .\nThis completes the proof.\nProof of Proposition 1. Consider minimizing (4) over a discrete set of source hypotheses wsrck (K k“1,\nmin kPrKs pD,wsrck q ď min kPrKs O\n˜\n1` 1cγk m pRpwsrck q ¨ T q cγk 1`cγk\n¸\n, (23)\nand recall that γk “ E\nz„D\n“ }∇2fpwsrck , zq}2 ‰\n` λ b\nRpwsrck q ,\nsuch that λ “ cρp1` lnpT qq ? 2β. Let\nγ̃k “ 1\nm\nm ÿ i“1 }∇2fpwsrck , ziq}2 ` λ b pRSpwsrck q .\nBy Hoeffding inequality, with high probability, we have that |γk ´ γ̃k| ď O ´\n1 4 ? m\n¯\n. Now we further upper bound (23) by upper bounding Rpwsrck q and applying union bound to get\nmin kPrKs pD,wsrck q ď min kPrKs O\n¨\n˚ ˚ ˝\nˆ\n1` 1 cqγ´k\n˙\n˜\npRSpwsrck q ` c logpKq m\n¸ cpγ` k\n1`cpγ` k m ´ 1 1`cpγ` k\n˛\n‹ ‹ ‚\nď min kPrKs O\n¨\n˝\nˆ\n1` 1 cqγ´k\n˙ pRSpwsrck q cpγ` k 1`cpγ` k ¨ a logpKq\nm 1 1`cpγ` k\n˛\n‚ ,\nwhich concludes the proof."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We establish a data-dependent notion of algorithmic stability for Stochastic Gradient Descent (SGD), and employ it to develop novel generalization bounds. This is in contrast to previous distribution-free algorithmic stability results for SGD which depend on the worst-case constants. By virtue of the data-dependent argument, our bounds provide new insights into learning with SGD on convex and non-convex problems. In the convex case, we show that the bound on the generalization error is multiplicative in the risk at the initialization point. In the non-convex case, we prove that the expected curvature of the objective function around the initialization point has crucial influence on the generalization error. In both cases, our results suggest a simple data-driven strategy to stabilize SGD by pre-screening its initialization. As a corollary, our results allow us to show optimistic generalization bounds that exhibit fast convergence rates for SGD subject to a vanishing empirical risk.",
    "creator" : "LaTeX with hyperref package"
  }
}