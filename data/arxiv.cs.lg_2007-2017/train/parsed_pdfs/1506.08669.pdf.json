{
  "name" : "1506.08669.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient and Parsimonious Agnostic Active Learning",
    "authors" : [ "Tzu-Kuo Huang", "Alekh Agarwal", "Daniel J. Hsu", "John Langford", "Robert E. Schapire" ],
    "emails" : [ "tkhuang@microsoft.com", "alekha@microsoft.com", "djhsu@cs.columbia.edu", "jcl@microsoft.com", "schapire@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "How can you best learn a classifier given a label budget?\nActive learning approaches are known to yield exponential improvements over supervised learning under strong assumptions [Cohn et al., 1994]. Under much weaker assumptions, streaming-based agnostic active learning [Balcan et al., 2006, Beygelzimer et al., 2009, 2010, Dasgupta et al., 2007, Zhang and Chaudhuri, 2014] is particularly appealing since it is known to work for any classifier representation and any label noise distribution with an i.i.d. data source.1 Here, a learning algorithm decides for each unlabeled example in sequence whether or not to request a label, never revisiting this decision. Restated then: What is the best possible active learning algorithm which works for any classifier representation, any label noise distribution, and is computationally tractable?\nComputational tractability is a critical concern, because most known algorithms for this setting [e.g., Balcan et al., 2006, Koltchinskii, 2010, Zhang and Chaudhuri, 2014] require explicit enumeration of classifiers, implying exponentially-worse computational complexity compared to typical supervised learning algorithms. Active learning algorithms based on empirical risk minimization (ERM) oracles [Beygelzimer et al., 2009, 2010, Hsu, 2010] can overcome this intractability by using passive classification algorithms as the oracle to achieve a computationally acceptable solution.\nAchieving generality, robustness, and acceptable computation has a cost. For the above methods [Beygelzimer et al., 2009, 2010, Hsu, 2010], a label is requested on nearly every unlabeled example where two empirically good classifiers disagree. This results in a poor label complexity, well short of information-theoretic limits [Castro and Nowak, 2008] even for general robust solutions [Zhang and Chaudhuri, 2014]. Until now.\nIn Section 3, we design a new algorithm ACTIVE COVER (AC) for constructing query probability functions that minimize the probability of querying inside the disagreement region—the set of points where good classifiers disagree—and never query otherwise. This requires a new algorithm that maintains a parsimonious cover of the set of empirically good classifiers. The cover is a result of solving an optimization problem (in Section 5) specifying the properties of a desirable query probability function. The cover size provides a practical knob between computation and label complexity, as demonstrated by the complexity analysis we present in Section 5.\nIn Section 4, we provider our main results which demonstrate that AC effectively maintains a set of good classifiers, achieves good generalization error, and has a label complexity bound tighter than previous approaches. The label complexity bound depends on the disagreement coefficient [Hanneke, 2009], which does not completely capture the advantage of the algorithm. In Appendix 4.2.2, we provide an example of a hard active learning problem where AC is\n1See the monograph of Hanneke [2014] for an overview of the existing literature, including alternative settings where additional assumptions are placed on the data source (e.g., separability) as is common in other works [Dasgupta, 2005, Balcan et al., 2007, Balcan and Long, 2013].\nar X\niv :1\n50 6.\n08 66\n9v 1\n[ cs\n.L G\n] 2\n9 Ju\nn 20\n15\nsubstantially superior to previous tractable approaches. Together, these results show that AC is better and sometimes substantially better in theory. The key aspects in the proof of our generalization results are presented in Section 7, with more technical details and label complexity analysis presented in the appendix.\nDo agnostic active learning algorithms work in practice? No previous works have addressed this question empirically. Doing so is important because analysis cannot reveal the degree to which existing classification algorithms effectively provide an ERM oracle. We conduct an extensive study in Section 6 by simulating the interaction of the active learning algorithm with a streaming supervised dataset. The results show that an online variant of AC (called OAC) is typically superior across a wide array of datasets. A summary of our results is presented in Figure 1 which shows the fraction of datasets where an algorithm has a better test error than a random sub-sampling at different query rates across 23 datasets, with details in Section 6 and Appendix G."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Let H ⊆ {±1}X be a set of binary classifiers, which we assume is finite for simplicity.2 Let EX [·] denote expectation with respect to X ∼ PX , the marginal of P over X . The error of a classifier h ∈ H is err(h) := Pr(X,Y )∼P(h(X) 6= Y ), and the error minimizer is denoted by h∗ := arg minh∈H err(h). The (importance weighted) empirical error of h ∈ H on a multiset S of importance weighted and labeled examples drawn from X × {±1}×R+ is err(h, S) := ∑ (x,y,w)∈S w · 1(h(x) 6= y)/|S|. The disagreement region for a subset of classifiers A ⊆ H is DIS(A) := {x ∈ X | ∃h, h′ ∈ A such that h(x) 6= h′(x)}. The regret of a classifier h ∈ H relative to another h′ ∈ H is reg(h, h′) := err(h)− err(h′), and the analogous empirical regret on S is reg(h, h′, S) := err(h, S)− err(h′, S). When the second classifier h′ in (empirical) regret is omitted, it is taken to be the (empirical) error minimizer inH.\nA streaming-based active learner receives i.i.d. labeled examples (X1, Y1), (X2, Y2), . . . from P one at a time; each label Yi is hidden unless the learner decides on the spot to query it. The goal is to produce a classifier h ∈ H with low error err(h), while querying as few labels as possible.\nIn the IWAL framework [Beygelzimer et al., 2009], a decision whether or not to query a label is made randomly: the learner picks a probability p ∈ [0, 1], and queries the label with that probability. Whenever p > 0, an unbiased error estimate can be produced using inverse probability weighting [Horvitz and Thompson, 1952]. Specifically, for any classifier h, an unbiased estimator E of err(h) based on (X,Y ) ∼ P and p is as follows: if Y is queried, then E = 1(h(X) 6= Y )/p; else, E = 0. It is easy to check that E(E) = err(h). Thus, when the label is queried, we\n2The assumption that H is finite can be relaxed to VC-classes using standard arguments.\nAlgorithm 1 ACTIVE COVER (AC) input: Constants c1, c2, c3, confidence δ, error radius γ, parameters α, β, ξ for (OP), epoch schedule 0 = τ0 < 3 =\nτ1 < τ2 < τ3 < . . . < τM satisfying τm+1 ≤ 2τm for m ≥ 1. initialize: epoch m = 0, Z̃0 := ∅, ∆0 := c1 √ 1 + c2 1 log 3, where\nm := 32(log(|H|/δ) + log τm)\nτm .\n1: for i = 4, . . . , n, do 2: if i = τm + 1 then 3: Set Z̃m = Z̃m−1 ∪ S, and S = ∅. 4: Let\nhm+1 := arg min h∈H err(h, Z̃m), (1)\n∆m := c1 √ merr(hm+1, Z̃m) + c2 m log τm, (2)\nAm+1 := {h | err(h, Z̃m)− err(hm+1, Z̃m) ≤ γ∆m}. (3)\n5: Compute the solution Pm+1(·) to the optimization problem (5). 6: m := m+ 1. 7: end if 8: Receive unlabeled data point Xi. 9: if Xi ∈ Dm := DIS(Am), then\n10: Draw Qi ∼ Bernoulli(Pm(Xi)). 11: Update the set of examples:4\nS := { S ∪ {(Xi, Yi, 1/Pm(Xi))}, Qi = 1 S ∪ {Xi, 1, 0}, otherwise.\n12: else S := S ∪ {(Xi, hm(Xi), 1)}. 13: 14: end if 15: end for 16: hM+1 := arg minh∈H err(h, Z̃M ).\nproduce the importance weighted labeled example (X,Y, 1/p).3"
    }, {
      "heading" : "3 Algorithm",
      "text" : "Our new algorithm, shown as Algorithm 1, breaks the example stream into epochs. The algorithm admits any epoch schedule so long as the epoch lengths satisfy τm−1 ≤ 2τm. For technical reasons, we always query the first 3 labels to kick-start the algorithm.At the start of epoch m, AC computes a query probability function Pm : X → [0, 1] which will be used for sampling the data points to query during the epoch. This is done by maintaining a few objects\n3If the label is not queried, we produce an ignored example of weight zero; its only purpose is to maintain the correct count of querying opportunities. This ensures that 1/|S| is the correct normalization in err(h, S).\n4See Footnote 3. Adding an example of importance weight zero simply increments |S| without updating other state of the algorithm, hence the label used does not matter.\nof interest during each epoch:\n1. In step 1, we compute the best classifier on the sample Z̃m that we have collected so far. Note that the sample consists of the queried, true labels on some examples, while predicted labels for the others.\n2. A radius ∆m is computed in step 2 based on the desired level of concentration we want the various empirical quantities to satisfy.\n3. The set Am in step 3 consists of all the hypotheses which are good according to our sample Z̃m, with the notion of good being measured as empirical regret being at most ∆m.\nWithin the epoch, Pm determines the probability of querying an example in the disagreement region for this set Am of “good” classifiers; examples outside this region are not queried but given labels predicted by hm. Consequently, the sample is not unbiased unlike some of the predecessors of our work. The various constants in Algorithm 1 must satisfy:\nα ≥ 1, η ≥ 864, ξ ≤ 1 8n M log n , β2 ≤ η 864γn M log n , γ ≥ η/4,\nc1 ≥ 2α √ 6, c2 ≥ ηc21/4, c3 ≥ 1. (4)\nEpoch Schedules: The algorithm as stated takes an arbitrary epoch schedule subject to τm < τm+1 ≤ 2τm. Two natural extremes are unit-length epochs, τm = m, and doubling epochs, τm+1 = 2τm. The main difference comes in the number of times (OP) is solved, which is a substantial computational consideration. Unless otherwise stated, we assume the doubling epoch schedule so that the query distribution and ERM classifier are recomputed only O(log n) times.\nOptimization problem (OP) to obtain Pm: AC computes Pm as the solution to the optimization problem (OP). In essence, the problem encodes the properties of a query probability function that are essential to ensure good generalization, while maintaining a low label complexity. As we will discuss later, some of the previous works can be seen as specific ways of construction feasible solutions to this optimization problem. The objective function of (OP) encourages small query probabilities in order to minimize the label complexity. It might appear odd that we do not use the more obvious choice for objective which would be EX [P (X)], however our choice simultaneously encourages low query probabilities and also provides a barrier for the constraint P (X) ≤ 1–an important algorithmic aspect as we will discuss in Section F.\nThe constraints (5) in (OP) bound the variance in our importance-weighted regret estimates for every h ∈ H. This is key to ensuring good generalization as we will later use Bernstein-style bounds which rely on our random variables having a small variance. Let us examine these constraints in more detail. The LHS of the constraints measures the variance in our empirical regret estimates for h, measured only on the examples in the disagreement region Dm. This is because the importance weights in the form of 1/Pm(X) are only applied to these examples; outside this region we use the predicted labels with an importance weight of 1. The RHS of the constraint consists of three terms. The first term ensures the feasibility of the problem, as P (X) ≡ 1/(2α2) for X ∈ Dm will always satisfy the constraints. The second empirical regret term makes the constraints easy to satisfy for bad hypotheses–this is crucial to rule out large label complexities in case there are bad hypotheses that disagree very often with hm. A benefit of this is easily seen when −hm ∈ H, which might have a terrible regret, but would force a near-constant query probability on the disagreement region if β = 0. Finally, the third term will be on the same order as the second one for hypotheses in Am, and is only included to capture the allowed level of slack in our constraints which will be exploited for the efficient implementation in Section 5.\nOf course, variance alone is not adequate to ensure concentration, and we also require the random variables of interest to be appropriately bounded. This is ensured through the constraints (6), which impose a minimum query probability on the disagreement region. Outside the disagreement region, we use the predicted label with an importance weight of 1, so that our estimates will always be bounded (albeit biased) in this region. Note that this optimization\nOptimization Problem (OP) to compute Pm\nmin P\nEX [\n1\n1− P (X) ] s.t. ∀h ∈ H EX [ 1(h(x) 6= hm(x) ∧ x ∈ Dm)\nP (X)\n] ≤ bm(h), (5)\n∀x ∈ X 0 ≤ P (x) ≤ 1, and ∀x ∈ Dm P (x) ≥ Pmin,m (6)\nwhere bm(h) = 2α2EX [Imh (X)] + 2β2γreg(h, hm, Z̃m−1)τm−1∆m−1 + ξτm−1∆2m−1, and\nPmin,m = min  c3√ τm−1err(hm,Z̃m−1)\nn M + log τm−1\n, 1\n2\n . (7)\nproblem is written with respect to the marginal distribution of the data points PX , meaning that we might have infinite number of the latter constraints. In Section 5, we describe how to solve this optimization problem efficiently, and using access to only unlabeled examples drawn from PX .\nFinally we verify that the choices for Pm according to some of the previous methods are indeed feasible in (OP). This is most easily seen for Oracular CAL [Hsu, 2010] which queries with probability 1 if X ∈ Dm and 0 otherwise. Since α ≥ 1 (4) in the variance constraints (5), the choice P (X) ≡ 1 for X ∈ Dm is feasible for (OP), and consequently Oracular CAL always queries more often than the optimal distribution Pm at each epoch. A similar argument can also be made for the IWAL method [Beygelzimer et al., 2010], which also queries in the disagreement region with probability 1, and hence suffers from the same suboptimality compared to our choice."
    }, {
      "heading" : "4 Generalization and Label Complexity",
      "text" : "We now present guarantees on the generalization error and label complexity of Algorithm 1 assuming a solver for (OP), which we provide in the next section."
    }, {
      "heading" : "4.1 Generalization guarantees",
      "text" : "Our first theorem provides a bound on generalization error. Define\nerrm(h) := 1\nτm m∑ j=1 (τj − τj−1)E(X,Y )∼P[1(h(X) 6= Y ∧X ∈ Dj)],\n∆∗0 := ∆0 and ∆ ∗ m := c1 √ merrm(h∗) + c2 m log τm for m ≥ 1.\nEssentially ∆∗m is a population counterpart of the quantity ∆m used in Algorithm 1, and crucially relies on errm(h ∗), the true error of h∗ restricted to the disagreement region instead of the empirical error of the ERM at epoch m. This quantity captures the inherent noisiness of the problem, and modulates the transition between O(1/ √ n) to O(1/n) type error bounds as we see next. Theorem 1. Pick any 0 < δ < 1/e such that |H|/δ > √\n192. Then recalling that h∗ = arg minh∈H err(h), we have for all epochs m = 1, 2, . . . ,M , with probability at least 1− δ\nreg(h, h∗) ≤ 16γ∆∗m for all h ∈ Am+1, and (8) reg(h∗, hm+1, Z̃m) ≤ η∆m/4. (9)\nThe theorem is proved in Section 7.2.2, using the overall analysis framework described in Section 7. Since we use γ ≥ η/4, the bound (9) implies that h∗ ∈ Am for all epochs m. This also maintains that all the predicted labels used by our algorithm are identical to those of h∗, since no disagreement amongst classifiers in Am was observed on those examples. This observation will be critical to our proofs, where we will exploit the fact that using labels predicted by h∗ instead of observed labels on certain examples only introduces a bias in favor of h∗, thereby ensuring that we never mistakenly drop the optimal classifier from our version space Am.\nThe bound (8) shows that every hypothesis in Am+1 has a small regret to h∗. Since the ERM classifier hm+1 is always in Am+1, this yields our main generalization error bound on the classifier hτm+1 output by Algorithm 1. Additionally, it also clarifies the definition of the setsAm as the set of good classifiers: these are classifiers which have small population regret relative to h∗ indeed. In the worst case, if errm(h∗) is a constant, then the overall regret bound is O(1/ √ n). The actual rates implied by the theorem, however depend on the properties of the distribution and below we illustrate this with two corollaries. We start with a simple specialization to the realizable setting.\nCorollary 1. Under the conditions of Theorem 1, suppose further that err(h∗) = 0. Then ∆m = ∆∗m = c2τm log τm and hence reg(h, h∗) ≤ 16c2τm log τm for all hypotheses h ∈ Am+1.\nIn words, the corollary demonstrates a Õ(1/n) rate after seeing n unlabeled examples in the realizable setting. Of course the use of errm(h∗) in defining ∆∗m allows us to retain the fast rates even when h\n∗ makes some errors but they do not fall in the disagreement region of good classifiers. One intuitive condition that controls the errors within the disagreement region is the low-noise condition of Tsybakov [2004], which asserts that there exist constants ζ > 0 and 0 < ω ≤ 1 such that\nPr(h(X) 6= h∗(X)) ≤ ζ · (err(h)− err(h∗))ω, ∀h ∈ H such that err(h)− err(h∗) ≤ ε0. (10)\nUnder this assumption, the extreme ω = 0 corresponds to the worst-case setting while ω = 1 corresponds to h∗ having a zero error on disagreement set of the classifiers with regret at most ε0. Under this assumption, we get the following corollary of Theorem 1.\nCorollary 2. Under conditions of Theorem 1, suppose further that Tsybakov’s low-noise condition (10) is satisfied with some parameters ζ, ω, and ε0 = 1. Then after m epochs, we have reg(h, h∗) = Õ ( τ − 12−ω m log(|H|/δ) ) .\nThe proof of this result is deferred to Appendix E. It is worth noting that the rates obtained here are known to be unimprovable for even passive learning under the Tsybakov noise condition Castro and Nowak [2008].5 Consequently, there is no loss of statistical efficiency in using our active learning approach. The result is easily extended for other values of ε0 by using the worst-case bound until the first epoch m0 when 16γ∆∗m0 drops below ε0 and then apply our analysis above from m0 onwards. We leave this development to the reader."
    }, {
      "heading" : "4.2 Label complexity",
      "text" : "Generalization alone does not convey the entire quality of an active learning algorithm, since a trivial algorithm queries always with probability 1, thereby matching the generalization guarantees of passive learning. In this section, we show that our algorithm can achieve the aforementioned generalization guarantees, despite having a small label complexity in favorable situations. We begin with a worst-case result in the agnostic setting, and then describe a specific example which demonstrates some key differences of our approach from its predecessors."
    }, {
      "heading" : "4.2.1 Disagreement-based label complexity bounds",
      "text" : "In order to quantify the extent of gains over passive learning, we measure the hardness of our problem using the disagreement coefficient [Hanneke, 2014], which is defined as\n5ω in our statement of the low-noise condition (10) corresponds to 1/κ in the results of Castro and Nowak [2008].\nθ = θ(h∗) := sup r>0 PX {x | ∃h ∈ H s.t.h∗(x) 6= h(x), PX {x′ | h(x′) 6= h∗(x′)} ≤ r} r . (11)\nIntuitively, given a set of classifiers H and a data distribution P, an active learning problem is easy if good classifiers disagree on only a small fraction of the examples, so that the active learning algorithm can increasingly restrict attention only to this set. With this definition, we have the following result for the label complexity of Algorithm 1.\nTheorem 2. Under conditions of Theorem 1, with probability at least 1 − δ, the expected number6 of label queries made by Algorithm 1 after n examples over M epochs is at most\n2θerrM (h ∗)n+ θ · Õ( √ nerrM (h∗) log(|H|/δ) + log(|H|/δ)).\nThe proof is in Appendix D. The dominant first term of the label complexity bound is linear in the number of unlabeled examples, but can be quite small if θ is small, or if errM (h∗) ≈ 0—it is indeed 0 in the realizable setting. We illustrate this aspect of the theorem with a corollary for the realizable setting.\nCorollary 3. Under the conditions of Theorem 2, suppose further that err(h∗) = 0. Then the expected number of label queries made by Algorithm 1 is at most θÕ(log(|H|/δ)).\nIn words, we attain a logarithmic label complexity in the realizable setting. We contrast this with the label complexity of IWAL [Beygelzimer et al., 2010], which grows as θ √ n independent of err(h∗). This leads to an exponential difference in the label complexities of the two methods in low-noise problems. A much closer comparison is with respect to the Oracular CAL algorithm [Hsu, 2010], which does have a dependence on √ nerr(h∗) in the second term, but has a worse dependence on θ. Just like Corollary 2, we can also obtain improved bounds on label complexity under the Tsybakov noise condition.\nCorollary 4. Under conditions of Theorem 2, suppose further that Tsybakov’s low-noise condition (10) is satisfied with some parameters ζ, ω, and ε0 = 1. Then after m epochs, the expected number of label queries made by Algorithm 1 is at most Õ ( τ 2(1−ω) 2−ω m log(|H|/δ) ) .\nThe proof of this result is deferred to Appendix E. The label complexity obtained above is indeed optimal in terms of the dependence on n, the number of unlabeled examples, matching known information-theoretic rates of Castro and Nowak [2008]. This can be seen since the regret from Corollary 2 falls as a function of the number of queries at a rate of Õ(q − 1 2(1−ω) m log(|H|/δ)) after m epochs, where qm is the number of label queries. This is indeed optimal according to the lower bounds of Castro and Nowak [2008], after recalling that ω = 1/κ in their results. Once again, the corollary highlights our improvements on top of IWAL, which does not attain this optimal label complexity.\nThese results, while strong, still do not completely capture the performance of our method. Indeed the proofs of these results are entirely based on the fact that we do not query outside the disagreement region, a property shared by the previous Oracular CAL algorithm [Hsu, 2010]. Indeed we only improve upon that result as we use more refined error bounds to define the disagreement region. However, such analysis completely ignores the fact that we construct a rather non-trivial query probability function on the disagreement region, as opposed to using any constant probability of querying over this entire region. This gives our algorithm the ability to query much more rarely even over the disagreement region, if the queries do not provide much information regarding the optimal hypothesis h∗. The next section illustrates an example where this gain can be quantified."
    }, {
      "heading" : "4.2.2 Improved label complexity for a hard problem instance",
      "text" : "We now present an example where the label complexity of Algorithm 1 is significantly smaller than both IWAL and Oracular CAL by virtue of rarely querying in the disagreement region. The example considers a distribution and a\n6Expectation is with respect to the coins tossed by the algorithm.\nclassifier space with the following structure: (i) for most examples a single good classifier predicts differently from the remaining classifiers (ii) on a few examples half the classifiers predict one way and half the other. In the first case, little advantage is gained from a label because it provides evidence against only a single classifier. ACTIVE COVER queries over the disagreement region with a probability close to Pmin in case (i) and probability 1 in case (ii), while others query with probability Ω(1) everywhere implying O( √ n) times more queries.\nConcretely, we consider the following binary classification problem. Let H denote the finite classifier space (defined later), and distinguish some h∗ ∈ H. Let U{−1, 1} denote the uniform distribution on {−1, 1}. The data distribution D(X ,Y) and the classifiers are defined jointly:\n• With probability ,\ny = h∗(x), h(x) ∼ U{−1, 1}, ∀h 6= h∗.\n• With probability 1− ,\ny ∼ U{−1, 1}, h∗(x) ∼ U{−1, 1}, hr(x) = −h∗(x) for some hr drawn uniformly at random fromH \\ h∗, h(x) = h∗(x) ∀h 6= h∗ ∧ h 6= hr.\nIndeed, h∗ is the best classifier because err(h∗) = ·0+(1− )(1/2) = (1− )/2, while err(h) = 1/2 ∀h 6= h∗. This problem is hard because only a small fraction of examples contain information about h∗. Ideally we want to focus label queries on those informative examples while skipping the uninformative ones. However, algorithms like IWAL, or more generally, active learning algorithms that determine label query probabilities based on error differences between a pair of classifiers, query frequently on the uninformative examples. Let u(h, h′) := 1(h(x) 6= y) − 1(h′(x) 6= y) denote the error difference between two different classifiers h and h′. Let C be a random variable such that C = 1 for the case and C = 0 for the 1− case. Then it is easy to see that\nE[u(h, h′) | C = 1] =  0, h 6= h∗, h′ 6= h∗, −1/2, h = h∗, h′ 6= h∗, 1/2, h 6= h∗, h′ = h∗,\nE[u(h, h′) | C = 0] = 0, ∀h 6= h′.\nTherefore, IWAL queries all the time on uninformative examples (C = 0). Now let us consider the label complexity of Algorithm 1 on this problem. Let us focus on the query probability inside the 1− region, and fix it to some constant p. Let us also allow a query probability of 1 on the region. Then the left hand side in the constraint (5) for any classifier h is at most +P (h(X) 6= hm(X))/p ≤ +2/(p(|H|−1)), since h and hm disagree only on those points in the 1 − region where one of them is picked as the disagreeing classifier hr in the random draw. On the other hand, the RHS of the constraints is at least ξτm−1∆2m−1 ≥ ξerr(hm, Z̃m−1), which is at least ξ/4 as long as is small enough and τm is large enough for empirical error to be close to true error. Consequently, assuming that ≤ ξ/8, we find that any p ≥ 16/(ξ(|H| − 1)) satisfies the constraints. Of course we also have that p ≥ Pmin,m, which is O(1/ √ τm) in this case since errm(h∗) is a constant. Consequently, for |H| large enough p = Pmin,m is feasible and hence optimal for the population (OP). Since we find an approximately optimal solution based on Theorem 4, the label complexity at epoch m is O(1/√τm). Summing things up, it can then be checked easily that we make O( √ n) queries over n examples, a factor of √ n smaller than baselines such as IWAL and Oracular CAL on this example."
    }, {
      "heading" : "5 Efficient implementation",
      "text" : "In Algorithm 1, the computation of hm is an ERM operation, which can be performed efficiently whenever an efficient passive learner is available. However, several other hurdles remain. Testing for x ∈ Dm in the algorithm, as well\nAlgorithm 2 Coordinate ascent algorithm to solve (OP) input Accuracy parameter ε > 0. initialize λ← 0.\n1: loop 2: Rescale: λ← s · λ where s = arg maxs∈[0,1]D(s · λ).\n3: Find h̄ = arg max h∈H EX [ Imh (X) Pλ(X) ] − bm(h).\n4: if EX [ Im h̄ (X)\nPλ(X)\n] − bm(h̄) ≤ ε then\n5: return λ 6: else 7: Update λh̄ as λh̄ ← λh̄ + 2 EX [Imh̄ (X)/Pλ(X)]− bm(h̄)\nEX [Imh̄ (X)/qλ(X) 3]\n.\n8: end if 9: end loop\nas finding a solution to (OP) are considerably more challenging. The epoch schedule helps, but (OP) is still solved O(log n) times, necessitating an extremely efficient solver.\nStarting with the first issue, we follow Dasgupta et al. [2007] who cleverly observed that x ∈ Dm can be efficiently determined using a single call to an ERM oracle. Specifically, to apply their method, we use the oracle to find7 h′ = arg min{err(h, Z̃m−1) | h ∈ H, h(x) 6= hm(x)}. It can then be argued that x ∈ Dm = DIS(Am) if and only if the easily-measured regret of h′ (that is, reg(h′, hm, Z̃m−1)) is at most γ∆m−1.\nSolving (OP) efficiently is a much bigger challenge because, as an optimization problem, it is enormous: There is one variable P (x) for every point x ∈ X , one constraint (5) for each classifier h and bound constraints (6) on P (x) for every x. This leads to infinitely many variables and constraints, with an ERM oracle being the only computational primitive available. Another difficulty is that (OP) is defined in terms of the true expectation with respect to the example distribution PX , which is unavailable.\nIn the following we first demonstrate how to efficiently solve (OP) assuming access to the true expectation EX [·], and then discuss a relaxation that uses expectation over samples."
    }, {
      "heading" : "5.1 Solving (OP) with the true expectation",
      "text" : "The main challenge here is that the optimization variable P (x) is of infinite dimension. We deal with this difficulty using Lagrange duality, which leads to a dual representation of P (x) in terms of a set of classifiers found through successive calls to an ERM oracle. As will become clear shortly, each of these classifiers corresponds to the most violated variance constraint (5) under some intermediate query probability function. Thus at a high level, our strategy is to expand the set of classifiers for representing P (x) until the amount of constraint violation gets reduced to an acceptable level.\nWe start by eliminating the bound constraints using barrier functions. Notice that the objective EX [1/(1− P (x))] is already a barrier at P (x) = 1. To enforce the lower bound (6), we modify the objective to\nEX [\n1\n1− P (X)\n] + µ2EX [ 1(X ∈ Dm)\nP (X)\n] , (12)\nwhere µ is a parameter chosen momentarily to ensure P (x) ≥ Pmin,m for all x ∈ Dm. Thus, the modified goal is to minimize (12) over non-negative P subject only to (5).\nWe solve the problem in the dual where we have a large but finite number of optimization variables, and efficiently maximize the dual using coordinate ascent with access to an ERM oracle over H. Let λh ≥ 0 denote the Lagrange\n7We only have access to an unconstrained oracle. But that is adequate to solve with one constraint. See Appendix F of [Karampatziakis and Langford, 2011] for details.\nmultiplier for the constraint (5) for classifier h. Then for any λ, we can minimize the Lagrangian L(P,λ) := EX [ 1\n1− P (X)\n] +µ2EX [ 1(X ∈ Dm)\nP (X) ] − ∑ h∈H λh ( bm(h)− EX [ 1(h(X) 6= hm(X) ∧X ∈ Dm) P (X) ]) over each primal variable P (X) yielding the solution\nPλ(x) = 1(x ∈ Dm)qλ(x)\n1 + qλ(x) , where qλ(x) =\n√ µ2 + ∑ h∈H λhImh (x) (13)\nand Imh (x) = 1(h(x) 6= hm(x) ∧ x ∈ Dm). Clearly, µ/(1 + µ) ≤ Pλ(x) ≤ 1 for all x ∈ Dm, so all the bound constraints (6) in (OP) are satisfied if we choose µ = 2Pmin,m. Plugging the solution Pλ into the Lagrangian, we obtain the dual problem of maximizing the dual objective\nD(λ) = EX [ 1(X ∈ Dm)(1 + qλ(X))2 ] − ∑ h∈H λhbm(h) + C0 (14)\nover λ ≥ 0. The constant C0 is equal to 1−Pr(Dm) where Pr(Dm) = Pr(X ∈ Dm). An algorithm to approximately solve this problem is presented in Algorithm 2. The algorithm takes a parameter ε > 0 specifying the degree to which all of the constraints (5) are to be approximated. Since D is concave, the rescaling step can be solved using a straightforward numerical line search. The main implementation challenge is in finding the most violated constraint (Step 3). Fortunately, this step can be reduced to a single call to an ERM oracle. To see this, note that the constraint violation on classifier h can be written as\nEX [ Imh (X) P (X) ] − bm(h) = EX [ 1(X ∈ Dm) ( 1 P (X) − 2α2 ) 1(h(X) 6= hm(X)) ] − 2β2γτm−1∆m−1(err(h, Z̃m−1)− err(hm, Z̃m−1))− ξτm−1∆2m−1.\nThe first term of the right-hand expression is the risk (classification error) of h in predicting samples labeled according to hm with importance weights of 1/P (x)− 2α2 if x ∈ Dm and 0 otherwise; note that these weights may be positive or negative. The second term is simply the scaled risk of h with respect to the actual labels. The last two terms do not depend on h. Thus, given access to PX (or samples approximating it, discussed shortly), the most violated constraint can be found by solving an ERM problem defined on the labeled samples in Z̃m−1 and samples drawn from PX labeled by hm, with appropriate importance weights detailed in Appendix F.1.\nWhen all primal constraints are approximately satisfied, the algorithm stops. Consequently, we can execute each step of Algorithm 2 with one call to an appropriately defined ERM oracle, and approximate primal feasibility is guaranteed when the algorithm stops. More specifically, we can prove the following guarantee on the convergence of the algorithm.\nTheorem 3. When run on the m-th epoch, Algorithm 2 has the following guarantees.\n1. It halts in at most Pr(Dm) 8P 3min,mε 2 iterations.\n2. The solution λ̂ ≥ 0 it outputs has bounded `1 norm: ‖λ̂‖1 ≤ Pr(Dm)/ε.\n3. The query probability function Pλ̂ satisfies:\n• The variance constraints (5) up to an additive factor of ε, i.e.,\n∀h ∈ H EX [ 1(h(x) 6= hm(x) ∧ x ∈ Dm)\nPλ̂(X)\n] ≤ bm(h) + ε,\n• The simple bound constraints (6) exactly,\n• Approximate primal optimality: EX [ 1\n1− Pλ̂(X)\n] ≤ EX [ 1\n1− P ∗(X)\n] + 4Pmin,mPr(Dm), (15)\nwhere P ∗ is the solution to (OP).\nThat is, we find a solution with small constraint violation to ensure generalization, and a small objective value to be label efficient. If ε is set to ξτm−1∆2m−1, an amount of constraint violation tolerable in our analysis, the number of iterations in Theorem 3 varies between O(τ3/2m−1) and O(τ2m−1) as the err(hm, Z̃m−1) varies between a constant and O(1/τm−1). The theorem is proved in Appendix F.2."
    }, {
      "heading" : "5.2 Solving (OP) with expectation over samples",
      "text" : "So far we considered solving (OP) defined on the unlabeled data distribution PX , which is not available in practice. A simple and natural substitute for PX is an i.i.d. sample drawn from it. Here we show that solving a properly-defined sample variant of (OP) leads to a solution to the original (OP) with similar guarantees as in Theorem 3.\nMore specifically, we define the following sample variant of (OP). Let S be a large sample drawn i.i.d. from PX , and (OPS) be the same as (OP) except with all population expectations replaced by empirical expectations taken with respect to S. Now for any ε ≥ 0, define (OPS,ε) to be the same as (OPS) except that the variance constraints (5) are relaxed by an additive slack of ε.\nEvery time ACTIVE COVER needs to solve (OP) (Step 5 of Algorithm 1), it draws a fresh unlabeled i.i.d. sample S of size u from PX , which can be done easily in a streaming setting by collecting the next u examples. It then applies Algorithm 2 to solve (OPS,ε) with accuracy parameter ε. Note that this is different from solving (OPS) with accuracy parameter 2ε. We establish the following convergence guarantees.\nTheorem 4. Let S be an i.i.d. sample of size u from PX . When run on the m-th epoch for solving (OPS,ε) with accuracy parameter ε, Algorithm 2 satisfies the following.\n1. It halts in at most P̂r(Dm) 8P 3min,mε\n2 iterations, where P̂r(Dm) := ∑ X∈S 1(X ∈ Dm)/u.\n2. The solution λ̂ ≥ 0 it outputs has bounded `1 norm: ‖λ̂‖1 ≤ P̂r(Dm)/ε.\n3. If u ≥ O((1/(Pmin,mε)4 + α4/ε2) log(|H|/δ)), then with probability ≥ 1− δ, the query probability function Pλ̂ satisfies:\n• All constraints of (OP) except with an additive slack of 2.5ε in the variance constraints (5), • Approximate primal optimality:\nEX [\n1\n1− Pλ̂(X)\n] ≤ EX [ 1\n1− P ∗(X)\n] + 8Pmin,mPr(Dm) + (2 + 4Pmin,m)ε,\nwhere P ∗ is the solution to (OP).\nThe proof is in Appendix F.3. Intuitively, the optimal solution P ∗ to (OP) is also feasible in (OPS,ε) since satisfying the population constraints leads to approximate satisfaction of sample constraints. Since our solution Pλ̂ is approximately optimal for (OPS,ε) (this is essentially due to Theorem 3), this means that the sample objective at Pλ̂ is not much larger than P ∗. We now use a concentration argument to show that this guarantee holds also for the population objective with slightly worse constants. The approximate constraint satisfaction in (OP) follows by a similar concentration argument. Our proofs use standard concentration inequalities along with Rademacher complexity to provide uniform guarantees for all vectors λ with bounded `1 norm.\nThe first two statements, finite convergence and boundedness of ‖λ̂‖1, are identical to Theorem 3 except Pr(Dm) is replaced by P̂r(Dm). When ε is set properly, i.e, to be ξ2τm−1∆2m−1, the number of unlabeled examples u in the third statement varies between O(τ2m−1) and O(τ4m−1) as the err(hm, Z̃m−1) varies between a constant and O(1/τm−1). The third statement shows that with enough unlabeled examples, we can get a query probability function almost as good as the solution to the population problem (OP)."
    }, {
      "heading" : "6 Experiments with Agnostic Active Learning",
      "text" : "While AC is efficient in the number of ERM oracle calls, it needs to store all past examples, resulting in large space complexity. As Theorem 3 suggests, the query probability function (13) may need as many asO(τ2i ) classifiers, further increasing storage demand. In Section 6.1 we discuss a scalable online approximation to ACTIVE COVER, ONLINE ACTIVE COVER (OAC), which we implemented and tested empirically with the setup in Section 6.2. Experimental results and discussions are in Section 6.3."
    }, {
      "heading" : "6.1 Online Active Cover (OAC)",
      "text" : "Algorithm 3 gives the online approximation that we implemented, which uses an epoch schedule of τi = i, assigning every new example to a new epoch. It involves some new notations which we explain below. To make explicit the dependence of a query probability function on both a weight vector λ over classifiers and the current epoch, we write them as subscripts:\nqλ,i(x) := √ (2Pmin,i)2 + ∑ h λh1(h(x) 6= hi(x)), (20)\nPλ,i(x) := 1(x ∈ Di) qλ,i(x)\n1 + qλ,i(x) . (21)\nWe use 1h for a |H|-dimensional binary vector with 1 in the entry corresponding to the classifier h and 0 elsewhere. To explain the connections between Algorithms 1 (AC) and 3 (OAC), we start with the update of the ERM classifier and thresholds, corresponding to Step 1 of AC and Step 6 of OAC. Instead of batch ERM oracles, OAC invokes online importance weighted ERM oracles that are stateful and process examples in a streaming fashion without the need to store them. The specific importance weighted oracle we use is a reduction to online importance-weighted logistic regression [Karampatziakis and Langford, 2011] implemented in Vowpal Wabbit (VW).\nInstead of computing the query probability function by solving a batch optimization problem as in Step 5 of AC, OAC maintains a fixed number l of classifiers that are intended to be a cover of the set of good classifiers. On every new example, this cover undergoes a sequence of online, importance weighted updates (Steps 7 to 13 of OAC), which are meant to approximate the coordinate ascent steps in Algorithm 2. The importance structure (16) is derived from (57), accounting for the fact that the algorithm simply uses the incoming stream of examples to estimate EX [·] rather than a separate unlabeled sample. The same approximation is also present in the updates (17) and (18), which are online estimates of the numerator and the denominator of the additive coordinate update in Step 7 of Algorithm 2. Because (17) is an online estimate, we need to explicitly enforce non-negativity.\nFinally, Steps 9 to 14 of AC and Steps 14 to 26 of OAC perform the querying of labels. As pointed out in Section 5, the test in Step 16 of OAC is done via an online technique detailed in Appendix F of Karampatziakis and Langford [2011]."
    }, {
      "heading" : "6.2 Setup",
      "text" : "We conduct an empirical comparison of OAC with the following active learning algorithms.\n• IWAL: A slight modification of Algorithm 1 of Beygelzimer et al. [2010], which performs importance-weighted sampling of labels and maintains an unbiased estimate of classification error. In computing the query probability, rather than using a conservative, problem-independent threshold as in Beygelzimer et al. [2010], we use the following error-dependent quantity: √\nC0 log k\nk − 1 ek−1 +\nC0 log k\nk − 1 , (22)\nwhere ek−1 is the importance-weighted error estimate after the algorithm processes k − 1 examples. C0 is the only active learning hyper-parameter. The query probability is set to 1 if the error difference Gk, defined in Step 2 of Algorithm 1 of Beygelzimer et al. [2010], is smaller than the threshold (22), and otherwise a decreasing function of Gk.\nAlgorithm 3 ONLINE ACTIVE COVER input: cover size l, parameters c0 and α.\n1: Initialize online importance weighted minimization oracles {Ot}lt=0, each controlling a classifier and some associated weights {(h(t), λ(t), ν(t), ω(t))}lt=1 with all weights initialized to 0. 2: Query the first three {Yi}3i=1 and stream {(Xi, Yi, 1)}2i=1 through O0. 3: Get classifier h3 and error estimate e2 from O0, and compute Pmin,3. 4: Let Y ∗3 := Y3, Ỹ3 := h3(X3) and W3 := 1. Set β := ( √ α/c0)/10. 5: for i = 4, . . . , n, do 6: Update the ERM, the error estimate and the threshold\nhi := O0((Xi−1, Y ∗ i−1,Wi−1)),\nei−1 := (i− 2)ei−2 + 1(Ỹi−1 6= Y ∗i−1)Wi−1\ni− 1 , ∆̂i−1 := √ c0ei−1/(i− 1) + max(2α, 4)c0 log(i− 1)/(i− 1).\n7: for t = 1, . . . , l do 8: λt := ∑ t′<t λ(t′)1h(t′) . 9: St := 2α 2 − 1/Pλt,i−1(Xi−1).\n10: Set up a vector c\ncy := 1(Xi−1 ∈ Di−1)|St|1(y 6= sign(St)Ỹi−1)+ 2β2(i− 2)∆̂i−2 ( 1(Xi−1 /∈ Di−1)1(y 6= Ỹi−1) + 1(Xi−1 ∈ Di−1)\nQi−11(y 6= Yi−1) Pi−1\n) . (16)\n11: Set Y(t) := arg miny cy and W(t) := |c1 − c−1|. 12: Update\nh(t) := Ot((Xi−1, Y(t),W(t))), ν(t) := max ( ν(t) − 2 ( ch(t)(Xi−1) + min(St, 0)1(Xi−1 ∈ Di−1) ) , 0 ) , (17)\nω(t) := ω(t) + 1(h(t)(Xi−1) 6= Ỹi−1 ∧Xi−1 ∈ Di−1)\nqλt,i(Xi−1) 3\n, (18)\nλ(t) := λ(t) + ν(t) ω(t) 1 ( (ν(t), ω(t)) 6= (0, 0) ) . (19)\n13: end for 14: Receive data point Xi and predict Ỹi := hi(Xi). 15: Compute Pmin,i := min ( ( √ (i− 1)ei−1 + log(i− 1))−1, 1/2 ) . 16: if Xi ∈ Di := DIS(Ai), then 17: Compute Pi := Pλ,i(Xi), where λ := ∑l t=1 λ(t)1h(t) . 18: Draw Qi ∼ Bernoulli(Pi). 19: if Qi = 1 then 20: Query Yi and set Y ∗i := Yi,Wi := 1/Pi. 21: else 22: Set Y ∗i := 1,Wi := 0. 23: end if 24: else 25: Set Y ∗i := Ỹi,Wi := 1. 26: end if 27: end for\n• ORA-I: An Oracular-CAL [Hsu, 2010] style variant of Algorithm 3. If the test in Step 16 of Algorithm 3 is true, meaning the new example Xi is in the current disagreement region, the query probability Pi is set to 1. This algorithm does not need to maintain a cover, but still uses two tuning parameters c0 and α to compute the threshold (79). Note that both the generalization and label complexity guarantees (Theorems 1 and 2) apply to this variant if a batch ERM oracle is used.\n• ORA-II: An Oracular-CAL [Hsu, 2010] style variant of IWAL, where the query probability is set to 1 if the error difference Gk, defined in Step 2 of Algorithm 1 of Beygelzimer et al. [2010], is smaller than the threshold√\nC0 log k\nk − 1 ek−1 +\nC0 log k\nk − 1 .\nOtherwise, the algorithm uses predicted labels by the current ERM hypothesis. Note that the error estimate ek−1 now uses both the queried labels and predicted labels, and is no longer unbiased. Like IWAL, it only has one hyper-parameter C0.\n• RANDOM: Passive learning on a labeled sub-sample drawn uniformly at random.\nWe implemented these algorithms8 in Vowpal Wabbit (VW) and performed experiments on 23 binary classification datasets with varying sizes (103 to 106) and diverse feature characteristics. Details about the datasets are in Appendix G.1. For each dataset we performed a random 80/20 training/testing split, ran the five algorithms under various hyperparameter settings each for one pass over the training data, and evaluated the learned classifiers on testing data. Our goals are:\n1. Understanding the trade-offs between test error and query rate achieved by different algorithms;\n2. Comparing different algorithms when each uses the best fixed hyper-parameter setting.\nWith regard to the second goal, note that it is in general very difficult to select active learning hyper-parameters on a per-task basis because labeled validation data are not available. However, with a variety of classification datasets, it might still be reasonable to look for the single hyper-parameter setting that performs the best on average across datasets, thereby reducing over-fitting to any individual dataset, and compare different algorithms under such fixed parameter settings. More details about hyper-parameters are in Appendix G.2."
    }, {
      "heading" : "6.3 Results and Discussions",
      "text" : "Figure 2 gives a summary of the performances of different algorithms. At a fixed query rate, defined as the fraction of label queries at the end of one pass over the training data, we consider the minimum test error achievable with at most that query rate. Taking this for each algorithm on each dataset, we compute the fraction of datasets on which an algorithm wins against RANDOM. Figure 2(a), which is the same as Figure 1, plots the win fractions9 at different query rates. OAC dominates all other agnostic active learning algorithms, and outperforms RANDOM except at low query rates where it is on parity. This result also shows that RANDOM is a strong baseline at low query rates.\nFigure 2(b) reveals the magnitude of the performance differences. Here we only show results for OAC and IWAL because ORA-I and ORA-II are significantly worse. To account for the varying hardness of different datasets, we scale the test error on a per-dataset basis. Formally, let error(a, p, d) denote the test error achieved by algorithm a with hyper-parameter setting p on dataset d. Let errormax(d) and errormin(d) denote the maximum and minimum test errors on dataset d any algorithm can achieve with any hyper-parameter setting. Then we define the relative test error as\nrel err(a, p, d) = error(a, p, d)− errormin(d) errormax(d)− errormin(d) . (23)\nAs in Figure 2(a), at a fixed query rate we consider the minimum relative test error achievable with at most that query rate. Taking this for each algorithm on each dataset, we compute the improvement in relative test error with respect to\n8For IWAL we use the existing implementation by the authors in VW. 9A tie counts as 0.5.\nRANDOM. Then we plot the medians, the 25-th and the 75-th quantiles of the improvements over datasets at different query rates. OAC is on par with RANDOM across the datasets at query rates lower than 10−2, and outperforms RANDOM at higher query rates. The magnitude of improvement is not very large, but it should be noted that on many of these datasets a very large improvement is not necessarily possible. In contrast, IWAL has higher relative test errors at query rates lower than 10−2, and only becomes competitive with RANDOM at higher query rates.\nFigure 3 shows minimum test errors at different query rates for three specific datasets. They are relatively large in size (more than 105 examples) and possess different levels of difficulties. The advantage of OAC over other algorithms is clear especially at low query rates. Results for the remaining 20 datasets are in Appendix G.3.\nFigure 4 gives results obtained by different algorithms under fixed hyper-parameter settings, which are selected to optimize trade-offs between test errors and query rates across all datasets. Let query(a, p, d) be the query rate of algorithm a with hyperparameters p on dataset d. Given a weight parameter w ∈ [0, 1], we define for each algorithm a the best hyper-parameter setting as\np∗w(a) := arg min p median d perf(w, a) := {w · query(a, p, d) + (1− w) · rel err(a, p, d)} , (24)\nwhere rel err is as defined in Equation 23. Figures 4(a) and 4(b) demonstrate how well each algorithm optimizes the combined metric (24) by plotting the curve of cumulative fraction of datasets on which an algorithm, with parameters fixed at p∗w(a) across datasets, achieves no more than a certain value of the weighted sum perf(w, a) in (24). A higher curve thus means a better performance. When the weight on query rate is 0.1, perf(w, a) tends to care mostly about test error, and the unbiased nature of IWAL possibly helps it outperform other algorithms at high query rates, while OAC is competitive at low query rates. At the other extreme when the weight is 0.9, query rate matters much more and OAC is superior to others.\nFigure 5 plots relative test errors and query rates across all datasets achieved by each algorithm using its best single parameter p∗w(a) in (24), with w varying from 0.1 to 0.9. The markers are plotted at\n(median d {query(a, p∗w(a), d)},median d {rel err(a, p∗w(a), d)})\nfor each algorithm a, and the vertical bars extend from the 25th to the 75th quantile of each algorithm a’s relative test errors achieved with p∗w(a) across datasets. OAC in general achieves test errors comparable to the other algorithms, but at lower query rates.\nHow much headroom for improvement is there by a better automatic tuning of hyperparameters? In addition to\nresults obtained with a fixed hyper-parameter setting, we examine performances of different algorithms when each uses the best hyper-parameter on a per-dataset basis. Figure 6 is the counterpart of Figure 4 in this setting, showing the cumulative fractions of datasets for which each algorithm a achieves a certain value of the performance metric perf in (24) when using dataset-dependent best hyper-parameters:\np∗w(a, d) := arg min p min d perf(w, a) :=\n{ w · query(a, p, d) + (1− w) · error(a, p, d)− errormin(d)\nerrormax(d)− errormin(d)\n} .\nFigure 6 suggests the possibility that with the right hyper-parameter settings, OAC may dominate all other algorithms at both extremes of the query-rate vs. test-error trade-off. Overall, we find that OAC achieves reasonable performance gains in terms of good generalization with a few labeled examples, compared with a number of baselines on a diverse collection of datasets."
    }, {
      "heading" : "7 Analysis of generalization ability",
      "text" : "In this section we present the main framework and analysis for the results on the generalization properties of the ACTIVE COVER algorithm. Our analysis is broken up into several steps. We start by setting up some additional notation for the proofs. Our analysis relies on two deviation bounds for the empirical regret and the empirical error of the ERM classifier. These are obtained by appropriately applying Freedman-style concentration bounds for martingales. Both these bounds depend on the variance and range of our error and regret estimates for all classifiers h ∈ H, and these quantities are controlled using the constraints (5) and (6) in the definition of the optimization problem (OP). Since our data consists of examples from different epochs, which use different query probabilities Pm, the above steps with appropriate manipulations yield bounds for the epoch m, in terms of various quantities involving the previous epochs. Theorem 1 and its corollaries are then obtained by setting up appropriate inductive claims. We make this intuition precise in the following sections."
    }, {
      "heading" : "7.1 Framework for generalization analysis",
      "text" : "Before we can prove our main results, we recall some notations and introduce a few additional ones. We also prove some technical lemmas in this section which are used to prove our main results.\nRecall the notation reg(h, h′) := err(h)− err(h′), h∗ ∈ arg minh∈H err(h), reg(h) := reg(h, h∗). Let Zm denote the set of importance-weighted examples in Z̃m, and the corresponding empirical error is denoted as:\nerr(h, Zm) := 1\nτm m∑ j=1 τj∑ i=τj−1+1 (Qi1(h(Xi) 6= Yi ∧Xi ∈ Dj) Pj(Xi) ) . (25)\nTaking expectations, we define the following quantities with respect to the sequence of regions {Dm}:\nerrm(h) := EX,Y [1(h(X) 6= Y ∧X ∈ Dm)], (26)\nerrm(h) := 1\nτm m∑ j=1 (τj − τj−1)errj(h).\nIntuitively, errm captures the population error of h, restricted to only the examples in the disagreement region. This is also the expectation of the sample error restricted to the importance-weighted examples in epoch m. Averaging these quantities, we obtain errm which is the expectation of the sample error over Zm. Centering around the corresponding errors of h∗, we obtain the following regret terms:\nregm(h) := errm(h)− errm(h∗),\nregm(h) := 1\nτm m∑ j=1 (τj − τj−1)regj(h).\nWhile the above quantities only concern the importance-weighted examples, it is also useful to measure error and regret terms over the entire biased sample. We define the empirical error and regret on Z̃m as follows:\nerr(h, Z̃m)\n:= 1\nτm m∑ j=1 τj∑ i=τj−1+1 ( 1(h(Xi) 6= hj(Xi) ∧Xi /∈ Dj) + Qi1(h(Xi) 6= Yi ∧Xi ∈ Dj) Pj(Xi) ) ,\nreg(h, h′, Z̃m) := err(h, Z̃m)− err(h′, Z̃m),\nand the associated expected regret:\nreg‡m(h, h ′) := EX [(1(h(X) 6= hm(X))− 1(h′(X) 6= hm(X)))1(X /∈ Dm)] +\nEX,Y [(1(h(X) 6= Y )− 1(h′(X) 6= Y ))1(X ∈ Dm)], (27)\nr̃egm(h, h ′) :=\n1\nτm m∑ j=1 (τj − τj−1)reg‡j(h, h ′). (28)\nTo simplify notation, we sometimes use the shorthand reg(h, Z̃m) := reg(h, hm+1, Z̃m). The quantity r̃egm(h, h ′) will play quite a central role in our analysis as it is the expectation of the empirical regret of h relative to h′ on our biased sample Z̃m. We also recall the earlier notations\n∆m := c1 √ merr(hm+1, Z̃m) + c2 m log τm,\nAm+1 := {h | err(h, Z̃m)− err(hm+1, Z̃m) ≤ γ∆m}, and\n∆∗m :=\n{( c1 √ merrm(h∗) + c2 m log τm ) , m ≥ 1.\n∆0, m = 0.\nThroughout the paper, we adopt the convention that the quantities (26) to (3) take the value of zero when m = 0. We use the shorthand m(i) to denote the epoch containing example i.\nWith the notations in place, we start with an extremely important lemma, which shows that the biased sample Z̃ which we create introduces a bias in the favor of good hypotheses, overly penalizing the bad hypotheses while favorably evaluating the optimal h∗.\nLemma 1 (Favorable Bias). ∀m ≥ 1,∀h̄ ∈ Am,∀h ∈ H, the following holds:\nreg‡m(h, h̄) ≥ reg(h, h̄).\nThe next key ingredient for our proofs is a deviation bound, which will be appropriately used to control the deviation of the empirical regret and error terms. Lemma 2 (Deviation Bounds). Pick 0 < δ < 1/e such that |H|/δ > √\n192. With probability at least 1 − δ the following holds. For all (h, h′) ∈ H2 and ∀m ≥ 1,\n|r̃egm(h, h′)− reg(h, h′, Z̃m)|\n≤ √√√√ m τm m∑ i=1 (τi − τi−1)EX [( 1(X /∈ Di) + 1(X ∈ Di) Pi(X) ) 1(h(X) 6= h′(X)) ] +\nm Pmin,m , (29)\n|err(h, Zm)− errm(h)|\n≤ √√√√ m τm m∑ i=1 (τi − τi−1)EX,Y [ 1(X ∈ Di ∧ h(X) 6= Y ) Pi(X) ] + m Pmin,m , (30)\nwhere\nm := 32\n( log(|H|/δ) + log τm\nτm\n) .\nThe lemma is obtained by applying a form of Freedman’s inequality presented in Appendix A. Intuitively, the deviations are small so long as the average importance weights over the disagreement region and the minimum query probability over the disagreement region are well-behaved. This lemma also highlights why r̃egm is a very natural quantity for our analysis, since the empirical regret on our biased sample Z̃ concentrates around it.\nTo keep the handling of probabilities simple, we assume for the bulk of this section that the conclusions of Lemma 2 hold deterministically. The failure probability is handled once at the end to establish our main results. Let E denote the event that the assertions of Lemma 2 hold deterministically, and we know that Pr(EC) ≤ δ. Based on the above lemma, we obtain the following propositions for the concentration of empirical regret and error terms.\nProposition 1 (Regret concentration). Fix an epoch m ≥ 1. Suppose the event E holds and assume that h∗ ∈ Aj for all epochs j ≤ m.\n|reg(h, h∗, Z̃m)− r̃egm(h, h∗)|\n≤ 1 4 r̃egm(h) + 2α √√√√ m τm m∑ i=1 (τi − τi−1)regi(hi) + 2α √ 3errm(h∗) m\n+ β √√√√2γ m∆m m∑ i=1 (τi − τi−1)(reg(h, Z̃i−1) + reg(h∗, Z̃i−1)) + 4∆m\nWe need an analogous result for the empirical error of the ERM at each epoch.\nProposition 2 (Error concentration). Fix an epoch m ≥ 1. Suppose the event E holds and assume that h∗ ∈ Aj for all epochs j ≤ m.\n|errm(h∗)− err(hm+1, Z̃m)| ≤ errm(h\n∗)\n2 + 3∆m 2 + reg(h∗, hm+1, Z̃m).\nWe now present the proofs of our main results based on these propositions."
    }, {
      "heading" : "7.2 Proofs of main results",
      "text" : "We prove a more general version of the theorem. Theorem 1 and its corollaries follow as consequences of this more general result.\nTheorem 5. For all epochs m = 1, 2, . . . ,M and all h ∈ H, we have with probability at least 1− δ\n|reg(h, h∗, Z̃m)− r̃egm(h, h∗)| ≤ 1\n2 r̃egm(h, h\n∗) + η\n4 ∆m, (31)\nreg(h∗, hm+1, Z̃m) ≤ η∆m\n4 , (32)\n|errm(h∗)− err(hm+1, Z̃m)| ≤ errm(h\n∗)\n2 + η 2 ∆m. (33)\nThe theorem is proved inductively. We first give the proof outline for this theorem, and then show how Theorem 1 and its corollaries follow."
    }, {
      "heading" : "7.2.1 Proof of Theorem 5",
      "text" : "The theorem is proved via induction. Let us start with the base case for m = 1. Clearly,\n|reg(h, h∗, Z̃1)− r̃eg1(h, h∗)| ≤ 1 ≤ η∆1/4,\nsince Pmin,1 = 1. The conclusions for the second and third statements follow similarly. This establishes the base case. Let us now assume that the hypothesis holds for i = 1, 2, . . . ,m− 1 and we establish it for the epoch i = m. We start from the conclusion of Proposition 1, which yields\n|reg(h, h∗, Z̃m)− r̃egm(h, h∗)|\n≤ 1 4 r̃egm(h) + 2α √√√√ m τm m∑ i=1\n(τi − τi−1)regi(hi)︸ ︷︷ ︸ T1\n+ 2α √\n3errm(h∗) m︸ ︷︷ ︸ T2\n+ β √√√√2γ m∆m m∑ i=1\n(τi − τi−1)(reg(h, Z̃i−1) + reg(h∗, Z̃i−1))︸ ︷︷ ︸ T3 +4∆m\nWe now control T1, T2 and T3 in the sum using our inductive hypothesis and the propositions in a series of lemmas. To state the lemmas cleanly, let Em refer to the event where the bounds (31)-(33) hold at epoch m. Then we have the following lemmas. The first lemma gives a bound on T1.\nLemma 3. Suppose that the event E holds and that the events Ei hold for all epochs i = 1, 2, . . . ,m − 1. Then we have\n2α √√√√ m τm m∑ i=1 (τi − τi−1)regi(hi) ≤ η∆m 12 + 24α2 m log τm.\nIntuitively, the lemma holds since Lemma 1 allows us to bound regi(hi) with r̃egi−1(hi). The latter is then controlled using the event Ei. Some algebraic manipulations then yield the lemma, with a detailed proofs in Appendix C. We next present a lemma that helps us control T2.\nLemma 4. Suppose that the event E holds and that the events Ei hold for all epochs i = 1, 2, . . . ,m − 1. Then we have\n2α √ 3errm(h∗) m ≤ 2α √ 6 merr(hm+1, Z̃m) + ∆m + 1\n4 reg(h∗, hm+1, Z̃m) + 33α 2 m.\nThe lemma follows more or less directly from Proposition 2 combined with some algebra. Finally, we present a lemma to bound T3.\nLemma 5. Suppose that the event E holds and that the events Ei hold for all epochs i = 1, 2, . . . ,m − 1. Then we have\nβ √√√√2γ m∆m m∑ i=1 (τi − τi−1)(reg(h, Z̃i−1) + reg(h∗, Z̃i−1)) ≤ 1 4 r̃egm(h, h ∗) + 7η∆m 72 .\nThe reg(h∗, hi, Z̃i−1) terms in the lemma are bounded directly due to the event Ei. For the second term, we observe that the empirical regret of h relative to hi is not too different from the empirical regret to h∗ (since h∗ has a small empirical regret by Ei). Furthermore, the empirical regret to h∗ is close to r̃egi−1(h, h∗) by the event Ei. These observations, along with some technical manipulations yield the lemma.\nGiven these lemmas, we can now prove the theorem in a relatively straightforward manner. Given our inductive hypothesis, the events Ei indeed hold for all epochs i = 1, 2, . . . ,m − 1 which allows us to invoke the lemmas. Substituting the above bounds on T1 from Lemma 3, T2 from Lemma 4 and T3 from 5 into Proposition 1 yields\n|reg(h, h∗, Z̃m)− r̃egm(h, h∗)|\n≤ 1 4 r̃egm(h) + η∆m 12 + 24α2 m log τm + 2α\n√ 6 merr(hm+1, Z̃m) + ∆m\n+ 1\n4 reg(h∗, hm+1, Z̃m) + 33α\n2 m + 1\n4 r̃egm(h, h\n∗) + 7η∆m\n72 + 4∆m\n≤ 1 2 r̃egm(h, h ∗) + 57α2 m log τm + 13η 72 ∆m + 2α\n√ 6 merr(hm+1, Z̃m) + 5∆m\n+ 1\n4 reg(h∗, hm+1, Z̃m)\nFurther recalling that c1 ≥ 2α √ 6 and c2 ≥ 57α2 by our assumptions on constants, we obtain\n|reg(h, h∗, Z̃m)− r̃egm(h, h∗)| ≤ 1\n2 r̃egm(h, h\n∗) + 13η\n72 ∆m + 6∆m +\n1 4 reg(h∗, hm+1, Z̃m). (34)\nTo complete the proof of the bound (31), we now substitute h = hm+1 in the above bound, which yields\n1 2 r̃egm(hm+1, h ∗)− 5 4 reg(h, h∗, Z̃m) ≤ 13η 72 ∆m + 6∆m.\nSince h∗ ∈ Ai for all epochs i ≤ m, we have r̃egm(h, h∗) ≥ reg(h, h∗) ≥ 0 for all classifiers h ∈ H. Consequently, we see that\nreg(h∗, hm+1, Z̃m) = −reg(hm+1, h∗, Z̃m) ≤ 52η\n360 ∆m +\n24\n5 ∆m ≤\nη 4 ∆m, (35)\nwhere the last inequality uses the condition 38η ≥ 1728. We can now substitute this back into our earlier bound (34) and obtain\n|reg(h, h∗, Z̃m)− r̃egm(h, h∗)|\n≤ 1 2 r̃egm(h, h ∗) + 13η 72 ∆m + 6∆m + η 16 ∆m ≤ 1 2 r̃egm(h, h ∗) + η 4 ∆m,\nwhere we use the condition η/144 ≥ 6. This completes the proof of the first part of our inductive claim. For the second part, this is almost a by product of the first part through Equation (35). Recalling that γ ≥ η/4 by assumption, this ensures that h∗ ∈ Am+1. We next establish the third part of the claim. This is obtained by combining our bound (35) with Proposition 2. We have\n|errm(h∗)− err(hm+1, Z̃m)| ≤ errm(h\n∗)\n2 + 3∆m 2 + reg(h∗, hm+1, Z̃m)\n≤ errm(h ∗)\n2 + 3∆m 2 + η∆m 4\n≤ errm(h ∗)\n2 + η∆m 2 ,\nsince η ≥ 6. This completes the third part. Finally, note that our analysis has been conditioned on the event E so far. By Lemma 2, Pr(EC) ≤ δ, which completes the proof of the theorem. We now provide a proof for Theorem 1."
    }, {
      "heading" : "7.2.2 Proof of Theorem 1",
      "text" : "We only prove the first part of the theorem. The second part is simply a restatement of the inequality (32) in Theorem 5. The first part is essentially a restatement of (31) in Theorem 5, except the bound uses ∆∗m instead of ∆m. In order to prove the theorem, pick any epoch m ≤M and h ∈ Am+1. Because h∗ ∈ Aj , 1 ≤ j ≤ m+ 1, we have by Lemma 1 that reg(h) ≤ r̃egm(h, h∗). It then suffices to bound r̃egm(h, h ∗). By the deviation bound (31), we have\nr̃egm(h, h ∗) ≤ reg(h, h∗, Z̃m) +\n1 2 r̃egm(h, h ∗) + η 4 ∆m\n≤ reg(h, hm+1, Z̃m) + 1\n2 r̃egm(h, h\n∗) + η\n4 ∆m\n≤ 1 2 r̃egm(h, h ∗) +\n( γ + η\n4\n) ∆m.\nRearranging terms leads to r̃egm(h, h\n∗) ≤ 4γ∆m because γ ≥ η/4. Now we show that ∆m ≤ 4∆∗m, which leads to the desired result. It is trivially true for m = 1 because ∆∗1 = ∆1. For m ≥ 2, by the deviation bound on the empirical error (33) we have\n∆m ≤ c1 √ m ( 3\n2 errm(h∗) +\nη 2 ∆m\n) + c2 m log τm\n≤ 2c1 √ merrm(h∗) + √ c21 mη\n2 ∆m + c2 m log τm\n≤ 2c1 √ merrm(h∗) + c21 mη\n4 + ∆m 2 + c2 m log τm\n≤ 2∆∗m + ∆m 2 ,\nwhere the last inequality uses our choice of constants c21η/4 ≤ c2. Rearranging terms completes the proof."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this paper, we proposed a new algorithm for agnostic active learning in a streaming setting. The algorithm has strong theoretical guarantees, maintaining good generalization properties while attaining a low label complexity in favorable settings. Specifically, we show that the algorithm has an optimal performance in a disagreement-based analysis of label complexity, as well in special cases such as realizable problems and under Tsybakov’s low-noise condition. Additionally, we present an interesting example that highlights the structural difference between our algorithm and some predecessors in terms of label complexities. Indeed a key improvement of our algorithm is that we do not always need to query over the entire disagreement region–a limitation of most computationally efficient predecessors. This is achieved through a careful construction of an optimization problem defining good query probability functions, which relies on using refined data-dependent error estimates.\nThe strong theoretical properties of our algorithm are also mirrored in the extensive empirical evaluation of an online variant, which performs well against a number of strong baselines across a suite of 23 datasets. Indeed this comprehensive empirical evaluation on a range of diverse datasets has not been previously done for agnostic active learning algorithms before to our knowledge, and is a key contribution of this work.\nWe believe that our work naturally leads to several interesting directions for future research. As the example in Section 4.2.2 reveals, the worst-case label complexity analysis in Theorem 2 is rather pessimistic. It would be interesting to obtain sharper characterization of the label complexity, by exploiting the structure of the query probability function over the disagreement region. This would likely involve understanding more fine-grained properties that make a problem easy or hard for active learning beyond the disagreement coefficient, and such a development might also lead to better algorithms. A limitation of the current theory is the somewhat poor dependence in Theorem 4 on the number of unlabeled examples needed to solve the optimization problem. Ideally, we would like to be able to use O(τm) unlabeled examples to solve (OP) at epoch m, and improving this dependence is perhaps the most important direction for future work. Finally, while AC is extremely attractive from a theoretical standpoint, a direct implementation still seems somewhat impractical. Obtaining theory for an algorithm even closer to the practical variant OAC would be an important step in bringing the theory and implementation closer."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to thank Kamalika Chaudhuri for helpful initial discussions."
    }, {
      "heading" : "A Deviation bound",
      "text" : "We use an adaptation of Freedman’s inequality [Freedman, 1975] as the main concentration tool.\nLemma 6. Let X1, X2, . . . , Xn be a martingale difference sequence adapted to the filtration Fi. Suppose there exists a function bn of X1, . . . , Xn that satisfies\n∀1 ≤ i ≤ n, |Xi| ≤ bn, 1 ≤ bn ≤ bmax,\nwhere bmax is a non-random quantity that may depend on n. Define\nSn := n∑ i=1 Xi,\nVn := n∑ i=1 E[X2i | Fi−1].\nPick any 0 < δ < 1/e2 and n ≥ 3. We have Pr ( Sn ≥ 2 √ Vn log(1/δ) + 3bn log(1/δ) ) ≤ 4 √ δ(2 + log2 bmax) log n.\nProof. Define rj := 2j for −1 ≤ j ≤ m := dlog2 bmaxe. Then we have Pr ( Sn ≥ 2 √ Vn log(1/δ) + 3bn log(1/δ) ) =\nm∑ j=0 Pr ( Sn ≥ 2 √ Vn log(1/δ) + 3bn log(1/δ) ∧ rj−1 < bn ≤ rj ) ≤\nm∑ j=0 Pr ( Sn ≥ 2 √ Vn log(1/δ) + 3rj−1 log(1/δ) ∧ bn ≤ rj )\n≤ m∑ j=0 Pr\n( Sn ≥ 2 √ Vn log(1/δ)\n2 + 3rj\nlog(1/δ)\n2 ∧ bn ≤ rj\n)\n≤ m∑ j=0 4(log n) √ δ (36) ≤ 4 √ δ(2 + log2 bmax) log n,\nwhere (36) is a direct consequence of Lemma 3 of Kakade and Tewari [2009] and the others result from simple algebra."
    }, {
      "heading" : "B Auxiliary results for Theorem 1",
      "text" : "Before presenting our regret analysis, we first establish several useful results.\nLemma 7. The threshold defined in (2) and the minimum probability Pmin,m defined in (7) satisfy the following for all m ≥ 1,\nτm−1∆m−1 ≤ τm∆m, (37) Pmin,m ≥ Pmin,m+1, (38) m\nPmin,m ≤ ∆m. (39)\nProof. Notice that\nτm−1 m−1 = 32(log(|H|/δ) + log τm−1) ≤ 32(log(|H|/δ) + log τm) = τm m. (40)\nWe first prove (37). It holds trivially for m = 1. For m ≥ 2 we have\nτm−1∆m−1\n= c1 √ τ2m−1 m−1err(hm, Z̃m−1) + c2τm−1 m−1 log τm−1\n≤ c1 √ (τm−1 m−1)τm−1err(hm+1, Z̃m−1) + c2τm−1 m−1 log τm−1\n≤ c1 √ (τm m)τmerr(hm+1, Z̃m) + c2τm m log τm\n= τm∆m,\nwhere the first inequality is by the fact that hm minimizes the empirical error on Z̃m−1 and the second inequality is by τm−1 m−1 ≤ τm m. Then for (38), it is easy to see√\nτm−1err(hm, Z̃m−1)\nn M + log τm−1\n≤\n√ τm−1err(hm+1, Z̃m−1)\nn M + log τm−1\n≤\n√ τmerr(hm+1, Z̃m)\nn M + log τm,\nfor m ≥ 1, implying Pmin,m ≥ Pmin,m+1. Finally to prove (39), we have that\nm Pmin,m ≤ m Pmin,m+1\n= max\n √ τm 2merr(hm+1, Z̃m)/(n M ) + m log τm\nc3 , 2 m  ≤ max  √ merr(hm+1, Z̃m) + m log τm\nc3 , 2 m  ≤ ∆m,\nwhere the second inequality is by τm m ≤ n M , and the third inequality is by our choices of c1, c2 and c3.\nWe also need a lemma regarding the epoch schedule.\nLemma 8. Let τm−1 < τm ≤ 2τm−1 for all m > 1. Then we have for all m ≥ 1, m∑ i=1 τi+1 − τi τi ≤ 4 log τm+1,\nm∑ i=1 (τi − τi−1)∆i−1 ≤ 4τm∆m log τm.\nProof. Note that we can rewrite the summation in question as\nm∑ i=1 τi+1 − τi τi = m∑ i=1 τi+1∑ j=τi+1 1 τi\n≤ m∑ i=1 τi+1∑ j=τi+1 2 τi+1 ,\nwhere the second inequality uses our assumption on epoch lengths. The summation can then be further bounded as\nm∑ i=1 τi+1 − τi τi ≤ m∑ i=1 τi+1∑ j=τi+1 2 j ≤ τm+1∑ i=1 2 i\n≤ 2(1 + log τm+1) (41) ≤ 4 log τm+1,\nwhere the third inequality is by the bound ∑n i=1 1/i ≤ 1 + log n, and the final inequality is by 1 ≤ log τm,m ≥ 1. To prove the second bound in the lemma, we write\nm∑ i=1 (τi − τi−1)∆i−1 = τ1∆0 + m−1∑ i=1 (τi+1 − τi)∆i\n= τ1∆0 + m−1∑ i=1 τi+1 − τi τi τi∆i ≤ τ1∆0 + (2 + 2 log τm)τm∆m ≤ (2 log τ1 − 2)τ1∆1 + (2 + 2 log τm)τm∆m ≤ (2 log τm − 2)τm∆m + (2 + 2 log τm)τm∆m = 4τm∆m log τm,\nwhere the first inequality is by (41) and τi∆i ≤ τm∆m (Lemma 7), the second inequality is by our choice of ∆0 and the fact that τ1∆1 ≤ 1, and the third inequality again uses τi∆i ≤ τm∆m."
    }, {
      "heading" : "C Proofs omitted from Section 7.2",
      "text" : "We now provide the proofs of the lemmas and propositions from Section 7.2 that were used in proving Theorem 1. We start with proofs of Lemmas 1 and 2. Proof of Lemma 1\nPick any m ≥ 1, h ∈ H and h̄ ∈ Am. Note that the definitions of reg‡m(h, h̄) and reg(h, h̄) only differ on X /∈ Dm := DIS(Am), and ∀X /∈ Dm, h̄(X) = hm(X). We thus have\nreg‡m(h, h̄)− reg(h, h̄) = EX,Y [ 1(X /∈ Dm) (( 1(h(X) 6= hm(X))− 1(h̄(X) 6= hm(X)) ) − ( 1(h(X) 6= Y )− 1(h̄(X) 6= Y )\n))] = EX,Y [1(X /∈ Dm) ( 1(h(X) 6= hm(X))− (1(h(X) 6= Y )− 1(hm(X) 6= Y )) ) ].\nThe desired result then follows from the inequality that\n1(h(X) 6= Y )− 1(hm(X) 6= Y ) ≤ 1(h(X) 6= hm(X)).\nProof of Lemma 2 Our proof strategy is to apply Lemma 6 to establish concentration of properly defined martingale difference sequences for fixed classifiers h, h′ and some epochm, and then use a union bound to get the desired statement. First we look at the concentration of the empirical regret on Z̃m. To avoid clutter, we overload our notation so thatDi = Dm(i), hi = hm(i) and Pi = Pm(i) when i is the index of an example rather than a round.\nFor any pair of classifiers h and h′, we define the random variables for the instantaneous regrets:\nR̃i := 1(Xi /∈ Di)(1(h(Xi) 6= hi(Xi))− 1(h′(Xi) 6= hi(Xi))) + 1(Xi ∈ Di)(1(h(Xi) 6= Yi)− 1(h′(Xi) 6= Yi))Qi/Pi(Xi)\nand the associated σ-fields Fi := σ({Xj , Yj , Qj}ij=1). We have that R̃i is measurable with respect to Fi. Therefore R̃i −E[R̃i | Fi−1] forms a martingale difference sequence adapted to the filtrations Fi, i ≥ 1, and\nE[R̃i | Fi−1] = reg‡m(i)(h, h ′)\naccording to (27) and the fact that Xi, Yi, Qi are independent from the past. To use Lemma 6, we first identify an upper bound on elements in the sequence:\n|R̃i −E[R̃i | Fi−1]| = |R̃i − reg‡m(i)(h, h ′)| ≤ max(R̃i, reg‡m(i)(h, h ′))\n≤ 1 Pmin,m(i) ≤ 1 Pmin,m , (42)\nfor all i such that m(i) ≤ m, where the last inequality is by Lemma 7. The definition of Pmin,m implies that\n1\nPmin,m ≤ max(\n√ τm−1/(n M ) + log τm−1, 2) ≤ 2 √ τm−1 + 1 (43)\nbecause n M ≥ 1. Then we consider the conditional second moment. Using the fact that\n(1(h(Xi) 6= Yi)− 1(h′(Xi) 6= Yi))2 ≤ 1(h(Xi) 6= h′(Xi)), (44)\nwe get\nE[(R̃i −E[R̃i | Fi−1])2 | Fi−1] = E[(R̃i − reg‡m(i)(h, h ′))2 | Fi−1] ≤ E[R̃2i | Fi−1]\n≤ E [( 1(Xi /∈ Di) +\n1(Xi ∈ Di)Qi Pi(Xi)\n)2 1(h(Xi) 6= h′(Xi)) | Fi−1 ]\n= E [( 1(Xi /∈ Di) +\n1(Xi ∈ Di)Qi Pi(Xi)2\n) 1(h(Xi) 6= h′(Xi)) | Fi−1 ] = E [( 1(Xi /∈ Di) +\n1(Xi ∈ Di) Pi(Xi)\n) 1(h(Xi) 6= h′(Xi)) | Fi−1 ] = EX [( 1(X /∈ Di) +\n1(X ∈ Di) Pi(X)\n) 1(h(X) 6= h′(X)) ] = EX [( 1(X /∈ Dm(i)) +\n1(X ∈ Dm(i)) Pm(i)(X)\n) 1(h(X) 6= h′(X)) ] (45)\nwhere the last two equalities are from the fact that Xi is independent from the past and replacing our overloaded notation respectively. Lemma 6 with (42), (43), and (45) then implies for any 0 < δm < 1/e2 and m ≥ 1, the following holds with probability at most 8 √ δm(2 + log2(2 √ τm−1 + 1)) log τm:\n|reg(h, h′, Z̃m)− r̃egm(h, h′)|\n≥ √√√√4 log(1/δm) τ2m m∑ i=1 (τi − τi−1)EX [( 1(X /∈ Di) + 1(X ∈ Di) Pi(X) ) 1(h(X) 6= h′(X)) ]\n+ 4 log(1/δm)\nnPmin,m . (46)\nThen we consider the concentration of the empirical error on the importance-weighted examples. Define the random examples for the empirical errors:\nEi := Qi1(h(Xi) 6= Yi ∧Xi ∈ Di)\nPi(Xi)\nand the associated σ-fields Fi := σ({Xj , Yj , Qj}ij=1). By the same analysis of the sequence of instantaneous regrets, we have Ei − E[Ei | Fi−1] is a martingale difference sequence adapted to the filtrations Fi, i ≥ 1, with the following properties:\nE[Ei | Fi−1] = E[1(Xi ∈ Di ∧ h(Xi) 6= Yi) | Fi−1] = errm(i)(h),\n|Ei − E[Ei | Fi−1]| ≤ 1 Pmin,m(i) ≤ 1 Pmin,m\n≤ 2 √ τm−1 + 1,\nfor all i such that m(i) ≤ m. Furthermore,\nE[(Ei − E[Ei | Fi−1])2 | Fi−1] ≤ E [ 1(Xi ∈ Di ∧ h(Xi) 6= Yi)\nPi(Xi) ∣∣∣∣ Fi−1] = EX,Y [ 1(X ∈ Di ∧ h(X) 6= Y )\nPi(X)\n] .\nWith these properties, Lemma 6 then implies for any 0 < δm < 1/e2 and m ≥ 1, the following holds with probability at most 8 √ δm(2 + log2(2 √ τm−1 + 1)) log τm:\n|err(h, Zm)− errm(h)| ≥ √√√√4 log(1/δm) τ2m m∑ i=1 (τi − τi−1)EX,Y [ 1(X ∈ Di ∧ h(X) 6= Y ) Pi(X) ]\n+ 4 log(1/δm)\nnPmin,m . (47)\nSetting\nδm =\n( δ\n192|H|2τ2m(log τm)2 )2 ensures that the probability of the union of the bad events (46), and (47) over all pairs of classifiers h, h′ and m ≥ 1 is bounded by δ > 0. Choosing δ ≤ |H|/ √ 192, we have\nlog(1/δm) = 2 log\n( 192|H|2τ2m(log τm)2\nδ ) ≤ 2(2 log(|H|/δ) + 4 log τm + log 192) ≤ 8(log(|H|/δ) + log τm),\nleading to the desired statement.\nWe then provide the proofs of Propositions 1 and 2. Proof of Proposition 1 By the inequality (29) of Lemma 2, we have\n|reg(h, h∗, Z̃m)− r̃egm(h, h∗)|\n≤ √√√√√√√ m τm m∑ i=1 (τi − τi−1)EX [( 1(X /∈ Di) + 1(X ∈ Di) Pi(X) ) 1(h(X) 6= h∗(X)) ] ︸ ︷︷ ︸\ndevm(h)\n+ m\nPmin,m (48)\nWe now control the term devm(h) in order to establish the proposition. We have\nτm m devm(h)\n= m∑ i=1 (τi − τi−1)EX [( 1(X ∈ Di) Pi(X) + 1(X /∈ Di) ) 1(h(X) 6= h∗(X)) ]\n≤ m∑ i=1 (τi − τi−1)EX [ 1(X ∈ Di) Pi(X) ( 1(h(X) 6= hi(X)) + 1(h∗(X) 6= hi(X)) ) + 1(X /∈ Di)1(h(X) 6= h∗(X))\n] ≤\nm∑ i=1 (τi − τi−1)EX [ 2α21(X ∈ Di) ( 1(h(X) 6= hi(X)) + 1(h∗(X) 6= hi(X)) ) + 2β2γτi−1∆i−1(reg(h, Z̃i−1) + reg(h ∗, Z̃i−1)) + 2ξτi−1∆ 2 i−1\n+ 1(h(X) 6= h∗(X) ∧X /∈ Di) ] ,\nwhere the second inequality uses our variance constraints in defining the distribution Pi for classifiers h and h∗. Note that\n1(h(X) 6= h∗(X)) ≤ 1(h(X) 6= Y ) + 1(h∗(X) 6= Y ) = (1(h(X) 6= Y )− 1(h∗(X) 6= Y )) + 21(h∗(X) 6= Y ),\nso that the final inequality can be rewritten as\nτm m devm(h)\n≤ m∑ i=1 (τi − τi−1) [ 2α2(regi(h) + 2regi(hi)) + 12α 2erri(h ∗) + 2β2γτi−1∆i−1(reg(h, Z̃i−1)\n+ reg(h∗, Z̃i−1)) + 2ξτi−1∆ 2 i−1 + EX [1(h(X) 6= h∗(X) ∧X /∈ Di)]\n] .\nWith the assumptions α ≥ 1 and h∗ ∈ Ai for all epochs i ≤ m, the first term regi(h) can be combined with the last disagreement term and bounded by 2α2reg‡i (h). Further noting that τi−1∆i−1 ≤ τm∆m by Lemma 7, we can further\nsimplify the inequality to\nτm m devm(h) ≤ 2α2 m∑ i=1 (τi − τi−1)reg‡i (h) + 4α 2 m∑ i=1 (τi − τi−1)regi(hi) + 12τmα2errm(h∗)\n+ 2β2γτm∆m m∑ i=1 (τi − τi−1)(reg(h, Z̃i−1)\n+ reg(h∗, Z̃i−1)) + 2ξ m∑ i=1 (τi − τi−1)τi−1∆2i−1.\nThe first summand is simply 2α2τmr̃egm(h) by definition. The final summand above can be bounded using Lemmas 7 and 8 since\nm∑ i=1 (τi − τi−1)τi−1∆2i−1 = m−1∑ i=1 (τi+1 − τi)τi∆2i ≤ τm∆m m−1∑ i=1 (τi+1 − τi)∆i\n≤ 4τ2m∆2m log τm.\nSubstituting the above inequalities back, we obtain\nτm m devm(h) ≤ 2α2τmr̃egm(h) + 4α2 m∑ i=1 (τi − τi−1)regi(hi) + 12τmα2errm(h∗)\n+ 2β2γτm∆m m∑ i=1 (τi − τi−1)(reg(h, Z̃i−1) + reg(h∗, Z̃i−1)) + 8ξτ2m∆2m log τm.\nSince √ a+ b ≤ √ a+ √ b, we can further bound\n√ devm(h) ≤ √ 2α2 mr̃egm(h) + 2α √√√√ m τm m∑ i=1 (τi − τi−1)regi(hi) + 2α √ 3errm(h∗) m\n+ β √√√√2γ m∆m m∑ i=1 (τi − τi−1)(reg(h, Z̃i−1) + reg(h∗, Z̃i−1))\n+ 2∆m √ 2ξτm m log τm.\nSubstituting this inequality back into our deviation bound (48), we obtain\n|reg(h, h∗, Z̃m)− r̃egm(h, h∗)|\n≤ m Pmin,m\n+ √\n2α2 mr̃egm(h) + 2α √√√√ m τm m∑ i=1 (τi − τi−1)regi(hi) + 2α √ 3errm(h∗) m\n+ β √√√√2γ m∆m m∑ i=1 (τi − τi−1)(reg(h, Z̃i−1) + reg(h∗, Z̃i−1)) + 2∆m √ 2ξτm m log τm.\nWe can further use Cauchy-Schwarz inequality to obtain the bound\n|reg(h, h∗, Z̃m)− r̃egm(h, h∗)|\n≤ 1 4 r̃egm(h) + 2α 2 m + 2α √√√√ m τm m∑ i=1 (τi − τi−1)regi(hi) + 2α √ 3errm(h∗) m\n+ β √√√√2γ m∆m m∑ i=1 (τi − τi−1)(reg(h, Z̃i−1) + reg(h∗, Z̃i−1)) + 2∆m √ 2ξτm m log τm\n+ m\nPmin,m\n≤ 1 4 r̃egm(h) + 2α 2 m + 2α √√√√ m τm m∑ i=1 (τi − τi−1)regi(hi) + 2α √ 3errm(h∗) m\n+ β √√√√2γ m∆m m∑ i=1 (τi − τi−1)(reg(h, Z̃i−1) + reg(h∗, Z̃i−1)) + ∆m + m Pmin,m\n≤ 1 4 r̃egm(h) + 2α √√√√ m τm m∑ i=1 (τi − τi−1)regi(hi) + 2α √ 3errm(h∗) m\n+ β √√√√2γ m∆m m∑ i=1 (τi − τi−1)(reg(h, Z̃i−1) + reg(h∗, Z̃i−1)) + 4∆m\nwhere the last two inequalities use our assumptions on ξ and α respectively.\nProof of Proposition 2 We start by observing that\n|errm(h∗)− err(hm+1, Z̃m)| ≤ |errm(h∗)− err(h∗, Z̃m)|+ reg(h∗, hm+1, Z̃m).\nSince h∗ ∈ Ai for all epochs i ≤ m, we know that h∗ agrees with all the predicted labels. Consequently, err(h∗, Z̃m) = err(h∗, Zm), where we recall that Zm is the set of all examples where we queried labels up to epoch m. This allows us to rewrite\n|errm(h∗)− err(h∗, Z̃m)| = |errm(h∗)− err(h∗, Zm)|.\nUnder the event E , the above deviation is bounded, according to Lemma 2, by√√√√ m τm m∑ i=1 (τi − τi−1)EX,Y 1(h∗(X) 6= Y,X ∈ Di) Pi(X) + m Pmin,m ≤ √ m errm(h∗) Pmin,m + m Pmin,m ,\nwhere the inequality uses the bound Pi(X) ≥ Pmin,i for all X ∈ Di and Pmin,i ≥ Pmin,m for all epochs i ≤ m by Lemma 7. A further application of Cauchy-Schwarz inequality yields the bound\n|errm(h∗)− err(h∗, Z̃m)| ≤ errm(h\n∗)\n2 + 3 m 2Pmin,m\n≤ errm(h ∗)\n2 + 3∆m 2 .\nCombining the bounds yields\n|errm(h∗)− err(hm+1, Z̃m)| ≤ errm(h\n∗)\n2 + 3∆m 2 + reg(h∗, hm+1, Z̃m),\nwhich completes the proof of the proposition.\nFinally, we prove Lemmas 3 to 5 used in the proof of Theorem 1. Proof of Lemma 3 We first bound the regi(hi) terms. For i = 1, we have\nreg1(h1) = reg(h1) ≤ 1 ≤ η∆0\n2\nby Pmin,1 = 1 and our choices of η and ∆0. For 2 ≤ i < m, we have\nregi(hi) = EX,Y [1(hi(X) 6= Y,X ∈ Di)− 1(h∗(X) 6= Y,X ∈ Di)] = reg(hi) ≤ r̃egi−1(hi, h∗),\nwhere the second equality uses the fact that h∗ ∈ Ai for all i ≤ m by inductive hypothesis (9) and the inequality uses Lemma 1. Consequently, we can bound regi−1(hi) using the event Ei, since reg(hi, h∗, Z̃i−1) = 0. The event Ei now further implies that\nregi(hi) ≤ r̃egi−1(hi, h∗) ≤ 2reg(hi, h∗, Z̃i−1) + η∆i−1 2 ≤ η∆i−1 2 .\nUsing this, we can simplify T1 as\nT1 = 2α √√√√ m τm m∑ i=1 (τi − τi−1)regi(hi) ≤ 2α √√√√ m τm m∑ i=1 (τi − τi−1) η∆i−1 2 (49)\n≤ 2α √\n2η m∆m log τm\n≤ η∆m 12 + 24α2 m log τm. (50)\nhere the second inequality is by Lemma 8 and the third inequality is by Cauchy-Schwarz.\nProof of Lemma 4 We first invoke Proposition 2, whose assumptions now hold due to the claim h∗ ∈ Ai in Ei for all i ≤ m, and obtain\nerrm(h ∗) = 2err(hm+1, Z̃m) + 3∆m + 2reg(h ∗, hm+1, Z̃m).\nThe above inequality allows us to simplify T2 as\nT2 = 2α √ 3 merrm(h∗) ≤ 2α √ 3 m ( 2err(hm+1, Z̃m) + 3∆m + 2reg(h∗, hm+1, Z̃m) ) ≤ 2α √ 6 merr(hm+1, Z̃m) + 2α √ 9 m∆m + 2α √ 6 mreg(h∗, hm+1, Z̃m)\n≤ 2α √ 6 merr(hm+1, Z̃m) + ∆m + 1\n4 reg(h∗, hm+1, Z̃m) + 33α 2 m, (51)\nwhere the last inequality uses the Cauchy-Schwarz inequality.\nProof of Lemma 5 Observe that the event Ei gives a direct bound of η∆i−1/4 on the reg(h∗, hi, Z̃i−1) terms. For the other term, recall by the same event that for all h ∈ H and for all i = 1, 2 . . . ,m− 1,\nreg(h, h∗, Z̃i) ≤ 3\n2 r̃egi(h, h\n∗) + η\n4 ∆i.\nCombining with the empirical regret bound for h∗, this implies that\nreg(h, Z̃i) ≤ 3\n2 r̃egi(h, h\n∗) + η\n2 ∆i.\nConsequently we have the bound\nT 23 ≤ β2γ∆m m m∑ i=1 (τi − τi−1) ( 3r̃egi−1(h, h ∗) + 3η 2 ∆i−1 )\nTo simplify further, note that by the definition of r̃egi(h, h ∗) and our earlier definition of reg‡i (h, h ∗), we have\nm∑ i=1 (τi − τi−1)r̃egi−1(h, h∗) = m−1∑ i=1 τi+1 − τi τi i∑ j=1 (τj − τj−1)reg‡j(h, h ∗)\n= m−1∑ j=1 (τj − τj−1)reg‡j(h, h ∗) m−1∑ i=j τi+1 − τi τi ≤ 4 log τm m−1∑ j=1 (τj − τj−1)reg‡j(h, h ∗) ≤ 4τm log τm r̃egm(h, h∗),\nwhere the first equality uses our convention r̃eg0(h, h ∗) = 0 and proper index shifting, and the first inequality uses Lemma 8. We also have\nm∑ i=1 (τi − τi−1)∆i−1 ≤ 4τm∆m log τm.\nby Lemma 8. Consequently, we can rewrite\nT 23 ≤ β2γ∆m m ( 12τm log τm r̃egm(h, h ∗) + 6τmη log τm∆m )\n= β2γτm m log τm∆m ( 12r̃egm(h, h ∗) + 6η∆m ) ≤ η∆mr̃egm(h, h ∗)\n72 + η2∆2m 144 ,\nwhere the last inequality is by our choice of β such that β2γn n log n ≤ η/864. Taking square roots, we obtain\nT3 ≤ √ η∆mr̃egm(h, h ∗)\n72 + η2∆2m 144\n≤ 1 4 r̃egm(h, h ∗) + 7η∆m 72\n(52)"
    }, {
      "heading" : "D Label Complexity",
      "text" : "Here we prove Theorem 2. Fix any epoch m and index i ≤ τm. Consider Xi ∈ Dm and define\nh̄i := { hm, hm(Xi) 6= h∗(Xi), h′i, h ′ i(Xi) 6= h∗(Xi),\nwhere h′i := arg minh∈H∧h(Xi)6=hm(Xi) err(h, Z̃m−1). Because Xi ∈ DIS(Am), we have h′i ∈ Am, implying h̄i ∈ Am. Theorem 5 shows that h∗ ∈ Am, so we have\nPr(h̄i(X) 6= h∗(X)) = Pr(h̄i(X) 6= h∗(X) ∧X ∈ Dm) ≤ regm(h̄i) + 2errm(h∗) ≤ 16γ∆∗m−1 + 2errm(h∗),\nwhere the last inequality is by Theorem 1. This implies that\nXi ∈ DIS({h | Pr(h(X) 6= h∗(X)) ≤ 16γ∆∗m−1 + 2errm(h∗)}).\nWe thus have\nE[1(Xi ∈ DIS(Am))] ≤ E[1(Xi ∈ DIS({h | Pr(h(X) 6= h∗(X)) ≤ 16γ∆∗m−1 + 2errm(h∗)}))] ≤ θ(16γ∆∗m−1 + 2errm(h∗)),\nwhere we the last inequality uses the definition of the disagreement coefficient\nθ(h∗) := sup r>0 Pr({X | ∃h ∈ H s.t. Pr(h(X) 6= h∗(X)) ≤ r, h∗(X) 6= h(X)}) r .\nThe expected number of label queries made by our algorithm after seeing n examples is upper-bounded w.p. 1− δ by\n3 + n∑ i=4 E[1(Xi ∈ Dm(i))] ≤ 3 + M∑ j=2 (τj − τj−1)θ(16γ∆∗j−1 + 2errj(h∗))\n≤ 3 + 2nθerrM (h∗) + 16γθ M∑ j=2 (τj − τj−1)∆∗j−1\n= 3 + 2nθerrM (h ∗) + 16γθ M∑ j=2 (τj − τj−1) τj−1 τj−1∆ ∗ j−1.\nA similar argument as Lemma 7 shows that τj∆∗j is increasing in j, so we have by a further invocation of Lemma 8\n3 + n∑ i=4 E[1(Xi ∈ Dm(i))]\n≤ 3 + 2nθerrM (h∗) + 128γθ(n− 1)∆∗M−1 log(n− 1) = 3 + 2nθerrM (h ∗)\n+θO (√ nerrM (h∗) ( log ( |H| δ ) log2 n+ log3 n ) + log ( |H| δ ) log2 n+ log3 n ) ."
    }, {
      "heading" : "E Proofs for Tsybakov’s low-noise condition",
      "text" : "We begin with a lemma that captures the behavior of the ∆∗m terms, errm(h\n∗) and the probability of disagreement region under the Tsybakov noise condition (10). The proofs of Corollaries 2 and 4 are immediate given the lemma.\nLemma 9. Under the conditions of Theorem 1, suppose further that the low-noise condition (10) holds. Then we have for all epochs m = 1, 2, . . . ,M\nerrm(h ∗) ≤ c m log τm τ\n2(1−ω) 2−ω m , and errm(h∗) ≤ 5c m log2 τm τ 2(1−ω) 2−ω m . (53)\nProof. We will establish the lemma inductively. We make the following inductive hypothesis. There exists a constant c > 0 (dependent on the distributional parameters) such that for all epochs j ≥ 1, the bounds (53) in the statement of the Lemma hold. The base case for j = 1 trivially follows since err1(h∗) = err1(h∗) = err(h∗) ≤ 1 ≤ c 1 log τ1 τ 2(1−ω) 2−ω 1 , which is clearly true for an appropriately large value of c. Suppose now that the claim is true for epochs j = 1, 2, . . . ,m− 1. We will establish the claim at epoch m. To see this, first note that we have\nerrm(h ∗) = Pr(1(h∗(X) 6= Y,X ∈ Dm)) ≤ Pr(X ∈ Dm).\nUnder the noise condition, we can further upper bound the probability of the disagreement region, since by Theorem 1 we obtain\nPr(X ∈ Dm) = Pr(X ∈ DIS(Am)) ≤ Pr ( X ∈ DIS({h ∈ H : reg(h) ≤ 16γ∆∗m−1) ) ≤ Pr ( X ∈ DIS(h ∈ H : Pr(h(X) 6= h∗(X)) ≤ ζ (16γ∆∗m−1)ω) ) ,\nwhere the first inequality follows from Theorem 1 and the second one is a consequence of Tsybakov’s noise condition (10). Recalling the definition of disagreement coefficient (11), this can be further upper bounded by\nPr(X ∈ Dm) ≤ θζ (16γ∆∗m−1)ω. (54) Hence, we have obtained the bound\nerrm(h ∗) ≤ θζ (16γ∆∗m−1)ω.\nNote that ∆∗m−1 = c1 √ m−1errm−1(h∗) + c2 m−1 log τm−1. Our inductive hypothesis (53) allows us to upper bound the errm−1 in this expression for ∆∗m−1 and hence we obtain\n∆∗m−1 ≤ c1 √ m−1 5c m−1 log 2 τm−1 τ 2(1−ω) 2−ω m−1 + c2 m−1 log τm−1\n≤ c1 m−1 log τm τ 1−ω 2−ω m √ 5c+ c2 m−1 log τm−1\n≤ mτm τm−1 log τm\n( c1 √ 5cτ 1−ω 2−ω m + c2 ) ≤ 2 m log τm ( c1 √ 5cτ 1−ω 2−ω m + c2 ) .\nSince τm ≥ 3 and 0 < ω ≤ 1, we can further write\n∆∗m−1 ≤ 2 m log τm τ 1−ω 2−ω m ( c1 √ 5c+ c2 ) . (55)\nSubstituting this inequality in our earlier bound on errm(h∗) yields\nerrm(h ∗) ≤ θζ ( 32γ m log τm τ 1−ω 2−ω m ( c1 √ 5c+ c2 ))ω .\nSince mτm log τm ≥ 1 and 0 < ω ≤ 1, we can further bound\nerrm(h ∗) ≤ θζ mτm log τm ( 32γ τ −1 2−ω m ( c1 √ 5c+ c2 ))ω = θζ mτm log τm ( 32γ ( c1 √ 5c+ c2 ))ω τ −ω 2−ω m\n= θζ mτ 2(1−ω) 2−ω m log τm ( 32γ ( c1 √ 5c+ c2 ))ω ≤ c m log τm τ 2(1−ω) 2−ω m .\nHere the last bound follows for any choice of c such that\nc ≥ θζ ( 32γ ( c1 √ 5c+ c2 ))ω .\nThe above inequality has a solution since the LHS is smaller than the RHS at c = 0, while for c large enough, the LHS grows linearly in c, while the RHS grows as cω/2, and hence is asymptotically smaller than the LHS.\nWe now verify the second part of our induction hypothesis for epoch m. Note that we have\nerrm(h ∗) =\n1\nτm m∑ j=1 (τj − τj−1)errj(h∗)\n≤ 1 τm m∑ j=1 (τj − τj−1) c j log τj τ 2(1−ω) 2−ω j\n= 1\nτm m∑ j=1 (τj − τj−1) τj c jτj log τj τ 2(1−ω) 2−ω j .\nWe now observe that τj is clearly increasing in j, and so is τj j by definition. Consequently, we can further upper bound this inequality by\nerrm(h ∗) ≤ 1\nτm mτm log τm m∑ j=1 (τj − τj−1) τj cτ 2(1−ω) 2−ω j\n(a) ≤ c m log τm τ 2(1−ω) 2−ω m 1 + m∑ j=2 (τj − τj−1) τj  = c m log τm τ 2(1−ω) 2−ω m\n1 + m−1∑ j=1 (τj+1 − τj) τj+1  ≤ c m log τm τ 2(1−ω) 2−ω m\n1 + m−1∑ j=1 (τj+1 − τj) τj \nwhere the inequality (a) holds since τj is increasing in j and ω ∈ (0, 1] so that the exponent on τj is non-negative, and the final inequality follows since τj ≤ τj+1. Invoking Lemma 8, we obtain\nerrm(h ∗) ≤ m log τm (1 + 4c log τm) τ\n2(1−ω) 2−ω\nm\n≤ 5c m log2 τm τ 2(1−ω) 2−ω m ,\nwhere we used the fact that 1 ≤ log τm. Therefore, we have established the second part of the inductive claim, finishing the proof of the lemma.\nUsing the lemma, we now prove the corollaries. Proof of Corollary 2 Based on the proof of Lemma 9, we see that ∆∗m satisfies the bound (55). Plugging this into the statement of Theorem 1 immediately yields the lemma.\nProof of Corollary 4 Based on the proof of Lemma 9, we see that the probability of the disagreement region follows the bound (54). Substituting the bound (55) yields the stated result."
    }, {
      "heading" : "F Analysis of the Optimization Algorithm",
      "text" : "We begin by showing how to find the most violated constraint (Step 3) by calling an importance-weighted ERM oracle. Then we prove Theorem 3, followed by the framework and proof for Theorem 4.\nF.1 Finding the Most Violated Constraint Recall our earlier notation Imh (x) = 1(h(x) 6= hm(x) ∧ x ∈ Dm). Consider solving (OP) using an unlabeled sample S of size u. Note that Step 3 is equivalent to\narg minh∈H bm(h)− ÊX [ Imh (X) Pλ(X) ] (56)\n= arg minh∈H 2γβ 2(τm − 1)∆m−1err(h, Z̃m−1) + ÊX [( 2α2 − 1\nPλ(X)\n) Imh (X) ] = arg minh∈H 2γβ 2(τm − 1)∆m−1err(h, Z̃m−1)\n+ÊX [(\n2α2 − 1 Pλ(X)\n) Imh (X) + max ( 1\nPλ(X) − 2α2, 0\n) 1(X ∈ Dm) ] = arg minh∈H 2γβ 2(τm − 1)∆m−1err(h, Z̃m−1)\n+ÊX [ max ( 2α2 − 1\nPλ(X) , 0\n) 1(X ∈ Dm)1(h(X) 6= hm(X)) ] +ÊX [ max ( 1\nPλ(X) − 2α2, 0\n) 1(X ∈ Dm)1(h(X) 6= −hm(X)) ] = arg minh∈H 2γβ\n2(τm − 1)∆m−1err(h, Z̃m−1) +ÊX [|sλ(X)|1(X ∈ Dm)1(h(X) 6= sign(sλ(X))hm(X))] ,\nwhere sλ(X) := 2α2− 1/Pλ(X). In the above derivation, the second equality is by the fact that the extra term added to the objective is independent of h and hence does not change the minimizer. The third equality uses a case analysis on the sign of sλ(X) and the identity 1− 1(h(X) 6= hm(X)) = 1(h(X) 6= −hm(X)). The last expression suggests that an importance-weighted error minimization oracle can find the desired classifier on examples {(X,Y ∗,W )} with labels and importance weights defined as:\nY ∗ := arg min Y c(X,Y ), W := |c(X, 1)− c(X,−1)|,\nwhere\nc(X,Y ) :=\n{ 2γβ2∆m−1 ( 1(Xi∈Dm(i)∧Y 6=Yi)Qi\nPm(i)(Xi) + 1(Xi /∈ Dm(i) ∧ Y 6= hm(i)(Xi))\n) , X = Xi ∈ Z̃m−1,\n1 u |sλ(X)|1(X ∈ Dm)1(Y 6= sign(sλ(X))hm(X)), X ∈ S.\n(57)\nF.2 Proof of Theorem 3 Where clear from context, we drop the subscript m.\nWe first show that each coordinate ascent step causes sufficient increase in the dual objective. Pick any h and λ. Let λ′ be identical to λ except that λ′h = λh + δ for some δ > 0. Then the increase in the dual objective D can be computed directly:\nD(λ′)−D(λ) = δEX [Imh (X)] + 2EX [1(X ∈ Dm)( √ qλ(X)2 + δImh (X)− qλ(X))]− δb(h)\n≥ δEX [Imh (X)] + 2EX [ qλ(X) ( δImh (X) 2qλ(X)2 − δ 2Imh (X)2 8qλ(X)4 )] − δb(h) (58)\n= δEX [( 1 + 1\nqλ(X)\n) Imh (X)− b(h) ] − δ2E [ Imh (X)2\n4qλ(X)3 ] = δ ( EX [ Imh (X) Pλ(X) ] − b(h) ) − δ 2 4 E [ Imh (X)2 qλ(X)3 ] . (59)\nThe inequality (58) uses the fact that √\n1 + z ≥ 1 + z/2 − z2/8 for all z ≥ 0 (provable, for instance, using Taylor’s theorem). The lower bound (59) on the increase in the objective value is maximized exactly at\nδ = 2 E[Imh (X)/Pλ(X)− b(h)] EX [Imh (X)2/qλ(X)3] , (60)\nas in Step (7). Plugging into (59), it follows that if h is chosen on some iteration of Algorithm 2 prior to halting then the dual objective D increases by at least\nEX [Imh (X)/Pλ(X)− b(h)]2\nEX [Imh (X)2/qλ(X)3] ≥ ε2µ3 (61)\nsince qλ(x) ≥ µ, and since EX [Imh (X)/Pλ(X)− b(h)] ≥ ε. The initial dual objective is D(0) = (1 + µ)2Pr(Dm). Further, by duality and the fact that P (X) = 1/2 is a feasible solution to the primal problem, we have D(λ) ≤ 2(1 + µ2)Pr(Dm). And of course, rescaling can never cause the dual objective to decrease. Combining, it follows that the coordinate ascent algorithm halts in at most Pr(Dm)(2(1 + µ\n2)− (1 + µ)2)/(ε2µ3) ≤ Pr(Dm)/(ε2µ3) rounds proving the bound given in the theorem. By this same reasoning, the left hand side of (61) is equal to δ · EX [Imh (X)/Pλ(X)− b(h)], which is at least δε. That is, the change on each round in the dual objective D is at least ε times the change in one of the coordinates λh. Furthermore, the rescaling step can never cause the weights λh to increase. Therefore, ε‖λ̂‖1 is upper bounded by the total change in the dual objective, which we bounded above. This proves the bound on ‖λ̂‖1 given in the theorem.\nTo see (15), consider first the function g(s) = D(s · λ) for λ as in the algorithm after the rescaling step has been executed. At this point, it is necessarily the case that s = 1 maximizes g over s ∈ [0, 1] (since λ has already been rescaled). This implies that g′(1) ≥ 0 where g′ is the derivative of g; that is,\n0 ≤ g′(1) = E [∑\nh λhImh (X) Ps·λ(X) ] − ∑ h λhb(h). (62)\nNow let F (P ) denote the modified primal objective function in (12), and let P̃ denote the minimum of this objective over all feasible solutions. Then\nF (Pλ̂) ≤ F (Pλ̂) + ∑ h λ̂h ( EX [ Imh (X) Pλ̂(X) ] − b(h) ) (63)\n= min P L(P, λ̂) (64) ≤ max λ min P L(P,λ)\n= F (P̃ ). (65)\nHere, (63) follows from (62); (64) by the definition of Pλ(X) as the minimizer of the Lagrangian; and (65) is by strong duality. Then we have\nE [\n1\n1− Pλ̂(X)\n] ≤ F (Pλ̂) ≤ F (P̃ ) ≤ F (P ∗) ≤ E [\n1\n1− P ∗(X)\n] + µPr(Dm).\nF.3 Proof of Theorem 4 For ε > 0, define Λε := {λ ∈ RH : λ ≥ 0, ‖λ‖1 ≤ 1/ε}. We begin with a simple lemma.\nLemma 10. Suppose φ : R×X → R beL-Lipschitz with respect to its first argument, and φ( ∑ h∈H λhImh (x), x) ≤ R for all λ ∈ Λε and x ∈ X . Let ÊX [·] denote the empirical expectation with respect to an i.i.d. sample from PX . For any δ ∈ (0, 1), with probability at least 1− δ, every λ ∈ Λε satisfies∣∣∣∣∣ÊX [ φ (∑ h∈H λhImh (X), X )] − EX [ φ (∑ h∈H λhImh (X), X\n)]∣∣∣∣∣ ≤ 2L ε · √ 2 ln |H| u +R · √ ln(1/δ) u .\nProof. Let x ∈ {0, 1}H denote the vector with xh = 1(h(x) 6= hm(x)), and define the linear function class\nF := {x 7→ 〈λ,x〉 : λ ∈ Λε} .\nBy a simple variant of the argument by Bartlett and Mendelson [2002], with probability at least 1− δ,∣∣∣∣∣ÊX [ φ (∑ h∈H λhImh (X), X )] − EX [ φ (∑ h∈H λhImh (X), X )]∣∣∣∣∣ ≤ 2L · Ru(F) +R · √ ln(1/δ) u\nfor all λ ∈ Λε, where Ru(F) is the expected Rademacher average for the linear function class F for an i.i.d. sample of size n. By Kakade et al. [2009], this Rademacher complexity satisfies\nRu(F) ≤ 1\nε √ 2 ln |H| u .\nThis completes the proof.\nLemma 11. Pick any δ ∈ (0, 1). Let ÊX [·] denote the empirical expectation with respect to an i.i.d. sample from PX . With probability at least 1− δ, every λ ∈ Λε satisfies∣∣∣∣EX [ 11− Pλ(X) ] − ÊX [ 1 1− Pλ(X) ]∣∣∣∣ ≤ √ 2 ln |H| µ2ε2u + √ (µ2 + 1/ε) ln(3/δ) u\nand for all h ∈ H,∣∣∣∣EX [Imh (X)Pλ(X) ] − ÊX [ Imh (X) Pλ(X) ]∣∣∣∣ ≤ √ 2 ln |H| µ4ε2u + √ ln(3|H|/δ) µ2u + √ ln(6|H|/δ) 2u\nand ∣∣∣EX [Imh (X)]− ÊX [Imh (X)]∣∣∣ ≤ √\nln(6|H|/δ) 2u .\nProof. Observe that 1/(1 − Pλ(x)) = 1 + qλ(x) for all λ ∈ Λε and x ∈ X . Now we apply Lemma 10 to the function φ1(z, x) := √ µ2 + z, which is (2µ)−1-Lipschitz with respect to its first argument. Since qλ(x) =\nf1( ∑ h∈H λhImh (x), x) ≤ √ µ2 + 1/ε for all λ ∈ Λε and x ∈ X , Lemma 10 implies that, with probability at least 1− δ/3, ∣∣∣∣EX [ 11− Pλ(X) ] − ÊX [ 1 1− Pλ(X) ]∣∣∣∣ ≤ 1µε √ 2 ln |H| u + √ (µ2 + 1/ε) ln(3/δ) u , ∀λ ∈ Λε. (66)\nNext, observe that for every h ∈ H and x ∈ X ,\nImh (x) Pλ(x) = Imh (x) + Imh (x) qλ(x) .\nBy Hoeffding’s inequality and a union bound, we have with probability at least 1− δ/3,∣∣∣EX [Imh (X)]− ÊX [Imh (X)]∣∣∣ ≤ √\nln(6|H|/δ) 2u , ∀h ∈ H. (67)\nNow we apply Lemma 10 to the functions φh(z, x) := Imh (x)/ √ µ2 + z for each h ∈ H; each function φh is (2µ2)−1-\nLipschitz with respect to its first argument. Furthermore, since φh( ∑ h∈H λhImh (x), x) = Imh (x)/qλ(x) ≤ 1/µ for all λ ∈ Λε and x ∈ X , Lemma 10 and a union bound over all h ∈ H implies that, with probability at least 1− δ/3∣∣∣∣EX [Imh (X)qλ(X) ] − ÊX [ Imh (X) qλ(X) ]∣∣∣∣ ≤ √ 2 ln |H| µ4ε2u + √ ln(3|H|/δ) µ2u , ∀λ ∈ Λε, h ∈ H. (68)\nFinally, by a union bound, all of (66), (67), and (68) hold simultaneously with probability at least 1− δ.\nWe can now prove Theorem 4. We first state a slightly more explicit version of the theorem, which is then proved.\nTheorem 6. Let S be an i.i.d. sample of size u from the PX . Suppose Algorithm 2 is run on the m-th epoch for solving (OPS,ε) up to slack ε in the variance constraints. Then the following holds:\n1. Algorithm 2 halts in at most P̂r(Dm) 8P 3min,mε\n2 iterations, where P̂r(Dm) := ∑ X∈S 1(X ∈ Dm)/u.\n2. The solution λ̂ ≥ 0 it outputs has bounded `1 norm:\n‖λ̂‖1 ≤ P̂r(Dm)/ε.\n3. There exists an absolute constant C > 0 such that the following holds. If\nu ≥ C ·\n(( 1\nP 4min,mε 2\n+ α4 ) · log |H|\nε2 +\n( 1\nP 2min,m +\n1 ε + α4\n) · log(1/δ)\nε2\n) ,\nthen with probability at least 1− δ, the query probability function Pλ̂(x) satisfies\n• All constraints of (OP) except with slack 2.5ε in constraints (5), • Approximate primal optimality:\nEX [\n1\n1− Pλ̂(X)\n] ≤ EX [ 1\n1− P ∗(X)\n] + 8Pmin,mPr(Dm) + (2 + 4Pmin,m)ε,\nwhere P ∗ is the solution to (OP).\nTheorem 4 is just a result of some simplifications in the O(·) notation in the above result. We now prove the theorem. Proof of Theorem 6 The first two statements, finite convergence and boundedness of the solution’s `1 norm, can be proved with the techniques in Appendix F.2 that establish the same for Theorem 3. We thus focus on proving the third statement here.\nLet ÊX [·] denote empirical expectation with respect to S. Hoeffding’s inequality implies that with probability at least 1− δ/2, ÊX [1(X ∈ Dm)] ≤ EX [1(X ∈ Dm)] + ε. (69) Also, Lemma 11 implies that with probability at least 1− δ/2,∣∣∣∣ÊX [ 11− Pλ(X) ] − EX [ 1 1− Pλ(X)\n]∣∣∣∣ ≤ ε, ∀λ ∈ Λε/2; (70)∣∣∣EX [Imh (X)]− ÊX [Imh (X)]∣∣∣ ≤ ε/(8α2), ∀h ∈ H; (71)∣∣∣∣EX [Imh (X)Pλ(X) ] − ÊX [ Imh (X) Pλ(X)\n]∣∣∣∣ ≤ ε/4, ∀λ ∈ Λε/2, h ∈ H. (72) Therefore, by a union bound, there is an event of probability mass at least 1 − δ on which Eqs. (69), (70), (71), (72) hold simultaneously. We henceforth condition on this event.\nBy Theorem 3, λ̂ satisfies ‖λ̂‖1 ≤ 1/ε, the bound constraints in (6), as well as ÊX [ Imh (X) Pλ̂(X) ] ≤ bm(h) + 2ε, ∀h ∈ H, (73)\nand\nÊX [\n1\n1− Pλ̂(X)\n] ≤ ÊX [ 1\n1− P̂ ∗ε (X)\n] + 4Pmin,mÊX [1(X ∈ Dm)] (74)\nwhere P̂ ∗ε is the optimal solution to (OPS,ε). We use this to show that Pλ̂ is a feasible solution for (OP2.5ε), and compare its objective value to the optimal objective value for (OP).\nApplying (71) and (72) to (73) gives EX [ Imh (X) Pλ̂(X) ] ≤ bm(h) + 2.5ε, ∀h ∈ H.\nSince Pλ̂ also satisfies the bound constraints in (6), it follows that Pλ̂ is feasible for (OP2.5ε). Now we turn to the objective value. Applying (69) and (70) to (74) gives\nEX [\n1\n1− Pλ̂(X)\n] ≤ ÊX [ 1\n1− P̂ ∗ε (X)\n] + 4Pmin,mEX [1(X ∈ Dm)] + (1 + 4Pmin,m)ε. (75)\nWe need to relate the first term on the right-hand side to the optimal objective value for (OP). Let λ∗ be the output of running Algorithm 2 for solving (OP) up to slack ε/2. By Theorem 3, λ∗ satisfies ‖λ∗‖1 ≤ 2/ε, the bound constraints in (6), as well as\nEX [ Imh (X) Pλ∗(X) ] ≤ bm(h) + ε/2, ∀h ∈ H,\nand\nEX [\n1\n1− Pλ∗(X)\n] ≤ EX [ 1\n1− P ∗(X)\n] + 4Pmin,mEX [1(X ∈ Dm)]. (76)\nApplying (70) to (76), we have ÊX [ 1\n1− Pλ∗(X)\n] ≤ EX [ 1\n1− P ∗(X)\n] + 4Pmin,mEX [1(X ∈ Dm)] + ε. (77)\nAnd applying (71) and (72) to (76) gives ÊX [ Imh (X) Pλ∗(X) ] ≤ bm(h) + ε, ∀h ∈ H. (78)\nThis establishes that λ∗ is a feasible solution for (OPS,ε). In particular,\nÊX\n[ 1\n1− P̂ ∗ε (X)\n] ≤ ÊX [ 1\n1− Pλ∗(X) ] ≤ EX [ 1\n1− P ∗(X)\n] + 4Pmin,mEX [1(X ∈ Dm)] + ε\nwhere the second inequality follows from (77). We now combine this with (75) to obtain EX [ 1\n1− Pλ̂(X)\n] ≤ EX [ 1\n1− P ∗(X)\n] + 8Pmin,mEX [1(X ∈ Dm)] + (2 + 4Pmin,m)ε."
    }, {
      "heading" : "G Experimental Details",
      "text" : "Here we provide more details about the experiments.\nG.1 Datasets Table 1 gives details about the 23 binary classification datasets used in our experiments, where n is the number of examples, d is the number of features, s is the average number of non-zero features per example, and r is the proportion of the minority class.\nG.2 Hyper-parameter Settings We start with the actual hyper-parameters used by OAC. Going back to Algorithm 1, we note that the tuning parameters get used in mostly the following three quantities: γ∆i−1 , α and β. We use this fact to reduce the number of input parameters. Let c0 := γ2c132(log(|H|/δ) + log(i− 1)) (treating log(i− 1) as a constant) and set η = 864, γ = η/4 and c2 = ηc21/4 according to our theory. Then we have\nγ∆i−1 = √ γ2c1 i−1err(hi, Z̃i−1) + γc2 i−1 log(i− 1)\n=\n√ c0err(hi, Z̃i−1)\ni− 1 + c0 c2 γc1 log(i− 1) i− 1 ,\nwhere c2γc1 = c1 = O(α). Based on this, we use\n∆̂i−1 :=\n√ c0err(hi, Z̃i−1)\ni− 1 + max(2α, 4)c0 log(i− 1) i− 1\n(79)\nin Algorithm 3 in place of γ∆i−1. Next we consider\nβ2 ≤ 1 216n n log n\n≈ γ 2c1\n216c0 log n ∵ n n ≈ c0/(γ2c1) by treating log n as a constant = O ( α\nc0\n) by again treating log n as a constant and c1 = O(α).\nBased on the last expression, we set β := √ α/c0 10 . In sum, the actual input parameters boil down to the cover size l, α ≥ 1 and c0, and we use them to set\nγ∆i−1 :=\n√ c0err(hi, Z̃i−1)\ni− 1 + max(2α, 4)c0 log(i− 1) i− 1 , β = √ α/c0 10 .\nFinally, we use the following setting for the minimum query probability:\nPmin,i = min  1√ (i− 1)err(hi, Z̃i−1) + log(i− 1) , 1 2  . Next we describe hyper-parameter settings for different algorithms. A common hyper-parameter is the learning rate of the underlying online oracle, which is a reduction to importance-weighted logistic regression. For all active learning algorithm, we try the following 11 learning rates: 10−1 ·{2−2, 2−1, . . . , 28}. Active learning hyper-parameter settings are given in the following table:\nalgorithm parameter settings total number of settings OAC (l, α, c0) ∈ {3, 6, 12, 24, 48} × {20, 21, . . . , 24} × {\n0.1 · {2−4, 2−3, . . . , ·2−1}, 0.1, 0.3, . . . , 0.9, 20, 21, . . . , 23 } 325 ORA-I (α, c0) the same as OAC 65\nIWAL C0 ∈ { 0.1 · {2−10, 2−9, . . . , 20}, 20, 21, . . . , 23 }\n15\nORA-II C0 ∈ {2−11, 2−10, . . . , 23} 15 RANDOM query rate ∈ { 10−3{20, 21, . . . , 29}, 0.75, 1 } 12\nGood hyper-parameters of the algorithms usually lie in the interior of these value ranges.\nG.3 More Experimental Results We provide detailed per-dataset results in Figures 7 and 8, which show minimum test errors over hyper-parameter settings that are achievable at different query rates for small (fewer than 105 examples) and large (more than 105 examples) datasets. On small datasets, OAC is generally competitive with other algorithms. On all (including the three shown in Figure 3) but two large datasets, bio and kdda, OAC outperforms other algorithms at most query rates, with a clear advantage at low query rates. Note that both bio and kdda, as shown in Table 1, are imbalanced. The fraction of the minority class in bio is about 1%, and the minimum test error is about 0.4%, a quite significant difference. IWAL strongly dominates other algorithms on this dataset, which suggests that using predicted labels, as done by the other three agnostic active learning algorithms, may be undesirable for highly imbalanced classification problems. There is less class skewness in kdda, but the minimum test error 12% is only slightly lower than the fraction of the minority class 14.7%. On this hard dataset, ORA-I, i.e., the Oracular CAL variant of OAC, outperforms other algorithms."
    } ],
    "references" : [ {
      "title" : "Active and passive learning of linear separators under log-concave distributions",
      "author" : [ "Maria-Florina Balcan", "Phil Long" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "Balcan and Long.,? \\Q2013\\E",
      "shortCiteRegEx" : "Balcan and Long.",
      "year" : 2013
    }, {
      "title" : "Agnostic active learning",
      "author" : [ "Maria-Florina Balcan", "Alina Beygelzimer", "John Langford" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2006
    }, {
      "title" : "Margin based active learning",
      "author" : [ "Maria-Florina Balcan", "Andrei Broder", "Tong Zhang" ],
      "venue" : "In Proceedings of the 20th annual conference on Learning theory,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2007
    }, {
      "title" : "Gaussian and Rademacher complexities: Risk bounds and structural results",
      "author" : [ "P. Bartlett", "S. Mendelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bartlett and Mendelson.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bartlett and Mendelson.",
      "year" : 2002
    }, {
      "title" : "Importance weighted active learning",
      "author" : [ "A. Beygelzimer", "S. Dasgupta", "J. Langford" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Beygelzimer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Beygelzimer et al\\.",
      "year" : 2009
    }, {
      "title" : "Agnostic active learning without constraints",
      "author" : [ "A. Beygelzimer", "D. Hsu", "J. Langford", "T. Zhang" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Beygelzimer et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Beygelzimer et al\\.",
      "year" : 2010
    }, {
      "title" : "Minimax bounds for active learning",
      "author" : [ "R.M. Castro", "R.D. Nowak" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Castro and Nowak.,? \\Q2008\\E",
      "shortCiteRegEx" : "Castro and Nowak.",
      "year" : 2008
    }, {
      "title" : "Improving generalization with active learning",
      "author" : [ "D. Cohn", "L. Atlas", "R. Ladner" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cohn et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Cohn et al\\.",
      "year" : 1994
    }, {
      "title" : "Coarse sample complexity bounds for active learning",
      "author" : [ "S. Dasgupta" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Dasgupta.,? \\Q2005\\E",
      "shortCiteRegEx" : "Dasgupta.",
      "year" : 2005
    }, {
      "title" : "A general agnostic active learning algorithm",
      "author" : [ "S. Dasgupta", "D. Hsu", "C. Monteleoni" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Dasgupta et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dasgupta et al\\.",
      "year" : 2007
    }, {
      "title" : "On tail probabilities for martingales",
      "author" : [ "D.A. Freedman" ],
      "venue" : "The Annals of Probability,",
      "citeRegEx" : "Freedman.,? \\Q1975\\E",
      "shortCiteRegEx" : "Freedman.",
      "year" : 1975
    }, {
      "title" : "Theoretical Foundations of Active Learning",
      "author" : [ "S. Hanneke" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Hanneke.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hanneke.",
      "year" : 2009
    }, {
      "title" : "Theory of disagreement-based active learning",
      "author" : [ "Steve Hanneke" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Hanneke.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hanneke.",
      "year" : 2014
    }, {
      "title" : "A generalization of sampling without replacement from a finite universe",
      "author" : [ "D.G. Horvitz", "D.J. Thompson" ],
      "venue" : "J. Amer. Statist. Assoc.,",
      "citeRegEx" : "Horvitz and Thompson.,? \\Q1952\\E",
      "shortCiteRegEx" : "Horvitz and Thompson.",
      "year" : 1952
    }, {
      "title" : "Algorithms for Active Learning",
      "author" : [ "Daniel J. Hsu" ],
      "venue" : "PhD thesis, University of California at San Diego,",
      "citeRegEx" : "Hsu.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hsu.",
      "year" : 2010
    }, {
      "title" : "On the generalization ability of online strongly convex programming algorithms",
      "author" : [ "S.M. Kakade", "A. Tewari" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Kakade and Tewari.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kakade and Tewari.",
      "year" : 2009
    }, {
      "title" : "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization",
      "author" : [ "Sham M Kakade", "Karthik Sridharan", "Ambuj Tewari" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Kakade et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2009
    }, {
      "title" : "Online importance weight aware updates",
      "author" : [ "Nikos Karampatziakis", "John Langford" ],
      "venue" : "UAI",
      "citeRegEx" : "Karampatziakis and Langford.,? \\Q2011\\E",
      "shortCiteRegEx" : "Karampatziakis and Langford.",
      "year" : 2011
    }, {
      "title" : "Rademacher complexities and bounding the excess risk in active learning",
      "author" : [ "Vladimir Koltchinskii" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Koltchinskii.,? \\Q2010\\E",
      "shortCiteRegEx" : "Koltchinskii.",
      "year" : 2010
    }, {
      "title" : "Optimal aggregation of classifiers in statistical learning",
      "author" : [ "A.B. Tsybakov" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "Tsybakov.,? \\Q2004\\E",
      "shortCiteRegEx" : "Tsybakov.",
      "year" : 2004
    }, {
      "title" : "Beyond disagreement-based agnostic active learning",
      "author" : [ "Chicheng Zhang", "Kamalika Chaudhuri" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Zhang and Chaudhuri.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhang and Chaudhuri.",
      "year" : 2014
    }, {
      "title" : "By a simple variant of the argument by Bartlett and",
      "author" : [ ],
      "venue" : "Λε}",
      "citeRegEx" : "F,? \\Q2002\\E",
      "shortCiteRegEx" : "F",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "1 Introduction How can you best learn a classifier given a label budget? Active learning approaches are known to yield exponential improvements over supervised learning under strong assumptions [Cohn et al., 1994].",
      "startOffset" : 194,
      "endOffset" : 213
    }, {
      "referenceID" : 6,
      "context" : "This results in a poor label complexity, well short of information-theoretic limits [Castro and Nowak, 2008] even for general robust solutions [Zhang and Chaudhuri, 2014].",
      "startOffset" : 84,
      "endOffset" : 108
    }, {
      "referenceID" : 20,
      "context" : "This results in a poor label complexity, well short of information-theoretic limits [Castro and Nowak, 2008] even for general robust solutions [Zhang and Chaudhuri, 2014].",
      "startOffset" : 143,
      "endOffset" : 170
    }, {
      "referenceID" : 11,
      "context" : "The label complexity bound depends on the disagreement coefficient [Hanneke, 2009], which does not completely capture the advantage of the algorithm.",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "Under much weaker assumptions, streaming-based agnostic active learning [Balcan et al., 2006, Beygelzimer et al., 2009, 2010, Dasgupta et al., 2007, Zhang and Chaudhuri, 2014] is particularly appealing since it is known to work for any classifier representation and any label noise distribution with an i.i.d. data source.1 Here, a learning algorithm decides for each unlabeled example in sequence whether or not to request a label, never revisiting this decision. Restated then: What is the best possible active learning algorithm which works for any classifier representation, any label noise distribution, and is computationally tractable? Computational tractability is a critical concern, because most known algorithms for this setting [e.g., Balcan et al., 2006, Koltchinskii, 2010, Zhang and Chaudhuri, 2014] require explicit enumeration of classifiers, implying exponentially-worse computational complexity compared to typical supervised learning algorithms. Active learning algorithms based on empirical risk minimization (ERM) oracles [Beygelzimer et al., 2009, 2010, Hsu, 2010] can overcome this intractability by using passive classification algorithms as the oracle to achieve a computationally acceptable solution. Achieving generality, robustness, and acceptable computation has a cost. For the above methods [Beygelzimer et al., 2009, 2010, Hsu, 2010], a label is requested on nearly every unlabeled example where two empirically good classifiers disagree. This results in a poor label complexity, well short of information-theoretic limits [Castro and Nowak, 2008] even for general robust solutions [Zhang and Chaudhuri, 2014]. Until now. In Section 3, we design a new algorithm ACTIVE COVER (AC) for constructing query probability functions that minimize the probability of querying inside the disagreement region—the set of points where good classifiers disagree—and never query otherwise. This requires a new algorithm that maintains a parsimonious cover of the set of empirically good classifiers. The cover is a result of solving an optimization problem (in Section 5) specifying the properties of a desirable query probability function. The cover size provides a practical knob between computation and label complexity, as demonstrated by the complexity analysis we present in Section 5. In Section 4, we provider our main results which demonstrate that AC effectively maintains a set of good classifiers, achieves good generalization error, and has a label complexity bound tighter than previous approaches. The label complexity bound depends on the disagreement coefficient [Hanneke, 2009], which does not completely capture the advantage of the algorithm. In Appendix 4.2.2, we provide an example of a hard active learning problem where AC is 1See the monograph of Hanneke [2014] for an overview of the existing literature, including alternative settings where additional assumptions are placed on the data source (e.",
      "startOffset" : 73,
      "endOffset" : 2805
    }, {
      "referenceID" : 4,
      "context" : "In the IWAL framework [Beygelzimer et al., 2009], a decision whether or not to query a label is made randomly: the learner picks a probability p ∈ [0, 1], and queries the label with that probability.",
      "startOffset" : 22,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : "Whenever p > 0, an unbiased error estimate can be produced using inverse probability weighting [Horvitz and Thompson, 1952].",
      "startOffset" : 95,
      "endOffset" : 123
    }, {
      "referenceID" : 14,
      "context" : "This is most easily seen for Oracular CAL [Hsu, 2010] which queries with probability 1 if X ∈ Dm and 0 otherwise.",
      "startOffset" : 42,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "A similar argument can also be made for the IWAL method [Beygelzimer et al., 2010], which also queries in the disagreement region with probability 1, and hence suffers from the same suboptimality compared to our choice.",
      "startOffset" : 56,
      "endOffset" : 82
    }, {
      "referenceID" : 19,
      "context" : "One intuitive condition that controls the errors within the disagreement region is the low-noise condition of Tsybakov [2004], which asserts that there exist constants ζ > 0 and 0 < ω ≤ 1 such that Pr(h(X) 6= h∗(X)) ≤ ζ · (err(h)− err(h∗))ω, ∀h ∈ H such that err(h)− err(h∗) ≤ ε0.",
      "startOffset" : 110,
      "endOffset" : 126
    }, {
      "referenceID" : 6,
      "context" : "It is worth noting that the rates obtained here are known to be unimprovable for even passive learning under the Tsybakov noise condition Castro and Nowak [2008].5 Consequently, there is no loss of statistical efficiency in using our active learning approach.",
      "startOffset" : 138,
      "endOffset" : 162
    }, {
      "referenceID" : 12,
      "context" : "1 Disagreement-based label complexity bounds In order to quantify the extent of gains over passive learning, we measure the hardness of our problem using the disagreement coefficient [Hanneke, 2014], which is defined as 5ω in our statement of the low-noise condition (10) corresponds to 1/κ in the results of Castro and Nowak [2008].",
      "startOffset" : 183,
      "endOffset" : 198
    }, {
      "referenceID" : 6,
      "context" : "1 Disagreement-based label complexity bounds In order to quantify the extent of gains over passive learning, we measure the hardness of our problem using the disagreement coefficient [Hanneke, 2014], which is defined as 5ω in our statement of the low-noise condition (10) corresponds to 1/κ in the results of Castro and Nowak [2008].",
      "startOffset" : 309,
      "endOffset" : 333
    }, {
      "referenceID" : 5,
      "context" : "We contrast this with the label complexity of IWAL [Beygelzimer et al., 2010], which grows as θ √ n independent of err(h∗).",
      "startOffset" : 51,
      "endOffset" : 77
    }, {
      "referenceID" : 14,
      "context" : "A much closer comparison is with respect to the Oracular CAL algorithm [Hsu, 2010], which does have a dependence on √ nerr(h∗) in the second term, but has a worse dependence on θ.",
      "startOffset" : 71,
      "endOffset" : 82
    }, {
      "referenceID" : 14,
      "context" : "Indeed the proofs of these results are entirely based on the fact that we do not query outside the disagreement region, a property shared by the previous Oracular CAL algorithm [Hsu, 2010].",
      "startOffset" : 177,
      "endOffset" : 188
    }, {
      "referenceID" : 6,
      "context" : "The label complexity obtained above is indeed optimal in terms of the dependence on n, the number of unlabeled examples, matching known information-theoretic rates of Castro and Nowak [2008]. This can be seen since the regret from Corollary 2 falls as a function of the number of queries at a rate of Õ(q − 1 2(1−ω) m log(|H|/δ)) after m epochs, where qm is the number of label queries.",
      "startOffset" : 167,
      "endOffset" : 191
    }, {
      "referenceID" : 6,
      "context" : "The label complexity obtained above is indeed optimal in terms of the dependence on n, the number of unlabeled examples, matching known information-theoretic rates of Castro and Nowak [2008]. This can be seen since the regret from Corollary 2 falls as a function of the number of queries at a rate of Õ(q − 1 2(1−ω) m log(|H|/δ)) after m epochs, where qm is the number of label queries. This is indeed optimal according to the lower bounds of Castro and Nowak [2008], after recalling that ω = 1/κ in their results.",
      "startOffset" : 167,
      "endOffset" : 467
    }, {
      "referenceID" : 8,
      "context" : "Starting with the first issue, we follow Dasgupta et al. [2007] who cleverly observed that x ∈ Dm can be efficiently determined using a single call to an ERM oracle.",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "See Appendix F of [Karampatziakis and Langford, 2011] for details.",
      "startOffset" : 18,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : "The specific importance weighted oracle we use is a reduction to online importance-weighted logistic regression [Karampatziakis and Langford, 2011] implemented in Vowpal Wabbit (VW).",
      "startOffset" : 112,
      "endOffset" : 147
    }, {
      "referenceID" : 17,
      "context" : "The specific importance weighted oracle we use is a reduction to online importance-weighted logistic regression [Karampatziakis and Langford, 2011] implemented in Vowpal Wabbit (VW). Instead of computing the query probability function by solving a batch optimization problem as in Step 5 of AC, OAC maintains a fixed number l of classifiers that are intended to be a cover of the set of good classifiers. On every new example, this cover undergoes a sequence of online, importance weighted updates (Steps 7 to 13 of OAC), which are meant to approximate the coordinate ascent steps in Algorithm 2. The importance structure (16) is derived from (57), accounting for the fact that the algorithm simply uses the incoming stream of examples to estimate EX [·] rather than a separate unlabeled sample. The same approximation is also present in the updates (17) and (18), which are online estimates of the numerator and the denominator of the additive coordinate update in Step 7 of Algorithm 2. Because (17) is an online estimate, we need to explicitly enforce non-negativity. Finally, Steps 9 to 14 of AC and Steps 14 to 26 of OAC perform the querying of labels. As pointed out in Section 5, the test in Step 16 of OAC is done via an online technique detailed in Appendix F of Karampatziakis and Langford [2011].",
      "startOffset" : 113,
      "endOffset" : 1307
    }, {
      "referenceID" : 4,
      "context" : "• IWAL: A slight modification of Algorithm 1 of Beygelzimer et al. [2010], which performs importance-weighted sampling of labels and maintains an unbiased estimate of classification error.",
      "startOffset" : 48,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "• IWAL: A slight modification of Algorithm 1 of Beygelzimer et al. [2010], which performs importance-weighted sampling of labels and maintains an unbiased estimate of classification error. In computing the query probability, rather than using a conservative, problem-independent threshold as in Beygelzimer et al. [2010], we use the following error-dependent quantity: √ C0 log k k − 1 ek−1 + C0 log k k − 1 , (22) where ek−1 is the importance-weighted error estimate after the algorithm processes k − 1 examples.",
      "startOffset" : 48,
      "endOffset" : 321
    }, {
      "referenceID" : 4,
      "context" : "• IWAL: A slight modification of Algorithm 1 of Beygelzimer et al. [2010], which performs importance-weighted sampling of labels and maintains an unbiased estimate of classification error. In computing the query probability, rather than using a conservative, problem-independent threshold as in Beygelzimer et al. [2010], we use the following error-dependent quantity: √ C0 log k k − 1 ek−1 + C0 log k k − 1 , (22) where ek−1 is the importance-weighted error estimate after the algorithm processes k − 1 examples. C0 is the only active learning hyper-parameter. The query probability is set to 1 if the error difference Gk, defined in Step 2 of Algorithm 1 of Beygelzimer et al. [2010], is smaller than the threshold (22), and otherwise a decreasing function of Gk.",
      "startOffset" : 48,
      "endOffset" : 686
    }, {
      "referenceID" : 14,
      "context" : "• ORA-I: An Oracular-CAL [Hsu, 2010] style variant of Algorithm 3.",
      "startOffset" : 25,
      "endOffset" : 36
    }, {
      "referenceID" : 14,
      "context" : "• ORA-II: An Oracular-CAL [Hsu, 2010] style variant of IWAL, where the query probability is set to 1 if the error difference Gk, defined in Step 2 of Algorithm 1 of Beygelzimer et al.",
      "startOffset" : 26,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "• ORA-II: An Oracular-CAL [Hsu, 2010] style variant of IWAL, where the query probability is set to 1 if the error difference Gk, defined in Step 2 of Algorithm 1 of Beygelzimer et al. [2010], is smaller than the threshold √ C0 log k k − 1 ek−1 + C0 log k k − 1 .",
      "startOffset" : 165,
      "endOffset" : 191
    } ],
    "year" : 2017,
    "abstractText" : "We develop a new active learning algorithm for the streaming setting satisfying three important properties: 1) It provably works for any classifier representation and classification problem including those with severe noise. 2) It is efficiently implementable with an ERM oracle. 3) It is more aggressive than all previous approaches satisfying 1 and 2. To do this we create an algorithm based on a newly defined optimization problem and analyze it. We also conduct the first experimental analysis of all efficient agnostic active learning algorithms, discovering that this one is typically better across a wide variety of datasets and label complexities.",
    "creator" : "LaTeX with hyperref package"
  }
}