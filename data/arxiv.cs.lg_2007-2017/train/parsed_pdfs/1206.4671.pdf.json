{
  "name" : "1206.4671.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Dependent Hierarchical Normalized Random Measures for Dynamic Topic Modeling",
    "authors" : [ "Changyou Chen", "Nan Ding", "Wray Buntine" ],
    "emails" : [ "cchangyou@gmail.com", "ding10@purdue.edu", "Wray.Buntine@nicta.com.au" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Dirichlet processes and their variants are popular in recent years, with applications found in diverse discrete domains such as topic modeling (Teh et al., 2006), ngram modeling (Teh, 2006), clustering (Socher et al., 2011), and image modeling (Li et al., 2011). These models take as input a base distribution and produce as output another distribution which is somewhat similar. Moreover, they can be used hierarchically. Together this makes them ideal for modeling structured data such as text and images.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nWhen modeling dynamic data or data from multiple sources, dependent nonparametric Bayesian models (MacEachern, 1999) are needed in order to harness related or previous information. Among these models, the hierarchical Dirichlet process (HDP) (Teh et al., 2006) is the most popular one. However, a basic assumption underlying the HDP is the full exchangeability of the sample path, which is often violated in practice, e.g., we could assume the content of ICML depends on previous years’ so order is important.\nTo overcome the full exchangeability limitation, several dependent Dirichlet process models have been proposed, for example, the dynamic HDP (Ren et al., 2008), the evolutionary HDP (Zhang et al., 2010), and the recurrent Chinese Restaurant process (Ahmed & Xing, 2010). Dirichlet processes are used because of simplicity and conjugacy (James et al., 2006). These models are constructed by incorporating the previous DP’s into the base distribution of the current DP. Dependent DPs have also been constructed using the underlying Poisson processes (Lin et al., 2010). However, recent research has shown that many real datasets have the power-law property, e.g., in images (Sudderth & Jordan, 2008), in topic-word distributions (Teh, 2006) and in document topic (label) distributions (Rubin et al., 2011). This makes the Dirichlet process an improper tool for modeling these datasets.\nAlthough there also exists some dependent nonparametric models with power-law phenomena, their dependencies are limited. For example, Bartlett et al. (2010) proposed a dependent hierarchical PitmanYor process that only allows deletion of atoms, while Sudderth & Jordan (2008) construct the dependent Pitman-Yor process by only allowing dependencies between atoms.\nIn this paper, we use a larger class of stochastic processes called normalized random measures with independent increments (NRM) (James et al., 2009). While this includes the Dirichlet process as a special case, some other versions of NRMs have the powerlaw property. This class of discrete random measures can also be constructed from Poisson processes. Given this, following (Lin et al., 2010), we analogically define superposition, subsampling and point transition on these normalized random measures, and construct a time dependent hierarchical model for dynamic topic modeling. By this, the dependencies are flexibly controlled between both jumps and atoms of the NRMs. All proofs and some extended theories are available in (Chen et al., 2012)."
    }, {
      "heading" : "2. Normalized Random Measures",
      "text" : ""
    }, {
      "heading" : "2.1. Background and Definitions",
      "text" : "This background on random measures follows (James et al., 2009).\nLet (S,S) be a measure space where S is the σ-algebra of S. Let ν be a measure on it. A Poisson process on S is a random subset Π ∈ S such that if N(A) is the number of points of Π in the measurable set A ⊆ S, then N(A) is a Poisson random variable with mean ν(A), and N(A1), · · · , N(An) are independent if A1, · · · , An are disjoint.\nBased on the definition, we define a complete random measure (CRM) on (X,B(X)) to be a linear functional of the Poisson random measure N(·), with mean measure ν(dt, dx) defined on a product space S = R+×X:\nµ̃(B) = ∫ R+×B tN(dt,dx),∀B ∈ B(X). (1)\nHere ν(dt,dx) is called the Lévy measure of µ̃.\nIt is worth noting that the CRM is usually written in the form µ̃(B) = ∑∞ k=1 Jkδxk(B), where J1, J2, · · · > 0 are called the jumps of the process, and x1, x2, · · · are a sequence of independent random variables drawn from a base measurable space (X,B(X))1. A normalized random measure (NRM) on (X,B(X)) is defined as µ = µ̃ µ̃(X) . We always use µ to denote an NRM, and µ̃ its unnormalized counterpart.\nTaking different Lévy measures ν(dt, dx), we can obtain different NRMs, and the form we consider is described in Section 2.3. Here we consider the case ν(dt, dx) = Mρη(dt)H(dx), where H(dx) is the base probability measure, M is the total mass acting as a\n1B(X) means the σ-algebra of X, we sometimes omit this and use X to denote the measurable space.\nconcentration parameter, and η is the set of other hyperparameters, depending on the specific NRM’s. We use NRM(M,η,H) to denote the corresponding normalized random measure."
    }, {
      "heading" : "2.2. Slice sampling NRMs",
      "text" : "We briefly introduce the ideas of slice sampling normalized random measures discussed in (“Slice 1” version, Griffin & Walker, 2011). It deals with the normalized random measure mixture of the type\nµ(·) = ∞∑ k=1 rkδθk(·), θsi ∼ µ(·), xi ∼ g0(·|θsi) (2)\nwhere rk = Jk/ ∑∞ l=1 Jl, θk’s are the component of the mixture model drawn i.i.d. from a parameter space H(·), si denotes the component that xi belongs to, and g0(·|θk) is the density function to generate data from component k. Given the observations ~x, a slice latent variable ui is introduced for each xi so that it only considers those components whose jump sizes Jk’s are larger than the corresponding ui’s. Furthermore, an auxiliary variable v is introduced to decouple each individual jump Jk and their infinite sum of the jumps∑∞ l=1 Jl appeared in the denominators of rk’s. It is shown in (Griffin & Walker, 2011) that the posterior of the infinite mixture model (2) with the above auxiliary variables is proportional to\nPµ(~θ, J1, · · · , JK ,K, ~u, L,~s, v|~x,H, ρη) ∝\nexp { −v\nK∑ k=1 Jk\n} exp { −M ∫ L 0 (1− exp {−vt}) ρη(t)dt }\nvN−1p(J1, · · · , JK) K∏ k=1 h(θk) N∏ i=1 1(Jsi > ui)g0(xi|θsi), (3)\nwhere 1(a) is an indicator function returning 1 if a is true and 0 otherwise, h(·) is the density of H(·), L = min{~u}, and p(J1, · · · , JK) = ∏K k=1 ρη(Jk)∫ ∞ L ρη(t)dt is the distribution for the jumps which are larger than L derived from the underlying Poisson process. Sampling for this mixture model iteratively cycles over {~θ, (J1, · · · , JK),K, ~u,~s, v} based on (3). Please refer to (Section 1.3 Chen et al., 2012) for more details."
    }, {
      "heading" : "2.3. Normalized generalized Gamma processes",
      "text" : "In this paper, we consider the normalized generalized Gamma processes. Generalized Gamma processes (Lijoi et al., 2007) (GGP) are random measures with the Lévy measure\nν(dt,dx) = M e−bt\nt1+a H(dx), b > 0, 0 < a < 1. (4)\nBy normalizing the GGP, we obtain the normalized generalized Gamma process (NGG)2. One of the most familiar special cases is the Dirichlet process, which is a normalized Gamma process where a → 0 and b = 1 and the concentration parameter appears as M .\nCrucially, unlike the DP, the NGG can produce the power-law phenomenon.\nProposition 1 ((Lijoi et al., 2007)) Let Kn be the number of components induced by the NGG with parameters a an b or the Dirichlet process with total mass M . Then for the NGG, Kn/n\na → Sab almost surely, where Sab is a strictly positive random variable parameterized by a and b. For the DP, Kn/ log(n)→M .\nTherefore, in order to better analyze certain kinds of real data, we propose to use the NGG in place of the Dirichlet process. In the next section, we propose a dynamic topic model which extends two major advances of the Dirichlet process: the HDP (Teh et al., 2006) and the dependent Dirichlet process (Lin et al., 2010), to normalized random measures."
    }, {
      "heading" : "3. Dynamic topic modeling with dependent hierarchical NRMs",
      "text" : "Our main interest is to construct a dynamic topic model that inherits partial exchangeability, meaning that the documents within each time frame are exchangeable, while between time frames they are not. To achieve this, it is crucial to model the dependency of the topics between different time frames. In particular, a topic can either inherit from the topics of earlier time frames with certain transformation, or be a completely new one which is ”born” in the current time frame. The above idea can be modeled by a series of hierarchical NRMs, one per time frame. Between the time frames, these hierarchical NRMs depend on each other through three dependency operators: superposition, subsampling and point transition, which will be defined below. The corresponding graphical model is shown in Figure 1(left) and the generating process for the model is as follows:\n• Generating independent NRMs µm for time frame m = 1, · · · , n:\nµm|H, η0 ∼ NRM(M0, η0, P0) (5)\nwhere H(·) = M0P0(·). M0 is the total mass for µm and P0 is the base distribution. In this paper, P0 is the Dirichlet distribution, η0 is the set of\n2In NGG, b can be absolved into M , thus we usually set b = 1, see (Chen et al., 2012) for detail.\nhyperparameters of the corresponding NRM, e.g., in NGG, η0 = {a, b}.\n• Generating dependent NRMs µ′m (from µm and µ′m−1), for time frame m > 1:\nµ′m = T (S q(µ′m−1))⊕ µm . (6)\nwhere the three dependency operators superposition (⊕), subsampling (Sq(·)) with acceptance rate q, and point transition (T (·)) are generalized from those of Dirichlet process (Lin et al., 2010). We will discuss them in more details in the following subsection.\n• Generating hierarchical NRM mixtures (µmj , θmji, xmji) for time frame m = 1, · · · , n, document j = 1, · · · , Nm, word i = 1, · · · ,Wmj :\nµmj = NRM(Mm, ηm, µ ′ m), (7) θmji|µmj ∼ µmj , xmji|θmji ∼ g0(·|θmji)\nwhere Mm is the total mass for µmj , g0(·|θmji) denotes the density function to generate data xmji from atom θmji."
    }, {
      "heading" : "3.1. The three dependency operators",
      "text" : "Adapting from the dependent Dirichlet process (Lin et al., 2010), the three dependency operators for the NRMs are defined as follows.\nSuperposition of normalized random measures Given n independent NRMs µ1, · · · , µn on X, the superposition (⊕) is:\nµ1 ⊕ µ2 ⊕ · · · ⊕ µn := c1µ1 + c2µ2 + · · ·+ cnµn .\nwhere the weights cm = µ̃m(X)∑ j µ̃j(X) and µ̃m is the unnormalized version of µm.\nSubsampling of normalized random measures Given a NRM µ = ∑∞ k=1 rkδθk on X, and a Bernoulli parameter q ∈ [0, 1], the subsampling of µ, is defined as\nSq(µ) := ∑ k:zk=1 rk∑ j zjrj δθk , (8)\nwhere zk ∼ Bernoulli(q) are Bernoulli random variables with acceptance rate q.\nPoint transition of normalized random measures Given a NRM µ = ∑∞ k=1 rkδθk on X, the point transition of µ, is to draw atoms θ′k from a transformed base measure to yield a new NRM as T (µ) := ∑∞ k=1 rkδθ′k .\nPoint transitions can be done in different ways with different transition kernels T (·). In this paper, following (Lin et al., 2010), when inheriting from NRM µ, we draw atoms θ′k from the base measure as µ conditioned on its current statistics. Other ways of constructing transition kernels are left for further research."
    }, {
      "heading" : "3.2. Properties of the dependency operators",
      "text" : "The three dependency operators on the NRMs inherit some of the nice properties from the underlying Poisson process. It not only enables quantitatively controlling dependencies introduced after and before the operations, as is shown in (Section 4 Chen et al., 2012), but also maintains a nice equivalence relation between the NRM’s and the corresponding CRM’s. In the following theorem, we will use ⊕̃, S̃q(·) and T̃ (·) to denote the three operations on their corresponding CRM’s3.\nTheorem 2 The following time dependent random measures (9) and (10) are equivalent:\n• Manipulate the normalized random measures:\nµ′m ∼ T (Sq(µ′m−1))⊕ µm,m > 1. (9)\n• Manipulate the completely random measures:\nµ̃′m ∼ T̃ (S̃q(µ̃′m−1))⊕ µ̃m,m > 1. µ′m = µ̃′m\nµ̃′m(X) , (10)\n3The definitions of ⊕̃, S̃q(·) and T̃ (·) are similar to the NRMs’, see (Section 1.5.1 Chen et al., 2012) for details.\nFurthermore, the resulting NRMs µ′m’s give the following:\nµ′m = m∑ j=1\n( qm−j µ̃j ) (X)∑m\nj′=1 (q m−j′ µ̃j′) (X)\nTm−j(µj),m > 1\nwhere qm−j µ̃ is the random measure with Lévy measure qm−jν(dt,dx) (ν(dt,dx) is the Lévy measure of µ̃). Tm−j(µ) denotes point transition on µ for (m−j) times ."
    }, {
      "heading" : "3.3. Reformulation of the proposed model",
      "text" : "Theorem 2 in the last section allows us to first take superposition, subsampling, and point transition on the completely random measures µ̃g’s and then do the normalization. Therefore, we make use of Theorem 2 to obtain the dynamic topic model in Figure 1(right) by expanding the recusive formula in (10), which is equivalent to the left one.\nThe generating process of the new model is:\n• Generating independent CRM’s µ̃m for time frame m = 1, · · · , n, following (1).\n• Generating µ′m for time frame m > 1, following (10).\n• Generating hierarchical NRM mixtures (µmj , θmji, xmji) following (7).\nThe reason for this reformulation is because the inference on the model in Figure 1(left) appears to be infeasible. In general, the posterior of an NRM introduce\ncomplex dependencies between jumps, thus sampling is unclear after taking the three dependency operators.\nOn the other hand, the model in Figure 1(right) is more amenable to computation because the NRMs and the three operators are decoupled. It allows us to first generate the dependent CRM’s, then use the slice sampler introduced in Section 2.2 to sample the posterior of the corresponding NRMs. From now on, we will focus on the model in Figure 1(right). In the next section, we discuss its sampling procedure."
    }, {
      "heading" : "4. Sampling",
      "text" : "To introduce our sampling method we use the familiar Chinese restaurant metaphor (e.g. (Teh et al., 2006)) to explain key statistics. In this model customers for the variable µmj correspond to words in a document, restaurants to documents, and dishes to topics. In time frame m,\n• xmji: the customer i in the jth restaurant.\n• smji: the dish that xmji is eating.\n• nmjk: nmjk = ∑ i δsmji=k ,\nthe number of customers in µmj eating dish k.\n• tmjr: the table r in the jth restaurant.\n• ψmjr: the dish that the table tmjr is serving.\n• n′mk: n′mk = ∑ j ∑ r δψmjr=k,\nthe number of customers4 in µ′m eating dish k.\n• ñ′mk: ñ′mk = n′mk, the number of customers in µ̃′m eating dish k.\n• ñmk: ñmk = ∑ m′≥m ñ ′ m′k,\nthe number of customers in µ̃m eating dish k.\nWe will do the sampling by marginalizing out µmj ’s. As it turns out, the remaining random variables that require sampling are smji, n ′ mk, as well as\nµ̃m = ∑ k Jmkδθk , µ̃ ′ m = ∑ k J ′mkδθk\nNote the tmjr and ψmjr are not sampled as we sample the n′mk directly. Thus our sampler deals with the following latent statistics and variables: smji, n ′ mk, Jmk, J ′ mk and some auxiliary variables are sampled to support these.\n4the customers in µ′m corresponds to the tables in µmj . For convenient, we also regard a CRM as a restaurant.\nSampling Jmk. Given ñmk, we use the slice sampler introduced in (Griffin & Walker, 2011) to sample these jumps, with the posterior given in (3). Note that the mass Mm’s are also sampled, see (Sec.1.3 Chen et al., 2012). The resulting {Jmk} are those jumps that exceed a threshold defined in the slice sampler, thus the number of jumps is finite.\nSampling J ′mk. J ′ mk is obtained by subsampling of {Jm′k}m′≤m5. By using a Bernoulli variable zmk,\nJ ′mk =\n{ Jm′k if zmk = 1\n0 if zmk = 0.\nWe compute the posterior p(zmk = 1|µ̃m, {ñ′mk}) to decide whether to inherit this jump to µ̃′m or not. These posteriors are given in (Corollary 3 Chen et al., 2012). In practice, we found it mixes faster if we integrate out zmk’s. (Lemma 9 Chen et al., 2012) shows that q-subsampling of a CRM with Lévy measure ν(·) results in another CRM with Lévy measure qν(·), thus the jump sizes in the resultant CRM are scaled by q, meaning that J ′mk = q m−m′Jm′k. After the sampling of {J ′mk}, we normalize it and obtain the NRM µ′m, µ ′ m = ∑ k rmkδθk where rmk =\nJ ′mk/ ∑ k′ J ′ mk′\nSampling smji, n ′ mk. The following procedures are similar to sampling an HDP. The only difference is that µmj and µ ′ m are NRMs instead of DPs. The sampling method goes as follows:\n• Sampling smji: We use a similar strategy as the sampling by direct assignment algorithm for the HDP (Teh et al., 2006), the conditional posterior of smji is:\np(smji = k|·) ∝ (ωk + ω0Mmrmk)g0(xmji|θk)\nwhere ω0 and ωk depend on the corresponding Lévy measure of µmj (see (Theorem 2 James et al., 2009)). When µmj is a DP, then ωk ∝ nmjk and ω0 ∝ 1. When µmj is a NGG, ωk ∝ nmjk − a and ω0 ∝ a(b + vmj)a, where vmj is the introduced auxiliary variables which can be sampled by an adaptive-rejection sampler using the posterior given in (Proposition 1 James et al., 2009).\n• Sampling n′mk: Using the similar strategy as in (Teh et al., 2006), we sample n′mk by simulating the (generalized) Chinese Restaurant Process, following the prediction rule (the probabilities of\n5 Since all the atoms across {µ̃m′} are unique, J ′mk is inherited from only one of {Jm′k}.\ngenerating a new table or sitting on existing tables) of µmk in (Proposition 2 James et al., 2009)."
    }, {
      "heading" : "5. Experiments",
      "text" : ""
    }, {
      "heading" : "5.1. Power-law in the NGG",
      "text" : "We first investigate the power-law phenomena in the NGG, we sample it using the scheme of (James et al., 2009) and compare it with the DP in Figure 2."
    }, {
      "heading" : "5.2. Datasets",
      "text" : "We tested our time dependent dynamic topic model on 9 datasets, removing stop-words and words appearing less than 5 times. ICML, JMLR, TPAMI are crawled from their websites and the abstracts are parsed. The preprocessed NIPS dataset is from (Globerson et al., 2007). The Person dataset is extracted from Reuters RCV1 using the query “person” under Lucene. The Twitter datasets are updates from three sports twitter accounts: ESPN FirstTake (Twitter1), sportsguy33 (Twitter2) and SportsNation (Twitter3) obtained with the TweetStream API (http://pypi.python.org/pypi/tweetstream) to collect the last 3200 updates from each. The Daily Kos blogs (BDT) were pre-processed by (Yano et al., 2009). Statistics for the data sets are given in Table 1.\nIllustration: Figure 3 gives an example of topic evolutions in the Twitter2 dataset. We can clearly see that the three popular sports in the USA, i.e., basketball, football and baseball, evolve reasonably with time. For example, MLB starts in April each year, showing a peak in baseball topic, and then slowly evolves with decreasing topic proportions. Also, in August one foot-\nball topic is born, indicating a new season begins. Figure 4 gives an example of the word probability change in a single topic for the JMLR."
    }, {
      "heading" : "5.3. Quantitative Evaluations",
      "text" : "Comparisons We first compare our model with two popular dynamic topic models where the author’s own code was available for our use: (1) the dynamic topic model by Blei and Lafferty (Blei & Lafferty, 2006) and (2) the hierarchical Dirichlet process, where we used a three level HDP, with the middle level DP’s representing the base topic distribution for the documents in a particular time. For fair comparison, similar to (Blei & Lafferty, 2006), we held out the data in previous time but used their statistics to help the training of the current time data, this is implemented in the HDP code by Teh. Furthermore, we also tested the proposed model without power-law, which is to use a DP instead of an NGG. We tested our model on the 9 datasets, for each dataset we used 80% for training and held out 20% for testing. The hyperparameters for DHNGG is set to a = 0.2 in this set of experiments with subsampling rate being 0.9, which is found to work well in practice. The topic-word distributions are symmetric Dirichlet with prior set to 0.3. Table 2 shows the test\nlog-likelihoods for all these methods, which are calculated by first removing the test words from the topics and adding them back one by one and collecting the add-in probabilities as the testing likelihood (Teh et al., 2006). For all the methods we ran 2000 burn in iterations, followed by 200 iterations to collect samples. The results are averages over these samples.\nFrom Table 2 we see the proposed model DHNGG works best, with an improvement of 1%-3% in test log-likelihoods over the HDP model. In contrast the time dependent model iDTM of Ahmed & Xing (2010) only showed a 0.1% improvement over HDP on NIPS, implying the superiority of DHNRM over iDTM.\nHyperparameter sensitivity In NGG, there are hyperparameters a and b, where a controls the behavior of the power-law. In this section we study the influences of these two hyperparameters to the model. We varied a among (0.1, 0.2, 0.3, 0.5, 0.7, 0.9) while fixed the subsampling rate to 0.9 in this experiment. We run these settings on all these datasets, the training likelihoods are shown in Figure 5. From these results we consider a = 0.2 to be a good choice in practice.\nInfluence of the subsampling rate One of the distinct features of our model compared to other time dependent topic models is that the dependency comes partially from subsampling the previous time random measures, thus it is interesting to study the impact of subsampling rates to this model. In this experiment, we fixed a = 0.2, and varied the subsampling rate q among (0.1, 0.2, 0.3, 0.5, 0.7, 0.9, 1.0). The results are shown in Figure 6. From Figure 6, it is interesting to see that on the academic datasets, e.g., ICML,JMLR, the best results are achieved when q is approximately equal to 1; these datasets have higher correlations. While for the Twitter datasets, the best results are\nachieved when q is equal to 0.5 ∼ 0.7, indicating that people tend to discuss more changing topics in these datasets."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We proposed dependent hierarchical normalized random measures. Specifically, we extend the three dependency operations for the Dirichlet process to normalized random measures and show how dependent models on NRMs can be implemented via dependent models on the underlying Poisson processes. Then we applied our model to dynamic topic modeling. Experimental results on different kinds of datasets demonstrate the superior performance of our model over existing models such as DTM, HDP and iDTM."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the reviewers for their valuable comments and Pinar Yanardag for collecting the Twitter data. NICTA\nis funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Center of Excellence program."
    } ],
    "references" : [ {
      "title" : "Timeline: A dynamic hierarchical Dirichlet process model for recovering birth/death and evolution of topics in text stream",
      "author" : [ "A. Ahmed", "E.P. Xing" ],
      "venue" : "In UAI, pp",
      "citeRegEx" : "Ahmed and Xing,? \\Q2010\\E",
      "shortCiteRegEx" : "Ahmed and Xing",
      "year" : 2010
    }, {
      "title" : "Forgetting counts: constant memory inference for a dependent hierarchical Pitman-Yor process",
      "author" : [ "N. Bartlett", "D. Pfau", "F. Wood" ],
      "venue" : "In ICML ’10",
      "citeRegEx" : "Bartlett et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bartlett et al\\.",
      "year" : 2010
    }, {
      "title" : "Dynamic topic models",
      "author" : [ "D. Blei", "J. Lafferty" ],
      "venue" : "In ICML ’06",
      "citeRegEx" : "Blei and Lafferty,? \\Q2006\\E",
      "shortCiteRegEx" : "Blei and Lafferty",
      "year" : 2006
    }, {
      "title" : "Theory of dependent hierarchical normalized random measures",
      "author" : [ "C. Chen", "W. Buntine", "N. Ding" ],
      "venue" : "Technical Report arXiv:1205.4159, ANU and NICTA,",
      "citeRegEx" : "Chen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2012
    }, {
      "title" : "Posterior simulation of normalized random measure mixtures",
      "author" : [ "J.E. Griffin", "S.G. Walker" ],
      "venue" : "J. Comput. Graph. Stat.,",
      "citeRegEx" : "Griffin and Walker,? \\Q2011\\E",
      "shortCiteRegEx" : "Griffin and Walker",
      "year" : 2011
    }, {
      "title" : "Conjugacy as a distinctive feature of the Dirichlet process",
      "author" : [ "L.F. James", "A. Lijoi", "I. Prünster" ],
      "venue" : "Scand. J. Stat.,",
      "citeRegEx" : "James et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "James et al\\.",
      "year" : 2006
    }, {
      "title" : "Posterior analysis for normalized random measures with independent increments",
      "author" : [ "L.F. James", "A. Lijoi", "I. Prünster" ],
      "venue" : "Scand. J. Stat.,",
      "citeRegEx" : "James et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "James et al\\.",
      "year" : 2009
    }, {
      "title" : "Controlling the reinforcement in Bayesian non-parametric mixture models",
      "author" : [ "A. Lijoi", "R.H. Mena", "I. Prünster" ],
      "venue" : "J. R. Stat. Soc. Ser. B,",
      "citeRegEx" : "Lijoi et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Lijoi et al\\.",
      "year" : 2007
    }, {
      "title" : "Construction of dependent Dirichlet processes based on Poisson processes",
      "author" : [ "D. Lin", "E. Grimson", "J. Fisher" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Lin et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2010
    }, {
      "title" : "Dependent nonparametric processes",
      "author" : [ "S.N. MacEachern" ],
      "venue" : "In Proc. of the SBSS",
      "citeRegEx" : "MacEachern,? \\Q1999\\E",
      "shortCiteRegEx" : "MacEachern",
      "year" : 1999
    }, {
      "title" : "The dynamic hierarchical Dirichlet process",
      "author" : [ "L. Ren", "D.B. Dunson", "L. Carin" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ren et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2008
    }, {
      "title" : "Statistical topic models for multi-label document classification",
      "author" : [ "T. Rubin", "A. Chambers", "P. Smyth", "M. Steyvers" ],
      "venue" : "Technical Report arXiv:1107.2462v2,",
      "citeRegEx" : "Rubin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rubin et al\\.",
      "year" : 2011
    }, {
      "title" : "Spectral Chinese restaurant processes: Nonparametric clustering based on similarities",
      "author" : [ "R. Socher", "A. Maas", "C.D. Manning" ],
      "venue" : "In AISTATS",
      "citeRegEx" : "Socher et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Shared segmentation of natural scenes using dependent Pitman-Yor processes",
      "author" : [ "E.B. Sudderth", "M.I. Jordan" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Sudderth and Jordan,? \\Q2008\\E",
      "shortCiteRegEx" : "Sudderth and Jordan",
      "year" : 2008
    }, {
      "title" : "A hierarchical Bayesian language model based on Pitman-Yor processes",
      "author" : [ "Y.W. Teh" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Teh,? \\Q2006\\E",
      "shortCiteRegEx" : "Teh",
      "year" : 2006
    }, {
      "title" : "Hierarchical Dirichlet processes",
      "author" : [ "Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei" ],
      "venue" : "Journal of the ASA,",
      "citeRegEx" : "Teh et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2006
    }, {
      "title" : "Predicting response to political blog posts with topic models",
      "author" : [ "T. Yano", "W. Cohen", "N.A. Smith" ],
      "venue" : "In Proc. of the NAACL-HLT,",
      "citeRegEx" : "Yano et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yano et al\\.",
      "year" : 2009
    }, {
      "title" : "Evolutionary hierarchical Dirichlet processes for multiple correlated time-varying corpora",
      "author" : [ "J. Zhang", "Y. Song", "C. Zhang", "S. Liu" ],
      "venue" : "In KDD",
      "citeRegEx" : "Zhang et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Dirichlet processes and their variants are popular in recent years, with applications found in diverse discrete domains such as topic modeling (Teh et al., 2006), ngram modeling (Teh, 2006), clustering (Socher et al.",
      "startOffset" : 143,
      "endOffset" : 161
    }, {
      "referenceID" : 14,
      "context" : ", 2006), ngram modeling (Teh, 2006), clustering (Socher et al.",
      "startOffset" : 24,
      "endOffset" : 35
    }, {
      "referenceID" : 12,
      "context" : ", 2006), ngram modeling (Teh, 2006), clustering (Socher et al., 2011), and image modeling (Li et al.",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "When modeling dynamic data or data from multiple sources, dependent nonparametric Bayesian models (MacEachern, 1999) are needed in order to harness related or previous information.",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 15,
      "context" : "Among these models, the hierarchical Dirichlet process (HDP) (Teh et al., 2006) is the most popular one.",
      "startOffset" : 61,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "To overcome the full exchangeability limitation, several dependent Dirichlet process models have been proposed, for example, the dynamic HDP (Ren et al., 2008), the evolutionary HDP (Zhang et al.",
      "startOffset" : 141,
      "endOffset" : 159
    }, {
      "referenceID" : 17,
      "context" : ", 2008), the evolutionary HDP (Zhang et al., 2010), and the recurrent Chinese Restaurant process (Ahmed & Xing, 2010).",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "Dirichlet processes are used because of simplicity and conjugacy (James et al., 2006).",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 8,
      "context" : "Dependent DPs have also been constructed using the underlying Poisson processes (Lin et al., 2010).",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : ", in images (Sudderth & Jordan, 2008), in topic-word distributions (Teh, 2006) and in document topic (label) distributions (Rubin et al.",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : ", in images (Sudderth & Jordan, 2008), in topic-word distributions (Teh, 2006) and in document topic (label) distributions (Rubin et al., 2011).",
      "startOffset" : 123,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "For example, Bartlett et al. (2010) proposed a dependent hierarchical PitmanYor process that only allows deletion of atoms, while Sudderth & Jordan (2008) construct the dependent Pitman-Yor process by only allowing dependencies between atoms.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "For example, Bartlett et al. (2010) proposed a dependent hierarchical PitmanYor process that only allows deletion of atoms, while Sudderth & Jordan (2008) construct the dependent Pitman-Yor process by only allowing dependencies between atoms.",
      "startOffset" : 13,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "In this paper, we use a larger class of stochastic processes called normalized random measures with independent increments (NRM) (James et al., 2009).",
      "startOffset" : 129,
      "endOffset" : 149
    }, {
      "referenceID" : 8,
      "context" : "Given this, following (Lin et al., 2010), we analogically define superposition, subsampling and point transition on these normalized random measures, and construct a time dependent hierarchical model for dynamic topic modeling.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "All proofs and some extended theories are available in (Chen et al., 2012).",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "This background on random measures follows (James et al., 2009).",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "Generalized Gamma processes (Lijoi et al., 2007) (GGP) are random measures with the Lévy measure",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "Proposition 1 ((Lijoi et al., 2007)) Let Kn be the number of components induced by the NGG with parameters a an b or the Dirichlet process with total mass M .",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 15,
      "context" : "In the next section, we propose a dynamic topic model which extends two major advances of the Dirichlet process: the HDP (Teh et al., 2006) and the dependent Dirichlet process (Lin et al.",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 8,
      "context" : ", 2006) and the dependent Dirichlet process (Lin et al., 2010), to normalized random measures.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "In this paper, P0 is the Dirichlet distribution, η0 is the set of In NGG, b can be absolved into M , thus we usually set b = 1, see (Chen et al., 2012) for detail.",
      "startOffset" : 132,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "where the three dependency operators superposition (⊕), subsampling (S(·)) with acceptance rate q, and point transition (T (·)) are generalized from those of Dirichlet process (Lin et al., 2010).",
      "startOffset" : 176,
      "endOffset" : 194
    }, {
      "referenceID" : 8,
      "context" : "Adapting from the dependent Dirichlet process (Lin et al., 2010), the three dependency operators for the NRMs are defined as follows.",
      "startOffset" : 46,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "In this paper, following (Lin et al., 2010), when inheriting from NRM μ, we draw atoms θ′ k from the base measure as μ conditioned on its current statistics.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 15,
      "context" : "(Teh et al., 2006)) to explain key statistics.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 15,
      "context" : "• Sampling smji: We use a similar strategy as the sampling by direct assignment algorithm for the HDP (Teh et al., 2006), the conditional posterior of smji is: p(smji = k|·) ∝ (ωk + ω0Mmrmk)g0(xmji|θk) where ω0 and ωk depend on the corresponding Lévy measure of μmj (see (Theorem 2 James et al.",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 15,
      "context" : "• Sampling nmk: Using the similar strategy as in (Teh et al., 2006), we sample nmk by simulating the (generalized) Chinese Restaurant Process, following the prediction rule (the probabilities of 5 Since all the atoms across {μ̃m′} are unique, J ′ mk is inherited from only one of {Jm′k}.",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "Power-law in the NGG We first investigate the power-law phenomena in the NGG, we sample it using the scheme of (James et al., 2009) and compare it with the DP in Figure 2.",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "The Daily Kos blogs (BDT) were pre-processed by (Yano et al., 2009).",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "log-likelihoods for all these methods, which are calculated by first removing the test words from the topics and adding them back one by one and collecting the add-in probabilities as the testing likelihood (Teh et al., 2006).",
      "startOffset" : 207,
      "endOffset" : 225
    }, {
      "referenceID" : 14,
      "context" : "log-likelihoods for all these methods, which are calculated by first removing the test words from the topics and adding them back one by one and collecting the add-in probabilities as the testing likelihood (Teh et al., 2006). For all the methods we ran 2000 burn in iterations, followed by 200 iterations to collect samples. The results are averages over these samples. From Table 2 we see the proposed model DHNGG works best, with an improvement of 1%-3% in test log-likelihoods over the HDP model. In contrast the time dependent model iDTM of Ahmed & Xing (2010) only showed a 0.",
      "startOffset" : 208,
      "endOffset" : 566
    } ],
    "year" : 2012,
    "abstractText" : "We develop dependent hierarchical normalized random measures and apply them to dynamic topic modeling. The dependency arises via superposition, subsampling and point transition on the underlying Poisson processes of these measures. The measures used include normalised generalised Gamma processes that demonstrate power law properties, unlike Dirichlet processes used previously in dynamic topic modeling. Inference for the model includes adapting a recently developed slice sampler to directly manipulate the underlying Poisson process. Experiments performed on news, blogs, academic and Twitter collections demonstrate the technique gives superior perplexity over a number of previous models.",
    "creator" : "LaTeX with hyperref package"
  }
}