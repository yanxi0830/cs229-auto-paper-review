{
  "name" : "1207.5554.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bellman Error Based Feature Generation using Random Projections on Sparse Spaces",
    "authors" : [ "Mahdi Milani Fard", "Yuri Grinberg", "Joelle Pineau", "Doina Precup" ],
    "emails" : [ "mmilan1@cs.mcgill.ca", "ygrinb@cs.mcgill.ca", "amirf@cs.mcgill.ca", "jpineau@cs.mcgill.ca", "dprecup@cs.mcgill.ca" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The accuracy of parametrized policy evaluation depends on the quality of the features used for estimating the value function. Hence, feature generation/selection in reinforcement learning (RL) has received a lot of attention (e.g. [1, 2, 3, 4, 5]). We focus on methods that aim to generate features in the direction of the Bellman error of the current value estimates (Bellman Error Based, or BEBF, features). Successive addition of exact BEBFs has been shown to reduce the error of a linear value estimator at a rate similar to value iteration [6]. Unlike fitted value iteration [7] which works with a fixed feature set, iterative BEBF generation gradually increases the complexity of the hypothesis space by adding new features and thus does not diverge, as long as the error in the generation does not cancel out the contraction effect of the Bellman operator [6].\nA number of methods have been introduced in RL to generate features related to the Bellman error, with a fair amount of success [5, 1, 4, 6, 3], but many of them fail to scale to high dimensional state spaces. In this work, we present an algorithm that uses the idea of applying random projections specifically in very large and sparse feature spaces. In short, we iteratively project the original features into exponentially smaller-dimensional spaces and apply linear regression to temporal differences to approximate BEBFs. We carry out a finite sample analysis that helps determine valid sizes of the projections and the number of iterations. Our analysis holds for both finite and continuous state spaces and is easy to apply with discretized or tile-coded features.\nThe proposed method is computationally favourable to many other feature extraction methods in high dimensional spaces, in that each iteration takes poly-logarithmic time in the number of dimensions. While providing guarantees on the reduction of the error, it needs minimal domain knowledge, as agnostic random projections are used in the process.\nOur empirical analysis shows how the algorithm can be applied to general tile-coded spaces. Our results indicate that the proposed method outperforms both gradient type methods, and also LSTD with random projections [8]. The algorithm is robust to the choice of parameters and needs minimal tweaking to work. It runs fast and has small memory complexity.\nar X\niv :1\n20 7.\n55 54\nv1 [\ncs .L\nG ]\n2 3\nJu l 2"
    }, {
      "heading" : "2 Notations and Background",
      "text" : "Throughout this paper, column vectors are represented by lower case bold letters, and matrices are represented by bold capital letters. |.| denotes the size of a set, and M(X ) is the set of measures on X . ‖.‖0 is Donoho’s zero “norm” indicating the number of non-zero elements in a vector. ‖.‖ denotes the L2 norm for vectors and the operator norm for matrices: ‖M‖ = supv ‖Mv‖/‖v‖. The Frobenius norm of a matrix is the defined as: ‖M‖F = √∑ i,j M 2 i,j . Also, we denote the Moore-Penrose pseudo-inverse of a matrix M with M†. The weighted L2 norm is defined as:\n‖f(x)‖ρ(x) = (∫ |f(x)|2 dρ(x) )(1/2) . (1)\nWe focus on spaces that are large, bounded and k-sparse. Our state is represented by a vector x ∈ X of D features, having ‖x‖ ≤ 1. We assume that x is k-sparse in some known or unknown basis Ψ, implying that X , {Ψz, s.t. ‖z‖0 ≤ k and ‖z‖ ≤ 1}. Such spaces occur both naturally (e.g. image, audio and video signals [9]) and also from most discretization-based methods (e.g. tile-coding)."
    }, {
      "heading" : "2.1 Markov Decision Process and Fast Mixing",
      "text" : "A Markov Decision Process (MDP)M = (X ,A, T,R) is defined by a (possibly infinite) set of states X , a set of actions A, a transition probability kernel T : X ×A →M(X ), where T (.|x, a) defines the distribution of next state given that action a is taken in state x, and a (possibly stochastic) reward function R : X × A → M([0, Rmax]). Throughout the paper, we focus on discounted-reward MDPs, with the discount factor denoted by γ ∈ [0, 1). At discrete time steps, the reinforcement learning agent chooses an action and receives a reward. The environment then changes to a new state according to the transition kernel.\nA policy is a (possibly stochastic) function from states to actions. The value of a state x for policy π, denoted by V π(x), is the expected value of the discounted sum of rewards ( ∑ t γ\ntrt) if the agent starts in state x and acts according to policy π. Defining R(x, π(x)) to be the expected reward at point x under policy π, the value function satisfies the Bellman equation:\nV π(x) = R(x, π(x)) + γ ∫ V π(y)T (dy|x, π(x)). (2)\nThere are many methods developed to find the value of a policy (policy evaluation) when the transition and reward functions are known. Among these there are dynamic programming methods in which one iteratively applies the Bellman operator [10] to an initial guess of the optimal value function. The Bellman operator T on a value estimate V is defined as:\nT V (x) = R(x, π(x)) + γ ∫ V (y)T (dy|x, π(x)), (3)\nWhen the transition and reward models are not known, one can use a finite sample set of transitions to learn an approximate value function. Least-squares temporal difference learning (LSTD) and its derivations [11, 12] are among the methods used to learn a value function based on a finite sample. LSTD type methods are efficient in their use of data, but fail to scale to high dimensional state spaces due to extensive computational complexity. Using LSTD in spaces induced by random projections is a way of dealing with such domains [8]. Stochastic gradient descent type method are also used for value function approximation in high dimensional state spaces, some with proofs of convergence in online and offline settings [13]. However gradient type methods typically have slow convergence rates and do not make efficient use of the data.\nTo arrive at a finite sample bound on the error of our algorithm, we assume certain mixing conditions on the Markov chain in question. We assume that the Markov chain uniformly quickly forgets its past (defined in detail in the appendix). There are many classes of chains that fall into this category (see e.g. [14]). Conditions under which a Markov chain uniformly quickly forgets its past are of major interest and are discussed in the appendix."
    }, {
      "heading" : "2.2 Bellman Error Based Feature Generation",
      "text" : "In high-dimensional state spaces, direct estimation of the value function fails to provide good results with small numbers of sampled transitions. Feature selection/extraction methods have thus been used to build better approximation spaces for the value functions [1, 2, 3, 4, 5]. Among these, we focus on methods that aim to generate features in the direction of the the Bellman error defined as:\neV (.) = T V (.)− V (.). (4) Let Sn = ((xt, rt)nt=1) be a random sample of size n, collected on an MDP with a fixed policy. Given an estimate V of the value function, temporal difference (TD) errors are defined to be:\nδt = rt + γV (xt+1)− V (xt). (5) It is easy to show that the expectation of the temporal difference given a point xt equals the Bellman error on that point [10]. TD-errors are thus proxies to estimating the Bellman error.\nUsing temporal differences, Menache et al. [15] introduced two algorithms to adapt basis functions as features for linear function approximation. Keller et al. [3] applied neighbourhood component analysis as a dimensionality reduction technique to construct a low dimensional state space based on the TD-error. In their work, they iteratively add feature that would help predict the Bellman error. Parr et al. [6] later showed that any BEBF extraction method with small angular approximation error will provably tighten approximation error in the value function estimate.\nOnline feature extraction methods have also been studied in the RL literature. Geramifard et al. [5] have recently introduced the incremental Feature Dependency Discovery (iFDD) as a fast online algorithm to extract non-linear binary feature for linear function approximation. In their work, one keeps a list of candidate features (non-linear combination of two active features), and among these adds the features that correlates the most with the TD-error.\nIn this work, we propose a method that generates BEBFs using linear regression in a small space induced by random projection. We first project the state features into a much smaller space and then regress a hyperplane to the TD-errors. For simplicity, we assume that regardless of the current estimate of the value function, the Bellman error is always linearly representable in the original feature space. This seems like a strong assumption, but is true, for example, in virtually any discretized space, and is also likely to hold in very high dimensional feature spaces1."
    }, {
      "heading" : "2.3 Random Projections and Inner Product",
      "text" : "It is well known that random projections of appropriate sizes preserve enough information for exact reconstruction with high probability (see e.g. [16, 17]). This is because random projections are norm and distance-preserving in many classes of feature spaces [17].\nThere are several types of random projection matrices that can be used. In this work, we assume that each entry in a projection ΦD×d is an i.i.d. sample from a Gaussian 2:\nφi,j = N (0, 1/d). (6)\nRecently, it has been shown that random projections of appropriate sizes preserve linearity of a target function on sparse feature spaces. A bound introduced in [18] and later tightened in [19] shows that if a function is linear in a sparse space, it is almost linear in an exponentially smaller projected space. An immediate lemma based on Theorem 2 of [19] bounds the bias induced by random projections: Lemma 1. Let ΦD×d be a random projection according to Eqn 6. Let X be a D-dimensional k-sparse space. Fix w ∈ RD and 1 > ξ > 0. Then, with probability > 1− ξ:\n∀x ∈ X : ∣∣〈ΦTw,ΦTx〉 − 〈w,x〉∣∣ ≤ (ξ)prj ‖w‖‖x‖, (7)\nwhere (ξ)prj = √ 48k d log 4D ξ .\n1In more general cases, the analysis has to be done with respect to the projected Bellman error (see e.g. [6]). We assume linearity of the Bellman error to simplify the derivations. 2The elements of the projection are typically taken to be distributed with N (0, 1/D), but we scale them by√ D/d, so that we avoid scaling the projected values (see e.g. [16]).\nHence, projections of size Õ(k logD) preserve the linearity up to an arbitrary constant. Along with the analysis of the variance of the estimators, this helps us bound the prediction error of the linear fit in the compressed space."
    }, {
      "heading" : "3 Compressed Linear BEBFs",
      "text" : "Linear function approximators can be used to estimate the value of a given state. Let Vm be an estimated value function described in a linear space defined by a feature set {ψ1, . . . ψm}. Parr et al. [6] show that if we add a new BEBF ψm+1 = eVm to the feature set, (with mild assumptions) the approximation error on the new linear space shrinks by a factor of γ. They also show that if we can estimate the Bellman error within a constant angular error, cos−1(γ), the error will still shrink.\nEstimating the Bellman error by regressing to temporal differences in high-dimensional sparse spaces can result in large prediction error. However, as discussed in Lemma 1, random projections were shown to exponentially reduce the dimension of a sparse feature space, only at the cost of a controlled constant bias. A variance analysis along with proper mixing conditions can also bound the estimation error due to the variance in MDP returns. One can thus bound the total prediction error with much smaller number of sampled transitions when the regression is applied in the compressed space.\nIn light of these results, we propose the Compressed Bellman Error Based Feature Generation algorithm (CBEBF). To simplify the bias–variance analysis and avoid multiple levels of regression, we present here a simplified version of compressed BEBF-based regression, in that new features are added to the value function approximator with constant weight 1 (i.e. no regression is applied on the generated BEBFs):\nAlgorithm 1: Simplified Compressed BEBFs Input: Sample trajectory Sn = ((xt, rt)nt=1), where xt is the observation received at time t, and rt is the observed reward; Number of BEBFs: m; Projection size schedule: d1, d2, . . . , dm Output: w: the linear coefficient of the value function approximator wD×1 ← 0; for i← 1 to m do\nGenerate random projection ΦD×di according to Eqn 6; Calculate TD-errors: δt = rt + γxTt+1w − xTt w; Let w′di×1 be the ordinary least-squares parameter using ΦTxt as inputs and δt as outputs; Update w← w + Φw′;\nend\nThe optimal number of BEBFs and the schedule of projection sizes need to be determined and are subjects of future discussion. But we show in the next section that logarithmic size projections should be enough to guarantee the reduction of error in value function prediction at each step. This makes the algorithm very attractive when it comes to computational and memory complexity, as the regression at each step is only on a small projected feature space. As we discuss in our empirical analysis, the algorithm is very fast and robust with respect to the selection of parameters.\nOne can view the above algorithm as a model selection procedure that gradually increases the complexity of the hypothesis space by adding more BEBFs to the feature set. This means that the procedure has to be stopped at some point to avoid over-fitting. This is relatively easy to do, as one can use a validation set and compare the estimated values against the empirical returns. The generation of BEBFs should stop when the validation error starts to rise.\nFinite Sample Analysis\nThis section provides a finite sample analysis of the proposed algorithm. Parts of the analysis are not tight and could use further work, but the bound suffices to prove reduction of the error as new BEBFs are added to the feature set.\nThe following theorem shows how well we can estimate the Bellman error by regression to the TDerrors in a compressed space. It highlights the bias–variance trade-off with respect to the choice of the projection size. Theorem 2. Let ΦD×d be a random projection according to Eqn 6. Let Sn = ((xt, rt)nt=1) be a sample trajectory collected on an MDP with a fixed policy with stationary distribution ρ, in a Ddimensional k-sparse feature space. Fix any estimate V of the value function, and the corresponding TD-errors δt’s bounded by ±δmax. Assume that the Bellman error is linear in the features with parameter w. For OLS regression we have w(Φ)ols = (XΦ)\n†δ, where X is the matrix containing xt’s and δ is the vector of TD-errors. Assume that X is of rank larger than d. There exist constants c1...4 depending only on the mixing conditions of the chain, such that for any fixed 0 < ξ1...5 < 1, with probability no less than 1− (ξ1 + ξ2 + ξ3 + ξ4 + ξ5):∥∥∥xTΦw(Φ)ols − eV (x)∥∥∥\nρ(x) ≤ (ξ1)prj mmax ‖w‖\n( 2 +mmax ∥∥(XΦ)†∥∥ 4√c1n log c2 ξ2 ) (8)\n+ δmaxmmax\nn\n∥∥Σ−1Φ ∥∥ ‖XΦ‖ √ 2k log 2D\nξ3 (9)\n+δmaxm 3 max\n√ d3\nn3 ∥∥Σ−1Φ ∥∥2 ‖XΦ‖ √ 2c3 log c4d2 ξ4 log 2d ξ5 (10)\n+Õ(n−2), (11) where (ξ1)prj is according to Lemma 1, mmax = maxz∈X ∥∥zTΦ∥∥ and ΣΦ is the feature covariance matrix under measure ρ.\nDetailed proof is included in the appendix. The sketch of the proof is as follows: Lemma 1 suggests that if the Bellman error is linear in the original features, the bias due to the projection can be bounded within a controlled constant error with logarithmic size projections (first line in the bound). If the Markov chain “forgets” exponentially fast, one can bound the on-measure variance part of the error by a constant error with similar sizes of sampled transitions [20] (second and third line of the bound).\nTheorem 2 can be further simplified by using concentration bounds on random projections as defined in Eqn 6. The norm of Φ can be bounded using the bounds discussed in [21]; we have with probability 1− δΦ:\n‖Φ‖ ≤ √ D/d+ √ (2 log(2/δΦ))/d+ 1 and\n‖Φ†‖ ≤ [√ D/d− √ (2 log(2/δΦ))/d− 1 ]−1 .\nSimilarly, when n > d, and the observed features are well-distributed, we expect that ‖XΦ‖ is of order Õ( √ n/d) and ‖(XΦ)†‖ is of order Õ( √ d/n). Also note that the projections are normpreserving and thus ‖ΦTx‖ = O(1) with high probability for all x. Assuming that n d and that Bellman errors are bounded by emax, we can rewrite the bound on the error up to logarithmic terms as:\nÕ (√ k logD ( emax √ 1\nd + δmax\n√ d\nn\n)) + Õ ( d3\nn\n) . (12)\nThe 1/ √ d term is a part of the bias due to the projection (excess approximation error). The √ d/n term is the variance term that shrinks with larger training sets (estimation error). We clearly observe the trade-off with respect to the compressed dimension d. With the assumptions discussed above, we can see that projection of size d = Õ(k logD) should be enough to guarantee arbitrarily small bias, as long as n = O(d3) holds3.\nThe following two lemmas complete the proof on the shrinkage of the error in the value function prediction:\n3Our crude analysis assumes that n = O(d3). This can be further brought down to O(d2) which is O((k logD)2) by our choice of d.\nLemma 3. Let V π be the value function of a policy π imposing stationary measure ρ, and let eV be the Bellman error under policy π for an estimate V . Given a BEBF ψ satisfying:\n‖ψ(x)− eV (x)‖ρ(x) ≤ ‖eV (x)‖ρ(x) , (13)\nwe have that:\n‖V π(x)− (V (x) + ψ(x))‖ρ(x) ≤ (γ + + γ) ‖V π(x)− V (x)‖ρ(x) . (14)\nTheorem 2 does not state the error in terms of ‖eV (x)‖ρ(x), but rather does it in term of the infinity norm emax. We expect a more careful analysis to give us a bound that could benefit directly from Lemma 3. However, we can still state the following immediate lemma about the contraction in the error:\nLemma 4. Let V π be the value function of a policy π imposing stationary measure ρ, and let eV be the Bellman error under policy π for an estimate V . Given a BEBF ψ satisfying:\n‖ψ(x)− eV (x)‖ρ(x) ≤ c, (15)\nwe have that after adding the BEBF to the estimated value, either the error contracts:\n‖V π(x)− (V (x) + ψ(x))‖ρ(x) < ‖V π(x)− V (x)‖ρ(x) , (16)\nor the error is already small:\n‖V π(x)− V (x)‖ρ(x) ≤ (1 + γ)\n(1− γ)2 c. (17)\nThis means that if we can control the error in BEBFs by some small constant, we can shrink the error up to a factor of that constant."
    }, {
      "heading" : "4 Empirical Analysis",
      "text" : "We evaluate our method on a challenging domain where the goal of the RL agent is to apply direct electrical neurostimulation such as to suppress epileptiform behavior in neural tissues. We use a generative model constructed from real-world data collected on slices of rat brain tissues [22]; the model is available in the RL-Glue framework. Observations are generated over a 5-dimensional real-valued state space. The discrete action choice corresponds to selecting the frequency at which neurostimulation is applied. The model is observed at 5 steps per second. The reward is 0 for steps when a seizure is occurring at the time of stimulation, 1/41 for when seizure happens without stimultion, 40/41 for each stimulation pulse, and 1 otherwise 4.\nOne of the challenges of this domain is that it is difficult to know a priori how to construct a good state representation. We use tile-coding to convert the continuous variables into a high dimensional binary feature space. We encode the policy as a 6th feature, divide each dimension into 6 tiles and use 10 randomly placed tile grids. That creates 10 × 66 = 466, 560 features. Only 10 of these are non-zero at any point, thus k = 10.\nWe apply the best clinical fixed rate policy (stimulation is applied at a consistent 1Hz) to collect our sample set [22]. Since the true value function is not known for this domain, we thus define our error in terms of Monte Carlo returns on a separate test set. Give an test set of size l, Monte Carlo returns are defined to be the discounted sum of rewards observed at each point, denoted by U(xi). Now for any estimated value function V , we define the return prediction error (RP error) to\nbe √\n1 l ∑l i=1 (U(xi)− V (xi)) 2.\nIn our first experiment, we analyze the RP error as a function of the number of generated BEBFs, for different selections of the size of projection d. We run these experiments with two sample sizes: 500 and 1500. The projection sizes are either 10, 20 or 30. Fixing d, we apply many iterations of the algorithm and observe the RP error on a testing set of size l = 5000. To account for the randomness\n4The choice of the reward model is motivated by medical considerations. See [22].\ninduced by the projections, we run these experiments 10 times each, and take the average. Figure 1 includes the results under the described setting.\nIt can seen in both plots in Figure 1, that the RP error decreases to some minimum value after a number of BEBFs are generated, and then the error start increasing slightly when more BEBFs are added to the estimate. The increase is due to over-fitting and can be easily avoided by crossvalidation. As stated before, this work does not include any analysis on the optimal number of iterations. Discussions on the possible methods for such optimization is considered as interesting avenues future work.\nAs expected, the optimal number of BEBFs depend heavily on the size of the projection: the smaller the projection, the more BEBFs need to be added. It is interesting to note that even though the minimum happens at different places, the value of the minimum RP error is not varying much as a function of the projection size. The difference gets even smaller with larger sample sizes. This means that the method is relatively robust with respect to the choice of d. We also observed small variance in the value of the optimal RP error, further confirming the robustness of the algorithm on this domain.\nThere are only a few methods that can be compared against our algorithm due to the high dimensional feature space. Direct regression on the original space with LSTD type algorithms (regularized or otherwise) is impossible due to the computational complexity. We expect most feature selection methods to perform poorly here, since all the features are of small and equal importance (note the different type of sparsity we assume in our work). The two main alternatives are randomized feature extraction (e.g. LSTD with random projections [8]) and online stochastic gradient type methods (e.g. GQ (λ) algorithm [13]).\nLSTD with random projections (Compressed LSTD, CLSTD), discussed in [8], is a simple algorithm in which one applies random projections to reduce the dimension of the state space to a manageable size, and then applies LSTD on the compressed space. We compare the RP error of CLSTD against our method. Among the gradient type methods, we chose the GQ (λ) algorithm [13], as it was expected to provide good consistency. However, since the algorithm was very sensitive to the choice of the learning rate schedule, the initial guess of the weight vector and the λ parameter, we failed to tune it to outperform even the CLSTD. The results on the GQ (λ) algorithm are thus excluded from this section and should be addressed in future works 5.\nFor a fair comparison between CBEBF and CLSTD, we assumed the existence of an oracle that would choose the optimal parameters for these method 6. Therefore, we compare the best RP error on the testing set as we vary the parameters in question. Figure 2 shows the best RP error of the algorithms. For CLSTD, the best RP error is chosen among the solutions with varying projection sizes (extensive search). For CBEBF, we fix the projection size to be 20, and vary the number of generated BEBFs (iteratively) to find the optimal number of iterations that minimizes the RP error.\n5A fair comparison cannot be made with gradient type methods in the absence of a good learning rate schedule. Typical choices were not enough to provide decent results.\n6Note that since there are one or two parameters for these methods, cross-validation should be enough to choose the optimal parameter, though for simplicity the discussion of that is left out of this work.\nAs seen in Figure 2, our method consistently outperforms CLSTD with a large margin, and the results are more robust with smaller variance. Comparing with the results presented in Figure 1, even the over-fitted solutions of CBEBF seem to outperform the best results of CLSTD.\nEach run of our algorithm with hundreds of BEBFs takes one or two minutes when working with thousands of samples and half a million features. The algorithm can easily scale to run with larger sample sizes and higher dimensional spaces, though a comparison cannot be made with CLSTD, since CLSTD (with optimal sizes of projection) fails to scale with increasing number of samples and dimensions."
    }, {
      "heading" : "5 Discussion",
      "text" : "In this work, we provide a simple, fast and robust feature extraction algorithm for policy evaluation in sparse and high dimensional state spaces. Using recent results on the properties of random projections, we prove that in sparse spaces, random projections of sizes logarithmic in the original dimension are enough to preserve the linearity. Therefore, BEBFs can be generated on compressed spaces induced by small random projections. Our finite sample analysis provides guarantees on the reduction of error after the addition of the discussed BEBFs.\nEmpirical analysis on a high dimensional space with unknown value function structure shows that CBEBF vastly outperforms LSTD with random projections and easily scales to larger problems. It is also more consistent in the output and has a much smaller memory complexity. We expect this behaviour to happen under most common state spaces. However, more empirical analysis should be done to confirm such hypothesis. Since the focus of this work is on feature extraction with minimal domain knowledge using agnostic random projections, we avoided the commonly used problem domains with known structures in the value function (e.g. mountain car [10]).\nCompared to other regularization approaches to RL [2, 23, 24], our random projection method does not require complex optimization, and thus is faster and more scalable.\nOf course finding the optimal choice of the projection size and the number of iterations is an interesting subject of future research. We expect the use of cross-validation to suffice for the selection of the optimal parameters due to the robustness in the choice of values. A tighter theoretical bound might also help provide an analytical closed form answer to these questions.\nOur assumption of the linearity of the Bellman error in the original space might be too strong for some state spaces. We avoided non-linearity in the original space to simplify the analysis. However, most of the discussions can be rephrased in terms of the projected Bellman error to provide more general results (e.g. see [6]).\nAppendix\nWe start with concentration bound on the rapidly mixing Markov processes. These will be used to bound the variance of approximations build upon the observed values."
    }, {
      "heading" : "6 Concentration Bounds for Mixing Chains",
      "text" : "We give an extension of Bernstein’s inequality based on [20].\nLet x1, . . . ,xn be a time-homogeneous Markov chain with transition kernel T (·|·) taking values in some measurable spaceX . We shall consider the concentration of the average of the Hidden-Markov Process\n(x1, f(x1)), . . . , (xn, f(xn)),\nwhere f : X → [0, b] is a fixed measurable function. To arrive at such an inequality, we need a characterization of how fast (xi) forgets its past.\nFor i > 0, let T i(·|x) be the i-step transition probability kernel: T i(A|x) = Pr{xi+1 ∈ A |x1 = x} (for all A ⊂ X measurable). Define the upper-triangular matrix Γn = (γij) ∈n×n as follows:\nγ2ij = sup (x,y)∈X 2 ‖T j−i(·|x)− T j−i(·|y)‖TV. (18)\nfor 1 ≤ i < j ≤ n and let γii = 1 (1 ≤ i ≤ n). Matrix Γn, and its operator norm ‖Γn‖ w.r.t. the Euclidean distance, are the measures of dependence for the random sequence x1,x2, . . . ,xn. For example if the xi’s are independent, Γn = I and ‖Γn‖ = 1. In general ‖Γn‖, which appears in the forthcoming concentration inequalities for dependent sequences, can grow with n. Since the concentration bounds are homogeneous in n/ ‖Γn‖2, a larger value ‖Γn‖2 means a smaller “effective” sample size. We say that a time-homogeneous Markov chain uniformly quickly forgets its past if\nτ = sup n≥1 ‖Γn‖2 < +∞. (19)\nFurther, τ is called the forgetting time of the chain. Conditions under which a Markov chain uniformly quickly forgets its past are of major interest. For further discussion on this, see [14].\nThe following result from [14] is a trivial corollary of Theorem 2 of Samson [20] (Theorem 2 is stated for empirical processes and can be considered as a generalization of Talagrand’s inequality to dependent random variables):\nTheorem 5. Let f be a measurable function on X whose values lie in [0, b], (xi)1≤i≤n be a homogeneous Markov chain taking values in X and let Γn be the matrix with elements defined by (18). Let\nz = 1\nn n∑ i=1 f(xi).\nThen, for every ≥ 0,\nP (z − E [z] ≥ ) ≤ exp ( −\n2 n\n2b ‖Γn‖2 (E [z] + )\n) ,\nP (E [z]− z ≥ ) ≤ exp ( −\n2 n\n2b ‖Γn‖2 E [z]\n) .\nThe following is an immediate application of the above theorem:\nLemma 6. Let f be a measurable function over X whose values lie in [0, b]. Let f̂ be the empirical average of f over the sample collected on the Markov chain. Under proper mixing conditions for the sample, there exists constants c1 > 0, c2 ≥ 1 which depend only on T such that for any 0 < ξ < 1, w.p. 1− ξ: ∣∣∣E [f ]− f̂ ∣∣∣ ≤ b√c1\nn log ( c2 ξ ) . (20)"
    }, {
      "heading" : "7 Proof of The Theorem 2",
      "text" : "Proof of Theorem 2. To begin the proof of the main theorem, first note that we can write the TDerrors as the sum of Bellman errors and some noise term: δt = eV (xt)+ηt. These noise terms form a series of martingale differences, as their expectation is 0 given all the history up to that point:\nE [ηt|x1 . . .xt, r1 . . . rt−1] = 0. (21)\nWe also have that the Bellman error is linear in the features, thus in vector form:\nδ = Xw + η. (22)\nUsing random projections, in the compressed space we have:\nδ = (XΦ)(ΦTw) + b + η, (23)\nwhere b is the vector of bias due to the projection. Let bmax = (ξ1) prj mmax‖w‖. We have from Lemma 1 that with probability 1− ξ1, for all x ∈ X :∣∣(xTΦ)(ΦTw)− eV (x)∣∣ ≤ bmax. (24) Thus, b is element-wise bounded in absolute value by bmax with high probability. The weighted L2 error in regression to the TD-error as compared to the Bellman error will thus be:∥∥∥xTΦw(Φ)ols − eV (x)∥∥∥ ρ(x) = ∥∥(xTΦ)(XΦ)†[(XΦ)(ΦTw) + b + η]− eV (x)∥∥ρ(x)\n= ∥∥(xTΦ)(ΦTw) + (xTΦ)(XΦ)†b + (xTΦ)(XΦ)†η − eV (x)∥∥ρ(x)\n≤ ∥∥(xTΦ)(ΦTw)− eV (x)∥∥ρ(x) + ∥∥(xTΦ)(XΦ)†b∥∥ ρ(x) + ∥∥(xTΦ)(XΦ)†η∥∥ ρ(x)\n≤ bmax + ∥∥(xTΦ)(XΦ)†b∥∥ ρ(x) + ∥∥(xTΦ)(XΦ)†η∥∥ ρ(x) . (25)\nThe second term is the regression to the bias, and the third term is the regression to the noise. We present lemmas that bound these terms. The theorem is proved by the application of Lemma 7 and Lemma 10."
    }, {
      "heading" : "7.1 Bounding the Regression to Bias Terms",
      "text" : "Lemma 7. Under the conditions and with probability defined in Theorem 2:∥∥(xTΦ)(XΦ)†b∥∥ ρ(x) ≤ (ξ1)prj mmax ‖w‖ ( 2 +mmax ∥∥(XΦ)†∥∥ 4√c1n log c2 ξ2 ) . (26)\nProof. Let g(x) = (xTΦ)(XΦ)†b, and let b(x) be the bias term induced by the projection at point x. We have that |b(x)| ≤ bmax and |g(x)| ≤ gmax, where gmax = mmax ∥∥(XΦ)†∥∥ bmax√n. Let g be the vector containing the g(xt) terms.\nGiven that g is the OLS regression on the observed points, the sum of squared errors for g should not be greater than any other linear regression, including the vector 0. Thus we have:\n‖g − b‖2 ≤ ‖b‖2 ≤ nb2max ⇒ ‖g − b‖ ≤ √ nbmax (27)\n⇒ ‖g − b‖1 ≤ √ n‖g − b‖ ≤ nbmax (28)\n⇒ ‖g‖1 ≤ 2nbmax. (29)\nLine 28 uses the L1 and L2 norm inequality. Again using the OLS error properties we have:\n‖b‖2 ≥ ‖g − b‖2 ≥ ‖g‖2 + ‖b‖2 − 2‖g‖1bmax. (30)\nUsing the bound of Line 29 we get:\n‖g‖2 ≤ 2‖g‖1bmax ⇒ ‖g‖2 ≤ 4nb2max (31)\n⇒ f̂ = 1 n n∑ t=1 g2(xt) ≤ 4b2max. (32)\nLet f(x) = g2(x). f is bounded by fmax = g2max. Applying Lemma 6, we get: ‖g‖2ρ(x) = E [f ] ≤ f̂ + fmax √ c1 n log c2 ξ2 ≤ 4b2max + g2max √ c1 n log c2 ξ2 . (33)\nApplying and inequality with the squared-root we get: ‖g‖ρ(x) ≤ 2bmax + gmax 4 √ c1 n log c2 ξ2 , (34)\nwhich proves the lemma after substitutions."
    }, {
      "heading" : "7.2 Bounding the Regression to Noise Terms",
      "text" : "To bound the regression to the noise, we need the following lemma on martingales:\nLemma 8. Let M be a matrix of size l × n, in which column t is a function of xt. Then with probability 1− ξ we have:\n‖Mη‖ ≤ δmax‖M‖F √ 2 log 2l\nξ . (35)\nProof. The inner product between each row of M and η can be bounded by a concentration inequality on martingales each failing with probability less than ξ/l. The lemma follows immediately by adding up the inner products.\nThe following lemma based on mixing conditions is also needed to bound the variance term.\nLemma 9. With the conditions of the theorem, with probability 1− ξ4, there exists a Yd×d with all the elements in [−1, 1], and thus ‖Y‖ ≤ d, such that:\n1 n (XΦ)TXΦ = ΣΦ + 0Y, (36)\nwhere 0 = m2max √ c3 n log c4d2 ξ4 .\nStated otherwise, if Y = 1 0 ( 1 n (XΦ) TXΦ−ΣΦ ) , then with probability 1− ξ4, Y is element-wise bounded by ±1.\nProof. This is a simple application of Lemma 6 to all the elements in 1n (XΦ) TXΦ using union bound, as the expectation is Σ, the chain is mixing and each element of XΦ is bounded by mmax.\nWith the above theorem, we can use the Taylor expansion of matrix inversion to have:\n((XΦ)TXΦ)−1 = 1\nn (ΣΦ + 0Y)\n−1 = 1\nn (Σ−1Φ − 0Σ −1 Φ YΣ −1 Φ +O( 2 0)). (37)\nLemma 10. Under the conditions and probabiliy defined in Theorem 2:∥∥(xTΦ)(XΦ)†η∥∥ ρ(x) ≤ δmaxmmax n ∥∥Σ−1Φ ∥∥ ‖XΦ‖ √ 2k log 2D ξ3 (38)\n+δmaxmmax\n√ d3\nn3 ∥∥Σ−1Φ ∥∥2 ‖XΦ‖ √ 2c3 log c4d2 ξ4 log 2d ξ5 (39)\n+Õ(n−2). (40)\nProof. Since X is of rank bigger than d, we have d < n, and with the use of random projections XΦ is full rank with probability 1 (see e.g. [18]). We can thus substitute the inverse by [(XΦ)TXΦ]−1(XΦ)T . Using Lemma 9, we get with probability 1− ξ4 for all x ∈ X :∥∥(xTΦ)(XΦ)†η∥∥ ρ(x) = ∥∥(xTΦ)((XΦ)TXΦ)−1(XΦ)T η∥∥ ρ(x) (41)\n= ∥∥∥∥(xTΦ) [ 1n (Σ−1Φ − 0Σ−1Φ YΣ−1Φ +O( 20)) ] (XΦ)T η ∥∥∥∥ ρ(x)\n(42)\n≤ ∥∥∥∥ 1n (xTΦ)Σ−1Φ (XΦ)T η ∥∥∥∥ ρ(x) + ∥∥∥ 0 n (xTΦ)Σ−1Φ YΣ −1 Φ (XΦ) T η ∥∥∥ ρ(x) +O ( 20 n ) . (43)\nTo bound the first term, let ei be the ith column of Ψ (see definition of X in the notation section). Thus {ei}1≤i≤D is an orthonormal basis under which x ∈ X is sparse, and all ei’s are in X . Applying Lemma 8, D times, we get that for all ei, with probability 1− ξ3/D:∣∣∣∣ 1n (eTi Φ)Σ−1Φ (XΦ)T η ∣∣∣∣ ≤ δmax ∥∥∥∥ 1n (eTi Φ)Σ−1Φ (XΦ)T ∥∥∥∥ √ 2 log 2D ξ3 (44)\n≤ δmaxmmax n\n∥∥Σ−1Φ ∥∥ ‖XΦ‖ √ 2 log 2D\nξ3 . (45)\nThe union bound gives us that Line 45 hold simultaneously for all ei’s with probability 1 − ξ3. Therefore with probability 1− ξ3 for any x = ∑ i αiei:(\n1 n (xTΦ)Σ−1Φ (XΦ) T η\n)2 = ( 1\nn D∑ i=1 (αie T i Φ)Σ −1 Φ (XΦ) T η\n)2 (46)\n≤\n( 1\nn D∑ i=1 |αi| ∣∣(eTi Φ)Σ−1Φ (XΦ)T η∣∣\n)2 (47)\n≤\n( δmaxmmax\nn\n∥∥Σ−1Φ ∥∥ ‖XΦ‖ √ 2 log 2D\nξ3 )2( D∑ i=1 |αi| )2 .(48)\nBecause x is k-sparse, we have that ∑D i=1 |αi| ≤ √ k‖x‖ ≤ √ k. As the above holds for all x = X , it holds for the expectation under ρ. We thus get:∥∥∥∥ 1n (xTΦ)Σ−1Φ (XΦ)T η ∥∥∥∥ ρ(x) ≤ δmaxmmax n ∥∥Σ−1Φ ∥∥ ‖XΦ‖ √ 2k log 2D ξ3 . (49) For the second term of Line 43, we first split and then apply Lemma 8.∣∣∣ 0 n (xTΦ)Σ−1Φ YΣ −1 Φ (XΦ) T η ∣∣∣ ≤ 0 n\n∥∥ΦTx∥∥∥∥Σ−1Φ ∥∥2 ‖Y‖ ∥∥(XΦ)T η∥∥ . (50) Using Lemma 9, we have with probability 1 − ξ4 that ‖Y‖ ≤ d. Applying Lemma 8 to the∥∥(XΦ)T η∥∥ term we get with probability 1− ξ5 for all x ∈ X :∣∣∣ 0\nn (xTΦ)Σ−1Φ YΣ −1 Φ (XΦ)\nT η ∣∣∣ ≤ 0\nn\n∥∥ΦTx∥∥∥∥Σ−1Φ ∥∥2 dδmax ‖XΦ‖F √ 2 log 2d\nξ5 (51)\n≤ 0 n mmax\n∥∥Σ−1Φ ∥∥2 dδmax√d ‖XΦ‖ √ 2 log 2d\nξ5 . (52)\nAs the above holds for all x ∈ X , it holds for any expectation on with measures defined on X :∥∥∥ 0 n (xTΦ)Σ−1Φ YΣ −1 Φ (XΦ) T η ∥∥∥ ρ(x) ≤ 0δmaxmmax √ d3 n ∥∥Σ−1Φ ∥∥2 ‖XΦ‖ √ 2 log 2d ξ5 . (53)\nSubstituting 0 of Lemma 9, and using Lines 49 and 53 into 43 will finish the proof."
    }, {
      "heading" : "8 Proof of Error Contraction Lemmas",
      "text" : "This section will finish the proof of the lemmas presented in the paper."
    }, {
      "heading" : "8.1 Proof of Lemma 3",
      "text" : "Proof of Lemma 3. We have that V π is the fixed point to the Bellman operator (i.e. T V π = V π), and that the operator is a contraction with respect to the weighted L2 norm on the stationary distribution ρ [25]:\n‖T V (x)− T V ′(x)‖ρ(x) ≤ ‖V (x)− V ′(x)‖ρ(x) . (54)\nWe thus have:\n‖V π(x)− (V (x) + ψ(x))‖ρ(x) (55) ≤ ‖V π(x)− T V (x)‖ρ(x) + ‖(T V (x)− V (x))− ψ(x)‖ρ(x) (56) ≤ ‖T V π(x)− T V (x)‖ρ(x) + ‖T V (x)− V (x)‖ρ(x) (57)\n≤ γ ‖V π(x)− V (x)‖ρ(x) + ( ‖T V (x)− T V π(x)‖ρ(x) + ‖V π(x)− V (x)‖ρ(x) ) (58)\n≤ (γ + γ + ) ‖V π(x)− V (x)‖ρ(x) . (59)"
    }, {
      "heading" : "8.2 Proof of Lemma 4",
      "text" : "Proof of Lemma 4. We have that:\n‖V π(x)− V (x)‖ρ(x) ≤ ‖T V π(x)− T V (x)‖ρ(x) + ‖T V (x)− V (x)‖ρ(x) (60)\n≤ γ ‖V π(x)− V (x)‖ρ(x) + ‖T V (x)− V (x)‖ρ(x) , (61)\nand thus:\n‖V π(x)− V (x)‖ρ(x) ≤ 1\n1− γ ‖T V (x)− V (x)‖ρ(x) . (62)\nLet = c/ ‖eV (x)‖ρ(x). If the contraction does not happen, then due to Lemma 3, we must have:\nγ + γ + ≥ 1 ⇒ > 1− γ 1 + γ\n(63)\n⇒ ‖T V (x)− V (x)‖ρ(x) ≤ 1 + γ\n1− γ c (64)\n⇒ ‖V π(x)− V (x)‖ρ(x) ≤ 1 + γ\n(1− γ)2 c. (65)"
    } ],
    "references" : [ {
      "title" : "Adaptive bases for reinforcement learning",
      "author" : [ "D. Di Castro", "S. Mannor" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Regularization and feature selection in least-squares temporal difference learning",
      "author" : [ "J.Z. Kolter", "A.Y. Ng" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Automatic basis function construction for approximate dynamic programming and reinforcement learning",
      "author" : [ "P.W. Keller", "S. Mannor", "D. Precup" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Extraction of reward-related feature space using correlation-based and reward-based learning methods",
      "author" : [ "P. Manoonpong", "F. Wörgötter", "J. Morimoto" ],
      "venue" : "Neural Information Processing. Theory and Algorithms,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Online discovery of feature dependencies",
      "author" : [ "A. Geramifard", "F. Doshi", "J. Redding", "N. Roy", "J.P. How" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Analyzing feature generation for value-function approximation",
      "author" : [ "R. Parr", "C. Painter-Wakefield", "L. Li", "M. Littman" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Generalization in reinforcement learning: Safely approximating the value function",
      "author" : [ "J. Boyan", "A.W. Moore" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1995
    }, {
      "title" : "LSTD with random projections",
      "author" : [ "M. Ghavamzadeh", "A. Lazaric", "O.A. Maillard", "R. Munos" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Learning sparse image codes using a wavelet pyramid architecture",
      "author" : [ "B.A. Olshausen", "P. Sallee", "M.S. Lewicki" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2001
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1998
    }, {
      "title" : "Technical update: Least-squares temporal difference learning",
      "author" : [ "J.A. Boyan" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2002
    }, {
      "title" : "Least-squares policy iteration",
      "author" : [ "M.G. Lagoudakis", "R. Parr" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2003
    }, {
      "title" : "λ): A general gradient algorithm for temporal-difference prediction learning with eligibility traces",
      "author" : [ "H.R. Maei", "R.S. Sutton. GQ" ],
      "venue" : "In Third Conference on Artificial General Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Model selection in reinforcement learning",
      "author" : [ "A.M. Farahmand", "C. Szepesvári" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Basis function adaptation in temporal difference reinforcement learning",
      "author" : [ "I. Menache", "S. Mannor", "N. Shimkin" ],
      "venue" : "Annals of Operations Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2005
    }, {
      "title" : "Detection and estimation with compressive measurements",
      "author" : [ "M.A. Davenport", "M.B. Wakin", "R.G. Baraniuk" ],
      "venue" : "Dept. of ECE, Rice University, Tech. Rep,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2006
    }, {
      "title" : "An introduction to compressive sampling",
      "author" : [ "E.J. Candès", "M.B. Wakin" ],
      "venue" : "Signal Processing Magazine, IEEE,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Compressed least-squares regression on sparse spaces",
      "author" : [ "M.M. Fard", "Y. Grinberg", "J. Pineau", "D. Precup" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Random projections preserve linearity in sparse spaces",
      "author" : [ "M.M. Fard", "Y. Grinberg", "J. Pineau", "D. Precup" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Concentration of measure inequalities for Markov chains and φ-mixing processes",
      "author" : [ "P.M. Samson" ],
      "venue" : "Annals of Probability,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2000
    }, {
      "title" : "Near-optimal signal recovery from random projections: Universal encoding strategies",
      "author" : [ "E.J. Candès", "T. Tao" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    }, {
      "title" : "Dynamic Representations for Adaptive Neurostimulation Treatment of Epilepsy",
      "author" : [ "K. Bush", "J. Pineau", "A. Guez", "B. Vincent", "G. Panuccio", "M. Avoli" ],
      "venue" : "4th International Workshop on Seizure Prediction,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2009
    }, {
      "title" : "Regularized policy iteration",
      "author" : [ "A.M. Farahmand", "M. Ghavamzadeh", "C. Szepesvári" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Linear complementarity for regularized policy evaluation and improvement",
      "author" : [ "J. Johns", "C. Painter-Wakefield", "R. Parr" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "Learning and value function approximation in complex decision processes",
      "author" : [ "B. Van Roy" ],
      "venue" : "PhD thesis, Massachusetts Institute of Technology,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[1, 2, 3, 4, 5]).",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 1,
      "context" : "[1, 2, 3, 4, 5]).",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 2,
      "context" : "[1, 2, 3, 4, 5]).",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 3,
      "context" : "[1, 2, 3, 4, 5]).",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "[1, 2, 3, 4, 5]).",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 5,
      "context" : "Successive addition of exact BEBFs has been shown to reduce the error of a linear value estimator at a rate similar to value iteration [6].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 6,
      "context" : "Unlike fitted value iteration [7] which works with a fixed feature set, iterative BEBF generation gradually increases the complexity of the hypothesis space by adding new features and thus does not diverge, as long as the error in the generation does not cancel out the contraction effect of the Bellman operator [6].",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "Unlike fitted value iteration [7] which works with a fixed feature set, iterative BEBF generation gradually increases the complexity of the hypothesis space by adding new features and thus does not diverge, as long as the error in the generation does not cancel out the contraction effect of the Bellman operator [6].",
      "startOffset" : 313,
      "endOffset" : 316
    }, {
      "referenceID" : 4,
      "context" : "A number of methods have been introduced in RL to generate features related to the Bellman error, with a fair amount of success [5, 1, 4, 6, 3], but many of them fail to scale to high dimensional state spaces.",
      "startOffset" : 128,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "A number of methods have been introduced in RL to generate features related to the Bellman error, with a fair amount of success [5, 1, 4, 6, 3], but many of them fail to scale to high dimensional state spaces.",
      "startOffset" : 128,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "A number of methods have been introduced in RL to generate features related to the Bellman error, with a fair amount of success [5, 1, 4, 6, 3], but many of them fail to scale to high dimensional state spaces.",
      "startOffset" : 128,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "A number of methods have been introduced in RL to generate features related to the Bellman error, with a fair amount of success [5, 1, 4, 6, 3], but many of them fail to scale to high dimensional state spaces.",
      "startOffset" : 128,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "A number of methods have been introduced in RL to generate features related to the Bellman error, with a fair amount of success [5, 1, 4, 6, 3], but many of them fail to scale to high dimensional state spaces.",
      "startOffset" : 128,
      "endOffset" : 143
    }, {
      "referenceID" : 7,
      "context" : "Our results indicate that the proposed method outperforms both gradient type methods, and also LSTD with random projections [8].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 8,
      "context" : "image, audio and video signals [9]) and also from most discretization-based methods (e.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : "Among these there are dynamic programming methods in which one iteratively applies the Bellman operator [10] to an initial guess of the optimal value function.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 10,
      "context" : "Least-squares temporal difference learning (LSTD) and its derivations [11, 12] are among the methods used to learn a value function based on a finite sample.",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "Least-squares temporal difference learning (LSTD) and its derivations [11, 12] are among the methods used to learn a value function based on a finite sample.",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "Using LSTD in spaces induced by random projections is a way of dealing with such domains [8].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 12,
      "context" : "Stochastic gradient descent type method are also used for value function approximation in high dimensional state spaces, some with proofs of convergence in online and offline settings [13].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 13,
      "context" : "[14]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Feature selection/extraction methods have thus been used to build better approximation spaces for the value functions [1, 2, 3, 4, 5].",
      "startOffset" : 118,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "Feature selection/extraction methods have thus been used to build better approximation spaces for the value functions [1, 2, 3, 4, 5].",
      "startOffset" : 118,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : "Feature selection/extraction methods have thus been used to build better approximation spaces for the value functions [1, 2, 3, 4, 5].",
      "startOffset" : 118,
      "endOffset" : 133
    }, {
      "referenceID" : 3,
      "context" : "Feature selection/extraction methods have thus been used to build better approximation spaces for the value functions [1, 2, 3, 4, 5].",
      "startOffset" : 118,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "Feature selection/extraction methods have thus been used to build better approximation spaces for the value functions [1, 2, 3, 4, 5].",
      "startOffset" : 118,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "(5) It is easy to show that the expectation of the temporal difference given a point xt equals the Bellman error on that point [10].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 14,
      "context" : "[15] introduced two algorithms to adapt basis functions as features for linear function approximation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "[3] applied neighbourhood component analysis as a dimensionality reduction technique to construct a low dimensional state space based on the TD-error.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] later showed that any BEBF extraction method with small angular approximation error will provably tighten approximation error in the value function estimate.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] have recently introduced the incremental Feature Dependency Discovery (iFDD) as a fast online algorithm to extract non-linear binary feature for linear function approximation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "[16, 17]).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 16,
      "context" : "[16, 17]).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 16,
      "context" : "This is because random projections are norm and distance-preserving in many classes of feature spaces [17].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "A bound introduced in [18] and later tightened in [19] shows that if a function is linear in a sparse space, it is almost linear in an exponentially smaller projected space.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 18,
      "context" : "A bound introduced in [18] and later tightened in [19] shows that if a function is linear in a sparse space, it is almost linear in an exponentially smaller projected space.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 18,
      "context" : "An immediate lemma based on Theorem 2 of [19] bounds the bias induced by random projections: Lemma 1.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "[6]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "[16]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[6] show that if we add a new BEBF ψm+1 = eVm to the feature set, (with mild assumptions) the approximation error on the new linear space shrinks by a factor of γ.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 19,
      "context" : "If the Markov chain “forgets” exponentially fast, one can bound the on-measure variance part of the error by a constant error with similar sizes of sampled transitions [20] (second and third line of the bound).",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 20,
      "context" : "The norm of Φ can be bounded using the bounds discussed in [21]; we have with probability 1− δΦ: ‖Φ‖ ≤ √ D/d+ √ (2 log(2/δΦ))/d+ 1 and ‖Φ†‖ ≤ [√ D/d− √ (2 log(2/δΦ))/d− 1 ]−1 .",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 21,
      "context" : "We use a generative model constructed from real-world data collected on slices of rat brain tissues [22]; the model is available in the RL-Glue framework.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 21,
      "context" : "We apply the best clinical fixed rate policy (stimulation is applied at a consistent 1Hz) to collect our sample set [22].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 21,
      "context" : "See [22].",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 7,
      "context" : "LSTD with random projections [8]) and online stochastic gradient type methods (e.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : "GQ (λ) algorithm [13]).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 7,
      "context" : "LSTD with random projections (Compressed LSTD, CLSTD), discussed in [8], is a simple algorithm in which one applies random projections to reduce the dimension of the state space to a manageable size, and then applies LSTD on the compressed space.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "Among the gradient type methods, we chose the GQ (λ) algorithm [13], as it was expected to provide good consistency.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "mountain car [10]).",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 1,
      "context" : "Compared to other regularization approaches to RL [2, 23, 24], our random projection method does not require complex optimization, and thus is faster and more scalable.",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 22,
      "context" : "Compared to other regularization approaches to RL [2, 23, 24], our random projection method does not require complex optimization, and thus is faster and more scalable.",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 23,
      "context" : "Compared to other regularization approaches to RL [2, 23, 24], our random projection method does not require complex optimization, and thus is faster and more scalable.",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "see [6]).",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 19,
      "context" : "We give an extension of Bernstein’s inequality based on [20].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "For further discussion on this, see [14].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "The following result from [14] is a trivial corollary of Theorem 2 of Samson [20] (Theorem 2 is stated for empirical processes and can be considered as a generalization of Talagrand’s inequality to dependent random variables): Theorem 5.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 19,
      "context" : "The following result from [14] is a trivial corollary of Theorem 2 of Samson [20] (Theorem 2 is stated for empirical processes and can be considered as a generalization of Talagrand’s inequality to dependent random variables): Theorem 5.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "[18]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "T V π = V ), and that the operator is a contraction with respect to the weighted L norm on the stationary distribution ρ [25]: ‖T V (x)− T V (x)‖ρ(x) ≤ ‖V (x)− V (x)‖ρ(x) .",
      "startOffset" : 121,
      "endOffset" : 125
    } ],
    "year" : 2017,
    "abstractText" : "We address the problem of automatic generation of features for value function approximation. Bellman Error Basis Functions (BEBFs) have been shown to improve the error of policy evaluation with function approximation, with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections to generate BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space are enough to guarantee contraction in the error. Empirical results demonstrate the strength of this method.",
    "creator" : "LaTeX with hyperref package"
  }
}