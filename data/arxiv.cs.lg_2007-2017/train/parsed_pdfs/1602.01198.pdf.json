{
  "name" : "1602.01198.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "k-variates++: more pluses in the k-means++",
    "authors" : [ "Richard Nock", "Roksana Boreli" ],
    "emails" : [ "richard.nock@nicta.com.au", "raphael.canyasse@polytechnique.edu", "roksana.boreli@nicta.com.au", "Frank.Nielsen@acm.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 2.\n01 19"
    }, {
      "heading" : "1 Introduction",
      "text" : "Arthur-Vassilvitskii’s (AV) k-means++ algorithm has been extensively used to address the hard membership clustering problem, due to its simplicity, experimental performance and guaranteed approximation of the global optimum; the goal being the k-partitioning of a dataset so as to minimize the sum of within-cluster squared distances to the cluster center (Arthur & Vassilvitskii, 2007), i.e., a centroid or a population minimizer (Nock et al., 2016).\nThe k-means++ non-uniform seeding approach has also been utilized in more complex settings, including tensor clustering, distributed, data stream, on-line and parallel clustering, clustering with non-metric distortions and even clustering with distortions not allowing population minimizers in closed form (Ailon et al., 2009; Balcan et al., 2013; Jegelka et al., 2009; Liberty et al., 2014; Nock et al., 2008; Nielsen & Nock, 2015). However, apart from the non-uniform seeding, all these algorithms are distinct and (seemingly) do not share many common properties.\nFinally, the application of k-means++ in some scenarios is still an open research topic, due to the related constraints – e.g., there is limited prior work in a differentially private setting (Nissim et al., 2007; Wang et al., 2015). Our contribution — In a nutshell, we describe a generalisation of the k-means++ seeding process, k-variates++, which still delivers an efficient approximation of the global optimum, and can be used to obtain and analyze efficient algorithms for a wide range of settings, including: distributed, streamed, on-line clustering, (differentially) private clustering, etc. . We proceed in two steps.\nFirst, we describe k-variates++ and analyze its approximation properties. We leverage two major components of k-means++: (i) data-dependent probes (specialized to observed data in the k-means++) are used to compute the weights for selecting centers, and (ii) selection of centers is based on an arbitrary family of densities (specialized to Diracs in the k-means++). Informally, the approximation properties (when only (ii) is considered), can be shown as:\nexpected cost(k-variates++) ≤ (2 + log k) · Φ , with\nΦ . = 6 · optimal noise-free cost + 2 · noise (bias + variance), where “noise” refers to the family of densities (note that constants are explicit in the bound). The dependence on these densities is arguably smaller than expectable (factor 2 for noise vs 6 for global optimum). There is also not much room for improvement: we show that the guarantee approaches the Fréchet-Cramér-RaoDarmois lowerbound.\nSecond, we use this general algorithm in two ways. We use it directly in a differential privacy setting, addressing a conjecture of (Nissim et al., 2007) with weaker assumptions. We also demonstrate the use of this algorithm for a reduction to other biased seeding algorithms for distributed, streamed or on-line clustering, and obtain the approximation bounds for these algorithms. This simple reduction technique allows us to analyze lightweight algorithms that compare favorably to the state of the art in the related domains (Ailon et al., 2009; Balcan et al., 2013; Liberty et al., 2014), from the approximation, assumptions and / or complexity aspects. Experiments against state of the art for the distributed and differentially private settings display that solid performance improvement can be obtained.\nThe rest of this paper is organised as follows: Section 2 presents k-variates++. Section 3 presents approximation properties for distributed, streamed and on-line clustering that use a reduction from k-variates++. Section 4 presents direct applications of k-variates++ to differential\nprivacy. Section 5 presents experimental results. Last Section discusses extensions (to more distortion measures) and conclude. In order not to laden the paper’s body, an Appendix, starting page 18, provides all proofs, extensive experiments and additional remarks on the paper’s content.\n2 k-variates++ We consider the hard clustering problem (Banerjee et al., 2005; Nock et al., 2016): given set A ⊂ Rd and integer k > 0, find centers C ⊂ Rd which minimizes the L22 potential to the centers (here, c(a) .= arg minc∈C ‖a− c‖22):\nφ(A;C) . = ∑ a∈A ‖a− c(a)‖22 , (2)\nAlgorithm 0 describes k-variates++. um denotes the uniform distribution over A (|A| = m). The parenthood with k-means++ seeding, which we name “k-means++” for short1 (Arthur & Vassilvitskii, 2007) can be best understood using Figure 1 (the red parts in Figure 1 are pinpointed in Algorithm 0). k-means++ is a random process that generates cluster centers from observed data A. It can be modelled using a two-stage generative process for a mixture of Dirac distributions: the first stage involves random variable Qt ∼ Mult(m,πt) whose parameters πt ∈ 4m (the m-dim probability simplex) are computed from the data and previous centers; sampling Qt chooses the Dirac distribution, which is then “sampled” for one center (and the process iterates). All the crux of the technique is the design of πt, which, under no assumption of the data, yield in expectation a k-means potential for the centers chosen that is within 8(2 + log k) of the global optimum (Arthur & Vassilvitskii, 2007).\nk-variates++ generalize the process in two ways: first, the update of πt depends on data and previous probes, using a sequence of probe functions ℘t : A → Rd (℘ = Id,∀t in k-means++). Second, Diracs are replaced by arbitrary but fixed local (sometimes also called noisy) distributions with parameters2 (µa,θa) that depend on A.\nLet Copt ⊂ Rd denote the set of k centers minimizing (2) on A. Let copt(a) .= arg minc∈Copt ‖a− c‖22 (a ∈ A), and\nφopt . = ∑ a∈A ‖a− copt(a)‖22 , (3)\nφbias . = ∑ a∈A ‖µa − copt(a)‖22 , (4)\nφvar . = ∑ a∈A tr (Σa) . (5)\nφopt is the optimal noise-free potential, φbias is the bias of the noise3, and φvar its variance, with Σa . = Ex∼pa [(x−µa)(x−µa)>] the covariance matrix of pa. Notice that when µa = a, φbias = φopt. Otherwise, it may hold that φbias < φopt, and even φbias = 0 if expectations coincide with Copt. Let Copt denote the partition of A according to the centers in Copt. We say that probe function ℘t is η-stretching if, informally, replacing points by their probes does not distort significantly the observed potential of an optimal cluster, with respect to its actual optimal potential. The formal definition follows.\nDefinition 1 Probe functions ℘t are said η-stretching on A, for some η ≥ 0, iff the following holds: for any cluster A ∈ Copt and any a0 ∈ A such that φ(℘t(A); {℘t(a0)}) 6= 0, for any set of at most k centers C ⊂ Rd,\nφ(A;C) φ(A; {a0}) ≤ (1 + η) · φ(℘t(A);C) φ(℘t(A); {℘t(a0)}) ,∀t . (6)\n1Both approaches can be completed with the same further local monotonous optimization steps like Lloyd or Hartigan iterations; furthermore, it is the biased seeding which holds the approximation properties of k-means++.\n2Because expectations are the major parameter for clustering, we split the parameters in the form of µa (expectation) and θa (other parameters, e.g. covariance matrix).\n3We term it bias by analogy with supervised classification, considering that the expectations of the densities could be used as models for the cluster centers (Kohavi & Wolpert, 1996).\nSince φ(A;Copt) = ∑ a0∈A φ(A; {a0}) (Arthur & Vassilvitskii, 2007) (Lemma 3.2), Definition 1 roughly states that the potential of an optimal cluster with respect to a set of cluster centers, relatively to its potential with respect to the optimal set of centers, does not blow up through probe function ℘t. The identity function is trivially 0-stretching, for any A. Many local transformations would be eligible for η-stretching probe functions with η small, including local translations, mappings to core-sets (Har-Peled & Mazumdar, 2004), mappings to Voronoi diagram cell centers (Boissonnat et al., 2010), etc. Notice that ineq. (6) has to hold only for optimal clusters and not any clustering of A. Let E[φ(A;C)] .= ∫ φ(A|C)dp(C) denote the expected potential over the random sampling of C in k-variates++.\nTheorem 2 For any dataset A, any sequence of η-stretching probe functions ℘t and any density {pa,a ∈ A}, the expected potential of k-variates++ satisfies:\nE[φ(A;C)] ≤ (2 + log k) · Φ , (7) with Φ .= (6 + 4η)φopt + 2φbias + 2φvar.\n(Proof in page 19) Five remarks are in order. First, we retrieve the result of (Arthur & Vassilvitskii, 2007) in their setting (η = φvar = 0, φbias = φopt). Second, in the case where φbias < φopt, we may beat AV’s bound. This is not due to an improvement of the algorithm, but to a finer analysis which shows that special settings may “naturally” favor the improvement. We shall see one example in the distributed clustering case. Third, apart from being η-stretching, there is no constraint on the choice of probe functions ℘t: it can be randomized, iteration dependent, etc. Fourth, the algorithm can easily be generalized to the case where points are weighted. Last, as we show in the following Lemma, the dependence in noise in ineq. (7) can hardly be improved in our framework.\nLemma 3 Suppose each point in A is replaced (i.i.d.) by a point sampled in pa with Σa = Σ. Then any clustering algorithm suffers: E[φ(A;C)] = Ω(|A|tr (Σ)). (Proof in page 22) We make use of k-variates++ in two different ways. First, we show that it can be used to prove approximation properties for algorithms operating in different clustering settings: distributed clustering, streamed clustering and on-line clustering. The proof involves a reduction (see page 23) from k-variates++ to each of these algorithms. By reduction, we mean there exists distributions and probe functions (even non poly-time computable) for which k-variates++ yields the same result in expectation as the other algorithm, thus directly yielding an approximability ratio of the global optimum for this latter algorithm via Theorem 2. Second, we show how kvariates++ can directly be specialized to address settings for which no efficient application of k-means++ was known.\n3 Reductions from k-variates++ Despite tremendous advantages, k-means++ has a serious downside: it is difficult to parallelize, distribute or stream it under relevant communication, space, privacy and/or time resource constraints (Bahmani et al., 2012). Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).\nAlgorithm 1 Dk-means++ (// PDk-means++) Input: Forgy nodes (Fi,Ai), i ∈ [n], for t = 1, 2, ..., k Round 1 : N∗ picks i∗ ∼qDt [n] and asks Fi∗ for a center; Round 2 : Fi∗ picks a ∼ui∗ Ai∗ and sends a to Fi,∀i;\n// PDk-means++: Fi∗ sends x ∼ p(µa,θa) to Fi,∀i; Round 3 : ∀i,Fi updates Dt(Ai) and sends it to N∗;\nOutput: C = set of broadcasted as (or xs);\nDistributed clustering We consider horizontally partitioned data among peers, in line with (Bahmani et al., 2012), and a setting significantly more restrictive than theirs: each peer can only locally run the standard operations of Forgy initialisation (that is, uniform random seeding) on its own data, unlike for example the biased distributions of (Bahmani et al., 2012). This is consistent with the notion that data handling peers are not necessarily computationally intensive resources. Additionally, due to privacy constraints, we limit the data sharing between nodes. We denote the nodes handling the data Forgy nodes. We have n such nodes, (Fi,Ai), i ∈ [n], where Ai is the dataset held by Fi. To enable more complex operations necessary to implement k-variates++, we introduce a special node, N∗, that has high computation power, but is not allowed to handle any data (points) from the Forgy nodes. We therefore split the location of the computational power from the location of the data. We also prevent the Forgy nodes from exchanging any data between themselves, with the sole exception of cluster centers. We note that none of the algorithms of (Ailon et al., 2009; Balcan et al., 2013; Bahmani et al., 2012) would be applicable to this setting without non-trivial modifications affecting their properties.\nAlgorithm 1 defines the mechanism that is consistent with our setting. It includes two variants: a protected version Dk-means++ where Forgy nodes directly share local centers and a private\nversion PDk-means++ where the nodes share noisy centers, such as to ensure a differentially private release of centers (with relevant noise calibration). Notations used in Algorithm 1 are as follows. Let Dt(Ai) . = ∑ a∈Ai Dt(a) and q D ti . = Dt(Ai) · ( ∑ j Dt(Aj)) −1 if t > 1 and qDti .\n= 1/n otherwise. Also, ui is uniform distribution on [mi], with mi . = |Ai|.\nTheorem 4 Let φFs .\n= ∑ i∈[n] ∑ a∈Ai ‖c(Ai)−a‖22 be the total spread of the Forgy nodes (c(Ai) . =\n(1/mi) · ∑ Ai a). At iteration k, the expected potential on the total data A .= ∪iAi satisfies ineq. (7) with\nΦ . =\n{ 10φopt + 6φ F s (Dk-means++)\n10φopt + 4φ F s + 2φvar (PDk-means++)\n. (8)\nHere, φopt is the optimal potential on total data A.\n(Proof in page 23) We note that the optimal potential is defined on the total data. The dependence on φFs , which is just the peer-wise variance of data, is thus rather intuitive. A positive point is that φFs is weighted by a factor smaller than the factor that weights the optimal potential. Another positive point is that this parameter can be computed from data, and among peers, without disclosing more data. Hence, it may be possible to estimate the loss against the centralized, k-means++ setting, taking as reference eq. (8). To gain insight in the leverage that Theorem 4 provides, Table 1 compares Dk-means++ to (Balcan et al., 2013)’s (ε is the coreset approximation parameter), even though the latter approach would not be applicable to our restricted framework. To be fair, we assume that the algorithm used to cluster the coreset in (Balcan et al., 2013) is k-means++. We note that, considering the communication complexity and the number of data points shared, Algorithm 1 is a clear winner. In fact, Algorithm 1 can also win from the approximability standpoint. The dependence in ε prevents to fix it too small in (Balcan et al., 2013). Comparing the bounds in row (III) shows that if ε > 1/4, then we can also be better from the approximability standpoint if the spread satisfies φFs = O(φopt). While this may not be feasible over arbitrary data, it becomes more realistic on several real-world scenarii, when Forgy nodes aggregate “local” data with respect to features, e.g., state-wise insurance data, city-wise financial data, etc. When n increases, this also becomes more realistic.\nStreaming clustering We have access to a stream S, with an assumed finite size: S is a sequence of points a1,a2, ...,am. We authorise the computation / output of the clustering at the end of the stream, but the memory n allowed for all operations satisfies n < m, such as n = mα with α < 1 in (Ailon et al., 2009). We assume for simplicity that each point can be stored in one storage memory unit. Algorithm 2 (Sk-means++) presents our approach. It relies on the standard “trick” of summarizing massive datasets via compact representations (synopses) before processing them (Indyk et al., 2014). The approximation properties of Sk-means++, proven using a reduction from k-variates++, hold regardless of the way synopses are built. They show that two key parameters may guide its choice: the spread of the synopses, analogous to the spread of Forgy nodes for distributed clustering, and the stretching properties of the synopses used as centers.\nTheorem 5 Let ℘(a) .= arg mins′∈S ‖a − s′‖22,∀a ∈ S. Let φ℘s . = ∑ a∈S ‖℘(a) − a‖22 be the spread of ℘ on synopses set S. Let η > 0 such that ℘ is η-stretching on S. Then the expected\nAlgorithm 2 Sk-means++ Input: Stream S Step 1: S .= {(sj,mj), i ∈ [n]} ← SYNOPSIS(S, n); Step 2: for t = 1, 2, ..., k\n2.1: if t = 1 then let sj ∼un S else sj ∼qSt S s.t.\nqSt (sj) . = mjDt(sj) ∑ j′∈[n] mj′Dt(sj′) −1 ; (9) // Dt(sj) . = minc∈C ‖sj − c‖22;\n2.2: C← C ∪ {sj}; Output: Cluster centers C;\nAlgorithm 3 OLk-means++ Input: Minibatch Sj , current weighted centers C; Step 1: if j = 1 then let s ∼u1 S1 else s ∼qOj Sj s.t.\nqOj (s) . = Dt(s) ∑ s′∈Sj Dt(s ′) −1 ; (10) // Dt(s) . = minc∈C ‖s− c‖22;\nStep 2: C← C ∪ {s};\npotential of Sk-means++ on stream S satisfies ineq. (7) with\nΦ . = (8 + 4η)φopt + 2φ ℘ s ,\nHere, φopt is the optimal potential on stream S.\n(Proof in page 25) It is not surprising to see that Sk-means++ looks like a generalization of (Ailon et al., 2009) and almost matches it (up to the number of centers delivered) when k′ k synopses are learned from k′-means#. Yet, we rely on a different — and more general — analysis of its approximation properties. Table 1 compares properties of Sk-means++ to (Ailon et al., 2009) (η relates to approximation of the k-means objective in inner loop).\nOn-line clustering This setting is probably the farthest from the original setting of the k-means++ algorithm. Here, points arrive in a sequence, finite, but of unknown size and too large to fit in memory (Liberty et al., 2014). We make no other assumptions – the sequence can be random, or chosen by an adversary. Therefore, the expected analysis we make is only with respect to the internal randomisation of the algorithm, i.e., for the fixed stream sequence as it is observed. We do not assume a feedback for learning (common for supervised learning); so, we do not assume that the algorithm has to predict a cluster for each point that arrives, yet it has to be easily modifiable to do so.\nOur approach is summarized in Algorithm 3 (OLk-means++), a variation of k-means++ which consists of splitting the stream S into minibatches Sj for j = 1, 2, ..., each of which is used to sample one center. u1 denotes the uniform distribution with support S1. Let R . = maxa,a′∈S ‖a − a′‖2( ∞) be the diameter of S.\nTheorem 6 Let ς > 0 be the largest real such that the following conditions are met (for any A ∈ Copt, j ≥ 1): for any set of at most k centers C, ∑ a,a′∈A ‖a − a′‖22 ≥ ς · (|A| 2 ) R2 and∑\na∈A∩Sj ‖a− c(a)‖22 ≥ ς · ∑ a∈A ‖a− c(a)‖22 (with c(a) defined in eq. (2)). Then the expected\npotential of OLk-means++on stream S satisfies ineq. (7) with\nΦ . = ( 4 + 32\nς2\n) · φopt ,\nwhere φopt is the optimal potential on stream S.\n(Proof in page 26) Notice that loss function φ(S,C) in eq. (2) implies the finiteness of S, and the existence of ς > 0; also, the second condition implies ς ≤ 1. In (Liberty et al., 2014), the clustering algorithm is required to have space and time at most polylog in the length of the stream. Hence, each minibatch can be reasonably large with respect to the stream — the larger they are, the larger ς . The knowledge of ς is not necessary to run OLk-means++; it is just a part of the approximation bound which quantifies the loss in approximation due to the fact that centers are computed from the partial knowledge of the stream. Table 1 compares properties of OLk-means++ to (Liberty et al., 2014) (we picked the fully on-line, non-heuristic algorithm). To compare the bounds, suppose that batches have the same size, b, so that log k = log(m/b). If batches are at least polylog size, up to what is hidden in the big-Oh notation, our approximation can be quite competitive when ς is large, e.g., if d is large and optimal clusters are not too small.\n4 Direct use of k-variates++ The most direct application domain of k-variates++ is differential privacy. Several algorithms have independently emphasised the idea that powerful mechanisms may be amended via a carefully designed noise mechanism to broaden their scope with new capabilities, without overly challenging their original properties. Examples abound (Hardt & Price, 2014; Kalai & Vempala, 2005; Chaudhuri et al., 2011; Chichignoud & Lousteau, 2014), etc. Few approaches are related to clustering, yet noise injected is big — the existence of a smaller, sufficient noise, was conjectured in (Nissim et al., 2007) — and approaches rely on a variety of assumptions or knowledge about the optimum (See Table 1) (Nissim et al., 2007; Wang et al., 2015). To apply k-variates++, we consider that ℘t = Id,∀t, and assume 0 < R ∞ s.t. maxa,a′∈A ‖a − a′‖2 ≤ R (a current assumption in the field (Dwork & Roth, 2014)).\nA general likelihood ratio bound for k-variates++ We show that the likelihood ratio of the same clustering for two “close” instances is governed by two quantities that rely on the neighborhood function. Most importantly for differential privacy, when densities p(µa,θa) are carefully chosen, this ratio always → 1 as a function of m, which is highly desirable for differential privacy. We let NNN(a) . = arg mina′∈N ‖a − a′‖2 denote the nearest neighbour of a in N, and let c(A) . = (1/|A|) ·∑a∈A a.\nDefinition 7 We say that neighborhood in A is δw-spread for some δw > 0 iff for any N ⊆ A with |N| = k − 1, and any B ⊆ A with |B| = |A| − 1,∑\na∈B\n‖a− NNN(a)‖22 ≥ R2\nδw . (11)\nDefinition 8 We say that neighborhood in A is δs-monotonic for some δs > 0 iff the following holds. ∀N ⊆ A with |N| ∈ {1, 2, ..., k − 1}, for any A ⊆ A\\N which is N-packed, we have:∑\na∈A\n‖a− NNN(a)‖22\n≤ (1 + δs) · ∑ a∈A ‖a− NNN∪{c(A)}(a)‖22 . (12)\nSet A is said N-packed iff there exists x ∈ Rd satisfying x = arg minc∈N∪{x} ‖a− c‖22, ∀a ∈ A. It is worthwhile remarking that as long as k < |A| ∞, both 0 < δw ∞ and 0 < δs ∞ always exist. Informally, δw brings that the sum of squared distances to any subset of k− 1 centers in A must not be negligible against the diameter R. δs yields a statement a bit more technical, but it roughly reduces to stating that adding one center to any set of at most k − 1 points that are already close to each other should not decrease significantly the overall potential to the set of centers. Figure 2 provides a schematic view of the property, showing that the modifications of the potential can be very local, thus yielding small δs in ineq. (12). The following Theorem uses the definition of neighbouring samples: samples A and A′ are neighbours, written A ≈ A′, iff they differ by one point. We also define P[C|A] to be the density of output C given input data A.\nTheorem 9 Fix ℘t = Id (∀t) and densities p(µ.,θ.) having the same support Ω in k-variates++. Suppose there exists %(R) > 0 such that densities p(µ.,θ.) satisfy the following pointwise likelihood\nratio constraint:\np(µa′ ,θa′ )(x)\np(µa,θa)(x) ≤ %(R) ,∀a,a′ ∈ A,∀x ∈ Ω . (13)\nThen, there exists a function f(.) such that, for any δw, δs > 0 such that A is δw-spread and δs-monotonic, for any A′ ≈ A, for any k > 0 and any C ⊂ Ω of size k output by Algorithm k-variates++ on whichever of A or A′, the likelihood ratio of C given A and A′ is upperbounded as:\nP[C|A′] P[C|A] ≤ (1 + δw) k−1+f(k)·δw·(1 + δs)k−1·%(R) . (14)\n(Proof in page 28) Notice that Theorem 9 makes just one assumption (13) about the densities, so it can be applied in fairly general settings, such as for regular exponential families (Banerjee et al., 2005). These are a key choice because they extensively cover the domain of distortions for which the average is the population minimiser.\nAn (almost) distribution-free 1 + o(1) likelihood ratio We now show that if A is sampled i.i.d. from any distribution D which satisfies the mild assumption that it is locally bounded everywhere (or almost surely) in a ball, then with high probability the right-hand side of ineq. (14) is 1 + o(1) where the little-oh vanishes with m. The proof, of independent interest, involves an explicit bound on δw and δs.\nTheorem 10 Suppose A with |A| = m > 1 sampled i.i.d. from distribution D whose support contains a L2 ball B2(0, R) with density inside in between m > 0 and M ≥ m. Let ρD .= M/ m (≥ 1). For any 0 < δ < 1/2, if (i) A ⊂ B(0, R) and (ii) the number of clusters k meets:\nk ≤ δ 2 4ρD · √m , (15)\nthen there is probability 1−δ over the sampling of A that k-variates++, instantiated as in Theorem 9, satisfies P[C|A′]/P[C|A] ≤ 1 + ρk\nD · g(m, k, d,R), ∀A′ ≈ A, with\ng(m, k, d,R) . = 4\nm 1 4 + 1 d+1\n+\n( 64\nk 2 d\n)k · %(2R)\nm . (16)\n(Proof in page 34) The key informal statement of Theorem 10 is that one may obtain with high probability some “good” datasets A, i.e., for which δw, δs are small, under very weak assumptions about the domain at hand. The key point is that if one has access to the sampling, then one can resample datasets A until a good one comes.\nApplications to differential privacy Let M be any algorithm which takes as input A and k, and returns a set of k centers C. Let PM [C|A] denote the probability, over the internal randomisation of M , that M returns C given A and k (k, fixed, is omitted in notations). Following is the definition of differential privacy (Dwork et al., 2006), tailored for conciseness to our clustering problem.\nDefinition 11 M is -differentially private (DP) for k clusters iff for any neighbors A ≈ A′, set C of k centers,\nPM [C|A′]/PM [C|A] ≤ exp . (17)\nA relaxed version of -DP is ( , δ)-DP, in which we require PM [C|A′] ≤ PM [C|A] · exp + δ; thus, -DP = ( , 0)-DP (Dwork & Roth, 2014). We show that low noise may be affordable to satisfy ineq. (17) using Laplace distribution, Lap(σ/ √ 2). We refer to the Laplace mechanism as a popular mechanism which adds to the output of an algorithm a sufficiently large amount of Laplace noise to be -DP. We refer to (Dwork et al., 2006) for details, and assume from now on that data belong to a L1 ball B1(0, R).\nTheorem 12 Using notations and setting of Theorem 9, let\ñ . = log ( exp( )− (1 + δw)k−1 f(k) · δw · (1 + δs)k−1 ) . (18)\nThen, k-variates++ with p(µ.,θ.) a product of Lap(σ1/ √ 2), for σ1 . = 2 √\n2R/̃, both meets ineq. (17) and its expected potential satisfies ineq. (7) with\nΦ = Φ1 . = 8 · ( φopt + mR2\ñ2\n) . (19)\nOn the other hand, if we opt for σ2 . = 2 √\n2kR/ , then k-variates++ is an instance of the Laplace mechanism and its expected potential satisfies ineq. (7) with\nΦ = Φ2 . = 8 · ( φopt + mk2R2\n2\n) . (20)\n(Proof in page 41) A question is how do σ1 (resp. Φ1) and σ2 (resp. Φ2) compare with each other, and how do they compare to the state of the art (Nissim et al., 2007; Wang et al., 2015) (we only consider methods with provable approximation bounds of the global optimum). The key fact is that, if m is sufficiently large, then it happens that we can fix δw = O(1/m) and δs = O(1). The proof of Theorem 10 (page 34) and the experiments (page 44) display that such regimes are indeed observed. In this case, it is not hard to show that ̃ = Ω( + logm), granting σ1 = o(σ2) since\nσ1 = O\n( R\n+ log(m)\n) , (21)\ni.e. the noise guaranteeing ineq. (17) vanishes at 1/ log(m) rate. Consequently, in this regime, Φ1 in eq. (19) becomes:\nΦ1 = Õ ( φopt +\nmR2\n( + logm)2\n) , (22)\nignoring all factors other than those noted. Thus, the noise dependence grows sublinearly in m. Since in this setting, unless all datapoints are the same, δw and δs for A and any possible neighbor\n4 5\n6 7\n8 9\n10 k 0\n10 20\n30 40\n50\np\n-4 -2 0 2 4 6 8\nρφ\nA′ are within 1 + o(1), it is also possible to overestimate δw and δs to still have δw = O(1/m) and δs = O(1) and grant -DP for k-variates++. Otherwise, the setting of Theorem 10 can be used to grant ( , δ)-DP without any tweak. Table 1 compares k-variates++ to (Nissim et al., 2007; Wang et al., 2015) in this large sample regime, which is actually a prerequisite for (Nissim et al., 2007; Wang et al., 2015). NotationO∗ removes all dependencies in their model parameters (assumptions, model parameters, and δ for the ( , δ)-DP in (Wang et al., 2015)), and λ is the separability assumption parameter (Nissim et al., 2007)4. The approximation bounds in (Nissim et al., 2007) consider Wasserstein distance between (estimated / optimal) centers, and not the potential involving data points like us. To obtain bounds that can be compared, we have used the simple trick that the observed potential is, up to a constant, no more than the optimal potential plus a fonction of the distance between (estimated / optimal) centers. This somewhat degrades the bound, but not enough for the observed discrepancies with our bound to reverse or even vanish. It is clear from the bounds that the noise dependence is significantly in our favor, and our bound is also significantly better at least when k is not too large."
    }, {
      "heading" : "5 Experiments",
      "text" : "The experiments carried out are provided in extenso in the Appendix (from page 44). Dk-means++ vs k-means++ and k-means‖ (Bahmani et al., 2012) To address algorithms that can be reduced from k-variates++ (Section 3), we have tested Dk-means++ vs state of the art approach k-means‖; to be fair with Dk-means++, we use k-means++ seeding as the reclustering algorithm in k-means‖. Parameters are in line with (Bahmani et al., 2012). To control the spread of Forgy nodes φFs (Theorem 4), each peer’s initial data consists of points uniformly sampled in a random hyperrectangle in a space of d = 50 (expected number of peers points mi = 500, ∀i). We sample\n4λ is named φ in (Nissim et al., 2007). We use λ to avoid confusion with clustering potentials.\npeers until a total of m ≈ 20000 point is sampled. Then, each point moves with p% chances to a uniformly sampled peer. We checked that φFs blows up with p, i.e., >20 times for p = 50% with respect to p = 0. A remarkable phenomenon was the fact that, even when the number of peers n is quite large (dozens on average), Dk-means++ is able to beat both k-means++ and k-means‖, even for large values of p, as computed by ratio ρφ(H) . = 100 · (φ(Dk-means++) − φ(H))/φ(H) for H ∈ {k-means++, k-means‖} (Figure 3). Another positive point is that the amount of data to compute a center for Dk-means++ is in average ≈ n times smaller than k-means‖.\nThe fact that Dk-means++, which locally implements the biased seeding, may be able to beat k-means++, which globally implements this seeding technique, is not surprising, and in fact may come from the leverage brought by the compartmentalization of distributed data: as discussed in deeper details in page 47, this may even improve the approximability ratio of Dk-means++ so that it beats the AV bound. k-variates++ vs Forgy-DP and GUPT To address algorithms that can be obtained via a direct use of k-variates++ (Section 4), we have tested it in a differential privacy framework vs state of the art approach GUPT (Mohan et al., 2012). We let = 1 in our experiments. We also compare it to Forgy DP (F-DP), which is just Forgy initialisation in the Laplace mechanism, with noise rate (standard dev.) ∝ kR/ . In comparison, the noise rate for GUPT is ∝ kR/(` ) at the end of its aggregation process, where ` is the number of blocks. Table 2 gives results for the average (over the choices of k) parameters used, k, ̃, and ratio ρ′φ where ρ ′ φ(H) . = φ(H)/φ(k-variates++) — values above 1 indicate better results for k-variates++. We use ̃ as the equivalent for k-variates++, i.e. the value that guarantees ineq. (17). From Theorem 12, when ̃ > , this brings a smaller noise magnitude, desirable for clustering. The obtained results show that k-variates++ becomes more of a contender with increasing m, but its relative performance tends to decrease with increasing k. This is in accordance with the “good” regime of Theorem 12. Results on synthetic domains display the same patterns, along with the fact that relative performances of k-variates++ improves with d, making it a relevant choice for ”big” domains.\nIn fact, extensive experiments on synthetic data (page 44) show that intuitions regarding the sublinear noise regime in eq. (22) are experimentally observed, and furthermore they may happen for quite small values of m."
    }, {
      "heading" : "6 Discussion and Conclusion",
      "text" : "We first show in this paper that the k-means++ analysis of Arthur and Vassilvitskii can be carried out on a significantly more general scale, aggregating various clustering frameworks of interest and for which no trivial adaptation of k-means++ was previously known. Our contributions stand at two levels: (i) we provide the “meta” algorithm, k-variates++, and two key results, one on its\napproximation abilities of the global optimum, and one on the likelihood ratio of the centers it delivers. We do expect further applications of these results, in particular to address several other key clustering problems: stability, generalisation and smoothed analysis (Arthur et al., 2011; von Luxburg, 2010); (ii) we provide two examples of application. The first is a reduction technique from k-variates++, which shows a way to obtain straight approximabilty results for other clustering algorithms, some being efficient proxies for the generalisation of existing approaches (Ailon et al., 2009). The second is a direct application of k-variates++ to differential privacy, exhibiting a noise component significantly better than existing approaches (Nissim et al., 2007; Wang et al., 2015).\nWe have not discussed here the possibility to replace the L22 distortion which computes the potential by elements from large and interesting classes — clustering being a huge practical problem, it is indeed reasonable to tailor the distortion to the application at hand. One example are Bregman divergences, that fail simple metric transforms (Acharyya et al., 2013). Another example are total divergences, that fail the simple computation of the population minimizers (Nock et al., 2016; Liu et al., 2012). Some do not even admit population minimizers in closed form (Nielsen & Nock, 2015). It turns out that k-variates++, and its good approximation properties, can be extended to such cases (see page 42) for total Jensen divergence (Nielsen & Nock, 2015)."
    }, {
      "heading" : "7 Acknowledgments",
      "text" : "Thanks are due to Stephen Hardy, Guillaume Smith, Wilko Henecka and Max Ott for stimulating discussions and feedback on the subject. Nicta is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Center of Excellence Program."
    }, {
      "heading" : "Appendix — Table of contents",
      "text" : "Appendix on proofs Pg 19 Proof of Theorem 2 Pg 19 Proof of Lemma 3 Pg 22 Comments on Table 1 Pg 22 Proofs of Theorems 4, 5 and 6 Pg 23 ↪→ Proof of Theorem 4 Pg 23 ↪→ Proof of Theorem 5 Pg 25 ↪→ Proof of Theorem 6 Pg 26 Proof of Theorem 9 Pg 28 Proof of Theorem 10 Pg 34 Proof of Theorem 12 Pg 41 Extension to non-metric spaces Pg 42"
    }, {
      "heading" : "Appendix on experiments Pg 44",
      "text" : "Experiments on Theorem 12 and the sublinear noise regime Pg 44 Experiments with Dk-means++, k-means++ and k-means‖ Pg 47 Experiments with k-variates++ and GUPT Pg 51"
    }, {
      "heading" : "8 Appendix on Proofs",
      "text" : "Several proofs rely on properties of the k-means++ algorithm that are not exploited in the proof of (Arthur & Vassilvitskii, 2007). We assume here the basic knowledge of the proof technique of (Arthur & Vassilvitskii, 2007)."
    }, {
      "heading" : "Proof of Theorem 2",
      "text" : "Let A denote a subset of A, and c(A) .= (1/|A|) ·∑a∈A a the barycenter of A. It is well known that c(A) = arg mina′∈Rd ∑ a∈A ‖a− a′‖22, so the potential of A,\nφ(A) . = ∑ a∈A ‖a− c(A)‖22 (23)\nis just the optimal potential of A if A defines a cluster in the optimal clustering. We also define the noisy potential of A as:\nφN(A) . = ∑ a∈A ∫ Ωa ‖x− c(A)‖22dpa(x) . (24)\nThe proof of Theorem 2 follows the same path as the proof of Theorem 3.1 in (Arthur & Vassilvitskii, 2007). Instead of reproducing the proof, we shall assume basic knowledge of the original proof and will just provide the side Lemmata that are sufficient for our more general result. The first Lemma is a generalization of Lemma 3.2 in (Arthur & Vassilvitskii, 2007).\nLemma 13 Let Copt denotes the optimal partition of A according to eq. (2). Let A be an arbitrary cluster in Copt. Let C be a single-cluster clustering whose center is chosen at random by one step of Algorithm k-variates++ (i.e. for t = 1). Then\nE[φ(A)] = φopt(A) + φNopt(A) . (25)\nProof The expected potential of cluster A is\nE[φ(A;C = ∅)]\n= 1 |A| · ∑ a0∈A ∫ Ωa0 ∑ a∈A ‖a− x‖22dpa0(x) = 1\n|A| · ∑ a0∈A ∫ Ωa0 ∑ a∈A ‖a− c(A) + c(A)− x‖22dpa0(x)\n= 1 |A| · ∑ a0∈A\n( ∑ a∈A ‖a− c(A)‖22 + |A| · ∫ Ωa0 ‖x− c(A)‖22dpa0(x)\n+2 ∑ a∈A〈a− c(A), c(A)− ∫ Ωa0 xdpa0(x)〉\n)\n= 1 |A| · ∑ a0∈A\n ∑ a∈A ‖a− c(A)‖22 + |A| · ∫ Ωa0 ‖x− c(A)‖22dpa0(x) +2〈 ∑ a∈A\na− |A|c(A)︸ ︷︷ ︸ =0 , c(A)− a0〉  =\n∑ a∈A ‖a− c(A)‖22 + ∑ a∈A ∫ Ωa0 ‖x− c(A)‖22dpa(x)\n= φopt(A) + φ N opt(A) ,\nas claimed.\nWhen pa is a Dirac anchored at a, we recover Lemma 3.2 in (Arthur & Vassilvitskii, 2007). The following Lemma generalizes Lemma 3.3 in (Arthur & Vassilvitskii, 2007).\nLemma 14 Suppose that the optimal clustering Copt is η-probe approximable. Let A be an arbitrary cluster inCopt, and letC be an arbitrary clustering with centers C. Suppose that the reference point a chosen according to (1) in Step 2.1 is in A. Then the random point x picked in Step 2.2 brings an expected potential that satisfies\nE[φ(A)] ≤ (6 + 4η) · φopt(A) + 2 · φNopt(A) . (26) Proof Let us denote c?(u) .= arg minx∈C ‖u−x‖22 (since C 6= Copt in general, c?(u) 6= copt(u)), and D(a) .= ‖a − c?(a)‖22 the contribution of a ∈ A to the k-means potential defined by C. We have, using Lemma 3.3 in (Arthur & Vassilvitskii, 2007) and Lemma 13,\nEx[φ(A;C ∪ {x})] = ∑ a0∈A Dt(a0)∑ a∈ADt(a) · ∑ a∈A ∫ Ωa0 min{D(a), ‖a− x‖22}dpa0(x) . (27)\nThe triangle inequality gives, for any a ∈ A,√ Dt(a0)\n. = ‖℘t(a0)− c?(℘t(a0))‖2 ≤ ‖℘t(a0)− c?(℘t(a))‖2 ≤ ‖℘t(a0)− ℘t(a)‖2 + ‖℘t(a)− c?(℘t(a))‖2 ; (28)\nsince (a+ b)2 ≤ 2a2 + 2b2, then Dt(a0) ≤ 2‖℘t(a0)− ℘t(a)‖22 + 2Dt(a), and so, after averaging over A,\nDt(a0) ≤ 2 |A| ∑ a∈A ‖℘t(a0)− ℘t(a)‖22 + 2 |A| ∑ a∈A Dt(a) , (29)\nand eq. (27) can be upperbounded as:\nEx[φ(A;C ∪ {x})] ≤ 2 |A| ∑ a0∈A ∑ a∈A ‖℘t(a0)− ℘t(a)‖22∑ a∈ADt(a) · ∑ a∈A ∫ Ωa0 min{D(a), ‖a− x‖22}dpa0(x)\n+ 2 |A| ∑ a0∈A ∑ a∈ADt(a)∑ a∈ADt(a) · ∑ a∈A ∫ Ωa0 min{D(a), ‖a− x‖22}dpa0(x)\n≤ 2|A| ∑ a0∈A ∑ a∈AD(a)∑ a∈ADt(a) · ∑ a∈A\n‖℘t(a0)− ℘t(a)‖22︸ ︷︷ ︸ . =P1 + 2\n|A| ∑ a0∈A ∑ a∈A ∫ Ωa0\n‖a− x‖22dpa0(x)︸ ︷︷ ︸ . =P2 . (30)\nWe bound the two potentials P1 and P2 separately, starting with P1. Fix any a0 ∈ A. If ∑ a∈A ‖℘t(a)− ℘t(a0)‖22 = 0, then trivially(∑ a∈A D(a) ) · (∑ a∈A ‖℘t(a0)− ℘t(a)‖22 ) ≤ (1 + η) · (∑ a∈A Dt(a) ) · (∑ a∈A ‖a0 − a‖22 ) ,(31)\nsince the right-hand side cannot be negative. If ∑ a∈A ‖℘t(a) − ℘t(a0)‖22 6= 0, then since ℘t is η-stretching, we have:∑ a∈A ‖a− c?(a)‖22∑ a∈A ‖a− a0‖22 ≤ (1 + η) · ∑ a∈A ‖℘t(a)− c?(℘t(a))‖22∑ a∈A ‖℘t(a)− ℘t(a0)‖22 , (32)\nwhich is exactly ineq. (31) after rearranging the terms. Ineq (31) implies\nP1 ≤ 2(1 + η) · 1 |A| ∑ a0∈A ∑ a∈A ‖a0 − a‖22\n= 4(1 + η) · φopt(A) , (33)\nwhere the equality follows from (Arthur & Vassilvitskii, 2007), Lemma 3.2. Also, Lemma 13 brings\nP2 = 2 · 1 |A| ∑ a0∈A ∫ Ωa0 ∑ a∈A ‖a− x‖22dpa0(x)\n= 2φopt(A) + 2φ N opt(A) . (34)\nWe therefore get\nEx[φ(A;C ∪ {x})] ≤ (6 + 4η) · φopt(A) + 2 · φNopt(A) , (35)\nas claimed.\nAgain, we recover Lemma 3.3 in (Arthur & Vassilvitskii, 2007) when pa is a Dirac and the probe function ℘ = Id. The rest of the proof of Theorem 2 consists of the same steps as Theorem 3.1 in (Arthur & Vassilvitskii, 2007), after having remarked that φNopt(A) can be simplified:\nφNopt(A) = ∑ a∈A ∫ Ωa0 ‖x− c(A)‖22dpa(x)\n= ∑ a∈A ∫ Ωa0 ‖x‖22dpa(x)− 2〈c(A),µa〉+ ‖c(A)‖22\n= ∑ a∈A ∫ Ωa0 ‖x− µa‖22dpa(x) + ‖µa‖22 − 2〈c(A),a〉+ ‖c(A)‖22\n= ∑ a∈A { tr (Σa) + ‖µa − c(A)‖22 } = φbias(A) + φvar(A) . (36)"
    }, {
      "heading" : "Proof of Lemma 3",
      "text" : "The proof is a simple application of the Fréchet-Cramér-Rao-Darmois bound. Consider the simple case k = 1 and a spherical Gaussian noise for p with a single point in A. Renormalize both sides of (7) by m .= |A| so that (1/m)∑a∈A tr (Σa) = tr (Σ). One sees that the left hand side of ineq. (7) is just an estimator of the variance of pa, which, by Fréchet-Darmois-Cramér-Rao bound, has to be at least the inverse of the Fisher information, that is in this case, the trace of the covariance matrix, i.e. tr (Σ)."
    }, {
      "heading" : "Comments on Table 1",
      "text" : "(Wang et al., 2015) are concerned with approximating subspace clustering, and so they are using a very different potential function, which is, between two subspaces S and S′, d(S, S′) = ‖UU> − U′U′>‖F , where U (resp. U′) is an orthonormal basis for S (resp. S′). To obtain an idea of the approximation on the k-means clustering problem that their technique yields, we compute φ in the projected space, using the fact that, because of the triangle inequality and the fact that projections are linear and do not increase norms,\n‖projU(a)− projU′(a′)‖2 = ‖(projU(a)− projU(a′)) + (projU(a′)− projU′(a′))‖2 (37) ≤ ‖projU(a)− projU(a′)‖2 + ‖projU(a′)− projU′(a′))‖2(38) ≤ ‖projU(a)− projU(a′)‖2 + 2‖a′‖2 . (39)\nTo account for the approximation in the inequalities, we then discard the rightmost term, replacing therefore ‖projU(a)− projU′(a′)‖2 by ‖projU(a)− projU(a′)‖2, which amounts, in the approximation bounds, to remove the dependence in the dimension. At this price, and using the trick to transfer the wasserstein distance between centers to L22 potential between points to cluster centers, we obtain the approximation bound in (β) of Table 1. While it has to be used with care, its main interest is in showing that the price to pay because of the noise component is in fact not decreasing in m."
    }, {
      "heading" : "Proofs of Theorems 4, 5 and 6",
      "text" : "The proof of these Theorems uses a reduction from k-variates++ to the corresponding algorithms, meaning that there exists particular probe functions and densities for which the set of centers delivered by k-variates++ is the same as the one delivered by the corresponding algorithms.\nDefinition 15 Let H (parameters omitted) be any hard membership k-clustering algorithm. We way that k-variates++ reduces to H iff there exists data, densities and probe functions depending on the instance of H such that, in expectation over the internal randomisation of H, the set of centers delivered by H are the same as the ones delivered by k-variates++. We note it\nk-variates++ H . (40)\nHence, whenever k-variates++ H, Theorem 2 immediately gives a guarantee for the approximation of the global optimum in expectation for H, but this requires the translation of the parameters involved in Φ in ineq. (7) to involve only parameters from H. In all our examples, this translation poses no problem at all."
    }, {
      "heading" : "Proof of Theorem 4",
      "text" : "Figure 4 presents the architecture of message passing in the Dk-means++/PDk-means++ framework. We first focus on the protected scheme, Dk-means++. We reduce k-variates++ to Algorithm\n1 using identity probe functions: ℘t = Id, ∀t. The trick in reduction relies on the densities. We let pµa,θa be uniform over the subset Ai to which a belongs. Thus, the support of densities is discrete, and C is a subset of A; furthermore, the probability qt(a) that a ∈ Ai is chosen at iteration t in k-variates++ actually simplifies to a convenient expression:\nqt(a) = q D ti · ui , (41)\nwhere we recall that\nqDti . =\n{ Dt(Ai) · ( ∑ j Dt(Aj))\n−1 if t > 1 (1/n) otherwise . (42)\nHence, picking a can be equivalently done by first picking Ai using qDt , and then, given the i chosen, sampling uniformly at random a in Ai, which is what Forgy nodes do. We therefore get the equivalence between Algorithm 1 and k-variates++ as instantiated.\nLemma 16 With data, densities and probes defined as before, k-variates++ Dk-means++. To get the approximability ratio of Dk-means++, we translate the parameters of Φ in ineq. (7). First, since (a+ b)2 ≤ 2a2 + 2b2,\nφbias . = ∑ a∈A ‖µa − copt(a)‖22\n= ∑ i∈[n] ∑ a∈Ai ‖c(Ai)− copt(a)‖22 (43)\n= ∑ i∈[n] ∑ a∈Ai ‖c(Ai)− a+ a− copt(a)‖22\n≤ 2 ∑ i∈[n] ∑ a∈Ai ‖c(Ai)− a‖22 + 2 ∑ a∈A ‖a− copt(a)‖22\n= 2φFs + 2φopt . (44)\nFurthermore,\nφvar . = ∑ a∈A tr (Σa)\n= ∑ a∈A ∫ Ωa ‖x− µa‖22dpa(x)\n= ∑ i∈[n] ∑ a∈Ai ∑ a′∈Ai 1 mi · ‖a′ − c(Ai)‖22\n= ∑ i∈[n] ∑ a∈Ai ‖a− c(Ai)‖22 = φFs . (45)\nThere remains to plug ineq. (44) and eq. (45) in Theorem 2, along with η = 0 (since ℘ = Id), to get E[φ(A;C)] ≤ (2 + log k) · (10φopt + 6φs), as in Theorem 4.\nThe private version, PDk-means++, follows immediately by leaving φvar in Φ instead of carrying eq. (45). This ends the proof of Theorem 4.\n℘ ="
    }, {
      "heading" : "Proof of Theorem 5",
      "text" : "The proof proceeds in the same way as for Theorem 4. The probe function (the same for every iteration, ℘t = ℘,∀t) is already defined in the statement of Theorem 5, from the definition of synopses. The distributions pµa,θa are Diracs anchored at the probe (synopses) locations. The centers chosen in k-variates++ are thus synopses, and it is not hard to check that the probability to pick a synopsis sj at iteration t factors in the same way as in the definition of qSt in eq. (9). We therefore get the equivalence between Algorithm 2 and k-variates++ as instantiated.\nLemma 17 With data, densities and probes defined as before, k-variates++ Sk-means++. The proof of the approximation property of Sk-means++ then follows from the fact that φvar = 0 (Diracs) and\nφbias . = ∑ a∈A ‖µa − copt(a)‖22\n= ∑ a∈A ‖℘(a)− copt(a)‖22\n= ∑ a∈A ‖℘(a)− a+ a− copt(a)‖22\n≤ 2 ∑ a∈A ‖℘(a)− a‖22 + 2 ∑ a∈A ‖a− copt(a)‖22\n= 2 ∑ a∈S ‖℘(a)− a‖22 + 2 ∑ a∈S ‖a− copt(a)‖22 = 2φ℘s + 2φopt (46)\n(using again (a+ b)2 ≤ 2a2 + 2b2). Using Theorem 2, this brings the statement of the Theorem.\nFigure 5 shows that the ”quality” of the probe function (spread φ℘s , stretching factor η) stem from the quality of the Voronoi diagram induced by the synopses in S."
    }, {
      "heading" : "Proof of Theorem 6",
      "text" : "The proof proceeds in the same way as for Theorem 4. The the reduction from k-variates++ to OLk-means++ relies on two things: first, the uniform choice of the first center in k-means++ can be replaced by picking the center uniformly in any subset of the data: it does not change the expected approximation properties of the algorithm (this comes from Lemma 3.4 in (Arthur & Vassilvitskii, 2007)); therefore, the choice q1 . = um in k-variates++ can be replaced with q1 . = u1 (uniform with support A1). Second, a particular probe function needs to be devised, sketched in Figure 6. Basically, all probe functions of a minibatch are the same: each point in the minibatch is probed to itself, while points occurring outside the minibatch are probed to their closest center. The reduction proceeds in the following steps: we first let A be the complete set of points in the stream S. Then, we let Aj denote the set of points of minibatch Sj . Remark that minibatch Aj occurs in the stream before Aj′ for j < j′, and minibatches induce a partition of A. Let j(t) denote the batch related to iteration t in k-variates++. We define the following probe function ℘t(a) in k-variates++, letting Aj the minibatch to which a belongs (we do not necessarily have j = j(t)):\n• if j = j(t), then ℘t(a) .= a;\n• else ℘t(a) .= arg minc∈C ‖a− c‖22 (remark that |C| ≥ 1 in this case). Finally, densities p(µ.,θ.) are Diracs anchored at selected points, like in k-means++. We get the equivalence between Algorithm 3 and k-variates++ as instantiated.\nLemma 18 With data, densities and probes defined as before, k-variates++ OLk-means++. The proof is immediate, since each minibatch is hit by a center exactly once in OLk-means++, and when one subset Aj is hit by a center, then the probe function makes that no other center can be sampled again from Aj (all contributions to the density qt are then zero in Aj). We now finish the proof of Theorem 6 by showing the same approximability ratio for k-variates++ as reduced. Because optimal clusters are ς-wide with respect to stream S, we have\n1 |A| · ∑ a,a′∈A ‖a− a′‖22 ≥ ς ·R .\nRecall that c(A) .= (1/|A|) ·∑a∈A a. For any a0 ∈ A, it holds that: 1\n|A| − 1 · ∑ a∈A ‖a− a0‖22 ≥ 1 |A| − 1 · ∑ a∈A ‖a− c(A)‖22 (47)\n= 1 |A| − 1 · ( 1 2|A| · ∑ a,a′∈A ‖a− a′‖22 ) (48)\n= 1 4 · 2|A|(|A| − 1) · ∑ a,a′∈A ‖a− a′‖22 ≥ ς 4 ·R . (49)\nIneq. (47) holds because c(A) is the population minimizer for optimal cluster A (see e.g., (Arthur & Vassilvitskii, 2007), Lemma 2.1). Since probes are points of A,\nφ(℘j(A); {℘j(a0)}) ≤ |A| ·R\n≤ 4|A| ς(|A| − 1) · ∑ a∈A ‖a− a0‖22 . (50)\nOn the other hand, we have: φ(℘t(A);C) = ∑\na∈A∩Sj\n‖a− c(a)‖22 , (51)\nbut since minibatches are ς accurate, ∑ a∈A∩Sj ‖a− c(a)‖22 ≥ ς · ∑ a∈A ‖a− c(a)‖22. Therefore, for any a0 ∈ A, φ(℘t(A);C)\nφ(℘t(A); {℘t(a0)}) ≥\n( ς2(|A| − 1)\n4|A|\n) · ∑ a∈A ‖a− c(a)‖22∑ a∈A ‖a− a0‖22\n=\n( ς2(|A| − 1)\n4|A|\n) · φ(A;C) φ(A; {a0}) . (52)\nIn other words, probe functions are η-stretching, for any η satisfying:\nη ≥ 4|A| ς2(|A| − 1) − 1 , (53)\nand they are therefore η-stretching for η = 8/ς2 − 1. There remains to check that, because of the densities chosen,\nφbias = φopt , (54) φvar = 0 . (55)\nThis ends the proof of Theorem 6."
    }, {
      "heading" : "Proof of Theorem 9",
      "text" : "To simplify notations in the proof, we let pa(x) denote the value of density p(µa,θa) on some x ∈ Ω. Let us denote Seq(n : k) the number of sequences of integers in set {1, 2, ..., n} having exactly k elements, whose cardinal is |Seq(n : k)| = n!/(n− k)!. For any sequence I ∈ Seq(n : k), we let Ii denote its ith element. For any set C . = {c1, c2, ..., ck} returned by Algorithm k-variates++with input instance set A .= {a1,a2, ...,an} ⊂ Ω, the density of C given A is:\nP[C|A] = ∑ σ∈Sk ∑ I∈Seq(n:k) p(σ, I,C|A) , (56)\nwhere Sk denotes the symmetric group on k elements, and the following shorthand is used:\np(σ, I,C|A) .= k∏ i=1 qi(aIi)paIi (cσ(i)) , (57)\nwhere qi is computed using eq. (1) and taking into account the modification due to the choice of each Ij for j < i in the sequence I .\nIn the following, we let A and A′ denote two sets of points that differ from one a (they have the same size), say an ∈ A and a′n ∈ A′, an 6= a′n. We analyze:\nP[C|A′] P[C|A] =\n∑ σ∈Sk ∑ I∈Seq(n:k) p(σ, I,C|A′)∑\nσ∈Sk\n∑ I∈Seq(n:k) p(σ, I,C|A) . (58)\nUsing the definition of q(.), we refine p(σ, I,C|A) as\np(σ, I,C|A) = N(I)∏k i=1 M(I i|A) · k∏ i=1 paIi (cσ(i)) , (59)\nwhere\nN(I) . = j∏ i=2 ‖aIi − NNIi(aIi)‖22 , (60)\nM(I i|A) .= {\nn if i = 1∑n j=1 ‖aj − NNIi(aj)‖22 otherwise , (61)\nand I i is the prefix sequence I1, I2, ..., Ii−1, and NNIi(a) . = arg minj≤i−1 ‖a− aIj‖2 is the nearest neighbor of a in the prefix sequence. Notice that there is a factor 1/m for q(.) at the first iteration that we omit in N(I) since it disappears in the ratio in eq. (58).\nWe analyze separately each element in (59), starting with N(I). We define the swapping operation s`(I) that returns the sequence in which aI` and aI`+1 are permuted, for 1 ≤ ` ≤ k − 1. This incurs non-trivial modifications in N(s`(I)) compared to N(I), since the nearest neighbors of aI` and aI`+1 may change in the permutation:\nN(s`(I)) = `−1∏ i=2 ‖aIi − NNIi(aIi)‖22\n· ‖aI`+1 − NNI`(aI`+1)‖22 · ‖aI` − NNI`∪{I`+1}(aI`)‖22︸ ︷︷ ︸ 6=‖aI`−NNI` (aI` )‖ 2 2·‖aI`+1−NNI`+1 (aI`+1 )‖ 2 2\n· k∏\ni=`+2\n‖aIi − NNIi(aIi)‖22 (62)\n(I ∪ {j} indicates that element j is put at the end of the sequence). We want to quantify the maximal increase in N(s`(I)) compared to N(I). The following Lemma shows that the maximal increase ratio is actually a constant, and thus does not depend on the data.\nLemma 19 The following holds true:\nN(s1(I)) = N(I) , (63) N(s`(I)) ≤ (1 + η)2N(I) ,∀2 ≤ ` ≤ k − 1 . (64)"
    }, {
      "heading" : "Here, 0 ≤ η ≤ 3 is a constant.",
      "text" : "The proof stems directly from the following Lemma.\nLemma 20 For any non-empty N ⊆ A and x ∈ Ω, let NNN(x) denote the nearest neighbor of x in N. There exists a constant 0 ≤ η ≤ 3 such that for any ai,aj ∈ A and any nonempty subset N ⊆ A\\{ai,aj},\n‖ai − NNN(ai)‖2 ‖ai − NNN∪{aj}(ai)‖2 ≤ (1 + η) · ‖aj − NNN(aj)‖2‖aj − NNN∪{ai}(aj)‖2 . (65)\nProof Since ‖aj − NNN∪{ai}(aj)‖2 ≤ ‖aj − NNN(aj)‖2, the proof is true for η = 0 when NNN(ai) = NNN∪{aj}(ai). So suppose that NNN(ai) 6= NNN∪{aj}(ai), implying NNN∪{aj}(ai) = aj . We distinguish two cases. Case 1/2, if NNN∪{ai}(aj) = ai, then we are reduced to showing that ‖ai − NNN(ai)‖2 ≤ (1 + η)‖aj−NNN(aj)‖2 under the conditions (C) that N∩B(ai, ‖ai−aj‖2) = ∅ and N∩B(aj, ‖ai− aj‖2) = ∅. Here, B(a, r) denotes the open ball of center a and radius R. The triangle inequality and conditions (C) bring\n‖ai − NNN(ai)‖2 ≤ ‖ai − aj‖2 + ‖aj − NNN(ai)‖2 ≤ ‖aj − NNN(aj)‖2 + ‖aj − NNN(ai)‖2 . (66)\nIf NNN(ai) = NNN(aj) then the inequality holds for η = 1. Otherwise, suppose that ‖aj − NNN(ai)‖2 > 3‖aj − NNN(aj)‖2. The triangle inequality yields again ‖aj − NNN(ai)‖2 ≤ ‖aj − ai‖2 + ‖ai − NNN(ai)‖2, and so we have the inequality:\n3‖aj − NNN(aj)‖2 < ‖aj − ai‖2 + ‖ai − NNN(ai)‖2 , (67)\nand since (C) holds, ‖aj − NNN(aj)‖2 ≥ ‖aj − ai‖2 which implies\n‖aj − NNN(aj)‖2 < 1\n2 · ‖ai − NNN(ai)‖2 . (68)\nOn the other hand, the triangle inequality brings again\n‖ai − NNN(aj)‖2 ≤ ‖ai − aj‖2 + ‖aj − NNN(aj)‖2 ≤ 2 · ‖aj − NNN(aj)‖2 (69) < 2 · 1\n2 · ‖ai − NNN(ai)‖2 = ‖ai − NNN(ai)‖2 , (70)\na contradiction since ‖ai − NNN(ai)‖2 ≤ ‖ai − al‖2, ∀al ∈ N by definition. Ineq. (69) uses (C) and ineq. (70) uses ineq. (68). Hence, if NNN(ai) 6= NNN(aj) then since ‖aj − NNN(ai)‖2 ≤ 3‖aj − NNN(aj)‖2, ineq. (66) brings ‖ai− NNN(ai)‖2 ≤ 4 · ‖aj − NNN(aj)‖2, and the inequality holds for η = 3. Case 2/2, if NNN∪{ai}(aj) 6= ai, then it implies NNN∪{ai}(aj) = NNN(aj) and so\n∃a∗ ∈ N : ‖aj − a∗‖2 ≤ ‖aj − ai‖2 . (71)\nIneq. (65) reduces to proving\n‖ai − NNN(ai)‖2 ≤ (1 + η) · ‖ai − aj‖2 , (72)\nbut ‖ai−a∗‖2 ≤ ‖ai−aj‖2 +‖aj−a∗‖2 ≤ 2‖ai−aj‖2, and since a∗ ∈ N, ‖ai−NNN(ai)‖2 ≤ ‖ai−a∗‖2 ≤ 2‖ai−aj‖2, and (72) is proved for η = 1. This achieves the proof of Lemma 20. Let I be any sequence not containing the index of a′n, and let I(i) denote the sequence in which we replace aIi by the index of a ′ n. The sequence of swaps\nI(k) = (sk−1 ◦ ... ◦ si+1 ◦ si)(I(i)) (73)\nproduces a sequence I(k) in which all elements different from a′n are in the same relative order as they are in I with respect to each other, and a′n is pushed to the end of the sequence in k\nth rank. We also have\nN(I(i)) ≤ (1 + η)2(k−i)N(I(k)) . (74)\nAll the properties we need on N(.) are now established. We turn to the analysis of M(I i|A).\nLemma 21 For any δs > 0 such that A is δs-monotonic, the following holds. For any N ⊆ A with |N| ∈ {1, 2, ..., k − 1}, ∀x,x′ ∈ Ω, we have:∑\na∈A ‖a− NNN∪{x}(a)‖22 ≤ (1 + δs) · ∑ a∈A ‖a− NNN∪{x′}(a)‖22 . (75)\nProof Since adding a point to N cannot increase the potential ∑ a∈A ‖a−NNN∪{x}(a)‖22, it comes∑\na∈A ‖a− NNN∪{x}(a)‖22 ≤ ∑ a∈A ‖a− NNN(a)‖22 ,∀x ∈ Ω . (76)\nConsider any x′ ∈ Ω such that∑a∈A ‖a−NNN∪{x′}(a)‖22 = ∑a∈A ‖a−NNN(a)‖22, i.e., all points of A are closer to a point in N than they are from x′. In this case, we obtain from ineq. (76),∑\na∈A ‖a− NNN∪{x}(a)‖22 ≤ ∑ a∈A ‖a− NNN∪{x′}(a)‖22 , (77)\nand since δs > 0, the statement of the Lemma holds. More interesting is the case wherex′ ∈ Ω is such that∑a∈A ‖a−NNN∪{x′}(a)‖22 <∑a∈A ‖a− NNN(a)‖22, implying x′ 6∈ N. In this case, let A .\n= {a ∈ A : NNN∪{x′}(a) = x′}, which is then non-empty. Let us denote for short c(A) .= (1/|A|) ·∑a∈A a. Since x′ 6∈ N, A∩N = ∅, and since A is δs-monotonic, then it comes from ineq. (76)∑\na∈A ‖a− NNN∪{x}(a)‖22 ≤ (1 + δs) · ∑ a∈A ‖a− NNN∪{c(A)}(a)‖22 . (78)\nWe have:∑ a∈A ‖a− NNN∪{c(A)}(a)‖22 = ∑ a∈A\\A ‖a− NNN∪{c(A)}(a)‖22 + ∑ a∈A ‖a− NNN∪{c(A)}(a)‖22\n≤ ∑ a∈A\\A ‖a− NNN∪{c(A)}(a)‖22 + ∑ a∈A ‖a− c(A)‖22\n≤ ∑ a∈A\\A ‖a− NNN∪{c(A)}(a)‖22 + ∑ a∈A ‖a− x′‖22 . (79)\nEq. (79) holds because the arithmetic average is the population minimizer of L22. Because of the definition of A, ∑\na∈A\\A ‖a− NNN∪{c(A)}(a)‖22 ≤ ∑ a∈A\\A ‖a− NNN(a)‖22\n= ∑ a∈A\\A ‖a− NNN∪{x′}(a)‖22 , (80)\nand, still because of the definition of A,∑ a∈A ‖a− x′‖22 = ∑ a∈A ‖a− NNN∪{x′}(a)‖22 , (81)\nso we get from (80) and (81) ∑ a∈A\\A ‖a − NNN∪{c(A)}(a)‖22 + ∑ a∈A ‖a − x′‖22 ≤ ∑ a∈A ‖a − NNN∪{x′}(a)‖22, and finally from ineq. (79),∑ a∈A ‖a− NNN∪{c(A)}(a)‖22 ≤ ∑ a∈A ‖a− NNN∪{x′}(a)‖22 , (82)\nwhich, using ineq. (78), completes the proof of Lemma 21.\nLemma 22 The following holds true, for any i > 1, any A′ ≈ A, any δw, δs > 0:\nA is δw-spread ⇒ (n 6∈ I i ⇒M(I i|A) ≤ (1 + δw) ·M(I i|A′)) , (83) A is δs-monotonic ⇒ (n ∈ I i ⇒M(I i|A) ≤ (1 + δs) ·M(I i|A′)) . (84)\nProof Suppose first that n 6∈ I i. In this case, since A is δw-spread,\nM(I i|A) = n∑ j=1 ‖aj − NNIi(aj)‖22\n= n−1∑ j=1 ‖aj − NNIi(aj)‖22 + ‖an − NNIi(aj)‖22\n≤ n−1∑ j=1 ‖aj − NNIi(aj)‖22 +R2\n≤ (1 + δw) · n−1∑ j=1 ‖aj − NNIi(aj)‖22 (85)\n≤ (1 + δw) · ( n−1∑ j=1 ‖aj − NNIi(aj)‖22 + ‖a′n − NNIi(a′n)‖22 ) = (1 + δw) ·M(I i|A′) , (86)\nas indeed computing the nearest neighbors do not involve the nth element of the sets, i.e. an or a′n. We have used in ineq. (85) the fact that A is δw-spread.\nWhen n ∈ I i, eq. (84) is an immediate consequence of Lemma 21 in which the distinct elements of A and A′ play the role of x and x′.\nLemma 23 For any δw > 0, if A is δw-spread, then for any N ⊆ A with |N| = k − 1, ∀x ∈ Ω, it holds that ‖x− NNN(x)‖22 ≤ δw ∑ a∈A ‖a− NNN(a)‖22.\nProof Follows directly from the fact that ‖x− NNN(x)‖22 ≤ R2 by assumption. Letting I(k) denote a sequence containing element n pushed to the end of the sequence, we get:∑\nσ∈Sk ∑ I∈Seq+(n:k) p(σ, I,C|A′)\n= ∑ σ∈Sk ∑ I∈Seq+(n:k) N(I)∏k i=1M(I i|A′) · pa′n(cσ(i)) · k∏ i=1:Ii 6=n paIi (cσ(i)) ≤ (1 + η)2(k−2)\n· ∑ σ∈Sk ∑ I∈Seq+(n:k) N(I(k))∏k i=1M(I i|A′) · pa′n(cσ(i)) · k∏ i=1:Ii 6=n paIi (cσ(i)) . (87)\nNow, take any element I ∈ Seq+(n : k) with a′n in position k, and change a′n by some a ∈ A. Any of these changes generates a different element I ′ ∈ Seq−(n : k), and so using Lemma 23 and the following two facts:\n• the fact that\npa′n(cσ(i)) ≤ %(R) · pa(cσ(i)) , (88)\nfor any a ∈ A,\n• the fact that, if A is δs-monotonic,\nM(I ia|A) ≤ (1 + δs) ·M(I i|A) , (89)\nfor any a ∈ A not already in the sequence, where Ia denotes the sequence I in which a′n has been replaced by a,\nwe get from ineq. (87),∑ σ∈Sk ∑ I∈Seq+(n:k) p(σ, I,C|A′)\n≤ (1 + η)2(k−2) · (1 + δs)k−1 · δw\n·%(R) · ∑ σ∈Sk ∑ I∈Seq−(n:k) N(I)∏k i=1 M(I i|A) · k∏ i=1 paIi (cσ(i)) . (90)\nLemma 24 For any δw, δs > 0 such that A is δw-spread and δs-monotonic, for any A′ ≈ A, we have:\nP[C|A′] P[C|A] ≤ (1 + δw)\nk−1 · ( 1 + δw · (\n1 + δs 1 + δw\n)k−1 · (1 + η)2(k−2) · %(R) ) . (91)\nProof We get from the fact that A is δw-spread,∑ σ∈Sk ∑ I∈Seq−(n:k) p(σ, I,C|A′) ≤ (1 + δw)k−1 · ∑ σ∈Sk ∑ I∈Seq−(n:k) p(σ, I,C|A) , (92)\nand furthermore ineq. (90) yields:\nP[C|A′] P[C|A] =\n∑ σ∈Sk ∑ I∈Seq(n:k) p(σ, I,C|A′)∑\nσ∈Sk\n∑ I∈Seq(n:k) p(σ, I,C|A)\n≤\n (1 + δw)k−1 ·∑σ∈Sk∑I∈Seq−(n:k) p(σ, I,C|A)+∑ σ∈Sk ∑ I∈Seq+(n:k) p(σ, I,C|A′)  ∑ σ∈Sk ∑ I∈Seq(n:k) p(σ, I,C|A)\n≤ (1 + δw)k−1\n·\n ∑ σ∈Sk ∑ I∈Seq−(n:k) p(σ, I,C|A) +\nδw · (\n1+δs 1+δw )k−1 · (1 + η)2(k−2) · %(R) ·∑σ∈Sk∑I∈Seq−(n:k) p(σ, I,C|A′)  ∑ σ∈Sk ∑ I∈Seq(n:k) p(σ, I,C|A)\n= (1 + δw) k−1 · ( 1 + δw · ( 1 + δs 1 + δw )k−1 · (1 + η)2(k−2) · %(R) )\n· ∑ σ∈Sk ∑ I∈Seq−(n:k) p(σ, I,C|A)∑\nσ∈Sk ∑ I∈Seq(n:k) p(σ, I,C|A)︸ ︷︷ ︸\n≤1\n.\nThis ends the proof of Lemma 24.\nSince\n(1 + δw) k−1 · ( 1 + δw · ( 1 + δs 1 + δw )k−1 · (1 + η)2(k−2) · %(R) ) = (1 + δw) k−1 + (1 + η)2(k−2) · δw · (1 + δs)k−1 · %(R) ,\nand η ≤ 3 from Lemma 19, we get Theorem 9 with\nf(k) . = 42k−4 . (93)"
    }, {
      "heading" : "Proof of Theorem 10",
      "text" : "Assume that density D contains a L2 ball B2(0, R) of radiusR, centered without loss of generality in 0. Fix 0 < κ < m − 1. For any α ∈ (0, 1) and N ⊆ A with |N| ∈ {1, 2, ...,κ} .= [κ]∗, let N ⊕ α .= ∪x∈NB2(x,α · R) be the union of all small balls centered around each element of N, each of radius α ·R. An important quantity is\nq∗ .\n= min N⊆A,|N|∈[κ]∗ µ(B2(0, R)\\N ⊕ α) µ(B2(0, R))\n(94)\nthe minimal mass of B2(0, R)\\N ⊕ α relatively to B2(0, R) as measured using D. As depicted in Figure 7, q∗ is a minimal value of the probability to escape the neighborhoods of N ⊕ α when\nsampling points according to D in ball B2(0, R). If, for some α that shall depend upon the dimension d and κ, q∗ is large enough, then the spread of points drawn shall guarantee ”small” values for δw and δs.\nThis is formalized in the following Theorem, which assumes m = M = 1, i.e. the ball has uniform density. Theorem 10 is a direct consequence of this Theorem.\nTheorem 25 Suppose A ⊂ B2(0, R). For any δ ∈ (0, 1), if\nm ≥ 3 ( κ\nq∗δ2\n)2 , (95)\nthen there is probability ≥ 1 − δ over its sampling that A is δw-spread and δs-monotonic for the following values of δw, δs:\nδw = 1\nq∗(1− δ)(m− κ− 1)α2 , (96)\nδs = m m− κ · (\n2 min {\n1 4 , q∗(1− δ)\n} · α\n)2 − 1 . (97)\nProof We first prove the following Lemma.\nLemma 26 Suppose A ⊂ B2(0, R). Let q∗ be defined as in eq. (94). Then for any δ ∈ (0, 1), if m meets ineq. (95), then there is probability ≥ 1− δ that\n|(B2(0, R)\\N ⊕ α) ∩ (A\\N)| ≥ q∗(1− δ)(m− κ) ,∀N ⊆ A, |N| ∈ [κ]∗ . (98)\nProof Since we assume A ⊂ B2(0, R), Chernoff bounds imply that for any fixed N ⊆ A with |N| ∈ [κ]∗,\nPD [ |(B2(0, R)\\N ⊕ α) ∩ (A\\N)| |A\\N| ≤ q∗(1− δ) ] ≤ exp ( −δ2q∗ |A\\N| /2 ) . (99)\nNow, remark that κ∑ j=1 ( m j ) ≤ mκ ,∀m,κ ≥ 1 . (100)\nThis can be proven by induction, m being fixed: it trivially holds for κ = 1 and κ = 2, and furthermore\nκ∑ j=1 ( m j ) = κ−1∑ j=1 ( m j ) + ( m κ ) ≤ mκ−1 + m!\n(m− κ)!κ! , (101)\nby induction at rank κ− 1. To prove that the right-hand side of (101) is no more than mκ, we just have to remark that\nm! (m− κ)!κ!mκ−1 < m κ!\n≤ m− 1 , (102)\nas long as κ > 1 and m > 1. So, the property at rank κ − 1 for κ > 1 implies property at rank κ, which concludes the induction.\nSo, we have at most mκ choices for N, so relaxing the choice of N, we get PD [ ∃N ⊆ A, |N| = κ : |(B2(0, R)\\N ⊕ α) ∩AN||AN| ≤ q∗(1− δ) ]\n≤ mκ exp ( −δ\n2q∗(m− κ) 2\n) . (103)\nWe want to compute the minimal m such that the right-hand side is no more than δ, this being equivalent to\nδ2q∗m ≥ 2 log ( mκ\nδ\n) + κδ2q∗ ,\nwhich, since δ ∈ (0, 1), is ensured if δ2q∗m ≥ 2κ log (m δ ) + κδ2q∗ . (104)\nSuppose\nm = 3\n( κ\nq∗δ2\n)2 .\nSince we trivially have κ2/(q∗δ2)2 ≥ κδ2q∗ (κ ≥ 1, q∗ ∈ (0, 1), δ ∈ (0, 1)), it is sufficient to prove:\n2κ\nq∗δ2 ≥ 2 log 3 + 2 log\n( κ2\nq2∗δ 5\n) , (105)\nwhich, again observing that δ ∈ (0, 1), holds if we can prove\nκ q∗δ2 ≥ log 2 + 3 2 · log\n( κ\nq∗δ2\n) , (106)\nwhich is equivalent to showing x ≥ (3/2) log x + log 2 for x ≥ 1, which indeed holds (end of the proof of Lemma 26).\nThe consequence of Lemma 26 is the following: if A ⊂ B2(0, R) and m satisfies (95), then for any N ⊆ A with |N| = k − 1, and any B ⊆ A with |B| = |A| − 1,∑\na∈B\n‖a− NNN(a)‖22 ≥ q∗(1− δ)(m− κ− 1)α2 ·R2 , (107)\nand so from Definition 7 A is δs-spread for:\nδw = 1\nq∗(1− δ)(m− κ− 1)α2 . (108)\nNow, suppose we add a single point x∗ in N. If, for some fixed α∗ ∈ (0,α/2],\nx∗ 6∈ a⊕ α∗ ,∀a ∈ A , (109)\nthen because of (107),∑ a∈A ‖a− NNN∪{x∗}(a)‖22 ≥ (m− κ) ·min { α2∗, q∗(1− δ)α2 } ·R2 . (110)\nOtherwise, consider one a∗ for which x∗ ∈ a∗ ⊕ α∗. If we replace a∗ by x∗ in all N in which a∗ belongs to in Lemma 26, then because x∗ ⊕ α∗ ⊂ a∗ ⊕ α, it comes from Lemma 26:∑\na∈A\n‖a− NNN∪{x∗}(a)‖22 ≥ 1\n4 · (m− κ) · q∗(1− δ)α2 ·R2 . (111)\nWe thus get in all cases∑ a∈A ‖a− NNN∪{c(A)}(a)‖22 ≥ min { α2 4 ,α2∗, q∗(1− δ)α2 } (m− κ) · q∗(1− δ) ·R2 ,(112)\nwhere c(A) is the arithmetic average computed according to the definition of δs-monotonicity, of any A ⊆ A\\N. Since N ⊆ A ⊂ B2(0, R), we have ∑ a∈A ‖a− NNN(a)‖22 ≤ 4mR2, and so∑\na∈A\n‖a− NNN(a)‖22 ≤ 4m min { α2\n4 ,α2∗, q∗(1− δ)α2 } (m− κ) · q∗(1− δ) · ∑ a∈A ‖a− NNN∪{c(A)}(a)‖22 ,(113)\nimplying from Definition 8 that δs-monotonicity holds with:\nδs = m m− κ · 4 min { α2\n4 ,α2∗, q∗(1− δ)α2 } · q∗(1− δ) − 1 . (114)\nThe statement of the Theorem follows with α∗ = α/2 (end of the proof of Theorem 25).\nWe finish the proof of Theorem 10. We have\nq∗ ≥ 1− καd , (115)\nwhere the lowerbound corresponds to the case where all neighborhoods in N ⊕ α are distinct and included in B2(0, R). So we have, for any fixed choice of α ∈ (0, 1),\nδw ≤ 1\nα2 · (1− καd)(1− δ)(m− κ− 1) . (116)\nTo minimize this upperbound, we pick α to maximize α2 · (1 − καd) with α ∈ (0, 1), which is easily achieved picking\nα =\n( 1\nκ(d+ 1)\n) 1 d\n, (117)\nand yields\nδw ≤ ( 1 + 1\nd\n) · 1\n(κ(d+ 1)) 2 d (1− δ)(m− κ− 1) ≤ ( 1 + 1\nd ) · 1 κ 2 d (1− δ)(m− κ− 1) . (118)\nBut we have for this choice, 1− καd = d/(d+ 1) ≥ 1/2, so as long as\nδ < 1/2 , (119)\nwe shall have q∗(1− δ) > 1/4 and so we shall have\nδs + 1 = 64 · m m− κ · 1 α2\n≤ 64 · m m− κ · 1 κ 2 d . (120)\nWe now go back to ineq. (14), which reads:\nP[C|A′] P[C|A] ≤ %1 + %2 , (121)\nwith\n%1 . = (1 + δw) k−1 , (122) %2 .\n= f(k) · δw · (1 + δs)k−1 · %(R) . (123) We upperbound separately both terms.\nLemma 27 Suppose ineqs (119) and (15) are met. Then\n%1 ≤ 1 + 4\nm 1 4 + 1 d+1\n. (124)\nProof Since d ≥ 1 and δ < 1/2, we get from ineq. (118) (using κ = k)\n(1 + δw) k−1 ≤ ( 1 + ( 1 + 1\nd ) · 1 k 2 d (1− δ)(m− k − 1) )k−1\n≤ ( 1 + 2\nk 2 d (1− δ)(m− k − 1)\n)k−1\n≤ ( 1 + 4\nk 2 d (m− k − 1)\n)k−1 . (125)\nLet h(k) be the right-hand side of ineq. (125). h(1) trivially meets ineq. (124). When k ≥ 2, h decreases until k = 2(m− 1)/(d + 2) and then increases. We thus just need to check ineq. (124) for k = 2 and k = √ m from ineq. (15). We get h(2) = 1 + 4/(41/d(m − 3)). For ineq. (124) to be satisfied, we need to have 41/d(m − 3) ≥ m 14 + 1d+1 , which holds if m ≥ 3 + m3/4 (d ≥ 1), that is, m ≥ 8. But since ineqs (119) and (15) are satisfied, we have m ≥ 16k2/δ2 ≥ 64k2 ≥ 64, and so h(2) satisfies ineq. (124).\nThere remains to check ineq. (124) for k = √ m. We have\nh( √ m) = ( 1 +\n4\nm 1 d (m−√m− 1)\n)√m−1\n≤ ( 1 + 4\nm 1 d (m−√m)\n)√m\n≤ ( 1 + 2\n√ m ·m 14 + 1d\n)√m , (126)\nsince any m ≥ 64, we have m−√m ≥ 2m3/4. To conclude, ineq (126) yields\nh( √ m) ≤ ( 1 +\n2 √ m ·m 14 + 1d )√m ≤ exp ( 2\nm 1 4 + 1 d ) ≤ 1 + 4\nm 1 4 + 1 d\n. (127)\nThe penultimate ineq. comes from 1 + x ≤ expx, and the last one comes from the fact that exp(2x) ≤ 1 + 4x for x ≤ 1. Since m 14 + 1d ≥ m 14 + 1d+1 , we obtain the statement of the Lemma for h( √ m). This concludes the proof of Lemma 27.\nLemma 28 Suppose ineqs (119) and (15) are met. Then\n%2 ≤ ( 64\nk 2 d\n)k · %(2R)\nm . (128)\nProof We fix κ = k, use f(k) = 42k−4 (eq. 93), so we get\n%2 = 4 2k−2 · ( 1 + 1\nd ) · 1 k 2 d (1− δ)(m− k − 1) · ( 64 · m m− k · 1 k 2 d )k−1 · %(2R)\n≤ 2 · 64k−1 · ( 1 + 1\nd ) · 1 k 2k d (m− k − 1) · ( 1 + k m− k )k−1 · %(2R) (129)\n≤ 4 · 1 (m− k − 1) ·\n( 1 +\nk\nm− k )k−1 ︸ ︷︷ ︸\n. =%3\n·64k−1 · 1 k 2k d · %(2R) , (130)\nusing the fact that δ < 1/2 and d ≥ 1. Now, we also have( 1 + k\nm− k\n)k−1 ≤ exp ( k2\nm− k\n) (131)\n≤ e , (132)\nas long as k ≤ (1/16) · √m, and furthermore, since m ≥ 64 (see the proof of Lemma 27), we also have 1/(m− k − 1) ≤ 5/m. We thus obtain\n%3 ≤ 20e\nm\n≤ 64 m , (133)\nwhich yields\n%2 ≤ ( 64\nk 2 d\n)k · %(2R)\nm , (134)\nas claimed.\nPutting altogether Lemmata 27 and 28, we get:\nP[C|A′] P[C|A] ≤ 1 +\n4\nm 1 4 + 1 d+1\n+\n( 64\nk 2 d\n)k · %(2R)\nm , (135)\nas claimed. There remains to check that, with our choice of α, the constraint on m in (95) is satisfied if\nm ≥ 12k 2\nδ4 (136)\nsince q∗ ≥ d/(d+ 1). We obtain the sufficient constraint on k:\nk ≤ δ 2 4 · √m , (137)\nwhich proves Theorem 10 when m = M = 1.\nWhen the density do not satisfy m = M = 1 we just have to remark that the lowerbound on q∗ is now\nq∗ ≤ εm εM · (1− καd) . (138)\nIneq. (118) becomes δw ≤ εM εm · ( 1 + 1 d ) · 1 κ 2 d (1− δ)(m− κ− 1) , (139)\nineq. (120) becomes\nδs + 1 ≤ εM εm · 64 · m m− κ · 1 κ 2 d . (140)\nSo, the only difference with the m = M = 1 is the ratio εM/εm (≥ 1) which multiplies all quantities of interest, and yields, in lieu of ineq. (135),\nP[C|A′] P[C|A] ≤ 1 + ( εM εm )k · (\n4\nm 1 4 + 1 d+1\n+\n( 64\nk 2 d\n)k · %(2R)\nm\n) , (141)\nwhich is the statement of Theorem 10."
    }, {
      "heading" : "Proof of Theorem 12",
      "text" : "When p(µa,θa) is a product of Laplace distributions Lap(b) (b being the scale parameter of the distribution (Dwork & Roth, 2014)), condition in ineq. (13) becomes:\np(µa′ ,θa′ )(x)\np(µa,θa)(x) ≤ exp (‖a− a′‖1 b ) = exp (√ 2‖a− a′‖1\nσ1\n)\n≤ exp ( 2 √ 2R\nσ1\n) , ∀a,a′ ∈ A, ∀x ∈ Ω , (142)\nassuming A ⊂ B1(0, R). Let us fix %(R) .= exp ( 2 √ 2R/σ1 ) . Since B1(0, R) ⊂ B2(0, R) (the L2 ball), we now want (1+δw)k−1 +f(k) ·δw · (1 + δs)k−1 ·%(R) = exp( ). Solving for σ1 yields:\nσ1 = 2 √ 2R log (\nexp( )−(1+δw)k−1 f(k)·δw·(1+δs)k−1 ) , (143) as claimed. The proof that k-variates++ meets ineq. (7) with\nΦ = Φ1 . = 8 · ( φopt + mR2\ñ2\n) (144)\ncomes from a direct application of Theorem 2, with\nη = 0 ,\nφbias = φopt , φvar = m · ( 2 √ 2R\ñ\n)2 .\nThe statements for σ2 and Φ2 are direct applications of the Laplace mechanism properties (Dwork & Roth, 2014; Dwork et al., 2006)."
    }, {
      "heading" : "Extension to non-metric spaces",
      "text" : "Since its inception, the k-means++ seeding technique has been successfully adapted to various distortion measures D(·‖·) to handle non-Euclidean features (Jegelka et al., 2009; Nock et al., 2008, 2016). Similarly, our extended seeding technique can be adapted to these scenarii: this boils down to putting the distortion as a free parameter of the algorithm, replacing Dt(a) (eq. (1)) by Dt(a) . = mina′∈PD(a‖a′). For example, by noticing that the squared Euclidean distance is merely an example of Bregman divergences (the well-known canonical divergences in information geometry of dually flat spaces), k-variates++ can be been extended to that family of dissimilarities (Nock et al., 2008). But more interesting examples now appear, that build on constraints that distortions have to satisfy for certain problems, like the invariance to rotations of the coordinate space. This is all the more challenging in practice for clustering since sometimes no-closed form solution are available for some of these divergences. Because it bypasses the construction of the population minimisers, k-variates++ offers an elegant solution to the problem. Such hard distortions include the skew Jeffreys α-centroids (Nock et al., 2016). This also include the recent class of total Bregman/Jensen divergences that are examples of conformal divergences (Nielsen & Nock, 2015; Nock et al., 2016). We give an example of the extension of k-variates++to the total Jensen divergence, to show that k-variates++ can approximate the optimal clustering even without closed form solutions for the population minimisers (Nielsen & Nock, 2015). For any convex function ϕ : Rd → R and α ∈ (0, 1), the skew Jensen divergence is\nJα(a,a ′) . = αϕ(a) + (1− α)ϕ(a′)− ϕ(αa+ (1− α)a′) , (145)\nand the total Jensen divergence is\ntJα(a,a ′) . = 1√ 1 + U2 · Jα(a,a′) , (146)\nwhere U .= (ϕ(a) − ϕ(a′))/‖a − a′‖2. There is no closed form solution for the population minimiser of tJα, yet we can prove the following Theorem, which builds upon Theorem 3 in (Nielsen & Nock, 2015).\nTheorem 29 In k-variates++, replace Dt(a) (eq. (1)) by Dt(a) . = mina′∈P tJα(a,a ′) ans suppose for simplicity that probe functions are identity: ℘t = Id,∀t. Denote φopt the optimal noisefree potential of the clustering problem using tJα as distortion measure. Then there exists a constant ω > 0 such that for any choice of densities pµ.,θ. , the expected tJα-potential φ of kvariates++ satisfies:\nE[φ(A;C)] ≤ ω · log k · (6φopt + 2φbias + 2φvar) , (147)\nwhere φvar is defined in Theorem 2 and φbias is defined in eq. (4)."
    }, {
      "heading" : "9 Appendix on Experiments",
      "text" : "Experiments on Theorem 12 and the sublinear noise regime ↪→ comments on ̃ An important parameter of Theorem 12 is ̃, which replaces in the computation of the noise standard deviation in σ1: the larger it is compared to , the less noise we can put while still ensuring P[C|A′]/P[C|A] ≤ exp in Definition 11. Recall its formula:\ñ . = log ( exp( )− (1 + δw)k−1 f(k) · δw · (1 + δs)k−1 ) . (148)\nThe experimental setting is the following one: we repeatedly sample clusters that are uniform in a subset of the domain (with limited, random size), taken to be a d-dimensional hyperrectangle of randomly chosen edge lengths. Each cluster contains a randomly picked number of points between\n1 and 1000. After each cluster is picked, we updated an estimation of δw and δs:\n• we compute δw by randomly picking B and N for a total number of nest iterations, with nest = 5000;\n• we compute δs by randomly picking N for a total number of nest iterations. Instead of computing A then x, we opt for the fast proxy which consists in replacing c(A) by a random data point, thus without making the N-packed test. This should reasonably overestimate δs and thus slightly loosen our approximation bounds.\nFigure 8 shows the dataset obtained for d = 10 at the end of the process. Predictably, the distribution on the whole space looks like a highly non-uniform cover by locally uniform clusters. Tables 4, 5 and 6 display results obtained for three different values of and three different values for the couple (d, k). To test the large sample regime intuition and the fact that the the noise dependence grows sublinearly in m, we have regressed in each plot ̃ as a function of m for\ñ(m) = a+ b logm . (149)\nThe plots obtained confirm a good approximation of this intuition, but they also display some more good news. The smaller , the larger can be ̃ relatively to , by order of magnitudes if is small. Hence, despite the fact that we evetually overestimate δs, we still get large ̃. Furthermore, if k is small, this ”large sample” regime in which ̃ > actually happens for quite small values of m.\nAlso, one may remark that the curves all look like an approximate translation of the same curve. This is not surprising, since we can reformulate\ñ = + log ( 1− U ) + g(m) , (150)\nwhene U .= (1 + δw)k−1 and g do not depend on . It happens that δw quickly decreases to very small values (bringing also a separate empirical validation of its behavior as computed in ineq. (139) in the proof of Theorem 10). Hence, we rapidly get for small m some ̃ that looks like\ñ ≈ + log ( 1− 1 + o(1) ) + g(m)\n≈ h( ) + g(m) , (151)\nwhich may explain what is observed experimentally. We can sumarise the global picture for ̃ vs by saying that it becomes more and more in favor of ̃ as data size (d or m) increase, but become less in favor of ̃ as the number of clusters k increases (predictably).\n↪→ comments on δw and δs Table 7 presents the estimated values of δw and δs for the settings of Tables 4, 5 and 6. We wanted to test the intuition as to whether, for m sufficiently large, it would hold that δw = O(1/m) while δs = O(1). The essential part is on δw, since such a behaviour would be sufficient for the sublinear growth of the noise dependence. One can check that such behaviours are indeed observed, and more: δw converges very rapidly to zero, at least for all settings in which\nwe have tested data generation. Another quite good news, is that δs seems indeed to be θ(1), but for an actual value which is also not large, so the denominator of eq. (148) is actually driven by f(k), even when, as we already said, we may have a tendency to overestimate δs with our randomized procedure.\nExperiments with Dk-means++, k-means++ and k-means‖ ? Experiments on synthetic data We have generated a set of m ≈20 000 points using the same kind of clusters as in the experiments related to Theorem 12: we add ”true” clusters until the total number of points exceeds 20 000. To simulate the spread of data among peers (Forgy nodes) and evaluate the influence of the spread of Forgy nodes (φFs ) for Dk-means++, we have devised the following protocol: let us name ”true” clusters the hyperrectangle clusters used to build the dataset. Each true cluster corresponds to the data held by a peer. Then, for some p ∈ [0, 100] (%), each point in each true cluster moves into another cluster, with probability p. The choice of the target cluster is made uniformly at random. Thus, as p increases, we get a clustering problem in which the data held by peers is more and more spread, and for which the spread of Forgy nodes\nφFs increases. Figure 9 presents a typical example of the spread for p = 50%. Notice that in this case many Forgy nodes have data spreading through a much larger domain than the initial, true clusters. Figure 10 displays that this happens indeed, as φFs is multiplied by a factor exceeding 20 (compared to φFs at p = 0) for the largest values of p.\nWe have compared Dk-means++ to k-means++ and k-means‖ (Bahmani et al., 2012). In the case of that latter algorithm, we follow the paper’s statements and pick the number of outer iterations to be dlog φ1e, where φ1 is the potential for one Forgy-chosen center. We also pick ` = 2k, considering that it is a value which gives some of the best experimental results in (Bahmani et al., 2012). Finally, we recluster the points at the end of the algorithm using k-means++. For each algorithm H ∈ {k-means++, k-means‖}, we run it on the complete dataset and its results are averaged over 10 runs. We run Dk-means++ for each p ∈ {0%, 1%, ..., 50%}. More precisely, for each p, we average the results of Dk-means++ over 10 runs. We use as metric the relative increase in the potential of Dk-means++ compared to H:\nρφ(H) . = φ(Dk-means++)− φ(H)\nφ(H) · 100 . (152)\nthat we plot as a function of φFs , or surface plot as a function of (k, p). The intuition for the former plot is that the larger φFs , the larger should be this ratio, since the data held by peers spreads across the domain and each peer is constrained to pick its centers with uniform seeding.\n↪→ Dk-means++ vs k-means++ Figure 8 presents results for ρφ(k-means++) = f(φFs ) obtained for various k. First, the intuition is indeed confirmed for k = 8, 9, 10, but an interesting phenomenon appears for k = 5: Dk-means++ almost consistently beats k-means++. The decrease in the average potential ranges up to 3%. Furthermore, this happens even for large values of φFs . Finally, for all but one value of k, there exists spread values for which Dk-means++ beats k-means++.\nThe surface plot in Figure 3 displays that superior performances of Dk-means++ are probably not random. One possible explanation to this phenomenon relies on the expression of φbias given in the proof of Theorem 4 (eq. (43)), recalled here:\nφbias . = ∑ a∈A ‖µa − copt(a)‖22\n= ∑ i∈[n] ∑ a∈Ai ‖c(Ai)− copt(a)‖22 .\n(153)\nRecall that φbias can be < φopt, and it can even be zero, in which case Theorem 2 says that the approximation bound may actually be better than that of k-means++ in (Arthur & Vassilvitskii, 2007) (furthermore, η = 0 for Dk-means++). Hence, what happens is pobably that in several cases, there exists a union of peers data (the number of peers is larger than k) that gives a at least reasonably good approximation of the global optimum. In all our experiments indeed, we obtained a number of peers larger than 30.\n↪→ Dk-means++ vs k-means‖ Figure 3 appear to display performances for Dk-means++ that are even more in favor of Dk-means++, compared to k-variates++. Figure 9 presents results for ρφ(k-means‖) = f(φFs ) obtained for various k. The fact that each of them is a vertical translation of a picture in Figure 8 comes from the fact that the results of k-means‖ and k-means++ do not depend on the spread of the neighbors φFs .\n? Experiments on real world data We consider the EuropeDiff dataset5 (Dataset characteristics provided in Table 10). Figures 11 and 12 give the results for the equivalent settings of the experimental data. To simulate N peers with real data, reasonably spread geographically, we have\n5http://cs.joensuu.fi/sipu/datasets/\nsampled N points (”peer centers”) with k-means++ seeding in data and then aggregated for each peer the subset of data in the corresponding Voronoi 1-NN cell. We then simulate the spread for parameter p as in the simulated data. Figures 11 and 12 globally display (and confirm) the same trends as for the simulated data. They, however, clearly emphasize this time that the spread of Forgy nodes φFs is one key parameter that drives the performances of Dk-means++. Notice also that Dk-means++ remains on this dataset competitive up to p ≥ 30%, which means that it remains competitive when a significant proportion of peers’ data is scattered without any constraint.\nTo further address the way the spread of Forgy nodes affects results, we have used another real world data with highly non-uniform distribution, Mopsi-Finland locations5 (m = 13467, d = 2). We have sampled peers using two different schemes for the peer centers: k-means++ and Forgy. In this latter initialisation, we just pick peer centers at random. In the former k-means++ initialisation, the initial peer centers are much more evenly geographically spread before we complete the peers data with the closest points. They remain more spread after the p% uniform displacement of data between peers, as shown on the top plots of Figure 13. What is interesting about this data is that it displays that if peers’ data are indeed geographically located, then Dk-means++ is competitive up to quite reasonable values of p ≤ 20% (depending on k). That, is Dk-means++ works well\nwhen each peer aggregates 80 % data which is reasonably ”localized in the domain” and 20 % data which can be located everywhere in the domain.\nExperiments with k-variates++ and GUPT Among the state-of-the-art approaches against which we could compare k-variates++, there are two major contenders, PINQ (McSherry, 2010) and GUPT (Mohan et al., 2012). Even when PINQ is a broad system, we switched our preferences to GUPT for the following reasons. The performance of k-means based on PINQ relies on two principal factors: the initialisation (like in the non differentially private version) and the number of iterations. To compete against heavily tuned specific applications, like k-variates++, this scheme requires substantial work for its optimisation. For example, if one allocates part of the privacy budget to release a differential private initialisation, the noise has to be proportional to the domain width, which would release poor centers. Also, generating points uniformly at random from the domain, to obtain data-independent initial centers, yields to a poor initialisation. Finally, the number of iterations has to be tuned very carefully: if too small, the algorithm keeps poor solutions; if too large, the number of iteration increase the added noise for privacy and harms PINQ’s final accuracy. We thus chose GUPT. k-means implemented in the GUPT proceeds the following way: the dataset is cut in a certain number of blocks ` (following (Mohan et al., 2012), we fix ` = m0.4 in our experiments), the usual k-means algorithm is performed on each block. Before releasing the final centroids, results are aggregated and a noise is applied. Finally, we also compare against the vanilla approach of Forgy Initialisation using the Laplace mechanism. The noise rate (i.e., standard deviation) is then proportional to ∝ kR/ (we do not run k-means afterwards, hence the privacy budget remains “small”). In comparison, GUPT adds noise ∝ kR/(` ) at the end of this aggregation process. Note that we disregard the fact that our data are multidimensional, which should require a finer-grained tuning of `, and choose to rely on the ` = m0.4 suggestion from (Mohan et al., 2012).\n↪→ Comparison on real world domains Our domains consist of 3 real-world datasets5. Lifesci contains the value of the top 10 principal components for a chemistry or biology experiment. Image\nis a 3D dataset with RGB vectors, and finally EuropeDiff is the differential coordinates of Europe map.\nTable 10 presents the extensinve results obtained, that are averaged in the paper’s body. We have fixed = 1 in the differentially privacy parameters. The column ̃ (eq. (18)) provides the differential privacy parameter which is equivalent from the protection standpoint, but exploits the computation of δw, δs (which we compute exactly, and not in a randomized way like in the experiments on Theorem 12 above) and ineq. (80). Therefore, each time ̃ > (=1 in our applications), it means that our analysis brings a sizeable advantage over “raw protection” by Laplace mechanism (in our application we chose for pµa,θa a Laplace distribution). R is computed from the data by an upperbound of the smallest enclosing ball radius. The results display several interesting patterns. First, the largest the domain, the better we compare with respect to the other algorithms. On EuropeDiff for example, we often have the ratio of the potentials φ(GUPT)/φ(k-variates++) of the order of dozens. Also, the performances of k-variates++ degrade if k increases, which is again consistent with the “good” regime of Theorem 10.\n↪→ Comparison on synthetic domains The synthetic datasets contain points uniformly sampled on a unit d-ball, in low dimension d = 2 and higher dimension d = 15 , we generated datasets with size in {105, 106}.\nvs F, d\n= 2\nvs F, d\n= 15\nvs G\nU PT\n,d =\n2 vs\nG U\nPT ,d\n= 15\nFi gu\nre 14\n: k\n-v ar\nia te\ns+ +\nvs Fo\nrg y\nin iti\nal is\nat io\nn di\nff er\nen tia\nlly pr\niv at\ne an\nd G\nU PT\n.W e\nus e\nra tio ρ ′ φ\nbe tw\nee n\nth e\npo te\nnt ia\nlo f\nth e\nco nt\nen de r in (F -D P, G U PT ) ov er th e po te nt ia lo f k -v ar ia te s+ + (p ot en tia ls ar e av er ag ed 30 tim es ). T he m or e re d, th e be tte r is k -v ar ia te s+ + w ith re sp ec tt o th e co nt en de r. G re y va lu es in di ca te le ss po si tiv e ou tc om es fo rk -v ar ia te s+ +; w hi te va lu es in di ca te th at k -v ar ia te s+ + do es no t m an ag e to fin d an ′ la rg er th an , an d th us do es no tm an ag e to pu ts m al le rn oi se ra te th an in th e L ap la ce m ec ha ni sm ."
    } ],
    "references" : [ {
      "title" : "Bregman divergences and triangle inequality",
      "author" : [ "S. Acharyya", "A. Banerjee", "D. Boley" ],
      "venue" : "In Proc. of the 13 SIAM International Conference on Data Mining,",
      "citeRegEx" : "Acharyya et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Acharyya et al\\.",
      "year" : 2013
    }, {
      "title" : "Streamkm++: A clustering algorithms for data streams",
      "author" : [ "Ackermann", "M.-R", "C. Lammersen", "M. Märtens", "C. Raupach", "C. Sohler", "K. Swierkot" ],
      "venue" : "ALENEX,",
      "citeRegEx" : "Ackermann et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ackermann et al\\.",
      "year" : 2010
    }, {
      "title" : "Streaming k-means approximation",
      "author" : [ "N. Ailon", "R. Jaiswal", "C. Monteleoni" ],
      "venue" : "In NIPS*22,",
      "citeRegEx" : "Ailon et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ailon et al\\.",
      "year" : 2009
    }, {
      "title" : "Scalable k-means++",
      "author" : [ "B. Bahmani", "B. Moseley", "A. Vattani", "R. Kumar", "S. Vassilvitskii" ],
      "venue" : "VLDB, pp",
      "citeRegEx" : "Bahmani et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bahmani et al\\.",
      "year" : 2012
    }, {
      "title" : "Distributed k-means and k-median clustering on general communication topologies",
      "author" : [ "Balcan", "M.-F", "S. Ehrlich", "Y. Liang" ],
      "venue" : "In NIPS*26,",
      "citeRegEx" : "Balcan et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 1995
    }, {
      "title" : "Differentially private empirical risk",
      "author" : [ "K. Chaudhuri", "C. Monteleoni", "Sarwate", "A.-D" ],
      "venue" : "minimization. JMLR,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2011
    }, {
      "title" : "Adaptive noisy clustering",
      "author" : [ "M. Chichignoud", "S. Lousteau" ],
      "venue" : "IEEE Trans. IT,",
      "citeRegEx" : "Chichignoud and Lousteau,? \\Q2014\\E",
      "shortCiteRegEx" : "Chichignoud and Lousteau",
      "year" : 2014
    }, {
      "title" : "The algorithmic foudations of differential privacy",
      "author" : [ "C. Dwork", "A. Roth" ],
      "venue" : "Found. & Trends in TCS,",
      "citeRegEx" : "Dwork and Roth,? \\Q2014\\E",
      "shortCiteRegEx" : "Dwork and Roth",
      "year" : 2014
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "C. Dwork", "F. McSherry", "K. Nissim", "A. Smith" ],
      "venue" : "TCC,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2006
    }, {
      "title" : "The noisy power method: a meta algorithm with applications",
      "author" : [ "M. Hardt", "E. Price" ],
      "venue" : "In NIPS*27,",
      "citeRegEx" : "Hardt and Price,? \\Q2014\\E",
      "shortCiteRegEx" : "Hardt and Price",
      "year" : 2014
    }, {
      "title" : "Composable core-sets for diversity and coverage maximization",
      "author" : [ "P. Indyk", "S. Mahabadi", "M. Mahdian", "Mirrokni", "V.-S" ],
      "venue" : "ACM PODS,",
      "citeRegEx" : "Indyk et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Indyk et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient algorithms for online decision problems",
      "author" : [ "A. Kalai", "S. Vempala" ],
      "venue" : "J. Comp. Syst. Sc.,",
      "citeRegEx" : "Kalai and Vempala,? \\Q2005\\E",
      "shortCiteRegEx" : "Kalai and Vempala",
      "year" : 2005
    }, {
      "title" : "An algorithm for online k-means clustering",
      "author" : [ "E. Liberty", "R. Sriharsha", "M. Sviridenko" ],
      "venue" : "CoRR, abs/1412.5721,",
      "citeRegEx" : "Liberty et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Liberty et al\\.",
      "year" : 2014
    }, {
      "title" : "Shape retrieval using hierarchical total bregman soft clustering",
      "author" : [ "M. Liu", "Vemuri", "B.-C", "S. .i Amari", "F. Nielsen" ],
      "venue" : "IEEE Trans. PAMI,",
      "citeRegEx" : "Liu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2012
    }, {
      "title" : "Privacy integrated queries: an extensible platform for privacy-preserving data analysis",
      "author" : [ "F. McSherry" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "McSherry,? \\Q2010\\E",
      "shortCiteRegEx" : "McSherry",
      "year" : 2010
    }, {
      "title" : "GUPT: privacy preserving data analysis made easy",
      "author" : [ "P. Mohan", "A. Thakurta", "E. Shi", "D. Song", "Culler", "D.-E" ],
      "venue" : "ACM SIGMOD,",
      "citeRegEx" : "Mohan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mohan et al\\.",
      "year" : 2012
    }, {
      "title" : "Smooth sensitivity and sampling in private data analysis",
      "author" : [ "K. Nissim", "S. Raskhodnikova", "A. Smith" ],
      "venue" : "ACM STOC, pp",
      "citeRegEx" : "Nissim et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Nissim et al\\.",
      "year" : 2007
    }, {
      "title" : "Mixed Bregman clustering with approximation guarantees",
      "author" : [ "R. Nock", "P. Luosto", "J. Kivinen" ],
      "venue" : "ECML, pp",
      "citeRegEx" : "Nock et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Nock et al\\.",
      "year" : 2008
    }, {
      "title" : "On conformal divergences and their population minimizers",
      "author" : [ "R. Nock", "F. Nielsen", "Amari", "S.-I" ],
      "venue" : "IEEE Trans. IT,",
      "citeRegEx" : "Nock et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nock et al\\.",
      "year" : 2016
    }, {
      "title" : "Fast and accurate k-means for large datasets",
      "author" : [ "M. Shindler", "A. Wong", "A. Meyerson" ],
      "venue" : "In NIPS*24,",
      "citeRegEx" : "Shindler et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shindler et al\\.",
      "year" : 2011
    }, {
      "title" : "Clustering stability: an overview",
      "author" : [ "U. von Luxburg" ],
      "venue" : "Found. & Trends in ML,",
      "citeRegEx" : "Luxburg,? \\Q2010\\E",
      "shortCiteRegEx" : "Luxburg",
      "year" : 2010
    }, {
      "title" : "Differentially private subspace clustering",
      "author" : [ "Y. Wang", "Wang", "Y.-X", "A. Singh" ],
      "venue" : "In NIPS*28,",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "2016). We give an example of the extension of k-variates++to the total",
      "author" : [ "Nock" ],
      "venue" : null,
      "citeRegEx" : "2015 and Nock,? \\Q2016\\E",
      "shortCiteRegEx" : "2015 and Nock",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : ", a centroid or a population minimizer (Nock et al., 2016).",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "The k-means++ non-uniform seeding approach has also been utilized in more complex settings, including tensor clustering, distributed, data stream, on-line and parallel clustering, clustering with non-metric distortions and even clustering with distortions not allowing population minimizers in closed form (Ailon et al., 2009; Balcan et al., 2013; Jegelka et al., 2009; Liberty et al., 2014; Nock et al., 2008; Nielsen & Nock, 2015).",
      "startOffset" : 306,
      "endOffset" : 432
    }, {
      "referenceID" : 12,
      "context" : "The k-means++ non-uniform seeding approach has also been utilized in more complex settings, including tensor clustering, distributed, data stream, on-line and parallel clustering, clustering with non-metric distortions and even clustering with distortions not allowing population minimizers in closed form (Ailon et al., 2009; Balcan et al., 2013; Jegelka et al., 2009; Liberty et al., 2014; Nock et al., 2008; Nielsen & Nock, 2015).",
      "startOffset" : 306,
      "endOffset" : 432
    }, {
      "referenceID" : 17,
      "context" : "The k-means++ non-uniform seeding approach has also been utilized in more complex settings, including tensor clustering, distributed, data stream, on-line and parallel clustering, clustering with non-metric distortions and even clustering with distortions not allowing population minimizers in closed form (Ailon et al., 2009; Balcan et al., 2013; Jegelka et al., 2009; Liberty et al., 2014; Nock et al., 2008; Nielsen & Nock, 2015).",
      "startOffset" : 306,
      "endOffset" : 432
    }, {
      "referenceID" : 16,
      "context" : ", there is limited prior work in a differentially private setting (Nissim et al., 2007; Wang et al., 2015).",
      "startOffset" : 66,
      "endOffset" : 106
    }, {
      "referenceID" : 21,
      "context" : ", there is limited prior work in a differentially private setting (Nissim et al., 2007; Wang et al., 2015).",
      "startOffset" : 66,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "We use it directly in a differential privacy setting, addressing a conjecture of (Nissim et al., 2007) with weaker assumptions.",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "This simple reduction technique allows us to analyze lightweight algorithms that compare favorably to the state of the art in the related domains (Ailon et al., 2009; Balcan et al., 2013; Liberty et al., 2014), from the approximation, assumptions and / or complexity aspects.",
      "startOffset" : 146,
      "endOffset" : 209
    }, {
      "referenceID" : 12,
      "context" : "This simple reduction technique allows us to analyze lightweight algorithms that compare favorably to the state of the art in the related domains (Ailon et al., 2009; Balcan et al., 2013; Liberty et al., 2014), from the approximation, assumptions and / or complexity aspects.",
      "startOffset" : 146,
      "endOffset" : 209
    }, {
      "referenceID" : 18,
      "context" : "2 k-variates++ We consider the hard clustering problem (Banerjee et al., 2005; Nock et al., 2016): given set A ⊂ R and integer k > 0, find centers C ⊂ R which minimizes the L2 potential to the centers (here, c(a) .",
      "startOffset" : 55,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "3 Reductions from k-variates++ Despite tremendous advantages, k-means++ has a serious downside: it is difficult to parallelize, distribute or stream it under relevant communication, space, privacy and/or time resource constraints (Bahmani et al., 2012).",
      "startOffset" : 230,
      "endOffset" : 252
    }, {
      "referenceID" : 1,
      "context" : "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).",
      "startOffset" : 161,
      "endOffset" : 293
    }, {
      "referenceID" : 2,
      "context" : "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).",
      "startOffset" : 161,
      "endOffset" : 293
    }, {
      "referenceID" : 3,
      "context" : "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).",
      "startOffset" : 161,
      "endOffset" : 293
    }, {
      "referenceID" : 12,
      "context" : "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).",
      "startOffset" : 161,
      "endOffset" : 293
    }, {
      "referenceID" : 19,
      "context" : "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).",
      "startOffset" : 161,
      "endOffset" : 293
    }, {
      "referenceID" : 3,
      "context" : "Property Them Us (1) (Bahmani et al., 2012) Communication complexity O(n` · log φ1) (expected) O(nk) (2) (Bahmani et al.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : ", 2012) Communication complexity O(n` · log φ1) (expected) O(nk) (2) (Bahmani et al., 2012) # data to compute one center m ≤ maxi∈[n](m/mi) (3) (Bahmani et al.",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 3,
      "context" : ", 2012) # data to compute one center m ≤ maxi∈[n](m/mi) (3) (Bahmani et al., 2012) Data points shared O(` · log φ1) (expected) k (4) (Bahmani et al.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : ", 2012) Data points shared O(` · log φ1) (expected) k (4) (Bahmani et al., 2012) Approximation bound O((log k) · φopt) (2 + log k) · ( 10φopt + 6φ F s )",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "(i) (Ailon et al., 2009) Time complexity (outer loop) — identical — (ii) (Ailon et al.",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : ", 2009) Time complexity (outer loop) — identical — (ii) (Ailon et al., 2009) Approximation bound (2 + log k)(1 + η) · 32φopt (2 + log k) · ((8 + 4η)φopt + 2φs ) (a) (Liberty et al.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : ", 2009) Approximation bound (2 + log k)(1 + η) · 32φopt (2 + log k) · ((8 + 4η)φopt + 2φs ) (a) (Liberty et al., 2014) Knowledge required Lowerbound φ∗ ≤ φopt None (b) (Liberty et al.",
      "startOffset" : 96,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : ", 2014) Knowledge required Lowerbound φ∗ ≤ φopt None (b) (Liberty et al., 2014) Approximation bound O(logm · φopt) (2 + log k) · ( 4 + (32/ς) ) φopt (A) (Nissim et al.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 16,
      "context" : ", 2014) Approximation bound O(logm · φopt) (2 + log k) · ( 4 + (32/ς) ) φopt (A) (Nissim et al., 2007) Knowledge required λ(φopt) None (B) (Nissim et al.",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : ", 2007) Knowledge required λ(φopt) None (B) (Nissim et al., 2007) Noise variance (σ) O(λkR/ ) O(R/( + logm)) (C) (Nissim et al.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : ", 2007) Noise variance (σ) O(λkR/ ) O(R/( + logm)) (C) (Nissim et al., 2007) Approximation bound O(φopt +mλkR/ ) O(log k(φopt +mR/( + logm))) (α) (Wang et al.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 21,
      "context" : ", 2007) Approximation bound O(φopt +mλkR/ ) O(log k(φopt +mR/( + logm))) (α) (Wang et al., 2015) Assumptions on φopt Several (separability, size of clusters, etc.",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 21,
      "context" : ") None (β) (Wang et al., 2015) Approximation bound O(φopt + km log(m)R/ ) O(log k(φopt +mR/( + logm)))",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "φ1 is the expected potential of a clustering with a single cluster over the whole data and ` is in general Ω(k) (Bahmani et al., 2012).",
      "startOffset" : 112,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "η is the approximation factor of the optimum in (Ailon et al., 2009).",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 16,
      "context" : "1 in (Nissim et al., 2007).",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "Distributed clustering We consider horizontally partitioned data among peers, in line with (Bahmani et al., 2012), and a setting significantly more restrictive than theirs: each peer can only locally run the standard operations of Forgy initialisation (that is, uniform random seeding) on its own data, unlike for example the biased distributions of (Bahmani et al.",
      "startOffset" : 91,
      "endOffset" : 113
    }, {
      "referenceID" : 3,
      "context" : ", 2012), and a setting significantly more restrictive than theirs: each peer can only locally run the standard operations of Forgy initialisation (that is, uniform random seeding) on its own data, unlike for example the biased distributions of (Bahmani et al., 2012).",
      "startOffset" : 244,
      "endOffset" : 266
    }, {
      "referenceID" : 2,
      "context" : "We note that none of the algorithms of (Ailon et al., 2009; Balcan et al., 2013; Bahmani et al., 2012) would be applicable to this setting without non-trivial modifications affecting their properties.",
      "startOffset" : 39,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "We note that none of the algorithms of (Ailon et al., 2009; Balcan et al., 2013; Bahmani et al., 2012) would be applicable to this setting without non-trivial modifications affecting their properties.",
      "startOffset" : 39,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "We authorise the computation / output of the clustering at the end of the stream, but the memory n allowed for all operations satisfies n < m, such as n = m with α < 1 in (Ailon et al., 2009).",
      "startOffset" : 171,
      "endOffset" : 191
    }, {
      "referenceID" : 10,
      "context" : "It relies on the standard “trick” of summarizing massive datasets via compact representations (synopses) before processing them (Indyk et al., 2014).",
      "startOffset" : 128,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : "(Proof in page 25) It is not surprising to see that Sk-means++ looks like a generalization of (Ailon et al., 2009) and almost matches it (up to the number of centers delivered) when k′ k synopses are learned from k′-means#.",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "Table 1 compares properties of Sk-means++ to (Ailon et al., 2009) (η relates to approximation of the k-means objective in inner loop).",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "Here, points arrive in a sequence, finite, but of unknown size and too large to fit in memory (Liberty et al., 2014).",
      "startOffset" : 94,
      "endOffset" : 116
    }, {
      "referenceID" : 12,
      "context" : "In (Liberty et al., 2014), the clustering algorithm is required to have space and time at most polylog in the length of the stream.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 12,
      "context" : "Table 1 compares properties of OLk-means++ to (Liberty et al., 2014) (we picked the fully on-line, non-heuristic algorithm).",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "Examples abound (Hardt & Price, 2014; Kalai & Vempala, 2005; Chaudhuri et al., 2011; Chichignoud & Lousteau, 2014), etc.",
      "startOffset" : 16,
      "endOffset" : 114
    }, {
      "referenceID" : 16,
      "context" : "Few approaches are related to clustering, yet noise injected is big — the existence of a smaller, sufficient noise, was conjectured in (Nissim et al., 2007) — and approaches rely on a variety of assumptions or knowledge about the optimum (See Table 1) (Nissim et al.",
      "startOffset" : 135,
      "endOffset" : 156
    }, {
      "referenceID" : 16,
      "context" : ", 2007) — and approaches rely on a variety of assumptions or knowledge about the optimum (See Table 1) (Nissim et al., 2007; Wang et al., 2015).",
      "startOffset" : 103,
      "endOffset" : 143
    }, {
      "referenceID" : 21,
      "context" : ", 2007) — and approaches rely on a variety of assumptions or knowledge about the optimum (See Table 1) (Nissim et al., 2007; Wang et al., 2015).",
      "startOffset" : 103,
      "endOffset" : 143
    }, {
      "referenceID" : 8,
      "context" : "Following is the definition of differential privacy (Dwork et al., 2006), tailored for conciseness to our clustering problem.",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 8,
      "context" : "We refer to (Dwork et al., 2006) for details, and assume from now on that data belong to a L1 ball B1(0, R).",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 16,
      "context" : "Φ2) compare with each other, and how do they compare to the state of the art (Nissim et al., 2007; Wang et al., 2015) (we only consider methods with provable approximation bounds of the global optimum).",
      "startOffset" : 77,
      "endOffset" : 117
    }, {
      "referenceID" : 21,
      "context" : "Φ2) compare with each other, and how do they compare to the state of the art (Nissim et al., 2007; Wang et al., 2015) (we only consider methods with provable approximation bounds of the global optimum).",
      "startOffset" : 77,
      "endOffset" : 117
    }, {
      "referenceID" : 16,
      "context" : "Table 1 compares k-variates++ to (Nissim et al., 2007; Wang et al., 2015) in this large sample regime, which is actually a prerequisite for (Nissim et al.",
      "startOffset" : 33,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : "Table 1 compares k-variates++ to (Nissim et al., 2007; Wang et al., 2015) in this large sample regime, which is actually a prerequisite for (Nissim et al.",
      "startOffset" : 33,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : ", 2015) in this large sample regime, which is actually a prerequisite for (Nissim et al., 2007; Wang et al., 2015).",
      "startOffset" : 74,
      "endOffset" : 114
    }, {
      "referenceID" : 21,
      "context" : ", 2015) in this large sample regime, which is actually a prerequisite for (Nissim et al., 2007; Wang et al., 2015).",
      "startOffset" : 74,
      "endOffset" : 114
    }, {
      "referenceID" : 21,
      "context" : "NotationO∗ removes all dependencies in their model parameters (assumptions, model parameters, and δ for the ( , δ)-DP in (Wang et al., 2015)), and λ is the separability assumption parameter (Nissim et al.",
      "startOffset" : 121,
      "endOffset" : 140
    }, {
      "referenceID" : 16,
      "context" : ", 2015)), and λ is the separability assumption parameter (Nissim et al., 2007)4.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "The approximation bounds in (Nissim et al., 2007) consider Wasserstein distance between (estimated / optimal) centers, and not the potential involving data points like us.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "Dk-means++ vs k-means++ and k-means‖ (Bahmani et al., 2012) To address algorithms that can be reduced from k-variates++ (Section 3), we have tested Dk-means++ vs state of the art approach k-means‖; to be fair with Dk-means++, we use k-means++ seeding as the reclustering algorithm in k-means‖.",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "Parameters are in line with (Bahmani et al., 2012).",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 16,
      "context" : "We sample 4λ is named φ in (Nissim et al., 2007).",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 15,
      "context" : "k-variates++ vs Forgy-DP and GUPT To address algorithms that can be obtained via a direct use of k-variates++ (Section 4), we have tested it in a differential privacy framework vs state of the art approach GUPT (Mohan et al., 2012).",
      "startOffset" : 211,
      "endOffset" : 231
    }, {
      "referenceID" : 2,
      "context" : "The first is a reduction technique from k-variates++, which shows a way to obtain straight approximabilty results for other clustering algorithms, some being efficient proxies for the generalisation of existing approaches (Ailon et al., 2009).",
      "startOffset" : 222,
      "endOffset" : 242
    }, {
      "referenceID" : 16,
      "context" : "The second is a direct application of k-variates++ to differential privacy, exhibiting a noise component significantly better than existing approaches (Nissim et al., 2007; Wang et al., 2015).",
      "startOffset" : 151,
      "endOffset" : 191
    }, {
      "referenceID" : 21,
      "context" : "The second is a direct application of k-variates++ to differential privacy, exhibiting a noise component significantly better than existing approaches (Nissim et al., 2007; Wang et al., 2015).",
      "startOffset" : 151,
      "endOffset" : 191
    }, {
      "referenceID" : 0,
      "context" : "One example are Bregman divergences, that fail simple metric transforms (Acharyya et al., 2013).",
      "startOffset" : 72,
      "endOffset" : 95
    }, {
      "referenceID" : 18,
      "context" : "Another example are total divergences, that fail the simple computation of the population minimizers (Nock et al., 2016; Liu et al., 2012).",
      "startOffset" : 101,
      "endOffset" : 138
    }, {
      "referenceID" : 13,
      "context" : "Another example are total divergences, that fail the simple computation of the population minimizers (Nock et al., 2016; Liu et al., 2012).",
      "startOffset" : 101,
      "endOffset" : 138
    }, {
      "referenceID" : 21,
      "context" : "Comments on Table 1 (Wang et al., 2015) are concerned with approximating subspace clustering, and so they are using a very different potential function, which is, between two subspaces S and S′, d(S, S′) = ‖UU> − UU‖F , where U (resp.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : "The statements for σ2 and Φ2 are direct applications of the Laplace mechanism properties (Dwork & Roth, 2014; Dwork et al., 2006).",
      "startOffset" : 89,
      "endOffset" : 129
    }, {
      "referenceID" : 17,
      "context" : "For example, by noticing that the squared Euclidean distance is merely an example of Bregman divergences (the well-known canonical divergences in information geometry of dually flat spaces), k-variates++ can be been extended to that family of dissimilarities (Nock et al., 2008).",
      "startOffset" : 259,
      "endOffset" : 278
    }, {
      "referenceID" : 18,
      "context" : "Such hard distortions include the skew Jeffreys α-centroids (Nock et al., 2016).",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 18,
      "context" : "This also include the recent class of total Bregman/Jensen divergences that are examples of conformal divergences (Nielsen & Nock, 2015; Nock et al., 2016).",
      "startOffset" : 114,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "We have compared Dk-means++ to k-means++ and k-means‖ (Bahmani et al., 2012).",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "We also pick ` = 2k, considering that it is a value which gives some of the best experimental results in (Bahmani et al., 2012).",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 14,
      "context" : "Experiments with k-variates++ and GUPT Among the state-of-the-art approaches against which we could compare k-variates++, there are two major contenders, PINQ (McSherry, 2010) and GUPT (Mohan et al.",
      "startOffset" : 159,
      "endOffset" : 175
    }, {
      "referenceID" : 15,
      "context" : "Experiments with k-variates++ and GUPT Among the state-of-the-art approaches against which we could compare k-variates++, there are two major contenders, PINQ (McSherry, 2010) and GUPT (Mohan et al., 2012).",
      "startOffset" : 185,
      "endOffset" : 205
    }, {
      "referenceID" : 15,
      "context" : "k-means implemented in the GUPT proceeds the following way: the dataset is cut in a certain number of blocks ` (following (Mohan et al., 2012), we fix ` = m in our experiments), the usual k-means algorithm is performed on each block.",
      "startOffset" : 122,
      "endOffset" : 142
    }, {
      "referenceID" : 15,
      "context" : "Note that we disregard the fact that our data are multidimensional, which should require a finer-grained tuning of `, and choose to rely on the ` = m suggestion from (Mohan et al., 2012).",
      "startOffset" : 166,
      "endOffset" : 186
    } ],
    "year" : 2016,
    "abstractText" : "k-means++ seeding has become a de facto standard for hard clustering algorithms. In this paper, our first contribution is a two-way generalisation of this seeding, k-variates++, that includes the sampling of general densities rather than just a discrete set of Dirac densities anchored at the point locations, and a generalisation of the well known Arthur-Vassilvitskii (AV) approximation guarantee, in the form of a bias+variance approximation bound of the global optimum. This approximation exhibits a reduced dependency on the ”noise” component with respect to the optimal potential — actually approaching the statistical lower bound. We show that k-variates++ reduces to efficient (biased seeding) clustering algorithms tailored to specific frameworks; these include distributed, streaming and on-line clustering, with direct approximation results for these algorithms. Finally, we present a novel application of k-variates++ to differential privacy. For either the specific frameworks considered here, or for the differential privacy setting, there is little to no prior results on the direct application of k-means++ and its approximation bounds — state of the art contenders appear to be significantly more complex and / or display less favorable (approximation) properties. We stress that our algorithms can still be run in cases where there is no closed form solution for the population minimizer. We demonstrate the applicability of our analysis via experimental evaluation on several domains and settings, displaying competitive performances vs state of the art. 1 ar X iv :1 60 2. 01 19 8v 2 [ cs .L G ] 1 3 Fe b 20 16",
    "creator" : "LaTeX with hyperref package"
  }
}