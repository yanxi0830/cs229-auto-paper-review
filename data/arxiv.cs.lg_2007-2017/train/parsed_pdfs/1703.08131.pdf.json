{
  "name" : "1703.08131.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Distributed Learning Over Networks in RKH Spaces Using Random Fourier Features",
    "authors" : [ "Pantelis Bouboulis" ],
    "emails" : [ "bouboulis@gmail.com,", "stheodor@di.uoa.gr.", "symeon.chouvardas@huawei.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n08 13\n1v 2\n[ cs\n.L G\n] 2\n4 M\nar 2\n01 7\nIndex Terms—Diffusion, KLMS, Distributed, RKHS, online learning.\nI. INTRODUCTION\nTHE topic of distributed learning, has grown rapidly overthe last years. This is mainly due to the exponentially increasing volume of data, that leads, in turn, to increased requirements for memory and computational resources. Typ-\nical applications include sensor networks, social networks,\nimaging, databases, medical platforms, e.t.c., [1]. In most of those, the data cannot be processed on a single processing\nunit (due to memory and/or computational power constrains)\nand the respective learning/inference problem has to be split into subproblems. Hence, one has to resort to distributed\nalgorithms, which operate on data that are not available on a single location but are instead spread out over multiple\nlocations, e.g., [2], [3], [4].\nIn this paper, we focus on the topic of distributed online learning and in particular to non linear parameter estimation\nand classification tasks. More specifically, we consider a\ndecentralized network which comprises of nodes, that observe data generated by a non linear model in a sequential fashion.\nEach node communicates its own estimates of the unknown parameters to its neighbors and exploits simultaneously a) the\ninformation that it receives and b) the observed datum, at each\ntime instant, in order to update the associated with it estimates. Furthermore, no assumptions are made regarding the presence\nof a central node, which could perform all the necessary\noperations. Thus, the nodes act as independent learners and\nP. Bouboulis and S. Theodoridis are with the Department of Informatics and Telecommunications, University of Athens, Greece, e-mails: panbouboulis@gmail.com, stheodor@di.uoa.gr. S. Chouvardas is with the Mathematical and Algorithmic Sciences Lab France Research Center, Huawei Technologies Co., Ltd., e-mail: symeon.chouvardas@huawei.com\nperform the computations by themselves. Finally, the task of\ninterest is considered to be common across the nodes and, thus,\ncooperation among each other is meaningful and beneficial, [5], [6].\nThe problem of linear online estimation has been considered in several works. These include diffusion-based algorithms,\ne.g., [7], [8], [9], ADMM-based schemes, e.g., [10], [11],\nas well as consencus-based ones, e.g., [12], [13]. The multitask learning problem, in which there are more than one\nparameter vectors to be estimated, has also been treated, e.g.,\n[14], [15]. The literature on online distributed classification is more limited; in [16], a batch distributed SVM algorithm is\npresented, whereas in [17], a diffusion based scheme suitable for classification is proposed. In the latter, the authors study the\nproblem of distributed online learning focusing on strongly-\nconvex risk functions, such as the logistic regression loss, which is suitable to tackle classification tasks. The nodes of\nthe network cooperate via the diffusion rationale. In contrast\nto the vast majority of works on the topic of distributed online learning, which assume a linear relationship between input and\noutput measurements, in this paper we tackle the more general problem, i.e., the distributed online non–linear learning task.\nTo be more specific, we assume that the data are generated by a model y = f(x), where f is a non-linear function that lies in a Reproducing Kernel Hilbert Space (RKHS). These\nare inner-product function spaces, generated by a specific\nkernel function, that have become popular models for nonlinear tasks, since the introduction of the celebrated Support\nVectors Machines (SVM) [18], [19], [20], [6].\nAlthough there have been methods that attempt to generalize\nlinear online distributed strategies to the non-linear domain\nusing RKHS, mainly in the context of the Kernel LMS e.g., [21], [22], [23], these have major drawbacks. In [21] and [23], the estimation of f , at each node, is given as an increasingly growing sum of kernel functions centered at the observed data. Thus, a) each node has to transmit the entire sum at\neach time instant to its neighbors and b) the node has to fuse together all sums received by its neighbors to compute the new\nestimation. Hence, both the communications load of the entire\nnetwork as well as the computational burden at each node grow linearly with time. Clearly, this is impractical for real\nlife applications. In contrast, the method of [22] assumes that\nthese growing sums are limited by a sparsification strategy; how this can be achieved is left for the future. Moreover, the\naforementioned methods offer no theoretical results regarding the consensus of the network. In this work, we present a\ncomplete protocol for distributed online non-linear learning\nfor both regression and classification tasks, overcoming the\n2 aforementioned problems. Moreover, we present theoretical\nresults regarding network-wise consensus and regret bounds. The proposed framework offers fixed-size communication and\ncomputational load as time evolves. This is achieved through an efficient approximation of the growing sum using the\nrandom Fourier features rationale [24]. To the best of our\nknowledge, this is the first time that such a method appears in the literature.\nSection II presents a brief background on kernel online\nmethods and summarizes the main tools and notions used\nin this manuscript. The main contributions of the paper are presented in section III. The proposed method, the related\ntheoretical results and extensive experiments can be found there. Section IV presents a special case of the proposed\nframework for the case of a single node. In this case, we\ndemonstrate how the proposed scheme can be seen as a fixedbudget alternative for online kernel based learning (solving the\nproblem of the growing sum). Finally, section V offers some\nconcluding remarks. In the rest of the paper, boldface symbols denote vectors, while capital letters are reserved for matrices. The symbol ⊗ denotes the Kronecker product of matrices and the symbol ·T the transpose of the respective matrix or vector. Finally, the symbol ‖ · ‖ refers to the respective ℓ2 matrix or vector norm."
    }, {
      "heading" : "II. PRELIMINARIES",
      "text" : ""
    }, {
      "heading" : "A. RKHS",
      "text" : "Reproducing Kernel Hilbert Spaces (RKHS) are inner product spaces of functions defined on X , whose respective point evaluation functional, i.e., Tx : H → X : Tx(f) = f(x), is linear and continuous for every x ∈ X . This is usually portrayed by the reproducing property [18], [6], [25], which links inner products in H with a specific (semi-)positive definite kernel function κ defined on X ×X (associated with the space H). As κ(·, x) lies in H for all x ∈ X , the reproducing property declares that 〈κ(·, y), κ(·, x)〉H = κ(x, y), for all x, y ∈ X . Hence, linear tasks, defined on the high dimensional space, H, (whose dimensionality can also be infinite) can be equivalently viewed as non-linear ones on the, usually, much lower dimensional space, X , and vice versa. This is the essence of the so called kernel trick: Any kernel-based learning method can be seen as a two step procedure, where firstly the original data are transformed from X to H, via an implicit map, Φ(x) = κ(·, x), and then linear algorithms are applied to the transformed data. There exist a plethora of different\nkernels to choose from in the respective literature. In this\npaper, we mostly focus on the popular Gaussian kernel, i.e., κ(x,y) = e‖x−y‖ 2/(2σ2), although any other shift invariant\nkernel can be adopted too.\nAnother important feature of RKHS is that any regularized ridge regression task, defined on H, has a unique solution, which can written in terms of a finite expansion of kernel\nfunctions centered at the training points. Specifically, given the set of training points {(xn, yn), n = 1, . . . , N, xn ∈ X, yn ∈ R}, the representer theorem [26], [18], states that the unique minimizer, f∗ ∈ H, of ∑N n=1 l(f(xn), yn)+λ‖f‖2H, admits a representation of the form f∗ = ∑N n=1 anκ(·, xn), where l is\nany convex loss function that measures the error between the actual system’s outputs, yn, and the estimated ones, f(xn), and ‖ · ‖H is the norm induced by the inner product."
    }, {
      "heading" : "B. Kernel Online Learning",
      "text" : "The aforementioned properties have rendered RKHS a popular tool for addressing non linear tasks both in batch and\nonline settings. Besides the widely adopted application on\nSVMs, in recent years there has been an increased interest on non linear online tasks around the squared error loss function.\nHence, there have been kernel-based implementations of LMS\n[27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs\n[34], focusing on the primal formulation of the task. Henceforth, we will consider online learning tasks based on the training sequences of the form D = {(xn, yn), n = 1, 2, . . . }, where xn ∈ Rd and yn ∈ R. The goal of the assumed learning tasks is to learn a non-linear input-output dependence, y = f(x), f ∈ H, so that to minimize a preselected cost. Note that these types of tasks include both classification (where yn = ±1) and regression problems (where yn ∈ R). Moreover, in the online setting, the data are assumed to arrive sequentially.\nAs a typical example of these tasks, we consider the KLMS,\nwhich is one of the simplest and most representative methods of this kind. Its goal is to learn f , so that to minimize the MSE, i.e., L(f) = E[(y−f(x))2]. Computing the gradient of L and estimating it via the current set of observations (in line with the stochastic approximation rationale, e.g., [6]), the estimate\nat the next iteration, employing the gradient descent method, becomes fn = fn−1+µǫnκ(xn, ·), where ǫn = yn−fn−1(xn) and µ is the step-size (see, e.g., [6], [35], [36] for more). Assuming that the initial estimate is zero, the solution after n− 1 steps turns out to be\nfn−1 = n−1 ∑\ni=1\nαiκ(·,xi), (1)\nwhere αi = µǫi. Observe that this is in line with the representer theorem. Similarly, the system’s output can be estimated as fn−1(xn) = ∑n−1\ni=1 αiκ(xn,xi). Clearly, this linear expansion grows indefinitely as n increases; hence the original form of KLMS is impractical. Typically, a sparsification strategy\nis adopted to bound the size of the expansion [37], [38], [39]. In these methods, a specific criterion is employed to decide whether a particular point, xn, is to be included to the expansion, or (if that point is discarded) how its respective output yn can be exploited to update the remaining weights of the expansion. There are also methods that can remove specific\npoints from the expansion, if their information becomes obsolete, in order to increase the tracking ability of the algorithm\n[40]."
    }, {
      "heading" : "C. Approximating the Kernel with random Fourier Features",
      "text" : "Usually, kernel-based learning methods involve a large\nnumber of kernel evaluations between training samples. In\nthe batch mode of operation, for example, this means that\n3 a large kernel matrix has to be computed, increasing the\ncomputational cost of the method significantly. Hence, to alleviate the computational burden, one common approach is\nto use some sort of approximation of the kernel evaluation. The most popular techniques of this category are the Nyström\nmethod [41], [42] and the random Fourier features approach\n[24], [43]; the latter fits naturally to the online setting. Instead of relying on the implicit lifting, Φ, provided by the kernel trick, Rahimi and Recht in [24] proposed to map the input data\nto a finite-dimensional Euclidean space (with dimension lower than H but larger than the input space) using a randomized feature map zΩ : R\nd → RD, so that the kernel evaluations can be approximated as κ(xn,xm) ≈ zΩ(xn)T zΩ(xm). The following theorem plays a key role in this procedure.\nTheorem 1. Consider a shift-invariant positive definite kernel κ(x − y) defined on Rd and its Fourier transform p(ω) = 1 (2π)d ∫ Rd κ(δ)e−iω\nT δdδ, which (according to Bochner’s theorem) it can be regarded as a probability density function.\nThen, defining zω,b(x) = √ 2 cos(ωTx+ b), it turns out that\nκ(x− y) = Eω,b[zω,b(x)zω,b(y)], (2)\nwhere ω is drawn from p and b from the uniform distribution on [0, 2π].\nFollowing Theorem 1, we choose to approximate κ(xn − xm) using D random Fourier features, ω1,ω2, . . . ,ωD, (drawn from p) and D random numbers, b1, b2, . . . , bD (drawn uniformly from [0, 2π]) that define a sample average:\nκ(xn − xm) ≈ 1\nD\nD ∑\ni=1\nzωi,bi(xm)zωi,bi(xn). (3)\nEvidently, the larger D is, the better this approximation becomes (up to a certain point). Details on the quality of this\napproximation can be found in [24], [43], [44], [45]. We note\nthat for the Gaussian kernel, which is employed throughout the paper, the respective Fourier transform is\np(ω) = ( σ/ √ 2π )D e− σ2‖ω‖2 2 , (4)\nwhich is actually the multivariate Gaussian distribution with mean 0D and covariance matrix 1 σ2 ID .\nWe will demonstrate how this method can be applied using the KLMS paradigm. To this end, we define the map zΩ : R d → RD as follows:\nzΩ(u) =\n√\n2\nD\n\n  cos(ωT1 u+ b1) ...\ncos(ωTDu+ bD)\n\n  , (5)\nwhere Ω is the (d+1)×D matrix defining the random Fourier features of the respective kernel, i.e.,\nΩ =\n(\nω1 ω2 ... ωD b1 b2 ... bD\n)\n, (6)\nprovided that ω’s and b’s are drawn as described in theorem 1. Employing this notation, it is straightforward to see that (3)\ncan be recast as κ(xn−xm) ≈ zΩ(xm)T zΩ(xn). Hence, the output associated with observation xn can be approximated as\nfn−1(xn) ≈ ( n−1 ∑\ni=1\nαizΩ(xi)\n)T\nzΩ(xn). (7)\nIt is a matter of elementary algebra to see that (7) can be equivalently derived by approximating the system’s output as f(x) ≈ θTzΩ(x), initializing θ0 to 0D and iteratively applying the following gradient descent type update: θn = θn−1 + µenzΩ(xn). Clearly, the procedure described here, for the case of the KLMS, can be applied to any other gradient-type kernel based\nmethod. It has the advantage of modeling the solution as a fixed size vector, instead of a growing sum, a property\nthat is quite helpful in distributed environments, as it will be\ndiscussed in section III."
    }, {
      "heading" : "III. DISTRIBUTED KERNEL-BASED LEARNING",
      "text" : "In this section, we discuss the problem of online learning in RKHS over distributed networks. Specifically, we consider K connected nodes, labeled k ∈ N = {1, 2, . . .K}, which operate in cooperation with their neighbors to solve a specific task. Let Nk ⊆ N denote the neighbors of node k. The network topology is represented as an undirected connected graph, consisting of K vertices (representing the nodes) and a set of edges connecting the nodes to each other (i.e., each node\nis connected to its neighbors). We assign a nonnegative weight ak,l to the edge connecting node k to l. This weight is used by k to scale the data transmitted from l and vice versa. This can be interpreted as a measure of the confidence level that node k assigns to its interaction with node l. We collect all coefficients into a K × K symmetric matrix A = (ak,l), such that the entries of the k-th row of A contain the coefficients used by node k to scale the data arriving from its neighbors. We make the additional assumption that A is doubly stochastic, so that the weights of all incoming and outgoing “transmissions” sum\nto 1. A common choice, among others, for choosing these\ncoefficients, is the Metropolis rule, in which the weights equal to:\nak,l =\n\n \n  1 max{|Nk|,|Nl|} , if l ∈ Nk, and l 6= k 1−∑i∈Nk\\k ak,i, if l = k 0, otherwise.\nFinally, we assume that each node, k, receives streaming data {(xk,n, yk,n), n = 1, 2, . . . }, that are generated from an input-output relationship of the form yk,n = f(xk,n) + ηk,n, where xk,n ∈ Rd, yk,n belongs to R and ηk,n represents the respective noise, for the regression task. The goal is to obtain an estimate of f . For classification, yn,k = φ(f(xk,n)), where, φ is a thresholding function; here we assume that yn,k ∈ {−1, 1}. Once more, the goal is to optimally estimate the classifier function f . Each one of the nodes aims to estimate f ∈ H by minimizing a specific convex cost function, L(x, y, f), using a (sub)gradient descent approach. We employ a simple\nCombine-Then-Adapt (CTA) rationale, where at each time instant, n, each node, k, a) receives the current estimates, fl,n−1,\n4 from all neighbors (i.e., from all nodes l ∈ Nk), b) combines them to a single solution, ψk,n−1 = ∑\nl∈Nk ak,lfl,n−1 and c) apply a step update procedure:\nfn = ψk,n−1 − µn∇fL(xn, yn, ψk,n−1).\nThe implementation of such an approach in the context of RKHS presents significant challenges. Keep in mind that, the\nestimation of the solution at each node is not a simple vector,\nbut instead it is a function, which is expressed as a growing sum of kernel evaluations centered at the points observed by\nthe specific node, as in (1). Hence, the implementation of a straightforward CTA strategy would require from each node to transmit its entire growing sum (i.e., the coefficients ai as well as the respective centers xi) to all neighbors. This would significantly increase both the communication load among the\nnodes, as well as the computational cost at each node, since the\nsize of each one of the expansions would become increasingly larger as time evolves (as for every time instant, they gather\nthe centers transmitted by all neighbors). This is the rationale adopted in [21], [22], [23] for the case of KLMS. Clearly,\nthis is far from a practical approach. Alternatively, one could\ndevise an efficient method to sparsify the solution at each node and then merge the sums transmitted by its neighbors.\nThis would require (for example) to search all the dictionaries,\ntransmitted by the neighboring nodes, for similar centers and treat them as a single one, or adopting a single pre-arranged\ndictionary (i.e., a specific set of centers) for all nodes and then fuse each observed point with the best-suited center.\nHowever, no such strategy has appeared in the respective\nliterature, perhaps due to its increased complexity and lack of a theoretical elegance.\nIn this paper, inspired by the random Fourier features\napproximation technique, we approximate the desired inputoutput relationship as y = θTzΩ(x) and propose a two step procedure: a) we map each observed point (xk,n, yk,n) to (zΩ(xk,n), yk,n) and then b) we adopt a simple linear CTA diffusion strategy on the transformed points. Note that in\nthe proposed scheme, each node aims to estimate a vector θ ∈ RD by minimizing a specific (convex) cost function, L(x, y, θ). Here, we imply that the model can be closely approximated by yk,n ≈ θTzΩ(xk,n) + ηk,n, for regression, and yk,n ∼ φ(θT zΩ(xk,n)) for classification, for all k, n, for some θ. We emphasize that L need not be differentiable. Hence, a large family of loss functions can be adopted. For example:\n• Squared error loss: L(x, y, θ) = (y − θTx)2. • Hinge loss: L(x, y, θ) = max(0, 1− yθTx).\nWe end up with the following generic update rule:\nψk,n = ∑\nl∈Nk ak,lθl,n−1, (8)\nθk,n = ψk,n − µk,n∇θL(zΩ(xk,n), yk,n,ψk,n), (9) where ∇θL(zΩ(xk,n), yk,n,ψk,n) is the gradient, or any subgradient of L(x, y, θ) (with respect to θ), if the loss function is not differentiable. Algorithm 1 summarizes the\naforementioned procedure. The advantage of the proposed\nscheme is that each node transmits a single vector (i.e., its\nAlgorithm 1 Random Fourier Features Distributed Online Kernel-based Learning (RFF-DOKL).\nD = {(xk,n, yk,n), k = 1, 2 . . . ,K, n = 1, 2, . . . } ⊲ Input Select a specific shift invariant (semi)positive definite kernel, a specific loss function L and a sequence of possible variable learning rates µn. Each node generates the same matrix Ω as in (6). θk,0 ← 0D, for all k. ⊲ Initialization for n = 1, 2, 3, ... do\nfor each node k do ψk,n = ∑\nl∈Nk ak,lθl,n−1. θk,n = ψk,n − µk,n∇θL(zΩ(xk,n), yk,n,ψk,n).\ncurrent estimate, θk,n) to its neighbors, while the merging of the solutions requires only a straightforward summation."
    }, {
      "heading" : "A. Consensus and regret bound",
      "text" : "In the sequel, we will show that, under certain assumptions, the proposed scheme achieves asymptotic consensus and that\nthe corresponding regret bound grows sublinearly with the\ntime. It can readily be seen that (8)-(9) can be written more compactly (for the whole network) as follows:\nθn = Aθn−1 −MnGn, (10) where θn := (θ T 1,n, . . . , θ T K,n)\nT ∈ RKD, Mn := diag{µ1,n, . . . , µK,n} ⊗ ID, Gn := [(uT1,n, . . . ,uTK,n]T ∈ R\nKD, where uk,n = ∇L(zΩ(xk,n), yk,n,ψk,n), and A := A⊗ ID. The necessary assumptions are the following: Assumption 1. The step size is time decaying and is bounded by the inverse square root of time, i.e., µk,n = µn ≤ µn−1/2. Assumption 2. The norm of the transformed input is bounded, i.e., ∃U1 such that ‖zΩ(xk,n)‖ ≤ U1, ∀k ∈ N , ∀n ∈ N. Furthermore, yk,n is bounded, i.e., |yk,n| ≤ V ∀k ∈ N , ∀n ∈ N for some V > 0.\nAssumption 3. The estimates are bounded, i.e., ∃U2 s.t. ‖θk,n‖ ≤ U2, ∀k ∈ N , ∀n ∈ N. Assumption 4. The matrix comprising the combination weights, i.e., A, is doubly stochastic (if the weights are chosen with respect to the Metropolis rule, this condition is met).\nNote that assumptions 2 and 3 are valid for most of the popular\ncost functions. As an example, we can study the squared error loss, i.e., L(x, y, θ) = 1/2(y − θTx)2, where: ‖∇L(zΩ(x), y, θ)‖ ≤ |y|‖zΩ(x)‖+ ‖θ‖‖zΩ(x)‖2 ≤ V U1 + U21U2. Following similar arguments, we can also prove that many\nother popular cost functions (e.g., the hinge loss, the logistic\nloss, e.t.c.) have bounded gradients too.\nProposition 1 (Asymptotic Consensus). All nodes converge\nto the same solution.\nProof. Consider a KD×KD consensus matrix A as in (10). As A is doubly stochastic, we have the following [9]:\n5 • ‖A‖ = 1. • Any consensus matrix A can be decomposed as\nA = X +BBT , (11)\nwhere B = [b1, . . . , bD] is an KD × D matrix, and bk = 1/ √ K(1⊗ ek), where ek, k = 1, . . . , D represent the standard basis of RD and X is a KD×KD matrix for which it holds that ‖X‖ < 1. • Aθ̆ = θ̆, for all θ̆ ∈ O := {θ ∈ RKD : θ = [θT , . . . , θT ]T , θ ∈ RD}. The subspace O is the so called consensus subspace of dimension D, and bk, k = 1, . . . , D, constitute a basis for this space. Hence, the orthogonal projection of a vector, θ, onto this linear\nsubspace is given by PO(θ) := BBTθ, for all θ ∈ RKD. In [9], it has been proved that, the algorithmic scheme achieves asymptotic consensus, i.e., ‖θk,n − θl,n‖ → 0, as n → ∞, for all k, l ∈ N , if and only if limn→∞ ‖θn − PO(θn)‖ = 0. We can easily check that the quantity\nrn := θn+1 −Aθn = −Mn+1Gn+1. (12) approaches 0, as n → ∞, since limn→∞ Mn = OKD (assumption 1) and the matrix Gn is bounded for all n. Rearranging the terms of (12) and iterating over n, we have:\nθn+1 = Aθn + rn = AAθn−1 +Arn−1 + rn = . . .\n= An+1θ0 +\nn ∑\nj=0\nAn−jrj .\nIf we left-multiply the previous equation by (IKD − BBT ) and follow similar steps as in [9, Lemma 2], it can be verified that lim n→∞ ‖ ( IKm −BBT )\nθn+1‖ = 0, which completes our proof.\nProposition 2. Under assumptions 1-4 (and a cost function\nwith bounded gradients) the networkwise regret is bounded by\nN ∑\ni=1\n∑ k∈N (L(xk,i, yk,i,ψk,i)− L(xk,i, yk,i, g)) ≤ γ\n√ N + δ,\nfor all g ∈ B[0D,U2], where γ, δ are positive constants and B[0D,U2] is the closed ball with center 0D and radius U2. Proof. See appendix A.\nRemark 1. It is worth pointing out that the theoretical\nproperties, which were stated before, are complementary. In\nparticular, the consensus property (Proposition 1) indicates\nthat the nodes converge to the same solution and the sub-\nlinearity of the regret implies that on average the algorithm\nperforms as well as the best fixed strategy. In fact, without\nthe regret related proof we cannot characterize the solution in\nwhich the nodes converge."
    }, {
      "heading" : "B. Diffusion SVM (Pegasos) Algorithm",
      "text" : "The case of the regularized hinge loss function, i.e., L(x, y, θ) = λ2 ‖θ‖2 +max{0, 1− yθTzΩ(x)}, for a specific value of the regularization parameter λ > 0, generates the Distributed Pegasos (see [34]). Note that the Pegasos solves\nthe SVM task in the primal domain. In this case, the gradient\nbecomes ∇θL(x, y, θ) = λθ − I+(1 − yθTzΩ(x))yzΩ(x), where I+ is the indicator function of (0,+∞), which takes a value of 1, if its argument belongs in (0,+∞), and zero otherwise. Hence the step-update equation of algorithm 1 becomes:\nθk,n = (1− 1n )ψk,n−1 +I+(1− ynψTk,n−1zΩ(xk,n)) yk,n λn zΩ(xk,n), (13)\nwhere, following [34], we have used a decreasing step size, µn = 1 λn . This scheme satisfies the required assumptions, hence consensus is guaranteed.\nWe have tested the performance of Distributed-Pegasos versus the non-cooperative Pegasus on four datasets downloaded\nfrom Leon Bottou’s LASVM web page [46]. The chosen\ndatasets are: a) the Adult dataset, b) the Banana dataset (where we have used the first 4000 points as training data and the\nremaining 1300 as testing data), c) the Waveform dataset (where we have used the first 4000 points as training data and\nthe remaining 1000 as testing data) and d) the MNIST dataset\n(for the task of classifying the digit 8 versus the rest). The sizes of the datasets are given in Table I. In all experiments, we\ngenerate random graphs (using MIT’s random graph routine,\nsee [47]) and compare the proposed diffusion method versus a noncooperative strategy (where each node works independent\nof the rest). For each realization of the experiments, a different random connected graph with M = 5 or M = 20 nodes was generated, with probability of attachment per node equal to 0.2 (i.e, there is a 20% probability that a specific node k is connected to any other node l). The adjacency matrix, A, of each graph was generated using the Metropolis rule. For\nthe non-cooperative strategies, we used a graph that connects each node to itself, i.e., A = I5 or A = I20 respectively. The latter, implies that no information is exchanged between the nodes, thus each node is working alone. Moreover, for each\nrealization, the corresponding dataset was randomly split into M subsets of equal size (one for every node). We note that the value of D affects significantly the quality of the approximation via the Fourier features rationale and\nthus it also affects the performance of the experiments. The value of D must be large enough so that the approximation is good, but not too large so that to the communicational\nand computational load become affordable. In practice, we can find a value for D so that any further increase results to almost negligible performance variation (see also section\nIV). All other parameters were optimized (after trials) to give the lowest number of test errors. Their values are reported\non Table IV. The algorithms were implemented in MatLab and the experiments were performed on a i7-3770 machine running at 3.4GHz with 32 Mb of RAM. Tables II and III report the mean test errors obtained by both procedures. For M = 5, the mean algebraic complexity of the generated graphs lies between 0.61 and 0.76 (different for each experiment), while the corresponding mean algebraic degree lies around 1.8. For M = 20, the mean algebraic complexity of the generated graphs lies around 0.70, while the corresponding mean algebraic degree lies around 3.9. The number inside the\nparentheses indicates the times of data reuse (i.e., running the\nalgorithm again over the same data, albeit with a continuously\n6\ndecreasing step-size µn), which has been suggested that improves the classification accuracy of Pegasos (see [34]). For\nexample, the number 2 indicates that the algorithm runs over a dataset of double size, that contains the same data pairs twice.\nFor the three first datasets (Adult, Banana, Waveform) we have\nrun 100 realizations of the experiment, while for the fourth (MNIST) we have run only 10 (to save time). Besides the\nADULT dataset, all other simulations show that the distributed implementation significantly outperforms the non-cooperative\none. For that particular dataset, we observe that for a single run the non-cooperative strategy behaves better (for M = 20), but as data reuse increases the distributed implementation reaches\nlower error floors."
    }, {
      "heading" : "C. Diffusion KLMS",
      "text" : "Adopting the squared error in place of L, i.e., L(x, y, θ) = (y − θTzΩ(x))2, and estimating the gradient by its current measurement, we take the Random Fourier Features Diffusion\nKLMS (RFF-DKLMS) and the step update becomes:\nθk,n = ψk,n−1 + µεk,nzΩ(xk,n), (14)\nwhere εk,n = yn − ψTk,n−1zΩ(xk,n). Although proposition 1 cannot be applied here (as it requires a decreasing step-size),\nwe can derive sufficient conditions for consensus following the results of the standard Diffusion LMS [8]. Henceforth, we\nwill assume that the data pairs are generated by\nyk,n =\nM ∑\nm=1\namκ(cm,xk,n) + ηk,n, (15)\nwhere c1, . . . , cM are fixed centers, xk,n are zero-mean i.i.d, samples drawn from the Gaussian distribution with covariance matrix σ2xId and ηk,n are i.i.d. noise samples drawn from N (0, σ2η). Following the RFF approximation rationale (for shift invariant kernels), we can write that\nyk,n =\nM ∑\nm=1\namEω,b[zω,b(cm)zω,b(xk,n)] + ηk,n\n= aTZTΩzΩ(xk,n) + ǫk,n + ηk,n, = θTo zΩ(xk,n) + ǫk,n + ηk,n,\nwhere ZΩ = (zΩ(c1), . . . , zΩ(cM )), a = (a1, . . . , aM ) T , θo = ZΩa and ǫk,n is the approximation error between the noise-free component of yk,n (evaluated only by the linear kernel expansion of (15)) and the approximation of this component using random Fourier features, i.e., ǫk,n = ∑M\nm=1 amκ(cm,xk,n)−θTo zΩ(xk,n). For the whole network we have the following\ny n = V Tn θo + ǫn + ηn, (16)\nwhere\n• y n := (y1,n, y2,n, . . . , yK,n)\nT ,\n• Vn := diag(zΩ(x1,n), zΩ(x2,n), . . . , zΩ(xK,n)), is a DK ×K matrix, • θo = ( θTo , θ T o , . . . , θ T o\n)T ∈ RDK , • ǫn = (ǫ1,n, ǫ2,n, . . . , ǫK,n)\nT ∈ RK , • η\nn = (η1,n, η2,n, . . . , ηK,n) T ∈ RK . Let x1, . . . ,xK ∈ Rd, y ∈ RK , be the random variables that generate the measurements of the nodes; it is straightforward to prove that the corresponding Wiener solution, i.e., θ∗ = argminθE[‖y − V Tθ‖2], becomes\nθ∗ = E[V V T ]−1E[V y], (17)\nprovided that the autocorrelation matrix R = E[V V T ] is invertible, where V = diag(zΩ(x1), zΩ(x2), . . . , zΩ(xK)) is a DK × K matrix that collects the transformed random variables for the whole network. Assuming that the inputoutput relationship of the measurements at each node follows\n(16), the cross-correlation vector takes the form\nE[V y] = E[V (V Tθo + ǫ+ η)]\n= E[V V T ]θo + E[V ǫ],\nwhere for the last relation we have used that η is a zero mean vector representing noise and that V and η are independent. For large enough D, the approximation error vector ǫ approaches 0K , hence the optimal solution becomes:\nθ∗ = E[V V T ]−1 ( E[V V T ]θo + E[V ǫ] )\n= θo + E[V V T ]−1E[V ǫ] ≈ θo.\n7 Here we actually imply that (16) can be closely approximated by yn ≈ Vnθo + ηn; hence, the RFF-DKLMS is actually the standard diffusion LMS applied on the data pairs {(zΩ(xk,n), yk,n), k = 1, . . . ,K, n = 1, 2 . . .}. The difference is that the input vectors zΩ(xk,n) may have non zero mean and do not follow, necessarily, the Gaussian distri-\nbution. Hence, the available results regarding convergence and stability of diffusion LMS (e.g., [48], [49]) cannot be applied\ndirectly (in these works the inputs are assumed to be zero mean\nGaussian to simplify the formulas related to stability). To this end, we will follow a slightly different approach. Regarding\nthe autocorrelation matrix, we have the following result:\nLemma 1. Consider a selection of samples ω1,ω2, . . . ,ωD, drawn from (4) such that ωi 6= ωj , for any i 6= j. Then, the matrix R = E[V V T ] is strictly positive definite (hence invertible).\nProof. Observe that the DK ×DK autocorrelation matrix is given by R = E[V V T ] = diag(Rzz, Rzz . . . , Rzz), where Rzz = E[zΩ(xk)zΩ(xk)\nT ], for all k = 1, 2, . . . ,K . It suffices to prove that the D × D matrix Rzz is strictly positive definite. Evidently, cTRzzc = c TE [ zΩ(xk)zΩ(xk) T ] c = E [ ( zΩ(xk) T c )2 ]\n≥ 0, for all c ∈ RD. Now, assume that there is a c ∈ RD such that E [ ( zΩ(xk) T c )2 ] = 0. Then zΩ(x) T c = 0 for all x ∈ RD, or equivalently, ∑D i=1 ci cos(ω T i x+bi) = 0, for all x ∈ RD. Thus, c = 0.\nAs expected, the eigenvalues of Rzz play a pivotal role in the convergence’s study of the algorithm. As Rzz is a strictly positive definite matrix, its eigenvalues satisfy 0 < λ1 ≤ λ2 ≤ · · · ≤ λD. Proposition 3. If the the step update µ satisfies: 0 < µ < 2λD , where λD is the maximum eigenvalue of Rzz , then the RFFDKLMS achieves asymptotic consensus in the mean, i.e.,\nlim n\nE[θk,n − θo] = 0D, for all k = 1, 2, . . . ,K.\nProof. See Appendix B.\nRemark 2. If xk,n ∼ N (0, σXId), it is possible to evaluate explicitly the entries of Rzz , i.e.,\nri,j = 1\n2 exp (−‖ωi − ωj‖2σ2X 2 ) cos(bi − bj)\n+ 1\n2 exp (−‖ωi + ωj‖2σ2X 2 ) cos(bi + bj).\nProposition 4. For stability in the mean-square sense, we must ensure that both µ and A satisfy:\n|ρ (ID2K2 − µ (R⊠ IDK + IDK ⊠R) (A⊠A))| < 1, where ⊠ denotes the unbalanced block Kronecker product.\nProof. See Appendix C.\nIn the following, we present some experiments to illustrate\nthe performance of the proposed scheme. We demonstrate that the estimation provided by the cooperative strategy is better\nthan having each node working alone (i.e., lower MSE). Sim-\nilar to section III-C, each realization of the experiments uses\na different random connected graph with M = 20 nodes and probability of attachment per node equal to 0.2. The adjacency matrix, A, of each graph was generated using the Metropolis rule (resulting to graphs with mean algebraic connectivity around 0.69), while for the non-cooperative strategies, we used a graph that connects each node to itself, i.e., A = I20. All parameters were optimized (after trials) to give the lowest MSE. The algorithms were implemented in MatLab and the\nexperiments were performed on a i7-3770 machine running at 3.4GHz with 32 Mb of RAM. 1) Example 1. A Linear Expansion in terms of kernels: In this set-up, we generate 5000 data pairs for each node using the following model: yk,n = ∑M m=1 amκ(cm,xk,n) + ηk,n, where xk,n ∈ R5 are drawn from N (0, I5) and the noise are i.i.d. Gaussian samples with ση = 0.1. The parameters of the expansion (i.e., a1, . . . , aM ) are drawn from N (0, 25), the kernel parameter σ is set to 5, the step update to µ = 1 and the number of random Fourier features to D = 2500. Figure 1(a) shows the evolution of the MSE over all network nodes for 100 realizations of the experiment. We note that the selected\nvalue of step size satisfies the conditions of proposition 3. 2) Example 2: Next, we generate the data pairs for each node using the following simple non-linear model: yk,n = wT0 xk,n +0.1 · (wT1 xk,n)2 + ηk,n, where ηk,n represent zeromean i.i.d. Gaussian noise with ση = 0.05 and the coefficients of the vectors w0,w1 ∈ R5 are i.i.d. samples drawn from N (0, 1). Similarly to Example 1, the kernel parameter σ is set to 5 and the step update to µ = 1. The number of random Fourier coefficients for RFFKLMS was set toD = 300. Figure 3(b) shows the evolution of the MSE over all network nodes for 1000 realizations of the experiment over 15000 samples. 3) Example 3: Here we adopt the following chaotic series model [50]: dk,n = dk,n−1\n1+d2k,n−1 + u3k,n−1, yk,n = dk,n + ηk,n,\nwhere ηn is zero-mean i.i.d. Gaussian noise with ση = 0.01 and un is also zero-mean i.i.d. Gaussian with σu = 0.15. The kernel parameter σ is set to 0.05, the number of Fourier features to D = 100 and the step update to µ = 1. We have also initialized d1 to 1. Figure 1(c) shows the evolution of the MSE over all network nodes for 1000 realizations of the\nexperiment over 500 samples. 4) Example 4: For the final example, we use another chaotic series model [50]: dk,n = uk,n+0.5vk,n−0.2dk,n−1+ 0.35dk,n−2, yk,n = φ(dk,n) + ηk,n,\nφ(dk,n) =\n{ dk,n 3(0.1+0.9d2k,n) 1/2 dk,n ≥ 0 −d2k,n(1−exp(0.7dk,n))\n3 dk,n < 0 ,\nwhere ηk,n, vk,n are zero-mean i.i.d. Gaussian noise with ση = 0.001 and σ 2 v = 0.0156 respectively, and uk,n = 0.5vk,n + η̂k,n, where η̂n is also i.i.d. Gaussian with σ 2 = 0.0156. The kernel parameter σ is set to 0.05 and the step update to µ = 1. We have also initialized d1, d2 to 1. Figure 3(d) shows the evolution of the MSE over all network nodes for 1000 realizations of the experiment over 1000 samples. The number of random Fourier features was set to D = 200."
    }, {
      "heading" : "IV. REVISITING ONLINE KERNEL BASED LEARNING",
      "text" : "In this section, we investigate the use of random Fourier\nfeatures as a general framework for online kernel-based learn-\n8 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 −20 −18 −16 −14 −12 −10 −8 −6 Non−cooperative KLMS Diffusion KLMS\n(a) Example 1\n2000 4000 6000 8000 10000 12000 14000 −20\n−18\n−16\n−14\n−12\n−10\n−8\n−6\n−4\nNon−cooperative KLMS Diffusion KLMS\n(b) Example 2\n50 100 150 200 250 300 350 400 450 500\n−35\n−30\n−25\n−20\n−15\n−10\nnoncooperative Diffusion\n(c) Example 3\n100 200 300 400 500 600 700 800 900 1000 −28\n−26\n−24\n−22\n−20\n−18\n−16\nnoncooperative Diffusion\n(d) Example 4\nFig. 1. Comparing the performances of RFF Diffusion KLMS versus the non-cooperative strategy.\nAlgorithm 2 Random Fourier Features Online Kernel-based Learning (RFF-OKL).\nD = {(xn, yn), n = 1, 2, . . . } ⊲ Input Select a specific (semi)positive definite kernel, a specific loss function L and a sequence of possible variable learning rates µn. Then generate the matrix Ω as in (6). θ0 ← 0D ⊲ Initialization for n = 1, 2, 3, ... do\nθn = θn−1 + µn∇θL(xn, yn, θn−1). ⊲ Step update\ning. The framework presented here can be seen as a special\ncase of the general distributed method presented in section\nIII for a network with a single node. Similar to the case of the standard KLMS, the learning algorithms considered\nhere adopt a gradient descent rationale to minimize a specific loss function, L(x, y, f) for f ∈ H, so that f approximates the relationship between x and y, where H is the RKHS induced by a specific choice of a shift invariant (semi)positive definite kernel, κ. Hence, in general, these algorithms can be summarized by the following step update equation: fn = fn−1 + µn∇fL(xn, yn, fn−1). Algorithm 2 summarizes the proposed procedure for online kernel-based learning. The\nperformance of the algorithm depends on the quality of the adopted approximation. Hence, a sufficiently large D has to be selected.\nAlthough algorithm 2 is given in a general setting, in the\nfollowing we focus on the fixed-budget KLMS. As it has been discussed in section II, KLMS adopts the MSE cost\nfunction, which in the proposed framework takes the form: L(x, y, θ) = E[(yn − θTzΩ(xn))2]. Hence, the respective step update equation of algorithm 2 becomes\nθn = θn−1 + µεnzΩ(xn), (18)\nwhere εn = yn − θTn−1zΩ(xn). Observe that, contrary to the typical implementations of KLMS, where the system’s output\nis a growing expansion of kernel functions and hence special\ncare has to be carried out to prune the so called dictionary,\nthe proposed approach employs a fixed-budget rationale, which\ndoesn’t require any further treatment. We call this scheme the Random Fourier Features KLMS (RFF-KLMS) [51], [52].\nThe study of the convergence properties of RFFKLMS is based on those of the standard LMS. Henceforth, we will\nassume that the data pairs are generated by\nyn = M ∑\nm=1\namκ(cm,xn) + ηn, (19)\nwhere c1, . . . , cM are fixed centers, xn are zero-mean i.i.d, samples drawn from the Gaussian distribution with covariance matrix σ2xId and ηn are i.i.d. noise samples drawn from N (0, σ2η). Similar to the diffusion case, the eigenvalues of Rzz , i.e., 0 < λ1 ≤ λ2 ≤ · · · ≤ λD , play a pivotal role in the convergence’s study of the algorithm. Applying\nsimilar assumptions as in the case of the standard LMS (e.g., independence between xn,xm, for n 6= m and between xn, ηn), we can prove the following results.\nProposition 5. For datasets generated by (19) we have:\n1) If 0 < µ < 2/λD, then RFFKLMS converges in the mean, i.e., E[θn − θo] → 0. 2) The optimal MSE is given by\nJoptn = σ 2 η + E[ǫn]− E[ǫnzΩ(xn)]R−1zz E[ǫnzΩ(xn)T ].\nFor large enough D, we have Joptn ≈ σ2η . 3) The excess MSE is given by J exn = Jn − Joptn =\ntr (RzzAn), where An = E[(θn − θo)(θn − θo)T ]. 4) If 0 < µ < 1/λD, then An converges. For large\nenough n and D we can approximate An’s evolution as An+1 ≈ An − µ (RzzAn +AnRzz) + µ2σ2ηRzz . Using this model we can approximate the steady-state MSE\n(≈ tr (RzzAn) + σ2η). Proof. The proofs use standard arguments as in the case of\nthe standard LMS. Hence we do not provide full details. The\nreader is addressed to any LMS textbook. 1) See Proposition 3. 2) Replacing θn with θo in Jn = E[ε 2 n] gives the result. For large enough D, ǫn is almost zero, hence we have J opt n ≈ σ2η . 3) Here, we use the additional assumptions that vn is independent of xn and that ǫn is independent of ηn. The result follows after replacing Jn and J opt n and performing simple algebra calculations. 4) Replacing θo and dropping out the terms that contain the term ǫn, the result is obtained.\nRemark 3. Observe that, while the first two results can\nbe regarded as special cases of the distributed case (see\nproposition 3 and the related discussion in section III), the\ntwo last ones describe more accurately the evolution of the\nsolution in terms of mean square stability, than the one given\nin proposition 4, for the general distributed scheme (where no formula for Bn is given). This becomes possible because the related formulas take a much simpler form, if the graph\nstructure is reduced to a single node.\nIn order to illustrate the performance of the proposed\nalgorithm and compare its behavior to the other variants\n9 of KLMS, we also present some related simulations. We\nchoose the QKLMS [39] as a reference, since this is one of the most effective and fast KLMS pruning methods. In\nall experiments, that are presented in this section (described below), we use the same kernel parameter, i.e., σ, for both RFFKLMS and QKLMS as well as the same step-update parameter µ. The quantization parameter q of the QKLMS controls the size of the dictionary. If this is too large, then\nthe dictionary will be small and the achieved MSE at steady\nstate will be large. Typically, however, there is a value for q for which the best possible MSE (which is very close to the MSE of the unsparsified version) is attained at steady\nstate, while any smaller quantization sizes provide negligible improvements (albeit at significantly increased complexity). In all experimental set-ups, we tuned q (using multiple trials) so that it leads to the best performance. On the other hand, the performance of RFFKLMS depends largely on D, which controls the quality of the kernel approximation. Similar to the case of QKLMS, there is a value for D so that RFFKLMS attains its lowest steady-state MSE, while larger values provide\nnegligible improvements. For our experiments, the chosen values for q and D provide results so that to trace out the results provided by the original (unsparsified) KLMS. Table V gives the mean training times for QKLMS and RFFKLMS\non the same i7-3770 machine using a MatLab implementation\n(both algorithms were optimized for speed). We note that the complexity of the RFFKLMS is O(Dd), while the complexity of QKLMS is O(Md). Our experiments indicate that in order to obtain similar error floors, the required complexity of RFFKLMS is lower than that of QKLMS.\n1) Example 5. A Linear expansion in terms of Kernels:\nSimilar to example 1 in section III-C, we generate 5000 data pairs using (19) and the same parameters (for only one node). Figure 2 shows the evolution of the MSE for 500 realizations of the experiment over different values of D. The algorithm reaches steady-state around n = 3000. The attained MSE is getting closer to the approximation given in proposition 5 (dashed line in the figure) as D increases. Figure 3(a) compares the performances of RFFKLMS and QKLMS for this particular set-up for 500 realizations of the experiment using 8000 data pairs. The quantization size of QKLMS was set to q = 5 and the number of Fourier features for the RFFKLMS was set to D = 2500. 2) Example 6: Next, we use the same non-linear model as in example 2 of section III, i.e., yn = w T 0 xn + 0.1 · (wT1 xn) 2 + ηn. The parameters of the model and the RFFKLMS are the same as in example 1. The quantization size of the QKLMS was set to q = 5, leading to an average dictionary size M = 100. Figure 3(b) shows the evolution of the MSE for both QKLMS and RFFKLMS running 1000 realizations of the experiment over 15000 samples. 3) Example 7: Here we adopt the same chaotic series\nmodel as in example 3 of section III-C, with the same parameters. Figure 3(c) shows the evolution of the MSE for both QKLMS and RFFKLMS running 1000 realizations of the experiment over 500 samples. The quantization parameter q for the QKLMS was set to q = 0.01, leading to an average dictionary size M = 7.\n5\nFouKLMS\n5\nFouKLMS\n4) Example 8: For the final example, we use the chaotic\nseries model of example 4 in section III-C with the same parameters. Figure 3(d) shows the evolution of the MSE for both QKLMS and RFFKLMS running 1000 realizations of the experiment over 1000 samples. The parameter q was set to q = 0.01, leading to M = 32."
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "We have presented a complete fixed-budget framework for\nnon-linear online distributed learning in the context of RKHS. The proposed scheme achieves asymptotic consensus under\nsome reasonable assumptions. Furthermore, we showed that\nthe respective regret bound grows sublinearly with time. In the case of a network comprising only one node, the proposed\nmethod can be regarded as a fixed budget alternative for online kernel-based learning. The presented simulations validate the\ntheoretical results and demonstrate the effectiveness of the\nproposed scheme.\n10"
    }, {
      "heading" : "APPENDIX A PROOF OF PROPOSITION 2",
      "text" : "In the following, we will use the notation Lk,n(θ) := L(xk,n, yk,n, θ) to shorten the respective equations. Choose any g ∈ B[0D,U2]. It holds that\n‖ψk,n − g‖2 − ‖θk,n − g‖2 = −‖ψk,n − θk,n‖2\n− 2〈θk,n −ψk,n,ψk,n − g〉 = −µ2n‖∇Lk,n(ψk,n)‖2 + 2µn〈∇Lk,n(ψk,n),ψk,n − g〉. (20)\nMoreover, as Lk,n is convex, we have:\nLk,n(θ) ≥ Lk,n(θ′) + 〈h, θ − θ′〉, (21)\nfor all θ, θ′ ∈ dom(Lk,n) where h := ∇Lk,n(θ) is the gradient (for a differentiable cost function) or a subgradient\n(for the case of a non–differentiable cost function). From (20),\n(21) and the boundness of the (sub)gradient we take\n‖ψk,n − g‖2 − ‖θk,n − g‖2 ≥ −µ2nU2 − 2µn(Lk,n(g) − Lk,n(ψk,n)), (22)\nwhere U is an upper bound for the (sub)gradient. Recall that for the whole network we have: ψ\nn = Aθn−1 and that for\nany doubly stochastic matrix, A, its norm equals to its largest eigenvalue, i.e., ‖A‖ = λmax = 1. A respective eigenvector is g = (gT . . . , gT )T ∈ RDK , hence it holds that g = Ag and\n‖ψ n − g‖ = ‖Aθn−1 −Ag‖ ≤ ‖A‖‖θn−1 − g‖\n= ‖θn−1 − g‖ (23)\nwhere ψ n = (ψTn , . . . ,ψ T n ) T ∈ RDK . Going back to (22) and summing over all k ∈ N , we have:\n∑ k∈N (‖ψk,n − g‖2 − ‖θk,n − g‖2) ≥\n−µ2nKU2 − 2µn ∑ k∈N (Lk,n(g) − Lk,n(ψk,n)). (24)\nHowever, for the left hand side of the inequality we obtain ∑\nk∈N (‖ψk,n−g‖2−‖θk,n−g‖2) = ‖ψn−g‖ 2−‖θn−g‖2.\nIf we combine the last relation with (23) and (24) we have\n‖θn−1 − g‖2 − ‖θn − g‖2 ≥ −µ2nKU2 − 2µn ∑\nk∈N (Lk,n(g)− Lk,n(ψk,n)). (25)\nThe last inequality leads to\n1\nµn ‖θn−1 − g‖2 −\n1\nµn+1 ‖θn − g‖2 =\n+ 1\nµn (‖θn−1 − g‖2 − ‖θn − g‖2)\n+\n(\n1 µn − 1 µn+1\n)\n‖θn − g‖2 ≥\n− µnKU2 − 2 ∑ k∈N (Lk,n(g)− Lk,n(ψk,n))\n+ 4KU22\n(\n1 µn − 1 µn+1\n)\n,\nwhere we have taken into consideration, Assumption 3 and the boundeness of g. Next, summing over i = 1, . . . , N + 1, taking into consideration that\n∑N i=1 µi ≤ 2µ\n√ N (Assumption\n1) and noticing that some terms telescope, we have:\n1 µ ‖θ0 − g‖2 − 1 µN+1 ‖θN − g‖2 ≥ −KU22µ\n√ N\n+ 2\nN ∑\ni=1\n∑ k∈N (Lk,i(ψk,i)− Lk,i(g)) + 4KU22\n(\n1 µ −\n√ N + 1\nµ\n)\n.\nRearranging the terms and omitting the negative ones com-\npletes the proof:\nN ∑\ni=1\n∑ k∈N (Lk,i(ψk,i)− Lk,i(g))\n≤ 1 2µ\n‖θ0 − g‖2 +KU2µ √ N + 2KU22\n√ N + 1\nµ\n≤ 1 2µ\n‖θ0 − g‖2 +KU2µ √ N + 2KU22\n√ N + 1\nµ ."
    }, {
      "heading" : "APPENDIX B PROOF OF PROPOSITION 3",
      "text" : "For the whole network, the step update of RFF-DKLMS\ncan be recasted as\nθn = Aθn−1 + µVnεn, (26)\nwhere εn = (ε1,n, ε2,n, . . . , εK,n) T and εk,n = yk,n − ψTk,nzΩ(xk,n), or equivalently, εn = yn − V T n Aθn−1. If we define Un = θn − θo and take into account that Ag = g, for all g ∈ RDK , such that g = (gT , gT , . . . , gT )T for g ∈ RD, we obtain:\nUn = Aθn−1 + µVn(yn − V T n Aθn−1)− θo\n= A(θn−1 − θo) + µVn(V Tn θo + ǫn + ηn − V T n Aθn−1) = AUn−1 − µVnV Tn AUn−1 + µVnǫn + µVnηn\nIf we take the mean values and assume that θk,n and zΩ(xk,n) are independent for all k = 1, . . . ,K , n = 1, 2, . . . , we have\nE[Un] = (IKD − µR)AE[Un−1] + µE[Vnǫn] + µE[Vnηn]. Taking into account that ηn and Vn are independent, that E[ηn] = 0 and that for large enoughD we have E[Vnǫn] ≈ 0, we can take E[Un] ≈ ((IKD − µR)A)n−1 E[U1]. Hence, if\n11\nall the eigenvalues of (IKD −µR)A have absolute value less than 1, we have that E[Un] → 0. However, since A is a doubly stochastic matrix we have ‖A‖ ≤ 1 and\n‖(IKD − µR)A‖ ≤ ‖IKD − µR‖‖A‖ ≤ ‖IKD − µR‖.\nMoreover, as IKD − µR is a diagonal block matrix, its eigenvalues are identical to the eigenvalues of its blocks, i.e., the eigenvalues of ID − µRzz . Hence, a sufficient condition for convergence is |1 − µλD(Rzz)| < 1, which gives the result.\nRemark 4. Observe that |λmax ((IKD − µR)A) | ≤ |λmax ((IKD − µR)IKD) |, which means that the spectral radius of (IKD − µR)A is generally smaller than that of (IKD − µR)IKD (which corresponds to the non-cooperative protocol). Hence, cooperation under the diffusion rationale\nhas a stabilizing effect on the network [8]."
    }, {
      "heading" : "APPENDIX C PROOF OF PROPOSITION 4",
      "text" : "Let Bn = E[UnU T n ], where Un = AUn−1 − µVnV T n AUn−1 +µVnǫn +µVnηn. Taking into account that the noise is i.i.d., independent from Un and Vn and that ǫn is close to zero (if D is sufficiently large), we can take that:\nBn =ABn−1A T − µABn−1ATR− µRABn−1AT\n+ µ2σ2ηR+ µ 2E[VnV T n AUn−1U T n−1A TVnV T n ].\nFor sufficiently small step-sizes, the rightmost term can be neglected [53], [49], hence we can take the simplified form\nBn =ABn−1A T − µABn−1ATR− µRABn−1AT\n+ µ2σ2ηR. (27)\nNext, we observe that Bn, R and A can be regarded as block matrices, that consist of K ×K blocks with size D×D. We will vectorize equation (27) using the vecbr operator, as this has been defined in [54]. Assuming a block-matrix C:\nC =\n\n\nC11 C12 ... C1K C21 C22 ... C2K\n. . .\n. . .\n. . .\nCK1 CK2 ... CKK\n\n,\nthe vecbr operator applies the following vectorization:\nvecbrC = (vecC T 11, vecC T 12, . . . , vecC T 1K , . . . ,\nvecCTK1 vecC T K2, . . . , vecC T KK) T .\nMoreover, it is closely related to the following block Kronecker product:\nD ⊠ C =\n\n\nD ⊗ C11 D ⊗ C12 ... D ⊗ C1K D ⊗ C21 D ⊗ C22 ... D ⊗ C2K\n. . .\n. . .\n. . .\nD ⊗ CK1 D ⊗ CK2 ... D ⊗ CKK\n\n.\nThe interested reader can delve into the details of the vecbr operator and the unbalanced block Kronecker product in [54]. Here, we limit our interest to the following properties:\n1) vecbr(DCE T ) = (E ⊠D) vecbr C. 2) (C ⊠D)(E ⊠ F ) = CE ⊠DF .\nThus, applying the vecbr operator, on both sizes of (27) we take bn = (A ⊠ A)bn−1 − µ ((RA) ⊠A) bn−1 − µ ((A)⊠RA) bn−1 + µ2σ2ηr, where bn = vecbr Bn and r = vecbrR. Exploiting the second property, we can take:\n(RA) ⊠A = (RA) ⊠ (IDKA) = (R⊠ IDK)(A⊠A),\nA⊠ (RA) = (IDKA)⊠ (RA) = (IDK ⊠R)(A ⊠A).\nHence, we finally get:\nbn =(ID2K2 − µ (R⊠ IDK − IDK ⊠A)) (A⊠A)bn−1 + µ2σ2ηr,\nwhich gives the result."
    } ],
    "references" : [ {
      "title" : "Modeling and optimization for big data analytics:(statistical) learning tools for our era of data deluge",
      "author" : [ "K. Slavakis", "G. Giannakis", "G. Mateos" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 31, no. 5, pp. 18–31, 2014.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Map-reduce for machine learning on multicore",
      "author" : [ "C. Chu", "S.K. Kim", "Y.-A. Lin", "Y. Yu", "G. Bradski", "A.Y. Ng", "K. Olukotun" ],
      "venue" : "Advances in neural information processing systems, vol. 19, p. 281, 2007.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Big data and cloud computing: current state and future opportunities",
      "author" : [ "D. Agrawal", "S. Das", "A. El Abbadi" ],
      "venue" : "Proceedings of the 14th International Conference on Extending Database Technology. ACM, 2011, pp. 530–533.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Data mining with big data",
      "author" : [ "X. Wu", "X. Zhu", "G.-Q. Wu", "W. Ding" ],
      "venue" : "IEEE transactions on knowledge and data engineering, vol. 26, no. 1, pp. 97–107, 2014.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Diffusion adaptation over networks",
      "author" : [ "A.H. Sayed" ],
      "venue" : "Academic Press Library in Signal Processing, vol. 3, pp. 323–454, 2013.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Machine Learning: A Bayesian and Optimization Perspective",
      "author" : [ "S. Theodoridis" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Adaptive robust distributed learning in diffusion sensor networks",
      "author" : [ "S. Chouvardas", "K. Slavakis", "S. Theodoridis" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 59, no. 10, pp. 4692–4707, 2011.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Diffusion least-mean squares over adaptive networks: Formulation and performance analysis",
      "author" : [ "C.G. Lopes", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 3122–3136, July 2008.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "An adaptive projected subgradient approach to learning in diffusion networks",
      "author" : [ "R.L. Cavalcante", "I. Yamada", "B. Mulgrew" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 57, no. 7, pp. 2762–2774, 2009.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Distributed LMS for consensus-based in-network adaptive processing",
      "author" : [ "I.D. Schizas", "G. Mateos", "G.B. Giannakis" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 57, no. 6, pp. 2365–2382, 2009.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Distributed recursive least-squares for consensus-based in-network adaptive estimation",
      "author" : [ "G. Mateos", "I.D. Schizas", "G.B. Giannakis" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 57, no. 11, pp. 4583–4588, 2009.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Parallel and distributed computation: Numerical Methods, 2nd ed",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Athena-Scientific,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1999
    }, {
      "title" : "Gossip algorithms for distributed signal processing",
      "author" : [ "A.G. Dimakis", "S. Kar", "J.M. Moura", "M.G. Rabbat", "A. Scaglione" ],
      "venue" : "Proceedings of the IEEE, vol. 98, no. 11, pp. 1847–1864, 2010.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1847
    }, {
      "title" : "Multitask diffusion adaptation over networks",
      "author" : [ "J. Chen", "C. Richard", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 62, no. 16, pp. 4129–4144, 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Distributed diffusion-based LMS for node-specific adaptive parameter estimation",
      "author" : [ "J. Plata-Chaves", "N. Bogdanovic", "K. Berberidis" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 63, no. 13, pp. 3448–3460, 2015.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Consensus-based distributed support vector machines",
      "author" : [ "P.A. Forero", "A. Cano", "G.B. Giannakis" ],
      "venue" : "Journal of Machine Learning Research, vol. 11, no. May, pp. 1663–1707, 2010.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "On distributed online classification in the midst of concept drifts",
      "author" : [ "Z.J. Towfic", "J. Chen", "A.H. Sayed" ],
      "venue" : "Neurocomputing, vol. 112, pp. 138–152, 2013.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning with Kernels: Support Vector Machines, Regularization, Optimization and Beyond",
      "author" : [ "B. Scholkopf", "A. Smola" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2002
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2004
    }, {
      "title" : "Pattern Recognition, 4 ed",
      "author" : [ "S. Theodoridis", "K. Koutroumbas" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "The diffusion-KLMS algorithm",
      "author" : [ "R. Mitra", "V. Bhatia" ],
      "venue" : "ICIT, 2014, Dec 2014, pp. 256–259.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Diffusion adaptation over networks with kernel least-mean-square",
      "author" : [ "W. Gao", "J. Chen", "C. Richard", "J. Huang" ],
      "venue" : "CAMSAP, 2015.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A diffusion kernel LMS algorithm for nonlinear adaptive networks",
      "author" : [ "C. Symeon", "D. Moez" ],
      "venue" : "ICASSP, 2016.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Random features for large scale kernel machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "NIPS, vol. 20, 2007.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "An introduction to support vector machines and other kernel-based learning methods",
      "author" : [ "N. Cristianini", "J. Shawe-Taylor" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2000
    }, {
      "title" : "Spline Models for Observational Data, volume 59 of CBMS- NSF Regional Conference Series in Applied Mathematics",
      "author" : [ "G. Wahba" ],
      "venue" : "Philadelphia: SIAM,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1990
    }, {
      "title" : "The kernel Least-Mean-Square algorithm",
      "author" : [ "W. Liu", "P. Pokharel", "J.C. Principe" ],
      "venue" : "IEEE Transanctions on Signal Processing, vol. 56, no. 2, pp. 543–554, Feb. 2008.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Extension of Wirtinger’s Calculus to Reproducing Kernel Hilbert spaces and the complex kernel LMS",
      "author" : [ "P. Bouboulis", "S. Theodoridis" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 59, no. 3, pp. 964–978, 2011.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A sliding-window kernel rls algorithm and its application to nonlinear channel identification",
      "author" : [ "S. Van Vaerenbergh", "J. Via", "I. Santamana" ],
      "venue" : "ICASSP, vol. 5, may 2006, p. V.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The kernel recursive least-squares algorithm",
      "author" : [ "Y. Engel", "S. Mannor", "R. Meir" ],
      "venue" : "IEEE Transanctions on Signal Processing, vol. 52, no. 8, pp. 2275–2285, Aug. 2004.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Sliding window generalized kernel affine projection algorithm using projection mappings",
      "author" : [ "K. Slavakis", "S. Theodoridis" ],
      "venue" : "EURASIP Journal on Advances in Signal Processing, vol. 19, p. 183, 2008.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Adaptive multiregression in reproducing kernel Hilbert spaces: the multiaccess MIMO channel case",
      "author" : [ "K. Slavakis", "P. Bouboulis", "S. Theodoridis" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems, vol. 23(2), pp. 260–276, 2012.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On line kernel-based classification using adaptive projection algorithms",
      "author" : [ "K. Slavakis", "S. Theodoridis", "I. Yamada" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 2781–2796, Jul. 2008.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Pegasos: primal estimated sub-gradient solver for svm",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter" ],
      "venue" : "Mathematical Programming, vol. 127, no. 1, pp. 3–30, 2011. [Online]. Available: http://dx.doi.org/10.1007/s10107-010-0420-4",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Online learning in reproducing kernel Hilbert spaces",
      "author" : [ "K. Slavakis", "P. Bouboulis", "S. Theodoridis" ],
      "venue" : "Signal Processing Theory and Machine Learning, ser. Academic Press Library in Signal Processing, R. Chellappa and S. Theodoridis, Eds. Academic Press, 2014, pp. 883–987.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Online prediction of time series data with kernels",
      "author" : [ "C. Richard", "J. Bermudez", "P. Honeine" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 57, no. 3, pp. 1058 –1067, march 2009.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Online dictionary learning for kernel LMS",
      "author" : [ "W. Gao", "J. Chen", "C. Richard", "J. Huang" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 62, no. 11, pp. 2765 – 2777, 2014.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Quantized kernel least mean square algorithm",
      "author" : [ "B. Chen", "S. Zhao", "P. Zhu", "J. Principe" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 1, pp. 22 –32, jan. 2012.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Self-organizing kernel adaptive filtering",
      "author" : [ "S. Zhao", "B. Chen", "C. Zheng", "P. Zhu", "J. Principe" ],
      "venue" : "EURASIP Journal on Advances in Signal Processing, (to appear).",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Using the Nystrom method to speed up kernel machines",
      "author" : [ "C. Williams", "M. Seeger" ],
      "venue" : "NIPS, vol. 14, 2001, pp. 682 – 688.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "On the Nystrom method for approximating a gram matrix for improved kernel-based learning",
      "author" : [ "P. Drineas", "M.W. Mahoney" ],
      "venue" : "JMLR, vol. 6, pp. 2153 – 2175, 2005.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Weighted sums of random kitchen sinks: replacing minimization with randomization in learning",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "NIPS, vol. 22, 2009, pp. 1313 – 1320.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On the error of random Fourier features",
      "author" : [ "D.J. Sutherland", "J. Schneider" ],
      "venue" : "UAI, 2015.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Nyström method vs random Fourier features: A theoretical and empirical comparison",
      "author" : [ "T. Yang", "Y.-F. Li", "M. Mahdavi", "J. Rong", "Z.-H. Zhou" ],
      "venue" : "NIPS, vol. 25, 2012, pp. 476–484.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Diffusion least-mean squares over adaptive networks: Formulation and performance analysis",
      "author" : [ "C.G. Lopes", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 3122–3136, 2008.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Diffusion LMS strategies for distributed estimation",
      "author" : [ "F.S. Cattiveli", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 58, no. 3, pp. 1035–1048, 2010.",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Stochastic behavior analysis of the gaussian kernel least-mean-square algorithm",
      "author" : [ "W. Parreira", "J. Bermudez", "C. Richard", "J.-Y. Tourneret" ],
      "venue" : "Signal Processing, IEEE Transactions on, vol. 60, no. 5, pp. 2208–2222, May 2012.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Online learning with kernels: Overcoming the growing sum problem",
      "author" : [ "A. Singh", "N. Ahuja", "P. Moulin" ],
      "venue" : "MLSP, September 2012.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Efficient KLMS and KRLS algorithms: A random Fourier feature perspective",
      "author" : [ "P. Bouboulis", "S. Pougkakiotis", "S. Theodoridis" ],
      "venue" : "SSP, 2016.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Digital Signal Processing Fundamentals",
      "author" : [ "S.C. Douglas", "M. Rupp" ],
      "venue" : "ch. Convergence Issues in the LMS Adaptive Filter,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", [1].",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 1,
      "context" : ", [2], [3], [4].",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 2,
      "context" : ", [2], [3], [4].",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 3,
      "context" : ", [2], [3], [4].",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "Finally, the task of interest is considered to be common across the nodes and, thus, cooperation among each other is meaningful and beneficial, [5], [6].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 5,
      "context" : "Finally, the task of interest is considered to be common across the nodes and, thus, cooperation among each other is meaningful and beneficial, [5], [6].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 6,
      "context" : ", [7], [8], [9], ADMM-based schemes, e.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 7,
      "context" : ", [7], [8], [9], ADMM-based schemes, e.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 8,
      "context" : ", [7], [8], [9], ADMM-based schemes, e.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 9,
      "context" : ", [10], [11], as well as consencus-based ones, e.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 10,
      "context" : ", [10], [11], as well as consencus-based ones, e.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 11,
      "context" : ", [12], [13].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 12,
      "context" : ", [12], [13].",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 13,
      "context" : ", [14], [15].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 14,
      "context" : ", [14], [15].",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 15,
      "context" : "The literature on online distributed classification is more limited; in [16], a batch distributed SVM algorithm is presented, whereas in [17], a diffusion based scheme suitable for classification is proposed.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "The literature on online distributed classification is more limited; in [16], a batch distributed SVM algorithm is presented, whereas in [17], a diffusion based scheme suitable for classification is proposed.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 17,
      "context" : "These are inner-product function spaces, generated by a specific kernel function, that have become popular models for nonlinear tasks, since the introduction of the celebrated Support Vectors Machines (SVM) [18], [19], [20], [6].",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 18,
      "context" : "These are inner-product function spaces, generated by a specific kernel function, that have become popular models for nonlinear tasks, since the introduction of the celebrated Support Vectors Machines (SVM) [18], [19], [20], [6].",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 19,
      "context" : "These are inner-product function spaces, generated by a specific kernel function, that have become popular models for nonlinear tasks, since the introduction of the celebrated Support Vectors Machines (SVM) [18], [19], [20], [6].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 5,
      "context" : "These are inner-product function spaces, generated by a specific kernel function, that have become popular models for nonlinear tasks, since the introduction of the celebrated Support Vectors Machines (SVM) [18], [19], [20], [6].",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 20,
      "context" : ", [21], [22], [23], these have major drawbacks.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 21,
      "context" : ", [21], [22], [23], these have major drawbacks.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 22,
      "context" : ", [21], [22], [23], these have major drawbacks.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 20,
      "context" : "In [21] and [23], the estimation of f , at each node, is given as an increasingly growing sum of kernel functions centered at the observed data.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 22,
      "context" : "In [21] and [23], the estimation of f , at each node, is given as an increasingly growing sum of kernel functions centered at the observed data.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 21,
      "context" : "In contrast, the method of [22] assumes that these growing sums are limited by a sparsification strategy; how this can be achieved is left for the future.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 23,
      "context" : "This is achieved through an efficient approximation of the growing sum using the random Fourier features rationale [24].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 17,
      "context" : "This is usually portrayed by the reproducing property [18], [6], [25], which links inner products in H with a specific (semi-)positive definite kernel function κ defined on X ×X (associated with the space H).",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "This is usually portrayed by the reproducing property [18], [6], [25], which links inner products in H with a specific (semi-)positive definite kernel function κ defined on X ×X (associated with the space H).",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 24,
      "context" : "This is usually portrayed by the reproducing property [18], [6], [25], which links inner products in H with a specific (semi-)positive definite kernel function κ defined on X ×X (associated with the space H).",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : ", N, xn ∈ X, yn ∈ R}, the representer theorem [26], [18], states that the unique",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 17,
      "context" : ", N, xn ∈ X, yn ∈ R}, the representer theorem [26], [18], states that the unique",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 26,
      "context" : "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 27,
      "context" : "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 28,
      "context" : "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 29,
      "context" : "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 30,
      "context" : "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 31,
      "context" : "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 32,
      "context" : "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 33,
      "context" : "Hence, there have been kernel-based implementations of LMS [27], [28], RLS [29], [30], APSM [31], [32] and other related methods [33], as well as online implementations of SVMs [34], focusing on the primal formulation of the task.",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 5,
      "context" : ", [6]), the estimate at the next iteration, employing the gradient descent method, becomes fn = fn−1+μǫnκ(xn, ·), where ǫn = yn−fn−1(xn) and μ is the step-size (see, e.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 5,
      "context" : ", [6], [35], [36] for more).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 34,
      "context" : ", [6], [35], [36] for more).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 35,
      "context" : "Typically, a sparsification strategy is adopted to bound the size of the expansion [37], [38], [39].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 36,
      "context" : "Typically, a sparsification strategy is adopted to bound the size of the expansion [37], [38], [39].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 37,
      "context" : "Typically, a sparsification strategy is adopted to bound the size of the expansion [37], [38], [39].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 38,
      "context" : "There are also methods that can remove specific points from the expansion, if their information becomes obsolete, in order to increase the tracking ability of the algorithm [40].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 39,
      "context" : "The most popular techniques of this category are the Nyström method [41], [42] and the random Fourier features approach [24], [43]; the latter fits naturally to the online setting.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 40,
      "context" : "The most popular techniques of this category are the Nyström method [41], [42] and the random Fourier features approach [24], [43]; the latter fits naturally to the online setting.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 23,
      "context" : "The most popular techniques of this category are the Nyström method [41], [42] and the random Fourier features approach [24], [43]; the latter fits naturally to the online setting.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 41,
      "context" : "The most popular techniques of this category are the Nyström method [41], [42] and the random Fourier features approach [24], [43]; the latter fits naturally to the online setting.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 23,
      "context" : "Instead of relying on the implicit lifting, Φ, provided by the kernel trick, Rahimi and Recht in [24] proposed to map the input data to a finite-dimensional Euclidean space (with dimension lower than H but larger than the input space) using a randomized feature map zΩ : R d → R, so that the kernel evaluations can be approximated as κ(xn,xm) ≈ zΩ(xn) zΩ(xm).",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 23,
      "context" : "Details on the quality of this approximation can be found in [24], [43], [44], [45].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 41,
      "context" : "Details on the quality of this approximation can be found in [24], [43], [44], [45].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 42,
      "context" : "Details on the quality of this approximation can be found in [24], [43], [44], [45].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 43,
      "context" : "Details on the quality of this approximation can be found in [24], [43], [44], [45].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "This is the rationale adopted in [21], [22], [23] for the case of KLMS.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "This is the rationale adopted in [21], [22], [23] for the case of KLMS.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 22,
      "context" : "This is the rationale adopted in [21], [22], [23] for the case of KLMS.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "As A is doubly stochastic, we have the following [9]:",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : "In [9], it has been proved that, the algorithmic scheme achieves asymptotic consensus, i.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 33,
      "context" : ", L(x, y, θ) = λ2 ‖θ‖2 +max{0, 1− yθzΩ(x)}, for a specific value of the regularization parameter λ > 0, generates the Distributed Pegasos (see [34]).",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 33,
      "context" : "where, following [34], we have used a decreasing step size, μn = 1 λn .",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 33,
      "context" : "decreasing step-size μn), which has been suggested that improves the classification accuracy of Pegasos (see [34]).",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 7,
      "context" : "Although proposition 1 cannot be applied here (as it requires a decreasing step-size), we can derive sufficient conditions for consensus following the results of the standard Diffusion LMS [8].",
      "startOffset" : 189,
      "endOffset" : 192
    }, {
      "referenceID" : 44,
      "context" : ", [48], [49]) cannot be applied directly (in these works the inputs are assumed to be zero mean Gaussian to simplify the formulas related to stability).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 45,
      "context" : ", [48], [49]) cannot be applied directly (in these works the inputs are assumed to be zero mean Gaussian to simplify the formulas related to stability).",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 46,
      "context" : "3) Example 3: Here we adopt the following chaotic series model [50]: dk,n = dk,n−1 1+d2k,n−1 + uk,n−1, yk,n = dk,n + ηk,n, where ηn is zero-mean i.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 46,
      "context" : "4) Example 4: For the final example, we use another chaotic series model [50]: dk,n = uk,n+0.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 47,
      "context" : "We call this scheme the Random Fourier Features KLMS (RFF-KLMS) [51], [52].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 48,
      "context" : "We call this scheme the Random Fourier Features KLMS (RFF-KLMS) [51], [52].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 37,
      "context" : "We choose the QKLMS [39] as a reference, since this is one of the most effective and fast KLMS pruning methods.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "has a stabilizing effect on the network [8].",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 49,
      "context" : "For sufficiently small step-sizes, the rightmost term can be neglected [53], [49], hence we can take the simplified form",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 45,
      "context" : "For sufficiently small step-sizes, the rightmost term can be neglected [53], [49], hence we can take the simplified form",
      "startOffset" : 77,
      "endOffset" : 81
    } ],
    "year" : 2017,
    "abstractText" : "We present a novel diffusion scheme for online kernel-based learning over networks. So far, a major drawback of any online learning algorithm, operating in a reproducing kernel Hilbert space (RKHS), is the need for updating a growing number of parameters as time iterations evolve. Besides complexity, this leads to an increased need of communication resources, in a distributed setting. In contrast, the proposed method approximates the solution as a fixed-size vector (of larger dimension than the input space) using Random Fourier Features. This paves the way to use standard linear combine-then-adapt techniques. To the best of our knowledge, this is the first time that a complete protocol for distributed online learning in RKHS is presented. Conditions for asymptotic convergence and boundness of the networkwise regret are also provided. The simulated tests illustrate the performance of the proposed scheme.",
    "creator" : "LaTeX with hyperref package"
  }
}