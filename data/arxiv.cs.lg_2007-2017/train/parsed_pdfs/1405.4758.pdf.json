{
  "name" : "1405.4758.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Lipschitz Bandits: Regret Lower Bounds and Optimal Algorithms",
    "authors" : [ "Stefan Magureanu", "Richard Combes", "Alexandre Proutiere", "MAGUREANU COMBES PROUTIERE" ],
    "emails" : [ "MAGUR@KTH.SE", "RICHARD.COMBES@SUPELEC.FR", "ALEPRO@KTH.SE" ],
    "sections" : [ {
      "heading" : null,
      "text" : "function of the arm, and where the set of arms is either discrete or continuous. For discrete Lipschitz bandits, we derive asymptotic problem specific lower bounds for the regret satisfied by any algorithm, and propose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz structure of the problem. In fact, we prove that OSLB is asymptotically optimal, as its asymptotic regret matches the lower bound. The regret analysis of our algorithms relies on a new concentration inequality for weighted sums of KL divergences between the empirical distributions of rewards and their true distributions. For continuous Lipschitz bandits, we propose to first discretize the action space, and then apply OSLB or CKL-UCB, algorithms that provably exploit the structure efficiently. This approach is shown, through numerical experiments, to significantly outperform existing algorithms that directly deal with the continuous set of arms. Finally the results and algorithms are extended to contextual bandits with similarities."
    }, {
      "heading" : "1. Introduction",
      "text" : "In their seminal paper, Lai and Robbins (1985) solve the classical stochastic Multi-Armed Bandit (MAB) problem. In this problem, the successive rewards of a given arm are i.i.d., and the expected rewards of the various arms are not related. They derive an asymptotic (when the time horizon grows large) lower bound of the regret satisfied by any algorithm, and present an algorithm whose regret matches this lower bound. This initial algorithm was quite involved, and many researchers have, since then, tried to devise simpler and yet efficient algorithms. The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.g. KL-UCB Garivier and Cappé (2011), Cappé et al. (2013) – note that the KL-UCB algorithm was initially proposed and analysed in Lai (1987), see (2.6). When the expected rewards of the various arms are not related as in Lai and Robbins (1985), the regret of the best algorithm essentially scales as O(K log(T )) where K denotes the number of arms, and T is the time horizon. When K is very large or even infinite, MAB problems become more challenging. Fortunately, in such scenarios, the expected rewards often exhibit some structural properties that the decision maker can exploit to design efficient algorithms. Various structures have\nc© 2014 S. Magureanu, R. Combes & A. Proutiere.\nar X\niv :1\n40 5.\n47 58\nv1 [\ncs .L\nG ]\n1 9\nM ay\nbeen investigated in the literature, e.g., Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005).\nIn this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008) adaptively define the set of arms to play, but used simplistic UCB indexes to sequentially select arms. In turn, these algorithms fail at exploiting the problem structure revealed by the past observed samples. For continuous bandits, we propose to first discretize the set of arms (as in Kleinberg et al. (2008)), and then apply OSLB, an algorithm that optimally exploits past observations and hence the problem specific structure. As it turns out, this approach outperforms algorithms directly dealing with continuous sets of arms.\nOur contributions. (a) For discrete Lipschitz bandit problems, we derive an asymptotic regret lower bound satisfied by any algorithm. This bound is problem specific in the sense that it depends in an explicit manner on the expected rewards of the various arms (this contrasts with existing lower bounds for continuous Lipschitz bandits).\n(b) We propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. We further present CKL-UCB (Combined KL-UCB), an algorithm that exhibits lower computational complexity than that of OSLB, and that is yet able to exploit the Lipschitz structure.\n(c) We provide a finite time analysis of the regret achieved under OSLB and CKL-UCB. The analysis relies on a new concentration inequality for a weighted sum of KL divergences between the empirical distributions of rewards and their true distributions. We believe that this inequality can be instrumental for various bandit problems with structure.\n(d) We evaluate our algorithms using numerical experiments for both discrete and continuous sets of arms. We compare their performance to that obtained using existing algorithms for continuous bandits.\n(e) We extend our results and algorithms to the case of contextual bandits with similarities as investigated in Slivkins (2011)."
    }, {
      "heading" : "2. Models",
      "text" : "We consider a stochastic multi-armed bandit problem where the set of arms is a subset {x1, . . . , xK} of the interval [0, 1]. Results can be easily extended to the case where the set of arms is a subset of a metric space as considered in Kleinberg et al. (2008). The set of arms is of finite cardinality, possibly\nlarge, and we assume without loss of generality that x1 < x2 < . . . < xK . Problems with continuous sets of arms are discussed in Section 7. Time proceeds in rounds indexed by n = 1, 2, . . .. At each round, the decision maker selects an arm, and observes the corresponding random reward. Arm xk is referred to as arm k for simplicity. For any k, the reward of arm k in round n is denoted by Xk(n), and the sequence of rewards (Xk(n))n≥1 is i.i.d. with Bernoulli distribution of mean θk (the results can be generalized to distributions belonging to a certain parametrized family of distributions, but to simplify the presentation, we restrict our attention to Bernoulli rewards). The vector θ = (θ1, . . . , θK) represents the expected rewards of the various arms. Let K = {1, . . . ,K}. We denote by θ? = maxk∈K θk the expected reward of the best arm. A sequential selection algorithm π selects in round n an arm kπ(n) ∈ K that depends on the past observations. In other words, for any n ≥ 1, if Fπn denotes the σ-algebra generated by (kπ(t), Xkπ(t)(t))1≤t≤n, then kπ(n + 1) is Fπn -measurable. Let Π denote the set of all possible sequential selection algorithms.\nWe assume that the expected reward is a Lipschitz function of the arm, and this structure is known to the decision maker. More precisely, there exists a positive constant L such that for all pairs of arms (k, k′) ∈ K,\n|θk − θk′ | ≤ L× |xk − xk′ |. (1)\nWe assume that L is also known. We denote by ΘL the set of vectors in [0, 1]K satisfying (1). The objective is to devise an algorithm π ∈ Π that maximizes the average cumulative reward up to a certain round T referred to as the time horizon (T is typically large). Such an algorithm should optimally exploit the Lipschitz structure of the problem. As always in bandit optimization, it is convenient to quantify the performance of an algorithm π ∈ Π through its expected regret (or regret for short) defined by:\nRπ(T ) = Tθ? − E[ T∑ n=1 Xkπ(n)(n)]."
    }, {
      "heading" : "3. Regret Lower Bound",
      "text" : "In this section, we derive an asymptotic (when T grows large) regret lower bound satisfied by any algorithm π ∈ Π. We denote by I(x, y) = x log(xy ) + (1 − x) log( 1−x 1−y ) the KL divergence between two Bernoulli distributions with respective means x and y. Fix the average reward vector θ = (θ1, . . . , θK). Let K− = {k ∈ K : θk < θ?} be the set of sub-optimal arms. For any k ∈ K−, we define λk = (λ1, . . . , λK) as: ∀i ∈ K, λki = max{θi, θ? −L|xk − xi|}. The expected reward vector λk is illustrated in Figure 3, and may be interpreted as the most confusing reward vector among vectors in ΘL such that arm k (which is sub-optimal under θ) is optimal under λk. This interpretation will be made clear in the proof of the following theorem. Without loss of generality, we restrict our attention to so-called uniformly good algorithms, as defined in Lai and Robbins (1985). π ∈ Π is uniformly good if for all θ ∈ ΘL, Rπ(T ) = o(T a) for all a > 0. Uniformly good algorithms exist – for example, the UCB algorithm is uniformly good.\nTheorem 1 Let π ∈ Π be a uniformly good algorithm. For any θ ∈ ΘL, we have:\nlim inf T→∞\nRπ(T ) log(T ) ≥ C(θ), (2)\nwhere C(θ) is the minimal value of the following optimization problem:\nmin ck≥0,∀k∈K− ∑ k∈K− ck × (θ? − θk) (3)\ns.t. ∀k ∈ K−, ∑ i∈K ciI(θi, λ k i ) ≥ 1. (4)\nThe regret lower bound is a consequence of results in optimal control of Markov chains, see Graves and Lai (1997). All proofs are presented in appendix. As in classical bandits, the minimal regret scales logarithmically with the time horizon. Observe that the lower bound (2) is smaller than the lower bound derived in Lai and Robbins (1985) when the various average rewards (θk, k ∈ K) are not related (i.e., in absence of the Lipschitz structure). Hence (2) quantifies the gain one may expect by designing algorithms optimally exploiting the structure of the problem. Note that for any k ∈ K−, the variable ck corresponding to a solution of (3) characterizes the number of times arm k should be played under an optimal algorithm: arm k should be roughly played ck log(n) times up to round n.\nIt should be also observed that our lower bound is problem specific (it depends on θ), which contrasts with existing lower bounds for continuous Lipschitz bandits, see e.g. Kleinberg et al. (2008). The latter are typically derived by selecting the problems that yield maximum regret. However, our lower bound is only valid for bandits with a finite set of arms, and cannot easily be generalized to problems with continuous sets of arms."
    }, {
      "heading" : "4. Algorithms",
      "text" : "In this section, we present two algorithms for discrete Lipschitz bandit problems. The first of these algorithms, referred to as OSLB (Optimal Sampling for Lipschitz Bandits), has a regret that matches\nAlgorithm 1 OSLB( ) For all n ≥ 1, select arm k(n) such that: If θ̂?(n) ≥ maxk 6=L(n) bk(n), then k(n) = L(n); Else If tk(n)(n) < K tk(n)(n), then k(n) = k(n);\nElse k(n) = k(n).\nthe lower bound derived in Theorem 1, i.e., it is asymptotically optimal. OSLB requires that in each round, one solves an LP similar to (3). The second algorithm, CKL-UCB (Combined KLUCB) is much simpler to implement, but has weaker theoretical performance guarantees, although it provably exploits the Lipschitz structure."
    }, {
      "heading" : "4.1. The OSLB Algorithm",
      "text" : "To formally describe OSLB, we introduce the following notations. For any n ≥ 1, let k(n) be the arm selected under OSLB in round n. tk(n) denotes the number of times arm k has been selected up to round n − 1. By convention, tk(1) = 0. The empirical reward of arm k at the end of round (n − 1) is θ̂k(n) = 1tk(n) ∑n−1 t=1 1{k(t) = k}Xk(t), if tk(n) > 0 and θ̂k(n) = 0 otherwise. We denote by L(n) = arg maxk∈K θ̂k(n) the arm with the highest empirical reward (ties are broken arbitrarily) at the end of round n − 1. Arm L(n) is referred to as the leader for round n. We also define θ̂?(n) = θ̂L(n)(n) as the empirical reward of the leader at the end of round n − 1. Let f(n) = log(n) + (3K + 1) log log(n). Further define, for all q ≥ 0 and k, the Lipschitz vector λq,k such that for any k′, λq,kk′ = q−L|xk − xk′ |. The sequential decisions made under OSLB are based on the indexes of the various arms. The index bk(n) of arm k for round n is defined by:\nbk(n) = sup{q ∈ [θ̂k(n), 1] : K∑ k′=1 tk′(n)I +(θ̂k′(n), λ q,k k′ ) ≤ f(n)}.\nNote that the index bk(n) is always well defined, even for small values of n, e.g. n = 1 (we have for all x > 0, I+(0, x) = − log(1 − x)). For any θ ∈ ΘL, let C(θ) denote the minimal value of the optimization problem (3), and let (ck(θ), k ∈ K−) be the values of the variables (ck, k ∈ K−) in (3) yielding C(θ). For simplicity, we define Ĉ(n) = C(θ̂(n)), and ĉk(n) = ck(θ̂(n)) for any k ∈ K−(n) where K−(n) = {k : θ̂k(n) < θ̂?(n)}. The design of OSLB stems from the observation that an optimal algorithm should satisfy limn→∞ tk(n)/(ck(θ) log(n)) = 1, almost surely, for all k ∈ K−. Hence we should force the exploration of arm k ∈ K−(n) in round n if tk(n) < ĉk(n) log(n). We define the arm k(n) to explore as k(n) = arg mink∈Ke(n) tk(n) where Ke(n) = {k ∈ K−(n) : tk(n) ≤ ĉk(n) log(n)}. If Ke(n) = ∅, k(n) = −1 (a dummy arm). Finally we define the least played arm as k(n) = arg mink tk(n). In the definitions of k(n) and k(n), ties are broken arbitrarily. We are now ready to describe OSLB. Its pseudo-code is presented in Algorithm 1.\nUnder OSLB, the leader is selected if its empirical average exceeds the index of other arms. If this is not the case, OSLB selects the least played arm k(n), if the latter has not been played enough, and arm k(n) otherwise. Note that the description of OSLB is valid in the sense that k(n) 6= −1 if θ̂?(n) < maxk 6=L(n) bk(n). After each round, all variables are updated, and in particular ĉk(n) for any k ∈ K−(n), which means that at each round we solve an LP, similar to (3).\nAlgorithm 2 CKL-UCB For all n ≥ 1, select arm k(n) such that: If ∃k such that tk(n) < log log(n), then k(n) = k (ties are broken arbitrarily); Else if bL(n)(n) ≥ max\nk 6=L(n) bk(n), then k(n) = L(n);\nElse k(n) = arg min k {tk(n) : bk(n) > bL(n)(n)} (ties are broken arbitrarily)."
    }, {
      "heading" : "4.2. The CKL-UCB Algorithm",
      "text" : "Next, we present the algorithm CKL-UCB (Combined KL - UCB). The sequential decisions made under CKL-UCB are based on the indexes bk(n), and CKL-UCB explores the apparently suboptimal arms by choosing the least played arms first. When the leaderL(n) has the largest index, it is played, and otherwise we play the arm in {k : bk(n) > bL(n)(n)}, the set of arms which are possibly better than the leader, with the least number of current plays. Note that in practice, the forced log log(n) exploration is unnecessary and only appears to aide in the regret analysis.\nThe rationale behind CKL-UCB is that if we are given a set of suboptimal arms, by exploring them, we will first eliminate arms whose expected reward is low (these arms do not require many plays to be eliminated). Note that the arm chosen by CKL-UCB is directly computed from the indexes, without solving an LP, and hence CKL-UCB is computationally light. From a practical perspective, CKL-UCB should also be more robust than OSLB in the sense that it does not take decisions based on the solution of the LP calculated with empirical averages θ̂(n). This could be problematic if the LP solution is very sensitive to errors in the estimate of θ."
    }, {
      "heading" : "5. Regret Analysis",
      "text" : "In this section, we provide finite time upper bounds for the regret achieved under OSLB and CKLUCB."
    }, {
      "heading" : "5.1. Concentration Inequalities",
      "text" : "To analyse the regret of algorithms for bandit optimization problems, one often has to leverage results related to the concentration-of-measure phenomenon. More precisely, here, in view of the definition of the indexes bk(n), we need to establish a concentration inequality for a weighted sum of KL divergences between the empirical distributions of rewards and their true distributions. We derive such an inequality. The latter extends to the multi-dimensional case the concentration inequality derived in Garivier (2013) for a single KL divergence. We believe that this inequality can be instrumental in the analysis of general structured bandit problems, as well as for statistical tests involving vectors whose components have distributions in a one-parameter exponential family (such as Bernoulli or Gaussian distributions). For simplicity, the inequality is stated for Bernoulli random variables only.\nWe use the following notations. For k ∈ K, let {Xk(n)}n∈N be a sequence of i.i.d. Bernoulli random variables with expectation θk and X(n) = (Xk(n), k ∈ K). We represent the history up to round n using the σ-algebra Fn = σ(X(1), . . . , X(n)), and define the natural filtration F = {Fn}n≥1. We consider a generic sampling rule B(n) = (Bk(n), k ∈ K) where Bk(n) ∈ {0, 1} for all k ∈ K. The sampling rule is assumed to be predictable in the sense that B(n) ∈ Fn−1.\nWe define the number of times that k was sampled up to round n− 1 by tk(n) = ∑n−1 t=1 Bk(t)\nand the sum Sk(n) = ∑n−1\nt=1 Bk(t)Xk(t). The empirical average for k is θ̂k(n) = Sk(n)/tk(n) if tk(n) > 0 and θ̂k(n) = 0 otherwise. Finally, we define the vectors θ̂(n) = (θ̂1(n), . . . , θ̂K(n)) and t(n) = (t1(n), . . . , tK(n)). When comparing vectors in RK , we use the component-by-component order unless otherwise specified.\nTheorem 2 For all δ ≥ (K + 1) and n ∈ N we have:\nP [ K∑ k=1 tk(n)I +(θ̂k(n), θk) ≥ δ ] ≤ e−δ ( dδ log(n)eδ K )K eK+1. (5)\nThe proof of Theorem 2 involves tools that are classically used in the derivation of concentration inequalities, but also requires the use of stochastic ordering techniques, see e.g. Müller and Stoyan (2002)."
    }, {
      "heading" : "5.2. Finite time analysis of OSLB",
      "text" : "Next we provide a finite time analysis of the regret achieved under OSLB, under the following mild assumption. This assumption greatly simplifies the analysis.\nAssumption 1 The solution of the LP (3) is unique.\nIt should be observed that the set of parameters θ ∈ ΘL such that Assumption 1 is satisfied constitutes a dense subset of ΘL.\nTheorem 3 For all > 0, under Assumption 1, the regret achieved under π = OSLB( ) satisfies: for all θ ∈ ΘL, for all δ > 0 and T ≥ 1,\nRπ(T ) ≤ Cδ(θ)(1 + ) log(T ) + C1 log log(T ) +K3 −1δ−2 + 3Kδ−2, (6)\nwhere Cδ(θ)→ C(θ), as δ → 0+, and C1 > 0.\nIn view of the above theorem, when is small enough, OSLB( ) approaches the fundamental performance limit derived in Theorem 1. More precisely, we have for all > 0 and δ > 0:\nlim sup T→∞\nRπ(T ) log(T ) ≤ Cδ(θ)(1 + ).\nIn particular, for any ζ > 0, one can find > 0 and δ > 0 such that Cδ(θ)(1 + ) ≤ (1 + ζ)C(θ), and hence, under π =OSLB( ),\nlim sup T→∞\nRπ(T ) log(T ) ≤ C(θ)(1 + ζ)."
    }, {
      "heading" : "5.3. Finite Time analysis of CKL-UCB",
      "text" : "In order to analyze the regret of CKL-UCB, we define the following optimization problem. Define the matrix of Kullback-Leibler divergence numbers A = (aik)i,k∈K with aik = I(θi, λ k,θ?\ni ). Consider an arm k 6= k?, a subset of arms N ⊂ {1, . . . ,K} \\ {k, k?}, and α0 ≥ 0. We define dk(A,α0,N ) the optimal value of the following linear program:\nmin α1,...,αK ∑ k′∈K−\\{k} αk′ak′k\ns.t. αk′ ≥ α0, ∀k′ 6∈ N , k′ 6= k?\nαk′ ≥ 0, ∀k′∑ k′′∈K−\\{k} αk′′ak′′k′ ≥ 1− α0akk′ , ∀k′ ∈ N .\nand ek(A,α0) = minN dk(A,α0,N ) where the minimum is taken over all possible subsets of {1, . . . ,K} \\ {k, k?}.\nTheorem 4 Under CKL-UCB, for all θ ∈ ΘL, all T ≥ 1, all 0 < δ < (θ? −maxk 6=k? θk)/2, and any suboptimal arm k ∈ K−, (i) we have:\nE[tk(T )] ≤ f(T )\nI(θk + δ, θ∗ − δ) + C1 log(log(T )) + 2δ\n−2.\nwith C1 ≥ 0 a constant. (ii) Furthermore, for all k ∈ K−, we have that:\nlim sup T→∞ E[tk(T )] log(T ) ≤ βk(θ).\nwhere βk(θ) = inf{α0 ≥ 0 : ak,kα0 + ek(A,α0) > 1}.\n(iii) Assume that there exists k′ such that 0 < akk′ < akk and such that for all k′′ we have that if ak′′k = 0 then ak′′k′ = 0 as well. Then βk(θ) < 1/akk = 1/I(θk, θ?).\nIn the above theorem, statement (i) shows that CKL-UCB plays arm k at most as much as KLUCB, so that CKL-UCB outperforms KL-UCB for any value of the parameters θ. Now statements (ii) and (iii) show that under certain assumptions, CKL-UCB plays arm k strictly less than KL-UCB, so that CKL-UCB indeed exploits the Lipshitz structure of the problem. Note that the conditions in (iii) holds for triangular reward functions, and other unimodal functions, and hence in these cases, CKL-UCB strictly outperforms KL-UCB. The regret analysis of CKL-UCB presented above is preliminary, and we believe that its performance guarantees can be further improved."
    }, {
      "heading" : "6. Contextual Bandit with Similarities",
      "text" : "The algorithms and results presented above can be extended to the case of contextual bandit problems with similarities as studied in Slivkins (2011). In such problems, in each round, the decision maker observes a context, and then decides which arm to select. The expected reward of the various\narms depends on the context, and is assumed to be Lipschitz in the arm and context. We assume that contexts arrive according to an i.i.d. process whose distribution is not known to the decision maker. This contrasts with most of the work in contextual bandits, where the context process is adversarial."
    }, {
      "heading" : "6.1. Model",
      "text" : "Let {y1, . . . , yJ} denote the set of possible contexts, assumed to be a subset of [0, 1]. We assume that y1 < . . . < yJ . For simplicity, context yj is referred to as context j. For each context j ∈ J = {1, . . . , J}, the expected rewards of the various arms are represented by a vector θ(j) = (θk(j), k ∈ K) (θk(j) is the expected reward of arm k when the context is j). We consider a general scenario where the reward is a Lipschitz function in both the arm and the context. There exists L (known to the decision maker) such that for all (i, k), (j, l) ∈ J ×K,\n|θk(i)− θl(j)| ≤ L×D((i, k), (j, l)), (7)\nwhere D refers to some metric over J × K. The choice of this metric is free, and allows us to consider different scenarios. For example, we may assume that the Lipschitz structure is stronger in terms of arms than in terms of contexts. In this case, we may choose, for some β > 1, D((i, k), (j, l)) = √ (β(yi − yj)2 + (xk − xl)2) . The set of θ = (θk(j), k ∈ K, j ∈ J ) satisfying (7) is denoted by ΘL,2. The context process is i.i.d.. The distribution of the observed context j(n) in round n is ψ, i.e., ψ(j) = P[j(n) = j]. Without loss of generality, we assume that for any j ∈ J , ψ(j) > 0. ψ is unknown to the decision maker. Let Xj,k(n) denote the reward of arm k obtained in round n when the context is j. For contextual bandits, we define the regret of algorithm π as follows:\nRπ(T ) = T ∑ j∈J ψ(j)θ?(j)− T∑ n=1 E[Xj(n),kπ(n)(n)]. (8)\nwhere θ?(j) denotes the reward of the best arm under context j, and as earlier kπ(n) denotes the arm selected under π in round n."
    }, {
      "heading" : "6.2. Regret Lower Bound",
      "text" : "To state the regret lower bound, we introduce for any context j ∈ J , K−(j) = {k ∈ K : θk(j) < θ?(j)} the set of suboptimal arms for context j. We also introduce for any context j ∈ J , and any k ∈ K, the vector (λj,kl (i), l ∈ K, i ∈ J ) such that\nλj,kl (i) = max{θl(i), θ ?(j)− LD((j, k), (i, l))}.\nTheorem 5 Let π be a uniformly good algorithm. Then, for any θ ∈ ΘL,2:\nlim inf T→∞\nRπ(T ) log(T ) ≥ C ′(θ) (9)\nwhere C ′(θ) is the minimal value of the following optimization problem:\nmin cj,k≥0,∀j,∀k ∑ j∈J ∑ k∈K−(j) cj,k × (θ∗(j)− θk(j)) (10)\ns.t. ∀j,∀k ∈ K−(j), ∑ i∈J ∑ l∈K ci,lI(θl(i), λ j,k l (i)) ≥ 1. (11)\nAlgorithm 3 CCKL-UCB For all n ≥ 1, observe context j = j(n), and select arm k(n) such that: If ∃k such that tk(j, n) < log log(n), then k(n) = k (ties are broken arbitrarily); Else if L(n, j) = arg max\nk bck(n, j), then k(n) = L(n, j);\nElse k(n) = arg min k {tk(j, n) : bck(j, n) > bcL(n)(j, n)} (ties are broken arbitrarily).\nObserve that our regret lower bound is problem specific, and again the values of the cj,k’s solving the above optimization problem can be interpreted as follows: an asymptotically optimal algorithm plays arm k when the context is j a number of times that scales as cj,k log(T ) as T grows large. Also note that the regret lower bound does not depend on the distribution ψ of the contexts."
    }, {
      "heading" : "6.3. Algorithms",
      "text" : "The algorithms proposed for Lipschitz bandits can be naturally extended to the case of contextual bandits with similarities. For conciseness, we just present CCKL-UCB (Contextual Combined KL - UCB), the extension of CKL-UCB. Its regret analysis can be conducted as that of CKL-UCB with minor modifications.\nTo describe CCKL-UCB, we introduce the following notations. Let θ̂k(j, n) denote the empirical average reward of arm k for context j up to round n− 1. tk(j, n) is the number of times context j is presented and arm k is chosen up to round n − 1. We define the index bck(j, n) of arm k for round n, when the context j is observed as:\nbck(n, j) = sup{q ∈ [θ̂k(j, n), 1] : ∑ i∈J ∑ l∈K tl(i, n)I +(θ̂l(i, n), λ q,k,j l (i, n)) ≤ f(n)},\nwhere λq,k,jl (i, n) = q − LD((j, k), (i, l)). As for Lipschitz bandits, the indexes are built so as to match the constraints (11) of the optimisation problem leading to the regret lower bound. The leader for round n and context j is defined L(n, j) = arg max\nk θ̂k(j, n) (ties are broken arbitrarily). In\nround n, CCKL-UCB plays the leader L(n, j(n)) for the current context if it has the highest index, and otherwise selects the least played arm which has an index higher than the leader L(n, j(n))."
    }, {
      "heading" : "7. Numerical Experiments",
      "text" : "In this section, we present numerical experiments illustrating the performance of our algorithms compared to other existing algorithms."
    }, {
      "heading" : "7.1. Discrete Lipschitz Bandits",
      "text" : "We first consider discrete bandit problems with 46 arms, and with time horizons less than T = 5.105 rounds. The regret is averaged over 150 runs. In Figure 2, we compare the performance of KL-UCB and CKL-UCB. For improved numerical performance, in the case of both algorithms we ignore the log log(n) terms in the indexes (i.e.f(n) = log(n)). On the left, we plot the expected reward as a function of the arm, as well as the (scaled) amount of times E[tk(n)]/ log(n) sub-optimal arm k is played under both algorithms, as function of time. Under KL-UCB, the amount of times for arm k approaches 1/I(θk, θ?), whereas under CKL-UCB, E[tk(n)] satisfy the upper bounds derived\nin Theorem 4. CKL-UCB explores suboptimal arms less often than KL-UCB, as it is designed to exploit the Lipschitz structure. On the right, we plot the expected regret as a function of time under both algorithms. The regret under CKL-UCB is always smaller than that under KL-UCB (the regret under KL-UCB is typically twice as large as that under CKL-UCB in this example). This illustrates the significant gains that one may achieve by efficiently exploiting the structure of the problem."
    }, {
      "heading" : "7.2. Continuous Lipschitz Bandits",
      "text" : "We now turn our attention to continuous Lipschitz bandits where the set of arms is [0,1]. We consider two reward functions that behave differently around their maximum: (1) θ(x) = 0.8− 0.5|0.5− x| (triangle) and (2) θ(x) = max(0.1, 0.9− 3.2 ∗ (0.7− x)2) (quadratic function). To adapt KL-UCB and CKL-UCB to this continuous setting, we use a uniform discretization of the\nset of arms, with δ−1 = d √ T/ log(T )e arms. This discretization is known to be order-optimal for functions which are regular around their maximum Kleinberg (2004). In order not to give a positive bias to KL-UCB and CKL-UCB, we make sure that the maximum of the reward functions is not achieved in one of the arms in the discretization: the maximum is placed at a distance of at least δ/4 from any arm in the discretization. We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al. (2008). The two latter algorithms have performance guarantees (they are order-optimal). We also compare KL-UCB and CKL-UCB to HOO+ and Zooming+, two improved versions of HOO and Zooming, respectively. In these tuned versions, the confidence radius (see Bubeck et al. (2008) and Kleinberg et al. (2008) for details) is set equal to √ log(n)/(2 ∗ tk(n)) in round n. HOO+ and Zooming+ exhibit better performance than their initial versions, but their regrets have not been analytically studied. In the experiments, we limit the time horizon to T = 25000 rounds, and the expected regret is calculated by averaging over 100 independent runs.\nFigure 3 presents the expected regret of the various algorithms for the triangular reward function (left) and for the quadratic reward function (right). First note that surprisingly, KL-UCB, an algorithm that does not leverage the Lipschitz structure, outperforms some of the algorithms designed to exploit the structure. Observe that CKL-UCB clearly outperforms KL-UCB and all other algorithms in both problem instances. For quadratic reward functions, it is known that the optimal discretization of the set of arms should roughly have (log(T )/T )1/4 arms, Combes and Proutiere (2014a). We also plot the regret achieved under CKL-UCB using this optimized discretization, and we observe that this indeed further reduces the regret.\nIt is worth noting that in the case of CKL-UCB most of the regret is caused by not discretizing enough around the top arm. In contrast, in the case of Zooming and HOO, most of the regret is caused by loose confidence bounds. Therefore, in future work we will explore the possibility of combining the adaptive discretization scheme of Zooming and HOO with efficient confidence bounds as used by CKL-UCB."
    }, {
      "heading" : "8. Conclusion",
      "text" : "We consider stochastic multi-armed bandits (discrete or continuous) where the expected reward is a Lipschitz function of the arm. For discrete Lipschitz bandits, we derive asymptotic lower bounds for the regret achieved under any algorithm. We propose OSLB and CKL-UCB, two algorithms that exploit the Lipschitz structure efficiently. OSLB is asymptotically optimal and CKL-UCB is a computationally light algorithm which efficiently exploits the Lipschitz structure. The regret analysis is based on a new concentration inequality for sums of KL divergences which can be instrumental for bandit problems with correlated arms. For continuous Lipschitz bandits, we adapt OSLB and CKL-UCB by using a simple discretization. For both discrete and continuous bandits, initial numerical experiments show that our approach significantly outperforms the state-of-the-art algorithms. Finally the results and algorithms are extended to contextual bandits with similarities."
    }, {
      "heading" : "Appendix A. Proof of Theorem 1",
      "text" : "To establish the asymptotic lower bound, we apply the techniques used in Graves and Lai (1997) to investigate efficient adaptive decision rules in controlled Markov chains. We recall here their general framework. Consider a controlled Markov chain (Xt)t≥0 on a finite state space S with a control set U . The transition probabilities given control u ∈ U are parametrized by θ taking values in a compact metric space Θ: the probability to move from state x to state y given the control u and the parameter θ is p(x, y;u, θ). The parameter θ is not known. The decision maker is provided with a finite set of stationary control laws G = {g1, . . . , gK} where each control law gj is a mapping from S to U : when control law gj is applied in state x, the applied control is u = gj(x). It is assumed that if the decision maker always selects the same control law g, the Markov chain is irreducible with stationary distribution πgθ . Now the expected reward obtained when applying control u in state x is denoted by r(x, u), so that the expected reward achieved under control law g is: µθ(g) = ∑ x r(x, g(x))π g θ(x). There is an optimal control law given θ whose expected reward is denoted µ?θ ∈ arg maxg∈G µθ(g). Now the objective of the decision maker is to sequentially select control laws so as to maximize the expected reward up to a given time horizon T . As for MAB problems, the performance of a decision scheme can be quantified through the notion of regret which compares the expected reward to that obtained by always applying the optimal control law.\nWe now apply the above framework to our Lipschitz bandit problem, and we consider θ ∈ ΘL. The Markov chain has values in {0, 1}. The set of control laws is G = {1, . . . ,K}. These laws are constant, in the sense that the control applied by control law k does not depend on the state of the Markov chain, and corresponds to selecting arm k. The transition probabilities are:\np(x, y; k, θ) = { θk, if y = 1, 1− θk, if y = 0.\nFinally, the reward r(x, k) is just given by the state x. We now fix θ ∈ ΘL. Define the set B(θ) consisting of all bad parameters λ ∈ ΘL such that k? is not optimal under parameter λ, but which are statistically indistinguishable from θ:\nB(θ) = {λ ∈ ΘL : λk? = θk? and max k λk > λk?},\nB(θ) can be written as the union of sets Bk(θ), k ∈ K− defined as:\nBk(θ) = {λ ∈ B(θ) : λk > λk?}.\nBy applying Theorem 1 in Graves and Lai (1997), we know that C(θ) is the minimal value of the following LP:\nmin ∑\nk ck(θ ? − θk) (12) s.t. infλ∈Bk(θ) ∑\nl∈K clI(θl, λl) ≥ 1, ∀k ∈ K− (13) ck ≥ 0, ∀k ∈ K. (14)\nTo conclude the proof, it is sufficient to remark that for any k,\ninf λ∈Bk(θ) ∑ l∈K clI(θl, λl) = ∑ l∈K clI(θl, λ k l ),\nwhich is easy in view of the definition of λk, by monotonicity of x 7→ I(θk, x) when x ≥ θk."
    }, {
      "heading" : "Appendix B. Proof of Theorem 2",
      "text" : "In this section, we first establish the concentration inequality assuming that Lemma 6 holds. We then prove Lemma 6, and to this aim, we state and use two further intermediate results, Lemmas 7 and 8, proved at the end of this section. Without loss of generality, we assume that tk(n) ≥ 1 for any k (the case where for some k, tk(n) = 0 is treated similarly).\nProof of Theorem 2. Let δ ≥ K + 1 and η > 0. Define D = dlog(n)/ log(1 + η)e, and the set D = {1, . . . , D}K . Introduce the following events:\nA = { K∑ k=1 tk(n)I +(θ̂k(n), θk) ≥ δ } ,\nBd = ∩Kk=1 { (1 + η)dk−1 ≤ tk(n) ≤ (1 + η)dk } , for all d ∈ D.\nWe have A = ∪d∈D(A∩Bd), and hence P[A] ≤ ∑\nd∈D P[A∩Bd]. We let η = 1/(δ−1) and apply Lemma 6 with tk = (1 + η)dk−1. Since δ ≥ K + 1, for η = 1/(δ − 1), δ ≥ (1 + η)K, and the application of Lemma 6 is legitimate. We obtain for all d ∈ D:\nP[A ∩Bd] ≤ ( δe\nK\n)K e−δ/(1+η).\nSince |D| = DK , we deduce that P[A] ≤ ( Dδe K )K e−δ/(1+η). Now with our choice η = 1/(δ − 1), and using the inequality log(1 + η) = − log(1/(1 + η)) ≥ 1− 1/(1 + η) = 1/δ, we get:\nP[A] ≤ e−δ ( δdδ log(n)e\nK\n)K eK+1,\nwhich concludes the proof.\nLemma 6 For any k = 1, . . . ,K, let 1 ≤ t̄k ≤ n. Let η > 0. Define the event:\nC = ∩Kk=1{t̄k ≤ tk(n) ≤ (1 + η)t̄k}.\nFor δ ≥ (1 + η)K, we have:\nP [ 1C\nK∑ k=1 tk(n)I +(θ̂k(n), θk) ≥ δ\n] ≤ ( δe\nK\n)K e−δ/(1+η).\nProof of Lemma 6. Define the event E = {1C ∑K k=1 tk(n)I +(θ̂k(n), θk) ≥ δ}. We shall prove that for all ζ ∈ (R+)K :\nP[∩Kk=1{1Ctk(n)I+(θ̂k(n), θk) ≥ ζk}] ≤ e−( ∑K k=1 ζk)/(1+η).\nLet ζ ∈ (R+)K . For t ≥ 0, we define xk(t) such that (i) if there exists 0 ≤ x ≤ θk such that tI+(x, θk) = ζk, then xk(t) = x, (ii) else xk(t) = 0. By monotonicity of I+, t 7→ xk(t) is increasing. Hence tk(n)I+(θ̂k(n), θk) ≥ ζk implies that θ̂k(n) ≤ xk(tk(n)) ≤ xk(t̄k(1 + η)). We also have t̄kI+(xk(t̄k(1 + η)), θk) = ζk/(1 + η).\nWe deduce that\nP[∩k{1Ctk(n)I+(θ̂k(n), θk) ≥ ζk}] ≤ P[∩k{θ̂k(n) ≤ xk(tk(n)), C}] ≤ P[∩k{θ̂k(n) ≤ xk(t̄k(1 + η)), C}]\n≤ K∏ k=1 e−t̄kI(xk(t̄k(1+η)),θk)] = e− ∑K k=1 ζk/(1+η),\nwhere the last inequality is obtained by applying Lemma 7 with Ck = xk(t̄k(1+η)). Next we apply Lemma 8 with Zk = 1Ctk(n)I+(θ̂k(n), θk) and a = 1/(1 + η). We get:\nP[E] ≤ (\nδe\nK(1 + η)\n)K e−δ/(1+η),≤ ( δe\nK\n)K e−δ/(1+η).\nLemma 7 For any k = 1, . . . ,K, let 1 ≤ t̄k ≤ n. Then for all 0 ≤ Ck ≤ θk we have:\nP[∩Kk=1{θ̂k(n) ≤ Ck, t̄k ≤ tk(n)}] ≤ K∏ k=1 e−t̄kI(Ck,θk).\nProof of Lemma 7. For all k = 1, . . . ,K and λ, we define\nφk(λ) = log(E[eλXk(1)]) = log(θkeλ + (1− θk)).\nOne can easily show that for all x ∈ [0, θk], I(x, θk) = supλ≤0{λx − φk(λ)}. Define the events F = F1 ∩ F2, where F1 = ∩Kk=1{t̄k ≤ tk(n)}, and F2 = ∩Kk=1{θ̂k(n) ≤ Ck}.\nFor all k, let λk ≤ 0, and define G(n) = exp (∑K k=1 λkSk(n)− tk(n)φk(λk) )\n. For all n′ ≤ n we have G(n′) = G(n′ − 1) ∏K k=1 e Bk(n ′)(λkXk(n\n′)−φk(λk)). Since Bk(n′) is Fn′−1 measurable and {Xk(n′)}k is independent of Fn′−1, we deduce that E[G(n′)|Fn′−1] = G(n′ − 1), i.e., G is a martingale. Furthermore E[G(n)] = 1.\nFor all k, we set λk = arg max\nλ≤0 {λCk − φk(λ)}, (15)\nso that λkCk − φk(λk) = I(Ck, θk). We have λk < 0 and therefore:\nP[F ] = P[∩Kk=1{Sk(n) ≤ tk(n)Ck , F1}]\n≤ P[ K∑ k=1 λkSk(n) ≥ K∑ k=1 λktk(n)Ck , F1]\n≤ P[1F1e ∑K k=1 λkSk(n) ≥ e ∑K k=1 λktk(n)Ck ]\n= P[1F1G(n) ≥ e ∑K k=1 tk(n)(λkCk−φk(λk))]\n= P[1F1G(n) ≥ e ∑K k=1 tk(n)I(Ck,θk)]\n≤ P[1F1G(n) ≥ e ∑K k=1 t̄kI(Ck,θk)].\nUsing Markov inequality and the fact that E[1F1G(n)] ≤ E[G(n)] = 1, and we obtain the announced result:\nP[F ] ≤ E[1F1G(n)]e− ∑K k=1 t̄kI(Ck,θk) ≤ e− ∑K k=1 t̄kI(Ck,θk).\nLemma 8 Let a > 0, K ≥ 2. Let Z ∈ RK be a random variable such that for all ζ ∈ (R+)K:\nP[Z ≥ ζ] ≤ e−a ∑K k=1 ζk .\nThen for all δ ≥ K/a ∈ R+:\nP[ K∑ k=1 Zk ≥ δ] ≤ ( aδe K )K e−aδ.\nProof of Lemma 8. Let Y ∈ (R+)K a vector whose components are independent and exponentially distributed with parameter a. Then, Z ≤uo Y since for all ζ ∈ (R+)K (see Lemma 9):\nP[Z ≥ ζ] ≤ e−a ∑K k=1 ζk = P[Y ≥ ζ].\nLet λ ∈ [0, a) and δ ∈ R+. Using Markov inequality we get:\nP[ K∑ k=1 Zk ≥ δ] = P[eλ ∑K k=1 Zk ≥ eλδ] ≤ e−λδE[eλ ∑K k=1 Zk ]\n= e−λδE[ K∏ k=1 eλZk ] ≤ e−λδE[ K∏ k=1 eλYk ]\n= e−λδ K∏ k=1 E[eλYk ].\nwhere we have used the results of Lemma 9 with fk(z) = ezλ for all k. Note that z 7→ ezλ is positive and increasing.\nFurthermore we have E[eλYk ] = ∫ +∞\n0 ae −ayeλydy = aa−λ . Hence we have established that for\nall 0 ≤ λ < a:\nP[ K∑ k=1 Zk ≥ δ] ≤ e−λδ aK (a− λ)K .\nSetting λ = a−K/δ ≥ 0, we obtain:\nP[ K∑ k=1 Zk ≥ δ] ≤ ( aδe K )K e−aδ.\nThe next lemma presents a result on multivariate stochastic ordering, see Müller and Stoyan\n(2002)[Theorem 3.3.16].\nLemma 9 Let X and Y be two random variables on RK . The following are equivalent: (i) X ≤uo Y , (ii) For all x ∈ RK , P[X ≥ x] ≤ P[Y ≥ x], (iii) For all collections of non negative increasing functions f1, . . . , fK we have E[ ∏K k=1 fk(Xk)] ≤\nE[ ∏K k=1 fk(Yk)]."
    }, {
      "heading" : "Appendix C. Proof of Theorem 3",
      "text" : "We first present two important corollaries of our concentration inequality (Theorem 2).\nCorollary 10 Let f(n) = log(n) + (3K + 1) log log(n). There exists n0 such that for all n ≥ n0:\nP [ K∑ k=1 tk(n)I +(θ̂k(n), θk) ≥ f(n) ] ≤ 1 n log(n) .\nCorollary 11 Let f(n) = log(n) + (3K + 1) log log(n), and define λq,kk′ = q−L|xk − xk′ |. Then there exists n0 such that for all n ≥ n0:\nP[bk(n) < θk] ≤ 1\nn log(n) .\nProof of Corollary 11. Since I+ is increasing in its second argument, the event bk(n) < θk implies that:\nK∑ k′=1 tk′(n)I +(θ̂k′(n), λ θk,k k′ ) ≥ f(n).\nFurthermore, by definition λθk,kk′ = θk − L|xk − xk′ | ≤ θk′ . Hence:\nK∑ k′=1 tk′(n)I +(θ̂k′(n), θk′) ≥ f(n).\nWe can now apply Corollary 10 and obtain:\nP[bk(n) < θk] ≤ P[ K∑ k′=1 tk′(n)I +(θ̂k′(n), θk′) ≥ f(n)] ≤\n1\nn log(n) .\nWe then give an important lemma that allows us to upper bound the average cardinalities of\nparticular sets of rounds. This lemma is stated and proved in Combes and Proutiere (2014b).\nLemma 12 Let k ∈ K, and > 0. Define Fn the σ-algebra generated by (Xk(t))1≤t≤n,1≤k≤K . Let Λ ⊂ N be a (random) set of instants. Assume that there exists a sequence of (random) sets (Λ(s))s≥1 such that (i) Λ ⊂ ∪s≥1Λ(s), (ii) for all s ≥ 1 and all n ∈ Λ(s), tk(n) ≥ s, (iii) |Λ(s)| ≤ 1, and (iv) the event n ∈ Λ(s) is Fn-measurable. Then for all δ > 0:\nE[ ∑ n≥1 1{n ∈ Λ, |θ̂k(n)− θk| > δ}] ≤ 1 δ2 . (16)\nWe are now ready to analyze the regret achieved under OSLB( ).\nProof of Theorem 3. Let S(θ) denote the set of solutions of (3) for a given θ. For any χ > 0, we define the set\nΓχ,θ = ∪{θ′:|θ′k−θk|<χ,∀k}S(θ ′),\nand for all k, cχk = sup{ck : c ∈ Γχ,θ}. In view of Lemma 13, θ ′ 7→ S(θ′) is upper hemicontinuous at θ and by Assumption 1 S(θ) reduces to a point. Therefore, for any open neigbourhood B of S(θ), there exists χ > 0 such that S(θ′) ⊂ B if supk |θ′k − θk| < χ. Hence for all k: c χ k → ck(θ), as χ→ 0. Fix 0 < δ < (θ? −maxk 6=k? θk)/2 and > 0. To simplify the notation, we replace by K in the Theorem 3, and prove the result for this choice of . Let k be a suboptimal arm. We derive an upper on the number of times it is played. Let n be a round where k is played, i.e., k(n) = k. In view of the design of OSLB( ), there are three possible scenarios: (a) k can be the leader and its empirical reward exceeds the indexes of other arms, L(n) = k and θ̂k(n) ≥ maxl bl(n); (b) k and k? are not the leader, and k can be either k(n) or k(n); (c) k? is the leader, and again k can be either k(n) or k(n). We investigate all cases, but we start by defining sets of rounds whose average cardinalities can be easily controlled:\nAk = {1 ≤ n ≤ T : k(n) = k, bk(n) ≤ θk} Bk = {n ≥ 1 : k(n) = k,min\nk′ tk′(n) ≥ tk(n),max k′ |θ̂k′(n)− θk′ | ≥ δ}\nEk = {n ≥ 1 : k(n) = k, |θ̂k(n)− θk| ≥ δ} Fk = {n ≥ 1 : k(n) = k, tk(n) ≤ min(tk′(n), tk?(n)), max\nl∈{k′,k?} |θ̂l(n)− θl| ≥ δ}\nand A = ∪kAk, B = ∪kBk, E = ∪kEk, F = ∪kFk. From the concentration inequality, and its corollaries, we have E[|A|] ≤ C1 log log(T ). We use Lemma 12 to bound the cardinalities of the other sets.\n• Bound for Bk. Let us fix k′ 6= k. We apply Lemma 12 to k′ with Λ(s) = {n : k(n) = k,minl tl(n) ≥ s, tk(n) = s}, and Λ = ∪sΛ(s). We get that:\nE [∣∣{n : k(n) = k,min\nl tl(n) ≥ tk(n), |θ̂k′(n)− θk′ | ≥ δ} ∣∣] ≤ 1 δ2 .\nWe conclude that: E[|Bk|] ≤ K/( δ2).\n• Bound for Ek. The application of lemma is direct here, and we get: E[|Ek|] ≤ 1/δ2.\n• Bound for Fk. Using the same argument as that used to bound the cardinality of Bk, we get: E[|Fk|] ≤ 2/δ2.\nNext we consider n /∈ A ∪ B ∪ E ∪ F such that k is played. We treat all cases (a), (b), and (c) that can arise in such a round.\nCase (a) We assume here that k = L(n) and that k(n) = k, so that θ̂k(n) ≥ maxl bl(n). Hence, since n /∈ Ak? , θ̂k(n) ≥ bk?(n) ≥ θ?. In summary, θ̂k(n) ≥ θ?, which is impossible because of our choice of δ (< θ? − θk), and n /∈ Ek.\nCase (b) Let k′ /∈ {k, k?} be the leader in round n, and assume that k(n) = k. We consider two subcases: (i) k = k(n), and (ii) k = k(n). (i) In this case, k has been played less than any other arm, and so tk(n) ≤ min(tk′(n), tk?(n)). On the other hand, since k′ is the leader, we have θ̂k′(n) ≥ θ̂k?(n), which implies that either θk′ or θk? is badly estimated. More precisely, we proved that n ∈ Fk, which is impossible. (ii) In this case, we know that tk(n) ≤ tk(n)(n)/ . In addition, again, we have θ̂k′(n) ≥ θ̂k?(n), and so either θk′ or θk? is badly estimated. We proved that n ∈ Bk, which is impossible.\nCase (c) Assume that k? = L(n). k is played, and we need to consider two subcases: (i) k = k(n), and (ii) k = k(n). (i) In this case, since k = k(n), we have tk(n) ≤ minl tl(n), and hence tk(n) ≤ minl tl(n). Since n /∈ Bk, in view of the previous inequality, all arms must be well-estimated, i.e., maxl |θ̂l(n) − θl| < δ. This implies that for all l ∈ K, ĉl(n) ≤ cδl . Now by definition in our algorithm, if k(n) = k = k(n), then tk(n) < tk(n)(n), and so tk(n) < maxl c δ l log(n). In other words, n ∈ Dk where\nDk = {1 ≤ n ≤ T, n /∈ A ∪B ∪ E ∪ F,L(n) = k?, k(n) = k, tk(n) ≤ max k′ cδk′ log(T )}.\nWe shall bound the size of Dk later in the proof. (ii) In this case, we must have tt(n)(n) ≥ tk(n). Hence since n /∈ Bk, all arms are well estimated, and hence again, for all l ∈ K, ĉl(n) ≤ cδl . In particular, since k is played, tk(n) ≤ cδk log(n), and thus n ∈ Ck where\nCk = {1 ≤ n ≤ T, n /∈ A ∪B, k(n) = k, tk(n) ≤ cδk log(T )}.\nNest we bound the expected cardinalities of Ck and Dk. Since tk(n) is incremented if n ∈ Ck or n ∈ Dk, we simply have:\n|Ck| ≤ cδk log(T ), |Dk| ≤ max k′ cδk′ log(T ).\nPutting it all together we have proven the announced regret bound:\nRπ(T ) ≤ ∑ k 6=k? (θ? − θk)(E[|Ck|] + E[|Dk|])\n+ E[|A|] + E[|B|] + E[|E|] + E[|F |], ≤ log(T ) ∑ k 6=k? (θ? − θk)(cδk + max k′ cδk′) + C1 log log(T ) +K 2 −1δ−2 + 3Kδ−2.\nThis completes the proof (because of our particular choice of , and maxl cδl ≤ ∑ l c δ l ).\nC.1. Continuity of solutions to parametric linear programs\nWe state and prove Lemma 13, a technical result about the continuity of the solutions of a parametric linear program with respect to its parameters. It follows from the general conditions of Wets (1985).\nLemma 13 Consider A ∈ (R+)K×K , c ∈ (R+)K , and T ⊂ (R+)K×K × (R+)K . Define t = (A, c). Consider the function Q and the set-valued map Q?\nQ(t) = inf x∈RK\n{cx|Ax ≥ 1, x ≥ 0}\nQ?(t) = {x : cx ≤ Q(t)|Ax ≥ 1, x ≥ 0}.\nAssume that:\n(i) For all t ∈ T , all rows and columns of A are non-identically 0\n(ii) mint∈T mink ck > 0\nThen:\n(a) Q is continuous on T .\n(b) Q? is upper hemicontinuous on T .\nProof. Define c0 = min(1,min\nt∈T min k ck) > 0,\nand a = max(k,k′)Ak,k′ . Define the sets K = {x|Ax ≤ 1}, D = {x|Ax ≤ c} and B = [0, c0/(aK)]\nK . Then B ⊂ K ∩ D, so that both K and D have non-empty interior. By Wets (1985)[Corollary 7], t → K and t → D are continuous on T since they have non-empty interior and all rows of (A, 1) and columns of ( A c ) are non identically 0. By Wets (1985)[Theorem 2], Q is continuous on T since both K and D are continuous on T , proving the first statement. Consider a sequence {(ti, xi)}i≥1, such that xi ∈ Q?(ti) and (ti, xi) → (t, x), i → ∞. Since for all i ≥ 1 cxi ≤ Q(ti) and Axi ≥ 1 we have, by continuity, Ax ≥ 1 and cx = Q(t) and so x ∈ Q?(t). Hence Q? is upper hemicontinuous."
    }, {
      "heading" : "Appendix D. Proof of Theorem 4",
      "text" : "D.1. Proof of (i)\nLet 0 < δ < (θ∗ −maxk∈K− θk)/2 fixed throughout the proof. Define the random sets of rounds: B = {1 ≤ n ≤ T : bk?(n) ≤ θ?} the set of rounds at which the index of the optimal arm underestimates its true value θ?, and Dk = {n : k(n) = k, bk(n) ≥ θ? − δ} the set of rounds at which k is selected and its index is larger than θ? − δ.\nLet k 6= k? be a suboptimal arm, and let n /∈ B such that k is selected k(n) = k. The possible events are:\n(a) If L(n) ∈ {k, k?} then bk(n) ≥ bk?(n) ≥ θ? since n /∈ B, so n ∈ Dk.\n(b) If L(n) = k′ /∈ {k, k?}, then bk(n) ≥ bk′(n) and:\n(b-i) If we further have bk′(n) ≥ θ? − δ then bk(n) ≥ θ? − δ so n ∈ Dk as well. (b-ii) Otherwise bk′(n) ≤ θ? − δ.\nDefine the random set of instants Ek = {n 6∈ B : k(n) = k, L(n) 6∈ {k, k?}, bk∗(n) > bL(n)(n), |θ̂k∗(n) − θk∗ | ≥ δ}. In the case (b-ii), we have bL(n)(n) ≤ θ? − δ < θ? ≤ bk?(n) since n /∈ B. Also by definition of L(n) we have that θ̂k?(n) ≤ θ̂L(n)(n) ≤ bL(n)(n) ≤ θ? − δ. So in case (b-ii) we have n ∈ Ek.\nIn summary, k(n) = k implies that n ∈ B∪Ek∪Dk so: E[tk(T )] ≤ E[|B|]+E[|Ek|]+E[|Dk|]. Let us upper bound the expected sizes of sets B, Ek and Dk.\nExpected size of B: From Theorem 2, there exists a constant C1 ≥ 0 such that E[|B|] is upper bounded by the Bertrand series:\nE[|B|] ≤ T∑ n=1 C1(n log(n)) −1 ≤ C1 log(log(T )),\nExpected size of Ek: If n ∈ Ek , we have bk∗(n) > bL(n)(n) > θ̂L(n)(n), so that by design of CKL-UCB, k(n) ∈ arg min\nk:bk(n)>bL(n)(n) tk(n) and k∗ ∈ {k : bk(n) > bL(n)(n)}. Since k(n) = k, we have tk(n) ≥ tk?(n). Define s = ∑n\nn′=1 1{n′ ∈ Ek}, this implies tk∗(n) ≥ s. Applying Lemma 12 as earlier, we conclude that E[Ek] ≤ δ−2.\nExpected size of Dk: Define F δk = {n : k(n) = k, |θ̂k(n) − θk| < δ} and F δk = {n : k(n) = k, |θ̂k(n) − θk| ≥ δ}. Let us consider a round n ∈ Dk ∩ F δk . Assume that tk(n) > f(n)/I(θk + δ, θ∗−δ). Since n ∈ Dk and k(n) = k, we have: bk(n) ≥ θ∗−δ. Therefore, from the monotonicity of I(x, y) in y when y > x, we have:\ntk(n)I(θ̂k(n), θ ∗ − δ) ≤ ∑ i∈K ti(n)I(θ̂i(n), λ θ∗−δ,k i ) ≤ ∑ i∈K ti(n)I(θ̂i(n), λ bk(n),k i ) = f(n) (17)\nwhere the last equality comes from our definition of bk(n). Furthermore, by our assumption and since θ̂k(n) ≤ θk + δ (since n ∈ F δk ):\nf(n) < tk(n)I(θk + δ, θ ∗ − δ) ≤ tk(n)I(θ̂k(n), θ∗ − δ),\nwhich contradicts (17). Thus for all rounds in n ∈ Dk∩F δk we have tk(n) ≤ f(n)/I(θk+δ, θ∗−δ) and consequently E[|Dk|] ≤ f(T )/I(θk + δ, θ∗ − δ) + E[|F δk |].\nAgain a direct application of Lemma 12 yields E[|F δk |] ≤ δ −2. Thus, we have:\nE[tk(T )] ≤ f(T )/I(θk + δ, θ∗ − δ) + C1 log(log(T )) + 2δ−2.\nD.2. Proof of (ii)\nWe work with a fixed sample path throughout the proof. Since for all k when T → ∞ we have tk(T )→∞ a.s., so by the law of large numbers θ̂k(T )→ θk as T →∞.\nFrom the first statement of the theorem, we have that for all k 6= k?, lim sup T→∞ E[tk(T )]/ log(T ) <\n∞ which implies that lim sup T→∞ tk(T )/ log(T ) < ∞ . In turn we have that tk?(T ) = T −∑ k 6=k? tk(T ) = T −O(log(T )), so that tk?(T )/T →T→∞ 1. By Pinsker’s inequality:\nθ̂k?(T ) ≤ bk?(T ) ≤ θ̂k?(T ) + √ 2f(T )/tk?(T )\nand we can deduce bk?(T )→ θ? as T →∞, because θ̂k?(T )→ θ? and f(T )/tk?(T ) = f(T )/(T− O(log(T )))→ 0 when T →∞.\nLet δ such that 0 < δ < (θ? − max k∈K− θk)/2, by the above reasoning there exists n0 ∈ N (depending on the sample path and δ) such that for all n ≥ n0 we have |bk?(n) − θ?| ≤ δ and |θ̂k(n)− θk| ≤ δ for all k. It is noted that for all n ≥ n0, L(n) = k?, since δ < (θ? −max\nk 6=k? θk)/2.\nLet α0 ≥ 0, and assume that there exists T large enough such that tk(T ) = α0f(T ) and α0f(T ) > tk(n0). Therefore there exists n0 ≤ n ≤ T such that tk(n) = α0f(T ) − 1 and k is selected at time n: k(n) = k. DefineN = {k′ : bk′(n) ≤ bk?(n)}. Consider k′ ∈ N , since n ≥ n0, we have that L(n) = k? and bL(n) ≤ θ? + δ. So bk′(n) ≤ θ? + δ which implies (by definition of bk′(n)): ∑\nk′′∈K tk′′(n)I(θk′′ − δ, λθ\n?+δ,k′\nk′′ ) ≥ f(n) (18)\nAlso, since k(n) = k, by design of CKL-UCB we have tk(n) = arg min k′ 6∈N tk′(n), so that :\ntk′(n) ≥ tk(n) = α0f(T )− 1 ≥ α0f(n)− 1, ∀k′ 6∈ N . (19)\nFinally, since k(n) = k, L(n) = k?, we must have bk(n) ≥ bk?(n) ≥ θ? − δ, so that:∑ k′∈K tk′(n)I(θk′ + δ, λ θ?−δ,k k′ ) ≤ f(n)\n(α0f(T )− 1)I(θk + δ, λθ ?−δ,k k ) + ∑ k′∈K\\{k} tk′(n)I(θk′ + δ, λ θ?−δ,k k′ ) ≤ f(n) (α0f(n)− 1)I(θk + δ, λθ ?−δ,k k ) +\n∑ k′∈K\\{k} tk′(n)I(θk′ + δ, λ θ?−δ,k k′ ) ≤ f(n) (20)\nDefine the matrix Ã = (ãk′k)k,k′ , with ãk′k = I(θk′ + δ, λ θ?−δ,k k′ ) for all k ′ and ãk′′k′ = I(θk′′ − δ, λθ ?+δ,k′\nk′′ ) for all k ′ 6= k and all k′′.\nDefine αk′(n) = tk′(n)/f(n) for all k′, and by dividing equations (20) , (19) and (18) by f(n), we obtain:\nα0ãkk + ∑\nk′∈K\\{k}\nαk′(n)ãk′k ≤ 1,\nαk′(n) ≥ α0 − 1 f(n) , ∀k′ /∈ N ,\nα0ãkk′ + ∑\nk′′∈K\\{k}\nαk′′(n)ãk′′k′ ≥ 1, ∀k′ ∈ N .\nIt is noted that α′k(n) ≥ 0 for all k′ by definition. Define α = (α1, . . . , αK) a limit point of the sequence (α(n))n≥1 (note that this sequence need not converge and might have several limit points). First letting n → ∞ along a converging subsequence and then letting δ → 0 the constraints above become:\nα0akk + ∑\nk′∈K\\{k}\nαk′ak′k ≤ 1,\nαk′ ≥ α0, ∀k′ /∈ N , α0akk′ + ∑\nk′′∈K\\{k}\nαk′′ak′′k′ ≥ 1, ∀k′ ∈ N .\nTherefore, by definition of dk, we must have:\nα0ãkk + dk(A,α0,N ) ≤ 1,\nand taking the infimum over N so that we obtain the condition:\nα0ãkk + ek(A,α0) ≤ 1. (21)\nNow consider α0 such that α0ãkk + ek(A,α0) > 1. Then in view of the necessary condition (21), we cannot have tk(T ) ≥ α0 log(T ), so that:\nlim sup T→∞\ntk(T )\nlog(T ) ≤ inf{α0 ≥ 0 : akkα0 + ek(A,α0) > 1} = βk(θ).\nWe get (ii) by Lebesgue’s dominated convergence theorem, since supT≥1 E[ tk(T ) log(T ) ] <∞ from (i).\nD.3. Proof of (iii)\nIn order to prove the last part of the theorem, it is sufficient to prove that for α0 = 1/akk, we have ek(A,α0) > 0 so that akkα0 + ek(A,α0) = 1 + ek(A,α0) > 1 so that βk(θ) < α0 = 1/akk.\nWe proceed by contradiction. Assume that ek(A,α0) = 0. Then there exists N a subset of {1, . . . ,K} \\ {k?, k} such that dk(A,α0,N ) = 0. As a consequence there exists α1, ..., αK such that:\n∑ k′∈K\\{k} αk′ak′k = 0\ns.t. αk′ ≥ α0, ∀k′ 6∈ N αk′ ≥ 0, ∀k′∑ k′′∈K\\{k} αk′′ak′′k′ ≥ 1− α0akk′ , ∀k′ ∈ N .\nConsider there exists k′ such that ak′k > 0. Then we must have k′ ∈ N , otherwise αk′ ≥ α0 = 1/akk and 0 = ∑ k′′∈K\\{k} αk′′ak′′k ≥ α0ak′k > 0, a contradiction. By the same reasoning we must also have αk′ = 0. As said in the theorem statement, assume that there exists k′ ∈ N such that 0 < akk′ < akk and assume that for all k′′ we have that if ak′′k = 0 then ak′′k′ as well. Considering k′′ = k′ in our assumption, since ak′k = 0 would imply ak′k′ = 0, we then have that ak′k > 0 since ak′k′ = I(θk′ , θ\n∗) > 0. From our previous argument we have that if ak′k > 0 then k′ ∈ N . Therefore :\n∑ k′′∈K\\{k}\nαk′′ak′′k′ ≥ 1− α0akk′ = 1− akk′/akk > 0∑ k′′ 6=k, ak′′k=0 αk′′ak′′k′ > 0 (22)\nBy assumption, ak′′k = 0 implies ak′′k′ = 0 so that the l.h.s. of (22) is zero and cannot be strictly larger than 0. This is a contradiction, proving that ek(A,α0) = 0 cannot occur and concludes the proof."
    } ],
    "references" : [ {
      "title" : "The continuum-armed bandit problem",
      "author" : [ "R. Agrawal" ],
      "venue" : "SIAM J. Control and Optimization,",
      "citeRegEx" : "Agrawal.,? \\Q1995\\E",
      "shortCiteRegEx" : "Agrawal.",
      "year" : 1995
    }, {
      "title" : "Finite time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Online optimization in x-armed bandits",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz", "C Szepesvári" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2008
    }, {
      "title" : "Kullback-leibler upper confidence bounds for optimal sequential allocation",
      "author" : [ "O. Cappé", "A. Garivier", "O. Maillard", "R. Munos", "G. Stoltz" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Cappé et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cappé et al\\.",
      "year" : 2013
    }, {
      "title" : "Unimodal bandits: Regret lower bounds and optimal algorithms",
      "author" : [ "R. Combes", "A. Proutiere" ],
      "venue" : "In Proc. of ICML,",
      "citeRegEx" : "Combes and Proutiere.,? \\Q2014\\E",
      "shortCiteRegEx" : "Combes and Proutiere.",
      "year" : 2014
    }, {
      "title" : "Unimodal bandits: Regret lower bounds and optimal algorithms",
      "author" : [ "R. Combes", "A. Proutiere" ],
      "venue" : "Technical Report, people.kth.se/ ̃alepro/pdf/tr-icml2014.pdf,",
      "citeRegEx" : "Combes and Proutiere.,? \\Q2014\\E",
      "shortCiteRegEx" : "Combes and Proutiere.",
      "year" : 2014
    }, {
      "title" : "Stochastic linear optimization under bandit feedback",
      "author" : [ "V. Dani", "T.P. Hayes", "S.M. Kakade" ],
      "venue" : "In Proc. of Conference On Learning Theory (COLT),",
      "citeRegEx" : "Dani et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2008
    }, {
      "title" : "Online convex optimization in the bandit setting: gradient descent without a gradient",
      "author" : [ "A. Flaxman", "A.T. Kalai", "H.B. McMahan" ],
      "venue" : "In Proc. of ACM/SIAM symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "Flaxman et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Flaxman et al\\.",
      "year" : 2005
    }, {
      "title" : "Informational confidence bounds for self-normalized averages and applications",
      "author" : [ "A. Garivier" ],
      "venue" : "In Information Theory Workshop,",
      "citeRegEx" : "Garivier.,? \\Q2013\\E",
      "shortCiteRegEx" : "Garivier.",
      "year" : 2013
    }, {
      "title" : "The KL-UCB algorithm for bounded stochastic bandits and beyond",
      "author" : [ "A. Garivier", "O. Cappé" ],
      "venue" : "In Proc. of Conference On Learning Theory (COLT),",
      "citeRegEx" : "Garivier and Cappé.,? \\Q2011\\E",
      "shortCiteRegEx" : "Garivier and Cappé.",
      "year" : 2011
    }, {
      "title" : "Asymptotically efficient adaptive choice of control laws in controlled markov chains",
      "author" : [ "T.L. Graves", "T.L. Lai" ],
      "venue" : "SIAM J. Control and Optimization,",
      "citeRegEx" : "Graves and Lai.,? \\Q1997\\E",
      "shortCiteRegEx" : "Graves and Lai.",
      "year" : 1997
    }, {
      "title" : "Nearly tight bounds for the continuum-armed bandit problem",
      "author" : [ "R. Kleinberg" ],
      "venue" : "In Proc. of the conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Kleinberg.,? \\Q2004\\E",
      "shortCiteRegEx" : "Kleinberg.",
      "year" : 2004
    }, {
      "title" : "Multi-armed bandits in metric spaces",
      "author" : [ "R. Kleinberg", "A. Slivkins", "E. Upfal" ],
      "venue" : "In Proc. of the 40th annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Kleinberg et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kleinberg et al\\.",
      "year" : 2008
    }, {
      "title" : "Adaptive treatment allocation and the multi-armed bandit problem",
      "author" : [ "T.L. Lai" ],
      "venue" : "The Annals of Statistics, 15(3):1091–1114,",
      "citeRegEx" : "Lai.,? \\Q1987\\E",
      "shortCiteRegEx" : "Lai.",
      "year" : 1987
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Lai and Robbins.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai and Robbins.",
      "year" : 1985
    }, {
      "title" : "Comparison Methods for Stochastic Models and Risks",
      "author" : [ "A. Müller", "D. Stoyan" ],
      "venue" : null,
      "citeRegEx" : "Müller and Stoyan.,? \\Q2002\\E",
      "shortCiteRegEx" : "Müller and Stoyan.",
      "year" : 2002
    }, {
      "title" : "Contextual bandits with similarity information",
      "author" : [ "A. Slivkins" ],
      "venue" : "In Proc. of Conference On Learning Theory (COLT),",
      "citeRegEx" : "Slivkins.,? \\Q2011\\E",
      "shortCiteRegEx" : "Slivkins.",
      "year" : 2011
    }, {
      "title" : "Continuity of solutions to parametric linear programs We state and prove Lemma 13, a technical result about the continuity of the solutions of a parametric linear program with respect to its parameters. It follows from the general conditions",
      "author" : [ ],
      "venue" : "Lemma 13 Consider",
      "citeRegEx" : "C.1.,? \\Q1985\\E",
      "shortCiteRegEx" : "C.1.",
      "year" : 1985
    }, {
      "title" : "D, so that both K and D have non-empty interior. By Wets (1985)[Corollary 7], t → K and t → D are continuous on T since they have non-empty interior and all rows of (A, 1) and columns",
      "author" : [ "K . Then B ⊂ K" ],
      "venue" : null,
      "citeRegEx" : "∩,? \\Q1985\\E",
      "shortCiteRegEx" : "∩",
      "year" : 1985
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Introduction In their seminal paper, Lai and Robbins (1985) solve the classical stochastic Multi-Armed Bandit (MAB) problem.",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.g. KL-UCB Garivier and Cappé (2011), Cappé et al.",
      "startOffset" : 45,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.g. KL-UCB Garivier and Cappé (2011), Cappé et al. (2013) – note that the KL-UCB algorithm was initially proposed and analysed in Lai (1987), see (2.",
      "startOffset" : 45,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.g. KL-UCB Garivier and Cappé (2011), Cappé et al. (2013) – note that the KL-UCB algorithm was initially proposed and analysed in Lai (1987), see (2.",
      "startOffset" : 45,
      "endOffset" : 226
    }, {
      "referenceID" : 1,
      "context" : "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.g. KL-UCB Garivier and Cappé (2011), Cappé et al. (2013) – note that the KL-UCB algorithm was initially proposed and analysed in Lai (1987), see (2.6). When the expected rewards of the various arms are not related as in Lai and Robbins (1985), the regret of the best algorithm essentially scales as O(K log(T )) where K denotes the number of arms, and T is the time horizon.",
      "startOffset" : 45,
      "endOffset" : 329
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz Agrawal (1995), Kleinberg et al.",
      "startOffset" : 12,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al.",
      "startOffset" : 12,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al.",
      "startOffset" : 12,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al.",
      "startOffset" : 12,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm.",
      "startOffset" : 12,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al.",
      "startOffset" : 12,
      "endOffset" : 711
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al.",
      "startOffset" : 12,
      "endOffset" : 736
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound.",
      "startOffset" : 12,
      "endOffset" : 758
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure.",
      "startOffset" : 12,
      "endOffset" : 861
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al.",
      "startOffset" : 12,
      "endOffset" : 1340
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al. (2008), Bubeck et al.",
      "startOffset" : 12,
      "endOffset" : 1365
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008) adaptively define the set of arms to play, but used simplistic UCB indexes to sequentially select arms.",
      "startOffset" : 12,
      "endOffset" : 1387
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008) adaptively define the set of arms to play, but used simplistic UCB indexes to sequentially select arms. In turn, these algorithms fail at exploiting the problem structure revealed by the past observed samples. For continuous bandits, we propose to first discretize the set of arms (as in Kleinberg et al. (2008)), and then apply OSLB, an algorithm that optimally exploits past observations and hence the problem specific structure.",
      "startOffset" : 12,
      "endOffset" : 1699
    }, {
      "referenceID" : 0,
      "context" : ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008) adaptively define the set of arms to play, but used simplistic UCB indexes to sequentially select arms. In turn, these algorithms fail at exploiting the problem structure revealed by the past observed samples. For continuous bandits, we propose to first discretize the set of arms (as in Kleinberg et al. (2008)), and then apply OSLB, an algorithm that optimally exploits past observations and hence the problem specific structure. As it turns out, this approach outperforms algorithms directly dealing with continuous sets of arms. Our contributions. (a) For discrete Lipschitz bandit problems, we derive an asymptotic regret lower bound satisfied by any algorithm. This bound is problem specific in the sense that it depends in an explicit manner on the expected rewards of the various arms (this contrasts with existing lower bounds for continuous Lipschitz bandits). (b) We propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. We further present CKL-UCB (Combined KL-UCB), an algorithm that exhibits lower computational complexity than that of OSLB, and that is yet able to exploit the Lipschitz structure. (c) We provide a finite time analysis of the regret achieved under OSLB and CKL-UCB. The analysis relies on a new concentration inequality for a weighted sum of KL divergences between the empirical distributions of rewards and their true distributions. We believe that this inequality can be instrumental for various bandit problems with structure. (d) We evaluate our algorithms using numerical experiments for both discrete and continuous sets of arms. We compare their performance to that obtained using existing algorithms for continuous bandits. (e) We extend our results and algorithms to the case of contextual bandits with similarities as investigated in Slivkins (2011).",
      "startOffset" : 12,
      "endOffset" : 3230
    }, {
      "referenceID" : 11,
      "context" : "Results can be easily extended to the case where the set of arms is a subset of a metric space as considered in Kleinberg et al. (2008). The set of arms is of finite cardinality, possibly",
      "startOffset" : 112,
      "endOffset" : 136
    }, {
      "referenceID" : 13,
      "context" : "Without loss of generality, we restrict our attention to so-called uniformly good algorithms, as defined in Lai and Robbins (1985). π ∈ Π is uniformly good if for all θ ∈ ΘL, Rπ(T ) = o(T a) for all a > 0.",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : "The regret lower bound is a consequence of results in optimal control of Markov chains, see Graves and Lai (1997). All proofs are presented in appendix.",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "The regret lower bound is a consequence of results in optimal control of Markov chains, see Graves and Lai (1997). All proofs are presented in appendix. As in classical bandits, the minimal regret scales logarithmically with the time horizon. Observe that the lower bound (2) is smaller than the lower bound derived in Lai and Robbins (1985) when the various average rewards (θk, k ∈ K) are not related (i.",
      "startOffset" : 92,
      "endOffset" : 342
    }, {
      "referenceID" : 10,
      "context" : "The regret lower bound is a consequence of results in optimal control of Markov chains, see Graves and Lai (1997). All proofs are presented in appendix. As in classical bandits, the minimal regret scales logarithmically with the time horizon. Observe that the lower bound (2) is smaller than the lower bound derived in Lai and Robbins (1985) when the various average rewards (θk, k ∈ K) are not related (i.e., in absence of the Lipschitz structure). Hence (2) quantifies the gain one may expect by designing algorithms optimally exploiting the structure of the problem. Note that for any k ∈ K−, the variable ck corresponding to a solution of (3) characterizes the number of times arm k should be played under an optimal algorithm: arm k should be roughly played ck log(n) times up to round n. It should be also observed that our lower bound is problem specific (it depends on θ), which contrasts with existing lower bounds for continuous Lipschitz bandits, see e.g. Kleinberg et al. (2008). The latter are typically derived by selecting the problems that yield maximum regret.",
      "startOffset" : 92,
      "endOffset" : 991
    }, {
      "referenceID" : 8,
      "context" : "The latter extends to the multi-dimensional case the concentration inequality derived in Garivier (2013) for a single KL divergence.",
      "startOffset" : 89,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "Müller and Stoyan (2002).",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "Contextual Bandit with Similarities The algorithms and results presented above can be extended to the case of contextual bandit problems with similarities as studied in Slivkins (2011). In such problems, in each round, the decision maker observes a context, and then decides which arm to select.",
      "startOffset" : 169,
      "endOffset" : 185
    }, {
      "referenceID" : 8,
      "context" : "This discretization is known to be order-optimal for functions which are regular around their maximum Kleinberg (2004). In order not to give a positive bias to KL-UCB and CKL-UCB, we make sure that the maximum of the reward functions is not achieved in one of the arms in the discretization: the maximum is placed at a distance of at least δ/4 from any arm in the discretization.",
      "startOffset" : 102,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al.",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al. (2008). The two latter algorithms have performance guarantees (they are order-optimal).",
      "startOffset" : 92,
      "endOffset" : 176
    }, {
      "referenceID" : 2,
      "context" : "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al. (2008). The two latter algorithms have performance guarantees (they are order-optimal). We also compare KL-UCB and CKL-UCB to HOO+ and Zooming+, two improved versions of HOO and Zooming, respectively. In these tuned versions, the confidence radius (see Bubeck et al. (2008) and Kleinberg et al.",
      "startOffset" : 92,
      "endOffset" : 443
    }, {
      "referenceID" : 2,
      "context" : "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al. (2008). The two latter algorithms have performance guarantees (they are order-optimal). We also compare KL-UCB and CKL-UCB to HOO+ and Zooming+, two improved versions of HOO and Zooming, respectively. In these tuned versions, the confidence radius (see Bubeck et al. (2008) and Kleinberg et al. (2008) for details) is set equal to √ log(n)/(2 ∗ tk(n)) in round n.",
      "startOffset" : 92,
      "endOffset" : 471
    }, {
      "referenceID" : 2,
      "context" : "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al. (2008). The two latter algorithms have performance guarantees (they are order-optimal). We also compare KL-UCB and CKL-UCB to HOO+ and Zooming+, two improved versions of HOO and Zooming, respectively. In these tuned versions, the confidence radius (see Bubeck et al. (2008) and Kleinberg et al. (2008) for details) is set equal to √ log(n)/(2 ∗ tk(n)) in round n. HOO+ and Zooming+ exhibit better performance than their initial versions, but their regrets have not been analytically studied. In the experiments, we limit the time horizon to T = 25000 rounds, and the expected regret is calculated by averaging over 100 independent runs. Figure 3 presents the expected regret of the various algorithms for the triangular reward function (left) and for the quadratic reward function (right). First note that surprisingly, KL-UCB, an algorithm that does not leverage the Lipschitz structure, outperforms some of the algorithms designed to exploit the structure. Observe that CKL-UCB clearly outperforms KL-UCB and all other algorithms in both problem instances. For quadratic reward functions, it is known that the optimal discretization of the set of arms should roughly have (log(T )/T )1/4 arms, Combes and Proutiere (2014a). We also plot the regret achieved under CKL-UCB using this optimized discretization, and we observe that this indeed further reduces the regret.",
      "startOffset" : 92,
      "endOffset" : 1394
    } ],
    "year" : 2014,
    "abstractText" : "We consider stochastic multi-armed bandit problems where the expected reward is a Lipschitz function of the arm, and where the set of arms is either discrete or continuous. For discrete Lipschitz bandits, we derive asymptotic problem specific lower bounds for the regret satisfied by any algorithm, and propose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz structure of the problem. In fact, we prove that OSLB is asymptotically optimal, as its asymptotic regret matches the lower bound. The regret analysis of our algorithms relies on a new concentration inequality for weighted sums of KL divergences between the empirical distributions of rewards and their true distributions. For continuous Lipschitz bandits, we propose to first discretize the action space, and then apply OSLB or CKL-UCB, algorithms that provably exploit the structure efficiently. This approach is shown, through numerical experiments, to significantly outperform existing algorithms that directly deal with the continuous set of arms. Finally the results and algorithms are extended to contextual bandits with similarities.",
    "creator" : "LaTeX with hyperref package"
  }
}