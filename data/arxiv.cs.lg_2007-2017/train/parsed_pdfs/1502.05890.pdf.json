{
  "name" : "1502.05890.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Contextual Semi-Bandit Learning",
    "authors" : [ "Akshay Krishnamurthy", "Alekh Agarwal" ],
    "emails" : [ "akshaykr@cs.cmu.edu", "alekha@microsoft.com", "mdudik@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "√ KLT lnN)1\nand Õ(L √ KT lnN), where K is the number of actions, L is the length of each action sequence, T is the number of rounds, and N is the number of policies. If the linear transformation is unknown, we show that an algorithm that first explores to learn the unknown weights via linear regression and thereafter uses the estimated weights can achieve Õ(‖w‖1(KT )3/4 √ lnN) regret, where w is the true (unknown) weight vector. Both algorithms use an optimization oracle to avoid explicit enumeration of the policies and consequently are computationally efficient whenever an efficient algorithm for the fully supervised setting is available."
    }, {
      "heading" : "1 Introduction",
      "text" : "Learning from partial feedback (“bandit” feedback) is of great practical importance and has seen a recent surge of research interest [Bubeck and Cesa-Bianchi, 2012]. Motivating examples include healthcare [Robins, 1989] – where we only observe the result of the treatment prescribed to the patient, but obtain no information about how other treatments would have worked – or Internet applications [Li et al., 2010, Bottou et al., 2013] – where we only observe the user’s reaction (e.g., click or purchase) to the content (e.g., advertisement, news article, merchandise description) that we show, but not to other possible content. These problems fall under the mathematical framework known as contextual bandits, where the learner repeatedly observes a context, then takes an action, and finally observes a reward for the chosen action. This learning paradigm captures the partial feedback aspect of the above examples as the learner does not observe rewards for unselected action. The aim of the learner is to maximize the reward over the course of many rounds of this interactions. ∗akshaykr@cs.cmu.edu †alekha@microsoft.com ‡mdudik@microsoft.com 1Throughout the paper, we use the Õ notation to suppress logarithmic dependence on K,T, L and lnN/δ.\nar X\niv :1\n50 2.\n05 89\n0v 1\n[ cs\n.L G\n] 2\n0 Fe\nThe above applications, and others, often involve making a complex decision and receiving more detailed feedback about this decision. For example, in internet applications, we often recommend a set of items, but record information about the users interaction with each individual item (e.g., click, conversion, or hover time). To model this richer interaction, we assume that on each round, the learner makes a composite action, which is an ordered tuple of simple actions, and receives reward for the composite action, but also feedback about each simple actions played. This additional feedback is unhelpful unless it somehow relates to the total reward, and we assume that this relationship is linear. When no contextual information is present, this setting is referred to as semi-bandits [Audibert et al., 2014] or slate bandits [Kale et al., 2010] in the literature. Our goal is to design learning algorithms whose running time and statistical performance (measured by regret) scale with the number of simple actions rather than the number of composite actions.\nIn the first part of the paper, we assume that the linear relationship between the reward and the feedback on the simple actions is known, and we derive a new algorithm for contextual semi-bandits that meets our goal. Our approach builds on the recent contextual bandit algorithms of Dudı́k et al. [2011] and Agarwal et al. [2014] and enjoys a regret guarantee between Õ( √ KLT lnN) and Õ(L √ KT lnN), depending on the hardness of the problem, where K is the number of simple actions, each composite action consists of L simple actions, T is the number of rounds of the interaction and N is the size of a policy class that we are competing against2. The policy class Π is a set of functions mapping contexts into composite actions (e.g., linear learners, decision trees, or neural nets), which we access via an optimization oracle. We show that the algorithm makes Õ(T 3/2) calls to the optimization oracle3, meaning that, given an efficient supervised learning algorithm, the algorithm has running time that is only logarithmic in |Π|. This contrasts with the work of Kale et al. [2010] on contextual semi-bandits, which explicitly enumerates the policy class, and therefore has running time that is linear in |Π|. Moreover, our algorithm only executes composite actions that policies choose, which allows implicit encoding of constraints defining valid composite actions.\nIn the second part of the paper, we move to a more general setting. We still assume that the reward for the composite action is linearly related to the feedback for the simple actions (which we call “features”), but we no longer assume that the weights in this relationship are known; the composite reward is, however, still observed. Our solution for this setting is a two-stage algorithm. In the first stage, the algorithm chooses actions randomly to learn the weights in the linear mapping and to estimate the performance of each policy. Based on these estimates, we pick an empirically optimal policy and use it to exploit in the second stage. We show that the regret of this algorithm scales as Õ(‖w‖1(KT )3/4 √ lnN), where w is the true (unknown) weight vector. The number of calls to the optimization oracle is independent of |Π| in this case. While we expect the statistical performance of this algorithm to be sub-optimal, we view this as the first step in generalizing contextual semi-bandits to richer settings that assume less about the form of the reward.\nRelated Work. There is a growing body of work on combinatorial bandits, which are also referred to as semi-bandits, or slate bandits [György et al., 2007, Uchiya et al., 2010, Kale et al., 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that Õ( √ KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. [2002] to the contextual semi-bandits, and achieve a Õ( √ KLT ) regret but require explicit enumeration of the policy class. Qin et al. [2014] instead generalize the LinUCB algorithm of Chu et al. [2011] to semi-bandits, imposing the assumption that the feedback on the simple actions is linearly related\n2Extension to VC classes is straightforward using standard arguments. 3The dependence can be improved to Õ(T 1/2) using warm-start and epoching ideas identical to Agarwal et al. [2014].\nAlgorithm 1: SEMIBANDIT-VCEE (Variance Constrained Explore Exploit) Algorithm\ninput Allowed failure probability δ ∈ (0, 1). 1: Q0 = 0, the all zeros-vector. Define: µt = min { 1 2K , √ ln(16t2|Π|/δ) Kpmint } .\n2: for each round t = 1, . . . , T do 3: Observe context xt ∈ X 4: Sample At ∼ Qµt−1t−1 (·|xt) with remaining mass on πt−1, the previous best policy. 5: Play At and observe rewards {yt(at,l)}Ll=1. 6: Obtain Qt by solving OP with Ht = Ht−1 ∪ (xt, {yt(at,l)}Ll=1, {qt(at,l)}Ll=1) and µt. 7: end for\nSemi-bandit Optimization Problem (OP)\nGiven history H and a smoothing parameter µ, define bπ = ‖w‖1 ‖w‖22 R̂egt(π) ψµpmin and ψ = 100. Find Q ∈ ∆|Π| such that: ∑ π∈Π Q(π)bπ ≤ 2KL/pmin (1)\n∀π ∈ Π. Êx∼H [ L∑ l=1\n1\nQµ(π(x)l|x) ] ≤ 2KL pmin + bπ (2)\nto the context. This contrasts with our setting: we make no assumptions about the feedback on the simple actions, but impose that the overall reward is linearly related to this feedback.\nExcept for the work of Qin et al. [2014], all research on semi-bandits assumes that the reward for the composite action is the sum of the features for the simple actions. Qin et al. [2014] generalize this slightly by assuming that the reward is a known function of the context and features. We are not aware of any work that attempts to learn a relationship between the reward for the composite action and the simple action features as we do in the second part of the paper.\nAnother popular strategy used to cope with large action spaces is referred to as linear or parametric bandits [Filippi et al., 2010, Rusmevichientong and Tsitsiklis, 2010, Chu et al., 2011]. Here, the context revealed at each round includes a feature vector for each action, and the reward for each action is an unknown linear (or parametric) function of its feature vector. This can be a good fit for some applications, but it does not model those features that are only observed after playing actions. While our set up also assumes a linear relationship between the reward for the composite action and the features for the simple actions, the crucial difference is that the features are observed only after playing the composite action, rather than before."
    }, {
      "heading" : "2 SEMIBANDIT-VCEE for Contextual Semi-Bandits with Known Weights",
      "text" : "When the weights in the linear transformation are known, we propose an algorithm that has a similar structure to a recent algorithm for the classical contextual bandit problem [Agarwal et al., 2014]. The algorithm maintains a distribution over policies and uses the smoothed version to play actions at each round.\nThe core of the algorithm involves finding this distribution by solving a convex optimization problem, which we call OP, using the past record of interaction H . OP is a feasibility problem that looks for a distribution with both low regret (measured by the empirical regret) and low variance. The constraint in Equation 1 enforces that the distribution has low empirical regret, thereby placing mass on policies that are performing well. On the other hand, the constraint in Equation 2, ensures that the variance of the importance weighted reward estimates remains small for each policy π, and therefore requires that the distribution is not too peaked. The constraint enforces a bound on the variance in reward estimates for the policy π, at a level regulated by the empirical regret of π, thereby ensuring sufficient exploration amongst all good policies. These two constraints juggle the exploration-exploitation tradeoff, as Equation 1 encourages placing mass on good policies, while Equation 2 encourages playing more uniformly.\nThe main differences between SEMIBANDIT-VCEE and the algorithm of Agarwal et al. [2014] are in the OP and the definitions. One crucial modification is in the variance constraint in Equation 2. This constraint involves the marginal probabilities of the simple actions rather than the composite actions as would be the most obvious adaptation of the original algorithm to our setting. By working with the simple actions rather than the composite actions, this constraint leads to much tighter control of the importance-weighted reward estimates and consequently an improved regret bound.\nA related modification is in the definition of the reward estimates, which leverages the additional feedback on the simple actions. Without using this additional feedback in the reward estimate, the constraint in Equation 2 would not relate to the variance of this quantity. In addition to these definitional changes, which are really the crux of the modifications, we also modify the OP to account for the influence of the weight vector on the scaling of the rewards and the influence of pmin on the smoothing distribution."
    }, {
      "heading" : "2.1 Regret Guarantee for SEMIBANDIT-VCEE",
      "text" : "Our analysis of this algorithm leads to the following regret guarantee:\nTheorem 1. For any T ∈ N, with probability at least 1 − δ, there is a universal constant c0 > 0 such that the regret of Algorithm 1 is at most:\n(4ψ + c0) ‖w‖22 ‖w‖1 L\n( K ln(16T 2|Π|/δ)\npmin + 12\n√ KT ln(16T 2|Π|/δ)\npmin\n) (3)\nAsymptotically, the regret of SEMIBANDIT-VCEE is Õ(‖w‖ 2 2 ‖w‖1L √ KT ln |Π|/pmin) which scales sublinearly with the number of simple actions K and only logarithmically with the number of composite actions, which is Θ(KL). Note that any classical contextual bandit algorithm that ignores the additional feedback and structure in this setting would suffer regret Ω̃( √ KLT ln |Π|). Therefore, our result shows how a dramatic reduction in regret can be obtained through additional feedback. In comparison with related work, our bound matches the result of Kale et al. [2010] on slate bandits, which falls into a special case of our setting. Specifically, they assume that weights w = 1 and that uniform exploration is possible, and they obtain an Õ( √ KLT ln |Π|) regret bound. Theorem 1 matches this bound, as ‖w‖22 = ‖w‖1 = L and pmin = L in this case. Our result improves on theirs in two directions: statistically we show how a non-uniform weight vector and restricted exploration distribution affects the regret and, computationally, our algorithm can be efficiently implemented with an optimization oracle while theirs cannot.\nWhen uniform exploration is not allowed, as considered by Kveton et al. [2015] in the non-contextual setting, we can set pmin = 1 and our bound is worse than theirs by a factor of √ L. This discrepancy may\nAlgorithm 2: Coordinate Descent Algorithm for Semi-Bandit OP\ninput History H and smoothing parameter µ. 1: Initialize weights Q = 0 ∈ ∆Π. 2: while true do 3: For all π, define:\nVπ(Q) = Êx∼H [ L∑ l=1\n1\nQµ(π(x)l|x)\n] , Sπ(Q) = Êx∼H [ L∑ l=1\n1\nQµ(π(x)l|x)2\n] , Dπ(Q) = Vπ(Q)− 2KL\npmin − bπ\n4: If ∑ π Q(π)( 2KL pmin + bπ) > 2KL pmin , replace Q by cQ where c = 2KL/pmin∑ π Q(π)(2KL/pmin+bπ) < 1. 5: Else if ∃π s.t. Dπ(Q) > 0, update Q(π) = Q(π) + απ(Q) where απ(Q) = Vπ(Q)+Dπ(Q)2(1−Kµ)Sπ(Q) . 6: Otherwise halt and output Q. 7: end while\nbe a by-product of moving to the more challenging contextual setting, as a UCB-style algorithm, which they use, is no longer suitable. In particular, all contextual bandit algorithms we are aware involve some degree of uniform exploration, and it seems that Õ(L √ KT ln |Π|) is unavoidable if the best exploration distribution has pmin = O(1)."
    }, {
      "heading" : "2.2 Computational Guarantee for SEMIBANDIT-VCEE",
      "text" : "We now turn to analyzing the computational aspects of Algorithm 1. The main bottleneck is in solving the optimization problem (OP), and our analysis focuses on this subroutine. This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplicatively shrinks all of the weights. Otherwise, if there is a policy that is found to violate the variance constraint in Equation 2, the algorithm adds weight to that policy, so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descent algorithm:\nTheorem 2. For any history H and parameter µ, Algorithm 2 halts and outputs a set of weights Q ∈ ∆|Π| that is feasible for (OP). Moreover, Algorithm 2 halts in no more than 8 log(1/(Kµ))µpmin iterations and each iteration can be implemented efficiently, with at most one call to AMO.\nSince the main ideas used to prove this theorem are borrowed from the proof of Agarwal et al. [2014], we only provide a sketch in Section 2.4. Equipped with this theorem, it is easy to see that the total number of calls to the AMO over the course of the execution of Algorithm 1 can be bounded as Õ ( T 3/2 √ K\npmin log(|Π|/δ)\n) by the setting of µt. Moreover, due to the nature of the coordinate descent algo-\nrithm, the weight vector Q remains sparse, so we can manipulate it efficiently and avoid running time that is linear in |Π|. As mentioned, this contrasts with the exponential-weights style algorithm of Kale et al. [2010] which maintains a dense weight vector over ∆|Π|.\nWe mention in passing that Agarwal et al. [2014] also develop two improvements that lead to a more efficient algorithm. They partition the game into epochs and only solve OP once every epoch, rather than in every round as we do here. They also show how to use the weight vector from the previous round to warm-start the next coordinate descent execution. Both of these optimizations can also be implemented\nhere, and they will lead to a better computational guarantee for the algorithm, although we omit these details to simplify the presentation."
    }, {
      "heading" : "2.3 Proof Sketch of Theorem 1",
      "text" : "The proof of the regret bound is quite technical and we sketch the arguments here, deferring all details to Appendix A. To start off, we establish two uniform deviation bounds, one on the variance estimates used in Equation 2, and the other on the reward estimate ηt(π) used in bπ . For the former, we show that if µt and t are large enough, then simultaneously for all P, π, with high probability\nV (P, π, µt) ≤ 6.4V̂t(P, π, µt) + 81.3KL/pmin,\nwhere V (P, π, µt) = Ex∼D ∑L l=1 1 Pµ(π(x)l|x) and V̂t is the empirical version. The other main deviation bound is on the reward estimates. We use Freedman’s inequality to show that with high probability, for each time t and each policy π:\n|ηt(π,w)−R(π)| ≤ ‖w‖22 ‖w‖1 µt−1 (Vt(π)pmin +KL) ,\nwhere Vt(π) = max0≤τ≤t−1 V (Q̃τ , π, µτ ), and Q̃τ is the distribution used in the τ th round by our algorithm. The important thing with these bounds is that the variance deviation does not depend on the size of the composite action space, and that V̂ (P, π, µt), which we control in Equation 2, does indeed control the variance of the reward estimates. This tighter control is the main statistical gain.\nEquipped with these deviation inequalities, we proceed to bound the deviation between the empirical and the true regret. This is made possible by leveraging the variance control in Equation 2, leading to a bound on the reward estimates, which make up the empirical regret, R̂egt(·). A careful inductive argument leads to the bounds (with high probability):\nReg(π) ≤ 2R̂egt(π) + c0 ‖w‖22 ‖w‖1 KLµt and R̂egt(π) ≤ 2Reg(π) + c0 ‖w‖22 ‖w‖1 KLµt\nNow the constraint in Equation 1 ensures that the empirical regret is small, which, by the above inequalities, also ensures that the actual regret when playing according to Q̃t is small. In particular at round t, we play with distribution Q̃µt−1t−1 and we show that:∑\nπ\nQ̃t−1(π)Reg(π) ≤ C ‖w‖22 ‖w‖1 KLµt−1\nThis bound applies to the unsmoothed distribution, so we also must control the regret associated with smoothing. However, we know that the per-round regret is bounded by ‖w‖1 and the smoothing probability is Kµt, so both terms are bounded by the sum of the µts, which grows at rate √ T ."
    }, {
      "heading" : "2.4 Proof Sketch of Theorem 2",
      "text" : "First, if the algorithm halts, then both of the conditions must be satisfied. The regret condition must be satisfied since we know that ∑ π Q(π)(2KL/pmin + bπ) ≤ 2KL/pmin which in particular implies that∑\nπ Q(π)bπ ≤ 2KL/pmin as required. Note that this also ensures that ∑ π Q(π) ≤ 1 so Q ∈ ∆|Π|. Finally,\nif we halted, then for each π, we must have Dπ(Q) ≤ 0 which implies Vπ(Q) ≤ 2KLpmin + bπ so the variance constraint is also satisfied.\nThe algorithm can be implemented by first accessing the oracle on the importance weighted history Ĥ to obtain πt (so that we can compute bπ). The low regret check in Step 4 of Algorithm 2 can be done efficiently, since each policy in the support of the current distribution Q was added at a previous iteration of Algorithm 2, and we can store the regret of the policy at that time for no extra computational burden. This allows us to always maintain the expected regret of the current distribution Q for no added cost. Finding a policy violating the variance check can be done by one call to the oracle AMO. At round t of the contextual bandit problem, we create a dataset of the form (xi, zi, vi) of size 2t. The first t terms come from the variance Vπ(Q) and the second t terms come from the rescaled empirical regret bπ . For τ ≤ t, we define xτ to be the τ th context,\nzτ (a) = 1\ntQµ(a|xτ ) , and vτ = 1.\nWith this definition, it is easily seen that Vπ(Q) = ∑t τ=1 v T τ zτ (π(xτ )). For τ > t, we define xτ to be the context from round τ − t and\nzτ (a) = −‖w‖1\n‖w‖22tψµpmin ŷτ (a), and vτ = w.\nIt can now be verified that ∑2t τ=t+1 v T τ zτ recovers the bπ term up to additive constants independent of the policy π (essentially up to the ηt(πt) term). Combining everything, it can be checked that:\nDπ(Q) = 2t∑ τ=1 zτ (π(xτ )) T vτ − 2KL pmin − ‖w‖1 ‖w‖22 ηt(πt) ψµpmin\nThe two terms at the end are independent of π so by calling the argmax oracle with this 2t sized dataset, we can find the policy π with the largest value of Dπ . If the largest value is non-positive, then no constraint violation exists. If it is strictly positive, then we have found a constraint violator to update the probability distribution on.\nAs for the iteration complexity bound, the analysis is based on the potential function:\nΦ(Q) =\n( Ê[RE(UAS(x)||Qµ(·|x))]\n1−Kµ +\n∑ π Q(π)bπ\n2K/pmin\n)\nwhere RE(p||q) = ∑ a∈A pa log(pa/qa) + qa − pa is the unnormalized relative entropy. In Appendix B, we show two main facts about Algorithm 2 and this potential function:\n1. When the regret constraint is violated, the shrinking update does not increase the potential. More formally, for any c < 1, we have Φ(cQ) ≤ Φ(Q) whenever ∑ π Qπ( 2KL pmin + bπ) > 2KL pmin . 2. The additive update when Dπ > 0 for some π lowers the potential by at least Lµpmin4(1−Kµ) .\nThese are the analogs of Lemmas 6 and 7 in Agarwal et al. [2014]. The proof of the first is based on showing that the derivative of the function g(c) = Φ(cQ) is positive so that by convexity of Φ, shrinking the weights Q can only decrease the potential. The proof of the second involves directly calculating the difference in potential before and after the update, and we use a second order taylor expansion of log(x) to obtain the quadratic term Sπ(Q).\nAlgorithm 3: SEMIBANDIT-EELS (Explore-Exploit Least Squares)\ninput Time Horizon T , failure probability δ ∈ (0, 1)\n1: Set H = ∅, Σ = 0 ∈ RL×L, λ? = √ KLT ln(8/δ) ‖w‖2 , nT = ( TK‖w‖2 ‖w‖1 )2/3 ( ln(8|Π|/δ)L )\n1/3. 2: while λmin(Σ) ≤ λ?, for at least nT rounds do 3: Observe context xt, play action At ∼ P , where P is the uniform distribution on A(xt). 4: Observe reward rt(At) and feature vector yt(At). 5: Update Σ = Σ + yt(At)yt(At)T , H = H ∪ (xt, {yt(at,l)}Ll=1, {p(at,l)}Ll=1). 6: end while 7: Estimate weights: ŵ = Σ−1 ( ∑ i yi(Ai)ri(Ai)) (Least Squares). 8: Optimize policy π̂ = AMO(ŵ,H) with importance weighted feature vectors. 9: For every remaining round, observe context xt and play At = π̂(xt).\nIt is also easy to see that Φ(0) ≤ L log(1/(Kµ))/(1 − Kµ), that Φ(Q) is convex in Q and Φ is nonnegative. All of these facts together means that with at most 4 log(1/(Kµ)µpmin executions of the variance update, we will have decreased the potential to zero. By the fact that the potential is non-negative, this bounds the number of executions of the additive update. As the shrinking update will never be executed twice in a row by construction, we can at worst alternate between the two updates, so that the total number of iterations is 8 log(1/(Kµ)\nµpmin . This proves the theorem."
    }, {
      "heading" : "3 Contextual Semi-bandits with Unknown Weights",
      "text" : "When the weights are unknown, we propose an algorithm that first explores and then exploits. The aim of this algorithm is to explore so that two things happen: we can accurately estimate the weights on the simple action features, and we can use these to accurately estimate the expected reward for each policy. In this section we assume that the time horizon T is known to the algorithm. We also assume that for each round, one can play uniformly over a subset of simple actions A(x) ⊂ A, meaning that all combinations and orderings of legal simple actions are allowed.\nPseudocode for our algorithm, SEMIBANDIT-EELS, is displayed in Algorithm 3. Structurally, the algorithm devotes the first several rounds to uniform exploration, which leads to reliable estimates for the weight vector as well as the expected feature vector for each policy. Taking their inner product, we naturally obtain good estimates of the reward for each policy, which can then be used in the AMO to find the policy with the best empirical performance. For the remaining rounds, the algorithm plays according to this policy. The analog of this algorithm in a normal contextual bandit setting would be to explore uniformly over the actions for first few rounds, find the best policy by using the estimated rewards and then exploiting with that policy.\nThe tradeoff between exploration and exploitation is negotiated by two things: the smallest eigenvalue of the feature covariance and a minimum number of rounds. Since we perform least squares in line 7 to obtain a vector ŵ that we use for reward estimation, by exploring until the eigenvalues of Σ are large, we obtain a bound on ‖ŵ − w‖2. The other stopping condition ensures that the important weighted reward feature vectors are well behaved, and combined, these two conditions ensure that we are competitive with the optimal policy in the exploitation rounds.\nThe more challenging part of the analysis is ensuring that we do not accumulate too much regret in the exploration phase. The difficulty is that the condition involving the feature covariance does not bound the number of exploration rounds. Our analysis proceeds by showing that the eigenvalues of the feature\ncovariance can be lower bounded by a quantity that also upper bounds the exploration regret. Consequently, during exploration rounds we either have good estimates of the weights, or we have not accumulated too much regret and can afford to explore more. Formalizing this intuition reveals the following regret bound for Algorithm 3:\nTheorem 3. For any T ∈ N with probability at least 1− δ, the regret of Algorithm 3 is at most:\nO ( ‖w‖1(KT )3/4 √ ln(LT 2|Π|/δ) ) (4)\nThis theorem guarantees sublinear regret for Algorithm 3, and, to our knowledge, it is the first result on learning a relationship between simple action features and rewards under semi-bandit feedback. We do however make some undesirable assumptions. The algorithm requires knowledge of the time horizon T , which can be relaxed by variants of the Epoch-Greedy [Langford and Zhang, 2008] or -greedy approaches, although the analysis here is significantly simpler. We also make a strong assumption on the structure of the action space A(x) at each round, which is much harder to relax. We require that the exploration regret is related to the minimum eigenvalue of the feature covariance, and while there are slightly weaker conditions on the exploration distribution that enable this, the two quantities are not related in general, and therefore we may not be able to learn the weights while guaranteeing low exploration regret as we do here. We remind the reader that the exploration problem is much harder here, than say linear bandits, where the feature vector is revealed ahead of time rather than features being observed after taking the actions.\nNote that one can always run a classical contextual bandit algorithm here, ignoring the task of weight estimation and the simple action features. Such an algorithm can at best achieve Õ( √ KLT ) regret, as the number of composite actions is Θ(KL). Thus our algorithm is favorable for shorter time horizons. We leave as future work the challenge of tempering the dependence on T to T 1/2.\nWe now sketch the proof of Theorem 3, with details in Appendix C. Proof of Theorem 3 (Sketch only) The proof of the theorem is based on trading off the regret associated with exploration and exploitation rounds. We state the intermediate results in terms of λ? and nT , and will optimize over them at the end of the proof. Based on the stopping conditions for the exploration phase of the algorithm, the exploitation regret is bounded by:\nExploitation Regret ≤ 2TK 2‖w‖2 √\nln(8|Π|/δ) LnT +\n√ c ln(8/δ)\nλ?  The two terms in this decomposition are based on using ηt(π, ŵ) as an estimate ofR(π), which involves the true weight vector. The first term stems from the deviation between the importance weighted feature vectors ŷ and the true features vectors y, and is small provided that nT is large. The second term bounds the least squares error ‖ŵ − w‖2, which is small provided that λ? is large.\nWe show that the exploration regret is bounded by:\nExploration Regret ≤ ‖w‖1 ( 2nT + √ 2T ln(8/δ) ) + ‖w‖2 (√ KTλ?\n2L + T 3/4\n√ 2K ln1/4(8LT 2/δ)\n)\nThe first term here is straightforward; if the eigenvalues of Σ grow quickly, we perform at most nT rounds of exploration and suffer at most 2‖w‖1 regret per round. If the eigenvalues grow slowly, then we may explore for longer, but we know that these eigenvalues are small for the entire exploration phase. The three other terms stem from relating these eigenvalues to the exploration regret.\nWe first argue that the exploration regret for round t is bounded by ‖w‖2 √ K(x) L Var(yt) where Var(yt) =\n1 K(x) ∑ a∈A(x)(yt(a)− ȳt)2. This follows from the Cauchy-Schwarz inequality and, by a standard deviation bound, implies that the cumulative regret up to round t is bounded as:\nt∑ τ=1 rτ (π?(xτ ))− rτ (Aτ ) ≤ ‖w‖2 t∑ τ=1\n√ K(xτ ) Var(yτ )\nL + ‖w‖1\n√ 2t ln(8/δ)\nThe second term in Equation 5 is exactly the deviation term here. We also show, by direct calculation, that if we explore uniformly (over a subset), then in expectation over our random choice of action, the smallest eigenvalue of the covariance matrix at round t is lower bounded by 2 ∑t τ=1 Var(yτ ). By the Matrix-Hoeffding inequality, the eigenvalues of the sample feature covariance concentrate around this population version, leading to the bound:\nΣt\n( 2\nt∑ τ=1 Var(yτ )− 4L √ t ln(2Lt2/δ)\n) IL,\nwhere Σt is the feature covariance at round t. These two bounds imply that if the covariance matrix has small eigenvalues, then the total exploration regret must also be small. The fact that we only explore when λmin(Σ) is small immediately translates into a bound on the accumulated exploration regret. The third term in Equation 5 comes precisely from this argument, while the last comes from the deviation of the eigenvalues of Σ. We arrive at the theorem by optimizing over λ? and nT ."
    }, {
      "heading" : "4 Conclusion and Discussion",
      "text" : "In this paper we studied the contextual semi-bandit problem where the learner plays a composite action and observes features for each simple action in this tuple in addition to the total reward. We assumed that the reward was linearly related to the features of the simple actions. If this linear relationship is known, we showed that an adaptation of the algorithm of Agarwal et al. [2014] achieves regret between Õ( √ KLT lnN) and Õ(L √ KT lnN), and can be implemented efficiently with access to an optimization oracle. If the weights are unknown, we provided a simple algorithm that achieves Õ(‖w‖1(KT )3/4 lnN) regret. These algorithms show how to leverage additional feedback to avoid regret that scales with KL, the size of the composite action space.\nSeveral interesting questions arise from our work:\n1. When the weights are known, can we obtain Õ( √ KLT ) regret even when the set of feasible actions\nare constrained, rather than Õ(L √ KT ) regret as in Theorem 1? The work on non-contextual combinatorial bandits suggests that the answer is yes [Kveton et al., 2015], but all algorithms for contextual bandit learning involve some degree of uniform exploration, which would prohibit such a regret bound. 2. When the weights are unknown, can we obtain O( √ T ) regret while still avoiding dependence on the\nsize of the composite action space? 3. Can we learn other transformations in this partial feedback setting? Many applications call for mod-\neling interaction between simple actions, so moving beyond linear transformations is not only of theoretical interest.\nWe hope to address these questions in future work."
    }, {
      "heading" : "A Full Proof of Theorem 1",
      "text" : "The proof hinges on two uniform deviation bounds, and then a careful inductive analysis of the regret using the OP. The first deviation bound shows that the variance estimates used in Equation 2 are suitable estimators for the true variance of the distribution. To state this deviation bound, we need some definitions:\nV (P, π, µ) = Ex∼Dx [ L∑ l=1\n1\nPµ(π(x)l|x)\n] V̂t(P, π, µ) = Êx∼Ht [ L∑ l=1\n1\nPµ(π(x)l|x)\n] (5)\nThe deviation bound is in the following Theorem:\nTheorem 4. For any δ ∈ (0, 1), if:\nµt ≥ √ ln(2|Π|t2/δ) Ktpmin\nt ≥ 4K ln(2|Π|t 2/δ)\npmin\nthen with probability at least 1− δ, for all distribution P over Π, all π ∈ Π, and all t ∈ N, we have:\nV (P, π, µt) ≤ 6.4V̂t(P, π, µt) + 81.3 KL\npmin (6)\nProof. The proof of this theorem is similar in spirit to a related theorem in Agarwal et al. [2014]. We first use Freedman’s inequality (Lemma 18) to argue that for a fixed P, π, µ, and t, the empirical version of the variance is close to the true variance. We use a discretization of the set of all distributions and then take a union bound to extend this deviation inequality to all P, π, µ, t. In particular, we have:\nLemma 5. For fixed P, π, µ, t and for any λ ∈ [ 0, µpminL ] , with probability at least 1− δ:\nV (P, π, µ)− V̂t(P, π, µ) ≤ (e− 2)λL µpmin V (P, π, µ) + ln(1/δ) tλ\nProof. Let:\nZi = L∑ l=1\n1\nPµ(π(xi)l|xi) − Ex∼Dx L∑ l=1\n1\nPµ(π(x)l|x) ,\nand notice that 1t ∑t i=1 Zt = V̂t(P, π, µ) − V (P, π, µ). Clearly, EZi = 0 for all i and maxi |Zi| ≤ L µpmin since when we smooth by µ, each action that π could play must appear with probabiity at least µpmin. By the Cauchy-Schwarz and Holder’s Inequalities, the conditional variance is:\nEx∼DxZ2i ≤ Ex∼Dx ( L∑ l=1\n1\nPµ(π(x)l|x)\n)2 ≤ LEx∼Dx L∑ l=1\n1\nPµ(π(x)l|x)2\n≤ L µpmin Ex∼Dx L∑ l=1\n1\nPµ(π(x)l|x) =\nL\nµpmin V (P, π, µ).\nThe lemma now follows by Freedman’s inequality.\nTo prove the variance deviation bound, we next use a discretization lemma from Dudı́k et al. [2011], which immediately implies that for any P , there exists a distribution P ′ supported on at most Nt policies such that for ct > 0, if Nt ≥ 6γ2t µtpmin :\nV (P, π, µ)− V (P ′, π, µt) + ct ( V̂t(P ′, π, µt)− V̂t(P, π, µt) ) ≤ γt(V (P, π, µt) + ctV̂t(P, π, µt))\nWe set γt = √\n1−Kµt Ntµtpmin + 3 1−KµtNtµtpmin , ct = 1 1− (e−2)Lλtµtpmin , Nt = d 12(1−Kµt)µtpmin e and λt = 0.66µtpmin/L and\ntake a union bound over all t ∈ N, Nt-point distributions P over Π, and all π ∈ Π to arrive at:\nV (P, π, µt) ≤ 6.4V̂t(P, π, µt) + 6.3L ln(2|Π|2m2/δ)\nµttpmin + 75L(1−Kµt) ln |Π| µ2t tp 2 min .\nThe theorem now follows from the stated bounds on µt and t.\nThe other main deviation bound is a straightforward application of Freedman’s inequality and a union bound. To state the lemma, we must introduct one more definition. Let Vt(π) = max0≤τ≤t−1 V (Q̃τ , π, µτ ) where Q̃τ is Qτ (the distribution computed at the τ th round of the game) with any additional mass placed on πτ , the empirical regret minimizer at round τ .\nLemma 6. For any λt−1 ∈ [0, µt−1pmin/‖w‖1] for all t ∈ N, π ∈ Π and for any δ ∈ (0, 1), with probability at least 1− δ:\n|ηt(π)−R(π)| ≤ ‖w‖22Vt(π)λt−1 + ln(4t2|Π|/δ)\ntλt−1 (7)\nProof. The rewards form a martingale and it is easy to see that the τ th term has range bounded by ‖w‖1pminµτ−1 ≤ ‖w‖1 pminµt−1 since the µs are non-increasing. Moreover the conditional variance can be bounded by using the Cauchy-Schwarz Inequality:\nE[Z2τ |Hτ−1] ≤ ‖w‖22 L∑ l=1 Ex∼DxEy|x y(π(x)l) 2 Q̃ µτ−1 τ−1 (π(x)l|x) ≤ ‖w‖22V (Q̃τ−1, π, µτ−1 ≤ ‖w‖22Vt(π)\nAnd the claim now follows by Freedman’s inequality.\nEquipped with these two deviation bounds we will proceed to prove the main theorem. Define dt = ln(16t2|Π|/δ) and let:\nt0 = min t { dt t ≤ pmin 4K } .\nNote that t0 ≥ 4 since dt ≥ 1 and K ≥ pmin. Set ρ = mint>t0 √ t/(t− 1) and note that ρ ≤ √ 2. With this\ndefinition of dt, we see that with µt ≥ √\ndt Ktpmin and t ≥ 4Kdt/pmin we have that with probability 1− δ/8 Equation 6 holds for all distributions P , policies π and t ∈ N (provided t ≥ 4Kdt/pmin, i.e. t ≥ t0). We also have that, for all t ∈ N, π ∈ Π, with probability ≥ 1− δ/4:\n|ηt(π)−R(π)| ≤ ‖w‖22Vt(π)λt−1 + dt\ntλt−1 ,\nand we will set:\nλt = 1\n‖w‖1 √ pmindt 2Kt 1[t ≤ t0] + µt−1pmin ‖w‖1 1[t > t0]\nNotice that to apply Lemma 6 we require λt ∈ [0, µtpmin‖w‖1 ] so our setting of λt is only valid for t ≥ t0. Let E denote the event that both the variance and reward deviation bounds hold and observe that P(E) ≥ 1− δ/2.\nUsing the variance constraint, it is straightforward to prove the following Lemma:\nLemma 7. Assume event E holds, then for any round t ∈ N and any policy π ∈ Π, let t? be the epoch achieving the max in the definition of Vt(π). Then there are universal constants θ1 > 2 and θ2 such that:\nVt(π) ≤  2KL pmin if µt? = 1 2K θ1KL\npmin + ‖w‖1 ‖w‖22 R̂egt?(π) θ2µt?pmin if µt? ≤ 1 2K\n(8)\nProof. The first claim follows trivially by the definition of Vt(π) and the choice of µt? . For the second claim, we use the variance deviation bound and the optimization constraint. In particular, since 12K > µt? =√ dt?/(Kt?pmin) we have that t? ≥ 4Kdt?/pmin so we can apply the variance deviation bound:\nV (Q̃t? , π, µt?) ≤ 6.4V̂t?(Q̃t? , π, µt?) + 81.3 KL\npmin ,\nand we can use the optimization constraint which gives an upper bound on V̂t?(Q̃t? , π, µt?):\nV̂t?(Q̃t? , π, µt?) ≤ V̂t?(Qt? , π, µt?) ≤ 2KL pmin + ‖w‖1 ‖w‖22 R̂egt?(π) ψµt?pmin\nThe bound follows by the choice θ1 = 94.1 and θ2 = ψ/6.4.\nWe next compare Reg(π) and R̂eg(π) using the variance bounds above.\nLemma 8. Assume event E holds and define c0 = 4ρ(1 + θ1). For all t ≥ t0 and all policies π ∈ Π:\nReg(π) ≤ 2R̂egt(π) + c0 ‖w‖22 ‖w‖1 KLµt and R̂egt(π) ≤ 2Reg(π) + c0 ‖w‖22 ‖w‖1 KLµt (9)\nProof. The proof is by induction on t. As the base case, consider t = t0 where, by definition we have µt = 1/(2K) for all t < t0 so that Vt(π) ≤ 2KL/pmin for all π ∈ Π by Lemma 7. Using the reward deviation bounds, which hold under E we have:\n|ηt(π)−R(π)| ≤ ‖w‖22Vt(π)λt + dt tλt ≤ 2KL‖w‖22λt/pmin + dt tλt ,\nfor all π ∈ Π. Since we are in round t0, we know that dt0/t0 ≤ pmin/(4K) so we can set λt as specified above. This gives:\n|ηt(π)−R(π)| ≤ 2 √ 2 ‖w‖22 ‖w‖1 KLµt0 .\nHere we use the fact that ‖w‖1 ≤ √ L‖w‖2 and the definition of µt0 = √ dt0/(Kt0pmin). Now both directions of the bound follow from the triangle inequality and the optimality of πt for ηt(·) and π? forR(·). We also use the fact that c0 ≥ 4 √ 2 by definition of θ1.\nFor the inductive step, fix some round t and assume that the claim holds for all for all t0 ≤ t′ < t and all π ∈ Π. By the optimality of πt for ηt and Lemma 6 (with our choice of λt = µt−1pmin/‖w‖1), we have:\nReg(π)− R̂egt(π) = (R(π?)−R(π))− (ηt(πt)− ηt(π)) ≤ (R(π?)−R(π))− (ηt(π?)− ηt(π))\n≤ (Vt(π?) + Vt(π)) ‖w‖22 ‖w‖1 µt−1pmin + 2‖w‖1dt tµt−1pmin\nNow by Lemma 7, there exists rounds i, j < t such that:\nVt(π) ≤ θ1KL pmin + ‖w‖1 ‖w‖22 R̂egi(π) θ2µipmin 1[µi < 1/(2K)]\nVt(π?) ≤ θ1KL pmin + ‖w‖1 ‖w‖22 R̂egj(π?) θ2µjpmin 1[µj < 1/(2K)]\nFor the term involving Vt(π) if µi ≥ 1/(2K) then trivially we have the bound:\nVt(π) ‖w‖22 ‖w‖1 µt−1pmin ≤ θ1 ‖w‖22 ‖w‖1 KLµt−1\nOn the other hand, if µi < 1/(2K) then by the applying the inductive hypothesis to R̂egi(π) we have:\n‖w‖1 ‖w‖22 R̂egi(π) θ2µipmin ≤ ‖w‖1 ‖w‖22 2Reg(π) θ2µipmin + c0KL θ2pmin\nVt(π) ‖w‖22 ‖w‖1 µt−1pmin ≤ (θ1 + c0 θ2 )KLµt−1 + 2Reg(π) θ2\nSimilarly for the Vt(π?) term, we have the bound:\nVt(π?) ‖w‖22 ‖w‖1 µt−1pmin ≤ (θ1 + c0 θ2 ) ‖w‖22 ‖w‖1 KLµt−1 + 2 Reg(π?) θ2 ≤ (θ1 + c0 θ2 ) ‖w‖22 ‖w‖1 KLµt−1,\nsince π? has no regret. Combining these bounds gives:\nReg(π) ≤ 1 1− 2θ2\n( R̂egt(π) + 2 ( θ1 +\nc0 θ2 ) ‖w‖22 ‖w‖1 KLµt−1 + 2‖w‖1dt tµt−1pmin ) Recall that θ1 = 94.1, θ2 = ψ/6.4, ψ = 100, c0 = 4ρ(1 + θ1) and ρ ≤ √ 2. This means that θ2/2 ≤ 1/2 so the pre-multiplier on the R̂egt(π) term is at most 2. For the third term, since dt/t is non-increasing, we have theb ound dttµt−1pmin ≤ Kµt−1 by the definition of µt−1. We also use the bound ‖w‖ 2 1 ≤ L‖w‖22. Since µt−1 ≤ ρµt we replace all µt−1 terms with ρµt above. Lastly, one can verify that if θ2 ≥ 4ρ, which it is given our choice of ψ = 100 and ρ ≤ √ 2, the pre-multiplier to the ‖w‖ 2 2\n‖w‖1KLµt term is bounded by c0. This gives one direction of the inequality.\nThe other direction proceeds similarly to before. Under event E we have:\nR̂egt(π)− Reg(π) = ηt(πt)− ηt(π)−R(π?) +R(π) ≤ ηt(πt)− ηt(π)−R(πt) +R(π)\n≤ (Vt(π) + Vt(πt)) ‖w‖22 ‖w‖1 µt−1pmin + 2‖w‖1dt tµt−1pmin\nAs before, we have the bound:\nVt(π) ‖w‖22 ‖w‖1 µt−1pmin ≤ (θ1 + c0 θ2 )KLµt−1 + 2Reg(π) θ2\nbut for the Vt(πt) term we must use the inductive hypothesis twice. We know there exists a round j < t for which\nVt(πt) ≤ θ1 KL pmin + ‖w‖1 ‖w‖22 R̂egj(π) θ2µjpmin 1[µj < 1/(2K)].\nApplying the inductive hypothesis twice gives:\n‖w‖1 ‖w‖22 R̂egj(πt) θ2µjpmin ≤ ‖w‖1 ‖w‖22\n( 2Reg(πt) + c0 ‖w‖22 ‖w‖1KLµj ) θ2µjpmin\n≤ ‖w‖1 ‖w‖22\n2 (\n2R̂egt(πt) + c0 ‖w‖22 ‖w‖1KLµt ) + c0 ‖w‖22 ‖w‖1KLµj\nθ2µjpmin\n≤ 3c0 θ2 KL pmin\nHere we use the inductive hypothesis twice, once at round j and once at round t and then use the fact that πt has no regret at round t, i.e. R̂egt(πt) = 0. We also use the fact that the µs are non-increasing so that µt/µj ≤ 1. This gives the bound:\nVt(πt) ‖w‖22 ‖w‖1 µt−1pmin ≤ (θ1 + 3c0 θ2 ) ‖w‖22 ‖w‖1 KLµt−1\nCombining the bounds for Vt(π) and Vt(πt) gives:\nR̂egt(π) ≤ ( 1 + 2\nθ2\n) Reg(π) + ( 2θ1 +\n4c0 θ2 ) ‖w‖22 ‖w‖1 KLµt−1 + 2‖w‖1dt tµt−1pmin\nSince θ2 ≥ 2 the pre-multiplier on the first term is at most 2. As before, the third term is bounded by 2 ‖w‖22 ‖w‖1KLµt−1 and µt−1 ≤ ρµt. Then, by definition of c0, θ1, θ2, we have that ρ(2θ1 + 4c0/θ2 + 2) ≤ c0 which proves the claim.\nThe last key ingredient of the proof is the following Lemma, which shows that the low-regret constraint in Equation 1, which is based on the regret estimates, actually ensures low regret.\nLemma 9. Assume event E holds. Then for every round t ∈ N:∑ π∈Π Q̃t−1(π)Reg(π) ≤ (4ψ + c0) ‖w‖22 ‖w‖1 Kµt−1 (10)\nProof. If t ≤ t0 then µt−1 = 1/(2K) in which case (since Reg(π) ≤ ‖w‖1):∑ π∈Π Q̃t−1(π)Reg(π) ≤ ‖w‖1 ≤ ‖w‖22 ‖w‖1 L ≤ 2ψ ‖w‖ 2 2 ‖w‖1 KLµt−1,\nso the claim holds. Now for t > t0 we have:∑ π∈Π Q̃t−1Reg(π) ≤ ∑ π∈Π Q̃t−1(π) ( 2R̂egt−1(π) + c0 ‖w‖22 ‖w‖1 KLµt−1 )\n≤ ( 2 ∑ π∈Π Qt−1(π)R̂egt−1(π) ) + c0 ‖w‖22 ‖w‖1 KLµt−1\n≤ (4ψ + c0) ‖w‖22 ‖w‖1 KLµt−1\nThe first inequality follows by Lemma 8 and the second follows from the fact that Q̃t−1 places its remaining mass on πt−1 which suffers no empirical regret at round t − 1. The last inequality is due to the low regret optimization constraint.\nTo control the regret, we must first add up the µts, which relate to the probability of exploring. Our definition of µt differs from Agarwal et al. [2014] only in the introduction of pmin, so by a straightforward adaptation we have:\nLemma 10. For any T ∈ N:\nT∑ t=1 µt ≤ 2 √ TdT Kpmin\nand T∑ t=1 µt−1 ≤ t0 2K + √ 8TdT Kpmin\nWe are finally ready to prove the theorem by adding up the total regret for the algorithm.\nLemma 11. For any T ∈ N, with probability at least 1− δ, the regret after T rounds is at most:\n‖w‖22 ‖w‖1 L\n[ 2 √\n2T ln(2/δ) + (4ψ + c0 + 1) ( 2Kdt0 pmin + √ 8KTdT pmin )]\nProof. For each round t ∈ N let Zt = rt(π?(xt)) − rt(At) − ∑ π∈Π Q̃ µt−1 t−1 Reg(π). Since at round t, we play action At with probability Q̃ µt−1 t−1 , this sequence of random variables is clearly centered. Moreover we have |Zi| ≤ 2‖w‖1 and it follows by Azuma’s inequality (Lemma 19) that with probability at least 1− δ/2:\nT∑ t=1 |Zt| ≤ 2‖w‖1 √ 2T ln(2/δ)\nTo control the mean, we use event E , which, by Theorem 4 and Lemma 6 holds with probability at least\n1− δ/2. By another union bound, with probability at least 1− δ, the regret of the algorithm is bounded by:\nRegret ≤ 2‖w‖1 √ 2T ln(2/δ) + T∑ t=1 ∑ π∈Π Q̃ µt−1 t−1 (π)Reg(π)\n≤ 2‖w‖1 √ 2T ln(2/δ) + T∑ t=1 ∑ π∈Π (1−Kµt−1)Q̃t−1(π)Reg(π) + ‖w‖1Kµt−1 ≤ 2‖w‖1 √ 2T ln(2/δ) +\nT∑ t=1 (4ψ + c0 + 1) ‖w‖22 ‖w‖1 LKµt−1\n≤ ‖w‖ 2 2\n‖w‖1 L\n[ 2 √\n2T ln(2/δ) + (4ψ + c0 + 1) ( t0 2 + √ 8KTdT pmin )]\n≤ ‖w‖ 2 2\n‖w‖1 L\n[ 2 √\n2T ln(2/δ) + (4ψ + c0 + 1) ( 2Kdt0 pmin + √ 8KTdT pmin )] Here the first inequality is from the application of Azuma’s inequality above. The second one uses the definition of Q̃µt−1t−1 to split into rounds where we play as Q̃t−1 and rounds where we explore. The exploration rounds occur with probability Kµt−1, and on those rounds we suffer regret at most ‖w‖1. For the other rounds, we use Lemma 9 and then we use Lemma 10. We also use the identity ‖w‖1 ≤ L‖w‖22/‖w‖1 in order to collect terms. Finally we use the fact that t0 ≥ 4Kdt0/pmin."
    }, {
      "heading" : "B Full Proof for Theorem 2",
      "text" : "In this section we prove Theorem 2, characterizing the coordinate descent optimization Algorithm 2. Recall that the potential function we use in this analysis is:\nΦ(Q) =\n( Ê[RE(UAS(x)||Qµ(·|x))]\n1−Kµ +\n∑ π Q(π)bπ\n2K/pmin ) with:\nRE(p||q) = ∑ a∈A pa ln(pa/qa) + qa − pa\nFor intuition, note that the partial derivative of the potential function with respect to a coordinateQ(π) relate exactly the variance Vπ(Q):\n∂Φ(Q) ∂Q(π) =  1t ∑tτ=1∑a∈π(xt) −pa(1−Kµ)Qµ(a|xt) + (1−Kµ) 1−Kµ + bπ\n2K/pmin  ≤ ( −pmin K Vπ(Q) + L+ pminbπ 2K\n) = pmin 2K ( −2Vπ(Q) + 2KL pmin + bπ\n) = pmin 2K (−Dπ(Q)− Vπ(Q))\nThis means that if Dπ(Q) > 0, then the partial derivative is very negative, and by increasing the weight on Q, we can decrease the potential function Φ.\nWe establish the five facts:\n1. Φ(0) ≤ L ln(1/(Kµ))/(1 −Kµ). This follows by the fact that the exploration distribution in Qµ is exactly UAS(x).\n2. Φ(Q) is convex in Q.\n3. Φ(Q) ≥ 0 for all Q.\n4. The shrinking update when the regret constraint is violated does not increase the potential. More formally, for any c < 1, we have Φ(cQ) ≤ Φ(Q) whenever ∑ π Qπ(2KL/pmin + bπ) > 2KL/pmin.\n5. The additive update when Dπ > 0 for some π lowers the potential by at least Lµpmin4(1−Kµ) .\nThe first three are fairly straightforward and the proof of the later two are based on the arguments of Agarwal et al. [2014]. For the first claim we have:\nΦ(0) = ∑\na∈A(x)\npa ln( pa Kµpa )− (1−Kµ)pa\n1−Kµ ≤ L ln(1/(Kµ)) 1−Kµ\nSince the marginals pa some to at most L. Convexity of this function follows from the fact that the unnormalized relative entropy is convex in the second argument, and the fact that the marginal distribution is linear in the vector Q. The third fact follows by the non-negative of both the empirical regret bπ and of the unnormalized relative entropy RE(·||·).\nFor the fourth fact, we prove the following lemma. Lemma 12. Let Q be a weight vector for which ∑ π Q(π)(2KL/pmin − bπ) > 2KL/pmin and define c = 2KL/pmin∑ π Q(π)(2KL/pmin−bπ) < 1. Then Φ(cQ) ≤ Φ(Q).\nProof. Define g(c) = B0Φ(cQ) and let Qµc (a|x) = (1−Kµ)cQ(a|x) +KµPunif (a|x). By the chain rule, using the calculation of the derivative above, we have:\ng′(c) = B0 ∑ π Q(π) ∂Φ(cQ) ∂Q(π)\n≥ pminB0 2K ∑ π Q(π) 2KL pmin + bπ − 2Ê ∑ a∈π(x)\n1\nQµc (a|x)  For the last term, we have:∑\nπ\nQ(π)Ê ∑\na∈π(x)\n1\nQµc (a|x) = Ê ∑ a∈A ∑ π∈Π Q(π)1[a ∈ π(x)] Qµc (a|x)\n= Ê ∑ a∈A Q(a|x) Qµc (a|x) = 1 c Ê ∑ a∈A cQ(a|x) Qµc (a|x)\nNow define qa = cQ(a|x) and the inner sum can be upper bounded by: ≤ ∑ a∈A qa (1−Kµ)qa + µpmin = K 1 K ∑ a∈A 1 (1−Kµ) + µpmin/qa\n≤ K 1 (1−Kµ) + Kµpmin∑\na qa\n≤ K 1 (1−Kµ) + KµpminL\n= KL\npmin 1 L pmin (1−Kµ) +Kµ ≤ KL pmin\nThe first inequality uses the lower bound pmin/K for exploration distribution, then we use Jensen’s inequality and the fact that ∑ a qa ≤ L since c < 1. Finally, we use the fact that L/pmin ≥ 1 and Kµ ≤ 1 so that the first term in the denominator is ≥ 1−Kµ. Plugging this in above we have:\nΦ(c) ≥ pminB0 2K (∑ π Qπ ( 2KL pmin + bπ ) − 2KL pmin ) > 0\nBy the condition in the algorithm. Since g is convex, this means that g(c) is nondecreasing for all values exceeding c. Since c < 1, we have:\nB0Φ(Q) = g(1) ≥ g(c) = B0Φ(cQ) So any positive B0 is fine.\nAnd for the fifth fact, we have:\nLemma 13. Let Q denote a set of weights and suppose, for some policy π, that Dπ(Q) > 0. Let Q′ be the new set of weights which is identical except that Q′(π) = Q(π) + α with α = απ(Q) > 0. Then\nΦ(Q)− Φ(Q′) ≥ µLpmin 4(1−Kµ)\nProof. Let Q′(·) = Q(·) + α· = π, i.e. the update we perform when π is found to violate the inequality in the algorithm. Since Q′µ(a|x) = Qµ(a|x) + (1−Kµ)α1[a ∈ π(x)] only updates a few coordinates of the marginal probabilities, we have by a direct calculation:\n2K(Φ(Q)− Φ(Q′)) = 2K\n( Ê ∑ a pa ln(pa/q µ a )− pa ln(pa/q′µa ) + qµa − q′µa\n(1−Kµ) − αbπpmin 2K\n)\n= 2K\n1−Kµ Ê ∑ a∈π(x) pa ln ( q′µa qµa )− α(2KL+ bπpmin) ≥ 2pmin\n1−Kµ Ê ∑ a∈π(x) ln ( 1 + α(1−Kµ) Qµ(a|x) )− pminα(2KL pmin + bπ ) The term inside the expectation can be bounded using the fact that ln(1 + x) ≥ x− x2/2:\nÊ ∑\na∈π(x)\nln ( 1 +\nα(1−Kµ) Qµ(a|x)\n) ≥ Ê ∑ a∈π(x) α(1−Kµ) Qµ(a|x) − 1 2 ( α(1−Kµ) Qµ(a|x) )2 = α(1−Kµ)Vπ(Q)− α2(1−Kµ)2\n2 Sπ(Q)\nPlugging this in above gives a lower bound:\n2K(Φ(Q)− Φ(Q′)) ≥ 2pminαVπ(Q)− (1−Kµ)pminα2Sπ(Q)− pminα( 2KL\npmin + bπ)\n= pminα(Vπ(Q) +Dπ(Q))− (1−Kµ)pminα2Sπ(Q)\nUsing the definition Dπ(Q) = Vπ(Q)− 2KLpmin − bπ . Now we set α = (Vπ(Q)+Dπ(Q)) 2(1−Kµ)Sπ(Q) as in the algorithm and obtain:\n2K(Φ(Q)− Φ(Q′)) ≥ pmin(Vπ(Q) +Dπ(Q)) 2\n4(1−Kµ)Sπ(Q)\nNote that Sπ(Q) ≥ 1µpminVπ(Q) (by bounding one of the terms in the square by the range which is µpmin) and that Vπ(Q) > 2KLpmin since Dπ(Q) > 0. this gives:\n2K(Φ(Q)− Φ(Q′)) ≥ µp 2 min(Vπ(Q) +Dπ(Q)) 2\n4(1−Kµ)Vπ(Q)\n≥ µp 2 minVπ(Q) 4(1−Kµ) ≥ KLµpmin 2(1−Kµ)\nDividing both sides of this inequality by 2K proves the lemma."
    }, {
      "heading" : "C Full Proof of Theorem 3",
      "text" : "The proof of the theorem is based on trading off the regret associated with exploration and exploitation rounds. The first ingredient of the proof is a decomposition of the exploitation regret. For any policy π we first bound the deviation from our estimate of the reward to the true reward.\nLemma 14. For all policies π ∈ Π, for any δ ∈ (0, 1), if we have explored for n ≥ ln(2|Π|/δ) rounds, then with probability ≥ 1− δ:\n|ηn(π, ŵn)−R(π)| ≤ K√ L ‖ŵt − w‖2 + 2K‖w‖2\n√ ln(2|Π|/δ)\nLn\nProof. Start with the decomposition:\n|ηn(π, ŵn)−R(π,w)| = ∣∣∣∣∣ 1n n∑ i=1 ŷn(π(xi)) T ŵ − Ex,yy(π(x))Tw ∣∣∣∣∣ ≤\n ∣∣∣∣∣ 1n n∑ i=1 ŷi(π(xi)) T ŵ − 1 n n∑ i=1 ŷi(π(xi)) Tw ∣∣∣∣∣+∣∣∣∣∣ 1n n∑ i=1 ŷi(π(xi)) Tw − Ex,yy(π(x))Tw ∣∣∣∣∣\nFor the first term, since the features are bounded between [0, 1] and the minimum probability for any action is ≥ L/K, by the Cauchy-Schwarz inequality, this term can be bounded by:∣∣∣∣∣ 1n n∑ i=1 ŷi(π(xi)) T ŵ − 1 n n∑ i=1 ŷi(π(xi)) Tw ∣∣∣∣∣ ≤ K√L‖ŵ − w‖2 The second term can be controlled by a deviation bound similar to Lemma 6 above. For all policies π ∈ Π, and for any δ ∈ (0, 1), if λ ∈ [0, LK‖w‖1 ] then with probability ≥ 1− δ, we have:∣∣∣∣∣ 1n n∑ i=1 ŷi(π(xi)) Tw − Ex,yy(π(x))Tw\n∣∣∣∣∣ ≤ K2‖w‖22λL + ln(2|Π|/δ)nλt Since the minimum probability is at least L/K, the range term is bounded by ‖w‖1K/L and the variance is bounded by ‖w‖22K2/L. The result follows now by choosing λ = 1K‖w‖2 √ L ln(2|Π|/δ)\nn which is a valid setting when n ≥ ln(2|Π|/δ).\nTo control the regret associated with the exploitation rounds, we also need to bound ‖ŵ − w‖2 which follows from a standard analysis of linear regression.\nLemma 15. Let Σ denote the feature covariance matrix after the exploration phase. There is a universal constant c > 0 such that for any δ ∈ (0, 2/e), with probability ≥ 1− δ:\n‖ŵ − w‖2Σ ≤ cL ln(2/δ)\nProof. This lemma is just the standard analysis of fixed-design linear regression with bounded noise. By definition of the ordinary least squares estimator, we have ŵ = Σ−1Y T r where Y ∈ Rn×L is the matrix of features, r ∈ Rn is the responses and Σ = Y TY is the feature covariance. The true weight vector can be written as w = Σ−1Y T (r − ξ) where ξ ∈ Rn is the noise vector. Thus:\n‖ŵ − w‖2Σ = ‖Σ−1Y T ξ‖2Σ = ξTY Σ−1Y T ξ.\nSince Σ−1 = (Y TY )−1 this matrix in the middle is a projection matrix, and it can be written as UUT where U ∈ Rn×d. We now have to bound the term ‖UT ξ‖22. Note that the vector ξ is a subgaussian random vector with independent components, so we can apply subgaussian tail bounds. Specifically, Lemma 20, due to Rudelson and Vershynin [2013], reveals that with probablity ≥ 1− δ:\n‖UT ξ‖2 ≤ √ L+ √ c ln(2/δ)\nfor some universal constant c > 0. Squaring this inequality and using the upper bound on δ leads to the claim.\nSince we only exploit after the exploration phase, we know that for any exploitation round λmin(Σ) ≥ λ?. This immediately gives a `2 bound on the weight vector estimate and consequently a bound on the exploitation regret. Setting both failure probabilities to be δ/4, as we will take a union bound over four total events in this proof, we have:\n‖ŵ − w‖22 ≤ ‖Σ−1‖2‖ŵ − w‖2Σ ≤ cL ln(8/δ)\nλ?\nR(π̂)−R(π?) ≤ 4K‖w‖2\n√ ln(8|Π|/δ)\nLn + 2K\n√ c ln(8/δ)\nλ?\nBy a union bound over the two events here, the last bound holds with probability at least 1− δ/2. Since we explore for at least nT rounds, this means that the total exploitation regret is at most:\nExploitation Regret ≤ 4TK‖w‖2 √ ln(8|Π|/δ) LnT + 2TK √ c ln(8/δ) λ?\nThe more challenging part of the analysis is to bound the exploration regret. Since the length of the exploration phase is governed by λmin(Σ), we need to understand the spectral properties of this matrix in order to control the exploration regret. In the following lemma, we establish a lower bound on the smallest eigenvalue of precisely this matrix.\nLemma 16. Fix δ ∈ (0, 1). Let t? denote the last round for which λmin(Σ) ≤ λ?. For any t ≤ t?, let Σt denote the feature covariance matrix at time t, with probability at least 1− δ:\nΣt\n( 2\nt∑ τ=1 Var(yτ )− 4L √ t ln(2Lt2/δ)\n) IL\nProof. The lemma follows from an application of the Matrix Bernstein inequality (Lemma 21) and an analysis of the mean of Σt, where the randomness is over the actions taken by the algorithm. To characterize the mean, consider a fixed exploration round τ with context xτ and let Kτ = |A(xτ )| denote the size of the feasible set of simple actions. Let Sτ = EA∼P [yτ (A)yτ (A)T ] ∈ RL×L be the mean matrix for that round. We have:\nzTSτz = L∑ l=1 z2l ∑\na∈A(xτ )\n1\nKτ y(a)2 + ∑ l 6=l′ zlzl′ ∑\na 6=a′∈A(xτ )\ny(a)y(a′)\nKτ (Kτ − 1)\n= ‖y‖22‖z‖22 Kτ + ∑ l 6=l′ zlz ′ l ∑ a,a′∈A(xτ ) y(a)y(a′) Kτ (Kτ − 1) − ∑ l 6=l′ zlz ′ l ∑ a\ny(a)2\nKτ (Kτ − 1)\nDefine ȳ = 1Kτ ∑ a∈A(xτ ) y(a), E(y 2) = 1Kτ ∑ a∈A(xτ ) y(a)\n2 and Var(y) = E(y2) − ȳ2. This expression becomes:\nzTSτz = E[y2]‖z‖22 + ∑ l 6=l′ zlzl′ ( Kτ Kτ − 1 ȳ2 − 1 Kτ − 1 Ey2 )\n= Kτ\nKτ − 1 Var(y)‖z‖22 +\n( Kτ\nKτ − 1 ȳ2 − 1 Kτ − 1 Ey2\n) (zT1)2\nThe second term is positive for any z, as long as the y vector is non-negative, as it is in our case. This means that for any unit vector z, we have:\nzTSτz ≥ Kτ\nKτ − 1 Var(y) ≥ 2 Var(y)\nSince if Kτ < 2 we only have one simple action available, so there are no feasible composite actions. Now we can apply the Matrix Hoeffding inequality. Let zτ = yτ (At) ∈ RL be the realization of the feature vector on round τ . Define the random matrices:\nZτ = zτz T τ − EA∼P [yτ (A)yτ (A)T ]\nNote that we are only considering the randomization of the actions A ∼ P and our bound leaves the dependence on nature’s randomness (i.e. the yτ ). It is easy to see that ‖Zτ‖2 ≤ 2L trivially, since y, z ∈ [0, 1], so that the scaling term in the concentration bound is σ2 ≤ 2tL2. The Matrix Hoeffding Inequality reveals that for all rounds t ∈ N with probability at least 1− δ we have:\n‖ t∑\nτ=1\nZτ‖2 ≤ 4L √ t ln(2t2L/δ)\nCombining with our lower bound on the mean for these matrices, we obtain the theorem.\nThe regret associated with exploration can be bounded as follows:\nLemma 17. Let t? denote the last round for which λmin(Σ) ≤ λ? and fix δ ∈ (0, 1). Then with probability at least 1− δ over the randomness of the actions of the algorithm, the regret up to round t? − 1 is bounded by:\nt?−1∑ t=1 rt(π?(xt))− rt(At) ≤ ‖w‖2 t?−1∑ t=1\n√ K Varp(yt)\nL + ‖w‖1\n√ 2(t? − 1) ln(2/δ)\nProof. Let us first characterize the expected regret, where expectation is just over the actions of the algorithm. Assume without loss of generality that π? plays A? = (α1, . . . , αL). Let ȳtl = Ea∼plyt(a) for each l and let ȳt = (ȳt1, . . . , ȳtL)T . By the Cauchy-Schwarz inequality:\nrt(π?(xt))− EA∼P rt(A) = EA∼P L∑ l=1 wl(yt(αl)− yt(al))\n= wT (yt(π?(xt))− ȳt) ≤ ‖w‖ √√√√ L∑ l=1 (yt(αl)− ȳtl)2\nBy Holder’s inequality:√√√√ L∑ l=1 (yt(αl)− ȳtl)2 ≤ √√√√( L∑ l=1 p(αl)(yt(αl)− ȳtl)2 )( max l 1 p(αl) ) ≤ √ K Var(yt) L ,\nsince each action appears with probability at least L/K. This gives:\nt?−1∑ t=1 rt(π?(xt))− EA∼P rt(A) ≤ ‖w‖2 t?−1∑ t=1\n√ K Var(yt)\nL\nTo account for the randomness in the algorithm, notice that the random variables rt(At)− EA∼P rt(A) are bounded by 2‖w‖1 and centered, so we can apply Hoeffding’s inequality. Specifically, with probability at least 1− δ, we have: ∣∣∣∣∣ t?−1∑ t=1 rt(At)− EA∼P rt(A)\n∣∣∣∣∣ ≤ ‖w‖1√2(t? − 1) ln(2/δ) Combining this deviation bound with the expected regret bound above proves the theorem.\nThe insight is that the exploration regret is related to the exploration stopping condition. In particular, the exploration regret associated with the rounds up until t? can be bounded as:\nt?−1∑ t=1 rt(π?(xt))− EA∼P rt(A)\n≤ ‖w‖2\n√ K\nL t?−1∑ t=1 √ Var(yt) + ‖w‖1 ( 1 + √ 2(t? − 1) ln(8/δ) )\n≤ ‖w‖2 √ Kt? L √√√√ t?∑ t=1 Var(yt) + ‖w‖1 ( 1 + √ 2t? ln(8/δ) )\n≤ ‖w‖2 √ Kt? L √ 1 2 λ? + 2L √ t? ln(8Lt2?/δ) + ‖w‖1 ( 1 + √ 2t? ln(8/δ) ) ≤ ‖w‖2 √ KTλ?\n2L + ‖w‖2T 3/4\n√ 2K ln1/4(8LT 2/δ) + ‖w‖1 ( 1 + √ 2T ln(8/δ) ) By allocating failure probability δ/4 to each of the two events above, this holds with probability ≥ 1− δ/2. Here we had to account for the fact that our upper bound on λmin(Σ) ≤ λ? is only valid up to round t? − 1, but the regret in any round, in particular round t?, can be bounded by ‖w‖1. The exploration regret associated with the rounds up until nT can be trivially bounded by nT ‖w‖1. This means that the total exploration regret is at most:\nExploration Regret ≤ ‖w‖1 ( 2nT + √ 2T ln(8/δ) ) + ‖w‖2 (√ KTλ?\n2L + T 3/4\n√ 2K ln1/4(8LT 2/δ) ) We now have to select both the thresholds λ? and nT . The terms involving λ? are:\nmin λ? ‖w‖2√KTλ? 2L + 2TK √ c ln(8/δ) λ?  ≤ 3(KT )3/4√c‖w‖2 4√ ln(8/δ) L\nAnd this bound is achieved when we set λ? = √ KLT log(8/δ)\n‖w‖2 . Optimizing over nT reveals that we should select nT = ( TK‖w‖2 ‖w‖1 )2/3 (ln(8|Π|/δ)/L)1/3 and gives the bound:\nmin nT\n2nT ‖w‖1 + 4TK‖w‖2 √\nln(8|Π|/δ) LnT\n ≤ 6(TK)2/3‖w‖2/32 3 √ ‖w‖1 ln(4|Π|/δ)\nL\nWith both of these bounds, the total regret is at most:\nRegret ≤  6(TK)2/3‖w‖2/32 3 √ ‖w‖1 ln(4|Π|/δ) L + 3(KT )3/4 √ c‖w‖2 4 √ ln(8/δ) L\n+ ‖w‖1 √ 2T ln(8/δ) + ‖w‖2T 3/4 √ 2K ln1/4(8LT 2/δ)\n= O ( ‖w‖1(KT )3/4 √ ln(LT 2|Π|/δ) ) .\nThis bound holds with probability at least 1− δ since both the exploration regret and the exploitation regret bounds each hold with probability at least 1− δ/2."
    }, {
      "heading" : "D Deviation Bounds",
      "text" : "We collect here several deviation bounds that we use in our proofs. All of these results are well known and we point to references rather than provide the proofs. The first inequality, which is a Bernstein-type deviation bound for martingales, is Freedman’s inequality, which is from Beygelzimer et al. [2011]\nLemma 18 (Freedman’s Inequality). Let X1, X2, . . . , XT be a sequence of real-valued random variables. Assume for all t ∈ {1, 2, . . . , T} that Xt ≤ R and E[Xt|X1, . . . , Xt−1] = 0. Define S = ∑T t=1Xt and\nV = ∑T t=1 E[X2t |X1, . . . , Xt−1]. For any δ ∈ (0, 1) and λ ∈ [0, 1/R], with probability at least 1− δ:\nS ≤ (e− 2)λV + ln(1/δ) λ\nWe also use Azuma’s inequality, a Hoeffding-type inequality for martingales.\nLemma 19 (Azuma’s Inequality). Let X1, X2, . . . , XT be a sequence of real-valued random variables. Assume for all t ∈ {1, 2, . . . , T} that Xt ≤ R and E[Xt|X1, . . . , Xt−1] = 0. Define S = ∑T t=1Xt. For any δ ∈ (0, 1) and λ ∈ [0, 1/R], with probability at least 1− δ:\nS ≤ R √ 2T ln(1/δ)\nWe also make use of a vector-valued version of Hoeffding’s inequality, due to Rudelson and Vershynin [2013].\nLemma 20 (Vector-valued subgaussian concentration). Let A ∈ Rm×n be a fixed matrix, and let X = (X1, . . . , Xn) be independent random variables with EXi = 0 and |Xi| ≤ 1 almost surely. Then there is a universal constant c > 0 such that, for any δ ∈ (0, 1), with probability at least 1− δ:\n‖AX‖2 − ‖A‖F ≤ ‖A‖2 √ c ln(1/δ)\nFinally, we use a well known matrix-valued version of Hoeffding’s inequality, for example from Tropp [2011].\nLemma 21 (Matrix-Hoeffding). Consider a finite sequence {Xk} of independent random, self-adjoint, matrices with dimension d, and let {Ak} be a sequence of fixed self-adjoint matrices. Assume that for each random matrix, we have EXk = 0 and X2k A2k almost surely. Then for any δ ∈ (0, 1), with probability at least 1− δ:\nλmax( ∑ k Xk) ≤ √ 8σ2 ln(d/δ) with σ2 = ‖ ∑ k A2k‖2"
    } ],
    "references" : [ {
      "title" : "Taming the monster: A fast and simple algorithm for contextual bandits",
      "author" : [ "Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E Schapire" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2014
    }, {
      "title" : "Minimax policies for combinatorial prediction",
      "author" : [ "Jean-Yves Audibert", "Sébastien Bubeck", "Gábor Lugosi" ],
      "venue" : "games. arXiv:1105.4871,",
      "citeRegEx" : "Audibert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Audibert et al\\.",
      "year" : 2011
    }, {
      "title" : "Regret in online combinatorial optimization",
      "author" : [ "Jean-Yves Audibert", "Sébastien Bubeck", "Gábor Lugosi" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Audibert et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Audibert et al\\.",
      "year" : 2014
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Contextual bandit algorithms with supervised learning guarantees",
      "author" : [ "Alina Beygelzimer", "John Langford", "Lihong Li", "Lev Reyzin", "Robert E Schapire" ],
      "venue" : "In Artificial Intelligence and Statistics,",
      "citeRegEx" : "Beygelzimer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Beygelzimer et al\\.",
      "year" : 2011
    }, {
      "title" : "Counterfactual reasoning and learning systems: The example of computational advertising",
      "author" : [ "Léon Bottou", "Jonas Peters", "Joaquin Quiñonero-Candela", "Denis Xavier Charles", "D. Max Chickering", "Elon Portugaly", "Dipankar Ray", "Patrice Simard", "Ed Snelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bottou et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bottou et al\\.",
      "year" : 2013
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "Sébastien Bubeck", "Nicolò Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi.",
      "year" : 2012
    }, {
      "title" : "Combinatorial pure exploration of multi-armed bandits",
      "author" : [ "Shouyuan Chen", "Tian Lin", "Irwin King", "Michael R Lyu", "Wei Chen" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Combinatorial multi-armed bandit: General framework and applications",
      "author" : [ "Wei Chen", "Yajun Wang", "Yang Yuan" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Chen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2013
    }, {
      "title" : "Contextual bandits with linear payoff functions",
      "author" : [ "Wei Chu", "Lihong Li", "Lev Reyzin", "Robert E Schapire" ],
      "venue" : "In Artificial Intelligence and Statistics,",
      "citeRegEx" : "Chu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chu et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient optimal learning for contextual bandits",
      "author" : [ "Miroslav Dudı́k", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang" ],
      "venue" : "In Uncertainty and Artificial Intelligence,",
      "citeRegEx" : "Dudı́k et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dudı́k et al\\.",
      "year" : 2011
    }, {
      "title" : "Parametric bandits: The generalized linear case",
      "author" : [ "Sarah Filippi", "Olivier Cappe", "Aurélien Garivier", "Csaba Szepesvári" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Filippi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Filippi et al\\.",
      "year" : 2010
    }, {
      "title" : "The on-line shortest path problem under partial monitoring",
      "author" : [ "András György", "Tamás Linder", "Gábor Lugosi", "György Ottucsák" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "György et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "György et al\\.",
      "year" : 2007
    }, {
      "title" : "Non-stochastic bandit slate problems",
      "author" : [ "Satyen Kale", "Lev Reyzin", "Robert E Schapire" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kale et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kale et al\\.",
      "year" : 2010
    }, {
      "title" : "Matroid bandits: Fast combinatorial optimization with learning",
      "author" : [ "Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Hoda Eydgahi", "Brian Eriksson" ],
      "venue" : "In Uncertainty and Artificial Intelligence,",
      "citeRegEx" : "Kveton et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kveton et al\\.",
      "year" : 2014
    }, {
      "title" : "Tight regret bounds for stochastic combinatorial semi-bandits",
      "author" : [ "Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesvári" ],
      "venue" : "In Artificial Intelligence and Statistics,",
      "citeRegEx" : "Kveton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kveton et al\\.",
      "year" : 2015
    }, {
      "title" : "The epoch-greedy algorithm for multi-armed bandits with side information",
      "author" : [ "John Langford", "Tong Zhang" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Langford and Zhang.,? \\Q2008\\E",
      "shortCiteRegEx" : "Langford and Zhang.",
      "year" : 2008
    }, {
      "title" : "A contextual-bandit approach to personalized news article recommendation",
      "author" : [ "Lihong Li", "Wei Chu", "John Langford", "Robert E. Schapire" ],
      "venue" : "In International Conference on World Wide Web,",
      "citeRegEx" : "Li et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "Contextual combinatorial bandit and its application on diversified online recommendation",
      "author" : [ "Lijing Qin", "Shouyuan Chen", "Xiaoyan Zhu" ],
      "venue" : "In SIAM International Conference on Data Mining,",
      "citeRegEx" : "Qin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2014
    }, {
      "title" : "The analysis of randomized and nonrandomized AIDS treatment trials using a new approach to causal inference in longitudinal studies",
      "author" : [ "J.M. Robins" ],
      "venue" : "In Health Service Research Methodology: A Focus on AIDS,",
      "citeRegEx" : "Robins.,? \\Q1989\\E",
      "shortCiteRegEx" : "Robins.",
      "year" : 1989
    }, {
      "title" : "Hanson-wright inequality and sub-gaussian concentration",
      "author" : [ "Mark Rudelson", "Roman Vershynin" ],
      "venue" : null,
      "citeRegEx" : "Rudelson and Vershynin.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rudelson and Vershynin.",
      "year" : 2013
    }, {
      "title" : "Linearly parameterized bandits",
      "author" : [ "Paat Rusmevichientong", "John N Tsitsiklis" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Rusmevichientong and Tsitsiklis.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rusmevichientong and Tsitsiklis.",
      "year" : 2010
    }, {
      "title" : "User-Friendly Tail Bounds for Sums of Random Matrices",
      "author" : [ "Joel A. Tropp" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "Tropp.,? \\Q2011\\E",
      "shortCiteRegEx" : "Tropp.",
      "year" : 2011
    }, {
      "title" : "Algorithms for adversarial bandit problems with multiple plays",
      "author" : [ "Taishi Uchiya", "Atsuyoshi Nakamura", "Mineichi Kudo" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Uchiya et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Uchiya et al\\.",
      "year" : 2010
    }, {
      "title" : "The proof of this theorem is similar in spirit to a related theorem",
      "author" : [ "Agarwal" ],
      "venue" : null,
      "citeRegEx" : "Agarwal,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal",
      "year" : 2014
    }, {
      "title" : "The first three are fairly straightforward and the proof of the later two are based on the arguments",
      "author" : [ "Agarwal" ],
      "venue" : null,
      "citeRegEx" : "Agarwal,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal",
      "year" : 2014
    }, {
      "title" : "Freedman’s Inequality)",
      "author" : [ "Beygelzimer" ],
      "venue" : "Let X1,",
      "citeRegEx" : "Beygelzimer,? \\Q2011\\E",
      "shortCiteRegEx" : "Beygelzimer",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "If the linear transformation is known, we analyze an algorithm that is structurally similar to the algorithm of Agarwal et al. [2014] and show that it enjoys a regret bound between Õ( √ KLT lnN) and Õ(L √ KT lnN), where K is the number of actions, L is the length of each action sequence, T is the number of rounds, and N is the number of policies.",
      "startOffset" : 112,
      "endOffset" : 134
    }, {
      "referenceID" : 6,
      "context" : "Learning from partial feedback (“bandit” feedback) is of great practical importance and has seen a recent surge of research interest [Bubeck and Cesa-Bianchi, 2012].",
      "startOffset" : 133,
      "endOffset" : 164
    }, {
      "referenceID" : 19,
      "context" : "Motivating examples include healthcare [Robins, 1989] – where we only observe the result of the treatment prescribed to the patient, but obtain no information about how other treatments would have worked – or Internet applications [Li et al.",
      "startOffset" : 39,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "When no contextual information is present, this setting is referred to as semi-bandits [Audibert et al., 2014] or slate bandits [Kale et al.",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 13,
      "context" : ", 2014] or slate bandits [Kale et al., 2010] in the literature.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "When no contextual information is present, this setting is referred to as semi-bandits [Audibert et al., 2014] or slate bandits [Kale et al., 2010] in the literature. Our goal is to design learning algorithms whose running time and statistical performance (measured by regret) scale with the number of simple actions rather than the number of composite actions. In the first part of the paper, we assume that the linear relationship between the reward and the feedback on the simple actions is known, and we derive a new algorithm for contextual semi-bandits that meets our goal. Our approach builds on the recent contextual bandit algorithms of Dudı́k et al. [2011] and Agarwal et al.",
      "startOffset" : 88,
      "endOffset" : 667
    }, {
      "referenceID" : 0,
      "context" : "[2011] and Agarwal et al. [2014] and enjoys a regret guarantee between Õ( √ KLT lnN) and Õ(L √ KT lnN), depending on the hardness of the problem, where K is the number of simple actions, each composite action consists of L simple actions, T is the number of rounds of the interaction and N is the size of a policy class that we are competing against2.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "[2011] and Agarwal et al. [2014] and enjoys a regret guarantee between Õ( √ KLT lnN) and Õ(L √ KT lnN), depending on the hardness of the problem, where K is the number of simple actions, each composite action consists of L simple actions, T is the number of rounds of the interaction and N is the size of a policy class that we are competing against2. The policy class Π is a set of functions mapping contexts into composite actions (e.g., linear learners, decision trees, or neural nets), which we access via an optimization oracle. We show that the algorithm makes Õ(T ) calls to the optimization oracle3, meaning that, given an efficient supervised learning algorithm, the algorithm has running time that is only logarithmic in |Π|. This contrasts with the work of Kale et al. [2010] on contextual semi-bandits, which explicitly enumerates the policy class, and therefore has running time that is linear in |Π|.",
      "startOffset" : 11,
      "endOffset" : 787
    }, {
      "referenceID" : 0,
      "context" : ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that Õ( √ KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al.",
      "startOffset" : 8,
      "endOffset" : 353
    }, {
      "referenceID" : 0,
      "context" : ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that Õ( √ KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting.",
      "startOffset" : 8,
      "endOffset" : 375
    }, {
      "referenceID" : 0,
      "context" : ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that Õ( √ KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al.",
      "startOffset" : 8,
      "endOffset" : 427
    }, {
      "referenceID" : 0,
      "context" : ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that Õ( √ KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. [2002] to the contextual semi-bandits, and achieve a Õ( √ KLT ) regret but require explicit enumeration of the policy class.",
      "startOffset" : 8,
      "endOffset" : 481
    }, {
      "referenceID" : 0,
      "context" : ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that Õ( √ KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. [2002] to the contextual semi-bandits, and achieve a Õ( √ KLT ) regret but require explicit enumeration of the policy class. Qin et al. [2014] instead generalize the LinUCB algorithm of Chu et al.",
      "startOffset" : 8,
      "endOffset" : 617
    }, {
      "referenceID" : 0,
      "context" : ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that Õ( √ KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. [2002] to the contextual semi-bandits, and achieve a Õ( √ KLT ) regret but require explicit enumeration of the policy class. Qin et al. [2014] instead generalize the LinUCB algorithm of Chu et al. [2011] to semi-bandits, imposing the assumption that the feedback on the simple actions is linearly related 2Extension to VC classes is straightforward using standard arguments.",
      "startOffset" : 8,
      "endOffset" : 678
    }, {
      "referenceID" : 0,
      "context" : "3The dependence can be improved to Õ(T 1/2) using warm-start and epoching ideas identical to Agarwal et al. [2014].",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 16,
      "context" : "Except for the work of Qin et al. [2014], all research on semi-bandits assumes that the reward for the composite action is the sum of the features for the simple actions.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "Except for the work of Qin et al. [2014], all research on semi-bandits assumes that the reward for the composite action is the sum of the features for the simple actions. Qin et al. [2014] generalize this slightly by assuming that the reward is a known function of the context and features.",
      "startOffset" : 23,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "When the weights in the linear transformation are known, we propose an algorithm that has a similar structure to a recent algorithm for the classical contextual bandit problem [Agarwal et al., 2014].",
      "startOffset" : 176,
      "endOffset" : 198
    }, {
      "referenceID" : 0,
      "context" : "The main differences between SEMIBANDIT-VCEE and the algorithm of Agarwal et al. [2014] are in the OP and the definitions.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 13,
      "context" : "In comparison with related work, our bound matches the result of Kale et al. [2010] on slate bandits, which falls into a special case of our setting.",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 13,
      "context" : "In comparison with related work, our bound matches the result of Kale et al. [2010] on slate bandits, which falls into a special case of our setting. Specifically, they assume that weights w = 1 and that uniform exploration is possible, and they obtain an Õ( √ KLT ln |Π|) regret bound. Theorem 1 matches this bound, as ‖w‖2 = ‖w‖1 = L and pmin = L in this case. Our result improves on theirs in two directions: statistically we show how a non-uniform weight vector and restricted exploration distribution affects the regret and, computationally, our algorithm can be efficiently implemented with an optimization oracle while theirs cannot. When uniform exploration is not allowed, as considered by Kveton et al. [2015] in the non-contextual setting, we can set pmin = 1 and our bound is worse than theirs by a factor of √ L.",
      "startOffset" : 65,
      "endOffset" : 720
    }, {
      "referenceID" : 0,
      "context" : "This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2).",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplicatively shrinks all of the weights. Otherwise, if there is a policy that is found to violate the variance constraint in Equation 2, the algorithm adds weight to that policy, so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descent algorithm: Theorem 2. For any history H and parameter μ, Algorithm 2 halts and outputs a set of weights Q ∈ ∆|Π| that is feasible for (OP). Moreover, Algorithm 2 halts in no more than 8 log(1/(Kμ)) μpmin iterations and each iteration can be implemented efficiently, with at most one call to AMO. Since the main ideas used to prove this theorem are borrowed from the proof of Agarwal et al. [2014], we only provide a sketch in Section 2.",
      "startOffset" : 43,
      "endOffset" : 1062
    }, {
      "referenceID" : 0,
      "context" : "This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplicatively shrinks all of the weights. Otherwise, if there is a policy that is found to violate the variance constraint in Equation 2, the algorithm adds weight to that policy, so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descent algorithm: Theorem 2. For any history H and parameter μ, Algorithm 2 halts and outputs a set of weights Q ∈ ∆|Π| that is feasible for (OP). Moreover, Algorithm 2 halts in no more than 8 log(1/(Kμ)) μpmin iterations and each iteration can be implemented efficiently, with at most one call to AMO. Since the main ideas used to prove this theorem are borrowed from the proof of Agarwal et al. [2014], we only provide a sketch in Section 2.4. Equipped with this theorem, it is easy to see that the total number of calls to the AMO over the course of the execution of Algorithm 1 can be bounded as Õ ( T 3/2 √ K pmin log(|Π|/δ) ) by the setting of μt. Moreover, due to the nature of the coordinate descent algorithm, the weight vector Q remains sparse, so we can manipulate it efficiently and avoid running time that is linear in |Π|. As mentioned, this contrasts with the exponential-weights style algorithm of Kale et al. [2010] which maintains a dense weight vector over ∆|Π|.",
      "startOffset" : 43,
      "endOffset" : 1591
    }, {
      "referenceID" : 0,
      "context" : "This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplicatively shrinks all of the weights. Otherwise, if there is a policy that is found to violate the variance constraint in Equation 2, the algorithm adds weight to that policy, so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descent algorithm: Theorem 2. For any history H and parameter μ, Algorithm 2 halts and outputs a set of weights Q ∈ ∆|Π| that is feasible for (OP). Moreover, Algorithm 2 halts in no more than 8 log(1/(Kμ)) μpmin iterations and each iteration can be implemented efficiently, with at most one call to AMO. Since the main ideas used to prove this theorem are borrowed from the proof of Agarwal et al. [2014], we only provide a sketch in Section 2.4. Equipped with this theorem, it is easy to see that the total number of calls to the AMO over the course of the execution of Algorithm 1 can be bounded as Õ ( T 3/2 √ K pmin log(|Π|/δ) ) by the setting of μt. Moreover, due to the nature of the coordinate descent algorithm, the weight vector Q remains sparse, so we can manipulate it efficiently and avoid running time that is linear in |Π|. As mentioned, this contrasts with the exponential-weights style algorithm of Kale et al. [2010] which maintains a dense weight vector over ∆|Π|. We mention in passing that Agarwal et al. [2014] also develop two improvements that lead to a more efficient algorithm.",
      "startOffset" : 43,
      "endOffset" : 1689
    }, {
      "referenceID" : 0,
      "context" : "These are the analogs of Lemmas 6 and 7 in Agarwal et al. [2014]. The proof of the first is based on showing that the derivative of the function g(c) = Φ(cQ) is positive so that by convexity of Φ, shrinking the weights Q can only decrease the potential.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "The algorithm requires knowledge of the time horizon T , which can be relaxed by variants of the Epoch-Greedy [Langford and Zhang, 2008] or -greedy approaches, although the analysis here is significantly simpler.",
      "startOffset" : 110,
      "endOffset" : 136
    }, {
      "referenceID" : 15,
      "context" : "When the weights are known, can we obtain Õ( √ KLT ) regret even when the set of feasible actions are constrained, rather than Õ(L √ KT ) regret as in Theorem 1? The work on non-contextual combinatorial bandits suggests that the answer is yes [Kveton et al., 2015], but all algorithms for contextual bandit learning involve some degree of uniform exploration, which would prohibit such a regret bound.",
      "startOffset" : 243,
      "endOffset" : 264
    }, {
      "referenceID" : 0,
      "context" : "If this linear relationship is known, we showed that an adaptation of the algorithm of Agarwal et al. [2014] achieves regret between Õ( √ KLT lnN) and Õ(L √ KT lnN), and can be implemented efficiently with access to an optimization oracle.",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "The proof of this theorem is similar in spirit to a related theorem in Agarwal et al. [2014]. We first use Freedman’s inequality (Lemma 18) to argue that for a fixed P, π, μ, and t, the empirical version of the variance is close to the true variance.",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "To prove the variance deviation bound, we next use a discretization lemma from Dudı́k et al. [2011], which immediately implies that for any P , there exists a distribution P ′ supported on at most Nt policies such that for ct > 0, if Nt ≥ 6 γ2 t μtpmin :",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "Our definition of μt differs from Agarwal et al. [2014] only in the introduction of pmin, so by a straightforward adaptation we have: Lemma 10.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "The first three are fairly straightforward and the proof of the later two are based on the arguments of Agarwal et al. [2014]. For the first claim we have:",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 20,
      "context" : "Specifically, Lemma 20, due to Rudelson and Vershynin [2013], reveals that with probablity ≥ 1− δ: ‖U ξ‖2 ≤ √ L+ √ c ln(2/δ)",
      "startOffset" : 31,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "The first inequality, which is a Bernstein-type deviation bound for martingales, is Freedman’s inequality, which is from Beygelzimer et al. [2011] Lemma 18 (Freedman’s Inequality).",
      "startOffset" : 121,
      "endOffset" : 147
    }, {
      "referenceID" : 20,
      "context" : "We also make use of a vector-valued version of Hoeffding’s inequality, due to Rudelson and Vershynin [2013]. Lemma 20 (Vector-valued subgaussian concentration).",
      "startOffset" : 78,
      "endOffset" : 108
    }, {
      "referenceID" : 22,
      "context" : "Finally, we use a well known matrix-valued version of Hoeffding’s inequality, for example from Tropp [2011]. Lemma 21 (Matrix-Hoeffding).",
      "startOffset" : 95,
      "endOffset" : 108
    } ],
    "year" : 2017,
    "abstractText" : "We study a variant of the contextual bandit problem, where on each round, the learner plays a sequence of actions, receives a feature for each individual action, and reward that is linearly related to these features. This setting has applications to network routing, crowd-sourcing, personalized search, and many other domains. If the linear transformation is known, we analyze an algorithm that is structurally similar to the algorithm of Agarwal et al. [2014] and show that it enjoys a regret bound between Õ( √ KLT lnN) and Õ(L √ KT lnN), where K is the number of actions, L is the length of each action sequence, T is the number of rounds, and N is the number of policies. If the linear transformation is unknown, we show that an algorithm that first explores to learn the unknown weights via linear regression and thereafter uses the estimated weights can achieve Õ(‖w‖1(KT ) √ lnN) regret, where w is the true (unknown) weight vector. Both algorithms use an optimization oracle to avoid explicit enumeration of the policies and consequently are computationally efficient whenever an efficient algorithm for the fully supervised setting is available.",
    "creator" : "LaTeX with hyperref package"
  }
}