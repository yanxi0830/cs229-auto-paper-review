{
  "name" : "1606.00313.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improved Regret Bounds for Oracle-Based Adversarial Contextual Bandits",
    "authors" : [ "Vasilis Syrgkanis", "Haipeng Luo", "Akshay Krishnamurthy" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 6.\n00 31\n3v 1\n[ cs\n.L G\n] 1\na regret of order O((KT ) 2 3 (logN) 1 3 ), where K is the number of actions, T is the number of iterations and N is the number of baseline policies. Our result is the first to break the O(T 3\n4 ) barrier that is achieved by recently introduced algorithms. Breaking this barrier was left as a major open problem. Our analysis is based on the recent relaxation based approach of Rakhlin and Sridharan [7]."
    }, {
      "heading" : "1 Introduction",
      "text" : "We study online decision making problems where a learner chooses an action based on some side information (context) and incurs some cost for that action with a goal of incurring minimal cost over a sequence of rounds. These contextual online learning settings form a powerful framework for modeling many important decision-making scenarios with applications ranging from personalized health care in the medical domain to content recommendation and targeted advertising in internet applications. Many of these applications also involve a partial feedback component wherein costs for alternative actions are unobserved, and are typically modeled as contextual bandits.\nThe contextual information present in these problems enables learning of a much richer policy for choosing actions based on context. In the literature, the typical goal for the learner is to have cumulative cost that is not much higher then the best policy π in a large set Π. This is formalized by the notion of regret, which is the learner’s cumulative cost minus the cumulative cost of the best fixed policy π in hindsight. Achieving this goal requires learning a rich policy, depending on Π.\nNaively one can view the contextual problem as a standard online learning problem where the set of possible “actions” available at each iteration is the set of policies. This perspective is fruitful, as classical algorithms, such as Hedge [5, 3] and Exp4 [2], give information theoretically optimal regret bounds of O( √ T log(|Π|)) in full-information and O( √\nTK log(|Π|) in the bandit setting, where T is the number of rounds, K is the number of actions, and Π is the policy set. However, naively lifting standard online learning algorithms to the contextual setting leads to a running time that is linear in the number of policies. Given that the optimal regret is only logarithmic in |Π| and that our high-level goal is to learn a very rich policy, we want to capture policy classes that are exponentially large. When we use a large policy class, existing algorithms are no longer computationally tractable.\nTo study this computational question, a number of recent papers have developed oracle-based algorithms that only access the policy class through an optimization oracle for the offline full-information problem. Oracle-based approaches harness the research in supervised learning that focuses on designing efficient algorithms for full-information problems and uses it for online and partial-feedback problems. Optimization oracles have been used in designing contextual bandit algorithms [1, 6, 4] that achieve the optimal\nO( √ KT log(|Π|)) regret while also being computationally efficient (i.e. requiring poly(K, log(Π), T ) oracle calls and computation). However, these results only apply when the contexts and costs are drawn at random and identically and independently at each iteration, contrasting with the computationally inefficient approaches that can handle adversarial inputs.\nTwo very recent works provide the first oracle efficient algorithms for the contextual bandit problem in adversarial settings [7, 8]. Rakhlin and Sridharan [7] considers a setting where the contexts are drawn i.i.d. from a known distribution with adversarial costs and they provide an oracle efficient algorithm with O(T 3 4K 1\n2 (log(|Π|)) 14 ) regret. Their algorithm also applies in the transductive setting where the sequence of contexts is known a priori. Srygkanis et. al [8] also obtain a T 3\n4 -style bound with a different oracle-efficient algorithm, but in a setting where the learner knows only the set of contexts that will arrive. Both of these results achieve very suboptimal regret bounds, as the dependence on the number of iterations is far from the optimal O( √ T )-bound. A major open question posed by both works is whether the O(T 3\n4 ) barrier can be broken.\nIn this paper, we provide an oracle-based contextual bandit algorithm that achieves regretO((KT ) 2 3 (log(|Π|)) 13 ) in both the i.i.d. context and the transductive settings considered by Rakhlin and Sridharan [7]. This bound matches that of the epoch-greedy algorithm of Langford and Zhang [6] that only applies to the fully stochastic setting. As in Rakhlin and Sridharan [7], our algorithm only requires access to a value oracle, which is weaker than the standard argmax oracle, and it makes K + 1 oracle calls per iteration. To our knowledge, this is the best regret bound achievable by an oracle-efficient algorithm for any adversarial contextual bandit problem.\nOur algorithm and regret bound are based on a novel and intricate analysis of the minimax problem that arises in the relaxation-based framework of Rakhlin and Sridharan [7]. Our proof requires analyzing the value of a sequential game where the learner chooses a distribution over actions and then the adversary chooses a distribution over costs in some bounded finite domain, with - importantly - a bounded variance. This is unlike the simpler minimax problem analyzed in [7], where the adversary is only constrained by the range of the costs. We provide this tighter minimax analysis in Section 3.\nApart from showing that this more structured minimax problem has a small value, we also need to derive an oracle-based strategy for the learner that achieves the improved regret bound. The additional constraints on the game require a much more intricate argument to derive this strategy which is an algorithm for solving a structured two-player minimax game. We present this part in Section 4."
    }, {
      "heading" : "2 Model and Preliminaries",
      "text" : "Basic notation. Throughout the paper we denote with x1:t a sequence of quantities {x1, . . . , xt} and with (x, y, z)1:t a sequence of tuples {(x1, y1, z1), . . .}. ∅ denotes an empty sequence. The vector of ones is denoted by 1 and the vector of zeroes is denoted by 0. Denote with [K] the set {1, . . . ,K} and ∆U the set of distributions over a set U . We also use ∆K as a shorthand for ∆[K].\nContextual online learning. We consider the following version of the contextual online learning problem. On each round t = 1, . . . , T , the learner observes a context xt and then chooses a probability distribution qt over a set of K actions. The adversary then chooses a cost vector ct ∈ [0, 1]K . The learner picks an action ŷt drawn from distribution qt, suffers a loss ct(ŷt) and observes only ct(ŷt) and not the loss of the other actions.\nThroughout the paper we will assume that the context xt at each iteration t is drawn i.i.d. from a distribution D. This is referred to as the hybrid i.i.d.-adversarial setting [7]. As in prior work [7], we assume that the learner can sample contexts from this distribution as needed. It is easy to adapt the arguments in the paper to apply for the transductive setting where the learner knows the sequence of contexts that will arrive. The cost vectors ct are chosen by a non-adaptive adversary.\nThe goal of the learner is to compete with a set of policies Π, where each policy π ∈ Π is a function mapping from the set of contexts to the set of actions. The cumulative expected regret with respect to the best fixed policy in hindsight is\nReg =\nT ∑\nt=1\nqt · ct − inf π∈Π\nT ∑\nt=1\nct(π(xt)) .\nOptimization value oracle. We will assume that we are given access to an optimization oracle that when given as input a sequence of contexts and loss vectors (x, c)1:t, it outputs the value of the cumulative loss of the best fixed policy: i.e.\ninf π∈Π\nt ∑\nτ=1\nct(π(xt)) . (1)\nThis can be viewed as an offline batch optimization or ERM oracle."
    }, {
      "heading" : "2.1 Relaxation based algorithms",
      "text" : "We briefly review the relaxation based framework proposed in [7]. The reader is directed to [7] for a more extensive exposition. We will also slightly augment the framework by adding some internal random state that the algorithm might keep and use subsequently and which does not affect the cost of the algorithm.\nA crucial concept in the relaxation based framework is the information obtained by the learner at the end of each round t ∈ [T ], which is the following tuple:\nIt(xt, qt, ŷt, ct, St) = (xt, qt, ŷt, ct(ŷt), St)\nwhere ŷt is the realized chosen action drawn from the distribution qt and St is some random string drawn from some distribution that can depend on qt, ŷt and ct(ŷt) and which can be used by the algorithm in subsequent rounds.\nDefinition 1 A partial-information relaxation Rel(·) is a function that maps (I1, . . . , It) to a real value for any t ∈ [T ]. A partial-information relaxation is admissible if for any t ∈ [T ], and for all I1, . . . , It−1:\nExt\n[\ninf qt sup ct Eŷt∼qt,St [ct(ŷt) + Rel(I1:t−1, It(xt, qt, ŷt, ct, St))]\n]\n≤ Rel(I1:t−1) (2)\nand for all x1:T , c1:T and q1:T :\nEŷ1:T∼q1:T ,S1:T [Rel(I1:T )] ≥ − inf π∈Π\nT ∑\nt=1\nct(π(xt)) . (3)\nDefinition 2 Any randomized strategy q1:T that certifies inequalities (2) and (3) is called an admissible strategy.\nA basic lemma proven in [7] is that if one constructs a relaxation and a corresponding admissible strategy, then the expected regret of the admissible strategy is upper bounded by the value of the relaxation at the beginning of time.\nLemma 1 ([7]) Let Rel be an admissible relaxation and q1:T be an admissible strategy. Then for any c1:T , we have\nE [Reg] ≤ Rel(∅) .\nWe will utilize this framework and construct a novel relaxation with an admissible strategy. We will show that the value of the relaxation at the beginning of time is upper bounded by the desired improved regret bound and that the admissible strategy can be efficiently computed assuming access to an optimization value oracle."
    }, {
      "heading" : "3 A Faster Contextual Bandit Algorithm",
      "text" : "First we define an unbiased estimator for each loss vector ct. In addition to doing the usual importance weighting, we also discretize the estimated loss to either 0 or L for some constant L ≥ K to be specified later.\nSpecifically, consider a random variable Xt which we construct at the end of each iteration conditioning on ŷt:\nXt =\n{\n1 with probability ct(ŷt)Lqt(ŷt) , 0 with the remainig probability . (4)\nThis is a valid random variable whenever mini qt(i) ≥ 1L , which will be ensured by the algorithm. This is the only random variable in the random string St that we used in the general formulation of the relaxation framework.\nNow the construction of an unbiased estimate for each ct based on the information It collected at the end of each round is: ĉt = LXteŷt . Observe that for any i ∈ [K]:\nEŷt∼qt,Xt [ĉt(i)] = L · Pr[ŷt = i] · Pr[Xt = 1|ŷt = i] = L · qt(i) · ct(i)\nLqt(i) = ct(i) .\nHence, ĉt is an unbiased estimate of ct. We are now ready to define our relaxation. Let ǫt ∈ {−1, 1}K be a Rademacher random vector (i.e. each coordinate is an independent Rademacher random variable, which is −1 or 1 with equal probability), and let Zt ∈ {0, L} be a random variable which is L with probability K/L and 0 otherwise. With the notation ρt = (x, ǫ, Z)t+1:T , our relaxation is defined as follows:\nRel(I1:t) = Eρt [R((x, ĉ)1:t, ρt)] , (5)\nwhere\nR((x, ĉ)1:t, ρt) = − inf π∈Π\n(\nt ∑\nτ=1\nĉτ (π(xτ )) +\nT ∑\nτ=t+1\n2ǫτ(π(xτ ))Zt\n)\n+ (T − t)K/L .\nNote that Rel(∅) is the following quantity, whose first part resembles a Rademacher average:\nRΠ = 2E(x,ǫ,Z)1:T\n[\ninf π∈Π\nT ∑\nτ=t+1\nǫτ (π(xτ ))Zt\n]\n+ TK/L .\nUsing the following Lemma (whose proof is deferred to the supplementary material) and the fact E [ Z2t ] ≤ KL, we can upper bound RΠ by O( √\nTKL log(N) + TK/L), which after tuning L will give the claimed O(T 2/3) bound.\nLemma 2 Let ǫt be Rademacher random vectors, and Zt be non-negative real-valued random variables, such that E [\nZ2t ] ≤ M . Then:\nEZ1:T ,ǫ1:T\n[\nsup π∈Π\nT ∑\nt=1\nǫt(π(xt)) · Zt ] ≤ √ 2TM log(N)\nTo show an admissible strategy for our relaxation, we next introduce some more notation. Let\nD = {L · ei : i ∈ [K]} ∪ {0},\nwhere (e1, . . . , eK) are the orthonormal basis vectors (i.e. ei is 1 at coordinate i and zero otherwise), and 0 is the all zeros vector. We will denote with ∆D the set of distributions over D. For a distribution p ∈ ∆(D), we will denote with p(i), for i ∈ {0, . . . ,K}, the probability assigned to vector ei, with the convention that e0 = 0. Also let ∆ ′ D = {p ∈ ∆D : p(i) ≤ 1/L, ∀i ∈ [K]}.\nBased on this notation our admissible strategy is defined as\nqt = Eρt [qt(ρt)] where qt(ρt) =\n(\n1− K L\n)\nq∗t (ρt) + 1\nL 1 (6)\nAlgorithm 1 A New Contextual Bandit Algorithm\nInput: parameter L ≥ K for each time step t ∈ [T ] do Observe xt. Draw ρt = (x, ǫ, Z)t+1:T where each xτ is drawn from the distribution of contexts, ǫτ is a Rademacher random vectors and Zτ ∈ {0, L} is L with probability K/L and 0 otherwise. Compute qt(ρt) based on Eq. (6) (using Algorithm 2). Predict ŷt ∼ qt(ρt) and observe ct(ŷt). Create an estimate ĉt = LXteŷt , where Xt is defined in Eq. (4) with qt in that equation instantiated with qt(ρt). end for\nand q∗t (ρt) = argmin\nq∈∆K sup pt∈∆′D\nEĉt∼pt [〈q, ĉt〉+R((x, ĉ)1:t, ρt)] . (7)\nAlgorithm 1 implements this admissible strategy. Note that it suffices to use qt(ρt) instead of qt for a random draw ρt in the algorithm to ensure the exact same guarantee in expectation. Moreover, in Section 4 we will show that qt(ρt) can be computed efficiently using an optimization value oracle.\nWe now prove that our relaxation and strategy are indeed admissible.\nTheorem 3 The relaxation defined in Equation (5) is admissible. An admissible randomized strategy for this relaxation is given by (6). The expected regret of the Algorithm 1 is upper bounded by\n2 √ 2TKL log(N) + TK/L, (8)\nfor any L ≥ K. Specifically, setting L = (KT/ log(N)) 13 when T ≥ K2 log(N), the regret is of order O((KT ) 2 3 (log(N)) 1 3 ).\nProof: We verify the two conditions for admissibility.\nFinal condition. It is clear that inequality (3) is satisfied since ĉt are unbiased estimates of ct:\nEŷ1:T ,X1:T [Rel(I1:T )] = Eŷ1:T ,X1:T\n[\nsup π∈Π\n− T ∑\nτ=1\nĉτ (π(xτ ))\n]\n≥ sup π∈Π −Eŷ1:T ,X1:T\n[\nT ∑\nτ=1\nĉτ (π(xτ ))\n]\n= sup π∈Π\n− T ∑\nτ=1\ncτ (π(xτ ))\nt-th Step condition. We now check that inequality (2) is also satisfied at some time step t ∈ [T ]. We reason conditionally on the observed context xt and show that qt defines an admissible strategy for the relaxation. Let q∗t = Eρt [q ∗ t (ρt)]. First observe that:\nEŷt,Xt [ct(ŷt)] = Eŷt∼qt [ct(ŷt)] = 〈qt, ct〉 ≤ 〈q∗t , ct〉+ 1\nL 〈1, ct〉 ≤ Eŷt,Xt [〈q∗t , ĉt〉] +\nK\nL\nWe remind that ĉt = LXteŷt is the unbiased estimate, which is a deterministic function of ŷt and Xt. Hence:\nsup ct∈[0,1]K Eŷt,Xt [ct(ŷt) + Rel(I1:t)] ≤ sup ct∈[0,1]K\nEŷt,Xt [〈q∗t , ĉt〉+ Rel(I1:t)] + K\nL\nWe now work with the first term of the right hand side:\nsup ct∈[0,1]K Eŷt,Xt [〈q∗t , ĉt〉+ Rel(I1:t)] = sup ct∈[0,1]K Eŷt,Xt [〈q∗t , ĉt〉+ Eρt [R((x, ĉ)1:t, ρt]]\n= sup ct∈[0,1]K\nEŷt,Xt [Eρt [〈q∗t (ρt), ĉt〉+R((x, ĉ)1:t, ρt)]]\nObserve that ĉt is a random variable taking values in D and such that the probability that it is equal to Lei can be upper bounded as:\nPr[ĉt = Lei] = Eρt [Pr[ĉt = Lei|ρt]] = Eρt [ qt(ρt)(i) ct(i)\nL · qt(ρt)(i)\n]\n≤ 1/L.\nThus we can upper bound the latter quantity by the supremum over all distributions in ∆′D, i.e.:\nsup ct∈[0,1]K Eŷt,Xt [〈q∗t , ĉt〉+ Rel(I1:t)] ≤ sup pt∈∆′D Eĉt∼pt [Eρt [〈q∗t (ρ), ĉt〉+R((x, ĉ)1:t, ρt)]]\nNow we can continue by pushing the expectation over ρt outside of the supremum, i.e.\nsup ct∈[0,1]K\nEŷt∼qt,Xt [〈q∗t , ĉt〉+ Rel(I1:t)] ≤ Eρt\n[\nsup pt∈∆′D\nEĉt∼pt [〈q∗t (ρt), ĉt〉+R((x, ĉ)1:t, ρt)] ]\nand working conditionally on ρt. Observe that by the definition of q ∗ t (ρt) the quantity inside the expectation is equal to: inf\nq∈∆K sup\npt∈∆′D\nEĉt∼pt [〈q, ĉt〉+R((x, ĉ)1:t, ρt)]\nWe can now apply the minimax theorem and upper bound the above by:\nsup pt∈∆′D inf q∈∆K\nEĉt∼pt [〈q, ĉt〉+R((x, ĉ)1:t, ρt)]\nSince the inner objective is linear in q, we continue with\nsup pt∈∆′D min i Eĉt∼pt [ĉt(i) +R((x, ĉ)1:t, ρt)]\nWe can now expand the definition of R(·):\nsup pt∈∆′D min i Eĉt∼pt\n[\nĉt(i) + sup π∈Π\n− ( t ∑\nτ=1\nĉτ (π(xτ )) +\nT ∑\nτ=t+1\n2ǫτ (π(xτ ))Zt\n)]\n+ (T − t)K/L\nWith the notation\nAπ = − t−1 ∑\nτ=1\nĉτ (π(xτ ))− T ∑\nτ=t+1\n2ǫτ(π(xτ ))Zt\nwe re-write the above quantity as:\nsup pt∈∆′D min i Eĉt∼pt\n[\nĉt(i) + sup π∈Π\n(Aπ − ĉt(π(xt))) ] + (T − t)K/L\nWe now upper bound the first term. The extra term (T − t)K/L will be combined with the extra K/L that we have abandoned to give the correct term (T − (t− 1))K/L needed for Rel(I1:t−1).\nObserve that we can re-write the first term by using symmetrization as:\nsup pt∈∆′D min i Eĉt∼pt\n[\nĉt(i) + sup π∈Π\n(Aπ − ĉt(π(xt))) ]\n= sup pt∈∆′D Eĉt∼pt\n[\nsup π∈Π (Aπ +min i\nEĉ′t∼pt [ĉ ′ t(i)]− ĉt(π(xt)))\n]\n≤ sup pt∈∆′D Eĉt∼pt\n[\nsup π∈Π\n(Aπ + Eĉ′t∼pt [ĉ ′ t(π(xt))]− ĉt(π(xt)))\n]\n≤ sup pt∈∆′D Eĉt,ĉ′t∼pt\n[\nsup π∈Π\n(Aπ + ĉ ′ t(π(xt))− ĉt(π(xt)))\n]\n= sup pt∈∆′D Eĉt,ĉ′t∼pt,δ\n[\nsup π∈Π\n(Aπ + δ (ĉ ′ t(π(xt))− ĉt(π(xt))))\n]\n≤ sup pt∈∆′D Eĉt∼pt,δ\n[\nsup π∈Π (Aπ + 2δĉt(π(xt)))\n]\nwhere δ is a random variable which is −1 and 1 with equal probability. The last inequality follows by splitting the supremum into two equal parts.\nConditioning on ĉt, consider the random variableMt which is −maxi ĉt(i) ormaxi ĉt(i) on the coordinates where ĉt is equal to zero and equal to ĉt on the coordinate that achieves the maximum. This is clearly an unbiased estimate of ĉt. Thus we can upper bound the last quantity by:\nsup pt∈∆′D Eĉt∼pt,δ\n[\nsup π∈Π\n(Aπ + 2δE [Mt(π(xt))|ĉt]) ]\n≤ sup pt∈∆′D Eĉt∼pt,δ,Mt\n[\nsup π∈Π (Aπ + 2δMt(π(xt)))\n]\nThe random vector δMt, conditioning on ĉt, is equal to −maxi ĉt(i) or maxi ĉt(i) with equal probability independently on each coordinate. Moreover, observe that for any distribution pt ∈ ∆′D, the distribution of the maximum coordinate of ĉt has support on {0, L} and is equal to L with probability at most K/L. Since the objective only depends on the distribution of the maximum coordinate of ĉt, we can continue the upper bound with a supremum over any distribution of random vectors whose coordinates are 0 with probability at least 1 − K/L and otherwise are −L or L with equal probability. Specifically, let ǫt be a Rademacher random vector, we continue with:\nsup Zt∈∆{0,L}:Pr[Zt=L]≤K/L Eǫt,Zt\n[\nsup π∈Π (Aπ + 2ǫt(π(xt))Zt)\n]\nNow observe that if we denote with a = Pr[Zt = L], the above is equal to:\nsup a:0≤a≤K/L\n(\n(1− a) sup π∈Π (Aπ) + aEǫt\n[\nsup π∈Π (Aπ + 2ǫt(π(xt))L)\n])\nWe now argue that this supremum is achieved by setting a = K/L. For that it suffices to show that:\nsup π∈Π\n(Aπ) ≤ Eǫt [\nsup π∈Π (Aπ + 2ǫt(π(xt))L)\n]\nwhich is true by observing that with π∗ = argsupπ∈Π(Aπ) one has:\nEǫt\n[\nsup π∈Π (Aπ + 2ǫt(π(xt))L)\n]\n≥ Eǫt [Aπ∗ + 2ǫt(π∗(xt))L)] = Aπ∗ + Eǫt [2ǫt(π∗(xt))L)] = Aπ∗ .\nThus we can upper bound the quantity we want by:\nEǫt,Zt\n[\nsup π∈Π (Aπ + 2ǫt(π(xt))Zt\n]\nAlgorithm 2 Computing q∗t (ρt)\nInput: a value optimization oracle, (x, ĉ)1:t−1, xt and ρt. Output: q ∈ ∆K as a solution of Eq. (7).\nCompute ψi as in Eq. (9) for all i = 0, . . . ,K using the optimization oracle. Compute φi = ψi−ψ0 L for all i ∈ [K]. Let m = 1 and q = 0. for each coordinate i ∈ [K] do Set q(i) = min{(φi)+,m}. Update m ← m− q(i). end for Distribute m arbitrarily on the coordinates of q if m > 0.\nwhere ǫt is a Rademacher random vector and Zt is now a random variable which is equal to L with probability K/L and is equal to 0 with the remaining probability.\nTaking expectation over ρt and xt and adding the (T − (t− 1))K/L term that we abandoned, we arrive at the desired upper bound of Rel(I1:t−1). This concludes the proof of admissibility.\nRegret bound. By applying Lemma 2 with E[Z2t ] = L 2Pr[Zt = L] = KL and invoking Lemma 1, we get the regret bound in Equation (8)."
    }, {
      "heading" : "4 Computational Efficiency",
      "text" : "In this section we will argue that if one is given access to a value optimization oracle (1), then one can run Algorithm 1 efficiently. Specifically, we will show that the minimizer of Equation (7) can be computed efficiently (see Algorithm 2).\nLemma 4 Computing the quantity defined in equation (7) for any given ρt can be done in time O(K) and with only K + 1 accesses to a value optimization oracle.\nProof: For i ∈ {0, . . . ,K}, let:\nψi = inf π∈Π\n(\nt−1 ∑\nτ=1\nĉτ (π(xτ )) + Lei(π(xt)) +\nT ∑\nτ=t+1\n2ǫτ(π(xτ ))Zt\n)\n(9)\nwith the convention e0 = 0. Then observe that we can re-write the definition of q ∗ t (ρt) as:\nq∗t (ρt) = arginf q∈∆K sup pt∈∆′D\nK ∑\ni=1\npt(i)(L · q(i)− ψi)− pt(0) · ψ0\nObserve that each ψi can be computed with a single oracle access. Thus we can assume that all K + 1 ψ’s are computed efficiently and are given. We now argue how to compute the minimizer.\nFor each given q, the supremum over pt can be characterized as follows. With the notation zi = L·q(i)−ψi and z0 = −ψ0 we re-write the minimax quantity as:\nq∗t (ρt) = arginf q∈∆K sup pt∈∆′D\nK ∑\ni=1\npt(i) · zi + pt(0) · z0\nObserve that if we didn’t have the constraint that pt(i) ≤ 1/L for i > 0, then we would have put all the probability mass on the maximum of the zi. However, now that we are constrained we will simply put as much probability mass as allowed on the maximum coordinate argmaxi∈{0,...,K} zi and continue to the next highest quantity. We repeat this until reaching the quantity z0. At that point, the probability mass that\nwe can put on coordinate 0 is unconstrained. Thus we can put all the remaining probability mass on this coordinate.\nLet z(1), z(2), . . . , z(K) denote the ordered zi quantities for i > 0 (from largest to smallest). Moreover, let µ ∈ [K] be the largest index such that z(µ) ≥ z0. By the above reasoning we get that for a given q, the supremum over pt is equal to:\nµ ∑\nt=1\nz(t) L + ( 1− µ L ) z0 =\nµ ∑\nt=1\nz(t) − z0 L + z0 .\nNow since for any t > µ, z(t) < z0, we can write the latter as:\nµ ∑\nt=1\nz(t) L + ( 1− µ L )\nz0 = K ∑\ni=1\n(zi − z0)+ L + z0\nwith the convention (x)+ = max{x, 0}. We thus further re-write the minimax expression as:\nq∗t (ρt) = arginf q∈∆K\nK ∑\ni=1\n(zi − z0)+ L + z0 = arginf q∈∆K\nK ∑\ni=1\n(zi − z0)+ L = arginf q∈∆K\nK ∑\ni=1\n( q(i)− ψi − ψ0 L\n)+\nLet φi = ψi−ψ0 L . The expression becomes: q ∗ t (ρt) = arginfq∈∆K ∑K t=1(q(i)− φi)+.\nThe latter is minimized as follows: consider any i ∈ [K] such that φi ≤ 0. Then putting any positive mass on such a coordinate i is going to lead to a marginal increase of 1. On the other hand if we put some mass on an index φi > 0, then that will not increase the objective until we reach the point where q(i) = φi. Thus a minimizer will distribute probability mass of min{∑i:φi>0 φi, 1}, on the coordinates for which φi > 0. The remainder mass (if any) can be distributed arbitrarily. See Algorithm 2 for details."
    }, {
      "heading" : "5 Discussion",
      "text" : "In this paper, we present a new oracle-efficient algorithm for adversarial contextual bandits and we prove that it achieves O((KT )2/3 log(|Π|)1/3) regret in the settings studied by Rakhlin and Sridharan [7]. This is the best regret bound that we are aware of among oracle-based algorithms.\nWhile our bound improves on the O(T 3/4) bounds in prior work [7, 8], achieving the optimalO( √ TK log(|Π|)) regret bound with an oracle based approach still remains an important open question. Another interesting avenue for future work involves understanding the role of transductivity assumptions and developing an algorithm that can handle the non-transductive fully adversarial setting. We look forward to pursuing these directions."
    }, {
      "heading" : "A Supplementary Lemma",
      "text" : "Lemma 2. Let ǫt be Rademacher random vectors, and Zt be non-negative real-valued random variables, such that E [\nZ2t ] ≤ M . Then:\nEZ1:T ,ǫ1:T\n[\nsup π∈Π\nT ∑\nt=1\nǫt(π(xt)) · Zt ] ≤ √ 2TM log(N)\nProof:\nEZ1:T ,ǫ1:T\n[\nsup π∈Π\nT ∑\nt=1\nǫt(π(xt)) · Zt ] = EZ1:T [ 1\nλ Eǫ1:T\n[\nlog\n(\nsup π∈Π\neλ ∑ T t=1 ǫt(π(xt))·Zt\n)]]\n≤ EZ1:T [ 1\nλ log\n(\nEǫ1:T\n[\nsup π∈Π\neλ ∑ T t=1 ǫt(π(xt))·Zt\n])]\n≤ EZ1:T\n[\n1 λ log\n(\nEǫ1:T\n[\n∑\nπ∈Π\neλ ∑ T t=1 ǫt(π(xt))·Zt\n])]\n= EZ1:T\n[\n1 λ log\n(\n∑\nπ∈Π\nEǫ1:T\n[\nT ∏\nt=1\neλǫt(π(xt))·Zt\n])]\n= EZ1:T\n[\n1 λ log\n(\n∑\nπ∈Π\nT ∏\nt=1\nEǫt\n[ eλǫt(π(xt))·Zt ]\n)]\nNow observe that Eǫt [ eλǫt(π(xt))·Zt ] = e λ·Zt+e−λ·Zt 2 ≤ eλ 2·Z2t /2. Thus:\nEZ1:T ,ǫ1:T\n[\nsup π∈Π\nT ∑\nt=1\nǫt(π(xt)) · Zt ] ≤ EZ1:T [ 1\nλ log\n(\n∑\nπ∈Π\nT ∏\nt=1\neλ 2·Z2t /2\n)]\n= EZ1:T\n[\n1 λ log ( Neλ 2 ∑ T t=1 Z2t /2 )\n]\n= 1\nλ log(N) + λEZ1:T\n[\nT ∑\nt=1\nZ2t /2\n]\n≤ 1 λ log(N) + λMT/2\nOptimizing over λ yields the result."
    } ],
    "references" : [ {
      "title" : "Taming the monster: A fast and simple algorithm for contextual bandits",
      "author" : [ "Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Gambling in a rigged casino: The adversarial multi-armed bandit pproblem",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1995
    }, {
      "title" : "How to use expert advice",
      "author" : [ "Nicolo Cesa-Bianchi", "Yoav Freund", "David Haussler", "David P Helmbold", "Robert E Schapire", "Manfred K Warmuth" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1997
    }, {
      "title" : "Efficient optimal learning for contextual bandits",
      "author" : [ "Miroslav Dudík", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang" ],
      "venue" : "In Uncertainty and Artificial Intelligence (UAI),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Yoav Freund", "Robert E Schapire" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1997
    }, {
      "title" : "The epoch-greedy algorithm for multi-armed bandits with side information",
      "author" : [ "John Langford", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "BISTRO: an efficient relaxation-based method for contextual bandits",
      "author" : [ "Alexander Rakhlin", "Karthik Sridharan" ],
      "venue" : "In Proceedings of the Twentieth International Conference on Machine Learning (ICML 2016),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Efficient algorithms for adversarial contextual learning",
      "author" : [ "Vasilis Syrgkanis", "Akshay Krishnamurthy", "Robert E. Schapire" ],
      "venue" : "In Proceedings of the Twentieth International Conference on Machine Learning (ICML 2016),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Our analysis is based on the recent relaxation based approach of Rakhlin and Sridharan [7].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "This perspective is fruitful, as classical algorithms, such as Hedge [5, 3] and Exp4 [2], give information theoretically optimal regret bounds of O( √ T log(|Π|)) in full-information and O( √ TK log(|Π|) in the bandit setting, where T is the number of rounds, K is the number of actions, and Π is the policy set.",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "This perspective is fruitful, as classical algorithms, such as Hedge [5, 3] and Exp4 [2], give information theoretically optimal regret bounds of O( √ T log(|Π|)) in full-information and O( √ TK log(|Π|) in the bandit setting, where T is the number of rounds, K is the number of actions, and Π is the policy set.",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "This perspective is fruitful, as classical algorithms, such as Hedge [5, 3] and Exp4 [2], give information theoretically optimal regret bounds of O( √ T log(|Π|)) in full-information and O( √ TK log(|Π|) in the bandit setting, where T is the number of rounds, K is the number of actions, and Π is the policy set.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "Optimization oracles have been used in designing contextual bandit algorithms [1, 6, 4] that achieve the optimal 1",
      "startOffset" : 78,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : "Optimization oracles have been used in designing contextual bandit algorithms [1, 6, 4] that achieve the optimal 1",
      "startOffset" : 78,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "Optimization oracles have been used in designing contextual bandit algorithms [1, 6, 4] that achieve the optimal 1",
      "startOffset" : 78,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "Two very recent works provide the first oracle efficient algorithms for the contextual bandit problem in adversarial settings [7, 8].",
      "startOffset" : 126,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "Two very recent works provide the first oracle efficient algorithms for the contextual bandit problem in adversarial settings [7, 8].",
      "startOffset" : 126,
      "endOffset" : 132
    }, {
      "referenceID" : 6,
      "context" : "Rakhlin and Sridharan [7] considers a setting where the contexts are drawn i.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 7,
      "context" : "al [8] also obtain a T 3 4 -style bound with a different oracle-efficient algorithm, but in a setting where the learner knows only the set of contexts that will arrive.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "context and the transductive settings considered by Rakhlin and Sridharan [7].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "This bound matches that of the epoch-greedy algorithm of Langford and Zhang [6] that only applies to the fully stochastic setting.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "As in Rakhlin and Sridharan [7], our algorithm only requires access to a value oracle, which is weaker than the standard argmax oracle, and it makes K + 1 oracle calls per iteration.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 6,
      "context" : "Our algorithm and regret bound are based on a novel and intricate analysis of the minimax problem that arises in the relaxation-based framework of Rakhlin and Sridharan [7].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 6,
      "context" : "This is unlike the simpler minimax problem analyzed in [7], where the adversary is only constrained by the range of the costs.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "The adversary then chooses a cost vector ct ∈ [0, 1] .",
      "startOffset" : 46,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : "-adversarial setting [7].",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "As in prior work [7], we assume that the learner can sample contexts from this distribution as needed.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "1 Relaxation based algorithms We briefly review the relaxation based framework proposed in [7].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "The reader is directed to [7] for a more extensive exposition.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "A basic lemma proven in [7] is that if one constructs a relaxation and a corresponding admissible strategy, then the expected regret of the admissible strategy is upper bounded by the value of the relaxation at the beginning of time.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 6,
      "context" : "Lemma 1 ([7]) Let Rel be an admissible relaxation and q1:T be an admissible strategy.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "Hence: sup ct∈[0,1] Eŷt,Xt [ct(ŷt) + Rel(I1:t)] ≤ sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Rel(I1:t)] + K L We now work with the first term of the right hand side: sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Rel(I1:t)] = sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Eρt [R((x, ĉ)1:t, ρt]] = sup ct∈[0,1] Eŷt,Xt [Eρt [〈q t (ρt), ĉt〉+R((x, ĉ)1:t, ρt)]]",
      "startOffset" : 14,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "Hence: sup ct∈[0,1] Eŷt,Xt [ct(ŷt) + Rel(I1:t)] ≤ sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Rel(I1:t)] + K L We now work with the first term of the right hand side: sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Rel(I1:t)] = sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Eρt [R((x, ĉ)1:t, ρt]] = sup ct∈[0,1] Eŷt,Xt [Eρt [〈q t (ρt), ĉt〉+R((x, ĉ)1:t, ρt)]]",
      "startOffset" : 57,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "Hence: sup ct∈[0,1] Eŷt,Xt [ct(ŷt) + Rel(I1:t)] ≤ sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Rel(I1:t)] + K L We now work with the first term of the right hand side: sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Rel(I1:t)] = sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Eρt [R((x, ĉ)1:t, ρt]] = sup ct∈[0,1] Eŷt,Xt [Eρt [〈q t (ρt), ĉt〉+R((x, ĉ)1:t, ρt)]]",
      "startOffset" : 163,
      "endOffset" : 168
    }, {
      "referenceID" : 0,
      "context" : "Hence: sup ct∈[0,1] Eŷt,Xt [ct(ŷt) + Rel(I1:t)] ≤ sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Rel(I1:t)] + K L We now work with the first term of the right hand side: sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Rel(I1:t)] = sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Eρt [R((x, ĉ)1:t, ρt]] = sup ct∈[0,1] Eŷt,Xt [Eρt [〈q t (ρt), ĉt〉+R((x, ĉ)1:t, ρt)]]",
      "startOffset" : 209,
      "endOffset" : 214
    }, {
      "referenceID" : 0,
      "context" : "Hence: sup ct∈[0,1] Eŷt,Xt [ct(ŷt) + Rel(I1:t)] ≤ sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Rel(I1:t)] + K L We now work with the first term of the right hand side: sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Rel(I1:t)] = sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Eρt [R((x, ĉ)1:t, ρt]] = sup ct∈[0,1] Eŷt,Xt [Eρt [〈q t (ρt), ĉt〉+R((x, ĉ)1:t, ρt)]]",
      "startOffset" : 267,
      "endOffset" : 272
    }, {
      "referenceID" : 0,
      "context" : ": sup ct∈[0,1] Eŷt,Xt [〈q t , ĉt〉+ Rel(I1:t)] ≤ sup pt∈∆D Eĉt∼pt [Eρt [〈q t (ρ), ĉt〉+R((x, ĉ)1:t, ρt)]] Now we can continue by pushing the expectation over ρt outside of the supremum, i.",
      "startOffset" : 9,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "sup ct∈[0,1] Eŷt∼qt,Xt [〈q t , ĉt〉+ Rel(I1:t)] ≤ Eρt [",
      "startOffset" : 7,
      "endOffset" : 12
    } ],
    "year" : 2016,
    "abstractText" : "We give an oracle-based algorithm for the adversarial contextual bandit problem, where either contexts are drawn i.i.d. or the sequence of contexts is known a priori, but where the losses are picked adversarially. Our algorithm is computationally efficient, assuming access to an offline optimization oracle, and enjoys a regret of order O((KT ) 2 3 (logN) 1 3 ), where K is the number of actions, T is the number of iterations and N is the number of baseline policies. Our result is the first to break the O(T 3 4 ) barrier that is achieved by recently introduced algorithms. Breaking this barrier was left as a major open problem. Our analysis is based on the recent relaxation based approach of Rakhlin and Sridharan [7].",
    "creator" : "LaTeX with hyperref package"
  }
}