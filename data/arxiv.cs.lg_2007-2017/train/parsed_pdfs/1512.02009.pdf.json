{
  "name" : "1512.02009.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Jointly Modeling Topics and Intents with Global Order Structure",
    "authors" : [ "Bei Chen", "Jun Zhu", "Nan Yang", "Tian Tian", "Ming Zhou", "Bo Zhang" ],
    "emails" : [ "{chenbei12@mails.,", "dcszj@,", "tiant13@mails.,", "dcszb@}tsinghua.edu.cn;", "mingzhou}@microsoft.com" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "People often organize utterances into meaningful and coherent documents, conforming to certain conventions and underlying structures. For example, scripts (Frermann, Titov, and Pinkal 2014), scientific papers (Ó Séaghdha and Teufel 2014), official mails, news and encyclopedia articles all have relatively fixed discourse structure and exhibit recurrent patterns. Learning the document structure is of great importance for discourse analysis and has various applications, such as text generation (Prasad et al. 2005) and text summarization (Louis, Joshi, and Nenkova 2010).\nThere are two important aspects of document structure learning: topic modeling and rhetorical structure modeling. Topic modeling assumes multiple topics often exist within a domain. It aims to discover the latent semantics of the documents, with many popular models such as Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003), in which each document is posited as an admixture over an underlying set of topics, and each word is draw from a specific topic. Rhetorical structure modeling aims to uncover the underlying organization of documents. Inspired by the discourse theory (Mann and Thompson 1988), each sentence in a document can be assigned a rhetorical function, or called in-\n∗Corresponding author. Copyright c© 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nIntent words Topic words Intent structure\nBACKGROUND: Modeling document structure is of great\nimportance for discourse analysis. OBJECTIVE: The goal of\nthis research is to capture the document intent structure. We\npresent GMM-LDA to jointly model the topics and intents.\nMETHOD: GMM-LDA is a topic modeling based Bayesian\nmodel that employs generalized Mallows model to capture the\nintent order structure and introduces binary variables to indicate\nthe types of words. RESULT: Experimental results on Chemical\nand Elements datasets demonstrate the superiority of our model.\nFigure 1: Demonstration of the intent and topic words. Stop words (in gray) can be removed by preprocessing.\ntent. For example, the sentences in a scientific paper may have different intents such as “background”, “objective”, “method” and “result”. Fig 1 presents an example of the intent structure of an abstract.\nDocument-level topics and sentence-level intents usually show contradictory characteristics. For example, it is often sensible to assume that the topics are relatively unchanged through one document (e.g., in LDA), while the sentences’ intents usually change following certain order in discourse. Furthermore, each document often follows a progression of nonrecurring coherent intents (Halliday and Hasan 1976), and the sentences with the same intent tend to appear within the same block of a document. Based on these observations, it’s natural to think that jointly considering these two incompatible structures can help to model document better.\nIn this paper, we present a hierarchical Bayesian model to discover both the topic structure and rhetorical structure of documents by jointly considering topics and the above order structure in discourse. To this end, we assume that all the words can be divided into two types: topic word and intent word. Specifically, topic words in a document are relevant to the document’s topic and spread throughout the document, while intent words mainly contribute to the rhetorical functions of the sentences. Following the example in Fig. 1, the words “document”, “discourse” and “intent” are likely to be topic words; they indicate the specific research domain of this document. Meanwhile, the words “result”, “dataset” and “demonstrate” are likely to be intent words and a sentence with these words may have the intent structure label “result”.\nar X\niv :1\n51 2.\n02 00\n9v 1\n[ cs\n.C L\n] 7\nD ec\n2 01\n5\nWe introduce a binary variable for each word to indicate its type, and model the topic structure and intent structure respectively using topic models. Inspired by the generalized Mallows model (GMM) (Fligner and Verducci 1986), we incorporate the intent order structure using GMM-Multi prior, which not only conforms to our intuition of nonrecurring coherent intents but also captures the global effects of the orders. To further improve the expressive ability, we present two important variants of GMM-LDA. One is to incorporate the known intent labels of sentences for supervised learning, and the other is to incorporate entropic regularization to better separate the words into two types under the regularized Bayesian inference framework (Zhu, Chen, and Xing 2014). Finally, we represent experiments on real datasets to demonstrate the effectiveness of our methods over several state-of-the-art baselines. To the best of our knowledge, we present the first model for topics and intents simultaneously, where the intent order structure is described globally by using GMM-Multi prior. In the rest paper, we first present GMM-LDA, followed by the supervised version and entropic regularization. Then we present experimental results with analysis. Finally, we discuss related work and conclude."
    }, {
      "heading" : "Unsupervised GMM-LDA Model",
      "text" : "We consider the following document structure learning problem. We are given a corpus D = {sd}Dd=1 with D documents, where a document sd is a sequence of Nd sentences denoted by sd = (wd1,wd2, ...,wdNd) and a sentence wds is a bag of Nds words denoted by wds = {wdsm}Ndsm=1. The size of the vocabulary is V . There are T topics and K intents in total. Topics are document-level, while intents are sentence-level. Our goal of document structure learning is to model the intent and the topic simultaneously and assign an intent label to each sentence in the corpus.\nWe build our models on the following assumptions, 1) Type: Each word in the corpus is either an intent word or a topic word; 2) Order: The intents of sentences within a document change following certain orders and the orders are similar within a domain; and 3) Coherence: The same intent does not appear in disconnected portions of a document.\nTo characterize the type assumption, we associate each word with a binary variable to indicate whether it is an intent word or a topic word. For the order and coherence assumptions, we introduce a GMM-Multi prior to model the intent order structure. We start with the description of GMM-Multi prior, then GMM-LDA in detail with its inference method."
    }, {
      "heading" : "GMM-Multi Prior for Intent Ordering",
      "text" : "According to the coherence assumption, the same intent could not be assigned to the unconnected portions of a document. To satisfy this assumption, we introduce a GMMMulti prior over possible intent permutations. GMM-Multi, motivated by (Chen et al. 2009), is an extension to the generalized Mallows model (Fligner and Verducci 1986). It concentrates probability mass on a small set of similar permutations around a canonical ordering π0, which confirms to the intuition that the intent orders are similar within a domain. The inversion representation of permutations is used\ninstead of the direct order sequence: If we set the canonical ordering π0 as the identity permutation (1, 2, ...,K), then any permutation π can be denoted as a (K−1)-dimensional vector υ = (υ1, υ2, ..., υK−1), where υk is the count of the numbers in π that are before k and greater than k. For instance, permutation π = (2, 1, 4, 5, 3) can be represented as υ = (1, 0, 2, 0), where υ3 = 2 as there are two numbers, 4 and 5, that are before 3 and greater than 3. υK is omitted as it is always zero. Obviously, a one-to-one correlation exists between these two kinds of permutation representations. Furthermore, each element of υ is independent of each other. The marginal distribution over υk is\nGMMk(υk; ρk) = e−ρkυk\nψk(ρk) , (1)\nwhere ψk(ρk) = 1−exp (−(K−k+1)ρk)\n1−exp (−ρk) is the normalization factor and ρk > 0 is the dispersion parameter. Then the probability mass function with parameters ρ = (ρ1, ρ2, ..., ρK−1) can be written as the product over all the components, so that the number of parameters grows linearly with the number of intents. Due to the exponential form, the conjugate prior for ρk is also an exponential distribution with two parameters :\nGMM0(ρk; υk,0, ν0) ∝ e(−ρkυk,0−logψk(ρk))ν0 . (2)\nIntuitively, ν0 is the number of previous trials and υk,0 is the average number in previous trials for υk. Let ρ0 be the prior value for each ρk. By setting the maximum likelihood estimate of ρk to be ρ0, we can obtain υk,0 = 1exp (ρ0)−1 − K−k+1 exp ((K−k+1)ρ0)−1 .\nThe GMM-Multi prior is defined over sentence intents zd via a generative process on a given inversion representation of permutation υd. Firstly, we draw a bag of intents ud (Nd elements) and each element uds∼Multinomial(λ). Then ∑ sI(uds = k) represents the number of sentences of intent k in this document. Secondly, we obtain the permutation of intentsπd = Compute−π(υd), where Compute−π transforms υd into the intent permutation πd. And finally, we obtain the intent structure of the sentences zd = Compute−z(ud,πd), so that zds is the intent label for sentencewds. Compute−z is the algorithm to obtain the intent structure zd using the bag of intents ud and the permutation πd. It arranges all the intent labels in ud as the order of πd with the same intent labels appearing together. For example, when ud = {1, 1, 2, 3, 3, 3, 5} and πd = (2, 1, 4, 5, 3), we can obtain zd = (2, 1, 1, 5, 3, 3, 3). Note that not all the K intent labels should appear in zd; it depends on the intent labels in ud. As in the example, ud does not contain 4, so 4 does not appear in the intent structure zd."
    }, {
      "heading" : "Generative Process of GMM-LDA",
      "text" : "Now we present GMM-LDA, an unsupervised Bayesian generative model, as illustrated in Fig. 2. GMM-LDA simultaneously models the topics (the blue part in Fig. 2) and the intents (the red part in Fig. 2). The binary variable bdsm denotes the type of word wdsm: if bdsm = 1, then wdsm is a topic word; if bdsm = 0, then wdsm is an intent word. Each\nsentence has a specific intent label zds, while each document sd has a topic mixing distribution θd. For topic words, there is a document-specific topic model. It is a hierarchical Bayesian model that posits each document as an admixture of T topics, where each topic βt is a multinomial distribution over a V -word vocabulary, drawn from a language model βt ∼ Dirichlet(β0) (t ∈ [T ]). For intent words, there is a rhetorical language model, in which the intent structure zd of document sd is generated from a bag of intent labels ud and an intent permutation πd follows the GMM-Multi prior. The total number of intents isK, and each intentαk is also a multinomial distribution over vocabulary, drawn from another language model αk ∼ Dirichlet(α0) (k ∈ [K]).\nFor each document sd, the generating process is 1. Draw a topic proportion θd ∼ Dirichlet(θ0). 2. Obtain the intent structure zd ∼ GMM-Multi(ρ,λ), so\nthat zds is the intent label for sentence wds. 3. For each word wdsm in document d,\n(a) Draw an indicator bdsm ∼ Bernoulli(γ). (b) If bdsm = 0, then wdsm is from intent part:\ndraw wdsm ∼ Multinomial(αzds). (c) if bdsm = 1, then wdsm is from topic part:\ndraw a topic tdsm ∼ Multinomial(θd), and draw wdsm ∼ Multinomial(βtdsm).\nFor fully-Bayesian GMM-LDA, we assume the following priors: γ ∼ Beta(γ0), λ ∼ Dirichlet(λ0), which is a distribution to express how likely each intent label is to appear regardless of positions, and each component of ρ follows ρk ∼ GMM0(ρ0, ν0), k ∈ [K − 1]. The variables subscripted with 0 are fixed prior hyperparameters."
    }, {
      "heading" : "Collapsed Gibbs Sampling",
      "text" : "Let M = {u,ρ,υ, t, b} denote all the variables to be learned during training. Then with Bayes’ theorem, the posterior distribution of GMM-LDA is\nq(M|D) ∝ p0(M)p(D|M), (3) where p0(M) is the prior and p(D|M) is the likelihood. Then it can be learned with a collapsed Gibbs sampler due to the conjugate priors. We naturally split the variables into four parts, namely u, ρ, υ and {t, b}, then sample them using their posterior distributions respectively.\nForu : For each document sd,ud is a bag of intent labels with Nd elements. We resample each element uds via:\nq(uds = x|D,M−uds) ∝ p0(uds = x)p(sd|M,D−sd)\n∝ (f−dsu=x + λ0) K∏ k=1 Γ(f−d0,k,· + V α0) Γ(f0,k,· + V α0) V∏ v=1 Γ(f0,k,v + α0) Γ(f−d0,k,v + α0) ,\nwhere the subscript “−” denotes that some elements are omitted from a set, e.g., M−uds is the set M except uds and D−sd is the set of all documents in D except sd. Let u = {ud}Dd=1 denote all the intent labels in the corpus. f−dsu=x is the count of times that intent label x appears in u except uds. Let wv denote the v-th word in the vocabulary1, where v ∈ [V ]. f0,k,v counts the times that wv appears as an intent word in the sentences with intent label k. Then, f0,k,· = ∑V v=1 f0,k,v . The superscript “−d” indicates that the frequency is calculated over all documents except sd. For ρ : We update each ρk from its posterior distribution:\nq(ρk|D,M−ρk)=GMM0 ( ρk; ∑D d=1υd,k+υk,0ν0\nD+ν0 , D+ν0\n) ,\nSince the normalization constant is unknown, it is intractable to sample directly from GMM0. Fortunately, slice sampling (Neal 2003) can be used to solve this problem.\nFor υ : For the inversion representation υd of document sd, each υd,k can be resampled independently from its posterior distribution:\nq(υd,k = v|D,M−υd,k)∝p0(υd,k = v)p(sd|M,D−sd)\n∝ p0(υd,k = v) K∏ k=1 Γ(f−d0,k,· + V α0) Γ(f0,k,· + V α0) V∏ v=1 Γ(f0,k,v + α0) Γ(f−d0,k,v + α0) ,\nwhere p0(υd,k = v) = GMMk(υd,k = v; ρk) is the prior. For t, b : Since tdsm has meaningful value only when bdsm = 1, we jointly sample bdsm and tdsm for the topics. The joint distributions are\nq(bdsm = 1, tdsm = t|D,M−{bdsm,tdsm})\n∝ (f−dsmb=1 + γ0) f−dsm1,t,v + β0\nf−dsm1,t,· + V β0\nf−dsm1,d,t + θ0\nf−dsm1,d,· + Tθ0 ,\nq(bdsm = 0, tdsm = ∅|D,M−{bdsm,tdsm})\n∝ (f−dsmb=0 + γ0) f−dsm0,zds,v + α0\nf−dsm0,zds,· + V α0 ,\nwhere fb=1 is the number of topic words in the corpus and fb=0 is the number of intent words. f1,t,v counts all the times that wv appears in the corpus with indicator variable value 1 and topic label t. Then, f1,t,· = ∑V v=1 f1,t,v . f1,d,t is the number of words in document sd with indicator variable value 1 and topic label t. Then f1,d,· = ∑T t=1 f1,d,t . The superscript “−dsm” indicates that the frequency is calculated exceptwdsm. According to the joint distribution of bdsm and tdsm, we compute the T + 1 non-zero probabilities, then do normalization and sample from the resulting multinomial.\n1wv is different from wdsm. wdsm is a word in sentence wds ."
    }, {
      "heading" : "Supervised GMM-LDA",
      "text" : "GMM-LDA is unsupervised; it learns the document structure without human annotation. As suggested by existing supervised topic models (McAuliffe and Blei 2008; Wang, Blei, and Li 2009; Zhu, Ahmed, and Xing 2012), the predictive power can be greatly improved with a small amount of labeled documents. Here, we present a supervised GMM-LDA which combines the known intent labels of sentences during learning.\nWe consider the setting where a part of the documents in the corpus are labeled, that is, each sentence is assigned an intent label. Our goal is to develop a supervised model to learn the intent structures for the remaining unlabeled documents. The simplest way to leverage the label information is to use the labels directly during learning instead of sampling them. However, the GMM describes the intent order structure in a global way, which makes the process more complicated. In the unsupervised case, the canonical intent permutation is the identity ordering (1, 2, ...,K) as the intent numbers are completely symmetric and not linked to any meaningful intent label. However, the true intent labels are already known in the supervised case. Then a challenging problem is to determine the canonical permutation π0. Moreover, for the part of labeled documents, we have the true intent structure zd, which is unnecessary to draw from the GMM-Multi prior. So the next challenging problem is how to leverage the known intent structure zd to help for learning. That is, how to obtain ud and πd using zd. Below, we discuss in detail on how to solve these problems.\nForπ0: As shown in Fig.3, we present an approximate three-step algorithm to obtain the canonical permutation π0. Step 1: We compute π′d for each labeled document sd, where π′d is the permutation of the Kd (6 K) intent labels appearing in document sd. We arrange the Kd intent labels in the order they appear in zd. However, there exist cases where the same intent label appears in disconnected portions of zd, which are rare in practice. When encountering these cases, we take the position that the label appears most consecutively in the sequence. If there are two or more of these occurrences, we take the first position (see Fig. 3 for some examples). Step 2: We introduce variables gij(i, j ∈ [K]), where gij counts the times of the intent label i appearing before j in all π′d. Then we define a directed graph G = (V, E), where V ∈ [K] is the set of nodes and E = {(i, j)|gij >= gji; i, j ∈ V} is the set of edges. Step 3:\nWe obtain the π0 by calculating the topological sequence of G. If there are circles in G, we randomly break them. If there are multiple topological sequences, we randomly take one. In our experiments, the real situations are always in accordance with our intuition that only one topological sequence can be obtained from G.\nFor ud and πd : To combine the label information, we compute ud and πd using zd and π0. ud can be obtained by directly putting all the ordered elements of zd into a bag. πd is a permutation of K numbers, which can be obtained by inserting the remaining K − Kd numbers into π′d . We want πd to be as close to π0 as possible, which is consistent with the idea of GMM. Specially, d(πd,π0) is the distance from πd to π0, which defined as the minimum number of swaps of adjacent elements needed to transform πd into the same order of π0. Inspired by the idea of greedy method, we insert theK−Kd numbers into π′d one by one and each step need to minimize the distance. During supervised learning, when it comes to a labeled document sd, we directly take the values of πd and ud instead of updating them."
    }, {
      "heading" : "GMM-LDA with Entropic Regularization",
      "text" : "GMM-LDA jointly models the two incompatible structures of documents by using a binary variable to indicate the type (intent or topic) of each word. It can happen that a same word located in two different positions are assigned with different types, which is somewhat unreasonable. Most of the time, the type of a word can be decided regardless of which position it appears in. For instance, the words “propose” and “experiment” are more likely to be intent words regardless of their positions. In order to model the significant divergence between topics and intents, we introduce the entropy of the words to make our model more descriptive. As we know, in information theory, the entropy of a discrete random variable X = {x1, x2, ..., xn} can explicitly be written as H(X) = − ∑n i=1 p(xi) log p(xi). Similarly, the entropy of a word wv in the vocabulary can be formulated as:\nH(bv) = − ∑ i=0,1 p(bv = i) log p(bv = i), (4)\nwhere p(bv = i) denotes the probability that word wv appears as an intent word (i = 0) or an topic word (i = 1) in the corpus. Lower entropic value means better separation.\nAs GMM-LDA is under Bayesian inference framework, it is challengeable to incorporate the entropic knowledge. Nevertheless, regularized Bayesian inference framework (Zhu, Chen, and Xing 2014) provides us an alternative interpretation of Bayesian inference (Williams 1980) and can combine domain knowledge flexibly. Specifically, GMM-LDA with entropic regularization can be formulated as:\nmin q(M)∈P KL(q(M)||q(M|D)) + c V∑ v=1 H(bv), (5)\nwhereP is the space of probability distributions and c > 0 is a regularization parameter. When c = 0, the optimal solution of the Kullback-Leibler divergence KL(q||p) is q(M) = q(M|D), the standard Bayesian posterior distribution as in\nEq. (3). When c > 0, the entropic knowledge is imposed as a regularization constraint. With mean-filed assumption, the inference of Eq. (5) is similar to that of GMM-LDA. Moreover, the entropic regularization is only relevant to b, which can be combined with both the unsupervised GMM-LDA and the supervised GMM-LDA."
    }, {
      "heading" : "Experimental Results",
      "text" : "To demonstrate the efficacy of our models, we evaluate the performance on two tasks: unsupervised clustering and supervised classification. We use two real datasets: 1) Chemical (Guo et al. 2010): It contains 965 abstracts of scientific papers about 5 kinds of chemicals, and each abstract focuses on one of the 5 topics. Each sentence is annotated with one of the 7 intent labels: Background, Objective, Related Work, Method, Result, Conclusion and Future Work; and 2) Elements (Chen et al. 2009): It consists of 118 articles from the English Wikipedia, and each article talks about one of the 118 chemical elements in the periodic table. Each paragraph is annotated with an intent label. We take the 8 most frequently occurring intent labels: Top-level Segment, History, Isotopes, Applications, Occurrence, Notable Characteristics, Precautions and Compounds, and filter out paragraphs with other labels. Although the intent structure is paragraphlevel in Element, while it is sentence-level in Chemical, the word “sentence” is used throughout the paper for simplicity. Tab. 2 summarizes the dataset statistics. Although both datasets are in chemistry domain, they have different characteristics that can be observed from the experimental results. In Chemical, the intent orders are relatively fixed due to the writing conventions of scientific papers, while they are more variable in Elements.\nData preprocessing involves removing a small set of stop words, tokens containing non-alphabetic characters, tokens appearing less than 3 times, tokens of length one and sentences with less than 5 valid tokens. We report the average results over 5 runs, while each run takes a sufficiently large\nnumber of iterations (e.g. 2000) to converge. Statistical significance is measured with t-test."
    }, {
      "heading" : "Unsupervised Clustering",
      "text" : "Our goal of unsupervised clustering is to learn an intent label for each sentence in the corpus without any true label information. Adjusted Rand Index (ARI)(Vinh, Epps, and Bailey 2010), recall, precision and F-score are used as our evaluation measures. F-score is the harmonic mean of recall and precision. For all the four measures, higher scores are better.\nWe consider two variants of our model: 1) GMM-LDA: Our unsupervised model; and 2) EGMM-LDA: GMM-LDA with entropic regularization. For hyperparameters, we set θ0 = 0.1, λ0 = 0.1, α0 = 0.1, β0 = 0.1 and γ0 = 1, since we find that the results are insensitive to them. ν0 is set to be 0.1 times the number of documents in the corpus. For EGMM-LDA, we set the regularization parameter c to be 0.1. The baseline methods we use are: 1) K-means: The feature used for each sentence is the bag of words. 2) Boilerplate-LDA: The model presented in (Ó Séaghdha and Teufel 2014). 3) GMM: The intent part of GMM-LDA, which is the content model by (Chen et al. 2009) and can be implemented by fixing all indicator variables bdsm to 0 during learning; and 4) GMM-LDA (Uniform): The model assumes a uniform distribution over all intent permutations and can be implemented by fixing ρ to zero.\nClustering performance: We set the number of topics T = 10 for Chemical and T = 5 for Elements. Tab. 1 shows the results. For Chemical, our two models outperform all other methods on four measures, with p-values smaller than 0.001 except for Boilerplate-LDA with K = 5. The exception of Boilerplate-LDA may be caused by the large variance in the results of multiple runs when K is small. Since the first-order Markov chain is used in Boilerplate-LDA for order structure learning, which only has a local view and is more susceptible to noise. Moreover, first-order Markov chain would select the same intent label for disconnected sentences within a document, which is against our intuition. Our models overcome these problems by using GMM and also achieve better performance. The simpler variants of our models achieve reasonable performance. GMM underperforms GMM-LDA, indicating that modeling topics and intents simultaneously provides a richer and more effective way to document structure learning. The bad performance\nof the uniform variant proves the indispensability to model the intent order structure. Moreover, our model yields better results with entropic regularization. As to Elements, the results are similar to that of Chemical. However, we can observe that GMM performs competitively in Elements, which shows the characteristic of this dataset that there is no obvious topic structure. We can also observe that the highest recall scores are obtained by K-means with very low precision scores, since K-means prefers to assign the same label to most of the sentences. It can thus be seen that the task is difficult and the richer models are required.\nHyperparameter ρ0: ρ0 controls the variability of the order structure and would be set according to different datasets. The model with large ρ0 assigns massive probabilities around the canonical permutation and the order structure is relatively fixed, while the model with small ρ0 relaxes the constraints and has a variety of orders far from the canonical one. For Elements with K = 10, we change ρ0 from 0.125 to 4 and report the F-scores of GMM-LDA in Fig. 4. It can be observed that the performance is stable in a wide range (e.g. 0.5 < ρ0 < 2). We set ρ0 = 2 for all the experiments except for Elements with K = 10, in which we set ρ0 = 1.\nTypes of words: To embody the intents and topics in the results, we assume that each word in the vocabulary is either an intent word or a topic word. If a word appears in the corpus more as an intent word than a topic word, we classify it as an intent word; otherwise, it is a topic word. Note that this additional condition is introduced only for the ease of demonstration. In order to see how our models separate these two types, we list 18 most commonly used words of each type in Chemical according to the result of EGMMLDA. As shown in Tab. 3, we can observe that almost all the intent words has rhetorical functions that can express the intents of sentences, while almost all the topic words are about chemical topics. The good separations justify our assumption and show the effectiveness of our models."
    }, {
      "heading" : "Supervised Classification",
      "text" : "Now, we evaluate our supervised models for classifying sentences. For each dataset, we randomly choose 20% documents; annotate their sentences with intent labels; and use them for training. Our goal is to learn the intent labels for the sentences in the remaining 80% documents. We report accuracy (ACC) and the ARI scores to show the improvements compared to the unsupervised learning. We again consider two variants of our model: 1) sGMM-LDA: Our supervised model; and 2) sEGMM-LDA: sGMM-LDA with entropic regularization. The baseline methods are : 1) SVM: We use the bag-of-words features, linear kernel and SVMLight tools (Joachims 1998). 2) sBoilerplate-LDA : The supervised version of Boilerplate-LDA, in which we fix the known labels during learning instead of updating them; and 3) sGMM : The intent part of our sGMM-LDA. All the settings are the same as that in the unsupervised learning.\nClassification performance: Tab. 4 presents the results. For Chemical, the best accuracies are achieved by our two models (p < 0.01), which again proves that our assumption of the two types of words is reasonable and the intent order structure can be better modeled by employing GMM. For Elements, sBoilerplate-LDA and sGMM perform competitively, while EGMM-LDA beats all the other methods (p < 0.05). It shows that our model is more robust with entropic regularization. The ARI scores improve a lot compared to that in Tab. 1, indicating that the predictive power can be largely improved with just 20% labeled documents.\nIntent words: We can obtain the canonical intent permutation by the known intent labels, at the same time the distribution over vocabulary can be learned. Tab. 5 shows the results on Chemical with sEGMM-LDA. We can observe that the canonical intent order (numbered from 0 to\n6) conforms to the convention in scientific writing. Moreover, from the high-frequency words of each intent, we can see that most of the words express the intent labels well. For instance, “study” and “investigated” express the intent Objective, while “increased” and “significantly” are for Result."
    }, {
      "heading" : "Related Work",
      "text" : "From the algorithmic perspective, our work is grounded in topic models, such as Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003), which have been widely developed for many NLP tasks. Instead of representing documents as bags of words, many expanded models take specific structural constraints into consideration (Purver et al. 2006; Gruber, Weiss, and Rosen-Zvi 2007). Among different models, our work has a closer relation to the models with order structure. For order modeling, Markov chain can only capture the dependence locally (Ó Séaghdha and Teufel 2014; Barzilay and Lee 2004; Elsner, Austerweil, and Charniak 2007), while the generalized Mallows model (GMM) (Fligner and Verducci 1986) has a global view (Chen et al. 2009; Du, Pate, and Johnson 2015; Cheng, Hühn, and Hüllermeier 2009). A more complete model can be obtained by dividing the words into different types. In early trial of zoneLDAb (Varga, Preotiuc-Pietro, and Ciravegna 2012), a type of words are for describing background, which are independent of the category of the sentence. Boilerplate-LDA (Ó Séaghdha and Teufel 2014) also considers two types: document-specific topic words and rhetorical words. Three types of words are learnt by a rule-based method in (Nguyen and Shirai 2015). However, global order structure is not considered in these models. Therefore, jointly modeling topics and intents with global order structure is of great value."
    }, {
      "heading" : "Conclusion and Future Work",
      "text" : "We present GMM-LDA (both unsupervised and supervised) for document structure learning, which simultaneously model topics and intents. The generalized Mallows model is employed to model the intent order globally. Moreover, we consider the entropic regularization to make the model more descriptive. Our results demonstrate the reasonability of our intuitions and the effectiveness of our models. For future work, we are interested in making our models richer by combining local coherence constraints.\nAcknowledgments This work was mainly done when the first author was an intern at Microsoft Research Asia. The work was supported by the National Basic Research Program (973 Program) of China (Nos. 2013CB329403, 2012CB316301), National NSF of China (Nos. 61322308, 61332007), TNList Big Data Initiative, and Tsinghua Initiative Scientific Research Program (Nos. 20121088071, 20141080934)."
    } ],
    "references" : [ {
      "title" : "Catching the drift: Probabilistic content models, with applications to generation and summarization",
      "author" : [ "Barzilay", "R. Lee 2004] Barzilay", "L. Lee" ],
      "venue" : null,
      "citeRegEx" : "Barzilay et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Barzilay et al\\.",
      "year" : 2004
    }, {
      "title" : "Content modeling using latent permutations",
      "author" : [ "Chen" ],
      "venue" : null,
      "citeRegEx" : "Chen,? \\Q2009\\E",
      "shortCiteRegEx" : "Chen",
      "year" : 2009
    }, {
      "title" : "Decision tree and instance-based learning for label ranking",
      "author" : [ "Hühn Cheng", "W. Hüllermeier 2009] Cheng", "J. Hühn", "E. Hüllermeier" ],
      "venue" : null,
      "citeRegEx" : "Cheng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2009
    }, {
      "title" : "Topic segmentation with an ordering-based topic model",
      "author" : [ "Pate Du", "L. Johnson 2015] Du", "J. Pate", "M. Johnson" ],
      "venue" : null,
      "citeRegEx" : "Du et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2015
    }, {
      "title" : "A unified local and global model for discourse coherence",
      "author" : [ "Austerweil Elsner", "M. Charniak 2007] Elsner", "J. Austerweil", "E. Charniak" ],
      "venue" : null,
      "citeRegEx" : "Elsner et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Elsner et al\\.",
      "year" : 2007
    }, {
      "title" : "Distance based ranking models. JRSS. Series B (Methodological) 359–369",
      "author" : [ "Fligner", "M. Verducci 1986] Fligner", "J. Verducci" ],
      "venue" : null,
      "citeRegEx" : "Fligner et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Fligner et al\\.",
      "year" : 1986
    }, {
      "title" : "A hierarchical bayesian model for unsupervised induction of script knowledge",
      "author" : [ "Titov Frermann", "L. Pinkal 2014] Frermann", "I. Titov", "M. Pinkal" ],
      "venue" : null,
      "citeRegEx" : "Frermann et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Frermann et al\\.",
      "year" : 2014
    }, {
      "title" : "Hidden topic markov models",
      "author" : [ "Weiss Gruber", "A. Rosen-Zvi 2007] Gruber", "Y. Weiss", "M. Rosen-Zvi" ],
      "venue" : "In AISTATS",
      "citeRegEx" : "Gruber et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Gruber et al\\.",
      "year" : 2007
    }, {
      "title" : "Identifying the information structure of scientific abstracts: an investigation of three different schemes",
      "author" : [ "Guo" ],
      "venue" : "In Workshop on Biomedical Natural Language Processing",
      "citeRegEx" : "Guo,? \\Q2010\\E",
      "shortCiteRegEx" : "Guo",
      "year" : 2010
    }, {
      "title" : "Cohesion in english",
      "author" : [ "Halliday", "M. Hasan 1976] Halliday", "R. Hasan" ],
      "venue" : null,
      "citeRegEx" : "Halliday et al\\.,? \\Q1976\\E",
      "shortCiteRegEx" : "Halliday et al\\.",
      "year" : 1976
    }, {
      "title" : "Discourse indicators for content selection in summarization",
      "author" : [ "Joshi Louis", "A. Nenkova 2010] Louis", "A. Joshi", "A. Nenkova" ],
      "venue" : "In Special Interest Group on Discourse and Dialogue,",
      "citeRegEx" : "Louis et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Louis et al\\.",
      "year" : 2010
    }, {
      "title" : "Rhetorical structure theory: Toward a functional theory of text organization. Text-Interdisciplinary Journal for the Study of Discourse 8(3):243–281",
      "author" : [ "Mann", "W. Thompson 1988] Mann", "S. Thompson" ],
      "venue" : null,
      "citeRegEx" : "Mann et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Mann et al\\.",
      "year" : 1988
    }, {
      "title" : "Supervised topic models",
      "author" : [ "McAuliffe", "J. Blei 2008] McAuliffe", "D. Blei" ],
      "venue" : "In NIPS",
      "citeRegEx" : "McAuliffe et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "McAuliffe et al\\.",
      "year" : 2008
    }, {
      "title" : "Topic modeling based sentiment analysis on social media for stock market prediction",
      "author" : [ "Nguyen", "T. Shirai 2015] Nguyen", "K. Shirai" ],
      "venue" : null,
      "citeRegEx" : "Nguyen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised learning of rhetorical structure with un-topic models",
      "author" : [ "Ó Séaghdha", "D. Teufel 2014] Ó Séaghdha", "S. Teufel" ],
      "venue" : "In COLING",
      "citeRegEx" : "Séaghdha et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Séaghdha et al\\.",
      "year" : 2014
    }, {
      "title" : "The penn discourse treebank as a resource for natural language generation",
      "author" : [ "Prasad" ],
      "venue" : "In CL Workshop on Using Corpora for NLG",
      "citeRegEx" : "Prasad,? \\Q2005\\E",
      "shortCiteRegEx" : "Prasad",
      "year" : 2005
    }, {
      "title" : "Unsupervised topic modelling for multi-party spoken discourse",
      "author" : [ "Purver" ],
      "venue" : null,
      "citeRegEx" : "Purver,? \\Q2006\\E",
      "shortCiteRegEx" : "Purver",
      "year" : 2006
    }, {
      "title" : "Unsupervised document zone identification using probabilistic graphical models",
      "author" : [ "Preotiuc-Pietro Varga", "A. Ciravegna 2012] Varga", "D. PreotiucPietro", "F. Ciravegna" ],
      "venue" : null,
      "citeRegEx" : "Varga et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Varga et al\\.",
      "year" : 2012
    }, {
      "title" : "Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance",
      "author" : [ "Epps Vinh", "N. Bailey 2010] Vinh", "J. Epps", "J. Bailey" ],
      "venue" : null,
      "citeRegEx" : "Vinh et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vinh et al\\.",
      "year" : 2010
    }, {
      "title" : "Simultaneous image classification and annotation",
      "author" : [ "Blei Wang", "C. Li 2009] Wang", "D. Blei", "F. Li" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2009
    }, {
      "title" : "Medlda: Maximum margin supervised topic models",
      "author" : [ "Ahmed Zhu", "J. Xing 2012] Zhu", "A. Ahmed", "E. Xing" ],
      "venue" : null,
      "citeRegEx" : "Zhu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2012
    }, {
      "title" : "Bayesian inference with posterior regularization and applications to infinite latent svms",
      "author" : [ "Chen Zhu", "J. Xing 2014] Zhu", "N. Chen", "E. Xing" ],
      "venue" : null,
      "citeRegEx" : "Zhu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Modeling document structure is of great importance for discourse analysis and related applications. The goal of this research is to capture the document intent structure by modeling documents as a mixture of topic words and rhetorical words. While the topics are relatively unchanged through one document, the rhetorical functions of sentences usually change following certain orders in discourse. We propose GMM-LDA, a topic modeling based Bayesian unsupervised model, to analyze the document intent structure cooperated with order information. Our model is flexible that has the ability to combine the annotations and do supervised learning. Additionally, entropic regularization can be introduced to model the significant divergence between topics and intents. We perform experiments in both unsupervised and supervised settings, results show the superiority of our model over several state-of-the-art baselines.",
    "creator" : "LaTeX with hyperref package"
  }
}