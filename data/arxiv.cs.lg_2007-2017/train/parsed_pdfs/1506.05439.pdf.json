{
  "name" : "1506.05439.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning with a Wasserstein Loss",
    "authors" : [ "Charlie Frogner", "Chiyuan Zhang", "Hossein Mobahi", "Mauricio Araya-Polo" ],
    "emails" : [ "frogner@mit.edu,", "chiyuan@mit.edu,", "tp@ai.mit.edu", "hmobahi@csail.mit.edu", "Mauricio.Araya@shell.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe efficient learning algorithms based on this regularization, extending the Wasserstein loss from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss and show connections with the total variation norm and the Jaccard index. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, achieving superior performance over a baseline that doesn’t use the metric."
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the problem of learning to predict a measure over a finite set. This problem includes many widely-used machine learning scenarios. For example, in multiclass classification, the set consists of the classes and a predicted distribution over classes is used to determine the top-K most likely classes (as in the ImageNet Large Scale Visual Recognition Challenge [ILSVRC]) or to do subsequent inference (as with acoustic modeling in speech recognition). Another example is semantic segmentation [1], where the set consists of the pixel locations, and a segment can be modeled as a uniform measure supported on a subset.\nIn practice, many learning problems have natural similarity or metric structure on the output space. For example, in semantic segmentation, spatial adjacency between pixel locations provides a strong cue for similarity of their labels, due to contiguity of segmented regions. Such spatial adjacency can be captured, for example, by the Euclidean distance between the pixel locations. And in the ILSVRC image classification task, the output set comprises 1000 visual categories that are organized in a hierarchy, from which various semantic similarity measures are derived. Hierarchical structure in the label space is also prevalent in document categorization problems. In the following, we call the similarity structure in the label space the ground metric or semantic similarity.\n∗Authors contributed equally.\nar X\niv :1\n50 6.\n05 43\n9v 1\n[ cs\n.L G\n] 1\nThe presence of a ground metric can be taken into account when measuring the prediction performance. For example, confusing dogs with cats might be a more severe error than confusing breeds of dogs. Intuitively, a loss incorporating this metric should encourage the algorithm to favor predictions that are, if not completely accurate, at least semantically similar to the ground truth.\nIn this paper, we develop a loss function for multi-label learning\nthat incorporates a metric on the output space by measuring the Wasserstein distance between a prediction and the target label, with respect to that metric. The Wasserstein distance is defined as the cost of the optimal transport plan for moving the mass in the predicted measure to match that in the target, and has been applied to a wide range of problems, including barycenter estimation [2], label propagation [3], and clustering [4]. To our knowledge, this paper represents the first use of the Wasserstein distance as a loss for supervised learning.\nIncorporating an output metric into the loss can meaningfully impact learning performance. Take, for example, a multiclass classification problem containing semantically near-equivalent categories. Figure 1 shows such a case from the ILSVRC, in which the categories Siberian husky and Eskimo dog are nearly indistinguishable. Such categories can introduce noise in human-labeled data, as the labelers may fail to make fine distinctions between the categories. We simulate this problem by identifying the classes with points on a grid in the two-dimensional plane and randomly switching the labels to neighboring classes. We compare the standard multiclass logistic loss to the Wasserstein loss, and measure the prediction performance with the Euclidean distance between the predicted class and the true class. As shown in Figure 2, The prediction performance of both losses degrades as more labels are perturbed. Importantly, by incorporating the ground metric, the Wasserstein loss yields predictions that are closer to the ground truth, across all noise levels. Section D.1 of the Appendix describes the experiment in more detail.\nThe main contributions of this paper are as follows. We formulate the problem of learning with knowledge of the ground metric, and propose the Wasserstein loss as an alternative to traditional information divergence-based loss functions. Specifically, we focus on empirical risk minimization (ERM) with the Wasserstein loss, and describe efficient learning algorithms based on entropic regularization of the optimal transport problem. Moreover, we justify ERM with the Wasserstein loss by showing a statistical learning bound and we draw connections with existing measures of performance. Finally, we evaluate the proposed loss on both synthetic examples and a real-world image annotation problem, demonstrating benefits for incorporating an output metric into the loss."
    }, {
      "heading" : "2 Related work",
      "text" : "Decomposable loss functions like KL Divergence and `p distances are very popular for probabilistic [1] or vector-valued [5] predictions, as each component can be evaluated independently, often leading to simple and efficient algorithms. The idea of exploiting smoothness in the label space according to a prior metric has been explored in many different forms, including regularization [6] and post-processing with graphical models [7]. Optimal transport provides a natural distance for probability distributions over metric spaces. In [2, 8], the optimal transport is used to formulate the Wasserstein Barycenter as a probability distribution with minimum Wasserstein distance to a set of\ngiven points on the probability simplex. [9] propagates histogram values on a graph by minimizing a Dirichlet energy induced by the optimal transport. The Wasserstein distance is also used to formulate a metric for comparing clusters in [10], as well as applied for image retrieval [11], contour matching [12], and many other problems that can be formulated as histogram matching [13]. However, to our knowledge, this is the first time it is used as a loss function in a discriminative learning framework. The closest work to this paper is a theoretical study [14] of an estimator that minimizes the optimal transport cost between the empirical distribution and the estimated distribution in the setting of statistical parameter estimation."
    }, {
      "heading" : "3 Learning with a Wasserstein loss",
      "text" : ""
    }, {
      "heading" : "3.1 Problem setup and notation",
      "text" : "Consider the problem of learning a map from X ∈ RDX to the space Y = RK+ of measures over a finite set K of size |K| = K. Assume K is a subset of a metric space with metric dK(·, ·). dK is called the ground metric, and it measures the semantic similarity in the label space. We perform learning over a hypothesis spaceH of predictors hθ : X → Y , parameterized by θ ∈ Θ. In the standard statistical learning setting, we get an i.i.d. sequence of training examples S = ((x1, y1), . . . , (xN , yN )), sampled from an unknown joint distribution PX×Y . Given a measure of performance (a.k.a. risk) E(·, ·), the goal is to find the predictor hθ ∈ H that minimizes the expected risk E[E(hθ(x), y)]. Typically E(·, ·) is difficult to optimize directly and the joint distribution PX×Y is unknown, so learning is performed via empirical risk minimization. Specifically, we solve\nmin hθ∈H\n{ ÊS [`(hθ(x), y) = 1\nN N∑ i=1 `(hθ(xi), yi) } with a loss function `(·, ·) acting as a surrogate of E(·, ·)."
    }, {
      "heading" : "3.2 Optimal transport and the exact Wasserstein loss",
      "text" : "Information divergence-based loss functions are widely used in learning with probability-valued outputs. But along with other popular measures like Hellinger distance and χ2 distance, these divergences are invariant to permutation of the elements in K, ignoring any metric structure on K. Given a cost function c : K × K → R, the optimal transport distance [15] measures the cheapest way to transport a probability measure µ1 to match µ2 with respect to c:\nWc(µ1, µ2) = inf γ∈Π(µ1,µ2) ∫ K×K c(κ1, κ2)γ(dκ1, dκ2) (1)\nwhere Π(µ1, µ2) is the set of joint probability measures on K × K having µ1 and µ2 as marginals. An important case is when the cost is given by a metric dK(·, ·) or its p-th power dpK(·, ·) with p ≥ 1. In this case, they are called Wasserstein distances [16], also known as the earth mover’s distances [11]. In this paper, we only work with discrete measures. In the case of probability measures, these are histograms in the simplex ∆K.\nWhen the ground truth y and the output of h both lie in the simplex ∆K, we can define a Wasserstein loss at x. Definition 3.1 (Exact Wasserstein Loss). For any hθ ∈ H, hθ : X → ∆K, let hθ(κ|x) = hθ(x)κ be the predicted value at element κ ∈ K, given input x ∈ X . Let y(κ) be the ground truth value for κ given by the corresponding label y. Then we define the Wasserstein loss as\nW pp (h(·|x), y(·)) = inf T∈Π(h(x),y) 〈T,M〉 (2)\nwhere M ∈ RK×K+ is the distance matrix Mκ,κ′ = d p K(κ, κ ′), and the set of valid transport plans is\nΠ(h(x), y) = {T ∈ RK×K+ : T1 = h(x), T>1 = y} (3) where 1 is the all-one vector.\nW pp is the cost of the optimal plan for transporting the predicted mass distribution h(x) to match the target distribution y. The penalty increases as more mass is transported over longer distances, according to the ground metric M ."
    }, {
      "heading" : "4 Efficient optimization",
      "text" : "The Wasserstein loss (2) is a linear program and Lagrangian duality gives a means of computing descent direction with respect to h(x). The dual LP of (2) is\ndW pp (h(x), y) = sup α,β∈CM α>h(x) + β>y, CM = {(α, β) ∈ RK×K : ακ + βκ′ ≤Mκ,κ′}. (4)\nAs (2) is a linear program, at an optimum the values of the dual and the primal are equal (see, e.g. [17]), hence the dual optimal α is a subgradient of the loss with respect to its first argument.\nComputing α is costly, as it entails solving a linear program with O(K2) contraints, with K being the dimension of the output space. This cost can be prohibitive when optimizing by gradient descent."
    }, {
      "heading" : "4.1 Entropic regularization of optimal transport",
      "text" : "Cuturi [18] proposes a smoothed transport objective that enables efficient approximation of both the transport matrix in (2) and the subgradient of the loss. [18] introduces an entropic regularization term that results in a strictly convex problem:\nλW pp (h(·|x), y(·)) = inf T∈Π(h(x),y) 〈T,M〉+ λH(T ), H(T ) = − ∑ κ,κ′ Tκ,κ′ log Tκ,κ′ . (5)\nImportantly, the transport matrix that solves (5) is a diagonal scaling of a matrix K = e−λM−1:\nT ∗ = diag(u)Kdiag(v) (6)\nfor u = eλα and v = eλβ , where α and β are the Lagrangian dual variables for (5).\nIdentifying such a matrix subject to equality constraints on the row and column sums is exactly a matrix balancing problem, which is well-studied in numerical linear algebra and for which efficient iterative algorithms exist [19]. [18] and [2] use the well-known Sinkhorn-Knopp algorithm."
    }, {
      "heading" : "4.2 Extending smoothed transport to the learning setting",
      "text" : "When the output vectors h(x) and y lie in the simplex, (5) can be used directly as a surrogate for (2). In this case, α is a subgradient of the objective and can be obtained from the optimal scaling vector u as α = 1λ log u. Note that there is a translation ambiguity here: any upscaling of the vector u can be paired with a corresponding downscaling of the vector v without altering the matrix T ∗ (and vice versa). This means that α is only defined up to a constant shift. In [2] the authors recommend choosing α = 1λ log u− 1 Kλ log u >1 so that α is tangent to the simplex.\nFor many learning problems, however, a normalized output assumption is unnatural. In image segmentation, for example, the target shape is not naturally represented as a histogram. And even when the prediction and the ground truth are constrained to the simplex, the observed label can be subject to noise that violates the constraint.\nThere is more than one way to generalize optimal transport to unnormalized measures. The objective we choose should deal effectively with the difference in total mass between h(x) and y while still being efficient to optimize."
    }, {
      "heading" : "4.3 Relaxed transport",
      "text" : "We propose a novel relaxation that extends smoothed transport to unnormalized measures. By replacing the equality constraints on the transport marginals in (5) with soft penalties with respect to KL divergence, we get an unconstrained approximate transport problem. The resulting objective is:\nλ,γa,γbWKL(h(·|x), y(·)) = min T∈RK×K+\n〈T,M〉+λH(T ) + γaK̃L (T1‖h(x)) + γbK̃L ( T>1‖y ) (7)\nwhere K̃L (w‖z) = w> log(w z) − 1>w + 1>z is the generalized KL divergence between w, z ∈ RK+ . Here represents element-wise division. As with the previous formulation, the optimal transport matrix with respect to (7) is a diagonal scaling of the matrix K.\n(a) Convergence to smoothed transport. (b) Approximation of exact Wasserstein. (c) Convergence of alternating projections (λ = 50).\nFigure 3: The relaxed transport problem (7) for unnormalized measures.\nProposition 4.1. The transport matrix T ∗ optimizing (7) satisfies T ∗ = diag(u)Kdiag(v), where u = (h(x) T ∗1)λγa , v = ( y (T ∗)>1 )λγb , and K = e−λM−1. And the optimal transport matrix is a fixed point for a Sinkhorn-like iteration.\nProposition 4.2. T ∗ = diag(u)Kdiag(v) optimizing (7) satisfies: i) u = h(x) γaλ γaλ+1 (Kv)− γaλ γaλ+1 , and ii) v = y γbλ γbλ+1 ( K>u )− γbλγbλ+1 , where represents element-wise multiplication. Unlike the previous formulation, (7) is unconstrained and differentiable with respect to h(x). The gradient is given by ∇h(x)WKL(h(·|x), y(·)) = γa (1− T ∗1 h(x)). When restricted to normalized measures, the relaxed problem (7) approximates smoothed transport (5). Figure 3a shows, for normalized h(x) and y, the relative distance between the values of (7) and (5) 1. For λ large enough, (7) converges to (5) as γa and γb increase.\n(7) also retains two properties of smoothed transport (5). Figure 3b shows that, for normalized outputs, the relaxed loss converges to the unregularized Wasserstein distance as λ, γa and γb increase 2. And Figure 3c shows that convergence of the iterations in (4.2) is nearly independent of the dimension K of the output space."
    }, {
      "heading" : "5 Properties of the Wasserstein loss",
      "text" : "In this section, we study the statistical properties of learning with the exact Wasserstein loss (2) as well as connections with two standard measures. Full proofs can be found in the appendix."
    }, {
      "heading" : "5.1 Generalization error",
      "text" : "Let S = ((x1, y1), . . . , (xN , yN )) be i.i.d. samples and hθ̂ be the empirical risk minimizer\nhθ̂ = argmin hθ∈H\n{ ÊS [ W pp (hθ(·|x), y) ] = 1\nN N∑ i=1 W pp (hθ(·|xi), yi)\n} .\nFurther assume H = s ◦ Ho is the composition of a softmax s and a base hypothesis space Ho of functions mapping into RK . The softmax layer outputs a prediction that lies in the simplex ∆K. Theorem 5.1. For p = 1, and any δ > 0, with probability at least 1− δ, it holds that\nE [ W 11 (hθ̂(·|x), y) ] ≤ inf hθ∈H E [ W 11 (hθ(·|x), y) ] + 32KCMRN (Ho) + 2CM\n√ log(1/δ)\n2N (8)\nwith the constant CM = maxκ,κ′Mκ,κ′ . RN (Ho) is the Rademacher complexity [21] measuring the complexity of the hypothesis spaceHo.\n1In figures 3a-c, h(x), y and M are generated as described in [18] section 5. In 3a-b, h(x) and y have dimension 256. In 3c, convergence is defined as in [18]. Shaded regions are 95% intervals.\n2The unregularized Wasserstein distance was computed using FastEMD [20].\nThe Rademacher complexity RN (Ho) for commonly used models like neural networks and kernel machines [21] decays with the training set size. This theorem guarantees that the expected Wasserstein loss of the empirical risk minimizer approaches the best achievable loss forH. As an important special case, minimizing the empirical risk with Wasserstein loss is also good for multiclass classification. Let y = eκ be the “one-hot” encoded label vector for the groundtruth class. Proposition 5.2. In the multiclass classification setting, for p = 1 and any δ > 0, with probability at least 1− δ, it holds that\nEx,κ [ dK(κθ̂(x), κ) ] ≤ inf hθ∈H KE[W 11 (hθ(x), y)] + 32K2CMRN (Ho) + 2CMK √ log(1/δ) 2N (9)\nwhere the predictor is κθ̂(x) = argmaxκ hθ̂(κ|x), with hθ̂ being the empirical risk minimizer.\nNote that instead of the classification error Ex,κ[1{κθ̂(x) 6= κ}], we actually get a bound on the expected semantic distance between the prediction and the groundtruth."
    }, {
      "heading" : "5.2 Connection with other standard measures",
      "text" : "The special case in which no prior similarity is assumed between the points is captured by the 0-1 ground metric, defined by M0−1κ,κ′ = 1κ 6=κ′ = 1− δκ,κ′ . In this case, it is known that the Wasserstein distance reduces to the total variation distance TV(·, ·): Proposition 5.3. For the 0-1 ground metric, ∀ probability measures µ, ν, W 10−1(µ, ν) = TV(µ, ν). The Wasserstein loss is also closely related to the Jaccard index [22], also known as intersectionover-union (IoU), which is a popular measure of performance in segmentation [23]. For two regions A and B in the image plane, the Jaccard index is defined as J(A,B) = |A ∩ B|/|A ∪ B|. The associated Jaccard distance dJ(A,B) = 1− J(A,B) is a metric on the space of all finite sets [22]. If we treat each region A as a uniform probability distribution UA supported on A, then it holds that Proposition 5.4. The Wasserstein loss W 10−1 is a proxy of dJ in the sense that for any 0 ≤ ε ≤ 1, W 10−1(U A,UB) ≤ ε if dJ(A,B) ≤ ε; conversely, dJ(A,B) ≤ 2ε if W 10−1(UA,UB) ≤ ε.\nWhen the Euclidean distance in the image plane is used as the ground metric, the general Wasserstein loss W pp is still a surrogate of dJ : Corollary 5.5. For any 0 ≤ ε ≤ 1, and p ≥ 1, under the ground metric d(κ, κ′) = ‖κ− κ′‖pp over the set of pixel coordinates, W pp (U A,UB) ≤ ε implies that dJ(A,B) ≤ 2ε.\nUnlike W 10−1, W p p is stronger than dJ because it ensures not only that the incorrectly predicted region is small, but also that it is not far away. The connection with the Jaccard distance can also be characterized for the case of non-uniform distributions. See section C.4 in the Appendix for details."
    }, {
      "heading" : "6 Empirical study",
      "text" : ""
    }, {
      "heading" : "6.1 Impact of the ground metric",
      "text" : "In this section, we show that the Wasserstein loss encourages smoothness with respect to an artificial metric on the MNIST handwritten digit dataset. This is a multi-class classification problem with output dimensions corresponding to the 10 digits, and we apply a ground metric dp(κ, κ′) = |κ − κ′|p, where κ, κ′ ∈ {0, . . . , 9} and p ∈ [0,∞). This metric encourages the recognized digit to be numerically close to the true one. We train a model independently for each value of p and plot the average predicted probabilities of the different digits on the test set in Figure 4.\nNote that as p → 0, the metric approaches the 0 − 1 metric d0(κ, κ′) = 1κ6=κ′ , which treats all incorrect digits as being equally unfavorable. In this case, as can be seen in the figure, the predicted probability of the true digit goes to 1 while the probability for all other digits goes to 0. As p increases, the predictions become more evenly distributed over the neighboring digits, converging to a uniform distribution as p→∞ 3.\n3To avoid numerical issues, we scale down the ground metric such that all of the distance values are in the interval [0, 1)."
    }, {
      "heading" : "6.2 Flickr tag prediction",
      "text" : "We apply the Wasserstein loss to a real world multi-label learning problem, using the recently released Yahoo/Flickr Creative Commons 100M dataset [24]. Our goal is tag prediction: we select 1000 descriptive tags along with two random sets of 10,000 images each, associated with these tags, for training and testing. We derive a distance metric between tags by using word2vec [25] to embed the tags as unit vectors, then taking their Euclidean distances. To extract image features we use MatConvNet [26]. Note that the set of tags is highly redundant and often many semantically equivalent or similar tags can apply to an image. The images are also incompletely tagged, as different users may prefer different tags. We therefore measure the prediction performance by the top-K cost, defined as CK = 1/K ∑K k=1 minj dK(κ̂k, κj), where {κj} is the set of groundtruth tags, and {κ̂k} are the tags with highest predicted probability. We find that a linear combination of the Wasserstein loss W pp and a KL divergence-based loss yields the best prediction results. Specifically, we train a linear model by minimizing W pp + αKL on the training set, where α controls the relative weight of KL. Figure 5a shows the top-K cost on the test set for the combined loss and the baseline KL loss. We additionally create a second dataset by removing redundant labels from the original dataset: this simulates the potentially more difficult case in which a single user tags each image, by selecting one tag to apply from amongst each cluster of applicable, semantically similar tags. Figure 3b shows that performance for both algorithms decreases on the harder dataset, while the combined Wasserstein loss continues to outperform the baseline.\nIn Figure 6, we show the effect on performance of varying the weight α on the KL loss. We observe that the optimum of the top-K cost is achieved when the Wasserstein loss is weighted more heavily than at the optimum of the AUC. This is consistent with a semantic smoothing effect of Wasserstein, which during training will favor mispredictions that are semantically similar to the ground truth, sometimes at the cost of lower AUC 4. We finally show two selected images from the test set in\n4The Wasserstein loss can achieve a similar trade-off alone as discussed in Section 6.1. However, the achievable range is usually limited by numerical stability when dealing with large values of the metric. In practice it is often easier to implement the trade-off by combining with a KL loss.\nFigure 7. These illustrate cases in which both algorithms make predictions that are semantically relevant, despite overlapping very little with the ground truth. The image on the left shows semantically irrelevant errors made by the baseline algorithm. More examples can be found in the appendix."
    }, {
      "heading" : "7 Conclusions and future work",
      "text" : "In this paper we have described a loss function for learning to predict a measure over a finite set, based on the Wasserstein distance. Optimizing with respect to the exact Wasserstein loss is computationally costly and we describe efficient algorithms based on entropic regularization, for learning both normalized and unnormalized measures. We have also described a statistical learning bound for the loss and shown connections with both the total variation norm and the Jaccard index. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space, and we demonstrate this property on a real-data tag prediction problem, achieving superior performance over a baseline that doesn’t incorporate the metric.\nAn interesting direction for future work may be to explore the connection between the Wasserstein loss and Markov random fields, as the latter are often used to encourage smoothness of predictions, via inference at prediction time."
    }, {
      "heading" : "A Relaxed transport",
      "text" : "Equation (7) gives the relaxed transport objective as λ,γa,γbWKL(h(·|x), y(·)) = min\nT∈RK×K+ 〈T,M〉+ λH(T ) + γaK̃L (T1‖h(x)) + γbK̃L\n( T>1‖y ) with K̃L (w‖z) = w> log(w z)− 1>w + 1>z.\nProof of Proposition 4.1. The first order condition for T ∗ optimizing (7) is\nMij + 1\nλ\n( log T ∗ij + 1 ) + γa (log T ∗1 h(x))i + γb ( log(T ∗)>1 y ) j = 0.\n⇒ log T ∗ij + γaλ log (T ∗1)i + γbλ log ( (T ∗)>1 ) j = −λMij + γaλ log h(x)i + γbλ log yj − 1 ⇒T ∗ij(T ∗1)γaλ((T ∗)>1)γbλ = exp (−λMij + γaλ log h(x)i + γbλ log yj − 1)\n⇒T ∗ij = (h(x) T ∗1) γaλ i\n( y (T ∗)>1 )γbλ j exp (−λMij − 1)\nHence T ∗ (if it exists) is a diagonal scaling of K = exp (−λM − 1).\nProof of Proposition 4.2. Let u = (h(x) T ∗1)γaλ and v = ( y (T ∗)>1 )γbλ, so T ∗ = diag(u)Kdiag(v). We have\nT ∗1 = diag(u)Kv\n⇒ (T ∗1)γaλ+1 h(x)γaλ = Kv\nwhere we substituted the expression for u. Re-writing T ∗1,\n(diag(u)Kv)γaλ+1 = diag(h(x)γaλ)Kv\n⇒uγaλ+1 = h(x)γaλ (Kv)−γaλ\n⇒u = h(x) γaλ γaλ+1 (Kv)− γaλ γaλ+1 .\nA symmetric argument shows that v = y γbλ γbλ+1 (K>u)− γbλ γbλ+1 ."
    }, {
      "heading" : "B Statistical Learning Bounds",
      "text" : "We establish the proof of Theorem 5.1 in this section. For simpler notation, for a sequence S = ((x1, y1), . . . , (xN , yN )) of i.i.d. training samples, we denote the empirical risk R̂S and risk R as\nR̂S(hθ) = ÊS [ W pp (hθ(·|x), y(·)) ] , R(hθ) = E [ W pp (hθ(·|x), y(·)) ] (10)\nLemma B.1. Let hθ̂, hθ∗ ∈ H be the minimizer of the empirical risk R̂S and expected risk R, respectively. Then\nR(hθ̂) ≤ R(hθ∗) + 2 sup h∈H |R(h)− R̂S(h)|\nProof. By the optimality of hθ̂ for R̂S ,\nR(hθ̂)−R(hθ∗) = R(hθ̂)− R̂S(hθ̂) + R̂S(hθ̂)−R(hθ∗) ≤ R(hθ̂)− R̂S(hθ̂) + R̂S(hθ∗)−R(hθ∗) ≤ 2 sup\nh∈H |R(h)− R̂S(h)|\nTherefore, to bound the risk for hθ̂, we need to establish uniform concentration bounds for the Wasserstein loss. Towards that goal, we define a space of loss functions induced by the hypothesis spaceH as\nL = { `θ : (x, y) 7→W pp (hθ(·|x), y(·)) : hθ ∈ H } (11)\nThe uniform concentration will depends on the “complexity” of L, which is measured by the empirical Rademacher complexity defined below. Definition B.2 (Rademacher Complexity [21]). Let G be a family of mapping from Z to R, and S = (z1, . . . , zN ) a fixed sample from Z . The empirical Rademacher complexity of G with respect to S is defined as\nR̂S(G) = Eσ [ sup g∈G 1 N n∑ i=1 σig(zi) ] (12)\nwhere σ = (σ1, . . . , σN ), with σi’s independent uniform random variables taking values in {+1,−1}. σi’s are called the Rademacher random variables. The Rademacher complexity is defined by taking expectation with respect to the samples S,\nRN (G) = ES [ R̂S(G) ] (13)\nTheorem B.3. For any δ > 0, with probability at least 1− δ, the following holds for all `θ ∈ L, E[`θ]− ÊS [`θ] ≤ 2RN (L) + √ C2M log(1/δ)\n2N (14)\nwith the constant CM = maxκ,κ′Mκ,κ′ .\nBy the definition of L, E[`θ] = R(hθ) and ÊS [`θ] = R̂S [hθ]. Therefore, this theorem provides a uniform control for the deviation of the empirical risk from the risk. Theorem B.4 (McDiarmid’s Inequality). Let S = {X1, . . . , XN} ⊂ X be N i.i.d. random variables. Assume there exists C > 0 such that f : X N → R satisfies the following stability condition\n|f(x1, . . . , xi, . . . , xN )− f(x1, . . . , x′i, . . . , xN )| ≤ C (15) for all i = 1, . . . , N and any x1, . . . , xN , x′i ∈ X . Then for any ε > 0, denoting f(X1, . . . , XN ) by f(S), it holds that\nP (f(S)− E[f(S)] ≥ ε) ≤ exp ( − 2ε 2\nNC2\n) (16)\nLemma B.5. Let the constant CM = maxκ,κ′Mκ,κ′ , then 0 ≤W pp (·, ·) ≤ CM .\nProof. For any h(·|x) and y(·), let T ∗ ∈ Π(h(x), y) be the optimal transport plan that solves (2), then W pp (h(x), y) = 〈T ∗,M〉 ≤ CM ∑ κ,κ′ Tκ,κ′ = CM\nProof of Theorem B.3. For any `θ ∈ L, note the empirical expectation is the empirical risk of the corresponding hθ:\nÊS [`θ] = 1\nN N∑ i=1 `θ(xi, yi) = 1 N N∑ i=1 W pp (hθ(·|xi), yi(·)) = R̂S(hθ)\nSimilarly, E[`θ] = R(hθ). Let Φ(S) = sup\n`∈L E[`]− ÊS [`] (17)\nLet S′ be S with the i-th sample replaced by (x′i, y ′ i), by Lemma B.5, it holds that\nΦ(S)− Φ(S′) ≤ sup `∈L ÊS′ [`]− ÊS [`] = sup hθ∈H\nW pp (hθ(x ′ i), y ′ i)−W pp (hθ(xi), yi) N ≤ CM N\nSimilarly, we can show Φ(S′)−Φ(S) ≤ CM/N , thus |Φ(S′)−Φ(S)| ≤ CM/N . By Theorem B.4, for any δ > 0, with probability at least 1− δ, it holds that\nΦ(S) ≤ E[Φ(S)] + √ C2M log(1/δ)\n2N (18)\nTo bound E[Φ(S)], by Jensen’s inequality, ES [Φ(S)] = ES [ sup `∈L E[`]− ÊS [`] ] = ES [ sup `∈L ES′ [ ÊS′ [`]− ÊS [`] ]] ≤ ES,S′ [ sup `∈L ÊS′ [`]− ÊS [`] ]\nHere S′ is another sequence of i.i.d. samples, usually called ghost samples, that is only used for analysis. Now we introduce the Rademacher variables σi, since the role of S and S′ are completely symmetric, it follows\nES [Φ(S)] ≤ ES,S′,σ [ sup `∈L 1 N N∑ i=1 σi(`(x ′ i, y ′ i)− `(xi, yi)) ]\n≤ ES′,σ [ sup `∈L 1 N N∑ i=1 σi`(x ′ i, y ′ i) ] + ES,σ [ sup `∈L 1 N N∑ i=1 −σi`(xi, yi) ] = ES [ R̂S(L) ] + ES′ [ R̂S′(L)\n] = 2RN (L)\nThe conclusion follows by combing (17) and (18).\nTo finish the proof of Theorem 5.1, we combine Lemma B.1 and Theorem B.3, and relate RN (L) to RN (H) via the following generalized Talagrand’s lemma [27]. Lemma B.6. Let F be a class of real functions, and H ⊂ F = F1 × . . . × FK be a K-valued function class. If m : RK → R is a Lm-Lipschitz function and m(0) = 0, then R̂S(m ◦ H) ≤ 2Lm ∑K k=1 R̂S(Fk). Theorem B.7 (Theorem 6.15 of [15]). Let µ and ν be two probability measures on a Polish space (K, dK). Let p ∈ [1,∞) and κ0 ∈ K. Then\nWp(µ, ν) ≤ 21/p ′ (∫ K dK(κ0, κ)d|µ− ν|(κ) )1/p , 1 p + 1 p′ = 1 (19)\nCorollary B.8. The Wasserstein loss is Lipschitz continuous in the sense that for any hθ ∈ H, and any (x, y) ∈ X × Y ,\nW pp (hθ(·|x), y) ≤ 2p−1CM ∑ κ∈K |hθ(κ|x)− y(κ)| (20)\nIn particular, when p = 1, we have W 11 (hθ(·|x), y) ≤ CM ∑ κ∈K |hθ(κ|x)− y(κ)| (21)\nWe cannot apply Lemma B.6 directly to the Wasserstein loss class, because the Wasserstein loss is only defined on probability distributions, so 0 is not a valid input. To get around this problem, we assume the hypothesis spaceH used in learning is of the form\nH = {s ◦ ho : ho ∈ Ho} (22) where Ho is a function class that maps into RK , and s is the softmax function defined as s(o) = (s1(o), . . . , sK(o)), with\nsk(o) = eok∑ j e oj , k = 1, . . . ,K (23)\nThe softmax layer produce a valid probability distribution from arbitrary input, and this is consistent with commonly used models such as Logistic Regression and Neural Networks. By working with the log of the groundtruth labels, we can also add a softmax layer to the labels.\nLemma B.9 (Proposition 2 of [28]). The Wasserstein distances Wp(·, ·) are metrics on the space of probability distributions of K, for all 1 ≤ p ≤ ∞. Proposition B.10. The map ι : RK × RK → R defined by ι(y, y′) = W 11 (s(y), s(y′)) satisfies\n|ι(y, y′)− ι(ȳ, ȳ′)| ≤ 4CM‖(y, y′)− (ȳ, ȳ′)‖2 (24)\nfor any (y, y′), (ȳ, ȳ′) ∈ RK × RK . And ι(0, 0) = 0.\nProof. For any (y, y′), (ȳ, ȳ′) ∈ RK × RK , by Lemma B.9, we can use triangle inequality on the Wasserstein loss,\n|ι(y, y′)− ι(ȳ, ȳ′)| = |ι(y, y′)− ι(ȳ, y′) + ι(ȳ, y′)− ι(ȳ, ȳ′)| ≤ ι(y, ȳ) + ι(y′, ȳ′)\nFollowing Corollary B.8, it continues as\n|ι(y, y′)− ι(ȳ, ȳ′)| ≤ CM (‖s(y)− s(ȳ)‖1 + ‖s(y′)− s(ȳ′)‖1) (25)\nNote for each k = 1, . . . ,K, the gradient∇ysk satisfies\n‖∇ysk‖2 = ∥∥∥∥∥ ( ∂sk ∂yj )K j=1 ∥∥∥∥∥ 2 = ∥∥∥(δkjsk − sksj)Kj=1∥∥∥ 2 = √√√√s2k K∑ j=1 s2j + s 2 k(1− 2sk) (26)\nBy mean value theorem, ∃α ∈ [0, 1], such that for yθ = αy + (1− α)ȳ, it holds that\n‖s(y)− s(ȳ)‖1 = K∑ k=1 ∣∣∣〈∇ysk|y=yαk , y − ȳ〉∣∣∣ ≤ K∑ k=1 ‖∇ysk|y=yαk ‖2‖y − ȳ‖2 ≤ 2‖y − ȳ‖2\nbecause by (26), and the fact that √∑\nj s 2 j ≤ ∑ j sj = 1 and √ a+ b ≤ √ a + √ b for a, b ≥ 0, it\nholds K∑ k=1 ‖∇ysk‖2 = ∑\nk:sk≤1/2\n‖∇ysk‖2 + ∑\nk:sk>1/2\n‖∇ysk‖2\n≤ ∑\nk:sk≤1/2\n( sk + sk √ 1− 2sk ) + ∑ k:sk>1/2 sk ≤ K∑ k=1 2sk = 2\nSimilarly, we have ‖s(y′)− s(ȳ′)‖1 ≤ 2‖y′ − ȳ′‖2, so from (25), we know\n|ι(y, y′)− ι(ȳ, ȳ′)| ≤ 2CM (‖y − ȳ‖2 + ‖y′ − ȳ′‖2) ≤ 2 √ 2CM ( ‖y − ȳ‖22 + ‖y′ − ȳ′‖22 )1/2 then (24) follows immediately. The second conclusion follows trivially as s maps the zero vector to a uniform distribution.\nProof of Theorem 5.1. Consider the loss function space preceded with a softmax layer\nL = {ιθ : (x, y) 7→W 11 (s(hoθ(x)), s(y)) : hoθ ∈ Ho}\nWe apply Lemma B.6 to the 4CM -Lipschitz continuous function ι in Proposition B.10 and the function space\nHo × . . .×Ho︸ ︷︷ ︸ K copies ×I × . . .× I︸ ︷︷ ︸ K copies\nwith I a singleton function space with only the identity map. It holds R̂S(L) ≤ 8CM ( KR̂S(Ho) +KR̂S(I) ) = 8KCM R̂S(Ho) (27)\nbecause for the identity map, and a sample S = (y1, . . . , yN ), we can calculate\nR̂S(I) = Eσ [ sup f∈I 1 N N∑ i=1 σif(yi) ] = Eσ [ 1 N N∑ i=1 σiyi ] = 0\nThe conclusion of the theorem follows by combining (27) with Theorem B.3 and Lemma B.1."
    }, {
      "heading" : "C Connection with other standard measures",
      "text" : "C.1 Connection with multiclass classification\nProof of Proposition 5.2. Given that the label is a “one-hot” vector y = eκ, the set of transport plans (3) degenerates. Specifically, the constraint T>1 = eκ means that only the κ-th column of T could be non-zero. Above that, the constraint T1 = hθ̂(·|x) ensures that the κ-th column of T actually equals to hθ̂(·|x). In other words, the set Π(hθ̂(·|x), eκ) contains only one feasible transport plan, so (2) can be computed directly as\nW pp (hθ̂(·|x), eκ) = ∑ κ′∈K Mκ′,κhθ̂(κ ′|x) = ∑ κ′∈K dpK(κ ′, κ)hθ̂(κ ′|x)\nNow let κ̂ = argmaxκ hθ̂(κ|x) be the prediction, we have hθ̂(κ̂|x) = 1− ∑ κ6=κ̂ hθ̂(κ|x) ≥ 1− ∑ κ 6=κ̂ hθ̂(κ̂|x) = 1− (K − 1)hθ̂(κ̂|x)\nTherefore, hθ̂(κ̂|x) ≥ 1/K, so\nW pp (hθ̂(·|x), eκ) ≥ d p K(κ̂, κ)hθ̂(κ̂|x) ≥ d p K(κ̂, κ)/K\nThe conclusion follows by applying Theorem 5.1 with p = 1.\nC.2 Connection with the total variation distance\nThe total variation distance between two distributions µ and ν is defined as TV(µ, ν) = supA⊂K |µ(A)− ν(A)|. It can be shown that\nTV(µ, ν) = 1\n2 ∑ κ∈K |µ(κ)− ν(κ)| = 1− ∑ κ∈K min(µ(κ), ν(κ)) (28)\nProof of Proposition 5.3. In the case of 0-1 ground metric, the transport cost becomes 〈T,M〉 = ∑ κ,κ′ Tκ,κ′Mκ,κ′ = 1− ∑ κ Tκ,κ (29)\nMoreover, by the constraint T ∈ Π(µ, ν), we have µ(κ) = ∑ κ′ Tκ,κ′ = Tκ,κ + ∑ κ′ 6=κ Tκ,κ′ ≥ Tκ,κ, ν(κ) = ∑ κ′ Tκ′,κ = Tκ,κ + ∑ κ′ 6=κ Tκ′,κ ≥ Tκ,κ\nTherefore, the minimum of (29) is achieved by T ∗κ,κ = min(µ(κ), ν(κ)) for the diagonal, with offdiagonal entries assigned arbitrarily as long as the constraints for Π(µ, ν) are met. As a result, it holds\nW 10−1(µ, ν) = 〈T ∗,M0−1〉 = 1− ∑ κ min(µ(κ), ν(κ)) (30)\nFollowing (28), we can seeW 10−1 equals to the total variation distance, which is also a scaled version of the `1 distance.\nC.3 Connection with the Jaccard distance\nLet W 10−1(·, ·) be the Wasserstein distance under the 0-1 metric defined by d(κ, κ′) = 1κ6=κ′ , then we have the following characterization of the Wasserstein distance between two uniform distributions over regions. Lemma C.1. Let A,B ⊂ K, then\n1−W 10−1(UA,UB) = |A ∩B|\nmax(|A|, |B|) (31)\nProof. The overlapping mass that does not need to transport is given by\nm0 = min\n( 1\n|A| ,\n1\n|B|\n) |A ∩B| (32)\nSince under 0-1 metric, any transport of mass has unit cost, so the minimum attainable transport cost is\n1× (1−m0) = 1− |A ∩B|\nmax(|A|, |B|) (33)\nProof of Proposition 5.4. Given that dJ(A,B) ≤ ε, by Lemma C.1, it holds\nW 10−1(U A,UB) = 1− |A ∩B| max(|A|, |B|) ≤ 1− |A ∩B| |A ∪B| = dJ(A,B) ≤ ε\nConversely, given that W 10−1(U A,UB) ≤ ε, again by Lemma C.1, it holds\n|A ∩B| max(|A|, |B|) ≥ 1− ε (34)\nBy symmetry, without loss of generality, assume |A| ≥ |B|, then\n|A ∩B| ≥ (1− ε)|A| (35)\nAs a result, we have\nJ(A,B) = |A ∩B| |A ∪B| = |A ∩B| |A|+ |B| − |A ∩B| ≥ (1− ε)|A| |A|+ |A| − (1− ε)|A| = 1− ε 1 + ε\nThe conclusion follows as\ndJ(A,B) = 1− J(A,B) ≤ 1− 1− ε 1 + ε = 2ε 1 + ε ≤ 2ε\nProof of Corollary 5.5. Note when the alphabet consists of integer coordinates of pixels, for any κ 6= κ′, ‖κ− κ′‖pp ≥ 1 for any p ≥ 1. Therefore, M0−1κ,κ′ ≤M p κ,κ′ , i.e. the 0-1 ground metric matrix is bounded by the p-Euclidean ground metric matrix, elementwise. Let T ∗ be the optimal transport plan under the p-Euclidean ground metric, which is also a feasible transport plan under the 0 − 1 ground metric. So\nW 10−1(U A,UB) = inf T∈Π(UA,UB) 〈T,M0−1〉 ≤ 〈T ∗,M0−1〉 ≤ 〈T ∗,Mp〉 = W pp (UA,UB)\nThe conclusion follows by a direct application of Proposition 5.4.\nC.4 Relation to the Jaccard distance (non-uniform case)\nLemma C.2. Let A,B ⊂ K, and µA, νB are two probability distributions supported on A,B, respectively, then\nW 10−1(µ A, νB) = 1−min\n{ µA(A ∩B), νB(A ∩B) } (36)\nProof. The amount of mass that completely matches is min{µA(A ∩ B), νB(A ∩ B)}. The rest of the mass needs to be moved around with unit cost 1, so the total minimum transport cost is 1−min{µA(A ∩B), νB(A ∩B)}.\nProposition C.3. Let µA and νB be two probability measures supported on A and B, respectively. Denote\nµ∗ = max κ∈A µA(κ), µo = min κ∈A µA(κ)\nν∗ = max κ∈B νB(κ), νo = min κ∈B µB(κ)\nthen W 10−1(µ A, νB) ≤ ε implies\ndJ(A,B) ≤ 2C∗ − 2Co(1− ε) 2C∗ − (1− ε)Co\n(37)\nwhere C∗ ≥ max(µ∗, ν∗) and 0 < Co ≤ min(µo, νo).\nProof. Notice obviously, for any X ⊂ K, we have the following properties µo|X| ≤ µA(X) ≤ µ∗|X| (38) νo|X| ≤ νB(X) ≤ ν∗|X| (39)\n(40)\nLet us first assume that W 10−1(µ A, νB) ≤ ε, following Lemma C.2, it implies 1−W 10−1(µA, νB) = min ( µA(A ∩B), νB(A ∩B) ) ≥ 1− ε (41)\nOn the other hand,\ndJ(A,B) = 1− |A ∩B| |A ∪B|\nSo we can get an upper bound of dJ(A,B) by deriving a lower bound on |A ∩ B| and an upper bound on |A ∪B|. By (38), (39), and (41), it holds\n|A ∩B| ≥ max { µA(A ∩B)\nµ∗ , νB(A ∩B) ν∗\n} ≥ max { 1− ε µ∗ , 1− ε ν∗ } ≥ 1− ε C∗\nwhere C∗ ≥ max{µ∗, ν∗}. Similarly, we have\n|A ∪B| = |A|+ |B| − |A ∩B| ≤ 1 µo + 1 νo − 1− ε C∗ ≤ 2 Co − 1− ε C∗\nwhere 0 < Co ≤ min{µo, νo}. It then follows that\ndJ(A,B) ≤ 1− 1−ε C∗\n2 Co − 1−εC∗\n≤ 1− Co(1− ε) 2C∗ − (1− ε)Co ≤ 2C ∗ − 2Co(1− ε) 2C∗ − (1− ε)Co\nProposition C.4. Following the same notation of Proposition C.3, dJ(A,B) ≤ ε implies\nW 10−1(µ A, νB) ≤ 2(C ∗ − Co) + ε(2Co − C∗) C∗(2− ε)\n(42)\nProof. By Lemma C.2, in order to upper bound W 10−1(µ A, νB), we only need to derive lower bounds for µA(A ∩B) and νB(A ∩B). By dJ(A,B) ≤ ε, it holds\n1− ε ≤ |A ∩B| |A ∪B| = |A ∩B| |A|+ |B| − |A ∩B| ≤ |A ∩B|\n1/µ∗ + 1/ν∗ − |A ∩B| where the inequality is from (38) and (39). As a result,\n|A ∩B| ≥ 1− ε 2− ε\n( 1\nµ∗ +\n1\nν∗\n) ≥ 1− ε\n2− ε 2 C∗\nwhere C∗ ≥ max{µ∗, ν∗}. By (38) again, we get\nµA(A ∩B) ≥ µo|A ∩B| ≥ 1− ε 2− ε 2µo C∗\nSimilarly, we have\nνB(A ∩B) ≥ 1− ε 2− ε 2νo C∗\nCombining, we get\n1−W 10−1(µA, νB) = min { µA(A ∩B), νB(A ∩B) } ≥ 2Co\nC∗ 1− ε 2− ε\nwhere 0 < Co ≤ min{µo, νo}. The conclusion follows immediately.\nREMARK: For the case of uniform distributions, C∗ = Co, Proposition C.3 and Proposition C.4 fall back to Proposition 5.4."
    }, {
      "heading" : "D Empirical study",
      "text" : "D.1 Noisy label example\nWe illustrate the phenomenon of semantic label noise of human labelers with a synthetic example. Consider a multiclass classification problem, where the labels corresponds to the vertices on aD×D lattice on the 2D plane. The Euclidean distance in R2 is used to measure the semantic similarity between labels. The observations for each category are samples of a isotropic Gaussian distribution centered at the corresponding vertex. Given a noise level t, we choose to flip the label of each training sample to one of the neighboring categories5 with probability t. Figure 8 shows the training set for 3× 3 lattice with noise level t = 0.1 and t = 0.5, respectively. We repeat 10 times for noise levels t = 0.1, 0.2, . . . , 0.9 andD = 3, 4, . . . , 7. A multiclass classifier based on logistic regression is trained with the standard divergenced based loss6 and the proposed Wasserstein loss7, respectively. The performance is measured by the Euclidean distance between the predicted class and the true class, averaged on the test set. Figure 2 compares the performance of the two loss functions.\nD.2 Full figure for the MNIST example\nThe full version of Figure 4 from Section 6.1 is shown in Figure 9.\nD.3 Details of the Flickr tag prediction experiment\nFrom the tags in the Yahoo Flickr Creative Commons dataset, we filtered out those not occurring in the WordNet8 database, as well those whose dominant lexical category was ”noun.location” or ”noun.time.” We also filtered out by hand nouns referring to geographical location or nationality, proper nouns, numbers, photography-specific vocabulary, and several words not generally descriptive of visual content (such as ”annual” and ”demo”). From the remainder, the 1000 most frequently occurring tags were used.\nWe list some of the 1000 selected tags here. The 50 most frequently occurring tags: travel, square, wedding, art, flower, music, nature, party, beach, family, people, food, tree, summer, water, concert, winter, sky, snow, street, portrait, architecture, car, live, trip, friend, cat, sign, garden, mountain, bird, sport, light, museum, animal, rock, show, spring, dog, film, blue, green, road, girl, event, red, fun, building, new, cloud. . . . and the 50 least frequent tags: arboretum, chick, sightseeing, vineyard, animalia, burlesque, key, flat, whale, swiss, giraffe, floor, peak, contemporary, scooter, society, actor, tomb, fabric, gala, coral, sleeping, lizard, performer, album, body, crew, bathroom, bed, cricket, piano, base, poetry, master, renovation, step, ghost, freight, champion, cartoon, jumping, crochet, gaming, shooting, animation, carving, rocket, infant, drift, hope.\n5Connected vertices on the lattice are considered neighbors, and the Euclidean distance between neighbors is set to 1.\n6Corresponds to maximum likelihood estimation of the logistic regression model. 7In this special case, corresponds to weighted maximum likelihood estimation, c.f. Section C.1. 8http://wordnet.princeton.edu\nWe train a multiclass linear logistic regression model with a linear combination of the Wasserstein loss and the KL divergence-based loss. The Wasserstein loss between the prediction and the normalized groundtruth is computed with a entropic regularization formulation using 10 iterations of the Sinkhorn-Knopp algorithm. Based on our observation of the ground metric matrix, we use p-norm with p = 13, and set λ = 50. This makes sure that the matrix K is reasonable sparse, enforcing semantic smoothness only in each local neighborhood. Stochastic gradient descent with a mini-batch size of 100, and momentum 0.7 is run for 100,000 iterations to optimize the objective function on the training set. The baseline is trained under the same setting, but with only the KL loss function.\nTo create the dataset with reduced redundancy, for each image in the training set, we compute the pairwise semantic distance for the groundtruth tags, and cluster them into “equivalent” tag-sets with a threshold of semantic distance 1.3. Within each tag-set, one random tag is selected.\nFigure 10 shows some more test images and predictions randomly picked from the test set.\n(a) Flickr user tags: zoo, run, mark; our proposals: running, summer, fun; baseline proposals: running, country, lake. (b) Flickr user tags: travel, architecture, tourism; our proposals: sky, roof, building; baseline proposals: art, sky, beach. (c) Flickr user tags: spring, race, training; our proposals: road, bike, trail; baseline proposals: dog, surf, bike.\n(d) Flickr user tags: family, trip, house; our proposals: family, girl, green; baseline proposals: woman, tree, family. (e) Flickr user tags: education, weather, cow, agriculture; our proposals: girl, people, animal, play; baseline proposals: concert, statue, pretty, girl.\n(f) Flickr user tags: garden, table, gardening; our proposals: garden, spring, plant; baseline proposals: garden, decoration, plant. (g) Flickr user tags: nature, bird, rescue; our proposals: bird, nature, wildlife; baseline proposals: ature, bird, baby.\nFigure 10: Examples of images in the Flickr dataset. We show the groundtruth tags and as well as tags proposed by our algorithm and baseline."
    } ],
    "references" : [ {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "Jonathan Long", "Evan Shelhamer", "Trevor Darrell" ],
      "venue" : "CVPR (to appear),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Fast Computation of Wasserstein Barycenters",
      "author" : [ "Marco Cuturi", "Arnaud Doucet" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Wasserstein Propagation for Semi-Supervised Learning",
      "author" : [ "Justin Solomon", "Raif M Rustamov", "Leonidas J Guibas", "Adrian Butscher" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Kernels for vector-valued functions: A review",
      "author" : [ "Neil D. Lawrence" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Nonlinear total variation based noise removal algorithms",
      "author" : [ "Leonid I Rudin", "Stanley Osher", "Emad Fatemi" ],
      "venue" : "Physica D: Nonlinear Phenomena,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1992
    }, {
      "title" : "Semantic image segmentation with deep convolutional nets and fully connected crfs",
      "author" : [ "Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L Yuille" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "A Smoothed Dual Approach for Variational Wasserstein Problems",
      "author" : [ "Marco Cuturi", "Gabriel Peyré", "Antoine Rolet" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Wasserstein propagation for semi-supervised learning",
      "author" : [ "Justin Solomon", "Raif Rustamov", "Leonidas Guibas", "Adrian Butscher" ],
      "venue" : "In ICML,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Comparing clusterings in space",
      "author" : [ "Michael Coen", "Hidayath Ansari", "Nathanael Fillmore" ],
      "venue" : "In ICML,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "The earth mover’s distance as a metric for image retrieval",
      "author" : [ "Yossi Rubner", "Carlo Tomasi", "Leonidas J Guibas" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2000
    }, {
      "title" : "Fast contour matching using approximate earth mover’s distance",
      "author" : [ "Kristen Grauman", "Trevor Darrell" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2004
    }, {
      "title" : "Approximate earth mover’s distance in linear time",
      "author" : [ "S Shirdhonkar", "D W Jacobs" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2008
    }, {
      "title" : "On minimum kantorovich distance estimators",
      "author" : [ "Federico Bassetti", "Antonella Bodini", "Eugenio Regazzini" ],
      "venue" : "Stat. Probab. Lett., 76(12):1298–1302,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "Optimal Transport: Old and New",
      "author" : [ "Cédric Villani" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "The Monge-Kantorovich problem: achievements, connections, and perspectives",
      "author" : [ "Vladimir I Bogachev", "Aleksandr V Kolesnikov" ],
      "venue" : "Russian Math. Surveys, 67(5):785,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Introduction to Linear Optimization",
      "author" : [ "Dimitris Bertsimas", "John N. Tsitsiklis", "John Tsitsiklis" ],
      "venue" : "Athena Scientific, Boston, third printing edition,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1997
    }, {
      "title" : "Sinkhorn Distances: Lightspeed Computation of Optimal Transport",
      "author" : [ "Marco Cuturi" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "A fast algorithm for matrix balancing",
      "author" : [ "Philip A Knight", "Daniel Ruiz" ],
      "venue" : "IMA Journal of Numerical Analysis,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Fast and robust Earth Mover’s Distances",
      "author" : [ "Ofir Pele", "Michael Werman" ],
      "venue" : "ICCV, pages 460–467,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "Rademacher and gaussian complexities: Risk bounds and structural results",
      "author" : [ "Peter L Bartlett", "Shahar Mendelson" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2003
    }, {
      "title" : "Optimal Decisions from Probabilistic Models: The Intersection-over-Union Case",
      "author" : [ "Sebastian Nowozin" ],
      "venue" : "CVPR, pages 548–555,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "The new data and new challenges in multimedia research",
      "author" : [ "Bart Thomee", "David A. Shamma", "Gerald Friedland", "Benjamin Elizalde", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li" ],
      "venue" : "arXiv preprint arXiv:1503.01817,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "MatConvNet – Convolutional Neural Networks for MATLAB",
      "author" : [ "A. Vedaldi", "K. Lenc" ],
      "venue" : "CoRR, abs/1412.4564,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Probability in Banach Spaces: Isoperimetry and Processes. Classics in Mathematics",
      "author" : [ "M. Ledoux", "M. Talagrand" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Another example is semantic segmentation [1], where the set consists of the pixel locations, and a segment can be modeled as a uniform measure supported on a subset.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "The Wasserstein distance is defined as the cost of the optimal transport plan for moving the mass in the predicted measure to match that in the target, and has been applied to a wide range of problems, including barycenter estimation [2], label propagation [3], and clustering [4].",
      "startOffset" : 234,
      "endOffset" : 237
    }, {
      "referenceID" : 2,
      "context" : "The Wasserstein distance is defined as the cost of the optimal transport plan for moving the mass in the predicted measure to match that in the target, and has been applied to a wide range of problems, including barycenter estimation [2], label propagation [3], and clustering [4].",
      "startOffset" : 257,
      "endOffset" : 260
    }, {
      "referenceID" : 0,
      "context" : "2 Related work Decomposable loss functions like KL Divergence and `p distances are very popular for probabilistic [1] or vector-valued [5] predictions, as each component can be evaluated independently, often leading to simple and efficient algorithms.",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "2 Related work Decomposable loss functions like KL Divergence and `p distances are very popular for probabilistic [1] or vector-valued [5] predictions, as each component can be evaluated independently, often leading to simple and efficient algorithms.",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 4,
      "context" : "The idea of exploiting smoothness in the label space according to a prior metric has been explored in many different forms, including regularization [6] and post-processing with graphical models [7].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "The idea of exploiting smoothness in the label space according to a prior metric has been explored in many different forms, including regularization [6] and post-processing with graphical models [7].",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 1,
      "context" : "In [2, 8], the optimal transport is used to formulate the Wasserstein Barycenter as a probability distribution with minimum Wasserstein distance to a set of",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 6,
      "context" : "In [2, 8], the optimal transport is used to formulate the Wasserstein Barycenter as a probability distribution with minimum Wasserstein distance to a set of",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 7,
      "context" : "[9] propagates histogram values on a graph by minimizing a Dirichlet energy induced by the optimal transport.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "The Wasserstein distance is also used to formulate a metric for comparing clusters in [10], as well as applied for image retrieval [11], contour matching [12], and many other problems that can be formulated as histogram matching [13].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "The Wasserstein distance is also used to formulate a metric for comparing clusters in [10], as well as applied for image retrieval [11], contour matching [12], and many other problems that can be formulated as histogram matching [13].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : "The Wasserstein distance is also used to formulate a metric for comparing clusters in [10], as well as applied for image retrieval [11], contour matching [12], and many other problems that can be formulated as histogram matching [13].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 11,
      "context" : "The Wasserstein distance is also used to formulate a metric for comparing clusters in [10], as well as applied for image retrieval [11], contour matching [12], and many other problems that can be formulated as histogram matching [13].",
      "startOffset" : 229,
      "endOffset" : 233
    }, {
      "referenceID" : 12,
      "context" : "The closest work to this paper is a theoretical study [14] of an estimator that minimizes the optimal transport cost between the empirical distribution and the estimated distribution in the setting of statistical parameter estimation.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 13,
      "context" : "Given a cost function c : K × K → R, the optimal transport distance [15] measures the cheapest way to transport a probability measure μ1 to match μ2 with respect to c: Wc(μ1, μ2) = inf γ∈Π(μ1,μ2) ∫",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "In this case, they are called Wasserstein distances [16], also known as the earth mover’s distances [11].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "In this case, they are called Wasserstein distances [16], also known as the earth mover’s distances [11].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 15,
      "context" : "[17]), hence the dual optimal α is a subgradient of the loss with respect to its first argument.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "1 Entropic regularization of optimal transport Cuturi [18] proposes a smoothed transport objective that enables efficient approximation of both the transport matrix in (2) and the subgradient of the loss.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 16,
      "context" : "[18] introduces an entropic regularization term that results in a strictly convex problem: W p p (h(·|x), y(·)) = inf T∈Π(h(x),y) 〈T,M〉+ λH(T ), H(T ) = − ∑",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "Identifying such a matrix subject to equality constraints on the row and column sums is exactly a matrix balancing problem, which is well-studied in numerical linear algebra and for which efficient iterative algorithms exist [19].",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 16,
      "context" : "[18] and [2] use the well-known Sinkhorn-Knopp algorithm.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "[18] and [2] use the well-known Sinkhorn-Knopp algorithm.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 1,
      "context" : "In [2] the authors recommend choosing α = 1 λ log u− 1 Kλ log u >1 so that α is tangent to the simplex.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 19,
      "context" : "RN (H) is the Rademacher complexity [21] measuring the complexity of the hypothesis spaceH.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 16,
      "context" : "In figures 3a-c, h(x), y and M are generated as described in [18] section 5.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "In 3c, convergence is defined as in [18].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 18,
      "context" : "The unregularized Wasserstein distance was computed using FastEMD [20].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 19,
      "context" : "The Rademacher complexity RN (H) for commonly used models like neural networks and kernel machines [21] decays with the training set size.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 20,
      "context" : "The Wasserstein loss is also closely related to the Jaccard index [22], also known as intersectionover-union (IoU), which is a popular measure of performance in segmentation [23].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 21,
      "context" : "We apply the Wasserstein loss to a real world multi-label learning problem, using the recently released Yahoo/Flickr Creative Commons 100M dataset [24].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 22,
      "context" : "We derive a distance metric between tags by using word2vec [25] to embed the tags as unit vectors, then taking their Euclidean distances.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 23,
      "context" : "To extract image features we use MatConvNet [26].",
      "startOffset" : 44,
      "endOffset" : 48
    } ],
    "year" : 2015,
    "abstractText" : "Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe efficient learning algorithms based on this regularization, extending the Wasserstein loss from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss and show connections with the total variation norm and the Jaccard index. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, achieving superior performance over a baseline that doesn’t use the metric.",
    "creator" : "LaTeX with hyperref package"
  }
}