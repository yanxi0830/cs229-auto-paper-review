{
  "name" : "1206.6262.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Scaling Life-long Off-policy Learning",
    "authors" : [ "Adam White", "Joseph Modayil" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Life-long learning is an approach to artificial intelligence based on learning from a long-stream of sensorimotor interaction generated by an agent interacting with its environment, Life-long learning emphasizes continual learning by an autonomous agent over long periods of time, perhaps months or years. This big data problem requires algorithms that scale efficiently to learn a multitude of diverse facts about the large stream of sensorimotor data. We purse a novel\nar X\niv :1\n20 6.\n62 62\nv1 [\ncs .A\nI] 2\n7 Ju\napproach to knowledge acquisition and verification based on algorithms from reinforcement learning and a life-time of sensorimotor interaction of a mobile robot.\nGeneral value functions (GVFs) provide an expressive language for representing sensorimotor knowledge about a long-lived agent’s interaction with the world (Sutton et al. 2011). Knowledge is represented as approximate value functions with a reward function, an outcome function and a pseudo-termination function conditioned on a policy. GVFs provide a semantics for experiential knowledge that is grounded in the sensorimotor data and verifiable by the agent, without human intervention—essential for scalability.\nIn recent computational studies (Modayil, White & Sutton 2011) have shown that predictions represented as GVFs can be learned at a massive scale with a high degree of accuracy. Under on-policy sampling, a mobile robot learned thousands of predictions about future sensor readings and state variables at several time scales operating at a 100 ms time step. These predictions were shown to be accurate compared to the optimal off-line solution.\nPrevious work focused on a limited form of prediction, learning the consequences of a single policy. The greatest potential benefit of life-long learning comes from learning about many policies in parallel, using off-policy learning. Parallel learning introduces a new dimension of scaling not considered in typical sequential life-long learning systems and has yet to be demonstrated at scale and in real time on a robot.\nMany new challenges arise in this off-policy learning setting. Scaling life-long learning in this way requires efficient learning methods that are robust under off-policy sampling. We use recently developed gradient temporal-difference learning method, GTD(λ), with linear function approximation, to learn GVFs on a robot. Gradient TD methods are the only learning methods that scale linearly in the size of the feature set, require constant computation time per step, do not require memory or a forgetting process, and are therefore the only methods available for off-policy life-long learning at scale on a mobile robot. Arguably, the computational constraints of this big-data problem make batch and least-squares approaches inappropriate for life-long learning, while recent work in machine learning has highlighted the value of simple online learning methods for big data problems (see Hsu et al., 2011).\nEvaluating off-policy learning at scale poses an additional challenge: determining prediction accuracy for policies that are never executed by the robot. We first show that our robot can learn hundreds of predictions about several policies, with interspersed on-policy tests. However, these tests require interrupting learning, placing an upper bound on the number of policies the robot can learn about. We propose two efficiently computable, online measures of offpolicy learning progress based on the off-policy objection function (MSPBE). Using these online measures, we demonstrate learning a thousand GVFs about a thousand unique target policies in real time on a robot. Our results represent a significant step towards scaling life-long learning."
    }, {
      "heading" : "2 On-policy and off-policy prediction with value",
      "text" : "functions\nTo begin, we consider how the problem of prediction is conventionally formulated in reinforcement learning. The interaction between an agent and its environment is modelled as a discrete-time dynamical system with function approximation. On each discrete time step t, the agent observes a feature vector φt ∈ Φ = Rn, that partially characterizes the current state of the environment. Note that the agent has no access to the underlying environment state space S, or to the current state st ∈ S; the observed feature vector, φt, is computed from information available to the agent and thus is only implicitly a function of the environmental state, φt = φ(st). At each time step, the agent takes an action at ∈ A, and the environment transitions into a new state producing a new feature vector, φt+1.\nIn conventional reinforcement learning, we seek to predict at each time the total future discounted reward, where reward rt ∈ R is a special signal received from the environment. More formally, we seek to learn a value function V : S → R, conditional on the agent following a particular policy. The time scale of the prediction is controlled by a discount factor γ ∈ [0, 1). With these terms defined, the precise quantity being predicted, called the return gt ∈ R, is\ngt = ∞∑ k=0 γkrt+k+1,\nand the value function is the expected value of the return,\nV (s) = Eπ [ ∞∑ k=0 γkrt+k+1 ∣∣∣st = s] , where the expectation is conditional on the actions (after t) being selected according to a particular policy π : Φ×A → [0, 1]. As is common in reinforcement learning, we estimate V with a linear approximation, Vθ(s) = θ\n>φ(s) ≈ V (s), where θ ∈ Rn.\nIn the most common on-policy setting, the policy that conditions the value function, π, is also the policy used to select actions and generate the training data. In general, however, these two policies may be different. The policy that conditions the value function is called the target policy because it is the target of the learning process, and in this paper we will uniformly denote it as π. The policy that generates actions and behaviour is called the behaviour policy, and in this paper we will denote it as b : Φ×A → [0, 1]. The most common setting, in which the two policies are the same, is called on-policy learning, and the setting in which they are different is called off-policy learning.\nConventional algorithms such as TD(λ) and Q-learning can be applied with function approximation in an on-policy setting, but may become unstable in an off-policy setting. Fewer algorithms work reliably in the off-policy setting. One such algorithm is GTD(λ), a gradient-TD algorithm designed to learn from\noff-policy sampling with function approximation (Maei, 2011). GTD(λ) is an incremental prediction algorithm, similar to TD(λ) (Sutton, 1988), except with an additional secondary set of learned weights w, and an additional step size parameter αw. The algorithm retains the computational advantages of TD(λ): its computational complexity is O(n) per step, and it can operate online and in real time. Unlike TD(λ), GTD(λ) is guaranteed to converge under off-policy sampling and with function approximation (linear and non-linear). The following pseudocode specifies the GTD(λ) algorithm.\nInitialize w0 and e0 to zero and θ0 arbitrarily. for each time step t, given observed sample φt, at, φt+1, and rt+1 do δt ← rt+1 + γθ>t φt+1 − θ>t φt ρt ← π(at|φt)b(at|φt) et ← ρt(φt + γλet−1) θt+1 ← θt + αv(δtet − γ(1− λ)(e>t wt)φt+1) wt+1 ← wt + αw(δtet − (φ>t wt)φt) end for\nThe GTD(λ), algorithm minimizes the λ-weighted mean-square projected Bellman error\nMSPBE(θ,Φ) = ||Vθ −ΠΦTλ,γπ Vθ||2D (1)\nwhere Φ is the matrix of all possible feature vectors φ, ΠΦ is a projection matrix that projects the value function onto the space representable by Φ, Tλ,γπ is the λ-weighted Bellman operator for the target policy π and discount factor γ, and D is a diagonal matrix whose diagonal entries correspond to the state visitation frequency under the behaviour policy b."
    }, {
      "heading" : "3 An architecture for large-scale, real-time off-",
      "text" : "policy learning on robots\nIn addition to learning about multiple policies, our approach is to learn multiple things about each policy. Both of these cases can be captured with the notion of general value functions. We envision an architecture in which many predictive questions are posed and answered in a generalized form of value function. Each such function, denoted vi : S → R, predicts the expected discounted sum of the future readings of some sensor. The ith value function pertains to the sensor readings r (i) t , the policy π (i), and the time scale γ(i):\nvi(s) = Eπ(i) [ ∞∑ k=0 (γ(i))kr (i) t+k+1 ∣∣∣st = s] . Off-policy methods can be used to learn approximate answers v\n(i) θ to predic-\ntive questions in the form of such value functions. Questions about what will happen to robot if it follows a behaviour different from its current behaviour, for\nexample, ‘what would be the effect on the rotational velocity, if my future actions consisted of clockwise rotation’. Policy-contingent questions substantially broaden the knowledge that can be acquired by the system and dramatically increases the scale of learning—millions of distinct predictive questions can be easily constructed from the space of policies, sensors and time scales.\nFigure 1 provides a graphical depiction of this parallel learning architecture. This architecture, called Horde by Sutton et al. (2011), has several desirable characteristics. Horde can run in real time, due to the linear computational complexity of GTD(λ). The architecture is potentially scalable because of the distributed nature of off-policy learning, but no experiments were performed to evaluate its ability to learn at scale in practice. The system is modular: the question specification, behaviour policy and the function approximation architecture are completely independent. As depicted in Figure 1, the predictions can be used as input to the function approximator. This enables the use of predictive state information (Littman et al., 2002) and learning of compositional predictions, similar to a TD network (Sutton and Tanner, 2005)."
    }, {
      "heading" : "4 Large-scale off-policy prediction on a robot",
      "text" : "The first question we consider is whether the Horde architecture supports largescale off-policy prediction in real time on a physical robot. All our evaluations were performed on a custom-built holonomic mobile robot (see Figure 2). The robot has a diverse set of 53 sensors for detecting external entities (ambient light, heat, infrared light, magnetic fields, and infrared reflectance) and also its internal status (battery voltages, acceleration, rotational velocity, motor velocities, motor currents, motor temperatures, and motor voltages). The robot can dock autonomously with its charging station and can run continually for twelve\nhours without recharging.\nThe raw sensorimotor vector was transformed into features, φt, by tile coding. This produced a binary vector, φt ∈ {0, 1}n, with a constant number of 1 features. The tile coder was comprised of many overlapping tilings of single sensors and pairs of sensors. The tile coding scheme produced a sparse feature vector with k = 6065 components with 457 features that were ones, including one bias feature whose value was always 1. More details of the feature representation are given in previous work (Modayil, White & Sutton 2011).\nConducting a fair evaluation presents a challenge for off-policy predictions on a robot; the most direct way to evaluate a prediction about a policy is to follow that policy for a period of time and measure the return. This direct on-policy test excursion is interspersed into a baseline off-policy behaviour used for learning. We followed this procedure to evaluate the predictions learned off-policy on the robot, with learning updates being suspended during the test excursions.\nTo generate behaviour data, the robot was confined to a small two meter square pen, executing one of five actions: A = {forward, reverse, rotate clockwise, rotate counter clockwise, stop}. A new action was selected for execution every 100ms. For the baseline learning behaviour, at each time step a random action was selected with a probability of 0.5, otherwise the last executed action was repeated. Normal execution was interrupted probabilistically to run a test excursion; on average an interruption occurred every 5 seconds. A test excursion consisted of selecting one of five constant action policies and following it for five seconds. After a test excursion was completed, the robot spent 2 seconds moving to the centre of the pen and then continued its random behaviour policy. The robot ran for 7.3 hours, visiting all portions of the pen many times. This produced 261,681 samples with half of the time spent on test excursions.\nWe used Horde to learn answers to 795 predictive questions from the experience generated by the behaviour described above. Each question vi, was formed by combining a γ(i) ∈ {0.0, 0.5, 0.8}, a constant action policy π(i) from {π(·, forward) = 1, π(·, reverse) = 1, . . . , π(·, stop) = 1}, and a prediction tar-\nget r(i) from one of the 53 sensors. Each question was of the form: at the current time t, what will be the expected discounted sum of the future values of r(i) if the robot follows π(i), with a constant pseudo-termination probability of 1 − γ(i)?. To ease comparison of the predictions across sensors with different output ranges, the values from each sensor were scaled to the maximum and minimum values in their specifications, so that the observed sensor values were bounded between [0,1]. Each time-step resulted in updates to exactly 159 GTD(λ) learners in parallel (corresponding to the policies that matched the action selected by the behaviour policy). Each question used identical learning parameters: αθ = 0.1/457 (457 is the number of active features), αw = 0.001αθ, and λ = 0.9.\nThe total computation time for a cycle under our conditions was 45ms, well within the 100ms duty cycle of the robot. The entire architecture was run on a 2.4GHz dual-core laptop with 4GB of RAM connected to the robot by a dedicated wireless link.\nWith the architecture in place to update many off-policy predictions in real time on a robot, we the evaluated on-policy test performance. More precisely, on each test execution, for each of the 159 questions pertaining to the selected test policy, we compared the prediction at the beginning of the test, v̂i(φt), with the truncated sample return gathered during the test excursion:\nĝit = 50∑ k=0 (γ(i))kr (i) t+k+1.\nFinally, each prediction error was normalized by the sample variance of each ĝit over all starting configurations observed (computed off-line), yielding a normalized mean squared return error (NMSRE)1 :\nNMSREit = (v̂ i − ĝi)2/Var[ĝi]. (2)\nThe NMSRE represents the percentage of the variance in the returns that remains unexplained by the predictor. For the questions whose sample returns are constant and thus have a zero sample variance, we define the NMSRE to be one.\nFigure 3 illustrates our main result: accurate off-policy predictions can be learned, in real time, on a physical robot at scale. These predictions were learned from a randomized behaviour policy with a shared feature representation using identical parallel instances of GTD(λ). Note that no question-specific tuning of learning parameters or features was needed. Another significant result is that no divergence was observed for any question. Note the average of the NMSRE for all the questions finished below 1.0: a substantial portion of the variance in the returns is being explained by the predictions. The learning parameters were important—divergence was observed on earlier runs with more aggressive settings (both larger values of αθ and smaller values of αw), thus demonstrating that convergence in this off-policy setting is not trivial."
    }, {
      "heading" : "5 An online measure of off-policy learning",
      "text" : "progress\nThe accuracy of the predictions learned in the previous experiment was evaluated with the return error observed during on-policy test excursions. These tests consume considerable wall-clock time, because for each sample the system must follow the target policy long enough to capture most of the probability mass of the infinite sample return and multiple samples are required to estimate the NMSRE. Interspersing on-policy tests to evaluate learning progress places a low limit on both the number of target policies and on the time-scale given by γ.\nThere are other subtle deficiencies with on-policy tests. The experimenter must choose a testing regime and frequency. Depending on how often tests are executed, there is a trade-off for how often the NMSRE is updated. Changes in the environment and novel robot experiences can cause inaccurate NMSRE estimates if the majority of time-steps are used for training. Testing with greater frequency ensures the estimated NMSRE closely matches current prediction quality, but slows learning.\nIn the function approximation setting, we propose instead to estimate the MSPBE. The GTD(λ) algorithm does not minimize the NMSRE, which mea-\n1 We use x to denote the exponential trace (also called the exponentially weighted average) of samples of xt; this is computed for a fixed time constant τ by x ≡ trace(x, t) = 1τ xt + (1 − 1 τ )trace(x, t− 1).\nsures prediction accuracy relative to sample returns, ignoring function approximation error. For an arbitrary question the NMSRE will never go to zero, though it does provide an indication of the quality of the feature representation. The GTD(λ) algorithm instead minimizes the MSPBE. Under some common technical assumptions, the MSPBE will converge to a zero error. The MSPBE can be estimated in real time during learning, and it provides an up-to-date measure of performance without sacrificing valuable robot data for evaluation.\nUsing the derivation given by Sutton et al. (2009), we can rewrite this error in terms of expectations:\nMSPBE(θ) = ||vθ −ΠTvθ||2B (3) = ||Π(vθ − Tvθ)||2B (4) = (Π(vθ − Tvθ))>B(Π(vθ − Tvθ)) (5) = (vθ − Tvθ)>Π>BΠ(vθ − Tvθ) (6) = (vθ − Tvθ)>B>Φ(Φ>BΦ)−1Φ>B(vθ − Tvθ) (7) = (Φ>B(Tvθ − vθ))>(Φ>BΦ)−1Φ>B(Tvθ − vθ) (8) = Eb[δφ]>Eb[φφ>]−1Eb[δφ] (9)\nThe GTD(λ) algorithm uses a second set of modifiable weights, w, to form a quasi-stationary estimate of the last two terms, namely the product of the inverse feature covariance matrix with the expected TD-update. This leads to the following linear-complexity approximation of the MSPBE:\nMSPBE(θ) ≈ (Eb[δφ])>w. (10)\nThe expected TD-update term, Eb[δφ], can be approximated with samples of δtet (switching from the forward view to backward view), where et is the eligibility trace vector. Additionally, the prediction error can be non-zero on samples where the target policy does not agree with the behaviour policy, π(i)(φt, a) 6= b(φt, a). The importance sampling ratio, π (i)(φ,a) b(φ,a) , can be used to account for these effects. This leads to two natural incremental algorithms for sampling the current MSPBE:\nMSPBEt,vector = δe > wt, (11)\nand MSPBEt,scalar = δe>w. (12)\nHere, the exponential traces for both MSPBEt,vector and MSPBEt,scalar are updated on each time step proportionally to π (i)(φ,a) b(φ,a) . The first measure is a more accurate approximation of Equation 10, but the second requires only storing a single real-valued scalar.\nThe first step in evaluating our online estimates of the MSPBE, is to compare them with the exact values of the MSPBE on a simulation domain. We used a simple 7 state Markov chain with an absorbing state on each end, deterministic\ntransitions and episodes beginning in the middle of the chain. Transitioning into the right-side terminal state produced a reward of 1.0, all other transitions incurred 0 reward. We used the inverted feature representation of Sutton et al (2009).\nTo determine the validity of our new measures we compared the vector and scalar MSPBE estimates with the true MSPBE (Equation 4) and the expensive sample estimate of the MSPBE (Equation 9). The computation of the true MSPBE requires complete knowledge of the chain MDP. The sample MSPBE requires an incremental estimate of the expected feature covariance matrix and a inverse operation. The sample MSPBE represents the best possible samplebased estimate of the MSPBE; our online measures can not be expected to track the true MSPBE better than the sample MSPBE. In this experiment we used a single instance of GTD with αθ = 0.05, αw = 0.1 and λ = 0.0 and results were averaged over 100 independent runs. The target policy selected the move-right action with probability 0.95 and a behaviour policy that selected move-right with probability 0.2.\nFigure 4 (left) illustrates the results of the chain experiment comparison: both our online measures provide an accurate estimate of the MSPBE on a simple chain domain. Figure 4 (right) compares the effect of a change in the world during learning. After 1000 episodes the θ weights were set to random values in [0, 1]. The secondary weights w, and traces used in computing the online estimates were not reset. The results depicted in Figure 4 (right) illustrate that the online estimates, sample MSPBE and the true MSPBE all react similarly to the change.\nTo evaluate the online MSPBE estimates on the robot, we compare aggregate error curves, averaged over all questions, on tasks where the robot experiences a\nsignificant change, similarly to the chain experiment. The robot was run exactly as before, with a subset of the predictions learned (γ = 0.8), for six hours. This time, the learned weight vector of each question θ(i), was set to zero after 40000 time steps. This change effectively reinitializes each question and effects the accuracy of all the predictions. In this experiment, we recorded the NMSRE, MSPBEt,vector and MSPBEt,scalar on every time step for 265 questions, except during test excursions. Note that the NMSRE is only updated after a test completes, while the MSPBE measures are updated on every non-test timestep.\nFigure 5 compares the convergence profile and reaction to change of the three error measures in terms of training time. As in the chain experiments, all three measures react quickly to the change. Note that both MSPBE estimates are initially at zero, as the vector w takes time to adapt to a useful value. Finally, note that the MSPBEt,vector and MSPBEt,scalar exhibit very similar trends, indicating that the Bellman error can be estimated with minimal storage requirements."
    }, {
      "heading" : "6 Large-scale off-policy prediction, with many",
      "text" : "target policies\nFree from the limitations of physically performing test excursions to evaluate predictions, we can learn about a much larger set of questions. In this section, we consider scaling the number of target policies and prediction time scales (magnitude of γ).\nTo increase the space of target policies, and still maintain a small set of finite actions, we consider discrete-action linearly parametrized Gibbs policy distributions:\nπu(a) = exp(−u>Ψa)∑\na′∈A exp(−u>Ψa′)\nwhere u is a vector of policy parameters. The feature vector for each action, Ψa ∈ Rn|A|, has a copy of φt as a subvector in an otherwise zero vector; and for each action the copy is offset by n so that a 6= a′ =⇒ Ψ>a Ψa′ = 0. Random policies are generated by selecting 60 components of u at random and assigning each a value independently drawn from the uniform distribution over [0, 1].\nIn this final experiment, we tested how well our architecture scales in the number of target policies. The robot’s behaviour was the same as before, but now learning was enabled on every step of the 7 hours experience. The questions were formed by sampling γ values from {0.0, 0.5, 0.8, 0.95}, reward from the full set of sensors with 1000 randomly generated policies. The value of γ = 0.95 corresponds to a 2 second prediction and would require over 30 seconds to accurately evaluate using the NMSRE. The 1000 questions, evaluated according to MSPBEt,scalar, were learned with a cycle time of 85ms on a 4-core desktop computer with 16 G of Ram; satisfying our real-time requirement of 100ms.\nFigure 6 presents the results of this experiment, namely that learning the temporally-extended consequences of many (untestibly many) different behaviours is possible in real time. The learning progress is measured by the MSPBE, which by the results in the previous section will be strongly coupled to on-policy prediction errors. Note that the ability to monitor learning progress across so many different behaviours is only possible due to the availability of the MSPBE. By acquiring many predictions about many different courses of behaviour, the robot can acquire detailed partial models of the dynamics of its environmental interaction."
    }, {
      "heading" : "7 Related Work",
      "text" : "Many of the ideas in this paper have precursors in the literature. The idea of policy-contingent predictions was developed along with the options framework for temporal abstraction (Sutton, Precup & Singh, 1999). Learning off-policy under function approximation was developed by importance sampling (Precup et al., 2006) in an approach that runs online, but can exhibit exponentially slow learning progress. Learning about many different policies can also support active\nexploration, an idea that has been explored in related work on curiosity-based learning (Singh et al., 2005).\nThe idea of building models from data has been explored, but not in the off-policy real-time setting. Thrun and Mitchell (1995) showed learning of sensor models offline. Many Bayesian approaches for online state estimation in robotics (e.g., Thrun et al., 2005) process a vast volume of observations in real time, but they do not learn system dynamics online. A Kalman filter (Kalman, 1965) can be viewed as learning an adaptive model of dynamics online, but it is only appropriate for small models with well-understood dynamics. Atkeson and Schaal (1997) showed learning of small one-time-step models. Kober and Peters (2011) showed on-policy episodic learning on a robot.\nA recent online spectral approach (Boots, Siddiqi & Gordon, 2011) finds small predictive state models for robotics. They use a sophisticated incremental method for making multiple action-conditional predictions, but it is unclear if this approach can operate under real-time constraints. The previous work that introduced the Horde architecture (Sutton et al., 2011) also demonstrated parallel off-policy learning on a robot. Their work was suggestive but did not demonstrate that the approach scaled in practice. Each experiment required a unique parameter set, function approximation scheme, and behaviour policy; this overhead is not practical for learning thousands of predictions. This paper shows that a large set of diverse, policy-contingent predictions can be learned using a shared feature set, common learning parameters, and data generated by a single random behaviour policy."
    }, {
      "heading" : "8 Conclusions and future work",
      "text" : "We provided the first demonstrations of large scale off-policy learning on a robot. We have shown that gradient TD methods can be used to learn hundreds of temporally-extended policy-contingent predictions from off-policy sampling. To achieve this goal required resolution of several challenges unique to the offpolicy setting. Most significantly we have developed on online estimate of offpolicy learning progress based on the Bellman error that does not increase the computational complexity of the horde architecture, can be sampled without interrupting learning and has good correspondence with the traditional mean squared prediction error. The addition of policy contingent, what-if, questions dramatically increases the scope and scale of questions that can be learned by horde providing further evidence of the significance of horde for life-long learning.\nOur experiments, while on a robot, are limited but there are several immediate directions for future work. The questions learned here are predictive questions about a policy, general value functions can also support learning control policies using greedy-gq (Maei 2011) a control variant of GTD(λ) with the same linear complexity. Predictive accuracy improvements should be achieved by employing adaptive behaviour policies (e.g., curiosity) and more powerful function approximators, while significantly increased scaling (more predictions and larger feature vectors) can be obtained with more computational resources."
    }, {
      "heading" : "9 References",
      "text" : "Atkeson, C. G., Schaal, S. (1997). Robot learning from demonstration. In Proc. 14th Int. Conf. on Machine Learning, pp. 12–20.\nBaird, L. C. (1995). Residual algorithms: Reinforcement learning with function approximation. In Proc. 12th Int. Conf. on Machine Learning, pp. 30–37.\nBoots, B., Siddiqi, S., Gordon, G. (2011). An online spectral learning algorithm for partially observable nonlinear dynamical systems. In Proc. Conf. of the Association for the Advancement of Artificial Intelligence.\nHsu, D., Karampatziakis, N., Langford, J., Smola, A. J. (2011). Parallel Online Learning. In The Computing Research Repository.\nKalman, R. E. (1960). A new approach to linear filtering and prediction problems. Trans. ASME, Journal of Basic Engineering 82:35–45.\nKober, J., Peters, J. (2011). Policy search for motor primitives in robotics. Machine Learning 84:171–203.\nKolter, J. Z. (2011). The fixed points of off-policy TD. In Advances in Neural Information Processing Systems 24.\nLittman, M. L., Sutton, R. S., Singh, S. (2002). Predictive representations of state. In Advances in Neural Information Processing Systems 14, pp. 1555– 1561.\nMaei, H. R. (2011). Gradient Temporal-Difference Learning Algorithms. PhD\nthesis, University of Alberta.\nModayil, J., White, A., Sutton, R. S. (2012). Multi-timescale Nexting in a Reinforcement Learning Robot. Proc. 12th Int. Conf. on Adaptive Behaviour.\nPrecup, D., Sutton, R. S., Paduraru, C., Koop, A., Singh, S. (2006). Off-policy learning with recognizers. In Advances in Neural Information Processing Systems 18.\nQuadrianto, N., Smola, A., Caetano, T., Vishwanathan, S. V. N., Petterson, J. (2010). Multitask learning without label correspondences. In Advances in Neural Information Processing Systems 23, pp. 1957–1965.\nSingh S., Barto, A. G., Chentanez, N. (2005). Intrinsically motivated reinforcement learning. In Advances in Neural Information Processing Systems 17, pp. 1281–1288.\nSutton, R. S. (1988). Learning to predict by the method of temporal differences. Machine Learning 3:9–44.\nSutton, R. S., Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.\nSutton, R. S., Precup D., Singh, S. (1999). Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence 112:181–211.\nSutton, R. S., Tanner, B. (2005). Temporal-difference networks. In Advances in Neural Information Processing Systems 17, pp. 1377–1384.\nSutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesvári, Cs., Wiewiora, E. (2009). Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Proc. 26th Int. Conf. on Machine Learning.\nSutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., Precup, D. (2011). Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. Proc. 10th Int. Conf. on Autonomous Agents and Multiagent Systems.\nTalvitie, E., Singh, S. (2011). Learning to make predictions in partially observable environments without a generative model. Journal of Artificial Intelligence Research 42:353–392.\nThrun, S., Burgard, W., Fox, D. (2005). Probabilistic Robotics. MIT Press.\nThrun, S., Mitchell, T. (1995). Lifelong robot learning. Robotics and Autonomous Systems."
    } ],
    "references" : [ {
      "title" : "Robot learning from demonstration",
      "author" : [ "C.G. Atkeson", "S. Schaal" ],
      "venue" : "Proc. 14th Int. Conf. on Machine Learning, pp. 12–20.",
      "citeRegEx" : "Atkeson and Schaal,? 1997",
      "shortCiteRegEx" : "Atkeson and Schaal",
      "year" : 1997
    }, {
      "title" : "Residual algorithms: Reinforcement learning with function approximation",
      "author" : [ "L.C. Baird" ],
      "venue" : "Proc. 12th Int. Conf. on Machine Learning, pp. 30–37.",
      "citeRegEx" : "Baird,? 1995",
      "shortCiteRegEx" : "Baird",
      "year" : 1995
    }, {
      "title" : "An online spectral learning algorithm for partially observable nonlinear dynamical systems",
      "author" : [ "B. Boots", "S. Siddiqi", "G. Gordon" ],
      "venue" : "Proc. Conf. of the Association for the Advancement of Artificial Intelligence.",
      "citeRegEx" : "Boots et al\\.,? 2011",
      "shortCiteRegEx" : "Boots et al\\.",
      "year" : 2011
    }, {
      "title" : "Parallel Online Learning",
      "author" : [ "D. Hsu", "N. Karampatziakis", "J. Langford", "A.J. Smola" ],
      "venue" : "The Computing Research Repository.",
      "citeRegEx" : "Hsu et al\\.,? 2011",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2011
    }, {
      "title" : "A new approach to linear filtering and prediction problems",
      "author" : [ "R.E. Kalman" ],
      "venue" : "Trans. ASME, Journal of Basic Engineering 82:35–45.",
      "citeRegEx" : "Kalman,? 1960",
      "shortCiteRegEx" : "Kalman",
      "year" : 1960
    }, {
      "title" : "Policy search for motor primitives in robotics",
      "author" : [ "J. Kober", "J. Peters" ],
      "venue" : "Machine Learning 84:171–203.",
      "citeRegEx" : "Kober and Peters,? 2011",
      "shortCiteRegEx" : "Kober and Peters",
      "year" : 2011
    }, {
      "title" : "The fixed points of off-policy TD",
      "author" : [ "J.Z. Kolter" ],
      "venue" : "Advances in Neural Information Processing Systems 24.",
      "citeRegEx" : "Kolter,? 2011",
      "shortCiteRegEx" : "Kolter",
      "year" : 2011
    }, {
      "title" : "Predictive representations of state",
      "author" : [ "M.L. Littman", "R.S. Sutton", "S. Singh" ],
      "venue" : "Advances in Neural Information Processing Systems 14, pp. 1555– 1561.",
      "citeRegEx" : "Littman et al\\.,? 2002",
      "shortCiteRegEx" : "Littman et al\\.",
      "year" : 2002
    }, {
      "title" : "Gradient Temporal-Difference Learning Algorithms",
      "author" : [ "H.R. Maei" ],
      "venue" : "PhD",
      "citeRegEx" : "Maei,? 2011",
      "shortCiteRegEx" : "Maei",
      "year" : 2011
    }, {
      "title" : "Multi-timescale Nexting in a Reinforcement Learning Robot",
      "author" : [ "J. Modayil", "A. White", "R.S. Sutton" ],
      "venue" : "Proc. 12th Int. Conf. on Adaptive Behaviour.",
      "citeRegEx" : "Modayil et al\\.,? 2012",
      "shortCiteRegEx" : "Modayil et al\\.",
      "year" : 2012
    }, {
      "title" : "Off-policy learning with recognizers",
      "author" : [ "D. Precup", "R.S. Sutton", "C. Paduraru", "A. Koop", "S. Singh" ],
      "venue" : "Advances in Neural Information Processing Systems 18.",
      "citeRegEx" : "Precup et al\\.,? 2006",
      "shortCiteRegEx" : "Precup et al\\.",
      "year" : 2006
    }, {
      "title" : "Multitask learning without label correspondences",
      "author" : [ "N. Quadrianto", "A. Smola", "T. Caetano", "S.V.N. Vishwanathan", "J. Petterson" ],
      "venue" : "Advances in Neural Information Processing Systems 23, pp. 1957–1965.",
      "citeRegEx" : "Quadrianto et al\\.,? 2010",
      "shortCiteRegEx" : "Quadrianto et al\\.",
      "year" : 2010
    }, {
      "title" : "Intrinsically motivated reinforcement learning",
      "author" : [ "Singh S.", "A.G. Barto", "N. Chentanez" ],
      "venue" : "Advances in Neural Information Processing Systems 17, pp. 1281–1288.",
      "citeRegEx" : "S. et al\\.,? 2005",
      "shortCiteRegEx" : "S. et al\\.",
      "year" : 2005
    }, {
      "title" : "Learning to predict by the method of temporal differences",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine Learning 3:9–44.",
      "citeRegEx" : "Sutton,? 1988",
      "shortCiteRegEx" : "Sutton",
      "year" : 1988
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Sutton and Barto,? 1998",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "R.S. Sutton", "Precup D.", "S. Singh" ],
      "venue" : "Artificial Intelligence 112:181–211.",
      "citeRegEx" : "Sutton et al\\.,? 1999",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Temporal-difference networks",
      "author" : [ "R.S. Sutton", "B. Tanner" ],
      "venue" : "Advances in Neural Information Processing Systems 17, pp. 1377–1384.",
      "citeRegEx" : "Sutton and Tanner,? 2005",
      "shortCiteRegEx" : "Sutton and Tanner",
      "year" : 2005
    }, {
      "title" : "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
      "author" : [ "R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "Szepesvári", "Cs.", "E. Wiewiora" ],
      "venue" : "Proc. 26th Int. Conf. on Machine Learning.",
      "citeRegEx" : "Sutton et al\\.,? 2009",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2009
    }, {
      "title" : "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
      "author" : [ "R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup" ],
      "venue" : "Proc. 10th Int. Conf. on Autonomous Agents and Multiagent Systems.",
      "citeRegEx" : "Sutton et al\\.,? 2011",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning to make predictions in partially observable environments without a generative model",
      "author" : [ "E. Talvitie", "S. Singh" ],
      "venue" : "Journal of Artificial Intelligence Research 42:353–392.",
      "citeRegEx" : "Talvitie and Singh,? 2011",
      "shortCiteRegEx" : "Talvitie and Singh",
      "year" : 2011
    }, {
      "title" : "Probabilistic Robotics",
      "author" : [ "S. Thrun", "W. Burgard", "D. Fox" ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Thrun et al\\.,? 2005",
      "shortCiteRegEx" : "Thrun et al\\.",
      "year" : 2005
    }, {
      "title" : "Lifelong robot learning",
      "author" : [ "S. Thrun", "T. Mitchell" ],
      "venue" : "Robotics and Autonomous Systems.",
      "citeRegEx" : "Thrun and Mitchell,? 1995",
      "shortCiteRegEx" : "Thrun and Mitchell",
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "GVFs have been shown able to represent a wide variety of facts about the world’s dynamics that may be useful to a long-lived agent (Sutton et al. 2011).",
      "startOffset" : 131,
      "endOffset" : 151
    }, {
      "referenceID" : 18,
      "context" : "General value functions (GVFs) provide an expressive language for representing sensorimotor knowledge about a long-lived agent’s interaction with the world (Sutton et al. 2011).",
      "startOffset" : 156,
      "endOffset" : 176
    }, {
      "referenceID" : 8,
      "context" : "off-policy sampling with function approximation (Maei, 2011).",
      "startOffset" : 48,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "GTD(λ) is an incremental prediction algorithm, similar to TD(λ) (Sutton, 1988), except with an additional secondary set of learned weights w, and an additional step size parameter αw.",
      "startOffset" : 64,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "This enables the use of predictive state information (Littman et al., 2002) and learning of compositional predictions, similar to a TD network (Sutton and Tanner, 2005).",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : ", 2002) and learning of compositional predictions, similar to a TD network (Sutton and Tanner, 2005).",
      "startOffset" : 75,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "This architecture, called Horde by Sutton et al. (2011), has several desirable characteristics.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 13,
      "context" : "Using the derivation given by Sutton et al. (2009), we can rewrite this error in terms of expectations:",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "We used the inverted feature representation of Sutton et al (2009). To determine the validity of our new measures we compared the vector and scalar MSPBE estimates with the true MSPBE (Equation 4) and the expensive sample estimate of the MSPBE (Equation 9).",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 10,
      "context" : "Learning off-policy under function approximation was developed by importance sampling (Precup et al., 2006) in an approach that runs online, but can exhibit exponentially slow learning progress.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 18,
      "context" : "The previous work that introduced the Horde architecture (Sutton et al., 2011) also demonstrated parallel off-policy learning on a robot.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "Thrun and Mitchell (1995) showed learning of sensor models offline.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "Atkeson and Schaal (1997) showed learning of small one-time-step models.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "Atkeson and Schaal (1997) showed learning of small one-time-step models. Kober and Peters (2011) showed on-policy episodic learning on a robot.",
      "startOffset" : 0,
      "endOffset" : 97
    }, {
      "referenceID" : 8,
      "context" : "The questions learned here are predictive questions about a policy, general value functions can also support learning control policies using greedy-gq (Maei 2011) a control variant of GTD(λ) with the same linear complexity.",
      "startOffset" : 151,
      "endOffset" : 162
    } ],
    "year" : 2012,
    "abstractText" : "We pursue a life-long learning approach to artificial intelligence that makes extensive use of reinforcement learning algorithms. We build on our prior work with general value functions (GVFs) and the Horde architecture. GVFs have been shown able to represent a wide variety of facts about the world’s dynamics that may be useful to a long-lived agent (Sutton et al. 2011). We have also previously shown scaling—that thousands of on-policy GVFs can be learned accurately in real-time on a mobile robot (Modayil, White & Sutton 2011). That work was limited in that it learned about only one policy at a time, whereas the greatest potential benefits of life-long learning come from learning about many policies in parallel, as we explore in this paper. Many new challenges arise in this off-policy learning setting. To deal with convergence and efficiency challenges, we utilize the recently introduced GTD(λ) algorithm. We show that GTD(λ) with tile coding can simultaneously learn hundreds of predictions for five simple target policies while following a single random behavior policy, assessing accuracy with interspersed on-policy tests. To escape the need for the tests, which preclude further scaling, we introduce and empirically validate two online estimators of the off-policy objective (MSPBE). Finally, we use the more efficient of the two estimators to demonstrate off-policy learning at scale—the learning of value functions for one thousand policies in real time on a physical robot. This ability constitutes a significant step towards scaling life-long off-policy learning.",
    "creator" : "LaTeX with hyperref package"
  }
}