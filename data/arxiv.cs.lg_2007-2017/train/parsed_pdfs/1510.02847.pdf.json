{
  "name" : "1510.02847.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Active Learning from Weak and Strong Labelers",
    "authors" : [ "Chicheng Zhang" ],
    "emails" : [ "chz038@eng.ucsd.edu", "kamalika@cs.ucsd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 0.\n02 84\n7v 2\n[ cs\n.L G\n] 1\n6 O\nThis work addresses active learning with labels obtained from strong and weak labelers, where in addition to the standard active learning setting, we have an extra weak labeler which may occasionally provide incorrect labels. An example is learning to classify medical images where either expensive labels may be obtained from a physician (oracle or strong labeler), or cheaper but occasionally incorrect labels may be obtained from a medical resident (weak labeler). Our goal is to learn a classifier with low error on data labeled by the oracle, while using the weak labeler to reduce the number of label queries made to this labeler. We provide an active learning algorithm for this setting, establish its statistical consistency, and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone."
    }, {
      "heading" : "1 Introduction",
      "text" : "An active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively make label queries to an oracle on a subset of these examples; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as few oracle queries as possible.\nAs labeling examples is a tedious task for any one person, many applications of active learning involve synthesizing labels from multiple experts who may have slightly different labeling patterns. While a body of recent empirical work [28, 29, 30, 26, 27, 12] has developed methods for combining labels from multiple experts, little is known on the theory of actively learning with labels from multiple annotators. For example, what kind of assumptions are needed for methods that use labels from multiple sources to work, when these methods are statistically consistent, and when they can yield benefits over plain active learning are all open questions.\nThis work addresses these questions in the context of active learning from strong and weak labelers. Specifically, in addition to unlabeled data and the usual labeling oracle in standard active learning, we have an extra weak labeler. The labeling oracle is a gold standard – an expert on the problem domain – and it provides high quality but expensive labels. The weak labeler is cheap, but may provide incorrect labels\n∗chz038@eng.ucsd.edu †kamalika@cs.ucsd.edu\non some inputs. An example is learning to classify medical images where either expensive labels may be obtained from a physician (oracle), or cheaper but occasionally incorrect labels may be obtained from a medical resident (weak labeler). Our goal is to learn a classifier in a hypothesis class whose error with respect to the data labeled by the oracle is low, while exploiting the weak labeler to reduce the number of queries made to this oracle. Observe that in our model the weak labeler can be incorrect anywhere, and does not necessarily provide uniformly noisy labels everywhere, as was assumed by some previous works [8, 24].\nA plausible approach in this framework is to learn a difference classifier to predict where the weak labeler differs from the oracle, and then use a standard active learning algorithm which queries the weak labeler when this difference classifier predicts agreement. Our first key observation is that this approach is statistically inconsistent; false negative errors (that predict no difference when O and W differ) lead to biased annotation for the target classification task. We address this problem by learning instead a costsensitive difference classifier that ensures that false negative errors rarely occur. Our second key observation is that as existing active learning algorithms usually query labels in localized regions of space, it is sufficient to train the difference classifier restricted to this region and still maintain consistency. This process leads to significant label savings. Combining these two ideas, we get an algorithm that is provably statistically consistent and that works under the assumption that there is a good difference classifier with low false negative error.\nWe analyze the label complexity of our algorithm as measured by the number of label requests to the labeling oracle. In general we cannot expect any consistent algorithm to provide label savings under all circumstances, and indeed our worst case asymptotic label complexity is the same as that of active learning using the oracle alone. Our analysis characterizes when we can achieve label savings, and we show that this happens for example if the weak labeler agrees with the labeling oracle for some fraction of the examples close to the decision boundary. Moreover, when the target classification task is agnostic, the number of labels required to learn the difference classifier is of a lower order than the number of labels required for active learning; thus in realistic cases, learning the difference classifier adds only a small overhead to the total label requirement, and overall we get label savings over using the oracle alone.\nRelated Work. There has been a considerable amount of empirical work on active learning where multiple annotators can provide labels for the unlabeled examples. One line of work assumes a generative model for each annotator’s labels. The learning algorithm learns the parameters of the individual labelers, and uses them to decide which labeler to query for each example. [29, 30, 13] consider separate logistic regression models for each annotator, while [20, 19] assume that each annotator’s labels are corrupted with a different amount of random classification noise. A second line of work [12, 16] that includes Pro-Active Learning, assumes that each labeler is an expert over an unknown subset of categories, and uses data to measure the class-wise expertise in order to optimally place label queries. In general, it is not known under what conditions these algorithms are statistically consistent, particularly when the modeling assumptions do not strictly hold, and under what conditions they provide label savings over regular active learning.\n[25], the first theoretical work to consider this problem, consider a model where the weak labeler is more likely to provide incorrect labels in heterogeneous regions of space where similar examples have different labels. Their formalization is orthogonal to ours – while theirs is more natural in a non-parametric setting, ours is more natural for fitting classifiers in a hypothesis class. In a NIPS 2014 Workshop paper, [21] have also considered learning from strong and weak labelers; unlike ours, their work is in the online selective sampling setting, and applies only to linear classifiers and robust regression. [11] study learning from multiple teachers in the online selective sampling setting in a model where different labelers have different regions of expertise.\nFinally, there is a large body of theoretical work [1, 9, 10, 14, 31, 2, 4] on learning a binary classifier based on interactive label queries made to a single labeler. In the realizable case, [22, 9] show that a generalization of binary search provides an exponential improvement in label complexity over passive learning. The problem is more challenging, however, in the more realistic agnostic case, where such approaches lead to inconsistency. The two styles of algorithms for agnostic active learning are disagreement-based active learning (DBAL) [1, 10, 14, 4] and the more recent margin-based or confidence-based active learning [2, 31]. Our algorithm builds on recent work in DBAL [4, 15]."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "The Model. We begin with a general framework for actively learning from weak and strong labelers. In the standard active learning setting, we are given unlabelled data drawn from a distribution U over an input space X , a label space Y = {−1,1}, a hypothesis class H , and a labeling oracle O to which we can make interactive queries.\nIn our setting, we additionally have access to a weak labeling oracle W which we can query interactively. Querying W is significantly cheaper than querying O; however, querying W generates a label yW drawn from a conditional distribution PW (yW |x) which is not the same as the conditional distribution PO(yO|x) of O.\nLet D be the data distribution over labelled examples such that: PD(x,y) = PU(x)PO(y|x). Our goal is to learn a classifier h in the hypothesis class H such that with probability ≥ 1−δ over the sample, we have: PD(h(x) 6= y)≤ minh′∈H PD(h′(x) 6= y)+ ε , while making as few (interactive) queries to O as possible.\nObserve that in this model W may disagree with the oracle O anywhere in the input space; this is unlike previous frameworks [8, 24] where labels assigned by the weak labeler are corrupted by random classification noise with a higher variance than the labeling oracle. We believe this feature makes our model more realistic.\nSecond, unlike [25], mistakes made by the weak labeler do not have to be close to the decision boundary. This keeps the model general and simple, and allows greater flexibility to weak labelers. Our analysis shows that if W is largely incorrect close to the decision boundary, then our algorithm will automatically make more queries to O in its later stages.\nFinally note that O is allowed to be non-realizable with respect to the target hypothesis class H .\nBackground on Active Learning Algorithms. The standard active learning setting is very similar to ours, the only difference being that we have access to the weak oracle W . There has been a long line of work on active learning [1, 7, 9, 14, 2, 10, 4, 31]. Our algorithms are based on a style called disagreement-based active learning (DBAL). The main idea is as follows. Based on the examples seen so far, the algorithm maintains a candidate set Vt of classifiers in H that is guaranteed with high probability to contain h∗, the classifier in H with the lowest error. Given a randomly drawn unlabeled example xt , if all classifiers in Vt agree on its label, then this label is inferred; observe that with high probability, this inferred label is h∗(xt). Otherwise, xt is said to be in the disagreement region of Vt , and the algorithm queries O for its label. Vt is updated based on xt and its label, and algorithm continues.\nRecent works in DBAL [10, 4] have observed that it is possible to determine if an xt is in the disagreement region of Vt without explicitly maintaining Vt . Instead, a labelled dataset St is maintained; the labels of the examples in St are obtained by either querying the oracle or direct inference. To determine whether an xt lies in the disagreement region of Vt , two constrained ERM procedures are performed; empirical risk is minimized over St while constraining the classifier to output the label of xt as 1 and −1 respectively. If\nthese two classifiers have similar training errors, then xt lies in the disagreement region of Vt ; otherwise the algorithm infers a label for xt that agrees with the label assigned by h∗.\nMore Definitions and Notation. The error of a classifier h under a labelled data distribution Q is defined as: errQ(h) = P(x,y)∼Q(h(x) 6= y); we use the notation err(h,S) to denote its empirical error on a labelled data set S. We use the notation h∗ to denote the classifier with the lowest error under D and ν to denote its error errD(h∗), where D is the target labelled data distribution.\nOur active learning algorithm implicitly maintains a (1− δ )-confidence set for h∗ throughout the algorithm. Given a set S of labelled examples, a set of classifiers V (S) ⊆ H is said to be a (1− δ )-confidence set for h∗ with respect to S if h∗ ∈V with probability ≥ 1−δ over S.\nThe disagreement between two classifiers h1 and h2 under an unlabelled data distribution U , denoted by ρU(h1,h2), is Px∼U(h1(x) 6= h2(x)). Observe that the disagreements under U form a pseudometric over H . We use BU(h,r) to denote a ball of radius r centered around h in this metric. The disagreement region of a set V of classifiers, denoted by DIS(V ), is the set of all examples x ∈ X such that there exist two classifiers h1 and h2 in V for which h1(x) 6= h2(x)."
    }, {
      "heading" : "3 Algorithm",
      "text" : "Our main algorithm is a standard single-annotator DBAL algorithm with a major modification: when the DBAL algorithm makes a label query, we use an extra sub-routine to decide whether this query should be made to the oracle or the weak labeler, and make it accordingly. How do we make this decision? We try to predict if weak labeler differs from the oracle on this example; if so, query the oracle, otherwise, query the weak labeler.\nKey Idea 1: Cost Sensitive Difference Classifier. How do we predict if the weak labeler differs from the oracle? A plausible approach is to learn a difference classifier hd f in a hypothesis class H d f to determine if there is a difference. Our first key observation is when the region where O and W differ cannot be perfectly modeled by H d f , the resulting active learning algorithm is statistically inconsistent. Any false negative errors (that is, incorrectly predicting no difference) made by difference classifier leads to biased annotation for the target classification task, which in turn leads to inconsistency. We address this problem by instead learning a cost-sensitive difference classifier and we assume that a classifier with low false negative error exists in H d f . While training, we constrain the false negative error of the difference classifier to be low, and minimize the number of predicted positives (or disagreements between W and O) subject to this constraint. This ensures that the annotated data used by the active learning algorithm has diminishing bias, thus ensuring consistency.\nKey Idea 2: Localized Difference Classifier Training. Unfortunately, even with cost-sensitive training, directly learning a difference classifier accurately is expensive. If d′ is the VC-dimension of the difference hypothesis class H d f , to learn a target classifier to excess error ε , we need a difference classifier with false negative error O(ε), which, from standard generalization theory, requires Õ(d′/ε) labels [6, 23]! Our second key observation is that we can save on labels by training the difference classifier in a localized manner – because the DBAL algorithm that builds the target classifier only makes label queries in the disagreement region of the current confidence set for h∗. Therefore we train the difference classifier only on this region and still maintain consistency. Additionally this provides label savings because while training the target\nclassifier to excess error ε , we need to train a difference classifier with only Õ(d′φk/ε) labels where φk is the probability mass of this disagreement region. The localized training process leads to an additional technical challenge: as the confidence set for h∗ is updated, its disagreement region changes. We address this through an epoch-based DBAL algorithm, where the confidence set is updated and a fresh difference classifier is trained in each epoch.\nMain Algorithm. Our main algorithm (Algorithm 1) combines these two key ideas, and like [4], implicitly maintains the (1− δ )-confidence set for h∗ by through a labeled dataset Ŝk. In epoch k, the target excess error is εk ≈ 12k , and the goal of Algorithm 1 is to generate a labeled dataset Ŝk that implicitly represents a (1−δk)-confidence set on h∗. Additionally, Ŝk has the property that the empirical risk minimizer over it has excess error ≤ εk.\nA naive way to generate such an Ŝk is by drawing Õ(d/ε2k ) labeled examples, where d is the VC dimension of H . Our goal, however, is to generate Ŝk using a much smaller number of label queries, which is accomplished by Algorithm 3. This is done in two ways. First, like standard DBAL, we infer the label of any x that lies outside the disagreement region of the current confidence set for h∗. Algorithm 4 identifies whether an x lies in this region. Second, for any x in the disagreement region, we determine whether O and W agree on x using a difference classifier; if there is agreement, we query W , else we query O. The difference classifier used to determine agreement is retrained in the beginning of each epoch by Algorithm 2, which ensures that the annotation has low bias.\nThe algorithms use a constrained ERM procedure CONS-LEARN. Given a hypothesis class H , a labeled dataset S and a set of constraining examples C, CONS-LEARNH(C,S) returns a classifier in H that minimizes the empirical error on S subject to h(xi) = yi for each (xi,yi) ∈C.\nIdentifying the Disagreement Region. Algorithm 4 identifies if an unlabeled example x lies in the disagreement region of the current (1− δ )-confidence set for h∗; recall that this confidence set is implicitly maintained through Ŝk. The identification is based on two ERM queries. Let ĥ be the empirical risk minimizer on the current labeled dataset Ŝk−1, and ĥ′ be the empirical risk minimizer on Ŝk−1 under the constraint that ĥ′(x) =−ĥ(x). If the training errors of ĥ and ĥ′ are very different, then, all classifiers with training error close to that of ĥ assign the same label to x, and x lies outside the current disagreement region.\nTraining the Difference Classifier. Algorithm 2 trains a difference classifier on a random set of examples which lies in the disagreement region of the current confidence set for h∗. The training process is costsensitive, and is similar to [17, 18, 6, 23]. A hard bound is imposed on the false-negative error, which translates to a bound on the annotation bias for the target task. The number of positives (i.e., the number of examples where W and O differ) is minimized subject to this constraint; this amounts to (approximately) minimizing the fraction of queries made to O.\nThe number of labeled examples used in training is large enough to ensure false negative error O(εk/φk) over the disagreement region of the current confidence set; here φk is the probability mass of this disagreement region under U . This ensures that the overall annotation bias introduced by this procedure in the target task is at most O(εk). As φk is small and typically diminishes with k, this requires less labels than training the difference classifier globally which would have required Õ(d′/εk) queries to O.\nAlgorithm 1 Active Learning Algorithm from Weak and Strong Labelers 1: Input: Unlabeled distribution U , target excess error ε , confidence δ , labeling oracle O, weak oracle W ,\nhypothesis class H , hypothesis class for difference classifier H d f . 2: Output: Classifier ĥ in H . 3: Initialize: initial error ε0 = 1, confidence δ0 = δ/4. Total number of epochs k0 = ⌈log 1ε ⌉. 4: Initial number of examples n0 = O( 1ε20 (d ln 1ε20 + ln 1δ0 )). 5: Draw a fresh sample and query O for its labels Ŝ0 = {(x1,y1), . . . ,(xn0 ,yn0)}. Let σ0 = σ(n0,δ0). 6: for k = 1,2, . . . ,k0 do 7: Set target excess error εk = 2−k, confidence δk = δ/4(k+1)2. 8: # Train Difference Classifier 9: ĥd fk ← Call Algorithm 2 with inputs unlabeled distribution U , oracles W and O, target excess error\nεk, confidence δk/2, previously labeled dataset Ŝk−1. 10: # Adaptive Active Learning using Difference Classifier 11: σk, Ŝk ← Call Algorithm 3 with inputs unlabeled distribution U , oracles W and O, difference classi-\nfier ĥd fk , target excess error εk, confidence δk/2, previously labeled dataset Ŝk−1. 12: end for 13: return ĥ ← CONS-LEARNH ( /0, Ŝk0 ).\nAlgorithm 2 Training Algorithm for Difference Classifier\n1: Input: Unlabeled distribution U , oracles W and O, target error ε , hypothesis class H d f , confidence δ , previous labeled dataset T̂ . 2: Output: Difference classifier ĥd f . 3: Let p̂ be an estimate of Px∼U(in disagr region(T̂ , 3ε2 ,x) = 1), obtained by calling Algorithm 5 with\nfailure probability δ/3. 1 4: Let U ′ = /0, i = 1, and\nm = 64 ·1024p̂\nε (d′ ln 512 ·1024p̂ ε + ln 72 δ ) (1)\n5: repeat 6: Draw an example xi from U . 7: if in disagr region(T̂ , 3ε2 ,xi) = 1 then # xi is inside the disagreement region 8: query both W and O for labels to get yi,W and yi,O. 9: end if\n10: U ′ =U ′∪{(xi,yi,O,yi,W )} 11: i = i+1 12: until |U ′|= m 13: Learn a classifier ĥd f ∈ H d f based on the following empirical risk minimizer:\nĥd f = argminhd f∈H d f m\n∑ i=1\n1(hd f (xi) = +1), s.t. m\n∑ i=1 1(hd f (xi) =−1∧ yi,O 6= yi,W )≤ mε/256p̂ (2)\n14: return ĥd f .\n1Note that if in Algorithm 5, the upper confidence bound of Px∼U (in disagr region(T̂ , 3ε2 ,x) = 1) is lower than ε/64, then we can halt Algorithm 2 and return an arbitrary hd f in H d f . Using this hd f will still guarantee the correctness of Algorithm 1.\nAdaptive Active Learning using the Difference Classifier. Finally, Algorithm 3 is our main active learning procedure, which generates a labeled dataset Ŝk that is implicitly used to maintain a tighter (1− δ )- confidence set for h∗. Specifically, Algorithm 3 generates a Ŝk such that the set Vk defined as:\nVk = {h : err(h, Ŝk)− min ĥk∈H err(ĥk, Ŝk)≤ 3εk/4}\nhas the property that:\n{h : errD(h)− errD(h∗)≤ εk/2} ⊆Vk ⊆ {h : errD(h)− errD(h∗)≤ εk}\nThis is achieved by labeling, through inference or query, a large enough sample of unlabeled data drawn from U . Labels are obtained from three sources - direct inference (if x lies outside the disagreement region as identified by Algorithm 4), querying O (if the difference classifier predicts a difference), and querying W . How large should the sample be to reach the target excess error? If errD(h∗) = ν , then achieving an excess error of ε requires Õ(dν/ε2k ) samples, where d is the VC dimension of the hypothesis class. As ν is unknown in advance, we use a doubling procedure in lines 4-14 to iteratively determine the sample size.\nAlgorithm 3 Adaptive Active Learning using Difference Classifier\n1: Input: Unlabeled data distribution U , oracles W and O, difference classifier hd f , target excess error ε , confidence δ , previous labeled dataset T̂ . 2: Output: Parameter σ , labeled dataset Ŝ. 3: Let ĥ = CONS-LEARNH ( /0, T̂ ). 4: for t = 1,2, . . . , do 5: Let δ t = δ/t(t +1). Define: σ(2t ,δ t) = 82t (2d ln 2e2t d + ln 24 δ t ). 6: Draw 2t examples from U to form St,U . 7: for each x ∈ St,U do: 8: if in disagr region(T̂ , 3ε2 ,x) = 0 then # x is inside the agreement region 9: Add (x, ĥ(x)) to Ŝt .\n10: else # x is inside the disagreement region 11: If hd f (x) = +1, query O for the label y of x, otherwise query W . Add (x,y) to Ŝt . 12: end if 13: end for 14: Train ĥt ← CONS-LEARNH ( /0, Ŝt). 15: if σ(2t ,δ t)+ √\nσ(2t ,δ t)err(ĥt , Ŝt)≤ ε/512 then 16: t0 ← t, break 17: end if 18: end for 19: return σ ← σ(2t0 ,δ t0), Ŝ ← Ŝt0 ."
    }, {
      "heading" : "4 Performance Guarantees",
      "text" : "We now examine the performance of our algorithm, which is measured by the number of label queries made to the oracle O. Additionally we require our algorithm to be statistically consistent, which means that the true error of the output classifier should converge to the true error of the best classifier in H on the data distribution D.\nAlgorithm 4 in disagr region(Ŝ,τ ,x): Test if x is in the disagreement region of current confidence set 1: Input: labeled dataset Ŝ, rejection threshold τ , unlabeled example x. 2: Output: 1 if x in the disagreement region of current confidence set, 0 otherwise. 3: Train ĥ ← CONS-LEARNH ({ /0, Ŝ}). 4: Train ĥ′x ← CONS-LEARNH ({(x,−ĥ(x))}, Ŝ}). 5: if err(ĥ′x, Ŝ)− err(ĥ, Ŝ)> τ then # x is in the agreement region 6: return 0 7: else # x is in the disagreement region 8: return 1 9: end if\nSince our framework is very general, we cannot expect any statistically consistent algorithm to achieve label savings over using O alone under all circumstances. For example, if labels provided by W are the complete opposite of O, no algorithm will achieve both consistency and label savings. We next provide an assumption under which Algorithm 1 works and yields label savings.\nAssumption. The following assumption states that difference hypothesis class contains a good cost-sensitive predictor of when O and W differ in the disagreement region of BU(h∗,r); a predictor is good if it has low false-negative error and predicts a positive label with low frequency. If there is no such predictor, then we cannot expect an algorithm similar to ours to achieve label savings.\nAssumption 1. Let D be the joint distribution: PD(x,yO,yW ) = PU(x)PW (yW |x)PO(yO|x). For any r,η > 0, there exists an hd fη ,r ∈ H d f with the following properties:\nPD(h d f η ,r(x) =−1,x ∈ DIS(BU(h∗,r)),yO 6= yW )≤ η (3)\nPD(h d f η ,r(x) = 1,x ∈ DIS(BU(h∗,r)))≤ α(r,η) (4)\nNote that (3), which states there is a hd f ∈ H d f with low false-negative error, is minimally restrictive, and is trivially satisfied if H d f includes the constant classifier that always predicts 1. Theorem shows that (3) is sufficient to ensure statistical consistency.\n(4) in addition states that the number of positives predicted by the classifier hd fη ,r is upper bounded by α(r,η). Note α(r,η) ≤ PU(DIS(BU(h∗,r))) always; performance gain is obtained when α(r,η) is lower, which happens when the difference classifier predicts agreement on a significant portion of DIS(BU(h∗,r)).\nConsistency. Provided Assumption 1 holds, we next show that Algorithm 1 is statistically consistent. Establishing consistency is non-trivial for our algorithm as the output classifier is trained on labels from both O and W .\nTheorem 1 (Consistency). Let h∗ be the classifier that minimizes the error with respect to D. If Assumption 1 holds, then with probability ≥ 1−δ , the classifier ĥ output by Algorithm 1 satisfies: errD(ĥ)≤ errD(h∗)+ε .\nLabel Complexity. The label complexity of standard DBAL is measured in terms of the disagreement coefficient. The disagreement coefficient θ(r) at scale r is defined as: θ(r) = suph∈H supr′≥r PU (DIS(BU (h,r′)) r′ ; intuitively, this measures the rate of shrinkage of the disagreement region with the radius of the ball BU(h,r) for any h in H . It was shown by [10] that the label complexity of DBAL for target excess generalization\nerror ε is Õ(dθ(2ν + ε)(1+ ν2ε2 )) where the Õ notation hides factors logarithmic in 1/ε and 1/δ . In contrast, the label complexity of our algorithm can be stated in Theorem 2. Here we use the Õ notation for convenience; we have the same dependence on log1/ε and log 1/δ as the bounds for DBAL.\nTheorem 2 (Label Complexity). Let d be the VC dimension of H and let d′ be the VC dimension of H d f . If Assumption 1 holds, and if the error of the best classifier in H on D is ν , then with probability ≥ 1−δ , the following hold:\n1. The number of label queries made by Algorithm 1 to the oracle O in epoch k at most:\nmk = Õ (d(2ν + εk−1)(α(2ν + εk−1,εk−1/1024)+ εk−1)\nε2k + d′P(DIS(BU(h∗,2ν + εk−1))) εk )\n(5)\n2. The total number of label queries made by Algorithm 1 to the oracle O is at most:\nÕ (\nsup r≥ε α(2ν + r,r/1024)+ r 2ν + r ·d ( ν2 ε2 +1 ) +θ(2ν + ε)d′ (ν ε +1 ))\n(6)"
    }, {
      "heading" : "4.1 Discussion",
      "text" : "The first terms in (5) and (6) represent the labels needed to learn the target classifier, and second terms represent the overhead in learning the difference classifier.\nIn the realistic agnostic case (where ν > 0), as ε → 0, the second terms are lower order compared to the label complexity of DBAL. Thus even if d′ is somewhat larger than d, fitting the difference classifier does not incur an asymptotically high overhead in the more realistic agnostic case. In the realizable case, when d′ ≈ d, the second terms are of the same order as the first; therefore we should use a simpler difference hypothesis class H d f in this case. We believe that the lower order overhead term comes from the fact that there exists a classifier in H d f whose false negative error is very low.\nComparing Theorem 2 with the corresponding results for DBAL, we observe that instead of θ(2ν + ε), we have the term supr≥ε α(2ν+r,r/1024) 2ν+r . Since supr≥ε α(2ν+r,r/1024) 2ν+r ≤ θ(2ν + ε), the worst case asymptotic label complexity is the same as that of standard DBAL. This label complexity may be considerably better however if supr≥ε α(2ν+r,r/1024) 2ν+r is less than the disagreement coefficient. As we expect, this will happen when the region of difference between W and O restricted to the disagreement regions is relatively small, and this region is well-modeled by the difference hypothesis class H d f .\nAn interesting case is when the weak labeler differs from O close to the decision boundary and agrees with O away from this boundary. In this case, any consistent algorithm should switch to querying O close to the decision boundary. Indeed in earlier epochs, α is low, and our algorithm obtains a good difference classifier and achieves label savings. In later epochs, α is high, the difference classifiers always predict a difference and the label complexity of the later epochs of our algorithm is the same order as DBAL. In practice, if we suspect that we are in this case, we can switch to plain active learning once εk is small enough.\nCase Study: Linear Classfication under Uniform Distribution. We provide a simple example where our algorithm provides a better asymptotic label complexity than DBAL. Let H be the class of homogeneous linear separators on the d-dimensional unit ball and let H d f = {h∆h′ : h,h′ ∈ H }. Furthermore, let U be the uniform distribution over the unit ball.\nSuppose that O is a deterministic labeler such that errD(h∗) = ν > 0. Moreover, suppose that W is such that there exists a difference classifier h̄d f with false negative error 0 for which PU(h̄d f (x) = 1) ≤ g.\nAdditionally, we assume that g = o( √\ndν); observe that this is not a strict assumption on H d f , as ν could be as much as a constant. Figure 1 shows an example in d = 2 that satisfies these assumptions. In this case, as ε → 0, Theorem 2 gives the following label complexity bound.\nCorollary 1. With probability ≥ 1− δ , the number of label queries made to oracle O by Algorithm 1 is Õ (\nd max( gν ,1)( ν2 ε2 +1)+d 3/2 ( 1+ νε )\n)\n, where the Õ notation hides factors logarithmic in 1/ε and 1/δ .\nAs g = o( √\ndν), this improves over the label complexity of DBAL, which is Õ(d3/2(1+ ν2ε2 )).\nLearning with respect to Data labeled by both O and W . Finally, an interesting variant of our model is to measure error relative to data labeled by a mixture of O and W – say, (1−β )O+βW for some 0 < β < 1. Similar measures have been considered in the domain adaptation literature [5].\nWe can also analyze this case using simple modifications to our algorithm and analysis. The results are presented in Corollary 2, which suggests that the number of label queries to O in this case is roughly 1−β times the label complexity in Theorem 2.\nLet O′ be the oracle which, on input x, queries O for its label w.p 1−β and queries W w.p β . Let D′ be the distribution: PD′(x,y) = PU(x)PO′(y|x), h′ = argminh∈H errD′(h) be the classifier in H that minimizes error over D′, and ν ′ = errD′(h′) be its true error. Moreover, suppose that Assumption 1 holds with respect to oracles O′ and W with α = α ′(r,η) in (4). Then, the modified Algorithm 1 that simulates O′ by random queries to O has the following properties.\nCorollary 2. With probability ≥ 1−2δ ,\n1. the classifier ĥ output by (the modified) Algorithm 1 satisfies: errD′(ĥ)≤ errD′(h′)+ ε .\n2. the total number of label queries made by this algorithm to O is at most:\nÕ ( (1−β ) (\nsup r≥ε α ′(2ν ′+ r,r/1024)+ r 2ν ′+ r ·d ( ν ′2 ε2 +1 ) +θ(2ν ′+ ε)d′ ( ν ′ ε +1 ) ))\nConclusion. In this paper, we take a step towards a theoretical understanding of active learning from multiple annotators through a learning theoretic formalization for learning from weak and strong labelers. Our work shows that multiple annotators can be successfully combined to do active learning in a statistically\nconsistent manner under a general setting with few assumptions; moreover, under reasonable conditions, this kind of learning can provide label savings over plain active learning.\nAn avenue for future work is to explore a more general setting where we have multiple labelers with expertise on different regions of the input space. Can we combine inputs from such labelers in a statistically consistent manner? Second, our algorithm is intended for a setting where W is biased, and performs suboptimally when the label generated by W is a random corruption of the label provided by O. How can we account for both random noise and bias in active learning from weak and strong labelers?"
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank NSF under IIS 1162581 for research support and Jennifer Dy for introducing us to the problem of active learning from multiple labelers."
    }, {
      "heading" : "A Notation",
      "text" : ""
    }, {
      "heading" : "A.1 Basic Definitions and Notation",
      "text" : "Here we do a brief recap of notation. We assume that we are given a target hypothesis class H of VC dimension d, and a difference hypothesis class H d f of VC dimension d′.\nWe are given access to an unlabeled distribution U and two labeling oracles O and W . Querying O (resp. W ) with an unlabeled data point xi generates a label yi,O (resp. yi,W ) which is drawn from the distribution PO(y|xi) (resp. PW (y|xi)). In general these two distributions are different. We use the notation D to denote the joint distribution over examples and labels from O and W :\nPD(x,yO,yW ) = PU(x)PO(yO|x)PW (yW |x)\nOur goal in this paper is to learn a classifier in H which has low error with respect to the data distribution D described as: PD(x,y) = PU(x)PO(y|x) and our goal is use queries to W to reduce the number of queries to O. We use yO to denote the labels returned by O, yW to denote the labels returned by W .\nThe error of a classifier h under a labeled data distribution Q is defined as: errQ(h) = P(x,y)∼Q(h(x) 6= y); we use the notation err(h,S) to denote its empirical error on a labeled data set S. We use the notation h∗ to denote the classifier with the lowest error under D. Define the excess error of h with respect to distribution D as errD(h)− errD(h∗). For a set Z, we occasionally abuse notation and use Z to also denote the uniform distribution over the elements of Z.\nConfidence Sets and Disagreement Region. Our active learning algorithm will maintain a (1 − δ )- confidence set for h∗ throughout the algorithm. A set of classifiers V ⊆ H produced by a (possibly randomized) algorithm is said to be a (1− δ )-confidence set for h∗ if h∗ ∈ V with probability ≥ 1− δ ; here the probability is over the randomness of the algorithm as well as the choice of all labeled and unlabeled examples drawn by it.\nGiven two classifiers h1 and h2 the disagreement between h1 and h2 under an unlabeled data distribution U , denoted by ρU(h1,h2), is Px∼U(h1(x) 6= h2(x)). Given an unlabeled dataset S, the empirical disagreement of h1 and h2 on S is denoted by ρS(h1,h2). Observe that the disagreements under U form a pseudometric over H . We use BU(h,r) to denote a ball of radius r centered around h in this metric. The disagreement region of a set V of classifiers, denoted by DIS(V ), is the set of all examples x ∈ X such that there exist two classifiers h1 and h2 in V for which h1(x) 6= h2(x).\nDisagreement Region. We denote the disagreement region of a disagreement ball of radius r centered around h∗ by\n∆(r) := DIS(B(h∗,r)) (7)\nConcentration Inequalities. Suppose Z is a dataset consisting of n iid samples from a distribution D. We will use the following result, which is obtained from a standard application of the normalized VC inequality. With probability 1−δ over the random draw of Z, for all h,h′ ∈ H ,\n|(err(h,Z)− err(h′,Z))− (errD(h)− errD(h′))| ≤ min ( √ σ(n,δ )ρZ(h,h′)+σ(n,δ ), √ σ(n,δ )ρD(h,h′)+σ(n,δ ) )\n(8)\n|(err(h,Z)− errD(h)| ≤ min ( √ σ(n,δ )err(h,Z)+σ(n,δ ), √ σ(n,δ )errD(h)+σ(n,δ ) )\n(9)\nwhere d is the VC dimension of H and the notation σ(n,δ ) is defined as:\nσ(n,δ ) = 8 n (2d ln 2en d + ln 24 δ ) (10)\nEquation (8) loosely implies the following equation:\n|(err(h,Z)− err(h′,Z))− (errD(h)− errD(h′))| ≤ √ 4σ(n,δ ) (11)\nThe following is a consequence of standard Chernoff bounds. Let X1, . . . ,Xn be iid Bernoulli random variables with mean p. If p̂ = ∑i Xi/n, then with probabiliy 1−δ ,\n|p̂− p| ≤ min( √ pγ(n,δ )+ γ(n,δ ), √ p̂γ(n,δ )+ γ(n,δ )) (12)\nwhere the notation γ(n,δ ) is defined as:\nγ(n,δ ) = 4 n ln 2 δ\n(13)\nEquation (12) loosely implies the following equation:\n|p̂− p| ≤ √ 4γ(n,δ ) (14)\nUsing the notation we just introduced, we can rephrase Assumption 1 as follows. For any r,η > 0, there exists an hd fη ,r ∈ H d f with the following properties:\nPD(h d f η ,r(x) =−1,x ∈ ∆(r),yO 6= yW )≤ η\nPD(h d f η ,r(x) = 1,x ∈ ∆(r))≤ α(r,η)\nWe end with an useful fact about σ(n,δ ).\nFact 1. The minimum n such that σ(n,δ/(log n(log n+1)))≤ ε is at most\n64 ε (d ln 512 ε + ln 24 δ )"
    }, {
      "heading" : "A.2 Adaptive Procedure for Estimating Probability Mass",
      "text" : "For completeness, we describe in Algorithm 5 a standard doubling procedure for estimating the bias of a coin within a constant factor. This procedure is used by Algorithm 2 to estimate the probability mass of the disagreement region of the current confidence set based on unlabeled examples drawn from U .\nLemma 1. Suppose p > 0 and Algorithm 5 is run with failure probability δ . Then with probability 1− δ , (1) the output p̂ is such that p̂ ≤ p ≤ 2p̂. (2) The total number of calls to O is at most O( 1p2 ln 1 δ p).\nProof. Consider the event\nE = { for all i ∈ N, |p̂i − p| ≤\n√\n4ln 2·2 i\nδ 2i }\nAlgorithm 5 Adaptive Procedure for Estimating the Bias of a Coin 1: Input: failure probability δ , an oracle O which returns iid Bernoulli random variables with unknown\nbias p. 2: Output: p̂, an estimate of bias p such that p̂ ≤ p ≤ 2p̂ with probability ≥ 1−δ . 3: for i = 1,2, . . . do 4: Call the oracle O 2i times to get empirical frequency p̂i.\n5: if\n√\n4ln 4·2 i\nδ 2i ≤ p̂i/3 then return p̂ = 2p̂i 3\n6: end if 7: end for\nBy Equation (14) and union bound, P(E)≥ 1−δ . On event E , we claim that if i is large enough that\n4\n√\n4ln 4·2 i\nδ 2i ≤ p (15)\nthen the condition in line 5 will be met. Indeed, this implies\n√\n4ln 4·2 i\nδ 2i\n≤ p−\n√\n4ln 4·2 i\nδ 2i\n3 ≤ p̂i 3\nDefine i0 as the smallest number i such that Equation (15) is true. Then by algebra, 2i0 = O( 1p2 ln 1 δ p). Hence the number of calls to oracle O is at most 1+2+ . . .+2i0 = O( 1p2 ln 1\nδ p). Consider the smallest i∗ such that the condition in line 5 is met. We have that\n√\n4ln 4·2 i∗\nδ 2i∗ ≤ p̂i∗/3\nBy the definition of E , |p− p̂i∗ | ≤ p̂i∗/3\nthat is, 2p̂i∗/3 ≤ p ≤ 4p̂i∗/3, implying p̂ ≤ p ≤ 2p̂."
    }, {
      "heading" : "A.3 Notations on Datasets",
      "text" : "Without loss of generality, assume the examples drawn throughout Algorithm 1 have distinct feature values x, since this happens with probability 1 under mild assumptions.\nAlgorithm 1 uses a mixture of three kinds of labeled data to learn a target classifier – labels obtained from querying O, labels inferred by the algorithm, and labels obtained from querying W . To analyze the effect of these three kinds of labeled data, we need to introduce some notation.\nRecall that we define the joint distribution D over examples and labels both from O and W as follows:\nPD(x,yO,yW ) = PU(x)PO(yO|x)PW (yW |x)\nwhere given an example x, the labels generated by O and W are conditionally independent.\nA dataset Ŝ with empirical error minimizer ĥ and a rejection threshold τ define a implicit confidence set for h∗ as follows: V (Ŝ,τ) = {h : err(h, Ŝ)− err(ĥ, Ŝ)≤ τ} At the beginning of epoch k, we have Ŝk−1. ĥk−1 is defined as the empirical error minimizer of Ŝk−1. The disagreement region of the implicit confidence set at epoch k, Rk−1 is defined as Rk−1 := DIS(V (Ŝk−1,3εk/2)). Algorithm 4 in disagr region(Ŝk−1,3εk/2,x) provides a test deciding if an unlabeled example x is inside Rk−1 in epoch k. (See Lemma 6.)\nDefine Ak to be the distribution D conditioned on the set {(x,yO,yW ) : x ∈ Rk−1}. At epoch k, Algorithm 2 has inputs distribution U , oracles W and O, target false negative error ε = εk/128, hypothesis class H d f , confidence δ = δk/2, previous labeled dataset Ŝk−1, and outputs a difference classfier ĥd fk . By the setting of m in Equation (1), Algorithm 2 first computes p̂k using unlabeled examples drawn from U , which is an estimator of PD (x ∈ Rk−1). Then it draws a subsample of size\nmk,1 = 64 ·1024p̂k\nεk (d ln 512 ·1024p̂k εk + ln 144 δk ) (16)\niid from Ak. We call the resulting dataset A ′k . At epoch k, Algorithm 3 performs adaptive subsampling to refine the implicit (1− δ )-confidence set. For each round t, it subsamples U to get an unlabeled dataset St,Uk of size 2 t . Define the corresponding (hypothetical) dataset with labels queried from both W and O as S tk . S t k, the (hypothetical) dataset with labels queried from O, is defined as:\nStk = {(x,yO)|(x,yO,yW ) ∈ S tk}\nIn addition to obtaining labels from O, the algorithm obtains labels in two other ways. First, if an x ∈ X \\Rk−1, then its label is safely inferred and with high probability, this inferred label ĥk−1(x) is equal to h∗(x). Second, if an x lies in Rk−1 but if the difference classifier ĥ d f k predicts agreement between O and W , then its label is obtained by querying W . The actual dataset Ŝtk generated by Algorithm 3 is defined as:\nŜtk = {(x, ĥk−1(x))|(x,yO,yW ) ∈ S tk ,x /∈ Rk−1}∪{(x,yO)|(x,yO,yW ) ∈ S tk ,x ∈ Rk−1, ĥd fk (x) = +1} ∪{(x,yW )|(x,yO,yW ) ∈ S tk ,x ∈ Rk−1, ĥd fk (x) =−1}\nWe use D̂k to denote the labeled data distribution as follows:\nPD̂k (x,y) = PU(x)PQ̂k(y|x)\nPQ̂k (y|x) =\n\n \n  I(ĥk−1(x) = y), x /∈ Rk−1 PO(y|x), x ∈ Rk−1, ĥd fk (x) = +1 PW (y|x), x ∈ Rk−1, ĥd fk (x) =−1\nTherefore, Ŝtk can be seen as a sample of size 2 t drawn iid from D̂k.\nObserve that ĥtk is obtained by training an ERM classifier over Ŝ t k, and δ t k = δk/2t(t +1). Suppose Algorithm 3 stops at iteration t0(k), then the final dataset returned is Ŝk = Ŝ t0(k) k , with a total\nnumber of mk,2 label requests to O. We define Sk = S t0(k) k , Sk = S t0(k) k and σk = σ(2 t0(k),δ t0(k)k ).\nFor k = 0, we define the notation Ŝk differently. Ŝ0 is the dataset drawn iid at random from D, with labels queried entirely to O. For notational convenience, define S0 = Ŝ0. σ0 is defined as σ0 = σ(n0,δ0), where σ(·, ·) is defined by Equation (10) and n0 is defined as:\nn0 = (64 ·10242)(2d ln(512 ·10242)+ ln 96 δ )\nRecall that ĥk = argminh∈H err(h, Ŝk) is the empirical error minimizer with respect to the dataset Ŝk. Note that the empirical distance ρZ(·, ·) does not depend on the labels in dataset Z, therefore, ρŜk(h,h\n′) = ρSk(h,h′). We will use them interchangably throughout."
    }, {
      "heading" : "A.4 Events",
      "text" : "Recall that δk = δ/(4(k+1)2),εk = 2−k. Define\nhd fk = h d f 2ν+εk−1,εk/512\nwhere the notation hd fr,η is introduced in Assumption 1. We begin by defining some events that we will condition on later in the proof, and showing that these events occur with high probability. Define event\nE1k := {\nPD(x ∈ Rk−1)/2 ≤ p̂k ≤ PD (x ∈ Rk−1),\nand For all hd f ∈ H d f , |PA ′k (h d f (x) =−1,yO 6= yW )−PAk(hd f (x) =−1,yO 6= yW )| ≤ εk\n1024PD (x ∈ Rk−1)\n+\n√\nmin(PAk(h d f (x) =−1,yO 6= yW ),PA ′k (hd f (x) =−1,yO 6= yW )) εk 1024PD (x ∈ Rk−1)\nand |PA ′k (h d f (x) = +1)−PAk(hd f (x) = +1)|\n≤ √\nmin(PAk(h d f (x) = +1),PA ′k (h\nd f (x) = +1)) εk\n1024PD (x ∈ Rk−1) + εk 1024PD (x ∈ Rk−1) }\nFact 2. P(E1k )≥ 1−δk/2.\nDefine event\nE2k = {\nFor all t ∈ N, for all h,h′ ∈ H ,\n|(err(h,Stk)− err(h′,Stk))− (errD(h)− errD(h′))| ≤ σ(2t ,δ tk)+ √ σ(2t ,δ tk)ρStk(h,h ′)\nand err(h, Ŝtk)− errD̂k(h) ≤ σ(2 t ,δ tk)+\n√\nσ(2t ,δ tk)errD̂k(h)\nand PS tk (ĥ d f k (x) =−1,yO 6= yW ,x ∈ Rk−1)−PD(ĥ d f k (x) =−1,yO 6= yW ,x ∈ Rk−1)\n≤ √\nγ(2t ,δ tk)PS tk (ĥ d f k (x) =−1,yO 6= yW ,x ∈ Rk−1)+ γ(2t ,δ tk)\nand PS tk (ĥ d f k (x) =−1∩ x ∈ Rk−1)≤ 2(PD (ĥ d f k (x) =−1,x ∈ Rk−1)+ γ(2t ,δ tk))\n}\nFact 3. P(E2k )≥ 1−δk/2.\nWe will also use the following definitions of events in our proof. Define event F0 as\nF0 = { for all h,h′ ∈H , |(err(h,S0)−err(h′,S0))−(errD(h)−errD(h′))| ≤σ(n0,δ0)+ √ σ(n0,δ0)ρS0(h,h′) }\nFor k ∈ {1,2, . . . ,k0}, event Fk is defined inductively as\nFk = Fk−1 ∩ (E1k ∩E2k )\nFact 4. For k ∈ {0,1, . . . ,k0}, P(Fk)≥ 1−δ0 −δ1 − . . .−δk. Specifically, P(Fk0)≥ 1−δ .\nThe proofs of Facts 2, 3 and 4 are provided in Appendix E."
    }, {
      "heading" : "B Proof Outline and Main Lemmas",
      "text" : "The main idea of the proof is to maintain the following three invariants on the outputs of Algorithm 1 in each epoch. We prove that these invariants hold simultaneously for each epoch with high probability by induction over the epochs. Throughout, for k ≥ 1, the end of epoch k refers to the end of execution of line 13 of Algorithm 1 at iteration k. The end of epoch 0 refers to the end of execution of line 5 in Algorithm 1.\nInvariant 1 states that if we replace the inferred labels and labels obtained from W in Ŝk by those obtained from O (thus getting the dataset Sk), then the excess errors of classifiers in H will not decrease by much. Invariant 1 (Approximate Favorable Bias). Let h be any classifier in H , and h′ be another classifier in H with excess error on D no greater than εk. Then, at the end of epoch k, we have:\nerr(h,Sk)− err(h′,Sk)≤ err(h, Ŝk)− err(h′, Ŝk)+ εk/16\nInvariant 2 establishes that in epoch k, Algorithm 3 selects enough examples so as to ensure that concentration of empirical errors of classifiers in H on Sk to their true errors.\nInvariant 2 (Concentration). At the end of epoch k, Ŝk, Sk and σk are such that: 1. For any pair of classifiers h,h′ ∈ H , it holds that:\n|(err(h,Sk)− err(h′,Sk))− (errD(h)− errD(h′))| ≤ σk + √ σkρSk(h,h′) (17)\n2. The dataset Ŝk has the following property:\nσk + √ σkerr(ĥk, Ŝk)≤ εk/512 (18)\nFinally, Invariant 3 ensures that the difference classifier produced in epoch k has low false negative error on the disagreement region of the (1−δ ) confidence set at epoch k. Invariant 3 (Difference Classifier). At epoch k, the difference classifier output by Algorithm 2 is such that\nPD(ĥ d f k (x) =−1,yO 6= yW ,x ∈ Rk−1)≤ εk/64 (19)\nPD(ĥ d f k (x) = +1,x ∈ Rk−1)≤ 6(α(2ν + εk−1,εk/512)+ εk/1024) (20)\nWe will show the following property about the three invariants. Its proof is deferred to Subsection B.4.\nLemma 2. There is a numerical constant c0 > 0 such that the following holds. The collection of events {Fk}k0k=0 is such that for k ∈ {0,1, . . . ,k0}: (1) If k = 0, then on event Fk, at epoch k,\n(1.1) Invariants 1,2 hold. (1.2) The number of label requests to O is at most m0 ≤ c0(d + ln 1δ ).\n(2) If k ≥ 1, then on event Fk, at epoch k, (2.1) Invariants 1,2,3 hold. (2.2) the number of label requests to O is at most\nmk ≤ c0 ((α(2ν + εk−1,εk/1024)+ εk)(ν + εk)\nε2k d(ln2 1 εk + ln2 1 δk )+ PU(x ∈ ∆(2ν + εk−1)) εk (d′ ln 1 εk + ln 1 δk ) )"
    }, {
      "heading" : "B.1 Active Label Inference and Identifying the Disagreement Region",
      "text" : "We begin by proving some lemmas about Algorithm 4 which identifies if an example lies in the disagreement region of the current confidence set. This is done by using a constrained ERM oracle CONS-LEARNH(·, ·) using ideas similar to [10, 15, 3, 4].\nLemma 3. When given as input a dataset Ŝ, a threshold τ > 0, an unlabeled example x, Algorithm 4 in disagr region returns 1 if and only if x lies inside DIS(V (Ŝ,τ)).\nProof. (⇒) If Algorithm 4 returns 1, then we have found a classifier ĥ′x such that (1) ĥx(x) =−ĥ(x), and (2) err(ĥ′x, Ŝ)− err(ĥ, Ŝ)≤ τ , i.e. ĥ′x ∈V (Ŝ,τ). Therefore, x is in DIS(V (Ŝ,τ)). (⇐) If x is in DIS(V (Ŝ,τ)), then there exists a classifier h∈H such that (1) h(x) =−ĥ(x) and (2) err(h, Ŝ)− err(ĥ, Ŝ)≤ τ . Hence by definition of ĥ′x, err(ĥ′x, Ŝ)− err(ĥ, Ŝ)≤ τ . Thus, Algorithm 4 returns 1.\nWe now provide some lemmas about the behavior of Algorithm 4 called at epoch k.\nLemma 4. Suppose Invariants 1 and 2 hold at the end of epoch k− 1. If h ∈ H is such that errD(h) ≤ errD(h∗)+ εk−1/2, then\nerr(h, Ŝk−1)− err(ĥk−1, Ŝk−1)≤ 3εk−1/4\nProof. If h ∈ H has excess error at most εk−1/2 with respect to D, then,\nerr(h, Ŝk−1)− err(ĥk−1, Ŝk−1) ≤ err(h,Sk−1)− err(ĥk−1,Sk−1)+ εk−1/16 ≤ errD(h)− errD(ĥk−1)+σk−1 + √ σk−1ρSk−1(h, ĥk−1)+ εk−1/16 ≤ εk−1/2+σk−1 + √ σk−1ρSk−1(h, ĥk−1)+ εk−1/16 ≤ 9εk−1/16+σk−1 + √ σk−1err(h, Ŝk−1)+ √ σk−1err(ĥk−1, Ŝk−1)\n≤ 9εk−1/16+σk−1 + √ σk−1err(h, Ŝk−1)+ √ σk−1(err(ĥk−1, Ŝk−1)+9εk−1/16)\nWhere the first inequality follows from Invariant 1, the second inequality from Equation (17) of Invariant 2, the third inequality from the assumption that h has excess error at most εk−1/2, and the fourth inequality from the triangle inequality, the fifth inequality is by adding a nonnegative number in the last term. Continuing,\nerr(h, Ŝk−1)− err(ĥk−1, Ŝk−1)\n≤ 9εk−1/16+4σk−1 +2 √ σk−1(err(ĥk−1, Ŝk−1)+9εk−1/16) ≤ 9εk−1/16+4σk−1 +2 √ σk−1err(ĥk−1, Ŝk−1)+2 √\nεk−1/512 ·9εk−1/16 ≤ 9εk−1/16+ εk−1/32+2 √\nεk−1/512 ·9εk−1/16 ≤ 3εk−1/4\nWhere the first inequality is by simple algebra (by letting D = err(h, Ŝk−1), E = err(ĥk−1, Ŝk−1)+9εk−1/16, F = σk−1 in D ≤ E +F + √ DF + √ EF ⇒ D ≤ E +4F +2 √ EF), the second inequality is from √ A+B ≤√\nA+ √\nB and σk−1 ≤ εk−1/512 which utilizes Equation (18) of Invariant 2, the third inequality is again by Equation (18) of Invariant 2, the fourth inequality is by algebra.\nLemma 5. Suppose Invariants 1 and 2 hold at the end of epoch k−1. Then,\nerrD(ĥk−1)− errD(h∗)≤ εk−1/8\nProof. By Lemma 4, we know that since h∗ has excess error 0 with respect to D,\nerr(h∗, Ŝk−1)− err(ĥk−1, Ŝk−1)≤ 3εk−1/4 (21)\nTherefore,\nerrD(ĥk−1)− errD(h∗)\n≤ err(ĥk−1,Sk−1)− err(h∗,Sk−1)+σk−1 + √ σk−1ρSk−1(ĥk−1,h∗) ≤ err(ĥk−1, Ŝk−1)− err(h∗, Ŝk−1)+σk−1 + √ σk−1ρSk−1(ĥk−1,h∗)+ εk−1/16 ≤ εk−1/16+σk−1 + √ σk−1(err(ĥk−1, Ŝk−1)+ err(h∗, Ŝk−1)) ≤ εk−1/16+σk−1 + √ σk−1(2err(ĥk−1, Ŝk−1)+3εk−1/4) ≤ εk−1/16+σk−1 + √ 2σk−1err(ĥk−1, Ŝk−1)+ √\nεk−1/512 ·3εk−1/4 ≤ εk−1/8\nwhere the first inequality is from Equation (17) of Invariant 2, the second inequality uses Invariant 1, the third inequality follows from the optimality of ĥk−1 and triangle inequality, the fourth inequality uses Equation (21), the fifth inequality uses the fact that √ A+B ≤ √ A+ √ B and σk−1 ≤ εk−1/512, which is from Equation (18) of Invariant 2, the last inequality again utilizes the Equation (18) of Invariant 2.\nLemma 6. Suppose Invariants 1, 2, and 3 hold in epoch k−1 conditioned on event Fk−1. Then conditioned on event Fk−1, the implicit confidence set Vk−1 =V (Ŝk−1,3εk/2) is such that: (1) If h ∈ H satisfies errD(h)− errD(h∗)≤ εk, then h is in Vk−1. (2) If h ∈ H is in Vk−1, then errD(h)− errD(h∗)≤ εk−1. Hence Vk−1 ⊆ BU(h∗,2ν + εk−1). (3) Algorithm 4, in disagr region, when run on inputs dataset Ŝk−1, threshold 3εk/2, unlabeled example x, returns 1 if and only if x is in Rk−1.\nProof. (1) Let h be a classifier with errD(h)− errD(h∗) ≤ εk = εk−1/2. Then, by Lemma 4, one has err(h, Ŝk−1)− err(ĥk−1, Ŝk−1)≤ 3εk−1/4 = 3εk/2. Hence, h is in Vk−1. (2) Fix any h in Vk−1, by definition of Vk−1,\nerr(h, Ŝk−1)− err(ĥk−1, Ŝk−1)≤ 3εk/2 = 3εk−1/4 (22)\nRecall that from Lemma 5, errD(ĥk−1)− errD(h∗)≤ εk−1/8\nThus for classifier h, applying Invariant 1 by taking h′ := ĥk−1, we get\nerr(h,Sk−1)− err(ĥk−1,Sk−1)≤ err(h, Ŝk−1)− err(ĥk−1, Ŝk−1)+ εk−1/32 (23)\nTherefore,\nerrD(h)− errD(ĥk−1)\n≤ err(h,Sk−1)− err(ĥk−1,Sk−1)+σk−1 + √ σk−1ρSk−1(h, ĥk−1) ≤ err(h,Sk−1)− err(ĥk−1,Sk−1)+σk−1 + √ σk−1(err(h, Ŝk−1)+ err(ĥk−1, Ŝk−1)) ≤ err(h, Ŝk−1)− err(ĥk−1, Ŝk−1)+σk−1 + √ σk−1(err(h, Ŝk−1)+ err(ĥk−1, Ŝk−1))+ εk−1/16 ≤ 13εk−1/16+σk−1 + √ σk−1(2err(ĥk−1, Ŝk−1)+3εk−1/4) ≤ 13εk−1/16+σk−1 + √ 2σk−1err(ĥk−1, Ŝk−1)+ √\nεk−1/512 ·3εk−1/4 ≤ 7εk−1/8\nwhere the first inequality is from Equation (17) of Invariant 2, the second inequality uses the fact that ρŜk−1(h,h\n′) = ρSk−1(h,h′)≤ err(h, Ŝk−1)+err(h′, Ŝk−1) for h,h′ ∈H , the third inequality uses Equation (23); the fourth inequality is from Equation (22); the fifth inequality is from the fact that √ A+B ≤ √ A+ √ B and σk−1 ≤ εk−1/512, which is from Equation (18) of Invariant 2, the last inequality again follows from Equation (18) of Invariant 2 and algebra. In conjunction with the fact that errD(ĥk−1)− errD(h∗)≤ εk−1/8, this implies\nerrD(h)− errD(h∗)≤ εk−1\nBy triangle inequality, ρ(h,h∗)≤ 2ν + εk−1, hence h ∈ BU(h∗,2ν + εk−1). In summary Vk−1 ⊆ BU(h∗,2ν + εk−1). (3) Follows directly from Lemma 3 and the fact that Rk−1 = DIS(Vk−1)."
    }, {
      "heading" : "B.2 Training the Difference Classifier",
      "text" : "Recall that ∆(r) = DIS(BU(h∗,r)) is the disagreement region of the disagreement ball centered around h∗ with radius r.\nLemma 7 (Difference Classifier Invariant). There is a numerical constant c1 > 0 such that the following holds. Suppose that Invariants 1 and 2 hold at the end of epoch k − 1 conditioned on event Fk−1 and that Algorithm 2 has inputs unlabeled data distribution U, oracle O, ε = εk/128, hypothesis class H d f , δ = δk/2, previous labeled dataset Ŝk−1. Then conditioned on event Fk, (1) ĥd fk , the output of Algorithm 2, maintains Invariant 3. (2)(Label Complexity: Part 1.) The number of label queries made to O is at most\nmk,1 ≤ c1 ( PU(x ∈ ∆(2ν + εk−1)) εk (d′ ln 1 εk + ln 1 δk ) )\nProof. (1) Recall that Fk = Fk−1 ∩E1k ∩E2k , where E1k , E2k are defined in Subsection A.4. Suppose event Fk happens.\nProof of Equation (19). Recall that ĥd fk is the optimal solution of optimization problem (2). We have by feasibility and the fact that on event E3k , 2p̂k ≥ PD(x ∈ Rk−1),\nPA ′k (ĥd fk (x) =−1,yO 6= yW )≤ εk 256p̂k ≤ εk 128PD (x ∈ Rk−1)\nBy definition of event E2k , this implies\nPAk(ĥ d f k (x) =−1,yO 6= yW )\n≤ PA ′k (ĥ d f k (x) =−1,yO 6= yW )+\n√\nPA ′k (ĥd fk (x) =−1,yO 6= yW ) εk 1024PD (x ∈ Rk−1) + εk\n1024PD (x ∈ Rk−1) ≤ εk\n64PD (x ∈ Rk−1)\nIndicating\nPD(ĥ d f k (x) =−1,yO 6= yW ,x ∈ Rk−1)≤ εk 64\nProof of Equation (20). By definition of hd fk in Subsection A.4, h d f k is such that:\nPD (h d f k (x) = +1,x ∈ ∆(2ν + εk−1))≤ α(2ν + εk−1,εk/512)\nPD(h d f k (x) =−1,yO 6= yW ,x ∈ ∆(2ν + εk−1))≤ εk/512\nBy item (2) of Lemma 6, we have Rk−1 ⊆ DIS(BU(h∗,2ν + εk−1)), thus\nPD(h d f k (x) = +1,x ∈ Rk−1)≤ α(2ν + εk−1,εk/512) (24)\nPD (h d f k (x) =−1,yO 6= yW ,x ∈ Rk−1)≤ εk/512 (25)\nEquation (25) implies that\nPAk(h d f k (x) =−1,yO 6= yW )≤ εk 512PD (x ∈ Rk−1)\n(26)\nRecall that A ′k is the dataset subsampled from Ak in line 3 of Algorithm 2. By definition of event E 1 k , we have that for hd fk ,\nPA ′k (hd fk (x) =−1,yO 6= yW )\n≤ PAk(h d f k (x) =−1,yO 6= yW )+\n√\nPAk(h d f k (x) =−1,yO 6= yW ) εk 1024PD (x ∈ Rk−1) + εk\n1024PD (x ∈ Rk−1) ≤ εk\n256PD (x ∈ Rk−1) ≤ εk 256p̂k\nwhere the second inequality is from Equation (26), and the last inequality is from the fact that p̂k ≤ PD(x ∈ Rk−1). Hence, h d f k is a feasible solution to the optimization problem (2). Thus,\nPAk(ĥ d f k (x) = +1)\n≤ PA ′k (ĥ d f k (x) = +1)+\n√\nPA ′k (ĥd fk (x) = +1) εk 1024PD (x ∈ Rk−1) + εk\n1024PD (x ∈ Rk−1) ≤ 2(PA ′k (ĥ d f k (x) = +1)+ εk 1024PD (x ∈ Rk−1) )\n≤ 2(PA ′k (h d f k (x) = +1)+ εk 1024PD (x ∈ Rk−1) )\n≤ 2((PAk(h d f k (x) = +1)+\n√\nPAk(h d f k (x) = +1) εk 1024PD (x ∈ Rk−1) + εk 1024PD (x ∈ Rk−1) )+ εk 1024PD (x ∈ Rk−1) )\n≤ 6(PAk(h d f k (x) = +1)+ εk 1024PD (x ∈ Rk−1) )\nwhere the first inequality is by definition of event E1k , the second inequality is by algebra, the third inequality is by optimality of ĥd fk in (2), PA ′k (ĥ d f k (x) = +1)≤ PA ′k (h d f k (x) = +1), the fourth inequality is by definition of event E1k , the fifth inequality is by algebra. Therefore,\nPD(ĥ d f k (x)=+1,x∈Rk−1)≤ 6(PD (h d f k (x)=+1,x∈Rk−1)+εk/1024)≤ 6(α(2ν +εk−1,εk/512)+εk/1024) (27) where the second inequality follows from Equation (24). This establishes the correctness of Invariant 3. (2) The number of label requests to O follows from line 3 of Algorithm 2 (see Equation (16)). That is, we can choose c1 large enough (independently of k), such that\nmk,1 ≤ c1 ( PD(x ∈ Rk−1) εk (d′ ln 1 εk + ln 1 δk ) ) ≤ c1 ( PU(x ∈ ∆(2ν + εk−1)) εk (d′ ln 1 εk + ln 1 δk ) )\nwhere in the second step we use the fact that on event Fk, by item (2) of Lemma 6, Rk−1 ⊆ DIS(BU(h∗,2ν + εk−1)), thus PD(x ∈ Rk−1)≤ PD (x ∈ ∆(2ν + εk−1)) = PU(x ∈ ∆(2ν + εk−1))."
    }, {
      "heading" : "B.3 Adaptive Subsampling",
      "text" : "Lemma 8. There is a numerical constant c2 > 0 such that the following holds. Suppose Invariants 1, 2, and 3 hold in epoch k− 1 on event Fk−1; Algorithm 3 receives inputs unlabeled distribution U, classifier ĥk−1, difference classifier ĥd f = ĥ d f k , target excess error ε = εk, confidence δ = δk/2, previous labeled dataset Ŝk−1. Then on event Fk, (1) Ŝk, the output of Algorithm 3, maintains Invariants 1 and 2. (2) (Label Complexity: Part 2.) The number of label queries to O in Algorithm 3 is at most:\nmk,2 ≤ c2 ((ν + εk)(α(2ν + εk−1,εk/512)+ εk) ε2k ·d(ln2 1 εk + ln2 1 δk ) )\nProof. (1) Recall that Fk = Fk−1 ∩E1k ∩E2k , where E1k , E2k are defined in Subsection A.4. Suppose event Fk happens.\nProof of Invariant 1. We consider a pair of classifiers h,h′ ∈ H , where h is an arbitrary classifier in H and h′ has excess error at most εk.\nAt iteration t = t0(k) of Algorithm 3, the breaking criteron in line 14 is met, i.e.\nσ(2t0(k),δ t0(k)k )+ √ σ(2t0(k),δ t0(k)k )err(ĥt0(k), Ŝ t0(k) k )≤ εk/512 (28)\nFirst we expand the definition of err(h,Sk) and err(h, Ŝk) respectively:\nerr(h,Sk)=PSk(ĥ d f k (x)=+1,h(x) 6= yO,x∈Rk−1)+PSk(ĥ d f k (x)=−1,h(x) 6= yO,x∈Rk−1)+PSk(h(x) 6= yO,x /∈Rk−1)\nerr(h, Ŝk)=PSk(ĥ d f k (x)=+1,h(x) 6= yO,x∈Rk−1)+PSk(ĥ d f k (x)=−1,h(x) 6= yW ,x∈Rk−1)+PSk(h(x) 6= h∗(x),x /∈Rk−1)\nwhere we use the fact that by Lemma 6, for all examples x /∈ Rk−1, ĥk−1(x) = h∗(x). We next show that PSk(ĥ d f k (x) = −1,h(x) 6= yO,x ∈ Rk−1) is close to PSk(ĥ d f k (x) =−1,h(x) 6= yW ,x ∈\nRk−1). From Lemma 7, we know that conditioned on event Fk,\nPD(ĥ d f k (x) =−1,yO 6= yW ,x ∈ Rk−1)≤ εk/64\nIn the meantime, from Equation (28), γ(2t0(k),δ t0(k)k ) ≤ σ(2t0(k),δ t0(k) k ) ≤ εk/512. Recall that Sk = S t0(k) k . Therefore, by definition of E2k ,\nPSk(ĥ d f k (x) =−1,yO 6= yW ,x ∈ Rk−1)\n≤ PD(ĥd fk (x) =−1,yO 6= yW ,x ∈ Rk−1)+ √ PD (ĥ d f k (x) =−1,yO 6= yW ,x ∈ Rk−1)γ(2t0(k),δ t0(k) k )+ γ(2 t0(k),δ t0(k)k ) ≤ PD(ĥd fk (x) =−1,yO 6= yW ,x ∈ Rk−1)+ √ PD (ĥ d f k (x) =−1,yO 6= yW ,x ∈ Rk−1)εk/512+ εk/512 ≤ εk/32\nBy triangle inequality, for all classifier h0 ∈ H ,\n|PSk(ĥ d f k (x) =−1,h0(x) 6= yO,x ∈ Rk−1)−PSk(ĥ d f k (x) =−1,h0(x) 6= yW ,x ∈ Rk−1)| ≤ εk/32 (29)\nSpecifically for h and h′, Equation (29) hold:\n|PSk(ĥ d f k (x) =−1,h(x) 6= yO,x ∈ Rk−1)−PSk(ĥ d f k (x) =−1,h(x) 6= yW ,x ∈ Rk−1)| ≤ εk/32\n|PSk(ĥ d f k (x) =−1,h′(x) 6= yO,x ∈ Rk−1)−PSk(ĥ d f k (x) =−1,h′(x) 6= yW ,x ∈ Rk−1)| ≤ εk/32\nCombining, we get:\n(PSk(ĥ d f k (x) =−1,h(x) 6= yW ,x ∈ Rk−1)−PSk(ĥ d f k (x) =−1,h′(x) 6= yW ,x ∈ Rk−1)) (30)\n− (PSk(ĥ d f k (x) =−1,h(x) 6= yO,x ∈ Rk−1)−PSk(ĥ d f k (x) =−1,h′(x) 6= yO,x ∈ Rk−1))≤ εk/16\nWe now show the labels inferred in the region X \\Rk−1 is “favorable” to the classifiers whose excess error is at most εk/2. By triangle inequality,\nPSk(h(x) 6= yO,x /∈ Rk−1)−PSk(h∗(x) 6= yO,x /∈ Rk−1)≤ PSk(h(x) 6= h∗(x),x /∈ Rk−1)\nBy Lemma 6, since h′ has excess error at most εk, h′ agrees with h∗ on all x inside X \\Rk−1 on event Fk−1, hence PSk(h ′(x) 6= h∗(x),x /∈ Rk−1) = 0. This gives\nPSk(h(x) 6= yO,x /∈ Rk−1)−PSk(h′(x) 6= yO,x /∈ Rk−1) ≤ PSk(h(x) 6= h∗(x),x /∈ Rk−1)−PSk(h′(x) 6= h∗(x),x /∈ Rk−1) (31)\nCombining Equations (30) and (31), we conclude that\nerr(h,Sk)− err(h′,Sk)≤ err(h, Ŝk)− err(h′, Ŝk)+ εk/16\nThis establishes the correctness of Invariant 1.\nProof of Invariant 2. Recall by definition of E2k the following concentration results hold for all t ∈ N:\n|(err(h,Stk)− err(h′,Stk))− (errD(h)− errD(h′))| ≤ σ(2t ,δ tk)+ √ σ(2t ,δ tk)ρStk(h,h ′))\nIn particular, for iteration t0(k) we have\n|(err(h,St0(k)k )− err(h′,S t0(k) k ))− (errD(h)− errD(h′))| ≤ σ(2t0(k),δ t0(k) k )+\n√\nσ(2t0(k),δ t0(k)k )ρSt0(k)k (h,h′)\nRecall that Ŝk = Ŝ t0(k) k , ĥk = ĥ t0(k) k , and σk = σ(2 t0(k),δ t0(k)k ), hence the above is equivalent to\n|(err(h,Sk)− err(h′,Sk))− (errD(h)− errD(h′))| ≤ σk + √ σkρSk(h,h′) (32)\nEquation (32) establishes the correctness of Equation (17) of Invariant 2. Equation (18) of Invariant 2 follows from Equation (28).\n(2) We define h̃k = argminh∈H errD̂k(h), and define ν̃k to be errD̂k(h̃k). To prove the bound on the number of label requests, we first claim that if t is sufficiently large that\nσ(2t ,δ tk)+ √ σ(2t ,δ tk)ν̃k ≤ εk/1536 (33)\nthen the algorithm will satisfy the breaking criterion at line 14 of Algorithm 3, that is, for this value of t,\nσ(2t ,δ tk)+ √ σ(2t ,δ tk)err(ĥt , Ŝ t k)≤ εk/512 (34)\nIndeed, by definition of E2k , if event Fk happens,\nerr(h̃k, Ŝ t k)\n≤ errD̂k(h̃k)+σ(2 t ,δ tk)+\n√\nσ(2t ,δ tk)errD̂k(h̃k)\n= ν̃k +σ(2t ,δ tk)+ √ σ(2t ,δ tk)ν̃k (35)\nTherefore,\nσ(2t ,δ tk)+ √ σ(2t ,δ tk)err(ĥ t k, Ŝ t k)\n≤ σ(2t ,δ tk)+ √ σ(2t ,δ tk)err(h̃k, Ŝ t k) ≤ σ(2t ,δ tk)+ √ σ(2t ,δ tk)(2ν̃k +2σ(2t ,δ t k)) ≤ 3σ(2t ,δ tk)+2 √\nσ(2t ,δ tk)ν̃k ≤ εk/512\nwhere the first inequality is from the optimality of ĥtk, the second inequality is from Equation (35), the third inequality is by algebra, the last inequality follows from Equation (33). The claim follows. Next, we solve for the minimum t that satisfies (33), which is an upper bound of t0(k). Fact 1 implies that there is a numerical constant c3 > 0 such that\n2t0(k) ≤ c3 ν̃k + εk\nε2k (d ln 1 εk + ln 1 δk ))\nThus, there is a numerical constant c4 > 0 such that\nt0(k)≤ c4(lnd + ln 1 εk + ln ln 1 δk )\nHence, there is a numerical constant c5 > 0 (that does not depend on k) such that the following holds. If event Fk happens, then the number of label queries made by Algorithm 3 to O can be bounded as follows:\nmk,2 = t0(k)\n∑ t=1 |St,Uk ∩{x : ĥ d f k (x) = +1}∩Rk−1|\n= t0(k)\n∑ t=1 2tPS tk (ĥ d f k (x) = +1,x ∈ Rk−1)\n≤ t0(k)\n∑ t=1 2t(2PD (ĥ d f k (x) = +1,x ∈ Rk−1)+2 ·4 ln 2δ tk 2t )\n≤ 4 ·2t0(k)PD(ĥd fk (x) = +1,x ∈ Rk−1)+8 · t0(k) ln 2\nδ t0(k)k\n≤ c5 ( ( (ν̃k + εk)PD (ĥ\nd f k (x) = +1,x ∈ Rk−1)\nε2k +1) ·d(ln2 1 εk + ln2 1 δk ) )\n≤ c5 ( ( (ν̃k + εk) ·6(α(2ν + εk−1,εk/512)+ εk/1024) ε2k +1) ·d(ln2 1 εk + ln2 1 δk ) )\nwhere the second equality is from the fact that |St,Uk ∩ {x : ĥ d f k (x) = −1}∩Rk−1| = |S t,U k | ·PS tk (ĥ d f k (x) = −1,x ∈ Rk−1), in conjunction with |St,Uk |= 2t ; the first inequality is by definition of E2k , the second and third inequality is from algebra that t0(k) ln 1\nδ t0(k)k ≤ c5d(ln2 1εk + ln 2 1 δk ) for some constant c5 > 0, along with the\nchoice of c2, the fourth step is from Lemma 7 which states that Invariant 3 holds at epoch k. What remains to be argued is an upper bound on ν̃k. Note that\nν̃k = min\nh∈H [PD(ĥ\nd f k (x) =−1,h(x) 6= yW ,x ∈ Rk−1)+PD(ĥ d f k (x) = +1,h(x) 6= yO,x ∈ Rk−1)+PD(h(x) 6= h∗(x),x /∈ Rk−1)]\n≤ PD(ĥd fk (x) =−1,h∗(x) 6= yW ,x ∈ Rk−1)+PD(ĥ d f k (x) = +1,h ∗(x) 6= yO,x ∈ Rk−1) ≤ PD(ĥd fk (x) =−1,h∗(x) 6= yO,x ∈ Rk−1)+PD(ĥ d f k (x) = +1,h\n∗(x) 6= yO,x ∈ Rk−1)+ εk/64 ≤ PD(ĥd fk (x) =−1,h∗(x) 6= yO,x ∈ Rk−1)+PD(ĥ d f k (x) = +1,h\n∗(x) 6= yO,x ∈ Rk−1)+PD(h(x) 6= yO,x /∈ Rk−1)+ εk/64 = ν + εk/64\nwhere the first step is by definition of errD̂k(h), the second step is by the suboptimality of h ∗, the third step is by Equation (29), the fourth step is by adding a positive term PD(h(x) 6= yO,x /∈ Rk−1), the fifth step is by definition of errD(h). Therefore, we conclude that there is a numerical constant c2 > 0, such that mk,2, the number of label requests to O in Algorithm 3 is at most\nc2 ((ν + εk)(α(2ν + εk−1,εk/512)+ εk) ε2k ·d(ln2 1 εk + ln2 1 δk ) )"
    }, {
      "heading" : "B.4 Putting It Together – Consistency and Label Complexity",
      "text" : "Proof of Lemma 2. With foresight, pick c0 > 0 to be a large enough constant. We prove the result by induction.\nBase case. Consider k = 0. Recall that F0 is defined as\nF0 = { for all h,h′ ∈H , |(err(h,S0)−err(h′,S0))−(errD(h)−errD(h′))| ≤σ(n0,δ0)+ √ σ(n0,δ0)ρS0(h,h′) }\nNote that by definition in Subsection A.3, Ŝ0 = S0. Therefore Invariant 1 trivially holds. When F0 happens, Equation (17) of Invariant 2 holds, and n0 is such that √ σ0 ≤ ε0/1024, thus,\nσ0 + √ σ0err(ĥ0, Ŝ0)≤ ε0/512\nwhich establishes the validity of Equation (18) of Invariant 2. Meanwhile, the number of label requests to O is\nn0 = 64 ·10242(d ln(512 ·10242)+ ln 96 δ ))≤ c0(d + ln 1 δ )\nInductive case. Suppose the claim holds for k′ < k. The inductive hypothesis states that Invariants 1,2,3 hold in epoch k− 1 on event Fk−1. By Lemma 7 and Lemma 8, Invariants 1,2,3 holds in epoch k on event\nFk. Suppose Fk happens. By Lemma 7, there is a numerical constant c1 > 0 such that the number of label queries in Algorithm 2 in line 12 is at most\nmk,1 ≤ c1 ( PU(x ∈ ∆(2ν + εk−1)) εk (d′ ln 1 εk + ln 1 δk ) )\nMeanwhile, by Lemma 8, there is a numerical constant c2 > 0 such that the number of label queries in Algorithm 3 in line 14 is at most\nmk,2 ≤ c2 ((α(2ν + εk−1,εk/512)+ εk)(ν + εk) ε2k ·d(ln2 1 εk + ln2 1 δk ) )\nThus, the number of label requests in total at epoch k is at most\nmk = mk,1 +mk,2\n≤ c0 ( ( α(2ν + εk−1,εk/512)+ εk)(ν + εk)\nε2k d(ln2 1 εk + ln2 1 δk )+ PU(x ∈ ∆(2ν + εk−1)) εk (d′ ln 1 εk + ln 1 δk ) )\nThis completes the induction.\nTheorem 3 (Consistency). If Fk0 happens, then the classifier ĥ returned by Algorithm 1 is such that\nerrD(ĥ)− errD(h∗)≤ ε\nProof. By Lemma 2, Invariants 1, 2, 3 hold at epoch k0. Thus by Lemma 5,\nerrD(ĥ)− errD(h∗) = errD(ĥk0)− errD(h∗)≤ εk0/8 ≤ ε\nProof of Theorem 1. This is an immediate consequence of Theorem 3.\nTheorem 4 (Label Complexity). If Fk0 happens, then the number of label queries made by Algorithm 1 to O is at most\nÕ((sup r≥ε α(2ν + r,r/1024) 2ν + r )d( ν2 ε2 +1)+ (sup r≥ε PU(x ∈ ∆(2ν + r)) 2ν + r )d′( ν ε +1))\nProof. Conditioned on event Fk0 , we bound the sum ∑ k0 k=0 mk.\nk0\n∑ k=0 mk\n≤ c0(d + ln 1 δ )+ c0\n( k0\n∑ k=1 (α(2ν + εk−1,εk/512)+ εk)(ν + εk) ε2k d(ln2 1 εk + ln2 1 δk )+ PU(x ∈ ∆(2ν + εk−1)) εk (d′ ln 1 εk + ln 1 δk ) )\n≤ c0(d + ln 1 δ )+ c0\n( k0\n∑ k=1 (α(2ν + εk−1,εk/512)+ εk)(ν + εk) ε2k d(3ln2 1 ε + 2ln2 1 δ )+ PU(x ∈ ∆(2ν + εk−1)) εk (2d′ ln 1 ε + ln 1 δ ) )\n≤ (sup r≥ε α(2ν + r,r/1024)+ r 2ν + r )d(3ln2 1 ε + 2ln2 1 δ )\nk0\n∑ k=0\n(ν + εk)2\nε2k + sup r≥ε\nPU(x ∈ ∆(2ν + r)) 2ν + r (2d′ ln 1 ε + ln 1 δ )\nk0\n∑ k=0 (ν + εk) εk\n≤ Õ((sup r≥ε α(2ν + r,r/1024)+ r 2ν + r )d( ν2 ε2 + 1)+ (sup r≥ε PU(x ∈ ∆(2ν + r)) 2ν + r )d′( ν ε + 1))\nwhere the first inequality is by Lemma 2, the second inequality is by noticing for all k ≥ 1, ln2 1εk + ln 2 1 δk ≤ 3ln2 1ε +2ln 2 1 δ and d ′ ln 1εk + ln 1 δk ≤ 2d ′ ln 1ε + ln 1 δ , the rest of the derivations follows from standard algebra.\nProof of Theorem 2. Item 1 is an immediate consequence of Lemma 2, whereas item 2 is a consequence of Theorem 4."
    }, {
      "heading" : "C Case Study: Linear Classfication under Uniform Distribution over Unit Ball",
      "text" : "We remind the reader the setting of our example in Section 4. H is the class of homogeneous linear separators on the d-dimensional unit ball and H d f is defined to be {h∆h′ : h,h′ ∈ H }. Note that d′ is at most 5d. Furthermore, U is the uniform distribution over the unit ball. O is a deterministic labeler such that errD(h∗) = ν > 0, W is such that there exists a difference classifier h̄d f with false negative error 0 for which PrU(h̄d f (x) = 1)≤ g = o( √ dν). We prove the label complexity bound provided by Corollary 1.\nProof of Corollary 1. We claim that under the assumptions of Corollary 1, α(2ν + r,r/1024) is at most g. Indeed, by taking hd f = h̄d f , observe that\nP(h̄d f (x) =−1,yW 6= yO,x ∈ ∆(2ν + r))≤ P(h̄d f (x) =−1,yW 6= yO) = 0\nP(h̄d f (x) = +1,x ∈ ∆(2ν + r))≤ g This shows that α(2ν + r,0)≤ g. Hence, α(2ν + r,r/1024) ≤ α(2ν + r,0)≤ g. Therefore,\nsup r:r≥ε α(2ν + r,r/1024)+ r 2ν + r ≤ sup r≥ε g+ r ν + r ≤ max( g ν ,1)\nRecall that the disagreement coefficient θ(2ν + r) ≤ √\nd for all r, and d′ ≤ 5d. Thus, by Theorem 2, the number of label queries to O is at most\nÕ\n(\nd max( g ν ,1)(\nν2 ε2 +1)+d3/2 ( 1+ ν ε ) )"
    }, {
      "heading" : "D Performance Guarantees for Learning with Respect to Data labeled by O and W",
      "text" : "An interesting variant of our model is to consider learning from data labeled by a mixture of O and W . Let DW be the distribution over labeled examples determined by U and W , specifically, PDW (x,y) = PU(x)PW (y|x). Let D′ be a mixture of D and DW , specifically D′ = (1−β )D+βDW , for some parameter β > 0. Define h′ to be the best classifier with respect to D′, and denote by ν ′ the error of h′ with respect to D′.\nLet O′ be the following mixture oracle. Given an example x, the label yO′ is generated as follows. O′ flips a coin with bias β . If it comes up heads, it queries W for the label of x and returns the result; otherwise O is queried and the result returned. It is immediate that the conditional probability induced by O′ is PO′(y|x) = (1−β )PO(y|x)+βPW (y|x), and D′(x,y) = PO′(y|x)PU (x).\nAssumption 2. For any r,η > 0, there exists an hd fη ,r ∈ H d f with the following properties:\nPD(h d f η ,r(x) =−1,x ∈ ∆(r),yO′ 6= yW )≤ η\nPD (h d f η ,r(x) = 1,x ∈ ∆(r))≤ α ′(r,η)\nRecall that the disagreement coefficient θ(r) at scale r is θ(r) = suph∈H supr′≥r PU (DIS(BU (h,r′))\nr′ , which only depends on the unlabeled data distribution U and does not depend on W or O.\nWe have the following corollary.\nCorollary 3 (Learning with respect to Mixture). Let d be the VC dimension of H and let d′ be the VC dimension of H d f . If Assumption 2 holds, and if the error of the best classifier in H on D′ is at most ν ′. Algorithm 1 is run with inputs unlabeled distribution U, target excess error ε , confidence δ , labeling oracle O′, weak oracle W, hypothesis class H , hypothesis class for difference classifier H d f , confidence δ . Then with probability ≥ 1−2δ , the following hold:\n1. the classifier ĥ output by Algorithm 1 satisfies: errD′(ĥ)≤ errD′(h′)+ ε .\n2. the total number of label queries made by Algorithm 1 to the oracle O is at most:\nÕ ( (1−β ) (\nsup r≥ε α ′(2ν ′+ r,r/1024)+ r 2ν ′+ r ·d ( ν ′2 ε2 +1 ) +θ(2ν ′+ ε)d′ ( ν ′ ε +1 ) ))\nProof Sketch. Consider running Algorithm 1 in the setting above. By Theorem 1 and Theorem 2, there is an event F such that P(F)≥ 1−δ , if event F happens, ĥ, the classifier learned by Algorithm 1 is such that\nerrD′(ĥ)≤ errD′(h′)+ ε\nBy Theorem 2, the number of label requests to O′ is at most\nmO′ = Õ (\nsup r≥ε α ′(2ν ′+ r,r/1024)+ r 2ν ′+ r ·d ( ν ′2 ε2 +1 ) +θ(2ν ′+ ε)d′ ( ν ′ ε +1 ) )\nSince O′ is simulated by drawing a Bernoulli random variable Zi ∼ Ber(1−β ) in each call of O′, if Zi = 1, then return O(x), otherwise return W (x). Define event\nH = { mO′\n∑ i=1 Zi ≤ 2((1−β )mO′ +4ln 2 δ )}\nby Chernoff bound, P(H)≥ 1−δ . Consider event J = F ∩H , by union bound, P(J)≥ 1−2δ . Conditioned on event J, the number of label requests to O is at most ∑mO′i=1 Zi, which is at most\nÕ ( (1−β ) (\nsup r≥ε α ′(2ν ′+ r,r/1024)+ r 2ν ′+ r ·d ( ν ′2 ε2 +1 ) +θ(2ν ′+ ε)d′ ( ν ′ ε +1 ) ))"
    }, {
      "heading" : "E Remaining Proofs",
      "text" : "Proof of Fact 2. (1) First by Lemma 1, PD(x ∈ Rk−1)/2 ≤ p̂k ≤ PD(x ∈ Rk−1) holds with probability 1− δk/6. Second, for each classifier hd f ∈ H d f , define functions f 1hd f , and f 2hd f associated with it. Formally,\nf 1hd f (x,yO,yW ) = I(h d f (x) =−1,yO 6= yW )\nf 2hd f (x,yO,yW ) = I(h d f (x) = +1)\nConsider the function class F 1 = { f 1hd f : hd f ∈ H d f }, F 2 = { f 2hd f : hd f ∈ H d f }. Note that both F 1 and F 2 have VC dimension d′, which is the same as H d f . We note that A ′k is a random sample of size mk drawn iid from Ak. The fact follows from normalized VC inequality on F 1 and F 2 and the choice of mk in Algorithm 2 called in epoch k, along with union bound.\nProof of Fact 3. For fixed t, we note that Stk is a random sample of size 2 t drawn iid from D. By Equation (8), for any fixed t ∈ N,\nP\n(\nfor all h,h′ ∈ H , |(err(h,Stk)− err(h′,Stk))− (errD(h)− errD(h′))| ≤ σ(2t ,δ tk)+ √ σ(2t ,δ tk)ρStk(h,h ′) ) ≥ 1− δ tk/8 (36)\nMeanwhile, for fixed t ∈N, note that Ŝtk is a random sample of size 2t drawn iid from D̂k. By Equation (8),\nP\n(\nfor all h,h′ ∈ H ,err(h, Ŝtk)− errD̂k(h)≤ σ(2 t ,δ tk)+\n√ σ(2t ,δ tk)errD̂k(h) ) ≥ 1−δ tk/8 (37)\nMoreover, for fixed t ∈ N, note that S tk is a random sample of size 2t drawn iid from D . By Equation (12),\nP\n(\nPS tk (ĥd fk (x) =−1,yO 6= yW ,x ∈ Rk−1)≤ PD(ĥ d f k (x) =−1,yO 6= yW ,x ∈ Rk−1)\n+\n√\nγ(2t ,δ tk)PD (ĥ d f k (x) =−1,yO 6= yW ,x ∈ Rk−1)+ γ(2t ,δ tk)\n)\n≥ 1−δ tk/8 (38)\nFinally, for fixed t ∈ N, note that S tk is a random sample of size 2t drawn iid from D . By Equation (12),\nP\n(\nPS tk (ĥd fk (x)=−1,x∈Rk−1)≤PD(ĥ d f k (x)=−1,x∈Rk−1)+\n√\nPD(ĥ d f k (x) =−1,x ∈ Rk−1)γ(2t ,δ tk)+γ(2t ,δ tk)\n)\n≥ 1−δ tk/8 (39)\nNote that by algebra,\nPD(ĥ d f k (x)=−1,x∈Rk−1)+\n√\nPD (ĥ d f k (x) =−1,x ∈ Rk−1)γ(2t ,δ tk)+γ(2t ,δ tk)≤ 2(PD(ĥ d f k (x)=−1,x∈Rk−1)+γ(2t ,δ tk))\nTherefore,\nP\n(\nPS tk (ĥd fk (x) =−1,x ∈ Rk−1)≤ 2(PD (ĥ d f k (x) =−1,x ∈ Rk−1)+ γ(2t ,δ tk))\n)\n≥ 1−δ tk/12 (40)\nThe proof follows by applying union bound over Equations (36), (37), (38) and (40) and t ∈N.\nWe emphasize that S tk is chosen iid at random after ĥ d f k is determined, thus uniform convergence argu-\nment over H d f is not necessary for Equations (38) and (40).\nProof of Fact 4. By induction on k.\nBase Case. For k = 0, it follows directly from normalized VC inequality that P(F0)≥ 1−δ0.\nInductive Case. Assume P(Fk−1)≥ 1−δ0 − . . .−δk−1 holds. By union bound,\nP(Fk)≥ P(Fk−1 ∩E1k ∩E2k )≥ P(Fk−1)−δk/2−δk/2 ≥ P(Fk−1)−δk\nHence, P(Fk)≥ 1−δ0 − . . .−δk. This finishes the induction. In particular, P(Fk0)≥ 1−δ0 − . . .δk0 ≥ 1−δ ."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "An active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as few label queries as possible. This work addresses active learning with labels obtained from strong and weak labelers, where in addition to the standard active learning setting, we have an extra weak labeler which may occasionally provide incorrect labels. An example is learning to classify medical images where either expensive labels may be obtained from a physician (oracle or strong labeler), or cheaper but occasionally incorrect labels may be obtained from a medical resident (weak labeler). Our goal is to learn a classifier with low error on data labeled by the oracle, while using the weak labeler to reduce the number of label queries made to this labeler. We provide an active learning algorithm for this setting, establish its statistical consistency, and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone.",
    "creator" : "LaTeX with hyperref package"
  }
}