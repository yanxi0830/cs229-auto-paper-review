{
  "name" : "1205.4893.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the practically interesting instances of MAXCUT",
    "authors" : [ "Yonatan Bilu", "Amit Daniely", "Michael Saks" ],
    "emails" : [ "yonatan@gmail.com", "amit.daniely@math.huji.ac.il", "nati@cs.huji.ac.il", "saks@math.rutgers.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Following [BL], we apply this perspective to MAXCUT, viewed as a clustering problem. Using a variety of techniques, we investigate practically interesting instances of this problem. Specifically, we show how to solve in polynomial time distinguished, metric, expanding and dense instances of MAXCUT under mild stability assumptions. In particular, (1 + )-stability (which is optimal) suffices for metric and dense MAXCUT. We also show how to solve in polynomial time Ω( √ n)-stable instances of MAXCUT, substantially improving the best previously known result.\n∗Parasight inc, Agudat sport hapoel 1, Jerusalem, Israel. yonatan@gmail.com †Department of Mathematics, Hebrew University, Jerusalem 91904, Israel. Supported in part by a binational Israel-USA grant 2008368. amit.daniely@math.huji.ac.il ‡School of Computer Science and Engineering, Hebrew University, Jerusalem 91904, Israel. Supported in part by a binational Israel-USA grant 2008368. nati@cs.huji.ac.il §Department of Mathematics, Rutgers University, Piscataway, NJ 08854. Supported in part by NSF under grant CCF-0832787 and by a binational Israel-USA grant 2008368. saks@math.rutgers.edu.\nar X\niv :1\n20 5.\n48 93\nv1 [\ncs .C\nC ]\n2 2\nM ay\n2 01"
    }, {
      "heading" : "1 Introduction",
      "text" : "The primary criterion used in computational complexity to evaluate algorithms is worst case behavior, so that a problem is infeasible if no efficient algorithm can solve all its instances. In practice, this approach is often overly pessimistic, and a more realistic (but fuzzy) criterion would be to say that a problem is feasible if there is an efficient algorithm that correctly solves all of its practically interesting instances. The difference can be very substantial, since for many computational problems, the vast majority of instances are completely irrelevant for practical purposes.\nAn important case in point is clustering, where one seeks a meaningful partition of a given set of data. Almost every formal manifestation of the clustering problem is NP -Hard, yet, a clustering instance is of practical interest only if the data can indeed be partitioned in a meaningful way. Random instances are not likely to have a meaningful partition, so data sets with a meaningful partition are very special. Thus, even if no efficient algorithm can find the optimal partition for every data set, this does not imply that clustering is hard in practice. As Tali Tishby put it in conversation many years ago, many practitioners hold the opinion that ”clustering is either easy or pointless”. That is, for a data sets that admit a meaningful partition of the data, finding it is not hard.\nCan this intuition be put on a solid theoretical foundation? Bilu and Linial [BL] proposed a framework for studying this issue. Generally speaking, their approach pertains to optimization problems with a continuous input space and discrete solution space. They proposed two criteria for an optimal solution to be evidently optimal. A solution is stable if it remains optimal under moderate perturbations of the input. A solution is distinguished if a transition to another solution reduces the value of the objective function in proportion to the distance between the two solutions. Concretely, they considered the case where the input is a weighted graph and the candidate solutions are cuts (or more generally, partitions). Here, a cut is γ-stable (for γ ≥ 1) if it remains optimal even if each input weight wij is perturbed to a value between wij and γwij . A cut is α-distinguished (for α ≥ 0) if moving to any other cut reduces the objective function by at least α times the sum of (weighted) degrees of the vertices that switched side.\nFollowing Bilu and Linial [BL], we apply these notions to the study of the (weighted) MAXCUT problem. We also investigate the more restricted problem of Metric-MAXCUT1 which arises often in the field of machine learning. Our main results are:\nTheorem 1.1 1. For every > 0 there is a polynomial time algorithm that correctly solves all (1 + )-locally stable instances of Metric-MAXCUT.\n2. For every > 0 and C > 1 there is a polynomial time algorithm that correctly solves all (1 + )-locally stable and C-dense instances of MAXCUT.\nThe condition of C-density rules out overly-weighted edges. The notion of γ-local stability is a substantial weakening of γ-stability. It is defined similarly, but we only require resilience to perturbations that modify edges which are all incident with the same vertex.\nTheorem 1.2 There is a polynomial time algorithm that solves all instances of MAXCUT that are\n1. α-distinguished and γ-locally stable with γ > 2 1− √ 1−α2 , or\n2. γ-locally stable with γ > 2 1− √ 1−h2 .\n1That is, MAXCUT, restricted to instances where the weight function is a metric.\nHere h is the Cheeger constant of the maximal cut.\nThis substantially improves a result from [BL] that works only for regular graphs and requires that γ > 5+ √\n1−α2 1− √ 1−α2 or γ > 5+ √ 1−h2 1− √ 1−h2 . It is also shown in [BL] that n-stable instances are feasible. Here we\nderive the same conclusion under the weaker (but still impractical) assumption of Ω( √ n)-stability.\nTheorem 1.3 There is a polynomial time algorithm that finds the optimal solution for every Ω( √ n)-stable instance of MAXCUT.\nSome notation and terminology\nHere the input to the MAXCUT problem is the complete graph on n vertices G = (V,E) along with a symmetric function with zero diagonal w : V × V → R+. Expressions such as ”w is bipartite” refer to the graph which is the support of w, which is always assumed to be connected. Our purpose is to find a cut (S, S̄), S ⊆ V for which ∑ a∈S, b∈S̄ w(a, b) is maximized.\nFix a cut (S, S̄). We use the self-explanatory terms “the vertices x, y are on the same side” or “separated” by this cut. We call the edge xy a cut edge or a non-cut edge when x, y are separated resp. on the same side of the cut. For A,B ⊂ V , we denote E(A,B) = {ab|a ∈ A, b ∈ B} and w(A,B) := ∑ uv∈E(A,B)w(u, v). Also τw(A) = τ(A) = w(A, Ā) and µ(A) = µw(A) = w(A, V ).\nLet A ⊆ V . We denote by ξ(A), the weight of the cut edges emanating from S, i.e., ξ(A) =∑ vu∈E(A,Ā)∩E(S,S̄)w(u, v) and by ι(A) = τ(A)− ξ(A) the weight of the non-cut edges. We slightly abuse notation for singletons A = {v} and pairs A = {u, v} and write τ(v) or ι(e) etc., where e = uv. The minimal, maximal and average degree of w are denoted by δ(w) = minv∈V µ(v),\nδ̄(w) = maxv∈V µ(v) and δ(w) = ∑ v∈V µ(v) n respectively. (The potentially confused reader may find the following Greek-mathematical dictionary useful: τ stands for “total”, ξ for “external” and ι for “internal”)."
    }, {
      "heading" : "1.1 Stable instances",
      "text" : "Definition 1.4 Let w : V × V → [0,∞) be an instance of MAXCUT and let γ ≥ 1. An instance w′ : V × V → [0,∞) is a γ-perturbation of w if\n∀u, v ∈ V, w(u, v) ≤ w′(u, v) ≤ γ · w(u, v)\nAn instance w is said to be γ-stable if there is a cut which forms a maximal cut for every γperturbation w′ of w.\nDefinition 1.5 Let γ ≥ 1. An instance w : V × V → [0,∞) for MAXCUT is γ-locally stable if there is a maximal cut (S, S̄) for which it is impossible to obtain a larger cut by switching the side of some vertex x and multiplying the edges in E(x, V \\ {x}) by numbers between 1 and γ.\nThe definitions of stability and local stability capture the intuition of an “evidently optimal” solution. The following more concrete equivalent definitions are usually more convenient to use.\nObservation 1 [BL] Let w : V × V → R be an instance of MAXCUT and let γ ≥ 1.\n• The instance w is γ-stable iff there is a maximal cut for which ξ(A) ≥ γ · ι(A) for every A ⊂ V .\n• The instance w is γ-locally stable iff there is a maximal cut for which ξ(x) ≥ γ · ι(x) for every x ∈ V .\nWe say that a (not necessarily maximal) cut (S, S̄) is γ-stable (resp. γ-locally stable) if the first (resp. second) condition in Observation 1 holds.\nAs Observation 1 shows, every instance is 1-stable, and being γ-stable for some γ > 1 is equivalent to having a unique maximal cut2. Finally, an instance is bipartite iff it is γ-stable for every γ ≥ 1. Thus, γ-stability is seen to be a relaxation of being bipartite.\nStability and local stability are quite different. As mentioned, for γ > 1 every instance has at most one γ-stable cut. On the other hand, there can be numerous γ-locally stable cuts: Consider the instance where w = 1 on the edges of a perfect matching and > 0 elsewhere. As → 0, the local stability tends to ∞. Yet, this instance is not γ-stable for any γ > 1. It is easy to check that this instance has exponentially many γ-locally stable maximal cuts. From the computational perspective the two properties are very different as well. Thus MAXCUT remains NP -hard even under arbitrarily high local stability (see [BL]), whereas we show here how to efficiently solve Ω( √ n)-stable instances. Also, it is easy to decide whether a given cut is γ-locally stable, but we do not know how to decide whether a given cut is γ-stable and we suspect that this problem is hard. In Section 4, following a simplified version of the algorithm in [BL] for Ω(n)-stable instances, we present a deterministic algorithm that solves every Ω( √ n)-stable instance, proving Theorem 1.3."
    }, {
      "heading" : "1.2 Distinguished and Expanding instances",
      "text" : "Let w : V × V → R+ be an instance of MAXCUT whose (unique) maximal cut is (S, S̄). We note that if all vertices of A ⊂ V switch side, then the weight of the cut decreases by ξ(A)− ι(A). Thus, we define\nDefinition 1.6 An instance w of MAXCUT is α-distinguished for α ≥ 0 if for every ∅ 6= A ⊂ V , ξ(A)− ι(A) ≥ α ·min{µ(A), µ(Ā)}.\nNote that every instance is 0-distinguished and being α-distinguished with α > 0 is equivalent to having a unique maximal cut. It is not hard to see that 1+α1−α -local stability is equivalent to α-local distinction, namely ξ(x)− ι(x) ≥ α · µ(x) for every x ∈ V .\nDistinction vs Stability. Let (S, S̄) be a maximal cut of w : V ×V → [0,∞). On the one hand, every α-distinguished instance is 1+α1−α -stable, because ξ(A)− ι(A) ≥ αµ(A) ≥ α(ξ(A) + ι(A)). On the other hand, highly stable instances need not be distinguished as the following bipartite example with V = {a1, . . . , an}∪̇{b1, . . . , bn} shows. Here w(ai, bj) is 1 when i = j and 1 otherwise. Clearly w is ∞-stable. Yet, switching the sides of all the vertices in {a1, . . . , an\n2 } ∪ {b1, . . . , bn 2 }\ndecreases the weight of the cut only slightly. Such examples motivate the stronger notion of distinction. Although the cut ({a1, . . . , an}, {b1, . . . , bn}) is infinitely stable, its optimality does not seem completely evident.\nDistinction and Expansion. Call w : V × V → R+ β-expanding if β ≤ h(w) where h(w) = min∅6=A⊂V τ(A) min{µ(A),µ(Ā)} is w’s Cheeger constant. An α-distinguished instance is αexpanding, though highly expanding instances can even have multiple maximal cuts. However, an instance that is both γ-stable and β-expanding is easily seen to be (β · γ−1γ+1)-distinguished. As this discussion implies, distinction is a conjunction of stability and expansion.\nIn section 3 we prove Theorem 1.2, using a spectral result from [BL]. In the appendix we re-derive this result and point out its close relation to the Geomans-Williamson algorithm [GW] and other spectral techniques.\n2To see that, note that if (S, S̄) is a γ-stable cut and (T, T̄ ) is another cut then w(T, T̄ ) = w(S, S̄)− ξ((T ∩ S) ∪ (T̄ ∩ S̄)) + ι((T ∩ S) ∪ (T̄ ∩ S̄)) ≤ w(S, S̄)− γ−1\nγ+1 τ((T ∩ S) ∪ (T̄ ∩ S̄)) < w(S, S̄)."
    }, {
      "heading" : "1.3 Metric and Dense instances",
      "text" : "In Section 2 we study metric instances. This is done through a reduction from metric to dense instances, so we consider such instances as well (Section 2.1).\nWe call w : V ×V → R C-dense for C ≥ 1 if ∀x, y ∈ V, w(x, y) ≤ C · τ(x)n . As shown in [AKK], for C > 1 fixed, C-dense MAXCUT is NP -Hard, but it has a PTAS. As we show, this PTAS can be adapted to correctly solve all instances of MAXCUT that are (1 + )-locally stable and C-dense for every > 0, C > 1. The algorithm samples O(log n) vertices and tests each of their bipartitions as a seed to a cut. As we show, w.h.p., one of the resulting cuts is the maximal cut, proving the second part of Theorem 1.1.\nIn Section 2.2 we deal with Metric-MAXCUT. As shown in [VK] (with credit to L. Trevisan) Metric-MAXCUT is NP -Hard. That paper also gives a reduction from metric to (4 + o(1))-dense instances of MAXCUT, thus yielding a PTAS for Metric-MAXCUT. We show that a slight variation of this reduction preserves local stability3, and therefore yields an efficient algorithms for (1 + )- locally stable instances of Metric-MAXCUT, proving Theorem 1.1 in full.\nThis algorithm for metric instances is far from being a practically applicable clustering method. Even though it is polynomial-time, the actual run times are prohibitively high. We view this more as an invitation to seek practical algorithms for γ-stable instances of metric MAXCUT for some reasonable values of γ. Specifically we provide such an algorithm for (3 + )-locally stable metric instances."
    }, {
      "heading" : "1.4 Relation with other work",
      "text" : "Smoothed analysis is the best known example of a method for analyzing instances of computational problems based on their practical significance. As this method shows [ST], a certain variant of the simplex algorithm solves in polynomial time almost every input. Even closer to our theme are several recent papers on clustering. In [ABS] polynomial time algorithms are given for 3-stable instances of k-means, k-medians and other “center based” clustering problems. The constant 3 was improved in [BL2] to (1 + √ 2) for k-median. The papers [DLS, AB, BBV] consider data sets that admit a good clustering and show how to cluster them efficiently. Also related to our work are the planted partition model [B] and semirandom model [FK] for MAXCUT. In these models instances are generated by splitting the vertices at random V = S∪̇S̄. Edges in S× S̄ (resp. S×S ∪ S̄× S̄) are picked with probability p, resp. q < p. In the semirandom model we also allow an adversary to add edges to S × S̄ and drop edges from S × S ∪ S̄ × S̄. As shown in [B, FK], a.a.s., (S, S̄) is the maximal cut and it can be efficiently found using certain algorithms. It not hard to see that for fixed p and q, this is a consequence of Theorem 1.1. The planted partition model is a random model that usually generates instances with a good partition, and those can be efficiently found. The semirandom model goes further by allowing an adversary to modify the input in a way that improves the optimal partition. Here we take an additional step forward, since we solve efficiently every instance with a good partition.\n3A word of caution: Our definition of stability and local stability for Metric-MAXCUT is more restrictive than one might think. We require the perturbed instance to satisfy the stability condition whether or not it is metric."
    }, {
      "heading" : "2 Algorithms for locally stable dense and metric instances",
      "text" : ""
    }, {
      "heading" : "2.1 Dense instances",
      "text" : "Theorem 2.1 For every C ≥ 1 and > 0 there is a randomized polynomial time algorithm that correctly solves all (1 + )-locally stable, C-dense instances of MAXCUT.\nThe analysis of the algorithm is based on the following lemma.\nLemma 2.2 Suppose that w : V × V → [0,∞) is a C-dense instance and let (S, S̄) be a γ-locally stable cut. Let X1, . . . , Xm be i.i.d. r.v. that are uniformly distributed on V . For x ∈ V , let Ax be the event that S+ > S−, where S± = ∑ w(x,Xi) over all i s.t. x and Xi are separated resp. on the same side. Then\nPr (∪xAx) ≤ |V | · exp ( −1\n2\n( 1\nC · γ − 1 γ + 1\n)2 ·m ) Proof The lemma follows from Hoeffding’s bound. For every x ∈ V , S+ − S− is a sum of m i.i.d. r.v.’s of expectation ξ(x)−ι(x)|V | ≥ γ−1 γ+1 τ(x) |V | . These r.v.’s are bounded in absolute value, by C · τ(x) |V | .\n2 Proof (Of Theorem 2.1) Let D = 2 ( C · 2+ )2 . Let m = D · ln(2|V |). Take an i.i.d. sample of m uniformly chosen points X1, . . . , Xm ∈ V . By the above lemma, with probability ≥ 0.5, there is a partition {X1, . . . , Xm} = L ∐ R such that the cut defined by S = {x ∈ V : w(x,R) > w(x, L)} is the optimal cut. Since the number of such partitions is (2 · |V |)ln(2)D, there are only polynomially many partitions to consider, yielding an efficient randomized algorithm for the problem.\n2\nCorollary 2.3 For every C ≥ 1 and > 0, a C-dense instance of MAXCUT has only poly(|V |)- many (1 + )-locally stable cuts.\nProof Consider the random cut (S, S̄), sampled as in the proof of Theorem 2.1, where the partition {X1, . . . , Xm} = L ∐ R is chosen uniformly at random. The proof Theorem 2.1 shows that for every (1+ )-locally stable cut (T, T̄ ), the probability that (S, S̄) = (T, T̄ ) is ≥ 0.5 · (2 · |X|)−D ln(2). Thus, there are at most 2 · (2 · |X|)D ln(2) such cuts.\n2"
    }, {
      "heading" : "2.2 Metric instances",
      "text" : "Given an instance w : V × V → [0,∞) of MAXCUT, we split its vertices as follows. Pick a set Ṽ and a surjective map π : Ṽ → V . A MAXCUT instance w̃ on Ṽ is defined as follows:\nw̃(x̃, ỹ) = w(x, y)\n|π−1(x)| · |π−1(y)|\nwhere π(x̃) = x, π(ỹ) = y. It is not hard to prove that\nProposition 2.4 Consider the following map from cuts of w to cuts of w̃ defined by\n(S, S̄) 7→ (π−1(S), π−1(S̄))\nThen\n1. This map preserves weights, stability and local stability of cuts.\n2. Restricted to the locally stable cuts (i.e., γ-locally stable cuts with γ > 1), this is a bijection onto the locally stable cuts of w̃.\n3. It maps maximal cuts to maximal cuts.\nAs the following proposition shows, the above construction is a reduction from metric to (4+o(1))dense instances.\nProposition 2.5 Let w : V × V → [0,∞) be an instance of Metric-MAXCUT with w(V, V ) = 2 · |V |2. Consider the map π : ∐ x∈V [bτw(x)c] → V . The instance w̃ obtained by π is (4 + o(1))dense.\nProof Let x̃, ỹ ∈ Ṽ such that π(x̃) = x, π(ỹ) = y. It is easy to see that (see [VK]) 2 · |V | · τw(x) ≥ w(V, V ), bτw(x)c ≥ ( 1− 1|V | ) τw(x), τw̃(x̃) = τw(x) bτw(x)c ≥ 1 and w(x, y) ≤ 1 |V |(τw(x) + τw(y)). Thus, we have\nw̃(x̃, ỹ) = w(x, y)\nbτw(x)c · bτw(y)c\n≤ 1 (1− 1/|V |)2 · w(x, y) τw(x) · τw(y)\n≤ 1 (1− 1/|V |)2\n· 1 |V | [τw(x) + τw(y)]\nτw(x) · τw(y)\n= 1 (1− 1/|V |)2 · (\n1\n|V |τw(x) +\n1\n|V |τw(y) ) ≤ 1\n(1− 1/|V |)2 · 4 w(V, V )\n≤ 1 (1− 1/|V |)2 · 4 |Ṽ |\n= (4 + o(1)) τw̃(x̃)\n|Ṽ |\n2\nCorollary 2.6 Let > 0.\n1. There is a randomized polynomial time algorithm for (1+ )-locally stable instances of MetricMAXCUT.\n2. The number of (1 + )-locally stable cuts in a metric instance is polynomial in |V |.\n2.2.1 A faster algorithm for (3 + )-stable metric instances\nProposition 2.7 Let (L,R) be a γ-locally stable cut of an instance, w, of Metric-MAXCUT. Then, for every x ∈ L, z ∈ R, w(x, z) ≥ ( γ2−1 γ ) · w(x,R)γ·|R|+|L| .\nProof Using γ-local stability and the triangle inequality we obtain\n1 γ w(x,R) ≥ w(x, L) = ∑ y∈L w(x, y)\n≥ ∑ y∈L (w(z, y)− w(x, z)) = w(z, L)− |L|w(x, z) ≥ γw(z,R)− |L|w(x, z) = γ\n∑ y∈R w(z, y)− |L|w(x, z)\n≥ γ ∑ y∈R (w(y, x)− w(z, x))− |L|w(x, z) = γw(x,R)− γ|R|w(x, z)− |L|w(x, z)\n2\nTheorem 2.8 Let (X,w) be an instance of Metric-MAXCUT and let (L,R) be a γ = (3+ )-locally stable cut with > 0. Then either L or R is a (metric) ball.\nProof W.l.o.g., |L| ≥ n2 . We find some x ∈ L such that ∀z ∈ R, w(z, x) > diam(L), thus proving our claim. Select some x, y ∈ L with w(x, y) = diam(L). For every z ∈ L, we write w(x, y) ≤ w(x, z) +w(y, z). Summing over every z ∈ L, this yields |L| ·w(x, y) ≤ w(x, L) +w(y, L). W.l.o.g., assume that w(x, L) ≥ |L|2 · w(x, y). By local stability,\nw(x, y) ≤ 2 |L| w(x, L) ≤ 2 · w(x,R) γ · |L|\n(1)\nBy proposition 2.7, every z ∈ R satisfies w(x, z) ≥ ( γ2−1 γ ) · w(x,R)γ·|R|+|L| . Combined with equation (1), and the assumptions that γ > 3 and |L| ≥ |R|, we obtain that w(x, z) > w(x, y) as claimed.\n2\nBy Theorem 2.8, the maximal cut of (3 + )-locally stable instances of Metric-MAXCUT can be found by simply considering all O(n2) balls.\nNote 2.9 Theorem 2.8 is tight in the following sense. We show an example of (3− )-stable metric instance (not just locally-stable), where neither side of its maximal cut is a ball, nor can it even be expressed as the union of few balls.\nHere is the example: It is a metric space (X,w) = (L ∐ R,w) where L = {l1, . . . , l2n}, R = {r1, . . . , r2n}. Generally speaking, the distance between two points which are both in L or in R is 1. The distance between a point in L and a point in R is 3, the following are exceptions to the general rule: ∀1 ≤ i ≤ n, w(l2i−1, l2i) = w(r2i−1, r2i) = 2 and ∀1 ≤ i ≤ 2n, w(li, ri) = 2 It is not hard to see that w is a (3− o(1))-stable metric instance and each side of its maximal cut cannot be decomposed into fewer than 2n balls."
    }, {
      "heading" : "3 Distinguished and Expanding Instances",
      "text" : "Let w : V × V → [0,∞) be an instance of MAXCUT with a maximal cut (S, S̄). We identify w with an n×n matrix W , where Wij = w(i, j). Define wcut : V ×V → R by wcut(u, v) = w(u, v) for uv ∈ E(S, S̄) and wcut(u, v) = 0 otherwise. Similarly, denote wuncut = w−wcut. Denote by Wcut and Wuncut the matrices corresponding to wcut and wuncut respectively. Finally, let D\ncut, Duncut, D and D′ be the diagonal matrices defined by Dcutii = ∑ jW cut ij , D uncut ii = ∑ jW uncut ij , D = D cut +Duncut and D′ = Dcut −Duncut.\nLemma 3.1 If w is γ-locally stable where γ > 2 1− √ 1−(h(wcut))2 , then W + D′ is a PSD matrix of rank n− 1.\nAs shown in [BL] there is an efficient algorithm that correctly solves all instances that satisfy the conclusion of the Lemma (As pointed out in the Appendix, the GW-algorithm solves all such instances). This proves the second part of Theorem 1.2. Proof First, we note that it is enough to prove that D− 1 2 (W +D′)D− 1 2 is a PSD matrix of rank n − 1. Let f : V → R be the vector defined by fi = √ Dii for i ∈ S and fi = − √ Dii for i ∈ S̄. Since fTD− 1 2 (W + D′)D− 1 2 f = 0, it is enough to show that vTD− 1 2 (W + D′)D− 1 2 v > 0 for every unit vector v that is orthogonal to f . Note that\nD− 1 2 (W +D′)D− 1 2 = D− 1 2 (Dcut +W cut −Duncut +W uncut)D− 1 2 (2)\nThe matrix D− 1 2 (W cut +Dcut)D− 1 2 is positive semi-definite and f is in its kernel (to see that, note that for u ∈ Rn, uT (W cut +Dcut)u = ∑\nijW cut ij (ui + uj) 2). Therefore we have\nvTD− 1 2 (W cut +Dcut)D− 1 2 v ≥ λ2 (3)\nwhere 0 = λ1 ≤ λ2 ≤ . . . ≤ λn are the eigenvalues of D− 1 2 (W cut +Dcut)D− 1 2 . Moreover, W uncut + Duncut 0 ⇒ 2Duncut Duncut −W uncut, where A B means that the matrix A − B is PSD. Thus, we have,\nvTD− 1 2 (Duncut −W uncut)D− 1 2 v ≤ 2 · vTD− 1 2DuncutD− 1 2 v ≤ 2 ·max\ni Duncutii Dii ≤ 2 γ + 1\n(4)\nCombining equations (2), (3) and (4), it is enough to show that λ2 > 2\nγ+1 . However, since wcut is\nbipartite, the matrices D− 1 2 (Dcut+W cut)D− 1 2 and D− 1 2 (Dcut−W cut)D− 1 2 have the same spectrum4. Also, D− 1 2 (Dcut−W cut)D− 1 2 and D−1(Dcut−W cut) have the same spectrum5 so it suffices to show that µ2 > 2 γ+1 , where µ2 is the second smallest eigenvalue of D −1(Dcut −W cut). By the known relation between expansion and the second eigenvalue of the Laplacian (e.g., Theorem 2.2 in [FN]), it follows that µ2 ≥ mini Dcutii Dii · (1− √ 1− h(wcut)2) ≥ γγ+1(1− √ 1− h(wcut)2)\n2\nFinally, to prove the first part of Theorem 1.2, it is enough to show that if w is α-distinguished then h(wcut) ≥ α. Indeed, for ∅ 6= A ⊂ V we have\nτwcut(A) = ξw(A) ≥ ξw(A)− ιw(A) ≥ α ·min{µw(A), µw(Ā)} ≥ α ·min{µwcut(A), µwcut(Ā)} 4To see that, let P : Rn → Rn be the operator that multiply by −1 the coordinates corresponding to one side of the cut and fixes the other. The operator P commute with diagonal matrices and satisfies WP = −PW . Thus, v be an eigenvector of D− 1 2 (Dcut +W cut)D− 1 2 with an eigenvalue λ iff Pv an eigenvector of D− 1 2 (Dcut +W cut)D− 1 2 with an eigenvalue λ. 5Since v is an eigenvector of D− 1 2 (Dcut−W cut)D− 1 2 with eigenvalue λ iff D− 1 2 v is an eigenvector of D−1(Dcut− W cut) with eigenvalue λ."
    }, {
      "heading" : "4 Algorithms for stable instances",
      "text" : "We begin with a useful observation.\nObservation 2 Let w be a γ-stable instance of MAXCUT, and let w′ be obtained from w by merging two vertices6 on the same side of w’s maximal cut. Then w′ is γ-stable and its maximal cut is induced from w’s maximal cut.\nBy the above observation, we conclude that in order to design an efficient algorithm for γ-stable instances, it is enough to show in every γ-stable instance, we can efficiently find a pair of vertices that are on the same side of the cut. Once two such vertices are found, we merge them and proceed recursively. This applies as well when γ is not a constant, but a non-decreasing function of n.\nAs an easy warm-up, we show how this observation yields a simple efficient algorithm that solves every 2n-stable instance w : V × V → R of MAXCUT. This is a simplification of an algorithm from [BL]. By observation 2, it suffices to find two vertices which are on the same side of the maximal cut. Pick an arbitrary vertex v ∈ V . If vu is the heaviest edge incident with v, then clearly w(v, u) ≥ 1n−1τ(v). On the other hand, by observation 1, ι(v) ≤ 1 2n+1τ(v), so w(v, u) > ι(v) and we conclude that vu is a cut edge. Now, let e be the heaviest edge incident with {u, v}, say e = vz. Again, w(v, z) ≥ 12(n−2)τ({u, v}) and by observation 1, ι({v, u}) ≤ 1 2n+1τ({v, u}), implying that w(v, z) > ι({v, u}). Consequently vz is a cut edge. But since vz and vu are cut edges, the vertices z and u are on the same side of the cut.\n4.1 A deterministic algorithm for O( √ n)-stable instances\nFollowing observation 2, the algorithm we present will find two vertices which are on the same side of the cut. Let w : V × V → R be a γ-stable instance of MAXCUT with γ > √ 8n+ 4 + 1 and let (S, S̄) be a maximal cut. We first deal with very heavy edges. Define\nT 1 := {vu : w(v, u) > 1 γ + 1 µ(v)}\nBy observation 1, all edges in T 1 are cut edges. Thus if there are two incident edges uv, vz ∈ T 1, then u and z are on the same side of the cut and we are done. It remains to consider the case where T 1 is a matching. Define\nT 2 = {uv /∈ T 1 : w(u, v) > 1 γ + 1 τ({u, z}) for some uz ∈ T 1}\nAgain, by observation 1, all edges in T 2 are cut edges. If T 2 is nonempty, say uv ∈ T 2, then there exists some uz ∈ T 1 with w(u, v) > 1γ+1τ({u, z}), which implies that v and z are on the same side of the cut. We proceed to consider the case where T 2 is empty.\nFor every u, v ∈ V define\nw̃(u, v) =\n{ 0 vu ∈ T 1\nw(u, v) o/w , ŵ(v) = { τ({u, v}) vu ∈ T 1 for some u ∈ V τ(v) o/w\nNote that ŵ(v) is well defined, since T 1 is a matching by assumption. Since T 2 = ∅ and T 1 is a matching, we have, for every u ∈ V , w̃(v, u) ≤ 1γ+1 ŵ(v) and, again by observation 1, ι(v) ≤\n6Let w : V ×V → R be an instance and let v, u ∈ V . The instance w′ : V ′×V ′ → R obtained upon merging v, u is defined as follows. V ′ = V \\{u, v}∪{v′} and w′(x, y) = w(x, y) for x, y ∈ V \\{v, u}, also, w′(v′, x) = w(v, x)+w(u, x).\n1 γ+1 ŵ(v). Next, we observe as well that separated vertices cannot have too many common neighbors. For u, v ∈ V we define n(u, v) := ∑\nz∈V w̃(v, z)w̃(z, u). If v and u are separated, say v ∈ S, u ∈ S̄, then\nn(u, v) = ∑ z∈S̄ w̃(v, z)w̃(z, u) + ∑ z∈S w̃(v, z)w̃(z, u)\n≤ 1 γ + 1 ŵ(v) · ι(u) + 1 γ + 1 ŵ(u) · ι(v) ≤ 2 (γ + 1)2 ŵ(u) · ŵ(v)\nThus, it suffices to find two vertices v, u with n(u, v) > 2 (γ+1)2 ŵ(u) · ŵ(v), and place them on the same side of the cut. Indeed, if no such pair exists we have\n1\n4 ∑ v∈V ŵ2(v) ≤ ∑ v∈V τ2w̃(v)\n= ∑\nu,v,z∈V w̃(u, z)w̃(z, v)\n= ∑\nu,v∈V, u 6=v n(u, v) + ∑ u,z∈V w̃2(u, z)\n≤ 2 (γ + 1)2 ∑ u,v∈V, u 6=v ŵ(u)ŵ(v) + ∑ u∈V 1 γ + 1 ŵ(u) ∑ z∈V w̃(u, z) ≤ 2 (γ + 1)2 ( ∑ u∈V ŵ(u))2 + 1 γ + 1 ∑ u∈V ŵ(u)τw̃(u) ≤ 2n (γ + 1)2 ∑ u∈V ŵ2(u) + 1 γ + 1 ∑ u∈V ŵ2(u)\nAnd it follows that γ ≤ √ 8n+ 4 + 1. A contradiction."
    }, {
      "heading" : "5 Conclusion and open problems",
      "text" : "Our results together with work from [AB, ABS, BL, DLS, BL2] show that in many cases practically interesting instances of hard problems are computationally feasible. Still much remains to be done toward a new paradigm of analyzing the complexity of computational problems of practical significance. Even if we restrict our attention to MAXCUT, many problems remain open. Here are some of the more significant challenges:\n• Following [BL], we recall the (admittedly bold) conjecture that there is a constant γ∗ > 1, s.t. γ∗-stable instances can be solved in polynomial time.\n• It is interesting seek the best possible dependency of γ on α in Theorem 1.2. We are quite certain that further improvements are possible.\n• With reference to Corollary 2.6, can you find a practically efficient algorithm for, say, 2-locally stable metric instances?"
    }, {
      "heading" : "A The Spectral approach and the GW algorithm",
      "text" : "Convex programming relaxations play a key role in the study of hard computational problem. They mostly play a prominent role in the search for approximate solutions. The Goemans-Williamson (GW) approximate solution for MAXCUT is a prime example of this approach. Can such algorithms\nprovide as well exact solutions for practically interesting instances? Many papers (e.g. [B, DP, GW, M]) study the relationships between the maximal cut and spectrum of matrices associated with the instance. Such ideas have led to various heuristics and approximation algorithms for MAXCUT. In section A we ask under which conditions those methods solve MAXCUT exactly. As shown is Section 3, distinguished instances satisfy such conditions.\nWe need some terminology. We identify an instance w of MAXCUT with an n× n matrix W , where Wij = w(i, j). A vector v ∈ Rn is called a generalized least eigenvector (GLEV) of W if there is a diagonal matrix D such that v it is an eigenvector of W +D, corresponding to (W +D)’s least eigenvalue, λ. By letting ∆ := D − λI we see that v is a GLEV iff v is in the kernel of W + ∆ for ∆ diagonal with W + D 0. (As usual A 0 means that A is positive semi-definite). A vector v ∈ Rn induces the cut (S, S̄) where S = {i : vi > 0}. An algorithm for MAXCUT is called spectral if it always returns a cut that is induced by a GLEV.\nMany popular approximation algorithms and heuristics for MAXCUT are spectral. They usually work by returning the cut induced by w’s lowest eigenvector (LEV) or by LEV’s of related matrices. As we note below, the GW-algorithm is also spectral. Here is the underlying logic of this approach. The characteristic vector of the cut (S, S̄) is defined as δS = χS − χS̄ where χA : V → {0, 1} is the indicator function of A. If D is a diagonal matrix, then δTS (W +D)δS = 2w(V ) + ∑n i=1(Dii − Wii)− 4w(S, S̄). Thus, the MAXCUT problem can be formulated as follows\nminimize vT (W +D)v subject to v ∈ {1,−1}n (5)\nA natural relaxations to this problem is.\nminimize vT (W +D)v subject to ||v|| = 1 (6)\nwhere || · || denotes the Euclidean norm. Now the set of solutions v of (6) coincides with the set of least eigenvectors of W +D. In view of (5), it is natural to consider the cut induced by such v.\nThe GW-Algorithm\nThere is another relaxation to (5), that seems unrelated to (6). It was suggested by [GW] and will play a major role in the sequel. In problem (5) we seek n vectors v1, . . . , vn in the 0 dimensional sphere S0 = {−1, 1} to minimize ∑ i,jWi,j〈vi, vj〉. Interesting relaxations are obtained by replacing S0 with Sm for some m. As observed by [GW] for m = n− 1, the relaxation\nminimize ∑ i,j Wi,j〈vi, vj〉\nsubject to vi ∈ Sn−1 (7)\nis feasible. In the ideal case, the solution v1, . . . , vn of (7) is contained in a copy of S 0, embedded in Sn−1. which makes it a solution for (5) (in its new formulation). Thus, in the ideal case, separated vectors correspond to two antipodal points, and all vertices that are on the same side of the cut get mapped to the same point. Even if this ideal picture does not hold, one may expect that the angle between separated vertices be large. Therefore, to extract a cut from v1, . . . , vn we need a method that tends to (combinatorially) separate vertices whose images on the sphere are far apart. In [GW] this is done by returning the cut induced by the vector u ∈ Rn defined by ui = 〈v, vi〉 where v ∈ Sn−1 is sampled uniformly. This yields the approximation ratio 0.879.\nTo solve (7) the GW algorithm finds first a solution P to the problem\nminimize P ◦W subject to P 0\nPii = 1, ∀i ∈ [n] (8)\nWhere P ◦W := ∑\n1≤i,j≤n Pij ·Wij . Since P 0 it is possible to find next vectors v1, . . . , vn such that Pij = 〈vi, vj〉. The dual to (8) is (see [GW])\nmaximize n∑ i=1 Dii subject to W −D 0. D is diagonal\n(9)\nAs observed in [GW], by SDP duality the optima of (8) and (9) coincide. Denote by P(W ) and D(W ) the set of optimal solutions to (8) and (9) respectively. Denote also P = {P ∈Mn(R) : P 0 and ∀i, Pii = 1}, D = {D ∈Mn(R) : D is diagonal}. We say that W is GW-bipolar if there exists a solution to (9) that also solves the binary problem (6) (i.e., it is contained in a copy of S0 embedded in Sn−1). Equivalently, W is GW-bipolar if P(W) contains a matrix of the form v · vT for some v ∈ {−1, 1}n. Finally, we shall say that W is strongly GW-bipolar if every solution to (7) is also a solution of (6). Our interest in strongly GW-bipolar instances is clear. The maximal cut of such an instance can be immediately read of the output of the GW-algorithm.\nAn overview. We start by asking which instances of MAXCUT can be solved exactly by a spectral algorithm. As we show, the maximal cut is induced by a ±1 GLEV iff the instance is GW-bipolar. More generally, an instance can be correctly solved by some spectral algorithm iff it is has a certain perturbation that is GW-bipolar. This provides additional motivation to the study of GW-bipolar instances.\nWe give a primal-dual characterizing of the set of solutions to the GW-relaxation. Specifically, we show that the dual GW problem (9) always has a unique solution D and the solutions of the primal problem are P(W ) = {P ∈ P : P · (W −D) = 0}. This allows us to conclude that the GWalgorithm is a spectral algorithm according to our definition. We also show that GW-bipolarity is equivalent to a condition from [BL], under which MAXCUT can be solved exactly in polynomial time.\nA.1 Cuts induced by GLEV’s\nLet w : V × V → R+ be an instance with an associated matrix W . We seek conditions under which a given cut S is induced by GLEV. Let v ∈ RV be a vector that induces the cut S. As noted before, v is a GLEV if and only if v is in the kernel of W +D for some diagonal matrix D for which W +D 0. Thus, v is a GLEV of W if and only if the optimum of the following SDP is 0.\nminimize P\nvT (W +D)v\nsubject to W +D 0 D is diagonal\n(10)\nThe dual program of (10) is\nmaximize P\nvTWv − P ◦W\nsubject to Pii = v 2 i\nP 0\n(11)\nSince (10) has a positive definite solution, strong duality holds. Thus, v is a GLEV iff the optimum of (11) is 0.\nNow, the optimum of the dual is 0 iff the perturbation of W defined by W ′ij = |vi| · |vj | ·Wij is GW-Bipolar. To see that, note that the mapping P ′ 7→ P where Pij = |vi| · |vj | · P ′ij maps the feasible solutions to the primal GW-relaxation (8) for W ′ onto the feasible solution to (11). Moreover, P ◦W = P ′ ◦W ′. Thus, the optimum of (11) is zero iff the optimum of the primal GW relaxation of W ′ is vTWv = δTSW\n′δS . Consequently, the optimum of (11) is 0 iff the optimum of (8) is attained by a ±1 vector, making W ′ GW-bipolar. Note that if v ”strongly induces” the cut S – that is, if all coordinates |vi| are roughly equal, then W ′ is just a small perturbation of W . Taking this to the extreme, we conclude that the cut is induced by a ±1 GLEV iff W is GW-bipolar.\nA.2 The GW algorithm and GW-bipolar instances\nWe start with a primal-dual characterization of D(W ) and P(W ).\nTheorem A.1 Let W be a non-negative symmetric matrix with 0-diagonal. Then,\n1. D(W ) is a singleton7.\n2. P(W ) = {P ∈ P : P (W −D(W )) = 0}\nLemma A.2 For every D0 ∈ D(W ), P 0 ∈ P(W ) we have\nP(W ) = {P ∈ P : P (W −D0) = 0}\nD(W ) = {D ∈ D : (W −D) 0, P 0(W −D) = 0}\nProof Let D0 ∈ D(W ), P ∈ P. By strong duality,\nP ∈ P(W )⇔W ◦ P = n∑ i=1 D0i ⇔W ◦ P = D0 ◦ P\nSince W −D0 and P are PSDs, P ◦ (W −D0) = 0⇔ P (W −D0) = 0. Thus,\nP(W ) = {P ∈ P : P (W −D0) = 0}\nSimilarly, let P 0 ∈ P(W ), D ∈ D such that W −D 0 then\nD ∈ D(W )⇔W ◦ P 0 = n∑ i=1 Di ⇔W ◦ P 0 = D ◦ P 0\nThus D(W ) = {D ∈ D : (W −D) 0, P 0(W −D) = 0}\n2\nProof (of Theorem A.1) Part 2 follows from part 1 and Lemma A.2, so it only remains to prove part 1. Fix some P 0 ∈ P(W ) and let D ∈ D(W ). By considering the (j, j) entry of P 0(W −D) = 0, we have\nDjj = n∑ i=1 P 0jiWij\nwhich determines D uniquely.\n7Henceforth we usually do not distinguish between D(W ) and the single matrix that it contains.\n2 Corollary A.3 GW is a spectral algorithm.\nProof Suppose that the optimum of the GW-relaxation is attained at P and let v1, . . . , vn ∈ Sn−1 be vectors such that Pij = 〈vi, vj〉. Let v ∈ Sn−1 be the vector sampled by the algorithm and let∑n\nj=1 αjvj be its orthogonal projection on span{v1, . . . , vn}. The cut returned by the algorithm is the one induced by the vector ui = 〈v, vi〉 = ∑ j αjPij . The vector u is a linear combination of P ’s columns. Thus, by Theorem A.1 it is in the kernel of the PSD matrix W −D(W ).\n2\nCorollary A.4 The GW algorithm correctly solves Ω(n3)-stable instances.\nProof In [BL] it is shown that if u is a GLEV of a γ-stable instance W such that γ ≥ max(i,j)∈E |uiuj |min(i,j)∈E |uiuj | then u induces the optimal cut. Let u be defined as in the proof of Corollary A.3. As shown, u is a GLEV. Moreover, by an easy probabilistic argument, w.h.p., ∀j, n−1.5 ≤ |uj | ≤ 1.\n2\nHere is a characterization of GW-bipolar matrices.\nTheorem A.5 Let W be an instance for MAXCUT with maximal cut S. Denote v = δS and let D be the diagonal matrix defined by Dii = −vi ∑ jWijvj. The following conditions are equivalent.\n1. W is GW-bipolar.\n2. δS is a GLEV of W .\n3. W +D 0\n4. The optimum of the dual of the GW-relaxation is attained at −D.\nProof As shown in section A.1 condition 1 is equivalent to condition 2. Suppose now that 3 holds. It is not hard to see that δS is in the kernel of W + D, so 2 holds. Condition 4 clearly entails condition 3. Finally, suppose that 1 holds. Let D′ be the solution of problem (9). Since W is GWbipolar, δS · δTS is an optimal primal solution. By Lemma A.2 we deduce that δS ∈ ker(W −D′). It follows that D′ = −D and 4 holds.\n2\nAs noted before, strongly GW-bipolar instances can be efficiently solved using the GW algorithm. In fact, for those instances there is no need to choose a random vector to produce a cut. Moreover, those instances can be solved simply by taking the sign pattern of the least eigenvector of W +D where D is the solution to problem (9). As we explain next, strong GW-bipolarity is just slightly stronger than GW-bipolarity. Let W be a GW-bipolar instance with maximal cut (S, S̄). Let W ′ be the (1 + )-perturbation of W that is obtained by multiplying cut edges by 1 + with > 0 arbitrarily small. We claim that it is strongly GW-bipolar. Let D be the diagonal matrix defined in Theorem A.5. We have W +D 0 if and only if for every u ∈ Sn−1\nuT (W +D)u = ∑\nij∈E(S,S̄)\nWij(ui + uj) 2 − ∑ ij /∈E(S,S̄) Wij(ui − uj)2 ≥ 0 (12)\nInequality (12) clearly holds for W ′ as well making it GW-bipolar. Moreover, since the maximal cut is connected, if u 6= ± 1√\nn · δS then ∑ ij∈E(S,S̄)W ′ ij(ui + uj) 2 > ∑ ij∈E(S,S̄)Wij(ui + uj) 2. Thus,\nuT (W ′+D′)u > uT (W +D)u ≥ 0 where D′ is the matrix corresponding to W ′ from Theorem A.5. Thus, the matrix W ′ + D′ has rank n − 1. By Theorem A.1 we conclude that δS · δTS is the only solution to the primal GW-problem for W ′, making W ′ strongly GW-bipolar.\nB A randomized algorithm for · nlog(n)-stable instances\nWe now describe a simple randomized algorithm that correctly solves · nlog(n) -stable instances of MAXCUT. So let w : V × V → [0,∞) be a γ-stable instance with γ = · nlog(n) . Our algorithm proceeds as follows.\n1. Set V0 = {v0} for some v0 ∈ V and set E0 = ∅.\n2. For t = 1 to |V | − 1\n• Sample a random edge vtut ∈ E(Vt−1, V̄t−1), where the probability of every edge is proportional to its weight.\n• Set Vt = Vt−1 ∪ {vt, ut}, Et = Et−1 ∪ {vtut}\n3. Note that (Vt, Et) is a tree for every t and for t = |V | − 1 this is a spanning tree. Return the bipartition corresponding to the two-coloring of this tree.\nAnalysis: In order to return the maximal cut, it is sufficient (in fact, also necessary) that for every t, the edge vtut be in the maximal cut. But, by observation 1, the edges in E(Vt, V̄t) that are in the maximal cut constitute ≥ γγ+1 fraction of all the edges in E(Vt, V̄t). Thus a lower bound on the success probability of the algorithm can be derived as follows:(\nγ\nγ + 1\n)n−1 ≥ ( 1− 1\nγ + 1 )n = ( 1− 1\n· nlog(n) + 1\n)n\n≥ ( 1− 1\n· nlog(n) )n = ( e− + o(1)\n)ln(n) = nln(e − +o(1)) = n− +o(1)\nIn particular, for fixed the process succeeds with probability that is at least inverse polynomial in n."
    } ],
    "references" : [ {
      "title" : "Which data sets are clusterable? a theoretical study of clusterability",
      "author" : [ "M. Ackerman", "S. Ben David" ],
      "venue" : null,
      "citeRegEx" : "Ackerman and David.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ackerman and David.",
      "year" : 2009
    }, {
      "title" : "Approximation schemes for dense instances of NP-hard problems",
      "author" : [ "S. Arora", "D. Karger", "M. Karpinski" ],
      "venue" : null,
      "citeRegEx" : "Arora et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 1995
    }, {
      "title" : "Center-based clustering under perturbation stability",
      "author" : [ "ABS] P. Awasthi", "A. Blum", "O. Sheffet" ],
      "venue" : "Information Processing Letters,",
      "citeRegEx" : "Awasthi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Awasthi et al\\.",
      "year" : 2011
    }, {
      "title" : "A discriminative framework for clustering via similarity functions",
      "author" : [ "BBV] M.F. Balcan", "A. Blum", "S. Vempala" ],
      "venue" : null,
      "citeRegEx" : "Balcan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2008
    }, {
      "title" : "Clustering under Perturbation Resilience",
      "author" : [ "M.F. Balcan", "Y. Liang" ],
      "venue" : "To appear (see http://arxiv.org/pdf/1112.0826v3.pdf),",
      "citeRegEx" : "Balcan and Liang.,? \\Q2012\\E",
      "shortCiteRegEx" : "Balcan and Liang.",
      "year" : 2012
    }, {
      "title" : "Are Stable instances Easy",
      "author" : [ "Y. Bilu", "N. Linial" ],
      "venue" : "Innovations in Computer Science (Beijing, China,",
      "citeRegEx" : "Bilu and Linial,? \\Q2010\\E",
      "shortCiteRegEx" : "Bilu and Linial",
      "year" : 2010
    }, {
      "title" : "Eigenvalues and graph bisection: An average case analysis",
      "author" : [ "R. Boppana" ],
      "venue" : "FOCS",
      "citeRegEx" : "Boppana.,? \\Q1987\\E",
      "shortCiteRegEx" : "Boppana.",
      "year" : 1987
    }, {
      "title" : "Laplacian eigenvalues and the maximum cut problem",
      "author" : [ "DP] C. Delorme", "S. Poljak" ],
      "venue" : "Math. Programming,",
      "citeRegEx" : "Delorme and Poljak.,? \\Q1993\\E",
      "shortCiteRegEx" : "Delorme and Poljak.",
      "year" : 1993
    }, {
      "title" : "Clustering is difficult only when it does not matter",
      "author" : [ "DLS] A. Daniely", "N. Linial", "M. Saks" ],
      "venue" : "http://www.cs.huji.ac.il/~nati/PAPERS/cluster_ez.pdf),",
      "citeRegEx" : "Daniely et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Daniely et al\\.",
      "year" : 2012
    }, {
      "title" : "Heuristics for semirandom graph problems",
      "author" : [ "U. Feige", "J. Kilian" ],
      "venue" : "J. Comput. System Sci.,",
      "citeRegEx" : "Feige and Kilian.,? \\Q2001\\E",
      "shortCiteRegEx" : "Feige and Kilian.",
      "year" : 2001
    }, {
      "title" : "On Cheeger-type inequalities for weighted graphs",
      "author" : [ "S. Friedland", "R. Nabban" ],
      "venue" : "Journal of Graph Theory, Volume 41, Issue",
      "citeRegEx" : "Friedland and Nabban.,? \\Q2002\\E",
      "shortCiteRegEx" : "Friedland and Nabban.",
      "year" : 2002
    }, {
      "title" : "Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems Using Semidefinite Programming",
      "author" : [ "M.X. Geomans", "D.P. Williamson" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Geomans and Williamson.,? \\Q1995\\E",
      "shortCiteRegEx" : "Geomans and Williamson.",
      "year" : 1995
    }, {
      "title" : "Spectral partitioning of random graphs",
      "author" : [ "F. McSherry" ],
      "venue" : null,
      "citeRegEx" : "McSherry.,? \\Q2001\\E",
      "shortCiteRegEx" : "McSherry.",
      "year" : 2001
    }, {
      "title" : "Smoothed analysis of algorithms: why the simplex algorithm usually takes polynomial time",
      "author" : [ "D. Spielman", "S.H. Teng" ],
      "venue" : null,
      "citeRegEx" : "Spielman and Teng.,? \\Q2001\\E",
      "shortCiteRegEx" : "Spielman and Teng.",
      "year" : 2001
    }, {
      "title" : "A Randomized Approximation Scheme for Metric MAX-CUT",
      "author" : [ "VK] W. Fernandez de la Vega", "Claire Kenyon" ],
      "venue" : null,
      "citeRegEx" : "Vega and Kenyon.,? \\Q1998\\E",
      "shortCiteRegEx" : "Vega and Kenyon.",
      "year" : 1998
    } ],
    "referenceMentions" : [ ],
    "year" : 2012,
    "abstractText" : "The complexity of a computational problem is traditionally quantified based on the hardness of its worst case. This approach has many advantages and has led to a deep and beautiful theory. However, from the practical perspective, this leaves much to be desired. In application areas, practically interesting instances very often occupy just a tiny part of an algorithm’s space of instances, and the vast majority of instances are simply irrelevant. Addressing these issues is a major challenge for theoretical computer science which may make theory more relevant to the practice of computer science. Following [BL], we apply this perspective to MAXCUT, viewed as a clustering problem. Using a variety of techniques, we investigate practically interesting instances of this problem. Specifically, we show how to solve in polynomial time distinguished, metric, expanding and dense instances of MAXCUT under mild stability assumptions. In particular, (1 + )-stability (which is optimal) suffices for metric and dense MAXCUT. We also show how to solve in polynomial time Ω( √ n)-stable instances of MAXCUT, substantially improving the best previously known result. ∗Parasight inc, Agudat sport hapoel 1, Jerusalem, Israel. yonatan@gmail.com †Department of Mathematics, Hebrew University, Jerusalem 91904, Israel. Supported in part by a binational Israel-USA grant 2008368. amit.daniely@math.huji.ac.il ‡School of Computer Science and Engineering, Hebrew University, Jerusalem 91904, Israel. Supported in part by a binational Israel-USA grant 2008368. nati@cs.huji.ac.il §Department of Mathematics, Rutgers University, Piscataway, NJ 08854. Supported in part by NSF under grant CCF-0832787 and by a binational Israel-USA grant 2008368. saks@math.rutgers.edu. ar X iv :1 20 5. 48 93 v1 [ cs .C C ] 2 2 M ay 2 01 2",
    "creator" : "LaTeX with hyperref package"
  }
}