{
  "name" : "1502.03508.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Adding vs. Averaging in Distributed Primal-Dual Optimization ",
    "authors" : [ "Chenxin Ma", "Martin Jaggi" ],
    "emails" : [ "CHM514@LEHIGH.EDU", "VSMITH@BERKELEY.EDU", "JAGGI@INF.ETHZ.CH", "JORDAN@CS.BERKELEY.EDU", "PETER.RICHTARIK@ED.AC.UK", "TAKAC.MT@GMAIL.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "With the wide availability of large datasets that exceed the storage capacity of single machines, distributed optimization methods for machine learning have become in-\ncreasingly important. Existing methods require significant communication between workers, frequently equaling the amount of local computation (or reading of local data). As a result, distributed machine learning suffers significantly from a communication bottleneck on real world systems, where communication is typically several orders of magnitudes slower than reading data from main memory.\nIn this work we focus on optimization problems with empirical loss minimization structure, meaning that the objective is a sum of the loss functions of each individual datapoint. This includes the most commonly used regularized variants of linear regression and classification methods. For this class of problems, the recently proposed COCOA approach (Yang, 2013; Jaggi et al., 2014; Konečný et al., 2014) has formulated a communication efficient primaldual coordinate ascent scheme which targets the communication bottleneck, by allowing more computation to be performed on data local subproblems on each machine before a single vector is communicated. By appropriately choosing the amount of local computation per round, this framework allows one to control the trade-off between communication and local computation based on the systems hardware at hand.\nHowever, the performance of COCOA (as well as the related primal SGD based methods) is slowed down significantly by the need to average updates between all machines. As the number of machines K grows, the updates get diluted and slowed down by 1/K, e.g. in the case\nar X\niv :1\n50 2.\n03 50\n8v 1\n[ cs\nwhere all machines except one would already have reached the solutions of their respective partial optimization tasks. A somewhat opposite problem appears if the updates are added instead, as the algorithms can then diverge, as we will observe in the practical experiments below.\nTo address both described issues, in this paper we develop a novel generalization of the local COCOA subproblems assigned to each worker, making the framework more powerful in the following sense: Without extra computational cost, the set of locally computed updates from the modified subproblems (one from each machine) can be combined much more efficiently between the machines. The COCOA+ updates proposed here can be aggressively added (hence the ‘+’-suffix), which gives a much faster convergence both in practice and also in theory. We will see that the difference is particularly significant as the number of machines K becomes large."
    }, {
      "heading" : "1.1. Contributions",
      "text" : "Strong Scaling. To our knowledge, our distributed optimization framework is the first to exhibit favorable strong scaling for the class of problems considered when the number of machines K increases, while the data size is kept fixed. More precisely, while the convergence rate of COCOA degrades as the number of workers K is increased, the stronger theoretical convergence rate here is – in the worst case – independent of K. Our practical experiments in Section 7 confirm the improved speed of convergence. Since the number of communicated vectors is only one per round and worker, this favorable scaling might be surprising. Indeed, for existing methods, splitting data among more machines generally increases the requirements for communication (Shamir & Srebro, 2014), which often has severe effects on overall runtime.\nTheoretical Analysis for Non-Smooth Functions. While the existing analysis for COCOA in (Jaggi et al., 2014) only covered smooth loss functions, here we extend the class of functions where the rates apply (to include e.g. Support Vector Machines and non-smooth regression variants). We provide a primal-dual convergence rate for both COCOA as well as our new method COCOA+ in the case of general convex (L-Lipschitz) losses.\nPrimal-Dual Convergence Rate. Furthermore, we additionally strengthen the rates by showing stronger primaldual convergence for both algorithmic frameworks, which are almost tight to their objective-only counterparts. Primal-dual rates for COCOA had not previously been analyzed in the general convex case. Our primal-dual rates allow efficient and practical certificates for the optimization quality, e.g. for stopping criterions. The new rates apply for both smooth and non-smooth, and for both COCOA as\nwell as the extended COCOA+.\nExperimental Results. We provide a thorough experimental comparison with competing algorithms, which we all implement in Spark. Our practical results confirm the strong scaling of COCOA+ as the number of machines K grows up to 100, while competing methods – including original COCOA – are slowing down significantly with larger K."
    }, {
      "heading" : "1.2. History and Related Work",
      "text" : "While optimal algorithms for the serial (single machine) case are already very well researched and understood, the literature in the distributed setting is relatively sparse. In particular, the details on optimal trade-offs between computation and communication, as well as optimization or statistical accuracy, are still widely unclear. For an overview over the currently active research field, we refer the reader to (Balcan et al., 2012; Richtárik & Takáč, 2013b; Duchi et al., 2013; Yang, 2013; ?; Liu & Wright, 2014; Fercoq & Richtárik, 2013; Fercoq et al., 2014; Jaggi et al., 2014; Shamir & Srebro, 2014; Shamir et al., 2014a; Zhang & Xiao, 2015; Qu & Richtárik, 2014) and the references therein. We provide a detailed comparison of our proposed framework to the related work in Section 6 below."
    }, {
      "heading" : "2. Setup",
      "text" : "We consider regularized empirical loss minimization problems of the following well-established form:\nmin w∈Rd\n{ P (w) := 1\nn n∑ i=1 `i(x T i w) + λ 2 ‖w‖2\n} . (1)\nHere the vectors {xi}ni=1 ⊂ Rd represent the training data examples, and the `i(.) are arbitrary convex real-valued loss functions (e.g. hinge loss), possibly depending on label information for the i-th datapoints. The constant λ > 0 is the regularization parameter.\nThe above class of problems includes many standard problems of wide interest in machine learning, statistics and signal processing, including Support Vector Machines, as well as regularized linear and logistic regression, ordinal regression, and others.\nDual Problem, and Primal-Dual Certificates. The conjugate dual optimization problem of (1) takes following form:\nmax α∈Rn D(α) := − 1n n∑ j=1 `∗j (−αj)− λ 2 ∥∥∥∥ 1λnAα ∥∥∥∥2  ,\n(2) Here the data matrix A = [x1, x2, . . . , xn] ∈ Rd×n collects all data-points as its columns, and `∗j is the conjugate\nfunction to `j . See e.g. (Shalev-Shwartz & Zhang, 2013c) for several concrete applications.\nIt is possible assign for any dual vector α ∈ Rn a corresponding primal feasible point\nw(α) = 1λnAα. (3)\nThe duality gap function is given as follows\nG(α) := P(w(α))−D(α). (4)\nBy weak duality, every value D(α) at a dual candidate α provides a lower bound for every primal value P (w), therefore the duality gap being a certificate for the approximation quality: The unknown true optimum P (w∗) must always lie within the duality gap, i.e. G(α) = P(w) − D(α) ≥ P(w)− P(w∗) ≥ 0.\nIn large-scale machine learning such as in the setting here, the availability of such a computable measure of approximation quality is a crucial benefit for training methods. In contrast, practitioners using classical primal-only methods such as SGD have no means by which to accurately detect if a model has been well trained, as P (w∗) is unknown.\nClasses of Loss-Functions. In order to simplify the presentation, we assume that all loss functions `i are always non-negative, and\n`i(0) ≤ 1 ∀i. (5)\nDefinition 1 (L-Lipschitz continuous loss). A function `i : R→ R is L-Lipschitz continuous if ∀a, b ∈ R, we have\n|`i(a)− `i(b)| ≤ L|a− b|. (6)\nDefinition 2 ((1/µ)-smooth loss). A function `i : R → R is (1/µ)-smooth if it is differentiable and its derivative is (1/µ)-Lipschitz continuous, i.e. ∀a, b ∈ R, we have\n|`′i(a)− `′i(b)| ≤ 1\nµ |a− b|. (7)"
    }, {
      "heading" : "3. The COCOA+ Algorithm Framework",
      "text" : "Data Partitioning. We write {Pk}Kk=1 for the given partition of the datapoints [n] := {1, 2, . . . , n} over the K worker machines. We denote the size of each part by nk = |Pk|. For any k ∈ [K] and α ∈ Rn we use the notation α[k] ∈ Rn for the vector\n(α[k])i := { 0, if i /∈ Pk, αi, otherwise.\nLocal Subproblems in COCOA+. We can define a datalocal subproblem of the original dual optimization problem (2), which can be solved on machine k and only requires\naccessing data which is already available locally on the machine, i.e. datapoints with i ∈ Pk.\nMore formally, we assign to each machine k the following subproblem, only depending on the change in local dual variables αi with i ∈ Pk:\nmax ∆α[k]∈Rn\nGσ ′\nk (∆α[k], w) (8)\nwhere\nGσ ′ k (∆α[k], w) := − 1\nn ∑ i∈Pk `∗i (−αi − (∆α[k])i)\n− 1 K λ 2 ‖w‖2 − 1 n wTA∆α[k]\n− λ 2 σ′ ∥∥∥ 1 λn A∆α[k] ∥∥∥2. (9) The role of the parameter σ′ is to measure the difficulty of the given data partition. For our purposes, we will see that it must be chosen not smaller than\nσ′ ≥ σ′min := γ max α∈Rn ‖Aα‖2∑K k=1 ‖Aα[k]‖2 . (10)\nWe will see (Lemma 3) that this parameter can be upperbounded by γK, which is trivial to calculate for all values of γ ∈ R. Later on, our main theorems below will show convergence rates for when γ is used as the aggregation parameter in our algorithm framework. Proofs of all statements are provided in the supplementary material.\nWe show experimentally (Section 7) that the following safe upper bound for σ′ has a minimal effect on overall performance of the algorithm.\nLemma 3. The choice of σ′ := γK is valid for (10), i.e.\nγK ≥ σ′min.\nInterpretation. The above definition of the local subproblems with objective functions Gσ′k are such that they closely approximate the dual objective D, as we vary the ‘local’ variable ∆α[k], in the following precise sense:\nLemma 4. For any α,∆α[1], . . . ,∆α[K] ∈ Rn it holds that\nD ( α+γ K∑ k=1 ∆α[k] ) ≥ (1−γ)D(α)+γ K∑ k=1 Gσ ′ k (∆α[k], w).\n(11)"
    }, {
      "heading" : "Notion of Approximation Quality of the Local Solver.",
      "text" : "Assumption 1 (Θ-approximate solution). We assume that there exists Θ ∈ [0, 1) such that ∀k ∈ [K], the local solver\nat any iteration t produces a (possibly) randomized approximate solution ∆α[k], which satisfies\nE [ Gσ ′\nk (∆α ∗ [k], w)− G\nσ′ k (∆α[k], w) ] ≤ (12)\nΘ ( Gσ ′\nk (∆α ∗ [k], w)− G\nσ′ k (0, w) ) ,\nwhere\n∆α∗ ∈ arg min ∆α∈Rn K∑ k=1 Gσ ′ k (∆α[k], w). (13)\nWe are now ready to describe the COCOA+ algorithm framework, shown in Algorithm 1. The crucial difference compared to the existing COCOA algorithm (Jaggi et al., 2014) is the more general subproblem, as defined in (9), as well as the aggregation parameter γ. These modifications allow for updates to the global vector w to be added directly.\nAlgorithm 1 COCOA+ Framework 1: Input: Datapoints A distributed according to partition {Pk}Kk=1. Aggregation parameter γ∈ [ 1K , 1], subproblem parameter σ′ for the subproblems Gσ′(∆α[k], w) for each k ∈ [K]. Starting point α(0) = 0 ∈ Rn, w(0) := 0 ∈ Rd.\n2: for t = 0, 1, 2, . . . do 3: for k ∈ {1, 2, . . . ,K} in parallel over computers do 4: call the local solver, computing a Θ-approximate solution ∆α[k] of the subproblem (9) 5: update α(t+1)[k] := α (t) [k] + γ∆α[k] 6: return ∆wk := 1λnA∆α[k] 7: end for 8: reduce w(t+1) := w(t) + γ ∑K k=1 ∆wk. (14)\n9: end for"
    }, {
      "heading" : "4. Convergence Guarantees",
      "text" : "Before being able to state our main convergence results, we introduce some useful quantities and the following main lemma characterizing the effect of iterations of Algorithm 1, for any chosen internal local solver. Lemma 5. Let `∗i be strongly1 convex with convexity parameter µ ≥ 0 with respect to the norm ‖·‖, ∀i ∈ [n]. Then for all iterations t of Algorithm 1 under Assumption 1, and any s ∈ [0, 1], it holds that\nE[D(α(t+1))−D(α(t))] ≥ (15) γ(1−Θ) ( sG(α(t))− σ ′\n2λ ( s n )2 R(t) ) ,\n1Note that the case of weakly convex `∗i (.) is explicitly allowed here as well, as the Lemma holds for the case µ = 0.\nwhere\nR(t) := −λµn(1−s)σ′s ‖u (t) − α(t)‖2 (16) + ∑K k=1‖A(u(t) − α(t))[k]‖2,\nu(t) = (u (t) 1 , . . . , u (t) n )T and\n− u(t)i ∈ ∂`i(w(α (t))Txi). (17)\nThe following lemma provides a uniform bound on R(t):\nLemma 6. Assuming the `i are L-Lipschitz continuous for all i ∈ [n], then\n∀t : R(t) ≤ 4L2 K∑ k=1\nσknk︸ ︷︷ ︸ =:σ , (18)\nwhere σk := max\nα[k]∈Rn ‖Aα[k]‖2 ‖α[k]‖2 . (19)\nRemark 7. If all data-points xi are normalized such that ‖xi‖ ≤ 1 ∀i ∈ [n], then σk ≤ |Pk| = nk. Furthermore, if we assume that the data partition is balanced, i.e. that nk = n/K for all k, then σ ≤ n2/K.\nThis can also be used to bound the constants R(t) appearing above as R(t) ≤ 4L 2n2\nK ."
    }, {
      "heading" : "4.1. Primal-Dual Convergence for General Convex Losses",
      "text" : "The following theorem shows the convergence for nonsmooth loss functions, in terms of objective values as well as primal-dual gap. The analysis in (Jaggi et al., 2014) only covered the case of smooth loss functions.\nTheorem 8. Consider Algorithm 1 starting with α(0) = 0 ∈ Rn and let Assumption 1 hold and ∀i ∈ [n] : `i(·) be L-Lipschitz continuous and G > 0 be the desired duality gap (and hence an upper-bound on primal sub-optimality). Then after T iterations, where\nT ≥ T0 + max{ ⌈ 1 γ(1−Θ) ⌉ ,\n4L2σσ′\nλn2 Gγ(1−Θ) }, (20) T0 ≥ t0 + ( 2\nγ(1−Θ)\n( 8L2σσ′ λn2 G − 1 ))\n+\n, (21)\nt0 ≥ max(0, ⌈ 1 γ(1−Θ) log( 2λn2(D(α∗)−D(α(0))) 4L2σσ′ ) ⌉ ), (22)\nwe have that the expected duality gap satisfies\nE[P(w(α))−D(α)] ≤ G,\nat the averaged iterate\nα := 1T−T0 ∑T−1 t=T0+1 α(t). (23)\nThe following corollary of the above theorem clarifies our main result: The more aggressive adding of the partial updates, as compared averaging, offers a very significant improvement in terms of total iterations needed. While the convergence in the ‘adding’ case becomes independent of the number of machines K, the ‘averaging’ regime shows the known degradation of the rate with growing K, which is a major drawback of the original COCOA algorithm.\nThis important difference in the convergence speed is not a theoretical artifact but also confirmed in our practical experiments below for different K, as shown e.g. in Figure 2.\nWe will demonstrate below that by choosing γ and σ′ accordingly, we can still recover the original COCOA algorithm and its rate.\nCorollary 9. Assume that all datapoints xi are bounded as ‖xi‖ ≤ 1 and that the data partition is balanced, i.e. that nk = n/K for all k. We consider two different possible choices of the aggregation parameter γ:\n• (COCOA Averaging, γ := 1K ): In this case, σ ′ := 1\nis a valid choice which satisfies (10). Then using σ ≤ n2/K in light of Remark 7, we have that T iterations are sufficient for primal-dual accuracy G, with\nT ≥ T0 + max{ ⌈ K\n(1−Θ)\n⌉ ,\n4L2\nλ G(1−Θ) }, T0 ≥ t0 + ( 2K\n(1−Θ)\n( 8L2 λK G − 1 ))\n+\n,\nt0 ≥ max(0, ⌈ K (1−Θ) log( 2λ(D(α∗)−D(α(0))) 4KL2 ) ⌉ ).\nHence the more machines K, the more iterations are need (in the worst case).\n• (COCOA+ Adding, γ := 1): In this case, the choice of σ′ := K satisfies (10). Then using σ ≤ n2/K in light of Remark 7, we have that T iterations are sufficient for primal-dual accuracy G, with\nT ≥ T0 + max{ ⌈ 1\n(1−Θ)\n⌉ ,\n4L2\nλ G(1−Θ) }, T0 ≥ t0 + ( 2\n(1−Θ)\n( 8L2 λ G − 1 ))\n+\n,\nt0 ≥ max(0, ⌈ 1 (1−Θ) log( 2λn(D(α∗)−D(α(0))) 4L2K ) ⌉ )\nwhich is significantly better than in the averaging case above.\nIn practice, usually σ n2/K, and hence the actual convergence rate can be much better than the proven worstcase bound. Table 1 shows that the actual value of σ is typically between one and two orders of magnitudes smaller compared to our used upper-bound n2/K."
    }, {
      "heading" : "4.2. Primal-Dual Convergence for Smooth Losses",
      "text" : "The following theorem shows the convergence for smooth loss functions, in terms of objective values as well as primal-dual gap.\nTheorem 10. Assume the loss functions functions `i are (1/µ)-smooth, for i ∈ [n]. We define σmax = maxk∈[K] σk.\nThen after T iterations of Algorithm 1, with\nT ≥ 1γ(1−Θ) λµn+σmaxσ\n′\nλµn log 1 D ,\nit holds that\nE[D(α∗)−D(α(T ))] ≤ D.\nFurthermore, after T iterations with\nT ≥ 1γ(1−Θ) λµn+σmaxσ\n′ λµn log ( 1 γ(1−Θ) λµn+σmaxσ ′ λµn 1 G ) ,\nwe have the expected duality gap\nE[P(w(α(T )))−D(α(T ))] ≤ G.\nThe following corollary clarifies the impact of the different aggregation of the updates in the COCOA+ scheme compared to COCOA, for the case of smooth losses. It will again show that while the COCOA variant suffers from degrading with the increase of the number of machines K, the new COCOA+ rate becomes independent of K.\nCorollary 11. Assume that all datapoints xi are bounded as ‖xi‖ ≤ 1 and that the data partition is balanced, i.e. that nk = n/K for all k. We again consider the same two different possible choices of the aggregation parameter γ:\n• (COCOA Averaging, γ := 1K ): In this case, σ ′ :=\n1 is a valid choice which satisfies (10). Then using σmax ≤ nk = n/K in light of Remark 7, we have that T iterations are sufficient for suboptimality D, with\nT ≥ 11−Θ λµK+1 λµ log 1 D\nHence the more machines K, the more iterations are need (in the worst case).\n• (COCOA+ Adding, γ := 1): In this case, the choice of σ′ := K satisfies (10). Then using σmax ≤ nk = n/K in light of Remark 7, we have that T iterations are sufficient for suboptimality G, with\nT ≥ 1(1−Θ) λµ+1 λµ log 1 D\nwhich is significantly better than in the averaging case above. Both convergence rates hold analogously for the duality gap."
    }, {
      "heading" : "4.3. Comparison with Original COCOA",
      "text" : "Remark 12. If we choose the averaging option γ := 1K for aggregating the updates, together with σ′ := 1, then the resulting Algorithm 1 is identical with COCOA analyzed in (Jaggi et al., 2014). However, their results provide only convergence for smooth loss functions `i and provide guarantees for dual sub-optimality and not the duality gap.\nFormally, when σ′ = 1, the subproblems (9) will differ from the original dual D(.) only by an additive constant, which does not affect the local optimization algorithms used within COCOA."
    }, {
      "heading" : "5. SDCA as a Local Solver",
      "text" : "The previous Section 4 has shown the convergence rates of Algorithm 1 provided that one uses a local solver giving an approximation quality Θ as described in Assumption 1. The obvious question to be answered is which solver can provide us a solution of that specified quality.\nIn this section we will show that coordinate ascent (SDCA) applied on the local subproblem will indeed deliver such a solution. The LOCALSDCA solver is summarized in Algorithm 2. As an input, the methods receives the local α variables, as well as a shared vector w (3)= w(α) being compatible with the last state of all local α ∈ Rn variables. The LOCALSDCA algorithm will then produce a random sequence of iterates {∆α(h)[k] } H h=1, on each part k ∈ [K].\nAlgorithm 2 LOCALSDCA (α[k], w, k,H) 1: Input: α[k], w = w(α) 2: Data: Local {(xi, yi)}i∈Pk 3: Initialize: ∆α(0)[k] ← 0 ∈ R n\n4: for h = 0, 1, . . . ,H − 1 do 5: choose i ∈ Pk uniformly at random 6: δ∗i = arg max\nδi∈R Gσ ′ k (∆α (h) [k] + δiei, w)\n7: ∆α(h+1)[k] ← ∆α (h) [k] + δ ∗ i ei 8: end for 9: Output: ∆α(H)[k]\nThe following two Theorems 13, 14 characterize the convergence of the LOCALSDCA method given in Algorithm 2, for both smooth and non-smooth functions.\nIn all the results we will use rmax := maxi∈[n] ‖xi‖2. Theorem 13. Assume the functions `i are (1/µ)−smooth for i ∈ [n]. Then Assumption 1 on the local approximation quality Θ is satisfied for LOCALSDCA as given in Algorithm 2, if we choose the number of inner iterations H as\nH ≥ nk σ′rmax + λnµ\nλnµ log\n1 Θ . (24)\nTheorem 14. Assume the functions `i are L-Lipschitz for i ∈ [n]. Then Assumption 1 on the local approximation quality Θ is satisfied for LOCALSDCA as given in Algorithm 2, if we choose the number of inner iterations H as\nH ≥ nk\n( 1−Θ\nΘ + σ′rmax 2Θλn2\n‖∆α∗[k]‖ 2\nGσ′k (∆α∗[k], w)− G σ′ k (0, w)\n) .\n(25)\nRemark 15. Between the different regimes allowed in COCOA+– ranging between averaging and adding the updates – the computational cost for obtaining the required local approximation quality varies with the choice of σ′. From the above worst-case upper bound, we note that the cost can increase with σ′, as aggregation becomes more aggressive. However, as we will see in the practical experiments in Section 7 below, the additional cost is negligible compared to the gain in speed from the different aggregation, when measured on real datasets."
    }, {
      "heading" : "6. Discussion and Related Work",
      "text" : "SGD-based Algorithms. For the empirical loss minimization problems of interest here, stochastic subgradient descent (SGD) based methods are well-established. Several distributed variants of SGD have been proposed (Niu et al., 2011; Liu et al., 2014; Duchi et al., 2013). Most of these build on the idea of a parameter-server, receiving all SGD updates performed by each local worker. The downside of this approach is that the amount of required communication is equal to the amount of data read locally (as in mini-batch SGD with a batch size of 1 per worker). Being tailored for very sparse problems, such SGD variants are in practice not competitive with the more communication efficient methods considered here, which allow more local updates per round.\nOne-Shot Communication Schemes. At the other extreme of the spectrum, we find the group of distributed methods only using a single round of communication such as (Zhang et al., 2013; Zinkevich et al., 2010; Mann et al., 2009; Zhang et al., 2013; McWilliams et al., 2014). These do require additional assumptions on the partitioning of\nthe data, and furthermore can not guarantee convergence to the optimum solution for all regularizers, as shown e.g. in (Shamir et al., 2014b). (Balcan et al., 2012) shows additional relevant lower bounds on the minimum number of communication rounds necessary for a given approximation quality for similar machine learning problems.\nMethods Allowing Local Optimization. It is apparent that the well performing methods must lie in the middle of the two extremes of the communication vs computation tradeoff. Therefore, we need to design more meaningful data-local subproblems to be solved per round of communication. In this spirit, (Shamir et al., 2014b; Zhang & Xiao, 2015) have proposed distributed Newton-type algorithms. In this approach, the subproblems have to be solved to very high accuracy for the convergence rate to hold, which is often prohibitive as the size of the data on one machine is still relative large.\nThe COCOA framework (Jaggi et al., 2014) allows using local solvers of weak local approximation quality in each round, while still giving a convergence rate for smooth losses. By making use of the primal-dual structure in the line of work of (Yu et al., 2012; Shalev-Shwartz & Zhang, 2013c; Yang, 2013; Yang et al., 2013; Jaggi et al., 2014), the COCOA framework as well as the extension here allow to have more control over the meaningful aggregation of updates between different machines.\nMini-Batch Methods. Mini-batch versions of both SGD and coordinate descent (CD) (Richtárik & Takáč, 2013b;a; Shalev-Shwartz & Zhang, 2013b; Yang, 2013; Qu & Richtárik, 2014; Qu et al., 2014) suffer from their convergence rate degrading towards the rate of batch gradient descent, as the size of the mini-batch is increased. This follows because the updates from all datapoints in the minibatch are based on the previous parameter vector w, contrasting methods that allow local updates such as COCOA or local-SGD (Jaggi et al., 2014). Furthermore, the aggregation parameter for mini-batch methods is harder to tune, as it can lie anywhere in the order of mini-batch size, being usually in the thousands in practice. The mini-batch size must be chosen by the tradeoff of cost of a communication round vs. local reading of a vector. In the COCOA setting, the aggregation parameter is slightly easier to tune, as it is in the smaller range given by number of machines. In our COCOA+ extension presented below, we show that we can get rid of tuning the aggregation parameter completely, by simply allowing additive aggregation.\nADMM. An alternative approach to distributed problems of our form of interest is to use the alternating direction method of multipliers (ADMM), as used for distributed SVM training e.g. in (Forero et al., 2010). This uses a penalty parameter balancing between the equality con-"
    }, {
      "heading" : "Dataset n d Sparsity",
      "text" : "straint w and the optimization objective (Boyd et al., 2011). However, the known convergence rates for ADMM are weaker than the more problem tailored methods mentioned previously, and also the choice of the penalty parameter is often unclear."
    }, {
      "heading" : "7. Numerical Experiments",
      "text" : "We present experiments on several large real-world datasets distributed across multiple machines, running on Apache Spark. We show that COCOA+ converges to the optimal solution faster in terms of total rounds as well as elapsed time as compared to COCOA in all cases, despite varying: the dataset, values of regularization, batch size, and cluster size (Section 7.2). In Section 7.3 we demonstrate that this performance translates to orders of magnitude improvement in convergence when scaling up the number of machines K, as compared to COCOA as well as to several other state-of-the-art methods. Finally, in Section 7.4 we investigate the impact of the subproblem parameter σ′ in the COCOA+ framework."
    }, {
      "heading" : "7.1. Implementation Details",
      "text" : "We implement COCOA+ and all other algorithms for comparison in Apache Spark (Zaharia et al., 2012), and run them in the Amazon cloud, using m3.large EC2 instances. We apply all methods to the binary hinge-loss support vector machine. The analysis for this non-smooth loss was not covered in (Jaggi et al., 2014) but has been captured here, and thus is both theoretically and practically justified. A summary of the datasets used is shown in Table 2."
    }, {
      "heading" : "7.2. Comparison of COCOA+ and COCOA",
      "text" : "We compare the COCOA+ and COCOA frameworks directly using two datasets (Covertype and RCV1) across various values of λ, the regularizer, in Figure 1. For each value of λ we consider both methods with different values of H , the number of local iterations performed before communicating to the master. For all runs of COCOA+ we use the safe upper bound of γK for σ′. In terms of both the total number of communications made and the elapsed time, COCOA+ (shown in blue) converges to the optimal solution faster than COCOA (red). The discrepancy is larger for greater values of λ, where the strongly convex regularizer has more of an impact and the problem difficulty is reduced. We also see a greater performance gap for smaller\nvalues of H , where there is frequent communication between the machines and the master, and changes between the algorithms therefore play a larger role."
    }, {
      "heading" : "7.3. Scaling the Number of Machines K",
      "text" : "In Figure 2 we demonstrate the ability of COCOA+ to scale with an increasing number of machines K. The experiments confirm the ability of strong scaling of the new method, as predicted by our theory in Section 4, in contrast to the competing methods. Unlike COCOA, which becomes linearly slower when increasing the number of machines, the performance of COCOA+ improves with additional machines, only starting to degrade slightly once K=16 for the RCV1 dataset."
    }, {
      "heading" : "7.4. Impact of the Subproblem Parameter σ′",
      "text" : "Finally, in Figure 3, we consider the effect of the choice of the subproblem parameter σ′ on convergence. We plot both the number of communications and clock time on a log-\nlog scale for the RCV1 dataset with K=8 and H=1e4. For γ = 1 (the most aggressive variant of COCOA+ in which updates are added) we consider several different values of σ′, ranging from σ′=1 to σ′=8. The value σ′=8 represents the safe upper bound of γK. We see that the optimal convergence occurs around σ′=4, and diverges for σ′ ≤ 2. Notably, we see that the easy to calculate upper bound of σ′ := γK (as given by Lemma 3) has only slightly worse performance than best possible subproblem parameter in our setting."
    }, {
      "heading" : "8. Conclusion",
      "text" : "In conclusion, we present a novel framework COCOA+ that allows for fast and communication-efficient additive aggregation in distributed algorithms for primal-dual optimization. We analyze the theoretical performance of this method, giving strong primal-dual convergence rates with outer iterations scaling independently of the number of machines. We extended our theory to allow for non-smooth losses. Our experimental results show significant speedups over previous methods, including the original COCOA\nframework as well as other state-of-the-art methods."
    }, {
      "heading" : "Appendix",
      "text" : ""
    }, {
      "heading" : "A. Technical Lemmas",
      "text" : "Lemma 16 (Lemma 21 in (Shalev-Shwartz & Zhang, 2013c)). Let `i : R → R be an L-Lipschitz continuous. Then for any real value a with |a| > L we have that `∗i (a) =∞.\nLemma 17. Assuming the loss functions `i are bounded by `i(0) ≤ 1 for all i ∈ [n] (as we have assumed in (5) above), then for the zero vector α(0) := 0 ∈ Rn, we have\nD(α∗)−D(α(0)) = D(α∗)−D(0) ≤ 1. (26)\nProof. For α := 0 ∈ Rn, we have w(α) = 1λnAα = 0 ∈ R d. Therefore, by definition of the dual objective D given in (2),\n0 ≤ D(α∗)−D(α) ≤ P (w(α))−D(α) = 0−D(α) (5),(2) ≤ 1."
    }, {
      "heading" : "B. Proofs",
      "text" : ""
    }, {
      "heading" : "B.1. Proof of Lemma 3",
      "text" : "See (Richtárik & Takáč, 2013b)."
    }, {
      "heading" : "B.2. Proof of Lemma 4",
      "text" : "Indeed, we have\nD(α+ γ K∑ k=1 ∆α[k]) = − 1 n n∑ i=1 `∗i (−αi − γ( K∑ k=1\n∆α[k])i)︸ ︷︷ ︸ A\n−λ 2 ‖ 1 λn A(α+ γ K∑ k=1\n∆α[k])‖2︸ ︷︷ ︸ B . (27)\nNow, let us bound the terms A and B separately. We have\nA = − 1 n K∑ k=1 (∑ i∈Pk `∗i (−αi − γ(∆α[k])i) ) = − 1 n K∑ k=1 (∑ i∈Pk `∗i (−(1− γ)αi − γ(α+ ∆α[k])i) )\n≥ − 1 n K∑ k=1 (∑ i∈Pk (1− γ)`∗i (−αi) + γ`∗i (−(α+ ∆α[k])i) ) .\nWhere the last inequality is due to Jensen. Now we will bound B.\nB = ‖ 1 λn A(α+ γ K∑ k=1 ∆α[k])‖2 = ‖w(α) + γ 1 λn K∑ k=1 A∆α[k]‖2 = ‖w(α)‖2 + K∑ k=1 2γ 1 λn w(α)TA∆α[k]\n+ γ( 1\nλn )2γ‖ K∑ k=1 A∆α[k]‖2 (10) ≤ ‖w(α)‖2 + K∑ k=1 2γ 1 λn w(α)TA∆α[k] + γ( 1 λn )2σ′ K∑ k=1 ‖Aα[k]‖2.\nPlugging A and B into (27) will give us\nD(α+ γ K∑ k=1 ∆α[k]) = − 1 n K∑ k=1 (∑ i∈Pk (1− γ)`∗i (−αi) + γ`∗i (−(α+ ∆α[k])i) )\n− γ λ 2 ‖w(α)‖2 − (1− γ)λ 2 ‖w(α)‖2 − λ 2 K∑ k=1 2γ 1 λn w(α)TA∆α[k] − λ 2 γ( 1 λn )2σ′ K∑ k=1 ‖Aα[k]‖2\n= − 1 n K∑ k=1 (∑ i∈Pk (1− γ)`∗i (−αi) ) − (1− γ)λ\n2 ‖w(α)‖2︸ ︷︷ ︸\n(1−γ)D(α)\n+ γ K∑ k=1 ( − 1 n ∑ i∈Pk `∗i (−(α+ ∆α[k])i)− 1 K λ 2 ‖w(α)‖2 − 1 n w(α)TA∆α[k] − λ 2 σ′‖ 1 λn Aα[k]‖2 )\n(9) = (1− γ)D(α) + γ K∑ k=1 Gσ ′ k (∆α[k], w)."
    }, {
      "heading" : "B.3. Proof of Lemma 5",
      "text" : "For sake of notation we will write α instead of α(t), w instead of w(α(t)) and u instead of u(t). Now, let us estimate the expected change of the dual objective. Using the definition of the dual update α(t+1) := α(t) + γ ∑ k ∆α[k] resulting in Algorithm 1, we have\nE[D(α(t))−D(α(t+1))] = E[D(α)−D(α+ γ K∑ k=1 ∆α[k])]\n(11) ≤ E[D(α)− (1− γ)D(α)− γ K∑ k=1 Gσ ′ k (∆α (t) [k] , w)] = γE[D(α)− K∑ k=1 Gσ ′ k (∆α (t) [k] , w)]\n= γE[D(α)− K∑ k=1 Gσ ′ k (∆α ∗ [k], w) + K∑ k=1 Gσ ′ k (∆α ∗ [k], w)− K∑ k=1 Gσ ′ k (∆α (t) [k] , w)]\n(12) ≤ γ D(α)− K∑ k=1 Gσ ′ k (∆α ∗ [k], w) + Θ  K∑ k=1 Gσ ′ (∆α∗[k], w)− K∑ k=1 Gσ ′ (0, w)︸ ︷︷ ︸\nD(α)\n \n= γ(1−Θ) D(α)− K∑ k=1 Gσ ′ k (∆α ∗ [k], w)︸ ︷︷ ︸\nC\n . (28)\nNow, let us upper bound the C term (we will denote by ∆α∗ = ∑K k=1 ∆α ∗ [k]):\nC (2),(9) =\n1\nn n∑ i=1 (`∗i (−αi −∆α∗i )− `∗i (−αi)) + 1 n w(α)TA∆α∗ + K∑ k=1 λ 2 σ′‖ 1 λn A∆α∗[k]‖ 2\n≤ 1 n n∑ i=1 (`∗i (−αi − s(ui − αi))− `∗i (−αi)) + 1 n w(α)TAs(u− α) + K∑ k=1 λ 2 σ′‖ 1 λn As(u− α)[k]‖2\nStrong conv. ≤ 1\nn n∑ i=1 ( s`∗i (−ui) + (1− s)`∗i (−αi)− µ 2 (1− s)s(ui − αi)2 − `∗i (−αi) ) + 1 n w(α)TAs(u− α)\n+ K∑ k=1 λ 2 σ′‖ 1 λn As(u− α)[k]‖2\n= 1\nn n∑ i=1 ( s`∗i (−ui)− s`∗i (−αi)− µ 2 (1− s)s(ui − αi)2 ) + 1 n w(α)TAs(u− α) + K∑ k=1 λ 2 σ′‖ 1 λn As(u− α)[k]‖2.\nThe convex conjugate maximal property implies that\n`∗i (−ui) = −uiw(α)Txi − `i(w(α)Txi). (29)\nMoreover, from the definition of the primal and dual optimization problems (1), (2), we can write the duality gap as\nG(α) := P(w(α))−D(α) (1),(2)= 1 n n∑ i=1 ( `i(x T j w) + ` ∗ i (−αi) + w(α)Txiαi ) . (30)\nHence,\nC (29) ≤ 1\nn n∑ i=1 −suiw(α)Txi − s`i(w(α)Txi)− s`∗i (−αi)−sw(α)Txiαi + sw(α)Txiαi︸ ︷︷ ︸ 0 −µ 2 (1− s)s(ui − αi)2 \n+ 1\nn w(α)TAs(u− α) + K∑ k=1 λ 2 σ′‖ 1 λn As(u− α)[k]‖2\n= 1\nn n∑ i=1 ( −s`i(w(α)Txi)− s`∗i (−αi)− sw(α)Txiαi ) + 1 n n∑ i=1 ( sw(α)Txi(αi − ui)− µ 2 (1− s)s(ui − αi)2 ) + 1\nn w(α)TAs(u− α) + K∑ k=1 λ 2 σ′‖ 1 λn As(u− α)[k]‖2\n(30) = −sG(α)− µ 2 (1− s)s 1 n n∑ i=1 ‖u− α‖2 + σ ′ 2λ ( s n )2 K∑ k=1 ‖A(u− α)[k]‖2. (31)\nNow (15) follows by plugging (31) to (28)."
    }, {
      "heading" : "B.4. Proof of Lemma 6",
      "text" : "For general convex functions, the strong convexity parameter is µ = 0, and hence the definition of R(t) becomes\nR(t) (16) = K∑ k=1 ‖A(u(t) − α(t))[k]‖2 (19) ≤ K∑ k=1 σk‖(u(t) − α(t))[k]‖2 Lemma 16 ≤ K∑ k=1 σk|Pk|4L2."
    }, {
      "heading" : "B.5. Proof of Theorem 8",
      "text" : "At first let us estimate expected change of dual feasibility. By using the main Lemma 5, we have\nE[D(α∗)−D(α(t+1))] = E[D(α∗)−D(α(t+1)) +D(α(t))−D(α(t))] (15) = D(α∗)−D(α(t))− γ(1−Θ)sG(α(t)) + γ(1−Θ) σ ′\n2λ ( s\nn )2R(t)\n(4) = D(α∗)−D(α(t))− γ(1−Θ)s(P(w(α(t)))−D(α(t))) + γ(1−Θ) σ ′ 2λ ( s\nn )2R(t)\n≤ D(α∗)−D(α(t))− γ(1−Θ)s(D(α∗)−D(α(t))) + γ(1−Θ) σ ′ 2λ ( s\nn )2R(t)\n(18) ≤ (1− γ(1−Θ)s) (D(α∗)−D(α(t))) + γ(1−Θ) σ ′ 2λ ( s\nn )24L2σ. (32)\nUsing (32) recursively we have\nE[D(α∗)−D(α(t))] = (1− γ(1−Θ)s)t (D(α∗)−D(α(0))) + γ(1−Θ) σ ′ 2λ ( s\nn )24L2σ t−1∑ j=0 (1− γ(1−Θ)s)j\n= (1− γ(1−Θ)s)t (D(α∗)−D(α(0))) + γ(1−Θ) σ ′ 2λ ( s\nn )24L2σ\n1− (1− γ(1−Θ)s)t\nγ(1−Θ)s\n≤ (1− γ(1−Θ)s)t (D(α∗)−D(α(0))) + s4L 2σσ′\n2λn2 . (33)\nChoice of s = 1 and t = t0 := max{0, d 1γ(1−Θ) log(2λn 2(D(α∗)−D(α(0)))/(4L2σσ′))e} will lead to\nE[D(α∗)−D(α(t))] ≤ (1− γ(1−Θ))t0 (D(α∗)−D(α(0))) + 4L 2σσ′\n2λn2 ≤ 4L\n2σσ′\n2λn2 +\n4L2σσ′\n2λn2 =\n4L2σσ′\nλn2 . (34)\nNow, we are going to show that\n∀t ≥ t0 : E[D(α∗)−D(α(t))] ≤ 4L2σσ′\nλn2(1 + 12γ(1−Θ)(t− t0)) . (35)\nClearly, (34) implies that (35) holds for t = t0. Now imagine that it holds for any t ≥ t0 then we show that it also has to hold for t+ 1. Indeed, using\ns = 1\n1 + 12γ(1−Θ)(t− t0) ∈ [0, 1] (36)\nwe obtain\nE[D(α∗)−D(α(t+1))] (32) ≤ (1− γ(1−Θ)s) (D(α∗)−D(α(t))) + γ(1−Θ) σ ′ 2λ ( s\nn )24L2σ\n(35) ≤ (1− γ(1−Θ)s) 4L\n2σσ′\nλn2(1 + 12γ(1−Θ)(t− t0)) + γ(1−Θ) σ\n′ 2λ ( s\nn )24L2σ\n(36) =\n4L2σσ′\nλn2\n( 1 + 12γ(1−Θ)(t− t0)− γ(1−Θ) + γ(1−Θ) 1 2\n(1 + 12γ(1−Θ)(t− t0))2 ) = 4L2σσ′\nλn2\n( 1 + 12γ(1−Θ)(t− t0)− 1 2γ(1−Θ)\n(1 + 12γ(1−Θ)(t− t0))2 ) ︸ ︷︷ ︸\nD\n.\nNow, we will upperbound D as follows\nD = 1 1 + 12γ(1−Θ)(t+ 1− t0) (1 + 12γ(1−Θ)(t+ 1− t0))(1 + 1 2γ(1−Θ)(t− 1− t0))\n(1 + 12γ(1−Θ)(t− t0))2︸ ︷︷ ︸ ≤1\n≤ 1 1 + 12γ(1−Θ)(t+ 1− t0) ,\nwhere in the last inequality we have used the fact that geometric mean is less or equal to arithmetic mean.\nIf α is defined as (23) then we obtain that\nE[G(α)] = E [ G ( T−1∑ t=T0 1 T−T0α (t) )] ≤ 1T−T0E [ T−1∑ t=T0 G ( α(t) )] (15),(18) ≤ 1T−T0E [ T−1∑ t=T0 ( 1 γ(1−Θ)s (D(α(t+1))−D(α(t))) + 4L 2σσ′s 2λn2 )]\n= 1 γ(1−Θ)s 1 T − T0 E [ D(α(T ))−D(α(T0)) ] + 4L 2σσ′s 2λn2 ≤ 1 γ(1−Θ)s 1 T − T0 E [ D(α∗)−D(α(T0)) ] + 4L 2σσ′s 2λn2 . (37)\nNow, if T ≥ d 1γ(1−Θ)e+ T0 such that T0 ≥ t0 we obtain\nE[G(α)] (37),(35) ≤ 1 γ(1−Θ)s 1 T − T0\n( 4L2σσ′\nλn2(1 + 12γ(1−Θ)(T0 − t0))\n) + 4L2σσ′s\n2λn2\n= 4L2σσ′\nλn2\n( 1\nγ(1−Θ)s 1 T − T0 1 1 + 12γ(1−Θ)(T0 − t0) + s 2\n) . (38)\nChoosing\ns = 1\n(T − T0)γ(1−Θ) ∈ [0, 1] (39)\ngives us\nE[G(α)] (38),(39) ≤ 4L\n2σσ′\nλn2\n( 1\n1 + 12γ(1−Θ)(T0 − t0) +\n1 (T − T0)γ(1−Θ) 1 2\n) . (40)\nTo have right hand side of (40) smaller then G it is sufficient to choose T0 and T such that\n4L2σσ′\nλn2\n( 1\n1 + 12γ(1−Θ)(T0 − t0)\n) ≤ 1\n2 G, (41)\n4L2σσ′\nλn2\n( 1\n(T − T0)γ(1−Θ) 1 2\n) ≤ 1\n2 G. (42)\nHence of if\nt0 + 2\nγ(1−Θ)\n( 8L2σσ′ λn2 G − 1 ) ≤ T0,\nT0 + 4L2σσ′\nλn2 Gγ(1−Θ) ≤ T,\nthen (41) and (42) are satisfied."
    }, {
      "heading" : "B.6. Proof of Theorem 10",
      "text" : "If the function `i(.) is (1/µ)-smooth then `∗i (.) is µ-strongly convex with respect to the ‖ · ‖ norm. From (16) we have\nR(t) (16) = −λµn(1−s)σ′s ‖u\n(t) − α(t)‖2 + ∑K\nk=1 ‖A(u(t) − α(t))[k]‖2\n(19) ≤ −λµn(1−s)σ′s ‖u\n(t) − α(t)‖2 + ∑K\nk=1 σk‖u(t) − α(t)[k]‖ 2\n≤ −λµn(1−s)σ′s ‖u (t) − α(t)‖2 + σmax ∑K k=1 ‖u(t) − α(t)[k]‖ 2\n= ( −λµn(1−s)σ′s + σmax ) ‖u(t) − α(t)‖2. (43)\nIf we plug\ns = λµn\nλµn+ σmaxσ′ ∈ [0, 1] (44)\ninto (43) we obtain that ∀t : R(t) ≤ 0. Putting the same s into (15) will give us\nE[D(α(t+1))−D(α(t))] (15),(44) ≥ γ(1−Θ) λµn λµn+ σmaxσ′ G(α(t)) ≥ γ(1−Θ) λµn λµn+ σmaxσ′ D(α∗)−D(α(t)). (45)\nUsing the fact that E[D(α(t+1))−D(α(t))] = E[D(α(t+1))−D(α∗)] +D(α∗)−D(α(t)) we have\nE[D(α(t+1))−D(α∗)] +D(α∗)−D(α(t)) (45) ≥ γ(1−Θ) λµn\nλµn+ σmaxσ′ D(α∗)−D(α(t))\nwhich is equivalent with\nE[D(α∗)−D(α(t+1))] ≤ (\n1− γ(1−Θ) λµn λµn+ σmaxσ′\n) D(α∗)−D(α(t)). (46)\nTherefore if we denote by (t)D = D(α∗)−D(α(t)) we have that\nE[ (t)D ] (46) ≤ (\n1− γ(1−Θ) λµn λµn+ σmaxσ′\n)t (0) D (26) ≤ (\n1− γ(1−Θ) λµn λµn+ σmaxσ′\n)t ≤ exp ( −tγ(1−Θ) λµn\nλµn+ σmaxσ′\n) .\nThe right hand side will be smaller than some D if\nt ≥ 1 γ(1−Θ)\nλµn+ σmaxσ ′\nλµn log\n1 D .\nMoreover, to bound the duality gap, we have\nγ(1−Θ) λµn λµn+ σmaxσ′ G(α(t)) (45) ≤ E[D(α(t+1))−D(α(t))] ≤ E[D(α∗)−D(α(t))].\nTherefore G(α(t)) ≤ 1γ(1−Θ) λµn+σmaxσ\n′\nλµn (t) D . Hence if D ≤ γ(1−Θ) λµn λµn+σmaxσ′ G then G(α(t)) ≤ G. Therefore after\nt ≥ 1 γ(1−Θ)\nλµn+ σmaxσ ′\nλµn log\n( 1\nγ(1−Θ) λµn+ σmaxσ\n′\nλµn\n1\nG ) iterations we have obtained a duality gap less than G."
    }, {
      "heading" : "B.7. Proof of Theorem 13",
      "text" : "Because `i are (1/µ)-smooth then functions `∗i are µ strongly convex with respect to the norm ‖ · ‖. The proof is based on techniques developed in recent coordinate descent papers, including (Richtárik & Takáč, 2014; 2013b; 2012; Tappenden et al., 2015; Mareček et al., 2014; Fercoq & Richtárik, 2013; Lu & Xiao, 2013; Fercoq et al., 2014; Qu & Richtárik, 2014; Qu et al., 2014) (Efficient accelerated variants were considered in (Fercoq & Richtárik, 2013; Shalev-Shwartz & Zhang, 2013a)).\nFirst, let us define the function F (ζ) : Rnk → R as F (ζ) := −Gσ′k ( ∑ i∈Pk ζiei, w). This function can be written in two\nparts F (ζ) = Φ(ζ) + f(ζ). The first part denoted by Φ(ζ) = 1n ∑ i∈Pk ` ∗ i (−αi − ζi) is strongly convex with convexity parameter µn with respect to the standard Euclidean norm. In our application, we think of the ζ variable collecting the local dual variables ∆α[k].\nThe second part we will denote by f(ζ) = 1K λ 2 ‖w(α)‖ 2 + 1n ∑ i∈Pk w(α) Txiζi + λ 2σ ′ 1 λ2n2 ‖ ∑ i∈Pk xiζi‖\n2. It is easy to show that the gradient of f is coordinate-wise Lipschitz continuous with Lipschitz constant σ ′\nλn2 rmax with respect to the standard Euclidean norm.\nFollowing the proof of Theorem 20 in (Richtárik & Takáč, 2012), we obtain that\nE[Gσ ′ k (∆α ∗ [k], w)− G σ′ k (∆α (h+1) [k] , w)] ≤\n( 1− 1\nnk 1 + µnλσ′rmax µnλ σ′rmax\n)( Gσ ′\nk (∆α ∗ [k], w)− G\nσ′\nk (∆α (h) [k] , w) ) = ( 1− 1\nnk\nλnµ\nσ′rmax + λnµ\n)( Gσ ′\nk (∆α ∗ [k], w)− G\nσ′\nk (∆α (h) [k] , w)\n) .\nOver all steps up to step h, this gives\nE[Gσ ′ k (∆α ∗ [k], w)− G σ′ k (∆α (h) [k] , w)] ≤\n( 1− 1\nnk\nλnµ\nσ′rmax + λnµ\n)h ( Gσ ′\nk (∆α ∗ [k], w)− G\nσ′ k (0, w) ) .\nTherefore, choosing H as in the assumption of our Theorem, given in Equation (24), we are guaranteed that( 1− 1nk λnµ σ′rmax+λnµ )H ≤ Θ, as desired."
    }, {
      "heading" : "B.8. Proof of Theorem 14",
      "text" : "Similarly as in the proof of Theorem 13 we define a composite function F (ζ) = f(ζ) + Φ(ζ). However, in this case functions `∗i are not guaranteed to be strongly convex. However, the first part has still a coordinate-wise Lipschitz continuous gradient with constant σ ′\nλn2 rmax with respect to the standard Euclidean norm. Therefore from Theorem 3 in (Tappenden et al., 2015) we have that\nE[Gσ ′ k (∆α ∗ [k], w)− G σ′ k (∆α (h) [k] , w)] ≤ nk nk + h\n( Gσ ′\nk (∆α ∗ [k], w)− G\nσ′ k (0, w) + 1\n2 σ′rmax λn2 ‖∆α∗[k]‖ 2\n) . (47)\nNow, choice of h = H from (25) is sufficient to have the right hand side of (47) to be ≤ Θ(Gσ′k (∆α∗[k], w)− G σ′ k (0, w))."
    } ],
    "references" : [ {
      "title" : "Distributed Learning, Communication Complexity and Privacy",
      "author" : [ "Balcan", "Maria-Florina", "Blum", "Avrim", "Fine", "Shai", "Mansour", "Yishay" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2012
    }, {
      "title" : "Accelerated, parallel and proximal coordinate descent",
      "author" : [ "Fercoq", "Olivier", "Richtárik", "Peter" ],
      "venue" : null,
      "citeRegEx" : "Fercoq et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Fercoq et al\\.",
      "year" : 2013
    }, {
      "title" : "Fast distributed coordinate descent for nonstrongly convex losses",
      "author" : [ "Fercoq", "Olivier", "Qu", "Zheng", "Richtárik", "Peter", "Takáč", "Martin" ],
      "venue" : "IEEE Workshop on Machine Learning for Signal Processing,",
      "citeRegEx" : "Fercoq et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Fercoq et al\\.",
      "year" : 2014
    }, {
      "title" : "Consensus-Based Distributed Support",
      "author" : [ "Forero", "Pedro A", "Cano", "Alfonso", "Giannakis", "Georgios B" ],
      "venue" : "Vector Machines. JMLR,",
      "citeRegEx" : "Forero et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Forero et al\\.",
      "year" : 2010
    }, {
      "title" : "Communication-efficient distributed dual coordinate ascent",
      "author" : [ "Jaggi", "Martin", "Smith", "Virginia", "Takáč", "Terhorst", "Jonathan", "Krishnan", "Sanjay", "Hofmann", "Thomas", "Jordan", "Michael I" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Jaggi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jaggi et al\\.",
      "year" : 2014
    }, {
      "title" : "Distributed optimization with arbitrary local solvers",
      "author" : [ "Konečný", "Jakub", "Ma", "Chenxin", "Richtárik", "Peter", "Jaggi", "Martin", "Takáč" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Konečný et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Konečný et al\\.",
      "year" : 2014
    }, {
      "title" : "Asynchronous stochastic coordinate descent: Parallelism and convergence properties",
      "author" : [ "Liu", "Ji", "Wright", "Stephen J" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2014
    }, {
      "title" : "An Asynchronous Parallel Stochastic Coordinate Descent Algorithm",
      "author" : [ "Liu", "Ji", "Wright", "Stephen J", "Ré", "Christopher", "Bittorf", "Victor", "Sridhar", "Srikrishna" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Liu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2014
    }, {
      "title" : "On the complexity analysis of randomized block-coordinate descent methods",
      "author" : [ "Lu", "Zhaosong", "Xiao", "Lin" ],
      "venue" : "arXiv preprint arXiv:1305.4723,",
      "citeRegEx" : "Lu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient LargeScale Distributed Training of Conditional Maximum Entropy Models",
      "author" : [ "Mann", "Gideon", "McDonald", "Ryan", "Mohri", "Mehryar", "Silberman", "Nathan", "Walker", "Daniel D" ],
      "venue" : null,
      "citeRegEx" : "Mann et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mann et al\\.",
      "year" : 2009
    }, {
      "title" : "Distributed block coordinate descent for minimizing partially separable functions",
      "author" : [ "Mareček", "Jakub", "Richtárik", "Peter", "Takáč", "Martin" ],
      "venue" : null,
      "citeRegEx" : "Mareček et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mareček et al\\.",
      "year" : 2014
    }, {
      "title" : "LOCO: Distributing Ridge Regression with Random Projections",
      "author" : [ "McWilliams", "Brian", "Heinze", "Christina", "Meinshausen", "Nicolai", "Krummenacher", "Gabriel", "Vanchinathan", "Hastagiri P" ],
      "venue" : "In NIPS 2014 Workshop on Distributed Machine Learning and Matrix Computations,",
      "citeRegEx" : "McWilliams et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "McWilliams et al\\.",
      "year" : 2014
    }, {
      "title" : "Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent",
      "author" : [ "Niu", "Feng", "Recht", "Benjamin", "Ré", "Christopher", "Wright", "Stephen J" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Niu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2011
    }, {
      "title" : "Coordinate descent with arbitrary sampling I: Algorithms and complexity",
      "author" : [ "Qu", "Zheng", "Richtárik", "Peter" ],
      "venue" : null,
      "citeRegEx" : "Qu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2014
    }, {
      "title" : "Randomized dual coordinate ascent with arbitrary sampling",
      "author" : [ "Qu", "Zheng", "Richtárik", "Peter", "Zhang", "Tong" ],
      "venue" : null,
      "citeRegEx" : "Qu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2014
    }, {
      "title" : "Parallel coordinate descent methods for big data optimization",
      "author" : [ "Richtárik", "Peter", "Takáč", "Martin" ],
      "venue" : null,
      "citeRegEx" : "Richtárik et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Richtárik et al\\.",
      "year" : 2012
    }, {
      "title" : "On optimal probabilities in stochastic coordinate descent methods",
      "author" : [ "Richtárik", "Peter", "Takáč", "Martin" ],
      "venue" : null,
      "citeRegEx" : "Richtárik et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Richtárik et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed coordinate descent method for learning with big data",
      "author" : [ "Richtárik", "Peter", "Takáč", "Martin" ],
      "venue" : "arXiv preprint arXiv:1310.2059,",
      "citeRegEx" : "Richtárik et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Richtárik et al\\.",
      "year" : 2013
    }, {
      "title" : "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
      "author" : [ "Richtárik", "Peter", "Takáč", "Martin" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Richtárik et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Richtárik et al\\.",
      "year" : 2014
    }, {
      "title" : "Accelerated minibatch stochastic dual coordinate ascent",
      "author" : [ "Shalev-Shwartz", "Shai", "Zhang", "Tong" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2013
    }, {
      "title" : "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
      "author" : [ "Shalev-Shwartz", "Shai", "Zhang", "Tong" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2013
    }, {
      "title" : "Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization",
      "author" : [ "Shalev-Shwartz", "Shai", "Zhang", "Tong" ],
      "venue" : "JMLR, 14:567–599,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed Stochastic Optimization and Learning",
      "author" : [ "Shamir", "Ohad", "Srebro", "Nathan" ],
      "venue" : "In Allerton,",
      "citeRegEx" : "Shamir et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shamir et al\\.",
      "year" : 2014
    }, {
      "title" : "Communication efficient distributed optimization using an approximate newton-type method",
      "author" : [ "Shamir", "Ohad", "Srebro", "Nathan", "Zhang", "Tong" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Shamir et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shamir et al\\.",
      "year" : 2014
    }, {
      "title" : "Communication-Efficient Distributed Optimization using an Approximate Newton-type Method",
      "author" : [ "Shamir", "Ohad", "Srebro", "Nathan", "Zhang", "Tong" ],
      "venue" : null,
      "citeRegEx" : "Shamir et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shamir et al\\.",
      "year" : 2014
    }, {
      "title" : "On the complexity of parallel coordinate descent",
      "author" : [ "Tappenden", "Rachael", "Takáč", "Martin", "Richtárik", "Peter" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Tappenden et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tappenden et al\\.",
      "year" : 2015
    }, {
      "title" : "Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent",
      "author" : [ "Yang", "Tianbao" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Yang and Tianbao.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yang and Tianbao.",
      "year" : 2013
    }, {
      "title" : "On Theoretical Analysis of Distributed Stochastic Dual Coordinate Ascent",
      "author" : [ "Yang", "Tianbao", "Zhu", "Shenghuo", "Jin", "Rong", "Lin", "Yuanqing" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2013
    }, {
      "title" : "Large Linear Classification When Data Cannot Fit in Memory",
      "author" : [ "Yu", "Hsiang-Fu", "Hsieh", "Cho-Jui", "Chang", "Kai-Wei", "Lin", "Chih-Jen" ],
      "venue" : "ACM Transactions on Knowledge Discovery from Data,",
      "citeRegEx" : "Yu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2012
    }, {
      "title" : "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing",
      "author" : [ "Zaharia", "Matei", "Chowdhury", "Mosharaf", "Das", "Tathagata", "Dave", "Ankur", "McCauley", "Murphy", "Franklin", "Michael J", "Shenker", "Scott", "Stoica", "Ion" ],
      "venue" : null,
      "citeRegEx" : "Zaharia et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zaharia et al\\.",
      "year" : 2012
    }, {
      "title" : "Communication-efficient distributed optimization of self-concordant empirical loss",
      "author" : [ "Zhang", "Yuchen", "Xiao", "Lin" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Communication-Efficient Algorithms for Statistical Optimization",
      "author" : [ "Zhang", "Yuchen", "Duchi", "John C", "Wainwright", "Martin J" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    }, {
      "title" : "Parallelized Stochastic Gradient Descent",
      "author" : [ "Zinkevich", "Martin A", "Weimer", "Markus", "Smola", "Alex J", "Li", "Lihong" ],
      "venue" : "NIPS 2010: Advances in Neural Information Processing Systems",
      "citeRegEx" : "Zinkevich et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zinkevich et al\\.",
      "year" : 2010
    }, {
      "title" : "Proof of Lemma 3 See (Richtárik & Takáč, 2013b). B.2",
      "author" : [ "B. Proofs B" ],
      "venue" : "Proof of Lemma",
      "citeRegEx" : "B.1.,? \\Q2013\\E",
      "shortCiteRegEx" : "B.1.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "For this class of problems, the recently proposed COCOA approach (Yang, 2013; Jaggi et al., 2014; Konečný et al., 2014) has formulated a communication efficient primaldual coordinate ascent scheme which targets the communication bottleneck, by allowing more computation to be performed on data local subproblems on each machine before a single vector is communicated.",
      "startOffset" : 65,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "For this class of problems, the recently proposed COCOA approach (Yang, 2013; Jaggi et al., 2014; Konečný et al., 2014) has formulated a communication efficient primaldual coordinate ascent scheme which targets the communication bottleneck, by allowing more computation to be performed on data local subproblems on each machine before a single vector is communicated.",
      "startOffset" : 65,
      "endOffset" : 119
    }, {
      "referenceID" : 4,
      "context" : "While the existing analysis for COCOA in (Jaggi et al., 2014) only covered smooth loss functions, here we extend the class of functions where the rates apply (to include e.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "For an overview over the currently active research field, we refer the reader to (Balcan et al., 2012; Richtárik & Takáč, 2013b; Duchi et al., 2013; Yang, 2013; ?; Liu & Wright, 2014; Fercoq & Richtárik, 2013; Fercoq et al., 2014; Jaggi et al., 2014; Shamir & Srebro, 2014; Shamir et al., 2014a; Zhang & Xiao, 2015; Qu & Richtárik, 2014) and the references therein.",
      "startOffset" : 81,
      "endOffset" : 337
    }, {
      "referenceID" : 2,
      "context" : "For an overview over the currently active research field, we refer the reader to (Balcan et al., 2012; Richtárik & Takáč, 2013b; Duchi et al., 2013; Yang, 2013; ?; Liu & Wright, 2014; Fercoq & Richtárik, 2013; Fercoq et al., 2014; Jaggi et al., 2014; Shamir & Srebro, 2014; Shamir et al., 2014a; Zhang & Xiao, 2015; Qu & Richtárik, 2014) and the references therein.",
      "startOffset" : 81,
      "endOffset" : 337
    }, {
      "referenceID" : 4,
      "context" : "For an overview over the currently active research field, we refer the reader to (Balcan et al., 2012; Richtárik & Takáč, 2013b; Duchi et al., 2013; Yang, 2013; ?; Liu & Wright, 2014; Fercoq & Richtárik, 2013; Fercoq et al., 2014; Jaggi et al., 2014; Shamir & Srebro, 2014; Shamir et al., 2014a; Zhang & Xiao, 2015; Qu & Richtárik, 2014) and the references therein.",
      "startOffset" : 81,
      "endOffset" : 337
    }, {
      "referenceID" : 4,
      "context" : "The crucial difference compared to the existing COCOA algorithm (Jaggi et al., 2014) is the more general subproblem, as defined in (9), as well as the aggregation parameter γ.",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "The analysis in (Jaggi et al., 2014) only covered the case of smooth loss functions.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "If we choose the averaging option γ := 1 K for aggregating the updates, together with σ′ := 1, then the resulting Algorithm 1 is identical with COCOA analyzed in (Jaggi et al., 2014).",
      "startOffset" : 162,
      "endOffset" : 182
    }, {
      "referenceID" : 12,
      "context" : "Several distributed variants of SGD have been proposed (Niu et al., 2011; Liu et al., 2014; Duchi et al., 2013).",
      "startOffset" : 55,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "Several distributed variants of SGD have been proposed (Niu et al., 2011; Liu et al., 2014; Duchi et al., 2013).",
      "startOffset" : 55,
      "endOffset" : 111
    }, {
      "referenceID" : 31,
      "context" : "At the other extreme of the spectrum, we find the group of distributed methods only using a single round of communication such as (Zhang et al., 2013; Zinkevich et al., 2010; Mann et al., 2009; Zhang et al., 2013; McWilliams et al., 2014).",
      "startOffset" : 130,
      "endOffset" : 238
    }, {
      "referenceID" : 32,
      "context" : "At the other extreme of the spectrum, we find the group of distributed methods only using a single round of communication such as (Zhang et al., 2013; Zinkevich et al., 2010; Mann et al., 2009; Zhang et al., 2013; McWilliams et al., 2014).",
      "startOffset" : 130,
      "endOffset" : 238
    }, {
      "referenceID" : 9,
      "context" : "At the other extreme of the spectrum, we find the group of distributed methods only using a single round of communication such as (Zhang et al., 2013; Zinkevich et al., 2010; Mann et al., 2009; Zhang et al., 2013; McWilliams et al., 2014).",
      "startOffset" : 130,
      "endOffset" : 238
    }, {
      "referenceID" : 31,
      "context" : "At the other extreme of the spectrum, we find the group of distributed methods only using a single round of communication such as (Zhang et al., 2013; Zinkevich et al., 2010; Mann et al., 2009; Zhang et al., 2013; McWilliams et al., 2014).",
      "startOffset" : 130,
      "endOffset" : 238
    }, {
      "referenceID" : 11,
      "context" : "At the other extreme of the spectrum, we find the group of distributed methods only using a single round of communication such as (Zhang et al., 2013; Zinkevich et al., 2010; Mann et al., 2009; Zhang et al., 2013; McWilliams et al., 2014).",
      "startOffset" : 130,
      "endOffset" : 238
    }, {
      "referenceID" : 0,
      "context" : "(Balcan et al., 2012) shows additional relevant lower bounds on the minimum number of communication rounds necessary for a given approximation quality for similar machine learning problems.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "The COCOA framework (Jaggi et al., 2014) allows using local solvers of weak local approximation quality in each round, while still giving a convergence rate for smooth losses.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 28,
      "context" : "By making use of the primal-dual structure in the line of work of (Yu et al., 2012; Shalev-Shwartz & Zhang, 2013c; Yang, 2013; Yang et al., 2013; Jaggi et al., 2014), the COCOA framework as well as the extension here allow to have more control over the meaningful aggregation of updates between different machines.",
      "startOffset" : 66,
      "endOffset" : 165
    }, {
      "referenceID" : 27,
      "context" : "By making use of the primal-dual structure in the line of work of (Yu et al., 2012; Shalev-Shwartz & Zhang, 2013c; Yang, 2013; Yang et al., 2013; Jaggi et al., 2014), the COCOA framework as well as the extension here allow to have more control over the meaningful aggregation of updates between different machines.",
      "startOffset" : 66,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "By making use of the primal-dual structure in the line of work of (Yu et al., 2012; Shalev-Shwartz & Zhang, 2013c; Yang, 2013; Yang et al., 2013; Jaggi et al., 2014), the COCOA framework as well as the extension here allow to have more control over the meaningful aggregation of updates between different machines.",
      "startOffset" : 66,
      "endOffset" : 165
    }, {
      "referenceID" : 13,
      "context" : "Mini-batch versions of both SGD and coordinate descent (CD) (Richtárik & Takáč, 2013b;a; Shalev-Shwartz & Zhang, 2013b; Yang, 2013; Qu & Richtárik, 2014; Qu et al., 2014) suffer from their convergence rate degrading towards the rate of batch gradient descent, as the size of the mini-batch is increased.",
      "startOffset" : 60,
      "endOffset" : 170
    }, {
      "referenceID" : 4,
      "context" : "This follows because the updates from all datapoints in the minibatch are based on the previous parameter vector w, contrasting methods that allow local updates such as COCOA or local-SGD (Jaggi et al., 2014).",
      "startOffset" : 188,
      "endOffset" : 208
    }, {
      "referenceID" : 3,
      "context" : "in (Forero et al., 2010).",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 29,
      "context" : "Implementation Details We implement COCOA+ and all other algorithms for comparison in Apache Spark (Zaharia et al., 2012), and run them in the Amazon cloud, using m3.",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 4,
      "context" : "The analysis for this non-smooth loss was not covered in (Jaggi et al., 2014) but has been captured here, and thus is both theoretically and practically justified.",
      "startOffset" : 57,
      "endOffset" : 77
    } ],
    "year" : 2015,
    "abstractText" : "Distributed optimization algorithms for largescale machine learning suffer from a communication bottleneck. Reducing communication makes the efficient aggregation of partial work from different machines more challenging. In this paper we present a novel generalization of the recent communication efficient primal-dual coordinate ascent framework (COCOA). Our framework, COCOA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes only allowed conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both COCOA as well as our new variants, and generalize the theory for both methods to also cover non-smooth convex loss functions. We provide an extensive experimental comparison on several real-world distributed datasets, showing markedly improved performance, especially when scaling up the number of machines.",
    "creator" : "LaTeX with hyperref package"
  }
}