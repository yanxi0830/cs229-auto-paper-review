{
  "name" : "1611.04847.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Subgraph Detection with cues using Belief Propagation",
    "authors" : [ "Rajesh Sundaresan", "A.Kadavankandy", "K. Avrachenkov", "L. Cottatellucci" ],
    "emails" : [ "arun.kadavankandy@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 1.\n04 84\n7v 1\n[ cs\n.L G\n] 1\n0 N\nov 2\n01 6\nIS S\nN 02\n49 -6\n39 9\nIS R\nN IN\nR IA\n/R R\n-- X\nX X\n-- F\nR +\nE N\nG\nRESEARCH REPORT\nN° XXX Oct 2016\nProject-Team Maestro\nSubgraph Detection with cues using Belief Propagation A.Kadavankandy, K. Avrachenkov, L. Cottatellucci, and Rajesh Sundaresan.\nRESEARCH CENTRE SOPHIA ANTIPOLIS – MÉDITERRANÉE\n2004 route des Lucioles - BP 93 06902 Sophia Antipolis Cedex\nSubgraph Detection with cues using Belief\nPropagation\nA.Kadavankandy∗, K. Avrachenkov,\nL. Cottatellucci, and Rajesh Sundaresan.\nProject-Team Maestro\nResearch Report n° XXX — Oct 2016 — 17 pages\nAbstract: We consider an Erdős-Rényi graph with n nodes and edge probability q that is embedded with a random subgraph of size K with edge probabilities p such that p > q. We address the problem of detecting the subgraph nodes when only the graph edges are observed, along with some extra knowledge of a small fraction of subgraph nodes, called cued vertices or cues. We employ a local and distributed algorithm called belief propagation (BP). Recent works on subgraph detection without cues have shown that global maximum likelihood (ML) detection strictly outperforms BP in terms of asymptotic error rate, namely, there is a threshold condition that the subgraph parameters should satisfy below which BP fails in achieving asymptotically zero error, but ML succeeds. In contrast, we show that when the fraction of cues is strictly bounded away from zero, i.e., when there exists non-trivial side-information, BP achieves zero asymptotic error even below this threshold, thus approaching the performance of ML detection.\nKey-words: Belief Propagation, Subgraph Detection, Semisupervised Learning, Random Graphs\n∗ Corresponding author, arun.kadavankandy@inria.fr\nLa Detection de Sousgraphes en presence des indices grâce\nau Belief Propagation\nRésumé : Nous considérons un graphe Erdős-Rényi qui a n sommets dont q est la probabilité d’arrêtes. La dessus il y un sousgraphe placé sur leurs m sommets selectionnés aléatoirement et leur probabilité d’arrêtes est p, en sorte que p > q. Nous proposons un algorithme distribué aux calculs locales à chaque sommet, tiré du “Belief Propagation” (BP), qui détecte les sommets du sousgraphe, quand on connait une fraction de sommets du sousgraphe en tant qu’indices. Des recherches récentes ont prouvé que la prestation du BP dans l’absence des indices est strictement inférior par rapport à la detection globale du maximum de vraisemblance (DMV). A l’opposé, ici on prouve qu’en presence des indices, la prestation du BP est à l’hauteur de celle de DMV, dans la sens où le premier reussie à detecter la sousgraphe avec une erreur qui tend a zéro, à chaque fois le dernier peut le faire, dans la limite où le nombre de sommets du graph tend l’infinité.\nMots-clés : Belief Propagation, Detection de Sousgraphes, Semisupervised Learning, Graphes Aléatoires\nSubgraph Detection with cues using Belief Propagation 3\n1 Introduction\nDetecting a small community of highly connected nodes in a sparse network is an important problem in data mining, machine learning, and theoretical computer science. This problem is linked to threat detection, anomaly detection, fault detection etc. in a network. Please see [1] for a survey. The hidden subgraph model with both the subgraph and the background modelled as ER graphs with different edge densities was proposed in [10] to study anomalous transactions in a computer network.\nIn this model the background graph is ER with n nodes and edge probability q. A random subset of K < n/2 vertices has the edge probabilities within it changed to p > q, without affecting any other edge. This graph, denoted G(K,n, p, q), can model a network with a hidden community [10]. See [8,9] for works on detecting the presence of such a subgraph in a given graph and detecting the subgraph nodes. A stream of recent works in this area suggests that there is a subgraph size below which detection is impossible by means of nearly linear-time algorithms, and above which efficient algorithms have been identified ( [2,7,9,11]). When q = 1/2 and p = 1, the subgraph detection problem reduces to the well-known clique detection problem ( [2], [9], [5]).\nWe use the framework of analysis developed in [7] and [11]. In [11] the author considers the problem of detecting the hidden community in G(K,n, p, q) under the assumption that p = a/n, q = b/n and K = κ/n, where a, b, κ are constants. A parameter λ (defined later) is introduced to characterise the “strength” of the subgraph. They develop a local BP algorithm and show it achieves zero asymptotic error when λ > 1/exp(1), whereas if λ ≤ 1/exp(1), BP does not do better than random guessing. In contrast, the ML detector achieves zero error asymptotically for any λ > 0. In [7], the authors consider a more general setting of sparse graphs and prove that BP succeeds when λ > 1/e. In [6] global ML detection is shown to achieve zero error rate asymptotically if λ = Θ((K/n) log(K/n)).\nThe optimality of BP with side information is shown in [12] and [4] for community detection on SBM with symmetric communities, but to the best of our knowledge no theoretical studies of BP have been made when cued vertices are available for detecting a small subgraph in G(K,n, p, q).\nOur contributions: In our work we consider the subgraph detection problem where some side information in the form of cued nodes is available. This fits within the framework of semisupervised learning. We develop a BP algorithm that detects the nodes of the subgraph in the presence of cues and prove that when the graph is dilute, with p, q = Θ(1/n), the fraction of miss-classified nodes approaches zero for any λ > 0 when there is a strictly positive fraction of cues. In other words we show that BP with cues succeeds in the entire regime where ML succeeds [11, Proposition 4.1].\nThe paper is organised as follows: In section 2 we describe our graph model and the problem. In section 3, we present our algorithm and its derivation. In section 4 we derive the asymptotic distribution of BP messages. In section 5 we prove our main result on the asymptotic error rate of our algorithm. In section 6 we present some simulation results to back up the theory.\n2 Model and Problem Definition\nLet G = (V,E) be a realisation of G(K,n, p, q). Let S be the set of subgraph nodes and C be the set of cued nodes. The latter is chosen from S by independent Bernoulli sampling with probability (w.p.) α < 1. Let p = a/n and q = b/n, where a and b are constants independent of n. Such graphs, with average degree O(1) are called dilute graphs. The results in this paper presuppose that κ = K/n is a constant independent of n. Our aim is to propose a candidate set Ŝ given G and C, assuming p, q,K and α are known, using local and recursive updates provided\nRR n° XXX\n4 Arun et al\nby BP. Note that this problem is identical to detecting the hidden labels σi of the graph nodes assigned such that σi = 1 if i ∈ S and σi = 0 otherwise.\nNotation and Nomenclature: A graph node is denoted by a lower case letter such as i. The graph distance between two nodes i and j is the length of the shortest sequence of edges to go from i to j. The neighbourhood of a node i, denoted by δi is the set of one-hop neighbours of i, i.e., nodes that are at a graph distance of one. Similarly, we also work with t− hop neighbours of i, which are the set of nodes at a distance of t from i. We use the following symbols to denote set operations: C = A\\B is the set of elements that belong to A and not B when B ⊂ A, and ∆ denotes the set difference, i.e., A∆B = (A∪B)\\(A∩B). The symbol ∼ denotes the distribution of a random variable (rv), for example X ∼ Poi(λ) means that X is a Poisson distributed rv with rate λ. Also, N (µ, σ2) denotes the Gaussian distribution with mean µ and variance σ2. The symbol D−→ denotes convergence in distribution [3].\n3 Belief Propagation Algorithm for Detection in the Pres-\nence of Cues\nIn this section we describe the local and distributed BP algorithm (1), which performs detection in the presence of side-information available in the form of cued nodes. The algorithm has two stages: message passing (2), and belief updation (1). At step t of Algorithm 1, each node u ∈ V \\C updates its own log-likelihood ratio:\nRtu = log\n( P(Gtu, C t u|σu = 1)\nP(Gtu, C t u|σu = 0)\n) ,\nwhere Gtu denotes the subgraph induced by the t-neighbourhood of u and C t u is the set of cues in Gtu. This computation is local, because it uses only messages transmitted to u by its neighbours i ∈ δu, given by\nRti→u = log\n( P(Gti, C t i |σi = 1)\nP(Gti, C t i |σi = 0)\n) ,\nwhere Gti and C t i are defined as done for u. It can be checked that the total computation time for tf steps of BP is O(tf |E|). Recall that the optimum detector that minimises the expected number of misclassified nodes is the ML detector [6] given as:\nσ̂i = 1{Ri>log(n−K/K(1−α))},\nwhere\nRi = log P(G,C|σi = 1) P(G,C|σi = 0) .\nThe output set size here may not be exactly equal to K, but this can be mitigated by some post-processing (for example, if the size is larger than K, we can simply pick the top K values of the likelihood function). This detector however requires the observation of the whole graph, and cannot be implemented in a distributed fashion. In addition, it is not computationally feasible, since it requires marginalising over 2n pair-wise dependent random variables over a large graph. We would like algorithms that, for a decision at a node u of the graph, rely only on the observation of the t-neighbourhood Gtu of u, BP being one of them.\nA small neighbourhood of a large sparse graph can be approximated by a tree. This is formalised in Lemma 1. In the following we present a Poisson random tree, which can be coupled with Gtu.\nInria\nSubgraph Detection with cues using Belief Propagation 5\nAlgorithm 1 BP with cues\n1: Initialize: Set R0i→j to 0, for all (i, j) ∈ E. Let tf < log(n)log(np) + 1. Set t = 0. 2: For all directed pairs (i, u) ∈ E, such that u /∈ C:\nRt+1i→u = −K(p− q) + ∑\nl∈Ci,l 6=u log\np q +\n∑\nl∈δi\\Ci,l 6=u log exp(Rtl→i − ν)(p/q)(1 − α) + 1 exp(Rtl→i − ν)(1 − α) + 1\n(1)\n3: If t < tf − 1 go back to 2, else go to 4 4: Compute R tf u for every u ∈ V \\C as follows:\nRt+1u = −K(p− q) + ∑\nl∈Cu log\np q +\n∑\nl∈δu\\Cu log exp(Rtl→u − ν)(p/q)(1 − α) + 1 exp(Rtl→u − ν)(1 − α) + 1\n(2)\n5: Output Ŝ as the union of C and the K − |C| set of nodes in V \\C with the largest values of R tf u .\nLet T tu be a labelled Galton-Watson (G-W) tree of depth t rooted at node u constructed as follows (as in [7]): The label τu at node u is chosen at random in the following way:\nP{τu = 1} = K\nn P{τu = 0} = n−K n .\nThe number of children Nu of the root u is Poisson-distributed with mean d1 = Kp+(n−K)q if τu = 1 and mean d0 = nq if τu = 0. Each child is also assigned a label. The number of children i with label τi = 1 is Poisson distributed with mean Kp if τu = 1 and mean Kq if τi = 0. The number of children with label τi = 0 is Poisson distributed with mean (n−K)q for both τu = 0 and τu = 1. By the independent splitting property of Poisson rvs, this is equivalent to assigning the label τi = 1 to each child i by sampling a Bernoulli rv with probability (w.p.) Kp/d1 if τu = 1 and Kq/d0 if τu = 0. Similarly τi = 0 w.p. (n −K)q/d1 and (n −K)q/d0 for τu = 0, 1 and 1 respectively. Namely, if i is a child of u,\nP(τi = 1|τu = 1) = Kp\nd1 , P(τi = 1|τu = 0) =\nKq\nd0 . (3)\nWe then assign the cue indicator function c such that ci = 1 w.p. α if τi = 1 and ci = 0 if τi = 0. The process is repeated up to depth t giving us C t u, the set of cued neighbours.\nConsider the problem of estimating the label τu of node u /∈ C based on an observation of T tu and Ctu. The optimal ML detector is given as\nτ̂u = 1{Λtu>log( (n−K) K(1−α) )},\nwhere Λtu = log(P(T t u, C t u|τu = 1)/P(T tu, Ctu|τu = 0)). By the following coupling lemma established in [7], the detection of label σu based on G t u is statistically identical to the detection of τu based on T tu :\nRR n° XXX\n6 Arun et al\nLemma 1 [7] For t such that (np)t = no(1), there exists a coupling such that (Gtu, σ t) = (T tu, τ t) with probability 1− n−1+o(1). In our case since p = a/n, any t = o(log(n)) satisfies the condition of the above lemma.\nConsequently, the likelihood ratios in a small neighbourhood Gtu of u are statistically identical to the likelihoods derived on the corresponding G-W tree, which are the BP messages. Hence we proceed by deriving BP recursions in Algorithm 1 for node u assuming Gtu is a tree. Consider a node u ∈ V \\C. We can express the likelihood ratio at u based on an observation of T t+1u , Ct+1u as\nΛt+1u = log P(T t+1u , C t+1 u |τu = 1)\nP(T t+1u , C t+1 u |τu = 0)\n= log P{Nu|τu = 1} P{Nu|τu = 0} + ∑\ni∈δu log\nP(T ti , ci, C t i |τu = 1) P(T ti , ci, C t i |τu = 0) , (4)\nby independence of the children of u given τu. Moreover, P(Nu|τu = 1) = dNu1 e−d1/Nu! , and similarly for P(Nu|τu = 0). Therefore we have\nlog P{Nu|τu = 1} P{Nu|τu = 0} = Nu log d1 d0 − (d1 − d0)\n= Nu log d1 d0 −K(p− q). (5)\nNext we look at the second term in (4). We analyse separately the cued neighbours of u and the non-cue neighbours.\nCase 1 ( ci = 1): We have\nlog P(T ti , ci, C t i |τu = 1)\nP(T ti , ci, C t i |τu = 0)\n(6)\n= log   P(T ti ,ci,C t i ,τi=1|τu=1)+ 0 P(T ti ,ci,C t i ,τi=0|τu=1)\nP(T ti ,ci,C t i ,τi=1|τu=0)+ 0 P(T ti ,ci,C t i ,τi=0|τu=0)\n \n(a) = log\n( P(T ti , ci, C t i , τi = 1|τu = 1)\nP(T ti , ci, C t i , τi = 1|τu = 0)\n)\n= log\n( P(T ti , ci, C t i |τi = 1)P(τi = 1|τu = 1)\nP(T ti , ci, C t i |τi = 1)P(τi = 1|τu = 0)\n)\n(b) = log Kp/d1 Kq/d0 , (7)\nwhere in step (a) we applied the fact that P(ci = 1, τi = 0) = 0 and in (b) we used (3). Case 2 (ci = 0): Observe that P(ci = 0|τi = 1) = 1− α and P(ci = 0|τi = 0) = 1. Note that\nP(T ti , ci, C t i |τu = 1) (8)\n= P(T ti , C t i |τi = 1)P(ci|τi = 1)P(τi = 1|τu = 1)\n+P(T ti , C t i |τi = 0)P(ci|τi = 0)P(τi = 0|τu = 1)\n= P(T ti , C t i |τi = 1)(1− α)\nKp\nd1 + P(T ti , C t i |τi = 0) (n−K)q d1 .\nInria\nSubgraph Detection with cues using Belief Propagation 7\nSimilarly, we can show\nP(T ti , ci, C t i |τu = 0)\n= P(T ti , C t i |τi = 1)(1− α)\nKq\nd0\n+P(T ti , C t i |τi = 0) (n−K)q d0 .\nLet us define\nΛti→u ≡ log ( P(T ti , C t i |τi = 1)\nP(T ti , C t i |τi = 0)\n) ,\nthe message that i sends to u at step t. We plug this into (8). Finally combining (5), (7) and (8) and replacing Λtu with R t u and Λ t i→u with R t i→u, we arrive at (2). The recursive equation (1) can be derived in exactly the same way by looking at the children of i ∈ δu.\n4 Asymptotic Error Analysis\nWe analyse the distributions of BP messages Λti given τi = 1 and τi = 0. This will help us to bound the error rate on a tree. This equals the error rate on G asymptotically since by the coupling Lemma 1 the two are the same with a probability that tends to 1. Notice that since we only focus on non-cued vertices the prior distribution after the observation of cues changes. Therefore P{τi = 1|ci = 0} = K(1−α)/(n−Kα) and P{τi = 0|ci = 0} = (n−K)/(n−Kα) are the prior probabilities of the uncued vertices. For convenience we put a line over the symbols for expectation and probability to denote conditioning w.r.t {ci = 0} when considering the posterior distributions (eg: E,P). Define υ = log ( n−K\nK(1−α)\n) .\nInstead of studying the distribution of Λti, i ∈ V \\C, we look at the log of the ratio of the a-posteriori probabilities of τi given as\nΛ̃ti = log ( P(τi = 1|T ti , Cti , ci = 0) P(τi = 0|T ti , Cti , ci = 0) ) .\nThis is just a matter of choice since by Bayes rule it holds that Λ̃ti = Λ t i − υ. Let ξt+10 , ξt+11 be the random variables with the same distribution as the messages Λ̃t+1i given τi = 0 and τi = 1 respectively, conditioned on {ci = 0}, in the limit as n → ∞. In view of the coupling formulation, it is then straightforward to show that they satisfy the following two recursive distributional evolutionary equations with initial conditions ξ00 = ξ 0 1 = log κ(1− α)/(1− κ):\nξ (t+1) 0 D = h+\nL0c∑\ni=1\nlog p\nq +\nL00∑\ni=1\nf(ξ (t) 0,i) +\nL01∑\ni=1\nf(ξ (t) 1,i) (9)\nξ (t+1) 1 D = h+\nL1c∑\ni=1\nlog p\nq +\nL10∑\ni=1\nf(ξ (t) 0,i) +\nL11∑\ni=1\nf(ξ (t) 1,i), (10)\nwhere, D = means that the L.H.S has the same distribution as the R.H.S. and h = −K(p− q) − log( n−KK(1−α) ) = −κ(a− b)− log( 1−κκ(1−α)) and the function f is defined as\nf(·) ≡ log ( exp(·)(p/q) + 1\nexp(·) + 1\n) .\nRR n° XXX\n8 Arun et al\nThe rvs ξt0,i are independent and identically distributed (iid) and identically distributed to ξ t 0, and ξt1,i are iid with the same distribution as ξ t 1. Furthermore, L00 ∼ Poi((n−K)q) is the rv that equals the number of children of u with label 0 if τu = 0, and L01 ∼ Poi(Kq(1−α)), the number of children with label 1 when τu = 0. Similarly L10 ∼ Poi((n−K)q) and L11 ∼ Poi(Kp(1− α)) denote the number of children of u with label 0 and 1 respectively when τu = 1. Lastly, L0c and L1c are the number of cued children of u when τu = 0 and τu = 1 respectively with L0c ∼ Poi(Kqα) and L1c ∼ Poi(Kpα). We define the parameter λ, interpreted as an effective SNR [11] of the detection problem, as\nλ = K2(p− q)2 (n−K)q (11)\n= κ2(a− b)2 (1− κ)b = κ2b(a/b− 1)2 (1− κ) . (12)\nIf P0 and P1 are the probability measures of ξ t 0 and ξ t 1 respectively, then they are related as\nfollows.\nLemma 2\ndP0 dP1 (ξ) = κ(1− α) 1− κ exp(−ξ).\nIn other words for any integrable function g(·)\nE[g(Λ̃tu)|τu = 0] = κ(1− α) 1− κ E[g(Λ̃ t u)e −Λ̃tu |τu = 1].\nProof: Following the logic in [11], we show this result for g(Λ̃tu) = 1{Λ̃u∈A}, A being some measurable set . The result for general g then follows because any integrable function can be obtained as the limit of a sequence of such rvs [3]. Let Y = (T tu, C t u), the observed rv. Therefore\nE[1{Λ̃tu∈A}|τu = 0] = P[Λ̃ t u ∈ A|τu = 0]\n= P(Λ̃tu ∈ A, τu = 0)\nP{τu = 0}\n= EY [P{Λ̃tu ∈ A, τu = 0|Y }]\nP{τu = 0}\n= EY\n[ 1{Λ̃tu ∈ A}P(τu = 0|Y )\nP(τu = 0)\n]\n(a) = EY\n[ 1{Λ̃tu ∈ A}e−Λ̃ t uP(τu = 1|Y )\nP(τu = 0)\n]\n= P(τu = 1)\nP(τu = 0) E1[1(Λ̃\nt u ∈ A)e−Λ̃ t u ]\n= κ(1− α) 1− κ E1[1(Λ̃ t u ∈ A)e−Λ̃ t u ],\nwhere in (a) we used the fact that P{τu=0|Y } P{τu=1|Y } = exp(−Λ̃tu), and E1 denotes expectation conditioned on the event {τu = 1}.\nInria\nSubgraph Detection with cues using Belief Propagation 9\nNote that the distributional equations (9) and (10) give the asymptotic distributions of the messages on the graph G as n → ∞. These equations do not depend on n because of the choice of p, q and K. For ease of analysis we will presently study the distributions in the limit where a, b → ∞. This limit is taken after n → ∞. Ideally, one would like to analyse the distributions for finite a and b, but this is left for future work. We have the following result on the Gaussianity of the asymptotic messages in the limit where a, b → ∞, after n → ∞.\nProposition 1 In the regime where λ and κ are held fixed and a, b → ∞, we have\nξt+10 D−→ N (− log 1− κ κ(1− α) − 1 2 µ(t+1), µ(t+1))\nξt+11 D−→ N (− log 1− κ κ(1− α) + 1 2 µ(t+1), µ(t+1)),\nwhere µ(t) satisfies the following recursion with initial condition µ0 = 0 :\nµ(t+1) = λα 1−κκ + λE\n( (1−α)2(1−κ)\nκ(1−α)+(1−κ) exp(−µ(t)/2− √ µ(t)Z)\n) , (13)\nwhere the expectation is w.r.t Z ∼ N (0, 1). Remark : When α = 0 (13) reduces to the recursion given in [11] as expected. Proof: Since λ is fixed and b → ∞, we have\nρ ≡ a/b = 1 + √\nλ(1 − κ) κ2b = 1 +O(b−1/2), (14)\nby (12) since λ and κ are fixed. In the proof we use Berry-Essen inequality for Poisson sums [7, Lemma 11]\nLemma 3 Let Sλ = X1 +X2 + . . .XNλ , where Xi : i ≥ 1 are independent, identically distributed random variables with mean µ, variance σ2 and E[|X3i |] ≤ g3, and for some λ > 0, Nλ is a Poi(λ) random variable independent of (Xi : i ≥ 1). Then\nsupx ∣∣∣∣∣P { Sλ − λµ√ λ(µ2 + σ2) } − Φ(x) ∣∣∣∣∣ ≤ CBEg 3 √ λ(µ2 + σ2)3 ,\nwhere CBE = 0.3041.\nFollowing [11], we prove the result by induction on t. First let us verify the result holds when t = 0, for the initial condition that ξ00 = ξ 0 1 = −υ. We only do this for ξt0 since for ξt1 the steps are similar. Observe that\nf(−υ)\n= log\n \npK(1−α) q(n−K) + 1 K(1−α) (n−K) + 1\n \n= log ( 1 + (ρ− 1)κ(1− α)\n1− κα\n)\n(a) = (ρ− 1)κ(1− α) 1− κα − (ρ− 1)2 2 κ2(1− α)2 (1− κα)2 +\nO(b−3/2), (15)\nRR n° XXX\n10 Arun et al\nwhere (a) follows from (14), and Taylor’s expansion around ρ = 1. Similarly,\nf2(−υ) = (ρ− 1)2κ 2(1 − α)2 (1− κα)2 +O(b −3/2), (16)\nlog(ρ) = log(1 + (ρ− 1)) = √\nλ(1− κ) κ2b − λ(1− κ) 2κ2b +\nO(b−3/2), (17)\nand\nlog2(ρ) = λ(1− κ)\nκ2b +O(b−3/2) (18)\nLet us verify the induction result for t = 0. Using the recursion (9) with ξ00 = log κ(1−α) 1−κ = −υ, we can express Eξ10 as\nEξ10 = −κb(ρ− 1)− υ + κbα log(ρ) + b(1− κα)f(−υ).\nNow using (15) and (17) we obtain\nEξ10 = −κ √\nλb(1 − κ) κ2\n− υ + κα √\nλ(1− κ)b κ2 − λ(1− κ)α 2κ\n+\n√ λ(1− κ)b\nκ2 κ(1− α)− (1 − α) 2 2(1− κα)λ(1− κ) (19)\n+O(b−1/2)\n= −υ − λ(1− κ) 2κ α− (1 − α) 2 2(1− κα)λ(1− κ) +O(b −1/2), (20)\nand\nVarξ10 = log 2(ρ)κbα+ f2(−υ)(1− κ)b+ f2(−υ)κb(1− α)\n(a) = λα(1 − κ) κ + (1− α)2(1− κ)λ 1− κα , (21)\nwhere in (a) we used (18) and (16). Comparing (20) and (21) with µ(1) in (13) using µ(0) = 0, we can verify the mean and variance recursions. Next we use Lemma3 to prove gaussianity. Note that we can express ξ10 − h as the Poisson sum of iid mixture random variables as follows\nξ10 − h = L0∑\ni=1\nXi,\nwhere L0 ∼ Poi(nq) = Poi(b), and L(Xi) = καL(p/q)+(1−κ)bL(f(−υ))+(κb(1−α))L(f(−υ)), keeping in mind the independent splitting property of Poissons, where L denotes the law of a random variable. Then by comparing with the form in Lemma 3, λ = b, and the term λ(µ2 + σ2) = Varξ10 = b(µ 2 + σ2), which is finite. Next we calculate E|Xi|3. It is easy to see that\nE|Xi|3 = κα log3(b) + ((1− κ) + κ(1− α))f3(−υ) (22) = O(b−3/2). (23)\nInria\nSubgraph Detection with cues using Belief Propagation 11\nHence bE|Xi|3= O(b−1/2). Therefore the RHS of Lemma (3) becomes\nCBEE|Xi|3√ λ(µ2 + σ2)3 = CBEE|Xi|3√ b3/b2(µ2 + σ2)3\n= CBEbE|Xi|3√ (b(µ2 + σ2))3 = O(b−1/2).\nHaving shown the induction hypothesis for t = 0, we now assume it holds for some t. Notice that f(x) = (ρ− 1) ex1+ex − 12 (ρ− 1)2( e x 1+ex ) 2 +O(b−3/2), by Taylor’s expansion, and using (14). Then by using dominated convergence theorem [3] and Lemma 2 we obtain\nEf(ξt0) = (ρ− 1) κ(1− α) 1− κ E 1 1 + eξ t 1 −\n(ρ− 1)2κ(1 − α) 2(1− κ) E\neξ t 1\n(1 + eξ t 1)2\n+O(b−3/2)\n(24)\nand\nEf(ξt1) = (ρ− 1)E eξ\nt 1\n1 + eξ t 1\n− (ρ− 1) 2\n2 E\ne2ξ t 1\n(1 + eξ t 1)2\n+O(b−3/2). (25)\nNow we take the expectation of both sides of (9) and (10). Here we use the fact that E ∑L\ni=1 Xi = EXiEL if L ∼ Poi and Xi are independent and identically distributed (iid) random variables, hence obtaining\nEξt+10 = h+ log( p\nq )κbα+ Ef(ξt0)(1 − κ)b+ Ef(ξt1)κb(1− α) (26)\nand\nEξt+11 = h+ log( p\nq )κaα+ Ef(ξt0)(1− κ)b+ Ef(ξt1)κa(1− α). (27)\nWe now substitute (24) and (25) in (26) to get:\nEξt+10 = h+ κbα log(ρ) + (1 − κ)b [ (ρ− 1)κ(1− α)\n1− κ E 1 1 + eξ t 1\n− (ρ− 1) 2κ(1− α) 2(1− κ) E eξ t 1 (1 + eξ t 1)2 +O(b−3/2)\n] +\nκb(1− α) [ (ρ− 1)E e ξt1\n1 + eξ t 1\n−\n(ρ− 1)2 2 E e2ξ t 1 (1 + eξ t 1)2 +O(b−3/2)\n] ,\nwhich on simplifying and grouping like terms becomes\nEξt+10 = h+ κbα log(ρ) + κ(a− b)(1 − α)− λ(1 − κ)(1− α)\n2κ E\neξ t 1\n1 + eξ t 1\n.\nRR n° XXX\n12 Arun et al\nSince h = −κ(a− b)− log (\n1−κ κ(1−α)\n)\nEξt+10 = − log ( 1− κ κ(1− α) ) − ακ(a− b) + κbα log(ρ)−\nλ(1 − κ)(1− α) 2κ E eξ t 1 1 + eξ t 1 .\nUsing (17) we get\n−ακ(a− b) + κbα log(ρ) = bακ(log(ρ)− (ρ− 1))\n= bακ(−λ(1− κ) 2κ2b +O(b−3/2)) = −λα(1 − κ) 2κ +O(b−1/2).\nFinally we obtain\nEξt+10 = − log( 1− κ κ(1 − α) )− αλ(1 − κ) 2κ −\nλ (1 − κ)(1− α)\n2κ E(\neξ t 1\n1 + eξ t 1\n) +O(b−1/2).\n(28)\nUsing exactly the same simplifications we can get\nEξt+11 = − log( 1− κ κ(1 − α) ) + αλ(1 − κ) 2κ +\nλ (1 − κ)(1− α)\n2κ E(\neξ t 1\n1 + eξ t 1\n) +O(b−1/2).\n(29)\nObserve that f2(x) = (ρ− 1)2 ( ex\n1+ex\n)2 +O(b−3/2). Therefore\nEf2(ξt0) = (ρ− 1)2E e2ξ\nt 0\n(1 + eξ t 0)2\n+O(b−3/2),\nand using Lemma 2 the above becomes\nEf2(ξt0) = (ρ− 1)2 κ(1− α) 1− κ E\neξ t 1\n(1 + eξ t 1)2\n+O(b−3/2). (30)\nSimilarly,\nEf2(ξt1) = (ρ− 1)2E e2ξ\nt 1\n(1 + eξ t 1)2\n+O(b−3/2). (31)\nNow we use the formula for the variance of Poisson sums Var ∑L\ni=1 Xi = EX 2 i EL, to get\nVar[ξt+10 ] = log 2(ρ)κbα+ (1− κ)bEf2(ξt0)+ κb(1− α)Ef2(ξt1) Var[ξt+11 ] = log\n2(ρ)κaα+ (1 − κ)bEf2(ξt0)+ κa(1− α)Ef2(ξt1).\nInria\nSubgraph Detection with cues using Belief Propagation 13\nSubstituting (30) and (31) into the above equations we get\nVarξt+11 = Varξ t+1 0 = λα(1 − κ) κ + λ(1 − κ)(1− α) κ\nE exp ξt1\n1 + exp(ξt1) .\n(32)\nLet us use µ(t+1) to denote Varξ (t+1) 1 = Varξ (t+1) 0 . Then\nEξt+10 = − log ( (1− κ) κ(1− α) ) − 1 2 µ(t+1) +O(b−1/2) Eξt+11 = − log ( (1− κ) κ(1− α) ) + 1 2 µ(t+1) +O(b−1/2). (33)\nNow we use the fact the induction assumption that ξt1 → N (Eξt1, µ(t)). Since the function 1/(1+ e−ξ t 1) is bounded, by Bounded Convergence Theorem this means E[1/(1 + e−ξ t 1)] → E[1/(1 + e−N (Eξ t 1,µ (t)))]. We can write N (Eξt1, µ(t)) = √ µ(t)Z+Eξt1, where Z ∼ N (0, 1). Therefore we can write and using (33) we obtain\nE 1\n1 + e−ξ t 1\n= E 1\n1 + e− √\nµ(t)Z (1−κ) κ(1−α)e −µ(t)2\n= E κ(1− α)\nκ(1 − α) + (1− κ)e(− √ µtZ−µ(t)2 ) .\nSubstituting the above into (32) gives us the recursion for µ(t+1) given in (13). Next we prove Gaussianity. Consider\nξt+10 − Eξt+10\n= log\n( p\nq\n) (L0c − EL0c) + L00∑\ni=1\n(f(ξt0,i)− Ef(ξt0)) +\nL01∑\ni=1\n(f(ξt1,i)− Ef(ξt1)) + (L00 − EL00)Ef(ξt0) +\n(L01 − EL01)Ef(ξt1). (34)\nLet us look at the second term. Let Xi = f(ξ t 0,i) − Ef(ξt0,i). Then it can be shown that\nEX2i = O(1/b). Let D ≡ ∑L00 i=1 Xi − ∑ EL00 i=1 Xi. Here the summation is taken up to i ≤ EL00.\nThen ED2 = |∑δi=1 Xi|2, where δ ≤ |L00 − EL00|+1, where the extra 1 is because EL00 may not be an integer. Therefore ED2 = EδE|X1|2≤ (C/b)((1 − κ)b + 1)1/2 = O(1/ √ b). Thus, we can replace the Poisson upper limits of the summations in the second and third terms of (34) by their means, leading to\nξt+10 − Eξt+10 = log ( p\nq\n) (L0c − EL0c) + EL00∑\ni=1\n(f(ξt0,i)− Ef(ξt0))\n+\nEL01∑\ni=1\n(f(ξt1,i)− Ef(ξt1)) + (L00 − EL00)Ef(ξt0)+\n(L01 − EL01)Ef(ξt1) + op(1),\n(35)\nRR n° XXX\n14 Arun et al\nwhere op(1) indicates a random variable that goes to zero in probability in the limit. The variance of the above term is µt+1, defined in (13), and it is finite for a fixed t. Now since we have an infinite sum of independent random variables as a, b → ∞, with zero mean and finite variance, from standard CLT we can conclude that the distribution tends N (0, µt+1).\n5 Detection Method\nIt is shown [7] that asymptotically the tests\nŜ0 = {i : Rti > log 1− κ\nκ(1− α)},\nand Ŝ, the output of Algorithm 1, have the same fraction of miss-classified nodes. So we now go on to show that Ŝ0 weakly recovers S, i.e., the expected fraction of missclassified nodes approaches 0, and the result for Ŝ follows. By Lemma 1 we work with Λi instead of Ri. Consider the estimator on the tree:\nτ̂i = { 1 if Λti ≥ log 1−κκ(1−α) , 0 otherwise.\nAlternatively, τ̂i = 1{Λ̃ti≥0} . The above estimator minimises the following error probability:\npe = P{τi = 1}P(τ̂i = 0|i ∈ S) + P(τ̂i = 1|i 6∈ S)P{τi = 0}.\nIn the following proposition, we state and prove the main result of our paper. We show that the expected fraction of miss-classified nodes goes to zero for an infinitesimally small subgraph size, for any λ > 0. This implies that BP with cue beats BP without cues, which requires λ > 1/e for zero asymptotic error rate ( [7, 11]).\nProposition 2 In the regime where a, b → ∞ we have\nlim κ→0\nES∆Ŝ\nK(1− α) → 0,\nfor any λ > 0, i.e., the expected fraction of miss-classified nodes tends to zero, as long as α is strictly positive.\nProof: We upperbound the error rate of Ŝ0 and the result for Ŝ follows based on the explanation in Section 5. By Lemma 1, the Λtu and R t u have the same distributions on an event whose probability goes to 1. Therefore it is sufficient to bound the error for the tree, as follows:\nES∆Ŝ0 K(1− α) = ( n−Kα K −Kα ) pe\n(a) = ( (1− κ) κ(1− α) ) P t0(ξ > 0) + P t 1(ξ < 0) (36)\nwhere in (a) P t0 and P t 1 denote probabilities w.r.t. the distributions of ξ t 0, ξ t 1 respectively. We now analyse the asymptotic value of each term in (36) in the limit as κ → 0 with α fixed. By Proposition 1 we have that in the limit where a → ∞ and b → ∞,\nP t1(ξ < 0) = Q ( 1√ µ(t) ( µ(t) 2 − log( (1− κ) κ(1− α) ) ))\nInria\nSubgraph Detection with cues using Belief Propagation 15\nwhere Q(·) denotes the standardQ function. Notice that by (13) we have that µ(t) ≥ λα(1−κ)/κ, since F (µ) ≡ E 1−κκ(1−α)+(1−κ) exp(−µ/2−√µZ) ≥ 0. In addition, by (32), µ(t) ≤ λ(1−κ) κ . Therefore µ(t) = Θ( 1κ ). Note that the lower bound on µ (t) is not useful when α = 0. Consequently limκ→0 1µ(t) log( (1−κ) κ(1−α) ) = 0. Therefore:\nP t1(ξ < 0) = 1\nκ Q(\n√ µ(t)(1 +O(κ)))\n≤ exp(−Θ(1/κ)) → 0.\nSimilarly we have\n1 κ P t0(ξ > 0) (37)\n= 1\nκ Q(\nlog( (1−κ)κ(1−α) ) + µ 2√\nµ(t) ) (38)\n≤ 1 κ exp(−Θ(1 κ )) (39) → 0. (40)\nSubstituting these back in (36) the result then follows.\n6 Numerical Experiments\nIn this section we provide simulation results to corroborate our theoretical findings and also to demonstrate the performance improvement of Algorithm 1 in the presence of side-information. We fix n = 104, b = 100, and κ = 0.005, giving K = 50. Next we sweep over different values of λ in the range [0.1, 0.8] and average over 1000 graph realisations to find the fraction of missclassified subgraph nodes for each value of λ. In Figure 1 we have the ratio between the number of subgraph nodes wrongly classified by the algorithm and the number of unlabelled subgraph nodes on the y-axis and λ on the x-axis. This demonstrates that there is a marked improvement in the performance of BP with the introduction of cues.\nIn Figure 2 we plot the theoretical error of Algorithm 1 given in (36) against κ for the two cases of α = 0 (no cues) and α = 0.1 (10% cues) for λ = 12e . We have chosen this value of λ in order to be below the detectability threshold of λ = 1e of BP without cues. We can observe that contrary to when α = 0, with α = 0.1 the error decreases as κ decreases, as proved in our analysis. We also observed this in our simulations where we obtained an error rate of 73.86% for κ = 4× 10−4 (n = 5× 104) with α = 0.1, whereas it was 0.995 when α = 0.\n7 Conclusions and Future Extensions\nIn this work we developed a local distributed BP algorithm that takes advantage of sideinformation to detect a dense subgraph embedded in a sparse graph. We obtained theoretical results based on density evolution on trees to show that it achieves zero asymptotic error regardless of the SNR parameter λ, unlike BP without cues, where there is a non-zero detectability threshold. We also obtained some simulation results on synthetic graphs to demonstrate the improvement in error rates in the presence of cues. In the future, we would like to investigate non-asymptotic properties of the algorithm for finite a and b and when K = o(n).\nRR n° XXX\n16 Arun et al\nInria\nSubgraph Detection with cues using Belief Propagation 17\nReferences\n[1] L. Akoglu, H. Tong, and D. Koutra, “Graph based anomaly detection and description: a survey,” Data Mining and Knowledge Discovery, vol. 29, no. 3, pp. 626–688, 2015.\n[2] N. Alon, M. Krivelevich, and B. Sudakov, “Finding a large hidden clique in a random graph,” Random Structures and Algorithms, vol. 13, no. 3-4, pp. 457–466, 1998.\n[3] P. Billingsley, Probability and measure. John Wiley & Sons, 2008.\n[4] T. T. Cai, T. Liang, and A. Rakhlin, “Inference via message passing on partially labeled stochastic block models,” arXiv preprint arXiv:1603.06923, 2016. [5] Y. Deshpande and A. Montanari, “Finding hidden cliques of size √ N/e in nearly linear\ntime,” Foundations of Computational Mathematics, vol. 15, no. 4, pp. 1069–1128, 2015.\n[6] B. Hajek, Y. Wu, and J. Xu, “Information limits for recovering a hidden community,” arXiv preprint arXiv:1509.07859, 2015.\n[7] ——, “Recovering a Hidden Community Beyond the Spectral Limit in O(|E|log∗|V |) Time,” arXiv Prepr. arXiv1510.02786, 2015.\n[8] A. Kadavankandy, L. Cottatellucci, and K. Avrachenkov, “Characterization of L1-norm statistic for Anomaly Detection in Erdös Rényi Graphs,” in CDC. IEEE, 2016. [Online]. Available: http://www.eurecom.fr/en/publication/list/author/cottatellucci-laura\n[9] A. Martinsson, “Lovasz θ function , SVMs and Finding Dense Subgraphs,” J. Mach. Learn. Res., vol. 14, pp. 3495–3536, 2013.\n[10] T. Mifflin, C. Boner, G. Godfrey, and J. Skokan, “A random graph model for terrorist transactions,” in 2004 IEEE Aerosp. Conf. Proc. (IEEE Cat. No.04TH8720), vol. 5. IEEE, 2004, pp. 3258–3264.\n[11] A. Montanari, “Finding one community in a sparse graph,” Journal of Statistical Physics, vol. 161, no. 2, pp. 273–299, 2015.\n[12] E. Mossel and J. Xu, “Local Algorithms for Block Models with Side Information,” in Proc. 2016 ACM Conf. Innov. Theor. Comput. Sci. - ITCS ’16. New York, New York, USA: ACM Press, jan 2016, pp. 71–80.\nContents\n1 Introduction 3\n2 Model and Problem Definition 3\n3 Belief Propagation Algorithm for Detection in the Presence of Cues 4\n4 Asymptotic Error Analysis 7\n5 Detection Method 14\n6 Numerical Experiments 15\n7 Conclusions and Future Extensions 15\nRR n° XXX\nRESEARCH CENTRE SOPHIA ANTIPOLIS – MÉDITERRANÉE\n2004 route des Lucioles - BP 93 06902 Sophia Antipolis Cedex\nPublisher Inria Domaine de Voluceau - Rocquencourt BP 105 - 78153 Le Chesnay Cedex inria.fr\nISSN 0249-6399\nThis figure \"logo-inria.jpg\" is available in \"jpg\" format from:\nhttp://arxiv.org/ps/1611.04847v1\nThis figure \"logo-inria.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1611.04847v1\nThis figure \"pagei.jpg\" is available in \"jpg\" format from:\nhttp://arxiv.org/ps/1611.04847v1\nThis figure \"pagei.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1611.04847v1\nThis figure \"rrpage1.jpg\" is available in \"jpg\" format from:\nhttp://arxiv.org/ps/1611.04847v1\nThis figure \"rrpage1.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1611.04847v1"
    } ],
    "references" : [ {
      "title" : "Graph based anomaly detection and description: a survey",
      "author" : [ "L. Akoglu", "H. Tong", "D. Koutra" ],
      "venue" : "Data Mining and Knowledge Discovery, vol. 29, no. 3, pp. 626–688, 2015.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Finding a large hidden clique in a random graph",
      "author" : [ "N. Alon", "M. Krivelevich", "B. Sudakov" ],
      "venue" : "Random Structures and Algorithms, vol. 13, no. 3-4, pp. 457–466, 1998.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Probability and measure",
      "author" : [ "P. Billingsley" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Inference via message passing on partially labeled stochastic block models",
      "author" : [ "T.T. Cai", "T. Liang", "A. Rakhlin" ],
      "venue" : "arXiv preprint arXiv:1603.06923, 2016.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Finding hidden cliques of size  √ N/e in nearly linear time",
      "author" : [ "Y. Deshpande", "A. Montanari" ],
      "venue" : "Foundations of Computational Mathematics, vol. 15, no. 4, pp. 1069–1128, 2015.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Information limits for recovering a hidden community",
      "author" : [ "B. Hajek", "Y. Wu", "J. Xu" ],
      "venue" : "arXiv preprint arXiv:1509.07859, 2015.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Recovering a Hidden Community Beyond the Spectral Limit in O(|E|log∗|V |) Time",
      "author" : [ "——" ],
      "venue" : "arXiv Prepr. arXiv1510.02786, 2015.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Characterization of L-norm statistic for Anomaly Detection in Erdös Rényi Graphs",
      "author" : [ "A. Kadavankandy", "L. Cottatellucci", "K. Avrachenkov" ],
      "venue" : "CDC. IEEE, 2016. [Online]. Available: http://www.eurecom.fr/en/publication/list/author/cottatellucci-laura",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Lovasz θ function , SVMs and Finding Dense Subgraphs",
      "author" : [ "A. Martinsson" ],
      "venue" : "J. Mach. Learn. Res., vol. 14, pp. 3495–3536, 2013.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A random graph model for terrorist transactions",
      "author" : [ "T. Mifflin", "C. Boner", "G. Godfrey", "J. Skokan" ],
      "venue" : "2004 IEEE Aerosp. Conf. Proc. (IEEE Cat. No.04TH8720), vol. 5. IEEE, 2004, pp. 3258–3264.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Finding one community in a sparse graph",
      "author" : [ "A. Montanari" ],
      "venue" : "Journal of Statistical Physics, vol. 161, no. 2, pp. 273–299, 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Please see [1] for a survey.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 9,
      "context" : "The hidden subgraph model with both the subgraph and the background modelled as ER graphs with different edge densities was proposed in [10] to study anomalous transactions in a computer network.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 9,
      "context" : "This graph, denoted G(K,n, p, q), can model a network with a hidden community [10].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "See [8,9] for works on detecting the presence of such a subgraph in a given graph and detecting the subgraph nodes.",
      "startOffset" : 4,
      "endOffset" : 9
    }, {
      "referenceID" : 8,
      "context" : "See [8,9] for works on detecting the presence of such a subgraph in a given graph and detecting the subgraph nodes.",
      "startOffset" : 4,
      "endOffset" : 9
    }, {
      "referenceID" : 1,
      "context" : "A stream of recent works in this area suggests that there is a subgraph size below which detection is impossible by means of nearly linear-time algorithms, and above which efficient algorithms have been identified ( [2,7,9,11]).",
      "startOffset" : 216,
      "endOffset" : 226
    }, {
      "referenceID" : 6,
      "context" : "A stream of recent works in this area suggests that there is a subgraph size below which detection is impossible by means of nearly linear-time algorithms, and above which efficient algorithms have been identified ( [2,7,9,11]).",
      "startOffset" : 216,
      "endOffset" : 226
    }, {
      "referenceID" : 8,
      "context" : "A stream of recent works in this area suggests that there is a subgraph size below which detection is impossible by means of nearly linear-time algorithms, and above which efficient algorithms have been identified ( [2,7,9,11]).",
      "startOffset" : 216,
      "endOffset" : 226
    }, {
      "referenceID" : 10,
      "context" : "A stream of recent works in this area suggests that there is a subgraph size below which detection is impossible by means of nearly linear-time algorithms, and above which efficient algorithms have been identified ( [2,7,9,11]).",
      "startOffset" : 216,
      "endOffset" : 226
    }, {
      "referenceID" : 1,
      "context" : "When q = 1/2 and p = 1, the subgraph detection problem reduces to the well-known clique detection problem ( [2], [9], [5]).",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 8,
      "context" : "When q = 1/2 and p = 1, the subgraph detection problem reduces to the well-known clique detection problem ( [2], [9], [5]).",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "When q = 1/2 and p = 1, the subgraph detection problem reduces to the well-known clique detection problem ( [2], [9], [5]).",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "We use the framework of analysis developed in [7] and [11].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "We use the framework of analysis developed in [7] and [11].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 10,
      "context" : "In [11] the author considers the problem of detecting the hidden community in G(K,n, p, q) under the assumption that p = a/n, q = b/n and K = κ/n, where a, b, κ are constants.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "In [7], the authors consider a more general setting of sparse graphs and prove that BP succeeds when λ > 1/e.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "In [6] global ML detection is shown to achieve zero error rate asymptotically if λ = Θ((K/n) log(K/n)).",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "The optimality of BP with side information is shown in [12] and [4] for community detection on SBM with symmetric communities, but to the best of our knowledge no theoretical studies of BP have been made when cued vertices are available for detecting a small subgraph in G(K,n, p, q).",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "The symbol D −→ denotes convergence in distribution [3].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "Recall that the optimum detector that minimises the expected number of misclassified nodes is the ML detector [6] given as: σ̂i = 1{Ri>log(n−K/K(1−α))}, where Ri = log P(G,C|σi = 1) P(G,C|σi = 0) .",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "Let T t u be a labelled Galton-Watson (G-W) tree of depth t rooted at node u constructed as follows (as in [7]): The label τu at node u is chosen at random in the following way: P{τu = 1} = K n P{τu = 0} = n−K n .",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "By the following coupling lemma established in [7], the detection of label σu based on G t u is statistically identical to the detection of τu based on T t u :",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "Lemma 1 [7] For t such that (np) = n, there exists a coupling such that (Gu, σ ) = (T t u, τ ) with probability 1− n−1+o(1).",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 10,
      "context" : "We define the parameter λ, interpreted as an effective SNR [11] of the detection problem, as",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "Proof: Following the logic in [11], we show this result for g(Λ̃u) = 1{Λ̃u∈A}, A being some measurable set .",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "The result for general g then follows because any integrable function can be obtained as the limit of a sequence of such rvs [3].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "Remark : When α = 0 (13) reduces to the recursion given in [11] as expected.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "Following [11], we prove the result by induction on t.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 2,
      "context" : "Then by using dominated convergence theorem [3] and Lemma 2 we obtain",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "5 Detection Method It is shown [7] that asymptotically the tests",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "This implies that BP with cue beats BP without cues, which requires λ > 1/e for zero asymptotic error rate ( [7, 11]).",
      "startOffset" : 109,
      "endOffset" : 116
    }, {
      "referenceID" : 10,
      "context" : "This implies that BP with cue beats BP without cues, which requires λ > 1/e for zero asymptotic error rate ( [7, 11]).",
      "startOffset" : 109,
      "endOffset" : 116
    } ],
    "year" : 2017,
    "abstractText" : "We consider an Erdős-Rényi graph with n nodes and edge probability q that is embedded with a random subgraph of size K with edge probabilities p such that p > q. We address the problem of detecting the subgraph nodes when only the graph edges are observed, along with some extra knowledge of a small fraction of subgraph nodes, called cued vertices or cues. We employ a local and distributed algorithm called belief propagation (BP). Recent works on subgraph detection without cues have shown that global maximum likelihood (ML) detection strictly outperforms BP in terms of asymptotic error rate, namely, there is a threshold condition that the subgraph parameters should satisfy below which BP fails in achieving asymptotically zero error, but ML succeeds. In contrast, we show that when the fraction of cues is strictly bounded away from zero, i.e., when there exists non-trivial side-information, BP achieves zero asymptotic error even below this threshold, thus approaching the performance of ML detection. Key-words: Belief Propagation, Subgraph Detection, Semisupervised Learning, Random Graphs ∗ Corresponding author, arun.kadavankandy@inria.fr La Detection de Sousgraphes en presence des indices grâce au Belief Propagation Résumé : Nous considérons un graphe Erdős-Rényi qui a n sommets dont q est la probabilité d’arrêtes. La dessus il y un sousgraphe placé sur leurs m sommets selectionnés aléatoirement et leur probabilité d’arrêtes est p, en sorte que p > q. Nous proposons un algorithme distribué aux calculs locales à chaque sommet, tiré du “Belief Propagation” (BP), qui détecte les sommets du sousgraphe, quand on connait une fraction de sommets du sousgraphe en tant qu’indices. Des recherches récentes ont prouvé que la prestation du BP dans l’absence des indices est strictement inférior par rapport à la detection globale du maximum de vraisemblance (DMV). A l’opposé, ici on prouve qu’en presence des indices, la prestation du BP est à l’hauteur de celle de DMV, dans la sens où le premier reussie à detecter la sousgraphe avec une erreur qui tend a zéro, à chaque fois le dernier peut le faire, dans la limite où le nombre de sommets du graph tend l’infinité. Mots-clés : Belief Propagation, Detection de Sousgraphes, Semisupervised Learning, Graphes Aléatoires Subgraph Detection with cues using Belief Propagation 3",
    "creator" : "LaTeX with hyperref package"
  }
}