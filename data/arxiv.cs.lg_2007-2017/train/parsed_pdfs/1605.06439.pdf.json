{
  "name" : "1605.06439.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning",
    "authors" : [ "Wouter M. Koolen" ],
    "emails" : [ "wmkoolen@cwi.nl", "pdg@cwi.nl", "tim@timvanerven.nl" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 5.\n06 43\n9v 1"
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider online sequential decision problems. We focus on full information settings, encompassing such interaction protocols as online prediction, classification and regression, prediction with expert advice or the Hedge setting, and online convex optimization (see Cesa-Bianchi and Lugosi 2006). The goal of the learner is to choose a sequence of actions with small regret, i.e. such that his cumulative loss is not much larger than the loss of the best fixed action in hindsight. This has to hold even in the worst case, where the environment is controlled by an adversary. After three decades of research there exist many algorithms and analysis techniques for a variety of such settings. For many\nsettings, adversarial regret lower bounds of order √ T are known, along with matching individual sequence algorithms [Shalev-Shwartz, 2011]. A more recent line of development is to design adaptive algorithms with regret guarantees that scale with some more refined measure of the complexity of the problem. For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al. [2014], Koolen et al. [2014], Koolen and Van Erven [2015], Luo and Schapire [2015], Wintenberger [2015]. Interestingly, the price for such adaptivity (i.e. the worsening of the worst-case regret bound) is typically extremely small (i.e. a constant factor in the regret bound). For online convex optimization (OCO), many different types of adaptivity have been explored, including by [Crammer et al., 2009, Duchi et al., 2011, McMahan and Streeter, 2010, Hazan and Kale, 2010, Chiang et al., 2012, Steinhardt and Liang, 2014, Koolen and Van Erven, 2016].\nHere we are interested in the question of whether such adaptive results are strong enough to lead to improved rates in the stochastic case when the data follow a “friendly” distribution. In specific cases it has been shown that fancy guarantees do imply significantly reduced regret. For example Gaillard et al. [2014] present a generic argument showing that a certain kind of second-order regret guarantees implies constant expected regret (the fastest possible rate) for i.i.d. losses drawn from a distribution with a gap (between expected loss of the best and all other actions). In this paper we significantly extend this result. We show that a variety of individual-sequence second-order regret guarantees imply fast regret rates for distributions under much milder stochastic assumptions. In particular, we will look at the Bernstein condition (see Bartlett and Mendelson 2006), which is the key to fast rates in the batch setting. This condition provides a parametrised interpolation (expressed in terms of the Bernstein exponent κ ∈ [0,1]) between the friendly gap case (κ = 1) and the stochastic worst case(κ = 0). We show that appropriate second-order guarantees automatically lead to adaptation to these parameters, for both the Hedge setting and for OCO. In the Hedge setting, we build on the guarantees available for the Squint algorithm [Koolen and Van Erven, 2015] and for OCO we rely on guarantees achieved by MetaGrad [Koolen and Van Erven, 2016] to obtain regret rates of order T 1−κ 2−κ (Theorem 3). We show this, not just in expectation, but also with high probability. Our proofs use that, for bounded losses, the Bernstein condition is equivalent to the so-called Central condition [Van Erven et al., 2015], which provides control over a martingale-type quantity that captures the second-order part of the bounds (Lemma 8). The rates we obtain include the slow worst-case √ T regime for κ = 0 and the fastest (doubly) logarithmic regime for κ = 1. The next section introduces the two settings we consider and the individual sequence guarantees we will use in each. It also reviews the stochastic criteria for fast rates and presents our main result. In Section 3 we consider a variety of examples illustrating the breadth of cases that we cover. In Section 4 we prove that second-order guarantees imply adaptation to Bernstein conditions."
    }, {
      "heading" : "2 Setup",
      "text" : ""
    }, {
      "heading" : "2.1 Hedge Setting",
      "text" : "We start with arguably the simplest setting of online prediction, the Hedge setting popularized by Freund and Schapire [1997]. To be able to illustrate the full reach of our stochastic assumption we will use a minor extension to countably infinitely many actions k ∈ N = {1,2, . . .}, customarily called experts. The protocol is as follows. Each round t the learner plays a probability mass function wt = (w1t ,w2t , . . .) on experts. Then the environment reveals the losses ℓt = (ℓ1t , ℓ2t , . . .) of the experts, where each ℓkt ∈ [0,1]. The learner incurs loss⟨wt, ℓt⟩ =∑k wkt ℓkt . The regret after T rounds compared to expert k is given by\nRkT ∶= T∑ t=1 (⟨wt, ℓt⟩ − ℓkt ) . The goal of the learner is to keep the regret small compared to any expert k. We will make use of Squint by Koolen and Van Erven [2015], a self-tuning algorithm for playing wt. Koolen and Van Erven [2015, Theorem 4] show that Squint with prior probability mass function π = (π1, π2, . . .) guarantees RkT ≤ √ V kT K k T +KkT where KkT = O(− lnπk + ln lnT ) for any expert k.\n(1)\nHere V kT ∶= ∑Tt=1 (⟨wt, ℓt⟩ − ℓkt )2 is a second-order term that depends on the algorithm’s own predictions wt. It is well-known that with K experts the worstcase lower bound is Θ(√T lnK) [Cesa-Bianchi and Lugosi, 2006, Theorem 3.7]. Taking a fat-tailed prior, for example πk = 1\nk(k+1) , and using V k T ≤ T , the above bound implies RkT ≤ O (√T (lnk + ln lnT )), matching the lower bound in some sense for all k simultaneously.\nThe question we study in this paper is what becomes of the regret when the sequence of losses ℓ1, ℓ2, . . . is drawn from some distribution P, not necessarily i.i.d. But before we expand on such stochastic cases, let us first introduce another setting."
    }, {
      "heading" : "2.2 Online Convex Optimization (OCO)",
      "text" : "We now turn to our second setting called online convex optimization [Shalev-Shwartz, 2011]. Here the set of actions is a compact convex set U ⊆ Rd. Each round t the learner plays a point wt ∈ U . Then the environment reveals a convex loss function ℓt ∶ U → R. The loss of the learner is ℓt(wt). The regret after T rounds compared to u ∈ U is given by\nRuT ∶= T∑ t=1 (ℓt(wt) − ℓt(u)) . The goal is small regret compared to any point u ∈ U . A common tool in the analysis of algorithms is the linear upper bound on the regret obtained from\nconvexity of ℓt (at non-differentiable points we may take any sub-gradient)\nRuT ≤ R̃ u T ∶= T∑ t=1 ⟨wt − u,∇ℓt(wt)⟩.\nWewill make use of (the full matrix version of)MetaGrad by Koolen and Van Erven [2016]. In their Theorem 8, they show that, simultaneously, R̃uT ≤ O (DG√T) and\nR̃uT ≤ √ V uT KT +DGKT where KT = O(d lnT ) for any u ∈ U , (2) where D bounds the two-norm diameter of U , G bounds ∥∇ℓt(wt)∥2 the twonorm of the gradients and V uT ∶= ∑Tt=1⟨wt−u,∇ℓt(wt)⟩2. The first bound matches the worst-case lower bound. The second bound (2) may be a factor √ KT worse, as V uT ≤ G 2D2T by Cauchy-Schwarz. Yet in this paper we will show fast rates in certain stochastic settings arising from (2). To simplify notation we will assume from now on that DG = 1 (this can always be achieved by scaling the loss).\nTo talk about stochastic settings we will assume that the sequence ℓt of loss functions (and hence the gradients ∇ℓt(wt)) are drawn from a distribution P, not necessarily i.i.d. This includes the common case of linear regression and classification where ℓt(u) = loss(⟨u,xt⟩, yt) with (xt, yt) sampled i.i.d. from some distribution and loss a fixed one-dimensional convex loss function (e.g. square loss, absolute loss, log loss, hinge loss, . . . )."
    }, {
      "heading" : "2.3 Parameterized Family of Stochastic Assumptions",
      "text" : "We now recall the Bernstein [Bartlett and Mendelson, 2006] and Central [Van Erven et al., 2015] stochastic conditions. In both cases the idea behind the assumption is to control the variance of the excess loss of the actions in the neighborhood of the best action.\nWe do not require that the losses are i.i.d., nor that the Bayes act is in the model. For the Hedge setting it suffices if there is a fixed expert k∗ that is always best, i.e. E [ℓk∗t ∣Gt−1] = infk E [ℓkt ∣Gt−1] almost surely for all t. (Here we denote by Gt−1 the sigma algebra generated by ℓ1, . . . , ℓt−1, and the almost surely quantification refers to the distribution of ℓ1, . . . , ℓt−1.) Similarly, for OCO we assume there is a fixed point u∗ ∈ U attaining minu∈U E [ℓt(u)∣Gt−1] at every round t. In either case there may be multiple candidate k∗ or u∗. In the succeeding we assume that one is selected. Note that for i.i.d. losses the existence of a minimiser is not such a strong assumption (it is even automatic in the OCO case due to compactness of U), while it is very strong beyond i.i.d. Yet it is not impossible (and actually interesting) as we will show by example in Section 3.\nBased on this loss minimiser, we define the excess losses, a family of random variables indexed by time t ∈ N and expert/point k ∈ N/u ∈ U as follows\nxkt ∶= ℓkt − ℓk∗t (Hedge) and xut ∶= ⟨u − u∗,∇ℓt(u)⟩ (OCO). (3)\nNote that for the Hedge setting we work with the loss directly. For OCO instead we talk about the linear upper bound on the loss, for this is the quantity that needs to be controlled to make use of the MetaGrad bound (2). With these variables in place, from this point on the story is the same for Hedge and for OCO. So let us write F for either the set N of experts or the set U of points, and f∗ for k∗ resp. u∗, and let us consider the family {xft ∣ f ∈ F , t ∈ N}. We call f ∈ F predictors. The point of these stochastic conditions is that they imply that the variance in the excess loss gets smaller the closer a predictor gets to the optimum in terms of expected excess loss. This is most directly seen in the Bernstein condition: Condition 1. Fix B ≥ 0 and κ ∈ [0,1]. The family (3) satisfies the (B,κ)Bernstein condition if E [(xft )2∣Gt−1] ≤ B E [xft ∣Gt−1]κ almost surely for all f ∈ F and rounds t ∈ N. Some authors refer to the κ = 1 case as the Massart condition. As shown by Van Erven et al. [2015, Theorem 5.4], for bounded excess losses (which we assume throughout), the Bernstein condition is equivalent to the following condition: Condition 2. Fix a function ǫ ∶ R+ → R+. The family (3) satisfies the ǫ-central condition if\n1 η lnE [e−ηxft ∣Gt−1] ≤ ǫ(η) almost surely for all f ∈ F , η ≥ 0 and t ∈ N.\nVan Erven et al. explicitly convert back and forth between the parameters of the Bernstein and Central Condition. In the remainder we will use that Bernstein implies Central (with ǫ(η) = O((Bη) 11−κ )). For completeness we include a proof in Appendix B."
    }, {
      "heading" : "2.4 Main Result",
      "text" : "In the stochastic case we evaluate the performance of algorithms by Rf ∗\nT , i.e. the regret compared to the predictor f∗ with minimal expected loss. The expectation E[Rf∗T ] is sometimes called the pseudo-regret. The following result shows that second-order methods automatically adapt to the Bernstein condition. (Proof sketch in Section 4.) Theorem 3. In any stochastic setting satisfying the (B,κ)-Bernstein Condition 1, the guarantees (1) for Squint and (2) for MetaGrad imply fast rates for the respective algorithms both in expectation and with high probability. That is,\nE[Rf∗T ] = O (K 12−κT T 1−κ2−κ ) , and for any δ > 0, with probability at least 1 − δ,\nR f∗ T = O ((KT − ln δ) 12−κT 1−κ2−κ ) ,\nwhere for Squint KT ∶=Kf∗T from (1) and for MetaGrad KT is as in (2). We see that Squint and MetaGrad (and any other second-order methods achieving the same bounds, as our results only use these bounds and do not depend on the details of the algorithms) adapt automatically to the Bernstein parameters of the distribution, without any tuning. Appendix F provides an extension of Theorem 3 that allows using Squint with uncountable F .\nCrucially, the bound provided by Theorem 3 is natural, and, in general, the best one can expect. This can be seen from considering the statistical learning setting, which is a special case of our setup. Here (xt, yt) are i.i.d. ∼ P and F is a set of functions from X to a set of predictions A, with ℓft ∶= ℓ(yt, f(xt)) for some loss function ℓ ∶ Y × A → [0,1] such as squared, 0/1, or absolute loss. In this setting one usually considers excess risk, which is the expected loss difference between the learned f̂ and the optimal f∗. The minimax expected (over training sample (xt, yt)) risk relative to f∗ is of order T −1/2 (see e.g. Massart and Nédélec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent κ > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions.\nIf F is sufficiently ‘simple’, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a κ-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) ⋅ T − 12−κ ). The bound interpolates between T −1/2 for κ = 0 and T −1 for κ = 1 (Massart condition). Results of Tsybakov [2004], Massart and Nédélec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.g. Tsybakov [2004], Audibert [2004, 2009], Bartlett et al. [2006]. By summing from t = 1 to T and using ERM at each t to classify the next data point (so that ERM becomes FTL, follow-theleader), this suggests that we can achieve a cumulative expected regret E[Rf∗T ] of order O ((logT ) ⋅ T 1−κ2−κ ). Theorem 3 shows that this is, indeed, also the rate that Squint attains in such cases if F is countable and the optimal f∗ has positive prior mass πf ∗\n(more on this condition below)— we thus see that Squint obtains exactly the rates one would expect from a statistical learning/classification perspective, and the minimax excess risk results in that setting suggests that these cumulative regret rates cannot be improved in general. It was shown earlier by Audibert [2004] that, when equipped with an oracle to tune the learning rate η as a function of t, the rates O ((logT ) ⋅ T 1−κ2−κ ) can also be achieved by Hedge, but the exact tuning depends on the unknown κ. Grünwald [2012] provides a means to tune η automatically in terms of the data, but his method — like ERM and all algorithms in the references above — may achieve linear regret in worst-case settings, whereas Squint keeps the O(√T ) guarantee for such cases.\nTheorem 3 only gives the desired rate for Squint if F is countable and πf∗ > 0. The combination of these two assumptions is strong or at least unnatural, and OCO cannot be readily used in all such cases either, so in Appendix F we therefore show how to extend Theorem 3 to the case of infinite F , which can be continuous and thus have πf ∗\n= 0, as long as F admits sufficiently small entropy numbers. Incidentally, this also allows us to show that Squint achieves regret rate O ((logT ) ⋅ T 1−κ2−κ ) when F = ⋃i=1,2,...Fi is a countably infinite union of Fi with appropriate entropy numbers; in such cases there can be, at every sample size, a classifier f̂ ∈ F with 0 empirical error, so that ERM/FTL will always overfit and cannot be used even if the Bernstein condition holds; Squint allows for aggregation of such models. In the remainder of the main text, we concentrate on applications for which Theorem 3 can be used directly, without extensions."
    }, {
      "heading" : "3 Examples",
      "text" : "We give examples motivating and illustrating the Bernstein/Central condition for the Hedge and OCO settings. Our examples in the Hedge setting will illustrate Bernstein with κ < 1 and non i.i.d. distributions. Our OCO examples were chosen to be natural and illustrate fast rates without curvature."
    }, {
      "heading" : "3.1 Hedge Setting: Gap implies Bernstein with κ = 1",
      "text" : "In the Hedge setting, we say that a distribution P (not necessarily i.i.d.) of expert losses {ℓkt ∣ t, k ∈ N} has gap α > 0 if there is an expert k∗ such that\nE [ℓk∗t ∣Gt−1] + α ≤ inf k≠k∗ E [ℓkt ∣Gt−1] almost surely for each round ∈ N. It is clear that the condition can only hold for k∗ the minimiser of the expected loss. Lemma 4. A distribution with gap α is ( 1 α ,1)-Bernstein. Proof. For all k, t we have E [(xkt )2∣Gt−1] ≤ 1 = 1αα ≤ 1α E [xkt ∣Gt−1] . By Theorem 3 we get the Rk ∗\nT = O(KT ) = O(ln lnT ) rate. Gaillard et al. [2014] show constant regret for finitely many experts and i.i.d. losses with a gap. Our alternative proof above shows that neither finiteness nor i.i.d. are essential for fast rates in this case.\n3.2 Hedge Setting: Any (1, κ)-Bernstein\nThe next example illustrates that we can sometimes get the fast rates without a gap. And it also shows that we can get any intermediate rate: we construct an example satisfying the Bernstein condition for any κ ∈ [0,1] of our choosing\n(such examples occur naturally in classiciation settings such as those consider in the example in Appendix F).\nFix κ ∈ [0,1]. Each expert k is parametrised by a real number δk ∈ [0,1/2]. The only assumption we make is that δk = 0 for some k, and infk{δk ∣ δk > 0} = 0. For a concrete example let us choose δ1 = 0 and δk = 1/k. Expert δ has loss 1/2±δ with probability 1±δ 2/κ−1\n2 independently between experts and rounds. Expert δ\nhas mean loss 1 2 + δ2/κ, and so δ = 0 is best, with loss deterministically equal\nto 1/2. The squared excess loss of δ is δ2. So we have the Bernstein condition with exponent κ (but no κ′ > κ) and constant 1, and the associated regret rate by Theorem 3.\nNote that for κ = 0 (the hard case) all experts have mean loss equal to 1 2 . So\nno matter which k∗ we designate as the best expert our expected regret is zero. Yet the experts do not agree, as their losses deviate from 1\n2 independently at\nrandom. Hence, by the central limit theorem, with high probability our regret is of order √ T . On the other side of the spectrum, for κ = 1 (the best case), we do not find a gap. We still have experts arbitrary close to the best expert in mean, but their expected excess loss squared equals their expected excess loss.\nERM/FTL may fail miserably on this type of examples. The clearest case is when {k ∣ δk > ǫ} is infinite for some ǫ > 0. Then at any t there will be experts that, by chance, incurred their lower loss every round. Picking any of them will result in expected instantaneous regret at least ǫ2/κ, leading to linear regret overall.\nThe requirement δk = 0 for some k is essential. If instead δk > 0 for all k then there is no best expert in the class. Theorem 13 in Appendix F shows how to deal with this case."
    }, {
      "heading" : "3.3 Hedge Setting: Markov Chains",
      "text" : "Suppose we model a binary sequence z1, z2, . . . , zT with m-th order Markov chains. As experts we consider all possible functions f ∶{0,1}m → {0,1} that map a history of length m to a prediction for the next outcome, and the loss of expert f is the 0/1-loss: ℓft = ∣f(zt−m, . . . , zt−1) − zt∣. (We initialize z1−m = . . . = z0 = 0.) A uniform prior on this finite set of 22 m\nexperts results in worst-case regret of order √ T 2m. Then, if the data are actually generated by an m-th order Markov chain with transition probabilities P(zt = 1 ∣ (zt−m, . . . , zt−1) = a) = pa, we have f∗(a) = 1{pa ≥ 12} and\nE [(xft )2∣(zt−m, . . . , zt−1) = a] = 1, E [xft ∣(zt−m, . . . , zt−1) = a] = 2∣pa − 12 ∣ for any f such that f(a) ≠ f∗(a). So the Bernstein condition holds with κ = 1 and B = 1\n2mina ∣pa−1 2 ∣ ."
    }, {
      "heading" : "3.4 OCO: Hinge Loss on the Unit Ball",
      "text" : "Let (x1, y1), (x2, y2), . . . be classification data, with yt ∈ {−1,+1}, and consider the hinge loss ℓt(u) = max{0,1 − yt⟨xt, u⟩}. Now suppose, for simplicity, that\nboth xt and u come from the d-dimensional unit Euclidean ball, such that⟨xt, u⟩ ∈ [−1,+1] and the hinge is never active, i.e. ℓt(u) = 1 − yt⟨xt, u⟩. Then, if the data turn out to be i.i.d. observations from a fixed distribution P, the Bernstein condition holds with κ = 1 (The proof can be found in Appendix D):\nLemma 5 (Unregularized Hinge Loss Example). Consider the hinge loss setting above, where ∣⟨xt, u⟩∣ ≤ 1. If the data are i.i.d., then the (B,κ)-Bernstein condition is satisfied with κ = 1 and B = 2λmax∥µ∥ , where λmax is the maximum\neigenvalue of E [XX⊺] and µ = E[YX], provided that ∥µ∥ > 0. In particular, if Xt is uniformly distributed on the sphere and Yt = sign(⟨ū,Xt⟩) is the noiseless classification of Xt according to the hyperplane with normal vector ū, then B ≤ c√\nd for some absolute constant c > 0.\nThe excluded case ∥µ∥ = 0 only happens in the degenerate case that there is nothing to learn, because µ = 0 implies that the expected hinge loss is 1, its maximal value, for all u."
    }, {
      "heading" : "3.5 OCO: Absolute Loss",
      "text" : "Let U = [0,1] be the unit interval. Consider ℓt(u) = ∣u− xt∣ where xt ∈ [0,1] are drawn i.i.d. from P. Let u∗ ∈ argminuE∣u − x∣ minimize the expected loss. In this case we may simplify ⟨w −u∗,∇ℓ(w)⟩ = (w −u∗) sign(w −x). To satisfy the Bernstein condition, we therefore want B such that, for all w ∈ [0,1],\nE [((w − u∗) sign(w − x))2] ≤ BE [(w − u∗) sign(w − x)]κ . That is, ∣w − u∗∣2−κ ≤ B2κ∣P(x ≤ w) − 1 2 ∣κ. For instance, if the distribution of x has a strictly positive density p(x) ≥m > 0, then u∗ is the median and ∣P(x ≤ w) − 1 2 ∣ = ∣P(x ≤ w) − P(x ≤ u∗)∣ ≥ m∣w − u∗∣, so the condition holds with κ = 1 and B = 1 2m\n. Alternatively, for a discrete distribution on two points a and b with probabilities p and 1 − p, the condition holds with κ = 1 and B = 1∣2p−1∣ , provided that p ≠ 1\n2 , as can be seen by bounding∣w − u∗∣ ≤ 1 and ∣P(x ≤ w) − 1 2 ∣ ≥ ∣p − 1 2 ∣."
    }, {
      "heading" : "4 Proof of Main Result",
      "text" : "This section builds up to prove our main result Theorem 3. We first introduce a handy abbreviation that allows us to reason simultaneously in expectation and with high probability. We then identify the minimal ǫ for which the Central Condition 2 holds. We then show how we can introduce a second-order adjustment, and characterize the cost. Combination with either worst-case regret bound then yields the desired result."
    }, {
      "heading" : "4.1 Notation: Exponential Stochastic Negativity and Inequality",
      "text" : "We introduce a convenient shorthand notation that we will use throughout this paper.\nDefinition 6. A random variable X is exponentially stochastically negative, denoted X ⊴ 0, if E[eX] ≤ 1. For any η ≥ 0, we write X ⊴η 0 if ηX ⊴ 0. For any pair of random variables X and Y , we say that X is exponentially stochastically less than Y , denoted X ⊴ Y , if X − Y ⊴ 0.\nLemma 7. Exponential stochastic negativity has the following useful properties:\n1. (Negativity). Let X ⊴ 0. As the notation suggests X is negative in expectation and with high probability. That is E [X] ≤ 0 and P{X ≥ − ln δ} ≤ δ for all δ > 0. 2. (Convex combination). Let {Xf} f∈F be a family of random variables and\nlet w be a distribution on F . If Xf ⊴ 0 for all f then Ef∼w[Xf ] ⊴ 0. 3. (Chain rule). Let X1,X2, . . . be adapted to filtration G1 ⊆ G2 . . . (i.e. Xt\nis Gt-measurable for each t). If Xt∣Gt−1 ⊴ 0 almost surely for all t, then∑Tt=1Xt ⊴ 0 for all T ≥ 0. Proof. Negativity: By Jensen’s inequality E [X] ≤ lnE [eX] ≤ 0, whereas by Markov’s inequality P{X ≥ − ln δ} = P{eX ≥ 1 δ } ≤ δE [eX] ≤ δ. Convex combination: By Jensen’s inequality E [eEf∼w[Xf ]] ≤ Ef∼w E [eXf ] ≤ 1. Chain rule: By induction. The base case T = 0 holds trivially. For T > 0 we have E [e∑Tt=1 Xt] = E [e∑T−1t=1 Xt E [eXT ∣GT−1]] ≤ E [e∑T−1t=1 Xt] ≤ 1."
    }, {
      "heading" : "4.2 Normalized Cumulant Generating Function",
      "text" : "To prove our main result we will make use of the Central Condition 2. For any distribution P this condition will hold for some ǫ (which may be trivial). In this section we construct the smallest ǫ for which it holds and derive a few useful properties of that ǫ.\nConsider the family (3) of excess loss variables xft . We assume that x f t ∈ [−1,1]\nis bounded in a range of width 2 and has positive mean E[xft ∣Gt−1] ≥ 0 by definition of f∗. As we will see, the complexity of our learning problem will be governed by the distribution of xft . In particular, we will look at the normalized cumulant generating function for η ≥ 0:\nǫ f t (η) ∶= 1η lnE [e−ηxft ∣Gt−1]\nBy construction −xft ⊴η ǫ f t (η). Boundedness of xft ∈ [−1,1] immediately results\nin ǫft (η) ∈ [−1,1]. Moreover, Hoeffding’s inequality (see e.g. Cesa-Bianchi and Lugosi\n[2006, Lemma 2.2]) tells us that ǫft (η) ≤ η/2 while Jensen’s inequality gives ǫ f t (η) ≥ −E [xft ∣Gt−1]. The dual representation ǫft (η) = supQ −EQ [x]− 1η KL(Q(x)∥P(xft ∣Gt−1)) reveals that ǫft (η) is increasing in η. The value at η = 0 is obtained by continuity from ǫft (0) ∶= limη→0 ǫft (η) = −E[xft ∣Gt−1] ≤ 0.\nTo get a uniform control over the class F , we will make use of the maximum ǫt(η) ∶= sup\nf∈F ǫ f t (η) and ǫ(η) ∶= sup t ǫt(η). (4)\nThe functions ǫt and ǫ inherit most properties of each ǫ f t , but in addition since f∗ ∈ F and ǫf∗t (η) = 0, we see that ǫt(η) ≥ 0 and also that ǫt(0) = 0. Moreover, since ǫt(η) ≤ η/2 we have limη→0 ǫt(η) = 0. In this paper we will judge the complexity of the interplay of the generating distribution P with the class F by how ǫt(η) → 0 as η → 0. By construction the Central Condition 2 holds with ǫ(η)."
    }, {
      "heading" : "4.3 A Second-order Adjustment to Exponential Stochastic Inequality",
      "text" : "We now show a technical lemma showing that, roughly, the square of a bounded Central random variable is exponentially stochastically less than that variable itself. Consider any random variable x ∈ [−1,1], and let us denote its normalized cumulant generating function by ǫ(η) = 1 η lnE [e−ηx]. (In particular, see\nDefinition 6, −x ⊴η ǫ(η) for all η ≥ 0.) Intuitively, small ǫ(η) ≪ η/2 is special, indicating that x cannot be often very negative. The following lemma shows that if x is special to degree ǫ(η), then the smaller quantity ≈ x − η\n2 x2 is also\nspecial at only mildly weaker degree ≈ ǫ(2η). Lemma 8. For any random variable x ∈ [−1,1] and any η ≥ 0\n1 η lnE [ecη2x2−ηx] ≤ ǫ(2η) + cηǫ(2η)2 where c = 1 1 + √ 1 + 4η2 .\nIn the notation of Section 4.1, the lemma reads\n−x ⊴η ǫ(η) for all η ≥ 0 implies cηx2−x ⊴η ǫ(2η)+cηǫ(2η)2 for all η ≥ 0. Proof. By Theorem 12 in Appendix A with γ = 2η and the largest admissible c for (5).\nNote that for η = 0 the lemma trivializes, telling us −E [x] ≤ ǫ(0) where we have in fact equality. Note also that the right-hand side is an increasing function in ǫ(2η) (the quadratic in ǫ(2η) has positive derivative for all ǫ(2η) ≥ − 1+ √\n4η2+1 2η < −1.)"
    }, {
      "heading" : "4.4 From Second-order Bound to Bound in Terms of Parameter of Distribution",
      "text" : "The next step toward fast rates is to obtain from a second-order bound, which involves the algorithm, another bound strictly in terms of the parameters of the distribution, which do not refer to the algorithm. The proof is in Appendix C.\nTheorem 9. Consider either Squint in the Hedge setting or MetaGrad for OCO. Let {xft ∣f ∈ F} be the associated the excess loss family from (3), and let ǫ(η) be, as in (4), the corresponding maximal normalized cumulant generating function. For the Hedge setting let KT ∶= K f∗\nT as in (1), for OCO let KT be as in (2). Then for each γ ≥ 0 with c as in Lemma 8,\nR f∗\nT ⊴γ\nKT cγ + T ǫ(2γ)(1+ cγ2) + 2KT .\nTo prove our main Theorem 3 we invoke the Bernstein Condition 1 to bound ǫ(2ǫ) as a polynomial in γ, and then tune γ to optimize the bound. The details of the proof can be found in Appendix E."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We show that it is possible for online learning methods to provide both the safety and robustness of a worst-case regret bound and be adaptive to favorable stochastic environments. We focus on Squint and MetaGrad, methods for online learning with individual sequence regret guarantees of a particular second order form. We show that such guarantees imply automatic adaptivity to the Bernstein parameters of stochastic environments, and result in the corresponding fast regret rates."
    }, {
      "heading" : "A Second-order Adjustment of Exponential Stochas-",
      "text" : "tic Inequality\nIn this section we prove a stronger form of Lemma 8. We would like to remark that our solution to this problem was inspired by the general moments problem studied by Mehta and Williamson [2014, Section 3], especially because this connection became invisible during the simplification of our proofs.\nWe will be thinking about two learning rates, 0 ≤ η ≤ γ. The larger one, γ, will be where we evaluate ǫ(γ). So γ controls the strength of the assumption. The smaller one, η, will be the learning rate at which we obtain the conclusion. The point is to get a large amount of quadratic x2 in the conclusion, as governed by the constant c. Obviously, the more greedy we are in η and γ, the smaller the c for which we can get any traction. This trade-off is captured by the following relationship between γ, η and c that we will make use of throughout this section.\n0 ≤ c ≤\n√ 2∣2η − γ∣ + γ2 + 1 − ∣2η − γ∣ − 1\n4η2 (5)\nPositivity of c is not that important, as the desired inequality is trivial for c ≤ 0. The following inequality is useful later.\nLemma 10. Let 0 ≤ η ≤ γ and c satisfy (5). Then\n1 ≥ 2cη\nProof. We need to show\n2η ≥ √ 2∣2η − γ∣ + γ2 + 1 − ∣2η − γ∣ − 1\nthat is (2η + ∣2η − γ∣ + 1)2 ≥ 2∣2η − γ∣ + γ2 + 1 Expanding the left-hand side square results in 4η2+4η∣2η−γ∣+4η+∣2η−γ∣2+2∣2η−γ∣+1 = 4η(2η−γ)+4η∣2η−γ∣+4η+γ2+2∣2η−γ∣+1 which definitely exceeds the right-hand side above.\nWe now put our assumption to use. In the following Lemma we show that it implies a not-in-expectation-but-with-a-correction-term version of the result we are after. Lemma 11. Fix 0 ≤ η ≤ γ and let c satisfy (5). Then for each x ∈ [−1,1] and ǫ ∈ [−1,1] we have\necη 2 x 2−ηx − e−γ(x+ǫ) − 1 γ η(1 + 2cηǫ)ecη2ǫ2+ηǫ ≤ ecη2ǫ2+ηǫ.\nProof. We will show that the left-hand side is maximized over x ∈ [−1,1] at x = −ǫ. First, its derivative equals\ne−γxη(h(−ǫ)− h(x)) where h(x) = (1 − 2cηx)ecη2x2+(γ−η)x. This indeed equals zero at x = −ǫ. To show that x = −ǫ is indeed a maximum and that there are no other maxima it suffices to show that h(x) is increasing on x ∈ [−1,1]. We have\nh′(x) = (−4c2η3x2 + 2cηx(2η − γ) − 2cη + γ − η) ecη2x2+(γ−η)x As the term in parentheses is concave in x, it suffices to show that h′(x) ≥ 0 for x ∈ {−1,1}, i.e. −4c2η3 − 2cη∣2η − γ∣ − 2cη + γ − η ≥ 0 Solving the quadratic in c, we see that this holds if (5), as required.\nFinally, we are ready for the general version of the claim.\nTheorem 12. Pick 0 ≤ η ≤ γ and c satisfying (5). Let ǫ ∈ [−1,1]. Then for any x ∈ [−1,1] with E e−γx ≤ eγǫ we have\nE ecη 2x2−ηx ≤ ecη 2ǫ2+ηǫ.\nProof. Taking expectation over Lemma 11, we find\nE ecη 2x2−ηx ≤ ecη 2ǫ2+ηǫ + E e−γ(x+ǫ) − 1 γ η(1 + 2cηǫ)ecη2ǫ2+ηǫ,\nand the claim follows by bounding the right-most term by 0. (Note that the factor 1 + 2cηǫ is positive by Lemma 10.)"
    }, {
      "heading" : "B Bernstein to Central",
      "text" : "An indexed family of random variables {xf ∣ f ∈ F} satisfies the (B,κ)-Bernstein condition if E [(xf)2] ≤ BE [xf ]κ for all f ∈ F and it satisfies the ǫ-Central condition if −xf ⊴η ǫ(η) for all f ∈ F and η ≥ 0 We now show that Bernstein implies Central. This is a special case of [Van Erven et al., 2015, Theorem 5.4, Part 1]. Assume Bernstein. Then by the Bernstein Sandwich [Koolen et al., 2014, Lemma C.2], simplifying eη − η − 1 ≤ η2, which holds for small enough η ≤ 1.79328, and by the Bernstein assumption\nǫf(η) = 1 η lnE [e−ηxf ] ≤ ηE[(xf)2] −E[xf ] ≤ ηB E[xf ]κ −E[xf ]\nThen (the maximizer is found at x = (Bηκ) 11−κ (which is ≤ 1 when Bκη ≤ 1, so for small enough η this is a reasonable point))\nǫ(η) = sup f ǫf(η) ≤ sup x ηBxκ − x = 1 − κ κ (Bηκ) 11−κ ."
    }, {
      "heading" : "C Proof of Theorem 9",
      "text" : "Proof. For the Hedge setting, let us write xt ∶= Ek∼wt[xkt ] for the excess loss of the learner in round t. Then xt ∈ [−1,1] and −xt ⊴η ǫ(η) by Lemma 7. Now by definition of xkt in (3) and R k T and V k T (see Section 2.1)\nT∑ t=1 xt = T∑ t=1 (⟨wt, ℓt⟩ − ℓk∗t ) = Rk∗T and T∑ t=1 (xt)2 = T∑ t=1\n(⟨wt, ℓt⟩ − ℓk∗t )2 = V k∗T . For OCO, let us write xt ∶= x wt t for the excess loss of the learner in round t.\nAgain xt ∈ [−1,1] and we have −xt ⊴η ǫ(η) by construction of ǫ(η). Moreover, from the definition of R̃uT and V u T from Section 2.2,\nT∑ t=1 xt = T∑ t=1 ⟨wt − u∗,∇ℓt(wt)⟩ = R̃u∗T and T∑ t=1 (xt)2 = T∑ t=1\n⟨wt − u∗,∇ℓt(wt)⟩2 = V u∗T . With this notation the second-order regret bounds (1) and (2) both state\nT∑ t=1 xt ≤ ¿ÁÁÀ( T∑ t=1\nx2t)KT +KT . (6) Now fix γ ≥ 0. For any t, as −xt ⊴η ǫ(η), Lemma 8 gives cγx2t − xt ⊴γ ǫ(2γ)(1+ cγ2) By telescoping over rounds (using the chain rule Lemma 7), we obtain\ncγ T∑ t=1 x2t − T∑ t=1 xt ⊴γ T ǫ(2γ)(1+ cγ2). (7) The individual sequence regret bound (6) gives us (since 2 √ ab = infη ηa + b/η) for every η ≥ 0 T∑ t=1 xt ≤ η 2 T∑ t=1 x2t + KT 2η +KT .\nPlugging in η = cγ (this implies η ∈ [0,1/2] and γ = 2η 1−4η2 ) and combination with (7) results in T∑ t=1 xt ⊴γ KT cγ + T ǫ(2γ)(1+ cγ2) + 2KT .\nFor the Hedge setting this proves the theorem. For OCO we finish with Ru ∗\nT ≤\nR̃u ∗\nT ."
    }, {
      "heading" : "D Proof of Lemma 5",
      "text" : "Proof. Since, by assumption, u and X have length at most 1, the hinge loss simplifies to ℓ(u) = 1−Y ⟨u,X⟩ with gradient ∇ℓ(u) = −YX. This implies that\nu∗ ∶= argmin u E [ℓ(u)] = µ∥µ∥ , (8)\nand\n(w −u∗)⊺E [∇ℓ(w)∇ℓ(w)⊺] (w −u∗) = (w −u∗)⊺E [XX⊺] (w −u∗) ≤ λmax(w −u∗)⊺(w −u∗) ≤ 2λmax(1 − ⟨w,u∗⟩) = 2λmax∥µ∥ (w −u∗)⊺(−µ) = 2λmax∥µ∥ (w −u∗)⊺ E [∇ℓ(w)] ,\nwhich proves the first part of the lemma For the second part, we first observe that λmax = 1/d. Then, to compute∥µ∥, assume without loss of generality that ∥ū∥ = 1, in which case ū = u∗. Now symmetry of the distribution of X conditional on ⟨X,u∗⟩ gives E [YX ∣ ⟨X,u∗⟩] = sign(⟨X,u∗⟩)E [X ∣ ⟨X,u∗⟩] = sign(⟨X,u∗⟩)⟨X,u∗⟩u∗ = ∣⟨X,u∗⟩∣u∗.\nBy rotational symmetry, we may further assume without loss of generality that u∗ = e1 is the first unit vector in the standard basis, and therefore ∥µ∥ = ∥E [∣⟨X,u∗⟩∣]u∗∥ = E [∣X1∣] . If Z = (Z1, . . . , Zd) is multivariate Gaussian N(0, I). Then X = Z/∥Z∥ is uniformly distributed on the sphere, so\nE[∣X1∣] = E [ ∣Z1∣∥Z∥] ≥ 14√d P(∣Z1∣ ≥ 12 ∧ ∥Z∥ ≤ 2 √ d) .\nSince P (∣Z1∣ < 12) ≤ 0.4 and P (∥Z∥ ≥ 2√d) ≤ 14d E [∥Z∥2] = 14 , we have P(∣Z1∣ ≥ 12 ∧ ∥Z∥ ≤ 2√d) ≥ 1 − 0.4 − 14 = 0.35, from which the conclusion of the second part follows with c = 8/0.35."
    }, {
      "heading" : "E Proof of Theorem 3",
      "text" : "Proof. As pointed out below Condition 2, the (B,κ)-Bernstein condition implies the central condition with ǫ(η) ≤ (ηB) 11−κ . By Theorem 9, using 1/c ≤ 2(1+ γ2) and c ≤ 1\n2 , we find that for all γ ≥ 0\nR f∗ T ⊴γ (1 + γ2)2KTγ + (1 + 12γ2)ǫ(2γ)T + 2KT . (9) By Lemma 7 this implies for all γ ≥ 0\nE[Rf∗T ] ≤ (1 + γ2)2KTγ + (1 + 12γ2)ǫ(2γ)T + 2KT . It remains to tune γ to exploit the stochastic condition expressed by ǫ. Reducing the above right-hand-side expression to its main terms and setting the derivative to zero suggests picking\nγ̂ = (2KT (1 − κ)(2B)− 11−κ T ) 1−κ 2−κ\n= O ((KT /T ) 1−κ2−κ ) . Plugging in this tuning, we find\nE[Rf∗T ] ≤ (2 − κ) (4KTB) 12−κ (T /(1− κ)) 1−κ2−κ + (5 − κ)KT Finally, using that (2−κ) (4B) 12−κ (1/(1 − κ)) 1−κ2−κ is maximized in κ at κ = 1− 1\n4B\nwhere it takes value 1 + 4B, we may simplify this to\nE[Rf∗T ] ≤ (1 + 4B) (KT /4) 12−κT 1−κ2−κ + (5 − κ)KT = O (K 12−κT T 1−κ2−κ ) , which gives the first claim of the Theorem. Lemma 7 applied to (9) also implies that for all δ ≥ 0 with probability at least 1 − δ\nR f∗ T ≤ (1 + γ2)2KTγ + (1 + 12γ2)ǫ(2γ)T + 2KT + − ln δγ . Tuning γ as before with KT replaced by KT + − ln δ2 yields the second claim."
    }, {
      "heading" : "F Continuous Models",
      "text" : "We now consider Squint with models of predictors F that have uncountably many elements so that in general πf ∗\n= 0, and each f ∈ F is a function from X to A, with ℓft ∶= ℓ(yt, f(xt)) for some fixed loss function ℓ ∶ Y ×A → [0,1]. This setting includes standard parametric models in classification and regression but also countable unions thereof as well as nonparametric models. We first present an extension of Theorem 3 to this case; we then give an illustration of this result with sup-norm metric entropy numbers.\nSquint can be straightforwardly applied to uncountable models, but now the weight vector wt output by Squint at time t takes the form of a distribution on the set F . For general distributions u on F , the loss u incurs at time t is now defined as ℓut ∶= Ef∼u[ℓft ], so that the loss of Squint at time t is given by ℓwtt , which generalizes the expression ⟨wt, ℓt⟩ for the countable case. The regret of Squint relative to an arbitrary u is thus given by RuT = Ef∼u∑Tt=1(ℓwtt − ℓft ), and the variance term in (1) generalizes to V uT = Ef∼u∑Tt=1 vft with vft = (ℓwtt − ℓft )2.\nFor such models we will use that, as shown by Koolen [2015], Squint satisfies the following quantile or ‘KL’ bound:\nRuT ≤ 2 √ V uT K u T +K u T (10)\nwhich holds for every distribution u on F and prior π, where nowKuT = O(KL(u∥π)+ ln lnT ) and KL(u∥π) is the KL divergence between prior π and the distribution u.\nNote that (10) generalizes the countable bound (1), which is retrieved if u is taken to be a point mass on k.\nTheorem 13 (Extension of Theorem 3). In any stochastic setting satisfying the (B,κ)-Bernstein Condition 1, the guarantee (10) for Squint implies fast rates for Squint in expectation (if there is sufficient prior mass on f that behave similarly to f∗ in expectation) and with high probability (if there is sufficient prior mass on f taht are guaranteed to behave similarly to f∗ on all x). That is, for all T , for any sequence u1, u2, . . . of distributions on F and sequence of numbers C1,C2, . . . that satisfy\nE[ T∑ t=1 ℓuTt ] ≤ E [ T∑ t=1 ℓ f∗ t ] +CT , (11) we have E[Rf∗T ] = O ((KT +CT ) 12−κ T 1−κ2−κ ) , and if (11) holds for every sequence (xT , yT ), then we also have for any δ > 0, with probability at least 1 − δ,\nR f∗ T = O ((KT +CT − ln δ) 12−κT 1−κ2−κ ) where KT ∶=K uT T from (10).\nWhile this theorem does allow us to use priors u with uncountable support, it is easiest to illustrate with priors with support on a discretized version (countable subset) of F which may assign probability 0 to f∗: Example 1. Consider the classification setting where J is either finite or N, and F = ⋃j∈J Fj is a finite or countable union of sub-models such that for δ > 0, F̈j,δ ⊂ Fj is a minimal δ-cover of F̈j in the ℓ∞-norm (that is, we require\nsupf∈Fj minḟ∈F̈j,δ supx∈X ,y∈Y ∥ℓ(y, f(x)−ℓ(y, ḟ(x)∥ ≤ δ). Define Γ ∶= {20,2−1, . . .}. Assume that for all j, N (Fj , δ) ∶= ∣F̈j,δ ∣ < ∞ and note that logN (Fj , δ) is the metric entropy of Fj in the sup norm at scale δ. Let πJ be a probability mass function on J and let πN be a probability distribution on N with − logπJ (j)πN(k) = O(log(jk)) and let π be the prior on ⋃j∈N,δ∈Γ F̈j,δ with mass function π given by, for f ∈ F̈j,2−k , π(f) = πJ (j)πN(j)/N (Fj,2−k)). Then Theorem 13 gives the following bound in expectation (and mutatis m utandis in probability):\nR f∗ T = O ((T 2−k +min j,k log(jk) + logN (Fj ,2−k)) 12−κ T 1−κ2−κ) . Bounds in terms of models with bounded ℓ∞-entropy numbers were considered before by, e.g. Gaillard and Gerchinovitz [2015] with bounded squared error loss. We note that, if F has logarithmic entropy numbers (e.g. F = F1 and logN (F1, ǫ) = O(− log ǫ), then, by plugging in k = ⌈log2 T ⌉, we find that this cumulative regret bound is of the form O((logT ) ⋅T 1−κ2−κ ), the standard rate referred to in the discussion underneath Theorem 3. In the case of larger (polynomial) entropy numbers, our bounds are presumably suboptimal compared to the bounds that can be obtained by ERM, since Squint is essentially a form of an exponentially weighted forecaster that cannot exploit the chaining technique, viz. the discussion by Gaillard and Gerchinovitz [2015], Audibert [2009] and Rakhlin and Sridharan [2014]. Nevertheless, unlike ERM, Squint is robust and will continue to achieve nontrivial regret under nonstochastic, adversarially generated data, even with polynomial entropy numbers.\nIn practice, one may often work with Fi which have small (e.g. logarithmic) entropy numbers relative to the pseudo-distance d(f1, f2) = P(f1(X) ≠ f2(X)) considered by e.g. Tsybakov [2004], Audibert [2004], which may be much smaller than the ℓ∞-numbers. In such cases, Theorem 13 can still be used to give good bounds in expectation.\nProof of Theorem 13 Consider a (for now) arbitrary sequence u1, u2, . . ., define KT ∶=K uT T and K ′ T =KT /4, and\nC′T = −(RuTT −Rf∗T ) or equivalently T∑ t=1 ℓuTt = T∑ t=1 ℓ f∗ t +C ′ T .\nOne easily shows that for general a, b, c ∈ R, one has (a−b)2/2 ≤ (a−c)2+(b−c)2. Applying the statement with a = ℓwtt , b = ℓ f t and c = ℓ\nf∗t gives vft ≤ 2(vf∗t +(xft )2). Summing over t = 1..T and taking expectation over f ∼ uT now gives V uT T ≤ 2V f ∗ T + 2ET where ET = Ef∼uT [∑Tt=1(xft )2].\nApplying this to the bound (10) above at uT , we get\nR f∗ T ≤ C ′ T +2 √(V f∗T +ET )2K ′T +K ′T = infη {C′T + η(V f∗T +ET ) + 2K ′ T η +K ′T} .\n(12)\nWe first prove an analogue to Theorem 9 for the uncountable setting, based on (12). As in that theorem, let, for given F , {xft ∣f ∈ F} be the associated the excess loss family from (3), and let ǫ(η) be, as in (4), the corresponding maximal normalized cumulant generating function. Let K ′T be as above. Fix γ ≥ 0 and let c be as in Lemma 8. Now as in the proof of Theorem 9 we have for all f ∈ F , x f t ∈ [−1,1] and −xft ⊴η ǫ(η) by construction of ǫ(η). Hence −xft ⊴γ ǫ(γ) for all\nf ∈ F , which implies −Ef∼wt xft ⊴γ ǫ(γ) and also −Ef∼uT xft ⊴γ ǫ(γ) , and hence by Lemma 8 (see remark below the lemma),\ncγ E f∼wt [xft ]2 − E f∼wt [xft ] ⊴γ ǫ(2γ)(1 + cγ2) and (13) cγ E\nf∼uT [xft ]2 − E f∼uT [xft ] ⊴γ ǫ(2γ)(1 + cγ2). (14)\nUsing rf ∗ t = Ef∼wt [xft ], again analogously to the proof of Theorem 9, we may telescope (13) over rounds to\ncγV f∗ T −R f∗ T ⊴γ T ǫ(2γ)(1+ cγ2) (15) Now we use (12) with η = cγ\n2 , which implies 2Rf\n∗ T ≤ 2C ′ T + cγ(V f∗T + ET ) +\n4K ′T /(cγ)+ 2K ′T . Combining this with (15), we find: U ⊴ 0 with U = γRf ∗ T − (2γC′T + cγ2ET + 4K ′T c + γ2K ′T + γT ǫ(2γ)(1+ cγ2)) . (16) Similarly to deriving (15), using the definition of ET , we may telescope (14) over rounds to get\nU ′ ⊴ 0 with U ′ = cγ2ET − γC ′ T − γT ǫ(2γ)(1+ cγ2). (17)\nWe may now combine (16) and (17) using Lemma 7 with w a distribution that puts mass 1/2 on random variable U and 1/2 on U ′, to get (U +U ′)/2 ⊴ 0, which can be rewritten to:\nγ\n2 ⎛⎝Rf ∗\nT − (2C′T + cγET + 4K ′Tcγ + 2K ′T + T ǫ(2γ)(1+ cγ2)) + cγET − γC ′ T − γǫ(2γ)(1+ cγ2)⎞⎠ ⊴ 0,\nand further to\n1 2 R f ∗ T ⊴γ 3 2 C′T + KT cγ + T ǫ(2γ)(1+ cγ2) + 2K ′T , (18)\nwhich is the required analogue of the statement of Theorem 9. Note that this statement holds for every sequence u1, . . . , uT , and C′T is a random variable that depends on data (xT , yT ).\nThe remainder of the proof of Theorem 13 now follows in a fashion entirely analogous to the proof of Theorem 3, as in Appendix E, where we use that we can bound C′T by CT , either in expectation or on all sequences; we omit further details where one uses (18) instead of the corresponding statement of Theorem 9; we omit further details."
    } ],
    "references" : [ {
      "title" : "PAC-Bayesian statistical learning theory",
      "author" : [ "Jean-Yves Audibert" ],
      "venue" : "PhD thesis, Université Paris VI,",
      "citeRegEx" : "Audibert.,? \\Q2004\\E",
      "shortCiteRegEx" : "Audibert.",
      "year" : 2004
    }, {
      "title" : "Fast learning rates in statistical inference through aggregation",
      "author" : [ "Jean-Yves Audibert" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Audibert.,? \\Q2009\\E",
      "shortCiteRegEx" : "Audibert.",
      "year" : 2009
    }, {
      "title" : "Empirical minimization",
      "author" : [ "Peter L. Bartlett", "Shahar Mendelson" ],
      "venue" : "Probability Theory and Related Fields,",
      "citeRegEx" : "Bartlett and Mendelson.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bartlett and Mendelson.",
      "year" : 2006
    }, {
      "title" : "Convexity, classification, and risk bounds",
      "author" : [ "Peter L Bartlett", "Michael I Jordan", "Jon D McAuliffe" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Bartlett et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bartlett et al\\.",
      "year" : 2006
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "Improved second-order bounds for prediction with expert advice",
      "author" : [ "Nicolò Cesa-Bianchi", "Yishay Mansour", "Gilles Stoltz" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2007
    }, {
      "title" : "Adaptive regularization of weight vectors",
      "author" : [ "Koby Crammer", "Alex Kulesza", "Mark Dredze" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Crammer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Crammer et al\\.",
      "year" : 2009
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Fast rates in statistical and online learning",
      "author" : [ "Tim van Erven", "Peter D. Grünwald", "Nishant A. Mehta", "Mark D. Reid", "Robert C. Williamson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Erven et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Erven et al\\.",
      "year" : 2015
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Freund and Schapire.,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund and Schapire.",
      "year" : 1997
    }, {
      "title" : "A chaining algorithm for online nonparametric regression",
      "author" : [ "Pierre Gaillard", "Sébastien Gerchinovitz" ],
      "venue" : "In Proceedings of the 28th Conference on Learning Theory (COLT),",
      "citeRegEx" : "Gaillard and Gerchinovitz.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gaillard and Gerchinovitz.",
      "year" : 2015
    }, {
      "title" : "A second-order bound with excess losses",
      "author" : [ "Pierre Gaillard", "Gilles Stoltz", "Tim van Erven" ],
      "venue" : "In JMLR Workshop and Conference Proceedings,",
      "citeRegEx" : "Gaillard et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gaillard et al\\.",
      "year" : 2014
    }, {
      "title" : "The safe Bayesian: learning the learning rate via the mixability gap",
      "author" : [ "Peter Grünwald" ],
      "venue" : "In Proceedings 23rd International Conference on Algorithmic Learning Theory (ALT",
      "citeRegEx" : "Grünwald.,? \\Q2012\\E",
      "shortCiteRegEx" : "Grünwald.",
      "year" : 2012
    }, {
      "title" : "Extracting certainty from uncertainty: Regret bounded by variation in costs",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Hazan and Kale.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2010
    }, {
      "title" : "Local Rademacher complexities and oracle inequalities in risk minimization",
      "author" : [ "Vladimir Koltchinskii" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Koltchinskii.,? \\Q2006\\E",
      "shortCiteRegEx" : "Koltchinskii.",
      "year" : 2006
    }, {
      "title" : "The relative entropy bound for Squint",
      "author" : [ "Wouter M. Koolen" ],
      "venue" : "Blog entry on http://blog.wouterkoolen.info/,",
      "citeRegEx" : "Koolen.,? \\Q2015\\E",
      "shortCiteRegEx" : "Koolen.",
      "year" : 2015
    }, {
      "title" : "Second-order quantile methods for experts and combinatorial games",
      "author" : [ "Wouter M. Koolen", "Tim van Erven" ],
      "venue" : "In Proceedings of the 28th Conference on Learning Theory (COLT),",
      "citeRegEx" : "Koolen and Erven.,? \\Q2015\\E",
      "shortCiteRegEx" : "Koolen and Erven.",
      "year" : 2015
    }, {
      "title" : "Metagrad: Faster convergence without curvature in online convex optimization",
      "author" : [ "Wouter M. Koolen", "Tim van Erven" ],
      "venue" : null,
      "citeRegEx" : "Koolen and Erven.,? \\Q2016\\E",
      "shortCiteRegEx" : "Koolen and Erven.",
      "year" : 2016
    }, {
      "title" : "Learning the learning rate for prediction with expert advice",
      "author" : [ "Wouter M. Koolen", "Tim van Erven", "Peter D. Grünwald" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Koolen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Koolen et al\\.",
      "year" : 2014
    }, {
      "title" : "Achieving all with no parameters: Adaptive normalhedge",
      "author" : [ "Haipeng Luo", "Robert E. Schapire" ],
      "venue" : "In Proceedings of the 28th Conference on Learning Theory (COLT),",
      "citeRegEx" : "Luo and Schapire.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luo and Schapire.",
      "year" : 2015
    }, {
      "title" : "Risk bounds for statistical learning",
      "author" : [ "Pascal Massart", "Élodie Nédélec" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Massart and Nédélec.,? \\Q2006\\E",
      "shortCiteRegEx" : "Massart and Nédélec.",
      "year" : 2006
    }, {
      "title" : "Adaptive bound optimization for online convex optimization",
      "author" : [ "H. Brendan McMahan", "Matthew J. Streeter" ],
      "venue" : "In Proceedings of the 23rd Conference on Learning Theory (COLT),",
      "citeRegEx" : "McMahan and Streeter.,? \\Q2010\\E",
      "shortCiteRegEx" : "McMahan and Streeter.",
      "year" : 2010
    }, {
      "title" : "From stochastic mixability to fast rates",
      "author" : [ "Nishant A Mehta", "Robert C Williamson" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mehta and Williamson.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mehta and Williamson.",
      "year" : 2014
    }, {
      "title" : "Online nonparametric regression",
      "author" : [ "Alexander Rakhlin", "Karthik Sridharan" ],
      "venue" : "In Proceedings of the 27th Conference on Learning Theory (COLT),",
      "citeRegEx" : "Rakhlin and Sridharan.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rakhlin and Sridharan.",
      "year" : 2014
    }, {
      "title" : "Follow the leader if you can, Hedge if you must",
      "author" : [ "Steven de Rooij", "Tim van Erven", "Peter D. Grünwald", "Wouter M. Koolen" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Rooij et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rooij et al\\.",
      "year" : 2014
    }, {
      "title" : "Exploiting easy data in online optimization",
      "author" : [ "Amir Sani", "Gergely Neu", "Alessandro Lazaric" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sani et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sani et al\\.",
      "year" : 2014
    }, {
      "title" : "Online learning and online convex optimization",
      "author" : [ "Shai Shalev-Shwartz" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Shalev.Shwartz.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz.",
      "year" : 2011
    }, {
      "title" : "Adaptivity and optimism: An improved exponentiated gradient algorithm",
      "author" : [ "Jacob Steinhardt", "Percy Liang" ],
      "venue" : "In Proceedings of the 31th Annual International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Steinhardt and Liang.,? \\Q2014\\E",
      "shortCiteRegEx" : "Steinhardt and Liang.",
      "year" : 2014
    }, {
      "title" : "Optimal aggregation of classifiers in statistical learning",
      "author" : [ "Alexandre B. Tsybakov" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Tsybakov.,? \\Q2004\\E",
      "shortCiteRegEx" : "Tsybakov.",
      "year" : 2004
    }, {
      "title" : "Optimal learning with Bernstein Online Aggregation",
      "author" : [ "Olivier Wintenberger" ],
      "venue" : null,
      "citeRegEx" : "Wintenberger.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wintenberger.",
      "year" : 2015
    }, {
      "title" : "Nevertheless, unlike ERM, Squint is robust and will continue to achieve nontrivial regret under nonstochastic, adversarially generated data, even with polynomial entropy numbers",
      "author" : [ "nique", "viz" ],
      "venue" : null,
      "citeRegEx" : "nique and viz.,? \\Q2014\\E",
      "shortCiteRegEx" : "nique and viz.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "settings, adversarial regret lower bounds of order √ T are known, along with matching individual sequence algorithms [Shalev-Shwartz, 2011].",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al.",
      "startOffset" : 83,
      "endOffset" : 110
    }, {
      "referenceID" : 4,
      "context" : "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al.",
      "startOffset" : 83,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al.",
      "startOffset" : 83,
      "endOffset" : 158
    }, {
      "referenceID" : 4,
      "context" : "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al. [2014], Koolen et al.",
      "startOffset" : 83,
      "endOffset" : 178
    }, {
      "referenceID" : 4,
      "context" : "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al. [2014], Koolen et al. [2014], Koolen and Van Erven [2015], Luo and Schapire [2015], Wintenberger [2015].",
      "startOffset" : 83,
      "endOffset" : 200
    }, {
      "referenceID" : 4,
      "context" : "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al. [2014], Koolen et al. [2014], Koolen and Van Erven [2015], Luo and Schapire [2015], Wintenberger [2015].",
      "startOffset" : 83,
      "endOffset" : 229
    }, {
      "referenceID" : 4,
      "context" : "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al. [2014], Koolen et al. [2014], Koolen and Van Erven [2015], Luo and Schapire [2015], Wintenberger [2015].",
      "startOffset" : 83,
      "endOffset" : 254
    }, {
      "referenceID" : 4,
      "context" : "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al. [2014], Koolen et al. [2014], Koolen and Van Erven [2015], Luo and Schapire [2015], Wintenberger [2015]. Interestingly, the price for such adaptivity (i.",
      "startOffset" : 83,
      "endOffset" : 275
    }, {
      "referenceID" : 4,
      "context" : "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al. [2014], Koolen et al. [2014], Koolen and Van Erven [2015], Luo and Schapire [2015], Wintenberger [2015]. Interestingly, the price for such adaptivity (i.e. the worsening of the worst-case regret bound) is typically extremely small (i.e. a constant factor in the regret bound). For online convex optimization (OCO), many different types of adaptivity have been explored, including by [Crammer et al., 2009, Duchi et al., 2011, McMahan and Streeter, 2010, Hazan and Kale, 2010, Chiang et al., 2012, Steinhardt and Liang, 2014, Koolen and Van Erven, 2016]. Here we are interested in the question of whether such adaptive results are strong enough to lead to improved rates in the stochastic case when the data follow a “friendly” distribution. In specific cases it has been shown that fancy guarantees do imply significantly reduced regret. For example Gaillard et al. [2014] present a generic argument showing that a certain kind of second-order regret guarantees implies constant expected regret (the fastest possible rate) for i.",
      "startOffset" : 83,
      "endOffset" : 1044
    }, {
      "referenceID" : 9,
      "context" : "1 Hedge Setting We start with arguably the simplest setting of online prediction, the Hedge setting popularized by Freund and Schapire [1997]. To be able to illustrate the full reach of our stochastic assumption we will use a minor extension to countably infinitely many actions k ∈ N = {1,2, .",
      "startOffset" : 115,
      "endOffset" : 142
    }, {
      "referenceID" : 14,
      "context" : "We will make use of Squint by Koolen and Van Erven [2015], a self-tuning algorithm for playing wt.",
      "startOffset" : 30,
      "endOffset" : 58
    }, {
      "referenceID" : 26,
      "context" : "2 Online Convex Optimization (OCO) We now turn to our second setting called online convex optimization [Shalev-Shwartz, 2011].",
      "startOffset" : 103,
      "endOffset" : 125
    }, {
      "referenceID" : 15,
      "context" : "Wewill make use of (the full matrix version of)MetaGrad by Koolen and Van Erven [2016]. In their Theorem 8, they show that, simultaneously, R̃ T ≤ O (DG√T) and",
      "startOffset" : 59,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "3 Parameterized Family of Stochastic Assumptions We now recall the Bernstein [Bartlett and Mendelson, 2006] and Central [Van Erven et al.",
      "startOffset" : 77,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : "Massart and Nédélec [2006], Audibert [2009]).",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "Massart and Nédélec [2006], Audibert [2009]).",
      "startOffset" : 28,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "Massart and Nédélec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent κ > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al.",
      "startOffset" : 28,
      "endOffset" : 232
    }, {
      "referenceID" : 0,
      "context" : "Massart and Nédélec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent κ > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al.",
      "startOffset" : 28,
      "endOffset" : 263
    }, {
      "referenceID" : 0,
      "context" : "Massart and Nédélec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent κ > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al.",
      "startOffset" : 28,
      "endOffset" : 283
    }, {
      "referenceID" : 0,
      "context" : "Massart and Nédélec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent κ > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al.",
      "startOffset" : 28,
      "endOffset" : 302
    }, {
      "referenceID" : 0,
      "context" : "Massart and Nédélec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent κ > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions.",
      "startOffset" : 28,
      "endOffset" : 331
    }, {
      "referenceID" : 0,
      "context" : "Massart and Nédélec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent κ > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions. If F is sufficiently ‘simple’, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a κ-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) ⋅ T − 1 2−κ ). The bound interpolates between T −1/2 for κ = 0 and T −1 for κ = 1 (Massart condition). Results of Tsybakov [2004], Massart and Nédélec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.",
      "startOffset" : 28,
      "endOffset" : 803
    }, {
      "referenceID" : 0,
      "context" : "Massart and Nédélec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent κ > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions. If F is sufficiently ‘simple’, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a κ-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) ⋅ T − 1 2−κ ). The bound interpolates between T −1/2 for κ = 0 and T −1 for κ = 1 (Massart condition). Results of Tsybakov [2004], Massart and Nédélec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.",
      "startOffset" : 28,
      "endOffset" : 831
    }, {
      "referenceID" : 0,
      "context" : "Massart and Nédélec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent κ > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions. If F is sufficiently ‘simple’, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a κ-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) ⋅ T − 1 2−κ ). The bound interpolates between T −1/2 for κ = 0 and T −1 for κ = 1 (Massart condition). Results of Tsybakov [2004], Massart and Nédélec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.",
      "startOffset" : 28,
      "endOffset" : 848
    }, {
      "referenceID" : 0,
      "context" : "Massart and Nédélec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent κ > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions. If F is sufficiently ‘simple’, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a κ-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) ⋅ T − 1 2−κ ). The bound interpolates between T −1/2 for κ = 0 and T −1 for κ = 1 (Massart condition). Results of Tsybakov [2004], Massart and Nédélec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.g. Tsybakov [2004], Audibert [2004, 2009], Bartlett et al.",
      "startOffset" : 28,
      "endOffset" : 1024
    }, {
      "referenceID" : 0,
      "context" : "Massart and Nédélec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent κ > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions. If F is sufficiently ‘simple’, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a κ-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) ⋅ T − 1 2−κ ). The bound interpolates between T −1/2 for κ = 0 and T −1 for κ = 1 (Massart condition). Results of Tsybakov [2004], Massart and Nédélec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.g. Tsybakov [2004], Audibert [2004, 2009], Bartlett et al. [2006]. By summing from t = 1 to T and using ERM at each t to classify the next data point (so that ERM becomes FTL, follow-theleader), this suggests that we can achieve a cumulative expected regret E[Rf T ] of order O ((logT ) ⋅ T 1−κ 2−κ ).",
      "startOffset" : 28,
      "endOffset" : 1071
    }, {
      "referenceID" : 0,
      "context" : "Massart and Nédélec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent κ > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions. If F is sufficiently ‘simple’, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a κ-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) ⋅ T − 1 2−κ ). The bound interpolates between T −1/2 for κ = 0 and T −1 for κ = 1 (Massart condition). Results of Tsybakov [2004], Massart and Nédélec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.g. Tsybakov [2004], Audibert [2004, 2009], Bartlett et al. [2006]. By summing from t = 1 to T and using ERM at each t to classify the next data point (so that ERM becomes FTL, follow-theleader), this suggests that we can achieve a cumulative expected regret E[Rf T ] of order O ((logT ) ⋅ T 1−κ 2−κ ). Theorem 3 shows that this is, indeed, also the rate that Squint attains in such cases if F is countable and the optimal f∗ has positive prior mass π ∗ (more on this condition below)— we thus see that Squint obtains exactly the rates one would expect from a statistical learning/classification perspective, and the minimax excess risk results in that setting suggests that these cumulative regret rates cannot be improved in general. It was shown earlier by Audibert [2004] that, when equipped with an oracle to tune the learning rate η as a function of t, the rates O ((logT ) ⋅ T 1−κ 2−κ ) can also be achieved by Hedge, but the exact tuning depends on the unknown κ.",
      "startOffset" : 28,
      "endOffset" : 1780
    }, {
      "referenceID" : 0,
      "context" : "Massart and Nédélec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent κ > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions. If F is sufficiently ‘simple’, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a κ-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) ⋅ T − 1 2−κ ). The bound interpolates between T −1/2 for κ = 0 and T −1 for κ = 1 (Massart condition). Results of Tsybakov [2004], Massart and Nédélec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.g. Tsybakov [2004], Audibert [2004, 2009], Bartlett et al. [2006]. By summing from t = 1 to T and using ERM at each t to classify the next data point (so that ERM becomes FTL, follow-theleader), this suggests that we can achieve a cumulative expected regret E[Rf T ] of order O ((logT ) ⋅ T 1−κ 2−κ ). Theorem 3 shows that this is, indeed, also the rate that Squint attains in such cases if F is countable and the optimal f∗ has positive prior mass π ∗ (more on this condition below)— we thus see that Squint obtains exactly the rates one would expect from a statistical learning/classification perspective, and the minimax excess risk results in that setting suggests that these cumulative regret rates cannot be improved in general. It was shown earlier by Audibert [2004] that, when equipped with an oracle to tune the learning rate η as a function of t, the rates O ((logT ) ⋅ T 1−κ 2−κ ) can also be achieved by Hedge, but the exact tuning depends on the unknown κ. Grünwald [2012] provides a means to tune η automatically in terms of the data, but his method — like ERM and all algorithms in the references above — may achieve linear regret in worst-case settings, whereas Squint keeps the O(√T ) guarantee for such cases.",
      "startOffset" : 28,
      "endOffset" : 1992
    }, {
      "referenceID" : 11,
      "context" : "Gaillard et al. [2014] show constant regret for finitely many experts and i.",
      "startOffset" : 0,
      "endOffset" : 23
    } ],
    "year" : 2016,
    "abstractText" : "We consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly), yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance). We quantify the friendliness of stochastic environments by means of the well-known Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint for the Hedge setting and MetaGrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the Bernstein parameters of the stochastic environment. We prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability.",
    "creator" : "LaTeX with hyperref package"
  }
}