{
  "name" : "1407.5158.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Tight convex relaxations for sparse matrix factorization",
    "authors" : [ "Emile Richard", "Guillaume Obozinski", "Jean-Philippe Vert" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 7.\n51 58\nv1 [\nst at\n.M L\n] 1"
    }, {
      "heading" : "1 Introduction",
      "text" : "A range of machine learning problems such as link prediction in graphs containing community structure (Richard et al., 2014), phase retrieval (Candès et al., 2013), subspace clustering (Wang et al., 2013) or dictionary learning for sparse coding (Mairal et al., 2010) amount to solve sparse matrix factorization problems, i.e., to infer a low-rank matrix that can be factorized as the product of two sparse matrices with few columns (left factor) and few rows (right factor). Such a factorization allows for more efficient storage, faster computation, more interpretable solutions, and, last but not least, it leads to more accurate estimates in many situations. In the case of interaction networks for example, the assumption that the network is organized as a collection of highly connected communities which can overlap implies that the adjacency matrix admits such a factorization. More generally, considering sparse low-rank matrices combines two natural forms of sparsity, in the spectrum and in the support, which can be motivated by the need to explain systems behaviors by a superposition of latent processes which only involve a few parameters. Landmark applications of sparse matrix factorization are sparse principal components analysis (SPCA, d’Aspremont et al., 2007; Zou et al., 2006) or sparse canonical correlation analysis (SCCA, Witten et al., 2009), which are widely used to analyze high-dimensional data such as genomic data. From a computational point of view, however, sparse matrix factorization is challenging since it typically leads to non-convex, NP-hard problems (Moghaddam et al., 2006). For instance, Berthet and Rigollet (2013) noted that solving sparse PCA with a single component is equivalent to the planted clique\nproblem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix. Many heuristics and relaxations have therefore been proposed, with and without theoretical guaranties, to approximatively solve the problems leading to sparse low-rank matrices. A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010). Despite these worst case computational hardness, simple generalizations of the power method have been proposed by Journée et al. (2010); Luss and Teboulle (2013); Yuan and Zhang (2013) for the sparse PCA problem with a single component. These algorithms perform well empirically and have been proved to be efficient theoretically under mild conditions by Yuan and Zhang (2013). Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d’Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations. Several authors also investigated the performance of regularizing a convex loss with linear combinations of the ℓ1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014). This penalty term can be related to the SDP relaxations of d’Aspremont et al. (2007, 2008) that penalize the trace and the element-wise ℓ1 norm of the positive semi-definite unknown. The statistical performance of these basic combinations of the two convex criteria has however been questioned by Krauthgamer et al. (2013); Oymak et al. (2012). Oymak et al. (2012) showed that for compressed sensing applications, no convex combination of the two norms improves over each norm taken alone. Krauthgamer et al. (2013) prove that the SDP relaxations fail at finding the sparse principal component outside the favorable regime where a simple diagonal thresholding algorithm (Amini and Wainwright, 2009) works. Moreover, these existing convex formulations either aim at finding only a rank one matrix, or a low rank matrix whose factors themselves are not necessarily guaranteed to be sparse. In this work, we propose two new matrix norms which, when used as regularizer for various optimization problems, do yield estimates for low-rank matrices with multiple sparse factors that are provably more efficient statistically than the ℓ1 and trace norms. The price to pay for this statistical efficiency is that, although convex, the resulting optimization problems are NP-hard, and we must resort to heuristic procedures to solve them. Our numerical experiments however confirm that we obtain the desired theoretical gain to estimate low-rank sparse matrices."
    }, {
      "heading" : "1.1 Contributions and organization of the paper",
      "text" : "More precisely, our contributions are:\n• Two new matrix norms (Section 2). In order to properly define matrix factorization, given sparsity levels of the factors denoted by integers k and q, we first introduce in Section 2.1 the (k, q)-rank of a matrix as the minimum number of left and right factors, having respectively k and q nonzeros, required to reconstruct a matrix. This index is a more involved complexity measure for matrices than the rank in that it conditions on the number of nonzero elements of the left and right factors of a matrix. Using this index, we propose in Section 2.2 two new atomic norms for matrices (Chandrasekaran et al., 2012). (i) Considering the convex hull unit\noperator norm matrices with (k, q)-rank = 1, we build a convex surrogate to low (k, q)-rank matrix estimation problem. (ii) We introduce a polyhedral norm built upon (k, q)-rank = 1 matrices with all non-zero entries of absolute value equal to 1. We provide in Section 2.3 an equivalent characterization of the norms as nuclear norms, in the sense of Jameson (1987), highlighting in particular a link to the k-support norm of Argyriou et al. (2012).\n• Using these norms to estimate sparse low-rank matrices (Section 3). We show how several problems such as bilinear regression or sparse PCA can be formulated as convex optimization problems with our new norms, and clarify that the resulting problems can however be NP-hard.\n• Statistical Analysis (Section 4). We study the statistical performance of the new norms and compare them with existing penalties. Our analysis goes first in Section 4.1 using slow rate type of upper bounds on the denoising error, which despite sub-optimality gives a first insight on the gap between the statistical performance of our (k, q)-trace norm and that of the ℓ1 and trace norms. Next we show in Section 4.2, using cone inclusions and estimates of statistical dimension, that our norms are superior to any convex combination of the trace norm and the ℓ1 norm in a number of different tasks. However, our analysis also shows that the factors gained over the rivals to estimate sparse low-rank matrices vanishes when we use our norm to estimate sparse vectors.\n• A working set algorithm (Section 5). While in the vector case the computation remains feasible in polynomial time, the norms we introduce for matrices can not be evaluated in polynomial time. We propose algorithmic schemes to approximately learn with the new norms. The same norms and meta-algorithms can be used as a regularizer in supervised problems such as bilinear and quadratic regression. Our algorithmic contribution does not consist in providing more efficient solutions to the rank-1 SPCA problem, but to combine atoms found by the rank-1 solvers in a principled way.\n• Numerical experiments (Section 6). We numerically evaluate the performance of our new norms on simulated data, and confirm the theoretical results. While our theoretical analysis only focuses on the estimation of sparse matrices with (k, q)-rank one, our simulations allow us to conjecture that the statistical dimension scales linearly with the (k, q)-rank and decays with the overlap between blocks. We also show that our model is competitive with the stateof-the-art on the problem of sparse PCA.\nDue to their length and technicality, all proofs are postponed to the appendices."
    }, {
      "heading" : "1.2 Notations",
      "text" : "For any integers 1 ≤ k ≤ p, [1, p] = {1, . . . , p} is the set of integers from 1 to p and Gpk denotes the set of subsets of k indices in [1, p]. For a vector w ∈ Rp, ‖w‖0 is the number of non-zero coefficients in w, ‖w‖1 = ∑p i=1 |wi| is its ℓ1 norm, ‖w‖2 = (∑p i=1 w 2 i ) 1 2 is its Euclidean norm, ‖w‖∞ = maxi |wi| is its ℓ∞ norm and supp(w) ∈ Gp‖w‖0 is its support, i.e., the set of indices of the nonzero entries of w. For any I ⊂ [1, p], wI ∈ Rp is the vector that is equal to w on I, and has 0 entries elsewhere. Given matrices A and B of the same size, 〈A,B〉 = tr(A⊤B) is the standard inner product of matrices. For any matrix Z ∈ Rm1×m2 the notations ‖Z‖0, ‖Z‖1, ‖Z‖∞, ‖Z‖Fro, ‖Z‖∗, ‖Z‖op and rank(Z) stand respectively for the number of nonzeros, entry-wise ℓ1 and ℓ∞ norms, the standard ℓ2 (or Frobenius) norm, the trace-norm (or nuclear norm, the sum of the singular values), the operator\nnorm (the largest singular value) and the rank of Z, while supp(Z) ⊂ [1,m1]× [1,m2] is the support of Z, i.e., the set of indices of nonzero elements of Z. When dealing with a matrix Z whose nonzero elements form a block of size k × q, supp(Z) takes the form I × J where (I, J) ∈ Gm1k × Gm2q . For a matrix Z and two subsets of indices I ⊂ [1,m1] and J ⊂ [1,m2], ZI,J is the matrix having the same entries as Z inside the index subset I × J , and 0 entries outside. This notation should not be confused with the notation Z(IJ) which we will sometimes use to denote a general matrix with support contained in I × J ."
    }, {
      "heading" : "2 Tight convex relaxations of sparse factorization constraints",
      "text" : "In this section we propose two new matrix norms allowing to formulate various sparse matrix factorization problems as convex optimization problems. We start by defining the (k, q)-rank of a matrix in Section 2.1, a useful generalization of the rank which also quantifies the sparseness of a matrix factorization. We then introduce two atomic norms defined as tight convex relaxations of the (k, q)-rank in Section 2.2: the (k, q)-trace norm, obtained by relaxing the (k, q)-rank over the operator norm ball, and the (k, q)-CUT norm, obtained by a similar construction with extra-constraints on the element-wise ℓ∞ of factors. In Section 2.3 we relate these matrix norms to vector norms using the concept of nuclear norms, establishing in particular a connection of the (k, q)-trace norm for matrices with the k-support norm of Argyriou et al. (2012), and the (k, q)-CUT norm to the vector k-norm, defined as the sum of the k largest components in absolute value of a vector (Bhatia, 1997, Exercise II.1.15).\n2.1 The (k, q)-rank of a matrix\nThe rank of a matrix Z ∈ Rm1×m2 is the minimum number of rank-1 matrices (i.e., outer products of vectors of the form ab⊤ for a ∈ Rm1 and b ∈ Rm2) needed to express Z as a linear combination of the form Z = ∑r i=1 aib ⊤ i . It is a versatile concept in linear algebra, central in particular to solve matrix factorization problems and low-rank approximations. The following definition generalizes this notion to incorporate constraints on the sparseness of the rank-1 elements:\nDefinition 1 ((k, q)-SVD and (k, q)-rank) For a matrix Z ∈ Rm1×m2 , we call (k, q)-sparse singular value decomposition of Z (or (k, q)-SVD) any decomposition of the form Z = ∑r i=1 ciaib ⊤ i where c1 ≥ c2 ≥ · · · ≥ cr > 0, ai (resp. bi) are unit vectors with at most k (resp. q) nonzero elements, and with minimal r, which we call the (k, q)-rank of Z. In such a decomposition, we refer to vectors ai and bi as left and right (k, q)-sparse singular vectors of Z, and to ci as its (k, q)-sparse singular values.\nThe (k, q)-rank and (k, q)-SVD of Z can equivalently be defined as the optimal value and one of the solutions of the optimization problem:\nmin ‖c‖0 s.t. Z = ∞∑\ni=1\nciaib ⊤ i , (ai, bi, ci) ∈ Am1k ×Am2q × R+ , (1)\nwhere for any 1 ≤ j ≤ n, Anj = {a ∈ Rn : ‖a‖0 ≤ j, ‖a‖2 = 1} .\nrefers to the set of n-dimensional unit vectors with at most j non-zero components. When k = m1 and q = m2, we recover the usual notions of rank and SVD of a matrix. In general, however, the (k, q)-rank and (k, q)-SVD do not share several important properties of the usual rank and SVD, as the following proposition shows:\nProposition 2 (Properties of the (k, q)-SVD)\n1. The (k, q)-rank of a matrix Z ∈ Rm1×m2 can be strictly larger than m1 and m2.\n2. The (k, q)-SVD is not necessarily unique.\n3. The (k, q)-sparse singular vectors are not necessarily orthogonal to each other.\nFor k = q = 1, the (1, 1)-SVD decomposes Z as a sum of matrices with only one non-zero element, showing that (1, 1)-rank(Z) = ‖Z‖0. Since Ani ⊂ Anj when i ≤ j, we deduce from the expression of the (k, q)-rank as the optimal value of (1) that the following tight inequalities hold:\n∀(k, q) ∈ [1,m1]× [1,m2] , rank(Z) ≤ (k, q)-rank(Z) ≤ ‖Z‖0 .\nThe (k, q)-rank is useful to formulate problems in which a matrix should be modeled as or approximated by a matrix with sparse low rank factors, with the assumption that the sparsity level of the factors is fixed and known. For example, the standard rank-1 SPCA problem consists in finding the symmetric matrix with (k, k)-rank equal to 1 and providing the best approximation of the sample covariance matrix (Zou et al., 2006).\n2.2 Two convex relaxations for the (k, q)-rank\nThe (k, q)-rank is obviously a discrete, nonconvex index, like the rank or the cardinality, leading to computational difficulties when one wants to estimate matrices with small (k, q)-rank. In this section, we propose two convex relaxations of the (k, q)-rank aimed at mitigating these difficulties. They are both instances of the atomic norms introduced by Chandrasekaran et al. (2012), which we first review.\nDefinition 3 (Atomic norm) Given a centrally symmetric compact subset A ⊂ Rp of elements called atoms, the atomic norm induced by A on Rp is the gauge function1 of A, defined by\n‖x‖A = inf {t > 0 : x ∈ t conv (A)} , (2)\nwhere conv(A) denotes the convex hull of A.\nChandrasekaran et al. (2012) show that the atomic norm induced by A is indeed a norm, which can be rewritten as\n‖x‖A = inf { ∑\na∈A ca : x =\n∑ a∈A caa, ca ≥ 0, ∀a ∈ A\n} , (3)\nand whose dual norm satisfies\n‖x‖∗A := sup {〈x, z〉 : ‖z‖A ≤ 1} = sup {〈x, a〉 : a ∈ A} . (4)\nWe can now define our first convex relaxation of the (k, q)-rank:\nDefinition 4 ((k, q)-trace norm) For a matrix Z ∈ Rm1×m2 , the (k, q)-trace norm Ωk,q(Z) is the atomic norm induced by the set of atoms:\nAk,q = { ab⊤ : a ∈ Am1k , b ∈ Am2q } . (5)\n1see Rockafellar (1997), p. 28, for a precise definition of gauge functions.\nIn words, Ak,q is the set of matrices Z ∈ Rm1×m2 such that (k, q)-rank(Z) = 1 and ‖Z‖op = 1. Plugging (5) into (3), we obtain an equivalent definition of the (k, q)-trace norm as the optimal value of the following optimization problem:\nΩk,q(Z) = min { ‖c‖1 : Z = ∞∑\ni=1\nciaib ⊤ i , (ai, bi, ci) ∈ Am1k ×Am2q × R+\n} . (6)\nComparing (6) to (1) shows that the (k, q)-trace norm is derived from the (k, q)-rank by replacing the non-convex ℓ0 pseudo-norm of c by its convex ℓ1 norm in the optimization problem. In particular, in the case k = m1 and q = m2, the (k, q)-trace norm is the usual trace norm (equal to the ℓ1-norm of singular values), i.e. the usual relaxation of the rank (which is the ℓ0-norm of the singular values). Similarly, when k = q = 1, the (k, q)-trace norm is simply the ℓ1 norm. Just like the (k, q)-rank interpolates between the ℓ0 pseudo-norm and the rank, the (k, q)-trace norm interpolates between the ℓ1 norm and the trace norm. Indeed, since Ani ⊂ Anj when i ≤ j, we deduce from the expression of Ωk,q as the optimal value of (6) that the following tight inequalities hold for any 1 ≤ k ≤ m1 and 1 ≤ q ≤ m2: Ωm1,m2(Z) = ‖Z‖∗ ≤ Ωk,q(Z) ≤ ‖Z‖1 = Ω1,1(Z) . (7) While the SVD decomposition of a matrix used to define its trace norm is the same as the one used to define its rank, this may not be the case anymore for the (k, q)-trace norm. Indeed, in the general case, the (k, q)-trace norm may not be simply the sum of (k, q)-sparse singular values associated to a (k, q)-SVD according to Definition 1, because the vectors c that solve (6) and (1) can be different. This justifies the following definition:\nDefinition 5 (Soft-(k, q)-SVD and soft-(k, q)-rank) For a matrix Z ∈ Rm1×m2 , we call soft(k, q)-sparse singular value decomposition (or soft-(k, q)-SVD) any decomposition Z = ∑r i=1 ciaib ⊤ i that solves (6) with c1 ≥ c2 ≥ . . . ≥ cr > 0. The soft-(k, q)-rank of Z is the minimum number of terms in a soft-(k, q)-SVD of Z.\nSimilar to the (k, q)-SVD, the soft-(k, q)-SVD lacks many important properties of the trace norm when k < m1 and q < m2:\nProposition 6 1. The soft-(k, q)-rank of a matrix can be strictly larger than its (k, q)-rank.\n2. The soft-(k, q)-SVD is not necessarily unique.\n3. The soft-(k, q)-sparse singular vectors are not necessarily orthogonal to each other.\nIn addition to (6), the next lemma provides another explicit formulation for the (k, q)-trace norm, its dual and its sub differential:\nLemma 7 For any Z ∈ Rm1×m2 we have\nΩk,q(Z) = inf   \n∑\n(I,J)∈Gm1 k ×Gm2q\n∥∥∥Z(I,J) ∥∥∥ ∗ : Z = ∑\n(I,J)\nZ(I,J) , supp(Z(I,J)) ⊂ I × J    , (8)\nand Ω∗k,q(Z) = max { ‖ZI,J‖op : I ∈ G m1 k , J ∈ Gm2q } . (9)\nThe subdifferential of Ωk,q at an atom A = ab ⊤ ∈ Ak,q with I0 = supp(a) and J0 = supp(b) is\n∂Ωk,q(A) = { A+ Z : AZ⊤I0,J0 = 0, A ⊤ZI0,J0 = 0, ∀(I, J) ∈ Gm1k × Gm2q ‖AI,J + ZI,J‖op ≤ 1 } .\n(10)\nOur second norm is again an atomic norm, but is obtained by focusing on a more restricted set of atoms. It is motivated by applications where we want to estimate matrices which, in addition to being sparse and low-rank, are constant over blocks, such as adjacency matrices of graphs with non-overlapping communities. For that purpose, consider first the subset of Amk made of vectors whose nonzero entries are all equal in absolute value:\nÃmk = { a ∈ Rm, ‖a‖0 = k , ∀i ∈ supp(a), |ai| = 1√k } .\nWe can then define our second convex relaxation of the (k, q)-rank:\nDefinition 8 ((k, q)-CUT norm) We define the (k, q)-CUT norm Ω̃k,q(Z) as the atomic norm induced by the set of atoms\nÃk,q = { ab⊤ : a ∈ Ãm1k , b ∈ Ãm2q } . (11)\nIn other words, the atoms in Ãk,q are the atoms of Ak,q whose nonzero elements all have the same amplitude. Our choice of terminology is motivated by the following relation of our norm to the CUT-polytope: in the case k = m1 and q = m2, the unit ball of Ω̃k,q coincides (up to a scaling factor of √ m1m2) with the polytope known as the CUT polytope of the complete graph on n vertices (Deza and Laurent, 1997), defined by CUT = conv {ab⊤ , a ∈ {±1}m1 , b ∈ {±1}m2} . The norm obtained as the gauge of the CUT polytope is therefore to the trace norm as Ω̃k,q is to Ωk,q."
    }, {
      "heading" : "2.3 Equivalent nuclear norms built upon vector norms",
      "text" : "In this section we show that the (k, q)-trace norm (Definition 4) and the (k, q)-CUT norm (Definition 8), which we defined as atomic norms induced by specific atom sets, can alternatively be seen as instances of nuclear norms considered by Jameson (1987). For that purpose it is useful to recall the general definition of nuclear norms and the characterization of the corresponding dual norms as formulated in Jameson (1987, Propositions 1.9 and 1.11):\nProposition 9 (nuclear norm) Let ‖·‖α and ‖·‖β denote any vector norms on Rm1 and Rm2, respectively, then\nν(Z) := inf\n{ ∑\ni\n‖ai‖α ‖bi‖β : Z = ∑\ni\naib ⊤ i\n} ,\nwhere the infimum is taken over all summations of finite length, is a norm over Rm1×m2 called the nuclear norm induced by ‖·‖α and ‖·‖β. Its dual is given by\nν∗(Z) = sup {a⊤Zb : ‖a‖α ≤ 1 , ‖b‖β ≤ 1} . (12)\nThe following lemma shows that the nuclear norm induced by two atomic norms is itself an atomic norm.\nLemma 10 If ‖·‖α and ‖·‖β are two atomic norms on Rm1 and Rm2 induced respectively by two atom sets A1 and A2, then the nuclear norm on Rm1×m2 induced by ‖·‖α and ‖·‖β is an atomic norm induced by the atom set:\nA = {ab⊤ : a ∈ A1 , b ∈ A2} .\nWe can deduce from it that the (k, q)-trace norm and (k, q)-CUT are nuclear norms, associated to particular vector norms:\nTheorem 11 1. The (k, q)-trace norm is the nuclear norm induced by θk on R m1 and θq on\nRm2 , where for any j ≥ 1, θj is the j-support norm introduced by Argyriou et al. (2012).\n2. The (k, q)-CUT norm is the nuclear norm induced by κk on R m1 and κq on R m2, where for any j ≥ 1:\nκj(w) = 1√ j max\n( ‖w‖∞, 1\nj ‖w‖1\n) . (13)\nFor the sake of completeness, let us recall the closed-form expression of the k-support norm θk shown by Argyriou et al. (2012). For any vector w ∈ Rp, let w̄ ∈ Rp be the vector obtained by sorting the entries of w by decreasing order of absolute values. Then it holds that\nθk(w) =    k−r−1∑\ni=1\n|w̄i|2 + 1\nr + 1\n( p∑\ni=k−r |w̄i|\n)2  1 2 , (14)\nwhere r ∈ {0, · · · , k − 1} is the unique integer such that |w̄k−r−1| > 1r+1 ∑p\ni=k−r |w̄i| ≥ |w̄k−r|, and where by convention |w̄0| = ∞. Of course, Theorem 11 implies that in the vector case (m2 = 1), the (k, q)-trace norm is simply equal to θk and the (k, q)-CUT norm is equal to κk. A representation of the “sharp edges\" of unit balls of θk, κk and a appropriately scaled ℓ1 norm can be found in Figure 1 for the case m1 = 3 and k = 2. In addition, the following results shows that the dual norms of θk and κk have simple explicit forms:\nProposition 12 The dual norms of θk and κk satisfy respectively:\nθ∗k(s) = max I:|I|=k ‖sI‖2 and κ∗k(s) = 1√ k max I:|I|=k ‖sI‖1 .\nTo conclude this section, let us observe that nuclear norms provide a natural framework to construct matrix norms from vector norms, and that other choices beyond θk and κk may lead to interesting norms for sparse matrix factorization. It is however known since Jameson (1987) (see also Bach, 2013; Bach et al., 2012) that the nuclear norm induced by vector ℓ1-norm is simply the ℓ1 of the matrix which fails to induce low rank (except in the very sparse case). However Bach et al. (2012) proposed nuclear norms associated with vectors norms that are similar to the elastic net penalty."
    }, {
      "heading" : "3 Learning matrices with sparse factors",
      "text" : "In this section, we briefly discuss how the (k, q)-trace norm and (k, q)-CUT norm can be used to attack various problems involving estimation of sparse low-rank matrices."
    }, {
      "heading" : "3.1 Denoising",
      "text" : "Suppose X ∈ Rm1×m2 is a noisy observation of a low-rank matrix with sparse factors, assumed to have low (k, q)-rank. A natural convex formulation to recover the noiseless matrix is to solve:\nmin Z\n1 2 ‖Z −X‖2Fro + λΩk,q(Z) , (15)\nwhere λ is a parameter to be tuned. Note that in the limit when λ → 0, one simply obtains a soft-(k, q)-SVD of X."
    }, {
      "heading" : "3.2 Bilinear regression",
      "text" : "More generally, given some empirical risk L(Z), it is natural to consider formulations of the form min Z L(Z) + λΩk,q(Z)\nto learn matrices that are a priori assumed to have a low (k, q)-rank. A particular example is bilinear regression, where, given two inputs x ∈ Rm1 and x′ ∈ Rm2 , one observes as output a noisy version of y = x⊤Zx′. Assuming that Z has low (k, q)-rank means that the noiseless response is a sum of a small number of terms, each involving only a small number of features from either of the input vectors. To estimate such a model from observations (xi, x ′ i, yi)i=1,...,n, one can consider the following convex formulation:\nmin Z\nn∑\ni=1\nℓ ( x⊤i Zx ′ i, yi ) + λΩk,q(Z) , (16)\nwhere ℓ is a loss function. A particular instance of (16) of interest is the quadratic regression problem, where m1 = m2 and xi = x ′ i for i = 1, . . . , n. Quadratic regression combined with additional constraints on Z is closely related to phase retrieval (Candès et al., 2013). It should be noted that if ℓ is the least-square loss, (16) can be rewritten in the form\nmin Z\n1 2 ‖X (Z)− y‖22 + λΩk,q(Z) ,\nwhere X (Z) is a linear transformation of Z, so that the problem is from the point of view of the parameter Z a linear regression with a well chosen feature map."
    }, {
      "heading" : "3.3 Subspace clustering.",
      "text" : "In subspace clustering, one assumes that the data can be clustered in such a way that the points in each cluster belong to a low dimensional space. If we have a design matrix X ∈ Rn×p with each row corresponding to an observation, then the previous assumption means that if X(j) ∈ Rnj×p is a matrix formed by the rows of cluster j, there exist a low rank matrix Z(j) ∈ Rnj×nj such that Z(j)X(j) = X(j). This means that there exists a block-diagonal matrix Z such that ZX = X with low-rank diagonal blocks. This idea, exploited recently by Wang et al. (2013) implies that Z is a sum of low rank sparse matrices; and this property still holds if the clustering is unknown. We therefore suggest that if all subspaces are of dimension k, Z may be estimated via\nmin Z Ωk,k(Z) s.t. ZX = X ."
    }, {
      "heading" : "3.4 Sparse PCA",
      "text" : "In sparse PCA (d’Aspremont et al., 2007; Witten et al., 2009; Zou et al., 2006), one tries to approximate an empirical covariance matrix Σ̂n by a low-rank matrix with sparse factors. Although this is similar to the denoising problem discussed in Section 3.1, one may wish in addition that the estimated sparse low-rank matrix be symmetric and positive semi-definite (PSD), in order to represent a plausible covariance matrix. This suggests to formulate sparse PCA as follows:\nmin Z {∥∥∥Σ̂n − Z ∥∥∥ Fro : (k, k)-rank(Z) ≤ r and Z 0 } , (17)\nwhere k is the maximum number of non-zero coefficient allowed in each principal direction. In contrast to sequential approaches that estimate the principal components one by one (Mackey, 2009), this formulation requires to find simultaneously a set of factors which are complementary to one another in order to explain as much variance as possible. A natural convex relaxation of (17) is\nmin Z\n{ 1\n2\n∥∥∥Σ̂n − Z ∥∥∥ 2\nFro + λΩk,k(Z) : Z 0\n} , (18)\nwhere λ is a parameter that controls in particular the rank of the approximation. However, although the solution of (18) is always PSD, its soft-(k, k)-SVD Z = ∑r i=1 ciaib ⊤ i may not be composed of symmetric matrices (if ai 6= bi), and even if ai = bj the corresponding ci may be negative, as the following proposition shows:\nProposition 13 1. The (k, k)-SVD of a PSD matrix is not necessarily a sum of symmetric terms.\n2. Some PSD matrices cannot be written as a positive combination of rank one (k, k)-sparse matrices, even for k > 1.\nThis may be unappealing, as one would like to interpret the successive rank-1 matrices as covariance matrices over a subspace that explain some of the total variance. One may therefore prefer a decomposition with less sparse or more factors, potentially capturing less variance. One solution is to replace Ωk,k in (18) by another penalty which directly imposes symmetric factors with non-negative weights. This is easily obtained by replacing the set of atoms Ak,k in Definition 4 by Ak, = {aa⊤, a ∈ Ak}, and considering the corresponding atomic norm which we denote by Ωk, . To be precise, Ωk, is not a norm but only a gauge because the set Ak, is not centrally symmetric. Instead of (18), it possible to use the following convex formulation of sparse PCA:\nmin Z\n1\n2\n∥∥∥Σ̂n − Z ∥∥∥ 2\nFro + λΩk, (Z) . (19)\nBy construction, the solution of (19) is not only PSD, but can be expanded as a sum of matrices Z = ∑r i=1 ciaia ⊤ i , where for all i = 1, . . . , r, the factor ai is k-sparse and the coefficient ci is positive. This formulation is therefore particularly relevant if Σ̂n is believed to be a noisy matrix of this form. It should be noted however that, by Proposition 13, Ωk, is infinite for some PSD matrices2, which implies that some PSD matrices cannot be approximated well with this formulation."
    }, {
      "heading" : "3.5 NP-hard convex problems",
      "text" : "Although the (k, q)-trace norm and related norms allow us to formulate several problems of sparse low-rank matrix estimation as convex optimization problems, it should be pointed out that this does not guarantee the existence of efficient computational procedures to solve them. Here we illustrate this with the special case of the best (k, q)-sparse and rank 1 approximation to a matrix, which turns out to be a NP-hard problem. Indeed, let us consider the three following optimization problems, which are equivalent since they return the same rank one subspace spanned by ab⊤:\nmin (a,b,c)∈Ak×Aq×R+ ‖X − cab⊤‖2Fro ; max (a,b)∈Ak×Aq a⊤Xb ; max Z: Ωk,q(Z)≤1 tr(XZ⊤) . (20)\nIn particular, if k = q and X = Σ̂n is an empirical covariance matrix, then the symmetric solutions of the problem considered are the solution to the following rank 1 SPCA problem\nmax z\n{ z⊤Σ̂nz : ‖z‖2 = 1 , ‖z‖0 ≤ k } , (21)\nwhich it is known to be NP-hard (Moghaddam et al., 2008). This shows that, in spite of being a convex formulation involving the (k, q)-trace norm, the third formulation in (20) is actually NPhard. In practice, we will propose heuristics in Section 6 to approximate the solution of convex optimization problems involving the (k, q)-trace norm.\n4 Statistical properties of the (k, q)-trace norm and the (k, q)-CUT norm\nIn this section we study theoretically the benefits of using the new penalties Ωk,q and Ω̃k,q to infer low-rank matrices with sparse factors, as suggested in Section 3, postponing the discussion of how to do it in practice to Section 5. Building upon techniques proposed recently to analyze the statistical properties of sparsity-inducing penalties, such as the ℓ1 penalty or more general atomic norms, we investigate two approaches to derive statistical guarantees. In Section 4.1 we study the expected dual norm of some noise process, from which we can deduce upper bounds on the learning rate for least squares regression and a simple denoising task. In Section 4.2 we estimate the statistical dimension of objects of interest both in the matrix and vector cases and compare the asymptotic rates, which shed light on the power of the norms we study when used as convex penalties. The results in Section 4.1 are technically easier to derive and contain bounds for a matrix of arbitrary (k, q)-rank. The results provided in Section 4.2 rely on a more involved set of tools, they provide more powerful bounds but we do not derive results for matrices of arbitrary (k, q)-rank.\n4.1 Performance of the (k, q)-trace norm in denoising\nIn this Section we consider the simple denoising setting (Section 3.1) where we wish to recover a low-rank matrix with sparse factors Z⋆ ∈ Rm1×m2 from a noisy observation Y ∈ Rm1×m2 corrupted\n2This is possible because Ωk, is only a gauge and not a norm.\nby additive Gaussian noise: Y = Z⋆ + σG , where σ > 0 and G is a random matrix with entries i.i.d. from N (0, 1). Given a convex penalty Ω : Rm1×m2 → R, we consider, for any λ > 0, the estimator\nẐλΩ ∈ argmin Z\n1 2 ‖Z − Y ‖2Fro + λΩ(Z) .\nThe following result, valid for any norm Ω, provides a general control of the estimation error in this setting, involving the dual norm of the noise: Lemma 14 If λ ≥ σΩ∗(G) then ∥∥∥ẐλΩ − Z⋆ ∥∥∥ 2\nFro ≤ 4λΩ(Z⋆) .\nThis suggests to study the dual norm of a random noise matrix Ω∗(G) in order to derive a upper bound on the estimation error. The following result provides such upper bounds, in expectation, for the (k, q)-trace norm as well as the standard ℓ1 and trace norms: Proposition 15 Let G ∈ Rm1×m2 be a random matrix with entries i.i.d. from N (0, 1). The expected dual norm of G for the (k, q)-trace norm, the ℓ1 norm and the trace norm is respectively bounded by:\nEΩ∗k,q(G) ≤ 4 (√\nk log m1 k + 2k +\n√ q log\nm2 q + 2q\n) ,\nE ‖G‖∗1 ≤ √ 2 log(m1m2) , E ‖G‖∗∗ ≤ √ m1 + √ m2 .\n(22)\nTo derive an upper bound in estimation errors from these inequalities, we consider for simplicity3 the oracle estimate ẐOracleΩ equal to Ẑ λ Ω where λ = σΩ\n∗(G). From Lemma 14 we immediately get the following control of the mean estimation error of the oracle estimator, for any penalty Ω:\nE ∥∥∥ẐOracleΩ − Z⋆ ∥∥∥ 2\nFro ≤ 4σΩ(Z⋆)E Ω∗(G) . (23)\nWe can now derive upper bounds in estimation errors for the different penalties in the so-called single spike model, where the signal Z⋆ consists of an atom ab⊤ ∈ Ak,q, and we observed a noisy matrix Y = ab⊤+σG. Since for an atom ab⊤ ∈ Ak,q while ‖ab⊤‖1 ≤ kq/ √ (kq) = √ kq, Ωk,q(ab\n⊤) = ‖ab⊤‖∗ = 1, we immediately get the following by plugging the upper bounds of Proposition 15 into (23):\nCorollary 16 When Z⋆ ∈ Ak,q is an atom, the expected errors of the oracle estimators using respectively the (k, q)-trace norm, the ℓ1 norm and the trace norm are respectively upper bounded by:\nE ∥∥∥ẐOracleΩk,q − Z ⋆ ∥∥∥ 2\nFro ≤ 8 σ\n(√ k log\nm1 k + 2k +\n√ q log\nm2 q + 2q\n) ,\nE ∥∥∥ẐOracle1 − Z⋆ ∥∥∥ 2\nFro ≤ 2σ‖Z⋆‖1\n√ 2 log(m1m2) ≤ 2σ √ 2kq log(m1m2) ,\nE ∥∥∥ẐOracle∗ − Z⋆ ∥∥∥ 2\nFro ≤ 2σ(√m1 + √ m2) .\n(24)\nTo make the comparison easy, orders of magnitudes of these upper bounds are gathered in Table 1 for the case where Z⋆ ∈ Ãk,q, and for the case where m1 = m2 = m and k = q = √ m. In the later\n3Similar bounds could be derived with large probability for the non-oracle estimator by controlling the deviations of Ω∗(G) from its expectation.\ncase, we see in particular that the (k, q)-trace norm has a better rate than the ℓ1 and trace norms, in m 1 4 instead of m 1 2 (up to logarithmic terms). Note that the largest value of ‖Z⋆‖1 is reached\nwhen Z⋆ ∈ Ãk,q and equals √ kq. By contrast, when Z⋆ ∈ Ak,q gets far from Ãk,q elements then the expected error norm diminishes for the ℓ1-penalized denoiser Ẑ Oracle 1 reaching σ √ 2 log(m1m2) on e1e ⊤ 1 while not changing for the two other norms.\nObviously the comparison of upper bounds is not enough to conclude to the superiority of (k, q)-trace norm and, admittedly, the problem of denoising considered here is a special instance of linear regression in which the design matrix is the identity, and, since this is a case in which the design is trivially incoherent, it is possible to obtain fast rates for decomposable norms such as the ℓ1 or trace norm (Negahban et al., 2012); however, slow rates are still valid in the presence of an incoherent design, or when the signal to recover is only weakly sparse, which is not the case for the fast rates. Moreover, the result proved here is valid for matrices of rank greater than 1. We present in the next section more involved results, based on lower and upper bounds on the so-called statistical dimension of the different norms (Amelunxen et al., 2013), a measure which is closely related to Gaussian widths."
    }, {
      "heading" : "4.2 Performance through the statistical dimension",
      "text" : "Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al. (2012); Foygel and Mackey (2014); Oymak et al. (2013) to quantify the statistical power of a convex nonsmooth regularizer used as a constraint or penalty. These results rely essentially on the fact that if the tangent cone4 of the regularizer at a point of interest Z is thiner, then the regularizer is more efficient at solving problems of denoising, demixing and compressed sensing of Z. The gain in efficiency can be quantified by appropriate measures of width of the tangent cone such as the Gaussian width of its intersection with a unit Euclidean ball (Chandrasekaran et al., 2012), or the closely related concept of statistical dimension of the cone, proposed by Amelunxen et al. (2013). In this section, we study the statistical dimensions induced by different matrix norms in order to compare their theoretical properties for exact or approximate recovery of sparse low-rank matrices. In particular, we will consider the norms Ωk,q, Ω̃k,q and linear combinations of the ℓ1 and trace norms, which have been used in the literature to infer sparse lowrank matrices (Oymak et al., 2012; Richard et al., 2012). For convenience we therefore introduce the notation Γµ for the norm that linearly interpolates between the trace norm and the (scaled) ℓ1 norm:\n∀µ ∈ [0, 1], ∀Z ∈ Rm1×m2 , Γµ(Z) := µ√ kq ‖Z‖1 + (1− µ) ‖Z‖∗ , (25)\nso that Γ0 is the trace norm and Γ1 is the ℓ1 norm up to a constant 5.\n4As detailed later, the tangent cone is the closure of the cone of descent directions. 5Note that the scaling ensures that Γµ(A) = 1 for µ ∈ [0, 1] and A ∈ Ãk,q ."
    }, {
      "heading" : "4.2.1 The statistical dimension and its properties",
      "text" : "Let us first briefly recall what the statistical dimension of a convex regularizer Ω : Rm1×m2 → R refers to, and how it is related to efficiency of the regularizer to recover a matrix Z ∈ Rm1×m2 . For that purpose, we first define the tangent cone TΩ(Z) of Ω at Z as the closure of the cone of descent directions, i.e.,\nTΩ(Z) := ⋃\nτ>0\n{H ∈ Rm1×m2 : Ω(Z + τH) ≤ Ω(Z)} . (26)\nThe statistical dimension S(Z,Ω) of Ω at Z can then be formally defined as\nS(Z,Ω) :=E [∥∥ΠTΩ(Z)(G) ∥∥2 Fro ] , (27)\nwhere G is a random matrix with i.i.d. standard normal entries and ΠTΩ(Z)(G) is the orthogonal projection of G onto the cone TΩ(Z). The statistical dimension is a powerful tool to quantify the statistical performance of a regularizer in various contexts, as the following non-exhaustive list of results shows.\n• Exact recovery with random measurements. Suppose we observe y = X (Z⋆) where X : Rm1×m2 → Rn is a random linear map represented by random design matrices Xi i = 1, . . . , n having iid entries drawn from N (0, 1/n). Then Chandrasekaran et al. (2012, Corollary 3.3) shows that\nẐ = argmin Z\nΩ(Z) s.th. X (Z) = y (28)\nis equal to Z⋆ with overwhelming probability as soon as n ≥ S(Z⋆,Ω). In addition Amelunxen et al. (2013, Theorem II) show that a phase transition occurs at n = S(Z⋆,Ω) between a situation where recovery fails with large probability (for n ≤ S(Z⋆,Ω)− γ√m1m2, for some γ > 0) to a situation where recovery works with large probability (for n ≥ S(Z⋆,Ω) + γ√m1m2).\n• Robust recovery with random measurements. Suppose we observe y = X (Z⋆) + ǫ where X is again a random linear map, and in addition the observation is corrupted by a random noise ǫ ∈ Rn. If the noise is bounded as ‖ǫ‖2 ≤ δ, then Chandrasekaran et al. (2012, Corollary 3.3) show that\nẐ = argmin Z\nΩ(Z) s.th. ‖X (Z)− y‖2 ≤ δ (29)\nsatisfies ∥∥∥Ẑ − Z⋆ ∥∥∥ Fro\n≤ 2δ/η with overwhelming probability as soon as n ≥ (S(Z⋆,Ω) + 3 2)/(1 − η)2.\n• Denoising. Assume a collection of noisy observations Xi = Z⋆ + σǫi for i = 1, · · · , n is available where ǫi ∈ Rm1×m2 has i.i.d. N (0, 1) entries, and let Y = 1n ∑n i=1 Xi denote their\naverage. Chandrasekaran and Jordan (2013, Proposition 4) prove that\nẐ = argmin Z\n‖Z − Y ‖Fro s.th. Ω(Z) ≤ Ω(Z⋆) (30)\nsatisfies E ∥∥∥Ẑ − Z⋆ ∥∥∥ 2\nFro ≤ σ2n S(Z⋆,Ω).\n• Demixing. Given two matrices Z⋆, V ⋆ ∈ Rm1×m2 , suppose we observe y = U(Z⋆) + V ⋆ where U : Rm1×m2 7→ Rm1×m2 is a random orthogonal operator. Given two convex functions Γ,Ω : Rm1×m2 → R, Amelunxen et al. (2013, Theorem III) show that\n(Ẑ, V̂ ) = argmin (Z,V )\nΩ(Z) s.th. Γ(V ) ≤ Γ(V ⋆) and y = U(Z) + V\nis equal to (Z⋆, V ⋆) with probability at least 1− η provided that\nS(Z⋆,Ω) +S(V ⋆,Γ) ≤ m1m2 − 4 √ m1m2 log 4\nη .\nConversely if S(Z⋆,Ω) +S(V ⋆,Γ) ≥ m1m2 + 4 √ m1m2 log 4 η , the demixing fails with probability at least 1− η."
    }, {
      "heading" : "4.2.2 Some cone inclusions and their consequences",
      "text" : "In this and subsequent sections, we wish to compare the behavior of Ωk,q and Ω̃k,q and Γµ, as defined in (25). Before estimating and comparing the statistical dimensions of these norms, which requires rather technical proofs, let us first show through simple geometric arguments that for a number of matrices, the tangent cones of the different norms are actually nested. This will allow us to derive deterministic improvement in performance when a norm is used as regularizer instead of another, which should be contrasted with the kind of guarantees that will be derived from bounds on the statistical dimension and which are typically statements holding with very high probability. The results in this section are proved in Appendix C.\nProposition 17 The norms considered satisfy the following equalities and inequalities:\n∀µ ∈ [0, 1], ∀Z ∈ Rm1×m2 , Γµ(Z) ≤ Ωk,q(Z) ≤ Ω̃k,q(Z), ∀µ ∈ [0, 1], ∀A ∈ Ãk,q, Γµ(A) = Ωk,q(A) = Ω̃k,q(A) = 1.\nPut informally, the unit balls of Ω̃k,q, Ωk,q and of all convex combinations of the trace norm and the scaled ℓ1-norm are nested and meet for matrices in Ãk,q. This property is illustrated in the vector case (for µ = 1) on Figure 1. In fact Ãk,q is a subset of the extreme points of the unit norms of all those norms except for the scaled ℓ1-norm (corresponding to the case µ = 1). Given that the unit balls meet on Ãk,q and are nested, their tangent cones on Ãk,q must also be nested: Corollary 18 The following nested inclusions of tangent cones hold:\n∀µ ∈ [0, 1], ∀A ∈ Ãk,q, TΓµ(A) ⊃ TΩk,q (A) ⊃ TΩ̃k,q(A) . (31)"
    }, {
      "heading" : "As a consequence, for any A ∈ Ãk,q, the statistical dimensions of the different norms satisfy:",
      "text" : "S(A, Ω̃k,q) ≤ S(A,Ωk,q) ≤ S(A,Γµ) . (32) As reviewed in Section 4.2.1, statistical dimensions provide estimates for the performance of the different norms in different contexts. Plugging (32) in these results shows that to estimate an atom in Ãk,q, using Ω̃k,q is at least as good as using Ωk,q which itself is at least as good as using any convex combination of the ℓ1 and trace norms. Note that the various statements in Section 4.2.1 provide upper bounds on the performance of the different norms, with are guarantees that are either probabilistic or hold in expectation. In fact, the inclusion of the tangent cones (31) and a fortiori the tangential inclusion of the unit balls imply much stronger results since it can also lead some deterministic statements, such as the following:\nCorollary 19 (Improvement in exact recovery) Consider the problem of exact recovery of a matrix Z∗ ∈ Ãk,q from random measurements y = X (Z∗) by solving (28) with the different norms. For any realization of the random measurements, exact recovery with Γµ for any 0 ≤ µ ≤ 1 implies exact recovery with Ωk,q which itself implies exact recovery with Ω̃k,q.\nNote that in the vector case (m2 = 1), where the (k, q)-trace norm Ωk,1 boils down to the k-support norm θk, the tangent cone inclusion (31) is not always strict:\nProposition 20 For any a ∈ Ãmk , TΓ1(a) = Tθk(a). In words, the tangent cone of the ℓ1 norm and of the the k-support norm are equal on k-sparse vectors with constant non-zero entries, which can be observed in Figure 1. This suggests that, in the vector case, the k-support norm is not better than the ℓ1 norm to recover such constant sparse k-vectors."
    }, {
      "heading" : "4.2.3 Bounds on the statistical dimensions",
      "text" : "The results presented in Section 4.2.2 apply only to a very specific set of matrices (Ãk,q), and do not characterize quantitatively the relative performance of the different norms. In this Section, we turn to more explicit estimations of the statistical dimension of the different norms at atoms in Ãk,q and Ak,q. We consider first the statistical dimension of the (k, q)-CUT norm Ω̃k,q on its atoms Ãk,q. The unit ball of Ω̃k,q is a vertex-transitive polytope with 2 k+q (m1\nk )(m2 q ) vertices. As a consequence, it\nfollows immediately from Corollary 3.14 in Chandrasekaran et al. (2012) and from the upper bound log (m k ) ≤ k(1 + log(m/k)), that6\nProposition 21 For any A ∈ Ãk,q, we have\nS(A, Ω̃k,q) ≤ 16(k + q) + 9 ( k log\nm1 k + q log m2 q\n) .\nUpper bounding the statistical dimension of the (k, q)-trace norm on its atoms Ak,q requires more work. First, atoms with very small coefficients are likely to be more difficult to estimate than atoms with large coefficients only. In the vector case, for example, it is known that the recovery of a sparse vector β with support I0 depends on its smallest coefficient βmin = mini∈I0 β 2 i (Wainwright, 2009). The ratio between βmin and the noise level can be thought of as the worst signal-to-noise ratio for the signal β. We generalize this idea to atoms in Ak,q as follows. Definition 22 (Atom strength) Let A = ab⊤ ∈ Ak,q with I0 = supp(a) and J0 = supp(b). Denote a2min = mini∈I0 a 2 i and b 2 min = minj∈J0 b 2 j . The atom strength γ(a, b) ∈ (0, 1] is\nγ(a, b) := (k a2min) ∧ (q b2min).\nNote that the atoms with maximal strength value 1 are the elements of Ãk,q. With this notion in hand we can now formulate an upper bound on the statistical dimension of Ωk,q:\nProposition 23 For A = ab⊤ ∈ Ak,q with strength γ = γ(a, b), we have\nS(A,Ωk,q) ≤ 322\nγ2 (k + q + 1) +\n160\nγ (k ∨ q) log (m1 ∨m2) . (33)\n6This result is actually stated informally for the special case of k = q = √ m = with m = m1 = m2 in the context\nof a discussion of the planted clique problem in Chandrasekaran and Jordan (2013).\nNote that the upper bounds obtained on atoms of Ãk,q for Ω̃k,q (Proposition 21) and Ωk,q (Proposition 23, with γ = 1) have the same rate up to k log k + q log q which is negligible compared to k logm1 + q logm2 when k ≪ m1 and q ≪ m2. Note that once the support is specified, the number of degrees of freedom for elements of Ãk,q is k + q − 1, which is matched up to logarithmic terms. It is interesting to compare these estimates to the statistical dimension of the ℓ1 norm, the trace norm, and their combinations Γµ. Table 2 summarizes the main results. The statistical dimension the ℓ1 norm on atoms in Ãk,q is of order kq log(m1m2/(kq)), which is worse than the statistical dimensions of Ωk,q and Ω̃k,q by a factor k ∧ q. On Ak,q, though, the statistical dimension of Ωk,q increases when the atom strength decreases, while the statistical dimension of the ℓ1 norm is independent of it and even decreases when the size of the support decreases. As for the trace norm alone, its statistical dimension is at least of order m1 + m2, which is unsurprisingly much worse that the statistical dimensions of Ωk,q and Ω̃k,q since it does not exploit the sparsity of the atoms. Finally, regarding the combination Γµ of the ℓ1 norm and of the trace norm, Oymak et al. (2012) has shown that it does not improve rates up to constants over the best of the two norms. More precisely, we can derive from Oymak et al. (2012, Theorem 3.2) the following result\nProposition 24 There exists M > 0 and C > 0 such that for any m1,m2, k, q ≥ M with m1/k ≥ M and m2/q ≥ M , for any A ∈ Ak,q and for any µ ∈ [0, 1], the following holds:\nS (A,Γµ) ≥ C ζ(a, b) ( (kq) ∧ (m1 +m2 − 1) ) − 2 ,\nwith\nζ(a, b) = 1− ( 1− ‖a‖ 2 1\nk\n)( 1− ‖b‖ 2 1\nq\n) .\nNote that ζ(a, b) ≤ 1 with equality if either a ∈ Ãm1k or b ∈ Ãm2q , so in particular ζ(a, b) = 1 for ab⊤ ∈ Ãk,q. In that case, we see that, as stated by Oymak et al. (2012), Γµ does not bring any improvement over the ℓ1 and trace norms taken imdividually, and in particular has a worse statistical dimension than Ωk,q and Ω̃k,q."
    }, {
      "heading" : "4.2.4 The vector case",
      "text" : "We have seen in Section 4.2.3 that the statistical dimension of the (k, q)-trace norm and of the (k, q)-CUT norm were smaller than that of the ℓ1 and the trace norms, and of their combinations,\nmeaning that theoretically they are more efficient regularizers to recover rank-one sparse matrices. In this section, we look more precisely at these properties in the vector case (m2 = q = 1), and show that, surprisingly, the benefits are lost in this case. Remember that, in the vector case, Ωk,q boils down to the k-support norm θk (14), while Ω̃k,q boils down to the norm κk (13). For the later, we can upper bound the statistical dimension at a k-sparse vector by specializing Proposition 21 to the vector case, and also derive a specific lower bound as follows:\nProposition 25 For any k-sparse vector a ∈ Ãpk,\nk\n2π log ( p− k k + 1 ) ≤ S(a, κk) ≤ 9k log p k + 16(k + 1) .\nFrom the explicit formulation of θk (14) we can derive an upper bound of the statistical dimension of θk on any sparse vector with at least k non-zero coefficients:\nProposition 26 For any s ≥ k, the statistical dimension of the k-support norm θk at an s-sparse vector w ∈ Rp is bounded by\nS(w, θk) ≤ 5\n4 s+ 2\n{ (r + 1)2 ‖w̃I2‖22\n‖w̃I1‖21 + |I1|\n} log p\ns , (34)\nwhere w̃ ∈ Rp denotes the vector with the same entries as w sorted by decreasing absolute values, r is as defined in equation (14), I2 = [1, k − r − 1] and I1 = [k − r, s]. In particular, when s = k, the following holds for any atom a ∈ Apk with strength γ = ka2min:\nS(a, θk) ≤ 5\n4 k +\n2k\nγ log\np k . (35)\nWe note that (35) has the same rate but tighter constants than the general upper bound (33) specialized to the vector case. In particular, this suggests that the γ−2 term in (35) may not be required. In the lasso case (k = 1), we recover the standard bound (Chandrasekaran et al., 2012):\nS(w, θk) ≤ 5\n4 s+ 2s log\np s , (36)\nwhich is also reached by θk on an atom a ∈ Ãpk because in that case γ = 1 in (35). On the other hand, for general atoms in Apk the upper bound (35) is always worse than the upper bound for the standard Lasso (36), and more generally the upper bound for general sparse vectors (34) is also never better than the one for the Lasso. Although these are only upper bounds, this raises questions on the utility of the k-support norm compared to the lasso to recover sparse vectors. The statistical complexities of the different regularizers in the vector case are summarized in Table 2. We note that, contrary to the low-rank sparse matrix case, the ℓ1-norm, the k-support norm, and the norm κk all have the same statistical dimension up to constants. Note that the tangent cone of the elastic net equals the tangent cone of the ℓ1-norm in any point (because the tangent cone of the ℓ2 norm is a half space that always contains the tangent cone of the ℓ1-norm) so that the elastic net has always the exact same statistical dimension as the ℓ1-norm."
    }, {
      "heading" : "5 Algorithms",
      "text" : "As seen in Section 3, many problems involving sparse low-rank matrix estimation can be formulated as optimization problems of the form:\nmin Z∈Rm1×m2\nL(Z) + λΩk,q(Z). (37)\nUnfortunately, although convex, this problem may be computationally challenging (Section 3.5). In this section, we present a working set algorithm to approximately solve such problems in practice when L is differentiable."
    }, {
      "heading" : "5.1 A working set algorithm",
      "text" : "Given a set S ⊂ Gm1k × Gm2q of pairs of row and column subsets, let us consider the optimization problem:\nmin (Z(IJ))\n(I,J)∈S\n  L ( ∑\n(I,J)∈S Z(IJ)\n) + λ ∑\n(I,J)∈S\n∥∥Z(IJ) ∥∥ ∗ : ∀(I, J) ∈ S, supp(Z (IJ)) ⊂ I × J    . (PS)\nLet (Ẑ(IJ))(I,J)∈S be a solution of this optimization problem. Then, by the characterization of Ωk,q(Z) in (8), Z = ∑ (I,J)∈S Ẑ (IJ) is the solution of (37) when S = Gm1k × Gm2q . Clearly, it is still the solution of (37) if S is reduced to the set of non-zero matrices Ẑ(IJ) at optimality often called active components. We propose to solve problem (37) using a so-called working set algorithm which solves a sequence of problems of the form (PS) for a growing sequence of working sets S, so as to keep a small number of non-zero matrices Z(IJ) throughout. Working set algorithms (Bach et al., 2011, Chap. 6) are typically useful to speed up algorithm for sparsity inducing regularizer; they have been used notably in the case of the overlapping group Lasso of Jacob et al. (2009) which is also naturally formulated via latent components. To derive the algorithm we write the optimality condition for (PS):\n∀(I, J) ∈ S , ∇L(Z)IJ ∈ −λ∂ ∥∥∥Z(IJ) ∥∥∥ ∗ .\nFrom the characterization of the subdifferential of the trace norm (Watson, 1992), writing Z(IJ) = U (IJ)Σ(IJ)V (IJ) the SVD of Z(IJ), this is equivalent to, for all (I, J) in S,\neither Z(IJ) 6=0 and ∇L(Z)IJ = −λ ( U (IJ)V (IJ) ⊤ +A )\nwith ‖A‖op ≤ 1 and AU (IJ) = A⊤V (IJ) = 0 , (38) or Z(IJ)=0 and ‖∇L(Z)]IJ‖op ≤ λ . (39)\nThe principle of the working set algorithm is to solve problem (PS) for the current set S so that (38) and (39) are (approximately) satisfied for (I, J) in S, and to check subsequently if there are any components not in S which violate (39). If not, this guarantees that we have found a solution to problem (37), otherwise the new pair (I, J) corresponding to the most violated constraint is added to S and problem (PS) is initialized with the previous solution and solved again. The resulting algorithm is Algorithm 1 (where the routine SSVDTPI is described in the next section). Problem (PS) is solved easily using the approximate block coordinate descent of Tseng and Yun (2009) (see\nalso Bach et al., 2011, Chap. 4), which consists in iterating proximal operators. The modifications to the algorithm to solve problems regularized by the norm Ωk, are relatively minor (they amount to replace the trace norms by penalization of the trace of the matrices Z(IJ) and by positive definite cone constraints) and we therefore do not describe them here. Determining efficiently which pair (I, J) possibly violates condition (39) is in contrast a more difficult problem that we discuss next.\nAlgorithm 1 Active set algorithm Require: L, tolerance ǫ > 0, parameters λ, k, q Set S = ∅, Z = 0 while c = true do\nRecompute optimal values of Z, (Z(IJ))(I,J)∈S for (PS) using warm start (I, J) ← SSVDTPI(∇L(Z), k, q, ǫ) if ‖[∇L(Z)]I,J‖op > λ then S ← S ∪ {(I, J)} else c ← false\nend if end while return Z, S, (Z(IJ))(I,J)∈S"
    }, {
      "heading" : "5.2 Finding new active components",
      "text" : "Once (PS) is solved for a given set S, (38) and (39) are satisfied for all (I, J) ∈ S. Note that (38) implies in particular that ‖∇L(Z)]IJ‖op = λ when Z(IJ) 6= 0 at optimality. Therefore, (39) is also satisfied for all (I, J) /∈ S if and only if\nmax (I,J)∈Gm1\nk ×Gm2q\n‖[∇L(Z)]IJ‖op ≤ λ , (40)\nand if this is not the case then any (I, J) that violates this condition is a candidate to be included in S. This corresponds to solving the following sparse singular value problem\nmax a,b\na⊤∇L(Z)b s.t. ab⊤ ∈ Ak,q . (k, q)-linRank-1\nThis problem is unfortunately NP-hard since rank 1 sparse PCA problem is a particular instance of it (when ∇L(Z) is replaced by a covariance matrix), and we therefore cannot hope to solve it exactly with efficient algorithms. Still, sparse PCA has been the object of a significant amount of research, and several relaxations and other heuristics have been proposed to solve it approximately. In our numerical experiments we use a truncated power iteration (TPI) method, also called TPower, GPower or CongradU in the PSD case (Journée et al., 2010; Luss and Teboulle, 2013; Yuan and Zhang, 2013), which has been proved recently by Yuan and Zhang (2013) to provide accurate solution in reasonable computational time under RIP type of conditions. Algorithm 2 provides a natural generalization of this algorithm to the non-PSD case. The algorithm follows the steps of a power method, the standard method for computing leading singular vectors of a matrix, with the difference that at each iteration a truncation step is use. We denote the truncation operator by Tk. It consists of keeping the k largest components (in absolute value) and setting the others to 0. Note that Algorithm 2 may fail to find a new active component for Algorithm 1 if it\nAlgorithm 2 SSVDTPI: Bi-truncated power iteration for (k, q)-linRank-1 Require: A ∈ Rm1×m2 , k, q and tolerance ǫ > 0 Pick a random initial point b(0) ∼ N (0, Im2) and let while |a(t)⊤Ab(t) − a(t−1)⊤Ab(t−1)|/|a(t−1) ⊤Ab(t−1)| > ǫ do\na ← Ab(t) \\\\ Power a ← Tk(a) \\\\ Truncate b ← A⊤a \\\\ Power b ← Tq(b) \\\\ Truncate a(t+1) ← a/‖a‖2 and b(t+1) ← b/‖b‖2 \\\\ Normalize t ← t+ 1\nend while I ← Supp(a(t)) and J ← Supp(b(t)) return (I, J)\nfinds a local maximum of ((k, q)-linRank-1) smaller than λ, and therefore result in the termination of Algorithm 1 on a suboptimal solution. On the positive side, note that Algorithm 1 is robust to some errors of Algorithm 2. For instance, if an incorrect component is added to S at some iteration, but the correct components are identified later, Algorithm 1 will eventually shrink the incorrect components to 0. One of the causes of failure of TPI type of methods is the presence of a large local maximum in the sparse PCA problem corresponding to a suboptimal component; incorporating this component in S will reduce the size of that local maximum, thereby increasing the chance of selecting a correct component the next time around."
    }, {
      "heading" : "5.3 Computational cost",
      "text" : "Note that when m1,m2 are large, solving PS involves the minimizations of trace norms of matrices of size k × q which, when k and q are small compared to m1 and m2 have low computational cost. The bottleneck for providing a computational complexity of the algorithm is the (k, q)-linRank-1 step. It has been proved by Yuan and Zhang (2013) that under some conditions the problem can be solved in linear time. If the conditions hold at every step of gradient, the overall cost of an iteration can be cast into the cost of evaluating the gradient and the evaluation of thin SVDs: O(k2q). Evaluating the gradient has a cost dependent on the risk function L. This cost for usual applications is O(m1m2). So assuming the RIP conditions required by Yuan and Zhang (2013) hold, the cost of Algorithm 2 is dominated by matrix-vector multiplications so of the order O(m1m2). The total cost of the algorithm for reaching a δ-accurate solution is therefore O((m1m2 + k\n2q)/δ). However the worst case complexity of the algorithm is non-polynomial as (k, q)-linRank-1 is nonpolynomial in general. We would like to point out that in our numerical experiments a warm start with singular vectors and multiple runs of the algorithm (k, q)-linRank-1 keeping track of the highest found variance has provided us a very fast and reliable solver. Further discussion on this step go beyond the scope of this work."
    }, {
      "heading" : "6 Numerical experiments",
      "text" : "In this section we report experimental results to assess the performance of sparse low-rank matrix estimation using different techniques. We start in Section 6.1 with simulations aiming at validating the theoretical results on statistical dimension of Ωk,q and assessing how they generalize to matrices\nwith (k, q)-rank larger than 1. In Section 6.2 we compare several techniques for sparse PCA on simulated data."
    }, {
      "heading" : "6.1 Empirical estimates of the statistical dimension.",
      "text" : "In order to numerically estimate the statistical dimension S(Z,Ω) of a regularizer Ω at a matrix Z, we add to Z a random Gaussian noise matrix and observe Y = Z + σG where G has normal i.i.d. entries following N (0, 1). We then denoise Y using (30) to form an estimate Ẑ of Z. For small σ, the normalized mean-squared error (NMSE) defined as\nNMSE(σ) := E\n∥∥∥Ẑ − Z ∥∥∥ 2\nFro\nσ2\nis a good estimate of the statistical dimension, since Oymak and Hassibi (2013) show that\nS(Z,Ω) = lim σ→0 NMSE(σ) .\nNumerically, we therefore estimate S(Z,Ω) by taking σ = 10−4 and measuring the empirical NMSE averaged over 20 repeats. We consider square matrices with m1 = m2 = 1000, and estimate the statistical dimension of Ωk,q, the ℓ1 and the trace norms at different matrices Z. The constrained denoiser (30) has a simple close-form for the ℓ1 and the trace norm. For Ωk,q, it can be obtained by a series of proximal projections (15) with different parameters λ until Ωk,q(Ẑ) has the correct value Ωk,q(Z). Since the noise is small, we found that it was sufficient and faster to perform a soft-(k, q)-SVD of Y by solving (15) with a small λ, and then apply the ℓ1 constrained denoiser to the set of soft-(k, q)-sparse singular values. We first estimate the statistical dimensions of the three norms at an atom Z ∈ Ãk,q, for different values of k = q. Figure 2 (top left) shows the results, which confirm the theoretical bounds summarized in Table 2. The statistical dimension of the trace norm does not depend on k, while that of the ℓ1 norm increases almost quadratically with k and that of Ωk,q increases linearly with k. As expected, Ωk,q interpolates between the ℓ1 norm (for k = 1) and the trace norm (for k = m1), and outperforms both norms for intermediary values of k. This experiments therefore confirms that our upper bound (33) on S(Z,Ωk,q) captures the correct order in k, although the constants can certainly be much improved, and that Algorithm 1 manages, in this simple setting, to correctly approximate the solution of the convex minimization problem. Second, we estimate the statistical dimension of Ωk,q on matrices with (k, q)-rank larger than 1, a setting for which we proved no theoretical result. Figure 2 (top left) shows the numerical estimate of S(Z,Ωk,q) for matrices Z which are sums of r atoms in Ãk,k with non-overlapping support, for k = 10 and varying r. We observe that the increase in statistical dimension is roughly linear in the (k, q)-rank. For a fixed (k, q)-rank of 3, the bottom plots of Figure 2 compare the estimated statistical dimensions of the three regularizers on matrices Z which are sums of 3 atoms in Ãk,k with non-overlapping (bottom left) or overlapping (bottom right) supports. The shapes of the different curves are overall similar to the rank 1 case, although the performance of Ωk,q degrades as the supports of atoms overlap. In both cases, Ωk,q consistently outperforms the two other norms. Overall these experiments suggest that the statistical dimension of Ωk,q at a linear combination of r atoms increases as Cr (k logm1 + q logm2) where the coefficient C increases with the overlap among the supports of the atoms."
    }, {
      "heading" : "6.2 Comparison of algorithms for sparse PCA",
      "text" : "In this section we compare the performance of different algorithms in estimating a sparsely factored covariance matrix that we denote Σ⋆. The observed sample consists of n random vector vectors generated i.i.d. according to N (0,Σ⋆+σ2Idp), where (k, k)-rank(Σ⋆) = 3. The matrix Σ⋆ is formed by adding 3 blocks of rank 1, Σ⋆ = a1a ⊤ 1 +a2a ⊤ 2 +a3a ⊤ 3 , having all the same sparsity ‖ai‖0 = k = 10,\n3×3 overlaps and nonzero entries equal to 1/ √ k. See Figure 3, bottom right plot for a representation of the ground truth Σ⋆. The noise level σ = 0.8 is set in order to make the signal to noise ratio below the level σ = 1 where a spectral gap appears and makes the spectral baseline (penalizing the trace of the PSD matrix) work. In our experiments the number of variables is p = 200 and n = 80 points are observed. To estimate the true covariance matrix from the noisy observation, first the sample covariance matrix is formed as\nΣ̂n = 1\nn\nn∑\ni=1\nxix ⊤ i ,\nand given as input to various algorithms which provide a new estimate Σ̂. The methods we compared are the following:\n• Raw sample covariance. The most basic is to output Σ̂n as the estimate of the covariance, which is not accurate due to presence of noise and underdeterminedness n < p.\n• Trace penalty on the PSD cone. This spectral algorithm solves the following optimization problem in the cone of PSD matrices:\nmin Z 0\n1\n2\n∥∥∥Z − Σ̂n ∥∥∥ 2\nFro + λTrZ .\n• ℓ1 penalty. In order to approximate the sample covariance Σ̂n by a sparse matrix a basic idea is to soft-threshold it element-by-element. This is equivalent to solving the following convex optimization problem:\nmin Z\n1\n2\n∥∥∥Z − Σ̂n ∥∥∥ 2\nFro + λ‖Z‖1 .\n• Trace + ℓ1 penalty. The restriction of Γµ to the PSD cone, which is equivalent to solving the following SDP\nmin Z 0\n1\n2\n∥∥∥Z − Σ̂n ∥∥∥ 2\nFro + λΓµ(Z) .\nThis approach needs to tune two parameters λ > 0, µ ∈ [0, 1].\n• Sequential sparse PCA. This is the standard way of estimating multiple sparse principal components which consists of solving the problem for a single component at each step t = 1 · · · r, and deflate to switch to the next (t+ 1)st component. The deflation step used in this algorithm is the orthogonal projection\nZt+1 = (Idp − utu⊤t )Zt (Idp − utu⊤t ) .\nThe tuning parameters for this approach are the sparsity level k and the number of principal components r.\nSample covariance Trace ℓ1 Trace + ℓ1 Sequential Ωk, 4.20 ± 0.02 0.98 ± 0.01 2.07 ± 0.01 0.96 ± 0.01 0.93 ± 0.08 0.59 ± 0.03\nWe report the relative errors ∥∥∥Σ̂− Σ⋆ ∥∥∥ Fro\n/ ‖Σ⋆‖Fro over 10 runs of our experiments in Table 3, and a representation of the estimated matrices can be found in Figure 3. We observe that sparse PCA methods using Ωk, and also the sequential method using deflation steps outperform spectral and ℓ1 baselines. In addition, penalizing Ωk, is superior to the sequential approach. This was expected since our algorithm minimizes a loss function that is close to the test errors reported, whereas the sequential scheme does not optimize a well-defined objective."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we proposed two new convex penalties, the (k, q)-trace norm and the (k, q)-CUT norm, specifically tailored to the estimation of low-rank matrices with sparse factors. Our motivation for proposing such convex formulations for sparse low-rank matrix inference was twofold. First, it allowed us to consider algorithmic schemes that are better understood when a problem is formulated as a convex optimization problem, even though the complexity of solving the problem exactly remains super-polynomial. Second, using convex geometry allowed us to provide sample complexity and statistical guarantees, and notably to show that the proposed estimators have much better statistical dimension than more standard convex combinations of the ℓ1 and trace norms. We observed that the improvement exists only for matrices: for sparse vectors, using our penalty (which boils down to the k-support norm in this case) does not improve over the standard ℓ1 norm, in terms of statistical dimension increase rate. One limitation of this work is that we assume that the sparsity of the factors is known and fixed. Lifting this constraint and investigating procedures that can adapt to the size of the blocks (like the ℓ1 norm adapts to the size of the support) is an interesting direction for future research. Another interesting direction is to use the nuclear norm formulation of the (k, q)-trace norm as in Lemma 10 to optimize the regularized problem."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Francis Bach for interesting discussions related to this work. This work was supported by the European Research Council (SMAC-ERC-280032)."
    }, {
      "heading" : "A Proofs of results in Sections 2 and 3.",
      "text" : "Proof [Proposition 2] To prove the first claim, note that a matrix of the form ab⊤ for a ∈ Am1k and b ∈ Am2q has at most kq non-zero terms. Therefore, the decomposition of a matrix with no null entries as a linear combination of such sparse matrices must count at least m1m2kq terms, which is larger than m1 ∨m2 when kq ≤ m1 ∧m2. To prove second claim, consider the case of the (2, 2)-SVD for the matrix Z = 11⊤ ∈ R3. It is impossible to write Z as the sum of two (2, 2)-sparse matrices, because it would then have at most 8 non-zero coefficients. But we have the decomposition.\n( 1 1 1 1 1 1 1 1 1 ) = ( 2 1 0 1 1 2 0 0 0 0 ) + ( 0 0 1 0 1 2 1 0 1 2 ) − ( 1 0 −1 0 0 0 −1 0 1 ) ,\nwhich shows that the (2, 2)-rank of Z is 3. Now, given that Z is invariant by any of the 6 permutations of the rows and any of the 6 permutations of the columns, Z admits at least 36 different (2, 2)-SVDs. To prove the third claim, observe that the decomposition proposed above for Z = 11⊤ ∈ R3 yields 9 left- and right-(2, 2)-sparse singular vectors that are obviously not orthogonal. It can actually be\nshown by systematic enumeration of all possible cases that it is impossible to find any (2, 2)-sparseSVD of Z whose left or right singular vectors are orthogonal.\nProof [Proposition 6] To prove the first claim, let us consider the matrix Z = 11⊤ ∈ R3. We showed in the proof of Proposition 2 above that its (2, 2)-rank is equal to 3. We now show that its soft-(2, 2)-rank is equal to 9. For that purpose, we express any soft-(2, 2)-SVD of Z as a minimizer of (8), and write the corresponding Lagrangian:\nL((Z(IJ))I,J ,K) = ∑\nI,J∈G2\n∥∥Z(IJ) ∥∥ ∗ + tr ( K⊤ ( Z − ∑\nI,J∈G2 Z(IJ)\n)) ,\nwhere (Z(IJ))I,J and K are the primal and dual variables. It is easy to check that the dual solution is the unique subgradient of Ω2,2 at Z which is equal to K\n∗ = 12Z. But any primal solution must satisfy tr(K∗⊤Z(IJ)) = ‖Z(IJ)‖∗. This implies that any primal solution (Z(IJ))I,J satisfies Z(IJ) ∝ 1I1⊤J . Then, one can check that ((12)1I1 ⊤ J )I,J∈G2 forms a basis of R\n3×3 so that any matrix Z admits a unique set of decomposition coefficients on that basis. This proves that the unique solution of (8) is the one such that Z(IJ) = 141I1 ⊤ J for all pairs (I, J) ∈ G2 × G2. This unique soft-(k, q)-SVD is composed of 9 terms, meaning that the soft-(k, q)-rank of Z is 9 while its (k, q)-rank is 3. To prove the second claim, let us consider the soft-(2, 2)-SVDs of Z = 1211\n⊤ ∈ R4. By proposition 17, 1 2‖Z‖1 ≤ Ω2,2(Z), but 12‖Z‖1 = 4 and 2Z = (1{1,2} + 1{3,4})(1{1,2} + 1{3,4})⊤ which shows that Ω2,2(Z) ≤ 4. So Ω2,2(Z) = 4. Considering that there are 3 ways to partition {1, 2, 3, 4} into sets of cardinality 2, Z admits at least 9 different optimal decompositions in the sense of the (2, 2)-softSVD since Z can be written in 9 different ways as the sum of four matrices of Ã2,2 with disjoint supports. Each of these decompositions attains the (2, 2)-rank which is equal to 4. Note also that by convexity any convex combination of these decompositions is also an optimal decomposition in the sense of the soft-(2, 2)-SVD, but can contain up to 36 terms! To prove the third claim, let us consider\nZ1 = (\n1 1 0 1 1 0 0 0 0\n) , Z2 = ( 0 0 0 0 1 1 0 1 1 ) , Z = Z1 + Z2 = ( 1 1 0 1 2 1 0 1 1 ) .\nAs Z1, Z2, Z are all positive semidefinite we have ‖Z1‖∗ = 2, ‖Z2‖∗ = 2, and ‖Z‖∗ = 4. By inequality (7), Ω2,2(Z) ≥ ‖Z‖∗ = 4 which proves that the decomposition Z = Z1 + Z2 is optimal: Ω2,2(Z) = 4. But 〈Z1, Z2〉 = 1. So this decomposition is a decomposition of Z onto linear combination of atoms 12Z1, 1 2Z2 ∈ A2,2 which are not orthogonal.\nProof [Lemma 7] We first show (9) from the definition of the dual norm Ω∗k,q:\nΩ∗k,q(Z) = max K {〈K,Z〉 : Ωk,q(K) ≤ 1}\n= max a,b\n{〈Z, ab⊤〉 : ab⊤ ∈ Ak,q}\n= max a,b\n{a⊤Zb : ‖a‖0 ≤ k , ‖b‖0 ≤ q , ‖a‖2 = ‖b‖2 = 1}\n= max I,J\n{ ‖ZI,J‖op : I ∈ G m1 k , J ∈ Gm2q } ,\nwhere the second equality follows from the fact that the maximization of a linear form over a bounded convex set is attained at one of the extreme points of the set. Given this closed-form expression of the dual norm, we prove the variational formulation (8) for the primal norm Ωk,q. Consider the function ˇΩk,q defined by\nˇΩk,q(Z) = inf   \n∑\n(I,J)∈Gm1 k ×Gm2q\n∥∥∥Z(I,J) ∥∥∥ ∗ : Z = ∑\n(I,J)\nZ(I,J) , supp(Z(I,J)) ⊂ I × J    .\nSince ˇΩk,q(Z) is defined as the infimum of a jointly convex function of Z and (Z (I,J))I∈Gm1\nk , J∈Gm2q\nobtained by minimizing w.r.t. to the latter variables, it is a an elementary fact from convex analysis that ˇΩk,q is a convex function of Z. It is also symmetric and positively homogeneous, which together with convexity prove that ˇΩk,q defines a norm. We can compute its dual norm as\nˇΩk,q ∗ (K) = max\nZ\n{ 〈K,Z〉 : ˇΩk,q(Z) ≤ 1 }\n= max (Z(IJ))(I,J)\n  〈K, ∑\n(I,J)\nZ(IJ)〉 : ∑\n(I,J)\n∥∥Z(IJ) ∥∥ ∗ ≤ 1 , supp(Z (IJ)) ⊂ I × J   \n= max (Z(IJ))(I,J),(η (IJ))(I,J)\n   ∑\n(I,J)\nη(I,J)〈KI,J , Z(IJ)〉 : ∥∥Z(IJ) ∥∥ ∗ ≤ η (IJ), ∑\n(I,J)\nη(IJ) ≤ 1   \n= max (η(IJ))(I,J)\n   ∑\n(I,J)\nη(IJ) ‖KI,J‖op : ∑\n(I,J)\nη(IJ) ≤ 1   \n= max (I,J) ‖KI,J‖op = Ω∗k,q(K) .\nThis proves that Ωk,q(K) = ˇΩk,q(K) since a norm is uniquely characterized by its dual norm. Finally, to show (10) we use the general characterization of the subdifferential of a norm (e.g., Watson, 1992):\nG ∈ ∂Ωk,q(A) ⇔ { Ωk,q(A) = 〈G,A〉 , Ω∗k,q(G) ≤ 1 .\nLet us denote a subgradient by G = A + Z. Since A = ab⊤ is an atom, we have Ωk,q(A) = 1. In addition, ‖A‖2Fro = Tr(ba⊤ab⊤) = 1, therefore the condition Ωk,q(A) = 〈G,A〉 boils down to 〈Z,A〉 = 0. Given the characterization of the dual norm (9), we therefore get:\n∂Ωk,q(A) = { A+ Z : 〈A,Z〉 = 0, ∀(I, J) ∈ Gm1k × Gm2q ‖AI,J + ZI,J‖op ≤ 1 } .\nLet now\nD(A) = { A+ Z : AZ⊤I0,J0 = 0, A ⊤ZI0,J0 = 0, ∀(I, J) ∈ Gm1k × Gm2q ‖AI,J + ZI,J‖op ≤ 1 } .\nSince 〈A,Z〉 = 〈A,ZI0,J0〉 = Tr (A⊤ZI0,J0), it is clear that D(A) ⊂ ∂Ωk,q(A). Conversely, let G = A+Z ∈ ∂Ωk,q(A). Then 〈A,Z〉 = 〈ab⊤, ZI0,J0〉 = a⊤ZI0,J0b = 0, and therefore, by Pythagorean equality applied to the orthogonal vectors a and ZI0,J0b:\n‖(AI0,J0 + ZI0,J0) b‖22 = ‖ab⊤b+ ZI0,J0b‖ 2 2 = ‖a+ ZI0,J0b‖ 2 2 = 1 + ‖ZI0,J0b‖ 2 2 ,\nbut since ‖AI0,J0 + ZI0,J0‖op ≤ 1 and ‖b‖2 = 1 we must have ‖ZI0,J0b‖2 = 0. This shows that AZ⊤I0,J0 = ab ⊤Z⊤I0,J0 = 0. The same reasoning starting with the orthogonal vectors b and Z ⊤ I0,J0\na shows that we also have A⊤ZI0,J0 = 0, implying that ∂Ωk,q(A) ⊂ D(A). This concludes the proof that ∂Ωk,q(A) = D(A), as claimed in (10).\nProof [Lemma 10] Let ν be the nuclear norm induced by two atomic norms ‖·‖α and ‖·‖β , induced themselves respectively by the two atom sets A1 and A2. Let A = { ab⊤ : a ∈ A1 , b ∈ A2 } and B = Conv ( A ) , then the key argument is to note that we have\n{ ab⊤ : ‖a‖α ≤ 1, ‖b‖β ≤ 1 } ⊂ B .\nIndeed, if a = ∑ i λiai and b = ∑ j λ ′ jbj with ai ∈ A1, bj ∈ A2 and ∑ i λi = ∑ j λ ′ j = 1, then with µij := λiλ ′ j, we have ab ⊤ = ∑ i,j µijaib ⊤ j and ∑ i,j µij = 1. The inclusion is then proved by density. By (12) the dual norm of ν satisfies\nν∗(Z) = sup { a⊤Zb : ‖a‖α ≤ 1 , ‖b‖β ≤ 1 } ,\nso that ν∗(Z) ≤ sup {〈Z, ab⊤〉 : ab⊤ ∈ B} = sup {〈Z, ab⊤〉 : ab⊤ ∈ A} ≤ ν∗(Z) ,\nwhere the middle equality is due to the fact that the maximum of a linear function on a convex set is attained at a vertex. We therefore have ν∗(Z) = sup {〈Z,A〉 : A ∈ A}. Given (4), this shows that ν is the atomic norm induced by A.\nProof [Theorem 11] Since the (k, q)-trace norm is the atomic norm induced by the atom set (5), Lemma 10 tells us that it is also the nuclear norm induced by the two atomic norms with atom sets Am1k and Am2q , which correspond exactly to the so-called k- and q-support norms of Argyriou et al. (2012). To prove the second statement, we proceed similarly to get that the (k, q)-CUT norm is the nuclear norm induced by the two atomic norms with atom sets Ãm1k and Ãm2k . Calling κk and κq these norms, we obtain an explicit formulation as follows:\nκk(w) = max s\n{〈s,w〉 : κ∗k(s) ≤ 1}\n= max { 〈s,w〉 : 1√\nk\nk∑\ni=1\n|s(i)| ≤ 1 }\n=    1 k √ k ‖w‖1 if ‖w‖1 ≥ k ‖w‖∞\n1√ k ‖w‖∞ if ‖w‖1 ≤ k ‖w‖∞\n= 1√ k max\n( ‖w‖∞ , 1\nk ‖w‖1\n) .\nProof [Lemma 12]\nThe form of θ∗k follows immediately from the fact that θ ∗ k(w) = max{a⊤w : a ∈ Ak}. Similarly for κ∗k, we have\nκ∗k(s) = max { 〈a, s〉 : a ∈ Ãk } = max\nI:|I|=k ‖sI‖1 = 1√ k\nk∑\ni=1\n|s(i)|,\nwhere s(i) denotes the the ith largest element of s in absolute value. This norm is proportional to a norm known as the vector k-norm or 1-k symmetric norm gauge.\nProof [Proposition 13] To prove the first claim, we show a counterexample for the (2, 2)-SVD in R4×4. Let I = {1, 2} and J = {3, 4}. The matrix Z = 11⊤ ∈ R4 can be written as Z = ZI,I + ZI,J + ZJ,I + ZJ,J , so its (2, 2)-rank is less than 4. However, it is not possible to write it as a sum of less than 6 symmetric (2, 2)-sparse matrices, because each of these matrices can only make one coefficient above the nondiagonal non-zero. Therefore, its (2, 2)-SVD must contain non-symmetric terms. To prove the second claim, note first that the case k = 1 is peculiar and not representative of the general case because the span of the PSD matrices of sparsity 1 are only the diagonal matrices, while the span of rank one PSD matrices of sparsity k × k for k > 1 is all the symmetric matrices. Now, we claim that it is not possible to write Z = 11⊤ ∈ R3 as a sum of PSD matrices that are (2, 2)-sparse and PSD. Indeed, if this was the case, this would imply the existence of a non zero vector v with a support of size at most 2 such that Z − vv⊤ ≻ 0. Since the only eigenvector of Z associated with a non-zero eigenvalue is the constant vector this is impossible."
    }, {
      "heading" : "B Proofs of results in Section 4.1",
      "text" : "Proof [Lemma 14] We prove a more general result than Lemma 14. Let Ω : Rm1×m2 → R be any matrix norm, and X : Rm1×m2 → Rn be a linear map. We denote by Xi (i = 1, . . . , n) the i-th design matrix defined by X (Z)i = 〈Z,Xi〉. For a given matrix Z⋆ ∈ Rm1×m2 , assume we observe:\nY = X (Z⋆) + ǫ , (41)\nwhere ǫ ∈ Rn is a centered random noise vector. We consider the following estimator of Z⋆:\nẐΩ ∈ argmin Z\n1\n2n ‖Y − X (Z)‖22 + λΩ(Z) , (42)\nfor some value of the parameter λ > 0. The following result generalizes standard results known for the ℓ1 and trace norms (e.g., Koltchinskii et al., 2011, Theorem 1) to any norm Ω. Theorem 27 If λ ≥ 1nΩ∗( ∑n i=1 ǫiXi) then\n1\n2n\n∥∥∥X (ẐΩ − Z⋆) ∥∥∥ 2\n2 ≤ inf Z\n{ 1\n2n ‖X (Z − Z⋆)‖22 + 2λΩ(Z)\n} . (43)\nLemma 14 is then a simple consequence of Theorem 27 by taking for X the identity map, upper bounding the right-hand side of (43) by the value 2λΩ(Z⋆) it takes for Z = Z⋆, and replacing λ by λ/n.\nProof [Theorem 27] By definition of ẐΩ (42), we have for all Z:\n1\n2n\n∥∥∥Y − X (ẐΩ) ∥∥∥ 2\n2 ≤ 1 2n ‖Y − X (Z)‖22 + λ\n( Ω(Z)− Ω(ẐΩ) ) ,\nwhich after developing the squared norm and replacing Y by (41) gives\n1\n2n\n∥∥∥X (ẐΩ) ∥∥∥ 2\n2 − 1 n 〈X (Z⋆)+ ǫ,X (ẐΩ)〉 ≤ 1 2n ‖X (Z)‖22 − 1 n 〈X (Z⋆)+ ǫ,X (Z)〉+λ\n( Ω(Z)− Ω(ẐΩ) ) ,\nand therefore\n1\n2n\n∥∥∥X (ẐΩ − Z⋆) ∥∥∥ 2\n2 ≤ 1 2n ‖X (Z − Z⋆)‖22 + 1 n 〈ǫ,X (ẐΩ − Z)〉+ λ\n( Ω(Z)− Ω(ẐΩ) ) . (44)\nNow, using the fact (true for any norm) that Ω(A)Ω⋆(B) ≥ 〈A,B〉 for any vectors A,B ∈ Rn, and taking λ ≥ 1nΩ∗( ∑n i=1 ǫiXi), we can upper bound the second term of the right-hand side of (44) by:\n1 n 〈ǫ,X (ẐΩ − Z)〉 = 1 n\nn∑\ni=1\nǫiX (ẐΩ − Z)i\n= 1\nn\nn∑\ni=1\nǫi〈Xi, ẐΩ − Z〉\n= 1\nn 〈\nn∑\ni=1\nǫiXi, ẐΩ − Z〉\n≤ 1 n Ω⋆\n( n∑\ni=1\nǫiXi ) Ω ( ẐΩ − Z )\n≤ λΩ ( ẐΩ − Z )\nPlugging this bound back in (44) finally gives\n1\n2n\n∥∥∥X (ẐΩ − Z⋆) ∥∥∥ 2\n2 ≤ 1 2n ‖X (Z − Z⋆)‖22 + λΩ(ẐΩ − Z) + λ\n( Ω(Z)− Ω(ẐΩ) )\n≤ 1 2n ‖X (Z − Z⋆)‖22 + 2λΩ(Z) ,\nthe last inequality being due to the triangle inequality.\nBefore proving Propositon 15, let us first derive an intermediary results useful to obtain an upper bound on the dual (k, q)-trace norm of a random matrix with i.i.d. normal entries.\nLemma 28 Let G be a m1 ×m2 random matrix with i.i.d. normally distributed entries. Then\nE max I∈Gk,J∈Gq\n‖GI,J‖2op ≤ 16 [( k log m1 k + q log m2 q ) + 2(k + q) ] .\nProof [Lemma 28] For a random matrix H ∈ Rk×q with i.i.d. standard normal entries, we have the following concentration inequality (e.g., Davidson and Szarek, 2001): for s ≥ 0,\nP[‖H‖op > √ k + √ q + s] ≤ exp(−s2/2) . (45)\nDenoting R = 2 (√ k + √ q ) , and f(x) = etx 2 , we have the sequence of inequalities\nE exp(t ‖H‖2op) = Ef(‖H‖op)\n=\n∫ ∞\n1 P[f(‖H‖op) > h] dh\n≤ ∫ 1+f(R)\n1 1 dh+\n∫ ∞\n1+f(R) P[f(‖H‖op) > h]dh\n= f(R) +\n∫ ∞\n0 P[‖H‖op > f−1(f(R) + 1 + ζ)]dζ\n≤ f(R) + ∫ ∞\n0 P[‖H‖op >\n1 2 R+ 1 2 f−1(1 + ζ)]dζ (46)\n≤ f(R) + ∫ ∞\n0 8ts exp\n( −s2/2 + 4ts2 ) ds (47)\n≤ f(R) + 4 t1 2 − 4t\n(48)\n≤ exp(8t(k + q)) + 8t 1− 8t ,\nwhere the change of variable used in (47) is 1 + ζ = f(2s) = e4ts 2 , (48) is true for any t < 18 , and\n(46) follows from the property of the inverse f−1(z) = √\nlog(z) t that it is strictly increasing on [1;∞)\nand sandwiched via\n1\n2\n{ f−1(z) + f−1(z′) } ≤ f−1(z + z′) ≤ f−1(z) + f−1(z′) . (49)\nTake now t = 18 − 18(k+q) . Since k + q ≥ 2, we have 1/16 ≤ t < 1/8. Therefore,\nEmax I,J\n‖GI,J‖2op = 1\nt log\n{ exp tEmax\nI,J ‖GI,J‖2op\n}\n≤ 1 t log\n{ E exp(tmax\nI,J ‖GI,J‖2op)\n}\n≤ 1 t log\n{∑\nI,J\nE exp(t ‖GI,J‖2op) }\n≤ 1 t log {( m1 k )( m2 q ) E exp(t ‖H‖2op) }\n≤ 1 t log {(e m1 k )k (e m2 q )q ( e8t(k+q) + 8t 1− 8t )}\n= 1\nt\n[( k log\nm1 k + q log m2 q\n) + k + q + 8t(k + q) + log ( 1 + 8t\n1− 8te −8t(k+q)\n)]\n≤ 16 [(\nk log m1 k + q log m2 q\n) + k + q ] + 8(k + q) +\n8\n1− 8te −8t(k+q)\n≤ 16 [(\nk log m1 k + q log m2 q\n) + 2(k + q) ] ,\nwhere in the last inequality we simply used 8/(1 − 8t) = 8(k + q) and exp(−8t(k + q)) ≤ 1.\nProof [Propositon 15] From Lemma 28 we have:\nEΩ∗k,q(G) = E max I∈Gk,J∈Gq ‖GI,J‖op\n≤ ( E max\nI∈Gk,J∈Gq ‖GI,J‖2op\n) 1 2\n≤ 4 [(\nk log m1 k + q log m2 q\n) + 2(k + q) ] 1 2\n≤ 4 (√\nk log m1 k + 2k +\n√ q log\nm2 q + 2q\n)\nThe upper bounds for the ℓ1 and trace norms are standard. See Vershynin (2012, Theorem. 5.32) for the tight upper bound on the operator norm E ‖G‖op ≤ √ m1 + √ m2, and for the upper bound on the element-wise ℓ∞ norm of G, use Jensen inequality followed by upper bounding the maximum of nonnegative scalars by their sum:\nexp (t E ‖G‖∞) ≤ E exp (t ‖G‖∞) ≤ m1m2 exp(t2/2) .\nTaking t = √ 2 log(m1m2) in the logarithms of the last inequality gives E ‖G‖∞ ≤ √ 2m1m2."
    }, {
      "heading" : "C Some cone inclusions (Proofs of results in Section 4.2.2)",
      "text" : "Let us start with a simple result useful to prove inclusions of tangent cones.\nLemma 29 Let f and g two convex functions from Rd such that f ≤ g and let x∗ such that f(x∗) = g(x∗). Then Tg(x∗) ⊂ Tf (x∗).\nProof [Lemma 29] Let h ∈ Rd and τ > 0 such that g(x∗ + τh) ≤ g(x∗). Then we also have\nf(x∗ + τh) ≤ g(x∗ + τh) ≤ g(x∗) = f(x∗) ,\nand therefore, for any τ > 0,\n{ h ∈ Rd : g(x∗ + τh) ≤ g(x∗) } ⊂ { h ∈ Rd : f(x∗ + τh) ≤ f(x∗) } .\nFrom the definition (26) of the tangent cone we deduce, by taking the union over τ > 0 and the closure of this inclusion, that Tg(x\n∗) ⊂ Tf (x∗). We can now prove the results in Section 4.2.2 Proof [Proposition 17] Consider a matrix A = ab⊤ ∈ Ãk,q. We have ‖A‖∗ = ‖a‖2‖b‖2 = 1, and ‖A‖1 = ‖a‖1 ‖b‖1 = √ kq. Since A is an atom of both the norm Ωk,q and the norm Ω̃k,q we have Ωk,q(A) = Ω̃k,q(A) = 1 so that, for any µ ∈ [0, 1],\nΓµ(A) = ‖A‖∗ = 1√ kq ‖A‖1 = Ωk,q(A) = Ω̃k,q(A) = 1 .\nBesides, for any matrix K ∈ Rm1×m2 , for all (I, J) ∈ Gm1k × Gm2q , we have ‖KI,J‖op ≤ ‖K‖op and ‖KI,J‖op ≤ ‖KI,J‖Fro ≤ √ kq ‖KI,J‖∞ so that Ω∗k,q(K) ≤ ‖K‖op and Ω∗k,q(K) ≤ √ kqmaxI,J ‖KI,J‖∞ =√\nkq ‖K‖∞. Given that Ãk,q ⊂ Ak,q, we also have that\nΩ̃∗k,q(K) = max A∈Ãk,q 〈A,K〉 ≤ max A∈Ak,q 〈A,K〉 = Ω∗k,q(K) .\nBy Fenchel duality, we therefore have for any Z ∈ Rm1×m2 and µ ∈ [0, 1]: µ√ kq ‖Z‖1 + (1− µ) ‖Z‖∗ ≤ Ωk,q(Z) ≤ Ω̃k,q(Z) .\nProof [Corollary 18] Combining Proposition 17 with Lemma 29 directly gives (31). (32) is then a direct consequence of the definition of the statistical dimension (27).\nProof [Corollary 19] A necessary and sufficient condition for exact recovery is the so called null space property which is the event that TΩ(Z\n∗) ∩Ker(X ) = {0}, where Ker(X ) is the kernel of the linear transformation X (Chandrasekaran et al., 2012, Proposition 2.1). The result therefore follows from the inclusion of the cones stated in Corollary 18.\nProof [Proposition 20]\nLet a ∈ Ãmk with supp(a) = I0, meaning that |ai| = 1/ √ k for i ∈ I0 and ai = 0 for i ∈ I∁0 . The sub differential of the scaled ℓ1 norm Γ1 at a is\n∂Γ1(a) = { s ∈ Rm : si = sign(ai) for i ∈ I0 , |si| ≤ 1 for i ∈ I∁0 } .\nFrom (10), we get that the subdifferential of θk at a is\n∂θk(a) = {a+ z : ∀i , aizi = 0 and ∀I ∈ Gmk , ‖aI + zI‖ ≤ 1} .\nThe first condition is equivalent to zi = 0 for i ∈ I0, which implies that the second is equivalent to |zi| ≤ 1/ √ k for i ∈ I∁0 . We deduce that s = a + z ∈ ∂θk(a) if and only if si = ai for i ∈ I0 and\n|si| ≤ 1/ √ k for i ∈ I∁0 , i.e.,\n∂θk(a) = 1√ k ∂Γ1(a) .\nThis shows that the subdifferentials of Γ1 and θk have the same conic hull, and Proposition 20 follows by noting that the tangent cone is the polar cone of the conic hull of the subdifferential (Rockafellar, 1997, Theorem 23.7)."
    }, {
      "heading" : "D Upper bound on the statistical dimension of Ωk,q (proof of Proposition 23)",
      "text" : "The aim of this appendix is to prove the upper bound on the statistical dimension Ωk,q given in Proposition 23. Given its level of technicality, we split the proof in several parts. We start with preliminaries and notations in Section D.1, before proving Proposition 23 in Section D.2. The proofs of several technical results needed in Section D.2 are postponed to Section D.3, D.4 and D.5.\nD.1 Preliminaries and notations\nLet us start with some notations used throughout Appendix D. A = ab⊤ ∈ Ak,q is an atom of Ωk,q, with I0 = supp(a) and J0 = supp(b). γ = γ(a, b) refers to the atom strength of A (Definition 22). For any I ∈ Gm1k and J ∈ Gm2q , let uI = aI/ ‖aI‖2 and vJ = bJ/ ‖bJ‖2. Note that while aI is a subvector of a, the notation uI does not refer to a subvector of some vector u and that therefore [uI ]I0 6= [uI0 ]I = aI since ‖aI0‖ = ‖a‖ = 1. To analyze the statistical dimension (27) of Ωk,q at A, it is useful to express it as follows (Chandrasekaran et al., 2012, Proposition 3.6):\nS(A,Ωk,q) := E [ dist ( G,NΩk,q (A) )2] , (50)\nwhere NΩk,q (A) is the normal cone of Ωk,q at A (i.e., the conic hull of the subdifferential of Ωk,q at A) and dist ( G,NΩk,q (A) ) denotes the Frobenius distance of the Gaussian matrix G with i.i.d. standard normal entries to NΩk,q (A). In order to upper bound this quantity, it is therefore important to characterize precisely the normal cone NΩk,q(A) . For that purpose, let us introduce further notations. We consider the following subspace of Rm1×m2\nspan(A) = { LA+AR : L ∈ Rm1×m1 , R ∈ Rm2×m2 } ,\nand denote by PA and P⊥A the orthogonal projectors onto span(A) and span⊥(A) respectively. Since A = ab⊤ with ‖a‖2 = ‖b‖2 = 1, we have the closed-form expressions P⊥A (Z) = (Idm1−aa⊤)Z(Idm2− bb⊤).\nFor any (I, J) ∈ Gm1k × Gm2q , consider now the subspace\nspanI,J(A) = { LI,IAI,J +AI,JRJ,J : L ∈ Rm1×m1 , R ∈ Rm2×m2 } ,\nand its orthogonal\nspan⊥I,J(A) = { Z ∈ Rm1×m2 : AI,JZ⊤I,J = A⊤I,JZI,J = 0 } .\nNote that span⊥I0,J0(A) is related to the subdifferential of Ωk,q at A, since according to (10) we can write it as\n∂Ωk,q(A) = { A+ Z : Z ∈ span⊥I0,J0(A) , ∀(I, J) ∈ G m1 k × Gm2q ‖AI,J + ZI,J‖op ≤ 1 } . (51)\nIt is possible to estimate the dimension of span⊥I0,J0(A) as follows:\nLemma 30 The dimension of spanI0,J0(A) is k + q − 1.\nProof [Lemma 30] For A = ab⊤, the range of L 7→ LI0,I0AI0,J0 equals the range of αI0 7→ αI0b⊤ which has dimension |I0| = k. By the same token, the range of R 7→ AI0,J0RJ0,J0 has dimension q. By definition of spanI0,J0(A) we therefore have\nspanI0,J0(A) = { αI0b ⊤ + aβ⊤J0 : α ∈ Rm1 , β ∈ Rm2 }\nand therefore by the inclusion-exclusion principle dim ( spanI0,J0(A) ) = k + q − 1.\nFinally we denote by ΠA,I,J the projector onto spanI,J(A), and by Π ⊥ A,I,J the projector onto span⊥I,J(A). They satisfy respectively\nΠA,I,J(Z) = PAI,J (ZI,J) and Π⊥A,I,J(Z) = Z −ΠA,I,J(Z) = Z − PAI,J (ZI,J) .\nD.2 Proof of Proposition 23\nProof [Proposition 23] In order to upper bound the statistical dimension of Ωk,q at A, we associate to any matrix G a matrix Ξ(G) belonging to the normal cone NΩk,q(A), where Ξ : R\nm1×m2 → Rm1×m2 is measurable. From the characterization of the statistical dimension (50), since dist ( G,NΩk,q (A) ) ≤ ‖G− Ξ(G)‖Fro, we will then get the upper bound:\nS(A,Ωk,q) = E [ dist ( G,NΩk,q (A) )2] ≤ E ‖G− Ξ(G)‖2Fro . (52)\nThe main steps in the proof are then (i) to define the mapping Ξ, (ii) to show that Ξ(G) ∈ NΩk,q(A) for all G, and (iiii) to upper bound E ‖G− Ξ(G)‖2Fro in order to derive an upper bound on S(A,Ωk,q) by (52). Given a measurable function ǫ : Rm1×m2 → R, let us therefore consider the mapping Ξ:\n∀G ∈ Rm1×m2 , Ξ(G) := ǫ(G)A+Π⊥A,I0,J0(G) . (53)\nThe following lemma provides a mapping ǫ to ensure that Ξ(G) ∈ NΩk,q(A).\nLemma 31 Let ǫ(G)2 be equal to\n16 γ2 ‖GI0,J0‖2op ∨ max\nI∈G m1 k J∈G m2 q\n‖GIJ‖2op ∨ max0≤i<k 0≤j<q\n(i,j) 6=(0,0)\n8\nγ (\ni k + j q ) max |I\\I0|=i |J\\J0|=j\n[∥∥∥G⊤I∩I0,J\\J0uI ∥∥∥ 2\n2 +\n∥∥GI\\I0,J∩J0vJ ∥∥2 2\n] .\n(54) Then, for every G ∈ Rm1×m2 , the matrix Ξ(G) defined in (53) belongs to the normal cone of Ωk,q at A.\nBy choosing ǫ(G) as in Lemma 31, the upper bound (52) because Ξ(G) ∈ NΩk,q(A). Using the decomposition G = ΠA,I0,J0(G) + Π ⊥ A,I0,J0 (G) we deduce\nS(A,Ωk,q) ≤ E ‖G− Ξ(G)‖2Fro = E ‖ǫ(G)A −ΠA,I0,J0(G)‖2Fro ≤ 2E ‖ǫ(G)A‖2Fro + 2E ‖ΠA,I0,J0(G)‖2Fro = 2E ǫ(G)2 + 2(k + q − 1), (55)\nwhere (55) is due to ‖A‖Fro = 1 and the fact that ‖ΠA,I0,J0(G)‖2Fro follows a chi-square distribution with k+ q−1 degrees of freedom, since by Lemma 30 this is the dimension of spanI0,J0(A). In order to upper bound E ǫ(G)2 we need the following two lemmata in addition to Lemma 28.\nLemma 32 E ‖GI0,J0‖2op ≤ 4(k + q) + 4 . (56)\nLemma 33\nEmax i,j\n8\nγ (\ni k + j q ) max |J\\J0|=j |I\\I0|=i\n[ ‖G⊤I∩I0,J\\J0uI‖ 2 2 + ‖GI\\I0,J∩J0vJ‖22 ]\n≤ 48 γ (k ∨ q) log ((m1 − k) ∨ (m2 − q)) + 64 γ (k ∨ q) .\nCombining Lemmata 28, 56 and 33 with the definition of ǫ(G) in (54) we deduce\nE ǫ(G)2 ≤ 16 γ2 [4(k + q) + 4] + 16\n[( k log\nm1 k + q log m2 q\n) + 2(k + q) ]\n+ 48\nγ (k ∨ q) log ((m1 − k) ∨ (m2 − q)) +\n64\nγ (k ∨ q)\n≤ ( 64\nγ2 +\n64\nγ + 32\n) (k + q + 1) + 16 ( k log\nm1 k + q log m2 q\n)\n+ 48\nγ (k ∨ q) log (m1 ∨m2)\n≤ 160 γ2 (k + q + 1) + 80 γ (k ∨ q) log (m1 ∨m2) .\nPlugging this upper bound into (55) finally proves Proposition 23.\nD.3 The scaling factor ǫ(G) ensures that Ξ(G) ∈ NΩk,q(A) (proof of Lemma 31) Proof [Lemma 31] To simplify notations let us denote\nG̃ :=Π⊥A,I0,J0(G) ,\nso that (53) becomes Ξ(G) = ǫ(G)A + G̃. To prove that Ξ(G) belongs to the normal cone of Ωk,q at A, it is sufficient to prove that ǫ(G)−1Ξ(G) = A + ǫ(G)−1G̃ is a subgradient of Ωk,q at A. By the characterization of the subgradient in (51), and since G̃ ∈ span⊥I0,J0(A), this is equivalent to∥∥∥AIJ + ǫ(G)−1 G̃IJ\n∥∥∥ op\n≤ 1 for any (I, J) ∈ Gm1k × Gm2q , which itself is equivalent to ∥∥∥AIJ + ǫ(G)−1 ΠA,I,J(G̃)\n∥∥∥ op ≤ 1 and ǫ(G)−1 ∥∥∥P⊥A (G̃I,J) ∥∥∥ op ≤ 1 . (57)\nFirst, the second inequality of (57) is satisfied since\n∥∥∥P⊥A (G̃I,J) ∥∥∥ op ≤ ∥∥∥G̃I,J ∥∥∥ op = ∥∥∥ [ Π⊥A,I0,J0(G) ] IJ ∥∥∥ op ≤ ‖[G]IJ‖op ≤ ǫ(G) .\nThere thus remains to prove the first inequality of (57). Note that the matrix AIJ+ǫ(G) −1 ΠA,I,J(G̃)\nhas rank 2, so its Frobenius norm is larger than its operator norm by at most a factor of √ 2. Working with the Frobenius norm is more convenient, so knowing that\n∥∥∥AIJ + ǫ(G)−1 ΠA,I,J(G̃) ∥∥∥ 2\nop ≤\n∥∥∥AIJ + ǫ(G)−1 ΠA,I,J(G̃) ∥∥∥ 2\nFro ,\nwe will establish an upper bound on the latter quantity which we denote by νI,J(G). Noting that AIJ = ‖aI‖2 ‖bJ‖2 uIv⊤J and that\nΠA,I,J(G̃) = uIu ⊤ I G̃IJ + G̃IJvJv ⊤ J − uIu⊤I G̃IJvJ v⊤J ,\nwe get\nνI,J(G) = ∥∥∥‖aI‖2 ‖bJ‖2 uIv⊤J + ǫ(G)−1 ( uIu ⊤ I G̃IJ + G̃IJvJv ⊤ J − uIu⊤I G̃IJvJv⊤J )∥∥∥ 2\nFro\n= ‖aI‖22 ‖bJ‖22 + 2\nǫ(G) ‖aI‖2 ‖bJ‖2 u⊤I G̃IJvJ\n+ 1\nǫ(G)2\n( u⊤I G̃IJG̃ ⊤ IJuI + v ⊤ J G̃ ⊤ IJ G̃IJvJ − 2(u⊤I G̃IJvJ)2 )\n≤ ‖aI‖22 ‖bJ‖22 + 2\nǫ(G) ‖aI‖2 ‖bJ‖2 u⊤I G̃IJvJ +\n1\nǫ(G)2\n( u⊤I G̃IJ G̃ ⊤ IJuI + v ⊤ J G̃ ⊤ IJ G̃IJvJ ) .\nThe following Lemma provides upper bounds on the different terms.\nLemma 34 We have\nu⊤I G̃IJvJ ≤ ∥∥aI0\\I ∥∥ 2 ∥∥bJ0\\J ∥∥ 2 ‖GI0J0‖op ,\nu⊤I G̃IJG̃ ⊤ IJuI ≤ ∥∥∥G⊤I∩I0,J\\J0uI ∥∥∥ 2\n2 + 2\n∥∥aI0\\I ∥∥2 2 ‖GI0,J0‖2op ,\nv⊤J G̃ ⊤ IJG̃IJvJ ≤ ∥∥GI\\I0,J∩J0vJ ∥∥2 2 + 2 ∥∥bJ0\\J ∥∥2 2 ‖GI0,J0‖2op .\nThis yields\nνI,J(G) ≤ ‖aI‖22 ‖bJ‖22 + 2\nǫ(G) ‖aI‖2 ‖bJ‖2 ∥∥aI0\\I ∥∥ 2 ∥∥bJ0\\J ∥∥ 2 ‖GI0J0‖op\n+ 1\nǫ(G)2\n(∥∥∥G⊤I∩I0,J\\J0uI ∥∥∥ 2\n2 + 2\n∥∥aI0\\I ∥∥2 2 ‖GI0,J0‖2op\n)\n+ 1\nǫ(G)2\n(∥∥GI\\I0,J∩J0vJ ∥∥2 2 + 2 ∥∥bJ0\\J ∥∥2 2 ‖GI0,J0‖2op )\n≤ ‖aI‖22 ‖bJ‖22 + γ\n2 ‖aI‖2 ‖bJ‖2 ∥∥aI0\\I ∥∥ 2 ∥∥bJ0\\J ∥∥ 2\n+ γ\n8\n( i\nk +\nj\nq\n) + γ2\n8\n(∥∥aI0\\I ∥∥2 2 + ∥∥bJ0\\J ∥∥2 2 ) ,\nwhere we used the definition of ǫ(G) (54) to derive the last inequality. Define α := ‖aI0\\I‖2 = 1 − ‖aI‖2 and β := ‖bJ0\\J‖2 = 1 − ‖bJ‖2. With these notations and rearranging the terms, we can rewrite the above inequality as\nνI,J(G) ≤ (1− α)(1 − β) + γ\n2\n√ αβ(1 − α)(1 − β) + γ 2\n8 (α+ β) +\nγ\n8\n( i\nk +\nj\nq\n) .\nSince 0 ≤ α, β ≤ 1 and using √αβ ≤ 12(α+ β), we have\nαβ ≤ 1 2 (α+ β) and\n√ αβ(1 − α)(1 − β) ≤ 1\n2 (α+ β) .\nThese inequalities yield\nνI,J(G) ≤ 1 + (α+ β) ( − 1 + 1\n2 +\nγ 4 +\nγ2\n8\n) + γ\n8\n( i\nk +\nj\nq\n) .\nBy definition of γ = min ι∈I0 ι′∈J0\n( k a2ι , q b 2 ι′ ) , we have ik ≤ αγ and j q ≤ β γ . Moreover, given that\n0 ≤ γ ≤ 1, we have 4γ −2−γ = 1γ (4−2γ−γ2) ≥ 1γ , so that factorizing γ 8 in the previous expression, we obtain\nνI,J(G) ≤ 1 + γ\n8\n[( − 4\nγ + 2 + γ\n) (α+ β) +\n( i\nk +\nj\nq\n)]\n≤ 1 + γ 8 [ −1 γ (α+ β) + ( i k + j q )] ≤ 1 ,\nwhich concludes the proof.\nD.4 Proof of Lemma 34\nLet us first start with a few useful technical lemmas.\nLemma 35 The matrix G̃IJ = [Π ⊥ A,I0,J0 (G)]IJ is of the form G̃IJ = G̃1 + G̃2 with\nG̃1 = GIJ −GI∩I0,J∩J0 and G̃2 = (IdI − aIa⊤)GI0J0 (IdJ − bb⊤J ).\nProof [Lemma 35]\nΠ⊥A,I0,J0(G) = G−ΠA,I0,J0(G) = G− aI0a⊤I0GI0J0 −GI0J0bJ0b⊤J0 + aI0a⊤I0GI0J0bJ0b⊤J0 = G−GI0J0 + (IdI0 − aI0a⊤I0)GI0J0 (IdJ0 − bJ0b⊤J0),\nso that [Π⊥A,I0,J0(G)]IJ = GIJ −GI∩I0,J∩J0 + (IdI − aIa⊤)GI0J0 (IdJ − bb⊤J ).\nLemma 36 We have u⊤I G̃1 = u ⊤ I GI∩I0,J\\J0 and G̃1vJ = GI\\I0,J∩J0vJ .\nProof [Lemma 36] Given that supp(uI) ⊂ I0, we have\nu⊤I G̃1 = u ⊤ I (GIJ −GI∩I0,J∩J0) = u⊤I (GI∩I0,J −GI∩I0,J∩J0) = u⊤I GI∩I0,J\\J0 ,\nwhich proves the first equality. The second one is proved similarly.\nLemma 37 ‖Id− bJb⊤‖2op ≤ 4\n3\nProof [Lemma 37] The largest singular value is attained on the span of bJ and bJc both on the left and on the right. Given that ‖b‖ = 1, it is therefore also the largest eigenvalue of the matrix of the linear operator restricted to this span which is equal to\n[ (1− x) − √ (1− x)x\n0 1\n] ,\nfor x = ‖bJ‖2. Tedious but simple calculations show that the squared operator norm of this matrix is equal to 1− x/2 + 1/2 √ x(4− 3x), which takes its maximum value 4/3 for x = 1/3.\nProof [Lemma 34] Given that G̃IJ = G̃1 + G̃2 and u ⊤ I G̃1 = uIG̃I∩I0,J\\J0 , we have u ⊤ I G̃1vJ = u ⊤ I G̃1vJ∩J0 = 0, so that\nu⊤I G̃IJvJ = u ⊤ I G̃2vJ\n= u⊤I (IdI − aIa⊤)GI0J0 (IdJ − bb⊤J )vJ ≤ ∥∥uI − ‖aI‖ a ∥∥∥∥GI0J0 ∥∥ op ∥∥vJ − ‖bJ‖ b ∥∥ ≤ ‖aI0\\I‖ ‖bJ0\\J‖ ‖GI0J0‖op,\nbecause ‖u⊤I (IdI − aIa⊤)‖2 = ∥∥uI − ‖aI‖ a ∥∥2 = 1 − 2‖aI‖2 + ‖aI‖2 = ‖aI0\\I‖2, and symmetrically∥∥vJ − ‖bJ‖ b ∥∥ = ‖bJ0\\J‖. This shows the first inequality. For the two next inequalities, note that\nu⊤I G̃IJG̃ ⊤ IJuI = ‖G̃⊤IJuI‖2 = ‖G̃⊤1 uI‖2 + ‖G̃⊤2 uI‖2\nbecause 〈G̃⊤1 uI , G̃⊤2 uI〉 = 0 as a result of the fact that by lemma 36, G̃⊤1 uI and G̃⊤2 uI have disjoint supports. Now ‖G̃⊤1 uI‖2 = ‖G⊤I∩I0,J\\J0uI‖ 2 2 and ‖G̃⊤2 uI‖ ≤ 2 ‖aI0\\I‖2 ‖GI0,J0‖2op, because ‖Id − bJb⊤‖2op ≤ 2 (see Lemma 37 for a proof). This shows the second inequality and the third follows by symmetry.\nD.5 Upper bounds for ǫ(G)2 (Proofs of Lemmata 32 and 33)\nProof [Lemma 32]\nUsing (45) and the fact that (√ k + √ q + s )2 ≤ 2 ( ( √ k + √ q)2 + s2 ) gives\nP [ ‖GI0,J0‖2op > 2 ( ( √ k + √ q)2 + s2 )] ≤ exp(−s2/2) .\nSetting t = 2s2 yields\nP [ ‖GI0,J0‖2op > 4(k + q) + t ] ≤ exp(−t/4) .\nIt follows that\nE ‖GI0,J0‖2op = ∫ ∞\n0 P(‖GI0,J0‖2op ≥ t′)dt′\n=\n∫ 4(k+q)\n0 dt′ +\n∫ ∞\n4(k+q) P(‖GI0,J0‖2op ≥ t′)dt′\n≤ 4(k + q) + ∫ ∞\n0 exp(−t/4)dt\n= 4(k + q) + 4 .\nProof [Lemma 33] As the sets I ∩ I0 × J\\J0 and I\\I0 × J ∩ J0 are disjoint, and uI , vJ of unit length, the random variable\nMI,J = ∥∥∥G⊤I∩I0,J\\J0uI ∥∥∥ 2\n2 +\n∥∥GI\\I0,J∩J0vJ ∥∥2 2\nfollows a chi-square distribution with i + j degrees of freedom, where i = |I\\I0| and j = |J\\J0|. Using Chernoff’s inequality and the form of the chi-square moment generating function, we have that for any fixed real number α and fixed index sets I and J , for all t ∈ (0, 1/2),\nP [ MI,J > α ] = P [ etMI,J > etα ] ≤ e−tα E etMI,J = e−tα(1− 2t)− i+j2 .\nTaking the maximum over index sets I and J with the same intersection sizes with I0 and J0 respectively, and using a union bound on the independent choices of I and J , yields\nP   max\n|I\\I0|=i |J\\J0|=j\nMI,J > α\n  ≤ ( m1 − k\ni\n)( m2 − q\nj\n) exp { −tα− i+ j\n2 log(1− 2t)\n}\n≤ exp { −tα− i+ j\n2 log(1− 2t) + i log(m1 − k) + j log(m2 − q)\n} .\nTaking α = λ(i+ j), we have for any t < 1/2 (assuming w.l.o.g. m1 − k ≥ m2 − q)\nP   max\n|I\\I0|=i |J\\J0|=j\nMI,J > λ(i+ j)\n  ≤ exp { −tλ(i+ j)− i+ j\n2 log(1− 2t) + i log(m1 − k) + j log(m2 − q)\n}\n≤ exp { (i+ j) ( −tλ− 1\n2 log(1− 2t) + log(m1 − k)\n)} .\nLet us introduce Mi,j = 1i+j max |I\\I0|=i |J\\J0|=j MI,J , and take t = 1 2\n( 1− 1m1−k ) < 12 . Then\nP   max\n0≤i<k 0≤j<q\n(i,j) 6=(0,0)\nMi,j > λ   ≤ ∑\n0≤i<k 0≤j<q\n(i,j) 6=(0,0)\nexp { (i+ j) ( −1 2 ( 1− 1 m1 − k ) λ+ 3 2 log(m1 − k) )}\n=\nk−1∑\ni=0\nβi q−1∑\nj=0\nβj − 1 = 1− β k 1− β 1− βq 1− β − 1 ≤ 2β ,\nwhere\nβ = exp { −1 2 ( 1− 1 m1 − k ) λ+ 3 2 log(m1 − k) } .\nAs a consequence, we have\nE[max i,j\nMi,j ] = ∫ ∞\n0 P[max i,j Mi,j > λ]dλ\n≤ ∫ 3(m1−k) m1−k−1 log k\n0 dλ+ 2 ∫ ∞ 3(m1−k) m1−k−1 log(m1−k) exp { 3 2 log(m1 − k)− 1 2 ( 1− 1 m1 − k ) λ } dλ\n≤ 3(m1 − k) m1 − k − 1 log k + 4 m1 − k\nm1 − k − 1 ≤ 6 log(m1 − k) + 8 .\nIt follows that\nE max 0≤i<k 0≤j<q\n(i,j) 6=(0,0)\n8\nγ (\ni k + j q ) max |J\\J0|=j |I\\I0|=i ‖G⊤I∩I0,J\\J0uI‖ 2 2 + ‖GI\\I0,J∩J0vJ‖22\n≤ 48 γ (k ∨ q) log ((m1 − k) ∨ (m2 − q)) + 64 γ (k ∨ q) . (58)"
    }, {
      "heading" : "E Lower bound on the statistical dimension of Γµ (Proof of Proposition 24)",
      "text" : "Let us start with a technical lemma:\nLemma 38 Let ab⊤ ∈ Ak,q, X : Rm1×m2 → Rn a linear map from the standard Gaussian ensemble and y = X (ab⊤). If n ≤ 19m1m2 and further\nn ≤ n0 := ζ(a, b) 1 64 ( (kq) ∧ (m1 +m2 − 1) ) − 2, with ζ(a, b) = 1− ( 1− ‖a‖ 2 1 k )( 1− ‖b‖ 2 1 q ) ,\nthen, with probability 1− c1 exp(−c2n0), solving formulation (28) with the norm Γµ fails to recover ab⊤ simultaneously for any values of µ ∈ [0, 1], where c1 and c2 are universal constants.\nProof [Lemma 38] The proof consists in applying theorem 3.2 in Oymak et al. (2012) for the combination of the ℓ1norm with the trace norm. We adapt slightly the notations of that paper to reflect the fact that we are working with matrices. Since we consider conic combinations of the ℓ1 and trace norms, the number of norms is therefore τ = 2. To apply the theorem we need to specify κ, θ, dmin, γ and C◦ in the notations of that paper. For each decomposable norm νj for j ∈ {1, 2}, with ν1 the ℓ1-norm and and ν2 the trace norm, given a point ab⊤ (which corresponds to the point x0 in Oymak et al., 2012), the authors define\n• Tj the supporting subspaces and Ej (ej in the paper), the orthogonal projection of any subgradient of the norm in ab⊤ (Definition 2.1),\n• Lj the Lipschitz constant of νj with respect to the Euclidean norm (Definition 2.2),\n• κj = ‖Ej‖2Fro\nL2j m1m2 dim(Tj) (Definition 2.2).\nLet ab⊤ ∈ Ak,q with support I0×J0 and sa = sign(a), sb = sign(b). Denoting eij the element of the canonical basis of Rm1×m2 , we have\n• T1 = span({eij}(i,j)∈I0×J0) so that dim(T1) = kq,\n• T2 = {av⊤ + ub⊤ | u ∈ Rm1 , v ∈ Rm2} so that dim(T2) = m1 +m2 − 1.\nBy definition dmin = dim(T1) ∧ dim(T2). We have\nE1 = sas ⊤ b , ‖E1‖2Fro = kq, E2 = ab⊤, ‖E2‖2Fro = 1, L1 = √ kq, L2 = √ m1 ∧m2 ,\nand thus κ1 = m1m2 kq , κ2 = m1m2 (m1 ∧m2)(m1 +m2 − 1) , so that κ = κ1 ∧ κ2 ≥ 1 2 .\nWe then have θ defined as θ = θ1 ∧ θ2 with θj = ‖E∩,j‖2/‖Ej‖2 where E∩,j is the projection of Ej on T1 ∩ T2. But E2 ∈ T1 so that θ2 = 1. The situation is less simple for E1. Indeed, E∩,1 = ‖a‖1as⊤b + ‖b‖1sab⊤ − ab⊤‖a‖1‖b‖1. Some calculations lead to\nθ21 = ‖a‖21 k + ‖b‖21 q − ‖a‖ 2 1 k ‖b‖21 q ,\nhence the definition of ζ(a, b) = θ2 = θ21 ∧ θ22. Theorem 3.2 in Oymak et al. (2012) offers the possibility of constraining the estimator to lie in a cone C. In our case, C = Rm1×m2 , given the definition of γ we therefore have γ ≤ 2. The result follows from applying the theorem with θ2 = ζ(a, b) and using κ\n81γ2τ ≥ 1/2 34.22.2 = 1 64 .\nProof [Proposition 24] Take M such that when m1,m2, k, q,m1/k,m2/q ≥ M then n0 is large enough to ensure 1 − c1 exp(−c2n0) > 4 exp (−32/17). Then, according to Lemma 38, solving (28) with the norm Γµ fails to recover A = ab⊤ with probability at least 4 exp (−32/17). On the other hand, Amelunxen et al. (2013, Theorem 7.1) shows that, when n ≥ S (A,Γµ)+ λ, for any λ ≥ 0, then solving (28) with the norm Γµ correctly recovers A with probability at least\n4 exp ( −λ2/8 ω2(A,Γµ) + λ ) , (59)\nwhere ω2(A,Γµ) = S (A,Γµ)∧ (m1m2 −S (A,Γµ)). Take λ = 16ω(A,Γµ), then using the fact that ω(A,Γµ) ≥ 1 we get that the probability (59) is smaller than 4 exp (−32/17). This implies that\nn0 ≤ S (A,Γµ) + λ ≤ S (A,Γµ) + 16 √ S (A,Γµ) ≤ 17S (A,Γµ) ."
    }, {
      "heading" : "F Bounds on the statistical dimension in the vector case (proofs of",
      "text" : "results of Section 4.2.4)\nF.1 Lower bound on the statistical dimension of κk (Proof of Proposition 25)\nLet us start with two technical lemmata.\nLemma 39 Let X(k) denote the kth order statistics of an i.i.d. sample X1, . . . ,Xn whose common distribution has a cdf F . Assume that F−1 is a convex function7 from [0, 1] to R. Then\nE[X(k)] ≥ F−1 ( k n+ 1 ) .\nProof [Lemma 39] Let f denote the pdf of X. We have\nE[X(k)] = n!\n(k − 1)!(n − k)!\n∫ ∞\n−∞ uF (u)k−1\n( 1− F (u) )n−r f(u) du\n= Γ(n+ 1)\nΓ(k) Γ(n − k + 1)\n∫ 1\n0 F−1(v) vk−1(1− v)n−rdv = E[F−1(V )] ,\nwith V ∼ Beta(k, n−k+1). Assuming that F−1 is a convex function, we have by Jensen’s inequality\nE[X(k)] = E[F −1(V )] ≥ F−1(E[V ]) = F−1 ( k n+ 1 ) .\n7Note that this implies that the essential support of the random variable is bounded below.\nLemma 40 Let G ∈ Rn be an standard normal vector, then we have\nE[κ∗k(G)] ≥ √ 2\nπ\n√ k log (n+ 1 k + 1 ) .\nProof [Lemma 40] Denote by F the cdf of the absolute value of a standard normal variable. Then,\nF (x) = Φ(x)− Φ(−x) = erf ( x√\n2\n) ,\nwhere Φ is the cdf of a standard Gaussian and erf denotes the error function. We use the following inequality due to Chu (1954):\n√ 1− e−x2 ≤ erf(x) ≤ √ 1− e−π4 x2 ,\nto deduce that\nF−1(y) ≥ √ − 2 π log(1− y2) .\nBy definition, we have E[κ∗k(G)] = 1√ k E[X(n) + . . . +X(n−k+1)] where Xi = |Gi| and G is a vector of independent standard normal variables. It can easily be checked that F−1 is a convex function. This implies, using Lemma 39, that\nE[κ∗k(G)] ≥ 1√ k\nk∑\nj=1\nF−1 ( 1− j\nn+ 1\n)\n≥ √ k F−1 (1 k k∑\nj=1\n( 1− j\nn+ 1\n)) (again by Jensen’s inequality)\n= √ k F−1 ( 1− k + 1\n2(n+ 1)\n)\n≥ √ k √ − 2 π log ( k + 1 (n+ 1) − ( k + 1 2(n + 1) )2) ≥ √ 2\nπ\n√ k log (n+ 1 k + 1 ) .\nProof [Proposition 25] We will denote the squared Gaussian width of the tangent cone intersected with a Euclidean unit ball by\nw(Tκk(a) ∩ Sm−1) = E [\nmax t∈Tκk (a)∩Sm−1\n〈t,G〉 ] ,\nwhereG ∈ Rm denotes a standard Gaussian vector. We have w(Tκk(a)∩Sm−1)2 ≤ S(a, κk)(Chandrasekaran et al., 2012, Proposition 3.6). We thus seek a lower bound of w(Tκk(a)∩ Sm−1). Since the tangent cone is polar to the normal cone, we have that\nTκk(a) = {t ∈ Rm | 〈s, t〉 ≤ 0, ∀s ∈ ∂κk(a)} .\nGiven a random Gaussian vector G, denote I0 the support of a and IG the indices of the k largest coefficients of G in absolute value outside of I0. Denote by s̃G = sign(GIG), i.e., the vector whose entries are zero outside of IG and equal to the sign of the corresponding coefficient of G otherwise. Define tG =\n1√ 2k (s̃G − a). By construction tG ∈ Sm−1. Let now consider s ∈ ∂κk(a), we have\n√ 2k 〈s, tG〉 = −〈s, a〉+ 〈s, s̃G〉 ≤ −1 + κk(s̃G)κ∗k(s) ≤ −1 + 1 = 0,\nso that tG ∈ Tκk(a). Therefore w(Tκk(a) ∩ Sm−1) ≥ E[〈tG, G〉] = 12√kE[〈s̃G, G〉] = 1 2E[κ ∗ k(G)], whence the result using Lemma 40 and w(Tκk(a) ∩ Sm−1)2 ≤ S(a, κk).\nF.2 Upper bound on the statistical dimension of θk (Proof of Proposition 26)\nProof [Proposition 26] Without loss of generality, let us assume that w ∈ Rp is a fixed vector having nonincreasing – in absolute value – coordinates, the first s of which are assumed to be nonzero. We compute the subdifferential of θk(w) directly by using (14). Remember that one characterization of the subdifferential is ∂θk(w) = {α ∈ Rp : θ∗k(α) ≤ 1 , α⊤w = θk(w)} . Letting r ∈ {0, · · · , k − 1} being the unique integer such that |wk−r−1| > 1r+1 ∑p i=k−r |wi| ≥ |wk−r|, let us partition the set of entries {1, · · · , p} into I2 = {1, · · · , k − r − 1}, I1 = {k − r, · · · , s} and I0 = {s + 1, · · · , p} (where each set may be empty). Then we can rewrite the expression of the k-support norm (14) as\nθk(w) 2 = ‖wI2‖22 +\n1\nr + 1 ‖wI1‖21 .\nThen necessarily each element α ∈ ∂θk(w) must satisfy    αi = wi θk(w) for i ∈ I2 , αi = ‖wI1‖1 sign(wi) (r+1)θk(w) for i ∈ I1 .\nAs for i ∈ I0, the coefficients αi do not impact α⊤w so they should also not impact θ∗k(α). If s < k this implies αi = 0, and if s ≥ k this means |αi| ≤ |αk|, and in that case k ∈ I1. With the convention ‖wI1‖1 = 0 when I1 = ∅, we finally get the following expression for the subdifferential:\n∂θk(w) = 1\nθk(w)\n{ wI2 + 1\nr + 1 ‖wI1‖1 (sgn(wI1) + hI0) : ‖h‖∞ ≤ 1\n} . (60)\nIn the case s < k, we have s = k − r + 1, I2 = [1, s], I1 = ∅ and I0 = [s+ 1, p]. In that case θk(w) = ‖w‖2 and ∂θk(w) = w/ ‖w‖2, showing that θk is differentiable at w, meaning θk is useless to recover w. Let us therefore only consider the case s ≥ k, in which case I1 6= ∅ and ‖wI1‖1 > 0. In order to compute the statistical dimension of θk at w, we use the characterization (50)\nS(w, θk) = E [ dist (g,Nθk(A)) 2 ] ,\nwhere g is a p-dimensional random vector with i.i.d. normal entries and Nθk(A) is the conic hull of ∂θk(w). We then get:\nS(w, θk) = E\n[ inf\nt>0 & u∈t∂θk(w) ‖u− g‖22\n]\n≤ inf t>0 E\n[ inf\nu∈t∂θk(w) ‖u− g‖22\n]\n≤ inf t>0 E inf h∈Rp,‖h‖∞≤1\n{∥∥∥∥gI2 − t (r + 1)\n‖wI1‖1 wI2\n∥∥∥∥ 2\n2\n+ ‖gI1 − t sgn(wI1)‖22\n+ ‖gI0 − thI0‖22 }\n≤ inf t>0\n{ |I2|+\n(r + 1)2 ‖wI2‖22 ‖wI1‖21 t2 + |I1|(1 + t2) + |I0| 2√ 2π 1 t exp\n( − t 2\n2\n)} (61)\n= inf t>0\n{ s+ t2 { (r + 1)2 ‖wI2‖22\n‖wI1‖21 + |I1|\n} + (p− s) 2√\n2π\n1 t exp\n( − t 2\n2\n)}\n≤ 5 4 s+ 2\n{ (r + 1)2 ‖wI2‖22\n‖wI1‖21 + |I1|\n} log p\ns , (62)\nwhere following Chandrasekaran et al. (2012, Annex C), for (61) we used the fact that for a standard normal random variable G ∼ N (0, 1)\nEG inf |η|≤1 (G− tη)2 ≤ 2√ 2π 1 t e− t2 2 ,\nwhile (62) is obtained by taking b = √\n2 log(p/s) and using s(1−s/p)√ π log(p/s) ≤ 14 . For the lasso case (k = 1), we have r = 0, I2 = ∅ and I1 = [1, s]. Plugging this into (62) we recover the standard bound (Chandrasekaran et al., 2012):\nS(w, θk) ≤ 5\n4 s+ 2s log\np s . (63)\nIn the general case 1 ≤ k ≤ s remember that, by definition of r,\n|wk−r−1| > ‖wI1‖1 r + 1 ≥ |wk−r| ,\nand therefore\n|I2| ≤ ‖wI2‖22 |wk−r−1|2 ≤ (r + 1) 2 ‖wI2‖22 ‖wI1‖21 ≤ ‖wI2‖ 2 2 |wk−r|2 . (64)\nPlugging the left-hand inequality of (64) into (62) and remembering that |I2|+ |I1| = s shows that the bound (62) obtained for θk, for any 1 ≤ k ≤ s, is never better than the bound (63) obtained for the lasso case k = 1. In the case s = k, the right-hand inequality of (64) applied to an atom w ∈ Akp with atom strength γ = k|wk|2 and unit ℓ2 norm leads to\n(r + 1)2 ‖wI2‖22 ‖wI1‖21 + |I1| ≤ ‖wI2‖22 |wk−r|2 + |I1| ≤ ‖wI2‖22 |wk|2 + ‖wI1‖22 |wk|2 = 1 |wk|2 = k γ ,\nfrom which we deduce by (62) the upper bound on the statistical dimension\n∀w ∈ Akp , S(w, θk) ≤ 5\n4 k +\n2k\nγ log\np k ."
    } ],
    "references" : [ {
      "title" : "Living on the edge: Phase transitions in convex programs with random data",
      "author" : [ "D. Amelunxen", "M. Lotz", "M.B. McCoy", "J.A. Tropp" ],
      "venue" : "Technical Report 1303.6672,",
      "citeRegEx" : "Amelunxen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Amelunxen et al\\.",
      "year" : 2013
    }, {
      "title" : "High-dimensional analysis of semidefinite relaxations for sparse principal components",
      "author" : [ "A.A. Amini", "M.J. Wainwright" ],
      "venue" : "Ann. Stat.,",
      "citeRegEx" : "Amini and Wainwright.,? \\Q2009\\E",
      "shortCiteRegEx" : "Amini and Wainwright.",
      "year" : 2009
    }, {
      "title" : "Sparse prediction with the k-support norm",
      "author" : [ "A. Argyriou", "R. Foygel", "N. Srebro" ],
      "venue" : "Adv. Neural. Inform. Process Syst.,",
      "citeRegEx" : "Argyriou et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Argyriou et al\\.",
      "year" : 2012
    }, {
      "title" : "Convex relaxations of structured matrix factorizations",
      "author" : [ "F. Bach" ],
      "venue" : "Technical Report 1309.3117,",
      "citeRegEx" : "Bach.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bach.",
      "year" : 2013
    }, {
      "title" : "Convex sparse matrix factorizations",
      "author" : [ "F. Bach", "J. Mairal", "J. Ponce" ],
      "venue" : "Technical Report 0812.1869,",
      "citeRegEx" : "Bach et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bach et al\\.",
      "year" : 2008
    }, {
      "title" : "Structured sparsity through convex optimization",
      "author" : [ "F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski" ],
      "venue" : "Stat. Sci.,",
      "citeRegEx" : "Bach et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bach et al\\.",
      "year" : 2012
    }, {
      "title" : "Complexity theoretic lower bounds for sparse principal component detection",
      "author" : [ "Q. Berthet", "P. Rigollet" ],
      "venue" : "COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14,",
      "citeRegEx" : "Berthet and Rigollet.,? \\Q2013\\E",
      "shortCiteRegEx" : "Berthet and Rigollet.",
      "year" : 2013
    }, {
      "title" : "PhaseLift: Exact and stable signal recovery from magnitude measurements via convex programming",
      "author" : [ "E.J. Candès", "T. Strohmer", "V. Voroninski" ],
      "venue" : "Comm. Pure Appl. Math.,",
      "citeRegEx" : "Candès et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Candès et al\\.",
      "year" : 2013
    }, {
      "title" : "Computational and statistical tradeoffs via convex relaxation",
      "author" : [ "V. Chandrasekaran", "M.I. Jordan" ],
      "venue" : "Proc. Natl. Acad. Sci. USA,",
      "citeRegEx" : "Chandrasekaran and Jordan.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chandrasekaran and Jordan.",
      "year" : 2013
    }, {
      "title" : "The convex geometry of linear inverse problems",
      "author" : [ "V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky" ],
      "venue" : "Found. Comput. Math.,",
      "citeRegEx" : "Chandrasekaran et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chandrasekaran et al\\.",
      "year" : 2012
    }, {
      "title" : "On bounds for the normal integral",
      "author" : [ "J.T. Chu" ],
      "venue" : "Biometrika, 42(1/2):263–265,",
      "citeRegEx" : "Chu.,? \\Q1954\\E",
      "shortCiteRegEx" : "Chu.",
      "year" : 1954
    }, {
      "title" : "A direct formulation for sparse PCA using semidefinite programming",
      "author" : [ "A. d’Aspremont", "L. El Ghaoui", "M.I. Jordan", "G.R.G. Lanckriet" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "d.Aspremont et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "d.Aspremont et al\\.",
      "year" : 2007
    }, {
      "title" : "Optimal solutions for sparse principal component analysis",
      "author" : [ "A. d’Aspremont", "F. Bach", "L. El Ghaoui" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "d.Aspremont et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "d.Aspremont et al\\.",
      "year" : 2008
    }, {
      "title" : "Local operator theory, random matrices and Banach spaces",
      "author" : [ "K.R. Davidson", "S.J. Szarek" ],
      "venue" : "Handbook of the Geometry of Banach Spaces,",
      "citeRegEx" : "Davidson and Szarek.,? \\Q2001\\E",
      "shortCiteRegEx" : "Davidson and Szarek.",
      "year" : 2001
    }, {
      "title" : "Geometry of Cuts and Metrics, volume 15 of Algorithms and Combinatorics",
      "author" : [ "M.M. Deza", "M. Laurent" ],
      "venue" : null,
      "citeRegEx" : "Deza and Laurent.,? \\Q1997\\E",
      "shortCiteRegEx" : "Deza and Laurent.",
      "year" : 1997
    }, {
      "title" : "Finding approximately rank-one submatrices with the nuclear norm and l1 norms",
      "author" : [ "X.V. Doan", "S.A. Vavasis" ],
      "venue" : "SIAM J. Optimiz.,",
      "citeRegEx" : "Doan and Vavasis.,? \\Q2013\\E",
      "shortCiteRegEx" : "Doan and Vavasis.",
      "year" : 2013
    }, {
      "title" : "Corrupted sensing: Novel guarantees for separating structured signals",
      "author" : [ "R. Foygel", "L. Mackey" ],
      "venue" : "IEEE Trans. Inform. Theory,",
      "citeRegEx" : "Foygel and Mackey.,? \\Q2014\\E",
      "shortCiteRegEx" : "Foygel and Mackey.",
      "year" : 2014
    }, {
      "title" : "Group lasso with overlap and graph lasso",
      "author" : [ "L. Jacob", "G. Obozinski", "J.-P. Vert" ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Jacob et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jacob et al\\.",
      "year" : 2009
    }, {
      "title" : "Summing and Nuclear Norms in Banach Space Theory. Number 8 in London Mathematical Society Student Texts",
      "author" : [ "G.J.O. Jameson" ],
      "venue" : null,
      "citeRegEx" : "Jameson.,? \\Q1987\\E",
      "shortCiteRegEx" : "Jameson.",
      "year" : 1987
    }, {
      "title" : "Large cliques elude the Metropolis process",
      "author" : [ "M. Jerrum" ],
      "venue" : "Random Struct. Alg.,",
      "citeRegEx" : "Jerrum.,? \\Q1992\\E",
      "shortCiteRegEx" : "Jerrum.",
      "year" : 1992
    }, {
      "title" : "Generalized power method for sparse principal component analysis",
      "author" : [ "M. Journée", "Y. Nesterov", "P. Richtárik", "R. Sepulchre" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Journée et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Journée et al\\.",
      "year" : 2010
    }, {
      "title" : "Nuclear norm penalization and optimal rates for noisy matrix completion",
      "author" : [ "V. Koltchinskii", "K. Lounici", "A.B. Tsybakov" ],
      "venue" : "Ann. Stat.,",
      "citeRegEx" : "Koltchinskii et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Koltchinskii et al\\.",
      "year" : 2011
    }, {
      "title" : "Do semidefinite relaxations really solve sparse PCA",
      "author" : [ "R. Krauthgamer", "B. Nadler", "D. Vilenchik" ],
      "venue" : "Technical Report 1306:3690,",
      "citeRegEx" : "Krauthgamer et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Krauthgamer et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient sparse coding algorithms",
      "author" : [ "H. Lee", "A. Battle", "R. Raina", "A.Y. Ng" ],
      "venue" : "Adv. Neural. Inform. Process Syst.,",
      "citeRegEx" : "Lee et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2007
    }, {
      "title" : "Conditional gradient algorithms for rank-one matrix approximations with a sparsity constraint",
      "author" : [ "R. Luss", "M. Teboulle" ],
      "venue" : "SIAM Rev.,",
      "citeRegEx" : "Luss and Teboulle.,? \\Q2013\\E",
      "shortCiteRegEx" : "Luss and Teboulle.",
      "year" : 2013
    }, {
      "title" : "Deflation methods for sparse PCA",
      "author" : [ "L.W. Mackey" ],
      "venue" : "Adv. Neural. Inform. Process Syst.,",
      "citeRegEx" : "Mackey.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mackey.",
      "year" : 2009
    }, {
      "title" : "Online learning for matrix factorization and sparse coding",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Mairal et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mairal et al\\.",
      "year" : 2010
    }, {
      "title" : "Spectral bounds for sparse PCA: Exact and greedy algorithms",
      "author" : [ "B. Moghaddam", "Y. Weiss", "Sh. Avidan" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Moghaddam et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Moghaddam et al\\.",
      "year" : 2006
    }, {
      "title" : "Sparse regression as a sparse eigenvalue problem",
      "author" : [ "B. Moghaddam", "A. Gruber", "Y. Weiss", "S. Avidan" ],
      "venue" : "In Information Theory and Applications Workshop,",
      "citeRegEx" : "Moghaddam et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Moghaddam et al\\.",
      "year" : 2008
    }, {
      "title" : "A unified framework for highdimensional analysis of M-estimators",
      "author" : [ "S. N Negahban", "P. Ravikumar", "M. J Wainwright", "B. Yu" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "Negahban et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Negahban et al\\.",
      "year" : 2012
    }, {
      "title" : "Sharp mse bounds for proximal denoising",
      "author" : [ "S. Oymak", "B. Hassibi" ],
      "venue" : "Technical Report 1305.2714,",
      "citeRegEx" : "Oymak and Hassibi.,? \\Q2013\\E",
      "shortCiteRegEx" : "Oymak and Hassibi.",
      "year" : 2013
    }, {
      "title" : "Simultaneously structured models with application to sparse and low-rank matrices",
      "author" : [ "S. Oymak", "A. Jalali", "M. Fazel", "Y.C. Eldar", "B. Hassibi" ],
      "venue" : "Technical Report 1212.3753,",
      "citeRegEx" : "Oymak et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Oymak et al\\.",
      "year" : 2012
    }, {
      "title" : "The squared-error of generalized LASSO: A precise analysis",
      "author" : [ "S. Oymak", "C. Thrampoulidis", "B. Hassibi" ],
      "venue" : "In 51st Annual Allerton Conference on Communication,",
      "citeRegEx" : "Oymak et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Oymak et al\\.",
      "year" : 2013
    }, {
      "title" : "Estimation of simultaneously sparse and low-rank matrices",
      "author" : [ "E. Richard", "P.-A. Savalle", "N. Vayatis" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "Richard et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Richard et al\\.",
      "year" : 2012
    }, {
      "title" : "Intersecting singularities for multi-structured estimation",
      "author" : [ "E. Richard", "F. Bach", "J.-P. Vert" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning (ICML-13),",
      "citeRegEx" : "Richard et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Richard et al\\.",
      "year" : 2013
    }, {
      "title" : "Link prediction in graphs with autoregressive features",
      "author" : [ "E. Richard", "S. Gaïffas", "N. Vayatis" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Richard et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Richard et al\\.",
      "year" : 2014
    }, {
      "title" : "Convex Analysis",
      "author" : [ "R.T. Rockafellar" ],
      "venue" : null,
      "citeRegEx" : "Rockafellar.,? \\Q1997\\E",
      "shortCiteRegEx" : "Rockafellar.",
      "year" : 1997
    }, {
      "title" : "A coordinate gradient descent method for nonsmooth separable minimization",
      "author" : [ "P. Tseng", "S. Yun" ],
      "venue" : "Math. Program.,",
      "citeRegEx" : "Tseng and Yun.,? \\Q2009\\E",
      "shortCiteRegEx" : "Tseng and Yun.",
      "year" : 2009
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "R. Vershynin" ],
      "venue" : null,
      "citeRegEx" : "Vershynin.,? \\Q2012\\E",
      "shortCiteRegEx" : "Vershynin.",
      "year" : 2012
    }, {
      "title" : "Information-theoretic limits on sparsity recovery in the high-dimensional and noisy setting",
      "author" : [ "M.J. Wainwright" ],
      "venue" : "IEEE Trans. Inform. Theory,",
      "citeRegEx" : "Wainwright.,? \\Q2009\\E",
      "shortCiteRegEx" : "Wainwright.",
      "year" : 2009
    }, {
      "title" : "Provable subspace clustering: When LRR meets SSC",
      "author" : [ "Y.-X. Wang", "H. Xu", "C. Leng" ],
      "venue" : "Adv. Neural. Inform. Process Syst.,",
      "citeRegEx" : "Wang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Characterization of the subdifferential of some matrix norms",
      "author" : [ "G.A. Watson" ],
      "venue" : "Lin. Alg. Appl.,",
      "citeRegEx" : "Watson.,? \\Q1992\\E",
      "shortCiteRegEx" : "Watson.",
      "year" : 1992
    }, {
      "title" : "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis",
      "author" : [ "D.M. Witten", "R. Tibshirani", "T. Hastie" ],
      "venue" : "URL http://dx.doi.org/10.1093/biostatistics/kxp008",
      "citeRegEx" : "Witten et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Witten et al\\.",
      "year" : 2009
    }, {
      "title" : "Truncated power method for sparse eigenvalue problems",
      "author" : [ "X.-T. Yuan", "T. Zhang" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Yuan and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yuan and Zhang.",
      "year" : 2013
    }, {
      "title" : "Sparse principal component analysis",
      "author" : [ "H. Zou", "T. Hastie", "R. Tibshirani" ],
      "venue" : "J. Comput. Graph. Stat.,",
      "citeRegEx" : "Zou et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2006
    }, {
      "title" : "offers the possibility of constraining the estimator to lie in a cone C. In our case, C = Rm1×m2 , given the definition of γ we therefore have γ ≤ 2. The result follows from applying the theorem with θ2 = ζ(a",
      "author" : [ "Oymak" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "q,m1/k,m2/q ≥ M then n0 is large enough to ensure 1 − c1 exp(−c2n0) > 4 exp (−32/17). Then, according to Lemma 38, solving (28) with the norm Γμ fails to recover A = ab with probability at least 4 exp (−32/17)",
      "author" : [ "M Take" ],
      "venue" : "On the other hand, Amelunxen et al",
      "citeRegEx" : "Take,? \\Q2013\\E",
      "shortCiteRegEx" : "Take",
      "year" : 2013
    }, {
      "title" : "Annex C), for (61) we used the fact that for a standard normal random variable G",
      "author" : [ "Chandrasekaran" ],
      "venue" : null,
      "citeRegEx" : "Chandrasekaran,? \\Q2012\\E",
      "shortCiteRegEx" : "Chandrasekaran",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We compute slow rates and an upper bound on the statistical dimension Amelunxen et al. (2013) of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual l1-norm, trace norm and their combinations.",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 35,
      "context" : "A range of machine learning problems such as link prediction in graphs containing community structure (Richard et al., 2014), phase retrieval (Candès et al.",
      "startOffset" : 102,
      "endOffset" : 124
    }, {
      "referenceID" : 7,
      "context" : ", 2014), phase retrieval (Candès et al., 2013), subspace clustering (Wang et al.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 40,
      "context" : ", 2013), subspace clustering (Wang et al., 2013) or dictionary learning for sparse coding (Mairal et al.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 26,
      "context" : ", 2013) or dictionary learning for sparse coding (Mairal et al., 2010) amount to solve sparse matrix factorization problems, i.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 44,
      "context" : "Landmark applications of sparse matrix factorization are sparse principal components analysis (SPCA, d’Aspremont et al., 2007; Zou et al., 2006) or sparse canonical correlation analysis (SCCA, Witten et al.",
      "startOffset" : 94,
      "endOffset" : 144
    }, {
      "referenceID" : 27,
      "context" : "From a computational point of view, however, sparse matrix factorization is challenging since it typically leads to non-convex, NP-hard problems (Moghaddam et al., 2006).",
      "startOffset" : 145,
      "endOffset" : 169
    }, {
      "referenceID" : 6,
      "context" : "For instance, Berthet and Rigollet (2013) noted that solving sparse PCA with a single component is equivalent to the planted clique",
      "startOffset" : 14,
      "endOffset" : 42
    }, {
      "referenceID" : 19,
      "context" : "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix.",
      "startOffset" : 8,
      "endOffset" : 22
    }, {
      "referenceID" : 23,
      "context" : "A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010).",
      "startOffset" : 158,
      "endOffset" : 197
    }, {
      "referenceID" : 26,
      "context" : "A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010).",
      "startOffset" : 158,
      "endOffset" : 197
    }, {
      "referenceID" : 1,
      "context" : "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d’Aspremont et al., 2007, 2008).",
      "startOffset" : 102,
      "endOffset" : 162
    }, {
      "referenceID" : 25,
      "context" : "Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix.",
      "startOffset" : 143,
      "endOffset" : 157
    }, {
      "referenceID" : 18,
      "context" : "(2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations.",
      "startOffset" : 196,
      "endOffset" : 211
    }, {
      "referenceID" : 15,
      "context" : "Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014).",
      "startOffset" : 204,
      "endOffset" : 282
    }, {
      "referenceID" : 31,
      "context" : "Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014).",
      "startOffset" : 204,
      "endOffset" : 282
    }, {
      "referenceID" : 1,
      "context" : "(2013) prove that the SDP relaxations fail at finding the sparse principal component outside the favorable regime where a simple diagonal thresholding algorithm (Amini and Wainwright, 2009) works.",
      "startOffset" : 161,
      "endOffset" : 189
    }, {
      "referenceID" : 11,
      "context" : "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix. Many heuristics and relaxations have therefore been proposed, with and without theoretical guaranties, to approximatively solve the problems leading to sparse low-rank matrices. A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010). Despite these worst case computational hardness, simple generalizations of the power method have been proposed by Journée et al. (2010); Luss and Teboulle (2013); Yuan and Zhang (2013) for the sparse PCA problem with a single component.",
      "startOffset" : 9,
      "endOffset" : 648
    }, {
      "referenceID" : 11,
      "context" : "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix. Many heuristics and relaxations have therefore been proposed, with and without theoretical guaranties, to approximatively solve the problems leading to sparse low-rank matrices. A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010). Despite these worst case computational hardness, simple generalizations of the power method have been proposed by Journée et al. (2010); Luss and Teboulle (2013); Yuan and Zhang (2013) for the sparse PCA problem with a single component.",
      "startOffset" : 9,
      "endOffset" : 674
    }, {
      "referenceID" : 11,
      "context" : "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix. Many heuristics and relaxations have therefore been proposed, with and without theoretical guaranties, to approximatively solve the problems leading to sparse low-rank matrices. A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010). Despite these worst case computational hardness, simple generalizations of the power method have been proposed by Journée et al. (2010); Luss and Teboulle (2013); Yuan and Zhang (2013) for the sparse PCA problem with a single component.",
      "startOffset" : 9,
      "endOffset" : 697
    }, {
      "referenceID" : 11,
      "context" : "problem (Jerrum, 1992), a notoriously hard problem when the size of the support is smaller than the square root of size of the matrix. Many heuristics and relaxations have therefore been proposed, with and without theoretical guaranties, to approximatively solve the problems leading to sparse low-rank matrices. A popular procedure is to alternatively optimize over the left and right factors in the factorization, formulating each step as a convex optimization problem (Lee et al., 2007; Mairal et al., 2010). Despite these worst case computational hardness, simple generalizations of the power method have been proposed by Journée et al. (2010); Luss and Teboulle (2013); Yuan and Zhang (2013) for the sparse PCA problem with a single component. These algorithms perform well empirically and have been proved to be efficient theoretically under mild conditions by Yuan and Zhang (2013). Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d’Aspremont et al.",
      "startOffset" : 9,
      "endOffset" : 889
    }, {
      "referenceID" : 1,
      "context" : "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d’Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations.",
      "startOffset" : 103,
      "endOffset" : 497
    }, {
      "referenceID" : 1,
      "context" : "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d’Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations.",
      "startOffset" : 103,
      "endOffset" : 621
    }, {
      "referenceID" : 1,
      "context" : "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d’Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations. Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014). This penalty term can be related to the SDP relaxations of d’Aspremont et al. (2007, 2008) that penalize the trace and the element-wise l1 norm of the positive semi-definite unknown. The statistical performance of these basic combinations of the two convex criteria has however been questioned by Krauthgamer et al. (2013); Oymak et al.",
      "startOffset" : 103,
      "endOffset" : 1387
    }, {
      "referenceID" : 1,
      "context" : "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d’Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations. Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014). This penalty term can be related to the SDP relaxations of d’Aspremont et al. (2007, 2008) that penalize the trace and the element-wise l1 norm of the positive semi-definite unknown. The statistical performance of these basic combinations of the two convex criteria has however been questioned by Krauthgamer et al. (2013); Oymak et al. (2012). Oymak et al.",
      "startOffset" : 103,
      "endOffset" : 1408
    }, {
      "referenceID" : 1,
      "context" : "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d’Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations. Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014). This penalty term can be related to the SDP relaxations of d’Aspremont et al. (2007, 2008) that penalize the trace and the element-wise l1 norm of the positive semi-definite unknown. The statistical performance of these basic combinations of the two convex criteria has however been questioned by Krauthgamer et al. (2013); Oymak et al. (2012). Oymak et al. (2012) showed that for compressed sensing applications, no convex combination of the two norms improves over each norm taken alone.",
      "startOffset" : 103,
      "endOffset" : 1429
    }, {
      "referenceID" : 1,
      "context" : "Several semidefinite programming (SDP) convex relaxations of the same problem have also been proposed (Amini and Wainwright, 2009; d’Aspremont et al., 2007, 2008). Based on the rank one approximate solutions, computing multiple principal components of the data is commonly done though successive deflations (Mackey, 2009) of the input matrix. Recently, several authors have investigated the possibility to formulate sparse matrix factorization as a convex optimization problem. Bach et al. (2008) showed that the convex relaxation of a number of natural sparse factorization are too coarse too succeed, while Bach (2013) investigated several convex formulations involving nuclear norms (Jameson, 1987), similar to the ones we investigate in this paper, and their SDP relaxations. Several authors also investigated the performance of regularizing a convex loss with linear combinations of the l1 norm and the trace norm, naturally leading to a matrix which is both sparse and low-rank (Doan and Vavasis, 2013; Oymak et al., 2012; Richard et al., 2012, 2013, 2014). This penalty term can be related to the SDP relaxations of d’Aspremont et al. (2007, 2008) that penalize the trace and the element-wise l1 norm of the positive semi-definite unknown. The statistical performance of these basic combinations of the two convex criteria has however been questioned by Krauthgamer et al. (2013); Oymak et al. (2012). Oymak et al. (2012) showed that for compressed sensing applications, no convex combination of the two norms improves over each norm taken alone. Krauthgamer et al. (2013) prove that the SDP relaxations fail at finding the sparse principal component outside the favorable regime where a simple diagonal thresholding algorithm (Amini and Wainwright, 2009) works.",
      "startOffset" : 103,
      "endOffset" : 1580
    }, {
      "referenceID" : 9,
      "context" : "2 two new atomic norms for matrices (Chandrasekaran et al., 2012).",
      "startOffset" : 36,
      "endOffset" : 65
    }, {
      "referenceID" : 17,
      "context" : "3 an equivalent characterization of the norms as nuclear norms, in the sense of Jameson (1987), highlighting in particular a link to the k-support norm of Argyriou et al.",
      "startOffset" : 80,
      "endOffset" : 95
    }, {
      "referenceID" : 2,
      "context" : "3 an equivalent characterization of the norms as nuclear norms, in the sense of Jameson (1987), highlighting in particular a link to the k-support norm of Argyriou et al. (2012). • Using these norms to estimate sparse low-rank matrices (Section 3).",
      "startOffset" : 155,
      "endOffset" : 178
    }, {
      "referenceID" : 2,
      "context" : "3 we relate these matrix norms to vector norms using the concept of nuclear norms, establishing in particular a connection of the (k, q)-trace norm for matrices with the k-support norm of Argyriou et al. (2012), and the (k, q)-CUT norm to the vector k-norm, defined as the sum of the k largest components in absolute value of a vector (Bhatia, 1997, Exercise II.",
      "startOffset" : 188,
      "endOffset" : 211
    }, {
      "referenceID" : 44,
      "context" : "For example, the standard rank-1 SPCA problem consists in finding the symmetric matrix with (k, k)-rank equal to 1 and providing the best approximation of the sample covariance matrix (Zou et al., 2006).",
      "startOffset" : 184,
      "endOffset" : 202
    }, {
      "referenceID" : 9,
      "context" : "They are both instances of the atomic norms introduced by Chandrasekaran et al. (2012), which we first review.",
      "startOffset" : 58,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : "Chandrasekaran et al. (2012) show that the atomic norm induced by A is indeed a norm, which can be rewritten as ‖x‖A = inf { ∑",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 36,
      "context" : "see Rockafellar (1997), p.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "Our choice of terminology is motivated by the following relation of our norm to the CUT-polytope: in the case k = m1 and q = m2, the unit ball of Ω̃k,q coincides (up to a scaling factor of √ m1m2) with the polytope known as the CUT polytope of the complete graph on n vertices (Deza and Laurent, 1997), defined by CUT = conv {ab , a ∈ {±1}1 , b ∈ {±1}2} .",
      "startOffset" : 277,
      "endOffset" : 301
    }, {
      "referenceID" : 18,
      "context" : "3 Equivalent nuclear norms built upon vector norms In this section we show that the (k, q)-trace norm (Definition 4) and the (k, q)-CUT norm (Definition 8), which we defined as atomic norms induced by specific atom sets, can alternatively be seen as instances of nuclear norms considered by Jameson (1987). For that purpose it is useful to recall the general definition of nuclear norms and the characterization of the corresponding dual norms as formulated in Jameson (1987, Propositions 1.",
      "startOffset" : 291,
      "endOffset" : 306
    }, {
      "referenceID" : 2,
      "context" : "The (k, q)-trace norm is the nuclear norm induced by θk on R m1 and θq on Rm2 , where for any j ≥ 1, θj is the j-support norm introduced by Argyriou et al. (2012). 2.",
      "startOffset" : 140,
      "endOffset" : 163
    }, {
      "referenceID" : 2,
      "context" : "For the sake of completeness, let us recall the closed-form expression of the k-support norm θk shown by Argyriou et al. (2012). For any vector w ∈ Rp, let w̄ ∈ Rp be the vector obtained by sorting the entries of w by decreasing order of absolute values.",
      "startOffset" : 105,
      "endOffset" : 128
    }, {
      "referenceID" : 5,
      "context" : "It is however known since Jameson (1987) (see also Bach, 2013; Bach et al., 2012) that the nuclear norm induced by vector l1-norm is simply the l1 of the matrix which fails to induce low rank (except in the very sparse case).",
      "startOffset" : 41,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : "It is however known since Jameson (1987) (see also Bach, 2013; Bach et al.",
      "startOffset" : 26,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "It is however known since Jameson (1987) (see also Bach, 2013; Bach et al., 2012) that the nuclear norm induced by vector l1-norm is simply the l1 of the matrix which fails to induce low rank (except in the very sparse case). However Bach et al. (2012) proposed nuclear norms associated with vectors norms that are similar to the elastic net penalty.",
      "startOffset" : 51,
      "endOffset" : 253
    }, {
      "referenceID" : 7,
      "context" : "Quadratic regression combined with additional constraints on Z is closely related to phase retrieval (Candès et al., 2013).",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 40,
      "context" : "This idea, exploited recently by Wang et al. (2013) implies that Z is a sum of low rank sparse matrices; and this property still holds if the clustering is unknown.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : "4 Sparse PCA In sparse PCA (d’Aspremont et al., 2007; Witten et al., 2009; Zou et al., 2006), one tries to approximate an empirical covariance matrix Σ̂n by a low-rank matrix with sparse factors.",
      "startOffset" : 27,
      "endOffset" : 92
    }, {
      "referenceID" : 42,
      "context" : "4 Sparse PCA In sparse PCA (d’Aspremont et al., 2007; Witten et al., 2009; Zou et al., 2006), one tries to approximate an empirical covariance matrix Σ̂n by a low-rank matrix with sparse factors.",
      "startOffset" : 27,
      "endOffset" : 92
    }, {
      "referenceID" : 44,
      "context" : "4 Sparse PCA In sparse PCA (d’Aspremont et al., 2007; Witten et al., 2009; Zou et al., 2006), one tries to approximate an empirical covariance matrix Σ̂n by a low-rank matrix with sparse factors.",
      "startOffset" : 27,
      "endOffset" : 92
    }, {
      "referenceID" : 25,
      "context" : "In contrast to sequential approaches that estimate the principal components one by one (Mackey, 2009), this formulation requires to find simultaneously a set of factors which are complementary to one another in order to explain as much variance as possible.",
      "startOffset" : 87,
      "endOffset" : 101
    }, {
      "referenceID" : 28,
      "context" : "which it is known to be NP-hard (Moghaddam et al., 2008).",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 29,
      "context" : "Obviously the comparison of upper bounds is not enough to conclude to the superiority of (k, q)-trace norm and, admittedly, the problem of denoising considered here is a special instance of linear regression in which the design matrix is the identity, and, since this is a case in which the design is trivially incoherent, it is possible to obtain fast rates for decomposable norms such as the l1 or trace norm (Negahban et al., 2012); however, slow rates are still valid in the presence of an incoherent design, or when the signal to recover is only weakly sparse, which is not the case for the fast rates.",
      "startOffset" : 411,
      "endOffset" : 434
    }, {
      "referenceID" : 0,
      "context" : "We present in the next section more involved results, based on lower and upper bounds on the so-called statistical dimension of the different norms (Amelunxen et al., 2013), a measure which is closely related to Gaussian widths.",
      "startOffset" : 148,
      "endOffset" : 172
    }, {
      "referenceID" : 9,
      "context" : "The gain in efficiency can be quantified by appropriate measures of width of the tangent cone such as the Gaussian width of its intersection with a unit Euclidean ball (Chandrasekaran et al., 2012), or the closely related concept of statistical dimension of the cone, proposed by Amelunxen et al.",
      "startOffset" : 168,
      "endOffset" : 197
    }, {
      "referenceID" : 31,
      "context" : "In particular, we will consider the norms Ωk,q, Ω̃k,q and linear combinations of the l1 and trace norms, which have been used in the literature to infer sparse lowrank matrices (Oymak et al., 2012; Richard et al., 2012).",
      "startOffset" : 177,
      "endOffset" : 219
    }, {
      "referenceID" : 33,
      "context" : "In particular, we will consider the norms Ωk,q, Ω̃k,q and linear combinations of the l1 and trace norms, which have been used in the literature to infer sparse lowrank matrices (Oymak et al., 2012; Richard et al., 2012).",
      "startOffset" : 177,
      "endOffset" : 219
    }, {
      "referenceID" : 0,
      "context" : "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al.",
      "startOffset" : 117,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al. (2012); Foygel and Mackey (2014); Oymak et al.",
      "startOffset" : 117,
      "endOffset" : 171
    }, {
      "referenceID" : 0,
      "context" : "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al. (2012); Foygel and Mackey (2014); Oymak et al.",
      "startOffset" : 117,
      "endOffset" : 197
    }, {
      "referenceID" : 0,
      "context" : "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al. (2012); Foygel and Mackey (2014); Oymak et al. (2013) to quantify the statistical power of a convex nonsmooth regularizer used as a constraint or penalty.",
      "startOffset" : 117,
      "endOffset" : 218
    }, {
      "referenceID" : 0,
      "context" : "2 Performance through the statistical dimension Powerful results from asymptotic geometry have recently been used by Amelunxen et al. (2013); Chandrasekaran et al. (2012); Foygel and Mackey (2014); Oymak et al. (2013) to quantify the statistical power of a convex nonsmooth regularizer used as a constraint or penalty. These results rely essentially on the fact that if the tangent cone of the regularizer at a point of interest Z is thiner, then the regularizer is more efficient at solving problems of denoising, demixing and compressed sensing of Z. The gain in efficiency can be quantified by appropriate measures of width of the tangent cone such as the Gaussian width of its intersection with a unit Euclidean ball (Chandrasekaran et al., 2012), or the closely related concept of statistical dimension of the cone, proposed by Amelunxen et al. (2013). In this section, we study the statistical dimensions induced by different matrix norms in order to compare their theoretical properties for exact or approximate recovery of sparse low-rank matrices.",
      "startOffset" : 117,
      "endOffset" : 857
    }, {
      "referenceID" : 9,
      "context" : "14 in Chandrasekaran et al. (2012) and from the upper bound log (m k ) ≤ k(1 + log(m/k)), that Proposition 21 For any A ∈ Ãk,q, we have S(A, Ω̃k,q) ≤ 16(k + q) + 9 ( k log m1 k + q log m2 q ) .",
      "startOffset" : 6,
      "endOffset" : 35
    }, {
      "referenceID" : 39,
      "context" : "In the vector case, for example, it is known that the recovery of a sparse vector β with support I0 depends on its smallest coefficient βmin = mini∈I0 β 2 i (Wainwright, 2009).",
      "startOffset" : 157,
      "endOffset" : 175
    }, {
      "referenceID" : 8,
      "context" : "This result is actually stated informally for the special case of k = q = √ m = with m = m1 = m2 in the context of a discussion of the planted clique problem in Chandrasekaran and Jordan (2013).",
      "startOffset" : 161,
      "endOffset" : 194
    }, {
      "referenceID" : 31,
      "context" : "Finally, regarding the combination Γμ of the l1 norm and of the trace norm, Oymak et al. (2012) has shown that it does not improve rates up to constants over the best of the two norms.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 31,
      "context" : "In that case, we see that, as stated by Oymak et al. (2012), Γμ does not bring any improvement over the l1 and trace norms taken imdividually, and in particular has a worse statistical dimension than Ωk,q and Ω̃k,q.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "In the lasso case (k = 1), we recover the standard bound (Chandrasekaran et al., 2012):",
      "startOffset" : 57,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "Working set algorithms (Bach et al., 2011, Chap. 6) are typically useful to speed up algorithm for sparsity inducing regularizer; they have been used notably in the case of the overlapping group Lasso of Jacob et al. (2009) which is also naturally formulated via latent components.",
      "startOffset" : 24,
      "endOffset" : 224
    }, {
      "referenceID" : 41,
      "context" : "From the characterization of the subdifferential of the trace norm (Watson, 1992), writing Z = U ΣV (IJ) the SVD of Z, this is equivalent to, for all (I, J) in S, either Z 6=0 and ∇L(Z)IJ = −λ ( U V (IJ) ⊤ +A )",
      "startOffset" : 67,
      "endOffset" : 81
    }, {
      "referenceID" : 37,
      "context" : "Problem (PS) is solved easily using the approximate block coordinate descent of Tseng and Yun (2009) (see",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 20,
      "context" : "In our numerical experiments we use a truncated power iteration (TPI) method, also called TPower, GPower or CongradU in the PSD case (Journée et al., 2010; Luss and Teboulle, 2013; Yuan and Zhang, 2013), which has been proved recently by Yuan and Zhang (2013) to provide accurate solution in reasonable computational time under RIP type of conditions.",
      "startOffset" : 133,
      "endOffset" : 202
    }, {
      "referenceID" : 24,
      "context" : "In our numerical experiments we use a truncated power iteration (TPI) method, also called TPower, GPower or CongradU in the PSD case (Journée et al., 2010; Luss and Teboulle, 2013; Yuan and Zhang, 2013), which has been proved recently by Yuan and Zhang (2013) to provide accurate solution in reasonable computational time under RIP type of conditions.",
      "startOffset" : 133,
      "endOffset" : 202
    }, {
      "referenceID" : 43,
      "context" : "In our numerical experiments we use a truncated power iteration (TPI) method, also called TPower, GPower or CongradU in the PSD case (Journée et al., 2010; Luss and Teboulle, 2013; Yuan and Zhang, 2013), which has been proved recently by Yuan and Zhang (2013) to provide accurate solution in reasonable computational time under RIP type of conditions.",
      "startOffset" : 133,
      "endOffset" : 202
    }, {
      "referenceID" : 20,
      "context" : "In our numerical experiments we use a truncated power iteration (TPI) method, also called TPower, GPower or CongradU in the PSD case (Journée et al., 2010; Luss and Teboulle, 2013; Yuan and Zhang, 2013), which has been proved recently by Yuan and Zhang (2013) to provide accurate solution in reasonable computational time under RIP type of conditions.",
      "startOffset" : 134,
      "endOffset" : 260
    }, {
      "referenceID" : 43,
      "context" : "It has been proved by Yuan and Zhang (2013) that under some conditions the problem can be solved in linear time.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 43,
      "context" : "It has been proved by Yuan and Zhang (2013) that under some conditions the problem can be solved in linear time. If the conditions hold at every step of gradient, the overall cost of an iteration can be cast into the cost of evaluating the gradient and the evaluation of thin SVDs: O(k2q). Evaluating the gradient has a cost dependent on the risk function L. This cost for usual applications is O(m1m2). So assuming the RIP conditions required by Yuan and Zhang (2013) hold, the cost of Algorithm 2 is dominated by matrix-vector multiplications so of the order O(m1m2).",
      "startOffset" : 22,
      "endOffset" : 469
    }, {
      "referenceID" : 30,
      "context" : "Fro σ2 is a good estimate of the statistical dimension, since Oymak and Hassibi (2013) show that",
      "startOffset" : 62,
      "endOffset" : 87
    } ],
    "year" : 2017,
    "abstractText" : "Based on a new atomic norm, we propose a new convex formulation for sparse matrix factorization problems in which the number of nonzero elements of the factors is assumed fixed and known. The formulation counts sparse PCA with multiple factors, subspace clustering and low-rank sparse bilinear regression as potential applications. We compute slow rates and an upper bound on the statistical dimension Amelunxen et al. (2013) of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual l1-norm, trace norm and their combinations. Even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes, we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results.",
    "creator" : "LaTeX with hyperref package"
  }
}