{
  "name" : "1204.6703.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Two SVDs Suffice: Spectral decompositions for probabilistic topic modeling and latent Dirichlet allocation",
    "authors" : [ "Animashree Anandkumar", "Dean P. Foster", "Daniel Hsu", "Sham M. Kakade", "Yi-Kai Liu" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 4.\n67 03\nv1 [\ncs .L\nWe provide a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of mixture models, including the popular latent Dirichlet allocation (LDA) model. For LDA, the procedure correctly recovers both the topic probability vectors and the prior over the topics, using only trigram statistics (i.e. third order moments, which may be estimated with documents containing just three words). The method, termed Excess Correlation Analysis (ECA), is based on a spectral decomposition of low order moments (third and fourth order) via two singular value decompositions (SVDs). Moreover, the algorithm is scalable since the SVD operations are carried out on k × k matrices, where k is the number of latent factors (e.g. the number of topics), rather than in the d-dimensional observed space (typically d ≫ k)."
    }, {
      "heading" : "1 Introduction",
      "text" : "There is general agreement that there are multiple unobserved or latent factors affecting observed data. Mixture models offer a powerful framework to incorporate the effects of these latent variables. A family of mixture models, popularly known as topic models, has generated broad interest on both theoretical and practical fronts.\nTopic models incorporate latent variables, the topics, to explain the observed co-occurrences of words in documents. They posit that each document has a mixture of active topics (possibly\n∗Contributions to this work by NIST, an agency of the US government, are not subject to copyright laws.\nsparse) and that each active topic determines the occurrence of words in the document. Usually, a Dirichlet prior is assigned to the distribution of topics in documents, giving rise to the so-called latent Dirichlet allocation (LDA) (Blei et al., 2003). These models possess a rich representational power since they allow for the words in each document to be generated from more than one topic (i.e. the model permits documents to be about multiple topics). This increased representational power comes at the cost of a more challenging unsupervised estimation problem, when only the words are observed and the corresponding topics are hidden.\nIn practice, the most common estimation procedures are based on finding maximum likelihood (ML) estimates, through either local search or sampling based methods, e.g. ExpectationMaximization (EM) (Redner and Walker, 1984), Gibbs sampling (Asuncion et al., 2011), and variational approaches (Hoffman et al., 2010). Another body of tools is based on matrix factorization (Hofmann, 1999; Lee and Seung, 1999). For document modeling, typically, the goal is to form a sparse decomposition of a term by document matrix (which represents the word counts in each document) into two parts: one which specifies the active topics in each document and the other which specifies the distributions of words under each topic.\nThis work provides an alternative approach to parameter recovery based on the method of moments (Lindsay, 1989; Lindsay and Basak, 1993), which attempts to match the observed moments with those posited by the model. Our approach does this efficiently through a spectral decomposition of the observed moments through two singular value decompositions. This method is simple and efficient to implement, based on only low order moments (third or fourth order), and is guaranteed to recover the parameters of a wide class of mixture models, including the LDA model. We exploit exchangeability of the observed variables and, more generally, the availability of multiple views drawn independently from the same hidden component."
    }, {
      "heading" : "1.1 Summary of Contributions",
      "text" : "We present an approach known as Excess Correlation Analysis (ECA) based on the knowledge of low order moments between the observed variables, assumed to be exchangeable (or, more generally, drawn from a multi-view mixture model). ECA differs from Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) in that it is based on two singular value decompositions: the first SVD whitens the data (based on the correlation between two variables) and the second SVD utilizes higher order moments (based on third or fourth order) to find directions which exhibit moments that are in excess of those suggested by a Gaussian distribution. Both SVDs are performed on matrices of size k × k, where k is the number of latent factors, making the algorithm scalable (typically the dimension of the observed space d ≫ k).\nThe method is applicable to a wide class of mixture models including exchangeable and multiview models. We first consider the class of exchangeable variables with independent latent factors, such as a latent Poisson mixture model (a natural Poisson model for generating the sentences in a document, analogous to LDA’s multinomial model for generating the words in a document). We establish that a spectral decomposition, based on third or fourth order central moments, recovers the parameters for this model class. We then consider latent Dirichlet allocation and show that a spectral decomposition of a modified third order moment (exactly) recovers both the probability distributions over words for each topic and the Dirichlet prior. Note that to obtain third order moments, it suffices for documents to contain just 3 words. Finally, we present extensions to multiview models, where multiple views drawn independently from the same latent factor are available. This includes the case of both pure topic models (where only one active topic is present in each\ndocument) and discrete hidden Markov models. For this setting, we establish that ECA correctly recovers the parameters and is simpler than the eigenvector decomposition methods of Anandkumar et al. (2012).\nFinally, while our presentation focuses on utilizing known statistics of the observed moments to emphasize the correctness of the methodology, “plug-in” moment estimates can be used with sampled data. Section 5 provides a basic outline of the statistical complexity of the method1."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "There are relatively few provably correct methods for learning topic models when documents contain more than one topic; the recent work of Arora et al. (2012) provides a notable exception. Arora et al. (2012) provides a non-negative matrix factorization approach, under natural separability assumptions. These results also apply to certain settings with correlated topics. Our approach does not require this assumption 2, and our algorithm is affine invariant.\nThe underlying approach taken is a certain diagonalization technique of the observed moments. We know of at least two different settings which utilize this idea for parameter estimation.\nChang (1996) utilizes eigenvector methods for discrete Markov models of evolution, where the models involve multinomial distributions. The idea has been extended to other discrete mixture models such as discrete hidden Markov models (HMMs) and mixture models with single active topics (see Mossel and Roch (2006); Hsu et al. (2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models.\nThe second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools.\nThe underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996). In particular, we show that the “topic” modeling problem exhibits a rather simple algebraic solution, where only two SVDs suffice for parameter estimation. Moreover, this approach also simplifies the algorithms in Mossel and Roch (2006); Hsu et al. (2009); Anandkumar et al. (2012), in that the eigenvector methods are no longer necessary (e.g. the approach leads to methods for parameter estimation in HMMs with only two SVDs rather than using eigenvector approaches, as in previous work).\n1Showing that estimating the third order moments is not as difficult as it might naively seem since we only need a k × k matrix to be accurate. For a detailed discussion of these techniques, refer to Anandkumar et al. (2012).\n2The underlying intuition and idea of separability in Arora et al. (2012) may help for practical guidance in the choice of θ to help reduce the sample size, in terms of the k dependence. Theoretically, our choice of θ is rather robust.\nFurthermore, the exchangeability assumption permits us to have arbitrary noise models (rather than additive Gaussian noise, which are not appropriate for multinomial and other discrete distributions). A key technical contribution is that we show how the basic diagonalization approach can be adapted to Dirichlet models, through a rather careful construction. This construction bridges the gap between the single topic models (as in Chang (1996); Anandkumar et al. (2012)) and the independent factor model.\nMore generally, the multi-view approach has been exploited in previous works for semi-supervised learning and for learning mixtures of well-separated distributions (e.g as in Ando and Zhang (2007); Kakade and Foster (2007); Chaudhuri and Rao (2008); Chaudhuri et al. (2009)). These previous works essentially use variants of canonical correlation analysis (Hotelling, 1935) between two views. This work shows that having a third view of the data permits rather simple estimation procedures with guaranteed parameter recovery."
    }, {
      "heading" : "2 The Exchangeable and Multi-view Models",
      "text" : "We have a random vector h = (h1, h2, . . . , hk) ⊤ ∈ Rk. This vector specifies the latent factors (i.e. the hidden state), where hi specifies the value taken by i-th factor. Denote the variance of hi as\nσ2i = E[(hi − E[hi])2]\nwhich we assume to be strictly positive, for each i, and denote the higher l-th central moments of hi as:\nµi,l := E[(hi − E[hi])l]\nAt most, we only use the first four moments in our analysis. Suppose we also have a sequence of exchangeable random vectors {x1, x2, x3, x4, . . . } ∈ Rd; these are considered to be the observed variables. Assume throughout that d ≥ k; that x1, x2, x3, x4, . . . ∈ Rd are conditionally independent given h; and there exists a matrix O ∈ Rd×k such that\nE[xv|h] = Oh\nfor each v ∈ {1, 2, 3, 4, . . . }. Throughout,\nAssumption 2.1. Suppose that O is full rank.\nThis is a mild assumption, which allows for identifiability of the columns of O. The goal is to estimate the matrix O, sometimes referred to as the topic matrix.\nImportantly, we make no assumptions on the noise model. In particular, we do not assume that the noise is additive (or that the noise is independent of h)."
    }, {
      "heading" : "2.1 Independent Latent Factors",
      "text" : "Here, suppose that h has a product distribution, i.e. each component of hi is independent from the rest. Two important examples of this setting are as follows:\n(Multiple) mixtures of Gaussians: Suppose xv = Oh+ η, where η is Gaussian noise and h is a binary vector (under a product distribution). Here, the i-th column Oi can be considered to be the mean of the i-th Gaussian component. This generalizes the classic mixture of k-Gaussians,\nas the model now permits any number of Gaussians to be responsible for generating the hidden state (i.e. h is permitted to be any of the 2k vectors on the hypercube, while in the classic mixture problem, only one component is responsible). We may also allow η to be heteroskedastic (i.e. the noise may depend on h, provided the linearity assumption E[xv|h] = Oh holds.)\n(Multiple) mixtures of Poissons: Suppose [Oh]j specifies the Poisson rate of counts for [xv]j . For example, xv could be a vector of word counts in the v-th sentence of a document (where x1, x2, . . . are words counts of a sequence sentences). Here, O would be a matrix with positive entries, and hi would scale the rate at which topic i generates words in a sentence (as specified by the i-th column of O). The linearity assumption is satisfied as E[xv|h] = Oh (note the noise is not additive in this case). Here, multiple topics may be responsible for generating the words in each sentence. This model provides a natural variant of LDA, where the distribution over h is a product distribution (while in LDA, h is a probability vector)."
    }, {
      "heading" : "2.2 The Dirichlet Model",
      "text" : "Now suppose the hidden state h is a distribution itself, with a density specified by the Dirichlet distribution with parameter α ∈ Rk+ (α is a strictly positive real vector). We often think of h as a distribution over topics. Precisely, the density of h ∈ ∆k−1 (where the probability simplex ∆k−1 denotes the set of possible distributions over k outcomes) is specified by:\npα(h) := 1\nZ(α)\nk∏\ni=1\nhαi−1i\nwhere\nZ(α) :=\n∏k i=1 Γ(αi)\nΓ(α0)\nand α0 := α1 + α2 + · · ·+ αk .\nIntuitively, α0 (the sum of the “pseudo-counts”) is a crude measure of the uniformity of the distribution. As α0 → 0, the distribution degenerates to one over pure topics (i.e. the limiting density is one in which, with probability 1, precisely one coordinate of h is 1 and the rest are 0).\nLatent Dirichlet Allocation: LDA makes the further assumption that each random variable x1, x2, x3, . . . takes on discrete values out of d outcomes (e.g. xv represents what the v-th word in a document is, so d represents the number of words in the language). Each column of O represents a distribution over the outcomes (e.g. these are the topic probabilities). The sampling procedure is specified as follows: First, h is sampled according to the Dirichlet distribution. Then, for each v, independently sample i ∈ {1, 2, . . . k} according to h, and, finally, sample xv according to the i-th column of O. Observe this model falls into our setting: represent xv with a “hot” encoding where [xv]j = 1 if and only if the v-th outcome is the j-th word in the vocabulary. Hence, Pr([xv ]j = 1|h) = [Oh]j and E[xv|h] = Oh. (Again, the noise model is not additive)."
    }, {
      "heading" : "2.3 The Multi-View Model",
      "text" : "The multi-view setting can be considered an extension of the exchangeable model. Here, the random vectors {x1, x2, x3, . . . } are of dimensions d1, d2, d3, . . . . Instead of a single O matrix, suppose for\neach v ∈ {1, 2, 3, . . . } there exists an Ov ∈ Rdv×k such that\nE[xv|h] = Ovh\nThroughout,\nAssumption 2.2. Suppose that Ov is full rank for each v.\nEven though the variables are no longer exchangeable, the setting shares much of the statistical structure as the exchangeable one; furthermore, it allows for significantly richer models. For example, Anandkumar et al. (2012) consider a special case of this multi-view model (where there is only one topic present in h) for the purposes of learning hidden Markov models.\nA simple factorial HMM: Here, suppose we have a time series of random hidden vectors h1, h2, h3, . . . and observations x1, x2, x3, . . . (we slightly abuse notation as h1 is a vector). Assume that each factor [ht]i ∈ {−1, 1}. The model parameters and evolution are specified as follows: We have an initial (product) distribution over the first h1. The “factorial” assumption we make is that each factor [ht]i evolves independently; in particular, for each component i, there are (time independent) transition probabilities pi,1→−1 and pi,1→−1. Also suppose that E[xt|ht] = Oht (where, again, O does not depend on the time).\nTo learn this model, consider the first three observations x1, x2, x3. We can embed this three timestep model into the multiview model using a single hidden state, namely h2, and, with an appropriate construction (of O1, O2, O3 and means shifts of xv to make the linearity assumption hold). Furthermore, if we recover O1, O2, O3 we can recover O and the transition model. See Anandkumar et al. (2012) for further discussion of this idea (for the single topic case)."
    }, {
      "heading" : "3 Identifiability",
      "text" : "The underlying question here is: what may we hope to recover about O with only knowledge of the distribution on x1, x2, x3, . . . . At best, we could only recover the columns of O up to permutation. At the other extreme, suppose no a priori knowledge of the distribution of h is assumed (e.g. it may not even be a product distribution). Here, at best, we can only recover the range of O. In particular, suppose h is distributed according to a multivariate Gaussian, then clearly the columns of O are not identifiable. To see this, transform O to OM (where M is any k × k invertible matrix) and transform the distribution on h (by M−1); after this transformation, the distribution over xv is unaltered and the distribution on h is still a multivariate Gaussian. Hence, O and OM are indistinguishable from any observable statistics. (These issues are well understood in setting of independent source separation, for additive noise models without exchangeable variables. See Comon and Jutten (2010)).\nThus, for the columns of O to be identifiable, the distribution on hmust have some non-Gaussian statistical properties. We consider three cases. In the independent factor model, we consider the cases when h is skewed and when h has excess kurtosis. We also consider the case that h is Dirichlet distributed."
    }, {
      "heading" : "4 Excess Correlation Analysis (ECA)",
      "text" : "We now present exact and efficient algorithms for recovering O. The algorithm is based on two singular value decompositions: the first SVD whitens the data (based on the correlation between\nAlgorithm 1 ECA, with skewed factors\nInput: vector θ ∈ Rk; the moments Pairs and Triples(η)\n1. Dimensionality Reduction: Find a matrix U ∈ Rd×k such that\nRange(U) = Range(Pairs).\n(See Remark 1 for a fast procedure.)\n2. Whiten: Find V ∈ Rk×k so V ⊤(U⊤ PairsU)V is the k × k identity matrix. Set:\nW = UV\n3. SVD: Let Λ be the set of (right) singular vectors, with unique singular values, of\nW⊤ Triples(Wθ)W\n4. Reconstruct: Return the set Ô:\nÔ = { (W+)⊤λ : λ ∈ Λ}\nwhere W+ is the pseudo-inverse (see Eq 1).\ntwo variables) and the second SVD is carried out on higher order moments (based on third or fourth order). We start with the case of independent factors, as these algorithms make the basic diagonalization approach clear.\nAs discussed in the Introduction, these approaches can been seen as extensions of the methodologies in Chang (1996); Cardoso and Comon (1996). Furthermore, as we shall see, the Dirichlet distribution bridges between the single topic models (as in Chang (1996); Anandkumar et al. (2012)) and the independent factor model.\nThroughout, we use A+ to denote the pseudo-inverse:\nA+ = (A⊤A)−1A⊤ (1)\nfor a matrix A with linearly independent columns (this allows us to appropriately invert non-square matrices)."
    }, {
      "heading" : "4.1 Independent and Skewed Latent Factors",
      "text" : "Denote the pairwise and threeway correlations as:\nµ := E[x1]\nPairs := E[(x1 − µ)(x2 − µ)⊤] Triples := E[(x1 − µ)⊗ (x2 − µ)⊗ (x3 − µ)]\nThe dimensions of Pairs and Triples are d2 and d3, respectively. It is convenient to project Triples to a matrix as follows:\nTriples(η) := E[(x1 − µ)(x2 − µ)⊤〈η, x3 − µ〉]\nRoughly speaking, we can think of Triples(η) as a reweighing of a cross covariance (by 〈η, x3 −µ〉). In addition to O not being identifiable up to permutation, the scale of each column of O is also not identifiable. To see this, observe the model over xi is unaltered if we both rescale any column Oi and appropriately rescale the variable hi. Without further assumptions, we can only hope to recover a certain canonical form of O, defined as follows:\nDefinition 1. (The Canonical O) We say O is in a canonical form if, for each i, σ2i = 1. In particular, the transformation O ← O diag(σ1, σ2, . . . , σk) (and a rescaling of h) places O in canonical form, and the distribution over x1, x2, x3, . . . is unaltered. Observe the canonical O is only specified up to the sign of each column (any sign change of a column does not alter the variance of hi).\nRecall µi,3 is the central third moment. Denote the skewness of hi as:\nγi = µi,3 σ3i\nThe first result considers the case when the skewness is non-zero.\nTheorem 4.1. (Independent and skewed factors) We have that:\n• (No False Positives) For all θ ∈ Rk, Algorithm 1 returns a subset of the columns of O, in a canonical form.\n• (Exact Recovery) Assume γi is nonzero for each i. Suppose θ ∈ Rk is a random vector uniformly sampled over the sphere Sk−1. With probability 1, Algorithm 1 returns all columns of O, in a canonical form.\nThe proof of this theorem is a consequence of the following lemma:\nLemma 4.1. We have:\nPairs = O diag(σ21 , σ 2 2 , . . . , σ 2 k)O ⊤\nTriples(η) = O diag(O⊤η) diag(µ1,3, µ2,3, . . . , µk,3)O ⊤\nThe proof of this Lemma is provided in the Appendix.\nProof. (of Theorem 4.1) The analysis is with respect to O it its canonical form. By the full rank assumption, U⊤ PairsU , which is a k× k matrix, is full rank; hence, the whitening step is possible. By construction:\nI = W⊤ PairsW\n= W⊤O diag(σ21 , σ 2 2 , . . . , σ 2 k)O ⊤W = (W⊤O)(W⊤O)⊤\n:= MM⊤\nwhere M := W⊤O. Hence, M is a k × k orthogonal matrix. Observe:\nW⊤ Triples(Wθ)W = W⊤O diag(O⊤Wθ) diag(γ1, γ2, . . . , γk)O ⊤W\n= M diag(M⊤θ) diag(γ1, γ2, . . . , γk)M ⊤\nSince M is an orthogonal matrix, the above is a (not necessarily unique) singular value decomposition of W⊤ Triples(Wθ)W . Denote the standard basis as e1, e2, . . . ek. Observe that Me1, . . .Mek are singular vectors. In other words, W⊤O1, . . . W\n⊤Ok are singular vectors, where Oi is the i-th column of O.\nAn SVD uniquely determines all singular vectors (up to sign) which have unique singular values. The diagonal of the matrix diag(M⊤θ) diag(γ1, γ2, . . . , γk) is the vector diag(γ1, γ2, . . . , γk)M\n⊤θ. Also, since M is a rotation matrix, the distribution of Mθ is also uniform on the sphere. Thus, if θ is uniformly sampled over the sphere, then every singular value will be nonzero (and distinct) with probability 1. Finally, for the reconstruction, we have\nW (W⊤W )−1Mei = W (W ⊤W )−1W⊤Oi = Oi,\nsince W (W⊤W )−1W⊤ is a projection operator (and the range of W and O are identical).\nRemark 1. (Finding Range(Pairs) efficiently) Suppose Θ ∈ Rd×k is a random matrix with entries sampled independently from a standard normal. Set U = Pairs Θ. Then, with probability 1, Range(U) = Range(Pairs).\nRemark 2. (No false positives) Note that if the skewness is 0 for some i then ECA will not recover the corresponding column. However, the algorithm does succeed for those directions in which the skewness is non-zero. This guarantee also provides the practical freedom to run the algorithm with multiple different directions θ, since we need only to find unique singular vectors (which may be easier to determine by running the algorithm with different choices for θ).\nRemark 3. (Estimating the skewness) It is straight forward to estimate the skewness corresponding to any column of O. Suppose λ is some unique singular vector (up to sign) found in step 3 of ECA (which was used to construct some column Oi), then:\nγi = λ ⊤W⊤ Triples(Wλ)Wλ\nis the corresponding skewness for Oi. This follows from the proof, since λ corresponds to some singular vector Mei and:\n(Mei) ⊤M diag(M⊤Mei) diag(γ1, γ2, . . . , γk)M ⊤Mei = γi\nusing that M is an orthogonal matrix."
    }, {
      "heading" : "4.2 Independent and Kurtotic Latent Factors",
      "text" : "Define the following matrix:\nQuadruples(η, η′) := E[(x1 − µ)(x2 − µ)⊤〈η, x3 − µ〉〈η′, x4 − µ〉] − (η⊤ Pairs η′) Pairs−(Pairs η)(Pairs η′)⊤ − (Pairs η′)(Pairs η)⊤\nAlgorithm 2 ECA; with kurtotic factors\nInput: vectors θ, θ′ ∈ Rk; the moments Pairs and Quadruples(η, η′)\n1. Dimensionality Reduction: Find a matrix U ∈ Rd×k such that\nRange(U) = Range(Pairs).\n2. Whiten: Find V ∈ Rk×k so V ⊤(U⊤ PairsU)V is the k × k identity matrix. Set:\nW = UV\n3. SVD: Let Λ be the set of (right) singular vectors, with unique singular values, of\nW⊤ Quadruples(Wθ,Wθ′)W\n4. Reconstruct: Return the set Ô:\nÔ = { (W+)⊤λ : λ ∈ Λ}\nwhere W+ is the pseudo-inverse (see Eq 1).\nThis is a subspace of the fourth moment tensor. Recall µi,4 is the central fourth moment. Denote the excess kurtosis of hi as:\nκi = µi,4 σ4i − 3\nFor Gaussian distributions, recall the kurtosis is 3, and so the excess kurtosis is 0. This function is also common in the source separation approaches (Hyvärinen et al., 2001) 3.\nIn settings where the latent factors are not skewed, we may hope that they are differentiated from a Gaussian distribution due to their fourth order moments. Here, Algorithm 2 is applicable:\nTheorem 4.2. (Independent and kurtotic factors) We have that:\n• (No False Positives) For all θ, θ′ ∈ Rk, Algorithm 2 returns a subset of the columns of O, in a canonical form.\n• (Exact Recovery) Assume κi is nonzero for each i. Suppose θ, θ′ ∈ Rk are random vectors uniformly and independently sampled over the sphere Sk−1. With probability 1, Algorithm 2 returns all the columns of O, in a canonical form.\nRemark 4. (Using both skewed and kurtotic ECA) Note that both algorithms never incorrectly return columns. Hence, if for every i, either the skewness or the excess kurtosis is nonzero, then by running both algorithms we will recover O.\n3Their algebraic method require more effort due to the additive noise and the lack of exchangeability. Here, the exchangeability assumption simplifies the approach and allows us to address models with non-additive noise (as in the Poisson count model discussed in the Section 2.\nThe proof of this theorem is a consequence of the following lemma:\nLemma 4.2. We have:\nQuadruples(η, η′) = O diag(O⊤η) diag(O⊤η′) diag(µ1,4 − 3σ41 , µ2,4 − 3σ42 , . . . , µk,4 − 3σ4k)O⊤\nThe proof of this Lemma is provided in the Appendix.\nProof. (of Theorem 4.2) The distinction from the argument in Theorem 4.1 is that:\nW⊤ Quadruples(Wθ,Wθ′))W = W⊤O diag(O⊤Wθ) diag(O⊤Wθ′) diag(κ1, κ2, . . . , κk)O ⊤W\n= M diag(M⊤θ) diag(M⊤θ′) diag(κ1, κ2, . . . , κk)M ⊤\nThe remainder of the argument follows that of the proof of Theorem 4.1."
    }, {
      "heading" : "4.3 Latent Dirichlet Allocation",
      "text" : "Now let us turn to the case where h has a Dirichlet density, where, each hi is not sampled independently. Even though the distribution on h is the product of hα1−1i , . . . h αk−1 i , the hi’s are not independent due to the constraint that h lives on the simplex. These dependencies suggest a modification for the moments to be used in ECA, which we now provide.\nSuppose α0 is known. Recall that α0 := α1 + α2 + · · · + αk (the sum of the “pseudo-counts”). Knowledge of α0 is significantly weaker than having full knowledge of the entire parameter vector α. A common practice is to specify the entire parameter vector α in a homogenous manner, with each component being identical (see Steyvers and Griffiths (2006)). Here, we need only specify the sum, which allows for arbitrary inhomogeneity in the prior.\nDenote the mean as µ = E[x1]\nDefine a modified second moment as\nPairsα0 := E[x1x ⊤ 2 ]− α0\nα0 + 1 µµ⊤\nand a modified third moment as\nTriplesα0(η) := E[x1x ⊤ 2 〈η, x3〉]− α0\nα0 + 2\n( E[x1x ⊤ 2 ]ηµ ⊤ + µη⊤E[x1x ⊤ 2 ] + 〈η, µ〉E[x1x⊤2 ] )\n+ 2α20\n(α0 + 2)(α0 + 1) 〈η, µ〉µµ⊤\nRemark 5. (Central vs Non-Central Moments) In the limit as α0 → 0, the Dirichlet model degenerates so that, with probability 1, only one coordinate of h equals 1 and the rest are 0 (e.g. each document is about 1 topic). Here, we limit to non-central moments:\nlim α0→0 Pairsα0 = E[x1x ⊤ 2 ] lim α0→0 Triplesα0(η) = E[x1x ⊤ 2 〈η, x3〉]\nIn the other extreme, the behavior limits to the central moments:\nlim α0→∞ Pairsα0 = E[(x1 − µ)(x2 − µ)⊤] lim α0→∞ Triplesα0(η) = E[(x1 − µ)(x2 − µ)⊤〈η, (x3 − µ)〉]\n(to prove the latter claim, expand the central moment and use that, by exchangeability, E[x1x ⊤ 2 ] = E[x2x ⊤ 3 ] = E[x1x ⊤ 3 ]).\nAlgorithm 3 ECA for latent Dirichlet allocation\nInput: a vector θ ∈ Rk; the moments Pairsα0 and Triplesα0 1. Dimensionality Reduction: Find a matrix U ∈ Rd×k such that\nRange(U) = Range(Pairsα0).\n(See Remark 1 for a fast procedure.)\n2. Whiten: Find V ∈ Rk×k so V ⊤(U⊤ Pairsα0 U)V is the k × k identity matrix. Set:\nW = UV\n3. SVD: Let Λ be the set of (right) singular vectors, with unique singular values, of\nW⊤ Triplesα0(Wθ)W\n4. Reconstruct and Normalize: Return the set Ô:\nÔ =\n{ (W+)⊤λ\n~1⊤(W+)⊤λ : λ ∈ Λ\n}\nwhere ~1 ∈ Rd is a vector of all ones and W+ is the pseudo-inverse (see Eq 1).\nOur main result here shows that ECA recovers both the topic matrix O, up to a permutation of the columns (where each column represents a probability distribution over words for a given topic) and the parameter vector α, using only knowledge of α0 (which, as discussed earlier, is a significantly less restrictive assumption than tuning the entire parameter vector). Also, as discussed in Remark 9, the method applies to cases where xv is not a multinomial distribution.\nTheorem 4.3. (Latent Dirichlet Allocation) We have that:\n• (No False Positives) For all θ ∈ Rk, Algorithm 3 returns a subset of the columns of O.\n• (Topic Recovery) Suppose θ ∈ Rk is a random vector uniformly sampled over the sphere Sk−1. With probability 1, Algorithm 3 returns all columns of O.\n• (Parameter Recovery) We have that:\nα = α0(α0 + 1)O + Pairsα0(O +)⊤~1\nwhere ~1 ∈ Rk is a vector of all ones.\nThe proof is a consequence of the following lemma:\nLemma 4.3. We have:\nPairsα0 = 1\n(α0 + 1)α0 O diag(α)O⊤\nand\nTriplesα0(η) = 2\n(α0 + 2)(α0 + 1)α0 O diag(O⊤η) diag(α)O⊤\nThe proof of this Lemma is provided in the Appendix.\nProof. (of Theorem 4.3) Note that with the following rescaling of columns:\nÕ = 1√\n(α0 + 1)α0 O diag(\n√ α1, √ α2, . . . , √ αk)\nwe have that h is in canonical form (i.e. the variance of each hi is 1). The remainder of the proof is identical to that of Theorem 4.1. The only modification is that we simply normalize the output of Algorithm 1. Finally, observe that claim for estimating α holds due to the functional form of Pairsα0 .\nRemark 6. (Limiting behaviors) ECA seamlessly blends between the single topic model (α0 → 0) of Anandkumar et al. (2012) and the skewness based ECA, Algorithm 1 (α0 → ∞). In the single topic case, Anandkumar et al. (2012) provide eigenvector based algorithms. This work shows that two SVDs suffice for parameter recovery.\nRemark 7. (Effective skewness) Note that in the transformation to the canonical Õ, the term that appears in the diagonal is the effective skewness based on modified moments. This turns out to be:\nγi = 2\n√ α0(α0 + 1)\n(α0 + 2)2 1 αi\nThis has implications for the sample complexity (see Section 5). Note that as any αi → ∞ the effective skewness tends to 0 (as expected).\nRemark 8. (Skewed and Kurtotic ECA for LDA) We conjecture that the fourth moments can be utilized in the Dirichlet case such that the resulting algorithm limits to the kurtotic based ECA, when α0 → ∞. Furthermore, the mixture of Poissions model discussed in Section 2 provides a natural alternative to the LDA model in this regime.\nRemark 9. (the Dirichlet model, more generally) It is not necessary that we have a multinomial distribution on xv, so long as E[xv|h] = Oh. In some applications, it might be natural for the observations to come from a different distribution (say xv may represent pixel intensities in an image or some other real valued quantity). For this case, where h has a Dirichlet prior (and where xv may not be multinomial), ECA still correctly recovers the columns of O. Furthermore, we need not normalize; the set {(W+)⊤λ : λ ∈ Λ} recovers O in a canonical form."
    }, {
      "heading" : "4.4 The Multi-View Extension",
      "text" : "Rather than O being identical for each xv, suppose for each v ∈ {1, 2, 3, 4, . . . } there exists an Ov ∈ Rdv×k such that E[xv|h] = Ovh For v ∈ {1, 2, 3}, define\nPairsv,v′ := E[(xv − µ)(x′v − µ)⊤] Triples132(η) := E[(x1 − µ)(x2 − µ)⊤〈η, x3 − µ〉]\nWe use the notation 132 to stress that Triples132(η) is a d1 × d2 sized matrix.\nAlgorithm 4 ECA; the multi-view case\nInput: vector θ ∈ Rk; the moments Pairsv,v′ and Triples132(η)\n1. Project views 1 and 2: Find matrices A ∈ Rk×d1 and B ∈ Rk×d2 such that APairs12 B⊤ is invertible. Set:\nP̃airs12 := APairs12 B ⊤ P̃airs31 := Pairs31 A ⊤ P̃airs32 := Pairs32 B ⊤\nT̃riples132(η) := ATriples132(η)B ⊤\n(See Remark 11 for a fast procedure.)\n2. Symmetrize: Reduce to a single view:\nPairs3 := P̃airs31(P̃airs ⊤ 12) −1P̃airs23\nTriples3(η) := P̃airs32(P̃airs12) −1T̃riples132(η)(P̃airs12) −1P̃airs13\n3. Estimate O3 with ECA: Call Algorithm 1, with θ, Pairs3, and Triples3(η).\nLemma 4.4. For v ∈ {1, 2, 3},\nPairsv,v′ = Ov diag(σ 2 1 , σ 2 2 , . . . , σ 2 k)O ′⊤ v\nTriples132(η) = O1 diag(O ⊤ 3 η) diag(µ1,3, µ2,3, . . . , µk,3)O ⊤ 2\nThe proof for Lemma 4.4 is analogous to those in Appendix A. These functional forms make deriving an SVD based algorithm more subtle. Using the methods in Anandkumar et al. (2012), eigenvector based method are straightforward to derive. However, SVD based algorithms are preferred due to their greater simplicity. The following lemma shows how the symmetrization step in the algorithm makes this possible.\nLemma 4.5. For Pairs3 and Triples3(η) defined in Algorithm 4, we have:\nPairs3 = O3 diag(σ 2 1 , σ 2 2 , . . . , σ 2 k)O ⊤ 3\nTriples3(η) = O3 diag(O ⊤ 3 η) diag(µ1,3, µ2,3, . . . , µk,3)O ⊤ 3\nProof. Without loss of generality, suppose Ov are in canonical form (for each i, σ 2 i = 1). Hence, APairs12 B ⊤ = AO1(BO2) ⊤. Hence, AO1 and BO2 are invertible. Note that:\nPairs31 A ⊤(B Pairs21 A ⊤)−1B Pairs23 = O3O ⊤ 1 A ⊤(BO2O ⊤ 1 A ⊤)−1BO2O ⊤ 3 = O3O ⊤ 3\nwhich proves the first claim. The proof of the second claim is analogous.\nAgain, we say that all Ov are in a canonical form if, for each i, σ 2 i = 1.\nTheorem 4.4. (The multi-view case) We have:\n• (No False Positives) For all θ ∈ Rk, Algorithm 4 returns a subset of O3, in a canonical form.\n• (Exact Recovery) Assume that γi is nonzero for each i. Suppose θ ∈ Rk is a random vector uniformly sampled over the sphere Sk−1. With probability 1, Algorithm 4 returns all columns of O3, in a canonical form.\nProof. (of Theorem 4.4) The proof is identical to that of Theorem 4.1.\nRemark 10. (Simpler algorithms for HMMs) Mossel and Roch (2006); Anandkumar et al. (2012) provide eigenvector based algorithms for HMM parameter estimation. These results show that we can achieve parameter estimation with only two SVDs (See Anandkumar et al. (2012) for the reduction of an HMM to the multi-view setting). The key idea is the symmetrization that reduces the problem to a single view. Remark 11. (Finding A and B) Suppose Θ,Θ′ ∈ Rd×k are random matrices with entries sampled independently from a standard normal. Set A = Pairs1,2 Θ and B = Pairs2,1Θ\n′. With probability 1, Range(A) = Range(O1) and Range(B) = Range(O2), and the invertibility condition will be satisfied (provided that O1 and O2 are full rank)."
    }, {
      "heading" : "5 Discussion: Sample Complexity and Sparsity",
      "text" : "Sample Complexity and Matrix Perturbation: A detailed sample complexity analysis will involve the details of the noise model on xv. Furthermore, the sample complexity of obtaining an accurate whitening matrix W is well understood (e.g. see Anandkumar et al. (2012) for example). In general, this sample complexity need not explicitly depend on the dimension d; rather it will depend on the distributional properties of xv and the minimal singular value of Pairs.\nInstead, let us focus on accuracy required in Step 3 of ECA, as this is the only novel aspect of the algorithm (say in comparison to analyses of PCA and CCA). To focus on this issue, suppose that we have an exact whitening matrix W .\nDefine the whitened random vector x̃v = W ⊤(xv −E[xv]), and denote the empirical average by:\nÊ[x̃1x̃ ⊤ 2 〈θ, x̃3〉]\nThis is our estimate of W⊤ Triples(Wθ)W , where the error is:\nE = E[x̃1x̃ ⊤ 2 〈θ, x̃3〉]− Ê[x̃1x̃⊤2 〈θ, x̃3〉]\nNote this is the estimation error of a k × k matrix (rather than a d× d matrix). Lemma C.1 (in the Appendix) shows that to obtain estimates of the columns that are ǫ-accurate (in ℓ2) we need the accuracy of ‖E‖2 to be O (\nǫγmin σmax(O)k3\n) . The dependence on k is not surprising\n(and may be improvable with a deterministic algorithm). The inverse dependency on the maximal singular value of O is also perhaps not surprising (the scale of units of ǫ are the same as the units of O, while E is unit free). The notable dependence is that the error is linear in the minimal skewness, showing the smaller the skewness the more accuracy we need to recover O.\nSparsity: Note that sparsity considerations have not entered into our analysis. Often, in high dimensional statistics, notions of sparsity are desired as this generally decreases the sample size requirements (often at an increased computational burden).\nHere, while these results have no explicit dependence on the sparsity level, sparsity is helpful in that it does implicitly affect the skewness (and the whitening) , which determines the sample complexity. As the model becomes less sparse, the skewness tends to 0. In particular, for the case of LDA, as α0 → ∞ both the whitening step and the second SVD need increased accuracy to handle the smaller singular values. Remark 7 expresses the skewness in terms of the parameter α, which quantifies the extent to which the skewness tends to 0 for large αi.\nPerhaps surprisingly, the sparsity level has no direct impact on the computational requirements of a “plug-in” empirical algorithm (beyond the linear time requirement of reading the data in order to construct the empirical statistics)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Kamalika Chaudhuri, Adam Kalai, Percy Liang, Chris Meek, David Sontag, and Tong Zhang for many invaluable insights."
    }, {
      "heading" : "A Analysis with Independent Factors",
      "text" : "Lemma A.1. (Hidden state moments) Let z = h− E[h]. For any vectors u, v ∈ Rk,\nE[zz⊤] = diag(σ21 , σ 2 2 , . . . , σ 2 k)\nE[zz⊤〈u, z〉] = diag(u) diag(µi,3, µ2,3, . . . , µk,3)\nand\nE[zz⊤〈u, z〉〈v, z〉] = diag(u) diag(v) diag(µ1,4 − 3σ41 , µ2,4 − 3σ42 , . . . , µk,4 − 3σ4k) + (u⊤E[zz⊤]v)E[zz⊤] + (E[zz⊤]u)(E[zz⊤]v)⊤ + (E[zz⊤]v)(E[zz⊤]u)⊤\nProof. Let a, b, u and v be vectors. Since the {zt} are independent and have mean zero, we have:\nE[〈a, z〉〈b, z〉] = E [( k∑\ni=1\naizi\n)( k∑\ni=1\nbizi\n)] = k∑\ni=1\naibiE[z 2 i ] =\nk∑\ni=1\naibiσ 2 i\nand\nE[〈a, z〉〈b, z〉〈u, z〉] = E [( k∑\ni=1\naizi\n)( k∑\ni=1\nbizi\n)( k∑\ni=1\nuizi\n)] = k∑\ni=1\naibiuiE[z 3 i ] =\nk∑\ni=1\naibiuiµi,3.\nFor the final claim, let us compute the diagonal and non-diagonal entries separately. First,\nE[zizi〈u, z〉〈v, z〉] = E[ ∑\nj,k\nujvkzizizjzk]\n= uiviE[z 4 i ] +\n∑\nj 6=i\nujvjE[z 2 i ]E[z 2 j ]\n= uiviµi,4 + σ 2 i\n∑\nj 6=i\nujvjσ 2 j\n= uiviµi,4 − uivi(σ2i )2 + σ2i ∑\nj\nujvjσ 2 j\n= uiviµi,4 − uivi(σ2i )2 + (u⊤E[zz⊤]v)σ2i\nFor j 6= i\nE[zizj〈u, z〉〈v, z〉] = E[ ∑\nk,l\nukvlzizjzkzl]\n= uivjE[z 2 i z 2 j ] + ujviE[z 2 i z 2 j ] = uivjσ 2 i σ 2 j + ujviσ 2 i σ 2 j = [E[zz⊤]u]i[E[zz ⊤]v]j + [E[zz ⊤]u]j [E[zz ⊤]v]i\nThe proof is completed by noting the (i, j)-th components of E[zz⊤〈u, z〉〈v, z〉] agree with the above moment expressions.\nThe proofs of Lemmas 4.1 and 4.2 follow.\nProof. (of Lemmas 4.1 and 4.2) By the conditional independence of {x1, x2, x3} given h, E[x1] = OE[h]\nand\nE[(x1 − µ)(x2 − µ)⊤] = E[E[(x1 − µ)(x2 − µ)⊤|h]] = E[E[(x1 − µ)|h]E[(x2 − µ)⊤|h]] = OE[(h− E[h])(h − E[h])⊤]O⊤\n= O diag(σ21 , σ 2 2 , . . . , σ 2 k)O ⊤\nby Lemma A.1. Similarly, the (i, j)-th entry of Triples(η) is\nE [ 〈ei, x1 − µ〉〈ej , x2 − µ〉〈η, x3 − µ〉 ] = E [ E[〈ei, x1 − µ〉〈ej , x2 − µ〉〈η, x3 − µ〉|h] ]\n= E [ E[〈ei, x1 − µ〉|h] · E[〈ej , x2 − µ〉|h] · E[〈η, x3 − µ〉|h] ] = E [ 〈ei, O(h− E[h])〉〈ej , O(h− E[h])〉〈η,O(h − E[h])〉 ] = E [ 〈O⊤ei, h− E[h]〉〈O⊤ej , h− E[h]〉〈O⊤η, h − E[h]〉 ] = e⊤i O diag(O ⊤η) diag(µi,3, µ2,3, . . . , µk,3)O ⊤ej .\nThe proof for Quadruples(η, η′) is analogous.\nThe proof for Lemma 4.4 is analogous to the above proofs."
    }, {
      "heading" : "B Analysis with Dirichlet Factors",
      "text" : "We first provide the functional forms of the first, second, and third moments. With these, we prove Lemma 4.3.\nB.1 Dirichlet moments\nLemma B.1. (Dirichlet moments) We have:\nE[h⊗ h] = 1 (α0 + 1)α0\n( diag(α) + αα⊤ )\nand\nE[h⊗ h⊗ h] = 1 (α0 + 2)(α0 + 1)α0\n( α⊗ α⊗ α+ k∑\ni=1\nαi ( ei ⊗ ei ⊗ α ) + k∑\ni=1\nαi ( α⊗ ei ⊗ ei )\n+\nk∑\ni=1\nαi ( ei ⊗ α⊗ ei ) + 2 k∑\ni=1\nαi ( ei ⊗ ei ⊗ ei )) .\nHence, for v ∈ Rk,\nE[(h⊗ h)〈v, h〉] = 1 (α0 + 2)(α0 + 1)α0\n( 〈v, α〉αα⊤ + diag(α)vα⊤ + αv⊤ diag(α)\n+ 〈v, α〉diag(α) + 2diag(v) diag(α) )\nProof. First, let us specify the following scalar moments. Univariate moments: Fix some i ∈ [k], and let α′ := α + p · ei for some positive integer p. Then\nE[hpi ] = Z(α′)\nZ(α)\n= Γ(αi + p) Γ(αi) · Γ(α0) Γ(α0 + p) = (αi + p− 1)(αi + p− 2) · · ·αi (α0 + p− 1)(α0 + p− 2) · · · α0 .\nIn particular,\nE[hi] = αi α0 E[h2i ] = (αi + 1)αi (α0 + 1)α0 E[h3i ] = (αi + 2)(αi + 1)αi (α0 + 2)(α0 + 1)α0 .\nBivariate moments: Fix i, j ∈ [k] with i 6= j, and let α′ := α+ p · ei+ q · ej for some positive integers p and q. Then\nE[hpi h q j ] =\nZ(α′)\nZ(α)\n= Γ(αi + p) · Γ(αj + q) Γ(αi) · Γ(αj) · Γ(α0) Γ(α0 + p+ q)\n=\n( (αi + p− 1)(αi + p− 2) · · ·αi ) · ( (αj + q − 1)(αj + q − 2) · · ·αj )\n(α0 + p+ q − 1)(α0 + p+ q − 2) · · ·α0 .\nIn particular,\nE[hihj ] = αiαj\n(α0 + 1)α0\nE[h2i hj ] = (αi + 1)αiαj\n(α0 + 2)(α0 + 1)α0 .\nTrivariate moments: Fix i, j, κ ∈ [k] all distinct, and let α′ := α+ ei + ej + eκ. Then\nE[hihjhκ] = Z(α′)\nZ(α)\n= Γ(αi + 1) · Γ(αj + 1) · Γ(ακ + 1) Γ(αi) · Γ(αj) · Γ(ακ) · Γ(α0) Γ(α0 + 3) = αiαjακ\n(α0 + 2)(α0 + 1)α0 .\nCompleting the proof: The proof for the second moment matrix and the third moment tensor follows by observing that each component agrees with the above expressions. For the final claim,\nE[(h⊗ h)〈v, h〉] = 1 (α0 + 2)(α0 + 1)α0\n( 〈v, α〉(α ⊗ α) + k∑\ni=1\nαivi ( ei ⊗ α ) + k∑\ni=1\nαivi ( α⊗ ei )\n+ k∑\ni=1\nαi〈v, α〉(ei ⊗ ei) + 2 k∑\ni=1\nαivi(ei ⊗ ei) )\n= 1\n(α0 + 2)(α0 + 1)α0\n( 〈v, α〉αα⊤ + diag(α)vα⊤ + αv⊤ diag(α)\n+ 〈v, α〉diag(α) + 2diag(v) diag(α) )\nwhich completes the proof.\nB.2 The proof of Lemma 4.3\nProof. Observe: E[x1] = OE[h]\nand E[x1x ⊤ 2 ] = E[E[x1x ⊤ 2 |h]] = OE[hh⊤]O⊤\nDefine the analogous quantity:\nPairsh = E[hh ⊤]− α0\nα0 + 1 E[h]E[h]⊤\nand so: Pairsα0 = OPairshO ⊤\nObserve:\nPairsh = E[hh ⊤]− 1\n(α0 + 1)α0 αα⊤\n= 1\n(α0 + 1)α0 diag(α)\nHence,\nPairsα0 = OPairshO ⊤ =\n1\n(α0 + 1)α0 O diag(α)O⊤\nwhich proves the first claim. Also, define:\nTriplesh(v) := E[(h⊗ h)〈v, h〉] − α0\nα0 + 2\n( E[hh⊤]vE[h]⊤ + E[h]v⊤E[hh⊤] + 〈v,E[h]〉E[hh⊤] )\n+ 2α20\n(α0 + 2)(α0 + 1) 〈v,E[h]〉E[h]E[h]⊤\nSince\nE[x1x ⊤ 2 〈η, x3〉] = E[E[x1x⊤2 〈η, x3〉|h]] = OE[hh⊤〈η,Oh〉]O⊤\n= OE[hh⊤〈O⊤η, h〉]O⊤\nwe have Triplesα0(η) = OTriplesh(O ⊤η)O⊤\nLet us complete the proof by showing:\nTriplesh(v) := 2\n(α0 + 2)(α0 + 1)α0 diag(v) diag(α)\nObserve:\n2 (α0 + 2)(α0 + 1)α0 diag(v) diag(α) = E[(h⊗h)〈v, h〉]− 1 (α0 + 2)(α0 + 1)α0\n( 〈v, α〉αα⊤+diag(α)vα⊤\n+ αv⊤ diag(α) + 〈v, α〉diag(α) )\nLet us handle each term separately. First,\n1 (α0 + 2)(α0 + 1)α0 〈v, α〉αα⊤ = α\n2 0\n(α0 + 2)(α0 + 1) 〈v,E[h]〉E[h]E[h]⊤\nAlso, since: 1\n(α0 + 1)α0 diag(α) = E[hh⊤]− 1 (α0 + 1)α0 αα⊤\nwe have:\n1\n(α0 + 2)(α0 + 1)α0\n( diag(α)vα⊤ + αv⊤ diag(α) + 〈v, α〉diag(α) )\n= 1\nα0 + 2\n( E[hh⊤]vα⊤ + αv⊤E[hh⊤] + 〈v, α〉E[hh⊤] ) − 3\n(α0 + 2)(α0 + 1)α0 〈v, α〉αα⊤\n= α0\nα0 + 2\n( E[hh⊤]vE[h]⊤ + E[h]v⊤E[hh⊤] + 〈v,E[h]〉E[hh⊤] ) − 3α 2 0\n(α0 + 2)(α0 + 1) 〈v,E[h]〉E[h]E[h]⊤\nHence,\n2\n(α0 + 2)(α0 + 1)α0 diag(v) diag(α) = E[(h⊗ h)〈v, h〉]\n− α0 α0 + 2\n( E[hh⊤]vE[h]⊤ + E[h]v⊤E[hh⊤] + 〈v,E[h]〉E[hh⊤] )\n+ 2α20\n(α0 + 2)(α0 + 1) 〈v,E[h]〉E[h]E[h]⊤\nwhich proves the claim."
    }, {
      "heading" : "C Sample Complexity",
      "text" : "Let us examine on the sample complexity relevant for Step 3 of ECA. To focus on this issue, suppose that we have an exact whitening matrix W . The sample complexity of obtaining such a W accurately is well understood (and, using techniques from Anandkumar et al. (2012) for example, this sample complexity need not explicitly depend on the dimension; rather it will depend on the distributional properties of xv.).\nDefine x̃v = W ⊤(xv − E[xv]). Denote the empirical average by:\nÊ[x̃1x̃ ⊤ 2 〈θ, x̃3〉]\nDenote the error as: E = E[x̃1x̃ ⊤\n2 〈θ, x̃3〉]− Ê[x̃1x̃⊤2 〈θ, x̃3〉] Let σi and vi denote the corresponding i-th singular value (in increasing order) and vector of E[x̃1x̃ ⊤\n2 〈θ, x̃3〉]. Similarly, let v̂i and σ̂i denote the corresponding i-th singular value (in increasing order) and vector of E[x̃1x̃ ⊤ 2 〈θ, x̃3〉]. For convenience, choose the sign of v̂i so that 〈vi, v̂i〉 ≥ 0.\nLemma C.1. (Target Accuracy for Reconstruction) We have that:\n‖E‖2 ≤ ǫ γminδ\n8k3\nThen with probability greater than 1− δ, we have for all i:\n‖vi − v̂i‖2 ≤ ǫ\nLet Oi be the column of O constructed as Oi = W (W ⊤W )−1 and let Ôi be the estimated reconstruction, i.e. Ôi = W (W ⊤W )−1. We also have for all i,\n‖Oi − Ôi‖2 ≤ ǫσ1(O)\nwhere σ1(O) is the largest singular value of O.\nFirst, a few lemmas are in order.\nLemma C.2. Suppose for all i\nσi ≥ ∆ |σi − σi+1| ≥ ∆\nwhere σ(·) denotes the vector of singular values, in increasing order. For all i, vi and v̂i, we have:\n‖vi − v̂i‖ ≤ 2 √ k‖E‖2\n∆ − ‖E‖2\nwhere the sign of v̂i chosen so 〈v̂i, v̂i〉 ≥ 0.\nProof. Let cos(θ) = 〈vi, v̂i〉 (which is positive since we assume 〈vi, v̂i〉 ≥ 0). We have: ‖vi − v̂i‖2 = 2(1 − cos(θ)) =≤ 2(1 − cos2(θ)) = 2 sin2(θ)\nBy Weyl’s theorem (see Lemma D.3) and by assumption,\nmin i\n|σ̂i − σj | ≥ ∆− ‖E‖2\nand min j 6=i |σ̂i − σj | ≥ min j 6=i |σi − σj | − ‖E‖2 ≥ ∆− ‖E‖2\nByWedin’s theorem (see Lemma D.2 applied to the split where vi and ṽi correspond to the subspaces U1 and Ũ1),\n| sin(θ)| ≤ √ 2 ‖E‖F ∆− ‖E‖2 ≤ √ 2\n√ k‖E‖2\n∆− ‖E‖2\nLemma C.3. Fix any δ ∈ (0, 1) and matrix A ∈ Rk×k. Let θ ∈ Rk be a random vector distributed uniformly over Sk−1. With probability greater than 1− δ, we have\nmin i 6=j\n|〈θ,A(ei − ej)〉| > mini 6=j ‖A(ei − ej)‖2 · δ√\nek2.5\nand\nmin i\n|〈θ,Aei〉| > mini ‖Aei‖2 · δ√\nek2.5\nProof. By Lemma D.1, for any fixed pair {i, j} ⊆ [k] and β := δ0/ √ e,\nPr [ |〈θ,A(ei − ej)〉| ≤ ‖A(ei − ej)‖2 ·\n1√ k · δ0√ e\n] ≤ exp ( 1\n2 (1− (δ20/e) + ln(δ20/e))\n) ≤ δ0.\nSimilarly, for each i\nPr [ |〈θ,Aei)〉| ≤ ‖Aei‖2 ·\n1√ k · δ0√ e\n] ≤ exp ( 1\n2 (1− (δ20/e) + ln(δ20/e))\n) ≤ δ0.\nLet δ0 := δ/k 2. The claim follows by a union bound over all ( k 2 ) + k ≤ k2 possibilities.\nWe now complete the argument.\nProof. (of Lemma C.1) Choose A = diag(γ1, γ2, . . . , γk)M ⊤. The proof of Theorem 4.1 shows 〈ei, Aθ〉 are the singular values. Also the minimal singular value of A is greater than γmin (since MM⊤ = I). Hence, we have:\nσi ≥ γminδ2k2.5 := ∆ |σi − σi+1| ≥ γminδ2k2.5\nNote that ǫ ≤ 2 (else the claim is trivially true). Hence, ǫ ≤ 2, which implies ‖E‖2 ≤ ∆/2. Thus,\n‖vi − v̂i‖ ≤ 2 √ k‖E‖2\n∆ − ‖E‖2 ≤ 4 √ k‖E‖2 ∆ = 8 k3‖E‖2 γminδ\nwhich proves the first claim. The final claim uses (W⊤O)(W⊤O)⊤ = I (as shown in proof of Theorem 4.1), which implies ‖W (W⊤W )−1‖2 ≤ σmin(W−1) ≤ σmax(W−1) = σ1(O)."
    }, {
      "heading" : "D Concentration and Matrix Perturbation Lemmas",
      "text" : "Lemma D.1. (from Dasgupta and Gupta (2003)) Let θ ∈ Rn be a random vector distributed uniformly over Sn−1, and fix a vector v ∈ Rn.\n1. If β ∈ (0, 1), then\nPr [ |〈θ, v〉| ≤ ‖v‖2 ·\n1√ n · β\n] ≤ exp ( 1\n2 (1− β2 + ln β2)\n) .\n2. If β > 1, then\nPr [ |〈θ, v〉| ≥ ‖v‖2 ·\n1√ n · β\n] ≤ exp ( 1\n2 (1− β2 + ln β2)\n) .\nProof. This is a special case of Lemma 2.2 from Dasgupta and Gupta (2003).\nLemma D.2 (Wedin’s theorem; Theorem 4.1, p. 260 in Stewart and Sun (1990))). Let A,E ∈ Rm×n with m ≥ n be given. Let A have the singular value decomposition\n \nU⊤1 U⊤2 U⊤3\n A [ V1 V2 ] =  \nΣ1 0 0 Σ2 0 0\n  .\nHere, we do not suppose Σ1 and Σ2 have singular values in any order. Let Ã := A + E, with analogous singular value decomposition (Ũ1, Ũ2, Ũ3, Σ̃1, Σ̃2, Ṽ1Ṽ2) (again with no ordering to the singular values). Let Φ be the matrix of canonical angles between range(U1) and range(Ũ1), and Θ be the matrix of canonical angles between range(V1) and range(Ṽ1). Suppose there exists a ∆ such that:\nmin i,j |[Σ̃1]i,i − [Σ2]j,j| > ∆ and min i,i |[Σ̃1]i,i| > ∆,\nthen\n‖ sinΦ‖2F + ‖ sinΘ‖2F ≤ 2‖E‖2F\nδ2 .\nLemma D.3 (Weyl’s theorem; Theorem 4.11, p. 204 in Stewart and Sun (1990)). Let A,E ∈ Rm×n with m ≥ n be given. Then\nmax i∈[n]\n|σi(A+ E)− σi(A)| ≤ ‖E‖2."
    } ],
    "references" : [ {
      "title" : "A method of moments for mixture models and hidden markov models",
      "author" : [ "A. Anandkumar", "D. Hsu", "S.M. Kakade" ],
      "venue" : null,
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2012
    }, {
      "title" : "Two-view feature generation model for semi-supervised learning",
      "author" : [ "R. Ando", "T. Zhang" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ando and Zhang.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ando and Zhang.",
      "year" : 2007
    }, {
      "title" : "Learning topic models — going beyond svd",
      "author" : [ "Saneev Arora", "Rong Ge", "Ankur Moitra" ],
      "venue" : null,
      "citeRegEx" : "Arora et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "Distributed gibbs sampling for latent variable models",
      "author" : [ "A. Asuncion", "P. Smyth", "M. Welling", "D. Newman", "I. Porteous", "S. Triglia" ],
      "venue" : "In Scaling Up Machine Learning: Parallel and Distributed Approaches. Cambridge Univ Pr,",
      "citeRegEx" : "Asuncion et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Asuncion et al\\.",
      "year" : 2011
    }, {
      "title" : "Independent component analysis, a survey of some algebraic methods",
      "author" : [ "Jean-Franois Cardoso", "Pierre Comon" ],
      "venue" : "In IEEE International Symposium on Circuits and Systems,",
      "citeRegEx" : "Cardoso and Comon.,? \\Q1996\\E",
      "shortCiteRegEx" : "Cardoso and Comon.",
      "year" : 1996
    }, {
      "title" : "Full reconstruction of Markov models on evolutionary trees: Identifiability and consistency",
      "author" : [ "J.T. Chang" ],
      "venue" : "Mathematical Biosciences,",
      "citeRegEx" : "Chang.,? \\Q1996\\E",
      "shortCiteRegEx" : "Chang.",
      "year" : 1996
    }, {
      "title" : "Learning mixtures of product distributions using correlations and independence",
      "author" : [ "K. Chaudhuri", "S. Rao" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Chaudhuri and Rao.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chaudhuri and Rao.",
      "year" : 2008
    }, {
      "title" : "Multi-view clustering via canonical correlation analysis",
      "author" : [ "K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2009
    }, {
      "title" : "Handbook of Blind Source Separation: Independent Component Analysis and Applications",
      "author" : [ "P. Comon", "C. Jutten" ],
      "venue" : "Academic Press. Elsevier,",
      "citeRegEx" : "Comon and Jutten.,? \\Q2010\\E",
      "shortCiteRegEx" : "Comon and Jutten.",
      "year" : 2010
    }, {
      "title" : "Learning the parts of objects by nonnegative matrix",
      "author" : [ "Daniel D. Lee", "H. Sebastian Seung" ],
      "venue" : "Computer Science,",
      "citeRegEx" : "Lee and Seung.,? \\Q2007\\E",
      "shortCiteRegEx" : "Lee and Seung.",
      "year" : 2007
    }, {
      "title" : "Matrix Perturbation Theory",
      "author" : [ "G.W. Stewart", "Ji-Guang Sun" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Stewart and Sun.,? \\Q1990\\E",
      "shortCiteRegEx" : "Stewart and Sun.",
      "year" : 1990
    }, {
      "title" : "Complexity Let us examine on the sample complexity relevant for Step 3 of ECA. To focus on this issue, suppose that we have an exact whitening matrix W",
      "author" : [ "C Sample" ],
      "venue" : null,
      "citeRegEx" : "Sample,? \\Q2012\\E",
      "shortCiteRegEx" : "Sample",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "ExpectationMaximization (EM) (Redner and Walker, 1984), Gibbs sampling (Asuncion et al., 2011), and variational approaches (Hoffman et al.",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : "For this setting, we establish that ECA correctly recovers the parameters and is simpler than the eigenvector decomposition methods of Anandkumar et al. (2012). Finally, while our presentation focuses on utilizing known statistics of the observed moments to emphasize the correctness of the methodology, “plug-in” moment estimates can be used with sampled data.",
      "startOffset" : 135,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996).",
      "startOffset" : 100,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "2 Related Work There are relatively few provably correct methods for learning topic models when documents contain more than one topic; the recent work of Arora et al. (2012) provides a notable exception.",
      "startOffset" : 154,
      "endOffset" : 174
    }, {
      "referenceID" : 1,
      "context" : "2 Related Work There are relatively few provably correct methods for learning topic models when documents contain more than one topic; the recent work of Arora et al. (2012) provides a notable exception. Arora et al. (2012) provides a non-negative matrix factorization approach, under natural separability assumptions.",
      "startOffset" : 154,
      "endOffset" : 224
    }, {
      "referenceID" : 1,
      "context" : "2 Related Work There are relatively few provably correct methods for learning topic models when documents contain more than one topic; the recent work of Arora et al. (2012) provides a notable exception. Arora et al. (2012) provides a non-negative matrix factorization approach, under natural separability assumptions. These results also apply to certain settings with correlated topics. Our approach does not require this assumption 2, and our algorithm is affine invariant. The underlying approach taken is a certain diagonalization technique of the observed moments. We know of at least two different settings which utilize this idea for parameter estimation. Chang (1996) utilizes eigenvector methods for discrete Markov models of evolution, where the models involve multinomial distributions.",
      "startOffset" : 154,
      "endOffset" : 676
    }, {
      "referenceID" : 1,
      "context" : "2 Related Work There are relatively few provably correct methods for learning topic models when documents contain more than one topic; the recent work of Arora et al. (2012) provides a notable exception. Arora et al. (2012) provides a non-negative matrix factorization approach, under natural separability assumptions. These results also apply to certain settings with correlated topics. Our approach does not require this assumption 2, and our algorithm is affine invariant. The underlying approach taken is a certain diagonalization technique of the observed moments. We know of at least two different settings which utilize this idea for parameter estimation. Chang (1996) utilizes eigenvector methods for discrete Markov models of evolution, where the models involve multinomial distributions. The idea has been extended to other discrete mixture models such as discrete hidden Markov models (HMMs) and mixture models with single active topics (see Mossel and Roch (2006); Hsu et al.",
      "startOffset" : 154,
      "endOffset" : 976
    }, {
      "referenceID" : 1,
      "context" : "2 Related Work There are relatively few provably correct methods for learning topic models when documents contain more than one topic; the recent work of Arora et al. (2012) provides a notable exception. Arora et al. (2012) provides a non-negative matrix factorization approach, under natural separability assumptions. These results also apply to certain settings with correlated topics. Our approach does not require this assumption 2, and our algorithm is affine invariant. The underlying approach taken is a certain diagonalization technique of the observed moments. We know of at least two different settings which utilize this idea for parameter estimation. Chang (1996) utilizes eigenvector methods for discrete Markov models of evolution, where the models involve multinomial distributions. The idea has been extended to other discrete mixture models such as discrete hidden Markov models (HMMs) and mixture models with single active topics (see Mossel and Roch (2006); Hsu et al. (2009); Anandkumar et al.",
      "startOffset" : 154,
      "endOffset" : 995
    }, {
      "referenceID" : 0,
      "context" : "(2009); Anandkumar et al. (2012)).",
      "startOffset" : 8,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs).",
      "startOffset" : 8,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models.",
      "startOffset" : 8,
      "endOffset" : 348
    }, {
      "referenceID" : 0,
      "context" : "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian).",
      "startOffset" : 8,
      "endOffset" : 743
    }, {
      "referenceID" : 0,
      "context" : "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools. The underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996).",
      "startOffset" : 8,
      "endOffset" : 1319
    }, {
      "referenceID" : 0,
      "context" : "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools. The underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996). In particular, we show that the “topic” modeling problem exhibits a rather simple algebraic solution, where only two SVDs suffice for parameter estimation.",
      "startOffset" : 8,
      "endOffset" : 1351
    }, {
      "referenceID" : 0,
      "context" : "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools. The underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996). In particular, we show that the “topic” modeling problem exhibits a rather simple algebraic solution, where only two SVDs suffice for parameter estimation. Moreover, this approach also simplifies the algorithms in Mossel and Roch (2006); Hsu et al.",
      "startOffset" : 8,
      "endOffset" : 1589
    }, {
      "referenceID" : 0,
      "context" : "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools. The underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996). In particular, we show that the “topic” modeling problem exhibits a rather simple algebraic solution, where only two SVDs suffice for parameter estimation. Moreover, this approach also simplifies the algorithms in Mossel and Roch (2006); Hsu et al. (2009); Anandkumar et al.",
      "startOffset" : 8,
      "endOffset" : 1608
    }, {
      "referenceID" : 0,
      "context" : "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools. The underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996). In particular, we show that the “topic” modeling problem exhibits a rather simple algebraic solution, where only two SVDs suffice for parameter estimation. Moreover, this approach also simplifies the algorithms in Mossel and Roch (2006); Hsu et al. (2009); Anandkumar et al. (2012), in that the eigenvector methods are no longer necessary (e.",
      "startOffset" : 8,
      "endOffset" : 1634
    }, {
      "referenceID" : 0,
      "context" : "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools. The underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996). In particular, we show that the “topic” modeling problem exhibits a rather simple algebraic solution, where only two SVDs suffice for parameter estimation. Moreover, this approach also simplifies the algorithms in Mossel and Roch (2006); Hsu et al. (2009); Anandkumar et al. (2012), in that the eigenvector methods are no longer necessary (e.g. the approach leads to methods for parameter estimation in HMMs with only two SVDs rather than using eigenvector approaches, as in previous work). Showing that estimating the third order moments is not as difficult as it might naively seem since we only need a k × k matrix to be accurate. For a detailed discussion of these techniques, refer to Anandkumar et al. (2012). The underlying intuition and idea of separability in Arora et al.",
      "startOffset" : 8,
      "endOffset" : 2067
    }, {
      "referenceID" : 0,
      "context" : "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools. The underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996). In particular, we show that the “topic” modeling problem exhibits a rather simple algebraic solution, where only two SVDs suffice for parameter estimation. Moreover, this approach also simplifies the algorithms in Mossel and Roch (2006); Hsu et al. (2009); Anandkumar et al. (2012), in that the eigenvector methods are no longer necessary (e.g. the approach leads to methods for parameter estimation in HMMs with only two SVDs rather than using eigenvector approaches, as in previous work). Showing that estimating the third order moments is not as difficult as it might naively seem since we only need a k × k matrix to be accurate. For a detailed discussion of these techniques, refer to Anandkumar et al. (2012). The underlying intuition and idea of separability in Arora et al. (2012) may help for practical guidance in the choice of θ to help reduce the sample size, in terms of the k dependence.",
      "startOffset" : 8,
      "endOffset" : 2141
    }, {
      "referenceID" : 3,
      "context" : "This construction bridges the gap between the single topic models (as in Chang (1996); Anandkumar et al.",
      "startOffset" : 73,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "This construction bridges the gap between the single topic models (as in Chang (1996); Anandkumar et al. (2012)) and the independent factor model.",
      "startOffset" : 87,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : "This construction bridges the gap between the single topic models (as in Chang (1996); Anandkumar et al. (2012)) and the independent factor model. More generally, the multi-view approach has been exploited in previous works for semi-supervised learning and for learning mixtures of well-separated distributions (e.g as in Ando and Zhang (2007); Kakade and Foster (2007); Chaudhuri and Rao (2008); Chaudhuri et al.",
      "startOffset" : 87,
      "endOffset" : 344
    }, {
      "referenceID" : 0,
      "context" : "This construction bridges the gap between the single topic models (as in Chang (1996); Anandkumar et al. (2012)) and the independent factor model. More generally, the multi-view approach has been exploited in previous works for semi-supervised learning and for learning mixtures of well-separated distributions (e.g as in Ando and Zhang (2007); Kakade and Foster (2007); Chaudhuri and Rao (2008); Chaudhuri et al.",
      "startOffset" : 87,
      "endOffset" : 370
    }, {
      "referenceID" : 0,
      "context" : "This construction bridges the gap between the single topic models (as in Chang (1996); Anandkumar et al. (2012)) and the independent factor model. More generally, the multi-view approach has been exploited in previous works for semi-supervised learning and for learning mixtures of well-separated distributions (e.g as in Ando and Zhang (2007); Kakade and Foster (2007); Chaudhuri and Rao (2008); Chaudhuri et al.",
      "startOffset" : 87,
      "endOffset" : 396
    }, {
      "referenceID" : 0,
      "context" : "This construction bridges the gap between the single topic models (as in Chang (1996); Anandkumar et al. (2012)) and the independent factor model. More generally, the multi-view approach has been exploited in previous works for semi-supervised learning and for learning mixtures of well-separated distributions (e.g as in Ando and Zhang (2007); Kakade and Foster (2007); Chaudhuri and Rao (2008); Chaudhuri et al. (2009)).",
      "startOffset" : 87,
      "endOffset" : 421
    }, {
      "referenceID" : 0,
      "context" : "For example, Anandkumar et al. (2012) consider a special case of this multi-view model (where there is only one topic present in h) for the purposes of learning hidden Markov models.",
      "startOffset" : 13,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "For example, Anandkumar et al. (2012) consider a special case of this multi-view model (where there is only one topic present in h) for the purposes of learning hidden Markov models. A simple factorial HMM: Here, suppose we have a time series of random hidden vectors h1, h2, h3, . . . and observations x1, x2, x3, . . . (we slightly abuse notation as h1 is a vector). Assume that each factor [ht]i ∈ {−1, 1}. The model parameters and evolution are specified as follows: We have an initial (product) distribution over the first h1. The “factorial” assumption we make is that each factor [ht]i evolves independently; in particular, for each component i, there are (time independent) transition probabilities pi,1→−1 and pi,1→−1. Also suppose that E[xt|ht] = Oht (where, again, O does not depend on the time). To learn this model, consider the first three observations x1, x2, x3. We can embed this three timestep model into the multiview model using a single hidden state, namely h2, and, with an appropriate construction (of O1, O2, O3 and means shifts of xv to make the linearity assumption hold). Furthermore, if we recover O1, O2, O3 we can recover O and the transition model. See Anandkumar et al. (2012) for further discussion of this idea (for the single topic case).",
      "startOffset" : 13,
      "endOffset" : 1209
    }, {
      "referenceID" : 8,
      "context" : "See Comon and Jutten (2010)).",
      "startOffset" : 4,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "As discussed in the Introduction, these approaches can been seen as extensions of the methodologies in Chang (1996); Cardoso and Comon (1996).",
      "startOffset" : 103,
      "endOffset" : 116
    }, {
      "referenceID" : 3,
      "context" : "As discussed in the Introduction, these approaches can been seen as extensions of the methodologies in Chang (1996); Cardoso and Comon (1996). Furthermore, as we shall see, the Dirichlet distribution bridges between the single topic models (as in Chang (1996); Anandkumar et al.",
      "startOffset" : 117,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : "As discussed in the Introduction, these approaches can been seen as extensions of the methodologies in Chang (1996); Cardoso and Comon (1996). Furthermore, as we shall see, the Dirichlet distribution bridges between the single topic models (as in Chang (1996); Anandkumar et al.",
      "startOffset" : 117,
      "endOffset" : 260
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, as we shall see, the Dirichlet distribution bridges between the single topic models (as in Chang (1996); Anandkumar et al. (2012)) and the independent factor model.",
      "startOffset" : 118,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "(Limiting behaviors) ECA seamlessly blends between the single topic model (α0 → 0) of Anandkumar et al. (2012) and the skewness based ECA, Algorithm 1 (α0 → ∞).",
      "startOffset" : 86,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "(Limiting behaviors) ECA seamlessly blends between the single topic model (α0 → 0) of Anandkumar et al. (2012) and the skewness based ECA, Algorithm 1 (α0 → ∞). In the single topic case, Anandkumar et al. (2012) provide eigenvector based algorithms.",
      "startOffset" : 86,
      "endOffset" : 212
    }, {
      "referenceID" : 0,
      "context" : "Using the methods in Anandkumar et al. (2012), eigenvector based method are straightforward to derive.",
      "startOffset" : 21,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "(Simpler algorithms for HMMs) Mossel and Roch (2006); Anandkumar et al. (2012) provide eigenvector based algorithms for HMM parameter estimation.",
      "startOffset" : 54,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "(Simpler algorithms for HMMs) Mossel and Roch (2006); Anandkumar et al. (2012) provide eigenvector based algorithms for HMM parameter estimation. These results show that we can achieve parameter estimation with only two SVDs (See Anandkumar et al. (2012) for the reduction of an HMM to the multi-view setting).",
      "startOffset" : 54,
      "endOffset" : 255
    }, {
      "referenceID" : 0,
      "context" : "see Anandkumar et al. (2012) for example).",
      "startOffset" : 4,
      "endOffset" : 29
    } ],
    "year" : 2012,
    "abstractText" : "Topic models can be seen as a generalization of the clustering problem, in that they posit that observations are generated due to multiple latent factors (e.g. the words in each document are generated as a mixture of several active topics, as opposed to just one). This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic probability vectors (the distributions over words for each topic), when only the words are observed and the corresponding topics are hidden. We provide a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of mixture models, including the popular latent Dirichlet allocation (LDA) model. For LDA, the procedure correctly recovers both the topic probability vectors and the prior over the topics, using only trigram statistics (i.e. third order moments, which may be estimated with documents containing just three words). The method, termed Excess Correlation Analysis (ECA), is based on a spectral decomposition of low order moments (third and fourth order) via two singular value decompositions (SVDs). Moreover, the algorithm is scalable since the SVD operations are carried out on k × k matrices, where k is the number of latent factors (e.g. the number of topics), rather than in the d-dimensional observed space (typically d ≫ k).",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}