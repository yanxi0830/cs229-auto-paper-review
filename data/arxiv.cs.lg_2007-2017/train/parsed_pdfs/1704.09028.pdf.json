{
  "name" : "1704.09028.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Time-Sensitive Bandit Learning and Satisficing Thompson Sampling",
    "authors" : [ "Daniel Russo", "David Tse", "Benjamin Van Roy" ],
    "emails" : [ "daniel.russo@kellogg.northwestern.edu", "bvr@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "As high level motivation, consider a recommendation system that interacts sequentially with a single user. The system faces the classic tradeoff between exploration and exploitation: by experimenting with different recommendations the system can learn to offer more effective personalized recommendations in the future, but this may require some degradation of current performance. While recommendation systems are often used as a motivating example for studying the multiarmed bandit problem, this problem has several salient features that are not addressed well by standard bandit algorithms and analysis (e.g. the UCB1 algorithm and analysis of Auer et al. [1]). First, modern recommendation systems have an enormous number of products, but when begining to interact with a new user, the system has a great deal of historical data from interactions with different users, and therefore begins with significant prior knowledge about the products. This prior knowledge presents itself in multiple ways. As certain products are typically much more popular than others, the system begins with evidence that certain types of recommendations will be more successful than others. In addition, data can be used to uncover relevant features of items and users, for example through matrix-completion. As a result, experience offering one type of item to a user can provide significant information about whether they will like a different type of product. Another distinguishing feature of this problem is the presence of a limited and uncertain horizon. The limited number of interactions means that a recommendation system likely won’t have enough experience with each single user to perfectly tailor its recommendations to them. Instead, it is natural to hope for a system that quickly learns to offer highly effective, but still suboptimal, recommendations to its users. The uncertain horizon means that one can’t predict a priori how many\nar X\niv :1\n70 4.\n09 02\n8v 1\n[ cs\n.L G\n] 2\n8 A\npr 2\ntimes the system will interact with a single user. As a result it is especially valuable to have strong performance during early interactions.\nThis work focuses on developing algorithms and a framework for theoretical analysis to address problems with these salient features. We build on the Thompson sampling algorithm (TS) [16] and a recent information theoretic analysis of its performance [14], but offer substantial advances to this thread of theoretical work. TS is able to leverage very general forms of prior information, including rich statistical models that encode a relationship between actions, and prior knowledge that some actions are more likely to offer strong performance than others. The information theoretic analysis of TS yields regret bounds that scale with the entropy of the optimal action distribution. This dependence reflects the performance benefits of prior information but also points to a substantial potential weakness. In particular, entropy generally increases with the number of actions and becomes infinite when they form a continuum. Such regret bounds can therefore be irrelevant when action spaces are very large or infinite. At the heart of this issue is the emphasis Thompson sampling and this information theoretic analysis place on identification of an optimal action. There are circumstances when a near-optimal action can be identified quickly even though an optimal one proves elusive.\nInstead of focusing on cumulative regret, we will compare algorithms based on their expected discounted regret, where the discount factor encodes time preferences. Note that minimizing expected discounted regret is equivalent to minimizing expected undiscounted regret in a problem where the horizon is a geometric random variable, and hence is uncertain. We introduce satisficing Thompson sampling (STS), a modified form of Thompson sampling designed to address problems with limited horizon. We bound discounted-regret by leveraging the information-theoretic concept of rate-distortion, which offers a means for reasoning about the value of information that is useful for identifying near-optimal, not just optimal, actions. Through simulation results, and instantiating these regret bounds on specific examples, we show STS can dramatically outperform TS and standard UCB algorithms when the optimal action is costly to learn relative to high-performing suboptimal actions.\nMany papers [5, 12, 13] have studied bandit problems with continuous action spaces, where it is also necessary to learn only approximately optimal actions. However, because these papers focus on the asymptotic growth rate of regret they implicitly emphasize later stages of learning, where the algorithm has already identified extremely high performing actions but exploration is needed to identify even better actions. Our discounted framework instead focuses on the initial cost of learning to attain good, but not perfect, performance. Recent papers [9, 10] study several heuristics for a discounted objective, though without an orientation toward formal regret analysis. The Knowledge Gradient algorithm of [15] also takes time horizon into account and can learn suboptimal actions when its not worthwhile to identify the optimal action. This algorithm tries to directly approximate the optimal Bayesian policy using a one-step lookahead heuristic, but unfortunately there are no performance guarantees for this method. Deshpande and Montanari [8] consider a linear bandit problem with dimension that is too large relative to the desired horizon. They propose an algorithm that limits exploration and learns something useful within this short time frame. Berry et al. [2], Wang et al. [17] and Bonald and Proutiere [3] study an infinitely-armed bandit problem in which it’s impossible to identify an optimal action and propose algorithms to minimizes the asymptotic growth rate of regret. While we will instantiate our general regret bound for STS on the infinitely-armed bandit problem, we use this example mostly to provide a simple analytic illustration. We hope that the flexibility of STS and our analysis framework allow this work to be applied to more complicated time-sensitive learning problems."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "An agent sequentially chooses actions (At)t∈N0 from the action set A and observes the corresponding outcomes (Yt,At)t∈N0 . There is a random outcome Yt,a ∈ Y associated with each a ∈ A and time t ∈ N0 ≡ {0, 1, 2..}. Let Yt ≡ (Yt,a)a∈A be the vector of outcomes at time t ∈ N0. There is a random variable θ such that, conditioned on θ, (Yt)t∈N0 is an iid sequence. Ours can be thought of as a Bayesian formulation, in which the distribution of θ represents the agent’s prior uncertainty about the true characteristics of the system, and conditioned on θ, the remaining randomness in Yt represents idiosyncratic noise in observed outcomes.\nThe agent associates a reward R(y) with each outcome y ∈ Y. Let Rt,a ≡ R(Yt,a) denote the reward corresponding to outcome Yt,a. The history available when selecting action At is\nFt = (A0, Y0,A0 , . . . , At−1, Yt−1,At−1).\nThe agent selects actions according to a policy, which is a sequence of functions (πt : t ∈ N0), each mapping a history and an exogenous random variable ξ to an action. In particular At = πt(Ft, ξ) for each t, where ξ is some random variable that is independent of θ and (Yt : t ∈ N0).\nWe denote by Y∞ an independent copy of Yt. In particular, P(Y∞ ∈ ·|θ) = P(Yt ∈ ·|θ) but conditioned on θ, Y∞ is drawn independently of (Yt : t ∈ N0). Let A∗ ∈ argmaxa∈A E[R(Y∞,a)|θ] denote the true optimal action and let R∗ = maxa∈AE[R(Y∞,a)|θ] denote the corresponding reward. As a performance metric, we consider expected discounted regret, defined by\nE [ ∞∑ t=0 αt(R∗ −Rt,At) ] ,\nwhich measures a discounted sum of the expected performance gap between a benchmark policy which always chooses the optimal action A∗ and the chosen actions (At : t ∈ N0). This deviates from the typical notion of expected regret in its dependence on a discount factor α ∈ [0, 1]. Regular expected regret corresponds to the case of α = 1. Smaller values of α convey time preference by weighting gaps in nearer-term performance higher than gaps in longer-term performance.\nIt is worth noting that minimizing expected discounted regret is equivalent to maximizing expected discounted reward, which is the objective used in the classical Bayesian formulation of the multi-armed bandit problem [11]. For problems of the kind we consider, expected discounted reward can in principle be maximized via dynamic programming. However, solving the associated dynamic programs is computationally intractable. As such, similarly with the bulk of the recent bandit learning literature, we consider heuristic policies and aim to bound regret, though in this paper we consider a discounted variation of regret."
    }, {
      "heading" : "3 Algorithms",
      "text" : "Thompson sampling (TS) is a popular algorithm that implements a useful decision policy. Over each tth period, TS selects an action At as follows:\n1. Sample θ̂t ∼ P(θ|Ft)\n2. Let At ∈ arg maxa∈AE [ Rt,a|θ = θ̂t ] We will assume that actions are indexed and that ties are broken by selecting the action with the smallest index. Note that, as should be the case for any decision policy, we can write TS\nas At = πt(Ft, ξ), for an appropriately defined (πt : t ∈ N0), where ξ is independent of θ and (Yt : t ∈ N0).\nAs one key contribution of this paper, we introduce a modification of TS, which we will call satisficing Thompson sampling (STS). While TS aims to identify an optimal action, STS is designed to identify an action that is sufficiently satisfying, or close enough to optimal. Over each tth period, STS selects an action At as follows:\n1. Sample θ̂t ∼ P(θ|Ft)\n2. Let At ∈ arg maxa∈AE [ Rt,a|θ = θ̂t ] 3. Let τ̂ = min { τ ∈ {1, . . . , t− 1} : E [ Rt,Aτ |θ = θ̂t ] + ≥ E [ Rt,At |θ = θ̂t\n]} 4. If τ̂ is not null then let At = Aτ̂\nNote that ≥ 0 is supplied to the algorithm as a tolerance parameter. When = 0, STS is equivalent to TS. Otherwise, STS attributes preference to selecting previously selected actions. As we will further discuss and formalize, this can result in substantial benefit in the face of time preference. In particular, when the optimal action requires a long time to learn but an -optimal action can be learned quickly, STS can quickly achieve -optimal performance where Thompson sampling would continue to incur significant losses deploying resources toward eventual identification of the optimal action.\nIt is worth mentioning that STS can be applied efficiently across the wide variety of problems that are amenable to Thompson sampling. This includes, for example, complex parametric bandit problems. For example, we present in Section 5 computational results involving a linear bandit problem with many arms and many parameters to learn.\nA probability matching property: Thompson sampling satisfies a powerful probability matching property: under TS, Pt(At = a) = Pt(A∗ = a) for all a ∈ A, and so action-sampling probabilities are matched to the posterior distribution of the optimal action. Under STS, actionsampling probabilities instead are essentially matched to the posterior-distribution of the first –optimal action sampled by the algorithm. More precisely, if τ = inf{t|E[Rt,At |θ] ≥ R∗ − } then at time t STS sets Pt(At = Ak) = Pt(τ = k) for each k < t. With probability Pt(τ ≥ t) STS selects a new, or previously un-sampled action. In this way, the algorithm aims to identify a satisfactory action while concentrating exploration effort on the smallest number of arms required to do so."
    }, {
      "heading" : "4 Example: Infinitely-Armed Deterministic Bandit",
      "text" : "To clarify our motivation, we now provide a simple analytic illustration of advantages enjoyed by STS. Consider a problem with a countable action space A = {1, 2, . . .} in which each action a ∈ A yields reward θa. Our prior over each θa is independent and uniform over the interval [0, 1]. The optimal reward is almost surely R∗ = 1.\nFor this problem, which we refer to as the infinitely-armed deterministic bandit problem, TS never selects the same action twice. This is because, with probability one, no action selected within a finite time horizon yields reward 1, and as such, at any point in time, there are better actions than those previously selected by TS. STS, in contrast, stops searching after finding an action that generates reward exceeding 1 − . After such an action is identified, STS repeatedly selects that action.\nThe benefits of STS can be formalized in terms of bounds on expected discounted regret. The following result, proved in the appendix, provides an expression for the expected discounted regret of TS in our infinitely-armed deterministic bandit problem.\nTheorem 1. For all α ∈ [0, 1], under Thompson sampling in the infinitely-armed deterministic bandit problem then\nE [ ∞∑ t=0 αt(R∗ −Rt,At) ] = 12(1− α) .\nIt is enlightening to compare this to the following bound on expected discounted regret of STS in our infinite deterministic bandit problem, which is also proved in the appendix. Theorem 2. For all α ∈ [0, 1], under satisficing Thompson sampling with tolerance = √\n1− α in the infinitely-armed deterministic bandit problem,\nE [ ∞∑ t=0 αt(R∗ −Rt,At) ] ≤ 1√ 1− α .\nFor α close to 1, 1/ √\n1− α 1/(1− α), and therefore STS vastly outperforms TS. In fact, as α approaches 1, the ratio between expected regret of TS and that of STS goes to infinity."
    }, {
      "heading" : "5 Computational Examples",
      "text" : "Computational studies involving a broader range of bandit problems further illustrate potential benefits afforded by STS. In this section, we present results from experiments with four bandit problems. Each case is designed so that near-optimal actions can be identified far sooner than the optimal action. In each case, the per-period regret of STS diminishes more rapidly than that of TS over early time periods.\nOur first is a deterministic bandit problem with 250 actions. The mean reward associated with each action is independently sampled from unif([0, 1]). When an action is sampled the realized reward is equal to the mean reward; in other words, there is no observation noise. Figure 1(a) plots per-period regret of TS and STS over 500 time periods, averaged over 5000 simulations, each with an independently sampled problem instance. For STS, we used a tolerance parameter of 0.05.\nWe next consider a problem that is the same as our previous one except with observation noise. In particular, instead of observing the mean reward, after selecting an action, we observe a binary reward that is one with probability equal to the mean reward. Figure 1(b) plots average per-period regret over 5000 simulations. For STS, we used a tolerance parameter of 0.05.\nWe now consider another bandit problem with 250 actions, each with a mean reward sampled independently independently from N(0, 1). Upon taking an action, we observe the sum of the action’s mean reward and an independent N(0, 1) sample that represents observation noise. Figure 1(c) plots per-period regret of TS and STS over 500 time periods, averaged over 5000 simulations, each with an independently sampled problem instance. For STS, we used a tolerance parameter of 0.5.\nFinally, we consider a bandit problem with mean rewards given by a linear function. In particular, mean rewards are given by a vector Lθ ∈ <|A|, where L ∈ <|A|×M is a randomly generated loadings matrix, with each row independently drawn uniformly from the unit sphere, and θ ∈ <M is sampled from N(0, I). For our computational study, we let A = {1, . . . , 250} and M = 250. The decision-maker knows L and begins with a N(0, I) prior on θ. Upon taking an action, we observe the sum of the action’s mean reward and an independent N(0, 2) sample that represents observation\nnoise. Figure 1(d) plots per-period regret of TS and STS over 500 time periods, averaged over 5000 simulations, each with an independently sampled problem instance. For STS, we used a tolerance parameter of 1.0."
    }, {
      "heading" : "6 A General Regret Bound",
      "text" : "This section provides a general discounted regret bound and a new information-theoretic analysis technique. We’ll leverage this general regret bound when analyzing STS in the next section. We begin by reviewing the information-theoretic analysis of Thompson sampling of Russo and Van Roy [14], on which our analysis builds. Before beginning, let us first introduce some additional notation."
    }, {
      "heading" : "6.1 Notation",
      "text" : "We denote by Et[·] = E[·|Ft] the expectation operator conditioned on the history up to time t and similarly define Pt(·) = P(·|Ft). We denote the entropy of a discrete random variable X by H(X),\nthe mutual information between two random variablesX and Y by I(X;Y ) and the Kullback-Leibler divergence between probability distributions P and Q by D(P ||Q). The definitions of entropy and mutual information depend on a base measure. We use Ht(·) and It(· , ·) to denote entropy and mutual-information when the base-measure is the posterior distribution Pt. For example, if X is a discrete random variable taking values in a set X ,\nHt(X) = − ∑ x∈X Pt(X = x) log Pt(X = x).\nDue to its dependence on the realized history Ft, Ht(X) is a random variable. The standard definition of conditional entropy integrates over this randomness, and in particular, E[Ht(X)] = H(X|Ft)."
    }, {
      "heading" : "6.2 Information Theoretic Analysis of Thompson Sampling",
      "text" : "The regret analysis in [14] relates the regret an algorithm incurs to the information it acquires about the identity of optimal action A∗ ∈ arg maxa E[R(Y∞,a)|θ]. They define the information ratio in a given period to be the ratio between the square of single-period expected regret and the information acquired about the optimal action:\nEt[R∗ −Rt,At ]2\nIt(A∗;Yt,At |ξ) . (1)\nIt’s shown that every algorithm satisfies a bound on un-discounted expected-regret up period T in terms of its average information ratio over the first T periods and the entropy of the optimal action H(A∗). Here the information ratio roughly captures the cost-per-bit of information the algorithm acquires about the optimum, and the entropy H(A∗) measures the magnitude of the decision-maker’s initial uncertainty about the identity of the optimal action. For a number of widely studied classes of online optimization problems, strong regret bounds for Thompson sampling can be derived by bounding the algorithm’s information ratio. Subsequent work by Bubeck et al. [6] and Bubeck and Eldan [4] bounds the information-ratio for bandit problems with convex reward functions."
    }, {
      "heading" : "6.3 A Modified Information Ratio",
      "text" : "This section introduces a modified information ratio, which is more appropriate for time-sensitive online learning problems. As motivation, consider the infinitely-armed deterministic bandit of Section 4. While no algorithm could identify an optimal action in that example, STS is able to efficiently converge to a satisfactory level of performance. In this sense, although the algorithm can’t identify the true optimum, it seems to acquire enough information to identify some highreward action Ã. Building on this intuition, our information-theoretic analysis will aim to formally relate regret to the information the algorithm acquires about this Ã. To help ground this discussion, consider two examples of such an Ã arising from different problem settings.\nExample 1. Consider the infinitely-armed deterministic bandit of Section 4. As time progresses, STS samples a sequence of actions (A0, A1, A2, ...). Let τ = min{t|θAt ≥ 1 − } denote the first time the algorithm samples an action that is –optimal and set Ã := Aτ to be the corresponding action. In this example, there are many –optimal, or ”satisfactory” actions, and Ã is taken to be the first one discovered by the algorithm.\nExample 2. Consider a bandit problem where mean-rewards are given by a linear function. In particular, A ⊂ Rd, and E[Rt,a|θ] = aT θ for an unknown vector θ. Suppose that θ ∼ N(0, I) and A consists of n vectors spread out uniformly along boundary of the d dimensional unit sphere {a ∈ Rd : ‖a‖2 = 1}. The optimal action A∗ = arg maxa∈A θTa is then uniformly distributed over A, and hence H(A∗) = logn. Here entropy tends to infinity the number of actions grows, and it takes an enormous amount of information to exactly identify A∗. For this example, we might take Ã to be a coarser version of A∗. In particular, for m n, let Ã consist of m vectors spread out uniformly along boundary of the d dimensional unit sphere {a ∈ Rd : ‖a‖2 = 1} and let Ã = arg maxa∈A θTa. This can be viewed as a form of lossy-compression, where one may have H(Ã) H(A∗) but E[Rt,Ã] ≥ E[Rt,A∗ ] + for some small > 0.\nIn each of these examples, the action Ã can be viewed as some random variable taking values in the action set A. In the second example, Ã is a deterministic function of θ, and is random only because of the randomness in θ. In the first example, Ã also depends on the algorithm’s internal randomness, which determines the order in which actions are sampled.\nTo address problems of this form, we introduce the following modified information ratio. For any (random) action Ã and (random) action process {At : t ∈ N0}, define\nΓ ( Ã, {At : t ∈ N0} ) = (1− α2) ∞∑ t=0 α2tE [ Et[R̃−Rt,At ]2 It(Ã;Yt,At |ξ) ] , (2)\nwhere R̃ = R(Y∞,Ã). Recall that Y∞ denotes an independent sample of the action-outcome vector. The subscript of Et and It indicates that the random variables are drawn from the probability space conditioned on Ft. The ratio Et[R̃ − Rt,At ]2/It(Ã;Yt,At |ξ) relates the current shortfall in performance relative to the benchmark action Ã to the amount of information acquired about the benchmark action. The right-hand-side of (2) is the discounted average of these single-period ratios. The square in the discount factor α is consistent with the problem’s original discount rate, since Et[αt(R̃−Rt,At)]2 = α2tEt[R̃−Rt,At ]2."
    }, {
      "heading" : "6.4 General Regret Bound",
      "text" : "The following theorem bounds the expected discounted regret of any algorithm, or action process, {At : t ∈ N0}, in terms of the information ratio (2).\nTheorem 3. For any action process {At : t ∈ N0} and Ã : Ω→ A\nE [ ∞∑ t=0 αt(R∗ −Rt,At) ] ≤ E[R ∗ − R̃] 1− α + √√√√Γ (Ã, {At : t ∈ N0})H(Ã|ξ) 1− α2 . (3)\nwhere R̃ = R(Y∞,Ã).\nThis bound decomposes regret into the sum of two terms; one which captures the discounted performance shortfall of the benchmark action Ã relative toA∗, and one which bounds the additional regret incurred while learning to identify Ã. Breaking things down further, the entropy H(Ã|ξ) measures the magnitude of the decision-maker’s initial uncertainty about Ã, and the information ratio measures the regret incurred in reducing this uncertainty. It’s worth highlighting that for any given action process, this bound holds simultaneously for all possible choices of Ã, and in particular, it holds for the Ã minimizing the right hand side of (3)."
    }, {
      "heading" : "6.5 Connections to Rate Distortion Theory",
      "text" : "In information-theory, the entropy of a source characterizes the length of an optimal lossless encoding. The celebrated rate-distortion theory [7, Chapter 10] characterizes the number of bits required for an encoding to be close in some loss metric. This theory resolves when it is possible to to derive a satisfactory lossy compression scheme while transmitting far less information than required for a lossless compression. At a high level, the developments in this paper represent a shift from entropy to the use of rate-distortion function. Whereas prior results depend on the entropy of A∗, Theorem 3 depends on a naturally defined rate distortion function for compressing the optimal decision A∗:\nR(D) := min E[R∗−R̃]≤D\nI(Ã;A∗).\nWhen Ã depends deterministically on A∗, I(Ã;A∗) = H(Ã), and hence the rate-distortion function describes the optimal tradeoff between the loss in reward E[R∗− R̃] and the entropy of Ã, precisely what is needed in minimizing the right hand side of (3)."
    }, {
      "heading" : "7 Information Ratio Analysis of the Infinitely-Armed Bandit",
      "text" : "The general regret bound of the previous section can be instantiated on two variants of the infinitearmed bandit problem. The next subsection revisits the deterministic infinite-armed bandit of Section 4, and shows how to derive a regret bound for STS using Theorem 3. Subsection 7.2 studies an extension of the infinite-armed bandit problem in which reward-observations are noisy. Again, in this setting Theorem 3 can be specialized to derive a regret bound for STS."
    }, {
      "heading" : "7.1 Infinitely–Armed Bandit with Deterministic Observations",
      "text" : "We now revisit the infinitely-armed deterministic bandit problem of Section 4. By specializing our general regret bound this setting, we will effectively recover the bound of Theorem 2 that was derived from direct analysis. Because there is no observation noise in this example, once STS samples an action with reward exceeding 1 − , it will sample it in all subsequent periods. Before that point, the algorithm knows with certainty that no previously-sampled action generates reward exceeding 1 − , and so a new action will be selected in every period. Let τ = min{t|θAt ≥ 1 − } denote the first time an –optimal action is sampled. The next result applies the general regret bound of Theorem 3 to this problem with Ã = Aτ , so the benchmark action is the first –optimal action sampled by STS.\nTheorem 4. For any α ∈ (0, 1), if STS is applied to the deterministic infinite bandit problem with tolerance ∈ (0, 1) then\nH(Ã|ξ) = H(τ) and Γ ( Ã, {At : t ∈ N0} ) ≤ 14 H(τ)\nwhere τ = min{t|θAt ≥ 1− } follows a Geometric distribution with parameter and Ã = Aτ . This implies that if = √ (1− α)/2,\nE [ ∞∑ t=0 αt(R∗ −Rt,At) ] ≤ √ 2 1− α."
    }, {
      "heading" : "7.2 Infinitely–Armed Bandit with Noisy Observations",
      "text" : "Now consider a generalization of the problem treated in the previous section that allows for noisy observations and non-uniform priors. We again assume there is a countable action space A = {1, 2, . . .}. Each action a ∈ A yields expected reward E[Rt,a|θ] = θa where the θa are drawn independently from a distribution whose support is the unit interval [0, 1]. Assume rewards are bounded in [0, 1] almost surely.\nWe’ll study the discounted regret incurred by STS with parameter ∈ (0, 1). Each action sampled by STS is –optimal with probability δ ≡ P(θa > 1 − ), but because observations are noisy, the algorithm may be uncertain about the quality of the actions it has sampled. The next result provides a regret bound for STS in this more complicated setting. The proof again leverages Theorem 3 with the benchmark action Ã taken to be the first –optimal action sampled by the algorithm. By bounding the problem’s information-ratio, we relate the regret incurred by STS to the information it acquires about the identity of Ã.\nTheorem 5. Suppose STS with tolerance parameter ∈ (0, 1) is applied to the infinite-armed bandit with noisy observations. Then, with probability 1, there exists t ∈ N0 with θAt ≥ 1− . If Ã = Aτ where τ = min{t : θAt ≥ 1− }, then\nH(Ã|ξ) ≤ 1 + log(1/δ) and Γ ( Ã, {At : t ∈ N0} ) ≤ 6 + 4/δ + (2/δ) log ( 1 1− α2 ) .\nTogether with Theorem 3 this implies\nE [ ∞∑ t=0 αt(R∗ −Rt,At) ] ≤ 1− α + √√√√(6 + 4/δ + (2/δ) log ( 11−α2)) (1 + log(1/δ)) 1− α2\n= Õ  1− α + √ 1/δ 1− α2  ."
    }, {
      "heading" : "8 Conclusion",
      "text" : "This paper introduces satisficing Thompson sampling – a variation of Thompson sampling that can offer vastly superior performance when the optimal action is costly to identify relative to high performing suboptimal actions. We have also developed a general information-theoretic framework for analyzing discounted regret. This framework provides a novel link between optimal decisionmaking with time preferences and the study of lossy data compression. Important questions remain open, but we hope this link will open up many paths for future research."
    }, {
      "heading" : "A Proof of Theorem 1: Regret of TS on the Infinitely-Armed Deterministic Bandit",
      "text" : "Proof. In every period t, TS samples a previously un-sampled action At /∈ {A1, ..., At−1}, which generates expected reward E[θAt ] = E[θ1] = 1/2. The optimal expected reward is 1, and therefore the expected discounted-regret of TS is\n∞∑ t=0 αt(1− 1/2) = 12(1− α) ."
    }, {
      "heading" : "B Proof of Theorem 2: Direct Analysis of the Infinitely-Armed Deterministic Bandit",
      "text" : "Proof. Let τ = min{t : θAt ≥ 1− }.\nE [ ∞∑ t=0 αt(R∗ −Rt) ] = E [ ∞∑ t=0 αt(1−Rt) ]\n= E [ E [ τ−1∑ t=0 αt(1−Rt) + ∞∑ t=τ αt(1−Rt,At) ∣∣∣τ]]\n= E [(1− ατ )(1− )\n2(1− α) + ατ 2(1− α) ] = E [(1− ατ )(1− ) 2(1− α) + 2(1− α) − (1− ατ ) 2(1− α)\n] = E [\n2(1− α) + (1− ατ )(1− 2 ) 2(1− α)\n] .\nNote that\nE[1− ατ ] = 1− ∞∑ t=0 (1− )tαt = 1− 1− α+ α = 1− α− + α 1− α+ α = (1− α)(1− ) 1− α(1− ) .\nTherefore E [(1− ατ )(1− 2 )\n2(1− α)\n] = (1− )(1− 2 )2(1− α(1− )) = (1− )(1− 2 ) 2( + (1− α)(1− )) .\nNow consider an upper bound that follows from choosing as a function of α. We can simplify our upper bound on regret as\n2(1− α) + (1− )(1− 2 ) 2( + (1− α)(1− )) ≤ 2(1− α) + 1 2\nThe minimizer of the right hand side is ∗ = √\n1− α. Plugging this in shows that Thompson sampling with a confidence bonus of ∗ satisfies the discounted regret bound\nE [ ∞∑ t=0 αt(R∗ −Rt,At) ] ≤ 1√ 1− α ."
    }, {
      "heading" : "C Proof of Theorem 3",
      "text" : "Proof. We first show that entropy bounds the expected accumulation of mutual-information. By the chain rule for mutual information, for any T ,\nE [ T−1∑ t=0 It(Ã;Yt,At |ξ) ] = T−1∑ t=0 I(Ã;Yt,At |ξ,Ht)\n= T−1∑ t=0 I(Ã;Yt,At |ξ, A0, Y0,A0 , . . . , At−1, Yt−1,At−1)\n= T−1∑ t=0 I(Ã;Yt,At |ξ, A0, Y0,A0 , . . . , At−1, Yt−1,At−1 , At)\n= I(Ã; (A0, Y0,A0 , . . . , At, Yt,At)|ξ) = H(Ã|ξ)−H(Ã|A0, Y0,A0 , . . . , At, Yt,At , ξ) ≤ H(Ã|ξ).\nTaking a the limit as T →∞ implies\nE [ ∞∑ t=0 It(Ã;Yt,At |ξ) ] = lim T→∞ E [ T∑ t=0 It(Ã;Yt,At |ξ) ] ≤ H(Ã|ξ),\nwhere the monotone convergence theorem justifies the exchange of limit and expectation. Now, fix any Ã and {At : t ∈ N0}, and let\nΓt ≡ Et[R̃−Rt,At ]2\nIt(Ã;Yt,At |ξ))\ndenote the (random) information ratio at time t under the benchmark action Ã and action process {At : t ∈ N0}. Then we have\nE [ ∞∑ t=0 αt(R∗ −Rt,At) ] = E [ ∞∑ t=0 αt(R∗ − R̃) ] + E [ ∞∑ t=0 αt(R̃−Rt,At) ]\n= E [ R∗ − R̃ ] 1− α + E [ ∞∑ t=0 √ α2tΓt √ It(Ã;Yt,At)|ξ) ]\n≤ E [ R∗ − R̃ ] 1− α + √√√√E [ ∞∑ t=0 α2tΓt ]√√√√E [ ∞∑ t=0 It(Ã;Yt,At |ξ) ]\n≤ E [ R∗ − R̃ ] 1− α + √√√√E [ ∞∑ t=0 α2tΓt ]√ H(Ã|ξ)\n= E [ R∗ − R̃ ] 1− α + √√√√Γ (Ã, {At : t ∈ N0})H(Ã|ξ) 1− α2 ,\nwhere the first inequality follows from the Cauchy-Schwarz inequality and the second was established earlier in this proof."
    }, {
      "heading" : "D Proof of Theorem 4: Information-Ratio Analysis of Infinitely– Armed Deterministic Bandit",
      "text" : "Lemma 6. Under STS with tolerance ∈ (0, 1) in the infinitely–armed deterministic bandit problem, τ = min{t|θAt ≥ 1 − } follows a Geometric distribution with parameter , and if Ã = Aτ then\nIt(Ã;Yt,At |ξ) = { H(τ) if Et[R∗ −Rt,At ] > 0 otherwise.\nProof. As time progresses, STS samples actions A1, A2, A3.... At each time t <= τ , it selects a previously un-sampled action At /∈ {A1, ...At−1}. It selects the actions At = Aτ in each period t > τ . Because P(θa ≥ 1− ) = for each a, we have that τ follows a Geometric distribution with parameter . Conditioned on τ ≥ t, the identity of At is determined by the algorithm’s internal random bits ξ. That is, the order of the new actions sampled by the algorithm is a function only of ξ. Therefore, H(Ã|ξ) = H(τ).\nUnder STS, if Et[R∗−Rt,At ] ≤ then At = Ã, and Γt = 0 since Et[R̃−Rt,At ] = 0. On the other hand, if Et[R∗ −Rt,At ] > then At /∈ {A1, ...At−1} and\nIt(Ã;Yt,At |ξ) = Ht(Ã|ξ)−Ht(Ã|ξ, Yt,At)\n= Ht(Ã|ξ)− ˆ 1 y=0 Ht(Ã|ξ, Yt,At ∈ dy)\n= Ht(Ã|ξ)− ˆ 1− y=0 Ht(Ã|ξ, Yt,At ∈ dy)− ˆ 1 y=1− Ht(Ã|ξ, Yt,At ∈ dy)\n= Ht(Ã|ξ)− ˆ 1− y=0 Ht(Ã|ξ, Yt,At ∈ dy)\n= Ht(Ã|ξ)− (1− )H(Ã|ξ) = Ht(Ã|ξ) = Ht(τ).\nTogether with the previous lemma, our general regret bound implies Theorem 4.\nProof of Theorem 4. We have\nΓ ( Ã, {At : t ∈ N0} ) = (1− α2)E [ ∞∑ t=0 α2t Et[R̃−Rt,At ]2 It(Ã;Yt,At |ξ) ]\n= (1− α2)E [ τ−1∑ t=0 α2t (1− )2 4 H(τ) + ∞∑ t=τ α2t 02 0 ]\n= E [ 1− α2τ ] (1− )2 4 H(τ) = (1− ) 2(1− α2 + α2 − )\n4 H(τ)(1− α2 + α2)\n≤ 14 H(τ) .\nIt follows from Theorem 3 that\nE [ ∞∑ t=0 αt(R∗ −Rt,At) ] ≤ E[R ∗ − R̃] 1− α + √√√√Γ (Ã, {At : t ∈ N0}) I(θ; Ã|ξ) 1− α2\n≤ 1− α + √ (1− )2(1− α2 + α2 − ) 4 (1− α2 + α2)(1− α2)\n= 1− α + √ 1− 3 4 2 + (1− )(1− α2))\n≤ 1− α + 1 2 .\nNow we consider an upper bound that follows from choosing as a function of α. The minimizer of 1−α + 1 2 is ∗ = √\n(1− α)/2. If {At : t ∈ N0} is generated STS with parameter ∗, the bound on regret becomes\nE [ ∞∑ t=0 αt(R∗ −Rt,At) ] ≤ √ 2 1− α."
    }, {
      "heading" : "E Proof of Theorem 5: Information Ratio Analysis of the InfinitelyArmed Bandit with Noisy Observations",
      "text" : "The proof of Theorem 5 leverages the probability matching property of STS highlighted in Section 3. Recall that Ã = Aτ where τ = min{t|θAt ≥ }. Throughout this proof, let At ≡ {A1, A2, ...At−1} denote the set of previously sampled actions. Under STS, P(At = a|Ft) = P(Ã = a|Ft) for all a ∈ At, and P(At /∈ At|Ft) = P(Ã /∈ At|Ft). The algorithm essentially performs a kind of probability matching on Ã.\nTheorem 5. Suppose STS with tolerance parameter ∈ (0, 1) is applied to the infinite-armed bandit with noisy observations. Then, with probability 1, there exists t ∈ N0 with θAt ≥ 1− . If Ã = Aτ where τ = min{t : θAt ≥ 1− }, then\nH(Ã|ξ) ≤ 1 + log(1/δ) and Γ ( Ã, {At : t ∈ N0} ) ≤ 6 + 4/δ + (2/δ) log ( 1 1− α2 ) .\nTogether with Theorem 3 this implies\nE [ ∞∑ t=0 αt(R∗ −Rt,At) ] ≤ 1− α + √√√√(6 + 4/δ + (2/δ) log ( 11−α2)) (1 + log(1/δ)) 1− α2\n= Õ  1− α + √ 1/δ 1− α2  . We begin with a lemma establishing that with probability 1 STS will eventually sample an – optimal action. At an intuitive level, this result follows from the algorithm’s probability matching property, which guarantees that whenever its likely that no –optimal action has been sampled previously, the algorithm is likely to select a previously un-sampled action. With probability δ this new action is –optimal.\nLemma 7. If STS with tolerance parameter ∈ (0, 1) is applied to the infinite-armed bandit with noisy observations, then, with probability 1, there exists t ∈ N0 with θAt ≥ 1− .\nProof. Our goal is to show P(τ <∞) = 1 where\nτ = inf{t : θAt ≥ 1− }.\nBy the so-called continuity of measure,\nP(τ <∞) = lim t→∞ P(τ ≤ t) = 1− lim t→∞ P(τ ≥ t).\nNow set β ≡ lim\nt→∞ P(τ ≥ t)\nBecause P(τ ≥ t) is a decreasing bounded sequence, this limit exists, and β = inft∈N0 P(τ ≥ t). The proof shows β = 0.\nBy the probability matching property of STS P(At /∈ At|Ft) = P(τ ≥ t|Ft). Then, by the definition of τ and the independence among the components of (θ1, θ2, ...)\nP(τ = t|Ft) = P(Ã /∈ At ∧At /∈ At|Ft)δ = P(Ã /∈ At|Ft)2δ = P(τ ≥ t|Ft)2δ.\nTaking expectations implies\nP(τ = t) = E [ P(τ ≥ t|Ft)2 ] δ ≥ E [P(τ ≥ t|Ft)]2 δ = P(τ ≥ t)2δ.\nThen P(τ ≥ t)−P(τ ≥ t+ 1) = P(τ = t) ≥ P(τ ≥ t)2δ.\nSince P(τ ≥ t) converges,\n0 = lim t→∞ (P(τ ≥ t)−P(τ ≥ t+ 1)) ≥ lim t→∞\nP(τ ≥ t)2δ = β2δ.\nSince β ∈ [0, 1] by definition, this implies β = 0.\nThe remaining proof will follow from a sequence of lemmas. We now bound the entropy of Ã.\nLemma 8. H(Ã|ξ) ≤ 1 + log(1/δ)\nProof. Because the order in which new actions are sampled is completely determined given ξ,\nH(Ã|ξ) = H(N) where N ∼ Geom(δ) is a geometric random variable. This implies\nH(Ã|ξ) = H (N)\n= − ∞∑ k=1 δ(1− δ)k−1 log(δ(1− δ)k−1)\n= − ∞∑ k=1 δ(1− δ)k−1 log(δ)− ∞∑ k=1 δ(1− δ)k−1 log((1− δ)k−1)\n= ∞∑ k=1 P(N = k) log(1/δ)− log(1− δ) ∞∑ k=1 δ(1− δ)k−1(k − 1)\n= log(1/δ) + log ( 1\n1− δ\n) (E[N ]− 1)\n= log(1/δ) + log (\n1 + δ1− δ )(1− δ δ ) ≤ 1 + log(1/δ).\nThe bound on entropy yields a regret bound when combined with a bound the information ratio. The next lemma gives bounds the one-step information ratio.\nLemma 9. Et[θÃ − θAt ]2\nIt(Ã;Yt,At |ξ) ≤ 2|At|+ 2/δ\nwhere At = ∪t−1s=1{As} is the set of actions that were sampled before period t, and δ ≡ P(θi ≥ 1− ) is the prior probability an arm is –optimal.\nProof. Define L ≡ E[θi|θi ≥ 1− ]−E[θi]\nand δ ≡ P(θi ≥ 1− ).\nHere δ is the probability an unsampled arm is optimal, and L is the difference between the expected reward of an optimal arm and that of an arm sampled uniformly at random. In the case where θi ∼ Unif(0, 1), δ = and L = (1− )/2.\nWe can write expected regret as\nEt[θÃ − θAt ] = ∑ a∈A Pt(Ã = a)Et[θa|Ã = a]− ∑ a∈A Pt(At = a)Et[θa]\n= ∑ a∈At Pt(Ã = a) ( Et[θa|Ã = a]−Et[θa] ) + ∑ a/∈At Pt(Ã = a)Et[θa|Ã = a]− ∑ a/∈At Pt(At = a)Et[θa]\n= ∑ a∈At Pt(Ã = a) ( Et[θa|Ã = a]−Et[θa] ) + Pt(Ã /∈ At)(E[θa|θa ≥ 1− ]−E[θa])\n= ∑ a∈At Pt(Ã = a) ( Et[θa|Ã = a]−Et[θa] ) ︸ ︷︷ ︸\n∆t,1\n+ Pt(Ã /∈ At)L︸ ︷︷ ︸ ∆t,2\nThis decomposes regret into the sum of two terms: one which captures the regret due to suboptimal selection within the set of previously sampled actions At, and one due to the remaining possibility that none of the sampled actions are optimal. The proof develops a similar decomposition for mutual information, and then lower bounds both terms.\nWe can express mutual information as follows:\nIt(Ã;Yt,At |ξ) = ∑ a∈A Pt(At = a)It(Ã;Yt,a|At = a)\n= ∑ a∈A Pt(Ã = a)It(Ã;Yt,a|At = a)\n= ∑ a∈At Pt(Ã = a)It(Ã;Yt,a) + Pt(Ã /∈ At)It(Ã;Yt,aN |At = aN )\nwhere aN ∈ Act is an arbitrary action that has not yet been sampled. (N stands for “new”) Now, using the shorthand Pt(X) = Pt(X ∈ ·) to denote the posterior distribution of a random variable X, we have\nIt(Ã;Yt,aN |At = aN ) = ∑ a∈A Pt(Ã = a|At = aN )D ( Pt(Yt,aN |Ã = a,At = aN )||Pt(Yt,aN ) ) ≥ Pt(Ã = aN |At = aN )D ( Pt(Yt,aN |Ã = aN , At = aN )||Pt(Yt,aN )\n) = Pt(Ã = aN |At = aN )D (Pt(Yt,aN |θaN ≥ 1− )||Pt(Yt,aN )) ≥ 2Pt(Ã = aN |At = aN ) (Et[R(Yt,aN )|θaN ≥ 1− ]−Et[R(Yt,aN )]) 2 = 2Pt(Ã = aN |At = aN )L2 = 2Pt(Ã /∈ At)P(Ã = aN |At = aN , Ã /∈ At)L2 = 2Pt(Ã /∈ At)δL2.\nFollowing the analysis from [14] shows∑ a∈At Pt(Ã = a)It(Ã;Yt,a) = ∑ a∈At Pt(Ã = a) ∑ ã∈A D ( Pt(Yt,a||Ã = ã)||Pt(Yt,a) ) ≥\n∑ a∈At Pt(Ã = a)2D ( Pt(Yt,a||Ã = a)||Pt(Yt,a) ) ≥ 2\n∑ a∈At Pt(Ã = a)2 ( Et[θa|Ã = a]−Et[θa] )2\n≥ 2 |At| ∑ a∈At Pt(Ã = a) ( Et[θa|Ã = a]−Et[θa] )2 . Therefore\nIt(Ã;Yt,At |ξ) ≥ 2 |At| ∑ a∈At Pt(Ã = a) ( Et[θa|Ã = a]−Et[θa] )2 ︸ ︷︷ ︸\nGt,1\n+ 2Pt(Ã /∈ At)2δL2︸ ︷︷ ︸ Gt,2 ,\nis lower bounded by the sum of two terms: one which captures the information gain due to refining knowledge about previously sampled actions, and one that captures the expected information gathered about previously unexplored actions.\nTo bound the information ratio we’ll separately consider two cases. If ∆1 ≥ ∆2, then\nEt[θÃ − θAt ]2 It(Ã;Yt,At |ξ) ≤ (2∆t,1) 2 Gt,1 +Gt,2 ≤ 4(∆t,1) 2 Gt,1 = 2|At|.\nIf instead ∆1 < ∆2, then\nEt[θÃ − θAt ]2 It(Ã;Yt,At |ξ) ≤ (2∆t,2) 2 Gt,1 +Gt,2 ≤ 4(∆t,2) 2 Gt,2 = 2 δ .\nThis shows Et[θÃ − θAt ]2\nIt(Ã; θ;Yt,At |ξ) ≤ 2|At|+ 2/δ.\nWe’d now like to use the previous result to bound\nΓ ( Ã, {At : t ∈ N0} ) = (1− α2) ∞∑ t=0 α2tE [ Et[R̃−Rt,At ]2 It(Ã;Yt,At |ξ) ]\n≤ 2/δ + 2(1− α2) ∞∑ t=0 α2tE[|At|].\nWe begin by bounding E[|At|].\nLemma 10. |A0| = 0 and for each T ∈ N0, E[|AT |] ≤ 2 + log(T )/δ.\nProof. Let τk = min{t ≤ T ||At| ≥ k} denote the first period before T in which k actions have been sampled. Then\nE[|AT |] = E[|Aτk |] + E[|AT | − |Aτk |] ≤ E[|Aτk |] + E[|Aτk+T | − |Aτk |]\n≤ k + E τk+T−1∑ t=τk 1(At /∈ At)\n= k + E T−1∑ s=0 P(Aτk+s /∈ Aτk+s|Hτk+s)\n= k + E T−1∑ s=0 P(Ã /∈ Aτk+s|Hτk+s)\n= k + T−1∑ s=0 P(Ã /∈ Aτk+s)\n≤ k + TP(Ã /∈ Aτk) = k + TP(Geom(δ) > k) = k + T (1− δ)k\n≤ k + Te−δk.\nChoosing k = dlog(T )/δe ≤ 1 + log(T )/δ, implies\nE[|AT |] ≤ 2 + log(T )/δ.\nThe next technical lemma shows ∑∞ t=1 γ\n−t log(t) = O((1/γ) log(1/γ)). Lemma 11. For any γ ∈ (0, 1),\n∞∑ t=1 γ−t log(t) ≤ 11− γ [ 1 + log ( 1 1− γ )] .\nProof. ∞∑ t=1 γ−t log(t) ≤ ∞∑ t=1 e−(1−γ)t log(t)\n= ∞∑ t=2 e−(1−γ)t log(t)\n∗ ≤ ˆ ∞\n1 e−(1−γ)x log(x+ 1)dx\n= 11− γ ˆ ∞ 1 e−u log ( u 1− γ + 1 ) du ≤ 11− γ ([ 1 + log ( 1 1− γ )]ˆ ∞ 1 e−udu+ ˆ ∞ 1 e−u log(u)du\n) = 11− γ ([ 1 + log ( 1 1− γ )] (1/e) + ˆ ∞ 1 e−u log(u)du ) ≤ 11− γ [ 1 + log ( 1 1− γ\n)] where the last step uses a numerical approximation to the indefinite integralˆ ∞\n1 e−u log(u)du ≈ .22\nalong with the fact that 1/e+ .22 ≈ .57 < 1. The inequality (*) uses that for any t ≥ 2\ne−(1−γ)t log(t) ≤ tˆ\nt−1\ne−(1−γ)x log(x+ 1)\nsince e−(1−γ)x is decreasing in x and log(x) is increasing in x.\nFinally we can conclude with the proof of Theorem 5. We have\nΓ ( Ã, {At : t ∈ N0} ) = (1− α2) ∞∑ t=0 α2tE [ Et[R̃−Rt,At ]2 It(Ã;Yt,At |ξ) ]\n≤ 2/δ + 2(1− α2) ∞∑ t=0 α2tE[|At|]\nSince\n(1− α2) ∞∑ t=0 α2tE[|At|] ≤ (1− α2) ∞∑ t=1 α2t (2 + log(t)/δ)\n≤ 3 + (1/δ)(1− α2) ∞∑ t=1 α2t log(t)\n≤ 3 + (1/δ) [ 1 + log ( 1 1− α2 )] ,\nthis implies\nΓ ( Ã, {At : t ∈ N0} ) ≤ 6 + 4/δ + (2/δ) log ( 1 1− α2 ) = O ( (1/δ) log ( 1 1− α2 )) and concludes the proof of Theorem 5."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "<lb>The literature on bandit learning and regret analysis has focused on contexts where the goal<lb>is to converge on an optimal action in a manner that limits exploration costs. One shortcoming<lb>imposed by this orientation is that it does not treat time preference in a coherent manner.<lb>Time preference plays an important role when the optimal action is costly to learn relative to<lb>near-optimal actions. This limitation has not only restricted the relevance of theoretical results<lb>but has also influenced the design of algorithms. Indeed, popular approaches such as Thompson<lb>sampling and UCB can fare poorly in such situations. In this paper, we consider discounted<lb>rather than cumulative regret, where a discount factor encodes time preference. We propose<lb>satisficing Thompson sampling – a variation of Thompson sampling – and establish a strong<lb>discounted regret bound for this new algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}