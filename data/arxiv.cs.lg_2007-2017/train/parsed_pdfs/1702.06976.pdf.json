{
  "name" : "1702.06976.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Heavy-Tailed Analogues of the Covariance Matrix for ICA",
    "authors" : [ "Joseph Anderson", "Navin Goyal", "Anupama Nandi", "Luis Rademacher" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "(1) A practical algorithm for heavy-tailed ICA that we call HTICA. We provide theoretical guarantees and show that it outperforms other algorithms in some heavy-tailed regimes, both on real and synthetic data. Like the current state-of-the-art, the new algorithm is based on the centroid body (a first moment analogue of the covariance matrix). Unlike the state-of-the-art, our algorithm is practically efficient. To achieve this, we use explicit analytic representations of the centroid body, which bypasses the use of the ellipsoid method and random walks.\n(2) We study how heavy tails affect different ICA algorithms, including HTICA. Somewhat surprisingly, we show that some algorithms that use the covariance matrix or higher moments can successfully solve a range of ICA instances with infinite second moment. We study this theoretically and experimentally, with both synthetic and real-world heavy-tailed data."
    }, {
      "heading" : "1 Introduction",
      "text" : "Independent component analysis (ICA) is a computational and statistical technique with applications in areas ranging from signal processing to machine learning and more. Formally, if S is an n-dimensional random vector with independent coordinates and A ∈ Rn×n is invertible, then the ICA problem is to estimate A given access to i.i.d. samples of the mixed signals X = AS. We say that X is generated by an ICA model X = AS. The recovery of A (the mixing matrix ) is possible only up to scaling and permutation of the columns. Moreover, for the recovery to be possible, the distributions of the random variables Si must not be Gaussian (except possibly one of them). Since its inception in the eighties (see [CJ10] for historical remarks), ICA has been thoroughly studied and a vast literature exists (e.g. [HKO01, CJ10]). The theory is well-developed and practical algorithms—e.g., FastICA [Hyv99], JADE [CS93]—are now available along with implementations, e.g. [CAS+]. However, to our knowledge, rigorous complexity analyses of these assume that the fourth moment of each component is finite: E(S4i ) <∞. If at least one of the independent components does not satisfy this assumption we will say that the input is in the heavy-tailed regime. Many ICA algorithms first preprocess the data to convert the given ICA model into another one where the mixing matrix A has orthogonal columns; this step is often called whitening. We will instead call it orthogonalization, as this describes more precisely the desired outcome. Traditional whitening is a second order method that may not make sense in the heavy-tailed regime. In this regime, it is not clear how the existing algorithms would\n∗The Ohio State University, Department of Computer Science and Engineering. andejose@cse.ohio-state.edu †Microsoft Research, India. navingo@microsoft.com ‡The Ohio State University, Department of Computer Science and Engineering. nandi.10@osu.edu §University of California, Davis, Mathematics Department. lrademac@ucdavis.edu\nar X\niv :1\n70 2.\n06 97\n6v 1\n[ cs\n.L G\n] 2\n2 Fe\nb 20\n17\nperform, because they depend on empirical estimation of various statistics of the data such as the covariance matrix or the fourth cumulant tensor, which diverge in general for heavy-tailed data. For example, for the covariance matrix in the mean-0 case this is done by taking the empirical average (1/N) ∑N i=1 x(i)x(i) T where the {x(i)} are i.i.d. samples of X. ICA in the heavy-tailed regime is of considerable interest, directly (e.g., [Kid01b, Kid01a, SYM01, CB04, CB05, SAML+05, WKZ09, JEK01, CS07, BC99]) and indirectly (e.g., [BG10, GTG09, WOH02]) and has applications in speech and finance. We also mention an informal connection with robust statistics: Algorithms solving heavy-tailed ICA might work by focusing on samples in a small (but high probability) region to get reliable statistics about the data and avoid the instability of the tail. Thus, if the data has outliers, the outliers are less likely to affect such an algorithm.\nRecent theoretical work [AGNR15] proposed a polynomial time algorithm for ICA that works in the regime where each component Si has finite (1 + γ)-moment for γ > 0. This algorithm follows the two phases of several ICA algorithms: (i) Orthogonalize the independent components. The purpose of this step is to apply an affine transformation to the samples from X so that the resulting samples correspond to an ICA model where the unknown matrix A has orthogonal columns. (ii) Learn the matrix with orthogonal columns. Each of these two phases required new techniques: (1) Orthogonalization via uniform distribution in the centroid body. The input is assumed to be samples from an ICA model X = AS where each Si is symmetrically distributed (w.l.o.g, see Sec. 2) and has at least (1 + γ)-moments. The goal is to construct an orthogonalization matrix B so that BA has orthogonal columns. In [AGNR15], the inverse of the square root of the covariance matrix of the uniform distribution in the centroid body is one such matrix. (2) Gaussian damping. The previous step allows one to assume that the mixing matrix A is orthogonal. The modified second step is: If X has density ρX(t) for t ∈ Rn, then the algorithm constructs another ICA model XR = ASR where XR has pdf proportional to ρX(t) exp(−‖t‖22/R2), where R > 0 is a parameter chosen by the algorithm. This explains the term Gaussian damping. This achieves two goals: (1) All moments of XR and SR are finite. (2) The product structure of is retained. This follows from two facts: A has orthogonal columns, and the Gaussian has independent components in any orthonormal basis. Because of these properties, the model can be solved by traditional ICA algorithms.\nThe algorithm in [AGNR15] is theoretically efficient but impractical. Their orthogonalization uses the ellipsoid algorithm for linear programming, which is not practical. It is not clear how to replace their use of the ellipsoid algorithm by practical linear programming tools, as their algorithm only has oracle access to a sort of dual and not an explicit linear program. Moreover, their orthogonalization technique uses samples uniformly distributed in the centroid body, generated by a random walk. This is computationally efficient in theory but, to the best of our knowledge, only efficient in practice for moderately low dimension.\nOur contributions. Our contributions are experimental and theoretical. We provide a new and practical ICA algorithm, HTICA, building upon the previous theoretical work in [AGNR15]. HTICA works as follows: (1) Compute an orthogonalization matrix B. (2) Pre-multiply samples by B to get an orthogonal model. (3) Damp the data, run an existing ICA algorithm. For step (1), we propose two theoretically sound and practically efficient ways below, orthogonalization via centroid body scaling and orthogonalization via covariance. Our algorithm is simpler and more efficient, but needs a more technical justification than the method in [AGNR15]. We demonstrate the effectiveness of HTICA on both synthetic and real-world data.\nOrthogonalization via centroid body scaling. We propose a more practical orthogonalization matrix than the one from [AGNR15] (orthogonalization via the uniform distribution in the centroid body, mentioned before). First, consider the centroid body of random vector X, denoted ΓX (this is really a function of the distribution of X; formal definition in Sec. 2). For intuition, it is helpful to think of the centroid body as an ellipsoid whose axes are aligned with the independent components of X. The centroid body is in general not an ellipsoid, but it has certain symmetries aligned with the independent components. Let random vector Y be a scaling of X along every ray so that points at infinity are mapped to the boundary of ΓX, the origin is mapped to itself and the scaling interpolates smoothly. One such scaling is obtained in the following way: It is helpful to consider how far a point is in its ray with respect to the boundary of ΓX. This is given by the Minkoswki functional of ΓX, denoted p : Rn → R, which maps the boundary of ΓX to 1 and interpolates linearly along every ray. We can then achieve the desired scaling by first mapping a given point to the boundary point on its ray (the mapping x 7→ x/p(x)) and then using the function tanh, which maps [0,∞) to\n[0, 1] with tanh(0) = 0 and limx→∞ tanh(x) = 1 to determine the final scale along the ray, namely, tanh p(x). More formally, our scaling is the following: Let Y be tanh p(X)p(X) X. We show in Sec. 4.1 that B = Cov(Y ) −1/2 is an orthogonalization matrix when Cov(Y ) is invertible. In order to make this practical, one needs a practical estimator of the Minkowski functional of ΓX from a sample of X. In Sec. 4.1 and 5, we describe such an algorithm and provide a theoretical justification, including finite sample estimates. The proposed algorithm is much simpler and practical than the one described in [AGNR15]. In particular, it avoids the use of the ellipsoid algorithm by the use of a closed-form linear programming representation of the centroid body (Prop. 10, Lemma 11) and new approximation guarantees between the empirical (sample estimate) and true centroid body of a heavy-tailed distribution. In Sec. 4.1, we discuss our practical implementation and show results where orthogonalization via centroid body scaling produces results with smaller error.\nOrthogonalization via covariance. Previously, (e.g., in [CB04]), the empirical covariance matrix was used for whitening in the heavy-tailed regime and, surprisingly, worked well in some situations. Unfortunately, the understanding of this was quite limited . We give a theoretical explanation for this phenomenon in a fairly general heavy-tailed regime: Covariance-based orthogonalization works well when each component Si has finite (1 + γ)-moment, where γ > 0. We also study this algorithm in experimental settings. As we will see, while orthogonalization via covariance improves over previous algorithms, in general orthogonalization via centroid body has better performance because it has better numerical stability; but there are some situations where orthogonalization via covariance matrix is better.\nEmpirical Study. We perform experiments on both synthetic and real data to see the effect of heavy-tails on ICA.\nIn the synthetic data setting, we generate samples from a fixed heavy-tailed distribution and study how well the algorithm can recover a random mixing matrix (Sec. 3).\nTo study the algorithm with real data, we use recordings of human speech provided by [Don09]. This involves a room with different arrangements of microphones, and six humans speaking independently. The speakers are recorded individually, so we can artificially mix them and have access to a ground truth. We study the statistical properties of the data, observing that it does indeed behave as if the underlying processes are heavy-tailed. The performance of our algorithm shows improvement over using FastICA on its own."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Heavy-tailed distributions arise in a wide range of applications (e.g., [Nol15]). They are characterized by the slow decay of their tails. Examples of heavy-tailed distributions include the Pareto and log-normal distributions.\nWe denote the pdf of random variable Z by ρZ . We will assume that our distributions are symmetric, that is ρ(x) = ρ(−x) for x ∈ R. As observed in [AGNR15], this is without loss of generality for our purposes. This follows from the fact that if X = AS is an ICA model, and if we let X ′ = AS′ be an i.i.d. copy of the same model, then X −X ′ = A(S − S′) is an ICA model with components of S − S′ having symmetric pdfs. One further needs to check that if the components of S are away from Gaussians then the same holds for S − S′; see [AGNR15]. We formulate our algorithms for the symmetric case; the general case immediately reduces to the symmetric case.\nFor K ⊆ Rn, K denotes the set of points that are at distance at most from K. The set K− is all points for which an -ball around them is still contained in K. The n-dimensional `p ball is denoted as B n p .\nAn important related family of distributions is that of stable distributions (e.g., [Nol15]). In general, the density of a stable distribution has no closed form, but is fully defined by four real-valued parameters. Some stable distributions do admit a closed form, such as the Cauchy and Gaussian distributions. For us the most important parameter is α ∈ (0, 2], known as the stability parameter; we will think of the other three parameters as being fixed to constants.\nWe use the notation poly(·) to indicate a function which is asymptotically upper bounded by a polynomial expression of the given variables.\nIf α = 2, the distribution is Gaussian (the only non-heavy-tailed stable distribution), and if α = 1, it is the Cauchy distribution.\nDefinition 1 (Centroid body). Let X ∈ Rn be a random vector with finite first moment, that is, for all u ∈ Rn we have E(|〈u,X〉|) < ∞. The centroid body of X is the compact convex set, denoted ΓX, whose support function is hΓX(u) = E(|〈u,X〉|). For a probability measure P, we define ΓP, the centroid body of P, as the centroid body of any random vector distributed according to P.\nNote that for the centroid body to be well-defined, the mean of the data must be finite. This excludes, for instance, the Cauchy distribution from consideration in the present work."
    }, {
      "heading" : "3 HTICA and experiments",
      "text" : "In this section, we show experimentally that heavy-tailed data poses a significant challenge for current ICA algorithms, and compare them with HTICA in different settings. We observe some clear situations where heavy-tails seriously affect the standard ICA algorithms, and that these problems are frequently avoided by using the heavy-tailed ICA framework. In some cases, HTICA does not help much, but maintains the same performance of plain FastICA.\nTo generate the synthetic data, we create a simple heavy-tailed density function fη(x) proportional to (|x|+ 1.5)−η, which is symmetric, and for η > 1, fη is the density of a distribution which has finite k < η − 1 moment. The signal S is generated with each Si independently distributed from fηi . The mixing matrix A ∈ Rn×n is generated with each coordinate i.i.d. N (0, 1), columns normalized to unit length. To compare the quality of recovery, the columns of the estimated mixing matrix, Ã are permuted to align with the closest matching column of A, via the Hungarian algorithm. We use the Frobenius norm to measure the error, but all experiments were also performed using the well-known Amari index [ACY+96]; the results have similar behavior and are not presented here."
    }, {
      "heading" : "3.1 Heavy-tailed ICA when A is orthogonal: Gaussian damping and experiments",
      "text" : "Focusing on the third step above, where the mixing matrix already has orthogonal columns, ICA algorithms already suffer dramatically from the presence of heavy-tailed data. As proposed in [AGNR15], Gaussian damping is a preprocessing technique that converts data from an ICA model X = AS, where A is unitary (columns are orthogonal with unit l2-norm) to data from a related ICA model XR = ASR, where R > 0 is a parameter to be chosen. The independent components of SR have finite moments of all orders and so the existing algorithms can estimate A.\nUsing samples ofX, we construct the damped random variableXR, with pdf ρXR(x) ∝ ρX(x) exp(−‖x‖ 2 /R2).\nTo normalize the right hand side, we can estimate\nKXR = E exp(−‖X‖ 2 /R2)\nso that ρXR(x) = ρX(x) exp(−‖x‖ 2 /R2)/KXR . If x is a realization of XR, then s = A −1x is a realization of the random variable SR and we have that SR has pdf ρSR(s) = ρXR(x). To generate samples from this distribution, we use rejection sampling on samples from ρX . When performing the damping, we binary search over R so that about 25% of the samples are rejected. For more details about the technical requirements for choosing R, see [AGNR15].\nFigure 1 shows that, when A is already a perfectly orthogonal matrix, but where S may have heavy-tailed coordinates, several standard ICA algorithms perform better after damping the data. In fact, without damping, some do not appear to converge to a correct solution. We compare ICA with and without damping in this case: (1) FastICA using the fourth cumulant (“FastICA - pow3”), (2) FastICA using log cosh (“FastICA - tanh”), (3) JADE, and (4) Second Order Joint Diagonalization as in, e.g., [Car89] ."
    }, {
      "heading" : "3.2 Experiments on synthetic heavy-tailed data",
      "text" : "We now present the results of HTICA using different orthogonalization techniques: (1) Orthogonalization via covariance (Section 4.2 (2) Orthogonalization via the centroid body (Section 4.1) (3) the ground truth, directly inverting the mixing matrix (oracle), and (4) No orthogonalization, and also no damping (for comparison with plain FastICA) (identity).\nThe “mixed” regime in the left and middle of Figure 2 (where some signals are not heavy-tailed) demonstrates a very dramatic contrast between different orthogonalization methods, even when only two heavy-tailed signals are present.\nIn the experiment with different methods of orthogonalization it was observed that when all exponents are the same or very close, orthogonalization via covariance performs better than orthogonalization via centroid and the true mixing matrix as seen in Figure 2. A partial explanation is that, given the results in Figure 1, we know that equal exponents favor FastICA without damping and orthogonalization (identity in Figure 2). The line showing the performance with no orthogonalization and no damping (“identity”) behaves somewhat erratically, most likely due the presence of the heavy-tailed samples. Additionally, damping and the choice of parameter R is sensitive to scaling. A scaled-up distribution will be somewhat hurt because fewer samples\nwill survive damping."
    }, {
      "heading" : "3.3 ICA on speech data",
      "text" : "While the above study on synthetic data provides interesting situations where heavy-tails can cause problems for ICA, we provide some results here which use real-world data, specifically human speech. To study the performance of HTICA on voice data, we first examine whether the data is heavy-tailed. The motivation to use speech data comes from observations by the signal processing community (e.g. [Kid00]) that speech data can be modeled by α-stable distributions. For an α-stable distribution, with α ∈ (0, 2), only the moments of order less than α will be finite. We present here some results on a data set of human speech according to the standard cocktail party model, from [Don09].\nThe physical setup of the experiments (the human speakers and microphones) is shown in Figure 3. To estimate whether the data is heavy-tailed, as in [Kid00], we estimate parameter α of a best-fit α-stable distribution. This estimate is in Figure 4 for one of the data sets collected. We can see that the estimated α is clearly in the heavy-tailed regime for some signals.\nUsing data from [Don09], we perform the same experiment as in Section 3.2: generate a random mixing matrix with unit length columns, mix the data, and try to recover the mixing matrix. Although the mixing\nis synthetic, the setting makes the resulting mixed signals same as real. Specifically, the experiment was conducted in a room with chairs, carpet, plasterboard walls, and windows on one side. There was natural noise including vents, computers, florescent lights, and traffic noise through the windows.\nFigure 4 demonstrates that HTICA (orthogonalizing with centroid body scaling, Section 4.1) applied to speech data yields some noticeable improvement in the recovery of the mixing matrix, primarily in that it is less susceptible to data that causes FastICA to have large error “spikes.” Moreover, in many cases, running only FastICA on the mixed data failed to even recover all of the speech signals, while HTICA succeeded. In these cases, we had to re-start FastICA until it recovered all the signals."
    }, {
      "heading" : "4 New approach to orthogonalization and a new analysis of em-",
      "text" : "pirical covariance\nAs noted above, the technique in [AGNR15], while being provably efficient and correct, suffers from practical implementation issues. Here we discuss two alternatives: orthogonalization by centroid body scaling and orthogonalization by using the empirical covariance. The former, orthogonalization via centroid body scaling, uses the samples already present in the algorithm rather than relying on a random walk to draw samples which are approximately uniform in the algorithm’s approximation of the centroid body (as is done in [AGNR15]). This removes the dependence on random walks and the ellipsoid algorithm; instead, we use samples that are distributed according to the original heavy-tailed distribution but non-linearly scaled to lie inside the centroid body. We prove in Lemma 3 that the covariance of this subset of samples is enough to orthogonalize the mixing matrix A. Secondly, we prove that one can, in fact, “forget” that the data is heavy tailed and orthogonalize by using the empirical covariance of the data, even though it diverges, and that this is enough to orthogonalize the mixing matrix A. However, as observed in experimental results, in general this has a downside compared to orthogonalization via centroid body in that it could cause numerical instability during the “second” phase of ICA as the data obtained is less well-conditioned. This is illustrated directly in the table in Figure 4 containing the singular value and condition number of the mixing matrix BA in the approximately orthogonal ICA model."
    }, {
      "heading" : "4.1 Orthogonalization via centroid body scaling",
      "text" : "In [AGNR15], another orthogonalization procedure, namely orthogonalization via the uniform distribution in the centroid body is theoretically proven to work. Their procedure does not suffer from the numerical instabilities and composes well with the second phase of ICA algorithms. An impractical aspect of that procedure is that it needs samples from the uniform distribution in the centroid body.\nWe described orthogonalization via centroid body in Section 1, except for the estimation of p(x), the Minkowski functional of the centroid body. The complete procedure is stated in Subroutine 1.\nWe now explain how to estimate the Minkowski functional. The Minkowski functional was informally described in Section 1. The Minkowski functional of ΓX is formally defined by p(x) := inf{t > 0 : x ∈ tΓX}. Our estimation of p(x) is based on an explicit linear program (LP) (10) that gives the Minkowski functional of the centroid body of a finite sample of X exactly and then arguing that a sample estimate is close to the actual value for ΓX. For clarity of exposition, we only analyze formally a special case of LP (10) that decides membership in the centroid body of a finite sample of X (LP (9)) and approximate membership in ΓX. This analysis is in Section 5. Accuracy guarantees for the approximation of the Minkowski functional follow from this analysis.\nLemma 2 ([AGNR15]). Let U be a family of n-dimensional product distributions. Let Ū be the closure of U under invertible linear transformations. Let Q(P) be an n-dimensional distribution defined as a function of P ∈ Ū . Assume that U and Q satisfy:\n1. For all P ∈ U , Q(P) is absolutely symmetric.\n2. Q is linear equivariant (that is, for any invertible linear transformation T we have Q(TP) = TQ(P)).\nSubroutine 1 Orthogonalization via centroid body scaling\nInput: Samples (X(i))Ni=1 of ICA model X = AS so each Si is symmetric with (1 + γ) moments. Output: Matrix B approximate orthogonalizer of A 1: for i = 1 : N do, 2: Let λ∗ be the optimal value of (10) with q = X(i). Let di = 1/λ\n∗. Let Y (i) = tanh didi X (i).\n3: end for 4: Let C = 1N ∑N i=1 Y (i)Y (i) T . Output B = C−1/2.\n3. For any P ∈ Ū , Cov(Q(P)) is positive definite.\nThen for any symmetric ICA model X = AS with PS ∈ U we have Cov(Q(PX))−1/2 is an orthogonalizer of X.\nLemma 3. Let X be a random vector drawn from an ICA model X = AS such that for all i we have E|Si| = 1 and Si is symmetrically distributed. Let Y = tanh p(X)p(X) X where p(X) is the Minkoswki functional of ΓX. Then Cov(Y )−1/2 is an orthogonalizer of X.\nProof. We will be applying Lemma 2. Let U denote the set of absolutely symmetric product distributions PW over Rn such that E|Wi| = 1 for all i. For PV ∈ Ū , let Q(PV ) be equal to the distribution obtained by scaling V as described earlier, that is, distribution of αV , where α = tanh p(V )p(V ) , p(V ) is the Minkoswki functional of ΓPV . For all PW ∈ U , Wi is symmetric and E|Wi| = 1 which implies that αW , that is, Q(PW ) is absolutely symmetric. Let PV ∈ Ū . Then Q(PV ) is equal to the distribution of αV . For any invertible linear transformation T and measurable set M, we have Q(TPV )(M) = Q(PTV )(M) = PαTV (M) = PαV (T−1M) = TQ(PV )(M). Thus Q is linear equivariant. Let P ∈ Ū . Then there exist A and PW ∈ U such that P = APW . We get Cov(Q(P)) = Cov(AQ(PW )). Let Wα = αW . Thus, Cov(AQ(PW )) = AE(WαWTα )AT where E(WαWTα ) is a diagonal matrix with elements E(α2W 2i ) which are non-zero because we assume E|Wi| = 1. This implies that Cov(Q(P)) is positive definite and thus by Lemma 2, Cov(Y )−1/2 is an orthogonalizer of X."
    }, {
      "heading" : "4.2 Orthogonalization via covariance",
      "text" : "Here we show the somewhat surprising fact that orthogonalization of heavy-tailed signals is sometimes possible by using the “standard” approach: inverting the empirical covariance matrix. The advantage here, is that it is computationally very simple, specifically that having heavy-tailed data incurs very little computational penalty on the process of orthogonalization alone. It’s standard to use covariance matrix for whitening when the second moments of all independent components exist [HKO01]: Given samples from the ICA model X = AS, we compute the empirical covariance matrix Σ̃ which tends to the true covariance matrix as we take more samples and set B = Σ̃−1/2. Then one can show that BA is a rotation matrix, and thus by pre-multiplying the data by B we obtain an ICA model Y = BX = (BA)S, where the mixing matrix BA is a rotation matrix, and this model is then amenable to various algorithms. In the heavy-tailed regime where the second moment does not exist for some of the components, there is no true covariance matrix and the empirical covariance diverges as we take more samples. However, for any fixed number of samples one can still compute the empirical covariance matrix. In previous work (e.g., [CB04]), the empirical covariance matrix was used for whitening in the heavy-tailed regime with good empirical performance; [CB04] also provided some theoretical analysis to explain this surprising performance. However, their work (both experimental and theoretical) was limited to some very special cases (e.g., only one of the components is heavy-tailed, or there are only two components both with stable distributions without finite second moment).\nWe will show that the above procedure (namely pre-multiplying the data by B := Σ̃−1/2) “works” under considerably more general conditions, namely if (1 + γ)-moment exists for γ > 0 for each independent component Si. By “works” we mean that instead of whitening the data (that is BA is rotation matrix) it does something slightly weaker but still just as good for the purpose of applying ICA algorithms in the next\nphase. It orthogonalizes the data, that is now BA is close to a matrix whose columns are orthogonal. In other words, (BA)T (BA) is close to a diagonal matrix (in a sense made precise in Theorem 5).\nLet X be a real-valued symmetric random variable such that E(|X|1+γ) ≤ M for some M > 1 and 0 < γ < 1. The following lemma from [AGNR15] says that the empirical average of the absolute value of X converges to the expectation of |X|. The proof, which we omit, follows an argument similar to the proof of the Chebyshev’s inequality. Let ẼN [|X|] be the empirical average obtained from N independent samples X(1), . . . , X(N), i.e., (|X(1)|+ · · ·+ |X(N)|)/N . Lemma 4. Let ∈ (0, 1). With the notation above, for N ≥ ( 8M ) 1 2 + 1 γ , we have Pr[|ẼN [|X|]−E[|X|]| > ] ≤\n8M 2Nγ/3 .\nTheorem 5 (Orthogonalization via covariance matrix). Let X be given by ICA model X = AS. Assume that there exist t, p,M > 0 and γ ∈ (0, 1) such that for all i we have\n(a) E(|Si|1+γ) ≤M <∞, (b) (normalization) E|Si| = 1, and (c) Pr(|Si| ≥ t) ≥ p. Let x(1), . . . , x(N) be i.i.d. samples according to X. Let Σ̃ = (1/N) ∑N k=1 x (k)x(k) T and B = Σ̃−1/2. Then for any , δ ∈ (0, 1), ‖(BA)TBA−D‖2 ≤ for a diagonal matrix D with diagonal entries d1, . . . , dn satisfying 0 < di, 1/di ≤ max{2/pt2, N4} for all i with probability 1 − δ when N ≥ poly(n,M, 1/p, 1/t, 1/ , 1/δ).\nProof idea. For i 6= j we have E(SiSj) = 0 (due to our symmetry assumption on S) and E(|SiSj |) = E(|Si|)E(|Sj |) <∞. We have (BA)TBA = L−1, where L = (1/N) ∑N k=1 s (k)s(k) T\n. The off-diagonal entries of L converge to 0: We have Li,j = ESiSj = (ESi)(ESj). Now by our assumption that (1 + γ)-moments exist, Lemma 4 is applicable and implies that empirical average ẼSi tends to the true average ESi as we increase the number of samples. The true average is 0 because of our assumption of symmetry (alternatively, we could just assume that the Xi and hence Si have been centered). The diagonal entries of L are bounded away from 0: This is clear when the second moment is finite, and follows easily by hypothesis (c) when it is not. Finally, one shows that if in L the diagonal entries highly dominate the off-diagonal entries, then the same is true of L−1. Proof. We have (BA)TBA = L−1, where L = (1/N) ∑N k=1 s (k)s(k) T\n. By assumption, ELij = 0 for i 6= j. Note that E|sisj |1+γ ≤M2 and so by Lemma 4, for i 6= j,\nP (|Lij | > 1) ≤ 8M2\n21N γ/3\nwhen N ≥ ( 8M 2\n1 )\n1 2 + 1 γ .\nNow let D := diag(L−111 , L −1 22 , . . . , L −1 nn). Then when |Lij | < 1 for all i 6= j, we have ‖L−D−1‖2 ≤\n‖L−D−1‖F ≤ n 1. The union bound then implies\nP (‖L−D−1‖2 < n 1) ≥ P (‖L−D −1‖F < n 1)\n≥ P (∀i 6= j, |Lij | ≤ 1) ≥ 1− 8n 2M2\n21N γ/3\n(1)\nwhen N ≥ ( 8M 2\n1 )\n1 2 + 1 γ .\nNext, we aim to bound ‖D‖2 which can be done by writing\n‖D‖2 = 1\nσmin(D−1) =\n1\nmini∈[n] Lii (2)\nwhere Lii = (1/N) ∑N k=1 s (k) i 2 . Consider the random variable 1(s2i ≥ t2). We can calculate E ∑ j 1(s (j) i 2 ≥ t2) ≥ Np and use a Chernoff bound to see\nP  ∑ k∈[N ] 1(s (k) i 2 ≥ t2) ≤ Np 2  ≤ exp(− Np 8 ) (3)\nand when ∑ k∈[N ] 1(s (k) i 2 ≥ t2) ≥ Np2 , we have Lii ≥ t\n2p/2. Then with probability at least 1−n exp(−Np/8), all entries of D−1 are at least t2p/2. Using this, if N ≥ N1 := (8/p) ln(3n/δ) then ‖D‖2 ≤ 2/pt2 with probability at least 1− δ/3.\nSimilarly, suppose that ‖D‖2 ≤ 2/pt2 and choose 1 = min{ t4p2 4n · 2 , 1 pt2 } and\nN2 := max\n{( 24n2M2\n21δ\n)3/γ , ( 8M2\n1\n) 1 2 + 1 γ }\nso that when N ≥ N2, we have ‖L−D−1‖2 ≤ 1/(2‖D‖2) and ‖L−D−1‖2 ≤ t4p2 /8 with probability at least 1− δ/3. Invoking (7), when N ≥ max{N1, N2}, we have\n‖L−1 −D‖2 ≤ 2‖D‖2‖L−D −1‖2 ≤ 2\n4 p2t4 t4p2 8 = (4)\nwith probability at least 1− 2δ/3. Finally, we upper bound 1/di for a fixed i by using Markov’s inequality:\nP\n( 1\ndi > N5\n) = P (Lii > N 4) = P ( N∑ j s (j) i 2 > N5 ) ≤ NP (S2i > N4) ≤ NP (|Si| > N2)\n≤ N E|Si| N2 = 1 N\n(5)\nso that 1/di ≤ N4 for all i with probability at least 1 − δ/3 when N ≥ N3 := n/3δ. Therefore, when N ≥ max{N1, N2, N3}, we have ‖L−1 −D‖2 ≤ , di ≤ 2/pt2, and 1/di ≤ N4 for all i with overall probability at least 1− δ.\nWe used the following technical result.\nLemma 6. Let ‖·‖ be a matrix norm such that ‖AB‖ ≤ ‖A‖‖B‖. Let matrices C,E ∈ Rn×n be such that ‖C−1E‖2 ≤ 1, and let C̃ = C + E. Then\n‖C̃−1 − C−1‖ ‖C−1‖ ≤ ‖C −1E‖ 1− ‖C−1E‖ . (6)\nThis implies that if ‖E‖2 = ‖C̃ − C‖2 ≤ 1/(2‖C−1‖2), then\n‖C̃−1 − C−1‖2 ≤ 2‖C −1‖22‖E‖2. (7)\nIn Theorem 5, the diagonal entries are lower bounded, which avoids some degeneracy, but they could still grow quite large because of the heavy tails. This is a real drawback of orthogonalization via covariance. HTICA, using the more sophisticated orthogonalization via centroid body scaling does not have this problem. We can see this in the right table of Figure 4, where the condition number of “centroid” is much smaller than the condition number of “covariance.”"
    }, {
      "heading" : "5 Membership oracle for the centroid body, without polarity",
      "text" : "We will see now how to implement an -weak membership oracle for ΓX directly, without using polarity. We start with an informal description of the algorithm and its correctness.\nThe algorithm implementing the oracle (Subroutine 2) is the following: Let q ∈ Rn be a query point. Let X1, . . . , XN be a sample of random vector X. Given the sample, let Y be uniformly distributed in {X1, . . . , XN}. Output YES if q ∈ ΓY , else output NO.\nIdea of the correctness of the algorithm: If q is not in (ΓX) , then there is a hyperplane separating q from (ΓX) . Let {x : aTx = b} be the hyperplane, satisfying ‖a‖ = 1, aT q > b and aTx ≤ b for every x ∈ (ΓX) . Thus, we have h(ΓX) (a) ≤ b and hΓX(a) ≤ b− . We have\nhΓY (a) = E(|aTY |) = (1/N) N∑ i=1 |aTXi|.\nBy Lemma 4, (1/N) ∑N i=1|aTXi| is within of E|aTX| = hΓX(a) ≤ b − when N is large enough with probability at least 1− δ over the sample X1, . . . , XN . In particular, hΓY (a) ≤ b, which implies q /∈ ΓY and the algorithm outputs NO, with probability at least 1− δ.\nIf q is in (ΓX)− , let y = q + q̂ ∈ ΓX. We will prove the following claim: Informal claim (Lemma 13): For p ∈ ΓX, for large enough N and with probability at least 1− δ there is z ∈ ΓY so that ‖z − p‖ ≤ /10. This claim applied to p = y to get z, convexity of ΓY and the fact that ΓY contains B ' σmin(A)Bn2 (Lemma 9) imply that q ∈ conv(B ∪ {z}) ⊆ ΓY and the algorithm outputs YES. We will prove the claim now. Let p ∈ ΓX. By the dual characterization of the centroid body (Proposition\n10), there exists a function λ : Rn → R such that p = E(λ(X)X) with −1 ≤ λ ≤ 1. Let z = 1N ∑N i=1 λ(Xi)Xi. We have EXi(λ(Xi)Xi) = p and EXi(|λ(Xi)Xi|1+γ) ≤ EXi(|Xi|1+γ) ≤M . By Lemma 4 and a union bound over every coordinate we get P(‖p− z‖ ≥ ) ≤ δ for N large enough."
    }, {
      "heading" : "5.1 Formal Argument",
      "text" : "Lemma 7 ([AGNR15]). Let S = (S1, . . . , Sn) ∈ Rn be an absolutely symmetrically distributed random vector such that E(|Si|) = 1 for all i. Then Bn1 ⊆ ΓS ⊆ [−1, 1]n. Moreover, n−1/2Bn2 ⊆ (ΓS)◦ ⊆ √ nBn2 .\nLemma 8 ([AGNR15]). Let X be a random vector on Rn. Let A : Rn → Rn be an invertible linear transformation. Then Γ(AX) = A(ΓX).\nLemma 9. Let S = (S1, . . . , Sn) ∈ Rn be an absolutely symmetrically distributed random vector such that E(|Si|) = 1 and E(|Si|1+γ) ≤M <∞ for all i. Let S(i), i = 1, . . . , N be a sample of i.i.d. copies of S. Let Y be a random vector, uniformly distributed in S(1), . . . , S(N). Then (1− ′)Bn1 ⊆ ΓY whenever\nN ≥ ( 16Mn4 ( ′)2 δ′ ) 1 2 + 3 γ .\nProof. From Lemma 7 we know ±ei ∈ ΓS. It is enough to apply Lemma 13 to ±ei with = ′/ √ n and δ = δ′/(2n). This gives, for any θ ∈ Sn−1, hΓY (θ) ≥ hΓS(θ)− ≥ hBn1 (θ)− ≥ (1− √ n )hBn1 (θ) = (1−\n′)hBn1 (θ). In particular, ΓY ⊇ (1− ′)Bn1 .\nProposition 10 (Dual characterization of centroid body). Let X be a n-dimensional random vector with finite first moment, that is, for all u ∈ Rn we have E(|〈u,X〉|) <∞. Then\nΓX = {E ( λ(X)X ) : λ : Rn → [−1, 1] is measurable}. (8)\nProof. Let K denote the rhs of the conclusion.We will show that K is a non-empty, closed convex set and show that hK = hΓX , which implies (8).\nBy definition, K is a non-empty bounded convex set. To see that it is closed, let (yk)k be a sequence in K such that yk → y ∈ Rn. Let λk be the function associated to yk ∈ K according to the definition of K. Let PX be the distribution of X. We have ‖λk‖L∞(PX) ≤ 1 and, passing to a subsequence kj , (λkj ) converges to λ ∈ L∞(PX) in the weak-∗ topology σ(L∞(PX), L1(PX)), where −1 ≤ λ ≤ 1. 1 This implies limj E(λkj (X)Xi) = limj ∫ Rn λkj (x)xi dPX(x) = ∫ Rn λ(x)xi dPX(x) = E(λ(X)Xi). Thus, we have y = limj ykj = limj E((λkj (X)X) = E(λ(X)X) and K is closed. To conclude, we compute hK and see that it is the same as the definition of hΓX . In the following equations λ ranges over functions such that λ : Rn → R is Borel-measurable and −1 ≤ λ ≤ 1.\nhK(θ) = sup y∈K 〈y, θ〉\n= sup λ\nE(λ(X)〈X, θ〉)\nand setting λ∗(x) = sgn〈x, θ〉,\n= E(λ∗(X)〈X, θ〉) = E(|〈X, θ〉|).\nLemma 11 (LP). Let X be a random vector uniformly distributed in {x(i)}Ni=1 ⊆ Rn. Let q ∈ Rn. Then:\n1. ΓX = 1N ∑N i=1[−x(i), x(i)].\n2. Point q ∈ ΓX iff there is a solution λ ∈ RN to the following linear feasibility problem:\n1\nN N∑ i=1 λix (i) = q − 1 ≤ λi ≤ 1 ∀i.\n(9)\n3. Let λ∗ be the optimal value of (always feasible) linear program\nλ∗ = maxλ\ns.t. 1\nN N∑ i=1 λix (i) = λq − 1 ≤ λi ≤ 1 ∀i\n(10)\nwith λ∗ =∞ if the linear program is unbounded. Then the Minkowski functional of ΓX at q is 1/λ∗.\nProof. 1. This is proven in [McM71]. It is also a special case of Proposition 10. We include an argument here for completeness. Let K := 1N ∑N i=1[−x(i), x(i)]. We compute hK to see it is the same as hΓX in\nthe definition of ΓX (Definition 1). As K and ΓX are non-empty compact convex sets, this implies K = ΓX. We have\nhK(y) = sup λi∈[−1,1]\n1\nN N∑ i=1 λix (i) · y\n1This is a standard argument, see [Bre11] for the background. Map x 7→ xi is in L1(PX). [Bre11, Theorem 4.13] gives that L1(PX) is a separable Banach space. [Bre11, Theorem 3.16] (Banach-Alaoglu-Bourbaki) gives that the unit ball in L∞(PX) is compact in the weak-* topology. [Bre11, Theorem 3.28] gives that the unit ball in L∞(PX) is metrizable and therefore sequentially compact in the weak-* topology. Therefore, any bounded sequence in L∞(PX) has a convergent subsequence in the weak-* topology.\n= max λi∈{−1,1}\n1\nN N∑ i=1 λix (i) · y\n= 1\nN N∑ i=1 |x(i) · y|\n= E(|X · y|).\n2. This follows immediately from part 1.\n3. This follows from part 1 and the definition of Minkowski functional.\nSubroutine 2 Weak Membership Oracle for ΓX\nInput: Query point q ∈ Rn, samples from symmetric ICA model X = AS, bounds sM ≥ σmax(A), sm ≤ σmin(A), closeness parameter , failure probability δ. Output: ( , δ)-weak membership decision for q ∈ ΓX. 1: Let N = poly(n,M, 1/sm, sM , 1/ , 1/δ). 2: Let (x(i))Ni=1 be an i.i.d. sample of X. 3: Check the feasibility of linear program (9). If feasible, output YES, otherwise output NO.\nProposition 12 (Correctness of Subroutine 2). Let X = AS be given by an ICA model such that for all i we have E(|Si|1+γ) ≤M <∞, Si is symmetrically distributed and normalized so that E|Si| = 1. Then, given a query point q ∈ Rn, > 0, δ > 0, sM ≥ σmax(A), and sm ≤ σmin(A), Subroutine 2 is an -weak membership oracle for q and ΓX with probability 1− δ using time and sample complexity poly(n,M, 1/sm, sM , 1/ , 1/δ).\nProof. Let Y be uniformly random in (x(i))Ni=1. There are two cases corresponding to the guarantees of the oracle:\n• Case q /∈ (ΓX) . Then there is a hyperplane separating q from (ΓX) . Let {x ∈ Rn : aTx = b} be the separating hyperplane, parameterized so that a ∈ Rn, b ∈ R, ‖a‖ = 1, aT q > b and aTx ≤ b for every x ∈ (ΓX) . In this case h(ΓX) (a) ≤ b and hΓX(a) ≤ b − . At the same time, hΓY (a) = E(|aTY |) = (1/N) ∑N i=1|aTx(i)|.\nWe want to apply Lemma 4 to aTX to get that hΓY (a) = (1/N) ∑N i=1|aTx(i)| is within of hΓX(a) = E(|aTX|). For this we need a bound on the (1 + γ)-moment of aTX. We use the bound from [AGNR15, Equation (10)]: E(|aTX|1+γ) ≤Mns1+γM . Lemma 4 implies that for\nN ≥ max  ( 8Mns1+γM 2δ )3/γ , ( 8Mns1+γM ) 1 2 + 1 γ  , (11) we have\nP ( | N∑ i=1 |aTx(i)| − E(|aTX|)| > ) ≤ δ.\nIn particular, with probability at least 1− δ we have hΓY (a) ≤ b, which implies q /∈ ΓY and, by Lemma 11, Subroutine 2 outputs NO.\n• Case q ∈ (ΓX)− . Let y = q + q̂ = q(1 + ‖q‖ ). Let α = 1 + ‖q‖ . Then y ∈ ΓX. Invoke Lemma 13\nfor i.i.d. sample (x(i))Ni=1 of X with p = y and equal to some 1 > 0 to be fixed later to conclude y ∈ (ΓY ) 1 . That is, there exist z ∈ ΓY such that\n‖z − y‖ ≤ 1. (12)\nLet w = z/α. Given (12) and the relationships y = αq and z = αw, we have\n‖w − q‖ ≤ ‖z − y‖ ≤ 1. (13)\nFrom Lemma 9 with ′ = 1/2 and equivariance of the centroid body (Lemma 8) we get ΓY ⊇ σmin(A) 2 √ n Bn2 . This and convexity of ΓY imply conv{σmin(A) 2 √ n Bn2 ∪{z}} ⊆ ΓY . In particular, the ball around w of radius\nr := ( 1− 1\nα\n) σmin(A)\n2 √ n\nis contained in ΓY . The choice 1 = r ≥ and (13) imply q ∈ ΓY and Subroutine 2 outputs YES whenever\nN ≥ ( 8Mn2\nr2δ\n) 1 2 + 1 γ\n.\nTo conclude, remember that q ∈ (ΓX)− . Therefore ‖q‖ + ≤ √ nσmax(A) (from Lemma 7 and equivariance of the centroid body, Lemma 8). This implies\nr = ‖q‖+ σmin(A) 2 √ n\n≥ σmin(A) 2nσmax(A)\nThe claim follows.\nLemma 13. Let X be a n-dimensional random vector such that for all coordinates i we have E(|Xi|1+γ) ≤ M <∞. Let p ∈ ΓX. Let (X(i))Ni=1 be an i.i.d. sample of X. Let Y be uniformly random in (X(i))Ni=1. Let\n> 0, δ > 0. If N ≥ ( 8Mn2\n2δ\n) 1 2 + 3 γ\n, then, with probability at least 1− δ, p ∈ (ΓY ) .\nProof. By Proposition 10, there exists a measurable function λ : Rn → R, −1 ≤ λ ≤ 1 such that p = E(Xλ(X)). Let\nz = 1\nN N∑ i=1 X(i)λ(X(i)).\nBy Proposition 10, z ∈ ΓY . We have EX(i)(X(i)λ(X(i))) = p and, for every coordinate j,\nEX(i)(|X (i) j λ(X (i))|1+γ) ≤ EX(i)(|X (i) j | 1+γ) ≤M.\nBy Lemma 4 and for any fixed coordinate j we have, over the choice of (X(i))Ni=1,\nP(|pj − zj | ≥ / √ n) ≤ 8M\n( / √ n)2Nγ/3\n= 8Mn\n2Nγ/3\nwhenever N ≥ (8M √ n/ ) 1 2 + 1 γ . A union bound over n choices of j gives:\nP(‖p− z‖ ≥ ) ≤ 8Mn 2\n2Nγ/3 .\nSo P(‖p− z‖ ≥ ) ≤ δ whenever\nN ≥ ( 8Mn2\n2δ )3/γ and N ≥ (8M √ n/ ) 1 2 + 1 γ . The claim follows."
    } ],
    "references" : [ {
      "title" : "A new learning algorithm for blind signal separation",
      "author" : [ "Shun Amari", "Andrzej Cichocki", "Howard H Yang" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Amari et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Amari et al\\.",
      "year" : 1996
    }, {
      "title" : "Heavy-tailed independent component analysis",
      "author" : [ "Joseph Anderson", "Navin Goyal", "Anupama Nandi", "Luis Rademacher" ],
      "venue" : "In 56th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2015),",
      "citeRegEx" : "Anderson et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2015
    }, {
      "title" : "Approximate likelihood for noisy mixtures",
      "author" : [ "Olivier Bermond", "Jean-François Cardoso" ],
      "venue" : "In Proc. ICA,",
      "citeRegEx" : "Bermond and Cardoso.,? \\Q1999\\E",
      "shortCiteRegEx" : "Bermond and Cardoso.",
      "year" : 1999
    }, {
      "title" : "Inference with multivariate heavy-tails in linear models",
      "author" : [ "Danny Bickson", "Carlos Guestrin" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bickson and Guestrin.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bickson and Guestrin.",
      "year" : 2010
    }, {
      "title" : "Functional analysis, Sobolev spaces and partial differential equations. Universitext",
      "author" : [ "Haim Brezis" ],
      "venue" : null,
      "citeRegEx" : "Brezis.,? \\Q2011\\E",
      "shortCiteRegEx" : "Brezis.",
      "year" : 2011
    }, {
      "title" : "Source separation using higher order moments",
      "author" : [ "J.F. Cardoso" ],
      "venue" : "In International Conference on Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "Cardoso.,? \\Q1989\\E",
      "shortCiteRegEx" : "Cardoso.",
      "year" : 1989
    }, {
      "title" : "Robustness of prewhitening against heavy-tailed sources. In Independent Component Analysis and Blind Signal Separation",
      "author" : [ "Aiyou Chen", "Peter J. Bickel" ],
      "venue" : "Fifth International Conference,",
      "citeRegEx" : "Chen and Bickel.,? \\Q2004\\E",
      "shortCiteRegEx" : "Chen and Bickel.",
      "year" : 2004
    }, {
      "title" : "Consistent independent component analysis and prewhitening",
      "author" : [ "Aiyou Chen", "Peter J Bickel" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "Chen and Bickel.,? \\Q2005\\E",
      "shortCiteRegEx" : "Chen and Bickel.",
      "year" : 2005
    }, {
      "title" : "Handbook of Blind Source Separation",
      "author" : [ "Pierre Comon", "Christian Jutten", "editors" ],
      "venue" : null,
      "citeRegEx" : "Comon et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Comon et al\\.",
      "year" : 2010
    }, {
      "title" : "Blind beamforming for non-gaussian signals",
      "author" : [ "J.-F. Cardoso", "A. Souloumiac" ],
      "venue" : "In Radar and Signal Processing, IEE Proceedings F,",
      "citeRegEx" : "Cardoso and Souloumiac.,? \\Q1993\\E",
      "shortCiteRegEx" : "Cardoso and Souloumiac.",
      "year" : 1993
    }, {
      "title" : "On portfolio selection under extreme risk measure: The heavy-tailed ica model",
      "author" : [ "Stephan Clemencon", "Skander Slim" ],
      "venue" : "International Journal of Theoretical and Applied Finance,",
      "citeRegEx" : "Clemencon and Slim.,? \\Q2007\\E",
      "shortCiteRegEx" : "Clemencon and Slim.",
      "year" : 2007
    }, {
      "title" : "The infinite factorial hidden markov model",
      "author" : [ "Jurgen V Gael", "Yee W Teh", "Zoubin Ghahramani" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Gael et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Gael et al\\.",
      "year" : 2009
    }, {
      "title" : "Independent Component Analysis",
      "author" : [ "Aapo Hyvarinen", "Juha Karhunen", "Erkki Oja" ],
      "venue" : null,
      "citeRegEx" : "Hyvarinen et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hyvarinen et al\\.",
      "year" : 2001
    }, {
      "title" : "Fast and robust fixed-point algorithms for independent component analysis",
      "author" : [ "A. Hyvarinen" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "Hyvarinen.,? \\Q1999\\E",
      "shortCiteRegEx" : "Hyvarinen.",
      "year" : 1999
    }, {
      "title" : "Novel characteristic function based criteria for ica",
      "author" : [ "A. Kankainen J. Eriksson", "V. Koivunen" ],
      "venue" : "In Proceedings ICA 2001,",
      "citeRegEx" : "Eriksson and Koivunen.,? \\Q2001\\E",
      "shortCiteRegEx" : "Eriksson and Koivunen.",
      "year" : 2001
    }, {
      "title" : "Alpha-stable distributions in signal processing of audio signals",
      "author" : [ "Preben Kidmose" ],
      "venue" : "In 41st Conference on Simulation and Modelling, Scandinavian Simulation Society,",
      "citeRegEx" : "Kidmose.,? \\Q2000\\E",
      "shortCiteRegEx" : "Kidmose.",
      "year" : 2000
    }, {
      "title" : "Blind Separation of Heavy Tail Signals",
      "author" : [ "Preben Kidmose" ],
      "venue" : "PhD thesis, Technical University of Denmark,",
      "citeRegEx" : "Kidmose.,? \\Q2001\\E",
      "shortCiteRegEx" : "Kidmose.",
      "year" : 2001
    }, {
      "title" : "Independent component analysis using the spectral measure for alpha-stable distributions",
      "author" : [ "Preben Kidmose" ],
      "venue" : "In Proceedings of IEEE-EURASIP Workshop on Nonlinear Signal and Image Processing,",
      "citeRegEx" : "Kidmose.,? \\Q2001\\E",
      "shortCiteRegEx" : "Kidmose.",
      "year" : 2001
    }, {
      "title" : "On zonotopes",
      "author" : [ "P. McMullen" ],
      "venue" : "Trans. Amer. Math. Soc.,",
      "citeRegEx" : "McMullen.,? \\Q1971\\E",
      "shortCiteRegEx" : "McMullen.",
      "year" : 1971
    }, {
      "title" : "Stable Distributions - Models for Heavy Tailed Data",
      "author" : [ "J.P. Nolan" ],
      "venue" : null,
      "citeRegEx" : "Nolan.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nolan.",
      "year" : 2015
    }, {
      "title" : "Blind source separation of noisy mixtures using a semi-parametric approach with application to heavy-tailed signals",
      "author" : [ "M. Sahmoudi", "K. Abed-Meraim", "M. Lavielle", "E. Kuhn", "Ph. Ciblat" ],
      "venue" : "In Proc. of EUSIPCO",
      "citeRegEx" : "Sahmoudi et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Sahmoudi et al\\.",
      "year" : 2005
    }, {
      "title" : "Super-efficiency in blind signal separation of symmetric heavy-tailed sources",
      "author" : [ "Yoav Shereshevski", "Arie Yeredor", "Hagit Messer" ],
      "venue" : "In Statistical Signal Processing,",
      "citeRegEx" : "Shereshevski et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Shereshevski et al\\.",
      "year" : 2001
    }, {
      "title" : "Ica by maximizing non-stability",
      "author" : [ "Baijie Wang", "Ercan E Kuruoglu", "Junying Zhang" ],
      "venue" : "In Independent Component Analysis and Signal Separation,",
      "citeRegEx" : "Wang et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning sparse topographic representations with products of student-t distributions",
      "author" : [ "Max Welling", "Simon Osindero", "Geoffrey E Hinton" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Welling et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Welling et al\\.",
      "year" : 2002
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Independent Component Analysis (ICA) is the problem of learning a square matrix A, given samples of X = AS, where S is a random vector with independent coordinates. Most existing algorithms are provably efficient only when each Si has finite and moderately valued fourth moment. However, there are practical applications where this assumption need not be true, such as speech and finance. Algorithms have been proposed for heavy-tailed ICA, but they are not practical, using random walks and the full power of the ellipsoid algorithm multiple times. The main contributions of this paper are: (1) A practical algorithm for heavy-tailed ICA that we call HTICA. We provide theoretical guarantees and show that it outperforms other algorithms in some heavy-tailed regimes, both on real and synthetic data. Like the current state-of-the-art, the new algorithm is based on the centroid body (a first moment analogue of the covariance matrix). Unlike the state-of-the-art, our algorithm is practically efficient. To achieve this, we use explicit analytic representations of the centroid body, which bypasses the use of the ellipsoid method and random walks. (2) We study how heavy tails affect different ICA algorithms, including HTICA. Somewhat surprisingly, we show that some algorithms that use the covariance matrix or higher moments can successfully solve a range of ICA instances with infinite second moment. We study this theoretically and experimentally, with both synthetic and real-world heavy-tailed data.",
    "creator" : "LaTeX with hyperref package"
  }
}