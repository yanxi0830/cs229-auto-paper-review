{
  "name" : "1705.10723.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast Regression with an `∞ Guarantee∗",
    "authors" : [ "Eric Price", "Zhao Song", "David P. Woodruff" ],
    "emails" : [ "zhaos@utexas.edu", "zhaos@utexas.edu", "dpwoodru@us.ibm.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The sketch-and-solve paradigm gives a bound on ‖x′ − x∗‖2 when A is well-conditioned. Our main result is that, when S is the subsampled randomized Fourier/Hadamard transform, the error x′ − x∗ behaves as if it lies in a “random” direction within this bound: for any fixed direction a ∈ Rd, we have with 1− d−c probability that\n〈a, x′ − x∗〉 . ‖a‖2‖x ′ − x∗‖2\nd 1 2−γ\n, (1)\nwhere c, γ > 0 are arbitrary constants. This implies ‖x′ − x∗‖∞ is a factor d 1 2−γ smaller than ‖x′ − x∗‖2. It also gives a better bound on the generalization of x′ to new examples: if rows of A correspond to examples and columns to features, then our result gives a better bound for the error introduced by sketch-and-solve when classifying fresh examples. We show that not all oblivious subspace embeddings S satisfy these properties. In particular, we give counterexamples showing that matrices based on Count-Sketch or leverage score sampling do not satisfy these properties.\nWe also provide lower bounds, both on how small ‖x′ − x∗‖2 can be, and for our new guarantee (1), showing that the subsampled randomized Fourier/Hadamard transform is nearly optimal. Our lower bound on ‖x′ − x∗‖2 shows that there is an O(1/ε) separation in the dimension of the optimal oblivious subspace embedding required for outputting an x′ for which ‖x′− x∗‖2 ≤ ‖Ax∗− b‖2 · ‖A†‖2, compared to the dimension of the optimal oblivious subspace embedding required for outputting an x′ for which ‖Ax′ − b‖2 ≤ (1 + )‖Ax∗ − b‖2, that is, the former problem requires dimension Ω(d/ 2) while the latter problem can be solved with dimension O(d/ ). This explains the reason known upper bounds on the dimensions of these two variants of regression have differed in prior work.\n∗A preliminary version of this paper appears in Proceedings of the 44th International Colloquium on Automata, Languages, and Programming (ICALP 2017).\nar X\niv :1\n70 5.\n10 72\n3v 1\n[ cs\n.D S]\n3 0\nM ay"
    }, {
      "heading" : "1 Introduction",
      "text" : "Oblivious subspace embeddings (OSEs) were introduced by Sarlos [Sar06] to solve linear algebra problems more quickly than traditional methods. An OSE is a distribution of matrices S ∈ Rm×n with m n such that, for any d-dimensional subspace U ⊂ Rn, with “high” probability S preserves the norm of every vector in the subspace. OSEs are a generalization of the classic JohnsonLindenstrauss lemma from vectors to subspaces. Formally, we require that with probability 1− δ,\n‖Sx‖2 = (1± ε)‖x‖2\nsimultaneously for all x ∈ U , that is, (1− ε)‖x‖2 ≤ ‖Sx‖2 ≤ (1 + ε)‖x‖2. A major application of OSEs is to regression. The regression problem is, given b ∈ Rn and A ∈ Rn×d for n ≥ d, to solve for\nx∗ = arg min x∈Rd ‖Ax− b‖2 (2)\nBecause A is a “tall” matrix with more rows than columns, the system is overdetermined and there is likely no solution to Ax = b, but regression will find the closest point to b in the space spanned by A. The classic answer to regression is to use the Moore-Penrose pseudoinverse: x∗ = A†b where\nA† = (A>A)−1A>\nis the “pseudoinverse” of A (assuming A has full column rank, which we will typically do for simplicity). This classic solution takes O(ndω−1 + dω) time, where ω < 2.373 is the matrix multiplication constant [CW90, Wil12, Gal14]: ndω−1 time to compute A>A and dω time to compute the inverse.\nOSEs speed up the process by replacing (2) with\nx′ = arg min x∈Rd ‖SAx− Sb‖2\nfor an OSE S on d + 1-dimensional spaces. This replaces the n × d regression problem with an m × d problem, which can be solved more quickly since m n. Because Ax − b lies in the d + 1- dimensional space spanned by b and the columns of A, with high probability S preserves the norm of SAx− Sb to 1± ε for all x. Thus,\n‖Ax′ − b‖2 ≤ 1 + ε 1− ε‖Ax ∗ − b‖2.\nThat is, S produces a solution x′ which preserves the cost of the regression problem. The running time for this method depends on (1) the reduced dimension m and (2) the time it takes to multiply S by A. We can compute these for “standard” OSE types:\n• If S has i.i.d. Gaussian entries, then m = O(d/ε2) is sufficient (and in fact, m ≥ d/ 2 is required [NN14]). However, computing SA takes O(mnd) = O(nd2/ε2) time, which is worse than solving the original regression problem (one can speed this up using fast matrix multiplication, though it is still worse than solving the original problem).\n• If S is a subsampled randomized Hadamard transform (SRHT) matrix with random sign flips (see Theorem 2.4 in [Woo14] for a survey, and also see [CNW16] which gives a recent improvement) then m increases to Õ(d/ε2 · log n), where Õ(f) = fpoly(log(f)). But now, we can compute SA using the fast Hadamard transform in O(nd log n) time. This makes the overall regression problem take O(nd log n+ dω/ε2) time.\n• If S is a random sparse matrix with random signs (the “Count-Sketch” matrix), then m = d1+γ/ε2 suffices for γ > 0 a decreasing function of the sparsity [CW13, MM13, NN13, BDN15, Coh16]. (The definition of a Count-Sketch matrix is, for any s ≥ 1, Si,j ∈ {0,−1/ √ s, 1/ √ s},\n∀i ∈ [m], j ∈ [n] and the column sparsity of matrix S is s. Independently in each column s positions are chosen uniformly at random without replacement, and each chosen position is set to −1/√s with probability 1/2, and +1/√s with probability 1/2.) Sparse OSEs can benefit from the sparsity of A, allowing for a running time of Õ(nnz(A)) + Õ(dω/ε2), where nnz(A) denotes the number of non-zeros in A.\nWhen n is large, the latter two algorithms are substantially faster than the naïve ndω−1 method."
    }, {
      "heading" : "1.1 Our Contributions",
      "text" : "Despite the success of using subspace embeddings to speed up regression, often what practitioners are interested is not in preserving the cost of the regression problem, but rather in the generalization or prediction error provided by the vector x′. Ideally, we would like for any future (unseen) example a ∈ Rd, that 〈a, x′〉 ≈ 〈a, x∗〉 with high probability.\nUltimately one may want to use x′ to do classification, such as regularized least squares classification (RLSC) [RYP03], which has been found in cases to do as well as support vector machines but is much simpler [ZP04]. In this application, given a training set of examples with multiple (non-binary) labels identified with the rows of an n × d matrix A, one creates an n × r matrix B, each column indicating the presence or absence of one of the r possible labels in each example. One then solves the multiple response regression problem minX ‖AX −B‖F , and uses X to classify future examples. A commonly used method is for a future example a, to compute 〈a, x1〉, . . . , 〈a, xr〉, where x1, . . . , xr are the columns of X. One then chooses the label i for which 〈a, xi〉 is maximum.\nFor this to work, we would like the inner products 〈a, x′1〉, . . . , 〈a, x′r〉 to be close to 〈a, x∗1〉, . . ., 〈a, x∗r〉, where X ′ is the solution to minX ‖SAX − SB‖F and X∗ is the solution to minX ‖AX − B‖F . For any O(1)-accurate OSE on d+ r dimensional spaces [Sar06], which also satisfies so-called approximate matrix multiplication with error ε′ = ε/ √ (d+ r), we get that\n‖x′ − x∗‖2 ≤ O(ε) · ‖Ax∗ − b‖2 · ‖A†‖2 (3)\nwhere ‖A†‖ is the spectral norm of A†, which equals the reciprocal of the smallest singular value of A. To obtain a generalization error bound for an unseen example a, one has\n|〈a, x∗〉 − 〈a, x′〉| = |〈a, x∗ − x′〉| ≤ ‖x∗ − x′‖2‖a‖2 = O(ε)‖a‖2‖Ax∗ − b‖2‖A†‖2, (4)\nwhich could be tight if given only the guarantee in (3). However, if the difference vector x′ − x∗ were distributed in a uniformly random direction subject to (3), then one would expect an Õ( √ d) factor improvement in the bound. This is what our main theorem shows:\nTheorem 1 (Main Theorem, informal). Suppose n ≤ poly(d) and matrix A ∈ Rn×d and vector b ∈ Rn are given. Let S ∈ Rm×n be a subsampled randomized Hadamard transform matrix with m = d1+γ/ε2 rows for an arbitrarily small constant γ > 0. For x′ = arg minx∈Rd ‖SAx− Sb‖2 and x∗ = arg minx∈Rd ‖Ax− b‖2, and any fixed a ∈ Rd,\n|〈a, x∗〉 − 〈a, x′〉| ≤ ε√ d ‖a‖2‖Ax∗ − b‖2‖A†‖2. (5)\nwith probability 1− 1/dC for an arbitrarily large constant C > 0. This implies that\n‖x∗ − x′‖∞ ≤ ε√ d ‖Ax∗ − b‖2‖A†‖2. (6)\nwith 1− 1/dC−1 probability. If n > poly(d), then by first composing S with a Count-Sketch OSE with poly(d) rows, one can achieve the same guarantee.\n(Here γ is a constant going to zero as n increases; see Theorem 10 for a formal statement of Theorem 1.)\nNotice that Theorem 1 is considerably stronger than that of (4) provided by existing guarantees. Indeed, in order to achieve the guarantee (6) in Theorem 1, one would need to set ε′ = ε/ √ d in existing OSEs, resulting in Ω(d2/ 2) rows. In contrast, we achieve only d1+γ/ 2 rows. We can improve the bound in Theorem 1 to m = d/ε2 if S is a matrix of i.i.d. Gaussians; however, as noted, computing S ·A is slower in this case.\nNote that Theorem 1 also makes no distributional assumptions on the data, and thus the data could be heavy-tailed or even adversarially corrupted. This implies that our bound is still useful when the rows of A are not sampled independently from a distribution with bounded variance.\nThe `∞ bound (6) of Theorem 1 is achieved by applying (5) to the standard basis vectors a = ei for each i ∈ [d] and applying a union bound. This `∞ guarantee often has a more natural interpretation than the `2 guarantee—if we think of the regression as attributing the observable as a sum of various factors, (6) says that the contribution of each factor is estimated well. One may also see our contribution as giving a way for estimating the pseudoinverse A† entrywise. Namely,\nwe get that (SA)†S ≈ A† in the sense that each entry is within additive O(ε √\nlog d d ‖A†‖2). There is\na lot of work on computing entries of inverses of a matrix, see, e.g., [ADL+12, LAKD08]. Another benefit of the `∞ guarantee is when the regression vector x∗ is expected to be ksparse (e.g. [Lee12]). In such cases, thresholding to the top k entries will yield an `2 guarantee a\nfactor √\nk d better than (3).\nOne could ask if Theorem 1 also holds for sparse OSEs, such as the Count-Sketch. Surprisingly, we show that one cannot achieve the generalization error guarantee in Theorem 1 with high probability, say, 1− 1/d, using such embeddings, despite the fact that such embeddings do approximate the cost of the regression problem up to a 1 + factor with high probability. This shows that the generalization error guarantee is achieved by some subspace embeddings but not all.\nTheorem 2 (Not all subspace embeddings give the `∞ guarantee; informal version of Theorem 20). The Count-Sketch matrix with d1.5 rows and sparsity d.25—which is an OSE with exponentially small failure probability—with constant probability will have a result x′ that does not satisfy the `∞ guarantee (6).\nWe can show that Theorem 1 holds for S based on the Count-Sketch OSE T with dO(C)/ 2 rows with 1 − 1/dC probability. We can thus compose the Count-Sketch OSE with the SRHT matrix and obtain an O(nnz(A)) + poly(d/ ) time algorithm to compute S ·TA achieving (6). We can also compute R · S · T ·A, where R is a matrix of Gaussians, which is more efficient now that STA only has d1+γ/ 2 rows; this will reduce the number of rows to d/ 2.\nAnother common method of dimensionality reduction for linear regression is leverage score sampling [DMIMW12, LMP13, PKB14, CMM15], which subsamples the rows of A by choosing each row with probability proportional to its “leverage scores”. With O(d log(d/δ)/ε2) rows taken, the result x′ will satisfy the `2 bound (3) with probability 1− δ. However, it does not give a good `∞ bound:\nTheorem 3 (Leverage score sampling does not give the `∞ guarantee; informal version of Theorem 23). Leverage score sampling with d1.5 rows—which satisfies the `2 bound with exponentially\nsmall failure probability—with constant probability will have a result x′ that does not satisfy the `∞ guarantee (6).\nFinally, we show that the d1+γ/ε2 rows that SRHT matrices use is roughly optimal:\nTheorem 4 (Lower bounds for `2 and `∞ guarantees; informal versions of of Theorem 14 and Corollary 18). Any sketching matrix distribution over m × n matrices that satisfies either the `2 guarantee (3) or the `∞ guarantee (6) must have m & min(n, d/ε2).\nNotice that our result shows the necessity of the 1/ε separation between the results originally defined in Equation (3) and (4) of Theorem 12 of [Sar06]. If we want to output some vector x′ such that ‖Ax′ − b‖2 ≤ (1 + ε)‖Ax∗ − b‖2, then it is known that m = Θ(d/ε) is necessary and sufficient. However, if we want to output a vector x′ such that ‖x′−x∗‖2 ≤ ε‖Ax∗− b‖2 · ‖A†‖2, then we show that m = Θ(d/ε2) is necessary and sufficient."
    }, {
      "heading" : "1.1.1 Comparison to Gradient Descent",
      "text" : "While this work is primarily about sketching methods, one could instead apply iterative methods such as gradient descent, after appropriately preconditioning the matrix, see, e.g., [AMT10, ZF13, CW13]. That is, one can use an OSE with constant ε to construct a preconditioner for A and then run conjugate gradient using the preconditioner. This gives an overall dependence of log(1/ ).\nThe main drawback of this approach is that one loses the ability to save on storage space or number of passes when A appears in a stream, or to save on communication or rounds when A is distributed. Given increasingly large data sets, such scenarios are now quite common, see, e.g., [CW09] for regression algorithms in the data stream model. In situations where the entries of A appear sequentially, for example, a row at a time, one does not need to store the full n× d matrix A but only the m× d matrix SA.\nAlso, iterative methods can be less efficient when solving multiple response regression, where one wants to minimize ‖AX − B‖ for a d × t matrix X and an n × t matrix B. This is the case when ε is constant and t is large, which can occur in some applications (though there are also other applications for which ε is very small). For example, conjugate gradient with a preconditioner will take Õ(ndt) time while using an OSE directly will take only Õ(nd+ d2t) time (since one effectively replaces n with O (d) after computing S · A), separating t from d. Multiple response regression, arises, for example, in the RLSC application above."
    }, {
      "heading" : "1.1.2 Proof Techniques",
      "text" : "Theorem 1. As noted in Theorem 2, there are some OSEs for which our generalization error bound does not hold. This hints that our analysis is non-standard and cannot use generic properties of OSEs as a black box. Indeed, in our analysis, we have to consider matrix products of the form S>S(UU>S>S)k for our random sketching matrix S and a fixed matrix U , where k is a positive integer. We stress that it is the same matrix S appearing multiple times in this expression, which considerably complicates the analysis, and does not allow us to appeal to standard results on approximate matrix product (see, e.g., [Woo14] for a survey). The key idea is to recursively reduce S>S(UU>S>S)k using a property of S. We use properties that only hold for specifics OSEs S: first, that each column of S is unit vector; and second, that for all pairs (i, j) and i 6= j, the inner product between Si and Sj is at most √ logn√ m with probability 1− 1/poly(n).\nTheorems 20 and 23. To show that Count-Sketch does not give the `∞ guarantee, we construct a matrix A and vector b as in Figure 1, which has optimal solution x∗ with all coordinates\n1/ √ d. We then show, for our setting of parameters, that there likely exists an index j ∈ [d] satisfying the following property: the jth column of S has disjoint support from the kth column of S for all k ∈ [d+ α] \\ {j} except for a single k > d, for which Sj and Sk share exactly one common entry in their support. In such cases we can compute x′j explicitly, getting |x′j − x∗j | = 1s√α . By choosing suitable parameters in our construction, this gives that ‖x′ − x∗‖∞ 1√d . The lower bound for leverage score sampling follows a similar construction.\nTheorem 14 and Corollary 18. The lower bound proof for the `2 guarantee uses Yao’s minimax principle. We are allowed to fix an m × n sketching matrix S and design a distribution over [A b]. We first write the sketching matrix S = UΣV > in its singular value decomposition (SVD). We choose the d+ 1 columns of the adjoined matrix [A, b] to be random orthonormal vectors. Consider an n × n orthonormal matrix R which contains the columns of V as its first m columns, and is completed on its remaining n−m columns to an arbitrary orthonormal basis. Then S · [A, b] = V >RR> · [A, b] = [UΣIm, 0] · [R>A,R>b]. Notice that [R>A,R>b] is equal in distribution to [A, b], since R is fixed and [A, b] is a random matrix with d+ 1 orthonormal columns. Therefore, S · [A, b] is equal in distribution to [UΣG,UΣh] where [G, h] corresponds to the first m rows of an n× (d+ 1) uniformly random matrix with orthonormal columns.\nA key idea is that if n = Ω(max(m, d)2), then by a result of Jiang [J+06], any m × (d + 1) submatrix of a random n×n orthonormal matrix has o(1) total variation distance to a d×d matrix of i.i.d. N(0, 1/n) random variables, and so any events that would have occurred had G and h been independent i.i.d. Gaussians, occur with the same probability for our distribution up to an 1− o(1) factor, so we can assume G and h are independent i.i.d. Gaussians in the analysis.\nThe optimal solution x′ in the sketch space equals (SA)†Sb, and by using that SA has the form UΣG, one can manipulate ‖(SA)†Sb‖ to be of the form ‖Σ̃†(ΣR)†Σh‖2, where the SVD of G is RΣ̃T . We can upper bound ‖Σ̃‖2 by √ r/n, since it is just the maximum singular value of a Gaussian ma-\ntrix, where r is the rank of S, which allows us to lower bound ‖Σ̃†(ΣR)†Σh‖2 by √ n/r‖(ΣR)†Σh‖2. Then, since h is i.i.d. Gaussian, this quantity concentrates to 1√ r ‖(ΣR)†Σh‖, since ‖Ch‖2 ≈ ‖C‖2F /n for a vector h of i.i.d. N(0, 1/n) random variables. Finally, we can lower bound ‖(ΣR)†Σ‖2F by ‖(ΣR)†ΣRR>‖2F by the Pythagorean theorem, and now we have that (ΣR)†ΣR is the identity, and so this expression is just equal to the rank of ΣR, which we prove is at least d. Noting that x∗ = 0 for our instance, putting these bounds together gives ‖x′ − x∗‖ ≥ √ d/r. The last ingredient is a way to ensure that the rank of S is at least d. Here we choose another distribution on inputs A and b for which it is trivial to show the rank of S is at least d with large probability. We require S be good on the mixture. Since S is fixed and good on the mixture, it is good for both distributions individually, which implies we can assume S has rank d in our analysis of the first distribution above."
    }, {
      "heading" : "1.2 Notation",
      "text" : "For a positive integer, let [n] = {1, 2, . . . , n}. For a vector x ∈ Rn, define ‖x‖2 = ( ∑n i=1 x 2 i ) 1 2 and ‖x‖∞ = maxi∈[n] |xi|. For a matrix A ∈ Rm×n, define ‖A‖2 = supx ‖Ax‖2/‖x‖2 to be the spectral norm of A and ‖A‖F = ( ∑ i,j A 2 i,j)\n1/2 to be the Frobenius norm of A. We use A† to denote the Moore-Penrose pseudoinverse of m×n matrix A, which if A = UΣV > is its SVD (where U ∈ Rm×n, Σ ∈ Rn×n and V ∈ Rn× for m ≥ n), is given by A† = V Σ−1U>.\nIn addition to O(·) notation, for two functions f, g, we use the shorthand f . g (resp. &) to indicate that f ≤ Cg (resp. ≥) for an absolute constant C. We use f h g to mean cf ≤ g ≤ Cf for constants c, C.\nDefinition 5 (Subspace Embedding). A (1± ) `2-subspace embedding for the column space of an n× d matrix A is a matrix S for which for all x ∈ Rd, ‖SAx‖22 = (1± )‖Ax‖22. Definition 6 (Approximate Matrix Product). Let 0 < < 1 be a given approximation parameter. Given matrices A and B, where A and B each have n rows, the goal is to output a matrix C so that ‖A>B − C‖F ≤ ‖A‖F ‖B‖F . Typically C has the form A>S>SB, for a random matrix S with a small number of rows. In particular, this guarantee holds for the subsampled randomized Hadamard transform S with O( −2) rows [DMMS11]."
    }, {
      "heading" : "2 Warmup: Gaussians OSEs",
      "text" : "We first show that if S is a Gaussian random matrix, then it satisfies the generalization guarantee. This follows from the rotational invariance of the Gaussian distribution.\nTheorem 7. Suppose A ∈ Rn×d has full column rank. If the entries of S ∈ Rm×n are i.i.d. N(0, 1/m), m = O(d/ε2), then for any vectors a, b and x∗ = A†b, we have, with probability 1 − 1/poly(d),\n|a>(SA)†Sb− a>x∗| . ε √\nlog d√ d ‖a‖2‖b−Ax∗‖2‖A†‖2.\nBecause SA has full column rank with probability 1, (SA)†SA = I. Therefore\n|a>(SA)†Sb− a>x∗| = |a>(SA)†S(b−Ax∗)| = |a>(SA)†S(b−AA†b)|.\nThus it suffices to only consider vectors b where A†b = 0, or equivalently U>b = 0. In such cases, SU will be independent of Sb, which will give the result. The proof is in Appendix A."
    }, {
      "heading" : "3 SRHT Matrices",
      "text" : "We first provide the definition of the subsampled randomized Hadamard transform(SRHT): let S = 1√\nrn PHnD. Here, D is an n×n diagonal matrix with i.i.d. diagonal entries Di,i, for which Di,i\nin uniform on {−1,+1}. The matrix Hn is the Hadamard matrix of size n× n, and we assume n is a power of 2. Here, Hn = [Hn/2, Hn/2;Hn/2, −Hn/2] and H1 = [1]. The r× n matrix P samples r coordinates of an n dimensional vector uniformly at random.\nFor other subspace embeddings, we no longer have that SU and Sb are independent. To analyze them, we start with a claim that allows us to relate the inverse of a matrix to a power series.\nClaim 8. Let S ∈ Rm×n, A ∈ Rn×d have SVD A = UΣV >, and define T ∈ Rd×d by\nT = Id − U>S>SU.\nSuppose SA has linearly independent columns and ‖T‖2 ≤ 1/2. Then\n(SA)†S = V Σ−1 ( ∞∑ k=0 T k ) U>S>S. (7)\nProof.\n(SA)†S = (A>S>SA)−1A>S>S\n= (V ΣU>S>SUΣV >)−1V ΣU>S>S\n= V Σ−1(U>S>SU)−1U>S>S\n= V Σ−1(Id − T )−1U>S>S\n= V Σ−1 ( ∞∑ k=0 T k ) U>S>S,\nwhere in the last equality, since ‖T‖2 < 1, the von Neumann series ∑∞ k=0 T k converges to\n(Id − T )−1.\nWe then bound the kth term of this sum:\nLemma 9. Let S ∈ Rr×n be the subsampled randomized Hadamard transform, and let a be a unit vector. Then with probability 1− 1/poly(n), we have\n|a>S>S(UU>S>S)kb| =O(logk n) · (O(d(log n)/r) + 1) k−12 · ( √ d‖b‖2(log n)/r + ‖b‖2(log 1 2 n)/r 1 2 )\nHence, for r at least d log2k+2 n log2(n/ε)/ε2, this is at most O(‖b‖2ε/ √ d) with probability at least 1− 1/poly(n).\nWe defer the proof of this lemma to the next section, and now show how the lemma lets us prove that SRHT matrices satisfy the generalization bound with high probability:\nTheorem 10. Suppose A ∈ Rn×d has full column rank with log n = do(1). Let S ∈ Rm×n be a subsampled randomized Hadamard transform with m = O(d1+α/ε2) for α = Θ( √ log logn\nlog d ). For any\nvectors a, b and x∗ = A†b, we have\n|a>(SA)†Sb− a>x∗| . ε√ d ‖a‖2‖b−Ax∗‖2‖Σ−1‖2\nwith probability 1− 1/poly(d). Proof. Define ∆ = Θ (\n1√ m\n) (logc d)‖a‖2‖b−Ax∗‖2‖Σ−1‖2. For a constant c > 0, we have that S is\na (1± γ)-subspace embedding (Definition 5) for γ = √\nd logc n m with probability 1− 1/poly(d) (see,\ne.g., Theorem 2.4 of [Woo14] and references therein), so ‖SUx‖2 = (1 ± γ)‖Ux‖2 for all x, which we condition on. Hence for T = Id − U>S>SU , we have ‖T‖2 ≤ (1 + γ)2 − 1 . γ. In particular, ‖T‖2 < 1/2 and we can apply Claim 8.\nAs in Section 2, SA has full column rank if S is a subspace embedding, so (SA)†SA = I and we may assume x∗ = 0 without loss of generality.\nBy the approximate matrix product (Definition 6), we have for some c that\n|a>V Σ−1U>S>Sb| ≤ log c d√ m ‖a‖2‖b‖2‖Σ−1‖2 ≤ ∆ (8)\nwith 1− 1/poly(d) probability. Suppose this event occurs, bounding the k = 0 term of (7). Hence it suffices to show that the k ≥ 1 terms of (7) are bounded by ∆.\nBy approximate matrix product (Definition 6), we also have with 1− 1/d2 probability that\n‖U>S>Sb‖F ≤ logc d√ m ‖U>‖F ‖b‖2 ≤\nlogc d √ d√\nm ‖b‖2.\nCombining with ‖T‖2 . γ we have for any k that\n|a>V Σ−1T kU>S>Sb| . γk(logc d) √ d√ m ‖a‖2‖Σ−1‖2‖b‖2.\nSince this decays exponentially in k at a rate of γ < 1/2, the sum of all terms greater than k is bounded by the kth term. As long as\nm & 1\nε2 d1+\n1 k logc n, (9)\nwe have γ = √\nd logc n m < εd −1/(2k)/ logc n, so that∑ k′≥k |a>V Σ−1T k′U>S>Sb| . ε√ d ‖a‖2‖Σ−1‖2‖b‖2.\nOn the other hand, by Lemma 9, increasing m by a Ck factor, we have for all k that\n|a>V >Σ−1U>S>S(UU>S>S)kb| . 1 2k ε√ d ‖a‖2‖b‖2‖Σ−1‖2\nwith probability at least 1− 1/poly(d), as long as m & d log2k+2 n log2(d/ε)/ε2. Since the T k term can be expanded as a sum of 2k terms of this form, we get that\nk∑ k′=1 |a>V Σ−1T kU>S>Sb| . ε√ d ‖a‖2‖b‖2‖Σ−1‖2\nwith probability at least 1− 1/poly(d), as long as m & d(C log n)2k+2 log2(d/ε)/ε2 for a sufficiently large constant C. Combining with (9), the result holds as long as\nm & d logc n\nε2 max((C log n)2k+2, d\n1 k )\nfor any k. Setting k = Θ( √\nlog d log logn) gives the result.\nCombining Different Matrices. In some cases it can make sense to combine different matrices that satisfy the generalization bound.\nTheorem 11. Let A ∈ Rn×d, and let R ∈ Rm×r and S ∈ Rr×n be drawn from distributions of matrices that are ε-approximate OSEs and satisfy the generalization bound (6). Then RS satisfies the generalization bound with a constant factor loss in failure probability and approximation factor.\nWe defer the details to Appendix B."
    }, {
      "heading" : "4 Proof of Lemma 9",
      "text" : "Proof. Each column Si of the subsampled randomized Hadamard transform has the same distribu-\ntion as σiSi, where σi is a random sign. It also has 〈Si, Si〉 = 1 for all i and | 〈Si, Sj〉 | . √ log(1/δ)√ r with probability 1− δ, for any δ and i 6= j. See, e.g., [LDFU13]. By expanding the following product into a sum, and rearranging terms, we obtain\na>S>S(UU>S>S)kb = ∑\ni0,j0,i1,j1,··· ,ik,jk\nai0bjkσi0σi1 · · ·σikσj0σj1 · · ·σjk\n·〈Si0 , Sj0〉(UU>)j0,i1〈Si1 , Sj1〉 · · · (UU>)jk−1,ik〈Sik , Sjk〉 = ∑ i0,jk ai0bjkσi0σjk ∑ j0,i1,j1,··· ,ik σi1 · · ·σikσj0σj1 · · ·σjk−1\n· 〈Si0 , Sj0〉(UU>)j0,i1〈Si1 , Sj1〉 · · · (UU>)jk−1,ik〈Sik , Sjk〉 = ∑ i0,jk σi0σjkZi0,jk\nwhere Zi0,jk is defined to be\nZi0,jk = ai0bjk ∑ i1,···ik j0,···jk−1 k∏ c=1 σic k−1∏ c=0 σjc · k∏ c=0 〈Sic , Sjc〉 k∏ c=1 (UU>)ic−1,jc\nNote that Zi0,jk is independent of σi0 and σjk . We observe that in the above expression if i0 = j0, i1 = j1, · · · , ik = jk, then the sum over these indices equals a>(UU>) · · · (UU>)b = 0, since\n〈Sic , Sjc〉 = 1 in this case for all c. Moreover, the sum over all indices conditioned on ik = jk is equal to 0. Indeed, in this case, the expression can be factored into the form ζ · U>b, for some random variable ζ, but U>b = 0.\nLet W be a matrix with Wi,j = σiσjZi,j . We need Khintchine’s inequality:\nFact 12 (Khintchine’s Inequality). Let σ1, . . . , σn be i.i.d. sign random variables, and let z1, . . . , zn be real numbers. Then there are constants C,C ′ > 0 so that\nPr [∣∣∣∣∣ n∑ i=1 ziσi ∣∣∣∣∣ ≥ Ct‖z‖2 ] ≤ e−C′t2 .\nWe note that Khintchine’s inequality sometimes refers to bounds on the moment of |∑i ziσi|, though the above inequality follows readily by applying a Markov bound to the high moments.\nWe apply Fact 12 to each column ofW , so that ifWi is the i-th column, we have by a union bound that with probability 1− 1/poly(n), ‖Wi‖2 = O(‖Zi‖2 √ log n) simultaneously for all columns i. It\nfollows that with the same probability, ‖W‖2F = O(‖Z‖2F log n), that is, ‖W‖F = O(‖Z‖F √\nlog n). We condition on this event in the remainder.\nThus, it remains to bound ‖Z‖F . By squaring Zi0,j0 and using that E[σiσj ] = 1 if i = j and 0 otherwise, we have,\nE σ\n[Z2i0,jk ] = a 2 i0b 2 jk ∑ i1,···ik j0,···jk−1 k∏ c=0 〈Sic , Sjc〉2 k∏ c=1 (UU>)2ic−1,jc (10)\nWe defer to Appendix E the proof that\nE S\n[‖Z‖2F ] ≤ (O(d(log n)/r) + 1)k−1 · (d‖b‖22(log2 n)/r2 + ‖b‖22(log n)/r)\nNote that we also have the bound:\n(O(d(log n)/r) + 1)k−1 ≤ (eO(d(logn)/r))k−1 ≤ eO(kd(logn)/r) ≤ O(1)\nfor any r = Ω(kd log n). Having computed the expectation of ‖Z‖2F , we now would like to show concentration. Consider\na specific\nZi0,jk = ai0bjk ∑ ik σik〈Sik , Sjk〉 · · · ∑ j1 σj1(UU >)j1,i2 ∑ i1 σi1〈Si1 , Sj1〉 ∑ j0 σj0〈Si0 , Sj0〉(UU>)j0,i1 .\nBy Fact 12, for each fixing of i1, with probability 1− 1/poly(n), we have\n∑ j0 σj0〈Si0 , Sj0〉(UU>)j0,i1 = O( √ log n) ∑ j0 〈Si0 , Sj0〉2(UU>)2j0,i1  12 . (11) Now, we can apply Khintchine’s inequality for each fixing of j1, and combine this with (11). With\nprobability 1− 1/poly(n), again we have∑ i1 σi1〈Si1 , Sj1〉 ∑ j0 σj0〈Si0 , Sj0〉(UU>)j0,i1\n= ∑ i1 σi1〈Si1 , Sj1〉O( √ log n) ∑ j0 〈Si0 , Sj0〉2(UU>)2j0,i1  12\n= O(log n) ∑ i1 〈Si1 , Sj1〉2 ∑ j0 〈Si0 , Sj0〉2(UU>)2j0,i1  12\nThus, we can apply Khintchine’s inequality recursively over all the 2k indexes j0, i1, j1, · · · , jk−1, ik, from which it follows that with probability 1 − 1/poly(n), for each such i0, jk, we have Z2i0,jk = O(logk n)E\nS [Z2i0,jk ], using (17). We thus have with this probability, that ‖Z‖2F = O(log k n)E S [‖Z‖2F ], completing the proof."
    }, {
      "heading" : "5 Lower bound for `2 and `∞ guarantee",
      "text" : "We prove a lower bound for the `2 guarantee, which immediately implies a lower bound for the `∞ guarantee.\nDefinition 13. Given a matrix A ∈ Rn×d, vector b ∈ Rn and matrix S ∈ Rr×n, denote x∗ = A†b. We say that an algorithm A(A, b, S) that outputs a vector x′ = (SA)†Sb “succeeds” if the following property holds: ‖x′ − x∗‖2 . ε‖b‖2 · ‖A†‖2 · ‖Ax∗ − b‖2.\nTheorem 14. Suppose Π is a distribution over Rm×n with the property that for any A ∈ Rn×d and b ∈ Rn, Pr\nS∼Π [A(A, b, S) succeeds ] ≥ 19/20. Then m & min(n, d/ε2).\nProof. The proof uses Yao’s minimax principle. Let D be an arbitrary distribution over Rn×(d+1), then E\n(A,b)∼D E S∼Π [A(A, b, S) succeeds ] ≥ 1 − δ. Switching the order of probabilistic quantifiers, an\naveraging argument implies the existence of a fixed matrix S0 ∈ Rm×n such that\nE (A,b)∼D [A(A, b, S0) succeeds ] ≥ 1− δ.\nThus, we must construct a distribution Dhard such that\nE (A,b)∼Dhard [A(A, b, S0) succeeds ] ≥ 1− δ,\ncannot hold for any Π0 ∈ Rm×n which does not satisfy m = Ω(d/ε2). The proof can be split into three parts. First, we prove a useful property. Second, we prove a lower bound for the case rank(S) ≥ d. Third, we show why rank(S) ≥ d is necessary.\n(I) We show that [SA, Sb] are independent Gaussian, if both [A, b] and S are orthonormal matrices. We can rewrite SA in the following sense,\nS︸︷︷︸ m×n · A︸︷︷︸ n×d = S︸︷︷︸ m×n R︸︷︷︸ n×n R>︸︷︷︸ n×n A︸︷︷︸ n×d\n= S [ S> S > ] [S S ] A = [ Im 0 ] [S S ] A = [ Im 0 ] Ã︸︷︷︸ n×d = Ãm︸︷︷︸ m×d\nwhere S is the complement of the orthonormal basis S, Im is a m×m identity matrix, and Ãm is the left m × d submatrix of Ã. Thus, using [J+06] as long as m = o(√n) (because of n = Ω(d3)) the total variation distance between [SA, Sb] and a random Gaussian matrix is small, i.e.,\nDTV ([SA, Sb], H) ≤ 0.01 (12)\nwhere each entry of H is i.i.d. Gaussian N (0, 1/n). (II) Here we prove the theorem in the case when S has rank r ≥ d (we will prove this is necessary in part III. Writing S = UΣV > in its SVD, we have\nS︸︷︷︸ m×n A = U︸︷︷︸ m×r Σ︸︷︷︸ r×r V >︸︷︷︸ r×n RR>A = UΣG (13)\nwhere R = [ V V ] . By a similar argument in Equation (12), as long as r = o( √ n) we have that G also can be approximated by a Gaussian matrix, where each entry is sampled from i.i.d. N (0, 1/n). Similarly, Sb = UΣh, where h also can be approximated by a Gaussian matrix, where each entry is sampled from i.i.d. N (0, 1/n).\nSince U has linearly independent columns, (UΣG)†UΣh = (ΣG)†U>UΣh = (ΣG)†Σh. The r × d matrix G has SVD G = R︸︷︷︸\nr×d Σ̃︸︷︷︸ d×d T︸︷︷︸ d×d , and applying the pseudo-inverse property\nagain, we have\n‖(SA)†Sb‖2 = ‖(ΣG)†Σh‖2 = ‖(ΣRΣ̃T )†Σh‖2 = ‖T †(ΣRΣ̃)†Σh‖2 = ‖(ΣRΣ̃)†Σh‖2 = ‖Σ̃†(ΣR)†Σh‖2,\nwhere the the first equality follows by Equation (13), the second equality follows by the SVD of G, the third and fifth equality follow by properties of the pseudo-inverse1 when T has orthonormal rows and Σ̃ is a diagonal matrix, and the fourth equality follows since ‖T †‖2 = 1 and T is an orthonormal basis.\nBecause each entry of G = RΣ̃T ∈ Rr×d is sampled from an i.i.d. Gaussian N (0, 1), using the result of [Ver10] we can give an upper bound for the maximum singular value of G: ‖Σ̃‖ . √ r n with probability at least .99. Thus,\n‖Σ̃†(ΣR)†Σh‖2 ≥ σmin(Σ̃†) · ‖(ΣR)†Σh‖2 = 1 σmax(Σ̃) ‖(ΣR)†Σh‖2 &\n√ n/r‖(ΣR)†Σh‖2.\nBecause h is a random Gaussian vector which is independent of (ΣR)†Σ, by Claim 15, Eh[‖(ΣR)†Σh‖22] = 1 n · ‖(ΣR)†Σ‖2F , where each entry of h is sampled from i.i.d. Gaussian N (0, 1/n). Then, using the Pythagorean Theorem,\n‖(ΣR)†Σ‖2F = ‖(ΣR)†ΣRR>‖2F + ‖(ΣR)†Σ(I −RR>)‖2F ≥ ‖(ΣR)†ΣRR>‖2F = ‖(ΣR)†ΣR‖2F = rank(ΣR)\n= rank(SA)\n= d.\n1https://en.wikipedia.org/wiki/Moore-Penrose_pseudoinverse\nThus, ‖x′ − x∗‖2 & √ d/r ≥ √ d/m = ε.\n(III) Now we show that we can assume that rank(S) ≥ d. We sample A, b based on the following distribution Dhard: with probability 1/2, A, b are sampled from D1; with probability 1/2, A, b are sampled from D2. In distribution D1, A is a random orthonormal basis and d is always orthogonal to A. In distribution D2, A is a d× d identity matrix in the top-d rows and 0s elsewhere, while b is a random unit vector. Then, for any (A, b) sampled from D1, S needs to work with probability at least 9/10. Also for any (A, b) sampled from D2, S needs to work with probability at least 9/10. The latter two statements follow since overall S succeeds on Dhard with probability at least 19/20.\nConsider the case where A, b are sampled from distribution D2. Then x∗ = b and OPT = 0. Then consider x′ which is the optimal solution to minx ‖SAx− Sb‖22, so x′ = (SA)†Sb = (SL)†SLb, where S can be decomposed into two matrices SL ∈ Rr×d and SR ∈ Rr×(n−d), S = [ SL SR ] . Plugging x′ into the original regression problem, ‖Ax′ − b‖22 = ‖A(SL)†SLb− b‖22, which is at most (1 + ε) OPT = 0. Thus rank(SL) is d. Since SL is a submatrix of S, the rank of S is also d.\nIt remains to define several tools which are used in the main proof of the lower bound.\nClaim 15. For any matrix A ∈ Rn×d, if each entry of a vector g ∈ Rd is chosen from an i.i.d Gaussian N (0, σ2), then E\ng [‖Ag‖22] = σ2‖A‖2F .\nProof.\nE g [‖Ag‖22] = Eg  n∑ i=1 ( d∑ j=1 Aijgj) 2  = E\ng  n∑ i=1 ( d∑ j=1 A2ijg 2 j + ∑ j 6=j′ AijAij′gjgj′)  =\nn∑ i=1 d∑ j=1 A2ijσ 2\n= σ2‖A‖2F .\nLet g1, g2, · · · , gt be i.i.d. N (0, 1) random variables. The random variables ∑t i=1 g 2 i are X 2 with\nt degree of freedom. Furthermore, the following tail bounds are known.\nFact 16 (Lemma 1 of [LM00]). Let g1, g2, · · · , gt be i.i.d. N (0, 1) random variables. Then for any x ≥ 0,\nPr [ t∑ i=1 g2i ≥ t+ 2 √ tx+ 2x ] ≤ exp(−x),\nand\nPr [ t∑ i=1 g2i ≤ t− 2 √ tx ] ≤ exp(−x).\nDefinition 17. Given a matrix A ∈ Rn×d, vector b ∈ Rn and matrix S ∈ Rr×n, denote x∗ = A†b. We say that an algorithm B(A, b, S) that outputs a vector x′ = (SA)†Sb “succeeds” if the following property holds:\n‖x′ − x∗‖∞ . ε√ d ‖b‖2 · ‖A†‖2 · ‖Ax∗ − b‖2.\nApplying ‖x′−x‖∞ ≥ 1√d‖x ′−x‖2 to Theorem 14 ,we obtain the `∞ lower bound as a corollary,\nCorollary 18. Suppose Π is a distribution over Rm×n with the property that for any A ∈ Rn×d and b ∈ Rn,\nPr S∼Π\n[B(A, b, S) succeeds ] ≥ 9/10.\nThen m & min(n, d/ε2)."
    }, {
      "heading" : "A Proof for Gaussian case",
      "text" : "Lemma 19. If the entries of S ∈ Rm×n are i.i.d. N(0, 1/m), m = O(d/ε2), and U>b = 0, then\n|a>(SA)†Sb| . ε √\nlog d√ d ‖a‖2‖b‖2‖Σ−1‖2\nfor any vectors a, b with probability 1− 1/poly(d).\nProof. With probability 1, the matrix SA has linearly independent columns, and so (SA)† is\n= (A>S>SA)−1A>S>\n= (V ΣU>S>SUΣV >)−1V ΣU>S>\n= V Σ−1(U>S>SU)−1Σ−1V >V ΣU>S>\n= V Σ−1(U>S>SU)−1U>S>.\nHence, we would like to bound\nX = a>V Σ−1(U>S>SU)−1U>S>Sb.\nIt is well-known (stated, for example, explicitly in Theorem 2.3 of [Woo14]) that with probability 1− exp(−d), the singular values of SU are (1± ε) for m = O(d/ε2). We condition on this event. It follows that\n‖V Σ−1(U>S>SU)−1U>S‖2 = ‖Σ−1(U>S>SU)−1U>S‖2 ≤ ‖Σ−1‖2‖(U>S>SU)−1‖2‖U>S‖2 ≤ ‖Σ−1‖2 · 1\n1− ε · (1 + ε)\n= O(‖Σ−1‖2),\nwhere the first equality uses that V is a rotation, the first inequality follows by sub-multiplicativity, and the second inequality uses that the singular values of SU are in the range [1− ε, 1 + ε]. Hence, with probability 1− exp(−d),\n‖a>V Σ−1(U>S>SU)−1U>S>‖2 = O(‖Σ−1‖2‖a‖2). (14)\nThe main observation is that since U>b = 0, SU is statistically independent from Sb. Hence, Sb is distributed as N(0, ‖b‖22Im), conditioned on the vector a>V Σ−1(U>S>SU)−1U>S>. It follows that conditioned on the value of a>V Σ−1(U>S>SU)−1U>S>, X is distributed as\nN(0, ‖b‖22‖a>V Σ−1(U>S>SU)−1U>S>‖22/m),\nand so using (14) , with probability 1− 1/poly(d), we have |X| = O(ε√log d‖a‖2‖b‖2‖Σ−1‖2/ √ d)."
    }, {
      "heading" : "B Combining Different Matrices",
      "text" : "In some cases it can make sense to combine different matrices that satisfy the generalization bound.\nTheorem 11. Let A ∈ Rn×d, and let R ∈ Rm×r and S ∈ Rr×n be drawn from distributions of matrices that are ε-approximate OSEs and satisfy the generalization bound (6). Then RS satisfies the generalization bound with a constant factor loss in failure probability and approximation factor.\nProof. For any vectors a, b, and x∗ = A†b we want to show\n|a>(RSA)†RSb− a>x∗| . ε√ d ‖a‖2‖b−Ax∗‖2‖A†‖2\nAs before, it suffices to consider the x∗ = 0 case. We have with probability 1− δ that\n|a>(SA)†Sb| . ε√ d ‖a‖2‖b‖2‖A†‖2;\nsuppose this happens. We also have by the properties of R, applied to SA and Sb, that\n|a>(RSA)†RSb− a>(SA)†Sb| . ε√ d ‖a‖2‖Sb‖2‖(SA)†‖2.\nBecause S is an OSE, we have ‖Sb‖2 ≤ (1 + ε) and ‖(SA)†‖2 & (1− ε)‖A†‖2. Therefore\n|a>(RSA)†RSb| . ε√ d ‖a‖2‖b‖2‖A†‖2\nWe describe a few of the applications of combining sketches.\nB.1 Removing dependence on n via Count-Sketch\nOne of the limitations of the previous section is that the choice of k depends on n. To prove that theorem, we have to assume that log d > log log n. Here, we show an approach to remove that assumption.\nThe main idea is instead of applying matrix S ∈ Rm×n to matrix A ∈ Rn×d directly, we pick two matrices S ∈ Rm×poly(d) and C ∈ Rpoly(d)×n, e.g. S is FastJL matrix and C is Count-Sketch matrix with s = 1. We first compute C · A, then compute S · (CA). The benefit of these operations is S only needs to multiply with a matrix (CA) that has poly(d) rows, thus the assumption we need is log d > log log(poly(d)) which is always true. The reason for choosing C as a Count-Sketch matrix with s = 1 is: (1) nnz(CA) ≤ nnz(A) (2) The running time is O(poly(d) · d+ nnz(A)).\nB.2 Combining Gaussians and SRHT\nBy combining Gaussians with SRHT matrices, we can embed into the optimal dimension O(d/ε2) with fast Õ(nd log n+ dω/ε4) embedding time.\nB.3 Combining all three\nBy taking Gaussians times SRHT times Count-Sketch, we can embed into the optimal dimension O(d/ε2) with fast O(nnz(A) + d4poly(1ε , log d)) embedding time."
    }, {
      "heading" : "C Count-Sketch does not obey the `∞ guarantee",
      "text" : "Here we demonstrate an A and a b such that Count-Sketch will not satisfy the `∞ guarantee with constant probability, so such matrices cannot satisfy the generalization guarantee (6) with high probability.\nTheorem 20. Let S ∈ Rm×n be drawn as a Count-Sketch matrix with s nonzeros per column. There exists a matrix A ∈ Rn×d and b ∈ Rn such that, if s2d . m . √ d3s, then the “true” solution x∗ = A†b and the approximation x′ = (SA)†Sb have large `∞ distance with constant probability:\n‖x′ − x∗‖∞ & √ d\nms ‖b‖2.\nPlugging in m = d1.5 and s = d0.25 we find that\n‖x′ − x∗‖∞ & 1/d3/8‖b‖2 1/ √ d‖b‖2,\neven though such a matrix is an OSE with probability exponential in s. Therefore there exists a constant c for which this matrix does not satisfy the generalization guarantee (6) with 1 − cd probability.\nProof. We choose the matrix A to be the identity on its top d rows: A = [ Id 0 ] . Choose some α ≥ 1, set the value of the first d coordinates of vector b to be 1√ d and set the value to be 1/ √ α for the next\nα coordinates, with the remaining entries all zero. Note that ‖b‖2 = √ 2, x∗ = (1/ √ d, . . . , 1/ √ d), and ‖Ax∗ − b‖2 = 1. Let Sk denote the kth column vector of matrix S ∈ Rm×n. We define two events, Event I, ∀k′ ∈ [d] and k′ 6= k, we have supp(Sk′)∩ supp(Sk) = ∅; Event II, ∃ a unique k′ ∈ {d+ 1, d+ 2, · · · , d+α} such that | supp(Sk′)∩ supp(Sk)| = 1, and all other k′ have supp(Sk′)∩ supp(Sk) = ∅. Using Claim 21, with probability at least .99 there exists a k for which both events hold.\nGiven the constructions of A and b described early, it is obvious that\nAx− b = [ x1 − 1√d , · · · , xd − 1√ d ,− 1√ α , · · · ,− 1√ α , 0, · · · , 0 ]> .\nConditioned on event I and II are holding, then denote supp(Sj) = {i1, i2, · · · , is}. Consider the terms involving xj in the quadratic form\nmin x ‖SAx− Sb‖22.\nit can be written as (s − 1)(xj − 1/ √ d)2 + (xj − 1/ √ d ± 1/√α)2. Hence the optimal x′ will have x′j = 1√ d ± 1 s √ α , which is different from the desired 1/ √ d by 1 s √ α . Plugging in our requirement of α h m2/(s3d2), we have\n‖x′ − x∗‖∞ ≥ 1\ns √ α & c\n√ sd2\nm2 & 1√ d\nwhere the last inequality follows by m . √ sd3. Thus, we get the result.\nClaim 21. If m = Ω(s2d), m = o(d2), α < d, and α = O( m 2\ns3d2 ), with probability at least .99 there\nexists a k ∈ [d] for which both event I and II hold.\nProof. If m = Ω(s2d), then for any i in {1, 2, ..., d}, let Xi be an indicator that the entries of column i are disjoint from all i′ in [d]\\{i}. Then E[Xi] ≥ .9999, so by Markov’s inequality, with probability .99, we have .99d columns having this property (indeed, the expected value of d − X is at most .0001d, so Pr[d − X ≥ .01d] ≤ E[d−X].01d ≤ .0001d.01d = .01). Define Event E to be that .99d columns of first d columns have the property that the entries of that column are disjoint from all the other d− 1 columns. Let S be the set of these .99d columns. Let N be the union of supports of columns in S.\nEach column i in {d+ 1, ..., d+ α} chooses s non-zero entries. Define event F ( which is similar as event E) to be that .99α columns of the next α columns have the property that the entries of that column are disjoint from all the other α − 1 columns. By the same argument, since α < d, with probability .99, we have .99α columns in {d + 1, ..., d + α} being disjoint from other columns in {d+ 1, ..., d+ α}. Condition on event F holding. Let L be the multiset union of supports of all columns in {d+ 1, ..., d+ α}. Then L has size α · s. Let M be the union of supports of all columns in {d+ 1, ..., d+ α}, that is, the set union rather than the multiset union. Note that |M | ≥ .99α · s because of .99α columns are disjoint from each other.\nThe intersection size x of N and M is hyper-geometrically distributed with expectation\nE[x] = s|S| · |M |\nm .\nBy a lower tail bound for the hypergeometric distribution 2 ,\nPr[x ≤ (p− t)n] ≤ exp(−2t2n),\nwhere p = s · |S|/m and n = |M |, so\nPr[x ≤ E[x]− t · |M |] ≤ exp(−2t2 · |M |) ≤ 0.01,\nwhere the last inequality follows by setting t = Θ(1/ √ |M |). Thus, we get with probability .99, the\nintersection size is at least s|S|·|M |m −Θ( √ |M |) .\nNow let W be the distinct elements in L\\M , so necessarily |W | ≤ .01α · s. By an upper tail bound for the hypergeometric distribution, the intersection size y of W and N satisfies\nPr[y ≥ (p+ t)n] ≤ exp(−2t2n),\nwhere p = s · |S|/m and n = |W |, we again get\nPr[y ≥ E[y] + t · |W |] ≤ exp(−2t2 · |W |).\nIf |W | = 0, then y = 0. Otherwise, we can set t = Θ(1/ √ |W |) so that this probability is less than\n.01, and we get with probability .99, the intersection size y is at most s · |S| · |W |/m + Θ( √ |W |).\nNote that we have that Θ( √ |M |) and Θ( √ |W |) are bounded by Θ(√s · α). Setting α = O( m2\ns3d2 )\nsuffices to ensure y is at most (1.01)s · |S| · |W |/m, and earlier that x is at least .99 · s · |S| · |M |/m. The probability one of the |S| blocks in N has two or more intersections with M is less than(\nx 2\n) times the probability two random distinct items in the intersection land in the block. This\nprobability is ( x 2 ) · ( s 2 )( s·|S|\n2 ) = Θ(x2/|S|2) = Θ(x2/d2) = Θ(m2/(d4s2)). So the expected number of such blocks is Θ(m2sd/(d4s2)) = Θ(m2/(d3s)) which is less than (.99 · s · |S| · |M |)/(2m) ≤ X/2 if m = o(d2), which we have. So, there are at least x/2 blocks which have intersection size exactly 1 with N . Note that the number of intersections of the |S| blocks with W is at most y, which is at most (1.01)s · |S| · |W |/m ≤ (1.01)s · |S| · 199 · |M |/m < x/2, and therefore there exists a block, that is, a column among the first d columns, which intersects M in exactly one position and does not intersect W . This is our desired column. Thus, we complete the proof."
    }, {
      "heading" : "D Leverage score sampling does not obey the `∞ guarantee",
      "text" : "Not only does Count-Sketch fail, but so does leverage score sampling, which is a technique that takes a subsample of rows of A with rescaling. In this section we show an A and a b such that leverage score sampling will not satisfy the `∞ guarantee. We start with a formal definition of leverage scores.\nDefinition 22 (Leverage Scores). Given an arbitrary n× d matrix A, with n > d, let U denote the n× d matrix consisting of the d left singular vectors of A, let U(i) denote the i−th row of the matrix U , so U(i) is a row vector. Then the leverage scores of the rows of A are given by li = ‖U(i)‖22, for i ∈ [n].\n2https://en.wikipedia.org/wiki/Hypergeometric_distribution\nThe leverage score sampling matrix can be thought of as a square diagonal matrix D ∈ Rn×n with diagonal entries chosen from some distribution. If Dii = 0, it means we do not choose the i-th row of matrix A., If Dii > 0, it means we choose that row of the matrix A and also rescale that row. We show that the leverage score sampling matrix cannot achieve `∞ guarantee, nor can it achieve our notion of generalization error.\nTheorem 23. Let D ∈ Rn×n be a leverage score sampling matrix with m nonzeros on the diagonal. There exists a matrix A ∈ Rn×d and a vector b ∈ Rn such that, if m . d √ d, then the “true” solution x∗ = A†b and the approximation x′ = (DA)†Db have large `∞ distance with constant probability:\n‖x′ − x∗‖∞ & 1√ d ‖b‖2.\nTherefore there exists a constant c for which this matrix does not satisfy the generalization guarantee (6) with 1− cd probability.\nProof. We choose the matrix A to be the identity on its top d rows, and L scaled identity matrices 1√ αd Id for the next dL rows, where L satisfies 1d + 1 αdL = 1 (to normalize each column of A), which implies L = α(d− 1). Choose some β ∈ [1, d). Set the value of the first d coordinates of vector b to be 1√\nd and set the value to be 1√ β for the next β coordinates, with the remaining entries all zero.\nNote that ‖b‖2 = √\n2. First, we compute ‖Ax− b‖22. Because β is less than d, there are two kinds of xj : one involves\nthe following term,\n( 1√ d xj − 1√ d )2 + (L− 1)( 1√ αd xj) 2, (15)\nwhere the optimal xj should be set to 1/d. The other involves the term:\n( 1√ d xj − 1√ d )2 + ( 1√ αd xj − 1√ β )2 + (L− 1)( 1√ αd xj) 2, (16)\nwhere the optimal xj should be set to 1/d+ 1/ √ αβd. Because we are able to choose α, β such that αβ & d, then xj = 1/d+ 1/ √ αβd . 1/d.\nSecond, we compute ‖DAx−Db‖22. With high probability, there exists a j satisfying Equation (16), but after applying leverage score sampling, the middle term of Equation (16) is removed. Let p1 = 1 d denote the leverage score of each of the top d rows of A, and let p2 = 1 αd denote the leverage score of each of the next Ld rows of A. We need to discuss the cases m > d and m ≤ d separately. If m > d, then the following term involves xj ,\n( 1√ p1 1√ d xj − 1√ p1 1√ d )2 + m− d d · ( 1√ p2 1√ αd xj) 2\n= 1\np1 ( 1√ d xj − 1√ d )2 + m− d d · 1 p2 ( 1√ αd xj) 2\n= d ( (\n1√ d xj − 1√ d )2 + m− d d α( 1√ αd xj) 2\n) .\nwhere the optimal xj should be set to\nxj = 1/d\n1/d+ (m− d)α/(αd2) = 1\n1 + (m− d)/d & 1\n(m− d)/d 1√\nd . by m d\n√ d\nIf m ≤ d, then the term involving xj is ( 1√p1 1√ d xj − 1√p1 1√ d )2 where the optimal xj should be set\nto be 1 1/ √ d.\nThird, we need to compute ‖Ax∗ − b‖22 and σmin(A). It is easy to see that σmin(A) because A is an orthonormal matrix. The upper bound for ‖Ax∗ − b‖22 = 2, and the lower bound is also a constant, which can be proved in the following way:\n‖Ax∗ − b‖22 = β∑ j=1 (15) + d∑ j=β+1 (16) ≥ d( 1√ d 1 d − 1√ d )2 & d · 1 d = 1."
    }, {
      "heading" : "E Bounding E[‖Z‖2F ]",
      "text" : "Before getting into the proof details, we define the key property of S being used in the rest of the proofs.\nDefinition 24 (All Inner Product Small(AIPS) Property). For any matrix S ∈ Rr×n, if for all i, j ∈ [n] with i 6= j we have\n|〈Si, Sj〉| = O( √ log n/ √ r),\nwe say that S satisfies the “AIPS” property.\nClaim 25. If S ∈ Rr×n is a subsampled Hadamard transform matrix, then the AIPS property holds with probability at least 1− 1/poly(n). Proof. From the structure of S, for any i 6= j, we have with probability 1 − 1/poly(n) such that |〈Si, Sj〉| = O( √ log n/ √ r). Applying a union bound over O(n2) pairs, we obtain that\nPr[ AIPS holds ] ≥ 1− 1/poly(n).\nThe main idea for bounding E[‖Z‖2F ] is to rewrite it as E[‖Z‖2F ] = E[ ‖Z‖2F | AIPS holds ] + E[ ‖Z‖2F | AIPS does not hold ]. Because Pr[ AIPS does not hold] is at most 1/poly(n), the first term dominates the second term, which means we only need to pay attention to the first term. We repeatedly apply this idea until all the S are removed.\nWe start by boundinbg E[‖Z‖2F ] by squaring Zi0,j0 and using that E[σiσj ] = 1 if i = j and 0 otherwise. Then, we obtain,\nE σ\n[Z2i0,jk ] = a 2 i0b 2 jk ∑ i1,···ik,j0,···jk−1 k∏ c=0 〈Sic , Sjc〉2 k∏ c=1 (UU>)2ic−1,jc . (17)\nWe thus have,∑ i0,jk,jk 6=ik a2i0〈Si0 , Sj0〉2b2jk〈Sik , Sjk〉 2 = a2j0‖b‖22O((log n)/r) + ‖a‖22‖b‖22O((log2 n)/r2) def = Cj0 ,\nwhere the first equality is from our conditioning, and the second equality is the definition of Cj0 . Hence, E\nS [‖Z‖2F ] is\n= ∑\ni1,···ik,j0,···jk−1\nk−1∏ c=1 〈Sic , Sjc〉2 k∏ c=1 (UU>)2jc−1,ic · ∑\ni0,jk,jk 6=ik\na2i0〈Si0 , Sj0〉2b2jk〈Sik , Sjk〉 2\n= ∑\ni1,···ik,j0,···jk−1\nk−1∏ c=1 〈Sic , Sjc〉2 k∏ c=1 (UU>)2jc−1,icCj0\n= ∑\ni1,···ik,j0,···jk−1\n〈Sik−1 , Sjk−1〉2(UU>)2jk−1,ik · k−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,icCj0 ,\nwhere the first equality follows from (17), the second equality by definition of Cj0 , and the final equality by factoring out c = k − 1 from one product and c = k − 2 from the other product.\nThe way to bound the term 〈Sik−1 , Sjk−1〉 is by separating the diagonal term where ik−1 = jk−1 and the non-diagonal term where ik−1 6= jk−1. We now use the aforementioned property of S, namely, that 〈Sik−1 , Sjk−1〉 = 1, if ik−1 = jk−1, while for ik−1 6= jk−1, we have with probability 1− 1/poly(n) that |〈Sik−1 , Sjk−1〉| = O( √ log n/ √ r) conditioned on AIPS holding.\nConditioned on AIPS holding, we can recursively reduce the number of terms in the product:\n‖Z‖2F\n= ∑\ni1,···ik,j0,···jk−1,ik−1 6=jk−1\nO((log n)/r) · (UU>)2jk−1,ik · k−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,icCj0\n+ ∑\ni1,···ik,j0,···jk−1,ik−1=jk−1\n1 · (UU>)2jk−1,ik · k−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,icCj0\n≤ ∑\ni1,···ik,j0,···jk−1\nO((log n)/r) · (UU>)2jk−1,ik · k−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,icCj0\n+ ∑\ni1,···ik,j0,···jk−1,ik−1=jk−1\n1 · (UU>)2jk−1,ik · k−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,icCj0 ,\nwhere the first equality follows from the property just mentioned, and the inequality follows by including back the tuples of indices for which ik−1 = jk−1, using that each summand is non-negative.\nOur next step will be to bound the term (UU>)2jk−1,ik . We have, ‖Z‖2F is\n≤ ∑\nik,jk−1\n(UU>)2ik,jk−1 ∑ i1,··· ,ik−1 j0,··· ,jk−2 O((log n)/r)\n· k−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,icCj0\n+ ∑\ni1,··· ,ik j0,··· ,jk−1 ik−1=jk−1\n1 · (UU>)2jk−1,ik k−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,icCj0\n= O(d(log n)/r) ∑\ni1,··· ,ik−1 j0,··· ,jk−2\nk−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,icCj0\n︸ ︷︷ ︸ A\n+ ∑\ni1,··· ,ik j0,··· ,jk−1 ik−1=jk−1\n1 · (UU>)2jk−1,ik k−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,icCj0\n︸ ︷︷ ︸ B\n,\nwhere the equality uses that ∑\nik,jk−1 (UU>)2ik,jk−1 = ‖UU>‖2F = d. We first upper bound term B:\n= ∑\ni1,··· ,ik j0,··· ,jk−1 ik−1=jk−1\n1 · (UU>)2jk−1,ik k−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,icCj0\n= ∑\ni1,··· ,ik−1 j0,j1,··· ,jk−1 ik−1=jk−1\nCj0 k−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,ic ∑ ik (UU>)2jk−1,ik\n= ∑\ni1,··· ,ik−1 j0,j1,··· ,jk−1 ik−1=jk−1\nCj0 k−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,ic |ejk−1UU>|2\n≤ ∑\ni1,··· ,ik−1 j0,j1,··· ,jk−1 ik−1=jk−1\nCj0 k−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,ic1\n= ∑\ni1,··· ,ik−1 j0,j1,··· ,jk−2\nCj0 k−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,ic ,\nwhere the first equality is the definition of B, the second equality follows by separating out the\nindex ik, the third equality uses that ∑\nik (UU>)2jk−1,ik = ‖ejk−1UU>‖22, that is, the squared norm\nof the jk−1-th row of UU>, the inequality follows since all rows of a projection matrix UU> have norm at most 1, and the final equality uses that jk−1 no longer appears in the expression.\nWe now merge our bounds for the terms A and B in the following way:\n‖Z‖2F ≤ A+B\n≤ O(d(log n)/r) ∑\ni1,··· ,ik−1 j0,··· ,jk−2\nk−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,icCj0\n+ ∑\ni1,··· ,ik−1 j0,j1,··· ,jk−2\nCj0 k−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,ic\n= (O(d(log n)/r) + 1) ∑\ni1,··· ,ik−1 j0,··· ,jk−2\nk−2∏ c=1 〈Sic , Sjc〉2 k−1∏ c=1 (UU>)2jc−1,icCj0\n≤ · · ·\n≤ (O(d(log n)/r) + 1)2 ∑\ni1,··· ,ik−2 j0,··· ,jk−3\nk−3∏ c=1 〈Sic , Sjc〉2 k−2∏ c=1 (UU>)2jc−1,icCj0\n≤ · · ·\n≤ (O(d(log n)/r) + 1)k−1 ∑ i1,j0 1∏ c=1 (UU>)2jc−1,icCj0 ≤ (O(d(log n)/r) + 1)k−1 (d‖b‖22(log2 n)/r2 + ‖b‖22(log n)/r),\nwhere the first two inequalities and first equality are by definition of A and B above. The first inequality follows by induction, since at this point we have replaced k with k − 1, and can repeat the argument, incurring another multiplicative factor of O(d(log n)/r)+1. Repeating the induction in this way we arrive at the last inequality. Finally, the last inequality follows by plugging in the definition of Cj0 , using that ∑ i1,j0\n(UU>)2j0,i1 = d, and∑ j0,i1 (UU>)2j0,i1a 2 j0 = ∑ j0 a2j0 ∑ i1 (UU>)2j0,i1 = ∑ j0 a2j0‖ej0UU>‖22 ≤ 1,\nwhere the inequality follows since each row of UU> has norm at most 1, and a is a unit vector. The final result is that\n‖Z‖2F ≤ (O(d(log n)/r) + 1)k−1 (d‖b‖22(log2 n)/r2 + ‖b‖22(log n)/r)."
    } ],
    "references" : [ {
      "title" : "On computing inverse entries of a sparse matrix in an out-ofcore environment",
      "author" : [ "Patrick Amestoy", "Iain S. Duff", "Jean-Yves L’Excellent", "Yves Robert", "François-Henry Rouet", "Bora Uçar" ],
      "venue" : "SIAM J. Scientific Computing,",
      "citeRegEx" : "Amestoy et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Amestoy et al\\.",
      "year" : 2012
    }, {
      "title" : "Blendenpik: Supercharging lapack’s least-squares solver",
      "author" : [ "Haim Avron", "Petar Maymounkov", "Sivan Toledo" ],
      "venue" : "SIAM J. Scientific Computing,",
      "citeRegEx" : "Avron et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Avron et al\\.",
      "year" : 2010
    }, {
      "title" : "Toward a unified theory of sparse dimensionality reduction in euclidean space",
      "author" : [ "Jean Bourgain", "Sjoerd Dirksen", "Jelani Nelson" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Bourgain et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bourgain et al\\.",
      "year" : 2015
    }, {
      "title" : "Ridge leverage scores for low-rank approximation",
      "author" : [ "Michael B Cohen", "Cameron Musco", "Christopher Musco" ],
      "venue" : "arXiv preprint arXiv:1511.07263,",
      "citeRegEx" : "Cohen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2015
    }, {
      "title" : "Optimal approximate matrix product in terms of stable rank",
      "author" : [ "Michael B Cohen", "Jelani Nelson", "David P. Woodruff" ],
      "venue" : "In Proceedings of the 43rd International Colloquium on Automata, Languages and Programming (ICALP 2016),",
      "citeRegEx" : "Cohen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2016
    }, {
      "title" : "Nearly tight oblivious subspace embeddings by trace inequalities",
      "author" : [ "Michael B. Cohen" ],
      "venue" : "In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Cohen.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cohen.",
      "year" : 2016
    }, {
      "title" : "Matrix multiplication via arithmetic progressions",
      "author" : [ "Don Coppersmith", "Shmuel Winograd" ],
      "venue" : "J. Symb. Comput.,",
      "citeRegEx" : "Coppersmith and Winograd.,? \\Q1990\\E",
      "shortCiteRegEx" : "Coppersmith and Winograd.",
      "year" : 1990
    }, {
      "title" : "Numerical linear algebra in the streaming model",
      "author" : [ "Kenneth L. Clarkson", "David P. Woodruff" ],
      "venue" : "In Proceedings of the forty-first annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Clarkson and Woodruff.,? \\Q2009\\E",
      "shortCiteRegEx" : "Clarkson and Woodruff.",
      "year" : 2009
    }, {
      "title" : "Low rank approximation and regression in input sparsity time",
      "author" : [ "Kenneth L Clarkson", "David P. Woodruff" ],
      "venue" : "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Clarkson and Woodruff.,? \\Q2013\\E",
      "shortCiteRegEx" : "Clarkson and Woodruff.",
      "year" : 2013
    }, {
      "title" : "Fast approximation of matrix coherence and statistical leverage",
      "author" : [ "Petros Drineas", "Malik Magdon-Ismail", "Michael W. Mahoney", "David P. Woodruff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2012
    }, {
      "title" : "Faster least squares approximation",
      "author" : [ "Petros Drineas", "Michael W Mahoney", "S Muthukrishnan", "Tamás Sarlós" ],
      "venue" : "Numerische Mathematik,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2011
    }, {
      "title" : "Powers of tensors and fast matrix multiplication",
      "author" : [ "François Le Gall" ],
      "venue" : "In International Symposium on Symbolic and Algebraic Computation, ISSAC ’14,",
      "citeRegEx" : "Gall.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gall.",
      "year" : 2014
    }, {
      "title" : "How many entries of a typical orthogonal matrix can be approximated by independent normals",
      "author" : [ "Tiefeng Jiang" ],
      "venue" : "The Annals of Probability,",
      "citeRegEx" : "Jiang,? \\Q2006\\E",
      "shortCiteRegEx" : "Jiang",
      "year" : 2006
    }, {
      "title" : "Computing entries of the inverse of a sparse matrix using the FIND algorithm",
      "author" : [ "Song Li", "Shaikh S. Ahmed", "Gerhard Klimeck", "Eric Darve" ],
      "venue" : "J. Comput. Physics,",
      "citeRegEx" : "Li et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2008
    }, {
      "title" : "Faster ridge regression via the subsampled randomized hadamard transform",
      "author" : [ "Yichao Lu", "Paramveer Dhillon", "Dean Foster", "Lyle Ungar" ],
      "venue" : "In Proceedings of the Neural Information Processing Systems (NIPS) Conference,",
      "citeRegEx" : "Lu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2013
    }, {
      "title" : "Adaptive estimation of a quadratic functional by model selection",
      "author" : [ "Beatrice Laurent", "Pascal Massart" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Laurent and Massart.,? \\Q2000\\E",
      "shortCiteRegEx" : "Laurent and Massart.",
      "year" : 2000
    }, {
      "title" : "Iterative row sampling",
      "author" : [ "Mu Li", "Gary L Miller", "Richard Peng" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Li et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2013
    }, {
      "title" : "Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression",
      "author" : [ "Xiangrui Meng", "Michael W Mahoney" ],
      "venue" : "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Meng and Mahoney.,? \\Q2013\\E",
      "shortCiteRegEx" : "Meng and Mahoney.",
      "year" : 2013
    }, {
      "title" : "Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings",
      "author" : [ "Jelani Nelson", "Huy L Nguyên" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Nelson and Nguyên.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nelson and Nguyên.",
      "year" : 2013
    }, {
      "title" : "Lower bounds for oblivious subspace embeddings. In Automata, Languages, and Programming ",
      "author" : [ "Jelani Nelson", "Huy L. Nguyên" ],
      "venue" : "41st International Colloquium,",
      "citeRegEx" : "Nelson and Nguyên.,? \\Q2014\\E",
      "shortCiteRegEx" : "Nelson and Nguyên.",
      "year" : 2014
    }, {
      "title" : "Provable deterministic leverage score sampling",
      "author" : [ "Dimitris Papailiopoulos", "Anastasios Kyrillidis", "Christos Boutsidis" ],
      "venue" : "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Papailiopoulos et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Papailiopoulos et al\\.",
      "year" : 2014
    }, {
      "title" : "Regularized least-squares classification",
      "author" : [ "Ryan Rifkin", "Gene Yeo", "Tomaso Poggio" ],
      "venue" : "Nato Science Series Sub Series III Computer and Systems Sciences,",
      "citeRegEx" : "Rifkin et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Rifkin et al\\.",
      "year" : 2003
    }, {
      "title" : "Improved approximation algorithms for large matrices via random projections",
      "author" : [ "Tamás Sarlós" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "Sarlós.,? \\Q2006\\E",
      "shortCiteRegEx" : "Sarlós.",
      "year" : 2006
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "Roman Vershynin" ],
      "venue" : "arXiv preprint arXiv:1011.3027,",
      "citeRegEx" : "Vershynin.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vershynin.",
      "year" : 2010
    }, {
      "title" : "Multiplying matrices faster than coppersmithwinograd",
      "author" : [ "Virginia Vassilevska Williams" ],
      "venue" : "In Proceedings of the 44th Symposium on Theory of Computing Conference,",
      "citeRegEx" : "Williams.,? \\Q2012\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 2012
    }, {
      "title" : "Sketching as a tool for numerical linear algebra",
      "author" : [ "David P. Woodruff" ],
      "venue" : "Foundations and Trends in Theoretical Computer Science,",
      "citeRegEx" : "Woodruff.,? \\Q2014\\E",
      "shortCiteRegEx" : "Woodruff.",
      "year" : 2014
    }, {
      "title" : "Randomized extended kaczmarz for solving least squares",
      "author" : [ "Anastasios Zouzias", "Nikolaos M. Freris" ],
      "venue" : "SIAM J. Matrix Analysis Applications,",
      "citeRegEx" : "Zouzias and Freris.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zouzias and Freris.",
      "year" : 2013
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Sketching has emerged as a powerful technique for speeding up problems in numerical linear algebra, such as regression. In the overconstrained regression problem, one is given an n × d matrix A, with n d, as well as an n × 1 vector b, and one wants to find a vector x̂ so as to minimize the residual error ‖Ax− b‖2. Using the sketch and solve paradigm, one first computes S · A and S · b for a randomly chosen matrix S, then outputs x′ = (SA)†Sb so as to minimize ‖SAx′ − Sb‖2. The sketch-and-solve paradigm gives a bound on ‖x′ − x∗‖2 when A is well-conditioned. Our main result is that, when S is the subsampled randomized Fourier/Hadamard transform, the error x′ − x∗ behaves as if it lies in a “random” direction within this bound: for any fixed direction a ∈ R, we have with 1− d−c probability that 〈a, x′ − x∗〉 . ‖a‖2‖x ′ − x∗‖2 d 1 2−γ , (1) where c, γ > 0 are arbitrary constants. This implies ‖x′ − x∗‖∞ is a factor d 1 2−γ smaller than ‖x′ − x∗‖2. It also gives a better bound on the generalization of x′ to new examples: if rows of A correspond to examples and columns to features, then our result gives a better bound for the error introduced by sketch-and-solve when classifying fresh examples. We show that not all oblivious subspace embeddings S satisfy these properties. In particular, we give counterexamples showing that matrices based on Count-Sketch or leverage score sampling do not satisfy these properties. We also provide lower bounds, both on how small ‖x′ − x∗‖2 can be, and for our new guarantee (1), showing that the subsampled randomized Fourier/Hadamard transform is nearly optimal. Our lower bound on ‖x′ − x∗‖2 shows that there is an O(1/ε) separation in the dimension of the optimal oblivious subspace embedding required for outputting an x′ for which ‖x′− x∗‖2 ≤ ‖Ax∗− b‖2 · ‖A†‖2, compared to the dimension of the optimal oblivious subspace embedding required for outputting an x′ for which ‖Ax′ − b‖2 ≤ (1 + )‖Ax∗ − b‖2, that is, the former problem requires dimension Ω(d/ ) while the latter problem can be solved with dimension O(d/ ). This explains the reason known upper bounds on the dimensions of these two variants of regression have differed in prior work. ∗A preliminary version of this paper appears in Proceedings of the 44th International Colloquium on Automata, Languages, and Programming (ICALP 2017). ar X iv :1 70 5. 10 72 3v 1 [ cs .D S] 3 0 M ay 2 01 7",
    "creator" : "LaTeX with hyperref package"
  }
}