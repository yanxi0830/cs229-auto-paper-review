{
  "name" : "0710.0485.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Prediction with expert advice for the Brier game",
    "authors" : [ "Vladimir Vovk" ],
    "emails" : [ "vovk@cs.rhul.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :0\n71 0.\n04 85\nv1 [\ncs .L\nG ]\n2 O\nIn this technical report I show that the Brier game of prediction is perfectly mixable and find the optimal learning rate and substitution function for it. These results are straightforward, but the computations are surprisingly messy.\n1 Loss bound\nA game of prediction consists of three components: the observation space Ω, the decision space Γ, and the loss function λ : Ω × Γ → R. In this note we are interested in the following Brier game [1]: Ω is a finite and non-empty set, Γ := P(Ω) is the set of all probability measures on Ω, and\nλ(ω, γ) = ∑\no∈Ω\n(γ{o} − δω{o}) 2 ,\nwhere δω ∈ P(Ω) is the probability measure concentrated at ω: δω{ω} = 1 and δω{o} = 0 for o 6= ω.\nThe Brier game is being played repeatedly by a learner having access to decisions made by a pool of experts, which leads to the following prediction protocol:\nPrediction with expert advice for the Brier game\nL0 := 0. Lk0 := 0, k = 1, . . . ,K. FOR N = 1, 2, . . . :\nExpert k announces γkN ∈ Γ, k = 1, . . . ,K. Learner announces γN ∈ Γ. Reality announces ωN ∈ Ω. LN := LN−1 + λ(ωN , γN). LkN := L k N−1 + λ(ωN , γ k N).\nEND FOR.\nAt each step of the game Learner is given K experts’ advice and is required to come up with his own decision; LN is his cumulative loss over the first N steps, and LkN is the kth expert’s cumulative loss over the first N steps.\nTheorem 1 Learner has a strategy ensuring\nLN ≤ min k=1,...,K\nLkN + lnK (1)\nfor all N = 1, 2, . . . and k ∈ {1, . . . ,K}. If a < 1, Learner does not have a strategy ensuring\nLN ≤ min k=1,...,K\nLkN + a lnK (2)\nfor all N = 1, 2, . . . and k ∈ {1, . . . ,K}.\n2 Proof of Theorem 1\nA vector f ∈ RΩ (understood to be a function f : Ω → R) is a superprediction if there is γ ∈ Γ such that, for all ω ∈ Ω, λ(ω, γ) ≤ f(ω); the set Σ of all superpredictions is the superprediction set. For each learning rate η > 0, let Φη : R Ω → (0,∞)Ω be the homeomorphism defined by\nΦη(f) : ω ∈ Ω 7→ e −ηf(ω), f ∈ RΩ.\nThe image Φη(Σ) of the superprediction set will be called the η-exponential superprediction set. It is known (see, e.g., [4], Appendix) that\nLN ≤ min k=1,...,K\nLkN + lnK\nη\ncan be guaranteed if and only if the η-exponential superprediction set is convex. Comparing this with (1) and (2) we can see that we are required to prove that\n• Φη(Σ) is convex when η ≤ 1;\n• Φη(Σ) is not convex when η > 1.\nDefine the η-exponential superprediction surface to be the part of the boundary of the η-exponential superprediction set Φη(Σ) lying inside (0,∞)\nΩ. The idea of the proof is to check that, for all η < 1, the Gauss–Kronecker curvature of this surface is nowhere vanishing (we will need some basic notions of elementary differential geometry; all definitions can be found in, e.g., [2]). Even when this is done, however, there is still uncertainty as to in which direction the surface is bulging (towards the origin or away from it). The standard argument (as in [2], Chapter 12, Theorem 6) based on the continuity of the smallest principal curvature shows that the η-exponential superprediction set is bulging away from the origin for small enough η: indeed, this is obviously true at some point, and so is true everywhere on the surface. By the continuity in η this is also true\nfor all η < 1. Now, since the η-exponential superprediction set is convex for all η < 1, it is also convex for η = 1.\nLet us now check that the Gauss–Kronecker curvature of the η-exponential superprediction surface is always positive when η < 1 and is sometimes negative when η > 1 (the rest of the proof, an elaboration of the above argument, will be easy). Set n := |Ω|; without loss of generality we assume Ω = {1, . . . , n}.\nThe parametric representation of the η-exponential superprediction surface is\n\n     \nx1 x2\n... xn−1\nxn\n\n      =\n\n      \ne−η((u 1 −1)2+(u2)2+···+(un−1)2+(un)2) e−η((u 1)2+(u2−1)2+···+(un−1)2+(un)2)\n...\ne−η((u 1)2+(u2)2+···+(un−1−1)2+(un)2) e−η((u 1)2+(u2)2+···+(un−1)2+(un−1)2)\n\n       , (3)\nwhere u1, . . . , un−1 are the coordinates on the surface, u1, . . . , un−1 ∈ (0, 1) subject to u1+ · · ·un−1 < 1, and un is a shorthand for 1−u1− · · ·−un−1. The derivative of (3) in u1 is\n∂\n∂u1\n\n     \nx1 x2\n... xn−1\nxn\n\n      = 2η\n\n      \n(un − u1 + 1)e−η((u 1 −1)2+(u2)2+···+(un−1)2+(un)2)\n(un − u1)e−η((u 1)2+(u2−1)2+···+(un−1)2+(un)2)\n...\n(un − u1)e−η((u 1)2+(u2)2+···+(un−1−1)2+(un)2)\n(un − u1 − 1)e−η((u 1)2+(u2)2+···+(un−1)2+(un−1)2)\n\n      \n∝\n\n      \n(un − u1 + 1)e2ηu 1\n(un − u1)e2ηu 2\n...\n(un − u1)e2ηu n−1\n(un − u1 − 1)e2ηu n\n\n       ,\nthe derivative in u2 is\n∂\n∂u2\n\n     \nx1 x2\n... xn−1\nxn\n\n      ∝\n\n      \n(un − u2)e2ηu 1\n(un − u2 + 1)e2ηu 2\n...\n(un − u2)e2ηu n−1\n(un − u2 − 1)e2ηu n\n\n       ,\nand so on, up to\n∂\n∂un−1\n\n     \nx1 x2\n... xn−1\nxn\n\n      ∝\n\n      \n(un − un−1)e2ηu 1 (un − un−1)e2ηu 2\n...\n(un − un−1 + 1)e2ηu n−1\n(un − un−1 − 1)e2ηu n\n\n       ,\nall coefficients of proportionality being equal and positive. A normal vector to the surface can be found as\nZ := ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ e1 e2 · · · en−1 en (un − u1 + 1)e2ηu 1 (un − u1)e2ηu 2 · · · (un − u1)e2ηu n−1 (un − u1 − 1)e2ηu n (un − u2)e2ηu 1 (un − u2 + 1)e2ηu 2 · · · (un − u2)e2ηu n−1 (un − u2 − 1)e2ηu n ... ... . . . ... ...\n(un − un−1)e2ηu 1\n(un − un−1)e2ηu 2 · · · (un − un−1 + 1)e2ηu n−1 (un − un−1 − 1)e2ηu n\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ .\nThe coefficient in front of e1 is the (n− 1)× (n− 1) determinant\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ (un − u1)e2ηu 2\n· · · (un − u1)e2ηu n−1\n(un − u1 − 1)e2ηu n\n(un − u2 + 1)e2ηu 2 · · · (un − u2)e2ηu n−1\n(un − u2 − 1)e2ηu n\n... . . .\n... ...\n(un − un−1)e2ηu 2 · · · (un − un−1 + 1)e2ηu n−1 (un − un−1 − 1)e2ηu n\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣\n∝ e−2ηu 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ un − u1 · · · un − u1 un − u1 − 1 un − u2 + 1 · · · un − u2 un − u2 − 1 ... . . . ... ...\nun − un−1 · · · un − un−1 + 1 un − un−1 − 1\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣\n= e−2ηu 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 1 · · · 1 un − u1 − 1 2 1 · · · 1 un − u2 − 1 1 2 · · · 1 un − u3 − 1 ... ... . . . ... ...\n1 1 · · · 2 un − un−1 − 1\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣\n= e−2ηu 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 1 · · · 1 un − u1 − 1 1 0 · · · 0 u1 − u2 0 1 · · · 0 u1 − u3 ... ... . . . ... ... 0 0 · · · 1 u1 − un−1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣\n= e−2ηu 1( (−1)n(un − u1 − 1) + (−1)n+1(u1 − u2) + (−1)n+1(u1 − u3) + · · ·+ (−1)n+1(u1 − un−1) )\n= e−2ηu 1 (−1)n ( (u2 + u3 + · · ·+ un)− (n− 1)u1 − 1 )\n= −e−2ηu 1 (−1)nnu1 ∝ u1e−2ηu 1 (4)\n(with a positive coefficient of proportionality, e2η, in the first ∝; the third equality follows from the expansion of the determinant along the last column and then along the first row).\nSimilarly, the coefficient in front of ei is proportional (with the same co-\nefficient of proportionality) to uie−2ηu i for i = 2, . . . , n − 1; indeed, the\n(n − 1) × (n − 1) determinant representing the coefficient in front of ei can be reduced to the form analogous to (4) by moving the ith row to the top.\nThe coefficient in front of en is proportional to\ne−2ηu n ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ un − u1 + 1 un − u1 · · · un − u1 un − u1 un − u2 un − u2 + 1 · · · un − u2 un − u2 ... ... . . . ... ... un − un−2 un − un−2 · · · un − un−2 + 1 un − un−2\nun − un−1 un − un−1 · · · un − un−1 un − un−1 + 1\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣\n= e−2ηu n ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 0 · · · 0 un − u1 0 1 · · · 0 un − u2 ... ... . . . ... ... 0 0 · · · 1 un − un−2\n−1 −1 · · · −1 un − un−1 + 1\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣\n= e−2ηu n ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 0 · · · 0 un − u1 0 1 · · · 0 un − u2 ... ... . . . ... ... 0 0 · · · 1 un − un−2\n0 0 · · · 0 nun\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = nune−2ηu n\n(with the coefficient of proportionality e2η(−1)n−1). The Gauss–Kronecker curvature at the point with coordinates (u1, . . . , un−1) is proportional (with a positive coefficient of proportionality, possibly depending on the point) to\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∂ZT ∂u1 ... ∂ZT ∂un−1\nZT\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣\n([2], Chapter 12, Theorem 5, with T standing for transposition). A straightforward calculation allows us to rewrite this determinant (ignoring the positive coefficient ((−1)n−1ne2η)n) as\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ (1− 2ηu1)e−2ηu 1\n0 · · · 0 (−1 + 2ηun)e−2ηu n\n0 (1− 2ηu2)e−2ηu 2 · · · 0 (−1 + 2ηun)e−2ηu n ... ... . . . ... ... 0 0 · · · (1− 2ηun−1)e−2ηu n−1 (−1 + 2ηun)e−2ηu n\nu1e−2ηu 1\nu2e−2ηu 2\n· · · un−1e−2ηu n−1\nune−2ηu n\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣\n∝ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1− 2ηu1 0 · · · 0 −1 + 2ηun 0 1− 2ηu2 · · · 0 −1 + 2ηun ... ... . . . ... ... 0 0 · · · 1− 2ηun−1 −1 + 2ηun\nu1 u2 · · · un−1 un\n∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣\n= u1(1− 2ηu2)(1− 2ηu3) · · · (1− 2ηun) + u2(1− 2ηu1)(1− 2ηu3) · · · (1− 2ηun)\n+ · · ·+ un(1 − 2ηu1)(1 − 2ηu2) · · · (1− 2ηun−1) (5)\n(with a positive coefficient of proportionality; to avoid calculation of the parities of various permutations, the reader might prefer to prove the last equality by induction in n, expanding the last determinant along the first column). Our goal is to show that the last expression in (5) is positive when η < 1 but can be negative when η > 1.\nIf η > 1, set u1 = u2 := 1/2 and u3 = · · · = un := 0. The last expression in (5) becomes negative. Therefore, the η-exponential superprediction set is not convex ([2], Chapter 13, Theorem 1).\nIt remains to consider the case η < 1. Set ti := 1 − 2ηu i, i = 1, . . . , n; the\nconstraints on the ti are\n− 1 < 1− 2η < ti < 1, t1 + · · ·+ tn = n− 2η > n− 2. (6)\nOur goal is to prove\n(1− t1)t2t3 · · · tn + (1− t2)t1t3 · · · tn + · · ·+ (1− tn)t1t2 · · · tn−1 > 0,\ni.e., t2t3 · · · tn + t1t3 · · · tn + · · ·+ t1t2 · · · tn−1 > nt1 · · · tn. (7)\nThis reduces to 1\nt1 + · · ·+\n1\ntn > n (8)\nif t1 · · · tn > 0, and to 1\nt1 + · · ·+\n1\ntn < n (9)\nif t1 · · · tn < 0. The remaining case is where some of the ti are zero; for concreteness, let tn = 0. By (6) we have t1 + · · · + tn−1 > n − 2, and so all of t1, . . . , tn−1 are positive; this shows that (7) is indeed true.\nLet us prove (8). Since t1 · · · tn > 0, all of t1, . . . , tn are positive (if two of them were negative, the sum t1 + · · · + tn would be less than n − 2; cf. (6)). Therefore,\n1 t1 + · · ·+ 1 tn > 1 + · · ·+ 1 ︸ ︷︷ ︸\nn times\n= n.\nTo establish (7) it remains to prove (9). Suppose, without loss of generality, that t1 > 0, t2 > 0, . . . , tn−1 > 0, tn < 0. Since the function t ∈ (0, 1] 7→ 1/t\nis convex, we can also assume, without loss of generality, t1 = · · · = tn−2 = 1. Then tn−1 + tn > 0, and so\n1\ntn−1 +\n1\ntn < 0;\ntherefore, 1\nt1 + · · ·+\n1\ntn−2 +\n1\ntn−1 +\n1\ntn < n− 2 < n.\nFinally, let us check that the positivity of the Gauss–Kronecker curvature implies the convexity of the η-exponential superprediction set, for η ≤ 1. Because of the continuity of the η-exponential superprediction surface in η we can and will assume, without loss of generality, that η < 1. The η-exponential superprediction surface will be oriented by choosing the normal vector field directed towards the origin; this can be done since\n\n \nx1\n... xn\n\n  ∝\n\n \ne2ηu 1\n... e2ηu n\n\n  , Z ∝\n\n \n−u1e−2ηu 1 ... −une−2ηu n\n\n  , (10)\nwith the first coefficient of proportionality positive (cf. (3) and the bottom row of the first determinant in (5)), and the scalar product of the two vectors in (10) is always negative.\nLet us first check that the smallest principal curvature\nk1 = k1(u 1, . . . , un−1, η)\nof the η-exponential superprediction surface is always positive (among the arguments of k1 we list not only the coordinates u\n1, . . . , un−1 of a point on the surface (3) but also the learning rate η ∈ (0, 1)). At least at some (u1, . . . , un−1, η) the value of k1(u\n1, . . . , un−1, η) is positive: take a sufficiently small η and the point on the surface (3) at which the maximum of x1 + · · ·+ xn is attained (the point of the η-exponential superprediction set at which the maximum is attained will lie on the surface since the maximum is attained at (x1, . . . , xn) = (1, . . . , 1) when η = 0). Therefore, for all (u1, . . . , un−1, η) the value of k1(u\n1, . . . , un−1, η) is positive: if k1 had different signs at two points in the set\n{ (u1, . . . , un−1, η) |u1 ∈ (0, 1), . . . , un−1 ∈ (0, 1),\nu1 + · · ·+ un−1 < 1, η ∈ (0, 1) } , (11)\nwe could connect these points by a continuous curve lying completely inside (11); at some point on the curve, k1 would be zero, in contradiction to the positivity of the Gauss–Kronecker curvature k1 · · · kn−1.\nNow it is easy to show that the η-exponential superprediction set is convex. Suppose there are two points A and B on the η-exponential superprediction surface such that the interval [A,B] contains points outside the η-exponential\nsuperprediction set. The intersection of the plane OAB, where O is the origin, with the η-exponential superprediction surface is a planar curve; the curvature of this curve at the point between A and B closest to the origin will be negative (with the curve oriented by directing the normal vector field towards the origin), contradicting the positivity of k1 at that point and Meusnier’s theorem (cf. (10)).\n3 Prediction algorithm\nTo achieve the loss bound (1) in Theorem 1 Learner can use the Aggregating Algorithm (AA, as described in, e.g., [3], Section 2.1, (15)) with η = 1. In this section we will find a substitution function for the AA for the Brier game with η ≤ 1, which is the only component of the AA not described explicitly in [3]. Our substitution function will not require that its input, the generalized prediction, should be computed from the normalized distribution on the experts; this is a valuable feature for generalizations to an infinite number of experts (as demonstrated in, e.g., [3], Appendix A.1).\nSuppose that we are given a generalized prediction (l1, . . . , ln) T computed by the Aggregating Pseudo-Algorithm (APA) from a normalized distribution on the experts. Since (l1, . . . , ln)\nT is a superprediction (remember that we are assuming η ≤ 1), we are only required to find a permitted prediction\n\n     \nλ1 λ2 ...\nλn−1 λn\n\n      =\n\n     \n(u1 − 1)2 + (u2)2 + · · ·+ (un−1)2 + (un)2 (u1)2 + (u2 − 1)2 + · · ·+ (un−1)2 + (un)2 ... (u1)2 + (u2)2 + · · ·+ (un−1 − 1)2 + (un)2 (u1)2 + (u2)2 + · · ·+ (un−1)2 + (un − 1)2\n\n     \n(12)\n(cf. (3)) satisfying λ1 ≤ l1, . . . , λn ≤ ln. (13)\nNow suppose we are given a generalized prediction (L1, . . . , Ln) T computed by the APA from an unnormalized distribution on the experts; in other words, we are given\n\n  L1 ... Ln\n\n  =\n\n  l1 + c ...\nln + c\n\n \nfor some c ∈ R. To find (12) satisfying (13) we can first find the largest t ∈ R such that (L1 − t, . . . , Ln − t)\nT is still a superprediction and then find (12) satisfying\nλ1 ≤ L1 − t, . . . , λn ≤ Ln − t. (14)\nSince t ≥ c, it is clear that (λ1, . . . , λn) T will also satisfy the required (13).\nProposition 1 Define s ∈ R by the requirement\nn∑\ni=1\n(s− Li) + = 2. (15)\nThe unique solution to the optimization problem t → max under the constraints (14) with λ1, . . . , λn as in (12) will be\nui = (s− Li)\n+\n2 , i = 1, . . . , n, (16)\nt = s− 1− (u1)2 − · · · − (un)2. (17)\nThere exists a unique s satisfying (15) since the left-hand side of (15) is a continuous, increasing (strictly increasing when positive) and unbounded above function of s. The substitution function is given by (16).\nProof of Proposition 1 Let us denote the ui and t defined by (16) and (17) as ui and t, respectively. To see that they satisfy the constraints (14), notice that the ith constraint can be spelt out as\n(u1)2 + · · ·+ (un)2 − 2ui + 1 ≤ Li − t,\nwhich immediately follows from (16) and (17). As a by-product, we can see that the inequality becomes an equality, i.e.,\nt = Li − 1 + 2u i − (u1)2 − · · · − (un)2, (18)\nfor all i with ui > 0. We can rewrite (14) as\n \n\nt ≤ L1 − 1 + 2u 1 − (u1)2 − · · · − (un)2, ... t ≤ Ln − 1 + 2u n − (u1)2 − · · · − (un)2,\n(19)\nand our goal is to prove that these inequalities imply t < t (unless u1 = u1, . . . , un = un). Choose ui (necessarily ui > 0 unless u1 = u1, . . . , un = un; in the latter case, however, we can, and will, also choose ui > 0) for which ǫi := u i − ui is maximal. Then every value of t satisfying (19) will also satisfy\nt ≤ Li − 1 + 2u i −\nn∑\nj=1\n(uj)2\n= Li − 1 + 2u i − 2ǫi −\nn∑\nj=1\n(uj)2 + 2 n∑\nj=1\nǫju j −\nn∑\nj=1\nǫ2j\n≤ Li − 1 + 2u i −\nn∑\nj=1\n(uj)2 −\nn∑\nj=1\nǫ2j ≤ t,\nwith the last ≤ following from (18) and becoming < when not all uj coincide with uj .\nAcknowledgments\nThis work was partially supported by EPSRC (grant EP/F002998/1), MRC (grant G0301107), and the Cyprus Research Promotion Foundation.\nReferences\n[1] Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78:1–3, 1950.\n[2] John A. Thorpe. Elementary Topics in Differential Geometry. Springer, New York, 1979.\n[3] Vladimir Vovk. Competitive on-line statistics. International Statistical Review, 69:213–248, 2001.\n[4] Vladimir Vovk. Defensive forecasting for optimal prediction with expert advice. Technical Report arXiv:0708.1503 [cs.LG], arXiv.org e-Print archive, August 2007."
    } ],
    "references" : [ {
      "title" : "Verification of forecasts expressed in terms of probability",
      "author" : [ "Glenn W. Brier" ],
      "venue" : "Monthly Weather Review,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1950
    }, {
      "title" : "Elementary Topics in Differential Geometry",
      "author" : [ "John A. Thorpe" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1979
    }, {
      "title" : "Competitive on-line statistics",
      "author" : [ "Vladimir Vovk" ],
      "venue" : "International Statistical Review,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2001
    }, {
      "title" : "Defensive forecasting for optimal prediction with expert advice",
      "author" : [ "Vladimir Vovk" ],
      "venue" : "Technical Report arXiv:0708.1503 [cs.LG], arXiv.org e-Print archive,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In this note we are interested in the following Brier game [1]: Ω is a finite and non-empty set, Γ := P(Ω) is the set of all probability measures on Ω, and",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : ", [4], Appendix) that",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 1,
      "context" : ", [2]).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 1,
      "context" : "The standard argument (as in [2], Chapter 12, Theorem 6) based on the continuity of the smallest principal curvature shows that the η-exponential superprediction set is bulging away from the origin for small enough η: indeed, this is obviously true at some point, and so is true everywhere on the surface.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "([2], Chapter 12, Theorem 5, with T standing for transposition).",
      "startOffset" : 1,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "Therefore, the η-exponential superprediction set is not convex ([2], Chapter 13, Theorem 1).",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : ", [3], Section 2.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 2,
      "context" : "In this section we will find a substitution function for the AA for the Brier game with η ≤ 1, which is the only component of the AA not described explicitly in [3].",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 2,
      "context" : ", [3], Appendix A.",
      "startOffset" : 2,
      "endOffset" : 5
    } ],
    "year" : 2017,
    "abstractText" : "In this technical report I show that the Brier game of prediction is perfectly mixable and find the optimal learning rate and substitution function for it. These results are straightforward, but the computations are surprisingly messy.",
    "creator" : "LaTeX with hyperref package"
  }
}