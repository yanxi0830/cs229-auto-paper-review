{
  "name" : "1305.2505.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions",
    "authors" : [ "Purushottam Kar", "Bharath K Sriperumbudur" ],
    "emails" : [ "purushot@cse.iitk.ac.in", "bs493@statslab.cam.ac.uk", "prajain@microsoft.com", "hk@cse.iitk.ac.in" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Several supervised learning problems involve working with pairwise or higher order loss functions, i.e., loss functions that depend on more than one training samProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\nple. Take for example the metric learning problem (Jin et al., 2009), where the goal is to learn a metric M that brings points of a similar label together while keeping differently labeled points apart. In this case the loss function used is a pairwise loss function `(M, (x, y), (x′, y′)) = φ (yy′ (1−M(x,x′))) where φ is the hinge loss function. In general, a pairwise loss function is of the form ` : H × X × X → R+ where H is the hypothesis space and X is the input domain. Other examples include preference learning (Xing et al., 2002), ranking (Agarwal & Niyogi, 2009), AUC maximization (Zhao et al., 2011) and multiple kernel learning (Kumar et al., 2012).\nIn practice, algorithms for such problems use intersecting pairs of training samples to learn. Hence the training data pairs are not i.i.d. and consequently, standard generalization error analysis techniques do not apply to these algorithms. Recently, the analysis of batch algorithms learning from such coupled samples has received much attention (Cao et al., 2012; Clémençon et al., 2008; Brefeld & Scheffer, 2005) where a dominant idea has been to use an alternate representation of the U-statistic and provide uniform convergence bounds. Another popular approach has been to use algorithmic stability (Agarwal & Niyogi, 2009; Jin et al., 2009) to obtain algorithm-specific results.\nWhile batch algorithms for pairwise (and higher-order) learning problems have been studied well theoretically, online learning based stochastic algorithms are more popular in practice due to their scalability. However, their generalization properties were not studied until recently. Wang et al. (2012) provided the first generalization error analysis of online learning methods\nar X\niv :1\n30 5.\n25 05\nv1 [\ncs .L\nG ]\n1 1\napplied to pairwise loss functions. In particular, they showed that such higher-order online learning methods also admit online to batch conversion bounds (similar to those for first-order problems (Cesa-Bianchi et al., 2001)) which can be combined with regret bounds to obtain generalization error bounds. However, due to their proof technique and dependence on L∞ covering numbers of function classes, their bounds are not tight and have a strong dependence on the dimensionality of the input space.\nIn literature, there are several instances where Rademacher complexity based techniques achieve sharper bounds than those based on covering numbers (Kakade et al., 2008). However, the coupling of different input pairs in our problem does not allow us to use such techniques directly.\nIn this paper we introduce a generic technique for analyzing online learning algorithms for higher order learning problems. Our technique, that uses an extension of Rademacher complexities to higher order function classes (instead of covering numbers), allows us to give bounds that are tighter than those of (Wang et al., 2012) and that, for several learning scenarios, have no dependence on input dimensionality at all.\nKey to our proof is a technique we call Symmetrization of Expectations which acts as a decoupling step and allows us to reduce excess risk estimates to Rademacher complexities of function classes. (Wang et al., 2012), on the other hand, perform a symmetrization with probabilities which, apart from being more involved, yields suboptimal bounds. Another advantage of our technique is that it allows us to obtain fast convergence rates for learning algorithms that use strongly convex loss functions. Our result, that uses a novel two stage proof technique, extends a similar result in the first order setting by Kakade & Tewari (2008) to the pairwise setting.\nWang et al. (2012) (and our results mentioned above) assume an online learning setup in which a stream of points z1, . . . , zn is observed and the penalty function used at the tth step is L̂t(h) = 1t−1 ∑t−1 τ=1 `(h, zt, zτ ). Consequently, the results of Wang et al. (2012) expect regret bounds with respect to these all-pairs penalties L̂t. This requires one to use/store all previously seen points which is computationally/storagewise expensive and hence in practice, learning algorithms update their hypotheses using only a bounded subset of the past samples (Zhao et al., 2011).\nIn the above mentioned setting, we are able to give generalization bounds that only require algorithms to give regret bounds with respect to finite-buffer penalty\nfunctions such as L̂buft (h) = 1|B| ∑ z∈B `(h, zt, z) where B is a buffer that is updated at each step. Our proofs hold for any stream oblivious buffer update policy including FIFO and the widely used reservoir sampling policy (Vitter, 1985; Zhao et al., 2011)1.\nTo complement our online to batch conversion bounds, we also provide a memory efficient online learning algorithm that works with bounded buffers. Although our algorithm is constrained to observe and learn using the finite-buffer penalties L̂buft alone, we are still able to provide high confidence regret bounds with respect to the all-pairs penalty functions L̂t. We note that Zhao et al. (2011) also propose an algorithm that uses finite buffers and claim an all-pairs regret bound for the same. However, their regret bound does not hold due to a subtle mistake in their proof.\nWe also provide empirical validation of our proposed online learning algorithm on AUC maximization tasks and show that our algorithm performs competitively with that of (Zhao et al., 2011), in addition to being able to offer theoretical regret bounds.\nOur Contributions:\n(a) We provide a generic online-to-batch conversion technique for higher-order supervised learning problems offering bounds that are sharper than those of (Wang et al., 2012).\n(b) We obtain fast convergence rates when loss functions are strongly convex.\n(c) We analyze online learning algorithms that are constrained to learn using a finite buffer.\n(d) We propose a novel online learning algorithm that works with finite buffers but is able to provide a high confidence regret bound with respect to the all-pairs penalty functions."
    }, {
      "heading" : "2. Problem Setup",
      "text" : "For ease of exposition, we introduce an online learning model for higher order supervised learning problems in this section; concrete learning instances such as AUC maximization and metric learning are given in Section 6. For sake of simplicity, we restrict ourselves to pairwise problems in this paper; our techniques can be readily extended to higher order problems as well.\nFor pairwise learning problems, our goal is to learn a\n1Independently, Wang et al. (2013) also extended their proof to give similar guarantees. However, their bounds hold only for the FIFO update policy and have worse dependence on dimensionality in several cases (see Section 5).\nreal valued bivariate function h∗ : X × X → Y, where h∗ ∈ H, under some loss function ` : H×Z×Z → R+ where Z = X × Y.\nThe online learning algorithm is given sequential access to a stream of elements z1, z2, . . . , zn chosen i.i.d. from the domain Z. Let Zt := {z1, . . . , zt}. At each time step t = 2 . . . n, the algorithm posits a hypothesis ht−1 ∈ H upon which the element zt is revealed and the algorithm incurs the following penalty:\nL̂t(ht−1) = 1\nt− 1 t−1∑ τ=1 `(ht−1, zt, zτ ). (1)\nFor any h ∈ H, we define its expected risk as:\nL(h) := E z,z′\nJ`(h, z, z′)K . (2)\nOur aim is to present an ensemble h1, . . . , hn−1 such that the expected risk of the ensemble is small. More specifically, we desire that, for some small > 0,\n1\nn− 1 n∑ t=2 L(ht−1) ≤ L(h∗) + ,\nwhere h∗ = arg min h∈H L(h) is the population risk minimizer. Note that this allows us to do hypothesis selection in a way that ensures small expected risk. Specifically, if one chooses a hypothesis as ĥ :=\n1 (n−1) ∑n t=2 ht−1 (for convex `) or ĥ := arg min\nt=2,...,n L(ht)\nthen we have L(ĥ) ≤ L(h∗) + .\nSince the model presented above requires storing all previously seen points, it becomes unusable in large scale learning scenarios. Instead, in practice, a sketch of the stream is maintained in a buffer B of capacity s. At each step, the penalty is now incurred only on the pairs {(zt, z) : z ∈ Bt} where Bt is the state of the buffer at time t. That is,\nL̂buft (ht−1) = 1 |Bt| ∑ z∈Bt `(ht−1, zt, z). (3)\nWe shall assume that the buffer is updated at each step using some stream oblivious policy such as FIFO or Reservoir sampling (Vitter, 1985) (see Section 5).\nIn Section 3, we present online-to-batch conversion bounds for online learning algorithms that give regret bounds w.r.t. penalty functions given by (1). In Section 4, we extend our analysis to algorithms using strongly convex loss functions. In Section 5 we provide generalization error bounds for algorithms that give regret bounds w.r.t. finite-buffer penalty functions given by (3). Finally in section 7 we present a novel memory efficient online learning algorithm with regret bounds."
    }, {
      "heading" : "3. Online to Batch Conversion Bounds for Bounded Loss Functions",
      "text" : "We now present our generalization bounds for algorithms that provide regret bounds with respect to the all-pairs loss functions (see Eq. (1)). Our results give tighter bounds and have a much better dependence on input dimensionality than the bounds given by Wang et al. (2012). See Section 3.1 for a detailed comparison.\nAs was noted by (Wang et al., 2012), the generalization error analysis of online learning algorithms in this setting does not follow from existing techniques for first-order problems (such as (Cesa-Bianchi et al., 2001; Kakade & Tewari, 2008)). The reason is that the terms Vt = L̂t(ht−1) do not form a martingale due to the intersection of training samples in Vt and Vτ , τ < t.\nOur technique, that aims to utilize the Rademacher complexities of function classes in order to get tighter bounds, faces yet another challenge at the symmetrization step, a precursor to the introduction of Rademacher complexities. It turns out that, due to the coupling between the “head” variable zt and the “tail” variables zτ in the loss function L̂t, a standard symmetrization between true zτ and ghost z̃τ samples does not succeed in generating Rademacher averages and instead yields complex looking terms.\nMore specifically, suppose we have true variables zt and ghost variables z̃t and are in the process of bounding the expected excess risk by analyzing expressions of the form\nEorig = `(ht−1, zt, zτ )− `(ht−1, z̃t, z̃τ ).\nPerforming a traditional symmetrization of the variables zτ with z̃τ would give us expressions of the form\nEsymm = `(ht−1, zt, z̃τ )− `(ht−1, z̃t, zτ ).\nAt this point the analysis hits a barrier since unlike first order situations, we cannot relate Esymm to Eorig by means of introducing Rademacher variables.\nWe circumvent this problem by using a technique that we call Symmetrization of Expectations. The technique allows us to use standard symmetrization to obtain Rademacher complexities. More specifically, we analyze expressions of the form\nE′orig = E z J`(ht−1, z, zτ )K− E z J`(ht−1, z, z̃τ )K\nwhich upon symmetrization yield expressions such as\nE′symm = E z J`(ht−1, z, z̃τ )K− E z J`(ht−1, z, zτ )K\nwhich allow us to introduce Rademacher variables since E′symm = −E′orig. This idea is exploited by the\nlemma given below that relates the expected risk of the ensemble to the penalties incurred during the online learning process. In the following we use the following extension of Rademacher averages (Kakade et al., 2008) to bivariate function classes:\nRn(H) = E t sup h∈H 1 n n∑ τ=1 τh(z, zτ ) |\nwhere the expectation is over τ , z and zτ . We shall denote composite function classes as follows : ` ◦H := {(z, z′) 7→ `(h, z, z′), h ∈ H}. Lemma 1. Let h1, . . . , hn−1 be an ensemble of hypotheses generated by an online learning algorithm working with a bounded loss function ` : H×Z ×Z → [0, B]. Then for any δ > 0, we have with probability at least 1− δ,\n1\nn− 1 n∑ t=2 L(ht−1) ≤ 1 n− 1 n∑ t=2 L̂t(ht−1)\n+ 2\nn− 1 n∑ t=2 Rt−1(` ◦ H) + 3B √ log nδ n− 1 .\nThe proof of the lemma involves decomposing the excess risk term into a martingale difference sequence and a residual term in a manner similar to (Wang et al., 2012). The martingale sequence, being a bounded one, is shown to converge using the AzumaHoeffding inequality. The residual term is handled using uniform convergence techniques involving Rademacher averages. The complete proof of the lemma is given in the Appendix A.\nSimilar to Lemma 1, the following converse relation between the population and empirical risk of the population risk minimizer h∗ can also be shown.\nLemma 2. For any δ > 0, we have with probability at least 1− δ,\n1\nn− 1 n∑ t=2 L̂t(h∗) ≤ L(h∗) + 2 n− 1 n∑ t=2 Rt−1(` ◦ H)\n+3B √ log 1δ n− 1 .\nAn online learning algorithm will be said to have an all-pairs regret bound Rn if it presents an ensemble h1, . . . , hn−1 such that\nn∑ t=2 L̂t(ht−1) ≤ inf h∈H n∑ t=2 L̂t(h) + Rn.\nSuppose we have an online learning algorithm with a regret bound Rn. Then combining Lemmata 1 and\n2 gives us the following online to batch conversion bound:\nTheorem 3. Let h1, . . . , hn−1 be an ensemble of hypotheses generated by an online learning algorithm working with a B-bounded loss function ` that guarantees a regret bound of Rn. Then for any δ > 0, we have with probability at least 1− δ,\n1\nn− 1 n∑ t=2 L(ht−1) ≤ L(h∗) + 4 n− 1 n∑ t=2 Rt−1(` ◦ H)\n+ Rn n− 1 + 6B √ log nδ n− 1 .\nAs we shall see in Section 6, for several learning problems, the Rademacher complexities behave as Rt−1(`◦ H) ≤ Cd · O ( 1√ t−1 ) where Cd is a constant dependent only on the dimension d of the input space and the O (·) notation hides constants dependent on the domain size and the loss function. This allows us to bound the excess risk as follows:∑n\nt=2 L(ht−1) n− 1 ≤ L(h ∗) + Rn n− 1 +O\n( Cd + √ log(n/δ)√ n− 1 ) .\nHere, the error decreases with n at a standard 1/ √ n rate (up to a √\nlog n factor), similar to that obtained by Wang et al. (2012). However, for several problems the above bound can be significantly tighter than those offered by covering number based arguments. We provide below a detailed comparison of our results with those of Wang et al. (2012)."
    }, {
      "heading" : "3.1. Discussion on the nature of our bounds",
      "text" : "As mentioned above, our proof enables us to use Rademacher complexities which are typically easier to analyze and provide tighter bounds (Kakade et al., 2008). In particular, as shown in Section 6, for L2 regularized learning formulations, the Rademacher complexities are dimension independent i.e. Cd = 1. Consequently, unlike the bounds of (Wang et al., 2012) that have a linear dependence on d, our bound becomes independent of the input space dimension. For sparse learning formulations with L1 or trace norm regularization, we have Cd = √ log d giving us a mild dependence on the input dimensionality.\nOur bounds are also tighter that those of (Wang et al., 2012) in general. Whereas we provide a confidence bound of δ < exp ( −n 2 + log n ) , (Wang et al., 2012)\noffer a weaker bound δ < (1/ )d exp ( −n 2 + log n ) .\nAn artifact of the proof technique of (Wang et al., 2012) is that their proof is required to exclude a constant fraction of the ensemble (h1, . . . , hcn) from the\nanalysis, failing which their bounds turn vacuous. Our proof on the other hand is able to give guarantees for the entire ensemble.\nIn addition to this, as the following sections show, our proof technique enjoys the flexibility of being extendable to give fast convergence guarantees for strongly convex loss functions as well as being able to accommodate learning algorithms that use finite buffers."
    }, {
      "heading" : "4. Fast Convergence Rates for Strongly Convex Loss Functions",
      "text" : "In this section we extend results of the previous section to give fast convergence guarantees for online learning algorithms that use strongly convex loss functions of the following form: `(h, z, z′) = g(〈h, φ(z, z′)〉) + r(h), where g is a convex function and r(h) is a σ-strongly convex regularizer (see Section 6 for examples) i.e. ∀h1, h2 ∈ H and α ∈ [0, 1], we have\nr(αh1 + (1− α)h2) ≤ αr(h1) + (1− α)r(h2)\n− σ 2 α(1− α) ‖h1 − h2‖2 .\nFor any norm ‖·‖, let ‖·‖∗ denote its dual norm. Our analysis reduces the pairwise problem to a first order problem and a martingale convergence problem. We require the following fast convergence bound in the standard first order batch learning setting:\nTheorem 4. Let F be a closed and convex set of functions over X . Let ℘(f,x) = p(〈f, φ(x)〉) + r(f), for a σ-strongly convex function r, be a loss function with P and P̂ as the associated population and empirical risk functionals and f∗ as the population risk minimizer. Suppose ℘ is L-Lipschitz and ‖φ(x)‖∗ ≤ R,∀x ∈ X . Then w.p. 1− δ, for any > 0, we have for all f ∈ F ,\nP(f)− P(f∗) ≤ (1 + ) ( P̂(f)− P̂(f∗) ) +\nCδ σn\nwhere Cδ = C 2 d · (4(1 + )LR)2 (32 + log(1/δ)) and Cd is the dependence of the Rademacher complexity of the class F on the input dimensionality d.\nThe above theorem is a minor modification of a similar result by Sridharan et al. (2008) and the proof (given in Appendix B) closely follows their proof as well. We can now state our online to batch conversion result for strongly convex loss functions.\nTheorem 5. Let h1, . . . , hn−1 be an ensemble of hypotheses generated by an online learning algorithm working with a B-bounded, L-Lipschitz and σ-strongly convex loss function `. Further suppose the learning algorithm guarantees a regret bound of Rn. Let Vn =\nmax { Rn, 2C 2 d log n log(n/δ) } Then for any δ > 0, we have with probability at least 1− δ,\n1\nn− 1 n∑ t=2 L(ht−1) ≤ L(h∗) + Rn n− 1\n+Cd · O\n(√ Vn log n log(n/δ)\nn− 1\n) ,\nwhere the O (·) notation hides constants dependent on domain size and the loss function such as L,B and σ.\nThe decomposition of the excess risk in this case is not made explicitly but rather emerges as a side-effect of the proof progression. The proof starts off by applying Theorem 4 to the hypothesis in each round with the following loss function ℘(h, z′) := E\nz J`(h, z, z′)K.\nApplying the regret bound to the resulting expression gives us a martingale difference sequence which we then bound using Bernstein-style inequalities and a proof technique from (Kakade & Tewari, 2008). The complete proof is given in Appendix C.\nWe now note some properties of this result. The effective dependence of the above bound on the input dimensionality is C2d since the expression √ Vn hides a Cd term. We have C 2 d = 1 for non sparse learning formulations and C2d = log d for sparse learning formulations. We note that our bound matches that of Kakade & Tewari (2008) (for first-order learning problems) up to a logarithmic factor."
    }, {
      "heading" : "5. Analyzing Online Learning Algorithms that use Finite Buffers",
      "text" : "In this section, we present our online to batch conversion bounds for algorithms that work with finitebuffer loss functions L̂buft . Recall that an online learning algorithm working with finite buffers incurs a loss L̂buft (h) = 1|Bt| ∑ z∈Bt `(ht−1, zt, z) at each step where Bt is the state of the buffer at time t.\nAn online learning algorithm will be said to have a finite-buffer regret bound Rbufn if it presents an ensemble h1, . . . , hn−1 such that\nn∑ t=2 L̂buft (ht−1)− inf h∈H n∑ t=2 L̂buft (h) ≤ Rbufn .\nFor our guarantees to hold, we require the buffer update policy used by the learning algorithm to be stream oblivious. More specifically, we require the buffer update rule to decide upon the inclusion of a particular point zi in the buffer based only on its stream index i ∈ [n]. Popular examples of stream oblivious policies include Reservoir sampling (Vitter, 1985) (referred to\nas RS henceforth) and FIFO. Stream oblivious policies allow us to decouple buffer construction randomness from training sample randomness which makes analysis easier; we leave the analysis of stream aware buffer update policies as a topic of future research.\nIn the above mentioned setting, we can prove the following online to batch conversion bounds:\nTheorem 6. Let h1, . . . , hn−1 be an ensemble of hypotheses generated by an online learning algorithm working with a finite buffer of capacity s and a Bbounded loss function `. Moreover, suppose that the algorithm guarantees a regret bound of Rbufn . Then for any δ > 0, we have with probability at least 1− δ,∑n\nt=2 L(ht−1) n− 1 ≤ L(h∗) + R buf n n− 1 +O ( Cd√ s +B √ log nδ s ) If the loss function is Lipschitz and strongly convex as well, then with the same confidence, we have∑n\nt=2 L(ht−1) n− 1 ≤ L(h∗) + R buf n n− 1 + Cd · O\n(√ Wn log n δ\nsn\n)\nwhere Wn = max { Rbufn , 2C2dn log(n/δ) s } and Cd is the dependence of Rn(H) on the input dimensionality d.\nThe above bound guarantees an excess error of Õ (1/s) for algorithms (such as Follow-the-leader (Hazan et al., 2006)) that offer logarithmic regret Rbufn = O (log n). We stress that this theorem is not a direct corollary of our results for the infinite buffer case (Theorems 3 and 5). Instead, our proofs require a more careful analysis of the excess risk in order to accommodate the finiteness of the buffer and the randomness (possibly) used in constructing it.\nMore specifically, care needs to be taken to handle randomized buffer update policies such as RS which introduce additional randomness into the analysis. A naive application of techniques used to prove results for the unbounded buffer case would result in bounds that give non trivial generalization guarantees only for large buffer sizes such as s = ω( √ n). Our bounds, on the other hand, only require s = ω̃(1).\nKey to our proofs is a conditioning step where we first analyze the conditional excess risk by conditioning upon randomness used by the buffer update policy. Such conditioning is made possible by the stream-oblivious nature of the update policy and thus, stream-obliviousness is required by our analysis. Subsequently, we analyze the excess risk by taking expectations over randomness used by the buffer update policy. The complete proofs of both parts of Theorem 6 are given in Appendix D.\nNote that the above results only require an online learning algorithm to provide regret bounds w.r.t. the finite-buffer penalties L̂buft and do not require any regret bounds w.r.t the all-pairs penalties L̂t.\nFor instance, the finite buffer based online learning algorithms OAMseq and OAMgra proposed in (Zhao et al., 2011) are able to provide a regret bound w.r.t. L̂buft (Zhao et al., 2011, Lemma 2) but are not able to do so w.r.t the all-pairs loss function (see Section 7 for a discussion). Using Theorem 6, we are able to give a generalization bound for OAMseq and OAMgra and hence explain the good empirical performance of these algorithms as reported in (Zhao et al., 2011). Note that Wang et al. (2013) are not able to analyze OAMseq and OAMgra since their analysis is restricted to algorithms that use the (deterministic) FIFO update policy whereas OAMseq and OAMgra use the (randomized) RS policy of Vitter (1985)."
    }, {
      "heading" : "6. Applications",
      "text" : "In this section we make explicit our online to batch conversion bounds for several learning scenarios and also demonstrate their dependence on input dimensionality by calculating their respective Rademacher complexities. Recall that our definition of Rademacher complexity for a pairwise function class is given by,\nRn(H) = E t sup h∈H 1 n n∑ τ=1 τh(z, zτ ) | .\nFor our purposes, we would be interested in the Rademacher complexities of composition classes of the form ` ◦ H := {(z, z′) 7→ `(h, z, z′), h ∈ H} where ` is some Lipschitz loss function. Frequently we have `(h, z, z′) = φ (h(x,x′)Y (y, y′)) where Y (y, y′) = y−y′ or Y (y, y′) = yy′ and φ : R → R is some margin loss function (Steinwart & Christmann, 2008). Suppose φ is L-Lipschitz and Y = sup\ny,y′∈Y |Y (y, y′)|. Then we have\nTheorem 7. Rn(` ◦ H) ≤ LYRn(H).\nThe proof uses standard contraction inequalities and is given in Appendix E. This reduces our task to computing the values of Rn(H) which we do using a two stage proof technique (see Appendix F). For any subset X of a Banach space and any norm ‖·‖p, we define ‖X‖p := sup\nx∈X ‖x‖p. Let the domain X ⊂ Rd.\nAUC maximization (Zhao et al., 2011): the goal here is to maximize the area under the ROC curve for a linear classification problem where the hypothesis space W ⊂ Rd. We have hw(x,x′) = w>x−w>x′ and `(hw, z, z ′) = φ ((y − y′)hw(x,x′)) where φ is the\nhinge loss. In case our classifiers are Lp regularized for p > 1, we can show thatRn(W) ≤ 2 ‖X‖q ‖W‖p √ q−1 n where q = p/(p− 1). Using the sparsity promoting L1 regularizer gives us Rn(W) ≤ 2 ‖X‖∞ ‖W‖1 √ e log d n . Note that we obtain dimension independence, for example when the classifiers are L2 regularized which allows us to bound the Rademacher complexities of kernelized function classes for bounded kernels as well.\nMetric learning (Jin et al., 2009): the goal here is to learn a Mahalanobis metric MW(x,x\n′) = (x − x′)>W(x − x′) using the loss function `(W, z, z′) = φ ( yy′ ( 1−M2W(x,x′) )) for a hypothesis class W ⊂ Rd×d. In this case it is possible to use a variety of mixed norm ‖·‖p,q and Schatten norm ‖·‖S(p) regularizations on matrices in the hypothesis class. In case we use trace norm regularization on the ma-\ntrix class, we get Rn(W) ≤ ‖X‖22 ‖W‖S(1) √ e log d n . The (2, 2)-norm regularization offers a dimension in-\ndependent bound Rn(W) ≤ ‖X‖22 ‖W‖2,2 √ 1 n . The mixed (2, 1)-norm regularization offers Rn(W) ≤ ‖X‖2 ‖X‖∞ ‖W‖2,1 √ e log d n .\nMultiple kernel learning (Kumar et al., 2012): the goal here is to improve the SVM classification algorithm by learning a good kernel K that is a positive combination of base kernels K1, . . . ,Kp i.e. Kµ(x,x ′) = ∑p i=1 µiKi(x,x\n′) for some µ ∈ Rp,µ ≥ 0. The base kernels are bounded, i.e. for all i, |Ki(x,x′)| ≤ κ2 for all x,x′ ∈ X The notion of goodness used here is the one proposed by Balcan & Blum (2006) and involves using the loss function `(µ, z, z′) = φ (yy′Kµ(x,x\n′)) where φ(·) is a margin loss function meant to encode some notion of alignment. The two hypothesis classes for the combination vector µ that we study are the L1 regularized unit simplex ∆(1) = {µ : ‖µ‖1 = 1,µ ≥ 0} and the L2 regularized unit sphere S2(1) = {µ : ‖µ‖2 = 1,µ ≥ 0}. We are able to show the following Rademacher complexity bounds for these classes: Rn(S2(1)) ≤ κ2 √ p n\nand Rn(∆(1)) ≤ κ2 √ e log p n .\nThe details of the Rademacher complexity derivations for these problems and other examples such as similarity learning can be found in Appendix F."
    }, {
      "heading" : "7. OLP : Online Learning with Pairwise Loss Functions",
      "text" : "In this section, we present an online learning algorithm for learning with pairwise loss functions in a finite buffer setting. The key contribution in this section\nAlgorithm 1 RS-x : Stream Subsampling with Replacement Input: Buffer B, new point zt, buffer size s, timestep t. 1: if |B| < s then //There is space 2: B ← B ∪ {zt} 3: else //Overflow situation 4: if t = s+ 1 then //Repopulation step 5: TMP← B ∪ {zt} 6: Repopulate B with s points sampled uniformly\nwith replacement from TMP. 7: else //Normal update step 8: Independently, replace each point of B with zt with probability 1/t. 9: end if\n10: end if\nAlgorithm 2 OLP : Online Learning with Pairwise Loss Functions Input: Step length scale η, Buffer size s Output: An ensemble w2, . . . ,wn ∈ W with low regret 1: w0 ← 0, B ← φ 2: for t = 1 to n do 3: Obtain a training point zt 4: Set step length ηt ← η√t 5: wt ← ΠW [ wt−1 + ηt |B| ∑ z∈B ∇w`(wt−1, zt, z) ]\n//ΠW projects onto the set W 6: B ← Update-buffer(B, zt, s, t) //using RS-x 7: end for 8: return w2, . . . ,wn\nis a buffer update policy that when combined with a variant of the GIGA algorithm (Zinkevich, 2003) allows us to give high probability regret bounds.\nIn previous work, Zhao et al. (2011) presented an online learning algorithm that uses finite buffers with the RS policy and proposed an all-pairs regret bound. The RS policy ensures, over the randomness used in buffer updates, that at any given time, the buffer contains a uniform sample from the preceding stream. Using this property, (Zhao et al., 2011, Lemma 2) claimed that E r L̂buft (ht−1) z = L̂t(ht−1) where the expectation is taken over the randomness used in buffer construction. However, a property such as E r L̂buft (h) z =\nL̂t(h) holds only for functions h that are either fixed or obtained independently of the random variables used in buffer updates (over which the expectation is taken). Since ht−1 is learned from points in the buffer itself, the above property, and consequently the regret bound, does not hold.\nWe remedy this issue by showing a relatively weaker claim; we show that with high probability we have L̂t(ht−1) ≤ L̂buft (ht−1) + . At a high level, this claim is similar to showing uniform convergence bounds for L̂buft . However, the reservoir sampling algorithm is not particularly well suited to prove such uniform conver-\ngence bounds as it essentially performs sampling without replacement (see Appendix G for a discussion). We overcome this hurdle by proposing a new buffer update policy RS-x (see Algorithm 1) that, at each time step, guarantees s i.i.d. samples from the preceding stream (see Appendix H for a proof).\nOur algorithm uses this buffer update policy in conjunction with an online learning algorithm OLP (see Algorithm 2) that is a variant of the well-known GIGA algorithm (Zinkevich, 2003). We provide the following all-pairs regret guarantee for our algorithm:\nTheorem 8. Suppose the OLP algorithm working with an s-sized buffer generates an ensemble w1, . . . ,wn−1. Then with probability at least 1− δ,\nRn n− 1 ≤ O\n( Cd √ log nδ s + √ 1 n− 1 )\nSee Appendix I for the proof. A drawback of our bound is that it offers sublinear regret only for buffer sizes s = ω(log n). A better regret bound for constant s or a lower-bound on the regret is an open problem."
    }, {
      "heading" : "8. Experimental Evaluation",
      "text" : "In this section we present experimental evaluation of our proposed OLP algorithm. We stress that the aim of this evaluation is to show that our algorithm, that enjoys high confidence regret bounds, also performs competitively in practice with respect to the OAMgra algorithm proposed by Zhao et al. (2011) since our results in Section 5 show that OAMgra does enjoy good\ngeneralization guarantees despite the lack of an allpairs regret bound.\nIn our experiments, we adapted the OLP algorithm to the AUC maximization problem and compared it with OAMgra on 18 different benchmark datasets. We used 60% of the available data points up to a maximum of 20000 points to train both algorithms. We refer the reader to Appendix J for a discussion on the implementation of the RS-x algorithm. Figure 1 presents the results of our experiments on 4 datasets across 5 random training/test splits. Results on other datasets can be found in Appendix K. The results demonstrate that OLP performs competitively to OAMgra while in some cases having slightly better performance for small buffer sizes."
    }, {
      "heading" : "9. Conclusion",
      "text" : "In this paper we studied the generalization capabilities of online learning algorithms for pairwise loss functions from several different perspectives. Using the method of Symmetrization of Expectations, we first provided sharp online to batch conversion bounds for algorithms that offer all-pairs regret bounds. Our results for bounded and strongly convex loss functions closely match their first order counterparts. We also extended our analysis to algorithms that are only able to provide finite-buffer regret bounds using which we were able to explain the good empirical performance of some existing algorithms. Finally we presented a new memory-efficient online learning algorithm that is able to provide all-pairs regret bounds in addition to performing well empirically.\nSeveral interesting directions can be pursued for future work, foremost being the development of online learning algorithms that can guarantee sub-linear regret at constant buffer sizes or else a regret lower bound for finite buffer algorithms. Secondly, the idea of a stream-aware buffer update policy is especially interesting both from an empirical as well as theoretical point of view and would possibly require novel proof techniques for its analysis. Lastly, scalability issues that arise when working with higher order loss functions also pose an interesting challenge."
    }, {
      "heading" : "Acknowledgment",
      "text" : "The authors thank the anonymous referees for comments that improved the presentation of the paper. PK is supported by the Microsoft Corporation and Microsoft Research India under a Microsoft Research India Ph.D. fellowship award."
    }, {
      "heading" : "A. Proof of Lemma 1",
      "text" : "Lemma 9 (Lemma 1 restated). Let h1, . . . , hn−1 be an ensemble of hypotheses generated by an online learning algorithm working with a bounded loss function ` : H× Z × Z → [0, B]. Then for any δ > 0, we have with probability at least 1− δ,\n1\nn− 1 n∑ t=2 L(ht−1) ≤ 1 n− 1 n∑ t=2 L̂t(ht−1)\n+ 2\nn− 1 n∑ t=2 Rt−1(` ◦ H) + 3B √ log nδ n− 1 .\nProof. As a first step, we decompose the excess risk in a manner similar to (Wang et al., 2012). For any h ∈ H let\nL̃t(h) := E zt\nr L̂t(h) ∣∣∣Zt−1z . This allows us to decompose the excess risk as follows:\n1\nn− 1 n∑ t=2 L(ht−1)− L̂t(ht−1)\n= 1\nn− 1  n∑ t=2 L(ht−1)− L̃t(ht−1)︸ ︷︷ ︸ Pt + L̃(ht−1)− L̂t(ht−1)︸ ︷︷ ︸ Qt  . By construction, we have E\nzt\nq Qt|Zt−1 y = 0 and hence\nthe sequence Q2, . . . , Qn forms a martingale difference sequence. Since |Qt| ≤ B as the loss function is bounded, an application of the Azuma-Hoeffding inequality shows that with probability at least 1− δ\n1\nn− 1 n∑ t=2 Qt ≤ B √ 2 log 1δ n− 1 . (4)\nWe now analyze each term Pt individually. By linearity of expectation, we have for a ghost sample Z̃t−1 = {z̃1, . . . , z̃t−1},\nL(ht−1) = E Z̃t−1\nt 1\nt− 1 t−1∑ τ=1 E z J`(ht−1, z, z̃τ )K | . (5)\nThe expression of L(ht−1) as a nested expectation is the precursor to performing symmetrization with expectations and plays a crucial role in overcoming coupling problems. This allows us to write Pt as\nPt = E Z̃t−1\nt 1\nt− 1 t−1∑ τ=1 E z J`(ht−1, z, z̃τ )K | − L̃t(ht−1)\n≤ sup h∈H\n[ E\nZ̃t−1\nt 1\nt− 1 t−1∑ τ=1 E z J`(h, z, z̃τ )K | − L̃t(h) ] ︸ ︷︷ ︸\ngt(z1,...,zt−1)\n.\nSince L̃t(h) = E z r 1 t−1 ∑t−1 τ=1 `(h, z, zτ ) ∣∣∣Zt−1z and ` is bounded, the expression gt(z1, . . . , zt−1) can have a variation of at most B/(t−1) when changing any of its (t−1) variables. Hence an application of McDiarmid’s inequality gives us, with probability at least 1− δ,\ngt(z1, . . . , zt−1) ≤ E Zt−1 Jgt(z1, . . . , zt−1)K+B\n√ log 1δ\n2(t− 1) .\nFor any h ∈ H, z′ ∈ Z, let ℘(h, z′) := 1t−1Ez J`(h, z, z ′)K. Then we can write E Zt−1 Jg(z1, . . . , zt−1)K as\nE Zt−1 t sup h∈H\n[ E\nZ̃t−1 t t−1∑ τ=1 ℘(h, z̃τ ) | − t−1∑ τ=1 ℘(h, zτ )\n]|\n≤ E Zt−1,Z̃t−1 t sup h∈H [ t−1∑ τ=1 ℘(h, z̃τ )− t−1∑ τ=1 ℘(h, zτ ) ]|\n= E Zt−1,Z̃t−1,{ τ} t sup h∈H [ t−1∑ τ=1 τ (℘(h, z̃τ )− ℘(h, zτ )) ]|\n≤ 2 t− 1 E Zt−1,{ τ} t sup h∈H [ t−1∑ τ=1 τE z J`(h, z, zτ )K ]|\n≤ 2 t− 1 E z,Zt−1,{ τ} t sup h∈H [ t−1∑ τ=1 τ `(h, z, zτ ) ]| = 2Rt−1(` ◦ H).\nNote that in the third step, the symmetrization was made possible by the decoupling step in Eq. (5) where we decoupled the “head” variable zt from the “tail” variables by absorbing it inside an expectation. This allowed us to symmetrize the true and ghost samples zτ and z̃τ in a standard manner. Thus we have, with probability at least 1− δ,\nPt ≤ 2Rt−1(` ◦ H) +B\n√ log 1δ\n2(t− 1) .\nApplying a union bound on the bounds for Pt, t = 2, . . . , n gives us with probability at least 1− δ,\n1\nn− 1 n∑ t=2 Pt ≤ 2 n− 1 n∑ t=2 Rt−1(` ◦ H) +B √ 2 log nδ n− 1 .\n(6)\nAdding Equations (4) and (6) gives us the result."
    }, {
      "heading" : "B. Proof of Theorem 4",
      "text" : "Theorem 10 (Theorem 4 restated). Let F be a closed and convex set of functions over X . Let ℘(f,x) = p(〈f, φ(x)〉) + r(f), for a σ-strongly convex function\nr, be a loss function with P and P̂ as the associated population and empirical risk functionals and f∗ as the population risk minimizer. Suppose ℘ is L-Lipschitz and ‖φ(x)‖∗ ≤ R,∀x ∈ X . Then w.p. 1 − δ, for any > 0, we have for all f ∈ F ,\nP(f)− P(f∗) ≤ (1 + ) ( P̂(f)− P̂(f∗) ) +\nCδ σn\nwhere Cδ = C 2 d · (4(1 + )LR)2 (32 + log(1/δ)) and Cd is the dependence of the Rademacher complexity of the class F on the input dimensionality d.\nProof. We begin with a lemma implicit in the proof of Theorem 1 in (Sridharan et al., 2008). For the function class F and loss function ℘ as above, define a new loss function µ : (f,x) 7→ ℘(f,x) − ℘(f∗,x) with M and M̂ as the associated population and empirical risk functionals. Let r =\n4L2R2C2d(32+log(1/δ)) σn . Then we\nhave the following\nLemma 11. For any > 0, with probability at least 1− δ, the following happens\n1. For all f ∈ F such that M(f) ≤ 16 ( 1 + 1 )2 r,\nwe have M(f) ≤ M̂(f) + 4 ( 1 + 1 ) r.\n2. For all f ∈ F such that M(f) > 16 ( 1 + 1 )2 r,\nwe have M(f) ≤ (1 + )M̂(f).\nThe difference in our proof technique lies in the way we combine these two cases. We do so by proving the following two simple results.\nLemma 12. For all f s.t. M(f) ≤ 16 ( 1 + 1 )2 r, we\nhave M(f) ≤ (1 + ) ( M̂(f) + 4 ( 1 + 1 ) r ) .\nProof. We notice that for all f ∈ F , we have M(f) = P(f)−P(f∗) ≥ 0. Thus, using Lemma 11, Part 1, we have M̂(f) + 4 ( 1 + 1 ) r ≥ M(f) ≥ 0. Since for any a, > 0, we have a ≤ (1 + )a, the result follows.\nLemma 13. For all f s.t. M(f) > 16 ( 1 + 1 )2 r, we\nhave M(f) ≤ (1 + ) ( M̂(f) + 4 ( 1 + 1 ) r ) .\nProof. We use the fact that r > 0 and thus 4(1 + ) ( 1 + 1 ) r > 0 as well. The result then follows from an application of Part 2 of Lemma 11.\nFrom the definition of the loss function µ, we have for any f ∈ F , M(f) = P(f) − P(f∗) and M̂(f) = P̂(f) − P̂(f∗). Combining the above lemmata with this observation completes the proof."
    }, {
      "heading" : "C. Proof of Theorem 5",
      "text" : "Theorem 14 (Theorem 5 restated). Let h1, . . . , hn−1 be an ensemble of hypotheses generated by an online learning algorithm working with a B-bounded, LLipschitz and σ-strongly convex loss function `. Further suppose the learning algorithm guarantees a regret bound of Rn. Let Vn = max { Rn, 2C 2 d log n log(n/δ)\n} Then for any δ > 0, we have with probability at least 1− δ,\n1\nn− 1 n∑ t=2 L(ht−1) ≤ L(h∗) + Rn n− 1\n+Cd · O\n(√ Vn log n log(n/δ)\nn− 1\n) ,\nwhere the O (·) notation hides constants dependent on domain size and the loss function such as L,B and σ.\nProof. The decomposition of the excess risk shall not be made explicitly in this case but shall emerge as a side-effect of the proof progression. Consider the loss function ℘(h, z′) := E\nz J`(h, z, z′)K with P and P̂ as\nthe associated population and empirical risk functionals. Clearly, if ` is L-Lipschitz and σ-strongly convex then so is ℘. As Equation (5) shows, for any h ∈ H, P(h) = L(h). Also it is easy to see that for any Zt−1, P̂(h) = L̃t(h). Applying Theorem 4 on ht−1 with the loss function ℘ gives us w.p. 1− δ,\nL(ht−1)− L(h∗) ≤ (1 + ) ( L̃t(ht−1)− L̃t(h∗) ) +\nCδ σ(t− 1)\nwhich, upon summing across time steps and taking a union bound, gives us with probability at least 1− δ,\n1\nn− 1 n∑ t=2 L(ht−1) ≤ L(h∗) + C(δ/n) logn σ(n− 1)\n+ 1 +\nn− 1 n∑ t=2 ( L̃t(ht−1)− L̃t(h∗) ) .\nLet ξt := ( L̃t(ht−1)− L̃t(h∗) ) − ( L̂t(ht−1)− L̂t(h∗) ) .\nThen using the regret bound Rn we can write,\n1\nn− 1 n∑ t=2 L(ht−1) ≤ L(h∗) + 1 + n− 1\n( Rn +\nn∑ t=2 ξt\n)\n+ C(δ/n) logn\nσ(n− 1) .\nWe now use Bernstein type inequalities to bound the sum ∑n t=2 ξt using a proof technique used in (Kakade & Tewari, 2008; Cesa-Bianchi & Gentile, 2008). We first note some properties of the sequence below.\nLemma 15. The sequence ξ2, . . . , ξn is a bounded martingale difference sequence with bounded conditional variance.\nProof. That ξt is a martingale difference sequence follows by construction: we can decompose the term ξt = φt − ψt where φt = L̃t(ht−1) − L̂t(ht−1) and ψt = L̃t(h∗) − L̂t(h∗), both of which are martingale difference sequences with respect to the common filtration F = {Fn : n = 0, 1, . . .} where Fn = σ (zi : i = 1, . . . , n).\nSince the loss function takes values in [0, B], we have |ξt| ≤ 2B which proves that our sequence is bounded.\nTo prove variance bounds for the sequence, we first use the Lipschitz properties of the loss function to get\nξt = ( L̃t(ht−1)− L̃t(h∗) ) − ( L̂t(ht−1)− L̂t(h∗) ) ≤ 2L ‖ht−1 − h∗‖ .\nRecall that the hypothesis space is embedded in a Banach space equipped with the norm ‖·‖. Thus we have E q ξ2t ∣∣Zt−1y ≤ 4L2 ‖ht−1 − h∗‖2. Now using σ-strong convexity of the loss function we have\nL(ht−1) + L(h∗) 2\n≥ L ( ht−1 + h ∗\n2\n) + σ\n8 ‖ht−1 − h∗‖2\n≥ L(h∗) + σ 8 ‖ht−1 − h∗‖2 .\nLet σ2t := 16L2 σ (L(ht−1)− L(h ∗)). Combining the two inequalities we get E q ξ2t ∣∣Zt−1y ≤ σ2t .\nWe note that although (Kakade & Tewari, 2008) state their result with a requirement that the loss function be strongly convex in a point wise manner, i.e., for all z, z′ ∈ Z, the function `(h, z, z′) be strongly convex in h, they only require the result in expectation. More specifically, our notion of strong convexity where we require the population risk functional L(h) to be strongly convex actually suits the proof of (Kakade & Tewari, 2008) as well.\nWe now use a Bernstein type inequality for martingales proved in (Kakade & Tewari, 2008). The proof is based on a fundamental result on martingale convergence due to Freedman (1975).\nTheorem 16. Given a martingale difference sequence Xt, t = 1 . . . n that is uniformly B-bounded and has conditional variance E q X2t |X1, . . . , Xt−1 y ≤ σ2t , we have for any δ < 1/e and n ≥ 3, with probability at least 1− δ, n∑ t=1 Xt ≤ max { 2σ∗, 3B √ log 4 log n δ }√ log 4 log n δ ,\nwhere σ∗ = √∑n\nt=1 σ 2 t .\nLet Dn = ∑n t=2 (L(ht−1)− L(h∗)). Then we can write the variance bound as\nσ∗ = √√√√ n∑ t=1 σ2t = √√√√ n∑ t=1 16L2 σ (L(ht−1)− L(h∗))\n= 4L √ Dn σ .\nThus, with probability at least 1− δ, we have n∑ t=1 ξt ≤ max { 8L √ Dn σ , 6B √ log 4 log n δ }√ log 4 log n δ .\nDenoting ∆ = √\nlog 4 lognδ for notational simplicity\nand using the above bound in the online to batch conversion bound gives us\nDn n− 1 ≤ 1 + n− 1\n( Rn + max { 8L √ Dn σ , 6B∆ } ∆ )\n+ C(δ/n) log n\nσ(n− 1) .\nSolving this quadratic inequality is simplified by a useful result given in (Kakade & Tewari, 2008, Lemma 4)\nLemma 17. For any s, r, d, b,∆ > 0 such that s ≤ r + max { 4 √ ds, 6b∆ } ∆,\nwe also have\ns ≤ r + 4 √ dr∆ + max {16d, 6b}∆2.\nUsing this result gives us a rather nasty looking expression which we simplify by absorbing constants inside the O (·) notation. We also make a simplifying adhoc assumption that we shall only set ∈ (0, 1]. The resulting expression is given below:\nDn ≤ (1 + )Rn +O ( C2d log n log(n/δ) + log log n\nδ ) +O (√( Rn + C2d log n log(n/δ) ) log log n\nδ\n) .\nLet Vn = max { Rn, 2C 2 d log n log (n/δ) } . Concentrating only on the portion of the expression involving and ignoring the constants, we get\nRn + C2d log n log(n/δ) +\n√ C2d log n log(n/δ)\nlog\nlog n\nδ\n≤ Rn + 2C2d log n log(n/δ) ≤ Vn + 2C2d log n log(n/δ)\n≤ 2Cd √ 2Vn log n log(n/δ),\nwhere the second step follows since ≤ 1 and the fourth step follows by using = √ 2C2d logn log(n/δ)\nVn ≤ 1.\nPutting this into the excess risk expression gives us\n1\nn− 1 n∑ t=2 L(ht−1) ≤ L(h∗) + Rn n− 1\n+ Cd · O\n(√ Vn log n log(n/δ)\nn− 1 ) which finishes the proof."
    }, {
      "heading" : "D. Generalization Bounds for Finite Buffer Algorithms",
      "text" : "In this section we present online to batch conversion bounds for learning algorithms that work with finite buffers and are able to provide regret bounds Rbufn with respect to finite-buffer loss functions L̂buft .\nAlthough due to lack of space, Theorem 6 presents these bounds for bounded as well as strongly convex functions together, we prove them separately for sake of clarity. Moreover, the techniques used to prove these two results are fairly different which further motivates this. Before we begin, we present the problem setup formally and introduce necessary notation.\nIn our finite buffer online learning model, one observes a stream of elements z1, . . . , zn. A sketch of these elements is maintained in a buffer B of size s, i.e., at each step t = 2, . . . , n, the buffer contains a subset of the elements Zt−1 of size at most s. At each step t = 2 . . . n, the online learning algorithm posits a hypothesis ht−1 ∈ H, upon which the element zt is revealed and the algorithm incurs the loss\nL̂buft (ht−1) = 1 |Bt| ∑ z∈Bt `(ht−1, zt, z),\nwhere Bt is the state of the buffer at time t. Note that |Bt| ≤ s. We would be interested in algorithms that are able to give a finite-buffer regret bound, i.e., for which, the proposed ensemble h1, . . . , hn−1 satisfies\nn∑ t=2 L̂buft (ht−1)− inf h∈H n∑ t=2 L̂buft (h) ≤ Rbufn .\nWe assume that the buffer is updated after each step in a stream-oblivious manner. For randomized buffer update policies (such as reservoir sampling (Vitter, 1985)), we assume that we are supplied at each step with some fresh randomness rt (see examples below) along with the data point zt. Thus the data received at time t is a tuple wt = (zt, rt). We shall refer to\nthe random variables rt as auxiliary variables. It is important to note that stream obliviousness dictates that rt as a random variable is independent of zt. Let W t−1 := {w1, . . . ,wt−1} and Rt−1 := {r1, . . . , rt−1}. Note that Rt−1 completely decides the indices present in the buffer Bt at step t independent of Z\nt−1. For any h ∈ H, define\nL̃buft := E zt\nr L̂buft ∣∣∣W t−1z . D.1. Examples of Stream Oblivious Policies\nBelow we give some examples of stream oblivious policies for updating the buffer:\n1. FIFO: in this policy, the data point zt arriving at time t > s is inducted into the buffer by evicting the data point z(t−s) from the buffer. Since this is a non-randomized policy, there is no need for auxiliary randomness and we can assume that rt follows the trivial law rt ∼ 1{r=1}.\n2. RS : the Reservoir Sampling policy was introduced by Vitter (1985). In this policy, at time t > s, the incoming data point zt is inducted into the buffer with probability s/t. If chosen to be induced, it results in the eviction of a random element of the buffer. In this case the auxiliary random variable is 2-tuple that follows the law\nrt = (r 1 t , r 2 t ) ∼\n( Bernoulli (s t ) , 1 s s∑ i=1 1{r2=i} ) .\n3. RS-x (see Algorithm 1): in this policy, the incoming data point zt at time t > s, replaces each data point in the buffer independently with probability 1/t. Thus the incoming point has the potential to evict multiple buffer points while establishing multiple copies of itself in the buffer. In this case, the auxiliary random variable is defined by a Bernoulli process: rt = (r 1 t , r 2 t . . . , r s t ) ∼(\nBernoulli ( 1 t ) ,Bernoulli ( 1 t ) , . . . ,Bernoulli ( 1 t )) .\n4. RS-x2 (see Algorithm 3): this is a variant of RSx in which the number of evictions is first decided by a Binomial trial and then those many random points in the buffer are replaced by the incoming data point. This can be implemented as follows: rt = (r 1 t , r 2 t ) ∼ ( Binomial ( s, 1t ) ,Perm(s)\n) where Perm(s) gives a random permutation of s elements.\nD.2. Finite Buffer Algorithms with Bounded Loss Functions\nWe shall prove the result in two steps. In the first step we shall prove the following uniform convergence style\nresult\nLemma 18. Let h1, . . . , hn−1 be an ensemble of hypotheses generated by an online learning algorithm working with a B-bounded loss function ` and a finite buffer of capacity s. Then for any δ > 0, we have with probability at least 1− δ,\n1\nn− 1 n∑ t=2 L(ht−1) ≤ 1 n− 1 n∑ t=2 L̂buft (ht−1) +B √ 2 log nδ s\n+ 2\nn− 1 n∑ t=2 Rmin{t−1,s}(` ◦ H).\nAt a high level, our proof progression shall follow that of Lemma 1. However, the execution of the proof will have to be different in order to accommodate the finiteness of the buffer and randomness used to construct it. Similarly, we shall also be able to show the following result.\nLemma 19. For any δ > 0, we have with probability at least 1− δ,\n1\nn− 1 n∑ t=2 L̂buft (h∗) ≤ L(h∗) + 3B √ log nδ s\n+ 2\nn− 1 n∑ t=2 Rmin{t−1,s}(` ◦ H).\nNote that for classes whose Rademacher averages behave as Rn(H) ≤ Cd · O (\n1√ n\n) , applying Lemma 7\ngives us Rn(`◦H) ≤ Cd ·O (\n1√ n\n) as well which allows\nus to show\n2\nn− 1 n∑ t=2 Rmin{t−1,s}(` ◦ H) = Cd · O ( 1√ s ) .\nCombining Lemmata 18 and 19 along with the definition of bounded buffer regret Rbufn gives us the first part of Theorem 6. We prove Lemma 18 below:\nProof (of Lemma 18). We first decompose the excess risk term as before\nn∑ t=2 L(ht−1)− L̂buft (ht−1)\n= n∑ t=2\nL(ht−1)− L̃buft (ht−1)︸ ︷︷ ︸ Pt + L̃buft (ht−1)− L̂buft (ht−1)︸ ︷︷ ︸ Qt .\nBy construction, the sequence Qt forms a martingale difference sequence, i.e., E\nzt\nq Qt|Zt−1 y = 0 and hence\nby an application of Azuma Hoeffding inequality we have\n1\nn− 1 n∑ t=2 Qt ≤ B √ 2 log 1δ n− 1 . (7)\nWe now analyze each term Pt individually. To simplify the analysis a bit we assume that the buffer update policy keeps admitting points into the buffer as long as there is space so that for t ≤ s+ 1, the buffer contains an exact copy of the preceding stream. This is a very natural assumption satisfied by FIFO as well as reservoir sampling. We stress that our analysis works even without this assumption but requires a bit more work. In case we do make this assumption, the analysis of Lemma 1 applies directly and we have, for any t ≤ s+ 1, with probability at least 1− δ,\nPt ≤ Rt−1(` ◦ H) +B\n√ log 1δ\n2(t− 1)\nFor t > s + 1, for an independent ghost sample {w̃1, . . . , w̃t−1} we have,\nE W̃ t−1\nr L̃buft z = E W̃ t−1 u v1 s ∑ z̃∈B̃t E z J`(ht−1, z, z̃)K } ~\n= E R̃t−1 u v E Z̃t−1 u v 1 s ∑ z̃∈B̃t E z J`(ht−1, z, z̃)K ∣∣∣∣∣∣ R̃t−1 } ~ } ~ .\nThe conditioning performed above is made possible by stream obliviousness. Now suppose that given R̃t−1 the indices τ̃1, . . . , τ̃s are present in the buffer B̃t at time t. Recall that this choice of indices is independent of Z̃t−1 because of stream obliviousness. Then we can write the above as\nE R̃t−1 u v E Z̃t−1 u v 1 s ∑ z̃∈B̃t E z J`(ht−1, z, z̃)K ∣∣∣∣∣∣ R̃t−1 } ~ } ~\n= E R̃t−1 u v E Z̃t−1 u v1 s s∑ j=1 E z q `(ht−1, z, z̃τ̃j ) y } ~ } ~\n= E R̃t−1\nu v E\nz̃1,...,z̃s u v1 s s∑ j=1 E z J`(ht−1, z, z̃j)K } ~ } ~\n= E R̃t−1 JL(ht−1)K = L(ht−1).\nWe thus have\nE W̃ t−1 u v1 s ∑ z̃∈B̃t E z J`(ht−1, z, z̃)K } ~ = L(ht−1). (8)\nWe now upper bound Pt as\nPt = L(ht−1)− L̃buft (ht−1)\n= E W̃ t−1 u v1 s ∑ z̃∈B̃t E z J`(ht−1, z, z̃)K } ~− L̃buft (ht−1)\n≤ sup h∈H  E W̃ t−1 u v1 s ∑ z̃∈B̃t E z J`(h, z, z̃)K } ~− L̃buft (h)  ︸ ︷︷ ︸\ngt(w1,...,wt−1)\n.\nNow it turns out that applying McDiarmid’s inequality to gt(w1, . . . ,wt−1) directly would yield a very loose bound. This is because of the following reason: since L̂buft (h) = 1|Bt| ∑ z∈Bt `(h, zt, z) depends only on s data points, changing any one of the (t−1) variables wi brings about a perturbation in gt of magnitude at most O (1/s). The problem is that gt is a function of (t − 1) s variables and hence a direct application of McDiarmid’s inequality would yield an excess\nerror term of √\nt log(1/δ) s2 which would in the end re-\nquire s = ω( √ n) to give any non trivial generalization bounds. In contrast, we wish to give results that would give non trivial bounds for s = ω̃(1).\nIn order to get around this problem, we need to reduce the number of variables in the statistic while applying McDiarmid’s inequality. Fortunately, we observe that gt effectively depends only on s variables, the data points that end up in the buffer at time t. This allows us to do the following. For any Rt−1, define\nδ(Rt−1) := P Zt−1\n[ gt(w1, . . . ,wt−1) > |Rt−1 ] .\nWe will first bound δ(Rt−1). This will allow us to show\nP W t−1 [gt(w1, . . . ,wt−1) > ] ≤ E Rt−1\nq δ(Rt−1) y ,\nwhere we take expectation over the distribution on Rt−1 induced by the buffer update policy. Note that since we are oblivious to the nature of the distribution over Rt−1, our proof works for any stream oblivious buffer update policy. Suppose that given Rt−1 the indices τ1, . . . , τs are present in the buffer Bt at time t. Then we have\ngt(w1, . . . ,wt−1;R t−1)\n= sup h∈H  E W̃ t−1 u v1 s ∑ z̃∈B̃t E z J`(h, z, z̃)K } ~− 1 s s∑ j=1 E z q `(h, z, zτj ) y \n=: g̃t(zτ1 , . . . , zτs).\nThe function g̃t can be perturbed at most B/s due to a change in one of zτj . Applying McDiarmid’s inequality\nto the function g̃t we get with probability at least 1−δ,\ng̃t(zτ1 , . . . , zτs) ≤ E Zt−1 Jg̃t(zτ1 , . . . , zτs)K +B\n√ log 1δ\n2s\nWe analyze E Zt−1 Jg̃t(zτ1 , . . . , zτs)K in Figure 2. In the third step in the calculations we symmetrize the true random variable zτj with the ghost random variable z̃τ̃j . This is contrasted with traditional symmetrization where we would symmetrize zi with z̃i. In our case, we let the buffer construction dictate the matching at the symmetrization step. Thus we get, with probability at least 1− δ over z1, . . . , zt−1,\ngt(w1, . . . ,wt−1;R t−1) ≤ 2Rs(` ◦ H) +B\n√ log 1δ\n2s\nwhich in turn, upon taking expectations with respect to Rt−1, gives us with probability at least 1 − δ over w1, . . . ,wt−1,\nPt = gt(w1, . . . ,wt−1) ≤ 2Rs(` ◦ H) +B\n√ log 1δ\n2s .\nApplying a union bound on the bounds for Pt, t = 2, . . . , n gives us with probability at least 1− δ,\n1\nn− 1 n∑ t=2 Pt ≤ 2 n− 1 n∑ t=2 Rmin{t−1,s}(` ◦ H)\n+B\n√ log nδ\n2s . (9)\nAdding Equations (7) and (9) gives us the result.\nD.3. Finite Buffer Algorithms with Strongly Convex Loss Functions\nIn this section we prove faster convergence bounds for algorithms that offer finite-buffer regret bounds and use strongly convex loss functions. Given the development of the method of decoupling training and auxiliary random variables in the last section, we can proceed with the proof right away.\nOur task here is to prove bounds on the following quantity\n1\nn− 1 n∑ t=2 L(ht−1)− L(h∗).\nProceeding as before, we will first prove the following result\nP Zn\n[ 1\nn− 1 n∑ t=2 L(ht−1)− L(h∗) > ∣∣∣∣∣Rn ] ≤ δ. (10)\nThis will allow us, upon taking expectations over Rn, show the following\nP Wn\n[ 1\nn− 1 n∑ t=2 L(ht−1)− L(h∗) >\n] ≤ δ,\nwhich shall complete the proof.\nIn order to prove the statement given in Equation (10), we will use Theorem 4. As we did in the case of all-pairs loss functions, consider the loss function ℘(h, z′) := E\nz J`(h, z, z′)K with P and P̂ as the as-\nsociated population and empirical risk functionals. Clearly, if ` is L-Lipschitz and σ-strongly convex then so is ℘. By linearity of expectation, for any h ∈ H, P(h) = L(h). Suppose that given Rt−1 the indices τ1, . . . , τs are present in the buffer Bt at time t. Applying Theorem 4 on ht−1 at the t\nth step with the loss function ℘ gives us that given Rt−1, with probability at least 1− δ over the choice of Zt−1,\nL(ht−1)− L(h∗) ≤ (1 + ) ( L̃buft (ht−1)− L̃buft (h∗) ) +\nCδ σ(min {s, t− 1}) ,\nwhere we have again made the simplifying (yet optional) assumption that prior to time t = s + 1, the buffer contains an exact copy of the stream. Summing across time steps and taking a union bound, gives us that given Rn, with probability at least 1− δ over the choice of Zn,\n1\nn− 1 n∑ t=2 L(ht−1) ≤ L(h∗) + C(δ/n) σ ( log 2s n− 1 + 1 s )\n+ 1 +\nn− 1 n∑ t=2 L̃buft (ht−1)− L̃buft (h∗).\nLet us define as before ξt := ( L̃buft (ht−1)− L̃buft (h∗) ) − ( L̂buft (ht−1)− L̂buft (h∗) ) .\nThen using the regret bound Rbufn we can write,\n1\nn− 1 n∑ t=2 L(ht−1) ≤ L(h∗) + 1 + n− 1 ( Rbufn + n∑ t=2 ξt )\n+ C(δ/n)\nσ\n( log 2s\nn− 1 +\n1\ns\n) .\nAssuming s < n/ log n simplifies the above expression to the following:\n1\nn− 1 n∑ t=2 L(ht−1) ≤ L(h∗) + 1 + n− 1 ( Rbufn + n∑ t=2 ξt )\n+ 2C(δ/n)\nσs .\nNote that this assumption is neither crucial to our proof nor very harsh as for s = Ω (n), we can always apply the results from the infinite-buffer setting using Theorem 5. Moving forward, by using the Bernsteinstyle inequality from (Kakade & Tewari, 2008), one can show with that probability at least 1− δ, we have n∑ t=1 ξt ≤ max { 8L √ Dn σ , 6B √ log 4 log n δ }√ log 4 log n δ ,\nwhere Dn = ∑n t=2 (L(ht−1)− L(h∗)). This gives us\nDn n− 1 ≤ 1 + n− 1\n( Rbufn + max { 8L √ Dn σ , 6B∆ } ∆ )\n+ 2C(δ/n)\nσs .\nUsing (Kakade & Tewari, 2008, Lemma 4) and absorbing constants inside the O (·) notation we get:\nDn ≤ (1 + )Rbufn +O ( C2dn log(n/δ)\ns + log\nlog n\nδ ) +O (√( Rbufn + C2dn log(n/δ)\ns\n) log log n\nδ\n) .\nLet Wn = max { Rbufn , 2C2dn log(n/δ) s } . Concentrating only on the portion of the expression involving and ignoring the constants, we get\nRbufn + C2dn log(n/δ)\ns +\n√ C2dn log(n/δ)\ns log\nlog n\nδ\n≤ Rbufn + 2C2dn log(n/δ)\ns ≤ Wn +\n2C2dn log(n/δ)\ns\n≤ 2Cd\n√ 2Wnn log(n/δ)\ns ,\nwhere the second step follows since ≤ 1 and s ≤ n and the fourth step follows by using =√\n2C2dn log(n/δ)\nWns ≤ 1 Putting this into the excess risk\nexpression gives us\n1\nn− 1 n∑ t=2 L(ht−1) ≤ L(h∗) + Rbufn n− 1\n+ Cd · O\n(√ Wn log(n/δ)\nsn\n) ,\nwhich finishes the proof. Note that in case Wn = R buf n , we get\n1\nn− 1 n∑ t=2 L(ht−1) ≤ L(h∗) + Rbufn n− 1\n+ Cd · O\n(√ Rbufn log(n/δ)\nsn\n) .\nOn the other hand if Rbufn ≤ 2C2dn log(n/δ) s , we get\n1\nn− 1 n∑ t=2 L(ht−1) ≤ L(h∗) + Rbufn n− 1\n+ C2d · O ( log(n/δ)\ns\n) ."
    }, {
      "heading" : "E. Proof of Theorem 7",
      "text" : "Recall that we are considering a composition classes of the form ` ◦H := {(z, z′) 7→ `(h, z, z′), h ∈ H} where ` is some Lipschitz loss function. We have `(h, z1, z2) = φ (h(x1, x2)Y (y1, y2)) where Y (y1, y2) = y1 − y2 or\nY (y1, y2) = y1y2 and φ : R → R involves some margin loss function. We also assume that φ is point wise L-Lipschitz. Let Y = sup\ny1,y2∈Y |Y (y1, y2)|.\nTheorem 20 (Theorem 7 restated).\nRn(` ◦ H) ≤ LYRn(H)\nProof. Let φ̃(x) = φ(x)−φ(0). Note that φ̃(·) is point wise L-Lipschitz as well as satisfies φ̃(0) = 0. Let Y = sup\ny,y′∈Y |Y (y, y′)|.\nWe will require the following contraction lemma that we state below.\nTheorem 21 (Implicit in proof of (Ledoux & Talagrand, 2002), Theorem 4.12). Let H be a set of bounded real valued functions from some domain X and let x1, . . . ,xn be arbitrary elements from X . Furthermore, let φi : R → R, i = 1, . . . , n be L-Lipschitz functions such that φi(0) = 0 for all i. Then we have\nE t sup h∈H 1 n n∑ i=1 iφi(h(xi)) | ≤ LE t sup h∈H 1 n n∑ i=1 ih(xi) | .\nUsing the above inequality we can state the following chain of (in)equalities:\nRn(` ◦ H) = E t sup h∈H 1 n n∑ i=1 i`(h, z, zi) |\n= E t sup h∈H 1 n n∑ i=1 iφ (h(x,xi)Y (y, yi)) |\n= E t sup h∈H 1 n n∑ i=1 iφ̃ (h(x,xi)Y (y, yi)) |\n+ φ(0)E\nt 1\nn n∑ i=1 i |\n= E t sup h∈H 1 n n∑ i=1 iφ̃ (h(x,xi)Y (y, yi)) |\n≤ LY E t sup h∈H 1 n n∑ i=1 ih(x,xi) | = LYRn(H),\nwhere the fourth step follows from linearity of expectation. The fifth step is obtained by applying the contraction inequality to the functions ψi : x 7→ φ̃(aix) where ai = Y (y, yi). We exploit the fact that the contraction inequality is actually proven for the empirical Rademacher averages due to which we can take ai = Y (y, yi) to be a constant dependent only on i,\nuse the inequality, and subsequently take expectations. We also have, for any i and any x, y ∈ R,\n|ψi(x)− ψi(y)| = ∣∣∣φ̃(aix)− φ̃(aiy)∣∣∣\n≤ L |aix− aiy| ≤ L |ai| |x− y| ≤ LY |x− y| ,\nwhich shows that every function ψi(·) is LY -Lipschitz and satisfies ψi(0) = 0. This makes an application of the contraction inequality possible on the empirical Rademacher averages which upon taking expectations give us the result."
    }, {
      "heading" : "F. Applications",
      "text" : "In this section we shall derive Rademacher complexity bounds for hypothesis classes used in various learning problems. Crucial to our derivations shall be the following result by (Kakade et al., 2008). Recall the usual definition of Rademacher complexity of a univariate function class F = {f : X → R}\nRn(F) = E t sup f∈F 1 n n∑ i=1 if(xi) | .\nTheorem 22 ((Kakade et al., 2008), Theorem 1). Let W be a closed and convex subset of some Banach space equipped with a norm ‖·‖ and dual norm ‖·‖∗. Let F : W → R be σ-strongly convex with respect to ‖·‖∗. Assume W ⊆ { w : F (w) ≤W 2∗ } . Furthermore, let X = {x : ‖x‖ ≤ X} and FW := {w 7→ 〈w,x〉 : w ∈ W,x ∈ X}. Then, we have\nRn(FW) ≤ XW∗\n√ 2\nσn .\nWe note that Theorem 22 is applicable only to first order learning problems since it gives bounds for univariate function classes. However, our hypothesis classes consist of bivariate functions which makes a direct application difficult. Recall our extension of Rademacher averages to bivariate function classes:\nRn(H) = E t sup h∈H 1 n n∑ i=1 ih(z, zi) |\nwhere the expectation is over i, z and zi. To overcome the above problem we will use the following two step proof technique:\n1. Order reduction: We shall cast our learning problems in a modified input domain where predictors behave linearly as univariate functions.\nMore specifically, given a hypothesis class H and domain X , we shall construct a modified domain X̃ and a map ψ : X × X → X̃ such that for any x,x′ ∈ X and h ∈ H, we have h(x,x′) = 〈h, ψ(x,x′)〉.\n2. Conditioning: For every x ∈ X , we will create a function class Fx = {x′ 7→ 〈h, ψ(x,x′)〉 : h ∈ H}. Since Fx is a univariate function class, we will use Theorem 22 to bound Rn(Fx). Since Rn(H) = E x\nJRn(Fx)K, we shall obtain Rademacher complexity bounds for H.\nWe give below some examples of learning situations where these results may be applied.\nAs before, for any subset X of a Banach space and any norm ‖·‖p, we define ‖X‖p := sup\nx∈X ‖x‖p. We\nalso define norm bounded balls in the Banach space as Bp(r) := { x : ‖x‖p ≤ r } for any r > 0. Let the domain X be a subset of Rd.\nFor sake of convenience we present the examples using loss functions for classification tasks but the same can be extended to other learning problems such as regression, multi-class classification and ordinal regression.\nF.1. AUC maximization for Linear Prediction\nIn this case the goal is to maximize the area under the ROC curve for a linear classification problem at hand. This translates itself to a learning situation where W,X ⊆ Rd. We have hw(x,x′) = w>x−w>x′ and `(hw, z1, z2) = φ ((y − y′)hw(x,x′)) where φ is the hinge loss or the exponential loss (Zhao et al., 2011).\nIn order to apply Theorem 22, we rewrite the hypothesis as hw(x,x\n′) = w>(x − x′) and consider the input domain X̃ = {x− x′ : x,x′ ∈ X} and the map ψ : (x,x′) 7→ x − x′. Clearly if X ⊆ {x : ‖x‖ ≤ X} then X̃ ⊆ {x : ‖x‖ ≤ 2X} and thus we have\n∥∥∥X̃∥∥∥ ≤ 2 ‖X‖ for any norm ‖·‖. It is now possible to regularize the hypothesis class W using a variety of norms.\nIf we wish to define our hypothesis class as Bq(·), q > 1, then in order to apply Theorem 22, we can use the regularizer F (w) = ‖w‖2q. If we wish the sparse\nhypotheses class, B1(W1), we can use the regularizer F (w) = ‖w‖2q with q = log d log d−1 as this regularizer is strongly convex with respect to the L1 norm (Kakade et al., 2012). Table 1 gives a succinct summary of such possible regularizations and corresponding Rademacher complexity bounds.\nKernelized AUC maximization: Since the L2 regularized hypothesis class has a dimension independent Rademacher complexity, it is possible to give guarantees for algorithms performing AUC maximization using kernel classifiers as well. In this case we have a Mercer kernel K with associated reproducing kernel Hilbert space HK and feature map ΦK : X → HK . Our predictors lie in the RKHS, i.e., w ∈ HK and we have hw(x,x\n′) = w> (ΦK(x)− ΦK(x′)). In this case we will have to use the map ψ : (x,x′) 7→ ΦK(x)− ΦK(x′) ∈ HK . If the kernel is bounded, i.e., for all x,x′ ∈ X , we have |K(x,x′)| ≤ κ2, then we can get a Rademacher average bound of 2κ ‖W‖2 √ 1 n .\nF.2. Linear Similarity and Mahalanobis Metric learning\nA variety of applications, such as in vision, require one to fine tune one’s notion of proximity by learning a similarity or metric function over the input space. We consider some such examples below. In the following, we have W ∈ Rd×d.\n1. Mahalanobis metric learning : in this case we wish to learn a metric MW(x,x\n′) = (x − x′)>W(x − x′) using the loss function `(MW, z, z\n′) = φ ( yy′ ( 1−M2W(x,x′) )) (Jin et al., 2009).\n2. Linear kernel learning : in this case we wish to learn a linear kernel function KW(x,x\n′) = x>Wx′,W 0. A variety of loss functions have been proposed to aid the learning process\n(a) Kernel-target Alignment : the loss function used is `(KW, z, z ′) = φ (yy′KW(x,x ′))\nwhere φ is used to encode some notion of alignment (Cristianini et al., 2001; Cortes et al., 2010b).\n(b) S-Goodness: this is used in case one wishes to learn a good similarity function that need not be positive semi definite (Bellet et al., 2012; Balcan & Blum, 2006) by defining\n`(KW, z) = φ ( y E (x′,y′) Jy′KW(x,x′)K ) .\nIn order to apply Theorem 22, we will again rewrite the hypothesis and consider a different input domain. For the similarity learning problem, write the similarity function as KW(x,x ′) = 〈 W,xx′> 〉 and consider\nthe input space X̃ = { xx′> : x,x′ ∈ X } ⊆ Rd×d along with the map ψ : (x,x′) 7→ xx′>. For the metric learning problem, rewrite the metric as MW(x,x\n′) =〈 W, (x− x′)(x− x′)> 〉 and consider the input space\nX̃ = { (x− x′)(x− x′)> : x,x′ ∈ X } ⊆ Rd×d along with the map ψ : (x,x′) 7→ (x− x′)(x− x′)>.\nIn this case it is possible to apply a variety of matrix norms to regularize the hypothesis class. We consider the following (mixed) matrix norms : ‖·‖1,1, ‖·‖2,1 and ‖·‖2,2. We also consider the Schatten norm ‖X‖S(p) := ‖σ(X)‖p that includes the widely used trace norm ‖σ(X)‖1. As before, we define norm bounded balls in the Banach space as follows:\nBp,q(r) := { x : ‖x‖p,q ≤ r } .\nUsing results on construction of strongly convex functions with respect to theses norms from (Kakade et al., 2012), it is possible to get bounds on the Rademacher averages of the various hypothesis classes. However these bounds involve norm bounds for the modified domain X̃ . We make these bounds explicit by expressing norm bounds for X̃ in terms of those for X . From the definition of X̃ for the similarity learning problems, we get, for any p, q ≥ 1, ∥∥∥X̃∥∥∥ p,q ≤ ‖X‖p ‖X‖q. Also, since every element of X̃ is of the form xx′>, it has only one non zero singular value ‖x‖2 ‖x′‖2 which gives us ∥∥∥X̃∥∥∥ S(p) ≤ ‖X‖22 for any p ≥ 1. For the metric learning problem, we can similarly get∥∥∥X̃∥∥∥ p,q ≤ 4 ‖X‖p ‖X‖q and ∥∥∥X̃∥∥∥ S(p) ≤ 4 ‖X‖22 for any p ≥ 1 which allows us to get similar bounds as those for similarity learning but for an extra constant factor. We summarize our bounds in Table 2. We note that (Cao et al., 2012) devote a substantial amount of effort to calculate these values for the mixed norms on a case-by-case basis (and do not consider Schatten norms either) whereas, using results exploiting strong convexity and strong smoothness from (Kakade et al., 2012), we are able to get the same as simple corollaries.\nF.3. Two-stage Multiple kernel learning\nThe analysis of the previous example can be replicated for learning non-linear Mercer kernels as well. Additionally, since all Mercer kernels yield Hilbertian metrics, these methods can be extended to learning Hilbertian metrics as well. However, since Hilbertian metric learning has not been very popular in literature, we restrict our analysis to kernel learning alone. We present this example using the framework proposed by (Kumar et al., 2012) due to its simplicity and generality.\nWe are given p Mercer kernels K1, . . . ,Kp that are bounded, i.e., for all i, |Ki(x,x′)| ≤ κ2 for all x,x′ ∈ X and our task is to find a combination of these kernels given by a vector µ ∈ Rp,µ ≥ 0 such that the kernel Kµ(x,x ′) = ∑p i=1 µiKi(x,x\n′) is a good kernel (Balcan & Blum, 2006). In this case the loss function used is `(µ, z, z′) = φ (yy′Kµ(x,x\n′)) where φ(·) is meant to encode some notion of alignment. Kumar et al. (2012) take φ(·) to be the hinge loss.\nTo apply Theorem 22, we simply use the “K-space” construction proposed in (Kumar et al., 2012). We write Kµ(x,x\n′) = 〈µ, z(x,x′)〉 where z(x,x′) = (K1(x,x ′), . . . ,Kp(x,x ′)). Consequently our modified input space looks like X̃ = {z(x,x′) : x,x′ ∈ X} ⊆ Rp with the map ψ : (x,x′) 7→ z(x,x′). Popular regularizations on the kernel combination vector µ include the sparsity inducing L1 regularization that constrains µ to lie on the unit simplex ∆(1) = {µ : ‖µ‖1 = 1,µ ≥ 0} and L2 regularization that restricts µ to lie on the unit sphere S2(1) = {µ : ‖µ‖2 = 1,µ ≥ 0}. Arguments similar to the one used to discuss the case of AUC maximization for linear predictors give us bounds on the Rademacher aver-\nages for these two hypothesis classes in terms of ∥∥∥X̃∥∥∥\n2 and ∥∥∥X̃∥∥∥\n∞ . Since ∥∥∥X̃∥∥∥ 2 ≤ κ2√p and ∥∥∥X̃∥∥∥ ∞ ≤ κ2, we\nobtain explicit bounds on the Rademacher averages that are given in Table 3.\nWe note that for the L1 regularized case, our bound has a similar dependence on the number of kernels, i.e., √\nlog p as the bounds presented in (Cortes et al., 2010a). For the L2 case however, we have a worse dependence of √ p than Cortes et al. (2010a) who get a 4 √ p dependence. However, it is a bit unfair to compare\nthe two bounds since Cortes et al. (2010a) consider single stage kernel learning algorithms that try to learn the kernel combination as well as the classifier in a single step whereas we are dealing with a two-stage process where classifier learning is disjoint from the kernel learning step."
    }, {
      "heading" : "G. Regret Bounds for Reservoir Sampling Algorithms",
      "text" : "The Reservoir Sampling algorithm (Vitter, 1985) essentially performs sampling without replacement which means that the samples present in the buffer are not i.i.d. samples from the preceding stream. Due to this, proving regret bounds by way of uniform convergence arguments becomes a bit more difficult. However, there has been a lot of work on analyzing learning algorithms that learn from non-i.i.d. data such as data generated by ergodic processes. Of particular interest is a result by Serfling 2 that gives Hoeffding style bounds for data generated from a finite population without replacement.\nAlthough Serfling’s result does provide a way to analyze the RS algorithm, doing so directly would require using arguments that involve covering numbers that offer bounds that are dimension dependent and that are not tight. It would be interesting to see if equivalents of the McDiarmid’s inequality and Rademacher averages can be formulated for samples obtained without replacement to get tighter results. For our purposes, we remedy the situation by proposing a new sampling algorithm that gives us i.i.d. samples in the buffer allowing existing techniques to be used to obtain regret bounds (see Appendices H and I)."
    }, {
      "heading" : "H. Analysis of the RS-x Algorithm",
      "text" : "In this section we analyze the RS-x substream sampling algorithm and prove its statistical properties. Recall that the RS-x algorithm simply admits a point into the buffer if there is space. It performs a Repopulation step at the first instance of overflow which involves refilling the buffer by sampling with replacement from all the set of points seen so far (including the one that caused the overflow). In subsequent steps, a Normal update step is performed. The following theorem formalizes the properties of the sampling algorithm\nTheorem 23. Suppose we have a stream of elements z1, . . . , zn being sampled into a buffer B of size s using\n2R. J. Serfling, Probability Inequalities for the Sum in Sampling without Replacement, The Annals of Statistics, 2(1):39-48, 1974.\nthe RS-x algorithm. Then at any time t ≥ s+ 2, each element of B is an i.i.d. sample from the set Zt−1.\nProof. To prove the results, let us assume that the buffer contents are addressed using the variables ζ1, . . . , ζs. We shall first concentrate on a fixed element, say ζ1 (which we shall call simply ζ for notational convenience) of the buffer and inductively analyze the probability law Pt obeyed by ζ at each time step t ≥ s+ 2.\nWe will prove that the probability law obeyed by ζ at time t is Pt(ζ) = 1t−1 ∑t−1 τ=1 1{ζ=zτ}. The law is interpreted as saying the following: for any τ ≤ t − 1, P [ζ = zτ ] = 1t−1 and shows that the element ζ is indeed a uniform sample from the set Zt−1. We would similarly be able to show this for all locations ζ2, . . . , ζs which would prove that the elements in the buffer are indeed identical samples from the preceding stream. Since at each step, the RS-x algorithm updates all buffer locations independently, the random variables ζ1, . . . , ζs are independent as well which would allow us to conclude that at each step we have s i.i.d. samples in the buffer as claimed.\nWe now prove the probability law for ζ. We note that the repopulation step done at time t = s+ 1 explicitly ensures that at step t = s+2, the buffer contains s i.i.d samples from Zs+1 i.e. Ps+2(ζ) = 1s+1 ∑s+1 τ=1 1{ζ=zτ}. This forms the initialization of our inductive argument. Now suppose that at the tth time step, the claim is true and ζ obeys the law Pt(ζ) = 1t−1 ∑t−1 τ=1 1{ζ=zτ}. At the tth step, we would update the buffer by making the incoming element zt replace the element present at the location indexed by ζ with probability 1/(t + 1). Hence ζ would obey the following law after the update(\n1− 1 t\n) Pt(ζ) + 1\nt 1{ζ=zt} =\n1\nt t∑ τ=1 1{ζ=zτ}\nwhich shows that at the (t+ 1)th step, ζ would follow the law Pt+1(ζ) = 1t ∑t τ=1 1{ζ=zτ} which completes the inductive argument and the proof."
    }, {
      "heading" : "I. Proof of Theorem 8",
      "text" : "We now prove Theorem 8 that gives a high confidence regret bound for the OLP learning algorithm when used along with the RS-x buffer update policy. Our proof proceeds in two steps: in the first step we prove a uniform convergence type guarantee that would allow us to convert regret bounds with respect to the finitebuffer penalties L̂buft into regret bounds in in terms of the all-pairs loss functions L̂t. In the second step we\nthen prove a regret bound for OLP with respect to the finite-buffer penalties.\nWe proceed with the first step of the proof by proving the lemma given below. Recall that for any sequence of training examples z1, . . . , zn, we define, for any h ∈ H, the all-pairs loss function as L̂t(h) = 1t−1 ∑t−1 τ=1 `(h, zt, zτ ). Moreover, if the online learning process uses a buffer, the we also define the finite-buffer loss function as L̂buft (ht−1) = 1 |Bt| ∑ z∈Bt `(ht−1, zt, z). Lemma 24. Suppose we have an online learning algorithm that incurs buffer penalties based on a buffer B of size s that is updated using the RS-x algorithm. Suppose further that the learning algorithm generates an ensemble h1, . . . , hn−1. Then for any t ∈ [1, n− 1], with probability at least 1−δ over the choice of the random variables used to update the buffer B until time t, we have\nL̂t(ht−1) ≤ L̂buft (ht−1) + Cd · O √ log 1δ s  Proof. Suppose t ≤ s+ 1, then since at that point the buffer stores the stream exactly, we have\nL̂t(ht−1) = L̂buft (ht−1)\nwhich proves the result. Note that, as Algorithm 2 indicates, at step t = s+1 the buffer is updated (using the repopulation step) only after the losses have been calculated and hence step t = s+ 1 still works with a buffer that stores the stream exactly.\nWe now analyze the case t > s+1. At each step τ > s, the RS-x algorithm uses s independent Bernoulli random variables (which we call auxiliary random variables) to update the buffer, call them rτ1 , . . . , r τ s where rτj is used to update the j th item ζj in the buffer. Let rtj := {r s+1 j , r 2 j , . . . , r t j} ∈ {0, 1} t denote an ensemble random variable composed of t − s independent Bernoulli variables. It is easy to see that the element ζj is completely determined at the t th step given rt−1j .\nTheorem 23 shows, for any t > s + 1, that the buffer contains s i.i.d. samples from the set Zt−1. Thus, for any fixed function h ∈ H, we have for any j ∈ [s],\nE rt−1j\nJ`(h, zt, ζj)K = 1\nt− 1 t−1∑ τ=1 `(h, zt, zτ )\nwhich in turn shows us that\nE rt−11 ,...,r t−1 s\nr L̂buft (h) z = 1\nt− 1 t−1∑ τ=1 `(h, zt, zτ ) = L̂t(h)\nNow consider a ghost sample of auxiliary random variables r̃t−11 , . . . , r̃ t−1 s . Since our hypothesis ht−1 is independent of these ghost variables, we can write\nE r̃t−11 ,...,r̃ t−1 s\nr L̂buft (ht−1) z = L̂t(ht−1)\nWe recall that error in the proof presented in Zhao et al. (2011) was to apply such a result on the true auxiliary variables upon which ht−1 is indeed dependent. Thus we have\nL̂t(ht−1)− L̂buft (ht−1)\n= E r̃t−11 ,...,r̃ t−1 s\nr L̂buft (ht−1) z − L̂buft (ht−1)\n≤ sup h∈H\n[ E\nr̃t−11 ,...,r̃ t−1 s\nr L̂buft (h) z − L̂buft (h) ] ︸ ︷︷ ︸\ngt(r t−1 1 ,...,r t−1 s )\nNow, the perturbation to any of the ensemble variables rj (a perturbation to an ensemble variable implies a perturbation to one or more variables forming that ensemble) can only perturb only the element ζj in\nthe buffer. Since L̂buft (ht−1) = 1s ∑\nz∈Bt `(ht−1, zt, z) and the loss function is B-bounded, this implies that a perturbation to any of the ensemble variables can only perturb g(rt−11 , . . . , r t−1 s ) by at most B/s. Hence an application of McDiarmid’s inequality gives us, with probability at least 1− δ,\ngt(r t−1 1 , . . . , r t−1 s ) ≤ E\nrt−1j\nq gt(r t−1 1 , . . . , r t−1 s ) y +B\n√ log 1δ\n2s\nAnalyzing the expectation term we get\nE rt−1j\nq gt(r t−1 1 , . . . , r t−1 s ) y\n= E rt−1j t sup h∈H\n[ E\nr̃t−11 ,...,r̃ t−1 s\nr L̂buft (h) z − L̂buft (h)\n]|\n≤ E rt−1j ,r̃ t−1 j u vsup h∈H 1 s s∑ j=1 `(h, zt, ζ̃j)− `(h, zt, ζj) }~ = E\nrt−1j ,r̃ t−1 j , j\nu vsup h∈H 1 s s∑ j=1 j ( `(h, zt, ζ̃j)− `(h, zt, ζj) )}~ ≤ 2 E\nrt−1j ,r̃ t−1 j , j\nu vsup h∈H 1 s s∑ j=1 j`(h, zt, ζj) }~ ≤ 2Rs(` ◦ H)\nwhere in the third step we have used the fact that symmetrizing a pair of true and ghost ensemble variables\nis equivalent to symmetrizing the buffer elements they determine. In the last step we have exploited the definition of Rademacher averages with the (empirical)\nmeasure 1t−1 ∑t−1 τ=1 δzτ imposed over the domain Z.\nFor hypothesis classes for which we have R̂s(` ◦ H) = Cd · O (√ 1 s ) , this proves the claim.\nUsing a similar proof progression we can also show the following:\nLemma 25. For any fixed h ∈ H and any t ∈ [1, n−1], with probability at least 1 − δ over the choice of the random variables used to update the buffer B until time t, we have\nL̂buft (h) ≤ L̂t(h) + Cd · O √ log 1δ s  Combining Lemmata 24 and 25 and taking a union bound over all time steps, the following corollary gives us a buffer to all-pairs conversion bound.\nLemma 26. Suppose we have an online learning algorithm that incurs buffer penalties based on a buffer B of size s that is updated using the RS-x algorithm. Suppose further that the learning algorithm generates an ensemble h1, . . . , hn−1. Then with probability at least 1−δ over the choice of the random variables used to update the buffer B, we have\nRn ≤ Rbufn + Cd (n− 1) · O (√ log nδ s ) ,\nwhere we recall the definition of the all-pairs regret as\nRn := n∑ t=2 L̂t(ht−1)− inf h∈H n∑ t=2 L̂t(h)\nand the finite-buffer regret as\nRbufn := n∑ t=2 L̂buft (ht−1)− inf h∈H n∑ t=2 L̂buft (h).\nProof. Let ĥ := arg inf h∈H\n∑n t=2 L̂t(h). Then Lemma 25\ngives us, upon summing over t and taking a union bound,\nn∑ t=2 L̂buft (ĥ) ≤ n∑ t=2 L̂t(ĥ) + Cd(n− 1) · O (√ log nδ s ) ,\n(11)\nwhereas Lemma 24 similarly guarantees n∑ t=2 L̂t(ht−1) ≤ n∑ t=2 L̂buft (ht−1) + Cd(n− 1) · O (√ log nδ s ) ,\n(12)\nwhere both results hold with high confidence. Adding the Equations (11) and (12) and using∑n t=2 L̂buft (ht−1) ≤ inf\nh∈H\n∑n t=2 L̂buft (ĥ)+Rbufn completes\nthe proof.\nAs the final step of the proof, we give below a finitebuffer regret bound for the OLP algorithm.\nLemma 27. Suppose the OLP algorithm working with an s-sized buffer generates an ensemble w1, . . . ,wn−1. Further suppose that the loss function ` being used is L-Lipschitz and the space of hypotheses W is a compact subset of a Banach space with a finite diameter D with respect to the Banach space norm. Then we have\nRbufn ≤ LD √ n− 1\nProof. We observe that the algorithm OLP is simply a variant of the GIGA algorithm (Zinkevich, 2003) being applied with the loss functions `GIGAt : w 7→ L̂buft (w). Since `GIGAt inherits the Lipschitz constant of L̂buft which in turn inherits it from `, we can use the analysis given by Zinkevich (2003) to conclude the proof.\nCombining Lemmata 26 and 27 gives us the following result:\nTheorem 28 (Theorem 8 restated). Suppose the OLP algorithm working with an s-sized buffer generates an ensemble w1, . . . ,wn−1. Then with probability at least 1− δ,\nRn n− 1 ≤ O\n( Cd √ log nδ s + √ 1 n− 1 )\nJ. Implementing the RS-x Algorithm\nAlthough the RS-x algorithm presented in the paper allows us to give clean regret bounds, it suffers from a few drawbacks. From a theoretical point of view, the algorithm is inferior to Vitter’s RS algorithm in terms of randomness usage. The RS algorithm (see (Zhao et al., 2011) for example) uses a Bernoulli random variable and a discrete uniform random variable at each time step. The discrete random variable takes values in [s] as a result of which the algorithm uses a total of O (log s) random bits at each step.\nAlgorithm 3 RS-x2 : An Alternate Implementation of the RS-x Algorithm\nInput: Buffer B, new point zt, buffer size s, timestep t Output: Updated buffer Bnew 1: if |B| < s then //There is space 2: Bnew ← B ∪ {zt} 3: else //Overflow situation 4: if t = s+ 1 then //Repopulation step 5: TMP = B ∪ {zt} 6: Bnew = φ 7: for i = 1 to s do 8: Select random r ∈ TMP with replacement 9: Bnew ← Bnew ∪ {r} 10: end for 11: else //Normal update step 12: Bnew ← B 13: Sample k ∼ Binomial(s, 1/t) 14: Remove k random elements from Bnew\n15: Bnew ← Bnew ∪ (∐k i=1 {zt} ) 16: end if 17: end if 18: return Bnew\nThe RS-x algorithm as proposed, on the other hand, uses s Bernoulli random variables at each step (to decide which buffer elements to replace with the incoming point) taking its randomness usage to O (s) bits. From a practical point of view this has a few negative consequences:\n1. Due to increased randomness usage, the variance of the resulting algorithm increases.\n2. At step t, the Bernoulli random variables required all have success probability 1/t. This quantity drops down to negligible values for even moderate values of t. Note that Vitter’s RS on the other hand requires a Bernoulli random variable with success probability s/t which dies down much more slowly.\n3. Due to the requirement of such high precision random variables, the imprecisions of any pseudo random generator used to simulate this algorithm become apparent resulting in poor performance.\nIn order to ameliorate the situation, we propose an alternate implementation of the normal update step of the RS-x algorithm in Algorithm 3. We call this new sampling policy RS-x2 . We shall formally demonstrate the equivalence of the RS-x and the RS-x2 policies by showing that both policies result in a buffer whose each element is a uniform sample from the preceding stream with replacement. This shall be done by proving that the joint distribution of the buffer elements remains the same whether the RS-x normal update is applied or the RS-x2 normal step is ap-\nplied (note that RS-x and RS-x2 have identical repopulation steps). This will ensure that any learning algorithm will be unable to distinguish between the two update mechanisms and consequently, our regret guarantees shall continue to hold.\nFirst we analyze the randomness usage of the RS-x2 update step. The update step first samples a number Kt ∼ B(s, 1/t) from the binomial distribution and then replaces Kt random locations with the incoming point. Choosing k locations without replacement from a pool of s locations requires at most k log s bits of randomness. Since Kt is sampled from the binomial distribution B(s, 1/t), we have Kt = O (1) in expectation (as well as with high probability) since t > s whenever this step is applied. Hence our randomness usage per update is at most O (log s) random bits which is much better than the randomness usage of RS-x and that actually matches that of Vitter’s RS upto a constant.\nTo analyze the statistical properties of the RS-x2 update step, let us analyze the state of the buffer after the update step. In the RS-x algorithm, the state of the buffer after an update is completely specified once we enumerate the locations that were replaced by the incoming point. Let the indicator variable Ri indicate whether the ith location was replaced or not. Let r ∈ {0, 1}s denote a fixed pattern of replacements. Then the original implementation of the update step of RS-x guarantees that\nP RS-x [ s∧ i=1 (Ri = ri) ] = ( 1 t )‖r‖1 ( 1− 1 t )s−‖r‖1 To analyze the same for the alternate implementation of the RS-x2 update step, we first notice that choosing k items from a pool of s without replacement is identical to choosing the first k locations from a random permutation of the s items. Let us denote ‖r‖1 = k. Then we have,\nP RS-x2 [ s∧ i=1 (Ri = ri) ] = s∑ j=1 P [ s∧ i=1 (Ri = ri) ∧Kt = j ]\n= P [ s∧ i=1 (Ri = ri) ∧Kt = k ]\n= P [ s∧ i=1 (Ri = ri) ∣∣∣∣∣Kt = k ] P [Kt = k]\nWe have P [Kt = k] = ( s\nk\n)( 1\nt\n)k ( 1− 1\nt )s−k The number of arrangements of s items such that some specific k items fall in the first k positions is k!(s−k)!.\nThus we have\nP RS-x2 [ s∧ i=1 (Ri = ri) ] = ( s k )( 1 t )k ( 1− 1 t )s−k k!(s− k)! s!\n=\n( 1\nt\n)k ( 1− 1\nt )s−k = P\nRS-x [ s∧ i=1 (Ri = ri) ]\nwhich completes the argument."
    }, {
      "heading" : "K. Additional Experimental Results",
      "text" : "Here we present experimental results on 14 different benchmark datasets (refer to Figure 3) comparing the OLP algorithm using the RS-x2 buffer policy with the OAMgra algorithm using the RS buffer policy. We continue to observe the trend that OLP performs competitively to OAMgra while enjoying a slight advantage in small buffer situations in most cases."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "In this paper, we study the generalization properties of online learning based stochastic methods for supervised learning problems where the loss function is dependent on more than one training sample (e.g., metric learning, ranking). We present a generic decoupling technique that enables us to provide Rademacher complexity-based generalization error bounds. Our bounds are in general tighter than those obtained by Wang et al. (2012) for the same problem. Using our decoupling technique, we are further able to obtain fast convergence rates for strongly convex pairwise loss functions. We are also able to analyze a class of memory efficient online learning algorithms for pairwise learning problems that use only a bounded subset of past training samples to update the hypothesis at each step. Finally, in order to complement our generalization bounds, we propose a novel memory efficient online learning algorithm for higher order learning problems with bounded regret guarantees.",
    "creator" : "LaTeX with hyperref package"
  }
}