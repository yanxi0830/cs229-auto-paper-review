{
  "name" : "1205.2602.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Entire Quantile Path of a Risk-Agnostic SVM Classifier",
    "authors" : [ "Jin Yu", "S.V.N. Vishwanathan", "Jian Zhang" ],
    "emails" : [ "jin.yu@anu.edu.au", "jianzhan}@stat.purdue.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "A quantile binary classifier uses the rule: Classify x as +1 if P (Y = 1|X = x) ≥ τ , and as −1 otherwise, for a fixed quantile parameter τ ∈ [0, 1]. It has been shown that Support Vector Machines (SVMs) in the limit are quantile classifiers with τ = 12 . In this paper, we show that by using asymmetric cost of misclassification SVMs can be appropriately extended to recover, in the limit, the quantile binary classifier for any τ . We then present a principled algorithm to solve the extended SVM classifier for all values of τ simultaneously. This has two implications: First, one can recover the entire conditional distribution P (Y = 1|X = x) = τ for τ ∈ [0, 1]. Second, we can build a risk-agnostic SVM classifier where the cost of misclassification need not be known apriori. Preliminary numerical experiments show the effectiveness of the proposed algorithm."
    }, {
      "heading" : "1 Introduction",
      "text" : "Support Vector Machines (SVMs) have emerged as a popular tool for binary classification. Given a set of n training instances xi and their corresponding labels yi ∈ {±1} the task of training a linear SVM classifier1 can be cast as a regularized risk minimization problem:\nminJ(w) := λ 2 ‖w‖2 + 1 n n∑ i=1 l(w>xi, yi), (1)\nwhere l is the so-called hinge loss defined by\nl(w>x, y) := max(0, 1− yw>x). (2) 1For ease of exposition we will stick to linear SVMs, although all our results extend to the non-linear case where one maps the data into an RKHS H via the map x→ φ(x) and uses the kernel k(x,x′) = 〈φ(x), φ(x′)〉H.\nIt is obvious that the hinge loss l(·) is non-negative and convex in w. The loss function measures the discrepancy between y and the prediction given by sign(w>x), while the L2 regularizer with regularization constant λ > 0 controls the complexity of the solution w. It has been shown (Lin, 2002) that the minimizer of the hinge loss is exactly sign(η(x)−1/2), where η(x) = P (Y = 1|X = x) is the probability of Y = 1 conditioned on X = x. Thus the SVM solution should approach the Bayes rule as the sample size gets large with appropriately chosen function class.\nOne problem of the standard SVM is that even though we can use the resulting SVM classifier ŵ = argminw J(w) to classify a new observation x, the prediction w>x does not have probabilistic interpretation. More importantly, the SVM classification results cannot be directly applied to situations where the misclassification cost is asymmetric, i.e. when the cost of a false positive error is different from that of a false negative error. To address such a problem, several methods have been proposed to convert the SVM output into well-calibrated probabilistic scores, such as (Platt, 2000). However, such methods either rely on parametric assumption or lack theoretical justification about the transformed scores.\nWe instead aim to estimate the quantity sign(η(x)−τ), which we call the quantile classification rule with τ ∈ [0, 1] as the quantile parameter. It can be shown that sign(η(x)−τ) is the minimizer of the asymmetric hinge loss, which assigns different costs to false positive and false negative errors. The SVM formulation with the asymmetric hinge loss can be defined as\nmin λ\n2 ‖w‖2 + n∑ i=1 cτyi max(0, 1− yiw >xi). (3)\nHere, cτyi controls the two types of misclassification cost, and for reasons that will become apparent\nshortly, we set\ncτyi := { [2(1− τ)]/n if yi = +1, (2τ)/n if yi = −1.\n(4)\nfor τ ∈ [0, 1]. Note that when τ = 12 , i.e., the misclassification costs are symmetric, we recover (1).\nThere are many natural applications where the cost of misclassification is not known in advance until the classifier is deployed. As an illustrative example consider the problem of spam detection (also see Figure 1). Given training emails the learning task is to distinguish between spam and non-spam emails. The tolerance of a user to spam is influenced by various factors. For instance, a busy professor might have a very low tolerance for spam. In other words, he/she might not mind losing a few genuine emails as long as all spam emails are kept out of his/her inbox. On the other hand, a not-so-busy graduate student might not mind a few spam emails as long as genuine emails are not lost in the junk mail folder. In such cases, the brute force approach of training a classifier for every user preference is both tedious and time consuming. Furthermore, one needs to train a new classifier for every user preference.\nIn this paper we present a principled algorithm to solve (3) for all values of τ simultaneously by utilizing the fact that the solution path is piecewise linear as a function of the quantile parameter τ ∈ [0, 1]. In other words, once our classifier is trained, we can recover the solution for any τ efficiently. Consequently, our classifier is risk agnostic. Furthermore, we show that (3) is an instance of the quantile classification problem, with τ being the quantile parameter.\nThe rest of the paper is organized as follows: In Section 2 we establish the connection between the asymmetric cost SVM and the quantile classification rule. In Section 3 we formulate the dual of (3). In Section 4 we describe the proposed algorithm and its worst case time complexity analysis. We discuss some related work in Section 5. Numerical experiments are presented in Section 6, and we conclude the paper with an outlook and discussion in Section 7."
    }, {
      "heading" : "2 Statistical Underpinnings",
      "text" : "Let (X, Y ) be a pair of random variables with training instances X ∈ X and labels Y ∈ {±1}. For any realization x of X, denote the conditional probability P (Y = 1|X = x) as η(x). Furthermore, let C+ (resp. C−) denote the cost of misclassifying a x labeled as +1 (resp. −1). The cost sensitive classification risk of\na decision function g : X → {±1} is defined as\nR(g) := C+P (Y = −1, g(X) = 1) (5) + C−P (Y = 1, g(X) = −1).\nThe following lemma follows from elementary Bayes decision theory (see e.g., Section 2.2 of Duda et al., 2001).\nLemma 2.1 For any decision function g\nR(sign(η(x)− τ)) ≤ R(g),\nwhere τ = C+C++C− ∈ [0, 1].\nLemma 2.1 says that when the misclassification cost is asymmetric, the classifier which leads to the minimum risk should take the form sign(η(x)− τ) where τ only depends on the ratio of the misclassification costs C+/C−. The following lemma, whose proof can be found in Appendix A, shows that sign(η(x)− τ) is the minimizer of the asymmetric hinge loss.\nLemma 2.2 For any τ ∈ [0, 1], the minimizer f∗ of EX,Y [ ((1 + Y )/2− τY ) (1− Y f(X))+ ] (6)\ntakes the form f∗(x) = sign(η(x)− τ).\nIn the infinite sample case, if we let λ → 0 as n → ∞ in (3), it is easy to see that the regularized risk converges to (6). Therefore, Lemma 2.2 implies that the estimator obtained by minimizing the regularized risk (3) is risk consistent.\nThe above observation has a number of consequences. First, it shows that the usual SVM with the symmetric hinge loss (2) estimates P (Y = 1|X = x) = 12 , a well known result (see e.g., Lin, 2002; Sollich, 2002). It also shows that the SVM with the asymmetric hinge loss (4) is essentially a quantile estimator. This result has been hinted many times (e.g. Grandvalet et al., 2006), but to the best of our knowledge, not proven rigorously."
    }, {
      "heading" : "3 Dual Formulation",
      "text" : "Similar to the case of standard SVM, we can rewrite (3) as a constrained optimization problem:\nmin λ\n2 ‖w‖2 + n∑ i=1 cτyiξi (7)\ns.t. ξi ≥ 0, ξi ≥ 1− yiw>xi, ∀i\nallows us to derive its dual as:\nmin D(α) := 1 2λ α>Qα−α>1 (8)\ns.t. 0 ≤ αi ≤ cτyi , ∀i.\nwhere 1 is a vector of all ones and Qij = yiyjx>i xj . Let ατ denote the optimal solution to (8) for a given τ . Then the primal solution wτ can be recovered via the primal-dual connection:\nwτ = 1 λ n∑ i=1 ατi yixi. (9)\nThe dual problem is a quadratic problem with box constrains, which can be solved by various optimization techniques (see e.g., Byrd et al., 1995; Moré and Toraldo, 1989)\nDefine ∇iD(ατ ) as the ith element of the gradient:\n∇D(ατ ) = 1 λ Qατ − 1; (10)\nand let L, M, andR index entries of the dual solution ατ such that\nL := {i : ∇iD(ατ ) < 0} = {i : yiwτ>xi < 1}, M := {i : ∇iD(ατ ) = 0} = {i : yiwτ>xi = 1}, (11) R := {i : ∇iD(ατ ) > 0} = {i : yiwτ>xi > 1},\nwhere the connection with the primal parameter wτ is made via (9). It is easy to find that L, M, and R in fact index the data xi which are in error, on the margin, and well-classified, respectively. Furthermore, it follows from the KKT conditions (see Appendix B) that the optimal dual solution ατ satisfies\nατi =  cτyi if i ∈ L, [0, cτyi ] if i ∈M, 0 if i ∈ R .\n(12)\nGiven index sets, I and J , let ατI be a vector of ατi with i ∈ I and QI J a submatrix of Q taking entries Qij with i ∈ I and j ∈ J . Then, we can define L+1 := L∩{i : yi = +1} and L−1 := L\\L+1, and use (12) to\ndecompose ατ into [ατ>L+ , α τ> L− , α τ> M , 0 >]>, where 0 is a vector of all zeros. Using (10), we can get a decomposed view of ∇D(ατ ):\n[∇LD(ατ )>,∇MD(ατ )>,∇RD(ατ )>]> := (13)\n1 λ  cτ+1QLL+11+cτ−1QLL−11+QLMατMcτ+1QML+11+cτ−1QML−11+QMMατM cτ+1QRL+11+c τ −1QRL−11+QRMα τ M − 1 Since ∇MD(ατ ) = 0 by the definition (11) ofM, we can use (13) to get a closed form representation of ατM:\nατM = Q −1 MM[λ1− c τ +1QML+11− cτ−1QML−11]. (14)\nNote that Q−1MM does not exist when QMM is rank deficient. Nevertheless, standard optimization techniques, such as the conjugate gradient method (Nocedal and Wright, 1999), should always recover ατM as a solution to a linear system.\nAn important observation from (4) is that the upper bound cτyi only changes linearly with τ : As we increase the quantile parameter τ to τ + , we have\ncτ+ yi = c τ yi − yi ∆c , where ∆c := 2 n . (15)\nAssume an deviation from τ does not change the index sets defined in (11), then (14) still holds for ατ+ M . Therefore, we can use (15) to expand it as:\nατ+ M = α τ M +∆c ∆ατM, where (16)\n∆ατM := Q −1 MM[QML+11−QML−11].\nThe optimality condition (12) then allows us to recover ατ+ from ατ via:\nατ+ L+1 ατ+ L−1 ατ+ M ατ+ R\n = \nατL+1 −∆c ατL−1 +∆c\nατM +∆c ∆ατM\n0\n . (17)\nProposition 3.1 For the dual of the quantile classification problem (8), there exists a set of quantiles {τk}Kk=1, τk ∈ [0, 1], such that we can find a solution path ατ that is continuous in τ , and linear in τ,∀τ ∈ (τk, τk+1).\nSee Appendix C for the proof of Proposition 3.1.\nProposition 3.1 shows that ατ is piecewise linear in τ . Using (13) and (17), we can see that the gradient ∇D(ατ ) has the same property. In particular, ∀ ∈ (0, τk+1 − τk), we have ∇MD(α(τk+ )) = 0 and\n∇ND(α(τk+ )) = ∇ND(ατk)\n+ ∆c\nλ [QN L−11−QN L+11+QN M ∆α τk M], (18)\nwhereM is the margin index set associated with ατk+ and N := L∪R is the complement set ofM."
    }, {
      "heading" : "4 Finding the Dual Solution Path",
      "text" : "It follows from Proposition 3.1 that if we can find a set of quantile parameters: K := {τk}Kk=1, that divide the interval [0, 1] into regions so that within these regions ατ changes linearly with τ , i.e., the index sets: L,M, and R remain fixed. Then we can quickly recover ατ for any value of τ from a ατk via (17). In what follows we present our algorithm (Algorithm 1) that is able to identify all τk, which we call kinks."
    }, {
      "heading" : "4.1 The Algorithm",
      "text" : "Our goal is to construct a sorted list of kinks {τk}Kk=1, at which one of the following events happens:\n1. Elements in N , i.e., not inM, move toM, 2. Elements inM move to L, 3. Elements inM move to R.\nTo this end, our algorithm starts with τ = 0, and then moves forward toward τ = 1 to identify all values of τ that alter the membership of an index.\nGiven a quantile parameter τk, its corresponding optimal dual solution ατk , and the associated index sets L,M, and R, we know from the definition (11) that ∇iD(ατk) 6= 0, ∀i ∈ N . This means that Event 1 happens when an > 0 deviation from τk just turns a nonzero element of the gradient to zero, i.e., ∇iD(ατk+ ) = 0, i ∈ N . We immediately see from (18) that the deviation that leads to Event 1 is:\ntoM = min{ i : i > 0}i∈N , where (19)\ni := n\n2\n[ −λ∇iD(ατk )\nQiL−11−QiL+11+QiM ∆α τk M\n] .\nWe know from the optimality condition (12) that an index i from M is just about to move into L (Event 2), when α(τk+ )i = c (τk+ ) yi . Expanding both sides of the last equation, using (15) and (16), shows that satisfies\nατki + 2 n ∆ατki = c τk yi − yi 2 n , i ∈M . (20)\nHere care must be taken when ατki = c τk yi , i ∈ M, i.e., ατki is on the boundary between L and M. In this case (20) can be reduced to ∆ατki = − yi; and if ∆ατki > −yi, then an arbitrarily small > 0 will cause α (τk+ ) i > c (τk+ ) yi , i.e., pushing the index i out toward L. Taking this boundary case into consideration, we determine a candidate using the following criteria:\ntoL = min{ i : i ≥ 0}i∈M, where (21)\ni :=  0 if ατki = c τk yi & ∆α τk i > −yi, +∞ if ατki = cτkyi & ∆α τk i ≤ −yi,\nn 2 (c τk yi − α τk i )/(∆α τk i + yi) otherwise.\nIf toL = 0, we treat τk as a kink, and update the index sets accordingly:\nM←M\\{itoL}, L ← L∪{itoL}, (22) where itoL = {i : i ∈M, i = toL},\nsuch that the updated index sets coincide with the index sets of the optimal dual solution ατ ,∀τ ∈ (τk, τk+1), τk+1 being the next kink.\nSimilarly, to detect Event 3, we find that satisfies ατk+ i = 0, ∀i ∈ M, and isolate α τk i = 0 case for special treatment:\ntoR = min{ i : i ≥ 0}i∈M, where (23)\ni :=  0 if ατki = 0 & ∆α τk i < 0, +∞ if ατki = 0 & ∆α τk i ≥ 0,\nn 2 (−α τk i )/(∆α τk i ) otherwise.\nIn the case where toR = 0, we should recognize τk as a kink, and shift the corresponding index from M to R. See Algorithm 1 for detailed implementation."
    }, {
      "heading" : "4.2 Complexity Analysis",
      "text" : "The time complexity of Algorithm 1 is dominated by the calculation of ∆ατM (16), which involves solving a linear system of size |M |. A standard solver such as the conjugate gradient method converges to the solution of such a linear system in at most O(r|M |2) time, r being the rank of QMM. Having computed ∆ατM, the main cost of finding\ntoM (19) is the O(n| N |) cost of matrix-vector multiplication; and the\nAlgorithm 1 Dual Path Finding (DPF) 1: input data {(xi, yi)}ni=1 and regularizer λ 2: output sorted list of kinks τk, corresponding dual solution ατk and index sets. 3: α0 = argminD(α), s.t. 0 ≤ αi ≤ c0yi , ∀i 4: construct L,M and R for α0 via (11) 5: calculate ∇D(α0) and ∆α0M 6: τ ← 0 and K ← {(0,α0M,L,M)} 7: while τ < 1 do 8: ← min{ toM(19), toL(21), toR(23)} 9: update ∇D(ατ ) to ∇D(ατ+ ) via (18) 10: update ατ to ατ+ via (17) 11: adjust index sets according to the event that triggers, e.g., if = toL, apply (22) 12: τ ← τ + 13: calculate ∆ατM 14: if = 0 then 15: overwrite the last element of K with (τ,ατM,L,M) (cf. discussion in Sec. 4.1) 16: else 17: K ← K∪{(τ,ατM,L,M)} 18: end if 19: end while 20: return K\ntime required to find toL (21) and toR (23) is linear in |M |. Once we find all the kinks, we can recover ατ for any τ ∈ [0, 1] via (17) in O(n) time by noting that ατM = α τk M + ∆c\n(τ−τk)∆ατkM, where ∆ατkM = (α τk+1 M − α τk M)/∆c\n(τk+1−τk) andM is associated with ατk .\nIn term of memory requirement, saving kink information at Step 17 of Algorithm 1 requires O(|M |) space for ατM; and after the initial O(n) cost of saving the entire L andM sets, we only need to keep track of the indices that move into or out of the two sets to recover them from their initial copy. We use the Q matrix in our calculation, e.g., (19). The Q matrix is usually dense; and caching it requires O(n2) space. This can be prohibitively expensive. However, noticing that Q = (Y X)(X>Y ), where Y is a diagonal matrix with labels yi on its diagonal andX = [x>1 , · · · ,x>n ] ∈ R n×d is a feature matrix, we can instead cache Y X, which is often very sparse. But, constructing Q from Y X at each step can be computationally expensive. Fortunately, since only the product of Q with a vector v is needed for our calculation, we can calculate it as Qv = Y X(X>Y v) to leverage fast sparse matrixvector product, and hence reduce the computational overhead. Although we do not have a formal bound on the size of | K |, our experiments show that it never exceeds O(n log n)."
    }, {
      "heading" : "5 Related Work",
      "text" : "Perhaps the closest in spirit to our paper is the work of Hastie et al. (2004), who studied the influence of the\nregularization constant λ on the generalization performance of a binary SVM. They showed that solutions to a SVM training problem is a piecewise linear function of λ. Based on this observation, they proposed an algorithm that finds SVM solutions for all values of λ. The regularization constant controls the balance between the regularization term and the empirical risk in the objective function (1) to prevent a classifier from overfitting the training data. Therefore, it plays an important role in improving prediction accuracy on unseen data. The effect of τ on the behaviour of a SVM classifier is fundamentally different from that of λ in a sense that τ determines the trade-off between the true positive rate (TPR) and the true negative rate (TNR) of a classifier by assigning asymmetric costs to false positive and false negative predictions. In applications where an appropriate balance between TPR and TNR is considered to be more important than prediction accuracy, e.g., in medical diagnosis, using a quantile classifier (3) with adjustable τ may be more desirable.\nAlthough SVM classifiers with built-in asymmetric misclassification costs have been applied to classification problems that are characterized by highly skewed training data and to problems arisen from medical diagnosis (Veropoulos et al., 1999; Morik et al., 1999; Grandvalet et al., 2006), no rigorous statistical properties were established. The misclassification cost is commonly chosen to reflect label proportions of training data or the ratio of false positive cost to false negative cost. From the optimization viewpoint training a SVM with asymmetric costs is very similar to the standard SVM training problem. Hence, optimization software such as SVMperf (Joachims, 2006) and LIBLINEAR (Hsieh et al., 2008) can be used for training. A common strategy to train a SVM classifier with multiple settings of asymmetric costs is to reassign costs, and retrain. Our DPF method exploits the piecewise linearity property of SVM dual solution, and finds the entire solution path in one shot. This allows us to quickly construct a classifier for any choice of misclassification costs in the post-training phrase.\nQuantile regression as an important statistical tool (Koenker and Bassett, 1978) has recently received attention from machine learning community. Takeuchi et al. (2006) showed that a quantile regression problem can be cast as a regularized risk minimization problem:\nmin λ\n2 ‖w‖2 + n∑ i=1 max(τ(yi − fi), (1− τ)(fi − yi)),\nwhere τ ∈ (0, 1) and fi = w>xi. This regression problem is very reminiscent of the quantile classification problem (3) we considered in this paper. In fact, by following the same principle as discussed in Section 4 we can extend our DPF method to find quantile regres-\nsion solutions for all choices of τ ."
    }, {
      "heading" : "6 Experiments",
      "text" : "We now evaluate the generalization performance of a quantile classifier for various values of τ , and compare the time complexity of our DPF method (Algorithm 1) with a state-of-the-art linear SVM classifier: LIBLINEAR version 1.32 (Hsieh et al., 2008). We used the LBFGSB quasi-Newton method of Byrd et al. (1995) to solve the dual problem at Step 3 of the Algorithm 1; and the conjugate gradient method was applied to find ∆ατM at Step 13. We ran DPF without caching the Q matrix.\nOur experiments used three datasets: the UCI diabetes dataset (Asuncion and Newman, 2007), the spam dataset for task A of the ECML/PKDD 2006 discovery challenge,2 and 3 × 104 worm splice samples from a biological dataset provided by Sören Sonnenburg.3 Table 1 summarizes the datasets and our parameter settings. In all experiments the regularization parameter was chosen from the set 10{−6,−5,··· ,−1}, such that a SVM classifier with symmetric misclassification costs achieves the highest prediction accuracy on the validation set, while generalization performance is reported on the test set. We included a bias b in the decision function: sign f(x) := w>x + b by using the following strategy: xi ← [x>i , 1]>,w ← [w>, b]>.\nAll experiments were carried out on a Linux machine with dual 2.4GHz Intel Core 2 processors and 4GB of RAM. Our Python code is available for download from http://users.rsise.anu.edu. au/∼jinyu/Code/DPF.tar.gz.\nOur first set of experiments shows the influence of the quantile τ on the behaviour of a classifier. As ατ changes, the generalization performance of the quantile classifier in terms of TPR (a.k.a. sensitivity) and TNR (a.k.a. specificity) changes accordingly. Figure 2 shows that TPR decreases (but not necessarily strictly decreases) with τ , while TNR has an opposite trend. This is because increasing τ corresponds to increasing the false positive cost C+ (cf. Lemma 2.2; see also\n2The original evaluation set (http://www. ecmlpkdd2006.org/challenge.html) was equally divided into training, validation, and test set.\n3http://www.fml.tuebingen.mpg.de/raetsch/ projects/lsmkl\nFigure 1, right), which leads the classifier to recognize more and more instances as negative samples at an expense of a decreasing TPR. At τ = 0 (resp. τ = 1), the classifier simply resorts to labeling all the points as + (resp. −). Therefore at these extreme points, the prediction accuracy depends on the proportion of the positive and negative samples in the dataset. For instance, on the splice dataset where 5.5% of the data is labeled as +, we obtain 5.5% accuracy at τ = 0. For intermediate values of τ the prediction accuracy depends on cleanliness of the dataset measured as the total percentage of the data which lies at the margin. For instance, on the spam dataset for τ = 0, around 0.28% of the training samples were at the margin. This number stabilized to about 0.32% for τ ∈ (0.0, 0.9], leading to very stable classification accuracy, as can be seen from Figure 2.\nClearly, finding the solution for any value of τ ∈ [0, 1] is more time consuming than finding the solution for a fixed τ . To investigate the excess time spent in this endeavor, we compare the time complexity of our DFP algorithm with one single run of LIBLINEAR, a state of the art linear SVM training algorithm which can handle asymmetric classification costs.4 Our second comparator is the LBFGSB algorithm, which can also be used to train a linear SVM for any fixed value of τ . The core functions of LIBLINEAR and LBFGSB are implemented in C++ and Fortran, respectively (We called these functions through their Python wrappers.), while our DPF algorithm is implemented in Python, which is inherently 2 to 5 times slower. Therefore, our CPU time comparison is in favor of LIBLINEAR and LBFGSB.\nRecall that our DFP algorithm invokes any linear SVM solver to find the initial solution,5 and then finds the solution path by constructing K. We compute the ratio of the CPU time spent on constructing K to the average time required by LIBLINEAR to find a solution for a given τ . The averaging is done by running LIBLINEAR (resp. LBFGSB) to compute the solution for τ = 0.1, 0.2, . . . , 0.9. As shown in Table 2 on the diabetes dataset DPF finds about 2×103 kinks, spending 2.8×103 (resp. 28) times of the average LIBLINEAR (resp. LBFGSB) running time. The running time of DPF increases to 3.6× 103 (resp. 69) times of that required by a typical run of LIBLINEAR (resp. LBFGSB) on the splice dataset where it finds over 2× 104 kinks. We found empirically that the number of kinks, | K |, increases with the size of training set, n, but is bounded by n log(n).\n4We called LIBLINEAR with input arguments ’-s 3 -B 1 -e 1e-3 -w1 (2-2τ) -w-1 (2τ) -c 1/(nλ)’.\n5Although in theory this is true, in practice we find that in the extreme case of τ = 0, LIBLINEAR’s performance degrades dramatically. Therefore, we exclusively use LBFGSB as an initial solver\nIt is not surprising that DFP is computationally more expensive than a single run of LIBLINEAR and LBFGSB. But as can be seen in Table 2, after one run of DFP, we can recover the solution for any τ efficiently. For instance, on the spam dataset, this only requires 0.012 seconds, compared to 0.467 seconds (resp. 11.102 seconds) for a single run of LIBLINEAR (resp. LBFGSB)."
    }, {
      "heading" : "7 Conclusions and Outlook",
      "text" : "In this paper we first show that minimizing the asymmetric hinge loss will lead to a quantile classifier which is risk optimal for asymmetric misclassification costs. We then present an algorithm which finds the entire solution path of a quantile classifier in a principled way. Given the entire solution path, we can construct a classifier for any given asymmetric classification cost very efficiently. Admittedly, our numerical experiments are preliminary. Running conjugate gradient repeatedly to find ατM is the main bottleneck in our DFP algorithm. We are exploring decomposition methods, which can take advantage of warm starts to reduce the computational burden. Future work includes extension of our algorithm to quantile regression and to multi-class classification problems."
    }, {
      "heading" : "Acknowledgements",
      "text" : "NICTA is funded by the Australian Government’s Backing Australia’s Ability and the Centre of Excellence programs. This work is also supported by the IST Program of the European Community, under the FP7 Network of Excellence, ICT-216886-NOE."
    }, {
      "heading" : "A Proof of Lemma 2.2",
      "text" : "Proof Let Lx(f) be the risk conditioned on X = x:\nLx(f) = E [((1 + Y )/2− τY )(1− Y f(X))+|X = x] = τ(1− η(x))(1 + f(x))+ + (1− τ)η(x)(1− f(x))+,\nthen we only need to show that f∗(x) minimizes Lx(f) for any fixed x.\nWe first show that if η(x) < τ then the minimizer f∗ satisfies f∗(x) = −1. Suppose not, that is, there exists x0 such that η(x0) < τ but f∗(x0) 6= −1. Let f̃(x) be the same as f∗(x) except that f̃(x0) = −1. Using the shorthand f∗(x0) = f∗, f̃(x0) = f̃ and η(x0) = η, we obtain\nLx0(f ∗) = τ(1− η)(1 + f∗)+ + (1− τ)η(1− f∗)+ ≥ (1− τ)η[(1− f∗)+ + (1 + f∗)+] ≥ 2(1− τ)η = Lx0(f̃),\nwhere the last inequality comes from Jensen’s inequality since (.)+ is a convex function. For the first inequality the bound is achieved only if f∗ ≤ −1; and for the second inequality the bound is achieved only if f∗ ∈ [−1, 1]. Thus when f∗ 6= −1 it leads to a contradiction. A symmetric argument can be used to show that if η(x) > τ then f∗(x) = 1.\nB KKT Optimality Conditions\nThe Lagrangian of the constrained optimization problem (8) takes the form of\nL(α,β,γ) := D(α)− n∑ i=1 βiαi + n∑ i=1 γi(αi − cτyi),\nwhere βi and γi are non-negative Lagrange multipliers. The KKT conditions (Nocedal and Wright, 1999) suggest that at optimum (ατ ,β∗,γ∗) we have\n∇iL(ατ ,β∗,γ∗) = ∇iD(ατ )− β∗i + γ∗i = 0, β∗i α τ i = 0,\nγ∗i (α τ i − cτyi) = 0,\n0 ≤ ατi ≤ cτyi , β ∗ i ≥ 0, γ∗i ≥ 0, ∀i.\nSimple analysis reveals that the above KKT optimality conditions constrain ατ to take the form given in (12)."
    }, {
      "heading" : "C Proof of Proposition 3.1",
      "text" : "Proof Suppose the index sets (11) of ατ remain unchanged for all τ ∈ (τk, τk+1). The linearity of ατ in (τk, τk+1) follows directly from (17). Let = τk+1 − τ , compute ατ+ from ατ via (17), and let τk+1 be chosen in such a way that the membership of an index i changes at ατ+ . This can only happen when ατ+ i takes its boundary values: 0 or c τ+ yi with ∇iD(ατ+ ) = 0, which means either an i ∈ M is about to leave M, or an i /∈ M just moves into M, where M is the margin index set of ατ . We now show that ατ+ is optimal. To show this, we only need to show ατ+ i is optimal. By construction ∇iD(ατ+ ) = 0, and since ατ+ i only takes 0 or cτ+ yi , the KKT optimality conditions (Appendix B) can be easily satisfied with appropriate choices of β∗i and γ∗i , implying that α τ+ i is optimal. Hence, α\nτ+ is optimal. Therefore, we can set ατk+1 = ατ+ , and use it as a starting point to construct subsequent dual solution path via (17). The dual solution path explored in this way is clearly continuous in τ ."
    } ],
    "references" : [ {
      "title" : "A limited memory algorithm for bound constrained optimization",
      "author" : [ "R. Byrd", "P. Lu", "J. Nocedal", "C. Zhu" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Byrd et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Byrd et al\\.",
      "year" : 1995
    }, {
      "title" : "Pattern Classification and Scene Analysis",
      "author" : [ "R.O. Duda", "P.E. Hart", "D.G. Stork" ],
      "venue" : null,
      "citeRegEx" : "Duda et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Duda et al\\.",
      "year" : 2001
    }, {
      "title" : "The entire regularization path for the support vector machine",
      "author" : [ "T. Hastie", "S. Rosset", "R. Tibshirani", "J. Zhu" ],
      "venue" : "JMLR, 5:1391–1415,",
      "citeRegEx" : "Hastie et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2004
    }, {
      "title" : "Training linear SVMs in linear time",
      "author" : [ "T. Joachims" ],
      "venue" : "In Proc. ACM Conf. Knowledge Discovery and Data Mining (KDD). ACM,",
      "citeRegEx" : "Joachims.,? \\Q2006\\E",
      "shortCiteRegEx" : "Joachims.",
      "year" : 2006
    }, {
      "title" : "Support vector machines and the bayes rule in classification",
      "author" : [ "Y. Lin" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "Lin.,? \\Q2002\\E",
      "shortCiteRegEx" : "Lin.",
      "year" : 2002
    }, {
      "title" : "Algorithms for bound constrained quadratic programming problems",
      "author" : [ "J.J. Moré", "G. Toraldo" ],
      "venue" : "Numerische Mathematik,",
      "citeRegEx" : "Moré and Toraldo.,? \\Q1989\\E",
      "shortCiteRegEx" : "Moré and Toraldo.",
      "year" : 1989
    }, {
      "title" : "Combining statistical learning with a knowledge-based approach - a case study in intensive care monitoring",
      "author" : [ "K. Morik", "P. Brockhausen", "T. Joachims" ],
      "venue" : "In Proc. Intl. Conf. Machine Learning,",
      "citeRegEx" : "Morik et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Morik et al\\.",
      "year" : 1999
    }, {
      "title" : "Probabilities for SV machines",
      "author" : [ "J. Platt" ],
      "venue" : "Advances in Large Margin Classifiers,",
      "citeRegEx" : "Platt.,? \\Q2000\\E",
      "shortCiteRegEx" : "Platt.",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "It has been shown (Lin, 2002) that the minimizer of the hinge loss is exactly sign(η(x)−1/2), where η(x) = P (Y = 1|X = x) is the probability of Y = 1 conditioned on X = x.",
      "startOffset" : 18,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "To address such a problem, several methods have been proposed to convert the SVM output into well-calibrated probabilistic scores, such as (Platt, 2000).",
      "startOffset" : 139,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "The dual problem is a quadratic problem with box constrains, which can be solved by various optimization techniques (see e.g., Byrd et al., 1995; Moré and Toraldo, 1989)",
      "startOffset" : 116,
      "endOffset" : 169
    }, {
      "referenceID" : 2,
      "context" : "Perhaps the closest in spirit to our paper is the work of Hastie et al. (2004), who studied the influence of the regularization constant λ on the generalization performance of a binary SVM.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "Although SVM classifiers with built-in asymmetric misclassification costs have been applied to classification problems that are characterized by highly skewed training data and to problems arisen from medical diagnosis (Veropoulos et al., 1999; Morik et al., 1999; Grandvalet et al., 2006), no rigorous statistical properties were established.",
      "startOffset" : 219,
      "endOffset" : 289
    }, {
      "referenceID" : 3,
      "context" : "Hence, optimization software such as SVM (Joachims, 2006) and LIBLINEAR (Hsieh et al.",
      "startOffset" : 41,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "We used the LBFGSB quasi-Newton method of Byrd et al. (1995) to solve the dual problem at Step 3 of the Algorithm 1; and the conjugate gradient method was applied to find ∆αM at Step 13.",
      "startOffset" : 42,
      "endOffset" : 61
    } ],
    "year" : 2009,
    "abstractText" : "A quantile binary classifier uses the rule: Classify x as +1 if P (Y = 1|X = x) ≥ τ , and as −1 otherwise, for a fixed quantile parameter τ ∈ [0, 1]. It has been shown that Support Vector Machines (SVMs) in the limit are quantile classifiers with τ = 1 2 . In this paper, we show that by using asymmetric cost of misclassification SVMs can be appropriately extended to recover, in the limit, the quantile binary classifier for any τ . We then present a principled algorithm to solve the extended SVM classifier for all values of τ simultaneously. This has two implications: First, one can recover the entire conditional distribution P (Y = 1|X = x) = τ for τ ∈ [0, 1]. Second, we can build a risk-agnostic SVM classifier where the cost of misclassification need not be known apriori. Preliminary numerical experiments show the effectiveness of the proposed algorithm.",
    "creator" : "TeX"
  }
}