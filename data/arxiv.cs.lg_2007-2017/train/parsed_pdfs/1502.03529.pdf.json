{
  "name" : "1502.03529.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Scalable Stochastic Alternating Direction Method of Multipliers",
    "authors" : [ "Shen-Yi Zhao", "Wu-Jun Li", "Zhi-Hua Zhou" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The alternating direction method of multipliers (ADMM) [1] is proposed to solve the problems which can be formulated as follows:\nmin x,y P (x,y) = f(x) + g(y) (1)\ns.t. Ax + By = c,\nwhere f(·) and g(·) are convex functions, A ∈ Rl×p and B ∈ Rl×q are matrices, c ∈ Rl is a vector, x ∈ Rp and y ∈ Rq are variables to be optimized (learned). By splitting the objective function P (·) into two parts f(·) and g(·), ADMM provides a flexible framework to handle many optimization problems. For example, by taking f(x) to be the square loss or logistic loss on the training set, g(y) to\nar X\niv :1\n50 2.\n03 52\n9v 3\n[ cs\n.L G\n] 2\nbe the L1-norm and the constraint to be x− y = 0, we can get the well-known lasso formulation [2]. Similarly, we can take more complex constraints than that in lasso to get more complex regularization problems such as the structured sparse regularization problems [3, 4]. Compared with other optimization methods such as gradient decent, ADMM has demonstrated better performance in many complex regularization problems [3, 4]. Furthermore, ADMM can be easily adapted to solve large-scale distributed problems [1]. Hence, ADMM has been widely used in a large variety of areas [1].\nDeterministic (batch) ADMM needs to visit all the samples in each iteration. Existing works have shown that batch ADMM is not efficient enough for big data applications with a large amount of training samples [5, 6]. Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4]. Hence, stochastic ADMM has become a hot research topic and attracted much attention [7, 4].\nOnline alternating direction method (OADM) [5] is the first online ADMM method. There is only regret analysis in OADM, based on which we can find that if OADM is adapted for stochastic settings with finite samples, the convergence rate of OADM is O(1/ √ T ) for general convex problems where f(x) and g(y) are convex but not necessarily to be strongly convex. Here, T is the number of iterations. Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4]. STOC-ADMM, RDA-ADMM, OPG-ADMM and OS-ADMM achieve a convergence rate of O(1/ √ T ) for general convex problems, worse than batch ADMM that has a convergence rate of O(1/T ) [8]. Different from STOC-ADMM, RDA-ADMM, OPG-ADMM and OS-ADMM, SA-ADMM [4] can achieve a convergence rate of O(1/T ) for general convex problems by using historic gradients to approximate the full gradients in each iteration. Thus, SA-ADMM is the only one which is scalable in terms of convergence rate (computation cost). However, SA-ADMM requires an extra memory which is typically very large to store the historic gradients on all samples, making it not scalable in terms of storage cost.\nIn this paper, we propose a novel method, called scalable stochastic ADMM (SCAS-ADMM), for large-scale optimization and learning problems. The main contributions of SCAS-ADMM are outlined as follows:\n• SCAS-ADMM achieves the same convergence rate of O(1/T ) for general convex problems as the best existing stochastic ADMM method (SAADMM) and batch ADMM. Therefore, SCAS-ADMM is scalable in terms of convergence rate (computation cost).\n• Different from SA-ADMM, SCAS-ADMM does not need an extra memory to store the historic gradients on all samples. Therefore, SCAS-ADMM is scalable in terms of memory (storage) cost.\n• Experimental results on graph-guided fused lasso [9] show that SCASADMM can achieve state-of-the-art performance in real applications."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Convex and Smooth Functions",
      "text" : "We use ‖a‖ to denote the Euclidean (L2) norm of a. A function h(·) is called ν′hLipschitz continuous if: ∃ν′h > 0, ∀a,b, ‖h(b)− h(a)‖ ≤ ν′h ‖b− a‖. Assume h(·) is differentiable, and let ∇h(a) denote the gradient of h(·) at a. A function h(·) is called convex if: ∀a,b, h(b) ≥ h(a) + [∇h(a)]T (b− a). Assume h(·) is convex and differentiable. h(·) is called νh-smooth if: ∃νh > 0,∀a,b, h(b) ≤ h(a) + [∇h(a)]T (b − a) + νh2 ‖b− a‖ 2 . This is equivalent to say that ∇h(·) is νh-Lipschitz continuous. Here, νh is called the Lipschits constant of h(·). A function h(·) is called strongly convex if: ∃µh > 0, ∀a,b, h(b) ≥ h(a) + [∇h(a)]T (b− a) + µh2 ‖b− a‖ 2 . A function h(·) is called general convex if h(·) is convex but not necessarily to be strongly convex."
    }, {
      "heading" : "2.2 ADMM",
      "text" : "ADMM solves (1) based on the augmented Lagrangian function:\nL(x,y,β) =f(x) + g(y) + βT (Ax + By − c) + ρ 2 ‖Ax + By − c‖2 , (2)\nwhere β is a vector of Lagrangian multipliers, and ρ > 0 is a penalty parameter. Just like the Gauss-Seidel method, ADMM iteratively updates the variables in an alternating manner as follows [1]:\nxt+1 = arg min x L(x,yt,βt), (3)\nyt+1 = arg min y L(xt+1,y,βt), (4)\nβt+1 = βt + ρ(Axt+1 + Byt+1 − c), (5)\nwhere xt, yt and βt denote the values of x, y and β at the tth iteration, respectively.\nIn the regularized risk minimization problem which this paper will focus on, the function f(x) usually has the following structure:\nf(x) = 1\nn n∑ i=1 fi(x), (6)\nwhere x denotes the model parameter, n is the number of training samples, and each fi(·) is the empirical loss caused by the ith sample. The function g(y) is usually a regularization term. For example, fi(x) = log(1 + exp\n−biaTi x) in logistic regression (LR), and fi(x) = (bi − aTi x)2 in least square, where (ai, bi) is the ith training sample with the class label bi. Taking g(y) = ‖y‖1 and the\nconstraint y = x, we can get the lasso formulation [2]. Similarly, we can get more complex regularization problems by taking more complex constraints like y = Ax.\nUnless otherwise stated, f(x) of the problem we are trying to solve in this paper is defined in (6). Then (3) becomes:\nxt+1 = arg min x { 1 n n∑ i=1 fi(x) + (βt) T (Ax + Byt − c) + ρ 2 ‖Ax + Byt − c‖2}.\n(7)\nFrom (7), it is easy to see that ADMM needs to visit all the n samples in each iteration. Hence, this version of ADMM is also called batch ADMM or deterministic ADMM. Some works [5, 8] have proved that the above batch ADMM has a convergence rate O(1/T ) for general convex problems where f(x) and g(y) are convex but not necessarily to be strongly convex, where T is the number of iterations.\nDifferent from batch ADMM, stochastic (online) ADMM visits only one sample or a mini-batch of samples in each iteration. Recent works have shown that stochastic ADMM can achieve better performance than batch ADMM to handle large-scale datasets in terms of computation complexity and accuracy [5, 6]. The computation of (4) and (5) for both batch ADMM and stochastic ADMM are the same, which can typically be easily completed. Hence, different stochastic ADMM methods mainly focus on proposing different solutions for (7)."
    }, {
      "heading" : "3 Scalable Stochastic ADMM",
      "text" : "In this section, we present the details of our SCAS-ADMM, which is scalable in terms of both convergence rate and storage cost. Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15]. But different from SVRG, our SCAS-ADMM can be used to model more complex problems with equality constraints.\nIn this paper, we assume that f(·) and all the {fi(·)} are vf -smooth. For g(·), we only assume it to be convex, but not necessarily to be smooth or Lipschitz continuous. This is a reasonable assumption for many machine learning problems, such as the lasso with logistic loss or square loss. The proof of the theorems of this paper can be found from the Appendix in the supplementary materials."
    }, {
      "heading" : "3.1 General Convex Problems",
      "text" : "In the general convex problems, f(·) is vf -smooth and general convex but not necessarily to be strongly convex."
    }, {
      "heading" : "3.1.1 Algorithm",
      "text" : "As in existing stochastic ADMM methods [6, 4], the update rules for y and β are still the same as those in (4) and (5). We only need to design a new strategy to update x. The algorithm for our SCAS-ADMM is briefly presented in Algorithm 1. It changes (7) to be:\nxt+1 = ∑Mt−1 m=0 wm Mt , (8)\nwhere Mt is a parameter denoting the number of iterations in the inner loop, and\nw0 = xt,\nwm+1 = πX (wm − ηt[∇fim(wm)−∇fim(w0) + zt + ATβt + ρAT (Awm + Byt − c)]), (9)\nwith im being an index randomly sampled from {1, 2, · · · , n}, zt = ∇f(xt) = 1 n ∑n i=1∇fi(xt) being the full gradient at xt, X being the domain of x, and πX (·) denoting the projection operation onto the domain X .\nAlgorithm 1 SCAS-ADMM for general convex problems\nInitialize: (x0,y0,β0), a convex set X for t = 0 to T − 1 do\nCompute zt = ∇f(xt) = 1n ∑n i=1∇fi(xt); w0 = xt; s = w0; for m = 0 to Mt − 2 do\nRandomly select an im from {1, 2, · · · , n}; wm+1 = πX (wm−ηt[∇fim(wm)−∇fim(w0)+zt+ATβt+ρAT (Awm+Byt− c)]); s = s + wm+1;\nend for xt+1 =\n1 Mt\ns; yt+1 = arg miny L(xt+1,y,βt); βt+1 = βt + ρ(Axt+1 + Byt+1 − c);\nend for Output: x̄T =\n1 T ∑T t=1 xt, ȳT = 1 T ∑T t=1 yt\nCompared with SVRG [15], the update rule in (9) has an extra vector ATβt+ ρAT (Awm + Byt − c) = ρATAwm + AT (βt + ρ(Byt − c)). If matrix A = 0, which means By = c, x and y are independent. Then Algorithm 1 will degenerate to SVRG since we only need to solve the minimization problem about f(x) and g(y) separately. We can find that SCAS-ADMM is more general than SVRG since it can solve the minimization problem with more complex equality constraints.\nBesides the memory to store A and B, the memory to store zt, w0, s, and wm+1 is only O(p), where p is the number of parameters, i.e., the length of vector\nx. Furthermore, it only needs some other memory to store {xt|t = 0, 1, · · · , T} and {yt|t = 0, 1, · · · , T}. This memory cost is typically small because T is not too large in practice. For example, T = 15 is enough for SCAS-ADMM to achieve satisfactory accuracy in our experiments which will be presented in Section 4. Furthermore, we can also find that SCAS-ADMM does not need to store the historic gradients for all samples which are used in SA-ADMM. Hence, SCAS-ADMM is scalable in terms of storage cost."
    }, {
      "heading" : "3.1.2 Convergence Analysis",
      "text" : "We call a set X is bounded by D if it satisfies: sup x,x′∈X ‖x− x′‖ ≤ D, where D is a constant. Assume we have got (xt,yt,βt), and we define:\nL(x) =L(x,yt,βt). (10)\nWe can get the following convergence theorem.\nTheorem 1. Assume the optimal solution of (2) is (x∗,y∗,β∗), X is bounded by D and contains x∗, f(x) and all the functions {fi(x)} are general convex and νf -smooth, and the function g(y) is convex. We have the following convergence result for Algorithm 1:\nE [f(x̄T ) + g(ȳT )− f(x∗)− g(y∗) + γ ‖Ax̄T + BȳT − c‖]\n≤ 1 T T−1∑ t=0 [ D2 2Mtηt + ηt(ν 2 LD 2 +G2t ) ] + ρ 2T ‖y0 − y∗‖2H + 1 ρT (‖β0‖2 + γ2),\n(11)\nwhere H = BTB, ‖x‖2H = xTHx, γ > 0 is a constant, νL is the Lipschitz constant of L(x), and Gt = ‖∇L(xt)‖.\nLet t = D2\n2Mtηt + ηt(ν 2 LD 2 + G2t ). To make f(x̄T ) + g(ȳT ) converge to f(x∗) + g(y∗), we need to make sure that ∑T−1 t=0 t is bounded or not too large. By taking ηt = 1\n(ν2LD 2+G2t )(t+1)\nδ , Mt = (ν 2 LD 2 +G2t )(t+ 1) 2δ, we have:\n• If δ > 1, then ∑∞ t=0 t is a constant, which means that f(x̄T ) + g(ȳT )\nconverges to f(x∗) + g(y∗) with a convergence rate of O( 1 T ).\n• If δ = 1, then ∑T−1 t=0 t = O(log T ), which means that f(x̄T ) + g(ȳT )\nconverges to f(x∗) + g(y∗) with a convergence rate of O( log T T ).\nHence, by choosing δ > 1, we can get a convergence rate O( 1T ) for our SCASADMM on general convex problems, which is the same as the best convergence rate achieved by existing stochastic ADMM method (SA-ADMM)."
    }, {
      "heading" : "3.2 Strongly Convex Problems",
      "text" : "In Algorithm 1, with the increase of t, the iteration number of the inner loop Mt needs to be increased and the step size ηt needs to be decreased. This might cause large computation when T gets large. We can get a better algorithm when f(x) in (1) is strongly convex."
    }, {
      "heading" : "3.2.1 Algorithm",
      "text" : "When f(x) is strongly convex, our SCAS-ADMM is briefly presented in Algorithm 2. We can find that Algorithm 2 is similar to Algorithm 1, but with constant values for Mt and ηt.\nAlgorithm 2 SCAS-ADMM for strongly convex problems\nInitialize: (x0,y0,β0), r = 2η − η 1− νLη\n2\n, s = η 1− νLη\n2 , a convex set X ; for t = 0 to T − 1 do\nCompute zt = ∇f(xt) = 1n ∑n i=1∇fi(xt); w0 = xt; s = 0; for m = 0 to M − 1 do\nRandomly select an im from {1, 2, · · · , n}; wm+1 = πX (wm−η[∇fim(wm)−∇fim(w0)+zt+ATβt+ρAT (Awm+Byt− c)]); w̃m+1 =\n1 2η\n(rwm + swm+1); s = s + w̃m+1;\nend for xt+1 =\n1 M s;\nyt+1 = arg miny L(xt+1,y,βt); βt+1 = βt + ρ(Axt+1 + Byt+1 − c);\nend for Output: x̄T =\n1 T ∑T t=1 xt, ȳT = 1 T ∑T t=1 yt"
    }, {
      "heading" : "3.2.2 Convergence Analysis",
      "text" : "Theorem 2. Assume the optimal solution of (2) is (x∗,y∗,β∗), all the functions {fi(x)} are general convex and νf -smooth, f(x) is strongly convex and νf -smooth, and g(y) is convex. We have the following result:\nE [f(x̄T ) + g(ȳT )− f(x∗)− g(y∗) + γ ‖Ax̄T + BȳT − c‖]\n≤µf 4T ‖x0 − x∗‖2 + ρ 2T ‖y0 − y∗‖2H + 1 ρT (‖β0‖2 + γ2), (12)\nwhere H = BTB, and γ > 0 is a constant.\nIn this case, we can set M and η to be constants. Please note that in the proof of Theorem 2, M and η need to satisfy the following conditions: η − νLη 2\n2 > 0, (4ν 2 L + µfνL 2 )η + ρλ1 ≤ µL, α 2Mη + 2ν2Lη 2−vLη ≤ µf 4 , where λ1 denotes\nthe maximum eigenvalue of ATA, and α = 1 − ρλ1s2 − µfs 4 . Different from\nAlgorithm 1, we do not need the convex set X in Algorithm 2 to be bounded or we do not even need such a set for unconstrained problems."
    }, {
      "heading" : "3.3 Comparison to Related Methods",
      "text" : "We compare our SCAS-ADMM to other stochastic ADMM methods in terms of three key factors: penalty term linearization, convergence rate on general convex problems and memory cost. The matrix inversion ( 1ηt I+ρA TA)−1 can be avoided by linearizing the penalty term ρ2 ‖Ax + By − c‖ 2\n[4]. Hence, penalty term linearization can be used to decrease computation cost. The comparison results are summarized in Table 1, where SA-IU-ADMM is a variant of SAADMM with penalty term linearization. Please note that A ∈ Rl×p, B ∈ Rl×q, x ∈ Rp, y ∈ Rq, c ∈ Rl, p is the number of parameters to learn, and n is the number of training samples.\nIt is easy to see that only SCAS-ADMM can achieve the best performance in terms of both convergence rate and memory cost. Other methods either achieve only sub-optimal convergence rate, or need more memory than SCAS-ADMM. In particular, SA-ADMM and SA-IU-ADMM need an extra memory as large as O(np) to store the historic gradients for all samples. Typically, n is very large in big data applications. Furthermore, SCAS-ADMM can also avoid the matrix inversion by linearizing the penalty term. Hence, SCAS-ADMM does be salable in terms of both computation cost and memory cost."
    }, {
      "heading" : "4 Experiments",
      "text" : "As in [6, 7, 4], we evaluate our method on the generalized lasso model [16] which can be formulated as follows:\nmin x\n1\nn n∑ i=1 fi(x) + λ ‖Ax‖1 , (13)\nwhere fi(x) is the logistic loss, A is a matrix to specify the desired structured sparsity pattern for x, and λ is the regularization hyper-parameter. We can get\ndifferent models like fused lasso and wavelet smoothing by specifying different A. In this paper, we focus on the graph-guided fused lasso [9] which is also used in [4]. As in [6, 4], we use sparse inverse covariance selection method [17] to get a graph matrix (sparsity pattern) G, based on which we can get A = [G; I]. In general, both G and A are sparse.\nWe can formulate (13) with the ADMM framework:\nmin x,y\nP (x,y) = 1\nn n∑ i=1 fi(x) + g(y), (14)\ns.t. Ax− y = 0,\nwhere g(y) = λ ‖y‖1."
    }, {
      "heading" : "4.1 Baselines and Datasets",
      "text" : "Three representative ADMM methods are adopted as baselines for comparison. They are:\n• Batch-ADMM [1]: The deterministic (batch) variant of ADMM which uses (7) to directly update x by visiting all training samples in each iteration.\n• STOC-ADMM [6]: The stochastic ADMM variant without using historic gradient for optimization, which has a convergence rate of O(1/ √ T ) for\ngeneral convex problems and O(log T/T ) for strongly convex problems.\n• SA-ADMM [4]: The stochastic ADMM variant by using historic gradient to approximate the full gradient, which has a convergence rate of O(1/T ) for general convex problems.\nPlease note that other methods, such as OPG-ADMM, RDA-ADMM and OSADMM, are not adopted for comparison because they have similar convergence rate as STOC-ADMM. Furthermore, both theoretical and empirical results have shown that SA-ADMM can outperform other methods like RDA-ADMM and OPG-ADMM [4]. The variant of SA-ADMM, SA-IU-ADMM, is also not adopted for comparison because it has similar performance as SA-ADMM [4].\nAlthough the Mt in Algorithm 1 should be increased as t increases, we simply set Mt = n in our experiments because SCAS-ADMM can also achieve good performance with this fixed value for Mt. Similarly, we set M = n in Algorithm 2.\nAs in [4], four widely used datasets are adopted to evaluate our method and other baselines. They are a9a, covertype, rcv1 and sido. All of them are for binary classification tasks. The detailed information about these datasets can be found in Table 2.\nAs in [4], for each dataset we randomly choose half of the samples for training and use the rest for testing. This random partition is repeated for 10 times and the average values are reported. The hyper-parameter λ in (14) is set by using the same values in [4], which are also listed in Table 2. We adopt the same strategy as that in [4] to set the hyper-parameters ρ in (2) and the\nstepsize. More specifically, we randomly choose a small subset of 500 samples from the training set, and then choose the hyper-parameters which can achieve the smallest objective value after running 5 data passes for stochastic methods or 100 data passes (iterations) for batch methods. As in [4], we use y(x̄T ) = Ax̄T to replace ȳT since the methods cannot necessarily guarantee that Ax̄T = ȳT .\nAll the experiments are conducted on a workstation with 12 Intel Xeon CPU cores and 64G RAM."
    }, {
      "heading" : "4.2 Convergence Results",
      "text" : "As in [4], we study the variation of the objective value on training set and the testing loss versus the number of effective passes over the data. For all methods, one effective pass over the data means n samples are visited. More specifically, one effective pass refers to one iteration in batch ADMM. For stochastic ADMM methods which visit one sample in each iteration, one effective pass refers to n iterations. For SCAS-ADMM, we set Mt = n and each iteration of the outer loop needs to visit 2n training samples. Hence, each iteration of the outer loop will contribute two effective passes. Although different methods will visit different numbers of samples in each iteration, we can see that the number of effective passes over the data is a good metric for fair comparison because it measures the computation costs of different methods in a unified way.\nFigure 1 shows the results for general convex problems with fi(x) being the logistic loss. Please note that the number of recorded points on the curve of SCAS-ADMM is half of those for other methods because each iteration of the outer loop of SCAS-ADMM will contribute two effective passes. As stated above, it is still fair to compare different methods with respect to the number of effective passes. In Figure 1, all the points with the same x-axis value from different curves have the same number of effective passes. Hence, for two points with the same x-axis value from any two different curves, the point with smaller y-axis value is better than the other one. We can find that all the stochastic methods outperform the Batch-ADMM in terms of both training speed and testing accuracy. SCAS-ADMM and SA-ADMM outperform STOC-ADMM, which is consistent with the theoretical analysis about convergence rate. Our SCASADMM can achieve comparable performance as SA-ADMM, which empirically verifies our theoretical result that SCAS-ADMM has the same convergence rate of O(1/T ) as SA-ADMM.\nBy adding a small L2 regularization term to the logistic loss, we can get strongly convex problems. Figure 2 shows the results for strongly convex problems. Once again, we can observe similar phenomenon as that in Figure 1. In particular, our SCAS-ADMM can achieve comparable convergence rate as SA-ADMM.\nAs for the memory (storage) cost, it is obvious that SCAS-ADMM needs much less memory than SA-ADMM from the theoretical analysis in Table 1. Hence, we do not empirically compare between them."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we have proposed a new stochastic ADMM method called SCASADMM, which can achieve the same convergence rate as the best existing stochastic ADMM method SA-ADMM on general convex problems. Furthermore, it costs much less memory than SA-ADMM. Hence, SCAS-ADMM is scalable in terms of both convergence rate and storage cost."
    }, {
      "heading" : "A Notations for Proof",
      "text" : "We let\nvm,t = ∇fim(wm)−∇fim(w0) + zt, (15) bm,t = βt + ρ(Awm + Byt − c), (16) pm,t = vm,t + A Tbm,t. (17)\nThen the update rule for wm+1 in the inner loop of Algorithm 1 can be rewritten as\nwm+1 = πX (wm − ηt(vm,t + ATbm,t)) = πX (wm − ηtpm,t). (18)\nAssume we have got (xt,yt,βt), and we define:\nL(x) =L(x,yt,βt), (19)\nLi(x) =fi(x) + g(yt) + βTt (Ax + Byt − c) + ρ\n2 ‖Ax + Byt − c‖2 . (20)"
    }, {
      "heading" : "B Lemmas for the Proof of Theorem 1",
      "text" : "Lemma 1. If f(x) is νf -smooth, then ∃νL > 0 that makes L(x) be νL-smooth.\nProof. According to the definition about νf -smooth, ∀a,b, we have ‖∇L(b)−∇L(a)‖ = ∥∥∇f(b)−∇f(a) + ρATA(b− a)∥∥\n≤ ‖∇f(b)−∇f(a)‖+ ∥∥ρATA(b− a)∥∥\n≤ νf ‖b− a‖+ ∥∥ρATA(b− a)∥∥\n≤ νf ‖b− a‖+ ρ √ λA ‖b− a‖\n= (νf + ρ √ λA) ‖b− a‖ ,\nwhere λA ≥ 0 is the largest eigenvalue of ATAATA. Hence, for any value of νL ≥ (νf + ρ √ λA), we can see that L(x) is νLsmooth.\nWe can find that νL is only determined by f(x), matrix A and the penalty parameter ρ, but has nothing to do with g(yt), yt, B and βt.\nThen, we have the following lemma about the variance of pm,t.\nLemma 2. The variance of pm,t satisfies:\nE(‖pm,t‖2) ≤ 2ν2LD2 + 2G2t , (21)\nwhere D is the bound of the domain of x, νL is the Lipschitz constant of the function L(x) defined in (19), and Gt = ‖∇L(xt)‖.\nProof. According to (17), we have\npm,t =vm,t + A Tbm,t\n=∇fim(wm) + ATbm,t − fim(w0)−ATb0,t + zt + ATb0,t =∇Lim(wm)−∇Lim(w0) +∇L(w0).\nThen we have:\nE(‖pm,t‖2) = 1\nn n∑ i=1 ‖∇Li(wm)−∇Li(w0) +∇L(w0)‖2\n≤ 2 n n∑ i=1 {‖∇Li(wm)−∇Li(w0)‖2 + ‖∇L(w0)‖2}\n≤{ 2 n n∑ i=1 ‖∇Li(wm)−∇Li(w0)‖2}+ 2 ‖∇L(w0)‖2 ≤2ν2LD2 + 2G2t .\nPlease note that w0 = xt, and we use the Lipschitz definition to get the result:\n‖∇Li(wm)−∇Li(w0)‖2 ≤ ν2L ‖wm −w0‖ 2 ≤ ν2LD2.\nLemma 3. For the estimation of xt+1, we have the following result:\nE [ f(xt+1)− f(x) + (ATαt+1)T (xt+1 − x) ] ≤ D 2\n2Mtηt + ηt(ν\n2 LD 2 +G2t ), (22)\nwhere αt+1 = βt + ρ(Axt+1 + Byt − c).\nProof. Since X is convex, we have: ∀x ∈ X ,\n‖wm+1 − x‖2 ≤ ‖wm − ηtpm,t − x‖2 (23)\n= ‖wm − x‖2 − 2ηtpTm,t(wm − x) + η2t ‖pm,t‖ 2 .\nFurthermore, it is easy to prove that E [vm,t] = ∇f(wm). Based on the results in Lemma 2, we can get the expectation on (23):\nE [ ‖wm+1 − x‖2 ] (24)\n≤E [ ‖wm − x‖2 ] − 2ηtE[(∇f(wm) + ATbm,t)T (wm − x)] + η2tE(‖pm,t‖ 2 )\n≤E [ ‖wm − x‖2 ] + η2t (2ν 2 LD 2 + 2G2t )− 2ηtE[f(wm)− f(x) + (ATbm,t)T (wm − x)].\nSumming up (24) from m = 0 to Mt − 1, we can get:\n2ηt Mt−1∑ m=0 E[f(wm)− f(x) + (ATbm,t)T (wm − x)]\n≤‖w0 − x‖2 +Mtη2t (2ν2LD2 + 2G2t )− E [ ‖wMt − x‖ 2 ]\n≤‖w0 − x‖2 +Mtη2t (2ν2LD2 + 2G2t ) ≤D2 +Mtη2t (2ν2LD2 + 2G2t ).\nWe can prove that f(wm) − f(x) + (ATbm,t)T (wm − x) is convex in wm. Furthermore, we have xt+1 =\n1 Mt ∑Mt−1 m=0 wm. By using the Jensen’s inequality,\nwe have:\n2ηtMtE [ f(xt+1)− f(x) + (ATαt+1)T (xt+1 − x) ] ≤2ηt\nMt−1∑ m=0 E[f(wm)− f(x) + (ATbm,t)T (wm − x)]\n≤D2 +Mtη2t (2ν2LD2 + 2G2t ),\nwhere αt+1 = βt + ρ(Axt+1 + Byt − c). Then, we can get:\nE [ f(xt+1)− f(x) + (ATαt+1)T (xt+1 − x) ] ≤ D 2\n2Mtηt + ηt(ν\n2 LD 2 +G2t ). (25)\nAccording to the results in [4], we have the following Lemma 4 and Lemma 5 about the estimation of yt+1 and αt+1.\nLemma 4. For the estimation of yt+1, we have: E [ g(yt+1)− g(y) + (BTαt+1)T (yt+1 − y) ] (26)\n≤ ρ 2 E [ ‖yt − y‖2H − ‖yt+1 − y‖ 2 H − ‖yt − yt+1‖ 2 H ] ,\nwhere αt+1 = βt + ρ(Axt+1 + Byt − c), H = BTB, and ‖y‖2H = yTHy.\nLemma 5. For the estimation of αt+1, we have: E [ −(Axt+1 + Byt+1 − c)T (αt+1 −α) ] (27)\n≤ 1 2ρ\nE [ ‖βt −α‖2 − ‖βt+1 −α‖2 ] + ρ 2 E [ ‖yt − yt+1‖2H ] ,\nwhere αt+1 = βt + ρ(Axt+1 + Byt − c), H = BTB, and ‖y‖2H = yTHy.\nThe proof of Lemma 4 and Lemma 5 can be directly derived from the results in [4], which is omitted here for space saving."
    }, {
      "heading" : "C Proof of Theorem 1",
      "text" : "Proof. Let u =  xy α , ut =  xtyt\nαt\n, ūT = 1T ∑Tt=1 ut, and F (u) =  ATαBTα −(Ax + By − c) . Summing up the equations in (22), (26) and (27), we have:\nE [ P (xt+1,yt+1)− P (x,y) + F (ut+1)T (ut+1 − u) ] (28)\n≤ D 2\n2Mtηt + ηt(ν\n2 LD 2 +G2t )\n+ ρ 2 E [ ‖yt − y‖2H − ‖yt+1 − y‖ 2 H − ‖yt − yt+1‖ 2 H ] + 1 2ρ E [ ‖βt −α‖2 − ‖βt+1 −α‖2 ] + ρ 2 E [ ‖yt − yt+1‖2H ] .\nIt is easy to prove that P (xt,yt)−P (x,y)+F (ut)T (ut−u) is convex in (xt,yt). Moreover, we have x̄T = 1 T ∑T t=1 xt and ȳT = 1 T ∑T t=1 yt. By using the Jensen’s inequality, we have:\nP (x̄T , ȳT )− P (x,y) + F (ūT )T (ūT − u) (29)\n≤ 1 T T∑ t=1 [ P (xt,yt)− P (x,y) + F (ut)T (ut − u) ] .\nSumming up (28) from t = 0 to T − 1, and using the result in (29), we have:\nE [ P (x̄T , ȳT )− P (x,y) + F (ūT )T (ūT − u) ] ≤ 1 T T−1∑ t=0 E [ P (xt+1,yt+1)− P (x,y) + F (ut+1)T (ut+1 − u)\n] ≤ 1 T T−1∑ t=0 [ D2 2Mtηt + ηt(ν 2 LD 2 +G2t )\n] + ρ\n2T ‖y0 − y‖2H +\n1\n2ρT ‖β0 −α‖2 . (30)\nThe result in (30) is satisfied for any (x,y,α). In particular, if we take x = x∗, y = y∗ and α = γ\nAx̄T+BȳT−c ‖Ax̄T+BȳT−c‖ , we have:\nE [P (x̄T , ȳT )− P (x∗,y∗) + γ ‖Ax̄T + BȳT − c‖] (31)\n≤ 1 T T−1∑ t=0 [ D2 2Mtηt + ηt(ν 2 LD 2 +G2t ) ] + ρ\n2T ‖y0 − y∗‖2H +\n1\nρT (‖β0‖2 + γ2)."
    }, {
      "heading" : "D Lemmas for the Proof of Theorem 2",
      "text" : "Lemma 6. The variance of pm,t satisfies:\n∀x, E(‖pm,t‖2) ≤ dm + ‖∇L(wm)‖2\n≤ 2ν2L ‖wm − x‖ 2 + 2ν2L ‖w0 − x‖ 2 + ‖∇L(wm)‖2 . (32)\nwhere dm , 1n ∑n i=1 ‖∇Li(wm)−∇Li(w0)‖ 2 .\nProof. ∀x\nE(‖pm,t‖2) = 1\nn n∑ i=1 ‖∇Li(wm)−∇Li(w0) +∇L(w0)‖2\n= ‖∇L(wm)‖2 − ‖∇L(wm)−∇L(w0)‖2 + 1\nn n∑ i=1 ‖∇Li(wm)−∇Li(w0)‖2\n≤‖∇L(wm)‖2 + 1\nn n∑ i=1 ‖∇Li(wm)−∇Li(w0)‖2\n≤ν2L ‖wm −w0‖ 2 + ‖∇L(wm)‖2 ≤2ν2L ‖wm − x‖ 2 + 2ν2L ‖w0 − x‖ 2 + ‖∇L(wm)‖2 .\nLemma 7. If η − νLη 2\n2 > 0, we have the following result for the variance of ∇L(wm):\n‖∇L(wm)‖2 ≤ 1\nη − νLη22 (L(wm)− E[L(wm+1)]) +\nνLη\n2− νLη dm. (33)\nProof. Since L(w) is convex in w, we can get\nL(wm+1) ≤ L(wm) +∇L(wm)T (wm+1 −wm) + νL 2 ‖wm+1 −wm‖2 .\nTaking expectation on both sides of the above equation, we get\nE[L(wm+1)] ≤ L(wm)− η ‖∇L(wm)‖2 + νLη\n2\n2 E(‖pm,t‖2).\nAccording to Lemma 6, we have\nE[L(wm+1)] ≤ L(wm)− η ‖∇L(wm)‖2 + νLη\n2\n2 (dm + ‖∇L(wm)‖2).\nThen we have\n(η − νLη 2\n2 ) ‖∇L(wm)‖2 ≤ L(wm)− E[L(wm+1)] +\nνLη 2\n2 dm. (34)\nChoosing a small η such that η − νLη 2\n2 > 0, we have\n‖∇L(wm)‖2 ≤ 1\nη − νLη22 (L(wm)− E[L(wm+1)]) +\nνLη\n2− νLη dm.\nLemma 8. We have the following result:\nE(‖wm+1 − x‖2) + 2η∇L(wm)T (wm − x) + η\n1− νLη2 (E[L(wm+1)]− L(x))\n≤‖wm − x‖2 + η\n1− νLη2 (L(wm)− L(x)) +\n2η2\n2− νLη dm.\nProof. From (18), we can get ∀x,\n‖wm+1 − x‖2 ≤ ‖wm − x‖2 − 2ηpTm,t(wm − x) + η2 ‖pm,t‖ 2 .\nThen, we have\nE(‖wm+1 − x‖2) ≤ ‖wm − x‖2 − 2η∇L(wm)T (wm − x) + η2E(‖pm,t‖2).\nAccording to Lemma 6 and Lemma 7, we have\nE(‖wm+1 − x‖2) + 2η∇L(wm)T (wm − x) ≤ ‖wm − x‖2 + η2dm + η2 ‖∇L(wm)‖2\n≤ ‖wm − x‖2 + η2dm + η\n1− νLη2 (L(wm)− E[L(wm+1)]) +\nνLη 3\n2− νLη dm.\nThen, we can get\nE(‖wm+1 − x‖2) + 2η∇L(wm)T (wm − x) + η\n1− νLη2 (E[L(wm+1)]− L(x))\n≤‖wm − x‖2 + η\n1− νLη2 (L(wm)− L(x)) +\n2η2\n2− νLη dm.\nLemma 9. Let λ1 denote the maximum eigenvalue of A TA, ∀w,x,y,β,\nL(w)− L(x) ≥ f(w)− f(x) + (ATb)T (w − x)− ρλ1 2 ‖w − x‖2 ,\nwhere b = β + ρ(Aw + By − c).\nProof. According to the definition L(w) = f(w) + g(y) + βT (Aw + By− c) + ρ 2 ‖Aw + By − c‖ 2 , we have\nL(w)− L(x)\n=f(w)− f(x) + βTA(w − x) + ρ 2 ‖Aw + By − c‖2 − ρ 2 ‖Ax + By − c‖2 =f(w)− f(x) + βTA(w − x) + ρ 2 [ ‖Aw‖2 − ‖Ax‖2 + 2(By − c)TA(w − x) ] =f(w)− f(x) + (AT (β + ρ(By − c))T (w − x) + ρ\n2 (‖Aw‖2 − ‖Ax‖2)\n=f(w)− f(x) + (AT (β + ρ(Aw + By − c))T (w − x)− ρ 2 ‖Aw −Ax‖2 =f(w)− f(x) + (ATb)T (w − x)− ρ 2 ‖Aw −Ax‖2 ≥f(w)− f(x) + (ATb)T (w − x)− ρλ1 2 ‖w − x‖2 .\nLemma 10. If f(w) is strongly convex and µf > ρλ1, we have the following result\nE [ f(xt+1)− f(x) + (ATαt+1)T (xt+1 − x) ] ≤ µf\n4 (‖xt − x‖2 − E ‖xt+1 − x‖2),\n(35)\nwhere αt+1 is the same as that in Lemma 3.\nProof. Note that\nr , 2η − η 1− νLη2 , s , η\n1− νLη2 .\nSince f(w) is strongly convex in w, we can prove that L(w) is also strongly convex in w. Then we have\n∇L(wm)T (wm − x) ≥ L(wm)− L(x) + µL 2 ‖wm − x‖2 . (36)\nBy combining the results in Lemma 8 and (36), we have\nE ‖wm+1 − x‖2 + µLs\n2 ‖wm − x‖2 + r∇L(wm)T (wm − x) + s(E[L(wm+1)]− L(x))\n≤‖wm − x‖2 + 2η2\n2− vLη dm.\nFor convenience, we use\nDm , f(wm)− f(x) + (ATbm,t)T (wm − x),\nwhere the definition of bm,t is in (16). And we also have\n∇L(wm) = ∇f(wm) + ATbm,t, ∇f(wm)T (wm − x) ≥ f(wm)− f(x) + µf 2 ‖wm − x‖2 .\nThen according to Lemma 6 and Lemma 9, we can get\n(1− ρsλ1 2 − µfs 4 )E(‖wm+1 − x‖2) + µLs 2 ‖wm − x‖2\n+ r(Dm + µf 2 ‖wm − x‖2) + sE(Dm+1 + µf 4 ‖wm+1 − x‖2)\n≤(1 + 4ν 2 Lη 2\n2− νLη ) ‖wm − x‖2 +\n4ν2Lη 2\n2− νLη ‖w0 − x‖2 ,\ni.e.,\n(1− ρsλ1 2 − µfs 4 )E(‖wm+1 − x‖2)\n+ r(Dm + µf 4 ‖wm − x‖2) + sE(Dm+1 + µf 4 ‖wm+1 − x‖2)\n≤(1 + 4ν 2 Lη 2 2− νLη − µfr 4 − µLs 2 ) ‖wm − x‖2 + 4ν2Lη 2 2− νLη ‖w0 − x‖2 .\nHere, η need to satisfy the following condition:\n1 + 4ν2Lη 2 2− νLη − µfr 4 − µLs 2 ≤ 1− ρsλ1 2 − µfs 4 ,\ni.e.,\n(4ν2L + µfνL\n2 )η + ρλ1 ≤ µL.\nLet α = 1− ρλ1s2 − µfs 4 . We have\nαE ‖wm+1 − x‖2 + rE(Dm + µf 4 ‖wm − x‖2) + sE(Dm+1 + µf 4 ‖wm+1 − x‖2)\n≤αE ‖wm − x‖2 + 4ν2Lη 2\n2− νLη ‖w0 − x‖2 .\nNote that r + s = 2η, and Dm is convex in wm. We take\nw̃m+1 = 1\n2η (rwm + swm+1),\nwhich is a convex combination of wm and wm+1. Then we have\nαE ‖wm+1 − x‖2 + 2ηE(D̃m+1 + µf 4 ‖w̃m+1 − x‖2) ≤ αE ‖wm − x‖2 +\n4ν2Lη 2\n2− νLη ‖w0 − x‖2 .\n(37)\nwhere D̃m+1 = f(w̃m+1) − f(x) + (AT b̃m+1,t)T (w̃m+1 − x), and b̃m+1,t = β + ρ(Aw̃m+1 + By − c).\nSumming up (37) from m = 0 to M−1, and taking xt+1 = 1M ∑M−1 m=0 w̃m+1,\nwe have\n2MηE(f(xt+1)− f(x) + (ATαt+1)T (xt+1 − x) + µf 4 ‖xt+1 − x‖2)\n≤(α+ 4Mν 2 Lη 2\n2− νLη ) ‖w0 − x‖2 ,\nwhere αt+1 = β + ρ(Axt+1 + By − c). Then, we have\nE [ f(xt+1)− f(x) + (ATαt+1)T (xt+1 − x) ] ≤( α\n2Mη +\n2ν2Lη\n2− νLη ) ‖xt − x‖2 − µf 4 E(‖xt+1 − x‖2)\n≤µf 4 (‖xt − x‖2 − E(‖xt+1 − x‖2)),\nwhere we assume that α2Mη + 2ν2Lη 2−vLη ≤ µf 4 ."
    }, {
      "heading" : "E Proof of Theorem 2",
      "text" : "Proof. Let u =  xy α , ut =  xtyt\nαt\n, ūT = 1T ∑Tt=1 ut, and F (u) =  ATαBTα −(Ax + By − c) . Summing up the equations in (35), (26) and (27), we have:\nE [ P (xt+1,yt+1)− P (x,y) + F (ut+1)T (ut+1 − u) ] ≤µf\n4 (‖xt − x‖2 − E ‖xt+1 − x‖2)\n+ ρ 2 E [ ‖yt − y‖2H − ‖yt+1 − y‖ 2 H − ‖yt − yt+1‖ 2 H ] + 1 2ρ E [ ‖βt −α‖2 − ‖βt+1 −α‖2 ] + ρ 2 E [ ‖yt − yt+1‖2H ] .\nIt is easy to prove that P (xt,yt)−P (x,y)+F (ut)T (ut−u) is convex in (xt,yt). Moreover, we have x̄T = 1 T ∑T t=1 xt and ȳT = 1 T ∑T t=1 yt. By using the Jensen’s inequality, we have:\nP (x̄T , ȳT )− P (x,y) + F (ūT )T (ūT − u)\n≤ 1 T T∑ t=1 [ P (xt,yt)− P (x,y) + F (ut)T (ut − u) ] .\nThen, we have: E [ P (x̄T , ȳT )− P (x,y) + F (ūT )T (ūT − u) ] ≤ 1 T T−1∑ t=0 E [ P (xt+1,yt+1)− P (x,y) + F (ut+1)T (ut+1 − u)\n] ≤µf\n4T ‖x0 − x‖2 +\nρ\n2T ‖y0 − y‖2H +\n1\n2ρT ‖β0 −α‖2 . (38)\nThe result in (38) is satisfied for any (x,y,α). In particular, if we take x = x∗, y = y∗ and α = γ\nAx̄T+BȳT−c ‖Ax̄T+BȳT−c‖ , we have:\nE [P (x̄T , ȳT )− P (x∗,y∗) + γ ‖Ax̄T + BȳT − c‖]\n≤µf 4 ‖x0 − x∗‖2 + ρ 2T ‖y0 − y∗‖2H + 1 ρT (‖β0‖2 + γ2)."
    } ],
    "references" : [ {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "Stephen P. Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1996
    }, {
      "title" : "Dual averaging and proximal gradient descent for online alternating direction multiplier method",
      "author" : [ "Taiji Suzuki" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Fast stochastic alternating direction method of multipliers",
      "author" : [ "Wenliang Zhong", "James Tin-Yau Kwok" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Online alternating direction method",
      "author" : [ "Huahua Wang", "Arindam Banerjee" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Stochastic alternating direction method of multipliers",
      "author" : [ "Hua Ouyang", "Niao He", "Long Tran", "Alexander G. Gray" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Towards an optimal stochastic alternating direction method of multipliers",
      "author" : [ "Samaneh Azadi", "Suvrit Sra" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "On the o(1/n) convergence rate of the douglas-rachford alternating direction method",
      "author" : [ "Bingsheng He", "Xiaoming Yuan" ],
      "venue" : "SIAM J. Numerical Analysis,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "A multivariate regression approach to association analysis of a quantitative trait",
      "author" : [ "Seyoung Kim", "Kyung-Ah Sohn", "Eric P. Xing" ],
      "venue" : "network. Bioinformatics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Dual averaging method for regularized stochastic learning and online optimization",
      "author" : [ "Lin Xiao" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Efficient online and batch learning using forward backward splitting",
      "author" : [ "John C. Duchi", "Yoram Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "A stochastic gradient method with an exponential convergence rate for finite training sets",
      "author" : [ "Nicolas Le Roux", "Mark W. Schmidt", "Francis Bach" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Optimization with first-order surrogate functions",
      "author" : [ "Julien Mairal" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "The solution path of the generalized lasso",
      "author" : [ "Ryan J. Tibshirani", "Jonathan Taylor" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The alternating direction method of multipliers (ADMM) [1] is proposed to solve the problems which can be formulated as follows:",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "be the L1-norm and the constraint to be x− y = 0, we can get the well-known lasso formulation [2].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "Similarly, we can take more complex constraints than that in lasso to get more complex regularization problems such as the structured sparse regularization problems [3, 4].",
      "startOffset" : 165,
      "endOffset" : 171
    }, {
      "referenceID" : 3,
      "context" : "Similarly, we can take more complex constraints than that in lasso to get more complex regularization problems such as the structured sparse regularization problems [3, 4].",
      "startOffset" : 165,
      "endOffset" : 171
    }, {
      "referenceID" : 2,
      "context" : "Compared with other optimization methods such as gradient decent, ADMM has demonstrated better performance in many complex regularization problems [3, 4].",
      "startOffset" : 147,
      "endOffset" : 153
    }, {
      "referenceID" : 3,
      "context" : "Compared with other optimization methods such as gradient decent, ADMM has demonstrated better performance in many complex regularization problems [3, 4].",
      "startOffset" : 147,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, ADMM can be easily adapted to solve large-scale distributed problems [1].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "Hence, ADMM has been widely used in a large variety of areas [1].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "Existing works have shown that batch ADMM is not efficient enough for big data applications with a large amount of training samples [5, 6].",
      "startOffset" : 132,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "Existing works have shown that batch ADMM is not efficient enough for big data applications with a large amount of training samples [5, 6].",
      "startOffset" : 132,
      "endOffset" : 138
    }, {
      "referenceID" : 4,
      "context" : "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 5,
      "context" : "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 2,
      "context" : "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 6,
      "context" : "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 3,
      "context" : "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 6,
      "context" : "Hence, stochastic ADMM has become a hot research topic and attracted much attention [7, 4].",
      "startOffset" : 84,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "Hence, stochastic ADMM has become a hot research topic and attracted much attention [7, 4].",
      "startOffset" : 84,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "Online alternating direction method (OADM) [5] is the first online ADMM method.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].",
      "startOffset" : 212,
      "endOffset" : 215
    }, {
      "referenceID" : 6,
      "context" : "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].",
      "startOffset" : 251,
      "endOffset" : 254
    }, {
      "referenceID" : 3,
      "context" : "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].",
      "startOffset" : 294,
      "endOffset" : 297
    }, {
      "referenceID" : 7,
      "context" : "STOC-ADMM, RDA-ADMM, OPG-ADMM and OS-ADMM achieve a convergence rate of O(1/ √ T ) for general convex problems, worse than batch ADMM that has a convergence rate of O(1/T ) [8].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 3,
      "context" : "Different from STOC-ADMM, RDA-ADMM, OPG-ADMM and OS-ADMM, SA-ADMM [4] can achieve a convergence rate of O(1/T ) for general convex problems by using historic gradients to approximate the full gradients in each iteration.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "• Experimental results on graph-guided fused lasso [9] show that SCASADMM can achieve state-of-the-art performance in real applications.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "Just like the Gauss-Seidel method, ADMM iteratively updates the variables in an alternating manner as follows [1]:",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "constraint y = x, we can get the lasso formulation [2].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "Some works [5, 8] have proved that the above batch ADMM has a convergence rate O(1/T ) for general convex problems where f(x) and g(y) are convex but not necessarily to be strongly convex, where T is the number of iterations.",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "Some works [5, 8] have proved that the above batch ADMM has a convergence rate O(1/T ) for general convex problems where f(x) and g(y) are convex but not necessarily to be strongly convex, where T is the number of iterations.",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "Recent works have shown that stochastic ADMM can achieve better performance than batch ADMM to handle large-scale datasets in terms of computation complexity and accuracy [5, 6].",
      "startOffset" : 171,
      "endOffset" : 177
    }, {
      "referenceID" : 5,
      "context" : "Recent works have shown that stochastic ADMM can achieve better performance than batch ADMM to handle large-scale datasets in terms of computation complexity and accuracy [5, 6].",
      "startOffset" : 171,
      "endOffset" : 177
    }, {
      "referenceID" : 9,
      "context" : "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 11,
      "context" : "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 13,
      "context" : "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 14,
      "context" : "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].",
      "startOffset" : 265,
      "endOffset" : 269
    }, {
      "referenceID" : 5,
      "context" : "1 Algorithm As in existing stochastic ADMM methods [6, 4], the update rules for y and β are still the same as those in (4) and (5).",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "1 Algorithm As in existing stochastic ADMM methods [6, 4], the update rules for y and β are still the same as those in (4) and (5).",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "Compared with SVRG [15], the update rule in (9) has an extra vector Aβt+ ρA (Awm + Byt − c) = ρAAwm + A (βt + ρ(Byt − c)).",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "The matrix inversion ( 1 ηt I+ρA TA)−1 can be avoided by linearizing the penalty term ρ2 ‖Ax + By − c‖ 2 [4].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 4,
      "context" : "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ √ T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ √ T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ √ T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ √ T ) O(lp+ lq) OS-ADMM [7] YES O(1/ √ T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ √ T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ √ T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ √ T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ √ T ) O(lp+ lq) OS-ADMM [7] YES O(1/ √ T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ √ T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ √ T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ √ T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ √ T ) O(lp+ lq) OS-ADMM [7] YES O(1/ √ T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ √ T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ √ T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ √ T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ √ T ) O(lp+ lq) OS-ADMM [7] YES O(1/ √ T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 6,
      "context" : "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ √ T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ √ T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ √ T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ √ T ) O(lp+ lq) OS-ADMM [7] YES O(1/ √ T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)",
      "startOffset" : 219,
      "endOffset" : 222
    }, {
      "referenceID" : 3,
      "context" : "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ √ T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ √ T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ √ T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ √ T ) O(lp+ lq) OS-ADMM [7] YES O(1/ √ T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)",
      "startOffset" : 256,
      "endOffset" : 259
    }, {
      "referenceID" : 3,
      "context" : "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ √ T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ √ T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ √ T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ √ T ) O(lp+ lq) OS-ADMM [7] YES O(1/ √ T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)",
      "startOffset" : 296,
      "endOffset" : 299
    }, {
      "referenceID" : 5,
      "context" : "As in [6, 7, 4], we evaluate our method on the generalized lasso model [16] which can be formulated as follows:",
      "startOffset" : 6,
      "endOffset" : 15
    }, {
      "referenceID" : 6,
      "context" : "As in [6, 7, 4], we evaluate our method on the generalized lasso model [16] which can be formulated as follows:",
      "startOffset" : 6,
      "endOffset" : 15
    }, {
      "referenceID" : 3,
      "context" : "As in [6, 7, 4], we evaluate our method on the generalized lasso model [16] which can be formulated as follows:",
      "startOffset" : 6,
      "endOffset" : 15
    }, {
      "referenceID" : 15,
      "context" : "As in [6, 7, 4], we evaluate our method on the generalized lasso model [16] which can be formulated as follows:",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 8,
      "context" : "In this paper, we focus on the graph-guided fused lasso [9] which is also used in [4].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "In this paper, we focus on the graph-guided fused lasso [9] which is also used in [4].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "As in [6, 4], we use sparse inverse covariance selection method [17] to get a graph matrix (sparsity pattern) G, based on which we can get A = [G; I].",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 3,
      "context" : "As in [6, 4], we use sparse inverse covariance selection method [17] to get a graph matrix (sparsity pattern) G, based on which we can get A = [G; I].",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "They are: • Batch-ADMM [1]: The deterministic (batch) variant of ADMM which uses (7) to directly update x by visiting all training samples in each iteration.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 5,
      "context" : "• STOC-ADMM [6]: The stochastic ADMM variant without using historic gradient for optimization, which has a convergence rate of O(1/ √ T ) for general convex problems and O(log T/T ) for strongly convex problems.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 3,
      "context" : "• SA-ADMM [4]: The stochastic ADMM variant by using historic gradient to approximate the full gradient, which has a convergence rate of O(1/T ) for general convex problems.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 3,
      "context" : "Furthermore, both theoretical and empirical results have shown that SA-ADMM can outperform other methods like RDA-ADMM and OPG-ADMM [4].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "The variant of SA-ADMM, SA-IU-ADMM, is also not adopted for comparison because it has similar performance as SA-ADMM [4].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "As in [4], four widely used datasets are adopted to evaluate our method and other baselines.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 3,
      "context" : "As in [4], for each dataset we randomly choose half of the samples for training and use the rest for testing.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 3,
      "context" : "The hyper-parameter λ in (14) is set by using the same values in [4], which are also listed in Table 2.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "We adopt the same strategy as that in [4] to set the hyper-parameters ρ in (2) and the",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "As in [4], we use y(x̄T ) = Ax̄T to replace ȳT since the methods cannot necessarily guarantee that Ax̄T = ȳT .",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 3,
      "context" : "2 Convergence Results As in [4], we study the variation of the objective value on training set and the testing loss versus the number of effective passes over the data.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "[1] Stephen P.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] Robert Tibshirani.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Taiji Suzuki.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Wenliang Zhong and James Tin-Yau Kwok.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Huahua Wang and Arindam Banerjee.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Hua Ouyang, Niao He, Long Tran, and Alexander G.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Samaneh Azadi and Suvrit Sra.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Bingsheng He and Xiaoming Yuan.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Seyoung Kim, Kyung-Ah Sohn, and Eric P.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Lin Xiao.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] John C.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Nicolas Le Roux, Mark W.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Julien Mairal.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] Shai Shalev-Shwartz and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] Rie Johnson and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Ryan J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "According to the results in [4], we have the following Lemma 4 and Lemma 5 about the estimation of yt+1 and αt+1.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "The proof of Lemma 4 and Lemma 5 can be directly derived from the results in [4], which is omitted here for space saving.",
      "startOffset" : 77,
      "endOffset" : 80
    } ],
    "year" : 2015,
    "abstractText" : "Most stochastic ADMM (alternating direction method of multipliers) methods can only achieve a convergence rate which is slower than O(1/T ) on general convex problems, where T is the number of iterations. Hence, these methods are not scalable in terms of convergence rate (computation cost). There exists only one stochastic method, called SA-ADMM, which can achieve a convergence rate of O(1/T ) on general convex problems. However, an extra memory is needed for SA-ADMM to store the historic gradients on all samples, and thus it is not scalable in terms of storage cost. In this paper, we propose a novel method, called scalable stochastic ADMM (SCAS-ADMM), for large-scale optimization and learning problems. Without the need to store the historic gradients on all samples, SCAS-ADMM can achieve the same convergence rate of O(1/T ) as the best stochastic method SA-ADMM and batch ADMM on general convex problems. Experiments on graph-guided fused lasso show that SCASADMM can achieve state-of-the-art performance in real applications.",
    "creator" : "LaTeX with hyperref package"
  }
}