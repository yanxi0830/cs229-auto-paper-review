{
  "name" : "1305.0103.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Masashi Sugiyama" ],
    "emails" : [ "christo@sg.cs.titech.ac.jp,", "sugi@cs.titech.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 5.\n01 03\nv1 [\ncs .L\nG ]\n1 M"
    }, {
      "heading" : "1 Introduction",
      "text" : "Gathering labeled data is expensive and time consuming in many practical machine learning problems, and therefore class labels are often absent. In this paper, we consider the problem of labeling, which is aimed at giving a label to each sample. Labeling is similar to classification, but it is slightly simpler than classification because classes do not have to be specified. That is, labeling just tries to split unlabeled samples into disjoint subsets, and class labels such as male/female or positive/negative are not assigned to samples.\nA naive approach to the labeling problem is to use a clustering technique which is aimed at assigning a label to each sample of the dataset to divide the dataset into disjoint clusters. The tacit assumption in clustering is that the clusters correspond to the underlying classes. However, this assumption is often violated in practical datasets, for example, when clusters are not well separated or a dataset exhibits within-class multimodality.\nAn example of the labeling problem is illustrated in Figure 1. Figure 1(a) denotes the densities of the two classes. Figure 1(b) denotes samples drawn from a mixture of the two original densities. Because the two clusters are highly overlapping, it may not be possible to properly label them by a clustering method.\nIn this paper we show that if one more dataset with a different class balance is available (Figure 1(c)), the labeling problem can be solved (Figures 1(d) and\n1(e)). More specifically, we show that a labeling for the samples can be obtained by estimating the sign of the difference between probability densities of two unlabeled datasets. A naive way is to first separately estimate two densities from two\nsets of samples and then take the sign of their difference to obtain a labeling. However, this naive procedure violates Vapnik’s principle[1]:\nIf you possess a restricted amount of information for solving some problem, try to solve the problem directly and never solve a more general problem as an intermediate step. It is possible that the available information is sufficient for a direct solution but is insufficient for solving a more general intermediate problem.\nThis principle was used in the development of support vector machines (SVMs): Rather than modeling two classes of samples, SVM directly learns a decision boundary that is sufficient for performing pattern recognition.\nIn the current context, estimating two densities is more general than labeling samples. Thus, the above naive scheme may be improved by estimating the density difference directly and then taking its sign to obtain the class labels. Recently, a method was introduced to directly estimate the density difference, called the least-squares density difference (LSDD) estimator [2]. Thus, the use of LSDD for labeling is expected to improve the performance.\nHowever, the LSDD-based procedure is still indirect; directly estimating the sign of the density difference would be the most suitable approach to labeling. In this paper, we show that the sign of the density difference can be directly estimated by lower-bounding the L1-distance between probability densities. Based on this, we give a practical algorithm for labeling and illustrate its usefulness through experiments on various real-world datasets."
    }, {
      "heading" : "2 Problem Formulation and Fundamental Approaches",
      "text" : "In this section, we formulate the problem of labeling, give our fundamental strategy, and consider two naive approaches."
    }, {
      "heading" : "2.1 Problem Formulation",
      "text" : "Suppose that there are two probability distributions p(x, y) and p′(x, y) on x ∈ R d and y ∈ {1,−1}, which are different only in class balances:\np(y) 6= p′(y) but p(x|y) = p′(x|y). (1)\nFrom these distributions, we are given two sets of unlabeled samples:\nXp = {xi} n i=1 i.i.d. ∼ p(x) and Xp′ = {x ′ j} n′ j=1 i.i.d. ∼ p′(x).\nThe goal of labeling is to obtain a labeling for the two sets of samples, Xp and Xp′ , that corresponds to the underlying class labels {yi}ni=1 and {y ′ j} n′\nj=1. However, different from classification, we do not obtain correct class labels, but we obtain correct class separation up to label commutation."
    }, {
      "heading" : "2.2 Fundamental strategy",
      "text" : "We wish to obtain a labeling for samples in Xp and Xp′ . Here we show that we can obtain the solution for the case where the class priors are equal. We may write the class-posterior distribution for the equal prior case as\nq(y = 1|x) = p(x|y)q(y)\nq(x) ,\nwhere q(y = 1) = q(y = −1) = 1 2 . A class label can then be assigned to a point by evaluating\nsign [q(y = 1|x)− q(y = −1|x)]\nWe can write the criterion as\nq(y = 1|x)− q(y = −1|x) = p(x|y = 1)1 2\nq(x) −\np(x|y = −1)1 2\nq(x) ,\n∝ p(x|y = 1)− p(x|y = −1).\nWe do not have any labeled samples to calculate p(x|y = 1)− p(x|y = −1), but we can rewrite it in terms of marginal distributions. To see this, the above is multiplied with p(y = 1)− p′(y = 1), which gives\np(x|y = 1)− p(x|y = −1) ∝ [p(y = 1)− p′(y = 1)] [p(x|y = 1)− p(x|y = −1)]\n∝ p(x, y = 1)− p′(x, y = 1)\n− p(y = 1)p(x|y = −1) + p′(y = 1)p(x|y = −1).\nNote that the sign may change since p(y = 1) − p′(y = 1) may be positive or negative. To write the third and fourth term as a joint distribution, we add and subtract p(x|y = −1), giving\np(x|y = 1)− p(x|y = −1) ∝ p(x, y = 1)− p′(x, y = −1) + [1− p(y = 1)] p(x|y =−1)\n− [1− p′(y = 1)] p(x|y = −1).\nSince p(y = −1) = 1− p(y = 1) and p′(y = −1) = 1− p′(y = 1), we can express the above as\nq(y = 1|x)− q(y = −1|x) ∝ p(x)− p′(x).\nThe exact class labels can not be recovered since the term p(y = 1)− p′(y = 1) can be positive or negative. Therefore, we assign the label y ∈ {1,−1} to a point x according to the following criterion:\ny = sign [p(x)− p′(x)]. (2)\nThus, now we need a good method to estimate sign [p(x)− p′(x)]."
    }, {
      "heading" : "2.3 Kernel Density Estimation",
      "text" : "A naive approach to estimating the sign of density-difference is to use kernel density estimators (KDEs) [3]. For Gaussian kernels, the KDE solutions are given by\np̂(x) ∝ n∑\ni=1\nexp ( − ‖x− xi‖ 2\n2σ2\n) and p̂′(x) ∝ n′∑\nj=1\nexp ( − ‖x− x′j‖ 2\n2σ′2\n) .\nThe Gaussian widths σ and σ′ may be determined based on least-squares crossvalidation [4]. Finally, a labeling is obtained as\ny = sign [p̂(x)− p̂′(x)]. (3)"
    }, {
      "heading" : "2.4 Direct Estimation of the Density Difference",
      "text" : "KDE is a nice density estimator, but it is not necessarily suitable in densitydifference estimation, because small estimation error incurred in each density estimate can cause a big error in the final density-difference estimate. More intuitively, good density estimators tend to be smooth and thus a densitydifference estimator obtained from such smooth density estimators tends to be over-smoothed [5,6].\nThe density difference can be estimated in a single shot using the least-squares density difference (LSDD) approach [2]. In this approach, we directly fit a model g(x) to the density difference under the square loss:\nĝ = argmin g\n1\n2\n∫ (g(x)− (p(x)− p′(x))) 2 dx,\nwhich can be efficiently obtained for a kernel density-difference model. A comprehensive review of LSDD is provided in Appendix B. Finally, a labeling is obtained as\ny = sign[ĝ(x)]."
    }, {
      "heading" : "3 Direct Estimation of the Sign of the Density Difference",
      "text" : "We expect that an improved solution can be obtained by LSDD over KDEs due to more direct nature of LSDD. However, LSDD is still indirect because the sign of density difference is inspected after the density difference is estimated. In this section, we show how to directly estimate the sign of the density difference."
    }, {
      "heading" : "3.1 Derivation of the Objective Function",
      "text" : "By lower-bounding the L1-distance between probability densities, defined as ∫\n|p(x)− p′(x)| dx, (4)\nwe can obtain the sign of the density difference. We begin by considering the following self-evident relation:\n|t| ≥ tz, if |z| ≤ 1.\nWe can apply this relation at each point x, to obtain\n|p(x)− p′(x)| ≥ g(x) [p(x)− p′(x)] if |g(x)| ≤ 1, ∀x.\nBy applying the above inequality to Eq.(4) and maximizing with respect to g(x), we can obtain the tightest lower bound as\n∫ |p(x)− p′(x)| dx ≥ sup\ng\n∫ g(x) [p(x)− p′(x)] dx (5)\ns.t. |g(x)| ≤ 1, ∀x.\nIt is straightforward to verify that the above relation will be met with equality when\ng(x) = sign (p(x)− p′(x)) .\nWhat makes the expression in the right-hand side of Eq.(5) especially useful is that the probability densities occur linearly in the integral. By replacing the integrals with sample averages and searching g(x) from a parametric family (denoted as gα(x)), we can write the above as\nα = argmin α\n1 n′\nn′∑\nj=1\ngα(x ′ j)−\n1\nn\nn∑\ni=1\ngα(xi)\ns.t. |gα(x)| ≤ 1, ∀x.\n(6)"
    }, {
      "heading" : "3.2 Optimization",
      "text" : "Here we briefly discuss how to solve the problem in Eq. (6). A more detailed explanation is given in Appendix A.\nThe function in Eq. (6) should satisfy the constraint |g(x)| ≤ 1, ∀x. We can consider a clipped version of the function that always satisfies the constraint,\ng̃(x) = R(g(x)), where R(z) =    1 z > 1, −1 z < −1,\nz otherwise.\nWe use a linear-in-parameter model,\ng(x) =\nb∑\nℓ=1\nαℓϕℓ(x),\nwhere ϕℓ(x) are the basis functions. Using the above definitions, we can rewrite Eq.(6) as\nJ(α) = 1\nn′\nn′∑\ni=1\nR\n( b∑\nℓ=1\nαℓϕℓ(x ′ i)\n) − 1\nn\nn∑\nj=1\nR\n( b∑\nℓ=1\nαℓϕℓ(xj)\n) + λ\n2\nb∑\nℓ=1\nα2ℓ , (7)\nwhere λ 2 ∑b ℓ=1 α 2 ℓ is a regularization term. Although the above is a non-convex problem, we can efficiently find a local optimal solution using the convex-concave procedure (CCCP) [7] (also known as difference of convex (d.c.) programming [8]). The CCCP procedure requires the objective function to be split into a convex and concave part,\nJ(α) = Jvex(α) + Jcave(α).\nThe concave part is then upper-bounded as\nJcave(α) ≤ J̄cave(α, b, c),\nwhere the bound is specified by b and c (details are given in Appendix A). This bound is convex w.r.t. b and c if α is fixed. Using this bound, the optimization problem can then be expressed as\nJ(α) ≤ Jvex(α) + J̄cave(α, b, c).\nThe strategy to minimize J(α) is then to alternately minimize the right-hand side by minimizing w.r.t. α (keeping b and c constant) and minimize w.r.t. b and c (keeping α constant). Minimization w.r.t. α minimizes the current upper bound and minimization w.r.t. b and c corresponds to tightening the bound at the current point.\nMinimization w.r.t. b and c can be performed by\nbi =\n{ 0 ∑b ℓ=1 αℓϕℓ(x ′ i) < 1,\n1 otherwise, and cj =\n{ 0 ∑b ℓ=1 αℓϕ(xj) < −1,\n1 otherwise. (8)\nMinimization of the upper bound (assuming b and c is constant) can be performed by solving the following convex quadratic problem:\nJ̄(α) = 1\nn′\nn′∑\ni=1\nξ′i+ 1\nn\nn∑\nj=1\nξj− b∑\nℓ=1\nαℓ  1 n′ n′∑\ni=1\nbiϕℓ(x ′ i)+\n1\nn\nn∑\nj=1\ncjϕℓ(xj)\n +λ\n2\nb∑\nℓ=1\nα2ℓ\ns.t. ξ′i ≥ 0, ξ ′ i ≥\nb∑\nℓ=1\nαℓϕℓ(x ′ i) + 1, ∀i = 1, . . . , n ′\nξj ≥ 0, ξj ≥ b∑\nℓ=1\nαℓϕℓ(xj)− 1 ∀j = 1, . . . , n.\n(9)\nThe above constrained problem can be solved with an off-the-shelf QP solver. Our final optimization algorithm is summarized below:\n1. Initialize the starting value:\nα1 ← argmin α Jvex(α).\n2. For t = 1, . . . T :\n(a) Tighten the upper-bound: Obtain b and c as\nb, c← argmin b,c\nJ̄(αt, b, c),\nby using Eq.(8).\n(b) Minimize the upper bound: Set\nαt+1 ← argmin α Jvex(α) + J̄cave(α, b, c, )\nby solving the convex problem in Eq.(9).\nIn practice, Gaussian kernels centered at the sample points in Xp and Xp′ are chosen as the basis functions. All hyper-parameters are set by cross-validation."
    }, {
      "heading" : "4 Experiments",
      "text" : "We first illustrate the operation of our method and characterize the failures of other methods on various toy examples. Then we use real-world benchmark data to show the superiority of our algorithm."
    }, {
      "heading" : "4.1 Numerical Illustration",
      "text" : "Toy Problem 1: We illustrate the problem and our method with a simple example. Suppose that the class-conditional densities for the two classes are given as\np(x|y = 1) = Nx (−12, I2×2) and p(x|y = −1) = Nx (12, I2×2) ,\nwhereNx(µ,Σ) denotes the normal density with mean µ and covarianceΣ w.r.t. x. 12 is a 2×1 vector of ones and I is a 2×2 identity matrix. We generate 2 sets of 30 samples with class-priors p(y = 1) = 0.3 and p′(y = 1) = 0.7, respectively. The result is illustrated in Figure 1. As can be seen from this example, we are able to obtain a labeling of the classes that roughly corresponds to the true (unknown) labels of the data.\nToy Problem 2: One way to obtain a labeling is to use clustering. The tacit assumption in clustering is that samples in the same cluster belong the same class. This assumption however is not always be true, for example, when the class conditional densities are multimodal. Here we consider a problem with the following class conditional densities:\np(x|y = 1) = 1\n2 Nx([3 0]\n⊤ , I2×2) +\n1 2 Nx([−3 0] ⊤ , I2×2)\np(x|y = −1) = 1\n2 Nx([0 3]\n⊤ , I2×2) +\n1 2 Nx([0 − 3] ⊤ , I2×2).\nThe two distributions are plotted in Figure 2a. We can try to obtain a class label by performing clustering on Xp∪Xp′ 1. The results for k-means and spectral clustering, given in Figures 2d and 2e, show that these methods fail to reveal the true labeling. On the other hand, the proposed method still gives a reasonable result (Figure 2f)."
    }, {
      "heading" : "4.2 Benchmark Datasets",
      "text" : "We compare our method against several competing methods on benchmark datasets. For each experiment, we constructed the datasets Xp and Xp′ by drawing n and n′ samples from the positive and negative classes of the datasets according to a prior of p(y = 1) and p′(y = 1). The labeling was then performed using these two datasets. Since we can obtain a labeling, but cannot determine the original class labels, we cannot measure the performance using the misclassification rate directly. Assume that the label assigned for the sample xi is\nli =\n{ −1 p(xi)− q(xi) < 0\n1 otherwise.\nThe misclassification rate (MCR) assuming that the current labels are correct is\nMCR := 1\nn\n∑\ni:lj 6=yi\n1 + 1\nn′\n∑\nj:l′ j 6=y′ i\n1.\nThe misclassification rate assuming that the labels are the opposite is 1−MCR. We define the labeling error rate (LER) as\nLER := min (MCR, 1−MCR) .\nNote that this definition is somewhat more optimistic than using the misclassification rate. The smaller the dataset is, the lower the error would be for randomly\n1 If clustering is performed separately on Xp and Xp′ , we do not know which clusters in each dataset correspond to the clusters in the other dataset. We can also not perform clustering on one dataset and apply it to the other dataset, since most clustering methods do not give out of sample labeling. For these reasons, it makes most sense to perform clustering on the combined dataset.\nassigning labels to samples: The expected LER for randomly assigning labels to samples (with equal probability) is\n1\n2n+n′(n+ n′)\nn+n′∑\ni=0\nmin (i, n+ n′ − i)\n( n+ n′\ni\n) .\nFor n+ n′ = 40, 60, 80, the expected labeling error rate is 0.437, 0.449, 0.456. We compared the following methods:\n– Direct Sign Density Difference (DSDD) Estimation (proposed): Directly estimate sign (p(x)− p′(x)) using the method described in Section 3. Hyperparameters are selected via cross validation. – Least-Squares Density Difference (LSDD) Estimation: Estimate sign [p(x)− p′(x)] by estimating p(x)− p′(x) using the least squares fitting method [9]. Hyperparameters are selected via cross validation. – Kernel Density Estimation (KDE): Estimate sign [p(x)− p′(x)] by estimating the densities p(x) and p′(x) with kernel density estimation (KDE). Hyperparameters are selected using least-squares cross validation. – K-Means (KM): Cluster the data into two clusters using the K-means algorithm. – Spectral Clustering (SC): Cluster the data into two clusters using the spectral clustering algorithm [10]. The affinity matrix was constructed with 7 nearest neighbors. – Squared-loss Mutual Information based Clustering (SMIC) : Cluster the data according to the SMIC method [11]. SMIC was chosen since it provides model selection, avoiding the need for subjective parameter tuning.\nWe compare the performance of the methods by varying the class balance. Two class balances were selected: one with a large difference between the classes (p(y = 1) = 0.2 and p′(y = 1) = 0.8) and one with a small difference between the two priors (p(y = 1) = 0.35 and p′(y = 1) = 0.65). The average labeling error rate and standard deviation of the two experiments, with |Xp| = |Xp′ | = 40 is given in Tables 1 and 2.\nFrom the results we see that methods which follow the approach proposed in Section 2 of estimating the sign of the density difference (i.e., DSDD, LSDD, and KDE) generally work better than methods using the cluster structure of the data (i.e., KM, SC and SMIC). The thyroid dataset lends itself to interpretation of why these methods work better. The labels in the thyroid dataset correspond to healthy and diseased. The diseased label is caused by either a hyper-functioning or hypo-functioning thyroid. These two underlying causes cause within-class multimodality which may cause clustering-based methods to fail.\nAmong the methods which estimate the sign of the density difference, we see that DSDD generally performs better than LSDD and LSDD in turn performs better than KDE. This is as expected since KDE solves a more general problem than LSDD, and LSDD solves a more general problem than DSDD. This pattern is even more pronounced on the more difficult case where the class balances are close to each other (Table 2)."
    }, {
      "heading" : "5 Conclusion",
      "text" : "The problem of unsupervised labeling of two unbalanced datasets was considered. We first showed that this problem can be solved if two unlabeled datasets having different class balances are available. The solution can be obtained by estimating of the sign of the difference between probability densities. We introduced a method to directly estimate the sign of the density difference and avoid density estimation. The method was shown on various datasets to outperform\ncompeting methods that either estimate the density difference or use the cluster structure of the data.\nBecause the sign of density difference corresponds to the Bayes optimal classifier under equal class balance, it may be estimated by any classifier that separates Xp and Xp′ . Following this idea, we tested the support vector machine (SVM) for estimating the sign of density difference. However, this did not work well due to the high overlap of Xp and Xp′—both the datasets are mixtures of two classes, only with different mixing ratios.\nFrom this classification point of view, we can actually see that our objective function (7) corresponds to the robust SVM [12] that minimizes the ramp loss (a clipped hinge loss). Thanks to the robustness brought by the ramp loss, the overlapped datasets Xp and Xp′ can be separated more reliably, and thus we obtained good estimation of the sign of density difference.\nFurthermore, this view conversely shows that the robust SVM is actually a suitable classification method because it directly estimates the Bayes optimal classifier, the sign of density difference. Labeling and classification are different problems, but one can actually give insight into the other. In the future work, we will further investigate the relation between labeling and classification."
    }, {
      "heading" : "A Optimization",
      "text" : "This section outlines the optimization of Eq. (7) using the convex concave procedure[7]. The non-convex function R(z) can be re-written as\nR(z) = C−1(z)− C1(z)− 1, where Cǫ(z) = max(0, z − ǫ).\nThe convex part of the objective function can then be expressed as\nJvex(α) = 1\nn′\nn′∑\ni=1\nC−1\n( b∑\nℓ=1\nαℓϕℓ(x ′ i)\n) + 1\nn\nn∑\nj=1\nC1\n( b∑\nℓ=1\nαℓϕℓ(xj)\n) + λ\n2\nb∑\nℓ=1\nα2ℓ ,\nand the concave part as\nJcave(α) = − 1\nn′\nn′∑\ni=1\nC1\n( b∑\nℓ=1\nαℓϕℓ(x ′ i)\n) − 1\nn\nn∑\nj=1\nC−1\n( b∑\nℓ=1\nαℓϕℓ(xj)\n) .\nThe following self-evident relation can be used to bound the concave part\ntz − ϕ(t) ≤ sup y∈R yz − ϕ(y)\n⇒ ϕ(t) ≥ tz − ϕ∗(z),\nwhere\nϕ∗(z) = sup y∈R yz − ϕ(y)\nis known as the convex conjugate. The convex conjugate of the function Cǫ(z) is\nC∗ǫ (z) =    ∞ z < 0 ǫz 0 ≤ z ≤ 1\n∞ z > 0.\nThis gives an upper bound on the concave function as\nJ̄cave(α, b, c) = 1\nn′\nn′∑\ni=1\n( C∗1 (bi)− bi b∑\nℓ=1\nαℓϕℓ(x ′ i)\n) + 1\nn\nn∑\nj=1\n( C∗−1(cj)− cj b∑\nℓ=1\nαℓϕℓ(xj)\n) ,\nwhere b = [b1 b2 . . . bn′ ] and c = [c1 c2 . . . cn] specify the bound.\nA.1 Tightening the bound\nThe bound can be tightened around α by minimizing Jcave(α, b, c) w.r.t. b and c. To ensure that we have a non-trivial bound, we can explicitly write the conjugate as constraints,\nJ̄cave(α, b, c) = 1\nn′\nn′∑\ni=1\nbi\n( 1− b∑\nℓ=1\nαℓϕℓ(x ′ i)\n) + 1\nn\nn∑\nj=1\ncj\n( −1− b∑\nℓ=1\nαℓϕℓ(xj)\n)\ns.t. 0 ≤ bi ≤ 1, 0 ≤ cj ≤ 1.\nThe above optimization problem is separable in all unknowns, and the optimal value can be obtained by Eq. (8).\nA.2 Minimizing the upper bound\nThe upper bound of the objective function with b and c is\nJvex(α) + J̄cave(α, b, c).\nBy replacing each function Cǫ(z) with a slack variable ξi, and the constraint\nξi ≥ 0, ξi ≥ z − ǫ,\nwe obtain the objective function in Eq. (9)"
    }, {
      "heading" : "B Least-squares estimation of the density difference",
      "text" : "In [9] it was proposed to directly estimate the density difference by fitting a model g(x) to the true density difference f(x) under a square loss:\nargmin g\n1\n2\n∫ ( g(x)− [p(x)− p′(x)] )2 dx.\nThe density difference was modeled by a linear-in-parameter model g(x):\ng(x) =\nb∑\nℓ=1\nθℓψℓ(x) = θ ⊤ ψ(x), (10)\nwhere b denotes the number of basis functions, ψ(x) = (ψ1(x), . . . , ψb(x)) ⊤ is a b-dimensional basis function vector, θ = (θ1, . . . , θb) ⊤ is a b-dimensional parameter vector, and ⊤ denotes the transpose. A Gaussian kernel model is used to model the density difference:\ng(x) =\nn+n′∑\nℓ=1\nθℓ exp\n( − ‖x− cℓ‖2\n2σ2\n) ,\nwhere (c1, . . . , cn, cn+1, . . . , cn+n′) := (x1, . . . ,xn,x ′ 1, . . . ,x ′ n′) are Gaussian kernel centers. For the model in Eq. (10), the optimal parameter θ∗ is given by\nθ∗ := argmin θ\n1\n2\n∫ ( g(x)− [p(x)− p′(x)] )2 dx\n= argmin θ\n[ 1\n2\n∫ g(x)2dx− ∫ g(x) [p(x)− p′(x)] dx ]\n= argmin θ\n[ 1\n2 θ⊤Hθ − h⊤θ\n]\n=H−1h,\nwhere H is the b× b matrix and h is the b-dimensional vector defined as\nH := ∫ ψ(x)ψ(x)⊤dx,\nh := ∫ ψ(x)p(x)dx− ∫ ψ(x)p′(x)dx.\nFor the Gaussian kernel model, the integral in H can be computed analytically as\nHℓ,ℓ′ =\n∫ exp ( − ‖x− cℓ‖2\n2σ2\n) exp ( − ‖x− cℓ′‖2\n2σ2\n) dx\n= (πσ2)d/2 exp ( − ‖cℓ − cℓ′‖2\n4σ2\n) ,\nwhere d is the dimensionality of x. Replacing the expectations in h by empirical estimators and adding an ℓ2regularizer to the objective function, we arrive at the following optimization problem:\nθ̂ := argmin θ\n[ 1\n2 θ⊤Hθ − ĥ\n⊤ θ +\n1 2 λθ⊤θ\n] , (11)\nwhere λ (≥ 0) is the regularization parameter and ĥ is the b-dimensional vector defined as\nĥ = 1\nn\nn∑\ni=1\nψ(xi)− 1\nn′\nn′∑\nj=1\nψ(x′j).\nTaking the derivative of the objective function in Eq.(11) and equating it to\nzero, we can obtain the solution θ̂ analytically as\nθ̂ = (H + λIb) −1 ĥ,\nwhere Ib denotes the b-dimensional identity matrix. Finally, the density difference estimator is\nf̂(x) = θ̂ ⊤ ψ(x)."
    } ],
    "references" : [ {
      "title" : "The Nature of Statistical Learning Theory",
      "author" : [ "V. Vapnik" ],
      "venue" : "Statistics for Engineering and Information Science Series. Springer",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Density-difference estimation",
      "author" : [ "M. Sugiyama", "T. Kanamori", "T. Suzuki", "M.C. du Plessis", "S. Liu", "I. Takeuchi" ],
      "venue" : "In Bartlett, P., Pereira, F., Burges, C., Bottou, L., Weinberger, K., eds.: Advances in Neural Information Processing Systems 25.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Density estimation for statistics and data analysis",
      "author" : [ "B. Silverman" ],
      "venue" : "Chapman and Hall, London, UK",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Nonparametric and semiparametric models",
      "author" : [ "W. Härdle", "M. Müller", "S. Sperlich", "A. Werwatz" ],
      "venue" : "Springer",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "On nonparametric discrimination using density differences",
      "author" : [ "P. Hall", "M.P. Wand" ],
      "venue" : "Biometrika 75(3)",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Two-sample test statistics for measuring discrepancies between two multivariate probability density functions using kernelbased density estimates",
      "author" : [ "N.H. Anderson", "P. Hall", "D. Titterington" ],
      "venue" : "Journal of Multivariate Analysis 50(1)",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "The concave-convex procedure (CCCP)",
      "author" : [ "A.L. Yuille", "A. Rangarajan" ],
      "venue" : "Advances in Neural Information Processing Systems 14, MIT Press",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "DC programming: overview",
      "author" : [ "R. Horst", "N.V. Thoai" ],
      "venue" : "Journal of Optimization Theory and Applications 103(1)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Density-difference estimation",
      "author" : [ "M. Sugiyama", "T. Kanamori", "T. Suzuki", "M.C. du Plessis", "S. Liu", "I. Takeuchi" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on 22(8)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "On informationmaximization clustering: Tuning parameter selection and analytic solution",
      "author" : [ "M. Sugiyama", "M. Yamada", "M. Kimura", "H. Hachiya" ],
      "venue" : "In Getoor, L., Scheffer, T., eds.: Proceedings of 28th International Conference on Machine Learning (ICML2011), Bellevue, Washington, USA",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : "Cambridge University Press, New York, NY, USA",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "However, this naive procedure violates Vapnik’s principle[1]: If you possess a restricted amount of information for solving some problem, try to solve the problem directly and never solve a more general problem as an intermediate step.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "Recently, a method was introduced to directly estimate the density difference, called the least-squares density difference (LSDD) estimator [2].",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "3 Kernel Density Estimation A naive approach to estimating the sign of density-difference is to use kernel density estimators (KDEs) [3].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "The Gaussian widths σ and σ may be determined based on least-squares crossvalidation [4].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "More intuitively, good density estimators tend to be smooth and thus a densitydifference estimator obtained from such smooth density estimators tends to be over-smoothed [5,6].",
      "startOffset" : 170,
      "endOffset" : 175
    }, {
      "referenceID" : 5,
      "context" : "More intuitively, good density estimators tend to be smooth and thus a densitydifference estimator obtained from such smooth density estimators tends to be over-smoothed [5,6].",
      "startOffset" : 170,
      "endOffset" : 175
    }, {
      "referenceID" : 1,
      "context" : "The density difference can be estimated in a single shot using the least-squares density difference (LSDD) approach [2].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "Although the above is a non-convex problem, we can efficiently find a local optimal solution using the convex-concave procedure (CCCP) [7] (also known as difference of convex (d.",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 7,
      "context" : ") programming [8]).",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 2,
      "context" : "p(x|y = 1) = 1 2 Nx([3 0] ⊤ , I2×2) + 1 2 Nx([−3 0] ⊤ , I2×2) p(x|y = −1) = 1 2 Nx([0 3] ⊤ , I2×2) + 1 2 Nx([0 − 3] ⊤ , I2×2).",
      "startOffset" : 20,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "p(x|y = 1) = 1 2 Nx([3 0] ⊤ , I2×2) + 1 2 Nx([−3 0] ⊤ , I2×2) p(x|y = −1) = 1 2 Nx([0 3] ⊤ , I2×2) + 1 2 Nx([0 − 3] ⊤ , I2×2).",
      "startOffset" : 83,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "– Least-Squares Density Difference (LSDD) Estimation: Estimate sign [p(x)− p(x)] by estimating p(x)− p(x) using the least squares fitting method [9].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 9,
      "context" : "– Spectral Clustering (SC): Cluster the data into two clusters using the spectral clustering algorithm [10].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 10,
      "context" : "– Squared-loss Mutual Information based Clustering (SMIC) : Cluster the data according to the SMIC method [11].",
      "startOffset" : 106,
      "endOffset" : 110
    } ],
    "year" : 2013,
    "abstractText" : "We consider the unsupervised learning problem of assigning labels to unlabeled data. A naive approach is to use clustering methods, but this works well only when data is properly clustered and each cluster corresponds to an underlying class. In this paper, we first show that this unsupervised labeling problem in balanced binary cases can be solved if two unlabeled datasets having different class balances are available. More specifically, estimation of the sign of the difference between probability densities of two unlabeled datasets gives the solution. We then introduce a new method to directly estimate the sign of the density difference without density estimation. Finally, we demonstrate the usefulness of the proposed method against several clustering methods on various toy problems and real-world datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}