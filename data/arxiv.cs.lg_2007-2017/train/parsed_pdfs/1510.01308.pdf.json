{
  "name" : "1510.01308.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Tight Variational Bounds via Random Projections and I-Projections",
    "authors" : [ "Lun-Kai Hsu", "Tudor Achim", "Stefano Ermon" ],
    "emails" : [ "luffykai@cs.stanford.edu", "tachim@cs.stanford.edu", "ermon@cs.stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Probabilistic inference is a core problem in machine learning, physics, and statistics [1]. Probabilistic inference methods are needed for training, evaluating, and making predictions with probabilistic models [2]. Developing scalable and accurate inference techniques is the key computational bottleneck towards deploying large-scale statistical models, but exact inference is known to be computationally intractable. The root cause is the curse of dimensionality – the number of possible scenarios to consider grows exponentially in the number of variables, and in continuous domains, the volume grows exponentially in the number of dimensions [3]. Approximate techniques are therefore almost always used in practice [2].\nSampling-based techniques and variational approaches are the two main paradigms for approximate inference [4, 5]. Sampling-based approaches attempt to approximate intractable, high-dimensional distributions using a (small) collection of representative samples [6, 7, 8]. Unfortunately, it is usually no easier to obtain such samples than it is to solve the original probabilistic inference problem [9]. Variational approaches, on the other hand, approximate an intractable distribution using a family of tractable ones. Finding the best approximation, also known as computing an I-Projection onto the family, is the key ingredient in all variational inference algorithms. In general, there is no guarantee on the quality of the approximation obtained. Intuitively, if the target model is too complex with respect to the family used, then the approximation will be poor.\nTo overcome this issue, we introduce a new class of random projections [10, 11, 12]. These projections take as input a probabilistic model and randomly perturb it, reducing its degrees of freedom. The projections can be computed efficiently and they reduce the effective dimensionality and\nLKH and TA contributed equally to this paper.\nar X\niv :1\n51 0.\n01 30\n8v 1\n[ cs\n.L G\n] 5\nO ct\ncomplexity of the target model. Our key result is that the randomly projected model can then be approximated with I-projections onto simple families of distributions such as mean field with provable guarantees on the accuracy, regardless of the complexity of the original model. Crucially, in the spirit of random projections for dimensionality reduction, the random projections affect key properties of the target distribution (such as the partition function) in a highly predictable way. The I-projection of the projected model can therefore be used to accurately recover properties of the original model with high probability.\nWe demonstrate the effectiveness of our approach by using mean field augmented with random projections to estimate marginals and the log partition function on models of synthetic and realworld data, empirically showing large improvements on both tasks."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Let p(x) = 1Z ∏ α ψα({x}α) be a probability distribution over n binary variables x ∈ {0, 1}n specified by an undirected graphical model 1 [1]. We further assume that p(x) is a member of an exponential family of distributions parameterized by θ ∈ Rd and with sufficient statistics φ(x) [5], i.e., p(x) = exp(θ·φ(x))Z . The constant Z = ∑ x∈{0,1}n ∏ α∈I ψα({x}α) is known as the partition function and ensures that the probability distribution is properly normalized. Computing the partition function is one of the key computational challenges in probabilistic reasoning and statistical machine learning, as it is needed to evaluate likelihoods and compare competing models of data. This computation is known to be intractable (#-P hard) in the worst-case, as the sum is defined over an exponentially large number of terms [13, 1]. We focus on variational approaches for approximating the partition function."
    }, {
      "heading" : "2.1 Variational Inference and I-projections",
      "text" : "The key idea of variational inference approaches is to approximate the intractable probability distribution p(x) with one that is more tractable. The approach is to define a family Q of tractable distributions and then look for a distribution in this family that minimizes a notion of divergence from p. Typically, the Kullback-Leibler divergence DKL(q||p) is used, which is defined as follows\nDKL(q||p) = ∑ x q(x) log q(x) p(x) = ∑ x q(x) log q(x)− θ · ∑ x q(x)φ(x) + logZ (1)\nA distribution q∗ ∈ Q that minimizes this divergence, q∗ = argminq∈QDKL(q||p), is called an information projection (I-projection) onto Q. Intuitively, q∗ is the “closest” distribution to p among all the distributions inQ. Typically, one choosesQ to be a family of tractable distributions for which inference is tractable, i.e., such that (1) can be evaluated efficiently. The simplest choice, which removes all conditional dependencies, is to letQ be the set of fully factored probability distributions over X , namelyQMF = {q(x)|q(x) = ∏ i qi(xi)}. This is known as the mean field approximation. Even when Q is tractable, computing an I-projection, i.e., minimizing the KL divergence, is a nonconvex optimization problem which can be difficult to solve.\nSince the KL-divergence is non-negative, equation (1) shows that any distribution q ∈ Q provides a lower bound on the value of the partition function\nlogZ ≥ max q∈Q − ∑ x q(x) log q(x) + θ · ∑ x q(x)φ(x) (2)\nThe distribution q∗ that minimizesDKL(q||p) is also the distribution that provides the tightest lower bound on the partition function by maximizing the RHS of equation (2). The larger the set Q is, the better q∗ can approximate p and the tighter the bound becomes. If Q is so large that p ∈ Q, then minq∈QDKL(q||p) = 0, because when q∗ = p, DKL(q∗||p) = 0. In general, however, there is no guarantee on the tightness of bound (2) even if the optimization can be solved exactly.\n1 We restrict ourselves to binary variables for the ease of exposition. Our approach applies more generally to discrete graphical models."
    }, {
      "heading" : "2.2 Random Projections",
      "text" : "We introduce a different class of random projections that we will use for probabilistic inference. Let P be the set of all probability distributions over {0, 1}n. We introduce a family of operators RmA,b : P → P , where m ∈ [0, n], A ∈ {0, 1}m×n, and b ∈ {0, 1}m. RmA,b ∈ R maps p(x) = 1 Z ∏ α ψα({x}α) to a new probability distribution RmA,b(p) restricted to {x : Ax = b mod 2} whose probability mass function is proportional to p. Formally,\nRmA,b(p)(x) = 1\nZ(A, b) ∏ α ψα({x}α)\nIn other words, for all x ∈ {x|Ax = b mod 2} RmA,b(p) is proportional to the original p(x), and the new normalization constant is\nZ(A, b) = ∑\nx|Ax=b mod 2 ∏ α ψα({x}α)\nThese operators are clearly idempotent and can thus be interpreted as projections.\nWhen the parameters A, b are chosen randomly, the operator RmA,b can be seen as a random projection. We consider random projections obtained by choosing A ∈ {0, 1}m×n and b ∈ {0, 1}m independently and uniformly at random, i.e., choosing each entry by sampling an independent unbiased Bernoulli random variable. This can be shown to implement a strongly universal hash function [10, 11]. Intuitively, the projection randomly subsamples the original space, selecting configurations x ∈ {0, 1}n pairwise independently with probability 2−m. It can be shown [12, 14] that\nE[Z(A, b)] = 2−mZ\nwhere the expectation is over the random choices of A, b, and that V ar [Z(A, b)] = 1 2m ( 1− 12m )∑ x ( ∏ α ψα({x}α))\n2. As we will formalize later, this random projection simplifies the model without losing too much information because it affects the partition function in a highly predictable way (knowing the expectation and the variance is sufficient to achieve high probability bounds).\nTo gain some intuition on the effect of the random projection, we can rewrite the linear system Ax = b mod 2 in reduced row-echelon form [12]. Assuming A is full-rank, we have that C = [Im|A′] where Im is the m×m identity matrix. The system Ax = b is mathematically equivalent to Cx = b′. For notational simplicity, we continue to use b instead of b′. We can equivalently rewrite the constraints Ax = b mod 2 as the following set of constraints\nx1 = n⊕ i=m+1 c1ixi ⊕ b1, · · · , xm = n⊕ i=m+1 cmixi ⊕ bm\nwhere ⊕ denotes the exclusive-or (XOR) operator. Thus, the random projection reduces the degrees of freedom of the model by m, as the first m variables are completely determined by the last n − m. For later development it will also be convenient to rewrite these linear equations modulo 2 as polynomial equations by changing variables from {0, 1} to {−1, 1}:\n(1− 2x1) = n∏\ni=m+1\n(1− 2C1ixi)(1− 2b1), · · · , (1− 2xm) = n∏\ni=m+1\n(1− 2Cmixi)(1− 2bm)\n(3)"
    }, {
      "heading" : "3 Combining Random Projections with I-Projections",
      "text" : "Given an intractable target distribution p and a candidate set of (tractable) distributions Q, there are two main issues with variational approximation techniques: (i) p can be far from the approximating familyQ in the sense that even the optimal q∗ = argminq∈QDKL(q‖p) can have a large divergence DKL(q\n∗‖p) and therefore yield a poor lower bound in Eq. (2), and (ii) the variational problem in Eq. (2) is non-convex and thus difficult to solve exactly in high dimensions. Our key idea is to address\n(i) by using the random projections introduced in the previous section to “simplify” p, producing a projection RmA,b(p) that provably is closer to Q. Crucially, because of the statistical properties of the random projection used, (variational) inferences on the randomly projected model RmA,b(p) reveal useful information about the original distribution p. Randomization plays a key role in our approach, boosting the power of variational inference. In fact, it is known that #-P problems (e.g., computing the partition function) can be approximated in BPPNP, i.e. in bounded error probabilistic polyonomial time by an algorithm that has access to a NP-oracle [15, 16, 17]. Randomness appears to be crucial, and the ability to solve difficult (NP-equivalent) optimization problems, such as the one in in Eq. (2), does not appear to be sufficient. By leveraging randomization, we are able to boost variational inference approaches (such as mean field), obtaining formal approximation guarantees for general target probability distributions p. A pictorial representation is given in Figure 1."
    }, {
      "heading" : "3.1 Provably Tight Variational Bounds on the Partition Function",
      "text" : "Let D = {δx0|x0 ∈ X} denote the set of degenerate probability distributions over {0, 1}n, i.e. probability distributions that place all the probability mass on a single configuration. There are 2n such probability distributions and the entropy of each is zero. Given any probability distribution p, its projection on D, i.e., argminq∈DDKL(q||p), is given by a distribution that places all the probability on argmaxx∈X log p(x). Thus computing the I-projection on D is equivalent to solving a Most Probable Explanation query [1] which is NP-equivalent in the worst-case.\nLetQ ⊇ D be a family of probability distributions that contains D. Our key result is that we can get provably tight bounds on the partition function Z by taking an I-projection onto Q after a suitable random projection. This is formalized by the following theorem:\nTheorem 1. Let Ai,t ∈ {0, 1}i×n iid∼ Bernoulli( 12 ) and b i,t ∈ {0, 1}i iid∼ Bernoulli( 12 ) for i ∈ [0, n] and t ∈ [1, T ]. Let Q be a family of distributions such that D ⊆ Q. Let\nγi,t = exp ( max q∈Q θ · ∑\nx:Ai,tx=bi,t\nq(x)φ(x)− ∑\nx:Ai,tx=bi,t\nq(x) log q(x) ) (4)\nbe the optimal solutions for the projected variational inference problems. Then for all i ∈ [0, n] and for all T ≥ 1 the rescaled variational solution is a lower bound for Z in expectation\nE\n[ 1\nT T∑ t=1 γi,t2i\n] = E[γi,t2i] ≤ Z\nThere also exists an m such that for any δ > 0 and positive constant α ≤ 0.0042, if T ≥ 1 α (log(n/δ)) then with probability at least (1− 2δ)\n1\nT T∑ t=1 γm,t2m ≥ Z 64(n+ 1)\n(5)\n4Z ≥Median ( γm,1, · · · , γm,T ) 2m ≥ Z\n32(n+ 1) (6)\nProof. See Appendix.\nThis proves that appropriately rescaled variational lower bounds obtained on the randomly projected models (aggregated through median or mean) are within a factor n of the true value Z, where n is the number of variables in the model. This is an improvement on prior variational approximations which can either be unboundedly suboptimal or provide guarantees that hold only in expectation [18]; in contrast, our bounds are tight and require a relatively small number of samples proportional to log n/δ. The proof, reported in the appendix for space reasons, relies on the following technical result which can be seen as a variational interpretation of Theorem 1 from [17] and is of independent interest: Theorem 2. For any δ > 0, and positive constant α ≤ 0.0042, let T ≥ 1α (log(n/δ)). Let A\ni,t ∈ {0, 1}i×n iid∼ Bernoulli( 12 ) and b i,t ∈ {0, 1}i iid∼ Bernoulli( 12 ) for i ∈ [0, n] and t ∈ [1, T ]. Let\nδi,t = min q∈D DKL(q||RiAi,t,bi,t(p))\nThen with probability at least (1− δ) n∑ i=0 exp ( Median ( −δi,1 + logZ(Ai,1, bi,1), · · · ,−δi,T + logZ(Ai,T , bi,T ) )) 2i−1 (7)\nis a 32-approximation to Z.\nThe proof can be found in the appendix. Intuitively, Theorem 2 states that one can always find a small number of degenerate distributions (which can be equivalently thought of as special states that can be discovered through random projections and KL-divergence minimization) that are with high probability representative of the original probabilistic model, regardless of how complex the model is. Theorem 1 extends this idea to more general families of distributions such as Mean Field."
    }, {
      "heading" : "3.2 Solving Randomly Projected Variational Inference Problems",
      "text" : "To apply the results from Theorem 1 we must choose a tractable approximating family D ⊆ Q for the I-projection part and integrate our random projections into the optimization procedure. We focus on mean field (Q = QMF ) as our approximating family, but the results can be easily extended to structured mean field [19]. For simplicity of exposition we consider only probabilistic models p with unary and binary factors (e.g. Ising models, restricted Boltzmann machines). That is, p(x) = exp(θ · φ(x))/Z, where φ(x) are single node and pairwise edge indicator variables. Recall that our projectionRmA,b(p) constrains the distribution p to {x|Ax = b mod 2}. The projected variational optimization problem (4) is therefore\nlogZ(A, b) ≥ max q\nθ · ∑\nx|Ax=b mod 2\nq(x)φ(x)− ∑\nx|Ax=b mod 2\nq(x) log q(x)\nOr, equivalently,\nlogZ(A, b) ≥ max µ\nθ · µ+ n∑\ni=m+1\nH(µi) (8)\nwhere µ is the vector of singleton and pairwise marginals of q(x) and H(µi) is the entropy of a Bernoulli random variable with parameter µi. To solve this optimization problem efficiently we need a clever way to take into account the parity constraints, for running traditional mean field with message passing as in [18] would fail in the normalization step because of the presence of hard parity constraints. The key idea is to consider the equivalent row-reduced representation of the constraints from (3) and define\nq(x1, · · · , xn) = n∏\ni=m+1\nqi(xi) m∏ k=1 1\n{ (1− 2xk) =\nn∏ i=m+1 (1− 2Ckixi)(1− 2bk)\n}\nwhere we have a set of independent “free variables” (wlog., the last n−m) and a set of “constrained variables” (the first m) that are always set as to satisfy the parity constraints. Since the variables x1, . . . , xm are fully determined by xm+1, . . . , xn, we see that the marginals µ1, . . . , µm are also determined by µm+1, . . . , µn, as shown by the following proposition: Proposition 1. The singleton and pairwise marginals in (8) can be computed as follows:\nSingleton marginals: for k ∈ [m+ 1, n], µk = Eq [xk] = qk(1). For k ∈ [1,m],\nµk = ( 1− (1− 2bk)\nn∏ i=m+1 (1− 2Ckiµi)\n) /2\nPairwise marginals: for k, ` ∈ [m+ 1, n], µkl = Eq[xkx`] = µkµ`. For k ∈ [m+ 1, n], ` ∈ [1,m]\nµkl =\n{ µk 1 2 (1 + (1− 2bl) ∏n i 6=k,i=m+1(1− 2Cliµi)) if Clk = 1\nµkµl otherwise\nFor k ∈ [1,m], ` ∈ [1,m]\nµkl = 1\n4 (1 + (1− 2bk)(1− 2bl) n∏ i=m+1 (1− µi(2Cki + 2Cli − 4CkiCli))\n− (1− 2bk) n∏\ni=m+1\n(1− 2Ckiµi)− (1− 2bl) n∏\ni=m+1\n(1− 2Cliµi))\nThe derivation is found in the appendix. We can therefore maximize the lower bound in (8) by optimizing only over the “free marginals” µm+1, . . . , µn, as the remaining one are completely determined per Proposition 1. Compared to a traditional mean field variational approximation, we have a problem with a smaller number of variables, but with additional non-convex constraints."
    }, {
      "heading" : "4 Algorithm: Mean Field with Random Projections",
      "text" : "Theorem 1 guarantees that the approximation to Z has a tight lower bound only if we are able to find globally optimal solutions for (8). However, nontrivial variational inference problems (2) are nonconvex in general even without any random projections and even whenQ is simple, e.g.,Q = QMF . We do not explicitly handle this nonconvexity, but nevertheless we show empirically that we can vastly improve on mean field lower bounds. The key insight for our optimization procedure is that the objective function is still coordinate-wise concave, like in a traditional mean-field approximation: Proposition 2. The objective function θ · µ+ ∑n i=m+1H(µi) in (8) is concave with respect to any particular free marginal µm+1, . . . , µn.\nProof. By inspection, all the marginals in Proposition 1 are linear with respect to any specific free marginal µm+1, . . . , µn. Since the entropy term is concave, the RHS in (8) is concave in each free marginal µm+1, . . . , µn.\nSince (8) is concave in each variable we devise a coordinate-wise ascent algorithm, called Mean Field with Random Projections (MFRP), for maximizing the lower bound in (8) over the free marginals defined in Proposition 1. Starting from a random initialization, we iterate over each free marginal µk and maximize (8) with the rest of the free marginals fixed by setting the gradient with respect to µk equal to 0 and solving for µk. The closed form expressions we use are reported in the appendix. Because the overall optimization problem is not concave the algorithm may converge at a local maximum; therefore, we use J random initializations and use the best lower bound found across the J runs of the ascent algorithm. For a given m, we repeat this procedure T times and return the median across the runs. Each coordinate ascent step for free marginal µi takes O(m+ n+ |Ecc|(n−m)) steps in expectation where Ecc is the number of variables co-occurring in a parity constraint. Recomputing the constrained marginals takes O(m(n−m)) steps. The algorithm returns the maximum of MFRP(p(x),m) over m ∈ [0, n]. If MFRP finds a global optimum, then Theorem 1 guarantees it is a tight lower bound for logZ with high probability. Since MFRP uses coordinate-wise ascent we cannot certify global optimality; however, our experiments show large improvements in the lower bound when compared to existing variational methods.\nAlgorithm 1 MFRP(p(x) ∝ ∏ α ψα({x}α),m)\nfor t = 1, · · · , T do . Do T random projections Generate parity bits b(t) iid∼ Bernoulli( 12 ) m . Generate random projection Rm A(t),b(t)\nGenerate matrix A(t) iid∼ Bernoulli( 12 ) m×n Row reduce A,b and permute to yield C = [I|A′] and b . Compute constraints Z̃(t) ← 0 for j = 1, · · · , J do . Try different initializations\nInitialize µ(j,t) iid∼ Unif(0, 1)n for l = 1, · · · ,m do . Compute constrained marginals\nµ(j,t) ← ( 1− ∏n i=m+1(1− 2Cliµ (j,t) i )(1− 2bl) ) /2\nend for while not converged do . Stop when the increment is small or timeout\nfor k = m+ 1, · · · , n do . Coordinate ascent over free marginals µ (j,t) k ← argmaxµk θ · µ(j,t) + ∑n i=m+1H(µ (j,t) i )\nfor l = 1, · · · ,m do . Update constrained marginals µ (j,t) l ← ( 1− ∏n i=m+1(1− 2Cliµ (j,t) i )(1− 2bl) ) /2\nend for end for\nend while end for Z̃(t) ← maxj exp(θ · µ(j,t) + ∑n i=m+1H(µ (j,t) i )) . Pick best over initializations\nend for Return 2mMedian ( Z̃(1), . . . , Z̃(T ) ) . Aggregate across projections"
    }, {
      "heading" : "5 Experiments",
      "text" : "We investigate MFRP’s empirical performance on Ising models and on Restricted Boltzmann Machines. In particular, we are interested in the log partition function estimates and in the quality of the marginal estimates. Where applicable, exact ground truth estimates are obtained with the libDAI implementation of Junction Tree [20]. Upper bounds are calculated with Tree-Reweighted Belief Propagation (TRW-BP) [21], also implemented in libDAI. All methods are compared to mean field (MF) optimized with coordinate-wise ascent and random restarts."
    }, {
      "heading" : "5.1 Ising Models",
      "text" : "We consider n×n binary grid Ising models with variables xi ∈ {−1, 1} and potentials ψij(xi, xj) = exp(wijxixj + fixi + fjxj). In particular, we look at mixed models where the wij’s are drawn uniformly from [−10, 10] and the fi’s uniformly from [−1, 1]. Figure 2 compares the log partition function estimates from MF, Junction Tree, MFRP, and TRWBP. For each grid size, we generated five different grids and computed the mean field estimate for each as a baseline lower bound. For each of the five grids we also computed the best MFRP lower bound over m ∈ [0, 20] with T = 5 trials each. For comparison we include the exact log partition calculation from Junction Tree up to n = 20 and the TRW-BP upper bounds for all n. We plot the mean and standard error bars of the log ratio of each estimate over mean field for each method over the five grids. Note that for large grid sizes, the lower bound provided by MFRP is hundreds of orders of magnitude better than than those found by mean field.\nFinally, we consider the empirical runtime of the method for varying grid sizes n and number of constraints m in Figure 3. As expected, the runtime for mean field grows linearly in the number of variables in the graph (quadratically with n) and there is a linear slowdown as constraints are added to the optimization."
    }, {
      "heading" : "5.2 Restricted Boltzmann Machines",
      "text" : "We train Restricted Boltzmann Machines (RBMs) [22] using Contrastive Divergence (CD) [23, 24] on the MNIST hand-written digits dataset. In an RBM there is a layer of nh hidden binary variables h = h1, · · · , hnh and a layer of nv binary visible units v = v1, · · · , vnv . The joint probability distribution is given by P (h, v) = 1Z exp(b\n′v + c′h + h′Wv). We use nh ∈ {100, 200} hidden units and nv = 28 × 28 = 784 visible units. We learn the parameters b, c,W using CD-k for k ∈ {1, 5, 15}, where k denotes the number of Gibbs sampling steps used in the inference phase, with 15 training epochs and minibatches of size 20.\nWe then use MF and MFRP to estimate the log partition function and also consider the aggregate marginals of the sub-problems. For most of the cases we see a clear improvement in both the log partition lower bounds and in the marginals, with the marginal for No. Hidden Nodes = 100, k = 15 similar visually to an average over all digits in the MNIST dataset."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We introduced a new, general approach to variational inference that combines random projections with I-projections to obtain provably tight lower bounds for the log partition function. Our approach is the first to leverage universal hash functions and their properties in a variational sense. We demonstrated the effectiveness of this idea by extending mean field with random projections and empirically showed a large improvement in the partition function lower bounds and marginals obtained on both synthetic and real world data. Natural extensions to the approach include applications to other\nvariational methods, like the Bethe approximation, and the use of more powerful global optimization techniques instead of the coordinate-wise ascent currently used."
    }, {
      "heading" : "A Appendix : Proofs",
      "text" : "Proof of Theorem 2.\nmin q∈D\nDKL(q||RiAi,t,bi,t(p)) =\nmin q∈D ∑ x|Ai,tx=bi,t mod 2 q(x) log q(x)− θ · ∑ x|Ai,tx=bi,t mod 2 q(x)φ(x) + logZ(Ai,t, bi,t) =\nmin q∈D −θ · ∑ x|Ai,tx=bi,t mod 2 q(x)φ(x) + logZ(Ai,t, bi,t) =\n− max x|Ai,tx=bi,t mod 2 θφ(x) + logZ(Ai,t, bi,t)\nTherefore −min\nq∈D DKL(q||RiAi,t,bi,t(p)) + logZ(A i,t, bi,t) = max x|Ai,tx=bi,t mod 2 θφ(x)\nSubstituting into Eq. (7) we can rewrite as n∑\ni=0\nexp ( Median ( max\nx|Ai,1x=bi,1 mod 2 θφ(x), · · · , max x|Ai,T x=bi,T mod 2 θφ(x)\n)) 2i−1 =\nn∑ i=0 Median ( exp ( max x|Ai,1x=bi,1 mod 2 θφ(x) ) , · · · , exp ( max x|Ai,T x=bi,T mod 2 θφ(x) )) 2i−1 =\nn∑ i=0 Median ( max x|Ai,1x=bi,1 mod 2 exp(θφ(x)), · · · , max x|Ai,T x=bi,T mod 2 exp(θφ(x)) ) 2i−1\nThe result then follows directly from Theorem 1 from [17].\nProof of Theorem 1. For the first part, we have the following relationship on the expectation:\nE[Z(Ai,t, bi,t)] = 2−iZ\nFrom the non-negativity of the KL divergence we have that for any q ∈ Q logZ(Ai,t, bi,t) ≥ θ · ∑\nx:Ai,tx=bi,t\nq(x)φ(x)− ∑\nx:Ai,tx=bi,t\nq(x) log q(x)\nlogZ(Ai,t, bi,t) ≥ max q∈Q\n{ θ ·\n∑ x:Ai,tx=bi,t q(x)φ(x)− ∑ x:Ai,tx=bi,t q(x) log q(x)\n}\nZ(Ai,t, bi,t) ≥ exp ( max q∈Q { θ · ∑ x:Ai,tx=bi,t q(x)φ(x)− ∑ x:Ai,tx=bi,t q(x) log q(x) }) , γi,t (9)\nWe take the expectation of both sides to yield\nZ 2i = E\n[ Z(Ai,t, b) ] ≥ E [ exp ( max q∈Q { θ · ∑ x:Ai,tx=b q(x)φ(x)− ∑ x:Ai,tx=b q(x) log q(x) })] = E[γi,t]\nFor the second part. Since the conditions of Theorem 2 are satisfied, we know that equation (7) holds with probability at least 1 − δ. From (7), since the terms in the sum are non-negative we have that the maximum element is at least 1/(n+ 1) of the sum:\nmax i exp\n( Median ( −min\nq∈D DKL(q||RiAi,1,bi,1(p)) + logZ(A i,1, bi,1), · · · ,\n−min q∈D DKL(q||RiAi,T ,bi,T (p)) + logZ(A i,T , bi,T )\n)) 2i−1 ≥ 1\n32 Z\n1\nn+ 1\nTherefore there exists m such that\nMedian ( −min\nq∈D DKL(q||RmAm,1,bm,1(p)) + logZ(A m,1, bm,1), · · · ,\n−min q∈D DKL(q||RmAm,T ,bm,T (p)) + logZ(A m,T , bm,T )\n) + (m− 1) log 2 ≥ − log 32 + logZ − log(n+ 1)\nWe also have min q∈Q DKL(q||RA,b(p)) ≤ min q∈D DKL(q||RA,b(p))\nbecause D ⊆ Q. Thus\nMedian ( −min\nq∈Q DKL(q||RmAm,1,bm,1(p)) + logZ(A m,1, bm,1), · · · ,\n−min q∈Q DKL(q||RmAm,T ,bm,T (p)) + logZ(A m,T , bm,T )\n) + (m− 1) log 2 ≥ − log 32 + logZ − log(n+ 1)\nFrom the definition of DKL, we have DKL(q||RmA,b(p)) = − ∑\nx:Ax=b mod 2\nq(x)φ(x) + ∑\nx:Ax=b mod 2\nq(x) log q(x) + logZ(A, b)\nPluggin in we get Median ( log γm,1, · · · , log γm,T ) + (m− 1) log 2 ≥ − log 32− log(n+ 1) + logZ\nand also Median ( log γm,1, · · · , log γm,T ) +m log 2 ≥ − log 32− log(n+ 1) + logZ\nwith probability at least 1− δ.\nMedian ( γm,1, · · · , γm,T ) 2m ≥ Z\n32(n+ 1)\nSince the terms are non zero,\n1\nT T∑ t=1 γm,t ≥ 1 2 Median ( γm,1, · · · , γm,T ) therefore with probability at least 1− δ\n1\nT T∑ t=1 γm,t2m ≥ Z 64(n+ 1)\nFrom Markov’s inequality\nP [ Z(Ai,t, bi,t) ≥ cE[Z(Ai,t, bi,t)] ] ≤ 1 c\nP [ Z(Ai,t, bi,t)2i ≥ cZ ] ≤ 1 c\nTherefore since Z(Ai,t, bi,t) ≥ γi,t from (9), setting c = 4 and i = m we get\nP [ γm,t2m ≥ 4Z ] ≤ 1\n4\nFrom Chernoff’s inequality, P [ 4Z ≥Median ( γm,1, · · · , γm,T ) 2m ] ≥ 1− δ\nand the claim follows from union bound.\nProof of Proposition 1. For singleton marginals, when k ∈ [m + 1, n], xk is a free variable and thus µk = Eq[xk] = qk(1). When k ∈ [1,m],\n(1− 2xk) = (1− 2bk) n∏\ni=m+1\n(1− 2Ckixi)\nTake the expectation on both side and since xi for i ∈ [m+ 1, n] are free (independent) variables, we have\n(1− 2µk) = (1− 2bk) n∏\ni=m+1\n(1− 2Ckiµi)\nThat is,\nµk = ( 1− (1− 2bk)\nn∏ i=m+1 (1− 2Ckiµi)\n) /2\nFor the binary marginal µkl, there are three cases: both xk, xl are free variables; one is free and the other is constrained; both are constrained. For the first case, k, ` ∈ [m+ 1, n], they are independent and thus\nµkl = Eq[xkx`] = µkµ`\nFor the second case, k ∈ [m+ 1, n], ` ∈ [1,m].\nµkl = Pr[xk = 1, xl = 1] = ∑\nxm+1, ..., xn, xk = 1 −1 = (1− 2xl) = (1− 2bl) ∏n i=m+1(1− 2Clixi)\nn∏ i=m+1 qi(xi)\nWhen Clk = 1, 1− 2Clkxk = −1,\nµkl = µk · ∑\nxm+1, ...xk−1, xk+1, ..., xn, 1 = (1− 2bl) ∏n i=m+1,i 6=k(1− 2Clixi)\nn∏ i=m+1,i 6=k qi(xi)\nLet’s introduce a new binary variable, u, satisfying the constraint\n(2u− 1) = (1− 2bl) n∏\ni6=k,i=m+1\n(1− 2Clixi)\nThen the above summation is over xm+1, ...xk−1, xk+1, ..., xn, u such that u = 1. The probability of u being 1 is the expected value of u. Therefore,\nµkl = µkE[u] = µk 1\n2 (1 + (1− 2bl) n∏ i 6=k,i=m+1 (1− 2Cliµi))\nWhen Clk = 0, xl is independent of xk, so µkl = µkµl For the last case, k, ` ∈ [1,m].\n(1− 2xk)(1− 2x`) = (1− 2bk)(1− 2b`) n∏\ni=m+1\n(1− 2Ckixi) n∏\ni=m+1\n(1− 2C`ixi)\nTaking the expected value of both side\n1− 2µl − 2µk + 4µkl = (1− 2bl)(1− 2bk) n∏\ni=m+1\nE[1− xi(2Cki + 2Cli) + 4CkiClix2i ]\nµkl = 1\n4 (−1 + 2µk + 2µl + (1− 2bk)(1− 2bl) n∏ i=m+1 (1− µi(2Cki + 2Cli − 4CkiCli)))\nPlug in the result of µk, µl:\nµkl = 1\n4 (1 + (1− 2bk)(1− 2bl) n∏ i=m+1 (1− µi(2Cki + 2Cli − 4CkiCli))\n− (1− 2bk) n∏\ni=m+1\n(1− 2Ckiµi)− (1− 2bl) n∏\ni=m+1\n(1− 2Cliµi))\nProposition 3 (The gradients for coordinate ascent). Assuming we are taking the gradient with respect to µk, where k ∈ [m+ 1, n]. 1. Unary term µk\n∂µk ∂µk = 1\nAnd thus ∂H(µk)\n∂µk =\n∂\n∂µk − (µk log(µk) + (1− µk) log(1− µk)) = log( 1− µk µk )\n2. Unary term µl, l ≥ m+ 1, l 6= k ∂µl ∂µk = 0\n∂H(µl)\n∂µk = 0\n3. Unary term µl, l ≤ m ∂µl ∂µk = ∂ ∂µk 1 2 (1− (1− 2bl) n∏ i=m+1 (1− 2Cliµi)) = (1− 2bl)Clk n∏ i=m+1,i6=k (1− 2Cliµi)\n4. Binary term, µkl, l ≥ m+ 1 ∂\n∂µk µkl = µl\n5. Binary term, µpl, both p, l ≥ m+ 1, p 6= k, l 6= k ∂\n∂µk µpl = 0"
    }, {
      "heading" : "6. Binary term, µkl, l ≤ m",
      "text" : "When Clk = 0, µkl = µkµl and its derivative is\nµl + µkµ ′ l = µl =\n1 2 (1− (1− 2bl) n∏ i=m+1,i 6=k (1− 2Cliµi))\nWhen Clk = 1, µkl = µk 12 (1 + (1− 2bl) ∏n i 6=k,i=m+1(1− 2Cliµi)). The derivative is\n1 2 (1 + (1− 2bl) n∏ i 6=k,i=m+1 (1− 2Cliµi))"
    }, {
      "heading" : "7. Binary term, µpl, where p ≥ m+ 1, p 6= k, l ≤ m",
      "text" : "When Clp = 0, µpl = µpµl and its derivative is\nµpµ ′ l When Clp = 1, µpl = µp 12 (1 + (1− 2bl) ∏n i 6=p,i=m+1(1− 2Cliµi)). The derivative is\n−µpCkl(1− 2bl) n∏\ni6=k,i6=p,i=m+1\n(1− 2Cliµi))\n8 Binary term µpl, where both p, l ≤ m ∂\n∂µk µpl =\n∂\n∂µk\n1 4 (1− (1− 2bp) n∏ i=m+1 (1− 2Cpiµi)\n− (1− 2bl) n∏\ni=m+1\n(1− 2Cliµi)\n+ (1− 2bp)(1− 2bl) n∏\ni=m+1\n(1− (2Cpi + 2Cli − 4cpiCli)µi))\n∂\n∂µk µpl = (1− 2bp)Cpk 2 n∏ i=m+1,i 6=k (1− 2Cpiµi) (= µ′p/2)\n+ (1− 2bl)Clk\n2\nn∏ i=m+1,i 6=k (1− 2Cpiµi) (= µ′l/2)\n− (1− 2bp)(1− 2bl)(2Cpk + 2Clk − 4CpkClk) 4 n∏ i=m+1,i 6=k (1− µi(2Cpi + 2Cli − 4CpiCli))\nAll gradients except the entropy one are independent of µk, so whole gradient can be expressed as\nc+ log( 1− µk µk ),\nwhere c is a constant with respect to µk. Therefore, the coordinate ascent step for µk is to set it to 1\n1 + exp(−c)"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "<lb>Information projections are the key building block of variational inference algo-<lb>rithms and are used to approximate a target probabilistic model by projecting it<lb>onto a family of tractable distributions. In general, there is no guarantee on the<lb>quality of the approximation obtained. To overcome this issue, we introduce a<lb>new class of random projections to reduce the dimensionality and hence the com-<lb>plexity of the original model. In the spirit of random projections, the projection<lb>preserves (with high probability) key properties of the target distribution. We show<lb>that information projections can be combined with random projections to obtain<lb>provable guarantees on the quality of the approximation obtained, regardless of<lb>the complexity of the original model. We demonstrate empirically that augmenting<lb>mean field with a random projection step dramatically improves partition function<lb>and marginal probability estimates, both on synthetic and real world data.",
    "creator" : "LaTeX with hyperref package"
  }
}