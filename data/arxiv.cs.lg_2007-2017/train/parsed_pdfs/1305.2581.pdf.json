{
  "name" : "1305.2581.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Accelerated Mini-Batch Stochastic Dual Coordinate Ascent",
    "authors" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "imization problems in machine learning. This paper considers an extension of SDCA under the minibatch setting that is often used in practice. Our main contribution is to introduce an accelerated minibatch version of SDCA and prove a fast convergence rate for this method. We discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of Nesterov [2007]."
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the following generic optimization problem. Let φ1, . . . , φn be a sequence of vector convex functions from Rd to R, and let g : Rd → R be a strongly convex regularization function. Our goal is to solve minx∈Rd P (x) where\nP (x) =\n[ 1\nn n∑ i=1 φi(x) + g(x)\n] . (1)\nFor example, given a sequence of n training examples (v1, y1), . . . , (vn, yn), where vi ∈ Rd and yi ∈ R, ridge regression is obtained by setting g(x) = λ2‖x‖\n2 and φi(x) = (x>vi − yi)2. Regularized logistic regression is obtained by setting φi(x) = log(1 + exp(−yix>vi)).\nThe dual problem of (1) is defined as follows: For each i, let φ∗i : Rd → R be the convex conjugate of φi, namely, φ∗i (u) = maxz∈Rd(z\n>u − φi(z)). Similarly, let g∗ be the convex conjugate of g. The dual problem is:\nmax α∈Rd×n D(α) where D(α) =\n[ 1\nn n∑ i=1 −φ∗i (−αi)− g∗ ( 1 n n∑ i=1 αi )] , (2)\nwhere for each i, αi is the i’th column of the matrix α. The dual objective has a different dual vector associated with each primal function. Dual Coordinate Ascent (DCA) methods solve the dual problem iteratively, where at each iteration of DCA, the dual objective is optimized with respect to a single dual vector, while the rest of the dual vectors are kept in tact. Recently, Shalev-Shwartz and Zhang [2013] analyzed a stochastic version of dual coordinate ascent, abbreviated by SDCA, in which at each round we choose which dual vector to optimize uniformly at random. In particular, let x∗ be the optimum of (1). We say that a solution x is -accurate if P (x) − P (x∗) ≤ . Shalev-Shwartz and Zhang [2013] have derived the following convergence guarantee for SDCA: If g(x) = λ2‖x‖ 2 2 and each φi is γ-smooth, then for every > 0, if we run SDCA for at least( n+ 1λγ ) log((n+ 1λγ ) · 1 )\nar X\niv :1\n30 5.\n25 81\nv1 [\nst at\n.M L\n] 1\n2 M\nay 2\n01 3\niterations, then the solution of the SDCA algorithm will be -accurate (in expectation). This convergence rate is significantly better than the more commonly studied stochastic gradient descent (SGD) methods that are related to SDCA1.\nAnother approach to solving (1) is deterministic gradient descent methods. In particular, Nesterov [2007] proposed an accelerated gradient descent (AGD) method for solving (1). Under the same conditions mentioned above, AGD finds an -accurate solution after performing\nO ( 1√ λγ log(1 ) ) iterations.\nThe advantage of SDCA over AGD is that each iteration involves only a single dual vector and in general costs O(d). In contrast, each iteration of AGD requires Ω(nd) operations. On the other hand, AGD has a better dependence on the condition number of the problem — the iteration bound of AGD scales with 1/ √ λγ while the iteration bound of SDCA scales with 1/(λγ).\nIn this paper we describe and analyze a new algorithm that interpolates between SDCA and AGD. At each iteration of the algorithm, we randomly pick a subset of m indices from {1, . . . , n} and update the dual vectors corresponding to this subset. This subset is often called a mini-batch. The use of mini-batches is common with SGD optimization, and it is beneficial when the processing time of a mini-batch of size m is much smaller than m times the processing time of one example (mini-batch of size 1). For example, in the practical training of neural networks with SGD, one is always advised to use mini-batches because it is more efficient to perform matrix-matrix multiplications over a mini-batch than an equivalent amount of matrix-vector multiplication operations (each over a single training example). This is especially noticeable when GPU is used: in some cases the processing time of a mini-batch of size 100 may be the same as that of a mini-batch of size 10. Another typical use of mini-batch is for parallel computing, which was studied by various authors for stochastic gradient descent (e.g., Dekel et al. [2012]). This is also the application scenario we have in mind, and will be discussed in greater details in Section 3.\nRecently, Takác et al. [2013] studied mini-batch variants of SDCA in the context of the Support Vector Machine (SVM) problem. They have shown that the naive mini-batching method, in whichm dual variables are optimized in parallel, might actually increase the number of iterations required. They then describe several “safe” mini-batching schemes, and based on the analysis of Shalev-Shwartz and Zhang [2013], have shown several speed-up results. However, their results are for the non-smooth case and hence they do not obtain linear convergence rate. In addition, the speed-up they obtain requires some spectral properties of the training examples. We take a different approach and employ Nesterov’s acceleration method, which has previously been applied to mini-batch SGD optimization. This paper shows how to achieve acceleration for SDCA in the mini-batch setting. The pseudo code of our Accelerated Mini-Batch SDCA, abbreviated by ASDCA, is presented below.\n1An exception is the recent analysis given in Le Roux et al. [2012] for a variant of SGD.\nProcedure Accelerated Mini-Batch SDCA\nParameters scalars λ, γ and θ ∈ [0, 1] ; mini-batch size m Initialize α(0)1 = · · · = α (0) n = ᾱ(t) = 0, x(0) = 0 Iterate: for t = 1, 2, . . . u(t−1) = (1− θ)x(t−1) + θ∇g∗(ᾱ(t−1)) Randomly pick subset I ⊂ {1, . . . , n} of size m and update the dual variables in I\nα (t) i = (1− θ)α (t−1) i − θ∇φi(u(t−1)) for i ∈ I α (t) j = α (t−1) j for j /∈ I\nᾱ(t) = ᾱ(t−1) + n−1 ∑\ni∈I(α (t) i − α (t−1) i )\nx(t) = (1− θ)x(t−1) + θ∇g∗(ᾱ(t)) end\nIn the next section we present our main result — an analysis of the number of iterations required by ASDCA. We focus on the case of Euclidean regularization, namely, g(x) = λ2‖x‖\n2. Analyzing more general strongly convex regularization functions is left for future work. In Section 3 we discuss parallel implementations of ASDCA and compare it to parallel implementations of AGD and SDCA. In particular, we explain in which regimes ASDCA can be better than both AGD and SDCA. In Section 4 we present some experimental results, demonstrating how ASDCA interpolates between AGD and SDCA. The proof of our main theorem is presented in Section 5. We conclude with a discussion of our work in light of related works in Section 6."
    }, {
      "heading" : "2 Main Results",
      "text" : "Our main result is a bound on the number of iterations required by ASDCA to find an -accurate solution. In our analysis, we only consider the squared Euclidean norm regularization,\ng(x) = λ\n2 ‖x‖2,\nwhere ‖·‖ is the Euclidean norm and λ > 0 is a regularization parameter. The analysis for general λ-strongly convex regularizers is left for future work. For the squared Euclidean norm we have\ng∗(α) = 1\n2λ ‖α‖2,\nand ∇g∗(α) = 1\nλ α .\nWe further assume that each φi is 1/γ-smooth with respect to ‖ · ‖, namely,\n∀x, z, φi(x) ≤ φi(z) +∇φi(z)>(x− z) + 1\n2γ ‖x− z‖2.\nFor example, if φi(x) = (x>vi − yi)2, then it is ‖vi‖2-smooth. The smoothness of φi also implies that φ∗i (α) is γ-strongly convex:\n∀θ ∈ [0, 1], φ∗i ((1− θ)α+ θβ) ≤ (1− θ)φ∗i (α) + θφ∗i (β)− θ(1− θ)γ\n2 ‖α− β‖2,\nTheorem 1. Assume that g(x) = 12λ‖x‖ 2 2 and for each i, φi is (1/γ)-smooth w.r.t. the Euclidean norm. Suppose that the ASDCA algorithm is run with parameters λ, γ,m, θ, where\nθ ≤ 1 4 min\n{ 1 , √ γλn\nm , γλn ,\n(γλn)2/3\nm1/3\n} . (3)\nDefine the dual sub-optimality by ∆D(α) = D(α∗)−D(α), where α∗ is the optimal dual solution, and the primal sub-optimality by ∆P (x) = P (x)−D(α∗). Then,\nmE∆P (x(t)) + nE∆D(α(t)) ≤ (1− θm/n)t[m∆P (x(0)) + n∆D(α(0))]."
    }, {
      "heading" : "It follows that after performing",
      "text" : "t ≥ n/m θ log\n( m∆P (x(0)) + n∆D(α(0))\nm\n)\niterations, we have that E[P (x(t))−D(α(t))] ≤ .\nLet us now discuss the bound, assuming θ is taken to be the right-hand side of (3). The dominating factor of the bound on t becomes\nn\nmθ =\nn m ·max\n{ 1 , √ m\nγλn ,\n1\nγλn ,\nm1/3\n(γλn)2/3\n} (4)\n= max\n{ n\nm ,\n√ n/m\nγλ ,\n1/m\nγλ ,\nn1/3\n(γλm)2/3\n} . (5)\nTable 1 summarizes several interesting cases, and compares the iteration bound of ASDCA to the iteration bound of the vanilla SDCA algorithm (as analyzed in Shalev-Shwartz and Zhang [2013]) and the Accelerated Gradient Descent (AGD) algorithm of Nesterov [2007]. In the table, we ignore constants and logarithmic factors.\nAs can be seen in the table, the ASDCA algorithm interpolates between SDCA and AGD. In particular, ASDCA has the same bound as SDCA when m = 1 and the same bound as AGD when m = n. Recall that the cost of each iteration of AGD scales with n while the cost of each iteration of SDCA does not scale with n. The cost of each iteration of ASDCA scales with m. To compensate for the difference cost per iteration for different algorithms, we may also compare the complexity in terms of the number of examples processed in Table 2. This is also what we will study in our empirical experiments. It should be mentioned that this comparison is meaningful in a single processor environment, but not in a parallel computing environment when multiple examples can be processed simultaneiously in a minibatch. In the next section we discuss under what conditions the overall runtime of ASDCA is better than both AGD and SDCA."
    }, {
      "heading" : "3 Parallel Implementation",
      "text" : "In recent years, there has been a lot of interest in implementing optimization algorithms using a parallel computing architecture (see Section 6). We now discuss how to implement AGD, SDCA, and ASDCA when having a computing machine with s parallel computing nodes.\nIn the calculations below, we use the following facts:\n• If each node holds a d-dimensional vector, we can compute the sum of these vectors in timeO(d log(s)) by applying a “tree-structure” summation (see for example the All-Reduce architecture in Agarwal et al. [2011]).\n• A node can broadcast a message with c bits to all other nodes in time O(c log2(s)). To see this, order nodes on the corners of the log2(s)-dimensional hypercube. Then, at each iteration, each node sends the message to its log(s) neighbors (namely, the nodes whose code word is at a hamming distance of 1 from the node). The message between the furthest away nodes will pass after log(s) iterations. Overall, we perform log(s) iterations and each iteration requires transmitting c log(s) bits.\n• All nodes can broadcast a message with c bits to all other nodes in time O(cs log2(s)). To see this, simply apply the broadcasting of the different nodes mentioned above in parallel. The number of iterations will still be the same, but now, at each iteration, each node should transmit cs bits to its log(s) neighbors. Therefore, it takes O(cs log2(s)) time.\nFor concreteness of the discussion, we consider problems in which φi(x) takes the form of `(x>vi, yi), where yi is a scalar and vi ∈ Rd. This is the case in supervised learning of linear predictors (e.g. logistic regression or ridge regression). We further assume that the average number of non-zero elements of vi is d̄. In very large-scale problems, a single machine cannot hold all of the data in its memory. However, we assume that a single node can hold a fraction of 1/s of the data in its memory.\nLet us now discuss parallel implementations of the different algorithms starting with deterministic gradient algorithms (such as AGD). The bottleneck operation of deterministic gradient algorithms is the calculation of the gradient. In the notation mentioned above, this amounts to performing order of nd̄ operations. If the data is distributed over s computing nodes, where each node holds n/s examples, we can calculate the gradient in time O(nd̄/s + d log(s)) as follows. First, each node calculates the gradient over its own n/s examples (which takes time O(nd̄/s)). Then, the s resulting vectors in Rd are summed up in time O(d log(s)).\nNext, let us consider the SDCA algorithm. On a single computing node, it was observed that SDCA is much more efficient than deterministic gradient descent methods, since each iteration of SDCA costs only Θ(d̄) while each iteration of AGD costs Θ(nd̄). When we have s nodes, for the SDCA algorithm, dividing the examples into s computing nodes does not yield any speed-up. However, we can divide the features into the s nodes (that is, each node will hold d/s of the features for all of the examples). This enables the computation of x>vi in (expected) time of O(d̄/s+ s log2(s)). Indeed, node t will calculate ∑ j∈Jt xjvi,j ,\nwhere Jt is the set of features stored in node t (namely, |Jt| = d/s). Then, each node broadcasts the resulting scalar to all the other nodes. Note that we will obtain a speed-up over the naive implementation only if s log2(s) d̄.\nFor the ASDCA algorithm, each iteration involves the computation of the gradient over m examples. We can choose to implement it by dividing the examples to the s nodes (as we did for AGD) or by dividing the features into the s nodes (as we did for SDCA). In the first case, the cost of each iteration is O(md̄/s+ d log(s)) while in the latter case, the cost of each iteration is O(md̄/s + ms log2(s)). We will choose between these two implementations based on the relation between d,m, and s.\nThe runtime and communication time of each iteration is summarized in the table below.\nAlgorithm partition type runtime communication time\nSDCA features d̄/s s log2(s)\nASDCA features d̄m/s ms log2(s)\nASDCA examples d̄m/s d log(s)\nAGD examples d̄n/s d log(s)\nWe again see that ASDCA nicely interpolates between SDCA and AGD. In practice, it is usually the case that there is a non-negligible cost of opening communication channels between nodes. In that case, it will be better to apply the ASDCA with a value of m that reflects an adequate tradeoff between the runtime of each node and the communication time. With the appropriate value of m (which depends on constants like the cost of opening communication channels and sending packets of bits between nodes), ASDCA may outperform both SDCA and AGD."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "In this section we demonstrate how ASDCA interpolates between SDCA and AGD. All of our experiments are performed for the task of binary classification with a smooth variant of the hinge-loss (see ShalevShwartz and Zhang [2013]). Specifically, let (v1, y1), . . . , (vm, ym) be a set of labeled examples, where for every i, vi ∈ Rd and yi ∈ {±1}. Define φi(x) to be\nφi(x) =  0 yix >vi > 1\n1/2− yix>vi yix>vi < 0 1 2(1− yix >vi) 2 o.w.\nWe also set the regularization function to be g(x) = λ2‖x‖ 2 2 where λ = 1/n. This is the default value for the regularization parameter taken in several optimization packages. Following Shalev-Shwartz and Zhang [2013], the experiments were performed on three large datasets with very different feature counts and sparsity. The astro-ph dataset classifies abstracts of papers from the physics ArXiv according to whether they belong in the astro-physics section; CCAT is a classification task taken from the Reuters RCV1 collection; and cov1 is class 1 of the covertype dataset of Blackard, Jock & Dean. The following table provides details of the dataset characteristics.\nDataset Training Size Testing Size Features Sparsity astro-ph 29882 32487 99757 0.08%\nCCAT 781265 23149 47236 0.16% cov1 522911 58101 54 22.22%\nWe ran ASDCA with values of m from the set {10−4n, 10−3n, 10−2n}. We also ran the SDCA algorithm and the AGD algorithm. In Figure 1 we depict the primal sub-optimality of the different algorithms as a function of the number of examples processed. Note that each iteration of SDCA processes a single example, each iteration of ASDCA processes m examples, and each iteration of AGD processes n examples. As can be seen from the graphs, ASDCA indeed interpolates between SDCA and AGD. It is clear from the graphs that SDCA is much better than AGD when we have a single computing node. ASDCA performance is quite similar to SDCA when m is not very large. As discussed in Section 3, when we have parallel computing nodes and there is a non-negligible cost of opening communication channels between nodes, running ASDCA with an appropriate value ofm (which depends on constants like the cost of opening communication channels) may yield the best performance."
    }, {
      "heading" : "5 Proof",
      "text" : "We use the following notation:\nf(x) = 1\nn n∑ i=1 φi(x) ,\n∆ᾱ(t) = ᾱ(t) − ᾱ(t−1) .\nIn addition, we use the notation Et to denote the expectation over the choice of the set I at iteration t, conditioned on the values of x(t−1) and α(t−1).\nOur first lemma calculates the expected value of ∆ᾱ(t).\nLemma 1. At each round t, we have\nEt[∆ᾱ(t)] = − θm\nn\n( ᾱ(t−1) +∇f(u(t−1)) ) .\nProof. By the definition of the update,\n∆ᾱ(t) = 1\nn ∑ i∈I (α (t) i −α (t−1) i ) = −θ n ∑ i∈I (α (t−1) i +∇φi(u (t−1))) = −θ n n∑ i=1 1[i ∈ I](α(t−1)i +∇φi(u (t−1))) .\nTaking expectation w.r.t. the choice of I and noting that Et[1[i ∈ I]] = m/n we obtain that\nEt[∆ᾱ(t)] = −θm n2 n∑ i=1 (α (t−1) i +∇φi(u (t−1))) = −θm n ( ᾱ(t−1) +∇f(u(t−1)) ) .\nNext, we upper bound the “variance” of ∆ᾱ(t), in the sense of the expected squared norm of the difference between ∆ᾱ(t) and its expectation.\nLemma 2. At each round t, we have\nEt‖∆ᾱ(t) − Et∆ᾱ(t)‖2 ≤ mθ2\nn3 n∑ i=1 ‖α(t−1)i +∇φi(u (t−1))‖2.\nProof. We introduce the simplified notation βi = −θ(α(t−1)i + ∇φi(u(t−1))) and µ = n mEt∆ᾱ\n(t). Note that βi is independent of the choice of I (thus can be considered as a deterministic number). Then βi = α (t) i − α (t−1) i when i ∈ I , and n−1 ∑n i=1 βi = µ. We thus have\nEt‖∆ᾱ(t) − Et∆ᾱ(t)‖2 = Et‖n−1 ∑ i∈I (βi − µ)‖2\n=n−2Et ∑ i,j∈I (βi − µ)>(βj − µ)\n=n−2Et ∑ i∈I ‖βi − µ‖2 + n−2Et ∑ i 6=j∈I (βi − µ)>(βj − µ).\nNote that for any i 6= j ∈ I: βi−µ and βj −µ can be regarded as zero-mean random vectors that are drawn uniformly at random from the same distribution without replacement. Therefore they are not positively correlated when i 6= j. That is, we have\nEt ∑ i 6=j∈I (βi − µ)>(βj − µ) ≤ 0.\nTherefore\nEt‖∆ᾱ(t) − Et∆ᾱ(t)‖2 ≤ 1 n2 Et ∑ i∈I ‖βi − µ‖2 = m n3 n∑ i=1 ‖βi − µ‖2\n≤m n3 n∑ i=1 ‖βi‖2 = mθ2 n3 n∑ i=1 ‖α(t−1)i +∇φi(u (t−1))‖2.\nRecall that the theorem upper bounds the expected value of m∆P (x(t)) + n∆D(α(t)), which in turns upper bound the duality gap at round t. The following lemma derives an upper bound on this quantity that depends on the value of this quantity at the previous iteration and three additional terms. We will later show that the sum of the additional terms is negative in expectation. The lemma uses standard algebraic manipulations as well as the assumptions on g and φi.\nLemma 3. For each round t we have[ m∆P (x(t)) + n∆D(α(t)) ] − (\n1− θm n\n)[ m∆P (x(t−1)) + n∆D(α(t−1)) ] ≤ a(t)I + b (t) I + c (t) I ,\nwhere\na (t) I =\nm 2γ ‖x(t) − u(t−1)‖2 − θ(1− θ)γ 2 ∑ i∈I ‖α(t−1)i +∇φi(u (t−1))‖2 ,\nb (t) I = θmf(u\n(t−1)) + mθ\nn n∑ i=1 φ∗i (−α (t−1) i ) +m∇f(u (t−1))>(−θu(t−1))\n+ ∑ i∈I [ −θφ∗i (−α (t−1) i ) + θ[∇φi(u (t−1))>u(t−1) − φi(u(t−1))] ] ,\nc (t) I = m∇f(u (t−1))>(x(t) − (1− θ)x(t−1)) + n+mθ 2λ ‖ᾱ(t)‖2 − n−mθ 2λ ‖ᾱ(t−1)‖2 .\nProof. Since m/n ≤ 1 we have[ m∆P (x(t)) + n∆D(α(t)) ] − (\n1− θm n\n)[ m∆P (x(t−1)) + n∆D(α(t−1)) ] ≤ [ m∆P (x(t)) + n∆D(α(t)) ] − [ (m− θm)∆P (x(t−1)) + (n− θm)∆D(α(t−1))\n] = m[∆P (x(t))−∆P (x(t−1))] + n[∆D(α(t))−∆D(α(t−1))] + θm[∆P (x(t−1)) + ∆D(α(t−1))] = m[P (x(t))− P (x(t−1))]− n[D(α(t))−D(α(t−1))] + θm[P (x(t−1))−D(α(t−1))] = m[P (x(t))− (1− θ)P (x(t−1))]− n[D(α(t))−D(α(t−1))]− θmD(α(t−1)) .\nTherefore, we need to show that the right-hand side of the above is upper bounded by a(t)I + b (t) I + c (t) I .\nStep 1: We first bound m[P (x(t))− (1− θ)P (x(t−1))]. Using the smoothness of f we have\nf(x(t)) ≤ f(u(t−1)) +∇f(u(t−1))>(x(t) − u(t−1)) + 1 2γ ‖x(t) − u(t−1)‖2\n= (1− θ)f(u(t−1)) + θf(u(t−1)) +∇f(u(t−1))>(x(t) − u(t−1)) + 1 2γ ‖x(t) − u(t−1)‖2\nand using the convexity of f we also have (1− θ)f(u(t−1)) ≤ (1− θ) [ f(x(t−1))−∇f(u(t−1))>(x(t−1) − u(t−1)) ] .\nCombining the above two inequalities and rearranging terms we obtain\nf(x(t)) ≤ (1− θ)f(x(t−1)) + θf(u(t−1)) (6) +∇f(u(t−1))> ( x(t) − θu(t−1) − (1− θ)x(t−1) ) + 1\n2γ ‖x(t) − u(t−1)‖2 .\nNext, using the convexity of g we have\ng(x(t)) = λ 2 ‖x(t)‖2 = λ 2 ∥∥∥∥(1− θ)x(t−1) + θλᾱ(t) ∥∥∥∥2 ≤ λ(1− θ)2 ‖x(t−1)‖2 + θ2λ‖ᾱ(t)‖2 .\nCombining this with (6) we obtain\nP (x(t)) = f(x(t)) + g(x(t)) ≤ (1− θ) ( f(x(t−1)) + λ2‖x (t−1)‖2 ) + θf(u(t−1))\n+∇f(u(t−1))> ( x(t) − θu(t−1) − (1− θ)x(t−1) ) + 1\n2γ ‖x(t) − u(t−1)‖2 + θ 2λ ‖ᾱ(t)‖2 ,\nwhich yields m [ P (x(t))− (1− θ)P (x(t−1)) ] ≤ mθf(u(t−1)) +m∇f(u(t−1))> ( x(t) − θu(t−1) − (1− θ)x(t−1) ) + m\n2γ ‖x(t) − u(t−1)‖2 + θm 2λ ‖ᾱ(t)‖2 . (7)\nStep 2: Next, we bound −n[D(α(t))−D(α(t−1))]. Using the definition of the dual update we have\n− n[D(α(t))−D(α(t−1))] = ∑ i∈I [ φ∗i (−α (t) i )− φ ∗ i (−α (t−1) i ) ] + n 2λ [ ‖ᾱ(t)‖2 − ‖ᾱ(t−1)‖2 ] . (8)\nFor all i ∈ I , we may use the definition of the update of α(t)i in the algorithm, the strong-convexity of φ∗i , and the equality in Fenchel-Young for gradients to obtain:\nφ∗i (−α (t) i ) ≤ (1− θ)φ ∗ i (−α (t−1) i ) + θφ ∗ i (∇φi(u(t−1)))− θ(1− θ)γ 2 ‖α(t−1)i +∇φi(u (t−1))‖2\n= (1− θ)φ∗i (−α (t−1) i ) + θ[∇φi(u (t−1))>u(t−1) − φi(u(t−1))]− θ(1− θ)γ\n2 ‖α(t−1)i +∇φi(u (t−1))‖2 .\nCombining this with (8) we get\n− n[D(α(t))−D(α(t−1))] ≤ (9)∑ i∈I [ −θφ∗i (−α (t−1) i ) + θ[∇φi(u (t−1))>u(t−1) − φi(u(t−1))]− θ(1− θ)γ 2 ‖α(t−1)i +∇φi(u (t−1))‖2 ] + n\n2λ\n[ ‖ᾱ(t)‖2 − ‖ᾱ(t−1)‖2 ] .\nStep 3: Summing (9), (7), and the equation\n−mθD(α(t−1)) = mθ n ∑ i φ∗i (−α (t−1) i ) + mθ 2λ ‖ᾱ(t−1)‖2\nwe obtain that\nm[P (x(t))− (1− θ)P (x(t−1))]− n[D(α(t))−D(α(t−1))]− θmD(α(t−1)) ≤ mθf(u(t−1)) +m∇f(u(t−1))> ( x(t) − θu(t−1) − (1− θ)x(t−1) ) + m\n2γ ‖x(t) − u(t−1)‖2 + θm 2λ ‖ᾱ(t)‖2\n+ ∑ i∈I [ −θφ∗i (−α (t−1) i ) + θ[∇φi(u (t−1))>u(t−1) − φi(u(t−1))]− θ(1− θ)γ 2 ‖α(t−1)i +∇φi(u (t−1))‖2 ] + n\n2λ\n[ ‖ᾱ(t)‖2 − ‖ᾱ(t−1)‖2 ] + mθ\nn ∑ i φ∗i (−α (t−1) i ) + mθ 2λ ‖ᾱ(t−1)‖2\n= a (t) I + b (t) I + c (t) I .\nLemma 4. At each round t, let a(t)I , b (t) I , c (t) I be as defined in Lemma 3. Then,\nEt[a (t) I + b (t) I + c (t) I ] ≤ 0 .\nProof. Recall,\na (t) I =\nm 2γ ‖x(t) − u(t−1)‖2 − θ(1− θ)γ 2 ∑ i∈I ‖α(t−1)i +∇φi(u (t−1))‖2 ,\nb (t) I = θmf(u\n(t−1)) + mθ\nn n∑ i=1 φ∗i (−α (t−1) i ) +m∇f(u (t−1))>(−θu(t−1))\n+ ∑ i∈I [ −θφ∗i (−α (t−1) i ) + θ[∇φi(u (t−1))>u(t−1) − φi(u(t−1))] ] ,\nc (t) I = m∇f(u (t−1))>(x(t) − (1− θ)x(t−1)) + n+mθ 2λ ‖ᾱ(t)‖2 − n−mθ 2λ ‖ᾱ(t−1)‖2 .\nStep 1: We first show that Et[b (t) I ] = 0. Indeed,\nEt [∑ i∈I [ −θφ∗i (−α (t−1) i ) + θ[∇φi(u (t−1))>u(t−1) − φi(u(t−1))] ]]\n= Et [ n∑ i=1 1[i ∈ I] [ −θφ∗i (−α (t−1) i ) + θ[∇φi(u (t−1))>u(t−1) − φi(u(t−1))] ]]\n= n∑ i=1 [ −θφ∗i (−α (t−1) i ) + θ[∇φi(u (t−1))>u(t−1) − φi(u(t−1))] ] Et [1[i ∈ I]]\n= m\nn n∑ i=1 [ −θφ∗i (−α (t−1) i ) + θ[∇φi(u (t−1))>u(t−1) − φi(u(t−1))] ]\n= − [ θmf(u(t−1)) + mθ\nn n∑ i=1 φ∗i (−α (t−1) i ) +m∇f(u (t−1))>(−θu(t−1))\n] .\nStep 2: We next show that\nEt[c (t) I ] =\nn+mθ 2λ Et [ ‖∆ᾱ(t) − Et∆ᾱ(t)‖2 ] − n−mθ 2λ ∥∥∥Et∆ᾱ(t)∥∥∥2 . Indeed, using Lemma 1 we know that\n∇f(u(t−1)) = −ᾱ(t−1) − n θm Et[∆ᾱ(t)] ,\nand by the definition of the update,\nx(t) − (1− θ)x(t−1) = θ λ ᾱ(t) .\nTherefore,\nEtc (t) I = m∇f(u (t−1))>Et[x(t) − (1− θ)x(t−1)] + n+mθ\n2λ Et‖ᾱ(t)‖2 − n−mθ 2λ ‖ᾱ(t−1)‖2\n= −m [ ᾱ(t−1) + n\nθm Et[∆ᾱ(t)] ]> [ θ λ Etᾱ(t) ] + n+mθ 2λ Et‖ᾱ(t)‖2 − n−mθ 2λ ‖ᾱ(t−1)‖2\n= −m [ ᾱ(t−1) + n\nθm Et[∆ᾱ(t)] ]> [ θ λ [Et∆ᾱ(t) + ᾱ(t−1)] ]\n+ n+mθ\n2λ Et‖∆ᾱ(t) + ᾱ(t−1)‖2 − n−mθ 2λ ‖ᾱ(t−1)‖2\n= −n λ ‖Et∆ᾱ(t)‖2 + n+mθ 2λ Et‖∆ᾱ(t)‖2 = n+mθ\n2λ\n[ Et‖∆ᾱ(t)‖2 − ‖Et∆ᾱ(t)‖2 ] − n−mθ\n2λ ‖Et∆ᾱ(t)‖2\n= n+mθ 2λ Et [ ‖∆ᾱ(t) − Et∆ᾱ(t)‖2 ] − n−mθ 2λ ‖Et∆ᾱ(t)‖2 .\nStep 3: Next, we upper bound Et[a (t) I ].\nUsing the definitions of x(t) and u(t−1) and the smoothness of g∗ we have\n‖x(t) − u(t−1)‖ = θ‖∇g∗(ᾱ(t))−∇g∗(ᾱ(t−1))‖ = θ λ ‖ᾱ(t) − ᾱ(t−1)‖ .\nTherefore,\na (t) I =\nmθ2 2γλ2 ‖∆ᾱ(t)‖2 − θ(1− θ)γ 2 ∑ i∈I ‖α(t−1)i +∇φi(u (t−1))‖2 .\nTaking expectation we obtain\nEta (t) I =\nmθ2 2γλ2 Et‖∆ᾱ(t)‖2 − θ(1− θ)γ 2 n∑ i=1 ‖α(t−1)i +∇φi(u (t−1))‖2 Et[1[i ∈ I]]\n= mθ2\n2γλ2 Et‖∆ᾱ(t)‖2 − θ(1− θ)γm 2n n∑ i=1 ‖α(t−1)i +∇φi(u (t−1))‖2\n≤ mθ 2\n2γλ2 Et‖∆ᾱ(t)‖2 − θ(1− θ)γm 2n · n 3 mθ2 Et‖∆ᾱ(t) − Et∆ᾱ(t)‖2 ,\nwhere the last inequality follows from Lemma 2. Using the equality\nEt‖∆ᾱ(t)‖2 = Et‖∆ᾱ(t) − Et∆ᾱ(t)‖2 + ‖Et∆ᾱ(t)‖2\nwe obtain that\nEta (t) I ≤\nmθ2 2γλ2 ‖Et∆ᾱ(t)‖2 −\n( (1− θ)γn2\n2θ − mθ\n2\n2γλ2\n) Et‖∆ᾱ(t) − Et∆ᾱ(t)‖2 .\nStep 4: To conclude the proof, we combine the bounds derived in the three steps above and get that\nEt[a (t) I + b (t) I + c (t) I ] ≤(\nmθ2 2γλ2 − n−mθ 2λ\n) ‖Et∆ᾱ(t)‖2 + ( mθ2\n2γλ2 + n+mθ 2λ − (1− θ)γn 2 2θ\n) Et‖∆ᾱ(t) − Et∆ᾱ(t)‖2 .\nA sufficient condition for the above to be non-positive is that\n1. mθ2 +mθγλ− γλn ≤ 0 2. mθ3 + γλθ(n+mθ)− (1− θ)(γλn)2 ≤ 0\nLet us require the stronger conditions 1.1 mθ2 − γλn/2 ≤ 0⇒ θ ≤ √ 0.5 γλn/(m)\n1.2 mθγλ− γλn/2 ≤ 0⇒ θ ≤ 0.5 (n/m) 2.1 mθ3 − (γλn)2/4 ≤ 0⇒ θ ≤ ( (γλn)2\n4m )1/3 2.2 γλθn− (γλn)2/4 ≤ 0⇒ θ ≤ γλn/4\n2.3 γλθ2m− (γλn)2/4 ≤ 0⇒ θ ≤ γλn/2√ γλm = 0.5\n√ γλn · n\nm\n2.4 θ(γλn)2 − (γλn)2/4 ≤ 0⇒ θ ≤ 1/4\nAn even strong condition is\nθ ≤ 1 4 min\n{ 1 , √ γλn\nm , γλn ,\n(γλn)2/3\nm1/3\n} . (10)\nProof of Theorem 1. Combining Lemma 3 and Lemma 4 yields\nEt [ m∆P (x(t)) + n∆D(α(t)) ] ≤ (\n1− θm n\n) Et [ m∆P (x(t−1)) + n∆D(α(t−1)) ] .\nTaking expectation with respect to the randomness in previous rounds, using the law of total probability, and applying the above inequality recursively, we conclude our proof."
    }, {
      "heading" : "6 Discussion and Related Work",
      "text" : "We have introduced an accelerated version of stochastic dual coordinate ascent with mini-batches. We have shown, both theoretically and empirically, that the resulting algorithm interpolates between the vanilla stochastic coordinate descent algorithm and the accelerated gradient descent algorithm.\nUsing mini-batches in stochastic learning has received a lot of attention in recent years. E.g. ShalevShwartz et al. [2007] reported experiments showing that applying small mini-batches in Stochastic Gradient Descent (SGD) decreases the required number of iterations. Dekel et al. [2012] and Agarwal and Duchi [2012] gave an analysis of SGD with mini-batches for smooth loss functions. Cotter et al. [2011] studied SGD and accelerated versions of SGD with mini-batches and Takác et al. [2013] studied SDCA with minibatches for SVMs. Duchi et al. [2010] studied dual averaging in distributed networks as a function of spectral properties of the underlying graph. However, all of these methods have a polynomial dependence on 1/ , while we consider the strongly convex and smooth case in which a log(1/ ) rate is achievable.2\nIt is interesting to note that most3 of these papers focus on mini-batches as the method of choice for distributing SGD or SDCA, while ignoring the option to divide the data by features instead of by examples. A possible reason is the cost of opening communication sockets as discussed in Section 3.\nThere are various practical considerations that one should take into account when designing a practical system for distributed optimization. We refer the reader, for example, to Dekel [2010], Low et al. [2010, 2012], Agarwal et al. [2011], Niu et al. [2011].\nThe more general problem of distributed PAC learning has been studied recently in Daume III et al. [2012], Balcan et al. [2012]. See also Long and Servedio [2011]. In particular, they obtain algorithm with O(log(1/ )) communication complexity. However, these works consider efficient algorithms only in the realizable case."
    } ],
    "references" : [ {
      "title" : "A reliable effective terascale linear learning system",
      "author" : [ "Alekh Agarwal", "Olivier Chapelle", "Miroslav Dudík", "John Langford" ],
      "venue" : "arXiv preprint arXiv:1110.4198,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2011
    }, {
      "title" : "Distributed learning, communication complexity and privacy",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Shai Fine", "Yishay Mansour" ],
      "venue" : "arXiv preprint arXiv:1204.3514,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2012
    }, {
      "title" : "Parallel coordinate descent for l1regularized loss minimization",
      "author" : [ "Joseph K Bradley", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Bradley et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bradley et al\\.",
      "year" : 2011
    }, {
      "title" : "Better mini-batch algorithms via accelerated gradient methods",
      "author" : [ "Andrew Cotter", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan" ],
      "venue" : "arXiv preprint arXiv:1106.4574,",
      "citeRegEx" : "Cotter et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cotter et al\\.",
      "year" : 2011
    }, {
      "title" : "Protocols for learning classifiers on distributed data",
      "author" : [ "Hal Daume III", "Jeff M Phillips", "Avishek Saha", "Suresh Venkatasubramanian" ],
      "venue" : "arXiv preprint arXiv:1202.6078,",
      "citeRegEx" : "III et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "III et al\\.",
      "year" : 2012
    }, {
      "title" : "Distribution-calibrated hierarchical classification",
      "author" : [ "Ofer Dekel" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Dekel.,? \\Q2010\\E",
      "shortCiteRegEx" : "Dekel.",
      "year" : 2010
    }, {
      "title" : "Optimal distributed online prediction using mini-batches",
      "author" : [ "Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Dekel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dekel et al\\.",
      "year" : 2012
    }, {
      "title" : "Distributed dual averaging in networks",
      "author" : [ "John Duchi", "Alekh Agarwal", "Martin J Wainwright" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2010
    }, {
      "title" : "A Stochastic Gradient Method with an Exponential Convergence Rate for Strongly-Convex Optimization with Finite Training Sets",
      "author" : [ "Nicolas Le Roux", "Mark Schmidt", "Francis Bach" ],
      "venue" : "arXiv preprint arXiv:1202.6258,",
      "citeRegEx" : "Roux et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Roux et al\\.",
      "year" : 2012
    }, {
      "title" : "Algorithms and hardness results for parallel large margin learning",
      "author" : [ "Phil Long", "Rocco Servedio" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Long and Servedio.,? \\Q2011\\E",
      "shortCiteRegEx" : "Long and Servedio.",
      "year" : 2011
    }, {
      "title" : "Graphlab: A new framework for parallel machine learning",
      "author" : [ "Yucheng Low", "Joseph Gonzalez", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin", "Joseph M Hellerstein" ],
      "venue" : "arXiv preprint arXiv:1006.4990,",
      "citeRegEx" : "Low et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Low et al\\.",
      "year" : 2010
    }, {
      "title" : "Distributed graphlab: A framework for machine learning and data mining in the cloud",
      "author" : [ "Yucheng Low", "Danny Bickson", "Joseph Gonzalez", "Carlos Guestrin", "Aapo Kyrola", "Joseph M Hellerstein" ],
      "venue" : "Proceedings of the VLDB Endowment,",
      "citeRegEx" : "Low et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Low et al\\.",
      "year" : 2012
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Nesterov.,? \\Q2005\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2005
    }, {
      "title" : "Gradient methods for minimizing composite objective function",
      "author" : [ "Yurii Nesterov" ],
      "venue" : null,
      "citeRegEx" : "Nesterov.,? \\Q2007\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2007
    }, {
      "title" : "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "Feng Niu", "Benjamin Recht", "Christopher Ré", "Stephen J Wright" ],
      "venue" : "arXiv preprint arXiv:1106.5730,",
      "citeRegEx" : "Niu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2011
    }, {
      "title" : "Parallel coordinate descent methods for big data optimization",
      "author" : [ "Peter Richtárik", "Martin Takáč" ],
      "venue" : "arXiv preprint arXiv:1212.0873,",
      "citeRegEx" : "Richtárik and Takáč.,? \\Q2012\\E",
      "shortCiteRegEx" : "Richtárik and Takáč.",
      "year" : 2012
    }, {
      "title" : "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro" ],
      "venue" : "In INTERNATIONAL CONFERENCE ON MACHINE LEARNING,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2007
    }, {
      "title" : "Mini-batch primal and dual methods",
      "author" : [ "Martin Takác", "Avleen Bijral", "Peter Richtárik", "Nathan Srebro" ],
      "venue" : "mization. Journal of Machine Learning Research,",
      "citeRegEx" : "Takác et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Takác et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "We discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of Nesterov [2007].",
      "startOffset" : 219,
      "endOffset" : 235
    }, {
      "referenceID" : 12,
      "context" : "In particular, Nesterov [2007] proposed an accelerated gradient descent (AGD) method for solving (1).",
      "startOffset" : 15,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : ", Dekel et al. [2012]).",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : ", Dekel et al. [2012]). This is also the application scenario we have in mind, and will be discussed in greater details in Section 3. Recently, Takác et al. [2013] studied mini-batch variants of SDCA in the context of the Support Vector Machine (SVM) problem.",
      "startOffset" : 2,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : ", Dekel et al. [2012]). This is also the application scenario we have in mind, and will be discussed in greater details in Section 3. Recently, Takác et al. [2013] studied mini-batch variants of SDCA in the context of the Support Vector Machine (SVM) problem. They have shown that the naive mini-batching method, in whichm dual variables are optimized in parallel, might actually increase the number of iterations required. They then describe several “safe” mini-batching schemes, and based on the analysis of Shalev-Shwartz and Zhang [2013], have shown several speed-up results.",
      "startOffset" : 2,
      "endOffset" : 542
    }, {
      "referenceID" : 5,
      "context" : ", Dekel et al. [2012]). This is also the application scenario we have in mind, and will be discussed in greater details in Section 3. Recently, Takác et al. [2013] studied mini-batch variants of SDCA in the context of the Support Vector Machine (SVM) problem. They have shown that the naive mini-batching method, in whichm dual variables are optimized in parallel, might actually increase the number of iterations required. They then describe several “safe” mini-batching schemes, and based on the analysis of Shalev-Shwartz and Zhang [2013], have shown several speed-up results. However, their results are for the non-smooth case and hence they do not obtain linear convergence rate. In addition, the speed-up they obtain requires some spectral properties of the training examples. We take a different approach and employ Nesterov’s acceleration method, which has previously been applied to mini-batch SGD optimization. This paper shows how to achieve acceleration for SDCA in the mini-batch setting. The pseudo code of our Accelerated Mini-Batch SDCA, abbreviated by ASDCA, is presented below. An exception is the recent analysis given in Le Roux et al. [2012] for a variant of SGD.",
      "startOffset" : 2,
      "endOffset" : 1163
    }, {
      "referenceID" : 12,
      "context" : "Table 1 summarizes several interesting cases, and compares the iteration bound of ASDCA to the iteration bound of the vanilla SDCA algorithm (as analyzed in Shalev-Shwartz and Zhang [2013]) and the Accelerated Gradient Descent (AGD) algorithm of Nesterov [2007]. In the table, we ignore constants and logarithmic factors.",
      "startOffset" : 246,
      "endOffset" : 262
    }, {
      "referenceID" : 0,
      "context" : "In the calculations below, we use the following facts: • If each node holds a d-dimensional vector, we can compute the sum of these vectors in timeO(d log(s)) by applying a “tree-structure” summation (see for example the All-Reduce architecture in Agarwal et al. [2011]).",
      "startOffset" : 248,
      "endOffset" : 270
    }, {
      "referenceID" : 1,
      "context" : "Dekel et al. [2012] and Agarwal and Duchi [2012] gave an analysis of SGD with mini-batches for smooth loss functions.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "Dekel et al. [2012] and Agarwal and Duchi [2012] gave an analysis of SGD with mini-batches for smooth loss functions.",
      "startOffset" : 0,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "Cotter et al. [2011] studied SGD and accelerated versions of SGD with mini-batches and Takác et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "Cotter et al. [2011] studied SGD and accelerated versions of SGD with mini-batches and Takác et al. [2013] studied SDCA with minibatches for SVMs.",
      "startOffset" : 0,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "Cotter et al. [2011] studied SGD and accelerated versions of SGD with mini-batches and Takác et al. [2013] studied SDCA with minibatches for SVMs. Duchi et al. [2010] studied dual averaging in distributed networks as a function of spectral properties of the underlying graph.",
      "startOffset" : 0,
      "endOffset" : 167
    }, {
      "referenceID" : 1,
      "context" : "Cotter et al. [2011] studied SGD and accelerated versions of SGD with mini-batches and Takác et al. [2013] studied SDCA with minibatches for SVMs. Duchi et al. [2010] studied dual averaging in distributed networks as a function of spectral properties of the underlying graph. However, all of these methods have a polynomial dependence on 1/ , while we consider the strongly convex and smooth case in which a log(1/ ) rate is achievable.2 It is interesting to note that most3 of these papers focus on mini-batches as the method of choice for distributing SGD or SDCA, while ignoring the option to divide the data by features instead of by examples. A possible reason is the cost of opening communication sockets as discussed in Section 3. There are various practical considerations that one should take into account when designing a practical system for distributed optimization. We refer the reader, for example, to Dekel [2010], Low et al.",
      "startOffset" : 0,
      "endOffset" : 929
    }, {
      "referenceID" : 0,
      "context" : "[2010, 2012], Agarwal et al. [2011], Niu et al.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "[2010, 2012], Agarwal et al. [2011], Niu et al. [2011]. The more general problem of distributed PAC learning has been studied recently in Daume III et al.",
      "startOffset" : 14,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "[2010, 2012], Agarwal et al. [2011], Niu et al. [2011]. The more general problem of distributed PAC learning has been studied recently in Daume III et al. [2012], Balcan et al.",
      "startOffset" : 14,
      "endOffset" : 162
    }, {
      "referenceID" : 0,
      "context" : "[2010, 2012], Agarwal et al. [2011], Niu et al. [2011]. The more general problem of distributed PAC learning has been studied recently in Daume III et al. [2012], Balcan et al. [2012]. See also Long and Servedio [2011].",
      "startOffset" : 14,
      "endOffset" : 184
    }, {
      "referenceID" : 0,
      "context" : "[2010, 2012], Agarwal et al. [2011], Niu et al. [2011]. The more general problem of distributed PAC learning has been studied recently in Daume III et al. [2012], Balcan et al. [2012]. See also Long and Servedio [2011]. In particular, they obtain algorithm with O(log(1/ )) communication complexity.",
      "startOffset" : 14,
      "endOffset" : 219
    } ],
    "year" : 2013,
    "abstractText" : "Stochastic dual coordinate ascent (SDCA) is an effective technique for solving regularized loss minimization problems in machine learning. This paper considers an extension of SDCA under the minibatch setting that is often used in practice. Our main contribution is to introduce an accelerated minibatch version of SDCA and prove a fast convergence rate for this method. We discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of Nesterov [2007].",
    "creator" : "LaTeX with hyperref package"
  }
}