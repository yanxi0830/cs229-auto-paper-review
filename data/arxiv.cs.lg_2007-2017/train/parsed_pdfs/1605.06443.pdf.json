{
  "name" : "1605.06443.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Structured Prediction Theory Based on Factor Graph Complexity",
    "authors" : [ "Corinna Cortes", "Vitaly Kuznetsov" ],
    "emails" : [ "corinna@google.com", "vitaly@cims.nyu.edu", "mohri@cims.nyu.edu", "yangs@cims.nyu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Structured prediction encompasses a broad family of important learning problems. These include key tasks in natural language processing such as part-of-speech processing, parsing, machine translation, and named-entity recognition, along with important areas in computer vision such as image processing and object recognition, and also crucial areas in speech processing such as pronunciation modeling and speech recognition.\nIn all these problems, the output space admits some structure. This may be a sequence of tags as in part-of-speech tagging, a parse tree as in context-free parsing, an acyclic graph as in dependency parsing, or labels of image segments as in object detection. Another unifying theme is that the natural loss function for each of these tasks admits a decomposition along the output substructures. For instance, the loss function can be the Hamming loss as in part-of-speech tagging, or it may be the edit-distance which is widely used in natural language and speech processing.\nThe output structure and the corresponding loss function make these problems significantly different from the (unstructured) binary classification problems extensively studied in learning theory. In recent years, a number of different algorithms have been designed for structured prediction, including conditional random field (CRF) [Lafferty et al., 2001], StructSVM [Tsochantaridis et al., 2005], Maximum-Margin Markov Network [Taskar et al., 2003], and search-based approaches such as [Daumé III et al., 2009, Doppa et al., 2014, Lam et al., 2015, Chang et al., 2015, Ross et al., 2011]. More recently, deep learning techniques have also been developed for a variety of tasks such as part-of-speech tagging [Jurafsky and Martin, 2009, Vinyals et al., 2015a], named-entity recognition\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 5.\n06 44\n3v 1\n[ st\nat .M\nL ]\n[Nadeau and Sekine, 2007], machine translation [Zhang et al., 2008], image segmentation [Lucchi et al., 2013], and image annotation [Vinyals et al., 2015b].\nHowever, in contrast to the plethora of algorithms, there have been relatively few studies devoted to the theoretical understanding of structured prediction problems [Bakir et al., 2007]. Existing structured prediction learning guarantees hold only for specific decomposable losses such as the Hamming loss [Taskar et al., 2003, Cortes et al., 2014, London et al., 2013, Collins, 2001] and do not cover many other natural loss functions, such as the edit-distance. They also typically apply only to specific factor graph models. The sole exception is the work of McAllester [2007], which provides PAC-Bayesian guarantees for arbitrary losses, although it is only in the special case of randomized algorithms using linear (count-based) hypotheses.\nThis paper presents a general theoretical analysis of structured prediction with a series of new results. We give new data-dependent learning guarantees for structured prediction for a broad family of loss functions and hypothesis sets with an arbitrary factor graph decomposition. Our guarantees are based on a new complexity measure, which we show can be estimated in terms of familiar quantities for several commonly used hypothesis sets. Our bounds also match or improve upon the best-known bounds in several special cases.\nWe further introduce a new framework for structured prediction based on the principle of Voted Risk Minimization (VRM), which discriminates between the complexity of different subfamilies within the full set of features. We present new learning bounds for this scenario and design two new families of algorithms, Voted Conditional Random Fields (VCRF) and Voted Structured Boosting (StructBoost) that directly benefit from these guarantees. These algorithms can make use of features selected out of a union of very complex sub-families, while still preserving the ability to generalize well. As proof of concept validating our theory, we report experimental results for the VCRF algorithm on multiple datasets.\nThe paper is organized as follows. In Section 2 we introduce the notation and definitions relevant to our discussion of structure prediction. In Section 3, we derive a series of new learning guarantees for structured prediction, which are then used to prove the VRM principle in Section 4. Section 5 develops the algorithmic framework which is directly based on our theory. In Section 6, we provide some preliminary experimental results that serve as a proof of concept for our theory."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Let X denote the input space and Y the output space. In structured prediction, the output space may be a set of sequences, images, graphs, parse trees, lists, or some other (typically discrete) objects admitting some structure. By this, we mean that the output structure can be decomposed into l substructures. For example, this may be l positions along a sequence, so that the output space Y is decomposable along these substructures: Y = Y1 × · · · × Yl. Here, Yk is the set of possible labels (or classes) that can assigned to substructure k.\nLoss functions. We denote by L : Y × Y → R+ a loss function measuring the dissimilarity of two elements of the output space Y . We will assume that the loss function L is definite, that is L(y, y′) = 0 iff y = y′. This assumption holds for all loss functions commonly used in structured prediction. A key aspect of structured prediction is that the loss function can be decomposed along the substructures Yk. As an example, L may be the Hamming loss defined by L(y, y′) = 1l ∑l k=1 1yk 6=y′k for all y = (y1, . . . , yl) and y′ = (y′1, . . . , y ′ l), with yk, y ′ k ∈ Yk. In the common case where Y is a set of sequences defined over a finite alphabet, L may be the edit-distance, which is widely used in natural language and speech processing applications, with possibly different costs associated to insertions, deletions and substitutions. L may also be a loss based on the negative inner product of the vectors of n-gram counts of two sequences, or its negative logarithm. Such losses have been used to approximate the BLEU score loss in machine translation. There are other losses defined in computational biology based on various string-similarity measures. Our theoretical analysis is general and applies to arbitrary bounded and definite loss functions.\nScoring functions and factor graphs. We will adopt the common approach in structured prediction where predictions are based on a scoring function mapping X × Y to R. Let H be a family of scoring functions. For any h ∈ H, we denote by h the predictor defined by h: for any x ∈ X , h(x) = argmaxy∈Y h(x, y).\nFurthermore, we will assume, as is standard in structured prediction, that each function h ∈ H can be decomposed as a sum. We will consider the most general case for such decompositions, which can be made explicit using the notion of factor graphs.1 A factor graph G is a tuple G = (V, F,E), where V are variable nodes, F factor nodes, and E a set of undirected edges between a variable node and a factor node. In our context, V can be identified with the set of substructure indices, that is V = {1, . . . , l}. For any factor node f , denote by N(f) ⊆ V the set of variable nodes connected to f via an edge and define Yf as the substructure set cross-product Yf = ∏ k∈N(f) Yk. Then, h admits the following decomposition as a sum of functions hf , each taking as argument an element of the input space x ∈ X and an element of Yf , yf ∈ Yf :\nh(x, y) = ∑\nf∈F hf (x, yf ). (1)\nFigure 1 illustrates this definition with two different decompositions.\nMore generally, we will consider the setting in which a factor graph may depend on a particular example xi: G(xi, yi) = Gi = ([li], Fi, Ei). A special case of this setting is for example when the size li (or length) of each example is allowed to vary and where the number of possible labels |Y| is even infinite.\nWe present other examples of such hypothesis sets and their decomposition in Section 3, where we discuss our learning guarantees. Note that such hypothesis setsH with an additive decomposition are those commonly used in most structured prediction algorithms [Tsochantaridis et al., 2005, Taskar et al., 2003, Lafferty et al., 2001]. This is largely motivated by the computational requirement for efficient training and inference. Our results, while very general, further provide a statistical learning motivation for such decompositions.\nLearning scenario. We consider the familiar supervised learning scenario where the training and test points are drawn i.i.d. according to some distribution D over X × Y . We will further adopt the standard definitions of margin, generalization error and empirical error. The margin ρh(x, y) of a hypothesis h for a labeled example (x, y) ∈ X × Y is defined by\nρh(x, y) = h(x, y)−max y′ 6=y h(x, y′). (2)\nLet S = ((x1, y1), . . . , (xm, ym)) be a training sample of size m drawn from Dm. We denote by R(h) the generalization error and by R̂S(h) the empirical error of h over S:\nR(h) = E (x,y)∼D [L(h(x), y)] and R̂S(h) = E (x,y)∼S [L(h(x), y)], (3)\nwhere the notation (x, y)∼S indicates that (x, y) is drawn according to the empirical distribution defined by S. The learning problem consists of using the sample S to select a hypothesis h ∈ H with small expected loss R(h).\nObserve that the definiteness of the loss function implies, for all x ∈ X , the following: L(h(x), y) = L(h(x), y) 1ρh(x,y)≤0. (4)\nWe will later use this identity in the derivation of surrogate loss functions. 1Factor graphs are typically used to indicate the factorization of a probabilistic model. We are not assuming probabilistic models, but they would be also captured by our general framework: h would then be − log of a probability."
    }, {
      "heading" : "3 General learning bounds for structured prediction",
      "text" : "In this section, we present new learning guarantees for structured prediction. Our analysis is general and applies to the broad family of definite and bounded loss functions introduced in the previous section. It is also general in the sense that it applies to general hypothesis sets and not just sub-families of linear functions. For linear hypotheses, we will give a more refined analysis that holds for arbitrary norm-p regularized hypothesis sets.\nOur learning guarantees are based on a new notion of Rademacher complexity, and they are somewhat finer than the previously best-known bounds of Taskar et al. [2003], which were given for the special case of the Hamming loss with norm-2 regularization using covering numbers. Our bounds are data-dependent, admit a favorable dependency in terms of sparsity and the effective factor graph, and hold for general loss functions and regularizations. For the same norm-2 regularization, they also admit a logarithmic term improvement. We will present below a more detailed comparison with previous work.\nThe theoretical analysis of structured prediction is more complex than for classification since, by definition, it depends on the properties of the loss function and the factor graph. These attributes capture the combinatorial properties of the problem, which must be exploited since the total number of labels is often exponential in the size of that graph. To tackle this problem, we first introduce a new complexity tool."
    }, {
      "heading" : "3.1 Complexity measure",
      "text" : "A key ingredient of our analysis is a new data-dependent notion of complexity that extends the classical Rademacher complexity. We define the empirical factor graph Rademacher complexity R̂GS (H) of a hypothesis setH for a sample S = (x1, . . . , xm) and factor graph G as follows:\nR̂GS (H) = 1\nm E [ sup h∈H m∑\ni=1\n∑\nf∈Fi\n∑\ny∈Yf\n√ |Fi| i,f,y hf (xi, y) ] ,\nwhere = ( i,f,y)i∈[m],f∈Fi,y∈Yf with i,f,ys independent Rademacher random variables uniformly distributed over {±1}. The factor graph Rademacher complexity ofH for a factor graph G is defined as the expectation: RGm(H) = ES∼Dm [ R̂GS (H) ] . As with the standard Rademacher complexity, the factor graph Rademacher complexity of a hypothesis set can be estimated from data in many cases. In some important cases, it also admits explicit upper bounds similar to those for the standard Rademacher complexity but with an additional dependence on the factor graph quantities. We will prove this for several families of regularized linear functions which are commonly used in structured prediction (Theorem 2)."
    }, {
      "heading" : "3.2 Generalization bounds",
      "text" : "In this section, we present new margin bounds for structured prediction based on the factor graph Rademacher complexity ofH. Our results hold both for the additive and the multiplicative empirical margin losses defined below\nR̂addρ (h) = E (x,y)∼S\n[ Φ∗ (\nmax y′ 6=y\nL(y′, y)− 1ρ [ h(x, y)− h(x, y′) ])] (5)\nR̂multρ (h) = E (x,y)∼S\n[ Φ∗ (\nmax y′ 6=y\nL(y′, y) ( 1− 1ρ [h(x, y)− h(x, y′)] ))] . (6)\nHere, Φ∗(r) = min(M,max(0, r)) for all r, with M = maxy,y′ L(y, y′). The following is our general data-dependent margin bound for structured prediction.\nTheorem 1. Fix ρ > 0. For any δ > 0, with probability at least 1− δ over the draw of a sample S of size m, the following holds for all h ∈ H,\nR(h) ≤ Raddρ (h) ≤ R̂addS,ρ(h) + 4 √ 2\nρ RGm(H) + √ log 1δ 2m ,\nR(h) ≤ Rmultρ (h) ≤ R̂multS,ρ (h) + 4 √ 2M\nρ RGm(H) + √ log 1δ 2m .\nThe proof of Theorem 1 is based on new contraction lemmas (Lemma 5 and Lemma 6) that can be of independent interest. The full proofs of Theorem 1 and Lemma 5 are given in Appendix A. As for standard Rademacher complexity, the empirical factor graph Rademacher complexity R̂GS (H) is sharply concentrated around its mean RGm(H). This can be used to derive similar bounds to those of Theorem 1 with RGm(H) replaced by R̂GS (H) and slight changes to the constant in the last terms of each inequality. All of these margin bounds can be extended to hold uniformly over ρ ∈ (0, 1] at the price of an additional term of the form √ (log log2 2 ρ )/m in the bound, using known techniques (see for example [Mohri et al., 2012]).\nTheorem 1 is the first data-dependent generalization guarantee for structured prediction with general loss functions, general hypothesis sets and arbitrary factor graphs for both multiplicative and additive margins. We will compare it to known special cases below.\nIndeed, the hypothesis set used by structured prediction convex algorithms such as StructSVM [Tsochantaridis et al., 2005], Max-Margin Markov Networks [Taskar et al., 2003] or Conditional Random Field (CRF) [Lafferty et al., 2001] is that of linear functions. More precisely, let Ψ be a feature mapping from (X × Y) to RN such that Ψ(x, y) = ∑f∈F Ψf (x, yf ), and let\nHp = {x 7→ w ·Ψ(x, y) : ‖w‖p ≤ Λp}.\nIn this case R̂Gm(Hp) can be efficiently estimated using random sampling and solving LP programs. Moreover, one can obtain explicit upper bounds on R̂Gm(Hp). To simplify our presentation, we will consider the case p = 1, 2, but our results can be extended to an arbitrary p ≥ 1 and, more generally, to arbitrary group norms.\nTheorem 2. For any sample S = (x1, . . . , xm), the following upper bounds hold for the empirical factor graph complexity ofH1 andH2:\nR̂GS (H1) ≤ Λ1r∞ m\n√ s log(2N), R̂GS (H2) ≤\nΛ2r2 m\n√∑m i=1 ∑ f∈Fi ∑ y∈Yf |Fi|,\nwhere r∞ = maxi,f,y ‖Ψf (xi, y)‖∞, r2 = maxi,f,y ‖Ψf (xi, y)‖2 and where s is a sparsity factor defined by s = maxj∈[1,N ] ∑m i=1 ∑ f∈Fi ∑ y∈Yf |Fi|1Ψj(xi,y)6=0.\nPlugging in these factor graph complexity upper bounds into Theorem 1 immediately yields explicit data-dependent structured prediction learning guarantees for linear hypotheses with general loss functions and arbitrary factor graphs (see Corollary 8). Observe that in the worst case the sparsity factor can be bounded as follows:\ns ≤ m∑\ni=1\n∑\nf∈Fi\n∑\ny∈Yf |Fi| ≤\nm∑\ni=1\n|Fi|2di ≤ mmax i |Fi|2di,\nwhere di = maxf∈Fi |Yf |. Thus, the factor graph Rademacher complexities of linear hypothesis in H1 scale as O( √ log(N) maxi |Fi|2di/m). This shows that the expected size of the factor graph is crucial for learning in this scenario and should be contrasted with other existing structured prediction guarantees that we discuss below, which assume a fixed upper bound on the size of the factor graph. Note that our result shows that learning is possible even with an infinite set Y . To the best of our knowledge, this is the first learning guarantee for learning with infinitely many classes.\nOur learning guarantee forH1 can additionally benefit from the sparsity of the feature mapping and observed data. In particular, in many applications Ψf,j is a binary indicator function that is non-zero for a single (x, y) ∈ X × Yf . For instance, in NLP, Ψf,j may indicate an occurrence of a certain\nn-gram in the input xi and output y. Observe that in this case, s = ∑m i=1 |Fi|2 ≤ mmaxi |Fi|2\nand the complexity term is only in O(maxi |Fi| √\nlog(N)/m), where N may depend linearly on di."
    }, {
      "heading" : "3.3 Special cases and comparisons",
      "text" : "Here, we discuss some special cases of interest for which we compare our learning guarantees with those given by previous work.\nMarkov networks. For the pairwise Markov networks with a fixed number of substructures l studied by Taskar et al. [2003], our equivalent factor admits l nodes, |Fi| = l, and the maximum size of Yf is di = k\n2 if each substructure of a pair can be assigned one of k classes. Thus, if we apply Corollary 8 with Hamming distance as our loss function and and divide the bound through by l, to normalize the loss to interval [0, 1] as in [Taskar et al., 2003], we obtain the following explicit form of our guarantee for an additive empirical margin loss, for all h ∈ H2:\nR(h) ≤ R̂addS,ρ(h) + 4Λ2r2 ρ\n√ 2k2\nm + √ log 1δ 2m .\nThis bound can be further improved by eliminating the dependency on k using an extension of our contraction Lemma 5 to ‖ · ‖∞,2 (see Lemma 6). The complexity term of Taskar et al. [2003] bounds depends on the following Õ( √ Λ22q\n2r22/m). Our bound has the same dependence on these key quantities, with no logarithmic term in our case. Note that, unlike the result of Taskar et al. [2003], our bound also holds for general loss functions and different p-norm regularizers. Moreover, our result for a multiplicative empirical margin loss is new, even in this special case.\nMulti-class classification. For standard (unstructured) multi-class classification, we have |Fi| = 1 and di = c, where c is the number of classes. In that case, for linear hypotheses, the complexity term of our bound varies as O(Λ2r2 √ c/ρ2m) for example for norm-2 regularization (Corollary 9), which improves upon the best known general margin bounds of Kuznetsov et al. [2014], who provide a guarantee that scales linearly with the number of classes instead. Moreover, in the special case where an individual wy is learned for each class y ∈ [c], we retrieve the recent favorable bounds given by Lei et al. [2015], albeit with a somewhat simpler formulation. In that case, for any (x, y), all components of the feature vector Ψ(x, y) are zero, except (perhaps) for the N components corresponding to class y, where N is the dimension of wy. In view of that, for example for a group-norm ‖ · ‖2,1- regularization, the complexity term of our bound varies as O(Λr √ (log c)/ρ2m), which matches the results of Lei et al. [2015] with a logarithmic dependency on c (ignoring some complex exponents of log c in their case). Additionally, note that unlike existing multi-class learning guarantees, our results hold for arbitrary loss functions. See Corollary 10 for further details. Our sparsity-based bounds can also be used to give bounds with logarithmic dependence on the number of classes when the features only take values in {0, 1}. Finally, using Lemma 6 instead of Lemma 5, the dependency on the number of classes can be further improved.\nWe conclude this section by observing that, since our guarantees are expressed in terms of the average size of the factor graph over a given sample, this invites us to search for a hypothesis set H and predictor h ∈ H such that the tradeoff between the empirical size of the factor graph and empirical error is optimal. In the next section, we will make use of a recently developed principle of Voted Risk Minimization (VRM) [Cortes et al., 2015] to reach this objective."
    }, {
      "heading" : "4 Voted Risk Minimization",
      "text" : "In many structured prediction applications such as natural language processing and computer vision, one may wish to exploit very rich features. However, the use of rich families of hypotheses could lead to overfitting. In this section, we show that it may be possible to use rich families in conjunction with simpler families, provided that fewer complex hypotheses are used (or that they are used with less mixture weight). We achieve this goal by deriving learning guarantees for ensembles of structure prediction rules that explicitly account for the differing complexities between families. This will motivate the algorithms that we present in Section 5.\nAssume that we are given p families H1, . . . ,Hp of functions mapping from X × Y to R. Define the ensemble family F = conv(∪pk=1Hk), that is the family of functions f of the form f = ∑T t=1 αtht, where α = (α1, . . . , αT ) is in the simplex ∆ and where, for each t ∈ [1, T ], ht is in Hkt for some kt ∈ [1, p]. We further assume that RGm(H1) ≤ RGm(H2) . . . ≤ RGm(Hp). As an example, Hks may be ordered by the size of the corresponding factor graphs.\nThe main result in this section is a generalization of the VRM theory to the structured prediction setting. The learning guarantees that we present are in terms of upper bounds on R̂addS,ρ(h) and R̂multS,ρ (h), which are defined as follows for all τ ≥ 0:\nR̂addS,ρ,τ (h) = E (x,y)∼S\n[ Φ∗ (\nmax y′ 6=y\nL(y′, y) + τ − 1ρ [ h(x, y)− h(x, y′) ])] (7)\nR̂multS,ρ,τ (h) = E (x,y)∼S\n[ Φ∗ (\nmax y′ 6=y\nL(y′, y) ( 1 + τ − 1ρ [h(x, y)− h(x, y′)] ))] . (8)\nFor simplicity, we assume in this section that |Y| = c < +∞.\nTheorem 3. Fix ρ > 0. For any δ > 0, with probability at least 1− δ over the draw of a sample S of size m, each of the following inequalities holds for all f ∈ F:\nR(f)− R̂addS,ρ,1(f) ≤ 4 √ 2\nρ\nT∑\nt=1\nαtR G m(Hkt) + C(ρ,M, c,m, p),\nR(f)− R̂multS,ρ,1(f) ≤ 4 √ 2M\nρ\nT∑\nt=1\nαtR G m(Hkt) + C(ρ,M, c,m, p).\nwhere C(ρ,M, c,m, p) = 2cρM √ log p m + 3M √⌈ 4 ρ2 log ( c2M2ρ2m 4 log p )⌉ log p m + log 2δ 2m .\nThe proof of this theorem crucially depends on the theory we developed in Section 3 and is given in Appendix A. The explicit dependence of this bound on the parameter vector α suggests that learning even with highly complex hypothesis sets could be possible so long as the complexity term, which is a weighted average of the factor graph complexities, is not too large. The theorem provides a quantitative way of determining the mixture weights that should be apportioned to each family. Furthermore, the dependency on the number of distinct feature map families Hk is very mild and therefore suggests that a large number of families can be used. These properties will be useful for motivating new algorithms for structured prediction."
    }, {
      "heading" : "5 Algorithms",
      "text" : "In this section, we derive several algorithms for structured prediction based on the VRM principle discussed in Section 4. We first give general convex upper bounds (Section 5.1) on the structured prediction loss which recover as special cases the loss functions used in StructSVM [Tsochantaridis et al., 2005], Max-Margin Markov Networks [Taskar et al., 2003], and Conditional Random Field (CRF) [Lafferty et al., 2001]. Next, we introduce a new algorithm, Voted Conditional Random Field (VCRF) Section 5.2, with accompanying experiments as proof of concept. We also present another algorithm, Voted StructBoost (VStructBoost), in Appendix C."
    }, {
      "heading" : "5.1 General framework for convex surrogate losses",
      "text" : "Given a labeled example (x, y) ∈ X × Y , the mapping h 7→ L(h(x), y) is typically not a convex function of h, which leads to computationally hard optimization problems. This motivates the use of convex surrogate losses. We first introduce a general formulation of surrogate losses for structured prediction problems. Lemma 4. For any u ∈ R+, let Φu : R→ R be a decreasing upper bound on v 7→ u1v≤0 such that u 7→ Φu(v) is increasing for a fixed v. Then, the following upper bound holds for any h ∈ H and (x, y) ∈ X × Y ,\nL(h(x), y) ≤ max y′ 6=y ΦL(y′,y)(h(x, y)− h(x, y′)). (9)\nThe proof is given in Appendix A. This result defines a general framework that enables us to straightforwardly recover many of the most common state-of-the-art structured prediction algorithms via suitable choices of Φu(v): (a) for Φu(v) = max(0, u(1−v)), the right-hand side of (9) coincides with the surrogate loss defining StructSVM [Tsochantaridis et al., 2005]; (b) for Φu(v) = max(0, u− v), it coincides with the surrogate loss defining Max-Margin Markov Networks [Taskar et al., 2003] when using for L the Hamming loss; and (c) for Φu(v) = log(1 + eu−v), it coincides with the surrogate loss defining Conditional Random Field (CRF) [Lafferty et al., 2001].\nMoreover, alternative choices of Φu(v) can help define new algorithms. In particular, we will refer to the algorithm based on the surrogate loss defined by Φu(v) = ue−v as StructBoost, in reference to the exponential loss used in AdaBoost. Another related alternative is based on the choice Φu(v) = eu−v . See Appendix C, for further details on this algorithm. In fact, for each of Φu(v) described above, the corresponding convex surrogate is an upper bound on either the multiplicative or additive margin loss introduced in Section 3. Therefore, each of these algorithms seeks a hypothesis that minimizes the generalization bounds presented in Section 3. To the best of our knowledge, this interpretation of these well-known structured prediction algorithms is also new. In what follows, we derive new structured prediction algorithms that minimize finer generalization bounds presented in Section 4."
    }, {
      "heading" : "5.2 Voted Conditional Random Field (VCRF)",
      "text" : "We first consider the convex surrogate loss based on Φu(v) = log(1 + eu−v), which corresponds to the loss defining CRF models. Using the monotonicity of the logarithm and upper bounding the maximum by a sum gives the following upper bound on the surrogate loss holds:\nmax y′ 6=y\nlog(1 + eL(y,y ′)−w·δΨ(x,y,y′)) = log\n( ∑\ny′∈Y eL(y,y\n′)−w·δΨ(x,y,y′) ) ,\nwhich, combined with the VRM principle, leads to the following optimization problem:\nmin w\n1\nm\nm∑\ni=1\nlog\n(∑\ny∈Y eL(y,yi)−w·δΨ(xi,yi,y)\n) + p∑\nk=1\n(λrk + β)‖wk‖1, (10)\nwhere rk = r∞|F (k)| √\nlogN . We refer to the learning algorithm based on the optimization problem (10) as VCRF. Note that for λ = 0, (10) coincides with the objective function of L1regularized CRF. Observe that we can also directly use maxy′ 6=y log(1 + eL(y,y\n′)−w·δΨ(x,y,y′)) or its upper bound ∑ y′ 6=y log(1 + e\nL(y,y′)−w·δΨ(x,y,y′)) as a convex surrogate. We can similarly derive an L2-regularization formulation of the VCRF algorithm.\nIn Appendix D, we describe efficient algorithms for solving the VCRF and VStructBoost optimization problems. Our algorithms can be further used with a large family of structured prediction losses that include the edit-distance and the n-gram loss function. To the best of our knowledge these results are also new."
    }, {
      "heading" : "6 Experiments",
      "text" : "In Appendix B, we corroborate our theory by reporting experimental results suggesting that the VCRF algorithm can outperform the CRF algorithm on a number of part-of-speech (POS) datasets."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We presented a general theoretical analysis of structured prediction. The data-dependent margin guarantees for structured prediction can be used to guide the design of algorithms or to determine the generalization guarantees of existing algorithms. Its explicit dependency on the properties of the factor graph used to define model decomposition and on feature sparsity can help shed new light on the role played by the graph and features in generalization. Our extension of the VRM theory to structured prediction provides a new analysis of generalization when using a very rich set of features, which is common in many applications such as natural language processing and leads to new algorithms, VCRF and VStructBoost. Our preliminary experimental results for VCRF serve as a proof of concept and motivate a more extensive empirical and algorithmic studies of these two families of algorithms."
    }, {
      "heading" : "A Proofs",
      "text" : "This appendix section gathers detailed proofs of all of our main results. In Appendix A.1, we prove a contraction lemma used as a tool in the proof of our general factor graph Rademacher complexity bounds (Appendix A.3). In Appendix A.7, we further extend our bounds to the Voted Risk Minimization setting. Appendix A.4 gives explicit upper bounds on the factor graph Rademacher complexity of several commonly used hypothesis sets. In Appendix A.8, we prove a general upper bound on a loss function used in structured prediction in terms of a convex surrogate.\nA.1 Contraction lemma\nThe following contraction lemma will be a key tool used in the proofs of our generalization bounds for structured prediction.\nLemma 5. LetH be a hypothesis set of functions mappingX to Rc. Assume that for all i = 1, . . . ,m, Ψi is µi-Lipschitz for Rc equipped with the norm-2 functions for some µi > 0. That is\n|Ψi(x′)−Ψi(x)| ≤ µi‖x′ − x‖2, for all (x,x′) ∈ (Rc)2. Then, for any sample S ofm points x1, . . . , xm ∈ X , the following inequality holds\n1 m E σ [ sup h∈H m∑\ni=1\nσiΨi(h(xi))\n] ≤ √ 2\nm E\n sup\nh∈H\nm∑\ni=1\nc∑\nj=1\nij µihj(xi)\n  , (11)\nwhere = ( ij)i,j and ijs are independent Rademacher variables uniformly distributed over {±1}.\nProof. Fix a sample S = (x1, . . . , xm). Then, we can rewrite the left-hand side of (12) as follows:\n1 m E σ [ sup h∈H m∑\ni=1\nσiΨi(h(xi))\n] = 1\nm E σ1,...,σm−1 [ E σm [ sup h∈H Um−1(h) + σmΨm(h(xm)) ]] ,\nwhere Um−1(h) = ∑m−1 i=1 σiΨi(h(xi)). Assume that the suprema can be attained and let h1,h2 ∈ H be the hypotheses satisfying Um−1(h1) + Ψm(h1(xm)) = sup\nh∈H Um−1(h) + Ψm(h(xm))\nUm−1(h2)−Ψm(h2(xm)) = sup h∈H Um−1(h)−Ψm(h(xm)).\nWhen the suprema are not reached, a similar argument to what follows can be given by considering instead hypotheses that are -close to the suprema for any > 0. By definition of expectation, since σm is uniformly distributed over {±1}, we can write\nE σm [ sup h∈H Um−1(h) + σmΨm(h(xm)) ]\n= 1\n2 sup h∈H\nUm−1(h) + Ψm(h(xm)) + 1\n2 sup h∈H\nUm−1(h)−Ψm(h(xm))\n= 1\n2 [Um−1(h1) + Ψm(h1(xm))] +\n1 2 [Um−1(h2)−Ψm(h2(xm))].\nNext, using the µm-Lipschitzness of Ψm and the Khintchine-Kahane inequality, we can write\nE σm [ sup h∈H Um−1(h) + σmΨm(h(xm)) ]\n≤ 1 2 [Um−1(h1) + Um−1(h2) + µm‖h1(xm)− h2(xm)‖2]\n≤ 1 2\n[ Um−1(h1) + Um−1(h2) + µm\n√ 2 E m1,..., mc\n[∣∣∣ c∑\nj=1\nmj ( h1j(xm)− h2j(xm) )∣∣∣ ]] .\nNow, let m denote ( m1, . . . , mc) and let s( m) ∈ {±1} denote the sign of ∑c j=1 mj ( h1j(xm)−\nh2j(xm) ) . Then, the following holds:\nE σm [ sup h∈H Um−1(h) + σm(Ψm ◦ h)(xm) ]\n≤ 1 2 E m\n[ Um−1(h1) + Um−1(h2) + µm √ 2 ∣∣∣ c∑\nj=1\nmj ( h1j(xm)− h2j(xm) )∣∣∣ ]\n= 1\n2 E m\n[ Um−1(h1) + µm √ 2 s( m) c∑\nj=1\nmjh1j(xm)\n+ Um−1(h2)− µm √ 2 s( m) c∑\nj=1\nmjh2j(xm)\n]\n≤ 1 2 E m [ sup h∈H ( Um−1(h) + µm √ 2 s( m) c∑\nj=1\nmjhj(xm) )\n+ sup h∈H\n( Um−1(h)− µm √ 2 s( m) c∑\nj=1\nmjhj(xm) )]\n= E m [ E σm [ sup h∈H Um−1(h) + µm √ 2σm c∑\nj=1\nmjhj(xm) ]]\n= E m [ sup h∈H Um−1(h) + µm √ 2 c∑\nj=1\nmjhj(xm) ]] ,\nProceeding in the same way for all other σis (i < m) completes the proof.\nA.2 Contraction lemma for ‖ · ‖∞,2-norm\nIn this section, we present an extension of the contraction Lemma 5, that can be used to remove the dependency on the alphabet size in all of our bounds. Lemma 6. Let H be a hypothesis set of functions mapping X × [d] to Rc. Assume that for all i = 1, . . . ,m, Ψi is µi-Lipschitz for Rc×d equipped with the norm-(∞, 2) for some µi > 0. That is |Ψi(x′)−Ψi(x)| ≤ µi‖x′ − x‖∞,2, for all (x,x′) ∈ (Rc×d)2. Then, for any sample S of m points x1, . . . , xm ∈ X , there exists a distribution U over [d]c×m such that the following inequality holds:\n1 m E σ [ sup h∈H m∑\ni=1\nσiΨi(h(xi))\n] ≤ √ 2\nm E υ∼U,\n sup\nh∈H\nm∑\ni=1\nc∑\nj=1\nij µihj(xi, υmj)\n  , (12)\nwhere = ( ij)i,j and ijs are independent Rademacher variables uniformly distributed over {±1} and υ = (υi,j)i,j is a sequence of random variables distributed according to U . Note that υi,js themselves do not need to be independent.\nProof. Fix a sample S = (x1, . . . , xm). Then, we can rewrite the left-hand side of (12) as follows:\n1 m E σ [ sup h∈H m∑\ni=1\nσiΨi(h(xi))\n] = 1\nm E σ1,...,σm−1 [ E σm [ sup h∈H Um−1(h) + σmΨm(h(xm)) ]] ,\nwhere Um−1(h) = ∑m−1 i=1 σiΨi(h(xi)). Assume that the suprema can be attained and let h1,h2 ∈ H be the hypotheses satisfying Um−1(h1) + Ψm(h1(xm)) = sup\nh∈H Um−1(h) + Ψm(h(xm))\nUm−1(h2)−Ψm(h2(xm)) = sup h∈H Um−1(h)−Ψm(h(xm)).\nWhen the suprema are not reached, a similar argument to what follows can be given by considering instead hypotheses that are -close to the suprema for any > 0. By definition of expectation, since σm is uniformly distributed over {±1}, we can write\nE σm [ sup h∈H Um−1(h) + σmΨm(h1(xm)) ]\n= 1\n2 sup h∈H\nUm−1(h) + Ψm(h1(xm)) + 1\n2 sup h∈H\nUm−1(h)−Ψm(h(xm))\n= 1\n2 [Um−1(h1) + Ψm(h1(xm))] +\n1 2 [Um−1(h2)−Ψm(h2(xm))].\nNext, using the µm-Lipschitzness of Ψm and the Khintchine-Kahane inequality, we can write\nE σm [ sup h∈H Um−1(h) + σm(Ψm ◦ h)(xm) ]\n≤ 1 2 [Um−1(h1) + Um−1(h2) + µm‖h1(xm)− h2(xm)‖∞,2]\n≤ 1 2\n[ Um−1(h1) + Um−1(h2) + µm\n√ 2 E m1,..., mc\n[∣∣∣ c∑\nj=1\nmj‖h1,j(xm, ·)− h2,j(xm, ·)‖∞ ∣∣∣ ]] .\nDefine the random variables υmj = υmj(σ) = argmaxk∈[d] |h1,j(xm, k)− h2,j(xm, k)|.\nNow, let m denote ( m1, . . . , mc) and let s( m) ∈ {±1} denote the sign of∑c j=1 mj‖h1,j(xm, ·)− h2,j(xm, ·)‖∞. Then, the following holds:\nE σm [ sup h∈H Um−1(h) + σm(Ψm ◦ h)(xm) ]\n≤ 1 2 E m\n[ Um−1(h1) + Um−1(h2) + µm √ 2 ∣∣∣ c∑\nj=1\nmj‖h1,j(xm, ·)− h2,j(xm, ·)‖∞ ∣∣∣ ]\n= 1\n2 E m\n[ Um−1(h1) + Um−1(h2)\n+ µm √ 2 s( m) c∑\nj=1\nmj |h1,j(xm, υmj)− h2,j(xm, υmj)| ]\n= 1\n2 E m\n[ Um−1(h1) + Um−1(h2)\n+ µm √ 2 s( m) c∑\nj=1\nmj(h1,j(xm, υmj)− h2,j(xm, υmj)) ]\n= 1\n2 E m\n[ Um−1(h1) + µm √ 2 s( m) c∑\nj=1\nmjh1,j(xm, υmj)\n+ Um−1(h2)− µm √ 2 s( m) c∑\nj=1\nmjh2,j(xm, υmj)\n] .\nAfter taking expectation over υ, the rest of the proof proceeds the same way as the argument in Lemma 5:\n1 2 E υ∼U, m\n[ Um−1(h1) + µm √ 2 s( m) c∑\nj=1\nmjh1,j(xm, υmj)\n+ Um−1(h2)− µm √ 2 s( m) c∑\nj=1\nmjh2,j(xm, υmj)\n]\n≤ 1 2 E υ∼U, m [ sup h∈H ( Um−1(h) + µm √ 2 s( m) c∑\nj=1\nmjhj(xm, υmj) )\n+ sup h∈H\n( Um−1(h)− µm √ 2 s( m) c∑\nj=1\nmjhj(xm, υmj) )]\n= E υ∼U, m [ E σm [ sup h∈H Um−1(h) + µm √ 2σm c∑\nj=1\nmjhj(xm, υmj) ]]\n= E υ∼U, m [ sup h∈H Um−1(h) + µm √ 2 c∑\nj=1\nmjhj(xm, υmj) ]] ,\nProceeding in the same way for all other σis (i < m) completes the proof.\nA.3 General structured prediction learning bounds\nIn this section, we give the proof of several general structured prediction bounds in terms of the notion of factor graph Rademacher complexity. We will use the additive and multiplicative margin losses of a hypothesis h, which are the population versions of the empirical quantities margin losses we introduced in (5) and (6) defined as follows:\nRaddρ (h) = E (x,y)∼D\n[ Φ∗ (\nmax y′ 6=y\nL(y′, y)− 1ρ [ h(x, y)− h(x, y′)\n])]\nRmultρ (h) = E (x,y)∼D\n[ Φ∗ (\nmax y′ 6=y\nL(y′, y) ( 1− 1ρ [h(x, y)− h(x, y′)] ))] .\nThe following is our general margin bound structured prediction.\nTheorem 1. Fix ρ > 0. For any δ > 0, with probability at least 1− δ over the draw of a sample S of size m, the following holds for all h ∈ H,\nR(h) ≤ Raddρ (h) ≤ R̂addS,ρ(h) + 4 √ 2\nρ RGm(H) + √ log 1δ 2m ,\nR(h) ≤ Rmultρ (h) ≤ R̂multS,ρ (h) + 4 √ 2M\nρ RGm(H) + √ log 1δ 2m .\nProof. Let Φu(v) = Φ∗(u − vρ ), where Φ∗(r) = min(M,max(0, r)). Observe that for any u ∈ [0,M ], u1v≤0 ≤ Φu(v) for all v. Furthermore, Φu(v) is an increasing function of u for any given v. Therefore, by Lemma 4 and monotonicity of Φ∗,\nR(h) ≤ E (x,y)∼D [max y′ 6=y ΦL(y′,y)(h(x, y)− h(x, y′))]\n= E (x,y)∼D\n[ Φ∗ (\nmax y′ 6=y\n( L(y′, y)− h(x, y)− h(x, y ′) ρ\n))]\n= Raddρ (h).\nDefine\nH0 = { (x, y) 7→ Φ∗ (\nmax y′ 6=y\n( L(y′, y)− h(x, y)− h(x, y ′) ρ )) : h ∈ H\n} ,\nH1 = {\n(x, y) 7→ max y′ 6=y\n( L(y′, y)− h(x, y)− h(x, y ′) ρ ) : h ∈ H\n} .\nBy standard Rademacher complexity bounds (Koltchinskii and Panchenko [2002]), for any δ > 0, with probability at least 1− δ, the following inequality holds for all h ∈ H:\nRaddρ (h) ≤ R̂addS,ρ(h) + 2Rm(H0) + √ log 1δ 2m ,\nwhere Rm(H0) is the Rademacher complexity of the familyH0:\nRm(H0) = 1\nm E S∼Dm E σ [ sup h∈H m∑\ni=1\nσiΦ ∗ (\nmax y′ 6=yi\n( L(y′, yi)− h(xi, yi)− h(xi, y′) ρ\n))]\nand where σ = (σ1, . . . , σm) with σis independent Rademacher random variables uniformly distributed over {±1}. Since Φ∗ is 1-Lipschitz, by Talagrand’s contraction lemma (Ledoux and Talagrand [1991], Mohri et al. [2012]), we have Rm(H0) ≤ Rm(H1). Now, observe that, by the sub-additivity of the supremum, the following inequality holds:\nRm(H1) ≤ 1\nm E S∼Dm E σ [ sup h∈H m∑\ni=1\nσi max y′ 6=yi\n( L(y′, yi) + h(xi, y ′)\nρ\n)]\n+ 1\nm E S∼Dm E σ [ sup h∈H m∑\ni=1\nσi h(xi, yi)\nρ\n] ,\nwhere we also used for the last term the fact that −σi and σi admit the same distribution. We use Lemma 5 to bound each of the two terms appearing on the right-hand side separately. To do so, we we first show the Lipschitzness of h 7→ maxy′ 6=yi ( L(y′, yi) + h(xi,y ′)\nρ\n) . Observe that the following\nchain of inequalities holds for any h, h̃ ∈ H:∣∣∣∣∣maxy 6=yi ( L(y, yi) + h(xi, y) ρ ) −max y 6=yi ( L(y, yi) + h̃(xi, y) ρ )∣∣∣∣∣\n≤ 1 ρ max y 6=yi\n∣∣∣h(xi, y)− h̃(xi, y) ∣∣∣\n≤ 1 ρ max y∈Y\n∣∣∣h(xi, y)− h̃(xi, y) ∣∣∣\n= 1\nρ max y∈Y\n∣∣∣ ∑\nf∈Fi (hf (xi, yf )− h̃f (xi, yf ))\n∣∣∣\n≤ 1 ρ\n∑ f∈Fi max y∈Y ∣∣∣(hf (xi, yf )− h̃f (xi, yf )) ∣∣∣\n= 1\nρ\n∑ f∈Fi max y∈Yf ∣∣∣(hf (xi, y)− h̃f (xi, y)) ∣∣∣\n≤ √ |Fi| ρ √∑\nf∈Fi\n[ max y∈Y |(hf (xi, y)− h̃f (xi, y))| ]2\n= √ |Fi| ρ √∑\nf∈Fi max y∈Y |(hf (xi, y)− h̃f (xi, y))|2\n≤ √ |Fi| ρ √∑\nf∈Fi\n∑ y∈Y |(hf (xi, y)− h̃f (xi, y))|2.\nWe can therefore apply Lemma 5, which yields\n1 m E σ [ sup h∈H m∑\ni=1\nσi max y′ 6=yi\n( L(y′, yi) + h(xi, y ′)\nρ\n)]\n≤ √ 2\nm E [ sup h∈H m∑\ni=1\n∑\nf∈Fi\n∑\ny∈Yf i,f,y\n√ |Fi| ρ hf (xi, y) ] = √ 2 ρ R̂GS (H).\nSimilarly, for the second term, observe that the following Lipschitz property holds:\n∣∣∣h(xi, yi) ρ − h̃(xi, yi) ρ ∣∣∣ ≤ 1 ρ max y 6=yi ∣∣∣h(xi, y)− h̃(xi, y) ∣∣∣\n≤ √ |Fi| ρ √∑\nf∈Fi\n∑ y∈Y |(hf (xi, y)− h̃f (xi, y))|2.\nWe can therefore apply Lemma 5 and obtain the following:\n1 m E σ [ sup h∈H m∑\ni=1\nσi h(xi, yi)\nρ\n)] ≤ √ 2\nm E [ sup h∈H m∑\ni=1\n∑\nf∈Fi\n∑\ny∈Yf i,f,y\n√ |Fi| ρ hf (xi, y) ] = √ 2 ρ R̂GS (H).\nTaking the expectation over S of the two inequalities shows that Rm(H1) ≤ 2 √ 2 ρ R̂ G S (H), which completes the proof of the first statement.\nThe second statement can be proven in a similar way with Φu(v) = Φ∗(u(1− vρ )). In particular, by standard Rademacher complexity bounds and Talagrand’s contraction lemma, we can write\nRmultρ (h) ≤ R̂multS,ρ (h) + 2Rm(H̃1) + √ log 1δ 2m ,\nwhere\nH̃1 = {\n(x, y) 7→ max y′ 6=y\nL(y′, y) ( 1− h(x, y)− h(x, y ′)\nρ\n) : h ∈ H } .\nWe observe that the following inequality holds: ∣∣∣∣∣maxy 6=yi L(y, yi) ( 1− h(xi, yi)− h(xi, y) ρ ) −max y 6=yi L(y, yi) ( 1− h̃(xi, yi)− h̃(xi, y) ρ )∣∣∣∣∣\n≤ 2M ρ max y∈Y\n∣∣∣h(xi, y)− h̃(xi, y) ∣∣∣.\nThen, the rest of the proof follows from Lemma 5 as in the previous argument.\nA.4 Bounds on factor graph Rademacher complexity\nThe following lemma is a standard bound on the expectation of the maximum of n zero-mean bounded random variables, which will be used in the proof of our bounds on factor graph Rademacher complexity.\nLemma 7. Let X1 . . . Xn be n ≥ 1 real-valued random variables such that for all j ∈ [1, n], Xj = ∑mj i=1 Yij where, for each fixed j ∈ [1, n], Yij are independent zero mean random variables with |Yij | ≤ tij . Then, the following inequality holds:\nE [\nmax j∈[1,n] Xj\n] ≤ t √ 2 log n,\nwith t = √ maxj∈[1,n] ∑mj i=1 t 2 ij .\nThe following are upper bounds on the factor graph Rademacher complexity forH1 andH2. Similar guarantees can be given for other hypothesis setsHp with p > 1.\nTheorem 2. For any sample S = (x1, . . . , xm), the following upper bounds hold for the empirical factor graph complexity ofH1 andH2:\nR̂GS (H1) ≤ Λ1r∞ m\n√ s log(2N), R̂GS (H2) ≤\nΛ2r2 m\n√∑m i=1 ∑ f∈Fi ∑ y∈Yf |Fi|,\nwhere r∞ = maxi,f,y ‖Ψf (xi, y)‖∞, r2 = maxi,f,y ‖Ψf (xi, y)‖2 and where s is a sparsity factor defined by s = maxj∈[1,N ] ∑m i=1 ∑ f∈Fi ∑ y∈Yf |Fi|1Ψj(xi,y)6=0.\nProof. By definition of the dual norm and Lemma 7 (or Massart’s lemma), the following holds:\nmR̂GS (H1) = E\n[ sup\n‖w‖1≤Λ1 w ·\nm∑\ni=1\n∑\nf∈Fi\n∑\ny∈Yf\n√ |Fi| i,f,yΨf (xi, y)\n]\n= Λ1 E [∥∥∥∥ m∑\ni=1\n∑\nf∈Fi\n∑\ny∈Yf\n√ |Fi| i,f,yΨf (xi, y) ∥∥∥∥ ∞ ]\n= Λ1 E   max j∈[1,N ],σ∈{−1,+1} σ m∑\ni=1\n∑\nf∈Fi\n∑\ny∈Yf\n√ |Fi| i,f,yΨf,j(xi, y)  \n= Λ1 E   max j∈[1,N ],σ∈{−1,+1} σ m∑\ni=1\n∑\nf∈Fi\n∑\ny∈Yf\n√ |Fi| i,f,yΨf,j(xi, y)1Ψf,j(xi,y) 6=0\n \n≤ Λ1 √√√√2 (\nmax j∈[1,N ]\nm∑\ni=1\n∑\nf∈Fi\n∑\ny∈Yf |Fi|1Ψj(xi,y)6=0\n) r2∞ log(2N)\n= Λ1r∞ √ 2s log(2N),\nwhich completes the proof of the first statement. The second statement can be proven in a similar way using the the definition of the dual norm and Jensen’s inequality:\nmR̂GS (H2) = E\n[ sup\n‖w‖2≤Λ2 w ·\nm∑\ni=1\n∑\nf∈Fi\n∑\ny∈Yf\n√ |Fi| i,f,yΨf (xi, y)\n]\n= Λ2 E [∥∥∥∥ m∑\ni=1\n∑\nf∈Fi\n∑\ny∈Yf\n√ |Fi| i,f,yΨf (xi, y) ∥∥∥∥ 2 ]\n= Λ2\n( E [∥∥∥∥ m∑\ni=1\n∑\nf∈Fi\n∑\ny∈Yf\n√ |Fi| i,f,yΨf (xi, y) ∥∥∥∥ 2\n2\n]) 12\n= Λ2\n( m∑\ni=1\n∑\nf∈Fi\n∑\ny∈Yf |Fi|‖Ψf (xi, y)‖22\n) 1 2\n≤ Λ2r2\n√√√√ m∑\ni=1\n∑\nf∈Fi\n∑\ny∈Yf |Fi|,\nwhich concludes the proof.\nA.5 Learning guarantees for structured prediction with linear hypotheses\nThe following result is a direct consequence of Theorem 1 and Theorem 2.\nCorollary 8. Fix ρ > 0. For any δ > 0, with probability at least 1− δ over the draw of a sample S of size m, the following holds for all h ∈ H1,\nR(h) ≤ R̂addS,ρ(h) + 4 √ 2\nρm Λ1r∞\n√ s log(2N) + 3 √ log 2δ 2m ,\nR(h) ≤ R̂multS,ρ (h) + 4 √ 2M\nρm Λ1r∞\n√ s log(2N) + 3 √ log 2δ 2m .\nSimilarly, for any δ > 0, with probability at least 1− δ over the draw of a sample S of size m, the following holds for all h ∈ H2,\nR(h) ≤ R̂addS,ρ(h) + 4 √ 2\nρm Λ2r2\n√∑m i=1 ∑ f∈Fi ∑ y∈Yf |Fi|+ 3 √ log 2δ 2m ,\nR(h) ≤ R̂multS,ρ (h) + 4 √ 2M\nρm Λ2r2\n√∑m i=1 ∑ f∈Fi ∑ y∈Yf |Fi|+ 3 √ log 2δ 2m .\nA.6 Learning guarantees for multi-class classification with linear hypotheses\nThe following result is a direct consequence of Corollary 8 and the observation that for multi-class classification |Fi| = 1 and di = maxf∈Fi |Yf | = c.\nCorollary 9. Fix ρ > 0. For any δ > 0, with probability at least 1− δ over the draw of a sample S of size m, the following holds for all h ∈ H1,\nR(h) ≤ R̂addS,ρ(h) + 4 √\n2Λ1r∞ ρ\n√ c log(2N)\nm + 3 √ log 2δ 2m ,\nR(h) ≤ R̂multS,ρ (h) + 4 √\n2MΛ1r∞ ρ\n√ c log(2N)\nm + 3 √ log 2δ 2m .\nSimilarly, for any δ > 0, with probability at least 1− δ over the draw of a sample S of size m, the following holds for all h ∈ H2,\nR(h) ≤ R̂addS,ρ(h) + 4 √\n2Λ2r2 ρ\n√ c\nm + 3 √ log 2δ 2m ,\nR(h) ≤ R̂multS,ρ (h) + 4 √\n2MΛ2r2 ρ\n√ c\nm + 3 √ log 2δ 2m .\nConsider the following set of linear hypothesis:\nH2,1 = {x 7→ w ·Ψ(x, y) : ‖w‖2,1 ≤ Λ2,1, y ∈ [c]}, where Ψ(x, y) = (0, . . . 0,Ψy(x), 0, . . . , 0)T ∈ RN1×...,Nc and w = (w1, . . . ,wc) with ‖w‖2,1 = ∑c y=1 ‖wy‖2. In this case, w · Ψ(x, y) = wy · Ψy(x). The standard scenario in multi-class classification is when Ψy(x) = Ψ(x) is the same for all y.\nCorollary 10. Fix ρ > 0. For any δ > 0, with probability at least 1− δ over the draw of a sample S of size m, the following holds for all h ∈ H2,1,\nR(h) ≤ R̂addS,ρ(h) + 16Λ2,1r2,∞(log(c))1/4\nρ √ m\n+ 3 √ log 2δ 2m ,\nR(h) ≤ R̂multS,ρ (h) + 16Λ2,1r2,∞(log(c))1/4\nρ √ m\n+ 3 √ log 2δ 2m ,\nwhere r2,∞ = maxx,y ‖Ψy(xi)‖2.\nProof. By definition of the dual norm andH2,1, the following holds:\nmR̂GS (H2,1) = E\n[ sup\n‖w‖2,1≤Λ w ·\nm∑\ni=1\n∑ y∈[c] i,yΨ(xi, y)\n]\n= ΛE [∥∥∥∥ m∑\ni=1\n∑ y∈[c] i,yΨ(xi, y) ∥∥∥∥ 2,∞ ]\n= ΛE [ max y ∥∥∥∥ m∑\ni=1\ni,yΨy(xi) ∥∥∥∥ 2\n]\n≤ Λ ( E [ max y ∥∥∥∥ m∑\ni=1\ni,yΨy(xi) ∥∥∥∥ 2\n2\n])1/2\n= Λ ( E [ max y m∑\ni=1\n∥∥∥∥Ψy(xi) ∥∥∥∥ 2\n2\n+ ∑\ni 6=j i,y j,yΨy(xi) ·Ψy(xj)\n])1/2\n≤ Λ (\nmax y\nm∑\ni=1\n∥∥∥∥Ψy(xi) ∥∥∥∥ 2\n2\n+ E [ max y ∑\ni 6=j i,y j,yΨy(xi) ·Ψy(xj)\n])1/2\nBy Lemma 7 (or Massart’s lemma), the following bound holds:\nE [ max y ∑\ni 6=j i,y j,yΨy(xi) ·Ψy(xj)\n] ≤ mr2,∞ √ log(c).\nSince, maxy ∑m i=1 ‖Ψy(xi)‖22 ≤ mr22,∞, we obtain that the following result holds:\nR̂GS (H2,1) ≤ √\n2Λr2,∞(log(c))1/4√ m ,\nand applying Theorem 1 completes the proof.\nA.7 VRM structured prediction learning bounds\nHere, we give the proof of our structured prediction learning guarantees in the setting of Voted Risk Minimization. We will use the following lemma.\nLemma 11. The function Φ∗ is sub-additive: Φ∗(x+ y) ≤ Φ∗(x) + Φ∗(y), for all x, y ∈ R.\nProof. By the sub-additivity of the maximum function, for any x, y ∈ R, the following upper bound holds for Φ∗(x+ y):\nΦ∗(x+ y) = min(M,max(0, x+ y)) ≤ min(M,max(0, x) + max(0, y)) ≤ min(M,max(0, x)) + min(M,max(0, y)) = Φ∗(x) + Φ∗(y),\nwhich completes the proof.\nFor the following proof, for any τ ≥ 0, the margin losses Raddρ,τ (h) and Rmultρ,τ (h) are defined as the population counterparts of the empirical losses define by (7) and (8).\nTheorem 3. Fix ρ > 0. For any δ > 0, with probability at least 1− δ over the draw of a sample S of size m, each of the following inequalities holds for all f ∈ F:\nR(f)− R̂addS,ρ,1(f) ≤ 4 √ 2\nρ\nT∑\nt=1\nαtR G m(Hkt) + C(ρ,M, c,m, p),\nR(f)− R̂multS,ρ,1(f) ≤ 4 √ 2M\nρ\nT∑\nt=1\nαtR G m(Hkt) + C(ρ,M, c,m, p).\nwhere\nC(ρ,M, c,m, p) = 2\ncρM\n√ log p\nm + 3M\n√ ⌈\n4 ρ2 log\n( c2M2ρ2m\n4 log p )⌉ log p m + log 2δ 2m .\nProof. The proof makes use of Theorem 1 and the proof techniques of Kuznetsov et al. [2014][Theorem 1] but requires a finer analysis both because of the general loss functions used here and because of the more complex structure of the hypothesis set.\nFor a fixed h = (h1, . . . , hT ), any α in the probability simplex ∆ defines a distribution over {h1, . . . , hT }. Sampling from {h1, . . . , hT } according to α and averaging leads to functions g of the form g = 1n ∑T i=1 ntht for some n = (n1, . . . , nT ), with ∑T t=1 nt = n, and ht ∈ Hkt . For any N = (N1, . . . , Np) with |N| = n, we consider the family of functions\nGF,N =\n{ 1\nn\np∑\nk=1\nNk∑\nj=1\nhk,j | ∀(k, j) ∈ [p]× [Nk], hk,j ∈ Hk } ,\nand the union of all such families GF,n = ⋃ |N|=nGF,N. Fix ρ > 0. For a fixed N, the factor graph Rademacher complexity of GF,N can be bounded as follows for any m ≥ 1:\nRGm(GF,N) ≤ 1\nn\np∑\nk=1\nNkR G m(Hk).\nThus, by Theorem 1, the following learning bound holds: for any δ > 0, with probability at least 1− δ, for all g ∈ GF,N,\nRaddρ, 12 (g)− R̂addS,ρ, 12 (g) ≤\n1\nn\n4 √ 2\nρ\np∑\nk=1\nNkR G m(Hk) + √ log 1δ 2m .\nSince there are at most pn possible p-tuples N with |N| = n,2 by the union bound, for any δ > 0, with probability at least 1− δ, for all g ∈ GF,n, we can write\nRaddρ, 12 (g)− R̂addS,ρ, 12 (g) ≤\n1\nn\n4 √ 2\nρ\np∑\nk=1\nNkR G m(Hk) +\n√ log p n\nδ\n2m .\nThus, with probability at least 1− δ, for all functions g = 1n ∑T i=1 ntht with ht ∈ Hkt , the following inequality holds\nRaddρ, 12 (g)− R̂addS,ρ, 12 (g) ≤\n1\nn\n4 √ 2\nρ\np∑\nk=1\n∑\nt:kt=k\nntR G m(Hkt) +\n√ log p n\nδ\n2m .\nTaking the expectation with respect to α and using Eα[nt/n] = αt, we obtain that for any δ > 0, with probability at least 1− δ, for all g, we can write\nE α [Raddρ, 12 (g)− R̂addS,ρ, 12 (g)] ≤\n4 √ 2\nρ\nT∑\nt=1\nαtR G m(Hkt) +\n√ log p n\nδ\n2m .\nFix n ≥ 1. Then, for any δn > 0, with probability at least 1− δn,\nE α [Raddρ, 12 (g)− R̂addS,ρ, 12 (g)] ≤\n4 √ 2\nρ\nT∑\nt=1\nαtR G m(Hkt) +\n√ log p n\nδn\n2m .\nChoose δn = δ2pn−1 for some δ > 0, then for p ≥ 2, ∑ n≥1 δn = δ 2(1−1/p) ≤ δ. Thus, for any δ > 0 and any n ≥ 1, with probability at least 1− δ, the following holds for all g:\nE α [Raddρ, 12 (g)− R̂addS,ρ, 12 (g)] ≤\n4 √ 2\nρ\nT∑\nt=1\nαtR G m(Hkt) +\n√ log 2p 2n−1\nδ\n2m . (13)\n2 The number S(p, n) of p-tuples N with |N| = n is known to be precisely ( p+n−1 p−1 ) .\nNow, for any f = ∑T t=1 αtht ∈ F and any g = 1n ∑T i=1 ntht, using (4), we can upper-bound R(f), the generalization error of f , as follows:\nR(f) = E [ L(f(x), y)1ρf (x,y)≤0 ] (14)\n≤ E [ L(f(x), y)1ρf (x,y)−(g(x,y)−g(x,yf ))<−ρ/2 ] + E [ L(f(x), y)1g(x,y)−g(x,yf )≤ρ/2 ] ≤M Pr [ ρf (x, y)− (g(x, y)− g(x, yf )) < −ρ/2 ] + E [ L(f(x), y)1g(x,y)−g(x,yf )≤ρ/2 ] ,\nwhere for any function ϕ : X × Y → [0, 1], we define y′ϕ as follows: y′ϕ = argmaxy′ 6=y ϕ(x, y). Using the same arguments as in the proof of Lemma 4, one can show that\nE [ L(f(x), y))1g(x,y)−g(x,yf )<ρ/2 ] ≤ Raddρ/2(g).\nWe now give a lower-bound on R̂addS,ρ,1(f) in terms ofR add S,ρ, 12 (g). To do so, we start with the expression of Radd S,ρ, 12 (g):\nR̂addS,ρ, 12 (g) = E (x,y)∼S\n[ Φ∗ (\nmax y′ 6=y\nL(y′, y) + 12− 1ρ [g(x, y)−g(x, y′)] )]\nBy the sub-additivity of max, we can write\nmax y′ 6=y\nL(y′, y) + 12− 1ρ [g(x, y)−g(x, y′)]\n≤ max y′ 6=y\n{ L(y, y′) + 1− f(x, y)− f(x, y\n′) ρ\n}\n+ max y′ 6=y\n{ − 1\n2 + f(x, y)− f(x, y′) ρ − g(x, y)− g(x, y ′) ρ\n} = X + Y,\nwhere X and Y are defined by\nX = max y′ 6=y\n( L(y, y′) + 1− f(x, y)− f(x, y\n′) ρ\n) ,\nY = −1 2 + max y′ 6=y\n( f(x, y)− f(x, y′)\nρ − g(x, y)− g(x, y ′) ρ\n) .\nIn view of that, since Φ∗ is non-decreasing and sub-additive (Lemma 11), we can write\nR̂addS,ρ, 12 (g) ≤ E (x,y)∼S [Φ∗(X + Y )] (15)\n≤ E (x,y)∼S [Φ∗(X) + Φ∗(Y )] = E (x,y)∼S [Φ∗(X)] + E (x,y)∼S [Φ∗(Y )]\n= R̂addS,ρ,1(f) + E (x,y)∼S [Φ∗(Y )]\n≤ R̂addS,ρ,1(f) +M E (x,y)∼S [1Y >0]\n= R̂addS,ρ,1(f) +M Pr (x,y)∼S [ max y′ 6=y { f(x, y)− g(x, y) + (g(x, y′)− f(x, y′)) } > ρ/2 ] .\nCombining (14) and (15) shows that R(f)− R̂addS,ρ,1(f) is bounded by\nRaddρ, 12 (g)− R̂addS,ρ, 12 (g) +M Pr\n[ ρf (x, y)− (g(x, y)− g(x, yf )) < −ρ/2 ]\n+M Pr (x,y)∼S [ max y′ 6=y {f(x, y)− g(x, y) + (g(x, y′)− f(x, y′))} > ρ/2 ] .\nTaking the expectation with respect to α shows that R(f)− R̂addS,ρ,1(f) is bounded by\nE α [ Raddρ, 12 (g)− R̂addS,ρ, 12 (g) ] +M E (x,y)∼D,α [ 1ρf (x,y)−(g(x,y)−g(x,yf ))<−ρ/2 ]\n+M E (x,y)∼S,α\n[ 1maxy′ 6=y{f(x,y)−g(x,y)+(g(x,y′)−f(x,y′))}>ρ/2 ] . (16)\nBy Hoeffding’s bound, the following holds:\nE α\n[ 1ρf (x,y)−(g(x,y)−g(x,yf ))<−ρ/2 ] = Pr\nα\n[ (f(x, y)− f(x, yf ))− (g(x, y)− g(x, yf )) < −ρ/2 ]\n≤ e−nρ2/8. Similarly, using the union bound and Hoeffding’s bound, the third expectation term appearing in (16) can be bounded as follows:\nE α [ 1maxy′ 6=y{f(x,y)−g(x,y)+(g(x,y′)−f(x,y′))}>ρ/2 ]\n= Pr α [ max y′ 6=y {f(x, y)− g(x, y) + (g(x, y′)− f(x, y′))} > ρ/2 ] ≤ ∑\ny′ 6=y Pr α\n[ f(x, y)− g(x, y) + (g(x, y′)− f(x, y′)) > ρ/2 ]\n≤ (c− 1)e−nρ2/8. Thus, for any fixed f , we can write\nR(f)− R̂addS,ρ,1(f) ≤ cMe−nρ 2/8 + E\nα [ Raddρ, 12 (g)− R̂addS,ρ, 12 (g) ] .\nTherefore, the following quantity upper bounds supf R(f)− R̂addS,ρ,1(f):\ncMe−nρ 2/8 + sup\ng E α [ Raddρ, 12 (g)− R̂addS,ρ, 12 (g) ] ,\nand, in view of (13), for any δ > 0 and any n ≥ 1, with probability at least 1− δ, the following holds for all f :\nR(f)− R̂addS,ρ,1(f) ≤ cMe−nρ 2/8 +\n4 √ 2\nρ\nT∑\nt=1\nαtR G m(Hkt) +\n√ log 2p 2n−1\nδ\n2m .\nChoosing n = ⌈\n4 ρ2 log\n( M2c2ρ2m\n4 log p\n)⌉ yields the following inequality:3\nR(f)− R̂addS,ρ,1(f) ≤ 4 √ 2\nρ\nT∑\nt=1\nαtR G m(Hkt) +\n2\ncρM\n√ log p\nm\n+ 3M\n√ ⌈\n4 ρ2 log\n( c2M2ρ2m\n4 log p )⌉ log p m + log 2δ 2m ,\nand concludes the proof.\nA.8 General upper bound on the loss based on convex surrogates\nHere, we present the proof of a general upper bound on a loss function in terms of convex surrogates.\n3To select n we consider f(n) = ce−nu + √ nv, where u = ρ2/8 and v = log p/m. Taking the derivative of f , setting it to zero and solving for n, we obtain n = − 1 2u W−1(− v2c2u ) where W−1 is the second branch of the Lambert function (inverse of x 7→ xex). Using the bound − log x ≤ −W−1(−x) ≤ 2 log x leads to the following choice of n: n = ⌈ − 1\n2u log( v 2c2u ) ⌉ .\nLemma 4. For any u ∈ R+, let Φu : R→ R be a decreasing upper bound on v 7→ u1v≤0 such that u 7→ Φu(v) is increasing for a fixed v. Then, the following upper bound holds for any h ∈ H and (x, y) ∈ X × Y ,\nL(h(x), y) ≤ max y′ 6=y ΦL(y′,y)(h(x, y)− h(x, y′)). (17)\nProof. By (4) and the property of Φu as an upper bound, for any (x, y) ∈ X × Y , we can write L(h(x), y) = L(h(x), y)1ρh(x,y)≤0\n≤ ΦL(h(x),y)(ρh(x, y)) = ΦL(h(x),y)(h(x, y)−max\ny′ 6=y h(x, y′)).\nIf h(x) 6= y, then, since maxy′ 6=y h(x, y′) = h(x, h(x)), the following holds: L(h(x), y) ≤ ΦL(h(x),y)(h(x, y)− h(x, h(x))\n≤ max y′ 6=y ΦL(y′,y)(h(x, y)− h(x, y′)).\nOtherwise, if h(x) = y, then L(h(x), y)) = 0 ≤ L(y′, y) for all y′ 6= y. Since u 7→ Φu(h(x, y) − h(x, y′)) is increasing, this implies\nΦL(h(x),y)(h(x, y)− h(x, y′)) ≤ ΦL(y′,y)(h(x, y)− h(x, y′)),\nfor all y′ 6= y. Using the fact that ΦL(h(x),y) is decreasing and the inequality just shown yields\nL(h(x), y) = ΦL(h(x),y)(h(x, y)−max y′ 6=y h(x, y′))\n= max y′ 6=y\nΦL(h(x),y)(h(x, y)− h(x, y′))\n≤ max y′ 6=y ΦL(y′,y)(h(x, y)− h(x, y′)),\nand completes the proof."
    }, {
      "heading" : "B Experiments",
      "text" : "This section reports the results of preliminary experiments with the VCRF algorithm. The experiments in this section are meant to serve as a proof of concept of the benefits of VRM-type regularization as suggested by the theory developed in this paper. We leave an extensive experimental study of other aspects of our theory, including general loss functions, convex surrogates and p-norms, to future work.\nFor our experiments, we chose the part-of-speech task (POS) that consists of labeling each word of a sentence with its correct part-of-speech tag. We used 10 POS datasets: Basque, Chinese, Dutch, English, Finnish, Finnish-FTB, Hindi, Tamil, Turkish and Twitter. The detailed description of these datasets is in Appendix B.1. Our VCRF algorithm can be applied with a variety of different families of feature functions Hk mapping X × Y to R. Details concerning features and complexity penalties rks are provided in Appendix B.2, while an outline of our hyperparameter selection and cross-validation procedure is given in Appendix B.3.\nThe average error and the standard deviation of the errors are reported in Table 1 for each data set. Our results show that VCRF provides a statistically significant improvement over L1-CRF on every dataset, with the exception of English and Dutch. One-sided paired t-test at 5% level was used to assess the significance of the results. It should be noted that for all of the significant results, VCRF outperformed L1-CRF on every fold. Furthermore, our results indicate that VCRF tends to produce models that are sparser than those of L1-CRF. This is highlighted in Table 2 of Appendix B.2. In a separate set of experiments, we have also tested the robustness of our algorithm to erroneous annotations and noise. The details and the results of these experiments are given in Appendix B.4.\nTable 2 illustrates the impact on sparsity of VCRF versus L1-regularized CRF. As can be seen, VCRF tends to produce models that are much more sparse, due to its heavy penalization on the large number of higher-order features.\nFurther details on the datasets and the specific features as well as more experimental results are provided below.\nB.1 Datasets\nTable 3 provides some statistics for each of the datasets that we use. These datasets span a variety of sizes, in terms of sentence count, token count, and unique token count. Most are annotated under the Universal Dependencies (UD) annotation system, with the exception of the Chinese (Palmer et al. [2007]), Turkish (Oflazer et al. [2003], Atalay et al. [2003]), and Twitter (Gimpel et al. [2011], Owoputi et al. [2013]) datasets.\nB.2 Features and Complexities\nThe standard features that are used in POS tagging are usually binary indicators that signal the occurrence of certain words, tags or other linguistic constructs such as suffixes, prefixes, punctuation, capitalization or numbers in a window around a given position in the sequence. In our experiments, we use the union of a broad family of products of such indicator functions. Let V denote the input vocabulary over alphabet Σ. For x ∈ V and t ≥ 0, let suff(x, t) be the suffix of length t for the word x and pref(x, t) the prefix. Then for k1, k2, k3 ≥ 0, we can define the following three families of base features:\nHwk1(s) = { x 7→ 1xs+rs−t+1=x′ : t, r ∈ N, r + t = k1, x ′ ∈ V k1 } , H tagk2 (s) = {y 7→ 1yss−k2+1=y′ : y ∈ ∆ k2}, Hspk3(s) = { x 7→ 1suff(xs,t)=S1pref(xs,r)=P : t, r ∈ N, t+ r = k3, S ∈ Σt, P ∈ Σr } .\nWe can then define a family of features Hk1,k2,k3 that consists of functions of the form\nΨ(x, y) =\nl∑\ns=1\nψ(x, yss−k2+1, s),\nwhere ψ(x, yss−k2+1, s) = h1(x)h2(y s s−k2+1)h3(x), for some h1 ∈ Hwk1(s), h2 ∈ H tag k2 (s), h3 ∈ Hspk3(s). As an example, consider the following sentence:\nDET NN VBD RB JJ The cat was surprisingly agile\nThen, at position s = 3, the following features h1 ∈ Hw3 (3), h2 ∈ Htag2 (3), h3 ∈ Hsp1 (3) would activate:\nh1(x) = 1x2=‘was’, x3=‘surprisingly’, x4=‘agile’(x)\nh2(y) = 1y2=’VBD’, y3=‘RB’(y)\nh3(x) = 1suff(x3,2)=‘ly’(x).\nSee Figure 2 for an illustration.\nNow, recall that the VCRF algorithm requires knowledge of complexities r(Hk1,k2,k3). By definition of the hypothesis set and rks\nr(Hk1,k2,k3) ≤ √\n2(k1 log |V |+ k2 log |∆|+ k3 log |Σ| m , (18)\nwhich is precisely the complexity penalty used in our experiments.\nThe impact of this added penalization can be seen in Table 2, where it is seen that the number of non-zero features for VCRF can be dramatically smaller than the number for L1-regularized CRF.\nB.3 Hyperparameter Tuning and Cross-Validation\nRecall that the VCRF algorithm admits two hyperparameters λ and β. In our experiments, we optimized over λ, β ∈ {1, 0.5, 10−1, . . . , 10−5, 0}. We compared VCRF against L1-regularized CRF, which is the special case of VCRF with λ = 0. For gradient computation, we used the procedure in Section D.2.1, which is agnostic to the choice of the underlying loss function. While our algorithms can be used with very general families of loss functions this choice allows an easy direct comparison with the CRF algorithm. We ran each algorithm for 50 full passes over the entire training set or until convergence.\nIn each of the experiments, we used 5-fold cross-validation for model selection and performance evaluation. Each dataset was randomly partitioned into 5 folds, and each algorithm was run 5 times, with a different assignment of folds to the training set, validation set and test set for each run. For each run i ∈ {0, . . . , 4}, fold i was used for validation, fold i+ 1( mod 5) was used for testing, and the remaining folds were used for training. In each run, we selected the parameters that had the lowest token error on the validation set and then measured the token error of those parameters on the test set. The average error and the standard deviation of the errors are reported in Table 1 for each data set.\nB.4 More Experiments\nIn this section, we present our results for a POS tagging task when noise is artificially injected into the labels. Specifically, for tokens corresponding to features that commonly appear in the dataset (at least five times in our experiments), we flip their associated POS label to some other arbitrary label with a specified probability.\nThe results of these experiments are given in Table 4. They demonstrate that VCRF outperforms L1CRF in the majority of cases. Moreover, these differences can be magnified from the original scenario, as can be seen on the examples of English and Twitter datasets at the 30% noise level.\nC Voted Structured Boosting (VStructBoost)\nIn this section, we consider algorithms based on the StructBoost surrogate loss, where we choose Φu(v) = ue\n−v . Let δΨ(x, y, y′) = Ψ(x, y)−Ψ(x, y′). This then leads to the following optimization problem:\nmin w\n1\nm\nm∑\ni=1\nmax y 6=yi\nL(y, yi)e −w·δΨ(xi,yi,y) +\np∑\nk=1\n(λrk + β)‖wk‖1. (19)\nOne disadvantage of this formulation is that the first term of the objective is not differentiable. Upper bounding the maximum by a sum leads to the following optimization problem:\nmin w\n1\nm\nm∑\ni=1\n∑ y 6=yi L(y, yi)e −w·δΨ(xi,yi,y) + p∑ k=1 (λrk + β)‖wk‖1. (20)\nWe refer to the learning algorithm based on the optimization problem (20) as VStructBoost. To the best of our knowledge, the formulations (19) and (20) are new, even with the standard L1- or L2-regularization."
    }, {
      "heading" : "D Optimization solutions",
      "text" : "Here, we show how the optimization problems in (10) and (20) can be solved efficiently when the feature vectors admit a particular factor graph decomposition that we refer to as Markov property.\nD.1 Markovian features\nWe will consider in what follows the common case where Y is a set of sequences of length l over a finite alphabet ∆ of size r. Other structured problems can be treated in similar ways. We will denote by ε the empty string and for any sequence y = (y1, . . . , yl) ∈ Y , we will denote by ys ′\ns = (ys, . . . , ys′) the substring of y starting at index s and ending at s ′. For convenience, for s ≤ 0,\nwe define ys by ys = ε.\nOne common assumption that we shall adopt here is that the feature vector Ψ admits a Markovian property of order q. By this, we mean that it can be decomposed as follows for any (x, y) ∈ X × Y:\nΨ(x, y) =\nl∑\ns=1\nψ(x, yss−q+1, s). (21)\nfor some position-dependent feature vector function ψ defined over X ×∆q × [l]. This also suggests a natural decomposition of the family of feature vectors Ψ = (Ψ1, . . . ,Ψp) for the application of VRM principle where Ψk is a Markovian feature vector of order k. Thus, Fk then consists of the family of Markovian feature functions of order k. We note that we can write Ψ = ∑p k=1 Ψ̃k with Ψ̃k = (0, . . . ,Ψk, . . . , 0). In the following, abusing the notation, we will simply write Ψk instead of Ψ̃k. Thus, for any x ∈ X and y ∈ Y ,4\nΨ(x, y) =\np∑\nk=1\nΨk(x, y). (22)\nFor any k ∈ [1, p], let ψk denote the position-dependent feature vector function corresponding to Ψk. Also, for any x ∈ X and y ∈ ∆l, define ψ̃ by ψ̃(x, yss−p+1, s) = ∑p k=1ψk(x, y s s−k+1, s). Observe then that we can write\nΨ(x, y) =\np∑\nk=1\nΨk(x, y) =\np∑\nk=1\nl∑\ns=1\nψk(x, y s s−k+1, s)\n=\nl∑\ns=1\np∑\nk=1\nψk(x, y s s−k+1, s)\n=\nl∑\ns=1\nψ̃(xi, y s s−p+1, s). (23)\nIn Sections D.2 and D.3, we describe algorithms for efficiently computing the gradient by leveraging the underlying graph structure of the problem.\nD.2 Efficient gradient computation for VCRF\nIn this section, we show how Gradient Descent (GD) and Stochastic Gradient Descent (SGD) can be used to solve the optimization problem of VCRF. To do so, we will show how the subgradient of the contribution to the objective function of a given point xi can be computed efficiently. Since the computation of the subgradient of the regularization term presents no difficulty, it suffices to show that the gradient of Fi, the contribution of point xi to the empirical loss term for an arbitrary i ∈ [m], can be computed efficiently. In the special case of the Hamming loss or when loss is omitted from the objective altogether, this coincides with the standard CRF training procedure. We extend this to more general families of loss function.\nFix i ∈ [m]. For the VCRF objective, Fi can be rewritten as follows:\nFi(w) = 1\nm log\n(∑\ny∈Y eL(y,yi)−w·δΨ(xi,yi,y)\n) = 1\nm log\n(∑\ny∈Y eL(y,yi)+w·Ψ(xi,y)\n) −w ·Ψ(xi, yi)\nm .\nThe following lemma gives the expression of the gradient of Fi and helps identify the key computationally challenging terms qw. Lemma 12. The gradient of Fi at any w can be expressed as follows:\n∇Fi(w) = 1\nm\nl∑\ns=1\n∑\nz∈∆p\n[ ∑\ny : yss−p+1=z\nqw(y) ] ψ̃(xi, z, s)− Ψ(xi, yi)\nm ,\nwhere, for all y ∈ Y ,\nqw(y) = eL(y,yi)+w·Ψ(xi,y)\nZw ,\nZw = ∑\ny∈Y eL(y,yi)+w·Ψ(xi,y).\n4Our results can be straightforwardly generalized to more complex decompositions of the form Ψ(x, y) =∑Q q=1 ∑p k=1 Ψq,k(x, y).\nProof. In view of the expression of Fi given above, the gradient of Fi at any w is given by\n∇Fi(w) = 1\nm\n∑\ny∈Y\neL(y,yi)+w·Ψ(xi,y)∑ ỹ∈Y e L(ỹ,yi)+w·Ψ(xi,ỹ) Ψ(xi, y)− Ψ(xi, yi) m\n= 1\nm E y∼qw [Ψ(xi, y)]−\nΨ(xi, yi)\nm .\nBy (23), we can write\nE y∼qw\n[Ψ(xi, y)] = ∑\ny∈∆l qw(y)\nl∑\ns=1\nψ̃(xi, y s s−p+1, s) =\nl∑\ns=1\n∑\nz∈∆p\n[ ∑\ny : yss−p+1=z\nqw(y) ] ψ̃(xi, z, s),\nwhich completes the proof.\nThe lemma implies that the key computation in the gradient is\nQw(z, s) = ∑\ny : yss−p+1=z\nqw(y) = ∑\ny : yss−p+1=z\neL(y,yi) ∏l t=1 e w·ψ̃(xi,ytt−p+1,t)\nZw , (24)\nfor all s ∈ [l] and z ∈ ∆p. The sum defining these terms is over a number of sequences y that is exponential in |∆|. However, we will show in the following sections how to efficiently compute Qw(z, s) for any s ∈ [l] and z ∈ ∆p in several important cases: (0) in the absence of a loss; (1) when L is Markovian; (2) when L is a rational loss; and (3) when L is the edit-distance or any other tropical loss.\nD.2.1 Gradient computation in the absence of a loss In that case, it suffices to show how to compute Z ′w = ∑ y∈Y e\nw·Ψ(xi,y) and the following term, ignoring the loss factors:\nQ′w(z, s) = ∑\ny : yss−p+1=z\nl∏\nt=1\new·ψ̃(xi,y t t−p+1,t), (25)\nfor all s ∈ [l] and z ∈ ∆p. We will show that Q′w(z, s) coincides with the flow through an edge of a weighted graph we will define, which leads to an efficient computation. We will use for any y ∈ ∆l, the convention ys = ε if s ≤ 0. Now, let A be the weighted finite automaton (WFA) with the following set of states:\nQA = { (ytt−p+1, t) : y ∈ ∆l, t = 0, . . . , l } ,\nwith IA = (ε, 0) its single initial state, FA = {(yll−p+1, l) : y ∈ ∆l} its set of final states, and a transition from state (yt−1t−p+1, t− 1) to state (yt−1t−p+2 b, t) with label b and weight ω(yt−1t−p+1 b, t) = ew·ψ̃(xi,y t−1 t−p+1b,t), that is the following set of transitions:\nEA = {( (yt−1t−p+1, t− 1), b, ω(yt−1t−p+1 b, t), (yt−1t−p+2 b, t) ) : y ∈ ∆l, b ∈ ∆, t ∈ [l] } .\nFigure 3 illustrates this construction in the case p = 2. The WFA A is deterministic by construction. The weight of a path in A is obtained by multiplying the weights of its constituent transitions. In view of that, Q′w(z, s) can be seen as the sum of the weights of all paths in A going through the transition from state (zp−11 , s− 1) to (zp2, s) with label zp. For any state (ytt−p+1, t) ∈ QA, let α((ytt−p+1, t)) denote the sum of the weights of all paths in A from IA to (ytt−p+1, t) and β((y t t−p+1, t)) the sum of the weights of all paths from (y t t−p+1, t) to a final state. Then, Q′w(z, s) is given by\nQ′w(z, s) = α ( (zp−11 , s− 1) ) × ω(z, s)× β ( (zp2, s) ) .\nNote also that Z ′w is simply the sum of the weights of all paths in A, that is Z ′ w = β((ε, 0)).\nSince A is acyclic, α and β can be computed for all states in linear time in the size of A using a single-source shortest-distance algorithm over the (+,×) semiring or the so-called forward-backward algorithm. Thus, since A admits O(l|∆|p) transitions, we can compute all of the quantities Q′w(z, s), s ∈ [l] and z ∈ ∆p and Z ′w, in time O(l|∆|p).\nD.2.2 Gradient computation with a Markovian loss\nWe will say that a loss function L is Markovian if it admits a decomposition similar to the features, that is for all y, y′ ∈ Y ,\nL(y, y′) = l∑\nt=1\nLt(y t t−p+1, y ′t t−p+1).\nIn that case, we can absorb the losses in the transition weights and define new transition weights ω′ as follows:\nω′(t, yt−1t−p+1 b) = e Lt(y t−1 t−p+1 b,(yi) t−1 t−p+1 b)ω(yt−1t−p+1 b, t).\nUsing the resulting WFA A′ and precisely the same techniques as those described in the previous section, we can compute all Qw(z, s) in time O(l|∆|p). In particular, we can compute efficiently these quantities in the case of the Hamming loss which is a Markovian loss for p = 1.\nD.3 Efficient gradient computation for VStructBoost\nIn this section, we briefly describe the gradient computation for VStructBoost, which follows along similar lines as the discussion for VCRF.\nFix i ∈ [m] and let Fi denote the contribution of point xi to the empirical loss in VStructBoost. Using the equality L(yi, yi) = 0, Fi can be rewritten as\nFi(w) = 1\nm\n∑ y 6=yi L(y, yi)e −w·δΨ(xi,yi,y) = 1 m e−w·Ψ(xi,yi) ∑ y∈∆l L(y, yi)e w·Ψ(xi,y).\nThe gradient of Fi can therefore be expressed as follows:\n∇Fi(w) = 1 m e−w·Ψ(xi,yi)\n∑\ny∈∆l L(y, yi)e\nw·Ψ(xi,y)Ψ(xi, y) (26)\n− 1 m e−w·Ψ(xi,yi)Ψ(xi, yi)\n∑\ny∈∆l L(y, yi)e\nw·Ψ(xi,y).\nEfficient computation of these terms is not straightforward, since the sums run over exponentially many sequences y. However, by leveraging the Markovian property of the features, we can reduce the calculation to flow computations over a weighted directed graph, in a manner analogous to what we demonstrated for VCRF.\nD.4 Inference\nIn this section, we describe an efficient algorithm for inference when using Markovian features. The algorithm consists of a standard single-source shortest-path algorithm applied to a WFA A′ differs from the WFA A only by the weight of each transition, defined as follows:\nEA′ = {( (ȳt−1t−p+1, t− 1), b,w · ψ̃(x, yt−1t−p+1b, t), (ȳt−1t−p+2 b, t) ) : y ∈ ∆l, b ∈ ∆, t ∈ [l] } .\nFurthermore, here, the weight of a path is obtained by adding the weights of its constituent transitions. Figure 4 shows A′ in the special case of p = 2. By construction, the weight of the unique accepting path in A′ labeled with y ∈ ∆l is∑lt=1 w · ψ̃(x, yt−1t−p+1b, t) = w ·Ψ(x, y). Thus, the label of the single-source shortest path, argminy∈∆l w ·Ψ(x, y), is the desired predicted label. Since A′ is acyclic, the running-time complexity of the algorithm is linear in the size of A′, that is O(l|∆|l)."
    }, {
      "heading" : "Appendix References",
      "text" : "N. B. Atalay, K. Oflazer, and B. Say. The annotation process in the turkish treebank. In LINC, 2003.\nK. Gimpel, N. Schneider, B. O’Connor, D. Das, D. Mills, J. Eisenstein, M. Heilman, D. Yogatama, J. Flanigan, and N. A. Smith. Part-of-speech tagging for twitter: annotation, features, and experiments. In ACL, 2011.\nV. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error of combined classifiers. Annals of Statistics, 30, 2002.\nV. Kuznetsov, M. Mohri, and U. Syed. Multi-class deep boosting. In Proceedings of NIPS, 2014.\nM. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer, 1991.\nM. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. The MIT Press, 2012.\nK. Oflazer, B. Say, D. Z. Hakkani-Tür, and G. Tür. Building a turkish treebank. In Text, Speech and Language Technology, volume 20, pages 261–277. Springer Netherlands, 2003.\nO. Owoputi, B. O’Connor, C. Dyer, K. Gimpel, N. Schneider, and N. A. Smith. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL-HLT, pages 380–390, 2013.\nM. Palmer, N. Xue, F. Xia, F.-D. Chiou, Z. Jiang, and M. Chang. Chinese treebank 6.0 LDC2007T36. Web Download, 2007."
    } ],
    "references" : [ {
      "title" : "The annotation process in the turkish treebank",
      "author" : [ "N.B. Atalay", "K. Oflazer", "B. Say" ],
      "venue" : "In LINC,",
      "citeRegEx" : "Atalay et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Atalay et al\\.",
      "year" : 2003
    }, {
      "title" : "Part-of-speech tagging for twitter: annotation, features, and experiments",
      "author" : [ "K. Gimpel", "N. Schneider", "B. O’Connor", "D. Das", "D. Mills", "J. Eisenstein", "M. Heilman", "D. Yogatama", "J. Flanigan", "N.A. Smith" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Gimpel et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gimpel et al\\.",
      "year" : 2011
    }, {
      "title" : "Empirical margin distributions and bounding the generalization error of combined classifiers",
      "author" : [ "V. Koltchinskii", "D. Panchenko" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Koltchinskii and Panchenko.,? \\Q2002\\E",
      "shortCiteRegEx" : "Koltchinskii and Panchenko.",
      "year" : 2002
    }, {
      "title" : "Multi-class deep boosting",
      "author" : [ "V. Kuznetsov", "M. Mohri", "U. Syed" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "Kuznetsov et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kuznetsov et al\\.",
      "year" : 2014
    }, {
      "title" : "Probability in Banach Spaces: Isoperimetry and Processes",
      "author" : [ "M. Ledoux", "M. Talagrand" ],
      "venue" : null,
      "citeRegEx" : "Ledoux and Talagrand.,? \\Q1991\\E",
      "shortCiteRegEx" : "Ledoux and Talagrand.",
      "year" : 1991
    }, {
      "title" : "Foundations of Machine Learning",
      "author" : [ "M. Mohri", "A. Rostamizadeh", "A. Talwalkar" ],
      "venue" : null,
      "citeRegEx" : "Mohri et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mohri et al\\.",
      "year" : 2012
    }, {
      "title" : "Building a turkish treebank",
      "author" : [ "K. Oflazer", "B. Say", "D.Z. Hakkani-Tür", "G. Tür" ],
      "venue" : "In Text, Speech and Language Technology,",
      "citeRegEx" : "Oflazer et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Oflazer et al\\.",
      "year" : 2003
    }, {
      "title" : "Improved part-of-speech tagging for online conversational text with word clusters",
      "author" : [ "O. Owoputi", "B. O’Connor", "C. Dyer", "K. Gimpel", "N. Schneider", "N.A. Smith" ],
      "venue" : "In Proceedings of NAACL-HLT,",
      "citeRegEx" : "Owoputi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Owoputi et al\\.",
      "year" : 2013
    }, {
      "title" : "Chinese treebank 6.0 LDC2007T36",
      "author" : [ "M. Palmer", "N. Xue", "F. Xia", "F.-D. Chiou", "Z. Jiang", "M. Chang" ],
      "venue" : "Web Download,",
      "citeRegEx" : "Palmer et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Palmer et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "All of these margin bounds can be extended to hold uniformly over ρ ∈ (0, 1] at the price of an additional term of the form √ (log log2 2 ρ )/m in the bound, using known techniques (see for example [Mohri et al., 2012]).",
      "startOffset" : 198,
      "endOffset" : 218
    }, {
      "referenceID" : 3,
      "context" : "In that case, for linear hypotheses, the complexity term of our bound varies as O(Λ2r2 √ c/ρ2m) for example for norm-2 regularization (Corollary 9), which improves upon the best known general margin bounds of Kuznetsov et al. [2014], who provide a guarantee that scales linearly with the number of classes instead.",
      "startOffset" : 209,
      "endOffset" : 233
    }, {
      "referenceID" : 3,
      "context" : "In that case, for linear hypotheses, the complexity term of our bound varies as O(Λ2r2 √ c/ρ2m) for example for norm-2 regularization (Corollary 9), which improves upon the best known general margin bounds of Kuznetsov et al. [2014], who provide a guarantee that scales linearly with the number of classes instead. Moreover, in the special case where an individual wy is learned for each class y ∈ [c], we retrieve the recent favorable bounds given by Lei et al. [2015], albeit with a somewhat simpler formulation.",
      "startOffset" : 209,
      "endOffset" : 470
    }, {
      "referenceID" : 3,
      "context" : "In that case, for linear hypotheses, the complexity term of our bound varies as O(Λ2r2 √ c/ρ2m) for example for norm-2 regularization (Corollary 9), which improves upon the best known general margin bounds of Kuznetsov et al. [2014], who provide a guarantee that scales linearly with the number of classes instead. Moreover, in the special case where an individual wy is learned for each class y ∈ [c], we retrieve the recent favorable bounds given by Lei et al. [2015], albeit with a somewhat simpler formulation. In that case, for any (x, y), all components of the feature vector Ψ(x, y) are zero, except (perhaps) for the N components corresponding to class y, where N is the dimension of wy. In view of that, for example for a group-norm ‖ · ‖2,1regularization, the complexity term of our bound varies as O(Λr √ (log c)/ρ2m), which matches the results of Lei et al. [2015] with a logarithmic dependency on c (ignoring some complex exponents of log c in their case).",
      "startOffset" : 209,
      "endOffset" : 877
    }, {
      "referenceID" : 2,
      "context" : "By standard Rademacher complexity bounds (Koltchinskii and Panchenko [2002]), for any δ > 0, with probability at least 1− δ, the following inequality holds for all h ∈ H: Radd ρ (h) ≤ R̂add S,ρ(h) + 2Rm(H0) + √ log 1δ 2m ,",
      "startOffset" : 42,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "Since Φ∗ is 1-Lipschitz, by Talagrand’s contraction lemma (Ledoux and Talagrand [1991], Mohri et al.",
      "startOffset" : 59,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "Since Φ∗ is 1-Lipschitz, by Talagrand’s contraction lemma (Ledoux and Talagrand [1991], Mohri et al. [2012]), we have Rm(H0) ≤ Rm(H1).",
      "startOffset" : 59,
      "endOffset" : 108
    }, {
      "referenceID" : 3,
      "context" : "The proof makes use of Theorem 1 and the proof techniques of Kuznetsov et al. [2014][Theorem 1] but requires a finer analysis both because of the general loss functions used here and because of the more complex structure of the hypothesis set.",
      "startOffset" : 61,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "Most are annotated under the Universal Dependencies (UD) annotation system, with the exception of the Chinese (Palmer et al. [2007]), Turkish (Oflazer et al.",
      "startOffset" : 111,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "[2007]), Turkish (Oflazer et al. [2003], Atalay et al.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "[2003], Atalay et al. [2003]), and Twitter (Gimpel et al.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "[2003], Atalay et al. [2003]), and Twitter (Gimpel et al. [2011], Owoputi et al.",
      "startOffset" : 8,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "[2003], Atalay et al. [2003]), and Twitter (Gimpel et al. [2011], Owoputi et al. [2013]) datasets.",
      "startOffset" : 8,
      "endOffset" : 88
    } ],
    "year" : 2016,
    "abstractText" : "We present a general theoretical analysis of structured prediction. By introducing a new complexity measure that explicitly factors in the structure of the output space and the loss function, we are able to derive new data-dependent learning guarantees for a broad family of losses and for hypothesis sets with an arbitrary factor graph decomposition. We extend this theory by leveraging the principle of Voted Risk Minimization (VRM) and showing that learning is possible with complex factor graphs. We both present new learning bounds in this advanced setting as well as derive two new families of algorithms, Voted Conditional Random Fields and Voted Structured Boosting, which can make use of very complex features and factor graphs without overfitting. Finally, we also validate our theory through experiments on several datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}