{
  "name" : "1305.0355.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition",
    "authors" : [ "Adel Javanmard", "Andrea Montanari" ],
    "emails" : [ "adelj@stanford.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "A popular approach is to estimate the regression coefficients through the Lasso (`1-regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that first solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set.\nWe formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set.\nContents"
    }, {
      "heading" : "1 Introduction 2",
      "text" : "1.1 An example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 Further related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.3 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7"
    }, {
      "heading" : "2 Deterministic designs 7",
      "text" : "2.1 Zero-noise problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.2 Noisy problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9"
    }, {
      "heading" : "3 Random Gaussian designs 10",
      "text" : "3.1 The n =∞ problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 The high-dimensional problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12"
    }, {
      "heading" : "4 UCI communities and crimes data example 14",
      "text" : "∗Department of Electrical Engineering, Stanford University. Email: adelj@stanford.edu\n†Department of Electrical Engineering and Department of Statistics, Stanford University. Email: montanar@ stanford.edu\nar X\niv :1\n30 5.\n03 55\nv1 [\nm at\nh. ST\n] 2\nM ay\n2 01"
    }, {
      "heading" : "5 Proof of Theorems 2.5 and 2.7 15",
      "text" : "5.1 Proof of Theorem 2.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 5.2 Proof of Theorem 2.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17"
    }, {
      "heading" : "6 Proof of Theorems 3.4 and 3.7 18",
      "text" : ""
    }, {
      "heading" : "A Proof of technical lemmas 23",
      "text" : ""
    }, {
      "heading" : "B Generalized irrepresentability vs. irrepresentability 28",
      "text" : "References 32"
    }, {
      "heading" : "1 Introduction",
      "text" : "In linear regression, we wish to estimate an unknown but fixed vector of parameters θ0 ∈ Rp from n pairs (Y1, X1), (Y2, X2), . . . , (Yn, Xn), with vectors Xi taking values in Rp and response variables Yi given by\nYi = 〈θ0, Xi〉+Wi , Wi ∼ N(0, σ2) , (1)\nwhere 〈 · , · 〉 is the standard scalar product. In matrix form, letting Y = (Y1, . . . , Yn)\nT and denoting by X the design matrix with rows XT1 , . . . , X T n , we have\nY = X θ0 +W , W ∼ N(0, σ2In×n) . (2)\nIn this paper, we consider the high-dimensional setting in which the number of parameters exceeds the sample size, i.e., p > n, but the number of non-zero entries of θ0 is smaller than p. We denote by S ≡ supp(θ0) ⊆ [p] the support of θ0, and let s0 ≡ |S|. We are interested in the ‘model selection’ problem, namely in the problem of identifying S from data Y , X.\nIn words, there exists a ‘true’ low dimensional linear model that explains the data. We want to identify the set S of covariates that are ‘active’ within this model. This problem has motivated a large body of research, because of its relevance to several modern data analysis tasks, ranging from signal processing [Don06, CRT06] to genomics [PZB+10, SK03]. A crucial step forward has been the development of model-selection techniques based on convex optimization formulations [Tib96, CD95, CT07]. These formulations have lead to computationally efficient algorithms that can be applied to large scale problems. Such developments pose the following theoretical question: For which vectors θ0, designs X, and noise levels σ, the support S can be identified, with high probability, through computationally efficient procedures? The same question can be asked for random designs X and, in this case, ‘high probability’ will refer both to the noise realization W , and to the design realization X. In the rest of this introduction we shall focus –for the sake of simplicity– on the deterministic settings, and refer to Section 3 for a treatment of Gaussian random designs.\nThe analysis of computationally efficient methods has largely focused on `1-regularized least squares, a.k.a. the Lasso [Tib96]. The Lasso estimator is defined by\nθ̂n(Y,X;λ) ≡ arg min θ∈Rp { 1 2n ‖Y −Xθ‖22 + λ‖θ‖1 } . (3)\nIn case the right hand side has more than one minimizer, one of them can be selected arbitrarily for our purposes. We will often omit the arguments Y , X, as they are clear from the context. (A closely related method is the so-called Dantzig selector [CT07]: it would be interesting to explore whether our results can be generalized to that approach.)\nIt was understood early on that, even in the large-sample, low-dimensional limit n → ∞ at p constant, supp(θ̂n) 6= S unless the columns of X with index in S are roughly orthogonal to the ones with index outside S [KF00]. This assumption is formalized by the so-called ‘irrepresentability condition’, that can be stated in terms of the empirical covariance matrix Σ̂ = (XTX/n). Letting Σ̂A,B be the submatrix (Σ̂i,j)i∈A,j∈B, irrepresentability requires\n‖Σ̂Sc,SΣ̂−1S,S sign(θ0,S)‖∞ ≤ 1− η , (4)\nfor some η > 0 (here sign(u)i = +1, 0, −1 if, respectively, ui > 0, = 0, < 0). In an early breakthrough, Zhao and Yu [ZY06] proved that, if this condition holds with η uniformly bounded away from 0, it guarantees correct model selection also in the high-dimensional regime p n. Meinshausen and Bülmann [MB06] independently established the same result for random Gaussian designs, with applications to learning Gaussian graphical models. These papers applied to very sparse models, requiring in particular s0 = O(n\nc), c < 1, and parameter vectors with large coefficients. Namely, scaling the columns of X such that Σ̂i,i ≤ 1, for i ∈ [p], they require θmin ≡ mini∈S |θ0,i| ≥ c √ s0/n.\nWainwright [Wai09] strengthened considerably these results by allowing for general scalings of s0, p, n and proving that much smaller non-zero coefficients can be detected. Namely, he proved that for a broad class of empirical covariances it is only necessary that θmin ≥ cσ √ (log p)/n. This scaling of the minimum non-zero entry is optimal up to constants. Also, for a specific classes of random Gaussian designs (including X with i.i.d. standard Gaussian entries), the analysis of [Wai09] provides tight bounds on the minimum sample size for correct model selection. Namely, there exists c`, cu > 0 such that the Lasso fails with high probability if n < c` s0 log p and succeeds with high probability if n ≥ cu s0 log p.\nWhile, thanks to these recent works [ZY06, MB06, Wai09], we understand reasonably well model selection via the Lasso, it is fundamentally unknown what model-selection performances can be achieved with general computationally practical methods. Two aspects of of the above theory cannot be improved substantially: (i) The non-zero entries must satisfy the condition θmin ≥ cσ/ √ n to be detected with high probability. Even if n = p and the measurement directions Xi are orthogonal, e.g., X = √ nIn×n, one would need |θ0,i| ≥ cσ/ √ n to distinguish the i-th entry from noise. For instance, in [JM13], the present authors prove a general upper bound on the minimax power of tests for hypotheses H0,i = {θ0,i = 0}. Specializing this bound to the case of standard Gaussian designs, the analysis of [JM13] shows formally that no test can detect θ0,i 6= 0, with a fixed degree of confidence, unless |θ0,i| ≥ cσ/ √ n. (ii) The sample size must satisfy n ≥ s0. Indeed, if this is not the case, for each θ0 with support of size |S| = s0, there is a one parameter family {θ0(t) = θ0 + t v}t∈R with supp(θ0(t)) ⊆ S, Xθ0(t) = Xθ0 and, for specific values of t, the support of θ0(t) is strictly contained in S.\nOn the other hand, there is no fundamental reason to assume the irrepresentability condition (4). This follows from the requirement that a specific method (the Lasso) succeeds, but is unclear why it should be necessary in general. The situation is very different for estimation consistency, e.g., for characterizing the `2 error ‖θ̂− θ0‖2. In that case the restricted isometry property (RIP) [CT05] (or one of its relaxations [BRT09, vdGB09]) is sufficient and –essentially– necessary.\nGauss-Lasso selector: Model selector for high dimensional problems\nInput: Measurement vector y, design model X, regularization parameter λ, support size s0. Output: Estimated support Ŝ. 1: Let T = supp(θ̂n) be the support of Lasso estimator θ̂n = θ̂n(y,X, λ) given by\nθ̂n(Y,X;λ) ≡ arg min θ∈Rp { 1 2n ‖Y −Xθ‖22 + λ‖θ‖1 } .\n2: Construct the estimator θ̂GL as follows:\nθ̂GLT = (X T TXT ) −1XTT y , θ̂ GL T c = 0 .\n3: Find s0-th largest entry (in modulus) of θ̂ GL T , denoted by θ̂ GL (s0) , and let\nŜ ≡ { i ∈ [p] : |θ̂GLi | ≥ |θ̂GL(s0)| } .\nIn this paper we prove that the Gauss-Lasso selector has nearly optimal model selection properties under a condition that is strictly weaker than irrepresentability. We call this condition the generalized irrepresentability condition (GIC). The Gauss-Lasso procedure uses the Lasso estimator to estimate a first model T ⊆ {1, . . . , p}. It then constructs a new estimator by ordinary least squares regression of the data Y onto the model T .\nWe prove that the estimated model is, with high probability, correct (i.e., Ŝ = S) under conditions comparable to the ones assumed in [MB06, ZY06, Wai09], while replacing irrepresentability by the weaker generalized irrepresentability condition. In the case of random Gaussian designs, our analysis further assumes the restricted eigenvalue property in order to establish a nearly optimal scaling of the sample size n with the sparsity parameter s0.\nIn order to build some intuition about the difference between irrepresentability and generalized irrepresentability, it is convenient to consider the Lasso cost function at ‘zero noise’:\nG(θ; ξ) ≡ 1 2n ‖X(θ − θ0)‖22 + ξ‖θ‖1\n= 1\n2 〈(θ − θ0), Σ̂(θ − θ0)〉+ ξ‖θ‖1 .\nLet θ̂ZN(ξ) be the minimizer of G( · ; ξ) and v ≡ limξ→0+ sign(θ̂ZN(ξ)). The limit is well defined by Lemma 2.2 below. The KKT conditions for θ̂ZN imply, for T ≡ supp(v),\n‖Σ̂T c,T Σ̂−1T,T vT ‖∞ ≤ 1 .\nSince G( · ; ξ) has always at least one minimizer, this condition is always satisfied. Generalized irrepresentability requires that the above inequality holds with some small slack η > 0 bounded away from zero, i.e.,\n‖Σ̂T c,T Σ̂−1T,T vT ‖∞ ≤ 1− η .\nNotice that this assumption reduces to standard irrepresentability cf. Eq. (4) if, in addition, we ask that v = sign(θ0). In other words, earlier work [MB06, ZY06, Wai09] required generalized irrepresentability plus sign-consistency in zero noise, and established sign consistency in non-zero noise. In this paper the former condition is shown to be sufficient.\nFrom a different point of view, GIC demands that irrepresentability holds for a superset of the true support S. It was indeed argued in the literature that such a relaxation of irrepresentability allows to cover a significantly broader set of cases (see for instance [BvdG11, Section 7.7.6]). However, it was never clarified why such a superset irrepresentability condition should be significantly more general than simple irrepresentability. Further, no precise prescription existed for the superset of the true support.\nOur contributions can therefore be summarized as follows:\n1. By tying it to the KKT condition for the zero-noise problem, we justify the expectation that generalized irrepresentability should hold for a broad class of design matrices.\n2. We thus provide a specific formulation of superset irrepresentability, prescribing both the superset T and the sign vector vT , that is –by itself– significantly more general than simple irrepresentability.\n3. We show that, under GIC, exact support recovery can be guaranteed using the Gauss-Lasso, and formulate the appropriate ‘minimum coefficient’ conditions that guarantee this.\nAs a side remark, even when simple irrepresentability holds, our results strengthen somewhat the estimates of [Wai09] (see below for details).\nThe paper is organized as follows. In the rest of the introduction we illustrate the range of applicability of GIC through a simple example and we discuss further related work. We finally introduce the basic notations to be used throughout the paper.\nSection 2 treats the case of deterministic designs X, and develops our main results on the basis of the GIC. Section 3 extends our analysis to the case of random designs. In this case GIC is required to hold for the population covariance, and the analysis is more technical as it requires to control the randomness of the design matrix. The proofs of our main results can be found in Sections 5 and 6, with several technical steps deferred to the Appendices."
    }, {
      "heading" : "1.1 An example",
      "text" : "In order to illustrate the range of new cases covered by our results, it is instructive to consider a simple example. A detailed discussion of this calculation can be found in Appendix B. The example corresponds to a Gaussian random design, i.e., the rows XT1 , . . .X T n are i.i.d. realizations of a pvariate normal distribution with mean zero. We write Xi = (Xi,1, Xi,2, . . . , Xi,p) T for the components of Xi. The response variable is linearly related to the first s0 covariates\nYi = θ0,1Xi,1 + θ0,2Xi,2 + · · ·+ θ0,s0Xi,s0 +Wi ,\nwhere Wi ∼ N(0, σ2) and we assume θ0,i > 0 for all i ≤ s0. In particular S = {1, . . . , s0}. As for the design matrix, first p− 1 covariates are orthogonal at the population level, i.e., Xi,j ∼ N(0, 1) are independent for 1 ≤ j ≤ p− 1 (and 1 ≤ i ≤ n). However the p-th covariate is correlated\nto the s0 relevant ones:\nXi,p = aXi,1 + aXi,2 + · · ·+ aXi,s0 + b X̃i,p .\nHere X̃i,p ∼ N(0, 1) is independent from {Xi,1, . . . , Xi,p−1} and represents the orthogonal component of the p-th covariate. We choose the coefficients a, b ≥ 0 such that s0a2 +b2 = 1, whence E{X2i,p} = 1 and hence the p-th covariate is normalized as the first (p − 1) ones. In other words, the rows of X are i.i.d. Gaussian Xi ∼ N(0,Σ) with covariance given by\nΣij =  1 if i = j,\na if i = p, j ∈ S or i ∈ S, j = p, 0 otherwise.\nFor a = 0, this is the standard i.i.d. design and irrepresentability holds. The Lasso correctly recovers the support S from n ≥ c s0 log p samples, provided θmin ≥ c′ √ (log p)/n. It follows from [Wai09] that this remains true as long as a ≤ (1 − η)/s0 for some η > 0 bounded away from 0. However, as soon as a > 1/s0, the Lasso includes the p-th covariate in the estimated model, with high probability (see Appendix B).\nAs it is shown in Appendix B, the Gauss-Lasso is successful for a significantly larger set of values of a. Namely, if\na ∈ [ 0,\n1− η s0\n] ∪ ( 1\ns0 , 1− η √ s0\n] ,\nthen it recovers S from n ≥ c s0 log p samples, provided θmin ≥ c′ √\n(log p)/n. While the interval ((1−η)/s0, 1/s0] is not covered by this result, we expect this to be due to the proof technique rather than to an intrinsic limitation of the Gauss-Lasso selector."
    }, {
      "heading" : "1.2 Further related work",
      "text" : "The restricted isometry property [CT05, CT07] (or the related restricted eigenvalue [BRT09] or compatibility conditions [vdGB09]) have been used to establish guarantees on the estimation and model selection errors of the Lasso or similar approaches. In particular, Bickel, Ritov and Tsybakov [BRT09] show that, under such conditions, with high probability,\n‖θ̂ − θ0‖22 ≤ Cσ2 s0 log p\nn .\nThe same conditions can be used to prove model-selection guarantees. In particular, Zhou [Zho10] studies a multi-step thresholding procedure whose first steps coincide with the Gauss-Lasso. While the main objective of this work is to prove high-dimensional `2 consistency with a sparse estimated model, the author also proves partial model selection guarantees. Namely, the method correctly recovers a subset of large coefficients SL ⊆ S, provided |θ0,i| ≥ cσ √ s0(log p)/n, for i ∈ SL. This means that the coefficients that are guaranteed to be detected must be a factor √ s0 larger than what is required by our results. Also related to model selection is the recent line of work on hypothesis testing in high-dimensional regression [ZZ11, Büh12]. These papers propose methods for testing hypotheses of the form H0,i =\n{θ0,i = 0}. In order to achieve a given significance level, they require –again– large coefficients, namely |θ0,i| ≥ cσ √ s0(log p)/n (see [JM13] for a discussion of this point). In [JM13], we investigate a hypothesis testing method that achieves any given significance level α for |θ0,i| ≥ cσ/ √ n, with c a constant that depends on α. Although the testing procedure can be used for general setting, the guarantee on its statistical power is provided only for some random Gaussian designs in an asymptotic sense. A very recent paper by van de Geer, Bühlmann and Ritov [vdGBR13] proposes a similar procedure and gives conditions under which the procedure achieves the semiparametric efficiency bound. Their analysis allows for general Gaussian and sub-Gaussian designs. However, it requires a sample size n ≥ C(s0 log p)2, namely the square of the optimal sample size.\nLet us finally mention that an alternative approach to establishing model-selection guarantees assumes a suitable mutual incoherence conditions. Lounici [Lou08] proves correct model selection under the assumption maxi 6=j |Σ̂ij | = O(1/s0). This assumption is however stronger than irrepresentability [vdGB09]. Candés and Plan [CP09] also assume mutual incoherence, albeit with a much weaker requirement, namely maxi 6=j |Σ̂ij | = O(1/(log p)). Under this condition, they establish model selection guarantees for an ideal scaling of the non-zero coefficients θmin ≥ cσ √ (log p)/n. However, this result only holds with high probability for a ‘random signal model’ in which the non-zero coefficients θ0,i have uniformly random signs.\nFinally, model selection consistency can be obtained without irrepresentability through other methods. For instance [Zou06] develops the adaptive Lasso, using a data-dependent weighted `1 regularization, and [Bac08] proposes the Bolasso, a resampling-based techniques. Unfortunately, both of these approaches are only guaranteed to succeed in the low-dimensional regime of p fixed, and n→∞."
    }, {
      "heading" : "1.3 Notations",
      "text" : "We provide a brief summary of the notations used throughout the paper. For a matrix A and set of indices I, J , we let AJ denote the submatrix containing just the columns in J and AI,J denote the submatrix formed by the rows in I and columns in J . Likewise, for a vector v, vI is the restriction of v to indices in I. Further, the notation A−1I,I represents the inverse of AI,I , i.e., A −1 I,I = (AI,I)\n−1. The maximum and the minimum singular values of A are respectively denoted by σmax(A) and σmin(A). We write ‖v‖p for the standard `p norm of a vector v. Specifically, ‖v‖0 denotes the number of nonzero entries in v. Also, ‖A‖p refers to the induced operator norm on a matrix A. We use ei to refer to the i-th standard basis element, e.g., e1 = (1, 0, . . . , 0). For a vector v, supp(v) represents the positions of nonzero entries of v. Throughout, we denote the rows of the design matrix X by X1, . . . , Xn ∈ Rp and denote its columns by x1, . . . , xp ∈ Rn. Further, for a vector v, sign(v) is the vector with entries sign(v)i = +1 if vi > 0, sign(v)i = −1 if vi < 0, and sign(v)i = 0 otherwise."
    }, {
      "heading" : "2 Deterministic designs",
      "text" : "An outline of this section is given below:\n1. We first consider the zero-noise problem W = 0, and prove several useful properties of the Lasso estimator in this case. In particular, we show that there exists a threshold for the regularization parameter below which the support of the Lasso estimator remains the same and contains supp(θ0). Moreover, the Lasso estimator support is not much larger than supp(θ0).\n2. We then turn to the noisy problem, and introduce the generalized irrepresentability condition (GIC) that is motivated by the properties of the Lasso in the zero-noise case. We prove that under GIC (and other technical conditions), with high probability, the signed support of the Lasso estimator is the same as that in the zero-noise problem.\n3. We show that the Gauss-Lasso selector correctly recovers the signed support of θ0."
    }, {
      "heading" : "2.1 Zero-noise problem",
      "text" : "Recall that Σ̂ ≡ (XTX/n) denotes the empirical covariance of the rows of the design matrix. Given Σ̂ ∈ Rp×p, Σ̂ 0, θ0 ∈ Rp and ξ ∈ R+, we define the zero-noise Lasso estimator as\nθ̂ZN(ξ) ≡ arg min θ∈Rp { 1 2n 〈(θ − θ0), Σ̂(θ − θ0)〉+ ξ‖θ‖1 } . (5)\nNote that θ̂ZN(ξ) is obtained by letting Y = Xθ0 in the definition of θ̂ n(Y,X; ξ).\nFollowing [BRT09], we introduce a restricted eigenvalue constant for the empirical covariance matrix Σ̂:\nκ̂(s, c0) ≡ min J⊆[p] |J |≤s\nmin u∈Rp\n‖uJc‖1≤c0‖uJ‖1\n〈u, Σ̂u〉 ‖u‖22 . (6)\nOur first result states that the support of θ̂ZN(ξ) is not much larger than the support of θ0, for any ξ > 0.\nLemma 2.1. Let θ̂ZN = θ̂ZN(ξ) be defined as per Eq. (17), with ξ > 0. Then, if s0 = ‖θ0‖0,\n‖θ̂ZN‖0 ≤ ( 1 +\n4‖Σ̂‖2 κ̂(s0, 1)\n) s0 . (7)\nThe proof of this lemma is deferred to Section A.1.\nLemma 2.2. Let θ̂ZN = θ̂ZN(ξ) be defined as per Eq. (5), with ξ > 0. Then there exist ξ0 = ξ0(Σ̂, S, θ0) > 0, T0 ⊆ [p], v0 ∈ {−1, 0,+1}p, such that the following happens. For all ξ ∈ (0, ξ0), sign(θ̂ZN(ξ)) = v0 and supp(θ̂\nZN(ξ)) = supp(v0) = T0. Further T0 ⊇ S, v0,S = sign(θ0,S) and ξ0 = mini∈S |θ0,i/[Σ̂−1T0,T0v0,T0 ]i|.\nProof of Lemma 2.2 can be found in Section A.2. Finally we have the following standard characterization of the solution of the zero-noise problem.\nLemma 2.3. Let θ̂ZN = θ̂ZN(ξ) be defined as per Eq. (5), with ξ > 0. Let T ⊇ S and v ∈ {+1, 0,−1}p be such that supp(v) = T . Then sign(θ̂ZN) = v if and only if∥∥∥Σ̂T c,T Σ̂−1T,T vT∥∥∥∞ ≤ 1 , (8)\nvT = sign ( θ0,T − ξΣ̂−1T,T vT ) . (9)\nFurther, if the above holds, θ̂ZN is given by θ̂ZNT c = 0 and\nθ̂ZNT = θ0,T − ξΣ̂−1T,T vT .\nLemma 2.3 is proved in Appendix A.3. Motivated by this result, we introduce the generalized irrepresentability condition (GIC) for\ndeterministic designs.\nGeneralized irrepresentability (deterministic designs). The pair (Σ̂, θ0), Σ̂ ∈ Rp×p, θ0 ∈ Rp satisfy the generalized irrepresentability condition with parameter η > 0 if the following happens. Let v0, T0 be defined as per Lemma 2.2. Then∥∥∥Σ̂T c0 ,T0Σ̂−1T0,T0v0,T0∥∥∥∞ ≤ 1− η . (10)\nIn other words we require the dual feasibility condition (8) –which always holds– to hold with a positive slack η."
    }, {
      "heading" : "2.2 Noisy problem",
      "text" : "Consider the noisy linear observation model as described in (2), and let r̂ ≡ (XTW/n). We begin with a standard characterization of sign(θ̂n), the signed support of the Lasso estimator (3).\nLemma 2.4. Let θ̂n = θ̂n(y,X;λ) be defined as per Eq. (3), and let z ∈ {+1, 0,−1}p with supp(z) = T . Further assume T ⊇ S. Then the signed support of the Lasso estimator is given by sign(θ̂n) = z if and only if ∥∥∥Σ̂T c,T Σ̂−1T,T zT + 1λ(r̂T c − Σ̂T c,T Σ̂−1T,T r̂T )∥∥∥∞ ≤ 1 , (11)\nzT = sign ( θ0,T − Σ̂−1T,T (λzT − r̂T ) ) . (12)\nLemma 2.4 is proved in Appendix A.4.\nTheorem 2.5. Consider the deterministic design model with empirical covariance matrix Σ̂ ≡ (XTX)/n, and assume that Σ̂i,i ≤ 1 for i ∈ [p]. Let T0 ⊆ [p], v0 ∈ {+1, 0,−1}p be the set and vector defined in Lemma 2.2, and t0 ≡ |T0|. Assume that\n(i) We have σmin(Σ̂T0,T0) ≥ Cmin > 0.\n(ii) The pair (Σ̂, θ0) satisfies the generalized irrepresentability condition with parameter η.\nConsider the Lasso estimator θ̂n = θ̂n(y,X;λ) defined as per Eq. (3), with regularization parameter\nλ = σ\nη\n√ 2c1 log p\nn , (13)\nfor some constant c1 > 1, and suppose that\n(iii) For some c2 > 0:\n|θ0,i| ≥ c2λ+ λ ∣∣[Σ̂−1T0,T0v0,T0 ]i∣∣ for all i ∈ S, (14)∣∣[Σ̂−1T0,T0v0,T0 ]i∣∣ ≥ c2 for all i ∈ T0 \\ S. (15)\nWe further assume, without loss of generality, η ≤ c2 √ Cmin. Then the following holds true:\nP { sign(θ̂n(λ)) = v0 } ≥ 1− 4p1−c1 . (16)\nTheorem 2.5 is proved in Section 5.1. Note that, even in the case standard irrepresentability holds (and hence T0 = S), this result improves over [Wai09, Theorem 1.(b)], in that the required lower bound for |θ0,i|, i ∈ S, does not depend on ‖Σ̂S,S‖∞. More precisely, Theorem 2.5 assumes |θ0,i| ≥ λ(c2 + |[Σ̂−1S,Sv0,S ]i|), for i ∈ S, which is weaker than the assumption of Theorem1.(b)[Wai09], namely, |θ0,i| ≥ λ(c+ ‖Σ̂−1S,S‖∞), since ‖v0,S‖∞ ≤ 1.\nRemark 2.6. Condition (i) in Theorem 2.5 requires the submatrix Σ̂T0,T0 to have minimum singular value bounded away form zero. Assuming Σ̂S,S to be non-singular is necessary for identifiability. Requiring the minimum singular value of Σ̂T0,T0 to be bounded away from zero is not much more restrictive since T0 is comparable in size with S, as stated in Lemma 2.1.\nWe next show that the Gauss-Lasso selector correctly recovers the support of θ0.\nTheorem 2.7. Consider the deterministic design model with empirical covariance matrix Σ̂ ≡ (XTX)/n, and assume that Σ̂i,i ≤ 1 for i ∈ [p]. Under the assumptions of Theorem 2.5,\nP ( ‖θ̂GL − θ0‖∞ ≥ µ ) ≤ 4p1−c1 + 2pe−nCminµ2/2σ2 .\nIn particular, if Ŝ is the model selected by the Gauss-Lasso, we have\nP(Ŝ = S) ≥ 1− 6 p1−c1/4 .\nThe proof of Theorem 2.7 is given in Section 5.2."
    }, {
      "heading" : "3 Random Gaussian designs",
      "text" : "In the previous section, we studied the case of deterministic design models which allowed for a straightforward analysis. Here, we consider the random design model which needs a more involved analysis. Within the random Gaussian design model, the rows Xi are distributed as Xi ∼ N(0,Σ) for some (unknown) covariance matrix Σ 0.\nIn order to study the performance of Gauss-Lasso selector in this case, we first define the population-level estimator. Given Σ ∈ Rp×p, Σ 0, θ0 ∈ Rp and ξ ∈ R+, the population-level estimator θ̂∞(ξ) = θ̂∞(ξ; θ0,Σ) is defined as\nθ̂∞(ξ) ≡ arg min θ∈Rp {1 2 〈(θ − θ0),Σ(θ − θ0)〉+ ξ‖θ‖1 } . (17)\nNotice that the minimizer is unique because Σ is strictly positive definite and hence the cost function on the right-hand side is strongly convex. In fact, the population-level estimator is obtained by assuming that the response vector Y is noiseless and n =∞, hence replacing the empirical covariance (XTX/n) with the exact covariance Σ in the lasso optimization problem (3).\nNotice that the population-level estimator θ̂∞ is deterministic, albeit X is a random design. We show that under some conditions on the covariance Σ and vector θ0, T ≡ supp(θ̂n) = supp(θ̂∞), i.e.,\nthe population-level estimator and the Lasso estimator share the same (signed) support. Further T ⊇ S. Since θ̂∞ (and hence T ) is deterministic, XT is a Gaussian matrix with rows drawn independently from N(0,ΣT,T ). This observation allows for a simple analysis of the Gauss-Lasso selector θ̂\nGL. An outline of the section is given below:\n1. We begin with proving several properties of the population-level estimator. Similar to the zero-noise problem in Section 2.1, we show that there exists a threshold ξ0, such that for all ξ ∈ (0, ξ0), supp(θ̂∞(ξ)) remains the same and contains supp(θ0). Moreover, supp(θ̂∞(ξ)) is not much larger than supp(θ0).\n2. We show that under GIC for covariance matrix Σ (and other sufficient conditions), with high probability, the signed support of the Lasso estimator is the same as the signed support of the population-level estimator.\n3. Following the previous steps, we show that the Gauss-Lasso selector correctly recovers the signed support of θ0."
    }, {
      "heading" : "3.1 The n =∞ problem",
      "text" : "In this section we derive several useful properties of the population-level problem (17). Comparing Eqs. (5) and (17), the estimators θ̂ZN(ξ) and θ̂∞(ξ) are defined in a very similar manner (the former is defined with respect to Σ̂ and the latter is defined with respect to Σ), and as we will see θ̂∞ also possesses the properties stated in Section 2.1.\nLet κ∞(s, c0) be the restricted eigenvalue constant for the covariance matrix Σ:\nκ(s, c0) ≡ min J⊆[p] |J |≤s\nmin u∈Rp\n‖uJc‖1≤c0‖uJ‖1\n〈u,Σu〉 ‖u‖22 . (18)\nThe proofs of the following Lemmas are very similar to the corresponding ones in Section 2.1, and are omitted.\nLemma 3.1. Let θ̂∞ = θ̂∞(ξ) be defined as per Eq. (17), with ξ > 0. Then, if s0 = ‖θ0‖0, ‖θ̂∞‖0 ≤ (\n1 + 4‖Σ‖2 κ(s0, 1)\n) s0 . (19)\nLemma 3.2. Let θ̂∞ = θ̂∞(ξ) be defined as per Eq. (17), with ξ > 0. Then there exist ξ0 = ξ0(Σ, S, θ0) > 0, T0 ⊆ [p], v0 ∈ {−1, 0,+1}p, such that the following happens. For all ξ ∈ (0, ξ0), sign(θ̂∞(ξ)) = v0 and supp(θ̂\n∞(ξ)) = supp(v0) = T0. Further T0 ⊇ S, v0,S = sign(θ0,S) and ξ0 = mini∈S |θ0,i/[Σ−1T0,T0v0,T0 ]i|.\nFinally we have the following standard characterization of the solution of the n = ∞ problem (17).\nLemma 3.3. Let θ̂∞ = θ̂∞(ξ) be defined as per Eq. (17), with ξ > 0. Let T ⊇ S and v ∈ {+1, 0,−1}p be such that supp(v) = T . Then sign(θ̂∞) = v if and only if∥∥∥ΣT c,TΣ−1T,T vT∥∥∥∞ ≤ 1 ,\nvT = sign ( θ0,T − ξΣ−1T,T vT ) .\nFurther, if the above holds, θ̂∞ is given by θ̂∞T c = 0 and\nθ̂∞T = θ0,T − ξΣ−1T,T vT .\nMotivated by this result, we introduce the following assumption.\nGeneralized irrepresentability (random designs). The pair (Σ, θ0), Σ ∈ Rp×p, θ0 ∈ Rp satisfy the generalized irrepresentability condition with parameter η > 0 if the following happens. Let v0, T0 be defined as per Lemma 3.2. Then∥∥∥ΣT c0 ,T0Σ−1T0,T0v0,T0∥∥∥∞ ≤ 1− η , (20)"
    }, {
      "heading" : "3.2 The high-dimensional problem",
      "text" : "We now consider the Lasso estimator (3). Recall the notations\nΣ̂ ≡ 1 n XTX , r̂ ≡ 1 n XTW .\nNote that Σ̂ ∈ Rp×p, r̂ ∈ Rp are both random quantities in the case of random designs.\nTheorem 3.4. Consider the Gaussian random design model with covariance matrix Σ 0, and assume that Σi,i ≤ 1 for i ∈ [p]. Let T0 ⊆ [p], v0 ∈ {+1, 0,−1}p be the deterministic set and vector defined in Lemma 3.2, and t0 ≡ |T0|. Assume that\n(i) We have σmin(ΣT0,T0) ≥ Cmin > 0.\n(ii) The pair (Σ, θ0) satisfies the generalized irrepresentability condition with parameter η.\nConsider the Lasso estimator θ̂n = θ̂n(y,X;λ) defined as per Eq. (3), with regularization parameter\nλ = 4σ\nη\n√ c1 log p\nn , (21)\nfor some constant c1 > 1, and suppose that\n(iii) For some c2 > 0:\n|θ0,i| ≥ c2λ+ 3 2 λ ∣∣[Σ−1T0,T0v0,T0 ]i∣∣ for all i ∈ S, (22)∣∣[Σ−1T0,T0v0,T0 ]i∣∣ ≥ 2c2 for all i ∈ T0 \\ S. (23)\nWe further assume, without loss of generality, η ≤ c2 √ Cmin.\nIf n ≥ max(M1,M3)t0 log p with\nM1 ≡ 74c1 η2Cmin , M3 ≡ 322c1 c22C 2 min ,\nthen the following holds true: P { sign(θ̂n(λ)) = v0 } ≥ 1− pe− n 10 − 6e− t0 2 − 8p1−c1 . (24)\nUnder standard irrepresentability, this result improves over [Wai09, Theorem 3.(ii)], in that the required lower bound for |θ0,i|, i ∈ S, does not depend on ‖Σ−1/2S,S ‖∞. More precisely, Theorem 2.5 assumes |θ0,i| ≥ λ(c2 + 1.5|[Σ−1S,Sv0,S ]i|), for i ∈ S, while Theorem 3.(ii)[Wai09] requires |θ0,i| ≥ cλ‖Σ−1/2S,S ‖2∞, for i ∈ S. Note that |[Σ −1 S,Sv0,S ]i| ≤ ‖Σ −1 S,S‖∞ ≤ ‖Σ −1/2 S,S ‖2∞.\nWhile being closely analogous to Theorem 2.5, the last theorem has somewhat worse constants. Indeed in the present case we need to control the randomness of the design matrix X in addition to the one of the noise.\nRemark 3.5. Condition (i) follows readily from the restricted eigenvalue constraint as in Eq. (18), i.e., κ∞(t0, 0) > 0. This is a reasonable assumption since T0 is not much larger than S0, as stated in Lemma 3.1.\nCorollary 3.6. Under the assumptions of Theorem 3.4, if n ≥ max(M̃1, M̃3)s0 log p, with\nM̃1 = ( 1 + 4‖Σ‖2\nκ∞(s0, 1)\n) M1 , M̃3 = ( 1 +\n4‖Σ‖2 κ∞(s0, 1)\n) M3 ,\nthen the following holds: P { sign(θ̂n(λ)) = v0 } ≥ 1− pe− n 10 − 6e− s0 2 − 8p1−c1 .\nProof (Corollary 3.6). The result follows readily from Theorem 3.4, noting that s0 ≤ t0 since S0 ⊆ T0, and t0 ≤ (1 + 4‖Σ‖2/κ∞(s0, 1))s0 as per Lemma 3.1.\nBelow, we show that the Gauss-Lasso selector correctly recovers the signed support of θ0.\nTheorem 3.7. Consider the random Gaussian design model with covariance matrix Σ 0, and assume that Σi,i ≤ 1 for i ∈ [p]. Under the assumptions of Theorem 3.4, and for n ≥ max(M̃1, M̃3)s0 log p, we have\nP ( ‖θ̂GL − θ0‖∞ ≥ µ ) ≤ pe− n 10 + 6e− s0 2 + 8p1−c1 + 2pe−nCminµ 2/2σ2 .\nMoreover, letting Ŝ be the model returned by the Gauss-Lasso selector, we have\nP(Ŝ = S) ≥ 1− p e− n 10 − 6 e− s0 2 − 10 p1−c1 .\nThe proof of Theorem 3.7 is deferred to Section 6.4.\nRemark 3.8. [Detection level] Let θmin ≡ mini∈S |θ0,i| be the minimum magnitude of the nonzero entries of vector θ0. By Theorem 3.7, Gauss-Lasso selector correctly recovers supp(θ0), with probability greater than 1− p e− n 10 − 6 e− s0 2 − 10 p1−c1, if n ≥ max(M̃1, M̃3)s0 log p, and\nθmin ≥ Cσ √ log p\nn\n( 1 + ‖Σ−1T0,T0‖∞ ) , (25)\nwhere C = C(c1, c2, η) is a constant depending on c1, c2, and η. Eq. (25) stems from the condition (iii) in Theorem 3.4.\nWe can further generalize this result. Define\nS1 = { i ∈ S : |θ0,i| ≥ Cσ √ log p\nn\n( 1 + ‖Σ−1T0,T0‖∞ )} ,\nand S2 = S\\S1. By a very similar argument to the proof of Theorem 3.4, the Gauss-Lasso selector can recover S1, if ‖θ0,S2‖ = O(σ √ log p/n). More precisely, letting W̃ = Xθ0,S2 + W , the response vector Y can be recast as Y = Xθ0,S1 + W̃ and the Gauss-Lasso selector treats the small entries θ0,S2 as noise."
    }, {
      "heading" : "4 UCI communities and crimes data example",
      "text" : "We consider a problem about predicting the rate of violent crimes in different communities within US, based on other demographic attributes of the communities. We evaluate the performance of the Gauss-Lasso selector on the UCI communities and crimes dataset [FA10]. The dataset consists of a univariate response variable and 122 predictive attributes for 1994 communities. The response variable is the total number of violent crimes per 100K population. Covariates are quantitative, including e.g., the average family income, the fraction of unemployed population, and the police operating budget. We consider a linear model as in (2) and perform model selection using GaussLasso selector and Lasso estimator.\nWe do the following preprocessing steps: (i) Each missing value is replaced by the mean of the non-missing values of that attribute for other communities; (ii) We eliminate 16 attributes to make the ensemble of the attribute vectors linearly independent; (iii) We normalize the columns to have mean zero and `2 norm √ n. Thus we obtain a design matrix Xtot ∈ Rntot×p with ntot = 1994 and p = 106. For the sake of performance evaluation, we need to know the true model, i.e., the true significant covariates. We let θ0 = (X T totXtot)\n−1XTtoty be the least square solution obtained from the whole dataset Xtot. The entries of θ0 are shown in Fig. 1. Clearly only a few of them are non negligible,\ncorresponding to the true model. We treat the entries with magnitude larger than 0.04 as truly active and the others as truly inactive. The number of active covariates according to this criterion is s0 = 13.\nWe choose random subsamples of size n = 85 from the communities and normalize each column of the resulting design matrix to have mean zero and `2 norm √ n. We use Gauss-Lasso selector and Lasso for model selection based on this design. Figures 2 and 3 respectively show the solution path for Gauss-Lasso and Lasso as the parameter λ changes form λ = 0.001 to λ = 1. The paths corresponding to the truly active set are in black and the paths corresponding to the truly inactive variables are in red. At λ = 1, the solutions θ̂GL(λ) and θ̂n(λ) have no active variables; for decreasing λ, each knot λk marks the entry or removal of some variables from the current active set of the Lasso solution. Therefore, the support of the Lasso solution T remains constant in between knots. Since Gauss-Lasso selector performs ordinary least squares restricted to T , its coordinate paths are constant in between knots. However, the Lasso paths are linear with respect to λ, with changes in slope at the knots (see e.g., [EHJT04] for a discussion).\nIt is clear from Figure 3 that the Lasso support either misses a large fraction of the truly active covariates, or includes many false positives. For instance at λ = 0.08, we get 4 true positives out of 13 and 4 false positives. On the other hand, for a smaller value of the regularization parameter, λ = 0.01, we get 10 true positives out of 13 and 8 false positives.1\nIf we consider on the other hand the Gauss-Lasso, any λ ≤ 0.02 produces a set of coefficients with a gap between large ones, that are mostly true positives, and small ones, that are mostly true negatives."
    }, {
      "heading" : "5 Proof of Theorems 2.5 and 2.7",
      "text" : "In this section we prove Theorems 2.5 and 2.7 using Lemmas 2.1 to 2.4. The latter are proved in the appendices."
    }, {
      "heading" : "5.1 Proof of Theorem 2.5",
      "text" : "By the condition (iii) in the statement of the theorem, we have\nλ < min i∈S ∣∣∣∣∣ θ0,i[Σ̂−1T0,T0v0,T0 ]i ∣∣∣∣∣ = ξ0 ,\nwhere the equality holds because of Lemma 2.2. By Lemma 2.2, we know that sign(θ̂ZN(λ)) = v0 and that supp(v0) = T0 contains the true support S. Applying Lemma 2.3, Eq. (9) and using the generalized irrepresentability assumption (10), we obtain∥∥∥Σ̂T0c,T0Σ̂−1T0,T0v0,T0∥∥∥∞ ≤ 1− η , (26)\nv0,T0 = sign ( θ0,T0 − λΣ̂−1T0,T0v0,T0 ) . (27)\n1We treat the entries of the Lasso solution with magnitude less than 0.005 as zero.\nGauss-Lasso\nLasso\nAlso, by Lemma 2.4, sign(θ̂n) = v0 if Eqs. (11) and (12) hold with z = v0 and T = T0, namely, if∥∥∥Σ̂T0c,T0Σ̂−1T0,T0v0,T0 + 1λ(r̂T0c − Σ̂T0c,T0Σ̂−1T0,T0 r̂T0)∥∥∥∞ ≤ 1 , (28) v0,T0 = sign ( θ0,T − Σ̂−1T0,T0(λv0,T0 − r̂T0) ) . (29)16\nIn the sequel, we show that these equations are satisfied, with probability lower bounded as per Eq. (16).\nWe begin with proving Eq. (28). Let T = (1/λ)(r̂T0c − Σ̂T0c,T0Σ̂ −1 T0,T0 r̂T0). We need to show that\n‖T ‖∞ ≤ η. Plugging for r̂, we get T ≡ XT c0 ΠX⊥T0 W/(nλ), where ΠX⊥T0 = I −XT0(XTT0XT0) −1XTT0 is the orthogonal projection onto the orthogonal complement of the column space of XT0 . Since W ∼ N(0, σ2In×n), the variable Tj = xTj ΠX⊥T0 W/(nλ) is normal with variance at most\n( σ nλ )2 ‖ΠX⊥T0 xj‖22 ≤ ( σ nλ )2 ‖xj‖22 ≤ σ2 nλ2 ,\nwhere we used the fact that ‖xj‖2 ≤ n, as Σ̂i,i ≤ 1. By the Gaussian tail bound with union bound over j ∈ T c0 , we obtain\nP(‖T ‖∞ ≤ η) ≥ 1− 2pe− nλ2η2 2σ2 = 1− 2p1−c1 . (30)\nWe next prove Eq. (29). Given Eq. (27), we need to show sign ( θ0,T0 − λΣ̂−1T0,T0v0,T0 ) = sign ( θ0,T0 − Σ̂−1T0,T0(λv0,T0 − r̂T0) ) .\nLet u ≡ θ0,T0 − λΣ̂−1T0,T0v0,T0 , and û ≡ θ0,T0 − Σ̂ −1 T0,T0 (λv0,T0 − r̂T0). By condition (iii), we have, for all i ∈ S, |ui| ≥ |θ0,i| − λ|[Σ̂−1T0,T0v0,T0 ]i| ≥ c2λ. Further, for all i ∈ T0 \\ S, we have |ui| = λ|[Σ̂−1T0,T0v0,T0 ]i| ≥ c2λ. Summarizing, for all i ∈ T0, we have |ui| ≥ c2λ. We will show that ‖u− û‖∞ = ‖Σ̂−1T0,T0 r̂T0‖∞ < c2λ, with high probability, thus implying sign(uT0) = sign(ûT0) as desired.\nLemma 5.1. The following holds true.\nP ( ‖Σ̂−1T0,T0 r̂T0‖∞ ≥ σ √ 2c1 log p n ‖Σ̂−1T0,T0‖ 1/2 2 ) ≤ 2p1−c1 . (31)\nLemma 5.1 is proved by noting that conditioned on XT0 , Σ̂ −1 T0,T0\nr̂T0 is a Gaussian vector and then applying standard tail bound inequality. The details are deferred to Section A.5.\nUsing Lemma 5.1 and the assumption η ≤ c2 √ Cmin, we get ‖u− û‖∞ < c2λ, with probability at least 1− 2p1−c1 . Putting all this together, Eqs. (28) and (29) hold simultaneously, with probability at least 1 − 4p1−c1 . This implies the thesis."
    }, {
      "heading" : "5.2 Proof of Theorem 2.7",
      "text" : "Recall that T = supp(θ̂n). On the event E ≡ {T = T0}, we have\nθ̂GLT = (X T TXT ) −1XTT (XT θ0,T +W ) = θ0,T + (X T TXT ) −1XTTW ,\nwhere the first equality holds since T = T0 ⊇ S and thus θ0,T c = 0. Further note that θ̂GLi − θ0,i, for i ∈ T , is a zero mean Gaussian vector with variance\nσ2‖eTi (XTTXT )−1XTT ‖2 ≤ σ2‖Σ̂−1T,T ‖2/n ≤ σ 2/(nCmin) .\nUsing tail bound inequality along with union bounding over i ∈ [p], we get\nP ( ‖θ̂GLT − θ0,T ‖∞ ≥ µ; E ) ≤ 2e−nCminµ2/2σ2 .\nAlso, under the assumptions of Theorem 2.5, P(E) ≥ 1− 4p1−c1 . Hence\nP ( ‖θ̂GLT − θ0,T ‖∞ ≥ µ ) ≤ P ( ‖θ̂GLT − θ0,T ‖∞ ≥ µ; E ) + P(Ec) ≤ 2e−nCminµ2/2σ2 + 4p1−c1 .\nSince θ̂GLT c = θ0,T c = 0, we get ‖θ̂GL−θ0‖∞ < µ, with probability at least 1−4p1−c1−2e−nCminµ 2/2σ2 .\nMoreover, if ‖θ̂GL − θ0‖ < θmin/2, then |θ̂GLi | > θmin/2 for i ∈ S and |θ̂GLi | < θmin/2, for i ∈ Sc. Hence, the s0 top entries of θ̂\nGL (in modulus), returned by the Gauss-Lasso selector, correspond to the true support S. Therefore,\nP(Ŝ = S) ≥ P(‖θ̂GL − θ0‖∞ < θmin/2)\n≥ 1− 4p1−c1 − 2pe−nCminθ2min/8σ2 ≥ 1− 6p1−c1/4 ,\nwhere the last inequality follows from the facts θmin ≥ c2λ, and η ≤ c2 √ Cmin."
    }, {
      "heading" : "6 Proof of Theorems 3.4 and 3.7",
      "text" : "By the condition (iii) in the statement of the theorem, we have\nλ ≤ 2 3 min i∈S ∣∣∣∣∣ θ0,i[Σ−1T0,T0v0,T0 ]i ∣∣∣∣∣ < ξ0 ,\nwhere the second inequality holds because of Lemma 3.2. Therefore, as a result of Lemma 3.2, we have sign(θ̂∞(λ)) = v0 and that supp(v0) = T0 contains the true support S. Applying Lemma 3.3 and using the generalized irrepresentability assumption, we have∥∥∥ΣT0c,T0Σ−1T0,T0v0,T0∥∥∥∞ ≤ 1− η , (32)\nv0,T0 = sign ( θ0,T0 − λΣ−1T0,T0v0,T0 ) . (33)\nMoreover, by Lemma 2.4, sign(θ̂n) = v0 if Eqs. (11) and (12) hold with z = v0 and T = T0, namely,∥∥∥Σ̂T0c,T0Σ̂−1T0,T0v0,T0 + 1λ(r̂T0c − Σ̂T0c,T0Σ̂−1T0,T0 r̂T0)∥∥∥∞ ≤ 1 , (34) v0,T0 = sign ( θ0,T − Σ̂−1T0,T0(λv0,T0 − r̂T0) ) . (35)\nThe rest of the proof is devoted to show the validity of these equations, with probability lower bounded as per Eq. (24)."
    }, {
      "heading" : "6.1 Proof of Eq. (34)",
      "text" : "It is immediate to see that Eq. (34) holds if the followings hold true:\nT1 ≡ ∥∥Σ̂T0c,T0Σ̂−1T0,T0v0,T0∥∥∞ ≤ 1− η2 , (36) T2 ≡ 1\nλ ∥∥r̂T0c − Σ̂T0c,T0Σ̂−1T0,T0 r̂T0∥∥∞ ≤ η2 . (37) In order to prove inequalities (36) and (37), it is useful to recall the following proposition from\nrandom matrix theory.\nProposition 6.1 ([DS01, Wai09, Ver12]). For k ≤ n, let X ∈ Rn×k be a random matrix with i.i.d rows drawn from N(0,Σ). Then the following hold true for all t ≥ 1 and τ ≡ 2( √ k n + t) + ( √ k n + t) 2 .\n(a) If Σ has maximum eigenvalue σmax <∞, then P ( ‖ 1 n XTX− Σ‖2 ≥ σmax τ ) ≤ 2e−nt2/2 .\n(b) If Σ has minimum eigenvalue σmin > 0, then P ( ‖( 1 n XTX)−1 − Σ−1‖2 ≥ σ−1min τ ) ≤ 2e−nt2/2 .\nWe consider the particular choice of t = √ k/n which is useful for future reference. Since k/n ≤ 1,\nwe get τ ≤ 8 √ k/n and therefore the specialized version of Proposition 6.1 reads:\nP ( ‖ 1 n XTX− Σ‖2 ≥ 8 √ k/nσmax ) ≤ 2e−k/2 , (38)\nP ( ‖( 1 n XTX)−1 − Σ−1‖2 ≥ 8 √ k/nσ−1min ) ≤ 2e−k/2 . (39)\nWe define the event E1 as E1 ≡ { ‖(Σ̂T0,T0)−1 − Σ−1T0,T0‖2 ≤ 8 √ t0/nC −1 min } .\nApplying Eqs. (38), (39) to XT0 , we conclude that\nP(Ec1) ≤ 2e−t0/2 . (40)\nWe now have in place all we need to bound the terms T1 and T2."
    }, {
      "heading" : "6.1.1 Bounding T1",
      "text" : "To bound T1, we employ similar techniques to those used in [Wai09, Theorem 3] to verify strict dual feasibility. The argument in [Wai09] works under the irrepresentability condition (see Eq. (26) therein) and we modify it to apply to the current setting, i.e., the generalized irrepresentability condition.\nWe begin by conditioning on XT0 . For j ∈ T0c, xj is a zero mean Gaussian vector and we can decompose it into a linear correlated part plus an uncorrelated part as\nxTj = Σj,T0Σ −1 T0,T0 XTT0 + T j ,\nwhere j ∈ Rn has i.i.d. entries distributed as ji ∼ N(0,Σj,j − Σj,T0Σ−1T0,T0ΣT0,j). Letting u = Σ̂T0c,T0Σ̂ −1 T0,T0 v0,T0 , we write\nuj = x T j XT0(X T T0XT0) −1v0,T0\n= Σj,T0(ΣT0,T0) −1v0,T0 + T j XT0(X T T0XT0) −1v0,T0 . (41)\nThe first term is bounded as |Σj,T0(ΣT0,T0)−1v0,T0 | ≤ 1−η as per Eq. (32). Letmj = Tj XT0(XTT0XT0) −1v0,T0 . Since Var( ji) ≤ Σj,j ≤ 1, conditioned on XT0 , mj is zero mean Gaussian with variance at most\nVar(mj) ≤ ‖XT0(XTT0XT0) −1v0,T0‖22\n≤ 1 n vT0,T0 (XTT0XT0 n )−1 v0,T0 ≤ 1 n ‖Σ̂−1T0,T0‖2 ‖v0,T0‖ 2 . (42)\nUnder the event E1, we have\n‖Σ̂−1T0,T0‖2 ≤ ‖Σ −1 T0,T0 ‖2 + ‖Σ̂−1T0,T0 − Σ −1 T0,T0 ‖2 ≤ (1 + 8 √ t0/n)C −1 min ≤ 9C −1 min , (43)\nand hence, Var(mj) ≤ 9t0/(nCmin). We now define the event E as\nE ≡ {\nmax j∈T c |mj | ≥\n√ 18c1 t0 log p\nnCmin\n} .\nBy the total probability rule, we have\nP(E) ≤ P(E ; E1) + P(Ec1) .\nUsing Gaussian tail bound and union bounding over j ∈ T0c, we obtain P(E ; E1) ≤ 2p1−c1 . Using the bound P(Ec1) ≤ 2e−t0/2, we arrive at:\nP ( max j∈T c |mj | > √ 18c1 t0 log p\nnCmin\n) ≤ 2p1−c1 + 2e− t0 2 . (44)\nUsing this, together with Eq. (32), in Eq. (41), we obtain that the following holds true with probability at least 1− 2p1−c1 − 2e−t0/2:\nT1 ≤ 1− η + √ 18c1 t0 log p\nnCmin . (45)\nIt is easy to check that the this implies T1 < 1 − η/2, for λ as claimed in Eq. (21) provided n ≥ M1t0 log p."
    }, {
      "heading" : "6.1.2 Bounding T2",
      "text" : "We bound T2 by the same technique used in proving Eq. (28). Let m = (1/λ)(r̂T0c−Σ̂T0c,T0Σ̂ −1 T0,T0 r̂T0). Plugging for r̂, we get m ≡ XT c0 ΠX⊥T0 W/(nλ). Since W ∼ N(0, σ2In×n), conditioned on X, the variable mj = x T j ΠX⊥T0 W/(nλ) is normal with variance at most\n( σ\nnλ )2‖ΠX⊥T0\nxj‖22 ≤ ( σ\nnλ )2‖xj‖2 ,\nwhere we used the contraction property of orthogonal projections. Now, define the event E as follows. E ≡ { ‖xj‖2 < 2n,∀j ∈ [p] } .\nNote that ‖xj‖2 d = Σj,jZ, where Z is a chi-squared random variable with n degrees of freedom. Using the standard chi-squared tail bounds [Joh01], for a fixed j, we have ‖xj‖2 < 2Σj,j n ≤ 2n, with probability at least 1− e−n/10. Union bounding over j ∈ [p], we obtain P(Ec) ≤ pe−n/10.\nUnder the event E , we have Var(mj) ≤ 2σ2/(nλ2). Employing the standard Gaussian tail bound along with union bounding over j ∈ T c0 , we obtain\nP(T2 ≥ η/2; E) ≤ 2pe− nλ2η2 16σ2 = 2p1−c1 . (46)\nHence,\nP(T2 ≥ η/2) ≤ P(T2 ≥ η/2; E) + P(Ec) ≤ 2p1−c1 + pe− n 10 . (47)"
    }, {
      "heading" : "6.2 Proof of Eq. (35)",
      "text" : "We next prove Eq. (35). Given Eq. (33), we need to show sign ( θ0,T0 − λΣ−1T0,T0v0,T0 ) = sign ( θ0,T0 − Σ̂−1T0,T0(λv0,T0 − r̂T0) ) .\nLet u ≡ θ0,T0 − λΣ−1T0,T0v0,T0 , and û ≡ θ0,T0 − Σ̂ −1 T0,T0 (λv0,T0 − r̂T0). By condition (iii), we have, for all i ∈ S, |ui| ≥ |θ0,i|−λ|[Σ−1T0,T0v0,T0 ]i| ≥ c2λ+(1/2)λ|[Σ −1 T0,T0\nv0,T0 ]i|. Further, for all i ∈ T0\\S, we have |ui| = λ|[Σ−1T0,T0v0,T0 ]i| ≥ c2λ+(1/2)λ|[Σ −1 T0,T0\nv0,T0 ]i|. Summarizing, for all i ∈ T0, we have\n|ui| ≥ c2λ+ 1 2 λ|[Σ−1T0,T0v0,T0 ]i| .\nWe will show that |ui − ûi| < c2λ + (1/2)λ|[Σ−1T0,T0v0,T0 ]i| for all i ∈ T0, with high probability, thus implying sign(uT0) = sign(ûT0) as desired. Since |ui− ûi| ≤ λ|[(Σ̂−1T0,T0−Σ −1 T0,T0\n)v0,T0 ]i|+ |[Σ̂−1T0,T0 r̂T0 ]i|, it suffices to show that\nT3(i) ≡ λ|[(Σ̂−1T0,T0 − Σ −1 T0,T0 )v0,T0 ]i ∣∣ < 1 2 λ|[Σ−1T0,T0v0,T0 ]i| for all i ∈ T0, (48) T4 ≡ ‖Σ̂−1T0,T0 r̂T0‖∞ < c2λ . (49)\nIn the sequel, we provide probabilistic bounds on T3(i) and T4.\n6.2.1 Bounding T3(i)\nLemma 6.2. Under the assumptions of Theorem 3.4, for any c′ > 1, t0 ≥ 4, we have\nP { ∃i ∈ T0 s.t. ∣∣[(Σ̂−1T0,T0 − Σ−1T0,T0)v0,T0 ]i∣∣ ≥ 16 √ c′c∗ t0 log p\nn\n∣∣[Σ−1T0,T0v0,T0 ]i∣∣ } ≤ 2e− t0 2 + 2p1−c ′ ,\nwhere c∗ ≡ (c2Cmin)−2.\nThe proof of Lemma 6.2 is presented in Section A.6. Applying this lemma, with probability at least 1−2e−t0/2−2p1−c1 , we have T3(i) < (1/2)λ|[Σ−1T0,T0v0,T0 ]i|\nprovided\n16\n√ c1c∗ t0 log p\nn ≤ 1 2 .\ni.e., for n ≥M3t0 log p."
    }, {
      "heading" : "6.2.2 Bounding T4",
      "text" : "Lemma 6.3. The following holds true.\nP ( T4 ≤ 3σ √ 2c1 log p\nnCmin\n) ≥ 1− 2e− t0 2 − 2p1−c1 . (50)\nLemma 6.3 is proved in Section A.7. From the last lemma, it follows that Eq. (49) holds with probability at least 1− 2e− t0 2 − 2p1−c1 ,\nprovided\n3σ\n√ 2c1 log p\nnCmin ≤ c2λ .\nChoosing λ as per Eq. (21), the latter is easily shown to follow from η ≤ c2 √ Cmin."
    }, {
      "heading" : "6.3 Summary: Proof of Theorem 3.4",
      "text" : "Now combining the bounds on T1,. . . T4, we get that for n ≥ max(M1,M3) t0 log p, Eqs. (34) and (35) hold simultaneously, with probability at least 1−pe−n/10−6e−t0/2−8p1−c1 . This implies sign(θ̂n(λ)) = v0."
    }, {
      "heading" : "6.4 Proof of Theorem 3.7",
      "text" : "Note that the matrix XT0 is a random Gaussian matrix with rows drawn independently form N(0,ΣT0,T0) (recall that T0 is a deterministic set determined by the population-level problem). Therefore, ‖Σ̂−1T0,T0‖2 ≤ 9‖Σ −1 T0,T0 ‖2 ≤ 9C−1min. Using Theorem 3.4 to bound the probability that T 6= T0, the proof proceeds along the same lines as the proof of Theorem 2.7."
    }, {
      "heading" : "Acknowledgements",
      "text" : "A.J. is supported by a Caroline and Fabian Pease Stanford Graduate Fellowship. This work was partially supported by the NSF CAREER award CCF-0743978, the NSF grant DMS-0806211, and the grants AFOSR/DARPA FA9550-12-1-0411 and FA9550-13-1-0036."
    }, {
      "heading" : "A Proof of technical lemmas",
      "text" : "A.1 Proof of Lemma 2.1\nBy a change of variables, it is easy to see that θ̂ZN(ξ) = θ0 +ξ û(ξ), where û(ξ) = arg minu∈Rp F (u; ξ) and\nF (u; ξ) ≡ 1 2 〈u, Σ̂u〉+ ‖uSc‖1 +\n( ‖ξ−1θ0,S + uS‖1 − ‖ξ−1θ0,S‖1 ) .\nThe rest of the proof is analogous to an argument in [BRT09]. Since, by definition, F (û; ξ) ≤ F (0; ξ), we have\n1 2 〈û, Σ̂û〉+ ‖ûSc‖1 − ‖ûS‖1 ≤ 0 (51)\nand hence ‖ûSc‖1 ≤ ‖ûS‖1. Using the definition of κ̂, with J = S, c0 = 1, we have\n0 ≥ 1 2 κ̂(s0, 1)‖û‖22 + ‖ûSc‖1 − ‖ûS‖1\n≥ 1 2 κ̂(s0, 1)‖ûS‖22 − ‖ûS‖1 ,\nand since ‖ûS‖22 ≥ ‖ûS‖21/s0, we deduce that\n‖ûS‖1 ≤ 2s0\nκ̂(s0, 1) .\nBy Eq. (51), this implies in turn\n〈û, Σ̂û〉 ≤ 4s0 κ̂(s0, 1) . (52)\nNow, consider the stationarity conditions of F . These imply\n(Σ̂û)i = −sign(ûi) , for i ∈ T \\ S.\nWe therefore have\n|T \\ S| ≤ ∑ i∈T\\S (Σ̂û)2i ≤ ‖Σ̂û‖22 ≤ ‖Σ̂‖2〈û, Σ̂û〉 ,\nand our claim follows by substituting Eq. (52) in the latter equation.\nA.2 Proof of Lemma 2.2\nBy a change of variables, it is easy to see that θ̂ZN(ξ) = θ0 +ξ û(ξ), where û(ξ) = arg minu∈Rp F (u; ξ) and\nF (u; ξ) ≡ 1 2 〈u, Σ̂u〉+ ‖uSc‖1 +\n( ‖ξ−1θ0,S + uS‖1 − ‖ξ−1θ0,S‖1 ) .\nNotice that, for any u ∈ Rp, limξ→0 F (u; ξ) = F0(u), where\nF0(u) ≡ 1\n2 〈u, Σ̂u〉+ ‖uSc‖1 + 〈sign(θ0,S), uS〉 .\nIndeed F (u; ξ) = F0(u) provided ξ ≤ mini∈S |θ0,i/ui|. Further, F (u; ξ) ≥ F0(u) for all u. Let u0 ≡ arg minu∈Rp F0(u), and set ξ0 ≡ mini∈S |θ0,i/u0,i|. Then, for any u 6= u0, and all ξ ∈ (0, ξ0), we have\nF (u; ξ) ≥ F0(u) > F0(u0) = F (u0; ξ) .\nHence u0 is the unique minimizer of F (u; ξ), i.e., û(ξ) = u0 for all ξ ∈ (0, ξ0). It follows that θ̂ZN(ξ) = θ0+ξ u0 for all ξ ∈ (0, ξ0) and hence sign(θ̂ZN(ξ)) = v0 and supp(θ̂ZN(ξ)) = T0 where we set\nv0,S ≡ sign(θ0,S) , v0,Sc ≡ sign(u0,Sc) , T0 ≡ S ∪ supp(u0) .\nFinally, the zero subgradient condition for u0 reads Σ̂u0 + z = 0, with zS = sign(θ0,S) and zSc ∈ ∂‖u0,Sc‖1. In particular, zT0 = v0,T0 and therefore u0,T0 = −Σ̂−1T0,T0vT0 . This implies\nξ0 ≡ min i∈S ∣∣∣∣ θ0,iu0,i ∣∣∣∣ = mini∈S ∣∣∣∣∣ θ0,i[Σ̂−1T0,T0v0,T0 ]i ∣∣∣∣∣ .\nA.3 Proof of Lemma 2.3\nWriting the zero-subgradient conditions for problem (5), we have\nΣ̂(θ̂ZN − θ0) = −ξu, u ∈ ∂‖θ̂ZN‖1.\nGiven that T ⊇ S, we have θ0,T c = 0, and thus\nΣ̂T,T (θ̂ ZN T − θ0,T ) = −ξuT ,\nΣ̂T c,T (θ̂ ZN T − θ0,T ) = −ξuT c .\nSolving for θ̂ZNT − θ0,T in terms of uT , we obtain\nΣ̂T c,T Σ̂ −1 T,TuT = uT c , θ̂ZNT = θ0,T − ξΣ̂−1T,TuT .\nThis proves the ‘only if’ part noting that uT = sign(θ̂ ZN T ) = vT , and ‖uT c‖∞ ≤ 1 since u ∈ ∂‖θ̂ZN‖1.\nNow suppose that Eqs. (8) and (9) hold true. Let θ̃T = θ0,T − ξΣ̂−1T,T vT , and θ̃T c = 0. We prove that θ̃ = θ̂ZN, by showing that it satisfies the zero-subgradient condition. By Eq. (9), vT = sign(θ̃T ). Define u ∈ Rp by letting uT = vT and uT c = Σ̂T c,T Σ̂ −1 T,T vT . Note that ‖uT c‖∞ ≤ 1 by Eq. (8), and so u ∈ ∂‖θ̃‖1. Moreover,\nΣ̂T,T (θ̃T − θ0,T ) = −ξuT Σ̂T c,T (θ̃T − θ0,T ) = −ξuT c ,\nCombining the above two equations, we get the zero-subgradient condition for (θ̃, u). Therefore, θ̃ = θ̂ZN, and v = sign(θ̂ZN).\nA.4 Proof of Lemma 2.4\nThe proof proceeds along the same lines as the proof of Lemma 2.3. We begin with proving the ‘only if’ part. The zero-subgradient condition for Problem 3 reads:\n− 1 n XT(Y −Xθ̂n) + λu = 0 , u ∈ ∂‖θ̂n‖1 .\nPlugging for Y = Xθ0 +W and r̂ = (X TW/n) in the above equation, we arrive at:\nΣ̂(θ̂n − θ0) = r̂ − λu .\nSince T ⊇ S, θ0,T c = 0, and writing the above equation for indices in T and T c separately, we obtain\nΣ̂T c,T (θ̂ n T − θ0,T ) = r̂T c − λuT c ,\nΣ̂T,T (θ̂ n T − θ0,T ) = r̂T − λuT .\nSolving for θ̂nT − θ0,T from the second equation, we get\nΣ̂T c,T Σ̂ −1 T,TuT +\n1 λ (r̂T c − Σ̂T c,T Σ̂−1T,T r̂T ) = uT c ,\nθ̂nT = θ0,T − Σ̂−1T,T (λuT − r̂T ) .\nThis proves Eqs. (11) and (12), since uT = sign(θ̂ n T ) = zT and ‖uT c‖∞ ≤ 1.\nWe next prove the other direction. Suppose that Eqs. (11) and (12) hold true. Let θ̃T = θ0,T − Σ̂−1T,T (λzT − r̂T ), and θ̃T c = 0. We prove that θ̃ = θ̂n, by showing that it satisfies the zero-subgradient condition. By Eq. (12), zT = sign(θ̃T ). Define u ∈ Rp by letting uT = zT and uT c = Σ̂T c,T Σ̂ −1 T,T zT + (r̂T c − Σ̂T c,T Σ̂ −1 T,T r̂T )/λ. Note that ‖uT c‖∞ ≤ 1 by Eq. (12), and so u ∈ ∂‖θ̃‖1. Moreover,\nΣ̂T,T (θ̃T − θ0,T ) = −(λuT − r̂T )\nΣ̂T c,T (θ̃T − θ0,T ) = −(λuT c − r̂T c) ,\nCombining the above two equations, we get the zero-subgradeint condition for (θ̃, u). Therefore, θ̃ = θ̂n, and z = sign(θ̂n).\nA.5 Proof of Lemma 5.1\nLet m = Σ̂−1T0,T0 r̂T0 = (X T T0 XT0) −1XTT0W . Conditioned on XT0 , mi is a zero mean Gaussian vector with variance σ2‖eTi (XT0XT0)−1XTT0‖ 2. By a Gaussian tail bound, we get\nP ( |mi| ≥ √ 2c1 log p σ‖eTi (XTT0XT0) −1XTT0‖ ) ≤ 2p−c1 .\nFurther, notice that ‖eTi (XTT0XT0) −1XTT0‖ 2 ≤ ‖Σ̂−1T0,T0‖2/n. By union bounding over i = 1, . . . , p, we have\nP ( ‖m‖∞ ≥ σ √ 2c1 log p\nn ‖Σ̂−1T0,T0‖ 1/2 2\n) ≤ 2p1−c1 .\nA.6 Proof of Lemma 6.2\nWe begin by stating and proving a lemma that is similar to Lemma 5 in [Wai09], but provides a stronger control.\nLemma A.1. Let Z ∈ Rn×k be a random matrix with i.i.d. Gaussian rows with zero mean and covariance Σ, with k ≥ 4. Further let a1, . . . , aM ∈ Rk and b1, . . . , bM ∈ Rk be non-random vectors. Then, letting Σ̂Z ≡ ZTZ/n, we have, for all ∆ > 0:\nP { ∃i ∈ [M ] s.t. ∣∣∣〈ai, (Σ̂−1Z − Σ−1)bi〉∣∣∣ ≥ 8 √ k\nn |〈ai,Σ−1bi〉|+ ∆ ‖Σ−1/2ai‖2‖Σ−1/2bi‖2\n}\n≤ 2e− k 2 + 2M exp { − n∆ 2\n256\n} . (53)\nProof. First notice that Z = Z̃Σ1/2 with Z̃ ∈ Rn×k a random matrix with i.i.d. standard Gaussian entries Zij ∼ N(0, 1). By substituting in the statement of the theorem, it is easy to check that we only need to prove our claim in the case Σ = Ik×k (i.e., for Z with i.i.d. entries), which we shall assume hereafter.\nDefining the event E∗ = {‖Σ̂−1 − I‖2 ≤ 8 √ k/n}, we have, by Eq. (39) and the union bound,\nP { ∃i ∈ [M ] s.t. ∣∣∣〈ai, (Σ̂−1 − I)bi〉∣∣∣ ≥ 8√k n |〈ai, bi〉|+ ∆ ‖ai‖2‖bi‖2 } ≤\n2 e−k/2 +M max i∈[M ] P {∣∣〈ai, (Σ̂−1 − I)bi〉∣∣ ≥ 8√k n |〈ai, bi〉|+ ∆ ‖ai‖2‖bi‖2; E∗ }\nWe can now concentrate on the last probability. Let α ≡ |〈ai, bi〉| and β ≡ (‖ai‖22‖bi‖22−〈ai, bi〉2)1/2. Since Σ̂ is distributed as RΣ̂RT for any orthogonal matrix R, we have\n〈ai, (Σ̂−1 − I)bi〉 d = α〈e1, (Σ̂−1 − I)e1〉+ β〈e1, (Σ̂−1 − I)e2〉 ,\nwhere d = denotes equality in distribution. Under the event E∗, we have |α〈e1, (Σ̂−1−I)e1〉| ≤ 8α √ k/n. Further (Σ̂−1 − I) = UDUT with U a uniformly random orthogonal matrix (with respect to Haar\nmeasure on the manifold of orthogonal matrices). Letting u1, u2 denote the first two rows of U we then have\nP {∣∣〈ai, (Σ̂−1 − I)bi〉∣∣ ≥ 8√k n |〈ai, bi〉|+ ∆ ‖ai‖2‖bi‖2; E∗ } ≤ P{|〈u1, Du2〉| ≥ ∆; E∗} .\nNotice that conditioned on u2 and D, u1 is uniformly random on a (k − 1)-dimensional sphere. Further, letting v2 = Du2, we have ‖v2‖2 ≤ 8 √ k/n. Hence, by isoperimetric inequalities on the sphere [Led01], we obtain\nP{|〈u1, Du2〉| ≥ ∆; E∗} ≤ sup ‖v2‖≤8 √ k/n P{|〈u1, v2〉| ≥ ∆| v2}\n≤ 2 exp { − (k − 2)∆ 2\n128k/n\n} ≤ 2 exp { − n∆ 2\n256\n} ,\nwhere the last inequality holds for all k ≥ 4. The proof is completed by substituting this inequality in the expressions above.\nWe are now in position to prove Lemma 6.2.\nProof (Lemma 6.2). We apply Lemma A.1 to Σ̂ = Σ̂T0,T0 , M = t0, ai = ei and bi = v0,T0 for i ∈ {1, . . . , t0}. We get\nP { ∃i ∈ T0 s.t. ∣∣[(Σ̂−1T0,T0 − Σ−1T0,T0)v0,T0 ]i∣∣ ≥ 8 √ t0 n ∣∣[Σ−1T0,T0v0,T0 ]i∣∣+ ∆‖Σ−1/2T0,T0ei‖2‖Σ−1/2T0,T0v0,T0‖2 } ≤\n2 e−t0/2 + 2t0 exp { − n∆ 2\n256\n} .\nNote that ‖Σ−1/2T0,T0ei‖2‖Σ −1/2 T0,T0 v0,T0‖2 ≤ C−1min‖ei‖2‖v0,T0‖2 = C −1 min √ t0. Further |[Σ−1T0,T0v0,T0 ]i ∣∣ ≥ 2c2, and hence ‖Σ−1/2T0,T0ei‖2‖Σ −1/2 T0,T0 v0,T0‖2 ≤ (1/2) √ c∗t0 |[Σ−1T0,T0v0,T0 ]i\n∣∣. We therefore get P { ∃i ∈ T0 s.t. ∣∣[(Σ̂−1T0,T0 − Σ−1T0,T0)v0,T0 ]i∣∣ ≥ (8 √ t0 n + ∆ 2 √ c∗t0 )∣∣[Σ−1T0,T0v0,T0 ]i∣∣ } ≤\n2 e−t0/2 + 2t0 exp { − n∆ 2\n256\n} .\nThe proof is completed by taking ∆ = 16 √ (c′ log p)/n.\nA.7 Proof of Lemma 6.3\nBy Lemma 5.1, we have\nP ( ‖Σ̂−1T0,T0 r̂T0‖∞ ≥ σ √ 2c1 log p n ‖Σ̂−1T0,T0‖ 1/2 2 ) ≤ 2p1−c1 .\nRecalling Eq. (43), under the event E1 we have ‖Σ̂−1T0,T0‖2 ≤ 9C −1 min. Since P(Ec1) ≤ 2e−t0/2, we arrive at:\nP ( ‖Σ̂−1T0,T0 r̂T0‖∞ ≥ 3σ √ 2c1 log p\nnCmin\n) ≤ 2p1−c1 + 2e− t0 2 ."
    }, {
      "heading" : "B Generalized irrepresentability vs. irrepresentability",
      "text" : "In this appendix we discuss the example provided in Section 1.1 in more details. The objective is to develop some intuition on the domain of validity of generalized irrepresentability, and compare it with the standard irrepresentability condition.\nAs explained in Section 1.1, let S = supp(θ0) = {1, . . . , s0} and consider the following covariance matrix:\nΣij =  1 if i = j,\na if i = p, j ∈ S or i ∈ S, j = p, 0 otherwise.\nEquivalently,\nΣ = Ip×p + a ( epu T S + uSe T p ) ,\nwhere uS is the vector with entries (uS)i = 1 for i ∈ S and (uS)i = 0 for i 6∈ S. It is easy to check that Σ is strictly positive definite for a ∈ (−1/√s0,+1/ √ s0). By redefining the p-th covariate, we can assume, without loss of generality, a ∈ [0,+1/√s0). We will further assume sign(θ0,i) = +1 for all i ∈ S.\nThis example captures the case of a single confounding variable, i.e., of an irrelevant covariate that correlates strongly with the relevant covariates, and with the response variable.\nWe will show that the Gauss-Lasso has a significantly broader domain of validity with respect to the simple Lasso.\nClaim B.1. Consider the Gaussian design defined above, and suppose that a > 1/s0. Then for any regularization parameter λ and for any sample size n, the probability of correct signed support recovery with Lasso is at most 1/2. (and is not guaranteed with high probability unless a ∈ [0, (1− η)/s0], for some constant η > 0.\nOn the other hand, Theorem 3.7 implies correct support recovery with the Gauss-Lasso from n = Ω(s0 log p) samples, for any\na ∈ [ 0,\n1− η s0\n] ∪ ( 1\ns0 , 1− η √ s0\n] . (54)\nProof. In order to prove that Gauss-Lasso correctly recovers the support of θ0, we will show that all the conditions of Theorem 3.4 and Theorem 3.7 hold with constants of order one, provided Eq. (54) holds. Vice versa, the irrepresentability condition does not hold unless a ∈ [0, 1/s0), and hence the simple Lasso fails outside this regime.\nWe now proceed to check the assumptions of Theorems 3.4 and 3.7, while showing that irrepresentability does not hold for a ≥ 1/s0. Restricted eigenvalues. We have λmin(Σ) = 1− a √ s0. In particular, for any set T ⊆ [p], we have λmin(ΣT,T ) ≥ 1− a √ s0 ≥ η. Also, for any constant c0 ≥ 0, κ(s0, c0) ≥ 1− a √ s0 ≥ η. Irrepresentability condition. We have ΣSS = Is0×s0 and hence ‖ΣScSΣ−1SS‖∞ = ‖Σp,S‖1 = as0. Hence the irrepresentability condition holds only if a ∈ [0, 1/s0). The corresponding irrepresentability parameter is η = 1− as0.\nFor large s0, the condition is only satisfied for a small interval in a, compared to the interval for which Σ is positive definite.\nGeneralized irrepresentability condition. In order to check this condition, we need to compute T0 and v0 defined as per Lemma 3.2. We have θ̂ ∞(ξ) = arg minθ∈Rp G(θ; ξ) where\nG(θ; ξ) ≡ 1 2 〈(θ − θ0),Σ(θ − θ0)〉+ ξ‖θ‖1\n= 1\n2 ‖θ − θ0‖22 + a〈uS , (θS − θ0,S)〉θp + ξ‖θ‖1 .\nFrom this expression, it is immediate to see that θ̂∞i (ξ) = 0 for i 6∈ S∪{p}. Further θ̂∞S∪{p}(ξ) satisfies\nθS − θ0,S + aθpuS + ξvS = 0 , (55) θp + a〈uS , (θS − θ0,S)〉+ ξvp = 0 , (56)\nwith vS ∈ ∂‖θS‖1 and vp ∈ ∂|θp|. Since θ0,S > 0, we have, from Eq. (55),\nθ̂∞S = θ0,S − (aθ̂∞p + ξ)uS ,\nprovided (aθ̂∞p + ξ) ≤ θmin. Substituting in Eq. (56) and solving for θp, we get\nθ̂∞p (ξ) = { 0 if a ∈ [0, 1/s0)( as0−1 1−a2s0 ) ξ if a ∈ [1/s0, 1/ √ s0).\nThis holds provided (aθ̂∞p + ξ) ≤ θmin, i.e., if ξ ≤ ξ∗ ≡ min(1, (1− a2s0)/(1− a)) θmin. Using the definition in Lemma 3.2, we have\nT0 = { S if a ∈ [0, 1/s0) S ∪ {p} if a ∈ [1/s0, 1/ √ s0),\nand v0,T0 = uT0 . We can now check the generalized irrepresentability condition. For a ∈ [0, 1/s0) we have ‖ΣT c0 ,T0Σ −1 T0,T0\nv0,T0‖∞ = ‖ΣSc,SΣ−1S,SuS‖∞ = as0, and therefore the generalized irrepresentability condition is satisfied with parameter η = 1−as0. For a ∈ [1/s0, 1/ √ s0), we have ‖ΣT c0 ,T0Σ −1 T0,T0\nv0,T0‖∞ = 0.\nWe therefore conclude that, for any fixed η ∈ (0, 1], the generalized irrepresentability condition with parameter η is satisfied for\na ∈ [ 0,\n1− η s0 ] ∪ [ 1 s0 , 1 √ s0 ) ,\na significant larger domain than for simple irrepresentability.\nMinimum entry condition. For a ∈ [0, 1/s0), we have T0 = S and it is therefore only necessary to check Eq. (22). Since [Σ−1T0,T0v0,T0 ]i = 1, this reads\n|θ0,i| ≥ ( c2 + 3\n2\n) λ = Cσ √ log p\nn ,\nwith C a constant. For a ∈ (1/s0, (1− η)/\n√ s0], we have T0 = S ∪ {p}. A straightforward calculation shows that∣∣[Σ−1T0,T0v0,T0 ]i∣∣ = 1− a1− a2s0 , for i ∈ S ,∣∣[Σ−1T0,T0v0,T0 ]p∣∣ = as0 − 11− a2s0 .\nIt is not hard to show for all a satisfying Eq. (54), we have∣∣[Σ−1T0,T0v0,T0 ]i∣∣ ≤ 11− (1− η)2 for i ∈ S, ∣∣[Σ−1T0,T0v0,T0 ]p∣∣ ≥ C , for some constant C > 0. It therefore follows that condition (22) holds if |θ0,i| ≥ C ′σ √ log p/n and condition (23) holds for c2 = C/2."
    } ],
    "references" : [ {
      "title" : "Bolasso: model consistent lasso estimation through the bootstrap",
      "author" : [ "Francis R Bach" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Bach,? \\Q2008\\E",
      "shortCiteRegEx" : "Bach",
      "year" : 2008
    }, {
      "title" : "Statistical significance in high-dimensional linear models, arXiv:1202.1377",
      "author" : [ "P. Bühlmann" ],
      "venue" : null,
      "citeRegEx" : "Bühlmann,? \\Q2012\\E",
      "shortCiteRegEx" : "Bühlmann",
      "year" : 2012
    }, {
      "title" : "Statistics for high-dimensional data, SpringerVerlag",
      "author" : [ "Peter Bühlmann", "Sara van de Geer" ],
      "venue" : null,
      "citeRegEx" : "Bühlmann and Geer,? \\Q2011\\E",
      "shortCiteRegEx" : "Bühlmann and Geer",
      "year" : 2011
    }, {
      "title" : "Examples of basis pursuit",
      "author" : [ "S.S. Chen", "D.L. Donoho" ],
      "venue" : "Proceedings of Wavelet Applications in Signal and Image Processing III (San Diego, CA),",
      "citeRegEx" : "Chen and Donoho,? \\Q1995\\E",
      "shortCiteRegEx" : "Chen and Donoho",
      "year" : 1995
    }, {
      "title" : "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information",
      "author" : [ "E. Candes", "J.K. Romberg", "T. Tao" ],
      "venue" : "IEEE Trans. on Inform. Theory",
      "citeRegEx" : "Candes et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Candes et al\\.",
      "year" : 2006
    }, {
      "title" : "Decoding by linear programming",
      "author" : [ "E.J. Candés", "T. Tao" ],
      "venue" : "IEEE Trans. on Inform. Theory",
      "citeRegEx" : "Candés and Tao,? \\Q2005\\E",
      "shortCiteRegEx" : "Candés and Tao",
      "year" : 2005
    }, {
      "title" : "The Dantzig selector: statistical estimation when p is much larger than n",
      "author" : [ "E. Candés", "T. Tao" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "Candés and Tao,? \\Q2007\\E",
      "shortCiteRegEx" : "Candés and Tao",
      "year" : 2007
    }, {
      "title" : "Local operator theory, random matrices and Banach spaces",
      "author" : [ "K.R. Davidson", "S.J. Szarek" ],
      "venue" : "Handbook on the Geometry of Banach spaces,",
      "citeRegEx" : "Davidson and Szarek,? \\Q2001\\E",
      "shortCiteRegEx" : "Davidson and Szarek",
      "year" : 2001
    }, {
      "title" : "Least angle regression",
      "author" : [ "Bradley Efron", "Trevor Hastie", "Iain Johnstone", "Robert Tibshirani" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "Efron et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Efron et al\\.",
      "year" : 2004
    }, {
      "title" : "UCI machine learning repository (communities and crime data set), http://archive.ics.uci.edu/ml, 2010, University of California, Irvine, School of Information and Computer Sciences",
      "author" : [ "A. Frank", "A. Asuncion" ],
      "venue" : null,
      "citeRegEx" : "Frank and Asuncion,? \\Q2010\\E",
      "shortCiteRegEx" : "Frank and Asuncion",
      "year" : 2010
    }, {
      "title" : "Hypothesis testing in high-dimensional regression under the gaussian random design model: Asymptotic theory",
      "author" : [ "Adel Javanmard", "Andrea Montanari" ],
      "venue" : "arXiv preprint arXiv:1301.4240,",
      "citeRegEx" : "Javanmard and Montanari,? \\Q2013\\E",
      "shortCiteRegEx" : "Javanmard and Montanari",
      "year" : 2013
    }, {
      "title" : "Chi-squared oracle inequalities, State of the Art in Probability and Statistics (M",
      "author" : [ "I. Johnstone" ],
      "venue" : "IMS Lecture Notes, Institute of Mathematical Statistics,",
      "citeRegEx" : "Johnstone,? \\Q2001\\E",
      "shortCiteRegEx" : "Johnstone",
      "year" : 2001
    }, {
      "title" : "Asymptotics for lasso-type estimators",
      "author" : [ "K. Knight", "W. Fu" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "Knight and Fu,? \\Q2000\\E",
      "shortCiteRegEx" : "Knight and Fu",
      "year" : 2000
    }, {
      "title" : "The concentration of measure phenomenon",
      "author" : [ "M. Ledoux" ],
      "venue" : "Mathematical Surveys and Monographs,",
      "citeRegEx" : "Ledoux,? \\Q2001\\E",
      "shortCiteRegEx" : "Ledoux",
      "year" : 2001
    }, {
      "title" : "Sup-norm convergence rate and sign concentration property of lasso and dantzig estimators",
      "author" : [ "Karim Lounici" ],
      "venue" : "Electronic Journal of statistics",
      "citeRegEx" : "Lounici,? \\Q2008\\E",
      "shortCiteRegEx" : "Lounici",
      "year" : 2008
    }, {
      "title" : "Regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer",
      "author" : [ "Jie Peng", "Ji Zhu", "Anna Bergamaschi", "Wonshik Han", "Dong-Young Noh", "Jonathan R Pollack", "Pei Wang" ],
      "venue" : "The Annals of Applied Statistics",
      "citeRegEx" : "Peng et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2010
    }, {
      "title" : "A simple and efficient algorithm for gene selection using sparse logistic regression, Bioinformatics",
      "author" : [ "Shirish Krishnaj Shevade", "S. Sathiya Keerthi" ],
      "venue" : null,
      "citeRegEx" : "Shevade and Keerthi,? \\Q2003\\E",
      "shortCiteRegEx" : "Shevade and Keerthi",
      "year" : 2003
    }, {
      "title" : "Regression shrinkage and selection with the Lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "J. Royal. Statist. Soc B",
      "citeRegEx" : "Tibshirani,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani",
      "year" : 1996
    }, {
      "title" : "On asymptotically optimal confidence regions and tests for high-dimensional models, arXiv:1303.0518",
      "author" : [ "S. van de Geer", "P. Bühlmann", "Y. Ritov" ],
      "venue" : null,
      "citeRegEx" : "Geer et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Geer et al\\.",
      "year" : 2013
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices, Compressed Sensing: Theory and Applications (Y.C",
      "author" : [ "R. Vershynin" ],
      "venue" : "Eldar and G. Kutyniok, eds.),",
      "citeRegEx" : "Vershynin,? \\Q2012\\E",
      "shortCiteRegEx" : "Vershynin",
      "year" : 2012
    }, {
      "title" : "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programming",
      "author" : [ "M.J. Wainwright" ],
      "venue" : "IEEE Trans. on Inform. Theory",
      "citeRegEx" : "Wainwright,? \\Q2009\\E",
      "shortCiteRegEx" : "Wainwright",
      "year" : 2009
    }, {
      "title" : "Thresholded Lasso for high dimensional variable selection and statistical estimation, arXiv:1002.1583v2",
      "author" : [ "S. Zhou" ],
      "venue" : null,
      "citeRegEx" : "Zhou,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhou",
      "year" : 2010
    }, {
      "title" : "The adaptive lasso and its oracle properties",
      "author" : [ "H. Zou" ],
      "venue" : "Journal of the American Statistical Association",
      "citeRegEx" : "Zou,? \\Q2006\\E",
      "shortCiteRegEx" : "Zou",
      "year" : 2006
    }, {
      "title" : "On model selection consistency of Lasso",
      "author" : [ "P. Zhao", "B. Yu" ],
      "venue" : "The Journal of Machine Learning Research",
      "citeRegEx" : "Zhao and Yu,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhao and Yu",
      "year" : 2006
    }, {
      "title" : "Confidence Intervals for Low-Dimensional Parameters in High-Dimensional",
      "author" : [ "C.-H. Zhang", "S.S. Zhang" ],
      "venue" : "Linear Models,",
      "citeRegEx" : "Zhang and Zhang,? \\Q2011\\E",
      "shortCiteRegEx" : "Zhang and Zhang",
      "year" : 2011
    } ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is ‘active’ (i.e., the corresponding coefficients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefficients through the Lasso (`1-regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that first solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set.",
    "creator" : "LaTeX with hyperref package"
  }
}