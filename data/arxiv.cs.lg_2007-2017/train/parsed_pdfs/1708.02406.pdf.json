{
  "name" : "1708.02406.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Robust Conditional Probabilities",
    "authors" : [ "Yoav Wald", "Amir Globerson" ],
    "emails" : [ "yoav.wald@mail.huji.ac.il", "gamir@post.tau.ac.il" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In classification tasks the goal is to predict a label Y for an object X . Assuming that the joint distribution of these two variables is p∗(x,y) then optimal prediction1 corresponds to returning the label y that maximizes the conditional probability p∗(y|x). Thus, being able to reason about conditional probabilities is fundamental to machine learning and probabilistic inference.\nIn the fully supervised setting, one can sidestep the task of estimating conditional probabilities by directly learning a classifier in a discriminative fashion. However, in unsupervised or semi-supervised settings, a reliable estimate of the conditional distributions becomes important. For example, consider an unlabeled input X . If we had a reliable estimate of p∗(y|x) we could decide whether to label the example or not, which could be used further within self-training [20, 33] or active learning contexts. Furthermore, as we show in our empirical results, such conditional probability estimates can be used as a regularizer for semi-supervised learning.\nThere are of course many approaches to “modelling” conditional distributions, from logistic regression to conditional random fields. However, these do not come with any guarantees of approximations to the true underlying conditional distributions of p∗ and thus cannot be used to reliably reason about these. This is due to the fact that such models make assumptions about the conditionals (e.g., conditional independence or parametric), which are unlikely to be satisfied in practice.\nAs an illustrative example for our motivation and setup, consider a set of n binary variables X1, ..., Xn whose distribution we are interested in. Suppose we have enough data to conclude that P [X1 = 1|X2 = 1] = 1. This lets us reason about many other probabilities. For example, we know that P [X1 = 1|X2 = 1, . . . , Xn = xn] = 1 for any setting of the x3, . . . , xn variables. This is a simple but powerful observation, as it translates knowledge about probabilities over small subsets to probability over large subsets. Now, what happens when P [X1 = 1|X2 = 1] = 0.99? In other words, what can we say about P [X1 = 1|X2 = 1, . . . , Xn = 0] given information about conditional probability P [Xi = xi|Xj = xj ]. As we show here, it is still possible to reason about such conditional probabilities even under this partial knowledge.\nMotivated by the above, we propose a novel model-free approach for reasoning about conditional probabilities. Specifically, we shall show how conditional probabilities can be lower bounded without making strong assumptions about the underlying distribution. The only assumption we make is\n1In the sense of minimizing prediction error.\nar X\niv :1\n70 8.\n02 40\n6v 1\n[ cs\n.L G\n] 8\nA ug\n2 01\n7\nthat certain low-order marginals of the distribution are known. We then show how these can be used to infer lower bounds on conditional distributions that are guaranteed to hold. One of the surprising outcomes of our analysis is that these lower bounds can be calculated efficiently, and often have an elegant closed form. Finally, we show how these bounds can be used as a regularizer in a semi-supervised setting, obtaining results that are competitive with variational autoencoders [14]."
    }, {
      "heading" : "2 Problem Setup",
      "text" : "We begin by defining notations to be used in what follows. Let X denote features and Y denote labels. Assume we have n features, denoted by random variables X1, . . . , Xn. If we have a single label we will denote it by Y . Otherwise, a multivariate label will be denoted by Y1, . . . , Yr. We assume all variables are discrete (i.e., can take on a finite set of values). Assume that X,Y are generated by some unknown underlying distribution p∗(X,Y ). Here we will assume that although we do not know p∗ we have access to the expected value of some vector function f : X,Y → Rd under p∗.23 Namely we assume we are given a vector a defined by a = Ep∗ [f(X,Y )]. Since a does not uniquely specify a distribution p∗, we will be interested in the set of all distributions where the expected value of f(X,Y ) is a. Denote this set by P(a), namely:\nP(a) = {q ∈ ∆ : Eq [f(X,Y )] = a} (1)\nwhere ∆ is the probability simplex of the appropriate dimension.\nWe shall specifically be interested in the case where the expected values correspond to simple marginals of the distribution p∗, such as those of a single feature and a label:\nµi(xi, y) = ∑\nx̄1,...,x̄n:x̄i=xi\np∗(x̄1, . . . , x̄n, y).\nSimilarly we may have access to the set of pairwise marginals µij(xi, xj , y) for all i, j ∈ E, where the set E corresponds to edges of a graph G (see also [7]). When the label is multivariate we may also incorporate marginals of the form µlk(yl, yk), then (l, k) ∈ E and we treat labels as part of the graph.\nWe denote the set of all such marginals by µ. And, as in Eq. (1) we define P(µ) to be the set of distributions whose marginals are given by µ. As we shall see later, the structure of the graph G will have implications on the types of bounds we can derive. Specifically, if G is tree shaped (i.e., has no cycles), tight bounds can be derived."
    }, {
      "heading" : "2.1 The Robust Conditionals Problem",
      "text" : "Our approach is to reason about conditional distributions using only the fact that p∗ ∈ P(µ). Our key goal is to lower bound these conditionals, since this will allow us to conclude that certain labels are highly likely in cases where the lower bound is large. We shall also be interested in upper and lower bounding joint probabilities, since these will play a key role in bounding the conditionals.\nOur goal is thus to solve the following optimization problems.\nmin p∈P(µ) p(x,y), max p∈P(µ) p(x,y), min p∈P(µ)\np (y | x). (2)\nIn all three problems, the constraint set is linear in p. However, note that p is specified by an exponential number of variables (one per assignment x1, . . . , xn) and thus it is not feasible to plug these constraints into an LP solver. In terms of objective, the min and max problems are linear, and the conditional is fractional linear. In what follows we show how all three problems can be solved efficiently for tree shaped graphs.\n2Abusing notation, we use X to denote both the random variable and its range of values. 3For simplicity we assume the expectation is exact. Generally it is of course only approximate, but concentration bounds can be used to quantify this accuracy as a function of data size. Furthermore, most of the methods described here can be extended to inexact marginals (e.g., see [6] for an approach that can be applied here)."
    }, {
      "heading" : "3 Related Work",
      "text" : "The problem of reasoning about a distribution based on its expected values has a long history, with many beautiful mathematical results. An early example is the classical Chebyshev inequality, which bounds the tail of a distribution given its first and second moments. This was significantly extended in the Chebyshev Markov Stieltjes inequality [2]. More recently, various generalized Chebyshev inequalities have been developed [3, 24, 29]. A typical statement of these is that several moments are given, and one seeks the minimum measure of some set S under any distribution that agrees with the moments. As [3] notes, most of these problems are NP hard, with isolated cases of tractability. Such inequalities have been used to obtain minimax optimal linear classifiers in [17]. The moment problems we consider here are very different from those considered previously, in terms of the finite support we require, our focus on bounding probabilities and conditional probabilities of assignments.\nThe above approaches consider worst case bounds on probabilities of certain events for distributions in P(a). A different approach is to pick a particular distribution in P and use it as an approximation (or model) of p∗. The most common choice for such a distribution is the maximum entropy distribution in P(a). Such log-linear models have found widespread use in statistics and machine learning. In particular, most graphical models can be viewed as maximum entropy distributions (e.g., see [15, 16]). However, the probabilities given by the maximum entropy model cannot be related to the true probabilities in any sense (e.g., upper or lower bound). This is where our approach markedly differs from entropy based assumptions. Another approach to reducing modeling assumptions is robust optimization, where data and certain model parameters are assumed not be known precisely, and optimality is sought in a worst case adversarial setting. Such an approach has been applied to machine learning in various settings (e.g, see [34, 19]), establishing close links to regularization. None of these approaches considers bounding probabilities as is our focus here.\nFinally, another elegant moment approach is that based on kernel mean embedding [25, 26]. In this approach, one maps a distribution into a set of expected values of a set of functions (possibly infinite). The key observation is that this mean embedding lies in an RKHS, and hence many operations, such as computing distribution similarity and covariances can be done implicitly. Most of the applications of this idea assume that the set of functions is rich enough to fully specify the distribution (i.e., characteristic kernels [27]). The focus is thus different from ours, where moments are not assumed to be fully informative, and the set P(a) contains many possible distributions. It would however be interesting to study possible uses of RKHS in our setting."
    }, {
      "heading" : "4 Calculating Robust Conditional Probabilities",
      "text" : "The optimization problems in Eq. (2) are linear programs (LP) and fractional LPs, where the number of variables scales exponentially with n. Yet, as we show in this section and Section 5, it turns out that in many non-trivial cases, they can be efficiently solved. Our focus below is on the case where the pairwise marginals correspond to a set E that forms a tree structured graph. The tree structure assumption is common in literature on Graphical Models, only here we do not make an inductive assumption on the generating distribution (i.e., we make none of the conditional independence assumptions that are implied by tree-structured graphical models). In the following sections we study solutions of robust conditional probabilities under the tree assumption. We will also discuss some extensions to the cyclic case. Finally, note that although the derivations here are for pairwise marginals, these can be extended to the non-pairwise case by considering clique-trees [e.g., see 31]. Pairs are used here to allow a clearer presentation.\nIn what follows, we first show that the conditional lower bound has a simple structure as stated in Theorem 4.1. This result does not immediately suggest an efficient algorithm since its denominator includes an exponentially sized LP. Next, in Section 4.2 we show how this LP can be reduced to a polynomially sized one, resulting in an efficient algorithm for the lower bound. Finally, in Section 5 we show that in certain cases there is no need to use a general purpose LP solver and the problem can be solved either in closed form or via combinatorial algorithms. Detailed proofs are provided in the appendix."
    }, {
      "heading" : "4.1 From Conditional Probabilities To Maximum Probabilities with Exclusion",
      "text" : "The main result of this section will reduce calculation of the robust conditional probability for p(y | x), to one of maximizing the probability of all labels other than y. This reduction by itself will not allow for efficient calculation of the desired conditional probabilities, as the new problem is also a large LP that needs to be solved. Still the result will take us one step further towards a solution, as it reveals the probability mass a minimizing distribution p will assign to x,y.\nThis part of the solution is related to a result from [10], where the authors derive the solution of minp∈P(µ) p(x,y). They prove that under the tree assumption this problem has a simple closed form solution, given by the functional I(x, y ; µ):\nI(x, y ; µ) = ∑ i (1− di)µi(xi, y) + ∑ ij∈E µij(xi, xj , y)  + . (3)\nHere [·]+ denotes the ReLU function [z]+ = max{z, 0} and di is the degree of node i in G. The above expression is suitable in case of a single label, it extends naturally to the multivariate case when we consider labels as part of the graph.\nIt turns out that robust conditional probabilities will assign the event x,y its minimal possible probability as given in Eq. (3). Moreover, it will assign all other labels their maximum possible probability. This is indeed a behaviour that may be expected from a robust bound, we formalize it in the main result for this part: Theorem 4.1. Let µ be a vector of tree-structured pairwise marginals, then\nmin p∈P(µ) p (y | x) = I(x,y ;µ) I(x,y ;µ) + maxp∈P(µ) ∑ ȳ 6=y p(x, ȳ) . (4)\nThe proof of this theorem is rather technical and we leave it for the appendix.\nWe note that the above result also applies to the “structured-prediction” setting where y is multivariate and we also assume knowledge of marginals µ(yi, yj). In this case, the expression for I(x,y ; µ) will also include edges between yi variables, and incorporate their degrees in the graph.\nThe important implication of Theorem 4.1 is that it reduces the minimum conditional problem to that of probability maximization with an assignment exclusion. Namely:\nmax p∈P(µ) ∑ ȳ 6=y p(x, ȳ). (5)\nAlthough this is still a problem with an exponential number of variables, we show in the next section that it can be solved efficiently."
    }, {
      "heading" : "4.2 Minimizing and Maximizing Probabilities",
      "text" : "To provide an efficient solution for Eq. (5), we turn to a class of joint probability bounding problems. Assume we constrain each variable Xi and Yj to a subset X̄i, Ȳj of its domain and would like to reason about the probability of this constrained set of joint assignments:\nU = { x,y | xi ∈ X̄i, yj ∈ Ȳj ∀i ∈ [n], j ∈ [r] } . (6)\nUnder this setting, an efficient algorithm for\nmax p∈P(µ) ∑ u∈U\\(x,y) p(u),\nprovides one to Eq. (5) and by the results of last section, also for robust conditional probabilities. To see this is indeed the case, assume we are given an assignment (x,y). Then setting X̄i = {xi} for all features and Ȳj = {1, . . . , |Yj |} for labels (i.e. U does not restrict labels), gives exactly Eq. (5). To derive the algorithm, we will find a compact representation of the LP, with a polynomial number of variables and constraints. The result is obtained by using tools from the literature on Graphical Models.\nIt shows how to formulate probability maximization problems over U as problems constrained by the local marginal polytope [31]. Its definition in our setting slightly deviates from its standard definition, as it does not require that probabilities sum up to 1 4: Definition 1. The set of locally consistent pseudo marginals over U is defined as:\nML(U) = {µ̃ | ∑ xi∈X̄i µ̃ij(xi, xj) = µ̃j(xj) ∀(i, j) ∈ E, xj ∈ X̄j}.\nThe partition function of µ̃, Z(µ̃), is given by ∑ xi∈X̄i µ̃i(xi).\nOur observation then is that Eq. (5) can be folded into a problem with polynomially many variables and constraints, by simply maximizing the partition function overML(U). Theorem 4.2. Let U be a universe of assignments as defined in Eq. (6), x ∈ U and µ a vector of tree-structured pairwise marginals, then the values of the following problems:\nmax p∈P(µ) ∑ u∈U p(u), max p∈P(µ) ∑ u∈U\\(x,y) p(u),\nare equal (respectively) to:\nmax µ̃∈ML(U),µ̃≤µ Z(µ̃), max µ̃∈ML(U),µ̃≤µ I(x,y ; µ̃)≤0 Z(µ̃). (7)\nThe LPs in Eq. (7) involve a polynomial number of constraints and variables and can thus be solved efficiently.\nProofs of this result can be obtained either by exploiting strong duality of LPs and the maxreparameterization property of functions that decompose over trees [32, 5], or by using the junctiontree theorem [31]. In the appendix we provide a proof based on the latter.\nTo conclude this section, we restate the main result: the robust conditional probability problem Eq. (2) can be solved in polynomial time by combining Theorems 4.1 and 4.2. As a by-product of this derivation we also presented efficient tools for bounding answers on a large class of probabilistic queries. While this is not the focus of the current paper, these tools may be a useful in probabilistic modelling, where we often combine estimates of low order marginals with assumptions on the data generating process. Bounds like the ones presented in this section give a quantitative estimate of the uncertainty that is induced by data and circumvented by our assumptions."
    }, {
      "heading" : "5 Closed Form Solutions and Combinatorial Algorithms",
      "text" : "The results of the previous section imply that the minimum conditional can be found by solving a poly-sized LP. Although this results in polynomial runtime, it is interesting to improve as much as possible on the complexity of this calculation. One reason is that application of the bounds might require solving them repeatedly within some larger learning probelm. For instance, in classification tasks it may be necessary to solve Eq. (4) for each sample in the dataset. An even more demanding procedure will come up in our experimental evaluation, where we learn features that result in high confidence under our bounds. There, we need to solve Eq. (4) over mini-batches of training data only to calculate a gradient at each training iteration. Since using an LP solver in these scenarios is impractical, we next derive more efficient solutions to some special cases of Eq. (4)."
    }, {
      "heading" : "5.1 Closed Form for Multiclass Problems",
      "text" : "The multiclass setting is a special case of Eq. (4) when y is a single label variable (e.g., a digit label in mnist with values y ∈ {0, . . . , 9}). In this case the problem in Eq. (2) is: minp∈P(µ) p (y | x). The solution of course depends on the type of marginals provided in P(µ). Here we will assume that we have access to joint marginals of the label y and pairs of features xi, xj corresponding to edges ij ∈ E of a graph G. We note that we can obtain similar results for the cases where some additional “unlabeled” statistics µij(xi, xj) are known.\n4We omit the labels Y1, . . . , Yr from this definition for notational convenience. Formally, the consistency constraints are also enforced for edges with nodes that correspond to labels.\nIt turns out that in both cases Eq. (5) has a simple solution. Here we write it for the case without unlabeled statistics.\nLemma 5.1. Let x ∈ X and µ a vector of tree-structured pairwise marginals, then\nmin p∈P(µ) p (y | x) = I(x, y ; µ) I(x, y ; µ) + ∑ ȳ 6=y minij µij(xi, xj , ȳ) . (8)\nThis lemma is based on a result that states maxp∈P(µ) p(x, ȳ) = minij µij(xi, xj , ȳ), it can either be proved by analyzing results in Thm. 4.2, or with a duality based argument which is how we prove it in the appendix."
    }, {
      "heading" : "5.2 Combinatorial Algorithms and Connection to Maximum Flow Problems",
      "text" : "In some cases, fast algorithms for the optimization problem in Eq. (5) can be derived by exploiting a tight connection of our problems to the Max-Flow problem. The problems are also closely related to the weighted Set Cover problem. To observe the connection to the latter, consider an instance of Set-Cover defined as follows. The universe is all assignments x. Sets are defined for each i, j, xi, xj and are denoted by Sij,xi,xj . The set Sij,xi,xj contains all assignments x̄ whose values at i, j are xi, xj . Moreoever, the set Sij,xi,xj has weight w(Sij,xi,xj ) = µij(xi, xj). Note that the number of items in sets is exponential, but there is a polynomial amount of sets. Now assume we would like to use these sets to cover some set of assignments U with the minimum possible weight. It turns out that under the tree structure assumption, this problem is closely related to the problem of maximizing probabilities.\nLemma 5.2. Let U be a set of assignments and µ a vector of tree-structured marginals. Then:\nmax p∈P(µ) ∑ u∈U p(u), (9)\nhas the same value as the standard LP relaxation [30] of the Set-Cover problem above.\nThe connection to Set-Cover may not give a path to efficient algorithms, but it does illuminate some of the results presented earlier. It is simple to verify that minij µij(xi, xj , ȳ) is a weight of a cover of x, ȳ, while Eq. (3) equals one minus the weight of a set that covers all assignments but x,y. A connection that we may exploit to obtain more efficient algorithms is to Max-Flow. When the graph defined by E is a chain, we show in the appendix that the value of Eq. (9) can be found by solving a flow problem on a simple network. We note that using the same construction, Eq. (5) turns out to be Max Flow under a budget constraint [1]. This may prove very beneficial for our goals, as it allows for efficient calculation of the robust conditionals we are interested in. Our conjecture is that this connection goes beyond chain graphs, but leave this for exploration in future work. The proofs for results in this section may also be found in the appendix."
    }, {
      "heading" : "6 Experiments",
      "text" : "To evaluate the utility of our bounds, we consider their use in settings of semi-supervised deep learning and structured prediction. For the bounds to be useful, the marginal distributions need to be sufficiently informative. In some datasets, the raw features already provide such information, as we show in Section 6.3. In other cases, such as images, a single raw feature (i.e., a pixel) does not provide sufficient information about the label. These cases are addressed in Section 6.1 where we show how to learn new features which do result in meaningful bounds. Using deep networks to learn these features turns out to be an effective method for semi-supervised settings, reaching results close to those demonstrated by Variational Autoencoders [14]. It would be interesting to use such feature learning methods for structured prediction too; however this requires incorporation of the max-flow algorithm into the optimization loop, and we defer this to future work."
    }, {
      "heading" : "6.1 Deep Semi-Supervised Learning",
      "text" : "Here we describe how our bounds can be used for semi-supervised learning. We learn a neural network whose last layer serve as the features Zi. The marginals of these with the label Y are used in\nour bounds (in the text we refer to these as Xi. Here we switch to Zi since Xi are understood as the raw features of the problem. e.g., the pixel values in the image). The features Zi will not be discrete since they are an output of a neural net. However, we will use a sigmoid activation for the last layer, so that the Zi values are bounded between 0 and 1. For now, let us consider the Zi as actual discrete variables with values {0, 1}, and we will later explain how to overcome their non-discrete values. Given an input x, we can calculate features z, and then calculate a set of bounds for p(y|z) for each value of y. Denote this bound by p̃y. Then the bound is used in two ways, depending on whether x has a label or not. If x has a label y, we add a standard cross-entropy term where p̃y are the logits. This pushes p̃y towards values that are maximized in the correct label. If x is unlabeled, we want to maximize the confidence of the prediction and thus add the entropy of the distribution qy ∝ p̃y to the objective, scaled by a regularization coefficient. This prefers solutions where p̃y is focused on one assignment. It is related to min-entropy regularization [12], but the entropy is of a distribution induced by our bounds. Finally, for classification we use the arg max of the distribution p̃y . Namely, we do not need to learn a softmax layer as is usually done.\nThe architecture used for mapping the input x (i.e., the image) into z is a standard multilayer perceptron (MLP), with fully connected layers, a ReLU activation at each layer, except a sigmoid in the last one. In our experiments we used hidden layers of sizes 1000, 500, 50 (so z is 50 dimensional). We also use batch normalization and add noise in hidden layers as described in [23] (however we do not use any component of their unsupervised cost function). To address the fact that Zi is not discrete, we use the natural smooth counterparts of the discrete operations. Marginals are calculated by considering Zi as an indicator variable (e.g., the probability p(Zi = 1) would just be the average of the Zi values). The min probability bound is calculated as follows:\np̃y = softmaxy( Ī(z, y ; µ̄) Ī(z, y ; µ̄) + ∑ ȳ 6=y minij µ̄ij(zi, zj , ȳ) ) , (10)\nwhere Ī , µ̄ are again the smoothed versions of I,µ.\nWe compare our results with those obtained by Variational Autoencoders and Ladder Networks. Although we do not expect to get the same high accuracies these methods obtain, getting comparable numbers with a simple regularizer (compared to the elaborate techniques used in these works) like the one we suggest, is an encouraging sign for the possibility of learning features that induce high confidence. We also compare to an architecture similar to ours, but that uses minimum entropy regularization [12] on a softmax layer connected to z (i.e., it does not use our bounds at all). In this case we also add `2 regularization on the weights of the soft-max layer, since otherwise entropy can always be driven to zero in the separable case. Finally, we also experimented with adding a hinge loss as a regularizer (as in Transductive SVM [13]), but omit it from the comparison because it did not yield significant improvement over a purely supervised MLP and entropy regularization."
    }, {
      "heading" : "6.2 MNIST Dataset",
      "text" : "We trained the models described above on the MNIST dataset, using 100 and 1000 labeled samples (see [14] for a similar setup). We set the two regularization parameters required for the entropy regularizer and the one required for our minimum probability regularizer with five fold cross validation. We used 10% of the training data as a validation set and compared error rates on the 10000 samples of the test set. Results are shown in Figure 1. They show that on the 1000 sample case we are slightly outperformed by VAE and for 100 samples we lose by 1%. Ladder networks outperform the other baselines.\nAccuracy vs. Coverage Curves: In self-training and co-training methods, a classifier adds its most confident predictions to the training set and then repeats training. A crucial factor in the success\nof such methods is the error in the predictions we add to the training pool. Classifiers that use confidence over unlabelled data as a regularizer are natural choices for base classifiers in such a setting. Therefore an interesting comparison to make is the accuracy we would get over the unlabeled data, had the classifier needed to choose its k most confident predictions.\nWe plot this curve as a function of k for the entropy regularizer and our min-probabilities regularizer. Samples in the unlabelled training data are sorted in descending order according to confidence. Confidence for a sample in entropy regularized MLP is calculated based on the value of the logit that the predicted label received in the output layer. For the robust probabilities classifier, the confidence of a sample is the minimum conditional probability the predicted label received. As can be observed in Figure 6.2, our classifier ranks its predictions better than the entropy based method. We attribute this to our classifier being trained to give robust bounds under minimal assumptions."
    }, {
      "heading" : "6.3 Multilabel Structured Prediction",
      "text" : "As mentioned earlier, in the structured prediction setting it is more difficult to learn features that yield high certainty. We therefore provide a demonstration of our method on a dataset where the raw features are relatively informative. The Genbase dataset taken from [28], is a protein classification multilabel dataset. It has 662 instances, divided into a training set of 463 samples and a test set of 199, each sample has 1185 binary features and 27 binary labels. We ran a structured-SVM algorithm, taken from [21] to obtain a classifier that outputs a labelling ŷ for each x in the dataset (the error of the resulting classifier was 2%). We then used our probabilistic bounds to rank the classifier’s predictions by their robust conditional probabilities. The bounds were calculated based on the set of marginals µij(xi, yj), estimated from the data for each pair of a feature and a label Xi, Yj . This set of marginals corresponds to a non-tree structure and we handled it as discussed in Section 7. Observing the values of our bounds, it turned out that 85% of these were above 0.99, indicating a high level of certainty that this is the correct label. Indeed only 0.59% of these 85% were actually errors. The remaining errors made by the classifier were assigned min conditional probability zero by our bounds, indicating low level of certainty."
    }, {
      "heading" : "7 Discussion",
      "text" : "We presented a method for bounding conditional probabilities of a distribution based only on knowledge of its low order marginals. Our results can be viewed as a new type of moment problem, bounding a key component of machine learning systems, namely the conditional distribution. As we show, calculating these bounds raises many challenging optimization questions, which surprisingly result in closed form expressions in some cases.\nWhile the results were limited to the tree structured case, some of the methods have natural extensions to the cyclic case that still result in robust estimations. For instance, the local marginal polytope in Eq. (7) can be taken over a cyclic structure and still give a lower bound on maximum probabilities. Also in the presence of the cycles, it possible to find the spanning tree that induces the best bound on\nEq. (3) using a maximum spanning tree algorithm. Plugging these solutions into Eq. (4) results in a tighter approximation which we used in our experiments.\nOur method can be extended in many interesting directions. Here we addressed the case of discrete random variables, although we also showed in our experiments how these can be dealt with in the context of continuous features. It will be interesting to calculate bounds on conditional probabilities given expected values of continuous random variables. In this case, sums-of-squares characterizations play a key role [18, 22], and their extension to the conditional case is an exciting challenge. It will also be interesting to study how these bounds can be used in the context of unsupervised learning. One natural approach here would be to learn constraint functions such that the lower bound is maximized.\nFinally, we plan to study the implications of our approach to diverse learning settings, from selftraining to active learning and safe reinforcement learning."
    }, {
      "heading" : "A Proof of Lem. 5.1",
      "text" : "Let us begin with the proof of Lem. 5.1, in which we derive the form of solutions used in our experiments.\nProof. We start by writing the problem down in the following manner:\nmin p∈P(µ)\np(x, y) p(x, y) + ∑ ŷ 6=y p(x, ŷ) .\nIt is obvious that in order to minimize the objective, the higher p(x, ŷ) is for ŷ 6= y and the lower p(x, y), the lower objective we get. We now notice that each of the assignments can be maximized or minimized independently, because they appear in totally distinct constraints in P(µ). This is true because all constraints in P(µ) are of the form:∑\nz:zi,zj=xi,xj\np(z, ȳ) = µij(xi, xj , ȳ).\nHence, for any pair y1 6= y2, non of the variables in {p(x1, y1) | x1 ∈ X} appear in the same constraint with a variable in {p(x2, y2) | x2 ∈ X}, so all variables p(x, ŷ), p(x, y) can be maximized or minimized separately. We already know from [10] that\nmin p∈P(µ) p(x, y) = I(x, y ; µ).\nIt is left to show that max p∈P(µ) p(x, ȳ) = min ij µij(xi, xj , ȳ),\nthen the result of the lemma follows immediately. To prove the above equality we take the dual LP of the left hand side:\nmin λ · µ (12) s.t. λ(x, y) ≥ 1\nλ(z, ȳ) ≥ 0 ∀z 6= x ∨ ȳ 6= y.\nHere λ(·) are the dual variables, which we can think of as a function that decomposes over a directed tree:\nλ(x, y) = λr(xr, y) + ∑ i6=r λi,pa(i)(xi, xpa(i), y) + λi(xi, y).\nThe inner product λ · µ is given by:∑ i,zi λi(zi)µi(zi) + ∑ ij∈E,zi,zj λij(zi, zj)µij(zi, zj). (13)\nLet us take the min-reparameterization of this function and then take its expectation over a distribution p ∈ P(µ). The following inequality holds for any feasible λ:\nEp [λ(x, y)] = ∑ zr µr(zr)λ̄r(zr, y) + ∑ i 6=r\nzi,zpa(i)\nµi,pa(i)(zi, zpa(i))(λ̄(zi, zpa(i), y)− λ̄pa(i)(zpa(i), y))\n≥ µr(xr)λ̄r(xr, y) + ∑ i 6=r µi,pa(i)(xi, xpa(i))(λ̄(xi, xpa(i), y)− λ̄pa(i)(xpa(i), y)).\nThe inequality is true because any feasible λ is non-negative, hence λ̄r(zr) ≥ 0 and because minmarginals over a pair of variables are always larger than those over one of them. We will conclude the proof by observing that:\n• The right hand side of the inequality is a combination of the µs that are consistent with x, y and the coefficients of this combination sum up to:\nλ̄r(xr, y) + ∑ i 6=r λ̄(xi, xpa(i), y)− λ̄pa(i)(xpa(i), y) = λ(x, y) ≥ 1.\nThe equality holds due to the reparametrization property in Eq. (11) and λ’s feasibility. Since the sum is higher than 1, the right hand side is also larger than any convex combination of the µs, which in turn is larger than the smallest element in the combination. We arrive at the conclusion that:\nEp [λ(x, y)] ≥ min ij µij(xi, xj , y).\n• It also holds that λ · µ = Ep [λ(x)], hence the objective of any feasible solution is larger than minij µij(xi, xj , y). On the other hand, setting λij(xi, xj , y) = 1 for a minimizing pair i, j and all other variables to 0 results in a feasible solution with exactly this objective. It follows that this must be the optimal value of the problem."
    }, {
      "heading" : "B Notations for Remainder of the Proofs",
      "text" : "To allow for a more convenient notation, from now on we treat labels as hidden variables. That is, instead of n features and r labels, we assume there are just n variables X1, . . . , Xn. The first m are hidden (these will play the role of a label) and the last n−m are observed, where m may be between 0 and n − 1. For an assignment x, we refer to the hidden part as xh and the observed as xo. The split into hidden and observed variables will mainly serve us in the proof of Thm. 4.1, in other proofs it is just more convenient to not split expressions to x,y.\nWe also denote the subvector of µ over hidden variables and edges between them as µh. That is, considering the items of µ are expressions µi(zi), µij(zi, zj), µh is the subvector containing items where i ∈ h, i, j ∈ h respectively. Define a similar vector µo for observed variables and edges between them. The vectors Ix, Ih,x are defined to have the same indices as µ,µh respectively, their value is 1 in indices consistent with x (i.e. zi, zj = xi, xj or zi = xi for entries that contain µij(zi, zj), µi(zi) respectively) and 0 otherwise. We will use the shorthand Ix for the vector I(x;µ)Ix. Some notations related to graphical properties of hidden and observed nodes will be required. The number of connected components in the subgraph of hidden variables and edges between them is\n|Ph|, similarly for observed variables we will use |Po|. The set of edges ij between hidden nodes (i.e. i, j ∈ h) is Eh, between a hidden and observed node (i.e. i ∈ o, j ∈ h w.l.o.g) is Eoh and between observed nodes (i.e. i, j ∈ o) is Eo. The degree of node i is di and the number of its hidden neighbors is dhi .\nFinally, we define variations on the objects related to graphical models that we use in the paper. The functional Ĩ(· ; µ) is the same functional defined in Eq. (3), only without the ReLU operator:\nĨ(x ; µ) = ∑ i (1− di)µi(xi) + ∑ ij∈E µij(xi, xj).\nWe will also use two variants on the local marginal polytope [31]: ML = { µ̃ | ∑ xj µ̃ij(xi,xj)=µ̃i(xi) ∀ij∈E,xi∑\nxi µ̃ij(xi,xj)=µ̃j(xj) ∀ij∈E,xj,\n∑ xi µ̃i(xi)=1 ∀i∑\nxi,xj µ̃i(xi,xj)=1 ∀i,j∈E\n} .\nOne variant we use isML(U) that was defined in the paper. The other isMhL, where items contain marginals only on hidden variables and edges between them:\nMhL = {µ̃ | ∑ xi∈X̄i µ̃ij(xi,xj)=µ̃j(xj) ∀(i,j)∈Eh∑\nxj∈X̄j µ̃ij(xi,xj)=µ̃i(xi) ∀(i,j)∈Eh}."
    }, {
      "heading" : "C Proof of Lem. 5.2",
      "text" : "We start by proving the connection to Set-Cover and then move on to Max-Flow.\nC.1 Connection to Set-Cover\nProof. Let us write down the dual of Eq. (9):\nmin λ · µ (14) s.t. λr(xr) + ∑ i 6=r λi,pa(i)(xi, xpa(i)) + λi(xi) ≥ 0 ∀x /∈ U\nλr(xr) + ∑ i 6=r λi,pa(i)(xi, xpa(i)) + λi(xi) ≥ 1 ∀x ∈ U,\nThis is already very similar to the LP Relaxation of Set-Cover, but with the significant difference that variables λ are unrestricted, where in the Set-Cover LP they are non-negative. This is where the tree structure plays an important role. Consider the min-reparameterization of any feasible solution λ(x):\nλ(x) = λ̄r(xr) + ∑ i6=r λ̄i,pa(i)(xi, xpa(i))− λ̄pa(i)(xpa(i)).\nSince λ is feasible and the constraints demand that λ(x) is non negative for all x, it is clear that λ̄r(xr) ≥ 0. Moreover, because λ̄ is a min-reparameterization it is easy to see that λ̄i,pa(i)(xi, xpa(i)) − λ̄pa(i)(xpa(i)) ≥ 0. This is true because constraining a minimization on xi, xpa(i) gives a higher result than constraining on xpa(i) alone.\nNow let us look at the LP Relaxation of the aforementioned Set-Cover problem:\nmin δ · µ (15) s.t. δr(xr) + ∑ i 6=r δi,pa(i)(xi, xpa(i)) + δi(xi) ≥ 0 ∀x /∈ U\nδr(xr) + ∑ i 6=r δi,pa(i)(xi, xpa(i)) + δi(xi) ≥ 1 ∀x ∈ U,\nδ ≥ 0\nObviously, if δ is feasible for Eq. (15), setting λ = δ gives a feasible solution to Eq. (14) with the same objective as δ’s in Eq. (15). That is, this problem is more constrained than Eq. (14). Yet given a\nfeasible solution to Eq. (14), we can use the min-reparameterization and obtain a feasible solution to the above problem with the same objective λ · µ:\nδi(xi) =\n{ λ̄r(xr) i = r\n0 i 6= r , δi,pa(i)(xi, xpa(i)) = λ̄i,pa(i)(xi, xpa(i))− λ̄pa(i)(xpa(i)).\nIt is easy to see that because of the min-reparameteriztion property, δ(x) = λ(x) for all x and δ ≥ 0. This means that δ is feasible and that the objectives are equal. To verify the latter, consider a distribution p ∈ P(µ). Taking the expectations of δ,µ with respect to p shows the equality in objectives:\nλ · µ = Ep [λ(x)] = Ep [δ(x)] = δ · µ.\nWe conclude that while the set cover LP Relaxation is more constrained, all feasible solutions of Eq. (14) can be mapped to feasible solutions of this relaxation in a manner that preserves the objective. Hence the problems have the same value.\nLet us emphasize the following two points:\n• This part of the lemma did not exploit the specific choice of U (being consisted of all assignments where variables take values in a certain set X̄i). That is, it holds for any choice of U , not only those of the form mentioned in Eq. (6).\n• The constraints for x /∈ U in Eq. (15) are redundant because δ ≥ 0. Removing these constraints and moving back from Eq. (15) to its dual, expressed with variables p, we get another formulation of Eq. (9). We will use this in the next part of the proof and also later on, we thus state it as a corollary.\nCorollary C.1. Let U be a universe of assignments (not necessarily of the form in Eq. (6)) and µ a tree-structured vector of marginals. The following LP has the same value as Eq. (9):\nmax p≥0 ∑ u∈U p(u) (16)\ns.t. ∑ u∈U\nui,uj=zi,zj\np(u) ≤ µi,j(zi, zj) ∀ij ∈ E, zi, zj\n∑ u∈U ui=zi p(u) ≤ µi(zi) ∀i ∈ V, zi\nC.2 Equivalence to Max-Flow\nAs stated in the Section 5.2, when the underlying graph is a chain, Eq. (9) is a Max-Flow problem. The equivalence to Max-Flow is apparent when thinking of every assignment x ∈ U as a path in a flow network. Assume our statistics µ are µ1,2, µ2,3, . . . , µn−1,n, then define a flow network with source and sink s, t and a node (i, xi) for each variable i and xi ∈ X̄i (i.e. one node for each variableassignment pair). The edges of the network are (i, xi)→ (i+ 1, xi+1) for each 0 ≤ i ≤ n− 1 and xi, xi+1 ∈ X̄i × X̄i+1, they will have capacity µi,i+1(xi, xi+1). Additionally we will have edges s→ (1, x1), (n, xn)→ t for each x1 and xn with unbounded capacity. It is simple to see that there is a one-to-one correspondence between paths from s to t and assignments in U . This is where U ’s special structure, stated in Eq. (6) of the paper comes into play. Also, the paths that go through each edge (i, xi) → (i + 1, xi+1) are exactly those of assignments z where zi, zi+1 = xi, xi+1. According to flow decomposition [9], the LP in Eq. (16) solves the Max-Flow problem on this network (where the flow is expressed as the sum of flows in all s − t paths in the network), with a single exception that it does not contain the constraints:∑\nu∈U ui=zi\np(u) ≤ µi(zi) ∀i ∈ V, zi.\nThus to finish the proof we will get convinced that these added constraints are redundant. Consider a solution p that only satisfies the constraints of pairwise marginals in Eq. (16), we will show it also\nsatisfies the constraints above. Let i ∈ [n] and xi ∈ X̄i and let j be a neighbour of i in the chain (the graph is connected, so there always is a neighbour), then:∑\nu∈U ui=xi\np(u) = ∑ uj∈X̄j\n∑ u∈U\nui,uj=xi,xj\np(u) ≤ ∑ uj∈X̄j µij(xi, uj) ≤ µi(xi).\nThis shows the constraint is satisfied and concludes our proof.\nThe next proof, that of Thm. 4.2, is for results on maximizing probabilities. When the underlying graph is a chain, these results are similar to the equivalence to Max-Flow that we just proved. When the graph is not a chain, they will give an LP that does not directly correspond to a Max-Flow problem, but is still of polynomial size. That is, it can be solved efficiently with a standard LP solver, but not necessarily with a combinatorial algorithm. Our conjecture is that combinatorial algorithms can be derived for other cases, but we defer this to future work."
    }, {
      "heading" : "D Proof of Thm. 4.2",
      "text" : "The theorem reformulates the following problems:\nmax p∈P(µ) ∑ u∈U p(u), max p∈P(µ) ∑ u∈U\\x p(u). (17)\nOur goal is to show that they have the same optimum as: max\nµ̃∈ML(U),µ̃≤µ Z(µ̃), max\nµ̃∈ML(U),µ̃≤µ I(x ; µ̃)≤0\nZ(µ̃). (18)\nProof. To show equality of the optimal values, let us offer a mapping between feasible solutions of the pairs of problems. From our previous results, both problems in Eq. (17) can be written in the form of Eq. (16) with U and U \\ x respectively. We will start by mapping feasible solutions of these problems to feasible solutions of Eq. (18).\nChoose an arbitrary root for the tree, r ∈ V , and turn the undirected tree to a directed one rooted in r. Consider a feasible solution p to the reformulated problem in Eq. (16) and define:\nµ̃i,pa(i)(ui, upa(i)) = ∑\nz∈U :zi,zpa(i)=ui,upa(i)\np(z) ∀(ui, upa(i)) ∈ X̄i × X̄pa(i)\nµ̃i(ui) = ∑\nz∈U :zi=ui\np(z) ∀ui ∈ X̄i\nIt is simple to prove that µ̃ ∈ML(U), because for any pair ij ∈ E it holds that:∑ uj∈X̄j µ̃i,j(ui, uj) = ∑ uj∈X̄j ∑ z∈U :zi,zj=ui,uj p(z) = ∑ z∈U :zi=ui p(z) = µ̃i(ui).\nAnd from p’s feasibility we also get µ̃ ≤ µ. This can be seen from inequalities of the following type: µ̃i,j(ui, uj) = ∑ z∈U :zi,zj=ui,uj p(z) ≤ µij(ui, uj).\nWe conclude that µ̃ is a feasible solution to Eq. (18) with objective: Z(µ̃) = ∑ ur∈X̄r µ̃r(zr) = ∑ ur∈X̄r ∑ z∈U :zr=ur p(z) = p(U).\nThis mapping only considered the first problem in Eq. (17). We can use the exact same construction when considering U \\ x as follows. Feasible solutions to Eq. (16) are functions p : U \\ x → R+, so extending p’s domain to U by setting p(x) = 0, the above equations remain unaltered. It is left to show that the resulting µ̃ satisfies I(x; µ̃) ≤ 0. If we examine the term I(x; µ̃), when di is the degree of node i in the graph, we get that:∑\ni (1− di)µ̃i(xi) + ∑ ij µ̃ij(xi, xj) = ∑ u∈U αup(u),\nαu , ∑ i Iui=xi − ∑ ij I(ui=xi)∨(uj=xj).\nSimple counting arguments show that αx = 1, while αu ≤ 0 for all u 6= x. Since we set p(x) = 0, it follows that ∑ u∈U αup(u) ≤ 0 and also I(x; µ̃).\nIt is left to provide a mapping from solutions of Eq. (18) to solutions of Eq. (17). We will provide a proof for the case where U = { u | ui ∈ X̄i ∀i ∈ [n] } .\nMore specifically, we will construct a function p : U → R+ whose marginals are µ̃ and summing it over all of its domain gives Z(µ̃). The construction is the same one used when proving that the local marginal polytope is equal to the marginal polytope for tree graphs [31]. To complete the proof, we will also need to show a construction when p’s domain is U \\ x (and U defined the same as above). We refer the reader to [10] where this detailed construction can be found. There the sum of p over its domain is 1, yet applying this construction to µ̃ gives a function that sums up to Z(µ̃).\nThe function p we suggest for the problem over domain U is:\np(u) = µ̃r(ur) ∏ i6=r µ̃i,pa(i)(ui, upa(i)) µ̃pa(i)(ui) .\nAssume r is set arbitrarily and 1, . . . , n is a topological ordering of the nodes. Notice that any choice of r and an ordering yields the same function p. It is simple to see that the function marginalizes to µ̃ if we let ij ∈ E, set i as the root and eliminate all variables other than i, j. To show that p’s sum over its domain U is exactly the partition function, eliminate all the variables to get:∑ x∈U p(x) =\n∑ u1∈X̄1 µ̃1(u1)  ∑ u2∈X̄2 µ̃2,pa(2)(u2, upa(2)) µ̃pa(2)(u2) . . .  ∑ un∈X̄n µ̃n,pa(n)(un, upa(n)) µ̃pa(n)(upa(n))  = ∑ u1∈X̄1 µ̃1(u1).\nHere we implicitly numbered the root node as 1. To conclude, we showed a mapping from µ̃ to a function p that is feasible for Eq. (17), completing the proof.\nFor the case U \\ x, as stated earlier, [10] offer a construction of a function that marginalizes to µ̃ and achieves p(x) = I(x ; µ). Thus enforcing I(x ; µ) ≤ 0 ensures there is a mapping from µ̃ to a function p with the same objective.\nNotice the equality in the above equation holds because of U ’s special structure that includes all the assignments that take values in sets X̄i. Different choices of U do not necessarily yield this equation, thus the theorem does not hold for all choices of U ."
    }, {
      "heading" : "E Proof of Thm. 4.1",
      "text" : "We recall the problem at hand of minimizing conditional probabilities:\nmin p∈P(µ)\np(xh | xo),\nwhere we assume w.l.o.g that xh = x1, . . . , xm are hidden variables, xo = xm+1, . . . , xn are observed, and x is the fixed assignment to both. Using the Charnes-Cooper variable transformation [4] between p(zh, zo) and\np(zh,zo) p(xo) for all z, and taking the dual of the resulting LP, we arrive at the following problem:\nmax λx (19) s.t. λr(zr) + ∑ i 6=r λi,pa(i)(zi, zpa(i)) + λi(zi) ≤ 0 ∀z : zo 6= xo\nλr(zr) + ∑ i 6=r λi,pa(i)(zi, zpa(i)) + λi(zi) ≤ −λx ∀z : zo = xo, zh 6= xh,\nλr(xr) + ∑ i 6=r λi,pa(i)(xi, xpa(i)) + λi(xi) ≤ 1− λx\nλ · µ ≥ 0.\nThe transformation is correct under the assumption that p(xo) > 0, which is reasonable to assume when we observe xo and try to infer xh.\nThe rest of the proof can now be decomposed into two main parts, one manipulates Eq. (19) and the other manipulates the second problem in Eq. (18): Lemma E.1. Let U be a set of the shape defined in Eq. (6) of the paper and µ a vector of tree shaped marginals. If\nmax p∈P(µ) ∑ u∈U p(u) > max p∈P(µ) ∑ u∈U\\x p(u), (20)\nthen it holds that:\nmax µ̃∈ML(U),µ̃≤µ\nI(x ; µ̃)≤0\nZ(µ̃) = max µ̃∈ML(U),µ̃≤µ−Ix\nI(x ; µ̃)=0\nZ(µ̃).\nLemma E.2. Eq. (19) has the same optimal value as: min µx (21)\ns.t. µ̃ ∈MhL, 0 ≤ µ̃ ≤ τµµh − µxIx µoτµ ≥ 1∑ zi µ̃i(zi) = τ̃ ∀i ∈ h\nµx + τ̃ = 1\nI(xh; µ̃) + (1− |Ph|)τ̃ ≤ 0 τµI(x;µ)− µx − I(xh; µ̃) + (|Ph| − 1)τ̃ ≤ 0.\nThe decision variables in in Eq. (21) are µ̃, τ̃ , τµ, µx, where µ̃ are pseudo-marginals on hidden variables and pairs of them that are connected by an edge. This form is very similar to that of problems in Eq. (18), and indeed their solutions are similar. Using Lem. E.1, we will show that a simple modification to the solution of the second problem in Eq. (18) leads to a solution of Eq. (21). This modification is shown in the following two lemmas, that also conclude the proof of Thm. 4.1. For now we assume the correctness of Lem. E.2 and Lem. E.1, their proofs are deferred to the end of this document.\nTo fit our problem into the formulation of Lem. E.1, define U using X̄i = {xi} for all observed variables i ∈ o and X̄j unrestricted for all hidden variables j ∈ h. Under this definition we have:\nmax p∈P(µ) ∑ u∈U p(u) = max p∈P(µ) p(xo),\nmax p∈P(µ) ∑ u∈U\\x p(u) = max p∈P(µ) ∑ zh 6=xh p(xo, zh).\nWe are now ready to use the above lemmas and conclude the proof. Lemma E.3. If I(x ; µ) ≤ 0 then\nmin p∈P(µ)\np(xh | xo) = 0,\nunless maxp∈P(µ) ∑ zh 6=xh p(zh,xo) = 0 and then the value is 1.\nProof. We assume that p(xo) is constrained to be larger than 0, otherwise the robust conditional probability problem is ill-defined. So it is trivial that if\nmax p∈P(µ) ∑ zh 6=xh p(xo, zh) = 0,\nthen p(x) = p(xo) and the conditional is 1. Now assume towards contradiction that minp∈P(µ) p(xh | xo) > 0, clearly we must have:\nmax p∈P(µ) ∑ u∈U p(u) > max p∈P(µ) ∑ u∈U\\x p(u),\nbecause otherwise equality must hold, so a maximizing distribution of the right hand side will have to achieve a conditional probability of 0. Then the conditions of Lem. E.1 hold and we have:\nmax p∈P(µ) ∑ zh 6=xh p(xo, zh) = max µ̃∈ML(U),µ̃≤µ\nI(x ; µ̃)=0\nZ(µ̃).\nDenote the value of the above problems as τ̃1 > 0, let µ̃1 be an optimal solution to the problem on the right hand side and µ̃1,h its sub-vector that corresponds to hidden variables and edges between them. Consider taking µ̃ = µ̃1,hτ̃1 , τ̃ = 1, µx = 0, we will show there exists a value of τµ such that µ̃, τ̃ , µx, τµ is a feasible solution to Eq. (21). The value of this solution is µx = 0, which contradicts the assumption that the minimum is strictly positive and concludes the proof.\nTo see such a value of τµ exists, note the following three points:\n• µ̃1 ∈ ML(U), µ̃1 ≤ µ and normalizes to τ̃1. So it also holds that µ̃ ∈ ML, µ̃ ≤ τ̃−11 µh, hence the first constraint of Eq. (21) is satisfied for any τµ ≥ τ̃−11 . Also from these results it is straightforward to see that the third and fourth constraints are satisfied.\n• Because we enforced p(xo) > 0, it holds that µo > 0. Thus the second constraint of Eq. (21) can also be satisfied if we take a large enough value for τµ (i.e. larger than one over the minimal item in µo).\n• Finally, we will show that\nI(xh; µ̃) + (1− |Ph|)τ̃ = 0. (22)\nThis means the fifth constraint is satisfied and more importantly, because I(x;µ) ≤ 0, the last constraint is satisfied for any positive value of τµ. To show that Eq. (22) holds, notice that:\nI(x; µ̃1) = 0,\nµ̃1,i(xi) = τ̃1 ∀i ∈ o, µ̃1,ij(xi, xj) = τ̃1 ∀(i, j) ∈ Eo, µ̃1,ij(xi, zj) = µ̃1,j(zj) ∀(i, j) ∈ Eoh, zj .\nThis first equality holds because it is a constraint in the problem that µ̃1 solves, the others because observed variables have only one possible value in U and µ̃1 ∈ ML(U). Let us write down I(x; µ̃1) and decompose the sums in its expression into smaller ones over observed and hidden variables, and to different types of edges:\nI(x; µ̃1) = ∑ i (1− di)µi(xi) + ∑ ij∈E µij(xi, xj)\n= ∑ i∈o (1− di)τ̃1 + ∑ i∈h (1− di)µ̃1,i(xi) + ∑ ij∈Eh µ̃1,ij(xi, xj)\n+ ∑\nij∈Eoh µ̃1,j(xj) + ∑ ij∈Eo τ̃1\n= 0\nSince the subgraph of observed nodes is a forest, it has |Eo| = |o|−|Po| edges. Furthermore,∑ i∈o di = |Eoh|+ 2|Eo| so we can rewrite the above expression as:\nI(x; µ̃1) = (|Po| − |Eoh|)τ̃1 + ∑ i∈h (1− dhi )µ̃1,i(xi) + ∑ ij∈Eh µ̃1,ij(xi, xj).\nNotice we also combined the summation over ij ∈ Eoh to that over i ∈ h, changing di to dhi . The entire graph being a tree, it must also hold that |Eoh| = |Ph|+ |Po| − 1. Plugging this into our expression, we get:\nI(x; µ̃1) = I(xh; µ̃1,h) + (1− |Ph|)τ̃1 = 0.\nNow because of the way we set µ̃, we arrive at:\nI(x; µ̃1)\nτ̃1 = I(xh; µ̃) + (1− |Ph|)τ̃ = 0,\nwhich gives Eq. (22).\nCombining the items above, we see that taking τµ larger than τ̃−11 and all entries of µ −1 o , gives a feasible solution as required.\nLemma E.4. If I(x;µ) > 0 then minp∈P(µ) p(xh | xo) = I(x;µ)I(x;µ)+maxp∈P(µ) ∑zh 6=xh p(zh,xo) . Proof. Obviously the right hand side is a lower bound on the minimum, we need to show there is a feasible solution that gives this bound. When I(x;µ) > 0 it is easy to see that the conditions of Lem. E.1 hold. So defining µ̃1, τ̃1 as we did in the proof of Lem. E.3, we can assume µ̃1 ≤ µ− Ix, I(xh; µ̃1) + (1− |Ph|) = 0. Now consider setting:\nτµ = 1\nI(x,µ) + τ̃1 , µ̃ = µ̃1,hτµ, τ̃ = τ̃1τµ, µx = I(x;µ)τµ.\nSince τ̃1 is defined as the value of the maximization problem in the denominator of the bound stated in the lemma, it can be seen that the value of µx is equal to this bound. So if this solution is feasible for Eq. (21), µx is also an upper bound on the robust conditional probability and it must also be the optimal value. We will simply go through each constraint in Eq. (21) and show this solution satisfies it:\n• µ̃ ∈ MhL, 0 ≤ µ̃ ≤ τµµh − µxIxh : since µ̃1 ∈ ML(U) and linear constraints stay satisfied after multiplying all variables by a positive scalar, we have µ̃ ∈MhL. Satisfaction of capacity constraints is also a direct consequence of µ̃1 satisfying capacity constraints: µ̃ = µ̃1,hτµ ≤ (µh − Ix)τµ = τµµh − µxIxh .\n• µi(xi)τµ ≥ 1 ∀i ∈ o, µij(xi, xj)τµ ≥ 1 ∀ij ∈ Eo: Notice that µ̃1 also has components for observed variables i ∈ o that satisfy τ̃1 = µ̃1,i(xi) ≤ µi(xi) − I(x;µ) and τ̃1 = µ̃1,ij(xi, xj) ≤ µij(xi, xj)− I(x;µ) for ij ∈ Eo. This gives us the constraints easily:\nτ̃1 + I(x;µ) = 1\nτµ ≤ µi(xi) ∀i ∈ o,\nand the same holds for every ij ∈ Eo. • ∑ zi µ̃i(zi) = τ̃ ∀i ∈ h, µx + τ̃ = 1: Easy to see from our setting of µ̃, τ̃ , µx, because\nµ̃1 normalizes to τ̃1.\n• I(xh; µ̃) + (1 − |Ph|)τ̃ ≤ 0, τµI(x;µ) − µx − I(xh; µ̃) + (|Ph| − 1)τ̃ ≤ 0: Using I(xh; µ̃) + (1− |Ph|)τ̃ = 0 (this was proved in the proof of Lem. E.3) and because we set µx = I(x;µ)τµ, it is easy to confirm these two constraints are satisfied.\nWe are left with the task of proving Lem. E.2 and Lem. E.1, this is the topic of the next section.\nE.1 Proofs of Lem. E.2 and Lem. E.1\nThe problem we are concerned with, Eq. (19), has an exponential number of constraints. We will see shortly that these constraints can be treated as constraints on the value of 2nd-best MAP problems [10], one over the tree shaped field λ(z) and the other over the forest shaped λ(zh,xo). To prove our results we will use a relaxation of these problems. Specifically, we will use the tightness of this relaxation in trees and forests to switch these constraints with a polynomially sized set, that is easier to handle analytically. Hence we turn to derive the set of linear constraints, this is done in a very similar manner to the derivation in [11].\nE.1.1 Second Best MAP using Dual Decomposition\nAs proved by the authors in [10], the 2nd-best MAP problem over a field λ(z), with excluded assignment x can be written as follows:\nmax µ̃\nλ · µ̃\ns.t. µ̃ ∈ML, Ĩ(x ; µ̃) ≤ |P | − 1,\nwhere |P | is the number of connected components. This is in fact a relaxation of the 2nd-best MAP problem, but it is exact when the graph is a tree or a forest. The dual of this problem is:\nmin δ,δx ∑ i δi + ∑ ij δij + (|P | − 1)δx\ns.t. λi(zi) + ∑ j δji(zi) + (di − 1)δxIzi=xi ≤ δi ∀i, zi\nλij(zi, zj)− δji(zi)− δij(zj)− δxIzi,zj=xi,xj ≤ δij ∀ij, (zi, zj) δx ≥ 0\nAt the optimum, δi, δij will just be equal to the maximum of the left hand side over different values of zi, zj (since the problem is a minimization problem), hence we can solve:\nmin δ,δx≥0 ∑ i max zi λi(zi) +∑ j δji(zi) + (di − 1)δxIzi=xi + ∑ ij max zi,zj { λij(zi, zj)− δji(zi)− δij(zj)− δxIzi,zj=xi,xj } + (|P | − 1)δx\nTo formulate a set of linear constraints that are satisfied if and only if this MAP value is smaller than a constant c, we can use auxiliary variables and a polynomial number of constraints, as done in [8]:∑\ni αi + ∑ ij αij + (|P | − 1)δx ≤ c (23)\nλi(zi) + ∑ j δji(zi) + (di − 1)δxIzi=xi ≤ αi ∀i, zi\nλij(zi, zj)− δji(zi)− δij(zj)− δxIzi,zj=xi,xj ≤ αij ∀ij, (zi, zj) δx ≥ 0.\nIn the next section we will place these constraints in Eq. (19) and move back to its own dual, after some manipulation this will give us Lem. E.2.\nE.1.2 Concluding the Proofs\nProof of Lem. E.2. Consider Eq. (19). Because we know that the optimal value of λx is in the segment [0, 1], this problem can be written as:\nmax λx (24) s.t. max\nz 6=x λ(z) ≤ 0\nmax zh 6=xh,zo=xo\nλ(z) ≤ −λx\nλ(xr) + ∑ i6=r λi,pa(i)(xi, xpa(i)) + λi(xi) ≤ 1− λx\nλ · µ ≥ 0.\nBegin by writing the full dual problem, where we plug the liner constraints described in Eq. (23) instead of the first two constraints in Eq. (24). The first 4 constraints are received by replacing the\nfirst 2nd-best MAP in Eq. (24), while the 4 constraints after these are for the second 2nd-best MAP in Eq. (24). On the right hand side we assign dual variables to each of the constraints:\nmax λx s.t. ∑ i αi + ∑ ij αij ≤ 0 τ̄\nλi(zi) + ∑ j δ̄ji(zi) + (di − 1)δ̄xIzi=xi ≤ αi ∀i, zi µ̄i(zi) λij(zi, zj)− δ̄ji(zi)− δ̄ij(zj)− δ̄xIzi,zj=xi,xj ≤ αij ∀ij, (zi, zj) µ̄ij(zi, zj) δ̄x ≥ 0∑ i∈h βi + ∑ ij∈Eh βij + (|Ph| − 1)δ̃x ≤ −λx − ∑ ij∈Eo λij(xi, xj)− ∑ i∈o λi(xi) τ̃\nλi(zi) + ∑ j∈o λji(xj , zi) + ∑ j∈h δ̃ji(zi) + (d h i − 1)δ̃xIzi=xi ≤ βi ∀i ∈ h, zi µ̃i(zi) λij(zi, zj)− δ̃ji(zi)− δ̃ij(zj)− δ̃xIzi,zj=xi,xj ≤ βij ∀ij ∈ Eh, (zi, zj) µ̃ij(zi, zj) δ̃x ≥ 0 λr(xr) + ∑ i6=r λi,pa(i)(xi, xpa(i)) + λi(xi) ≤ 1− λx µx λ · µ ≥ 0 τµ\nBecause we assume (V,E) is connected, the coefficient of δ̄x in the first constraint is 0 and this variable does not appear in the constraint. Yet the subgraph of hidden variables might not be connected. Recall we denoted its number of connected components by |Ph|, this explains the coefficient of δ̄x in the fifth consraint. Now we take the dual of the above and get the problem:\nmin µx\ns.t. µx + τ̃ = 1 λx µ̄i(zi) + µ̃i(zi)− µi(zi)τµ + Izi=xiµx = 0 ∀i ∈ h, zi λi(zi), i ∈ h µ̄ij(zi, zj) + µ̃ij(zi, zj)− µij(zi, zj)τµ + Izi,zj=xi,xjµx = 0 ∀ij ∈ Eh, (zi, zj) λij(zi, zj) µ̄i(zi) + Izi=xi(τ̃ + µx)− µi(zi)τµ = 0 ∀i ∈ o, zi λi(zi), i ∈ o µ̄ij(zi, zj) + Izi,zj=xi,xj (τ̃ + µx)− µij(zi, zj)τµ = 0 ∀ij ∈ Eo, (zi, zj) λij(zi, zj) µ̄ij(zi, zj) + Izj=xj (µ̃i(zi) + Izi=xiµx)− µij(zi, zj)τµ = 0 ∀ij ∈ Eho, (zi, zj) λij(zi, zj)∑ zj µ̄ij(zi, zj) = µ̄i(zi) ∀ij ∈ E, zi δ̄ji(zi)∑\nzj µ̃ij(zi, zj) = µ̃i(zi) ∀ij ∈ Eh, zi δ̃ji(zi)∑\nzi µ̄i(zi) = τ̄ ∀i αi∑\nzi µ̃i(zi) = τ̃ ∀i βi∑ i (1− di)µ̄i(xi) + ∑ ij µ̄ij(xi, xj) ≤ 0 δ̄x∑\ni (1− dhi )µ̃i(xi) + ∑ ij µ̃ij(xi, xj) + (1− |Ph|)τ̃ ≤ 0 δ̃x\nAll variables in the problem are constrained to be non negative as well. The right column denotes the primal variables that each dual constraint corresponds to, in the third row these variables are λij for ij ∈ Eh, while in the fifth and sixth they are for ij ∈ Eo and Eho respectively. Notice that we can simplify the problem by using the second to sixth equality constraints and eliminate variables µ̄. Local consistency constraints for µ̄:∑\nzj\nµ̄ij(zi, zj) = µ̄i(zi) ∀ij ∈ E, zi,\nwill be satisfied because of µ̃ and µ’s local consistency, while normalization constraints:∑ zi µ̄i(zi) = τ̄ ∀i,\nare also satisfied because µ̃ normalizes to τ̃ . Combining the above switch of variables into the constraint Ĩ(x ; µ̄) ≤ 0, it becomes:\nτµĨ(x ; µ)− µx − Ĩ(xh ; µ̃) + ( ∑ i∈o (di − 1)− |Eo|)τ̃ ≤ 0.\nWe already showed in the proof of Lem. E.3 that the term ∑ i∈o (di − 1)− |Eo| is equal to |Ph| − 1, turning the above constraint to:\nτµĨ(x ; µ)− µx − Ĩ(xh ; µ̃) + (|Ph| − 1)τ̃ ≤ 0.\nSo we end up with the following problem:\nmin µx s.t. µx + τ̃ = 1\nµ̃i(zi)− µi(zi)τµ + Izi=xiµx ≤ 0 ∀i ∈ h, zi µ̃ij(zi, zj)− µij(zi, zj)τµ + Izi,zj=xi,xjµx ≤ 0 ∀ij ∈ Eh, (zi, zj) µi(xi)τµ ≥ 1 ∀i ∈ o µij(xi, xj)τµ ≥ 1 ∀ij ∈ Eo µ̃i(zi) + Izi=xiµx − µij(zi, xj)τµ ≤ 0 ∀ij ∈ Eho∑ zj\nµ̃ij(zi, zj) = µ̃i(zi) ∀ij ∈ Eh, zi∑ zi µ̃i(zi) = τ̃ ∀i ∈ h\nτµI(x ; µ)− µx − I(xh ; µ̃) + (|Ph| − 1)τ̃ ≤ 0 I(xh; µ̃) + (1− |Ph|)τ̃ ≤ 0\nSimplifying notation using the vectors µh, Ix,µo that we defined in Section B, the problem takes the shape of Eq. (21)\nProof of Lem. E.2. From Thm. 4.2 we know that:\nmax µ̃∈ML(U),µ̃≤µ Z(µ̃) = max p∈P(µ) ∑ u∈U p(u),\nmax µ̃∈ML(U),µ̃≤µ\nI(x ; µ̃)≤0\nZ(µ̃) = max p∈P(µ) ∑ u∈U\\x p(u).\nNow for each i, (i, j) ∈ E, consider replacing constraints in P(µ) as follows:∑ z:zi,zj=xi,xj p(z) = µij(xi, xj)→ ∑\nz:zi,zj=xi,xj , z 6=x\np(z) ≤ µij(xi, xj)− I(x,µ),\n∑ z:zi=xi p(z) = µi(xi)→ ∑\nz:zi=xi z 6=x\np(z) ≤ µi(xi)− I(x,µ).\nWe will denote this set by P̃(µ). Since for any p ∈ P(µ) we know that p(x) ≥ I(x,µ), it holds that P(µ) ⊆ P̃(µ), which means the maximum of the new problem is higher than that of the original for both problems (on U and U \\ x):\nmax p∈P(µ) ∑ u∈U p(u) ≤ max p∈P̃(µ) ∑ u∈U p(u)\nmax p∈P(µ) ∑ u∈U\\x p(u) ≤ max p∈P̃(µ) ∑ u∈U\\x p(u)\nTaking the dual of this new problem on U \\ x we obtain:\nmin λ\nλ · (µ− Ix)\ns.t. λ(z) ≥ 1 ∀z ∈ U \\ x λ(z) ≥ 0 ∀z /∈ U λij(xi, xj) ≥ 0, λi(xi) ≥ 0 ∀i ∈ V, (i, j) ∈ E\nFrom the result in Cor. C.1, we can consider the variables to be non-negative (i.e. λ ≥ 0), the second constraint is redundant and can be removed. Furthermore, the first constraint is in fact a constraint on the value of the 2nd-best MAP problem on −λ(z) (i.e. minimization of λ(z) while excluding x).\nAdapting the constraints in Eq. (23) to a minimization problem and switching into our problem we get:\nmin λ≥0,δx≥0,α,δ\nλ · (µ− Ix) (25)\ns.t. ∑ i αi + ∑ ij αij ≥ 1\nλi(zi) + ∑ j δji(zi) + (1− di)δxIzi=xi ≥ αi ∀i, zi ∈ X̄i\nλij(zi, zj)− δji(zi)− δij(zj) + δxIzi,zj=xi,xj ≥ αij ∀ij, (zi, zj) ∈ X̄i × X̄j .\nTaking the dual of this problem, it is easy to see it equals to:\nmax µ̃∈ML(U),µ̃≤µ−Ix\nI(x ; µ̃)≤0\nZ(µ̃).\nThe constraints of this problem are more strict than the ones in the original, therefore its value is lower:\nmax p∈P(µ) ∑ u∈U\\x p(u) = max µ̃∈ML(U),µ̃≤µ\nI(x ; µ̃)≤0\nZ(µ̃) ≥ max µ̃∈ML(U),µ̃≤µ−Ix\nI(x ; µ̃)≤0\nZ(µ̃) = max p∈P̃(µ) ∑ u∈U\\x p(u).\nWe gather that an equality must hold:\nmax p∈P(µ) ∑ u∈U\\x p(u) = max p∈P̃(µ) ∑ u∈U\\x p(u) = max µ̃∈ML(U),µ̃≤µ−Ix\nI(x ; µ̃)≤0\nZ(µ̃).\nTo complete the proof we need to show the existence a solution µ̃ that is optimal for the problem on the right hand side and satisfies I(x ; µ̃) = 0. Then assume towards contradiction that Eq. (20) holds and there is no optimal solution where I(x ; µ̃) = 0. Since the problem is feasible, some optimal solution µ∗ does exist and from complementary slackness, there is a corresponding solution λ∗, 0,α∗, δ∗ to Eq. (25). Since the value of δx is 0, then λ∗,α∗, δ∗ is also a feasible solution to the dual of:\nmax p∈P̃(µ) ∑ u∈U p(u),\nwhich means λ∗ · (µ − Ix) is an upper bound on this problem. To conclude, we concatenate the inequalities we have so far:\nmax p∈P(µ) ∑ u∈U p(u) ≤ max p∈P̃(µ) ∑ u∈U p(u) ≤ λ∗ · (µ− Ix) = max p∈P(µ) ∑ u∈U\\x p(u).\nThis inequality contradicts the hard inequality we assumed at the statement of the lemma, therefore there exists an optimal solution where I(x ; µ̃) = 0 and we can incorporate this equality into the constraints without changing the value of the problem."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Conditional probabilities are a core concept in machine learning. For ex-<lb>ample, optimal prediction of a label Y given an inputX corresponds to maximizing<lb>the conditional probability of Y given X . A common approach to inference tasks<lb>is learning a model of conditional probabilities. However, these models are often<lb>based on strong assumptions (e.g., log-linear models), and hence their estimate of<lb>conditional probabilities is not robust and is highly dependent on the validity of<lb>their assumptions.<lb>Here we propose a framework for reasoning about conditional probabilities without<lb>assuming anything about the underlying distributions, except knowledge of their<lb>second order marginals, which can be estimated from data. We show how this<lb>setting leads to guaranteed bounds on conditional probabilities, which can be calcu-<lb>lated efficiently in a variety of settings, including structured-prediction. Finally, we<lb>apply them to semi-supervised deep learning, obtaining results competitive with<lb>variational autoencoders.",
    "creator" : "LaTeX with hyperref package"
  }
}