{
  "name" : "1402.0288.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Transductive Learning with Multi-class Volume Approximation",
    "authors" : [ "Gang Niu", "Masashi Sugiyama" ],
    "emails" : [ "gang@sg.cs.titech.ac.jp", "bohr.dai@gmail.com", "christo@sg.cs.titech.ac.jp", "sugi@cs.titech.ac.jp" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The history of the large volume principle (LVP) goes back to the early age of the statistical learning theory when Vapnik (1982) introduced it for the case of hyperplanes. But it did not gain much attention until a creative approximation was proposed in El-Yaniv et al. (2008) to implement LVP for the case of soft response vectors. From then on, it has been applied to various binary learning problems successfully, such as binary transductive\nar X\niv :1\n40 2.\n02 88\nlearning (El-Yaniv et al., 2008), binary clustering (Niu et al., 2013a), and outlier detection (Li and Ng, 2013).\nLVP is a learning-theoretic principle which views learning as hypothesis selecting from a certain hypothesis space H. Despite the form of the hypothesis, H can always be partitioned into a finite number of equivalence classes after we observe certain data, where an equivalence class is a set of hypotheses that generate the same labeling of the observed data. LVP, as one of the learning-theoretic principles from the statistical learning theory, prioritizes those equivalence classes according to the volume they occupy in H. See the illustration in Figure 1: The blue ellipse represents H, and it is partitioned into C1, . . . , C4 each occupying a quadrant of the Cartesian coordinate system R2 intersected with H; LVP claims that C1 and C3 are more preferable than C2 and C4, since C1 and C3 have larger volume than C2 and C4.\nIn practice, the hypothesis space H cannot be as simple as in Figure 1. It frequently locates in very high-dimensional spaces where exact or even quantifiable volume estimation is challenging. Therefore, El-Yaniv et al. (2008) proposed a volume approximation to bypass the volume estimation. Instead of focusing on the equivalence classes of H, it directly focuses on the hypotheses in H since learning is regarded as hypothesis selecting in LVP. It defines H via an ellipsoid, measures the angles from hypotheses to the principal axes of H, and then prefers hypotheses near the long principal axes to those near the short ones. This manner is reasonable, since the long principal axes of H lie in large-volume regions. In Figure 1, h and h′ are two hypotheses and v1/v2 is the long/short principal axis; LVP advocates that h is more preferable than h′ as h is close to v1 and h\n′ is close to v2. We can adopt this volume approximation to regularize our loss function, which has been demonstrated helpful for various binary learning problems.\nNevertheless, the volume approximation in El-Yaniv et al. (2008) only fits binary learning problem settings in spite of its potential advantages. In this paper, we naturally extend it to a more general definition that can be applied to some transductive problem settings including but not limited to multi-class learning (Zhou et al., 2003), multi-label learning (Kong et al., 2013), and serendipitous learning (Zhang et al., 2011). We adopt the same strategy as El-Yaniv et al. (2008): For n data and c labels, a hypothesis space is defined in Rn×c and linked to an ellipsoid in Rnc, such that the equivalence classes and the volume approximation can be defined accordingly. Similarly to the binary volume approximation, our approach is also distribution free, that is, the labeled and unlabeled data do not necessarily share the same marginal distribution. This advantage of transductive learning over (semi-supervised) inductive learning is especially useful for serendipitous problems where the labeled and unlabeled data must not be identically distributed.\nWe name the learning method which realizes the proposed multi-class volume approximation multi-class approximate volume regularization (MAVR). It involves a non-convex optimization problem, but the globally optimal solution is almost surely unique and accessible in O(n3) time following Forsythe and Golub (1965). Moreover, we theoretically provide stability and error analyses for MAVR, as well as experimentally compare it to two state-of-the-art methods in Zhou et al. (2003) and Belkin et al. (2006) using USPS, MNIST, 20Newsgroups and Isolet.\nThe rest of this paper is organized as follows. In Section 2 the binary volume approximation is reviewed, and in Section 3 the multi-class volume approximation is derived. In Section 4, we develop and analyze MAVR. Finally, the experimental results are in Section 5."
    }, {
      "heading" : "2 Binary Volume Approximation",
      "text" : "The binary volume approximation in El-Yaniv et al. (2008) involves a few key concepts: The soft response vector, the hypothesis space and the equivalence class, and the power and volume of equivalence classes. We review the concepts in this section for later use in the next section.\nSuppose that X is the domain of input data, and most often but not necessarily, X ⊂ Rd where d is a natural number. Given a set of n data Xn = {x1, . . . , xn} where xi ∈ X , a soft response vector is an n-dimensional vector\nh := (h1, . . . , hn) > ∈ Rn, (1)\nso that hi stands for a soft or confidence-rated label of xi. For binary transductive learning problems, a soft response vector h suggests that xi is from the positive class if hi > 0, xi is from the negative class if hi < 0, and the above two cases are equally possible if hi = 0.\nA hypothesis space is a collection of hypotheses. The volume approximation requires a symmetric positive-definite matrix Q ∈ Rn×n which contains the pairwise information about Xn. Consider the hypothesis space\nHQ := {h | h>Qh ≤ 1}, (2)\nwhere the hypotheses are soft response vectors. The set of sign vectors {sign(h) | h ∈ HQ} contains all of N = 2n possible dichotomies of Xn, and HQ can be partitioned into a finite number of equivalence classes C1, . . . , CN , such that for fixed k, all hypotheses in Ck will generate the same labeling of Xn.\nThen, in statistical learning theory, the power of an equivalence class Ck is defined as the probability mass of all hypotheses in it (Vapnik, 1998, p. 708), i.e.,\nP(Ck) := ∫ Ck p(h)dh, k = 1, . . . , N,\nwhere p(h) is the underlying probability density of h over HQ. The hypotheses in Ck which has a large power should be preferred according to Vapnik (1998).\nWhen no specific domain knowledge is available (i.e., p(h) is unknown), it would be natural to assume the continuous uniform distribution p(h) = 1/ ∑N k=1 V(Ck), where\nV(Ck) := ∫ Ck dh, k = 1, . . . , N,\nis the volume of Ck. That is, the volume of an equivalence class is defined as the geometric volume of all hypotheses in it. As a result, P(Ck) is proportional to V(Ck), and the larger the value V(Ck) is, the more confident we are of the hypotheses chosen from Ck.\nHowever, it is very hard to accurately compute the geometric volume of even a single convex body in Rn, let alone all 2n convex bodies, so El-Yaniv et al. (2008) introduced an efficient approximation. Let λ1 ≤ · · · ≤ λn be the eigenvalues of Q, and v1, . . . ,vn be the associated orthonormal eigenvectors. Actually, the hypothesis space HQ in Eq. (2) is geometrically an origin-centered ellipsoid in Rn with vi and 1/ √ λi as the direction and length of its i-th principal axis. Note that a small angle from a hypothesis h in Ck to some vi with a small/large index i (i.e., a long/short principal axis) implies that V(Ck) is large/small (cf. Figure 1). Based on this crucial observation, we define\nV (h) := n∑ i=1 λi ( h>vi ‖h‖2 )2 = h>Qh ‖h‖22 , (3)\nwhere h>vi/‖h‖2 means the cosine of the angle between h and vi. We subsequently expect V (h) to be small when h lies in a large-volume equivalence class, and conversely to be large when h lies in a small-volume equivalence class."
    }, {
      "heading" : "3 Multi-class Volume Approximation",
      "text" : "In this section, we propose a more general multi-class volume approximation that fits for several problem settings."
    }, {
      "heading" : "3.1 Problem settings",
      "text" : "Recall the setting of binary transductive problems (Vapnik, 1998, p. 341). A fixed set Xn = {x1, . . . , xn} of n points from X is observed, and the labels y1, . . . , yn ∈ {−1,+1} of these points are also fixed but unknown. A subset Xl ⊂ Xn of size l is picked uniformly at random, and then yi is revealed if xi ∈ Xl. We call Sl = {(xi, yi) | xi ∈ Xl} the labeled data and Xu = Xn \\Xl the unlabeled data. Using Sl and Xu, the goal is to predict yi of xi ∈ Xu (while any unobserved x ∈ X \\Xn is currently left out of account).\nA slight modification suffices to extend the setting. Instead of y1, . . . , yn ∈ {−1,+1}, we assume that y1, . . . , yn ∈ Y where Y = {1, . . . , c} is the domain of labels and c is a natural number. Though the binary setting is popular, this multi-class setting has been studied in just a few previous works such as Szummer and Jaakkola (2001) and Zhou et al. (2003). Without loss of generality, we assume that each of the c labels possesses some labeled data.\nIn addition, it would be a multi-label setting, if y1, . . . , yn ⊆ Y with Y = {1, . . . , c} where each yi is a label set, or if y1, . . . , yn ∈ Y with Y = {−1, 0, 1}c where each yi is a label vector. To the best of our knowledge, the former setting has been studied only in Kong et al. (2013) and the latter setting has not been studied yet. The latter setting is more general, since the former one requires labeled data to be fully labeled, while the latter one allows labeled data to be partially labeled. A huge challenge of multi-label problems is that some label sets or label vectors might have no labeled data (Kong et al., 2013), since there are 2c possible label sets and 3c possible label vectors.\nA more challenging serendipitous setting which is a multi-class setting but some labels have no labeled data has been studied in Zhang et al. (2011). Let Yl = {yi | xi ∈ Xl} and Yu = {yi | xi ∈ Xu, yi 6∈ Yl}, then we have #Yu ≥ 1 where # measures the cardinality. It is still solvable when #Yu = 1 if a special label of outliers is allowed and when #Yu > 1 as a combination of classification and clustering problems. Zhang et al. (2011) is the unique previous work which successfully dealt with #Yu = 2 and #Yu = 3."
    }, {
      "heading" : "3.2 Definitions",
      "text" : "The multi-class volume approximation to be proposed can handle all the problem settings discussed so far in a unified manner. In order to extend the binary definitions, we need only to extend the hypothesis and the hypothesis space.\nTo begin with, we allocate a soft response vector in Eq. (1) for each of the c labels:\nh1 = (h1,1, . . . , hn,1) >, . . . ,hc = (h1,c, . . . , hn,c) >.\nThe value hi,j is a soft or confidence-rated label of xi concerning the j-th label and it suggests that\n• xi should possess the j-th label, if hi,j > 0; • xi should not possess the j-th label, if hi,j < 0; • the above two cases are equally possible, if hi,j = 0.\nFor multi-class and serendipitous problems, yi is predicted by ŷi = arg maxj hi,j. For multi-label problems, we need a threshold Th that is either preset or learned since usually positive and negative labels are imbalanced, and yi can be predicted by ŷi = {j | hi,j ≥ Th}; or we can use the label set prediction methods proposed in Kong et al. (2013).\nThen, a soft response matrix as our transductive hypothesis is an n-by-c matrix defined by H = (h1, . . . ,hc) ∈ Rn×c, (4) and a stacked soft response vector as an equivalent hypothesis is an nc-dimensional vector defined by\nh = vec(H) = (h>1, . . . ,h > c) > ∈ Rnc,\nwhere vec(H) is the vectorization of H formed by stacking its columns into a single vector. As the binary definition of the hypothesis space, a symmetric positive-definite matrix Q ∈ Rn×n which contains the pairwise information about Xn is provided, and we assume further that a symmetric positive-definite matrix P ∈ Rc×c which contains the pairwise information about Y is available. Consider the hypothesis space\nHP,Q := {H | tr(H>QHP ) ≤ 1}, (5)\nwhere the hypotheses are soft response matrices. Let P ⊗ Q ∈ Rnc×nc be the Kronecker product of P and Q. Due to the symmetry and the positive definiteness of P and Q, the Kronecker product P ⊗Q is also symmetric and positive definite, and HP,Q in (5) could be defined equivalently as\nHP,Q := {H | vec(H)>(P ⊗Q) vec(H) ≤ 1}. (6)\nThe equivalence of Eqs. (5) and (6) comes from the fact that tr(H>QHP ) = vec(H)>(P ⊗ Q) vec(H) following the well-known identity (see, e.g., Theorem 13.26 of Laub, 2005)\n(P>⊗Q) vec(H) = vec(QHP ).\nAs a consequence, there is a bijection between HP,Q and\nEP,Q := {h | h>(P ⊗Q)h ≤ 1}\nwhich is geometrically an origin-centered ellipsoid in Rnc. The set of sign vectors {sign(h) | h ∈ EP,Q} spreads over all the N = 2nc quadrants of Rnc, and thus the set of sign matrices {sign(H) | H ∈ HP,Q} contains all of N possible dichotomies of Xn×{1, . . . , c}. In other words, HP,Q can be partitioned into N equivalence classes C1, . . . , CN , such that for fixed k, all soft response matrices in Ck will generate the same labeling of Xn × {1, . . . , c}.\nThe definition of the power is same as before, and so is the definition of the volume: V(Ck) := ∫ Ck dH, k = 1, . . . , N.\nBecause of the bijection between HP,Q and EP,Q, V(Ck) is likewise the geometric volume of all stacked soft response vectors in the intersection of the k-th quadrant of Rnc and EP,Q. By a similar argument to the definition of V (h), we define\nV (H) := h>(P ⊗Q)h ‖h‖22 = tr(H>QHP ) ‖H‖2Fro , (7)\nwhere h = vec(H) and ‖H‖Fro means the Frobenius norm of H. We subsequently expect V (H) to be small when H lies in a large-volume equivalence class, and conversely to be large when H lies in a small-volume equivalence class.\nNote that V (H) and V (h) are consistent for binary learning problem settings. We can constrain h1 + h2 = 0n if c = 2 where 0n is the all-zero vector in Rn. Let P = I2 where I2 is the identity matrix of size 2, then\nV (H) = h>1Qh1 + h > 2Qh2\n‖h1‖22 + ‖h2‖22 = h>1Qh1 ‖h1‖22 = V (h1),\nwhich coincides with V (h) defined in Eq. (3). Similarly to V (h), for two soft response matrices H and H ′ from the same equivalence class, V (H) and V (H ′) may not necessarily be the same value. In addition, the domain of V (H) could be extended to Rn×c though the definition of V (H) is originally null for H outside HP,Q."
    }, {
      "heading" : "4 Multi-class Approximate Volume Regularization",
      "text" : "The proposed volume approximation motivates a family of new transductive methods taking it as a regularization. We develop and analyze an instantiation in this section whose optimization problem is non-convex but can be solved exactly and efficiently."
    }, {
      "heading" : "4.1 Model",
      "text" : "First of all, we define the label indicator matrix Y ∈ Rn×c for convenience whose entries can be from either {0, 1} or {−1, 0, 1} depending on the problem settings and whether negative labels ever appear. Specifically, we can set Yi,j = 1 if xi is labeled to have the j-th label and Yi,j = 0 otherwise, or alternatively we can set Yi,j = 1 if xi is labeled to have the j-th label, Yi,j = −1 if xi is labeled to not have the j-th label, and Yi,j = 0 otherwise.\nLet ∆(Y,H) be our loss function measuring the difference between Y and H. The multi-class volume approximation motivates the following family of transductive methods:\nmin H∈HP,Q\n∆(Y,H) + γ · tr(H >QHP )\n‖H‖2Fro ,\nwhere γ > 0 is a regularization parameter. The denominator ‖H‖2Fro is quite difficult to tackle, so we would like to eliminate it as El-Yaniv et al. (2008) and Niu et al. (2013a).\nWe fix a scale parameter τ > 0, constrain H to be of norm τ , replace the feasible region HP,Q with Rn×c by extending the domain of V (H) implicitly, and it becomes\nmin H∈Rn×c\n∆(Y,H) + γ tr(H>QHP )\ns.t. ‖H‖Fro = τ. (8)\nAlthough the optimization in (8) is done in Rn×c, the regularization is carried out relative to HP,Q, since under the constraint ‖H‖Fro = τ , the regularization tr(H>QHP ) is a weighted sum of the squares of cosines between vec(H) and the principal axes of EP,Q like El-Yaniv et al. (2008).\nSubsequently, we denote by y1, . . . ,yn and r1, . . . , rn the c-dimensional vectors that satisfy Y = (y1, . . . ,yn) > and H = (r1, . . . , rn) >. Consider the following loss functions to be ∆(Y,H) in optimization (8):\n1. Squared losses over all data ∑\nXn ‖yi − ri‖22; 2. Squared losses over labeled data ∑\nXl ‖yi − ri‖22; 3. Linear losses over all data ∑\nXn −y>iri; 4. Linear losses over labeled data ∑\nXl −y>iri;\nThe first loss function has been used for multi-class transductive learning (Zhou et al., 2003) and the binary counterparts of the fourth and third loss functions have been used for binary transductive learning (El-Yaniv et al., 2008) and clustering (Niu et al., 2013a). Actually, the third and fourth loss functions are identical since yi for xi ∈ Xu is identically zero, and the first loss function is equivalent to them in (8) since ∑ Xn ‖yi‖22 and∑\nXl ‖yi‖22 are constants and ∑ Xn ‖ri‖22 = τ 2 is also a constant. The second loss function is undesirable for (8) due to an issue of the time complexity which will be discussed later. Thus, we instantiate ∆(Y,H) := ∑ Xn ‖yi − ri‖22 = ‖Y − H‖2Fro, and optimization (8) becomes\nmin H∈Rn×c\n‖Y −H‖2Fro + γ tr(H>QHP )\ns.t. ‖H‖Fro = τ. (9)\nWe refer to constrained optimization problem (9) as multi-class approximate volume regularization (MAVR). An unconstrained version of MAVR is then\nmin H∈Rn×c\n‖Y −H‖2Fro + γ tr(H>QHP ). (10)"
    }, {
      "heading" : "4.2 Algorithm",
      "text" : "Optimization (9) is non-convex, but we can rewrite it using the stacked soft response vector h = vec(H) as\nmin h∈Rnc\n‖y − h‖22 + γh>(P ⊗Q)h\ns.t. ‖h‖2 = τ, (11)\nwhere y = vec(Y ) is the vectorization of Y . In this representation, the objective is a second-degree polynomial and the constraint is an origin-centered sphere, and fortunately we could solve it exactly and efficiently following Forsythe and Golub (1965). To this end, a fundamental property of the Kronecker product is necessary (see, e.g., Theorems 13.10 and 13.12 of Laub, 2005):\nTheorem 1. Let λQ,1 ≤ · · · ≤ λQ,n be the eigenvalues and vQ,1, . . . ,vQ,n be the associated orthonormal eigenvectors of Q, λP,1 ≤ · · · ≤ λP,c and vP,1, . . . ,vP,c be those of P , and the eigen-decompositions of Q and P be Q = VQΛQV > Q and P = VPΛPV > P . Then, the eigenvalues of P ⊗Q are λP,jλQ,i associated with orthonormal eigenvectors vP,j ⊗vQ,i for j = 1, . . . , c, i = 1, . . . , n, and the eigen-decomposition of P ⊗Q is P ⊗Q = VPQΛPQV>PQ, where ΛPQ = ΛP ⊗ ΛQ and VPQ = VP ⊗ VQ.\nAfter we ignore the constants ‖y‖22 and ‖h‖22 in the objective of optimization (11), the Lagrange function is\nΦ(h, ρ) = −2h>y + γh>(P ⊗Q)h− ρ(h>h− τ 2), where ρ ∈ R is the Lagrangian multiplier for ‖h‖22 = τ 2. The stationary conditions are\n∂Φ/∂h = −y + γ(P ⊗Q)h− ρh = 0nc, (12) ∂Φ/∂ρ = h>h− τ 2 = 0. (13)\nHence, for any locally optimal solution (h, ρ) where ρ/γ is not an eigenvalue of P ⊗ Q, we have\nh = (γP ⊗Q− ρInc)−1y (14) = VPQ(γΛPQ − ρInc)−1V>PQy = (VP ⊗ VQ)(γΛPQ − ρInc)−1 vec(V>QY VP ) (15)\nbased on Eq. (12) and Theorem 1. Next, we search for the feasible ρ for (12) and (13) which will lead to the globally optimal h. Let z = vec(V>QY VP ), then plugging (15) into (13) gives us z>(γΛPQ − ρInc)−2z − τ 2 = 0. (16) Let us sort the eigenvalues λP,1λQ,1, . . . , λP,cλQ,n into a non-descending sequence {λPQ,1, . . . , λPQ,nc}, rearrange {z1, . . . , znc} accordingly, and find the smallest k0 which satisfies zk0 6= 0. As a result, Eq. (16) implies that\ng(ρ) = nc∑ k=k0 z2k (γλPQ,k − ρ)2 − τ 2 = 0 (17)\nfor any stationary ρ. By Theorem 4.1 of Forsythe and Golub (1965), the smallest root of g(ρ) determines a unique h so that (h, ρ) is the globally optimal solution to Φ(h, ρ), i.e., h minimizes the objective of (11) globally. For this ρ, the only exception when it cannot determine h by Eq. (14) is that ρ/γ is an eigenvalue of P ⊗ Q, but this happens with probability zero. Finally, the theorem below points out the location of this ρ (the proof is in the appendix):\nAlgorithm 1 MAVR\nInput: P , Q, Y , γ and τ Output: H and ρ\n1: Eigen-decompose P and Q; 2: Construct the function g(ρ); 3: Find the smallest root of g(ρ); 4: Recover h using ρ and reshape h to H.\nTheorem 2. The function g(ρ) defined in Eq. (17) has exactly one root in the interval [ρ0, γλPQ,k0) and no root in the interval (−∞, ρ0), where ρ0 = γλPQ,k0 − ‖y‖2/τ .\nThe algorithm of MAVR is summarized in Algorithm 1. It is easy to see that fixing ρ = −1 in Algorithm 1 instead of finding the smallest root of g(ρ) suffices to solve optimization (10). Moreover, for a special case P = Ic where Ic is the identity matrix of size c, any stationary H is simply\nH = (γQ− ρIn)−1Y = VQ(γΛQ − ρIn)−1V>QY.\nLet z = V>QY 1c where 1c is the all-one vector in Rc, and k0 is the smallest number that satisfies zk0 6= 0. Then the smallest root of\ng(ρ) = ∑n\nk=k0 z2k/(γλQ,k − ρ)2 − τ 2\ngives us the feasible ρ leading to the globally optimal H. The asymptotic time complexity of Algorithm 1 is O(n3). More specifically, eigendecomposing Q in the first step of Algorithm 1 costs O(n3), and this is the dominating computation time. Eigen-decomposing P just needs O(c3) and is negligible under the assumption that n c without loss of generality. In the second step, it requires O(nc log(nc)) for sorting the eigenvalues of P ⊗Q and O(n2c) for computing z. Finding the smallest root of g(ρ) based on a binary search algorithm uses O(log(‖y‖2)) in the third step, and ‖y‖2 ≤ √ l for multi-class problems and ‖y‖2 ≤ √ lc for multi-label problems. In the final step, recovering h is essentially same as computing z and costs O(n2c). We would like to comment a bit more on the asymptotic time complexity of MAVR. Firstly, we employ the squared losses over all data rather than the squared losses over labeled data. If the latter loss function was plugged in optimization (8), Eq. (14) would become h = (γP ⊗Q− ρInc + Ic ⊗ J)−1y, where J is an n-by-n diagonal matrix such that Ji,i = 1 if xi is labeled and Ji,i = 0 if xi is unlabeled. The inverse in the expression above cannot be computed using the eigendecompositions of P and Q, and hence the computational complexity would increase from O(n3) to O(n3c3). Secondly, given fixed P and Q but different Y , γ, and τ , the computational complexity is O(n2c) if we reuse the eigen-decompositions of P and Q and the sorted eigenvalues of P ⊗ Q. This property is especially advantageous for validating\nand selecting hyperparameters. It is also quite useful for picking different Xl ⊂ Xn to be labeled following transductive problem settings. Finally, the asymptotic time complexity O(n3) can hardly be improved based on existing techniques for optimizations (9) and (10). Even if ρ is fixed in optimization (10), the stationary condition Eq. (12) is a discrete Sylvester equation which consumes O(n3) for solving it (Sima, 1996)."
    }, {
      "heading" : "4.3 Theoretical analyses",
      "text" : "We provide two theoretical results. Under certain assumptions, the stability analysis upper bounds the difference of two optimal H and H ′ trained with two different label indicator matrices Y and Y ′, and the error analysis bounds the difference of H from the ground truth.\nTheorem 2 guarantees that ρ < γλPQ,k0 . In fact, with high probability over the choice of Y , it holds that k0 = 1 (we did not meet k0 > 1 in our experiments). For this reason, we make the following assumption:\nFix P and Q, and allow Y to change according to the partition of Xn into different Xl and Xu. There is Cγ,τ > 0, which just depends on γ and τ , such that for all optimal ρ trained with different Y , ρ ≤ γλPQ,1 − Cγ,τ .\nNote that for unconstrained MAVR, there must be Cγ,τ > 1 since γλPQ,1 > 0 and ρ = −1. Based on the above assumption and the lower bound of ρ in Theorem 2, we can prove the theorem below.\nTheorem 3 (Stability of MAVR). Assume the existence of Cγ,τ . Let (H, ρ) and (H ′, ρ′) be two globally optimal solutions trained with two different label indicator matrices Y and Y ′ respectively. Then,\n‖H −H ′‖Fro ≤ ‖Y − Y ′‖Fro/Cγ,τ + |ρ− ρ′|min{‖Y ‖Fro, ‖Y ′‖Fro}/C2γ,τ . (18)\nConsequently, for MAVR in optimization (9) we have\n‖H −H ′‖Fro ≤ ‖Y − Y ′‖Fro/Cγ,τ + ‖Y ‖Fro‖Y ′‖Fro/τC2γ,τ ,\nand for unconstrained MAVR in optimization (10) we have\n‖H −H ′‖Fro ≤ ‖Y − Y ′‖Fro/Cγ,τ .\nIn order to present an error analysis, we assume there is a ground-truth soft response matrix H∗ with two properties. Firstly, the value of V (H∗) should be bounded, namely,\nV (H∗) = tr(H∗>QH∗P )\n‖H∗‖2Fro ≤ Ch,\nwhere Ch > 0 is a small number. This ensures that H ∗ lies in a large-volume region. Otherwise MAVR implementing the large volume principle can by no means learn some H close to H∗. Secondly, Y should contain certain information about H∗. MAVR makes\nuse of P , Q and Y only and the meanings of P and Q are fixed already, so MAVR may access the information about H∗ only through Y . To make Y and H∗ correlated, we assume that Y = H∗ + E where E ∈ Rn×c is a noise matrix of the same size as Y and H∗. All entries of E are independent with zero mean, and the variance of them is σl or σu depending on its correspondence to a labeled or an unlabeled position in Y . We could expect that σl σu, such that the entries of Y in labeled positions are close to the corresponding entries of H∗, but the entries of Y in unlabeled positions are completely corrupted and uninformative for recovering H∗. Notice that we need this generating mechanism of Y even if Ch/γ is the smallest eigenvalue of P ⊗Q, since P ⊗Q may have multiple smallest eigenvalues and ±H have totally different meanings. Based on these assumptions, we can prove the theorem below.\nTheorem 4 (Accuracy of MAVR). Assume the existence of Cγ,τ , Ch, and the generating process of Y from H∗ and E. Let l̃ and ũ be the numbers of the labeled and unlabeled positions in Y and assume that EE‖Y ‖2Fro ≤ l̃ where the expectation is with respect to the noise matrix E. For each possible Y , let H be the globally optimal solution trained with it. Then,\nEE‖H −H∗‖Fro ≤ ( √ ChγλPQ,1/Cγ,τ )‖H∗‖Fro\n+ ( max {√ l̃/τ − γλPQ,1 − 1, γλPQ,1 − Cγ,τ + 1 } /Cγ,τ ) ‖H∗‖Fro\n+ √ l̃σ2l + ũσ 2 u/Cγ,τ (19)\nfor MAVR in optimization (9), and\nEE‖H −H∗‖2Fro ≤ (Ch/4)‖H∗‖2Fro + l̃σ2l + ũσ2u (20)\nfor unconstrained MAVR in optimization (10).\nThe proofs of Theorems 3 and 4 are in the appendix. Considering the instability bounds in Theorem 3 and the error bounds in Theorem 4, unconstrained MAVR is superior to constrained MAVR in both cases. That being said, bounds are just bounds. We will demonstrate the potential of constrained MAVR in the next section by experiments."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we numerically evaluate MAVR."
    }, {
      "heading" : "5.1 Serendipitous learning",
      "text" : "We show how to handle serendipitous problems by MAVR directly without performing clustering (Hartigan and Wong, 1979; Ng et al., 2001; Sugiyama et al., 2014) or estimating the class-prior change (du Plessis and Sugiyama, 2012). The experimental results are\ndisplayed in Figure 2. There are 5 data sets, and the latter 3 data sets are from ZelnikManor and Perona (2004). The matrix Q was specified as the normalized graph Laplacian (see, e.g., von Luxburg, 2007)1 Lnor = In−D−1/2WD−1/2, where W ∈ Rn×n is a similarity matrix and D ∈ Rn×n is the degree matrix of W . The matrix P was specified by\nP1 =  1 0 0 0 0 1 0 0 0 0 3 1 0 0 1 1 , P2 =  1 0 0 0 0 3 0 1 0 0 1 0 0 1 0 1 , P3 =  1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 3 ,\nP4 =  1 1/2 1/2 1/2\n1/2 2 0 1/2 1/2 0 2 1/2 1/2 1/2 1/2 3\n, P5 =  1 1/2 1/21/2 1 0\n1/2 0 1\n, P6 = \n1 1/2 1/2 1/2 1/2 1 0 0 1/2 0 1 0 1/2 0 0 1 . For data sets 1 and 2 we used the Gaussian similarity\nWi,j = exp(−‖xi − xj‖22/(2σ2)) 1Though the graph Laplacian matrices have zero eigenvalues, they would not cause algorithmic prob-\nlems when used as Q.\nwith the kernel width σ = 0.25, and for data sets 3 to 5 we applied the local-scaling similarity (Zelnik-Manor and Perona, 2004)\nWi,j = exp(−‖xi − xj‖22/(2σiσj)), σi = ‖xi − x(k)i ‖2\nwith the number of nearest neighbors k = 7, where x (k) i is the k-th nearest neighbor of\nxi in Xn. We set γ = 99 and τ = √ l. Furthermore, a class balance regularization was imposed for data sets 2 to 5. The detail is omitted here due to the space limit, while the idea is to encourage balanced total responses of all c classes. For this regularization, the regularization parameter was γ′ = 1. We can see that in Figure 2, MAVR successfully classified the data belonging to the known classes and simultaneously clustered the data belonging to the unknown classes. By specifying different P , we could control the influence of the known classes on the unknown classes."
    }, {
      "heading" : "5.2 Multi-class learning",
      "text" : "A state-of-the-art multi-class transductive learning method named learning with local and global consistency (LGC) (Zhou et al., 2003) is closely related to MAVR. Actually, if we specify P = Ic and Q = Lnor, unconstrained MAVR will be reduced to LGC exactly. Although LGC is motivated by the label propagation viewpoint, it can be written as optimization (4) in Zhou et al. (2003). Here, we illustrate the nuance of constrained MAVR and LGC that is unconstrained MAVR using an artificial data set.\nThe artificial data set 3circles is generated as follows. We have three classes with the class ratio 1/6, 1/3, 1/2. Let yi be the ground-truth label of xi, then xi is generated by\nxi = (6yi cos(ai) + i,1, 5yi sin(ai) + i,2) > ∈ R2,\nwhere ai is an angel drawn i.i.d. from the uniform distribution U(0, 2π), and i,1 and i,2 are noises drawn i.i.d. from the normal distribution N (0, σ2 ). We vary one factor and fix all other factors. The default values of these factors are σ = 0.5, σ = 0.5, l = 3, n = 300, γ = 99, and τ = √ l. Figure 3 shows the experimental results, where the means with the standard errors of the classification error rates are plotted. For each task that corresponds to a full specification of all factors, MAVR and LGC were repeatedly ran on 100 random samplings. We can see from Figure 3 that the performance of LGC was usually not as good as MAVR.\nOver the past decades, a huge number of transductive learning and semi-supervised learning methods have been proposed based on various motivations as graph cut (Blum and Chawla, 2001), random walk (Zhu et al., 2003), manifold regularization (Belkin et al., 2006), and information maximization (Niu et al., 2013b), just to name a few. A state-of-the-art semi-supervised learning method called Laplacian regularized least squares (LapRLS) (Belkin et al., 2006) is included to be compared with MAVR besides LGC.\nThe experimental results are reported in Figure 4. Similarly to Figure 3, the means with the standard errors of the classification error rates are shown where 4 methods were repeatedly ran on 100 random samplings for each task. We considered another specification of Q as the unnormalized graph Laplacian Lun = D − W which was also employed by LapRLS. The cosine similarity is defined by\nWi,j = x > ixj/‖xi‖2‖xj‖2 if xi ∼k xj, Wi,j = 0 otherwise,\nwhere xi ∼k xj means xi and xj are among the k-nearest neighbors of each other. We set l = n/10 for all involved n in Figure 4, and there seems no reliable model selection method given very few labeled data, so we select the best hyperparameters for each method using the labels of unlabeled data from 10 additional random samplings. Specifically, σ is the median distance × {1/16, 1/8, 1/4, 1/2, 1}, and k is from {1, 3, 5, 7, 9} for both localscaling and cosine similarities; τ is √ l × {1/16, 1/8, 1/4, 1/2, 1}. The hyperparameters are all fixed since it resulted in more stable performance. For MAVR, LGC, and λI of LapRLS, it was fixed to 99 if the Gaussian and cosine similarities were used and 1 if the local-scaling similarity was used; λA of LapRLS was 10\n−3 if the Gaussian and local-scaling similarities were used and 103 if the cosine similarity was used since LapRLS also needed W that was too sparse and near singular, but an exception was panel (i) where λA = 10 −3 gave lower error rates of LapRLS. We can see from Figure 4 that two MAVR methods often compared favorably with the state-of-the-art methods LGC and LapRLS, which implies that our proposed multi-class volume approximation is reasonable and practical."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We proposed a multi-class volume approximation that can be applied to several transductive problem settings such as multi-class, multi-label and serendipitous learning. The\nresultant learning method is non-convex in nature but can be solved exactly and efficiently. It is theoretically justified by our stability and error analyses and experimentally demonstrated promising."
    }, {
      "heading" : "A Proofs",
      "text" : "A.1 Proof of Theorem 2\nThe derivative of g(ρ) is\ng′(ρ) = nc∑ k=k0 2z2k (γλPQ,k − ρ)3 − τ 2.\nHence, g′(ρ) > 0 whenever ρ < γλPQ,k0 , and g(ρ) is strictly increasing in the interval (−∞, γλPQ,k0). Moreover,\nlim ρ→−∞ g(ρ) = −τ 2 and lim ρ→γλPQ,k0 g(ρ) = +∞,\nand thus g(ρ) has exactly one root in (−∞, γλPQ,k0). Notice that ‖z‖2 = ‖ vec(V>QY VP )‖2 = ‖V>PQy‖2 = ‖y‖2 since VPQ is an orthonormal matrix, and then ρ0 = γλPQ,k0 − ‖y‖2/τ = γλPQ,k0 − ‖z‖2/τ . As a result,\ng(ρ0) = nc∑ k=k0 z2k (γλPQ,k − ρ0)2 − τ 2\n= nc∑ k=k0 z2k (γλPQ,k − γλPQ,k0 + ‖z‖2/τ)2 − τ 2\n≤ nc∑ k=k0 z2k (‖z‖2/τ)2 − τ 2\n=\n(∑nc k=k0\nz2k ‖z‖22\n− 1 ) τ 2\n≤ 0,\nwhere the first inequality is because λPQ,k ≥ λPQ,k0 for k ≥ k0. The fact that g(ρ0) ≤ 0 concludes that the only root in (−∞, γλPQ,k0) is in [ρ0, γλPQ,k0) but not (−∞, ρ0).\nA.2 Proof of Theorem 3\nDenote by h = vec(H), y = vec(Y ) and M = (γP ⊗Q− ρInc), and denote by h′, y′ and M ′ similarly. Let λmin(·) and λmax(·) be two functions extracting the smallest and largest eigenvalues of a matrix. Under our assumption,\nλmin(M) = γλPQ,1 − ρ ≥ Cγ,τ > 0\nwhich means that M is positive definite, and so is M ′. By Eq. (14),\nh− h′ = M−1y −M ′−1y′ = M−1(y − y′) + (M−1 −M ′−1)y′ = M−1(y − y′) +M−1(M ′ −M)M ′−1y′ = M−1(y − y′) + (ρ′ − ρ)M−1M ′−1y′.\nNote that ‖Av‖2 ≤ λmax(A)‖v‖2 for any symmetric positive-definite matrix A and any vector v, as well as λmax(AB) ≤ λmax(A)λmax(B) for any symmetric positive-definite matrices A and B. Hence,\n‖h− h′‖2 = ‖M−1(y − y′) + (ρ′ − ρ)M−1M ′−1y′‖2 ≤ ‖M−1(y − y′)‖2 + |ρ− ρ′|‖M−1M ′−1y′‖2 ≤ λmax(M−1)‖y − y′‖2 + λmax(M−1)λmax(M ′−1)|ρ− ρ′|‖y′‖2\n≤ ‖y − y ′‖2 Cγ,τ + |ρ− ρ′|‖y′‖2 C2γ,τ ,\nwhere the first inequality is the triangle inequality, the second inequality is because M−1 and M ′−1 are symmetric positive definite, and the third inequality follows from λmax(M −1) = 1/λmin(M) and λmax(M ′−1) = 1/λmin(M\n′). Due to the symmetry of h and h′,\n‖h− h′‖2 ≤ ‖y − y′‖2 Cγ,τ + |ρ− ρ′|min{‖y‖2, ‖y′‖2} C2γ,τ .\nThis inequality is the vectorization of (18). For MAVR in optimization (9), Theorem 2 together with our assumption indicates that\nγλPQ,1 − ‖y‖2/τ ≤ ρ < γλPQ,1, γλPQ,1 − ‖y′‖2/τ ≤ ρ′ < γλPQ,1,\nso |ρ′ − ρ| ≤ max{‖y‖2/τ, ‖y′‖2/τ} and\n‖h− h′‖2 ≤ ‖y − y′‖2 Cγ,τ + max{‖y‖2, ‖y′‖2}min{‖y‖2, ‖y′‖2} τC2γ,τ\n= ‖y − y′‖2 Cγ,τ + ‖y‖2‖y′‖2 τC2γ,τ .\nFor unconstrained MAVR in optimization (10), we have\n‖h− h′‖2 ≤ ‖y − y′‖2 Cγ,τ ,\nsince ρ = ρ′ = −1.\nA.3 Proof of Theorem 4\nDenote by h = vec(H), y = vec(Y ), h∗ = vec(H∗), e = vec(E), and M = γP ⊗ Q. The Kronecker product P ⊗ Q is symmetric and positive definite, and then M1/2 is a well-defined symmetric and positive-definite matrix. We can know based on V (H∗) ≤ Ch that\n‖M1/2h∗‖2 = √ γh∗ > (P ⊗Q)h∗ ≤ √ γCh‖h∗‖22 = √ γCh‖h∗‖2.\nLet λmin(·) and λmax(·) be two functions extracting the smallest and largest eigenvalues of a matrix. In the following, we will frequently use that ‖Av‖2 ≤ λmax(A)‖v‖2 for any symmetric positive-definite matrix A and any vector v.\nConsider unconstrained MAVR in optimization (10) first. Since ρ = −1,\nh− h∗ = (M + Inc)−1y − h∗ = (M + Inc)\n−1(h∗ + e)− (M + Inc)−1(M + Inc)h∗ = −(M + Inc)−1Mh∗ + (M + Inc)−1e.\nAs a consequence,\nE‖h− h∗‖22 = ‖(M + Inc)−1Mh∗‖22 + E‖(M + Inc)−1e‖22,\nsince E[(M + Inc)−1e] = (M + Inc)−1Ee = 0nc. Subsequently,\n‖(M + Inc)−1Mh∗‖2 ≤ λmax((M + Inc)−1M1/2) · ‖M1/2h∗‖2 ≤ λmax((γP ⊗Q+ Inc)−1(γP ⊗Q)1/2) · √ γCh‖h∗‖2\n= √ γChλmax\n( √ γ\nγ + 1 (ΛPQ + Inc)\n−1Λ 1/2 PQ ) ‖h∗‖2\n≤ √ Chλmax((ΛPQ + Inc) −1Λ 1/2 PQ)‖h∗‖2 ≤ 1 2 √ Ch‖h∗‖2,\nwhere the last inequality is because the eigenvalues of (ΛPQ + Inc) −1Λ 1/2 PQ are√\nλPQ,1\nλPQ,1+1 , . . . ,\n√ λPQ,nc λPQ,nc+1 and\nsupλ≥0\n√ λ\nλ+ 1 =\n1 2 .\nOn the other hand,\nE‖(M + Inc)−1e‖22 ≤ (λmax((M + Inc)−1))2 · E‖e‖22\n= E[e>e]\n(λmin(M + Inc))2\n≤ l̃σ2l + ũσ2u.\nHence,\nE‖h− h∗‖22 ≤ 1\n4 Ch‖h∗‖22 + l̃σ2l + ũσ2u,\nwhich completes the proof of inequality (20). Next, consider MAVR in optimization (9). We would have\nh− h∗ = (M − ρInc)−1y − h∗ = (M − ρInc)−1(h∗ + e)− (M − ρInc)−1(M − ρInc)h∗ = −(M − ρInc)−1(M − (ρ+ 1)Inc)h∗ + (M − ρInc)−1e.\nIn general, E[(M−ρInc)−1e] 6= 0nc since ρ depends on e. Furthermore, M−(ρ+1)Inc may have negative eigenvalues when γλPQ,1 − 1 < ρ ≤ γλPQ,1 − Cγ,τ . Taking the expectation of ‖h− h∗‖2,\nE‖h− h∗‖2 ≤ E‖(M − ρInc)−1(M − (ρ+ 1)Inc)h∗‖2 + E‖(M − ρInc)−1e‖2 ≤ E‖(M − ρInc)−1Mh∗‖2 + E[|ρ+ 1|‖(M − ρInc)−1h∗‖2] + E‖(M − ρInc)−1e‖2.\nSubsequently, E‖(M − ρInc)−1Mh∗‖2 ≤ supρ λmax((M − ρInc)−1M1/2) · √ γCh‖h∗‖2\n= supρ √ Chλmax ( (ΛPQ − ρ/γInc)−1Λ1/2PQ ) ‖h∗‖2 ≤ √ Ch‖h∗‖2 · supρ≤γλPQ,1−Cγ,τ supλ≥λPQ,1 ( √ λ\nλ− ρ/γ\n)\n≤ √ ChγλPQ,1 Cγ,τ ‖h∗‖2.\nOn the other hand,\nE[|ρ+ 1|‖(M − ρInc)−1h∗‖2] ≤ E|ρ+ 1| · supρ λmax((M − ρInc)−1)‖h∗‖2\n≤ ‖h ∗‖2\nCγ,τ · Emax{−ρ− 1, supρ ρ+ 1}\n≤ ‖h ∗‖2\nCγ,τ ·max{E‖y‖2/τ − γλPQ,1 − 1, γλPQ,1 − Cγ,τ + 1}\n= ‖h∗‖2 Cγ,τ\n·max{ √ l̃/τ − γλPQ,1 − 1, γλPQ,1 − Cγ,τ + 1}.\nwhere we used the fact that supρ ρ is independent of e, and applied Jensen’s inequality to obtain that E‖y‖2 ≤ √ E‖y‖22 ≤ √ l̃.\nIn the end,\nE‖(M − ρInc)−1e‖2 ≤ supρ λmax((M − ρInc)−1) · E‖e‖2\n≤ E √ e>e\nCγ,τ ≤ √ E[e>e] Cγ,τ\n=\n√ l̃σ2l + ũσ 2 u\nCγ,τ ,\nwhere the third inequality is due to Jensen’s inequality. Therefore, inequality (19) follows by combining the three upper bounds of expectations."
    } ],
    "references" : [ {
      "title" : "Manifold regularization: a geometric framework for learning from labeled and unlabeled examples",
      "author" : [ "M. Belkin", "P. Niyogi", "V. Sindhwani" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Belkin et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Belkin et al\\.",
      "year" : 2006
    }, {
      "title" : "Learning from labeled and unlabeled data using graph mincuts",
      "author" : [ "A. Blum", "S. Chawla" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Blum and Chawla.,? \\Q2001\\E",
      "shortCiteRegEx" : "Blum and Chawla.",
      "year" : 2001
    }, {
      "title" : "Semi-supervised learning of class balance under class-prior change by distribution matching",
      "author" : [ "M.C. du Plessis", "M. Sugiyama" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Plessis and Sugiyama.,? \\Q2012\\E",
      "shortCiteRegEx" : "Plessis and Sugiyama.",
      "year" : 2012
    }, {
      "title" : "Large margin vs. large volume in transductive learning",
      "author" : [ "R. El-Yaniv", "D. Pechyony", "V. Vapnik" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "El.Yaniv et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "El.Yaniv et al\\.",
      "year" : 2008
    }, {
      "title" : "On the stationary values of a second-degree polynomial on the unit sphere",
      "author" : [ "G. Forsythe", "G. Golub" ],
      "venue" : "Journal of the Society for Industrial and Applied Mathematics,",
      "citeRegEx" : "Forsythe and Golub.,? \\Q1965\\E",
      "shortCiteRegEx" : "Forsythe and Golub.",
      "year" : 1965
    }, {
      "title" : "A k-means clustering algorithm",
      "author" : [ "J.A. Hartigan", "M.A. Wong" ],
      "venue" : "Applied Statistics,",
      "citeRegEx" : "Hartigan and Wong.,? \\Q1979\\E",
      "shortCiteRegEx" : "Hartigan and Wong.",
      "year" : 1979
    }, {
      "title" : "Transductive multi-label learning via label set propagation",
      "author" : [ "X. Kong", "M. Ng", "Z.-H. Zhou" ],
      "venue" : "IEEE Transaction on Knowledge and Data Engineering,",
      "citeRegEx" : "Kong et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2013
    }, {
      "title" : "Matrix Analysis for Scientists and Engineers",
      "author" : [ "A.J. Laub" ],
      "venue" : "Society for Industrial and Applied Mathematics,",
      "citeRegEx" : "Laub.,? \\Q2005\\E",
      "shortCiteRegEx" : "Laub.",
      "year" : 2005
    }, {
      "title" : "Maximum volume outlier detection and its applications in credit risk analysis",
      "author" : [ "S. Li", "W. Ng" ],
      "venue" : "International Journal on Artificial Intelligence Tools,",
      "citeRegEx" : "Li and Ng.,? \\Q2013\\E",
      "shortCiteRegEx" : "Li and Ng.",
      "year" : 2013
    }, {
      "title" : "Squared-loss mutual",
      "author" : [ "2013a. G. Niu", "W. Jitkrittum", "B. Dai", "H. Hachiya", "M. Sugiyama" ],
      "venue" : null,
      "citeRegEx" : "Niu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2013
    }, {
      "title" : "Algorithms for Linear-Quadratic Optimization",
      "author" : [ "V. Sima" ],
      "venue" : null,
      "citeRegEx" : "Sima.,? \\Q1996\\E",
      "shortCiteRegEx" : "Sima.",
      "year" : 1996
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : "U. von Luxburg. A tutorial on spectral clustering. Statistics and Computing,",
      "citeRegEx" : "1982",
      "shortCiteRegEx" : "1982",
      "year" : 1998
    }, {
      "title" : "Self-tuning spectral clustering",
      "author" : [ "Zelnik-Manor", "P. Perona" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zelnik.Manor and Perona.,? \\Q2007\\E",
      "shortCiteRegEx" : "Zelnik.Manor and Perona.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "But it did not gain much attention until a creative approximation was proposed in El-Yaniv et al. (2008) to implement LVP for the case of soft response vectors.",
      "startOffset" : 82,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : "learning (El-Yaniv et al., 2008), binary clustering (Niu et al.",
      "startOffset" : 9,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : ", 2013a), and outlier detection (Li and Ng, 2013).",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "learning (El-Yaniv et al., 2008), binary clustering (Niu et al., 2013a), and outlier detection (Li and Ng, 2013). LVP is a learning-theoretic principle which views learning as hypothesis selecting from a certain hypothesis space H. Despite the form of the hypothesis, H can always be partitioned into a finite number of equivalence classes after we observe certain data, where an equivalence class is a set of hypotheses that generate the same labeling of the observed data. LVP, as one of the learning-theoretic principles from the statistical learning theory, prioritizes those equivalence classes according to the volume they occupy in H. See the illustration in Figure 1: The blue ellipse represents H, and it is partitioned into C1, . . . , C4 each occupying a quadrant of the Cartesian coordinate system R intersected with H; LVP claims that C1 and C3 are more preferable than C2 and C4, since C1 and C3 have larger volume than C2 and C4. In practice, the hypothesis space H cannot be as simple as in Figure 1. It frequently locates in very high-dimensional spaces where exact or even quantifiable volume estimation is challenging. Therefore, El-Yaniv et al. (2008) proposed a volume approximation to bypass the volume estimation.",
      "startOffset" : 10,
      "endOffset" : 1172
    }, {
      "referenceID" : 6,
      "context" : ", 2003), multi-label learning (Kong et al., 2013), and serendipitous learning (Zhang et al.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "Nevertheless, the volume approximation in El-Yaniv et al. (2008) only fits binary learning problem settings in spite of its potential advantages.",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "Nevertheless, the volume approximation in El-Yaniv et al. (2008) only fits binary learning problem settings in spite of its potential advantages. In this paper, we naturally extend it to a more general definition that can be applied to some transductive problem settings including but not limited to multi-class learning (Zhou et al., 2003), multi-label learning (Kong et al., 2013), and serendipitous learning (Zhang et al., 2011). We adopt the same strategy as El-Yaniv et al. (2008): For n data and c labels, a hypothesis space is defined in Rn×c and linked to an ellipsoid in R, such that the equivalence classes and the volume approximation can be defined accordingly.",
      "startOffset" : 42,
      "endOffset" : 486
    }, {
      "referenceID" : 2,
      "context" : "Nevertheless, the volume approximation in El-Yaniv et al. (2008) only fits binary learning problem settings in spite of its potential advantages. In this paper, we naturally extend it to a more general definition that can be applied to some transductive problem settings including but not limited to multi-class learning (Zhou et al., 2003), multi-label learning (Kong et al., 2013), and serendipitous learning (Zhang et al., 2011). We adopt the same strategy as El-Yaniv et al. (2008): For n data and c labels, a hypothesis space is defined in Rn×c and linked to an ellipsoid in R, such that the equivalence classes and the volume approximation can be defined accordingly. Similarly to the binary volume approximation, our approach is also distribution free, that is, the labeled and unlabeled data do not necessarily share the same marginal distribution. This advantage of transductive learning over (semi-supervised) inductive learning is especially useful for serendipitous problems where the labeled and unlabeled data must not be identically distributed. We name the learning method which realizes the proposed multi-class volume approximation multi-class approximate volume regularization (MAVR). It involves a non-convex optimization problem, but the globally optimal solution is almost surely unique and accessible in O(n) time following Forsythe and Golub (1965). Moreover, we theoretically provide stability and error analyses for MAVR, as well as experimentally compare it to two state-of-the-art methods in Zhou et al.",
      "startOffset" : 42,
      "endOffset" : 1373
    }, {
      "referenceID" : 2,
      "context" : "Nevertheless, the volume approximation in El-Yaniv et al. (2008) only fits binary learning problem settings in spite of its potential advantages. In this paper, we naturally extend it to a more general definition that can be applied to some transductive problem settings including but not limited to multi-class learning (Zhou et al., 2003), multi-label learning (Kong et al., 2013), and serendipitous learning (Zhang et al., 2011). We adopt the same strategy as El-Yaniv et al. (2008): For n data and c labels, a hypothesis space is defined in Rn×c and linked to an ellipsoid in R, such that the equivalence classes and the volume approximation can be defined accordingly. Similarly to the binary volume approximation, our approach is also distribution free, that is, the labeled and unlabeled data do not necessarily share the same marginal distribution. This advantage of transductive learning over (semi-supervised) inductive learning is especially useful for serendipitous problems where the labeled and unlabeled data must not be identically distributed. We name the learning method which realizes the proposed multi-class volume approximation multi-class approximate volume regularization (MAVR). It involves a non-convex optimization problem, but the globally optimal solution is almost surely unique and accessible in O(n) time following Forsythe and Golub (1965). Moreover, we theoretically provide stability and error analyses for MAVR, as well as experimentally compare it to two state-of-the-art methods in Zhou et al. (2003) and Belkin et al.",
      "startOffset" : 42,
      "endOffset" : 1539
    }, {
      "referenceID" : 0,
      "context" : "(2003) and Belkin et al. (2006) using USPS, MNIST, 20Newsgroups and Isolet.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "The binary volume approximation in El-Yaniv et al. (2008) involves a few key concepts: The soft response vector, the hypothesis space and the equivalence class, and the power and volume of equivalence classes.",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "However, it is very hard to accurately compute the geometric volume of even a single convex body in R, let alone all 2 convex bodies, so El-Yaniv et al. (2008) introduced an efficient approximation.",
      "startOffset" : 137,
      "endOffset" : 160
    }, {
      "referenceID" : 6,
      "context" : "A huge challenge of multi-label problems is that some label sets or label vectors might have no labeled data (Kong et al., 2013), since there are 2 possible label sets and 3 possible label vectors.",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 6,
      "context" : "To the best of our knowledge, the former setting has been studied only in Kong et al. (2013) and the latter setting has not been studied yet.",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "To the best of our knowledge, the former setting has been studied only in Kong et al. (2013) and the latter setting has not been studied yet. The latter setting is more general, since the former one requires labeled data to be fully labeled, while the latter one allows labeled data to be partially labeled. A huge challenge of multi-label problems is that some label sets or label vectors might have no labeled data (Kong et al., 2013), since there are 2 possible label sets and 3 possible label vectors. A more challenging serendipitous setting which is a multi-class setting but some labels have no labeled data has been studied in Zhang et al. (2011). Let Yl = {yi | xi ∈ Xl} and Yu = {yi | xi ∈ Xu, yi 6∈ Yl}, then we have #Yu ≥ 1 where # measures the cardinality.",
      "startOffset" : 74,
      "endOffset" : 655
    }, {
      "referenceID" : 6,
      "context" : "To the best of our knowledge, the former setting has been studied only in Kong et al. (2013) and the latter setting has not been studied yet. The latter setting is more general, since the former one requires labeled data to be fully labeled, while the latter one allows labeled data to be partially labeled. A huge challenge of multi-label problems is that some label sets or label vectors might have no labeled data (Kong et al., 2013), since there are 2 possible label sets and 3 possible label vectors. A more challenging serendipitous setting which is a multi-class setting but some labels have no labeled data has been studied in Zhang et al. (2011). Let Yl = {yi | xi ∈ Xl} and Yu = {yi | xi ∈ Xu, yi 6∈ Yl}, then we have #Yu ≥ 1 where # measures the cardinality. It is still solvable when #Yu = 1 if a special label of outliers is allowed and when #Yu > 1 as a combination of classification and clustering problems. Zhang et al. (2011) is the unique previous work which successfully dealt with #Yu = 2 and #Yu = 3.",
      "startOffset" : 74,
      "endOffset" : 943
    }, {
      "referenceID" : 6,
      "context" : "For multi-label problems, we need a threshold Th that is either preset or learned since usually positive and negative labels are imbalanced, and yi can be predicted by ŷi = {j | hi,j ≥ Th}; or we can use the label set prediction methods proposed in Kong et al. (2013). Then, a soft response matrix as our transductive hypothesis is an n-by-c matrix defined by H = (h1, .",
      "startOffset" : 249,
      "endOffset" : 268
    }, {
      "referenceID" : 3,
      "context" : "The denominator ‖H‖Fro is quite difficult to tackle, so we would like to eliminate it as El-Yaniv et al. (2008) and Niu et al.",
      "startOffset" : 89,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "The denominator ‖H‖Fro is quite difficult to tackle, so we would like to eliminate it as El-Yaniv et al. (2008) and Niu et al. (2013a).",
      "startOffset" : 89,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : ", 2003) and the binary counterparts of the fourth and third loss functions have been used for binary transductive learning (El-Yaniv et al., 2008) and clustering (Niu et al.",
      "startOffset" : 123,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "Although the optimization in (8) is done in Rn×c, the regularization is carried out relative to HP,Q, since under the constraint ‖H‖Fro = τ , the regularization tr(H>QHP ) is a weighted sum of the squares of cosines between vec(H) and the principal axes of EP,Q like El-Yaniv et al. (2008). Subsequently, we denote by y1, .",
      "startOffset" : 267,
      "endOffset" : 290
    }, {
      "referenceID" : 4,
      "context" : "In this representation, the objective is a second-degree polynomial and the constraint is an origin-centered sphere, and fortunately we could solve it exactly and efficiently following Forsythe and Golub (1965). To this end, a fundamental property of the Kronecker product is necessary (see, e.",
      "startOffset" : 185,
      "endOffset" : 211
    }, {
      "referenceID" : 4,
      "context" : "1 of Forsythe and Golub (1965), the smallest root of g(ρ) determines a unique h so that (h, ρ) is the globally optimal solution to Φ(h, ρ), i.",
      "startOffset" : 5,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "(12) is a discrete Sylvester equation which consumes O(n) for solving it (Sima, 1996).",
      "startOffset" : 73,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "1 Serendipitous learning We show how to handle serendipitous problems by MAVR directly without performing clustering (Hartigan and Wong, 1979; Ng et al., 2001; Sugiyama et al., 2014) or estimating the class-prior change (du Plessis and Sugiyama, 2012).",
      "startOffset" : 117,
      "endOffset" : 182
    }, {
      "referenceID" : 1,
      "context" : "Over the past decades, a huge number of transductive learning and semi-supervised learning methods have been proposed based on various motivations as graph cut (Blum and Chawla, 2001), random walk (Zhu et al.",
      "startOffset" : 160,
      "endOffset" : 183
    }, {
      "referenceID" : 0,
      "context" : ", 2003), manifold regularization (Belkin et al., 2006), and information maximization (Niu et al.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "A state-of-the-art semi-supervised learning method called Laplacian regularized least squares (LapRLS) (Belkin et al., 2006) is included to be compared with MAVR besides LGC.",
      "startOffset" : 103,
      "endOffset" : 124
    } ],
    "year" : 2014,
    "abstractText" : "Given a hypothesis space, the large volume principle by Vladimir Vapnik prioritizes equivalence classes according to their volume in the hypothesis space. The volume approximation has hitherto been successfully applied to binary learning problems. In this paper, we extend it naturally to a more general definition which can be applied to several transductive problem settings, such as multi-class, multi-label and serendipitous learning. Even though the resultant learning method involves a non-convex optimization problem, the globally optimal solution is almost surely unique and can be obtained in O(n3) time. We theoretically provide stability and error analyses for the proposed method, and then experimentally show that it is promising.",
    "creator" : "LaTeX with hyperref package"
  }
}