{
  "name" : "1502.06895.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the consistency theory of high dimensional variable screening",
    "authors" : [ "Xiangyu Wang", "Chenlei Leng", "David Dunson" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 2.\n06 89\n5v 1\n[ m\nat h.\nST ]\n2 4\nFe b\nVariable screening is a fast dimension reduction technique for assisting high dimensional feature selection. As a preselection method, it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model. The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones. When the data dimension p is substantially larger than the sample size n, variable screening becomes crucial as 1) Faster feature selection algorithms are needed; 2) Conditions guaranteeing selection consistency might fail to hold.\nThis article studies a class of linear screening methods and establishes consistency theory for this special class. In particular, we prove the weak diagonally dominant (WDD) condition is a necessary and sufficient condition for strong screening consistency. As concrete examples, we show two screening methods SIS andHOLP are both strong screening consistent (subject to additional constraints) with large probability if n > O((ρs + σ/τ)2 log p) under random designs. In addition, we relate the WDD condition to the irrepresentable condition, and highlight limitations of SIS."
    }, {
      "heading" : "1 Introduction",
      "text" : "The rapidly growing data dimension has brought new challenges to statistical variable selection, a crucial technique for identifying important variables to facilitate interpretation and improve prediction accuracy. Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al., 2009; Zhang, 2010). Regularized methods can consistently recover the support of coefficients, i.e., the non-zero signals, via optimizing regularized loss functions under certain conditions (Zhao and Yu, 2006; Wainwright, 2009;\nLee et al., 2013). However, in the big data era when p far exceeds n, such regularized methods might fail due to two reasons. First, the conditions that guarantee variable selection consistency might fail to hold when p >> n; Second, the computation burden of the corresponding optimization problem increases dramatically with large p.\nBearing these concerns in mind, Fan and Lv (2008) propose the concept of “variable screening”, a fast technique that reduces data dimensionality from p to a size comparable to n, with all predictors having non-zero coefficients preserved. They propose a marginal correlation based fast screening technique “Sure Independence Screening” (SIS) that can preserve signals with large probability. However, this method relies on a strong assumption that the marginal correlations between the response and the important predictors are high (Fan and Lv, 2008), which is easily violated in the practice. Li et al. (2012) extends the marginal correlation to the Spearman’s rank correlation, which is shown to gain certain robustness but is still limited by the same strong assumption. Wang (2009) and Cho and Fryzlewicz (2012) take a different approach to attack the screening problem. They both adopt variants of a forward selection type algorithm that includes one variable at a time for constructing a candidate variable set for further refining. These methods eliminate the strong marginal assumption in Fan and Lv (2008) and have been shown to achieve better empirical performance. However, such improvement is limited by the extra computational burden caused by their iterative framework, which is reported to be high when p is large (Wang and Leng, 2013). To ameliorate concerns in both screening performance and computational efficiency, Wang and Leng (2013) develop a new type of screening method termed “High-dimensional ordinary least-square projection” (HOLP ). This new screener relaxes the strong marginal assumption required by SIS and can be computed efficiently (complexity is O(n2p)), thus scalable to ultra-high dimensionality.\nThis article focuses on linear models for tractability. As computation is one vital concern for designing a good screening method, we primarily focus on a class of linear screeners that can be efficiently computed, and study their theoretical properties. The main contributions of this article lie in three aspects.\n1. We define the notion of strong screening consistency to provide a unified framework\nfor analyzing screening methods. In particular, we show a necessary and sufficient condition for a screening method to be strong screening consistent is that the screening matrix is weak diagonally dominant (WDD). This condition gives insights into the design of screening matrices, while providing a framework to assess the effectiveness of screening methods.\n2. We relate WDD to the irrepresentable condition (Zhao and Yu, 2006) that is necessary\nand sufficient for sign consistency of lasso (Tibshirani, 1996). In contrast to the irrepresentable condition (IC) that is specific to the design matrix, WDD involves another ancillary matrix that can be chosen arbitrarily. Such flexibility allows WDD to hold even when IC fails if the ancillary matrix is carefully chosen (as in HOLP ). When the ancillary matrix is chosen as the design matrix, certain equivalence is shown between WDD and IC, revealing the difficulty for SIS to achieve screening consistency.\n3. We study the behavior of SIS and HOLP under random designs, and prove that\na sample size of n = O ( (ρs + σ/τ)2 log p ) is sufficient for SIS and HOLP to be screening consistent, where s is the sparsity, ρ measures the diversity of signals and τ/σ evaluates the signal-to-noise ratio. This is to be compared to the sign consistency results in Wainwright (2009) where the design matrix is fixed and assumed to follow the irrepresentable condition.\nThe article is organized as follows. In Section 1, we set up the basic problem and describe the framework of variable screening. In Section 2, we provide a deterministic necessary and sufficient condition for consistent screening. Its relationship with the irrepresentable condition is discussed in Section 3. In Section 4, we prove the consistency of SIS and HOLP under random designs by showing the WDD condition is satisfied with large probability, although the requirement on SIS is much more restictive."
    }, {
      "heading" : "2 Linear screening",
      "text" : "Consider the usual linear regression\nY = Xβ + ǫ,\nwhere Y is the n× 1 response vector, X is the n× p design matrix and ǫ is the noise. The regression task is to learn the coefficient vector β. In the high dimensional setting where p >> n, a sparsity assumption is often imposed on β so that only a small portion of the coordinates are non-zero. Such an assumption splits the task of learning β into two phases. The first is to recover the support of β, i.e., the location of non-zero coefficients; The second is to estimate the value of these non-zero signals. This article mainly focuses on the first phase.\nAs pointed out in the introduction, when the dimensionality is too high, using regularization methods methods raises concerns both computationally and theoretically. To reduce the dimensionality, Fan and Lv (2008) suggest a variable screening framework by finding a\nsubmodel\nMd = {i : |β̂i| is among the largest d coordinates of |β̂|} or Mγ = {i : |β̂i| > γ}.\nLet Q = {1, 2, · · · , p} and define S as the true model with s = |S| being its cardinarlity. The hope is that the submodel size |Md| or |Mγ| will be smaller or comparable to n, while S ⊆ Md or S ⊆ Mγ. To achieve this goal two steps are usually involved in the screening analysis. The first is to show there exists some γ such that mini∈S |β̂i| > γ and the second step is to bound the size of |Mγ| such that |Mγ| = O(n). To unify these steps for a more comprehensive theoretical framework, we put forward a slightly stronger definition of screening consistency in this article.\nDefinition 2.1. (Strong screening consistency) An estimator β̂ (of β) is strong screening consistent if it satisfies that\nmin i∈S |β̂i| > max i 6∈S |β̂i| (1)\nand\nsign(β̂i) = sign(βi), ∀i ∈ S. (2)\nRemark 2.1. This definition does not differ much from the usual screening property studied in the literature, which requires mini∈S |β̂i| > max(n−s)i 6∈S |β̂i|, where max(k) denotes the kth largest item.\nThe key of strong screening consistency is the property (1) that requires the estimator to preserve consistent ordering of the zero and non-zero coefficients. It is weaker than variable selection consistency in Zhao and Yu (2006). The requirement in (2) can be seen as a relaxation of the sign consistency defined in Zhao and Yu (2006), as no requirement for β̂i, i 6∈ S is needed. As shown later, such relaxation tremendously reduces the restriction on the design matrix, and allows screening methods to work for a broader choice of X .\nThe focus of this article is to study the theoretical properties of a special class of screeners\nthat take the linear form as\nβ̂ = AY\nfor some p × n ancillary matrix A. Examples include sure independence screening (SIS) where A = XT/n and high-dimensional ordinary least-square projection (HOLP ) where A = XT (XXT )−1. We choose to study the class of linear estimators because linear screening\nis computationally efficient and theoretically tractable. We note that the usual ordinary least-squares estimator is also a special case of linear estimators although it is not well defined for p > n."
    }, {
      "heading" : "3 Deterministic guarantees",
      "text" : "In this section, we derive the necessary and sufficient condition that guarantees β̂ = AY to be strong screening consistent. The design matrix X and the error ǫ are treated as fixed in this section and we will investigate random designs later. We consider the set of sparse coefficient vectors defined by\nB(s, ρ) = { β ∈ Rp : |supp(β)| ≤ s, maxi∈supp(β) |βi| mini∈supp(β) |βi| ≤ ρ } .\nThe set B(s, ρ) contains vectors having at most s non-zero coordinates with the ratio of the largest and smallest coordinate bounded by ρ. Before proceeding to the main result of this section, we introduce some terminology that helps to establish the theory.\nDefinition 3.1. (Weak diagonally dominant matrix) A p × p symmetric matrix Φ is weak diagonally dominant with sparsity s if for any I ⊆ Q, |I| ≤ s− 1 and i ∈ Q \\ I\nΦii > C0max\n{\n∑ j∈I |Φij + Φkj|, ∑ j∈I |Φij − Φkj|\n}\n+ |Φik| ∀k 6= i, k ∈ Q \\ I,\nwhere C0 ≥ 1 is a constant.\nNotice this definition implies that for i ∈ Q \\ I\nΦii ≥ C0 ( ∑\nj∈I |Φij + Φkj|+\n∑ j∈I |Φij − Φkj|\n)\n/2 ≥ C0 ∑\nj∈I |Φij |, (3)\nwhich is related to the usual diagonally dominant matrix. The weak diagonally dominant matrix provides a necessary and sufficient condition for any linear estimators β̂ = AY to be strong screening consistent. More precisely, we have the following result.\nTheorem 1. For the noiseless case where ǫ = 0, a linear estimator β̂ = AY is strong screening consistent for every β ∈ B(s, ρ), if and only if the screening matrix Φ = AX is weak diagonally dominant with sparsity s and C0 ≥ ρ.\nThe noiseless case is a good starting point to analyze β̂. Intuitively, in order to preserve the correct order of the coefficients in β̂ = AXβ, one needs AX to be close to a diagonally\ndominant matrix, so that β̂i, i ∈ MS will take advantage of the large diagonal terms in AX to dominate β̂i, i 6∈ MS that is just linear combinations of off-diagonal terms.\nWhen noise is considered, the condition in Theorem 1 needs to be changed slightly to accommodate extra discrepancies. In addition, the smallest non-zero coefficient has to be lower bounded to ensure a certain level of signal-to-noise ratio. Thus, we augment our previous definition of B(s, ρ) to have a signal strength control\nBτ (s, ρ) = {β ∈ B(s, ρ)| min i∈supp(β) |βi| ≥ τ}.\nThen we can obtain the following modified Theorem.\nTheorem 2. With noise, the linear estimator β̂ = AY is strong screening consistent for every β ∈ Bτ (s, ρ) if Φ = AX − 2τ−1‖Aǫ‖∞Ip is weak diagonally dominant with sparsity s and C0 ≥ ρ.\nThe condition in Theorem 2 can be further tailored to a necessary and sufficient version with extra manipulation on the noise term. Nevertheless, this might not be useful in practice due to the randomness in noise. In addition, the current version of Theorem 2 is already tight in the sense that there exists some noise vector ǫ such that the condition in Theorem 2 is also necessary for strong screening consistency.\nTheorems 1 and 2 establish ground rules for verifying consistency of a given screener and provide practical guidance for screening design. In Section 4, we consider some concrete examples of ancillary matrix A and prove that conditions in Theorems 1 and 2 are satisfied by the corresponding screeners with large probability under random designs."
    }, {
      "heading" : "4 Relationship with the irrepresentable condition",
      "text" : "The weak diagonally dominant (WDD) condition is closely related to the strong irrepresentable condition (IC) proposed in Zhao and Yu (2006) as a necessary and sufficient condition for sign consistency of lasso. Assume each column of X is standardized to have mean zero. Letting C = XTX/n and β be a given coefficient vector, the IC is expressed as\n‖CSc,SC−1S,S · sign(βS)‖∞ ≤ 1− θ (4)\nfor some θ > 0, where CA,B represents the sub-matrix of C with row indices in A and column indices in B. The authors enumerate several scenarios of C such that IC is satisfied. Following result verifies some of these scenarios for screening matrix Φ.\nCorollary 1. If Φii = 1, ∀i and |Φij| < c/(2s), ∀i 6= j for some 0 ≤ c < 1 as defined in Corollary 1 and 2 in Zhao and Yu (2006), then Φ is a weak diagonally dominant matrix with sparsity s and C0 ≥ 1/c. If |Φij| < r|i−j|, ∀i, j for some 0 < r < 1 as defined in Corollary 3 in Zhao and Yu (2006), then Φ is a weak diagonally dominant matrix with sparsity s and C0 ≥ (1−r)2/(4r).\nA more explicit but nontrivial relationship between IC and WDD is illustrated below\nwhen |S| = 2.\nTheorem 3. Assume Φii = 1, ∀i and |Φij | < r, ∀i 6= j. If Φ is weak diagonally dominant with sparsity 2 and C0 ≥ ρ, then Φ satisfies\n‖ΦSc,SΦ−1S,S · sign(βS)‖∞ ≤ ρ−1\n1− r\nfor all β ∈ B(2, ρ). On the other hand, if Φ satisfies the irrepresentable condition for all β ∈ B(2, ρ) for some θ, then Φ is a weak diagonally dominant matrix with sparsity 2 and\nC0 ≥ 1 1− θ 1− r 1 + r .\nTheorem 3 demonstrates certain equivalence between IC and WDD. However, it is worth noting that IC is directly imposed on the covariance matrix XTX/n. This makes IC a strong assumption that is easily violated; for example, when the predictors are highly correlated. In contrast to IC, WDD is imposed on matrix AX where there is still flexibility for choosing A. As we show in Section 4, the ancillary matrix A defined in HOLP satisfies WDD even when predictors are highly correlated and IC fails to hold. Therefore, WDD is a weaker constraint in some sense.\nFor sure independence screening, the screening matrix Φ = XTX/n coincides with the covariance matrix, making WDD and IC effectively equivalent. The following theorem formalizes this.\nTheorem 4. Let A = XT/n and standardize columns of X to have sample variance one. Assume X satisfies the sparse Riesz condition (Zhang and Huang, 2008), i.e,\nmin π⊆Q, |π|≤s\nλmin(X T πXπ/n) ≥ µ,\nfor some µ > 0. Now if AX is weak diagonally dominant with sparsity s + 1 and C0 ≥ ρ with ρ > √ s/µ, then X satisfies the irrepresentable condition for any β ∈ B(s, ρ).\nIn other words, under the condition ρ > √ s/µ, the strong screening consistency of SIS\nfor B(s+ 1, ρ) implies the model selection consistency of lasso for B(s, ρ).\nTheorem 4 illustrates the difficulty of SIS. The necessary condition that guarantees good screening performance of SIS also guarantees the model selection consistency of lasso. However, such a strong necessary condition does not mean that SIS should be avoided in practice given its substantial advantages in terms of simplicity and computational efficiency. The strong screening consistency defined in this article is stronger than conditions commonly used in justifying screening procedures as in Fan and Lv (2008)."
    }, {
      "heading" : "5 Screening under random designs",
      "text" : "In this section, we consider linear screening under random designs when X and ǫ are Gaussian. The theory developed in this section can be easily extended to a broader family of distributions, for example, where ǫ follows a sub-Gaussian distribution (Vershynin, 2010) and X follows an elliptical distribution (Fan and Lv, 2008; Wang and Leng, 2013). We focus on the Gaussian case for conciseness. Let ǫ ∼ N(0, σ2) and X ∼ N(0,Σ). We prove the screening consistency of SIS and HOLP by verifying the condition in Theorem 2. Recall the ancillary matrices for SIS and HOLP are defined respectively as\nASIS = X T/n, AHOLP = X T (XXT )−1.\nFor simplicity, we assume Σii = 1 for i = 1, 2, · · · , p. To verify the WDD condition, it is essential to quantify the magnitude of the entries of AX and Aǫ.\nLemma 1. Let Φ = ASISX, then for any t > 0 and i 6= j ∈ Q, we have\nP\n( |Φii − Σii| ≥ t ) ≤ 2 exp { −min ( t2n 8e2K , tn 2eK )} ,\nand\nP\n( |Φij − Σij | ≥ t ) ≤ 6 exp { −min ( t2n 72e2K , tn 6eK )} ,\nwhere K = ‖X 2(1) − 1‖ψ1 is a constant, X 2(1) is a chi-square random variable with one degree of freedom and the norm ‖ · ‖ψ1 is defined in Vershynin (2010).\nLemma 1 states that the screening matrix Φ = ASISX for SIS will eventually converge to the covariance matrix Σ in l∞ when n tends to infinity and log p = o(n). Thus, the screening performance of SIS strongly relies on the structure of Σ. In particular, the (asymptotically) necessary and sufficient condition for SIS being strong screening consistent is Σ satisfying the WDD condition. For the noise term, we have the following lemma.\nLemma 2. Let η = ASISǫ. For any t > 0 and i ∈ Q, we have\nP (|ηi| ≥ σt) ≤ 6 exp { −min ( t2n 72e2K , tn 6eK )} ,\nwhere K is defined the same as in Lemma 1.\nThe proof of Lemma 2 is essentially the same as the proof of off-diagonal terms in Lemma 1 and is thus omitted. As indicated before, the necessary and sufficient condition for SIS to be strong screening consistent is that Σ follows WDD. As WDD is usually hard to verify, we consider a stronger sufficient condition inspired by Corollary 1.\nTheorem 5. Let r = maxi 6=j |Σij|. If r < 12ρs , then for any δ > 0, if the sample size satisfies\nn > 144K\n(\n1 + 2ρs+ 2σ/τ\n1− 2ρsr\n)2\nlog(3p/δ), (5)\nwhereK is defined in Lemma 1, then with probability at least 1−δ, Φ = ASISX−2τ−1‖ASISǫ‖∞Ip is weak diagonally dominant with sparsity s and C0 ≥ ρ. In other words, SIS is screening consistent for any β ∈ Bτ (s, ρ).\nThe requirement that maxi 6=j |Σij | < 1/(ρsr) or the necessary and sufficient condition that Σ is WDD strictly constrains the correlation structure of X , causing the difficulty for SIS to be strong screening consistent. Moreover, these constraints are independent of sample sizes and cannot be overcome by large n. As shown below, HOLP relaxes these constraints by a constraint on the conditional number κ = λmax(Σ)/λmin(Σ). For HOLP we instead have the following result.\nLemma 3. Let Φ = AHOLPX. Assume p > c0n for some c0 > 1, then for any C > 0 there exists some 0 < c1 < 1 < c2 and c3 > 0 such that for any t > 0 and any i ∈ Q, j 6= i, we have\nP\n(\n|Φii| < c1κ−1 n\np\n) ≤ 2e−Cn, P ( |Φii| > c2κ n\np\n)\n≤ 2e−Cn\nand\nP\n(\n|Φij | > c4κ 3 2 t\n√ n\np\n)\n≤ 5e−Cn + 2e−t2/2,\nwhere c4 = √ c2(c0−c1)√ c3(c0−1) .\nLemma 3 quantifies the entries of the screening matrix for HOLP . As illustrated in the\nlemma, regardless of the covariance Σ, diagonal terms of Φ are always O(n p ) and the offdiagonal terms are O( √ n p ). Thus, with n ≥ O(s2), Φ is likely to satisfy the WDD condition with large probability. For the noise vector we have the following result.\nLemma 4. Let η = AHOLP ǫ. Assume p > c0n for some c0 > 1, then for any C > 0 there exist the same c1, c2, c3 as in Lemma 3 such that for any t > 0 and i ∈ Q,\nP\n(\n|ηi| ≥ 2σ\n√ c2κt\n1− c−10\n√ n\np\n)\n< 4e−Cn + 2e−t 2/2,\nif n ≥ 8C/(c0 − 1)2.\nThe following theorem results after combining Lemma 3 and 4.\nTheorem 6. Assume p > c0n for some c0 > 1. For any δ > 0, if the sample size satisfies\nn > max\n{\n2C ′κ5(ρs + σ/τ)2 log(3p/δ), 8C (c0 − 1)2 } , (6)\nwhere C ′ = max{4c 2 4\nc2 1 , 4c2 c2 1 (1−c−1 0 )2 } and c1, c2, c3, c4, C are the same constants defined in Lemma\n3, then with probability at least 1− δ, Φ = AHOLPX − 2τ−1‖AHOLP ǫ‖∞Ip is weak diagonally dominant with sparsity s and C0 ≥ ρ. This implies HOLP is screening consistent for any β ∈ Bτ (s, ρ).\nThere are several interesting observations on equation (5) and (6). First, (ρs + σ/τ)2 appears in both expressions, suggesting the optimal screener might also possess a similar sample size requirement in the form of\nnopt = O\n(\n(ρs + σ/τ)2 log(p)\n)\n.\nWe note that ρs evaluates the sparsity and the diversity of the signal β while σ/τ is closely related to the signal-to-noise ratio. Furthermore, HOLP relaxes the correlation constraint r < 1/(2ρs) or the covariance constraint (Σ is WDD) with the conditional number constraint. Thus for any Σ, as long as the sample size is large enough, strong screening consistency is assured. Finally, HOLP provides an example to satisfy the WDD condition in answer to the question raised in Section 2."
    }, {
      "heading" : "6 Concluding remarks",
      "text" : "This article studies the theoretical properties of a class of high dimensional variable screening methods. In particular, we establish a necessary and sufficient condition in the form of weak\ndiagonally dominant screening matrices for strong screening consistency of a linear screener. We verify the condition for both SIS and HOLP under random designs. In addition, we show a close relationship between WDD and the irrepresentable condition, highlighting the difficulty of using SIS in screening for arbitrarily correlated predictors.\nFor future work, it is of interest to see how linear screening can be adapted to compressed sensing (Xue and Zou, 2011) and how techniques such as preconditioning (Jia and Rohe, 2012) can improve the performance of marginal screening and variable selection."
    }, {
      "heading" : "A Proofs for Section 3",
      "text" : "In this section, we prove the two theorems in Section 3.\nProof of Theorem 1. If Φ is weak diagonally dominant with sparsity s and C0 ≥ ρ, we have for any I ⊆ Q and |I| ≤ s− 1,\nΦii > ρmax\n{\n∑ j∈I |Φij + Φkj|, ∑ j∈I |Φij − Φkj|\n}\n+ |Φik| ∀k 6= i ∈ Q \\ I.\nRecall β̂ = Φβ. Suppose S is the index set of non-zero predictors. For any i ∈ S, k 6∈ S, of we fix I = S \\ {i}, we have\n|β̂i| = |Φiiβi + ∑\nj∈I Φijβj| ≥ |βi|(Φii +\n∑\nj∈I\nβj βi Φij)\n= |βi|(Φii + ∑\nj∈I\nβj βi (Φij + Φkj) + Φki − ∑\nj∈I\nβj βi Φkj − Φki)\n> −|βi|( ∑\nj∈I\nβj βi Φkj + Φki) = − |βi| βi ( ∑ j∈I βjΦkj + βiΦki)\n= −sign(βi) · β̂k.\nSimilarly we have\n|β̂i| = |Φiiβi + ∑\nj∈I Φijβj| ≥ |βi|(Φii +\n∑\nj∈I\nβj βi Φij)\n= |βi|(Φii + ∑\nj∈I\nβj βi (Φij − Φkj)− Φki + ∑\nj∈I\nβj βi Φkj + Φki)\n> |βi|( ∑\nj∈I\nβj βi Φkj + Φki) = sign(βi) · β̂k.\nTherefore, whatever value sign(βi) is, it always holds that |β̂i| > |β̂k|. Since this result is true for any i ∈ S, k 6∈ S, we have\nmin i∈S |β̂i| > max k 6∈S |β̂k|.\nTo prove the sign consistency for non-zero coefficients, notice that for i ∈ S,\nΦii > ρ( ∑\nj∈I |Φij + Φkj |+\n∑ j∈I |Φij − Φkj|)/2 ≥ ρ ∑ j∈I |Φij|.\nThus,\nβ̂iβi = Φiiβ 2 i +\n∑ j∈I Φijβjβi = β 2 i (Φii + ∑ j∈I βj βi Φij) > 0.\nOn the other hand, if β̂ is screening consistent, i.e., |β̂i| ≥ |β̂k| and β̂iβi ≥ 0, we can construct S = I ∪ {i} for any fixed i, k, I. Without loss of generality, we assume Φik ≥ 0. If we select β such that βi > 0, then the strong screening consistency implies β̂i > β̂k and β̂i > −β̂k. From β̂i > β̂k we have\nΦiiβi + ∑\nj∈I Φijβj >\n∑ j∈I Φkjβj + Φkiβi.\nBy rearranging terms and selecting β ∈ B(s, ρ) as βi = 1, βj = −ρ · sign(Φij − Φkj), j ∈ S we have\nΦii > − ∑\nj∈I (Φij − Φkj)βj + Φki = ρ\n∑ j∈I |Φij − Φkj|+ |Φki|.\nFollowing the same argument on β̂i ≥ −β̂k with a choice of βi = 1, βj = −ρ · sign(Φij + Φkj), j ∈ S we have\nΦii > ρ ∑\nj∈I |Φij + Φkj|+ |Φki|.\nThis concludes the proof.\nProof of Theorem 2. Proof of Lemma 3 follows almost the same as the sufficiency part of Theorem 1. Notice that now the definition of β̂ becomes\nβ̂ = XT (XXT )−1Xβ +XT (XXT )−1ǫ.\nIf the condition holds, i.e., for any i ∈ S, I = S \\ {i} and k 6∈ S, we have\nΦii > ρmax\n{\n∑ j∈I |Φij + Φkj|, ∑ j∈I |Φij − Φkj|\n}\n+ |Φik|+ 2τ−1‖XT (XXT )−1ǫ‖∞.\nDefining η = XT (XXT )−1ǫ, we have for any i ∈ S,\n|β̂i| = |Φiiβi + ∑\nj∈I Φijβj + ηi| ≥ |βi|(Φii +\n∑\nj∈I\nβj βi Φij + β −1 i ηi)\n= |βi|(Φii + ∑\nj∈I\nβj βi (Φij + Φkj) + Φki + β −1 i (ηi + ηk)− ∑\nj∈I\nβj βi Φkj − Φki − β−1i ηk)\n> −|βi|( ∑\nj∈I\nβj βi Φkj + Φki + β −1 i ηk) = − |βi| βi ( ∑ j∈I βjΦkj + βiΦki + ηk)\n= −sign(βi) · β̂k,\nSimilarly, we can prove |β̂i| > sign(βi) · β̂k, and thus |β̂i| > |β̂k|, which implies that\nmin i∈S |β̂i| > max k 6∈S |β̂k|.\nThe weak sign consistency is established since\nβ̂iβi = Φiiβ 2 i +\n∑ j∈I Φijβjβi + ηiβi = β 2 i (Φii + ∑ j∈I βj βi Φij + β −1 i ηi) > 0,\nfor any βi 6= 0.\nThe tightness of this theorem is given by the case when ǫ = 0, for which the condition\nhas already been shown to be necessary and sufficient in Theorem 1."
    }, {
      "heading" : "B Proofs for Section 4",
      "text" : "In this section, we prove results from Section 4.\nProof of Corollary 1. Letting I ⊆ Q, |I| ≤ s− 1, we have for any i 6= k ∈ Q \\ I,\nΦii− 1\nc max\n{\n∑ j∈I |Φij + Φkj |, ∑ j∈I |Φij − Φkj |\n}\n+ |Φik| ≥ 1− 1\nc\n(\n2(s− 1) c 2s + c 2s\n)\n= 1\n2s > 0.\nThis completes the proof for the first case.\nNow for the second case, notice that the sum of an entire row (except the diagonal term)\ncan be bounded by ∑ j 6=i |Φij| < 2 ∑∞ k=1 r k < 2r 1−r . Therefore, we have\nΦii− (1− r)2\n4r max\n{\n∑ j∈I |Φij + Φkj|, ∑ j∈I |Φij − Φkj|\n}\n− |Φik| > 1− (1− r)2\n2r\n∑ j 6=i |Φij | − r = 0.\nProof of Theorem 3. First, from WDD to IC: Without loss of generality, we assume S = {1, 2}. For any k ∈ Q \\ S, we have ∣\n∣ ∣ ∣ [Φk1 Φk2]Φ −1 S, Ssign(βS)\n∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ sign(β1)(Φk1 − Φ12Φk2) + sign(β2)(−Φ12Φk1 + Φk2) 1− Φ212 ∣ ∣ ∣ ∣ .\nThe r.h.s. becomes |Φk1 + Φk2|(1 − Φ12)/(1 − Φ212) when sign(β1) = sign(β2) and |Φk1 −\nΦk2|(1 + Φ12)/(1− Φ212) when sign(β1) = −sign(β2). In either case we have\n∣ ∣ ∣ ∣ [Φk1 Φk2]Φ −1 S, Ssign(βS) ∣ ∣ ∣ ∣ ≤ max\n{ |Φ1k + Φ2k|, |Φ1k − Φ2k| }\n1− r < ρ−1 1− r .\nSecond, from IC to WDD: Let I ⊆ Q, |I| = 1 and i 6= k ∈ Q \\ I. Without loss of generality, we assume i = 1, k = 2, and we construct S = {1, 2}. Now for any j ∈ I, using the same formula as shown above, we have\n1− θ ≥ ∣ ∣ ∣\n∣\n[Φj1 Φj2]Φ −1 S, Ssign(βS)\n∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ sign(β1)(Φj1 − Φ12Φj2) + sign(β2)(−Φ12Φj1 + Φj2) 1− Φ212 ∣ ∣ ∣ ∣ .\nUsing the same result on the r.h.s., i.e., it becomes |Φk1 + Φk2|(1 − Φ12)/(1 − Φ212) when sign(β1) = sign(β2) and |Φk1−Φk2|(1+Φ12)/(1−Φ212) when sign(β1) = −sign(β2), we have for any j ∈ I that\nmax\n{ |Φ1j + Φ2j |, |Φ1j − Φ2j | } ≤ (1− θ)(1 + r).\nAs a result, we have\n∑ j∈I max\n{ |Φ1j + Φ2j |, |Φ1j − Φ2j | } < (1− θ)(1 + r) < (1− θ)1 + r 1− r ( Φ11 − |Φ12| ) ,\nwhich implies\nΦ11 > 1 1− θ 1− r 1 + r ∑\nj∈I max\n{ |Φ1j + Φ2j |, |Φ1j − Φ2j | } + |Φ12|.\nProof of Theorem 4. We just need to check (4). We prove the absolute value of the first coordinate of CSc, SC −1 S, S · sign(βS) is less than one, and the rest just follow the same argument. From the condition we know C = XTX/n is weak diagonally dominant. Then equation (3) implies that for any I ⊆ Q with |I| = s, we have for any k 6∈ I,\nρ ∑\ni∈I |Cki| < 1.\nNow for any S ⊆ Q with |S| = s, we choose I = S and let αT be the first row of CSc, S =\nXTScXS/n, we have\n|αT (XTSXS/n)−1sign(βS)| ≤ ‖α‖2‖sign(βS)‖2µ−1.\nBecause ρ ∑s i=1 |αi| < 1, we have\nρ2 s ∑\ni=1\nα2i < ρ 2(\ns ∑\ni=1\n|αi|)2 < 1,\nwhich implies that\n|αT (XTSXS/n)−1sign(βS)| ≤ ρ−1 √ sµ−1 =\n√ s ρµ < 1."
    }, {
      "heading" : "C Proofs for Section 5 (SIS)",
      "text" : "Proofs in Section 6 are divided into two parts. In this section, we provide the proofs related to SIS, and leave those pertaining to HOLP to the next section. The proof requires the following proposition,\nProposition 1. Assume Xi ∼ X 2(1), i = 1, 2, · · · , n, where X 2(1) is the chi-square distribution with one degree of freedom. Then for any t > 0, we have\nP (| ∑n\ni=1Xi n − 1| ≥ t) ≤ 2 exp { −min ( t2n 8e2K , tn 2eK )} ,\nwhere K = ‖X 2(1)− 1‖ψ1. Alternatively, for any C > 0, there exists some 0 < c3 < 1 < c4 such that,\nP ( ∑n i=1Xi n ≤ c3) ≤ e−Cn, (7)\nand\nP ( ∑n i=1Xi n ≥ c4) ≤ e−Cn.\nProof. It is a direct application of Proposition 5.16 in Vershynin (2010). Notice that in the proof of Proposition 5.16 we have C = 2e2 and c = e/2 for X 2(1)− 1.\nProof of Lemma 1. For diagonal term we have for any i ∈ {1, 2, · · · , p}\nΦii − Σii = ∑n k=1 x 2 ik\nn − 1,\nwhere xik, k = 1, 2, · · · , n’s are n iid standard normal random variables. Using Proposition 1, we have for any t > 0,\nP\n( |Φii − Σii| ≥ t ) ≤ 2 exp { −min ( t2n 8e2K , tn 2eK )} . (8)\nFor the off-diagonal term, we have for any i 6= j,\nΦij − Σij = ∑n k=1 xikxjk n − Σij\n=\n∑n k=1(xik + xjk) 2 2n − ∑n k=1 x 2 ik 2n − ∑n k=1 x 2 jk 2n − Σij\n= 1\n2\n(∑n k=1(xik + xjk) 2\nn − (2 + 2Σij)\n)\n− 1 2\n(∑n k=1 x 2 ik n − 1 ) − 1 2 ( ∑n k=1 x 2 jk n − 1 ) .\nNotice that xik + xjk ∼ N(0, 2 + 2Σij). Hence the three terms in the above equation can be bounded using the same inequality before, i.e., for any t > 0,\nP\n( |Φij − Σij | ≥ (2 + Σij)t ) ≤ 6 exp { −min ( t2n 8e2 , tn 2e )} .\nClearly, we have Σij ≤ √ Σii √ Σjj ≤ 1. Therefore, we have\nP\n( |Φij − Σij | ≥ t ) ≤ 6 exp { −min ( t2n 72e2K , tn 6eK )} .\nProof of Lemma 2. The proof is essentially the same for proving the off diagonal terms of Φ as in Lemma 1. The only difference is that E(Φij) = Σij while E(Xǫ) = 0. Note\nηi/σ =\n∑n k=1 xikǫk/σ\nn =\n∑n k=1(xik + ǫk/σ) 2 2n − ∑n k=1 x 2 ik 2n − ∑n k=1 ǫ 2 k/σ 2 2n .\nUsing Proposition 1, we have\nP\n( |ηi/σ| ≥ t ) ≤ 6 exp { −min ( t2n 72e2K , tn 6eK )} .\nNow we turn to the proof of Theorem 5.\nProof of Theorem 5. Taking union bound on the results from Lemma 1 and 2, we have for any t > 0,\nP\n(\nmin i∈Q\nΦii ≤ 1− t ) ≤ 2p exp { −min ( t2n 8e2K , tn 2eK )} ,\nP\n(\nmax i 6=j\n|Φij | ≥ r + t ) ≤ 6(p2 − p) exp { −min ( t2n 72e2K , tn 6eK )} ,\nand\nP\n(\nmax i∈Q\n|ηi| ≥ σt ) ≤ 6p exp { −min ( t2n 72e2K , tn 6eK )} .\nThus, when p > 2 we have\nP\n(\nmin i∈Q Φii ≤ 1− t or max i 6=j |Φij | ≥ r + t or max i∈Q\n|ηi| ≥ σt ) ≤ 7p2 exp { −min ( t2n 72e2K , tn 6eK )} .\nIn other words, for any δ > 0, when n ≥ K log(7p2/δ), with probability at least 1 − δ, we have\nmin i∈Q\nΦii ≥ 1− 6 √ 2e\n√\nK log(7p2/δ)\nn , max i 6=j |Φij| ≤ r + 6\n√ 2e\n√\nK log(7p2/δ)\nn ,\nand\nmax i∈Q\n|ηi| ≤ 6 √ 2eσ\n√\nK log(7p2/δ)\nn .\nA sufficient condition for Φ to be weak diagonally dominant is that\nmin i Φii > 2ρsmax i 6=j |Φij |+ 2τ−1max i |ηi|.\nPlugging in the values and solving the inequality, we have (notice that 7p2/δ < 9p2/δ2) Φ is WDD as long as\nn > 144K\n(\n1 + 2ρs+ 2σ/τ\n1− 2ρsr\n)2\nlog(3p/δ).\nThis completes the proof."
    }, {
      "heading" : "D Proofs for Section 5 (HOLP)",
      "text" : "In this section we prove Lemma 3, 4 and Theorem 5. Several propositions and lemmas are needed for establishing the whole theory. We list all prerequisite results without proofs but provide readers references for complete proofs.\nLet P ∈ O(p) be a p × p orthogonal matrix from the orthogonal group O(p). Let H denote the first n columns of P . Then H is in the Stiefel manifold (Chikuse, 2003). In general, the Stiefel manifold Vn,p is the space whose points are n-frames in Rp represented as the set of p× n matrices X such that XTX = In. Mathematically, we can write\nVn,p = {X ∈ Rp×n : XTX = In}.\nThere is a natural measure (dX) called Haar measure on the Stiefel manifold, invariant under both right orthogonal and left orthogonal transformations. We standardize it to obtain a probability measure as [dX ] = (dX)/V (n, p), where V (n, p) = 2nπnp/2/Γn(1/2p).\nLemma 5. (Chikuse, 2003, Page 41-44) Supposed that a p × n random matrix Z has the density function of the form\nfZ(Z) = |Σ|−n/2g(ZTΣ−1Z),\nwhich is invariant under the right-orthogonal transformation of Z, where Σ is a p×p positive definite matrix. Then its orientation Hz = Z(Z TZ)−1/2 has the matrix angular central Gaussian distribution (MACG) with a probability density function\nMACG(Σ) = |Σ|−n/2|HTz Σ−1Hz|−p/2.\nIn particular, if Z is a p× n matrix whose distribution is invariant under both the left- and right-orthogonal transformations, then HY , with Y = BZ for BB T = Σ, has the MACG(Σ) distribution.\nWhen n = 1, the MACG distribution becomes the angular central Gaussian distribution, a description of the multivariate Gaussian distribution on the unit sphere (Watson et al., 1983).\nLemma 6. (Chikuse, 2003, Page 70, Decomposition of the Stiefel manifold) Let H be a p×n random matrix on Vn,p, and write\nH = (H1 H2),\nwith H1 being a p× q matrix where 0 < q < n. Then we can write\nH2 = G(H1)U1,\nwhere G(H1) is any matrix chosen so that (H1 G(H1)) ∈ O(p); as H2 runs over Vn−q,p, U1 runs over Vn−q,p−q and the relationship is one to one. The differential form [dH ] for the normalized invariant measure on Vn,p is decomposed as the product\n[dH ] = [dH1][dU1]\nof those [dH1] and [dU1] on Vq,p and Vn−q,p−q, respectively.\nLemma 7. [Lemma 4 in Fan and Lv (2008)]Let U be uniformly distributed on the Stiefel manifold Vn,p. Then for any C > 0, there exist c ′ 1, c ′ 2 with 0 < c ′ 1 < 1 < c ′ 2, such that\nP\n(\neT1UU T e1 < c ′ 1\nn p\n)\n≤ 2e−Cn,\nand\nP\n(\neT1UU T e1 > c ′ 2\nn p\n)\n≤ 4e−Cn.\nSome of our proof requires concentration properties of a random Gaussian matrix and\nX 21 random variables. For a Wigner matrix, we have the following result.\nLemma 8. Assume Z is a n × p matrix with p > c0n for some c0 > 1. Each entry of Z follows a Gaussian distribution with mean zero and variance one and are independent. Then for any t > 0, with probability at least 1− 2 exp(−t2/2), we have\n(1− c−10 − t/p)2 ≤ λmin(ZZT/p) < λmax(ZZT/p) ≤ (1 + c−10 + t/p)2.\nFor any C > 0, taking t = √ 2Cn, we have with probability 1− 2 exp(−Cn/2),\n(1− c−10 − √ 2C\nc0 √ n )2 ≤ λmin(ZZT/p) ≤ (1 + c−10 +\n√ 2C c0 √ n )2.\nProof. This is essentially Corollary 5.35 in Vershynin (2010).\nThe conditional number of Σ is controled by κ, which simulaneously controls the largest\nand the smallest eigenvalues.\nProposition 2. Assume the conditional number of Σ is κ and Σii = 1 for i = 1, 2, · · · , p,\nthen we have\nλmin(Σ) ≥ κ−1 and λmax(Σ) ≤ κ.\nProof. Notice that p = tr(Σ) = ∑p\ni=1 λi. Therefore, we have\np/λmax ≥ pκ−1 and p/λmin(Σ) ≤ pκ,\nwhich completes the proof.\nNow we prove the main results for HOLP.\nProof of Lemma 3. Consider a transformed n × p random matrix Z = XΣ−1/2, which, by definition, follows standard multivariate Gaussian. Consider its SVD decomposition,\nZ = V DUT ,\nwhere V ∈ O(n), D is a diagonal matrix and U is a p× n random matrix belonging to the Stiefel manifold Vn,p. With such notion, we can rewrite the projection matrix as\nXT (XXT )−1X = Σ1/2U(UTΣU)−1UTΣ1/2 = HHT ,\nwhere H = Σ1/2U(UTΣU)−1/2 and H ∈ Vn,p−1. Therefore, the two quantities that we are interested in are Φii = e T i HH Tei (diagonal term) and Φij = e T i HH Tej (off-diagonal term), where eTi is the p−dimensional unit vector with the ith coordinate being one. The proof is divided into two parts, where in the first part we consider diagonal terms and the second part takes care of off-diagonal terms.\nPart I: First, we consider the diagonal term eTi HH Tei. Recall the definition of H and\neTi HH Tei = e T i Σ\n1 2U(UTΣU)−1UTΣ 1\n2 ei.\nThere always exists some orthogonal matrix Q that rotates the vector Σ 1\n2 ei to the direction\nof e1, i.e,\nΣ 1 2v = ‖Σ 12v‖Qe1.\nThen we have\neTi HH Tei = ‖Σ\n1 2 ei‖2eT1QTU(UTΣU)−1UTQe1 = ‖Σ 1 2 v‖2eT1 Ũ(UTΣU)−1Ũe1,\nwhere Ũ = QTU is uniformly distributed on Vn,p, because U is uniformly distributed on Vn,p (see discussion in the beginning). Now the magnitude of eTi HH T eI can be evaluated in two parts. For the norm of the vector Σ 1 2 v, we have\nλmin(Σ) ≤ eTi Σei = ‖Σ 1 2 e)i‖2 ≤ λmax(Σ), (9)\nand for the remaining part,\neT1 Ũ(U TΣU)−1Ũe1 ≤ λmax((UTΣU)−1)‖Ũe1‖2 ≤ λmin(Σ)−1‖Ũe1‖2,\nand\neT1 Ũ(U TΣU)−1Ũe1 ≥ λmin((UTΣU)−1)‖Ũe1‖2 ≥ λmax(Σ)−1‖Ũe1‖2.\nConsequently, we have\neTi HH Tei ≤\nλmax(Σ) λmin(Σ) eT1UU T e1, e T i HH Tei ≥ λmin(Σ) λmax(Σ) eT1 UU T e1. (10)\nTherefore, following Proposition 7, for any C > 0 we have\nP\n(\neTi HH Tei < c ′ 1c4κ\n−1n\np\n)\n≤ 2e−Cn,\nand\nP\n(\neTi HH Tei > c ′ 2c −1 4 κ\nn1\np\n)\n≤ 2e−Cn.\nDenoting c′1c4 by c1 and c ′ 2c −1 4 by c2, we obtain the equation in Lemma 3.\nPart II: Second, for off-diagonal terms, although the proof is almost identical to the proof of Lemma 5 in Wang and Leng (2013), we still provide a complete version here due to the importance of this result.\nThe proof depends on the decomposition of Stiefel manifold. Without loss of generality, we prove the bound only for eT2HH Te1, then the other off-diagonal terms should follow exactly the same argument. According to Lemma 6, we can decompose H = (T1, H2) with T1 = G(H2)H1, where H2 is a p× (n− 1) matrix, H1 is a (p−n+1)× 1 vector and G(H2) is a matrix such that (G(H2), H2) ∈ O(p). The invariant measure on the Stiefel manifold can\nbe decomposed as\n[H ] = [H1][H2]\nwhere [H1] and [H2] are Haar measures on V1,n−p+1, Vn−1,p (Notice that q = n − 1 in this decomposition) respectively. As pointed out before, H has the MACG(Σ) distribution, which possesses a density as\np(H) ∝ |HTΣ−1H|−p/2[dH ].\nUsing the identity for matrix determinant\n∣ ∣ ∣ ∣ ∣ A B\nC D\n∣ ∣ ∣ ∣ ∣ = |A||D − CA−1B| = |D||A− BD−1C|,\nwe have\nP (H1, H2) ∝ |HT2 Σ−1H2|−p/2(T T1 Σ−1T1 − T T1 Σ−1H2(HT2 Σ−1H2)−1HT2 Σ−1T1)−p/2\n= |HT2 Σ−1H2|−p/2(HT1 G(H2)T (Σ−1 − Σ−1H2(HT2 Σ−1H2)−1HT2 Σ−1)G(H2)H1)−p/2 = |HT2 Σ−1H2|−p/2(HT1 G(H2)TΣ−1/2(I − T2)Σ−1/2G(H2)H1)−p/2,\nwhere T2 = Σ −1/2H2(H T 2 Σ −1H2) −1HT2 Σ −1/2 is an orthogonal projection onto the linear space spanned by the columns of Σ−1/2H2. It is easy to verify the following result by using the definition of G(H2),\n[Σ1/2G(H2)(G(H2) TΣG(H2)) −1/2, Σ−1/2H2(H T 2 Σ −1H2) −1/2] ∈ O(p),\nand therefore we have\nI − T2 = Σ1/2G(H2)(G(H2)TΣG(H2))−1G(H2)TΣ1/2,\nwhich simplifies the density function as\nP (H1, H2) ∝ |HT2 Σ−1H2|−p/2(HT1 (G(H2)TΣG(H2))−1H1)−p/2.\nNow it becomes clear thatH1|H2 follows the Angular Central Gaussian distribution ACG(Σ′),\nwhere\nΣ′ = G(H2) TΣG(H2).\nNext, we relate the target quantity eT1HH Te2 to the distribution of H1. Notice that for\nany orthogonal matrix Q ∈ O(n), we have\neT1HH Te2 = e T 1HQQ THTe2 = e T 1H ′H ′T e2.\nWrite H ′ = HQ = (T ′1, H ′ 2), where T ′ 1 = [T ′(1) 1 , T ′(2) 1 , · · · , T ′(p) 1 ], H ′ 2 = [H ′(i,j) 2 ]. If we choose Q such that the first row of H ′2 are all zero (this is possible as we can choose the first column of Q being the first row of H upon normalizing), i.e.,\neT1H ′ = [T ′(1) 1 , 0, · · · , 0] eT2H ′ = [T ′(2) 1 , H ′(2,1) 2 , · · · , H ′(2,n−1) 2 ],\nthen immediately we have eT1HH T e2 = e T 1H ′H ′T e2 = T ′(1) 1 T ′(2) 1 . This indicates that\neT1HH Te2\n(d) = T (1) 1 T (2) 1\n∣ ∣ ∣ ∣ eT1H2 = 0.\nAs shown at the beginning, H1 followsACG(Σ ′) conditional onH2. LetH1 = (h1, h2, · · · , hp)T\nand let xT = (x1, x2, · · · , xp−n+1) ∼ N(0,Σ′), then we have\nhi (d) = xi √\nx21 + · · ·+ x2p−n+1 .\nNotice that T1 = G(H2)H1, a linear transformation on H1. Defining y = G(H2)x, we have\nT (i) 1 (d) = yi √\ny21 + · · ·+ y2p , (11)\nwhere y ∼ N(0, G(H)Σ′G(H)T ) is a degenerate Gaussian distribution. This degenerate distribution contains an interesting form. Letting z ∼ N(0,Σ), we know y can be expressed as y = G(H)G(H)Tz. Write G(H2)\nT as [g1, g2] where g1 is a (p− n+ 1)× 1 vector and g2 is a (p− n+ 1)× (p− 1) matrix, then we have\nG(H2)G(H2) T =\n(\ngT1 g1 g T 1 g2 gT2 g1 g T 2 g2\n)\n.\nWe can also write HT2 = [0n−1,1, h2] where h2 is a (n − 1) × (p − 1) matrix, and using the\northogonality, i.e., [H2 G(H2)][H2 G(H2)] T = Ip, we have\ngT1 g1 = 1, g T 1 g2 = 01,p−1 and g T 2 g2 = Ip−1 − h2hT2 .\nBecause h2 is a set of orthogonal basis in the p − 1 dimensional space, gT2 g2 is therefore an orthogonal projection onto the space {h2}⊥ and gT2 g2 = AAT where A = gT2 (g2gT2 )−1/2 is a (p− 1)× (p− n) orientation matrix on {h2}⊥. Together, we have\ny =\n(\n1 0 0 AAT\n)\nz.\nThis relationship allows us to marginalize y1 out with y following a degenerate Gaussian distribution.\nWe now turn to transform the condition eT1H2 = 0 onto constraints on the distribution of\nT (i) 1 . Letting t 2 1 = e T 1HH Te1, then e T 1H2 = 0 is equivalent to T (1)2 1 = e T 1HH Te1 = t 2 1, which implies that\neT1HH Te2\n(d) = T (1) 1 T (2) 1\n∣ ∣ ∣ ∣ T (1)2 1 = e T 1HH Te1.\nBecause the magnitude of eT1HH Te1 has been obtained in Part I, we can now condition on the value of eT1HH Te1 to obtain the bound on T (2) 1 . From T (1)2 1 = t 2 1, we obtain that,\n(1− t21)y21 = t21(y22 + y23 + · · ·+ y2p). (12)\nNotice this constraint is imposed on the norm of ỹ = (y2, y3, · · · , yp) and is thus independent of (y2/‖ỹ‖, · · · , yp/‖ỹ‖). Equation (12) also implies that\n(1− t21)(y21 + y22 + · · ·+ y2p) = y22 + y23 + · · ·+ y2p. (13)\nTherefore, combining (11) with (12), (13) and integrating y1 out, we have\nT (i) 1 | T (1) 1 = t1 (d) =\n√\n1− t21yi √\ny22 + · · ·+ y2p , i = 2, 3, · · · , p,\nwhere (y2, y3, · · · , yp) ∼ N(0, AATΣ22AAT ) with Σ22 being the covariance matrix of z2, · · · , zp. To bound the numerator, we use the classical tail bound on the normal distribution as\nfor any t > 0, (σi = √ var(yi) ≤ √ λmax(AATΣ22AAT ) ≤ λmax(Σ)1/2 ≤ κ1/2, Proposition 2),\nP (|yi| > tσi) ≤ 2e−t 2/2. (14)\nFor the denominator, letting z̃ ∼ N(0, Ip−1), we have\nỹ = AATΣ 1/2 22 z̃ and ỹ T ỹ = z̃TΣ 1/2 22 AA TΣ 1/2 22 z̃ (d) =\np−n ∑\ni=1\nλiX 2i (1),\nwhere X 2i (1) are iid chi-square random variables and λi are non-zero eigenvalues of matrix Σ\n1/2 22 AA TΣ 1/2 22 . Here λi’s are naturally upper bounded by λmax(Σ). To give a lower bound,\nnotice that Σ 1/2 22 AA TΣ 1/2 22 and AΣ22A T possess the same set of non-zero eigenvalues, thus\nmin i\nλi ≥ λmin(AΣ22AT ) ≥ λmin(Σ).\nTherefore,\nλmin(Σ) ∑p−n i=1 X 2i (1) p− n ≤ ỹT ỹ p− n ≤ λmax(Σ) ∑p−n i=1 X 2i (1) p− n .\nThe quantity ∑p−n i=1 X 2i (1)\np−n can be bounded by Proposition 1. Combining with Proposition 2,\nwe have for any C > 0, there exists some c3 > 0 such that\nP\n( ỹT ỹ/(p− n) < c3κ−1 ) ≤ e−C(p−n).\nTherefore, T (2) 1 can be bounded as\nP\n( |T (2)1 | < √ 1− t21κt√ c3 √ p− n ∣ ∣T (1) 1 = t1 ) ≤ e−C(p−n) + 2e−t2/2.\nUsing the results from the diagonal term, we have\nP\n(\nt21 > c2κ n\np\n) ≤ 2e−Cn. and P ( t21 < c1κ −1n\np\n)\n≤ 2e−Cn.\nConsequently, we have\nP\n(\n|eT1HHTe2| > c4κ 3 2 t\n√ n\np\n)\n= P\n(\n|T (1)1 T (2) 1 | > c4κ 3 2 t\n√ n\np\n∣ ∣T (1) 1 = t1\n)\n≤ P (\nT (1)2 1 > c2κ\nn p |T (1)1 = t1\n)\n+ P\n( |T (2)1 | < κt √ 1− c1n/p√ c3 √ p− n ∣ ∣T (1) 1 = t1 )\n≤ 5e−Cn + 2e−t2/2,\nwhere c4 = √ c2(c0−1)√ c3(c0−1) .\nProof of Lemma 4. Notice that conditioning on X , for any fixed index i, eTi X T (XXT )−1ǫ follows a normal distribution with mean zero and variance σ2‖eTi XT (XXT )−1‖22. We can first bound the variance and then apply the normal tail bound (14) again to obtain an upper bound for the error term.\nThe variance term follows\nσ2eTi X T (XXT )−2Xei ≤ σ2λmax ( (XXT )−1 ) eTi HH Tei.\nThe eTi HH Tei part can be bounded according to Lemma 3, while the first part follows\nλmax ( (XXT )−1 ) = λmax ( (ZΣZT )−1 ) ≤ λ−1min(ZZT )λ−1min(Σ) = κ p λ−1min(p −1ZZT ).\nThus, using Lemma 8 and 3, we have\nσ2‖eTi XT (XXT )−1‖22 ≤ 4σ2c2 (1− c−10 )2 nκ2 p2 , (15)\nwith probability at least 1− 4 exp(−Cn) if n > 8C/(c0 − 1)2. Now combining (15) and (14) we have for any t > 0,\nP\n(\n|eTi XT (XXT )−1ǫ| ≥ 2σ\n√ c2κt\n1− c−10\n√ n\np\n)\n< 4e−Cn + 2e−t 2/2.\nProof of Theorem 6. The proof depends on Lemma 3 and 4, and a careful choice of the value of t in these two lemmas. We first take union bounds of the two lemmas to obtain\nP (min i∈Q\n|Φii| < c1κ−1 n p ) ≤ 2pe−Cn,\nP (max i 6=j\n|Φij | > c4κ 3 2 t\n√ n\np ) ≤ 5(p2 − p)e−Cn + 2(p2 − p)e−t2/2,\nand\nP\n(\n‖XT (XXT )−1ǫ‖∞ ≥ 2σ\n√ c2κt\n1− c−10\n√ n\np\n)\n< 4pe−Cn + 2pe−t 2/2.\nNotice that once we have\nmin i |Φii| > 2sρmax ij |Φij |+ 2τ−1‖XT (XXT )−1ǫ‖∞, (16)\nthen the proof is complete because Φ− 2τ−1‖XT (XXT )−1ǫ‖∞ is already a weak diagonally dominant matrix. Let t = √ Cn/ν. The above equation then requires\nc1κ −1n p − 2c4\n√ Cκ 3 2 sρ\nν\nn p − 2σ\n√ c2Cκt\n(1− c−10 )τν n p\n= (c1κ −1 − 2c4\n√ Cκ 3 2 sρ\nν − 2σ\n√ c2Cκ\n(1− c−10 )τν ) n p > 0,\nwhich implies that\nν > 2c4\n√ Cκ5/2ρs\nc1 +\n2σ √ c2Cκ 2\nc1(1− c−10 )τ = C1κ\n5/2ρs+ C2κ 2τ−1σ > 1, (17)\nwhere C1 = 2c4\n√ C\nc1 , C2 =\n2 √ c2C\nc1(1−c−10 ) . Therefore, the probability that (16) does not hold is\nP\n(\n{ (16) does not hold }\n)\n< (p+ 5p2)e−Cn + 2p2e−Cn/ν < (7 + 1\nn )p2e−Cn/ν 2 ,\nwhere the second inequality is due to the fact that p > n and ν > 1. Now for any δ > 0, (16) holds with probability at least 1− δ requires that\nn ≥ ν 2\nC\n( log(7 + 1/n) + 2 log p− log δ ) ,\nwhich is certainly satisfied if (notice that √ 8 < 3),\nn ≥ 2ν 2\nC log\n3p\nδ .\nNow pushing ν to the limit as shown in (17) gives the precise condition we need, i.e.\nn > 2C ′κ5(ρs+ τ−1σ)2 log 3p\nδ ,\nwhere C ′ = max{4c 2 4\nc2 1 , 4c2 c2 1 (1−c−1 0 )2 }."
    } ],
    "references" : [ {
      "title" : "Compressive sensing",
      "author" : [ "R. Baraniuk" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "Baraniuk,? \\Q2007\\E",
      "shortCiteRegEx" : "Baraniuk",
      "year" : 2007
    }, {
      "title" : "Simultaneous analysis of lasso and dantzig selector",
      "author" : [ "P.J. Bickel", "Y. Ritov", "A.B. Tsybakov" ],
      "venue" : null,
      "citeRegEx" : "Bickel et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bickel et al\\.",
      "year" : 2009
    }, {
      "title" : "The dantzig selector: statistical estimation when p is much larger than n",
      "author" : [ "E. Candes", "T. Tao" ],
      "venue" : null,
      "citeRegEx" : "Candes and Tao,? \\Q2007\\E",
      "shortCiteRegEx" : "Candes and Tao",
      "year" : 2007
    }, {
      "title" : "Statistics on special manifolds, volume 174",
      "author" : [ "Y. Chikuse" ],
      "venue" : null,
      "citeRegEx" : "Chikuse,? \\Q2003\\E",
      "shortCiteRegEx" : "Chikuse",
      "year" : 2003
    }, {
      "title" : "High dimensional variable selection via tilting",
      "author" : [ "H. Cho", "P. Fryzlewicz" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Cho and Fryzlewicz,? \\Q2012\\E",
      "shortCiteRegEx" : "Cho and Fryzlewicz",
      "year" : 2012
    }, {
      "title" : "Compressed sensing",
      "author" : [ "D.L. Donoho" ],
      "venue" : "IEEE Transactions on Information",
      "citeRegEx" : "Donoho,? \\Q2006\\E",
      "shortCiteRegEx" : "Donoho",
      "year" : 2006
    }, {
      "title" : "Variable selection via nonconcave penalized likelihood and its oracle properties",
      "author" : [ "J. Fan", "R. Li" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Fan and Li,? \\Q2001\\E",
      "shortCiteRegEx" : "Fan and Li",
      "year" : 2001
    }, {
      "title" : "Sure independence screening for ultrahigh dimensional feature space",
      "author" : [ "J. Fan", "J. Lv" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Fan and Lv,? \\Q2008\\E",
      "shortCiteRegEx" : "Fan and Lv",
      "year" : 2008
    }, {
      "title" : "Preconditioning to comply with the irrepresentable condition. arXiv preprint arXiv:1208.5584",
      "author" : [ "J. Jia", "K. Rohe" ],
      "venue" : null,
      "citeRegEx" : "Jia and Rohe,? \\Q2012\\E",
      "shortCiteRegEx" : "Jia and Rohe",
      "year" : 2012
    }, {
      "title" : "On model selection consistency of m-estimators with geometrically decomposable penalties",
      "author" : [ "J.D. Lee", "Y. Sun", "J.E. Taylor" ],
      "venue" : "Advances in Neural Processing Information Systems",
      "citeRegEx" : "Lee et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2013
    }, {
      "title" : "Robust rank correlation based screening",
      "author" : [ "G. Li", "H. Peng", "J. Zhang", "L Zhu" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2012
    }, {
      "title" : "Forward regression for ultra-high dimensional variable screening",
      "author" : [ "H. Theory. Wang" ],
      "venue" : null,
      "citeRegEx" : "Wang,? \\Q2009\\E",
      "shortCiteRegEx" : "Wang",
      "year" : 2009
    }, {
      "title" : "High dimensional ordinary least square projection for variable",
      "author" : [ "C. Leng" ],
      "venue" : "American Statistical Association,",
      "citeRegEx" : "X. and Leng,? \\Q2013\\E",
      "shortCiteRegEx" : "X. and Leng",
      "year" : 2013
    }, {
      "title" : "Nearly unbiased variable selection under minimax concave penalty",
      "author" : [ "Zhang", "C.-H" ],
      "venue" : null,
      "citeRegEx" : "Zhang and C..H.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhang and C..H.",
      "year" : 2010
    }, {
      "title" : "The sparsity and bias of the lasso selection",
      "author" : [ "Zhang", "C.-H", "J. Huang" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2008
    }, {
      "title" : "On model selection consistency of lasso",
      "author" : [ "P. Zhao", "B. Yu" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Zhao and Yu,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhao and Yu",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al.",
      "startOffset" : 123,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al.",
      "startOffset" : 123,
      "endOffset" : 153
    }, {
      "referenceID" : 6,
      "context" : "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al., 2009; Zhang, 2010).",
      "startOffset" : 196,
      "endOffset" : 288
    }, {
      "referenceID" : 2,
      "context" : "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al., 2009; Zhang, 2010).",
      "startOffset" : 196,
      "endOffset" : 288
    }, {
      "referenceID" : 1,
      "context" : "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al., 2009; Zhang, 2010).",
      "startOffset" : 196,
      "endOffset" : 288
    }, {
      "referenceID" : 7,
      "context" : "However, this method relies on a strong assumption that the marginal correlations between the response and the important predictors are high (Fan and Lv, 2008), which is easily violated in the practice.",
      "startOffset" : 141,
      "endOffset" : 159
    }, {
      "referenceID" : 15,
      "context" : "We relate WDD to the irrepresentable condition (Zhao and Yu, 2006) that is necessary 2",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "Bearing these concerns in mind, Fan and Lv (2008) propose the concept of “variable screening”, a fast technique that reduces data dimensionality from p to a size comparable to n, with all predictors having non-zero coefficients preserved.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "Bearing these concerns in mind, Fan and Lv (2008) propose the concept of “variable screening”, a fast technique that reduces data dimensionality from p to a size comparable to n, with all predictors having non-zero coefficients preserved. They propose a marginal correlation based fast screening technique “Sure Independence Screening” (SIS) that can preserve signals with large probability. However, this method relies on a strong assumption that the marginal correlations between the response and the important predictors are high (Fan and Lv, 2008), which is easily violated in the practice. Li et al. (2012) extends the marginal correlation to the Spearman’s rank correlation, which is shown to gain certain robustness but is still limited by the same strong assumption.",
      "startOffset" : 32,
      "endOffset" : 612
    }, {
      "referenceID" : 6,
      "context" : "Bearing these concerns in mind, Fan and Lv (2008) propose the concept of “variable screening”, a fast technique that reduces data dimensionality from p to a size comparable to n, with all predictors having non-zero coefficients preserved. They propose a marginal correlation based fast screening technique “Sure Independence Screening” (SIS) that can preserve signals with large probability. However, this method relies on a strong assumption that the marginal correlations between the response and the important predictors are high (Fan and Lv, 2008), which is easily violated in the practice. Li et al. (2012) extends the marginal correlation to the Spearman’s rank correlation, which is shown to gain certain robustness but is still limited by the same strong assumption. Wang (2009) and Cho and Fryzlewicz (2012) take a different approach to attack the screening problem.",
      "startOffset" : 32,
      "endOffset" : 787
    }, {
      "referenceID" : 4,
      "context" : "Wang (2009) and Cho and Fryzlewicz (2012) take a different approach to attack the screening problem.",
      "startOffset" : 16,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "Wang (2009) and Cho and Fryzlewicz (2012) take a different approach to attack the screening problem. They both adopt variants of a forward selection type algorithm that includes one variable at a time for constructing a candidate variable set for further refining. These methods eliminate the strong marginal assumption in Fan and Lv (2008) and have been shown to achieve better empirical performance.",
      "startOffset" : 16,
      "endOffset" : 341
    }, {
      "referenceID" : 4,
      "context" : "Wang (2009) and Cho and Fryzlewicz (2012) take a different approach to attack the screening problem. They both adopt variants of a forward selection type algorithm that includes one variable at a time for constructing a candidate variable set for further refining. These methods eliminate the strong marginal assumption in Fan and Lv (2008) and have been shown to achieve better empirical performance. However, such improvement is limited by the extra computational burden caused by their iterative framework, which is reported to be high when p is large (Wang and Leng, 2013). To ameliorate concerns in both screening performance and computational efficiency, Wang and Leng (2013) develop a new type of screening method termed “High-dimensional ordinary least-square projection” (HOLP ).",
      "startOffset" : 16,
      "endOffset" : 682
    }, {
      "referenceID" : 7,
      "context" : "To reduce the dimensionality, Fan and Lv (2008) suggest a variable screening framework by finding a 3",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 15,
      "context" : "It is weaker than variable selection consistency in Zhao and Yu (2006). The requirement in (2) can be seen as a relaxation of the sign consistency defined in Zhao and Yu (2006), as no requirement for β̂i, i 6∈ S is needed.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "It is weaker than variable selection consistency in Zhao and Yu (2006). The requirement in (2) can be seen as a relaxation of the sign consistency defined in Zhao and Yu (2006), as no requirement for β̂i, i 6∈ S is needed.",
      "startOffset" : 52,
      "endOffset" : 177
    }, {
      "referenceID" : 15,
      "context" : "4 Relationship with the irrepresentable condition The weak diagonally dominant (WDD) condition is closely related to the strong irrepresentable condition (IC) proposed in Zhao and Yu (2006) as a necessary and sufficient condition for sign consistency of lasso.",
      "startOffset" : 171,
      "endOffset" : 190
    }, {
      "referenceID" : 15,
      "context" : "If Φii = 1, ∀i and |Φij| < c/(2s), ∀i 6= j for some 0 ≤ c < 1 as defined in Corollary 1 and 2 in Zhao and Yu (2006), then Φ is a weak diagonally dominant matrix with sparsity s and C0 ≥ 1/c.",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 15,
      "context" : "If Φii = 1, ∀i and |Φij| < c/(2s), ∀i 6= j for some 0 ≤ c < 1 as defined in Corollary 1 and 2 in Zhao and Yu (2006), then Φ is a weak diagonally dominant matrix with sparsity s and C0 ≥ 1/c. If |Φij| < r|i−j|, ∀i, j for some 0 < r < 1 as defined in Corollary 3 in Zhao and Yu (2006), then Φ is a weak diagonally dominant matrix with sparsity s and C0 ≥ (1−r)2/(4r).",
      "startOffset" : 97,
      "endOffset" : 283
    }, {
      "referenceID" : 7,
      "context" : "The strong screening consistency defined in this article is stronger than conditions commonly used in justifying screening procedures as in Fan and Lv (2008).",
      "startOffset" : 140,
      "endOffset" : 158
    }, {
      "referenceID" : 7,
      "context" : "The theory developed in this section can be easily extended to a broader family of distributions, for example, where ǫ follows a sub-Gaussian distribution (Vershynin, 2010) and X follows an elliptical distribution (Fan and Lv, 2008; Wang and Leng, 2013).",
      "startOffset" : 214,
      "endOffset" : 253
    }, {
      "referenceID" : 8,
      "context" : "For future work, it is of interest to see how linear screening can be adapted to compressed sensing (Xue and Zou, 2011) and how techniques such as preconditioning (Jia and Rohe, 2012) can improve the performance of marginal screening and variable selection.",
      "startOffset" : 163,
      "endOffset" : 183
    }, {
      "referenceID" : 0,
      "context" : "References Baraniuk, R. (2007). Compressive sensing.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector.",
      "startOffset" : 11,
      "endOffset" : 146
    }, {
      "referenceID" : 0,
      "context" : "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705–1732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n.",
      "startOffset" : 11,
      "endOffset" : 273
    }, {
      "referenceID" : 0,
      "context" : "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705–1732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313–2351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174.",
      "startOffset" : 11,
      "endOffset" : 411
    }, {
      "referenceID" : 0,
      "context" : "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705–1732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313–2351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting.",
      "startOffset" : 11,
      "endOffset" : 526
    }, {
      "referenceID" : 0,
      "context" : "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705–1732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313–2351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593–622. Donoho, D. L. (2006). Compressed sensing.",
      "startOffset" : 11,
      "endOffset" : 690
    }, {
      "referenceID" : 0,
      "context" : "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705–1732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313–2351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593–622. Donoho, D. L. (2006). Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289–1306. Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties.",
      "startOffset" : 11,
      "endOffset" : 795
    }, {
      "referenceID" : 0,
      "context" : "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705–1732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313–2351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593–622. Donoho, D. L. (2006). Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289–1306. Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348–1360. Fan, J. and Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space.",
      "startOffset" : 11,
      "endOffset" : 972
    }, {
      "referenceID" : 0,
      "context" : "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705–1732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313–2351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593–622. Donoho, D. L. (2006). Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289–1306. Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348–1360. Fan, J. and Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849–911. Jia, J. and Rohe, K. (2012). Preconditioning to comply with the irrepresentable condition.",
      "startOffset" : 11,
      "endOffset" : 1163
    }, {
      "referenceID" : 0,
      "context" : "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705–1732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313–2351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593–622. Donoho, D. L. (2006). Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289–1306. Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348–1360. Fan, J. and Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849–911. Jia, J. and Rohe, K. (2012). Preconditioning to comply with the irrepresentable condition. arXiv preprint arXiv:1208.5584. Lee, J. D., Sun, Y., and Taylor, J. E. (2013). On model selection consistency of m-estimators with geometrically decomposable penalties.",
      "startOffset" : 11,
      "endOffset" : 1304
    }, {
      "referenceID" : 11,
      "context" : "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 11,
      "context" : "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512–1524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening.",
      "startOffset" : 0,
      "endOffset" : 181
    }, {
      "referenceID" : 11,
      "context" : "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512–1524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6.",
      "startOffset" : 0,
      "endOffset" : 362
    }, {
      "referenceID" : 11,
      "context" : "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512–1524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6. Wiley New York. Xue, L. and Zou, H. (2011). Sure independence screening and compressed random sensing.",
      "startOffset" : 0,
      "endOffset" : 439
    }, {
      "referenceID" : 11,
      "context" : "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512–1524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6. Wiley New York. Xue, L. and Zou, H. (2011). Sure independence screening and compressed random sensing. Biometrika, 98(2):371–380. Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty.",
      "startOffset" : 0,
      "endOffset" : 546
    }, {
      "referenceID" : 11,
      "context" : "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512–1524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6. Wiley New York. Xue, L. and Zou, H. (2011). Sure independence screening and compressed random sensing. Biometrika, 98(2):371–380. Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2):894–942. Zhang, C.-H. and Huang, J. (2008). The sparsity and bias of the lasso selection in highdimensional linear regression.",
      "startOffset" : 0,
      "endOffset" : 688
    }, {
      "referenceID" : 11,
      "context" : "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512–1524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6. Wiley New York. Xue, L. and Zou, H. (2011). Sure independence screening and compressed random sensing. Biometrika, 98(2):371–380. Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2):894–942. Zhang, C.-H. and Huang, J. (2008). The sparsity and bias of the lasso selection in highdimensional linear regression. The Annals of Statistics, 36(4):1567–1594. Zhao, P. and Yu, B. (2006). On model selection consistency of lasso.",
      "startOffset" : 0,
      "endOffset" : 842
    }, {
      "referenceID" : 3,
      "context" : "Then H is in the Stiefel manifold (Chikuse, 2003).",
      "startOffset" : 34,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "[Lemma 4 in Fan and Lv (2008)]Let U be uniformly distributed on the Stiefel manifold Vn,p.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 11,
      "context" : "Part II: Second, for off-diagonal terms, although the proof is almost identical to the proof of Lemma 5 in Wang and Leng (2013), we still provide a complete version here due to the importance of this result.",
      "startOffset" : 107,
      "endOffset" : 128
    } ],
    "year" : 2017,
    "abstractText" : "Variable screening is a fast dimension reduction technique for assisting high dimensional feature selection. As a preselection method, it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model. The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones. When the data dimension p is substantially larger than the sample size n, variable screening becomes crucial as 1) Faster feature selection algorithms are needed; 2) Conditions guaranteeing selection consistency might fail to hold. This article studies a class of linear screening methods and establishes consistency theory for this special class. In particular, we prove the weak diagonally dominant (WDD) condition is a necessary and sufficient condition for strong screening consistency. As concrete examples, we show two screening methods SIS andHOLP are both strong screening consistent (subject to additional constraints) with large probability if n > O((ρs + σ/τ)2 log p) under random designs. In addition, we relate the WDD condition to the irrepresentable condition, and highlight limitations of SIS.",
    "creator" : "LaTeX with hyperref package"
  }
}