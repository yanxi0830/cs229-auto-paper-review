{
  "name" : "1602.05916.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning",
    "authors" : [ "Niloofar Yousefi", "Yunwen Lei", "Marius Kloft", "Mansooreh Mollaghasemi", "Georgios Anagnastapolous" ],
    "emails" : [ "niloofaryousefi@knights.ucf.edu,", "yunwen.lei@hotmail.com,", "kloft@hu-berlin.de,", "Mansooreh.Mollaghasemi@ucf.edu", "georgio@fit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 2.\n05 91\n6v 1\n[ cs\n.L G\nKeywords: Multi-task Learning, Kernel Methods, Generalization Bound, Local Rademacher Complexity\n1 Introduction\nMulti-Task Learning (MTL) refers to the concurrent learning of a collection of conceptually related tasks, each of which features its own data. Such an approach can be advantageous over learning each task independently, when the tasks lack a sufficient body of observed data. MTL is accomplished by jointly constraining the tasks’ hypothesis spaces, so that tasks mutually regularize the learning of others, based on their inter-task relatedness; this exchange mechanism is often referred to as information sharing. Pioneering works on MTL include [16, 9, 2, 4]. Nowadays, MTL frameworks are routinely employed in a variety of settings. Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.\nMTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect. 1.3. Let T denote the number of tasks being co-learned and n denote the number of available observations per task. Then, in terms of convergence rates w.r.t. n and T , the fastest-converging error or excess risk bounds derived in these works, whether distribution- or data-dependent, are of the order O(1/ √ nT ).\nMore recently, the seminal works in [26] and [7] introduced a more nuanced variant of these complexities, termed Local Rademacher Complexity (LRC) (as opposed to the original Global Rademacher Complexity (GRC)). This new, modified function class complexity measure is attention-worthy, since, as shown in [7], a LRCs-based (local) analysis is capable of producing more rapidly-converging excess risk bounds, when compared to the ones obtained via a GRC (global) analysis. This can be attributed to the fact that, unlike LRCs, GRCs ignore the fact that learning algorithms typically choose well-performing hypotheses that belong only to a subset of the entire hypothesis space under consideration. The end result of this distinction empowers a local analysis to provide less conservative and, hence, sharper bounds than when a global analysis is employed. To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].\n1.1 Our Contributions\nThrough one of Bousquet’s Talagrand-type concentration inequalities adapted to the MTL case, this paper’s main contribution is the derivation of sharp bounds on the excess MTL risk in terms of the distribution- and data-dependent LRC. For a given number of tasks T , these bounds admit faster (asymptotic) convergence characteristics in the number of observations per task n, when compared to corresponding bounds hinging on the GRC. Thence, these faster rates allow for heighten confidence that the MTL hypothesis selected by a learning algorithm approaches the best-in-class solution as n increases beyond a certain threshold. We also prove a new bound on the LRC, which generally holds for hypothesis classes using strongly convex regularizers. This bound readily facilitates the bounding of the LRC for a range of such regularizers (not only for MTL, but also for the standard i.i.d. setting), as we demonstrate for classes induced by graph-based, Schatten- and group-norm regularizers. Moreover, we prove matching lower bounds showing that, aside from constants, the LRC-based bounds are tight for the considered applications.\nOur derived bounds reflect that one can trade off a slow convergence speed w.r.t. T for an improved convergence rate w.r.t. n. The latter one ranges, in the worst case, from the typical GRC-based bounds O(1/ √ n), all the way up to the fastest rate of order O(1/n) by allowing the bound to depend less on T . This trade-off is perhaps best exemplified in the case of Schatten norms, for which the two rates (exponents) always sum up to −1. Nevertheless, the premium in question becomes less relevant to MTL, in which T is typically considered as fixed.\n1.2 Organization of the paper\nThe paper is organized as follows: Sect. 2 lays the foundations for our analysis by considering a Talagrandtype concentration inequality suitable for deriving our bounds. Next, in Sect. 3, after suitably defining LRCs for MTL hypothesis spaces, we provide our LRC-based excess MTL risk bounds. Based on these bounds, we follow up this section with a local analysis of linear MTL frameworks, in which task-relatedness is presumed and enforced by imposing a strongly-convex norm constraint. In more detail, leveraging off the Fenchel-Young inequality, Sect. 4 presents a generic upper bound for the relevant LRC, which is subsequently specialized to the case of group norm, Schatten norm and graph-regularized linear MTL. After illustrating the tightness of the upper bounds, Sect. 5 supplies the corresponding excess risk bounds. The paper concludes with Sect. 6, which compares side by side the GRC- and LRC-based excess risk bounds for the aforementioned hypothesis spaces, as well as two additional related cases.\n1.3 Previous Related Works\nEarlier works that investigate MTL generalization guarantees employing Rademacher averages include [38], which considers linear MTL frameworks for binary classification. In these frameworks, all tasks are preprocessed by a common bounded linear operator and operator norm constraints are used to control the complexity of the associated hypothesis spaces. The GRC-based error bounds derived are of order O(1/ √ n) and non-vanishing w.r.t. T in the distribution-dependent case and of order O(1/ √ nT ) in the data-dependent case. Another study, [39], provides bounds for the empirical and expected Rademacher complexities of\nlinear transformation classes. Based on Hölder’s inequality, GRC-based risk bounds of order O(1/ √ n) and non-vanishing w.r.t. T are established for MTL hypothesis spaces with graph-based and LSq -Schatten norm regularizers, where q ∈ {2} ∪ [4,∞].\nThe subject of MTL generalization guarantees experienced renewed attention in more years. In [21], the authors take advantage of the strongly-convex nature of certain matrix-norm regularizers to easily obtain generalization bounds for a variety of machine learning problems. Part of their work is devoted to the realm of online and off-line MTL. In the latter case, which pertains to the focus of our work, the paper provides a distribution-dependent GRC-based excess risk bound of order O(1/ √ nT ). Moreover, [41] presents a global Rademacher complexity analysis leading to both data and distribution-dependent excess risk bounds of order O( √\nlog(n)/n) and non-vanishing w.r.t. T for a trace norm regularized MTL model. Also, [40] examines the bounding of (global) Gaussian complexities of function classes that result from considering composite maps, as it is typical in MTL among other settings. An application of the paper’s results yields data-dependent MTL risk bounds of order O(1/ √ n) and non-vanishing w.r.t. T . More recently, [42] presents excess risk bounds for both MTL and Learning-To-Learn (LTL) settings and reveals conditions, under which MTL is more beneficial over learning tasks independently. The accompanying bounds are of order O(1/ √ nT ) and, compared to the results in [39, 41], it enjoys the advantage of vanishing as T → +∞. Finally, due to being domains related to MTL, but, at the same time, less connected to the focus of this paper, we only mention in passing a few works that pertain to generalization guarantees in the realm of life-long learning and domain adaptation. Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45]. Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].\n1.4 Basic Assumptions & Notation\nConsider T supervised learning tasks sampled from the same input-output space X × Y. Each task t is presented by an independent random variable (Xt, Yt) governed by a probability distribution µt. Also, the i.i.d. samples related to each task t are described by the sequence (X it , Y i t ) n i=1, which is distributed according to µt. In what follows, we use the following notational conventions: vectors and matrices are depicted in bold face. The superscript T , when applied to a vector/matrix, denotes the transpose of that quantity. We define NS := {1, . . . , S}. For any random variablesX,Y and functions f we use Ef(X,Y ) and EXf(X,Y ) to denote the expectation w.r.t. all the involved random variables and the conditional expectation w.r.t. the random variable X . For any vector-valued function f = (f1, . . . , fT ), we introduce the following two notations:\nPf := 1\nT\nT∑\nt=1\nPft = 1\nT\nT∑\nt=1\nE(f(Xt)), Pnf := 1\nT\nT∑\nt=1\nPnft = 1\nT\nT∑\nt=1\n1\nn\nn∑\ni=1\nf(X it).\nFor any loss function ℓ and any f = (f1, . . . , fT ) we define ℓft the function defined by ℓft((Xt, Yt) T t=1) = ℓ(ft(Xt), Yt).\n2 Talagrand-Type Inequality for Multi-Task Learning\nThe derivation of our LRC-based error bounds for MTL is founded on the following modified Talagrand’s concentration inequality [13, 50] adapted to the context of MTL, showing that the uniform deviation between the true and empirical means in a vector-valued function class F can be dominated by the associated multitask Rademacher complexity plus a term involving the variance of functions in F . We defer the proof in Appendix.\nTheorem 1 (Talagrand-Type Inequality for MTL). Let F = {f := (f1, . . . , fT )} be a class of vectorvalued functions satisfying supt,x |ft(x)| ≤ b. Let X := (X it) (T,Nt) (t,i)=(1,1) be a vector of ∑T t=1 Nt independent random variables where X1t , . . . , X n t , ∀t are identically distributed. Let {σit}t,i be a sequence of independent Rademacher variables. If 1T ∑T t=1 supf∈F E [ ft(X 1 t ) ]2 ≤ r, then, for every x > 0, with probability at least\n1− e−x,\nsup f∈F\n(Pf − Pnf) ≤ 4R(F) + √ 2xr\nnT +\n8bx 3nT , (1)\nwhere n := mint∈NT Nt, and the multi-task Rademacher complexity of function class F is defined as\nR(F) := EX,σ {\nsup f=(f1,...,fT )∈F\n1\nT\nT∑\nt=1\n1\nNt\nNt∑\ni=1\nσitft(X i t)\n}\n.\nNote that the same bound also holds for supf∈F(Pnf − Pf). In Theorem 1, the data from different tasks assumed to be mutually independent, which is typical in the MTL setting [38]. To present the results in a clear way we always assume in the following that the available data for each task is the same, namely n.\n3 Excess MTL Risk Bounds based on Local Rademacher Com-\nplexities\nTheorem 1 motivates us to extend the classical LRC R(F , r) for a scalar-valued function class F : R(F , r) := EX,σ [ supf∈F ,V (f)≤r 1 n ∑n i=1 σif(Xi) ] to the Multi-Task Local Rademacher Complexity (MT-LRC) R(F , r) and its empirical counterpart R̂(F , r) for a vector-valued function class F as follows:\nR(F , r) :=E [\nsup f=(f1,...,fT )∈F\nV (f)≤r\n1\nnT\n∑\nt∈NT i∈Nn\nσitft(X i t ) ] , R̂(F , r) := Eσ [\nsup f=(f1,...,fT )∈F\nVn(f)≤r\n1\nnT\n∑\nt∈NT i∈Nn\nσitft(X i t) ] , (2)\nwhere V (f ) and Vn(f ) are upper bounds on the variance and conditional variances of the functions in F , respectively. This paper takes the choice V (f ) = Pf2 and Vn(f) = Pnf\n2. Analogous to single task learning, the challenge in applying MT-LRC (2) to refine the existing learning rates is to find an optimal radius trading-off the size of the set {f ∈ F : V (f) ≤ r} and its complexity, which, as we show below, reduces to the calculation of the fixed-point of a sub-root function. We call a function ψ sub-root if it is non-decreasing, non-negative and r 7→ ψ(r)/√r is non-increasing for r ≥ 0. We call the unique solution of the equation ψ(r) = r the fixed point of ψ. We suppose that the loss function ℓ and the hypothesis space F satisfy the following conditions: Assumptions 1.\n1. There is a function f∗ = (f∗1 , . . . , f ∗ T ) ∈ F satisfying Pℓf∗ = inff∈F Pℓf .\n2. There is constant B′ ≥ 1, such that for every ft ∈ F we have P (ft − f∗t )2 ≤ B′P (ℓft − ℓf∗t ).\n3. There is a constant L, such that the loss function ℓ is L-Lipschitz in its first argument.\nWe now present the main result of this section showing that the excess error of MTL can be bounded by the fixed-point of a sub-root function dominating the MT-LRC.\nTheorem 2 (Distribution-dependent excess risk bound for MTL). Let F = {f := (f1, . . . , fT )} be a class of vector-valued functions satisfying supt,x |ft(x)| ≤ b. Let X := (X it , Y it ) (T,n) (t,i)=(1,1) be a vector of nT independent random variables where (X1t , Y 1 t ) . . . , (X n t , Y n t ), ∀t are identically distributed. Suppose that Assumptions 1 holds. Let ψ be a sub-root function with the fixed point r∗ such that BLR(F∗, r) ≤ ψ(r), ∀r ≥ r∗, where R(F∗, r) is the LRC of the functions class F∗:\nR(F∗, r) := EX,σ [\nsup f∈F ,L2P (f−f∗)2≤r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t) ] . (3)\nThen, we have the following bounds in terms of the fixed point r∗ of ψ(r):\n1. For any function class F , K > 1 and x > 0, with probability at least 1− e−x,\nP (ℓf − ℓf∗) ≤ K\nK − 1Pn(ℓf − ℓf ∗) +\n500K\nB r∗ +\n(6Lb+ 10BK)x\nnT .\n2. For any convex function class F , K > 1 and x > 0, with probability at least 1− e−x,\nP (ℓf − ℓf∗) ≤ K\nK − 1Pn(ℓf − ℓf ∗) +\n32K\nB r∗ +\n(3Lb+ 4BK)x\nnT .\nProof. Introduce the following class of excess loss functions:\nH∗F := { hf : (Xt, Yt) T t=1 7→ (ℓ(ft(Xt), Yt)− ℓ(f∗t (Xt), Yt)) T t=1 ,f = (f1, . . . , fT ) ∈ F } . (4)\nFrom Assumptions 1, it can be seen that for any function h ∈ H∗F , P (ℓf − ℓf∗)2 ≤ L2P (f − f∗)2 ≤ BP (ℓf − ℓf∗), where B = B′L2. This implies\nV (hf ) = Ph 2 f ≤ L2P (f − f∗)2 ≤ BP (f − f∗).\nAlso, using Talagrand’s Lemma [29], one can verify\nBR(H∗F , r) = BEX,σ\n\n  sup\nf∈F , V (hf )≤r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitht(X i t , Y i t )\n\n \n= BEX,σ\n\n  sup\nf∈F , V (hf )≤r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitℓft(X i t , Y i t )\n\n \n≤ BLR(F∗, r) ≤ ψ(r).\nApplying Theorem A.2 (which is the extension of Theorem 3.3 of [7] to MTL function classes) to the function class H∗F completes the proof.\nThe following excess-risk bound is immediate by noting that Pn(ℓf̂ − ℓf∗) ≤ 0.\nCorollary 3. Let f̂ be any element of F satisfying Pnℓf̂ = inff∈F Pnℓf . Assume that the conditions of Theorem 2 hold. Then for any x > 0 and r > ψ(r), with probability at least 1− e−x,\nP (ℓ f̂ − ℓf∗) ≤\n500K\nB r∗ +\n(6Lb+ 10BK)x\nnT . (5)\nThe next theorem, analogous to Theorem 5.4 in [7], presents a data-dependent version of (5) replacing the Rademacher complexity in Corollary 3 with its empirical counterpart.\nTheorem 4 (Data-dependent excess risk bound for MTL). Let f̂ be any element of F satisfying Pnℓf̂ = inff∈F Pnℓf . Assume that the condition of Theorem 2 hold. Define\nψ̂n(r) = c1R̂(F∗, c3r) + c2x nT , R̂(F∗, c3r) := Eσ\n[\nsup f∈F ,\nL2Pn(f−f̂)2≤c3r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t) ] ,\nwhere c1 = 2Lmax(B, 16Lb), c2 = 8L 2b2+c1 and c3 = 4+128K+4B(3Lb+4BK)/c2. Then for any K > 1 and x > 0, with probability at least 1− 4e−x, we have\nP (ℓ f̂ − ℓf∗) ≤\n500K\nB r̂∗ +\n(6Lb+ 10BK)x\nnT ,\nwhere r̂∗ is the fixed point of the sub-root function ψ̂n(r).\nProof. The proof of this Theorem repeats the same basic steps utilized in Theorem 5.4 in [7] and, therefore, can be found in the Appendix.\n4 Local Rademacher Complexity Bounds for MTL models with\nStrongly Convex Regularizers\nThis section presents very general MT-LRC bounds, based on the distribution-dependent excess risks established in Theorem 2, for hypothesis spaces defined by strongly convex regularizers, which allows us to immediately derive, as specific application cases, LRC bounds for group-norm, Schatten-norm, and graphregularized MTL models. It should be mentioned that similar data-dependent MT-LRC bounds are also available by a similar deduction process.\n4.1 Preliminaries\nWe consider linear MTL models where we associate to each task-wise function ft a weight wt ∈ H by ft(X) = 〈wt, φ(X)〉. Here φ is a feature map associated to a Mercer kernel k satisfying k(X, X̃) = 〈φ(X), φ(X̃)〉, ∀X, X̃ ∈ X and wt belongs to the reproducing kernel Hilbert space HK induced by k with inner product 〈·, ·〉. We assume that the multi-task model W = (w1, . . . ,wT ) ∈ H× . . .×H is learned by a regularization scheme:\nmin W R (W ) + C\nT∑\nt=1\nn∑\ni=1\nℓ( 〈 wt, φ(X i t ) 〉 H , Y i t ), (6)\nwhere the regularizer R(·) is used to enforce a priori information to avoid over-fitting. This regularization scheme amounts to performing Empirical Risk Minimization (ERM) in the hypothesis space\nF := { X 7→ [〈w1, φ(X1)〉 , . . . , 〈wT , φ(XT )〉]T : R (W ) ≤ R } , (7)\nwhere for generality we consider regularizers of the form R(W ) = ∥ ∥D1/2W ∥ ∥ with a positive operator D defined in H. The hypothesis space associated to group and Schatten norms can be recovered by taking D = I and appropriate norms. Furthermore, the graph-regularized MTL can be specialized by taking D = L+ ηI with L being a graph-Laplacian and η being a regularization constant. Our general discussion shows that all these MTL models can be covered in a framework with the notion of strong convexity.\n4.2 General Bound on the LRC\nNow, we can provide the main result of this section which gives a LRC bound for any general MTL hypothesis space of the form (7).\nTheorem 5 (Distribution-dependent MT-LRC bounds by strong convexity). Suppose that R(W ) in (6) is µ-strongly convex with R∗(0) = 0 and ‖k‖∞ ≤ β ≤ ∞. Let X1t , . . . , Xnt be an i.i.d. sample drawn from Pt. Also, assume that for each task t, the eigenvalue-eigenvector decomposition of the Hilbert-Schmidt covariance operator is given by Jt = E(φ(Xt) ⊗ φ(Xt)) = ∑∞ j=1 λ j tu j t ⊗ ujt , where (ujt )∞j=1 forms an orthonormal basis of H, and (λjt )∞j=1 are the corresponding eigenvalues, for the task t, arranged in non-increasing order. Then for every positive operator D on RT , any r > 0 and any non-negative integers h1, . . . , hT :\nR(F , r) ≤ min 0≤ht≤∞\n \n\n√\nr ∑T\nt=1 ht nT + 1 T\n√\n2R\nµ EX,σ\n∥ ∥ ∥D −1/2V ∥ ∥ ∥ 2\n∗\n   , (8)\nwhere V = ( ∑\nj>ht 〈 1 n ∑n i=1 σ i tφ(X i t),u j t 〉 u j t )T\nt=1 .\nProof. Note that with the help of LRC definition, we have for any function class F ,\nR(F , r) = 1 nT EX,σ\n \n sup f=(f1,...,fT )∈F ,\nPf2≤r\nn∑\ni=1\n〈\n(wt) T t=1 , ( σitφ(X i t ) )T\nt=1\n〉\n \n\n= 1\nT EX,σ\n \n sup f∈F , Pf2≤r\n〈\n(wt) T t=1 ,\n\n\n∞∑\nj=1\n〈\n1\nn\nn∑\ni=1\nσitφ(X i t),u j t\n〉\nu j t\n\n\nT\nt=1\n〉 \n\n\n≤ 1\nT EX,σ\n \n sup Pf2≤r\n〈\n\nht∑\nj=1\n√\nλjt\n〈\nwt,u j t\n〉\nu j t\n\n\nT\nt=1\n,\n\n\nht∑\nj=1\n√\nλjt\n−1 〈\n1\nn\nn∑\ni=1\nσitφ(X i t ),u j t\n〉\nu j t\n\n\nT\nt=1\n〉 \n\n\n︸ ︷︷ ︸\nA1 (9)\n+ 1\nT EX,σ\n \n sup f∈F\n〈\n(wt) T t=1 ,\n  ∑\nj>ht\n〈\n1\nn\nn∑\ni=1\nσitφ(X i t),u j t\n〉\nu j t\n\n\nT\nt=1\n〉 \n\n\n︸ ︷︷ ︸\nA2 . (10)\nStep 1. Controlling A1: Applying Cauchy-Schwartz (C.S.) and Hölder inequalities on A1 yields the following\nA1 ≤ 1\nT EX,σ\n \n\nsup Pf2≤r\n\n  \n\n \nT∑\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ht∑\nj=1\n√\nλjt\n〈\nwt,u j t\n〉\nu j t ∥ ∥ ∥ ∥ ∥ ∥\n2 \n \n1 2\n\n \nT∑\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ht∑\nj=1\n√\nλjt\n−1 〈\n1\nn\nn∑\ni=1\nσitφ(X i t),u j t\n〉\nu j t ∥ ∥ ∥ ∥ ∥ ∥\n2 \n \n1 2\n\n  \n \n\n= 1\nT EX,σ\n \n sup Pf2≤r\n\n \n\n\nT∑\nt=1\nht∑\nj=1\nλjt\n〈\nwt,u j t\n〉2\n\n\n1 2\n\n\nT∑\nt=1\nht∑\nj=1\nλjt −1\n〈\n1\nn\nn∑\ni=1\nσitφ(X i t),u j t\n〉2 \n\n1 2\n\n \n \n\n.\nWith the help of Jensen’s inequality and regarding the fact that EX,σ 〈 1 n ∑n i=1 σ i tφ(X i t ),u j t 〉2 = λjt n and Pf2 ≤ r implies 1T ∑T t=1 ∑∞ j=1 λ j t 〈 wt,u j t 〉2\n≤ r (see Lemma 2 in the Appendix for the proof), we can further bound A1 as\nA1 ≤\n√\nr ∑T\nt=1 ht nT . (11)\nStep 2. Controlling A2: We use strong convexity assumption on the regularizer in order to further bound the second term A2.\nLet λ > 0. Applying (A.27) with w = D1/2W and v = λD−1/2V gives\n〈 D1/2W , λD−1/2V 〉 ≤ R(W ) + 〈 ▽R∗ (0) , λD−1/2V 〉 + λ2\n2µ\n∥ ∥ ∥D −1/2V ∥ ∥ ∥ 2\n∗ .\nTaking expectation on both sides and optimizing λ gives\nA2 = 1\nT EX,σ\n{\nsup f∈F\n〈 D1/2W ,D−1/2V 〉 }\n≤ min 0<λ<∞\n{ R\nλT +\nλ\n2µT EX,σ\n∥ ∥ ∥D −1/2V ∥ ∥ ∥ 2\n∗\n}\n= 1\nT\n√\n2R\nµ EX,σ\n∥ ∥ ∥D −1/2V ∥ ∥ ∥ 2\n∗ . (12)\nCombining (12) with (11) completes the proof.\nIn the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].\n4.3 Group Norm Regularized MTL\nWe first consider the group norm regularized MTL capturing the inter-task relationships by the group norm ‖W ‖2,q := (∑T t=1 ‖wt‖ q 2 )1/q [20, 4, 32, 49], for which the associated hypothesis space takes the form\nFq := { X 7→ [〈w1, φ(X1)〉 , . . . , 〈wT , φ(XT )〉]T : ‖W ‖2,q ≤ Rmax } . (13)\nCorollary 6. If 1 ≤ q ≤ 2 in (13), the LRC of function class Fq can be bounded as\nR(Fq, r) ≤ √ √ √ √ √ √ 4\nnT ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∞∑ j=1 min ( rT 1− 2 q∗ , 2eq∗3Rmax T λjt )   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n+\n√ 2βeRmaxq ∗ 32T 1 q∗\nnT . (14)\nProof. The proof follows by applying Khintchine (A.28) and Rosenthal (A.29) inequalities to further bound the expectation term in (8) which gives,\nA2(Fq) ≤ √ √ √ √ √ √ 2eq∗2Rmax nT 2µ ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∑ j>ht λjt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n+\n√ 2βeRmaxq ∗T 1 q∗\nnT √ µ\n. (15)\nNow, combining (11) and (15) provides the bound on R(Fq, r) as\nR(Fq, r) ≤\n√\nr ∑T\nt=1 ht nT + √ √ √ √ √ √ 2eq∗2Rmax nT 2µ ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∑ j>ht λjt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n+\n√ 2βeRmaxq ∗T 1 q∗\nnT √ µ\n, (16)\nThen using the following inequalities according which for any non-negative numbers α1, α2 ∈ R+ and any non-negative vectors a1,a2 ∈ RT with 0 ≤ q ≤ p ≤ ∞ any s ≥ 1,\n(⋆) √ α1 + √ α2 ≤ √ 2(α1 + α2) (17)\n(⋆⋆) lp − to− lq : ‖a1‖q = 〈1,a1〉 1 q Hölder ≤\n(\n‖1‖(p/q)∗ ‖a q 1‖(p/q)\n) 1 q\n= T 1 q− 1p ‖a1‖p (18)\n(⋆ ⋆ ⋆) ‖a1‖s + ‖a2‖s ≤ 21− 1 s ‖a1 + a2‖s ≤ 2 ‖a1 + a2‖, (19)\nwe can obtain the desired result. See the Appendix for the detailed proof.\nRemark 7. Since the LRC bound above is not monotonic in q it is more reasonable to state the above bound in terms of q ≤ κ, as taking κ = q is not always the optimal choice. Trivially for the group norm regularizer with any κ ≥ q, it holds that ‖W ‖2,κ ≤ ‖W‖2,q and therefore R(Fq, r) ≤ R(Fκ, r). Thus we have the following bound on R(Fq, r) for any κ ∈ [q, 2],\nR(Fq, r) ≤ √ √ √ √ √ √ 4\nnT ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∞∑ j=1 min ( rT 1− 2 κ∗ , 2eκ∗3Rmax T λjt )   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nκ∗\n2\n+\n√ 2βeRmaxκ ∗ 32T 1 κ∗\nnT .\nRemark 8 (Sparsity-inducing group-norm). Among p ≥ 1, a particular group-norm of independent interest is the sparsity-inducing group-norm achieved by q = 1 [6, 4], for which we can take κ∗ = logT to get that\nR(F1, r) ≤\n√ √ √ √ 4\nnT\n∥ ∥ ∥\n( ∞∑\nj=1\nmin ( rT 1− 2 κ∗ , 2eκ∗3Rmax T λjt ))T t=1 ∥ ∥ ∥ κ∗\n2\n+\n√ 2βeRmaxκ ∗ 32T 1 κ∗\nnT\n(lκ∗ 2\n−to−l∞)\n≤\n√ √ √ √ 4\nnT\n∥ ∥ ∥\n( ∞∑\nj=1\nmin ( rT, 2e3(logT )3\nT Rmaxλ\nj t\n))T\nt=1 ∥ ∥ ∥ ∞ +\n√ 2βRmaxe 3 2 (logT ) 3 2\nnT .\nTo investigate the tightness of the bound in (14), we derive the lower bound which holds for the LRC of Fq with any q ≥ 1.\nTheorem 9 (Lower bound). The following lower bound holds for the local Rademacher complexity of Fq in (14) with any q ≥ 1. There is an absolute constant c so that ∀t, if λ1t ≥ 1/(nR2max) then for all r ≥ 1n and q ≥ 1,\nR(Fq,R,T , r) ≥\n√ √ √ √\nc\nnT 1− 2 q∗\n∞∑\nj=1\nmin\n(\nrT 1− 2 q∗ , R2max T λj1\n)\n. (20)\nProof. The proof can be found in the Appendix.\nA comparison between the lower bound in (20) and the upper bound in (14) can be clearly illustrated by assuming identical eigenvalue tail sums ∑\nj≥∞ λ j t for all tasks, for which the upper bound translates to\nR(Fq,R,T , r) ≤\n√ √ √ √\n4\nnT 1− 2 q∗\n∞∑\nj=1\nmin\n(\nrT 1− 2 q∗ , 2eq∗3Rmax T λjt\n)\n+\n√ 2βeRmaxq ∗ 32T 1 q∗\nnT ,\nmatching the lower bound in (20) up to constants of factors. A similar comparison analysis can be performed for MTL models with Schatten norm and graph regularizers.\n4.4 Schatten Norm Regularized MTL\n[6] developed a spectral regularization framework for MTL where the Schatten p-norm ‖W‖Sq := [ tr ( W TW ) q 2 ] 1 q is studied as a concrete example, corresponding to perform ERM in the following hypothesis space:\nFSq := { X 7→ [〈w1, φ(X1)〉 , . . . , 〈wT , φ(XT )〉]T : ‖W‖Sq ≤ R′max } . (21)\nCorollary 10. For any 1 ≤ q ≤ 2 in (21), the LRC of function class FSq is bounded as\nR(FSq , r) ≤\n√ √ √ √ 4\nnT\n∥ ∥ ∥\n( ∞∑\nj=1\nmin ( r, 2q∗(q∗ − 1)R′max\nT λjt\n))T\nt=1 ∥ ∥ ∥ 1 .\nProof. The proof is provided in the Appendix.\nRemark 11 (Sparsity-inducing Schatten-norm (trace norm)). Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22]. Note that for any q ≥ 1, it holds that R(FS1 , r) ≤ R(FSq , r). Therefore, choosing the optimal q∗ = 2 yields that\nR(FS1 , r) ≤\n√ √ √ √ 4\nnT\n∥ ∥ ∥\n( ∞∑\nj=1\nmin ( r, 4R′max\nT λjt\n))T\nt=1 ∥ ∥ ∥ 1 .\n4.5 Graph Regularized MTL\nThe idea underlying graph regularized MTL is to force the classifiers of related tasks close to each other by penalizing the squared distance ‖wt −ws‖2 with different weights ωts. We consider the following graph regularized MTL [39]\nR(W ) = 1\n2T\nT∑\nt=1\nT∑\ns=1\nωts‖wt −ws‖2 + η\nT\nT∑\nt=1\n‖wt‖2 = 1\nT\nT∑\nt=1\nT∑\ns=1\n(L+ ηI)ts 〈wt,ws〉 ,\nwhere L is the graph-Laplacian associated to a matrix of edge-weights (ωts)ts, I is the identity in R T , and η > 0 is a regularization parameter. According to the identity 1T ∑T t=1 ∑T s=1 ( L + ηI ) ts 〈 wt,ws 〉 = (1/T )‖(L+ ηI)1/2W ‖2F , the corresponding hypothesis space is:\nFG := { X 7→ [ 〈 w1, φ(X1) 〉 , . . . , 〈 wT , φ(XT ) 〉 ]T : ‖D1/2W ‖F ≤ R′′max } . (22)\nCorollary 12. For any positive definite matrix D in (22), the LRC of FG is bounded by\nR(FG, r) ≤\n√ √ √ √ 4\nnT\n∥ ∥ ∥\n( ∞∑\nj=1\nmin ( r, 2D−1tt R ′′ max\nT λjt )\n))T\nt=1 ∥ ∥ ∥ 1 . (23)\nProof. See the Appendix for the proof.\n5 Excess Risk Bounds for MTL models with Strongly Convex\nRegularizers\nIn this section we will provide the distribution and data-dependent excess risk bounds for the hypothesis spaces considered earlier. Note that, due to the space limitation, the proofs of the results are provided only for the hypothesis space Fq with q ∈ [1, 2] in (13). However, for the group and LSq -Schatten norm (q ∈ [1, 2]) regularized MTL, the proofs can be obtained in a very similar way.\nTheorem 13 (Distribution-dependent excess risk bound for a MTL problem with a strong convex L2,q group-norm regularizer). Assume the convex class Fq in (13) has ranges in [−b, b], and let the loss function ℓ in Problem (6) be such that Assumptions 1 are satisfied. Let f̂ be any element of Fq with 1 ≤ q ≤ 2 which satisfies Pnℓf̂ = inff∈Fq Pnℓf . Assume moreover that k is a positive semi-definite kernel on X such that ‖k‖∞ ≤ β ≤ ∞. Denote by r∗ the fixed point of 2BLR(Fq, r4L2 ). Then, for any K > 1 and x > 0, with probability at least 1− e−x, the excess loss of function class Fq is bounded as\nP (ℓ f̂ − ℓf∗) ≤\n32K\nB r∗ +\n(3Lb+ 4BK)x\nnT , (24)\nwhere for the fixed point r∗ of the local Rademacher complexity 2BLR(Fq, r4L2 ), it holds\nr∗ ≤ min 0≤ht≤∞\nB2 ∑T\nt=1 ht Tn + 4BL √ √ √ √ √ √ 2eq∗3Rmax nT 2 ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∑ j>ht λjt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n+ 4 √ 2βeRmaxq ∗ 32 T 1 q∗\nnT , (25)\nwhere h1, . . . , hT are arbitrary non-negative integers.\nProof. First notice that Fq is convex, thus it is star-shaped around any of its points. Hence according to Lemma 3.4 in [7], Fq is a sub-root function. Moreover, because of the symmetry of σit and because Fq is convex and symmetric, it can be shown that R(F∗q , r) ≤ 2R(Fq, r4L2 ), where R(F∗q , r) is defined according to (3) for the class of functions Fq. Therefore, it suffices to find the fixed point of 2BLR(Fq, r4L2 ) by solving φ(r) = r. For this purpose, we will use (16) as a bound for R(Fq, r), and we solve √ αr + γ = r which is equivalent to solving r2 − (α+ 2γ)r + γ2 = 0, where we define\nα = B2\n∑T t=1 ht\nTn , and γ = 2BL\n√ √ √ √ √ √ 2eq∗3Rmax nT 2 ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∑ j>ht λjt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n+ 2 √ 2βeRmaxBLq ∗ 32T 1 q∗\nnT . (26)\nIt is not hard to verify that r∗ ≤ α+ 2γ. Substituting the definition of α and γ gives the result.\nRegarding the fact that λjt are decreasing with respect to j, we can assume ∃dt : λjt ≤ dtj−αt for some αt > 1. As examples, this assumption holds for finite rank kernels as well as convolution kernels. Thus, it can be shown\n∑\nj>ht\nλjt ≤ dt ∑\nj>ht\nj−αt ≤ dt ∫ ∞\nht\nx−αtdx = dt\n[ 1\n1− αt x1−αt\n]∞\nht\n= − dt 1− αt h1−αtt . (27)\nnote that by lp − to− lq conversion, we have\nB2 ∑T\nt=1 ht Tn ≤ B\n√\nB2T ∑T\nt=1 h 2 t\nn2T 2 ≤ B\n√ √ √ √ B2T 2− 2 q∗ ∥ ∥ ∥(h2t ) T t=1 ∥ ∥ ∥ q∗\n2\nn2T 2 .\nNow, applying, (17) and (19), and inserting (27) into (25), it holds for a group norm regularized MTL with 1 ≤ q ≤ 2,\nr∗ ≤ min 0≤ht≤∞ 2B\n√ √ √ √ √ ∥ ∥ ∥ ∥ ∥ ∥ ( B2T 2− 2 q∗ h2t n2T 2 − 32dteq ∗3RmaxL2 nT 2(1− αt) h1−αtt )T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n+ 4 √ 2βeRmaxBLq ∗ 32T 1 q∗\nnT . (28)\nTaking the derivative of the above bound with respect to ht and setting it to zero yields the optimal ht as\nht = ( 16dteq ∗3RmaxB −2L2T 2 q∗ −2n ) 1 1+αt .\nNote that substituting the above for α := mint∈NT αt and d = maxt∈NT dt into (28), we can upper-bound the fixed point of r∗ as\nr∗ ≤ 8B 2\nn\n√\ne α+ 1 α− 1 ( dq∗3RmaxB −2L2T 2 q∗ −2n ) 1 1+α\n+ 4 √ 2βeRmaxBLq ∗ 32T 1 q∗\nnT .\nAlso, the convergence rate of r∗ can be determined as\nr∗ = O\n\n\n(\nT 2− 2 q∗\nq∗3\n) −1 1+α\nn −α 1+α\n\n .\nIt can be seen that the convergence rate can be as slow as O ( q∗3/2T 1/q ∗\nT √ n\n)\n(for small α where at least one\nαt ≈ 1), and as fast as O(n−1) (for large α where for all t, αt 7→ ∞). Therefore, one can observe that the bound obtained for the fixed point together with Theorem 13 provides a bound for the excess risk which leads to the following theorem.\nRemark 14 (Excess risk bounds for some strong convex matrix norm regularized MTL problems). Assume the convex class Fq in (13) has ranges in [−b, b], and let the loss function ℓ in Problem (6) be such that Assumptions 1 are satisfied. Assume moreover that k is a positive semidefinite kernel on X such that ‖k‖∞ ≤ β ≤ ∞. Also, denote α := mint∈NT αt and d = maxt∈NT dt. Also,\n• Group norm (1 ≤ q ≤ 2): If f̂ satisfies Pnℓf̂ = inff∈Fq Pnℓf , and r∗ is the fixed point of the local Rademacher complexity 2BLR(Fq, r4L2 ) with any 1 ≤ q ≤ 2 in (13) and any K > 1, it holds with probability at least 1− e−x,\nP (ℓ f̂ − ℓf∗) ≤ min\nκ∈[q,2] 423K\n√\nα+ 1 α− 1 ( dκ∗3RmaxL 2 ) 1 1+α B α−1 α+1 ( T 2 κ∗ −2 ) 1 1+α n −α 1+α\n+ 299\n√ βRmaxKLκ ∗ 32T 1 κ∗\nnT +\n(3Lb+ 4BK)x\nnT . (29)\n• Schatten-norm (1 ≤ q ≤ 2): If f̂ satisfies Pnℓf̂ = inff∈FSq Pnℓf , and r∗ is the fixed point of the local Rademacher complexity 2BLR(FSq , r4L2 ) with any 1 ≤ q ≤ 2 in (21) and any K > 1, it holds with probability at least 1− e−x,\nP (ℓ f̂ − ℓf∗) ≤ 256K\n√\nα+ 1 α− 1 ( dq∗(q∗ − 1)R′maxL2 ) 1 1+α B α−1 α+1 T −1 1+αn −α 1+α\n+ (3Lb+ 4BK)x\nnT . (30)\n• Graph regularizer: If f̂ satisfies Pnℓf̂ = inff∈Fq Pnℓf , and r∗ is the fixed point of the local Rademacher complexity 2BLR(FG, r4L2 ) with any positive operator D in (22) and any K > 1, it holds with probability at least 1− e−x,\nP (ℓ f̂ − ℓf∗) ≤ 256K\n√\nα+ 1 α− 1 ( dR′′maxL 2D−1max ) 1 1+α B α−1 α+1 T −1 1+αn −α 1+α\n+ (3Lb+ 4BK)x\nnT . (31)\nwhere D−1max := maxt∈NT D −1 tt .\nCorollary 15 (Data-dependent excess risk bound for a MTL problem with a strong convex L2,q group-norm regularizer). Assume the convex class Fq in (13) has ranges in [−b, b], and let the loss function ℓ in Problem (6) be such that Assumptions 1 are satisfied. Let f̂ be any element of Fq with 1 ≤ q ≤ 2 which satisfies Pnℓf̂ = inff∈Fq Pnℓf . Assume moreover that k is a positive semidefinite kernel on X such that ‖k‖∞ ≤ β ≤ ∞. Let Kt be the n×n kernel Gram matrix of task t with entries (Kt)ij := k(X it , Xjt ); denote λ̂1t , . . . λ̂nt its ordered eigenvalues. Let r̂∗ be the fixed point of\nψ̂n(r) = c1R̂(F∗q , c3r) + c2x\nnT ,\nwhere c1 = 2Lmax (B, 16Lb), c2 = 8L 2b2 + c1 and c3 = 4 + 128K + 4B(3Lb+ 4BK)/c2, and\nR̂(F∗q , c3r) := Eσ\n\n  \nsup f∈Fq,\nL2Pn(f−f̂) 2≤c3r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t) ∣ ∣ ∣ ∣ ∣ { xit } t∈NT ,i∈Nn\n\n   . (32)\nThen, for any K > 1 and x > 0, with probability at least 1 − 4e−x the excess loss of function class Fq is bounded as\nP (ℓ f̂ − ℓf∗) ≤\n32K\nB r̂∗ +\n(3Lb+ 4BK)x\nnT , (33)\nwhere for the fixed point r̂∗ of the empirical local Rademacher complexity ψ̂n(r), it holds\nr̂∗ ≤ c 2 1c3\n∑T t=1 ht\nnTL2 + 4\n√ √ √ √ √ √ 2c21q ∗2Rmax nT 2 ∥ ∥ ∥ ∥ ∥ ∥ ∥   n∑ j>ht λ̂jt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n+ 2c2x\nnT ,\nwhere h1, . . . , hT are arbitrary non-negative integers, and ( ˆ λjt ) n j=1 are eigenvalues of the empirical Gram matrix K obtained from kernel function k.\nProof. The proof of the result is provided in the Appendix.\n6 Discussion\n6.1 Global vs. Local Rademacher Complexity Bounds\nThis section is devoted to compare the excess risk bounds based on local Rademacher complexity to those of the global ones.\nFirst, note that to obtain the GRC-based bounds, we apply Theorem 16 of [38], as we consider the same setting and assumptions for tasks’ distributions as considered in this work. This theorem presents an MTL bound based on the notion of GRC.\nTheorem 16 (MTL excess risk bound based on GRC; Theorem 16 of [38] ). Let F be a class of vector-valued functions f = (f1, . . . , fT ) : X 7→ RT , that maps X into [−b, b]T , and let X = (Xti ) (n,T ) (i,t)=(1,1) be a vector of independent random variables where for all fixed t, Xt1, . . . , X t n are identically distributed according to Pt. Then for every x > 0, with probability at least 1− e−x,\nsup f∈F\n(Pf − Pnf) ≤ 2R(F) + √ bx\nnT . (34)\nProof. As it has been shown in [38], the proof of this theorem is based on using McDiarmid’s inequality for Z defined in Theorem 1, and noticing that for the function class F with values in [−b, b], it holds that |Z − Zs,j | ≤ 2b/nT .\nIt can be observed that, in order to obtain the excess risk bound in the above theorem, one has to bound the GRC term R(F) in (34). Therefore, we first upper-bound the GRC of different hypothesis spaces considered in the previous sections.\nTheorem 17 (Distribution-dependent GRC bounds). Assume that the conditions of Theorem 5 hold. Then, the following results hold for the GRC of the hypothesis spaces in (13), (21) and (22), respectively.\n• Group-norm regularizer: For any q ≥ 1 in (13), the GRC of the function class Fq can be bounded as\n∀κ > q : R(Fq) ≤ √\n2eκ∗3Rmax nT 2 ∥ ∥ ∥(tr (Jt)) T t=1 ∥ ∥ ∥ κ∗\n2\n+\n√ 2βeRmaxκ ∗ 32 T 1 κ∗\nnT .\n• Schatten-norm regularizer: For any q ≥ 1 in (21), the GRC of the function class FSq can be bounded as\nR(FSq ) ≤ √ 2R′maxq ∗(q∗ − 1)\nnT 2\n∥ ∥ ∥(tr (Jt)) T t=1 ∥ ∥ ∥ 1 . (35)\n• Graph regularizer: For any positive operator D in (22), the GRC of the function class FG can be bounded as\nR(FG) ≤ √\n2R′′max nT 2 ∥ ∥ ∥ ( D−1tt tr(Jt) )T t=1 ∥ ∥ ∥ 1 . (36)\nProof. The proof of the results can be found in the Appendix.\nNotice that, assuming a unique bound for the traces of all task’s kernels, the bound above is determined\nby O\n(\nq∗ 3 2 T 1 q∗\nT √ n\n)\n. Also, taking q∗ = logT , we obtain the bound R(F1) of order (log T ) 3 2\nT √ n . We can also remark\nthat when the traces of the kernels are bounded, the bounds for the Schatten norm and graph regularizers are the order of O (\n1√ nT\n)\n.\nNote that for the purpose of comparison, we concentrate only on the parameters R, n, T, q∗ and α and assume all the other parameters are fixed and hidden in the big-O notation. Also, for the sake of simplicity, we assume that the eigenvalues of all tasks satisfy λjt ≤ dj−α (with α ≥ 1). Note that from Theorem 16, it follows that a bound on the global Rademacher complexity provides also a bound on the excess risk. This together with Theorem 17, gives the GRC-based excess risk bounds of the following forms\nGroup norm: ∀κ ∈ [q, 2], P (ℓ f̂ − ℓf∗) = O\n(\n(Rmaxκ ∗3) 1 2\n(\nT 2− 2 κ∗ )− 12 n− 1 2\n)\n.\nSchatten-norm: ∀q ∈ [1, 2], P (ℓ f̂ − ℓf∗) = O\n(\n(R′maxq ∗(q∗ − 1)) 12 T− 12n− 12\n)\n.\nGraph regularizer: P (ℓ f̂ − ℓf∗) = O\n(\n(R′′max) 1 2 T− 1 2n− 1 2\n)\n. (37)\nwhich can be compared to their LRC-based counterparts as following\nGroup norm: ∀κ ∈ [q, 2], P (ℓ f̂ − ℓf∗) = O\n(\n(Rmaxκ ∗3) 1 1+α\n(\nT 2− 2 κ∗ )− 11+α n −α 1+α\n)\n.\nSchatten-norm: ∀q ∈ [1, 2], P (ℓ f̂ − ℓf∗) = O\n(\n(R′maxq ∗(q∗ − 1)) 11+αT −11+αn −α1+α\n)\n.\nGraph regularizer: P (ℓ f̂ − ℓf∗) = O\n(\n(R′′max) 1 1+αT −1 1+αn −α 1+α\n)\n. (38)\nAs mentioned earlier in Remark 7, the bounds for the class of group norm regularizers for 1 ≤ q ≤ 2 is not monotonic in q; they are minimized for q∗ = 23 logT . Therefore, we split our analysis for the group norm into two cases: first, we consider q∗ ≥ 23 logT , which leads to the optimal choice κ∗ = q∗, and taking the minimum of the global and local bounds gives\nP (ℓ f̂ − ℓf∗) ≤ O\n(\nmin\n{\n(Rmaxκ ∗3) 1 2\n(\nT 2− 2 κ∗ )− 12 n− 1 2 , (Rmaxκ ∗3) 1 1+α ( T 2− 2 κ∗ )− 11+α n −α 1+α\n})\n.\nIt can be seen that for α ≈ 1, the minimum of the two terms are the same and local analysis has no advantages over the global one, however, for large value of α (more specially when α → ∞), it can be shown that local analysis improves over the global one, if T and Rmax can grow with n such that T/ √ Rmax = O( √ n).\nsecondly, we assume q∗ ≤ 23 logT in which the best choice is κ∗ = 23 logT . Then, the excess risk bound reads as\nP (ℓ f̂ − ℓf∗) ≤ O\n( min {\nR1/2max(logT ) 3/2T−1n−1/2, n−1\n})\n,\nand the local analysis improves over the global one, when T/ √ Rmax(logT )3 = O( √ n). A similar analysis for Schatten norm and graph regularized hypothesis spaces shows that the local analysis is beneficial over the global one whenever T/R = O(n).\nA close appraisal of the results in (37) and (38) points to a conservation of asymptotic rates between n and T , when all other remaining quantities are held fixed. This phenomenon is more apparent for the Shatten norm and graph-based regularization cases. It can be seen that, for both the global and local analysis results, the rates (exponents) of n and T sum up to −1. In the local analysis case, the trade-off is determined by the value of α, which can facilitate faster n-rates and compromise with slower T -rates. A similar trade-off is witnessed in the case of group norm regularization, but this time between n and T 2(1− 1 κ∗\n), instead of T , due to specific character of the group norm.\n6.2 Comparisons to Related Works\nAlso, it would be interesting to compare our (global and local) results for the trace norm regularized MTL with the GRC-baesd excess risk bound provided in [41] wherein they apply a trace norm regularizer to capture the tasks’ relatedness. It is worth mentioning that they consider a very slightly different hypothesis space for W , which in our notation reads as\nFS1 = { W : ‖W ‖S1 ≤ R ′ max √ T } . (39)\nThe intuition behind this assumption is interpreted as: assuming a common vector w for all tasks, the regularizer should not be a function of number of tasks [41]. Given the task averaged covariance operator C := 1/T ∑T\nt=1 Jt, the excess risk bound in [41] reads as (for the L-Lipschitz loss function ℓ, and F with ranges in [−b, b])\nP (ℓ f̂ − ℓf∗) ≤ 2LR′max\n(√\n‖C‖∞ n + 5\n√\nln(nT ) + 1\nnT\n)\n+\n√\nbLx nT .\nOne can easily verify that the trace norm is a Schatten norm with q = 1. Note that for any q ≥ 1 it holds that FS1 ⊆ FSq , which implies R(FS1) ≤ R(FSq ). This together with Theorem 17 and Theorem 16 (applied to the class of excess loss functions) can provide a GRC-based excess risk bound. Therefore, considering the trace norm hypothesis space (39) and the optimal value of q∗ = 2, translates our global and local bounds to the following\n1. GRC-based excess risk bound:\nP (ℓ f̂ − ℓf∗) ≤\n4L\nT 1/4\n√\nR′max nT ∥ ∥ ∥(tr (Jt)) T t=1 ∥ ∥ ∥ 1 +\n√\nbLx nT .\n2. LRC-based excess risk bound (∀α > 1):\nP (ℓ f̂ − ℓf∗) ≤ 256K\n√\nα+ 1 α− 1 ( 2dR′maxL 2 ) 1 1+α B α−1 α+1 T −1 2(1+α)n −α 1+α + (3Lb+ 4BK)x nT . (40)\nNow, assume that each operator Jt is of rank M and denote its maximum eigenvalue by λ max t . If λmax := maxt∈NT {λmaxt }, then it is easy to verify that tr(Jt) ≤ Mλmaxt and ‖C‖∞ ≤ λmax, which leads to the following GRC-based bounds\nOurs: P (ℓ f̂ − ℓf∗) ≤\n4L\nT 1/4\n√\nMλmaxR′max n +\n√\nbLx nT , (41)\n[41]: P (ℓ f̂ − ℓf∗) ≤ 2LR′max\n(√\nλmax n + 5\n√\nln(nT ) + 1\nnT\n)\n+\n√\nbLx nT . (42)\nOne can observe that as n → ∞, in all cases the bound approaches to zero, however our local bound in (40) at a rate of n−α/1+α, our global bound in (41) at a slower rate of 1/ √ n, and the one in (42) at the slowest rate of √ lnn/n.\nWe remark that, as T → ∞, the bound in (40) and (41) vanish at a rate of T−1/2(1+α) and √ 1/T 1/2,\nrespectively. Also, the bound in (42) approaches to the limiting value 2LR′max √\nλmax/n at a faster rate of √\nlnT/T , however it does not vanish even for very large value of T . Another interesting comparison can be performed between our bounds and the one introduced in [39] for a graph regularized hypothesis spaces similar to (22). [39] provides a bound on the empirical GRC, however, similar to the proof of Corollary 12, we can easily convert it to a distribution dependent GRC bound which in our notation reads as (assuming that ∥ ∥ ∥D 1/2W ∥ ∥ ∥ ≤ √ TR′′max)\nR (FG) ≤ √\nR′′2max nT ∥ ∥ ∥ ( D−1tt tr(Jt) )T t=1 ∥ ∥ ∥ 1 .\nNow, with D = L+ ηI and the assumption of rank M for Jts, it can be shown that\n∥ ∥ ∥ ( D−1tt tr(Jt) )T\nt=1 ∥ ∥ ∥ 1 = T∑\nt=1\nD−1tt tr(Jt) ≤ Mλmax ( T∑\nt=1\nD−1tt\n)\n= Mλmaxtr ( D−1 ) =\n= Mλmaxtr (L+ ηI) −1 = Mλmax\n( T∑\nt=1\n1\nδt + η +\n1\nη\n)\n= Mλmax\n( T\nδmin + η +\n1\nη\n)\n.\nwhere λmax is defined as before, and (δt) T t=1 are the eigenvalues of Laplacian matrix L with δmin := mint∈NT δt. Therefore, the GRC-based excess risk bounds are obtained as\nOurs: P (ℓ f̂ − ℓf∗) ≤ 2L√ n\n√\n2MλmaxR′′max T 1/2\n( 1\nδmin +\n1\nTη\n)\n+\n√\nbLx nT . (43)\n[39]: P (ℓ f̂ − ℓf∗) ≤ 2L√ n\n√\nMλmaxR′′2max\n( 1\nδmin +\n1\nTη\n)\n+\n√\nbLx nT . (44)\nalso, from Remark 14, the LRC-based bound is given as\nP (ℓ f̂ − ℓf∗) ≤ 256K\n√\nα+ 1 α− 1 ( dR′′maxL 2D−1max ) 1 1+α B α−1 α+1 T −1 2(1+α)n −α 1+α + (3Lb+ 4BK)x nT . (45)\nThe above results show that when n → ∞, all three bounds above approach zero, albeit, the global bounds with a rate of √\n1/n, and the local one with a faster rate of n−α/α+1, as α > 1. Also, as T → ∞, our bound in (43) vanishes at a rate of √ 1/T 1/2, while the bound in (44) approaches to the limiting value\n2LR′′max√ n\n√\nMλmax δmin\nat a faster rate of √\n1/T , however the cost of learning does not vanish. At the end, our local bound in (45) approaches zero at the slowest rate of T−1/2(1+α).\nA Appendix\nProofs of the results in Sect. 2: “Talagrand-Type Inequality for\nMulti-Task Learning”\nOur proof of Theorem 1 is based on the following Bousquet’s version of Talagrand inequality. Theorem A.1 (Theorem 6.1 in [14]). Let (Z,Z ′1, . . . , Z ′ n) be a sequence of A-measurable random variables and (Zk) n k=1 be a sequence of random variables Akn-measurable, where Akn is a sigma field generated by {Z1, . . . , Zn}/Zk. Assume that there exist c > 0, such that for all k = 1, . . . , n the following inequalities are satisfied\nZ ′k ≤ Z − Zk ≤ c, EknZ ′k = 0, n∑\nk=1\n(Z − Zk) ≤ Z.\nIf we have ∑n\nk=1 E k n [Z ′ k] 2 ≤ nσ2 and ν = 2cE(Z)+nσ2. Then with probability at least 1−e−x, for all x > 0, we have\nZ ≤ EZ + √ 2νx+ cx\n3 .\nProof of Theorem 1\nDefine the quantities\nZ := sup f∈F\n1\nT\nT∑\nt=1\n1\nNt\nNt∑\ni=1\n[ Eft(Xt)− ft(X it) ] ,\nZs,j := sup f∈F\n1\nT\nT∑\nt=1\n1\nNt\nNt∑\ni=1\n[ Eft(Xt)− ft(X it) ] − 1\nTNs\n[ Efs(Xs)− fs(Xjs ) ] .\nAlso, let f̂ := (f̂1, . . . f̂T ) be such that Z = 1 T ∑T t=1 1 Nt ∑Nt i=1\n[ Ef̂t(Xt)− f̂t(X it) ] and similarly f̂ s,j =\n(f̂1 s,j , . . . f̂T s,j\n) be the function achieving the supremum in the definition of Zs,j . Based on the definition of supremum, taking n := mint∈NT Nt, one can easily verify that\n1\nNsT\n[\nEf̂s s,j (Xs)− f̂s s,j (Xjs ) ] ≤ Z − Zs,j ≤ 1\nNsT\n[ Ef̂s(Xs)− f̂s(Xjs ) ] ≤ 2b nT .\nLet Z ′s,j := 1\nNsT\n[\nE[f̂s s,j (Xs)]− f̂s s,j (Xjs ) ]\nand c := 2bnT . One can verify that Z is sub-additive, that is ∑T\ns=1 ∑Ns j=1(Z − Zs,j) ≤ Z, and EZ ′s,j = 0. Thus, all the conditions of Theorem A.1 are satisfied. Also, we\nhave\nE [ Z ′s,j ]2 = 1\nN2s T 2 E\n[\nEf̂s s,j (Xs)− f̂s s,j (Xjs ) ]2 ≤ 1 N2s T 2 E [ f̂s s,j (Xjs ) ]2 . (A.1)\nTherefore,\nT∑\ns=1\nNs∑\nj=1\nE [ Z ′s,j ]2 ≤ T∑\ns=1\nNs∑\nj=1\n1\nN2s T 2 E\n[\nf̂s s,j (Xjs ) ]2\n≤ T∑\ns=1\nNs∑\nj=1\n1\nN2s T 2 sup f∈F\nE [ fs(X j s ) ]2\n= T∑\ns=1\nNs∑\nj=1\n1\nN2s T 2 sup f∈F\nE [ fs(X 1 s ) ]2\n≤ r nT ,\nwhere in the equality above, we used the fact that for fixed s, Xjss are identically distributed. Therefore, applying Theorem A.1 for Z implies the following with probability at least 1− e−x\nZ ≤ EZ + √ 2x ( r\nnT + 2cEZ\n)\n+ cx\n3 .\nSubstituting c = 2bnT , and using the simple inequalities √ u+ v ≤ √u+√v and 2√uv ≤ u+v for any u, v ≥ 0, we get\nZ ≤ 2EZ + √ 2xr\nnT +\n8bx 3nT . (A.2)\nThe first term in the right-hand side of the above inequality, EZ, can also be upper-bounded using the same approach as in Theorem 16 in [38]. Let X ′ be an i.i.d. copy of the XnT -valued random variable X . Then,\nEZ = EX\n[\nsup f∈F\n1 T EX′\n[ T∑\nt=1\n1\nNt\nNt∑\ni=1\n( ft ( X ′it ) − ft ( X it ))\n]]\n≤ EXX′ [\nsup f∈F\n1\nT\nT∑\nt=1\n1\nNt\nNt∑\ni=1\n( ft ( X ′it ) − ft ( X it ))\n]\n= EXX′\n[\nsup f∈F\n1\nT\nT∑\nt=1\n1\nNt\nNt∑\ni=1\nσit ( ft ( X ′it ) − ft ( X it ))\n]\n,\nsince for any i and t, the random variable ft ( X ′it ) − f ( X it ) has a symmetric distribution w.r.t. 0, therefore\nEZ ≤ EXEσ [\nsup f∈F\n2\nT\nT∑\nt=1\n1\nNt\nNt∑\ni=1\nσitft(X i t)\n]\n= 2R(F),\nwhere R(F) is the Rademacher complexity of function class F . Therefore, upper-bounding (A.2), one can obtain with probability at least 1− e−x,\nZ ≤ 4R(F) + √ 2xr\nnT +\n8bx 3nT ,\nwhich completes the proof.\nProofs of the results in Sect. 3: “Excess MTL Risk Bounds based\non Local Rademacher Complexities”\nThe following theorem in the core of the proof of Theorem 2 in Sect. 3.\nTheorem A.2 (Distribution-dependent bound for MTL). Let F = {f := (f1, . . . , fT )} be a class of vectorvalued functions satisfying supt,x |ft(x)| ≤ b. Let X := (X it , Y it ) (T,n) (t,i)=(1,1) be a vector of nT independent random variables where (X1t , Y 1 t ) . . . , (X n t , Y n t ), ∀t are identically distributed. Assume that there exist a constant B and a function T : F 7→ R+ such that for every f ∈ F , it holds that Pf2 ≤ V (f ) ≤ BPf . Let ψ be a sub-root function with the fixed point r∗. If ψ satisfies, for any r ≥ r∗,\nBR(F , r) ≤ ψ(r),\nwhere R(F , r) is the LRC of the function class F defined as\nR(F , r) := E\n\n  sup\nf∈F , V (f)≤r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t )\n\n  . (A.3)\nThen,\n1. For any function class F , K > 1 and x > 0, with probability at least 1− e−x,\nPf ≤ K K − 1Pnf + 500K B r∗ + (6b+ 10BK)x nT . (A.4)\n2. For any convex function class F , K > 1 and x > 0, with probability at least 1− e−x,\nPf ≤ K K − 1Pnf + 32K B r∗ + (3b+ 4BK)x nT . (A.5)\nProof. Define the rescaled version of F (its restriction to variance radius r) as\nFr := { f ′ = (f ′1, . . . , f ′ T ) , f ′ t :=\nr\nmax (r, V (f )) ft, ∀t, ft ∈ F\n}\n. (A.6)\nFirst, we will show that every f ′ ∈ Fr satisfies Pf ′2 ≤ r. Indeed, if we consider f ∈ F such that V (f) ≤ r, then by the definition of Fr, f ′t = ft, hence Pf ′2 = Pf2 ≤ V (f) ≤ r. Otherwise, if V (f) ≥ r, then f ′t = rft/V (f). Thus we have\nPf ′2 = 1\nT\nT∑\nt=1\nPf ′2t = r2\n(V (f )) 2\n(\n1\nT\nT∑\nt=1\nPf2t\n)\n= r2\n(V (f)) 2Pf\n2 ≤ r 2\n(V (f )) 2V (f) ≤ r.\nTherefore, we can conclude that for any f ′ ∈ Fr, it holds Pf ′2 ≤ r. Also, as the functions in F has ranges in [−b, b] and 0 ≤ r/max(r, V (f)) ≤ 1, it can be seen that any f ′ ∈ Fr satisfies ∣ ∣f ′ ∣ ∣ ≤ 2b, and ‖f ′ − Pf ′‖∞ ≤ 2b, consequently. Applying Theorem 1 on function class Fr, for all x > 0, with probability greater than 1− e−x, gives\nsup f ′∈Fr\n[Pf ′ − Pnf ′] ≤ 4R(Fr) + √ 2xr\nnT +\n8bx 3nT . (A.7)\nNow for the proof of the first part, let F(u, v) := {f : u ≤ V (f ) ≤ v}, and define for class Fr,\nRnf := 1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t), Rn(F) := sup\nf∈F Rnf . (A.8)\nClearly, the Rademacher complexity of Fr is ERn(Fr). Also, it can be shown for any sets A and B\nE\n[\nsup f ′∈A∪B\nRnf ′ ] ≤ E [\nsup f ′∈A\nRnf ′ ] + E [\nsup f ′∈B\nRnf ′ ] . (A.9)\nNote that for every f ∈ F it holds that V (f ) ≤ BPf ≤ Bb. Also, let λ > 1 and define k to be the smallest integer such that rλk+1 ≥ Bb. Thus by (A.9),\nR(Fr) = E [\nsup f ′∈Fr\nRnf ′ ] = E [\nsup f∈F\n1\nnT\nT∑\nt=1\nn∑\ni=1\nr\nmax(r, V (f )) σitft(X i t)\n]\n≤ E [\nsup f∈F(0,r)\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t)\n]\n+ E\n[\nsup f∈F(r,bB)\n1\nnT\nT∑\nt=1\nn∑\ni=1\nr\nV (f ) σitft(X i t)\n]\n≤ R(F , r) + k∑\nj=0\nλ−jE\n[\nsup f∈F(rλj,rλj+1) Rnf\n]\n, (A.10)\nwhere in the last inequality, we applied (A.9) for the union of intervals Aj := (rλj , rλj+1). Therefore, it can be seen that ψ(r) ≥ BR(F , r) implies\nR(Fr) ≤ R(F , r) + k∑\nj=0\nλ−jE\n[\nsup f∈F(rλj,rλj+1) Rnf\n]\n≤ ψ(r) B + 1 B\nk∑\nj=0\nλ−jψ(rλj+1). (A.11)\nNow since ψ(r) is a sub-root function, by the assumption it follows that for any λ ≥ 1, ψ(λr) ≤ √ λψ(r), hence\nR(Fr) ≤ ψ(r)\nB\n 1 + √ λ k∑\nj=0\nλ−j/2\n\n .\nBy the optimal λ = 4, the right-hand side can be upper-bounded by 5ψ(r)/B. Finally, for r ≥ r∗, it holds ψ(r) ≤ √ r/r∗ψ(r∗) = √ rr∗ and thus\nR(Fr) ≤ 5\nB\n√ rr∗. (A.12)\nCombining (A.7) and (A.12), for any r ≥ r∗ and x > 0, with probability at least 1− e−x, we have:\nsup f ′∈Fr\nPf ′ − Pnf ′ ≤ 20\nB\n√ rr∗ +\n√\n2xr nT + 8bx 3nT . (A.13)\nFinally, in the following we need to convert the upper bound for functions in the weighted class Fr into a bound for functions in the initial class F . Denote the left hand side of the above inequality by V +r . We will show that if V +r ≤ rBK , then\nPf ≤ K K − 1Pnf + r BK . (A.14)\nFirst, note that for any f ′ ∈ Fr, it holds that Pf ′ ≤ Pnf ′ + V +r . We also showed earlier, if V (f ) ≤ r then f ′ = f , hence\nPf ≤ Pnf + r\nBK . (A.15)\nOtherwise, if V (f) ≥ r, then f ′ = rf/V (f). Thus, r\nV (f ) Pf ≤ r V (f ) Pnf + V + r ≤ r V (f) Pnf + r BK , (A.16)\nwhich coupled with V (f ) ≤ BPf , implies that\nPf ≤ Pnf + Pf\nK . (A.17)\nCombining (A.15) and (A.17) implies that if supf ′∈Fr Pf ′ − Pnf ′ ≤ rBK , then (A.14) holds. Setting\nA = (20 √ r∗/B + √ 2x/nT ) and C = 8bx/3nT , the upper bound (A.13) can be written as A √ r + C. Now, we want to choose r0 ≥ r∗ such that the upper bound of (A.13) becomes of a form r0/BK. We achieve this by considering the largest solution of A √ r0 + C = r0/BK which satisfies r0 ≤ (BK)2A2 + 2BKC. Therefore, for every f ∈ F we have\nPf ≤ K K − 1Pnf + r0 BK\n≤ K K − 1Pnf +BKA 2 + 2C\n≤ K K − 1Pnf +BK\n(\n400 B2 r∗ + 40 B\n√\n2xr∗\nnT +\n2x\nnT\n)\n+ 16bx\n3nT . (A.18)\nUsing the fact that for any u, v ≥ 0 and α > 0 it holds 2√uv ≤ αu+v/α, we have √\n2xr∗/nT ≤ Bx/(5nT )+ 5r∗/(2B), we can complete the proof.\nAlso, the proof of the second part follows from the fact that Fr ⊆ {f ∈ star(F , 0) : V (f ) ≤ r}. From the other side, any convex function class F is star-shaped around any of its points. Therefore, R(Fr) in (A.7) can be bounded as R(Fr) ≤ R(F , r) ≤ ψ(r)/B. Then, following the same argument to convert the bound for the weighted class Fr into a bound for the functions in F completes the proof.\nThe following lemma which is a consequence of Corollary 2.2 from [7] is essential component of the proof in Theorem 4.\nLemma 1. Assume F is a class of vector-valued functions that map X into [−b, b] with b > 0. For every x > 0, if r satisfies\nr ≥ 16L2bEσ,X\n \n\nsup f∈F ,\nL2P (f−f∗)2≤r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t )\n \n\n+ 8L2b2x\nnT ,\nthen, with probability at least 1− e−x, {\nf ∈ F : L2P (f − f∗)2 ≤ r } ⊂ { f ∈ F : L2Pn (f − f∗)2 ≤ 2r } .\nProof. First, define\nF∗r := { f ′ = (f ′1, . . . , f ′ T ) : ∀t, f ′t = (ft − f∗t )2, ft ∈ F , L2P (f − f∗)2 ≤ r } .\nNote that for all t ∈ NT , (ft − f∗t )2 ∈ [0, b2]. Also, for any function in F∗r , it holds that\nPf ′2 = 1\nT\nT∑\nt=1\nPf ′2t = 1\nT\nT∑\nt=1\nP (ft − f∗t )4 ≤ b2\nT\nT∑\nt=1\nP (ft − f∗t )2 = b2P (f − f∗) 2 ≤ b\n2r\nL2 .\nTherefore, by Theorem 1, with probability at least 1− e−x, every f ′ ∈ F∗r satisfies\nPnf ′ ≤ Pf ′ + 4R(F∗r ) +\n√\n2b2xr nTL2 + 8b2x 3nT , (A.19)\nwhere\nR(F∗r ) = Eσ,X\n \n\nsup f∈F ,\nL2P (f−f∗)2≤r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσit(ft(X i t)− f∗t (X it))2\n \n\n≤ 2bEσ,X\n \n\nsup f∈F ,\nL2P (f−f∗)2≤r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t)\n \n\n. (A.20)\nThe last inequality follows from the facts that g(x) = x2 is 2b-Lipschitz on [−b, b] and f is fixed. This together with (A.19), gives\nPnf ′ ≤ Pf ′ + 8bEσ,X\n \n\nsup f∈F ,\nL2P (f−f∗)2≤r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t)\n \n\n+\n√\n2b2xr nTL2 + 8b2x 3nT\n≤ r L2 + 8bEσ,X\n \n\nsup f∈F ,\nL2P (f−f∗)2≤r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t )\n \n\n+ r\n2L2 +\n11b2x\n3nT . (A.21)\nNote that multiplying both side by L2 completes the proof.\nproof of Theorem 4\nDefine the function ψ(r) as\nψ(r) = c1 2 E\n\n  sup\nf∈F , L2P (f−f∗)2≤r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t )\n\n + (c2 − c1)x nT . (A.22)\nSince F is convex, it is star-shaped around any of its points, thus using Lemma 3.4 in [7] it can be shown that ψ(r) defined in (A.22) is a sub-root function. with the help of Corollary 3 and Assumptions 1, we have with probability at least 1− e−x\nL2P ( f̂ − f∗ )2 ≤ BP (\nℓ f̂ − ℓf∗\n)\n≤ 32Kr + (3Lb+ 4BK)Bx nT . (A.23)\nDenote the right hand side of the above inequality by s. Since s ≥ r ≥ r∗, then by the property of sub-root functions it holds that s ≥ ψ(s), and thus\ns ≥ 16L2bE\n\n  sup\nf∈F , L2P (f−f∗)2≤r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t)\n\n +\n8L2b2x\nnT .\nApplying Lemma 1, we have with probability at least 1− e−x, {\nf ∈ F , L2P (f − f∗)2 ≤ s } ⊂ { f ∈ F , L2Pn (f − f∗)2 ≤ 2s } .\nCombining this with (A.23), gives with probability at least 1− 2e−x,\nL2Pn\n( f̂ − f∗ )2 ≤ 2 ( 32Kr + (3Lb+ 4BK)Bx\nnT\n)\n≤ 2 ( 32K + (3Lb+ 4BK)B\nc2\n)\nr. (A.24)\nwhere in the last inequality we used the fact that r ≥ ψ(r) ≥ c2x/nT . Taking c = 2(32K+(3Lb+4BK)B/c2), and applying triangle inequality, if (A.24) holds, then for any f ∈ F , we have\nL2Pn\n( f − f̂ )2 ≤ ( √ L2Pn (f − f∗)2 + √ L2Pn ( f∗ − f̂ )2 )2\n(√\nL2Pn (f − f∗)2 + √ cr\n)2\n. (A.25)\nNow, applying Lemma 1 for r ≥ ψ(r), implies that with probability at least 1− 3e−x, {\nf ∈ F , L2P (f − f∗)2 ≤ r } ⊂ { f ∈ F , L2Pn (f − f∗)2 ≤ 2r } ,\nwhich coupled with (A.25), implies that with probability at least 1− 3e−x, {\nf ∈ F , L2P (f − f∗)2 ≤ r } ⊂ { f ∈ F , L2Pn ( f − f̂ )2 ≤ (√ 2 + √ c )2 r } .\nThus, with the help of Lemma A.4 in [7], it can be shown that with probability at least 1− 4e−x,\nψ(r) ≤ c1Eσ\n\n  sup\nf∈F , L2P (f−f∗)2≤r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t) ∣ ∣ ∣ ∣ ∣ { xit } t∈NT ,i∈Nn\n\n +\nc2x nT\n≤ c1Eσ\n\n  \nsup f∈F ,\nL2Pn(f−f̂)2≤( √ 2+ √ c)2r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t) ∣ ∣ ∣ ∣ ∣ { xit } t∈NT ,i∈Nn\n\n   + c2x\nnT\n≤ c1Eσ\n\n  sup\n∀t, f∈F , L2Pn(f−f̂)2≤(4+2c)r\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t ) ∣ ∣ ∣ ∣ ∣ { xit } t∈NT ,i∈Nn\n\n +\nc2x nT\n≤ ψ̂(r). (A.26)\nSetting r = r∗ and applying Lemma 4.3 of [7], gives r∗ ≤ r̂∗ which together with (A.23) yields the result.\nProofs of the results in Sect. 4: “Local Rademacher Complexity\nBounds for MTL models with Strongly Convex Regularizers”\nIn the following, we would like to provide some basic notions of convex analysis which are helpful in understanding the results of Sect. 4.\nDefinition 1 (Strong Convexity). A function R : X 7→ R is µ-strong convex w.r.t. a norm ‖.‖ if and only if ∀x, y ∈ X and ∀α ∈ (0, 1), we have\nR(αx + (1− α)y) ≤ αR(x) + (1− α)R(y)− µ 2 α(1 − α)‖x− y‖2.\nDefinition 2 (Strong Smoothness). A function R∗ : X 7→ R is 1µ -strong smooth w.r.t. a norm ‖.‖∗ if and only if R∗ is everywhere differentiable and ∀x, y ∈ X , we have\nR∗(x+ y) ≤ R∗(x) + 〈▽R∗(x), y〉+ 1 2µ ‖y‖2∗ .\nProperty 1 (Theorem 3 in [21]: Strong convexity/strong smoothness duality). A function R is µ-strongly convex w.r.t. the norm ‖.‖ if and only if its Fenchel conjugate R∗ is 1µ -strongly smooth w.r.t. the dual norm ‖.‖∗. The Fenchel conjugate R∗ is defined as\nR∗(w) := sup v {〈w,v〉 −R(v)} .\nProperty 2 (Fenchel-Young inequality). The definition of Fenchel dual implies that for any strong convex function R,\n∀w,v ∈ S, 〈w,v〉 ≤ R(w) +R∗(v).\nCombining this with the strong duality property of R∗ gives the following\n〈w,v〉 −R(w) ≤ R∗(v) ≤ R∗(0) + 〈▽R∗(0),v〉+ 1 2µ ‖v‖2∗ . (A.27)\nLemma 2. Assume that the conditions of Theorem 5 hold. Then, for ever f ∈ Fq,\n(a) Pf2 ≤ r implies 1/T ∑Tt=1 ∑∞ j=1 λ j t\n〈\nwt,u j t\n〉2\n≤ r.\n(b) EX,σ 〈 1 n ∑n i=1 σ i tφ(X i t ),u j t 〉2 = λjt n .\nProof. Part (a)\nPf2 = 1\nT\nT∑\nt=1\nE (〈 wt, φ(X i t) 〉)2 1\nT\nT∑\nt=1\nE (〈 wt ⊗wt, φ(X it )⊗ φ(X it) 〉)\n= 1\nT\nT∑\nt=1\n〈 wt ⊗wt,EX ( φ(X it)⊗ φ(X it ) )〉 = 1\nT\nT∑\nt=1\n∞∑\nj=1\nλjt\n〈 wt ⊗wt,ujt ⊗ ujt 〉\n= 1\nT\nT∑\nt=1\n∞∑\nj=1\nλjt\n〈\nwt,u j t\n〉〈\nwt,u j t\n〉\n= 1\nT\nT∑\nt=1\n∞∑\nj=1\nλjt\n〈\nwt,u j t\n〉2\n≤ r.\nPart (b)\nEX,σ\n〈\n1\nn\nn∑\ni=1\nσitφ(X i t ),u j t\n〉2\n= 1\nn2 EX,σ\nn∑\ni,k=1\nσitσ k t\n〈\nφ(X it ),u j t\n〉〈\nφ(Xkt ),u j t\n〉\nσti.i.d.= 1\nn2 EX\n( n∑\ni=1\n〈\nφ(X it ),u j t\n〉2 )\n= 1\nn\n〈\n1\nn\nn∑\ni=1\nEX ( φ(X it )⊗ φ(X it) ) ,ujt ⊗ ujt\n〉\n= 1\nn\n∞∑\nl=1\nλlt\n〈 ult ⊗ ult,ujt ⊗ ujt 〉 = λjt n .\nWe will use following lemmas in the proof of the LRC bound for the L2,q-group norm regularized MTL in Corollary 6.\nLemma 3 (Khintchine-Kahane Inequality [47]). Let H be an inner-product space with induced norm ‖·‖H, v1, . . . , vM ∈ H and σ1, . . . , σn i.i.d. Rademacher random variables. Then, for any p ≥ 1, we have that\nEσ ∥ ∥ ∥ ∥ ∥ n∑\ni=1\nσivi ∥ ∥ ∥ ∥ ∥ p\nH\n≤ ( c n∑\ni=1\n‖vi‖2H\n) p 2\n. (A.28)\nwhere c := max {1, p− 1}. The inequality also holds for p in place of c.\nLemma 4 (Rosenthal-Young Inequality; Lemma 3 of [24]). Let X1, . . . , Xn be independent, non-negative random variables satisfying Xi ≤ B < +∞ almost surely for all i = 1, . . . , n. If q ≥ 12 , cq := (2qe)q, then it holds\nE\n(\n1\nn\nn∑\ni=1\nXi\n)q ≤ Cq [( B\nn\n)q\n+\n(\n1\nn\nn∑\ni=1\nEXi\n)q]\n. (A.29)\nProof of Corollary 6\nFor the group norm regularizer ‖W ‖2,q, we can further bound the expectation term in (12) for D = I as following\nE := EX,σ ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∑ j>ht 〈 1 n n∑ i=1 σitφ(X i t ),u j t 〉 u j t   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥ 2\n2,q∗\n= EX,σ\n\n \nT∑\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∑\nj>ht\n〈\n1\nn\nn∑\ni=1\nσitφ(X i t),u j t\n〉\nu j t ∥ ∥ ∥ ∥ ∥ ∥\nq∗ \n \n2 q∗\nJensen ≤ EX\n\n \nT∑\nt=1\nEσ ∥ ∥ ∥ ∥ ∥ ∥ ∑\nj>ht\n〈\n1\nn\nn∑\ni=1\nσitφ(X i t),u j t\n〉\nu j t ∥ ∥ ∥ ∥ ∥ ∥\nq∗ \n \n2 q∗\n(A.28)\n≤ EX\n\n  \nT∑\nt=1\n\n q\n∗ n∑\ni=1\n∥ ∥ ∥ ∥ ∥ ∥ ∑\nj>ht\n〈 1\nn φ(X it ),u j t\n〉\nu j t ∥ ∥ ∥ ∥ ∥ ∥\n2 \n \nq∗\n2\n\n  \n2 q∗\n= q∗\nn EX\n\n \nT∑\nt=1\n  ∑\nj>ht\n1\nn\nn∑\ni=1\n〈\nφ(X it ),u j t\n〉2\n\n\nq∗\n2\n\n \n2 q∗\nJensen ≤ q\n∗\nn\n\n \nT∑\nt=1\nEX\n  ∑\nj>ht\n1\nn\nn∑\ni=1\n〈\nφ(X it ),u j t\n〉2\n\n\nq∗\n2\n\n \n2 q∗\n(A.30)\nNote that for q ≤ 2, it holds that q∗/2 ≥ 1. Therefore we cannot employ Jensen inequality to move the expectation operator inside the inner term, and we need to apply the Rosenthal-Young (R+Y) inequality (see Lemma 4 in the Appendix), which yields\nE R+Y ≤ q\n∗\nn\n\n \nT∑\nt=1\n(eq∗) q∗ 2\n\n \n( β\nn\n) q∗ 2\n+\n  ∑\nj>ht\n1\nn\nn∑\ni=1\nEX\n〈\nφ(X it),u j t\n〉2\n\n\nq∗\n2\n\n \n\n \n2 q∗\n= q∗\nn\n\n \nT∑\nt=1\n(eq∗) q∗ 2\n\n \n( β\nn\n) q∗ 2\n+\n  ∑\nj>ht\nλjt\n\n\nq∗\n2\n\n \n\n \n2 q∗\n. (A.31)\nThis can be further bounded, using the sub-additivity of q∗ √ . and √ . respectively in (††) and (†) below,\nE (†) ≤ eq\n∗2\nn\n\n   \n\nT\n( β\nn\n) q∗ 2\n\n\n2 q∗\n+\n\n \nT∑\nt=1\n  ∑\nj>ht\nλjt\n\n\nq∗\n2\n\n \n2 q∗\n\n   \n(††) ≤ eq\n∗2\nn\n\n  \nβT 2 q∗\nn + ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∑ j>ht λjt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n\n  \n= βeq∗2T 2 q∗\nn2 +\neq∗2\nn ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∑ j>ht λjt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n. (A.32)\nPlugging this into (12), and using subadditivity of √ . gives,\nA2(Fq) ≤ √ √ √ √ √ √ 2eq∗2Rmax nT 2µ ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∑ j>ht λjt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n+\n√ 2βeRmaxq ∗T 1 q∗\nnT √ µ\n. (A.33)\nNow, combining (11) and (A.33) provides the bound on R(Fq, r) as\nR(Fq, r) ≤\n√\nr ∑T\nt=1 ht nT + √ √ √ √ √ √ 2eq∗2Rmax nT 2µ ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∑ j>ht λjt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n+\n√ 2βeRmaxq ∗T 1 q∗\nnT √ µ\n(A.34)\n(⋆) ≤ √ √ √ √ √ √ √ 2\nnT\n\n   r\nT∑\nt=1\nht + 2eq∗2Rmax\nTµ\n∥ ∥ ∥ ∥ ∥ ∥ ∥   ∑ j>ht λjt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n\n   +\n√ 2βeRmaxq ∗T 1 q∗\nnT √ µ\n(⋆⋆)\n≤ √ √ √ √ √ √ √ 2\nnT\n\n   rT 1− 2 q∗ ∥ ∥ ∥(ht) T t=1 ∥ ∥ ∥ q∗\n2\n+ 2eq∗2Rmax\nTµ\n∥ ∥ ∥ ∥ ∥ ∥ ∥   ∑ j>ht λjt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n\n   +\n√ 2βeRmaxq ∗T 1 q∗\nnT √ µ\n(⋆⋆⋆)\n≤ √ √ √ √ √ √ 4\nnT ∥ ∥ ∥ ∥ ∥ ∥ ∥  rT 1− 2 q∗ ht + 2eq∗2Rmax Tµ ∑ j>ht λjt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n+\n√ 2βeRmaxq ∗T 1 q∗\nnT √ µ\n.\nwhere in (⋆), (⋆⋆) and (⋆⋆⋆) we applied following inequalities receptively, according which for all non-negative numbers α1 and α2, and non-negative vectors a1,a2 ∈ RT with 0 ≤ q ≤ p ≤ ∞ and s ≥ 1 it holds\n(⋆) √ α1 + √ α2 ≤ √ 2(α1 + α2)\n(⋆⋆) lp − to− lq : ‖a1‖q = 〈1,a1〉 1 q Hölder ≤\n(\n‖1‖(p/q)∗ ‖a q 1‖(p/q)\n) 1 q\n= T 1 q− 1p ‖a1‖p\n(⋆ ⋆ ⋆) ‖a1‖s + ‖a2‖s ≤ 21− 1 s ‖a1 + a2‖s ≤ 2 ‖a1 + a2‖s .\nSince inequality (⋆ ⋆ ⋆) holds for all non-negative ht, it follows\nR(Fq, r) ≤ √ √ √ √ √ √ 4\nnT ∥ ∥ ∥ ∥ ∥ ∥ ∥  min ht≥0 rT 1− 2 q∗ ht + 2eq∗2Rmax Tµ ∑ j>ht λjt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n+\n√ 2βeRmaxq ∗T 1 q∗\nnT √ µ\n≤ √ √ √ √ √ √ 4\nnT ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∞∑ j=1 min ( rT 1− 2 q∗ , 2eq∗2Rmax Tµ λjt )   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n+\n√ 2βeRmaxq ∗T 1 q∗\nnT √ µ\n.\nAlso, from Theorem 3 and Theorem 13 in [21], it can be shown that R(W ) = ‖W ‖22,q is 1q∗ -strongly convex w.r.t. the group norm ‖.‖2,q. , we can conclude the result.\nProof of Theorem 9\nR(Fq,R,T , r) = 1\nT EX,σ\n \n\nsup Pf2≤r,\n‖W‖2,q≤Rmax\nT∑\nt=1\n〈\nwt, 1\nn\nn∑\ni=1\nσitφ(X i t )\n〉\n \n\n= 1\nT EX,σ\n  \n\nsup 1/T\n∑T t=1 E〈wt,φ(Xt)〉\n2≤r, ‖W ‖2,q≤Rmax\nT∑\nt=1\n〈\nwt, 1\nn\nn∑\ni=1\nσitφ(X i t )\n〉\n  \n\n≥ 1 T EX,σ\n \n\nsup ∀t EX〈wt,φ(Xt)〉2≤r,\n‖W ‖2,q≤Rmax, ‖w1‖2=...=‖wt‖2\nT∑\nt=1\n〈\nwt, 1\nn\nn∑\ni=1\nσitφ(X i t )\n〉\n \n\n= 1\nT EX,σ\n \n sup ∀t EX〈wt,φ(Xt)〉2≤r, ∀t ‖wt‖2≤RmaxT − 1 q\nT∑\nt=1\n〈\nwt, 1\nn\nn∑\ni=1\nσitφ(X i t )\n〉\n \n\n= 1\nT\nT∑\nt=1\nEX,σ\n \n sup ∀t EX〈wt,φ(Xt)〉2≤r, ∀t ‖wt‖2≤RmaxT − 1 q\n〈\nwt, 1\nn\nn∑\ni=1\nσitφ(X i t )\n〉\n \n\n= EX,σ\n \n sup EX〈w1,φ(X1)〉2≤r, ‖w1‖2≤RmaxT − 1 q\n〈\nw1, 1\nn\nn∑\ni=1\nσi1φ(X i 1)\n〉\n \n\n= R(F 1,RT − 1 q ,1 , r).\nAccording to [43], it can be shown that there is a constant c such that if λ1t ≥ 1nR2max , then for all r ≥ 1 n\nit holds R(F 1,RT − 1 q ,1\n, r) ≥ √\nc n ∑∞ j=1 min\n(\nr, R2T− 2 q λj1\n)\n, which with some algebra manipulations gives the\ndesired result. The following lemma is used in the proof of the LRC bounds for the LSq -Schatten norm regularized MTL in Corollary 10.\nLemma 5 (Non-commutative Khintchine’s inequality [33]). Let Q1, . . . ,Qn be a set of arbitrary m × n matrices, and let σ1, . . . , σn be a sequence of independent Bernoulli random variables. Than for all p ≥ 2,\n\nEσ ∥ ∥ ∥ ∥ ∥ n∑\ni=1\nσiQi ∥ ∥ ∥ ∥ ∥ p\nSp\n\n\n1/p\n≤ p1/2 max\n \n ∥ ∥ ∥ ∥ ∥ ∥ ( n∑\ni=1\nQTi Qi )1/2 ∥ ∥ ∥ ∥ ∥ ∥ Sp , ∥ ∥ ∥ ∥ ∥ ∥ ( n∑ i=1 QiQ T i )1/2 ∥ ∥ ∥ ∥ ∥ ∥ Sp\n \n\n. (A.35)\nProof of Corollary 10\nIn order to find an LRC bound for a LSq -Schatten norm regularized hypothesis space (21), one just needs to bound the expectation term in (8). Define U it as a matrix with T columns where its only non-zero t th column is defined as ∑\nj>ht 〈 1 nφ(X i t ),u j t 〉 u j t . Also, note that for the Schatten norm regularized hypothesis\nspace (21), it holds that D = I. Therefore, applying Lemma 5 yields,\nEX,σ ∥ ∥ ∥D −1/2V ∥ ∥ ∥ 2\n∗ = EX,σ ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∑ j>ht 〈 1 n n∑ i=1 σitφ(X i t),u j t 〉 u j t   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥ 2\nSq∗\n= EX,σ ∥ ∥ ∥ ∥ ∥ T∑\nt=1\nn∑\ni=1\nσitU i t ∥ ∥ ∥ ∥ ∥ 2\nSq∗\nJensen ≤ EX\n \n Eσ ∥ ∥ ∥ ∥ ∥ T∑\nt=1\nn∑\ni=1\nσitU i t ∥ ∥ ∥ ∥ ∥ q∗\nSq∗\n \n\n2 q∗\n(A.35)\n≤ EX\n \n\nq∗1/2 max\n \n ∥ ∥ ∥ ∥ ∥ ∥ ( T∑\nt=1\nn∑\ni=1\n( U it )T U it )1/2 ∥ ∥ ∥ ∥ ∥ ∥ Sq∗ , ∥ ∥ ∥ ∥ ∥ ∥ ( T∑ t=1 n∑ i=1 U it ( U it )T )1/2 ∥ ∥ ∥ ∥ ∥ ∥ Sq∗\n \n\n \n\n2\nA© = q∗EX ∥ ∥ ∥ ∥ ∥ ∥ ( T∑\nt=1\nn∑\ni=1\n( U it )T U it )1/2 ∥ ∥ ∥ ∥ ∥ ∥ 2\nSq∗\n= q∗EX\n\n tr\n( T∑\nt=1\nn∑\ni=1\n( U it )T U it\n) q ∗\n2\n\n \n2 q∗\n= q∗EX\n\n  \n\n \nT∑\nt=1\nn∑\ni=1\n∥ ∥ ∥ ∥ ∥ ∥ ∑\nj>ht\n〈 1\nn φ(X it ),u j t\n〉\nu j t ∥ ∥ ∥ ∥ ∥ ∥\n2 \n \nq∗\n2\n\n  \n2 q∗\n= q∗EX\n\n \nT∑\nt=1\nn∑\ni=1\n∥ ∥ ∥ ∥ ∥ ∥ ∑\nj>ht\n〈 1\nn φ(X it ),u j t\n〉\nu j t ∥ ∥ ∥ ∥ ∥ ∥\n2 \n \n= q∗\nn2 EX\n\n\nT∑\nt=1\nn∑\ni=1\n∑\nj>ht\n〈\nφ(X it ),u j t\n〉2\n\n\nJensen ≤ q\n∗\nn2\n\n\nT∑\nt=1\nn∑\ni=1\n∑\nj>ht\nλjt\n  = q∗\nn ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∑ j>ht λjt   T\nt=1 ∥ ∥ ∥ ∥ ∥ ∥ ∥ 1 . (A.36)\nwhere in A©, we assumed that the first term in the max argument is the largest Note that using Theorem 11 in [21], it can be shown that the regularization function R(W ) = ‖W ‖2Sq with q ∈ [1, 2] is (q − 1)-strongly convex w.r.t. q-Schatten norm ‖.‖Sq . Plugging this into (8) completes the proof.\nProof of Corollary 12\nSimilar to the proof of Corollary 10, for the graph regularized hypothesis space (22), one can bound the expectation term in (8) as\nEX,σ ∥ ∥ ∥D −1/2V ∥ ∥ ∥ 2\n∗ = EX,σ\n[ tr ( V TD−1V )]\nJensen ≤ EX\n\n 1\nn2\nT,T ∑\nt,s=1\nn,n ∑\ni,l=1\n∑\nj>ht\n∑\nk>hs\nD−1st Eσ ( σitσ l s\n) 〈\nφ(X it),u j t 〉 〈 φ(X ls),u k s 〉 〈 u j t ,u k s 〉\n\n\n= EX\n\n 1\nn\nT∑\nt=1\nD−1tt ∑\nj>ht\n1\nn\nn∑\ni=1\n〈\nφ(X it ),u j t\n〉2\n\n\n= 1\nn\nT∑\nt=1\nD−1tt ∑\nj>ht\n1\nn\nn∑\ni=1\nEX\n〈\nφ(X it ),u j t\n〉2\n= 1\nn\nT∑\nt=1\n∑\nj>ht\nD−1tt λ j t =\n1\nn ∥ ∥ ∥ ∥ ∥ ∥ ∥  D−1tt ∑ j>ht λjt   T\nt=1 ∥ ∥ ∥ ∥ ∥ ∥ ∥ 1 . (A.37)\nproof of Corollary 15\nFirst notice that R̂(F∗q , c3r) ≤ 2R̂(Fq, c3r4L2 ). Also, similar to the proof of Remark 6 it can be show that\nR̂(Fq, c3r\n4L2 ) ≤\n√\nc3r ∑T\nt=1 ht 4nTL2 + √ √ √ √ √ √ 2q∗2Rmax nT 2 ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∑ j>ht λjt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n.\nTherefore,\nψ̂n(r) ≤ 2c1\n\n  \n√\nc3r ∑T\nt=1 ht 4nTL2 + √ √ √ √ √ √ 2q∗2Rmax nT 2 ∥ ∥ ∥ ∥ ∥ ∥ ∥   n∑ j>ht λ̂jt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n\n   + c2x\nnT\n=\n√\nc21c3r ∑T\nt=1 ht nTL2 + √ √ √ √ √ √ 8c21q ∗2Rmax nT 2 ∥ ∥ ∥ ∥ ∥ ∥ ∥   n∑ j>ht λ̂jt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n+ c2x\nnT .\nDenote the right hand side by ψ̂ubn (r). Solving the fixed point equation ψ̂ ub n (r) =\n√ αr + γ = r for\nα = c21c3\n∑T t=1 ht\nnTL2 , γ =\n√ √ √ √ √ √ 8c21q ∗2Rmax nT 2 ∥ ∥ ∥ ∥ ∥ ∥ ∥   n∑ j>ht λ̂jt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n+ c2x\nnT (A.38)\ngives r̂∗ ≤ α+ 2γ. Substituting α and γ completes the proof.\nProof of the results in Sect. 6.1: “Global vs. Local Rademacher\nComplexity Bounds”\nProof of Theorem 17\nNote that regarding the definition of A2 in (10), the global rademacher complexity for each case can be obtained by replacing the tail-sum ∑\nj>ht λjt in the bound of its corresponding A2(F) by ∑∞ j=1 λ j t = tr(Jt).\nIndeed, similar to the proof of Theorem 5, it can be shown that for the group norm with κ = q ∈ [1, 2],\nR(Fq) = EX,σ {\nsup f=(f1,...,fT )∈Fq\n1\nnT\nT∑\nt=1\nn∑\ni=1\nσitft(X i t)\n}\n≤ 1 T\n√ √ √ √ √ 2R\nµ EX,σ ∥ ∥ ∥ ∥ ∥ ∥ ( 1 n n∑\ni=1\nσitφ(X i t )\n)T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ 2\n2,q∗\n.\nAlso, one can verify the following\nEX,σ ∥ ∥ ∥ ∥ ∥ ∥ ( 1 n n∑\ni=1\nσitφ(X i t)\n)T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ 2\n2,q∗\n= EX,σ ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∞∑ j=1 〈 1 n n∑ i=1 σitφ(X i t),u j t 〉 u j t   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥ 2\n2,q∗\n≤ q ∗2βeT 2 q∗\nn2 +\nq∗2e\nn ∥ ∥ ∥ ∥ ∥ ∥ ∥   ∞∑ j=1 λjt   T\nt=1\n∥ ∥ ∥ ∥ ∥ ∥ ∥\nq∗\n2\n= q∗2βeT 2 q∗\nn2 +\nq∗2e\nn\n∥ ∥ ∥(tr (Jt)) T t=1 ∥ ∥ ∥\nq∗\n2\n. (A.39)\nwhere the last inequality obtained in a similar way in (A.32). The GRC bounds for the other cases can be easily derived in a very similar way.\nReferences\n[1] Qi An, Chunping Wang, Ivo Shterev, Eric Wang, Lawrence Carin, and David B Dunson. Hierarchical kernel stick-breaking process for multi-task image analysis. In Proceedings of the 25th international conference on Machine learning, pages 17–24. ACM, 2008.\n[2] Rie Kubota Ando and Tong Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. The Journal of Machine Learning Research, 6:1817–1853, 2005.\n[3] Andreas Argyriou, Stéphan Clémençon, and Ruocong Zhang. Learning the graph of relations among multiple tasks. ICML workshop on New Learning Frameworks and Models for Big Data, 2014.\n[4] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243–272, 2008.\n[5] Andreas Argyriou, Andreas Maurer, and Massimiliano Pontil. An algorithm for transfer learning in a heterogeneous environment. In Machine Learning and Knowledge Discovery in Databases, pages 71–85. Springer, 2008.\n[6] Andreas Argyriou, Massimiliano Pontil, Yiming Ying, and Charles A Micchelli. A spectral regularization framework for multi-task structure learning. In Advances in neural information processing systems, pages 25–32, 2007.\n[7] Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. Annals of Statistics, pages 1497–1537, 2005.\n[8] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. J. Mach. Learn. Res., 3:463–482, March 2003. Available from: http://dl.acm.org/citation.cfm?id=944919.944944.\n[9] Jonathan Baxter. A model of inductive bias learning. J. Artif. Intell. Res.(JAIR), 12(149-198):3, 2000.\n[10] Shai Ben-David and Reba Schuller Borbely. A notion of task relatedness yielding provable multiple-task learning guarantees. Machine learning, 73(3):273–287, 2008.\n[11] Shai Ben-David and Reba Schuller. Exploiting task relatedness for multiple task learning. In Learning Theory and Kernel Machines, pages 567–580. Springer, 2003.\n[12] Steffen Bickel, Jasmina Bogojeska, Thomas Lengauer, and Tobias Scheffer. Multi-task learning for hiv therapy screening. In Proceedings of the 25th international conference on Machine learning, pages 56–63. ACM, 2008.\n[13] Olivier Bousquet. A bennett concentration inequality and its application to suprema of empirical processes. Comptes Rendus Mathematique, 334(6):495–500, 2002.\n[14] Olivier Bousquet. Concentration inequalities for sub-additive functions using the entropy method. In Evariste Gin, Christian Houdr, and David Nualart, editors, Stochastic Inequalities and Applications, volume 56 of Progress in Probability, pages 213–247. Birkhuser Basel, 2003. Available from: http://dx.doi.org/10.1007/978-3-0348-8069-5_14, doi:10.1007/978-3-0348-8069-5_14.\n[15] Bin Cao, Nathan N Liu, and Qiang Yang. Transfer learning for collective link prediction in multiple heterogenous domains. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 159–166, 2010.\n[16] Rich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.\n[17] Corinna Cortes, Marius Kloft, and Mehryar Mohri. Learning kernels using local rademacher complexity. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2760–2768. Curran Associates, Inc., 2013. Available from: http://papers.nips.cc/paper/4896-learning-kernels-using-local-rademacher-complexity.pdf.\n[18] Corinna Cortes and Mehryar Mohri. Algorithmic Learning Theory: 22nd International Conference, ALT 2011, Espoo, Finland, October 5-7, 2011. Proceedings, chapter Domain Adaptation in Regression, pages 308–323. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011. Available from: http://dx.doi.org/10.1007/978-3-642-24412-4_25, doi:10.1007/978-3-642-24412-4_25.\n[19] Corinna Cortes and Mehryar Mohri. Domain adaptation and sample bias correction theory and algorithm for regression. Theoretical Computer Science, 519:103 – 126, 2014. Algorithmic Learning Theory. Available from: http://www.sciencedirect.com/science/article/pii/S0304397513007184, doi:http://dx.doi.org/10.1016/j.tcs.2013.09.027.\n[20] A Evgeniou and Massimiliano Pontil. Multi-task feature learning. Advances in neural information processing systems, 19:41, 2007.\n[21] Sham M Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. Regularization techniques for learning with matrices. The Journal of Machine Learning Research, 13(1):1865–1890, 2012.\n[22] Zhuoliang Kang, Kristen Grauman, and Fei Sha. Learning with whom to share in multi-task feature learning. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 521–528, 2011.\n[23] Marius Kloft and Gilles Blanchard. The local rademacher complexity of lpnorm multiple kernel learning. In J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 2438–2446. Curran Associates, Inc., 2011. Available from: http://papers.nips.cc/paper/4259-the-local-rademacher-complexity-of-lp-norm-multiple-kernel-learning.pdf.\n[24] Marius Kloft and Gilles Blanchard. On the convergence rate of lp-norm multiple kernel learning. The Journal of Machine Learning Research, 13(1):2465–2502, 2012.\n[25] V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error of combined classifiers. Ann. Statist., 30(1):1–50, 02 2002. Available from: http://dx.doi.org/10.1214/aos/1015362183, doi:10.1214/aos/1015362183.\n[26] Vladimir Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimization. Ann. Statist., 34(6):2593–2656, 12 2006. Available from: http://dx.doi.org/10.1214/009053606000001019, doi:10.1214/009053606000001019.\n[27] Vladimir Koltchinskii. Rademacher complexities and bounding the excess risk in active learning. J. Mach. Learn. Res., 11:2457–2485, December 2010. Available from: http://dl.acm.org/citation.cfm?id=1756006.1953014.\n[28] Abhishek Kumar and Hal Daume III. Learning task grouping and overlap in multi-task learning. arXiv preprint arXiv:1206.6417, 2012.\n[29] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer Science & Business Media, 2013.\n[30] Yunwen Lei, Lixin Ding, and Yingzhou Bi. Local rademacher complexity bounds based on covering numbers. arXiv:1510.01463 [cs.AI], 2015.\n[31] Cong Li, Michael Georgiopoulos, and Georgios C Anagnostopoulos. Multitask classification hypothesis space with improved generalization bounds. Neural Networks and Learning Systems, IEEE Transactions on, 2013.\n[32] K Lounici, M Pontil, AB Tsybakov, and SA Van De Geer. Taking advantage of sparsity in multi-task learning. In COLT 2009-The 22nd Conference on Learning Theory, 2009.\n[33] F. Lust-Piquard. Khintchine inequalities in cp (1 < p < ∞). COMPTES RENDUS DE L ACADEMIE DES SCIENCES SERIE I-MATHEMATIQUE, 303(7):289–292, 1986.\n[34] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In Proceedings of The 22nd Annual Conference on Learning Theory (COLT 2009). Omnipress, June 2009.\n[35] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with multiple sources. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1041–1048. Curran Associates, Inc., 2009. Available from: http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources.pdf.\n[36] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Multiple source adaptation and the rÉnyi divergence. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09, pages 367–374, Arlington, Virginia, United States, 2009. AUAI Press. Available from: http://dl.acm.org/citation.cfm?id=1795114.1795157.\n[37] Yishay Mansour and Mariano Schain. Robust domain adaptation. Annals of Mathematics and Artificial Intelligence, 71(4):365–380, 2013. Available from: http://dx.doi.org/10.1007/s10472-013-9391-5, doi:10.1007/s10472-013-9391-5.\n[38] Andreas Maurer. Bounds for linear multi-task learning. The Journal of Machine Learning Research, 7:117–139, 2006.\n[39] Andreas Maurer. The rademacher complexity of linear transformation classes. In Learning Theory, pages 65–78. Springer, 2006.\n[40] Andreas Maurer. Algorithmic Learning Theory: 25th International Conference, ALT 2014, Bled, Slovenia, October 8-10, 2014. Proceedings, chapter A Chain Rule for the Expected Suprema of Gaussian Processes, pages 245–259. Springer International Publishing, Cham, 2014. Available from: http://dx.doi.org/10.1007/978-3-319-11662-4_18, doi:10.1007/978-3-319-11662-4_18.\n[41] Andreas Maurer and Massimiliano Pontil. Excess risk bounds for multitask learning with trace norm regularization. In Conference on Learning Theory, volume 30, pages 55–76, 2013.\n[42] Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask representation learning. arXiv preprint arXiv:1505.06279, 2015.\n[43] Shahar Mendelson. On the performance of kernel classes. The Journal of Machine Learning Research, 4:759–771, 2003.\n[44] Luca Oneto, Alessandro Ghio, Sandro Ridella, and Davide Anguita. Local rademacher complexity: Sharper risk bounds with and without unlabeled samples. Neural Networks, 65:115 – 125, 2015. Available from: http://www.sciencedirect.com/science/article/pii/S0893608015000404, doi:http://dx.doi.org/10.1016/j.neunet.2015.02.006.\n[45] Anastasia Pentina and Shai Ben-David. Multi-task and lifelong learning of kernels. In Algorithmic Learning Theory, pages 194–208. Springer, 2015.\n[46] Anastasia Pentina and Christoph H Lampert. Lifelong learning with non-iid tasks. In Advances in Neural Information Processing Systems, pages 1540–1548, 2015.\n[47] G Peshkir and Albert Nikolaevich Shiryaev. The khintchine inequalities and martingale expanding sphere of their action. Russian Mathematical Surveys, 50(5):849–904, 1995.\n[48] Ting Kei Pong, Paul Tseng, Shuiwang Ji, and Jieping Ye. Trace norm regularization: Reformulations, algorithms, and multi-task learning. SIAM Journal on Optimization, 20(6):3465–3489, 2010.\n[49] Bernardino Romera-Paredes, Andreas Argyriou, Nadia Berthouze, and Massimiliano Pontil. Exploiting unrelated tasks in multi-task learning. In International Conference on Artificial Intelligence and Statistics, pages 951–959, 2012.\n[50] Michel Talagrand. New concentration inequalities in product spaces. Inventiones mathematicae, 126(3):505–563, 1996.\n[51] S Thrun. Learning to learn: Introduction. In In Learning To Learn, 1996.\n[52] I. Tolstikhin, G. Blanchard, and M. Kloft. Localized complexities for transductive learning. In Proceedings of the 27th Conference on Learning Theory, volume 35, pages 857–884. JMLR, 2014. Available from: http://jmlr.org/proceedings/papers/v35/tolstikhin14.pdf.\n[53] Qian Xu, Sinno Jialin Pan, Hannah Hong Xue, and Qiang Yang. Multitask learning for protein subcellular location prediction. IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB), 8(3):748–759, 2011.\n[54] Chao Zhang, Lei Zhang, and Jieping Ye. Generalization bounds for domain adaptation. In F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 3320–3328. Curran Associates, Inc., 2012. Available from: http://papers.nips.cc/paper/4684-generalization-bounds-for-domain-adaptation.pdf.\n[55] Yu Zhang and Dit-Yan Yeung. Multi-task warped gaussian process for personalized age estimation. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2622–2629. IEEE, 2010."
    } ],
    "references" : [ {
      "title" : "Hierarchical kernel stick-breaking process for multi-task image analysis",
      "author" : [ "Qi An", "Chunping Wang", "Ivo Shterev", "Eric Wang", "Lawrence Carin", "David B Dunson" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "A framework for learning predictive structures from multiple tasks and unlabeled data",
      "author" : [ "Rie Kubota Ando", "Tong Zhang" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "Learning the graph of relations among multiple tasks",
      "author" : [ "Andreas Argyriou", "Stéphan Clémençon", "Ruocong Zhang" ],
      "venue" : "ICML workshop on New Learning Frameworks and Models for Big Data,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Convex multi-task feature learning",
      "author" : [ "Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "An algorithm for transfer learning in a heterogeneous environment",
      "author" : [ "Andreas Argyriou", "Andreas Maurer", "Massimiliano Pontil" ],
      "venue" : "In Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "A spectral regularization framework for multi-task structure learning",
      "author" : [ "Andreas Argyriou", "Massimiliano Pontil", "Yiming Ying", "Charles A Micchelli" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Local rademacher complexities",
      "author" : [ "Peter L Bartlett", "Olivier Bousquet", "Shahar Mendelson" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2005
    }, {
      "title" : "Rademacher and gaussian complexities: Risk bounds and structural results",
      "author" : [ "Peter L. Bartlett", "Shahar Mendelson" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2003
    }, {
      "title" : "A model of inductive bias learning",
      "author" : [ "Jonathan Baxter" ],
      "venue" : "J. Artif. Intell. Res.(JAIR),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2000
    }, {
      "title" : "A notion of task relatedness yielding provable multiple-task learning guarantees",
      "author" : [ "Shai Ben-David", "Reba Schuller Borbely" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Exploiting task relatedness for multiple task learning",
      "author" : [ "Shai Ben-David", "Reba Schuller" ],
      "venue" : "In Learning Theory and Kernel Machines,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2003
    }, {
      "title" : "Multi-task learning for hiv therapy screening",
      "author" : [ "Steffen Bickel", "Jasmina Bogojeska", "Thomas Lengauer", "Tobias Scheffer" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "A bennett concentration inequality and its application to suprema of empirical processes",
      "author" : [ "Olivier Bousquet" ],
      "venue" : "Comptes Rendus Mathematique,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "Concentration inequalities for sub-additive functions using the entropy method",
      "author" : [ "Olivier Bousquet" ],
      "venue" : "Birkhuser Basel,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2003
    }, {
      "title" : "Transfer learning for collective link prediction in multiple heterogenous domains",
      "author" : [ "Bin Cao", "Nathan N Liu", "Qiang Yang" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Multitask learning",
      "author" : [ "Rich Caruana" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1997
    }, {
      "title" : "Learning kernels using local rademacher complexity",
      "author" : [ "Corinna Cortes", "Marius Kloft", "Mehryar Mohri" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Algorithmic Learning Theory: 22nd International Conference, ALT",
      "author" : [ "Corinna Cortes", "Mehryar Mohri" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Domain adaptation and sample bias correction theory and algorithm for regression",
      "author" : [ "Corinna Cortes", "Mehryar Mohri" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Multi-task feature learning",
      "author" : [ "A Evgeniou", "Massimiliano Pontil" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2007
    }, {
      "title" : "Regularization techniques for learning with matrices",
      "author" : [ "Sham M Kakade", "Shai Shalev-Shwartz", "Ambuj Tewari" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Learning with whom to share in multi-task feature learning",
      "author" : [ "Zhuoliang Kang", "Kristen Grauman", "Fei Sha" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "The local rademacher complexity of lpnorm multiple kernel learning",
      "author" : [ "Marius Kloft", "Gilles Blanchard" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "On the convergence rate of lp-norm multiple kernel learning",
      "author" : [ "Marius Kloft", "Gilles Blanchard" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Empirical margin distributions and bounding the generalization error of combined classifiers",
      "author" : [ "V. Koltchinskii", "D. Panchenko" ],
      "venue" : "Ann. Statist., 30(1):1–50,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2002
    }, {
      "title" : "Local rademacher complexities and oracle inequalities in risk minimization",
      "author" : [ "Vladimir Koltchinskii" ],
      "venue" : "Ann. Statist., 34(6):2593–2656,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2006
    }, {
      "title" : "Rademacher complexities and bounding the excess risk in active learning",
      "author" : [ "Vladimir Koltchinskii" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2010
    }, {
      "title" : "Learning task grouping and overlap in multi-task learning",
      "author" : [ "Abhishek Kumar", "Hal Daume III" ],
      "venue" : "arXiv preprint arXiv:1206.6417,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "Probability in Banach Spaces: isoperimetry and processes",
      "author" : [ "Michel Ledoux", "Michel Talagrand" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2013
    }, {
      "title" : "Local rademacher complexity bounds based on covering numbers",
      "author" : [ "Yunwen Lei", "Lixin Ding", "Yingzhou Bi" ],
      "venue" : "[cs.AI],",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Multitask classification hypothesis space with improved generalization bounds",
      "author" : [ "Cong Li", "Michael Georgiopoulos", "Georgios C Anagnostopoulos" ],
      "venue" : "Neural Networks and Learning Systems, IEEE Transactions on,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2013
    }, {
      "title" : "Taking advantage of sparsity in multi-task learning",
      "author" : [ "K Lounici", "M Pontil", "AB Tsybakov", "SA Van De Geer" ],
      "venue" : "In COLT 2009-The 22nd Conference on Learning Theory,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2009
    }, {
      "title" : "Khintchine inequalities in cp (1 < p < ∞)",
      "author" : [ "F. Lust-Piquard" ],
      "venue" : "COMPTES RENDUS DE L ACADEMIE DES SCIENCES SERIE I-MATHEMATIQUE,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1986
    }, {
      "title" : "Domain adaptation: Learning bounds and algorithms",
      "author" : [ "Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh" ],
      "venue" : "In Proceedings of The 22nd Annual Conference on Learning Theory (COLT",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2009
    }, {
      "title" : "Domain adaptation with multiple sources",
      "author" : [ "Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2009
    }, {
      "title" : "Multiple source adaptation and the rÉnyi divergence",
      "author" : [ "Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh" ],
      "venue" : "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2009
    }, {
      "title" : "Robust domain adaptation",
      "author" : [ "Yishay Mansour", "Mariano Schain" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2013
    }, {
      "title" : "Bounds for linear multi-task learning",
      "author" : [ "Andreas Maurer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2006
    }, {
      "title" : "The rademacher complexity of linear transformation classes. In Learning Theory, pages 65–78",
      "author" : [ "Andreas Maurer" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2006
    }, {
      "title" : "Algorithmic Learning Theory: 25th International Conference, ALT",
      "author" : [ "Andreas Maurer" ],
      "venue" : null,
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2014
    }, {
      "title" : "Excess risk bounds for multitask learning with trace norm regularization",
      "author" : [ "Andreas Maurer", "Massimiliano Pontil" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2013
    }, {
      "title" : "The benefit of multitask representation learning",
      "author" : [ "Andreas Maurer", "Massimiliano Pontil", "Bernardino Romera-Paredes" ],
      "venue" : "arXiv preprint arXiv:1505.06279,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2015
    }, {
      "title" : "On the performance of kernel classes",
      "author" : [ "Shahar Mendelson" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2003
    }, {
      "title" : "Local rademacher complexity: Sharper risk bounds with and without unlabeled samples",
      "author" : [ "Luca Oneto", "Alessandro Ghio", "Sandro Ridella", "Davide Anguita" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2015
    }, {
      "title" : "Multi-task and lifelong learning of kernels",
      "author" : [ "Anastasia Pentina", "Shai Ben-David" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2015
    }, {
      "title" : "Lifelong learning with non-iid tasks",
      "author" : [ "Anastasia Pentina", "Christoph H Lampert" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2015
    }, {
      "title" : "The khintchine inequalities and martingale expanding sphere of their action",
      "author" : [ "G Peshkir", "Albert Nikolaevich Shiryaev" ],
      "venue" : "Russian Mathematical Surveys,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 1995
    }, {
      "title" : "Trace norm regularization: Reformulations, algorithms, and multi-task learning",
      "author" : [ "Ting Kei Pong", "Paul Tseng", "Shuiwang Ji", "Jieping Ye" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2010
    }, {
      "title" : "Exploiting unrelated tasks in multi-task learning",
      "author" : [ "Bernardino Romera-Paredes", "Andreas Argyriou", "Nadia Berthouze", "Massimiliano Pontil" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2012
    }, {
      "title" : "New concentration inequalities in product spaces",
      "author" : [ "Michel Talagrand" ],
      "venue" : "Inventiones mathematicae,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 1996
    }, {
      "title" : "Learning to learn: Introduction",
      "author" : [ "S Thrun" ],
      "venue" : "In In Learning To Learn,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 1996
    }, {
      "title" : "Localized complexities for transductive learning",
      "author" : [ "I. Tolstikhin", "G. Blanchard", "M. Kloft" ],
      "venue" : "In Proceedings of the 27th Conference on Learning Theory,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2014
    }, {
      "title" : "Multitask learning for protein subcellular location prediction",
      "author" : [ "Qian Xu", "Sinno Jialin Pan", "Hannah Hong Xue", "Qiang Yang" ],
      "venue" : "IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB),",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2011
    }, {
      "title" : "Generalization bounds for domain adaptation",
      "author" : [ "Chao Zhang", "Lei Zhang", "Jieping Ye" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2012
    }, {
      "title" : "Multi-task warped gaussian process for personalized age estimation",
      "author" : [ "Yu Zhang", "Dit-Yan Yeung" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Pioneering works on MTL include [16, 9, 2, 4].",
      "startOffset" : 32,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : "Pioneering works on MTL include [16, 9, 2, 4].",
      "startOffset" : 32,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "Pioneering works on MTL include [16, 9, 2, 4].",
      "startOffset" : 32,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "Pioneering works on MTL include [16, 9, 2, 4].",
      "startOffset" : 32,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 14,
      "context" : "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 54,
      "context" : "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 52,
      "context" : "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 24,
      "context" : "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 7,
      "context" : "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 37,
      "context" : "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 38,
      "context" : "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 20,
      "context" : "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.",
      "startOffset" : 221,
      "endOffset" : 225
    }, {
      "referenceID" : 40,
      "context" : "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 39,
      "context" : "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 41,
      "context" : "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.",
      "startOffset" : 242,
      "endOffset" : 246
    }, {
      "referenceID" : 25,
      "context" : "More recently, the seminal works in [26] and [7] introduced a more nuanced variant of these complexities, termed Local Rademacher Complexity (LRC) (as opposed to the original Global Rademacher Complexity (GRC)).",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "More recently, the seminal works in [26] and [7] introduced a more nuanced variant of these complexities, termed Local Rademacher Complexity (LRC) (as opposed to the original Global Rademacher Complexity (GRC)).",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "This new, modified function class complexity measure is attention-worthy, since, as shown in [7], a LRCs-based (local) analysis is capable of producing more rapidly-converging excess risk bounds, when compared to the ones obtained via a GRC (global) analysis.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 26,
      "context" : "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 22,
      "context" : "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 16,
      "context" : "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 51,
      "context" : "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 43,
      "context" : "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].",
      "startOffset" : 267,
      "endOffset" : 271
    }, {
      "referenceID" : 29,
      "context" : "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].",
      "startOffset" : 316,
      "endOffset" : 320
    }, {
      "referenceID" : 37,
      "context" : "3 Previous Related Works Earlier works that investigate MTL generalization guarantees employing Rademacher averages include [38], which considers linear MTL frameworks for binary classification.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 38,
      "context" : "Another study, [39], provides bounds for the empirical and expected Rademacher complexities of",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 20,
      "context" : "In [21], the authors take advantage of the strongly-convex nature of certain matrix-norm regularizers to easily obtain generalization bounds for a variety of machine learning problems.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 40,
      "context" : "Moreover, [41] presents a global Rademacher complexity analysis leading to both data and distribution-dependent excess risk bounds of order O( √ log(n)/n) and non-vanishing w.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 39,
      "context" : "Also, [40] examines the bounding of (global) Gaussian complexities of function classes that result from considering composite maps, as it is typical in MTL among other settings.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 41,
      "context" : "More recently, [42] presents excess risk bounds for both MTL and Learning-To-Learn (LTL) settings and reveals conditions, under which MTL is more beneficial over learning tasks independently.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 38,
      "context" : "The accompanying bounds are of order O(1/ √ nT ) and, compared to the results in [39, 41], it enjoys the advantage of vanishing as T → +∞.",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 40,
      "context" : "The accompanying bounds are of order O(1/ √ nT ) and, compared to the results in [39, 41], it enjoys the advantage of vanishing as T → +∞.",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 50,
      "context" : "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].",
      "startOffset" : 83,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].",
      "startOffset" : 83,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].",
      "startOffset" : 83,
      "endOffset" : 99
    }, {
      "referenceID" : 45,
      "context" : "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].",
      "startOffset" : 83,
      "endOffset" : 99
    }, {
      "referenceID" : 44,
      "context" : "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 33,
      "context" : "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 35,
      "context" : "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 34,
      "context" : "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 53,
      "context" : "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 36,
      "context" : "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "2 Talagrand-Type Inequality for Multi-Task Learning The derivation of our LRC-based error bounds for MTL is founded on the following modified Talagrand’s concentration inequality [13, 50] adapted to the context of MTL, showing that the uniform deviation between the true and empirical means in a vector-valued function class F can be dominated by the associated multitask Rademacher complexity plus a term involving the variance of functions in F .",
      "startOffset" : 179,
      "endOffset" : 187
    }, {
      "referenceID" : 49,
      "context" : "2 Talagrand-Type Inequality for Multi-Task Learning The derivation of our LRC-based error bounds for MTL is founded on the following modified Talagrand’s concentration inequality [13, 50] adapted to the context of MTL, showing that the uniform deviation between the true and empirical means in a vector-valued function class F can be dominated by the associated multitask Rademacher complexity plus a term involving the variance of functions in F .",
      "startOffset" : 179,
      "endOffset" : 187
    }, {
      "referenceID" : 37,
      "context" : "In Theorem 1, the data from different tasks assumed to be mutually independent, which is typical in the MTL setting [38].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 28,
      "context" : "Also, using Talagrand’s Lemma [29], one can verify",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "3 of [7] to MTL function classes) to the function class H∗ F completes the proof.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : "4 in [7], presents a data-dependent version of (5) replacing the Rademacher complexity in Corollary 3 with its empirical counterpart.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : "4 in [7] and, therefore, can be found in the Appendix.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 38,
      "context" : "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].",
      "startOffset" : 228,
      "endOffset" : 249
    }, {
      "referenceID" : 19,
      "context" : "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].",
      "startOffset" : 228,
      "endOffset" : 249
    }, {
      "referenceID" : 5,
      "context" : "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].",
      "startOffset" : 228,
      "endOffset" : 249
    }, {
      "referenceID" : 3,
      "context" : "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].",
      "startOffset" : 228,
      "endOffset" : 249
    }, {
      "referenceID" : 30,
      "context" : "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].",
      "startOffset" : 228,
      "endOffset" : 249
    }, {
      "referenceID" : 2,
      "context" : "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].",
      "startOffset" : 228,
      "endOffset" : 249
    }, {
      "referenceID" : 19,
      "context" : "3 Group Norm Regularized MTL We first consider the group norm regularized MTL capturing the inter-task relationships by the group norm ‖W ‖2,q := (∑T t=1 ‖wt‖ q 2 )1/q [20, 4, 32, 49], for which the associated hypothesis space takes the form Fq := { X 7→ [〈w1, φ(X1)〉 , .",
      "startOffset" : 168,
      "endOffset" : 183
    }, {
      "referenceID" : 3,
      "context" : "3 Group Norm Regularized MTL We first consider the group norm regularized MTL capturing the inter-task relationships by the group norm ‖W ‖2,q := (∑T t=1 ‖wt‖ q 2 )1/q [20, 4, 32, 49], for which the associated hypothesis space takes the form Fq := { X 7→ [〈w1, φ(X1)〉 , .",
      "startOffset" : 168,
      "endOffset" : 183
    }, {
      "referenceID" : 31,
      "context" : "3 Group Norm Regularized MTL We first consider the group norm regularized MTL capturing the inter-task relationships by the group norm ‖W ‖2,q := (∑T t=1 ‖wt‖ q 2 )1/q [20, 4, 32, 49], for which the associated hypothesis space takes the form Fq := { X 7→ [〈w1, φ(X1)〉 , .",
      "startOffset" : 168,
      "endOffset" : 183
    }, {
      "referenceID" : 48,
      "context" : "3 Group Norm Regularized MTL We first consider the group norm regularized MTL capturing the inter-task relationships by the group norm ‖W ‖2,q := (∑T t=1 ‖wt‖ q 2 )1/q [20, 4, 32, 49], for which the associated hypothesis space takes the form Fq := { X 7→ [〈w1, φ(X1)〉 , .",
      "startOffset" : 168,
      "endOffset" : 183
    }, {
      "referenceID" : 5,
      "context" : "Among p ≥ 1, a particular group-norm of independent interest is the sparsity-inducing group-norm achieved by q = 1 [6, 4], for which we can take κ∗ = logT to get that R(F1, r) ≤ √ √ √ √ 4 nT ∥ ∥ ∥ ( ∞ ∑",
      "startOffset" : 115,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "Among p ≥ 1, a particular group-norm of independent interest is the sparsity-inducing group-norm achieved by q = 1 [6, 4], for which we can take κ∗ = logT to get that R(F1, r) ≤ √ √ √ √ 4 nT ∥ ∥ ∥ ( ∞ ∑",
      "startOffset" : 115,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "4 Schatten Norm Regularized MTL [6] developed a spectral regularization framework for MTL where the Schatten p-norm ‖W‖Sq := [ tr ( W W ) q 2 ] 1 q",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 40,
      "context" : "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 47,
      "context" : "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].",
      "startOffset" : 210,
      "endOffset" : 221
    }, {
      "referenceID" : 27,
      "context" : "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].",
      "startOffset" : 210,
      "endOffset" : 221
    }, {
      "referenceID" : 21,
      "context" : "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].",
      "startOffset" : 210,
      "endOffset" : 221
    }, {
      "referenceID" : 38,
      "context" : "We consider the following graph regularized MTL [39]",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "Note that, due to the space limitation, the proofs of the results are provided only for the hypothesis space Fq with q ∈ [1, 2] in (13).",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "Note that, due to the space limitation, the proofs of the results are provided only for the hypothesis space Fq with q ∈ [1, 2] in (13).",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "However, for the group and LSq -Schatten norm (q ∈ [1, 2]) regularized MTL, the proofs can be obtained in a very similar way.",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "However, for the group and LSq -Schatten norm (q ∈ [1, 2]) regularized MTL, the proofs can be obtained in a very similar way.",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : "4 in [7], Fq is a sub-root function.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 37,
      "context" : "First, note that to obtain the GRC-based bounds, we apply Theorem 16 of [38], as we consider the same setting and assumptions for tasks’ distributions as considered in this work.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 37,
      "context" : "Theorem 16 (MTL excess risk bound based on GRC; Theorem 16 of [38] ).",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 37,
      "context" : "As it has been shown in [38], the proof of this theorem is based on using McDiarmid’s inequality for Z defined in Theorem 1, and noticing that for the function class F with values in [−b, b], it holds that |Z − Zs,j | ≤ 2b/nT .",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "Schatten-norm: ∀q ∈ [1, 2], P (l f̂ − lf∗) = O ( (R′ maxq ∗(q∗ − 1)) 1 2 T− 12n− 12 ) .",
      "startOffset" : 20,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "Schatten-norm: ∀q ∈ [1, 2], P (l f̂ − lf∗) = O ( (R′ maxq ∗(q∗ − 1)) 1 2 T− 12n− 12 ) .",
      "startOffset" : 20,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "Schatten-norm: ∀q ∈ [1, 2], P (l f̂ − lf∗) = O ( (R′ maxq ∗(q∗ − 1)) 1 1+αT −1 1+αn −α 1+α ) .",
      "startOffset" : 20,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "Schatten-norm: ∀q ∈ [1, 2], P (l f̂ − lf∗) = O ( (R′ maxq ∗(q∗ − 1)) 1 1+αT −1 1+αn −α 1+α ) .",
      "startOffset" : 20,
      "endOffset" : 26
    }, {
      "referenceID" : 40,
      "context" : "2 Comparisons to Related Works Also, it would be interesting to compare our (global and local) results for the trace norm regularized MTL with the GRC-baesd excess risk bound provided in [41] wherein they apply a trace norm regularizer to capture the tasks’ relatedness.",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 40,
      "context" : "(39) The intuition behind this assumption is interpreted as: assuming a common vector w for all tasks, the regularizer should not be a function of number of tasks [41].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 40,
      "context" : "Given the task averaged covariance operator C := 1/T ∑T t=1 Jt, the excess risk bound in [41] reads as (for the L-Lipschitz loss function l, and F with ranges in [−b, b])",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 40,
      "context" : "[41]: P (l f̂ − lf∗) ≤ 2LR′ max (√ λmax n + 5 √ ln(nT ) + 1 nT ) + √ bLx nT .",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "Another interesting comparison can be performed between our bounds and the one introduced in [39] for a graph regularized hypothesis spaces similar to (22).",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 38,
      "context" : "[39] provides a bound on the empirical GRC, however, similar to the proof of Corollary 12, we can easily convert it to a distribution dependent GRC bound which in our notation reads as (assuming that ∥ ∥ ∥D W ∥ ∥ ∥ ≤ √ TR′′ max)",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "[39]: P (l f̂ − lf∗) ≤ 2L √ n √ MλmaxR′′2 max ( 1 δmin + 1 Tη ) + √ bLx nT .",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "1 in [14]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 37,
      "context" : "2) The first term in the right-hand side of the above inequality, EZ, can also be upper-bounded using the same approach as in Theorem 16 in [38].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 6,
      "context" : "2 from [7] is essential component of the proof in Theorem 4.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 6,
      "context" : "4 in [7] it can be shown that ψ(r) defined in (A.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : "4 in [7], it can be shown that with probability at least 1− 4e−x, ψ(r) ≤ c1Eσ ",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : "3 of [7], gives r∗ ≤ r̂∗ which together with (A.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 20,
      "context" : "Property 1 (Theorem 3 in [21]: Strong convexity/strong smoothness duality).",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 46,
      "context" : "Lemma 3 (Khintchine-Kahane Inequality [47]).",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 23,
      "context" : "Lemma 4 (Rosenthal-Young Inequality; Lemma 3 of [24]).",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 20,
      "context" : "Also, from Theorem 3 and Theorem 13 in [21], it can be shown that R(W ) = ‖W ‖22,q is 1 q -strongly convex w.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 42,
      "context" : "According to [43], it can be shown that there is a constant c such that if λt ≥ 1 nRmax , then for all r ≥ 1 n it holds R(F 1,RT − 1 q ,1 , r) ≥ √ c n ∑∞ j=1 min ( r, R2T− 2 q λj1 ) , which with some algebra manipulations gives the desired result.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 32,
      "context" : "Lemma 5 (Non-commutative Khintchine’s inequality [33]).",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 20,
      "context" : "where in A ©, we assumed that the first term in the max argument is the largest Note that using Theorem 11 in [21], it can be shown that the regularization function R(W ) = ‖W ‖2Sq with q ∈ [1, 2] is (q − 1)-strongly convex w.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "where in A ©, we assumed that the first term in the max argument is the largest Note that using Theorem 11 in [21], it can be shown that the regularization function R(W ) = ‖W ‖2Sq with q ∈ [1, 2] is (q − 1)-strongly convex w.",
      "startOffset" : 190,
      "endOffset" : 196
    }, {
      "referenceID" : 1,
      "context" : "where in A ©, we assumed that the first term in the max argument is the largest Note that using Theorem 11 in [21], it can be shown that the regularization function R(W ) = ‖W ‖2Sq with q ∈ [1, 2] is (q − 1)-strongly convex w.",
      "startOffset" : 190,
      "endOffset" : 196
    }, {
      "referenceID" : 0,
      "context" : "Indeed, similar to the proof of Theorem 5, it can be shown that for the group norm with κ = q ∈ [1, 2], R(Fq) = EX,σ {",
      "startOffset" : 96,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "Indeed, similar to the proof of Theorem 5, it can be shown that for the group norm with κ = q ∈ [1, 2], R(Fq) = EX,σ {",
      "startOffset" : 96,
      "endOffset" : 102
    } ],
    "year" : 2017,
    "abstractText" : "We show a Talagrand-type of concentration inequality for MTL, using which we establish sharp excess risk bounds for Multi-Task Learning (MTL) in terms of distributionand data-dependent versions of the Local Rademacher Complexity (LRC). We also give a new bound on the LRC for strongly convex hypothesis classes, which applies not only to MTL but also to the standard i.i.d. setting. Combining both results, one can now easily derive fast-rate bounds on the excess risk for many prominent MTL methods, including—as we demonstrate—Schatten-norm, group-norm, and graph-regularized MTL. The derived bounds reflect a relationship akeen to a conservation law of asymptotic convergence rates. This very relationship allows for trading off slower rates w.r.t. the number of tasks for faster rates with respect to the number of available samples per task, when compared to the rates obtained via a traditional, global Rademacher analysis.",
    "creator" : "LaTeX with hyperref package"
  }
}