{
  "name" : "1601.03466.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dynamic Privacy For Distributed Machine Learning Over Network",
    "authors" : [ "Tao Zhang", "Quanyan Zhu" ],
    "emails" : [ "t.z.1992.nyc@gmail.com", "quanyan.zhu@nyu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Privacy-preserving distributed machine learning becomes increasingly important due to the rapid growth of amount of data and the importance of distributed learning. This paper develops algorithms to provide privacy-preserving learning for classification problem using the regularized empirical risk minimization (ERM) objective function in a distributed fashion. We use the definition of differential privacy, developed by Dwork et al. privacy to capture the notion of privacy of our algorithm. We provide two methods. We first propose the dual variable perturbation, which perturbs the dual variable before next intermediate minimization of augmented Lagrange function over the classifier in every ADMM iteration. In the second method, we apply the output perturbation to the primal variable before releasing it to neighboring nodes. We call the second method primal variable perturbation. Under certain conditions on the convexity and differentiability of the loss function and regularizer, our algorithms is proved to provide differential privacy through the entire learning process. We also provide theoretical results for the accuracy of the algorithm, and prove that both algorithms converges in distribution. The theoretical results show that the dual variable perturbation outperforms the primal case. The tradeoff between privacy and accuracy is examined in the numerical experiment. Our experiment shows that both algorithms performs similar in managing the privacy-accuracy tradeoff, and primal variable perturbaiton is slightly better than\nthe dual case."
    }, {
      "heading" : "1. Introduction",
      "text" : "Distributed machine learning has become increasingly important due to the rapid growth of amount of data and the increasing of model complexity. In practice, the amount of training data can range from 1T B to 1PB [2]. With this training data, it is possible to develop complex models with 109 to 1012 parameters [2, 5]. In centralized learning, these training data are shared by all the nodes participating in the learning process via centralized collection, and the parameters are available to all these nodes. In many cases, especially for the statistical learning, all nodes must frequently use the shared data and parameters in order to improve the parameters during the learning process. The centralized learning is not encouraged due to several aspects such as high computational complexity, scalability, and communication overhead, to name a few. As a result, decentralization of the dataset as well as distributed algorithms become more and more important.\nThe main goal of distributed learning is to decentralize the problem to multiple local subproblems. There are many ways to establish the decentralization, and the alternating direction method of multiplier (ADMM) is a well suited algorithm to deal with large scale distributed optimization problems. ADMM algorithm trains the model purely based on the information exchange among the neighboring nodes, rather than the en-\nar X\niv :1\n60 1.\n03 46\n6v 1\n[ cs\n.L G\n] 1\n4 Ja\nn 20\ntire network, and it has been proved that ADMM for convex optimization problem is convergent to the centralized problem under some specific conditions [6].\nMany benefits have raised in the field of distributed machine learning. Google, eBay, Linkedin and Apple were among the corporations to take advantage of the massive data collected from their customers or users. They use technology like machine learning to improve decision making, reducing cost, provide new products and services. The benefits of distributed machine learning are undeniable, but it also presents serious privacy issues; there are possible internal and external attacks to the training data, which are stored in digital databases, such as social network data, web search histories, financial information, and medical records.\nThe general ADMM-based distributed learning has a certain level of privacy by avoiding the centralized collection of training datasets. Indeed, the decentralization has avoided the direct sharing of local dataset that contains sensitive information. However, deleting or anonymizing the sensitive information from the training dataset may reduce the accuracy of the learning model; even if the accuracy is not affected, some sensitive information can be still re-identified from the remaining information. These kinds of attacks have been studied in many works; for example, the adversary can use some background knowledge and cross correlation with other databeses to extract the private information [32, 30]. Other examples such as when the dataset has certain structural features an attacker is able to learn from the private model. These attackers can be from the outside as well as inside of the learing network.\nIn this paper, we focus on the ADMM-based distributed machine learning on the problem of classification. We use the empirical risk minimization (ERM) to construct the objective function of the problem. The ERM method use the dataset to construct an approximation of the expected risk , which is usually referred to the empirical risk. The classifier is chosen by minimizing the empirical risk. In this paper, we regularize the ERM with an additional term, the regularizer, in the empricial loss function to avoid overfitting,\nwhich means that although the minimum of the empirical risk can be close to zero, the expected risk we are interested in can be very large.\nOur goal is to develop an learning algorithm that can preserve the privacy of training data in every local node from both the internal and the external attackers during the entire learning process. Specifically, we develop randomized algorithms that can provide privacy in terms of αdifferential privacy [4, 9] while keeping the learning procedure accurate. Our algorithms hold for loss functions and regularizers that satisfy specific conditions of convexity and differentiability. For training, we propose two privacy preserving estimates of the regularized ERM-beased optimization. The first is primal variable perturbation; this is based on the output perturbation developed by Dwork et al. [4], which adds noise to the output of the non-private regularized ERM algorithm. In our method, we add noise to the intermediate updated primal variable of each node of ADMMbased distributed algorithm before sharing this primal variable to neighboring nodes. We call the second case dual variable perturbation, in which we perturb the dual variable of every node at each ADMM iteration before next iteration.\nOur results are applicable to general ERM optimization problems, and we use numerical experiments to the classification problem based on logistic regression. Differential privacy model aims to ensure that even if the adversary has knowledge of all the dataset except one data point, the adversary should not be able to distinguish whether an individual datapoint is present or absent, by adding randomness to the output of the algorithm; thus, the differential privacy not only aims to protect the specific data points present in the dataset but also all the possible datapoints for that dataset. Since there are no conditions ofr the dataset for the purpose of privacy preservation, the randomness incurs a cost in the performance while guaranteeing the differential privacy. Therefore, managing the tradeoff between privacy and accuracy is critical. Under the assumption that the data points in the dataset are drawn from an unknown but fixed distribution, we prove the accuracy of the distributed learning algorithm in terms of the privacy parameters. Another impor-\ntant issue is the convergence of ADMM. There are many convergences results for ADMM discussed in literature. Based on the accuracy analysis, we also discuss the convergence of our private ADMM method.\nThe contributions of this paper are shown as follows:\n• We derive a method, dual variable perturbation, in which we add randomness to the dual variable before the next update of the primal variable. The differential privacy is guaranteed for every ADMM iteration as well as the final trained output. • Based on the output perturbation developed by Dwork et al. [4], we develop a private ADMM-based distributed algorithm for regulatized ERM, which applies primal variable perturbation. In this technique, the randomness rises when every node transmits the primal parameter to the corresponding neighboring nodes. It is guaranteed to provide differential privacy for the every intermediate update. For the final update, we apply the dual variable perturbation in order to increase the accuracy. • We provide the theoretical guarantees of accuracy of both algorithms with L2 regularization. Based on the accuracy analysis, we also show that both algorithms are convergent in distribution with different probability densities. • We implement our methods by experiments on a dateset of UCI Machine Learning Repositories [16]. We provide a method to select the optimal privacy parameter α by solving an optimization problem given a specific utility function of privacy. The test results show that both the algorithm performs similarly, but the primal variable perturbation slightly outperforms the dual variable perturbation. However, theoretical analysis shows that dual variable perturbation has higher probability of accuracy and better sample requirement than does\nthe primal case. Both algorithms are suitable for the both types of attacks we are interested in."
    }, {
      "heading" : "1.1. Related Work",
      "text" : "There has been a significant amount of literature on the distributed classification learning algorithms. These works mainly focus on either enhancing the efficiency of the learning model, or on producing a global classifier from multiple distributed local classifier trained at the corresponding individual node. In the first kind of these works, researchers focus on making the distributed algorithm suitable to datasets of very large size; some ([14]) use MapReduce to explore the performance improvements. The second kind of works includes methods such as ADMM methods ([1]) , parameter averaging ([10]) voting classifiction ([13]), mixing parameters ([7]). Our distributed algorithm is based on ADMM, in which the centralized problem acts as a group of coupled distributed convex optimization subproblems with the consensus constaints on the primal parameters.\nResearch on privacy has been studied in a significant number of works since at least [20]. Recent literature on privacy includes anonymization [22], privacy-preserving data mining [19,20,21], cryptographic approaches [33, 35]. Simple anonymization approaches are ineffective. Individual information can be re-identified by simply using a small amount of side information [32,16]. In privacy-preserving data mining research, the privacy can be pried through, for example, composition attacks, in which case the adversary have some prior knowledge. Other works on data perturbation for privacy (for instance [25, 26]) focus on additive or multiplicative perturbation of individual samples, which might affect certain relationships among different samples in the database.\nThe idea of increasing privacy by adding noise has been studied for decades (for example, [49]; and see [48] for more details). The main perturbation techniques can be summarized into two basic classes. One is input perturbation, in which the training datasets are randomly modified prior to\nlearning. The other one is output perturbation, where the exact solution is obtained from the true datasets but the noisy randomized version of the solution is released. There exist some inherent limitations for these two methods. Since Agrawal and Srikant’s work in [50], increasing number of work studies the limitations and applicability of noise perturbation, and the definitions of privacy started to expand. In Dwork et al.’s basic definition of privacy [4], ε-indistinguishability or differential privacy, a change in a single entry of the dataset incurs a small change in the distribution from the view of any adversary via a specific measure of distance in a worst-case scenario.\nDifferential privacy has been used in a large number of works in privacy research (for example, [4, 11, 27, 28, 29]) since it was first proposed by Dwork et al [4]. Differential privacy is immune to the composition attacks mentioned above [30]. Later works include differential-private contingency tables [10], and differential-private combinatorial optimization [8]. Moreover, Wasserman and Zhou study the differential privacy more statistically. A body of exist literature also studies the differential-private machine learning. For example, Kasiviswanathan et al. derives a general method for probabilistically approximately correct (PAC, [47]) in [46]. Other examples includes the work of Blum et al. in [9] that provides a method to deliver the dataset differentially privately. Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]). There are two main approaches used in differential privacy: perturbation by Laplace noise, and other exponential mechanism. In this paper, we focus on the Laplace noise perturbation. Laplace noise perturbation, especially the Laplace noise addition, is the primary method for the differential privacy.\nThe rest of the paper is organized as follows. Section 2 outlines the centralized ERM objective function and then shows the equivalent distributed form; the corresponding privacy concerns are described. In Section 3, algorithms are produced, and the analysis of privacy guarantee is provided. Section 4 discusses the tradeoff between privacy and accuracy, and the convergence of the algo-\nrithms. Finally, Section 5 and 6 present numerical experiments and concluding remarks."
    }, {
      "heading" : "2. Problem Statement",
      "text" : "Consider a connected network shown in Figure 1, which contains P nodes described by one undirected graph G(P,E ) with the set of nodes P = {1,2,3, ...,P}, and edges E represened by lines denoting the linkes between connected nodes. A particular node p ∈P only exchanges information between its neighboring node j ∈Np, where j ∈Np is the set of all neighboring nodes of node p, and Np = |Np| is the number of neighboring nodes of node p. The network is connected but not necessary fully connected; there can be local cycles (e.g. local central node p and i, j,k ∈Np). In a connected network, there must exist a path i1, i2, i3, ..., im−1, im of length at least 1 connecting node i1 and im. Each node p contains a dataset Dp defined as follows:\nDp = {(xip,yip)⊂ X×Y : i = 0,1, ...,Bp},\nof size Bp with data vector xip ∈ X ⊆Rd , and the corresponding label yip ∈Y = {−1,1}. The entire network therefore has a set of data as:\nD̂ = ⋃ p∈P Dp.\nThe target of the centralized classification algorithm is to find a classifier f : X → Y using all available data D̂ that enables each node in the network to classify any data x′ input to a label y′ ∈ {−1,1}.\nSuppose that D̂ is available to the fusion center node, then we can choose the global classifier f : X → Y that minimizes the following centralized regularized emprical risk minimization problem (CR-ERM)\nmin f\nZC( f |D̂) := CR\nBp\nP\n∑ p=1\nBp ∑ i=1 L̂ (yip, f T xip)+ρR( f ),\n(1) where CR ≤ Bp is a regularization parameter, and ρ > 0 is the parameter that controls the impact of the regularizer. The loss function L̂ (yip, f T xip) : Rd → R, is used to measure the quality of the classifier trained. In this paper, we focus on the specific loss function:\nL̂ (yip, f T xip) = L (yip f T xip)\nThe function R( f ) is a regularizer that prevent overfitting. We aim to solve the centralized optimization problem (1) in a distributed fashion while achieving the same performance as in the centralized case. The decentralized equivalent enables node p to contribute by optimizing only the p-dependent terms of the objective function without exchanging any training data to other nodes p′ 6= p. In this paper, we have the following assumptions\nAssumption 1. - The loss function L is strictly convex and doubly differentiable of f with |L ′| ≤ 1 and |L ′′| ≤ c1, where c1 is a constant. Both L and L ′ are continuous.\nAssumption 2. - The regularizer function R(·) is continuous differentiable and 1-strongly convex. Both R(·) and ∇R(·) are continuous.\nAssumption 3. - We assume that ‖xip‖≤ 1. Since yip ∈ {−1,1}, then |yip|= 1."
    }, {
      "heading" : "2.1. Distributed ERM",
      "text" : "To solve (1) in a distributed way, we first reform the objective function. The global variable f in CR-ERM is coupling the problem over the network. To decouple, we replace f by P copies of f ; thus the global variable becomes auxiliary pernode variables { fp}Pp=1. Consensus constraints are required to force necessary global consistency\ncondition f1 = f2 = ... = fP since the network is connected. Let ZD denote ZD({ fp}p∈P |D̂) be the decentralized objective function. An equivalent distributed form of the CR-ERM is\nmin { fp}Pp=1\nZD := CR\nBp\nP\n∑ p=1\nBp ∑ i=1 L (yip f Tp xip)+ P ∑ p=1 ρR( fp).\ns.t. fp = f j, p = 1, ...,P, j ∈Np. (2)\nNow the problem (2) can be solved distributively by using the alternative direction method of multiplier (ADMM).\nAccording to Lemma1 in [1], if { fp}Pp=1 represnet a feasible solution of (2) and the network is connected, then problems (1) and (2) are equivalent, that is, f = fp, p = 1, ...,P, where f is a feasible solution of (1).\nIn order to solve (2) by ADMM, we use the redundant variables {w jp} to assist to decouple fp of node p from its neighors j ∈Np. Thus the distrubted regularized emprical risk minimization problem (DR-ERM) becomes\nmin { fp}Pp=1 ZD.\ns.t. fp = wp j,wp j = f j, p = 1, ...,P, j ∈Np (3)\nThen the node-p-based individual objective function of (3) is\nZp( fp|Dp) := CR\nBp\nBp ∑ i=1 L (yip f Tp xip)+ρR( fp).\nThe augmented Lagrange funciton associated with the distributed optimization problem is:\nLD({ fp},{wp j},{λ kp j}) =ZD + P\n∑ p=1 ∑ i∈Np\n( λ api )T ( fp−wpi)\n+ P\n∑ p=1 ∑ i∈Np\n( λ bpi )T (wpi− fi)\n+ η 2\nP\n∑ p=1 ∑ i∈Np (‖ fp−wpi ‖2\n+ ‖ wpi− fi ‖2). (4)\nThe distributed iterations solving (3) are:\n{ fp(t+1)}Pp=1 = arg min { fp}Pp=1\nLD ( { fp},{wp j(t)},{λ kp j(t)} ) ,\n(5)\n{wp j(t +1)}Pp=1 = arg min {wp j}Pp=1\nLD ( { fp(t +1)},{wp j}, {λ kp j(t)} ) ,\n(6)\nλ ap j(t +1) = λ a p j(t)+η( fp(t +1)−wp j(t +1)),\np ∈P, j ∈Np, (7)\nλ bp j(t +1) = λ b p j(t)+η(wp j(t +1)− fp(t +1))′\np ∈P, j ∈Np. (8)\nThe general ADMM convergence is shown in Appendix A. Since the iterations (5)-(8) are proved to have the general form of ADMM iterations (see Appendix I), then the convergence of the decentralized regularized ERM is guaranteed.\nFrom (4), the augmented Lagrange function is linear-quadratic in wpi; thus, there is a closed form of wpi(t +1) at each iteration. Then we can replace wpi terms in (5), (7), (8) by its closed expression. Moreover, by initializing the dual variables λ kp j = 0d×d , and let λp(t) = ∑ j∈Np λ k p j, p∈P , j ∈Np, k = a, b, we then can combine (7) and (8) into one update. As a result, the update procedures (5) to (8) can be further simplified through replacing wpi by its corresponding closed form in (4). The simplified ADMM iteration is shown as follows, due to Lemma 3 of [1].\nLet LN(t) denotes LN({ fp},{ fp(t)},{λp(t)}), and\nLN(t) =ZD +2 P\n∑ p=1 λp(t)T fp\n+η P\n∑ p=1 ∑ i∈Np ‖ fp−\n1 2 ( fp(t)+ fi(t)) ‖2 .\nThe ADMM iterations (5)-(8) can be reduced to\n{ fp(t+1)}Pp=1 = arg min { fp}Pp=1 LN({ fp},{ fp(t)},{λp(t)}),\n(9)\nλp(t +1) = λp(t)+ η 2 ∑j∈Np [ fp(t +1)− f j(t +1)].\n(10) We denote the node-p-based nonprivate augmented Lagrange function\nLN p({ fp},{ fp(t)},{λp(t)}) as LN p(t):\nLN p(t) = CR\nBp\nBp ∑ i=1 L (yip f Tp xip)+ρR( fp)+2λp(t) T fp\n+η ∑ i∈Np ‖ fp−\n1 2 ( fp(t)+ fi(t)) ‖2 .\n(11) Thus, every node p updates fp(t + 1) at each iteration as follows\nfp(t +1) = argmin fp LN p(t).\nAlgorithm 1 Distributed ERM Required:Randomly initialize fp,λp = 0d×1 for every p Inputs:D̂ 1: for t = 0,1,2,3, ... Do 2: for p = 1,2,3, ...P Do 3: Compute fp(t +1) via (9). 4: end for 5: for p = 1,2,3, ...P Do 6: Broadcast fp(t +1) to all neighbors j ∈Np 7: end for 8: for p = 1,2,3, ...P Do 9: Compute λp(t +1) via (10) 10: end for 11: end for Outputs: f ∗\nADMM-based distributed ERM iterations (9) to (10) is illustrated in Figure 2 and summarized in Algorithm 1. Every node p ∈P updates its local d×1 estimates fp(t) and λp(t). At iteration t +1, node p updates the local fp(t +1) through\n(9). Next, node p broadcasts the latest fp(t+1) to all its neighboring nodes j ∈Np. Iteration t + 1 finishes as each node updates the λp(t + 1) via (10).\nEvery iteration of our algorithm is still a minimization problem similar to the centralized problem (1). However, the number of variables participating in solving (9) per node per iteration, which is Np, is much smaller than that in the centralized problem, which is ∑Pp=1 Np. There are several methods to solve (9). For instance, projected gradient method, Newton method, and Broyden–Fletcher–Goldfarb–Shanno (BFGS) method that approximates the Newton method, to name a few.\nADMM based distributed machine learning has benefits due to high scalability, economic communication, and a certain level of privacy. The privacy arised is mainly due to the local parameter exchange among neighboring nodes instead of centralized communication. Dually, the parameter of each node is anonymous to the non-neighboring nodes. However, the neighboring nodes can access to the parameter without privacy protection; also, as shown in Section 1, simple anonymization is not good enough because it is still possible for adversary to extract the sensitive information with side information about the target."
    }, {
      "heading" : "2.2. Privacy Concerns",
      "text" : "Although the data stored at each node is not exchanged during the entire ADMM algorithm, the potential privacy risk still exists. Suppose the dataset Dp stored at node p contains sensitive information in data point (xi,yi) that is not allowed to be released to other nodes in the network or anyone else outside.\nIn the distributed version of algorithm, node p optimizes only the p-dependent parts of the centralized problem. Let K : Rd → R be the randomized version of Algorithm 1, and let { f ∗p}p∈P be the output of K at all the nodes. Then { f ∗p}p∈P is random. Let Ktp be the node-p-dependent subalgorithm of K at iteration t, and let fp(t) be the output of Ktp(Dp) at iteration t inputing Dp. fp(t) is random at each t.\nConsider an adversary, who knows all the data about node p except for the (xip,yip). The adversary is able to extract much additional information about (xip,yip) by observing the output of the algorithm. For the adversary inside the network, the sensitive information can even be leaked at any iteration of the training process. Therefore, it is necessary to develop a privacy preserved distributed ADMM algorithm for classification problem. We consider two types of attacks: • Type 1: This attack is from adversaries\noutside the network, who do not have access to the intermediate ADMM iteration. The attack observes the output { f ∗p}p∈P of algorithm K and aims to extract additional information of the private data point of the training dataset.\n• Type 2: This attack is from the adversaries that can get access to the intermediate ADMM iterations. This attack aims to obtain additional information about the private data point of the the training dataset by observing the intermediate output fp(t) of Ktp for all p ∈P .\nWe denote our privacy of distributed network based on the definition of differential privacy in [4]. Specifically, we require that a change of any single data point in the dataset might only change the distribution of the output of the algorithm slightly, which is visible to the adversary; this is done by adding randomness to the output of the algorithm. Let Dp and D′p be two datasets differing in one data points; i.e., let (xip,yip)⊂Dp, and (x′ip,y ′ ip)⊂D′p, then (xip,yip) 6= (x′ip,y′ip). In other words, their Hamming Distance\nHd(Dp,D′p) = Bp\n∑ i=0 1{i : xi 6= x′i} (12)\nequals 1: i.e. Hd(Dp,D′p) = 1.\nDefinition 1. (Networked αp-Differential Privacy) Consider a network consisits of P nodes P = 1, 2, ...P, and each node p has a training dataset Dp, and D̂ = ⋃ p∈P Dp. Let K : Rd→R be a randomized version of Algorithm 1. K outputs { f ∗p}p∈P , where f ∗p =K(Dp) is the corresponding output at node p. Let D′p be any dataset with\nHd(D′p,Dp) = 1, and let g ∗ p = K(D ′ p). Then, K is networked αp-differential private, if for any datasets D′p for all p ∈P , known by the adversary of Type 1 attack, and for all possible sets of the outcomes S ⊆ R, the following inequality holds:\nPr[ f ∗p ∈ S]≤ eαp ·Pr[g∗p ∈ S]. (13)\nThe probability is taken over f ∗p the output of K(·) at each node p ∈P . The privacy raised is called networked αp-differential Privacy.\nDefinition 1 specifies the privacy required against Type 1 attack. More specifically, networked αp-differential private algorithms can prevent adversaries from obtaining much additional information by simply observing the output of the algorithm. This is because that no matter how the adversaries adjust the dataset D′p (Hd(D ′ p,Dp) = 1), the distribution of output can only change slightly.\nFor the privacy preserved against Type 2 attack, we have the following definition.\nDefinition 2. (Dynamic α tp-Differential Privacy) Consider a network consisits of P nodes P = 1, 2, ...P, and each node p has a training dataset Dp, and D̂ = ⋃ p∈P Dp. Let K : Rd → R be a randomized version of Algorithm 1. Let Ktp be the node-p-dependent sub-algorithm of K, optimizating ADMM iteration at t and outputing fp(t). Let D′p be any dataset with Hd(D ′ p,Dp) = 1, and let gp(t) = Ktp(D ′ p). We say that the algorithm K is dynamic α tp-differential private if for any dataset D′p for all p∈P known by the adversary of Type 2 attack, and for all possible sets of the outcomes S⊆R, the following inequality holds:\nPr[ fp(t) ∈ S]≤ eα t p ·Pr[gp(t) ∈ S], (14)\nfor all time t during a learning process. The probability is taken over fp(t), the output of Ktp. The privacy raised for algorithm K is called dynamic α tp-differential Privacy.\nDefinition 2 provides the privacy against Type 2 attack. Dually, in dynamic α tp-differential private algorithms, adversaries of Type 2 attack cannot extract much additional information by observing the intermediate updates of fp(t). This is because\nthat inputing any D′p with Hd(D ′ p,Dp) = 1 to the algorithm, the distribution of the output will not change much if any one data point is changed in D′p.\nClearly, the algorithm with ADMM iterations shown in (9) to (11) is neither networked αpdifferential private nor dynamic α tp-differential private. This is because the intermedate and final optimal output fp’s are deterministic given dataset Dp. For D′p with Hd(Dp,D ′ p) = 1, the classifier will change completely, and the probability density Pr([ fp|D′p]) = 0, which leads to the ratio of probabilities Pr[ fp|Dp]Pr[ fp|D′p] → ∞.\nIn order to provide the differential privacies defnined in Definition 1 and 2, we propose two algorithms, dual variable perturbation and primal variable perturbation, which are described in Section 3.1 and 3.2, respectively. Both algorithms can provide the two types of differential privacy defined in Section 2.2. However, we modify the primal variable perturbation by replacing the last ADMM iteration with dual variable perturbation in order to improve the accruacy of the final outputed classifier."
    }, {
      "heading" : "3. Dynamic Private Preserving",
      "text" : "In this section, we describe two algorithms that provide networked and dynamic α-differential privacy defined in Section 2.2, respectively."
    }, {
      "heading" : "3.1. Dual Variable Perturbation",
      "text" : "In order to provide differential privacy defined in Definition 1 and 2, we introduce our first private algorithm, dual variable perturbation, in which we perturb the dual variable {λp(t)}Pp=1 with a random noise vector εp(t), which has the probability density function:\nK (ε)∼ e−ζp(t)‖ε‖, (15)\nwhere ζp(t) is a parameter related to the value of αp(t). Let µp(t) = λp(t)+ εp(t) be the perturbed dual variable. Now the corresponding nodep-based augmented Lagrange function LN p(t) becomes Ldual ( fp, fp(t),µp(t + 1),{ fi(t)}i∈Np ) .\nUse Ldual(t) to denote Ldual (\nfp, fp(t),µp(t + 1),{ fi(t)}i∈Np ) , and we have\nLdual(t) = CR\nBp\nBp ∑ i=1 L (yip f Tp xip)+ρR( fp)\n+2µp(t +1)T fp + Φ 2 ‖ fp ‖2\n+η ∑ i∈Np ‖ fp−\n1 2 ( fp(t)+ fi(t)) ‖2,\n(16) where Φ2 ‖ fp ‖\n2 is an additional penalizer. As a result, the minimization of Ldual(t) becomes random. We slightly change the iterations (9) to (10) as follows:\nµp(t +1) = λp(t)+ CR\n2Bp εp(t +1), (17)\nfp(t +1) = argmin fp Ldual(t), (18)\nλp(t +1) = λp(t)+ η 2 ∑j∈Np [ fp(t +1)− f j(t +1)].\n(19) We perturb the dual variable λp(t) via an additional variable µp in (19). This is because the dual variable is not exchanged during the training and is only used within the corresponding node; thus the direct perturbation to λp will affect the accuracy by the accumulated noise and is not necessary. We have the following theorem.\nTheorem 1. Let α̂ = αp(t) − ln ( 1 +\nc1 Bp CR ( ρ+2ηNp ))2. If α̂ > 0, then Φ = 0; else, let Φ = c1Bp\nCR (eαp(t)/4−1)\n−ρ−2ηNp, and as a result\nα̂ = αp(t)/2. Under Assumption 1, 2 and 3, if the distributed classification optimization problem with objective function (2) can be solved by Algorithm 2, then the algorithm A1 solving this distributed problem is dynamic α-differential private with αp(t) for each node p ∈P at time\nt. The ratio of conditional probabilities of fp(t) is bounded as:\nQ( fp(t)|D) Q( fp(t)|D′p) ≤ eαp(t), (20)\nwhere Q( fp(t)|D) and Q( fp(t)|D′p) are the probability density functions of fp(t) given dataset D and D′p, respectively, and Hd(D,D ′ p) = 1.\nProof: See Appendix B\nAlgorithm 2 Dual Variable Perturbation Required:Randomly initialize fp,λp = 0d×1 for every p Inputs:D̂,{[αp(1),αp(2), ...]}Pp=1 1: for t = 0,1,2,3,... Do 2: for p = 1,2,3,...P Do 3: Let α̂ = αp(t)− ln ( 1+ c1Bp\nCR\n( ρ+2ηNp ))2. 4: If α̂ > 0, then Φ = 0, else, Φ =\nc1 Bp CR (eαp(t)/4−1) −\nρ−2ηNp and α̂ = αp(t)/2. 5: Draw noise εp(t) according to (15) with ζp(t) = α̂ 6: Compute µp(t +1) via (17) 7: Compute fp(t +1) via (15) 8: with augmented Lagrange function as (16). 9: end for 10: for p = 1,2,3,...P Do 11: Broadcast fp(t+1) to all neighbors j ∈Np 12: end for 13: for p = 1,2,3,...P Do 14: Compute λp(t +1) via (16) 15: end for 16: end for Outputs: { f ∗p}Pp=1\nThe algorithm corresponding to Theorem 1 is illustrated in Figure 3 (a) and (c), and summarized in Algorithm 2. All nodes have its corresponding value of ρ . Every node p ∈P updates its local estimates µp(t), fp(t) and λp(t) at time t; at time t+1, node p first perturbs the dual variable λp(t) obtained at time t to get µp(t + 1) via (17), and then uses training dataset Dp to compute fp(t+1) via (18). Next, node p distributes fp(t + 1) to all its neighboring nodes. The (t + 1)-th update finishes when each node has updated its local\nλp(t + 1) via (19). The final iteration is exactly the same as the intermeidate iterations.\nTheorem 1 links Algorithm 1 with Definition 2. We observe that an algorithm satisfies Definition 2 also satisfies Definition 1 for any individual node p∈P . Therefore, any distributed algorithm that is dynamic α-differential private is also networked α-differential private. Thus, we have the following corollary.\nCorollary 1.1. If each node in the network chooses the same privacy parameter αp(t) = α∗(t) for all p ∈P at each time, then the algorithm meets Theorem 1 also provide networked α-differential privacy with α = α∗(t).\nCorollary 1.1 can be proved by directly substituting αp(t) = α∗(t) for all p ∈P .\nThe definitions of differential privacy in Section 2 (and also in, for example, [4, 11, 12]) only consider the change of output distribution corresponding to a change of a single entry of the dataset. However, in many cases, the sensitive information may be contained in more than one data point. Actually, Theorem 1 can be extended to deal with the dataset that has multiple sensitive data entries.\nCorollary 1.2. Let D and D′p be two datasets with Hd(D,D′p) = c2, c2 ≥ 1. Then an algorithm meets Theorem 1 can compute a fp(t) that has the following bounded ratio of condtional densities:\nQ( fp(t)|D) Q( fp(t)|D′p) ≤ ec2α ′p(t), (21)\nProof: See Appendix C.\nHowever, the output distribution must change corresponging to the a change of multiple data entries; as a result, the level of privacy has to decrease, especially for large c2 and large BP."
    }, {
      "heading" : "3.2. Primal Variable Perturbation",
      "text" : "In this case, we perturb the primal variable { fp(t + 1)}Pp=0 before releasing this variable to the neighboring nodes of each local node. This algorithm can also provide differential privacy defined in Definition 1 and 2. Let\nthe node-p-based augmented Lagrange function Lprim ( fp, fp(t),εp(t),λp(t),{Vi(t)}i∈Np ) be represented as Lprim(t):\nLprim(t) = CR\nBp\nBp ∑ i=1 L (yip f Tp xip)+ρR( fp)+2λp(t) T fp\n+η ∑ i∈Np ‖ fp−\n1 2 ( fp(t)+Vi(t)− εp(t)) ‖2 .\nWe use the un-perturbed primal fp(t) obtained at time t in the augmented Lagrange function and subtract the noise vector εp(t) added at time t in order to reduce the noise in the minimization in (22); εp(t) is static at time t +1. The privacy of releasing primal variable is not affected.\nThe following iterations specify the corresponding ADMM iterations.\nThe distributed iteration provideing dynamic αp(t)-differential privacy at time t is\nfp(t +1) = argmin fp Lprim(t), (22)\nVp(t +1) = fp(t +1)+ εp(t +1), (23)\nλp(t +1) = λp(t)+ η 2 ∑j∈Np [Vp(t +1)−Vj(t +1)], (24) where εp(t + 1) is the random noise vector with the density function (15). The aurgmented Lagrange function is (11). When the ADMM iteration meets the stop time, we input D̂, and the latest { fp(t)}p and {λp(t)}p obtained from (22) and (24), respectively, to Algorithm 1 to iterate (17) to (19) one time.\nThe following theorem states the result of primal variable perturbation.\nTheorem 2. Under Assumption 1, 2 and 3, if the distributed classification optimization problem with objective function (2) can be solved by Algorithm 3 with ζp(t) = ρBpαp(t) 2 , then the algorithm A2 solving this distributed problem is dynamic α-differential private with αp(t) for each node p ∈P at time t. The ratio of conditional probabilities of fp(t) is bounded as in (20).\nProof: See Appendix D.\nCorollary 1.1 and 1.2 also hold for Theorem 2.\nCorollary 2.1. If all the nodes have the same privacy parameter α∗(t) at each time, then the algorithm meets Theorem 2 also provide networked α-differential privacy with α = α∗(t).\nSimilar to Corollary 1.1, Corollary 2.1 can be proved by substituting α = α∗(t) for all p ∈P .\nCorollary 2.2. Let D and D′p be two datasets with Hd(D,D′p) = c2, c2 ≥ 1. Any algorithm satisfies Theorem 2 can produce a private fp(t), which has the following bounded ratio of condtional densities at each iteration:\nQ( fp(t)|D) Q( fp(t)|D′p) ≤ ec2α ′p(t). (25)\nThe proof of Corollary 2.2 is the same as that of Corollary 1.2 in Appendix C.\nAlgorithm 3 Primal Variable Perturbation Required:Randomly initialize fp,λp = 0d×1 for every p Inputs:D̂,{[αp(1),αp(2), ...]}Pp=1 1: for t = 0,1,2,3,... Do 2: for p = 1,2,3,...P Do 3: Draw noise εp(t) according to (15) with\nζp(t) = ρBpαp(t)\n2CR 4: Compute fp(t +1) via (22) 5: with augmented Lagrange function as (11). 6: Compute Vp(t +1) via (23) 7: end for 8: for p = 1,2,3,...P Do 9: Broadcast Vp(t +1) to all neighbors j ∈Np 10: end for 11: for p = 1,2,3,...P Do 12: Compute λp(t +1) via (16) 13: end for 14: if t = stop time 15 Input D̂, and the latest { fp(t)}p and {λp(t)}p obtained in above Step 4 and 12, respectively, to Algorithm 1 to\niterate Step 1 once. 16: end for Outputs: { f ∗p}Pp=1\nThe algorithm associated with Theorem 2 is illustrated in Figure 3 (b)-(c), and is summarized in Algorithm 3. Each node p ∈P updates fp(t),\nVp(t) and λp(t) at time t. Then, at time t + 1, training dataset is used to compute fp(t + 1) via (22), which is then perturbed to obtain Vp(t +1) via (23). Next, Vp(t + 1) is distributed to all the neighboring nodes of node p. Finally, λp(t+1) is updated via (24). The final iteration follows the dual variable perturbation."
    }, {
      "heading" : "4. Accuracy and Convergence Analysis",
      "text" : "In this section, we discuss the accuracy of Algorithm 1 and 2. We establish performance bounds for regularization functions with L2 norm. Our analysis is based on the following assumptions:\nAssumption 4. - The data points {(xpi,ypi)} Bp i=1 are drawn i.i.d. from a fixed but unknown probability distribution Pxy(xpi,ypi).\nAssumption 5. - εp(t) is drawn from (15) with the same αp(t) = α(t) for all p ∈P .\nWe define the expected loss as\nĈ( fp) :=CRE(x,y)∼Pxy(L (y f T x)).\nLet Ẑ be the expected objective as\nẐ( fp) := Ĉ( fp)+ρR( fp).\nWe also defined the constrained objectives for perturbed ADMM-based algorithms. Let ε pi(t) = εp(t)− εi(t), for i ∈ Np. Specifically, at each iteration t, we define:\nZdual( fp, t|Dp) := Zp( fp|Dp)+ CR\nBp εp(t)T fp,\nZprim( fp, t|Dp) := Zp( fp|Dp)\n−η ∑ i∈Np\n( ( fp−\n1 2 ( fp(t)+ fi(t))T\n· (ε pi(t))+ 1 4 ( ε pi(t) )2) .\nLet ε tp = εp(t), the noise vector generated at time t. The objective Zdual( fp, t|Dp) (respectively, Zprim( fp, t|Dp)) is the corresponding nodep based objective function for the Algorithm 1 (respectively, Algorithm 2) if we fix the noise as ε tp generated at time t for Ldual( fp, t|Dp) throughout the entire ADMM process.\nLet f̂p(t + 1), f nonp (t + 1) and f ∗ p(t + 1) be the population optimum, (non-private) empirical optimum, and private (empirical) optimum, respectively, defined at iteration t +1 as:\nf̂p(t +1) = argmin fp Ẑ( fp),\nf nonp (t +1) = argminfp Zp( fp, t|Dp),\nf ∗p(t +1) = argminfp Z( fp, t|Dp),\nwhere Z represents Zdual or Zprim, respectively. Let Fp(t + 1) = argmin fp LnonP( fp, t|Dp)be the updated non-private classifier at iteration t + 1. From Theorem 9 (see Appendix A), the sequence {Fp(t +1)} is bounded and converges to an optimal value f nonp (t + 1) as time t → ∞. Thus,there exists a constant ∆non(t) such that:\nĈ(Fp(t))−Ĉ( f non(t))≤ ∆non(t).\nLet fp(t+1) be the minimizer of the corresponding augmented Lagrange function of Zpriv at time t. Since both Zdual( fp, t|Dp) and Zprim( fp, t|Dp) are real and convex; similarly, the sequence { fp(t)} is bounded and fp(t) converges to f ∗p(t), which is a limit point of fp(t), and there exists a constant ∆privp (t)=∆dualp (t) or ∆ prim p (t) given noise vector εp(t) such that\nĈ( fp(t))−Ĉ( f ∗p(t))≤ ∆privp (t).\nWe will show that the performance of the algorithm can depend on the number of data points, Bp, of the dataset Dp, for all p∈P . Let f 0p(t) be a reference classifier at time t with the expected loss as Ĉ∗ = Ĉ( f 0p(t)). Specifically, the performance of the algorithm is measured by the Bp, which is a function of ‖ f 0p(t) ‖ required to obtain a classifier fp(t) that minimizes the expected loss within some accuracy:\nĈ( fp(t))≤ Ĉ∗(t)+αacc +∆privp (t).\nwhere αacc is the optimization accuracy. We say that every learned fp(t) is αacc-optimal if it satisfies the above inequality. First, we provide the theorem about the performance of the non-private ADMM-based algorithm.\nTheorem 3. Let R( fp(t))= 12 ‖ fp(t) ‖ 2, and f 0p(t) such that Ĉ( f 0p(t)) = C ∗ E(t) for all p ∈ P at\ntime t, and a real number δ > 0. Let Fp(t + 1) = argmin fp LnonP( fp, t|Dp) be the output of Algorithm 1. If Assumption 1 and 4 are satisfied, then there exists a constant βnon such that if the number of data points, Bp in Dp = { (xip,yip) ⊂\nR d×{−1,1} } satisfy:\nBp > βnon max ({CR ‖ f 0p(t +1) ‖2 ln( 1δ ) α2acc } t=1 ) ,\nthen Fp(t +1) satisfies: P ( Ĉ(Fp(t+1))≤ Ĉ∗(t+1)+αacc+∆non(t) ) ≥ 1−δ .\nProof: See Appendix E.\nWe now establish the performance bounds for Algorithm 1, dual variable perturbation, which is summarized in the following theorem.\nTheorem 4. Let R( fp(t))= 12 ‖ fp(t) ‖ 2, and f 0p(t) such that Ĉ( f 0p(t)) =C ∗ E(t) for all p ∈P , and a real number δ > 0. If Assumption 1, 4 and 5 are satisfied, then there exists a constant βdual such that if the number of data points, Bp in Dp ={ (xip,yip)⊂Rd×{−1,1} } satisfy:\nBp > βdual max ({‖ f 0p(t +1) ‖ d ln( dδ ) αaccαp(t) } t=1 ,\n{CRc1 ‖ f 0p(t +1) ‖2 αaccαp(t) } t=1 ,\n{CR ‖ f 0p(t +1) ‖2 ln( 1δ ) α2acc } t=1 ) ,\nthen f ∗p(t +1) satisfies:\nP ( Ĉ( f ∗p(t +1))≤ Ĉ∗(t +1)+αacc ) ≥ 1−2δ .\nProof: See Appendix F.\nCorollary 4.1. Let fp(t + 1) = argminLdual( fp, t|Dp) be the intermediate updated classifier of Algorithm 2 and let f 0p(t) be a reference classifier such that Ĉ( f 0p(t) = Ĉ\n∗(t). If all the conditions of Theorem 3 are satisfied, then, fp(t +1) satisfies\nP ( Ĉ( fp(t+1))≤ Ĉ∗(t)+αacc+∆dualp (t) ) ≥ 1−2δ .\nProof: The following holds for fp(t) and f ∗p(t)\nĈ( fp(t))−Ĉ( f ∗p(t))≤ ∆dualp (t).\nFrom Theorem 3, P ( Ĉ( f ∗p(t +1))≤ Ĉ∗(t +1)+αacc ) ≥ 1−2δ .\nTherefore, we can have: P ( Ĉ( fp(t +1))≤ Ĉ∗(t)+αacc +∆dualp ) ≥ 1−2δ .\nTheorem 4 and Corollary 4.1 can guarantee the privacy defined in both Definition 1 and 2. The following theorem is used to analyze the performance bound of un-perturbed classifier fp(t +1) in (22), which minimizes Lprim(t) that involves noise vectors from Vp(t) perturbed at the previous iteration.\nTheorem 5. Let R( fp(t))= 12 ‖ fp(t) ‖ 2, and f 0p(t) such that Ĉ( f 0p(t)) = C ∗ E(t), and a real number δ > 0. From Assumption 1, we have the loss function L (·) is convex and differentiable with L ′(·) ≤ 1. If Assumption 4 and 5 are satisfied, then there exists a constant β Aprim such that if the number of data points, Bp in Dp = { (xip,yip) ⊂\nR d×{−1,1} } satisfies:\nBp > β Aprim max ({CR ‖ f 0p(t +1) ‖3 ηNpd ln( dδ ) α2accαp(t) } t=1 ,\n{CR ‖ f 0p(t +1) ‖2 ln( 1δ ) α2acc } t=1 ) ,\nthen f ∗p(t +1) satisfies: P ( Ĉ( f ∗p(t +1))≤ Ĉ∗(t +1)+αacc ) ≥ 1−2δ .\nProof: See Appendix G.\nTheorem 6. Let R( fp(t)) = 12 ‖ fp(t) ‖ 2, and f 0p(t) such that Ĉ( f 0 p(t)) = C ∗ E(t), and a real number δ > 0. Let f ∗p(t + 1) = argminZprim(t) be αacc-accurate according to Theorem 4. From Assumption 1 we have that the loss function L (·) is convex and differentiable with L ′(·) ≤ 1, and we also assume that L ′ satisfies:\n|L ′(a)−L ′(b)| ≤ c4|a−b|\nfor all pairs (a,b) with a constant c4. If Assumption 4 and 5 are satisfied, then there exists a constant β Bprim such that if the number of data points, Bp in Dp = { (xip,yip) ⊂ Rd ×{−1,1}\n} satisfies:\nBp >β Bprim max ({CR ‖ f 0p(t +1) ‖3 ηNpd ln( dδ )\nα2accαp(t)\n} t=1\n,{CR ‖ f 0p(t +1) ‖2 ln( 1δ ) α2acc } t=1 ,\n{4CB ‖ f 0(t +1) ‖ d( ln( dδ ))2 αaccαp(t) } t=1 ,\n{4 ‖ f 0p(t +1) ‖3 ηNpd ln( dδ ) α2accαp(t) } t=1 ,\n{4(CR) 32 ‖ f 0p(t +1) ‖2 d ln( dδ ) α3/2acc αp(t) } t=1 ) ,\nthen V ∗p (t +1) = f ∗ p(t +1)+ εp(t +1) satisfies: P ( Ĉ(V ∗p (t +1))≤ Ĉ∗(t +1)+αacc ) ≥ 1−3δ .\nProof: See Appendix H.\nCorollary 6.1. Let fp(t + 1) = argminLprim( fp, t|Dp) be the intermediate updated classifier of Algorithm 3, and let f 0p(t) be a reference classifier such that Ĉ( f 0p(t) = Ĉ\n∗(t). If all the conditions of Theorem 5 are satisfied, then, Vp(t +1) = fp(t +1)+ εp(t +1) satisfies\nP ( Ĉ(Vp(t+1))≤ Ĉ∗(t)+αacc+∆primp (t) ) ≥ 1−3δ .\nProof: Since\nĈ( fp(t))−Ĉ( f ∗p(t))≤ ∆primp (t),\nthen\nĈ(Vp(t))−Ĉ(V ∗p (t))≤ ∆primp (t).\nFrom Theorem 5, V ∗p (t +1) satisfies P ( Ĉ(V ∗p (t +1))≤ Ĉ∗(t +1)+αacc ) ≥ 1−3δ .\nTherefore, we have: P ( Ĉ(Vp(t +1))≤ Ĉ∗(t +1)+αacc+∆primp (t) ) ≥ 1−3δ .\nSince at the last iteration of primal variable perturbation we use the same iteration as that\nof the dual variable perturbation, Theorem 6 and Corollary 6.1 only guarantee the dynamic α tpdifferential privacy for primal variable perturbation. As a result, we combine the conditions of Theorem 4 and 6 to guarantee the networked αpdifferential privacy. Thus, we have the following corollary.\nCorollary 6.2. Let f ∗p be the final output classifier of Algorithm 3 of node p, and let f 0p(t) be a reference classifier such that Ĉ( f 0p(t) = Ĉ\n∗(t). If all the conditions of Theorem 4 and 6 are satisfied, then, f ∗p satisfies\nP ( Ĉ( f ∗p)≤ Ĉ∗(t)+αacc +∆dualp (t) ) ≥ 1−5δ .\nProof: We need all the conditions of Theorem 6 to be satisfied in order to guarantee the privacy during the intermediate iterations. All the conditions of Theorem 4 are satisfied so that the networked αp-differential privacy is provided. Combining Theorem 4 and 6 gives the probability no less than 1−5δ .\nClearly, the privacy rises by trading the accuracy. It is essential to manage the tradeoff between the privacy and accuracy in order to establish both the privacy and accuracy with at least satisfied level.\nAnother important issue we care about is the convergence of the Algorithm 1 and 2. Our analysis based on the assumption that all the conditions of Theorem 3 to 5 are satisfed. As shown in Appendix A, the non-private ADMM algorithm is convergent. In our private algorithms, the augmented Lagrange function (11) and (16) are solvable since both of them are convex. Also, the matrix A = Id is an identity matrix in our case, thus AT A is nonsingular. Theorem 9 shows that the non-private ADMM-based optimizaiton is convergent. However, our algorithms do not necessarily converge to one optimum classifier for all the nodes; different node can have different value of convergent classifier, but all of them have similar performance.\nWe first analysis the convergence of dual variable perturbation. We summarize the convergence analysis in the following theorem.\nProposition 7. Let f 0p(t) be a reference classifier such that Ĉ( f 0p) = C ∗ E(t) for all node p ∈P at\ntime t. If all the conditions of Theorem 4 are satisfied, then fp(t) = argminLdual( fp, t − 1|Dp) is convergent in distribution with probability ≤ 1−2δ . Proof: See Appendix J.\nThe convergence of primal variable perturbation only consider the primal variable fp(t+1) at each time before perturbation. It is summarized in the folowing theorem.\nProposition 8. Let R( fp(t)) = 12 ‖ fp(t) ‖ 2, and f 0p(t) such that Ĉ( f 0 p(t)) = C ∗ E(t), and a real number δ > 0. If all the conditions of Theorem 6 is satisfied, then fp(t) = argminLprim( fp, t|Dp) is convergent in distribution with probability 1−3δ . Proof: See Appendix K."
    }, {
      "heading" : "5. Numerical Experiments",
      "text" : "In this section, we test Algorithm 2 and 3 with real world training dataset. Consider the following examples. The classification method used is logistic regression. Potential application scenarios include but nor limited to the following two.\nExample 5.1. (Potential Customer Classification) Consider a network of P companies agreed to collaborate to develop an algorithm that can classify the target customers by predicting their annual incomes based on thier information such as age, sex, occupation, and education. Suppose Dp is the customer data records stored at company p. The learning process of the algorithm is based on all available datasets {Dp}Pp=1, rather than company p alone. The company p learns the model only by its own training dataset Dp, and there is no data exchange between different companies. The intermediate updated classifier fp(t) is the only shared information. Moreover, company p only communicate with its neighboring companies. The companies want to increase the privacy level of the algorithm, and make sure the final algorithm and also the learning process preserves the privacy of the sensitive information against other companies in this network as well as other parties from outside.\nExample 5.2. (International Collaborative AntiTerrorist) Consider a group of countries with\ncorresponding datasets D containing intelligence about the terrorism. All the countries are willing to collaborate in order to classify jointly possible terrorist entering their countries. However, the confidential information involved in the intelligence prevents each countries to open access to the dataset of other countries. In this case, differential privacy model can preserve the confidential intelligence while producing an accurate classifier of terrorist."
    }, {
      "heading" : "5.1. Privacy Preserved Logistic Regression",
      "text" : "In the experiments, we use our algorithm to develop a dynamic differential private logisitic regression. The logistic regression has the loss function:\nLLR(yip f T xip) = log(1+ exp(−yip f Tp xip)). (26) The first derivative and teh second derivative are:\nL ′LR = −yipxip\n1+ exp(yip f Tp xip)\nL ′′LR = y2ipxipx T ip\n(1+ exp(yip f Tp xip)(1+ exp(−yip f Tp xip) ,\nwhich can be bounded as |L ′LR| ≤ 1 and L ′′LR≤ 14 , respectively, according to Assumption 3. Therefore, the loss function of logistic regression satisfies the conditions shown in Assumption 1. In this case, R(Fp) = 12 ‖ fp ‖\n2, and c1 = 14 . And we can directly apply the loss function LLR to Theorem 1 and 2 with R( f ) = 12 ‖ fp ‖\n2, and c1 = 14 , and then it can provide αp(t)-differential privacy for any p ∈P at time t = 1, 2, ... of a distributed logistic regression problem."
    }, {
      "heading" : "5.2. Pre-Processing",
      "text" : "We test our algorithms to this example. The classification method used is logistic regression. We simulate the customer information by Adult dataset from UCI Machine Learning Repository [11], which contains demographic information such as age, sex, education, occupation, marital status, and native country. There are 48842 data samples. The prediction task is to determine\nwhether a person’s annul income is greater than $50k.\nIn order to process the Adult dataset to our algorithm, we remove all the missing data points, and follow the data cleaning process of [12]. Also, we convert the categorial attributes to a binary vector. For other non-numerical descriptive attributes such as different countries in the category of native country, we replace them by their own frequency of occurance in the corresponding category. Moreover, each column is first normalized to make sure the maximum value is 1; then each row is normalized so that the L2 norm of each data sample is at most 1."
    }, {
      "heading" : "5.3. Privacy-Accuracy Tradeoff",
      "text" : "In this experiment, we study the privacyaccuracy tradeoff of Algorithm 2 and 3. The privacy is quantified by the value of αp(t).\nWhen αp(t) becomes larger, the ratio of densities of the classifier fp(t) on two different data sets is larger, which implies a higher belief of the adversary when one data point in data set D is changed; thus, it provides lower privacy. However, the accuracy of the algorithm increases as αp(t) becomes larger. As shown in Figure 4, larger αp(t) gives better convergence of the algorithms; moreover, from Figure 4, we can see that the dual variable perturbation is slightly more robust to noise than is the primal case given the same value of αp(t). When αp(t) is small, the model is more private but less accurate. Therefore, the utilities of privacy and accuracy shoud satisfy the following assumption:\nAssumption 6. - the utilities of privacy and accuracy should be monotonic with respect to\nαp(t) but in different directions, say decreasingly and increasingly, respectively.\nAs a result, The quality of classifier is measured by the empirical loss C(t) = C R\nBp ∑ Bp i=1 L (yip fp(t) T xip). Given the dataset Dp and a αp(tx) at a specific time tx, there exists a corresponding fp(tx) minimizing (16). Thus, there must be a function Lacc() to capture relationship between αp(t) and C(t): Lacc(αp(t)) = C(t). The function Lacc is obtained by curve fitting given the experimental data points (αp(t),C(t)). Let Upriv(αp(t)) be the utility of privacy. Besides the decreasing monotonicity, Upriv(αp(t)) should be convex and doubly differentiable function of αp(t).\nGiven the privacy utility function Upriv(αp(t)), there exists an optimal value of α∗p(t) that minimizes the following problem:\nminJ (t) = Lacc(αp(t))−Upriv(αp(t)) s.t. 0 < αp(t)≤ αU , 0≤ Lacc(αp(t))≤ c3\n(27) where αU and c3 are the threshold values for αp(t) and Lacc, respectively, beyond which is considered as non-private and non-accurate, respectively. The above discussion is summarized in the following definition.\nDefinition 3. (Optimal Private) If there exists a value of privacy parameter α∗p(t) that minimizes (35):\nα∗p(t) = arg minαp(t) J (t)\ns.t. αL ≤ αp(t)≤ αU , 0≤ Lacc(αp(t))≤ c3 (28) then by choosing thie value as the privacy prarmeter, every iteration of Algorithm 1 and 2 for each node p ∈P is optimal private.\nFor training the classifier, we tried a few fixed values of ρ and test the empirical loss Lep(t) of the classifier. Then, we selected the value of ρ that minimizes the empirical loss for a fixed αp (0.3 in this experiment). We also test the nonprivate version of Algorithm, and the corresponding minimum value of ρ is obtained as the control. We chose the corresponding optimal value of the regularization prarmeter ρ for each algorithm as shown in Table 1.\nTable 2 shows the values of CR chosen for each algorithm and the non-private case. Figure (4) shows the convergence of dual and primal\nvariable perturbations at different value of αp(t). Larger values of αp yields better convergence for both perturbations. Moreover, the dual variable perturbation has smaller variance of empirical loss than does the primal perturbation. However, larger αp incurs poorer privacy. This tradeoff is discussed below.\nThe utility function of privacy is chosen according to the specification in Section 4 as:\nUpriv(αp(t)) = ωp1 · ln ωp2\nωp3αp(t)+ωp4α2p(t) ,\n(29) where ωp1 and ωp2 are two positive constants. Taking the derivatives and double derivatives with respective to αp(t),\nU ′priv(αp(t)) =− ωp1 ( ωp3 +2ωp4αp(t) ) ωp3αp(t)+ωp4α2p(t) ,\nU ′′priv(αp(t))= −2ωp1ωp4 +ωp1\n( ωp3 +2ωp4αp(t) )2 α2p(t)+ωp3 .\nFor αp(t) > 0, U ′priv(αp(t)) < 0 and U ′′priv(αp(t)) > 0, which imply decreasingly\nmonotonicity and convexity, respectively. The function Lacc(αp(t)) is determined by data fitting from {(αp(t),Lep(t)}t=0. In our experiment, we choose ωp1 = 0.02, ωp2 = 6, ωp3 = 9, ωp4 = 1.\nFigure 5 shows the privacy-accuracy tradeoff of dual variable perturbation at different iterations. From curve fitting, we model the function\nLacc(αp(t)) = c4 · e−c5αp(t)+ c6 (30)\nwhere c4, c5,c6 and are three non-negative constant. From the experimental results, we determine c4 = 0.2, c5 = 25, c6 = min{Lep(t1)}t1=0; these values are applicable for all iteraions.\nFigure 6 presents the privacy-accuracy tradeoff of primal perturbation at different iterations. We model the function Lacc the same as (38). From the plots in Figure 5, we can see that the experimental results of Lacc(αp(t)) given {αp(t)} for primal variable perturbation experimences more oscillation than the dual variable perturbation does. For iteration t > 1, c4 = 20, c5 = 20, c6 = 181 ∑ 100 t=20 Lep(t). Figure 7 and 8 compare the privacy-accuracy tradeoff of dual and primal variable perturbations in terms of empirical loss and misclassification error rate, respectively. As shown, the reaction of empirical loss of dual variable perturbation is more stable than the primal variable perturbation for most values of αp(t). Moreover, the dual perturbation gives better error rate for most of αp, which implies better management of tradeoff between privacy and accuracy.\nWe determine αU = 1, c3 = 0.135. Let α∗p be the value such that the corresponding is optimal private. Substitute (37) and (38) to (35), and we then take derivative of T in (35) with respect to αp(t), and set it to 0 at α∗p:\nωp1(ωp3−2ωp4αp(t)) =c4c5(ωp3αp(t) +ωp4α2p(t)) · e−c5αp(t).\nThe optimum value of αp(t) at each time t is obtained by soving the above equation.\nFigure 7 and 8 shows the privacy-accuracy tradeoff of the final optimum classifier in terms of empirical loss and misclassification error rate (MER). The MER is determined by the fraction of times the trained classifier predict a wrong label. We can see that primal variable peroforms slightly\nbetter than dual variable perturbation with respect to the empirical loss."
    }, {
      "heading" : "6. Conclusion",
      "text" : "This work developed two ADMM-based algorithms to solve a centralized regularized ERM in a distributed fashion while providing α-differential privacy for the ADMM iterations as well as the final trained output. Thus, the sensitive information stored in the training dataset at each node is protected against both the internal and the external adversares.\nBased on distributed training datasets, Algorithm 1 perturbs the dual variable λp(t) for every\nnode p ∈P at iteration t; For the next iteration, t + 1, the perturbed version of λp(t) is involved in the update of primal variable fp(t + 1). Thus the perturbation created at time t provides privacy at time t + 1. In Algorithm 2, we perturb the primal variable fp(t), whose noisy version is then released to the neighboring nodes. In this case, the perturbation added at time t make the training process private at time t. Moreover, since the primal variables are shared among all the nieghboring nodes, at time t, the noise directly involved in the optimization of parameter update comes from multiple nodes; as a result, the updated variable has more randomness than the dual perturbation case.\nIn general, the accuracy decreases as privacy requirements are more stringent. The tradeoff between the privacy and accuracy is studied. Our experiments on real data from UCI Machine Learning Repository show that dual variable perturbation is more robust to the noise than the primal variable perturbation. The dual variable perturbation outperforms the primal case at balancing the privacy-accuracy tradeoff as well as learning quality.\nHowever, there are several conditions for the loss function and the regularizer function, which are summarized in Assumption 1 to 3. The conditions for dual variable perturbation and primal variable perturbation are similar except that the loss funciton is required to be bounded doubly differentiable for dual variable perturbation. Thus, for the loss functions and regularizer functions satisfing Assumption 1 to 3, we recommand the dual variable perturbation algorithm, which can obtain more accurate results while keep the αdifferential privacy to a good level."
    }, {
      "heading" : "Appendix A. Alternating Direction Method of Multipliers",
      "text" : "Consider a convex optimization problem:\nmin x g1(x)+g2(Ax) s.t. x ∈ S1, Ax ∈ S2, (31)\nwhere g1 : Rs1 → R and g2 : Rs2 → R1 are both convex functions, A ∈ Rs2×s1 is a matrix, S1 ∈ Rs1\nand S2 ∈ Rs2 are two non-empty polyhedral sets. Using an additional auxiliary variable v ∈ Rs2 yields an equivalent form of (33) as:\nmin x g1(x)+g2(v).\ns.t. Ax = v\nx ∈ S1, v ∈ S2\n(32)\nThe corresponding augmented Lagrange function of (34) is:\nL(x,v,λ ) =g1(x)+g2(v)+λ T (Ax− v)\n+ η 2 ‖ Ax− v ‖2,\n(33)\nwhere λ ∈ Rs2 is the Lagrange multiplier corresponding to the constraints Ax = v, and η > 0 is a penalty parameter that controls the effect of constraints violation in (34). The ADMM first minimizes L(x,v,λ ) with respect to primal variable x, and then keeping the value of x fixed at the just computed value, with respect to the auxiliary variable v. After that, the dual variable λ is updated in a gradient ascending manner. Specifically, ADMM iterates at time t +1 is:\nx(t +1) = min x L(x,v(t),λ (t)), (34)\nv(t +1) = min x L(x(t +1),v,λ (t)), (35)\nλ (t +1) = λ (t)+η(Ax(t +1)− v(t +1)), (36)\nThe following theorem states the convergence of ADMM.\nTheorem 9. ([51]) Assume (33) is solvable, and either AT A is nonsingular or S1 is bounded. Then • a sequence {x(t),v(t),λ (t)} generated\nby ADMM iterations (36) to (38) is bounded, and every limit points of {x(t)} is an optimal solution of (33): {x(t),v(t)} converges to a solution of (33).\n• {λ} converges to a solution of the dual problem:\nmin λ∈S2 G1(λ )+G2(λ ), (37)\nwhere\nG1(λ ) = inf x\ng1(x)+λ T Ax,\nG2(λ ) = inf v\ng2(v)−λ T v."
    }, {
      "heading" : "Appendix B. Proof of Theorem 1",
      "text" : "Proof: (Theorem 1) Let fp(t + 1) be the optimal primal variable with zero duality gap. From the Assumption 1 and 2, we know that both the loss funciton L and the regularizer R(·) are differentiable and convex, and by using the Karush-Kuhn-Tucker (KKT) optimality condition (stationarity), we have\n0 = CR\nBp\nBp ∑ i=1 yipL ′(yip fp(t +1)T xip)xip +ρ∇R( fp)\n+2 ( CR\n2Bp εp(t)+λp(t)\n) +(Φ+2ηNp) fp(t +1)\n−η ∑ i∈Np ( fp(t)+ fi(t)),\nfrom which we can establish the relationship between the noise εp(t) and the optimal primal variable fp(t +1) as:\nεp(t) =− Bp\n∑ i=1 yipL ′(yip fp(t +1)T xip)xip− Bp CR ρ∇R( fp)\n− 2Bp CR λp(t)− Bp CR (Φ+2ηNp) fp(t +1) + Bpη CR ∑i∈Np ( fp(t)+ fi(t)).\n(38) Under Assumption 1, the augmented Lagrange function Ldual(t) is strictly convex, thus there is a unique value of fp(t + 1) for fixed εp(t) and dataset Dp. The equation (38) shows that for any value of fp(t + 1), we can find a unique value of εp(t) such that fp(t + 1) is the minimizer of Ldual . Therefore, given a dataset Dp, the relation between εp(t) and fp(t +1) is bijective.\nLet Dp and D′p be two datasets with Hd(Dp,D′p) = 1, (xi,yi) ∈ Dp and (x′i,y′i) ∈ D′p are the corresponding two different data points. Let two matrices J f (εp(t)|Dp) and J f (ε ′p(t)|D′p) denote the Jacobian matrices of mapping from fp(t + 1) to εp(t) and ε ′p(t), respectively. Then, transformation from noise fp(t + 1) to εp(t) by\nJacobian yields:\nQ( fp(t +1)|Dp) Q( fp(t +1)|D′p) = q(εp(t)|Dp) q(ε ′p(t)|D′p) |det(J f (εp(t)|Dp))|−1 |det(J f (ε ′p(t)|D′p))|−1 ,\n(39)\nwhere q(εp(t)|Dp) and q(ε ′p(t)|D′p) are the densities of εp(t) and ε ′p(t), respectively, given fp(t + 1) when the datasets are Dp and D′p, respectively.\nTherefore, in order to prove the ratio of conditional densities of optimal primal variable is bounded as:\nQ( fp(t)|D) Q( fp(t)|D′p) ≤ eαp(t),\nwe have to show:\nq(εp(t)|Dp) q(ε ′p(t)|D′p) · |det(J f (εp(t)|Dp))|−1 |det(J f (ε ′p(t)|D′p))|−1\n≤ eαp(t).\nWe first bound the ratio of the determinant of Jacobian matrices, and then the ratio of conditional densities of the noise vectors.\nLet xa be the a-th element of the vector x, and (a,b). Let E ∈Rd×d be a matrix, then let E(a,b) denote the (a,b)-th entry of the matrix E. Thus, the (m,n)-th entry of J f (εp(t)) is:\nJ f (εp(t))(m,n) =− Bp\n∑ i=1 (y2i L ′′(yi fp(t +1)T xi)x (m) i x (n) i\n− Bp CR ρ∇2R( fp(t +1))(m,n) − Bp CR (Φ+2ηNp)1( j = k).\nLet J0f (xi,yi)= (y2i L ′′(yi fp(t+1)T xi)xixTi , thn the Jacobian matrix can be expressed as:\nJ f (εp(t)|Dp) =− Bp\n∑ i=1 J0f (xi,yi)− Bp CR ρ∇2R( fp(t +1))\n− Bp CR (Φ+2ηNp)Id .\nLet M = J0f (x′i,y′i) − J0f (xi,yi), and H = −J f (εp(t)|Dp), and thus J f (εp(t)|D′p) = −(M+ H). Let h j(W) be the j-th largest eigenvalue of\na symmetric matrix W ∈Rd×d with rank θ Then we have the following fact:\ndet(I+W) = θ\n∏ j (1+h j(W)).\nSince the matrix xixTi has rank 1, then matrix M has rank at most 2; thus matrix H−1M has rank at most 2; therefore, we have:\ndet(H+M) = det(H) ·det(I+H−1M) = det(H) · (1+h1(H−1M))(1+h2(H−1M).\nThus, the ratio of determinants of the Jacobian matrices can be expressed as:\n|det(J f (εp(t)|Dp))|−1\n|det(J f (ε ′p(t)|D′p))|−1 = |det(H+M)| |det(H)|\n=|det(I+H−1M)| =(1+h1(H−1M))(1+h2(H−1M) =|1+h1(H−1M)+h2(H−1M) +h1(H−1M)h2(H−1M)|.\nBased on Assumption 2, all the eigenvalues of ∇2R( fp(t + 1)) is greater than 1 [32]. Thus, from Assumption 1, matrix H has all eigenvalues at least BpCR ( ρ + Φ + 2ηNp ) . Therefore, |h1(H−1M)| ≤ |hi(M)|Bp CR ( ρ+Φ+2ηNp\n) . Let σi(M) be the non-negative singular value of the symmetric matrix M. According to [3], we have the inequality\n∑ i |hi(M)| ≤∑ i σi(M). (40)\nThus, we have\n|h1(M)|+ |h2(M)| ≤ σ1(M)+σ2(M).\nLet ‖ X ‖Σ= ∑i σi be the trace norm of X. Then according to the trace norm inequality, we have:\n‖M ‖Σ≤‖ J0(x′i,y′i) ‖Σ + ‖ −J0(xi,yi) ‖Σ .\nAs a result, based on the upper bounds from Assumption 1 and 3, we have:\n|h1(M)|+ |h2(M)| ≤‖ J0(x′i,y′i) ‖Σ + ‖ −J0(xi,yi) ‖Σ ≤ |(y2i L ′′(yi fp(t +1)T xi)|· ‖ xi ‖ + |(y′2i L ′′(y′i fp(t +1)T x′i)|· ‖ x′i ‖ ≤ 2c1,\nwhich follows h1(M)h2(M) ≤ c21. Finally, the ratio of determinants of Jacobian matrices is bounded as:\n|det(J f (εp(t)|Dp))|−1\n|det(J f (ε ′p(t)|D′p))|−1 ≤ (1+ c1Bp CR ( ρ +Φ+2ηNp ) ) )2\n= eα , (41)\nwhere α = ln (\n1+ c1Bp CR ( ρ+Φ+2ηNp ))2. Now, we bound the ratio of densities of εp(t). Let sur(E) be the surface area of the sphere in d dimension with radius E, and sur(E) = sur(1) · Ed−1. We can write:\nq(εp(t)|Dp) q(ε ′p|D′p) = K (εp(t))\n‖εp(t)‖d−1 sur(‖ε1(t)‖)\nK (ε ′p(t)) ‖ε ′p(t)‖d−1\nsur(‖ε ′p(t)‖)\n≤ eζp(t)(‖ε ′p(t)‖−‖εp(t)‖)\n≤ eα̂ ,\n(42)\nwhere α̂ is a constant satisfying the above inequality. Since we want to bound the ratio of densities of fp(t +1)\nQ( fp(t +1)|Dp) Q( fp(t +1)|D′p) ≤ eαp(t),\nwe need α̂ ≤ αp(t)−α. For non-negative Φ, let α̂ = αp(t)− ln ( 1+ c1\nBp CR ( ρ +2ηNp ))2. If α̂ > 0, then we fix Φ= 0, and thus α̂ =αp(t)− α . Otherwise, let Φ = c1Bp\nCR (eαp(t)/4−1)\n−ρ − 2ηNp,\nand α̂ = αp(t)2 which makes α̂ =αp(t)−α . Therefore, we can have\n|det(J f (b1|Dp))|−1\n|det(J f (b2|D′p))|−1 ≤ eαp(t)−α̂ .\nFrom the upper bounds stated in Assumption 1 and 3, the l2 norm of the difference of ε1 and ε2 can be bounded as:\n‖ ε ′p(t)− εp(t) ‖= Bp\n∑ i=1 ‖ yipL ′(y′ip fp(t +1)T x′ip)x′ip\n− (yipL ′(yip fp(t +1)T xip)xip ‖ ≤2.\nThus,‖ ε ′p(t) ‖ − ‖ εp(t) ‖≤‖ ε ′p(t)− εp(t) ‖≤ 2. Therefore, by selecting ζp(t) = α̂2 , we can bound the ratio of conditional densities of fp(t +1) as:\nQ( fp(t +1)|Dp) Q( fp(t +1)|D′p) ≤ eαp(t),\nand prove that the dual variable perturbation can provide αp(t)-differential privacy."
    }, {
      "heading" : "Appendix C. Proof of Corollary 1.2",
      "text" : "Proof: (Corollary 1.2) We prove this corollary by induction. For c2 = 1, it is true since this is exactly the case of Theorem 1. Suppose Corollary 1.2 is held for Hd(Dp,D′p)= c2. Let Hd(Dp,D ′ p)= c2+1. Clearly, there must exist a dataset D′′p such that Hd(Dp,D′′p) = 1, and Hd(D ′ p,D ′′ p) = c2. Thus, from (13), we have:\nQ( fp(t)|Dp) Q( fp(t)|D′p) = Q( fp(t)|Dp) Q( fp(t)|D′′p) · Q( fp(t)|D′′p) Q( fp(t)|D′p)\n≤ eαp(t)ec2αp(t) = e(c2+1)αp(t). (43)\nTherefore, the induction hypothesis is true and Corollary 1.2 is proven."
    }, {
      "heading" : "Appendix D. Proof of Theorem 2",
      "text" : "Proof: (Theorem 2) Let Dp and D′p be two datasets with Hd(Dp,D′p) = 1. Since only Vp(t) is released, then our target is to prove the following:\nQ(Vp(t +1)|Dp) Q(Vp(t +1)|D′p) ≤ eαp(t). (44)\nFrom (23), we have:\nQ(Vp(t +1)|Dp) Q(Vp(t +1)|D′p) = K (εp(t)) K (ε ′p(t)) = e−ζp(t)‖εp(t)‖ e−ζp(t)‖ε ′ p(t)‖ .\n(45) Therefore, in order to make the model to provide αp(t)-differential privacy, we need to find a ζp(t) that satisfies\nζp(t)(‖ εp(t) ‖ − ‖ ε ′p(t) ‖)≤ αp(t). (46)\nLet V A = argminVp Lprim(t|Dp), and V B = argminVp Lprim(t|D′p), where Lprim(t|D) is the\naugmented Lagrange function for primal variable perturbation given dataset D.\nLet F , G be defined at each node p ∈P as:\nF(Vp(t)) = Lprim(t|Dp),\nG(Vp(t)) = Lprim(t|D′p)−Lprim(t|Dp).\nThus, G(Vp) = C R Bp ∑ Bp i=1(L (y ′ ipV T p x ′ ip) − L (yipV Tp xip)). According to Assumption 2, we can imply that Lprim(t|Dp) = F(Vp(t)) and Lprim(t|D′p) = F(Vp(t)) + G(Vp(t)) are both ρ-strong convex. Differentating G(Vp(t)) with respect to Vp(t) gives:\n∇G(Vp) = CR\nBp (y′ipL ′(y′ipV T p x ′ ip)x ′ ip − (yipL (yipV Tp xip)xip.\nFrom Assumption 1 and 3, ‖ ∇G(Vp) ‖≤ 2C R\nBp .\nFrom definitions of V A and V B, we have:\n∇F(V A) = ∇F(V B)+∇F(V B) = 0\nFrom Lemma 14 in [52] and the fact that F(·) is ρ-strongly convex, weh have the following inequality:\n〈∇F(V A)−F(V B),V A−V B〉 ≥ ρ ‖V A−V B ‖2;\ntherefore, Cauchy-Schwarz inequality yields:\n‖V A−V B ‖ · ‖ ∇G(V B) ‖ ≥ (V A−V B)T ∇G(V B) = 〈∇F(V A)−F(V B),V A−V B〉 ≥ ρ ‖V A−V B ‖2 .\nDividing both sides by ρ ‖V A−V B ‖ gives:\n‖V A−V B ‖≤ 1 ρ ‖ ∇G(V B) ‖≤ 2C\nR\nρBp . (47)\nFrom (23), we have\n‖V A−V B ‖≤ 1 ρ ‖ ∇G(V B) ‖=‖ εp(t)− ε ′p(t) ‖ .\nThus, we can bound\nζp(t)(‖ εp(t) ‖ − ‖ ε ′p(t) ‖)≤ ζp(t)(‖ εp(t)− ε ′p(t) ‖)\n≤ 2C R\nBpρ ζp(t)\nTherefore, by choosing ζp(t) = ρBpαp(t)\n2CR , the inequality (43) holds; thus primal variable perturbation is dynamic αp-differential private at each node p."
    }, {
      "heading" : "Appendix E. Proof of Theorem 3",
      "text" : "Proof: (Theorem 3) Let\nf̂p(t +1) = argmin Ẑ( fp, t),\nf̃p(t +1) = argminZp( fp, t|Dp),\nand let f nonp (t +1) be the estimated optimum that is practical result of the algorithm. We assume that f nonp (t+1) is very close to the actually so that Zp( f nonp (t+1), t|Dp)−Zp( f̃p(t+1), t|Dp)≈ 0. For the non-private ERM, Shalev-Shwartz and Srebro in [53] show that for a specific reference classifier f0(t + 1) at time t + 1 such that Ĉ( f 0(t + 1)) = C∗E, we have:\nĈ( f nonp (t +1)) =Ĉ ∗ + ( Ẑ( f nonp (t +1), t)− Ẑ( f̂p(t +1), t) ) + ( Ẑ( f̂p(t +1), t)− Ẑ( f 0p(t +1), t)\n) +\nρ 2 ‖ f 0p(t +1) ‖2 − ρ 2 ‖ f nonp (t +1) ‖2 .\nFrom Sridharan et al. [54], we have, with probability at least 1−δ Ẑ( f nonp (t +1), t)− Ẑ( f̂p(t +1), t) ≤ 2 ( Zp( f nonp (t +1), t|Dp)−Zp( f̃p(t +1), t|Dp)\n) +O ( CR\nln( 1δ ) Bpρ\n) .\nSince Zp( f nonp (t+1), t|Dp)−Zp( f̃p(t+1), t|Dp)≈ 0, then\nẐ( f nonp (t +1), t)− Ẑ( f̂p(t +1), t)\n≤ O (\nCR ln( 1δ ) Bpρ\n) .\nIf we choose ρ ≤ αacc‖ f 0p (t+1)‖2 , then\nρ 2 ‖ f 0p(t +1) ‖2 − ρ 2 ‖ f nonp (t +1) ‖2≤ αacc 2 .\nThus Ĉ( f nonp (t +1))≤Ĉ∗+O (\nCR ln( 1δ ) Bpρ\n) +\nαacc 2 .\nTherefore, we can find the value of Bp by solving\nO (\nCR ln( 1δ ) Bpρ\n) +\nαacc 2 ≤ αacc\nWe get:\nBp > βnon max\n( CR ‖ f 0p(t +1) ‖2 ln( 1δ )\nα2acc\n) .\nIf we determine different reference classifier f 0p(t + 1) at different time, then we need to find the maximum value across the time and among different value of ‖ f 0p(t +1) ‖:\nBp > βnon max ({CR ‖ f 0p(t +1) ‖2 ln( 1δ ) α2acc } t=1 ) .\nLet Fp(t +1) = argmin fp Lnon( fp, t|Dp). Since\nĈ(Fp(t +1)) = Ĉ( f nonp (t +1))+∆ non(t),\nthen\nĈ(Fp(t +1))≤ Ĉ∗(t +1)+αacc +∆non(t),\nwith probability no less than 1−δ ."
    }, {
      "heading" : "Appendix F. Proof of Theorem 4",
      "text" : "Proof: (Theorem 4) First we define the following optimal variables:\nf̂p(t +1) = argmin Ẑ( fp, t),\nf nonp (t +1) = argminZp( fp, t|Dp),\nf ∗p(t +1) = argminZdual( fp, t|Dp),\nand as defined in Theorem 3, Ĉ( f 0p(t + 1)) = Ĉ ∗ at time t + 1. We use the analysis of ShalevShwartz and Srebro in [53] (also see the work of Chaudhuri et al. in [11]), and have the follows:\nĈ( f ∗p(t +1)) =Ĉ( f 0 p(t +1)) + ( Ẑ( f ∗p(t +1), t)− Ẑ( f̂p(t +1), t) ) + ( Ẑ( f̂p(t +1), t)− Ẑ( f 0p(t +1), t)\n) +\nρ 2 ‖ f 0p(t +1) ‖2 − ρ 2 ‖ f ∗p(t +1) ‖2 .\n(48) Now we bound each terms in the right hand side of (47) as follows. From Assumption 1, we have\nL ′ ≤ c1. By choosing Bp > 5c1CR‖ f 0p (t+1)‖2\nαaccαp(t) , and ρ > αacc2‖ f 0p (t+1)‖2 , and since αp(t)≤ 1, we have:\nα̂ =αp(t)− ln ( 1+ c1\nBp CR ( ρ +2ηNp ))2 >αp(t)− ln(1+ c1CR\nBpρ )2\n>αp(t)− ln(1+ 2αp(t)\n5 )2\n>αp(t)− 4αp(t)\n5 = αp(t) 5 .\nThen, according to Algorithm 1, we choose the corresponding ζp(t) = αp(t) 4 because α̂ > 0. Let Λ be the event\nΛ := {\nZp( f ∗p(t +1), t|Dp)≤ Zp( f nonp (t +1), t|Dp)\n+ 16d2\n( ln( dδ ) )2 ρB2pαp(t)2 } .\nSince α̂ > αp(t)2 > 0, and applying Lemma 11 yields:\nPεp(t) ( Λ ) ≥ 1−δ .\nFrom the work of Sridharan et al. in [54], the following inequality holds with probability 1−δ\nẐ( f ∗p(t +1))− Ẑ( f̂p(t +1))≤ 2 ( Zp( f ∗p(t +1), t|Dp)\n−Zp( f nonp (t +1), t|Dp) )\n+O ( ln( 1δ )\nBpρ\n)\n≤ 32d2\n( ln( dδ ) )2 ρB2pαp(t)2\n+O ( ln( 1δ )\nBpρ\n) .\nThe big-O notation hides only fixed numerical constants, which depend on the derivative of the loss function and the upper bounds of the data points shown in Assumption 3. Combining the above two processes, Ẑ( f ∗p(t +1))− Ẑ( f̂p(t +1)) is bounded as shown above with probability 1− 2δ .\nFrom the definitions of f 0p(t+1) and f̂p(t+1), we can get Ẑ( f̂p(t + 1), t)− Ẑ( f 0p(t + 1), t) < 0.\nSince P≥ 1, then by selecting ρ = αacc‖ f 0p (t+1)‖2 , we can bound\nρ 2 ‖ f 0p(t +1) ‖2 − ρ 2 ‖ f ∗p(t +1) ‖2≤ αacc 2 .\nTherefore, from (47), we have:\nĈ( f ∗p(t +1))≤C∗E + 32d2\n( ln( dδ ) )2 ρB2pαp(t)2\n+O (\nCR ln( 1δ ) Bpρ\n) +\nαacc 2 ,\nwith ρ = 6αacc‖ f 0p (t+1)‖2 . The lower bounds of Bp is determined by solving the following:\n32d2 ( ln( dδ ) )2\nρB2pαp(t)2 +O\n( CR\nln( 1δ ) Bpρ\n) +\nαacc 2 ≤ αacc.\nLemma 10. Let Z be a gamma random variable with density function Γ(k,θ), where k is an integer, and let δ > 0. Then we have:\nP(Z < kθ ln( k δ ))≥ 1−δ .\nProof: (Lemma 10) Since Z is a gamma random variable Γ(k,θ), then we can express Z as follows:\nZ = k\n∑ i=1 Zi,\nwhere {Zi}ki=1 are independent exponential random variable with density function Exp( 1θ ); thus, for each Zi we have:\nP(Zi ≤ θ ln( k δ )) = 1− δ k .\nSince Zii=1 are independent, we have:\nP(Z < kθ ln( k δ )) =\nk\n∏ i=1 P(Zi ≤ θ ln( k δ ))\n= (1− δ k )k ≥ 1−δ .\nLemma 11. Let α̂ > 0, and f ∗p(t + 1) = argminZdual( fp, t|Dp), and f nonp (t + 1) =\nargminZp( fp, t|Dp). Let Λ be the event Λ := {\nZp( f ∗p(t +1), t|Dp)≤ Zp( f nonp (t +1), t|Dp)\n+ 16d2\n( ln( dδ ) )2 ρB2pαp(t)2 } .\nUnder Assumption 1 and 2, we have: Pεp(t) ( Λ ) ≥ 1−δ .\nThe probability Pεp(t) is taken over the noise vector εp(t).\nProof: (Lemma 11) Since α̂ > 0, Φ = 0; then f ∗p(t + 1) = argminZdual( fp, t|Dp) can be expressed as:\nf ∗p(t +1) = argmin ( Zp( fp, t|Dp)+2εp(t)T fp ) .\nThus, we have:\nZp( f ∗p(t +1), t|Dp)≤ Zp( f nonp (t +1), t|Dp)\n+ CR\nBp εp(t)T ( f nonp (t +1)− f ∗p(t +1)).\nFirstly, we bound the l2-norm ‖ f nonp (t + 1)− f ∗p(t + 1) ‖. We use the similar procedure to establish (46) in Appendix D by setting F(Y ) = Zp(Y, t|Dp) and G(Y ) = C R\nBp εp(t); thus, based on\nAssumption 1 and 2, we have:\n‖ f nonp (t +1)− f ∗p(t +1) ‖≤ 1 ρ ‖ ∇ ( 2εp(t)T fp ) ‖\n≤ CR ‖ εp(t) ‖\nBpρ .\nCauchy-Schwarz inequality yields:\nZp( f ∗p(t +1), t|Dp)−Zp( f nonp (t +1), t|Dp) ≤‖ Zp( f ∗p(t +1), t|Dp)−Zp( f nonp (t +1), t|Dp) ‖\n≤ 2 Bp ‖ εp(t)T ( f nonp (t +1)− f ∗p(t +1) ‖\n≤ ( CR )2 ‖ εp(t) ‖2\nB2pρ .\nSince the noise vector εp(t) is drawn from\nK (ε)∼ e−ζp(t)‖ε‖,\nthen ‖ εp(t) ‖ is drawn from Γ(d, 1ζp(t)) = Γ(d, 2 α̂ ). Then by using Lemma 10 with ‖ εp(t) ‖≤ 2d ln( dδ )\nα̂ , we have:\nLnonP( f ∗p(t +1), t|Dp)−LnonP( f nonp (t +1), t|Dp) ≤ 4d2 ( ln( dδ ) )2\nρB2pαp(t)2 .\nwith probability no less than 1−δ ."
    }, {
      "heading" : "Appendix G. Proof of Theorem 5",
      "text" : "Proof: (Theorem 5) Similar to the proof of Theorem 4 in Appendix F, we define the following optimal variables:\nf̂p(t +1) = argminZE( fp, t),\nf nonp (t +1) = argminZp( fp, t|Dp),\nf ∗p(t +1) = argminZprim( fp, t|Dp).\nLet Ĉ( f 0p(t + 1)) = Ĉ ∗ at time t + 1. We use the analysis of Shalev-Shwartz and Srebro in [53] (also see the work of Chaudhuri et al. in [11]), and have the follows,\nĈ( f ∗p(t +1)) =Ĉ( f 0 p(t +1)) + ( Ẑ( f ∗p(t +1), t)− Ẑ( f̂p(t +1), t) ) + ( Ẑ( f̂p(t +1), t)− Ẑ( f 0p(t +1), t)\n) +\nρ 2 ‖ f 0p(t +1) ‖2 − ρ 2 ‖ f ∗p(t +1) ‖2 .\n(49) According to Theorem 2, we choose ζp(t) = ρBpαp(t) 2CR > 0. Thus, applying Lemma 14, we have:\nZp( f ∗p(t +1), t|Dp)−Zp( f nonp (t +1), t|Dp) ≤ 16 ( CR )2η2N2pd2( ln( dδ ))2 ρ3B2pαp(t)2 ,\nwith probability no smaller than 1−δ . Then we use the result of Sridharan et al. in [54], with\nprobability no smaller than 1−δ : Ẑ( f ∗p(t +1))−Ẑ( f̂p(t +1))≤ 2 ( Zp( f ∗p(t +1), t|Dp)\n−Zp( f ∗p(t +1), t|Dp) )\n+O ( ln( dδ )\nBpρ ) ≤\n32 ( CR )2η2N2pd2( ln( dδ ))2 ρ3B2pαp(t)2\n+O ( ln( 1δ )\nBpρ\n) .\nCombining the above two processes, we have the probability no smaller than 1−2δ .\nIn order to bound the last two terms in (48), we select ρ = αacc‖ f 0p (t+1)‖2 ; as a result,\nρ 2 ‖ f 0p(t +1) ‖2 − ρ 2 ‖ f ∗p(t +1) ‖2≤ αacc 2 .\nFrom the definitions of f̂p(t + 1) and f 0p(t + 1), we have:\nẐ( f̂p(t +1), t)− Ẑ( f 0p(t +1), t)≤ 0.\nThe value of Bp is determined such that\nĈ( f ∗p(t +1))≤ Ĉ∗+αacc.\nTherefore, we find the bounds of Bp by solving 32 ( CR )2η2N2pd2( ln( dδ ))2 ρ3B2pαp(t)2 +O (CR ln( 1δ ) Bpρ ) + αacc 2\n≤ αacc,\nwith ρ = αacc‖ f 0p (t+1)‖2 .\nLemma 12. Let f and g be two probability density functions. If there exists a constant c6 such that f (x) = c6g(x) for all x ∈Rd , then:\nf (x) = g(x).\nProof: (Lemma 12) From the property of probability density function, we have:\n1 = ∫ ∞ −∞ f (x)dx\n=c6 · ∫ ∞ −∞ g(x)dx =c6.\nTherefore, c6 = 1, and f (x) = g(x).\nLemma 13. Let {Z j}Kj=1 be independent gamma random variables with density Γ(β j,h). Then Z = ∑Kj=1 is a gamma random variable with Γ(∑Kj β j,h)\nProof: (Lemma 13) We prove Lemma 11 by induction. First, we show it is true for K = 2. Let g(·) = Γ(β1 + β2,h), and fZ1+Z2(z) be the joint probability density of Z1 and Z2. Then, we have fZ1+Z2(z) = 0 = g(z) for all z < 0. Let r > 0, then\nfZ1+Z2(r) =( fZ1 ∗ fz2)(r) = ∫ r\n0 fZ1(x)∗ fZ2(r− x)dx\n= 1\nΓ(β1)Γ(β2) ∫ r 0 he−hx(hx)β1−1heh(r−x)\n· ( h(r− x)) )β2−1dx = hβ1+β2e−hr\nΓ(β1)Γ(β2) ∫ r 0 xβ1−1(r− x)β2−1dx\n= he−hrhβ1+β2−1\nΓ(β1)Γ(β2) ∫ 1 0 (ry)β1−1 ( r(1− y) )β2−1rdy = he−hr(rh)β1+β2−1\nΓ(β1 +β2) · Γ(β1 +β2) Γ(β1)Γ(β2)\n· ∫ 1\n0 yβ1−1(1− y)β2−1dy\n=g(r) · c6, (50)\nwhere c6 = Γ(β1+β2) Γ(β1)Γ(β2) ∫ 1 0 y β1−1(1 − y)β2−1dy is a constant. From Lemma 11, we prove that fZ1+Z2(z) = g(z).\nNow we assume it is also true for K = K. We next prove it is also true for K′ = K + 1. Let f K(z)= f∑Kj=1(z), and g\nK(z)=Γ(∑Kj=1 β j,h). Then we have:\nf K+1(z) = ( f K(z)∗ fZK+1)(z).\nBy replacing fZ1(z) by f K(z), fZ2(z) by fZK+1(z), β1 by ∑Kj=1 β j, and β2 by βK+1 in (47), we can prove\nf K+1(z) = gK+1(z) · c7,\nwhere c7 = Γ(∑Kj=1 β j+βK+1)\nΓ(∑Kj=1 β j)Γ(βK+1)\n∫ 1 0 y ∑Kj=1 β j−1(1 −\ny)βK+1−1dy is a constant. Thus form Lemma 12, f K+1(z) = gK+1(z) Therefore, by induction, Lemma 13 is proved.\nThe following Lemma is analogous to Lemma 11\nLemma 14. Let ζp(t) > 0, and f ∗p(t + 1) = argminZprim( fp, t|Dp), and f nonp (t + 1) = argminZp( fp, t|Dp). Suppose that the noise vector εt(t) generated at time t has the same value of αp(t) for all p ∈P . Let Λ be the event\nΛ := {\nZp( f ∗p(t +1), t|Dp)≤ Zp( f nonp (t +1), t|Dp)\n+ 16 ( CR )2η2N2pd2( ln( dδ ))2 ρ3B2pαp(t)2 } .\nIf the loss function L is convex and differentiable with |L | ≤ 1, then we have:\nPεp(t) ( Λ ) ≥ 1−δ .\nThe probability Pεp(t) is taken over the noise vector εp(t).\nProof: (Lemma 14) Let ε pi(t) = εp(t)− εi(t) with probability density Pε pi . Let f ∗ p(t + 1) = argminZprim( fp, t|Dp), and it can be expressed as:\nf ∗p(t +1) =argmin ( Zp( fp, t|Dp)\n−η ∑ i∈Np\n( ( fp−\n1 2 ( fp(t)+ fi(t))T\n· (ε pi(t))+ 1 4 ( ε pi(t) )2) .\nThus, we have:\nZp( f ∗p(t +1), t|Dp) ≤ Zp( f nonp (t +1), t|Dp) −η ∑\ni∈Np ( f nonp (t +1)− f ∗p(t +1))T · ε pi.\nFirstly, we bound the l2-norm ‖ f nonp (t + 1)− f ∗p(t + 1) ‖. We use the similar procedure to establish (46) in Appendix D by setting F(·) = Zp(Z, t|Dp) and G(Z) = η ∑i∈Np ( ε pi )T (·);\nthus,based on Assumption 1 and 2, we have:\n‖ f nonp (t +1)− f ∗p(t +1) ‖ ≤ 1 ρ ‖ ∑\ni∈Np ∇(ηNp( f ∗p(t +1)) T ε pi) ‖\n≤ ∑ i∈Np\nη ‖ ε pi(t) ‖ ρ\n= ∑ i∈Np\nη ( ‖ εp(t)− ε j(t) ‖ ) ρ\n≤ ∑ i∈Np\nη ( ‖ εp(t) ‖+ ‖ ε j(t) ‖ ) ρ .\nSince αp(t) is the same for all p ∈P at time t; thus ζ j(t) = ρBpαp(t) 2CR for all j ∈P . Since ε j(t) is drawn from (15), then, ‖ εp(t) ‖ is gamma with Γ(d, 1ζp(t)) for all p ∈P . Let\n‖ εpi ‖⊕=‖ εp(t) ‖+ ‖ εi(t) ‖ .\nThus\n‖ f nonp (t +1)− f ∗p(t +1) ‖ ≤ ∑ i∈Np\nη ( ‖ εpi ‖⊕ ) ρ\n= ηNp\n( ‖ εpi ‖⊕ ) ρ .\nCauchy-Schwarz inequality yields:\nZp( f ∗p(t +1), t|Dp)−Zp( f ∗p(t +1), t|Dp) ≤‖ Zp( f ∗p(t +1), t|Dp)−Zp( f ∗p(t +1), t|Dp) ‖\n≤ η2N2p\n( ‖ εpi ‖⊕ )2 ρ ,\nand from Lemma 12 we have the P‖ε p j‖ = Γ(2d, 2C R\nρBpαp(t)). Applying Lemma 10 with ‖\nε p j(t) ‖⊕≤ 4C Rd ln( dδ )\nρBpαp(t) yields:\nZp( f ∗p(t +1), t|Dp)−Zp( f nonp (t +1), t|Dp) ≤ 16 ( CR )2η2N2pd2( ln( dδ ))2 ρ3B2pαp(t)2\nwith probability no smaller than 1−δ"
    }, {
      "heading" : "Appendix H. Proof of Theorem 6",
      "text" : "Proof: (Theorem 6) Again, we define the following optimal variables:\nf̂p(t +1) = argminZE( fp, t),\nf nonp (t +1) = argminZp( fp, t|Dp),\nf ∗p(t +1) = argminZprim( fp, t|Dp),\nV ∗p (t +1) = f ∗ p(t +1)+ εp(t).\nNow we make f 0p(t +1) such that Ĉ( f ∗ p(t +1)) = Ĉ∗(t + 1) be the reference at time t + 1. We use the analysis of Shalev-Shwartz and Srebro in [53] (also see the work of Chaudhuri et al. in [11]), and have the follows,\nĈ(V ∗p (t +1)) =Ĉ( f 0 p(t +1)) + ( Ẑ(V ∗p (t +1), t)− Ẑ( f̂p(t +1), t) ) + ( Ẑ( f̂p(t +1), t)− Ẑ( f 0p(t +1), t)\n) +\nρ 2 ‖ f 0p(t +1) ‖2 − ρ 2 ‖V ∗p (t +1) ‖2 .\n(51) If R( fp(t)) = 12 ‖ fp(t) ‖\n2, then ‖∇2R( fp(t)) ‖≤ 1. Thus, we can apply Lemma 15 with τ = 1:\nZprim(V ∗p (t +1), t|Dp)−Zprim( f ∗p(t +1), t|Dp) ≤ 4 ( CR )2d2(ρ + c4CR)( ln( dδ ))2\nρ2B2pαp(t)2 ,\nwith probability ≥ 1− δ over the noise. In the proof of Theorem 5, we have, with probability 1−δ :\nZp( f ∗p(t +1), t|Dp)−Zp( f nonp (t +1), t|Dp)\n≤ 4η2N2pd2\n( ln( dδ ) )2 ρ3B2pαp(t)2 .\nTherefore, with probability 1−2δ , we have\nZp(V ∗p (t +1), t|Dp)−Zp( f nonp (t +1), t|Dp)\n≤ 4η2N2pd2\n( ln( dδ ) )2 ρ3B2pαp(t)2 + 4d2 ( ρ + c4 )( ln( dδ ) )2 ρ2B2pαp(t)2 .\nSridharan et al. in [54] shows, with probability 1−δ :\nẐ(V ∗p (t +1))− Ẑ( f̂p(t +1)) ≤ 2 ( Zprim(Vp(t +1), t|Dp)−Zprim( f ∗p(t +1), t|Dp) )\n+O (\nCR ln( dδ ) Bpρ\n)\n≤ 8 ( CR )2d2(ρ + c4CR)( ln( dδ ))2\nρ2B2pαp(t)2 +\n8η2N2pd2 ( ln( dδ ) )2\nρ3B2pαp(t)2 +O (\nCR ln( 1δ ) Bpρ\n) .\nCombining the above two processes, we have the probability no smaller than 1−3δ . Since f̂p(t + 1) = argmin Ẑ( fp, t), then( Ẑ( f̂p(t +1), t)− Ẑ( f 0p(t +1), t) ≤ 0. For the last two terms, we select ρ = αacc‖ f 0p (t+1)‖2 in order to make them bounded by αacc2 .\nThe value of Bp is determined by solving\n8 ( CR )2d2(ρ + c4CR)( ln( dδ ))2\nρ2B2pαp(t)2 +\n8η2N2pd2 ( ln( dδ ) )2\nρ3B2pαp(t)2 +O (\nCR ln( 1δ ) Bpρ\n) +\nαacc 2 = αacc,\nwith ρ = αacc‖ f 0p (t+1)‖2 , such that\nP ( Ĉ(V ∗p (t +1))≤ Ĉ∗(t +1)+αacc ) ≥ 1−3δ .\nWe get:\nBp =max ({4CB ‖ f 0(t +1) ‖ d( ln( dδ ))2 αaccαp(t) } t=1 ,\n{4 ‖ f 0p(t +1) ‖3 ηNpd ln( dδ ) α2accαp(t) } t=1 ,\n{4(CR) 32 ‖ f 0p(t +1) ‖2 d ln( dδ ) α3/2acc αp(t) } t=1 ) .\nHowever, the accuracy of V ∗p (t+1) depends on f ∗p(t +1), thus we also have to make\nP ( Ĉ( f ∗p(t +1))≤ Ĉ∗(t +1)+αacc ) ≥ 1−2δ .\nCombining the result of Theorem 5, we have Bp >β Bprim max ({CR ‖ f 0p(t +1) ‖3 ηNpd ln( dδ )\nα2accαp(t)\n} t=1\n,{CR ‖ f 0p(t +1) ‖2 ln( 1δ ) α2acc } t=1 ,\n{4CB ‖ f 0(t +1) ‖ d( ln( dδ ))2 αaccαp(t) } t=1 ,\n{4 ‖ f 0p(t +1) ‖3 ηNpd ln( dδ ) α2accαp(t) } t=1 ,\n{4(CR) 32 ‖ f 0p(t +1) ‖2 d ln( dδ ) α3/2acc αp(t) } t=1 ) .\nAs a result, the value of Bp is determined by taking the intersection of\nLemma 15. Assume R( fp(t)) is doubly differentiable w.r.t. fp(t) with ‖ ∇2R( fp(t)) ‖≤ τ for all fp(t). Suppose the loss function L is differentiable, L ′ is continuous, and satisfies\n|L ′(a)−L ′(b)| ≤ c4|a−b|\nfor all pairs (a,b) with a constant c4. Let f ∗p(t + 1) = argminZprim( fp, t|Dp), and V ∗p (t + 1) = f ∗p(t + 1) + εp(t), where the noise vector εp(t) is drawn from (15) with the same αp(t) for all p ∈P at time t. Let Λ be the event\nΛ := {\nZprim(V ∗p (t +1), t|Dp)≤ Zprim( f ∗p(t +1), t|Dp)\n+ 4 ( CR )2d2(ρτ + c4CR)( ln( dδ ))2\nρ2B2pαp(t)2 .\nUnder Assumption 1 and 2, we have: Pεp(t) ( Λ ) ≥ 1−δ .\nThe probability Pεp(t) is taken over the noise vector εp(t).\nProof: (Lemma 15) From Assumption 3, we know that the data points in dataset Dp satisfy: ‖ xip ‖≤ 1, and |yip|= 1. From Assumption 1 and 2, R(·) and L are differentiable. Suporse R(·) is doubly differentiable and ∇2R(·)≤ τ . Let 0≤\nϕ ≤ 1, then the Mean Value Theorem and CauchySchwarz inequality give:\nZprim(V ∗p (t +1), t|Dp)−Zprim( f ∗p(t +1), t|Dp) = (V ∗p (t +1)− f ∗p(t +1))T ∇Zprim ( ϕ f ∗p(t +1)\n+(1−ϕ)V ∗p (t +1) ) ≤‖V ∗p (t +1)− f ∗p(t +1) ‖\n· ‖ ∇Zprim ( ϕ f ∗p(t +1)+(1−ϕ)V ∗p (t +1) ) ‖ .\nLet ε pi(t) = εp(t)− εi(t). From the definition of Zprim( fp, t|Dp), we have:\nZprim( fp, t|Dp) =Zp( fp, t|Dp)\n−η ∑ i∈Np\n( ( fp−\n1 2 ( fp(t)+ fi(t))T · (ε pi(t))\n+ 1 4 (ε pi(t))2\n) .\nTaking the derivative of Zprim w.r.t. fp gives\n∇Zprim( fp, t|Dp) = CR\nBp\nBp ∑ i=1 yipL ′(yip f Tp xip)xip\n+ρ∇R( fp)−η ∑ j∈Np ε pi(t).\nSince ∇Zprim( f ∗p(t +1), t|Dp) = 0, then we have:\n∇Zprim ( ϕ f ∗p(t +1)+(1−ϕ)V ∗p (t +1)|Dp )\n= ∇Zprim( f ∗p(t +1), t|Dp) −ρ ( ∇R( f ∗p(t +1))−∇R ( ϕ f ∗p(t +1)\n+(1−ϕ)V ∗p (t +1) ))\n−C R\nBp\nBp ∑ i=1\n( yip ( L ′(yip f ∗p(t +1) T xip)\n−L ′(yip ( ϕ f ∗p(t +1)+(1−ϕ)V ∗p (t +1) )T xip))xip). Let\nT =yip ( L ′(yip f ∗p(t +1) T xip)\n−L ′(yip ( ϕ f ∗p(t +1)+(1−ϕ)V ∗p (t +1) )T xip))xip. Based on the condition on the loss function:\n|L ′(a)−L ′(b)| ≤ c4|a−b|,\nwe can bound T as: T ≤|yip| ‖ xip ‖ · |L ′(yip f ∗p(t +1)T xip)\n−L ′(yip ( ϕ f ∗p(t +1)+(1−ϕ)V ∗p (t +1) )T xip)| ≤|yip| ‖ xip ‖ ·c4 · |yip(1−ϕ)( f ∗p(t +1)−V ∗p (t +1))T xip| ≤c4 · (1−ϕ)|yip|2 ‖ xip ‖2‖ f ∗p(t +1)−V ∗p (t +1) ‖ ≤c4 · (1−ϕ) ‖ f ∗p(t +1)−V ∗p (t +1) ‖ .\nSince we assume R(·) is doubly differentiable, we then apply the Mean Value Theorem:\n‖ ∇R( f ∗p(t +1))−∇R ( ϕ f ∗p(t +1)+(1−ϕ)V ∗p (t +1) ) ‖ ≤ (1−ϕ) ‖ f ∗p(t +1)−V ∗p (t +1) ‖ · ‖ ∇2R(ξ ) ‖,\nwhere ξ ∈Rd . Therefore, we have: ∇Zprim ( ϕ f ∗p(t +1)+(1−ϕ)V ∗p (t +1)|Dp )\n≤ (1−ϕ) ‖ f ∗p(t +1)−V ∗p (t +1) ‖ ·ρ· ‖ ∇2R(ξ ) ‖ +CRc4 · (1−ϕ) ‖ f ∗p(t +1)−V ∗p (t +1) ‖\n≤ (1−ϕ)· ‖ f ∗p(t +1)−V ∗p (t +1) ‖ ( ρτ +CRc4 )\n≤‖ f ∗p(t +1)−V ∗p (t +1) ‖ ( ρτ +CRc4 ) .\nSince f ∗p(t + 1)−V ∗p (t + 1) = εp(t), with density Γ(d, 2C R\nρBpαp(t)) then we can apply Lemma 10 to ‖ f ∗p(t+1)−V ∗p (t+1) ‖. Thus, with ‖ f ∗p(t+1)− V ∗p (t +1) ‖≤ 2CRd ln( dδ ) ρBpαp(t) , we have:\nZprim(V ∗p (t +1), t|Dp)−Zprim( f ∗p(t +1), t|Dp) ≤ 4 ( CR )2d2(ρτ + c4CR)( ln( dδ ))2\nρ2B2pαp(t)2 ,\nwith probability no less than 1−δ .\nAppendix I. Proof that iterations (5) to (8) are convergent ADMM algorithm shown in Appendix A\nThe goal here is to cast (3) in the form of (31) and show that updates (5)-(8) correspond to (33)-(35) in Appendix A. We first reform the constraints { fp = wp j, wp j = f j}p∈P, j∈Np to A f = w, where f = [ f1, f2, ..., fP]T . For all the\nnodes in the network, the constraint fp = wp j can be written as:\n{ f1 = w1 j} j∈N1 ...\n{ fP = wP j} j∈NP .\n(52)\nLet w = [{wT1 j} j∈N1 , ...,{w T P j} j∈NP ] T\nand let A be a block-diagonal matrix with diagonal\nAi = [Id , ...,Id ]T .\nThus,\nA = ∣∣∣∣∣∣∣ A1 . . . AP ∣∣∣∣∣∣∣ . Therefore, we can write (51) in the form of matrix and vector as:\nA f = w. (53)\nLet |E | be the number of links in the network. Then there are ∑Pi=1 Np = 2|E |, where the factor 2 of 2|E | is from the fact that there are two constraints between two nodes: fp = wp j and f j = w jp. Then matrix A ∈ R2d|E |×dP, and Ai ∈ RdNp×dNp .\nNow we consider the constraint fp = w jp. We can also list it acorss the nodes as:\n{ f1 = w j1} j∈N1 ...\n{ fP = w jP} j∈NP .\n(54)\nSimilarly, let\nw1 = [{wTj1} j∈N1 , ...,{w T jP} j∈NP ] T .\nand then we can write (53) in the form as:\nA f = w1. (55)\nIt can be observed that replacing each wi j in w by w ji gives w1. Now we express w1 in terms of w. Let Sw be a 2|E |×2|E | matrix defined as:\nSw = [{s1 j} j∈N1 , ...,{sP j} j∈NP ],\nwhere sp j = [ ( s1p j )T , ..., ( sPp j )T ]T\nis a 2|E | × 1 indictor vector. Let δ (·, ·) be the Kronecker’s delta. Then\nsap j = [{δ (p−b, j−b)}b∈Na ] T .\nThus, we can write w1 in terms of w as: w1 = ( Sw⊗ Id ) w. (56)\nTherefore, (54) can be written as: A f = ( Sw⊗ Id ) w, (57)\nwhere ⊗ denotes Kronecker product. Let A1 = [ AT AT ]T , and S1 = [IT2d|E |(Sw ⊗ Id )T ]T . Then, we can combine (54) and (56) as:\nA1 f = S1w. (58)\nThus, we can re-write (3) as:\nmin Zdec = CR\nBp\nP\n∑ p=1\nBp ∑ i=1 L (yip f Tp xip)+ P ∑ p=1 ρR( fp)\ns.t. A1 f = S1w. (59)\nNow, let\ng1( f ) = CR\nBp\nP\n∑ p=1\nBp ∑ i=1 L (yip f Tp xip)+ P ∑ p=1 ρR( fp)\ng2(w) = 0 S1 = RdP\nS2 = { w ∈ R4d|E ||w = S1w′for somew′ ∈ R2d|E | } .\nThus, problem (3) has the type of (31). Therefore, the ADMM-based algorithm with updates (5)- (8) is convergent according to Theorem 9 in Appendix A."
    }, {
      "heading" : "Appendix J. Proof of Proposition 7",
      "text" : "Proof: (Proposition 7) According to Corollary 4.1, fp(t) is αacc-optimal at each time t, and\nP ( Ĉ( fp(t))≤ Ĉ∗(t)+αacc +∆dualp (t) ) ≥ 1−2δ ,\nand from Theorem 3, we have P ( Ĉ(Fp(t))≤ Ĉ∗(t)+αacc +∆non(t) ) ≥ 1−δ .\nThen, we have: P ( Ĉ( fp(t))≤ Ĉ(Fp(t))+∆dualp (t)−∆non(t) ) ≥ 1−2δ .\nIt also holds for t→∞ when the Fp(t) converges to f nonp (t + 1). Therefore, fp(t) performs similar to f nonp (t), and the error between them is caused by the noise {εp(t)}.\nTaking the gradient of Ldual (16) and setting it to 0 at fp(t) give (37) in Appendix:\nεp(t) =− Bp\n∑ i=1 yipL ′(yip fp(t +1)T xip)xip− Bp CR ρ∇R( fp)\n− 2Bp CR λp(t)− Bp CR (Φ+2ηNp) fp(t +1) + Bpη CR ∑i∈Np ( fp(t)+ fi(t)).\nFollowing the similar argument in the proof of Theorem 1 in Appendix B, we claim that the relation between εp(t) and fp(t +1) is bijective.\nLet J f (εp(t)|Dp) be the Jacobian matrix transforming from fp(t+1)→ εp(t) as (See Appendix B for more details):\nJ f (εp(t)|Dp) =− Bp\n∑ i=1 J0f (xi,yi)− Bp CR ρ∇2R( fp(t +1))\n− Bp CR (Φ+2ηNp)Id .\nBy transformation through Jacobian, we have:\nQ( fp(t)|Dp)\n=K (εp(t)) ‖ εp(t) ‖d−1\nsur(‖ εp(t) ‖) |det(J f (εp(t)|Dp))|−1\n=K (εp(t)) 1\nsur(‖ 1 ‖) |det(J f (εp(t)|Dp))|−1,\nwhere sur(E) is the surface area of the sphere in d dimension with radius E, and sur(E) = sur(1) · Ed−1. Since εp(t) is Laplace random variable with density K , and |det(J f (εp(t)|Dp))| is a bounded function of fp(t + 1). Thus, Q( fP(t)|Dp) is a bounded density function. Therefore, with probability greater than 1−2δ , Algorithm 2 converges in distribution."
    }, {
      "heading" : "Appendix K. Proof of Proposition 8",
      "text" : "Proof: (Proposition 8) From Corollary 6.1, we have: P ( Ĉ(Vp(t +1))≤ Ĉ∗(t +1)+αacc+∆primp (t) ) ≥ 1−3δ ,\nand from Theorem 3, P ( Ĉ(Fp(t))≤ Ĉ∗(t)+αacc +∆non(t) ) ≥ 1−δ , then, P ( Ĉ(Vp(t +1))≤ Ĉ(Fp(t))+∆primp (t)−∆non(t)\n) ≥ 1−3δ .\nIt also holds for t→∞ when the Fp(t) converges to f nonp (t + 1). Therefore, Vp(t) performs similar to f nonp (t), and the error between them is caused by the noise {εp(t)}.\nLet fp(t + 1) = argminLprim(t) with zero duality gap, and let ε pi(t) = εp(t)− εi(t). Under the Assumption 1 and 2, using the Karush-KuhnTucker (KKT) optimality condition (stationarity), we have\n0 = CR\nBp\nBp ∑ i=1 yipL ′(yip fp(t +1)T xip)xip +ρ∇R( fp)\n−η ∑ i∈Np ε pi(t).\nLet ε p(t) = ∑i∈Np ε pi(t). Then we can establish the relationship between the noise ε pi(t) and the optimal primal variable fp(t +1) as:\nε p(t) = CR\nBpη\nBp ∑ i=1 yipL ′(yip fp(t +1)T xip)xip\n+ ρ η ∇R( fp).\nAgain, following the similar argument in the proof of Theorem 1 in Appendix B, we claim that there is bijection between εp(t) and fp(t +1).\nLet J1f (ε p(t)|Dp) be the Jacobian matrix transforming from fp(t + 1) to ε p(t) based on the above equation. Let J1f (ε p(t)|Dp)(a,b) be the (a,b) entry of matrix J1f (ε p(t)|Dp), then\nJ1f (ε p(t)|Dp)(a,b)\n= CR\nBpη\nBp ∑ i=1 y2ipL ′′(yip fp(t +1)T xip)x (a) ip x (b) ip\n+ ρ η ∇2R( fp)(a,b).\nThus,\nJ1f (ε p(t)|Dp) =\nCR\nBpη\nBp ∑ i=1 y2ipL ′′(yip fp(t +1)T xip)xipxTip\n+ ρ η ∇2R( fp).\nWe now find the probability density function of ε pi(t). Since εp(t) and εi(t) are independent, then their joint density function Ppi(z) is:\nPpi(z) = 1 κ\ne− ( ζp(t)+ζ j(t) ) ‖z‖,\nwhere κ is a normalizing constant. Since αp(t) is fixed for all nodes at time t, then all the nodes have the same value of ζp(t) = ζ (t). Then\nPpi(εp(t),εi(t)) = 1 κ\ne−2ζ (t) ( ‖εp(t)‖−‖εi(t)‖ ) .\nThen the cumulative distribution function of ε pi(t) is Fε pi(t)(z) =P(ε pi ≤ z)\n= ∫ ∞\n∞ ∫ ∞ ε p−z Ppi(εp(t),εi(t))Ppi(z)dεp(t)dε j(t).\nThus, the density function of ε pi(t) is\nPpi(z) = dFε pi(t)(z) dz .\nTherefore, the density function of ε p(t) = ∑i∈Np ε pi(t) can be expressed as:\nPε p(t)(z) = Np\n∏ i∈Np *Ppi(z),\nwhere ∏Npi∈Np * is the Np-fold convolution. By transformation through Jacobian, we have:\nQA( fp(t +1)|Dp) =Pε p(t)(εp(t)) ‖ εp(t) ‖d−1\nsur(‖ ε1(t) ‖) · |det(J1f (ε p(t)|Dp))|−1 = Pε p(t)(εp(t)) 1 sur(‖ 1 ‖) · |det(J1f (ε p(t)|Dp))|−1,\nwhere sur(E) is the surface area of the sphere in d dimension with radius E, and sur(E) = sur(1) · Ed−1. |det(J1f (ε p(t)|Dp))| is a bounded function of fp(t +1).\nSince Vp(t + 1) = fp(t + 1) + εp(t + 1) and fp(t +1) and εp(t +1) are independent, then we can find the probability density function, Pt+1Vp , of Vp(t +1) as:\nPt+1Vp (z) = (Q A( fP(t +1)|Dp)∗K )(z).\nTherefore, with probability greater than 1− 3δ , Algorithm 2 converges in distribution."
    } ],
    "references" : [ {
      "title" : "Consensus-based distributed support vector machines",
      "author" : [ "P.A. Forero", "A. Cano", "G. Giannakis" ],
      "venue" : "JMLR, 11:1663–1707,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Scaling Distributed Machine Learning with the Parameter Server",
      "author" : [ "Mu Li", "Dave Andersen", "Alex Smola", "Junwoo Park", "Amr Ahmed", "Vanja Josifovski", "James Long", "Eugene Shekita", "Bor-Yiing Su" ],
      "venue" : "Operating Systems Design and Implementation (OSDI),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Singular Value Inequalities: New Approaches to Conjectures",
      "author" : [ "Peter Chilstrom" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "C. Dwork", "F. McSherry", "K. Nissim", "A. Smith" ],
      "venue" : "Proc. of the Third Theory of Cryptography Conference – TCC 2006,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Sibyl: A system for large scale supervised machine learning",
      "author" : [ "K. Canini" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundations and Trends in Machine Learning",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Distributed training strategies for the structured perceptron",
      "author" : [ "McDonald", "Ryan", "Hall", "Keith", "Mann", "Gideon" ],
      "venue" : "In NAACL HLT,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Differentially private approximation algorithms",
      "author" : [ "A. Gupta", "K. Ligett", "F. McSherry", "A. Roth", "K. Talwar" ],
      "venue" : "In Proceedings of the 2010 ACM-SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Smooth sensitivity and sampling in private data analysis",
      "author" : [ "K. Nissim", "S. Raskhodnikova", "A. Smith" ],
      "venue" : "Proceedings of the 39th STOC,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "Privacy, accuracy, and consistency too: a holistic solution to contingency table release",
      "author" : [ "B. Barak", "K. Chaudhuri", "C. Dwork", "S. Kale", "F. McSherry", "K. Talwar" ],
      "venue" : "In Proceedings of the 26th ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "Differentially private empirical risk minimization",
      "author" : [ "K Chaudhuri", "C Monteleoni", "AD Sarwate" ],
      "venue" : "Journal of machine learning research: JMLR 12,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Differential privacy and robust statistics",
      "author" : [ "C. Dwork", "J. Lei" ],
      "venue" : "In Proceedings of the 41st ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms",
      "author" : [ "Collins", "Michael" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "Singular Value Inequalities: New Approaches to Conjectures",
      "author" : [ "P. Chilstrom" ],
      "venue" : "UNF Theses and Dissertations",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "An empirical comparison of voting classification algorithms: Bagging, boosting, and variants",
      "author" : [ "Bauer", "Eric", "Kohavi", "Ron" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1999
    }, {
      "title" : "Bundle methods for regularized risk minimization",
      "author" : [ "Teo", "Choon Hui", "S.V.N. Vishwanthan", "Smola", "Alex J", "Le", "Quoc V" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "A statistical framework for differential privacy",
      "author" : [ "L. Wasserman", "S. Zhou" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Our data, ourselves: Privacy via distributed noise generation",
      "author" : [ "C. Dwork", "K. Kenthapadi", "F. McSherry", "I. Mironov", "M. Naor" ],
      "venue" : "In Serge Vaudenay, editor, EUROCRYPT,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Towards a methodology for statistical disclosure control",
      "author" : [ "T. Dalenius" ],
      "venue" : "Statistik Tidskrift,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1977
    }, {
      "title" : "Privacy-preserving data mining",
      "author" : [ "R. Agrawal", "R. Srikant" ],
      "venue" : "SIGMOD Record,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2000
    }, {
      "title" : "k-anonymity: a model for protecting privacy",
      "author" : [ "L. Sweeney" ],
      "venue" : "International Journal of Uncertainty, Fuzziness and KnowledgeBased Systems,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2002
    }, {
      "title" : "Limiting privacy breaches in privacy preserving data mining",
      "author" : [ "A. Evfimievski", "J. Gehrke", "R. Srikant" ],
      "venue" : "In Proceedings of the 22nd ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2003
    }, {
      "title" : "l-diversity: Privacy beyond k-anonymity",
      "author" : [ "A. Machanavajjhala", "J. Gehrke", "D. Kifer", "M. Venkitasubramaniam" ],
      "venue" : "In Proceedings of the 22nd International Conference on Data Engineering (ICDE),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2006
    }, {
      "title" : "Privacy preserving mining of association rules",
      "author" : [ "A. Evfimievski", "R. Srikant", "R. Agrawal", "J. Gehrke" ],
      "venue" : "Information Systems,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2004
    }, {
      "title" : "Multiplicative noise for masking continuous data",
      "author" : [ "J. Kim", "W. Winkler" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2003
    }, {
      "title" : "The price of privacy and the limits of LP decoding",
      "author" : [ "C. Dwork", "F. McSherry", "K. Talwar" ],
      "venue" : "Proceedings of the 39th STOC,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2007
    }, {
      "title" : "Mechanism design via differential privacy",
      "author" : [ "F. McSherry", "K. Talwar" ],
      "venue" : "Proceedings of the 48th FOCS,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2007
    }, {
      "title" : "Practical privacy: the SuLQ framework",
      "author" : [ "A. Blum", "C. Dwork", "F. McSherry", "K. Nissim" ],
      "venue" : "Proceedings of the 24th PODS,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2005
    }, {
      "title" : "Differential privacy with compression",
      "author" : [ "S. Zhou", "K. Ligett", "L. Wasserman" ],
      "venue" : "In Proceedings of the 2009 International Symposium on Information Theory, Seoul, South Korea,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2009
    }, {
      "title" : "Robust deanonymization of large sparse datasets (how to break anonymity of the netflix prize dataset)",
      "author" : [ "A. Narayanan", "V. Shmatikov" ],
      "venue" : "In Proceedings of 29th IEEE Symposium on Security and Privacy,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2008
    }, {
      "title" : "Cryptographic techniques for privacypreserving data mining",
      "author" : [ "B. Pinkas" ],
      "venue" : "ACM SIGKDD Explorations Newsletter,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2002
    }, {
      "title" : "Secure multiparty computation of approximations",
      "author" : [ "J. Feigenbaum", "Y. Ishai", "T. Malkin", "K. Nissim", "M.J. Strauss", "R.N. Wright" ],
      "venue" : "ACM Trans. Algorithms,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2006
    }, {
      "title" : "etc.On the Linear Convergence of the ADMM in Decentralized Consensus Optimization",
      "author" : [ "Wei Shi", "Qing Ling" ],
      "venue" : "IEEE TRANSACTIONS ON SIG- NAL PROCESSING,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2014
    }, {
      "title" : "Wherefore art thou R3579X? Anonymized social networks, hidden patterns, and structural steganography",
      "author" : [ "L. Backstrom", "C. Dwork", "J. Kleinberg" ],
      "venue" : "In Proceedings of the 16th International World Wide Web Conference,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2007
    }, {
      "title" : "Learning your identity and disease from research papers: information leaks in genome wide association study",
      "author" : [ "R. Wang", "Y.F. Li", "X. Wang", "H. Tang", "X.-Y. Zhou" ],
      "venue" : "In ACM Conference on Computer and Communications Security,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2009
    }, {
      "title" : "Privacy-preserving classification of vertically partitioned data via random kernels",
      "author" : [ "O.L. Mangasarian", "E.W. Wild", "G. Fung" ],
      "venue" : "ACM Transactions on Knowledge Discovery from Data,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2008
    }, {
      "title" : "Multiplicative noise for masking continuous data",
      "author" : [ "J. Kim", "W. Winkler" ],
      "venue" : null,
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2003
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2004
    }, {
      "title" : "Privacy-preserving datamining on vertically partitioned databases",
      "author" : [ "C. Dwork", "K. Nissim" ],
      "venue" : "Proc. CRYPTO,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2004
    }, {
      "title" : "A General Analysis of the Convergence of ADMM",
      "author" : [ "R. Nishihara", "L. Lessard", "B. Recht", "A. Packard", "M. Jordan" ],
      "venue" : "International Conference on Machine Learning",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2015
    }, {
      "title" : "What can we learn privately",
      "author" : [ "S.A. Kasiviswanathan", "H.K. Lee", "K. Nissim", "S. Raskhodnikova", "A. Smith" ],
      "venue" : "In Proc. of FOCS,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2008
    }, {
      "title" : "A theory of the learnable",
      "author" : [ "L.G. VALIANT" ],
      "venue" : "Communications of the ACM",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 1984
    }, {
      "title" : "Securitycontrol methods for statistical databases: a comparative study",
      "author" : [ "N.R. Adam", "J.C. Wortmann" ],
      "venue" : "ACM Computing Surveys,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 1989
    }, {
      "title" : "Secure statistical databases with random sample queries",
      "author" : [ "Dorothy E. Denning" ],
      "venue" : "ACM Transactions on Database Systems,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 1980
    }, {
      "title" : "Privacy-preserving data mining",
      "author" : [ "Rakesh Agrawal", "Ramakrishnan Srikant" ],
      "venue" : "SIGMOD Conference,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2000
    }, {
      "title" : "Parallel and Distributed Computation: Numerical Methods",
      "author" : [ "D. Bertsekas", "J. Tsitsiklis" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 1997
    }, {
      "title" : "Online Learning: Theory, Algorithms, and Applications",
      "author" : [ "S. Shalev-Shwartz" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2007
    }, {
      "title" : "SVM optimization : Inverse dependence on training set size",
      "author" : [ "S. Shalev-Shwartz", "N. Srebro" ],
      "venue" : "In The 25th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2008
    }, {
      "title" : "Fast rates for regularized objectives",
      "author" : [ "K. Sridharan", "N. Srebro", "S. Shalev-Shwartz" ],
      "venue" : "In Proceedings of the 22nd Annual Conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "In practice, the amount of training data can range from 1T B to 1PB [2].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "With this training data, it is possible to develop complex models with 109 to 1012 parameters [2, 5].",
      "startOffset" : 94,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "With this training data, it is possible to develop complex models with 109 to 1012 parameters [2, 5].",
      "startOffset" : 94,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : "tire network, and it has been proved that ADMM for convex optimization problem is convergent to the centralized problem under some specific conditions [6].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 30,
      "context" : "These kinds of attacks have been studied in many works; for example, the adversary can use some background knowledge and cross correlation with other databeses to extract the private information [32, 30].",
      "startOffset" : 195,
      "endOffset" : 203
    }, {
      "referenceID" : 3,
      "context" : "Specifically, we develop randomized algorithms that can provide privacy in terms of αdifferential privacy [4, 9] while keeping the learning procedure accurate.",
      "startOffset" : 106,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : "Specifically, we develop randomized algorithms that can provide privacy in terms of αdifferential privacy [4, 9] while keeping the learning procedure accurate.",
      "startOffset" : 106,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "[4], which adds noise to the output of the non-private regularized ERM algorithm.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4], we develop a private ADMM-based distributed algorithm for regulatized ERM, which applies primal variable perturbation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "• We implement our methods by experiments on a dateset of UCI Machine Learning Repositories [16].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : "In the first kind of these works, researchers focus on making the distributed algorithm suitable to datasets of very large size; some ([14]) use MapReduce to explore the performance improvements.",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "The second kind of works includes methods such as ADMM methods ([1]) , parameter averaging ([10]) voting classifiction ([13]), mixing parameters ([7]).",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "The second kind of works includes methods such as ADMM methods ([1]) , parameter averaging ([10]) voting classifiction ([13]), mixing parameters ([7]).",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "The second kind of works includes methods such as ADMM methods ([1]) , parameter averaging ([10]) voting classifiction ([13]), mixing parameters ([7]).",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "The second kind of works includes methods such as ADMM methods ([1]) , parameter averaging ([10]) voting classifiction ([13]), mixing parameters ([7]).",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 19,
      "context" : "Research on privacy has been studied in a significant number of works since at least [20].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 21,
      "context" : "Recent literature on privacy includes anonymization [22], privacy-preserving data mining [19,20,21], cryptographic approaches [33, 35].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 18,
      "context" : "Recent literature on privacy includes anonymization [22], privacy-preserving data mining [19,20,21], cryptographic approaches [33, 35].",
      "startOffset" : 89,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "Recent literature on privacy includes anonymization [22], privacy-preserving data mining [19,20,21], cryptographic approaches [33, 35].",
      "startOffset" : 89,
      "endOffset" : 99
    }, {
      "referenceID" : 20,
      "context" : "Recent literature on privacy includes anonymization [22], privacy-preserving data mining [19,20,21], cryptographic approaches [33, 35].",
      "startOffset" : 89,
      "endOffset" : 99
    }, {
      "referenceID" : 31,
      "context" : "Recent literature on privacy includes anonymization [22], privacy-preserving data mining [19,20,21], cryptographic approaches [33, 35].",
      "startOffset" : 126,
      "endOffset" : 134
    }, {
      "referenceID" : 30,
      "context" : "Individual information can be re-identified by simply using a small amount of side information [32,16].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 15,
      "context" : "Individual information can be re-identified by simply using a small amount of side information [32,16].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 24,
      "context" : "Other works on data perturbation for privacy (for instance [25, 26]) focus on additive or multiplicative perturbation of individual samples, which might affect certain relationships among different samples in the database.",
      "startOffset" : 59,
      "endOffset" : 67
    }, {
      "referenceID" : 25,
      "context" : "Other works on data perturbation for privacy (for instance [25, 26]) focus on additive or multiplicative perturbation of individual samples, which might affect certain relationships among different samples in the database.",
      "startOffset" : 59,
      "endOffset" : 67
    }, {
      "referenceID" : 44,
      "context" : "The idea of increasing privacy by adding noise has been studied for decades (for example, [49]; and see [48] for more details).",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 43,
      "context" : "The idea of increasing privacy by adding noise has been studied for decades (for example, [49]; and see [48] for more details).",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 45,
      "context" : "Since Agrawal and Srikant’s work in [50], increasing number of work studies the limitations and applicability of noise perturbation, and the definitions of privacy started to expand.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "’s basic definition of privacy [4], ε-indistinguishability or differential privacy, a change in a single entry of the dataset incurs a small change in the distribution from the view of any adversary via a specific measure of distance in a worst-case scenario.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "Differential privacy has been used in a large number of works in privacy research (for example, [4, 11, 27, 28, 29]) since it was first proposed by Dwork et al [4].",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 10,
      "context" : "Differential privacy has been used in a large number of works in privacy research (for example, [4, 11, 27, 28, 29]) since it was first proposed by Dwork et al [4].",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 26,
      "context" : "Differential privacy has been used in a large number of works in privacy research (for example, [4, 11, 27, 28, 29]) since it was first proposed by Dwork et al [4].",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 27,
      "context" : "Differential privacy has been used in a large number of works in privacy research (for example, [4, 11, 27, 28, 29]) since it was first proposed by Dwork et al [4].",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 28,
      "context" : "Differential privacy has been used in a large number of works in privacy research (for example, [4, 11, 27, 28, 29]) since it was first proposed by Dwork et al [4].",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "Differential privacy has been used in a large number of works in privacy research (for example, [4, 11, 27, 28, 29]) since it was first proposed by Dwork et al [4].",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 9,
      "context" : "Later works include differential-private contingency tables [10], and differential-private combinatorial optimization [8].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "Later works include differential-private contingency tables [10], and differential-private combinatorial optimization [8].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 42,
      "context" : "derives a general method for probabilistically approximately correct (PAC, [47]) in [46].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 41,
      "context" : "derives a general method for probabilistically approximately correct (PAC, [47]) in [46].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "in [9] that provides a method to deliver the dataset differentially privately.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]).",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 8,
      "context" : "Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]).",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 9,
      "context" : "Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]).",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 26,
      "context" : "Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]).",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 27,
      "context" : "Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]).",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 28,
      "context" : "Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]).",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 39,
      "context" : "Many works studied the tradeoff privacy and accuracy while developing and exploring the theory of differential privacy (examples include [4, 9, 10, 27, 28, 29, 44]).",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : "According to Lemma1 in [1], if { fp}p=1 represnet a feasible solution of (2) and the network is connected, then problems (1) and (2) are equivalent, that is, f = fp, p = 1, .",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "The simplified ADMM iteration is shown as follows, due to Lemma 3 of [1].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "We denote our privacy of distributed network based on the definition of differential privacy in [4].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "The definitions of differential privacy in Section 2 (and also in, for example, [4, 11, 12]) only consider the change of output distribution corresponding to a change of a single entry of the dataset.",
      "startOffset" : 80,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "The definitions of differential privacy in Section 2 (and also in, for example, [4, 11, 12]) only consider the change of output distribution corresponding to a change of a single entry of the dataset.",
      "startOffset" : 80,
      "endOffset" : 91
    }, {
      "referenceID" : 11,
      "context" : "The definitions of differential privacy in Section 2 (and also in, for example, [4, 11, 12]) only consider the change of output distribution corresponding to a change of a single entry of the dataset.",
      "startOffset" : 80,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "We simulate the customer information by Adult dataset from UCI Machine Learning Repository [11], which contains demographic information such as age, sex, education, occupation, marital status, and native country.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 11,
      "context" : "In order to process the Adult dataset to our algorithm, we remove all the missing data points, and follow the data cleaning process of [12].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 46,
      "context" : "([51]) Assume (33) is solvable, and either AT A is nonsingular or S1 is bounded.",
      "startOffset" : 1,
      "endOffset" : 5
    }, {
      "referenceID" : 30,
      "context" : "Based on Assumption 2, all the eigenvalues of ∇2R( fp(t + 1)) is greater than 1 [32].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "According to [3], we have the inequality",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 47,
      "context" : "From Lemma 14 in [52] and the fact that F(·) is ρ-strongly convex, weh have the following inequality:",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 48,
      "context" : "For the non-private ERM, Shalev-Shwartz and Srebro in [53] show that for a specific reference classifier f0(t + 1) at time t + 1 such that Ĉ( f 0(t + 1)) = C∗E, we have: Ĉ( f non p (t +1)) =Ĉ ∗",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 49,
      "context" : "[54], we have, with probability at least 1−δ Ẑ( f non p (t +1), t)− Ẑ( f̂p(t +1), t) ≤ 2 ( Zp( f non p (t +1), t|Dp)−Zp( f̃p(t +1), t|Dp) )",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 48,
      "context" : "We use the analysis of ShalevShwartz and Srebro in [53] (also see the work of Chaudhuri et al.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "in [11]), and have the follows:",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 49,
      "context" : "in [54], the following inequality holds with probability 1−δ",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 48,
      "context" : "We use the analysis of Shalev-Shwartz and Srebro in [53] (also see the work of Chaudhuri et al.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 10,
      "context" : "in [11]), and have the follows,",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 49,
      "context" : "in [54], with",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 48,
      "context" : "We use the analysis of Shalev-Shwartz and Srebro in [53] (also see the work of Chaudhuri et al.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 10,
      "context" : "in [11]), and have the follows,",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 49,
      "context" : "in [54] shows, with probability 1−δ :",
      "startOffset" : 3,
      "endOffset" : 7
    } ],
    "year" : 2017,
    "abstractText" : "Privacy-preserving distributed machine learning becomes increasingly important due to the rapid growth of amount of data and the importance of distributed learning. This paper develops algorithms to provide privacy-preserving learning for classification problem using the regularized empirical risk minimization (ERM) objective function in a distributed fashion. We use the definition of differential privacy, developed by Dwork et al. privacy to capture the notion of privacy of our algorithm. We provide two methods. We first propose the dual variable perturbation, which perturbs the dual variable before next intermediate minimization of augmented Lagrange function over the classifier in every ADMM iteration. In the second method, we apply the output perturbation to the primal variable before releasing it to neighboring nodes. We call the second method primal variable perturbation. Under certain conditions on the convexity and differentiability of the loss function and regularizer, our algorithms is proved to provide differential privacy through the entire learning process. We also provide theoretical results for the accuracy of the algorithm, and prove that both algorithms converges in distribution. The theoretical results show that the dual variable perturbation outperforms the primal case. The tradeoff between privacy and accuracy is examined in the numerical experiment. Our experiment shows that both algorithms performs similar in managing the privacy-accuracy tradeoff, and primal variable perturbaiton is slightly better than the dual case.",
    "creator" : "LaTeX with hyperref package"
  }
}