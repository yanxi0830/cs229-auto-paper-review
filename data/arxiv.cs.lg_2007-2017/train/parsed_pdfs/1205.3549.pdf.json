{
  "name" : "1205.3549.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Normalized Maximum Likelihood Coding for Exponential Family with Its Applications to Optimal Clustering",
    "authors" : [ "So Hirai", "Kenji Yamanishi" ],
    "emails" : [ "Hirai@mist.i.u-tokyo.ac.jp", "yamanishi@mist.i.u-tokyo.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 5.\n35 49\nv2 [\ncs .L\n∗Graduate School of Information Science and Technology, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, JAPAN Email: So Hirai@mist.i.u-tokyo.ac.jp He currently belongs to NTT DATA Corporation.\n†Graduate School of Information Science and Technology, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, JAPAN Email: yamanishi@mist.i.u-tokyo.ac.jp"
    }, {
      "heading" : "1 Introduction",
      "text" : ""
    }, {
      "heading" : "1.1 Motivation and Previous Works",
      "text" : "This paper addresses the issue of how to calculate the normalized maximum likelihood (NML) code-length for a given sequence. Suppose that we are given an n tuple of mdimensional data xn = (x1, · · · ,xn) ∈ X n, where each xi ∈ X ⊆ R m. We define the NML distribution fNML relative to a model class M = {f(X n; θ) : θ ∈ Θ} (n = 1, 2, · · · ) by\nfNML(x n;M) =\nf(xn; θ̂(xn,M))\nC(M) , (1)\nC(M) =\n∫\nf(xn; θ̂(xn),M)dxn,\nwhere Θ is a parameter spece and θ̂ is a maximum likelihood estimator of θ from xn. The NML code-length for xn relative to M is calculated as follows:\n− log fNML(x n;M) = − log f(xn; θ̂(xn,M)) + log C(M),\nIt is known from [8] that the NML code-length is optimal in the sense that it achieves the minimum of Shtarkov’s minimax criterion [12]. The NML code-length is called the stochastic complexity [8] and has been employed as a criterion for statistical model selection on the basis of the minimum description length (MDL) principle [10, 4]. However, there is a problem that the normalization term may diverge and a straightforward computation of the normalization term in the NML code-length is highly expensive. The purpose of this paper is twofold. One is to propose a method for efficient computing the NML code-length for the exponential family and Gaussian mixture models. The other is to demonstrate the validity of its applications to optimal clustering.\nRissanen [8] derived a formula of an asymptotic approximation of the NML code-length:\n− log p(xn; θ̂(xn)) + k\n2 log\nn\n2π + log\n∫\n√\n|I(θ)|dθ + o(1),\nwhere I(θ) is the Fisher information matrix. Note that this formula takes an asymptotic form. A method for exactly computing the NML code-length has been desired. In the case where the data domain is discrete, there is a problem that the time for a straightforward computation of the normalization term is exponential in data size even for the simplest case where the class of distributions is that of mutinomial distributions. Kontkanen and Myllymäki proposed efficient algorithms for the NML code-length for multinomial distributions and Näive Bayes model [6, 7]. Meanwhile, in the case where the data domain is continuous and not bounded, there is a problem that the normalization term may diverge for, e.g., Gaussian distributions. Rissanen proposed a method for circumventing this problem for linear regression models by making an elliptic constraint for the data domain so that the normalization term does not diverge [9]. Giurcăneanu et. al. proposed another\nmethod using an rhomboid constraint [3]. Note that all of these works [9, 3] considered 1- dimensional Gaussian distributions. Hirai and Yamanishi [5] applied Rissanen’s technique to the computation of the NML code-length for multi-variate Gaussian distributions.\nWe are specifically concerned with the applications of the NML code-length to clustering. A mixture model may be used as a probabilistic model of clustering where each mixture component corresponds to a cluster. The estimation of the mixture size is one of the most important issues in clustering. Kontkanen and Myllymäki [7] proposed an efficient algorithm for NML-based clustering with optimal choices of mixture size for the case where the data domain was discrete. Hirai and Yamanishi [5] proposed an algorithm for efficiently computing the NML code-length for Gaussian mixture models (GMM) for the case where the data domain was continuous."
    }, {
      "heading" : "1.2 Significance of This Paper",
      "text" : ""
    }, {
      "heading" : "1) An extension of the computation of the NML code-length to the exponential family.",
      "text" : "We extend Hirai and Yamanishi’s method [5] for computing the NML code-length for Gaussian distributions and GMMs to exponential family including Gamma distributions, logistic distributions, etc. Then we give a method for calculating the NML code-length in a general form.\n2) An improvement of the NML code-length for Gaussian distributions and GMMs using the renormalizing technique. We apply Rissanen’s renormalizing technique [9] into Gaussian distributions and GMMs to derive new formulas for computing the NML codelengths for them. Conventional formulas in [5] depend on the parameters by which the data domain is restricted. The new formulas are obtained by renormalizing the likelihood with respect to the parameters, and are improved in that they are less dependent on hyperparameters than those in [5]. We call the resulting code-length the renormalized maximum likelihood code-length (RNML). Note that the RNML are different from Rissanen’s original one [9] in that they are derived for the case where data is multi-dimensional while Rissanen considered a specific case where it was 1-dimensional.\n3) An empirical demonstration of the superiority of RNML over other criteria in the clustering scenario. We apply the RNML code-length to the clustering scenario in which a GMM is used as a model for clustering. In it we employ artificial data sets to empirically demonstrate the validity of RNML in the estimation of the number of clusters. We show that the number of clusters chosen by the RNML-based criterion converges significantly faster to the true one than those chosen by other criteria such as AIC, BIC, and the original NML."
    }, {
      "heading" : "2 NML Code-Length for Exponential Family",
      "text" : "In this section, we introduce a method of computing the NML code-length for the exponential family."
    }, {
      "heading" : "2.1 Exponential Family",
      "text" : "Below we define the exponential family.\nDefinition 1 The probability density function belonging to the exponential family takes the following form:\nf(X ; θ) = h(X) exp { η(θ)TT (X)− A(η(θ)) } , (2)\nwhere θ ∈ RD is a real-valued parameter vector (D is the number of parameters) and A(η) is a normalization term.\nThe joint distribution of data xn is given as follows:\nf(xn; θ) = n ∏\ni=1\nh(xi) exp { η(θ)TT (xi)− A(η(θ)) } .\nThen the maximum likelihood estimate (MLE): θ̂(xn) satisfies:\nEη(θ̂(xn))[T (X)] = 1\nn\nn ∑\ni=1\nT (xi)."
    }, {
      "heading" : "2.2 NML Code-Length for Exponential Family",
      "text" : "Below we consider how to calculate the normalization term: C(M) as in (1) for the exponential family. Suppose that for any data, the MLE of θ from the data can analytically be obtained. It is known that for the exponential family, the MLE can be calculated as a function of sufficient statistics. Hence we may denote the MLE as follows:\nθ̂(xn) = Θ\n(\n1\nn\nn ∑\ni=1\nT (xi)\n)\n,\nwhere the Θ(x) is a certain function of x. Below we show how to calculate C(M) by circumventing the problem that it may diverge. The function to be integrated is expanded as follows:\nf(xn; θ) = h(xn|θ̂(xn))× exp {\nnηTθ Θ −1(θ̂(xn))− nA(ηθ)\n}\n= H(xn|θ̂(xn))×\nD ∏\nd=1\ngd(θ̂d(x n)|θ).\nHere we denote ηθ = η(θ) and define the function H(x n|θ̂(xn)) def = δ(θ̂(xn) = θ̂) (δ(·) is a delta function), and the gd(θ̂d(x n)|θ) is the distribution of the MLE for the d-th part of the parameter θd. Notice here that θd is not a component of θ but rather a part of it– a collection of components. We assume here that parameter parts {θd} are independent with respect to d. We fix θ̂(xn) = θ̂ and let\ng(θ̂) def =\nD ∏\nd=1\ngd(θ̂d|θ̂).\nWe can calculate the normalization term C(M) by integrating g(θ̂) with respect to θ̂ over the restricted domain as follows:\nC(M) =\n∫\nY (α)\ng(θ̂)dθ̂,\nwhere we restrict the domain for the integral to be Y (α) where α is a parameter by which the integral θ̂ is specified.\nIn summary, for the exponential family, the NML code-length can analytically be obtained provided that the following conditions are fulfilled:\n1. The MLE of θ can be calculated analytically.\n2. The integral of g(θ̂) with respect to θ̂ can analytically be obtained."
    }, {
      "heading" : "2.3 Examples",
      "text" : "Below we give examples of calculation of the NML code-lengths for the exponential family. For the sake of simplicity, we focus on the normalization term C(M) as in (1)."
    }, {
      "heading" : "2.3.1 Gamma Distributions",
      "text" : "Gamma distributions belong to the exponential family. The density function of xn for a Gamma distribution is defined as follows:\nf(xn; k, θ) = n ∏\ni=1\n1\nΓ(k) · θk · xk−1i · exp\n{ − xi θ } ,\nwhere k is a shape parameter and θ is a scale parameter. The MLE of θ can analytically be obtained. We consider the case where k is known and fixed. The MLE of θ is given by θ̂(xn) = ∑n\ni=1 xi/(kn). Thus the joint distribution of\nxn is given as follows:\nf(xn; k, θ) = 1\nΓ(k)n · θkn ·\nn ∏\ni=1\nxk−1i · exp\n{\n− 1\nθ\nn ∑\ni=1\nxi\n}\n= H(xn| k, θ̂(xn)) · g(θ̂(xn); k, θ),\nwhere θ̂ is distributed according to the Gamma distribution with a shape parameter kn and a scale parameter θ/(kn). Hence g(θ̂(xn); k, θ) is calculated as follows:\ng(θ̂(xn); k, θ) = (kn)kn · θ̂(xn)kn−1\nΓ(kn) · θkn · exp\n{\n− kn\nθ θ̂(xn)\n}\n.\nFix θ̂(xn) = θ̂ and let H(xn| k, θ̂(xn)) = δ(θ̂(xn) = θ̂). Then we have\ng(θ̂; k) def =g(θ̂; k, θ̂) =\n(kn)kn\nΓ(kn) · ekn · 1 θ̂ .\nLetting hyper-parameters be θmin, θmax and the domain be\nY (θmin, θmax) = { yn|θmin ≤ θ̂(y n) ≤ θmax } ,\nthe normalization term C(M) is obtained by taking an integral of g(θ̂; k) with respect to θ̂ over Y (θmin, θmax)as follows:\nC(M) = (kn)kn\nΓ(kn) · ekn\n∫ θmax\nθmin\n1 θ̂ dθ̂ =\n(kn)kn\nΓ(kn) · ekn log θmax θmin .\nHence, for fixed k, we obtain a finite value of C(M) for Gamma distributions."
    }, {
      "heading" : "2.3.2 Logistic Distributions",
      "text" : "The logistic distributions belong to the exponential family. The density function of xn for a logistic distribution with a parameter θ is defined as\nf(xn; θ) = n ∏\ni=1\nθe−xi\n(1 + e−xi)θ+1 .\nThe MLE of θ is analytically obtained as θ̂(xn) = n/( ∑n i=1 log(1 + e −xi)). Thus the joint density of xn is written as\nf(xn; θ) = θn · exp\n{\n−\nn ∑\ni=1\nxi − n(θ + 1)\nθ̂(xn)\n}\n= H(xn|θ̂(xn)) · g(θ̂(xn); θ),\nwhere n/θ̂(xn) is distributed according to the Gamma distribution with a shape parameter n and a scale parameter 1/θ. Thus g(θ̂(xn); θ) is written as\ng(θ̂(xn); θ) = θn\nΓ(n) ·\n(\nn\nθ̂(xn)\n)n−1\n· exp\n{\n− nθ\nθ̂(xn)\n}\n.\nFix θ̂(xn) = θ̂ and let H(xn|θ̂(xn)) = δ(θ̂(xn) = θ̂). Then we have\ng(θ̂) def =g(θ̂; θ̂) =\nnn−1\nΓ(n) · en · θ̂.\nLetting R be a parameter, we define the restricted domain as\nY (R) = { yn|θ̂(yn) ≤ R } . (3)\nThen the normalization term C(M) is obtained by taking an integral of g(θ̂) with respect to θ̂ as follows:\nC(M) = nn−1\nΓ(n) · en\n∫ R\n0\nθ̂ dθ̂ = nn−1\nΓ(n) · en R2.\nThus we obtain the normalization term C(M) that doesn’t diverge."
    }, {
      "heading" : "3 Re-normalized Maximum Likelihood",
      "text" : "We show how to compute the RNML code-length for a GMM. Let xn = (x1, · · · ,xn), xi = (xi1, · · · , xim)\n⊤ (i = 1, · · · , n) be a given sequence where xi is distributed according to a Gaussian distribution with mean µ ∈ Rm and variance-covariance matrix Σ ∈ Rm×m for a some positive integer m with density:\nf(x;µ,Σ) = 1\n(2π) m 2 |Σ| 1 2\nexp { − 1\n2 (x− µ)⊤Σ−1(x− µ)\n}\n.\nNotice here that the normalization term in (1) diverges. Hirai and Yamanishi [5] derived a formula of the NML distribution by restricting the range of data so that the maximum likelihood lies in a bounded range specified by parameters. It is given as follows:\nfNML(x n;R, λmin) def =\nf(xn; µ̂(xn), Σ̂(xn))\nC(R, λmin) ,\nwhere\nC(R, λmin) =\n∫\nY (R,λmin)\nf(yn; µ̂(yn), Σ̂(yn))dyn,\nY (R, λmin) def = {yn| ||µ̂(yn)||2 ≤ R, λ (j) min ≤ λ̂j(y n)\n(j = 1, · · · , m), yn ∈ X n}, (4)\nwhere R, λmin = (λ (1) min, · · · , λ (m) min) are parameters, and λ̂j(y n) is the j-th largest eigenvalue of Σ̂(yn). The normalization term C(R, λmin) is expanded as follows [5]:\nC(R, λmin) = 2m+1R\nm 2 ∏m j=1 λ (j) min\n− m\n2\nmm+1Γ(m 2 )\n× ( n\n2e\n) mn\n2 1\nΓm( n−1 2 ) ,\nIf we set the parameters: R, λmin to be bounded, then the normalization term is also bounded.\nNote here that the value of the normalization term depends on the choice of parameters: R, λmin. Next we consider the optimization of the NML code-length with respect to the parameters: R, λmin. That is, we choose the optimal parameters so that they achieve the minimum of the following NML code-length: − log fNML(x\nn;R, λmin). The values of R, λmin that make the NML code-length shortest can be considered as the maximum likelihood (ML) estimates from xn. The terms including R, λmin in the NML code-length are given as:\nm\n2 logR−\nm\n2\nm ∑\nj=1\nlog λ (j) min. (5)\nConsidering the range of parameters: (4), the ML estimates of R, λmin are given as follows:\nR̂(yn) = ||µ̂(yn)||2,\nλ̂ (j) min(y n) = λ̂j(y n) (j = 1, · · · , m).\nWe then introduce hyper parameters: γ = (λ1, λ2, R1, R2) and define the renormalized maximum likelihood (RNML) distribution by\nfRNML(x n; γ) =\nfNML(x n; γ, R̂(xn), λ̂min(x n))\nC(γ) ,\nwhere the normalization term is expanded as follows:\nC(γ) =\n∫\nY (γ)\nfNML(y n; γ, R̂(yn), λ̂min(y n))dyn,\nY (γ) = {yn| V ( √\nR1) ≤ V (\n√\nR̂(yn)) ≤ V ( √\nR2),\nλ1 ≤ λ̂ (j) min(y n) ≤ λ2 (j = 1, · · · , m), y n ∈ X n},\nwhere V (r) = 2π m 2 rm/(mΓ(m 2 )), which denotes the volume of the m-dimensional ball with radius r. The normalization term C(γ) is rewritten as\nC(γ) = (m\n2\n)m+1 · log R2 R1 · ( log λ2 λ1 )m .\nThe terms including the hyper-parameters R1, R2, λ1, λ2 in the RNML code-length are given by\nlog log R2 R1 +m log log λ2 λ1 ,\nwhile those including the parameters R, λ (j) min in the NML code-length are given by (5). Comparing them each other, we see that the dependency of the RNML code-length on the hyper parameters is lower than that of the NML code-length on the parameters by logarithmic order.\nWe further give a new formula of the RNML code-length relative to a GMM.\nTheorem 2 The RNML code-length of xn relative to a GMM is expanded as follows:\nℓRNML(x n, zn; γ,K)\n= − log f(xn, zn;K, µ̂(xn, zn), Σ̂(xn, zn)) + log C1(K,n)\n+ log C2(K,n) + logB(x n, zn) +K log I(m,γ),\nwhere\nC1(K,n) = ∑\nh1+···+hK=n\nn!\nh1! · · · hK !\nK ∏\nk=1\n(hk\nn\n)hk , (6)\nC2(K,n) = ∑\nh1+···+hK=n\nn!\nh1! · · · hK !\nK ∏\nk=1\n(hk\nn\n)hk · J(hk),\n(7)\nB(xn, zn) =\nK ∏\np=1\n2m+1 · ||µ̂p(x n, zn)||m · |Σ̂p(x n, zn)|− m 2\nmm+1Γ(m2 ) ,\nI(m,γ) = C(γ) = (m\n2\n)m+1 · log R2 R1 · ( log λ2 λ1 )m ,\nJ(hk) = (hk\n2e\n)mhk ·\n1\nΓm( hk−1 2 ) . (8)\nHere hk denotes the number of data belonging to the k-th cluster, and µ̂p, Σ̂p denote mean and the ML estimates of the variance-covariance matrix for the p-th cluster.\nNote that straightforward computation of C1(K, n) and C2(K, n) as in (6) and (7) requires O(nK) time. Below we give methods for efficient computation of C1(K, n) and C2(K, n). As for the computation of C1(K, n), Kontkanen and Myllymäki proved the following theorem:\nTheorem 3 [6] C1(K, n) satisfies the recursive formula:\nC1(K + 2, n) = C1(K + 1, n) + n\nK C1(K, n). (9)\nHence C1(K, n) is computed in time O(n+K).\nAs for the computation of C2(K, n), we newly give the following result:\nTheorem 4 C2(K, n) satisfies the following formula:\nC2(K + 1, n) = ∑\nr1+r2=n\nnCr1 (r1 n )r1 (r2 n )r2 C2(K, r1)J(r2), (10)\nwhere J(r2) is as in (8). Hence C2(K, n) is computed in time O(n 2K).\nCombining all of the theorem as above, we see that the RNML code-length of xn relative to a GMM is computed in time O(n2K)."
    }, {
      "heading" : "4 Experimental Results",
      "text" : ""
    }, {
      "heading" : "4.1 Comparison with AIC and BIC",
      "text" : "This section gives experimental results showing the validity of the RNML for GMMs. We generated a number of data sequences of size n according to the true GMM M of mixture size K. Each mixture component is a Gaussian distribution with mean µk and variancecovariance matrix Σk (k = 1, · · · , K). For each data sequence x\nn generated according to the true model M, we also generated their corresponding cluster indices zn using the EM algorithm [2], where zi showed which cluster xi came from (i = 1, . . . , n). In our experiment, we repeated cluster generation using the EM algorithm 100 times by changing initial values of the algorithm. We compared the four criteria: RNML, NML, Akaike’s Information Criterion (AIC) [1] and Bayesian Information Criterion (BIC) [11] for the choice of the number of clusters. We calculated RNML and NML according to the method proposed in the previous sections and [5]. We calculated AIC and BIC as follows:\nAIC(xn, zn;K) = −2 log f(xn, zn;K, θ̂(xn, zn))\n+m(m+ 3)K +K,\nBIC(xn, zn;K) = −2 log f(xn, zn;K, θ̂(xn, zn))\n+ m(m+ 3)K\n2\nK ∑\nk=1\nlog hk +K log n.\nWe measured their performance in terms of the identification probability P (K) and the benefit B(K) defined as follows: Letting K be the true number of clusters and K∗ be the one chosen using any criterion,\nP (K) = Prob(K∗ = K),\nB(K) = max\n{\n0, 1− |K∗ −K|\nT\n}\n, (11)\nwhere T is a given constant. The identification probability P (K) is the probability that the algorithm outputs the true number of clusters. The benefit is a score assigned to K so that if K = K∗ it takes the maximum value 1, and it decreases linearly to zero as |K∗−K| increases to T . The resulting benefit is calculated as the average of the benefits taken over all of random generation. We compared RNML, AIC, and BIC in terms of how fast the identification probability and the benefit converge as sample size n increases.\nFig. 1 and Fig. 2 show graphs of accuracy rates and benefit vs data size for the case where the data dimension was m = 5 and the true number of clusters was K = 3. Here we set T = 2 in the calculation of B(K) in (11).\n0 500 1000 1500 2000 0\n0.2\n0.4\n0.6\n0.8\n1\nsample size\nA cc\nur ac\ny R\nat e\n3 clusters, 5 dimension\nAIC BIC NML RNML\nFigure 1: Accuracy Rates\n0 500 1000 1500 2000 0\n0.2\n0.4\n0.6\n0.8\n1\nsample size\nbe ne\nfit\n3 clusters, 5 dimension\nAIC BIC NML RNML\nFigure 2: Benefit\n0 5 10 15 20 25 30 35 40 200\n400\n600\n800\n1000\n1200\n1400\n1600\nparameter 10x\nda ta\ns iz\ne re\nqu ire\nd fo\nr A\nR /b\nen ef\nit to\ne xc\nee d\n0. 8\nparameter and data size required for AR/benefit to exceed 0.8\nAR , NML AR , RNML benefit , NML benefit , RNML\nFigure 3: Data Size Required for Accuracy Rate/benefit to Exceed 0.8 vs Parameter Values\nWe see from these results that RNML achieved the highest identification probability, the highest benefit, and the fastest rate of convergence among all of the criteria: AIC, BIC, NML, and RNML. Specifically this was the case when the data size was not so large. This\nimplies that RNML was effective as a criterion for selecting an optimal number of clusters even when the data size was relatively small.\nTable 1, 2, 3, and 4 show the results on benefit obtained by varying the data dimension and the true number of clusters, where each numerical value in Tables indicates the least data size required for benefit to exceed 0.8. Here Inf shows that benefit did not exceed 0.8.\nWe see from these results that for most of pairs of m and K, RNML achieves high benefit with smaller data size than AIC, BIC, and NML. This implies that the number of clusters estimated by RNML is within ±1 of the true one with sufficiently high probability."
    }, {
      "heading" : "4.2 Dependency of NML and RNML on Parameters",
      "text" : "Fig.3 shows graphs of least data size required for accuracy rate and benefit to achieve 80% and 0.8 versus parameter values, respectively. We define parameter θ as θ = R2/R1 = λ2/λ1 in RNML, and θ = R = λ (j) min −m in NML. We see that the RNML do not depend on parameter values more than NML. It implies that the dependency of RNML on parameter values is much less than that of NML."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have proposed a general method for computing the NML code-length for the exponential family.We have developed it by generalizing the existing method for restricting the data domain so that the NML code-length does not diverge. We have specifically focused on Gaussian distributions and GMMs to propose a new efficient method for computing the RNML for them. We have developed it by extending Rissanen’s renormalizing technique into multi-variate Gaussian distributions. We have applied this method to the clustering\nissue, in which we have selected the optimal number of clusters on the basis of the RNML code-length. We have empirically demonstrated using artificial data that our method makes the estimate of the number of clusters converge significantly faster to the true one than AIC, BIC, and NML."
    } ],
    "references" : [ {
      "title" : "A new look at the statistical model identification",
      "author" : [ "H. Akaike" ],
      "venue" : "IEEE Trans. on Automatic Control,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1974
    }, {
      "title" : "Maximum likelihood from incomplete data via the em",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.B. Rubin" ],
      "venue" : "J.Royal Staitst. Soc.B, 39:1–38",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Variable selection in linear regression: Several approaches based on normalized maximum likelihood",
      "author" : [ "C.D. Giurcăneanu", "S.A. Razavi", "A. Liski" ],
      "venue" : "Signal Processing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "The Minimum Description Length Principle",
      "author" : [ "P.D. Grünwald" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "Efficient computation of normalized maximum likelihood coding for gaussian mixtures with its applications to optimal clustering",
      "author" : [ "S. Hirai", "K. Yamanishi" ],
      "venue" : "The IEEE ISIT, pages 1031–1035",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A linear time algorithm for computing the multinomial stochastic complexity",
      "author" : [ "P. Kontkanen", "P. Myllymäki" ],
      "venue" : "Information Processing Letters, 103:227–233",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "An empirical comparison of nml",
      "author" : [ "P. Kontkanen", "P. Myllymäki" ],
      "venue" : "Proceedings of the 2008 International, pages 125–131",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Fisher information and stochastic complexity",
      "author" : [ "J. Rissanen" ],
      "venue" : "IEEE Trans. on Information Theory,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1996
    }, {
      "title" : "MDL denoising",
      "author" : [ "J. Rissanen" ],
      "venue" : "IEEE Trans. on Information Theory,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2000
    }, {
      "title" : "Information and Complexity in Statistical Modeling",
      "author" : [ "J. Rissanen" ],
      "venue" : "Springer",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Estimating the dimension of a model",
      "author" : [ "G. Schwarz" ],
      "venue" : "Annals of Statistics 6 (2), pages 461–464",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Universal sequential coding of single messages",
      "author" : [ "M. Shtarkov Yu" ],
      "venue" : "Problems of Information Transmission,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1987
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "The NML code-length for x relative to M is calculated as follows: − log fNML(x ;M) = − log f(x; θ̂(x,M)) + log C(M), It is known from [8] that the NML code-length is optimal in the sense that it achieves the minimum of Shtarkov’s minimax criterion [12].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "The NML code-length for x relative to M is calculated as follows: − log fNML(x ;M) = − log f(x; θ̂(x,M)) + log C(M), It is known from [8] that the NML code-length is optimal in the sense that it achieves the minimum of Shtarkov’s minimax criterion [12].",
      "startOffset" : 248,
      "endOffset" : 252
    }, {
      "referenceID" : 7,
      "context" : "The NML code-length is called the stochastic complexity [8] and has been employed as a criterion for statistical model selection on the basis of the minimum description length (MDL) principle [10, 4].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : "The NML code-length is called the stochastic complexity [8] and has been employed as a criterion for statistical model selection on the basis of the minimum description length (MDL) principle [10, 4].",
      "startOffset" : 192,
      "endOffset" : 199
    }, {
      "referenceID" : 3,
      "context" : "The NML code-length is called the stochastic complexity [8] and has been employed as a criterion for statistical model selection on the basis of the minimum description length (MDL) principle [10, 4].",
      "startOffset" : 192,
      "endOffset" : 199
    }, {
      "referenceID" : 7,
      "context" : "Rissanen [8] derived a formula of an asymptotic approximation of the NML code-length: − log p(x; θ̂(x)) + k 2 log n 2π + log ∫",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 5,
      "context" : "Kontkanen and Myllymäki proposed efficient algorithms for the NML code-length for multinomial distributions and Näive Bayes model [6, 7].",
      "startOffset" : 130,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "Kontkanen and Myllymäki proposed efficient algorithms for the NML code-length for multinomial distributions and Näive Bayes model [6, 7].",
      "startOffset" : 130,
      "endOffset" : 136
    }, {
      "referenceID" : 8,
      "context" : "Rissanen proposed a method for circumventing this problem for linear regression models by making an elliptic constraint for the data domain so that the normalization term does not diverge [9].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 2,
      "context" : "method using an rhomboid constraint [3].",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : "Note that all of these works [9, 3] considered 1dimensional Gaussian distributions.",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "Note that all of these works [9, 3] considered 1dimensional Gaussian distributions.",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "Hirai and Yamanishi [5] applied Rissanen’s technique to the computation of the NML code-length for multi-variate Gaussian distributions.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "Kontkanen and Myllymäki [7] proposed an efficient algorithm for NML-based clustering with optimal choices of mixture size for the case where the data domain was discrete.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "Hirai and Yamanishi [5] proposed an algorithm for efficiently computing the NML code-length for Gaussian mixture models (GMM) for the case where the data domain was continuous.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "We extend Hirai and Yamanishi’s method [5] for computing the NML code-length for Gaussian distributions and GMMs to exponential family including Gamma distributions, logistic distributions, etc.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 8,
      "context" : "We apply Rissanen’s renormalizing technique [9] into Gaussian distributions and GMMs to derive new formulas for computing the NML codelengths for them.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 4,
      "context" : "Conventional formulas in [5] depend on the parameters by which the data domain is restricted.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "The new formulas are obtained by renormalizing the likelihood with respect to the parameters, and are improved in that they are less dependent on hyperparameters than those in [5].",
      "startOffset" : 176,
      "endOffset" : 179
    }, {
      "referenceID" : 8,
      "context" : "Note that the RNML are different from Rissanen’s original one [9] in that they are derived for the case where data is multi-dimensional while Rissanen considered a specific case where it was 1-dimensional.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "Hirai and Yamanishi [5] derived a formula of the NML distribution by restricting the range of data so that the maximum likelihood lies in a bounded range specified by parameters.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "The normalization term C(R, λmin) is expanded as follows [5]: C(R, λmin) = 2R m 2 ∏m j=1 λ (j) min − m 2",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "As for the computation of C1(K, n), Kontkanen and Myllymäki proved the following theorem: Theorem 3 [6] C1(K, n) satisfies the recursive formula: C1(K + 2, n) = C1(K + 1, n) + n K C1(K, n).",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "For each data sequence x n generated according to the true model M, we also generated their corresponding cluster indices z using the EM algorithm [2], where zi showed which cluster xi came from (i = 1, .",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 0,
      "context" : "We compared the four criteria: RNML, NML, Akaike’s Information Criterion (AIC) [1] and Bayesian Information Criterion (BIC) [11] for the choice of the number of clusters.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : "We compared the four criteria: RNML, NML, Akaike’s Information Criterion (AIC) [1] and Bayesian Information Criterion (BIC) [11] for the choice of the number of clusters.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 4,
      "context" : "We calculated RNML and NML according to the method proposed in the previous sections and [5].",
      "startOffset" : 89,
      "endOffset" : 92
    } ],
    "year" : 2012,
    "abstractText" : "We are concerned with the issue of how to calculate the normalized maximum likelihood (NML) code-length. There is a problem that the normalization term of the NML code-length may diverge when it is continuous and unbounded and a straightforward computation of it is highly expensive when the data domain is finite . In previous works it has been investigated how to calculate the NML code-length for specific types of distributions. We first propose a general method for computing the NML code-length for the exponential family. Then we specifically focus on Gaussian mixture model (GMM), and propose a new efficient method for computing the NML to them. We develop it by generalizing Rissanen’s re-normalizing technique. Then we apply this method to the clustering issue, in which a clustering structure is modeled using a GMM, and the main task is to estimate the optimal number of clusters on the basis of the NML code-length. We demonstrate using artificial data sets the superiority of the NML-based clustering over other criteria such as AIC, BIC in terms of the data size required for high accuracy rate to be achieved. Graduate School of Information Science and Technology, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, JAPAN Email: So Hirai@mist.i.u-tokyo.ac.jp He currently belongs to NTT DATA Corporation. Graduate School of Information Science and Technology, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, JAPAN Email: yamanishi@mist.i.u-tokyo.ac.jp",
    "creator" : "LaTeX with hyperref package"
  }
}