{
  "name" : "1612.05231.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNN",
    "authors" : [ "Li Jing", "Yichen Shen", "John Peurifoy", "Scott Skirlo" ],
    "emails" : [ "LJING@MIT.EDU", "YCSHEN@MIT.EDU", "TENAD@MIT.EDU", "JPEURIFO@MIT.EDU", "SSKIRLO@MIT.EDU", "TEGMARK@MIT.EDU", "SOLJACIC@MIT.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Deep Neural Networks (LeCun et al., 2015) have been successful on numerous difficult machine learning tasks, including image recognition (Krizhevsky et al., 2012; Donahue et al., 2015), speech recognition (Hinton et al., 2012) and natural language processing (Collobert et al., 2011; Bahdanau et al., 2014; Sutskever et al., 2014). However, deep neural networks have long suffered from vanishing and exploding gradient problems (Hochreiter, 1991; Bengio et al., 1994), which are known to be caused by ma-\ntrix eigenvalues far from unity being raised to large powers. Because the severity of these problems grows with the the depth of a neural network, they are particularly grave for Recurrent Neural Networks (RNNs), whose recurrence can be equivalent to thousands or millions of equivalent hidden layers.\nSeveral solutions have been proposed to solve these problems for RNNs. Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), which helps RNNs contain information inside hidden layers with gates, remains one of the the most popular RNN implementations. Other recently proposed methods such as GRU (Cho et al., 2014) and Bidirectional RNNs (Berglund et al., 2015) also perform well in numerous applications. However, none of these approaches has fundamentally solved the vanishing and exploding gradient problems, and gradient clipping is often required to keep gradients in a reasonable range.\nA recently proposed solution strategy is using orthogonal weight matrices or their complex generalization (unitary matrices) (Saxe et al., 2013; Le et al., 2015; Arjovsky et al., 2015), because all their eigenvalues will then have absolute values of unity, and can safely be raised to large powers. This has been shown to help both when weight matrices are initialized to be unitary (Saxe et al., 2013; Le et al., 2015) and when they are kept unitary during training, either by restricting them to a more tractable matrix subspace (Arjovsky et al., 2015) or by alternating gradient-descent steps with projections onto the unitary subspace (Wisdom et al.,\nar X\niv :1\n61 2.\n05 23\n1v 1\n[ cs\n2016).\nIn this paper, we will first present an Efficient Unitary Neural Network (EUNN) architecture that parametrizes the entire space of unitary matrices in a systematic, complete and computationally efficient way, thereby eliminating the need for time-consuming unitary subspace-projections. Our architecture enables a wide range of capacity-tunability to represent subspace unitary models by fixing some of our parameters; the above-mentioned unitary subspace models correspond to special cases of our architecture.\nWe then benchmark EUNN’s performance on the standard copying task and MNIST task, and find that for the copying task, a small subspace of the full unitary space is preferred, while for the MNIST task, a tuning closer to the full unitary space perform better. We show that our optimally tuned architecture significantly outperforms the above-mentioned non-unitary and unitary methods. We show that our EUNN algorithm with an O(N) hidden layer size can compute up to the entire N × N gradient matrix using O(1) computational steps and memory access per parameter. This is superior to the O(N) computational complexity of the existing training method for a full-space unitary network (Wisdom et al., 2016) and O(logN) more efficient than the subspace Unitary RNN (Arjovsky et al., 2015)."
    }, {
      "heading" : "2. Background",
      "text" : ""
    }, {
      "heading" : "2.1. Basic Recurrent Neural Networks",
      "text" : "A recurrent neural network takes an input sequence and uses the current hidden state to generate a new hidden state during each step, memorizing past information in the hidden layer. We first review the basic RNN architecture.\nConsider an RNN updated at regular time intervals t = 1, 2, ... whose input is the sequence of vectors x(t) and a whose hidden layer h(t) is updated according to the following rule:\nh(t) = σ(Ux(t) + Wh(t−1) + a), (1)\nwhere σ is the nonlinear activation function (see Figure 1) and a the hidden-to-hidden-layer bias vector. The output is generated by\ny(t) = softmax(Wh(t) + b), (2)\nwhere b is the hidden-to-output-layer bias vector. For t = 0, the hidden layer h(0) can be initialized to some special vector or set as a trainable variable. For convenience of notation, we define z(t) = Ux(t) + Wh(t−1) + a so that h(t) = σ(z(t))."
    }, {
      "heading" : "2.2. The Vanishing and Exploding Gradient Problems",
      "text" : "When training the neural network to minimize a cost function C that depends on a parameter vector a, the gradient descent method updates this vector to a− λ∂C∂a , where λ is a fixed learning rate and ∂C∂a ≡ ∇C. For an RNN, the vanishing or exploding gradient problem only occurs during back propagation from hidden to hidden layers, so we will only focus on the gradient for hidden layers. Training the input-to-hidden and hidden-to-output matrices is relatively trivial once the hidden-to-hidden matrix has been successfully optimized.\nIn order to evaluate ∂C∂Wij , one first computes the derivative ∂C ∂h(t) using the chain rule:\n∂C\n∂h(t) =\n∂C ∂h(T ) ∂h(T ) ∂h(t) (3)\n= ∂C\n∂h(T ) T−1∏ k=t ∂h(k+1) ∂h(k) (4)\n= ∂C\n∂h(T ) T−1∏ k=t D(k)W−1, (5)\nwhere D(k) = diag{σ′(Ux(k) + Wh(k−1) + a)} is the Jacobian matrix of the pointwise nonlinearity. For large times T , the term ∏ W−1 plays a significant role. As long as the eigenvalues of D(k) are of order unity, then if W has eigenvalues λi 1, they will cause gradient explosion | ∂C ∂h(T )\n| → ∞, while if W has eigenvalues λi 1, they can cause gradient vanishing, | ∂C\n∂h(T ) | → 0. Either situation\nprevents the RNN from working efficiently."
    }, {
      "heading" : "3. Unitary RNNs",
      "text" : ""
    }, {
      "heading" : "3.1. Partial Space Unitary RNNs",
      "text" : "In a breakthrough paper, Arjovsky, Shah & Bengio (Arjovsky et al., 2015) shown that unitary RNNs can overcome the exploding and vanishing gradient problems and perform well on long term memory tasks if the hiddento-hidden matrix in parametrized in the following unitary form:\nW = D3T2F−1D2ΠT1FD1. (6)\nHere D1,2,3 are diagonal matrices with each element eiωj , j = 1, 2, · · · , n. T1,2 are reflection matrices, and T = I−2 v̂v̂ †\n||v̂||2 , where v̂ is a vector with each of its entries as a parameter to be trained. Π is a permutation matrix. F and F−1 are Fourier and inverse Fourier transform matrices respectively. Since each factor matrix here is unitary, the product W is a also unitary matrix.\nThis model uses O(N) parameters, which spans merely a part of the whole O(N2)-dimensional space of unitary N × N matrices to enable computational efficiency. Several subsequent papers have tried to expand the space to O(N2) in order to achieve better performance, as summarized below."
    }, {
      "heading" : "3.2. Full Space Unitary RNNs",
      "text" : "In order to maximize the power of Unitary RNNs, it is preferable to optimize the weight matrix W over the full space of unitary matrices rather than a subspace as above. A straightforward method for implementing this is by simply updating W with standard back-propagation and then projecting the resulting matrix (which will typically no longer be unitary) back onto to the space of unitary matrices. Defining Gij ≡ ∂C∂Wij as the gradient with respect to W, this can be implemented by the procedure defined by (Wisdom et al., 2016):\nA(t) ≡ G(t) † W(t) −W(t) † G(k), (7) W(t+1) ≡ ( I + λ\n2 A(t)\n)−1( I− λ\n2 A(t)\n) W(t).(8)\nThis method shows that full space unitary networks are superior on many RNN tasks (Wisdom et al., 2016). A key limitation is that the back-propation in this method cannot avoid N -dimensional matrix multiplication, incurring an O(N3) computational cost."
    }, {
      "heading" : "4. Efficient Unitary Neural Network (EUNN) Architectures",
      "text" : "In the following, we first describe a general parametrization method able to represent arbitrary unitary matrices with up to N2 degrees of freedom. We then present an efficient algorithm for doing back-propagation in this parametrization scheme, requiring only O(1) computational and memory access steps to obtain the gradient for each parameter. Finally, we show that our scheme performs significantly better than the above mentioned methods on two well-known benchmarks."
    }, {
      "heading" : "4.1. Unitary Matrix Parametrization",
      "text" : "Any N × N unitary matrix UN can be represented as a product of rotation matrices {Rij} and a diagonal matrix\nD, such that UN = D ∏N i=2 ∏i−1 j=1 Rij , where Rij denotes the N -dimensional identity matrix with the elements Rii, Rij , Rji and Rjj replaced as follows (Reck et al., 1994; Clements et al.):(\nRii Rij Rji Rjj\n) = ( eiφij cos θij −eiφij sin θij\nsin θij cos θij\n) . (9)\nRij is thus parametrized by the two real numbers θij and φij . Each of these matrices Rij performs a U(2) unitary transformation on a two-dimensional subspace of the Ndimensional Hilbert space, leaving an (N−2)-dimensional subspace unchanged. In other words, a series of U(2) rotations can be used to successively make all off-diagonal elements of the given N × N unitary matrix zero. This generalizes the familiar factorization of a 3D rotation matrix into 2D rotations parametrized by the three Euler angles. To provide intuition for how this works, let us briefly describe a simple way of doing this that is similar to Gaussian elimination by finishing one column at a time. There are infinitely many alternative decomposition schemes as well; Fig. 2 shows two that are particularly convenient to implement in software (and even in neuromorphic hardware (Shen et al., 2016)). The unitary matrix UN is multiplied from the right by a succession of unitary matrices RNj for j = N − 1, · · · , 1. Once all elements of the last row except the one on the diagonal are zero, this row will not be affected by later transformations. Since all transformations are unitary, the last column will then also contain only zeros except on the diagonal:\nUNRN,N−1RN,N−2 · ·RN,1 =\n=\n( UN−1 0\n0 eiwN\n) . (10)\nThe effective dimensionality of the the matrix U is thus reduced toN−1. The same procedure can then be repeated N − 1 times until the effective dimension of U is reduced to 1, leaving us with a diagonal matrix:1\nUNRN,N−1RN,N−2 · · ·Ri,jRi,j−1 · · ·R3,1R2,1 = D, (11) where D is a diagonal matrix whose diagonal elements are eiwj , from which we can write the direct representation of UN as\nUN = DR −1 2,1R −1 3,1 . . .R −1 N,N−2R −1 N,N−1\n= DR′2,1R ′ 3,1 . . .R ′ N,N−2R ′ N,N−1. (12)\n1Note that Gaussian Elimination would make merely the upper triangle of a matrix vanish, requiring a subsequent series of rotations (complete Gauss-Jordan Elimination) to zero the lower triangle. We need no such subsequent series because since U is unitary: it is easy to show that if a unitary matrix is triangular, it must be diagonal.\nwhere\nR′ij = R(−θij ,−φij) = R(θij , φij)−1 = R−1ij . (13)\nThis parametrization thus involves N(N − 1)/2 different θij-values, N(N − 1)/2 different φij-values and N different wi-values, combining to N2 parameters in total and spans the entire unitary space. Note that we can always fix a portion of our parameters to span only a unitary subspace. Our benchmark test below will show that for certain tasks, parametrization of the full unitary space is unnecessary 2."
    }, {
      "heading" : "4.2. Efficient Implementation of Unitary Matrix",
      "text" : "The representation in Eq. (12) can be made more compact by reordering and grouping specific rotational matrices, as has been shown in the optical community (Reck et al., 1994; Clements et al.) in the context of universal multiport interferometers. For example, a unitary matrix can be decomposed as (Clements et al.)\nUN = D ( R (1) 1,2R (1) 3,4 . . .R (1) N/2−1,N/2 ) × ( R\n(1) 2,3R (1) 4,5 . . .R (1) N/2−2,N/2−1 ) × . . .\n= DF (1) A F (2) B . . .F (L) B , (14)\n2for certain tasks, based on our experimental result, we found full unitary space is not even desired\nwhere every\nF (l) A = R (l) 1,2R (l) 3,4 . . .R (l) N/2−1,N/2\n=  R (l) 1,2 0 0 · · · 0 0 R (l) 3,4 0 · · · 0 0 0 . . . · · · 0\n0 0 · · · R(l)N/2−3,N/2−2 0 0 0 · · · 0 R(l)N/2−1,N/2\n (15)\nis a block diagonal matrix, with N angle parameters in total, and\nF (l) B = R (l) 2,3R (l) 4,5 . . .R (l) N/2−2,N/2−1\n=  1 0 0 · · · 0 0 R (l) 2,3 0 · · · 0 0 0 . . . · · · 0\n0 0 · · · R(l)N/2−2,N/2−1 0 0 0 · · · 0 1\n (16)\nwithN−1 parameters, as is schematically shown in Fig. 2.\nFollowing this physics-inspired scheme, we decompose our unitary hidden-to-hidden layer matrix W as\nW = DF (1) A F (2) B F (3) A F (4) B · · ·F (L) B . (17)\nNote that by choosing different L-values, UN will span a different subspace of the unitary space, and when L = N , UN will span the entire unitary space.\nTo implement this decomposition efficiently in an RNN, we apply vector element-wise (Hadamard) multiplications and permutations: we evaluate the product F(l)A x as\nF (l) A x = v1 ◦ x + v2 ◦ swap(x) (18)\nwhere ◦ represents element-wise multiplication, and\nv1 = (e iφ (l) 1 cos θ (l) 1 , cos θ (l) 1 , e iφ (l) 1 cos θ (l) 2 , cos θ (l) 2 , · · · ), (19)\nv2 = (−eiφ (l) 1 sin θ (l) 1 , sin θ (l) 1 ,−eiφ (l) 2 sin θ2, sin θ (l) 2 , · · · ), (20)\nswap(x) = (x2, x1, x4, x3, x6, x5, · · · ). (21)\nIn detail, the pseudo-code for implementing the operation F is as follows:\nSince the operations ◦ and swap takeO(N) computational steps, evaluating F(l)A x only requires O(N) steps. For the same reason, the evaluation of the product F(l)B x also takes O(N) computation steps. The product Dx is trivial, consisting if an element-wise vector multiplication. Therefore, the product with the total unitary matrix, Wx can be computed in onlyO(N2) steps, and only requireO(N2) memory access.\nAlgorithm 1 Forward propagation for operation F with parameters θi and φi.\nInput: x (size N ); parameters θ and φ (size N/2). Output: y (size N ). for k = 1 to N/2 do y2k−1 ← x2k−1 exp(iφk) cos θk + x2k cos θk y2k ← −x2k−1 exp(iφk) sin θk + x2k sin θk end for\nCrucially, the back propagation of this algorithm can be performed with the same O(N2) efficiency in terms of number of steps and memory access, as demonstrated by the following pseudo-code:\nAlgorithm 2 Back propagation for operation F with parameters θi and φi.\nInput: original x (size N ), original output y (size N ), parameters θ and φ (size N/2), gradient dy of output (size N ). Output: gradient dx of input, gradients dθ and dφ of parameters. for k = 1 to N/2 do dθk ← −dy2k−1(exp(−iφk) sin θkx2k−1 − exp(−iφk) cos θkx2k)\ndφk ← dy2k−1i(exp(−iφk) cos θk − exp(−iφk) sin θk)\ndx2k−1 ← dy2k−1 exp(iφi) cos θk + dy2k cos θk\ndx2k ← dy2k−1(− exp(iφk) sin θk) + dy2k sin θk end for"
    }, {
      "heading" : "5. Experimental test of our method",
      "text" : "In this section, we compare the performance of our Efficient Unitary Recurrent Neural Network (EURNN) with\n1. an LSTM RNN (Hochreiter & Schmidhuber, 1997),\n2. a Partial Space URNN (Arjovsky et al., 2015), and\n3. a Projective URNN (Wisdom et al., 2016).\nWe use the learning rate 10−3 and the decay rate 0.9. For the LSTM, the clipping gradient is set to unity."
    }, {
      "heading" : "5.1. Memory Copying Task",
      "text" : "We first compare these networks by applying them all to the so-called Memory Copying Task, which remains the most characteristically difficult task for RNNs (Hochreiter & Schmidhuber, 1997; Arjovsky et al., 2015; Henaff et al.,\n2016). Fig. 3 plots the performance accuracy (loss) of each network as a function of the number of time steps, and shows that our EURNN has the best performance for memorizing extremely long term information and has the fastest learning speed in terms of number of steps.\nThe copy task tests the network’s ability to remember information seen T time steps earlier, as illustrated by the following example where T = 20 and M = 5:\nInput: BACCA--------------------:----Output: --------------------------BACCA\nSpecifically, the task is defined as follows (Hochreiter & Schmidhuber, 1997; Arjovsky et al., 2015; Henaff et al., 2016). An alphabet consists of symbols {ai}, the first n of which represent data, and the remaining two representing “blank” and “start recall”, respectively; in the above example, n = 3 and {ai} = {A,B,C,−, :}. The input consists of M random data symbols (M = 5 above) followed by T − 1 blanks, the “start recall” symbol and M more blanks. The desired output consists of M + T blanks followed by the data sequence. The cost function C is defined as the cross entropy of the input and output sequences, which vanishes for perfect performance. A useful baseline performance is that of the memoryless strategy, which outputs M + T blanks followed by M random data symbols and produces a cross entropy\nC = M log n\nT + 2M . (22)\nThe popular LSTM RNN architecture suffers from long term memory problems and only performs well on the copy task for small time delays T , while URNNs have been shown to succeed up to T = 500 but not for T > 1000 (Arjovsky et al., 2015). In our numerical experiment here, we use n = 8 data symbols and input length M = 10, which corresponds to the same amount of information as (Arjovsky et al., 2015) and (Wisdom et al., 2016). The symbol for each input is represented by an n-dimensional one-hot vector, whose elements are all set to 0 except the one corresponding to the symbol, which is set to 1. We trained all five RNNs for T = 200 and T = 1000 with the same batch size 128 using RMSProp optimization (Fig3). The result shows that the LSTM does not manage to beat the memoryless baseline performance, whereas the unitary RNN’s do. Moreover, we see that by choosing smaller L (so that our W spans a smaller subspace of unitary space), the EURNN converges toward optimal performance significantly more efficiently (and also faster in wall clock time) than the partial (Arjovsky et al., 2015) and projective (Wisdom et al., 2016) unitary methods, and also perform more robustly. A full-capacity unitary matrix this appears unnec-\nessary in this application 3."
    }, {
      "heading" : "5.2. MNIST Task",
      "text" : "The MNIST handwriting recognition problem remains one of the most popular benchmarks for testing learning ability. 28×28 sized grayscale images of hand-written digits are classified with a target label between 0-9. Following the procedure of (Arjovsky et al., 2015; Wisdom et al., 2016), we apply RNN models to the MNIST handwriting data set by feeding one row of pixels to the RNN at each step. The\n3our intuitive explanation to this is in order to do well in this task the network only needs to remember each individual entries (while a low-capacity unitary matrix is sufficient for remembering), but do not need to remember the correlations between neighboring entries.\nRNN is thus fed 28 successive inputs of length 28 each, and the final outputs are probability distributions over the ten digits.\nWe find that for this task, larger L (larger capacity unitary space) is preferred. When we choose L = N (so W spans the full unitary space), the EURNN is seen to converge toward an optimal performance significantly more efficiently than for a smaller L cases, such as the partial unitary methods (Arjovsky et al., 2015) and LSTM (Hochreiter & Schmidhuber, 1997). A full-capacity unitary matrix thus appears preferable for the MNIST task 4."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We have presented a method for implementing an Efficient Unitary Neural Network (EUNN) whose computational cost is merely O(1) per parameter, which is O(logN) more efficient than the partial unitary method of (Arjovsky et al., 2015) and O(N) more efficient than the full unitary methods discussed above. It significantly outperforms existing RNN architectures on the standard Copy Task and MNIST Task using a comparable parameter count, hence demonstrating the highest recorded ability to memorize sequential information over long time periods.\nThis unitary training technique can also be applied to classification tasks with general fully connected neural networks; in the appendix we provide the the derivation of\n4Our intuitive explanation here is for MNIST task it not only need to remember each pixels in the picture, but also need to remember the correlation between different pixels (full-capacity matrix allow different pixels to talk to each other)\nan efficient back propagation algorithm for fully connected networks.\nWe want to emphasize the generality and tunability of our method. The ordering of the N(N − 1)/2 rotational U(2) matrices that we used above is merely one of many possibilities; we used it simply as a concrete example. Other ordering options that can result in spanning full unitary matrix space can be used for our algorithm as well, with identical speed and memory performance. If the unitary matrix is decomposed into a different product of matrices (Eqn. 12), our efficient backpropagation method is still applicable.\nAlso, if one wishes to use a smaller number of parameters covering only a subspace of all unitary matrices, a subset of the N(N − 1)/2 matrices Rij can easily be chosen, which corresponds to setting some of the parameters to fixed values. This means that, in comparison with most of the previous literature on unitary neural networks, the restriction to a smaller unitary subspace can be done in a controllable way. This tunability of the span of the unitary space and, correspondingly, the total number of parameters makes it possible to use different capacities for different tasks, thus opening the way to an optimal performance of the EUNN. For example, as we have shown, a small unitary subspace is preferred for the copying task, whereas the MNIST task is better performed by a EUNN covering a considerably larger unitary space.\nThis powerful and robust unitary RNN architecture appears promising for natural language processing because of its ability to efficiently handle even very high dimensionality.\nNote added: Recently, during the final proofreading phase of our manuscript, another paper that goes in a similar direction as ours (but parametrizing the unitary matrix by using reflections rather than rotations) was posted (Mhammedi et al., 2016)."
    }, {
      "heading" : "Acknowledgment",
      "text" : "We thank Hugo Larochelle, Yann LeCun, Luis Seoane and David Theurel for helpful discussions and comments. This work was supported in part by the Army Research Office through the Institute for Soldier Nanotechnologies under contract W911NF-13-D0001. The work of T.D. was also supported by the Institute of International Education through the Fulbright Scholarship."
    } ],
    "references" : [ {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo" ],
      "venue" : "IEEE transactions on neural networks,",
      "citeRegEx" : "Bengio et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "Bidirectional recurrent neural networks as generative models",
      "author" : [ "Berglund", "Mathias", "Raiko", "Tapani", "Honkala", "Mikko", "Kärkkäinen", "Leo", "Vetek", "Akos", "Karhunen", "Juha T" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Berglund et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Berglund et al\\.",
      "year" : 2015
    }, {
      "title" : "On the properties of neural machine translation: Encoder-decoder approaches",
      "author" : [ "Cho", "Kyunghyun", "Van Merriënboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1409.1259,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Collobert", "Ronan", "Weston", "Jason", "Bottou", "Léon", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Orthogonal rnns and long-memory tasks",
      "author" : [ "Henaff", "Mikael", "Szlam", "Arthur", "LeCun", "Yann" ],
      "venue" : "arXiv preprint arXiv:1602.06662,",
      "citeRegEx" : "Henaff et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Henaff et al\\.",
      "year" : 2016
    }, {
      "title" : "Untersuchungen zu dynamischen neuronalen netzen",
      "author" : [ "Hochreiter", "Sepp" ],
      "venue" : "Diploma, Technische Universität München, pp",
      "citeRegEx" : "Hochreiter and Sepp.,? \\Q1991\\E",
      "shortCiteRegEx" : "Hochreiter and Sepp.",
      "year" : 1991
    }, {
      "title" : "Long shortterm memory",
      "author" : [ "Hochreiter", "Sepp", "Schmidhuber", "Jürgen" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "A simple way to initialize recurrent networks of rectified linear units",
      "author" : [ "Le", "Quoc V", "Jaitly", "Navdeep", "Hinton", "Geoffrey E" ],
      "venue" : "arXiv preprint arXiv:1504.00941,",
      "citeRegEx" : "Le et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient orthogonal parametrisation of recurrent neural networks using householder reflections",
      "author" : [ "Mhammedi", "Zakaria", "Hellicar", "Andrew", "Rahman", "Ashfaqur", "Bailey", "James" ],
      "venue" : "arXiv preprint arXiv:1612.00188,",
      "citeRegEx" : "Mhammedi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mhammedi et al\\.",
      "year" : 2016
    }, {
      "title" : "Experimental realization of any discrete unitary operator",
      "author" : [ "Reck", "Michael", "Zeilinger", "Anton", "Bernstein", "Herbert J", "Bertani", "Philip" ],
      "venue" : "Phys. Rev. Lett.,",
      "citeRegEx" : "Reck et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Reck et al\\.",
      "year" : 1994
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya" ],
      "venue" : "arXiv preprint arXiv:1312.6120,",
      "citeRegEx" : "Saxe et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Saxe et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep learning with coherent nanophotonic circuits",
      "author" : [ "Shen", "Yichen", "Harris", "Nicholas C", "Skirlo", "Scott", "Prabhu", "Mihika", "Baehr-Jones", "Tom", "Hochberg", "Michael", "Sun", "Xin", "Zhao", "Shijie", "Larochelle", "Hugo", "Englund", "Dirk" ],
      "venue" : "arXiv preprint arXiv:1610.02365,",
      "citeRegEx" : "Shen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing",
      "author" : [ "Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Full-capacity unitary recurrent neural networks",
      "author" : [ "Wisdom", "Scott", "Powers", "Thomas", "Hershey", "John", "Le Roux", "Jonathan", "Atlas", "Les" ],
      "venue" : "In Advances In Neural Information Processing Systems,",
      "citeRegEx" : "Wisdom et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wisdom et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : ", 2015) have been successful on numerous difficult machine learning tasks, including image recognition (Krizhevsky et al., 2012; Donahue et al., 2015), speech recognition (Hinton et al.",
      "startOffset" : 103,
      "endOffset" : 150
    }, {
      "referenceID" : 3,
      "context" : ", 2012) and natural language processing (Collobert et al., 2011; Bahdanau et al., 2014; Sutskever et al., 2014).",
      "startOffset" : 40,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : ", 2012) and natural language processing (Collobert et al., 2011; Bahdanau et al., 2014; Sutskever et al., 2014).",
      "startOffset" : 40,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "However, deep neural networks have long suffered from vanishing and exploding gradient problems (Hochreiter, 1991; Bengio et al., 1994), which are known to be caused by matrix eigenvalues far from unity being raised to large powers.",
      "startOffset" : 96,
      "endOffset" : 135
    }, {
      "referenceID" : 2,
      "context" : "Other recently proposed methods such as GRU (Cho et al., 2014) and Bidirectional RNNs (Berglund et al.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : ", 2014) and Bidirectional RNNs (Berglund et al., 2015) also perform well in numerous applications.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "A recently proposed solution strategy is using orthogonal weight matrices or their complex generalization (unitary matrices) (Saxe et al., 2013; Le et al., 2015; Arjovsky et al., 2015), because all their eigenvalues will then have absolute values of unity, and can safely be raised to large powers.",
      "startOffset" : 125,
      "endOffset" : 184
    }, {
      "referenceID" : 8,
      "context" : "A recently proposed solution strategy is using orthogonal weight matrices or their complex generalization (unitary matrices) (Saxe et al., 2013; Le et al., 2015; Arjovsky et al., 2015), because all their eigenvalues will then have absolute values of unity, and can safely be raised to large powers.",
      "startOffset" : 125,
      "endOffset" : 184
    }, {
      "referenceID" : 11,
      "context" : "This has been shown to help both when weight matrices are initialized to be unitary (Saxe et al., 2013; Le et al., 2015) and when they are kept unitary during training, either by restricting them to a more tractable matrix subspace (Arjovsky et al.",
      "startOffset" : 84,
      "endOffset" : 120
    }, {
      "referenceID" : 8,
      "context" : "This has been shown to help both when weight matrices are initialized to be unitary (Saxe et al., 2013; Le et al., 2015) and when they are kept unitary during training, either by restricting them to a more tractable matrix subspace (Arjovsky et al.",
      "startOffset" : 84,
      "endOffset" : 120
    }, {
      "referenceID" : 14,
      "context" : "This is superior to the O(N) computational complexity of the existing training method for a full-space unitary network (Wisdom et al., 2016) and O(logN) more efficient than the subspace Unitary RNN (Arjovsky et al.",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 14,
      "context" : "Defining Gij ≡ ∂C ∂Wij as the gradient with respect to W, this can be implemented by the procedure defined by (Wisdom et al., 2016): A ≡ G † W −W † G, (7)",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 14,
      "context" : "This method shows that full space unitary networks are superior on many RNN tasks (Wisdom et al., 2016).",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "2 shows two that are particularly convenient to implement in software (and even in neuromorphic hardware (Shen et al., 2016)).",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 10,
      "context" : "(Reck et al., 1994) or b) the square decomposition method of Clements et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 14,
      "context" : "a Projective URNN (Wisdom et al., 2016).",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "We first compare these networks by applying them all to the so-called Memory Copying Task, which remains the most characteristically difficult task for RNNs (Hochreiter & Schmidhuber, 1997; Arjovsky et al., 2015; Henaff et al., 2016).",
      "startOffset" : 157,
      "endOffset" : 233
    }, {
      "referenceID" : 4,
      "context" : "Specifically, the task is defined as follows (Hochreiter & Schmidhuber, 1997; Arjovsky et al., 2015; Henaff et al., 2016).",
      "startOffset" : 45,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : ", 2015) and (Wisdom et al., 2016).",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 14,
      "context" : ", 2015) and projective (Wisdom et al., 2016) unitary methods, and also perform more robustly.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 14,
      "context" : "EURNN corresponds to our algorithm, projective URNN corresponds to algorithm presented in (Wisdom et al., 2016), URNN corresponds to the algorithm presented in (Arjovsky et al.",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 14,
      "context" : "Following the procedure of (Arjovsky et al., 2015; Wisdom et al., 2016), we apply RNN models to the MNIST handwriting data set by feeding one row of pixels to the RNN at each step.",
      "startOffset" : 27,
      "endOffset" : 71
    }, {
      "referenceID" : 14,
      "context" : "EURNN corresponds to our algorithm, projective URNN corresponds to algorithm presented in (Wisdom et al., 2016), URNN corresponds to the algorithm presented in (Arjovsky et al.",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "Note added: Recently, during the final proofreading phase of our manuscript, another paper that goes in a similar direction as ours (but parametrizing the unitary matrix by using reflections rather than rotations) was posted (Mhammedi et al., 2016).",
      "startOffset" : 225,
      "endOffset" : 248
    } ],
    "year" : 2017,
    "abstractText" : "We present a method for implementing an Efficient Unitary Neural Network (EUNN) whose computational complexity is merelyO(1) per parameter and has full tunability, from spanning part of unitary space to all of it. We apply the EUNN in Recurrent Neural Networks, and test its performance on the standard copying task and the MNIST digit recognition benchmark, finding that it significantly outperforms a non-unitary RNN, an LSTM network, an exclusively partial space URNN and a projective URNN with comparable parameter numbers.",
    "creator" : "LaTeX with hyperref package"
  }
}