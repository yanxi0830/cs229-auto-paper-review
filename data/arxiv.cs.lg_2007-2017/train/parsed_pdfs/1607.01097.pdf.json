{
  "name" : "1607.01097.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "AdaNet: Adaptive Structural Learning of Artificial Neural Networks",
    "authors" : [ "Corinna Cortes", "Xavi Gonzalvo" ],
    "emails" : [ "corinna@google.com", "xavigonzalvo@google.com", "vitalyk@google.com", "mohri@cims.nyu.edu", "yangs@cims.nyu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep learning has become a very powerful framework for machine learning and has enjoyed strong success in many areas in recent years. In the supervised setting, mapping and representing an input through increasingly more abstract layers of feature representation has shown to be extremely effective in areas such as natural language processing, image captioning, and many others. The concept of multilayer feature representations and modeling machine learning problems using a network of neurons is also motivated and guided by studies of the brain, neurological behavior, and cognition.\nHowever, despite the compelling arguments for using neural networks as a general template for solving machine learning problems, the training of these models and design of the right network for a given task has been filled with many theoretical gaps and practical concerns. For training a network, one needs to specify an often large network architecture with several layers and nodes, and then solve a difficult non-convex optimization problem. Additionally, the pre-specified architecture is often treated as a hyperparameter which is tuned using a validation set. These spaces can become exorbitantly large (e.g. [Krizhevsky et al., 2012]). From an optimization perspective, there is no guarantee of stability of an output model or near optimality of the learning objective, and often, one needs to implement ad hoc methods (e.g. gradient clipping [Pascanu et al., 2013]) to produce coherent models. From the statistical standpoint, large-scale hyperparameter tuning for an effective network architecture is extremely wasteful of data (due to cross validation), and can also exhaust a lot of time and resources (e.g. grid search, random search [Bergstra et al., 2011]).\nIn this paper, we attempt to remedy some of these issues. Accepting the general structure of a neural network as an effective parametrized model for supervised learning, we provide a theoretical analysis of this model and proceed to derive an algorithm benefitting from that theory. In the process, we introduce a framework for training neural networks that:\n1. uses a stable and robust algorithm with a unique solution. 2. can produce much sparser and/or shallower networks compared to existing methods.\nar X\niv :1\n60 7.\n01 09\n7v 1\n[ cs\n.L G\n] 5\n3. adapts the structure and complexity of the network to the difficulty of the particular problem at hand, with no pre-defined architecture.\n4. is accompanied and in fact motivated by strong data-dependent generalization bounds, validating their adaptivity and statistical efficacy.\n5. is intuitive from the cognitive standpoint that originally motivated neural network architectures.\nNot all machine learning problems admit the same level of difficulty, and different tasks naturally require varying levels of complexity. The typical approach to training a neural network requires the model-builder to know and specify as an architecture the right level of complexity. This is often unreasonably hard and can lead to large amounts of hyperparameter tuning, a statistically wasteful task. Moreover, if a network architecture is specified a priori and trained using back-propagation, the model will always have as many layers as the one specified because there needs to be at least one path through the network in order for the hypothesis to be non-trivial. Single weights may be pruned [Han et al., 2015], a technique originally termed Optimal Brain Damage [LeCun et al., 1990], but the architecture itself is unchanged. This imposes a stringent lower bound on the complexity of the model and can make the model prone to overfitting when there is insufficient data.\nIn contrast to enforcing high complexity, we will attempt to learn the requisite model complexity for a machine learning problem in an adaptive way. Starting from a simple single layer neural network, we will add more neurons and additional layers as needed. From the cognitive perspective, we will adapt the neural complexity and architecture to the difficulty of the problem. The additional neurons that we add will be carefully selected and penalized according to rigorous estimates from the theory of statistical learning. This will serve as a catalyst for the sparsity of our model as well as the strong generalization bonds that we will be able to derive. Incredibly, our method will also turn out to be convex and hence more stable than the current methodologies employed."
    }, {
      "heading" : "2 Related Work",
      "text" : "There has been extensive work involving structure learning for neural networks (e.g. [Kwok and Yeung, 1997, Leung et al., 2003, Islam et al., 2003, Lehtokangas, 1999, Islam et al., 2009]). All these publications seek to grow and prune the neural network architecture using some heuristic (e.g. genetic, information theoretic, or correlation). The structure learning algorithm introduced in this paper is based directly on optimizing generalization performance, which is precisely the learning goal in the batch setting.\nFrom the theory perspective, there have been several major lines of research on the theoretical understanding of neural networks. The first deals with understanding properties of the objective function used when training neural networks. (e.g. [Choromanska et al., 2014, Sagun et al., 2014, Zhang et al., 2015, Livni et al., 2014]). The second involves studying the black-box optimization algorithms that are often used for training these networks (e.g. [Hardt et al., 2015, Lian et al., 2015]). The third analyzes the statistical and generalization properties of the neural networks that are created (e.g. [Bartlett, 1998]). The fourth takes the generative point of view (e.g. [Arora et al., 2014, 2015]), assuming that the data actually comes from a particular network and then attempting to recover it. The fifth investigates the expressive ability of neural networks and analyzing what types of mappings they can learn (e.g. [Cohen et al., 2015, Eldan and Shamir, 2015]).\nThis paper takes the discriminative approach to machine learning and incorporates the first three methodologies, starting with a theoretical analysis of neural networks, to deriving a computationally tractable objective function, and to finally describing a precise optimization method. [Janzamin et al., 2015] is another paper that touches on multiple theory components, analyzing the generalization and training of two-layer neural networks through tensor methods. Our work uses different methods, applies to arbitrary networks, and also learns a network structure from a single layer."
    }, {
      "heading" : "3 Preliminaries",
      "text" : "Let X denote the input space. We consider the standard supervised binary classification scenario and assume that training and test points are drawn i.i.d. according to some distribution D over\nX × {−1,+1} and denote by S = ((x1, y1), . . . , (xm, ym)) a training sample of size m drawn according to Dm. Given any x ∈ X, we denote by Φ(x) ∈ Rn0 the feature representation of x. The standard description of a modern feedforward network is a network of layers of nodes, where each layer is mapped to the layer above it via a linear mapping composed with a component-wise nonlinear transformation. To make this precise, we define a neural network as follows.\nLet l denote the number of layers in the network. The networks we learn can be potentially very deep, that is l can be very large. For each k ∈ [l], denote by nk the maximum number of nodes in layer k.\nLet 1 ≤ p ≤ ∞ and k ≥ 1. Then define the set H(p)k to be the family of functions at layer k of the network in the following way:\nH (p) 1 = { x 7→ u ·Φ(x) : u ∈ Rn0 , ‖u‖p ≤ Λ1 } (1)\nH (p) k =\n{ x 7→ ( nk−1∑ j=1 uj(ϕk−1 ◦ hj)(x) ) : u ∈ Rnk−1 , ‖u‖p ≤ Λk, hj ∈ H(p)k−1 } , (∀k > 1) (2)\nwhere Λk > 0 is a hyperparameter and where ϕk is an activation function. Common activation functions include the Rectified Linear Unit (ReLU) ϕk(x) = max{0, x} and the sigmoid function ϕk(x) = 1 1+e−x (see e.g. [Goodfellow et al., 2016]), although our work will allow for any 1-Lipschitz activation function. The choice of norm p here is left to the learner and will determine both the sparsity of the network and the accompanying learning guarantee of the resulting model.\nLet H denote the union of these families of functions and their reflections: H = ⋃l k=1(H (p) k ∪ (−H(p)k )). Any feedforward neural network, then, can be written as a composition of mappings f = fl ◦ fl−1 ◦ . . . ◦ f1, where fk ∈ H(p)k . Intuitively, each transformation represents the encoding of the original data into an abstract layer of feature representation from which learnability is presumed to be “easier.”\nNote that in our definition, the activation function is not directly included in the unit itself but instead only applied when feeding the neuron into the next layer. While this choice of notation ultimately represents the same family of models, it is a subtle but important distinction that will be crucial for our theory as well as algorithmic design, in particular for deriving Lemma 1 and Lemma 4."
    }, {
      "heading" : "4 Theoretical properties of artificial neural networks",
      "text" : "We measure the performance of a hypothesis f ∈ H by its expected loss over the data’s distribution D, also known as the generalization error: R(f) = E(x,y)∼D[1yf(x)≤0]. We typically also want to measure the performance of the model on our training sample S. This will be done using the empirical margin loss: R̂S,ρ(f) = 1m ∑m i=1 1yif(xi)≤ρ, where the margin refers to the ρ term. Hypothesis functions that allow for large margin with small empirical margin loss intuitively represent classifiers with high confidence of accuracy.\nGiven a hypothesis set H of functions mapping from X to R, we denote by R̂S(H) the empirical Rademacher complexity of H for the sample S: R̂S(H) = 1m Eσ [suph∈H ∑m i=1 σih(xi)] , and by Rm(H) the Rademacher complexity of H defined by Rm(H) = ES∼Dm [R̂S(H)]. The empirical Rademacher complexity measures the correlation of a hypothesis set with random noise over the sample and is a problem-dependent measure of model complexity. It can also be shown to relate to other classical notions of model complexity, such as the VC-dimension and covering number (see e.g. [Vapnik, 1998, Mohri et al., 2012])."
    }, {
      "heading" : "4.1 Learning bounds on the Rademacher complexity of network hypothesis sets",
      "text" : "Since neural networks are built as compositions of layers, it is natural from the theoretical standpoint to first analyze the complexity of any layer in terms of the complexity of its previous layer. Our first result demonstrates that this can indeed be done, and that the empirical Rademacher complexity of\nany intermediate layer k in the network is bounded by the empirical Rademacher complexity of its input times a term that depends on a power of the size of the layer:\nLemma 1. Let 1p + 1 q = 1. Then for k ≥ 2, the empirical Rademacher complexity of H (p) k for a sample S of size m can be upper bounded as follows in terms of that of H(p)k−1:\nR̂S(H (p) k ) ≤ 2Λkn\n1 q\nk−1R̂S(H (p) k−1).\nThe proofs of this result, as well as all those that follow in this section, can be found in Appendix A.\nBy analyzing the complexity of the initial layer, we can derive a bound on the complexity of every layer in closed form without the need for any recurrence relation: Lemma 2. Let r∞ = maxj∈[1,n1],i∈[1,m] |[Φ(xi)]j |, and 1p + 1 q = 1. Then for any k ≥ 1, the empirical Rademacher complexity of H(p)k for a sample S of size m can be upper bounded as follows:\nR̂S(H (p) k ) ≤ 2 k−1r∞ ( k∏ j=1 Λjn 1 q j−1 )√ 2 log(2n0) m .\nThe above two lemmas are instructive and intuitive in the sense that they convey the message that additional layers in a neural network contribute to increased complexity of a model. Because of this, while large models are more powerful, they also become increasingly more prone to overfitting. Moreover, the Rademacher complexity bounds also suggest that model complexity can increase much more due to a single additional layer as opposed to an additional node.\nGuided by this insight, we will seek to learn models that will be parsimonious in model complexity. Specifically, we will learn adaptive neural networks that consider all layers of feature representation simultaneously, emphasize shallower layers of representation more heavily, and only activate deeper ones when necessary. We will represent such models using the notation: f = ∑l k=1 ∑nk j=1 wk,jhk,j , where wk,j’s are weights and hk,j ∈ H(p)k . The motivation for this type of network is reinforced by the following learning guarantee: Theorem 3 (ADANET Generalization Bound). Let l̂ ∈ [1, l] and for each k ∈ [1, l̂], let n̂k ∈ [1, nk]. For k ∈ [1, l̂] and j ∈ [1, n̂k], let wk,j ∈ R and hk,j ∈ H(p)k . Finally, let r∞ = maxj∈[1,n1],i∈[1,m] |[Φ(xi)]j |, and 1p + 1 q = 1. Then for any δ > 0, with probability at least 1 − δ over the sample S of size m drawn i.i.d. according to distribution Dm, the following inequality holds for any f = ∑l̂ k=1 ∑n̂k j=1 wk,jhk,j with ∑l̂ k=1 ∑n̂k j=1 wk,j = 1:\nR(f) ≤ R̂S,ρ + 4\nρ l̂∑ k=1 n̂k∑ j=1 wk,j4 k−1r∞ ( k∏ i=1 Λin 1 q i−1 )√ 2 log(2n0) m + C(ρ, l,m, δ),\nwhere C(ρ, l,m, δ) = 2ρ √ log(l) m + √ d 4ρ2 log( ρ2m log(l) )e log(l) m + log( 2δ ) 2m .\nThe generalization bound above informs us that the complexity of the neural network returned is a weighted combination of the complexities of each node in the neural network, where the weights are precisely the ones that define our network. Specifically, this again agrees with our intuition that deeper networks are more complex and suggests that if we can find a model that has both small empirical error and most of its weight on the shallower nodes, then such a model will generalize well.\nToward this goal, we will design an algorithm that directly seeks to minimize upper bounds of this generalization bound. In the process, our algorithm will train neural networks that discriminate deeper networks from shallower ones. This is a novel property that existing regularization techniques in the deep learning toolbox do not enforce. Techniques such as l2 and l1 regularization and dropout (see e.g. Goodfellow et al. [2016]) are generally applied uniformly across all nodes in the network."
    }, {
      "heading" : "5 Algorithm",
      "text" : "This section describes our algorithm, ADANET, for adaptive deep learning. ADANET adaptively grows the structure of a neural network, balancing model complexity with margin maximization. We start with a high-level description of the algorithm before proceeding to a more detailed presentation."
    }, {
      "heading" : "5.1 Overview",
      "text" : "Our algorithm starts with the network reduced to the input layer, corresponding to the input feature vector, and an output unit, fully connected, and then augments or modifies the network over T rounds. At each round, it either augments the network with a new node or updates the weights defining the function h ∈ H(p)k of an existing node of the network at layer k. A new node may be selected at any layer k ∈ [1, l] already populated or start on a new layer but, in all cases, it is chosen with links only to existing nodes in the network in the layer below plus a connection to the output unit. Existing nodes are either those of the input layer or nodes previously added to the network by the algorithm. Figure 1(a) illustrates this design.\nThe choice of the node to construct or update at each round is a key aspect of our algorithm. This is done by iteratively optimizing an objective function that we describe in detail later. At each round, the choice of the best node minimizing the current objective is subject to the following trade-off: the best node selected from a lower layer may not help reduce the objective as much as one selected from a higher layer; on the other hand, nodes selected from higher layers augment the network with substantially more complex functions, thereby increasing the risk of overfitting. To resolve this tension quantitatively, our algorithm selects the best node at each round based on a combination of the amount by which it helps reduce the objective and the complexity of the family of hypotheses defined nodes at that layer.\nThe output node of our network is connected to all the nodes created during these T rounds, so that the hypothesis will directly use all nodes in the network. As our theory demonstrated in Theorem 3, this can significantly reduce the complexity of our model by assigning more weight to the shallower nodes. At the same time, it also provides us the flexibility to learn larger and more complex models. In fact, the family of neural networks that we search over is actually larger than the family of feedforward neural networks typically considered using back-propagation due to these additional connections.\nAn additional more sophisticated variant of our algorithm is depicted in Figure 1(b). In this design, the nodes created at each round can be connected not just to the nodes of the previous layer, but to those of any layer below. This allows for greater model flexibility, and by modifying the definitions of the hypotheses sets 1 and 2, we can adopt a principled complexity-sensitive way for learning these types of structures as well.\nIn the next section, we give a more formal description of our algorithm, including the exact optimization problem as well as a specific search process for new nodes."
    }, {
      "heading" : "5.2 Objective function",
      "text" : "Recall the definition of our hypothesis space conv(H) = ⋃l k=1(H (p) k ∪ (−H (p) k )), which is the convex hull of all neural networks up to depth l and naturally includes all neural networks of depth l – the common hypothesis space in deep learning. Note that the set of all functions in H is infinite, since the weights corresponding to any function can be any real value inside their respective lp balls.\nDespite this challenge, we will efficiently discover a finite subset of H, denoted by {h1, . . . , hN}, that will serve as the basis for our convex combination. Here, N will also represent the maximum number of nodes in our network. Thus, we have that N = ∑l k=1 nk, and N will also generally be\nassumed as very large. Moreover, we will actually define and update our set {h1, . . . , hN} online, in a manner that will be made precise in Section 5.4. We will also rely on the natural bijection between the two enumerations {h1, . . . , hN} and {hk,j}k∈[l],j∈[nk], depending on which is more convenient. The latter is useful for saying that hk,j ∈ H(p)k . Moreover, for any j ∈ [N ], we will denote by kj ∈ [l], the layer in which hypothesis hj lies. For simplicity, we will also write as rj the Rademacher complexity of the family of functions H (p) kj containing hj : rj = Rm(H (p) kj ).\nLet x 7→ Φ(−x) be a non-increasing convex function upper bounding the 0/1 loss, x 7→ 1x≤0, with Φ differentiable over R and Φ′(x) 6= 0 for all x. Φ may, for instance, be the exponential function, Φ(x) = ex as in the AdaBoost of Freund and Schapire [1997] or the logistic function, Φ(x) = log(1 + ex) as in logistic regression.\nAs in regularized boosting style methods (e.g. [Rätsch et al., 2001]), our algorithm will apply coordinate descent to the following objective function over RN :\nF (w) = 1\nm m∑ i=1 Φ ( 1− yi N∑ j=1 wjhj(xi) ) + N∑ j=1 Γj |wj |, (3)\nwhere Γj = λrj + β with λ ≥ 0 and β ≥ 0 hyperparameters. The objective function is the sum of the empirical error based on a convex surrogate loss function x 7→ Φ(−x) of the binary loss and a regularization term. The regularization term is a weighted-l1 penalty that contains two sub-terms: a standard norm-1 regularization which admits β as a parameter, and a term that discriminates functions hj based on their complexity (i.e. rj) and which admits λ as a parameter.\nOur algorithm can be viewed as an instance of the DeepBoost algorithm of Cortes et al. [2014]. However, unlike DeepBoost, which combines decision trees, ADANET algorithm learns a deep neural network, which requires both deep learning-specific theoretical analysis as well as an online method for constructing and searching new nodes. Both of these aspects differ significantly from the decision tree framework in DeepBoost, and the latter is particularly challenging due to the fact that our hypothesis space H is infinite."
    }, {
      "heading" : "5.3 Coordinate descent",
      "text" : "Let wt = (wt,1, . . . , wt,N )> denote the vector obtained after t ≥ 1 iterations and let w0 = 0. Let ek denote the kth unit vector in RN , k ∈ [1, N ]. The direction ek and the step η selected at the tth round are those minimizing F (wt−1 + ηek). Let ft−1 = ∑N j=1 wt−1,jhj . Then we can write\nF (wt−1 + ηek)= 1\nm m∑ i=1 Φ ( 1− yift−1(xi)−ηyihk(xi) ) + ∑ j 6=k Γj |wt−1,j |+ Γk|wt−1,k + η|,\nFor any t ∈ [1, T ], we will maintain the following distribution Dt over our sample: Dt(i) = Φ′ ( 1−yift−1(xi) )\nSt , where St is a normalization factor, St = ∑m i=1 Φ\n′(1− yift−1(xi)). Moreover, for any s ∈ [1, T ] and j ∈ [1, N ] and a given hypothesis hj bounded by C > 0, we will consider s,j , the weighted error of hypothesis hj over the distribution Ds: s,j = C2 [ 1− Ei∼Ds [yihj(xi) C ]] . These weighted errors will be crucial for “scoring” the direction that the algorithm takes at each round."
    }, {
      "heading" : "5.4 Search and active coordinates",
      "text" : "As already mentioned, a key aspect of our AdaNet algorithm is the construction of new hypotheses at each round. We do not enumerate allN hypotheses at the beginning of the algorithm, because it would be extremely difficult to select good candidates before seeing the data. At the same time, searching through all node possible combinations using the data would be a computationally infeasible task.\nInstead, our search procedure will be online, building upon the nodes that we already have in our network and selecting at most a single at a time. Specifically, at each round, the algorithm selects a\nnode out of the following set of “active” candidates: existing nodes in our network or new nodes with connections to existing nodes in some layer k.\nThere are many potential methods to construct new candidate nodes, and at first glance, scoring every possible new node with connections to existing nodes may seem a computational impediment. However, by using Banach space duality, we can compute directly and efficiently in closed form the new node that best optimizes the objective at each layer."
    }, {
      "heading" : "5.4.1 New candidate nodes",
      "text" : "Given a distribution D over the sample S and a tuple of hypotheses hk = (hk,1, . . . , hk,nk) ⊂ H (p) k , we denote by Margin(D, hk,j) the weighted margin of hypothesis hk,j composed with its activation on distribution D:\nMargin(D, hk,j) = Ei∼D[yi(ϕk ◦ hk,j)(xi)],\nand we denote by Margin(D,hk) the vector of weighted margins of all nodes in layer k:\nMargin(D,hk) = ( Ei∼D[yi(ϕk ◦ hk,1)(xi)], . . . ,Ei∼D[yi(ϕk ◦ hk,nk)(xi)] ) .\nFor any layer k in an existing neural network, a vector u ∈ Rnk uniquely specifies a node that connects from nodes in the previous layer k − 1. Let ũk denote such a new node in layer k, and let l̂ be the number of layers with non-zero nodes. Then for layers 2 ≤ k ≤ l̂, if the number of nodes is less than the maximum allowed size nk, we will consider as candidates the nodes with the largest weighted margin. Remarkably, these nodes can be computed efficiently and in closed form:\nLemma 4 (Construction of new candidate nodes, see proof in Appendix B). Fix (hk−1,j) nk−1 j=1 ⊂ H (p) k−1. Then the solution ũk to the optimization problem\nmax ‖u‖p≤Λk\nEi∼D [ yi nk−1∑ j=1 uj(ϕk−1 ◦ hk−1,j)(xi) ] ,\ncan be computed coordinate-wise as:\n(ũk)j = Λk\n‖Margin(D,hk−1)‖ q p q\n|Margin(D, hk−1,j)|q−1 sgn (Margin(D, hk−1,j)) ,\nand the solution has value: Λk ‖Margin(D,hk−1)‖q ."
    }, {
      "heading" : "5.5 Pseudocode",
      "text" : "In this section, we present the pseudocode for our algorithm, ADANET, which applies the greedy coordinate-wise optimization procedure described in Section 5.3 on the objective presented in Section 5.2 with the search procedure described in Section 5.4.\nThe algorithm takes as input the sample S, the maximum number of nodes per layer (nk)lk=1, the complexity penalties (Γk)lk=1, the lp norms of the weights defining new nodes (Λk) l k=1, and upper bounds on the nodes in each layer (Ck)lk=1. ADANET then initializes all weights to zero, sets the distribution to be uniform, and considers the active set of coordinates to be the initial layer in the network. Then the algorithm repeats the following sequence of steps: it computes the scores of the existing nodes in the method EXISTINGNODES, of the new candidate nodes in NEWNODES, and finds the node with the best score (|dk,j | or |d̃k|) in BESTNODE. After finding this “coordinate”, it updates the step size and distribution before proceeding to the next iteration (as described in Section 5.3). The precise pseudocode is provided in Figure 2, and details of its derivation are given in Section B."
    }, {
      "heading" : "5.6 Convergence of ADANET",
      "text" : "Remarkably, the neural network that ADANET outputs is competitive against the optimal weights for any sub-network that it sees during training. Moreover, it achieves this guarantee in linear time. The precise statement and proof are provided in Appendix D."
    }, {
      "heading" : "5.7 Large-scale implementation of AdaNet",
      "text" : "We describe a large-scale implementation of the ADANET optimization problem using state-of-the-art techniques from stochastic optimization in Appendix E."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We presented a new framework for analyzing and learning artificial neural networks. Our method optimizes for generalization performance, and it explicitly and automatically addresses the trade-off between model architecture and empirical risk minimization, ideas that have been under-explored in deep learning. Our techniques are general and can be applied to other neural network architectures, including CNNs and LSTMs as well as to other learning settings such as multi-class classification and regression, all of which serve as interesting avenues for future work."
    }, {
      "heading" : "A Proofs of theoretical guarantees",
      "text" : "Lemma 1. Let 1p + 1 q = 1. Then for k ≥ 2, the empirical Rademacher complexity of H (p) k for a sample S of size m can be upper bounded as follows in terms of that of H(p)k−1:\nR̂S(H (p) k ) ≤ 2Λkn\n1 q\nk−1R̂S(H (p) k−1).\nProof.\nR̂S(H (p) k ) =\n1 m E σ  sup hj∈H(p)k−1 ‖u‖p≤Λk m∑ i=1 σi nk−1∑ j=1 uj(ϕk−1 ◦ hj)(xi) \n= 1\nm E σ  sup hj∈H(p)k−1 ‖u‖p≤Λk nk−1∑ j=1 uj m∑ i=1 σi(ϕk−1 ◦ hj)(xi)  =\nΛk m E σ  sup hj∈H(p)k−1 ∥∥∥∥ m∑ i=1 σi(ϕk−1 ◦ hj)(xi) ∥∥∥∥ q  (def. of dual norm) = Λkn 1 q\nk−1 m E σ  sup hj∈H(p)k−1 ∥∥∥∥ m∑ i=1 σi(ϕk−1 ◦ hj)(xi) ∥∥∥∥ ∞  (equiv. of lp norms and sup) = Λkn 1 q\nk−1 m E σ  sup h∈H(p)k−1 ∣∣∣∣ m∑ i=1 σi(ϕk−1 ◦ h)(xi) ∣∣∣∣ \n= Λkn\n1 q\nk−1 m E σ  sup h∈H(p)k−1 s∈{−1,+1} s m∑ i=1 σi(ϕk−1 ◦ h)(xi)  (def. of absolute value)\n≤ Λkn\n1 q\nk−1 m E σ  sup h∈H(p)k−1 m∑ i=1 σi(ϕk−1 ◦ h)(xi)  +\nΛk m E σ  sup h∈H(p)k−1 m∑ i=1 −σi(ϕk−1 ◦ h)(xi)  = 2Λkn 1 q\nk−1 m E σ  sup h∈H(p)k−1 m∑ i=1 σi(ϕk−1 ◦ h)(xi)  ≤ 2Λkn 1 q\nk−1 m E σ  sup h∈H(p)k−1 m∑ i=1 σih(xi)  (Talagrand’s inequality) ≤ 2Λkn 1 q k−1R̂S(H (p) k−1)\nLemma 2. Let r∞ = maxj∈[1,n1],i∈[1,m] |[Φ(xi)]j |, and 1p + 1 q = 1. Then for any k ≥ 1, the empirical Rademacher complexity of H(p)k for a sample S of size m can be upper bounded as follows:\nR̂S(H (p) k ) ≤ 2 k−1r∞ ( k∏ j=1 Λjn 1 q j−1 )√ 2 log(2n0) m .\nProof.\nR̂S(H (p) 1 ) =\n1 m E σ\n[ sup\n‖u‖p≤Λ1 m∑ i=1 σiu ·Φ(xi)\n]\n= 1\nm E σ\n[ sup\n‖u‖p≤Λ1 u · m∑ i=1 σiΦ(xi)\n]\n= Λ1 m E σ [∥∥∥∥ m∑ i=1 σi[Φ(xi)] ∥∥∥∥ q ] (def. of dual norm)\n≤ Λ1n 1 q 0\nm E σ [∥∥∥∥ m∑ i=1 σi[Φ(xi)] ∥∥∥∥ ∞ ] (equivalence of lp norms)\n= Λ1n\n1 q 0\nm E σ [ max j∈[1,n1] ∣∣∣∣ m∑ i=1 σi[Φ(xi)]j ∣∣∣∣ ]\n(def. of l∞ norm)\n= Λ1n\n1 q 0\nm E σ  max j∈[1,n1]\ns∈{−1,+1}\nm∑ i=1 σis[Φ(xi)]j  (def. of absolute value) ≤ Λ1n 1 q 0 r∞ √ m √ 2 log(2n0)\nm = r∞Λ1n\n1 q 0\n√ 2 log(2n0)\nm . (Massart’s lemma)\nThe result then follows by application of Lemma 1.\nTheorem 3 (AdaNet Generalization Bound). Let l̂ ∈ [1, k] and for each k ∈ [1, l̂], let n̂k ∈ [1, nk]. For k ∈ [1, l̂] and j ∈ [1, n̂k], let wk,j ∈ R and hk,j ∈ H(p)k . Then for any δ > 0, with probability at least 1− δ over the sample S of size m drawn i.i.d. according to distribution Dm, the following inequality holds for any f = ∑l̂ k=1 ∑n̂k j=1 wk,jhk,j with ∑l̂ k=1 ∑n̂k j=1 wk,j = 1:\nR(f) ≤ R̂S,ρ + 4\nρ l̂∑ k=1 n̂k∑ j=1 wk,j4 k−1r∞ ( k∏ i=1 Λin 1 q i−1 )√ 2 log(2n0) m + 2 ρ √ log(l) m\n+\n√⌈ 4\nρ2 log\n( ρ2m\nlog(l)\n)⌉ log(p)\nm +\nlog( 2δ )\n2m ,\nProof. By considering the symmetrized sets H = ⋃l k=1(H (p) k ∪ (−H (p) k )), we can assume without loss of generality that the weights wk,j are non-negative.\nWe will now use the following structural learning guarantee for ensembles of hypotheses:\nLemma 5 (DeepBoost Generalization Bound, Theorem 1 Cortes et al. [2014]). Let l > 1. Assume there exists some decomposition of the hypothesis spaceH = ∪li=1Hi. Fix ρ > 0. Given any h ∈ H and a sample S, denote the empirical margin loss as R̂S,ρ. Then for any δ > 0, with probability at least 1− δ over the sample S of size m drawn i.i.d. according to distribution Dm, for any αt ∈ R+ such that ∑T t=1 αt = 1, the following inequality holds for f = ∑T t=1 αtht:\nR(f) ≤ R̂S,ρ + 4\nρ T∑ t=1 αtRm(Hkt) + 2 ρ\n√ log(l)\nm\n+\n√⌈ 4\nρ2 log\n( ρ2m\nlog(l)\n)⌉ log(l)\nm +\nlog( 2δ )\n2m ,\nwhere for each ht ∈ H, kt denotes the smallest k ∈ [1, l] such that ht ∈ Hkt .\nLemma 5 implies that\nR(f) ≤ R̂S,ρ + 4\nρ l̂∑ k=1 n̂k∑ j=1 wk,jRm(H (p) k ∪ (−H (p) k )) + 2 ρ\n√ log(l)\nm\n+ √ d 4 ρ2 log ( ρ2m log(l) ) e log(p) m + log( 2δ ) 2m .\nBy using symmetrization in Lemma 2, l̂∑\nk=1 n̂k∑ j=1 wk,jRm(H (p) k ∪ (−H (p) k )) ≤ l̂∑ k=1 4k−1r∞ ( k∏ i=1 Λin 1 q i−1 )√2 log(2n0) m n̂k∑ j=1 wk,j ."
    }, {
      "heading" : "B Proofs for algorithmic design",
      "text" : "B.1 New candidate nodes\nLemma 4 (Construction of new candidate nodes). Fix (hk−1,j) nk−1 j=1 ⊂ H (p) k−1. Then the solution ũk to the optimization problem\nmax ‖u‖p≤Λk Ei∼D[yi nk−1∑ j=1 uj(ϕk−1 ◦ hk−1,j)(xi)],\ncan be computed coordinate-wise as:\n(ũk)j = Λk\n‖Margin(D,hk−1)‖ q p q\n|Margin(D, hk−1,j)|q−1 sgn (Margin(D, hk−1,j)) ,\nand the solution has value: Λk ‖Margin(D,hk−1)‖q\nProof. By linearity of expectation,\nũk = argmax ‖u‖p≤Λk Ei∼D[yi nk−1∑ j=1 uj(ϕk−1 ◦ hk−1,j)(xi)] = argmax ‖u‖p≤Λk u ·Margin(D,hk−1).\nWe claim that\n(ũk)j = Λk\n‖Margin(D,hk−1)‖ q p q\n|Margin(D, hk−1,j)|q−1 sgn (Margin(D, hk−1,j)) .\nTo see this, note first that by Holder’s inequality, u ·Margin(D,hk−1) ≤ ‖u‖p‖Margin(D,hk−1)‖q ≤ Λk‖Margin(D,hk−1)‖q,\nand the expression on the right-hand side is our proposed value. At the same time, our choice of ũk also satisfies this upper bound:\nũk ·Margin(D,hk−1) = nk−1∑ j=1 (ũk)j Margin(D, hk−1,j)\n= nk−1∑ j=1\nΛk\n‖Margin(D,hk−1)‖ q p q\n|Margin(D, hk−1,j)|q\n= Λk\n‖Margin(D,hk−1)‖ q p q\n‖Margin(D, hk−1,j)‖qq\n= Λk‖Margin(D,hk−1)‖q. Thus, ũk is a solution to the optimization problem and achieves the claimed value.\nB.2 Derivation of coordinate descent update\nRecall the form of our objective function:\nF (wt) = 1\nm m∑ i=1 Φ ( 1− yi l∑ k=1 nk∑ j=1 wt,k,jhk,j(xi) ) + l∑ k=1 nk∑ j=1 Γk|wk,j |.\nWe want to find the directional derivative with largest magnitude as well as the optimal step-size in this coordinate direction.\nSince F is non-differentiable at 0 for each coordinate (due to the weighted l1 regularization), we must choose a representative of the subgradient. Since, F is convex, it admits both left and right directional derivatives, which we denote by\n∇+k,jF (w) = lim η→0+ F (w + ηek,j)− F (w) η , ∇−k,jF (w) = lim η→0− F (w + ηek,j)− F (w) η .\nMoreover, convexity ensures that ∇−k,jF ≤ ∇ + k,jF . Now, let δk,jF be the element of the subgradient that we will use to compare descent magnitudes, so that (kt, jt) = argmaxk∈[1,l],j∈[1,nk] ∣∣δk,jF (wt)∣∣. This subgradient will always be chosen as the one closest to 0:\nδk,jF (w) = 0 if ∇−k,jF (w) ≤ 0 ≤ ∇ + k,jF (w)\n= ∇+k,j(w) if ∇ − k,jF (w) ≤ ∇ + k,jF (w) ≤ 0 = ∇−k,j(w) if 0 ≤ ∇ − k,jF (w) ≤ ∇ + k,jF (w).\nSuppose that wt,k,j 6= 0. Then by continuity, for η sufficiently small, wt,k,j and wt,k,j + ηek,j have the same sign so that\nF (wt + ηek,j) = 1\nm m∑ i=1 Φ ( 1− yi l∑ k=1 nk∑ j=1 wt,k,jhk,j(xi)− ηyihk,j(xi) )\n+ ∑\n(k̃,j̃) 6=(k,j)\nΓk|wt,k̃,j̃ |+ Γk sgn(wt,k,j)(wt,k,j + η).\nFurthermore, F is differentiable in the (k, j)-th at wt, which implies that\n∇k,jF (wt) = 1\nm m∑ i=1 −yihk,j(xi)Φ′ ( 1− yi l∑ k=1 nk∑ j=1 wt,k,jhk,j(xi) ) + sgn(wt,k,j)Γk\n= 1\nm m∑ i=1 yihk,j(xi)Dt(i)St + sgn(wt,k,j)Γk\n= (2 t,k,j − Ck) St m + sgn(wt,k,j)Γk\nWhen wt,k,j = 0, we can consider the left and right directional derivatives:\n∇+k,jF (wt) = (2 t,k,j − Ck) St m + Γk\n∇−k,jF (wt) = (2 t,k,j − Ck) St m − Γk\nMoreover, ∣∣∣∣ t,k,j − Ck2 ∣∣∣∣ ≤ Γkm2St ⇔ ∇−k,jF (w) ≤ 0 ≤ ∇+k,jF (w)\nt,k,j − Ck 2 ≤ −Γkm 2St ⇔ ∇−k,jF (w) ≤ ∇ + k,jF (w) ≤ 0\nΓkm\n2St ≤ t,k,j − Ck 2\n⇔ 0 ≤ ∇−k,jF (w) ≤ ∇ + k,jF (w),\nso that we have\nδk,jF (wt) = (2 t,k,j − Ck) St m + sgn(wt,k,j)Γk if ∣∣∣∣ t,k,j − Ck2 ∣∣∣∣ ≤ Γkm2St = 0 else if\n∣∣∣∣ t,k,j − Ck2 ∣∣∣∣ ≤ Γkm2St\n= (2 t,k,j − Ck) St m − sgn\n( t,k,j −\nCk 2\n) Γk otherwise ."
    }, {
      "heading" : "C Other components of pseudocode",
      "text" : "Figures 5, 6, 7, 8 present the remaining components of the pseudocode for ADANET with exponential loss.\nThe initial weight vector w0 is initialized to 0, and the initial weight distribution D1 is uniform over the coordinates.\nThe best node is simply the one with the highest score |dk,j | (or |d̃k|) among all existing nodes and the new candidate nodes.\nThe step-size taken at each round is the optimal step in the direction computed. For exponential loss functions, this can be computed exactly, and in general, it can be approximated numerically via line search methods (since the objective is convex).\nThe updated distribution at time t will be proportional to Φ′(1 − yift−1(xi), as explained in Section 5.3."
    }, {
      "heading" : "D Convergence of ADANET",
      "text" : "Theorem 6. Let Φ be a twice-continuously differentiable function with Φ′′ > 0, and suppose we terminate ADANET after O ( log(1/ ) ) iterations if it does not add a new node. Let Is ⊂ [1, N ] be the first j nodes added by ADANET, and let w∗Is = argminw∈PIs (RN+ ) F (w), where PIs denotes\nprojection onto RIs . Let N̂ = ∑l̂ k=1 n̂k be the total number of nodes constructed by the ADANET algorithm at termination. Then ADANET will terminate after at most O(N̂ log(1/ )) iterations, producing a neural network and a set of weights such that\nF (wAdaNet)− min s∈[1,N̂ ] F (w∗Is) <\nProof. Recall that\nF (w) = 1\nm m∑ i=1 Φ ( 1− yi N∑ j=1 wjhj(xi) ) + N∑ j=1 Γj |wj |.\nSince ADANET initializes all weights to 0 and grows the coordinate space in an online fashion, we may consider the algorithm in epochs, so that if the support of wt at any given t is Is, then\nF (w) = FIs(w) = 1\nm m∑ i=1 Φ ( 1− yi ∑ j∈Is wjhj(xi) ) + ∑ j∈Is Γj |wj |.\nFix an epoch s ∈ [1, N̂ ]. The optimal set of weights within the support Is is given by w∗Is , and this solution exists because F is a coercive convex function (due to the weighted l1 regularization). Let M ∈ Rm×N be a matrix with elements given by Mi,j = hj(xi)yi, and let ei ∈ Rm be a i-th elementary basis vector of Rm. Then for any vector w ∈ RN , e>i Mw = ∑N j=1 wjhj(xi)yi. Thus, if we define denote by GIs(z) = 1 m ∑m i=1 Φ(1− e>i z), then the first component of FIs can be written as GIs(Mw).\nMoreover, since Φ is twice continuously differentiable and Φ′′ > 0, it follows that GIs is twice continuously differentiable and strictly convex. We can also compute that for any w ∈ RN ,\nm∑\nwhich is positive definite since Φ′′ > 0. Finally, since H is symmetric, we can, at the cost of flipping some hj → −hj , equivalently write: FIs(w) = g(Mw) + ∑ j∈Is Γjwj , subject to wj ≥ 0.\nThus, the problem of minimizing FIs is equivalent to the optimization problem studied in Luo and Tseng [1992], and we have verified that the conditions are satisfied as well. If the algorithm doesn’t add another coordinate, then it performs the Gauss-Southwell method on the coordinates Is. By Theorem 2.1 in Luo and Tseng [1992], this method will converge to the optimal set of weights with support in Is linearly. This implies that if the algorithm terminates, it will maintain the error guarantee: FIs(wADANET)− FIs(w∗Is) < . If the algorithm does add a new coordinate before termination, then we can apply the same argument to FIs+1 .\nThus, when the algorithm does finally terminate, we maintain the error guarantee for every subset Is ⊂ [1, N ] of nodes that we create, and the total run time is at most O(N̂ log(1/ )) iterations."
    }, {
      "heading" : "E Large-scale implementation: ADANET+",
      "text" : "Our optimization problem is a regularized empirical risk minimization problem of the form: F (x) = G(x) + ψ(x) = 1m ∑m i=1 fi(x) + ∑N j=1 ψj(xj), where each fi is smooth and convex and ψ, also convex, decomposes across the coordinates of x.\nFor any subset B ⊂ [1,m], let GB = 1m ∑ i∈B fi denote the components of the ERM objective that correspond to that subset. For any j ∈ [1, N ], let∇j denote the partial derivative in the coordinate j. We can leverage the Mini-Batch Randomized Block Coordinate Descent (MBRCD) technique introduced by Zhao et al. [2014]. Their work can be viewed as the randomized block coordinate descent variant of the Stochastic Variance Reduced Gradient (SVRG) family of algorithms (Johnson and Zhang [2013], Xiao and Zhang [2014]).\nOur algorithm divides the entire training period into K epochs. At every step, the algorithm samples two mini-batches from [1,m] uniformly. The first is used to approximate the ERM objective for the descent step, and the second is used to apply the NEWNODES subroutine in Figure 4 to generate new candidate coordinates. After generating these coordinates, the algorithm samples a coordinate from the set of new coordinates and the existing ones. Based on the coordinate chosen, it updates the active set. Then the algorithm computes a gradient descent step with the SVRG estimator in place of the gradient and with the sampled approximation of the true ERM objective. It then makes a proximal update with ψ as the proximal function. Finally, the “checkpoint parameter” of the SVRG estimator is updated at the end of every epoch.\nWhile our optimization problem itself is not strongly convex, we can apply the MBRCD indirectly by adding a strongly convex regularizer, solving: F̃ (x) = F (x) + γR(x), where R is a 1-strongly convex function, to still yield a competitive guarantee.\nMoreover, since our non-smooth component is a weighted-l1 term, the proximal update can be solved efficiently and in closed form in a manner that is similar to the Iterative Shrinkage-Thresholding Algorithm (ISTA) of Beck and Teboulle [2009].\nFigure 9 presents the pseudocode of the algorithm, ADANET+."
    }, {
      "heading" : "Appendix References",
      "text" : "A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM\njournal on imaging sciences, 2(1):183–202, 2009.\nR. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315–323, 2013.\nZ.-Q. Luo and P. Tseng. On the convergence of coordinate descent method for convex differentiable minimization. Journal of Optimization Theory and Applications, 72(1):7 – 35, 1992.\nL. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057–2075, 2014.\nT. Zhao, M. Yu, Y. Wang, R. Arora, and H. Liu. Accelerated mini-batch randomized block coordinate descent method. In Advances in neural information processing systems, pages 3329–3337, 2014."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "We present a new theoretical framework for analyzing and learning artificial neural networks. Our approach simultaneously and adaptively learns both the structure of the network as well as its weights. The methodology is based upon and accompanied by strong data-dependent theoretical learning guarantees, so that the final network architecture provably adapts to the complexity of any given problem.",
    "creator" : "LaTeX with hyperref package"
  }
}