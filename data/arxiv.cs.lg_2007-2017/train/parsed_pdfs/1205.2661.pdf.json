{
  "name" : "1205.2661.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs",
    "authors" : [ "Peter L. Bartlett", "Ambuj Tewari" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We provide an algorithm that achieves the optimal regret rate in an unknown weakly communicating Markov Decision Process (MDP). The algorithm proceeds in episodes where, in each episode, it picks a policy using regularization based on the span of the optimal bias vector. For an MDP with S states and A actions whose optimal bias vector has span bounded by H, we show a regret bound of Õ(HS √ AT ). We also relate the span to various diameter-like quantities associated with the MDP, demonstrating how our results improve on previous regret bounds."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "In reinforcement learning, an agent interacts with an environment while trying to maximize the total reward it accumulates. Markov Decision Processes (MDPs) are the most commonly used model for the environment. To every MDP is associated a state space S and an action space A. Suppose there are S states and A actions. The parameters of the MDP then consist of S · A state transition distributions Ps,a and S · A rewards r(s, a). When the agent takes action a in state s, it receives reward r(s, a) and the probability that it moves to state s′ is Ps,a(s′). The agent does not know the parameters Ps,a and r(s, a) of the MDP in advance but has to learn them by directly interacting with the environment. In doing so, it faces the exploration vs. exploitation trade-off that Kearns and Singh [2002] succinctly describe as,\n“. . . should the agent exploit its cumulative experience so far, by executing the action that currently seems best, or should it execute a different action, with the hope of gaining information or experience that could lead\nto higher future payoffs? Too little exploration can prevent the agent from ever converging to the optimal behavior, while too much exploration can prevent the agent from gaining near-optimal payoff in a timely fashion.”\nSuppose the agent uses an algorithm G to choose its actions based on the history of its interactions with the MDP starting from some initial state s1. Denoting the (random) reward obtained at time t by rt, the algorithm’s expected reward until time T is\nRG(s1, T ) = E [ T∑ t=1 rt ] .\nSuppose λ? is the optimal per-step reward. An important quantity used to measure how well G is handling the exploration vs. exploitation trade-off is the regret,\n∆G(s1, T ) = λ?T −RG(s1, T ) .\nIf ∆G(s1, T ) is o(T ) then G is indeed learning something useful about the MDP since its expected average reward then converges to the optimal value λ? (which can only be computed with the knowledge of the MDP parameters) in the limit T →∞.\nHowever, asymptotic results are of limited value and therefore it is desirable to have finite time bounds on the regret. To obtain such results, one has to work with a restricted class of MDPs. In the theory of MDPs, four fundamental subclasses have been studied. Unless otherwise specified, by a policy, we mean a deterministic stationary policy, i.e. simply a map π : S → A.\nErgodic Every policy induces a single recurrent class, i.e. it is possible to reach any state from any other state.\nUnichain Every policy induces a single recurrent class plus a possibly empty set of transient states.\nCommunicating For every s1, s2 ∈ S, there is some policy that takes one from s1 to s2.\nWeakly Communicating The state space S decomposes into two sets: in the first, each state is reachable from every other state in the set under some policy; in the second, all states are transient under all policies.\nUnless we modify our criterion, it is clear that regret guarantees cannot be given for general MDPs since the agent might be placed in a state from which reaching the optimally rewarding states is impossible. So, we consider the most general subclass:weakly communicating MDPs. For such MDPs, the optimal gain λ? is state independent and is obtained by solving the following optimality equations,\nh? + λ?e = max a∈A\n( r(s, a) + P>s,ah ? ) . (1)\nHere, h? is known as the bias vector. Note that if h? is a bias vector then so is h? + ce where e is the all 1’s vector. If we want to make the dependence of h?(s) and λ? on the underlying MDP M explicit, we will write them as h?(s;M) and λ?(M) respectively.\nIn this paper, we give an algorithm Regal, that achieves Õ ( sp(h?(M))S √ AT )\nregret with probability at least 1 − δ when started in any state of a weakly communicating MDP M . Here sp(h) is the span of h defined as,\nsp(h) := max s∈S h(s)−min s∈S h(s) .\nThe Õ(·) notation hides factors that are logarithmic in S,A, T and 1/δ.\nRegal is based on the idea of regularization that has been successfully applied to obtain low regret algorithms in other settings involving decision making under uncertainty. The main idea is simple to describe. Using its experience so far, the agent can build a set M such that with high probability, the true MDP lies in M. If the agent assumes that it is in the best of all possible worlds, it would choose an MDP M ′ ∈M to maximize λ?(M) and follow the optimal policy for M ′. Instead, Regal picks M ′ to trade-off high gain for low span, by maximizing the following regularized objective,\nλ?(M)− C sp(h?(M)) ."
    }, {
      "heading" : "1.1 RELATED WORK",
      "text" : "The problem of simultaneous estimation and control of MDPs has been studied in the control theory [Kumar\nand Varaiya, 1986], operations research [Burnetas and Katehakis, 1997] and machine learning communities. In the machine learning literature, finite time bounds for undiscounted MDPs were pioneered by Kearns and Singh [2002]. Their seminal work spawned a long thread of research [Brafman and Tennenholtz, 2002, Kakade, 2003, Strehl and Littman, 2005, Strehl et al., 2006, Auer and Ortner, 2007, Tewari and Bartlett, 2008, Auer et al., 2009a]. These efforts improved the S and A dependence of their bounds, investigated lower bounds and explored other ways to study the exploration vs. exploitation trade-off.\nThe results of Auer and Ortner [2007] and Tewari and Bartlett [2008] applied to unichain and ergodic MDPs respectively. Recently, Auer et al. [2009a] gave an algorithm for communicating MDPs whose expected regret after T steps is, with high probability,\nÕ(D(M)S √ AT ) .\nHere, D(M) is the diameter of the MDP M defined as\nD(M) := max s1 6=s2 min π Tπs1→s2 ,\nwhere Tπs1→s2 is the expected number of steps it takes policy π to get to s2 from s1. By definition, any communicating MDP has finite diameter. Let O denote the set of average-reward optimal policies. Previous work had considered various notions of “diameter”, such as,\nDworst(M) := max π max s1 6=s2 Tπs1→s2\nDopt(M) := min π∈O max s1 6=s2 Tπs1→s2 .\nClearly D ≤ Dopt ≤ Dworst, and so, everything else being the same, an upper bound on regret in terms of D is stronger.\nWe propose another diameter that we call the one-way diameter,\nDow(M) := max s min π Tπs1→s̄ , (2)\nwhere s̄ = argmaxs h?(s). We study the relationship between sp(h?), Dow and D in Section 4 below and prove that sp(h?) ≤ Dow ≤ D whenever the rewards are in [0, 1]. So, we not only extend the result of Auer et al. [2009a] to weakly communicating MDPs but also replace D by a smaller quantity, sp(h?)."
    }, {
      "heading" : "2 PRELIMINARIES",
      "text" : "We make the simplifying assumption that the rewards r(s, a) ∈ [0, 1] are known and only the transition probabilities Ps,a are unknown. This is not a restrictive assumption as the rewards can also be estimated at the\ncost of increasing the regret bound by some constant factor. The heart of the problem lies in not knowing the transition probabilities.\nRecall the definition of the dynamic programming operator T ,\n(T V )(s) := max a∈A\n( r(s, a) + P>s,aV ) .\nLet V n(s) denote the maximum (over all policies, stationary or non-stationary) possible expected reward obtainable in n steps starting from s. This can be computed via the simple recursion,\nV 0(s) = 0\nV n+1 = T V n .\nNote that the optimality equations (1) can be written succinctly in terms of the operator T ,\nh? + λ?e = T h? . (3)"
    }, {
      "heading" : "3 REGAL",
      "text" : "In this section, we present Regal, an algorithm (see Algorithm 1) inspired by the Ucrl2 algorithm of Auer et al. [2009a]. Before we describe the algorithm, let us set up some notation. Let N(s, a, s′; t) be the number of times the state-action-state triple (s, a, s′) has been visited up to time t. Let N(s, a; t) be defined similarly. Then, an estimate of the state transition probability at time t is\nP̂ ts,a(s ′) = N(s, a, s′; t) max{N(s, a; t), 1} , (4)\nUsing these estimates, we can define a set M(t) of MDPs such that the true MDP lies in Mk with high probability. The set M(t) consists of all those MDPs whose transition probabilities satisfy,∥∥∥Ps,a − P̂ ts,a∥∥∥\n1 ≤\n√ 12S log(2At/δ)\nmax{N(s, a, t), 1} . (5)\nLike Ucrl2, Regal works in episodes. Let tk denote the start of episode k. We will abbreviate N(s, a, s′; tk) and N(s, a; tk) to Nk(s, a, s′) and Nk(s, a) respectively. Also, let vk(s, a) = Nk+1(s, a)−Nk(s, a) be the number of times (s, a) is visited during episode k.\nFor choosing a policy to follow in episode k, Regal first finds an MDP Mk ∈ Mk := M(tk) that maximizes a “regularized” average optimal reward,\nλ?(M)− Ck sp(h?(M)) . (6)\nHere Ck is a regularization parameter that is to be set appropriately at the start of episode k. Regal then\nAlgorithm 1 REGularization based Regret Minimizing ALgorithm (Regal)\nfor episodes k = 1, 2, . . . , do tk ← current time Mk is the set of MDPs whose transition function satisfies (5) with t = tk Choose Mk ∈ Mk to maximize the following criterion over Mk,\nλ?(M)− Ck sp(h?(M)) .\nπk ← average reward optimal policy for Mk Follow πk until some s, a pair gets visited Nk(s, a) times\nend for\nfollows the average reward optimal policy πk for Mk until some state-action pair is visited Nk(s, a) times. When this happens, Regal enters the next episode.\nTheorem 1. Suppose Algorithm 1 is run for T steps in a weakly communicating MDP M starting from some state s1. Then, with probability at least 1 − δ, there exist choices for the regularization parameters Ck such that,\n∆(s1, T ) = O ( sp(h?(M))S √ AT log(AT/δ) ) .\nThe proof of this theorem is given in Section 6. Unfortunately, as the proof reveals, the choice of the parameter Ck in the theorem requires knowledge of the counts vk(s, a) before episode k begins. Therefore, we cannot run Regal with these choices of Ck. However, if an upper bound H on the span sp(h?(M)) of the true MDP M is known, solving the constrained version of the optimization problem (6) gives Algorithm 2 that enjoys the following guarantee.\nTheorem 2. Suppose Algorithm 2 is run for T steps in a weakly communicating MDP M starting in some state s1. Let the input parameter H be such that H ≥ sp(h?(M)). Then, with probability at least 1− δ,\n∆(s1, T ) = O ( HS √ AT log(AT/δ) ) .\nFinally, we also present a modification of Regal that does not require knowledge of an upper bound on sp(h∗)). The downside is that the regret guarantee we can prove becomes Õ(sp(h?) √ S3AT ). The modification, Regal.D, that is given as Algorithm 3 uses the so called doubling trick to guess the length of an episode. If the guess turns out to be incorrect, the guess is doubled.\nTheorem 3. Suppose Algorithm 3 is run for T steps in a weakly communicating MDP M starting from\nsome state s1. Let the input parameter c be chosen as c = 2S √ 12 log(2AT/δ) + √ 2 log(1/δ) .\nThen, with probability at least 1− δ, ∆(s1, T ) = O ( sp(h?(M)) √ S3AT log2(AT/δ) ) .\nAlgorithm 2 Regal.C: Constrained Optimization version of Regal\nInput parameter: H for episodes k = 1, 2, . . . , do tk ← current time Mk is the set of MDPs whose transition function satisfies (5) with t = tk Choose Mk ∈ Mk by solving the following optimization over M ∈Mk,\nmax λ?(M) subject to sp(h?(M)) ≤ H .\nπk ← average reward optimal policy for Mk Follow πk until some s, a pair gets visited Nk(s, a) times\nend for\nAlgorithm 3 Regal.D: Regal with doubling trick Input parameter: c for episodes k = 1, 2, . . . , do tk ← current time Mk is the set of MDPs whose transition function satisfies (5) with t = tk j ← 1 while every (s, a) has been visited less than Nk(s, a) times in this episode do `k,j ← 2j Choose Mk,j ∈ Mk to maximize the following criterion over Mk,\nλ?(M)− c√ `k,j sp(h?(M)) .\nπk,j ← average reward optimal policy for Mk,j Follow πk,j until: `k,j time steps have elapsed OR some s, a pair gets visited Nk(s, a) times\nend while end for"
    }, {
      "heading" : "4 SPAN AND DIAMETER",
      "text" : "Theorem 4. In any weakly communicating MDP, for any states s1, s2, and any policy π,\nh?(s2)− h?(s1) ≤ λ?Tπs1→s2\nProof. Let us first prove the theorem for all aperiodic weakly communicating MDPs. In such MDPs, value iteration is known to converge [Puterman, 1994]. That is, if we define a sequence of vectors using vn+1 = T vn starting from an arbitrary v0, then\nlim n→∞ vn(s1)− vn(s2) = h?(s1)− h?(s2) ,\nfor any s1, s2. If we choose v0 = 0, then vn = V n and hence we have\nlim n→∞ V n(s2)− V n(s1) = h?(s2)− h?(s1) ,\nfor any s1, s2. Let π be a stationary policy. Consider the following n-step non-stationary policy. Follow π starting from s1 and wait until s2 is reached. Suppose this happens in τ steps. Then follow the optimal (n− τ)-step policy. Note that τ is a random variable and, by definition,\nE[τ ] = Tπs1→s2 .\nThe expected reward obtained in this way is at least E[V n−τ (s2)]. This has to be less than the n-step optimal reward. That is,\nV n(s1) ≥ E[V n−τ (s2)] .\nThus, we have\nh?(s2)− h?(s1) = lim n→∞ V n(s2)− V n(s1)\n≤ lim n→∞ V n(s2)− E[V n−τ (s2)]\n= lim n→∞\nE[V n(s2)− V n−τ (s2)]\n= E[ lim n→∞\nV n(s2)− V n−τ (s2)]\n= E[λ?τ ] = λ?Tπs1→s2 .\nNow, if the MDP M is periodic apply the following aperiodicity transform [Puterman, 1994] to get a new MDP M̃\nr̃(s, a) = θr(s, a)\nP̃s,a = (1− θ)es + θPs,a ,\nwhere θ ∈ (0, 1). Note that M̃ is also weakly communicating. Let h̃?, λ̃? and T̃πs1→s2 denote the quantities associated with M̃ . It is easily verified that these are related to the corresponding quantities for M as follows,\nh̃? = h?\nλ̃? = θλ?\nT̃πs1→s2 = Tπs1→s2 θ .\nUsing these relations and the fact that we have proved the result for M̃ gives us,\nh?(s2)− h?(s1) = h̃?(s2)− h̃?(s1) ≤ λ̃?T̃πs1→s2\n= θλ? Tπs1→s2 θ = λ?Tπs1→s2 .\nThus, we have proved the result for all weakly communicating MDPs.\nWe can now derive the fact that sp(h?) ≤ Dow. Corollary 5. For any weakly communicating MDP M with reward in [0, 1], the one-way diameter is an upper bound on the span of the optimal bias vector,\nsp(h?(M)) ≤ Dow(M) ≤ D(M) .\nMoreover, both inequalities above can be arbitrarily loose.\nProof. The inequalities follow immediately from Theorem 4. To show that they can be loose, consider a two-state two-action MDP M such that\nP1,1 = (1, 0)> , P1,2 = (1− , )> , P2,1 = (0, 1)> , P2,2 = (0, 1)> ,\nand r(1, a) = 1 − α, r(2, a) = 1 for all a. It is easy to verify that λ? = 1 and h? = (−α/ , 0)> satisfy the optimality equations (3). Thus, sp(h?) = α/ and s̄ = 2. Therefore, Dow = 1/ . Moreover, D = ∞. Thus, both inequalities in the corollary can be arbitrarily loose."
    }, {
      "heading" : "5 LOWER BOUND",
      "text" : "Using Corollary 5, the bound of Theorem 1 can be written as Õ ( Dow(M)S √ AT )\n. In this section, we provide a lower bound that matches the upper bound except for the dependence on S.\nUsing the results in Section 4, it is possible to show that the algorithms of Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] enjoy a regret bound of Õ(Dow(M) √ SAT ) for ergodic MDPs. We therefore conjecture that the lower bound is tight.\nTheorem 6. There exists a universal constant c0 such that for any S,A, dow and any algorithm G, there exists an MDP M with Dow(M) ≤ dow such that for any T > SA and any s, we have\n∆G(s, T ) ≥ c0dow √ SAT .\nProof Sketch. Due to lack of space, we only outline the argument. We modify the MDP used by Auer et al. [2009a] to prove their lower bound. They build a large MDP with S states, A actions by putting together S/2 copies of an MDP with 2 states, A actions. For our lower bound, we modify the construction of their 2- state MDP as follows. For all a ∈ A, P1,a = (1 − α, α)>, r(1, a) = 0 and r(2, a) = 1. Thus, state 2 is the only rewarding state. For all but a single action a? ∈ A, P2,a = (δ, 1 − δ)> and for a?, set P2,a? = (δ − , 1− δ + ). We will choose α, δ and later such that < δ α.\nThe idea is that taking a? in state 2 increases the probability of staying in that rewarding state slightly. So, the agent needs to figure out which action is a?. To do this, each action needs to be probed at least cδ/ 2 times. The difference in the average reward of the policy that takes action a? in state 2 and the one that does not is\nα α+ δ − − α α+ δ > 4α\nprovided δ < α. Now connect the 1 states of the S/2 copies of the MDP using A additional deterministic actions per state in an A-ary tree. Suppose only one of the copies has the good action a? in it. Now, we need to probe at least cSAδ/ 2 times. Choosing δ α means that most of the time, we are in state 2 and number of probes is a good approximation to the time elapsed. Thus, in probing for a? we incur at least\ncSAδ\n2 4α = cSAδ 4α\nregret. Now set = δ √ SA/T and δ = α2. If T > SA, then < δ and regret is at least\nc √ SAT\n4α .\nFinally choose α = 1/dow. Noting that Dow for this MDP is 1/α, proves the theorem (note that the MDP we have constructed has diameter D = D2ow)."
    }, {
      "heading" : "6 ANALYSIS",
      "text" : "In this section we will prove Theorems 1,2 and 3. We will first set up notation and recall key lemmas from the technical report of Auer et al. [2009b] that will be useful for proving all three theorems.\nLet M denote the true (but unknown) underlying MDP. The quantities λ? and h? will be for this MDP throughout this section. Let `k = ∑ s,a vk(s, a) be the length of episode k. Let λk, h?k denote λ(M k), h?(Mk) respectively. The transition matrices of πk in Mk and\nM will be denoted by P̃k and Pk respectively. Further, assume that h? and h?k have been normalized to have their minimum component equal to 0, so that sp(h?) = ‖h?‖∞ and sp(h?k) = ‖h?k‖∞.\nLet vk, rk ∈ RS denote vectors such that\nvk(s) = vk(s, πk(s)) ,\nrk(s) = r(s, πk(s)) .\nNote that, by definition of πk, we have,\nh?k + λ ? ke = rk + P̃kh ? k . (7)\nThe following bound on the numberm of episodes until time T was proved in Auer et al. [2009b].\nLemma 7. The number m of episodes of Algorithms 1,2 and 3 up to step T ≥ SA is upper bounded as,\nm ≤ SA log2 8T SA .\nLet ∆k be the regret incurred in episode k, ∆k = ∑ s,a vk(s, a)[λ? − r(s, a)] .\nNote that the total regret equals,∑ k∈G ∆k + ∑ k∈B ∆k , (8)\nwhere G = {k : M ∈ Mk} and B = {1, . . . ,m} −G. The following result from Auer et al. [2009b] assures us that the contribution from “bad” episodes (in which the true MDP M does not lie in Mk) is small. Lemma 8. For Algorithms 1,2 and 3, with probability at least 1− δ, ∑\nk∈B\n∆k ≤ √ T ."
    }, {
      "heading" : "6.1 PROOF OF THEOREM 1",
      "text" : "Consider an episode k ∈ G. Then, we have\nλ?k − Ck sp(h?k) ≥ λ? − Ck sp(h?) . (9)\nTherefore, ∆k = ∑ s,a vk(s, a)[λ? − r(s, a)]\n≤ ∑ s,a vk(s, a)[λ?k − Ck sp(h?k) + Ck sp(h?)− r(s, a)]\n= ∑ s,a vk(s, a)[λ?k − r(s, a)]\n− Ck ∑ s,a vk(s, a)[sp(h?k)− sp(h?)]\n= vk(λ?ke− rk)− Ck ∑ s,a vk(s, a)[sp(h?k)− sp(h?)]\n= vk(P̃k − I)h?k − Ck ∑ s,a vk(s, a)[sp(h?k)− sp(h?)]\n[using (7)]\n= vk(P̃k −Pk + Pk − I)h?k − Ck ∑ s,a vk(s, a)[sp(h?k)− sp(h?)]\n≤ ‖vk(P̃k −Pk)‖1 sp(h?k) + vk(Pk − I)h?k − Ck ∑ s,a vk(s, a)[sp(h?k)− sp(h?)] . (10)\nLemma 9. With probability at least 1− δ,∑ k∈G vk(Pk − I)h?k\n≤ ∑ k∈G sp(h?k) √ 2`k log(1/δ)\n+ (sp(h?) + max k∈G 1 Ck )(m+ log(1/δ) .\nProof. Using Bernstein’s inequality (see, for example, [Cesa-Bianchi and Lugosi, 2006, Lemma A.8]) instead of Hoeffding-Azuma in the argument of [Auer et al., 2009b, Section B.4], we get, with probability at least 1− δ,∑\nk∈G\nvk(Pk − I)h?k ≤ √ 2 ∑ k∈G sp(h?k)2`k log(1/δ)\n+ max k∈G sp(h?k)(m+ log(1/δ) .\nEquation (9) gives sp(h?k) ≤ sp(h?)+1/Ck. Using this and sub-additivity of square-root gives the lemma.\nIf k ∈ G then Mk,M ∈Mk and so we also have\n‖vk(P̃k −Pk)‖1 ≤ 2 ∑ s,a vk(s, a)\n√ 12S log(2AT/δ)\nNk(s, a)\n(11)\nUsing this, Lemma 9 and plugging these into (10), we get ∑ k∈G ∆k ≤ ∑ k∈G sp(h?k) ( 2 ∑ s,a vk(s, a) √ 12S log(2AT/δ) Nk(s, a)\n+ √ 2`k log(1/δ)− Ck ∑ s,a vk(s, a) ) + ∑ k∈G Ck`k sp(h?)\n+ (sp(h?) + max k∈G 1 Ck )(m+ log(1/δ) .\nEquation (9) in Auer et al. [2009b] gives,∑ k ∑ s,a vk(s, a) Nk(s, a) ≤ √ 8SAT . (12)\nTherefore, choosing Ck to satisfy, Ck = 2 ∑ s,a vk(s, a) √ 12S log 2ATδ Nk(s,a) + √\n2`k log 1δ `k\ngives us∑ k∈G ∆k ≤ ∑ k∈G Ck`k sp(h?)\n+ (sp(h?) +\n√ T\n2 log(1/δ) (m+ log(1/δ)\n= O ( sp(h?)S √ AT log(AT/δ) ) Combining this with (8) and Lemma 8 finishes the proof of Theorem 1."
    }, {
      "heading" : "6.2 PROOF OF THEOREM 2",
      "text" : "Consider an episode k ∈ G. Then, we have λ?k ≥ λ?. Therefore,\n∆k = ∑ s,a vk(s, a)[λ? − r(s, a)]\n≤ ∑ s,a vk(s, a)[λ?k − r(s, a)]\n= vk(λ?ke− rk) = vk(P̃k − I)h?k = vk(P̃k −Pk + Pk − I)h?k ≤ ‖vk(P̃k −Pk)‖1 sp(h?k) + vk(Pk − I)h?k . (13)\nThe following lemma is proved like Lemma 9 (in this case, Hoeffding-Azuma suffices). Lemma 10. With probability at least 1− δ,∑\nk∈G\nvk(Pk − I)h?k ≤ H (√ 2T log(1/δ) +m ) .\nEquation (11) still holds if k ∈ G. Plugging it and Lemma 10 into (13), we get\n∑ k∈G ∆k ≤ H (∑ k∈G ∑ s,a 2vk(s, a) √ 12S log(2AT/δ) Nk(s, a)\n+ √ 2T log(1/δ) +m ) .\nUsing (12) now gives,∑ k∈G ∆k ≤ O(HS √ AT log(AT/δ)) .\nCombining this with (8) and Lemma 8 finishes the proof of Theorem 2."
    }, {
      "heading" : "6.3 PROOF OF THEOREM 3",
      "text" : "In this case, we have episodes consisting of several sub-episodes whose lengths increase in geometric progression. Let vk,j(s, a) be the number of times (s, a) is visited during sub-episode j of episode k. Thus, `k,j = ∑ s,a vk,j(s, a). Let λk,j , h ? k,j denote λ(Mk,j), h?(Mk,j) respectively. The transition matrices of πk,j in Mk,j and M will be denoted by P̃k,j and Pk,j respectively. Let vk,j , rk,j be defined accordingly.\nConsider an episode k ∈ G. Then, as in the proof of Theorem 1, we have,\nλ?k,j − Ck,j sp(h?k,j) ≥ λ? − Ck,j sp(h?) ,\nand therefore,\n∆k,j := ∑ s,a vk,j(s, a)[λ? − r(s, a)]\n≤ ∑ s,a vk,j(s, a)[λ?k,j − Ck,j sp(h?k,j)\n+ Ck,j sp(h?)− r(s, a)] = vk,j(λ?k,je− rk,j)\n− Ck,j ∑ s,a vk,j(s, a)[sp(h?k,j)− sp(h?)]\n= vk,j(P̃k,j − I)h?k,j − Ck,j ∑ s,a vk,j(s, a)[sp(h?k,j)− sp(h?)]\n≤ ‖vk,j(P̃k,j −Pk,j)‖1 sp(h?k,j) + vk,j(Pk,j − I)h?k,j − Ck,j ∑ s,a vk,j(s, a)[sp(h?k,j)− sp(h?)] . (14)\nIf k ∈ G then Mk,j ,M ∈Mk. Therefore,\n‖vk,j(P̃k,j −Pk)‖1\n≤ 2 ∑ s,a vk,j(s, a)\n√ 12S log(2AT/δ)\nNk(s, a)\n≤ 2 √ 12S log(2AT/δ) √∑\ns,a\nvk,j(s, a) · √√√√∑ s,a vk,j(s, a) Nk(s, a)\n≤ 2 √ 12S log(2AT/δ) √ `k,j · √ S\n[∵ vk,j(s, a) ≤ vk(s, a) ≤ Nk(s, a)] = 2S √ 12`k,j log(2AT/δ) (15)\nNote that this step differs slightly from its counterpart in the proof of Theorem 1. Here, we used CauchySchwarz (for the second inequality above) to get a bound solely in terms of the length `k,j of the subepisode. This allows us to deal with the problem of not knowing the visit counts vk,j(s, a) in advance.\nThe following lemma is proved exactly like Lemma 9. Lemma 11. With probability at least 1− δ,∑\nk∈G ∑ j vk,j(Pk,j − I)h?k,j\n≤ ∑ k∈G ∑ j sp(h?k,j) √ 2`k,j log(1/δ)\n+ (sp(h?) + max k,j 1 Ck,j )(m log2(T ) + log(1/δ) .\nCombining (15) and Lemma 11 with (14), we have,∑ k∈G ∆k\n= ∑ k∈G ∑ j ∆k,j\n≤ ∑ k∈G ∑ j sp(h?k,j) ( 2S √ 12`k,j log(2AT/δ)\n+ √ 2`k,j log(1/δ)− Ck,j`k,j )\n+ sp(h?) ∑ k∈G ∑ j Ck,j`k,j\n+ (sp(h?) + max k,j 1 Ck,j )(m log2(T ) + log(1/δ) .\nNow, setting Ck,j = c/ √ `k,j for\nc = 2S √ 12 log(2AT/δ) + √ 2 log(1/δ)\ngives us,∑ k∈G ∆k\n≤ sp(h?) ∑ k∈G ∑ j c √ `k,j (16)\n+ (sp(h?) + cmax k,j\n√ `k,j)(m log2(T ) + log(1/δ) .\nSince `k,j = 2j , ∑ j √ `k,j ≤ 6\n√ `k + 2. Therefore,∑\nk∈G ∑ j √ `k,j ≤ ∑ k∈G 6 √ `k + 2\n≤ 6 √ m √∑ k `k + 2m = 6 √ m √ T + 2m .\nSubstituting this into (16), gives∑ k∈G ∆k = O ( sp(h?) √ S3AT log2(AT/δ) ) .\nCombining this with (8) and Lemma 8 finishes the proof of Theorem 3."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We gratefully acknowledge the support of DARPA under award FA8750-05-2-0249."
    }, {
      "heading" : "A. N. Burnetas and M. N. Katehakis. Optimal adaptive",
      "text" : "policies for Markov decision processes. Mathematics of Operations Research, 22(1):222–255, 1997.\nNicolò Cesa-Bianchi and Gábor Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.\nSham Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, Gatsby Computational Neuroscience Unit, University College London, 2003.\nMichael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 49:209–232, 2002.\nP. R. Kumar and P. P. Varaiya. Stochastic systems: Estimation, identification, and adaptive control. Prentice Hall, 1986.\nMartin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, 1994.\nAlexander L. Strehl and Michael Littman. A theoretical analysis of model-based interval estimation. In Proceedings of the Twenty-Second International Conference on Machine Learning, pages 857–864. ACM Press, 2005.\nAlexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. PAC model-free reinforcement learning. In Proceedings of the Twenty-Third International Conference on Machine Learning, 2006.\nAmbuj Tewari and Peter L. Bartlett. Optimistic linear programming gives logarithmic regret for irreducible MDPs. In Advances in Neural Information Processing Systems 20, pages 1505–1512. MIT Press, 2008."
    } ],
    "references" : [ {
      "title" : "Logarithmic online regret bounds for undiscounted reinforcement learning",
      "author" : [ "Peter Auer", "Ronald Ortner" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Auer and Ortner.,? \\Q2007\\E",
      "shortCiteRegEx" : "Auer and Ortner.",
      "year" : 2007
    }, {
      "title" : "Nearoptimal regret bounds for reinforcement learning",
      "author" : [ "Peter Auer", "Thomas Jaksch", "Ronald Ortner" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Auer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2009
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning (full version), 2009b. URL: http://institute.unileoben.ac.at/infotech/publications/ ucrl2.pdf",
      "author" : [ "Peter Auer", "Thomas Jaksch", "Ronald Ortner" ],
      "venue" : null,
      "citeRegEx" : "Auer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2009
    }, {
      "title" : "R-MAX – a general polynomial time algorithm for near-optimal reinforcement learning",
      "author" : [ "Ronen I. Brafman", "Moshe Tennenholtz" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Brafman and Tennenholtz.,? \\Q2002\\E",
      "shortCiteRegEx" : "Brafman and Tennenholtz.",
      "year" : 2002
    }, {
      "title" : "Optimal adaptive policies for Markov decision processes",
      "author" : [ "A.N. Burnetas", "M.N. Katehakis" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Burnetas and Katehakis.,? \\Q1997\\E",
      "shortCiteRegEx" : "Burnetas and Katehakis.",
      "year" : 1997
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "On the Sample Complexity of Reinforcement Learning",
      "author" : [ "Sham Kakade" ],
      "venue" : "PhD thesis, Gatsby Computational Neuroscience Unit,",
      "citeRegEx" : "Kakade.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kakade.",
      "year" : 2003
    }, {
      "title" : "Near-optimal reinforcement learning in polynomial time",
      "author" : [ "Michael Kearns", "Satinder Singh" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Kearns and Singh.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kearns and Singh.",
      "year" : 2002
    }, {
      "title" : "Stochastic systems: Estimation, identification, and adaptive control",
      "author" : [ "P.R. Kumar", "P.P. Varaiya" ],
      "venue" : null,
      "citeRegEx" : "Kumar and Varaiya.,? \\Q1986\\E",
      "shortCiteRegEx" : "Kumar and Varaiya.",
      "year" : 1986
    }, {
      "title" : "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "author" : [ "Martin L. Puterman" ],
      "venue" : null,
      "citeRegEx" : "Puterman.,? \\Q1994\\E",
      "shortCiteRegEx" : "Puterman.",
      "year" : 1994
    }, {
      "title" : "A theoretical analysis of model-based interval estimation",
      "author" : [ "Alexander L. Strehl", "Michael Littman" ],
      "venue" : "In Proceedings of the Twenty-Second International Conference on Machine Learning,",
      "citeRegEx" : "Strehl and Littman.,? \\Q2005\\E",
      "shortCiteRegEx" : "Strehl and Littman.",
      "year" : 2005
    }, {
      "title" : "PAC model-free reinforcement learning",
      "author" : [ "Alexander L. Strehl", "Lihong Li", "Eric Wiewiora", "John Langford", "Michael L. Littman" ],
      "venue" : "In Proceedings of the Twenty-Third International Conference on Machine Learning,",
      "citeRegEx" : "Strehl et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Strehl et al\\.",
      "year" : 2006
    }, {
      "title" : "Optimistic linear programming gives logarithmic regret for irreducible MDPs",
      "author" : [ "Ambuj Tewari", "Peter L. Bartlett" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Tewari and Bartlett.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tewari and Bartlett.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "exploitation trade-off that Kearns and Singh [2002] succinctly describe as,",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : "The problem of simultaneous estimation and control of MDPs has been studied in the control theory [Kumar and Varaiya, 1986], operations research [Burnetas and Katehakis, 1997] and machine learning communities.",
      "startOffset" : 98,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "The problem of simultaneous estimation and control of MDPs has been studied in the control theory [Kumar and Varaiya, 1986], operations research [Burnetas and Katehakis, 1997] and machine learning communities.",
      "startOffset" : 145,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "The problem of simultaneous estimation and control of MDPs has been studied in the control theory [Kumar and Varaiya, 1986], operations research [Burnetas and Katehakis, 1997] and machine learning communities. In the machine learning literature, finite time bounds for undiscounted MDPs were pioneered by Kearns and Singh [2002]. Their seminal work spawned a long thread of research [Brafman and Tennenholtz, 2002, Kakade, 2003, Strehl and Littman, 2005, Strehl et al.",
      "startOffset" : 146,
      "endOffset" : 329
    }, {
      "referenceID" : 0,
      "context" : ", 2006, Auer and Ortner, 2007, Tewari and Bartlett, 2008, Auer et al., 2009a]. These efforts improved the S and A dependence of their bounds, investigated lower bounds and explored other ways to study the exploration vs. exploitation trade-off. The results of Auer and Ortner [2007] and Tewari and Bartlett [2008] applied to unichain and ergodic MDPs respectively.",
      "startOffset" : 8,
      "endOffset" : 283
    }, {
      "referenceID" : 0,
      "context" : ", 2006, Auer and Ortner, 2007, Tewari and Bartlett, 2008, Auer et al., 2009a]. These efforts improved the S and A dependence of their bounds, investigated lower bounds and explored other ways to study the exploration vs. exploitation trade-off. The results of Auer and Ortner [2007] and Tewari and Bartlett [2008] applied to unichain and ergodic MDPs respectively.",
      "startOffset" : 8,
      "endOffset" : 314
    }, {
      "referenceID" : 0,
      "context" : ", 2006, Auer and Ortner, 2007, Tewari and Bartlett, 2008, Auer et al., 2009a]. These efforts improved the S and A dependence of their bounds, investigated lower bounds and explored other ways to study the exploration vs. exploitation trade-off. The results of Auer and Ortner [2007] and Tewari and Bartlett [2008] applied to unichain and ergodic MDPs respectively. Recently, Auer et al. [2009a] gave an algorithm for communicating MDPs whose expected regret after T steps is, with high probability, Õ(D(M)S √ AT ) .",
      "startOffset" : 8,
      "endOffset" : 395
    }, {
      "referenceID" : 1,
      "context" : "So, we not only extend the result of Auer et al. [2009a] to weakly communicating MDPs but also replace D by a smaller quantity, sp(h).",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "In this section, we present Regal, an algorithm (see Algorithm 1) inspired by the Ucrl2 algorithm of Auer et al. [2009a]. Before we describe the algorithm, let us set up some notation.",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 9,
      "context" : "In such MDPs, value iteration is known to converge [Puterman, 1994].",
      "startOffset" : 51,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "Now, if the MDP M is periodic apply the following aperiodicity transform [Puterman, 1994] to get a new MDP M̃",
      "startOffset" : 73,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "Using the results in Section 4, it is possible to show that the algorithms of Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] enjoy a regret bound of Õ(Dow(M) √ SAT ) for ergodic MDPs.",
      "startOffset" : 78,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "Using the results in Section 4, it is possible to show that the algorithms of Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] enjoy a regret bound of Õ(Dow(M) √ SAT ) for ergodic MDPs.",
      "startOffset" : 78,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "We modify the MDP used by Auer et al. [2009a] to prove their lower bound.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "We will first set up notation and recall key lemmas from the technical report of Auer et al. [2009b] that will be useful for proving all three theorems.",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "The following bound on the numberm of episodes until time T was proved in Auer et al. [2009b]. Lemma 7.",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "The following result from Auer et al. [2009b] assures us that the contribution from “bad” episodes (in which the true MDP M does not lie in M) is small.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "Equation (9) in Auer et al. [2009b] gives, ∑",
      "startOffset" : 16,
      "endOffset" : 36
    } ],
    "year" : 2009,
    "abstractText" : "We provide an algorithm that achieves the optimal regret rate in an unknown weakly communicating Markov Decision Process (MDP). The algorithm proceeds in episodes where, in each episode, it picks a policy using regularization based on the span of the optimal bias vector. For an MDP with S states and A actions whose optimal bias vector has span bounded by H, we show a regret bound of Õ(HS √ AT ). We also relate the span to various diameter-like quantities associated with the MDP, demonstrating how our results improve on previous regret bounds.",
    "creator" : "TeX"
  }
}