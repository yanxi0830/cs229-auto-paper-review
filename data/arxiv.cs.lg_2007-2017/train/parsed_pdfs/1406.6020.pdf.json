{
  "name" : "1406.6020.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 6.\n60 20\nv1 [\ncs .L\nG ]\n2 3\nJu n\n20 14"
    }, {
      "heading" : "1 Introduction",
      "text" : "Bandit with mixing arms. The bandit problem consists in an agent who has to choose at each step between K arms. A stochastic process is associated to each arm, and pulling an arm produces a reward which is the realization of the corresponding stochastic process. The objective of the agent is to maximize its long term reward. It is classically assumed that the stochastic process associated to each arm is a sequence of independently and identically distributed (i.i.d) random variables (see, e.g. [1]). In that case, the challenge the agent has to face is the well-known exploration/exploitation problem: she has to simultaneously make sure that she collects information from all arms to try to identify the most rewarding ones —this is exploration— and to maximize the rewards along the sequence of pulls she performs —this is exploitation. Many algorithms have been proposed to solve this trade-off between exploration and exploitation [2, 3, 4, 1]. We propose to go a step further than the i.i.d setting and to work in the situation where the process associated with each arm is a stationary ϕ-mixing process and the rewards are thus dependent from one another, but with a strength of dependence that weakens over time. From an application point of view, this is a reasonable dependence structure: if a user clicks on some ad (a typical use of bandit algorithms) at some point in time, it is very unlikely that she will click again on this ad in the near future. As it shall appear in the sequel, working with such dependent observations poses the question of how informative are some of the rewards with respect to the value of an arm since, because of the dependencies and the high correlation between close-by (in time) rewards, they might not reflect the true ’value’ of the arms. However, as the dependencies weaken over time, some kind of independence might be recovered if rewards are ignored. This actually requires to deal with a new trade-off exploration/exploitation/independence that need be precisely handled.\nRested and Restless case. A closely related setup that addresses the bandit problem with dependent rewards is when they are distributed according to Markov processes, such as Markov chains and Markov decision process (MDP) [5, 6], where the dependences between rewards are of bounded range, which is what distinguishes those works with ours. Contributions in this area study two settings, that we will analyze as well: the rested case, where the process attached to an arm evolves only when the arm is pulled, and the restless case, where all processes simultaneously evolve at each time step.\nContributions and structure of the paper. We define the notion of a ϕ-mixing bandit and its regret (Section 2), we provide a general analysis and an algorithm to solve the rested case where the size of independence blocks is fixed (Section 3). We provide another approach where these sizes are computed from the data by introducing another algorithm (Section 4). Finally, in Section 5, we provide an algorithm and a regret analysis to deal with the restless case."
    }, {
      "heading" : "2 Overview of the problem",
      "text" : "Let (ω,F ,P) be a probability space. We recall the definitions of stationary and of ϕ-mixing processes:\nDefinition 1 (Stationarity). A sequence of random variables X = {Xt}+∞t=−∞ is stationary if, for any t and nonnegative integerm and s, the random subsequences (Xt, . . . , Xt+m) and (Xt+s, . . . , Xt+m+s) are identically distributed.\nDefinition 2 (ϕ-mixing process). Let X = {Xt}+∞t=−∞ be a stationary sequence of random variables. For any i, j ∈ Z∪{−∞,+∞}, let σji denote the σ-algebra generated by the random variables Xt, i ≤ t ≤ j. Then, for any positive integer n, the ϕ-mixing coefficient ϕ(n) of the stochastic process X is defined as\nϕ(n) = sup t,A∈σ+∞\nt+k ,B∈σt −∞\n|P [A|B]− P [A]| . (1)\nX is said to be ϕ-mixing if ϕ(n) → 0 as n→ ∞.\nWe are interested in the problem of sampling from a K-armed ϕ-mixing bandit. In our setting, pulling arm k at time t provides the agent with a realization of the random variable Xk\nτk(t) , where\nτk(t) = t in the restless case and τk(t) is the number of times arm k was pulled in the rested case, and where the family (Xkt )t≥1 satisfies the following hypotheses :\n1. ∀k, (Xkt )t∈Z is stationary;\n2. the sequences (Xkt )t∈Z are ϕ-mixing;\n3. each Xk1 takes values in a discrete finite set.\nThis setting assumes the possibility of long-term dependencies between the rewards output by the arms. It is important to note that by definition of the ϕ-mixing processes, the amount of dependence decreases with time. Hence, as evoked earlier, in order to choose which arm to pull, the agent is forced to address the exploration/exploitation/independence trade-off where independence may be partially recovered by ignoring some rewards so as to make computations on data that are distant in time, i.e. data that are not too correlated (thanks to the mixing property).\nIt is critical to note that unlike in the i.i.d. framework, Hoeffding inequality cannot be applied in this setting, thus the widely used upper confidence bound (UCB) algorithms cannot be used here. In the case of stationary ϕ-mixing distributions, we have the following theorem from [7].\nTheorem 1 ([7, 8]). Let ψ : Um → R be a function defined over a countable space U , and X be a stationary ϕ mixing process. If ψ is l-Lipschitz with respect to the Hamming metric for some l > 0, then the following holds for all t > 0:\nPX [|ψ(X)− Eψ(X)| > t] ≤ 2 exp [ − t 2\n2ml2‖Λm‖2∞\n] , (2)\nwhere ‖Λm‖∞ ≤ 1 + 2 ∑m k=1 ϕ(k).\nIn the following, we consider a more general framework than the one usually encountered in the bandit literature. Instead of looking at cumulative gains, we look at rewards computed according to Lipschitz functions meeting the requirements of the concentration inequality stated in Theorem 1.\nMore precisely, we suppose that we have K known families (ψkt )t≥1 k = 1, . . . ,K of functions such that for every non-negative integer m:\n1. ψkm : Um → R accounts for the reward associated with m consecutive outcomes of arm k;\n2. ψkm is 1-Lipchitz with respect to the Hamming metric.\nIn the following sections, we use m to identify the reward functions (ψkm)1≤k≤K we want to optimize, b to refer to the size of the independence blocks and we consider blocs of s trials. We study three different scenarios. In the next section, we will present a general algorithm and regret analysis in the rested case where m is fixed and s . = m + b. In Section 4, with an additional hypothesis on the ϕ-mixing processes, we will take another approach to the rested case when m + b | s (i.e. s is a multiple of m + b) by including the notion of dependency into the regret. Finally, in Section 5, we present a general algorithm and regret analysis for the restless case."
    }, {
      "heading" : "3 Mixing Bandits in the Rested Case",
      "text" : "Regret analysis. Here, we are going to analyze the situation where m and b are fixed. Our goal is to show that a simple algorithm derived from UCB that works by making blocks s . = m + b consecutive trials on each arm has low regret, with a notion of regret that we define in the sequel. The running time index is therefore of the form of st, with t the number of times arm selection has been performed, and the sequence accessed to are such as (Xkst, . . . , X k (s+1)t−1). The reward that is accessed to at time t (with a slight abuse of notation that makes us use t as the time index) when pulling arm k does not make use of the full information provided by this sequence but instead is ψkm(X k t,s,b) where\nXkt,s,b . = (Xkst, . . . , X k (s+1)t−b−1); (3)\nthis means that only the first m points from (Xst, . . . , X(s+1)t−1) are taken advantage of. Given τ , the total number of trials of s-blocks, the regret that we are going to work with is\nR .= τµ∗ψ,m − K∑\nk=1\nEτk(τ)µ k ψ,m (4)\nwhere: µ∗ψ,m . = maxk=1,...,K µ k ψ,m, µ k ψ,m . = EXk1 ,...,Xkmψ k m(X k 1 , . . . , X k m) and τk(t) is the number of times arm k has been chosen given that a total (i.e. over all arms) of t pulls of s-blocks have been performed.\nThe arm selection procedure of the algorithm that we propose and dub Block-UCB, is depicted in Algorithm 1, where the function Λk is defined as\nΛk(t) . = 1 + 2\nt∑\nr=1\nϕk(rb + (r − 1)m). (5)\nIt is possible to show that Block-UCB has the following regret.\nTheorem 2 (Regret of Block-UCB). The regret of Block-UCB is bounded by\n∑\nk:µk ψ,m <µ∗ ψ,m\n( uk∆k + 1\nα− 2\n) ,\nwhere the uk’s are the solutions of the problems\nuk∆ 2 k − 8αΛ2k(uk) log τ = 0, k = 1, . . . ,K. (6)\nThe result of Theorem 2 hinges on the derivation of a concentration inequality for each arm\nk that relates the random variable 1 τ ∑τ−1 r=0 ψ k m ( Xkr,s,b ) to µkψ,m. To establish this concentration inequality, we study the random variables Γkb (X k 0,s,b, . . . , X k τ−1,s,b), defined for b ≥ 0 as\nΓkb (X k 0,s,b, . . . , X k τ−1,s,b)\n. =\n1\nτ\nτ−1∑\nr=0\nψkm(X k rs, . . . , X k (r+1)s−b−1)− µkψ,m. (7)\nThe concentration inequality that we are going to use to prove our regret bound is the following:\nAlgorithm 1 Main iteration of Block-UCB\nChoose arm It as\nIt ∈ argmax k\n1\nt\nt−1∑\nr=0\nψkm ( Xkr,s,b ) + Λk(τk(t− 1))\n√ 2α log t\nτk(t− 1) .\nTheorem 3. For all τ, k, b, and assuming that ψkm takes value in [0; 1]:\nP (∣∣∣Γkb (Xk0,s,b, . . . , Xkτ−1,s,b) ∣∣∣ ≥ ε ) ≤ exp ( − τε 2\n2Λ2k(τ)\n) , (8)\nwhere Λk is defined in (5).\nProof. We make the proof for some arm k. We also assume that s and b are fixed and to lighten the notation, we drop the dependence on these variables when no confusion is possible: we use Xr (resp. Γ) for Xkr,s,b (resp. Γb). The proof hinges on the fact that since (Xt)t≥0 is a stationary mixing sequence with mixing coefficients (ϕ(t))t≥1, (Xr,)r≥0 is a stationary mixing sequence with mixing coefficients (ϕ(q))q≥1 such that ϕ(q) . = ϕ(qb + (q − 1)m) (see Proposition 5, Appendix). To obtain the targeted result, we make use of the concentration inequality of Theorem 1 with the function Γ to get (8) and we exploit the observation that EΓkb = 0, which results from the stationarity of (Xt)t≥0.\nLet q be an integer in {0, . . . , r − 1}, x0, . . . , xr−1 and x′q blocks from Us−b. Then:\n∣∣Γ(x0, . . . , xq, . . . , xr−1)− Γ(x0, . . . , x′q, . . . , xr−1) ∣∣ = ∣∣∣∣ 1\nτ (ψs−b(xq)− ψs−b(x′q))\n∣∣∣∣ ≤ 1\nτ ,\nwhich comes from the range of ψs−b being [0; 1]. Γ is therefore 1/τ -Lipschitz with respect to the Hamming metric, which, combined with (Xr)r≥0 being a ϕ-mixing sequence, gives (8).\nThe proof of the previous theorem uses the following more general result, that is of independent interest.\nTheorem 4 (General Regret). Suppose that the arms we work with are such that\n∀k, P( ∣∣µ̂kτ − µk ∣∣ ≥ ε) exp (−θk(τ)γk(ε)) , (9)\nwhere τ is the number of data the empirical mean µ̂kτ is computed on, and θk and εk are increasing functions defined on (0;+∞].\nConsider the regret defined by\nR . = τµ∗ −\nK∑\nk=1\nEτk(τ)µk\nwhere τk(t) is the number of times a (suboptimal) arm k has been chosen up to time t. The (α, θ, γ)-UCB that chooses at iteration t an arm It according to\nIt ∈ argmax µ̂kτk(t−1) + γ −1 k\n( α\nθk(τk(t− 1)) log t\n)\nhas regret bounded by: ∑\nk:µi<µ∗\n(⌈ θ−1k ( α log τ\nγk(∆k/2)\n)⌉ ∆k + 1\nα− 2\n) . (10)\nProof. Note that Theorem 2 is a consequence of this theorem with θk(s) = s/Λ 2 k(s) and γk(ε) = ε2/2.\nThe proof use the standard technique to prove the regret of UCB-like algorithms. Namely, at iteration t, if It = i for i not optimal, then one of the following events E∗1 (t), E2(i, t), E3(i, t) must occur\nE∗1 (t) . = { µ̂∗ < µ∗ − γ−1∗ ( α\nθ∗(τ∗(t− 1)) log t\n)} , (11)\nE2(i, t) .= { µi ≤ µ̂i − γ−1i ( α\nθi(τi(t− 1)) log t\n)} , (12)\nE3(i, t) .= { τi(t− 1) ≤ θ−1i ( α log τ\nγi(∆i/2)\n)} . (13)\nIndeed, if none of the events occurs then (using ∆i = µ ∗ − µi)\nµ̂ ∗+γ−1\n∗\n(\nα\nθ∗(τ∗(t− 1)) log t\n)\n(11)\n≥ µi+∆i (12) > µi+2γ −1 i\n(\nα\nθi(τi(t− 1)) log τ\n)\n(13) > µ̂i+γ −1 i\n(\nα\nθi(τi(t− 1)) log t\n)\nwhere we have used that t 7→ γ−1i (a log t) is an increasing function of t on [1;∞) whenever a > 0. This implies that It 6= i, which contradicts our working hypothesis.\nIf we let u be defined as:\nu . = ⌈ θ−1i ( α log τ\nγi(∆i/2)\n)⌉ ,\nthen, for i suboptimal, we have the following\nEτi(τ) =\nτ∑\nt=1\nE1[It=i] ≤ u+ τ∑\nt=u+1\nE1[It=i ∧ ¬E3(i,t)] ≤ u+ τ∑\nt=u+1\nE1[E∗1 (t) ∨ E2(i,t)]\n≤ u+ τ∑\nt=u+1\n[P(∃t : E∗1 (t)) + P(∃t : E2(i, t))]\nUsing the union bound and Equation (9), both probabilities P(∃t : E∗1 (t)), P(∃t : E2(i, t)) can be bounded from above by 1/tα. Standard calculations allow us to get desired result (10).\nDiscussion. Some observations must be made regarding the result of Theorem 2. First, as we used Theorem 4 to prove our regret bound, it is necessary for the result to hold for the functions θk . = τ/Λ2k(τ) to be increasing. In addition, the regret only makes sense if it is bounded, i.e. if the uk∆k are bounded. Finally, if these conditions hold, it might be interesting to find, for some fixed horizon τ , the value of b that minimizes the regret. We now depicts common settings for the ϕk that yield instructive results and that build upon the previous remarks. For the sake of conciseness, we will assume that all ϕ1 = . . . = ϕK and we use ϕ to refer to the mixing coefficients.\nIndependent case. if ϕ = 0, i.e. we are in the independent case, and the θk’s are naturally increasing. In addition, it is straightforward to observe that the best use of the data is achieved for b = 0, i.e. each and every reward is used to estimate the quality of an arm.\nCase Λ < +∞. In that case, we are again back to a situation almost similar to the usual independent case. The θk’s are increasing, the uk are well-defined and the regret as the usual O( ∑\nk log τ/∆k) form.\nAlgebraically mixing case. Here, ϕ(t) = ϕ0t −p for p > 1, and a few calculations give\nΛ(τ ) = 1 + 2ϕ0\nτ ∑\nr=1\n1\n(rs−m)p ≤ 1 + 2ϕ0\n(\n1 +\n∫ τ\n1\n1\n(rs−m)p dr\n)\n= 1 + 2ϕ0 + 2ϕ0\ns(p− 1)\n(\n1\nbp−1 −\n1\n(τs−m)p−1\n)\nUsing this upper bound to find the uks as in (6) and to solve for b so that this bound is minimized provides a way to find a data-dependent b. Another (coarser) way to look at the algebraically mixing situation is not to optimize for b and to consider that it is a particular case of the previous case, since ∑ t ϕ(t) <∞, i.e. Λ < +∞. This assumption is made in the rest of the paper."
    }, {
      "heading" : "4 m + b | s: Expressing the Independence Trade-Off in the",
      "text" : "Regret\nThis section introduces another way of encoding the trade-off between exploration, exploitation and independence. As before, we consider sequences of s trials, but among the s results, we seek to optimize the number m of results we use to update the empirical value of the arms and the number b of results we ignore in order to improve the independence between the considered realization of our random variables.\nIn addition to the case s = m+ b, we also consider the situation where β(m+ b) = s with β ∈ N and β > 1. The sequence of s trials can then be interpreted as β successive sequences of m + b trials, and the value of this particular (m, b) distribution is thus multiplied by β."
    }, {
      "heading" : "4.1 Hypotheses and Regrets",
      "text" : "In this section we make the following additional assumption on the ϕ-mixing processes (Xkt )t≥0:\n∀1 ≤ k ≤ K, ∀b ∈ N, Mk(b) .= 1 + ∑\ni≥1\nϕk(b(i+ 1)) < +∞.\nNote that this is equivalent to the widely used assumption that the ϕk(i) are summable over i (see for instance the case of algebraically mixing sequence mentioned before). Also, note that Mk(1) is an upper bound of (Λkm)m which appears in Theorem 1, and Mk(·) is a decreasing function such that Mk(b) ≥ 1.\nThe setting is the following: at each step, the agent pulls an arm s times, and has to choose how to split those s elements between a meaningful part of m elements, used to update empirical values of the arm, and the non-significant part of b element, used to strengthen the independence between the variables. For each such combination m+ b | s and for each arm k, we define the value of the combination (m, b, k) as:\nνkm,b . = βm,b Mk(b) µkm, (14)\nwhere µkm . = EXk1 ,...,Xkmψ k m(X k 1 , . . . , X k m) and βm,b = s/(m + b). This value explicitly shows the trade-off between independence (through Mk(b)) and exploitation (through the µ k m,b). The value of an arm k is then defined as the maximum value of the possible combination (m, b, k), for m+b|s. With this in mind, we define the regret R at time t as :\nR .= K∑\nk=1\nE(T (k)) ( νk ∗\nm∗,b∗ − νkm∗ k ,b∗ k\n) (15)\nwhere (k∗,m∗, b∗) = argmax(k,m,b) ν k mk,bk , (m∗k, b ∗ k) = argmax(m,b) ν k mk,bk\n, and T (k) denotes the number of times arm k was pulled.\nIt is important to note that one of the main difference between (15) and the classical formulation of regret from a multi arm bandit in the i.i.d. case is that in our setting, we are comparing the value of the best combination of the best arm with the value of the best combination of the pulled arm."
    }, {
      "heading" : "4.2 Concentration inequality and algorithm",
      "text" : "We now introduce a concentration inequality tailored for the νkm,b of (14).\nProposition 1. Let 1 ≤ k ≤ K, 1 ≤ m ≤ s, b = m − s, n ∈ N∗, ψkm : Um → R be a 1-Lipschitz with respect to the Hamming metric function defined over a countable space U . Suppose that the ϕk(i) are summable over i, and let us define Mk(b) = 1 + ∑ i∈N∗ ϕ\nk(i(b + 1)), and ζsm : N 7→ N, ζsm(t) = t+ b⌊(t− 1)/m⌋. Then the following holds for all t > 0:\nP\n[ 1\nMk(b) ∣∣∣∣∣ 1 n n−1∑\ni=0\nψkm ( Xkζsm(im+1), · · · , X k ζsm((i+1)m) ) − µkm ∣∣∣∣∣ > t ] ≤ 2 exp [ −nt 2 2 ] . (16)\nAlgorithm 2 Block-UCB with parameters s, α fixed\nt← 0, ψ̂km,b,0 ← 0, k = 1, . . . ,K,m+ b | s, τk,0 ← 0, k = 1, . . . ,K for t = 1, . . . , τ do\nSelect arm k̂ with k̂ ∈ argmax k max m+b|s\ns\nm+ b\n( ψ̂km,b,t−1 + √ 2α(m+ b) log(t)\nsτk,t−1\n)\nUpdate the block counts τk,t ← τk,t−1 + δkk̂, ∀k Compute the values of the ψ̂km,b,t, ∀m+ b | s\nψ̂k̂m,b,t ← 1\nMk(b)βτk̂,t\nβ(τ k̂,t −1)∑\nr=0\nψk̂m ( X k̂r(m+b), . . . , X k̂ (m+b)r+m−1 )\nψ̂km,t ← ψ̂km,t−1, for k 6= k̂\nend for\nProof. This proposition naturally follows from the proof of Theorem 1 with the function φkn = 1 n ∑n−1 i=0 ψ k m ( Xζsm(im+1), · · · , Xζsm((i+1)m) ) which is 1/n-Lipschitz with respect to the Hamming metric, and Proposition 5 ( see Appendix).\nNote that the upper bound on the probability that appears in (16) is uniform over (k,m, b). This is crucial to define our algorithm and to analyze its regret (more details in the next subsection). We now introduce Algorithm 2 that is designed for the particular setting of ϕ-mixing bandit problem just described. First, note that since the pair (m, b) (with m + b | s) which gives the best result for each arm is unknown, the algorithm needs to compute an empirical estimator for each of these combinations for each arm. In other words, the algorithm needs to efficiently and simultaneously learn both the best combination and the best arm."
    }, {
      "heading" : "4.3 Regret Analysis",
      "text" : "In this subsection we provide an upper bound for the regret of Algorithm 2. Proposition 2. Let R be the regret as defined in (15) and let η : N 7→ N, η(s) .= ∑s\ni=1 i1i|s Then,\nwith ∆i = ν k∗ m∗,b∗ − νim∗ i ,b∗ i\nR ≤ ∑\n1≤i≤K\n( (1 + η(s))∆i + 8αs log(t)\n∆i\n) .\nProof. The main difference with the standard technique to proving regret bounds comes from the fact that the value of each arm is the maximum of its coordinate: as suchs we have to consider the following event\nψk ∗ t ≤ νk ∗ m∗,b∗\n∃m+ b | s, νkm∗ k ,b∗ k ≤ s m+ b\n( ψ̂km,b,t−1 − √ 2α(m+ b) log(t)\nsτk,t−1\n)\n∃m+ b | s, 2 √ 2α(m+ b) log(t)\nsτk,t−1 ≥ νk∗m∗,b∗ − νkm∗ k ,b∗ k\nBy carefully using the property of the maximum, the result can be recovered. All the details of the proof can be found in the supplementary material.\nWe have just seen another approach to the rested mixing bandit. By assuming the summability of the ϕk, and by properly defining the value of an arm, we were able using Algorithm 2 to address the case where m and b are no longer fixed, but must be computed from the data by the agent.\nIt is interesting to note that the upper bound in Proposition 2 differs from the usual bound in the classical dependence-free setting by two multiplicative constants: η(s), which encodes the total number of combination of pair (m, b) such that m+ b | s, and s, which is in fact used as an upper bound for s/(m+ b)."
    }, {
      "heading" : "5 Restless ϕ mixing bandits",
      "text" : "In this section, we provide an analysis for the restless ϕ-mixing bandit. Recall that contrarily to the rested case studied previously, the stochastic processes associated to each arm evolves regardless of the actions of the agent. This difference is of paramount importance in the ϕ-mixing setting. Indeed, in the rested case, the agent was bound to ignore some realizations obtained from an arm to enforce the independence and therefore the accuracy of its predictor. In the restless case, instead of pulling an arm to no avail, an agent willing to increase the independence of the realization of an arm k can pull another arm k′ 6= k to gather information about k′ while enforcing the independence of k. This idea is central to this section.\nLike in Section 4, we assume that ∀k = 1, · · · ,K, the stochastic process Xk is a ϕ-mixing process, and its mixing coefficients ϕk(i) are summable, and we define the following upper bound function Mk, which differs for the one defined in the previous section:\n∀1 ≤ k ≤ K, ∀b ∈ N, Mk(b) .= 1 + ∑\ni≥b\nϕk(i) < +∞.\nIn the restless ϕ-mixing bandit, the agent pulls the arm k in sequences of mk trials, where the mk are fixed parameters and may differ for each arm. The mean value of this sequence is defined as follows:\nµk . = EXk1 ,...,Xkmk ψkm(X k 1 , . . . , X k mk )\nand we use the same definition of regret as defined in Section 3. In the restless setting, an interesting way of dealing with the trade-off exploration/exploitation/independence appears: in addition to the usual exploration, it might be interesting for the agent to pull an apparently sub-optimal arm to get an increased independence on the result of the other arms –since the time between two consecutive sequences of pull decrease their dependency. In order to study this trade-off, we introduce a suitable concentration inequality.\nProposition 3. Let 1 ≤ k ≤ K, 1 ≤ m ≤ s, b = m − s, n ∈ N∗, ψkm : Um → R be a 1-Lipschitz with respect to the Hamming metric function defined over a countable space U . Let ζsm : N 7→ N, be such that\nζsm(t) = t+ b1t≥m+1.\nThen the following holds for all t > 0:\nP [∣∣∣∣∣ 1 n n−1∑\ni=0\nψkm ( Xkζsm(im+1), · · · , X k ζsm((i+1)m) ) − µk ∣∣∣∣∣ > t ] ≤ 2 exp [ − nt 2\n2M2k(b)\n] . (17)\nProof. This proposition follows from the proof of Theorem 1 using the same technique as Proposition 1.\nIt is interesting to note that the independence trade-off naturally appears in the right term of inequality 17 within the Mk, and will modify the upper confidence bound. We introduce algorithm 3 to solve this particular setting of ϕ mixing bandit problem, and we provide a regret analysis for this algorithm.\nProposition 4 (Regret analysis). Let R(t) be the regret at time t for Algorithm 3. Then, with ∆i = µ\nk∗ − µi, Rt ≤ ∑\n1≤i≤K\n( ∆i + 8α log(t)\n∆i\n) .\nAlgorithm 3 Restless Block-UCB with parameters mk, α fixed\nt← 0, ψ̂k0 ← 0, τk,0 ← 0, , ηk,0 ← 0, k = 1, . . . ,K for t = 1, . . . , τ do\nSelect arm k̂ ∈ argmax k ψ̂kt−1 +\n√ 2α log(t)\nτk,t−1M2k(ηk,t) Update counters and timers :\nη k̂,t ← 0, τ k̂,t ← τ k̂,t−1 + 1,\n∀k 6= k̂, τk,t ← τk,t−1, and ηk,t ← ηk,t−1 +mk̂ Compute the values of the ψ̂kt , m+ b | s\nψ̂k̂t ← 1\nτ k̂,t\nτ k̂,t −1∑\nr=0\nψk̂m ( X k̂rm\nk̂ , . . . , X k̂(r+1)m k̂ −1\n)\nψ̂kt ← ψ̂kt−1, for k 6= k̂\nend for\nProof. The proof uses the same ideas as the previous regret analysis from Section 3 and 4, and naturally follows from Proposition 3. The proof can be found in the supplementary materials.\nIn this section, we have provided an algorithm for the restless ϕ-mixing framework. We have seen that the restless setting is a natural framework to use with ϕ-mixing bandit, as it naturally makes it possible to decrease the dependence of the variables for the arms not chosen. Our algorithm takes advantage of this observation and we were able to show that is has low regret. In order to do so, in addition to the usual τk, the number of time a given arm has been pulled, the Algorithm 3 computes the ηk, the time spent since the last time the arm k was sampled. Indeed, as seen in (17), as ηk increases, M2k(ηk) decreases and the optimistic value of the arm increases."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have studied an extension of the multi-armed bandit problem to the stationary ϕ-mixing framework, both in the rested and in the restless case. We have provided both a theoretical analysis in a general framework, and a more practical study of the problem in the case of fast mixing sequences (with ∑ ϕ(i) < +∞). For each of theses cases, we provided algorithms and accompanying regret analyses, which are strict extensions of the methods that exist for the i.i.d situation, as usual results might be recovered from our bounds when the mixing coefficients are all 0. Future works might include a study of the restless case where the mk has to be computed from the data, as well as a study in the more difficult case of β-mixing processes."
    }, {
      "heading" : "A Appendix",
      "text" : "Proposition 5. Let m, b ∈ N and s = m + b, Xt be a ϕ-mixing process on Ω taking value in R, with mixing coefficient ϕX(·) , and and ψ : Rm 7→ R be a measurable function. Then the stochastic process Zt defined by\nZt = ψ(Xst+1, . . . , Xst+m)\nis also a ϕ-mixing process with mixing coefficient ϕZ = ϕX ◦ κ, where κ(t) = bt+m(t− 1). Proof. In the following we use σ(A) to denote the σ-algebra generated by A. First note that since ψ is measurable σ(ψ−1(R)) ⊂ σ(Rm), and as a consequence\nσ(Zt) ⊂ σ(Xst+1, . . . , Xst+m)\n(since σ-algebra are closed under countable intersection) . Now for any i, j ∈ Z ∪ {−∞,+∞}, let σji (Z) denote the σ-algebra generated by the random variables Zk, i ≤ k ≤ j. Then, for any positive integer k, the ϕ-mixing coefficient ϕZ(t) of the stochastic process Z is defined as\nϕZ(t) = sup n,A∈σ+∞\nn+t(Z),B∈σ n −∞ (Z)\n|P [A|B]− P [A]|\n≤ sup n,A∈σ+∞\ns(n+t) (X),B∈σns+m −∞ (X)\n|P [A|B]− P [A]|\n= ϕX (s(n+ t)− ns−m) = ϕX(tb + (t− 1)m)"
    }, {
      "heading" : "B Proof of Proposition 2",
      "text" : "Proof : If at time t, the arm chosen is k instead of k∗, then one of the following must be true :\nψk ∗ t ≤ νk ∗ m∗,b∗ (18)\n∃m+ b | s, νkm∗ k ,b∗ k ≤ βm,b ( ψ̂km,b,t−1 − √ αJm,b(k, t− 1) ) (19)\n∃m+ b | s, 2 √ 2αβm,b log(t)\nτk,t−1 ≥ νk∗m∗,b∗ − νkm∗ k ,b∗ k\n(20)\nOtherwise, ∀m+ b | s,\nψk ∗ t ≥ νk ∗ m∗,b∗ ≥ νkm∗ k ,b∗ k + 2\n√ 2αβm,b log(t)\nτk,t−1\n≥ νkm∗ k ,b∗ k + 2\n√ 2αβm,b log(t)\nτk,t−1\n≥ βm,b ( ψ̂km,b,t−1 + √ αJm,b(k, t− 1) )\nSince the last line is true ∀m+b | s,, we deduce that ψk∗t ≥ ψkt hence T (t) 6= k, which is absurd. Then, we need to bound the probability of the events defined by (18), (19) and (20). Because ψk ∗\nt is defined as a maximum, we have\nP(ψk ∗ t ≤ νk ∗ m∗,b∗) ≤ P ( βm∗,b∗(ψ̂ k m,b,t−1 − √ αJm∗,b∗(k ∗, t− 1)) ≤ νk∗m∗,b∗ )\n≤ 1 tα ,\nusing Proposition 1. Now, by definition of νkm∗\nk ,b∗ k ,\nP ( ∃m+ b | s, νkm∗\nk ,b∗ k ≤ βm,b(ψ̂km,b,t−1 −\n√ αJm,b(k, t− 1))\n)\n≤ P ( ∃m+ b | s, νkm,b ≤ βm,b(ψ̂km,b,t−1 − √ αJm,b(k, t− 1)) )\n≤ σ(s) max m+b|s P\n( νkm,b ≤ βm,b(ψ̂km,b,t−1 − √ αJm,b(k, t− 1)) )\n≤ σ(s) tα\nwhere we used Proposition 1 again at the last line. Finally,\n{ ∃m+ b | s, 2 √ 2αβm,b log(t)\nτk,t−1 ≥ νk∗m∗,b∗ − νkm∗ k ,b∗ k\n} ⊂ { 2 √ 2αs log(t)\nτk,t−1 ≥ νk∗m∗,b∗ − νkm∗ k ,b∗ k\n}\n=\n{ 8αs log(t)\n(νk ∗ m∗,b∗ − νkm∗ k ,b∗ k )2\n≥ τk,t−1 }\ni.e. the event defined by (20) happens at most u =\n⌈ 8αs log(t)\n(νk ∗ m∗,b∗ − νkm∗ k ,b∗ k )2\n⌉ times.\nhence the conclusion"
    }, {
      "heading" : "C Proof of Proposition 5",
      "text" : "Proof. If at time t, the arm chosen is k instead of k∗, then one of the following must be true :\nψk ∗ t ≤ µk ∗\n(21)\nµk ≤ ( ψ̂kt−1 − √ αJ(k, t− 1) ) (22)\n2\n√ 2α log(t)\nM2k(ηk,t)τk,t−1 ≥ µk∗ − µk (23)\nFrom the last inequality, we deduce that :\nτk,t−1 ≤ 8α log(t) M2k(ηk,t)∆2k ≤ 8α log(t) ∆2k (24)\nsince M2k(ηk,t) ≤ 1."
    } ],
    "references" : [ {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "Advances in Applied Mathematics, 6:422",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Finite-time analysis of the multi- armed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning Journal, 47(23):235–256",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Ucb revisited: Improved regret bounds for the stochastic multi-armed bandit problem",
      "author" : [ "P. Auer", "R. Ortner" ],
      "venue" : "Periodica Mathematica Hungarica, 61:5565",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : "volume 5 of Foundation and Trends in Machine Learning. NOW",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Online learning of rested and restless bandits",
      "author" : [ "C. Tekin", "M. Liu" ],
      "venue" : "IEEE Transactions on Information Theory, 58(8):5588–5611",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Concentration inequalities for dependent random variables via the martingale method",
      "author" : [ "L. Kontorovich", "K. Ramanan" ],
      "venue" : "The Annals of Probability, 36(6):2126–2158",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Stability bounds for stationary -mixing and -mixing processes",
      "author" : [ "M. Mohri", "A. Rostamizadeh" ],
      "venue" : "Journal of Machine Learning Research, 11:789–814",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[1]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "Many algorithms have been proposed to solve this trade-off between exploration and exploitation [2, 3, 4, 1].",
      "startOffset" : 96,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "Many algorithms have been proposed to solve this trade-off between exploration and exploitation [2, 3, 4, 1].",
      "startOffset" : 96,
      "endOffset" : 108
    }, {
      "referenceID" : 3,
      "context" : "Many algorithms have been proposed to solve this trade-off between exploration and exploitation [2, 3, 4, 1].",
      "startOffset" : 96,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "Many algorithms have been proposed to solve this trade-off between exploration and exploitation [2, 3, 4, 1].",
      "startOffset" : 96,
      "endOffset" : 108
    }, {
      "referenceID" : 4,
      "context" : "A closely related setup that addresses the bandit problem with dependent rewards is when they are distributed according to Markov processes, such as Markov chains and Markov decision process (MDP) [5, 6], where the dependences between rewards are of bounded range, which is what distinguishes those works with ours.",
      "startOffset" : 197,
      "endOffset" : 203
    }, {
      "referenceID" : 5,
      "context" : "In the case of stationary φ-mixing distributions, we have the following theorem from [7].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "Theorem 1 ([7, 8]).",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 6,
      "context" : "Theorem 1 ([7, 8]).",
      "startOffset" : 11,
      "endOffset" : 17
    } ],
    "year" : 2014,
    "abstractText" : "We study the bandit problem where arms are associated with stationary φ-mixing processes and where rewards are therefore dependent: the question that arises from this setting is that of recovering some independence by ignoring the value of some rewards. As we shall see, the bandit problem we tackle requires us to address the exploration/exploitation/independence trade-off. To do so, we provide a UCB strategy together with a general regret analysis for the case where the size of the independence blocks (the ignored rewards) is fixed and we go a step beyond by providing an algorithm that is able to compute the size of the independence blocks from the data. Finally, we give an analysis of our bandit problem in the restless case, i.e., in the situation where the time counters for all mixing processes simultaneously evolve.",
    "creator" : "LaTeX with hyperref package"
  }
}