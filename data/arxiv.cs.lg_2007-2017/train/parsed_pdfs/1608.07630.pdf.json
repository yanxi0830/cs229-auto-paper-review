{
  "name" : "1608.07630.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Global Analysis of Expectation Maximization for Mixtures of Two Gaussians",
    "authors" : [ "Ji Xu", "Daniel Hsu", "Arian Maleki" ],
    "emails" : [ "jixu@cs.columbia.edu,", "djhsu@cs.columbia.edu,", "arian@stat.columbia.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Introduction\nSince Fisher’s 1922 paper (Fisher, 1922), maximum likelihood estimators (MLE) have become one of the most popular tools in many areas of science and engineering. The asymptotic consistency and optimality of MLEs have provided users with the confidence that, at least in some sense, there is no better way to estimate parameters for many standard statistical models. Despite its appealing properties, computing the MLE is often intractable. Indeed, this is the case for many latent variable models {f(Y, z;η)}, where the latent variables z are not observed. For each setting of the parameters η, the marginal distribution of the observed data Y is (for discrete z)\nf(Y;η) = ∑ z f(Y, z;η) .\nIt is this marginalization over latent variables that typically causes the computational difficulty. Furthermore, many algorithms based on the MLE principle are only known to find stationary points of the likelihood objective (e.g., local maxima), and these points are not necessarily the MLE.\nE-mail: jixu@cs.columbia.edu, djhsu@cs.columbia.edu, arian@stat.columbia.edu\nar X\niv :1\n60 8.\n07 63\n0v 1\n[ m\nat h.\nST ]\n2 6\n1.1 Expectation Maximization\nAmong the algorithms mentioned above, Expectation Maximization (EM) has attracted more attention for the simplicity of its iterations, and its good performance in practice (Dempster et al., 1977; Redner and Walker, 1984). EM is an iterative algorithm for climbing the likelihood objective starting from an initial setting of the parameters η̂〈0〉. In iteration t, EM performs the following steps:\nE-step: Q̂(η | η̂〈t〉) , ∑ z f(z | Y; η̂〈t〉) log f(Y, z;η) , (1)\nM-step: η̂〈t+1〉 , arg max η Q̂(η | η̂〈t〉) , (2)\nIn many applications, each step is intuitive and can be performed very efficiently. Despite the popularity of EM, as well as the numerous theoretical studies of its behavior, many important questions about its performance—such as its convergence rate and accuracy—have remained unanswered. The goal of this paper is to address these questions for specific models (described in Section 1.2) in which the observation Y is an i.i.d. sample from a mixture of two Gaussians.\nTowards this goal, we study an idealized execution of EM in the large sample limit, where the E-step is modified to be computed over an infinitely large i.i.d. sample from a Gaussian mixture distribution in the model. In effect, in the formula for Q̂(η | η̂〈t〉), we replace the observed data Y with a random variable Y ∼ f(y;η?) for some Gaussian mixture parameters η? and then take its expectation. The resulting E- and M-steps in iteration t are\nE-step: Q(η | η〈t〉) , EY [∑ z f(z | Y ;η〈t〉) log f(Y , z;η) ] , (3)\nM-step: η〈t+1〉 , arg max η Q(η | η〈t〉) . (4)\nThis sequence of parameters (η〈t〉)t≥0 is fully determined by the initial setting η〈0〉. We refer to this idealization as Population EM. Not only does Population EM shed light on the dynamics of EM in the large sample limit, but it can also reveal some of the fundamental limitations of EM. Indeed, if Population EM cannot provide an accurate estimate for the parameters η?, then intuitively, one would not expect the EM algorithm with a finite sample size to do so either. (To avoid confusion, we refer the original EM algorithm run with a finite sample as Sample-based EM.)\n1.2 Models and Main Contributions\nIn this paper, we study EM in the context of two simple yet popular and well-studied Gaussian mixture models. The two models, along with the corresponding Sample-based EM and Population EM updates, are as follows:\nModel 1. The observation Y is an i.i.d. sample from the mixture distribution 0.5N(−θ?,Σ) + 0.5N(θ?,Σ); Σ is a known covariance matrix in Rd, and θ? is the unknown parameter of interest.\n1. Sample-based EM iteratively updates its estimate of θ? according to the following equation:\nθ̂ 〈t+1〉 = 1\nn n∑ i=1 ( 2wd ( yi, θ̂ 〈t〉)− 1)yi, (5)\nwhere y1, . . . ,yn are the independent draws that comprise Y,\nwd(y,θ) , φd(y − θ)\nφd(y − θ) + φd(y + θ) ,\nand φd is the density of a Gaussian random vector with mean 0 and covariance Σ.\n2. Population EM iteratively updates its estimate according to the following equation:\nθ〈t+1〉 = E(2wd(Y ,θ〈t〉)− 1)Y , (6)\nwhere Y ∼ 0.5N(−θ?,Σ) + 0.5N(θ?,Σ).\nModel 2. The observation Y is an i.i.d. sample from the mixture distribution 0.5N(µ?1,Σ) + 0.5N(µ?2,Σ). Again, Σ is known, and (µ?1,µ?2) are the unknown parameters of interest.\n1. Sample-based EM iteratively updates its estimate of µ?1 and µ?2 at every iteration according to the following equations:\nµ̂ 〈t+1〉 1 =\n∑n i=1 vd(yi, µ̂ 〈t〉 1 , µ̂\n〈t〉 2 )yi∑n\ni=1 vd(yi, µ̂ 〈t〉 1 , µ̂ 〈t〉 2 )\n, (7)\nµ̂ 〈t+1〉 2 =\n∑n i=1(1− vd(yi, µ̂ 〈t〉 1 , µ̂\n〈t〉 2 ))yi∑n\ni=1(1− vd(yi, µ̂ 〈t〉 1 , µ̂ 〈t〉 2 ))\n, (8)\nwhere y1, . . . ,yn are the independent draws that comprise Y, and\nvd(y,µ1,µ2) , φd(y − µ1)\nφd(y − µ1) + φd(y − µ2) .\n2. Population EM iteratively updates its estimates according to the following equations:\nµ 〈t+1〉 1 =\nEvd(Y ,µ 〈t〉 1 ,µ 〈t〉 2 )Y\nEvd(Y ,µ 〈t〉 1 ,µ 〈t〉 2 )\n, (9)\nµ 〈t+1〉 2 =\nE(1− vd(Y ,µ 〈t〉 1 ,µ 〈t〉 2 ))Y\nE(1− vd(Y ,µ 〈t〉 1 ,µ 〈t〉 2 ))\n, (10)\nwhere Y ∼ 0.5N(µ?1,Σ) + 0.5N(µ?2,Σ).\nOur main contribution in this paper is a new characterization of the stationary points and dynamics of EM in both of the above models.\n1. We prove convergence for the sequence of iterates for Population EM from each model: the sequence (θ〈t〉)t≥0 converges to either θ?, −θ?, or 0; the sequence ((µ〈t〉1 ,µ 〈t〉 2 ))t≥0 converges to\neither (µ?1,µ?2), (µ?2,µ?1), or ((µ?1 + µ?2)/2, (µ?1 + µ?2)/2). We also fully characterize the initial parameter settings that lead to each limit point.\n2. Using this convergence result for Population EM, we also prove that the limits of the Samplebased EM iterates converge in probability to the unknown parameters of interest, as long as Sample-based EM is initialized at points where Population EM would converge to these parameters as well.\nFormal statements of our results are given in Section 2.\n1.3 Background and Related Work\nThe EM algorithm was formally introduced by Dempster et al. (1977) as a general iterative method for computing parameter estimates from incomplete data. Although EM is billed as a procedure for maximum likelihood estimation, it is known that with certain initializations, the final parameters returned by EM may be far from the MLE, both in parameter distance and in log-likelihood value (Wu, 1983). Several works characterize local convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Chrétien and Hero, 2008). However, these analyses do not distinguish between global maximizers and other stationary points (except, e.g., when the likelihood function is unimodal). Thus, as an optimization algorithm for maximizing the log-likelihood objective, the “worst-case” performance of EM is somewhat discouraging.\nFor a more optimistic perspective on EM, one may consider a “best-case” analysis, where (i) the data are an iid sample from a distribution in the given model, (ii) the sample size is sufficiently large, and (iii) the starting point for EM is sufficiently close to the parameters of the data generating distribution. Conditions (i) and (ii) are ubiquitous in (asymptotic) statistical analyses, and (iii) is a generous assumption that may be satisfied in certain cases. Redner and Walker (1984) show that in such a favorable scenario, EM converges to the MLE almost surely for a broad class of mixture models. Moreover, recent work of Balakrishnan et al. (2014) gives non-asymptotic convergence guarantees in certain models; importantly, these results permit one to quantify the accuracy of a pilot estimator required to effectively initialize EM. Thus, EM may be used in a tractable two-stage estimation procedures given a first-stage pilot estimator that can be efficiently computed.\nIndeed, for the special case of Gaussian mixtures, researchers in theoretical computer science and machine learning have developed efficient algorithms that deliver the highly accurate parameter estimates under appropriate conditions. Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either dα or kβ for some α, β > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894).\nMost relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for well-separated mixtures. In a related but different vein, Dasgupta and Schulman (2007) analyze a variant of EM with a particular initialization scheme, and proves fast convergence to the true parameters, again for well-separated mixtures in high-dimensions. For mixtures of two Gaussians, it is possible to exploit symmetries to get sharper analyses. Indeed, Chaudhuri et al. (2009b) uses these symmetries to prove that a variant of Lloyd’s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption. Lastly, for the specific case of our Model 1, Balakrishnan et al. (2014) proves linear convergence of EM (as well as a gradient-based variant of EM) when started in a sufficiently small neighborhood around the true parameters; here, the size of the neighborhood grows with the separation between the two\nmixture components (which must be sufficiently large). Their analysis also proceeds by studying Population EM, and then relating Sample-based EM to it. Remarkably, by focusing attention on the local region around the true parameters, they obtain non-asymptotic bounds on the parameter estimation error. Our work is complementary to their result in that we focus on asymptotic limits rather than finite sample analysis. This allows us to provide a global analysis of EM, without any separation assumption; such an analysis cannot be deduced from the results of Balakrishnan et al. by taking limits.\n2 Analysis of EM for Mixtures of Two Gaussians\nIn this section, we present our results for Population EM and Sample-based EM under both Model 1 and Model 2, and also discuss further implications about the expected log-likelihood function. Without loss of generality, we may assume that the known covariance matrix Σ is the identity matrix Id. Throughout, we denote the Euclidean norm by ‖ · ‖, and the signum function by sgn(·) (where sgn(0) = 0, sgn(z) = 1 if z > 0, and sgn(z) = −1 if z < 0).\n2.1 Main Results for Population EM\nWe present results for Population EM for both models, starting with Model 1.\nTheorem 1. Assume θ? ∈ Rd \\ {0}. Let (θ〈t〉)t≥0 denote the Population EM iterates for Model 1, and suppose 〈θ〈0〉,θ?〉 6= 0. There exists κθ ∈ (0, 1)—depending only on θ? and θ〈0〉—such that∥∥∥θ〈t+1〉 − sgn(〈θ〈0〉,θ?〉)θ?∥∥∥ ≤ κθ · ∥∥∥θ〈t〉 − sgn(〈θ〈0〉,θ?〉)θ?∥∥∥ .\nThe proof of Theorem 1, as well as all other omitted proofs, is given in Appendix A. Theorem 1 asserts that if θ〈0〉 is not on the hyperplane {x ∈ Rd : 〈x,θ?〉 = 0}, then the sequence (θ〈t〉)t≥0 converges to either θ? or −θ?.\nOur next result shows that if 〈θ〈0〉,θ?〉 = 0, then (θ〈t〉)t≥0 still converges, albeit to 0.\nTheorem 2. Let (θ〈t〉)t≥0 denote the Population EM iterates for Model 1. If 〈θ〈0〉,θ?〉 = 0, then\nθ〈t〉 → 0 as t→∞ .\nTheorems 1 and 2 together characterize the fixed points of Population EM for Model 1, and fully specify the conditions under which each fixed point is reached. The results are simply summarized in the following corollary.\nCorollary 1. If (θ〈t〉)t≥0 denote the Population EM iterates for Model 1, then\nθ〈t〉 → sgn(〈θ〈0〉,θ?〉)θ? as t→∞ .\nWe now discuss Population EM with Model 2. To state our results more concisely, we use the following re-parameterization of the model parameters and Population EM iterates:\na〈t〉 , µ 〈t〉 1 + µ 〈t〉 2\n2 − µ\n? 1 + µ ? 2\n2 , b〈t〉 ,\nµ 〈t〉 2 − µ 〈t〉 1\n2 , θ? , µ?2 − µ?1 2 . (11)\nIf the sequence of Population EM iterates ((µ〈t〉1 ,µ 〈t〉 2 ))t≥0 converges to (µ ? 1,µ ? 2), then we expect b〈t〉 → θ?. Hence, we also define β〈t〉 as the angle between b〈t〉 and θ?, i.e.,\nβ〈t〉 , arccos ( 〈b〈t〉,θ?〉 ‖b〈t〉‖‖θ?‖ ) ∈ [0, π] .\n(This is well-defined as long as b〈t〉 6= 0 and θ? 6= 0.) We first present results on Population EM with Model 2 under the initial condition 〈b〈0〉,θ?〉 6= 0.\nTheorem 3. Assume θ? ∈ Rd \\ {0}. Let (a〈t〉, b〈t〉)t≥0 denote the (re-parameterized) Population EM iterates for Model 2, and suppose 〈b〈0〉,θ?〉 6= 0. Then b〈t〉 6= 0 for all t ≥ 0. Furthermore, there exist κa ∈ (0, 1)—depending only on ‖θ?‖ and |〈b〈0〉,θ?〉/‖b〈0〉‖|—and κβ ∈ (0, 1)—depending only on ‖θ?‖, 〈b〈0〉,θ?〉/‖b〈0〉‖, ‖a〈0〉‖, and ‖b〈0〉‖—such that\n‖a〈t+1〉‖2 ≤ κ2a · ‖a〈t〉‖2 + ‖θ?‖2 sin2(β〈t〉)\n4 ,\nsin(β〈t+1〉) ≤ κtβ · sin(β〈0〉) .\nBy combining the two inequalities from Theorem 3, we conclude\n‖a〈t+1〉‖2 = κ2ta ‖a〈0〉‖2 + ‖θ?‖2\n4 t∑ τ=0 κ2τa · sin2(β〈t−τ〉)\n≤ κ2ta ‖a〈0〉‖2 + ‖θ?‖2\n4 t∑ τ=0 κ2τa κ 2(t−τ) β · sin 2(β〈0〉)\n≤ κ2ta ‖a〈0〉‖2 + ‖θ?‖2 4 t ( max { κa, κβ })t sin2(β〈0〉) .\nTheorem 3 shows that the re-parameterized Population EM iterates converge, at a linear rate, to the average of the two means (µ?1 +µ?2)/2, as well as the line spanned by θ\n?. The theorem, however, does not provide any information on the convergence of the magnitude of b〈t〉 to the magnitude of θ?. This is given in the next theorem.\nTheorem 4. Assume θ? ∈ Rd \\{0}. Let (a〈t〉, b〈t〉)t≥0 denote the (re-parameterized) Population EM iterates for Model 2, and suppose 〈b〈0〉,θ?〉 6= 0. Then there exist T0 > 0, κb ∈ (0, 1), and cb > 0—all depending only on ‖θ?‖, |〈b〈0〉,θ?〉/‖b〈0〉‖|, ‖a〈0〉‖, and ‖b〈0〉‖—such that∥∥∥b〈t+1〉 − sgn(〈b〈0〉,θ?〉)θ?∥∥∥2 ≤ κ2b · ∥∥∥b〈t〉 − sgn(〈b〈0〉,θ?〉)θ?∥∥∥2 + cb · ‖a〈t〉‖ ∀t > T0 .\nIf 〈b〈0〉,θ?〉 = 0, then we show convergence of the (re-parameterized) Population EM iterates to the degenerate solution (0,0).\nTheorem 5. Let (a〈t〉, b〈t〉)t≥0 denote the (re-parameterized) Population EM iterates for Model 2. If 〈b〈0〉,θ?〉 = 0, then\n(a〈t〉, b〈t〉) → (0,0) as t→∞ .\nTheorems 3, 4, and 5 together characterize the fixed points of Population EM for Model 2, and fully specify the conditions under which each fixed point is reached. The results are simply summarized in the following corollary.\nCorollary 2. If (a〈t〉, b〈t〉)t≥0 denote the (re-parameterized) Population EM iterates for Model 2, then\na〈t〉 → µ ? 1 + µ ? 2\n2 as t→∞ ,\nb〈t〉 → sgn(〈b〈0〉,µ?2 − µ?1〉) µ?2 − µ?1\n2 as t→∞ .\n2.2 Main Results for Sample-based EM\nUsing the results on Population EM presented in the above section, we can now establish consistency of (Sample-based) EM. We focus attention on Model 2, as the same results for Model 1 easily follow as a corollary. First, we state a simple connection between the Population EM and Sample-based EM iterates.\nTheorem 6. Suppose Population EM and Sample-based EM for Model 2 have the same initial parameters: µ̂〈0〉1 = µ 〈0〉 1 and µ̂ 〈0〉 2 = µ 〈0〉 2 . Then for each iteration t ≥ 0,\nµ̂ 〈t〉 1 → µ 〈t〉 1 and µ̂ 〈t〉 2 → µ 〈t〉 2 as n→∞ ,\nwhere convergence is in probability.\nNote that Theorem 6 does not necessarily imply that the fixed point of Sample-based EM (when initialized at (µ̂〈0〉1 , µ̂ 〈0〉 2 ) = (µ 〈0〉 1 ,µ 〈0〉 2 )) is the same as that of Population EM. It is conceivable that as t→∞, the discrepancy between (the iterates of) Sample-based EM and Population EM increases. We show that this is not the case: the fixed points of Sample-based EM indeed converge to the fixed points of Population EM.\nTheorem 7. Suppose Population EM and Sample-based EM for Model 2 have the same initial parameters: µ̂〈0〉1 = µ 〈0〉 1 and µ̂ 〈0〉 2 = µ 〈0〉 2 . If 〈µ 〈0〉 2 − µ 〈0〉 1 ,θ ?〉 6= 0, then\nlim sup t→∞\n|µ̂〈t〉1 − µ 〈t〉 1 | → 0 and lim sup t→∞ |µ̂〈t〉2 − µ 〈t〉 2 | → 0 as n→∞ ,\nwhere convergence is in probability.\n2.3 Population EM and Expected Log-likelihood\nDo the results we derived in the last section regarding the performance of EM provide any information on the performance of other ascent algorithms, such as gradient ascent, that aim to maximize the loglikelihood function? To address this question, we show how our analysis can determine the stationary points of the expected log-likelihood and characterize the shape of the expected log-likelihood in a neighborhood of the stationary points. Let G(η) denote the expected log-likelihood, i.e.,\nG(η) , E(log fη(Y )) = ∫ f(y;η∗) log f(y;η) dy,\nwhere η∗ denotes the true parameter value. Also consider the following standard regularity conditions:\nR1 The family of probability density functions f(y;η) have common support. R2 ∇η ∫ f(y;η∗) log f(y;η) dy = ∫ f(y;η∗)∇η log f(y;η) dy, where ∇η denotes the gradient with\nrespect to η. R3 ∇η(E ∑ z f(z | Y ;η〈t〉)) log f(Y , z; η) = E ∑ z f(z | Y ;η〈t〉)∇η log f(Y , z;η).\nThese conditions can be easily confirmed for many models including the Gaussian mixture models. The following theorem connects the fixed points of the Population EM and the stationary points of the expected log-likelihood.\nLemma 1. Let η̄ ∈ Rd denote a stationary point of G(η). Also assume that Q(η | η〈t〉) has a unique and finite stationary point in terms of η for every η〈t〉, and this stationary point is its global maxima. Then, if the model satisfies conditions R1–R3, and the Population EM algorithm is initialized at η̄, it will stay at η̄. Conversely, any fixed point of Population EM is a stationary point of G(η).\nProof. Let η̄ denote a stationary point of G(η). We first prove that η̄ is a stationary point of Q(η | η̄).\n∇ηQ(η | η̄) ∣∣ η=η̄ = ∫ ∑ z f(z | y; η̄) ∇ηf(y, z;η) ∣∣ η=η̄ f(y, z; η̄) f(y;η∗) dy\n= ∫ ∑ z ∇ηf(y, z;η) ∣∣ η=η̄ f(y; η̄) f(y;η∗) dy\n= ∫ ∇ηf(y,η)∣∣η=η̄ f(y; η̄) f(y;η∗) dy = 0 ,\nwhere the last equality is using the fact that η̄ is a stationary point of G(η). Since Q(η | η̄) has a unique stationary point, and we have assumed that the unique stationary point is its global maxima, then Population EM will stay at that point. The proof of the other direction is similar.\nRemark 1. The fact that η∗ is the global maximizer of G(η) is well-known in the statistics and machine learning literature (e.g., Conniffe, 1987). Furthermore, the fact that η∗ is a global maximizer of Q(η | η∗) is known as the self-consistency property (Balakrishnan et al., 2014).\nIt is straightforward to confirm the conditions of Lemma 1 for mixtures of Gaussians. This lemma confirms that Population EM may be trapped in every local maxima. However, less intuitively it may get stuck at local minima or saddle points as well. Our next result characterizes the stationary points of G(θ) for Model 1.\nCorollary 3. G(θ) has only three stationary points. If d = 1 (so θ = θ ∈ R), then 0 is a local minima of G(θ), while θ∗ and −θ∗ are global maxima. If d > 1, then 0 is a saddle point, and θ? and −θ? are global maxima.\nThe proof is a straightforward result of Lemma 1 and Corollary 1. The phenomenon that Population EM may stuck in local minima or saddle points also happens in Model 2. We can employ Corollary 2 and Lemma 1 to explain the shape of the expected log-likelihood function G. To simplify the notation, we consider the re-parametrization a , µ1+µ22 and b , µ2−µ1 2 .\nCorollary 4. G(a, b) has three stationary points:( µ?1 + µ ? 2\n2 , µ?2 − µ?1 2\n) ,\n( µ?1 + µ ? 2\n2 , µ?1 − µ?2 2\n) , and\n( µ?1 + µ ? 2\n2 , µ?1 + µ ? 2 2\n) .\nThe first two points are global maxima. The third point is a saddle point.\n3 Concluding Remarks\nOur analysis of Population EM and Sample-based EM shows that the EM algorithm can, at least for the Gaussian mixture models studied in this work, compute statistically consistent parameter estimates. Previous analyses of EM only established such results for specific methods of initializing\nEM (e.g., Dasgupta and Schulman, 2007; Balakrishnan et al., 2014); our results show that they are not really necessary in the large sample limit. However, in any real scenario, the large sample limit may not accurately characterize the behavior of EM. Therefore, these specific methods for initialization, as well as non-asymptotic analysis, are clearly still needed to understand and effectively apply EM.\nThere are several interesting directions concerning EM that we hope to pursue in follow-up work. The first considers the behavior of EM when the dimension d = dn may grow with the sample size n. Our proof of Theorem 7 reveals that the parameter error of the t-th iterate (in Euclidean norm) is of the order √ d/n as t → ∞. Therefore, we conjecture that the theorem still holds as long as dn = o(n). This would be consistent with results from statistical physics on the MLE for Gaussian mixtures, which characterize the behavior when dn ∝ n as n→∞ (Barkai and Sompolinsky, 1994).\nAnother natural direction is to extend these results to more general Gaussian mixture models (e.g., with unequal mixing weights or unequal covariances) and other latent variable models.\nAcknowledgements. The second named author thanks Yash Deshpande and Sham Kakade for many helpful initial discussions. JX and AM were partially supported by NSF grant CCF-1420328. DH was partially supported by NSF grant DMREF-1534910 and a Sloan Fellowship.\nReferences\nD. Achlioptas and F. McSherry. On spectral learning of mixtures of distributions. In Eighteenth Annual Conference on Learning Theory, pages 458–469, 2005.\nS. Arora and R. Kannan. Learning mixtures of separated nonspherical Gaussians. The Annals of Applied Probability, 15(1A):69–92, 2005.\nS. Balakrishnan, M. J. Wainwright, and B. Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. ArXiv e-prints, August 2014.\nN. Barkai and H. Sompolinsky. Statistical mechanics of the maximum-likelihood density estimation. Physical Review E, 50(3):1766–1769, Sep 1994.\nM. Belkin and K. Sinha. Polynomial learning of distribution families. In Fifty-First Annual IEEE Symposium on Foundations of Computer Science, pages 103–112, 2010.\nS. C. Brubaker and S. Vempala. Isotropic PCA and affine-invariant clustering. In Forty-Ninth Annual IEEE Symposium on Foundations of Computer Science, 2008.\nK. Chaudhuri and S. Rao. Learning mixtures of product distributions using correlations and independence. In Twenty-First Annual Conference on Learning Theory, pages 9–20, 2008.\nK. Chaudhuri, S. M. Kakade, K. Livescu, and K. Sridharan. Multi-view clustering via canonical correlation analysis. In ICML, 2009a.\nK. Chaudhuri, S. Dasgupta, and A. Vattani. Learning mixtures of gaussians using the k-means algorithm. CoRR, abs/0912.0086, 2009b.\nS. Chrétien and A. O. Hero. On EM algorithms and their proximal generalizations. ESAIM: Probability and Statistics, 12:308–326, May 2008.\nD. Conniffe. Expected maximum log likelihood estimation. Journal of the Royal Statistical Society. Series D, 36(4):317–329, 1987.\nS. Dasgupta. Learning mixutres of Gaussians. In Fortieth Annual IEEE Symposium on Foundations of Computer Science, pages 634–644, 1999.\nS. Dasgupta and L. Schulman. A probabilistic analysis of EM for mixtures of separated, spherical Gaussians. Journal of Machine Learning Research, 8(Feb):203–226, 2007.\nA. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum-likelihood from incomplete data via the EM algorithm. J. Royal Statist. Soc. Ser. B, 39:1–38, 1977.\nR. A. Fisher. On the mathematical foundations of theoretical statistics. Philosophical Transactions of the Royal Society, London, A., 222:309–368, 1922.\nM. Hardt and E. Price. Tight bounds for learning a mixture of two gaussians. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, pages 753–760, 2015.\nD. Hsu and S. M. Kakade. Learning mixtures of spherical Gaussians: moment methods and spectral decompositions. In Fourth Innovations in Theoretical Computer Science, 2013.\nA. T. Kalai, A. Moitra, and G. Valiant. Efficiently learning mixtures of two Gaussians. In Forty-second ACM Symposium on Theory of Computing, pages 553–562, 2010.\nR. Kannan, H. Salmasian, and S. Vempala. The spectral method for general mixture models. SIAM Journal on Computing, 38(3):1141–1156, 2008.\nV. Koltchinskii. Oracle inequalities in empirical risk minimization and sparse recovery problems. In École d′été de probabilités de Saint-Flour XXXVIII, 2011.\nS. P. Lloyd. Least squares quantization in PCM. IEEE Trans. Information Theory, 28(2):129–137, 1982.\nJ. B. MacQueen. Some methods for classification and analysis of multivariate observations. In Proceedings of the fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281–297. University of California Press, 1967.\nA. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In Fifty-First Annual IEEE Symposium on Foundations of Computer Science, pages 93–102, 2010.\nK. Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society, London, A., 185:71–110, 1894.\nR. A. Redner and H. F. Walker. Mixture densities, maximum likelihood and the EM algorithm. SIAM Review, 26(2):195–239, 1984.\nP. Tseng. An analysis of the EM algorithm and entropy-like proximal point methods. Mathematics of Operations Research, 29(1):27–44, Feb 2004.\nS. Vempala and G. Wang. A spectral algorithm for learning mixtures models. Journal of Computer and System Sciences, 68(4):841–860, 2004.\nC. F. J. Wu. On the convergence properties of the EM algorithm. The Annals of Statistics, 11(1): 95–103, Mar 1983.\nL. Xu and M. I. Jordan. On convergence properties of the EM algorithm for Gaussian mixtures. Neural Computation, 8:129–151, 1996.\nA Proofs of the Main Results\nA.1 Organization\nThis appendix is devoted to the proofs of our main results, and is organized as follows.\n• Section A.2 establishes a connection between Model 1 and Model 2 for Population EM. This connection enables us to use the analysis of Population EM for Model 2 for the analysis of Population EM for Model 1.\n• Section A.3 presents several structural properties of Population EM. These properties will be used in the proofs of our main results.\n• Section A.4 introduces several notations that will be used in the proofs of our main results.\n• Section A.5 presents the proof of Theorem 3.\n• Section A.6 presents the proof of Theorem 4.\n• Section A.7 presents the proof of Theorem 1.\n• Section A.8 presents the proof of Theorem 5, which also implies Theorem 2.\n• Section A.9 presents the proof of Theorem 6.\n• Section A.10 presents the proof of Theorem 7.\n• Appendix B includes a few auxiliary results that are used in the proofs of our main results.\nA.2 Connection Between Models 1 and 2\nIn this section, we draw a connection between Model 1 and Model 2. This will enable us to conclude most of the results for Model 1 from the results we prove for Model 2. First, consider the re-parametrization introduced in (11). The iterations of Population EM can be written in terms of these new parameters a〈t〉, b〈t〉 as\na〈t+1〉 = γ〈t+1〉(1− 2p〈t+1〉) 2p〈t+1〉(1− p〈t+1〉) , (12) b〈t+1〉 = γ〈t+1〉\n2p〈t+1〉(1− p〈t+1〉) , (13)\nwhere\nγ〈t+1〉 = Ewd(Y − a〈t〉, b〈t〉)Y\n= ∫ wd(y − a〈t〉, b〈t〉)yφ+d (y,θ ?) dy , (14)\np〈t+1〉 = Ewd(Y − a〈t〉, b〈t〉)\n= ∫ wd(y − a〈t〉, b〈t〉)φ+d (y,θ ?) dy . (15)\nAbove, we use\nφ+d (y,θ ?) ,\n1 2 (φd(y − θ?) + φd(y + θ?))\nas shorthand for the Gaussian mixture density 0.5N(−θ?, Id) + 0.5N(θ?, Id). The following lemma establishes a connection between the iterations of Population EM for Model 1 and for Model 2.\nLemma 2. If a〈0〉 = 0, then a〈t〉 = 0 for every t. Furthermore,\nb〈t+1〉 = 2Ewd(Y , b〈t〉)Y = E(2wd(Y , b〈t〉)− 1)Y .\nObserve that the expression for b〈t+1〉 in Lemma 2 is the same as the Population EM update under Model 1, given in (6).\nLemma 2 tells us that Model 1 is a special case of Model 2 if we know the mean (µ?1 + µ?2)/2 is known. In this case, b〈t〉 is regarded as an estimate of (µ?2 − µ?1)/2, in the same way that θ〈t〉 is an estimate of θ? in Model 1. (This explains our choice of the notation θ? , (µ?2 − µ?1)/2 in (11).)\nThe proof of Lemma 2 is a simple induction that exploits the fact that wd(Y , b〈t〉)+wd(−Y , b〈t〉) = 1: if a〈t〉 = 0, then\np〈t+1〉 = Ewd(Y , b〈t〉) = 1 2 (Ewd(Y , b〈t〉) + Ewd(−Y , b〈t〉)) = 1 2 .\nA.3 Some Structural Properties of Population EM\nAn important structural property of Population EM is that the updates are orthogonally invariant. This means that our analysis of Population EM can make use of any orthogonal basis as the coordinate system without affecting the conclusions. This is spelled out in the following lemma.\nLemma 3. Let A ∈ Rd×d denote an orthogonal matrix. Define, ã〈t〉 , Aa〈t〉, b̃〈t〉 , Ab〈t〉, θ̃? , Aθ?, and γ̃〈t〉 = Aγ〈t〉. Then\nγ̃〈t+1〉 = ∫ wd(y − ã〈t〉, b̃ 〈t〉 )yφ+d (y, θ̃ ?) dy,\np〈t+1〉 = ∫ wd(y − ã〈t〉, b̃ 〈t〉 )φ+d (y, θ̃ ?) dy.\nand\nã〈t+1〉 = γ̃〈t+1〉(1− 2p〈t+1〉) 2p〈t+1〉(1− p〈t+1〉) ,\nb̃ 〈t+1〉 = γ̃〈t+1〉\n2p〈t+1〉(1− p〈t+1〉) . (16)\nProof. The proof is a simple change of integration variables from y to ỹ = A−1/2y.\nUsing the Lemma 3, we can establish some simple invariances about Population EM, which we state in the following lemmas.\nLemma 4. For all t ≥ 0,\nsgn ( 〈a〈t〉, b〈t〉〉 ) = sgn ( 〈a〈t+1〉, b〈t+1〉〉 ) = sgn ( 1\n2 − p〈t+1〉\n) ,\nsgn ( 〈b〈t〉,θ?〉 ) = sgn ( 〈b〈t+1〉,θ?〉 ) .\nLemma 5. The following holds for any two settings of (a〈0〉(1), b 〈0〉 (1)) and (a 〈0〉 (2), b 〈0〉 (2)).\n1. If a〈0〉(1) = −a 〈0〉 (2) and b 〈0〉 (1) = b 〈0〉 (2), then\na 〈t〉 (1) = −a 〈t〉 (2) and b 〈t〉 (1) = b 〈t〉 (2) , ∀t ≥ 0 .\n2. If b〈0〉(1) = −b 〈0〉 (2) and a 〈0〉 (1) = a 〈0〉 (2), then\na 〈t〉 (1) = a 〈t〉 (2) and b 〈t〉 (1) = −b 〈t〉 (2) , ∀t ≥ 0 .\nThe proof of Lemma 4 is in Appendix B.1, and the proof of Lemma 5 is in Appendix B.2. These two lemmas imply that in our analysis of Population EM, we may assume without loss of generality that\n〈a〈0〉, b〈0〉〉 ≥ 0 and 〈b〈0〉,θ?〉 ≥ 0 .\nNext, we show that effectively all of the action of Population EM takes place in a two-dimensional subspace.\nLemma 6. Consider any matrix U ∈ Rd×k such that U>U = Ik and θ? and d are in the range of U . For any matrix V ∈ Rd×l such that V >U = 0, and any vector c ∈ Rd, we have\nV >E [ wd(Y − c,d)Y ] = 0\nwhere Y ∼ 0.5N(−θ?, Id) + 0.5N(θ?, Id).\nProof. It is easy to see that because θ? is in the range of U , we have that U>Y is independent of V >Y . Moreover, because d is in the range of U , we have UU>d = d. Thus, for any y ∈ Rd,\nwd(y − c,d) = wk(U>(y − c),U>d) .\nThis implies\nV >Ewd(Y − c,d)Y = Ewd(U>(Y − c),U>d)V >Y = E [ wd(U >(Y − c),U>d) ] · E [ V >Y ] = 0\nwhere the last equality follows because Y has mean zero.\nLemma 7. Let M0 denote the span of b〈0〉 6= 0 and θ? 6= 0 for the model Y ∼ 0.5N(−θ?, Id) + 0.5N(θ?, Id). Then b〈t〉 ∈M0 for all t ≥ 0.\nProof. This follows from Lemma 6 and induction, by letting the columns of U be an orthonormal basis for M0, and letting the columns of V to be a basis for the orthogonal complement of M0.\nRecall that in Section 2.1, we defined the angle between b〈t〉 and θ? as β〈t〉. For our analysis, it will turn out to be useful to consistently refer to the cosine and sine of this angle, and hence we should regard the angle as possibly being any value between 0 and 2π. To do this, we fix an orthogonal basis {e〈t〉1 , · · · , e 〈t〉 d } such that\n〈b〈t〉, e〈t〉1 〉 = ‖b 〈t〉‖ and θ? = 〈θ?, e〈t〉1 〉e 〈t〉 1 + 〈θ ?, e 〈t〉 2 〉e 〈t〉 2 , (17)\nand define β〈t〉 ∈ [0, 2π] be the angle such that\ncosβ〈t〉 = 〈b〈t〉,θ?〉 ‖b〈t〉‖2‖θ?‖2 , (18) sinβ〈t〉 = 〈θ?, e〈t〉2 〉 ‖θ?‖ . (19)\nUsing this definition of the angle β〈t〉, we can establish the following monotonicity property.\nLemma 8. If 0 ≤ β〈0〉 < π/2, then β〈0〉 ≥ β〈1〉 ≥ . . . β〈t〉 ≥ . . . ≥ 0.\nProof. We assume β〈0〉 > 0, the extension to β〈0〉 = 0 is straightforward. Define α〈t〉 as the angle between b〈t〉 and b〈t+1〉 such that\ncosα〈t〉 = 〈b〈t〉, b〈t+1〉〉 ‖b〈t〉‖2‖b〈t+1〉‖2 , (20) sinα〈t〉 = 〈b〈t+1〉, e〈t〉2 〉 ‖b〈t+1〉‖ . (21)\nThe strategy of the proof is to use induction to prove that the following three statements hold for ∀t ≥ 0:\n(i) β〈t〉 ∈ (0, π2 ).\n(ii) α〈t〉 ∈ (0, β〈t〉).\n(iii) β〈t+1〉 = β〈t〉 − α〈t〉 ∈ (0, β〈t〉).\nIt is clear that the claim of the lemma holds if (iii) holds for all t ≥ 0. The inductive argument uses the following chain of arguments for step t:\nClaim 1 If (i) holds for t, then (ii) holds for t.\nClaim 2 If (i) and (ii) hold for t, then (iii) holds for t.\nClaim 3 If (i), (ii), and (iii) hold for t, then (i) holds for t+ 1.\nSince (i) holds for t = 0 by assumption, it suffices to prove Claims 1–3. Claim 3 is trivially true, and Claim 2 follows from the fact that θ? and all b〈t〉 lie in a the same two-dimensional subspace. So we just have to prove Claim 1. For sake of clarity, we choose the orthogonal basis e〈t〉1 , e 〈t〉 2 , . . . , e 〈t〉 d satisfying (17) to simplify the calculation. Let U t be the orthogonal matrix whose rows are e〈t〉1 , e 〈t〉 2 , . . . , e 〈t〉 d , so\nU tb 〈t〉 = (‖b〈t〉‖, 0, 0, . . . , 0)> , U tθ? = (θ?〈t〉,1, θ ? 〈t〉,2, 0, . . . , 0) > .\nDefine\nb̃ 〈t+1〉 〈t〉 , U tb 〈t+1〉 , b̃ 〈t〉 , U tb 〈t〉 , θ̃t , U tθ ? .\nAlso, let b̃〈t+1〉〈t〉,i denote the i th element of b̃ 〈t+1〉 〈t〉 . We also define the same notations of γ̃ 〈t+1〉 〈t〉 , γ̃ 〈t〉, γ̃ 〈t+1〉 〈t〉,i for γ〈t〉 and ã〈t+1〉〈t〉 , ã 〈t〉, ã 〈t+1〉 〈t〉,i for a\n〈t〉. Using this coordinate system and Lemma 7, Claim 1 is equivalent to proving that if θ?〈t〉,2 > 0, then α\n〈t〉 > 0 and α〈t〉 < β〈t〉. Therefore, in the rest of the proof, we essentially do these two steps.\n1. α〈t〉 > 0: First note that b̃ 〈t+1〉\nand γ̃〈t+1〉 are in the same direction. Hence, to prove that α〈t〉 > 0 we should show that γ̃〈t+1〉〈t〉,2 > 0. We have\nγ̃ 〈t+1〉 〈t〉,2 = ∫ wd(y − ã〈t〉, b̃ 〈t〉 )y2φ + d (y, θ̃t) dy\n= ∫ w(y1 − ã〈t〉1 , ‖b 〈t〉‖)y2 1\n2 (φd(y − θ̃t) + φd(y + θ̃t)) dy\n= 1\n2\n∫ w(y1 − ã〈t〉1 , ‖b 〈t〉‖)φ(y1 − θ?〈t〉,1) dy1 ∫ y2φ(y2 − θ?〈t〉,2) dy2\n+ 1\n2\n∫ w(y1 − ã〈t〉1 , ‖b 〈t〉‖)φ(y1 + θ?〈t〉,1) dy1 ∫ y2φ(y2 + θ ? 〈t〉,2) dy2 (22)\n= θ?〈t〉,2 ∫ w(y1 − ã〈t〉1 , ‖b\n〈t〉‖)1 2 (φ(y1 − θ?〈t〉,1)− φ(y1 + θ ? 〈t〉,1)) dy1 (23)\n= θ?〈t〉,2 ∫ ∞ 0\ne2y1‖b̃ 〈t〉‖ − e−2y1‖b̃ 〈t〉‖\ne2y1‖b̃ 〈t〉‖ + e2ã 〈t〉 1 ‖b̃ 〈t〉‖ + e−2y1‖b̃ 〈t〉‖ + e−2ã 〈t〉 1 ‖b̃\n〈t〉‖ φ−(y1, θ ? 〈t〉,1) dy1\n= θ?〈t〉,2S(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1), (24)\nwhere φ−(y, θ) , 12(φ(y−θ)−φ(y+θ)) is shorthand for the difference of two Gaussian densities, and S : R3 → R is defined by\nS(xa, xb, xθ) , ∫ ∞\n0\ne2yxb − e−2yxb e2yxb + e−2xaxb + e−2yxb + e2xaxb · 1 2 √ 2π (e−(y−xθ) 2/2 − e−(y+xθ)2/2) dy\n= ∫ ∞ 0 w(y − xa, xb) 1 2 (φ(y − xθ)− φ(y + xθ)) dy .\nHence it is clear that θ?〈t〉,2 > 0 implies α 〈t〉 > 0 since S(xa, xb, xθ) > 0 for all xb > 0 and xθ > 0.\n2. α〈t〉 < β〈t〉: We just need to show α〈t〉 < π/2 and compare the co-tangent of α〈t〉 and β〈t〉.\nThis means that we have to show γ̃〈t+1〉〈t〉,1 > 0 and compare γ̃ 〈t+1〉 〈t〉,1\nγ̃ 〈t+1〉 〈t〉,2\nwith θ?〈t〉,1 θ?〈t〉,2 . We first calculate\nγ̃ 〈t+1〉 〈t〉,1 .\nγ̃ 〈t+1〉 〈t〉,1 = ∫ wd(y − ã〈t〉, b̃ 〈t〉 )y1φ + d (y, θ̃t) dy\n= ∫ w(y1 − ã〈t〉1 , ‖b 〈t〉‖)y1φ+d (y, θ̃t) dy (25)\n= ∫ w(y1 − ã〈t〉1 , ‖b 〈t〉‖)y1φ+(y1, θ?〈t〉,1) dy1 (26)\n= Γ(ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1)\n= ∫ ∞ 0\ne2y1‖b̃ 〈t〉‖ − e−2y1‖b̃ 〈t〉‖\ne2y1‖b̃ 〈t〉‖ + e2ã 〈t〉 1 ‖b̃ 〈t〉‖ + e−2y1‖b̃ 〈t〉‖ + e−2ã 〈t〉 1 ‖b̃\n〈t〉‖ y1φ\n+(y1, θ ? 〈t〉,1) dy1,\n(27)\nwhere Γ : R3 → R is defined as\nΓ(xa, xb, xθ) = ∫ w(y − xa, xb)y 1\n2 (φ(y − xθ) + φ(y + xθ)) dy .\nIt is clear that γ̃〈t+1〉〈t〉,1 > 0. For comparing the co-tangent of two angles, we need to further simplify γ̃〈t+1〉〈t〉,1 . We have,\nγ̃ 〈t+1〉 〈t〉,1 = Γ(ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1)\n= ∫ w(y1 − ã〈t〉1 , ‖b 〈t〉‖)y1 1\n2 (φ(y1 − θ?〈t〉,1) + φ(y1 + θ ? 〈t〉,1)) dy1 (28)\n= θ?〈t〉,1 ∫ w(y1 − ã〈t〉1 , ‖b\n〈t〉‖)1 2 (φ(y1 − θ?〈t〉,1)− φ(y1 + θ ? 〈t〉,1)) dy1\n+ ∫ w(y1 − ã〈t〉1 , ‖b\n〈t〉‖)1 2 {(y1 − θ?〈t〉,1)φ(y1 − θ ? 〈t〉,1) + (y1 + θ ? 〈t〉,1)φ(y1 + θ ? 〈t〉,1)} dy1\n= θ?〈t〉,1S(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1) + ∫ ∞ −∞ w(y1 + θ ? 〈t〉,1 − ã 〈t〉 1 , ‖b 〈t〉‖)y1 1 2 φ(y1) dy1\n+ ∫ ∞ −∞ w(y1 − θ?〈t〉,1 − ã 〈t〉 1 , ‖b 〈t〉‖)y1 1 2 φ(y1) dy1 .\n= θ?〈t〉,1S(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1) +R(‖b̃ 〈t〉‖, ã〈t〉1 − θ ? 〈t〉,1) +R(‖b 〈t〉‖, ã〈t〉1 + θ ? 〈t〉,1), (29)\nwhere R : R2 → R is defined as R(xb, x) , ∫ +∞\n0\ne2yxb − e−2yxb e2yxb + e2xxb + e−2yxb + e−2xxb 1 2 √ 2π ye−y 2/2 dy ,\n= ∫ ∞ −∞ w(y − x, xb)y 1 2 φ(y) dy .\nEmploying (29) and (24) we have\ncotα〈t〉 = b̃ 〈t+1〉 〈t〉,1\nb̃ 〈t+1〉 〈t〉,2\n= γ̃ 〈t+1〉 〈t〉,1\nγ̃ 〈t+1〉 〈t〉,2\n= θ?〈t〉,1 θ?〈t〉,2 + R(‖b̃〈t〉‖, ã〈t〉1 − θ?〈t〉,1) +R(‖b 〈t〉‖, ã〈t〉1 + θ?〈t〉,1)\nγ̃ 〈t+1〉 〈t〉,2\n> θ?〈t〉,1\nθ?〈t〉,2 = cotβ〈t〉 ,\nwhere the last inequality is due to the fact that R(xb, x) > 0 for all xb > 0.\nA.4 Notations for the Remaining Proofs\nIn this section we collect the main notations that will be used in the proofs of our results. For the basic notation, we let φd(x) and Φd(x) denote the pdf and CDF for d-dimension standard Gaussian\ndistribution respectively. We use φ(x) and Φ(x) as shorthand for one-dimension case. Let φ+d (x, xθ) denote the pdf for X ∼ 0.5N(−xθ, I) + 0.5N(xθ, I), i.e.,\nφ+d (x,xθ) = 1\n2 (φd(x− xθ) + φd(x+ xθ)).\nWe shorthand φ+1 (x, xθ) as φ +(x, xθ) if x ∈ R. In addition, we let φ−(x, xθ) denote the difference between these two pdf, i.e.,\nφ−(x,xθ) = 1\n2 (φ(x− xθ)− φd(x+ xθ)).\nNext, we introduce the notations for important functions. Similar to the proof of Lemma 8, in many other proofs we will use the rotation matrix U t that satisfies U tb〈t〉 = (‖b〈t〉‖2, 0, 0, . . . , 0)>, and U tθ? = (θ?〈t〉,1, θ ? 〈t〉,2, 0, . . . , 0) >. We also use the notations b̃ 〈t+1〉 〈t〉 , U tb 〈t+1〉, b̃ 〈t〉 , U tb 〈t〉 and θ̃? , U tθ ?. Also, let b̃〈t+1〉〈t〉,i denote the i th element of b̃ 〈t+1〉 〈t〉 . We also define the same notations of γ̃ 〈t+1〉 〈t〉 , γ̃ 〈t〉, γ̃ 〈t+1〉 〈t〉,i for γ 〈t〉 and ã〈t+1〉〈t〉 , ã 〈t〉, ã 〈t+1〉 〈t〉,i for a 〈t〉. By Lemma 4 and 5, we can assume that 〈b〈t〉,θ?〉 ≥ 0 and 〈b〈t〉,a〈t〉〉 ≥ 0 for all t ≥ 0 without loss of generality. Hence we have\nθ?〈t〉,1 ≥ 0, and ã 〈t〉 1 ≥ 0.\nSince for any i ≥ 3, the ith coordinate is zero in the span of b̃〈t〉 and θ̃?, we prove that b̃〈t+1〉〈t〉,i = 0 for i ≥ 3. Then according to (16) we have\nã〈t+1〉 = γ̃〈t+1〉(1− 2p〈t+1〉) 2p〈t+1〉(1− p〈t+1〉) ,\nb̃ 〈t+1〉 = γ̃〈t+1〉\n2p〈t+1〉(1− p〈t+1〉) . (30)\nThe first notation we would like to introduce P : R3 → R P (xa, xb, xθ) , ∫ eyxb−xaxb\neyxb−xaxb + e−yxb+xaxb 1 2 √ 2π (e−(y−xθ) 2/2 + e−(y+xθ) 2/2) dy\n= ∫ w(y − xa, xb)φ+(y, xθ) dy . (31)\nThe importance of this function is clarified in the following calculations:\np〈t+1〉 = Ew(Y − ã〈t〉, b̃〈t〉)\n= ∫ w(y1 − ã〈t〉1 , b̃ 〈t〉 1 )φ +(y, θ?〈t〉,1)\n=\n∫ ey1‖b̃ 〈t〉‖−ã〈t〉1 ‖b̃ 〈t〉‖\ney1‖b̃ 〈t〉‖−ã〈t〉1 ‖b̃ 〈t〉‖ + e−y1‖b̃ 〈t〉‖+ã〈t〉1 ‖b̃ 〈t〉‖\n1 2 √ 2π (e−\n(y1−θ ? 〈t〉,1) 2\n2 + e− (y1+θ\n? 〈t〉,1) 2 2 ) dy1\n= P (ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1), (32)\nOur second notation is the Γ : R3 → R, where Γ(xa, xb, xθ) , ∫ eyxb−xaxb\neyxb−xaxb + e−yxb+xaxb y\n1 2 √ 2π (e−(y−xθ) 2/2 + e−(y+xθ) 2/2) dy.\n= ∫ w(y − xa, xb)yφ+(y, xθ) dy. (33)\nNote that according to (27), we have\nγ̃ 〈t+1〉 〈t〉,1 = Γ(ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1).\nThe next function we introduce here is S : R3 → R S(xa, xb, xθ) , ∫ ∞\n0\ne2yxb − e−2yxb e2yxb + e−2xaxb + e−2yxb + e2xaxb 1 2 √ 2π (e−(y−xθ) 2/2 − e−(y+xθ)2/2) dy\n= ∫ w(y − xa, xb)φ−(y, xθ) dy.\nNote that according to (24),\nγ̃ 〈t+1〉 〈t〉,2 = θ ? 〈t〉,2S(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1). (34)\nAnother notation we introduce here is R : R2 → R defined as R(xb, x) , ∫ +∞\n0\ne2yxb − e−2yxb e2yxb + e2xxb + e−2yxb + e−2xxb 1 2 √ 2π ye−y 2/2 dy\n= 1\n2\n∫ w(y − x, xb)yφ(y) dy.\nAccording to (29),\nγ̃ 〈t+1〉 〈t〉,1 = Γ(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)\n= θ?〈t〉,1S(ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1) +R(‖b̃ 〈t〉‖, ã〈t〉1 − θ ? 〈t〉,1) +R(‖b̃ 〈t〉‖, ã〈t〉1 + θ ? 〈t〉,1). (35)\nThe final notation that we will be using in this paper is the function F : R2 → R:\nF (xb, xθ) = ∫ e(y+xθ)xb − e−(y+xθ)xb e(y+xθ)xb + e−(y+xθ)xb (y + xθ) 1√ 2π e−y 2/2 dy\n= ∫ (2w(y + xθ, xb)− 1)(y + xθ)φ(y) dy. (36)\nTo understand where this function may appear, note that\nΓ(0, xb, xθ) = ∫ w(y, xb)y 1\n2 φ+(y, xθ) dy\n= ∫ w(y, xb)y 1\n2 φ(y − xθ) dy +\n∫ w(y, xb)y 1\n2 φ(y + xθ) dy\n= ∫ w(y + xθ, xb)(y + xθ) 1\n2 φ(y) dy +\n∫ w(y − xθ, xb)(y − xθ) 1\n2 φ(y) dy\n= ∫ w(y + xθ, xb)(y + xθ) 1\n2 φ(y) dy −\n∫ w(−y − xθ, xb)(y + xθ) 1\n2 φ(y) dy\n= 1\n2 ∫ e(y+xθ)xb − e−(y+xθ)xb e(y+xθ)xb + e−(y+xθ)xb (y + xθ) 1√ 2π e−y 2/2 dy = 1 2 F (xb, xθ). (37)\nA.5 Proof of Theorem 3\nWithout loss of generality, we assume that 〈b〈0〉,θ?〉 > 0 and 〈a〈0〉, b〈0〉〉 ≥ 0. We employ the notations and equations reviewed in Appendix A.4. For notational simplicity we also define Rs , R(‖b̃ 〈t〉‖, ã〈t〉1 − θ?〈t〉,1) + R(‖b̃ 〈t〉‖, ã〈t〉1 + θ?〈t〉,1) and Ss , S(ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1). Since b 〈t+1〉 and γ〈t+1〉 are in the same direction, we have\ncosβ〈t+1〉 = 〈b̃〈t+1〉, θ̃?〉 ‖θ̃?‖‖b̃〈t+1〉‖ = 〈γ̃〈t+1〉, θ̃?〉 ‖θ̃?‖‖γ̃〈t+1〉‖\n= γ̃ 〈t+1〉 〈t〉,1 θ ? 〈t〉,1 + γ̃ 〈t+1〉 〈t〉,2 θ ? 〈t〉,2 ‖θ̃?‖ √\n(γ̃ 〈t+1〉 〈t〉,1 ) 2 + (γ̃ 〈t+1〉 〈t〉,2 ) 2\n(a) =\n(θ?〈t〉,1) 2Ss + θ ? 〈t〉,1Rs + (θ ? 〈t〉,2) 2Ss ‖θ̃?‖ √\n(θ?〈t〉,1) 2S2s +R 2 s + 2Rsθ ? 〈t〉,1Ss + (θ ? 〈t〉,2) 2S2s\n= ‖θ̃?‖2Ss + θ?〈t〉,1Rs ‖θ̃?‖ √ ‖θ̃?‖2S2s +R2s + 2Rsθ?〈t〉,1Ss ,\nwhere Equality (a) is the result of (35) and (34). Hence it is straightforward to check that\nsinβt+1 = θ?〈t〉,2Rs ‖θ̃?‖ √ (‖θ̃?‖2S2s +R2s + 2Rsθ?〈t〉,1Ss) ≤ θ?〈t〉,2 ‖θ̃?‖ Rs Rs + θ?〈t〉,1Ss .\nNote that since b̃〈t〉2 = 0, we have θ?〈t〉,2\n‖θ̃?‖ = sin(β〈t〉).\nHence, we have\nsinβ〈t+1〉 = Rs\nRs + θ?〈t〉,1Ss sinβ〈t〉. (38)\nOur goal is to prove that there exists 0 < κβ < 1, such that Rs/(Rs + θ?〈t〉,1Ss) ≤ κβ at every iteration. Toward this goal we will prove that θ?〈t〉,1Ss > 0. First note that since according to Lemma 8 the angle β〈t〉 is decreasing θ?〈t〉,1 is an increasing sequence. Hence, θ ? 〈t〉,1Ss ≥ θ ? 〈0〉,1Ss. Our goal is to show that Ss > 0. Note that Ss = S(ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1) is only zero if ‖b 〈t〉‖ = 0 and can only go to zero if ã〈t〉1 →∞ or ‖b 〈t〉‖ → ∞. Hence, if we find a lower bound for inft ‖b〈t〉‖ and prove that we have an upper bound for supt ‖a〈t〉‖ and supt ‖b〈t〉‖, then we obtain a non-zero lower bound for Ss. The following two lemmas prove our claims:\nLemma 9. For any initialization a〈0〉, b〈0〉 ∈ Rd, we have\n‖a〈t〉‖2 ≤ max{‖a〈0〉‖2, 2 π + ‖θ?‖2 2 , 16 9 + 73 36 ‖θ?‖2} , c2U,1,∀t ≥ 0, ‖b〈t〉‖2 ≤ max{‖b〈0〉‖2, ‖θ?‖2 + 1 4c2U,2(1− cU,2)2 (1 + ‖θ?‖2)} , c2U,3,∀t ≥ 0,\nwhere cU,2 = 14(1− Φ(cU,1 + ‖θ ?‖)). Hence, {‖a〈t〉‖, ‖b〈t〉‖}t belong to a compact set.\nWe postpone the proof of this lemma until Appendix A.5.1, but the fact that the estimates remain bounded should not be surprising for the reader. Lemma 10. Let b〈t〉 and a〈t〉 denote the estimates of Population EM. There exists a value cl > 0 depending on ‖θ?‖, 〈b〈0〉,θ?〉, ‖a〈0〉‖, and ‖b〈0〉‖ such that\n‖b〈t〉‖ ≥ min(‖b〈0〉‖, cl) , cL,1.\nWe postpone the proof of this claim to Appendix A.5.2. Note that according to Lemma 9 we know that supt ‖a〈t〉‖ ≤ cU,1 and supt ‖b〈t〉‖ ≤ cU,3. Hence, we define\nκβ = max cL,1≤‖b̃ 〈t〉‖≤cU,3,‖ã〈t〉‖≤cU,1\nRs\nθ?〈0〉,1S(ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1) +Rs) ∈ (0, 1),\nthen (38) implies sinβ〈t+1〉 ≤ κβ sinβ〈t〉. This proves the first claim in Theorem 3. Our next goal is to prove the second claim, i.e.,\n‖a〈t+1〉‖2 ≤ κ2a‖a〈t〉‖2 + ‖θ?‖2 sinβ〈t〉\n4 .\nAs before we write ‖a〈t+1〉‖2 = (ã〈t+1〉〈t〉,1 ) 2 + (ã 〈t+1〉 〈t〉,2 )\n2 and then bound each term separately. According to (30), we have\nã 〈t+1〉 〈t〉,1 =\nΓ(ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1)(1− 2P (ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1)) P (ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1)) (?) ≤ κaã〈t〉1 ≤ κa‖ã 〈t〉‖. (39)\nwhere Inequality (?) is due to the following lemma:\nLemma 11. For any θ ≥ 0, there exists a constant κa ∈ (0, 1) only depending on θ and continuous for θ > 0 such that\nΓ(xa, xb, θ)(1− 2P (xa, xb, θ)) 2P (xa, xb, θ)(1− P (xa, xb, θ)) ≤ κaxa, ∀xa ≥ 0, xb > 0.\nThe proof of this Lemma is presented in Appendix A.5.3. Our next step is to establish the convergence of ãt+1t,2 . From (30) and (34) we have:\nã 〈t+1〉 〈t〉,2 =\nγ̃ 〈t+1〉 〈t〉,2 (1− 2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1))\n2P (ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1))\n= θ?〈t〉,2 S(ã\n〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− 2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1))\n2P (ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1)) .\nAnd according to (32), we have\nP (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1) = ∫ w(y − ã〈t〉1 , ‖b 〈t〉‖)φ+(y, θ?〈t〉,1) dy\n= ∫ ∞ y=0 (w(y − ã〈t〉1 , ‖b 〈t〉‖) + w(−y − ã〈t〉1 , ‖b 〈t〉‖))φ+(y, θ?〈t〉,1) dy\n= ∫ ∞ y=0\ne2y‖b 〈t〉‖ + 2e−2ã 〈t〉 1 ‖b 〈t〉‖ + e−2y‖b 〈t〉‖\ne2y‖b 〈t〉‖ + e2ã 〈t〉 1 ‖b 〈t〉‖ + e−2y‖b 〈t〉‖ + e−2ã 〈t〉 1 ‖b\n〈t〉‖ φ+(y, θ?〈t〉,1) dy\n> S(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1) ≥ 0. (40)\nHence, we have\nã 〈t+1〉 〈t〉,2 ≤\nθ?〈t〉,2 2 = ‖θ?‖ sinβ〈t〉 2 . (41)\nCombining (39) and (41) establishes the second part of our main Theorem.\nA.5.1 Proof of Lemma 9\nIn this section we use the notations and equations that are summarize in Appendix A.4. Without loss of generality, we assume that 〈b〈0〉,θ?〉 > 0 and 〈a〈0〉, b〈0〉〉 ≥ 0 and it is straightforward to show that if ‖b〈0〉‖ = 0, then\na〈t〉 = b〈t〉 = 0, ∀t ≥ 1.\nHence, we assume that ‖b〈0〉‖ > 0. We again rotate the coordinates with the U t matrix we introduced in Appendix A.4. Under this coordinate systems we know that ã〈t+1〉〈t〉,i = b̃ 〈t+1〉 〈t〉,i = 0 for every i ≥ 3. Hence, we will prove the boundedness of ã〈t+1〉〈t〉,1 , ã 〈t+1〉 〈t〉,2 , b̃ 〈t+1〉 〈t〉,1 , and b̃ 〈t+1〉 〈t〉,2 . We start with bounding ã 〈t+1〉 〈t〉,2 and b̃ 〈t+1〉 〈t〉,2 . According to (30) we have\nã 〈t+1〉 〈t〉,2 =\nγ̃ 〈t+1〉 〈t〉,2 (1− 2p 〈t+1〉) 2p〈t+1〉(1− p〈t+1〉) ≤ γ̃ 〈t+1〉 〈t〉,2 2p〈t+1〉 (b) = θ?〈t〉,2S(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1) 2p〈t+1〉 ,\nb̃ 〈t+1〉 〈t〉,2 =\nγ̃ 〈t+1〉 〈t〉,2\n2p〈t+1〉(1− p〈t+1〉) (c)\n≤ γ̃ 〈t+1〉 〈t〉,2\np〈t+1〉 (d) =\nθ?〈t〉,2S(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1) p〈t+1〉 , (42)\nwhere Equalities (b) and (d) are due to (34). To obtain Inequality (c) we used the following chain of arguments: According to Lemma 4, p〈t〉 ≤ 0.5 for every t. Hence, 2(1− p〈t+1〉) ≥ 1.\nWith exactly same calculation showed in (40), we have\np〈t+1〉 = P (ã 〈t〉 i , ‖b 〈t〉‖, θ?〈t〉,1) > S(ã 〈t〉 i , ‖b 〈t〉‖, θ?〈t〉,1).\nTogether with (42), we obtain\n|ã〈t+1〉〈t〉,2 | ≤ |θ?〈t〉,2| 2 ≤ ‖θ ?‖ 2 . |b̃〈t+1〉〈t〉,2 | ≤ |θ?〈t〉,2|\n2(1− p〈t+1〉) ≤ ‖θ?‖. (43)\nHence the only remaining step is to bound ã〈t+1〉〈t〉,1 and b̃ 〈t+1〉 〈t〉,1 . To bound ã 〈t+1〉 〈t〉,1 we consider two separate cases.\n1. ã〈t〉1 ≥ θ?〈t〉,1 ≥ 0, t ≥ 0: First note that according to (30), we have\n0 ≤ ã〈t+1〉〈t〉,1 = γ̃ 〈t+1〉 〈t〉,1 (1− 2p 〈t+1〉)\n2p〈t+1〉(1− p〈t+1〉)\n= Γ(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− 2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1))\n2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1))\n≤ Γ(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)\n2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1) . (44)\nHence to bound ã〈t+1〉〈t〉,1 we require a bound for\nΓ(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)\n2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1) .\nOur next lemma provides such a bound.\nLemma 12. If xa ≥ xθ ≥ 0, xb ≥ 0, we have\nΓ(xa, xb, xθ)\n2P (xa, xb, xθ) ≤\nxa + √ 2 π\n2 .\nWe prove this lemma in the Appendix B.4. Combining (44) and Lemma 12 proves\n(ã 〈t+1〉 〈t〉,1 )\n2 ≤ ( ã 〈t〉 1 +\n√ 2 π\n2 )2 ≤ ‖a〈t〉‖2 + 2π 2 .\nTherefore, combined with (43), we have\n‖a〈t+1〉‖2 = ‖ã〈t+1〉‖ = (ã〈t+1〉〈t〉,1 ) 2 + (ã 〈t+1〉 〈t〉,2 ) 2\n≤ ‖a〈t〉‖2 + 2π 2 + ‖θ?‖2 4 ≤ max{(‖a〈t〉‖)2, 2 π + ‖θ?‖2 2 }. (45)\n2. ã〈t〉1 < θ ? 〈t〉,1: Again according to (30) we have\n0 ≤ ã〈t+1〉〈t〉,1 = γ̃ 〈t+1〉 〈t〉,1 (1− 2p 〈t+1〉)\n2p〈t+1〉(1− p〈t+1〉)\n= Γ(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− 2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1))\n2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)) .\nWe know from Lemma 4 that p〈t+1〉 ≤ 0.5. In the range 0 < p〈t+1〉 ≤ 0.5,\n(1− 2p〈t+1〉) 2p〈t+1〉(1− p〈t+1〉)\nis a positive decreasing function of p〈t+1〉. Hence, if we a lower bound for P may lead to an upper bound for ã〈t+1〉〈t〉,1 . The following lemma provides such an upper bound.\nLemma 13. If xa ≥ xθ ≥ 0, xb ≥ 0, we have\nP (xa, xb, xθ) ≥ 1\n2 (1− Φ(xa − xθ)) +\n1 2 (1− Φ(xa + xθ)).\nIf 0 ≤ xa < xθ, xb ≥ 0, we have P (xa, xb, xθ) ≥ 1\n4 .\nwhere Φ(x) is the CDF for a standard Gaussian distribution.\nThe proof of this lemma is presented in the Appendix B.3. By plugging P (ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1) = 0.25 in (46), we have\n0 ≤ ã〈t+1〉〈t〉,1 = γ̃ 〈t+1〉 〈t〉,1 (1− 2p 〈t+1〉) 2p〈t+1〉(1− p〈t+1〉) ≤ 4 3 Γ(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)\n≤ 4 3\n∫ |y|φ+(y, θ?〈t〉,1) dy ≤ 4\n3\n√∫ y2φ+(y, θ?〈t〉,1) dy\n= 4\n3\n√ 1 + (θ?〈t〉,1) 2 ≤ 4 3 √ 1 + ‖θ?‖2. (46)\nCombining this with (43), we obtain\n‖a〈t+1〉‖2 ≤ 16 9 (1 + ‖θ?‖2) + ‖θ ?‖2 4\n= 16\n9 +\n73 36 ‖θ?‖2 (47)\nTherefore combining (45) and (47), we have\n‖a〈t〉‖2 ≤ max{‖a〈0〉‖2, 2 π + ‖θ?‖2 2 , 16 9 + 73 36 ‖θ?‖2} = c2U,1 <∞, ∀t ≥ 0\nSo far we have bounded {‖a〈t〉‖}t≥0 by cU,1. Also, in (43) we obtained an upper bound for ã〈t+1〉〈t〉,1 . Our next step is to obtain an upper bound for b̃〈t+1〉〈t〉,1 . First note that\nb̃ 〈t+1〉 〈t〉,1 =\nγ̃ 〈t+1〉 〈t〉,1\n2p〈t+1〉(1− p〈t+1〉) .\nHence, we have to find an upper bound for Γ and a lower bound for P . Note that\n∂P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)\n∂ã 〈t〉 1\n= − ∫\n2‖b〈t〉‖ (ey‖b 〈t〉‖−ã〈t〉1 ‖b 〈t〉‖ + e−y‖b 〈t〉‖+ã〈t〉1 ‖b 〈t〉‖)2 φ+(y, θ?〈t〉,1) dy ≤ 0.\nTherefore p〈t+1〉 = P (ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1) is a decreasing function of ã 〈t〉 1 . Since ã 〈t〉 1 ≤ ‖a〈t〉‖ ≤ cU,1, with Lemma 13, we have ∀t ≥ 0\np〈t+1〉 = P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1) ≥ P (cU,1, ‖b 〈t〉‖, θ?〈t〉,1)\n≥ min{1 4 , 1 2 (1− Φ(cU,1 − θ?〈t〉,1)) + 1 2 (1− Φ(cU,1 + θ?〈t〉,1))} ≥ 1 4 (1− Φ(cU,1 + ‖θ?‖)) , cU,2 > 0.\nNote that in (46), we derived an upper bound for Γ(ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1). Therefore, we have\n0 ≤ b̃〈t+1〉〈t〉,1 = γ̃ 〈t+1〉 〈t〉,1\n2p〈t+1〉(1− p〈t+1〉)\n≤ 1 2cU,2(1− cU,2) Γ(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1) ≤ 1 2cU,2(1− cU,2) √ 1 + ‖θ?‖2\nThus with (43), we have\n‖b〈t〉‖2 ≤ max{‖b〈0〉‖2, ‖θ?‖2 + 1 4c2U,2(1− cU,2)2 (1 + ‖θ?‖2)} = c2U,3 < ∞,∀t ≥ 0.\nThis completes the proof of Lemma 9.\nA.5.2 Proof of Lemma 10\nWithout loss of generality we only consider the case 〈b〈t〉,θ?〉 > 0 and 〈a〈t〉, b〈t〉〉 ≥ 0. Before we start the proof we remind the reader a couple of facts that we have proved in Lemma 8 and Lemma 9.\n1. supt ‖a〈t〉‖ ≤ cU,1 and supt ‖b〈t〉‖ ≤ cU,3.\n2. b〈1〉, b〈2〉, . . . , b〈t〉, . . . and θ? are all on the same two-dimensional plane:\n3. The angle β〈t〉 , arccos ( 〈b〈t〉,θ?〉 ‖b〈t〉‖‖θ?‖ ) is non-increasing in terms of t.\n4. a〈t〉 is in the same direction as b〈t〉 for all t ≥ 1.\nWe use the notations we summarized in Appendix A.4. Similar to that section we consider the U t transformed vectors ã〈t〉, b̃ 〈t〉 . Note that we have\n‖b〈t+1〉‖ = ‖b̃〈t+1〉‖ ≥ b̃〈t+1〉〈t〉,1\n= Γ(ã 〈t〉 1 , ‖b̃ 〈t〉‖, θ?〈t〉,1)\n2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1))\n≥ 2Γ(ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1), (48)\nwhere Γ and P are defined in (33) and (31). Hence, the goal of the rest of the proof is to show that:\n2Γ(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1) ≥ min{‖b 〈t〉‖, cl}.\nThe main idea of this part is as follows. First note that\n∂Γ(xa, xb, xθ)\n∂xb\n∣∣∣∣ xb=0 = 1 + |xθ|2 2 .\nHence, intuitively speaking we can argue that there exists a neighborhood of xb = 0 on which the derivative is always larger than 0.5. Hence, when ‖b〈t〉‖ belongs to this neighborhood, ‖b〈t+1〉‖ is larger than ‖b〈t〉‖ and cannot go to zero. Next lemma justifies this claim.\nLemma 14. For θ?〈0〉,1 > 0 there exists a value δb only depending on cU,1, θ ? 〈0〉,1 and ‖θ ?‖ such that\ninf 0≤xa≤cU,1,0≤xb≤δb,θ?〈0〉,1≤xθ≤‖θ ?‖\n∂Γ(xa, xb, xθ)\n∂xb ≥ 1/2.\nWe present the proof of this result in the Appendix B.6. We remind the reader that according to Lemma 9, ‖a〈t〉‖ ≤ cU,1. Furthermore, since the angle β〈t〉 is a non increasing sequence we have θ?〈t〉,1 ≥ θ ? 〈0〉,1. Suppose that ‖b 〈t〉‖ ≤ δb. Then from (48) we know that ‖b〈t+1〉‖ ≥ 2Γ(ã 〈t〉 1 , ‖b\n〈t〉‖, θ?〈t〉,1). Also from the mean value theorem we have:\n|Γ(ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1)− Γ(ã 〈t〉 1 , 0, θ ? 〈t〉,1)| =\n∂Γ(ã 〈t〉 1 , xb, θ ? 〈t〉,1)\n∂xb |xb=ξ‖b 〈t〉‖ ≥ 1 2 ‖b〈t〉‖,\nwhere ξ ∈ [0, ‖b〈t〉‖] and to obtain the last inequality we used Lemma 14 and the fact that ‖b〈t〉‖ ≤ δb. So far we have proved that if ‖b〈t〉‖ ≤ δb, then ‖b〈t+1〉‖ ≥ ‖b〈t〉‖. But, we have not ruled out the possibility of the situation in which ‖b〈t〉‖ ≥ δb, but ‖b〈t+1〉‖ is close to zero. That requires a simple continuity argument. Note that since Γ is a continuous function of all its variables, its infimum over a compact set is achieved at certain point. Since, the value of Γ(x1, xb, xθ) is only zero when xb = 0, we conclude that the infimum is not zero. Hence, we conclude that\ncl , inf 0≤xa≤cU,1,δb≤xb≤cU,3,θ?〈0〉,1≤xθ≤‖θ‖ 2Γ(xa, xb, xθ) > 0.\nHence, we have if ‖b〈t〉‖ ≥ δb, then\n‖b〈t+1〉‖ ≥ 2Γ(ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1) ≥ cl.\nTherefore combining the result of ‖b〈t〉‖ ≤ δb and ‖b〈t〉‖ ≥ δb, we know Lemma 10 holds.\nA.5.3 Proof of Lemma 11\nWe consider three cases and deal with them separately: (i) 0 < xa < θ, (ii) xa ≥ θ, (iii) xa = 0.\n(i) 0 < xa < θ: Let x1 , θ + xa > θ − xa , x2 > 0. We first simplify the left hand side of the inequality. Our main goal in this section is to derive sharp upper bounds for Γ(xa, xb, θ) and 1− 2P (xa, xb, θ). We start with Γ(xa, xb, θ). Note that\nΓ(xa, xb, θ) = ∫ w(y − xa, xb)yφ+(y, θ) dy\n= xaP (xa, xb, θ) + ∫ (w(y − xa, xb)− 1\n2 +\n1 2 )(y − xa)φ+(y, θ) dy\n= xaP (xa, xb, θ)− 1\n2 xa +\n∫ (w(y − xa, xb)− 1\n2 )(y − xa)φ+(y, θ) dy\n= xaP (xa, xb, θ)− 1\n2 xa +\n1\n2\n∫ (w(y − xa, xb)− 1\n2 )(y − xa)(φ(y − θ) + φ(y + θ)) dy\n= xaP (xa, xb, θ)− 1\n2 xa +\n1 4 (F (xb, θ − xa) + F (xb, θ + xa))\n= xaP (xa, xb, θ)− 1\n2 xa +\n1 4 (F (xb, x1) + F (xb, x2)), (49)\nwhere F is defined in (36). Next, we find an upper bound for 12(F (xb, x1) + F (xb, x2)). Note\nthat ∀xb ≥ 0, xθ ≥ 0, we have\nF (xb, xθ) = ∫ eyxb − e−yxb eyxb + e−yxb y 1√ 2π e−(y−xθ) 2/2 dy\n≤ ∫ ∞\n0 y 1√ 2π e−(y−xθ) 2/2 dy + ∫ ∞ 0 y 1√ 2π e−(y+xθ) 2/2 dy\n= xθ ∫ xθ −xθ 1√ 2π e−y 2/2 dy + ∫ ∞ −xθ y 1√ 2π e−y 2/2 dy + ∫ ∞ xθ y 1√ 2π e−y 2/2 dy ≤ xθ(1− 2Φ(−xθ)) + 2φ(xθ) , l(xθ).\nTherefore, if we plug in xθ = x1 and xθ = x2, we have\n1 2 (F (xb, x1) + F (xb, x2)) ≤ 1 2 (l(x1) + l(x2)) ≤ l(x1), (50)\nwhere the last inequality holds since l(x) is an increasing function. This can be proved by taking the derivative of l(x):\ndl(xθ)\ndxθ = 1− 2Φ(−xθ) + 2xθφ(xθ)− 2xθφ(xθ) = 1− 2Φ(−xθ) ≥ 0.\nCombining (49) and (50) we obtain\nΓ(xa, xb, θ) ≤ xaP (xa, xb, θ)− 1\n2 xa + 2l(x1). (51)\nNow we obtain an upper bound for 1− 2P (xa, xb, θ). Note that, 1− 2P (xa, xb, θ) = ∫ ( 1\n2 − e\nyxb\neyxb + e−yxb ) 1√ 2π (e−(y+xa−θ) 2/2 + e−(y+xa+θ) 2/2) dy\n= ∫ ( 1\n2 − e\nyxb\neyxb + e−yxb ) 1√ 2π (e−(y−x2) 2/2 + e−(y+x1) 2/2) dy\n=\n∫ e−yxb − eyxb\n2(eyxb + e−yxb) 1√ 2π (e−(y−x2) 2/2 + e−(y+x1) 2/2) dy\n=\n∫ eyxb − e−yxb\n2(eyxb + e−yxb) 1√ 2π (e−(y−x1) 2/2 − e−(y−x2)2/2) dy\n= K(x1, xb)−K(x2, xb), (52)\nwhere K(x, b) , ∫\neyb−e−yb 2(eyb+e−yb) 1√ 2π e−(y−x) 2/2 dy. The following lemma proved in the Appendix B.7 summarizes some of the nice properties of this function, which will be used later in our proof.\nLemma 15. K(x, b) is a concave, strictly increasing function of x. Furthermore, K(0, xb) = 0.\nGiven (51) and (52) we can now prove the claimed upper bound in Lemma 11. We have\nΓ(xa, xb, θ)(1− 2P (xa, xb, θ)) 2P (xa, xb, θ)(1− P (xa, xb, θ))\n= {xaP (xa, xb, θ)− 12xa + 1 4(F (xb, x1) + F (xb, x2))}(1− 2P (xa, xb, θ))\n2P (xa, xb, θ)(1− P (xa, xb, θ))\n= xa + 1 2(F (xb, x1) + F (xb, x2))(1− 2P (xa, xb, θ))− xa\n4P (xa, xb, θ)(1− P (xa, xb, θ))\n= xa + 1 2(F (xb, x1) + F (xb, x2))(K(x1, xb)−K(x2, xb))− x1−x2 2\n4P (xa, xb, θ)(1− P (xa, xb, θ))\n≤ xa + l(x1)(K(x1, xb)−K(x2, xb))− 12(x1 − x2)\n4P (xa, xb, θ)(1− P (xa, xb, θ)) . (53)\nIt is straightforward to use the concavity of K(x, xb) in terms of x and prove that the function K(x1,xb)−K(x2,xb) x1−x2 is a decreasing function of x2. Hence, it is maximized at x2 = 0. Since K(0, xb) = 0, proved in Lemma 15, we have\nK(x1, xb)−K(x2, xb) ≤ K(x1, xb)\nx1 (x1 − x2). (54)\nCombining (53) and (54) implies:\nΓ(xa, xb, θ)(1− 2P (xa, xb, θ)) 2P (xa, xb, θ)(1− P (xa, xb, θ)) ≤ xa + (l(x1)\nK(x1,xb) x1 − 12)(x1 − x2) 4P (xa, xb, θ)(1− P (xa, xb, θ))\n= xa + (l(x1)\nK(x1,xb) x1 − 12)xa 2P (xa, xb, θ)(1− P (xa, xb, θ))\n= (1 + l(x1)\nK(x1,xb) x1 − 12 2P (xa, xb, θ)(1− P (xa, xb, θ)) )xa. (55)\nOur next step is to find an upper bound for (l(x1) K(x1,xb) x1 − 12). Note that\nK(x1, xb) =\n∫ eyxb − e−yxb\n2(eyxb + e−yxb) 1√ 2π e−(y−x1) 2/2 dy\n= ∫ ∞ 0 1 2 1√ 2π e−(y−x1) 2/2 dy − ∫ ∞ 0\ne−yxb\n(eyxb + e−yxb) 1√ 2π e−(y−x1) 2/2 dy\n− ∫ ∞\n0\n1\n2 1√ 2π e−(y+x1) 2/2 dy + ∫ ∞ 0\ne−yxb\n(eyxb + e−yxb) 1√ 2π e−(y+x1) 2/2 dy\n≤ ∫ x1\n0\n1√ 2π e−y 2/2 dy = 1 2 − Φ(−x1).\nFinally to obtain an upper bound for 1x l(x)( 1 2 − Φ(−x)) we use the following lemma:\nLemma 16. Define l(x) , x(1− 2Φ(−x)) + 2φ(x), then for all x > 0, we have\nl(x)(12 − Φ(−x)) x < 1 2 .\nThe proof of this lemma is presented in Appendix B.8. Using this lemma, we have\n2l(x1)( 1 2 − Φ(−x1)) x1 < 1,∀x1 = xa + θ ∈ [θ, 2θ].\nBy continuity of the function l(x)( 1 2 −Φ(−x)) x , we have\nκ̄a(θ) = sup x1∈[θ,2θ] 2l(x1)( 1 2 − Φ(−x1)) x1 < 1.\nIt is straightforward to prove that κ̄a(θ) is a continuous function of θ ∈ (0,∞). Since 4P (xa, xb, θ)(1− P (xa, xb, θ)) ≤ 1, we can bound (55) in the following way:\nΓ(xa, xb, θ)(1− 2P (xa, xb, θ)) 2P (xa, xb, θ)(1− P (xa, xb, θ)) ≤ (1 + l(x1)\n1 2 −Φ(−x1) x1\n− 12 2P (xa, xb, θ)(1− P (xa, xb, θ)) )xa\n≤ (1 + κ̄a(θ)− 1 4P (xa, xb, θ)(1− P (xa, xb, θ)) )xa ≤ κ̄a(θ)xa, ∀0 < xa < θ, xb > 0. (56)\nThis completes the proof of part (i).\n(ii) xa ≥ θ: According to Lemma 4, 1− 2P (xa, xb, θ) ≥ 0. Together with Lemma 12 we have\nΓ(xa, xb, θ)(1− 2P (xa, xb, θ)) 2P (xa, xb, θ)(1− P (xa, xb, θ)) ≤ (xa +\n√ 2 π )(1− 2P (xa, xb, θ))\n2(1− P (xa, xb, θ))\n= xa +\n√ 2 π (1− 2P (xa, xb, θ))− xa\n2(1− P (xa, xb, θ)) . (57)\nFrom Lemma 13, we have\n1− 2P (xa, xb, θ) ≤ Φ(xa + θ) + Φ(xa − θ)− 1. (58)\nNote that\n∂(Φ(xa + θ) + Φ(xa − θ)− 1) ∂xa\n= φ(xa + θ) + φ(xa − θ) ≤ √ 2\nπ , ∀xa,\nand Φ(0 + θ) + Φ(0− θ)− 1 = 0.\nTherefore, from (58) and mean value theorem, we have 1− 2P (xa, xb, θ) ≤ √ 2\nπ xa.\nTogether with (57) and P (xa, xb, θ)) ≤ 12 , we have\nΓ(xa, xb, θ)(1− 2P (xa, xb, θ)) 2P (xa, xb, θ)(1− P (xa, xb, θ)) ≤ xa +\n√ 2 π (1− 2P (xa, xb, θ))− xa\n2(1− P (xa, xb, θ))\n≤ xa − 1− 2π\n2(1− P (xa, xb, θ)) xa\n≤ (1 2 + 1 π )xa. (59)\n(iii) xa = 0: It is straightforward to prove that P (0, xb, θ) = 12 . Hence,\nΓ(xa, xb, θ)(1− 2P (xa, xb, θ)) 2P (xa, xb, θ)(1− P (xa, xb, θ)) = 0.\nCombining Case (i), (ii), (iii), we conclude that if we define\nκa(θ) =  max(κ̄a(θ), 1 2 + 1 π ), θ > 0 1\n2 +\n1 π , θ = 0\n,\nthen the statement of Lemma A.5.3 holds.\nA.6 Proof of Theorem 4\nIt is straightforward to use Theorem 3 and show that for every δa > 0, there exists a value of Tδa such that for every t > Tδa , ‖a〈t〉‖ ≤ δa. For the moment suppose that the following claim is true: there exists δa > 0, κb, cb only depending on ‖θ?‖, |θ?〈0〉,1| and the initialization {‖a\n〈0〉‖, ‖b〈0〉‖}, such that if ‖a〈t〉‖ ≤ δa for some t, then the next iteration b〈t+1〉 satisfies the following equation:\n‖b〈t+1〉 − θ?‖2 ≤ κ2b‖b〈t〉 − θ?‖2 + cb‖a〈t〉‖.\nIf we combine this claim with the result of Theorem 3, we obtain Theorem 4. Hence, the problem reduces to proving the above claim.\nNote that in Lemma 9, Lemma 10 and Lemma 8, we have\n‖a〈t〉‖ ∈ [0, cU,1], ‖b〈t〉‖ ∈ [cL,1, cU,3], θ?〈t〉,1 ∈ [θ ? 〈0〉,1, ‖θ ?‖], ∀t ≥ 0.\nTherefore, it is again straightforward to see that the following lemma implies our claim:\nLemma 17. For any a〈t〉, b〈t〉,θ? ∈ R2, if ‖a〈t〉‖ ∈ [0, Ua], ‖b〈t〉‖ ∈ [Lb, Ub], 〈θ ?,b〈t〉〉 ‖b〈t〉‖\n∈ [Lθ, ‖θ?‖],∀t ≥ 0, where Lb > 0, Lθ > 0 then there exists δa ∈ (0,min{Lθ, 1}];κb ∈ (0, 1); cb > 0 such that ∀‖a〈t〉‖ ∈ [0, δa], ‖b〈t〉‖ ∈ [Lb, Ub], 〈θ\n?,b〈t〉〉 ‖b〈t〉‖ ∈ [Lθ, ‖θ?‖], the next iteration b〈t+1〉 satisfying\n‖b〈t+1〉 − θ?‖2 ≤ κ2b‖b〈t〉 − θ?‖2 + cb‖a〈t〉‖,\nwhere δa, κb and cb are functions of only Ua, Lb, Ub, Lθ, ‖θ?‖.\nTo prove this lemma, we use the notations and definition that are summarized in Appendix A.4. In particular, we use the rotation matrix U t introduced in that section and rotate all the vectors a, b, θ, q with this matrix. Note that according to Lemma 7 we know that b〈t+1〉 lies in the span of θ? and b〈t〉 and hence, b̃ 〈〈t〉,i,t+1〉 = 0 for i ≥ 3. Therefore we only need to consider the first two coordinates. Our strategy of proving this lemma is to prove the following two claims for the first two coordinates:\n1. There exists κs ∈ (0, 1) such that |b̃〈t+1〉〈t〉,2 − θ ? 〈t〉,2| ≤ κs|θ ? 〈t〉,2|.\n2. There exists κ′b ∈ (0, 1) and δa > 0 such that if ‖a〈t〉‖ ≤ δa, then\n|b̃〈t+1〉〈t〉,1 − θ ? 〈t〉,1| ≤ κ ′ b|‖b〈t〉‖ − θ?〈t〉,1|+ (16‖θ‖+ 6)‖a 〈t〉‖.\nWe will then combine the above two claims to obtain Lemma 17.\n1. Proof of |b̃〈t+1〉〈t〉,2 − θ ? 〈t〉,2| ≤ κs|θ ? 〈t〉,2|: To prove our first claim, first note that according to (30)\nand (34) we have\nb̃ 〈t+1〉 〈t〉,2 =\nγ̃ 〈t+1〉 〈t〉,2\n2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1))\n= θ?〈t〉,2 S(ã\n〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)\n2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1))\nHence\n|b̃〈t+1〉〈t〉,2 − θ ? 〈t〉,2| = |θ ? 〈t〉,2|(1−\nS(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)\n2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)) )\n≤ |θ?〈t〉,2|(1− 2S(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1))\nBy definition of function S, it is straightforward to conclude that S(ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1) is only zero if ‖b〈t〉‖ = 0 and can only go to zero if ã〈t〉1 → ∞ or ‖b 〈t〉‖ → ∞. Therefore, combined with the continuity of S, we conclude that\nκs , sup ã 〈t〉 1 ∈[0,Ua],‖b 〈t〉‖∈[Lb,Ub],θ?〈t〉,1∈[Lθ,‖θ ?‖]\n1− 2S(ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1) < 1, (60)\nwhere κs only depends on Ua, Lb, Ub, Lθ and ‖θ?‖. Hence,\n|b̃〈t+1〉〈t〉,2 − θ ? 〈t〉,2| ≤ κs|θ ? 〈t〉,2|. (61)\n2. Proof of |b̃〈t+1〉〈t〉,1 − θ ? 〈t〉,1| ≤ κ ′ b|‖b 〈t〉‖ − θ?〈t〉,1|+ (16‖θ‖+ 6)‖a 〈t〉‖: Note that\n|b̃〈t+1〉〈t〉,1 − θ ? 〈t〉,1| = ∣∣∣∣∣∣ Γ(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)\n2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)) − θ?〈t〉,1 ∣∣∣∣∣∣ = ∣∣∣∣∣∣ 2Γ(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)− θ ? 〈t〉,1 + θ ? 〈t〉,1(2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)− 1) 2\n4P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)) ∣∣∣∣∣∣ ≤ ∣∣∣∣∣∣ 2Γ(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)− θ ? 〈t〉,1\n4P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)) ∣∣∣∣∣∣ + ∣∣∣∣∣∣ θ?〈t〉,1(2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)− 1) 2\n4P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1))\n∣∣∣∣∣∣ (62)\nFurthermore in (49), we proved that\nΓ(xa, xb, xθ) = xaP (xa, xb, θ)− 1\n2 xa +\n1 4 (F (xb, xa + xθ) + F (xb, xθ − xa)).\nTo see the definitions of P,Γ, and F you may refer to Appendix A.4. Hence, we have\n|2Γ(ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1)− θ ? 〈t〉,1|\n= |ã〈t〉1 (2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)− 1) + 1 2 (F (‖b〈t〉‖, θ?〈t〉,1 − ã 〈t〉 1 )− F (‖b 〈t〉‖, θ?〈t〉,1))\n+ 1\n2 (F (‖b〈t〉‖, θ?〈t〉,1 + ã 〈t〉 1 )− F (‖b 〈t〉‖, θ?〈t〉,1)) + (F (‖b 〈t〉‖, θ?〈t〉,1)− θ ? 〈t〉,1)|. (63)\nCombining (62) and (63), we conclude that in order to obtain an upper bound for |b̃〈t+1〉〈t〉,1 −θ ? 〈t〉,1| we have to find the following bounds:\n(a) Obtain an upper bound for |2P (ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1)− 1|. (b) Obtain an upper bound for |F (‖b〈t〉‖, xθ)− F (‖b〈t〉‖, θ?〈t〉,1)| for all θ ? 〈t〉,1 ∈ [Lθ, ‖θ\n?‖] and |xθ − θ?〈t〉,1| ≤ Lθ.\n(c) Obtain an upper bound for |F (‖b〈t〉‖, θ?〈t〉,1)− θ ? 〈t〉,1| for all θ ? 〈t〉,1 ∈ [Lθ, ‖θ ?‖] and ‖b〈t〉‖ ∈ [Lb, Ub] (d) Obtain a lower bound for 4P (ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)).\nWe summarize our strategy for bounding each of these terms below:\n(a) Upper bound for |2P (ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1)−1|: It is straightforward to confirm 2P (0, ‖b 〈t〉‖, θ?〈t〉,1) = 1 2 . Hence, we have to calculate |2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)− 2P (0, ‖b 〈t〉‖, θ?〈t〉,1)|. According to\nmean value theorem\n|P (ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1)− P (0, ‖b 〈t〉‖, θ?〈t〉,1)| = ∣∣∣∣∣∣ ∂P (xa, ‖b〈t〉‖, θ?〈t〉,1) ∂xa |xa=ξ ∣∣∣∣∣∣ (ã〈t〉1 ), (64) where ξ ∈ [0, ã〈t〉1 ]. Therefore we only need to bound | ∂P (xa,‖b〈t〉‖,θ?〈t〉,1) ∂xa\n|. Note that∣∣∣∣∣∣ ∂P (xa, ‖b〈t〉‖, θ?〈t〉,1)) ∂xa |xa=0 ∣∣∣∣∣∣ = ∫ 2‖b〈t〉‖\n(ey‖b 〈t〉‖−xa‖b〈t〉‖ + e−y‖b 〈t〉‖+xa‖b〈t〉‖)2 φ+(y, θ?〈t〉,1) dy|xa=0\n=\n∫ 2‖b〈t〉‖\n(ey‖b 〈t〉‖ + e−y‖b 〈t〉‖)2 φ+(y, θ?〈t〉,1) dy\n=\n∫ 2‖b〈t〉‖\n(ey‖b 〈t〉‖ + e−y‖b 〈t〉‖)2 φ(y − θ?〈t〉,1) dy. (65)\nNext we show that ∣∣∣∣∂P (xa,‖b〈t〉‖,θ?〈t〉,1))∂xa |xa=0 ∣∣∣∣ is a decreasing function of θ?〈t〉,1 and hence can be upper bounded by ∣∣∣∣∂P (xa,‖b〈t〉‖,0))∂xa |xa=0 ∣∣∣∣:\n∂ ∫ 2‖b〈t〉‖\n(ey‖b 〈t〉‖+e−y‖b 〈t〉‖)2 φ(y − θ?〈t〉,1) dy\n∂θ?〈t〉,1\n=\n∫ 2‖b〈t〉‖\n(ey‖b 〈t〉‖ + e−y‖b 〈t〉‖)2 (y − θ?〈t〉,1)φ(y − θ ? 〈t〉,1) dy\n= − ∫\n2‖b〈t〉‖ (ey‖b 〈t〉‖ + e−y‖b 〈t〉‖)2 dφ(y − θ?〈t〉,1)\n= − ∫\n4‖b〈t〉‖2(ey‖b〈t〉‖ − e−y‖b〈t〉‖) (ey‖b 〈t〉‖ + e−y‖b 〈t〉‖)3 φ(y − θ?〈t〉,1) dy ≤ 0\nHence, ∣∣∣∣∣∣ ∂P (xa, ‖b〈t〉‖, θ?〈t〉,1)) ∂xa |xa=0 ∣∣∣∣∣∣ ≤ ∫ 2‖b〈t〉‖ (ey‖b 〈t〉‖ + e−y‖b 〈t〉‖)2 φ(y) dy\n≤ ∫ ∞\n0 4‖b〈t〉‖e−2y‖b\n〈t〉‖φ(y) dy ≤ √ 2\nπ . (66)\nOur next goal is to show that there exists δ1 > 0 is a function of only Lb, Ub, Lθ, ‖θ?‖ such that\nsup ã 〈t〉 1 ∈[0,δ1],‖b 〈t〉‖∈[Lb,Ub],θ?〈t〉,1∈[Lθ,‖θ ?‖]\n∣∣∣∣∣∣ ∂P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)\n∂ã 〈t〉 1 ∣∣∣∣∣∣ ≤ 1. (67) This is a simple proof by contradiction. Since we have already done similar arguments in the proof of Lemma 14, for the sake of brevity we skip this argument. By combining (64) and (67) we conclude:\n|1− 2P (ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1)| ≤ 2ã 〈t〉 1 , ∀ã 〈t〉 1 ∈ [0, δ1], ‖b 〈t〉‖ ∈ [Lb, Ub], θ?〈t〉,1 ∈ [Lθ, ‖θ ?‖]. (68)\n(b) Upper bound for |F (‖b〈t〉‖, xθ)− F (‖b〈t〉‖, θ?〈t〉,1)|: Again by employing the mean value\ntheorem, we conclude that we have to bound ∂F (‖b 〈t〉‖,xθ) ∂xθ in a neighborhood of xθ = θ?〈t〉,1\nfor all ‖b〈t〉‖ ∈ [Lb, Ub], θ?〈t〉,1 ∈ [Lθ, ‖θ ?‖]. Note that, ∀xθ ≥ 0∣∣∣∣∣∂F (‖b〈t〉‖, xθ)∂xθ ∣∣∣∣∣ = ∣∣∣∣∫ (2w(y, ‖b〈t〉‖)− 1)y(y − xθ)φ(y − xθ) dy∣∣∣∣\n= ∣∣∣∣∫ (2w(y, ‖b〈t〉‖)− 1){(y − xθ)2 + xθ(y − xθ)}φ(y − xθ) dy∣∣∣∣ =\n∣∣∣xθ ∫ ey‖b〈t〉‖ − e−y‖b〈t〉‖ ey‖b 〈t〉‖ + e−y‖b 〈t〉‖ (y − xθ)φ(y − xθ) dy\n+\n∫ ey‖b 〈t〉‖ − e−y‖b〈t〉‖\ney‖b 〈t〉‖ + e−y‖b\n〈t〉‖ (y − xθ)2φ(y − xθ) dy ∣∣∣ (a) < ∣∣∣∣∣xθ ∫ 4‖b〈t〉‖ (ey‖b 〈t〉‖ + e−y‖b 〈t〉‖)2 φ(y − xθ) dy\n∣∣∣∣∣+ 1 (b) = 2xθ ∣∣∣∣∣∂(P (xa, ‖b〈t〉‖, xθ))∂xa |xa=0 ∣∣∣∣∣+ 1,\nwhere to obtain Inequality (a) we used integration by parts and also the fact that ey‖b 〈t〉‖−e−y‖b〈t〉‖ ey‖b 〈t〉‖+e−y‖b 〈t〉‖ < 1. To see why (b) holds, one may check (65). By employing (66), we then conclude that\n|∂F (‖b 〈t〉‖, xθ) ∂xθ | < xθ 4√ 2π + 1 ≤ 4‖θ?‖+ 1,∀xθ ∈ [0, 2‖θ?‖].\nTherefore, using mean value theorem, we have ∀|xθ − θ?〈t〉,1| ≤ Lθ, θ ? 〈t〉,1 ∈ [Lθ, ‖θ ?‖],\n|F (‖b〈t〉‖, xθ)− F (‖b〈t〉‖, θ?〈t〉,1)| ≤ (4‖θ ?‖+ 1)|xθ − θ?〈t〉,1|.\nHence, we have ∀ã〈t〉1 ∈ [0, Lθ], θ?〈t〉,1 ∈ [Lθ, ‖θ ?‖],\n|1 2 (F (‖b〈t〉‖, θ?〈t〉,1 − ã 〈t〉 1 )− F (‖b 〈t〉‖, θ?〈t〉,1)) + 1 2 (F (‖b〈t〉‖, θ?〈t〉,1 + ã 〈t〉 1 )− F (‖b 〈t〉‖, θ?〈t〉,1))| ≤ (4‖θ?‖+ 1)ã〈t〉1 . (69)\n(c) Upper bound for |F (‖b〈t〉‖, θ?〈t〉,1)− θ ? 〈t〉,1|:\nBecause the proof of this part has many algebraic steps we postpone it to the Appendix B.10.\nLemma 18. Given xb ∈ [Lb, Ub], xθ ∈ [Lθ, ‖θ?‖] where 0 < Lb ≤ Lθ ≤ ‖θ?‖ ≤ Ub < ∞, there exists κ′′b ∈ (0, 1) is a function of only Lb, Ub, Lθ, ‖θ ?‖ such that\n|F (xb, xθ)− xθ| ≤ κ′′b |xb − xθ|, ∀xb ∈ [Lb, Ub], xθ ∈ [Lθ, ‖θ?‖].\n(d) Lower bound for 4P (ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)): Note that\n1\n4P (0, ‖b〈t〉‖, θ?〈t〉,1)(1− P (0, ‖b 〈t〉‖, θ?〈t〉,1))\n− 1 = 0.\nLet p = min{ 1−κ′′b 2κ′′b\n, 1} (This choice will become clear later in the proof). Using contradiction arguments similar to the ones employed in the proof of Lemma 14, it is straight forward to see that there exists δ2 > 0 only depending on Lb, Ub, Lθ, ‖θ?‖ such that\nsup ã 〈t〉 1 ∈[0,δ2],‖b 〈t〉‖∈[Lb,Ub],θ?〈t〉,1∈[Lθ,‖θ ?‖]\n1\n4P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)) − 1 ≤ p.(70)\nNow combining (62), (63), (68), (69), (70) and Lemma 18 we conclude that for all ã〈t〉1 ∈ [0,min{δ1, Lθ, 1}], ‖b〈t〉‖ ∈ [Lb, Ub], θ?〈t〉,1 ∈ [Lθ, ‖θ ?‖],\n|2Γ(ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1)− θ ? 〈t〉,1|\n≤ |ã〈t〉1 (2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)− 1)|+ | 1 2 (F (‖b〈t〉‖, θ?〈t〉,1 − ã 〈t〉 1 )− F (‖b 〈t〉‖, θ?〈t〉,1))|\n+|1 2 (F (‖b〈t〉‖, θ?〈t〉,1 + ã 〈t〉 1 )− F (‖b 〈t〉‖, θ?〈t〉,1))|+ |F (‖b 〈t〉‖, θ?〈t〉,1)− θ ? 〈t〉,1|\n≤ 2(ã〈t〉1 ) 2 + (4‖θ?‖+ 1)ã〈t〉1 + κ ′′ b |‖b〈t〉‖ − θ?〈t〉,1| ≤ (4‖θ?‖+ 3)ã〈t〉1 + κ ′′ b |‖b〈t〉‖ − θ?〈t〉,1|.\nHence together with (68) again and (70) in (62), we have ∀ã〈t〉1 ∈ [0,min{δ1, Lθ, 1, δ2}], ‖b 〈t〉‖ ∈ [Lb, Ub], θ ? 〈t〉,1 ∈ [Lθ, ‖θ ?‖]\n|b̃〈t+1〉〈t〉,1 − θ ? 〈t〉,1| ≤ ∣∣∣∣∣∣ 2Γ(ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)− θ ? 〈t〉,1\n4P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)) ∣∣∣∣∣∣ + ∣∣∣∣∣∣ θ?〈t〉,1(2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)− 1) 2\n4P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)(1− P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)) ∣∣∣∣∣∣ ≤ (1 + p) ( |2Γ(ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1)− θ ? 〈t〉,1|+ θ ? 〈t〉,1 ( 2P (ã 〈t〉 1 , ‖b 〈t〉‖, θ?〈t〉,1)− 1 )2)\n≤ (1 + p)((4‖θ?‖+ 3)ã〈t〉1 + κ ′′ b |‖b〈t〉‖ − θ?〈t〉,1|+ 4‖θ ?‖ã〈t〉1 ) ≤ 2(8‖θ?‖+ 3)ã〈t〉1 + ( 1− κ′′b\n2κ′′b + 1)κ′′b |‖b〈t〉‖ − θ?〈t〉,1|\n≤ (16‖θ?‖+ 6)‖a〈t〉‖+ 1 + κ′′b\n2 |‖b〈t〉‖ − θ?〈t〉,1|.\nIn summary, if we set δa , min{δ1, Lθ, 1, δ2} > 0 and κ′b , 1+κ′′b 2 < 1, we have ∀ã 〈t〉 1 ≤ ‖a〈t〉‖ ∈ [0, δa], ‖b〈t〉‖ ∈ [Lb, Ub], θ?〈t〉,1 ∈ [Lθ, ‖θ ?‖],\n|b̃〈t+1〉〈t〉,1 − θ ? 〈t〉,1| ≤ (16‖θ ?‖+ 6)‖a〈t〉‖+ κ′b|‖b〈t〉‖ − θ?〈t〉,1|.\nSo far we have proved in (61) and (71) and the following bounds:\n|b̃〈t+1〉〈t〉,1 − θ ? 〈t〉,1| ≤ (16‖θ ?‖+ 6)‖a〈t〉‖+ κ′b|‖b〈t〉‖ − θ?〈t〉,1| |b̃〈t+1〉〈t〉,2 − θ ? 〈t〉,2| ≤ κs|θ ? 〈t〉,2|.\nLet κb = max{κs, κ′b} ∈ (0, 1) and c′b = 16‖θ ?‖+ 6. Then, we conclude that\n‖b〈t+1〉 − θ?‖2 = |b̃〈t+1〉〈t〉,1 − θ ? 〈t〉,1| 2 + |b̃〈t+1〉〈t〉,2 − θ ? 〈t〉,2| 2\n≤ ((16‖θ?‖+ 6)‖a〈t〉‖+ κ′b|‖b〈t〉‖ − θ?〈t〉,1|) 2 + (κsθ ? 〈t〉,2) 2 ≤ κ2b(|‖b〈t〉‖ − θ?〈t〉,1| 2 + |θ?〈t〉,2| 2) + (c′b) 2‖a〈t〉‖2 + 2c′b‖a〈t〉‖κb|‖b〈t〉‖ − θ?〈t〉,1| ≤ κ2b‖b〈t〉 − θ?‖2 + ((c′b)2 + 2c′bUb + 2c′b‖θ?‖)‖a〈t〉‖.\nSetting cb = (c′b) 2 + 2c′bUb + 2c ′ b‖θ ?‖ completes the proof of Lemma 17.\nA.7 Proof of Theorem 1\nTo prove this result we will use some of the results we have proved in the last few sections. We summarize them here.\n(i) In Section 1.2 we showed that to study the dynamics of Population EM for Model 1, it is sufficient to study\nθ〈t+1〉 = E((2wd(Y ,θ〈t〉)− 1)Y ), (71)\nwhere wd(y,θ〈t〉) = φ(y−θ〈t〉;I)\nφ(y−θ〈t〉;I)+φ(y+θ〈t〉;I) and Y ∼ 12N(−θ ?, Id) + 1 2N(θ ?, Id).\n(ii) In Section 1.2 and A.2 we showed that to study the dynamics of Population EM for Model 2, it is sufficient to study\na〈t+1〉 = γ〈t+1〉(1− 2p〈t+1〉) 2p〈t+1〉(1− p〈t+1〉) , b〈t+1〉 = γ〈t+1〉\n2p〈t+1〉(1− p〈t+1〉) . (72)\nwhere\nγ〈t+1〉 = Ewd(Y − a〈t〉, b〈t〉)Y , p〈t+1〉 = Ewd(Y − a〈t〉, b〈t〉),\nwhere Y ∼ 12N(−θ ?, Id) + 1 2N(θ ?, Id).\n(iii) According to Lemma 2, (72) reduces to (71), if a〈0〉 = 0. In other words, if we set a〈0〉 = 0, then a〈t〉 = 0 and b〈t〉 = θ〈t〉. This in turn implies that if we analyze the convergence dynamics of (72), we immediately obtain the convergence of (71) by setting a〈0〉 = 0.\n(iv) If β〈t〉 denotes the angle between θ? and b〈t〉, then we proved in Appendix A.5 that for (72) we have\n| sinβ〈t+1〉| ≤ κβ| sinβ〈t〉|.\nThe same is true for a〈0〉 = 0 initialization.\n(v) According to Lemma 9 and Lemma 10, we have ‖b〈t〉‖ ∈ [cL,1, cU,3]. According to Lemma 8, we have θ?〈t〉,1 ∈ [θ ? 〈0〉,1, ‖θ ?‖]. The same is true for a〈0〉 = 0 initialization.\nNote that we can employ Theorem 4 to claim that (by setting a〈t〉 = 0) if 〈b〈t〉,θ?〉 > 0, then for the symmetric case, there exists T0 such that for every t > T0,\n|b〈t+1〉 − θ?| ≤ κ2b |b〈t〉 − θ?|.\nHowever, our claim in Theorem 1 is stronger. In fact we would like to show that for the symmetric case the geometric convergence starts at iteration 1. We use the notations and equations developed in Appendix A.4. In particular, we use the rotation matrix U t introduced there and rotate all the vectors b〈t〉 and γ〈t〉 with U t. Note that according to Lemma 7 we know that b〈t+1〉 lies in the span of θ? and b〈t〉 and hence, b̃〈t+1〉〈t〉,i = 0 for i ≥ 3. Therefore we only need to consider the first two coordinates. According to (30) and (34), we have\n|b̃〈t+1〉〈t〉,2 − θ ? 〈t〉,2| =\n|γ̃〈t+1〉〈t〉,2 |\n2P (0, ‖b〈t〉‖, θ?〈t〉,1)(1− P (0, ‖b 〈t〉‖, θ?〈t〉,1))\n= |θ?〈t〉,2| ∣∣∣∣∣∣ S(0, ‖b〈t〉‖, θ?〈t〉,1)\n2P (0, ‖b〈t〉‖, θ?〈t〉,1)(1− P (0, ‖b 〈t〉‖, θ?〈t〉,1))\n− 1 ∣∣∣∣∣∣ = |θ?〈t〉,2||2S(0, ‖b 〈t〉‖, θ?〈t〉,1)− 1|, (73)\nwhere the last equality is due to the fact that P (0, ‖b〈t〉‖, θ?〈t〉,1) = 1 2 . Also, according to (60) we have\nκs , sup ã 〈t〉 1 ∈[0,Ua],‖b 〈t〉‖∈[Lb,Ub],θ?〈t〉,1∈[Lθ,‖θ ?‖]\n1− 2S(ã〈t〉1 , ‖b 〈t〉‖, θ?〈t〉,1) < 1.\nHence, let Ua = 0, Lb = cL,1, Ub = cL,3 and Lθ = θ?〈0〉,1, we conclude that\n|b̃〈t+1〉〈t〉,2 − θ ? 〈t〉,2| ≤ κs|θ ? 〈t〉,2|. (74)\nSimilarly, employing (30) for the first coordinate and using the fact that P (0, ‖b〈t〉‖, θ?〈t〉,1) = 1 2 , we obtain\n|b̃〈t+1〉〈t〉,1 − θ ? 〈t〉,1| = |2Γ(0, ‖b 〈t〉‖, θ?〈t〉,1)− θ ? 〈t〉,1| = |F (‖b 〈t〉‖, θ?〈t〉,1)− θ ? 〈t〉,1|. (75)\nNote that the last equality is due to (37). According to Lemma 18, we know that there exists κ′′b ∈ (0, 1) which is a function of only Lb, Ub, Lθ, ‖θ ?‖ such that\n|F (xb, xθ)− xθ| ≤ κ′′b |xb − xθ|,∀xb ∈ [Lb, Ub], xθ ∈ [Lθ, ‖θ?‖]. (76)\nLet Lb = cL,1, Ub = cL,3 and Lθ = θ?〈0〉,1. Combining (74), (75) and (76) completes the proof.\nA.8 Proof of Theorem 5\nA.8.1 Main Steps of the Proof\nThe proof for the case 〈b〈0〉,θ?〉 = 0 is very different from the proof of the case 〈b〈0〉,θ?〉 6= 0. It seems that the convergence of the algorithm to its stationary point may not be geometric and hence proof ideas we developed for the case 〈b〈0〉,θ?〉 6= 0 are not applicable here. Hence, we prove Theorem 4 using the following strategy:\n(i) We first characterize all the stationary points of Population EM. Let (a, b) denote the stationary points and we show that a = 0 and b ∈ {−θ?,0,θ?}. This is discussed in Appendix A.8.2.\n(ii) We then show that any accumulation point of {(a〈t〉, b〈t〉)} is one of the stationary points. Let (a∞, b∞) denote any accumulation point. This is discussed in (i) in Appendix A.8.3.\n(iii) We show that if 〈b〈0〉,θ?〉 = 0, b∞ can not converge to −θ? or θ?. Hence, the algorithm has to converge to 0. Since a∞ = 0 for all stationary points, we have {a〈t〉, b〈t〉} converges to (0,0). This is discussed in (ii) in Appendix A.8.3\nA.8.2 Characterizing the Fixed Points of Population EM\nFirst note that if we write the iterations of Population EM in terms of a〈t〉 and b〈t〉 we obtain\na〈t+1〉 = γ〈t+1〉(1− 2p〈t+1〉) 2p〈t+1〉(1− p〈t+1〉) , b〈t+1〉 = γ〈t+1〉\n2p〈t+1〉(1− p〈t+1〉) ,\nwhere\nγ〈t+1〉 = ∫ wd(y − a〈t〉, b〈t〉)yφ+d (y,θ ?) dy,\np〈t+1〉 = ∫ wd(y − a〈t〉, b〈t〉)φ+d (y,θ ?) dy.\nIf (γ〈t〉, p〈t〉,a〈t〉, b〈t〉) converges to (γ, p,a, b), then it is straightforward to show that\na = γ(1− 2p) 2p(1− p) , (77) b = γ\n2p(1− p) , (78)\nγ = ∫ wd(y − a, b)yφ+d (y,θ ?) dy, (79)\np = ∫ wd(y − a, b)φ+d (y,θ ?) dy. (80)\nHence, the main step of the proof is to characterize the solutions of these four equations. We first consider the one-dimensional setting in which Y ∈ R and prove the following two facts:\n(i) The only feasible solution for a is zero.\n(ii) We then set a = 0 and show that the only possible solutions for b are −θ∗, 0, θ∗.\nWe should prove the above two by considering the following four different cases: (1) a ≥ 0, b ≥ 0, (2) a ≥ 0, b ≤ 0, (3) a ≤ 0, b ≥ 0, (4) a ≤ 0, b ≤ 0. Since the four cases are similar we focus on the first case only, i.e., a ≥ 0, b ≥ 0. To prove that the only possible solution of a is zero, note that (77) can be written as\na = Γ(a, b, θ∗)(1− 2P (a, b, θ∗)) 2P (a, b, θ∗)(1− P (a, b, θ∗)) (1) ≤ κaa. (81)\nwhere κa < 1. Note that Inequality (1) is a result of Lemma 11. Note that (81) implies that a must be zero.\nThe only remaining step is to examine the solutions for b. It is straightforward to prove that P (0, b, θ∗) = 12 . Hence, we can simplify (78) to\nb = 2Γ(0, b, θ∗) = F (b, θ∗), (82)\nwhere the last equality is due to (37). The following lemma enables us to characterize the solutions of (82).\nLemma 19. F (xb, xθ) is a concave function of xb ≥ 0. Furthermore, we have the following: (i) F (0, xθ) = 0, (ii) F (xθ, xθ) = xθ.\nThe proof of this lemma is presented in the Appendix B.9. It is straightforward to use the above properties and show that F (xb, xθ) has in fact the shape that is exhibited in Figure 1, which proves our claim in the one dimensional setting.\nTo extend the proof to higher dimensions, we rotate the coordinates. Suppose that the fixed point is a, b, p,γ. Let M̃ denote a rotation for which the following two hold: (i) b̃ , M̃b = (‖b‖2, 0, 0, . . . , 0) and (ii) θ̃? , M̃θ? = (θ̃?1, θ̃?2, 0, . . . , 0). Define γ̃ = M̃γ and ã = M̃a. Lemma 3 shows that if we let b̃, ã, γ̃, p denote the corresponding fixed point in the new coordinates, then they satisfy the same equations, i.e.,\nã = γ̃(1− 2p) 2p(1− p) , (83) b̃ = γ̃\n2p(1− p) , (84)\nγ̃ = ∫ wd(y − ã, b̃)yφ+d (y,θ ?) dy, (85)\np = ∫ wd(y − ã, b̃)φ+d (y,θ ?) dy. (86)\nFirst, it is straightforward to employ (83) and (84) and confirm that ∀i ≥ 2 we have\nγ̃i = 2b̃ip(1− p) = 0, ãi = b̃i(1− 2p) = 0.\nHence, with θ̃?i = 0,∀i ≥ 3, we only need to consider the first two coordinates. Our goal is to prove the following two statements:\n(i) If θ̃?2 6= 0, then b̃1 = 0 and ã1 = 0. In other words, both a and b are zero.\n(ii) If θ̃?2 = 0, then the problem will be reduced to the one-dimensional problem that we have already discussed. Hence, it is straightforward to characterize the fixed points.\nHere we focus on case (i), i.e., θ̃?2 6= 0. Since γ̃2 = 0, we have\n0 = γ̃2 = ∫ wd(y − ã, b̃)y2φ+d (y, θ̃ ?) dy\n= 1\n2\n∫ w(y1 − ã1, ‖b‖)φ(y1 − θ̃?1) dy1 ∫ y2φ(y2 − θ̃?2) dy2\n+ 1\n2\n∫ w(y1 − ã1, ‖b‖)φ(y1 + θ̃?1) dy1 ∫ y2φ(y2 + θ̃?2) dy2\n= θ̃?2 ∫ w(y1 − ã1, ‖b‖)φ−(y1, θ̃?1) dy1\n= θ̃?2 ∫ +∞ 0 (w(y1 − ã1, ‖b‖)− w(−y1 − ã1, ‖b‖))φ−(y1, θ̃?1) dy1\n= θ̃?2 ∫ +∞ 0\ne2y1‖b‖ − e−2y1‖b‖\ne2y1‖b‖ + e−2y1‖b‖ + e2‖ã‖‖b‖ + e−2‖ã‖‖b‖ 1 2 √ 2π (e−(y1−θ̃ ? 1) 2/2 − e−(y1+θ̃?1)2/2) dy1.\nIt is straightforward to see that since θ̃?2 6= 0, then θ̃?1 = 0. Hence, from (83) and the definitions of Γ and P functions given in Appendix A.4 we have\n‖a‖ = ‖ã‖ = |ã1| = |b̃1||1− 2p| = Γ(‖ã‖, ‖b‖, 0)(1− 2P (‖ã‖, ‖b‖, 0)) 2P (‖ã‖, ‖b‖, 0)(1− P (‖ã‖, ‖b‖, 0)) ≤ κa‖ã‖,\nwhere the last inequality is due to Lemma 11. We know that κa < 1. Therefore we have a = 0 and\n‖b‖ = b̃1 = γ̃1\n2p(1− p) = 2γ̃1\n= 2 ∫ w(y − ã, b̃)y1φ+d (y, θ̃ ?) dy\n= 2\n∫ ey1‖b‖\ney1‖b‖ + e−y1‖b‖ y1 1√ 2π e−y 2 1/2 dy\n= 0.\nThus the only solution is (a, b) = (0,0).\nA.8.3 Proof of Convergence for Population EM\nWe can break the proof into the following steps. Let {a〈t〉, b〈t〉}∞t=1 denote all the estimates of the Population EM algorithm.\n(i) We first prove that every accumulation point of {a〈t〉, b〈t〉}∞t=1 satisfies the fixed point equations:\na = γ(1− 2p) 2p(1− p) , b = γ\n2p(1− p) ,\nγ = Ewd(Y − a, b)Y p = Ewd(Y − a, b),\nwhere Y ∼ 12N(−θ ?, I) + 12N(θ ?, I). This proof is presented in Appendix B.11. We have already proved that these fixed point equations only have the following solutions : a = 0 and b ∈ {−θ?,0,θ?}. We proved in Lemma 4 that sgn(〈b〈t〉,θ?〉) = sgn(〈b〈t+1〉,θ?〉). Hence, we conclude that 〈b〈t〉,θ?〉 = 0 for every t. The only possible fixed point is hence (0,0). In summary, in the first step we prove that it only has one accumulation point, that is (0,0).\n(ii) Next we prove that {a〈t〉, b〈t〉}∞t=1 is a convergent sequence. Suppose that the sequence does not converge to (0,0), then there exists an such that for every T , there exists a t > T such that\n‖(a〈t〉, b〈t〉)‖2 > . We construct a subsequence of our sequence in the following way: Set T = 1 and pick t1 > T such that ‖(a〈t1〉, b〈t1〉)‖2 > . Now, set T = t1 + 1, and pick t2 > T such that ‖(a〈t2〉, b〈t2〉)‖2 > . Continue the process until we construct a sequence {(a〈tn〉, b〈tn〉)}∞n=1. According to Lemma 9 {(a〈tn〉, b〈tn〉)}∞n=1 is in a compact set and has a convergent subsequence. But according to part (i) the converging subsequence of this sequence must converge to (0,0) which is in contradiction with the construction of the sequence {(a〈tn〉, b〈tn〉)}∞n=1. Hence {a〈t〉, b〈t〉}∞t=1 must be a convergent sequence and converges to (0,0)\nA.9 Proof of Theorem 6 Let â〈t〉 = µ̂ 〈t〉 1 +µ̂ 〈t〉 2\n2 and b̂ 〈t〉 = µ̂ 〈t〉 2 −µ̂ 〈t〉 1 2 . Then the iteration functions based on (â 〈t〉, b̂ 〈t〉 ) are the\nfollowing:\nâ〈t+1〉 = q̂〈t+1〉(1− 2p̂〈t+1〉) 2p̂〈t+1〉(1− p̂〈t+1〉) + ȳ 2(1− p̂〈t+1〉) , (87)\nb̂ 〈t+1〉 = q̂〈t+1〉\n2p̂〈t+1〉(1− p̂〈t+1〉) − ȳ 2(1− p̂〈t+1〉) . (88)\nwhere\nȳ = 1\nn n∑ i=1 yi,\nq̂〈t+1〉 = 1\nn n∑ i=1 wd(yi − â〈t〉, b̂ 〈t〉 )yi, (89)\np̂〈t+1〉 = 1\nn n∑ i=1 wd(yi − â〈t〉, b̂ 〈t〉 ), (90)\nwhere\nwd(y − xa,xb) = φd(y − xa − xb)\nφd(y − xa + xb) + φd(y − xa − xb)\n= e〈y−xa,xb〉\ne〈y−xa,xb〉 + e−〈y−xa,xb〉 .\nTherefore q̂〈t〉 and p̂〈t〉 are the empirical versions of γ〈t〉 and p〈t〉 respectively. Our first goal is, for each i ∈ {1, 2}, to compare the Population EM sequence (µ〈t〉i )t≥0 to the Sample-based EM sequence (µ̂ 〈t〉 i )t≥0, provided that the initial values µ 〈0〉 i and µ̂ 〈0〉 i are the same. We prove that\nâ〈t〉→a〈t〉 in probability, and b̂〈t〉→b〈t〉 in probability, as n→∞. (91)\nWe prove by induction. For t = 0, it is clear that (91) holds because both Population EM and Sample-based EM start with the same initialization. For t = 1, by Weak Large Law Numbers (WLLN), we have\nq̂〈1〉 = 1\nn n∑ i=1 wd(yi − â〈0〉, b̂ 〈0〉 )yi p→ Ewd(y − â〈0〉, b̂ 〈0〉 )y = γ〈1〉,\np̂〈1〉 = 1\nn n∑ i=1 wd(yi − â〈0〉, b̂ 〈0〉 ) p→ Ewd(y − â〈0〉, b̂ 〈0〉 ) = p〈1〉,\nȳ = 1\nn n∑ i=1 yi p→ Ey = 0.\nSince p〈1〉 ∈ (0, 1), by employing the continuous mapping theorem, we have\nâ〈1〉 = q̂〈1〉(1− 2p̂〈1〉) 2p̂〈1〉(1− p̂〈1〉) + ȳ 2(1− p̂〈1〉) → γ 〈1〉(1− 2p〈1〉) 2p〈1〉(1− p〈1〉) = a〈1〉 in probability,\nb̂ 〈1〉 = q̂〈1〉\n2p̂〈1〉(1− p̂〈1〉) +\nȳ 2(1− p̂〈1〉) → γ\n〈1〉\n2p〈1〉(1− p〈1〉) = b〈1〉 in probability.\nTherefore (91) holds for t = 1. Now we assume that (91) holds for t ≥ 1, and our goal is to prove it for t+ 1. Note that∥∥∥∥∂wd(y − xa,xb)∂xa ∥∥∥∥ = ∥∥∥∥∥− 2xb(e〈y,xb〉−〈xa,xb〉 + e−〈y,xb〉+〈xa,xb〉)2\n∥∥∥∥∥ ≤ ‖xb‖\n2 ,∥∥∥∥∂wd(y − xa,xb)∂xb ∥∥∥∥ = ∥∥∥∥∥ 2(y − xa)(e〈y−xa,xb〉 + e−〈y−xa,xb〉)2 ∥∥∥∥∥ ≤ ‖y − xa\n2 ‖\n≤ ‖y‖+ ‖xa‖ 2 .\nTherefore we have\n|p〈t+1〉 − p̂〈t+1〉| = |Ewd(y − a〈t〉, b〈t〉)− 1\nn n∑ i=1 wd(yi − â〈t〉, b̂ 〈t〉 )|\n≤ |Ewd(y − a〈t〉, b〈t〉)− 1\nn n∑ i=1 wd(yi − a〈t〉, b〈t〉)|+ ∣∣∣∣∣∣ 1n n∑ i=1 wd(yi − a〈t〉, b〈t〉)− 1 n n∑ i=1 wd(yi − â〈t〉, b̂ 〈t〉 ) ∣∣∣∣∣∣ ≤ |Ewd(y − a〈t〉, b〈t〉)− 1\nn n∑ i=1 wd(yi − a〈t〉, b〈t〉)|\n+ ∣∣∣∣∣∣∣ 1 n n∑ i=1 ‖b〈t〉ξ ‖ 2 ‖â〈t〉 − a〈t〉‖+ ‖yi‖+ ‖a 〈t〉 ξ ‖ 2 ‖b̂〈t〉 − b〈t〉‖  ∣∣∣∣∣∣∣\n≤ |Ewd(y − a〈t〉, b〈t〉)− 1\nn n∑ i=1 wd(yi − a〈t〉, b〈t〉)|+ 1 2 ∑n i=1 ‖yi‖ n ‖b̂〈t〉 − b〈t〉‖\n+ ‖b〈t〉ξ ‖ 2 ‖â〈t〉 − a〈t〉‖+ ‖a〈t〉ξ ‖ 2 ‖b̂〈t〉 − b〈t〉‖  , where\na 〈t〉 ξ = ξa 〈t〉 + (1− ξ)â〈t〉, and b〈t〉ξ = ξb 〈t〉 + (1− ξ)b̂〈t〉, for some ξ ∈ [0, 1].\nBy WLLN, induction assumption and\n‖a〈t〉ξ ‖ ≤ 2‖a 〈t〉‖+ ‖a〈t〉 − â〈t〉‖, and ‖b〈t〉ξ ‖ ≤ 2‖b 〈t〉‖+ ‖b〈t〉 − b̂〈t〉‖,\nwe have\n|p〈t+1〉 − p̂〈t+1〉| ≤ |Ewd(y − a〈t〉, b〈t〉)− 1\nn n∑ i=1 wd(yi − a〈t〉, b〈t〉)|+ 2‖b〈t〉‖+ ‖b〈t〉 − b̂〈t〉‖ 2 ‖â〈t〉 − a〈t〉‖\n+ 2‖a〈t〉‖+ ‖a〈t〉 − â〈t〉‖\n2 ‖b̂〈t〉 − b〈t〉‖+ 1 2 ∑n i=1 ‖yi‖ n ‖b̂〈t〉 − b〈t〉‖ → 0 in probability.\nSimilarly, we have\n‖γ〈t+1〉 − q̂〈t+1〉‖ = ‖Ewd(y − a〈t〉, b〈t〉)y − 1\nn n∑ i=1 wd(yi − â〈t〉, b̂ 〈t〉 )yi‖\n≤ ‖Ewd(y − a〈t〉, b〈t〉)y − 1\nn n∑ i=1 wd(yi − a〈t〉, b〈t〉)yi‖\n+‖ 1 n n∑ i=1 wd(yi − a〈t〉, b〈t〉)yi − 1 n n∑ i=1 wd(yi − â〈t〉, b̂ 〈t〉 )yi‖\n≤ ‖Ewd(y − a〈t〉, b〈t〉)y − 1\nn n∑ i=1 wd(yi − a〈t〉, b〈t〉)yi‖\n+ ∥∥∥∥∥∥ 1n n∑ i=1 ( ‖b〈t〉ξ ‖ 2 ‖â〈t〉 − a〈t〉‖+ ‖yi‖+ ‖a 〈t〉 ξ ‖ 2 ‖b̂〈t〉 − b〈t〉‖)yi ∥∥∥∥∥∥ ≤ ‖Ewd(y − a〈t〉, b〈t〉)y − 1\nn n∑ i=1 wd(yi − a〈t〉, b〈t〉)yi‖+ ∥∥∥∥∥∥ 12n n∑ i=1 ‖yi‖yi ∥∥∥∥∥∥ ‖b̂〈t〉 − b〈t〉‖ +(‖a〈t〉‖‖b〈t〉 − b̂〈t〉‖+ ‖b〈t〉‖‖a〈t〉 − â〈t〉‖+ ‖a〈t〉 − â〈t〉‖‖b〈t〉 − b̂〈t〉‖)‖ 1\nn n∑ i=1 yi‖.\nBy WLLN and induction assumption, we have\n‖γ〈t+1〉 − q̂〈t+1〉‖ → 0 in probability.\nTherefore with p〈t+1〉 ∈ (0, 1), we have\nâ〈t+1〉 = q̂〈t+1〉(1− 2p̂〈t+1〉) 2p̂〈t+1〉(1− p̂〈t+1〉) + ȳ 2(1− p̂〈t+1〉) → γ 〈t+1〉(1− 2p〈t+1〉) 2p〈t+1〉(1− p〈t+1〉) = a〈t+1〉 in probability,\nb̂ 〈t+1〉 = q̂〈t+1〉\n2p̂〈t+1〉(1− p̂〈t+1〉) +\nȳ 2(1− p̂〈t+1〉) → γ\n〈t+1〉\n2p〈t+1〉(1− p〈t+1〉) = b〈t+1〉 in probability.\nHence (91) holds for t+ 1. With induction, we completes the proof of this lemma.\nA.10 Proof of Theorem 7\nA.10.1 Roadmap of the Proof\nThe main idea of the proof is simple. We first show that if we initialize Sample-based EM in a way that â〈0〉 is small enough and b̂ 〈0〉 is in small neighborhood of θ?, then the sampled based EM will\nconverge to a point whose distance from θ? is O( √ d/n) with probability converging to 1 as n→∞. Let’s call this neighborhood of (a, b), N0,θ? . According to Theorem 4 and 3 we know that Population EM converges to the true parameter under quite general initialization. Hence, there exists an iteration T0 at which the estimate of Population EM is in N0,θ? . We know from Theorem 6 that at iteration T0, â〈T0〉 → a〈T0〉 and b̂ 〈T0〉 → b〈T0〉 in probability. Hence, with probability converging to 1, (â〈T0〉, b̂〈T0〉) ∈ N0,θ? , and\nhence (â〈t〉, b̂ 〈t〉 ) converge to a point that is at a distance O( √ d/n) from (0,θ?). In other words, if â∞ and b̂ ∞ the limiting estimates, then\n‖â∞‖ = O( √ d/n),\n‖b̂∞ − θ?‖ = O( √ d/n),\nwith probability converging to 1, which is equivalent to what we wanted to prove. As is clear from the above discussion, the only challenging part is to prove that if (â〈0〉, b̂ 〈0〉 ) is in small neighborhood of (0,θ?), then the sampled-based EM will converge to a point whose distance from (0,θ?) is O( √ d/n). The proof of this fact is our main goal in the rest of this proof.\nWe remind the reader that according to Theorems 3 and 4 the estimates of Population EM satisfy the following equations (if initialized properly):\n‖a〈t〉‖ → 0, ‖b〈t〉 − θ?‖ → 0, (92)\nAlso, we know from the arguments provided in the proof of Theorem 6 that â〈t〉 and b̂ 〈t〉\nconverge to a〈t〉 and b〈t〉 in probability. Hence, we expect to have a similar equations for â〈t〉 and b̂ 〈t〉 , except for probably an error term that will vanish as n→∞. The only issue that may happen is that the errors that are introduced in each iteration may accumulate and will let to a non-vanishing error for t→∞. Our first lemma shows that this does not happen.\nLemma 20. Suppose that there exist κa ∈ (0, 1), κb ∈ (0, 1) and cb > 0 such that for all t′ ≥ 1, we have\n‖â〈t′〉‖ ≤ κa‖â〈t ′−1〉‖+ a, (93)\n‖b̂〈t ′〉 − θ?‖ ≤ κb‖b̂ 〈t′−1〉 − θ?‖+ √ cb‖â〈t ′−1〉‖+ b, (94)\nfor some a, b > 0. Then we have ∀t ≥ 0,\n‖â〈t〉‖ ≤ (κa)t‖â〈0〉‖+ 1\n1− κa a, (95)\n‖b̂〈t〉 − θ?‖ ≤ (κb)t‖b̂ 〈0〉 − θ?‖+ t √ cb‖â〈0〉‖(max{ √ κa, κb})t +\n1\n1− κb\n√ cb\n1− κa a +\n1\n1− κb b\n(96)\nThe proof of this lemma will be presented to in Appendix A.10.2. According to this lemma as long as the errors that are introduced in each iteration are bounded by a and b, the overall error will also remain bounded and are, in the worst case, proportional to √ a and b. Hence, if α → 0 and b → 0 as n→∞, the overall errors will go to zero too. Hence, proving that (93) and (94) hold for a → 0 and → 0 will complete the proof Theorem 7. Lemma 21. There exists constants κa ∈ ( √ 3 2 , 1), κb ∈ (0, 1); cb > 0 and\nδa ∈ (0,min{1, √ 3\n2 ‖θ?‖, (1− κb) 2(1− (κa)2)‖θ?‖2 4cb })\nonly depending on θ?, such that if the initialization (â〈0〉, b̂ 〈0〉 ) satisfies\n‖â〈0〉‖ ≤ δa, and ‖b̂ 〈0〉 − θ?‖ ≤ √ 1− (κa)2‖θ?‖,\nthen ∀t ≥ 0, we have\n‖â〈t+1〉‖ ≤ κa‖â〈t〉‖+ a,\n‖b̂〈t+1〉 − θ?‖ ≤ κb‖b̂ 〈t〉 − θ?‖+ √ cb‖â〈t〉‖+ b,\nwith probability at least 1− 3δ. The value of the other constants are the following cθ = 4(‖θ?‖+ 2) √ 3d+ ln(1/δ)\nn ,\nCθ = 3‖θ?‖cθ, a = b = 9Cθ + cθ ρ(2− ρ) + 12Cθ ρ(2− ρ) + cθ 2− ρ ,\n(97)\nwhere ρ = sup‖xa‖≤1,‖xb‖≤ 32‖θ?‖max{P (xa,xb,θ ?), 1 − P (xa,xb,θ?)} ∈ (0, 1). The function P is defined in Appendix A.4. In addition, assume n is large enough to satisfy the following conditions:\nCθ < ρ\n2 ,\na = b ≤ min{(1− κa)δa, 1\n2 (1− κb)\n√ 1− (κa)2‖θ?‖}.\n(98)\nProof. Note that\nwd(y − xa,xb) , e〈y−xa,xb〉\ne〈y−xa,xb〉 + e−〈y−xa,xb〉 .\nWe showed the following equations in Appendix A.9:\nâ〈t+1〉 = q̂〈t+1〉(1− 2p̂〈t+1〉) 2p̂〈t+1〉(1− p̂〈t+1〉) + ȳ 2(1− p̂〈t+1〉) ,\nb̂ 〈t+1〉 = q̂〈t+1〉\n2p̂〈t+1〉(1− p̂〈t+1〉) − ȳ 2(1− p̂〈t+1〉) .\nwhere\nȳ = 1\nn n∑ i=1 yi,\nq̂〈t+1〉 = 1\nn n∑ i=1 wd(yi − â〈t〉, b̂ 〈t〉 )yi,\np̂〈t+1〉 = 1\nn n∑ i=1 wd(yi − â〈t〉, b̂ 〈t〉 ),\nWe will show in Appendix A.10.3 that with probability at least 1− 3δ, we have\n‖ 1 n n∑ i=1 yi‖ ≤ 4(‖θ?‖+ 2) √ 3d+ ln(1/δ) n = cθ, (99)\nsup ‖xb‖≤ 32‖θ ?‖,‖xa‖≤1 | 1 n n∑ i=1 wd(yi − xa,xb)− EY wd(y − xa,xb)| ≤ Cθ, (100)\nsup ‖xb‖≤ 32‖θ ?‖,‖xa‖≤1 ‖ 1 n n∑ i=1 (wd(yi − xa,xb)− 1 2 )yi − E(wd(y − xa,xb)− 1 2 )y‖ ≤ 9 2 Cθ.\n(101)\nNote that by setting δ = 1n , we see that cθ → 0, Cθ → 0, and δ → 0 simultaneously. In the rest of the proof we assume that (99), (100) and (101) hold. Let\nγ̄〈t+1〉 = Ewd(y − â〈t〉, b̂ 〈t〉 )y, p̄〈t+1〉 = Ewd(y − â〈t〉, b̂ 〈t〉 ),\nand\nā〈t+1〉 = γ̄〈t+1〉(1− 2p̄〈t+1〉) 2p̄〈t+1〉(1− p̄〈t+1〉) , b̄ 〈t+1〉 = γ̄〈t+1〉 2p̄〈t+1〉(1− p̄〈t+1〉) .\nThe following lemma that will be proved in Appendix A.10.4 is a key step in our analysis:\nLemma 22. There exists κa ∈ (0, 1) such that if ‖b̂ 〈t〉 − θ?‖ ≤ min{ √ 1− (κa)2, 12}‖θ ?‖, then\n‖ā〈t+1〉‖ ≤ κa‖â〈t〉‖, (102)\nFurthermore, there exist δ′a ∈ (0, 1), κb ∈ (0, 1) and cb > 0 such that if ‖â〈t〉‖ ∈ [0, δ′a], then\n‖b̄〈t+1〉 − θ?‖ ≤ κb‖b̂ 〈t〉 − θ?‖+ √ cb‖â〈t〉‖. (103)\nConstant κa, κb, δ′a and cb only depend on θ ?.\nThe above equations provide connections between (ā〈t+1〉, b̄〈t+1〉) and (â〈t〉, b̂ 〈t〉\n). Next, we establish connection between (ā〈t+1〉, b̄〈t+1〉) and (â〈t+1〉, b̂ 〈t+1〉 ). In the rest of the proof we assume that κa ∈ ( √ 3/2, 1). If κa is less than √ 3/2 we set it to √\n3/2. This is just for making notations simpler and has no specific technical reason.\nNote that from (87), we have\n‖â〈t+1〉‖ = ‖ q̂ 〈t+1〉(1− 2p̂〈t+1〉)\n2p̂〈t+1〉(1− p̂〈t+1〉) +\nȳ\n2(1− p̂〈t+1〉) ‖\n≤ ‖ q̂ 〈t+1〉(1− 2p̂〈t+1〉)\n2p̂〈t+1〉(1− p̂〈t+1〉) ‖+ ‖ ȳ 2(1− p̂〈t+1〉) ‖\n≤ ∣∣∣∣∣ (1− 2p̂〈t+1〉)2p̂〈t+1〉(1− p̂〈t+1〉) ∣∣∣∣∣ ‖q̂〈t+1〉 − γ̄〈t+1〉‖+ ‖ γ̄〈t+1〉(1− 2p̄〈t+1〉)2p̄〈t+1〉(1− p̄〈t+1〉) ‖+ ‖ ȳ2(1− p̂〈t+1〉)‖\n+ ∥∥∥∥∥∥ b̄ 〈t+1〉 ((p̄〈t+1〉)2 + (1− p̄〈t+1〉)2 − (1− 2p̄〈t+1〉)|p̂〈t+1〉 − p̄〈t+1〉|) p̂〈t+1〉(1− p̂〈t+1〉) ∥∥∥∥∥∥ |p̂〈t+1〉 − p̄〈t+1〉| ≤ ∣∣∣∣∣ 12p̂〈t+1〉(1− p̂〈t+1〉) ∣∣∣∣∣ ‖q̂〈t+1〉 − γ̄〈t+1〉‖+ ∥∥∥∥∥∥ 3b̄ 〈t+1〉 p̂〈t+1〉(1− p̂〈t+1〉)\n∥∥∥∥∥∥ |p̂〈t+1〉 − p̄〈t+1〉| +‖ā〈t+1〉‖+ ‖ ȳ\n2(1− p̂〈t+1〉) ‖. (104)\nFurthermore, from (88) we have\n‖b̂〈t+1〉 − b̄〈t+1〉‖ = ‖ q̂ 〈t+1〉\n2p̂〈t+1〉(1− p̂〈t+1〉) − ȳ 2(1− p̂〈t+1〉) − γ̄\n〈t+1〉\n2p̄〈t+1〉(1− p̄〈t+1〉) ‖\n≤ ‖ q̂ 〈t+1〉\n2p̂〈t+1〉(1− p̂〈t+1〉) − γ̄\n〈t+1〉\n2p̄〈t+1〉(1− p̄〈t+1〉) ‖+ ‖ ȳ 2(1− p̂〈t+1〉) ‖\n≤ ∣∣∣∣∣ 12p̂〈t+1〉(1− p̂〈t+1〉) ∣∣∣∣∣ ‖q̂〈t+1〉 − γ̄〈t+1〉‖+ ‖ ȳ2(1− p̂〈t+1〉)‖\n+ ∥∥∥∥∥∥ b̄ 〈t+1〉 (1− 2p̄〈t+1〉 + |p̂〈t+1〉 − p̄〈t+1〉|) p̂〈t+1〉(1− p̂〈t+1〉) ∥∥∥∥∥∥ |p̂〈t+1〉 − p̄〈t+1〉| ≤ ∣∣∣∣∣ 12p̂〈t+1〉(1− p̂〈t+1〉) ∣∣∣∣∣ ‖q̂〈t+1〉 − γ̄〈t+1〉‖+ ∥∥∥∥∥∥ 3b̄ 〈t+1〉 p̂〈t+1〉(1− p̂〈t+1〉)\n∥∥∥∥∥∥ |p̂〈t+1〉 − p̄〈t+1〉|+ ‖ ȳ2(1− p̂〈t+1〉)‖. (105)\nSuppose for the moment that ‖â〈t〉‖ ∈ [0, 1] and ‖b̂〈t〉 − θ?‖ ≤ 12‖θ ?‖. It is straightforward to use (100) and (98) and the definition of ρ in the statement of Lemma 21 to prove\np̂〈t+1〉 ∈ (ρ 2 , 1− ρ 2 ). (106)\nBy combining (99)-(101), (104)), (105), and (106) we obtain\n‖â〈t+1〉‖ ≤ ‖ā〈t+1〉‖+ 9Cθ + cθ ρ(2− ρ) + 12Cθ ρ(2− ρ) + cθ 2− ρ = ‖ā〈t+1〉‖+ a,\n‖b̂〈t+1〉 − b̄〈t+1〉‖ ≤ 9Cθ + cθ ρ(2− ρ) + 12Cθ ρ(2− ρ) + cθ 2− ρ = b, (107)\nand hence ‖b̂〈t+1〉 − θ?‖ ≤ ‖b̄〈t+1〉 − θ?‖+ b. Now suppose that the assumptions of Lemma 22 hold, i.e., ‖â〈t〉‖ ∈ [0, δ′a] and ‖b̂\n〈t〉 − θ?‖ ≤√ 1− (κa)2‖θ?‖. Then (107) implies that\n‖â〈t+1〉‖ ≤ ‖ā〈t+1〉‖+ a ≤ κa‖â〈t〉‖+ a,\n‖b̂〈t+1〉 − θ?‖ ≤ ‖b̄〈t+1〉 − θ?‖+ b ≤ κb‖b̂ 〈t〉 − θ?‖+ √ cb‖â〈t〉‖+ b. (108)\nNote that (108) is the result we claimed in Lemma 21. However, to obtain (103), which is one of the main steps in deriving (108) we have assumed that\n‖â〈t〉‖ ∈ [0, δ′a] and ‖b̂ 〈t〉 − θ?‖ ≤ √ 1− (κa)2‖θ?‖.\nIn order to prove the above equation holds for every t, we will prove an even stronger statement:\n‖â〈t〉‖ ∈ [0, δa] and ‖b̂ 〈t〉 − θ?‖ ≤ √ 1− (κa)2‖θ?‖, (109)\nwhere δa = min{δ′a, (1−κb)2(1−(κa)2)‖θ?‖2 4cb }. We use induction to prove that (109) holds ∀t ≥ 0. By the assumptions of this Lemma, the initial estimates (â〈0〉, b̂ 〈0〉 ) satisfy (109). Hence the base of the\ninduction is true. Suppose (109) holds for t ≥ 0, then for t+ 1 (108) holds. Hence all we need to prove is that\nκa‖â〈t〉‖+ a ≤ δa,\nand κb‖b̂ 〈t〉 − θ?‖+ √ cb‖â〈t〉‖+ b ≤ √ 1− (κa)2}‖θ?‖. (110)\nFor the first inequality, since the condition on n in (98) ensure that a ≤ (1− κa)δa, together with induction assumption that ‖â〈t〉‖ ≤ δa, we have\n‖â〈t+1〉‖ ≤ κa‖â〈t〉‖+ a ≤ κaδa + (1− κa)δa ≤ δa.\nTo prove (110) note that the condition on n ensure that\nb ≤ 1\n2 (1− κb)\n√ 1− (κa)2‖θ?‖.\nAlso the condition on δa and ‖â〈t〉‖ ≤ δa ensure that√ cb‖â〈t〉‖ ≤ 1\n2 (1− κb)\n√ 1− (κa)2‖θ?‖.\nHence with induction assumption that ‖b̂〈t〉 − θ?‖ ≤ √ 1− (κa)2‖θ?‖, we have\n‖b̂〈t+1〉 − θ?‖ ≤ κb‖b̂ 〈t〉 − θ?‖+ √ cb‖â〈t〉‖+ b\n≤ κb √ 1− (κa)2‖θ‖+ 1\n2 (1− κb)\n√ 1− (κa)2‖θ?‖+ 1\n2 (1− κb)\n√ 1− (κa)2‖θ?‖\n= √ 1− (κa)2‖θ?‖.\nHence the second part of (109) holds for t+ 1. This completes the proof.\nA.10.2 Proof of Lemma 20\nWe first prove (95) for ‖â〈t〉‖. Clearly the result holds for t = 0. For all t ≥ 1, using the condition (93) on ‖â〈t′〉‖ for all t′ ≤ t, we have\n‖â〈t〉‖ ≤ κa‖â〈t−1〉‖+ a ≤ κa(κa‖â〈t−2〉‖+ a) + a\n≤ (κa)t‖â〈0〉‖+ a t−1∑ i=0 (κa) i ≤ (κa)t‖â〈0〉‖+ 1\n1− κa a.\nHence (95) holds. Next, we prove (96) for ‖b̂〈t〉‖. Clearly the result holds for t = 0. For all t ≥ 1, using the condition (94) on ‖b̂〈t ′〉‖ for all t′ ≤ t, we have\n‖b̂〈t〉 − θ?‖ ≤ κb‖b̂ 〈t−1〉 − θ?‖+ √ cb‖â〈t−1〉‖+ b\n≤ κb(κb‖b̂ 〈t−2〉 − θ?‖+ √ cb‖â〈t−2〉‖+ b) + b\n≤ (κb)t‖b̂ 〈0〉 − θ?‖+ √ cb t−1∑ i=0 (κb) t−1−i √ ‖â〈i〉‖+ b t−1∑ i=0 (κb) i\n≤ (κb)t‖b̂ 〈0〉 − θ?‖+ √ cb t−1∑ i=0 (κb) t−1−i √ ‖â〈i〉‖+ 1 1− κb b.\nFrom (95), we have ∀t ≥ 0,√ ‖â〈t〉‖ ≤ √ (κa)t‖â〈0〉‖+\n1\n1− κa a ≤ (κa)\nt 2 √ ‖â〈0〉‖+ √ 1\n1− κa a.\nHence we have\n‖b̂〈t〉 − θ?‖ ≤ (κb)t‖b̂ 〈0〉 − θ?‖+ √ cb t−1∑ i=0 (κb) t−1−i √ ‖â〈i〉‖+ 1 1− κb b\n≤ (κb)t‖b̂ 〈0〉 − θ?‖+ √ cb t−1∑ i=0 (κb) t−1−i(( √ κa) i √ ‖â〈0〉‖+ √ 1 1− κa a) +\n1\n1− κb b\n= (κb) t‖b̂〈0〉 − θ?‖+ √ cb‖â〈0〉‖ t−1∑ i=0 (κb) t−1−i( √ κa) i + √ cb 1− κa a t−1∑ i=0 (κb) i +\n1\n1− κb b\n≤ (κb)t‖b̂ 〈0〉 − θ?‖+ t √ cb‖â〈0〉‖(max{ √ κa, κb})t +\n1\n1− κb\n√ cb\n1− κa a +\n1\n1− κb b.\nThis completes the proof of this lemma.\nA.10.3 Proof of (99)-(101)\nLemma 23. Let y1, · · · , yn i.i.d.∼ 12N(θ ?, Id) + 1 2N(−θ ?, Id). Then, we have\n(1) ‖ 1n ∑n i=1 yi‖ ≤ 4(‖θ ?‖+ 1) √ 2d+ln(1/δ) n , with probability at least 1− δ.\n(2) sup‖xb‖≤c,‖xa‖≤1 | 1 n ∑n i=1 wd(yi−xa,xb)−Ewd(Y −xa,xb)| ≤ 8c(‖θ ?‖+ 2) √ d+2+ln(1/δ) n , with\nprobability at least 1− δ.\n(3) sup‖xb‖≤c,‖xa‖≤1 ‖ 1 n ∑n i=1(wd(yi − xa,xb) − 1 2)yi − E(wd(Y − xa,xb) − 1 2)Y ‖ ≤ 36c(‖θ ?‖ +\n2)\n√ d+2+ln(1/δ)\nn , with probability at least 1− δ.\nProof. We first prove the first claim (1). Note that yi can be expressed by yi = ζiθ ? + ωi, where ζi are i.i.d sequence of Rademacher variables and ωi are i.i.d N(0, Id) Gaussian random variables.\nTherefore we have,\n‖ 1 n n∑ i=1 yi‖2 = ‖ 1 n n∑ i=1 ζiθ ? + 1 n n∑ i=1 ωi‖2\n= 1 n ‖ 1√ n n∑ i=1 ζiθ ? + 1√ n n∑ i=1 ωi‖2.\nNote that ‖ 1√ n ∑n i=1ωi‖2 dist. = ν, where ν ∼ χ2(d). Hence, using Cramér-Chernoff inequality, we have probability at least 1− δ2 such that∣∣∣∣∣∣‖ 1√n n∑ i=1 ωi‖2 − d\n∣∣∣∣∣∣ ≤√8d ln(2/δ) ≤ d+ 2 ln(2/δ) for sufficiently large n. Moreover, for Rademacher variables ζi, using Hoeffding’s inequality, we have with probability at least 1− δ2 such that\n| 1√ n n∑ i=1 ζi| ≤ √ 2 ln(2/δ)\nTherefore, we have probability at least 1− δ such that\n‖ 1 n n∑ i=1 yi‖ ≤ 1√ n ‖ 1√ n n∑ i=1 ζiθ ? + 1√ n n∑ i=1 ωi‖\n≤ 1√ n √√√√2‖ 1√ n n∑ i=1 ζiθ ?‖2 + 2‖ 1√ n n∑ i=1 ωi‖2\n≤ 1√ n\n√ 2(2 ln(2/δ))‖θ?‖2 + 2(2d+ 2 ln 2/δ)\n= 2\n√ ln(2/δ)(‖θ?‖2 + 1) + d\nn ≤ 4(‖θ?‖+ 1) √ 2d+ ln(1/δ)\nn .\nFor the second claim, define\nZ+ , sup ‖xb‖≤c,‖xa‖≤1\n1\nn n∑ i=1 wd(yi − xa,xb)− Ewd(Y − xa,xb).\nThen we have ∀‖xb‖ ≤ c, ‖xa‖ ≤ 1\nEeλZ+ (i)\n≤ EY ,Y ′e λ sup‖xb‖≤c,‖xa‖≤1\n1 n ∑n i=1(wd(yi−xa,xb)−wd(y′i−xa,xb))\n= EY ,Y ′,ξe λ sup‖xb‖≤c,‖xa‖≤1\n1 n ∑n i=1 ξi(wd(yi−xa,xb)−wd(y′i−xa,xb))\n≤ EY ,Y ′,ξe λ sup‖xb‖≤c,‖xa‖≤1\n| 1 n ∑n i=1 ξi(wd(yi−xa,xb)−wd(y′i−xa,xb))|\n≤ Eξ{EY ( eλ sup‖xb‖≤c,‖xa‖≤1 | 1 n ∑n i=1 ξi(wd(yi−xa,xb)− 1 2 )| )\n×EY ′ ( eλ sup‖xb‖≤c,‖xa‖≤1 | 1 n ∑n i=1 ξi(wd(y ′ i−xa,xb)− 1 2 )| ) }\n≤ EY ,ξe2λ sup‖xb‖≤c,‖xa‖≤1 | 1 n\n∑n i=1 ξi(wd(yi−xa,xb)− 1 2\n)|,\nNote that to obtain Inequality (i) we have used Jensen’s inequality. Also, ξi are i.i.d sequence of Rademacher variables. To simplify the final expression even further, we use the following lemma from Koltchinskii (2011)\nLemma 24. Let H ∈ Rn and let ψi : R 7→ R, i = 1, · · · , n be functions such that ψi(0) = 0 and\n|ψi(u)− ψi(v)| ≤ |u− v| ∈ R.\nFor all convex nondecreasing functions Ψ : R+ 7→ R+,\nEΨ( 1 2 sup h∈H | n∑ i=1 ψi(hi) i|) ≤ EΨ(sup h∈H | n∑ i=1 hi i|),\nwhere i are i.i.d. Rademacher random variables.\nSince wd(y − xa,xb) is a function of 〈y − xa,xb〉 and\n|wd(y − xa,xb)− wd(y − x′a,x′b)| ≤ 1 2 |〈y − xa,xb〉 − 〈y − x′a,x′b〉|,\nletting Ψ(x) = e2λx and ψi(x) = 2e x\nex+e−x − 1 with hi = 〈yi − xa,xb〉 in Lemma 24, we have\nEeλZ+ ≤ EY ,ξe2λ sup‖xb‖≤c,‖xa‖≤1 | 1 n\n∑n i=1 ξi(wd(yi,xa,xb)− 1 2 )|\n≤ EY,ξe2λ sup‖xb‖≤c,‖xa‖≤1 | 1 n\n∑n i=1 ξi〈yi−xa,xb〉|\n(ii) = EY ,ξe2λ sup‖xb‖≤c,‖xa‖≤1 1 n\n∑n i=1 ξi〈yi−xa,xb〉\n≤ EY ,ξe2λ sup‖xb‖≤c 1 n ∑n i=1 ξi〈yi,xb〉e2λ sup‖xb‖≤c,‖xa‖≤1〈xa,xb〉 1 n ∑n i=1 ξi ≤ EY ,ξe2λc‖ 1 n ∑n i=1 ξiyi‖e2λc| 1 n ∑n i=1 ξi| ≤ (EY ,ξe4λc‖ 1 n ∑n i=1 ξiyi‖)1/2(Eξe4λc| 1 n ∑n i=1 ξi|)1/2 ≤ (EY e4λc‖ 1 n ∑n i=1 yi‖︸ ︷︷ ︸\npart 1\n)1/2(Eξe4λc| 1 n ∑n i=1 ξi|︸ ︷︷ ︸\npart 2\n)1/2,\nwhere last equality holds for the fact that the distribution of yi is symmetric and equality (ii) holds for the fact that 1n ∑n i=1 ξi〈yi − xa,xb〉 is symmetric in terms of xb and the constraints on xb is symmetric. For part 1, we use the notation {uj , j = 1, · · · ,M} for a 1/2-covering of the d-dimensional sphere, Spd , {v ∈ Rd, ‖v‖ = 1}. Note that, for all v′,v ∈ Spd,\n| 1 n n∑ i=1 〈yi,v′〉 − 1 n n∑ i=1 〈yi,v〉| ≤ ‖v′ − v‖ sup ‖u‖=1 1 n n∑ i=1 〈yi,u〉,\ntherefore, we have for all u ∈ Spd\n1\nn n∑ i=1 〈yi,u〉 ≤ max j∈[M ] { 1 n n∑ i=1 〈yi,uj〉}+ ‖uj − u‖ sup ‖u‖=1 1 n n∑ i=1 〈yi,u〉,\nand hence\n1 n ‖ n∑ i=1 yi‖ = sup ‖u‖=1 1 n n∑ i=1 〈yi,u〉 ≤ 2 max j∈[M ] { 1 n n∑ i=1 〈yi,uj〉}. (111)\nrecall that yi = ζiθ ? + ωi. Hence, we have\nEY e〈yi,uj〉 = Eζeζ〈θ ?,uj〉Eωe〈ωi,uj〉\n= 1\n2 (e〈θ ?,uj〉 + e−〈θ ?,uj〉)e 1 2 ≤ e\n‖θ?‖2+1 2 , (112)\nwhere last inequality holds because of\n1 2 (e‖θ ?‖ + e−‖θ ?‖) ≤ e ‖θ?‖2 2 .\nTherefore we have\nEY ,ξe4λc‖ 1 n ∑n i=1 yi‖ = EY ,ξe4λc sup‖u‖=1 1 n ∑n i=1〈yi,u〉\n≤ EY ,ξe8λcmaxj∈[M ] 1 n\n∑n i=1〈yi,uj〉\n≤ M∑ j=1 EY ,ξe8λc 1 n ∑n i=1〈yi,uj〉 ≤ e32λ2c2 ‖θ?‖2+1 n +2d. (113)\nFor part 2, notice that 1n ∑n i=1 ξi is symmetric, we have\nEξe4λc| 1 n ∑n i=1 ξi| ≤ 2Eξe4λc 1 n ∑n i=1 ξi\n≤ 2(Eξe 4λc n ξ)n ≤ e 8λ2c2 n +1. (114)\nTherefore combining (113) and (114), we have\nEeλZ+ ≤ e16λ2c2 ‖θ?‖2+1 n +d × e 4λ2c2 n + 1 2\n≤ e16λ2c2 ‖θ?‖2+2 n +d+ 1 2 .\nUsing Markov Inequality: P (Z+ > ) ≤ EeλZ+−λ ,∀ , λ > 0,\nchoosing λ = n 32c2(‖θ?‖2+2) , we have\nP (Z+ > ) ≤ e 16c2λ2(‖θ?‖2+2) n +d+ 1 2 −λ\n= e − n\n2\n64c2(‖θ?‖2+2) +d+ 1\n2 .\nTherefore\n| sup ‖xb‖≤c,‖xa‖≤1\n1\nn n∑ i=1 wd(yi − xa,xb)− Ewd(Y − xa,xb)| ≤ 8c(‖θ?‖+ 2) √ d+ 2 + ln(1/δ) n ,\nwith probability at least 1− δ. For the last claim, we borrow a technique in the proof of corollary 2 in B.2 in Balakrishnan et al. (2014). Let\nZ = sup ‖xb‖≤c,‖xa‖≤1\n‖ 1 n n∑ i=1 (wd(yi − xa,xb)− 1 2 )yi − E(wd(Y − xa,xb)− 1 2 )Y ‖,\nand\nZu = sup ‖xb‖≤c,‖xa‖≤1\n| 1 n n∑ i=1 (wd(yi − xa,xb)− 1 2 ))〈yi,u〉 − E(wd(Y − xa,xb)− 1 2 )〈Y ,u〉|\nwe have\nEeλZ = EY eλ sup‖u‖=1 Zu ≤ Ee2λmaxj∈[M ] Zuj ≤ M∑ j=1 Ee2λZuj\n≤ M∑ j=1 EY ,ξe4λ sup‖xb‖≤c,‖xa‖≤1 1 n ∑n i=1 ξi(wd(yi−xa,xb)− 1 2 )〈yi,uj〉,\nwhere ξi are i.i.d. sequence of Rademacher variables and the last inequality holds for standard symmetrization result for empirical process. Since\n|(2wd(yi−xa,xb)−1)〈yi,uj〉−(2wd(yi−x′a,x′b)−1)〈yi,uj〉| ≤ |〈yi−xa,xb〉−〈yi−x′a,x′b〉|〈yi,uj〉,\nlet Ψ(x) = e2λx and ψi(x) = ( 2e x\nex−e−x − 1)〈yi,uj〉 with hi = 〈yi − xa,xb〉 in Lemma 24, we have\nEeλZ ≤ M∑ j=1 EY ,ξe4λ sup‖xb‖≤c,‖xa‖≤1 | 1 n ∑n i=1 ξi〈yi−xa,xb〉〈yi,uj〉|\niii = M∑ j=1 EY ,ξe4λ sup‖xb‖≤c,‖xa‖≤1 1 n ∑n i=1 ξi〈yi−xa,xb〉〈yi,uj〉 ≤ M∑ j=1 EY ,ξe4λ sup‖xb‖≤c 1 n ∑n i=1 ξi〈yi,xb〉〈yi,uj〉e4λ sup‖xb‖≤c,‖xa‖≤1 1 n ∑n i=1 ξi〈xa,xb〉〈yi,uj〉 ≤ M∑ j=1 (EY ,ξe8λ sup‖xb‖≤c 1 n ∑n i=1 ξi〈yi,xb〉〈yi,uj〉) 1 2 (EY ,ξe8λ sup‖xb‖≤c,‖xa‖≤1 1 n ∑n i=1 ξi〈xa,xb〉〈yi,uj〉) 1 2 ≤ M∑ j=1 (EY ,ξe8λc‖ 1 n ∑n i=1 ξiyiy > i ‖op︸ ︷︷ ︸\npart1\n) 1 2 (EY ,ξe8λc 1 n | ∑n i=1 ξi〈yi,uj〉|︸ ︷︷ ︸\npart2\n) 1 2 ,\nwhere ‖ · ‖op is l2-operator norm of a matrix(maximum singular value), equality (iii) holds for the fact that 1n ∑n i=1 ξi〈yi − xa,xb〉〈yi,uj〉 is symmetric in terms of xb and constraints of xb is symmetric. The correctness of the last inequality is shown in B.2 of Balakrishnan et al. (2014). For part 1, as shown in B.2 of Balakrishnan et al. (2014) we have\nEY ,ξe8λc‖ 1 n\n∑n i=1 ξiyiy\n> i ‖op ≤ EY ,ξe16λcmaxj′∈[M ] 1 n\n∑n i=1 ξi〈yi,uj′ 〉2\n(115)\nRecall that yi = ζiθ ? + ωi and (112), we have\nEY e〈yi,uj〉 = Eζeζ〈θ ?,uj〉Eωe〈ωi,uj〉 ≤ e ‖θ?‖2+1 2 .\nTherefore Eeλξ〈yi,uj〉 2 ≤ e (‖θ?‖2+1)λ2 2 , for small enough λ.\nTherefore\nEY ,ξe16λcmaxj′∈[M ] 1 n ∑n i=1 ξi〈yi,uj′ 〉2 ≤ M∑ j′=1 EY ,ξe16λc 1 n ∑n i=1 ξi〈yi,uj′ 〉2\n≤ e (‖θ?‖2+1)(16cλ)2 2n +2d.\nFor part 2, since ξi〈yi,uj〉 dist. = 〈yi,uj〉, using (112), we have\nEY ,ξe8λc 1 n | ∑n i=1 ξi〈yi,uj〉| = EY e8λc 1 n | ∑n i=1〈yi,uj〉|\niv ≤ 2EY e8λc 1 n\n∑n i=1〈yi,uj〉\n≤ e (‖θ?‖2+1)(8cλ)2 2n +1,\nwhere inequality (iv) holds for the fact that the distribution of 1n ∑n\ni=1〈yi,uj〉 is symmetric. Therefore, combining part 1 and part 2, we have\nEeλZ ≤ M∑ j=1 e (‖θ?‖2+1)(16cλ)2 4 +de (‖θ?‖2+1)(8cλ)2 4 + 1 2\n≤ e 81(‖θ?‖2+1)c2λ2 n +3d+ 1 2 .\nUsing Markov Inequality: P (Z > ) ≤ EY eλZ−λ ,∀ , λ > 0,\nchoosing λ = n 32c2(‖θ?‖2+2) , we have\nP (Z > ) ≤ e 81c2λ2(‖θ?‖2+1) n +3d+ 1 2 −λ\n= e − n\n2\n324c2(‖θ?‖2+1) +3d+ 1\n2 .\nTherefore\nsup ‖xb‖≤c,‖xa‖≤1\n‖ 1 n n∑ i=1 (wd(yi − xa,xb)− 1 2 )yi − Ewd(Y − xa,xb)Y ‖ ≤ 36c(‖θ?‖+ 2)\n√ d+ 2 + ln(1/δ)\nn ,\nwith probability at least 1− δ.\nA.10.4 Proof of Lemma 22 Since ā〈t+1〉 and b̄〈t+1〉 are the result of first iteration based on initialization (â〈t〉, b̂ 〈t〉\n) in Population EM model where initialization (â〈t〉, b̂ 〈t〉 ) satisfying the corresponding condition mentioned in the lemma. Hence to prove the lemma holds for all t ≥ 0, it is sufficient to prove that for any initialization (a〈0〉, b〈0〉) satisfying the same condition, we have ‖a〈1〉‖ ≤ κa‖a〈0〉‖ for (102) and ‖b〈1〉 − θ?‖ ≤ κb‖b〈0〉 − θ?‖+ √ cb‖a〈0〉‖ for (103). To achieve this goal, we use the notations and definition that are summarized in Appendix A.4. We first prove the first claim:\n‖a〈1〉‖ ≤ κa‖a〈0〉‖. (116)\nIf a〈0〉 = 0, we immediately have (116) holds. If a〈0〉 6= 0, because of Lemma 4 and Lemma 5, we assume 〈a〈0〉, b〈0〉〉 > 0 without loss of generality, thus ã〈0〉1 > 0. Since b̃ 〈1〉 (1 − 2p〈1〉) = ã〈1〉, we\nknow they are in the same direction, thus the angle between a〈1〉 and θ? is the same angle between b〈1〉 and θ?, i.e., β〈1〉. Furthermore, according to Lemma 8, we have β〈1〉 ≤ β〈0〉. Hence we have\n‖a〈1〉‖ = ã 〈1〉 1\ncosβ〈1〉 ≤ ã\n〈1〉 1\ncosβ〈0〉 . (117)\nTherefore, we need to bound ã〈1〉1 and 1 cosβ〈0〉 . According to (39) and Lemma 11 we have,\nã 〈1〉 1 =\nΓ(ã 〈0〉 1 , ‖b 〈0〉‖, θ?〈0〉,1)(1− 2P (ã 〈0〉 1 , ‖b 〈0〉‖, θ?〈0〉,1)) P (ã 〈0〉 1 , ‖b 〈0〉‖, θ?〈0〉,1)(1− P (ã 〈0〉 1 , ‖b 〈0〉‖, θ?〈0〉,1)) ≤ κ′aã 〈0〉 1 ≤ κ ′ a‖a〈0〉‖,\nwhere κ′a ∈ (0, 1) is a continuous function of θ?〈0〉,1 > 0. Since the condition of ‖b 〈0〉 − θ?‖ ≤ 12‖θ ?‖ implies that θ?〈0〉,1 ≥ √ 3 2 ‖θ ?‖ > 0, we have\nκa , sup θ?〈0〉,1∈[ √ 3 2 ‖θ?‖,‖θ?‖]\n√ κ′a(θ ? 〈0〉,1) ∈ (0, 1),\nand κa only depends on θ?. Now for 1cosβ〈0〉 , by the condition of ‖b 〈0〉 − θ?‖ ≤\n√ 1− (κa)2‖θ?‖, we\nhave cosβ〈0〉 ≥ κa. Hence, combining the two parts in (117), we have\n‖a〈1〉‖ ≤ ã 〈1〉 1 cosβ〈0〉 ≤ κ′a(θ ? 〈0〉,1) κa ‖a〈0〉‖ ≤ κa‖a〈0〉‖.\nHence (116) holds. Next we prove the second claim: ‖b〈1〉 − θ?‖ ≤ κb‖b〈0〉 − θ?‖+ √ cb‖a〈0〉‖.\nAccording to Lemma 17 we can conclude there exists δ′a ∈ (0, 1), κb ∈ (0, 1) and cb > 0 such that that if ‖a〈0〉‖ ≤ δ′a, then\n‖b〈1〉 − θ?‖ ≤ √ κ2b‖b 〈0〉 − θ?‖2 + cb‖a〈0〉‖ ≤ κb‖b〈0〉 − θ?‖+ √ cb‖a〈0〉‖,\nwhere δ′a, κb and cb only depend on Ua = 1, Lb = 1 2‖θ ?‖, Ub = 32‖θ ?‖, Lθ =\n√ 3\n2 ‖θ ?‖ and ‖θ?‖. Hence\nδ′a, κb and cb only depend on θ ?. This completes the proof.\nB Proofs of Auxiliary Results\nB.1 Proof of Lemma 4\nFrom (12) and (12), we know that\n〈b〈t+1〉,θ?〉 = 〈γ 〈t+1〉,θ?〉\np〈t+1〉(1− p〈t+1〉) ,\n〈a〈t+1〉, b〈t+1〉〉 = ‖b〈t+1〉‖2(1− 2p〈t+1〉).\nSince p〈t+1〉 ∈ (0, 1) and ‖θ?‖ > 0, we have\nsgn(〈b〈t+1〉,θ?〉) = sgn(〈γ〈t+1〉,θ?〉) sgn(〈a〈t+1〉, b〈t+1〉〉) = sgn(1− 2p〈t+1〉) sgn(‖b〈t+1〉‖2)\n= sgn(1− 2p〈t+1〉)| sgn(〈b〈t+1〉,θ?〉)|.\nHence if we have\nsgn(〈γ〈t+1〉,θ?〉) = sgn(〈b〈t〉,θ?〉), (118)\nand\nsgn(1− 2p〈t+1〉) = sgn(〈a〈t〉, b〈t〉〉), (119)\nthen immediately, we have\nsgn(〈b〈t+1〉,θ?〉) = sgn(〈b〈t〉,θ?〉),\nand\nsgn(〈a〈t+1〉, b〈t+1〉〉) = sgn(1− 2p〈t+1〉)| sgn(〈b〈t+1〉,θ?〉)| = sgn(〈a〈t〉, b〈t〉〉)| sgn(〈b〈t〉,θ?〉)| = sgn(〈a〈t〉, b〈t〉〉).\nHence our next goal is to prove (118) and (119). Consider the rotation matrix O for which θ̃? , Oθ? has all its coordinates except the first one equal to zero, i.e., θ̃? = (‖θ?‖, 0, . . . , 0)>. Also, let b̃ 〈t〉 , Ob〈t〉, and ã〈t〉 , Oa〈t〉. According to Lemma 3, we have\n〈γ̃〈t+1〉, θ̃?〉 = 〈θ̃?,Ewd(Y − ã〈t〉, b̃ 〈t〉 )Y 〉 = ‖θ̃?‖ ∫ wd(y − ã〈t〉, b̃ 〈t〉 )y1φ + d (y, θ̃ ?) dy\n= ‖θ̃?‖ ∫ e− ∑d i=2 y 2 i /2\n√ 2π d−1 (\n∫ wd(y − ã〈t〉, b̃ 〈t〉 )y1φ +(y1, ‖θ̃?‖) dy1) dy2 · · · dyd\n(120)\nHence, define y2...d = (y2, · · · , yd) and B(y2...d) , ∑d i=2 yib 〈t〉 i , then to prove (118), it is sufficient to prove\nsgn( ∫ wd(y − ã〈t〉, b̃ 〈t〉 )y1φ +(y1, ‖θ̃?‖) dy1) = sgn(〈b〈t〉,θ?〉) = sgn(‖θ?‖b̃〈t〉1 )\n= sgn(b̃ 〈t〉 1 ), ∀y2...d, B(y2...d).\nNote that∫ wd(y − ã〈t〉, b̃ 〈t〉 )y1φ +(y1, ‖θ̃?‖) dy1\n= ∫ +∞ 0 (wd((y1,y2...d) > − ã〈t〉, b̃〈t〉)− wd((−y1,y2...d)> − ã〈t〉, b̃ 〈t〉 ))y1φ +(y1, ‖θ̃?‖) dy1\n= ∫ +∞ 0\ne2y1b̃ 〈t〉 1 − e−2y1b̃ 〈t〉 1\ne2y1b̃ 〈t〉 1 + e−2y1b̃ 〈t〉 1 + e2B(y2...d)−2〈ã 〈t〉,b̃ 〈t〉〉 + e−2B(y2...d)+2〈ã 〈t〉,b̃ 〈t〉〉\ny1φ +(y1, ‖θ̃?‖) dy1.\nHence, we have\nsgn( ∫ wd(y − ã〈t〉, b̃ 〈t〉 )y1φ +(y1, ‖θ̃?‖) dy1) = sgn(b̃〈t〉1 ), ∀y2...d, B(y2...d).\nTo prove (119), according to Lemma 3, we have\n2p〈t+1〉 − 1 = E(2wd(Y − ã〈t〉, b̃ 〈t〉 )− 1)\n= E(wd(Y − ã〈t〉, b̃ 〈t〉 ) + wd(−Y − ã〈t〉, b̃ 〈t〉 )− 1)\n= E( e2〈Y ,b̃\n〈t〉〉 + e−2〈Y ,b̃ 〈t〉〉 + 2e−2〈ã 〈t〉,b̃ 〈t〉〉\ne2〈Y ,b̃ 〈t〉〉 + e−2〈Y ,b̃ 〈t〉〉 + e2〈ã 〈t〉,b̃ 〈t〉〉 + e−2〈ã 〈t〉,b̃\n〈t〉〉 − 1)\n= E e−2〈ã\n〈t〉,b̃ 〈t〉〉 − e2〈ã〈t〉,b̃ 〈t〉〉\ne2〈Y ,b̃ 〈t〉〉 + e−2〈Y ,b̃ 〈t〉〉 + e2〈ã 〈t〉,b̃ 〈t〉〉 + e−2〈ã 〈t〉,b̃\n〈t〉〉 .\nHence, we have\nsgn(1− 2p〈t+1〉) = sgn(〈ã〈t〉, b̃〈t〉〉).\nThis completes the proof of this lemma.\nB.2 Proof of Lemma 5\nProof. We use induction to prove for claim (i) and the proof for claim (ii) is similar. Clearly (i) holds for t = 0. If (i) holds for t, then for t+ 1, note that ∀a, b ∈ Rd\nEwd(Y − a, b)Y = Ewd(Y + a, b)Y , Ewd(Y − a, b) = 1− Ewd(Y + a, b), Ewd(Y − a, b)Y = −Ewd(Y − a,−b)Y , Ewd(Y − a, b) = 1− Ewd(Y − a,−b).\nHence by definition of a〈t〉 and b〈t〉, it is straight forward to see\na 〈t+1〉 (1) = −a 〈t+1〉 (2) , b 〈t+1〉 (1) = b 〈t+1〉 (2) .\nHence claim (i) holds for t+ 1. By induction, we know claim (i) holds for all t ≥ 0.\nB.3 Proof of Lemma 13\nAccording to the definition of P (xa, xb, xθ) presented in Appendix A.4 we have\nP (xa, xb, xθ) = ∫ w(y − xa, xb)φ+(y, xθ) dy\n= ∫ w(y, xb)φ +(y + xa, xθ) dy\n= ∫ y≥0 φ+(y + xa, xθ) dy + ∫ y≥0 w(−y, xb)(φ+(y − xa, xθ)− φ+(y + xa, xθ)) dy.\n(121)\nwhere the last equality used the fact that w(y, xb) + w(−y, xb) = 1. If xa ≥ xθ ≥ 0, then\n2(φ+(y − xa, xθ)− φ+(y + xa, xθ)) = φ(y − xa + xθ) + φ(y − xa − xθ)− φ(y + xa + xθ) + φ(y + xa − xθ) = φ(y − xa + xθ)− φ(y + xa − xθ) + φ(y − xa − xθ)− φ(y + xa + xθ) ≥ 0, ∀y ≥ 0.\nHence with (121), the above equation implies that if xa ≥ xθ ≥ 0, we have P (xa, xb, xθ) ≥ ∫ y≥0 φ+(y + xa, xθ) dy\n= 1\n2 (1− Φ(xa − xθ)) +\n1 2 (1− Φ(xa + xθ)). (122)\nThis completes the proof of the first part of Lemma 13. Now we discuss the second part, i.e., the case xa < xθ. First note that P is a decreasing function of xa, since xb ≥ 0 and\n∂P (xa, xb, xθ)\n∂xa = −\n∫ 2xb\n(eyxb−xaxb + e−yxb+xaxb)2 φ+(y, xθ) dy ≤ 0.\nTherefore, we have\nP (xa, xb, xθ) ≥ P (xθ, xb, xθ) ≥ 1\n4 +\n1 2 (1− Φ(2xθ))\nwhere the last inequality holds because xa = xθ satisfies the condition of (122). Hence, it immediately gives us that if xa < xθ, then\nP1(xa, xb, xθ) ≥ 1\n4 .\nThis completes the proof.\nB.4 Proof of Lemma 12\nWe warn the reader that in this proof we use the proof of Lemma 13, presented in the last section. According to the definition of Γ presented in Appendix A.4, we have\nΓ(xa, xb, xθ) = ∫ w(y − xa, xb)yφ+(y, xθ) dy\n= xaP (xa, xb, xθ) + ∫ w(y − xa, xb)(y − xa)φ+(y, xθ) dy\n= xaP (xa, xb, xθ) + ∫ w(y, xb)yφ +(y + xa, xθ) dy\n< xaP (xa, xb, xθ) + ∫ y≥0 yφ+(y + xa, xθ) dy = xaP (xa, xb, xθ) + 1\n2 ∫ y≥0 {(y + xa − xθ)φ(y + xa − xθ) + (y + xa + xθ)φ(y + xa + xθ)}dy\n−1 2\n{ (xa − xθ) ∫ y≥0 φ(y + xa − xθ) dy − (xa + xθ) ∫ y≥0 φ(y + xa + xθ) dy } = xaP (xa, xb, xθ) + 1\n2\n{ φ(xθ − xa)− (xa − xθ)(1− Φ(xa − xθ)) } + 1\n2\n{ φ(xθ + xa)− (xa + xθ)(1− Φ(xa + xθ)) } = xaP (xa, xb, xθ) + 1\n2 (W (xa + xθ) +W (xa − xθ)), (123)\nwhere W (x) = φ(x)− x(1− Φ(x)). Therefore we should find an upper bound for W (x). Towards this goal we use the following lemma:\nLemma 25. Let φ(x),Φ(x) denote the pdf and CDF of standard Gaussian respectively. Then we have\nφ(x)\n1− Φ(x) < x+\n√ 2\nπ , ∀x > 0.\nThe proof of this Lemma is presented in Appendix B.5. Therefore from this lemma, we have\nW (x) <\n√ 2\nπ (1− Φ(x)).\nHence we can upper bound (123) by the following inequality:\nΓ(xa, xb, xθ) ≤ xaP (xa, xb, xθ) + √ 2\nπ {1 2 (1− Φ(xa + xθ)) + 1 2 (1− Φ(xa − xθ))}.\nFrom Lemma 13, we have\nP (xa, xb, xθ) ≥ 1\n2 (1− Φ(xa − xθ)) +\n1 2 (1− Φ(xa + xθ)), ∀xa ≥ xθ.\nTherefore, if xa ≥ xθ, then we have Γ(xa, xb, xθ) ≤ xaP (xa, xb, xθ) + √ 2\nπ {1 2 (1− Φ(xa + xθ)) + 1 2 (1− Φ(xa − xθ))}\n≤ xaP (xa, xb, xθ) + √ 2\nπ P (xa, xb, xθ),\n(124)\nwhich completes the proof.\nB.5 Proof of Lemma 25\nIt is equivalent to show that\nr(x) , (x+\n√ 2\nπ )(1− Φ(x))− φ(x) > 0, ∀x > 0.\nTaking the first derivative of the left hand side, we have\ndr(x)\ndx = 1− Φ(x)− φ(x)(x+\n√ 2\nπ ) + xφ(x) = 1− Φ(x)−\n√ 2\nπ φ(x).\nTaking the second derivative, we have\nd2 r(x)\ndx2 = −φ(x) +\n√ 2\nπ xφ(x) = (\n√ 2\nπ x− 1)φ(x).\nHence, we have r′′(x) < 0 if x < √ π/2 and r′′(x) > 0 if x > √ π/2. Therefore, r′(x) is first strictly decreasing then strictly increasing function of x for x ≥ 0. Since r′(0) = 1/2 − 1/π > 0, r′( √ π/2) = −0.04008391 and\nlim x→∞ r′(x) = lim x→∞\n1− Φ(x)− √ 2\nπ φ(x) = 0,\nwe know there exists x0 ∈ (0, √ π/2) such that r′(x) > 0 if x < x0 and r′(x) < 0 if x > x0. Hence, r(x) is first strictly increasing and then strictly decreasing function of x ≥ 0. Since r(x) = 0 and\n| lim x→∞ r(x)| ≤ lim x→∞ (x+\n√ 2\nπ )(1− Φ(x)) + φ(x)\n≤ 2 lim x→∞ ∫ +∞ y=x xφ(y) dy\n≤ 2 lim x→∞ ∫ +∞ y=x yφ(y) dy = 2 lim x→∞ φ(x) = 0.\nHence, we have r(x) > 0, ∀x > 0. This completes the proof of this Lemma.\nB.6 Proof of Lemma 14\nWe first calculate the derivative ∂Γ(xa,xb,xθ)∂xb at zero:\n∂Γ(xa, xb, xθ)\n∂xb |xb=0 =\n∫ 2(y − xa)\n(eyxb−xaxb + e−yxb+xaxb)2 yφ+(y, xθ) dy|xb=0\n= 1\n2\n∫ y2 − xay\n2 (φ(y − xθ) + φ(y + xθ)) dy\n= 1\n2 (1 + x2θ).\nThis derivative is clearly larger than 0.5. Now we prove the main result by contradiction. Suppose that the claim of the lemma is not correct. Then, for any fixed {cU,1, |θ?〈0〉,1|, ‖θ\n?‖}, ∀δ > 0, we have aδ ∈ [0, cU,1], bδ ∈ [0, δ], θδ ∈ [θ?〈0〉,1, ‖θ ?‖] such that\n∂Γ(xa, xb, xθ)\n∂xb |xa=aδ,xb=bδ,xθ=θδ <\n1 2 .\nTherefore, for any sequence {δi} such that δi → 0, we have\naδi ∈ [0, cU,1], bδi ∈ [0, δi], θδi ∈ [θ ? 〈0〉,1, ‖θ ?‖].\nSince the sequence {aδi , bδi , θδi}∞i=1, belong to a compact set, there exists a subsequence δij such that {(aδij , bδij , θδij )} converges to a limit (a ∞, b∞, θ∞) satisfying\na∞ ∈ [0, cU,1], b∞ ∈ [0, lim j→∞ δij = 0], θ ∞ ∈ [θ?〈0〉,1, ‖θ ?‖].\nBy continuity of ∂Γ(xa,xb,xθ)∂xb , we have\n1 2 ≥ lim j→∞ ∂Γ(xa, xb, xθ) ∂xb |xa=aδij ,xb=bδij ,xθ=θδij\n= ∂Γ(xa, xb, xθ)\n∂xb |xa=a∞,xb=0,xθ=θ∞\n= 1\n2 (1 + (θ∞)2) >\n1 2 .\nThis contradiction proves that Lemma 14 is correct.\nB.7 Proof of Lemma 15\nFirs note that if x = 0, then\nK(0, xb) =\n∫ eyxb − e−yxb\n2(eyxb + e−yxb) 1√ 2π e−y 2/2 dy,\nwhich is the integral of an odd function and is hence equal to zero. To prove that the function is increasing and concave for x ≥ 0, we calculate its derivatives. It is straightforward to see that\n∂K(x, xb)\n∂x =\n∫ eyxb − e−yxb\n2(eyxb + e−yxb) (y − x)φ(y − x) dy = − ∫\neyxb − e−yxb 2(eyxb + e−yxb) dφ(y − x)\n= ∫ φ(y − x) 2xb\n(eyxb + e−yxb)2 dy > 0,\nwhere the last equality is the result of integration by parts. Similarly, ∀x ≥ 0\n∂2K(x, xb)\n∂x2 =\n∫ (y − x)φ(y − x) 2xb\n(eyxb + e−yxb)2 dy = − ∫\n2xb (eyxb + e−yxb)2 dφ(y − x)\n(a) = − ∫ φ(y − x) 4x2b(e yxb − e−yxb)\n(eyxb + e−yxb)3 dy\n= ∫ ∞ 0 (φ(y + x)− φ(y − x)) 4x2b(e yxb − e−yxb) (eyxb + e−yxb)3 dy < 0,\nwhere equality (a) is an application of integration by parts.\nB.8 Proof of Lemma 16\nRecall the definition of l(x) in Appendix A.5.3:\nl(x) = x(1− 2Φ(−x)) + 2φ(x).\nDefine\nJ(x) = 1\n2 (x− l(x)(1− 2Φ(−x))) = 2φ(x)Φ(−x) + 2xΦ(−x)− φ(x)− 2xΦ(−x)2.\nWe would like to show that J(x) ≥ 0. Hence, we analyze the shape of the function J(x) by taking the derivatives, for all x > 0\ndJ(x)\ndx = −2φ(x)2 + 2Φ(−x)− xφ(x)− 2Φ(−x)2\nd2 J(x)\ndx2 = φ(x)(4φ(x)x− 3 + x2 + 4Φ(−x))\ndJ ′′(x)/φ(x)\ndx = 2x(1− 2xφ(x)) ≥ 2x(1−\n√ 2\nπ ) > 0.\nTherefore J ′′(x)/φ(x) is an strictly increasing function of x. With J ′′(0) < 0 and J ′′(10) > 0, we have J ′(x) is first strictly decreasing then strictly increasing function of x. Since J ′(0) = 1/2− 1/π > 0\nand limx→∞ J ′(x) = 0, we know J(x) achieves its minimum at either 0 or ∞. Since J(0) = 0 and limx→∞ J(x) = 0, we have J(x) > 0, ∀x > 0. This completes the proof of this lemma.\nB.9 Proof of Lemma 19\nAccording to the definition of functions F (xb, xθ), Q(xa, xb, θ) and (37) in Appendix A.4 we have\nF (xb, xθ) = 2Γ(0, xb, xθ)\n= ∫ e(y+xθ)xb − e−(y+xθ)xb e(y+xθ)xb + e−(y+xθ)xb (y + xθ)φ(y) dy. (125)\nTo prove the concavity of this function we show that\n∂2F (xb, xθ)\n∂x2b ≤ 0.\nWe have ∂F (xb, xθ)\n∂xb =\n∫ 4\n(e(y+xθ)xb + e−(y+xθ)xb)2 (y + xθ)\n2φ(y) dy ≥ 0,\n∂2F (xb, xθ)\n∂x2b = ∫ −8(e(y+xθ)xb − e−(y+xθ)xb) (e(y+xθ)xb + e−(y+xθ)xb)3 (y + xθ) 3φ(y) dy\n= − ∫\n8(eyxb − e−yxb) (eyxb + e−yxb)3 y3φ(y − xθ) dy ≤ 0.\nTherefore F is increasing and strictly concave in xb > 0. Now we only need to calculate F (0, xθ) and F (xθ, xθ). From (125), we have\nF (0, xθ) =\n∫ e0 − e−0\ne0 + e0 (y + xθ)φ(y) dy = 0.\nUsing definition of F , we have\nF (xθ, xθ) = 2Q(0, xθ, xθ)\n= 2\n∫ eyxθ\neyxθ + e−yxθ y\n1 2 √ 2π (e−(y−xθ) 2/2 + e−(y+xθ) 2/2) dy\n= ∫ eyxθy\n1√ 2π e−(y 2+x2θ)/2 dy\n= xθ\nThis completes the proof of this lemma.\nB.10 Proof of Lemma 18\nAccording to the definition of function F , we have\n∂F (xb, xθ)− xθ ∂xb\n|xb=xθ = ∫\n4y2\n(eyxb + e−yxb)2 1√ 2π e−(y−xθ) 2/2 dy|xb=xθ\n=\n∫ 2y2\neyxθ + e−yxθ 1√ 2π e−(y 2+x2θ)/2 dy\n≤ e− x2θ 2 .\nWe claim there exists δ > 0 is a function of only Lb, Ub, Lθ, ‖θ?‖ such that ∀|xb − xθ| ∈ [0, δ], xb ∈ [Lb, Ub], xθ ∈ [Lθ, ‖θ?‖],\n∂F (xb, xθ)− xθ ∂xb\n≤ 1 + e −L 2 θ 2\n2 . (126)\nWe prove it by contradiction. If not, for all δ > 0, we have bδ ∈ [Lb, Ub], θδ ∈ [Lθ, ‖θ?‖], |bδ−θδ| ∈ [0, δ] such that\n∂F (xb, xθ)− xθ ∂xb |xb=bδ,xθ=θδ > 1 + e−\nL2θ 2\n2 .\nFor any sequence {δi} such that δi → 0, there exists subsequence δij such that {(bδij , θδij )} converge to the limits (b∞, θ∞). By compactness of the choice of xb, xθ, we have\nb∞ ∈ [Lb, Ub], θ∞ ∈ [Lθ, ‖θ?‖], |b∞ − θ∞| ∈ [0, lim j→∞ δij = 0].\nBy continuity of ∂F (xb,xθ)−xθ∂xb , we have\n1 + e− L2θ 2\n2 ≤ lim j→∞ ∂F (xb, xθ)− xθ ∂xb |xb=bδij ,xθ=θδij\n= ∂F (xb, xθ)− xθ\n∂xb |xb=xθ=θ∞\n= e− (θ∞)2 2 < 1 + e−\nL2θ 2\n2 .\nContradiction! Hence we have Eq.(126) holds and ∀|xb − xθ| ∈ [0, δ], xb ∈ [Lb, Ub], xθ ∈ [Lθ, ‖θ?‖],\n|F (xb, xθ)− xθ| ≤ |F (xθ, xθ)|+ | 1 + e−\nL2θ 2\n2 (xb − xθ)| =\n1 + e− L2θ 2\n2 |xb − xθ|.\nFrom Lemma 19, we have\n|F (xb, xθ)− xθ| < |xb − xθ|, ∀|xb − xθ| /∈ [0, δ), xb ∈ [Lb, Ub], xθ ∈ [Lθ, ‖θ?‖].\nLet\nκ′′b = max{ 1 + e−\nL2θ 2\n2 , sup |xb−xθ|/∈[0,δ),xb∈[Lb,Ub],xθ∈[Lθ,‖θ?‖] |F (xb, xθ)− xθ| |xb − xθ| },\nby continuity of the function |F (xb,xθ)−xθ||xb−xθ| , we have κ ′′ b ∈ (0, 1) is a function of only Lb, Ub, Lθ, ‖θ ?‖ and\n|F (xb, xθ)− xθ| ≤ κ′′b |xb − xθ|, ∀xb ∈ [Lb, Ub], xθ ∈ [Lθ, ‖θ?‖].\nThis completes the proof of this lemma.\nB.11 Cluster Points of Population EM\nLemma 26. Any clustering point (a, b) of the estimates of the Population EM {(a〈t〉, b〈t〉)}t satisfy the following equations:\na = γ(1− 2p) 2p(1− p) , b = γ\n2p(1− p) ,\nγ = Ewd(Y − a, b)Y , p = Ewd(Y − a, b),\nwhere Y ∼ 0.5N(−θ?, I) + 0.5N(θ?, I). Proof. Here is a summary of our strategy to prove this result. We first prove that ‖a〈t+1〉−a〈t〉‖ → 0 and ‖b〈t+1〉 − b〈t〉‖ → 0 as t → ∞. Then we use the following simple argument to prove that in fact the clustering points must satisfy the above fixed point equations. Suppose that (a, b) is an accumulation point. Then there is a subsequence {(a〈ti〉, b〈ti〉)}∞i=1 that converges to (a, b). Since we have ‖a〈t+1〉 − a〈t〉‖ → 0 and ‖b〈t+1〉 − b〈t〉‖ → 0, we can simply argue that {(a〈ti+1〉, b〈ti+1〉)}∞i=1 also converges to (a, b). We know that\na〈ti+1〉 = γ〈ti〉(1− 2p〈ti〉) 2p〈ti〉(1− p〈ti〉) , b〈ti+1〉 = γ〈ti〉\n2p〈ti〉(1− p〈ti〉) ,\nγ〈ti+1〉 = Ewd(Y − a〈t〉, b〈t〉)Y p〈ti+1〉 = Ewd(Y − a〈t〉, b〈t〉).\nBy taking the limit i→∞ from both sides of the above equations we obtain the fixed point equations. Hence, the rest of the section is devoted to the proof of ‖a〈t+1〉 − a〈t〉‖ → 0 and ‖b〈t+1〉 − b〈t〉‖ → 0. The technique we us to prove this claim was first developed in [2]. Since a〈t〉 = (µ〈t〉1 + µ 〈t〉 2 )/2 and b〈t〉 = (µ 〈t〉 2 − µ 〈t〉 1 )/2, we only need to prove that ‖µ 〈t+1〉 1 − µ 〈t〉 1 ‖ → 0 and ‖µ 〈t+1〉 2 − µ 〈t〉 2 ‖ → 0.\nDefine the following notion of distance between two parameter vectors: D(η,ν) = −Ef(z|Y ;ν) ∑ z ln ( f(z|Y ;η) f(z|Y ;ν) ),\nwhere f(·) indicates corresponding pdf. Let µ〈t〉 is a shorthand for (µ〈t〉1 ,µ 〈t〉 2 ). As the first step of our proof we would like to show that D(µ〈t+1〉,µ〈t〉)→ 0. From (3), we have\nQf (η|ν) = E ∑ z f(z|Y ;ν) ln (f(z,Y ;η)) = E ln (f(Y ;η)) + E ∑ z f(z|Y ;ν) ln (f(z|Y ;η))\n= E ln (f(Y ;η))−D(η,ν) +H(ν,ν) = L(η)−D(η,ν) +H(ν,ν),\nwhere\nL(η) , E ln (f(Y |η)) = E ln (1 2 φd(Y − η1) + 1 2 φd(Y − η2))\n≤ −d 2 ln 2π, (127)\nH(η,ν) , E ∑ z f(z|y;ν) ln (f(z|y;η)). (128)\nHence, µ〈t+1〉 = argmaxµ′Qf (µ ′|µ〈t〉) = argmaxµ′{L(µ′)−D(µ′,µ〈t〉)}.\nNote that every estimate of Population EM is obtained in a trade-off between maximizing the expected log-likelihood and minimizing the distance between the two consecutive estimates. First note that\nL(µ〈t+1〉)−D(µ〈t+1〉,µ〈t〉) ≥ L(µ〈t〉)−D(µ〈t〉,µ〈t〉) = L(µ〈t〉) (129)\nHence, L(µ〈t+1〉) ≥ L(µ〈t〉) + D(µ〈t+1〉,µ〈t〉). Therefore, {L(µ〈t〉)} is a non-decreasing sequence. Since according to (127), L(µ) is upper bounded, thus {L(µ〈t〉)}t converges. Also, according to (129) we have\n0 ≤ D(µ〈t+1〉,µ〈t〉) ≤ L(µ〈t+1〉)− L(µ〈t〉)→ 0, as t→∞.\nThis implies that {D(µ〈t+1〉,µ〈t〉)} → 0, as t→∞.\nNote that D(·, ·) is a measure of discrepancy between its two arguments. However, our goal is to show that the Euclidean distance between µ〈t〉 and µ〈t+1〉 goes to zero. The rest of the proof is devoted to this claim. Since,\nµ〈t〉 = Ef(z|Y ;µ〈t−1〉)Y Ef(z|Y ;µ〈t−1〉) ,\nµ〈t+1〉 = Ef(z|Y ;µ〈t〉)Y Ef(z|Y ;µ〈t〉) ,\nin order to prove ‖µ〈t+1〉 − µ〈t〉‖ → 0, we should show that ∀z ∈ {1, 0}\n‖Ef(z|Y ;µ〈t+1〉)Y − Ef(z|Y ;µ〈t〉)Y ‖ → 0,\nand |Ef(z|Y ;µ〈t+1〉)− Ef(z|Y ;µ〈t〉)| → 0.\nIf we define ψ(x) = − lnx+ x− 1, then\nD(η,ν) = E ∑ z ψ( f(z|Y ;η) f(z|Y ;ν) )f(z|Y ;ν), (130)\nSince ψ(x) > 0 for every value of x > 0, the fact that D(µ〈t+1〉,µ〈t〉)→ 0 implies that ∀z ∈ {1, 0}\nEψ( f(z|Y ;µ〈t+1〉) f(z|Y ;µ〈t〉) )f(z|Y ;µ〈t〉) → 0, as t→∞. (131)\nHence, for all z ∈ {1, 0}\nEψ( f(z|Y ;µ〈t+1〉) f(z|Y ;µ〈t〉) )f(z|Y ;µ〈t〉)\n= E { (f(z|Y ;µ〈t+1〉)− f(z|Y ;µ〈t〉))− ( ln f(z|Y ;µ〈t+1〉)− ln f(z|Y ;µ〈t〉) ) f(z|Y ;µ〈t〉) } (i) = E\nf(z|Y ;µ〈t〉) 2ξ2 (f(z|Y ;µ〈t+1〉)− f(z|Y ;µ〈t〉))2\n(ii) ≥ Ef(z|Y ;µ〈t〉)(f(z|Y ;µ〈t+1〉)− f(z|Y ;µ〈t〉))2 ≥ Ef(z|Y ;µ〈t〉)(f(z|Y ;µ〈t+1〉)− f(z|Y ;µ〈t〉))2I(‖Y ‖ < M), ∀t,M > 0.\nwhere Equality (i) is the result of the Taylor expansion on lnX and ξ is a number between f(z|Y ;µ〈t+1〉) and f(z|Y ;µ〈t〉) and Inequality (ii) holds for the fact that\nf(z|Y ;µ〈t+1〉), f(z|Y ;µ〈t〉) ∈ (0, 1), ∀z ∈ {1, 0}.\nHence, with (131), we have\nEf(z|Y ;µ〈t〉)(f(z|Y ;µ〈t+1〉)− f(z|Y ;µ〈t〉))2I(‖Y ‖ < M)→ 0, as t→∞, ∀M > 0, z ∈ {1, 0}.\nAccording to Lemma 9 {(a〈tn〉, b〈tn〉)}∞n=1 is in a compact set and hence so is {µ〈t〉}. Since f(z|y;µ〈t〉) is a continuous function of y and µ〈t〉 with f(z|y;µ〈t〉) > 0 and compactness of {µ〈t〉}, there exists a constant c only depending on M such that\nf(z|y;µ〈t〉) > c, ∀‖y‖ < M, z ∈ {1, 0}, t > 0.\nTherefore, for all M > 0, z ∈ {1, 0}, we have\nE(f(z|Y ;µ〈t+1〉)− f(z|Y ;µ〈t〉))2I(‖Y ‖ < M)→ 0, as t→∞. (132)\nAlso, for all t ≥ 0, z ∈ {1, 0}, we have\nE(f(z|Y ;µ〈t+1〉)− f(z|Y ;µ〈t〉))2I(‖y‖ ≥M) ≤ EI(‖Y ‖ ≥M)→ 0, as M →∞. (133)\nWith (132) and (133), we have\nE(f(z|Y ;µ〈t+1〉)− f(z|Y ;µ〈t〉))2 → 0, as t→∞, ∀z ∈ {1, 0}.\nTherefore for all z ∈ {1, 0}, as t→∞, we have,\n‖E(f(z|Y ;µ〈t+1〉)− f(z|Y ;µ〈t〉))Y ‖ ≤ √ E(f(z|Y ;µ〈t+1〉)− f(z|Y ;µ〈t〉))2E‖Y ‖2 → 0,\n|Ef(z|Y ;µ〈t+1〉)− f(z|Y ;µ〈t〉)| ≤ √ E(f(z|Y ;µ〈t+1〉)− f(z|Y ;µ〈t〉))2 → 0.\nHence with compactness on sequence {µ〈t〉}, we have\n‖µ〈t+2〉1 − µ 〈t+1〉 1 ‖ = ∥∥∥∥∥Ef(z = 0|Y ;µ〈t+1〉)YEf(z = 0|Y ;µ〈t+1〉) − Ef(z = 0|Y ;µ〈t〉)YEf(z = 0|Y ;µ〈t〉) ∥∥∥∥∥→ 0, as t→∞,\nand\n‖µ〈t+2〉2 − µ 〈t+1〉 2 ‖ = ∥∥∥∥∥Ef(z = 1|Y ;µ〈t+1〉)YEf(z = 1|Y ;µ〈t+1〉) − Ef(z = 1|Y ;µ〈t〉)YEf(z = 1|Y ;µ〈t〉) ∥∥∥∥∥→ 0, as t→∞.\nThis completes the proof of this lemma."
    } ],
    "references" : [ {
      "title" : "On spectral learning of mixtures of distributions",
      "author" : [ "D. Achlioptas", "F. McSherry" ],
      "venue" : "In Eighteenth Annual Conference on Learning Theory,",
      "citeRegEx" : "Achlioptas and McSherry.,? \\Q2005\\E",
      "shortCiteRegEx" : "Achlioptas and McSherry.",
      "year" : 2005
    }, {
      "title" : "Learning mixtures of separated nonspherical Gaussians",
      "author" : [ "S. Arora", "R. Kannan" ],
      "venue" : "The Annals of Applied Probability,",
      "citeRegEx" : "Arora and Kannan.,? \\Q2005\\E",
      "shortCiteRegEx" : "Arora and Kannan.",
      "year" : 2005
    }, {
      "title" : "Statistical guarantees for the EM algorithm: From population to sample-based analysis",
      "author" : [ "S. Balakrishnan", "M.J. Wainwright", "B. Yu" ],
      "venue" : null,
      "citeRegEx" : "Balakrishnan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Balakrishnan et al\\.",
      "year" : 2014
    }, {
      "title" : "Statistical mechanics of the maximum-likelihood density estimation",
      "author" : [ "N. Barkai", "H. Sompolinsky" ],
      "venue" : "Physical Review E,",
      "citeRegEx" : "Barkai and Sompolinsky.,? \\Q1994\\E",
      "shortCiteRegEx" : "Barkai and Sompolinsky.",
      "year" : 1994
    }, {
      "title" : "Polynomial learning of distribution families",
      "author" : [ "M. Belkin", "K. Sinha" ],
      "venue" : "In Fifty-First Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Belkin and Sinha.,? \\Q2010\\E",
      "shortCiteRegEx" : "Belkin and Sinha.",
      "year" : 2010
    }, {
      "title" : "Isotropic PCA and affine-invariant clustering",
      "author" : [ "S.C. Brubaker", "S. Vempala" ],
      "venue" : "In Forty-Ninth Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Brubaker and Vempala.,? \\Q2008\\E",
      "shortCiteRegEx" : "Brubaker and Vempala.",
      "year" : 2008
    }, {
      "title" : "Learning mixtures of product distributions using correlations and independence",
      "author" : [ "K. Chaudhuri", "S. Rao" ],
      "venue" : "In Twenty-First Annual Conference on Learning Theory,",
      "citeRegEx" : "Chaudhuri and Rao.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chaudhuri and Rao.",
      "year" : 2008
    }, {
      "title" : "Multi-view clustering via canonical correlation analysis",
      "author" : [ "K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning mixtures of gaussians using the k-means algorithm",
      "author" : [ "K. Chaudhuri", "S. Dasgupta", "A. Vattani" ],
      "venue" : "CoRR, abs/0912.0086,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2009
    }, {
      "title" : "On EM algorithms and their proximal generalizations",
      "author" : [ "S. Chrétien", "A.O. Hero" ],
      "venue" : "ESAIM: Probability and Statistics,",
      "citeRegEx" : "Chrétien and Hero.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chrétien and Hero.",
      "year" : 2008
    }, {
      "title" : "Expected maximum log likelihood estimation",
      "author" : [ "D. Conniffe" ],
      "venue" : "Journal of the Royal Statistical Society. Series D,",
      "citeRegEx" : "Conniffe.,? \\Q1987\\E",
      "shortCiteRegEx" : "Conniffe.",
      "year" : 1987
    }, {
      "title" : "Learning mixutres of Gaussians",
      "author" : [ "S. Dasgupta" ],
      "venue" : "In Fortieth Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Dasgupta.,? \\Q1999\\E",
      "shortCiteRegEx" : "Dasgupta.",
      "year" : 1999
    }, {
      "title" : "A probabilistic analysis of EM for mixtures of separated, spherical Gaussians",
      "author" : [ "S. Dasgupta", "L. Schulman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Dasgupta and Schulman.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dasgupta and Schulman.",
      "year" : 2007
    }, {
      "title" : "Maximum-likelihood from incomplete data via the EM algorithm",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.B. Rubin" ],
      "venue" : "J. Royal Statist. Soc. Ser. B,",
      "citeRegEx" : "Dempster et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "Dempster et al\\.",
      "year" : 1977
    }, {
      "title" : "On the mathematical foundations of theoretical statistics",
      "author" : [ "R.A. Fisher" ],
      "venue" : "Philosophical Transactions of the Royal Society,",
      "citeRegEx" : "Fisher.,? \\Q1922\\E",
      "shortCiteRegEx" : "Fisher.",
      "year" : 1922
    }, {
      "title" : "Tight bounds for learning a mixture of two gaussians",
      "author" : [ "M. Hardt", "E. Price" ],
      "venue" : "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,",
      "citeRegEx" : "Hardt and Price.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hardt and Price.",
      "year" : 2015
    }, {
      "title" : "Learning mixtures of spherical Gaussians: moment methods and spectral decompositions",
      "author" : [ "D. Hsu", "S.M. Kakade" ],
      "venue" : "In Fourth Innovations in Theoretical Computer Science,",
      "citeRegEx" : "Hsu and Kakade.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hsu and Kakade.",
      "year" : 2013
    }, {
      "title" : "Efficiently learning mixtures of two Gaussians",
      "author" : [ "A.T. Kalai", "A. Moitra", "G. Valiant" ],
      "venue" : "In Forty-second ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Kalai et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kalai et al\\.",
      "year" : 2010
    }, {
      "title" : "The spectral method for general mixture models",
      "author" : [ "R. Kannan", "H. Salmasian", "S. Vempala" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Kannan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kannan et al\\.",
      "year" : 2008
    }, {
      "title" : "Oracle inequalities in empirical risk minimization and sparse recovery problems",
      "author" : [ "V. Koltchinskii" ],
      "venue" : "In École d′été de probabilités de Saint-Flour XXXVIII,",
      "citeRegEx" : "Koltchinskii.,? \\Q2011\\E",
      "shortCiteRegEx" : "Koltchinskii.",
      "year" : 2011
    }, {
      "title" : "Least squares quantization in PCM",
      "author" : [ "S.P. Lloyd" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "Lloyd.,? \\Q1982\\E",
      "shortCiteRegEx" : "Lloyd.",
      "year" : 1982
    }, {
      "title" : "Some methods for classification and analysis of multivariate observations",
      "author" : [ "J.B. MacQueen" ],
      "venue" : "In Proceedings of the fifth Berkeley Symposium on Mathematical Statistics and Probability,",
      "citeRegEx" : "MacQueen.,? \\Q1967\\E",
      "shortCiteRegEx" : "MacQueen.",
      "year" : 1967
    }, {
      "title" : "Settling the polynomial learnability of mixtures of Gaussians",
      "author" : [ "A. Moitra", "G. Valiant" ],
      "venue" : "In Fifty-First Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Moitra and Valiant.,? \\Q2010\\E",
      "shortCiteRegEx" : "Moitra and Valiant.",
      "year" : 2010
    }, {
      "title" : "Mixture densities, maximum likelihood and the EM algorithm",
      "author" : [ "R.A. Redner", "H.F. Walker" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Redner and Walker.,? \\Q1984\\E",
      "shortCiteRegEx" : "Redner and Walker.",
      "year" : 1984
    }, {
      "title" : "An analysis of the EM algorithm and entropy-like proximal point methods",
      "author" : [ "P. Tseng" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Tseng.,? \\Q2004\\E",
      "shortCiteRegEx" : "Tseng.",
      "year" : 2004
    }, {
      "title" : "A spectral algorithm for learning mixtures models",
      "author" : [ "S. Vempala", "G. Wang" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Vempala and Wang.,? \\Q2004\\E",
      "shortCiteRegEx" : "Vempala and Wang.",
      "year" : 2004
    }, {
      "title" : "On the convergence properties of the EM algorithm",
      "author" : [ "C.F.J. Wu" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Wu.,? \\Q1983\\E",
      "shortCiteRegEx" : "Wu.",
      "year" : 1983
    }, {
      "title" : "On convergence properties of the EM algorithm for Gaussian mixtures",
      "author" : [ "L. Xu", "M.I. Jordan" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Xu and Jordan.,? \\Q1996\\E",
      "shortCiteRegEx" : "Xu and Jordan.",
      "year" : 1996
    }, {
      "title" : "δ. For the last claim, we borrow a technique in the proof of corollary 2 in B.2",
      "author" : [ "Balakrishnan" ],
      "venue" : null,
      "citeRegEx" : "Balakrishnan,? \\Q2014\\E",
      "shortCiteRegEx" : "Balakrishnan",
      "year" : 2014
    }, {
      "title" : "ξi〈yi − xa,xb〉〈yi,uj〉 is symmetric in terms of xb and constraints of xb is symmetric. The correctness of the last inequality is shown in B.2 of Balakrishnan et al",
      "author" : [ "Balakrishnan" ],
      "venue" : "EY ,ξe",
      "citeRegEx" : "Balakrishnan,? \\Q2014\\E",
      "shortCiteRegEx" : "Balakrishnan",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "1 Introduction Since Fisher’s 1922 paper (Fisher, 1922), maximum likelihood estimators (MLE) have become one of the most popular tools in many areas of science and engineering.",
      "startOffset" : 41,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "1 Expectation Maximization Among the algorithms mentioned above, Expectation Maximization (EM) has attracted more attention for the simplicity of its iterations, and its good performance in practice (Dempster et al., 1977; Redner and Walker, 1984).",
      "startOffset" : 199,
      "endOffset" : 247
    }, {
      "referenceID" : 23,
      "context" : "1 Expectation Maximization Among the algorithms mentioned above, Expectation Maximization (EM) has attracted more attention for the simplicity of its iterations, and its good performance in practice (Dempster et al., 1977; Redner and Walker, 1984).",
      "startOffset" : 199,
      "endOffset" : 247
    }, {
      "referenceID" : 26,
      "context" : "Although EM is billed as a procedure for maximum likelihood estimation, it is known that with certain initializations, the final parameters returned by EM may be far from the MLE, both in parameter distance and in log-likelihood value (Wu, 1983).",
      "startOffset" : 235,
      "endOffset" : 245
    }, {
      "referenceID" : 26,
      "context" : "Several works characterize local convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Chrétien and Hero, 2008).",
      "startOffset" : 140,
      "endOffset" : 188
    }, {
      "referenceID" : 24,
      "context" : "Several works characterize local convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Chrétien and Hero, 2008).",
      "startOffset" : 140,
      "endOffset" : 188
    }, {
      "referenceID" : 9,
      "context" : "Several works characterize local convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Chrétien and Hero, 2008).",
      "startOffset" : 140,
      "endOffset" : 188
    }, {
      "referenceID" : 11,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either dα or kβ for some α, β > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 224,
      "endOffset" : 447
    }, {
      "referenceID" : 1,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either dα or kβ for some α, β > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 224,
      "endOffset" : 447
    }, {
      "referenceID" : 12,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either dα or kβ for some α, β > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 224,
      "endOffset" : 447
    }, {
      "referenceID" : 25,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either dα or kβ for some α, β > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 224,
      "endOffset" : 447
    }, {
      "referenceID" : 18,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either dα or kβ for some α, β > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 224,
      "endOffset" : 447
    }, {
      "referenceID" : 0,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either dα or kβ for some α, β > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 224,
      "endOffset" : 447
    }, {
      "referenceID" : 6,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either dα or kβ for some α, β > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 224,
      "endOffset" : 447
    }, {
      "referenceID" : 5,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either dα or kβ for some α, β > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 224,
      "endOffset" : 447
    }, {
      "referenceID" : 17,
      "context" : "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).",
      "startOffset" : 174,
      "endOffset" : 289
    }, {
      "referenceID" : 4,
      "context" : "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).",
      "startOffset" : 174,
      "endOffset" : 289
    }, {
      "referenceID" : 22,
      "context" : "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).",
      "startOffset" : 174,
      "endOffset" : 289
    }, {
      "referenceID" : 16,
      "context" : "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).",
      "startOffset" : 174,
      "endOffset" : 289
    }, {
      "referenceID" : 15,
      "context" : "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).",
      "startOffset" : 174,
      "endOffset" : 289
    }, {
      "referenceID" : 21,
      "context" : "(2009b) uses these symmetries to prove that a variant of Lloyd’s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption.",
      "startOffset" : 75,
      "endOffset" : 104
    }, {
      "referenceID" : 20,
      "context" : "(2009b) uses these symmetries to prove that a variant of Lloyd’s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption.",
      "startOffset" : 75,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : "3 Background and Related Work The EM algorithm was formally introduced by Dempster et al. (1977) as a general iterative method for computing parameter estimates from incomplete data.",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "Several works characterize local convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Chrétien and Hero, 2008). However, these analyses do not distinguish between global maximizers and other stationary points (except, e.g., when the likelihood function is unimodal). Thus, as an optimization algorithm for maximizing the log-likelihood objective, the “worst-case” performance of EM is somewhat discouraging. For a more optimistic perspective on EM, one may consider a “best-case” analysis, where (i) the data are an iid sample from a distribution in the given model, (ii) the sample size is sufficiently large, and (iii) the starting point for EM is sufficiently close to the parameters of the data generating distribution. Conditions (i) and (ii) are ubiquitous in (asymptotic) statistical analyses, and (iii) is a generous assumption that may be satisfied in certain cases. Redner and Walker (1984) show that in such a favorable scenario, EM converges to the MLE almost surely for a broad class of mixture models.",
      "startOffset" : 164,
      "endOffset" : 979
    }, {
      "referenceID" : 0,
      "context" : "Moreover, recent work of Balakrishnan et al. (2014) gives non-asymptotic convergence guarantees in certain models; importantly, these results permit one to quantify the accuracy of a pilot estimator required to effectively initialize EM.",
      "startOffset" : 25,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "Moreover, recent work of Balakrishnan et al. (2014) gives non-asymptotic convergence guarantees in certain models; importantly, these results permit one to quantify the accuracy of a pilot estimator required to effectively initialize EM. Thus, EM may be used in a tractable two-stage estimation procedures given a first-stage pilot estimator that can be efficiently computed. Indeed, for the special case of Gaussian mixtures, researchers in theoretical computer science and machine learning have developed efficient algorithms that deliver the highly accurate parameter estimates under appropriate conditions. Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either dα or kβ for some α, β > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al.",
      "startOffset" : 25,
      "endOffset" : 678
    }, {
      "referenceID" : 0,
      "context" : ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894).",
      "startOffset" : 8,
      "endOffset" : 447
    }, {
      "referenceID" : 0,
      "context" : ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated.",
      "startOffset" : 8,
      "endOffset" : 640
    }, {
      "referenceID" : 0,
      "context" : ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for well-separated mixtures.",
      "startOffset" : 8,
      "endOffset" : 839
    }, {
      "referenceID" : 0,
      "context" : ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for well-separated mixtures. In a related but different vein, Dasgupta and Schulman (2007) analyze a variant of EM with a particular initialization scheme, and proves fast convergence to the true parameters, again for well-separated mixtures in high-dimensions.",
      "startOffset" : 8,
      "endOffset" : 1016
    }, {
      "referenceID" : 0,
      "context" : ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for well-separated mixtures. In a related but different vein, Dasgupta and Schulman (2007) analyze a variant of EM with a particular initialization scheme, and proves fast convergence to the true parameters, again for well-separated mixtures in high-dimensions. For mixtures of two Gaussians, it is possible to exploit symmetries to get sharper analyses. Indeed, Chaudhuri et al. (2009b) uses these symmetries to prove that a variant of Lloyd’s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption.",
      "startOffset" : 8,
      "endOffset" : 1313
    }, {
      "referenceID" : 0,
      "context" : ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for well-separated mixtures. In a related but different vein, Dasgupta and Schulman (2007) analyze a variant of EM with a particular initialization scheme, and proves fast convergence to the true parameters, again for well-separated mixtures in high-dimensions. For mixtures of two Gaussians, it is possible to exploit symmetries to get sharper analyses. Indeed, Chaudhuri et al. (2009b) uses these symmetries to prove that a variant of Lloyd’s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption. Lastly, for the specific case of our Model 1, Balakrishnan et al. (2014) proves linear convergence of EM (as well as a gradient-based variant of EM) when started in a sufficiently small neighborhood around the true parameters; here, the size of the neighborhood grows with the separation between the two",
      "startOffset" : 8,
      "endOffset" : 1660
    }, {
      "referenceID" : 2,
      "context" : "Furthermore, the fact that η∗ is a global maximizer of Q(η | η∗) is known as the self-consistency property (Balakrishnan et al., 2014).",
      "startOffset" : 107,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "EM (e.g., Dasgupta and Schulman, 2007; Balakrishnan et al., 2014); our results show that they are not really necessary in the large sample limit.",
      "startOffset" : 3,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "This would be consistent with results from statistical physics on the MLE for Gaussian mixtures, which characterize the behavior when dn ∝ n as n→∞ (Barkai and Sompolinsky, 1994).",
      "startOffset" : 148,
      "endOffset" : 178
    }, {
      "referenceID" : 19,
      "context" : "To simplify the final expression even further, we use the following lemma from Koltchinskii (2011) Lemma 24.",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "2 in Balakrishnan et al. (2014). Let",
      "startOffset" : 5,
      "endOffset" : 32
    }, {
      "referenceID" : 2,
      "context" : "2 of Balakrishnan et al. (2014). For part 1, as shown in B.",
      "startOffset" : 5,
      "endOffset" : 32
    }, {
      "referenceID" : 2,
      "context" : "2 of Balakrishnan et al. (2014). For part 1, as shown in B.2 of Balakrishnan et al. (2014) we have EY ,ξe 1 n ∑n i=1 ξiyiy > i ‖op ≤ EY ,ξej∈[M ] 1 n ∑n i=1 ξi〈yi,uj′ 〉2 (115) Recall that yi = ζiθ ? + ωi and (112), we have EY e〈yi,uj〉 = Eζe ?jEωeij ≤ e ‖θ?‖2+1 2 .",
      "startOffset" : 5,
      "endOffset" : 91
    } ],
    "year" : 2016,
    "abstractText" : "Expectation Maximization (EM) is among the most popular algorithms for estimating parameters of statistical models. However, EM, which is an iterative algorithm based on the maximum likelihood principle, is generally only guaranteed to find stationary points of the likelihood objective, and these points may be far from any maximizer. This article addresses this disconnect between the statistical principles behind EM and its algorithmic properties. Specifically, it provides a global analysis of EM for specific models in which the observations comprise an i.i.d. sample from a mixture of two Gaussians. This is achieved by (i) studying the sequence of parameters from idealized execution of EM in the infinite sample limit, and fully characterizing the limit points of the sequence in terms of the initial parameters; and then (ii) based on this convergence analysis, establishing statistical consistency (or lack thereof) for the actual sequence of parameters produced by EM.",
    "creator" : "LaTeX with hyperref package"
  }
}