{
  "name" : "1405.3396.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Reducing Dueling Bandits to Cardinal Bandits",
    "authors" : [ "Nir Ailon", "Zohar Karnin" ],
    "emails" : [ "nailon@cs.technion.ac.il", "zkarnin@gmail.com", "tj@cs.cornell.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "When interacting with an online system, users reveal their preferences through the choices they make. Such a choice – often termed implicit feedback – may be the click or tap on a particular link in a web-search ranking, or watching a particular movie among a set of recommendations. Connecting to a classic body of work in econometrics and empirical work in information retrieval Joachims et al. (2007), such implicit feedback is typically viewed as an ordinal preference between alternatives (i.e., “A is better than B”), but it does not provide reliable cardinal valuations (i.e., “A is very good, B is mediocre”).\nar X\niv :1\n40 5.\n33 96\nv1 [\ncs .L\nTo formalize the problem of learning from preferences, we consider the following interactive online learning model, which we call the Utility-Based Dueling Bandits Problem (UBDB) similar to Yue et al. (2012); Yue & Joachims (2011). At each iteration t, the learning system presents two actions xt, yt ∈ X to the user, where X is the set (either finite or infinite) of possible actions. Each of the two actions has an associated random reward (or utility) for the user, which we denote by ut and vt, respectively. The quantity ut (resp. vt) is drawn from a distribution that depends on xt (resp. yt) only. We assume these utilities are in [0, 1]. The learning system is rewarded the average utility Uavt = (ut + vt)/2 of the two actions it presents, but it does not observe this reward. Instead, it only observes the user’s binary choice among the two alternative actions xt, yt, which depends on the respective utilities ut and vt. In particular, we model the observed choice as a {0, 1}-valued random variable bt distributed as\nPr[bt = 0|(ut, vt)] = φ(ut, vt) Pr[bt = 1|(ut, vt)] = φ(vt, ut) , (1.1)\nwhere φ : [0, 1] × [0, 1] 7→ [0, 1] is a link function. Clearly, the link function has to satisfy φ(A,B)+φ(B,A) = 1. Below we concentrate on linear link functions (defined in Sec. 2). The binary choice is interpreted as a stochastic preference response between the left alternative xt (if bt = 0) and the right alternative yt (if bt = 1). The utility Uav captures the overall latent user experience from the pair of alternatives. A concrete example of this UBDB game is learning for web search, where X is a set of ranking functions among which the search engine selects two for each incoming query; the search engine then presents an interleaving Chapelle et al. (2012) of the two rankings, from which it can sense a stochastic preference between the two ranking functions based on the user’s clicking behavior.\nThe purpose of this paper is to show how UBDB can be reduced to the conventional (cardinal) stochastic Multi-Armed Bandit (MAB) problem1, which has been studied since 1952 Robbins (1952). In MAB, the system chooses only a single action xt ∈ X in each round and directly observes its cardinal reward ut, which is assumed to be drawn from a latent but fixed distribution attached to xt. The set X in the traditional MAB game is of finite cardinality K. In more general settings Dani et al. (2008); Mannor & Shamir (2011), this set can be infinite but structured in some way. Dani et al. (2008), for example, assume a stochastic setting in which X is a convex, bounded subset of Rn, and the expectation µ(x) of the corresponding value distribution is 〈µ, x〉, where µ ∈ Rn is an unknown coefficient vector and 〈·, ·〉 is the inner product with respect to the standard basis. We refer to this as the linear expected utility setting. We study here both the finite setting and the infinite setting.\nMain results. We provide general reductions from UBDB to MAB. More precisely, we use a MAB strategy as a black-box for helping us play the UBDB game. The art is in exactly how to use a black-box designed for MAB in order to play UBDB. We present one method, Doubler (Section 3) which adds an extraO(log T ) factor to the expected regret function compared to that of the MAB black-box, assuming the MAB black-box\n1One armed bandit is a popular slang for slot machines in casinos, and the MAB game describes the problem faced by a gambler who can choose one machine to play at each instance.\nhas polylogarithmic (in T ) regret, where T is the time horizon. When the MAB blackbox has polynomial regret, only an extra O(1) factor is incurred. This algorithm works for infinite bandit spaces. We also present a reduction algorithm MultiSBM (Section 4) which works for finite bandit spaces and gives an O(log T ) regret, assuming the MAB black-box enjoys an O(log T ) expected regret function with some mild higher moment assumptions. These assumptions are satisfied, for example, by the seminal UCB algorithm Auer et al. (2002). Our analysis in fact shows that for sufficiently large T , the regret of MultiSBM is asymptotically identical to that of UCB not only in terms of the time horizon T but in terms of second order terms such as the differences between the values of the arms; it follows that MultiSBM is asymptotically optimal in the second order terms as well as in T . Finally, we propose a third algorithm Sparring (Section 5) which we conjecture to enjoy regret bounds comparable to those of the MAB algorithms hiding in the black boxes it uses. We base the conjecture on arguments about a related, but different problem. In experiments (Section 7) comparing our reductions with special-purpose UBDB algorithms, Sparring performs clearly the best, further supporting our conjecture.\nAll results in this extended abstract assume the linear link function (see Section 2), but we also show preliminary results for other interesting link functions in Appendix D.\nContributions in relation to previous work. While specific algorithms for specific cases of the Dueling Bandits problem already exist Yue et al. (2012); Yue & Joachims (2011, 2009), our reductions provide a general approach to solving the UBDB. In particular, this paper provides general reductions that make it possible to transfer the large body of MAB work on exploiting structure in X to the dueling case in a constructive and algorithmic way. Second, despite the generality of the reductions their regret is asymptotically comparable to the tournament elimination strategies in Yue et al. (2012); Yue & Joachims (2011) for the finite case as T →∞, and better than the regret of the online convex optimzation algorithm of Yue & Joachims (2009) for the infinite case (albeit in a more restricted setting).\nIn our setting, the reward and feedback of the agent playing the online game are, in some sense, orthogonal to each other, or decoupled. A different type of decoupling was also considered in Avner et al.’s work Avner et al. (2012), although this work cannot be compared to theirs. There is yet more work on bandit games where the algorithm plays two bandits (or more) in each iteration, e.g. Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al. (2003); Ailon et al. (2012) as well as noisy sorting Karp & Kleinberg (2007); Feige et al. (1994), which are not the setting studied here. Finally, our results connect multi-armed bandits and online optimization to the classic econometric theory of discrete choice, with its use of preferential or choice information to recover values of goods (see Train (2009) and references therein).\nAnother important topic related to our work is that of partial monitoring games. The idea was introduced by Piccolboni & Schindelhauer (2001). The objective in partial monitoring is to choose at each round an action from some finite set of actions, and receive a reward based on some unknown function chosen by an oblivious process. The observed information is defined as some (known) function of the chosen action and the current choice of the oblivious process. One extreme setting in which\nthe observed information equals the reward captures MAB. In the other extreme, the observed information equals the entire vector of rewards (for all actions), giving rise to the so-called full information game. Our setting is a strict case of partial monitoring as it falls in neither extremes. Most papers dealing with partial monitoring either discuss non-stochastic settings or present problem-independent results. In both cases the regret is lower bounded by √ T , which is inapplicable to our setting (see Antos et al. (2012) for a characterization of partial monitoring problems). Bartók et al. Bartók et al. (2012) do present problem dependent bounds. Using their work, a logarithmic (in T ) bound can be deduced for the dueling bandit problem, at least in the finite case. However, the dependence on the number of arms is quadratic, whereas we present a linear one in what follows. Our algorithms are also much simpler and directly take advantage of the structure of the problem at hand."
    }, {
      "heading" : "2 Definitions",
      "text" : "The set of actions (or arms) is denoted by X . In a standard stochastic MAB (multiarmed bandit) game, each bandit x ∈ X has an unknown associated expected utlity µ(x) ∈ [0, 1]. At each step t the algorithm chooses some xt ∈ X and receives from “nature” a random utility ut ∈ [0, 1], drawn from a distribution of expectation µ(xt). This utility is viewed by the algorithm.2 The regret at time T of an algorithm is defined as R(T ) = ∑T t=1(µ(x\n∗) − ut). where x∗ is such that µ(x∗) = maxx∈X µ(x) (we assume the maximum is achievable). Throughout, for x ∈ X we will let ∆x denote µ(x∗)− µ(x) whenever we deal with MAB. (We will shortly make reference to some key results on MAB in Section 2.1.)\nIn this work we will use MAB algorithms as black boxes. To that end, we define a Singleton Bandit Machine (SBM) as a closed computational unit with an internal timer and memory. A SBM S supports three operations: reset, advance and feedback. The reset operation simply clears its state.3 The advance operation returns the next bandit to play, and feedback is used for simulating a feedback (the utility). It is assumed that advance and feedback operations are invoked in an alternating fashion. For example, if we want to use a SBM to help us play a traditional MAB game we first invoke reset(S), then invoke and set x1 ← advance(S), we will play x1 against nature and observe u1 and then invoke feedback(S, u1). We then invoke and set x2 ← advance(S), then we’ll play x2 against nature and observe u2, then invoke feedback(S, u2) and so on. For all SBM’s S that will be used in the algorithms in this work, we will only invoke the operation feedback(S, ·) with values in [0, 1].\nIn the utility based dueling bandit game (UBDB), the algorithm chooses (xt, yt) ∈ X ×X at each step, and a corresponding pair of random utilities (ut, vt) ∈ [0, 1] are given rise to, but not observed by the algorithm. We assume ut is drawn from a distribution of expectation µ(xt) and vt independently from a distribution of expectation µ(yt). The algorithm observes a choice variable bt ∈ {0, 1} distributed according to the law (1.1). This random variable should be thought of as the outcome of a duel,\n2It is typically assumed that this distribution depends on xt only, but this assumption can be relaxed. 3We assume the bandit space X is universally known to all SBM’s.\nor match between xt and yt. The outcome bt = 1 (resp. bt = 0) should be interpreted as “yt is chosen’ (resp. “xt is chosen”).4 The link function φ, which is assumed to be known, quantitatively determines how to translate the utilities ut, vt to winning probabilities. The linear link function φlin is defined by\nPr[bt = 1|(ut, vt)] = φlin(ut, vt) := 1 + vt − ut\n2 ∈ [0, 1] .\nThe unobserved reward is Uavt = (ut+ vt)/2, and the corresponding regret after T steps is Rav(T ) := ∑T t=1(µ(x\n∗)−Uavt ), where x∗ = argmaxx∈X µ(x). This implies that expected zero regret is achievable by setting (xt, yt) = (x∗, x∗). In practice, these two identical alternatives would be displayed as one, as would naturally happen in interleaved retrieval evaluation Chapelle et al. (2012). It should be also clear that playing (x∗, x∗) is pure exploitation, because the feedback is then an unbiased coin with zero exploratory information.\nWe also consider another form of (unobserved) utility, which is given as U choicet := ut(1 − bt) + vtbt. We call this choice-based utility, since the utility that is obtained depends on the user’s choice. Accordingly, we define Rchoicet := µ(x\n∗) − U choicet . In words, the player receives reward associated with either the left bandit or the right bandit, whichever was actually chosen. The utility U choice captures the user’s experience after choosing a result. In an e-commerce system, U choice may capture conversion, namely, the monetary value of the choice. Although both utility modelings Uav and U choice are well motivated by applications, we avoid dealing with choice based utilities and regrets for the following reason: regret bounds with respect to Uav imply similar regret bounds with respect to U choice.\nObservation 2.1. Assuming a link function where u > v implies φ(u, v) > 1/2, for any xt, yt, E[Rchoicet |(xt, yt)] ≤ E[Ravt |(xt, yt)].\n(Due to lack of space, the proof can be found in Appendix E.) The observation’s assumption on the link function in words is: when presented with two items, the item with the larger utility is more likely to be chosen. This clearly happens for any reasonable link function. We henceforth assume utility Uav and regretRav and will no longer make references to choice-based versions thereof."
    }, {
      "heading" : "2.1 Classic Stochastic MAB: A Short Review",
      "text" : "We review some relevant classic MAB literature. We begin with the well known UCB policy (Algorithm 1) for MAB in the finite case. The commonly known analysis of UCB provides expected regret bounds. For the finite X case, we need a less known, robust guarantee bounding the probability of playing a sub-optimal arm too often. Lemma 2.2 is implicitly proved in Auer et al. (2002). For completeness, we provide an explicit proof in Appendix A.\n4 We have just defined a two-level model in which the distribution of the random variable bt is determined by the outcome two other random variables ut, vt. For simplicity, the reader is encouraged to assume that (ut, vt) is deterministically (µ(xt), µ(yt)). Most technical difficulties in what follows are already captured by this simpler case.\nAlgorithm 1 UCB algorithm for MAB with |X| = K arms. Parameter α affects tail of regret per action in X . ∀x ∈ X , set µ̂x =∞ ∀x ∈ X , set tx = 0 set t = 1 while true do\nlet x be the index maximizing µ̂x + √\n(α+2) ln(t) 2tx\nplay x and update µ̂x as the average of rewards so far on action x; increment tx by 1. t← t+ 1\nend while\nLemma 2.2. AssumeX is finite. Fix a parameter α > 0. LetH := ∑ x∈X\\{x∗} 1/∆x. When running the UCB policy (Algorithm 1) with parameter α for T rounds the expected regret is bounded by\n2(α+ 2)H ln(T ) +K α+ 2\nα = O(αH lnT ) .\nFurthermore, lex x ∈ X denote some suboptimal arm and let s ≥ 4α ln(T )/∆2x. Denote by ρx(T ) the random variable counting the number of times arm x was chosen up to time T . Then Pr[ρx(T ) ≥ s] ≤ 2α · (s/2) −α.\nFor the infinite case, we will review a well known setting and result which will later be used to highlight the usefulness of Algorithm 2 (and the ensuing Theorem 3.1). In this setting, the set X of arms is an arbitrary (infinite) convex set in Rd. Here, the player chooses at each time point a vector x ∈ X and observes a stochastic reward with an expected value of 〈µ, x〉, for some unknown vector µ ∈ Rd.5 This setting was dealt with by Dani et al. (2008). They provide an algorithm for this setting that could be thought of as linear optimization under noisy feedback. Their algorithm provides (roughly) √ T regret for general convex bodies and polylog(T ) regret for polytopes. Formally, for general convex bodies, they prove the following.\nLemma 2.3 (Dani et al. 2008). Algorithm CONFIDENCEBALL1 (resp. CONFIDENCEBALL2) of Dani et al. (2008), provides an expected regret ofO (√ dT log3 T ) (resp. O (√ d2T log3 T ) ) for any convex set of arms.\nIn case X is a polytope with vertex set V and there is a unique vertex v∗ ∈ V achieving maxx∈X 〈µ, x〉, and any other vertex v ∈ V satisfies the gap condition 〈µ, v〉 ≤ 〈µ, v∗〉 −∆ for some ∆ > 0, we say we are in the ∆-gap case.\nLemma 2.4 (Dani et al. 2008). Assume the ∆-gap case. Algorithm CONFIDENCEBALL1 (resp. CONFIDENCEBALL2) of Dani et al. (2008), provides an expected regret of O ( ∆−1d2 log3 T ) (resp. O ( ∆−1d3 log3 T ) ).\n5Affine linear functions can also be dealt with by adding a coordinate fixed as 1.\nAlgorithm 2 (Doubler): Reduction for finite and infinite X with internal structure. 1: S ← new SBM over X 2: L ← an arbitrary singleton in X 3: i← 1, t← 1 4: while true do 5: reset(S) 6: for j = 1...2i do 7: choose xt uniformly from L 8: yt ← advance(S) 9: play (xt, yt), observe choice bt 10: feedback(S, bt) 11: t← t+ 1 12: end for 13: L ← the multi-set of arms played as yt in the last for-loop 14: i← i+ 1 15: end while"
    }, {
      "heading" : "3 UBDB Strategy for Large or Structured X",
      "text" : "In this section we consider UBDB in the case of a large or possibly infinite set of arms X , and the linear link function. The setting where X is large typically occurs when some underlying structure for X exists through which it is possible to gain information regarding one arm via queries to another. Our approach, called Doubler, is best explained by thinking of the UBDB strategy as a competition between two players, one controlling the choice of the left arm and the other, the choice of the right one. The objective of each player is to win as many rounds possible, hence intuitively, both players should play the arms with the largest approximated value. Since we are working with a stochastic environment it is not clear how to analyze a game in which both players are adaptive, and whether such a game would indeed lead to a low regret dueling match (see also Section 5 for a related discussion). For that reason, we make sure that at all times one player has a fixed stochastic strategy, which is updated infrequently.\nWe divide the time axis into exponentially growing epochs. In each epoch, the left player plays according to some fixed (stochastic) strategy which we define shortly, while the right one plays adaptively according to a strategy provided by a SBM. At the beginning of a new epoch, the distribution governing the left arm changes in a way that mimics the actions of the right arm in the previous epoch. The formal definition of Doubler is given in Algorithm 2.\nThe following theorem bounds the expected regret of Algorithm 2 as a function of the total number T of steps and the expected regret of the SBM that is used.\nTheorem 3.1. Consider a UBDB game over a set X . Assume the SBM S in Line 1 of Doubler (Algorithm 2) has an expected regret of c logα T after T steps, for all T . Then the expected regret of Doubler is at most 2c αα+1 log\nα+1 T . If the expected regret of the SBM is bounded by some function f(T ) = Ω(Tα) (with α > 0), then the expected regret of Doubler is at most O(f(T )).\nThe proof is deferred to Appendix B. By setting the SBM S used in Line 1 as the algorithms CONFIDENCEBALL1 or CONFIDENCEBALL2 of Dani et al. (2008), we obtain the following:\nCorollary 3.2. Consider a UBDB game over a setX . Assume that the SBM S in Line 1 of Doubler is algorithm CONFIDENCEBALL2 (resp. CONFIDENCEBALL1). If X is a compact convex set, then the expected regret of Doubler is at most O( √ dT log3(T ))\n(resp.O( √ d2T log3(T ))). In the ∆-gap setting (see discussion leading to Lemma 2.4),\nthe expected regret is bounded by O ( ∆−1d2 log4(T ) ) (resp. O ( ∆−1d3 log4(T ) ) ).\nIn the finite case, one may set the SBM S to the standard UCB, and obtain:\nCorollary 3.3. Consider a UBDB game over a finite set X of cardinality K. Let ∆i be the difference between the reward of the best arm and the i’th best arm. Assume the SBM S in Line 1 of Doubler is UCB. Then the expected regret of Doubler is at most O(H log2(T )) where H := ∑K i=2 ∆ −1 i\nMemory requirement issues: A possible drawback of Doubler is its need to store the history of yt from the last epoch in memory, translating to a possible memory requirement of Ω(T ). This situation can be avoided in many natural cases. The first is the case where X is embedded in a real linear space and the expectation µ(x) is a linear function. Here, there is no need to store the entire history of choices of the left arm but rather the average arm (recall that here the arms are thought of as vectors in Rd, hence the average is well defined). Playing the average arm (as xt) instead of picking an arm uniformly from the list of chosen arm gives the same result with memory requirements equivalent to storage of one arm. In other cases (e.g., X is not even geometrically embedded) this cannot be done. Nevertheless, as long as we are in a ∆-gap case, as T grows, the arm played as yt is the optimal one with increasingly higher probability. In more detail, if the regret incurred in a time epoch is R, then the number of times a suboptimal arm is played is at most R/∆. As R is polylogarithmic in T , the required space is polylogarithmic in T as well. We do not elaborate further on memory requirements and leave this as future research."
    }, {
      "heading" : "4 UBDB Strategy for Unstructured X",
      "text" : "In this section we present and analyze an alternative reduction strategy, called MultiSBM, particularly suited for the finite X case where the elements of X typically have no structure. MultiSBM will not incur an additional logarithmic factor as our previous approach did. Unlike the algorithms in Yue & Joachims (2011); Yue et al. (2012), we will avoid running an elimination tournament, but just resort to a standard MAB strategy by reduction. Denote K = |X|. The idea is to use K different SBMs in parallel, where each instance is indexed by an element in X . In step t we choose a left arm xt ∈ X in a way that will be explained shortly. The right arm, yt is chosen according to the suggestion on the SBM indexed by xt, and the binary choice is fed back to that\nAlgorithm 3 (MultiSBM): Reduction for unstructured finite X by using K SBMs in parallel.\n1: For all x ∈ X: Sx ← new SBM over X , reset(Sx) 2: y0 ← arbitrary element of X 3: t← 1 4: while true do 5: xt ← yt−1 6: yt ← advance(Sxt) 7: play (xt, yt), observe choice bt 8: feedback(Sxt , bt) 9: t← t+ 1\n10: end while\nSBM. In the next round, xt+1 is set to be yt, namely, the right arm becomes the left one in the next step. Algorithm 3 describes MultiSBM exactly.\nNaively, the regret of the algorithm can be shown to be at most K times that of a single SBM. However, it turns out that the regret is in fact asymptotically competitive with that of a single SBM, without the extraK factor. Specifically, we show that the total regret is in fact dominated solely by the regret of the SBM corresponding to the arm with maximal utility. To achieve this, we assume that the SBM’s implement a strategy with a certain robustness property that implies a bound not only on the expected regret, but also on the tail of the regret distribution. More precisely, an inverse polynomial tail distribution is necessary. Interestingly, the assumption is satisfied by the UCB algorithm Auer et al. (2002) (as detailed in Lemma 2.2). Recall that x∗ ∈ X denotes an arm with largest valuation µ(x), and that ∆x := µ(x∗)− µ(x) for all x ∈ X . Assume ∆x > 0 for all x 6= x∗.6\nDefinition 4.1. Let Tx be the number of times a (sub-optimal) arm x ∈ X is played when running the policy T rounds. A MAB policy is said to be α-robust when it has the following property: for all s ≥ 4α∆−2x ln(T ), it holds that Pr[Tx > s] < 2α (s/2) −α.\nRecall that as discussed in Section 2.1, in Auer et al.’s (2002) classic UCB policy this property can be achieved by slightly enlarging the confidence region.\nTheorem 4.2. The total expected regret of MultiSBM (Algorithm 3) in the UBDB game is\nO ( Hα lnT +Hα ( K lnK+K ln lnT − ∑ x 6=x∗ ln ∆x )) ,\nassuming the policy of the SBMs defined in Line 1 isα-robust forα = max(3, ln(K)/ ln ln(T )). The robustness can be ensured by choosing the UCB policy (Algorithm 1) for the SBM with parameter α.\nNote that achieving (α = 3)-robustness requires implementing a variant of UCB with a slight modification of the confidence interval parameter in each SBM. Therefore,\n6If this is not the case, our statements still hold, yet the proof becomes slightly more technical. As there is no real additional complication to the problem under this setting, we ignore this case.\nif the horizon T is large enough so that ln lnT > (lnK)/3, then the total regret is comparable to that of UCB in the standard MAB game.\nThe proof of the theorem is deferred to Appendix C. The main idea behind the proof is showing that a certain “positive feedback loop” emerges: if the expected regret incurred by the right arm at some time t is low, then there is a higher chance that x∗ will be played as the left arm at time t+ 1. Conversely, if any fixed arm (in particular, x∗) is played very often as the left arm, then the expected regret incurred by the right arm decreases rapidly."
    }, {
      "heading" : "5 A Heuristic Approach",
      "text" : "In this section we describe a heuristic called Sparring for playing UBDB, which shows extremely good performance in our experiments. Unfortunately, as of yet we were unable to prove performance bounds that explain its empirical performance. Sparring uses two SBMs, corresponding to left and right. In each round the pair of arms is chosen according to the strategies of the two corresponding SBMs. The SBM corresponding to the chosen arm receives a feedback of 1 while the other receives 0. The formal algorithm is described in Algorithm 4.\nThe intuition for this idea comes from analysis of an adversarial version of UDBD, in which it can be easily shown that the resulting expected regret of Sparring is at most a constant times the regret of the two SBMs which emulate an algorithm for adversarial MAB. (We omit the exact discussion and analysis for the adversarial counterpart of UDBD in this extended abstract.) We conjecture that the regret of Sparring is asymptotically bounded by the combined regret of the algorithms hiding in the SBM’s, with (possibly) a small overhead. Proving this conjecture is especially interesting for settings in whichX is infinite and a MAB algorithm with polylogarithmic regret exists. Indeed, previous literature based on tournament elimination strategies does not apply to infinite X , and Doubler presented earlier is probably suboptimal due to the extra log-factor it incurs.\nProving the conjecture appears to be tricky due to the fact that the left (resp. right) SBM does not see a stochastic environment, because its feedback depends on nonstochastic choices made by the right (resp. left) SBM. Perhaps there exist bad settings where both strategies would be mutually ‘stuck’ in some sub-optimal state. We leave the analysis of this approach as an interesting problem for future research. Our experiments will nevertheless include Sparring."
    }, {
      "heading" : "6 Notes",
      "text" : "Lower Bound: Our results contain upper bounds for the regret of the dueling bandit problem. We note that a matching lower bound, up to logarithmic terms can be shown via a simple reduction to the MAB problem. This reduction is the reverse of the others presented here: simulate a SBM by using a UBDB solver. It is an easy exercise to obtain such a reduction whose regret w.r.t. the MAB problem is at most twice the\nAlgorithm 4 (Sparring): Reduction to two SBMs. 1: SL, SR ← two new SBMs over X 2: reset(SL), reset(SR), t← 1 3: while true do 4: xt ← advance(SL); yt ← advance(SR) 5: play (xt, yt), observe choice bt ∈ {0, 1} 6: feedback(SL,1bt=0); feedback(SR,1bt=1) 7: t← t+ 1 8: end while\nregret of the dueling bandit problem. It follows that the same lower bounds of the classic MAB problem apply to the UBDB problem.\nAdversarial Setting: One may also consider an adversarial setting for the UBDB problem. Here, utilities of the arms that are assumed to be constant in the stochastic case are assumed to change each round in some arbitrary way. We do not elaborate on this setting due to space constraints but mention that (a) a lower bound of √ KT matching that of the MAB problem is valid in the UDBD setting, and (b) the Sparring algorithm, when using SBM solvers for the adversarial setting, can be shown to obtain the same regret bounds of said SBM solvers."
    }, {
      "heading" : "7 Experiments",
      "text" : "We now present several experiments comparing our algorithms with baselines consisting of the state-of-the-art INTERLEAVED FILTER (IF) Yue et al. (2012) and BEAT THE MEAN BANDIT (BTMB) Yue & Joachims (2011). Our experiments are exhaustive, as we include scenarios for which no bounds were derived (e.g. nonlinear link functions), as well as the much more general scenario in which BTMB was analyzed Yue & Joachims (2011).\nHenceforth, the set X of arms is {A,B,C,D,E, F}. For applications such as the interleaving search engines Chapelle et al. (2012), 6 arms is realistic. We considered 5 choices of the expected value function µ(·) and 3 link functions78.\nlinear φ(x, y) = (1 + x− y)/2 natural φ(x, y) = x/(x+ y) logit φ(x, y) = (1 + exp{y − x})−1\nName µ(A) µ(B) µ(C) µ(D) µ(E) µ(F ) 1good 0.8 0.2 0.2 0.2 0.2 0.2 2good 0.8 0.7 0.2 0.2 0.2 0.2 3good 0.8 0.7 0.7 0.2 0.2 0.2 arith 0.8 0.7 0.575 0.45 0.325 0.2 geom 0.8 0.7 0.512 0.374 0.274 0.2\n7To be precise, the actual expected utility vector µ was a random permutation of the one given in the table. This was done to prevent initialization bias arising from the specific implementation of the algorithms.\n8Note that in row ’arith’, µ(2)..µ(6) form an arithmetic progression, and in row ’geom’ they form a geometric progression.\nFor each 15 combinations of arm values and link function we ran all 5 algorithms IF, BTMB, MultiSBM, Doubler, and Sparring with random inputs spanning a time horizon of up to 32000.\nWe also set out to test our algorithms in a scenario defined in Yue & Joachims (2011). We refer to this setting as YJ. Unlike our setting, where choice probabilities are derived from (random) latent utilities together with a link function, in YJ an underlying unknown fixed matrix (Pxy) is assumed, where Pxy is the probability of arm x chosen given the pair (x, y). The matrix satisfies very mild constraints. Following Yue & Joachims (2011), define xy = (Pxy − Pyx)/2. The main constraint is, for some unknown total order over X , the imposition x y ⇐⇒ (x, y) > 0. The optimal arm x∗ is maximal in the total order. The regret incurred by playing the pair (xt, yt) at time t is 12 ( x∗xt + x∗yt).\nThe BTMB algorithm Yue & Joachims (2011) proposed for YJ is, roughly speaking, a tournament elimination scheme, in which a working set of candidate arms is maintained. Arms are removed from the set whenever there is high certainty about their suboptimality. Although the YJ setting is more general than ours, our algorithms can be executed without any modification, giving rise to an interesting comparison with BTMB. For this comparison, we shall use the same matrix ( xy)x,y∈X as in Yue & Joachims (2011), which was empirically estimated from an operational search engine.\nA B C D E F A 0 0.05 0.05 0.04 0.11 0.11 B −0.05 0 0.05 0.06 0.08 0.10 C −0.05 −0.05 0 0.04 0.01 0.06 D −0.04 −0.04 −0.04 0 0.04 0 E −0.11 −0.08 −0.01 −0.04 0 0.01 F −0.11 −0.10 −0.06 0 −0.01 0\n(Note that x∗ = A B C D E F .)\nExperiment Results and Analysis Figure 1 contains the expected regrets of these described scenarios as a function of the log (to the base 2) of the time, averaged over 400 executions, with one standard deviation confidence bars. The experiments reveal some interesting results. First, the heuristic approach is superior to all others in all of the settings. Second, among the other algorithms, the top two are the algorithms IF and MultiSBM, where MultiSBM is superior in a wide variety of scenarios."
    }, {
      "heading" : "8 Future work",
      "text" : "We dealt with choice in sets of size 2. What happens in cases where the player chooses from larger sets? We also analyzed only the linear choice function. See Appendix D for an extension of the results in Section 4 to other link functions.\nBoth algorithms Doubler and MultiSBM treated the left and right sides asymmetrically. This did not allow us to consider distinct expected valuation functions for the left and right positions. 9 Algorithm Sparring is symmetric, further motivating the question of proving its performance guarantees.\n9Such a case is actually motivated in a setting where, say, the perceived user valuation of items appearing lower in the list are lower, giving rise to bias toward items appearing at the top.\nProving (or refuting) the conjecture in Section 5 regarding the regret of Sparring is an interesting open problem. Much like our proof idea for the guarantee of MultiSBM, there is clearly a positive feedback loop between the two SBM’s in Sparring: the\nmore often the left (resp. right) arm is played optimally, the right (resp. left) arm would experience an environment which is closer to that of the standard MAB, and would hence incur expected regret approximately that of the SBM it implements."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors thank anonymous reviewers for thorough and insightful reviews. This research was funded in part by NSF Awards IIS-1217686 and IIS-1247696, a Marie Curie Reintegration Grant PIRG07-GA-2010-268403, an Israel Science Foundation grant 1271/33 and a Jacobs Technion-Cornell Innovation Institute grant."
    }, {
      "heading" : "A Robustness of the UCB algorithm",
      "text" : "For completeness, we present a proof of robustness for the UCB algorithm, presented as Algorithm 1 below. Note that we did not make an effort to bound the constants in the proof. We start by presenting Chernoff’s inequality providing a tail bound for estimations of variables contained in [0, 1].\nLemma A.1. Let Y1, . . . , Yt be i.i.d variables supported in [0, 1]. Then for any ε > 0 it holds that\nPr\n[ 1\nt t∑ i=1 Yi − E[Yi] > ε\n] ≤ e−2tε 2\nRecall that in our setting, there are K arms, each with an expected reward. For convenience we assume the set of bandits X is the set {1, . . . ,K} and further assume for the purpose of the analysis that arm 1 has the largest expected reward. We denote by ∆i the difference between the reward of arm 1 and that of arm i.\nProof of Lemma 2.2. For convenience, define β = α + 2 where α is the robustness parameter given as input to the algorithm. For i > 1, define\nui(t) = 2β ln(t)/∆ 2 i\nIf at time t, arm i where i > 1 (i.e. i is suboptimal) was chosen, one of the following must be true\n1. ρi(t) < ui(t) 2. µ̂i > µi + √\nβ ln(t) 2ρi(t)\n3. µ̂1 + √\nβ ln(t) 2ρ1(t) < µ1\nHere, ρi(t), ρ1(t) denote the number of times arms i and 1 (the optimal arm) were pulled up to time t. Indeed, if all 3 are false we have\nµ̂1 +\n√ β ln(t)\n2ρ1(t) ≥ µ1 = µi + ∆i ≥\nµi + 2\n√ β ln(t)\n2ρi(t) ≥ µ̂i +\n√ β ln(t)\n2ρi(t)\nand the i’th arm cannot be chosen. Hence, denoting ρi(T ) the number of times arm i is queried in a total budget of T queries, we have\nE[ρi(T )− ui(T )] ≤ T∑\nt=ui(T )+1\nPr[(2) or (3)]\nTo bound the probability of event (2) occurring, we use Chernoff’s inequality (Lemma A.1)\nPr[(2)] ≤ Pr [ ∃ρi ∈ [t] : µ̂i > µi + √ β ln(t)\n2ρi\n] ≤\nt · t−β = t1−β .\nThe bound for event (3) is analogous. It follows that the probability of events (2) or (3) occurring is bounded by 2t1−β and\nE[ρi(T )− ui(T )] ≤ T∑\nt=ui(T )+1\n2t1−β ≤\n2 β − 2 ( 2β ln(T )∆−2i )2−β (A.1)\nProving the bound on the expected regret is now a matter of simple calculation\nE[R] = ∑ i>1 E[∆i · ρi(T )] ≤\n2 ∑ i ∆i β − 2 + ∑ i>1 2β ln(T )/∆i ≤ 2K β − 2 + 2βH ln(T )\nWe proceed to prove the high probability bounds on the number of pulls of a suboptimal arm. Denote by ρsi (T ) the number of times arm i was chosen starting from the time point t ≥ s. Assuming s ≥ 2β ln(T )∆−2i , by the same arguments leading to equation A.1 we have\nE[ρsi (T )− ui(T )] ≤ 2\nβ − 2 s2−β\nAssume that arm i was chosen at least s times for some\ns ≥ 4(β + 2) ln(T ) ∆2i\nit follows that ρs−ui(T )−1i (T ) ≥ ui(T ) + 1. The probability of this happening is bounded by Markov’s inequality by\nPr[ρi(T ) ≥ s] ≤ Pr [ ρ s−ui(T )−1 i (T )− ui(T ) ≥ 1 ] ≤\nE [ ρ s−ui(T )−1 i (T )− ui(T ) ] ≤\n2\nβ − 2 (s− ui(T )− 1)2−β ≤\n2\nβ − 2 (s 2 )2−β The last inequality holds since\ns ≥ 4(β + 2) ln(T ) ∆2i ≥ 2 + 4β ln(T ) ∆2i = 2ui(T ) + 2"
    }, {
      "heading" : "B Proof of Theorem 3.1",
      "text" : "Let B(T ) denote the supremum of the expected regret of the SBM S (defined in line1 of Algorithm 2) after T steps, over all possible utility distributions of the arm set X .\nFix a phase i in the algorithm. The length Ti of the phase is exactly 2i. For all time steps t inside the phase, the left bandit xt is drawn from some fixed distribution. Let µ′ denote the common expectation E[ut] = Ext [ut|xt] of the reward of the left arm in all steps t in the phase. Now, the SBM S (defined in Line 1) is playing a standard MAB\ngame over the set X with binary rewards. Let bt denote the binary reward in the t’th step (within the phase). By construction,\nE[bt|vt, ut] = vt − ut + 1\n2 ∈ [0, 1] . (B.1)\nBy conditional expectation, for all y ∈ X ,\nE[bt|yt = y] = µ(y)− µ′ + 1\n2 ∈ [0, 1] . (B.2)\nNote that the arm with highest expected reward is y = x∗. By the definition of the bound function B(T ), the total expected regret (in the traditional MAB sense) of the SBM S in the phase is at most B(Ti) = B(2i). This means, that\nE [∑ t ( bt − µ(x∗)− µ′ + 1 2 )] ≤ B(2i) ,\nwhere the summation runs over t in the phase. But this clearly means, using (B.2), that\nE [∑ t µ(yt)− µ(x∗) 2 ] ≤ B(2i) .\nBut notice that E[vt] = EytE[vt|yt] = E[µ(yt)]. Hence,\nE [∑ t vt − µ(x∗) 2 ] ≤ B(2i) .\nIn words, this says that the expected contribution of the right arm to the regret (in the UDBD game) in phase i is at mostB(2i). It remains to bound the expected contribution to the regret of the left bandit in phase i, which is drawn by a distribution which assigns to all x ∈ X a probability proportional to the frequency of x played as the right arm in the previous phase.10 By the principle of conditional expectation, and due to the linearity of the link function, the expected regret incurred by xt (in each step in the phase) is exactly the average expected regret contributed by the right bandit in phase i − 1, and hence at most B(2i−1)/2i−1. This means that the total expected regret incurred by the left bandit in phase i is bounded by 2i(B(2i−1)/2i−1) = 2B(2i−1). Concluding, for a time horizon of T uniquely decomposable as 2+4+8+ · · ·+2k+Z for some integers k ≥ 1 and 0 ≤ Z ≤ 2k+1-1, the total expected regret is given by the following function of T :\n1/2 + 3B(2) + 3B(4) + · · ·+ 3B(2k) +B(Z) . (B.3)\nThe theorem claim is now obtained by simple analysis of (B.3).\n10If X is infinite, to be precise we need to say that the distribution is also supported on the set of arms played on the right side in the previous phase."
    }, {
      "heading" : "C Proof of Theorem 4.2",
      "text" : "To follow the proof, it is important to understand that in MultiSBM (Algorithm 3), exactly one SBM is advanced at each step in Line 6. This means that the internal timer of each SBM may be (and usually is) strictly behind the iteration counter of the algorithm, which is measured by the variable t. Denote by ρx(t) the total number of times Sx was advanced after t iterations of the algorithm, for all x ∈ X .\nWe now assume that all coin tosses are fixed (obliviously) in advance. This allows us to discuss the regret of the SBM Sx (line 1) after T ′ internal steps even if in practice the value t for which ρx(t) = T ′ might be much larger than the total number of arm pulls T , and in fact, may not even exist.\nNotice that internally, Sx sees a world in which the reward is binary, and the expected reward for bandit y ∈ X is exactly (µ(y) − µ(x) + 1)/2 at each internal step. This is because when Sx is advanced, the left bandit (in the UBDB game) is identically x. It follows that in all SBMs, the suboptimalities are the same and are ∆y/2 for arm y.\nFor x ∈ X and integer T ′ > 0, let\nRx(T ′) =\n1\n2 ∑ t:ρx(t)≤T ′,xt=x ∆yt\nIn words, this is the contribution of the right bandit choices to the UBDB regret at all times t for which the left bandit is chosen as x, and Sx’s internal counter has not surpassed T ′. The expression Rx(T ′) , by the last discussion, also measures the expected internal regret seen by Sx after T ′ internal steps. Similarly, we define\nRxy(T ′) = #{t : ρx(t) ≤ T ′, xt = x, yt = y}∆y/2\nThis measures a part of Rx(T ′) for which the right bandit is y. We start with an observation expressing the regret of the entire process as a function of the different Rxy’s. It will be useful to define ρxy(T ′) = #{t : ρx(t) ≤ T ′, xt = x, yt = y}, so that Rxy(T ′) = ρxy(T ′)∆y/2.\nObservation C.1. For any T ≥ 1, the total regret R(T ) of MultiSBM after T steps satisfies ∣∣∣R(T )− 2∑x∈X∑y∈X Rxy(ρx(T ))∣∣∣ ≤ 0.5. We conclude that in order to bound the expected regret R(T ) it suffices to bound the expressions E[Rxy(ρx(T ))]. By using the upper bound of ρx(T ) ≤ T , we get the trivial bound for E[R(T )] of K times the expected regret of a single machine. The main insight is to exploit the fact that typically, ρx(T ) is order of lnT for suboptimal x. We begin with the observation that for any fixed x, y ∈ X (x suboptimal), s ≥ 8α,\nPr[Rxy(T ) ≥ (s lnT )/∆y] = Pr[Rxy(T ) ≥ ((s/2) lnT )/(∆y/2)] = Pr[ρxy(T ) ≥ ((s/2) lnT )/(∆y/2)2]\n≤ ( (s/4) lnT )/(∆y/2) 2 )−α ≤ (s lnT )−α (C.1)\nThis is immediate from the α-robustness of the SBM and the fact we choose α > 2. For the same assumption on s and x, y and using the union bound,\nPr [ ∃p ∈ {0, . . . , dln lnT e} : Rxy ( ee p ) ≥ s · p/∆y ] ≤ 2s−α (C.2)\nWe now bound the quantity ρx(T ) for any nonoptimal fixed x. Using the (trivial) fact that all z ∈ X satisfy ρz(T ) ≤ T , together with the fact that SBM Sx is advanced in each iteration only if x was the right bandit in the previous one, we have that for all suboptimal x,\nPr[ρx(T ) ≥ (sK lnT )/∆2x] ≤ ∑ z∈X Pr [Rzx(T ) ≥ (s lnT )/∆x] ≤ K/(s lnT )α, (C.3)\nwhere the rightmost inequality is by union bound and (C.1). Fix some x, y ∈ X (x suboptimal). The last two inequalities give rise to a random variable Z defined as the minimal scalar for which we have\n∀T ′ ∈ [e, ee, ee 2 , . . . , ee dln ln(T )e ],\nρx(T ) ≤ (ZK lnT )/∆2x, Rxy(T ′) ≤ (Z lnT ′)/∆y By (C.2)-(C.3) we have that for all s ≥ 8α, Pr[Z ≥ s] ≤ 2s−α + K(s lnT )−α. Also, conditioned on the event that {Z ≤ s} we have that Rxy(ρx(T )) ≤ Rsxy := s ·e · ln((sK lnT )/∆2x)/∆y , which isO ( s∆−1y (ln lnT + lnK + ln s+ ln(1/∆x)) ) . Combining, E[Rxy(ρx(T ))] is bounded above by:\nR8α−1xy + ∞∑ i=0 R8α+ixy (2(8α+ i) −α +K((8α+ i) lnT )−α) .\nFor α = max{3, 2 + (lnK)/ ln lnT )}, it is easy to verify that the last expression converges to O(R8αxy ), hence\nE[Rxy(ρx(T ))] = O ( α∆−1y (ln lnT + lnK + ln(1/∆x)) ) .\nConcluding, the total expected regret E[R] is at most 0.5+E[Rx∗+ ∑ x,y∈X\\{x∗}Rxy], clearly proving the theorem."
    }, {
      "heading" : "D Extension to more General Models",
      "text" : "Assume the setting of Section 4. In this section we assume for simplicity that for any t and any choice of xt, yt, the utilities are deterministically ut = µ(xt), vt = µ(yt). In Yue & Joachims (2011), the dueling bandit problem is presented where a more relaxed assumption is made on the probabilities of the outcomes of duels. Each pair of arm x, y is assigned a parameter ∆(x, y) such that the probability of x being chosen when dueling with y is 0.5 + ∆(x, y). It is assumed that there exists some order over the arms and the ∆’s hold two properties.\n• (Relaxed) Stochastic Transitivity: For some γ ≥ 1 and any pair x∗ x y we have γ∆(x∗, y) ≥ max{∆(x∗, x),∆(x, y)}.\n• (Relaxed) Stochastic Triangle Inequality: For some γ ≥ 1 and any pair x∗ x y we have γ∆(x∗, y) ≤ ∆(x∗, x) + ∆(x, y).\nWe have analyzed MultiSBM (Algorithm 3) under the assumption that ∆(x, y) = (µ(x)− µ(y))/2. It can be easliy verified that our proof holds for arbitrary ∆’s under the following assumption:\n• (Relaxed) Extended Stochastic Triangle Inequality. For some γ ≥ 1, and any pair x, y (where it does not necessarily hold that x y) it holds that γ∆(x∗, y) ≤ ∆(x∗, x) + ∆(x, y).\nThis property is clearly held for ∆(x, y) = (µ(x) − µ(y))/2. However, it holds for a wider family of ∆’s. For example, it holds for ∆(x, y) = µ(x)/(µ(x) + µ(y)), assuming all µ’s are in the region [1/γ, 1]. The effect of γ to the regret is given in the following theorem:\nTheorem D.1. Assume the probability for the outcome of a duel is defined according to ∆(x, y), where ∆ has the Relaxed Extended Stochastic Triangle Inequality with parameter γ. The total expected regret of MultiSBM in the UBDB game is asymptotic to\nγHα K ln(K) +K ln ln(T ) + ∑ x∈X\\{x∗} ln(1/∆x) + ln(T )Hα\nassuming the invoked MAB policy is α-robust for α = max(3, ln(K)/ ln ln(T )).\nNotice that γ does not enter the summand of ln(T ), meaning that for large values of T , the regret is unaffected by γ. We defer the proof of the theorem to the full version of the paper."
    }, {
      "heading" : "E Proof of Observation 2.1",
      "text" : "By definition,\nE[Rchoicet |(xt, yt)] = µ(x∗)− E[U choicet |(xt, yt)] .\nBut now note that by the definition of the link function and of U choicet ,\nE[U choicet |(xt, yt)] = φ(ut, vt)ut + φ(vt, ut)vt ≥ ut + vt\n2\nwhere we used the assumption that for u > v, φ(u, v) > 1/2. Now notice that the expression on the right is exactly E[Uav|(xt, yt)]. Hence,\nE[Rchoicet |(xt, yt)] ≤ µ(x∗)− E[Uavt |(xt, yt)] = E[Ravt ] ."
    } ],
    "references" : [ {
      "title" : "Optimal algorithms for online convex optimization with multi-point bandit feedback",
      "author" : [ "Agarwal", "Alekh", "Dekel", "Ofer", "Xiao", "Lin" ],
      "venue" : "In COLT, pp",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2010
    }, {
      "title" : "Active learning using smooth relative regret approximations with applications",
      "author" : [ "Ailon", "Nir", "Begleiter", "Ron", "Ezra", "Esther" ],
      "venue" : "Journal of Machine Learning Research Proceedings Track,",
      "citeRegEx" : "Ailon et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ailon et al\\.",
      "year" : 2012
    }, {
      "title" : "Toward a classification of finite partial-monitoring games",
      "author" : [ "Antos", "András", "Bartók", "Gábor", "Pál", "Dávid", "Szepesvári", "Csaba" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Antos et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Antos et al\\.",
      "year" : 2012
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Auer", "Peter", "Cesa-Bianchi", "Nicolò", "Fischer", "Paul" ],
      "venue" : "Mach. Learn.,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Decoupling exploration and exploitation in multi-armed bandits",
      "author" : [ "Avner", "Orly", "Mannor", "Shie", "Shamir", "Ohad" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Avner et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Avner et al\\.",
      "year" : 2012
    }, {
      "title" : "An adaptive algorithm for finite stochastic partial monitoring",
      "author" : [ "Bartók", "Gábor", "Zolghadr", "Navid", "Szepesvári", "Csaba" ],
      "venue" : "arXiv preprint arXiv:1206.6487,",
      "citeRegEx" : "Bartók et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bartók et al\\.",
      "year" : 2012
    }, {
      "title" : "Large-scale validation and analysis of interleaved search evaluation",
      "author" : [ "O. Chapelle", "T. Joachims", "F. Radlinski", "Yue", "Yisong" ],
      "venue" : "ACM Transactions on Information Systems (TOIS),",
      "citeRegEx" : "Chapelle et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2012
    }, {
      "title" : "Stochastic linear optimization under bandit feedback",
      "author" : [ "Dani", "Varsha", "Hayes", "Thomas P", "Kakade", "Sham M" ],
      "venue" : "In COLT, pp",
      "citeRegEx" : "Dani et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2008
    }, {
      "title" : "Computing with noisy information",
      "author" : [ "Feige", "Uriel", "Raghavan", "Prabhakar", "Peleg", "David", "Upfal", "Eli" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Feige et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Feige et al\\.",
      "year" : 1994
    }, {
      "title" : "An efficient boosting algorithm for combining preferences",
      "author" : [ "Freund", "Yoav", "Iyer", "Raj D", "Schapire", "Robert E", "Singer", "Yoram" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Freund et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Freund et al\\.",
      "year" : 2003
    }, {
      "title" : "Large margin rank boundaries for ordinal regression",
      "author" : [ "R Herbrich", "Graepel", "Thore", "Obermayer", "Klaus" ],
      "venue" : "Book chapter, Advances in Large Margin Classifiers,",
      "citeRegEx" : "Herbrich et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Herbrich et al\\.",
      "year" : 2000
    }, {
      "title" : "Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search",
      "author" : [ "T. Joachims", "L. Granka", "Pan", "Bing", "H. Hembrooke", "F. Radlinski", "G. Gay" ],
      "venue" : "ACM Transactions on Information Systems (TOIS),",
      "citeRegEx" : "Joachims et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Joachims et al\\.",
      "year" : 2007
    }, {
      "title" : "Noisy binary search and its applications",
      "author" : [ "Karp", "Richard M", "Kleinberg", "Robert" ],
      "venue" : "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,",
      "citeRegEx" : "Karp et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Karp et al\\.",
      "year" : 2007
    }, {
      "title" : "From bandits to experts: On the value of sideobservations",
      "author" : [ "Mannor", "Shie", "Shamir", "Ohad" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Mannor et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mannor et al\\.",
      "year" : 2011
    }, {
      "title" : "Discrete prediction games with arbitrary feedback and loss",
      "author" : [ "Piccolboni", "Antonio", "Schindelhauer", "Christian" ],
      "venue" : "In Computational Learning Theory,",
      "citeRegEx" : "Piccolboni et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Piccolboni et al\\.",
      "year" : 2001
    }, {
      "title" : "Some aspects of the sequential design of experiments",
      "author" : [ "H. Robbins" ],
      "venue" : "Bulletin of the AMS,",
      "citeRegEx" : "Robbins,? \\Q1952\\E",
      "shortCiteRegEx" : "Robbins",
      "year" : 1952
    }, {
      "title" : "Discrete Choice Methods with Simulation",
      "author" : [ "Train", "Keneth" ],
      "venue" : null,
      "citeRegEx" : "Train and Keneth.,? \\Q2009\\E",
      "shortCiteRegEx" : "Train and Keneth.",
      "year" : 2009
    }, {
      "title" : "Interactively optimizing information retrieval systems as a dueling bandits problem",
      "author" : [ "Yue", "Yisong", "T. Joachims" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Yue et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yue et al\\.",
      "year" : 2009
    }, {
      "title" : "Beat the mean bandit",
      "author" : [ "Yue", "Yisong", "Joachims", "Thorsten" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Yue et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Yue et al\\.",
      "year" : 2011
    }, {
      "title" : "The k-armed dueling bandits problem",
      "author" : [ "Yue", "Yisong", "Broder", "Josef", "Kleinberg", "Robert", "Joachims", "Thorsten" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "Yue et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yue et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Connecting to a classic body of work in econometrics and empirical work in information retrieval Joachims et al. (2007), such implicit feedback is typically viewed as an ordinal preference between alternatives (i.",
      "startOffset" : 97,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : "To formalize the problem of learning from preferences, we consider the following interactive online learning model, which we call the Utility-Based Dueling Bandits Problem (UBDB) similar to Yue et al. (2012); Yue & Joachims (2011).",
      "startOffset" : 190,
      "endOffset" : 208
    }, {
      "referenceID" : 17,
      "context" : "To formalize the problem of learning from preferences, we consider the following interactive online learning model, which we call the Utility-Based Dueling Bandits Problem (UBDB) similar to Yue et al. (2012); Yue & Joachims (2011). At each iteration t, the learning system presents two actions xt, yt ∈ X to the user, where X is the set (either finite or infinite) of possible actions.",
      "startOffset" : 190,
      "endOffset" : 231
    }, {
      "referenceID" : 6,
      "context" : "A concrete example of this UBDB game is learning for web search, where X is a set of ranking functions among which the search engine selects two for each incoming query; the search engine then presents an interleaving Chapelle et al. (2012) of the two rankings, from which it can sense a stochastic preference between the two ranking functions based on the user’s clicking behavior.",
      "startOffset" : 218,
      "endOffset" : 241
    }, {
      "referenceID" : 6,
      "context" : "A concrete example of this UBDB game is learning for web search, where X is a set of ranking functions among which the search engine selects two for each incoming query; the search engine then presents an interleaving Chapelle et al. (2012) of the two rankings, from which it can sense a stochastic preference between the two ranking functions based on the user’s clicking behavior. The purpose of this paper is to show how UBDB can be reduced to the conventional (cardinal) stochastic Multi-Armed Bandit (MAB) problem1, which has been studied since 1952 Robbins (1952). In MAB, the system chooses only a single action xt ∈ X in each round and directly observes its cardinal reward ut, which is assumed to be drawn from a latent but fixed distribution attached to xt.",
      "startOffset" : 218,
      "endOffset" : 570
    }, {
      "referenceID" : 6,
      "context" : "A concrete example of this UBDB game is learning for web search, where X is a set of ranking functions among which the search engine selects two for each incoming query; the search engine then presents an interleaving Chapelle et al. (2012) of the two rankings, from which it can sense a stochastic preference between the two ranking functions based on the user’s clicking behavior. The purpose of this paper is to show how UBDB can be reduced to the conventional (cardinal) stochastic Multi-Armed Bandit (MAB) problem1, which has been studied since 1952 Robbins (1952). In MAB, the system chooses only a single action xt ∈ X in each round and directly observes its cardinal reward ut, which is assumed to be drawn from a latent but fixed distribution attached to xt. The set X in the traditional MAB game is of finite cardinality K. In more general settings Dani et al. (2008); Mannor & Shamir (2011), this set can be infinite but structured in some way.",
      "startOffset" : 218,
      "endOffset" : 878
    }, {
      "referenceID" : 6,
      "context" : "A concrete example of this UBDB game is learning for web search, where X is a set of ranking functions among which the search engine selects two for each incoming query; the search engine then presents an interleaving Chapelle et al. (2012) of the two rankings, from which it can sense a stochastic preference between the two ranking functions based on the user’s clicking behavior. The purpose of this paper is to show how UBDB can be reduced to the conventional (cardinal) stochastic Multi-Armed Bandit (MAB) problem1, which has been studied since 1952 Robbins (1952). In MAB, the system chooses only a single action xt ∈ X in each round and directly observes its cardinal reward ut, which is assumed to be drawn from a latent but fixed distribution attached to xt. The set X in the traditional MAB game is of finite cardinality K. In more general settings Dani et al. (2008); Mannor & Shamir (2011), this set can be infinite but structured in some way.",
      "startOffset" : 218,
      "endOffset" : 902
    }, {
      "referenceID" : 6,
      "context" : "A concrete example of this UBDB game is learning for web search, where X is a set of ranking functions among which the search engine selects two for each incoming query; the search engine then presents an interleaving Chapelle et al. (2012) of the two rankings, from which it can sense a stochastic preference between the two ranking functions based on the user’s clicking behavior. The purpose of this paper is to show how UBDB can be reduced to the conventional (cardinal) stochastic Multi-Armed Bandit (MAB) problem1, which has been studied since 1952 Robbins (1952). In MAB, the system chooses only a single action xt ∈ X in each round and directly observes its cardinal reward ut, which is assumed to be drawn from a latent but fixed distribution attached to xt. The set X in the traditional MAB game is of finite cardinality K. In more general settings Dani et al. (2008); Mannor & Shamir (2011), this set can be infinite but structured in some way. Dani et al. (2008), for example, assume a stochastic setting in which X is a convex, bounded subset of R, and the expectation μ(x) of the corresponding value distribution is 〈μ, x〉, where μ ∈ R is an unknown coefficient vector and 〈·, ·〉 is the inner product with respect to the standard basis.",
      "startOffset" : 218,
      "endOffset" : 975
    }, {
      "referenceID" : 1,
      "context" : "These assumptions are satisfied, for example, by the seminal UCB algorithm Auer et al. (2002). Our analysis in fact shows that for sufficiently large T , the regret of MultiSBM is asymptotically identical to that of UCB not only in terms of the time horizon T but in terms of second order terms such as the differences between the values of the arms; it follows that MultiSBM is asymptotically optimal in the second order terms as well as in T .",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "These assumptions are satisfied, for example, by the seminal UCB algorithm Auer et al. (2002). Our analysis in fact shows that for sufficiently large T , the regret of MultiSBM is asymptotically identical to that of UCB not only in terms of the time horizon T but in terms of second order terms such as the differences between the values of the arms; it follows that MultiSBM is asymptotically optimal in the second order terms as well as in T . Finally, we propose a third algorithm Sparring (Section 5) which we conjecture to enjoy regret bounds comparable to those of the MAB algorithms hiding in the black boxes it uses. We base the conjecture on arguments about a related, but different problem. In experiments (Section 7) comparing our reductions with special-purpose UBDB algorithms, Sparring performs clearly the best, further supporting our conjecture. All results in this extended abstract assume the linear link function (see Section 2), but we also show preliminary results for other interesting link functions in Appendix D. Contributions in relation to previous work. While specific algorithms for specific cases of the Dueling Bandits problem already exist Yue et al. (2012); Yue & Joachims (2011, 2009), our reductions provide a general approach to solving the UBDB.",
      "startOffset" : 75,
      "endOffset" : 1190
    }, {
      "referenceID" : 1,
      "context" : "These assumptions are satisfied, for example, by the seminal UCB algorithm Auer et al. (2002). Our analysis in fact shows that for sufficiently large T , the regret of MultiSBM is asymptotically identical to that of UCB not only in terms of the time horizon T but in terms of second order terms such as the differences between the values of the arms; it follows that MultiSBM is asymptotically optimal in the second order terms as well as in T . Finally, we propose a third algorithm Sparring (Section 5) which we conjecture to enjoy regret bounds comparable to those of the MAB algorithms hiding in the black boxes it uses. We base the conjecture on arguments about a related, but different problem. In experiments (Section 7) comparing our reductions with special-purpose UBDB algorithms, Sparring performs clearly the best, further supporting our conjecture. All results in this extended abstract assume the linear link function (see Section 2), but we also show preliminary results for other interesting link functions in Appendix D. Contributions in relation to previous work. While specific algorithms for specific cases of the Dueling Bandits problem already exist Yue et al. (2012); Yue & Joachims (2011, 2009), our reductions provide a general approach to solving the UBDB. In particular, this paper provides general reductions that make it possible to transfer the large body of MAB work on exploiting structure in X to the dueling case in a constructive and algorithmic way. Second, despite the generality of the reductions their regret is asymptotically comparable to the tournament elimination strategies in Yue et al. (2012); Yue & Joachims (2011) for the finite case as T →∞, and better than the regret of the online convex optimzation algorithm of Yue & Joachims (2009) for the infinite case (albeit in a more restricted setting).",
      "startOffset" : 75,
      "endOffset" : 1639
    }, {
      "referenceID" : 1,
      "context" : "These assumptions are satisfied, for example, by the seminal UCB algorithm Auer et al. (2002). Our analysis in fact shows that for sufficiently large T , the regret of MultiSBM is asymptotically identical to that of UCB not only in terms of the time horizon T but in terms of second order terms such as the differences between the values of the arms; it follows that MultiSBM is asymptotically optimal in the second order terms as well as in T . Finally, we propose a third algorithm Sparring (Section 5) which we conjecture to enjoy regret bounds comparable to those of the MAB algorithms hiding in the black boxes it uses. We base the conjecture on arguments about a related, but different problem. In experiments (Section 7) comparing our reductions with special-purpose UBDB algorithms, Sparring performs clearly the best, further supporting our conjecture. All results in this extended abstract assume the linear link function (see Section 2), but we also show preliminary results for other interesting link functions in Appendix D. Contributions in relation to previous work. While specific algorithms for specific cases of the Dueling Bandits problem already exist Yue et al. (2012); Yue & Joachims (2011, 2009), our reductions provide a general approach to solving the UBDB. In particular, this paper provides general reductions that make it possible to transfer the large body of MAB work on exploiting structure in X to the dueling case in a constructive and algorithmic way. Second, despite the generality of the reductions their regret is asymptotically comparable to the tournament elimination strategies in Yue et al. (2012); Yue & Joachims (2011) for the finite case as T →∞, and better than the regret of the online convex optimzation algorithm of Yue & Joachims (2009) for the infinite case (albeit in a more restricted setting).",
      "startOffset" : 75,
      "endOffset" : 1662
    }, {
      "referenceID" : 1,
      "context" : "These assumptions are satisfied, for example, by the seminal UCB algorithm Auer et al. (2002). Our analysis in fact shows that for sufficiently large T , the regret of MultiSBM is asymptotically identical to that of UCB not only in terms of the time horizon T but in terms of second order terms such as the differences between the values of the arms; it follows that MultiSBM is asymptotically optimal in the second order terms as well as in T . Finally, we propose a third algorithm Sparring (Section 5) which we conjecture to enjoy regret bounds comparable to those of the MAB algorithms hiding in the black boxes it uses. We base the conjecture on arguments about a related, but different problem. In experiments (Section 7) comparing our reductions with special-purpose UBDB algorithms, Sparring performs clearly the best, further supporting our conjecture. All results in this extended abstract assume the linear link function (see Section 2), but we also show preliminary results for other interesting link functions in Appendix D. Contributions in relation to previous work. While specific algorithms for specific cases of the Dueling Bandits problem already exist Yue et al. (2012); Yue & Joachims (2011, 2009), our reductions provide a general approach to solving the UBDB. In particular, this paper provides general reductions that make it possible to transfer the large body of MAB work on exploiting structure in X to the dueling case in a constructive and algorithmic way. Second, despite the generality of the reductions their regret is asymptotically comparable to the tournament elimination strategies in Yue et al. (2012); Yue & Joachims (2011) for the finite case as T →∞, and better than the regret of the online convex optimzation algorithm of Yue & Joachims (2009) for the infinite case (albeit in a more restricted setting).",
      "startOffset" : 75,
      "endOffset" : 1786
    }, {
      "referenceID" : 1,
      "context" : "These assumptions are satisfied, for example, by the seminal UCB algorithm Auer et al. (2002). Our analysis in fact shows that for sufficiently large T , the regret of MultiSBM is asymptotically identical to that of UCB not only in terms of the time horizon T but in terms of second order terms such as the differences between the values of the arms; it follows that MultiSBM is asymptotically optimal in the second order terms as well as in T . Finally, we propose a third algorithm Sparring (Section 5) which we conjecture to enjoy regret bounds comparable to those of the MAB algorithms hiding in the black boxes it uses. We base the conjecture on arguments about a related, but different problem. In experiments (Section 7) comparing our reductions with special-purpose UBDB algorithms, Sparring performs clearly the best, further supporting our conjecture. All results in this extended abstract assume the linear link function (see Section 2), but we also show preliminary results for other interesting link functions in Appendix D. Contributions in relation to previous work. While specific algorithms for specific cases of the Dueling Bandits problem already exist Yue et al. (2012); Yue & Joachims (2011, 2009), our reductions provide a general approach to solving the UBDB. In particular, this paper provides general reductions that make it possible to transfer the large body of MAB work on exploiting structure in X to the dueling case in a constructive and algorithmic way. Second, despite the generality of the reductions their regret is asymptotically comparable to the tournament elimination strategies in Yue et al. (2012); Yue & Joachims (2011) for the finite case as T →∞, and better than the regret of the online convex optimzation algorithm of Yue & Joachims (2009) for the infinite case (albeit in a more restricted setting). In our setting, the reward and feedback of the agent playing the online game are, in some sense, orthogonal to each other, or decoupled. A different type of decoupling was also considered in Avner et al.’s work Avner et al. (2012), although this work cannot be compared to theirs.",
      "startOffset" : 75,
      "endOffset" : 2078
    }, {
      "referenceID" : 0,
      "context" : "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step.",
      "startOffset" : 0,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al.",
      "startOffset" : 0,
      "endOffset" : 182
    }, {
      "referenceID" : 0,
      "context" : "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al. (2003); Ailon et al.",
      "startOffset" : 0,
      "endOffset" : 204
    }, {
      "referenceID" : 0,
      "context" : "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al. (2003); Ailon et al. (2012) as well as noisy sorting Karp & Kleinberg (2007); Feige et al.",
      "startOffset" : 0,
      "endOffset" : 225
    }, {
      "referenceID" : 0,
      "context" : "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al. (2003); Ailon et al. (2012) as well as noisy sorting Karp & Kleinberg (2007); Feige et al.",
      "startOffset" : 0,
      "endOffset" : 274
    }, {
      "referenceID" : 0,
      "context" : "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al. (2003); Ailon et al. (2012) as well as noisy sorting Karp & Kleinberg (2007); Feige et al. (1994), which are not the setting studied here.",
      "startOffset" : 0,
      "endOffset" : 295
    }, {
      "referenceID" : 0,
      "context" : "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al. (2003); Ailon et al. (2012) as well as noisy sorting Karp & Kleinberg (2007); Feige et al. (1994), which are not the setting studied here. Finally, our results connect multi-armed bandits and online optimization to the classic econometric theory of discrete choice, with its use of preferential or choice information to recover values of goods (see Train (2009) and references therein).",
      "startOffset" : 0,
      "endOffset" : 559
    }, {
      "referenceID" : 0,
      "context" : "Agarwal et al. Agarwal et al. (2010), although there the feedback is cardinal and not relative in each step. There is much work on learning from example pairs Herbrich et al. (2000); Freund et al. (2003); Ailon et al. (2012) as well as noisy sorting Karp & Kleinberg (2007); Feige et al. (1994), which are not the setting studied here. Finally, our results connect multi-armed bandits and online optimization to the classic econometric theory of discrete choice, with its use of preferential or choice information to recover values of goods (see Train (2009) and references therein). Another important topic related to our work is that of partial monitoring games. The idea was introduced by Piccolboni & Schindelhauer (2001). The objective in partial monitoring is to choose at each round an action from some finite set of actions, and receive a reward based on some unknown function chosen by an oblivious process.",
      "startOffset" : 0,
      "endOffset" : 726
    }, {
      "referenceID" : 2,
      "context" : "In both cases the regret is lower bounded by √ T , which is inapplicable to our setting (see Antos et al. (2012) for a characterization of partial monitoring problems).",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : "In both cases the regret is lower bounded by √ T , which is inapplicable to our setting (see Antos et al. (2012) for a characterization of partial monitoring problems). Bartók et al. Bartók et al. (2012) do present problem dependent bounds.",
      "startOffset" : 93,
      "endOffset" : 204
    }, {
      "referenceID" : 6,
      "context" : "In practice, these two identical alternatives would be displayed as one, as would naturally happen in interleaved retrieval evaluation Chapelle et al. (2012). It should be also clear that playing (x∗, x∗) is pure exploitation, because the feedback is then an unbiased coin with zero exploratory information.",
      "startOffset" : 135,
      "endOffset" : 158
    }, {
      "referenceID" : 3,
      "context" : "2 is implicitly proved in Auer et al. (2002). For completeness, we provide an explicit proof in Appendix A.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "5 This setting was dealt with by Dani et al. (2008). They provide an algorithm for this setting that could be thought of as linear optimization under noisy feedback.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 7,
      "context" : "3 (Dani et al. 2008).",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 7,
      "context" : "3 (Dani et al. 2008). Algorithm CONFIDENCEBALL1 (resp. CONFIDENCEBALL2) of Dani et al. (2008), provides an expected regret ofO (√ dT log T ) (resp.",
      "startOffset" : 3,
      "endOffset" : 94
    }, {
      "referenceID" : 7,
      "context" : "4 (Dani et al. 2008).",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 7,
      "context" : "4 (Dani et al. 2008). Assume the ∆-gap case. Algorithm CONFIDENCEBALL1 (resp. CONFIDENCEBALL2) of Dani et al. (2008), provides an expected regret of O ( ∆−1d2 log T ) (resp.",
      "startOffset" : 3,
      "endOffset" : 117
    }, {
      "referenceID" : 7,
      "context" : "By setting the SBM S used in Line 1 as the algorithms CONFIDENCEBALL1 or CONFIDENCEBALL2 of Dani et al. (2008), we obtain the following:",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 17,
      "context" : "Unlike the algorithms in Yue & Joachims (2011); Yue et al. (2012), we will avoid running an elimination tournament, but just resort to a standard MAB strategy by reduction.",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "Interestingly, the assumption is satisfied by the UCB algorithm Auer et al. (2002) (as detailed in Lemma 2.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : "Interestingly, the assumption is satisfied by the UCB algorithm Auer et al. (2002) (as detailed in Lemma 2.2). Recall that x∗ ∈ X denotes an arm with largest valuation μ(x), and that ∆x := μ(x∗)− μ(x) for all x ∈ X . Assume ∆x > 0 for all x 6= x∗.6 Definition 4.1. Let Tx be the number of times a (sub-optimal) arm x ∈ X is played when running the policy T rounds. A MAB policy is said to be α-robust when it has the following property: for all s ≥ 4α∆−2 x ln(T ), it holds that Pr[Tx > s] < 2 α (s/2) −α. Recall that as discussed in Section 2.1, in Auer et al.’s (2002) classic UCB policy this property can be achieved by slightly enlarging the confidence region.",
      "startOffset" : 64,
      "endOffset" : 571
    }, {
      "referenceID" : 16,
      "context" : "We now present several experiments comparing our algorithms with baselines consisting of the state-of-the-art INTERLEAVED FILTER (IF) Yue et al. (2012) and BEAT THE MEAN BANDIT (BTMB) Yue & Joachims (2011).",
      "startOffset" : 134,
      "endOffset" : 152
    }, {
      "referenceID" : 16,
      "context" : "We now present several experiments comparing our algorithms with baselines consisting of the state-of-the-art INTERLEAVED FILTER (IF) Yue et al. (2012) and BEAT THE MEAN BANDIT (BTMB) Yue & Joachims (2011). Our experiments are exhaustive, as we include scenarios for which no bounds were derived (e.",
      "startOffset" : 134,
      "endOffset" : 206
    }, {
      "referenceID" : 16,
      "context" : "We now present several experiments comparing our algorithms with baselines consisting of the state-of-the-art INTERLEAVED FILTER (IF) Yue et al. (2012) and BEAT THE MEAN BANDIT (BTMB) Yue & Joachims (2011). Our experiments are exhaustive, as we include scenarios for which no bounds were derived (e.g. nonlinear link functions), as well as the much more general scenario in which BTMB was analyzed Yue & Joachims (2011). Henceforth, the set X of arms is {A,B,C,D,E, F}.",
      "startOffset" : 134,
      "endOffset" : 420
    }, {
      "referenceID" : 6,
      "context" : "For applications such as the interleaving search engines Chapelle et al. (2012), 6 arms is realistic.",
      "startOffset" : 57,
      "endOffset" : 80
    } ],
    "year" : 2014,
    "abstractText" : "We present algorithms for reducing the Dueling Bandits problem to the conventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits problem is an online model of learning with ordinal feedback of the form “A is preferred to B” (as opposed to cardinal feedback like “A has value 2.5”), giving it wide applicability in learning from implicit user feedback and revealed and stated preferences. In contrast to existing algorithms for the Dueling Bandits problem, our reductions – named Doubler, MultiSBM and Sparring – provide a generic schema for translating the extensive body of known results about conventional Multi-Armed Bandit algorithms to the Dueling Bandits setting. For Doubler and MultiSBM we prove regret upper bounds in both finite and infinite settings, and conjecture about the performance of Sparring which empirically outperforms the other two as well as previous algorithms in our experiments. In addition, we provide the first almost optimal regret bound in terms of second order terms, such as the differences between the values of the arms.",
    "creator" : "LaTeX with hyperref package"
  }
}