{
  "name" : "1005.2296.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Learning of Noisy Data with Kernels",
    "authors" : [ "Nicolò Cesa-Bianchi", "Shai Shalev Shwartz" ],
    "emails" : [ "cesa-bianchi@dsi.unimi.it", "shais@cs.huji.ac.il", "ohadsh@cs.huji.ac.il" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In many machine learning applications training data are typically collected by measuring certain physical quantities. Examples include bioinformatics, medical tests, robotics, and remote sensing. These measurements have errors that may be due to several reasons: sensor costs, communication constraints, or intrinsic physical limitations. In all such cases, the learner trains on a distorted version of the actual “target” data, which is where the learner’s predictive ability is eventually evaluated. In this work we investigate the extent to which a learning algorithm can achieve a good predictive performance when training data are corrupted by noise with unknown distribution.\nWe prove upper and lower bounds on the learner’s cumulative loss in the framework of online learning, where examples are generated by an arbitrary and possibily adversarial source. We model the measurement error via a random perturbation which affects each instance observed by the learner. We do not assume any specific property of the noise distribution other than zero-mean and bounded variance. Moreover, we allow the noise distribution to change at every step in an adversarial way and fully hidden from the learner. Our positive results are quite general: by using a randomized unbiased estimate for the loss gradient and a randomized feature mapping to estimate kernel values, we show that a variant of online gradient descent can learn functions in any dot-product (e.g., polynomial) or Gaussian RKHS under any given analytic convex loss function. Our techniques are readily extendable to other kernel types as well.\nIn order to obtain unbiased estimates of loss gradients and kernel values, we allow the learner to query a random number of independently perturbed copies of the current unseen instance. We show how low-variance estimates can be computed using a number of queries that is constant with high probability. This is in sharp contrast with standard averaging techniques which attempts to directly estimate the noisy instance, as these require a sample whose size depends on the scale of the problem. Finally, we formally show that learning is impossible, even without kernels, when only one perturbed copy of each instance can be accessed. This is true for essentially any reasonable loss function.\nOur paper is organized as follows. In the next subsection we discuss related work. In Sec. 2 we introduce our setting and justify some of our choices. In Sec. 4 we present our main results but before that, in Sec. 3, we discuss the techniques used to obtain them. In the same section, we also explain why existing techniques are insufficient to deal with our problem. The detailed proofs and subroutine implementations appear in Sec. 5, with some of the more technical lemmas and proofs\nar X\niv :1\n00 5.\n22 96\nv2 [\ncs .L\nG ]\n2 0\nM ay\n2 01\nrelegated to the appendix. We wrap up with a discussion on possible avenues for future work in Sec. 6."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "In the machine learning literature, the problem of learning from noisy examples, and, in particular, from noisy training instances, has traditionally received a lot of attention —see, for example, the recent survey [11]. On the other hand, there are comparably few theoretically-principled studies on this topic. Two of them focus on models quite different from the one studied here: random attribute noise in PAC boolean learning [3, 8], and malicious noise [9, 5]. In the first case, learning is restricted to classes of boolean functions and the noise must be independent across each boolean coordinate. In the second case, an adversary is allowed to perturb a small fraction of the training examples in an arbitrary way, making learning impossible in a strong informational sense unless this perturbed fraction is very small (of the order of the desired accuracy for the predictor).\nThe previous work perhaps closest to the one presented here is [10], where binary classification mistake bounds are proven for the online Winnow algorithm in the presence of attribute errors. Similarly to our setting, the sequence of instances observed by the learner is chosen by an adversary. However, in [10] the noise is generated by an adversary, who may change the value of each attribute in an arbitrary way. The final mistake bound, which only applies when the noiseless data sequence is linearly separable without kernels, depends on the sum of all adversarial perturbations."
    }, {
      "heading" : "2 Setting",
      "text" : "We consider a setting where the goal is to predict values y ∈ R based on instances x ∈ Rd. In this paper we focus on kernel-based linear predictors of the form x 7→ 〈w,Ψ(x)〉, where Ψ is a feature mapping into some reproducing kernel Hilbert space (RKHS). We assume there exists a kernel function that efficiently implements dot products in that space, i.e., k(x,x′) = 〈Ψ(x),Ψ(x′)〉. Note that a special case of this setting is linear kernels, where Ψ(·) is the identity mapping and k(x,x′) = 〈x,x′〉.\nThe standard online learning protocol for linear prediction with kernels is defined as follows: at each round t, the learner picks a linear hypothesis wt from the RKHS. The adversary then picks an example (xt, yt) and reveals it to the learner. The loss suffered by the learner is `(〈wt,Ψ(xt)〉, yt), where ` is a known and fixed loss function. The goal of the learner is to minimize regret with respect to a fixed convex set of hypotheses W, namely\nT∑ t=1 `(〈wt,Ψ(xt)〉, yt)− min w∈W T∑ t=1 `(〈w,Ψ(xt)〉, yt).\nTypically, we wish to find a strategy for the learner, such that no matter what is the adversary’s strategy of choosing the sequence of examples, the expression above is sub-linear in T .\nWe now make the following twist, which limits the information available to the learner: instead of receiving (xt, yt), the learner observes yt and is given access to an oracle At. On each call, At returns an independent copy of xt + Zt, where Zt is a zero-mean random vector with some known finite bound on its variance (in the sense that E [ ‖Zt‖2 ] ≤ a for some uniform constant a). In general, the distribution of Zt is unknown to the learner. It might be chosen by the adversary, and change from round to round or even between consecutive calls to At. Note that here we assume that yt remains unperturbed, but we emphasize that this is just for simplicity - our techniques can be readily extended to deal with noisy values as well.\nThe learner may call At more than once. In fact, as we discuss later on, being able to call At more than once is necessary for the learner to have any hope to succeed. On the other hand, if the learner calls At an unlimited number of times, it can reconstruct xt arbitrarily well by averaging, and we are back to the standard learning setting. In this paper we focus on learning algorithms that call At only a small, essentially constant number of times, which depends only on our choice of loss function and kernel (rather than T , the norm of xt, or the variance of Zt, which will happen with näıve averaging techniques). Moreover, since the number of queries is bounded with very high probability, one can even produce an algorithm with an absolute bound on the number of queries, which will fail or introduce some bias with an arbitrarily small probability. For simplicity, we ignore these issues in this paper.\nIn this setting, we wish to minimize the regret in hindsight with respect to the unperturbed data and averaged over the noise introduced by the oracle, namely\nE [ T∑ t=1 `(〈wt,Ψ(xt)〉, yt)− min w∈W T∑ t=1 `(〈w,Ψ(xt)〉, yt) ] (1)\nwhere the random quantities are the predictors w1,w2, . . . generated by the learner, which depend on the observed noisy instances (in the appendix, we briefly discuss alternative regret measures, and why they are unsatisfactory). This kind of regret is relevant where we actually wish to learn from data, without the noise causing a hindrance. In particular, consider the batch setting, where the examples {(xt, yt)}Tt=1 are actually sampled i.i.d. from some unknown distribution, and we wish to find a predictor which minimizes the expected loss E[`(〈w,x〉, y)] with respect to new examples (x, y). Using standard online-to-batch conversion techniques, if we can find an online algorithm with a sublinear bound on Eq. (1), then it is possible to construct learning algorithms for the batch setting which are robust to noise. That is, algorithms generating a predictor w with close to minimal expected loss E[`(〈w,x〉, y)] among all w ∈ W.\nWhile our techniques are quite general, the exact algorithmic and theoretical results depend a lot on which loss function and kernel is used. Discussing the loss function first, we will assume that `(〈w,Ψ(x)〉, y) is a convex function of w for each example (x, y). Somewhat abusing notation, we assume the loss can be written either as `(〈w,Ψ(x)〉, y) = f(y〈w,Ψ(x)〉) or as `(〈w,Ψ(x)〉, y) = f(〈w,Ψ(x)〉 − y) for some function f . We refer to the first type as classification losses, as it encompasses most reasonable losses for classification, where y ∈ {−1,+1} and the goal is to predict the label. We refer to the second type as regression losses, as it encompasses most reasonable regression losses, where y takes arbitrary real values. For simplicity, we present some of our results in terms of classification losses, but they all hold for regression losses as well with slight modifications.\nWe present our results under the assumption that the loss function is “smooth”, in the sense that `′(a) can be written as ∑∞ n=0 γna\nn, for any a in its domain. This assumption holds for instance for the squared loss `(a) = a2, the exponential loss `(a) = exp(a), and smoothed versions of loss functions such as the hinge loss and the absolute loss (we discuss examples in more details in Subsection 4.2). This assumption can be relaxed under certain conditions, and this is further discussed in Subsection 3.2.\nTurning to the issue of kernels, we note that the general presentation of our approach is somewhat hampered by the fact that it needs to be tailored to the kernel we use. In this paper, we focus on two families of kernels: Dot Product Kernels: the kernel k(x,x′) can be written as a function of 〈x,x′〉. Examples of such kernels k(x,x′) are linear kernels 〈x,x′〉; homogeneous polynomial kernels (〈x,x′〉)n, inhomogeneous polynomial kernels (1 + 〈x,x′〉)n; exponential kernels e〈x,x′〉; binomial kernels (1 + 〈x,x′〉)−α, and more (see for instance [14, 16]).\nGaussian Kernels: k(x,x′) = e−‖x−x ′‖2/σ2 for some σ2 > 0.\nAgain, we emphasize that our techniques are extendable to other kernel types as well."
    }, {
      "heading" : "3 Techniques",
      "text" : "Our results are based on two key ideas: the use of online gradient descent algorithms, and construction of unbiased gradient estimators in the kernel setting. The latter is based on a general method to build unbiased estimators for non-linear functions, which may be of independent interest."
    }, {
      "heading" : "3.1 Online Gradient Descent",
      "text" : "There exist well developed theory and algorithms for dealing with the standard online learning setting, where the example (xt, yt) is revealed after each round, and for general convex loss functions. One of the simplest and most well known ones is the online gradient descent algorithm due to Zinkevich [17]. Since this algorithm forms a basis for our algorithm in the new setting, we briefly review it below (as adapted to our setting).\nThe algorithm initializes the classifier w1 = 0. At round t, the algorithm predicts according to wt, and updates the learning rule according to wt+1 = P ( wt − ηt∇t ) , where ηt is a suitably chosen\nconstant which might depend on t; ∇t = `′ ( yt〈wt,Ψ(xt)〉 ) ytΨ(xt) is the gradient of ` ( yt〈w,Ψ(xt)〉 ) with respect to wt; and P is a projection operator on the convex set W, on whose elements we wish to achieve low regret. In particular, if we wish to compete with hypotheses of bounded squared norm Bw, P simply involves rescaling the norm of the predictor so as to have squared norm at most Bw. With this algorithm, one can prove regret bounds with respect to any w ∈ W.\nA “folklore” result about this algorithm is that in fact, we do not need to update the predictor by the gradient at each step. Instead, it is enough to update by some random vector of bounded variance, which merely equals the gradient in expectation. This is a useful property in settings where (xt, yt) is not revealed to the learner, and has been used before, such as in the online bandit setting (see for instance [6, 7, 1]). Here, we will use this property in a new way, in order to devise\nalgorithms which are robust to noise. When the kernel and loss function are linear (e.g., Ψ(x) = x and `(a) = ca+ b for some constants b, c), this property already ensures that the algorithm is robust to noise without any further changes. This is because the noise injected to each xt merely causes the exact gradient estimate to change to a random vector which is correct in expectation: If we assume ` is a classification loss, then\nE [`′(yt〈wt,Ψ(x̃t)〉)Ψ(x̃t)] = E [cx̃t] = xt.\nOn the other hand, when we use nonlinear kernels and nonlinear loss functions, using standard online gradient descent leads to systematic and unknown biases (since the noise distribution is unknown), which prevents the method from working properly. To deal with this problem, we now turn to describe a technique for estimating expressions such as `′ ( yt〈wt,Ψ(xt)〉 ) in an unbiased manner. In Subsection 3.3, we discuss how Ψ(xt) can be estimated in an unbiased manner."
    }, {
      "heading" : "3.2 Unbiased Estimators for Non-Linear Functions",
      "text" : "Suppose that we are given access to independent copies of a real random variable X, with expectation E[X], and some real function f , and we wish to construct an unbiased estimate of f(E[X]). If f is a linear function, then this is easy: just sample x from X, and return f(x). By linearity, E[f(X)] = f(E[X]) and we are done. The problem becomes less trivial when f is a general, nonlinear function, since usually E[f(X)] 6= f(E[X]). In fact, when X takes finitely many values and f is not a polynomial function, one can prove that no unbiased estimator can exist (see [13], Proposition 8 and its proof). Nevertheless, we show how in many cases one can construct an unbiased estimator of f(E[X]), including cases covered by the impossibility result. There is no contradiction, because we do not construct a “standard” estimator. Usually, an estimator is a function from a given sample to the range of the parameter we wish to estimate. An implicit assumption is that the size of the sample given to it is fixed, and this is also a crucial ingredient in the impossibility result. We circumvent this by constructing an estimator based on a random number of samples.\nHere is the key idea: suppose f : R → R is any function continuous on a bounded interval. It is well known that one can construct a sequence of polynomials (Qn(·))∞n=1, where Qn(·) is a polynomial of degree n, which converges uniformly to f on the interval. If Qn(x) = ∑n i=0 γn,ix i, let\nQ′n(x1, . . . , xn) = ∑n i=0 γn,i ∏i j=1 xj . Now, consider the estimator which draws a positive integer N according to some distribution P(N = n) = pn, samples X for N times to get x1, x2, . . . , xN , and returns 1pN ( Q′N (x1, . . . , xN )−Q′N−1(x1, . . . , xN−1) ) , where we assume Q′0 = 0. The expected value of this estimator is equal to:\nEN,x1,...,xN [ 1\npN\n( Q′N (x1, . . . , xN )−Q′N−1(x1, . . . , xN−1) )] =\n∞∑ n=1 pn pn Ex1,...,xn [ Q′n(x1, . . . , xn)−Q′n−1(x1, . . . , xn−1) ] =\n∞∑ n=1 ( Qn(E[X])−Qn−1(E[X]) ) = f(E[X]).\nThus, we have an unbiased estimator of f(E[X]). This technique appeared in a rather obscure early 1960’s paper [15] from sequential estimation theory, and appears to be little known, particularly outside the sequential estimation community. However, we believe this technique is interesting, and expect it to have useful applications for other problems as well.\nWhile this may seem at first like a very general result, the variance of this estimator must be bounded for it to be useful. Unfortunately, this is not true for general continuous functions. More precisely, let N be distributed according to pn, and let θ be the value returned by the estimator. In [2], it is shown that if X is a Bernoulli random variable, and if E[θNk] <∞ for some integer k ≥ 1, then f must be k times continuously differentiable. Since E[θNk] ≤ (E[θ2] + E[N2k])/2, this means that functions f which yield an estimator with finite variance, while using a number of queries with bounded variance, must be continuously differentiable. Moreover, in case we desire the number of queries to be essentially constant (i.e. choose a distribution for N with exponentially decaying tails), we must have E[Nk] < ∞ for all k, which means that f should be infinitely differentiable (in fact, in [2] it is conjectured that f must be analytic in such cases).\nThus, we focus in this paper on functions f which are analytic, i.e., they can be written as f(x) = ∑∞ i=0 γix i for appropriate constants γ0, γ1, . . .. In that case, Qn can simply be the truncated\nTaylor expansion of f to order n, i.e., Qn = ∑n i=0 γix\ni. Moreover, we can pick pn ∝ 1/pn for any p > 1. So the estimator becomes the following: we sample a nonnegative integer N according to P(N = n) = (p − 1)/pn+1, sample X independently N times to get x1, x2, . . . , xN , and return θ = γN pN+1 p−1 x1x2 · · ·xN where we set θ = p p−1γ0 if N = 0. 1 We have the following:\nLemma 1. For the above estimator, it holds that E[θ] = f(E[X]). The expected number of samples used by the estimator is 1/(p− 1), and the probability of it being at least z is p−z. Moreover, if we assume that f+(x) = ∑∞ n=0 |γn|xn exists for any x in the domain of interest, then\nE[θ2] ≤ p p− 1 f2+\n(√ pE[X2] ) .\nProof. The fact that E[θ] = f(E[X]) follows from the discussion above. The results about the number of samples follow directly from properties of the geometric distribution. As for the second moment, E[θ2] equals\nEN,x1,...,xN [ γ2N p2(N+1)\n(p− 1)2 x21x 2 2 · · ·x2N\n] = ∞∑ n=0 (p− 1)p2(n+1) (p− 1)2pn+1 γ2nEx1,...,xn [ x21x 2 2 · · ·x2n ] = p\np− 1 ∞∑ n=0 γ2np n ( E[X2] )n =\np\np− 1 ∞∑ n=0 ( |γn| (√ pE[X2] )n)2 ≤ p p− 1 ( ∞∑ n=0 |γn| (√ pE[X2] )n)2 = p p− 1 f2+ (√ pE[X2] ) .\nThe parameter p provides a tradeoff between the variance of the estimator and the number of samples needed: the larger is p, the less samples do we need, but the estimator has more variance. In any case, the sample size distribution decays exponentially fast, so the sample size is essentially bounded.\nIt should be emphasized that the estimator associated with Lemma 1 is tailored for generality, and is suboptimal in some cases. For example, if f is a polynomial function, then γn = 0 for sufficiently large n, and there is no reason to sample N from a distribution supported on all nonnegative integers - it just increases the variance. Nevertheless, in order to keep the presentation unified and general, we will always use this type of estimator. If needed, the estimator can always be optimized for specific cases.\nWe also note that this technique can be improved in various directions, if more is known about the distribution of X. For instance, if we have some estimate of the expectation and variance of X, then we can perform a Taylor expansion around the estimated E[X] rather than 0, and tune the probability distribution of N to be different than the one we used above. These modifications can allow us to make the variance of the estimator arbitrarily small, if the variance of X is small enough. Moreover, one can take polynomial approximations to f which are perhaps better than truncated Taylor expansions. In this paper, for simplicity, we will ignore these potential improvements.\nFinally, we note that a related result in [2] implies that it is impossible to estimate f(E[X]) in an unbiased manner when f is discontinuous, even if we allow a number of queries and estimator values which are infinite in expectation. Therefore, since the derivative of the hinge loss is not continuous, estimating in an unbiased manner the gradient of the hinge loss with arbitrary noise appears to be impossible. Thus, if online learning with noise and hinge loss is at all feasible, a rather different approach than ours will need to be taken."
    }, {
      "heading" : "3.3 Unbiasing Noise in the RKHS",
      "text" : "The third component of our approach involves the unbiased estimation of Ψ(xt), when we only have unbiased noisy copies of xt. Here again, we have a non-trivial problem, because the feature mapping Ψ is usually highly non-linear, so E[Ψ(x̃t)] 6= Ψ(E[x̃t]) in general. Moreover, Ψ is not a scalar function, so the technique of Subsection 3.2 will not work as-is.\n1 Admittedly, the event N = 0 should receive zero probability, as it amounts to “skipping” the sampling altogether. However, setting P(N = 0) = 0 appears to improve the bound in this paper only in the smaller order terms, while making the analysis in the paper more complicated.\nTo tackle this problem, we construct an explicit feature mapping, which needs to be tailored to the kernel we want to use. To give a very simple example, suppose we use the homogeneous 2nddegree polynomial kernel, k(r, s) = (〈r, s〉)2. It is not hard to verify that the function Ψ : Rd 7→ Rd2 , defined via Ψ(x) = (x1x1, x1x2, . . . , xdxd), is an explicit feature mapping for this kernel. Now, if we query two independent noisy copies x̃, x̃′ of x, we have that the expectation of the random vector (x̃1x̃ ′ 1, x̃1x̃ ′ 2, . . . , x̃dx̃ ′ d) is nothing more than Ψ(x). Thus, we can construct unbiased estimates of Ψ(x) in the RKHS. Of course, this example pertains to a very simple RKHS with a finite dimensional representation. By a randomization trick somewhat similar to the one in Subsection 3.2, we can adapt this approach to infinite dimensional RKHS as well. In a nutshell, we represent Ψ(x) as an infinite-dimensional vector, and its noisy unbiased estimate is a vector which is non-zero on only finitely many entries, using finitely many noisy queries. Moreover, inner products between these estimates can be done efficiently, allowing us to implement the learning algorithms, and use the resulting predictor on test instances."
    }, {
      "heading" : "4 Main Results",
      "text" : ""
    }, {
      "heading" : "4.1 Algorithm",
      "text" : "We present our algorithmic approach in a modular form. We start by introducing the main algorithm, which contains several subroutines. Then we prove our two main results, which bound the regret of the algorithm, the number of queries to the oracle, and the running time for two types of kernels: dot product and Gaussian (our results can be extended to other kernel types as well). In itself, the algorithm is nothing more than a standard online gradient descent algorithm with a standard O( √ T ) regret bound. Thus, most of the proofs are devoted to a detailed discussion of how the subroutines are implemented (including explicit pseudo-code). In this section, we just describe one subroutine, based on the techniques discussed in Sec. 3. The other subroutines require a more detailed and technical discussion, and thus their implementation is described as part of the proofs in Sec. 5. In any case, the intuition behind the implementations and the techniques used are described in Sec. 3.\nFor simplicity, we will focus on a finite-horizon setting, where the number of online rounds T is fixed and known to the learner. The algorithm can easily be modified to deal with the infinite horizon setting, where the learner needs to achieve sub-linear regret for all T simultaneously. Also, for the remainder of this subsection, we assume for simplicity that ` is a classification loss, namely can be written as a function of `(y〈w,Ψ(x)〉). It is not hard to adapt the results below to the case where ` is a regression loss (where ` is a function of 〈w,Ψ(x)〉 − y).\nWe note that at each round, the algorithm below constructs an object which we denote as Ψ̃(xt). This object has two interpretations here: formally, it is an element of a reproducing kernel Hilbert space (RKHS) corresponding to the kernel we use, and is equal in expectation to Ψ(xt). However, in terms of implementation, it is simply a data structure consisting of a finite set of vectors from Rd. Thus, it can be efficiently stored in memory and handled even for infinite-dimensional RKHS.\nAlgorithm 1 Kernel Learning Algorithm with Noisy Input\nParameters: Learning rate η > 0, number of rounds T , sample parameter p > 1. Initialize:\nαi = 0 for all i = 1, . . . , T . Ψ̃(xi) for all i = 1, . . . , T\n// Ψ̃(xi) is a data structure which can store a variable number of vectors in Rd For t = 1 . . . T\nDefine wt = ∑t−1 i=1 αiΨ̃(xi) Receive At, yt // The oracle At provides noisy estimates of xt Let Ψ̃(xt) := Map Estimate(At, p) // Get unbiased estimate of Ψ(xt) in the RKHS Let g̃t := Grad Length Estimate(At, yt, p) // Get unbiased estimate of `\n′(yt〈wt,Ψ(xt)〉) Let αt := −g̃tη/ √ T // Perform gradient step\nLet ñt := ∑t i=1 ∑t j=1 αt,iαt,jProd(Ψ̃(xi), Ψ̃(xj)) // Compute squared norm, where Prod(Ψ̃(xi), Ψ̃(xj)) returns 〈Ψ̃(xi), Ψ̃(xj)〉 If ñt > Bw // If norm squared is larger than Bw, then project\nLet αi := αi √ Bw ñt for all i = 1, . . . , t\nLike Ψ̃(xt), wt+1 has also two interpretations: formally, it is an element in the RKHS, as defined\nin the pseudocode. In terms of implementation, it is defined via the data structures Ψ̃(x1), . . . , Ψ̃(xt) and the values of α1, . . . , αt at round t. To apply this hypothesis on a given instance x, we compute∑t i=1 αt,iProd(Ψ̃(xi),x ′), where Prod(Ψ̃(xi),x ′) is a subroutine which returns 〈Ψ̃(xi),Ψ(x′)〉 (a pseudocode is provided as part of the proofs later on). We now turn to the main results pertaining to the algorithm. The first result shows what regret bound is achievable by the algorithm for any dot-product kernel, as well as characterize the number of oracle queries per instance, and the overall running time of the algorithm.\nTheorem 1. Assume that the loss function ` has an analytic derivative `′(a) = ∑∞ n=0 γna\nn for all a in its domain, and let `′+(a) = ∑∞ n=0 |γn|an (assuming it exists). Assume also that the kernel k(x,x′) can be written as Q(〈x,x′〉) for all x,x′ ∈ Rd. Finally, assume that E[‖x̃t‖2] ≤ Bx̃ for any x̃t returned by the oracle at round t, for all t = 1, . . . , T . Then, for all Bw > 0 and p > 1, it is possible to implement the subroutines of Algorithm 1 such that:\n• The expected number of queries to each oracle At is p(p−1)2 . • The expected running time of the algorithm is O ( T 3 (\n1 + dp(p−1)2\n)) .\n• If we run Algorithm 1 with η = Bw /√ u`′+ (√ (p− 1)u ) , where u = Bw ( p p−1 )2 Q(pBx̃), then\nE [ T∑ t=1 `(yt〈wt,Ψ(xt)〉)− min w : ‖w‖2≤Bw T∑ t=1 `(yt〈w,Ψ(xt)〉) ] ≤ `′+ (√ (p− 1)u )√ uT .\nThe expectations are with respect to the randomness of the oracles and the algorithm throughout its run.\nWe note that the distribution of the number of oracle queries can be specified explicitly, and it decays very rapidly - see the proof for details. Also, for simplicity, we only bound the expected regret in the theorem above. If the noise is bounded almost surely or with sub-Gaussian tails (rather than just bounded variance), then it is possible to obtain similar guarantees with high probability, by relying on Azuma’s inequality or variants thereof (see for example [4]).\nWe now turn to the case of Gaussian kernels. Theorem 2. Assume that the loss function ` has an analytic derivative `′(a) = ∑∞ n=0 γna\nn for all a in its domain, and let `′+(a) = ∑∞ n=0 |γn|an (assuming it exists). Assume that the kernel k(x,x′) is defined as exp(−‖x − x‖2/σ2). Finally, assume that E[‖x̃t‖2] ≤ Bx̃ for any x̃t returned by the oracle at round t, for all t = 1, . . . , T . Then for all Bw > 0 and p > 1 it is possible to implement the subroutines of Algorithm 1 such that\n• The expected number of queries to each oracle At is 3p(p−1)2 . • The expected running time of the algorithm is O ( T 3 (\n1 + dp(p−1)2\n)) .\n• If we run Algorithm 1 with η = Bw /√ u`′+ (√ (p− 1)u ) , where\nu = Bw\n( p\np− 1\n)3 exp (√ pBx̃ + 2p √ Bx̃\nσ2 ) then\nE [ T∑ t=1 `(yt〈wt,Ψ(xt)〉)− min w : ‖w‖2≤Bw T∑ t=1 `(yt〈w,Ψ(xt)〉) ] ≤ `′+( √ (p− 1)u) √ uT .\nThe expectations are with respect to the randomness of the oracles and the algorithm throughout its run.\nAs in Thm. 1, note that the number of oracle queries has a fast decaying distribution. Also, note that with Gaussian kernels, σ2 is usually chosen to be on the order of the example’s squared norms. Thus, if the noise added to the examples is proportional to their original norm, we can assume that Bx̃/σ\n2 = O(1), and thus u which appears in the bound is also bounded by a constant. As previously mentioned, most of the subroutines are described in the proofs section, as part of the proof of Thm. 1. Here, we only show how to implement Grad Length Estimate subroutine,\nwhich returns the gradient length estimate g̃t. The idea is based on the technique described in Subsection 3.2. We prove that g̃t is an unbiased estimate of `\n′(yt〈wt,Ψ(xt)〉), and bound E[g̃2t ]. As discussed earlier, we assume that `′(·) is analytic and can be written as `′(a) = ∑∞ n=0 γna n.\nSubroutine 1 Grad Length Estimate(At, yt, p)\nSample nonnegative integer n according to P(n) = (p− 1)/pn+1 For j = 1, . . . , n\nLet Ψ̃(xt)j := Map Estimate(At) // Get unbiased estimate of Ψ(xt) in the RKHS\nReturn g̃t := ytγn pn+1 p−1 ∏n j=1 (∑t−1 i=1 αt−1,iProd(Ψ̃(xi), Ψ̃(xt)j) )\nLemma 2. Assume that E[Ψ̃(xt)] = Ψ(xt), and that Prod(Ψ̃(x), Ψ̃(x′)) returns 〈Ψ̃(x), Ψ̃(x′)〉 for all x,x′. Then for any given wt = αt−1,1Ψ̃(x1) + · · ·+ αt−1,t−1Ψ̃(xt−1) it holds that\nEt[g̃t] = yt`′(yt〈wt,Ψ(xt)〉) and Et[g̃2t ] ≤ p p− 1 ` ′ +\n(√ pBwBΨ̃(x) )2 where the expectation is with respect to the randomness of Subroutine 1, and `′+(a) = ∑∞ n=0 |γn|an.\nProof. The result follows from Lemma 1, where g̃t corresponds to the estimator θ, the function f corresponds to `′, and the random variable X corresponds to 〈wt, Ψ̃(xt)〉 (where Ψ̃(xt) is random and wt is held fixed). The term E[X2] in Lemma 1 can be upper bounded as\nEt [( 〈wt, Ψ̃(xt)〉 )2] ≤ ‖wt‖2 Et [‖Ψ̃(xt)‖2] ≤ BwBΨ̃(x) ."
    }, {
      "heading" : "4.2 Loss Function Examples",
      "text" : "Theorems 1 and 2 both deal with generic loss functions ` whose derivative can be written as∑∞ n=0 γna n, and the regret bounds involve the functions `′+(a) = ∑∞ n=0 |γn|an. Below, we present a few examples of loss functions and their corresponding `′+. As mentioned earlier, while the theorems in the previous subsection are in terms of classification losses (i.e., ` is a function of y〈w,Ψ(x)〉), virtually identical results can be proven for regression losses (i.e., ` is a function of 〈w,Ψ(x)〉 − y), so we will give examples from both families. Working out the first two examples is straightforward. The proofs of the other two appear in Sec. 5. The loss functions are illustrated graphically in Fig. 1.\nExample 1. For the squared loss function, `(〈w,x〉, y) = (〈w,x〉− y)2, we have `′+ (√ (p− 1)u) ) =\n2 √ (p− 1)u.\nExample 2. For the exponential loss function, `(〈w,x〉, y) = ey〈w,x〉, we have `′+ (√ (p− 1)u ) = e √ (p−1)u.\nExample 3. Consider a “smoothed” absolute loss function `σ(〈w,Ψ(x)〉 − y), defined as an antiderivative of Erf(sa) for some s > 0 (see proof for exact analytic form). Then we have that\n`′+ (√ (p− 1)u ) ≤ 12 +\n1 s √ π(p−1)u\n( es 2(p−1)u − 1 ) .\nExample 4. Consider a “smoothed” hinge loss `(y〈w,Ψ(x)〉), defined as an antiderivative of (Erf(s(a − 1)) − 1)/2 for some s > 0 (see proof for exact analytic form). Then we have that `′+ (√ (p− 1)u ) ≤ 2\ns √ π(p−1)u\n( es 2(p−1)u−1 ) .\nFor any s, the loss function in the last two examples are convex, and respectively approximate the absolute loss ∣∣〈w,Ψ(x)〉− y∣∣ and the hinge loss max{0, 1− y〈w,Ψ(x)〉} arbitrarily well for large enough s. Fig. 1 shows these loss functions graphically for s = 1. Note that s need not be large in order to get a good approximation. Also, we note that both the loss itself and its gradient are computationally easy to evaluate.\nFinally, we remind the reader that as discussed in Subsection 3.2, performing an unbiased estimate of the gradient for non-differentiable losses directly (such as the hinge loss or absolute loss) appears to be impossible in general. On the flip side, if one is willing to use a random number of queries with polynomial rather than exponential tails, then one can achieve much better sample complexity results, by focusing on loss functions (or approximations thereof) which are only differentiable to a bounded order, rather than fully analytic. This again demonstrates the tradeoff between the sample size and the amount of information that needs to be gathered on each training example."
    }, {
      "heading" : "4.3 One Noisy Copy is Not Enough",
      "text" : "The previous results might lead one to wonder whether it is really necessary to query the same instance more than once. In some applications this is inconvenient, and one would prefer a method which works when just a single noisy copy of each instance is made available. In this subsection we show that, unfortunately, such a method cannot be found. Specifically, we prove that under very mild assumptions, no method can achieve sub-linear regret when it has access to just a single noisy copy of each instance. On the other hand, for the case of squared loss and linear kernels, our techniques can be adapted to work with exactly two noisy copies of each instance,2 so without further assumptions, the lower bound that we prove here is indeed tight. For simplicity, we prove the result for linear kernels (i.e., where k(x,x′) = 〈x,x′〉). It is an interesting open problem to show improved lower bounds when nonlinear kernels are used. We also note that the result crucially relies on the learner not knowing the noise distribution, and we leave to future work the investigation of what happens when this assumption is relaxed.\nTheorem 3. Let W be a compact convex subset of Rd, and let `(·, 1) : R 7→ R satisfies the following: (1) it is bounded from below; (2) it is differentiable at 0 with `′(0, 1) < 0. For any learning algorithm which selects hypotheses from W and is allowed access to a single noisy copy of the instance at each round t, there exists a strategy for the adversary such that the sequence w1,w2, . . . of predictors output by the algorithm satisfies\nlim sup T→∞ max w∈W\n1\nT T∑ t=1 ( `(〈wt,xt〉, yt)− `(〈w,xt〉, yt) ) > 0\nwith probability 1 with respect to the randomness of the oracles.\nNote that condition (1) is satisfied by virtually any loss function other than the linear loss, while condition (2) is satisfied by most regression losses, and by all classification calibrated losses, which include all reasonable losses for classification (see [12]). The most obvious example where the conditions are not satisfied is when `(·, 1) is a linear function. This is not surprising, because when `(·, 1) is linear, the learner is always robust to noise (see the discussion at Sec. 3).\nThe intuition of the proof is very simple: the adversary chooses beforehand whether the examples are drawn i.i.d. from a distribution D, and then perturbed by noise, or drawn i.i.d. from some other distribution D′ without adding noise. The distributions D,D′ and the noise are designed so that the examples observed by the learner are distributed in the same way irrespective to which of the two sampling strategies the adversary chooses. Therefore, it is impossible for the learner accessing a single copy of each instance to be statistically consistent with respect to both distributions simultaneously. As a result, the adversary can always choose a distribution on which the algorithm will be inconsistent, leading to constant regret. The full proof is presented in Section 5.3.\n2 In a nutshell, for squared loss and linear kernels, we just need to estimate 2(〈wt,xt〉 − yt)xt in an unbiased manner at each round t. This can be done by computing 2(〈wt, x̃t〉 − yt)x̃′t, where x̃t, x̃′t are two noisy copies of xt."
    }, {
      "heading" : "5 Proofs",
      "text" : "Due to the lack of space, some of the proofs are given in the the appendix."
    }, {
      "heading" : "5.1 Preliminary Result",
      "text" : "To prove Thm. 1 and Thm. 2, we need a theorem which basically states that if all subroutines in algorithm 1 behave as they should, then one can achieve an O( √ T ) regret bound. This is provided in the following theorem, which is an adaptation of a standard result of online convex optimization (see, e.g., [17]). The proof is given in Appendix D.\nTheorem 4. Assume the following conditions hold with respect to Algorithm 1:\n1. For all t, Ψ̃(xt) and g̃t are independent of each other (as random variables induced by the\nrandomness of Algorithm 1) as well as independent of any Ψ̃(xi) and g̃i for i < t."
    }, {
      "heading" : "2. For all t, E[Ψ̃(xt)] = Ψ(xt), and there exists a constant BΨ̃ > 0 such that E[‖Ψ̃(xt)‖2] ≤ BΨ̃.",
      "text" : "3. For all t, E[g̃t] = yt`′(yt〈wt,Ψ(xt)〉), and there exists a constant Bg̃ > 0 such that E[g̃2t ] ≤ Bg̃. 4. For any pair of instances x,x′, Prod(Ψ̃(x), Ψ̃(x′)) = 〈Ψ̃(x), Ψ̃(x′)〉.\nThen if Algorithm 1 is run with η = √\nBw Bg̃BΨ̃ , the following inequality holds\nE [ T∑ t=1 ` ( yt〈wt,Ψ(xt)〉 ) − min w : ‖w‖2≤Bw T∑ t=1 ` ( yt〈w,Ψ(xt)〉 )] ≤ √ BwBg̃BΨ̃T .\nwhere the expectation is with respect to the randomness of the oracles and the algorithm throughout its run."
    }, {
      "heading" : "5.2 Proof of Thm. 1",
      "text" : "In this subsection, we present the proof of Thm. 1. We first show how to implement the subroutines of Algorithm 1, and prove the relevant results on their behavior. Then, we prove the theorem itself.\nIt is known that for k(·, ·) = Q(〈x,x′〉) to be a valid kernel, it is necessary that Q(〈x,x′〉) can be written as a Taylor expansion ∑∞ n=0 βn(〈x,x′〉)n, where βn ≥ 0 (see theorem 4.19 in [14]). This makes these types of kernels amenable to our techniques. We start by constructing an explicit feature mapping Ψ(·) corresponding to the RKHS induced by our kernel. For any x,x′, we have that\nk(x,x′) = ∞∑ n=0 βn(〈x,x′〉)n = ∞∑ n=0 βn ( d∑ i=1 xix ′ i )n\n= ∞∑ n=0 βn d∑ k1=1 · · · d∑ kn=1 xk1xk2 · · ·xknx′k1x ′ k2 · · ·x ′ kn\n= ∞∑ n=0 d∑ k1=1 · · · d∑ kn=1 (√ βnxk1xk2 · · ·xkn )(√ βnx ′ k1x ′ k2 · · ·x ′ kn ) .\nThis suggests the following feature representation: for any x, Ψ(x) returns an infinite-dimensional vector, indexed by n and k1, . . . , kn ∈ {1, . . . , d}, with the entry corresponding to n, k1, . . . , kn being√ βnxk1 · · ·xkn . The dot product between Ψ(x) and Ψ(x′) is similar to a standard dot product between two vectors, and by the derivation above equals k(x,x′) as required. We now use a slightly more elaborate variant of our unbiased estimate technique, to derive an unbiased estimate of Ψ(x). First, we sample N according to P(N = n) = (p − 1)/pn+1. Then, we query the oracle for x for N times to get x̃(1), . . . , x̃(N), and formally define Ψ̃(x) as\nΨ̃(x) = √ βn pn+1\np− 1 d∑ k1=1 · · · d∑ kn=1 x̃ (1) k1 · · · x̃(n)kn en,k1,...,kn (2)\nwhere en,k1,...,kn represents the unit vector in the direction indexed by n, k1, . . . , kn as explained above. Since the oracle queries are i.i.d., the expectation of this expression is ∞∑ n=0 p− 1 pn+1 √ βn pn+1 p− 1 d∑ k1=1 · · · d∑ kn=1 E [ x̃ (1) k1 · · · x̃(n)kn ] en,k1,...,kn = ∞∑ n=0 d∑ k1=1 · · · d∑ kn=1 √ βnx (1) k1 · · ·x(n)kn en,k1,...,kn\nwhich is exactly Ψ(x). We formalize the needed properties of Ψ̃(x) in the following lemma.\nLemma 3. Assuming Ψ̃(x) is constructed as in the discussion above, it holds that E[Ψ̃(x)] = Ψ(x) for any x. Moreover, if the noisy samples x̃t returned by the oracle At satisfy E[‖x̃t‖2] ≤ Bx̃, then\nE [ ‖Ψ̃(xt)‖2 ] ≤ p p− 1 Q(pBx̃)\nwhere we recall that Q defines the kernel by k(x,x′) = Q(〈x,x′〉).\nProof. The first part of the lemma follows from the discussion above. As to the second part, note that by (2), E [ ‖Ψ̃(xt)‖2 ] = E βn p2n+2 (p− 1)2 d∑ k1...,kn=1 ( x̃ (1) t,k1 · · · x̃(N)t,kn )2 = E βn p2n+2 (p− 1)2 n∏ j=1 ∥∥x̃(j)t ∥∥2 \n= ∞∑ n=0 p− 1 pn+1 βn p2n+2 (p− 1)2 ( E [ x̃2t ])n = p p− 1 ∞∑ n=0 βn ( pE [ x̃2t ])n ≤ p p− 1 ∞∑ n=0 βn ( pBx̃ )n = p p− 1 Q(pBx̃)\nwhere the second-to-last step used the fact that βn ≥ 0 for all n.\nOf course, explicitly storing Ψ̃(x) as defined above is infeasible, since the number of entries is\nhuge. Fortunately, this is not needed: we just need to store x̃ (1) t , . . . , x̃ (N) t . The representation above is used implicitly when we calculate dot products between Ψ̃(x) and other elements in the RKHS, via the subroutine Prod. We note that while N is a random quantity which might be unbounded, its distribution decays exponentially fast, so the number of vectors to store is essentially bounded.\nAfter the discussion above, the pseudocode for Map Estimate below should be self-explanatory.\nSubroutine 2 Map Estimate(At, p)\nSample nonnegative integer N according to P(N = n) = (p− 1)/pn+1 Query At for N times to get x̃ (1) t , . . . , x̃ (N) t Return x̃ (1) t , . . . , x̃ (N) t as Ψ̃(xt).\nWe now turn to the subroutine Prod, which given two elements in the RKHS, returns their dot product. This subroutine comes in two flavors: either as a procedure defined over Ψ̃(x), Ψ̃(x′) and returning 〈Ψ̃(x), Ψ̃(x′)〉 (Subroutine 3); or as a procedure defined over Ψ̃(x),x′ (Subroutine 4, where the second element is an explicitely given vector) and returning 〈Ψ̃(x),Ψ(x′)〉. This second variant of Prod is needed when we wish to apply the learned predictor on a new given instance x′.\nSubroutine 3 Prod(Ψ̃(x), Ψ̃(x′))\nLet x(1), . . . ,x(n) be the index and vectors comprising Ψ(x) Let x′(1), . . . ,x′(n ′) be the index and vectors comprising Ψ(x′) If n 6= n′ return 0, else return βn p 2n+2 (p−1)2 ∏n j=1〈x̃(j), x̃′(j)〉\nLemma 4. Prod(Ψ̃(x), Ψ̃(x′)) returns 〈Ψ̃(x)Ψ̃(x′)〉.\nProof. Using the formal representation of Ψ̃(x), Ψ̃(x′) in (2), we have that 〈Ψ̃(x), Ψ̃(x′)〉 is 0 whenever n 6= n′ (because then these two elements are composed of different unit vectors with respect to an orthogonal basis). Otherwise, we have that\n〈Ψ̃(x)Ψ̃(x′)〉 = βn p2n+2 (p− 1)2 d∑\nk1,...,kn=1\nx̃ (1) k1 · · · x̃(n)kn x̃ ′(1) k1 · · · x̃′(n)kn\n= βn p2n+2\n(p− 1)2\n( d∑\nk1=1\nx̃ (1) k1 x̃ ′(1) k1\n) · · · ( d∑\nkN=1\nx̃ (n) kN x̃ ′(n) kN ) = βn p2n+2 (p− 1)2 N∏ j=1 ( 〈x̃(j), x̃′(j)〉 ) which is exactly what the algorithm returns, hence the lemma follows.\nThe pseudocode for calculating the dot product 〈Ψ̃(x),Ψ(x′)〉 (where x′ is known) is very similar, and the proof is essentially the same.\nSubroutine 4 Prod(Ψ̃(x),x′)\nLet n,x(1), . . . ,x(n) be the index and vectors comprising Ψ(x) Return βn pn+1 p−1 ∏n j=1〈x̃(j),x′〉\nWe are now ready to prove Thm. 1. First, regarding the expected number of queries, notice that to run Algorithm 1, we invoke Map Estimate and Grad Length Estimate once at round t. Map Estimate uses a random number B of queries distributed as P(B = n) = (p − 1)/pn+1, and Grad Length Estimate invokes Map Estimate a random number C of times, distributed as P(C = n) = (p − 1)/pn+1. The total number of queries is therefore ∑C+1 j=1 Bj , where Bj for all j are i.i.d. copies of B. The expected value of this expression, using a standard result on the expected value of a sum of a random number of independent random variables, is equal to (1 + E[C])E[Bj ], or( 1 + 1p−1 ) 1 p−1 = p (p−1)2 .\nIn terms of running time, we note that the expected running time of Prod is O ( 1 + dp−1 ) , this because it performs N multiplications of inner products, each one with running time O(d), and E[N ] = 1p−1 . The expected running time of Map Estimate is O ( 1 + 1p−1 ) . The expected\nrunning time of Grad Length Estimate is O ( 1 + 1p−1 ( 1 + 1p−1 ) +T ( 1 + dp−1 )) , which can be written\nas O (\np (p−1)2 + T ( 1 + dp−1 )) . Since Algorithm 1 at each of T rounds calls Map Estimate once,\nGrad Length Estimate once, Prod for O(T 2) times, and performs O(1) other operations, we get that the overall runtime is\nO ( T ( 1 + 1\np− 1 +\np\n(p− 1)2 + T\n( 1 + d\np− 1\n) + T 2 ( 1 + d\np− 1\n))) .\nSince 1p−1 ≤ p (p−1)2 , we can upper bound this by\nO ( T ( 1 +\np\n(p− 1)2 + T 2\n( 1 +\ndp\n(p− 1)2\n))) = O ( T 3 ( 1 + dp\n(p− 1)2\n)) .\nThe regret bound in the theorem follows from Thm. 4, with the expressions for constants following from Lemma 2, Lemma 3, and Lemma 4."
    }, {
      "heading" : "5.3 Proof Sketch of Thm. 3",
      "text" : "To prove the theorem, we use a more general result which leads to non-vanishing regret, and then show that under the assumptions of Thm. 3, the result holds. The proof of the result is given in Appendix F.\nTheorem 5. Let W be a compact convex subset of Rd and pick any learning algorithm which selects hypotheses from W and is allowed access to a single noisy copy of the instance at each round t. If there exists a distribution over a compact subset of Rd such that\nargmin w∈W\nE [ `(〈w,x〉, 1) ] and argmin w∈W ` ( 〈w,E[x]〉, 1 ) (3)\nare disjoint, then there exists a strategy for the adversary such that the sequence w1,w2, · · · ∈ W of predictors output by the algorithm satisfies\nlim sup T→∞ max w∈W\n1\nT T∑ t=1 ( `(〈wt,xt〉, yt)− `(〈w,xt〉, yt) ) > 0\nwith probability 1 with respect to the randomness of the oracles.\nAnother way to phrase this theorem is that the regret cannot vanish, if given examples sampled i.i.d. from a distribution, the learning problem is more complicated than just finding the mean of the data. Indeed, the adversary’s strategy we choose later on is simply drawing and presenting examples from such a distribution. Below, we sketch how we use Thm. 5 in order to prove Thm. 3. A full proof is provided in Appendix E.\nWe construct a very simple one-dimensional distribution, which satisfies the conditions of Thm. 5: it is simply the uniform distribution on {3x,−x}, where x is the vector (1, 0, . . . , 0). Thus, it is enough to show that\nargmin w : |w|2≤Bw `(3w, 1) + `(−w, 1) and argmin w : |w|2≤Bw `(w, 1) (4)\nare disjoint, for some appropriately chosen Bw. Assuming the contrary, then under the assumptions on `, we show that the first set in Eq. (4) is inside a bounded ball around the origin, in a way which is independent of Bw, no matter how large it is. Thus, if we pick Bw to be large enough, and assume that the two sets in Eq. (4) are not disjoint, then there must be some w such that both `(3w, 1) + `(−w, 1) and `(w, 1) have a subgradient of zero at w. However, this can be shown to contradict the assumptions on `, leading to the desired result."
    }, {
      "heading" : "6 Future Work",
      "text" : "There are several interesting research directions worth pursuing in the noisy learning framework introduced here. For instance, doing away with unbiasedness, which could lead to the design of estimators that are applicable to more types of loss functions, for which unbiased estimators may not even exist. Also, it would be interesting to show how additional information one has about the noise distribution can be used to design improved estimates, possibly in association with specific losses or kernels. Another open question is whether our lower bound (Thm. 3) can be improved when nonlinear kernels are used."
    }, {
      "heading" : "A Alternative Notions of Regret",
      "text" : "In the online setting, one may consider notions of regret other than 1. One choice is\nT∑ t=1 `(〈wt,Ψ(x̃t)〉, yt)− min w∈W T∑ t=1 `(〈w,Ψ(x̃t)〉, yt)\nbut this is too easy, as it reduces to standard online learning with respect to examples which happen to be noisy. Another kind of regret we may want to minimize is\nT∑ t=1 `(〈wt,Ψ(x̃t)〉, yt)− min w∈W `(〈wt,Ψ(xt)〉, yt) . (5)\nThis is the kind of regret which is relevant for actually predicting the values yt well based on the noisy instances. Unfortunately, in general this is too much to hope for. To see why, assume we deal with a linear kernel (so that Ψ(x) = x), and assume `(w,x, y) = (〈w,x〉 − y)2. Now, suppose that the adversary picks some w∗ 6= 0 in W, which might be even known to the learner, and at each round t provides the example (w∗/‖w∗‖, 1). It is easy to verify that Eq. (5) in this case equals\nT∑ t=1 (〈wt, x̃t〉 − 1)2 − 0 .\nRecall that the learner chooses wt before x̃t is revealed. Therefore, if the noise which leads to x̃t has positive variance, it will generally be impossible for the learner to choose wt such that 〈wt, x̃t〉 is arbitrarily close to 1. Therefore, the equation above cannot grow sub-linearly with T ."
    }, {
      "heading" : "B Proof of Thm. 2",
      "text" : "The analysis in this subsection is similar to the one of Subsection 5.2, focusing on Gaussian kernels. Namely, we assume here that the kernel k(x,x′) is equal to e−‖x−x ′‖2/σ2 for some σ2 > 0.\nWe start by constructing an explicit feature mapping Ψ(·) corresponding to the RKHS induced by our kernel. For any x,x′, we have that\nk(x,x′) = e−‖x−x ′‖2/σ2 = e−‖x‖ 2/σ2e−‖x ′‖2/σ2e2〈x,x ′〉/σ2\n= e−‖x‖ 2/σ2e−‖x ′‖2/σ2 ( ∞∑ n=0 (2〈x,x′〉)n σ2nn! )\n= e−‖x‖ 2/σ2e−‖x ′‖2/σ2 ( ∞∑ n=0 d∑ k1=1 · · · d∑ kn=1 (2/σ2)n n! xk1 · · ·xknx′k1 · · ·x ′ kn ) .\nThis suggests the following feature representation: for any x, Ψ(x) returns an infinite-dimensional vector, indexed by n and k1, . . . , kn ∈ {1, . . . , d}, with the entry corresponding to n, k1, . . . , kn being e−‖x‖ 2/σ2 (2/σ 2)n\nn! xk1 . . . xkn . The dot product between Ψ(x) and Ψ(x ′) is similar to a standard dot\nproduct between two vectors, and by the derivation above equals k(x,x′) as required.\nThe idea of deriving an unbiased estimate of Ψ(x) is the following: first, we sample N1, N2 independently according to P(N1 = n1) = P(N2 = n2) = (p − 1)/pn+1. Then, we query the oracle for x for 2N1 +N2 times to get x̃ 1, . . . , x̃(2N1+N2), and formally define Ψ̃(x) as\nΨ̃(x) = (−1)N1pN1+N2+22N2\nN1!N2!σ2N1+2N2(p− 1)2 N1∏ j=1 〈x̃(2j−1), x̃(2j)〉  d∑ k1,...,kN2=1 x̃ (2N1+1) k1 · · · x̃(2N1+N2)kN2 eN2,k1,...,kN2  (6)\nwhere eN2,k1,...,kN2 represents the unit vector in the direction indexed by N2, k1, . . . , kN2 as explained above. Since the oracle calls are i.i.d., it is not hard to verify that the expectation of the expression\nabove is( ∞∑ n1=0 p− 1 pn1+1 (−1)n1pn1+1 n1!σ2n1(p− 1) (〈x,x〉)n1 ) ∞∑ n2=0 p− 1 pn2+1 pn2+12n2 n2!σ2n2(p− 1) d∑ k1,...,kn2=1 xk1 · · ·xkn2 en2,k1,...,kn2  =\n( ∞∑ n1=0 (−‖x‖2/σ2)n1 n1! ) ∞∑ n2=0 (2/σ2)n2 n2! d∑ k1,...,kn2=1 xk1 · · ·xkn2 en2,k1,...,kn2  = e−‖x‖ 2/σ2\n ∞∑ n2=0 d∑ k1,...,kn2=1 (2/σ2)n2 n2! xk1 · · ·xkn2 en2,k1,...,kn2  which is exactly Ψ(x) as defined above.\nTo actually store Ψ̃(x) in memory, we simply keep and x̃(1), . . . , x̃(2N1+N2). The representation\nabove is used implicitly when we calculate dot products between Ψ̃(x) and other elements in the\nRKHS, via the subroutine Prod. We formalize the needed properties of Ψ̃(x) in the following lemma.\nLemma 5. Assuming the construction of Ψ̃(x) as in the discussion above, it holds that Et[Ψ̃(x)] = Ψ(x) for all x. Moreover, if the noisy sample x̃t returned by the oracle At satisfies E[‖x̃t‖2] ≤ Bx̃, then\nE [ ‖Ψ̃(xt)‖2 ] ≤ ( p\np− 1\n)2 e( √ pBx̃+2p √ Bx̃)/σ 2\nProof. The first part of the lemma follows from the discussion above. As to the second part, note that by (6), we have that\n‖Ψ̃(xt)‖2 = p2N1+2N2+422N2( N1!N2!σ2N1+2N2(p− 1)2 )2 N1∏ j=1 ( 〈x̃(2j−1), x̃(2j)〉 )2 d∑ k1,...,kN2=1 ( x̃ (2N1+1) k1 . . . x̃ (2N1+N2) kN2 )2 =\np2N1+2N2+422N2( N1!N2!σ2N1+2N2(p− 1)2 )2 N1∏ j=1 ( 〈x̃(2j−1), x̃(2j)〉 )2N2∏ j=1 ‖x̃(N1+j)‖2 \n≤ p 2N1+2N2+422N2( N1!N2!σ2N1+2N2(p− 1)2 )2B2N1x̃ BN2x̃ .\nThe expectation of this expression over N1, N2 is equal to( ∞∑ n1=0 p− 1 pn1+1 p2n1+2 (n1!σ2n1(p− 1))2 B2n1x̃ )( ∞∑ n2=0 p− 1 pn2+1 p2n2+222n2 (n2!σ2n2(p− 1))2 Bn2x̃ )\n=\n( p\np− 1 )2( ∞∑ n1=0 (pB2x̃) n1 (n1!σ2n1)2 )( ∞∑ n2=0 (4p2Bx̃) n2 (n2!σ2n2)2 )\n=\n( p\np− 1 )2( ∞∑ n1=0 ( ( √ pBx̃/σ 2)n1 n1! )2)( ∞∑ n2=0 ( (2p √ Bx̃/σ 2)n2 n2! )2)\n≤ ( p\np− 1 )2(( ∞∑ n1=0 ( √ pBx̃/σ 2)n1 n1! )( ∞∑ n2=0 (2p √ Bx̃/σ 2)n2 n2! ))2 = ( p p− 1 )2 e( √ pBx̃+2p √ Bx̃)/σ 2 .\nAfter the discussion above, the pseudocode for Map Estimate below should be self-explanatory.\nSubroutine 5 Map Estimate(At, p)\nSample N1 according to P(N1 = n1) = (p− 1)/pn1+1 Sample N2 according to P(N2 = n2) = (p− 1)/pn2+1 Query At for 2N1 +N2 times to get x̃ (1) t , . . . , x̃ (2N1+N2) t Return x̃ (1) t , . . . , x̃ (2N1+N2) t as Ψ̃(xt).\nWe now turn to the subroutine Prod, which given two elements in the RKHS, returns their dot product. This subroutine comes in two flavors: either as a procedure defined over Ψ̃(x), Ψ̃(x′) and returning 〈Ψ̃(x), Ψ̃(x′)〉 (Subroutine 6); or as a procedure defined over Ψ̃(x),x′ (Subroutine 7, where the second element is an explicitly given vector) and returning 〈Ψ̃(x),Ψ(x′)〉. This second variant of Prod is needed when we wish to apply the hypothesis on a new (known) instance x′.\nSubroutine 6 Prod(Ψ̃(x), Ψ̃(x′))\nLet x̃(n), . . . , x̃(2n1+n2) be the vectors comprising Ψ̃(x) Let x̃′ (1) , . . . , x̃′ (2n′1+n ′ 2) be the vectors comprising Ψ̃(x′) If n′2 6= n′2 return 0, else return (−1)n1+n′1pn1+n′1+2n2+422n2\nn1!n′1!(n2!) 2σ2(n1+n ′ 1+2n2)(p− 1)4 × (∏n1 j=1〈x̃(2j−1), x̃(2j)〉 )(∏n′1 j=1〈x̃′(2j−1), x̃′(2j)〉 )(∏n2 j=1〈x̃(2n1+j), x̃′(2n ′ 1+j)〉 ) The proof of the following lemma is a straightforward algebraic exercise, similar to the proof of\nLemma 4.\nLemma 6. Prod(Ψ̃(x), Ψ̃(x′)) returns 〈Ψ̃(x), Ψ̃(x′)〉.\nThe pseudocode for calculating the dot product 〈Ψ̃(x),Ψ(x′)〉 (where x′ is known) is very similar, and the proof is essentially the same.\nSubroutine 7 Prod(Ψ̃(x),x′)\nLet x(1), . . . ,x(2n1+n2) be the vectors comprising Ψ̃(x) Return\n(−1)n1pn1+n2+222n2 n1!(n2!)2σ2(n1+2n2)(p− 1)2 e−‖x ′‖2/σ2  n1∏ j=1 〈x̃(2j−1), x̃(2j)〉  n2∏ j=1 〈x̃(2n1+j),x′〉 . We are now ready to prove Thm. 2. First, regarding the expected number of queries, notice that to run Algorithm 1, we invoke Map Estimate and Grad Length Estimate once at round t. Map Estimate uses a random number 2B1 + B2 of queries, where B1, B2 are independent and distributed as P(B1 = n) = P(B2 = n) = (p− 1)/pn+1. Grad Length Estimate invokes Map Estimate a random number C of times, where P(C = n) = (p − 1)/pn+1. The total number of queries is therefore ∑C+1 j=1 (2Bj,1 +Bj,2), where Bj,1, Bj,2 are i.i.d. copies of B1, B2 respectively. The expected value of this expression, using a standard result on the expected value of a sum of a random number\nof random variables, is equal to (1 + E[C])(2E[Bj,1] + E[Bj,2]), or ( 1 + 1p−1 ) 3 p−1 = 3p (p−1)2 .\nIn terms of running time, the analysis is completely identical to the one performed in the proof of Thm. 1, and the expected running time is the same up to constants.\nThe regret bound in the theorem follows from Thm. 4, with the expressions for constants following from Lemma 2, Lemma 5, and Lemma 6."
    }, {
      "heading" : "C Proof of Examples 3 and 4",
      "text" : "Examples 3 and 4 use the error function Erf(a) in order to construct smooth approximations of the hinge loss and the absolute loss (see Fig. 1). The error function is useful for our purposes, since it is analytic in all of R, and smoothly interpolates between −1 for a 0 and 1 for a 0. Thus, it can be used to approximate derivative of losses which are piecewise linear, such as the hinge loss `(a) = max{0, 1− a} and the absolute loss `(a) = |a|.\nTo approximate the absolute loss, we use the antiderivative of Erf(sa). This function represents a smooth upper bound on the absolute loss, which becomes tighter as s increases. It can be verified that the antiderivative (with the constant free parameter fixed so the function has the desired behavior) is\n`(a) = a Erf(sa) + e−s 2a2\nσ √ π .\nWhile this loss function may seem to have slightly complex form, we note that our algorithm only needs to calculate the derivative of this loss function at various points (namely Erf(sa) for various values of a), which can be easily done.\nBy a Taylor expansion of the error function, we have that\n`′(a) = 2√ π ∞∑ n=0 (−1)n(sa)2n+1 n!(2n+ 1) .\nTherefore, `′+(a) in this case is at most\n2√ π ∞∑ n=0 (sa)2n+1 n!(2n+ 1) ≤ 2 as √ π ∞∑ n=0 (sa)2(n+1) (n+ 1)! = 2 as √ π ( eσ 2a2−1 ) .\nWe now turn to deal with Example 4. This time, we use the antiderivative of (Erf(s(a− 1))− 1)/2. This function smoothly interpolates between −1 for a −1 and 0 for a 0. Therefore, its antiderivative with respect to x represents a smooth upper bound on the hinge loss, which becomes tighter as s increases. It can be verified that the antiderivative (with the constant free parameter fixed so the function has the desired behavior) is\n`(a) = (a− 1)(Erf(s(a− 1))− 1) 2 + e−s 2(a−1)2 2 √ πs .\nBy a Taylor expansion of the error function, we have that\n`′(a) = −1 2 + 1√ π ∞∑ n=0 (−1)n(s(a− 1))2n+1 n!(2n+ 1) .\nThus, `′+(a) in this case can be upper bounded by\n1 2 + 1√ π ∞∑ n=0 (sa)2n+1 n!(2n+ 1) ≤ 1 2 + 1 as √ π ∞∑ n=0 (sa)2(n+1) (n+ 1)! ≤ 1 2 + 1 as √ π ( es 2a2 − 1 ) ."
    }, {
      "heading" : "D Proof of Thm. 4",
      "text" : "Our algorithm corresponds to Zinkevich’s algorithm [17] in a finite horizon setting, where we assume\nthe sequence of examples is g̃1Ψ̃(x1), . . . , g̃T Ψ̃(xT ), the cost function is linear, and the learning rate\nat round t is η/ √ T . By a straightforward adaptation of the standard regret bound for that algorithm (see [17]), we have that for any w such that ‖w‖2 ≤ Bw, T∑ t=1 〈wt, g̃tΨ̃(xt)〉 − T∑ t=1 〈w, g̃tΨ̃(xt)〉 ≤ 1 2 ( Bw η + η T T∑ t=1 ‖g̃tΨ̃(xt)‖2 ) √ T .\nWe now take expectation of both sides in the inequality above. The expectation of the right-hand side is simply\nE\n[ 1\n2 ( Bw η + η T T∑ t=1 Et [ g̃2t ] Et [ ‖Ψ̃(xt)‖2 ])√ T ] ≤ 1 2 ( Bw η + ηBg̃BΨ̃ )√ T .\nAs to the left-hand side, note that\nE [ T∑ t=1 〈wt, g̃tΨ̃(xt)〉 ] = E [ T∑ t=1 Et [ 〈wt, g̃tΨ̃(xt)〉 ]] = E [ T∑ t=1 〈wt, yt`′ ( yt〈wt,Ψ(xt)〉 ) Ψ(xt)〉 ] .\nAlso,\nE [ T∑ t=1 〈w, g̃tΨ̃(xt)〉 ] = T∑ t=1 〈w, `′ ( yt〈wt,Ψ(xt)〉 ) Ψ(xt)〉 .\nPlugging in these expectations and choosing η = √\nBw Bg̃BΨ̃ , we get that for any w such that ‖w‖2 ≤ Bw,\nE [ T∑ t=1 〈wt, yt`′ ( yt〈wt,Ψ(xt)〉 ) Ψ(xt)〉 − T∑ t=1 〈w, `′ ( yt〈wt,Ψ(xt)〉 ) Ψ(xt)〉 ] ≤ √ BwBg̃BΨ̃T .\nTo get the theorem, we note that by convexity of `, the left-hand side above can be lower bounded by\nE [ T∑ t=1 `(yt〈wt,Ψ(xt)〉)− T∑ t=1 `(yt〈w,Ψ(xt)〉) ] ."
    }, {
      "heading" : "E Proof of Theorem 3",
      "text" : "Fix a large enough Bw ≥ 1 to be specified later. Let x = (1, 0, . . . , 0) and let D to be the uniform distribution on {3x,−x}. To prove the result then we just need to show that\nargmin w : |w|2≤Bw `(3w, 1) + `(−w, 1) and argmin w : |w|2≤Bw `(w, 1) (7)\nare disjoint, for some appropriately chosen Bw. First, we show that the first set above is a subset of {w : |w|2 ≤ R} for some fixed R which does not depend on Bw. We do a case-by-case analysis, depending on how `(·, 1) looks like.\n1. `(·, 1) monotonically increases in R. Impossible by assumption (2).\n2. `(·, 1) monotonically decreases in R. First, recall that since `(·, 1) is convex, it is differentiable almost anywhere, and its derivative is monotonically increasing. Now, since `(·, 1) is convex and bounded from below, `′(w, 1) must tend to 0 as w →∞ (wherever `(·, 1) is differentiable, which is almost everywhere by convexity). Moreover, by assumption (2), `′(w, 1) is upper bounded by a strictly negative constant for any w < 0. As a result, the gradient of `(3w, 1) + `(−w, 1), which equals 3`′(3w, 1) − `′(−w, 1), must be positive for large enough w > 0, and negative for large enough w < 0, so the minimizers of `(3w, 1) + `(−w, 1) are in some bounded subset of R.\n3. There is some s ∈ R such that `(·, 1) monotonically decreases in (−∞, s) and monotonically increases in (s,∞). If the function is constant in (s,∞) or in (−∞, s), we are back to one of the two previous cases. Otherwise, by convexity of `(·), we must have some a, b, a ≤ s ≤ b, such that `(·, 1) is strictly decreasing at (−∞, a), and strictly increasing at (b,∞). In that case, it is not hard to see that `(3w, 1) + `(−w, 1) must be strictly increasing for any w > max{|a|, |b|}, and strictly decreasing for any w < −max{|a|, |b|}. So again, the minimizers of `(3w, 1) + `(−w, 1) are in some bounded subset of R.\nWe are now ready to show that the two sets in (7) must be disjoint. Suppose we pick Bw large enough so that the first set in (7) is strictly inside {w : |w|2 ≤ Bw}. Assume on the contrary that there is some w, |w|2 < Bw, which belongs to both sets in (7). By assumption (2) and the fact that w minimizes `(w, 1), we may assume w > 0. Therefore, 0 ∈ ∂`(w, 1) as well as 0 ∈ ∂(`(3w, 1) + `(−w, 1)), where ∂f is the (closed and convex) subgradient set of a convex function f . By subgradient calculus, this means there is some a/3 ∈ ∂`(3w, 1) and b ∈ ∂`(−w, 1) such that a/3 − b = 0. This implies that ∂`(3w, 1) ∩ ∂`(−w, 1) 6= ∅. Now, suppose that max ∂`(−w, 1) < 0. This would mean that min ∂`(3w, 1) < 0. But then `(·, 1) is strictly decreasing at (w, 3w), and in particular `(w, 1) > `(3w, 1), contradicting the assumption that w minimizes `(·, 1). So we must have max ∂`(−w, 1) ≥ 0. Moreover, min ∂`(−w, 1) ≤ 0 (because w minimizes `(·, 1) and −w < w). Since the subgradient set is closed and convex, it follows that 0 ∈ ∂`(−w, 1). Therefore, both w and −w minimize `(·, 1). But this means that `′(0) = 0, in contradiction to assumption (2)."
    }, {
      "heading" : "F Proof of Thm. 5",
      "text" : "Let D be a distribution which satisfies (3). The idea of the proof is that the learner cannot know if D is the real distribution (on which regret is measured) or the distribution which includes noise. Specifically, consider the following two adversary strategies:\n1. At each round, draw an example from D, and present it to the learner (with the label 1) without adding noise.\n2. At each round, pick the example ED[x], add to it zero-mean noise sampled from Z − ED[x], where Z is distributed according to D, and present the noisy example (with the label 1) to the learner.\nIn both cases the examples presented to a learner appear to come from the same distribution D. Hence, any learner observing one copy of each example cannot know which of the two strategies is played by the adversary. Since (3) implies that the set of optimal learner strategies for each of the\ntwo adversary strategies are disjoint, by picking an appropriate strategy the adversary can force a constant regret.\nTo formalize this argument, fix any learning algorithm that observes one copy of each example and let w1,w2, . . . be the sequence of generated predictors. Then it is sufficient to show that at least one of the following two holds\nlim sup T→∞ max w∈W\nE\n[ 1\nT T∑ t=1 `(〈wt,xt〉, 1)− ` (〈w,xt〉, 1)\n] > 0 (8)\nlim sup T→∞\n1\nT T∑ t=1 `(〈wt,E[x]〉, 1)− min w∈W ` ( 〈w,E[x]〉, 1 ) > 0 w.p. 1 (9)\nwhere in both cases the expectation is with respect to D and “w.p. 1” refers to the randomness of the noise. First note that (8) is implied by\nlim sup T→∞\n1\nT T∑ t=1 `(〈wt,xt〉, 1)− min w∈W E [ `(〈w,x〉, 1) ] > 0 w.p. 1. (10)\nSince W is compact, D is assumed to be supported on a compact subset, and ` is convex and hence continuous, then `(〈w,x〉, 1) is almost surely bounded. So by Azuma’s inequality\n∞∑ T=1 P\n( 1\nT T∑ t=1 ( Et [ `(〈wt,x〉, 1) ] − `(〈wt,xt〉, 1) ) ≥ ) <∞ ∀ > 0 .\nwhere the expectation Et[ · ] is conditioned on the randomness in the previous rounds. Letting w̄t = 1 t ∑t s=1 ws (which belongs to W for all t since it is a convex set), we have\n1\nT T∑ t=1 `(〈wt,xt〉, 1) ≥ 1 T T∑ t=1 Et[`(〈wt,x〉, 1)] ≥ E [ ` ( 〈w̄T ,x〉, 1 )] where the first inequality holds with probability 1 as T →∞ by the Borel-Cantelli lemma, and the second one holds for every T because ` is convex.\nSimilarly,\n1\nT T∑ t=1 `(〈wt,E[x]〉, 1) ≥ ` ( 〈w̄T ,E[x]〉, 1 ) .\nHence (9)–(10) are obtained if we show that no single sequence of predictors w̄1, w̄2, . . . simultaneously satisfies\nlim sup T→∞ F1(w̄T ) ≤ 0 and lim sup T→∞ F2(w̄T ) ≤ 0 (11)\nwhere F1(wT ) = E [ ` ( 〈w̄T ,x〉, 1 )] −min w∈W E [ ` (〈w,x〉, 1) ] F2(wT ) = ` ( 〈w̄T ,E[x]〉, 1 ) −min w∈W ` ( 〈w,E[x]〉, 1 ) .\nSuppose on the contrary that there was such a sequence. Since w̄T ∈ W for all T , and W is compact, the sequence w̄1, w̄2, . . . has at least a cluster point w̄ ∈ W. Moreover, it is easy to verify that the functions F1 and F2 are continuous. Indeed, `(〈·,E[x]〉, 1) is continuous by convexity of ` and E[`(〈·,x〉, 1)] is continuous by the compactness assumptions. Hence, any cluster point of w̄1, w̄2, . . . is also a cluster point of both F1 and F2. Since F1, F2 ≥ 0 by construction, and we are assuming that neither F1(w̄) > 0 nor F1(w̄) > 0 for any cluster point w̄, we must have F1(w̄) = F2(w̄) = 0. But this means that w̄ belongs to both sets appearing in (3), in contradiction to the assumption they are disjoint. Thus, no sequence of predictors satisfies (11), as desired."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2010,
    "abstractText" : "We study online learning when individual instances are corrupted by adversarially chosen<lb>random noise. We assume the noise distribution is unknown, and may change over time<lb>with no restriction other than having zero mean and bounded variance. Our technique relies<lb>on a family of unbiased estimators for non-linear functions, which may be of independent<lb>interest. We show that a variant of online gradient descent can learn functions in any dot-<lb>product (e.g., polynomial) or Gaussian kernel space with any analytic convex loss function.<lb>Our variant uses randomized estimates that need to query a random number of noisy copies<lb>of each instance, where with high probability this number is upper bounded by a constant.<lb>Allowing such multiple queries cannot be avoided: Indeed, we show that online learning is<lb>in general impossible when only one noisy copy of each instance can be accessed.",
    "creator" : "LaTeX with hyperref package"
  }
}