{
  "name" : "1606.00577.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Source-LDA: Enhancing probabilistic topic models using prior knowledge sources",
    "authors" : [ "Justin Wood" ],
    "emails" : [ "juwood03@ucla.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nExisting topic modeling is often based off of Latent Dirichlet allocation (LDA) [1] and involves analyzing a given corpus to produce a distribution over words for each latent topic and a distribution over latent topics for each document. Although the distributions representing topics have been shown to be useful and generally representative of a linguistic topic, assigning labels to topics is often left to manual interpretation.\nThe identification of topic labels in the general sense is useful in mapping different terms into a single label. For example, terms including pencil, laptop, ruler, eraser, and book can be mapped to the label “School Supplies.” Adding descriptive semantics to each topic can help people, especially those without domain knowledge, to understand topics obtained by topic models.\nAn example application of accurate labels to topics is that of summarization systems for primary care physicians, who are faced with the challenges of being inundated with too much data for a patient and too little time to comprehend it all [2].\nThe labels can be used to more appropriately and quickly identify a patient’s medical history, leading to better outcomes for the patient. This added information can add significant value to this field which already utilizes topic modeling without labeling [3], [4], as well as many other domains.\nEven though some approaches in the labeling of topics do exist, these techniques do their fitting of labels to topics after completion of the topic modeling process or require a supervised input set. As a result, the label assigned to a topic is limited only to the results of an unaware topic model. By assigning the label after the topic modeling or with the only outside knowledge as just the label itself the topic model is unable to integrate additional knowledge derived from the label into the topic assignment process. These problems are best illustrated via a simple case study.\n1) Case Study: Suppose a corpus of a news source that consists of two articles is given by documents d1 and d2 each with three words:\nd1 - pencil, pencil, umpire\nd2 - ruler, ruler, baseball\nLDA with the traditionally used collapsed Gibbs sampler, standard hyperparameters and the number of topics (K) set as two would result in different results for different runs due to the inherent stochastic nature, but it is very possible to obtain the following result of topic assignments:\nd1 - pencil1, pencil1, umpire2\nd2 - ruler2, ruler2, baseball1\nBut these assignments to topics differs from the ideal solution that involves knowing the context of the topics in which these words come from. If the topic modeling was to incorporate additional knowledge about the topics “School Supplies” and “Baseball” then a topic modeling process can more likely generate the ideal topic assignments of:\nd1 - pencil2, pencil2, umpire 1\nd2 - ruler2, ruler2, baseball 1\nand also assign a label of “School Supplies” to topic 1 and “Baseball” to topic 2.\nThe goal of Source-LDA is to incorporate such outside knowledge into the topic modeling process in order to improve the quality of topic assignments and effectively label topics.\nThere does exist approaches that take aim at incorporating preexisting information into the topic modeling process [5]– [7], but can be either too lenient or too strict. In the concept topic model [5], for example, a multinomial distribution is placed over known concepts with associated word sets. This pioneering approach does integrate existing information, but does not take into account prior distributions. For example if a document is generated about the topic “School Supplies” it is much more probable to see the word “pencil” than the word “compass” even though both words may be associated with the topic “School Supplies”. This technique also requires some supervision which requires manually inputting preexisting concepts and their bags of words.\nA differing approach given by Hansen et al. [6] incorporates a preexisting distribution based off of Wikipedia but is restricting. This approach fulfills the goal of incorporating preexisting information with their distributions but requires the topic in the generated corpus to be too similar to the Wikipedia distribution.\nThe Source-LDA model is a balance between these two approaches. The goal is to take into account known topics and their distributions, but allow some variance between the preexisting topic and the inferred topic. This should be accomplished in as unsupervised a manner as possible. In Source-LDA, the user only has to enter into the topic model a superset of possible topics for a given corpus, which can be easily obtained with minimal manual intervention.\nA summary of the contributions of this work are:\n1) We propose a novel technique to topic modeling that\ntakes into account preexisting topic distributions.\n2) We show how to find the appropriate topics in a corpus\ngiven an input set that contains a subset of the topics used to generate a corpus.\n3) We introduce an approach that allows for variance from\nan input topic to the latent topic discovered during the topic modeling process.\nThe rest of this paper is organized as follows: In section 2, we give a brief introduction to the LDA algorithm and the Dirichlet distribution. A more detailed description of the Source-LDA algorithm is presented in section 3. In section 4, the algorithm\n1 https://github.com/ucla-scai/Source-LDA\nis used and evaluated by PMI. Related literature will be highlighted in section 5. Section 6 gives the conclusions of this paper.\nFor reproducible research, we make all of our code available online.1"
    }, {
      "heading" : "II. PRELIMINARIES",
      "text" : ""
    }, {
      "heading" : "A. Dirichlet Distribution",
      "text" : "The Dirichlet distribution is a distribution over probability mass functions with a specific number of atoms and is commonly used in Bayesian models. A property of the Dirichlet that is often used in inference of Bayesian models is conjugacy to the multinomial distribution. This allows for the posterior of a random variable with a multinomial likelihood and a Dirichlet prior to also be a Dirichlet distribution.\nThe parameters are given as a vector denoted by α. The probability density function for a given probability mass function (PMF) θ and parameter vector α of length J is defined as:\n\uD835\uDC53(\uD835\uDF03, \uD835\uDEFC) = Γ(∑ \uD835\uDEFC\uD835\uDC56)\n\uD835\uDC3D \uD835\uDC56\n∏ Γ(\uD835\uDEFC\uD835\uDC56) \uD835\uDC3D \uD835\uDC56\n∏ \uD835\uDF03\uD835\uDC56 \uD835\uDEFC\uD835\uDC56−1\n\uD835\uDC3D\n\uD835\uDC56\nA sample from the Dirichlet distribution produces a PMF that is parameterized by α. The choice of a particular set of α values influences the outcome of the generated PMF. If all α values are the same (symmetric parameter), as α approaches 0 the probability will be concentrated on a smaller set of atoms. As α approaches infinity then the PMF will become the uniform distribution. If all αi are natural numbers then each individual αi can be thought of as “virtual” count for the ith value [8]."
    }, {
      "heading" : "B. Latent Dirichlet Allocation",
      "text" : "LDA is the basis for many existing probabilistic topic models, and the framework for the approach presented by this paper. Since we enhance the LDA model in our proposed approach it is worth giving a brief overview of the algorithm and model of LDA.\nLDA is a hierarchical Bayes model which utilizes Dirichlet priors to estimate the intractable latent variables of the model. At a high level, LDA is based on a generative model in which each word of an input document from a corpus is chosen by first selecting a topic that corresponds to that word and then selecting the word from a topic to word distribution. Each topic to word distribution and word to topic distribution is drawn from its respective Dirichlet distribution. The formal definition of the generative algorithm over a corpus is:\n1. For each of the K topics \uD835\uDF19\uD835\uDC58:"
    }, {
      "heading" : "2. Choose \uD835\uDF19\uD835\uDC58∼ Dir(β)",
      "text" : "3. For each of the D documents d:"
    }, {
      "heading" : "4. Choose Nd ∼ Poisson(ξ)",
      "text" : ""
    }, {
      "heading" : "5. Choose θd ∼ Dir(α)",
      "text" : "6. For each of the N d words wn,d:\n7. Choose zn,d ∼ Multinomial(θ)\n8. Choose wn,d ∼ Multinomial(\uD835\uDF19\uD835\uDC67\uD835\uDC5B,\uD835\uDC51)\nFrom the generative algorithm the resultant Bayes model is\nshown by figure 1(a).\nThe approach used to infer the latent θ distribution,\n\uD835\uDF19 distribution, and z is the use of Bayes’ law\n\uD835\uDC43(\uD835\uDF03, \uD835\uDF19, \uD835\uDC67|\uD835\uDC64, \uD835\uDEFC, \uD835\uDEFD) = \uD835\uDC5D(\uD835\uDF03, \uD835\uDF19, \uD835\uDC67, \uD835\uDC64|\uD835\uDEFC, \uD835\uDEFD)\n\uD835\uDC5D(\uD835\uDC64|\uD835\uDEFC, \uD835\uDEFD)\nUnfortunately the computation of this equation exactly is intractable. Hence, it must be approximated with techniques such as expectation-maximization [1], Gibbs sampling or collapsed Gibbs sampling [9].\nIII. PROPOSED APPROACH\nSource-LDA is an extension of the LDA generative model in which after a known set of topics are identified, a secondary word to topic distribution is generated and sampled under certain conditions. The desiderata is to enhance existing LDA topic modeling by integrating existing information into the topic modeling process. The relevant terms and concepts used in the following discussion are defined below.\nDefinition 1 (Knowledge source): A knowledge source is a collection of text that is focused on describing a single concept. For example the knowledge sources used in our experiments are Wikipedia articles about the categories that we selected from the Reuters dataset.\nDefinition 2 (Source Distribution): The source distribution is a discrete probability distribution over the words of a document describing a topic. The probability mass function is given by\n∀\uD835\uDC64 ∈ \uD835\uDC4A, \uD835\uDC53(\uD835\uDC64) = \uD835\uDC5B\uD835\uDC64\n∑ \uD835\uDC5B\uD835\uDC64\uD835\uDC56 \uD835\uDC3A \uD835\uDC56\nwhere W is the set of all words in the document, G = |W|, and nw is the count of word w in the document.\nDefinition 3 (Source Hyperparameters): For a given knowledge source document the knowledge source hyperparameters are defined by the vector (X1,X2,...,XV ) where \uD835\uDC4B\uD835\uDC56 = \uD835\uDC5B\uD835\uDC64\uD835\uDC56 + \uD835\uDF00 and \uD835\uDF00 is a very small positive number. V is the size of the vocabulary of the corpus for which we are topic\nmodeling, and nwi is the count of the \uD835\uDC64\uD835\uDC56 \uD835\uDC61ℎ word from the corpus vocabulary in the knowledge source document if it exists and \uD835\uDF00 otherwise.\nThree approaches are given that capture the intent of Source-LDA, the first being a simple enhancement to the LDA model that allows for the influencing of topic distributions, but suffers from the limitation of needing more user intervention. The second approach allows for the mixing of unknown topics and the third approach combines the previous two approaches and moves toward a complete solution to topic modeling based off of existing knowledge sources."
    }, {
      "heading" : "A. Bijective Mapping",
      "text" : "In the simplest approach, the Source-LDA model assumes that there exists a 1-to-1 mapping between a known set of topics and the topics used to generate a corpus. The generative model then assumes that, instead of selecting topic to word distributions from sampling from the Dirichlet distribution, a set of K distributions are given as input and sampled from after each topic assignment is sampled for a given token position. The generative process for a corpus adapted from the traditional LDA generative model during the construction of the \uD835\uDF19 distributions is as follows (for brevity only the relevant parts of the existing LDA algorithm are shown):\n1. For each of the K topics \uD835\uDF19\uD835\uDC58:\n2. δk ← (Xk,1,Xk,2,...,Xk,V )"
    }, {
      "heading" : "3. Choose \uD835\uDF19\uD835\uDC58 ∼ Dir(δk)",
      "text" : "Where (Xk,1,Xk,2,...,Xk,V ) represents the knowledge source hyperparameters for the kth knowledge source document. The generative model only differs from the traditional LDA model in how each φ is built. Therefore the derivation for inference is a simple factor as well. To approximate the distributions for θ and φ a collapsed Gibbs sampler can approximate the z assignments as follows:\nP(w i|zi=j,z−i,wi) ∝ P(wi|zi=j,z−i,w−i)P(zi=j|z−i)\nFrom the Bayesian Model the following equations can be easily be generated\nP(wi|zi=j,z−i,w−i)= ∫P(wi|zi=j,\uD835\uDF19\uD835\uDC57)P(\uD835\uDF19\uD835\uDC57|z−i,w−i)d\uD835\uDF19\uD835\uDC57\nwith\nP(\uD835\uDF19\uD835\uDC57|z−i,w−i) ∝ P(w−i|\uD835\uDF19\uD835\uDC57,z−i)P(\uD835\uDF19\uD835\uDC57)\nP(\uD835\uDF19\uD835\uDC57|z−i,w−i) = Dir(δj + nw−i,j)\nP(wi|zi=j,\uD835\uDF19\uD835\uDC57) = \uD835\uDF19\uD835\uDC64\uD835\uDC56,\uD835\uDC57\nFor brevity since the prior probability is unchanged in the “Bijection Mapping” model we will skip the derivation which is well defined in other articles [9]–[11].\nPutting the two equations together gives the final Gibbs sampling equation:\nGiven the approximation to the topic assignments, the θ and φ distributions can be calculated as:\n(1)\nIn the case when all topics are known this model has the advantage of conforming the φ distributions to the known distributions, but has three drawbacks. First, even though there is some variability between the φ distribution and source distribution, as illustrated by figure 2, there may be cases in which this constraint should be relaxed even further. This is\nbecause it is entirely possible to generate a corpus about a known topic without exactly following the frequencies at which the topic is discussed in its respective article. This model also requires the user to input the known topics, and other possible supervised approaches may be better suited to the task [13]– [15]. The third drawback is that we are not allowing the possibility that the corpus was generated from a mixture of known topics and unknown topics, which is a more realistic scenario for an arbitrary clinical document. The next model aims to resolve this last deficiency."
    }, {
      "heading" : "B. Known Mixture of Topics",
      "text" : "The next model presupposes that in the topic model it is known how many topics are source topics (as well as their source distributions) and how many are regular nonsource topics. The previous approach works quite well in this situation in that a non-source topic will have a symmetric beta parameter which will capture assignments which were unallocated due to a low count of a source topic.\nThe resulting model helps to solve the existing problems of the bijective model and only requires a minor input to the existing generative model. The resulting model works quite well with the bijective model in that the symmetric Dirichlet prior can be used to guide a topic toward being a general unlabeled topic or a source topic. The model changes as shown below with a minor change to the generative algorithm and the collapsed Gibbs sampling.\n1. For each of the K topics φk:\n2. if k ≤ T then\n3. Choose φ k ∼ Dir(β)\n4. else\n5. δk ← (Xk,1,Xk,2,...,Xk,V )"
    }, {
      "heading" : "6. Choose φk ∼ Dir(δk)",
      "text" : "Where T is the total number of non source topics. The change required to the collapsed Gibbs sampling is then:\nand\nThis approach gives the benefit of allowing a mixture of known topics and unknown topics, but problems still arise in that the Dirichlet distributions for the source distribution may be too restricting."
    }, {
      "heading" : "C. Source-LDA",
      "text" : "By using the counts as hyperparameters, the resultant φ distribution will take on the shape of the knowledge source distribution. But this may be counter to the aim of enhancing existing topic modeling. With the goal to influence the φ distribution, it is entirely plausible for there to be divergence between the two distributions. In other words, φ may not need to strictly follow the corresponding knowledge source distribution.\n1) Variance from the source distribution: To allow for this relaxation, another parameter λ is introduced into the model which is used to allow for a higher deviance from the source distribution. The addition of λ changes the existing generative model only slightly and allows for a variance for each individual βi, which frees us from an overly restrictive binding to the associated knowledge source distribution.\n5. δk ← [(Xk,1)λ,(Xk,2)λ,...,(Xk,V )λ]\nThe λ parameter acts as a measure of how much divergence is allowed for a given modeled topic from the knowledge source distribution. Figure 3 shows how the JS Divergence changes with changes to the λ parameter. With the introduction of λ as an input parameter, the new topic model has the advantage of allowing variance and also leaves the collapsed Gibbs sampling equation unchanged. However this also requires a uniform variance from the knowledge base distribution for all latent topics. This can be a problem if the corpus was generated with some topics influenced strongly while others less so. To solve this we can introduce λ as a hidden parameter of the model.\n2) Approximating λ: In the ideal situation λ will be as close to 1 for most knowledge based latent topics, with the flexibility to deviate as required by the data. For this we assume a Gaussian prior over λ with mean set to µ. The variance then becomes a modeled parameter that conceptually can be thought of as how much variance from the knowledge source distribution we wish to allow in our topic model. In assuming a Gaussian prior for λ, we must integrate λ out of the collapsed Gibbs sampling equations (only the probability of wi under topic j is shown, the probability of topic j in document d is unchanged and omitted).\nφ then becomes\nUnfortunately closed form expressions for these integrals are hard to obtain and so they must be approximated numerically during sampling.\nAnother problem arises in that the change of λ is not in par with the change of the Gaussian distribution, as can be seen in figure 3. To make the changes of λ more in line with that expected from the Gaussian PDF, we must map each individual λ value in the range 0 to 1 with a value which produces a change in the JS divergence in a linear fashion. We approximate a function, g(x) with a linear derivative, shown in figure 4. Our collapsed Gibbs sampling equations then becomes:\n3) Superset Topic Reduction: Another problem arises with knowing the right mixture of source topics and known topics. It is also entirely possible that many source topics may not be used by the generative model. Our desire to leave the\nmodel as unsupervised as possible calls for input that is a superset of the actual generative topic selection in order to avoid manual topic selection. In the case of modeling only a specific number of topics over the corpus, the problem then becomes how to choose which knowledge source latent topics to model vs. how many unlabeled topics to allow in the model.\nThe goal then is to allow for a superset of knowledge source topics as input and then during the inference to select the best set of these with a mixture of unknown topics where the total number of unlabeled topics is given as input K. The approach given is to use a mixture of K unlabeled topics alongside the labeled knowledge source topics. The total number of topics then becomes T. During the inference we eliminate topics which are not assigned to any documents. At the end of the sampling phase we then can use a clustering algorithm (such as k-means, JS divergence) to further reduce the modeled topics and give a total of K topics. As described more in the experimental section, with the goal of capturing topics that\nwere frequently occurring in the corpus, topics not appearing in a frequent enough of documents were eliminated.\nThe complete generative process is shown in figure 1(b) and\ndescribed below:\n1. For each of the T topics φt:\n2. if t ≤ K then\n3. Choose φt ∼ Dir(β)\n4. else"
    }, {
      "heading" : "5. Choose λt ∼ N(µ,σ)",
      "text" : "6. δt ← [(Xt,1)g(λt),(Xt,2)g(λt),...,(Xt,V )g(λt)]"
    }, {
      "heading" : "7. Choose φt ∼ Dir(δt)",
      "text" : "8. For each of the D documents d:"
    }, {
      "heading" : "9. Choose Nd ∼ Poisson(ξ)",
      "text" : "10. Choose θd ∼ Dir(α)\n11. For each of the Nd words wn,d:\n12. Choose zn,d ∼ Multinomial(θ)\n13. Choose wn,d ∼ Multinomial(φzn,d)\nThe full collapsed Gibbs sampling algorithm is given in\nalgorithm 1.\n4) Analysis: By using the a clustering algorithm or thresholding the topic document frequency the collapsed Gibbs algorithm is guaranteed to produce K topics. The running time is a function of the number of iterations I, average words per document Davg, number of documents D, number of topics T and number of approximation steps A, and is O(I×Davg×D×T×A). This differs only from the traditional collapsed Gibbs sampling in LDA by an increase of (T−K)A. But\nsince we have built the approach to potentially have a large T −K this difference can have a significant impact on running times.\nApproaches exist that can parallelize the sampling procedure, but these are often approximations or can potentially have slower than baseline running times [16]–[18]. We present two modifications to the original algorithm that allow for approximation while guaranteeing the exactness of the results to the original Gibbs sampling. The first one makes use of prefix sums rules [19] and guarantees a running time of I D D T A\n, with P being the number of parallel units.\nThis algorithm is given by algorithm 2.\nThis algorithm is practical in situations where T − K is large, but suffers from the limitations of the number of context switches required for the threads to wait at their respective barriers. A simpler implementation approach that reduces the number of context switches is to add the sums for each thread then wait for a barrier. When the barrier is released then just add the end values together and in parallel add the\nInput: Dirichlet hyperparameters α, β, a corpus C, vocabulary V , unbiased topic count K, total topic count T, a set of bias topics S, mean µ, variance σ, and iteration count I. Output: θ, φ\nprocedure COLLAPSED GIBBS(α, β, C, V , T) for t = K + 1 to T do\nCalculate g t end for Initialize Ctopics to random topic assignments Update nw and nd from C topics for iter = 1 to I do\nfor i = 1 to C do for j = 1 to |Ci| do Ctopicsi,j ← Sample(i,j) end for end for\nend for Calculate θ according to (1) Calculate φ according to (4) return θ,φ\nend procedure\nprocedure SAMPLE(i, j) Decrement nw and nd accordingly for t = 1 to K do\nCalculate pt according to (2) end for for t = K + 1 to T do\nCalculate pt according to (3) end for topic ∼ Multinomial(p) Increment nw and nd accordingly return topic\nend procedure\nprocedure SAMPLE(i, j) Decrement nw and\nnd accordingly for i from 0 to T − 1 in parallel do if i ≤ K then\nAlgorithm 1 Collapsed Gibbs\nAlgorithm 2 Prefix Sums Parallel Sampling\nCalculate pi according to (2) else Calculate pi according to (3) end if\npi ← pi−1 + pi end for for d from 0 to (lnT) − 1 do\nfor i from 0 to T − 1 by 2d+1 in parallel do p(i+2d+1−1) ←\np(i+2d−1) + p(i+2d+1−1) end for end for pT−1) ← 0 for d from (lnT) − 1 down to 0 do\nfor i from 0 to T − 1 by 2d+1 in parallel do h ←\np(i+2d−1) p(i+2d+1−1) ← p(i+2d+1−1) p(i+2d+1−1) ← h + p(i+2d+1−1)\nend for end for topic ← Binary Search(p) Increment nw and nd accordingly return topic\nend procedure\nremaining necessary items. This approach is given in algorithm 3. The running time is then for ;\notherwise it is O(I × Davg × D × P × A)\nThese two algorithms allow for mitigation of the increase in the number of topics and should approach times very similar to those of standard LDA runs. They are also very extensible and can be used in other optimization algorithms. Algorithm 3 Simple Parallel Sampling\nprocedure SAMPLE(i, j) Decrement nw and nd accordingly for i from 0 to T − 1 in parallel do if i ≤ K then\nCalculate pi according to (2) else Calculate pi according to (3) end if\npi ← pi−1 + pi end for for i from 0 to T − 1 by T/P do\npi ← p(i−T/P) + pi endsi ← pi end for for i from 0 to T − 1 in parallel do\ndiff ← pend − endsi pi ← diff + pi end for topic ← Binary Search(p) Increment nw and nd accordingly return topic\nend procedure"
    }, {
      "heading" : "IV. EVALUATION",
      "text" : "To test the results of the Source-LDA algorithm we set up a experiments to test against traditional LDA. We describe in more detail below the experimental setups and metrics used to compare results."
    }, {
      "heading" : "A. Reuters Newswire Analysis",
      "text" : "1) Experimental Setup: Source-LDA, LDA, EDA, and CTM were run against the Reuters-21578 newswire collection. This collection contains documents from the Reuters newswire from 1987. The dataset contains 21,578 articles, among a large set of categories. One important feature of the dataset are a set of given categories that we can use for our topic labeling. We select 80 distinct categories as our superset for possible\nknowledge bases. These include broad categories such as shipping, interest rates, and trade, as well as more refined categories such as rubber, zinc, and coffee. Our choice to apply our topic labeling method to this dataset is due to the fact that the Reuters dataset is widely used for information retrieval and text categorization applications. Due to its widespread use, it can considerably aid us in comparing our results to other studies. Additionally, because it contains distinct categories that we can use as our known set of topics, we can easily demonstrate the viability of our model. The Source-LDA, EDA, and CTM supplementary distributions were generated by first obtaining a list of topics from the Reuters-21578 dataset. Next, for each topic, the corresponding Wikipedia article was crawled and the words in the topic were counted, forming their respective distributions. For all models, a symmetric Dirichlet parameter of 50/T (where T is the number of topics) and 200/V (where V is the size of the vocabulary) was used for α and β respectively. For Source-LDA, µ was set to 1.0 and σ to 0.2. The bag of words used in the CTM were taken from the top 10 words by frequency for each topic. The models showed good convergence after 1000 iterations. After sampling was complete for LDA, the resulting topic to word distribution was mapped using an information retrieval (IR) approach. The IR approach was to use cosine similarity of documents mapped to term frequency-inverse document frequency (TF-IDF) vectors with TF-IDF weighted query vectors formed from the top 10 words per topic.\n2) Experimental Results: After the algorithms converged, LDA was labeled using the IR approach. Given similar labels from the models it is an intuitive approach to compare the word assignments to each topic model. Since the explicit topics are just the source topics for EDA these results were omitted. Example comparisons are shown in table I. These label assignments show a more accurate assignment of ngrams to topics than bot LDA and CTM. LDA appears to suffer from mixing of different concepts into a single topic, for example with the topic “Cystic Fibrosis”, and CTM assigns more weight to less important words. Source-LDA is more consistent with the meaning of the topic as opposed to what words you may find when talking about this topic, which can\nbe generally applied to many diseases."
    }, {
      "heading" : "B. Wikipedia Corpus",
      "text" : "1) Experimental Setup: A corpus of Wikipedia vocabulary articles was generating by following the steps of the generative model for Source-LDA, where the K topics chosen are a subset of a larger collection of Wikipedia topics. The number of topics (K) was given as 100, chosen from an entire collection of 578 topics (B), then number of documents (D) was given as 200 and the average document word count (Davg) as 300. After these 200 documents were generated the topic assignments were\nrecorded and used as the ground truth measurement. The word assignments were used as the corpus and three different topic models were applied to these documents. The first one was the bijective mapping model in which only the topics used in the generative model where used as input. The second topic model was LDA, and the third was Source-LDA. For the SourceLDA model µ was set to 1.0 and σ set to 0.0. For all models, a symmetric Dirichlet parameter of 50/T (where T is the number of topics) and 200/V ( where V is the size of the vocabulary) was used for α and β respectively. After running the models for 1000 iterations they were evaluated against the ground truth measurement.\n2) Experimental Results: The topic assignments were recorded for all models and the results compared against each other. Of interest is the overall number of correct topic assignments each model with the results shown in 5(a). Since the LDA model has unknown topics, JS divergence was used to map each LDA topic to its best matching Wikipedia topic. As expected the bijective mapping (SRC-Exact) had the best results, with the Source-LDA model (SRC-Unk) not too far behind.\nIn the second analysis 5(b) the topic to document distributions were analyzed using JS Divergence, and is irrespective of any unknown mapping. The results again show the order to be Source-LDA to be effective accurately mapping topics to documents.\nThe outputs from Source-LDA, the bijective model, and\nLDA were also evaluated by pointwise mutual information\n(PMI). The results detailed by figure 5(c), show that by PMI, Source-LDA provides a better mapping of n-grams to topics on average as the area under the curve for Source-LDA is larger than that of LDA.\nC. Performance Benchmarking\nTo show the performance gains used by the parallel sampling algorithm and experiment was set up to generate topics randomly from a given vocabulary. The corpus was generated using the same parameters as in section 4.2 but with B ranging from 100 to 10000. The benchmarking is visualized by figure\n5(d)."
    }, {
      "heading" : "V. RELATED WORK",
      "text" : "Much existing literature exists related to the proposed approach in this paper. These methods are mainly extensions of LDA, and add to the original model by introducing enhancements such as topic labeling, integration with contextual information and hierarchical modeling."
    }, {
      "heading" : "A. Topic Labeling",
      "text" : "In the early research stage, labels were often generated by hand [20]–[23]. Though manual labeling may generate more understandable and accurate semantics of a topic, it costs a lot of human effort and it is prone to subjectivity [24]. For example, in the most conventional LDA model, topics are interpreted by selecting the top words in the distribution [1], [20], [24], [25]. The Topics over Time (TOT) model implements continuous time stamps with each topic [24]. The model has been applied in three kinds of datasets, and results show more accurate topics and better timestamp predictions. However, the interpretation of topics is manual and post-hoc labeling can be time-consuming and subjective. As we can see, automatically generating meaningful topic labels is an\nimportant challenge for probabilistic topic modeling.\nMei et al. proposed probabilistic approaches to automatically interpreting multinomial topic models objectively. The intuition of this algorithm was to minimize the semantic distance between the topic model and the label. To this end, they extracted candidate labels from noun phrases chunked by an NLP Chunker and most significant 2-grams. Then\nthey ranked labels to minimize Kullback-Leibler divergence and maximize mutual information between a topic model and a label. The approach achieved the automatic interpretation of topics, but available candidate labels were limited to phrases inside documents.\nLau et al came up with an automatic topic label generation method which obtains candidate labels from Wikipedia articles containing the top-ranking topic terms, top-ranked document titles, and sub-phrases. To rank those candidates topic labels, they used different lexical measurements, such as point-wise mutual information (PMI), Student’s t-test, Dice’s coefficient and the log likelihood ratio [26]. Supervised methods like support vector regression (SVR) were also applied in the ranking process. Results showed that supervised algorithm outperforms unsupervised baseline in all four corpora.\nIn previous approaches, topics were treated individually and relation among topics was not considered. Mao et al created hierarchical descriptor for topics, and results proved that innertopic relation could increase the accuracy of topic labels [27]. Hulpus et al proposed a graph-based approach for topic labeling [28]. In Yashar Mehdad’s work, they built an entailment graph over phrases. Based on that, they then aggregated relevant phrases by generalization and merging [29]."
    }, {
      "heading" : "B. Supervised Labeling",
      "text" : "Supervised Latent Dirichlet Allocation (sLDA) is a supervised approach to labeling topics [13]. The approach includes a response variable into the LDA model to obtain latent topics that potentially provide an optimal prediction for the response variable of a new unlabeled document. This approach requires, during training, the manual input of individual topic labels and is constrained to permitting one label per topic.\nSimilar to sLDA is Discriminative LDA (DiscLDA) which attempts to solve the same problem as sLDA, but differs in the approach [14]. The differing approach was centered around introducing a class-dependent linear transformation on the topic mixture proportions. This transformation matrix was learned through a conditional likelihood criterion. This method has the benefit of both reducing the dimension of documents in the corpus and labeling the lower dimension documents.\nBoth sLDA and DiscLDA only allow for a supervised input set that label a single topic. An approach that allows is shown by ( d ).\nfor multiple labels in a topic is given by Labeled LDA (LLDA) [15]. This model differs in the generation of multinomial distribution theta over the topics in the model. The scaling parameter is then modified by a label projection matrix to restrict the\ndistribution to those topics considered most relevant to the document."
    }, {
      "heading" : "C. Contextual Integration",
      "text" : "An existing approach that takes into account concepts supplied by outside sources requires a manual input set of relevant terms [30]. In the topic model then these concepts are applied to the assignment of topics to a token in a document. Alongside this concept topic modeling a hierarchical method can also be used to incorporate concepts into a hierarchical structure. This work shows the utility of bringing in outside knowledge into topic modeling.\nAn approach that integrates Wikipedia information into the topic modeling differs than the supervised approach by only requiring an existing Wikipedia article [6]. The assumption in this work is that in the generative process the topics are selected from the Wikipedia word distributions. The results show that Wikipedia articles can be used as effective topics in topic modeling.\nWikipedia again was shown as a basis for topic modeling, albeit for a tangential approach, entity disambiguation [6]. The approach involved topic modeling as a way of annotating entities in text. This involved the use of a large dataset of topics so efficient methods were introduced. Experiments against a public dataset resulted in a state of the art performance."
    }, {
      "heading" : "VI. CONCLUSION",
      "text" : "We have described in this paper a novel methodology for unsupervised topic modeling with meaningful n-gram labels, as well as provided parallel algorithms to speed up the inference process. This methodology uses existing labeled knowledge sources to influence a topic model in order to allow the labels from these external sources to be used for topics generated over a corpus of interest. In addition, this approach results in more meaningful topics generated based on the quality of the external knowledge source. We have tested our methodology against the Reuters-21578 newswire collection corpus for labeling and Wikipedia as external knowledge sources. The analysis of the quality of topic models using PMI show SourceLDA to enhance existing topic models."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "A popular approach to topic modeling involves extracting co-occurring n-grams of a corpus into semantic themes. The set of n-grams in a theme represents an underlying topic, but most topic modeling approaches are not able to label these sets of words with a single n-gram. Such labels are useful for topic identification in summarization systems. This paper introduces a novel approach to labeling a group of n-grams comprising an individual topic. The approach taken is to complement the existing topic distributions over words with a known distribution based on a predefined set of topics. This is done by integrating existing labeled knowledge sources representing known potential topics into the probabilistic topic model. These knowledge sources are translated into a distribution and used to set the hyperparameters of the Dirichlet generated distribution over words. In the inference these modified distributions guide the convergence of the latent topics to conform with the complementary distributions. This approach ensures that the topic inference process is consistent with existing knowledge. The label assignment from the complementary knowledge sources are then transferred to the latent topics of the corpus. The results show both accurate label assignment to topics as well as improved topic generation than those obtained using various labeling approaches of Latent Dirichlet allocation (LDA) when compared by pointwise mutual information (PMI) assessment.",
    "creator" : "Microsoft® Word 2013"
  }
}