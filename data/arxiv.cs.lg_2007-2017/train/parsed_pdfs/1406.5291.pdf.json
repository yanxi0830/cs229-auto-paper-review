{
  "name" : "1406.5291.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generalized Dantzig Selector: Application to the k-support norm",
    "authors" : [ "Soumyadeep Chatterjee", "Sheng Chen" ],
    "emails" : [ "chatter@cs.umn.edu", "shengc@cs.umn.edu", "banerjee@cs.umn.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The Dantzig Selector (DS) [2, 3] provides an alternative to regularized regression approaches such as Lasso [16, 19] for sparse estimation. While DS does not consider a regularized maximum likelihood approach, [2] has established clear similarities between the estimates from DS and Lasso. While norm regularized regression approaches have been generalized to more general norms, such as decomposable norms [11], the literature on DS has primarily focused on the sparse L1 norm case, with a few notable exceptions which have considered extensions to sparse group-structured norms [8].\nIn this paper, we consider linear models of the form y = Xθ∗+w, where y ∈ Rn is a set of observations, X ∈ Rn×p is a design matrix, and w ∈ Rn is i.i.d. noise. For any given norm R(·), the parameter θ∗ is assumed to structured in terms of having a low value of R(θ∗). For this setting, we propose the following Generalized Dantzig Selector (GDS) for parameter estimation:\nθ̂ = argmin θ∈Rp\nR(θ)\ns.t.R∗ ( XT (y −Xθ) ) ≤ λp ,\n(1)\nwhere R∗(·) is the dual norm of R(·), and λp is a suitable constant. If R(·) is the L1 norm, (1) reduces to standard DS [3]. A key novel aspect of GDS is that the constraint is in terms of the dual norm R∗(·) of the\nar X\niv :1\n40 6.\n52 91\nv3 [\nst at\n.M L\n] 2\noriginal structure inducing norm R(·). It is instructive to contrast GDS with the recently proposed atomic norm based estimation framework [4] which, unlike GDS, considers constraints based on the L2 norm of the error ‖y −Xθ‖2, and focuses only on atomic norms.\nIn this paper, we consider both computational and statistical aspects of the GDS. For the L1-norm Dantzig selector, [3] proposed a primal-dual interior point method since the optimization is a linear program. DASSO and its generalization proposed in [6, 5] focused on homotopy methods, which provide a piecewise linear solution path through a sequential simplex-like algorithm. However, none of the algorithms above can be immediately extended to our general formulation. In recent work, the Alternating Direction Method of Multipliers (ADMM) has been applied to the L1 Dantzig selection problem [9, 18], and the linearized version in [18] proved to be efficient. Motivated by such results for DS, we propose a general inexact ADMM [17] framework for GDS where the primal update steps, interestingly, turn out respectively to be proximal updates involving R(θ) and its convex conjugate, the indicator of R∗(x) ≤ λ. As a result, by Moreau decomposition, it suffices to develop efficient proximal update for eitherR(θ) or its conjugate. On the statistical side, we establish non-asymptotic high-probability bounds on the estimation error ‖θ̂ − θ∗‖2. Interestingly, the bound depends on the Gaussian width of the unit norm ball ofR(·) as well as the Gaussian width of suitable set where the estimation error belongs [4, 13].\nAs a non-trivial example of the GDS framework, we consider estimation using the recently proposed k-support norm [1, 10]. We show that proximal operators for k-support norm can be efficiently computed in O(p log p+log k log(p−k)), and hence the estimation can be done efficiently. Note that existing work [1, 10] on k-support norm has focused on the proximal operator for the square of the k-support norm, which is not directly applicable in our setting. On the statistical side, we provide upper bounds for the Gaussian widths of the unit norm ball and the error set as needed in the GDS framework, yielding the first statistical recovery guarantee for estimation with the k-support norm.\nThe rest of the paper is organized as follows: We establish general optimization and statistical recovery results for GDS for any norm in Section 2. In Section 3, we present efficient algorithms and estimation error bounds for the k-support norm. We present experimental results in Section 4 and conclude in Section 5. All technical analyses and proofs are in the supplement."
    }, {
      "heading" : "2 General Optimization and Statistical Recovery Guarantees",
      "text" : "The problem in (1) is a convex program, and a suitable choice of λp ensures that the feasible set is not empty. We start the section with an inexact ADMM framework for solving problems of the form (1), and then present bounds on the estimation error establishing statistical consistency of GDS."
    }, {
      "heading" : "2.1 General Optimization Framework using Inexact ADMM",
      "text" : "In optimization, we temporarily drop the subscript p of λp for convenience. We let A = XTX, u = XTy, and define the set Cλ = {v : R∗(v) ≤ λ}. The optimization problem is equivalent to\nmin θ,v∈Rp\nR(θ) s.t. u−Aθ = v, v ∈ Cλ . (2)\nDue to the nonsmoothness of bothR andR∗, solving (2) can be quite challenging and a generally applicable algorithm is Alternating Direction Method of Multipliers (ADMM). The augmented Lagrangian function for (2) is given as\nLR(θ,v, z) = R(θ) + 〈z,Aθ + v − u〉+ ρ\n2 ||Aθ + v − u||22 . (3)\nin which z is the Lagrange multiplier and ρ controls the penalty introduced by the quadratic term. The iterative updates of the variables (θ,v, z) in standard ADMM are given by\nθk+1 ← argmin θ LR(θ,vk, zk) , (4) vk+1 ← argmin v∈Cλ LR(θk+1,v, zk) , (5) zk+1 ← zk + ρ(Aθk+1 + vk+1 − u) . (6)\nNote that update (4) amounts to a regularized least squares problem of θ, which can be computationally expensive. Thus we use an inexact update for θ instead, which can alleviate the computational cost and lead to a quite simple algorithm. Inspired by [18], we consider a simpler subproblem for the θ-update which minimizes\nL̃kR(θ,vk, zk) = R(θ) + 〈zk,Aθ + vk − u〉+ ρ\n2\n(∥∥Aθk + vk − u∥∥2 2 +\n2 〈 θ − θk,AT (Aθk + vk − u) 〉 + µ\n2\n∥∥θ − θk∥∥2 2 ) ,\n(7)\nwhere µ is a user-defined parameter. L̃kR(θ,vk, zk) can be viewed as an approximation of LR(θ,vk, zk) with the quadratic term linearized at θk. Then the update (4) is replaced by\nθk+1 ← argmin θ L̃kR(θ,vk, zk)\n= argmin θ { 2R(θ) ρµ + 1 2 ∥∥∥θ − (θk − 2 µ AT (Aθk + vk − u + z k ρ ) )∥∥∥2 2 } .\n(8)\nSimilarly the update of v in (5) can be recast as\nvk+1 ← argmin v∈Cλ LR(θk+1,v, zk) = argmin v∈Cλ\n1\n2 ∥∥v − (u−Aθk+1 − zk ρ ) ∥∥2 2 . (9)\nIn fact, the updates of both θ and v turn out to compute certain proximal operators. In general, the proximal operator proxh(·) of a closed proper convex function h : Rp → R ∪ {+∞} is defined as\nproxh(x) = argmin w∈Rp {1 2 ‖w − x‖22 + h(w) } .\nHence it is easy to see that (8) and (9) correspond to prox 2R ρµ (·) and proxICλ (·), respectively, where ICλ(·) is the indicator function of set Cλ given by\nICλ(x) = { 0 if x ∈ Cλ +∞ if otherwise .\nIn Algorithm 1, we provide our general ADMM for the GDS. For the ADMM to work, we need two subroutines that can efficiently compute the proximal operators for the functions in Line 3 and 4 respectively. The simplicity of the proposed approach stems from the fact that we in fact need only one subroutine, for any one of the functions, since the functions are conjugates of each other.\nAlgorithm 1 ADMM for Generalized Dantzig Selector\nInput: A = XTX, u = XTy, ρ, µ Output: Optimal θ̂ of (1)\n1: Initialize (θ,v, z) 2: while not converged do 3: θk+1 ← prox 2R\nρµ\n( θk − 2µAT (Aθk + vk − u + z k ρ ) )\n4: vk+1 ← proxICλ ( u−Aθk+1 − zkρ ) 5: zk+1 ← zk + ρ(Aθk+1 + vk+1 − u) 6: end while\nProposition 1 Given β > 0 and a norm R(·), the two functions, f(x) = βR(x) and g(x) = ICβ (x) are convex conjugate to each other, thus giving the following identity,\nx = proxf (x) + proxg(x) . (10)\nProof: The Proposition 1 simply follows the definition of convex conjugate and dual norm, and (10) is just Moreau decomposition provided in [12].\nThe decomposition enables conversion of the two types of proximal operator to each other at negligible cost (i.e., vector subtraction). Thus we have the flexibility in Algorithm 1 to focus on the proximal operator that is efficiently computable, and the other can be simply obtained through (10). Remark on convergence: Note that Algorithm 1 is a special case of inexact Bregman ADMM proposed in [17], which matches the case of linearizing quadratic penalty term by using Bϕ′θ(θ,θk) = 1 2‖θ − θk‖22 as Bregman divergence. In order to converge, the algorithm requires µ2 to be larger than the spectral radius of ATA, and the convergence rate is O(1/T ) according to Theorem 2 in [17]."
    }, {
      "heading" : "2.2 Statistical Recovery for Generalized Dantzig Selector",
      "text" : "Our goal is to provide error bounds on ‖θ̂ − θ∗‖2 between the population parameter θ∗ and the minimizer θ̂ of (1). Let the error vector be defined as ∆̂ = θ̂ − θ∗. For any set Ω ⊆ Rp, we would measure the size of this set using its Gaussian width [14, 4], which is defined as ω(Ω) = Eg [supz∈Ω〈g, z〉] , where g is a vector of i.i.d. standard Gaussian entries. We also consider the error cone TR(θ∗), generated by the set of possible error vectors ∆ and containing the error vector ∆̂, defined as\nTR(θ∗) := cone {∆ ∈ Rp : R(θ∗ + ∆) ≤ R(θ∗)} . (11)\nNote that this set contains a restricted set of directions and does not in general span the entire space of Rp. Further, let ΩR := {u : R(u) ≤ 1}. With these definitions, we obtain our main result.\nTheorem 1 Suppose the design matrix X consists of i.i.d. Gaussian entries with zero mean variance 1, and we solve the optimization problem (1) with\nλp ≥ cE [ R∗(XTw) ] . (12)\nThen, with probability at least (1− η1 exp(−η2n)), we have\n‖θ̂ − θ∗‖2 ≤ 4cΨRω(ΩR)\nκL √ n\n, (13)\nwhere ω(TR(θ∗) ∩ Sp−1) is the Gaussian width of the intersection of TR(θ∗) and the unit spherical shell Sp−1, ω(ΩR) is the Gaussian width of the unit norm ball, κL > 0 is the gain given by\nκL = 1\nn\n( `n − ω(TR(θ∗) ∩ Sp−1) )2 , (14)\nΨR = sup∆∈TR R(∆)/‖∆‖2 is a norm compatibility factor, `n is the expected length of a length n i.i.d. standard Gaussian vector with n√\nn+1 < `n <\n√ n, and c > 1, η1, η2 > 0 are constants.\nRemark: The choice of λp is also intimately connected to the notion of Gaussian width. Note that for X i.i.d. Gaussian entries, and w i.i.d. standard Gaussian vector, XTw = ‖w‖2 ( XT w‖w‖2 ) = ‖w‖2z where z is an i.i.d. standard Gaussian vector. Therefore,\nλp ≥ cE [ R∗(XTw) ] = cEw[‖w‖2] ·EX [ R∗(XT w‖w‖2 ) ] (15)\n= cEw[‖w‖2]Ez [\nsup u: R(u)≤1\n〈u, z〉 ]\n(16)\n= c`nω (ΩR) , (17)\nwhich is a scaled Gaussian width of the unit ball of the normR(·). Example: L1-norm Dantzig Selector When R(·) is chosen to be L1 norm, the dual norm is the L∞ norm, and (1) is reduced to the standard DS, given by\nθ̂ = argmin θ∈Rp\n‖θ‖1 s.t. ‖XT (y −Xθ)‖∞ ≤ λ . (18)\nWe know that proxβ‖·‖1(·) is given by the elementwise soft-thresholding operation[ proxβ‖·‖1(x) ] i\n= sign(xi) ·max(0, |xi| − β) . (19) Based on Proposition 1, the ADMM updates in Algorithm 1 can be instantiated as\nθk+1 ← prox 2‖·‖1 ρµ\n( θk − 2\nµ AT (Aθk + vk − u + z\nk ρ ) ) ,\nvk+1 ← (u−Aθk+1 − z k\nρ )− proxλ‖·‖1\n( u−Aθk+1 − z k\nρ\n) ,\nzk+1 ← zk + ρ(Aθk+1 + vk+1 − u) , where the update of v leverages the decomposition (10). Similar updates were used in [18] for L1-norm Dantzig selector.\nFor statistical recovery, we assume that θ∗ is s-sparse, i.e., contains s non-zero entries, and that ‖θ∗‖2 = 1, so that ‖θ∗‖1 ≤ s. It was shown in [4] that the Gaussian width of the set (TL1(θ∗) ∩ Sp−1) is upper bounded as ω(TL1(θ∗)∩ Sp−1)2 ≤ 2s log (p s ) + 54s. Also note that E [ R∗(XTw) ] = E[‖w‖2]E[‖g‖∞] ≤√\nn √\nlog p, where g is a vector of i.i.d. standard Gaussian entries [3]. Further, [11] has shown that ΨR =√ s. Therefore, if we solve (18) with λp = c √ n log p, then\n‖θ̂ − θ∗‖2 ≤ 4c √ s log p\nκL √ n = O\n(√ s log p\nn\n) (20)\nwith high probability, which agrees with known results for DS [2, 3].\n3 Dantzig Selection with k-support norm We first introduce some notations. Given any θ ∈ Rp, let |θ| denote its absolute-valued counterpart and θ↓ denote the permutation of θ with its elements arranged in decreasing order. In previous work [1, 10], the k-support norm is defined as\n‖θ‖spk = min  ∑ I∈G(k) ‖vI‖2 : supp(vI) ⊆ I, ∑ I∈G(k) vI = θ  , (21) where G(k) denotes the set of subsets of {1, . . . , p} of cardinality at most k. The unit ball of this norm is the set Ck = conv {θ ∈ Rp : ‖θ‖0 ≤ k, ‖θ‖2 ≤ 1} . The dual norm of the k-support norm is given by\n‖θ‖sp∗k = max { ‖θG‖2 : G ∈ G(k) } = ( k∑ i=1 |θ|↓2i ) 1 2 . (22)\nThe k-support norm was proposed in order to overcome some of the empirical shortcomings of the elastic net [20] and the (group)-sparse regularizers. It was shown in [1] to behave similarly as the elastic net in the sense that the unit norm ball of the k-support norm is within a constant factor of √ 2 of the unit elastic net ball. Although multiple papers have reported good empirical performance of the k-support norm on selecting highly correlated features, wherein L1 regularization fails, there exists no statistical analysis of the k-support norm. Besides, current computational methods consider square of k-support norm in their formulation, which might fail to work out in certain cases.\nIn the rest of this section, we focus on GDS withR(θ) = ‖θ‖spk given as\nθ̂ = argmin θ∈Rp\n‖θ‖spk s.t. ‖XT (y −Xθ)‖ sp∗ k ≤ λp . (23)\nFor the indicator function ICλ(·) of the dual norm, we present a fast algorithm for computing its proximal operator by exploiting the structure of its solution, which can be directly plugged in Algorithm 1 to solve (23). Further, we prove statistical recovery bounds for k-support norm Dantzig selection, which hold even for a high-dimensional scenario, where n < p."
    }, {
      "heading" : "3.1 Computation of Proximal Operator",
      "text" : "In order to solve (23), either proxλ‖·‖spk (·) or proxICλ (·) for ‖ · ‖ sp∗\nk should be efficiently computable. Existing methods [1, 10] are inapplicable to our scenario since they compute the proximal operator for squared k-support norm, from which proxICλ (·) cannot be directly obtained. In Theorem 2, we show that proxICλ (·) can be efficiently computed, and thus Algorithm 1 is applicable.\nTheorem 2 Given λ > 0 and x ∈ Rp, if ‖x‖sp∗k ≤ λ, then w∗ = proxICλ (x) = x. If ‖x‖ sp∗ k > λ, define Asr = ∑r i=s+1 |x| ↓ i , Bs = ∑s i=1(|x| ↓ i )\n2, in which 0 ≤ s < k and k ≤ r ≤ p, and construct the nonlinear equation of β,\n(k − s)A2sr [\n1 + β\nr − s+ (k − s)β\n]2 − λ2(1 + β)2 +Bs = 0 . (24)\nLet βsr be given by\nβsr = { nonnegative root of (24) if s > 0 and the root exists 0 otherwise . (25)\nThen the proximal operator w∗ = proxICλ (x) is given by\n|w∗|↓i =  1 1+βs∗r∗ |x|↓i if 1 ≤ i ≤ s∗√ λ2−Bs∗ k−s∗ if s ∗ < i ≤ r∗ and βs∗r∗ = 0 As∗r∗ r∗−s∗+(k−s∗)βs∗r∗ if s∗ < i ≤ r∗ and βs∗r∗ > 0\n|x|↓i if r∗ < i ≤ p\n, (26)\nwhere the indices s∗ and r∗ with computed |w∗|↓ make the following two inequalities hold,\n|w∗|↓s∗ > |w∗|↓k , (27)\n|x|↓r∗+1 ≤ |w∗|↓k < |x| ↓ r∗ . (28)\nThere might be multiple pairs of (s, r) satisfying the inequalities (27)-(28), and we choose the pair with the smallest ‖|x|↓ − |w|↓‖2. Finally, w∗ is obtained by sign-changing and reordering |w∗|↓ to conform to x.\nRemark: The nonlinear equation (24) is quartic, for which we can use general formula to get all the roots [15]. In addition, if it exists, the nonnegative root is unique, as we show in the proof.\nTheorem 2 indicates that computing proxICλ (·) requires sorting of entries in |x| and a two-dimensional linear search of s∗ and r∗. Hence the total time complexity is O(p log p + k(p − k)). However, a more careful observation can particularly reduce the search complexity from O(k(p− k)) to O(log k log(p− k)), which is motivated by Theorem 3.\nTheorem 3 In search of (s∗, r∗) defined in Theorem 2, there can be only one r̃ for a given candidate s̃ of s∗, such that the inequality (28) is satisfied. Moreover if such r̃ exists, then for any r < r̃, the associated |w̃|↓k violates the first part of (28), and for r > r̃, |w̃| ↓ k violates the second part of (28). On the other hand, based on the r̃, we have following assertion of s∗,\ns∗  > s̃ if r̃ does not exist ≥ s̃ if r̃ exists and corresponding |w̃|↓k satisfies (27) < s̃ if r̃ exists but corresponding |w̃|↓k violates (27) . (29)\nBased on Theorem 3, the accelerated search procedure of (s∗, r∗) is to execute a two-dimensional binary search, and Algorithm 2 gives the details. Therefore the total time complexity becomes O(p log p + log k log(p− k)). Compared with previous proximal operators for squared k-support norm, this complexity is better than that in [1], and roughly the same as the most recent one in [10].\n3.2 Statistical Recovery Guarantees for k-support norm\nThe analysis of the generalized Dantzig Selector for k-support norm consists of addressing two key challenges. First, note that Theorem 1 requires an appropriate choice of λp. Second, one needs to compute the Gaussian width of the subset of the error set TR(θ∗) ∩ Sp−1. For the k-support norm, we can get upper bounds to both of these quantities. We start by defining some notations. Let G∗ ⊆ G(k) be the set of groups intersecting with the support of θ∗, and let S be the union of groups in G∗, such that s = |S|. Then, we have the following bounds which are used for choosing λp, and bounding the Gaussian width.\nAlgorithm 2 Algorithm for computing proxICλ (·) of ‖ · ‖ sp∗ k Input: x, k, λ Output: w∗ = proxICλ (x)\n1: if ‖x‖sp∗k ≤ λ then 2: w∗ := x 3: else 4: l := 0, u := k − 1, and sort |x| to get |x|↓ 5: while l ≤ u do 6: s̃ := b(l + u)/2c, and binary search for r̃ that satisfies (28) and compute w̃ based on (26) 7: if r̃ does not exist then 8: l := s̃+ 1 9: else if r̃ exists and (27) is satisfied then\n10: w∗ := w̃, l := s̃+ 1 11: else if r̃ exists but (27) is not satisfied then 12: u := s̃− 1 13: end if 14: end while 15: end if\nTheorem 4 For the k-support norm Generalized Dantzig Selection problem (23), we obtain For the ksupport norm Generalized Dantzig Selection problem (23), we obtain\nE [ R∗(XTw) ] ≤ √n (√ 2k log (pe k ) + √ k ) (30)\nω(ΩR) ≤ (√ 2k log (pe k ) + √ k ) (31)\nω(TA(θ∗) ∩ Sp−1)2 ≤ (√ 2k log ( p− k − ⌈ s k ⌉ + 2 ) + √ k )2 · ⌈ s k ⌉ + s . (32)\nWe prove these two bounds using the analysis technique for group lasso with overlaps developed in [13]. Thereafter, choosing λp = √ n (√ 2k log (pe k ) + √ k )\n, and under the assumptions of Theorem 1, we obtain the following result on the error bound for the minimizer of (23).\nCorollary 1 Suppose that all conditions of Theorem 1 hold, and we solve (23) with λp chosen as above. Then, with high probability, we obtain\n‖θ̂ − θ∗‖2 ≤ 4cΨR\n(√ 2k log (pe k ) + √ k )\nκL √ n\n(33)\nRemark The error bound provides a natural interpretation for the two special cases of the k-support norm, viz. k = 1 and k = p. First, for k = 1 the k-support norm is exactly the same as the L1 norm, and the error\nbound obtained will be O (√\ns log p n\n) , the same as known results of DS, and shown in Section 2.2. Second,\nfor k = p, the k-support norm is equal to the L2 norm, and the error cone (11) is then simply a half space (there is no structural constraint). Therefore, ΨR = O(1), and the error bound scales as O (√ p n ) ."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "On optimization side, our ADMM framework is concentrated on its generality, and its efficiency has been shown in [18] for the special case of L1 norm. Hence we focus on the efficiency of different proximal operators related to k-support norm. On statistical side, we concentrate on the behavior and performance of GDS with k-support norm. All experiments are implemented in MATLAB."
    }, {
      "heading" : "4.1 Efficiency of Proximal Operator",
      "text" : "We tested four proximal operators related to k-support norm, which are our normal proxICλ (·) and its accelerated version, prox 1\n2β (‖·‖spk )2 (·) in [1], and proxλ 2 ‖·‖2Θ (·) in [10]. The dimension p of vector in experiment varied from 1000 to 10000, and the ratio p/k = {200, 100, 50, 20}. As illustrated in Figure 1, in general, the speedup of accelerated proxICλ (·) is considerable when compared with the normal proxICλ (·) and prox 1\n2β (‖·‖spk )2 (·). Empirically it is also slightly better than the proxλ 2 ‖·‖2Θ (·)."
    }, {
      "heading" : "4.2 Statistical Recovery on Synthetic Data",
      "text" : "Data generation We fixed p = 600, and θ∗ = (10, . . . , 10︸ ︷︷ ︸ 10 , 10, . . . , 10︸ ︷︷ ︸ 10 , 10, . . . , 10︸ ︷︷ ︸ 10 , 0, 0, . . . , 0︸ ︷︷ ︸ 570 ) throughout the experiment, in which nonzero entries were divided equally into three groups. The design matrix X were generated from a normal distribution such that the entries in the same group have the same mean sampled from N (0, 1). X was normalized afterwards. The response vector y was given by y = Xθ∗ + 0.01 × N (0, 1). The number of samples n is specified later.\nROC curves with different k We fixed n = 400 to obtain the ROC plot for k = {1, 10, 50} as shown in Figure 2(a). λp ranged from 10−2 to 103.\nL2 error vs. n We investigated how theL2 error ‖θ̂−θ∗‖2 of Dantzig selector changes as the number of samples increases, where k = {1, 10, 50} and n = {30, 60, 90, . . . , 300}. The plot is shown in Figure 2(b).\nL2 error vs. k We also looked at the L2 error with different k. We again fixed n = 400 and varied k from 1 to 39. For each k, we repeated the experiment 100 times, and obtained the mean and standard deviation plot in Figure 2(c)."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we introduced the GDS, which generalizes the standard L1-norm Dantzig Selector to estimation with any norm, such that structural information encoded in the norm can be efficiently exploited. A flexible framework based on inexact ADMM is proposed for solving the GDS, which only requires one of conjugate proximal operators to be efficiently solved. Further, we provide a unified statistical analysis framework for the GDS, which utilizes Gaussian widths of certain restricted sets for proving consistency. In the non-trivial example of k-support norm, we showed that the proximal operators used in the inexact ADMM can be computed more efficiently compared to previously proposed variants. Our statistical analysis for the k-support norm provides the first result of consistency of this structured norm. Last, experimental results provided sound support to the theoretical development in the paper."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The research was supported by NSF grant IIS-1029711. The work was also supported by NSF grants IIS0916750, IIS-0953274 and CNS-1314560 and by NASA grant NNX12AQ39A. A. B. acknowledges support from IBM and Yahoo."
    }, {
      "heading" : "A Proof of Theorem 1",
      "text" : "Statement of Theorem: Suppose the design matrix X consists of i.i.d. Gaussian entries with zero mean variance 1, and we solve the optimization problem (1) with\nλp ≥ cE [ R∗(XTw) ] . (34)\nThen, with probability at least (1− η1 exp(−η2n)), we have\n‖θ̂ − θ∗‖2 ≤ 4cΨRω(ΩR)\nκL √ n\n, (35)\nwhere ω(TR(θ∗) ∩ Sp−1) is the Gaussian width of the intersection of TR(θ∗) and the unit spherical shell Sp−1, ω(ΩR) is the Gaussian width of the unit norm ball, κL > 0 is the gain given by\nκL = 1\nn\n( `n − ω(TR(θ∗) ∩ Sp−1) )2 , (36)\nΨR = sup∆∈TR R(∆)/‖∆‖2 is a norm compatibility factor, `n is the expected length of a length n i.i.d. standard Gaussian vector with n√\nn+1 < `n <\n√ n, and c > 1, η1, η2 > 0 are constants.\nProof: We use the following lemma for the proof.\nLemma 1 Suppose we solve the minimization problem (1) with λp ≥ R∗ ( XTw ) . Then the error vector ∆̂ belongs to the set TR(θ∗) := cone {∆ ∈ Rp : R(θ∗ + ∆) ≤ R(θ∗)} , (37)\nand the error ∆̂ = θ̂ − θ∗ satisfies the following bound R∗ ( XTX∆̂ ) ≤ 2λp (38)\nProof: By our choice of λp, both θ∗ and θ̂ lie in the feasible set of (1) , and by optimality of θ̂, R ( θ∗ + ∆̂ ) = R(θ̂) ≤ R(θ∗) . (39)\nAlso, by triangle inequality R∗ ( XTX∆̂ ) = R∗ ( XTX(θ̂ − θ∗) ) (40)\n≤ R∗ ( XT (y −Xθ∗) ) +R∗ ( XT (y −Xθ̂) ) ≤ 2λp . (41)\nNow, note that X and w are independent and we can rewrite\nEX,w [ R∗(XTw) ] = Ew [ EX [ R∗(XTw)|w ]] = Ew [ ‖w‖2EX [ R∗ ( XT w\n‖w‖2\n) |w ]] . (42)\nSince w/‖w‖2 is an isotropic unit vector uniformly distributed over the surface of the unit sphere,( XT w‖w‖2 ) = g is an i.i.d. N (0, 1) Gaussian vector. Therefore\nEX,w [ R∗(XTw) ] = Ew[‖w‖2]Eg[R∗(g)] . (43)\nAlso, note that R∗(·) is Lipschitz continuous with Lipschitz constant of 1 w.r.t. the normR∗, and hence by Gaussian concentration of Lipschitz functions [7],\nP (R∗(g) ≥ Eg[R∗(g)] + τ) ≤ exp [ −τ 2\n2\n] , (44)\nand similarly ‖w‖2 ≤ `n + τ with probability at least 1 − exp(−τ2/2), where n√n+1 ≤ `n ≤ √ n is the expected length of w. Therefore, for some c > 1 choosing λp ≥ cE [ R∗(XTw) ] = c `nEg[R∗(g)] implies that\nP ( λp ≥ R∗(XTw) ) ≥ ( 1− exp [ − c1E 2 g[R∗(g)]\n2\n])( 1− exp [ −c2` 2 n\n2\n]) = 1−η′1 exp(−η′2n) , (45)\nfor some constant c1, c2, η′1, η ′ 2 > 0. Further, note that Eg[R∗(g)] = ω(ΩR), the Gaussian width of the unit ball of normR. Also, from Lemma 1, we have\nR∗ ( XTX∆̂ ) ≤ 2λp (46)\nNow, note that ‖X∆̂‖22 = 〈∆̂,XTX∆̂〉 ≤ |〈∆̂,XTX∆̂〉| ≤ R(∆̂)R∗ ( XTX∆̂ ) ≤ 2λpR(∆̂) , (47) where we have used Hölder’s inequality, and the boundR∗ ( XTX∆̂ ) ≤ 2λp from above.\nNext, we use Gordon’s theorem, which states that for X with i.i.d. Gaussian (0, 1) entries,\nE [ min\nz∈TR(θ∗)∩Sp−1 ‖Xz‖2\n] ≥ `n − ω ( TR(θ∗) ∩ Sp−1 ) , (48)\nwhere `n is the expected length of an i.i.d. Gaussian random vector of length n, and ω ( TR(θ∗) ∩ Sp−1 ) is the Gaussian width of the set Ω = ( TR(θ∗) ∩ Sp−1 ) . Now, since the function X → minz∈Ω ‖Xz‖2 is Lipschitz continuous with constant 1 over the set Ω, we can use Gaussian concentration of Lipschitz functions [7] to obtain\n‖X∆‖2 ≥ 1\n2\n( `n − ω(TR(θ∗) ∩ Sp−1) ) ‖∆‖2 (49)\n⇒ 1√ n ‖X∆‖2 ≥\n( `n − ω(TR(θ∗) ∩ Sp−1) ) 2 √ n ‖∆‖2 (50)\n⇒ 1 n ‖X∆‖22 ≥ κL 2 ‖∆‖22 , (51)\nwith probability greater than 1− exp ( −18 ( `n − ω(TR(θ∗) ∩ Sp−1) )2) = 1− η′′1 exp(−η′′2n), where κL =(\n`n − ω(TR(θ∗) ∩ Sp−1) )2 /n > 0 is the gain, and η′′1 , η ′′ 2 > 0 are constants .\nCombining (51) and (47), and using the choice of λp, we obtain\n‖θ̂n − θ∗‖2 = ‖∆̂‖2 ≤ 4cE\n[ R∗(XTw) ] κLn R(∆) ‖∆‖2 ≤ 4cΨRω(ΩR) κL √ n\n(52)\nwith probability greater than (1 − η′1 exp(−η′2n))(1 − η′′1 exp(−η′′2n)) = 1 − η1 exp(−η2n), for constants η1, η2 where\nΨR = sup ∆∈TR R(∆) ‖∆‖2 . (53)\nThe statement of the theorem follows."
    }, {
      "heading" : "B Proof of Theorem 2",
      "text" : "Statement of Theorem: Given λ > 0 and x ∈ Rp, if ‖x‖sp∗k ≤ λ, then w∗ = proxICλ (x) = x. If ‖x‖sp∗k > λ, define Asr = ∑r i=s+1 |x| ↓ i , Bs = ∑s i=1(|x| ↓ i )\n2, in which 0 ≤ s < k and k ≤ r ≤ p, and construct the nonlinear equation of β,\n(k − s)A2sr [\n1 + β\nr − s+ (k − s)β\n]2 − λ2(1 + β)2 +Bs = 0 . (54)\nLet βsr be given by\nβsr = { nonnegative root of (54) if s > 0 and the root exists 0 otherwise . (55)\nThen the proximal operator w∗ = proxICλ (x) is given by\n|w∗|↓i =  1 1+βs∗r∗ |x|↓i if 1 ≤ i ≤ s∗√ λ2−Bs∗ k−s∗ if s ∗ < i ≤ r∗ and βs∗r∗ = 0 As∗r∗ r∗−s∗+(k−s∗)βs∗r∗ if s∗ < i ≤ r∗ and βs∗r∗ > 0\n|x|↓i if r∗ < i ≤ p\n, (56)\nwhere the indices s∗ and r∗ with computed |w∗|↓ make the following two inequalities hold,\n|w∗|↓s∗ > |w∗|↓k , (57)\n|x|↓r∗+1 ≤ |w∗|↓k < |x| ↓ r∗ . (58)\nThere might be multiple pairs of (s, r) satisfying the inequalities (57)-(58), and we choose the pair with the smallest ‖|x|↓ − |w|↓‖2. Finally, w∗ is obtained by sign-changing and reordering |w∗|↓ to conform to x. Proof: Let w∗ = proxICλ (x) = argminw∈Cλ 1 2‖x − w‖22. For simplicity, we drop the constant 12 in later discussion. Given a vector x, we use the notation xi:j to denote its subvector (xi,xi+1, . . . ,xj). We consider the following two cases.\nCase 1: if ‖x‖sp∗k ≤ λ, it is trivial that w∗ = x, which is also the global minimizer of ‖x−w‖22 without the constraint x ∈ Cλ.\nCase 2: if ‖x‖sp∗k > λ, first we start by noting that given x and w, the following inequality holds\n‖x−w‖22 = ‖x‖22 − 2〈x,w〉+ ‖w‖22 ≥ ‖x‖22 − 2〈|x|↓, |w|↓〉+ ‖w‖22 ,\nwhich implies that w∗ should achieve this lower bound by conforming with the signs and orders of elements in x. Without loss of generality, we are simply focused on the case where x = |x|↓.\nFor w∗ to be the optimal, w∗k:p should be chosen such that w ∗ k:r = (w ∗ k,w ∗ k, . . . ,w ∗ k) and w ∗ r+1:p =\nx∗r+1:p, where r satisfies\nxr > w ∗ k ≥ xr+1 ,\notherwise either the decreasing order of w∗ will be violated or the ‖xk:p −wk:p‖2 is not minimized. As for w∗1:k−1, we similarly assume w ∗ s+1:k−1 = (w ∗ k,w ∗ k, . . . ,w ∗ k) for some 0 ≤ s ≤ k − 1, then w∗1:s should be chosen to minimize ‖x1:s −w1:s‖2 such that\n‖w1:s‖22 = ‖w∗1:k‖22 − ‖w∗s+1:k‖22 ≤ λ2 − (k − s)(w∗k)2.\nBy Cauchy-Schwarz Inequality, we have\n‖x1:s −w1:s‖22 ≥ ‖x1:s‖22 − 2‖x1:s‖2‖w1:s‖2 + ‖w1:s‖22 ,\nwhere the equality holds when w∗1:s follows the form of w ∗ 1:s = 1 1+βsr x1:s, and βsr ≥ 0 satisfies the constraint Bs\n(1+βsr)2 = λ2 − (k − s)(wk)2.\nSo far we have figured out the structure of w∗ = (w∗1:s,w ∗ s+1:r,w ∗ r+1:p), in which the three subvectors, compared with x, are shrunk by a common factor 1 + βsr, constant w∗k, or unchanged. Next we need to determine the value of βsr and w∗k. By optimality, ‖x −w‖22 = ‖x1:r −w1:r‖22 must be minimized at w∗, so we have the following problem,\nmin β,wk\n‖x1:r −w1:r‖22 = ‖x1:s −w1:s‖22 + ‖xs+1:r −ws+1:r‖22\n= ( β\n1 + β )2Bs + r∑ i=s+1 (xi −wk)2 (59)\ns.t. (‖w‖sp∗k )2 = Bs\n(1 + β)2 + (k − s)(wk)2 = λ2 (60)\nReplacing wk in (59) with wk =\n√ λ2− Bs\n(1+β)2\nk−s obtained from (60), we express ‖x1:r −w1:r‖22 as a function of β,\nΦsr(β) = ( β\n1 + β )2Bs + r∑ i=s+1 ( xi −\n√ λ2 − Bs\n(1+β)2 k − s )2 (61)\nSet derivative of Φsr(β) to be zero, we have\nd\ndβ Φsr(β) =\nd\ndβ\n[ ( β\n1 + β )2Bs + r∑ i=s+1 ( xi −\n√ λ2 − Bs\n(1+β)2 k − s )2] (62)\n= 2β\n(1 + β)3 Bs −\n2AsrBs (1 + β)3(k − s) √ λ2− Bs (1+β)2\nk−s\n+ 2(r − s)Bs\n(k − s)(1 + β)3 (63)\n= 2Bs (k − s)(1 + β)3 [ (k − s)β − Asr√\nλ2− Bs (1+β)2\nk−s\n+ (r − s) ] = 0 (64)\nIf s > 0, then Bs > 0 and (64) is equivalent to (54). And we can see that the quantity inside the bracket of (64) is monotonically increasing when β ≥ max(0, √ Bs−λ λ ), thus ensuring the nonnegative root βsr is unique if it exists. If the nonnegative root exists, the expression for w∗s+1:r can be obtained from (64), whose entries are all equal to w∗k.\nIf s > 0 and a nonnegative root of (64) is nonexistent, the derivative is always positive when β ≥ 0, which means that Φsr(β) is increasing. Hence the minimizer of Φsr(β) is βsr = 0. If s = 0, we actually do not care about the value of βsr because the problem defined by (59) and (60) is independent of β, and we set it to be 0 for simplicity. According to (60), both cases of βsr = 0 lead to the same expression for w∗s+1:r in (56).\nAs we do not know beforehand which s and r to choose, we need to search for s∗ and r∗ that give the smallest ‖|x|↓ − |w|↓‖2, and also need to check whether the w∗ obtained by (56) is in decreasing order, which are the conditions (57) and (58) presented in Theorem 2."
    }, {
      "heading" : "C Proof of Theorem 3",
      "text" : "To prove Theorem 3, we first need the following corollary from Theorem 2.\nCorollary 2 When β ≥ max(0, √ Bs−λ λ ), Φsr(β) defined in (61) is decreasing when β < βsr, and increasing when β > βsr. Equivalently, Φsr(β) = ‖x1:r −w1:r‖22, when treated as function of wk, is decreasing when wk < w∗k and increasing when wk > w ∗ k.\nProof: The first part simply follows the monotonicity of ddβΦsr(β) mentioned in the proof of Theorem 2, which implies that ddβΦsr(β) is negative when β < βsr, and positive when β > βsr . The constraint (60) implies that wk increases as β increases. So ‖x1:r−w1:r‖22, as a function of wk, has the same monotonicity w.r.t. wk.\nStatement of Theorem: In search of (s∗, r∗) defined in Theorem 2, there can be only one r̃ for a given candidate s̃ of s∗, such that the inequality (58) is satisfied. Moreover if such r̃ exists, then for any r < r̃, the associated |w̃|↓k violates the first part of (58), and for r > r̃, |w̃| ↓ k violates the second part of (58). On the other hand, based on the r̃, we have following assertion of s∗,\ns∗  > s̃ if r̃ does not exist ≥ s̃ if r̃ exists and corresponding |w̃|↓k satisfies (57) < s̃ if r̃ exists but corresponding |w̃|↓k violates (57) . (65)\nProof: We again focus on the case of x = |x|↓. First we show by contradiction that for a given s̃, the r̃ that satisfies (58) can be at most one.\nSuppose there are two indices, say r1 and r2, which satisfy that condition with the same s̃. Without loss of generality, let r1 < r2, we know that their corresponding w(1) and w(2) should minimize ‖x1:r1−w1:r1‖22 and ‖x1:r2 −w1:r2‖22, respectively. As r1 < r2, then w (1) k ≥ xr2 > w (2) k according to (58). Construct\nw′ = ( x1\n1 + β′ , . . . , xs̃ 1 + β′︸ ︷︷ ︸\ns̃\n,xr2 , . . . ,xr2︸ ︷︷ ︸ r2−s̃ ,xr2+1, . . . ,xp)\nwhere β′ is chosen to satisfy the constraint (60) with w′k = xr2 , and ‖x1:r2 −w (2) 1:r2 ‖22 can be decomposed\nas\n‖x1:r2 −w (2) 1:r2 ‖22 = ‖x1:r1 −w (2) 1:r1 ‖22 + ‖xr1+1:r2 −w (2) r1+1:r2 ‖22 > ‖x1:r1 −w′1:r1‖22 + ‖xr1+1:r2 −w′r1+1:r2‖22 = ‖x1:r2 −w′1:r2‖22\nwhich contradicts that w(2)1:r2 minimizes ‖x1:r2 −w1:r2‖22. Note that ‖x1:r1 −w (2) 1:r1 ‖22 > ‖x1:r1 −w′1:r1‖22 simply follows Corollary 2 as w(1)k ≥ xr2 = w′k > w (2) k , and ‖xr1+1:r2 − w (2) r1+1:r2\n‖22 > ‖xr1+1:r2 − w′r1+1:r2‖22 is due to the fact that xr1+1 ≥ . . . ≥ xr2 = w′k > w (2) k .\nNext we show by contradiction that if r̃ exists for given s̃, then any r < r̃ violates the first part of (58), and any r > r̃ violates the second part.\nLet w̃ denote the minimizer of ‖x1:r̃ −w1:r̃‖22. Suppose r < r̃ and the first part of (58) is not violated, then its second part must be violated due to the uniqueness of r̃. Then we can construct new\nw′ = ( x1\n1 + β′ , . . . , xs̃ 1 + β′︸ ︷︷ ︸\ns̃\n,xr̃, . . . ,xr̃︸ ︷︷ ︸ r̃−s̃ ,xr̃+1, . . . ,xp) ,\nwhere β′ is again chosen to satisfy the constraint (60) with w′k = xr̃. This by the same argument for proving the uniqueness of r̃ make the following inequality hold,\n‖x1:r̃ − w̃1:r̃‖22 = ‖x1:r − w̃1:r‖22 + ‖xr+1:r̃ − w̃r+1:r̃‖22 > ‖x1:r −w′1:r‖22 + ‖xr+1:r̃ −w′r+1:r̃‖22 = ‖x1:r̃ −w′1:r̃‖22 .\nThis contradicts that w̃ is the minimizer of ‖x1:r̃−w1:r̃‖22. Similar argument applies to the case when r > r̃. Let β′′ satisfy (60) together with w′′k = xr+1, and we construct\nw′′ = ( x1\n1 + β′′ , . . . , xs 1 + β′′︸ ︷︷ ︸\ns̃\n,xr+1, . . . ,xr+1︸ ︷︷ ︸ r−s̃ ,xr+1, . . . ,xp) ,\nwhich gives smaller ‖x1:r − w1:r‖22 than any w with wk < xr+1. Therefore it is impossible for r > r̃ to violate the first inequality.\nFinally we show the assertion (65) for s∗. We note that given s̃ , finding solution to the proximal operator can be viewed as minimization of (59) under the constraint ‖w1:k‖2 ≤ λ and wk = wk−1 = . . . = ws̃+1. So for s < s̃, the minimization problem is equivalent to the one for s̃ under additional constraint ws̃+1 = ws̃ = . . . = ws+1. If the r̃ does not exist, for s < s̃, r̃ is nonexistent either, thus s∗ > s̃. If the r̃ exists and (57) is satisfied, then s∗ ≥ s̃ because s < s̃ considers a more restricted problem and is unable to obtain a smaller ‖x−w‖2.\nFor the situation in which r̃ exists for s̃ but the associated w̃k violates (57), we show by contradiction that for any s > s̃, (57) is also violated.\nAssume that w′ (different from the previously used) satisfies both (57) and (58) for s′ = s̃ + 1 and the corresponding r′. It is not difficult to see that w′k < w̃k and r\n′ ≥ r̃, otherwise ‖w′1:k‖2 > λ. By the violation we have shown for r, the minimizer of (59) for (s′, r̃), denoted by w′′, satisfies w′′k ≤ w′k (Note that w′ is the minimizer of (59) for (s′, r′) and r′ ≥ r̃). Combined with w′k < w̃k, this indicates by Corollary 2 that Φs′r̃(·) is increasing on the interval [w′′k, w̃k]. Then we consider two sequential modifications on w̃,\n1. Replacing the w̃1:s′ in w̃ with ‖w̃1:s′‖2 ‖x1:s′‖2 x1:s′ ,\n2. Decreasing w̃s′+1:r̃ by certain amount and amplifying the new w̃1:s′ by some factor, such that (60) still holds for s′ and w̃s′+1 = w̃s′ .\nNote that the two modifications both decrease ‖x1:r̃ − w̃1:r̃‖2. Decrease in Modification 1 is the result of Cauchy Schwarz Inequality, and decrease in Modification 2 is due to the monotonicity of Φs′r̃(·) we mentioned afront. The modified w̃ satisfies w̃s̃+1 = w̃s̃+2 = . . . = w̃k, thus contradicting that the old w̃ is the minimizer of (59) for (s̃, r̃). Hence, by induction, we conclude that for any s′ > s̃, its solution also violates (57).\nAssembling the conclusions above, we have (65) for s∗."
    }, {
      "heading" : "D Proof of Theorem 4",
      "text" : "Statement of Theorem: For the k-support norm Generalized Dantzig Selection problem (23), we obtain\nE [ R∗(XTw) ] ≤ √n (√ 2k log (pe k ) + √ k ) (66)\nω(ΩR) ≤ (√ 2k log (pe k ) + √ k ) (67)\nω(TA(θ∗) ∩ Sp−1)2 ≤ (√ 2k log ( p− k − ⌈ s k ⌉ + 2 ) + √ k )2 · ⌈ s k ⌉ + s . (68)\nProof: We first illustrate that the k-support norm is an atomic norm, and then prove Theorem 4.\nD.1 k-Support norm as an Atomic Norm\nHere we show that k-support norm satisfies the definition of atomic norms [4]. Consider Gj to be the set of all subsets of {1, 2, . . . , p} of size j, so that\nG(k) = {Gj}kj=1 . (69)\nFor every j, consider the set\nAj = {w : ‖(wGj )‖2 = 1, Gj ∈ Gj , wi = 1√ j , ∀i ∈ Gj , wi = 0, ∀i /∈ Gj} , (70)\ncorresponding to Gj , and the union of such sets\nA = {Aj}j∈{1,...,k} . (71)\nNote that since every non-zero element in a vector in Aj is 1√j , such an element cannot be represented as a convex combination of elements of the set Al, l < j, whose non-zero elements are 1√l . Therefore none of the elements w in the set A lies in the convex hull of the other elements A \\ {w}. Further, note that\nconv(A) = Ck , (72)\nand the k-support norm defines the gauge function of the A. Thus the k-support norm is an atomic norm.\nD.2 The Error set and its Gaussian width\nNote that the cardinality of the set G(k) is\nM =\n( p\nk\n) + ( p\nk − 1\n) + ( p\nk − 2\n) + · · ·+ ( p\n1\n) (73)\nThe error set is given by\nTA(θ∗) = cone{∆ ∈ Rp : ‖∆ + θ∗‖spk ≤ ‖θ∗‖ sp k } . (74)\nNote that this set is a cone, and we can define the normal cone of this set as\nNA(θ∗) = {u : 〈u,∆〉 ≤ 0, ∀∆ ∈ TA(θ∗)} (75) (76)\nThe following proposition, shown in [13], shows that the normal cone can be written in terms of the dual norm of the k-support norm.\nProposition 2 The normal cone to the tangent cone defined in (74) can written as\nNA(θ∗) = {u : ∃t > 0 s.t. 〈u,θ∗〉 = t‖θ∗‖spk , ‖u‖ sp∗ k ≤ t} . (77)\nWe provide a simple proof of this statement for our case for ease of understanding. Proof: We re-write the definition of the normal cone in terms of the estimated parameter θ̂ as\nNA(θ∗) = {u ∈ Rp : 〈u,θ − θ∗〉 ≤ 0,∀θ − θ∗ ∈ TA(θ∗)} . (78)\nNote that this means that u ∈ NA(θ∗) if and only if\n〈u,θ − θ∗〉 ≤ 0, ∀‖θ‖spk ≤ ‖θ∗‖ sp k (79) ⇒〈u,θ〉 ≤ 〈u,θ∗〉 ∀‖θ‖spk ≤ ‖θ∗‖ sp k . (80)\nNow, we claim that 〈u,θ∗〉 ≥ 0 for all such u. This can be shown as follows. Assume the contrary, i.e. there exists a û ∈ NA(θ∗) such that 〈û,θ∗〉 < 0. Now, noting that (−θ∗) ∈ TA(θ∗), we have\n〈û,−θ∗〉 = −〈û,θ∗〉 > 0 , (81)\nso that û /∈ NA(θ∗), which is a contradiction, and the claim follows. Therefore, we can write 〈u,θ∗〉 = t‖θ∗‖spk (82) for some t ≥ 0. Then, u ∈ NA(θ∗) if and only if\n∃t ≥ 0 , 〈u,θ∗〉 = t‖θ∗‖spk , 〈u,θ〉 ≤ t‖θ∗‖ sp k ∀‖θ‖ sp k ≤ ‖θ∗‖ sp k . (83)\nSince 〈u,θ〉 ≤ t‖θ∗‖spk , ∀‖θ‖ sp k ≤ ‖θ∗‖ sp k ⇒ ‖u‖ sp∗\nk ≤ t , (84) the statement follows.\nThe k-support norm can be thought of as a group sparse norm with overlaps, such as been dealt with in [13]. Therefore, we can utilize some of the analysis techniques developed in [13], specialized to the\nstructure of the k-support norm. We begin by stating a theorem which enables us to bound the Gaussian width of the error set. Henceforth, we write NA = NA(θ∗) and TA = TA(θ∗) where the dependence on θ∗ is understood.\nFirst, we define sets that involve the support set of θ∗. Let us define the set G∗ ⊆ G(k) to be the set of all groups in G(k) which overlap with the support of θ∗, i.e.\nG∗ = {G ∈ G(k) : G ∩ supp(θ∗) 6= ∅} . (85) Let S be the union of all groups in G∗, i.e. S = ⋃G∈G∗ G, and the size of S be |S| = s. We are going to use three lemmas in order to prove the above bound. The first lemma, proved in [4], upper bounds the Gaussian width by an expected distance to the normal cone as follows.\nLemma 2 ([4] Proposition 3.6) Let C be any nonempty convex in Rp, and g ∼ N (0, Ip) be a random gaussian vector. Then ω(C ∩ Sp−1) ≤ Eg[dist(g,C∗)] , (86) where C∗ is the polar cone of C.\nNote that NA is the polar cone of TA by definition. Therefore, using Jensen’s inequality, we obtain ω(TA ∩ Sp−1)2 ≤ E2g[dist(g,NA)] ≤ Eg[dist(g,NA)2] ≤ Eg[‖g − z(g)‖22] , (87)\nwhere z(g) ∈ NA is a (random) vector constructed to lie always in the normal cone. The construction proceeds as follows. Constructing z(g): Note that θ∗Sc = 0. Let us choose a vector v ∈ NA such that ‖v‖sp∗k = 1 and vSc = 0 . (88) We can choose an appropriately scaled v so that\n〈v,θ∗〉 = ‖θ∗‖spk , (89) and let us write without loss of generality v = [vS vSc ].\nNext, let g ∼ N (0, Ip), and write g = [gS gSc ]. We define the quantity\nt(g) = max { ‖gG‖2 : G ∈ G(k), G ⊆ Sc } = max  (∑ i∈G g2i ) 1 2 : G ∈ G(k), G ⊆ Sc  , (90)\nand let z = z(g) = [zS zSc ] such that\nzS = t(g)vS , zSc = gSc . (91)\nNote that 〈z,θ∗〉 = t(g)〈vS ,θ∗S〉 = t(g)‖θ∗‖spk , (92)\nand ‖z‖sp∗k = max { ‖zG‖2 : G ∈ G(k) } (93)\n= max { max{‖zG‖2 : G ∈ G(k), G ⊆ S} , max{‖zG‖2 : G ∈ G(k), G ⊆ Sc} }\n(94)\n(a) = max { t(g)‖v‖sp∗k , t(g) } (95)\n= t(g) (96)\nwhere (a) follows from the definition of t(g) and the fact that\nmax{‖zG‖2 : G ∈ G(k), G ⊆ S} = t(g) max{‖vG‖2 : G ∈ G(k), G ⊆ S} = t(g)‖v‖sp ∗ k , (97)\nand since ‖v‖sp∗k = 1. Therefore, z(g) ∈ NA(θ∗) by definition in (77) . In order to upper bound the expectation of t(g), we use the following comparison inequality from [13].\nLemma 3 ([13] Lemma 3.2) Let q1, q2, . . . , qL be L, χ-squared random variables with d degrees of freedom. Then\nE [ max\n1≤i≤L qi\n] ≤ (√ 2 logL+ √ d )2 . (98)\nLast, we prove an upper bound on the expected value of t(g), as shown in the following lemma.\nLemma 4 Consider G∗ ⊆ G(k) to be the set of groups intersecting with the support of θ∗, and let S be the union of groups in G∗, such that s = |S|. Then,\nEg[t(g) 2] ≤ (√ 2k log ( p− k − ⌈ s k ⌉ + 2 ) + √ k )2 . (99)\nProof: Note that\nEg[t(g) 2] = Eg [( max { ‖gG‖2 : G ∈ G(k), G ⊆ Sc })2] (100)\n≤ Eg [ max { ‖gG‖22 : G ∈ G(k), G ⊆ Sc }] (101)\nEach term ‖gG‖22 is a χ-squared variable with at most k degrees of freedom. Since the set S has size s, the set G∗ has to contain at least sk = ⌈ s k ⌉ groups of size k. Therefore,\ns = |S| ≥ k + (sk − 1) , (102)\nand therefore the size of its complement is upper bounded by\n|Sc| ≤ p− k − sk + 1 . (103)\nTherefore the following inequality provides an upper bound on the number of groups involved in computing the maximum in (101)∣∣∣{G ∈ G(k), G ⊆ Sc}∣∣∣ ≤ (p− k − sk + 1\nk\n) + ( p− k − sk + 1\nk − 1\n) + · · ·+ ( p− k − sk + 1\n1\n) (104)\n≤ (p− k − sk + 2)k (105)\nwhere we have used the following inequality( n\nh\n) ≤ n h\nh! , ∀n ≥ h ≥ 0 , (106)\nwhich also provides k∑\nh=1\n( n\nh\n) ≤ (n+ 1)k . (107)\nTherefore, we can upper bound (101) using Lemma 3 as\nEg[t(g) 2] ≤ Eg [ max { ‖gG‖22 : G ∈ G(k), G ⊆ Sc }] (108)\n≤ (√ 2 log ( (p− k − ⌈ s k ⌉ + 2)k ) + √ k )2 (109)\nand the statement follows.\nNow we are ready to prove the upper bound on the Gaussian width. First, note that\nω(TA(θ∗) ∩ Sp−1)2 ≤ Eg[dist(g,NA(θ∗))2] (110) (a) ≤ Eg[‖g − z(g)‖22] (111) = Ew[‖zS − gS‖22] (112)\n(b) = E[‖zS‖22] + E[‖gS‖22] (113) (c) = E[t(g) 2] · ‖vS‖22 + |S| (114)\n(d) ≤\n(√ 2k log ( (p− k − ⌈ s k ⌉ + 2) ) + √ k )2 · ⌈ s k ⌉ + s , (115)\nwhere (a) follows from the definition of distance to a set, (b) follows from the independence of gS and gSc , (c) follows from the fact that the expected length of an |S| length random i.i.d. Gaussian vector is √ |S|, and\n(d) follows since |S| = ksk , and that ‖vS‖2 ≤ √⌈ s k ⌉ ‖vS‖sp ∗ k = √⌈ s k ⌉ . Thus inequality (68) follows.\nNext, we prove inequality (66). Let us denote t = XT (\nw ‖w‖2\n) , and note that t ∼ N (0, Ip). Also note\nthat E [ R∗(XTw) ] = E[‖w‖2‖]E[R∗(t)], and\n‖t‖sp∗k = max{‖tG‖2 : G ∈ G(k)} . (116)\nTherefore, we can use Lemma 3 in order to bound the expectation E[‖t‖sp∗k ] as\nE[‖t‖sp∗k ] = E[max{‖tG‖2 : G ∈ G(k)}] (117) = E[max{‖tG‖2 : G ∈ G(k), |G| = k} (118)\n≤ (√ 2 log ( p\nk\n) + √ k ) (119)\n≤ (√ 2k log (pe k ) + √ k ) , (120)\nwhere we have used the following inequality obtained using Stirling’s approximation( p\nk ) ≤ (pe k )k . (121)\nTherefore, inequality (66) follows, and by our choice of λp, with high probability, θ∗ lies in the feasible set. Last, note that\nω(ΩR) = E[‖t‖sp ∗ k ] ≤ (√ 2k log (pe k ) + √ k ) , (122)\nas proved above."
    } ],
    "references" : [ {
      "title" : "Sparse prediction with the k-support norm",
      "author" : [ "Andreas Argyriou", "Rina Foygel", "Nathan Srebro" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Simultaneous analysis of lasso and dantzig selector",
      "author" : [ "Peter J Bickel", "Ya’acov Ritov", "Alexandre B Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "The Dantzig selector: Statistical estimation when p is much larger than n",
      "author" : [ "Emmanuel Candes", "Terence Tao" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "The convex geometry of linear inverse problems",
      "author" : [ "Venkat Chandrasekaran", "Benjamin Recht", "Pablo A Parrilo", "Alan S Willsky" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "A generalized dantzig selector with shrinkage",
      "author" : [ "Gareth M. James", "Peter Radchenko" ],
      "venue" : "tuning. Biometrika,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Dasso: connections between the dantzig selector and lasso",
      "author" : [ "Gareth M. James", "Peter Radchenko", "Jinchi Lv" ],
      "venue" : "Journal of the Royal Statistical Society Series B,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Probability in Banach Spaces: isoperimetry and processes, volume 23",
      "author" : [ "Michel Ledoux", "Michel Talagrand" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1991
    }, {
      "title" : "The group dantzig selector",
      "author" : [ "Han Liu", "Jian Zhang", "Xiaoye Jiang", "Jun Liu" ],
      "venue" : "JMLR Proceedings,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "An alternating direction method for finding dantzig selectors",
      "author" : [ "Zhaosong Lu", "Ting Kei Pong", "Yong Zhang" ],
      "venue" : "Computational Statistics & Data Analysis,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "New Perspectives on k-Support and Cluster Norms",
      "author" : [ "A.M. McDonald", "M. Pontil", "D. Stamos" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers",
      "author" : [ "Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Universal measurement bounds for structured sparse signal recovery",
      "author" : [ "Nikhil S Rao", "Ben Recht", "Robert D Nowak" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "On sparse reconstruction from fourier and gaussian measurements",
      "author" : [ "Mark Rudelson", "Roman Vershynin" ],
      "venue" : "Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Galois Theory, Third Edition",
      "author" : [ "I. Stewart" ],
      "venue" : "Chapman Hall/CRC Mathematics Series. Taylor & Francis,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2003
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1996
    }, {
      "title" : "Bregman Alternating Direction Method of Multipliers",
      "author" : [ "H. Wang", "A. Banerjee" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "The linearized alternating direction method of multipliers for dantzig selector",
      "author" : [ "Xiangfeng Wang", "Xiaoming Yuan" ],
      "venue" : "SIAM J. Scientific Computing,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "On model selection consistency of lasso",
      "author" : [ "Peng Zhao", "Bin Yu" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "1 Introduction The Dantzig Selector (DS) [2, 3] provides an alternative to regularized regression approaches such as Lasso [16, 19] for sparse estimation.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "1 Introduction The Dantzig Selector (DS) [2, 3] provides an alternative to regularized regression approaches such as Lasso [16, 19] for sparse estimation.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "1 Introduction The Dantzig Selector (DS) [2, 3] provides an alternative to regularized regression approaches such as Lasso [16, 19] for sparse estimation.",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 17,
      "context" : "1 Introduction The Dantzig Selector (DS) [2, 3] provides an alternative to regularized regression approaches such as Lasso [16, 19] for sparse estimation.",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "While DS does not consider a regularized maximum likelihood approach, [2] has established clear similarities between the estimates from DS and Lasso.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 10,
      "context" : "While norm regularized regression approaches have been generalized to more general norms, such as decomposable norms [11], the literature on DS has primarily focused on the sparse L1 norm case, with a few notable exceptions which have considered extensions to sparse group-structured norms [8].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : "While norm regularized regression approaches have been generalized to more general norms, such as decomposable norms [11], the literature on DS has primarily focused on the sparse L1 norm case, with a few notable exceptions which have considered extensions to sparse group-structured norms [8].",
      "startOffset" : 290,
      "endOffset" : 293
    }, {
      "referenceID" : 2,
      "context" : "If R(·) is the L1 norm, (1) reduces to standard DS [3].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "It is instructive to contrast GDS with the recently proposed atomic norm based estimation framework [4] which, unlike GDS, considers constraints based on the L2 norm of the error ‖y −Xθ‖2, and focuses only on atomic norms.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "For the L1-norm Dantzig selector, [3] proposed a primal-dual interior point method since the optimization is a linear program.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "DASSO and its generalization proposed in [6, 5] focused on homotopy methods, which provide a piecewise linear solution path through a sequential simplex-like algorithm.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 4,
      "context" : "DASSO and its generalization proposed in [6, 5] focused on homotopy methods, which provide a piecewise linear solution path through a sequential simplex-like algorithm.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "In recent work, the Alternating Direction Method of Multipliers (ADMM) has been applied to the L1 Dantzig selection problem [9, 18], and the linearized version in [18] proved to be efficient.",
      "startOffset" : 124,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "In recent work, the Alternating Direction Method of Multipliers (ADMM) has been applied to the L1 Dantzig selection problem [9, 18], and the linearized version in [18] proved to be efficient.",
      "startOffset" : 124,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "In recent work, the Alternating Direction Method of Multipliers (ADMM) has been applied to the L1 Dantzig selection problem [9, 18], and the linearized version in [18] proved to be efficient.",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 15,
      "context" : "Motivated by such results for DS, we propose a general inexact ADMM [17] framework for GDS where the primal update steps, interestingly, turn out respectively to be proximal updates involving R(θ) and its convex conjugate, the indicator of R∗(x) ≤ λ.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "Interestingly, the bound depends on the Gaussian width of the unit norm ball ofR(·) as well as the Gaussian width of suitable set where the estimation error belongs [4, 13].",
      "startOffset" : 165,
      "endOffset" : 172
    }, {
      "referenceID" : 11,
      "context" : "Interestingly, the bound depends on the Gaussian width of the unit norm ball ofR(·) as well as the Gaussian width of suitable set where the estimation error belongs [4, 13].",
      "startOffset" : 165,
      "endOffset" : 172
    }, {
      "referenceID" : 0,
      "context" : "As a non-trivial example of the GDS framework, we consider estimation using the recently proposed k-support norm [1, 10].",
      "startOffset" : 113,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "As a non-trivial example of the GDS framework, we consider estimation using the recently proposed k-support norm [1, 10].",
      "startOffset" : 113,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "Note that existing work [1, 10] on k-support norm has focused on the proximal operator for the square of the k-support norm, which is not directly applicable in our setting.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "Note that existing work [1, 10] on k-support norm has focused on the proximal operator for the square of the k-support norm, which is not directly applicable in our setting.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "Inspired by [18], we consider a simpler subproblem for the θ-update which minimizes L̃R(θ,v, z) = R(θ) + 〈z,Aθ + v − u〉+ ρ 2 (∥∥Aθk + v − u∥∥2 2 +",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "Remark on convergence: Note that Algorithm 1 is a special case of inexact Bregman ADMM proposed in [17], which matches the case of linearizing quadratic penalty term by using Bφθ(θ,θk) = 1 2‖θ − θk‖2 as Bregman divergence.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 15,
      "context" : "In order to converge, the algorithm requires μ2 to be larger than the spectral radius of ATA, and the convergence rate is O(1/T ) according to Theorem 2 in [17].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 12,
      "context" : "For any set Ω ⊆ Rp, we would measure the size of this set using its Gaussian width [14, 4], which is defined as ω(Ω) = Eg [supz∈Ω〈g, z〉] , where g is a vector of i.",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "For any set Ω ⊆ Rp, we would measure the size of this set using its Gaussian width [14, 4], which is defined as ω(Ω) = Eg [supz∈Ω〈g, z〉] , where g is a vector of i.",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 16,
      "context" : "Similar updates were used in [18] for L1-norm Dantzig selector.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "It was shown in [4] that the Gaussian width of the set (TL1(θ) ∩ Sp−1) is upper bounded as ω(TL1(θ)∩ Sp−1)2 ≤ 2s log (p s ) + 54s.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "standard Gaussian entries [3].",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 10,
      "context" : "Further, [11] has shown that ΨR = √ s.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 1,
      "context" : "with high probability, which agrees with known results for DS [2, 3].",
      "startOffset" : 62,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "with high probability, which agrees with known results for DS [2, 3].",
      "startOffset" : 62,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "In previous work [1, 10], the k-support norm is defined as",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 9,
      "context" : "In previous work [1, 10], the k-support norm is defined as",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "It was shown in [1] to behave similarly as the elastic net in the sense that the unit norm ball of the k-support norm is within a constant factor of √ 2 of the unit elastic net ball.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "Existing methods [1, 10] are inapplicable to our scenario since they compute the proximal operator for squared k-support norm, from which ICλ (·) cannot be directly obtained.",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 9,
      "context" : "Existing methods [1, 10] are inapplicable to our scenario since they compute the proximal operator for squared k-support norm, from which ICλ (·) cannot be directly obtained.",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "Remark: The nonlinear equation (24) is quartic, for which we can use general formula to get all the roots [15].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "Compared with previous proximal operators for squared k-support norm, this complexity is better than that in [1], and roughly the same as the most recent one in [10].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "Compared with previous proximal operators for squared k-support norm, this complexity is better than that in [1], and roughly the same as the most recent one in [10].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 11,
      "context" : "We prove these two bounds using the analysis technique for group lasso with overlaps developed in [13].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : "4 Experimental Results On optimization side, our ADMM framework is concentrated on its generality, and its efficiency has been shown in [18] for the special case of L1 norm.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "1 Efficiency of Proximal Operator We tested four proximal operators related to k-support norm, which are our normal ICλ (·) and its accelerated version, prox 1 2β (‖·‖ k )2 (·) in [1], and proxλ 2 ‖·‖Θ (·) in [10].",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 9,
      "context" : "1 Efficiency of Proximal Operator We tested four proximal operators related to k-support norm, which are our normal ICλ (·) and its accelerated version, prox 1 2β (‖·‖ k )2 (·) in [1], and proxλ 2 ‖·‖Θ (·) in [10].",
      "startOffset" : 209,
      "endOffset" : 213
    } ],
    "year" : 2015,
    "abstractText" : "We propose a Generalized Dantzig Selector (GDS) for linear models, in which any norm encoding the parameter structure can be leveraged for estimation. We investigate both computational and statistical aspects of the GDS. Based on conjugate proximal operator, a flexible inexact ADMM framework is designed for solving GDS, and non-asymptotic high-probability bounds are established on the estimation error, which rely on Gaussian width of unit norm ball and suitable set encompassing estimation error. Further, we consider a non-trivial example of the GDS using k-support norm. We derive an efficient method to compute the proximal operator for k-support norm since existing methods are inapplicable in this setting. For statistical analysis, we provide upper bounds for the Gaussian widths needed in the GDS analysis, yielding the first statistical recovery guarantee for estimation with the k-support norm. The experimental results confirm our theoretical analysis.",
    "creator" : "LaTeX with hyperref package"
  }
}