{
  "name" : "1702.06269.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Memory and Communication Efficient Distributed Stochastic Optimization with Minibatch-Prox",
    "authors" : [ "Jialei Wang", "Weiran Wang", "N. Srebro", "WANG WANG SREBRO" ],
    "emails" : [ "JIALEI@UCHICAGO.EDU", "WEIRANWANG@TTIC.EDU", "NATI@TTIC.EDU" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n06 26\n9v 2\n[ cs\n.L G\n] 9\noptimal and achieves near-linear speedups (up to logarithmic factors). Our approach allows a communication-memory tradeoff, with either logarithmic communication but linear memory, or polynomial communication and a corresponding polynomial reduction in required memory. This communication-memory tradeoff is achieved throughminibatch-prox iterations (minibatch passiveaggressive updates), where a subproblem on a minibatch is solved at each iteration. We provide a novel analysis for such a minibatch-prox procedure which achieves the statistical optimal rate regardless of minibatch size and smoothness, thus significantly improving on prior work."
    }, {
      "heading" : "1. Introduction",
      "text" : "Consider the stochastic convex optimization (generalized learning) problem (Nemirovskii and Yudin, 1983; Vapnik, 1995; Shalev-Shwartz et al., 2009):\nmin w∈Ω φ(w) := Eξ∼D [ℓ(w, ξ)] (1)\nwhere our goal is to learn a predictor w from the convex domain Ω given the convex instantaneous (loss) function ℓ(w, ξ) and i.i.d. samples ξ1, ξ2, . . . from some unknown data distribution D. When optimizing on a single machine, stochastic approximation methods such as stochastic gradient descent (SGD) or more generally stochastic mirror descent, are ideally suited for the problem as they typically have optimal sample complexity requirements, and run in linear time in the number of samples, and thus also have optimal runtime. Focusing on an ℓ2 bounded domain with B = sup\nw∈Ω ‖w‖ and L-Lipschitz loss, the min-max optimal sample complexity is n(ε) = O(L2B2/ε2), and this is achieved by SGD using O(n(ǫ)) vector operations. Furthermore, if examples are obtained one at a time (in a streaming setting or through access to a “button” generating examples), we only need to store O(1) vectors in memory. The situation is more complex in the distributed setting where no single method is known that is optimal with respect to sample complexity, runtime, memory and communication. Specifically, consider m machines where each machine i = 1, ...,m receives samples ξi1, ξi2, ... drawn from the\n∗ Equal contributions.\nc© 2017 J. Wang, W. Wang & N. Srebro.\nsame distribution D. This can equivalently be thought of as randomly distributing samples across m servers. We also assume the objective is β-smooth, taking L, β = O(1) in our presentation of results. The goal is to find a predictor ŵ ∈ Ω satisfying E [φ(ŵ)−minw∈Ω φ(w)] ≤ ε using the smallest possible number of samples per machine, the minimal elapsed runtime, and the smallest amount of communication, and also minimal memory on each machine (again, when examples are received or generated one at a time). Ideally, we could hope for a method with linear speedup, i.e. O(n(ǫ)/m) runtime, using the statistically optimal number of samples O(n(ǫ)) and constant or near-constant communication and memory. Throughout we measure runtime in terms of vector operations, memory in terms of number of vectors that need to be stored on each machine and communication in terms of number of vectors sent per machine1. These resource requirements are summarized in Table 1.\nOne simple approach for distributed stochastic optimization is minibatch SGD (Cotter et al., 2011; Dekel et al., 2012), where in each update we use a gradient estimate based on mb examples: b examples from each of the m machines. Distributed minibatch SGD attains optimal statistical performance with O (n(ε)/m) runtime, as long as the minibatch size is not too large: Dekel et al. (2012) showed that the minibatch size can be as large as bm = O( √ n(ε)), and Cotter et al. (2011) showed that with acceleration this can be increased to bm = O(n(ε)3/4). Using this maximal minibatch size for accelerated minibatch SGD thus yields a statistically optimal method with linear speedup in runtime, O(1) memory usage, and O(n(ε)1/4) rounds of communication–see Table 1. This is the most communication-efficient method with true linear speedup we are aware of.\nAn alternative approach is to use distributed optimization to optimize the regularized empirical\nobjective:\nmin w\nφS(w) + ν\n2 ‖w‖2 , (2)\nwhere φS is the empirical objective on n(ǫ) i.i.d. samples, distributed across the machines and ν = O(L/(B √ n(ε))). A naive approach here is to use accelerate gradient descent, distributing the gradient computations, but this, as well as approaches based on ADMM (Boyd et al., 2011), are dominated by minibatch SGD (Shamir and Srebro 2014 and see also Table 1). Better alternatives take advantage of the stochastic nature of the problem: DANE (Shamir et al., 2014) requires only O(B2m) rounds of communication for squared loss problems, while DiSCO (Zhang and Lin, 2015) and AIDE (Reddi et al., 2016)) reduce this further to O(B1/2m1/4) rounds of communication. However, these communication-efficient methods usually require expensive computation on each local machine, solving an optimization problem on all local data at each iteration. Even if this can be done in near-linear time, it is still difficult to obtain computational speedup compared with single machine solution, and certainly not linear speedups—see Table 1. Furthermore, since each round of these methods involves optimization over a fixed training set, this training set must be stored thus requiring n(ε)/m memory per machine. Designing stochastic distributed optimization problems with linear, or near-linear, speedups, and low communication and memory requirements is thus still an open problem. We make progress in this paper analyzing and presenting methods with near-linear speedups and better communication and memory requirements. As with the analysis of DANE, DiSCO and AIDE, our analysis is rigorous only for least squared problems, and so all results should be taken in that context (the methods themselves are applicable to any distributed stochastic convex optimization problem).\n1. In all methods involved, communication is used to average vectors across machines and make the result known to\none or all machines. We are actually counting the number of such operations.\nC o m m u n ic at io n\nMemory\nAcc. Mini. SGD DSVRG MP-DSVRG"
    }, {
      "heading" : "Our contributions",
      "text" : "batch proximal updates may be of independent interest and useful in other contexts and as a basis for other methods.\nNotations We denote by w∗ = argminw∈Ω φ(w) the optimal solution to (1). Throughout the paper, we assume the instantaneous function ℓ(w, ξ) is L-Lipschitz and λ-strongly convex inw for some λ ≥ 0 on the domain Ω:\n∣∣ℓ(w, ξ)− ℓ(w′, ξ) ∣∣ ≤ L ∥∥w −w′ ∥∥ ,\nℓ(w, ξ)− ℓ(w′, ξ) ≥ 〈 ∇ℓ(w′, ξ), w −w′ 〉 + λ\n2\n∥∥w −w′ ∥∥2 , ∀w,w′ ∈ Ω.\nSometimes we also assume ℓ(w, ξ) is β-smooth inw:\nℓ(w, ξ)− ℓ(w′, ξ) ≤ 〈 ∇ℓ(w′, ξ), w −w′ 〉 + β\n2\n∥∥w −w′ ∥∥2 , ∀w,w′ ∈ Ω.\nFor distributed stochastic optimization, our analysis focuses on the least squares loss ℓ(w, ξ) = 1 2(w ⊤x− y)2 where ξ = (x, y)."
    }, {
      "heading" : "2. Distributed SVRG for stochastic convex optimization",
      "text" : "Recently, Lee et al. (2015) suggested using fast randomized optimization algorithms for finite-sums, and in particular the SVRG algorithm, as a distributed optimization approach for (2). The authors noted that, for SVRG, when the the sample size n(ε) dominates the problem’s condition number β/ν where β is the smoothness parameter of ℓ(w, ξ), the time complexity is dominated by computing the batch gradients. This operation can be trivially parallelized. The stochastic updates, on the other hand, can be implemented on a single machine while the other machines wait, with the only caveat being that only sampling-without-replacement can be implemented this way. The use of without-replacement sampling was theoretically justified in a recent analysis by Shamir (2016).\nIn the distributed stochastic convex optimization setting considered here, DSVRG in fact achieves\nlinear speedup in certain regime as follows. In each iteration of the algorithm, each machine first computes its local gradient and average them with one communication round to obtain the global batch gradient, and then a single machine performs the SVRG stochastic updates by processing its local data once (sampling the n(ε)/m examples without replacement). By the linear convergence of SVRG, as long as the number of stochastic updates n(ε)/m is larger than β/ν = O(βB √ n(ε)/L), the algorithm converges to O(ǫ)-suboptimality (in both the empirical and stochastic objective) in O(log 1/ε) = O (log n(ε)) iterations; and this condition is satisfied2 for n(ε) & m2.\nClearly, in the above regime, each iteration of DSVRG uses two rounds of communications and the total communication complexity is O (n(ε)). On the other hand, the computation for each machine is compute the local gradient (in time O(n(ε)/m)) in each iteration, resulting in a total time complexity of O(n(ε) log n(ε)/m). This explains the DSVRG entry in Table 1.\nBeing communication- and computation-efficient, DSVRG requires each machine to store a portion of the sample set for ERM to make multiple passes over them, and is therefore not memoryefficient. In fact, this disadvantage is shared by previously known communication-efficient distributed optimization algorithms, including DANE, DiSCO, and AIDE. In order to develop amemoryand communication-efficient algorithm for distributed stochastic optimization, we need to bypass the ERM setting and this is enabled by the following minibatch-prox algorithm."
    }, {
      "heading" : "3. The minibatch-prox algorithm for stochastic optimization",
      "text" : "In this section, we describe and analyze the minibatch-prox algorithm for stochastic optimization, which allows us to use arbitrarily large minibatch size without slowing down the convergence rate. We first present the basic version where each proximal objective is solved exactly for each minibatch, which achieves the optimal convergence rate. Then, we show that if each minibatch objective is solved accurately enough, the algorithm still converges at the optimal rate, opening the opportunity for efficient implementations."
    }, {
      "heading" : "3.1. Exact minibatch-prox",
      "text" : "The “exact” minibatch-prox is defined by the following iterates: for t = 1, . . . ,\nwt = argmin w∈Ω ft(w),\nwhere ft(w) := φIt(w) + γt 2 ‖w −wt−1‖2 = 1 b\n∑ ξ∈It ℓ(w, ξ) + γt 2 ‖w −wt−1‖2 , (3)\nγt > 0 is the (inverse) stepsize parameter at time t, and It is a set of a b samples from the unknown distribution D. To understand the updates in (3), we first observe by the first order optimality condition for ft(w) that\n∇φIt(wt) + γt(wt −wt−1) ∈ −NΩ(wt), (4)\n2. If n(ε) & m2 does not hold, we can use a “hot-potato” style algorithm where we process all data once on machine i and pass the predictor to machine i+1 until we obtain sufficiently many stochastic updates. But then the computation efficiency deteriorates and we no longer have linear speedup in runtime.\nwhere∇φIt(wt) is some subgradient of φIt(w) atwt, andNΩ(wt) = {y| 〈w −wt, y〉 ≤ 0, ∀w ∈ Ω} is the normal cone of Ω at wt. Equivalently, the above condition implies\nwt = PΩ ( wt−1 − 1\nγt ∇φIt(wt)\n) , (5)\nwhere PΩ(w) denotes the projection ofw onto Ω. The update rule (5) resembles that of the standard minibatch gradient descent, except the gradient is evaluated at the “future” iterate.\nProximal steps, of the form (3) or equivalently (5), are trickier to implement compared to (stochastic) gradient steps, as they involve optimization of a subproblem, instead of merely computing and adding gradients. Nevertheless, they have been suggested, used and studied in several contexts. Crammer et al. (2006) proposed the “passive aggressive” update rule, where a margin-based loss from a single example with a quadratic penalty is minimized—this corresponds to (3) with a “batch size” of one. More general loss functions, still for “batch sizes” of one, were also analyzed in the online learning setting (Cheng et al., 2006; Kulis and Bartlett, 2010). For finite-sum objectives, methods based on incremental/stochastic proximal updates were studied by Bertsekas (2011, 2015); Defazio (2016). Needell and Tropp (2014) analyzed a randomized block Kaczmarz method in the context of solving linear systems, which also minimizes the empirical loss on a randomly sampled minibatch. To the best of our knowledge, no prior work has analyzed the general minibatch variant of proximal updates for stochastic optimization except Li et al. (2014). However, the analysis of Li et al. (2014) assumes a stringent condition which is hard to verify (and is often violated) in practice, which we will discuss in detail in this section.\nThe following lemma provides the basic property of the update at each iteration.\nLemma 1 For any w ∈ Ω, we have\nλ+ γt γt ‖wt −w‖2 ≤ ‖wt−1 −w‖2 − ‖wt−1 −wt‖2 − 2 γt (φIt(wt)− φIt(w)) . (6)\nTo derive the convergence guarantee, we need to relate φIt(wt) to φ(w). The analysis of Li et al. (2014) for minibatch-prox made the assumption that for all t ≥ 1:\nEIt [Dφ(wt;wt−1)] ≤ EIt [ DφIt (wt;wt−1) ] + γt 2 ‖wt −wt−1‖2 , (7)\nwhereDf (w,w ′) = f(w)− f(w′)−〈∇f(w′), w −w′〉 denotes the Bregman divergence defined by the potential function f . This condition is hard to verify, and may constrain the stepsize to be very small. For example, as the authors argued, if ℓ(w, ξ) is β-smooth with respect to w, we have\nDφ(wt;wt−1) ≤ β\n2 ‖wt −wt−1‖2 ,\nand combined with the fact that DφIt (wt;wt−1) ≥ 0, one can guarantee (7) by setting γt ≥ β. However, to obtain the optimal convergence rate, Li et al. (2014) needed to set γt = O( √ T/b) which would imply b = O(T ) in order to have γt ≥ β. In view of this implicit constraint that the minibatch size b can not be too large, the analysis of Li et al. (2014) does not really show advantage of minibatch-prox over minibatch SGD, whose optimal minibatch size is precisely b = O(T ).\nOur analysis is free of any additional assumptions. The key observation is that, when b is large, we expect φIt(w) to be close to φ(w). Define the stochastic objective\nFt(w) := EIt [ft(w)] = φ(w) + γt 2 ‖w −wt−1‖2 . (8)\nThen wt is the “empirical risk minimizer” of Ft(w) as it solves the empirical version ft(w) with b samples. Using a stability argument (Shalev-Shwartz et al., 2009), we can establish the “generalization” performance for the (inexact) minimizer of the minibatch objective.\nLemma 2 For the minibatch-prox algorithm,we have\n|EIt [φ(wt)− φIt(wt)]| ≤ 4L2\n(λ+ γt)b .\nMoreover, if a possibly randomized algorithm Aminimizes ft(w) up to an error of ηt, i.e.,A returns an approximate solution w̃t such that EA [ft(w̃t)− ft(wt)] ≤ ηt, we have\n|EIt,A [φ(w̃t)− φIt(wt)]| ≤ 4L2\n(λ+ γt)b + √ 2L2ηt λ+ γt .\nCombining Lemma 1 and Lemma 2, we obtain the following key lemma regarding the progress on the stochastic objective at each iteration of minibatch-prox.\nLemma 3 For iteration t of exact minibatch-prox, we have for any w ∈ Ω that\nλ+ γt γt EIt ‖wt −w‖2 ≤ ‖wt−1 −w‖2 − 2 γt EIt [φ(wt)− φ(w)] +\n8L2\nγt(λ+ γt)b . (9)\nWe are now ready to bound the overall convergence rates of minibatch-prox.\nTheorem 4 (Convergence of exact minibatch-prox — weakly convex ℓ(w, ξ)) For L-Lipschitz instantaneous function ℓ(w, ξ), set γ = √\n8T b · L‖w0−w∗‖ for t = 1, . . . , T in minibatch-prox. Then\nfor ŵT = 1 T ∑T t=1 wt, we have\nE [φ(ŵT )− φ(w∗)] ≤ √ 8L√ bT ‖w0 −w∗‖ .\nTheorem 5 (Convergence of exact minibatch-prox — strongly convex ℓ(w, ξ)) ForL-Lipschitz and λ-strongly convex instantaneous function ℓ(w, ξ), set γt = λ(t−1)\n2 for t = 1, . . . , T in minibatch-\nprox. Then for ŵT = 2\nT (T+1) ∑T t=1 twt, we have\nE [φ(ŵT )− φ(w∗)] ≤ 16L2\nλb(T + 1) ."
    }, {
      "heading" : "3.2. Inexact minibatch-prox",
      "text" : "We now study the case where instead of solving the subproblems ft(w) exactly, we only solve it approximately to sufficient accuracy. The “inexact” minibatch-prox uses a possibly randomized algorithm A for approximately solving one subproblem on a minibatch in each iteration, and generates the following iterates: for t = 1, . . . ,\nw̃t ≈ w̄t := argmin w∈Ω f̃t(w) where f̃t(w) := φIt(w) + γt 2 ‖w − w̃t−1‖2 , (10)\nand EA [ f̃t(w̃t)− f̃t(w̄t) ] ≤ ηt.\nAnalogous to Lemma 3, we can derive the following lemma using stability of inexact minimizers.\nLemma 6 Fix any w ∈ Ω. For iteration t of inexact minibatch-prox, we have\nEIt,A [φ(w̃t)− φ(w)] ≤ γt 2 EIt,A ‖w̃t−1 −w‖2 − λ+ γt 2 EIt,A ‖w̃t −w‖2 + 4L2\n(λ+ γt)b\n+ √ 2L2ηt λ+ γt + √ 2(λ+ γt)ηt · √ EIt,A ‖w̃t −w‖2. (11)\nNote that when ηt = 0, the above guarantee reduces to that of exact minibatch-prox. We now show that when the minibatch subproblems are solved sufficiently accurately, we still\nobtain the O(1/ √ bT ) rate for weakly-convex loss and O(1/(λbT )) rate for strongly-convex loss.\nTheorem 7 (Convergence of inexact minibatch-prox — weakly convex ℓ(w, ξ)) ForL-Lipschitz instantaneous function ℓ(w, ξ), set γt = γ = √\n8T b · L‖w0−w∗‖ for all t ≥ 1 in inexact minibatch-\nprox. Assume that for all t ≥ 1, the error in minimizing f̃t(w) satisfies for some δ > 0 that\nEA [ f̃t(w̃t)−min\nw\nf̃t(w) ] ≤ min ( c1 ( T\nb\n) 1 2\n, c2\n( T\nb\n) 3 2 ) · L ‖w̃0 −w∗‖\nt2+2δ .\nThen for ŵT = 1 T ∑T t=1 w̃t, we have E [φ(ŵT )− φ(w∗)] ≤ c3L‖w0−w∗‖√ bT , where c3 only depends on c1, c2 and δ. For example, by setting c1 = 10 −4, c2 = 10−4, δ = 1/2, we have\nE [φ(ŵT )− φ(w∗)] ≤ √ 10L ‖w0 −w∗‖√\nbT .\nTheorem 8 (Convergence of inexact minibatch-prox — strongly convex ℓ(w, ξ)) ForL-Lipschitz and λ-strongly convex instantaneous function ℓ(w, ξ), set γt = λ(t−1)\n2 for t = 1, . . . in inexact\nminibatch-prox. Assume that for all t ≥ 1, the error in minimizing f̃t(w) satisfies for some δ > 0 that\nEA [ f̃t(w̃t)−min\nw\nf̃t(w) ] ≤ min ( c1 ( T\nb\n) , c2 ( T\nb\n)2) · L 2\nt3+2δλ .\nThen for ŵT = 2\nT (T+1) ∑T t=1 tw̃t, we have E [φ(ŵT )− φ(w∗)] ≤ c3L 2\nλbT , where c3 only depends on c1, c2 and δ.\nRemark 9 The final inequalities in Theorem 4 and 7 actually apply more generally to all predictors in the domain. That is, our proofs still hold withw∗ replaced by any w ∈ Ω:\nE [φ(ŵT )− φ(w)] ≤ O ( L ‖w0 −w‖√\nbT\n) , w ∈ Ω.\nThis allows us to compete with any predictor in the domain (other than the minimizer). For example, in order to compete on φ(w) with the set of predictors with small norm {w : ‖w‖ ≤ B}, we can set the domain Ω = Rd and initialize with w0 = 0. In view of the above inequality, we still obtain the\noptimal rateO (\nLB√ bT\n) from minibatch-prox by solving simpler, unconstrained subproblems (though\nwe might have ‖ŵT ‖ > B)."
    }, {
      "heading" : "4. Communication-efficient distributed minibatch-prox with SVRG",
      "text" : "We now apply the theoretical results of minibatch-prox to the distributed stochastic learning setting, and propose a novel algorithm that is both communication and computation efficient, and being able to explore trade-offs between memory and communication efficiency.\nSuppose we have m machines in a distributed environment. For each outer loop of our algorithm, each machine i draws a minibatch I (i) t of b samples independently from other machines, and denote It = ∪mi=1I (i) t which contains bm samples. To apply the minibatch-prox algorithm from the previous section, we need to find an approximate solution to the following problem:\nmin w\nf̃t(w) := φIt(w) + γ\n2 ‖w −wt−1‖2 . (12)\nSince the objective (12) involves functions from different machines, we use distributed optimization algorithms for solving it. In Li et al. (2014), the authors proposed a simple algorithm EMSO to approximately solve (12), where each machine first solve its own local objective, i.e.,\nw (i) t = argmin\nw\nφ I (i) t\n+ γ\n2 ‖w −wt−1‖2 , (13)\nand then all machines average their local solutions via one round of communication: wt = 1 m ∑m i=1w (i) t .\nWe note that this can be considered as the “one-shot-averaging” approach (Zhang et al., 2012) for solving (12). Although this approach was shown to work well empirically, no convergence guarantee for the original stochastic objective (1) was provided by Li et al. (2014). Here we instead use the distributed SVRG (DSVRG) algorithm (Lee et al., 2015; Shamir, 2016) to approximately solve (12), as DSVRG enjoys excellent communication and computation cost when the problem is well conditioned (cf. Table 1).3\nWe detail our algorithm, named MP-DSVRG (minibatch-prox with DSVRG), in Algorithm 1. The algorithm consists of two nested loops, where t, k are iteration counters for minibatch-prox (the outer for-loop), and DSVRG (the inner for-loop) respectively. In each outer loop, each machine draws a minibatch I (i) t to form the objective (12), which will be solved approximately by the inner loops. Moreover, each machine splits its local dataset into pi batches: I (i) = ∪pij=1B (i) j . In\n3. It is also possible to equip minibatch-prox with other communication-efficient distributed optimization algorithms,\nfor example in Appendix D, we present a minibatch-prox DANE (MP-DANE) algorithm which uses the accelerated DANE method for solving (12).\nAlgorithm 1Minibatch-prox with DSVRG for distributed stochastic convex optimization.\nInitialize w0 = 0. for t = 1, 2, . . . , T do\n% Outer loop performs minibatch-prox. Each machine i draws a minibatch I (i) t of b samples from the underlying data distribution, and split I (i) t to pi batches of size b/pi: B (i) 1 , B (i) 2 , ..., B (i) pi Initialize z0 ← wt−1, x0 ← wt−1, j ← 1, s ← 1 for k = 1, 2, . . . ,K do\n1. All machines perform one round of communication to compute the average gradient:\n∇φIt(zk−1) ← 1\nm\nm∑\ni=1\n∇φ I (i) t (zk−1)\n2. Machine j performs stochastic updates by going through B (j) s once without replacement:\nxr ← xr−1 − η (∇ℓ(xr−1, ξl)−∇ℓ(zk−1, ξl) +∇φIt(zk−1) + γ(xr−1 −wt−1))\nfor ξl ∈ B(j)s . 3. Machine j update zk:\nzk ← 1\n|B(j)s |\n|B(j)s |∑\nr=0\nxr,\nand broadcast zk to other machines. 4. Update indices: s ← s+ 1, if s > pj then\ns ← 1, j ← j + 1. end if\nend for\nUpdate wt ← zK . end for\nOutput: wT is the approximate solution.\neach inner loop, all machines communicate to calculate the global gradient (averaged local gradients) of (12), and then one of the machines j picks a local batch B (j) s to perform the stochastic updates, where the local batch contains enough samples such that one pass of stochastic updates on B (j) s decrease the objective quickly. We perform two rounds of communication in each inner loop, one for computing the global gradient, and one for broadcasting the new predictor obtained by a machine j. As we will show in the next section, by carefully choosing the parameters, we will obtain a convergent algorithm for distributed stochastic convex optimization with better efficiency guarantees than previous methods.\nWe now present detailed analysis for the computation/communication complexity of Algorithm 1 for stochastic quadratic problems, and compare it with related methods in the literature. Throughout this section, we have ℓ(w, ξ) = 12(w ⊤x − y)2 where ξ = (x, y). We assume that\nℓ(w, ξ) is β-smooth and L-Lipschitz in w,4 and we would like to learn a predictor that is competitive to all predictors with norm at most B. Note that each ℓ(w, ξ) is only weakly convex."
    }, {
      "heading" : "4.1. Efficiency of MP-DSVRG",
      "text" : "For the distributed stochastic convex optimization problems, we are concerned with efficiency in terms of sample, communication, computation and memory. Recall that for convex L-Lipshitz, Bbounded problems, to learn a predictor ŵ with ε-generalization error, i.e., E [φ(ŵ)− φ(w∗)] ≤ ε, we require the sample size to be at least n(ε) = O(L2B2/ε2). This sample complexity matches the worst case lower bound, and can be achieved by vanilla SGD.\nThe theorem below shows that with careful choices of parameters in the outer and inner loops, MP-DSVRG achieves both communication and computation efficiency with the optimal sample complexity.\nTheorem 10 (Efficiency of MP-DSVRG) Set the parameters in Algorithm 1 as follows:\n(outer loop) T = n(ε)\nbm , γ =\n√ 8n(ε)L\nbmB , pi = O\n(√ n(ε)L\nβmB\n)\n(inner loop) K = O (log n(ε)) .\nThen we have E [ φ (\n1 T ∑T t=1 wt ) − φ(w∗) ] ≤ √ 40BL√ n(ε) = O (ε) .\nMoreover, Algorithm 1 can be implemented with O ( n(ε) bm log n(ε) ) rounds of communication,\nand each machine performs O ( n(ε) m log n(ε) ) vector operations in total.\nWe comment on the choice of parameters. For sample efficiency, we fix the sample size n(ε) and number of machines m, and so we can tradeoff the local minibatch size b and the total number of outer iterations T , maintaining bT = n(ε)m . For any b, the regularization parameters in the “large\nminibatch” problem is set to γ = √\n8T bm · LB =\n√ 8n(ε)L\nbmB according to Theorem 7. Moreover,\nwe choose the number of batches pi in each local machine in a way that performing one pass of stochastic updates over a single batch by without-replacement sampling is sufficient to reduce the objective by a constant factor."
    }, {
      "heading" : "5. Discussion and conclusion",
      "text" : "In this paper, we made progress toward linear speedup, communication and memory efficient methods for distributed stochastic optimization, although we still do not have an algorithm that obtains the “ideal” distributed stochastic optimization performance of linear speedup with constant or nearconstant communication and memory. There is also no single known algorithm that dominates all others, with different methods being preferable in terms of different resources. These tradeoffs, up to log-factors, are given in Table 1 and the memory, communication and runtime requirements are also schematically depicted in Figure 2. In the figure, the horizontal axis corresponds to the “minibatch” size, which can be controlled with accelerated minibatch SGD and MP-DSVRG, while other methods are batch methods which consider the entire data set.\n4. We can equivalently assume ‖x‖2 ≤ β and y is bounded.\nFrom Figure 2 we can see that DSVRG (equivalent to MP-DSVRG when b = n(ε)/m) dominates the other methods (up to log-factors) in terms of runtime and communication—it has smaller communication requirements than DiSCO/AIDE (and better than DANE) with nearly the same optimal runtime of accelerated minibatch SGD. But like other batch methods, it requires storing and re-accessing the entire data set. Accelerated minibatch SGD is the only one of these methods requiring only O(1) memory per machine, and it achieves true linear speedup, but due to the limit on the maximal allowed minibatch size, has relatively high communication cost. MPDSVRG allows bridging these two extremes of memory and communication, trading off between memory usage and communication. The trade-off is almost an extrapolation, except that in the lowmemory high-communication extreme, MP-DSVRG still requires (small) polynomial memory, not minibatch-SGD’s O(1) memory, and its runtime still involve a logarithmic factor while minibatchSGD achieves true linear speedup.\nInstead of using DSVRG to solve each proximal subproblem in a minibatch-prox iteration, we can also use any other distributed optimization approach. For example, we can consider using DiSCO or DANE. This is depicted as “MP-DANE” in Figure 2. Again, an external minibatch-prox loop allows trading off memory for communication. For small minibatch sizes, up to a critical value of bmp-dane = Θ(n(ε)/(m 2B2)), MP-DANE enjoys the same guarantees as MP-DSVRG. But for larger minibatch sizes, such an approach starts suffering from DANE/DiSCO’s inferior runtime and communication requirements compared to DSVRG.\nWe emphasize that the above discussion is based on guarantees established only for least square problems and ignores log-factors. We are unfortunately not aware of distributed stochastic optimization guarantees that improve over minibatch SGD (i.e., achieve even near-linear speedup with lower communication requirements) for general smooth objectives, or achieve true linear speedup (and improved communication guarantees) even for least-square problems."
    }, {
      "heading" : "Acknowledgement",
      "text" : "Research was partially supported by an Intel ICRI-CI award and NSF awards IIS 1302662 and BIGDATA 1546500. We would like to thank Ohad Shamir for discussions about Distributed SVRG and Tong Zhang for discussions about minibatch-prox."
    }, {
      "heading" : "Appendix A. Analysis of exact minibatch-prox",
      "text" : ""
    }, {
      "heading" : "A.1. Proof of Lemma 1",
      "text" : "Proof Observe that (4) implies γt(wt−1 −wt) is a subgradient at wt of the sum of φIt(w) and the indicator function of Ω (which has value 0 in Ω and∞ otherwise), and thus we have for anyw ∈ Ω that\nφIt(w)− φIt(wt) ≥ γt 〈wt−1 −wt, w −wt〉+ λ\n2 ‖w −wt‖2 . (14)\nFor anyw ∈ Ω, we can bound its distance to wt−1 as\n‖wt−1 −w‖2 = ‖wt−1 −wt +wt −w‖2\n= ‖wt−1 −wt‖2 + 2 〈wt−1 −wt, wt −w〉+ ‖wt −w‖2 ≥ ‖wt−1 −wt‖2 + 2\nγt (φIt(wt)− φIt(w)) +\nλ γt ‖w −wt‖2 + ‖wt −w‖2\n= λ+ γt γt ‖wt −w‖2 + 2 γt (φIt(wt)− φIt(w)) + ‖wt−1 −wt‖2\nwhere we have used (14) in the first inequality. Rearranging the terms yields the desired result."
    }, {
      "heading" : "A.2. Proof of Lemma 2",
      "text" : "The following lemma, which is essentially shown by Shalev-Shwartz et al. (2009, Theorem 6), characterizes the convergence of the empirical loss to the population counterpart for the (approximate) regularized empirical risk minimizer.\nLemma 11 Let the instantaneous function ℓ(w, ξ) be L-Lipschitz and λ-strongly convex in w. Consider the following regularized ERM problem with sample set Z = {ξ1, . . . , ξn}:\nŵ = argmin w∈Ω\nF̂ (w) where F̂ (w) := 1\nn\nn∑\ni=1\nℓ(w, ξi) + r(w),\nand the regularizer r(w) is γ-strongly convex. Denote by G(w) = Eξ [ℓ(w, ξ)] and Ĝ(w) = 1 n ∑n i=1 ℓ(w, ξi) the expected and the empirical losses respectively.\n1. For the regularized empirical risk minimizer ŵ, we have\n∣∣∣EZ [ G(ŵ)− Ĝ(ŵ) ]∣∣∣ ≤ 4L 2\n(λ+ γ)n .\n2. If for any given dataset Z , a possibly randomized algorithmAminimizes F̂ (w) up to an error of η, i.e., A returns an approximate solution w̃ such that EA [ F̂ (w̃)− F̂ (ŵ) ] ≤ η, we have\n∣∣∣EZ,A [ G(w̃)− Ĝ(ŵ) ]∣∣∣ ≤ 4L 2\n(λ+ γ)n +\n√ 2L2η\nλ+ γ .\nProof We prove the lemma by a stability argument.\nExact ERM Denote by Z(i) the sample set that is identical to Z except that the i-th sample ξi is replaced by another random sample ξ′i, by F̂ (i)(w) the empirical objective defined using Z(i), i.e.,\nF̂ (i)(w) := 1\nn\n  ∑\nj 6=i ℓ(w, ξi) + ℓ(w, ξ\n′ i)\n + r(w),\nand by ŵ(i) = argmin w∈Ω F̂ (i)(w) the empirical risk minimizer of F̂ (i)(w). By the definition of the empirical objectives, we have\nF̂ (ŵ(i))− F̂ (ŵ) = ℓ(ŵ (i), ξi)− ℓ(ŵ, ξi)\nn +\n∑ j 6=i ℓ(ŵ\n(i), ξi)− ℓ(ŵ, ξi) n + r(ŵ(i))− r(ŵ)\n= ℓ(ŵ(i), ξi)− ℓ(ŵ, ξi)\nn + ℓ(ŵ, ξ′i)− ℓ(ŵ(i), ξ′i) n + ( F̂ (i)(ŵ(i))− F̂ (i)(ŵ) )\n≤ ∣∣ℓ(ŵ(i), ξi)− ℓ(ŵ, ξi) ∣∣ n + ∣∣ℓ(ŵ, ξ′i)− ℓ(ŵ(i), ξ′i) ∣∣ n ≤ 2L n ∥∥∥ŵ(i) − ŵ ∥∥∥ (15)\nwhere we have used the fact that ŵ(i) is the minimizer of F̂ (i)(w) in the first inequality, and the L-Lipschitz continuity of ℓ(w, ξ) in the second inequality.\nOn the other hand, it follows from the (λ+ γ)-strong convexity of F̂ (w) that\nF̂ (ŵ(i))− F̂ (ŵ) ≥ (λ+ γ) 2\n∥∥∥ŵ(i) − ŵ ∥∥∥ 2 . (16)\nCombining (15) and (16) yields ∥∥ŵ(i) − ŵ ∥∥ ≤ 4L(λ+γ)n . Again, by the L-Lipschitz continuity of ℓ(w, ξ), we have that for any sample ξ that\n∣∣∣ℓ(ŵ, ξ)− ℓ(ŵ(i), ξ) ∣∣∣ ≤ L ∥∥∥ŵ(i) − ŵ ∥∥∥ ≤ 4L 2\n(λ+ γ)n . (17)\nSince Z and Z(i) are both i.i.d. sample sets, we have\nEZ [G(ŵ)] = EZ(i) [ G(ŵ(i)) ] = EZ(i)∪{ξi} [ ℓ(ŵ(i), ξi) ] .\nAs this holds for all i = 1, . . . , n, we can also write\nEZ [G(ŵ)] = 1\nn\nn∑\ni=1\nEZ(i)∪{ξi}\n[ ℓ(ŵ(i), ξi) ] . (18)\nOn the other hand, we have\nEZ\n[ Ĝ(ŵ) ] = EZ\n[ 1\nn\nn∑\ni=1\nℓ(ŵ, ξi)\n] = 1\nn\nn∑\ni=1\nEZ [ℓ(ŵ, ξi)] . (19)\nCombining (18) and (19) and using the stability (17), we obtain\nEZ\n[ G(ŵ)− Ĝ(ŵ) ] = 1\nn\nn∑\ni=1\nEZ∪{ξ′i} [ ℓ(ŵ(i), ξi)− ℓ(ŵ, ξi) ] ∈ [ − 4L 2 (λ+ γ)n ,\n4L2\n(λ+ γ)n\n] .\nInexact ERM For the approximate solution w̃, due to the (λ+ γ)-strong convexity of F̂ (w), we have\nEA ‖w̃ − ŵ‖2 ≤ 2\nλ+ γ EA\n[ F̂ (w̃)− F̂ (ŵ) ] ≤ 2η\nλ+ γ ,\nand thus EA ‖w̃ − ŵ‖ ≤ √ 2η λ+γ by the fact that Ex\n2 ≥ (Ex)2 for any random variable x. It then follows from the Lipschitz continuity of G(w) that\nEA |G(w̃)−G(ŵ)| ≤ L · EA ‖w̃ − ŵ‖ ≤ √ 2L2η\nλ+ γ .\nFinally, we have by the triangle inequality and the stability of exact ERM that\n∣∣∣EZ,A [ G(w̃)− Ĝ(ŵ) ]∣∣∣ ≤ EZ [EA |G(w̃)−G(ŵ)|] + ∣∣∣EZ [ G(ŵ)− Ĝ(ŵ) ]∣∣∣\n≤ √ 2L2η\nλ+ γ +\n4L2\n(λ+ γ)n .\nThen Lemma 2 follows from the fact that that our stochastic objective (8) is equipped with\nL-Lipschitz, λ-strongly convex loss φ(w) and γt-strongly convex regularizer γt 2 ‖w −wt−1‖ 2 ."
    }, {
      "heading" : "A.3. Proof of Lemma 3",
      "text" : "Proof We have by Lemma 2 that\n|EIt [φIt(wt)− φ(wt)]| ≤ 4L2\n(λ+ γt)b .\nTake expectation of (6) over the random sampling of It and we obtain\nλ+ γt γt EIt ‖wt −w‖2 ≤ ‖wt−1 −w‖2 − 2 γt (EIt [φIt(wt)]− φ(w))\n= ‖wt−1 −w‖2 − 2\nγt (EIt [φIt(wt)− φ(wt)] + EIt [φ(wt)− φ(w)])\n≤ ‖wt−1 −w‖2 − 2\nγt EIt [φ(wt)− φ(w)] +\n2 γt |EIt [φIt(wt)− φ(wt)]|\n≤ ‖wt−1 −w‖2 − 2\nγt EIt [φ(wt)− φ(w)] +\n8L2\nγt(λ+ γt)b ."
    }, {
      "heading" : "A.4. Proof of Theorem 4",
      "text" : "Proof When ℓ(w, ξ) is weakly convex (i.e., λ = 0), we further set γt = γ for all t ≥ 1. Applying Lemma 3 withw = w∗ yields\nEIt [φ(wt)− φ(w∗)] ≤ γ\n2\n( ‖wt−1 −w∗‖2 − EIt ‖wt −w∗‖2 ) + 4L2\nγb . (20)\nSumming (20) for t = 1, . . . , T yields\nT∑\nt=1\nE [φ(wt)− φ(w∗)] ≤ γ\n2 ‖w0 −w∗‖2 +\n4L2T\nγb .\nMinimizing the RHS over γ gives the optimal choice\nγ =\n√ 8T\nb · L‖w0 −w∗‖ ,\nwith a corresponding regret\n1\nT\nT∑\nt=1\nE [φ(wt)− φ(w∗)] ≤ √ 8L√ bT ‖w0 −w∗‖ .\nAs a result, by returning the uniform average ŵT = 1 T ∑T t=1 wt, we have due to the convexity of φ(w) that\nE [φ(ŵT )− φ(w∗)] ≤ √ 8L√ bT ‖w0 −w∗‖ ."
    }, {
      "heading" : "A.5. Proof of Theorem 5",
      "text" : "Proof Let ℓ(w, ξ) be λ-strongly convex for some λ > 0. Applying Lemma 3 withw = w∗ yields\nEIt [φ(wt)− φ(w∗)] ≤ ( γt 2 ‖wt−1 −w∗‖2 − λ+ γt 2 EIt ‖wt −w∗‖2 ) +\n4L2\n(λ+ γt)b . (21)\nSetting γt = λ(t−1) 2 for t = 1, . . . , 5, the above inequality becomes\nEIt [φ(wt)− φ(w∗)] ≤ ( λ(t− 1)\n4 ‖wt−1 −w∗‖2 −\nλ(t+ 1)\n4 EIt ‖wt −w∗‖2\n) +\n8L2\nλb(t+ 1)\n≤ ( λ(t− 1)\n4 ‖wt−1 −w∗‖2 −\nλ(t+ 1)\n4 EIt ‖wt −w∗‖2\n) + 8L2\nλbt ,\nand therefore\nt · EIt [φ(wt)− φ(w∗)] ≤ λ\n4\n( (t− 1)t ‖wt−1 −w∗‖2 − t(t+ 1)EIt ‖wt −w∗‖2 ) + 8L2\nλb .\nSumming this inequality for t = 1, . . . , T yields\nT∑\nt=1\nt · E [φ(wt)− φ(w∗)] ≤ 8L2T\nλb .\nAs a result, by returning the weighted average ŵT = 2\nT (T+1) ∑T t=1 twt, we have due to the con-\nvexity of φ(w) that φ(ŵT ) ≤ 2T (T+1) ∑T t=1 t · φ(wt) and\nE [φ(ŵT )− φ(w∗)] ≤ 2\nT (T + 1)\nT∑\nt=1\nt · E [φ(wt)− φ(w∗)] ≤ 16L2\nλb(T + 1) ."
    }, {
      "heading" : "Appendix B. Analysis of inexact minibatch-prox",
      "text" : ""
    }, {
      "heading" : "B.1. Proof of Lemma 6",
      "text" : "Proof Due to the (λ+ γt)-strong convexity of f̃t(w), we have\nEA ‖w̃t − w̄t‖2 ≤ 2\nλ+ γt EA\n[ f̃t(w̃t)− f̃t(w̄t) ] ≤ 2ηt\nλ+ γt .\nApplying Lemma 1 to the exact minimizer w̄t yields\nφIt(w̄t)− φIt(w) ≤ γt 2 ‖w̃t−1 −w‖2 − λ+ γt 2 ‖w̄t −w‖2 .\n5. This choice is inspired by the stepsize rule of Lacoste-Julien et al. (2012) for stochastic gradient descent.\nTherefore, for the t-th iteration, we have\nEIt,A [φ(w̃t)− φ(w)] = EIt,A [φ(w̃t)− φIt(w̄t)] + EIt [φIt(w̄t)− φIt(w)]\n≤ 4L 2\n(λ+ γt)b + √ 2L2ηt λ+ γt + γt 2 ‖w̃t−1 −w‖2 − λ+ γt 2 EIt ‖w̄t −w‖2\n≤ 4L 2\n(λ+ γt)b + √ 2L2ηt λ+ γt + γt 2 ‖w̃t−1 −w‖2 − λ+ γt 2 EIt,A (‖w̃t −w‖ − ‖w̃t − w̄t‖)2\n≤ 4L 2\n(λ+ γt)b + √ 2L2ηt λ+ γt + γt 2 ‖w̃t−1 −w‖2 − λ+ γt 2 EIt,A ‖w̃t −w‖2\n+ (λ+ γt) · EIt,A [‖w̃t − w̄t‖ · ‖w̃t −w‖]\n≤ 4L 2\n(λ+ γt)b + √ 2L2ηt λ+ γt + γt 2 ‖w̃t−1 −w‖2 − λ+ γt 2 EIt,A ‖w̃t −w‖2\n+ (λ+ γt) √ EIt,A ‖w̃t − w̄t‖2 · √ EIt,A ‖w̃t −w‖2\n≤ 4L 2\n(λ+ γt)b + √ 2L2ηt λ+ γt + γt 2 ‖w̃t−1 −w‖2 − λ+ γt 2 EIt,A ‖w̃t −w‖2\n+ √ 2(λ+ γt)ηt · √ EIt,A ‖w̃t −w‖2\nwhere we have applied Lemma 11 to the approximate minimizer w̃t in the first inequality, used the triangle inequality ‖w̄t −w‖ ≥ |‖w̃t −w‖ − ‖w̃t − w̄t‖| in the second inequality, dropped a negative term in the third inequality, and used the Cauchy-Schwarz inequality for random variables in the fourth inequality."
    }, {
      "heading" : "B.2. Proof of Theorem 7",
      "text" : "When ℓ(w, ξ) is weakly convex (i.e., λ = 0), set γt = γ for all t ≥ 1 as in exact minibatch-prox. Then summing (11) for t = 1, . . . , T yields\nT∑\nt=1\nE [φ(w̃t)− φ(w∗)] + γ\n2 E ‖w̃T −w∗‖2 ≤\nγ 2 ‖w̃0 −w∗‖2 +\n4L2T\nγb +\nT∑\nt=1\n√ 2L2ηt γ\n+ T∑\nt=1\n√ 2γηt · √ E ‖w̃t −w∗‖2 (22)\nwhere the expectation is taken over random sampling and the randomness of A in the first T iterations. To resolve the recursion, we need the following lemma by Schmidt et al. (2011).\nLemma 12 Assume that the non-negative sequence {uT } satisfies the following recursion for all T ≥ 1:\nu2T ≤ ST + T∑\nt=1\nλtut,\nwith ST an increasing sequence, S0 ≥ u20 and λt ≥ 0 for all t. Then, for all T ≥ 1, we have\nuT ≤ 1\n2\nT∑\nt=1\nλt +  ST + ( 1\n2\nT∑\nt=1\nλt\n)2  1 2 ≤ √\nST +\nT∑\nt=1\nλt.\nWe are now ready to prove Theorem 7.\nProof Bounding √ E ‖w̃t −w∗‖2. Dropping the ∑T t=1 E [φ(w̃t)− φ(w∗)] term from (22) which is non-negative due to the optimality of w∗, we obtain\nE ‖w̃T −w∗‖2 ≤ ‖w̃0 −w∗‖2 + 8L2T\nγ2b +\nT∑\nt=1\n√ 8L2ηt γ3 + T∑\nt=1\n√ 8ηt γ · √ E ‖w̃t −w∗‖2.\nNow apply Lemma 12 (using uT = √ E ‖w̃T −w∗‖2, ST = ‖w̃0 −w∗‖2+ 8L 2T γ2b + ∑T t=1 √ 8L2ηt γ3 , and λt = √ 8ηt γ ) and the fact that √ x+ y ≤ √x+√y for x, y ≥ 0, we have\n√ E ‖w̃T −w∗‖2 ≤ ‖w̃0 −w∗‖+\n√ 8L2T\nγ2b +\nT∑\nt=1\n√ 8ηt γ +\n√√√√ T∑\nt=1\n√ 8L2ηt γ3\nWe have thus bounded the sequence of √ E ‖w̃T −w∗‖2 by a non-negative increasing sequence.\nBounding function values. Dropping theE ‖w̃T −w∗‖2 term from (22) which is non-negative, we obtain\nT∑\nt=1\nE [φ(w̃t)− φ(w∗)]\n≤ γ 2 ‖w̃0 −w∗‖2 +\n4L2T\nγb +\nT∑\nt=1\n√ 2L2ηt γ + T∑\nt=1\n√ 2ηtγ · √ E ‖w̃t −w∗‖2\n≤ γ 2 ‖w̃0 −w∗‖2 +\n4L2T\nγb +\nT∑\nt=1\n√ 2L2ηt γ + ( T∑\nt=1\n√ 2ηtγ ) · max 1≤t≤T √ E ‖w̃t −w∗‖2\n≤ γ 2 ‖w̃0 −w∗‖2 +\n4L2T\nγb +\nT∑\nt=1\n√ 2L2ηt γ\n+\n( T∑\nt=1\n√ 2ηtγ ) ·  ‖w̃0 −w∗‖+ √ 8L2T\nγ2b +\nT∑\nt=1\n√ 8ηt γ +\n√√√√ T∑\nt=1\n√ 8L2ηt γ3   . (23)\nTo achieve the same order of regret as in exact minibatch-prox, we require that ηt decays with t, and in particular\nηt ≤ min ( c1 ( T\nb\n) 1 2\n, c2\n( T\nb\n) 3 2 ) · L ‖w̃0 −w∗‖\nt2+2δ (24)\nfor some δ > 0. Note that ηt has the unit of function value. Let c := ∑∞ i=1 1 i1+δ ≤ 1+δδ which only depends on δ (as a concrete example, we have c = π 2\n6 when δ = 2).\nUsing the choice of γ = √\n8T b · L‖w0−w∗‖ , we obtain from (24) that\nT∑\nt=1\n√ 8ηt γ = T∑\nt=1\n√√ 8b\nT · ‖w0 −w∗‖ L · ηt ≤ 8 1 4 c 1 2 1 ‖w̃0 −w∗‖\nT∑\nt=1\n1\nt1+δ\n≤ 8 14 c 1 2 1 c ‖w̃0 −w∗‖ ,\nT∑\nt=1\n√ 8L2ηt γ3 = T∑\nt=1\n√√ b3\n8T 3 · ‖w0 −w∗‖ 3 L · ηt ≤ 8− 1 4 c 1 2 2 ‖w̃0 −w∗‖2\nT∑\nt=1\n1\nt1+δ\n≤ 8− 14 c 1 2 2 c ‖w̃0 −w∗‖2 .\nContinuing from (23) and substituting in the value of γ, we have\nT∑\nt=1\nE [φ(w̃t)− φ(w∗)] ≤ √ 8T\nb · L ‖w̃0 −w∗‖+\nγ\n2\nT∑\nt=1\n√ 8L2ηt γ3\n+ γ\n2\n( T∑\nt=1\n√ 8ηt γ ) ·  2 ‖w̃0 −w∗‖+ T∑\nt=1\n√ 8ηt γ +\n√√√√ T∑\nt=1\n√ 8L2ηt γ3  \n=\n√ 8T\nb · L ‖w̃0 −w∗‖+\n√ 2T\nb · L‖w0 −w∗‖ · 8− 14 c 1 2 2 c ‖w̃0 −w∗‖2\n+\n√ 2T\nb · L‖w0 −w∗‖ · 8 14 c 1 2 1 c ‖w̃0 −w∗‖×\n( 2 ‖w̃0 −w∗‖+ 8 1 4 c 1 2 1 c ‖w̃0 −w∗‖+ √ 8− 1 4 c 1 2 2 c ‖w̃0 −w∗‖2 )\n= c3\n√ T\nb · L ‖w̃0 −w∗‖ .\nThe suboptimality of ŵT is then due to the convexity of φ(w):\nE [φ(ŵT )− φ(w∗)] ≤ 1\nT\nT∑\nt=1\nE [φ(w̃t)− φ(w∗)] = c3L ‖w̃0 −w∗‖√\nbT ."
    }, {
      "heading" : "B.3. Proof of Theorem 8",
      "text" : "Proof We have by Lemma 6 that\nEIt,A [φ(w̃t)− φ(w∗)] ≤ λ(t− 1)\n4 ‖w̃t−1 −w∗‖2 −\nλ(t+ 1)\n4 EIt,A ‖w̃t −w∗‖2\n+ 8L2\nλb(t+ 1) + √ 4L2ηt λ(t+ 1) + √ λ(t+ 1)ηt · √ EIt,A ‖w̃t −w∗‖2.\nRelaxing the 1t+1 to 1 t on the RHS, and multiplying both sides by t, we further obtain\nt · EIt,A [φ(w̃t)− φ(w∗)] ≤ λ(t− 1)t\n4 ‖w̃t−1 −w∗‖2 −\nλt(t+ 1)\n4 EIt,A ‖w̃t −w∗‖2\n+ 8L2\nλb +\n√ 4L2tηt\nλ +\n√ λtηt · √ EIt,A [ t(t+ 1) ‖w̃t −w∗‖2 ] .\nSumming this inequality for t = 1, . . . , T yields\nT∑\nt=1\nt · E [φ(w̃t)− φ(w∗)] + λT (T + 1)\n4 E ‖w̃T −w∗‖2\n≤ 8L 2T\nλb +\nT∑\nt=1\n√ 4L2tηt\nλ +\nT∑\nt=1\n√ λtηt · √ E [ t(t+ 1) ‖w̃t −w∗‖2 ] . (25)\nBounding √ E ‖w̃t −w∗‖2. Dropping the ∑T t=1 t · E [φ(w̃t)− φ(w∗)] term from (25) which\nis non-negative due to the optimality of w∗, we obtain\nE [ T (T + 1) ‖w̃T −w∗‖2 ] ≤32L 2T\nλ2b +\nT∑\nt=1\n√ 64L2tηt\nλ3 +\nT∑\nt=1\n√ 16tηt λ · √ E [ t(t+ 1) ‖w̃t −w∗‖2 ] .\nApplying Lemma 12 (using uT =\n√ E [ T (T + 1) ‖w̃T −w∗‖2 ] , ST = 32L2T λ2b + ∑T t=1 √ 64L2tηt λ3 ,\nand λt = √ 16tηt λ ), we have\n√ E [ T (T + 1) ‖w̃T −w∗‖2 ] ≤ √ 32L2T\nλ2b +\nT∑\nt=1\n√ 16tηt λ +\n√√√√ T∑\nt=1\n√ 64L2tηt\nλ3 .\nBounding function values. Dropping theE ‖w̃T −w∗‖2 term from (25) which is non-negative, we obtain\nT∑\nt=1\nt · E [φ(w̃t)− φ(w∗)] ≤ 8L2T\nλb +\nT∑\nt=1\n√ 4L2tηt\nλ\n+\n( T∑\nt=1\n√ λtηt ) ·   √ 32L2T\nλ2b +\nT∑\nt=1\n√ 16tηt λ +\n√√√√ T∑\nt=1\n√ 64L2tηt\nλ3\n  . (26)\nTo achieve the same order of regret as in exact minibatch-prox, we require that ηt decays with t, and in particular\nηt ≤ min ( c1 ( T\nb\n) , c2 ( T\nb\n)2) · L 2\nt3+2δλ (27)\nfor some δ > 0. Note that ηt has the unit of function value. Let c := ∑∞ i=1 1 i1+δ ≤ 1+δδ . Then (27) ensures that\nT∑\nt=1\n√ tηt λ ≤ c√c1 √ L2T λ2b , and\nT∑\nt=1\n√ L2tηt λ3 ≤ c√c2 · L2T λ2b .\nContinuing from (26), we have\nT∑\nt=1\nt · E [φ(w̃t)− φ(w∗)] ≤ 8L2T\nλb + 2c\n√ c2 · L2T\nλb\n+ c √ c1\n√ L2T\nb\n(√ 32L2T\nλ2b + 4c\n√ c1\n√ L2T\nλ2b +\n4 √\n64c2c2\n√ L2T\nλ2b\n)\n= c3 2 · L 2T λb .\nIn view of the convexity of φ(w), by returning the weighted average ŵT = 2\nT (T+1) ∑T t=1 tw̃t, we\nhave\nE [φ(ŵT )− φ(w∗)] ≤ 2\nT (T + 1)\nT∑\nt=1\nt · E [φ(w̃t)− φ(w∗)] ≤ c3L\n2\nλb(T + 1) .\nB.4. Connection to minibatch stochastic gradient descent\nTo see the connection between minibatch-prox and minibatch SGD, note that if we solve the linearized minibatch problem exactly, we obtain the minibatch stochastic gradient descent algorithm:\nw̃t = argmin w∈Ω φIt(w̃t−1) +∇〈φIt(w̃t−1), w − w̃t−1〉+ γt 2 ‖w − w̃t−1‖2 .\nFollowing Cotter et al. (2011), we assume that ℓ(w, ξ) is β-smooth: ∥∥∇ℓ(w, ξ)−∇ℓ(w′, ξ) ∥∥ ≤ β ∥∥w −w′ ∥∥ , ∀w,w′ ∈ Ω.\nWe then have the following guarantee for each iterate of minbatch SGD.\nProposition 13 For iteration t of minibatch SGD, we have\nEIt [φ(w̃t)− φ(w∗)] ≤ 2L2\n(γt − β)b + γt − λ 2 ‖w∗ − w̃t−1‖2 − γt 2 EIt ‖w∗ − w̃t‖2 . (28)\nProof Our proof closely follows that of Cotter et al. (2011).\nDue to the smoothness of φ, we have that\nφ(w̃t) ≤ φ(w̃t−1) + 〈∇φ(w̃t−1), w̃t − w̃t−1〉+ β\n2 ‖w̃t − w̃t−1‖2\n≤ φ(w̃t−1) + 〈∇φ(w̃t−1)−∇φIt(w̃t−1), w̃t − w̃t−1〉+ β\n2 ‖w̃t − w̃t−1‖2\n+ 〈∇φIt(w̃t−1), w̃t − w̃t−1〉\n= φ(w̃t−1) + ‖∇φ(w̃t−1)−∇φIt(w̃t−1)‖ · ‖w̃t − w̃t−1‖+ β\n2 ‖w̃t − w̃t−1‖2\n+ 〈∇φIt(w̃t−1), w̃t − w̃t−1〉\n≤ φ(w̃t−1) + 1\n2(γt − β) ‖∇φ(w̃t−1)−∇φIt(w̃t−1)‖2 + γt − β 2 ‖w̃t − w̃t−1‖2\n+ β\n2 ‖w̃t − w̃t−1‖2 + 〈∇φIt(w̃t−1), w̃t − w̃t−1〉\n= φ(w̃t−1) + 1\n2(γt − β) ‖∇φ(w̃t−1)−∇φIt(w̃t−1)‖2 + γt 2 ‖w̃t − w̃t−1‖2\n+ 〈∇φIt(w̃t−1), w̃t − w̃t−1〉 (29)\nwhere we have used the Cauchy-Schwarz inequality in the second inequality, and the inequality xy ≤ x22α + αy2 2 in the third inequality.\nNow, since w̃t is the minimizer of the γt-strongly convex function\nγt 2 ‖w − w̃t−1‖2 + 〈∇φIt(w̃t−1), w − w̃t−1〉\nin Ω, we have according to Lemma 1 (replacing the local objective with its linear approximation) that\nγt 2 ‖w∗ − w̃t−1‖2 + 〈∇φIt(w̃t−1), w∗ − w̃t−1〉\n≥ γt 2 ‖w̃t − w̃t−1‖2 + 〈∇φIt(w̃t−1), w̃t − w̃t−1〉+ γt 2 ‖w∗ − w̃t‖2 .\nSubstituting this into (29) gives\nφ(w̃t) ≤ φ(w̃t−1) + 1\n2(γt − β) ‖∇φ(w̃t−1)−∇φIt(w̃t−1)‖2 + γt 2 ‖w∗ − w̃t−1‖2\n+ 〈∇φIt(w̃t−1), w∗ − w̃t−1〉 − γt 2 ‖w∗ − w̃t‖2 .\nTaking expectation of this inequality over the random sampling of It further leads to\nEIt [φ(w̃t)] ≤ φ(w̃t−1) + 1\n2(γt − β) EIt ‖∇φ(w̃t−1)−∇φIt(w̃t−1)‖2 + γt 2 ‖w∗ − w̃t−1‖2\n+ 〈∇φ(w̃t−1), w∗ − w̃t−1〉 − γt 2 EIt ‖w∗ − w̃t‖2\n≤ φ(w∗) + 1\n2(γt − β) EIt ‖∇φ(w̃t−1)−∇φIt(w̃t−1)‖2\n+ γt − λ\n2 ‖w∗ − w̃t−1‖2 − γt 2 EIt ‖w∗ − w̃t‖2 (30)\nwhere in the second inequality we have used the fact that\nφ(w∗) ≥ φ(w̃t−1) + 〈∇φ(w̃t−1), w∗ − w̃t−1〉+ λ\n2 ‖w∗ − w̃t−1‖2\ndue to the convexity of φ(w). On the other hand, let It = {ξ1, . . . , ξb}, we have\nEIt ‖∇φ(w)−∇φIt(w)‖2\n= EIt ∥∥∥∥∥∇φ(w)− 1 b b∑\ni=1\n∇ℓ(w, ξi) ∥∥∥∥∥ 2\n= EIt ∥∥∥∥∥ 1 b b∑\ni=1\n(∇φ(w)−∇ℓ(w, ξi)) ∥∥∥∥∥ 2\n= 1\nb2\nb∑\ni=1\nEξi ‖∇φ(w)−∇ℓ(w, ξi)‖2 + 1\nb2\n∑ i 6=j EIt 〈∇φ(w)−∇ℓ(w, ξi), ∇φ(w)−∇ℓ(w, ξj)〉\n= 1\nb · Eξ ‖∇φ(w)−∇ℓ(w, ξ)‖2\n≤ 4L 2\nb\nwhere we used the fact that the samples are i.i.d. in the fourth equality, and that ‖∇φ(w)‖ , ‖∇ℓ(w, ξ)‖ ≤ L in the last inequality. Continuing from (30) yields the desired result.\nComparing this result to (20) and (21), we observe that minibatch SGD has a similar recursion to that exact minibatch-prox, except the appearance of β in the denominator of the “stability” term. We now show that this difference leads to significant difference in convergence rate.\nLet ℓ(w, ξ) be weakly convex (λ = 0), and γt = γ for all t ≥ 1. Summing (28) over t = 1, . . . , T gives\nT∑\nt=1\nE [φ(w̃t)− φ(w∗)] ≤ 2L2T (γ − β)b + γ 2 ‖w∗ − w̃0‖2 .\nMinimizing the RHS over γ gives\nγ = β +\n√ 4T\nb · L‖w∗ − w̃0‖ ,\nwhich leads to\n1\nT\nT∑\nt=1\nE [φ(w̃t)− φ(w∗)] ≤ 2L ‖w∗ − w̃0‖√\nbT + β ‖w∗ − w̃0‖2 2T .\nSo we obtain the familiar O (\n1√ bT + 1T\n) rate for minibatch SGD."
    }, {
      "heading" : "Appendix C. Proof of Theorem 10",
      "text" : "Proof On the one hand, as we choose γ as Theorem 7 suggested, we just need to verify that the inexactness conditions in Theorem 7 is satisfied, i.e., for t = 1, . . . , T , we require (recall that w∗t = argminw f̃t(w))\nf̃t(wt)− f̃t(w∗t ) ≤ 1\n104 ·min\n(( T\nbm\n)1/2 , ( T\nbm )3/2) · LB t3 .\nOn the other hand, we can bound the initial suboptimality of f̃t(w) when initializing from wt−1. This is because, by the optimality of w∗t , we have ‖w∗t −wt−1‖ = ∥∥∥ 1γ∇φIt(w∗t ) ∥∥∥ ≤ L/γ, and\nf̃t(wt−1)− f̃t(w∗t ) = 0 + φIt(wt−1)− γ 2 ‖w∗t −wt−1‖2 − φIt(w∗t )\n≤ φIt(wt−1)− φIt(w∗t ) ≤ L ‖w∗t −wt−1‖ ≤ L2/γ. (31)\nCombining the above two inequalities, the initial versus final error for the K DSVRG iterations is bounded by\n104 ·max (( bm\nT\n)1/2 , ( bm\nT\n)3/2) · t3 · L\nBγ\n= 104 ·max (( bm\nT\n)1/2 , ( bm\nT\n)3/2) · T 3 · L\nB · bmB√\n8n(ε)L\n= O ( max ( n(ε)2\nbm , bm · n(ε)\n))\n= O ( n2(ε) )\nwhere we have used the definition of γ and T = n(ε)bm in the first and second step respectively. By the iteration complexity results for sampling without-replacement DSVRG (Shamir, 2016, Theorem 4), we have the desired suboptimality in f̃t(w) using\nK = O (log n(ε)) (32)\niterations, as long as the batch size b/pi is larger than the problem condition number. Now, the condition number of f̃(w) is\nβ + γ\nγ = O ( βbmB√ n(ε)L ) .\nEquating this to the batch size b/pi yields the pi specified in the theorem. It is also easy to check that K γ = O(bm), i.e., the total number of stochastic updates is less than the total number of samples, as required by Shamir (2016, Theorem 4).\nCommunication: the total rounds of communication required by Algorithm 1 is\nKT = O ( n(ε)\nmb log n(ε)\n) .\nComputation: For each communication round, each machine need to compute the local full gradient, which can be done in parallel, and then one of the machines perform b/pi steps of stochastic update. So the computation cost is\nKT ( b+ b\npi\n) = O ( n(ε)\nm log n(ε)\n) .\nMemory: It is straightforward to see each machine only need to maintain b samples."
    }, {
      "heading" : "Appendix D. Communication-efficient distributed minibatch-prox with DANE",
      "text" : "As discussed in Section 4, it is also possible to use other efficient distributed optimization solver for minibatch-prox. Here we present a novel method that use the distributed optimization algorithm DANE (Shamir et al., 2014) and its accelerated variant AIDE (Reddi et al., 2016) for solving (12), which define better local objectives than EMSO and take into consideration the similarity between local objectives.\nWe detail our algorithm, named MP-DANE, in Algorithm 2. The algorithm consists of three nested loops, where t, r and k are iteration counters for minibatch-prox (the outer for-loop), AIDE (the intermediate for-loop) and DANE (the inner for-loop) respectively. Compared to EMSO, DANE adds a gradient correction term to (13) which can be compute efficiently with one round of communication. On top of that, AIDE uses the idea of universal catalyst (Lin et al., 2015) and adds an extra quadratic term to improve the strong-convexity of the objective for faster convergence, i.e., in order to solve (12), AIDE solves multiple instances of the “augmented large minibatch” problems of the form\nmin w∈Ω\nf̄t,r(w) := φIt(w) + γ\n2 ‖w −wt−1‖2 +\nκ 2 ‖w − yr−1‖2 (36)\nwith carefully chosen extrapolation points yr−1. At each DANE iteration, we perform two rounds of communication, one for averaging the local gradients, and one for averaging the local updates, and the amount of data we communicate per round has the same size of the predictor.\nTo sum up, in Algorithm 2, we have introduced two levels of inexactness. First, we only approximately solve the “large minibatch” subproblem (12) in each outer loop; results from the previous section guarantee the convergence of this approach. Second, we only approximately solve the local subproblems (33) to sufficient accuracy in each inner loop; the analysis of “inexact DANE” (for the non-stochastic setting) provides guarantee for this approach (Reddi et al., 2016), and enables us to use state-of-the-art SGD methods (e.g., SVRG Johnson and Zhang, 2013; Xiao and Zhang, 2014) for solving local subproblems. Overall, we obtain a convergent algorithm for distributed stochastic convex optimization.\nWe now present detailed analysis for the computation/communication complexity of Algo-\nrithm 2 for stochastic quadratic problems, and compare it with related methods in the literature."
    }, {
      "heading" : "D.1. Efficiency of MP-DANE",
      "text" : "We present the main results of this section (full analysis is deferred to Appendix D.3), which show that with careful choices of the minibatch size and the desired accuracy in each level of approxi-\nAlgorithm 2MP-DANE for distributed stochastic convex optimization.\nInitialize w0. for t = 1, 2, . . . , T do\nEach machine i draws a minibatch I (i) t of b samples from the underlying data distribution. Initialize y0 ← wt−1, x0 ← wt−1. for r = 1, 2, . . . , R do\nInitialize z0 ← yr−1, α0 = √ γ/(γ + κ). for k = 1, 2, . . . ,K do 1. All machines perform one round of communication to compute the average gradient\n∇φIt(zk−1) ← 1\nm\nm∑\ni=1\n∇φ I (i) t (zk−1).\n2. Each machine i approximately solves the local objective to θ-accuracy:\napply prox-SVRG to find z (i) k s.t. ∥∥∥z(i)k − z (i)∗ k ∥∥∥ ≤ θ ∥∥∥zk−1 − z(i)∗k ∥∥∥\nwhere z (i)∗\nk = argmin z∈Ω φ I (i) t (z) + 〈 ∇φIt(zk−1)−∇φI(i)t (zk−1), z 〉 + γ 2 ‖z−wt−1‖2\n+ κ\n2 ‖z− yr−1‖2 . (33)\n3. All machines reach consensus by averaging local updates through another round of communication:\nzk ← 1\nm\nm∑\ni=1\nz (i) k . (34)\nend for Update xr ← zK . Compute αr ∈ (0, 1) such that α2r = (1− αr)α2r−1 + γαk/(γ + κ), and compute\nyr = xr +\n( αr−1(1− αr−1)\nαr + α 2 r−1\n) (xr − xr−1). (35)\nend for\nUpdate wt ← xr. end for\nOutput: wT is the approximate solution.\nmate solution, MP-DANE achieves both communication and computation efficiency with the optimal sample complexity. Interestingly, the choices of parameters differ in two regimes which are separated by an “optimal” minibatch size (also denoted as bmp-dane in the main text)\nb∗ = n(ε)L2\n32m2β2B2 log(md) .\nTheorem 14 (Efficiency of MP-DANE for b ≤ b∗) Set the parameters in Algorithm 2 as follows:\n(outer loop) b ≤ b∗ = n(ε)L 2\n32m2β2B2 log(md) , T =\nn(ε)\nbm , γ =\n√ 8n(ε)L\nbmB ,\n(intermediate loop) κ = 0, R = 1,\n(inner loop) θ = 1\n6 , K = O (log n(ε)) .\nThen we have E [ φ (\n1 T ∑T t=1 wt ) − φ(w∗) ] ≤ √ 40BL√ n(ε) = O (ε) .\nMoreover, Algorithm 2 can be implemented with Õ ( n(ε) bm ) rounds of communication, and each\nmachine performs Õ ( n(ε) m ) vector operations in total, where the notation Õ(·) hides poly-logarithmic dependences on n(ε).\nWhen we choose b = b∗, Algorithm 1 can be implemented with Õ ( mβ2B2\nL2\n) rounds of commu-\nnication, Õ ( n(ε) bm ) vector operations, and O ( n(ε)L2 m2β2B2 ) memory for each machine.\nWe comment on the choice of parameters. For sample efficiency, we fix the sample size n(ε) and number of machines m, and so we can tradeoff the local minibatch size b and the total number of outer iterations T , maintaining bT = n(ε)m . For any b, the regularization parameters in the “large\nminibatch” problem is set to γ = √\n8T bm · LB =\n√ 8n(ε)L\nbmB according to Theorem 7. When b ≤ b∗, we note that (37) can be satisfied with κ = 0 and there is no need for acceleration by AIDE (R = 1). Then the values of θ and K follow from Lemma 18.\nRemark 15 The above theorem suggests that in the regime of b ≤ b∗, we only need to have logarithmic number of DANE iterations for solving each “large minibatch” problem, and logarithmic number of passes over the local data during each DANE iteration. We present experimental results validating our theory in Appendix E.\nThe next theorem shows that when we use a large minibatch size b in Algorithm 2, we can still satisfy the condition (37) by adding extra regularization (κ > 0), and then apply accelerated DANE.\nTheorem 16 (Efficiency of MP-DANE for b ≥ b∗) Set the parameters in Algorithm 2 as follows:\n(outer loop) b ≥ b∗ = n(ε)L 2\n32m2β2B2 log(md) , T =\nn(ε)\nbm , γ =\n√ 8n(ε)L\nbmB ,\n(intermediate loop) κ = 16β\n√ log(dm)\nb − γ, R = O\n( b1/4m1/2 · β1/2B1/2 n(ε)1/4 · L1/2 log n(ε) ) ,\n(inner loop) θ = 1\n6 , K = O (log n(ε)) .\nThen we have E [ φ (\n1 T ∑T t=1 wt ) − φ(w∗) ] ≤ √ 40BL√ n(ε) = O (ε) .\nMoreover, Algorithm 2 can be implemented with Õ ( n(ε)3/4·β1/2B1/2 b3/4m1/2·L1/2 ) rounds of communication,\nand each machine performs Õ ( b1/4n(ε)3/4·β1/2B1/2 m1/2·L1/2 ) vector operations in total, where the notation Õ(·) hides poly-logarithmic dependences on n(ε).\nD.2. Two regimes of multiple resource tradeoffs\nFrom the above analysis, we summarized in Table 2 the resources required by MP-DANE. We observe two interesting regimes, separated by the minibatch size b∗ ≍ n(ε)/(m2B2), that present different tradeoffs between communication, computation and memory.\n• When 1 ≤ b ≤ b∗, the computation complexity remains Õ (n(ε)/m) which is independent of b. This means we always achieve near-linear speedup in this regime. Moreover, there is a tradeoff between communication and memory: the communication complexity decreases,\nwhile the memory cost increases as the minibatch size b increases, both at the linear rate. Thus in this regime, we can trade communication for memory without affecting computation.\n• When b∗ < b ≤ bmax, the computation starts to increase with b at the rate b1/4 which is slower than linear, while the communication cost continues to decrease at the rate b3/4 which is also slower than linear. Thus in this regime, we can trade communication for computation\nand memory."
    }, {
      "heading" : "D.3. Analysis of MP-DANE",
      "text" : "In order to fully analyze Algorithm 2, we need several auxiliary lemmas that characterize the iteration complexity of solving the local problem (33) by prox-SVRG (Xiao and Zhang, 2014), the large minibatch problem (12) by DANE (Shamir et al., 2014) and AIDE (Reddi et al., 2016)."
    }, {
      "heading" : "D.3.1. SOME AUXILIARY LEMMAS",
      "text" : "First, we apply prox-SVRG to the local problem (33), pushing all terms but φ I (i) t (z) in to the proximal operator. The benefit of this approach (as opposed to using plain SVRG Johnson and Zhang, 2013) is that the smoothness parameter that determines the iteration complexity is simply β, same results hold when applying prox-SAGA (Defazio et al., 2014) as well. For sampling without replacement SVRG, the current analysis works only for plain SVRG, so we quote the results from (Shamir, 2016).\nLemma 17 (Iteration complexity of SVRG for (33)) For any target accuracy θ > 0, with initialization zk−1, prox-SVRG outputs z (i) k such that ∥∥∥z(i)k − z (i)∗ k ∥∥∥ ≤ θ ∥∥∥zk−1 − z(i)∗k ∥∥∥ after\nO (( b+ β\nγ + κ\n) · log (β + γ + κ)\n(γ + κ)θ2\n)\nvector operations, and sampling without replacement SVRG outputs z (i) k such that ∥∥∥z(i)k − z (i)∗ k ∥∥∥ ≤ θ ∥∥∥zk−1 − z(i)∗k ∥∥∥ after\nO (( b+ β + κ\nγ + κ\n) · log (β + γ + κ)\n(γ + κ)θ2\n)\nvector operations.\nProof Observe that the objective (33) by f (i) k (z), which is an quadratic function of z with the Hessian matrix Hi = ∇2φI(i)t (z) + (γ + κ)I (γ + κ)I. As a result, the suboptimality of z (i) k is\nǫfinal = f (i) k (z (i) k )− f (i) k (z (i)∗ k ) =\n1\n2\n( z (i) k − z (i)∗ k )⊤ Hi ( z (i) k − z (i)∗ k ) ≥ γ + κ\n2\n∥∥∥z(i)k − z (i)∗ k ∥∥∥ 2 .\nTo satisfy the requirement of ∥∥∥z(i)k − z (i)∗ k ∥∥∥ ≤ θ ∥∥∥zk−1 − z(i)∗k ∥∥∥, we require\nǫfinal ≤ (γ + κ)θ2\n2\n∥∥∥zk−1 − z(i)∗k ∥∥∥ 2 .\nOn the other hand, when initializing from zk−1, the initial suboptimality is\nǫinit = f (i) k (zk−1)− f (i) k (z (i)∗ k ) ≤ σmax(Hi)\n2\n∥∥∥zk−1 − z(i)∗k ∥∥∥ 2 ≤ β + γ + κ\n2\n∥∥∥zk−1 − z(i)∗k ∥∥∥ 2 .\nTherefore, it suffices to have\nǫinit ǫfinal = (β + γ + κ) (γ + κ)θ2 .\nNoting that φ I (i) t (z) is the sum of b components, and each component is β-smooth while the overall function f (i) k is (γ + κ)-strongly convex, the lemma follows directly from the convergence guarantee of prox-SVRG (Xiao and Zhang, 2014, Corollary 1), and sampling without replacement SVRG (Shamir, 2016, Theorem 4).\nNext, we state the convergence rates of “inexact DANE” and AIDE, which can be easily derived from Reddi et al. (2016). At the outer loop t and intermediate loop r, let x∗r = argminw f̄t,r(w) be the exact minimizer of the “augmented large minibatch” problem (36), which is approximately solved by the inner DANE iterations.\nLemma 18 (Iteration Complexity of inexact DANE) Let θ = 16 , and assume that\nb(γ + κ)2 ≥ 256β2 log(dm/δ). (37)"
    }, {
      "heading" : "By initializing from yr−1, and setting the number of inner iterations in Algorithm 2 to be",
      "text" : "K = ⌈1 2 log4/3 (β + γ + κ) (γ + κ)η ⌉ ,\nwe have with probability 1− δ over the sample set It that\nf̄t,r(xr)− f̄t,r(x∗r) ≤ η ( f̄t,r(yr−1)− f̄t,r(x∗r) ) .\nProof Denote by Hi = ∇2φI(i)t (z) + (γ + κ)I the Hessian matrix of the local objective (33) for machine i. Let H = 1m ∑m i=1Hi be the Hessian matrix of the global objective (36), and\nH̃−1 = 1m ∑m i=1 H −1 i . As our objective is quadratic, Hi,H, H̃\n−1 remain unchanged during the inner iterations. By Reddi et al. (2016, Theorem 1), we have\n‖zk − x∗r‖ ≤ (∥∥∥H̃−1H − I ∥∥∥+ θ m m∑\ni=1\n∥∥H−1i H ∥∥ ) ‖zk−1 − x∗r‖ . (38)\nSince∇2ℓ(w, ξ) ≤ β, by Shamir et al. (2014, Lemma 2), we have with probability at least 1−δ over the sample set It that\n‖Hi −H‖ ≤ √ 32β2 log(dm/δ)\nb =: ρ, i = 1, . . . ,m.\nOn the other hand, we have Hi (γ + κ)I and\n4ρ2\n(γ + κ)2 =\n128β2 log(dm/δ)\nb(γ + κ)2 ≤ 1 2\nby our assumption (37). By Shamir et al. (2014, Lemma 1), we have\n∥∥∥H̃−1H − I ∥∥∥ ≤ 1\n2 . (39)\nMoreover, we have\nθ\nm\nm∑\ni=1\n∥∥H−1i H ∥∥ ≤ θ\nm\nm∑\ni=1\n(1 + ∥∥H−1i H − I ∥∥)\n≤ θ m\nm∑\ni=1\n(1 + ∥∥H−1i ∥∥ ∥∥H −H−1i ∥∥)\n≤ θ m\nm∑\ni=1\n( 1 + ρ\nγ + κ\n)\n≤ θ m\nm∑\ni=1\n( 1 + 1\n2 √ 2\n)\n≤ 3θ 2 ≤ 1 4 . (40)\nPlugging (39) and (40) into (38) yields\n‖zk − x∗r‖ ≤ 3 4 ‖zk−1 − x∗r‖ ,\nand thus ‖zK − x∗r‖ ≤ (3/4)K ‖yr−1 − x∗r‖. To guarantee the suboptimality in the objective f̄t,r(w), we note that\nf̄t,r(zK)− f̄t,r(x∗r) = 1 2 (zK − x∗r)⊤H(zK − x∗r) ≤ β + γ + κ 2 ‖zK − x∗r‖2\n≤ ( 3\n4 )2K β + γ + κ 2 ‖yr−1 − x∗r‖2\n≤ ( 3\n4 )2K β + γ + κ γ + κ ( f̄t,r(yr−1)− f̄t,r(x∗r) )\nwhere we have used the fact that ft,r(w) is (γ + κ)-strongly convex in the last inequality. Setting( 3 4 )2K β+γ+κ γ+κ = η, and noting xr = zK , we obtain the desired iteration complexity.\nAt the outer iteration t of Algorithm 2, we are trying to approximately minimize the objective (12) by iteratively (approximately) solving R instances of the “augmented” problem (36). Let w∗t be the exact minimizer of the “large minibatch” subproblem (12):\nw∗t = argmin w f̃t(w).\nThe following lemma characterizes the accelerated convergence rate.\nLemma 19 (Acceleration by universal catalyst, Theorem 3.1 of Lin et al. (2015)) Assume that for all r ≥ 1, we have\nf̄t,r(xr)− f̄t,r(x∗r) ≤ 2\n9\n( 1− 9\n10\n√ γ\nγ + κ\n)R · ( f̃t(x0)− f̃t(w∗t ) ) ,\nthen\nf̃t(xR)− f̃t(w∗t ) ≤ 800(γ + κ)\nγ\n( 1− 9\n10\n√ γ\nγ + κ\n)R+1 ( f̃t(x0)− f̃t(w∗t ) ) ."
    }, {
      "heading" : "D.3.2. PROOF OF THEOREM 14",
      "text" : "Proof First of all, because R = 1, our algorithm collapses into two nested loops. On the one hand, as we choose γ as Theorem 7 suggested, we just need to verify the inexactness conditions in Theorem 7 is satisfied, i.e., for t = 1, . . . , T , we require (recall that w∗t = argminw f̃t(w))\nf̃t(wt)− f̃t(w∗t ) ≤ 1\n104 ·min\n(( T\nbm\n)1/2 , ( T\nbm\n)3/2) · 2LB\nt3 .\nOn the other hand, we can bound the initial suboptimality f̃t(w) (cf. derivation for (31)):\nf̃t(w̃t−1)− f̃t(w∗t ) ≤ L2/γ.\nUsing Lemma 18, we know as long as the inequality (37) is satisfied, we have the desired\nsuboptimality in f̃t(w) using (cf. the derivation for (32))\nK = O (log n(ε))\nrounds of communication, where we have plugged in the value of γ in the second step. It remains to verify the condition (37), by our choice of γ and b, we have\nbγ2 = 8n(ε)L2 bm2B2 ≥ 8n(ε)L 2 b∗m2B2 = 256β2 log(md), (41)\nas desired.\nNext we summarize the communication, computation, and memory efficiency. Communication: the total rounds of communication required by Algorithm 2 is\nKRT = O ( n(ε)\nmb log n(ε)\n) .\nComputation: For each communication round, we need to solve the local problem (33) using prox-SVRG. Now, in view of (41), we have β = O( √ bγ). This implies that βγ = O( √ b) and thus by Lemma 17, the dominant term of the iteration complexity of prox-SVRG is\nO ( b log β + γ\nγ\n) = O (b log n(ε)) .\nMultiplying this with the number of communication rounds yields the desired computation complexity.\nMemory: It is straightforward to see each machine only need to maintain b samples."
    }, {
      "heading" : "D.3.3. PROOF OF THEOREM 16",
      "text" : "Proof First, it is straightforward to verify the condition (37):\nb(γ + κ)2 = 256β2 log(dm).\nSimilarly to Theorem 14, we need the ratio between final versus initial error for the R AIDE iterations to be\nratio = O(n(ε)).\nEquating this ratio to be 800(γ+κ)\nγ\n( 1− 910 √ γ γ+κ )R+1 , we have\nR = 10\n9\n√ γ + κ\nγ log\n( 800(γ + κ)\nγ · 1 ratio\n)\n= O ( b1/4m1/2 · β1/2B1/2 n(ε)1/4 · L1/2 log n(ε) ) .\nNow according to Lemma 19, the final suboptimality for f̄t,r(w) need to be\nǫfinal = 2\n9\n( 1− 9\n10\n√ γ\nγ + κ\n)R · ( f̃t(x0)− f̃t(w∗t ) ) .\nLet us initialize minw f̄t,r(w) by x0. By definition, we have f̄t,r(w) ≥ f̃t(w) and thus\nǫinit = f̄t,r(x0)− f̄t,r(x∗r) ≤ f̃t(x0)− f̃t(xr∗) ≤ f̃t(x0)− f̃t(w∗t )\nwhere we have used the fact that w∗t is the minimizer of f̃t(w) in the second inequality. This means we only need the initial versus final suboptimality of solving f̄t,r(w) to be\n1 η = ǫinit ǫfinal = 9 2\n( 1− 9\n10\n√ γ\nγ + κ\n)−R ,\nwhich, according to Lemma 18, is achieved by inexact DANE with\nK = O ( log 1\nη + log\nβ + γ + κ\nγ + κ\n)\n= O ( R √ γ\nγ + κ\n)\n= O (log n(ε)) .\niterations.\nNext we analyze the communication and computation efficiency of our algorithm. Communication: The total rounds of communication is\nKRT = O ( log n(ε) · b\n1/4m1/2 · β1/2B1/2 n(ε)1/4 · L1/2 log n(ε) · n(ε) bm\n)\n= O ( n(ε)3/4 · β1/2B1/2 b3/4m1/2 · L1/2 log 2 n(ε) ) .\nComputation: Similar to the case of b ≤ b∗, for each DANE local subproblem (33), the sample size b is larger than its condition number. Therefore, the total computational cost is\nO(bKRT ) = O ( b1/4n(ε)3/4 · β1/2B1/2\nm1/2 · L1/2 log 2 n(ε)\n) ."
    }, {
      "heading" : "Appendix E. Experiments",
      "text" : "In this section we present empirical results to support our theoretical analysis of MP-DANE. We perform least squares regression and classification on several publicly available datasets6; the statistics of these datasets and the corresponding losses are summarized in Table 3. For each dataset, we\n6. https://www.csie.ntu.edu.tw/˜cjlin/libsvm/\nrandomly select half of the samples for training, and the remaining samples are used for estimating the stochastic objective.\nFor MP-DANE, we use SAGA (Defazio et al., 2014) to solve each local DANE subproblem (33) and fix the number of SAGA steps to b (i.e., we just make one pass over the local data), while varying the number of DANE rounds K over {1, 2, 4, 8, 16}. For simplicity, we do not use catalyst acceleration and set R = 1 and κ = 0 in all experiments. Our experiments simulate a distributed environment with m machines, for m = 4, 8, 16. We conduct a simple comparison with minibatch SGD. Stepsizes for SAGA and minibatch SGD are set based on the smoothness parameter of the loss.\nWe plot in Figure 3 the estimated population objective vs. minibatch size b for different parameters. We make the following observations.\n• For minibatch SGD, as b increases, the objective often increases quickly, this is because minibatch SGD can not uses large minibatch sizes while preserving sample efficiency.\n• For MP-DANE, the objective increases much more slowly as b increases. This demonstrates the effectiveness of minibatch-prox for using large minibatch sizes.\n• Running more iterations of DANE often helps, but with diminishing returns. This validates our theory that only a near-constant number of DANE iterations is needed for solving the\nlarge minibatch objective, without affecting the sample efficiency.\nm = 4 m = 8 m = 16"
    } ],
    "references" : [ {
      "title" : "Incremental proximal methods for large scale convex optimization",
      "author" : [ "Dimitri P. Bertsekas" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "Bertsekas.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 2011
    }, {
      "title" : "Incremental aggregated proximal and augmented Lagrangian algorithms",
      "author" : [ "Dimitri P. Bertsekas" ],
      "venue" : "[cs.SY], November",
      "citeRegEx" : "Bertsekas.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 2015
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Boyd et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Boyd et al\\.",
      "year" : 2011
    }, {
      "title" : "Implicit online learning with kernels",
      "author" : [ "Li Cheng", "S.V.N. Vishwanathan", "Dale Schuurmans", "Shaojun Wang", "Terry Caelli" ],
      "venue" : "In Proceedings of the 19th International Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Cheng et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2006
    }, {
      "title" : "Better mini-batch algorithms via accelerated gradient methods",
      "author" : [ "Andrew Cotter", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Cotter et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cotter et al\\.",
      "year" : 2011
    }, {
      "title" : "Online passive-aggressive algorithms",
      "author" : [ "Koby Crammer", "Ofer Dekel", "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Crammer et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Crammer et al\\.",
      "year" : 2006
    }, {
      "title" : "A simple practical accelerated method for finite sums",
      "author" : [ "Aaron Defazio" ],
      "venue" : "In Advances In Neural Information Processing Systems,",
      "citeRegEx" : "Defazio.,? \\Q2016\\E",
      "shortCiteRegEx" : "Defazio.",
      "year" : 2016
    }, {
      "title" : "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Defazio et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Defazio et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimal distributed online prediction using mini-batches",
      "author" : [ "Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Dekel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dekel et al\\.",
      "year" : 2012
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Johnson and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Johnson and Zhang.",
      "year" : 2013
    }, {
      "title" : "Implicit online learning",
      "author" : [ "Brian Kulis", "Peter L. Bartlett" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "Kulis and Bartlett.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kulis and Bartlett.",
      "year" : 2010
    }, {
      "title" : "A simpler approach to obtaining an o(1/t) convergence rate for the projected stochastic subgradient method",
      "author" : [ "Simon Lacoste-Julien", "Mark Schmidt", "Francis Bach" ],
      "venue" : "[cs.LG],",
      "citeRegEx" : "Lacoste.Julien et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lacoste.Julien et al\\.",
      "year" : 2012
    }, {
      "title" : "Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity",
      "author" : [ "Jason D Lee", "Qihang Lin", "Tengyu Ma", "Tianbao Yang" ],
      "venue" : "arXiv preprint arXiv:1507.07595,",
      "citeRegEx" : "Lee et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient mini-batch training for stochastic optimization",
      "author" : [ "Mu Li", "Tong Zhang", "Yuqiang Chen", "Alexander J. Smola" ],
      "venue" : "In Proc. of the 20th ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining (SIGKDD",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "A universal catalyst for first-order optimization",
      "author" : [ "Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Lin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "Paved with good intentions: Analysis of a randomized block kaczmarz method",
      "author" : [ "Deanna Needell", "Joel A. Tropp" ],
      "venue" : "Linear Algebra and its Applications,",
      "citeRegEx" : "Needell and Tropp.,? \\Q2014\\E",
      "shortCiteRegEx" : "Needell and Tropp.",
      "year" : 2014
    }, {
      "title" : "Problem complexity and method efficiency",
      "author" : [ "A. Nemirovskii", "D.B. Yudin" ],
      "venue" : "in optimization,",
      "citeRegEx" : "Nemirovskii and Yudin.,? \\Q1983\\E",
      "shortCiteRegEx" : "Nemirovskii and Yudin.",
      "year" : 1983
    }, {
      "title" : "Aide: Fast and communication efficient distributed optimization",
      "author" : [ "Sashank J Reddi", "Jakub Konečnỳ", "Peter Richtárik", "Barnabás Póczós", "Alex Smola" ],
      "venue" : "arXiv preprint arXiv:1608.06879,",
      "citeRegEx" : "Reddi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Reddi et al\\.",
      "year" : 2016
    }, {
      "title" : "Convergence rates of inexact proximal-gradient methods for convex optimization",
      "author" : [ "Mark Schmidt", "Nicolas Le Roux", "Francis Bach" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Schmidt et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 2011
    }, {
      "title" : "Stochastic convex optimization",
      "author" : [ "Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan" ],
      "venue" : "In Proc. of the 22th Annual Conference on Learning Theory (COLT’09),",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2009
    }, {
      "title" : "Without-replacement sampling for stochastic gradient methods: Convergence results and application to distributed optimization",
      "author" : [ "Ohad Shamir" ],
      "venue" : "arXiv preprint arXiv:1603.00570,",
      "citeRegEx" : "Shamir.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shamir.",
      "year" : 2016
    }, {
      "title" : "Distributed stochastic optimization and learning",
      "author" : [ "Ohad Shamir", "Nathan Srebro" ],
      "venue" : "In 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton),",
      "citeRegEx" : "Shamir and Srebro.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shamir and Srebro.",
      "year" : 2014
    }, {
      "title" : "Communication-efficient distributed optimization using an approximate Newton-type method",
      "author" : [ "Ohad Shamir", "Nathan Srebro", "Tong Zhang" ],
      "venue" : "In Proc. of the 31st Int. Conf. Machine Learning (ICML",
      "citeRegEx" : "Shamir et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shamir et al\\.",
      "year" : 2014
    }, {
      "title" : "Analysis of MP-DANE In order to fully analyze Algorithm 2, we need several auxiliary lemmas that characterize the iteration complexity of solving the local problem (33) by prox-SVRG (Xiao and Zhang, 2014), the large minibatch problem (12) by DANE (Shamir et al., 2014) and AIDE (Reddi",
      "author" : [ "memory. D" ],
      "venue" : null,
      "citeRegEx" : "D.3.,? \\Q2016\\E",
      "shortCiteRegEx" : "D.3.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Introduction Consider the stochastic convex optimization (generalized learning) problem (Nemirovskii and Yudin, 1983; Vapnik, 1995; Shalev-Shwartz et al., 2009):",
      "startOffset" : 88,
      "endOffset" : 160
    }, {
      "referenceID" : 19,
      "context" : "Introduction Consider the stochastic convex optimization (generalized learning) problem (Nemirovskii and Yudin, 1983; Vapnik, 1995; Shalev-Shwartz et al., 2009):",
      "startOffset" : 88,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "One simple approach for distributed stochastic optimization is minibatch SGD (Cotter et al., 2011; Dekel et al., 2012), where in each update we use a gradient estimate based on mb examples: b examples from each of the m machines.",
      "startOffset" : 77,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "One simple approach for distributed stochastic optimization is minibatch SGD (Cotter et al., 2011; Dekel et al., 2012), where in each update we use a gradient estimate based on mb examples: b examples from each of the m machines.",
      "startOffset" : 77,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : "A naive approach here is to use accelerate gradient descent, distributing the gradient computations, but this, as well as approaches based on ADMM (Boyd et al., 2011), are dominated by minibatch SGD (Shamir and Srebro 2014 and see also Table 1).",
      "startOffset" : 147,
      "endOffset" : 166
    }, {
      "referenceID" : 22,
      "context" : "Better alternatives take advantage of the stochastic nature of the problem: DANE (Shamir et al., 2014) requires only O(B2m) rounds of communication for squared loss problems, while DiSCO (Zhang and Lin, 2015) and AIDE (Reddi et al.",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 17,
      "context" : ", 2014) requires only O(B2m) rounds of communication for squared loss problems, while DiSCO (Zhang and Lin, 2015) and AIDE (Reddi et al., 2016)) reduce this further to O(B1/2m1/4) rounds of communication.",
      "startOffset" : 123,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "One simple approach for distributed stochastic optimization is minibatch SGD (Cotter et al., 2011; Dekel et al., 2012), where in each update we use a gradient estimate based on mb examples: b examples from each of the m machines. Distributed minibatch SGD attains optimal statistical performance with O (n(ε)/m) runtime, as long as the minibatch size is not too large: Dekel et al. (2012) showed that the minibatch size can be as large as bm = O( √ n(ε)), and Cotter et al.",
      "startOffset" : 78,
      "endOffset" : 389
    }, {
      "referenceID" : 3,
      "context" : "One simple approach for distributed stochastic optimization is minibatch SGD (Cotter et al., 2011; Dekel et al., 2012), where in each update we use a gradient estimate based on mb examples: b examples from each of the m machines. Distributed minibatch SGD attains optimal statistical performance with O (n(ε)/m) runtime, as long as the minibatch size is not too large: Dekel et al. (2012) showed that the minibatch size can be as large as bm = O( √ n(ε)), and Cotter et al. (2011) showed that with acceleration this can be increased to bm = O(n(ε)3/4).",
      "startOffset" : 78,
      "endOffset" : 481
    }, {
      "referenceID" : 5,
      "context" : "This can be viewed as a minibatch generalization to the passive-aggressive algorithm (Crammer et al., 2006) and has been considered in various contexts (Kulis and Bartlett, 2010; Toulis and Airoldi, 2014).",
      "startOffset" : 85,
      "endOffset" : 107
    }, {
      "referenceID" : 10,
      "context" : ", 2006) and has been considered in various contexts (Kulis and Bartlett, 2010; Toulis and Airoldi, 2014).",
      "startOffset" : 52,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "This can be viewed as a minibatch generalization to the passive-aggressive algorithm (Crammer et al., 2006) and has been considered in various contexts (Kulis and Bartlett, 2010; Toulis and Airoldi, 2014). We show that such an approach achieves the optimal statistical rate in terms of the number of samples used independent of the number of iterations, i.e. with anyminibatch size. This significantly improves over the previous analysis of Li et al. (2014), as the guarantee is better, it entirely avoid the dependence on the minibatch size and does not rely on additional assumptions as in Li et al.",
      "startOffset" : 86,
      "endOffset" : 458
    }, {
      "referenceID" : 5,
      "context" : "This can be viewed as a minibatch generalization to the passive-aggressive algorithm (Crammer et al., 2006) and has been considered in various contexts (Kulis and Bartlett, 2010; Toulis and Airoldi, 2014). We show that such an approach achieves the optimal statistical rate in terms of the number of samples used independent of the number of iterations, i.e. with anyminibatch size. This significantly improves over the previous analysis of Li et al. (2014), as the guarantee is better, it entirely avoid the dependence on the minibatch size and does not rely on additional assumptions as in Li et al. (2014). The guarantee holds for any Lipschitz (even non-smooth) objective.",
      "startOffset" : 86,
      "endOffset" : 609
    }, {
      "referenceID" : 12,
      "context" : "Distributed SVRG for stochastic convex optimization Recently, Lee et al. (2015) suggested using fast randomized optimization algorithms for finite-sums, and in particular the SVRG algorithm, as a distributed optimization approach for (2).",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "Distributed SVRG for stochastic convex optimization Recently, Lee et al. (2015) suggested using fast randomized optimization algorithms for finite-sums, and in particular the SVRG algorithm, as a distributed optimization approach for (2). The authors noted that, for SVRG, when the the sample size n(ε) dominates the problem’s condition number β/ν where β is the smoothness parameter of l(w, ξ), the time complexity is dominated by computing the batch gradients. This operation can be trivially parallelized. The stochastic updates, on the other hand, can be implemented on a single machine while the other machines wait, with the only caveat being that only sampling-without-replacement can be implemented this way. The use of without-replacement sampling was theoretically justified in a recent analysis by Shamir (2016).",
      "startOffset" : 62,
      "endOffset" : 823
    }, {
      "referenceID" : 3,
      "context" : "More general loss functions, still for “batch sizes” of one, were also analyzed in the online learning setting (Cheng et al., 2006; Kulis and Bartlett, 2010).",
      "startOffset" : 111,
      "endOffset" : 157
    }, {
      "referenceID" : 10,
      "context" : "More general loss functions, still for “batch sizes” of one, were also analyzed in the online learning setting (Cheng et al., 2006; Kulis and Bartlett, 2010).",
      "startOffset" : 111,
      "endOffset" : 157
    }, {
      "referenceID" : 2,
      "context" : "Crammer et al. (2006) proposed the “passive aggressive” update rule, where a margin-based loss from a single example with a quadratic penalty is minimized—this corresponds to (3) with a “batch size” of one.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "For finite-sum objectives, methods based on incremental/stochastic proximal updates were studied by Bertsekas (2011, 2015); Defazio (2016). Needell and Tropp (2014) analyzed a randomized block Kaczmarz method in the context of solving linear systems, which also minimizes the empirical loss on a randomly sampled minibatch.",
      "startOffset" : 100,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "For finite-sum objectives, methods based on incremental/stochastic proximal updates were studied by Bertsekas (2011, 2015); Defazio (2016). Needell and Tropp (2014) analyzed a randomized block Kaczmarz method in the context of solving linear systems, which also minimizes the empirical loss on a randomly sampled minibatch.",
      "startOffset" : 100,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "For finite-sum objectives, methods based on incremental/stochastic proximal updates were studied by Bertsekas (2011, 2015); Defazio (2016). Needell and Tropp (2014) analyzed a randomized block Kaczmarz method in the context of solving linear systems, which also minimizes the empirical loss on a randomly sampled minibatch. To the best of our knowledge, no prior work has analyzed the general minibatch variant of proximal updates for stochastic optimization except Li et al. (2014). However, the analysis of Li et al.",
      "startOffset" : 100,
      "endOffset" : 483
    }, {
      "referenceID" : 0,
      "context" : "For finite-sum objectives, methods based on incremental/stochastic proximal updates were studied by Bertsekas (2011, 2015); Defazio (2016). Needell and Tropp (2014) analyzed a randomized block Kaczmarz method in the context of solving linear systems, which also minimizes the empirical loss on a randomly sampled minibatch. To the best of our knowledge, no prior work has analyzed the general minibatch variant of proximal updates for stochastic optimization except Li et al. (2014). However, the analysis of Li et al. (2014) assumes a stringent condition which is hard to verify (and is often violated) in practice, which we will discuss in detail in this section.",
      "startOffset" : 100,
      "endOffset" : 526
    }, {
      "referenceID" : 0,
      "context" : "For finite-sum objectives, methods based on incremental/stochastic proximal updates were studied by Bertsekas (2011, 2015); Defazio (2016). Needell and Tropp (2014) analyzed a randomized block Kaczmarz method in the context of solving linear systems, which also minimizes the empirical loss on a randomly sampled minibatch. To the best of our knowledge, no prior work has analyzed the general minibatch variant of proximal updates for stochastic optimization except Li et al. (2014). However, the analysis of Li et al. (2014) assumes a stringent condition which is hard to verify (and is often violated) in practice, which we will discuss in detail in this section. The following lemma provides the basic property of the update at each iteration. Lemma 1 For any w ∈ Ω, we have λ+ γt γt ‖wt −w‖ ≤ ‖wt−1 −w‖ − ‖wt−1 −wt‖ − 2 γt (φIt(wt)− φIt(w)) . (6) To derive the convergence guarantee, we need to relate φIt(wt) to φ(w). The analysis of Li et al. (2014) for minibatch-prox made the assumption that for all t ≥ 1: EIt [Dφ(wt;wt−1)] ≤ EIt [ DφIt (wt;wt−1) ] + γt 2 ‖wt −wt−1‖ , (7)",
      "startOffset" : 100,
      "endOffset" : 956
    }, {
      "referenceID" : 13,
      "context" : "However, to obtain the optimal convergence rate, Li et al. (2014) needed to set γt = O( √ T/b) which would imply b = O(T ) in order to have γt ≥ β.",
      "startOffset" : 49,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "However, to obtain the optimal convergence rate, Li et al. (2014) needed to set γt = O( √ T/b) which would imply b = O(T ) in order to have γt ≥ β. In view of this implicit constraint that the minibatch size b can not be too large, the analysis of Li et al. (2014) does not really show advantage of minibatch-prox over minibatch SGD, whose optimal minibatch size is precisely b = O(T ).",
      "startOffset" : 49,
      "endOffset" : 265
    }, {
      "referenceID" : 19,
      "context" : "Using a stability argument (Shalev-Shwartz et al., 2009), we can establish the “generalization” performance for the (inexact) minimizer of the minibatch objective.",
      "startOffset" : 27,
      "endOffset" : 56
    }, {
      "referenceID" : 13,
      "context" : "In Li et al. (2014), the authors proposed a simple algorithm EMSO to approximately solve (12), where each machine first solve its own local objective, i.",
      "startOffset" : 3,
      "endOffset" : 20
    }, {
      "referenceID" : 12,
      "context" : "Here we instead use the distributed SVRG (DSVRG) algorithm (Lee et al., 2015; Shamir, 2016) to approximately solve (12), as DSVRG enjoys excellent communication and computation cost when the problem is well conditioned (cf.",
      "startOffset" : 59,
      "endOffset" : 91
    }, {
      "referenceID" : 20,
      "context" : "Here we instead use the distributed SVRG (DSVRG) algorithm (Lee et al., 2015; Shamir, 2016) to approximately solve (12), as DSVRG enjoys excellent communication and computation cost when the problem is well conditioned (cf.",
      "startOffset" : 59,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "Although this approach was shown to work well empirically, no convergence guarantee for the original stochastic objective (1) was provided by Li et al. (2014). Here we instead use the distributed SVRG (DSVRG) algorithm (Lee et al.",
      "startOffset" : 142,
      "endOffset" : 159
    }, {
      "referenceID" : 4,
      "context" : "Figure 2: Illustration of theoretical guarantees for MP-DSVRG and the comparison with accelerated minibatch SGD (Cotter et al., 2011), DiSCO (Zhang and Lin, 2015), AIDE (Reddi et al.",
      "startOffset" : 112,
      "endOffset" : 133
    }, {
      "referenceID" : 17,
      "context" : ", 2011), DiSCO (Zhang and Lin, 2015), AIDE (Reddi et al., 2016), DSVRG (Lee et al.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : ", 2016), DSVRG (Lee et al., 2015), and MP-DANE (proposed and analyzed in Appendix D).",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "This choice is inspired by the stepsize rule of Lacoste-Julien et al. (2012) for stochastic gradient descent.",
      "startOffset" : 48,
      "endOffset" : 77
    }, {
      "referenceID" : 18,
      "context" : "To resolve the recursion, we need the following lemma by Schmidt et al. (2011).",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "Following Cotter et al. (2011), we assume that l(w, ξ) is β-smooth: ∥∇l(w, ξ)−∇l(w′, ξ) ∥∥ ≤ β ∥w −w′ ∥∥ , ∀w,w′ ∈ Ω.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 4,
      "context" : "Proof Our proof closely follows that of Cotter et al. (2011). Due to the smoothness of φ, we have that φ(w̃t) ≤ φ(w̃t−1) + 〈∇φ(w̃t−1), w̃t − w̃t−1〉+ β 2 ‖w̃t − w̃t−1‖ ≤ φ(w̃t−1) + 〈∇φ(w̃t−1)−∇φIt(w̃t−1), w̃t − w̃t−1〉+ β 2 ‖w̃t − w̃t−1‖ + 〈∇φIt(w̃t−1), w̃t − w̃t−1〉 = φ(w̃t−1) + ‖∇φ(w̃t−1)−∇φIt(w̃t−1)‖ · ‖w̃t − w̃t−1‖+ β 2 ‖w̃t − w̃t−1‖ + 〈∇φIt(w̃t−1), w̃t − w̃t−1〉 ≤ φ(w̃t−1) + 1 2(γt − β) ‖∇φ(w̃t−1)−∇φIt(w̃t−1)‖ + γt − β 2 ‖w̃t − w̃t−1‖",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 22,
      "context" : "Here we present a novel method that use the distributed optimization algorithm DANE (Shamir et al., 2014) and its accelerated variant AIDE (Reddi et al.",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : ", 2014) and its accelerated variant AIDE (Reddi et al., 2016) for solving (12), which define better local objectives than EMSO and take into consideration the similarity between local objectives.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 14,
      "context" : "On top of that, AIDE uses the idea of universal catalyst (Lin et al., 2015) and adds an extra quadratic term to improve the strong-convexity of the objective for faster convergence, i.",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "Second, we only approximately solve the local subproblems (33) to sufficient accuracy in each inner loop; the analysis of “inexact DANE” (for the non-stochastic setting) provides guarantee for this approach (Reddi et al., 2016), and enables us to use state-of-the-art SGD methods (e.",
      "startOffset" : 207,
      "endOffset" : 227
    }, {
      "referenceID" : 22,
      "context" : "Analysis of MP-DANE In order to fully analyze Algorithm 2, we need several auxiliary lemmas that characterize the iteration complexity of solving the local problem (33) by prox-SVRG (Xiao and Zhang, 2014), the large minibatch problem (12) by DANE (Shamir et al., 2014) and AIDE (Reddi et al.",
      "startOffset" : 247,
      "endOffset" : 268
    }, {
      "referenceID" : 17,
      "context" : ", 2014) and AIDE (Reddi et al., 2016).",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 7,
      "context" : "The benefit of this approach (as opposed to using plain SVRG Johnson and Zhang, 2013) is that the smoothness parameter that determines the iteration complexity is simply β, same results hold when applying prox-SAGA (Defazio et al., 2014) as well.",
      "startOffset" : 215,
      "endOffset" : 237
    }, {
      "referenceID" : 20,
      "context" : "For sampling without replacement SVRG, the current analysis works only for plain SVRG, so we quote the results from (Shamir, 2016).",
      "startOffset" : 116,
      "endOffset" : 130
    }, {
      "referenceID" : 17,
      "context" : "Next, we state the convergence rates of “inexact DANE” and AIDE, which can be easily derived from Reddi et al. (2016). At the outer loop t and intermediate loop r, let xr = argminw f̄t,r(w) be the exact minimizer of the “augmented large minibatch” problem (36), which is approximately solved by the inner DANE iterations.",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "1 of Lin et al. (2015)) Assume that for all r ≥ 1, we have f̄t,r(xr)− f̄t,r(xr) ≤ 2 9 ( 1− 9 10 √ γ γ + κ )R · ( f̃t(x0)− f̃t(w t ) ) ,",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "For MP-DANE, we use SAGA (Defazio et al., 2014) to solve each local DANE subproblem (33) and fix the number of SAGA steps to b (i.",
      "startOffset" : 25,
      "endOffset" : 47
    } ],
    "year" : 2017,
    "abstractText" : "We present and analyze an approach for distributed stochastic optimization which is statistically optimal and achieves near-linear speedups (up to logarithmic factors). Our approach allows a communication-memory tradeoff, with either logarithmic communication but linear memory, or polynomial communication and a corresponding polynomial reduction in required memory. This communication-memory tradeoff is achieved throughminibatch-prox iterations (minibatch passiveaggressive updates), where a subproblem on a minibatch is solved at each iteration. We provide a novel analysis for such a minibatch-prox procedure which achieves the statistical optimal rate regardless of minibatch size and smoothness, thus significantly improving on prior work.",
    "creator" : "LaTeX with hyperref package"
  }
}