{
  "name" : "1402.2331.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Computational Limits for Matrix Completion",
    "authors" : [ "Moritz Hardt", "Raghu Meka", "Prasad Raghavendra", "Benjamin Weitz" ],
    "emails" : [ "mhardt@us.ibm.com.", "meka@microsoft.com.", "prasad@cs.berkeley.edu.", "bsweitz@eecs.berkeley.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "It is well known that Matrix Completion in its full generality is NP-hard. However, little is known if make additional assumptions such as incoherence and permit the algorithm to output a matrix of slightly higher rank. In this paper we prove that Matrix Completion remains computationally intractable even if the unknown matrix has rank 4 but we are allowed to output any constant rank matrix, and even if additionally we assume that the unknown matrix is incoherent and are shown 90% of the entries. This result relies on the conjectured hardness of the 4-Coloring problem. We also consider the positive semidefinite Matrix Completion problem. Here we show a similar hardness result under the standard assumption that P ,NP.\nOur results greatly narrow the gap between existing feasibility results and computational lower bounds. In particular, we believe that our results give the first complexity-theoretic justification for why distributional assumptions are needed beyond the incoherence assumption in order to obtain positive results. On the technical side, we contribute several new ideas on how to encode hard combinatorial problems in low-rank optimization problems. We hope that these techniques will be helpful in further understanding the computational limits of Matrix Completion and related problems.\n∗IBM Research Almaden. Email: mhardt@us.ibm.com. †Microsoft Research. Email: meka@microsoft.com. ‡University of California, Berkeley. Email: prasad@cs.berkeley.edu. Supported by NSF Career Award and the\nSloan Fellowship. §University of California, Berkeley. Email: bsweitz@eecs.berkeley.edu. Supported by the NSF GRFP.\nar X\niv :1\n40 2.\n23 31\nv2 [\ncs .C\nC ]\n1 0\nA pr\n2 01"
    }, {
      "heading" : "1 Introduction",
      "text" : "Suppose we observe a subset of the entries of an unknown low-rank matrix M, can we recover the matrix M knowing this subset alone? This problem, called Matrix Completion, is of fundamental interest in a number of fields including statistics, machine learning, signal processing and theoretical computer science. It is widely applicable to the design of recommender systems as popularized by the famous Netflix Prize. We are interested in understanding the compuational complexity of Matrix Completion.\nMuch of the theory of Matrix Completion revolves around a beautiful line of positive results. These results show that under certain assumptions there is a natural semidefinite relaxation that solves the problem efficiently even if the number of visible entries is asymptotically much smaller than the total number of entries [CR09, CT10, Rec11]. Specifically, these feasibility assumptions state that:\nLow rank. M has rank k where k is typically constant or very slowly growing.\nIncoherence. The row and columns spaces of M are incoherent. Informally, a subspace U of Rn is incoherent if for every standard basis vector ei ∈ Rn, the Euclidean norm of the projected vector PU ei is much smaller than 1. Here, PU denotes the orthogonal projection onto the space U.\nRandomness. Finally, the subset of entries is drawn uniformly at random from M with a certain sufficient sampling density p.\nAmong these assumptions the last one is particularly taxing. In most applications, the algorithm designer cannot choose the subset of revealed entries. Instead nature determines the subsample, e.g., available user/movie ratings on Netflix. Often it is argued that without the randomness assumption, the solution to the problem may no longer be uniquely determined. But rather than insisting on uniqueness of the solution, it is natural to only require consistency with the given subset. That is, we only require the solution to agree with the observed entries. There could be multiple valid solutions. Moreover, algorithmically two additional relaxations are natural. First, we can attempt to make the problem easier by allowing some slack in terms of the rank r > k of the solution. Second, we can allow an approximation error on the observed entries. That is, rather than matching the observable entries exactly we allow the algorithm to find a solution that is close in Frobenius norm.\nSurprisingly, even with these relaxations the status of some deceptively simple algorithmic questions remained wide open. For example:\nQuestion 1.1. Given entries of an incoherent rank 4 matrix, can we find a rank 100 matrix that is approximately consistent with this set of entries in polynomial time?\nEven though the problem might appear to be very simple, neither upper bounds nor lower bounds are known. Matrix Completion in its full generality is of course NP-hard, but no hardness result is known for the problem we just described. In fact, for small k, the main prior hardness result we are aware of is due to Peeters [Pee96] who showed that given a subset of a rank 3 matrix it is NP-hard to find an exactly consistent matrix of equal rank.1 However, Peeters’ hardness result does not apply to the various relaxations of interest.\n1Several results are known over finite fields, but Matrix Completion over the reals is of particular interest in applications.\nThe lack of applicable hardness results for Matrix Completion is partially due to the nature of the problem. Low-rank decompositions over the reals do not seem to exhibit the same combinatorial rigidity common to most NP-hard optimization problems. This conundrum arises in a number of interesting machine learning problems such as Sparse PCA and Robust PCA. Indeed, only recently did Berthet and Rigollet give evidence for computational hardness of the Sparse PCA problem by reducing to the Planted Clique problem in a natural setting [BR13]. For Robust PCA, the hardness result of Hardt and Moitra [HM13] appeals to the conjectured hardness of Small Set Expansion.\nOur goal is to make progress on understanding the computational complexity of Matrix Completion in the natural relaxed setting that we described above. We show that under a plausible hardness assumption, there is in fact no polynomial time algorithm that solves the task. An immediate corollary is that even if we adopt the first two feasibility assumptions, some distributional assumption on the revealed entries is necessary in order to make Matrix Completion tractable.\nWe also consider a natural variant of Matrix Completion where the unknown matrix is positive semidefinite and so must be the output matrix. The positive semidefinite completion problem arises naturally in the context of Support Vector Machines (SVM). The kernel matrix used in SVM learning must be positive semidefinite as it is the Gram matrix of feature vectors. But oftentimes the kernel matrix is derived from partial similarity information resulting in incomplete kernel matrices. In fact, this is a typical situation in medical and biological application domains [TAA03]. In such cases the data analyst would like to complete the partial kernel matrix to a full kernel matrix while ensuring positive semidefiniteness. Moreover, since it is often infeasible to store a dense n×n matrix, it is desirable to also have a low-rank representation of the kernel matrix [FS02]. This is precisely the low-rank positive semidefinite completion problem. Our results establish strong hardness results for this problem under natural relexations. In this case we show that for any k > 2 it is NP-hard to complete a partially given rank k matrix by a rank (2k − 1) matrix."
    }, {
      "heading" : "1.1 Our Results",
      "text" : "We will restrict our attention to symmetric n × n matrices throughout. As we are proving hardness results, this only makes the results stronger. We begin with a formal definition of the Matrix Completion problem. Here, we restrict our attention to the case where both input and output have bounded coefficients as is the case in most application settings.\nDefinition 1.2. We define the (k, r,p,ε,c)-Completion problem as follows:\nInput: A matrix A ∈ (R∪{⊥})n×n and a set Ω ⊆ [n]× [n] of size |Ω| > pn2 such that there exists a rank k matrixM with bounded entries |M(i, j)| 6 c for all i and j, such that for all (i, j) ∈Ω we have A(i, j) =M(i, j) and for all (i, j) <Ω we have A(i, j) =⊥.\nOutput: A rank r matrix B with bounded coefficients |B(i, j)| 6 c for all i and j, such that B approximatesAwith small root-mean-squared error (RMSE): ∑ (i,j)∈Ω |A(i, j)−B(i, j)|2 6 εn.\nWe will use (k, r,p,c)-Completion as a shorthand for (k, r,p,0, c)-Completion, i.e. exact completion. We also use (k, r, c)-Completion as a shorthand for (k, r,0,0, c)-Completion.\nTo state our first result we introduce the problem of coloring a k-colorable graph with r colors.\nDefinition 1.3. We define the (k, r)-Coloring problem as follows:\nInput: A k-colorable graph G.\nOutput: An r-coloring of the graph G.\nOur second theorem will appeal to a closely related variant of the problem in which the output is an independent set of size n/r rather than an r-coloring.\nDefinition 1.4. We define the (k, r)-Indepedent-Set problem as follows:\nInput: A k colorable graph G.\nOutput: An independent set of size n/r in the graph G.\nNotice that if there exists an r-coloring of the graph then one of the color classes will be an independent set of size n/r. Thus, (k, r)-Indepedent-Set reduces to (k, r)-Coloring. Despite extensive work on algorithms for k-Coloring [Wig82, BR90, BK97, KMS98, ACC06], the problem has remained notoriously hard. Given a 3-colorable graph, the best algorithms [Chl07, iKT14] known can only find an independent set of size at most n1−Ω(1). In particular, (k, r)-Coloring and (k, r)-Indepedent-Set with k = 4 and r = O(1) remains hopelessly out of reach of existing algorithmic techniques. From a complexity standpoint it is believed that the (k, r)-Indepedent-Set problem (and hence (k, r)-Coloring) cannot be solved in polynomial time for even k = 4 and r =O(1). This is further supported by the work of [DS10] who show this to be the case under a variant of the Unique Games Conjecture (called 2-to-1 Label Cover) which by now underlies a number of hardness results in complexity theory.\nWe will show that assuming (k, r)-Coloring and (k, r)-Indepedent-Set are hard for k = 4, r =O(1), the Matrix Completion problem is hard in a range of natural parameters even on incoherent matrices and even if most entries are revealed. To make the theorems precise we state the assumption concretely and give a formal definition of incoherence.\nConjecture 1.5. The (k, r)-Coloring problem is not in P for any r > k > 3 and r =O(1).\nConjecture 1.6. The (k, r)-Indepedent-Set problem is not in P for any r > k > 3 and r =O(1).\nThe coherence of a matrix is defined as follows.\nDefinition 1.7. A symmetric n×n matrix M of rank k has coherence µ if there exists a singular value decomposition M = UΣV > such that for every standard basis vectors ei ∈ Rn we have that ‖e>i U‖2 6 √ kµ/n and ‖e>i V ‖2 6 √ kµ/n.\nNote that Conjecture 1.5 is weaker than Conjecture 1.6. With the above definitions we have the following results.\nTheorem 1.8. Assume Conjecture 1.5. Then, for any constants c > 1, k > 3 and r > k, there is no polynomial time algorithm that solves the (k, r,0.9, c)-Completion problem on matrices of coherence µ 6 O(1). Further, for all 1/2 > ε > 0, the same conclusion holds even if we are only required to compute a rank r matrix which approximates each entry with additive error at most ε.\nIn most practical scenarios it suffices to look for a low-rank completion with small RMSE error. Our next result addresses this situation.\nTheorem 1.9. Assume Conjecture 1.6. Then, for any constants k > 4, r > k, and 0 < ε < 1/(2cr)2, there is no polynomial time algorithm that solves the (k, r,0.9, ε, c)-Completion problem on matrices of coherence µ 6O(1).\nThis result should be contrasted with positive results showing that (k,k,O(kµ(log2n))/n)Completion is easy so long as the entries are revealed randomly [Rec11].\nPositive Semidefinite Completions. We define the (k, r,p)-PSD-Completion problem the same way we defined (k, r,p)-Completion except that we drop the bound on the coefficients and additionally require that both M and B must be positive semidefinite. Our result here is incomparable to the previous one and it relies on the standard NP-hardness assumption.\nTheorem 1.10. Assume that P ,NP. Then for every even k > 2 there is no polynomial time algorithm that solves the (k,2k − 1,0.9)-PSD-Completion problem.\nThis theorem strengthens a recent result by E.-Nagy et al. [ENLV13] who showed that (k,k,0)-PSD-Completion is NP-hard for every k > 2.\nWe also prove a version of Theorem 1.10 for approximate completion:\nTheorem 1.11. Assume that P , NP. Then for every even k > 6 and ε < O(k−5), there is no polynomial time algorithm that solves the (k,2k − 1,0.9)-PSD-Completion problem\neven if the output matrix only approximates each entry with additive error at most ε."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We are very grateful to Phil Long for insightful early contributions to this work. In fact, he first conjectured that (k, r)-Completion should be as hard as (k, r)-Coloring as established by Theorem 1.8. The authors also thank the Simons Institute for the Theory of Computing at Berkeley for its hospitality."
    }, {
      "heading" : "1.2 Further Related Work",
      "text" : "There have been several hardness results for Matrix Completion over finite fields drawing on its connection to problems in coding theory. See, for example, the discussion in [HKY06, TBD12]. The Matrix Completion problem over the reals seems to behave rather differently and techniques do not seem to transfer from the finite field case.\nPSD completions are also natural objects in discrete optimization and the study of the geometry of graphs. We refer the reader to the recent work of E.-Nagy, Laurent and Varvitsiotis [ENLV13] for a more extensive discussion of related work in this area."
    }, {
      "heading" : "1.3 Proof Overview",
      "text" : "We now give a highlevel outline of our proofs."
    }, {
      "heading" : "1.3.1 Matrix Completion",
      "text" : "While the hardness assumption in our reduction (Conjecture 1.5) is similar in spirit to that of Peeters’ original reduction, our proof works in a very different manner.\nLet G = (V ,E) be a graph with |V | = n and |E| =m. Now define the n× n partial matrix PG such that PG(i, i) = 1 for every i ∈ [n], and PG(i, j) = 0 if (i, j) ∈ E. The intuition behind this reduction is that, if G is k-colorable with coloring function f : V → [k], then\nMf = ∑ i∈[k] 1f −1(i)1 T f −1(i)\nis a rank-k completion of PG. Peeters [Pee96] showed how to gadgetize a graph G so that these were the only rank-k completions of PG. However, the gadgets in that work are unable to force any structure on completions of rank higher than k. To decode colorings from high rank completions, we will consider a special factorization of the completion. The row vectors of this factorization will have bounded norm, which will allow us to cover them with a constant number of small balls in Rr . If the balls are small enough, then any two vectors that lie in the same ball cannot have zero dot product, so their corresponding vertices cannot have an edge in G. We then use the balls to color the vertices.\nThe above argument works when we look at exact completions (or those with entry-wise error bounds). To obtain our main result, Theorem 1.9, we focus on more general structure of any low-rank completion, in this case the existence of large non-zero rectangles. We will prove, under some mild assumptions on any approximate (in RMSE) completion M of PG, that M has a large non-zero square, which corresponds to an independent set in the graph G."
    }, {
      "heading" : "1.3.2 Positive Semidefinite Matrix Completion",
      "text" : "We give two reductions for the (k, r)-PSD-Completion problem: one from the Partition problem and one from a constraint satisfaction problem Exact-one-in-k-SAT. The first reduction has the advantage of being simple but only works for exact completion. Our second reduction is more involved but is much more robust and it works even when we allow for errors and gives us the theorem on approximate completions from the introduction.\nConsider an instance of the (k, r)-PSD-Completion problem with input A ∈ (R∪ {⊥})n×n. Our goal is to find a PSD matrix B which agrees with A on the set of non-⊥ entries. Now, recall that a characterization of PSD matrices is that a n×n matrix B is PSD if and only if it can be factored as B =UU> for some matrix U . If we let u1, . . . ,un be the rows of the matrix U , then we have Bij = 〈ui ,uj〉. The vectors u1, . . . ,un are called the Gram vectors of B. In the context of PSD-Completion, the revealed entries of A place equality constraints on the inner products of the Gram vectors:\n〈ui ,uj〉 = Aij , if Aij ,⊥.\nMoreover, these constraints completely characterize the problem and finding a rank r solution for the completion problem is equivalent to finding a set of vectors u1, . . . ,un ∈Rr satisfying the above constraints. We will adopt this perspective in our reductions and view the partial matrix as a list of such inner-product constraints.\nWe design constraints to simulate ±1 variables which we can then use as gadgets to reduce from many different problems. For the Partition problem, we follow an idea proposed in [ENLV13] and associate every item in the partition with a two-dimensional basis, and constrain that the (i + 1)th basis is a θ-rotation of the ith basis (including the first and nth bases), where θ depends on the element ai in the Partition problem. This creates a cyclic dependence on the rotations of the bases that forces the total sum of the rotations to be an integer multiple of 2π. However, the important things to note are that these rotations can be in one of two\ndirection: clockwise or counter-clockwise, and if the angles are small enough then the sum of the rotations must be zero. Thus we find a partition based on which rotations went clockwise and which went counter-clockwise. By constraining sums of basis vectors in addition to the basis vectors themselves, we can force the same rotational structure in three dimensions as in two, yielding the gap.\nFor the Exact-one-in-k-SAT problem, we similarly associate every variable with a basis and use the inner product constraints to force these bases to be special rotations of a reference gadget. We interpret the variable as being +1 or −1 depending on if the rotation is a \"clockwise\" or \"counter-clockwise\" rotation. Because the relations of Exact-one-in-k-SAT are linear, i.e. the sum of the values of the variables in each clause is exactly (k − 2), it is easy to force satisfying assignments. See Section 3.2 for a more thorough description and the appendix for the full details."
    }, {
      "heading" : "2 Hardness for Matrix Completion",
      "text" : "In this section we show that the matrix completion problem is hard even with relaxed rank constraints and allowing for approximate completions. In particular, we will give a reduction to prove Theorem 1.9. We defer the proof of Theorem 1.8 to the appendix.\nLet G = (V ,E) be a graph with |V | = n and |E| = m. Now define the partial matrix PG ∈ (R∪⊥)n×n as follows:\nPG(i, j) =  1 if i = j 0 if (i, j) ∈ E ⊥ otherwise .\nAs described in the introduction, if G is k-colorable with coloring function f : V → [k], then Mf = ∑ i∈[k] 1f −1(i)1 T f −1(i)\nis a rank-k completion of PG. Note that Mf has coherence µ = n k (mini |f −1(i)|)−1. However, we may assume that there is a perfectly balanced coloring of G, for example by copying the graph k times. Thus we can take Mf to have coherence exactly µ = 1. We next prove that under some mild additional assumptions any approximate low-rank completion M of PG yields a large independent set of G.\nOur technique relies on a lemma in [LMSS07] that guarantees the existence of a good factorization of low rank matrices, in the sense that the norms of the row and column vectors of the factorization are small. We state the lemma here for completeness:\nLemma 2.1. Let M be a matrix with rank r. Then there exists an r-dimensional factorization M = XY T such that every row vector of X and Y has norm at most (cr)1/4, where c = maxij |M(i, j)|.\nThe proof of this statement in its original paper was given in a nonconstructive manner using John’s theorem from convex analysis, but such a good factorization can be found in polynomial time using semidefinite programming. See the appendix for the details.\nLemma 2.2. Let G = (V ,E) be a graph and define PG as above with Ω ⊆ [n]× [n] the set of revealed entries. Let M be a rank r matrix such that∑\n(i,j)∈Ω (M(i, j)− PG(i, j))2 6 εn\nand |M(i, j)| 6 c. Then G has an independent set T of size at least\n|T | > (1− 4(cr) 2ε)n\nr √ π(8 √ cr)r .\nMoreover, there is a randomized polynomial time algorithm for finding such an independent set given M.\nProof. By a simple averaging argument, there can be only εδ2n entries of M that are different from PG by more than δ. Thus there are at least (1 − ε/δ2)n rows and columns such that |M(i, j) − PG(i, j)| 6 δ for any row i and column j. Let M ′ be this submatrix of M. Certainly rank(M ′) 6 rank(M) = r, so Lemma 2.1 tells us we can find a factorization XY T =M with row vectors ui and vi such that ‖ui‖,‖vj‖ 6 (cr)1/4 for all i, j ∈ [n]. Let θ(ui ,vj) denote the angle between vectors ui and vj . Since ui · vi > 1− δ for all i ∈ [n], we derive\ncos(θ(ui ,vi)) > (1− δ) √ cr and ‖u′i‖,‖v ′ i‖ > (1− δ)(cr) −1/4.\nNow in order to find an independent set, we have to look for entries with M(i, j) > δ to be assured that indeed (i, j) < E. From the bound on the norms of ui and vj , if cosθ(ui ,vj ) > δ √ cr\n(1−δ)2 , then M(i, j) > δ. In order to capture these points, we will pick the points in a random cone. Let φ denote the angle such that cosφ = δ √ cr\n(1−δ)2 . Our random procedure to find T is\n– Normalize ũi = ui/‖ui‖ and ṽi = vi/‖vi‖.\n– Pick a random unit vector x ∈Rr .\n– For every i ∈ S, if ũi · x > cos(φ/2) and ṽi · x > cos(φ/2) then put i ∈ T .\nFor i, j ∈ T , since θ(ũi ,x) < φ/2 and θ(ṽi ,x) < φ/2, by triangle inequality, θ(ũi , ṽi) < φ. As noted above, since cosθ(ũi , ṽi) > cosφ, we get ui · vj > δ, thus M(i, j) > δ and so PG(i, j) > 0. We bound |T | by checking the probability that i is placed in T . For each i, let wi be the angle bisector of ũi and ṽi and define\nAi = { x : ‖x‖ = 1,θ(x,wi) <\n1 2\n(φ−θ(ũi , ṽi)) } .\nWe will show that if x ∈ Ai was chosen as our random vector, then i ∈ T . This implies that the probability that i ∈ T is at least area(Ai)/area(Sr−1). For Ai to have positive area, we need δ small enough that φ > θ(ũi , ṽi). To this end, pick δ = 1/2cr. It is a standard argument that the area of Ai is bounded below by the (r − 1)-volume of a sphere with radius\nbi = sin ( φ−θ(ũi , ṽi)\n2 ) Now noting that cosφ = δ √ cr/(1− δ)2 and cosθi > (1− δ)/ √ cr and using a Taylor Series approximation\nsin 1 2\n( cos−1 ( x2/2\nx(1− x2/2)2\n) − cos−1 ( x(1− x2/2) )) > x 4 −O(x3) > x 8\nwhere x = 1/ √ cr and the last inequality follows as long as √ cr > 1. Now\narea(Ai) area(Sr−1) > br−1i r √ π ,\nand thus i ∈ T with probability at least 1/r √ π(8 √ cr)r . Now using linearity of expectation,\nE[|T |] > (1− ε/δ 2)n\n2r √ π(8 √ cr)r > (1− 4(cr)2ε)n r √ π(8 √ cr)r .\nThe above reduction produces a partial matrix PG that has |V |+ |E| revealed entries, which could be much less than 0.9n2 if the graph G is sparse. However, we can simply pad the matrix PG with zeros, i.e. output the 10|V | × 10|V |matrix\nP ′G = [ PG 0 0 0 ] .\nCombined with Lemma 2.2, the above implies Theorem 1.9."
    }, {
      "heading" : "3 Hardness for Positive Semidefinite Matrix Completion",
      "text" : "To prove hardness for the (k, r,p,ε)-Completion problem we appealed to a conjectured coloring hardness. In this section we show that this assumption can be weakened to the usual NPhardness if the matrices under consideration are positive semi-definite. In particular, in this section we prove Theorem 1.10. We first present the hardness for the exact completion problem with ε = 0 with a reduction from Partition. We then sketch the second reduction from Exactone-in-k-SAT that is capable of handling errors on the constraints and proves Theorem 1.11. The full details on the second reduction are deferred to the appendix."
    }, {
      "heading" : "3.1 Exact Completion",
      "text" : "Our reduction is similar to Theorem 3.3 in [ENLV13], but with extra constraints to retain structure in higher rank completions. We will reduce from the partition problem, i.e. given numbers a1, . . . , an, find a set I ⊆ [n] such that ∑ i∈I ai = ∑ i<I ai . We will reduce this problem to (2,3)-PSD-Completion and amplify the gap. Recall that a partial PSD matrix is equivalent to a list of inner product constraints. Given an instance (a1, . . . , an), we will output a set of constraints on 3n different vectors. These vectors will be indexed by I = [n] × [3]. Assume without loss of generality that ∑ i ai = 1. Now constrain\n– us ·us = 1 for all s ∈ I\n– u(i,1) ·u(i,2) = 0 for all i ∈ [n].\n– u(i,3) ·u(i,1) = u(i,3) ·u(i,2) = 1√2 . Equivalently, u(i,3) = 1√ 2 (u(i,1) +u(i,2)).\n– u(i,1) · u(i+1,1) = u(i,2) · u(i+1,2) = u(i,3) · u(i+1,3) = cosai for all i ∈ [n], where addition is performed modulo n.\nThe intuition here is that in a rank-2 decomposition of this matrix, for every i {u(i,1),u(i,2)} is an orthonormal basis, and the (i + 1)st basis is an angle ai-rotation of the ith basis. This rotation can be in one of two directions, clockwise or counter-clockwise. However since the 1st basis is an an-rotation of the nth basis, after rotating by every ai we must be back where we started. Since ∑ i ai = 1 < 2π, this means that the total rotation must be zero, so we partition the ai based on whether the corresponding rotation was clock-wise or counterclockwise. The additional constraints on the sums of basis vectors will force this structure even in a rank-3 decomposition.\nLemma 3.1. There is a set of vectors {us}s∈I lying in R3 satisfying the above constraints if and only if there is a set of such vectors lying in R2.\nProof. Let {us}s∈I be a set of vectors lying in R3 satisfying the constraints. For each i, there is an orthogonal transformation Qi that maps Qi(u(i,1)) = u(i+1,1), Qi(u(i,2)) = u(i+1,2). Writing Qi in the basis {u(i,1),u(i,2),u(i,1) ×u(i,2)}, and accounting for the constraints of the {us}s∈I , we have\nQi =  cosai x −x cosai A y z  but Qi has orthogonal columns, which implies that yz = 0, so either y = 0 or z = 0. But Qi also has columns of norm 1, so x = ±sinai , which implies that both y and z are zero. Thus\nQi =  cosai ±sinai 0 ∓sinai cosai 0\n0 0 1  . This implies that {u(i,1),u(i,2)} and {u(i+1,1),u(i+1,2) lie in the same plane. Repeating the argument we get that {u(i,1),u(i,2)}i lie in the same plane for every i.\nLemma 3.2. There is a partition of (a1, . . . , an) if and only if there is a set of vectors satisfying the constraints lying in R2.\nProof. First, assume there is a partition (I, I) of [n] and set θk = ∑ i∈I,i<k ai − ∑ i<I,i<k ai\nand by convention θ1 = 0. Note that by the definition of θ and partitions, θn = ±an. Now set u(i,1) = e1 cosθi + e2 sinθi , u(i,2) = e1 cosθi − e2 sinθi , and u(i,3) = 1√2 (u(i,1) + u(i,2)) for every i ∈ [n]. Then u(i,1) ·u(i+1,1) = u(i,2) ·u(i+1,2) = cos(θi −θi+1) = cosai for every i < n. Finally, since θn = ±an and θ1 = 0, u(n,1) and u(n,2) are at an angle an with u(1,1) and u(1,2) respectively, so u(n,1) ·u(1,1) = u(n,2) ·u(n,1) = cosan.\nConversely, suppose there is a set of vectors lying in R2 satisfying the constraints. Since the vectors are unit vectors and u(i,1) ·u(i+1,1) = cosai , we know u(i+1,1) makes an angle ai with u(i,1), so for every i ∈ [n],\nu(i,1) = cos  i−1∑ j=1 sjaj u(1,1) + sin  i−1∑ j=1 sjaj u(1,2)\nwhere s ∈ {+1,−1}n. Finally, since u(n,1) is at an angle an with u(1,1), we have\nu(1,1) = cos  n∑ j=1 sjaj u(1,1) + sin  n∑ j=1 sjaj u(1,2) Since ∑ j aj = 1 < 2π, we must have ∑n j=1 sjaj = 0. Hence the set I = {i : si = +1} yields a solution to the Partition problem on the instance (a1, . . . , an).\nThe NP-hardness of (2,3)-PSD-Completion follows from Lemma 3.1 and Lemma 3.2. There’s a simple amplification one can do to prove hardness for (k,2k − 1)-PSD-Completion for any even k. Simply take the matrix A from the (2,3)-PSD-Completion reduction and output the matrix\nM =  A 0 · · · 0 0 A · · · 0 ... ... . . .\n... 0 0 · · · A  where A appears k/2 times. Then any completion of M of rank at most 2k − 1 will restrict to a completion of A of rank at most 3. We can also pad with additional zeros to boost the number of revealed entries up to 0.9n2. This completes the proof of Theorem 1.10."
    }, {
      "heading" : "3.2 Tolerating Errors",
      "text" : "In this section we sketch the reduction in the proof of Theorem 1.11. Because Partition is only NP-hard in a weak sense (there exists an algorithm which runs in time polynomial in the size of the weights a1, . . . , an), we require a different starting problem to prove hardness while tolerating errors in the constraints.\nTheorem 3.3. For every constant k > 3, constant r with 2k 6 r 6 4k − 1, and ε < O(r−5), there is a reduction from Exact-one-in-k-SAT that, given an instance Φ , outputs a partial matrix PΦ with the following property: If there is a rank r matrix M such that |M(i, j)− PΦ (i, j)| < ε whenever PΦ (i, j) ,⊥, then Φ is satisfiable. Furthermore, if Φ is satisfiable, there is a rank k completion of PΦ .\nThere are two main components to the reduction in Theorem 3.3. The first is the variable gadget, a set of constraints that forces only two configurations for a set of vectors, and we can interpret these configurations as being a +1 or −1 assignment to the variable. The second is the clause gadget, a set of constraints designed to force the interpreted assignment to be satisfying.\nFor each variable in the instance Φ , the variable gadget is a set of constraints that creates a 2k-dimensional orthonormal basis. There is also a special \"reference basis\" that we use as a reference point because inner product constraints are invariant to rotations. For each variable, we constrain its basis to be a special rotation of the reference basis. The rotation is special in the sense that it is a set of identical rotations in k pairs of two-dimensional subspaces. Because a rotation in two dimensions has exactly two configurations, rotate clockwise or rotate counter-clockwise, there are only two possible rotations of the variable’s basis. We interpret each of these rotations as setting the variable to +1 or −1.\nFor each clause in Φ , the clause gadget is a set of constraints that are intended to construct a vector whose ith coordinate is the value (either +1 or −1) of the ith variable appearing in Φ .\nFinally, we constrain that the sum of the elements of Φ is exactly (k −2). This forces exactly one of the coordinates of the vector to be −1, so the variables must be set to a satisfying assignment.\nIt is not obvious how we are able to force such specific structure on the rotations in the variable gadget even when the ambient dimension gets as high as 4k −1. By constraining dot products of sums of basis vectors in the variable gadget we are able to resolve this problem. The full details and proof of Theorem 3.3 are located in the appendix for the interested reader."
    }, {
      "heading" : "4 Conclusion and Open Problems",
      "text" : "Our goal was to narrow the gap between existing positive results on Matrix Completion and computational lower bounds. For a hardness result to be compelling it must account for natural algorithmic relaxations. We showed that several relaxations that are natural from an algorithmic machine learning point of view do not make the problem easier. From a complexity theoretic perspective, these are the first hardness of approximation results for Matrix Completion over the reals. A consequence of our work is that the popular incoherence assumption by itself is not sufficient to make the problem tractable. An interesting question is if conversely the assumption of uniformly random entries by itself already makes the problem easy.\nQuestion 4.1. Is Matrix Completion hard when the observed entries are chosen randomly, but the observed matrix is not incoherent?\nAnother challenging question is to determine the precise hardness threshold for (k, r)completion.\nQuestion 4.2. For any k > 3, what is the largest r > k such that (k, r)-Completion is hard? Can we find a matching algorithm?\nResolving this question will likely require progress on both lower bounds and algorithms. Deceptively simple algorithmic questions are still open such as the following. Question 4.3. We know that (3, √ n)-Coloring is easy [Wig82]. Is (3, √ n)-Completion easy?"
    }, {
      "heading" : "A Finding Good Factorizations of Low Rank Matrices",
      "text" : "In this section we will prove a constructive version of Lemma 2.1, restated below for convenience.\nLemma A.1. (Lemma 2.1 restated) LetM be a matrix with rank r. Then there exists an r-dimensional factorization M = XY T such that every row vector of X and Y has norm at most (cr)1/4, where c = maxij |M(i, j)|.\nIn [LMSS07] this lemma was proven in a non-constructive manner using John’s Theorem from convex analysis. However, as the authors therein note, there is a simple semi-definite program (SDP) one can write to compute a good factorization. Unfortunately, this SDP may give a factorization that is not r-dimensional, but instead up to (m+n+ 1)-dimensional. Here we give a simple self-contained algorithm to compute the decomposition M = XY T .\nProof (of an algorithm to construct decomposition as in Lemma 2.1). Let M be a rank r matrix, and let c = maxij |M(i, j)|. To construct a good r-dimensional factorization of M, we proceed in a few steps:\n1. Compute a factorization M = UV T such that U and V have short row vectors using semi-definite programming. This factorization is not necessarily r-dimensional.\n2. Project every row of V onto the row space of U to get V ′.\n3. Factor V ′ = YB, where B is an orthonormal basis for the row space of V ′.\n4. Output X =UBT and Y . This will be an r-dimensional factorization with short rows.\nFor the first step, consider the following SDP with variables {ui}mi=1, {vj} n j=1 and η.\nMinimize η\nSubject to\nui · vj =M(i, j) ∀i ∈ {1, . . . ,m}j ∈ {1, . . . ,n} ui ·ui 6 η ∀i ∈ {1, . . . ,m} vj · vj 6 η ∀j ∈ {1, . . . ,n}\nThis SDP computes the row vectors of two matrices U and V such that M = UV T and minimizes their row norms. The lemma in [LMSS07] guarantees that there exists a factorization with the norm of every row vector of U and V bounded by (cr)1/4, and thus the factorization found by the SDP is at least this good. However, since there are m+n+ 1 variables, the vectors returned could be up to (m+n+ 1)-dimensional.\nLet V ′ be the matrix whose rows are the rows of V projected onto the row space of U . Note that we still have M =U (V ′)T , and projections can only decrease the lengths of the rows of V . We make the following claim.\nClaim A.2. Rank of V ′ is equal to rank of M, i.e., r.\nProof. Let ru and rv denote the ranks of U and V ′, respectively. Clearly rv > r because V ′ is a factor of M.\nNow pick linearly independent subsets E of ru rows of U and F of rv rows of V ′. Then EFT is a submatrix of M, and thus must have rank at most r. However, we will argue below that the rank of EFT must be equal to rv , which shows that rv 6 r. We already know that rv > r, which implies that rv = r as desired.\nTo see that rank of EFT is rv , recall that the row space of V ′ is contained in the row space of U . Since E and F form a basis for row-spaces of U and V ′, row space of F is contained in the row space of E. So for any vector x, xT F lies in the row space of F, and thus in the row space of E. This implies that whenever xT F is nonzero, E(FT x) must be nonzero as well. So the rank of EFT is the same as the rank of FT , i.e. rv .\nNow since V ′ has rank r, there is a factorization V ′ = YB, where the rows of B are an r-dimensional orthonormal basis for the row space V ′, and Y is the matrix whose rows are the coordinates for the rows of V ′ in the basis B. Note that since B is orthonormal, the norms of rows of Y are the same as the norms of rows of V ′. Let X =UBT . Note that the rows of X are given by the the projections of the rows of U onto the basis B. Since B is orthonormal, this projection cannot increase the norms of the rows.\nObserve that XY T =UBT (V ′)T =M. Moreover XY T is an r-dimensional factorization for M because B is an r-dimensional basis. Finally, the norms of rows of X and Y are smaller than the norms of rows of U and V given by the semidefinite program, as we only decreased the lengths at each step described above."
    }, {
      "heading" : "B Hardness of Matrix Completion",
      "text" : "In this section we will prove Theorem 1.8. The basic setup is the same as in the proof of Theorem 1.9. Let G = (V ,E) be a graph with |V | = n and |E| =m. Now define the partial matrix PG ∈ (R∪⊥)n×n as follows:\nPG(i, j) =  1 if i = j 0 if (i, j) ∈ E ⊥ otherwise .\nThen, as before, if G is k-colorable with coloring function f : V → [k], then Mf = ∑ i∈[k] 1f −1(i)1 T f −1(i)\nis a rank-k completion of PG. Similar to the proof Theorem 1.9 we can also assume that Mf has coherence µ = 1. We next prove that under some mild additional assumptions any entry-wiseapproximate low-rank completion M of PG yields a coloring of G with few colors (as opposed to just finding a large independent set as done earlier).\nLemma B.1. Let G be a graph and define the partial matrix PG as above. Let M be a rank-r matrix with bounded coefficients |M(i, j)| 6 c for all i, j ∈ [n] such that M approximates PG, i.e.\n|M(i, j)− PG(i, j)| < ε whenever PG(i, j) , ⊥. If ε < 1 then there is an algorithm that colors G using only\nχ(G) 6 ( 4 √ cr\n1− 2ε )2r colors.\nProof. Because the entries of M are bounded by c, Lemma 2.1 shows how to find a factorization XY T = M such that, if ui and vi are the row vectors of X and Y respectively, for all i, j ∈ [n], we have ‖ui‖,‖vj‖ 6 (cr)1/4. Note that since ui · vi > 1− ε for all i, this implies a corresponding lower bound of ‖ui‖,‖vj‖ > (1− ε)(cr)−1/4.\nNow let W be a δ-net of the hypercube of sidelength 2(cr)1/4 in Rr centered at the origin (we will pick δ sufficiently small later). In particular we can pick W so that |W | 6 (2(cr)1/4/δ)r . Now define the following coloring function f : V →W ×W : f (i) = (w1,w2) if ui ∈ Bδ(w1) and vi ∈ Bδ(w2). If ui or vi lie in multiple balls of the δ-net, simply pick one arbitrarily (perhaps the closest point w ∈W ). The function f must color every vertex because the δ-net covers every vector.\nWe prove that f is a valid coloring if δ < (1− 2ε)(cr)−1/4/2. If i and j are the same color, vj lies in the same ball as vi . Then,\n|〈ui ,vi〉 − 〈ui ,vj〉| = |〈ui ,vi − vj〉| 6 ‖ui‖ · ‖vi − vj‖ 6 (cr)1/4(2δ)\n< (1− 2ε)\nand since ui · vi > 1 − ε, we must have |ui · vj | > ε, so (i, j) < E. Now note that the size of the coloring is at most\nχ(G) 6 |W |2 = ( 2(cr)1/4\nδ\n)2r = ( 4 √ cr\n1− 2ε )2r and this completes the proof.\nFinally, we can pad the output matrix with zeros just like in the previous section in order to achieve a larger number (0.9 fraction) of revealed entries. Combined with Lemma B.1, the above implies Theorem 1.8.\nC CSP Reduction for PSD-Completion\nIn this section, we include a CSP based hardness reduction for PSD-Completion that is robust and applicable with noise.\nLet φ be an instance of a Exact-one-in-k-SAT with variables x1, . . . ,xn and clauses C1, . . . ,Cm. Recall that a partial PSD matrix is equivalent to a list of inner product constraints. We will describe our reduction in this framework. Let x0 be a \"reference variable\" unused in φ. For every variable x ∈ {x0, . . . ,xn}, index a set of vectors {u(x,s)}s∈I by I = [2k]∪ ((2k 2 ) × {±1} ) . Then form the internal variable constraints\n– u(x,s) ·u(x,s) = 1 for all s ∈ I .\n– u(x,i) ·u(x,j) = δij for i, j ∈ [2k].\n– u(x,i,j,+1) ·u(x,i) = u(x,i,j,+1) ·u(x,j) = 1√2 for (i, j) ∈ (2k 2 ) .\n– u(x,i,j,−1) ·u(x,i) = −u(x,i,j,−1) ·u(x,j) = 1√2 for (i, j) ∈ (r1 2 ) .\nThese constraints force {u(x,i)}i to be a 2k-dimensional orthonormal basis for any x, as well as\nu(x,i,j,+1) = 1 √\n2\n( u(x,i) +u(x,j) ) ,\nand u(x,i,j,−1) = 1 √\n2\n( u(x,i) −u(x,j) ) .\nLet p : [2k]→ [2k] be the function p(i) = i + 1 if i is odd, and p(i) = i − 1 if i is even. Now for every variable x ∈ {x1, . . . ,xn}, form the external variable constraints\n– u(x0,i) ·u(x,j) = 0 if j , p(i).\n– u(x0,i,p(i),+1) ·u(x,i,p(i),+1) = 0 for odd i ∈ [2k].\n– u(x0,i,j,+1) ·u(x0,i,j,−1) for (i, j) ∈ (2k 2 ) and i, j odd.\nLet C0 be a \"reference clause\" not referring to any clause of φ. Index a set of vectors {uC}C by {C0, . . . ,Cm}. Form the internal clause constraints\n– uC0 ·uC0 = 1.\n– uC0 ·u(x0,2g−1) = 1√ k for every g ∈ [k].\nFor every clause C ∈ {C1, . . . ,Cm}, with variables {xi1 ,xi2 , . . . ,xik } with signs {s1, . . . , sk}, i.e. each sg ∈ {+1,−1},\n– uC ·uC = 1.\n– uC ·u(xig ,2g) = sg√ k for every g ∈ [k].\nFinally, for each clause C ∈ {C1, . . . ,Cm}, the external clause constraints are\n– uC0 ·uC = (1− 2/k) for every C ∈ {C1, . . . ,Cm}.\nThen we have the following theorem:\nTheorem C.1. If there is a satisfying assignment to φ, then there is a set of vectors lying in R2k satisfying the above constraints exactly. Conversely, if {u(x,s)}s∈I and {uCj }j∈[m] are vectors in R r that satisfy the constraints up to an additive ±ε, and r 6 4k −1 and ε < 10−6k−5, then there is a satisfying assignment to φ.\nProof. To start, we prove completeness. Let f be a satisfying assignment to φ. Then we propose the following vectors:\n– u(x0,i) = ei , where ei is the ith standard basis vector.\n– For every x ∈ {x1, . . . ,xn}, set u(x,p(i)) = f (x)ei for i odd and u(x,p(i)) = −f (x)ei for i even. For every i, j ∈ (2k 2 ) , set u(x,i,j,+1) and u(x,i,j,−1) to the normalized sum and difference of u(x,i)\nand u(x,j).\n– uC0 = 1√ k ∑ g u(x0,2g−1).\n– For everyC ∈ {C1, . . . ,Cm}with variables {xi1 , . . . ,xik } and signs {s1, . . . , sk}, set uC = 1√ k ∑ g sgu(xig ,2g).\n. It is clear that all internal constraints are satisfied. The external variable constraints are also easy to verify. For any clause C,\nuC ·uC0 = 1 k k∑ g,g ′=1 sg(u(xig ,2g) ·u(x0,2g ′−1))\n= 1 k k∑ g,g ′=1 sgf (xig )(e2g−1 · e2g ′−1)\n= 1 k k∑ g=1 sgf (xig ).\nSince f is a satisfying assignment to φ, exactly one variable in the clause is −1, thus the sum is exactly (k − 2), and so uC ·uC0 = (1− 2/k).\nTo prove soundness, we will start by assuming that all internal constraints are satisfied exactly, and only the external constraints contain errors. We will decode a satisfying assignment to φ under this assumption. Then we will show how to take the initial set of vectors and adjust them slightly to get a set of vectors perfectly satisfying the internal constraints.\nLemma C.2. Fix r < 4k. For each x ∈ {x0,x1, . . . ,xn}, let {u(x,s)}s∈I be a set of vectors in Rr satisfying the internal variable constraints exactly, and assume every external variable constraint is satisfied up to a small additive ±δ such that δ < 1/12k. Then for any x ∈ {x1, . . . ,xn} and odd i, i′ ∈ [2k],\nsign(u(x0,i) ·u(x,p(i))) = sign(u(x0,i′) ·u(x,p(i′)))\nand 1 > |u(x0,i) ·u(x,p(i))| > 1− 12δk\nProof. Because the internal constraints are satisfied, T0 = {u(x0,i)}i∈[2k] and Tx = {u(x,i)}i∈[2k] are orthonormal bases, so there is an orthonormal transformation Q : Rr → Rr such that Q(u(x0,i)) = u(x,i). We write Q in any basis containing T0:\nQ = [ Q′ A B C ] where Q′ is a transformation from T0 to itself. Because |u(x0,i) · u(x,j)| 6 δ for any j , p(i), Q ′ is at most δ except on the 2× 2 block diagonal. Now |u(x0,i,p(i),+1) · u(x,i,p(i),+1)| 6 δ implies that |Q(i,p(i)) −Q(p(i), i)| 6 3δ. Finally, |u(x0,i,i′ ,+1) · u(x,p(i),p(i′)−1)| 6 δ for odd i and i\n′ implies that |Q(i,p(i))−Q(i′ ,p(i′))| 6 3δ. Because of these conditions, we can write Q′ = R+ S, where\nR = R1 ⊕R2 ⊕ · · · ⊕Rk\nand\nRg = [\n0 a −a 0\n] for every g ∈ [k]\nand |S(i, j)| 6 6δ for every i, j. Then for any unit vector x ∈ span(T0),\n‖Q′x‖ > ‖Rx‖ − ‖Sx‖ > |a| − 6δk.\nIn particular, let x be a null vector of B, which exists because B is an (r −2k)×2k matrix, and r < 4k. Then\nQ [ x 0 ] = [ Q′x 0 ] and since Q is an orthogonal transformation, this implies ‖Q′x‖ = 1, and thus |a| > 1−6δk. Now recalling the definition of Q, for any odd i, u(x0,i) ·u(x,p(i)) =Q(i,p(i)) = a+S(i,p(i)), and thus for any odd i\n1 > |u(x0,i) ·u(x,p(i))| > 1− 12δk\nand the sign is the same for any odd i if δ < 112δk . The upper bound follows from the fact that u(x0,i) and u(x,p(i)) are unit vectors.\nWe take the interpretation that the variable x is set to sign(u(x0,1) ·u(x,2)). Now we prove that this assignment must be a satisfying assignment because of the clause constraints.\nLemma C.3. For each x ∈ {x0, . . . ,xn}, let {u(x,s)}s∈I satisfy the assumptions of the previous lemma, and for each C ∈ {C1, . . . ,Cm} with variables {xi1 ,xi2 , . . . ,xik } with signs {s1, . . . , sk}, let\nuC = 1 √ k k∑ g=1 sgu(xig ,2g)\nand let\nuC0 = 1 √ k k∑ g=1 u(x0,2g−1).\nThen if the constraints uC0 ·uC = (1− 2/k) are satisfied up to an additive ±δ and\nδ <min ( 2\n13k2 , 2 24k + k2 ) then the assignment f (x) = sign(u(x0,1) ·u(x,2)) is a satisfying assignment.\nProof. Let C ∈ {C1, . . . ,Cm}. From the definitions of uC and uC0 ,\nuC0 ·uC = 1 k k∑ g,g ′=1 sg ( u(x0,2g ′−1) ·u(xig ,2g) )\n= 1 k k∑ g=1 sg ( u(x0,2g−1) ·u(xig ,2g) ) + 1 k k∑ g,g ′ sg(u(x0,2g ′−1) ·u(xig ,2g))\nwhere xig are the variables appearing in clause C. We argue that |uC0 ·uC − (1−4/r1)| < δ implies that f is satisfying. Note that the dot products in first have magnitudes between 1 and 1−24δk,\nand those in the second sum have magnitudes at most δ. If δ < 2/(24k + k2), even if k − 2 of the dot products have sign +1 and 2 have sign −1,\n(1− 4/r1)−uC0 ·uC > (1− 4/r1)− 1 k [(k − 2)− 2(1− 12δk) + k(k − 1)δ)]\n= 2/k − δ(23 + k)\n> 2\n24k + k2\n> δ\nNow if δ < 2/13k2, then even if all k of the dot products have sign +1\nuC0 ·uC − (1− 4/r1) > 1 k [k(1− 12δk)− k(k − 1)δ]− (1− 2/k)\n= 2/k − δ(13k − 1)\n> 2\n13k2\n> δ.\nThese two facts mean that |uC0 ·uC − (1− 2/k)| < δ implies that exactly one of the dot products in the first sign can have sign −1 and the rest must have sign +1, i.e. that f satisfies the clause C. This is true for every clause, so f must be a satisfying assignment.\nThese lemmas prove Theorem 1.11 if only the external variable constraints experience errors, and uC and uC0 are constructed properly. The next sequence of lemmas show how to transform the problem from errors on all constraints into errors on only external constraints.\nLemma C.4. Let {u(x,s)}s∈I be a set of vectors satisfying the internal constraints to within an additive ±ε for ε < O(1/k). Then there is a set of vectors {ũ(x,s)}s∈I satisfying the internal constraints exactly such that\n‖ũ(x,s) −u(x,s)‖ 6 3 √ ε.\nProof. Let u(x,1) = u ⊥ 1 +u ‖ 1, where u ⊥ 1 is the part of u(x,1) perpendicular to the subspace span({u(x,i)}i,1), and u‖1 is the part parallel. Note that since |u(x,1) ·u(x,i)| 6 ε for any i , 1,\n‖u‖1‖ 6 √\n2k − 1 ε√ 1− ε .\nLet ũ(x,1) = u ⊥ 1 /‖u ⊥ 1 ‖. Then\n‖ũ(x,1) −u(x,1)‖ = ‖u⊥1 (1/‖u ⊥ 1 ‖ − 1)−u ‖ 1‖ 6 ( 1/‖u⊥1 ‖ − 1 ) ‖u⊥1 ‖+ ‖u ‖ 1‖\n= 1− √ ‖u(x,1)‖2 − ‖u ‖ 1‖2 + ‖u ‖ 1‖ 6 1− √ 1− ε+ 2 √\n2k − 1 ε√ 1− ε\n6 ε(1 + 2 √ k)\nwhere the last inequality uses a series approximation. Now proceeding inductively, let u(x,i) = u ⊥ i + u ‖ i , where the subspace considered is span({ũ(x,j)}j<i) ∪ span({u(x,j)}j>i), and we obtain an identical bound on the difference of norms. Note that the {ũ(x,i)}i∈r1 satisfy the internal constraints, and define the remaining vectors to be the sums forced by the remaining constraints.\nTo bound the dot products of sums of basis vectors, we first bound∥∥∥∥ 1√ 2 (u(x,i) +u(x,j))−u(x,i,j,+1) ∥∥∥∥ =\n= ( 1 √\n2 (u(x,i) +u(x,j))−u(x,i,j,+1)\n) · ( 1 √\n2 (u(x,i) +u(x,j))−u(x,i,j,+1) )1/2 6 ( 2(1 + ε) + ε − 2 √ 2(1/ √ 2− ε) )1/2\n6 √ ε √ 3 + 2 √ 2\nand now∥∥∥ũ(x,i,j,+1) −u(x,i,j,+1)∥∥∥ = ∥∥∥∥∥∥ 1√2(ũ(x,i) + ũ(x,j))−u(x,i,j,+1) ∥∥∥∥∥∥\n6 ∥∥∥∥∥∥ 1√2(ũ(x,i) + ũ(x,j))− 1√2(u(x,i) −u(x,j)) ∥∥∥∥∥∥+ ∥∥∥∥∥∥u(x,i,j,+1) − 1√2(u(x,i) +u(x,j)) ∥∥∥∥∥∥ 6 ε √ 2(1 + 2 √ k) + √ ε √ 3 + 2 √ 2 6 3 √ ε\nwhere the last inequality follows because, since ε < O(1/k), ε √ k is dominated by √ ε, and we simply round the coefficients to integers. Note that this also means 3 √ ε > ε(1 + 2 √ k).\nLemma C.5. For any two x,y ∈ {x0, . . . ,xn} and s ∈ I ,\n|ũ(x,s) · ũ(y,s) −u(x,s) ·u(y,s)| 6 7 √ ε\nProof. To compress notation, let u(x,s) = u and u(y,s) = v and likewise for the tilde versions.\n|〈ũ −u, ṽ − v〉| = |〈ũ, ṽ〉+ 〈u,v〉 − 〈ũ,v〉 − 〈u, ṽ〉| > |〈ũ, ṽ〉 − 〈u,v〉| − |〈ũ,v〉 − 〈u,v〉| − |〈u, ṽ〉 − 〈u,v〉| > |〈ũ, ṽ〉 − 〈u,v〉| − ‖ũ −u‖ − ‖ṽ − v‖\nAnd note that |〈ũ −u, ṽ − v〉| 6 ‖u − ũ‖‖v − ṽ‖\nand thus\n|〈ũ, ṽ〉 − 〈u,v〉| 6 ‖u − ũ‖‖v − ṽ‖+ ‖u − ũ‖+ ‖v − ṽ‖ 6 9ε+ 6 √ ε\n6 7 √ ε\nLemma C.6. For each clauseC ∈ {C1, . . . ,Cm} containing variables {xi1 , . . . ,xir1/2}, let ũC = 1√ k ∑ g sg ũ(xig ,2g), and let ũC0 = 1√ k ∑ g ũ(xig ,2g−1). Then for any C ∈ {C1, . . . ,Cm},\n|〈ũC0 , ũC〉 − 〈uC0 ,uC〉| 6 35 √ ε2k\nProof. For any clause C containing variables {xi1 , . . . ,xik }, we bound the norm,∥∥∥∥∥∥∥∥uC − 1√k k∑ g=1 sgu(xig ,2g) ∥∥∥∥∥∥∥∥ 6  uC − 1√k k∑ g=1 sgu(xig ,2g)  · uC − 1√k k∑ g=1 sgu(xig ,2g)   1/2\n6 ( 1 + ε+ k · 1\nk (1 + ε) + 1 k · k(k − 1)ε − 2k · 1√ k (1/ √ k − ε) )1/2 6 (ε(1 + k + 2 √ k))1/2 6 √ 2kε\nand ∥∥∥∥∥∥∥∥ũC − 1√k k∑ g=1 sgu(xig ,2g) ∥∥∥∥∥∥∥∥ 6 1√k k∑ g=1 ‖ũ(xig ,2g) −u(xig ,2g)‖\n6 ε(2k + √ k)\nand thus by triangle inequality,\n‖ũC −uC‖ 6 √ 2kε+ ε(2k + √ k) 6 2 √ kε\nbecause kε < √ kε. The identical calculation bounds ‖ũC0 −uC0‖. Finally, just as in the proof of Lemma Lemma C.5, for any clause C, we have\n|〈ũC0 , ũC〉 − 〈uC0 ,uC〉 6 ‖ũC0 −uC0‖‖ũC −uC‖+ ‖ũC0 −uC0‖+ ‖ũC −uC‖\n6 4kε+ 4 √ kε 6 5 √ kε\nIf every constraint experiences error at most ε, then we can construct an alternate solution that satisfies the internal constraints exactly and every external constraint experiences error at most δ 6 5 √ kε. Since we require at most\nδ 6min ( 1\n24k , 2 25k2 , 2 48k + k2 ) error on the external constraints, we can handle error at most ε < O(k−5). This completes the proof of the theorem."
    } ],
    "references" : [ {
      "title" : "New approximation guarantee for chromatic number",
      "author" : [ "Sanjeev Arora", "Eden Chlamtac", "Moses Charikar" ],
      "venue" : "In Proc. 38th Annual Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Arora et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2006
    }, {
      "title" : "An o(n3/14)-coloring algorithm for 3-colorable graphs",
      "author" : [ "Blum", "Karger" ],
      "venue" : "IPL: Information Processing Letters,",
      "citeRegEx" : "Blum and Karger.,? \\Q1997\\E",
      "shortCiteRegEx" : "Blum and Karger.",
      "year" : 1997
    }, {
      "title" : "A better performance guarantee for approximate graph coloring",
      "author" : [ "B. Berger", "J. Rompel" ],
      "venue" : null,
      "citeRegEx" : "Berger and Rompel.,? \\Q1990\\E",
      "shortCiteRegEx" : "Berger and Rompel.",
      "year" : 1990
    }, {
      "title" : "Complexity theoretic lower bounds for sparse principal component detection",
      "author" : [ "Quentin Berthet", "Philippe Rigollet" ],
      "venue" : "In Proc. 26th COLT,",
      "citeRegEx" : "Berthet and Rigollet.,? \\Q2013\\E",
      "shortCiteRegEx" : "Berthet and Rigollet.",
      "year" : 2013
    }, {
      "title" : "Approximation algorithms using hierarchies of semidefinite programming relaxations",
      "author" : [ "Eden Chlamtac" ],
      "venue" : "In Proc. 48th Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Chlamtac.,? \\Q2007\\E",
      "shortCiteRegEx" : "Chlamtac.",
      "year" : 2007
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel J. Candès", "Benjamin Recht" ],
      "venue" : "Foundations of Computional Mathematics,",
      "citeRegEx" : "Candès and Recht.,? \\Q2009\\E",
      "shortCiteRegEx" : "Candès and Recht.",
      "year" : 2009
    }, {
      "title" : "The power of convex relaxation: near-optimal matrix completion",
      "author" : [ "Emmanuel J. Candès", "Terence Tao" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Candès and Tao.,? \\Q2010\\E",
      "shortCiteRegEx" : "Candès and Tao.",
      "year" : 2010
    }, {
      "title" : "On the conditional hardness of coloring a 4-colorable graph with super-constant number of colors",
      "author" : [ "Irit Dinur", "Igor Shinkar" ],
      "venue" : "In APPROX-RANDOM,",
      "citeRegEx" : "Dinur and Shinkar.,? \\Q2010\\E",
      "shortCiteRegEx" : "Dinur and Shinkar.",
      "year" : 2010
    }, {
      "title" : "Complexity of the positive semidefinite matrix completion problem with a rank constraint. In Discrete Geometry and Optimization, volume 69 of Fields Institute Communications, pages 105–120",
      "author" : [ "Marianna E.-Nagy", "Monique Laurent", "Antonios Varvitsiotis" ],
      "venue" : null,
      "citeRegEx" : "E..Nagy et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "E..Nagy et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient svm training using low-rank kernel representations",
      "author" : [ "Shai Fine", "Katya Scheinberg" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Fine and Scheinberg.,? \\Q2002\\E",
      "shortCiteRegEx" : "Fine and Scheinberg.",
      "year" : 2002
    }, {
      "title" : "The complexity of matrix completion",
      "author" : [ "Nicholas J.A. Harvey", "David R. Karger", "Sergey Yekhanin" ],
      "venue" : "In Proc. 19th Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "Harvey et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Harvey et al\\.",
      "year" : 2006
    }, {
      "title" : "Algorithms and hardness for robust subspace recovery",
      "author" : [ "Moritz Hardt", "Ankur Moitra" ],
      "venue" : "In Proc. 26th COLT,",
      "citeRegEx" : "Hardt and Moitra.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hardt and Moitra.",
      "year" : 2013
    }, {
      "title" : "Coloring 3-colorable graphs with o(n1/5) colors",
      "author" : [ "Ken ichi Kawarabayashi", "Mikkel Thorup" ],
      "venue" : "In STACS,",
      "citeRegEx" : "Kawarabayashi and Thorup.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kawarabayashi and Thorup.",
      "year" : 2014
    }, {
      "title" : "Approximate graph coloring by semidefinite programming",
      "author" : [ "David Karger", "Rajeev Motwani", "Madhu Sudan" ],
      "venue" : "JACM: Journal of the ACM,",
      "citeRegEx" : "Karger et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Karger et al\\.",
      "year" : 1998
    }, {
      "title" : "Complexity measures of sign matrices",
      "author" : [ "Nati Linial", "Shahar Mendelson", "Gideon Schechtman", "Adi Shraibman" ],
      "venue" : "In Proc. 39th ACM Symposium on the Theory of Computing (STOC). ACM,",
      "citeRegEx" : "Linial et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Linial et al\\.",
      "year" : 2007
    }, {
      "title" : "Orthogonal representations over finite fields and the chromatic number of graphs",
      "author" : [ "René Peeters" ],
      "venue" : null,
      "citeRegEx" : "Peeters.,? \\Q1996\\E",
      "shortCiteRegEx" : "Peeters.",
      "year" : 1996
    }, {
      "title" : "A simpler approach to matrix completion",
      "author" : [ "Benjamin Recht" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Recht.,? \\Q2011\\E",
      "shortCiteRegEx" : "Recht.",
      "year" : 2011
    }, {
      "title" : "The em algorithm for kernel matrix completion with auxiliary data",
      "author" : [ "Koji Tsuda", "Shotaro Akaho", "Kiyoshi Asai" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Tsuda et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Tsuda et al\\.",
      "year" : 2003
    }, {
      "title" : "Rank minimization over finite fields: Fundamental limits and coding-theoretic interpretations",
      "author" : [ "V.Y.F. Tan", "L. Balzano", "S.C. Draper" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Tan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "Matrix Completion is the problem of recovering an unknown real-valued low-rank matrix from a subsample of its entries. Important recent results show that the problem can be solved efficiently under the assumption that the unknown matrix is incoherent and the subsample is drawn uniformly at random. Are these assumptions necessary? It is well known that Matrix Completion in its full generality is NP-hard. However, little is known if make additional assumptions such as incoherence and permit the algorithm to output a matrix of slightly higher rank. In this paper we prove that Matrix Completion remains computationally intractable even if the unknown matrix has rank 4 but we are allowed to output any constant rank matrix, and even if additionally we assume that the unknown matrix is incoherent and are shown 90% of the entries. This result relies on the conjectured hardness of the 4-Coloring problem. We also consider the positive semidefinite Matrix Completion problem. Here we show a similar hardness result under the standard assumption that P ,NP. Our results greatly narrow the gap between existing feasibility results and computational lower bounds. In particular, we believe that our results give the first complexity-theoretic justification for why distributional assumptions are needed beyond the incoherence assumption in order to obtain positive results. On the technical side, we contribute several new ideas on how to encode hard combinatorial problems in low-rank optimization problems. We hope that these techniques will be helpful in further understanding the computational limits of Matrix Completion and related problems. ∗IBM Research Almaden. Email: mhardt@us.ibm.com. †Microsoft Research. Email: meka@microsoft.com. ‡University of California, Berkeley. Email: prasad@cs.berkeley.edu. Supported by NSF Career Award and the Sloan Fellowship. §University of California, Berkeley. Email: bsweitz@eecs.berkeley.edu. Supported by the NSF GRFP. ar X iv :1 40 2. 23 31 v2 [ cs .C C ] 1 0 A pr 2 01 4",
    "creator" : "LaTeX with hyperref package"
  }
}