{
  "name" : "1301.6659.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Clustering-Based Matrix Factorization (under review)",
    "authors" : [ "Nima Mirbakhsh", "Charles X. Ling" ],
    "emails" : [ "smirbakh@uwo.ca", "cling@csd.uwo.ca" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Recommender systems are emerging technologies that can be found in many present-day applications. Netflix1 recommends users a list of movies that they may be interested in; Google News2 tracks the news that users are following and gives a list of recommended articles, Amazon3 suggests everything from books to magazines. All of these recommendations come from engines that call recommendation systems. These are\n1http://www.netflix.com 2http://news.google.com 3http://www.amazon.com\nUnder Review\nsystems that find relevant information and recommendations for users based on located users’ preferences (news, books, movies, musics, etc. ). Any information about users and items can be applied in a recommendation system, but there are two problems with information: 1) sometimes this information is barely accessible 2) with so much information, we should have algorithms that practically process it, and accurate results should be attained in a reasonable time. That is why scalability and accuracy have been the two most focused measures in recommendation systems in the last two decades (Konstan & Riedl, 2012).\nFor example, Figure 1 presents an artificial dataset which includes 5 users who rate newly released movies. The solid arrows reflect the users’ preferences and the dashed arrows reflect users’ non-preferences on these items. The main problem is to find related users and items based on their profiles or known preferences and infer the users’ unknown preferences considering these relations. Collaborative filtering is a well-known solution in recommendation systems that uses only the known preferences for modeling this problem. It relates two items based on the fact that many users have purchased or rated those items, or it relates two users based on their similar purchases or preferences. This solution is in contrast to the content filtering solution which focuses on analyzing sets of user profiles and product features to find similarities.\nNeighborhood based approaches are popular collaborative filtering methods (Töscher & Jahrer, 2009). This approach uses the known preferences to calculates similarities between users and finds the h most similar users to every users (it does same for items). It then deduces an unknown rating of user u for an item i, rui, considering ratings of that item by the h highest similar user to user u. Or, considering ratings that user u has revealed on the h highest similar items to item i (Töscher et al., 2008). For example this approach finds user 2 and 3 as the most similar\nar X\niv :1\n30 1.\n66 59\nv1 [\ncs .L\nG ]\n2 8\nJa n\n20 13\nusers to user 1 and then applies their preferences on item Madagascar to predict the unknown preference of user1 on item Madagascar. In practice, finding similar users and items pairs is costly as there are a large number of users and items. On the other hand, using only related items and users to find unknown preferences is too rigid. It just reflects the low-level interests of users and cannot be generalized easily to find users’ abstract tastes.\nMatrix factorization ((Koren et al., 2009), (Töscher et al., 2008), (Jamali & Ester, 2010)) is a common dimension reduction algorithm that generalizes users’ preferences and items’ histories in a very limited number of latent features (such as 5 or 10 features). It uses these latent features to predict possible preferences or ratings of users on items. Matrix factorization works based on the singular value decomposition (SVD) but in a lower dimension which finds a set of latent vectors that reflect the rating behavior of users. In our example, first matrix factorization deduces user 1’s interests and item Madagascar’s features in their latent vectors. It then uses these latent vectors to predict if user 1 is interested in item Madagascar. Hence, it is a good solution to deal with the complexity of finding relations as well as generalizing them. It also shows promising results in experiment using collaborative information (Koren et al., 2009).\nNeighborhood aware matrix factorization is an extension of matrix factorization which applies the neighborhood model in the matrix factorization algorithm. There are two approaches, going back to our example: 1) by predicting the possible ratings of most similar users to user1 on item Madagascar based on their latent vectors. This is then used to predict rui (by weighted averaging), or 2) by using matrix factorization technique similarities are found between users and\nitems with less cost and more generality. It then uses these similarities (similarity between user 1, 2, and 3, and items IceAge,Brave, Madagascar) and known preferences to predict the unknown rating. They both share the same rationality but are different regarding how they calculate the similarity and if they use rigid known preferences or predicted unknown preferences in the prediction time. The Netflix prize competition has shown that this approach outperforms almost every other approach on prediction accuracy (Töscher & Jahrer, 2009), (Töscher et al., 2008), (Koren & Bell, 2011).\nInspired by this idea, in our proposed model we try to find the communities between users and items and then apply the effect of the latent factors of these homogeneous communities in the basic model. It has two advantages opposed to the common neighborhood models:\n1. We generalize the users’ interests and items’ features for the communities that they belong to. For example, user 1 may belongs to community Adults and item Madagascar may belongs to community Cartoons. Thus, in addition to considering if user1 is interested in item Madagascar, we apply the effect that community Adults is interested in community Cartoons as well.\n2. We consider deeper similarities. Item-item models check if user 1 is interested in item Madagascar and its similar items (IceAge and Brave). Useruser models also check if user 1 and its similar users (2 and 3) are interested in item Madagascar. In our approach, we check to see if user 1, 2 and 3 are interested in items Madagascar, IceAge and Brave.\nThus, the previous approaches likely predict a high interest for user 1 on item Madagascar because of a high similarity between user 3 and 1. But in a more general view they belong to the communities that do not share many interests (Adults on Cartoons). Hence in our proposed model, we first try to find these homogeneous communities (Adults, Cartoons, Drama, and so on.) and then consider the effect of these communities’ behaviors beside the behavior of each user and item in an extension of Matrix Factorization. However, these communities are not usually easily understandable (like Drama and Adults in our example) by using collaborative information.\nWe successfully apply a standard clustering algorithm on a small dataset which works great. However, common clustering methods such as Kmeans and Hierarchical Clustering cannot handle large datasets. These\nmethods need to compare all the items and users pairs to find their similarities for clustering them. Thus, we employ Locality Sensitive Hashing (LSH) methods by a new technique to have a fast clustering method in our proposed model. LSH methods are usually used for dimension reduction purposes and other methods are applied then on the reduced dimension for clustering or classification objectives. In our method, we first use Minhash as the hash function of LSH to reduce the dimension and then consider each reduced dimension vector (bucket) as a cluster. This approach causes a less accurate and estimated clustering. In our experiment results, we show how using this estimated clustering method instead of using common clustering methods will affect the model’s accuracy.\nWe use Root Mean Squared Error (RMSE) as the accuracy measure through some well known datasets including Netflix, Epinions, Flixster , and movielens 100K which are previously used in many publications such as (Koren et al., 2009), (Jamali & Ester, 2010), (Salakhutdinov & Mnih, 2008), and (Herlocker et al., 1999). Our empirical experiment show that our proposed algorithm make a significant improvement of accuracy without adding much complexity to the model. For instance, our proposed method’s accuracy is 0.8122 on the Netflix dataset which is better than the Netflix prize winner’s accuracy of 0.8567. To the best of our knowledge, our proposed method outperforms all other published neighborhood models in accuracy and complexity."
    }, {
      "heading" : "2. Problem Definition",
      "text" : "In a general recommendation problem, we have a set of users U = {u1, u2, . . . , un} and a set of items I = {i1, i2, . . . , im} that they are companied by a rating matrix R = [rui]n×m where rui represents the rating of user u on item i. This rating values are limited in a range [a, b] depending on applications but a range [0, 5] or a boolean value {0, 1} are usually quite common forms of rating in real-world applications. A recommendation system’s goal is to find out an unknown rating rui for user u and item i using information including the rating matrix R.\nCollaborating filtering consists of predicting unknown rijs based on the known ri′j′ s inside the matrix R:\nR =\n i1 i2 . . . im u1 r11 r12 . . . r1m u2 r21 r22 . . . r2m ... ... ... . . . ... un rn1 rn2 . . . rnm \nMatrix Factorization addresses this problem by decomposing the ratings matrix, R, into two lower dimension matrices Q and P which contain corresponding vectors in the length of k for every item and user respectively, where k m,n. Such a model is close to the singular value decomposition (SVD) technique for finding latent factors in information retrieval (Koren et al., 2009).\nR =  i1 i2 . . . im q11 q12 . . . q1m q21 q22 . . . q2m ... ... . . .\n... qk1 qk2 . . . qkm\n T ×  u1 u2 . . . un p11 p12 . . . p1n p21 p22 . . . p2n ... ... . . .\n... pk1 pk2 . . . pkn\n\nThe learning algorithm starts by a random initialization of matricesQ and P . In every learning step it then tries to change the initialized variables in the way that the product matrix converge to the known values of R. In the prediction case, the product of learned matrices will be used to predict unknown rijs.\nThus Matrix Factorization characterizes every user and item by corresponding them a latent vector. Assume qi ∈ Rk as the corresponding vector for item i and pu ∈ Rk as the corresponding vector for user u. It is supposed that the dot product of the user-item vectors results in the user’s rating on the item:\nrui = q T i pu\nLets define the error as the real rating minus the predicted value in each step, eui. In the learning phase, the goal will be minimization of the regularized squared error on a train set, K, as follows:\nmin( ∑\n(u,i)∈K\n(rui − qTi pu)2 + λ(|qi|2 + |pu|2))\nRegularized value prevents over-fitting on the model. Thus, the problem will change to minimizing the square error of perdition as well as keeping absolute values of the latent variables as minimal as possible. Also, a bias value usually is corresponded to each user and item to reflect their mean ratings. Adding the biases, the above statement will change to:\nmin( ∑\n(u,i)∈K\n(rui − qTi pu − b2i − b2u)2\n+λ(|qi|2 + |pu|2 + |bu|2 + |bi|2))\nAlgorithm 1 presents the learning pseudo using a stochastic gradient descent technique giving a learning rate γ, and a regularizing rate λ .\nAlgorithm 1 Updating\nInput: Train set K Initializing matrices P , Q and bias values repeat\nfinding the error of predictions, eui, for every known rui in train set. for i = 1 to m− 1 do qi ← qi + γ(eui.pu − λ.qi) pu ← pu + γ(eui.qi − λ.pu) bi ← bi + γ(eui − λ.bi) bu ← bu + γ(eui − λ.bu)\nend for until for limited number of epoches"
    }, {
      "heading" : "3. Clustering-Based Matrix Factorization",
      "text" : "In our proposed extension, we add neighborhood information on the basic Matrix Factorization model in a novel way. There are two common approaches in neighborhood aware matrix factorization models: 1) item − item models which consider if user u is interested in item i its similar items. 2) user−user models that consider if user u and its similar users are interested in item i. In addition to complexity of finding similar users and items these models ignore the effect of the interests or features of communities that they belong to. Hence, we first try to find these communities and then apply the effect of the latent factors of these homogeneous communities in the basic model. As discussed in Section 1, it has two advantages opposed to the common neighborhood models:\n1. We generalize the users’ interests and items’ features for the communities that they belong to. Thus, we consider if general interests of users are matched with general features of items as well.\n2. We consider deeper similarities. More specifically, we consider if user u and its similar users are interested in item i as well as its similar items.\nHence, our proposed model is an improvement of the item-item/user-user fusion models. We assume the ratings in a range of [1, 5]. For every user u and item i, vectors Iu =< I(u, i1), I(u, i2), . . . , I(u, im) > and Ii =< I(u1, i), I(u2, i), . . . , I(un, i) > are made respectively in the following form:\nI(uj , i) = { 1 ruj ,i > µ 0 otherwise\nand:\nI(u, ij) = { 1 ru,ij > µ 0 otherwise\nwhich µ is the average of ratings. In this way users(items) with the same co-rating(co-rated) profile will go to the same clusters. Although for the recommendation problem with a boolean rating {0, 1}, the exact rating values can be used to make the vectors.\nWe cluster the users and items to find their homogeneous communities. Then, we correspond every cluster (homogeneous communities) a latent vector as well as corresponding each user and item a vector like basic matrix factorization.\nAfter finding n′ < n and m′ < m homogeneous communities of users and items respectively, we consider the rating between these communities in a new matrix R∗ then. In the new rating matrix every r∗u′i′ reflects the rating of community u′ of users on the community i′ of items as follows:\nR∗ =\n Ci1 Ci2 . . . Cim′ Cu1 r ∗ 11 r ∗ 12 . . . r ∗ 1m′ Cu2 r ∗ 21 r ∗ 22 . . . r ∗ 2m′ ... ... ... . . .\n... Cun′ r ∗ n′1 r ∗ n′2 . . . r ∗ n′m′  Hence, in this method the predictor function for every r∗u′i′ would change to:\nr∗u′i′ = q T ci′ pcu′ (1)\nwhere qci′ and pcu′ are corresponding values for clusters ci′ and cu′ . Thus, instead of predicting a rating for pair of users and items, in this method we will find a rating for their cluster pairs as well and use a fusion of both ratings in the final predictor function as follows:\nrui = αq T i pu + (1− α)qTcipcu + bi + bu (2)\nwhere α will control the effect of both sentences in the predictor’s result and ci and cu are the clusters that item i and user u belong to respectively. We name the combination form in our experimental results as the Clustering-Based Matrix Factorization (CBMF) . Our experiment shows that CBMF significantly improves the accuracy. However, using common clustering algorithms for large datasets is not practically. Thus, we use Locality Sensitive Hashing functions with a new technique to keep our model practically efficient."
    }, {
      "heading" : "3.1. Locality Sensitive Hashing",
      "text" : "As noticed earlier, the computation cost dealing with large datasets of recommendation systems is important. Hence, we use Locality-sensitive hashing (LSH)\nmethods establishing a new technique to estimate the clusters instead of using common similarity-based clustering methods. In the literature, LSH is usually employed for reducing dimension and then clustering algorithms such as k-means are applied on the reduced dimension for clustering purposes. Even in this way, a similarity check between all users and items is needed which is still costly. LSH is a method of performing probabilistic dimension reduction to hash the input items in the way that similar items are mapped to same buckets with a high probability (Broder et al., 1998). We use Min-wise independent permutations (MinHash) as the hash function in our method. It consists of β hash functions, like H =< h1, h2, . . . , hβ >, in the following form:\nhi = min{π(a)}\nwhere a is the input vector, and π(a) is a permutation on the indexes of the input vector a. The min function will return the first non-zero index of the permuted vector.\nMore specifically, β is the size of new dimension. These achieved low dimensional vectors (name buckets) can be considered as the estimated clusters. Reducing β leads to making fewer clusters and vice versa. However in this way we will loose information as well as the output clusters are weak estimations. In our model we try β = 1, 2, . . . , 6 to evaluate using a different number of estimated clusters (instead of using common clustering methods) on the accuracy of the model. Applying this LSH method in our proposed model does not add a costly extension on the basic matrix factorization mode. Hence, the time of learning algorithm will increase only less than twice of basic matrix factorization in the worst case scenario (m′ = m and n′ = n)."
    }, {
      "heading" : "3.2. Repeating clustering and using hierarchical clusters in CBMF",
      "text" : "LSH algorithms rely on probabilistic logic. Thus, it is supposed that using the LSH’s buckets for several times will decrease the clustering error. This can be modeled by re-clustering fixed β for h times. We then add the effect of new clusters in the predictor function (Repeated CBMF). Or, by applying all h most accurate clusters with different size of buckets in the predictor function(Hierarchical CBMF). However, using large h will linearly increase the learning time. Thus using re-clustering or more and less general clusters will decrease the effect of LSH clustering error and will add more information to the model. As noticed earlier, changing β will affect the number of clusters and the generality of clusters. Thus starting with β = 1,\nthis technique achieves the most general clusters and by increasing the β the number of clusters would be increased and consequently achieving less general clusters. In our experiment, we first test our CBMF algorithm with different size of buckets (β) on the validation set and then:\n1. We select the h most accurate βs in a new predictor function as follows:\nrui = αq T i pu + (1−α)( h∑ j=1 W βj .qT c βj i p c βj u ) + bi + bu\n(3)\nThe weights (W βj ) are selected in a proportion to how accurate the CBMF algorithm is using the βj on the validation set.\n2. We select the most accurate β and then apply Minhash algorithm h times and with same β on the same train set. By adding the effect of the new-clusters, we change the predictor function in same way as formula 3 using equals weights (W βj )."
    }, {
      "heading" : "4. Experiment Results",
      "text" : "We setup our experiment on several well-known recommendation system datasets including Netflix, Epinions, Flixster, and MovieLens100k1. These datasets are previously used in related articles such as (Töscher & Jahrer, 2009), (Jamali & Ester, 2010), and (Herlocker et al., 1999). Table 1 shows a general statistics of these datasets.\nWe apply a repeated random sub-sampling validation which in each fold divides the known ratings in two sets; 80% for train set and the rest for test set. We use three folds and get the mean of these folds’ test set accuracies as the final accuracy of the model. We first collect all the items that were rated over 3 by users (4 for Netflix) in the train set and do the same for items. Then, we establish the LSH method on this collection\n1The implementation package is publicly accessible at: https://www.dropbox.com/sh/x710ouz9ytdaeqf/0OHGQtQTv9\nto cluster the items and the users. As described in section Locality Sensitive Hashing, we try different size of buckets to see the effect of increasing and decreasing in the number of clusters.\nRoot Mean Squared Error (RMSE) is used as the evaluation metric as follows:\nRMSE =\n√∑ (u,i)∈Ktest(rui − r ′ ui) 2\n|Ktest|\nwhere Ktest is the test set, rui is the actual rating, and rui is the predicted rating for user u on item i. We use a threshold of 0.005 as the stopping point in all the implemented algorithms. The learning process will be stopped if the difference of the train set’s RMSEs between two epochs goes down the threshold. Experiments show us that using a threshold in opposed to using a fix number of epochs prevents over-fitting in the learning model. For example, employing 100-500 epochs to learn the Basic Matrix Factorization on the MovieLens dataset reaches 0.90 of RMSE for the test set but using the threshold to stop the learning process improves the RMSE to 0.81.\nFigure 2 illustrates a comparison between basic MF, neighborhood aware MF by (Töscher et al., 2008), and our proposed model on movielens100k dataset. In Basic CBMF, we calculates all the similarities between users and items and employ them in a bottom-up hierarchical clustering. In the CBMF, the costly clustering is replaced by assuming Minhash’s buckets as clusters. As it is expected, using more accurate clustering method will improve the accuracy for our proposed model. But, it is usually unpractical for large datasets. Although, using Minhash buckets as clusters still improves the RMSE more than other costly neighborhood aware matrix factorization.\nAs figure 3 shows, CBMF not only has an expressive less RMSE against the other well-known neighborhood extensions ((Töscher et al., 2008) and (Koren & Bell, 2011)) it also outperforms the 2009 Netflix prize winner’s ensemble algorithm by (Töscher & Jahrer, 2009)."
    }, {
      "heading" : "4.1. Number of Clusters",
      "text" : "Unfortunately using the buckets as the clusters limits the choices to select the number of clusters. Cause it cannot be defined before hand, and increasing in the length of buckets, β, will increase the number of clusters. Figure 5 shows the change of similarity among the items in same clusters for different β values. Increasing β, the similarity obviously will be increased but the number of clusters will be increased as well. Figure 4 shows how changing β affects the RMSE results for the Netflix dataset. A question is: How the\nβ should be selected?\nTo determine β, we apply a cross-validation on the train set, and then changing the buckets’ length we select the β with lowest RMSE. Figure 6 illustrates applying our proposed method on the movielens dataset changing β and in a 3-folds cross-validation. We are using same β for users and items. As figure 6 shows β = 2 has the best RMSE result for the verification sets. The average of test sets’ RMSEs is smoothly similar to the average of validation sets’ RMSEs for all the different βs."
    }, {
      "heading" : "4.2. Complexity",
      "text" : "As an advantage of employing LSH methods, the proposed model does not add a costly extension on the basic matrix factorization. It increases the learning\ntime of the basic matrix factorization by less than twice in each epoch. Table 2 shows the complexity of our proposed model, Basic Matrix Factorization, and two Neighborhood Aware Matrix Factorization models by (Töscher et al., 2008), and (Koren & Bell, 2011). The second statements show complexity of pre-processing for each model. Basic matrix factorization and Neighborhood Aware Matrix Factorization by (Koren & Bell, 2011) have no preprocessing but the (Töscher et al., 2008) proposed model add a preprocessing complexity of O(n2 + m2) to find the similar users and items. R(u) is the set of items for which their ratings by u are available, and likewise, R(i) denotes the set of users who rated item i. Thus, ∑ u |R(u)| will be the number of known rating values in matrix R. The preprocessing of our algorithm includes hashing each input vector to a new lower dimension applying Minhash function. Thus, our preprocessing complexity would be a tracing through users and items which is O(m+ n) ."
    }, {
      "heading" : "4.3. Selecting the Alpha Paremeter (α)",
      "text" : "For selecting α, two approaches can be applied:\n1. To train the rating matrix for users-items and the rating matrix for the clusters separately. Thus,\nFigure 8 shows the RMSE result of changing α for the first case and figure 7 shows the results for the second case of selecting α. For α = 1 the effect of predictor function of clusters is not considered in both cases. Thus, the RMSE result is as same as the basic matrix factorization for users-items rating matrix."
    }, {
      "heading" : "4.4. Using Contents",
      "text" : "So far we only use collaborative information in our model. It is supposed that applying users’ profiles and items’ features to the model will increase the clustering\naccuracy which will consequently improve the model’s RMSE. As the datasets that we have already used do not provide any profiles for items and users, we use a smaller version of the dataset that is provided in the ”KDD Cup 2012- track1” to experiment this effect. The dataset consists of a Chinese social network, Tencent Weibo, users’ and items’ profiles and list of recommended items to some users where they have accepted or rejected them (boolean values). Our Experiment shows that using users’ and items’ keywords in the model improves the accuracy by 1%."
    }, {
      "heading" : "4.5. Using RCBMF and HCBMF",
      "text" : "At the end, we use the validation results of CBMF to apply the effect of using multiple buckets on the model.\nAs noticed earlier, we are doing this in two cases: 1) Repeating the most accurate CBMF model changing the β (RCBMF), and 2) Applying h most accurate models changing the β (HCBMF). We set h = 3 in our experiment in both algorithms.\nAs figure 9 shows, we experience different improvement of accuracies for the models in different datasets. For instance, HCBMF shows significant improvement of RMSE for Epinions datasets, but on the other hand its result for Flixter is not better than simple CBMF. RCBMF has better result for Movielens100k, Netflix, and Flixter datasets. We tune the parameters (including W βj and α ) using cross-validation in our experiment but we still employ same learning rate (γ) and regularizing parameter (λ) as CBMF."
    }, {
      "heading" : "5. Related Works",
      "text" : "(Töscher et al., 2008) presents a neighborhood-aware matrix factorization which includes neighborhood information in the basic Matrix factorization. Their proposed algorithm computes three predictions for every user-item pair: a prediction rMFui based on basic matrix factorization; a prediction ruserui based on a userneighborhood model; and finally a prediction ritemui which is based on a item-neighborhood model. A combination of these three predictions is the final prediction of this algorithm. The rating prediction ruserui is computed as follows:\nruserui =\n∑ v∈Uj(u) c\nuser uv rvi∑\nv∈Uj(u) c user uv\nwhere UJ(u) denotes the set of J users with highest correlation to user u. These correlations are reached by counting the number of co-rating of users, and co-\nbeing-rated of items. The paper has a similar approach in computing ritemui . It is mentioned by (Töscher et al., 2008) that using this neighborhood-aware improves the accuracy and even learning convergence. However, it is still sensitive to the choice of J . By increasing J the processing time of learning in each epoch will be increased linearly. On the other hand counting correlations between pairs of users and item would be costly in large datasets.\nKoren et al. in (Koren & Bell, 2011) proposes an itemitem model and a user-user model in a new factorization approach and then, as a fusion, use both in a single model to predict unknown ratings. They associate each user u with two vectors pu, zu ∈ Rk and use the dot product of these two vectors to model relation between user u and v as wuv = p T u zv. Proposed user-user model is as follows:\nr̂ui = µ+ bu+ bi+ |R(i)|−1/2 ∑ v∈R(i) (r̂vi − bvi)pTu zv\nwhere R(i) contains all the users who rated item i. They employ same rationality for their proposed itemitem model. Finally, (Koren & Bell, 2011) proposes a fusion of item-item and user-user models in a single model to have the advantage of both sides’ information.\n(Desrosiers & Karypis, 2011) presents a complete survey on neighborhood-based recommendation methods that covers many other extensions on matrix factorization. However, as noticed with more details in sections 1 and 3, our proposed model apply deeper similarities as well as more general interests in the predictor function. Hence, the CBMF is an improvement of itemitem/user-user fusion models.\nShahabi in (Shahabi et al., 2001) uses LSH methods to make a scalable web-base recommendation system (named Yoda). They customize LSH method to a Fuzzy Locality Sensitive Hashing method and use it to reduct the items dimension space. First a large wish list of items will be refined for a user, and then k-nearest-neighbor (KNN) is used in the reduced dimension space to find the closest items to the user’s preferences in the item wish list. (Shahabi et al., 2001) reports improvement of accuracy with less cost against the basic KNN method. However, the KNN’s accuracy is far away to even basic matrix factorization in practice (Töscher & Jahrer, 2009)."
    }, {
      "heading" : "6. Conclusion",
      "text" : "Matrix Factorization is a popular method in Recommendation Systems showing promising results on ac-\ncuracy and complexity. In this paper we propose an extension of matrix factorization that use clustering paradigm to cluster similar users and items in several communities. We then establish the effect of these communities on the prediction model. More specifically, our proposed model is an improvement of itemitem/user-user fusion models where we apply deeper similarities and more generalized interests. As the datasets are usually huge in recommendation systems, we use a new technique for clustering which keeps our model practically efficient. Our experiments show very promising accuracy that to the best of our knowledge it outperforms all other proposed recommender methods. For instance, our proposed method’s accuracy is 0.8122 on the Netflix dataset which is better than the Netflix prize winner’s accuracy of 0.8567."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Recommender systems are emerging technologies that nowadays can be found in many applications such as Amazon, Netflix, and so on. These systems help users find relevant information, recommendations, and their preferred items. Matrix Factorization is a popular method in Recommendation Systems showing promising results in accuracy and complexity. In this paper we propose an extension of matrix factorization that uses the clustering paradigm to cluster similar users and items in several communities. We then establish their effects on the prediction model then. To the best of our knowledge, our proposed model outperforms all other published recommender methods in accuracy and complexity. For instance, our proposed method’s accuracy is 0.8122 on the Netflix dataset which is better than the Netflix prize winner’s accuracy of 0.8567.",
    "creator" : "LaTeX with hyperref package"
  }
}