{
  "name" : "1202.6228.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "PAC-Bayesian Generalization Bound on Confusion Matrix for Multi-Class Classification∗",
    "authors" : [ "Emilie Morvant", "Sokol Koço", "Liva Ralaivola" ],
    "emails" : [ "firstname.name@lif.univ-mrs.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Machine Learning, PAC-Bayes generalization bounds, Confusion Matrix, Concentration Inequality, Multi-Class Classification"
    }, {
      "heading" : "1 Introduction",
      "text" : "The PAC-Bayesian framework, first introduced by McAllester (1999b), provides an important field of research in learning theory. It borrows ideas from the philosophy of Bayesian inference and mix them with techniques used in statistical approaches of learning. Given a family of classifiers F , the ingredients of a PAC-Bayesian bound are a prior distribution P over F , a learning sample S and a posterior distribution Q over F . Distribution P conveys some prior belief on what are the best classifiers from F (prior any access to S); the classifiers expected to be the most performant for the classification task at hand therefore have the largest weights under P. The posterior distribution Q is learned/adjusted using the information provided by the training set S. The essence of PAC-Bayesian results is to bound the risk of the stochastic Gibbs classifier associated with Q (Catoni, 2004) —in order to predict the label of an example x, the Gibbs classifier first draws a classifier f from F according to Q and then returns f(x) as the predicted label.\nWhen specialized to appropriate function spaces F and relevant families of prior and posterior distributions, PAC-Bayes bounds can be used to characterize the error of a few existing classification methods. An example deals with the risk of methods based upon the idea of the majority vote in the case of binary classification. We may notice that if Q is the posterior distribution, the error of the Q-weighted majority vote classifier, which makes a prediction for x according to ∑ f f(x)Q(f), is bounded by twice the error of the Gibbs classifier. If the classifiers from F on which the distribution Q puts a lot of weight are good enough, then the bound on the risk of the Gibbs classifier can be an informative bound for the risk of the Q-weighted majority vote. With a more elaborated argument, Langford and Shawe-Taylor (2002) give a PAC-Bayes bound for Support Vector Machine (SVM) which closely relates the risk of the Gibbs classifier and that of the corresponding majority vote classifier, and where the margin of the examples enter into play. In their study, both the prior and posterior distribution are normal distributions, with different means and variances. Empirical results show that this bound is a good estimator of the risk of SVMs (Langford, 2005).\n∗This work was supported in part by the french projects VideoSense ANR-09-CORD-026 and DECODA ANR-09-CORD005-01 of the ANR in part by the IST Programme of the European Community, under the PASCAL2 Network of Excellence, IST-2007-216886. This publication only reflects the authors’ views.\n1\nar X\niv :1\n20 2.\n62 28\nv6 [\nst at\n.M L\n] 2\n2 O\nct 2\n01 3\nPAC-Bayes bounds can also be used to derive new supervised learning algorithms. For example, Lacasse et al. (2007) have introduced an elegant bound on the risk of the majority vote, which holds for any space F . This bound is used to derive an algorithm, namely MinCq (Laviolette et al., 2011), which achieves empirical results on par with state-of-the-art methods. Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001).\nIn this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.MH/AdaBoost.MR algorithms of Schapire and Singer (1999) and SAMME algorithm by Zhu et al. (2009).\nThe originality of our work is that we consider the confusion matrix of the Gibbs classifier as an error measure. We believe that in the multiclass framework, it is more relevant to consider the confusion matrix as the error measure than the mere misclassification error, which corresponds to the probability for some classifier h to err on x. The information as to what is the probability for an instance of class p to be classified into class q (with p 6= q) by some predictor is indeed crucial in some applications (think of the difference between false-negative and false-positive predictions in a diagnosis automated system). To the best of our knowledge, we are the first to propose a generalization bound on the confusion matrix in the PAC-Bayesian framework. The result that we propose heavily relies on the matrix concentration inequality for sums of random matrices introduced by Tropp (2011). One may anticipate that generalization bounds for the confusion matrix may also be obtained in other framework than the PAC-Bayesian one, such as the uniform stability framework, the online learning framework and so on.\nThe rest of this paper is organized as follows. Sec. 2 introduces the setting of multiclass learning and some of the basic notation used throughout the paper. Sec. 3 briefly recalls the folk PAC-Bayes bound as introduced in McAllester (2003). In Sec. 4, we present the main contribution of this paper, our PAC-Bayes bound on the confusion matrix, followed by its proof in Sec. 5. We discuss some future works in Sec. 6."
    }, {
      "heading" : "2 Setting and Notations",
      "text" : "This section presents the general setting that we consider and the different tools that we will make use of."
    }, {
      "heading" : "2.1 General Problem Setting",
      "text" : "We consider classification tasks over the input space X⊆Rd of dimension d. The output space is denoted by Y ={1, . . . , Q}, where Q is the number of classes. The learning sample is denoted by S = {(xi, yi)}mi=1 where each example is drawn i.i.d. from a fixed —but unknown— probability distribution D defined over X × Y . Dm denotes the distribution of a m-sample. F ⊆ RX is a family of classifiers f : X → Y . P and Q are respectively the prior and the posterior distributions over F . Given the prior distribution P and the training set S, the learning process consists in finding the posterior distribution Q leading to a good generalization.\nSince we make use of the prior distribution P on F , a PAC-Bayes generalization bound depends on the Kullback-Leibler divergence (KL-divergence):\nKL(Q‖P) = Ef∼Q log Q(f)\nP(f) . (1)\nThe function sign(x) is equal to +1 if x ≥ 0 and −1 otherwise. The indicator function I(x) is equal to 1 if x is true and 0 otherwise."
    }, {
      "heading" : "2.2 Conventions and Basics on Matrices",
      "text" : "Throughout the paper we consider only real-valued square matrices C of order Q (the number of classes). tC is the transpose of the matrix C, IdQ denotes the identity matrix of size Q and 0 is the zero matrix.\nThe results given in this paper are based on a concentration inequality of Tropp (2011) for a sum of random self-adjoint matrices. In the case when a matrix is not self-adjoint and is real-valued, we use the\nTechnical Report V 4.0 2\ndilation of such a matrix, given in Paulsen (2002), which is defined as follows: S(C)def= (\n0 C tC 0\n) . (2)\nThe symbol ‖ · ‖ corresponds to the operator norm also called the spectral norm: it returns the largest singular value of its argument, which is defined by\n‖C‖ = max{λmax(C),−λmin(C)}, (3)\nwhere λmax and λmin are respectively the algebraic maximum and minimum singular value of C. Note that the dilation preserves spectral information, so we have:\nλmax ( S(C) ) = ‖S(C)‖ = ‖C‖. (4)\nSince ‖ · ‖ is a regular norm, the following equality obviously holds:\n∀a ∈ R, ‖aC‖ = |a|‖C‖. (5)\nGiven the matrices C and D both made of nonnegative elements and such that 0 ≤ C ≤ D (element-wise), we have:\n0 ≤ C ≤ D⇒ ‖C‖ ≤ ‖D‖. (6)"
    }, {
      "heading" : "3 The Usual PAC-Bayes Theorem",
      "text" : "In this section, we recall the main PAC-Bayesian bound in the binary classification case as presented in (McAllester, 2003; Seeger, 2002; Langford, 2005). The set of labels we consider is Y = {−1; 1} (with Q = 2) and, for each classifier f ∈ F , the predicted output of x ∈ X is given by sign(f(x)). The true risk R(f) and the empirical error RS(f) of f are defined as:\nR(f) def = E(x,y)∼DI(f(x 6= y)) ; RS(f) def =\n1\nm m∑ i=1 I(f(xi 6= yi)).\nThe learner’s aim is to choose a posterior distribution Q on F such that the risk of the Q-weighted majority vote (also called the Bayes classifier) BQ is as small as possible. BQ is defined by:\nBQ(x) = sign [Ef∼Qf(x)] .\nThe true risk R(BQ) and the empirical error RS(BQ) of the Bayes classifier are defined as the probability that it commits an error on an example:\nR(BQ) def = P(x,y)∼D (BQ(x) 6= y) . (7)\nHowever, the PAC-Bayes approach does not directly bound the risk of BQ. Instead, it bounds the risk of the stochastic Gibbs classifier GQ which predicts the label of x ∈ X by first drawing f according to Q and then returning f(x). The true risk R(GQ) and the empirical error RS(GQ) of GQ are therefore:\nR(GQ) = Ef∼QR(f) ; RS(GQ) = Ef∼QRS(f). (8)\nNote that in this setting, if BQ misclassifies x, then at least half of the classifiers (under Q) commit an error on x. Hence, we directly have: R(BQ) ≤ 2R(GQ). Thus, an upper bound on R(GQ) gives rise to an upper bound on R(BQ).\nWe present the PAC-Bayes theorem which gives a bound on the error of the stochastic Gibbs classifier.\nTheorem 1 (i.i.d. binary classification PAC-Bayes Bound). For any D, any F , any P of support F , any δ ∈ (0, 1], we have,\nPS∼Dm\n( ∀Q on F , kl ( RS(GQ), R(GQ) ) ≤ 1 m [ KL(Q‖P) + ln ξ(m) δ ]) ≥ 1− δ,\nwhere kl(a, b) def = a ln ab + (1− a) ln 1−a 1−b , and ξ def = ∑m i=0 ( m i ) (i/m)i(1− i/m)m−i.\nWe now provide a novel PAC-Bayes bound in the context of multiclass classification by considering the confusion matrix as an error measure.\nTechnical Report V 4.0 3"
    }, {
      "heading" : "4 Multiclass PAC-Bayes Bound",
      "text" : ""
    }, {
      "heading" : "4.1 Definitions and Setting",
      "text" : "As said earlier, we focus on multiclass classification. The output space is Y = {1, . . . , Q}, with Q > 2. We only consider learning algorithms acting on learning sample S = {(xi, yi)}mi=1 where each example is drawn i.i.d according to D, such that |S| ≥ Q and myj ≥ 1 for every class yj ∈ Y , where myj is the number of examples of real class yj . In the context of multiclass classification, an error measure can be the confusion matrix. Especially, we consider a confusion matrix builds upon the classical definition based on conditional probalities: It is inherent (and desirable) to ’hide’ the effects of diversely represented classes. Concretely, for a given classifier f ∈ F and a sample S = {(xi, yi)}mi=1 ∼ Dm, the empirical confusion matrix DfS = (d̂pq)1≤p,q≤Q of f is defined as follows:\n∀(p, q), d̂pq def = m∑ i=1 1 myi I(f(xi) = q)I(yi = p).\nThe true confusion matrix Df = (dpq)1≤p,q≤Q of f over D corresponds to:\n∀(p, q), dpq def = Ex|y=pI ( f(x) = q ) = P(x,y)∼D(f(x) = q|y = p).\nIf f correctly classifies every example of the sample S, then all the elements of the confusion matrix are 0, except for the diagonal ones which correspond to the correctly classified examples. Hence the more there are non-zero elements in a confusion matrix outside the diagonal, the more the classifier is prone to err. Recall that in a learning process the objective is to learn a classifier f ∈ F with a low true error (i.e. with good generalization guarantees), we are thus only interested in the errors of f . Our objective is then to find f leading to a confusion matrix with the more zero elements outside the diagonal. Since the diagonal gives the conditional probabilities of ’correct’ predictions, we propose to consider a different kind of confusion matrix by discarding the diagonal values. Then the only non-zero elements of the new confusion matrix correspond to the examples that are misclassified by f . For all f ∈ F we define the empirical and true confusion matrices of f by respectively CfS = (ĉpq)1≤p,q≤Q and C\nf = (cpq)1≤p,q≤Q such that:\n∀(p, q), ĉpq def =\n{ 0 if q = p\nd̂pq otherwise, (9)\n∀(p, q), cpq def = { 0 if q = p dpq = P(x,y)∼D(f(x) = q|p = y) otherwise.\n(10)\nNote that if f correctly classifies every example of a given sample S, then the empirical confusion matrix CfS is equal to 0. Similarly, if f is a perfect classifier over the distribution D, then the true confusion matrix is equal to 0. Aiming at controlling the confusion matrix of a classifier is therefore a relevant task. More precisely, one may aim at a confusion matrix that is ‘small’, where ‘small’ means as close to 0 as possible. As we shall see, the size of a confusion matrix will be measured by its operator norm."
    }, {
      "heading" : "4.2 Main Result: Confusion PAC-Bayes Bound for the Gibbs Classifier",
      "text" : "Our main result is a PAC-Bayes generalization bound that holds for the Gibbs classifier GQ in the particular context of multiclass prediction, where the empirical and true error measures are respectively given by the confusion matrices defined by (9) and (10). In this case, we can define the true and the empirical confusion matrices of GQ respectively by:\nCGQ = Ef∼QES∼DmC f S ; C GQ S = Ef∼QC f S .\nGiven f ∼ Q and a sample S ∼ Dm, our objective is to bound the difference between CGQ and CGQS , the true and empirical errors of the Gibbs classifier. Remark the error rate P (f(x) 6= y) of a classifier f might be directly computed as the 1-norm of tCfp with p the vector of prior class probabilities. A route to get results based on the confusion matrix would then be to have a bound on the induced 1- norm of C (which is defined by: max ‖tCfp‖1/‖p‖1). However, we do not have at hand concentration\nTechnical Report V 4.0 4\ninequalities for the 1-norm of matrices and but we only have at our disposal such concentration inequalities for the operator norm. Since we have ‖u‖1 ≤ √ Q‖u‖2 for any Q-dimensional vector u, we have that P (f(x) 6= y)≤ √ Q‖Cf‖op, trying to minimize the operator norm of Cf might be a relevant strategy to control the risk. This norm will allow us to formally relate the true and empirical confusion matrices of the Gibbs classifier and also to provide a bound on ‖CGQ‖ of the true confusion matrix.\nHere is our main result.\nTheorem 2. Let X ⊆ Rd be the input space, Y = {1, . . . , Q} the output space, D a distribution over X × Y (with Dm the distribution of a m-sample) and F a family of classifiers from X to Y . Then for every prior distribution P over F and any δ ∈ (0, 1], we have:\nPS∼Dm { ∀Q on F , ‖CGQS −C GQ‖ ≤ √ 8Q\nm− − 8Q\n[ KL(Q||P) + ln (m− 4δ )]} ≥ 1− δ,\nwhere m− = miny=1,...,Qmy corresponds to the minimal number of examples from S which belong to the same class.\nProof. Deferred to Section 5.\nNote that, for all y ∈ Y , we need the following hypothesis: my > 8Q, which is not too strong a limitation. Finally, we rewrite Theorem 2 to have the size of the confusion matrix under consideration.\nCorollary 1. We consider the hypothesis of the Theorem 2. We have:\nPS∼Dm { ∀Q on F , ‖CGQ‖ ≤ ‖CGQS ‖+ √ 8Q\nm− − 8Q\n[ KL(Q||P) + ln (m− 4δ )]} ≥ 1− δ.\nProof. By application of the reverse triangle inequality |‖A‖ − ‖B‖| ≤ ‖A−B‖ to Theorem 2.\nFor a fixed prior P on F , both Theorem 2 and Corollary 1 yield a bound on the estimation (through the operator norm) of the true confusion matrix of the Gibbs classifier over all1 posterior distribution Q on F , though this is more explicit in the corollary. Let the number of classes Q be a constant, then the true risk is upper-bounded by the empirical risk of the Gibbs classifier and a term depending on the number of training examples, especially on the value m− which corresponds to the minimal quantity of examples that belong to the same class. This means that the larger m−, the closer the empirical confusion matrix of the Gibbs classifier to its true matrix. These bounds use first-order information and vary as O(1/ √ m−), which is a typical rate of bounds not using second-order information."
    }, {
      "heading" : "4.3 Upper Bound on the Risk of the Majority Vote Classifier",
      "text" : "Our multiclass upper bound given for the risk of Gibbs classifiers leads to an upper bound for the risk of Bayes classifiers in the following way by the Proposition 1. We recall that the Bayes classifier BQ is well known as majority vote classifier under a given posterior distribution Q. In the multiclass setting, BQ is such that for any example it returns the majority class under the measure Q and we define it as:\nBQ(x) = argmaxc∈Y [ Ef∈QI(f(x) = c) ] . (11)\nWe define the conditional Gibbs risk R(GQ, p, q) and Bayes risk R(GQ, p, q) as\nR(GQ, p, q) = Ex∼D|y=pEf∼QI(f(x) = q), (12) R(BQ, p, q) = Ex∼D|y=pI ( argmaxc∈Y [ Ef∈QI(f(x) = c) = q ]) . (13)\nThe former is the (p, q) entry of CGQ (if p 6= q) and the latter is the (p, q) entry of CBQ .\nProposition 1. Given Q ≥ 2 the number of class. The true conditional risk of the Bayes classifier and the one of the Gibbs classifier are related by the following inequality:\n∀(q, p), R(BQ, p, q) ≤ QR(GQ, p, q). (14) 1This includes any Q chosen by the learner after observing S.\nTechnical Report V 4.0 5\nProof. Deferred to Appendix.\nThis proposition implies the following result.\nCorollary 2. Given Q ≥ 2 the number of class. The true confusion matrix of the Bayes classifier CBQ and the one of the Gibbs classifier CGQ are related by the following inequality:\n‖CBQ‖ ≤ Q‖CGQ‖. (15)\nProof. Deferred to Appendix."
    }, {
      "heading" : "5 Proof of Theorem 2",
      "text" : "This section gives the formal proof of Theorem 2. We first introduce a concentration inequality for a sum of random square matrices. This allows us to deduce the PAC-Bayes generalization bound for confusion matrices by following the same “three step process” as the one given in McAllester (2003); Seeger (2002); Langford (2005) for the classic PAC-Bayesian bound."
    }, {
      "heading" : "5.1 Concentration Inequality for the Confusion Matrix",
      "text" : "The main result of our work is based on the following corollary of a result on the concentration inequality for a sum of self-adjoint matrices given by Tropp (2011) (see Theorem 3 in Appendix) – this theorem generalizes Hoeffding’s inequality to the case self-adjoint random matrices. The purpose of the following corollary is to restate the Theorem 3 so that it carries over to matrices that are not self-adjoint. It is central to us to have such a result as the matrices we are dealing with, namely confusion matrices, are rarely symmetric.\nCorollary 3. Consider a finite sequence {Mi} of independent, random, square matrices of order Q, and let {ai} be a sequence of fixed scalars. Assume that each random matrix satisfies EiMi = 0 and ‖Mi‖ ≤ ai almost surely. Then, for all ≥ 0,\nP { ‖ ∑ i Mi‖ ≥ } ≤ 2.Q. exp ( − 2 8σ2 ) , (16)\nwhere σ2 def = ∑ i a 2 i .\nProof. We want to verify the hypothesis given in Theorem 3 in order to apply it. Let {Mi} be a finite sequence of independent, random, square matrices of order Q such that EiMi = 0 and let {ai} be a sequence of fixed scalars such that ‖Mi‖ ≤ ai. We consider the sequence {S(Mi)} of random self-adjoint matrices with dimension 2Q. By the definition of the dilation, we directly obtain EiS(Mi) = 0. From Equation (4), the dilation preserves the spectral information. Thus, on the one hand, we have:\n‖ ∑ i Mi‖ = λmax ( S (∑ i Mi )) = λmax (∑ i S(Mi) ) .\nOn the other hand, we have:\n‖Mi‖ = ‖S(Mi)‖ = λmax ( S(Mi) ) ≤ ai.\nTo assure the hypothesis S(Mi)2 4 A2i , we need to find a suitable sequence of fixed self-adjoint matrices {Ai} of dimension 2Q (where 4 refers to the semidefinite order on self-adjoint matrices). Indeed, it suffices to construct a diagonal matrix defined as λmax ( S(Mi) ) Id2Q for ensuring S(Mi)2 4 ( λmax ( S(Mi) ) Id2Q )2 .\nMore precisely, since for every i we have λmax ( S(Mi) ) ≤ ai, we fix Ai as a diagonal matrix with ai on the diagonal, i.e. Ai def = aiId2Q, with ‖ ∑ iA 2 i ‖ = ∑ i a 2 i = σ\n2. Finally, we can invoke Theorem 3 to obtain the concentration inequality (16).\nTechnical Report V 4.0 6\nIn order to make use of this corollary, we rewrite confusion matrices as sums of example-based confusion matrices. That is, for each example (xi, yi) ∈ S, we define its empirical confusion matrix by Cfi = (ĉpq(i))1≤p,q≤Q as follows:\n∀p, q, ĉpq(i) def =  0 if q = p1 myi I(f(x) = q)I(yi = p) otherwise.\nwhere myi is the number of examples of class yi ∈ Y belonging to S. Given an example (xi, yi) ∈ S, the example-based confusion matrix contains at most one non zero-element when f misclassifies (xi, yi). In the same way, when f correctly classifies (xi, yi) then the example-based confusion matrix is equal to 0. Concretely, for every sample S = {(xi, yi)}mi=1 and every f ∈ F , our error measure is then\nCfS = m∑ i=1 Cfi .\nIt naturally appears that we penalize only when f errs. Moreover, e further introduce the random square matrices C′\nf i = (ĉ ′ pq(i))1≤p,q≤Q defined by:\n∀p, q, ĉ′pq(i) def =  0 if ĉpq(i) = 0 1\nmyi\n( I(f(xi) = q)I(yi = p)− ES∼(D)m 1\nmy I(f(x) = q)I(y = p)\n) otherwise.\n(17)\nThe term ES∼(D)m 1my I(f(x) = q)I(y = p), when ĉpq(i) 6= 0, is equivalent to the expectation (according to S ∼ (D)m) of the elements ĉpq of CfS , such that p = yi and q = h(xi). Equation (17) is then equivalent to:\n∀p, q, ĉ′pq(i) def = { 0 if ĉpq(i) = 0 ĉpq(i)− 1myi ES∼(D)m ĉpq otherwise.\nFor sake of clarity, given an example (xi, yi) and for every S ∼ (D)m, we denote CfS|i the matrix with at most one non-zero element of coordinates (p, q) equals to ĉpq with p = yi and q = h(xi). Then, we obtain the following definition of C′ f i :\nC′ f i = C f i −\n1\nmyi ES∼DmC f S|i, (18)\nwhich verify EiC′fi = 0. We have yet to find a suitable ai for a given C ′f i . Let λmaxi be the maximum singular value of C ′f i . It is easy to verified that λmaxi ≤ 1myi . Thus, for all i we fix ai equal to 1 myi .\nFinally, with the introduced notations, Corollary 3 leads to the following concentration inequality:\nP { ‖ m∑ i=1 C′ f i ‖ ≥ } ≤ 2.Q. exp ( − 2 8σ2 ) . (19)\nThis inequality (19) allows us to demonstrate our Theorem 2 by following the process of McAllester (2003); Seeger (2002); Langford (2005)."
    }, {
      "heading" : "5.2 “Three Step Proof” Of Our Bound",
      "text" : "First, thanks to concentration inequality (19), we prove the following lemma.\nLemma 1. Let Q be the size of CfS and C ′f i = C f i − 1myi ES∼DmC f S|i defined as in (18). Then the following bound holds for any δ ∈ (0, 1]:\nPS∼Dm { Ef∼P [ exp ( 1− 8σ2 8σ2 ‖ m∑ i=1 C′ f i ‖2 )] ≤ 2Q 8σ2δ } ≥ 1− δ\nTechnical Report V 4.0 7\nProof. For readability reasons, we note C′ f S = ∑m i=1 C ′f i . If Z is a real valued random variable so that P (Z ≥ z) ≤ k exp(−n.g(z)) with g(z) non-negative, non-decreasing and k a constant, then P (exp ((n− 1)g(Z)) ≥ ν) ≤ min(1, kν−n/(n−1)). We apply this to the concentration inequality (19). Choosing g(z) = z2 (nonnegative), z = , n = 18σ2 and k = 2Q, we obtain the following result:\nP { exp ( 1− 8σ2\n8σ2 ‖C′fS‖\n) ≥ ν } ≤ min ( 1, 2Qν−1/(1−8σ 2) ) .\nNote that exp (\n1−8σ2 8σ2 ‖C ′f S‖ ) is always non-negative. Hence it allows us to compute its expectation as:\nE [ exp (1− 8σ2 8σ2 ‖C′fS‖ )] = ∫ ∞ 0 P { exp (1− 8σ2 (8σ2) ‖C′fS‖ ) ≥ ν } dν\n≤ 2Q+ ∫ ∞ 1 2Qν−1/(1−8σ 2)dν = 2Q− 2Q1− 8σ 2\n8σ2\n[ ν−8σ 2/(1−8σ2) ]∞ 1\n= 2Q+ 2Q 1− 8σ2\n8σ2\n= 2Q\n8σ2 .\nFor a given classifier f ∈ F , we have: ES∼Dm [ exp ( 1− 8σ2\n8σ2 ‖C′fS‖\n)] ≤ 2Q\n8σ2 . (20)\nThen, if P is a probability distribution over F , Equation (20) implies that: ES∼Dm [ Ef∼P exp ( 1− 8σ2\n8σ2 ‖C′fS‖\n)] ≤ 2Q\n8σ2 . (21)\nUsing Markov’s inequality2, we obtain the result of the lemma.\nThe second step to prove Theorem 2 is to use the shift given in McAllester (2003). We recall this result in the following lemma.\nLemma 2 (Donsker-Varadhan inequality Donsker and Varadhan (1975)). Given the Kullback-Leibler divergence3 KL(Q‖P) between two distributions P and Q and let g(·) be a function, we have:\nEa∼Q [ g(b) ] ≤ KL(Q‖P) + lnEx∼P [ exp(g(b)) ] .\nProof. See McAllester (2003).\nRecall that C′ f S = ∑m i=1 C ′f i . With g(b) = 1−8σ2 8σ2 b 2 and b = ‖C′fS‖, Lemma 2 implies:\nEf∼Q\n[ 1− 8σ2\n8σ2 ‖C′fS‖2\n] ≤ KL(Q‖P) + lnEf∼P [ exp ( 1− 8σ2\n8σ2 ‖C′fS‖2\n)] . (22)\nThe last step that completes the proof of Theorem 2 consists in applying the result we obtained in Lemma 1 to Equation (22). Then, we have:\nEf∼Q [ 1− 8σ2\n8σ2 ‖C′fS‖2\n] ≤ KL(Q‖P) + ln 2Q\n8σ2δ . (23)\nSince g(·) is clearly convex, we apply Jensen’s inequality4 to (23). Then, with probability at least 1−δ over S, and for every distribution Q on F , we have:(\nEf∼Q‖C′ f S‖ )2 ≤ 8σ 2\n1− 8σ2\n( KL(Q‖P) + ln 2Q\n8σ2δ\n) . (24)\nSince C′ f S = ∑m i=1 [ Cfi − ES∼DmC f i ] , then the bound (24) is quite similar to the one given in Theorem 2.\nWe present in the next section, the calculations leading to our PAC-Bayesian generalization bound.\n2see Theorem 4 in Appendix. 3The KL-divergence is defined in Equation (1). 4see Theorem 5 in Appendix.\nTechnical Report V 4.0 8"
    }, {
      "heading" : "5.3 Simplification",
      "text" : "We first compute the variance parameter:\nσ2 = m∑ i=1 a2i .\nFor that purpose, in Section 5.1 we showed that for each i ∈ {1, . . . ,m}, we can choose ai = 1myi , where yi is the class of the i-th example and myi is the number of examples of class yi. Thus we have:\nσ2 = m∑ i=1 1 m2yi = Q∑ y=1 ∑ i:yi=y 1 m2y = Q∑ y=1 1 my .\nFor sake of simplification of Equation (24) and since the term on the right side of this equation is an increasing function with respect to σ2, we propose to upper-bound σ2:\nσ2 = Q∑ y=1 1 my ≤ Q miny=1,...,Qmy . (25)\nLet m− def = miny=1,...,Qmy, then using Equation (25), we obtain the following bound from Equation (24):(\nEf∼Q[‖C′ f S‖] )2 ≤ 8Q m− − 8Q ( KL(Q‖P) + ln m− 4δ ) .\nThen:\nEf∼Q[‖C′ f S‖] ≤\n√ 8Q\nm− − 8Q\n( KL(Q‖P) + ln m−\n4δ\n) . (26)\nIt remains to replace C′ f S = ∑m i=1 [ Cfi − ES∼DmC f i ] . Recall that CGQ = Ef∼QES∼DmC f S and C GQ S =\nEf∼QCfS , we obtain:\nEf∼Q[‖C′ f S‖] = Ef∼Q [ ‖ m∑ i=1 [ Cfi − 1 myi ES∼DmC f S|i ] ‖ ]\n= Ef∼Q [ ‖ m∑ i=1 [ Cfi ] − m∑ i=1 [ 1 myi ES∼DmC f S|i ] ‖ ] = Ef∼Q [ ‖CfS − ES∼DmC f S‖ ]\n≥ ‖Ef∼Q [ CfS − ES∼DmC f S ] ‖\n= ‖Ef∼QCfS − Ef∼QES∼DmC f S‖ = ‖CGQS −C GQ‖. (27)\nBy substituting the left part of the inequality (26) with the term (27), we find the bound of our Theorem 2."
    }, {
      "heading" : "6 Discussion and Future Work",
      "text" : "This work gives rise to many interesting questions, among which the following ones. Some perspectives will be focused on instantiating our bound given in Theorem 2 for specific multi-class frameworks, such as multi-class SVM Weston and Watkins (1998); Crammer and Singer (2002); Lee et al. (2004) and multi-class boosting (AdaBoost.MH/AdaBoost.MR Schapire and Singer (2000), SAMME Zhu et al. (2009), AdaBoost.MM Mukherjee and Schapire (2011)). Taking advantage of our theorem while using the confusion matrices, may allow us to derive new generalization bounds for these methods.\nAdditionally, we are interested in seeing how effective learning methods may be derived from the risk bound we propose. For instance, in the binary PAC-Bayes setting, the algorithm MinCq proposed by\nTechnical Report V 4.0 9\nLaviolette et al. (2011) minimizes a bound depending on the first two moments of the margin of the Q-weighted majority vote. From our Theorem 2 and with a similar study, we would like to design a new multi-class learning algorithm and observe how sound such an algorithm could be. This would probably require the derivation of a Cantelli-Tchebycheff deviation inequality in the matrix case.\nBesides, it might be very interesting to see how the noncommutative/matrix concentration inequalities provided by Tropp (2011) might be of some use for other kinds of learning problem such as multi-label classification, label ranking problems or structured prediction issues.\nFinally, the question of extending the present work to the analysis of algorithms learning (possibly infinite-dimensional) operators as Abernethy et al. (2009) is also very exciting."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose a new PAC-Bayesian generalization bound that applies in the multi-class classification setting. The originality of our contribution is that we consider the confusion matrix as an error measure. Coupled with the use of the operator norm on matrices, we are capable of providing generalization bound on the ‘size’ of confusion matrix (with the idea that the smaller the norm of the confusion matrix of the learned classifier, the better it is for the classification task at hand). The derivation of our result takes advantage of the concentration inequality proposed by Tropp (2011) for the sum of random self-adjoint matrices, that we directly adapt to square matrices which are not self-adjoint.\nThe main results are presented in Theorem 2 and Corollary 1. The bound in Theorem 2 is given on the difference between the true risk of the Gibbs classifier and its empirical error. While the one given in Corollary 1 upper-bounds the risk of the Gibbs classifier by its empirical error. Moreover we have bound the risk of the Bayes classifier by the one of the Gibbs classifier.\nAn interesting point is that our bound depends on the minimal quantity m− of training examples belonging to the same class, for a given number of classes. If this value increases, i.e. if we have a lot of training examples, then the empirical confusion matrix of the Gibbs classifier tends to be close to its true confusion matrix. A point worth noting is that the bound varies as O(1/ √ m−), which is a typical rate in bounds not using second-order information. The present work gives rise to a few algorithmic and theoretical questions that we have discussed in the previous section.\nAppendix\nTheorem 3 (Concentration Inequality for Random Matrices Tropp (2011)). Consider a finite sequence {Mi} of independent, random, self-adjoint matrices with dimension Q, and let {Ai} be a sequence of fixed self-adjoint matrices. Assume that each random matrix satisfies EMi = 0 and M2i 4 A2i almost surely. Then, for all ≥ 0,\nP { λmax (∑ i Mi ) ≥ } ≤ Q. exp ( − 2 8σ2 ) ,\nwhere σ2 def = ‖ ∑ iA 2 i ‖ and 4 refers to the semidefinite order on self-adjoint matrices.\nTheorem 4 (Markov’s inequality). Let Z be a random variable and z ≥ 0, then:\nP (|Z| ≥ z) ≤ E(|Z|) z .\nTheorem 5 (Jensen’s inequality). Let X be an integrable real-valued random variable and g(·) be a convex function, then:\nf(E[Z]) ≤ E[g(Z)].\nProofs of Proposition 1 and its Corollary 2\nThis section gives the formal proofs of Proposition 1 and its Corollary 2.\nTechnical Report V 4.0 10\nProof of Proposition 1\nConsider a labeled pair (x, y). Let us introduce the notation γq(x) for q ∈ Y such that:\nγq(x) = Ef∼QI(f(x) = q) = ∑\nf :f(x)=q\nQ(q).\nObviously, ∑ q∈Y γq(x) = 1.\nRecall that the conditional Gibbs risk R(GQ, p, q) and Bayes risk R(GQ, p, q) are defined as:\nR(GQ, p, q) = Ex∼D|y=pEf∼QI(f(x) = q) = Ex∼D|y=pγq(x), (12) R(BQ, p, q) = Ex∼D|y=pI(argmaxc∈Y γc(x) = q) (13)\nThe former is the (p, q) entry of CGQ (if p 6= q) and the latter is the (p, q) entry of CBQ . For q 6= y to be predicted by the majority vote classifier, it is necessary and sufficient that\nγq(x) ≥ γc(x), ∀c ∈ Y, c 6= q.\nThis might be equivalently rewritten as:\nI(argmaxc γc(x) = q) = I(∧c,c 6=q γq(x) ≥ γc(x)) (28)\n(note that the expectation of the left-hand side which respect to D|y=p is R(BQ, p, q) —cf. (13)). Now remark that:\nI(∧c,c 6=q γq(x) ≥ γc(x)) = 1⇔ γq(x)− γc(x), ∀c ∈ Y, c 6= q ⇒ ∑\nc∈Y,c6=q\n(γq(x)− γc(x)) ≥ 0\n⇔ ∑\nc∈Y,c 6=q\nγq(x)− ∑\nc∈Y,c6=q\nγc(x) ≥ 0\n⇔ (Q− 1)γq(x)− (1− γq(x)) ≥ 0\n⇔ γq(x) ≥ 1\nQ .\nwhere we have used ∑ c∈Y γc(x) = 1 in the next to last line. This means that:\nI(∧c,c 6=q γq(x) ≥ γc(x)) = 1⇒ I ( γq(x) ≥ 1\nQ\n) = 1,\nfrom which we get:\nI(∧c,c 6=q γq(x) ≥ γc(x)) ≤ I ( γq(x) ≥ 1\nQ\n) ,\nthat is, by virtue of (28):\nI (argmaxc γc(x) = q) ≤ I ( γq(x) ≥ 1\nQ\n) .\nWe then may use that γ ≥ θI (γ ≥ 1/Q) ,∀γ ∈ [0, 1], θ ∈ [0, 1], as illustrated on Figure 1, to obtain\n1 Q I ( γq(x) ≥ 1 Q ) ≤ γq(x)⇔ I ( γq(x) ≥ 1 Q ) ≤ Qγq(x),\nand, combining with the previous inequality:\nI (argmaxc γc(x) = q) ≤ Qγq(x).\nTaking the expectation of both sides with respect to x ∼ D|y=p, we get:\nR(BQ, p, q) ≤ QR(GQ, p, q).\nTechnical Report V 4.0 11\nProof of Corollary 2\nFrom the definitions of R(GQ, p, q) (12) and R(BQ, p, q) (13), we directly obtain from Proposition 1:\nCBQ ≤ QCGQ . (29)\nWe dilate CBQ and QCGQ , then (29) is rewritten as:\nS(CBQ) ≤ S(QCGQ).\nSince all component of a confusion matrix are positive, we have 0 ≤ S(CBQ) ≤ S(QCGQ). We can thus apply the property (6). We obtain:\nλmax(S(CBQ)) ≤ λmax(S(QCGQ)). (30)\nThen, with property (4), (30) is rewritten as:\n‖CBQ‖ ≤ ‖QCGQ‖.\nFinally, by application of (5):\n‖CBQ‖ ≤ Q‖CGQ‖."
    } ],
    "references" : [ {
      "title" : "A new approach to collaborative filtering",
      "author" : [ "J. Abernethy", "F. Bach", "T. Evgeniou", "Vert", "J.-P" ],
      "venue" : null,
      "citeRegEx" : "Abernethy et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Abernethy et al\\.",
      "year" : 2009
    }, {
      "title" : "PAC-bayesian supervised classification: The thermodynamics of statistical learning",
      "author" : [ "O. Springer. Catoni" ],
      "venue" : null,
      "citeRegEx" : "Catoni,? \\Q2007\\E",
      "shortCiteRegEx" : "Catoni",
      "year" : 2007
    }, {
      "title" : "On the algorithmic implementation of multiclass kernel-based vector",
      "author" : [ "K. ArXiv e-prints. Crammer", "Y. Singer" ],
      "venue" : null,
      "citeRegEx" : "Crammer and Singer,? \\Q2002\\E",
      "shortCiteRegEx" : "Crammer and Singer",
      "year" : 2002
    }, {
      "title" : "PAC-Bayesian Generalization Bounds on the Confusion",
      "author" : [ "E. Morvant", "S. Koço", "L. Ralaivola" ],
      "venue" : null,
      "citeRegEx" : "Morvant et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Morvant et al\\.",
      "year" : 2007
    }, {
      "title" : "An improved predictive accuracy bound for averaging",
      "author" : [ "Research", "J. 6:273–306. Langford", "M. Seeger", "N. Megiddo" ],
      "venue" : null,
      "citeRegEx" : "Research et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Research et al\\.",
      "year" : 2001
    }, {
      "title" : "PAC-bayes & margins",
      "author" : [ "J. Langford", "J. Shawe-Taylor" ],
      "venue" : "Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "Langford and Shawe.Taylor,? \\Q2002\\E",
      "shortCiteRegEx" : "Langford and Shawe.Taylor",
      "year" : 2002
    }, {
      "title" : "From PAC-Bayes Bounds to Quadratic Programs",
      "author" : [ "F. Press. Laviolette", "M. Marchand", "Roy", "J.-F" ],
      "venue" : "Processing Systems",
      "citeRegEx" : "Laviolette et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Laviolette et al\\.",
      "year" : 2011
    }, {
      "title" : "Multicategory support vector machines, theory, and application",
      "author" : [ "Y. Lin", "G. Wahba" ],
      "venue" : "Majority Votes. In Proceedings of the International Conference on Machine",
      "citeRegEx" : "Y. et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Y. et al\\.",
      "year" : 2004
    }, {
      "title" : "A theory of multiclass boosting. CoRR, abs/1108.2989",
      "author" : [ "I. Mukherjee", "R.E. Schapire" ],
      "venue" : "Computational learning theory (COLT),",
      "citeRegEx" : "Mukherjee and Schapire,? \\Q2011\\E",
      "shortCiteRegEx" : "Mukherjee and Schapire",
      "year" : 2011
    }, {
      "title" : "Improved boosting algorithms using confidence-rated predictions",
      "author" : [ "R.E. Press. Schapire", "Y. Singer" ],
      "venue" : null,
      "citeRegEx" : "Schapire and Singer,? \\Q1999\\E",
      "shortCiteRegEx" : "Schapire and Singer",
      "year" : 1999
    }, {
      "title" : "BoosTexter: A boosting-based system for text categorization",
      "author" : [ "R.E. Schapire", "Y. Singer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Schapire and Singer,? \\Q2000\\E",
      "shortCiteRegEx" : "Schapire and Singer",
      "year" : 2000
    }, {
      "title" : "Multi-class adaboost",
      "author" : [ "J. Weston", "C. Watkins" ],
      "venue" : "Technical Report V",
      "citeRegEx" : "Weston and Watkins,? \\Q1998\\E",
      "shortCiteRegEx" : "Weston and Watkins",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "The essence of PAC-Bayesian results is to bound the risk of the stochastic Gibbs classifier associated with Q (Catoni, 2004) —in order to predict the label of an example x, the Gibbs classifier first draws a classifier f from F according to Q and then returns f(x) as the predicted label. When specialized to appropriate function spaces F and relevant families of prior and posterior distributions, PAC-Bayes bounds can be used to characterize the error of a few existing classification methods. An example deals with the risk of methods based upon the idea of the majority vote in the case of binary classification. We may notice that if Q is the posterior distribution, the error of the Q-weighted majority vote classifier, which makes a prediction for x according to ∑ f f(x)Q(f), is bounded by twice the error of the Gibbs classifier. If the classifiers from F on which the distribution Q puts a lot of weight are good enough, then the bound on the risk of the Gibbs classifier can be an informative bound for the risk of the Q-weighted majority vote. With a more elaborated argument, Langford and Shawe-Taylor (2002) give a PAC-Bayes bound for Support Vector Machine (SVM) which closely relates the risk of the Gibbs classifier and that of the corresponding majority vote classifier, and where the margin of the examples enter into play.",
      "startOffset" : 111,
      "endOffset" : 1122
    }, {
      "referenceID" : 6,
      "context" : "This bound is used to derive an algorithm, namely MinCq (Laviolette et al., 2011), which achieves empirical results on par with state-of-the-art methods.",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001).",
      "startOffset" : 42,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al.",
      "startOffset" : 43,
      "endOffset" : 302
    }, {
      "referenceID" : 1,
      "context" : "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002).",
      "startOffset" : 43,
      "endOffset" : 321
    }, {
      "referenceID" : 1,
      "context" : "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.",
      "startOffset" : 43,
      "endOffset" : 351
    }, {
      "referenceID" : 1,
      "context" : "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.",
      "startOffset" : 43,
      "endOffset" : 457
    }, {
      "referenceID" : 1,
      "context" : "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.",
      "startOffset" : 43,
      "endOffset" : 522
    }, {
      "referenceID" : 1,
      "context" : "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.MH/AdaBoost.MR algorithms of Schapire and Singer (1999) and SAMME algorithm by Zhu et al.",
      "startOffset" : 43,
      "endOffset" : 588
    }, {
      "referenceID" : 1,
      "context" : "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.MH/AdaBoost.MR algorithms of Schapire and Singer (1999) and SAMME algorithm by Zhu et al. (2009). The originality of our work is that we consider the confusion matrix of the Gibbs classifier as an error measure.",
      "startOffset" : 43,
      "endOffset" : 629
    }, {
      "referenceID" : 1,
      "context" : "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.MH/AdaBoost.MR algorithms of Schapire and Singer (1999) and SAMME algorithm by Zhu et al. (2009). The originality of our work is that we consider the confusion matrix of the Gibbs classifier as an error measure. We believe that in the multiclass framework, it is more relevant to consider the confusion matrix as the error measure than the mere misclassification error, which corresponds to the probability for some classifier h to err on x. The information as to what is the probability for an instance of class p to be classified into class q (with p 6= q) by some predictor is indeed crucial in some applications (think of the difference between false-negative and false-positive predictions in a diagnosis automated system). To the best of our knowledge, we are the first to propose a generalization bound on the confusion matrix in the PAC-Bayesian framework. The result that we propose heavily relies on the matrix concentration inequality for sums of random matrices introduced by Tropp (2011). One may anticipate that generalization bounds for the confusion matrix may also be obtained in other framework than the PAC-Bayesian one, such as the uniform stability framework, the online learning framework and so on.",
      "startOffset" : 43,
      "endOffset" : 1533
    }, {
      "referenceID" : 1,
      "context" : "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.MH/AdaBoost.MR algorithms of Schapire and Singer (1999) and SAMME algorithm by Zhu et al. (2009). The originality of our work is that we consider the confusion matrix of the Gibbs classifier as an error measure. We believe that in the multiclass framework, it is more relevant to consider the confusion matrix as the error measure than the mere misclassification error, which corresponds to the probability for some classifier h to err on x. The information as to what is the probability for an instance of class p to be classified into class q (with p 6= q) by some predictor is indeed crucial in some applications (think of the difference between false-negative and false-positive predictions in a diagnosis automated system). To the best of our knowledge, we are the first to propose a generalization bound on the confusion matrix in the PAC-Bayesian framework. The result that we propose heavily relies on the matrix concentration inequality for sums of random matrices introduced by Tropp (2011). One may anticipate that generalization bounds for the confusion matrix may also be obtained in other framework than the PAC-Bayesian one, such as the uniform stability framework, the online learning framework and so on. The rest of this paper is organized as follows. Sec. 2 introduces the setting of multiclass learning and some of the basic notation used throughout the paper. Sec. 3 briefly recalls the folk PAC-Bayes bound as introduced in McAllester (2003). In Sec.",
      "startOffset" : 43,
      "endOffset" : 1996
    }, {
      "referenceID" : 7,
      "context" : "Some perspectives will be focused on instantiating our bound given in Theorem 2 for specific multi-class frameworks, such as multi-class SVM Weston and Watkins (1998); Crammer and Singer (2002); Lee et al.",
      "startOffset" : 141,
      "endOffset" : 167
    }, {
      "referenceID" : 2,
      "context" : "Some perspectives will be focused on instantiating our bound given in Theorem 2 for specific multi-class frameworks, such as multi-class SVM Weston and Watkins (1998); Crammer and Singer (2002); Lee et al.",
      "startOffset" : 168,
      "endOffset" : 194
    }, {
      "referenceID" : 2,
      "context" : "Some perspectives will be focused on instantiating our bound given in Theorem 2 for specific multi-class frameworks, such as multi-class SVM Weston and Watkins (1998); Crammer and Singer (2002); Lee et al. (2004) and multi-class boosting (AdaBoost.",
      "startOffset" : 168,
      "endOffset" : 213
    }, {
      "referenceID" : 2,
      "context" : "Some perspectives will be focused on instantiating our bound given in Theorem 2 for specific multi-class frameworks, such as multi-class SVM Weston and Watkins (1998); Crammer and Singer (2002); Lee et al. (2004) and multi-class boosting (AdaBoost.MH/AdaBoost.MR Schapire and Singer (2000), SAMME Zhu et al.",
      "startOffset" : 168,
      "endOffset" : 290
    }, {
      "referenceID" : 2,
      "context" : "Some perspectives will be focused on instantiating our bound given in Theorem 2 for specific multi-class frameworks, such as multi-class SVM Weston and Watkins (1998); Crammer and Singer (2002); Lee et al. (2004) and multi-class boosting (AdaBoost.MH/AdaBoost.MR Schapire and Singer (2000), SAMME Zhu et al. (2009), AdaBoost.",
      "startOffset" : 168,
      "endOffset" : 315
    }, {
      "referenceID" : 2,
      "context" : "Some perspectives will be focused on instantiating our bound given in Theorem 2 for specific multi-class frameworks, such as multi-class SVM Weston and Watkins (1998); Crammer and Singer (2002); Lee et al. (2004) and multi-class boosting (AdaBoost.MH/AdaBoost.MR Schapire and Singer (2000), SAMME Zhu et al. (2009), AdaBoost.MM Mukherjee and Schapire (2011)).",
      "startOffset" : 168,
      "endOffset" : 358
    }, {
      "referenceID" : 0,
      "context" : "Finally, the question of extending the present work to the analysis of algorithms learning (possibly infinite-dimensional) operators as Abernethy et al. (2009) is also very exciting.",
      "startOffset" : 136,
      "endOffset" : 160
    } ],
    "year" : 2013,
    "abstractText" : "In this work, we propose a PAC-Bayes bound for the generalization risk of the Gibbs classifier in the multi-class classification framework. The novelty of our work is the critical use of the confusion matrix of a classifier as an error measure; this puts our contribution in the line of work aiming at dealing with performance measure that are richer than mere scalar criterion such as the misclassification rate. Thanks to very recent and beautiful results on matrix concentration inequalities, we derive two bounds showing that the true confusion risk of the Gibbs classifier is upper-bounded by its empirical risk plus a term depending on the number of training examples in each class. To the best of our knowledge, this is the first PAC-Bayes bounds based on confusion matrices.",
    "creator" : "LaTeX with hyperref package"
  }
}