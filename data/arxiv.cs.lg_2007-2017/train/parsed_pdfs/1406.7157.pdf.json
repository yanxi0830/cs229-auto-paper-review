{
  "name" : "1406.7157.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Incentive Compatible Multi-Armed-Bandit Crowdsourcing Mechanism with Quality Assurance",
    "authors" : [ "Shweta Jain", "Sujit Gujar", "Satyanath Bhat", "Onno Zoeter", "Y. Narahari" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Consider a company that provides financial advice to a collection of clients on whether to invest in a particular security or not. In order to provide such advice to each client, the company has a pool of financial consultants. Gathering the opinion of as many consultants as possible and aggregating their opinions (for example, using majority voting) increases the probability of providing a high accuracy advice, however, it also entails increased costs. The company\nar X\niv :1\n40 6.\n71 57\nv3 [\ncs .G\nT ]\n1 7\nJu n\n20 15\nhas two conflicting business requirements, firstly to keep the costs low, and secondly, to provide an advice that meets a minimum threshold accuracy. The individual financial consultants have unknown skill sets (qualities) and their costs are typically private information. Since the consultants are strategic, they might report higher costs for their services. On the other hand, to meet an accuracy threshold, the company needs to learn the qualities of the consultants. Assuming the qualities of the consultants do not change with time, their qualities can be learnt by giving them homogeneous tasks.\nAs an abstraction to such problems, we consider a series of homogeneous binary labeling tasks. There is a pool of agents and each agent has different expertise or quality which is fixed but unknown. Since the tasks are homogeneous, the quality of an agent does not change from one task to another. In addition, each agent incurs a cost to perform the task and the cost of an agent is his private information and thus it can be strategically misreported. As a design objective, it is required that the final outcome, obtained by aggregating the answers from the selected agents, achieves a certain target accuracy. The target accuracy level parameter provides a handle on the trade off between cost and accuracy. A high value of the target accuracy level enhances the probability of getting the right answer but at the same time may call for a larger number of agents to be commissioned leading to raise in costs. Therefore, one can choose a suitable target accuracy level as per task sensitivity at hand. In a nutshell, the goal is to select a subset of the strategic agents with minimum cost to achieve a desired accuracy level of the aggregated answer for each task, at the same time giving the right incentives to the agents so that they report their costs truthfully.\nIn the absence of strategic play or when the costs are known, the setting reduces to a machine learning problem. Here, the requester has to select an appropriate set of workers so as to minimize the costs while learning the qualities. Though the requester can learn the qualities of the workers over a period by observing their performance on similar tasks, selecting low quality workers repeatedly may incur significant costs. Thus, the requester faces a dilemma of exploration (where he has to learn the qualities of the workers) versus exploitation (where he has to choose the workers optimally based on learnt qualities). A natural solution to this problem can be be explored using techniques developed for the multi-armed bandit (MAB) problem [28, 3]. In fact many existing works considered learning qualities in non strategic crowdsourcing settings [35, 31]. However, an important challenge in our setting is the need to ensure the accuracy constraint which in turn depends on unknown qualities. Thus, there is a need to develop a new framework to address the accuracy constraint.\nAn additional challenge arises when the costs of the workers are private and the strategic workers try to manipulate the learning algorithm by misreporting their costs so as to benefit themselves. In the strategic version of the above problem, we have the additional task to elicit the true costs using a suitable mechanism. If qualities were known, a natural way to ensure truthfulness is to use a classic mechanism such as the VCG (Vickery-Clarke-Groves) mechanism which satisfies many desirable game theoretic properties. However, since the qualities are not known and need to be learnt, a VCG mechanism cannot be applied directly [6]. Thus, we need to solve the problem of learning qualities while eliciting the costs simultaneously. In short, we need to meld the techniques from machine learning and game theory that would ensure honest behavior of\nthe workers while the requester learns the qualities. Often such mechanisms are referred to as Multi-Armed Bandit Mechanisms or simply MAB mechanisms [6, 5, 16, 32]. The above MAB mechanisms, however, are not designed to achieve a target accuracy level but to only learn the qualities. The MAB mechanism proposed in this paper induces honest behavior, learns qualities and also achieves required target accuracy."
    }, {
      "heading" : "1.1 Contributions",
      "text" : "The above discussion highlights the need to design a new approach to solve the problem of selecting a subset of the strategic workers to achieve a target accuracy level in a cost optimal way. This paper solves such a problem for the first time by modeling this problem in the multi-armed bandit mechanism framework. We consider two versions of this problem: (1) a non strategic version where the costs are known and the qualities of the workers have to be learnt and (2) a strategic version where the costs are to be truthfully elicited as well. In particular, the following are our contributions in this paper.\n• We propose a novel framework, Assured Accuracy Bandit (AAB) where we formulate an optimization problem.\n• We provide a lower bound on the regret that any MAB algorithm in the AAB framework has to suffer (Theorem 3.1).\nWe consider two versions of AAB. Non Strategic Version\n• In this setting, we design a novel algorithm, which we call, Non Strategic Constraint Confidence Bound (CCB-NS).\n• Though the true qualities are not known, our algorithm makes sure that the accuracy constraint is satisfied with high probability (Theorem 4.1).\n• We provide an upper bound on the number of times the algorithm selects a suboptimal worker set for a given problem that depends on the target accuracy level and the true qualities (Theorem 4.2). The upper bound achieved by our algorithm matches the lower bound upto a constant factor.\nStrategic Version\n• In the strategic version of this problem where workers may not report their costs truthfully, we modify the CCB-NS algorithm to an adaptive exploration separated algorithm, which we call Strategic Constrained Confidence Bound (CCB-S) and prove that the allocation rule provided by the CCB-S is ex-post monotone (Theorem 5.2) in terms of the cost.\n• Given this ex-post monotone allocation rule, we adopt the existing techniques [5] to design an ex-post truthful and ex-post individually rational mechanism (Corollary 5.3).\n• For a particular optimization problem, we extend the CCB-S algorithm to the non-exploration separated algorithm by exploiting the specific structure of the optimization problem. For this, we also show the efficacy of our algorithms and compare the algorithms with a variant of εt−greedy algorithm [3] through simulations.\nTo the best of our knowledge, this is the first mechanism that learns the qualities of strategic agents (in this case crowd workers) who have costs as private information where a certain target accuracy level is achieved for each task. In general, MAB mechanisms are popular in the context of sponsored search auctions which are forward auctions. We extend the work to a crowdsourcing context which is a reverse auction setting."
    }, {
      "heading" : "1.1.1 Organization",
      "text" : "The paper is organized as follows. We present a summary of the relevant work in Section 2. In Section 3, we provide a general formulation of the problem. Next, we present our model in two different stages. First, we discuss the non strategic model in Section 4 and next in Section 5, we discuss the strategic version using mechanism design. In Section 6, we provide an extension of the strategic version to a more practical setting where we provide conditions when an approximate solution of the optimization problem can be incorporated and workers can be eliminated in the strategic setting, thus avoiding higher cost in exploration steps. In this Section we also compare our algorithm with a variant of traditional MAB algorithm, εt−greedy algorithm through simulations. Future work and conclusions are provided in Section 7."
    }, {
      "heading" : "2 Related Work",
      "text" : "First, we describe the state of the art addressing the non strategic versions of problems in crowdsourcing such as learning the qualities of the workers to improve the accuracy of the predicted answer. We then look into mechanism design literature in crowdsourcing. Our setting involves both learning and mechanism design. MAB mechanisms provide a natural solution in such setting. We also review relevant MAB problems and MAB mechanism design literature.\nLearning in Crowdsourcing\nHo et. al. [24] considered a similar setting where an assured quality needs to be satisfied for each task. However, they dealt with a specific error probability function with a uniform and known cost of the workers. We address the heterogeneous setting with costs being privately held by strategic workers and we work with any general error probability function. Abraham et. al. [1] consider a setting where a certain accuracy is required to be met for a given micro-task. The authors considered the problem of aggregating answers in a sequential way until a certain accuracy is achieved. Homogeneous workers are assumed in a cluster and thus the goal is to select a single optimal crowd for a single task. In a general setting, their assumption of a crowd having sufficient number of homogeneous quality workers may not hold. Our setting is more general where an optimal subset of workers (arms), with heterogeneous qualities, needs to be selected at one go for a given micro-task. [19] consider a model where, workers have different quality for each variety of task. In order to assign a task to the worker, his quality on the previous similar tasks need to be estimated. However, an assured accuracy model is not considered. Improving the quality of answers while minimizing the cost is considered by Karger et al. [27] where the final answer is predicted using a low rank approximation method. Work by Raykar et\nal. [31] considers learning a classifier while learning the qualities of the workers using EM [15] algorithm. Viappiani et al. [36] consider a Bayesian approach to learn the class label which take noisy observations from experts. Though, the models proposed [36, 15, 31] work well experimentally, there are no analytic guarantees on the predicted outcome. Tran-Thanh et al. [35] present an MAB algorithm for efficient selection of capacitated workers where each worker can perform only limited number of tasks. The authors formulated this as a knapsack problem. For each task, a single non strategic worker is selected as opposed to the subset selection of strategic workers, whose costs need to be elicited.\nThough the literature addresses how to learn the quality of workers, none of the above papers addresses the challenge in meeting the target accuracy level on each task in a heterogeneous cost model. We also consider the strategic version where the costs can be misreported by the workers.\nMechanism Design in Crowdsourcing\nA majority of the literature on mechanism design in crowdsourcing involves design of pricing strategies with online workers. Babaioff et al. [4] use an MAB mechanism to determine an optimal pricing mechanism for a crowdsourcing problem having homogeneous qualities within a specified budget (known as bandits with knapsack). Work by Singla and Krause [34] assumes costs to be private information and proposes a posted price mechanism to elicit the true costs from the users using MAB mechanisms while maintaining a budget constraint. Mechanism design in online procurement auctions [4, 34, 7, 33] considers homogeneous quality workers. Our setting is more general where an auction mechanism is considered to elicit the true costs from the workers with heterogeneous qualities.\nGarg et al. [20] and Bhat et al. [9] consider the costs of the workers to be public and the qualities to be private and strategic. Another line of work involves incentivizing people to work with their true qualities, when the qualities are privately held by the workers [37] in peer prediction markets. Cavallo and Jain [11] analyze crowdsourcing tasks as winner take it all auctions in game theoretic settings. They assume that only one worker gets paid and do not try to learn the qualities over period. [23, 22] adopt techniques from online mechanism design for eliciting the worker preferences but do not address the task accuracy problem.\nMechanism design theory typically has been used in crowdsourcing either to elicit the costs of the workers where qualities are homogeneous and known or to elicit the qualities of the workers assuming the costs to be known. Our work addresses the setting where the qualities of heterogeneous workers are to be learnt and the heterogeneous costs are to be elicited.\nMAB Algorithms\nA rich body of literature is available on the MAB problem. Our problem belongs to the stochastic MAB setting, where the reward of each arm is fixed but unknown. A recent survey by Bubeck and Cesa-Bianchi [10] compiles several variations on stochastic and non-stochastic MAB problems. The setting that is closest to ours is considered by Shipra Agrawal and Nikhil Devanur [2] where a general bandit problem with concave rewards and convex constraints is solved.\nOur problem setting is a further generalization, as the constraints in AAB are not convex. Moreover, the constraint is satisfied in expectation in [2] as opposed to our work, where the constraint needs to be satisfied at each round. The Probably Approximately Correct (PAC) learning framework is considered in [18, 26, 38]. Our learning algorithm may appear closely related to the PAC learning setting but it differs in a subtle but important way. The solution obtained from any PAC algorithm is approximately correct with high probability after arms are pulled for a certain number of rounds, which depends on the provided approximation factor and the confidence. In our setting, the goal is to select an optimal set with high probability since a constraint needs to be satisfied with respect to stochastic qualities. Moreover, the number of exploration steps are adaptive that depends on the true qualities and the target accuracy level as opposed to the fixed number of exploration rounds in the PAC setting. The combinatorial MAB problem introduced by Chen, Wang, and Yuan [13] is relevant to our work. Pure exploration strategy in combinatorial framework is considered in [12] where the objective is to identify an optimal subset from given feasible subsets. However, in our setting collection of feasible sets is not given and has to be learnt over the time and this makes our work different from [13, 12] in the non strategic setting. A constrained MAB problem for single pull is discussed by Ding et. al. [17], where each arm is associated with random rewards and the goal of the algorithm is to maximize the reward such that total cost which is also stochastic in all rounds does not exceed the budget but the constraint is on overall rounds instead of each round.\nMAB Mechanisms\nMulti-armed Bandit mechanisms in the forward setting, in particular, as applied to sponsored search auction are recent advancements that combine the area of MAB problems and mechanism design. Any deterministic truthful MAB mechanism must be exploration separated i.e. allocation in the learning phase should not depend on the bids and thus the regret of any such algorithm is at least O(T 2/3) where T is the total number of rounds [6, 16]. The results are also extended to multiple pull multi-armed bandits i.e. to the case of multiple slot sponsored search auction [21, 32]. The techniques developed in these papers cannot be adopted to this setting because 1) the workers need to be paid in spite of their failure as opposed to the setting where a payment is made only if there is a success or a click 2) the setting is the constrained multi-armed bandit setting as opposed to the setting of traditional multi-armed setting where a best subset of arms is selected without constraints. Also note that, we are considering a reverse auction setting as opposed to the forward auction in the existing literature on MAB mechanisms. Babaioff et al. [5] design a general procedure which takes any monotone allocation rule as input and converts it into a randomized truthful mechanism which implements the input allocation rule with high probability and requires evaluation of the input allocation rule exactly once. As an application of this transformation, an MAB mechanism that is ex-post incentive compatible and ex-post individual rational with regret of O(T 1/2) is proposed. In our current work, we use this transformation and propose an ex-post monotone allocation rule in the case of a reverse auction in a constrained multi-armed bandit setting. Mechanism proposed in [8] can be translated in this setting to balance the trade-off between quality and the cost.\nHowever, the authors do not cater to the final accuracy of the task and selecting a single worker was considered as oppose to our model.\nOur preliminary results appeared in [25] where we considered only a certain type of error probability function to ensure the target accuracy level. This current paper represent a significant improvement over our previous paper and the techniques developed in this paper are applicable to a general class of error probability functions satisfying monotonicity and bounded smoothness properties which we define later."
    }, {
      "heading" : "3 The Model",
      "text" : "Let N be a set of n crowdsourcing workers available for working on T homogeneous crowdsourcing tasks. Each agent or worker i has an associated quality qi ∈ [0.5, 1], which represents the probability that the answer given by him is correct. By homogeneous or similar binary labeling tasks, we mean that each worker’s quality is the same for all the tasks. We assume that the workers are not spammers and their quality of service is at least 0.5. The quality of any worker i is assumed to be independent of the qualities of other workers. A worker i incurs a cost ci ∈ R which is privately held and can be reported strategically by the workers. Let 1 − α be the target accuracy level (α is the threshold level) provided by the requester that determines the trade-off between the cost and the accuracy to be achieved for a particular task. We consider binary classification tasks where the labels are either zero or one. Our model is summarized in Figure 1.\nNotations are summarized in Table 1. The error on a task with inputs from the workers depends on the qualities of the workers and the rule to aggregate these answers. We abstract this as error probability function which we describe in the following subsection."
    }, {
      "heading" : "3.1 Error Probability Function",
      "text" : "Let fS(q) be any function that represents the error probability (hence (1 − fS(q)) captures the accuracy) when a set S is selected with quality profile q = (q1, q2, . . . , qn). The problem we seek to solve in this paper involves minimizing the cost, at the same time, satisfying the constraint that fS(q) < α where (1−α) is the target accuracy level. Depending on the aggregation rule and the requester requirements, different error probability functions could be defined.\nOur framework and the solution approach are general and work for any error probability function that satisfies the following properties:\n• Monotonicity: The error probability function fS(q) is said to be monotone if for all quality profiles q and q′ such that if ∀i ∈ N , q′i ≤ qi, we have,\nfS(q ′) < α =⇒ fS(q) < α, ∀S ⊆ N , ∀α ∈ [0, 1] .\nThat is, an increase in quality of each worker can only increase the accuracy or decrease the error probability.\n• Bounded smoothness: The error probability function fS(q) satisfies bounded smoothness if there exists a strictly increasing, continuous (hence, invertible) function h such that if\nmax i |qi − q′i| ≤ δ =⇒ |fS(q)− fS(q′)| ≤ h(δ), ∀S ⊆ N ,∀q, q′ ∈ [0.5, 1] .\nThat is, when the two quality profiles are close, the difference in error probability function with respect to these quality profiles is bounded by a monotone continuous function h.\nThese properties are similar to the properties satisfied by the reward function in [13] and are satisfied by various error probability functions. Next, we give certain examples of error probability functions that satisfy the properties of monotonicity and bounded smoothness when majority voting is used as an aggregation rule. Note that, the algorithm is general enough to incorporate any aggregation rule and any error probability function if monotonicity and bounded smoothness properties are satisfied."
    }, {
      "heading" : "3.1.1 Examples of error probability functions",
      "text" : "Let S be the selected set with players {1, 2, . . . , s} with the quality profile q, such that q1 ≤ q2 ≤ . . . ≤ qs to whom we assign a certain task t. For notation convenience, we drop t and let ỹi ∈ {−1, 1} be the reported label that we get from the worker i ∈ {1, 2, . . . , s} and ỹ(S) = (ỹ1, ỹ2, . . . , ỹs) be the vector of reported labels from the workers set S. Then, the predicted label ŷ when a majority voting rule is used as an aggregation rule is given by:\nŷ = 1 if ∑ i∈S ỹi > 0,\n0 otherwise. (1)\nExample 3.1 The probability of the most likely outcome that leads to an error is given by [25]:\nP(ES(q)) = max ỹ(S)∈{−1,1}S P(ỹ(S), ŷ 6= y|y)\n= max ỹ(S)∈{−1,1}S\n(P(ŷ 6= y|y, ỹ(S))P(ỹ(S)|y))\n= (1− q1)(1− q2) . . . (1− qs′)qs′+1 . . . qs, where s′ = b((s+ 1)/2)c.\nNote that once y, ỹ(S) is fixed, ŷ 6= y is either true or false and hence P(ŷ 6= y|y, ỹ(S))=0 or 1. P(ỹ(S)|y)) is maximum when the top half quality workers make mistakes in which case, ŷ 6= y. Instead of satisfying the constraints with respect to P(ES(q)), one can satisfy the constraint with respect to the quantity P̂(ES(q)) which is given as:\nfS(q) = P̂(ES(q)) = (1− q1)(1− q2) . . . (1− qs′) . (2)\nNote that fS(q) satisfies monotonicity as well as the bounded smoothness property.\nExample 3.2 The average probability of error is given by [29]:\nP(ES(q)) = P(y = 1)P(ŷ = −1|y = 1) + P(y = −1)P(ŷ = 1|y = −1)\n= πP ( s∑ i=1 ỹi ≤ 0|y = 1 ) + (1− π)P ( s∑ i=1 ỹi > 0|y = −1 ) ,\nwhere π is the probability that true label y is 1. Let us now focus on P ( s∑ i=1 ỹi ≤ 0|y = 1 ) .\nE[ỹi|y = 1] = −P(ỹi = −1|y = 1) + P(ỹi = 1|y = 1) = (2qi − 1) .\nNow,\nP ( s∑ i=1 ỹi ≤ 0|y = 1 ) = P ( s∑ i=1 ỹi − E [ s∑ i=1 ỹi|y = 1 ] ≤ −E [ s∑ i=1 ỹi|y = 1 ] |y = 1 )\n= P ( s∑ i=1 ỹi − E [ s∑ i=1 ỹi ] ≤ − s∑ i=1 (2qi − 1) )\n≤ exp  − ( s∑ i=1 (2qi − 1) )2 2 ∑s i=1 1  (By Hoeffding’s inequality). Similarly, it can be shown that:\nP ( s∑ i=1 ỹi > 0|y = −1 ) ≤ exp  − ( s∑ i=1 (2qi − 1) )2 2 ∑s i=1 1  . Assuming π = 1− π = 0.5, we get,\nP(ES(q)) ≤ fS(q) = exp  − ( s∑ i=1 (2qi − 1) )2 2 ∑s i=1 1  .\nAgain, one can verify that function fS(q) is monotone and satisfies bounded smoothness property. If the quality of every worker i satisfies 12 + ≤ qi ≤ 1, then, (2qi − 1) ≥ 2 and the above expression can be simplified as:\nP(ES(q)) ≤ exp  − ( s∑ i=1 (2qi − 1) )2 2 ∑s i=1 1  ≤ exp ( − s∑ i=1 (2qi − 1) ) = fS(q).\nFrom the above examples we see that it is reasonable to assume monotonicity and bounded smoothness for fS(q).\nNow, we describe our framework in which the optimization problem takes the center stage."
    }, {
      "heading" : "3.2 Assured Accuracy Bandit (AAB) Framework",
      "text" : "Recall that a task t ∈ {1, . . . , T} needs to be completed with an assured accuracy provided by the requester with the optimal cost in a sequential fashion. Thus, for each task t, the goal of the requester is to select a set of workers St, such that the error probability function is less then the threshold level. At the same time, the requester has to make sure that the tasks are completed optimally in terms of costs. Hence for each task t, we need to solve the following optimization problem.\nmin Xti∈{0,1} ∑ i ciX t i , s.t., f{i:Xti =1}(q) < α.\n(3)\nwhere the qualities of the workers are not known a priori and hence need to be learnt by giving tasks repeatedly to the workers. Also, solving the optimization problem, the requester has to make sure that the constraint in (3) is satisfied with respect to the true qualities with high confidence. We refer to this novel framework as Assured Accuracy Bandits (AAB)."
    }, {
      "heading" : "3.2.1 Regret in AAB Framework",
      "text" : "The performance of any MAB algorithm is typically measured by the regret it achieves. Regret in an MAB framework is defined to be the reward difference between the learning algorithm and the optimal algorithm. We will see later that our algorithm is designed in such a way that for each task t, the constraint given by (3) is satisfied with probability (1 − µ), where µ is the confidence parameter with which constraint is satisfied. Thus we can define the regret of an algorithm A if the constraint is satisfied as:\nR(A) = T∑ t=1 ∑ i∈St ci − T ∑ i∈S∗ ci, (4) St is the set selected by the algorithm A for task t,\nS∗ is the optimal set with known qualities.\nSince the constraint for each task is satisfied with probability (1 − µ), we can also bound the total expected regret by:\nE[R(A)] = (1− µ) ( T∑ t=1 ∑ i∈St ci − T ∑ i∈S∗ ci ) + µLT , (5)\nwhere L is the cost that is incurred by the requester if the constraint fails to satisfy. We are considering a setting where value of L is large, and the requester would not want to violate the constraint. However, due to stochasticity involved in learning the qualities, there is a small probability (µ) with which the constraint can be violated. With µ = 1/T , we get the regret expression as:\nE[R(A)] = (1− 1 T ) ( T∑ t=1 ∑ i∈St ci − T ∑ i∈S∗ ci ) + L ."
    }, {
      "heading" : "3.3 Lower Bound on the Regret",
      "text" : "We first start with an important property called as ∆−separated property that we assume any quality profile q satisfies. The property is given as follows:\nDefinition 3.1 (∆-Separated Property:) We say that q is ∆-Separated with respect to the threshold α if ∃∆ > 0 such that, ∆ = infS⊆N |fS(q) − α|. That is, no set of workers, S, has probability of error fS(q) ∈ [α−∆, α+ ∆].\nGiven a quality profile q that satisfies ∆−separated property, we now provide a lower bound on the regret that any algorithm in AAB framework has to suffer.\nTheorem 3.1 Let nS(A) denotes the number of times a worker set S is selected till time T by any algorithm A. Consider any algorithm that solves the optimization problem given by Equation (3) and satisfies E[nS(A)] = o(T a) ∀a > 0 for any subset of worker S which is not optimal. Then, the following holds:\nlim inf T→∞ E[R(A)] ≥ lnT (h−1(∆))2 ,\nwhere, ∆ = infS⊆N |fS(q)− α| and h(.) is the bounded smooth function.\nProof: The proof follows similar steps given in [10] for the lower bound proof of classical MAB problem for Bernoulli reward. For p1, p2 ∈ [0, 1], denote kl(p1, p2) the Kullback-Leibler divergence between a Bernoulli of parameter p1 and a Bernoulli of parameter p2 defined as:\nkl(p1, p2) = p1 ln (p1 p2 ) + (1− p1) ln (1− p1 1− p2 ) .\nIt is easy to see that the function x 7→ kl(p1, x) is a continuous function.\n• Consider two workers with quality profile, q = (q1, q2) with f{1}(q) < α < f{2}(q) and c1 > c2. Since, kl divergence is a continuous function\nand error probability function f is monotone, for any > 0, one can find quality profile q′ = (q1, q ′ 2) such that f{1}(q ′) < f{2}(q ′) < α and kl(q2, q ′ 2) ≤ (1 + )kl(q2, 1 − α). Thus, worker 1 is optimal with quality profile q but worker 2 optimal with quality profile q′. Denote P,E as the probability, expectation taken with respect to random variables generated with quality profile q and P′,E′ as the probability, expectation taken with respect to random variables generated with quality profile q′.\n• Denote X2,1, X2,2, . . . , X2,T as the sequence of successes obtained when allocating tasks to worker 2 where successes are coming from quality profile q. For any t ∈ {1, 2, . . . , T}, let\nk̂lt = t∑ i=1 ln q2X2,i + (1− q2)(1−X2,i) q′2X2,i + (1− q′2)(1−X2,i) .\nNote that, E[k̂ln2(A)] = n2(A)kl(q2, q′2). We also have the following change of measure identity for any event B in the σ−algebra generated by X2,1, X2,2, . . . X2,T :\nP′(B) = E[IB exp(−k̂ln2(A))]. (6)\n• Now, consider the event CT = {n2(A) < 1− kl(q2,q′2) ln(T ) and k̂ln2(A) ≤ (1− 2 ) ln(T )}. We will prove that P(CT )→ 0 as T →∞. From Equation (6):\nP′(CT ) = E[ICT exp(−k̂ln2(A))] ≥ e −(1− /2) ln(T )P(CT ).\nLet fT = 1−\nkl(q2,q′2) ln(T ). Then using Markov’s inequality we have,\nP(CT ) ≤ T (1− /2)P′(CT ) ≤ T (1− /2)P′(n2(A) < fT ) ≤ T (1− /2) E′(T − n2(A))\nT − fT .\nSince there are only two workers here, T − n2(A) = n1(A). With respect to quality profile q′ since worker 1 is sub-optimal, we have:\nP(CT ) ≤ T (1− /2) T a\nT − fT , ∀a > 0.\nConsider a < /2 then we get P(CT )→ 0 as T →∞.\n• Now, we will prove that P(n2(A) < fT )→ 0 as T →∞. We have,\nP(CT ) ≥ P ( n2(A) < fT and max\nt≤fT k̂lt ≤\n( 1−\n2\n) ln(T ) ) = P ( n2(A) < fT and kl(q2, q ′ 2)\n(1− ) ln(T ) max t≤fT k̂lt ≤ 1− /2 1− kl(q2, q ′ 2)\n) .\nUsing the maximal version of the strong law of large numbers and since kl(q2, q ′ 2) > 0 and 1− /2 1− > 1, we get:\nlim T→∞\nP ( kl(q2, q ′ 2)\n(1− ) ln(T ) max t≤fT k̂lt ≤ 1− /2 (1− ) kl(q2, q ′ 2)\n) = 1.\nThus, from the previous point we get P(n2(A) < fT ) → 0 as T → ∞. Thus, we get,\nE[n2(A)] > 1− 1 +\nln(T )\nkl(q2, 1− α) .\nUsing the fact that kl(p1, p2) ≤ (p1−p2) 2\np2(1−p2) and worker 2 is suboptimal with\nquality profile q we get: E[R(A)] > (\n1− 1 +\n) ln(T )\nkl(q2, 1− α) (c2 − c1) ≥ ( 1− 1 + ) α(1− α) ln(T ) (q2 − (1− α))2 (c2 − c1).\n• Since, (1−α) is the target accuracy, fS(1−α) = α, for any subset S. From the bounded smoothness property, |f{2}(q) − f{2}(1 − α)| ≤ h(q2 − (1 − α)) =⇒ (q2−(1−α))2 ≥ (h−1(|f{2}(q)−α|))2. If we choose q1 and q2 such that ∆ = |f{2}(q)−α| then we get E[R](A)] > ( 1− 1+ ) α(1−α) ln(T )\n(h−1(∆))2 (c2−c1), yielding the lower bound.\nThe above theorem proves that, in order to reach to the optimal solution in AAB framework, it is required to pull a sub-optimal arm atleast O (\nln(T ) (h−1(∆))2 ) number of times. Here, ∆ depends on the problem instance."
    }, {
      "heading" : "3.4 Related MAB Algorithms",
      "text" : "There are various MAB algorithms for different related settings. Here we list the two algorithms closest to our setting."
    }, {
      "heading" : "3.4.1 The UCB Algorithm",
      "text" : "The UCB algorithm proposed by Auer et al. [3] works for the classical MAB problem where only one arm is pulled at any given time. This algorithm maintains an upper confidence bound (hence the name UCB) on each arm which depends on its empirical reward as well as on the exploration factor to give the arm enough number of pulls. The algorithm achieves a regret of O(lnT ) and it is the best possible regret that can be achieved. One extension to the multiple pull setting is to pull the arms in the increasing order of their upper confidence bound."
    }, {
      "heading" : "3.4.2 The CUCB Algorithm",
      "text" : "The CUCB algorithm [13] generalizes UCB to the combinatorial setting. At each time, a subset of arms is pulled and the rewards of all the selected arms are revealed. The algorithm works for general non-linear reward functions as long as monotonicity and bounded smoothness properties are satisfied by the reward functions. The general idea of the algorithm is to select the subset such that the reward is maximized with respect to the upper confidence bounds of all the arms similar to the UCB algorithm where a single arm is selected with the highest upper confidence bound. The CUCB algorithm achieves a regret of O(lnT ). The AAB framework handles an unknown stochastic constraint given by fS(q) < α in contrast to the CUCB algorithm where the reward function is stochastic with known constraints.\nWe now provide some realistic assumptions that we have made in our model to design algorithms for AAB framework."
    }, {
      "heading" : "3.5 Assumptions",
      "text" : "• We consider a series of binary classification tasks as an abstraction to our problem.\n• We assume that the error probability function satisfies the assumptions of monotonicity and bounded smoothness. These assumptions are natural and are satisfied by many interesting error probability functions.\n• We assume that the true label is observed once the task is completed. To motivate this assumption, we recall the trading example given in the introduction section where the company and the clients can realize the true label by the end of the day, for example, in an intraday trading.\n• We assume that if all the workers are selected, then the constraint is always satisfied with respect to the true qualities. Thus, if qualities are only partially learnt, then the algorithm can select the complete set and satisfy the constraint. This is equivalent to saying that there are enough good workers."
    }, {
      "heading" : "4 Non Strategic Version",
      "text" : "In this setting, our goal is to learn the qualities of the workers while assuming costs to be publicly known. We solve a general optimization problem given by Equation (3) with unknown qualities which are to be learnt over a period of time. Since the workers obtain their labels according to the true qualities, the constraint has to be satisfied with respect to the true qualities. Since these qualities are unknown to the requester, he has to make sure that the constraint is satisfied with high probability. Note that our algorithm works in a general setting with any aggregation rule and with any error probability function that satisfies the monotonicity and the bounded smoothness properties. Thus, the algorithm uses the aggregation rule as a black box.\nDefinition 4.1 (Aggregate) An aggregate function takes the noisy labels of the selected set as input and produces a label ŷ which best captures the opinion\nof labeler set. We call this label as the aggregated label. The aggregate function should ensure that the resulting error probability function satisfies the properties of monotonicity and bounded smoothness. For example, if the majority voting rule is used, the aggregated label ŷ is computed using the equation (1).\nWe now present the non strategic version of the Constrained Confidence Bound algorithm (CCB-NS) that satisfies the constraint for each task with high probability."
    }, {
      "heading" : "4.1 CCB-NS Algorithm",
      "text" : "The CCB-NS algorithm (presented in Algorithm 1) works on the principle of the UCB algorithm [3] and ensures that the constraint in (3) is satisfied with high confidence µ. Input to the algorithm is parameter α, the target accuracy (which is assumed to be same for all the tasks), the number of tasks T , the number of workers n, and confidence level µ with which the constraint in (3) is required to be satisfied. The output of the algorithm will be the subset St and predicted label ŷt for each task t. The predicted label ŷt is decided based on an aggregation function (AGGREGATE) defined in 4.1 with noisy labels collected from the worker set St as input.\nInitially all the workers are selected to have some estimate about the qualities (Step 2). Their reported labels are aggregated and a label is predicted. Next, the algorithm observes the true label and updates the mean quality estimates, the upper and lower confidence bounds. Let ni(t) denote the number of times the ith worker is assigned the task and ki(t) denote the number of times the worker has provided the correct label up to the task t. Similar to the UCB algorithm [3], the algorithm maintains upper confidence and lower confidence bounds on qualities. These bounds are given as follows:\nq̂+i (t) = q̂i(t)+\n√ 1\n2ni(t) ln\n( 2n\nµ\n) , q̂−i (t) = q̂i(t)− √ 1\n2ni(t) ln\n( 2n\nµ\n) , where q̂i(t) = ki(t)\nni(t) .\nBy Hoeffding’s inequality, one can prove that the true quality qi lies between q̂−i (t) and q̂ + i (t) with probability 1− µ n for any task t and for any worker i. The bounds used by UCB1 algorithm given in [3] is given by:\nq̂+i (t) = q̂i(t) +\n√ 2 ln t\nni(t) , q̂−i (t) = q̂i(t)−\n√ 2 ln t\nni(t) .\nSince, we want to extend the algorithm to the strategic setting, we are using a constant term 2nµ in the bounds instead of t. We initialize q̂ + i (t) and q̂ − i (t) by 1 and 0.5 respectively as the true qualities of the workers lie between [0.5, 1]. In the algorithm, we represent q̂i, q̂ + i and q̂ − i to be the estimates till t number of tasks. The key idea in our algorithm is, till we have identified the optimal subset of workers, we solve the optimization problem using the upper confidence bound on the qualities which gives a cost effective subset. However, this subset need not meet the desired accuracy. Hence we add another subset of the workers from the remaining workers (using subroutine MINIMAL) that combined together ensures that the target accuracy is met even when we use the lower estimates, that is q̂− in the constraints. The fact that qi ≥ q̂−i with probability at least\nALGORITHM 1: CCB-NS Algorithm\nInput: Set of workers N , number of tasks T , parameter α, confidence level µ\nOutput: Labeler selection set St, Label ŷt for all tasks t ∈ {1, 2, . . . , T} 1 ∀i ∈ N , q̂+i = 1, q̂ − i = 0.5, ki(1) = 0 // Initialize UCB and LCB on\nqualities 2 S1 = N // Select all workers initially 3 Observe ỹ(S1) and ŷ1 = AGGREGATE(ỹ(S1)) (Definition 4.1) 4 Observe true label y1 5 ∀i ∈ N , ni(1) = 1, ki(1) = 1 if ỹ1i = y1 and q̂i = ki(1)/ni(1) 6 t = 2\n7 St = arg min S⊆N ∑ i∈S ci s.t. fS(q̂ +) < α 8 while fSt(q̂ −) > α do 9 // Explore (not the optimal set, add more workers to satisfy the constraint)\n10 St = St ∪MINIMAL(St,N \\ St, q̂−) 11 Observe labels of selected labelers ỹ(St) 12 ŷt = AGGREGATE(ỹ(St)) 13 Observe true label yt 14 for i ∈ St do 15 ni(t) = ni(t− 1) + 1 16 if ỹti = y\nt then 17 ki(t) = ki(t− 1) + 1\n18 q̂i = ki(t)/ni(t), q̂ + i = q̂i +\n√ 1\n2ni(t) ln( 2nµ ), q̂−i = q̂i − √ 1 2ni(t) ln( 2nµ )\n19 t = t+ 1\n20 St = arg min S⊆N ∑ i∈S ci s.t. fS(q̂ +) < α 21 t∗ = t 22 St ∗ = St 23 for t = t∗ + 1 to T do 24 // Exploit (optimal set with high probability) 25 St = St ∗ 26 Observe labels of selected labelers ỹ(St) 27 ŷt = AGGREGATE(ỹ(St))\n28 Subroutine: MINIMAL(St, S, q) 29 Return a minimal set S′ ⊆ S of workers such that fSt∪S′(q) < α 30 If no such set S′ exists then return S\n1− µn and the monotonicity of the error function ensures that the target accuracy level is achieved in each round with high probability. Once the algorithm finds a subset that is optimal with respect to the upper confidence on qualities and achieves the target accuracy even when using the lower confidence on qualities,\nthe algorithm stops learning and uses this set for the remaining tasks. We prove (Lemma 4.1) that this is the required optimal set with high probability. Note that, in Step 10 if the MINIMAL function cannot find a set satisfying the target accuracy level using the lower confidence bound, then it simply returns N which meets the target accuracy level by our assumption.\nWe first see that the algorithm CCB-NS satisfies the constraint at each round with high probability. Note that by Hoeffding’s inequality, for each i, q̂−i ≤ qi ≤ q̂+i with probability 1 − µ n . Since the workers make error independently, we have, ∀i ∈ N q̂−i ≤ qi ≤ q̂ + i with probability ( 1− µn\n)n ≥ (1− µ) by Bernoulli’s inequality. Thus, ∀i ∈ N q̂−i ≤ qi ≤ q̂ + i with probability greater then 1 − µ. For brevity of notation, in the rest of the paper we will use q̂− ≤ q ≤ q̂+ to represent q̂−i ≤ qi ≤ q̂ + i ∀i ∈ N .\nFrom this, we make an important observation, as with probability at least, 1− µ, q̂− ≤ q ≤ q̂+ and monotonicity of f(.),\nw.p. at least 1− µ, fS(q̂+) < fS(q) < fS(q̂−) ∀S ⊆ N (7)\nTheorem 4.1 The CCB-NS algorithm satisfies the constraint in Equation (3) with probability at least 1-µ at every round t.\nProof:\n• By our assumption that if all the workers are selected, the constraint is always satisfied, thus, in the rounds in which all the workers are selected, the constraint is satisfied.\n• Now, if set St is returned by CCB-NS, then, fSt(q̂ −) < α =⇒ fSt(q) < α with probability 1− µ (From Equation 7).\nWe now show that if the algorithm exits the while loop in Step 21 then the set St ∗ = S∗ (the optimal set) with probability at least 1 − µ. For simplicity, in the rest of the paper, we assume that there exists a unique optimal set S∗, though the results can be easily generalized to the case where there are multiple optimal sets. Lemma 4.1 Set St ∗\nreturned by the CCB-NS algorithm is an optimal set with probability (w.p.) at least 1− µ. That is, C(St∗) = C(S∗) w.p. 1− µ. Proof: Let t∗ be the round in which CCB-NS stops exploring. At t = t∗,\n• Since fS∗(q) < α, we have, fS∗(q̂+) < α with probability 1 − µ (From Equation (7)).\n• Let St ∗\nbe the set of workers selected by CCB-NS. As CCB-NS solves the optimization problem 3,\nC(St ∗ ) ≤ C(S∗) .\nAt t∗,\nfSt∗ (q̂ −) < α⇒ fSt∗ (q) ≤ fSt∗ (q̂−) < α with probability at least 1− µ,\n⇒ C(St ∗ ) = C(S∗) ."
    }, {
      "heading" : "4.2 Regret Analysis of CCB-NS",
      "text" : "In this section, we aim to bound the number of non-optimal rounds for the CCB-NS algorithm presented in Algorithm 1.\nDefinition 4.2 (Non-optimal Subset) We say that at round t, a set St selected by the algorithm is a non-optimal subset, if St 6= S∗.\nDefinition 4.3 (Non-optimal Round:) We say a round t is a non-optimal round if the selected set St is not the optimal set S∗.\nWe bound the number of exploration steps. Since the algorithm selects a set which satisfies the constraint for each task with high probability, we can bound the overall regret by bounding the number of rounds in which the algorithm selects a sub-optimal set St i.e. C(St) > C(S∗). If C(St) = C(S∗), then we get zero regret for those rounds with probability (1 − µ). We will show that the number of non-optimal rounds depends on the value of ∆ where ∆ = infS⊆N |fS(q) − α|. The value of ∆ is typically unknown to the requester since qualities are unknown but our algorithm does not require the value of ∆ beforehand and thus, CCB-NS is adaptive in nature.\nLemma 4.2 If ∀i ∈ N , number of times a worker i is selected till tasks t, ni(t) ≥ 2(h−1(∆))2 ln( 2n µ ), then for any task t,\n1. ∀S ⊆ N , S 6= S∗, fS(q) > α =⇒ fS(q̂+) > α with probability 1− µ.\n2. fS∗(q̂ −) < α with probability 1− µ.\nProof: Let l = 2(h−1(∆))2 ln( 2n µ ).\n• By Hoeffding’s inequality, q̂+i −qi ≤ 2 √ 1 2ni(t) ln( 2nµ ) ≤ 2 √ 1 2l ln( 2n µ ) ∀ni(t) ≥\nl, with probability 1− µn .\n• Substituting l = 2(h−1(∆))2 ln( 2n µ ), q̂ + i −qi ≤ h−1(∆) with probability 1− µ n .\nThus, q̂+ − q ≤ h−1(∆) with probability 1− µ.\n• By bounded smoothness and monotonicity, fS(q)−fS(q̂+) ≤ h(h−1(∆)) ≤ ∆ with probability 1− µ.\n• Similarly, ∀S ⊆ N , fS(q̂−)− fS(q) ≤ ∆ with probability 1− µ.\nThus, fS(q̂ +) ≥ fS(q)−∆ and fS∗(q̂−) ≤ fS∗(q) + ∆. Thus, fS(q) > α⇒ fS(q̂+) > α (∵ fS(q) > α+ ∆,∆-separated property). And, fS∗(q̂ −) < α (∵ fS∗(q) < α−∆,∆-separated property).\nLemma 4.3 If a non-optimal set St is selected for the task t then there exists a worker i ∈ St such that ni(t) ≤ 2(h−1(∆))2 ln( 2n µ ) with probability 1− µ.\nProof: A non-optimal subset St could be selected in two ways:\n• fSt(q̂+) < α but fSt(q) > α,\n• fS∗(q̂−) > α.\nFrom Lemma 4.2, if ni(t) ≥ 2(h−1(∆))2 ln( 2n µ ) ∀i ∈ S t, then, both the conditions are violated and thus a non-optimal subset is not selected.\nTheorem 4.2 The number of non-optimal rounds by the CCB-NS algorithm is bounded by 2n(h−1(∆))2 ln( 2n µ ) with probability 1− µ.\nProof:\n• Lemma 4.1 shows that the CCB-NS exploitation rounds are optimal rounds.\n• A new parameter ui(t) is associated with each worker. Whenever a set St is selected then, ui(t) = ui(t) + 1 s.t i ∈ St and i = arg min\nj∈St uj(t).\n• Every time a non-optimal subset St is selected, ui(t) of only one worker is updated with the lowest value of ui(t) so far, such that i ∈ St. Thus, ui(t) ≤ ni(t) ∀i ∈ N ∀t ∈ {1, . . . , T}.\n• Thus, from Lemma 4.3, the number of exploration rounds is bounded by 2n\n(h−1(∆))2 ln( 2n µ ) with probability 1− µ.\nHence the theorem follows.\nCorollary 4.3 The total expected regret is bounded by( 1− 1\nT\n) 2n\n(h−1(∆))2 ln(2nT )C(N ) + L,\nwhere L is the loss incurred by the requester if the constraint is not satisfied.\nThe above corollary can be obtained by substituting µ = 1/T . We see that the regret by CCB-NS algorithm matches the lower bound in AAB framework up to a constant factor. In this section, we assumed that the costs are known. However, in real world situations, we need to elicit them truthfully from the strategic workers which we address next."
    }, {
      "heading" : "5 Strategic Version",
      "text" : "Without proper incentives, the strategic agents may not report their costs truthfully. In this section, we propose an algorithm which we call, CCB-S that satisfies a certain monotonicity property and then present a mechanism design implementation. We will first describe the game theoretic version of the problem addressed in the previous section."
    }, {
      "heading" : "5.1 The Model",
      "text" : "Denote the true cost of a worker i by ci and the reported cost by ĉi. The valuation of a worker i is given by vi = −ci. We denote the requester as agent\n0 and the valuation of the requester, when the task is allocated to the worker set S, by:\nv0(S) =\n{ R if fS(q) < α,\n−L otherwise.\nHere, fS(q) is the error probability function that satisfies monotonicity and bounded smoothness properties. We denote 1− α as the target accuracy level. The parameter R denotes the reward that the requester gets for satisfying the constraint and L denotes the loss he incurs if the constraint in Equation (3) is not satisfied. Note that the requester is not considered to be strategic. Social welfare W (S) is given by:\nW (S) =  R− ∑ i∈S ci if fS(q) < α, −L− ∑ i∈S ci otherwise.\nA mechanismM is denoted by the pair (A,P), where A = (A1,A2, . . . ,An) is the allocation vector where Ai represents number of tasks allocated to worker i and P = (P1,P2, . . . ,Pn) is the payment vector where Pi denotes the total payment made to the worker i which depends on the reported cost profile ĉ. We work in a quasi-linear setting where the utility of every agent is given by:\nui(ci, ĉ; q) = −ci.Ai(ĉ; q) + Pi(ĉ; q) .\nWe consider the problem where a heavy penalty is incurred for providing the wrong answer and thus, the parameter L is large. We now review some of the desirable properties that the mechanism M should satisfy:\nDefinition 5.1 (Incentive Compatible) A mechanism M = (A,P) is said to be Incentive Compatible if reporting true valuations is a dominant strategy for all the workers. That is, ∀i ∈ N ,\n− ciAi(ci, ĉ−i; q) + Pi(ci, ĉ−i; q) ≥ −ciAi(ĉi, ĉ−i; q) + Pi(ĉi, ĉ−i; q) ∀ĉi ∈ [0, 1] and ĉ−i ∈ [0, 1]n−1 .\nDefinition 5.2 (Individual Rationality) A mechanism M = (A,P) is said to be individually rational for a worker if participating in the mechanism always gives him positive utility. That is, ∀i ∈ N ,\n−ĉiAi(ĉi, c−i; q) + Pi(ĉi, c−i; q) ≥ 0 ∀ĉi ∈ [0, 1] and ∀c−i ∈ [0, 1]n−1 .\nAn important characterization for incentive compatible mechanisms provided by Myerson [30] states that for a mechanism to be truthful, the allocation rule should be monotone in terms of reported bids by the players. Babaioff et. al. [5] provide a generic transformation that takes any monotone allocation rule and outputs a mechanism which is incentive compatible and individually rational. We can use this generic transformation to design the mechanism in our setting. We first provide the definition of monotonicity in our setting.\nDefinition 5.3 (Monotonicity of Allocation Rule) An allocation rule A is monotone if for every worker i, and for every fixed ĉ−i ∈ [0, 1]n−1,\nĉi ≤ ĉ′i ⇒ Ai(ĉi, ĉ−i; q) ≥ Ai(ĉ′i, ĉ−i; q),\nwhere Ai(ĉi, ĉ−i; q) is the number of tasks given to the ith worker with bids ĉi and ĉ−i.\nSince there is randomness involved due to learnt qualities, let us first define every possible random seed. The random variables are the labels provided by the workers and can affect the learnt qualities and thus the allocation rule.\nDefinition 5.4 (Success Realization) A success realization is a matrix ρ ∈ {0, 1,−1}n×T such that,\nρit =  1 if ỹti = y t,\n0 if ỹti 6= yt, −1 if worker i is not selected for the tthtask.\nWe also define relevant weaker notions of incentive compatibility, individual rationality and monotonicity. Note that, the allocation and payment rule will depend on success realizations when the true qualities are not known.\nDefinition 5.5 (Ex-Post Incentive Compatibility) We say that a mechanism is ex-post incentive compatible if all the bidders are truthful for every success realization irrespective of the bids of other workers, i.e., ∀i ∈ N , ∀ρ ∈ {0, 1,−1}n×T\n− ciAi(ci, ĉ−i, ρ) + Pi(ci, ĉ−i, ρ) ≥ −ciAi(ĉi, ĉ−i, ρ) + Pi(ĉi, ĉ−i, ρ), ∀ĉi ∈ [0, 1], ĉ−i ∈ [0, 1]n−1 .\nDefinition 5.6 (Ex-Post Individual Rationality) We say that a mechanism is ex-post individual rational if for every success realization, truth telling does not give negative utility to any player corresponding to any bids of other players,\n−ĉiAi(ĉi, c−i, ρ) + Pi(ĉi, c−i, ρ) ≥ 0 ∀ĉi ∈ [0, 1], c−i ∈ [0, 1]n−1, ρ ∈ {0, 1,−1}n×T .\nDefinition 5.7 (Ex-Post Monotone Allocation Rule) If the allocation rule is monotone with respect to every success realization then we say that it is ex-post monotone.\nAti(ĉi, c−i; ρ) ≤ Ati(ci, c−i; ρ), ∀i ∈ N , ∀t ∈ {1, 2, . . . , T}, ∀ĉi ≥ ci, ∀ρ . (8)\nBased on the above preliminaries for truthful implementation of a MAB algorithm, we now present the strategic version of CCB-NS algorithm which we call the CCB-S algorithm."
    }, {
      "heading" : "5.2 The CCB-S Algorithm",
      "text" : "As we have seen in the previous section, we need monotonicity of an allocation rule for incentive compatibility. In CCB-NS, if the selected set St in Step 8 does not satisfy the constraint with q̂−, in Step 10, we add agents to satisfy the constraint. This step does not consider strategic costs and thus leads to a violation of monotonicity. Hence, we design a new algorithm which achieves the necessary monotonicity.\nIn order to ensure truthfulness, we modify the CCB-NS algorithm to select all the workers if the constraint is not satisfied with respect to the lower confidence bound (Step 10). We present CCB-S in Algorithm 2. We then show that the allocation rule given by the algorithm is ex-post monotone. Thus, we can\napply results from [5] to achieve an ex-post incentive compatible and ex-post individual rational mechanism. Before going to the formal analysis of CCB-S, we first formally present an important result in [5], which is relevant in our setting:\nTheorem 5.1 [5] Let A be a stochastically monotone (resp., ex-post monotone) MAB allocation rule. There exists a transformation such that the mechanism M obtained by applying the transformation to the allocation rule A satisfies the following properties: (a) M is stochastically truthful (resp., ex-post truthful), and ex-post individually rational. (b) For each success realization, the difference in expected welfare between A and M is at most γn where 0 < γ < 1 is the parameter provided to the transformation.\nLet ACCB−S be the allocation induced by the CCB-S. Using Theorem 5.1, we transform it to ÃCCB−S and payment to be PCCB−S . With this we propose a new mechanism CCB-S, where MCCB−S = {ÃCCB−S ,PCCB−S} for the requester to select a subset of strategic workers with unknown qualities. We now prove that CCB-S has desirable game theoretic properties."
    }, {
      "heading" : "5.3 Analysis of MCCB−S",
      "text" : "As we are using an exploration-seperated allocation rule, one natural way to ensure truthfulness is to apply the classical VCG mechanism. We cannot apply the VCG payment scheme in this algorithm as computing VCG payments requires the computation of an allocation rule in the absence of worker i which cannot be determined by the algorithm since learning stops after computing the optimal set.\nIn order to design an ex-post incentive compatible and ex-post individual rational mechanism, it is enough to design an ex-post monotone allocation rule. Thus, we will show that our algorithm achieves ex-post monotonicity and hence we can achieve an ex-post truthful and ex-post individual rational mechanism (Theorem 5.1).\nTheorem 5.2 The allocation rule given by the CCB-S algorithm (ACCB−S) is ex-post monotone.\nProof: For notation brevity, let us denote ACCB−S by A. In order to prove monotonicity, we need to prove the following:\nAti(ĉi, c−i; ρ) ≤ Ati(ci, c−i; ρ), ∀i ∈ N , ∀t ∈ {1, 2, . . . , T}, ∀ĉi ≥ ci, ∀ρ .\nFor a fixed success realization ρ, let us denote Ati(ĉi, c−i; ρ) by Ati(ĉi, c−i) for notation brevity. Since task t = 1 is given to all the workers irrespective of their bids, we have A1j (ĉi, c−i) = A1j (ci, c−i) = 1 ∀j ∈ N . Let t be the largest time step such that, ∀j, At−1j (ĉi, c−i) = A t−1 j (ci, c−i) = t − 1 (Exploration round with ĉi and ci). And ∃i such that,\nAti(ĉi, c−i) 6= Ati(ci, c−i) .\nSince other costs and quality estimates are the same, this can happen only when in one case worker i is selected, while in the other case worker i is not\nALGORITHM 2: CCB-S Algorithm\nInput: Set of workers N , number of tasks T , parameter α, confidence level µ\nOutput: Labeler selection set St, Label ŷt for all tasks t ∈ {1, 2, . . . , T} 1 ∀i ∈ N , q̂+i = 1, q̂ − i = 0.5, ki(1) = 0 // Initialize UCB and LCB on\nqualities 2 S1 = N // Select all workers initially 3 Observe ỹ(S1) and ŷ1 = AGGREGATE(ỹ(S1)) (Definition 4.1) 4 Observe true label y1 5 ∀i ∈ N , ni(1) = 1, ki(1) = 1 if ỹ1i = y1 and q̂i = ki(1)/ni(1) 6 t = 2\n7 St = arg min S⊆N ∑ i∈S ci s.t. fS(q̂ +) < α 8 while fSt(q̂ −) > α do\n9 // Explore (not the optimal set, select all the workers) 10 St = N 11 Observe judgments of selected labelers ỹ(St) 12 ŷt = AGGREGATE(ỹ(St)) 13 Observe true label yt 14 for i ∈ St do 15 ni(t) = ni(t− 1) + 1 16 if ỹti = y\nt then 17 ki(t) = ki(t− 1) + 1\n18 q̂i = ki(t)/ni(t), q̂ + i = q̂i +\n√ 1\n2ni(t) ln( 2nµ ), q̂−i = q̂i − √ 1 2ni(t) ln( 2nµ )\n19 t = t+ 1\n20 St = arg min S⊆N ∑ i∈S ci s.t. fS(q̂ +) < α 21 t∗ = t 22 St ∗ = St 23 for t = t∗ + 1 to T do 24 // Exploit (optimal set with high probability) 25 St = St ∗ 26 Observe judgements of selected labelers ỹ(St) 27 ŷt = AGGREGATE(ỹ(St))\nselected. Let the two sets of workers selected with ci and ĉi be S(ci) and S(ĉi) respectively. Since the optimization problem involves cost minimization and quality updates are the same, we have,\nAti(ĉi, c−i) = t− 1 which implies i /∈ S(ĉi), Ati(ci, c−i) = t which implies i ∈ S(ci) .\nSince i /∈ S(ĉi), the selected set S(ĉi) satisfies the lower confidence bound too\n(exploitation round with bid ĉi) and thus for the rest of the tasks, only S(ĉi) is selected and thus we have, Ati(ĉi, c−i) ≤ Ati(ci, c−i).\nWe denote the mechanism MCCB−S = (ACCB−S ,PCCB−S). As given by Theorem 5.1, PCCB−S can be derived by applying the transformation given in [5]. Thus, we obtain the following corollary:\nCorollary 5.3 The CCB-S algorithm produces an ex-post incentive compatible and ex-post individual rational mechanism.\nOne can apply the transformation presented in [5] which takes any ex-post monotone allocation rule as input and outputs a randomized mechanism which is ex-post incentive compatible and ex-post individually rational.\nRegret Analysis\nThe proposed algorithm is adaptive exploration separated and the number of exploration steps is determined based on how learning progresses. We will show that t∗ returned by Step 21 of the algorithm is bounded by 2(h−1(∆))2 ln( 2n µ ). In Lemma 5.1, we prove that after l = 2(h−1(∆))2 ln( 2n µ ) steps, there is no set S which satisfies the constraint with respect to upper confidence bound and its cost is less then the optimal cost. Moreover, after l = 2(h−1(∆))2 ln( 2n µ ) steps, we have fS∗(q̂ −) < α with probability 1− µ.\nLemma 5.1 After l = 2(h−1(∆))2 ln( 2n µ ) number of uniform exploration rounds,\n1. for all sets S 6= S∗, fS(q) > α =⇒ fS(q̂+) > α with probability 1− µ\n2. fS∗(q̂ −) < α with probability 1− µ.\nThe proof follows from Lemma 4.2 as after l uniform exploration rounds, we have ni(t) ≥ l, ∀i ∈ N\nAs a result of Lemmas 4.1 and 5.1, we have the following theorem which gives us the bound on the number of non-optimal rounds:\nTheorem 5.4 The number of non-optimal rounds of the CCB-S algorithm is bounded by 2(h−1(∆))2 ln( 2n µ ) with probability 1− µ.\nProof:\n• From Lemma 4.1, the CCB-S exploitation rounds are optimal rounds.\n• From Lemma 5.1, the number of exploration rounds is bounded by 2(h−1(∆))2 ln( 2n µ )\nwith probability 1− µ.\nHence the theorem follows.\nRemark (1): The algorithm CCB-S turns out to be an exploration separated algorithm where the number of exploration steps is adaptive unlike the algorithms presented in [16, 21] and bounds on the number of exploration steps depend on the parameters ∆ and µ.\nRemark (2): When the value of ∆ is very small compared to T i.e. (h−1(∆))2 < 1 T ln ( 2n µ ) , then the algorithm might not converge before T time steps. In a classical MAB algorithm, for example UCB1, there is an inverse dependence on ∆∗ (difference between sub-optimal arm and optimal arm). If ∆∗ is low, then UCB1 suffers a large regret. For practical situations, where ∆ is very low, the requester could provide a range for target accuracy to circumvent high regret. More details are given in Section 6.3.\nWe have the following Corollary that follows from Theorem 5.4:\nCorollary 5.5 The total expected regret is bounded by( 1− 1\nT\n) 2\n(h−1(∆))2 ln(2nT )C(N ) + L,\nwhere L is the loss incurred by the requester if the constraint is not satisfied."
    }, {
      "heading" : "6 Practical Aspects and Experimental Results",
      "text" : "Up until now, our focus was on a combinatorial framework that solves a general optimization problem. A naive implementation of the CCB-NS algorithm and the CCB-S algorithm may lead to two problems: 1) computational complexity of the underlying optimization problem 2) high cost of exploration for the CCB-S algorithm.\nIn practice, often the underlying optimization problems are well studied combinatorial problems. Due to this, often we may still be able to use the AAB framework to address the complexity concerns through efficient approximation algorithms that satisfy monotonicity that we define later. In this section, we consider the majority rule as the aggregation rule and solve Example 3.2 by formulating it as a minimum knapsack problem. The minimum knapsack problem is NP-hard, however, there exists polynomial time greedy approximate algorithm that yields a factor of 2 approximation for this problem.\nTo ensure truthfulness, CCB-S selects all the workers in the exploration steps and this might result in very high cost when n is large. In general, it is difficult to eliminate the low quality or high cost workers due to the combinatorial nature of the problem. However, if there exists a structure to the optimization problem, it is often possible to eliminate the workers. In the approximate solution of the minimum knapsack problem, we show that it is possible to early identify and eliminate the workers of high cost and low quality in CCB-S algorithm. This elimination avoids high cost of exploration."
    }, {
      "heading" : "6.1 Working with Approximate Solutions",
      "text" : "The key to incorporate approximate algorithm in AAB framework is to show its monotonicity in cost. The CCB-S algorithm that uses the solution returned by the monotone approximate algorithm gives a monotone allocation rule which is essential for incentive compatibility.\nDefinition 6.1 (Monotone Algorithm) An algorithm is said to be monotone if the allocation A returned by the algorithm is monotone in cost i.e. if two input instances are (c, q) and (c+, q) such that ci < c + i , for some i and cj = c + j ∀j 6= i, then Ai(c+, q) = 1⇒ Ai(c, q) = 1.\nDefinition 6.2 ((β, γ) Approximate Algorithm) An algorithm is said to be a (β, γ) approximate algorithm, if for some β ≥ 1 and γ ≤ 1, the solution set S returned by the algorithm is such that P[C(S) ≤ βC(S∗)] ≥ γ. Here, S∗ is the solution returned by optimal algorithm.\nProposition 6.1 If there exists a (β, γ) approximation algorithm that is monotone, then, incorporating that (β, γ) approximation scheme in the CCB-S algorithm will result in an ex-post monotone allocation rule.\nThis is easy to see from Theorem 5.2. Note that, all the workers are selected in the exploration rounds, and in exploitation rounds, if a worker i is selected with a certain cost, he will also be selected with a lower cost due to the monotonicity property of the approximate algorithm. Remark: The regret notion in the approximation setting is similar to that in the exact version as the underlying optimization problem with known quality is also solved using the approximate algorithm.\nWe now present an example of the minimum knapsack optimization problem and a greedy solution of the problem. In the greedy solution, it is possible to eliminate the workers without violating monotonicity condition, thus, avoiding high cost of exploration. We present the elimination strategy for this example."
    }, {
      "heading" : "6.2 An Illustrative Example with Low Regret",
      "text" : "From Example 3.2, if all the workers have qualities of at least 23 , i.e. qi > 2/3 and = 1/6, then the optimization problem of minimizing cost and satisfying the accuracy constraint of α can be formulated as follows:\nmin S∈N C(S)\ns.t. ∑ i∈S (2qi − 1) ≥ 6 ln ( 1 α ) This turns out to be the minimum knapsack problem when ci ≥ 0 and\n2qi − 1 ≥ 0 ∀i. Denote ai = 2qi − 1 and M = 6 ln ( 1 α ) , we have the following optimization problem:\nmin S∈N C(S) (9) s.t. ∑ i∈S ai ≥M\n6.2.1 Greedy algorithm (GA)\nThe minimum knapsack problem has a greedy deterministic algorithm which gives a (2, 1)−approximate solution [14]. The algorithm denoted by GA is as follows:\n• Arrange the workers in ascending order of their ci/ai ratio. Without loss of generality, let us assume that the workers are indexed such that c1 a1 ≤ c2a2 ≤ . . . ≤ cn an .\n• Let k1 be the index of the worker such that ∑k1 i ai < M but ∑k1 i ai +\nak1+1 ≥M . Let S0 = {1, 2, . . . , k1}. • Let k2 be the index of the worker such that ∑k1 i=1 ai + aj ≥M ∀k1 + 1 ≤\nj ≤ k2− 1, but ∑k1 i=1 ai + ak2 < M . Let B0 = {k1 + 1, k1 + 2, . . . , k2− 1}.\n• Let k3 be the index of the worker such that ∑k1 i ai + ∑k3 k2 ai < M but∑k1\ni ai + ∑k3 k2 ai + ak3+1 ≥M . Let S1 = {k2, k2 + 1, . . . , k3}.\n• In general let Sl = {k2l, k2l + 1, . . . , k2l+1} and Bl = {k2l+1 + 1, k2l+1 + 2, . . . k2l+2 − 1}, where k0 = 1 and k2l is such that: ∑l j=0 ∑k2j+1 i=k2j ai < M\nbut ∑l j=0 ∑k2j+1 i=k2j ai + am ≥M ∀k2l+1 + 1 ≤ m ≤ k2l+2 − 1.\n• Among the sets, Sl ∪ {j} s.t. j ∈ Bl, pick the set which has the minimum cost and output that as the solution.\nLemma 6.1 [14]. The greedy algorithm GA gives a solution which is (2, 1)−approximate to the optimal solution i.e.\nC(SGA) ≤ 2C(S∗)\nwhere SGA and S∗ are the the solutions returned by the algorithm GA and optimal algorithm respectively.\nLemma 6.2 The allocation rule given by greedy algorithm is monotone in cost i.e. if worker i gets a task with cost ci, he also gets a task with cost c − i when the costs and the qualities of the other workers are fixed and c−i < ci.\nProof: We will prove this case by case. Let worker i be selected with the cost ci. Call the sets S0, S1, . . . , Sq as small sets and the elements of these sets as small elements since the constraint is not satisfied with these elements. Similarly, the sets B0, B1, . . . , Bq are called as big sets and the elements of these sets are called as big elements. Let the set returned by the algorithm be S0∪S1∪ . . .∪Sq∪{j} where, j ∈ Bq with cost ci. Now, consider following cases:\n1. With cost ci, worker i belongs to set Sl where l ≤ q: Now consider the following cases when the cost of worker i is decreased from ci to c − i :\n(a) Worker i remains in Sl but can appear before some other workers in Sl. In this case, nothing will change as no other worker has changed positions. Thus, an optimal solution will still be S0∪S1∪. . .∪Sq∪{j} since cost has only reduced and worker i will get selected.\n(b) Worker i moves to some Sm with m ≤ l. Since i was already in the small set, Sm will remain small. Moreover, all the other workers from small sets till Sq remains small. Thus, the optimal solution will not change and i will get selected.\n(c) Note that the worker i can never become a big element by reducing cost.\n2. With cost ci, worker j = i. Thus, i is a big element with cost ci. Again, consider the following cases when worker i changes his bid to c−i :\n(a) Worker i becomes big element such that i ∈ Bm with m ≤ q. The optimal set will be S0 ∪ S1 ∪ . . . ∪ Sm ∪ {i} and hence i remains in the solution.\n(b) Worker i becomes small such that i ∈ Sm with m ≤ q. Since i was big till set Sq, some worker k from some small set Sl with l ≤ q will become big. Then, the optimal set will be S0 ∪ S1 ∪ . . . Sl ∪ {k} will become optimal and hence i will be selected."
    }, {
      "heading" : "6.2.2 Elimination Strategy with Greedy Algorithm GA",
      "text" : "Let us suppose we have (with suitable relabelling)\nc1 â−1 ≤ c2 â−2 ≤ . . . ≤ ck â−k , (10)\nwhere, â−i = 2q̂ − i − 1. Let the set {1, . . . , k} be such that a set S ⊆ {1, 2, . . . , k} is selected by GA, and thus, they meet the accuracy constraint with their lower confidence bounds. Further, let us suppose there exists an agent r ∈ {k + 1, . . . , n} such that ck\nâ−k ≤ cr â+r and cr ≥ ci,∀i ∈ {1, . . . , k}, then agent r can be\ndiscarded “safely”. By safely, we mean that with qualities known perfectly, GA algorithm has a candidate solution of cost less than or equal to any candidate solution containing r with probability (1− µ). Proof: In the run of GA algorithm with true qualities, the elements 1, . . . , k precedes r due to ck\nâ−k ≤ cr â+r with probability (1− µ).\nAs {1, . . . , k}meets the accuracy constraint with LCB, they meet it with true qualities also (with high probability). Therefore, there exists a p ∈ {1, . . . , k}, which belongs to a big set in the run of GA with true qualities. Any candidate set with p in the run will be of the form ∪qi=1Si ∪ {p}. Any candidate set with r will be of the form ∪li=1Si ∪ {r} with l ≥ q. Therefore, in the run of GA, we can ignore any candidate solutions with r and hence r can be dropped safely. This is because C(∪qi=1Si ∪ {p}) ≤ C(∪lj=1Sj ∪ {r}) as ∪ q i=1Si ⊆ ∪lj=1Sj and cp ≤ cr.\nLemma 6.3 Algorithm GA with elimination strategy described above produces monotone allocation rule in terms of cost.\nProof: Note that in the exploration phase, if the worker i reduces his cost, he can be eliminated at a later stage only and thus the number of allocations in the exploration phase increases. In exploitation phase, the monotonicity is immediate from Lemma 6.2.\nWe call the algorithm GA with elimination strategy as CCB-SE (constrained confidence bound algorithm with strategic elimination). Due to the above Lemma, CCB-SE algorithm can be transformed into incentive compatible mechanism."
    }, {
      "heading" : "6.3 Simulation Results",
      "text" : "In this section, we compare the efficacy of the proposed algorithms via simulations. For simulations, we use the minimum knapsack problem described\nin the previous section which is solved using the greedy algorithm GA. We compare the regret of four algorithms namely, CCB-NS, CCB-S, CCB-SE and a variant of the εt−greedy algorithm. The εt−greedy algorithm [3] solves the classical multi-armed bandit problem which involves the selection of the single best arm. In the classical version, a random arm is explored with probability εt and the optimal arm (with the highest empirical mean) is selected with probability 1− εt. We extend the algorithm to the AAB setting by exploring all the workers with probability εt and with probability (1−εt), we select the minimum cost worker subset which meets the target constraint with the empirically estimated qualities. The parameter εt = min{1, 100t } decreases with time so as to give more weight to exploitation than exploration. Note that, the εt algorithm is not strategyproof.\nIn the simulations, we have selected the number of agents to be 1100. To emphasize the fact that CCB-SE algorithm identifies bad workers early, out of the 1100 workers, 600 workers are chosen with cost as 20 and quality as 2/3 whereas, the other 500 workers are chosen with the costs uniformly drawn between 10 and 20 and the quality uniformly drawn between 2/3 and 1. The required target accuracy is chosen to be 0.9 with α = 0.1. Since the value of ∆ can be arbitrarily low, we adopt the following strategy for the implementation. We solve the optimization problem with UCB for a target accuracy of 0.95 but check the lower confidence bound with target accuracy 0.9. This ensures that the constraint is never violated, however, it may result in extra cost of workers for the rest of the rounds. Since the costs of the workers are not adversarially chosen, the expected difference between the optimal set with accuracy 0.9 and 0.95 is not large. In general, if the requester gives a target accuracy range of (1−α, 1−α+ξ) such that the upper confidence bound is solved using accuracy 1 − α + ξ but the lower bound is checked with accuracy 1 − α, then it is possible to control the number of non-optimal rounds and it can be shown that the number of\nnon-optimal rounds is at most min (\n1 16(h−1(ξ))2 ln ( 2n µ ) , 2(h−1(∆))2 ln ( 2n µ )) . In\nthe εt−greedy algorithm, the worker set is chosen such that the target accuracy of 0.9 is achieved with respect to the estimated qualities. For simulations, we have chosen T to be 104. However, if T is large, one can choose a smaller value of ξ. Over 1200 runs of simulations, we observed that none of the four algorithms violated the stochastic constraint with respect to the true qualities.\nWith ξ = 0.05, the comparison of the average regret and the negative social welfare is given in Figures 2 and 3 respectively. The regret is compared against the greedy solution returned by GA algorithm with true qualities. We ran 1000 samples to generate the graphs. We see that the algorithm CCB-NS converges much faster when compared to the εt−greedy algorithm. We also see that the cost of CCB-SE algorithm reduces significantly in a few iterations only. We also compare the total cost between CCB-NS algorithm and εt−greedy algorithm with change in the number of workers (Figure 4). The simulations show that the CCB-NS algorithm outperforms the εt−greedy algorithm even when there are fewer number of workers."
    }, {
      "heading" : "7 Summary and Future Work",
      "text" : "Motivated by the need for a mechanism where the worker qualities are not known a priori and their costs are private information, we considered the problem of selecting an optimal subset of the workers so that the outcome obtained from aggregating labels from the selected workers attains a target accuracy level. We proposed a novel framework, Assured Accuracy Bandit (AAB) in this setting and developed an algorithm, Strategic Constrained Confidence Bound (CCB-S) for the same, which also leads to an ex-post incentive compatible and ex-post individually rational mechanism. We have provided bounds on the number of exploration steps that depends on the target accuracy level and the true qualities.\nOften, the optimization problem to be solved for each task inherently has exponential time complexity. In most cases, there exist efficient approximate algorithms for solving the optimization problem. If these algorithms are monotone, then the algorithms can be combined with CCB-S algorithm to provide a truthful, IR mechanism.\nAn interesting line of future research could be to improve the convergence rate of CCB-S. The slow convergence of CCB-S can be attributed to the algorithm being exploration separated. If there exists a structure to the algorithm for solving the combinatorial optimization problem, then some strategy for eliminating workers in the strategic setting can be adapted. We have seen a strategy in one example. A generalization of this to all possible optimization problem may require more assumption on the function fS(q) and forms an interesting future direction. Working with soft constraint formulation of this problem forms another extension for the future."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "<lb>Consider a requester who wishes to crowdsource a series of identical<lb>binary labeling tasks to a pool of workers so as to achieve an assured accu-<lb>racy for each task, in a cost optimal way. The workers are heterogeneous<lb>with unknown but fixed qualities and their costs are private. The problem<lb>is to select for each task an optimal subset of workers so that the outcome<lb>obtained after aggregating the labels from the selected workers guaran-<lb>tees a target accuracy level. The problem is a challenging one even in a<lb>non strategic setting since the accuracy of aggregated label depends on<lb>unknown qualities. We develop a novel multi-armed bandit (MAB) mech-<lb>anism for solving this problem. First, we propose a framework, Assured<lb>Accuracy Bandit (AAB), which leads to a MAB algorithm, Constrained<lb>Confidence Bound for a Non Strategic setting (CCB-NS). We derive an<lb>upper bound on the number of time steps the algorithm chooses a sub-<lb>optimal set that depends on the target accuracy level and true qualities.<lb>A more challenging situation arises when the requester not only has to<lb>learn the qualities of the workers but also elicit their true costs. We mod-<lb>ify the CCB-NS algorithm to obtain an adaptive exploration separated<lb>algorithm which we call Constrained Confidence Bound for a Strategic<lb>setting (CCB-S). CCB-S algorithm produces an ex-post monotone alloca-<lb>tion rule and thus can be transformed into an ex-post incentive compatible<lb>and ex-post individually rational mechanism that learns the qualities of<lb>the workers and guarantees a given target accuracy level in a cost optimal<lb>way. We also provide a lower bound on the number of times any algo-<lb>rithm should select a sub-optimal set and we see that the lower bound<lb>matches our upper bound upto a constant factor. We provide insights<lb>on the practical implementation of this framework through an illustrative<lb>example and we show the efficacy of our algorithms through simulations.",
    "creator" : "LaTeX with hyperref package"
  }
}