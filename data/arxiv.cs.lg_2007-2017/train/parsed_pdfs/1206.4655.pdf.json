{
  "name" : "1206.4655.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Modelling transition dynamics in MDPs with RKHS embeddings",
    "authors" : [ "Steffen Grünewälder", "Guy Lever", "Luca Baldassarre", "Arthur Gretton" ],
    "emails" : [ "steffen@cs.ucl.ac.uk", "g.lever@cs.ucl.ac.uk", "l.baldassarre@cs.ucl.ac.uk", "m.pontil@cs.ucl.ac.uk", "arthur.gretton@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s)."
    }, {
      "heading" : "1. Introduction",
      "text" : ""
    }, {
      "heading" : "1.1. Preliminaries",
      "text" : "Throughout we denote expectations by E[·], and the probability over events by P(·). We denote by B(X ) and Cb(X ) the Banach spaces of bounded functions and bounded continuous functions on X , each equipped with the sup-norm || · ||∞.\nWe consider in particular the problem in which we control a trajectory {xt} ∞ t=0 over X by sequentially choosing actions at ∈ A at each time step t ≥ 0, once xt is revealed, after which we receive a reward rt+1 = r(xt, at). We denote a set of deterministic policies Π = AX . The objective is to find a policy π which maximises the expected sum of rewards obtained by following π: E [ ∑∞ t=0 γ trt+1(Xt, At)|X0 = x,At = π(Xt)].\nFor a policy π ∈ Π we denote the associated value function,\nV π(x) := E\n[ ∞∑\nt=0\nγtrt+1(Xt, At)|X0 = x,At = π(Xt) ] ,\nand recall that V π(x) = r(x, π(x)) + γEX∼P (·|x,π(x))[V\nπ(X)]. We define the optimal value function V ∗(x) := maxπ∈Π V\nπ(x) for all x ∈ X , and an optimal policy to be any π∗ such that π∗ ∈ argmaxπ∈Π V\nπ(x) for all x ∈ X . For a given action-value function Q : X × A → R we define the greedy policy w.r.t. Q by πQ(x) := argmaxa∈A Q(x, a) (choosing arbitrarily in the case of a tie) and the optimal action-value function,\nQ∗(x, a) := r(x, a) + γEX∼P (·|x,a)[V ∗(X)], (1)\nso that π∗ = πQ∗ (see e.g. (Szepesvari, 2009) for this background). We require the following well-known result, which is proved in the Appendix for reference (Grünewälder et al., 2012):\n1Equal contribution.\nLemma 1.1. (Singh & Yee, 1994)[Corollary 2] For any action-value function Q : X × A → R, the greedy policy πQ satisfies ||V πQ − V ∗||∞ ≤ 2 1−γ ||Q ∗ −Q||∞.\nWe are interested in the case where P is unknown but a sample S := {(xi, ai, x ′ i)} m i=1 is provided, drawn i.i.d. from a distribution P̃ such that P̃ (X ′i = x ′ i|Xi = xi, Ai = ai) = P (x ′ i|xi, ai) for all i (the marginal probabilities need not match). Note the abuse of notation here – subscripts index samples and not time steps."
    }, {
      "heading" : "1.2. Overview of the approach",
      "text" : "A number of recent studies have focused on efficient evaluation of conditional expectations on functions that are “well behaved” in the sense that they belong to a reproducing kernel Hilbert space (RKHS). These approaches have been particularly successful in performing inference in graphical models, where the model parameters are learned nonparametrically from data (Song et al., 2010b; 2009; 2011). The key insight in these works is that conditional probabilities can be represented as functions in an RKHS, called conditional distribution embeddings. The conditional expectation of any function in the RKHS then becomes a linear operation, where we take the inner product with the appropriate distribution embedding.\nMany methods for solving problems in MDPs require the computation of expectations of functions (value functions for example) with respect to transition dynamics, and so (approximations of) the operators\nf 7→ EX∼P (·|x,a)[f(X)] (2)\nare required. A direct but computationally costly approach would be to first learn a conditional density estimate (difficult in high dimensions), followed by (possibly intractable) integrals to compute the expectation. By contrast, our approach is a two stage process for learning in MDPs: we first use the theory of RKHS embeddings to estimate the operators (2) directly (over a specific class of functions in an RKHS), then use these estimated operators in standard approaches for solving MDPs – here we consider dynamic programming methods for value estimation and policy optimisation. The application to dynamic programming is described in more detail in Sec. 3."
    }, {
      "heading" : "1.3. Advantages of the approach",
      "text" : "A direct kernel-based approach has a number of advantages. First, like density estimates, conditional embeddings can be learned from a training sample: we do not need to address the problem of modeling system dynamics, such as the differential equa-\ntions governing a robot arm. Unlike density estimates, however, distribution embedding estimates do not scale poorly with the dimension d of the underlying space: the risk of a kernel density estimate increases as O(m−4/(4+d)) when the optimal bandwidth is used (Wasserman, 2006)[Sec. 6.5]. By contrast, the rate of convergence for conditional mean embeddings is independent of the dimension of the underlying space (Song et al., 2010b)[Thm. 1].\nSecond, the solution to many control problems involves computation of high dimensional integrals to obtain expectations, which is prohibitively costly. By contrast, RKHS embeddings explicitly provide a representation of the expectation operator as an RKHS inner product, which reduces calculating expectations to a computation of linear complexity in the number of training points used to represent the embedding, and avoids any intermediate problems such as density estimation and sample selection for numerical integration. Thus, the approach provides a framework for alleviating the curse of dimensionality in MDPs (particularly if, for example, sparsification of the embedding is considered, which we address briefly in the Appendix (Grünewälder et al., 2012)). The conditional distribution embeddings themselves may be computed exactly at cost cubic in the training sample size, and approximated to good accuracy at linear cost.\nA third advantage is that we can provide convergence results in the infinite sample case. Thm. 3.2 demonstrates how a performance guarantee for value iteration using embeddings decomposes into guarantees for value iteration and gurantees for the embeddings, upper bounding the difference ||V π̂κ − V ∗||∞ between the optimal value V ∗ and the value V π̂κ of the policy π̂κ found by performing value iteration using the embeddings after κ iterations. This bound contains a term involving how well we can approximate V ∗ in our model class (a chosen RKHS) – which usually corresponds to smoothness assumptions on V ∗ – and can decrease by increasing the richness of the RKHS. A second term captures how quickly we can learn the embeddings for the operator (2) over functions in the chosen RKHS. This bound can be specialised to give convergence guarantees for specific settings by plugging in guarantees for the two components: in Corollary 3.3, we specialise to the common setting of finite state space and positive definite kernel and obtain that ||V π̂κ − V ∗||∞ → 0.\nAs a final advantage, the method applies wherever kernels may be defined, including on high dimensional or continuous state spaces, manifolds (kernels on the surface of a sphere (Wendland, 2005) are of particu-\nlar interest in robotics), and partially observable tasks where only sensor measurements are available."
    }, {
      "heading" : "1.4. Relation to existing methods",
      "text" : "Kernel methods have become increasingly popular in RL. Methods include kernel LSTD (Xu et al., 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor & Parr, 2009). Based on the q-value estimates it is possible to optimise the policy. Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al. (2009), Ormoneit & Sen (1999) and Kroemer & Peters (2011). Here, transition models (densities) are learned with the help of Gaussian processes or kernel density estimates. Using them for value estimation or policy optimisation usually leads to difficult integration to be solved numerically via, e.g., an intermediate sampling method. In contrast we use kernels to directly learn the expectation operators and avoid numerical integration. Finally, in (Parr et al., 2008) a way is proposed to approximate expectations in a low dimensional state representation. In contrast to our approach the paper assumes that the true expectation is known."
    }, {
      "heading" : "2. RKHS embeddings of transition probability kernels",
      "text" : "Given a set Z and a positive semi-definite (p.s.d.) kernel K : Z ×Z → R (see e.g. Steinwart & Christmann, 2008, for details) we denote byHK ⊆ R\nZ its unique reproducing kernel Hilbert space (RHKS), and by 〈·, ·〉K the inner product in HK . Due to the reproducing property of K in HK we have h(x) = 〈K(x, ·), h〉K for all h ∈ HK . We recall the notion of a universal kernel: given a Banach space of functions F ⊆ RZ a kernel is F-universal if HK is dense in F . We denote ρK := supz∈Z √ K(z, z) and refer to kernels K such that ρK < ∞ as bounded kernels.\nFollowing Sriperumbudur et al. (2010), given any probability distribution P and p.s.d. kernel K on a set Z a distribution embedding of P in HK is an element µ ∈ HK such that 〈µ, h〉K = EZ∼P [h(Z)] for all h ∈ HK . In our application, given p.s.d. kernels L : X×X → R and K : (X×A)×(X×A) → R, we are interested in the embedding of the expectation operator (2) corresponding to the state transition probability kernel P , over the domain HL; that is, an element µ(x,a) ∈ HL such that 〈µ(x,a), f〉L = E[f(Xt+1)|Xt = x,At = a], for all f ∈ HL and for all t ≥ 0 – recall that the Markov property implies such a µ(x,a) is independent of time. Recalling Sec. 1.1, given the sample S, we will consider a sample-based estimate of\nthe expectation operator (2). This will be achieved by identifying an element µ(x,a) ∈ HL such that, for all f ∈ HL, 〈µ(x,a), f〉L approximates EX∼P (·|x,a)[f(X)]. Following (Song et al., 2009; 2010b) an estimate is\nµ(x,a) :=\nm∑\ni=1\nαi(x, a)L(x ′ i, ·) ∈ HL, (3)\nwhere αi(x, a) = ∑m j=1 WijK((xj , aj), (x, a)), and where W := (K + λmI)−1, K = (K((xi, ai), (xj , aj))) m ij=1, and λ is a regularization parameter. We assume w.l.o.g. x′i 6= x ′ j for all x′i, x ′ j in the expansion (3).\n1 In some situations, the estimate (3) is consistent in the RHKS norm sense and uniformly over X ×A: the following result, proved in the appendix, follows directly from (Song et al., 2010b)[Thm. 1].\nLemma 2.1. Suppose K is a bounded kernel and the conditions of (Song et al., 2010b)[Thm. 1] are satisfied.2 Then sup(x,a)∈X×A{||µ(x,a) − µ(x,a)||L} ∈ OP̃ (λ 1 2 + λ− 3 2m− 1 2 ), and thus by choosing λ → 0, λ3m → ∞ we have that, for any ǫ > 0,\nPS∼P̃m\n( sup\n(x,a)∈X×A\n||µ(x,a) − µ(x,a)||L > ǫ\n) → 0.\nBy the reproducing property of L, we have\n〈µ(x,a), f〉L =\nm∑\ni=1\nαi(x, a)f(x ′ i)\nIn this work, for theoretical analysis, we consider a normalised version of (3):\nµ̂(x,a) :=\nm∑\ni=1\nα̂i(x, a)L(x ′ i, ·) ∈ HL, (4)\nwhere α̂i(x, a) = αi(x,a)∑\nm j=1 |αj(x,a)|\n. This is a technical con-\nsideration which will later ensure that we can define a\n1We can otherwise form a new expansion in which the x′i are unique by summing any αi(x, a) as necessary.\n2These conditions require that the mapping (x, a) 7→ EX∼P (·|(x,a))[f(X)] be an element of HK for all f ∈ HL, and that the operator CY XC −3/2 XX be HilbertSchmidt, where CY X and CXX are covariance operators: see (Song et al., 2009) or Appendix D.1 for details (Grünewälder et al., 2012). The first condition is a smoothness assumption on the distribution, and for the convergence guarantee of Corollary 3.3 we specialise to the simple setting of finite state space, in which case this condition is trivially satisfied.The second condition is guaranteed in our case when, for example, the marginal density of the initial state X from P̃ is bounded away from zero and the RKHSs HK , HL are of finite dimensionality.\nAlgorithm 1 Estimate Conditional Expectation input Sample of transitions S := {(x1, a1, x ′ 1), . . . , (xm, am, x ′ m)}, kernel K on X ×A and a kernel L on states X output A conditional expectation estimate µ(x,a) Build kernel matrix K for samples {(x1, a1), . . . , (xm, am)} Calculate coefficient vector αi := ∑ j≤m WijK((xj , aj), (x, a)), where W := (K + λmI) −1\nCalculate the estimate µ(x,a) := ∑m i=1 αi(x, a)L(x ′ i, ·)\nAlgorithm 2 Estimate Value\ninput Sample S, policy π, conditional expectation estimate µ(x,a), discount γ, max. number of iterations N , error threshold θ, reward function r output Value estimate V̂ n = 1, error = 1; define a value vector for states x′1, ..., x ′ m:\nV := 0 while n ≤ N and error > θ do\nfor all i ≤ m do V ′(x′i) ← r(x ′ i, π(x ′ i)) + γ〈µ(x′\ni ,π(x′ i )), V 〉L\nend for n ← n+ 1, error ← ||V ′ − V ||∞, V ← V ′\nend while return V̂ (x) = r(x, π(x)) + γ〈µ(x,π(x)), V 〉L\nAlgorithm 3 Approximate Value Iteration\ninput Sample S, discount γ, maximum number of iterations N , reward function r, error threshold θ output µ(x,a), approximate optimal value V̂\nn = 1, error = 1; define a value vector for states x′1, ..., x ′ m: V := 0 Run Alg. 1 to get µ(x,a) while n ≤ N and error > θ do\nfor all i ≤ m do V ′(x′i) ← maxa∈A r(x ′ i, a) + γ〈µ(x′\ni ,a), V 〉L\nend for n ← n+ 1, error ← ||V ′ − V ||∞, V ← V ′\nend while return V̂ (x) = maxa∈A r(x, a) + γ〈µ(x,a), V 〉L\ncertain contraction mapping. We now demonstrate the consistency of the estimators defined by (4) for finite state spaces, by showing that in the limit of large data the normalization of α̂ has no effect. The following lemma is proved in the Appendix (Grünewälder et al., 2012).\nLemma 2.2. Under the conditions of Lemma 2.1, and if |X | < ∞ and L is strictly positive definite, by choosing λ → 0, λ3m → ∞ we have that, for any ǫ > 0,\nPS∼P̃m\n( sup\n(x,a)∈X×A\n||µ(x,a) − µ̂(x,a)||L > ǫ\n) → 0."
    }, {
      "heading" : "3. Application to MDPs",
      "text" : "The learnt embeddings are applied to MDPs by recalling (4) and defining an operator\nÊ(x,a)[f ] := m∑\ni=1\nα̂i(x, a)f(x ′ i). (5)\nWhen f ∈ HL we have that Ê(x,a)[f ] = 〈µ̂(x,a), f〉L ≈ EX∼P (·|(x,a))[f(x)]. When f /∈ HL the quality of the approximation will further depend upon how well f can be approximated by a low norm function in HL. This operator can be used in place of the true unknown expectation operator (2) in any MDP method which makes use of such expectations, such as dynamic programming. As an example below, we analyse value iteration, but similar considerations yield similar analyses for other methods. We summarize a joint value estimation algorithm and policy optimisation approach in the Algorithm boxes above.\nIf we knew P , and could efficiently compute expectations, we could define the Bellman operator B as\n(BV )(x) := max a∈A {r(x, a) + γEX∼P (·|x,a)[V (X)]}, (6)\nwhere we suppose that the image of B is always a measurable function.3 Recall that picking an arbitrary V0 and iterating Vk+1 = BVk converges in sup-norm, Vk → V\n∗ (see e.g. Szepesvari, 2009). Since we do not know P , we use the embeddings µ̂(x,a) and, recalling\n(5), define the operator B̂ : B(X ) → B(X ) as\n(B̂V )(x) := max a∈A {r(x, a) + γÊ(x,a)[V ]}. (7)\nIt is necessary to define B̂ on functions which are not in HL, and this possibility introduces a term in the analysis which captures how well V ∗ can be approximated in HL (See Thm. 3.2). By Lemma 2.2, in the limit of large data, the operator defined by (7) converges to an expectation operator on functions in HL, and thus B̂ can be seen to approximate B defined by (6) on HL. The following result is proved in the Appendix (Grünewälder et al., 2012):\nProposition 3.1. B̂ is a sup-norm contraction on the space B(X ) with Lipschitz constant γ.\nSince B̂ defines a sup-norm contraction mapping on a complete metric space, by Banach’s fixed point theorem (e.g. Granas & Dugundji, 2003) there exists a\n3We suppose for simplicity that any necessary conditions to ensure this are met, since strictly speaking B is defined only on measurable functions, see for example (Bertsekas & Shreve, 1978) for a discussion of the issues. In particular, these conditions are met when |X | < ∞.\nunique fixed point V̂ ∗ of B̂, such that choosing V̂0 arbitrarily and iterating V̂k+1 = B̂V̂k converges, V̂k → V̂\n∗, in sup-norm,\n||V̂k − V̂ ∗||∞ ≤\nγk\n1− γ ||V̂1 − V̂0||∞. (8)\nSuppose we perform κ iterations, obtaining the estimate V̂κ ≈ V̂\n∗. Once V̂κ is obtained we form a policy π̂κ on-the-fly 4 by acting greedily w.r.t. Q̂κ(x, a), where\nQ̂κ(x, a) := r(x, a) + γÊ(x,a)[V̂κ], (9)\nso that the learned policy is\nπ̂κ(x) := argmax a∈A Q̂κ(x, a). (10)\nfor each x in a trajectory.\nConsistency: We now discuss the consistency of π̂κ as an estimate of an optimal policy π∗. The following theorem decomposes the convergence of V π̂κ to the optimal value function V ∗, in terms of the convergence of value iteration, the convergence of the embeddings and how well we can approximate V ∗ in sup-norm by a (low || · ||L-norm) function in HL. This is a generic bound into which we can plug any suitable guarntees for an embedding method. In Corollary 3.3, we specialise the result to the finite state space case, where we can approximate V ∗ arbitrarily well.\nTheorem 3.2.\n||V π̂κ − V ∗||∞ ≤ 2γ\n(1− γ)2\n( γκ||V̂1 − V̂0||∞\n+ 2||V ∗−Ṽ ∗||∞+ sup (x,a) ||µ(x,a) − µ̂(x,a)||L||Ṽ ∗||L\n) , (11)\nwhere Ṽ ∗ is any element of HL. Thus, whenever sup(x,a) ||µ(x,a) − µ̂(x,a)||L → 0 in P̃ -probability, we have that, for any chosen Ṽ ∗ ∈ HL,\n||V π̂κ − V ∗||∞ ≤ 4γ\n(1− γ)2 ||V ∗ − Ṽ ∗||∞ + ǫκ + ǫm,\n(12)\nwhere ǫκ → 0 and ǫm → 0 with convergence in P̃ - probability.\nProof. (Sketch, see Appendix for full proof (Grünewälder et al., 2012).) The proof hinges upon obtaining the following chain of convergences,\nÊ(x,a)[V̂κ] →(a) Ê(x,a)[V̂ ∗] ≈(b) Ê(x,a)[V ∗]\n≈ Ê(x,a)[Ṽ ∗] →(c) EX∼P (·|(x,a))[Ṽ ∗(X)] ≈ EX∼P (·|(x,a))[V ∗(X)].\n4Meaning that we only need to calculate π̂κ(x) at points x in a trajectory as and when required.\nThe convergence (a) is a standard result for contraction mappings, (b) requires a new lemma relating the fixed points of similar contraction mappings, and (c) is possible using Lemma 2.2 because Ṽ ∗ ∈ HL. Once this is obtained we recall that π̂κ is greedy w.r.t. Q̂κ defined by (9), and apply Lemma 1.1, since the optimal policy is greedy w.r.t. Q∗.\nWe now interpret Thm. 3.2. The upper bound is,\n||V π̂κ − V ∗||∞ ≤ 2γ\n(1− γ)2\n( (i)︷ ︸︸ ︷\nγκ||V̂1 − V̂0||∞\n+ 2||V ∗ − Ṽ ∗||∞︸ ︷︷ ︸ (ii) + sup (x,a) ||µ(x,a) − µ̂(x,a)||L||Ṽ ∗||L\n︸ ︷︷ ︸ (iii)\n) .\nHere (i) is the standard difference between the value estimate of the initial policy and the value estimate of the policy that we get after applying one dynamic programming update. This term decreases to 0 with growing κ because γ < 1. (ii) is the distance from the optimal value V ∗ to any approximation Ṽ ∗ in the RKHS, and is therefore small when Ṽ ∗ is close to V ∗ and so can be smaller when HL is chosen to be a richer class. Finally, (iii) measures the quality of the learned embedding: ||µ(x,a) − µ̂(x,a)||L is the distance between the empirical estimate µ̂ of the conditional distribution embedding of x′ given (x, a), and the population conditional embedding µ, measured in the RKHS with kernel L. This difference is weighted by ||Ṽ ∗||L, the RKHS norm of the approximation Ṽ ∗. Intuitively, a lower RKHS norm implies a smoother function: when the norm is smaller, Ṽ ∗ is smoother, and the convergence faster. Thus (iii) requires us to obtain a better conditional mean embedding (via more training samples) when the value function is non-smooth. In other words, our approach favors smooth value functions, although given sufficient evidence, non-smooth functions can also be learned. One specialization is to the case when V ∗ ∈ Cb(X ) and L is a Cb(X )-universal kernel (Steinwart & Christmann, 2008, Section 4.6). In this case we can choose Ṽ ∗ such that ||V ∗ − Ṽ ∗||∞ is arbitrarily small in (11).\nWe now specialise Thm. 3.2 to the case where |X | < ∞ and where L is strictly positive definite kernel on X (we then know from Lemma 2.2 that sup(x,a) ||µ(x,a)− µ̂(x,a)||L → 0 and that all real-valued functions are in the associated RKHS). Thus consistency is attained in otherwise very general conditions – the following is proved in the appendix:\nCorollary 3.3. Let |X | < ∞ and L be strictly positive definite. Under the conditions of Lemma 2.2 we\nModelling transition dynamics in MDPs with RKHS embeddings\n10 00 S am pl es True Value\n10 20 30 40 50\n10\n20\n30\n40\n50\nEstimated Value\n10 20 30 40 50\n10\n20\n30\n40\n50\nPolicy\n10 20 30 40 50\n10\n20\n30\n40\n50\n50 00\nS am\npl es\n10 20 30 40 50\n10\n20\n30\n40\n50\n10 20 30 40 50\n10\n20\n30\n40\n50 10 20 30 40 50\n10\n20\n30\n40\n50\n0 1 2 3 41 2 3 4 5\n2 4 6 8 2 4 6 8\nFigure 1. The left column shows the (true) value of the learned policy (color coded). The middle column shows the estimated value and the right column shows the policy. Actions are shown in the plot via a color code: yellow: go down; brown: left; dark blue: up; light blue: right. The 5000 sample policy is better (see for example the scale on the value color bars) and estimated value is close to true value. The patchy coloring is not a problem as, for example, in the bottom right it does not matter if the agent first goes up or to the left. The method has essentially learnt the task.\nhave that ||V π̂κ − V ∗||∞ → 0 with convergence in P̃ - probability.\nComplexity analysis: Once the embeddings are learnt, the complexity of learning the approximate value function Q̂κ is O(m\n2|A|κ): due to the expansion of µ̂(x,a) in the m points in S, computing each expectation is O(m) and we only ever need to know the evalu-\nation of each iterate V̂k at the m points in S. Applying the learnt policy (10) to a trajectory (x0, x1, . . . , xT ) of length T , is similarly O(m|A|T ). In Sec. B of the Supplementary material, we propose a sparser representation of the embedding, using an incomplete Cholesky approximation (Shawe-Taylor & Cristianini, 2004)[Sec. 5.2]. This reduces the cost of learning the embeddings from cubic to linear in m, and allows us to compute subsequent expectations in O(ℓ), where generally ℓ ≪ m."
    }, {
      "heading" : "4. Experiments",
      "text" : "We performed three experiments, using the embeddings in value estimation and policy optimization. The first experiment was an MDP with a fully observed discrete state space, to demonstrate convergence of the value function with increasing training sample size. The second and third experiments evaluate our approach on a classical control task and a task with high dimensional states. In policy optimisation we compare to LSPI (Lagoudakis & Parr, 2003) where we use the q-value estimator from (Engel et al., 2005), and for value estimation we compare to NPDP5 (Kroemer & Peters, 2011). We achieve better performance in all our experiments.\nWe briefly address the choice of the regularization term λ. It can be shown that the conditional embeddings\n5We thank the authors for providing code.\nsolve,\nµ̂ := argmin µ∈H\n[ m∑\ni=1\n‖L(x′i, ·)− µ(xi, ai)‖ 2 L + λ ‖µ‖ 2 H\n] .\nwhere H ⊆ (HL) (X×A), recovering the vector-valued regression setting of Micchelli & Pontil (2005) (see Sec. D for details) which provides cross validation scheme for the parameter λ."
    }, {
      "heading" : "4.1. Experiment 1",
      "text" : "The first experiment is a navigation experiment in a 50 x 50 room. The reward is a Gaussian centered in the middle of the room. The agent has four actions: go north, east, south or west. Each action has a success rate of 80 % and results in random movement with 20 % chance. The state space is fully observed. We learn the conditional distribution embedding from either 1000 or 5000 uniformly sampled transitions, uniformity ensuring we avoid exploration artifacts. We used a Gaussian kernel and cross-validated to determine the regulariser. Results are shown in Figure 1."
    }, {
      "heading" : "4.2. Experiment 2",
      "text" : "We consider the under-actuated pendulum swing up task (Deisenroth et al., 2009). We generate a discretetime approximation of the continuous-time pendulum dynamics as done in (Deisenroth et al., 2009). Starting from an arbitrary state the goal is to swing the pendulum up and balance it in the inverted position. The applied torque is u ∈ [−5, 5]Nm and is not sufficient for a direct swing up. The state space is defined by the angle θ ∈ [−π, π] and the angular velocity, ω ∈ [−7, 7]. The reward is given by the function r(θ, ω) = exp(−θ2 − 0.2ω2). For policy learning we compared to the GP-based LSPI approach and for value learning to NPDP. The results of the comparison are shown in Fig. 2.\nDetails for the policy learning setting: We sampled uniformly from the state and action space and used a Gaussian kernel on both, selecting as kernel width the average K-neighbour distance, where K is one quarter of the sample size. We considered a discretization of the action space into 25 actions and we measured the difference between the value function evaluated on a grid of 25 × 25 points to the optimal value obtained by dynamic programming using the deterministic system dynamics. We compared over different sample sizes and averaged the performance over 10 repetitions.\nDetails for the value estimation setting: We used the optimal policy to generate samples. The goal was to predict the value of the optimal policy. The performance of NPDP depends strongly on the bandwidth parameter of the used kernel (a Gaussian). For parameter selection, we optimised performance on a validation set over a grid all free parameters (bandwidth for NPDP, bandwidth and λ for the embedding), and report the error on an independent test set. The relatively poor scaling of NPDP with increased sample size is due to the numerical integration step in (Kroemer & Peters, 2011, Algorithm 1)."
    }, {
      "heading" : "4.3. Experiment 3",
      "text" : "Our final experiment is a high dimensional task where sensor measurements are available, and no state description is present. The environment consists of two rooms connected via a short corridor (Böhmer, 2012). The sensor measurements are images from a 3D renderer, and we aggregate four orientations (north, east, south and west) for a panorama, since the camera im-\nages are ambiguous, especially close to the walls. The task of the agent is to reach a goal located in one of the rooms, using only the images to orient itself. Training points were chosen uniformly over the input space. We used a Gaussian kernel and cross-validated the regularization parameter. Results for 4000 training points are shown in Figure 3. We compared to the GP based LSPI approach using the same kernel and settings for both approaches; results are shown in Figure 2. Our method improves with increasing sample\nnumbers. The GP based LSPI approach has obvious difficulties with this task and does not improve. We did not apply NPDP to exp. 3, as it would be computationally intractable in given the high dimensionality."
    }, {
      "heading" : "5. Conclusions and Outlook",
      "text" : "We have proposed a novel application of RKHS embeddings to learning expectation operators associated to transition dynamics in MDPs, with particular focus on their use in dynamic programming methods. The approach avoids the need for density estimates, sampling methods for evaluation of integrals, or explicit models of the system; is computationally efficient, having cost linear in the number of samples used in training (or even sublinear, with appropriate approximations); and has performance guarantees. Future work will focus on generalizing to more complex state and action spaces, and extending the convergence results to continuous state spaces. Another important generalization concerns the sampling distribution, which here is assumed to be iid, but one can expect similar results to hold in the non-iid case."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors want to thank for the support of the EPSRC #EP/H017402/1 (CARDyAL) and the European Union #FP7-ICT-270327 (Complacs)."
    } ],
    "references" : [ {
      "title" : "Kernel independent component analysis",
      "author" : [ "F.R. Bach", "M.I. Jordan" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Bach and Jordan,? \\Q2002\\E",
      "shortCiteRegEx" : "Bach and Jordan",
      "year" : 2002
    }, {
      "title" : "Mathematical issues in dynamic programming",
      "author" : [ "D. Bertsekas", "S. Shreve" ],
      "venue" : "Academic press,",
      "citeRegEx" : "Bertsekas and Shreve,? \\Q1978\\E",
      "shortCiteRegEx" : "Bertsekas and Shreve",
      "year" : 1978
    }, {
      "title" : "Robot Navigation using Reinforcement Learning and Slow Feature Analysis",
      "author" : [ "W. Böhmer" ],
      "venue" : "ArXiv,",
      "citeRegEx" : "Böhmer,? \\Q2012\\E",
      "shortCiteRegEx" : "Böhmer",
      "year" : 2012
    }, {
      "title" : "Reinforcement learning with gaussian processes",
      "author" : [ "Y. Engel", "S. Mannor", "R. Meir" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Engel et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Engel et al\\.",
      "year" : 2005
    }, {
      "title" : "Modelling transition dynamics in mdps with rkhs embeddings",
      "author" : [ "S. Grünewälder", "G. Lever", "L. Baldassarre", "M. Pontil", "A. Gretton" ],
      "venue" : "In arXiv,",
      "citeRegEx" : "Grünewälder et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Grünewälder et al\\.",
      "year" : 2012
    }, {
      "title" : "A non-parametric approach to dynamic programming",
      "author" : [ "O. Kroemer", "J. Peters" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Kroemer and Peters,? \\Q2011\\E",
      "shortCiteRegEx" : "Kroemer and Peters",
      "year" : 2011
    }, {
      "title" : "Least-squares policy iteration",
      "author" : [ "M.G. Lagoudakis", "R. Parr" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Lagoudakis and Parr,? \\Q2003\\E",
      "shortCiteRegEx" : "Lagoudakis and Parr",
      "year" : 2003
    }, {
      "title" : "On learning vector-valued functions",
      "author" : [ "C.A. Micchelli", "M. Pontil" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Micchelli and Pontil,? \\Q2005\\E",
      "shortCiteRegEx" : "Micchelli and Pontil",
      "year" : 2005
    }, {
      "title" : "Kernel-based reinforcement learning",
      "author" : [ "D. Ormoneit", "S. Sen" ],
      "venue" : "In Machine Learning,",
      "citeRegEx" : "Ormoneit and Sen,? \\Q1999\\E",
      "shortCiteRegEx" : "Ormoneit and Sen",
      "year" : 1999
    }, {
      "title" : "An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning",
      "author" : [ "R. Parr", "L. Li", "G. Taylor", "C. Painter-Wakefield", "M.L. Littman" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Parr et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Parr et al\\.",
      "year" : 2008
    }, {
      "title" : "Gaussian processes in reinforcement learning",
      "author" : [ "C.E. Rasmussen", "M. Kuss" ],
      "venue" : "In NIPS. MIT Press,",
      "citeRegEx" : "Rasmussen and Kuss,? \\Q2003\\E",
      "shortCiteRegEx" : "Rasmussen and Kuss",
      "year" : 2003
    }, {
      "title" : "Methods of modern mathematical physics. Vol. 1: Functional Analysis",
      "author" : [ "M. Reed", "B. Simon" ],
      "venue" : null,
      "citeRegEx" : "Reed and Simon,? \\Q1980\\E",
      "shortCiteRegEx" : "Reed and Simon",
      "year" : 1980
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : null,
      "citeRegEx" : "Shawe.Taylor and Cristianini,? \\Q2004\\E",
      "shortCiteRegEx" : "Shawe.Taylor and Cristianini",
      "year" : 2004
    }, {
      "title" : "An upper bound on the loss from approximate optimal-value functions",
      "author" : [ "S.P. Singh", "R.C. Yee" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Singh and Yee,? \\Q1994\\E",
      "shortCiteRegEx" : "Singh and Yee",
      "year" : 1994
    }, {
      "title" : "Kernels and regularization on graphs",
      "author" : [ "A.J. Smola", "R.I. Kondor" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Smola and Kondor,? \\Q2003\\E",
      "shortCiteRegEx" : "Smola and Kondor",
      "year" : 2003
    }, {
      "title" : "Hilbert space embeddings of conditional distributions with applications to dynamical systems",
      "author" : [ "L. Song", "J. Huang", "A.J. Smola", "K. Fukumizu" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Song et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2009
    }, {
      "title" : "Hilbert space embeddings of hidden Markov models",
      "author" : [ "L. Song", "B. Boots", "S. Siddiqi", "G. Gordon", "A. Smola" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Song et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2010
    }, {
      "title" : "Nonparametric tree graphical models",
      "author" : [ "L. Song", "A. Gretton", "C. Guestrin" ],
      "venue" : null,
      "citeRegEx" : "Song et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2010
    }, {
      "title" : "Kernel belief propagation",
      "author" : [ "L. Song", "A. Gretton", "D. Bickson", "Y. Low", "C. Guestrin" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Song et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2011
    }, {
      "title" : "Hilbert space embeddings and metrics on probability measures",
      "author" : [ "B. Sriperumbudur", "A. Gretton", "K. Fukumizu", "G. Lanckriet", "B. Schölkopf" ],
      "venue" : null,
      "citeRegEx" : "Sriperumbudur et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sriperumbudur et al\\.",
      "year" : 2010
    }, {
      "title" : "Algorithms for reinforcement learning",
      "author" : [ "C. Szepesvari" ],
      "venue" : null,
      "citeRegEx" : "Szepesvari,? \\Q2009\\E",
      "shortCiteRegEx" : "Szepesvari",
      "year" : 2009
    }, {
      "title" : "Kernelized value function approximation for reinforcement learning",
      "author" : [ "G. Taylor", "R. Parr" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Taylor and Parr,? \\Q2009\\E",
      "shortCiteRegEx" : "Taylor and Parr",
      "year" : 2009
    }, {
      "title" : "Scattered Data Approximation",
      "author" : [ "H. Wendland" ],
      "venue" : null,
      "citeRegEx" : "Wendland,? \\Q2005\\E",
      "shortCiteRegEx" : "Wendland",
      "year" : 2005
    }, {
      "title" : "Kernel least-squares temporal difference learning",
      "author" : [ "X. Xu", "T. Xie", "D. Hu", "X. Lu" ],
      "venue" : "International Journal of Information Technology,",
      "citeRegEx" : "Xu et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "(Szepesvari, 2009) for this background).",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 4,
      "context" : "We require the following well-known result, which is proved in the Appendix for reference (Grünewälder et al., 2012):",
      "startOffset" : 90,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "Thus, the approach provides a framework for alleviating the curse of dimensionality in MDPs (particularly if, for example, sparsification of the embedding is considered, which we address briefly in the Appendix (Grünewälder et al., 2012)).",
      "startOffset" : 211,
      "endOffset" : 237
    }, {
      "referenceID" : 22,
      "context" : "As a final advantage, the method applies wherever kernels may be defined, including on high dimensional or continuous state spaces, manifolds (kernels on the surface of a sphere (Wendland, 2005) are of particu-",
      "startOffset" : 178,
      "endOffset" : 194
    }, {
      "referenceID" : 23,
      "context" : "Methods include kernel LSTD (Xu et al., 2005) and GPTD (Engel et al.",
      "startOffset" : 28,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : ", 2005) and GPTD (Engel et al., 2005).",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : "Finally, in (Parr et al., 2008) a way is proposed to approximate expectations in a low dimensional state representation.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : ", 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor & Parr, 2009). Based on the q-value estimates it is possible to optimise the policy. Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al.",
      "startOffset" : 18,
      "endOffset" : 284
    }, {
      "referenceID" : 3,
      "context" : ", 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor & Parr, 2009). Based on the q-value estimates it is possible to optimise the policy. Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al. (2009), Ormoneit & Sen (1999) and Kroemer & Peters (2011).",
      "startOffset" : 18,
      "endOffset" : 310
    }, {
      "referenceID" : 3,
      "context" : ", 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor & Parr, 2009). Based on the q-value estimates it is possible to optimise the policy. Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al. (2009), Ormoneit & Sen (1999) and Kroemer & Peters (2011).",
      "startOffset" : 18,
      "endOffset" : 333
    }, {
      "referenceID" : 3,
      "context" : ", 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor & Parr, 2009). Based on the q-value estimates it is possible to optimise the policy. Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al. (2009), Ormoneit & Sen (1999) and Kroemer & Peters (2011). Here, transition models (densities) are learned with the help of Gaussian processes or kernel density estimates.",
      "startOffset" : 18,
      "endOffset" : 361
    }, {
      "referenceID" : 15,
      "context" : "Following (Song et al., 2009; 2010b) an estimate is",
      "startOffset" : 10,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "Following Sriperumbudur et al. (2010), given any probability distribution P and p.",
      "startOffset" : 10,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "These conditions require that the mapping (x, a) 7→ EX∼P (·|(x,a))[f(X)] be an element of HK for all f ∈ HL, and that the operator CY XC −3/2 XX be HilbertSchmidt, where CY X and CXX are covariance operators: see (Song et al., 2009) or Appendix D.",
      "startOffset" : 213,
      "endOffset" : 232
    }, {
      "referenceID" : 4,
      "context" : "1 for details (Grünewälder et al., 2012).",
      "startOffset" : 14,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "The following lemma is proved in the Appendix (Grünewälder et al., 2012).",
      "startOffset" : 46,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "The following result is proved in the Appendix (Grünewälder et al., 2012): Proposition 3.",
      "startOffset" : 47,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "(Sketch, see Appendix for full proof (Grünewälder et al., 2012).",
      "startOffset" : 37,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "In policy optimisation we compare to LSPI (Lagoudakis & Parr, 2003) where we use the q-value estimator from (Engel et al., 2005), and for value estimation we compare to NPDP (Kroemer & Peters, 2011).",
      "startOffset" : 108,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "The environment consists of two rooms connected via a short corridor (Böhmer, 2012).",
      "startOffset" : 69,
      "endOffset" : 83
    } ],
    "year" : 2012,
    "abstractText" : "We propose a new, nonparametric approach to learning and representing transition dynamics in Markov decision processes (MDPs), which can be combined easily with dynamic programming methods for policy optimisation and value estimation. This approach makes use of a recently developed representation of conditional distributions as embeddings in a reproducing kernel Hilbert space (RKHS). Such representations bypass the need for estimating transition probabilities or densities, and apply to any domain on which kernels can be defined. This avoids the need to calculate intractable integrals, since expectations are represented as RKHS inner products whose computation has linear complexity in the number of points used to represent the embedding. We provide guarantees for the proposed applications in MDPs: in the context of a value iteration algorithm, we prove convergence to either the optimal policy, or to the closest projection of the optimal policy in our model class (an RKHS), under reasonable assumptions. In experiments, we investigate a learning task in a typical classical control setting (the under-actuated pendulum), and on a navigation problem where only images from a sensor are observed. For policy optimisation we compare with leastsquares policy iteration where a Gaussian process is used for value function estimation. For value estimation we also compare to the NPDP method. Our approach achieves better performance in all experiments. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).",
    "creator" : "PDFSplit! (http://www.splitpdf.com)"
  }
}