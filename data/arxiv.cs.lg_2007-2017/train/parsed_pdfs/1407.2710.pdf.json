{
  "name" : "1407.2710.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems",
    "authors" : [ "Aaron J. Defazio", "Tibério S. Caetano", "Justin Domke" ],
    "emails" : [ "AARON.DEFAZIO@ANU.EDU.AU", "TIBERIO.CAETANO@NICTA.COM.AU", "JUSTIN.DOMKE@NICTA.COM.AU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Many recent advances in the theory and practice of numerical optimization have come from the recognition and exploitation of structure. Perhaps the most common structure is that of finite sums. In machine learning when applying empirical risk minimization we almost always end up with an optimization problem involving the minimization of a sum with one term per data point.\nThe recently developed SAG algorithm (Schmidt et al., 2013) has shown that even with this simple form of structure, as long as we have sufficiently many data points we are able to do significantly better than black-box optimization techniques in expectation for smooth strongly convex problems. In practical terms the difference is often a factor of 10 or more.\nThe requirement of sufficiently large datasets is fundamental to these methods. We describe the precise form of this as the big data condition. Essentially, it is the requirement that the amount of data is on the same order as the condition number of the problem. The strong convexity requirement\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nis not as onerous. Strong convexity holds in the common case where a quadratic regularizer is used together with a convex loss.\nThe SAG method and the Finito method we describe in this work are similar in their form to stochastic gradient descent methods, but with one crucial difference: They store additional information about each data point during optimization. Essentially, when they revisit a data point, they do not treat it as a novel piece of information every time.\nMethods for the minimization of finite sums have classically been known as Incremental gradient methods (Bertsekas, 2010). The proof techniques used in SAG differ fundamentally from those used on other incremental gradient methods though. The difference hinges on the requirement that data be accessed in a randomized order. SAG does not work when data is accessed sequentially each epoch, so any proof technique which shows even non-divergence for sequential access cannot be applied.\nA remarkable property of Finito is the tightness of the theoretical bounds compared to the practical performance of the algorithm. The practical convergence rate seen is at most twice as good as the theoretically predicted rate. This sets it apart from methods such as LBFGS where the empirical performance is often much better than the relatively weak theoretical convergence rates would suggest.\nThe lack of tuning required also sets Finito apart from stochastic gradient descent (SGD). In order to get good performance out of SGD, substantial laborious tuning of multiple constants has traditionally been required. A multitude of heuristics have been developed to help choose these constants, or adapt them as the method progresses. Such heuristics are more complex than Finito, and do not have the same theoretical backing. SGD has application outside of convex problems of course, and we do not propose that Finito will replace SGD in those settings. Even on strongly convex problems SGD does not exhibit linear convergence like Finito does.\nar X\niv :1\n40 7.\n27 10\nv1 [\ncs .L\nG ]\n1 0\nJu l 2\n01 4\nThere are many similarities between SAG, Finto and stochastic dual coordinate descent (SDCA) methods (Shalev-Shwartz & Zhang, 2013). SDCA is only applicable to linear predictors. When it can be applied, it has linear convergence with theoretical rates similar to SAG and Finito."
    }, {
      "heading" : "2. Algorithm",
      "text" : "We consider differentiable convex functions of the form\nf(w) = 1\nn n∑ i=1 fi(w).\nWe assume that each fi has Lipschitz continuous gradients with constant L and is strongly convex with constant s. Clearly if we allow n = 1, virtually all smooth, strongly convex problems are included. So instead, we will restrict ourselves to problems satisfying the big data condition.\nBig data condition: Functions of the above form satisfy the big data condition with constant β if\nn ≥ βL s\nTypical values of β are 1-8. In plain language, we are considering problems where the amount of data is of the same order as the condition number (L/s) of the problem."
    }, {
      "heading" : "2.1. Additional Notation",
      "text" : "We superscript with (k) to denote the value of the scripted quantity at iteration k. We omit the n superscript on summations, and subscript with i with the implication that indexing starts at 1. When we use separate arguments for each fi, we denote them φi. Let φ̄(k) denote the average φ̄(k) = 1n ∑n i φ (k) i . Our step length constant, which depends on β, is denoted α. We use angle bracket notation for dot products 〈·, ·〉."
    }, {
      "heading" : "2.2. The Finito algorithm",
      "text" : "We start with a table of known φ(0)i values, and a table of known gradients f ′i(φ (0) i ), for each i. We will update these two tables during the course of the algorithm. The step for iteration k, is as follows:\n1. Update w using the step:\nw(k) = φ̄(k) − 1 αsn ∑ i f ′i(φ (k) i ).\n2. Pick an index j uniformly at random, or using without-replacement sampling as discussed\nin Section 3.\n3. Set φ(k+1)j = w (k) in the table and leave the other\nvariables the same (φ(k+1)i = φ (k) i for i 6= j).\n4. Calculate and store f ′j(φ (k+1) j ) in the table.\nOur main theoretical result is a convergence rate proof for this method.\nTheorem 1. When the big data condition holds with β = 2, α = 2 may be used. In that setting, if we have initialized all φ(0)i the same, the convergence rate is:\nE [ f(φ̄(k)) ] − f(w∗) ≤ 3\n4s\n( 1− 1\n2n )k ∥∥∥f ′(φ̄(0))∥∥∥2 . See Section 5 for the proof. In contrast, SAG achieves a( 1− 18n ) rate when β = 2. Note that on a per epoch basis,\nthe Finito rate is ( 1− 12n )n ≈ exp(−1/2) = 0.606. To put that into context, 10 epochs will see the error bound reduced by more than 148x.\nOne notable feature of our method is the fixed step size. In typical machine learning problems the strong convexity constant is given by the strength constant of the quadratic regularizer used. Since this is a known quantity, as long as the big data condition holds α = 2 may be used without any tuning or adjustment of Finito required. This lack of tuning is a major feature of Finito.\nIn cases where the big data condition does not hold, we conjecture that the step size must be reduced proportionally to the violation of the big data condition. In practice, the most effective step size can be found by testing a number of step sizes, as is usually done with other stochastic optimisation methods.\nA simple way of satisfying the big data condition is to duplicate your data enough times so then holds. This is not as effective in practice as just changing the step size, and of course it uses more memory. However it does fall within the current theory.\nAnother difference compared to the SAG method is that we store both gradients and points φi. We do not actually need twice as much memory however as they can be stored summed together. In particular we store the quantities pi = f ′i(φi) − αsφi, and use the update rule w = − 1αsn ∑ i pi. This trick does not work when step lengths are adjusted during optimization however. The storage of φi is also a disadvantage when the gradients f ′i(φi) are sparse but φi are not sparse, as it can cause significant additional memory usage. We do not recommend the usage of Finito when gradients are sparse.\nThe SAG algorithm differs from Finito only in thew update\nand step lengths:\nw(k) = w(k−1) − 1 16Ln ∑ i f ′i(φ (k) i )."
    }, {
      "heading" : "3. Randomness is key",
      "text" : "By far the most interesting aspect of the SAG and Finito methods is the random choice of index at each iteration. We are not in an online setting, so there is no inherent randomness in the problem. Yet it seems that a randomized method is required. Neither method works in practice when the same ordering is used each pass, or in fact with any non-random access scheme we have tried. It is hard to emphasize enough the importance of randomness here. The technique of pre-permuting the data, then doing in order passes after that, also does not work. Reducing the step size in SAG or Finito by 1 or 2 orders of magnitude does not fix the convergence issues either.\nOther methods, such as standard SGD, have been noted by various authors to exhibit speed-ups when random sampling is used instead of in order passes, but the differences are not as extreme as convergence v.s. nonconvergence. Perhaps the most similar problem is that of coordinate descent on smooth convex functions. Coordinate descent cannot diverge when non-random orderings are used, but convergence rates are substantially worse in the non-randomized setting (Nesterov 2010, Richtarik & Takac 2011).\nReducing the step size α by a much larger amount, namely by a factor of n, does allow for non-randomized orderings to be used. This gives an extremely slow method however. This is the case covered by the MISO (Mairal, 2013). A similar reduction in step size gives convergence under nonrandomized orderings for SAG also. Convergence rates for incremental sub-gradient methods with a variety of orderings appear in the literature also (Nedic & Bertsekas, 2000).\nSampling without replacement is much faster\nOther sampling schemes, such as sampling without replacement, should be considered. In detail, we mean the case where each ”pass” over the data is a set of sampling without replacement steps, which continue until no data remains, after which another ”pass” starts afresh. We call this the permuted case for simplicity, as it is the same as re-permuting the data after each pass. In practice, this approach does not give any speedup with SAG, however it works spectacularly well with Finito. We see speedups of up to a factor of two using this approach. This is one of the major differences in practice between SAG and Finito. We should note that we have no theory to support this case however. We are not aware of any analysis that proves faster convergence rates of any optimization method under\na sampling without replacement scheme. An interesting discussion of SGD under without-replacement sampling appears in Recht & Re (2012).\nThe SDCA method is also sometimes used with a permuted ordering (Shalev-Shwartz & Zhang, 2013), our experiments in Section 7 show that this sometimes results in a large speedup over uniform random sampling, although it does not appear to be as reliable as with Finito."
    }, {
      "heading" : "4. Proximal variant",
      "text" : "We now consider composite problems of the form\nf(w) = 1\nn ∑ i fi(w) + λr(w),\nwhere r is convex but not necessarily smooth or strongly convex. Such problems are often addressed using proximal algorithms, particularly when the proximal operator for r:\nproxrλ(z) = argminx 1\n2 ‖x− z‖2 + λr(x)\nhas a closed form solution. An example would be the use of L1 regularization. We now describe the Finito update for this setting. First notice that when we set w in the Finito method, it can be interpreted as minimizing the quantity:\nB(x) = 1\nn ∑ i fi(φi) + 1 n ∑ i 〈f ′i(φi), x− φi〉\n+ αs\n2n ∑ i ‖x− φi‖2 ,\nwith respect to x, for fixed φi. This is related to the upper bound minimized by MISO, where αs is instead L. It is straight forward to modify this for the composite case:\nBλr(x) = λr(x) + 1\nn ∑ i fi(φi) + 1 n ∑ i 〈f ′i(φi), x− φi〉\n+ αs\n2n ∑ i ‖x− φi‖2 .\nThe minimizer of the modified Bλr can be expressed using the proximal operator as:\nw = proxrλ/αs\n( φ̄− 1\nαsn ∑ i f ′i(φi)\n) .\nThis strongly resembles the update in the standard gradient descent setting, which for a step size of 1/L is\nw = proxrλ/L\n( w(k−1) − 1\nL f ′(w(k−1))\n) .\nWe have not yet developed any theory supporting the proximal variant of Finito, although empirical evidence suggests it has the same convergence rate as in the non-proximal case."
    }, {
      "heading" : "5. Convergence proof",
      "text" : "We start by stating two simple lemmas. All expectations in the following are over the choice of index j at step k. Quantities without superscripts are at their values at iteration k.\nLemma 1. The expected step is\nE[w(k+1)]− w = − 1 αsn f ′(w).\nI.e. the w step is a gradient descent step in expectation ( 1αsn ∝ 1 L ). A similar equality also holds for SGD, but not for SAG.\nProof.\nE[w(k+1)]− w\n= E\n[ 1\nn (w − φj)−\n1\nαsn\n( f ′j(w)− f ′j(φj) )] = 1\nn (w − φ̄)− 1 αsn f ′(w) + 1 αsn2 ∑ i f ′i(φi)\nNow simplify 1n (w − φ̄) as − 1 αsn2 ∑ i f ′ i(φi), so the only term that remains is − 1αsnf ′(w).\nLemma 2. (Decomposition of variance) We can decompose 1n ∑ i ‖w − φi‖ 2 as\n1\nn ∑ i ‖w − φi‖2 = ∥∥w − φ̄∥∥2 + 1 n ∑ i ∥∥φ̄− φi∥∥2 . Proof.\n1\nn ∑ i ‖w − φi‖2\n= ∥∥w − φ̄∥∥2 + 1\nn ∑ i ∥∥φ̄− φi∥∥2 + 2 n ∑ i 〈 w − φ̄, φ̄− φi 〉 = ∥∥w − φ̄∥∥2 + 1\nn ∑ i ∥∥φ̄− φi∥∥2 + 2 〈w − φ̄, φ̄− φ̄〉 = ∥∥w − φ̄∥∥2 + 1\nn ∑ i ∥∥φ̄− φi∥∥2 .\nMain proof\nOur proof proceeds by construction of a Lyapunov function T ; that is, a function that bounds a quantity of interest, and that decreases each iteration in expectation. Our Lyapunov function T = T1 + T2 + T3 + T4 is composed of the sum of the following terms,\nT1 = f(φ̄),\nT2 = − 1\nn ∑ i fi(φi)− 1 n ∑ i 〈f ′i(φi), w − φi〉 ,\nT3 = − s\n2n ∑ i ‖w − φi‖2 ,\nT4 = s\n2n ∑ i ∥∥φ̄− φi∥∥2 . We now state how each term changes between steps k + 1 and k. Proofs are found in the appendix in the supplementary material:\nE[T (k+1) 1 ]−T1 ≤\n1\nn\n〈 f ′(φ̄), w − φ̄ 〉 + L\n2n3 ∑ i ‖w − φi‖2 ,\nE[T (k+1) 2 ]− T2 ≤ −\n1 n T2 − 1 n f(w)\n+ ( 1 α − β n ) 1 sn3 ∑ i ‖f ′i(w)− f ′i(φi)‖ 2 + 1\nn\n〈 φ̄− w, f ′(w) 〉 − 1 n3 ∑ i 〈f ′i(w)− f ′i(φi), w − φi〉 ,\nE[T (k+1) 3 ]− T3 = −(\n1 n + 1 n2 )T3 + 1 αn\n〈 f ′(w), w − φ̄ 〉 − 1\n2α2sn3 ∑ i ‖f ′i(φi)− f ′i(w)‖ 2 ,\nE[T (k+1) 4 ]− T4 = −\ns\n2n2 ∑ i ∥∥φ̄− φi∥∥2 + s 2n ∥∥φ̄− w∥∥2 − s\n2n3 ∑ i ‖w − φi‖2 .\nTheorem 2. Between steps k and k+1, if 2α− 1 α2−β+ β α ≤ 0, α ≥ 2 and β ≥ 2 then\nE[T (k+1)]− T ≤ − 1 αn T.\nProof. We take the three lemmas above and group like terms to get\nE[T (k+1)]− T ≤ 1 n\n〈 f ′(φ̄), w − φ̄ 〉 + 1\nn2 ∑ i fi(φi)\n− 1 n f(w) + 1 n2 ∑ i 〈f ′i(φi), w − φi〉 + (1− 1 α ) 1 n 〈 f ′(w), φ̄− w\n〉 + ( L\nsn + 1)\ns\n2n2 ∑ i ‖w − φi‖2\n− 1 n3 ∑ i 〈f ′i(w)− f ′i(φi), w − φi〉 + (1− 1 2α ) 1 αsn3 ∑ i ‖f ′i(φi)− f ′i(w)‖ 2\n+ s\n2n ∥∥w − φ̄∥∥2 − s 2n2 ∑ i ∥∥φ̄− φi∥∥2 . Next we cancel part of the first line using\n1\nαn\n〈 f ′(φ̄), w − φ̄ 〉 ≤ 1 αn f(w)− 1 αn f(φ̄)− s 2αn ∥∥w − φ̄∥∥2 , based on B3 in the Appendix. We then pull terms occurring in − 1αnT together, giving E[T (k+1)]− T ≤\n− 1 αn T + (1− 1 α ) 1 n\n〈 f ′(φ̄)− f ′(w), w − φ̄ 〉 + (1− 1\nα ) [ − 1 n f(w)− 1 n T2 ] + ( L\nsn + 1− 1 α ) s 2n2 ∑ i ‖w − φi‖2\n− 1 n3 ∑ i 〈f ′i(w)− f ′i(φi), w − φi〉 + (1− 1 2α ) 1 αsn3 ∑ i ‖f ′i(φi)− f ′i(w)‖ 2 + (1− 1 α ) s 2n ∥∥w − φ̄∥∥2 − (1− 1 α ) s 2n2 ∑ i\n∥∥φ̄− φi∥∥2 . Next we use the standard inequality (B5)\n(1− 1 α ) 1 n\n〈 f ′(φ̄)− f ′(w), w − φ̄ 〉 ≤ −(1− 1\nα ) s n ∥∥w − φ̄∥∥2 , which changes the bottom row to−(1− 1α ) s 2n\n∥∥w − φ̄∥∥2− (1 − 1α ) s 2n2 ∑ i\n∥∥φ̄− φi∥∥2. These two terms can then be grouped using Lemma 2, to give\nE[T (k+1)]− T ≤ − 1 αn T + L 2n3 ∑ i ‖w − φi‖2\n+ (1− 1 α ) [ − 1 n f(w)− 1 n T2 ] − 1 n3 ∑ i 〈f ′i(w)− f ′i(φi), w − φi〉 + (1− 1 2α ) 1 αsn3 ∑ i ‖f ′i(φi)− f ′i(w)‖ 2 .\nWe use the following inequality (Corollary 11 in Appendix) to cancel against the ∑ i ‖w − φi‖ 2 term:\n1\nβ [ − 1 n f(w)− 1 n T2 ] ≤ 1 n3 ∑ i 〈f ′i(w)− f ′i(φi), w − φi〉\n− L 2n3 ∑ i ‖w − φi‖2 − 1 2sn3 ∑ i ‖f ′i(w)− f ′i(φi)‖ 2 ,\nand then apply the following similar inequality (B7 in Ap-\npendix) to partially cancel ∑ i ‖fi(φi)− fi(w)‖\n2:( 1− 1\nα − 1 β )[ − 1 n f(w)− 1 n T2 ] ≤ − ( 1− 1\nα − 1 β\n) β\n2sn3 ∑ i ‖f ′i(φi)− f ′i(w)‖ 2 .\nLeaving us with\nE[T (k+1)]− T ≤ − 1 αn T\n+ ( 2 α − 1 α2 − β + β α ) 1 2sn3 ∑ i ‖f ′i(φi)− f ′i(w)‖ 2 .\nThe remaining gradient norm term is non-positive under the conditions specified in our assumptions.\nTheorem 3. The Lyapunov function bounds f(φ̄)− f(w∗) as follows:\nf(φ̄(k))− f(w∗) ≤ αT (k).\nProof. Consider the following function, which we will call R(x):\nR(x) = 1\nn ∑ i fi(φi) + 1 n ∑ i 〈f ′i(φi), x− φi〉\n+ s\n2n ∑ i ‖x− φi‖2 .\nWhen evaluated at its minimum with respect to x, which we denote w′ = φ̄ − 1sn ∑ i f ′ i(φi), it is a lower bound on f(w∗) by strong convexity. However, we are evaluating at w = φ̄ − 1αsn ∑ i f ′ i(φi) instead in the (negated) Lyapunv function. R is convex with respect to x, so by definition\nR(w) = R (( 1− 1\nα\n) φ̄+ 1 α w′ )\n≤ (\n1− 1 α\n) R(φ̄) + 1\nα R(w′).\nTherefore by the lower bounding property f(φ̄)−R(w) ≥ f(φ̄)− (\n1− 1 α\n) R(φ̄)− 1\nα R(w′)\n≥ f(φ̄)− (\n1− 1 α\n) f(φ̄)− 1\nα f(w∗)\n= 1\nα\n( f(φ̄)− f(w∗) ) .\nNow note that T ≥ f(φ̄)−R(w). So\nf(φ̄)− f(w∗) ≤ αT.\nTheorem 4. If the Finito method is initialized with all φ(0)i the same,and the assumptions of Theorem 2 hold, then the\nconvergence rate is:\nE [ f(φ̄(k)) ] − f(w∗) ≤ c\ns\n( 1− 1\nαn )k ∥∥∥f ′(φ̄(0))∥∥∥2 , with c = ( 1− 12α ) .\nProof. By unrolling Theorem 2, we get E[T (k)] ≤ (\n1− 1 αn\n)k T (0).\nNow using Theorem 3\nE [ f(φ̄(k)) ] − f(w∗) ≤ α ( 1− 1\nαn\n)k T (0).\nWe need to control T (0) also. Since we are assuming that all φ0i start the same, we have that\nT (0) = f(φ̄(0))− 1 n ∑ i fi(φ̄ (0))\n− 1 n ∑ i 〈 f ′i(φ̄ (0)), w(0) − φ̄(0) 〉 − s 2 ∥∥∥w(0) − φ̄(0)∥∥∥2 = 0− 〈 f ′(φ̄(0)), w(0) − φ̄(0) 〉 − s\n2 ∥∥∥∥− 1αsf ′(φ̄(0)) ∥∥∥∥2\n= 1\nαs ∥∥∥f ′(φ̄(0))∥∥∥2 − 1 2α2s ∥∥∥f ′(φ̄(0))∥∥∥2 = ( 1− 1\n2α\n) 1\nαs\n∥∥∥f ′(φ̄(0))∥∥∥2 ."
    }, {
      "heading" : "6. Lower complexity bounds and exploiting problem structure",
      "text" : "The theory for the class of smooth, strongly convex problems with Lipschitz continuous gradients under first order optimization methods (known as S1,1s,L) is well developed. These results require the technical condition that the dimensionality of the input space Rm is much larger than the number of iterations we will take. For simplicity we will assume this is the case in the following discussions.\nIt is known that problems exist in S1,1s,L for which the iterate convergence rate is bounded by:∥∥∥w(k) − w∗∥∥∥2 ≥ (√L/s− 1√\nL/s+ 1 )2k ∥∥∥w(0) − w∗∥∥∥2 . In fact, when s and L are known in advance, this rate is achieved up to a small constant factor by several methods, most notably by Nesterov’s accelerated gradient descent method (Nesterov 1988, Nesterov 1998). In order to achieve convergence rates faster than this, additional assumptions must be made on the class of functions considered.\nRecent advances have shown that all that is required to achieve significantly faster rates is a finite sum structure, such as in our problem setup. When the big data condition holds our method achieves a rate 0.6065 per epoch in expectation. This rate only depends on the condition number indirectly, through the big data condition. For example, with L/s = 1, 000, 000, the fastest possible rate for a black box method is a 0.996, whereas Finito achieves a rate of 0.6065 in expectation for n ≥ 4, 000, 000, or 124x faster. The required amount of data is not unusual in modern machine learning problems. In practice, when quasinewton methods are used instead of accelerated methods, a speedup of 10-20x is more common."
    }, {
      "heading" : "6.1. Oracle class",
      "text" : "We now describe the (stochastic) oracle class FS1,1s,L,n(R\nm) for which SAG and Finito most naturally fit.\nFunction class: f(w) = 1n ∑n i=1 fi(w), with fi ∈ S1,1s,L(R m).\nOracle: Each query takes a point x ∈ Rm, and returns j, fj(w) and f ′j(w), with j chosen uniformly at random.\nAccuracy: Find w such that E[ ∥∥w(k) − w∗∥∥2] ≤ .\nThe main choice made in formulating this definition is putting the random choice in the oracle. This restricts the methods allowed quite strongly. The alternative case, where the index j is input to the oracle in addition to x, is also interesting. Assuming that the method has access to a source of true random indices, we call that class DS1,1s,L,n(R\nm). In Section 3 we discuss empirical evidence that suggests that faster rates are possible in DS1,1s,L,n(R m) than for FS1,1s,L,n(R m).\nIt should first be noted that there is a trivial lower bound rate for f ∈ SS1,1s,L,β(Rm) of ( 1− 1n ) reduction per step. Its not clear if this can be achieved for any finite β. Finito is only a factor of 2 off this rate, namely ( 1− 12n ) at β = 2, and asymptotes towards this rate for very large β. SDCA, while not applicable to all problems in this class, also achieves the rate asymptotically.\nAnother case to consider is the smooth convex but nonstrongly convex setting. We still assume Lipschitz continuous gradients. In this setting we will show that for sufficiently high dimensional input spaces, the (non-stochastic) lower complexity bound is the same for the finite sum case and cannot be better than that given by treating f as a single black box function.\nThe full proof is in the Appendix, but the idea is as follows: when the fi are not strongly convex, we can choose them such that they do not interact with each other, as long as the\ndimensionality is much larger than k. More precisely, we may choose them so that for any x and y and any i 6= j, 〈f ′i(x), f ′j(y)〉 = 0 holds. When the functions do not interact, no optimization scheme may reduce the iterate error faster than by just handling each fi separately. Doing so in an in-order fashion gives the same rate as just treating f using a black box method.\nFor strongly convex fi, it is not possible for them to not interact in the above sense. By definition strong convexity requires a quadratic component in each fi that acts on all dimensions."
    }, {
      "heading" : "7. Experiments",
      "text" : "In this section we compare Finito, SAG, SDCA and LBFGS. We only consider problems where the regularizer is large enough so that the big data condition holds, as this is the case our theory supports. However, in practice our method can be used with smaller step sizes in the more general case, in much the same way as SAG.\nSince we do not know the Lipschitz constant for these problems exactly, the SAG method was run for a variety of step sizes, with the one that gave the fastest rate of convergence plotted. The best step-size for SAG is usually not what the theory suggests. Schmidt et al. (2013) suggest using 1L instead of the theoretical rate 116L . For Finito, we find that using α = 2 is the fastest rate when the big data condition holds for any β > 1. This is the step suggested by our theory when β = 2. Interestingly, reducing α to 1 does not improve the convergence rate. Instead we see no further improvement in our experiments.\nFor both SAG and Finito we used a differing step rule than suggested by the theory for the first pass. For Finito, during the first pass, since we do not have derivatives for each φi yet, we simply sum over the k terms seen so far\nw(k) = 1\nk k∑ i φ (k) i − 1 αsk k∑ i f ′i(φ (k) i ),\nwhere we process data points in index order for the first pass only. A similar trick is suggested by Schmidt et al. (2013) for SAG.\nSince SDCA only applies to linear predictors, we are restricted in possible test problems. We choose log loss for 3 binary classification datasets, and quadratic loss for 2 regression tasks. For classification, we tested on the ijcnn1 and covtype datasets1, as well as MNIST2 classifying 0- 4 against 5-9. For regression, we choose the two datasets from the UCI repository: the million song year regression\n1http://www.csie.ntu.edu.tw/˜cjlin/ libsvmtools/datasets/binary.html\n2http://yann.lecun.com/exdb/mnist/\ndataset, and the slice-localization dataset. The training portion of the datasets are of size 5.3×105, 5.0×104, 6.0×104, 4.7× 105 and 5.3× 104 respectively.\nFigure 6 shows the results of our experiments. Firstly we can see that LBFGS is not competitive with any of the incremental gradient methods considered. Secondly, the nonpermuted SAG, Finito and SDCA often converge at very similar rates. The observed differences are usually down to the speed of the very first pass, where SAG and Finito are using the above mentioned trick to speed their convergence. After the first pass, the slopes of the line are usually comparable. When considering the methods with permutation each pass, we see a clear advantage for Finito. Interestingly, it gives very flat lines, indicating very stable convergence."
    }, {
      "heading" : "8. Related work",
      "text" : "Traditional incremental gradient methods (Bertsekas, 2010) have the same form as SGD, but applied to finite sums. Essentially they are the non-online analogue of SGD. Applying SGD to strongly convex problems does not yield linear convergence, and in practice it is slower than the linear-converging methods we discuss in the remainder of this section.\nBesides the methods that fall under the classical Incremental gradient moniker, SAG and MISO (Mairal, 2013) methods are also related. MISO method falls into the class of upper bound minimization methods, such as EM and classical gradient descent. MISO is essentially the Finito method, but with step sizes n times smaller. When using these larger step sizes, the method is no longer a upper bound minimization method. Our method can be seen as MISO, but with a step size scheme that gives neither a lower nor upper bound minimisation method. While this work was under peer review, a tech report (?) was put on arXiv that establishes the convergence rate of MISO with step α = 1 and with β = 2 as 1− 13n per step. This similar but not quite as good as the 1− 12n rate we establish.\nStochastic Dual Coordinate descent (Shalev-Shwartz & Zhang, 2013) also gives fast convergence rates on problems for which it is applicable. It requires computing the convex conjugate of each fi, which makes it more complex to implement. For the best performance it has to take advantage of the structure of the losses also. For simple linear classification and regression problems it can be effective. When using a sparse dataset, it is a better choice than Finito due to the memory requirements. For linear predictors, its theoretical convergence rate of ( 1− β(1+β)n ) per step is a little faster than what we establish for Finito, however it does not appear to be faster in our experiments.\nFigure 6. Convergence rate plots for test problems"
    }, {
      "heading" : "9. Conclusion",
      "text" : "We have presented a new method for minimization of finite sums of smooth strongly convex functions, when there is a sufficiently large number of terms in the summation. We additionally develop some theory for the lower complexity bounds on this class, and show the empirical performance of our method."
    } ],
    "references" : [ {
      "title" : "Incremental gradient, subgradient, and proximal methods for convex optimization: A survey",
      "author" : [ "Bertsekas", "Dimitri P" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Bertsekas and P.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bertsekas and P.",
      "year" : 2010
    }, {
      "title" : "Optimization with first-order surrogate functions",
      "author" : [ "Mairal", "Julien" ],
      "venue" : null,
      "citeRegEx" : "Mairal and Julien.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mairal and Julien.",
      "year" : 2013
    }, {
      "title" : "Stochastic Optimization: Algorithms and Applications, chapter Convergence Rate of Incremental Subgradient Algorithms",
      "author" : [ "Nedic", "Angelia", "Bertsekas", "Dimitri" ],
      "venue" : "Kluwer Academic,",
      "citeRegEx" : "Nedic et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Nedic et al\\.",
      "year" : 2000
    }, {
      "title" : "On an approach to the construction of optimal methods of minimization of smooth convex functions",
      "author" : [ "Nesterov", "Yu" ],
      "venue" : "Ekonomika i Mateaticheskie Metody,",
      "citeRegEx" : "Nesterov and Yu.,? \\Q1988\\E",
      "shortCiteRegEx" : "Nesterov and Yu.",
      "year" : 1988
    }, {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization problems",
      "author" : [ "Nesterov", "Yu" ],
      "venue" : "Technical report, CORE,",
      "citeRegEx" : "Nesterov and Yu.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nesterov and Yu.",
      "year" : 2010
    }, {
      "title" : "Beneath the valley of the noncommutative arithmetic-geometric mean inequality: conjectures, case-studies, and consequences",
      "author" : [ "Recht", "Benjamin", "Re", "Christopher" ],
      "venue" : "Technical report, University of Wisconsin-Madison,",
      "citeRegEx" : "Recht et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2012
    }, {
      "title" : "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
      "author" : [ "Richtarik", "Peter", "Takac", "Martin" ],
      "venue" : "Technical report, University of Edinburgh,",
      "citeRegEx" : "Richtarik et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Richtarik et al\\.",
      "year" : 2011
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "Schmidt", "Mark", "Roux", "Nicolas Le", "Bach", "Francis" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Schmidt et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 2013
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "Shalev-Shwartz", "Shai", "Zhang", "Tong" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "The recently developed SAG algorithm (Schmidt et al., 2013) has shown that even with this simple form of structure, as long as we have sufficiently many data points we are able to do significantly better than black-box optimization techniques in expectation for smooth strongly convex problems.",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "Schmidt et al. (2013) suggest using 1 L instead of the theoretical rate 1 16L .",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "A similar trick is suggested by Schmidt et al. (2013) for SAG.",
      "startOffset" : 32,
      "endOffset" : 54
    } ],
    "year" : 2014,
    "abstractText" : "Recent advances in optimization theory have shown that smooth strongly convex finite sums can be minimized faster than by treating them as a black box ”batch” problem. In this work we introduce a new method in this class with a theoretical convergence rate four times faster than existing methods, for sums with sufficiently many terms. This method is also amendable to a sampling without replacement scheme that in practice gives further speed-ups. We give empirical results showing state of the art performance.",
    "creator" : "LaTeX with hyperref package"
  }
}