{
  "name" : "1602.02454.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Efficient Algorithms for Adversarial Contextual Learning",
    "authors" : [ "Vasilis Syrgkanis", "Akshay Krishnamurthy", "Robert E. Schapire" ],
    "emails" : [ "VASY@MICROSOFT.COM", "AKSHAYKR@CS.CMU.EDU", "SCHAPIRE@MICROSOFT.COM" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 2.\n02 45\n4v 1\n[ cs\n.L G\n] 8\nF eb\n√\nK log(N)) in the transductive setting and O(T 2/3d3/4K √\nlog(N)) in the separator setting, where K is the number of actions, N is the number of baseline policies, and d is the size of the separator. We actually solve the more general adversarial contextual semi-bandit linear optimization problem, whilst in the full information setting we address the even more general contextual combinatorial optimization. We provide several extensions and implications of our algorithms, such as switching regret and efficient learning with predictable sequences."
    }, {
      "heading" : "1. Introduction",
      "text" : "We study contextual online learning, a powerful framework that encompasses a wide range of sequential decision making problems. Here, on every round, the learner receives\ncontextual information which can be used as an aid in selecting an action. In the full-information version of the problem, the learner then observes the loss that would have been suffered for each of the possible actions, while in the much more challenging bandit version, only the loss that was actually incurred for the chosen action is observed. The contextual bandit problem is of particular practical relevance, with applications to personalized recommendations, clinical trials, and targeted advertising.\nAlgorithms for contextual learning, such as Hedge (Freund & Schapire, 1997; Cesa-Bianchi et al., 1997) and Exp4 (Auer et al., 1995), are well-known to have remarkable theoretical properties, being effective even in adversarial, non-stochastic environments, and capable of performing almost as well as the best among an exponentially large family of policies, or rules for choosing actions at each step. However, the space requirements and running time of these algorithms are generally linear in the number of policies, which is far too expensive for a great many applications which call for an extremely large policy space. In this paper, we address this gap between the statistical promise and computational challenge of algorithms for contextual online learning in an adversarial setting.\nAs an approach to solving online learning problems, we posit that the corresponding batch version is solvable. In other words, we assume access to a certain optimization oracle for solving an associated batch-learning problem. Concrete instances of such an oracle include empirical risk minimization procedures for supervised learning, algorithms for the shortest paths problem, and dynamic programming.\nSuch an oracle is central to the Follow-the-PerturbedLeader algorithms of Kalai & Vempala (2005), although these algorithms are not generally efficient since they require separately “perturbing” each policy in the entire\nspace. Oracles of this kind have also been used in designing efficient contextual bandit algorithms (Agarwal et al., 2014; Langford & Zhang, 2008; Dudı́k et al., 2011); however, these require a much more benign setting in which contexts and losses are chosen randomly and independently rather than by an adversary.\nIn this paper, for a wide range of problems, we present computationally efficient algorithms for contextual online learning in an adversarial setting, assuming oracle access. We give results for both the full-information and bandit settings. To the best of our knowledge, these results are the first of their kind at this level of generality.\nOverview of results. We begin by proposing and analyzing in Section 2 a new and general Follow-the-PerturbedLeader algorithm in the style of Kalai & Vempala (2005). This algorithm only accesses the policy class using the optimization oracle.\nWe then apply these results in Section 3 to two settings. The first is a transductive setting (Ben-David et al., 1997) in which the learner knows the set of arriving contexts a priori, or, less stringently, knows only the set, but not necessarily the actual sequence or multiplicity with which each context arrives. In the second, small-separator setting, we assume that the policy space admits the existence of a small set of contexts, called a separator, such that any two policies differ on at least one context from the set. The size of the smallest separator for a particular policy class can be viewed as a new measure of complexity, different from the VC dimension, and potentially of independent interest.\nWe study these for a generalized online learning problem called online combinatorial optimization, which includes as special cases transductive contextual experts, online shortest-path routing, online linear optimization (Kalai & Vempala, 2005), and online submodular minimization (Hazan & Kale, 2012).\nIn Section 4, we extend our results to the bandit setting, or in fact, to the more general semi-bandit setting, using a technique of Neu & Bartók (2013). Among our main results, we obtain regret bounds for the adversarial contextual bandit problem of O(T 3/4 √\nK log(N)) in the transductive setting, and O(T 2/3d3/4K √\nlog(N)) in the smallseparator setting, where T is the number of time steps, K the number of actions, N the size of the policy space, and d the size of the separator. Being sublinear in T , these bounds imply the learner’s performance will eventually be almost as good as the best policy, although they are worse than the generally optimal dependence on T of O( √ T ), obtained by many of the algorithms mentioned above. On the other hand, these preceding algorithms are computationally intractable when the policy space is gigantic, while ours runs in polynomial time, assuming access to an optimization or-\nacle. Improving these bounds without sacrificing computational efficiency remains an open problem.\nIn Section 5, we give an efficient algorithm when regret is measured in comparison to a competitor that is allowed to switch from one policy to another a bounded number of times. Here, we show that the optimization oracle can be efficiently implemented given an oracle for the original policy class. Specifically, this leads to a fully efficient algorithm for the online switching shortest path problem in directed acyclic graphs.\nFinally, Section 6 shows how “path length” regret bounds can be derived in the style of Rakhlin & Sridharan (2013b). Such bounds have various applications, for instance, in obtaining better bounds for playing repeated games (Rakhlin & Sridharan, 2013a; Syrgkanis et al., 2015).\nOther related work. Contextual, transductive online learning using an optimization oracle was previously studied by Kakade & Kalai (2005), whose work was later extended and improved by Cesa-Bianchi & Shamir (2011) using a generalization of a technique from Cesa-Bianchi et al. (1997). However, these previous results are for binary classification or other convex losses defined on one-dimensional predictions and outcomes; as such, they are special cases of the much more general setting we consider in the present paper.\nAwerbuch & Kleinberg (2008) present an efficient algorithm for the online shortest paths problem. This can be viewed as solving an adversarial bandit problem with a very particular optimization oracle over an exponentially large but highly structured space of “policies” corresponding to paths in a graph. However, their setting is clearly far more restrictive and structured than ours is."
    }, {
      "heading" : "2. Online Learning with Oracles",
      "text" : "We start by analyzing the family of Follow the Perturbed Leader algorithms in a very general online learning setting. Parts of this generic formulation follow the recent formulation of Daskalakis & Syrgkanis (2015), but we present a more refined analysis which is essential for our contextual learning result in the next sections. The main theorem of this section is essentially a generalization of Theorem 1.1 of Kalai & Vempala (2005).\nConsider an online learning problem where at each timestep an adversary picks an outcome yt ∈ Y and the algorithm picks a policy πt ∈ Π from some policy space Π.1 The algorithm receives a loss: ℓ(πt, yt), which could be\n1We refer to the choice of the learner as a policy, for uniformity of notation with subsequent sections, where the learner will choose some policy that maps contexts to actions.\npositive or negative. At the end of each iteration the algorithm observes the realized outcome yt. We will denote with y1:t a sequence of outcomes {y1, y2, . . . , yt}. Moreover, we denote with:\nL(π, y1:t) = t ∑\nτ=1\nℓ(π, yτ ), (1)\nthe cumulative utility of a fixed policy π ∈ Π for a sequence of choices y1:t of the adversary. The goal of the learning algorithm is to achieve loss that is competitive with the best fixed policy in hindsight. As the algorithms we consider will be randomized, we will analyze the expected regret,\nREGRET = sup π⋆∈Π E\n[\nT ∑\nt=1\nℓ(πt, yt)− T ∑\nt=1\nℓ(π⋆, yt)\n]\n, (2)\nwhich is the worst case difference between the cumulative loss of the learner and the loss of any fixed policy π ∈ Π. We consider adversaries that are adaptive, which means that they can choose the outcome yt at time t, using knowledge of the entire history of interaction. The only knowledge not available to an adaptive adversary is any randomness used by the learning algorithm at time t. In contrast, an oblivious adversary is one that picks the sequence of outcomes y1:T before the start of the learning process.\nTo develop computationally efficient algorithms that compete with large sets of policies Π, we assume that we are given oracle access to the following optimization problem.\nDefinition 1 (Optimization oracle). Given outcomes y1:t compute the fixed optimal policy for this sequence:\nM ( y1:t ) = argminπ∈ΠL(π, y1:t). (3)\nWe will also assume that the oracle performs consistent deterministic tie-breaking: i.e. whenever two policies are tied, then it always outputs the same policy.\nIn this generic setting, we define a new family of FollowThe-Perturbed-Leader (FTPL) algorithms where the perturbation takes the form of extra samples of outcomes (see Algorithm 1). In each round, the learning algorithm draws a random sequence of outcomes independently, and appends this sequence to the outcomes experienced during the learning process. The algorithm invokes the oracle on this augmented outcome sequence, and plays the resulting policy.\nPerturbed Leader Regret Analysis. We give a general theorem on the regret of a perturbed leader algorithm with sample perturbations. In the sections that follow we will give instances of this analysis in specific settings.\nAlgorithm 1 Follow the perturbed leader with fake sample perturbations - FTPL.\nfor each time step t do Draw a random sequence of outcomes {z} = (z1, . . . , . . . , zk) independently, based on some timeindependent distribution over sequences. Both the length of the sequence and the outcome zi ∈ Y at each iteration of the sequence can be random Denote with {z} ∪ y1:t−1 the augmented sequence where we append the extra outcome samples {z} at the beginning of sequence y1:t−1\nInvoke oracle M and play policy:\nπt = M ( {z} ∪ y1:t−1 ) . (4)\nend for\nTheorem 1. For a distribution over sample sequences {z} and a sequence of adversarially and adaptively chosen outcomes y1:T , define:\nSTABILITY = T ∑\nt=1\nE{z} [ ℓ(πt, yt)− ℓ(πt+1, yt) ]\nERROR = E{z}\n\nmax π∈Π\n∑\nzτ∈{z} ℓ(π, zτ )\n\n\n− E{z}\n\nmin π∈Π\n∑\nzτ∈{z} ℓ(π, zτ)\n\n ,\nwhere πt is defined in Equation (4). Then the expected regret of Algorithm 1 is upper bounded by,\nREGRET ≤ STABILITY + ERROR. (5)\nThis theorem shows that any FTPL-variant where the perturbation can be described as a random sequence of outcomes has regret bounded by the two terms STABILITY and ERROR. Below we will instantiate this theorem to obtain concrete regret bounds for several problems.\nThe proof of the theorem is based on a well-known “bethe-leader” argument. We first show that if we included the tth loss vector in the oracle call at round t, we would have regret bounded by ERROR, and then we show that the difference between our algorithm and this foreseeing one is bounded by STABILITY. See Appendix A for the proof."
    }, {
      "heading" : "3. Adversarial Contextual Learning",
      "text" : "Our first specialization of the general setting is to contextual online combinatorial optimization. In this learning setting, at each iteration, the learning algorithm picks a binary\nAlgorithm 2 Contextual Follow the Perturbed Leader Algorithm - CONTEXT-FTPL(X, ǫ).\nInput: parameter ǫ, set of contexts X , policies Π. for each time step t do\nDraw a sequence {z} = (z1, . . . , zd) of d fake samples. The context associated with sample zx is equal to x and each coordinate of the loss vector ℓx is drawn i.i.d. from a Laplace(ǫ) Pick and play according to policy\nπt = M({z} ∪ y1:t−1) (6)\nend for\naction vector at ∈ A ⊆ {0, 1}K , from some feasibility set A. We will interchangeably use at both as a vector and as the set {j ∈ [K] : at(j) = 1}. The adversary picks a outcome yt = (xt, f t) where xt belongs to some context space X and f t : A → R is a cost function that maps each feasible action vector a ∈ A to a cost f t(a). The goal of the learning algorithm is to achieve low regret relative to a set of policies Π ⊂ (X → A) that map contexts to feasible action vectors. At each iteration the algorithm picks a policy πt and incurs a cost ℓ(πt, yt) = f t(πt(xt)). In this section, we consider the full-information problem, where after each round, the entire loss function f t is revealed to the learner. Online versions of a number of important learning tasks, including cost-sensitive classification, multi-label prediction, online linear optimization (Kalai & Vempala, 2005) and online submodular minimization (Hazan & Kale, 2012) are all special cases of the contextual online combinatorial optimization problem, as we will see below.\nContextual Follow the Perturbed Leader. We will analyze the performance of an instantiation of the FTPL algorithm in this setting. To specialize the algorithm, we need only specify the distribution from which the sequence of fake outcomes {z} is drawn at each time-step. This distribution is parameterized by a subset of contexts X ⊆ X , with |X | = d and a noise parameter ǫ. We draw the sequence {z} as follows: for each context x ∈ X , we add the fake sample zx = (x, fx) where fx is a linear loss function based on a loss vector ℓx ∈ RK , meaning that fx(a) = 〈a, ℓx〉. Each coordinate of the loss vector ℓx is drawn from a independent Laplace distribution with parameter ǫ, i.e. for each coordinate j ∈ [K] the density of ℓx(j) at q is f(q) = ǫ2 exp{−ǫ|q|}. The latter distribution has mean 0 and variance 2ǫ2 . Using this distribution for fake samples gives an instantiation of Algorithm 1, which we refer to as CONTEXT-FTPL(X, ǫ) (see Algorithm 2).\nWe analyze CONTEXT-FTPL(X, ǫ) in two settings: the transductive setting and the small separator setting.\nDefinition 2. In the transductive setting, at the beginning of the learning process, the adversary reveals to the learner the set of contexts that will arrive, although the ordering and multiplicity need not be revealed.\nDefinition 3. In the small separator setting, there exists a set X ⊂ X such that for any two distinct policies π, π′ ∈ Π, there exists x ∈ X such that π(x) 6= π′(x).\nIn the transductive setting, the set X that we use in CONTEXT-FTPL(X, ǫ) is precisely this set of contexts that will arrive, which by assumption is available to the learning algorithm. In this small separator setting, the set X used by CONTEXT-FTPL is the separating set. This enables nontransductive learning, but one must be able to compute a small separator prior to learning. Below we will see examples where this is possible.\nWe now turn to bounding the regret of CONTEXT-FTPL(X, ǫ). Let d = |X | be the number of contexts that are used in the definition of the noise distribution, let N = |Π| ≤ dK , and let m denote the maximum number of non-zero coordinates that any policy can choose on any context, i.e. m = maxa∈A ‖a‖1. Even though at times we might constrain the sequence of loss functions that the adversary can pick (e.g. linear non-negative losses), we will assume that the oracle M can handle at least linear loss functions with both positive and negative coordinates. Our main result is:\nTheorem 2 (Complete Information Regret). CONTEXT-FTPL(X, ǫ) achieves regret against any adaptively and adversarially chosen sequence of contexts and loss functions:\n1. In the transductive setting:\nREGRET ≤ 4ǫK · T ∑\nt=1\nE [ ‖f t‖2∗ ]\n+ 10\nǫ\n√ dm log(N)\n2. In the transductive setting, when loss functions are linear and non-negative, i.e. f t(a) = 〈a, ℓt〉 with ℓt ∈ RK≥0:\nREGRET ≤ ǫ · T ∑\nt=1\nE [ 〈πt(xt), ℓt〉2 ] + 10\nǫ\n√ dm log(N)\n3. In the small separator setting:\nREGRET ≤ 4ǫKd · T ∑\nt=1\nE [ ‖f t‖2∗ ]\n+ 10\nǫ\n√ dm log(N)\nwhere ‖f t‖∗ = maxa∈A |f t(a)|.\nWhen ǫ is set optimally, loss functions are in [0, 1], and loss vectors are in [0, 1]K , these give regret:2 O ( (dm)1/4 √ KT log(N) ) in the first set-\nting, O ( d1/4m5/4 √ T log(N) ) in the second and\nO ( m1/4d3/4 √ KT log(N) ) in the third.\nTo prove the theorem we separately upper bound the STABILITY and the ERROR terms and then Theorem 2 follows from Theorem 1. One key step is a refined ERROR analysis that leverages the symmetry of the Laplace distribution to obtain a bound with dependence √ d rather than d. This is possible only if the perturbation is centered about zero, and therefore does not apply to other FPTL variants that use non-negative distributions such as exponential or uniform (Kalai & Vempala, 2005). Due to lack of space we defer proof details to Appendix B.\nThis general theorem has implications for many specific settings that have been extensively studied in the literature. We turn now to some examples.\nExample 1. (Transductive Contextual Experts) The contextual experts problem is the online version of costsensitive multiclass classification, and the full-information version of the widely-studied contextual bandit problem. The setting is as above, but A corresponds to sets with cardinality 1, meaning that m = 1 in our formulation. As a result, CONTEXT-FTPL can be applied as is, and the second claim in Theorem 2 shows that the algorithm has regret at most O ( d1/4 √ T log(N) ) if at most d contexts\narrive. In the worst case this bound is O(T 3/4 √\nlog(N)), since the adversary can choose at most T contexts. To our knowledge, this is the first fully oracle-efficient algorithm for online adversarial cost-sensitive multiclass classification, albeit in the transductive setting.\nThis result can easily be lifted to infinite policy classes that have small Natarajan Dimension (a multi-class analog of VC-dimension), since such classes behave like finite ones once the set of contexts is fixed. Thus, in the transductive setting, Theorem 2 can be applied along with the analog of the Sauer-Shelah lemma, leading to a sublinear regret bound for classes with finite Natarajan dimension. On the other hand, in the non-transductive case it is possible to construct examples where achieving sublinear regret against a VC class is information-theoretically hard, demonstrating a significant difference between the two settings. See Corollary 15 and Theorem 16 in the Appendix E for details on these arguments.\nExample 2. (Non-contextual Shortest Path Routing and Linear Optimization) For the case when the linear op-\n2Observe that when loss vectors are in [0, 1]K , then the linear loss function is actually in [0, m] not in [0, 1].\ntimization corresponds to computing the shortest (s, t)path in a DAG, then K and m equal to the number of edges and the problem can be solved in poly-time even when edge costs are negative. More generally, CONTEXT-FTPL can also be applied to non-contextual problems, which is a special case where d = 1. In such a case, CONTEXT-FTPL reduces to the classical FTPL algorithm with Laplace instead of Exponential noise, and Theorem 2 matches existing results for online linear optimization (Kalai & Vempala, 2005). In particular, for problems without context, CONTEXT-FTPL has regret that scales with √ T .\nExample 3. (Online sub-modular minimization) A special case of our setting is the online-submodular minimization problem studied in previous work (Hazan & Kale, 2012; Jegelka & Bilmes, 2011). As above, this is a non-contextual online combinatorial optimization problem, where the loss function f t presented at each round is submodular. Here, CONTEXT-FTPL reduces to the strongly polynomial algorithm of Hazan & Kale (2012), although our noise follows a Laplace instead of Uniform distribution. A straightforward application of the first claim of Theorem 2 shows that CONTEXT-FTPL achieves regret at most O(KH √\nT log(K)) if the losses are bounded in [−H,H ], and a slightly refined analysis of the error terms gives O(KH √ T ) regret. This matches the FTPL analysis of Hazan & Kale (2012), although they also develop an algorithm based on online convex optimization that achieves O(H √ KT ) regret.\nExample 4. (Contextual Experts with linear policy classes) The third clause of Theorem 2 gives strong guarantees for the non-transductive contextual experts problem, provided one can construct a small separating set of contexts. Often this is possible, and we provide some examples here.\n1. For binary classification where the policies are boolean disjunctions (conjunctions) over n binary variables, the set of 1-sparse (n − 1-sparse) boolean vectors form a separator of size n. This is easy to see as two disjunctions must disagree on at least one variable, so they will make different predictions on the vector that is non-zero only in that component. Note that the size of the small separator is independent of the time horizon T and logarithmic in the number of policies. Thus, Theorem 2 shows that CONTEXT-FTPL suffers at most O( √ T log(N)) re-\ngret since d = log(N),m = 1 and K = 2.\n2. For binary classification in n dimensions, consider a discretization of linear classifiers defined as follows, the separating hyperplane of each classifier is defined by choosing the intercept with each axis from one of\nO(1/τ) values (possibly including something denoting no intercept). Then a small separator includes, for each axis, one point between each pair in the discretization, for a total of O(n/τ) points. This follows since any two distinct classifiers have different intercepts for at least one axis, and our small separator has one point between these two different intercepts, leading to different predictions. Note that the number of classifiers in the discretization is O(τ−n). Here Theorem 2 shows that CONTEXT-FTPL suffers at most O(n √ T\nτ3/4 (log( 1τ )) 1/4) regret since N = O(τ−n), d = n τ ,m = 1 and K = 2. This bound has a undesireable polynomial dependence on the discretization resolution τ but avoids exponential dimension dependence.\nThus we believe that the smallest separator size for a policy class can be viewed as a new complexity measure, which may be of independent interest."
    }, {
      "heading" : "4. Linear Losses and Semi-Bandit Feedback",
      "text" : "In this section, we consider contextual learning with semibandit feedback and linear non-negative losses. At each round t of this learning problem, the adversary chooses a non-negative vector ℓt ∈ RK≥0 and sets the loss function to f t(a) = 〈a, ℓt〉. The learner chooses an action at ∈ A ⊂ {0, 1}K accumulates loss f t(at) and observes ℓt(j) for each j ∈ at. In other words, the learner observes the coefficients for only the elements in the set that he picked. Notice that if A is the one-sparse vectors, then this setting is equivalent to the well-studied contextual bandit problem (Langford & Zhang, 2008).\nSemi-bandit algorithm. Our semi-bandit algorithm proceeds as follows: At each iteration it makes a call to CONTEXT-FTPL(ǫ), which returns a policy πt and implies a chosen action at = πt(xt). The algorithm plays the action at, observes the coordinates of the loss {ℓt(j)}j∈at and proceeds to construct an proxy loss vector ℓ̂t, which it passes to the instance of CONTEXT-FTPL, before proceeding to the next round.\nTo describe the construction of ℓ̂t, let pt(π) = Pr[πt = π|Ht−1] denote the probability that CONTEXT-FTPL returns policy π at time-step t conditioned on the past history (observed losses and contexts, chosen actions, current iteration’s context, internal randomness etc., which we denote with Ht−1). For any element j ∈ [K], let:\nqt(j) = ∑\nπ∈Π:j∈π(xt) pt(π) (7)\ndenote the probability that element j is included in the action chosen by CONTEXT-FTPL(X, ǫ) at time-step t.\nTypical semi-bandit algorithms aim to construct proxy loss vectors by dividing the observed coordinates of the loss by the probabilities qt(j) and setting other coordinates to zero, which is the well-known inverse propensity scoring mechanism (Horvitz & Thompson, 1952). Unfortunately, in our case, the probabilities qt(j) stem from randomness fed into the oracle, so that they are implicit maintained and therefore must be approximated.\nWe therefore construct ℓ̂t through a geometric sampling scheme due to Neu & Bartók (2013). For each j ∈ πt(xt), we repeatedly invoke the current execution of the CONTEXT-FTPL algorithm with fresh noise, until it returns a policy that includes j in its action for context xt. The process is repeated at most L times for each j ∈ πt(xt) and the number of invocations is denoted J t(j). The vector ℓ̂t that is returned to the full feedback algorithm is zero for all j /∈ πt(xt), and for each j ∈ πt(xt) it is ℓ̂t(j) = J t(j) · ℓt(j). By Lemma 1 of Neu & Bartók (2013), this process yields a proxy loss vector ℓ̂t that satisfies,\nE\n[ ℓ̂t(j) | Ht−1 ] = ( 1− ( 1− qt(j) )L ) ℓt(j). (8)\nThe semi-bandit algorithm feeds this proxy loss vector to the CONTEXT-FTPL instance and proceeds to the next round.\nThe formal description of the complete bandit algorithm is given in Algorithm 3 and we refer to it as CONTEXT-SEMI-BANDIT-FTPL(X, ǫ, L). We bound its regret in the transductive and small separator setting.\nTheorem 3. The expected regret of CONTEXT-SEMI-BANDIT-FTPL(X, ǫ, L) in the semibandit setting against any adaptively and adversarially chosen sequence of contexts and linear non-negative losses, with ‖ℓt‖∗ ≤ 1, is at most:\n• In the transductive setting:\nREGRET ≤ 2ǫmKT + 10 ǫ\n√ dm log(N) + KT\neL\n• In the small separator setting:\nREGRET ≤ 8ǫK2dLmT+10 ǫ\n√ dm log(N)+ KT\neL\nFor L = √ KT and optimal ǫ, the regret is O ( d1/4m3/4 √ KT log(N) ) in the first setting.\nFor L = T 1/3 and optimal ǫ, the regret is O ( (md)3/4KT 2/3 √ log(N) ) in the second setting.\nMoreover, each iteration of the algorithm requires mL oracle calls and otherwise runs in polynomial time in d,K .\nAlgorithm 3 Contextual Semi-Bandit Algorithm - CONTEXT-SEMI-BANDIT-FTPL(X, ǫ, L).\nInput: parameter ǫ,M , set of contexts X , policies Π. Let D denote a distribution over a sequence of d samples, {z} = (z1, . . . , zd), where the context associated with sample zx is equal to x and each coordinate of the loss vector ℓx is drawn i.i.d. from a Laplace(ǫ) for each time-step t do\nDraw a sequence {z}t from distribution D. Pick and play according to policy\nπt = M({z} ∪ (x1:t−1, ℓ̂1:t−1)) (9)\nObserve loss ℓt(j) for each j ∈ πt(xt) Set ℓ̂t(j) = 0 for any j /∈ πt(xt) Set ℓ̂t(j) = J t(j) · ℓt(j), for each j ∈ πt(xt), where J t(j) is computed by the following geometric sampling process: for each element j ∈ πt(xt) do\nfor each iteration i = 1, . . . , L do Draw a sequence {y}i from distribution D. Compute πi = M({y}i ∪ (x1:t−1, ℓ̂1:t−1)) If j ∈ πi(xt) then stop and return J t(j) = i\nend for end for If process finished without setting J t(j), then set J t(j) = L\nend for\nThis is our main result for adversarial variants of the contextual bandit problem. In the most well-studied setting, i.e. contextual bandits, we have m = 1, so our regret bound is O(d1/4 √\nKT log(N)) in the transductive setting and O(d3/4KT 2/3 √\nlog(N)) in the small separator setting. Since for the transductive case d ≤ T and for the small-separator case d can be independent of T (see discussion above), this implies sublinear regret for adversarial contextual bandits in either setting. To our knowledge this is the first oracle-efficient sublinear regret algorithm for variants of the contextual bandit problem. However, as we mentioned before, neither regret bound matches the optimal O( √\nKT log(N)) rate for this problem, which can be achieved by computationally intractable algorithms. An interesting open question is to develop computationally efficient, statistically optimal contextual bandit algorithms."
    }, {
      "heading" : "5. Switching Policy Regret",
      "text" : "In this section we analyze switching regret for the contextual linear optimization setting, i.e. regret that compares to the best sequence of policies that switches at most k times. Such a notion of regret was first analyzed by Herbster & Warmuth (1998) and several algo-\nrithms, that are not computationally efficient for large policy spaces, have been designed since then (e.g. (Luo & Schapire, 2015)). Our results provide the first computationally efficient switching regret algorithms assuming offline oracle access.\nFor this setting we will assume that the learner knows the exact sequence x1:T of contexts ahead of time and not only the set of potential contexts. The extension stems from the realization that we can simply think of time t as part of the context at time-step t. Thus now the contexts are of the form x̃t = (t, xt). Moreover, policies in the augmented context space are now of the form: π̃(x̃t) = πI(t)(xt), where I(t) is a selector which maps a time-step t to a policy π ∈ Π, with the constraint that the number of time-steps such that I(t) 6= I(t − 1) is at most k. If the original policy space Π was of size N , the new policy space, denoted Π̃, is of size Ñ at most T kNk, since there are at most T k partitions of time into k consequetive intervals and each of the k intervals can be occupied by N possible policies. Moreover, in this augmented context space, the number of possible contexts, denoted X̃ is equal to d̃ = T .\nThus if we run CONTEXT-FTPL(X, ǫ) on this augmented context and policy space, Theorem 2, bounds the regret against all policies in the augmented policy space Π̃. Since, regret against the augmented policy space, corresponds to switching regret against the original set of policies, the following corollary is immediate:\nCorollary 4 (Contextual Switching Regret). In the transductive complete information setting, CONTEXT-FTPL(X̃, ǫ) applied to the augmented policy space Π̃, achieves k-switching regret against any adaptively and adversarially chosen sequence of contexts and losses at most: O ( m1/4 √ Kk log(TN)T 3/4 ) for general\nloss functions in [0, 1] and O ( √ k log(TN)m5/4T 3/4 )\nfor linear losses with loss vectors in [0, 1]K .\nIt remains to show is that we can efficiently solve the offline optimization problem for the new policy space Π̃, if we have access to an optimization oracle for the original policy space Π. Then we can claim that CONTEXT-FTPL(X̃, ǫ) in the augmented context and policy space is also an efficient algorithm. We show that the latter is true via a dynamic programming approach. The approach generalizes beyond contextual linear optimization settings.\nLemma 5. The oracle M̃ in the augmented space,\nM̃(ỹ1:T ) = arginfπ̃∈Π̃\nT ∑\nτ=1\n〈π̃(τ, xτ ), ℓτ 〉 (10)\nis computable in O(Tk) time, with O(T 2) calls to the oracle over the original space, M . This process can be amor-\ntized so that solving a sequence of T problems in the augmented space requires O(T 2) calls to M in total.\nProof. Oracle M̃ must compute the best sequence of policies π1, . . . , πT , such that πt 6= πt−1 at most k times. Let R(t, q) denote the loss of the optimal sequence of policies up to time-step t and with at most q switches. Then it is easy to see that:\nR(t, q) = min τ≤t\nR(τ, q−1)+L ( M(yτ+1:t), yτ+1:t ) , (11)\ni.e. compute the best sequence of policies up till some time step τ ≤ t with at most q− 1 switches and then augment it with the optimal fixed policy for the period (τ +1, t). Then take the best over possible times τ ≤ t. This can be implemented by first invoking oracle M for every possible period [τ1, τ2]. Then filling up iteratively all the entries R(t, q). For q = 0, the problem R(t, 0) corresponds to exactly the original oracle problem M , hence for each t, we can solve the problem R(t, 0). Computing all values of R(t, q) then takes time Tk in total.\nExample 5. (Efficient switching regret for non-contextual problems) When the original space has no contexts, our result above implies the first efficient sub-linear switching regret algorithm for online linear optimization. In this case, the transductivity assumption is trivially satisfied as there is no contextual information, and our the instance of CONTEXT-FTPL runs on a sequence of contexts that just encode time. One concrete example where linear optimization with both positive and negative weights is polynomially solvable is the online shortest path problem on a directed acyclic graph. Our result implies a fully efficient, sublinear switching regret algorithm for the online shortest-path problem on a DAG, and our algorithm performs t shortest-path computations at the tth iteration. The result also covers other examples, such as online matroid optimization."
    }, {
      "heading" : "6. Efficient Path Length Regret Bounds",
      "text" : "In this section we examine a variant of our CONTEXT-FTPL(ǫ) algorithm that is efficient and achieves regret that is upper bounded by structural properties of the utility sequence. Our algorithm is framed in terms of a generic predictor that the learner has access to and the regret is upper bounded by the deviation of the true loss vector from the predictor. For specific instances of the predictor this leads to path length bounds (Chiang et al., 2012) or variance based bounds (Hazan & Kale, 2010). Our approach is general enough to allow for generalizations of variance and path length that can incorportate contextual information and can be viewed as an efficient version and a generalization of\nthe results of Rakhlin & Sridharan (2013b) on learning with predictable sequences. Such results have also found applications in learning in game theoretic environments (Rakhlin & Sridharan, 2013a; Syrgkanis et al., 2015).\nThe algorithm is identical to CONTEXT-FTPL(ǫ) with the exception that now the policy that is used at time-step t is:\nπt = M({z} ∪ y1:t−1 ∪ (xt, Qt)) (12)\nwhere Qt ∈ {0, 1}K → RK is an arbitrary loss function predictor, which can depend on the observed history up to time t. This predictor can be interpreted as partial side information that the learner has about the loss function that will arrive at time-step t. Given such a predictor we can define the error between the predictor and the actual sequence:\nEt = E [ ‖f t −Qt‖2∗ ]\n(13)\nTheorem 6 (Predictor based regret bounds). The regret of CONTEXT-FTPL(X, ǫ) with predictors and complete information,\n1. In the transductive setting is upper bounded by:\nREGRET ≤ 4ǫK T ∑\nt=1\nEt + 10 √ dm log(N)\nǫ\n2. In the small separator setting is upper bounded by:\nREGRET ≤ 4ǫKd T ∑\nt=1\nEt + 10 √ dm log(N)\nǫ\nPicking ǫ optimally gives regret\nO\n(\n(dm)1/4 √ K log(N) ∑T t=1 Et ) in the first set-\nting and O\n(\nm1/4d3/4 √ K log(N) ∑T t=1 Et ) in the\nsecond.\nEven without contexts, our result is the first efficient path length regret algorithm for online combinatorial optimization. For instance, for the case of non-contextual, online combinatorial optimization an instantiation of our al-\ngorithm achieves regret O\n(\nm1/4 √ K log(K) ∑T t=1 Et )\nagainst adaptive adversaries. For learning with expert, m = 1 and K is number of experts, the results of Rakhlin & Sridharan (2013b) provide a non-efficient\nO\n(\n√\nlog(K) ∑T t=1 Et ) . Thus our bound incurs an extra\ncost of √ K in comparison. Removing this extra factor of√\nK in an efficient manner is an interesting open question."
    }, {
      "heading" : "7. Discussion",
      "text" : "In this work we give fully oracle efficient algorithms for adversarial online learning problems including contextual\nexperts, contextual bandits, and problems involving linear optimization or switching experts. Our main algorithmic contribution is a new Follow-The-Perturbed-Leader style algorithm that adds perturbed low-dimensional statistics. We give a refined analysis for this algorithm that guarantees sublinear regret for all of these problems. All of our results hold against adaptive adversaries, both with full and partial feedback.\nWhile our algorithms achieve sublinear regret in all problems we consider, we do not always match the regret bounds attainable by inefficient alternatives. An interesting direction for future work is whether fully oracle-based algorithms can achieve optimal regret bounds in the settings we consider. Another interesting direction focuses on a deeper understanding of the small-separator condition and whether it enables efficient non-transductive learning in other settings. We look forward to studying these questions in future work."
    }, {
      "heading" : "A. Omitted Proofs from Section 2",
      "text" : ""
    }, {
      "heading" : "A.1. Proof of Theorem 1",
      "text" : "We prove the theorem by analyzing a slightly modified algorithm, that only draws the perturbation once at the beginning of the learning process but is otherwise identical. The bulk of the proof is devoted to bounding this modified algorithm’s regret against oblivious adversaries, i.e. an adversary that chooses the outcomes y1:T before the learning process begins. We use this regret bound along with a reduction due to Hutter and Poland (Hutter & Poland, 2005) (see their Lemma 12) to obtain a regret bound for Algorithm 1 against adaptive adversaries. We provide a proof of this reduction in Appendix A.2 and proceed here with the analysis of the modified algorithm.\nTo bound the regret of the modified algorithm, consider letting the algorithm observe yt ahead of time, so that at each time step t, the algorithm plays πt+1 = M ( {z} ∪ y1:t ) . Notice trivially that the regret of the modified algorithm is,\nREGRET = T ∑\nt=1\nℓ(πt, yt)−min π∈Π\nℓ(π∗, yt) = T ∑\nt=1\nℓ(πt, yt)− ℓ(πt+1, yt) + T ∑\nt=1\nℓ(πt+1, yt)−min π∈Π ℓ(π∗, yt)\nThe first sum here is precisely the STABILITY term in the bound, so we must show that the second sum is bounded by ERROR. This is proved by induction in the following lemma.\nLemma 7 (Be-the-leader with fixed sample perturbations). For any realization of the sample sequence {z} and for any policy π∗:\nT ∑\nt=1\n( ℓ(πt+1, yt)− ℓ(π∗, yt) ) ≤ max π∈Π\n∑\nzτ∈{z} ℓ(π, zτ )−min π∈Π\n∑\nzτ∈{z} ℓ(π, zτ ) (14)\nProof. Denote with k the length of sequence {z}. Consider the sequence {z} ∪ y1:T and let a1 = M({z}). We will show that for any policy π∗:\nk ∑\nτ=1\nℓ(π1, zτ ) +\nT ∑\nt=1\nℓ(πt+1, yt) ≤ k ∑\nτ=1\nℓ(π∗, zτ ) + T ∑\nt=1\nℓ(π∗, yt) (15)\nFor T = 0, the latter trivially holds by the definition of a1. Suppose it holds for some T , we will show that it holds for T + 1. Since the induction hypothesis holds for any π∗, applying it for aT+2, i.e.,:\nk ∑\nτ=1\nℓ(π1, zτ ) +\nT+1 ∑\nt=1\nℓ(πt+1, yt) ≤ k ∑\nτ=1\nℓ(πT+2, zτ ) +\nT ∑\nt=1\nℓ(πT+2, yt) + ℓ(πT+2, yT+1)\n=\nk ∑\nτ=1\nℓ(πT+2, zτ ) +\nT+1 ∑\nt=1\nℓ(πT+2, yt)\nBy definition of aT+2 the latter is at most: ∑k τ=1 ℓ(π ∗, zτ ) + ∑T+1 t=1 ℓ(π ∗, yt) for any π∗. Which proves the induction step. Thus, by re-arranging Equation (15) we get:\nT ∑\nt=1\n( ℓ(πt+1, yt)− ℓ(π∗, yt) ) ≤ k ∑\nτ=1\n( ℓ(π∗, zτ)− ℓ(π1, zτ) ) ≤ max π∈Π\nk ∑\nτ=1\nℓ(π, zτ )−min π∈Π\nk ∑\nτ=1\nℓ(π, zτ )\nThus the regret of the modified algorithm against an oblivious adversary is bounded by STABILITY+ERROR. By applying the reduction of Hutter and Poland (Hutter & Poland, 2005) (see Appendix A.2 for a proof sketch), the regret of Algorithm 1 is bounded is bounded in the same way.\nA.2. From adaptive to oblivious adversaries\nWe will utilize a generic reduction provided in Lemma 12 of (Hutter & Poland, 2005), which states that given that in Algorithm 1 we draw independent randomization at each iteration, it suffices to provide a regret bound only for oblivious adversaries, i.e., the adversary picks a fixed sequence y1:T ahead of time without observing the policies of the player. Moreover, for any such fixed sequence of an oblivious adversary, the expected utility of the algorithm can be easily shown to be equal to the expected utility if we draw a single random sequence {z} ahead of time and use the same random vector all the time.\nThe proof is as follows: by linearity of expectation and the fact that each sequence {z}t drawn at each time-step t is identically distributed:\nE{z}1,...,{z}t\n[\nT ∑\nt=1\nu(M({z}t ∪ y1:t−1), yt) ] = T ∑\nt=1\nE{z}t [ u(M({z}t ∪ y1:t−1), yt) ]\n=\nT ∑\nt=1\nE{z}1 [ u(M({z}1 ∪ y1:t−1), yt) ]\n=E{z}1\n[\nT ∑\nt=1\nu(M({z}1 ∪ y1:t−1), yt) ]\nThe latter is equivalent to the expected reward if we draw a single random sequence {z} ahead of time and use the same random vector all the time. Thus it is sufficient to upper bound the regret of this modified algorithm, which draws randomness only once.\nThus it is sufficient to upper bound the regret of this modified algorithm, which draws randomness only once."
    }, {
      "heading" : "B. Omitted Proofs from Section 3",
      "text" : ""
    }, {
      "heading" : "B.1. Bounding the Laplacian Error",
      "text" : "The upper bound on the ERROR term is identical in all settings, since it only depends on the input noise distribution, which is the same for all variants and for which it does not matter whether X is the set of contexts that will arrive or a separator. In subsequent sections we will upper bound the stability of the algorithm in each setting.\nLemma 8 (Laplacian Error Bound). Let {z} denote a sample from the random sequence of fake samples used by CONTEXT-FTPL(X, ǫ). Then:\nERROR = E{z}\n[\nmax π∈Π\n∑ x∈X 〈π(x), ℓx〉\n]\n− E{z}\n[\nmin π∈Π\n∑ x∈X 〈π(x), ℓx〉\n]\n≤ 10 ǫ\n√ dm log(N) (16)\nProof. First we start by observing that each random variable ℓx(j) is distributed i.i.d. according to a Laplace(ǫ) distribution. Since a Laplace distribution is symmetric around 0, we get that ℓx(j) and −ℓx(j) are distributed identically. Thus we can write:\nE{z}\n[\nmin π∈Π\n∑ x∈X 〈π(x), ℓx〉\n]\n= E{z}\n[\nmin π∈Π\n∑ x∈X 〈π(x),−ℓx〉\n]\n= −E{z}\n[\nmax π∈Π\n∑ x∈X 〈π(x), ℓx〉\n]\nHence we get:\nERROR = 2 · E{z}\n[\nmax π∈Π\n∑ x∈X 〈π(x), ℓx〉\n]\n(17)\nWe now bound the latter expectation via a moment generating function approach. For any λ ≥ 0:\nE{z}\n[\nmax π∈Π\n∑ x∈X 〈π(x), ℓx〉\n]\n= 1\nλ E{z}\n[\nmax π∈Π\nλ ∑\nx∈X 〈π(x), ℓx〉\n]\n= 1\nλ log\n{\nexp\n{\nE{z}\n[\nmax π∈Π\nλ ∑\nx∈X 〈π(x), ℓx〉\n]}}\nBy convexity and monotonicity of the exponential function:\nE{z}\n[\nmax π∈Π\n∑ x∈X 〈π(x), ℓx〉\n]\n≤ 1 λ log\n{\nE{z}\n[\nmax π∈Π exp\n{\nλ ∑\nx∈X 〈π(x), ℓx〉\n}]}\n≤ 1 λ log\n{\n∑ π∈Π E{z}\n[\nexp\n{\nλ ∑\nx∈X 〈π(x), ℓx〉\n}]}\n≤ 1 λ log\n{\n∑\nπ∈Π\n∏\nx∈X E [exp {λ〈π(x), ℓx〉}]\n}\n= 1\nλ log\n\n\n\n∑\nπ∈Π\n∏\nx∈X E\n\nexp\n\n\n\nλ ∑\nj:π(x)(j)=1\nℓx(j)\n\n\n\n\n\n\n\n\n= 1\nλ log\n\n\n\n∑\nπ∈Π\n∏\nx∈X\n∏\nj:π(x)(j)=1\nE [exp {λℓx(j)}]\n\n\n\nFor any j ∈ [K] and x ∈ X , ℓx(j) is a Laplace(ǫ) random variable. Hence, the quantity E [exp{λℓx(j)}] is the moment generating function of the Laplacian distribution evaluated at λ, which is equal to 1\n1−λ2 ǫ2\nprovided that λ < ǫ. Since\nsupx,π |{j ∈ [K] : π(x)(j)}| ≤ m, we get:\nE{z}\n[\nmax π∈Π\n∑ x∈X 〈π(x), ℓx〉\n]\n≤ 1 λ log\n\n\n\nN\n(\n1\n1− λ2ǫ2\n)dm \n\n\n= 1\nλ log(N) +\ndm\nλ log\n(\n1\n1− λ2ǫ2\n)\nBy simple calculus, it is easy to derive that 11−x ≤ e2x for any x ≤ 14 .3 Thus as long as we pick λ ≤ ǫ2 , we get:\nE{z}\n[\nmax π∈Π\n∑ x∈X 〈π(x), ℓx〉\n]\n≤ 1 λ log(N) + dm λ log\n(\nexp\n{\nλ2\nǫ2\n})\n= 1\nλ log(N) +\n2dmλ\nǫ2\nPicking λ = ǫ 2 √ dm and since N ≥ 2:\nE{z}\n[\nmax π∈Π\n∑ x∈X 〈π(x), ℓx〉\n] ≤ 2 √ dm log(N)\nǫ +\n√ 2dm\nǫ ≤ 5\n√ dm log(N)\nǫ"
    }, {
      "heading" : "B.2. Bounding Stability: Transductive Setting",
      "text" : "We now turn to bounding the stability in the transductive combinatorial optimization setting. Combining the following lemma with the error bound in Lemma 8 and applying Theorem 1 proves the first claim of Theorem 2.\n3 Consider the function f(x) = (1− x)e2x − 1. Then f(0) = 0 and f ′(x) = e2x(1− 2x), which is ≥ 0 for 0 ≤ x ≤ 1/2.\nLemma 9 (Transductive Stability). For all t ∈ [T ] and for any sequence y1:t of contexts x1:t and loss functions f1:t with f i : {0, 1}K → RK , the stability of CONTEXT-FTPL(X, ǫ) is upper bounded by:\nE{z} [ f t(πt(xt))− f t(πt+1(xt)) ] ≤ 4ǫK · ‖f t‖2∗\nProof. By the definition of ‖f t‖∗:\nE{z} [ f t(πt(xt))− f t(πt+1(xt)) ] ≤ 2‖f t‖∗ Pr [ πt(xt) 6= πt+1(xt) ]\nNow observe that:\nPr [ πt(xt) 6= πt+1(xt) ] ≤ ∑\nj∈K\n( Pr[j ∈ πt(xt), j /∈ πt+1(xt)] + Pr[j /∈ πt(xt), j ∈ πt(xt)] )\nWe bound the probability Pr[j ∈ πt(xt), j /∈ πt+1(xt)]. We condition on all random variables of {z} except for the random variable ℓxt(j), i.e. the random loss placed at coordinate j on the sample associated with context xt. Denote the event corresponding to an assignment of all these other random variables as E−xtj . Let ℓxtj denote a loss vector which is ℓxt(j) on the j-th coordinate and zero otherwise. Also let:\nΦ(π) =\nt−1 ∑\nτ=1\nf τ (π(xτ )) + ∑\nx∈X−{xt} 〈π(x), ℓx〉+ 〈π(xt), ℓxt − ℓxtj〉 (18)\nLet π∗ = argminπ∈Π:j∈π(xt)Φ(π) and π̃ = minπ∈Π:j /∈π(xt) Φ(π). The event that {j ∈ πt(xt)} happens only if:\nΦ(π∗) + ℓxt(j) ≤ Φ(π̃) (19)\nLet and ν = Φ(π̃)− Φ(π∗). Thus j ∈ πt(xt) only if:\nℓxt(j) ≤ ν (20)\nNow if: ℓxt(j) < ν − 2‖f t‖∗ (21)\nthen it is easy to see that {j ∈ πt+1(xt)}, since an extra loss of f t(a) ∈ [0, 1] cannot push j out of the optimal solution. More elaborately, for any other policy π ∈ Π, such that j /∈ π(xt), the loss of π∗ including time-step t is bounded as:\nΦ(π∗) + ℓxt(j) + f t(π∗(xt)) < Φ(π)− 2‖f t‖∗ + f t(π∗(xt))\n< Φ(π)− ‖f t‖∗ < Φ(π) + f t(π(xt))\nThus any policy π, such that j /∈ π(xt) is suboptimal after seeing the loss at time-step t. Thus\nPr[j ∈ πt(xt), j /∈ πt+1(xt) | E−xtj ] ≤ Pr[ℓxt(j) ∈ [ν − 2‖f t‖∗, ν] | E−xtj ]\nSince all other random variables are independent of ℓxt(j) and ℓxt(j) is a Laplacian with parameter ǫ:\nPr[ℓxt(j) ∈ [ν − 2‖f t‖∗, ν] | E−xtj ] = Pr[ℓxt(j) ∈ [ν − 2‖f t‖∗, ν]]\n= ǫ\n2\n∫ ν\nν−2‖ft‖∗ e−ǫ|z|dz ≤ ǫ 2\n∫ ν\nν−2‖ft‖∗ dz ≤ ǫ‖f t‖∗\nSimilarly it follows that that: Pr[j /∈ πt(xt) and j ∈ πt+1(xt)] ≤ ǫ‖f t‖∗. To sum we get that:\nE{z} [ f t(πt(xt))− f t(πt+1(xt)) ] ≤ 2‖f t‖∗ Pr [ πt(xt) 6= πt+1(xt) ] ≤ 4ǫK‖f t‖2∗"
    }, {
      "heading" : "B.3. Bounding Stability: Transductive Setting with Linear Losses",
      "text" : "In the transductive setting with linear losses, we provide a significantly more refined stability bound, which enables applications to partial information or bandit settings. As before, combining this stability bound with the error bound in Lemma 8 and applying Theorem 1 gives the second claim of Theorem 2. Lemma 10 (Multiplicative Stability). For any sequence y1:T for all t ∈ [T ] of contexts and non-negative linear loss functions, the stability of CONTEXT-FTPL(X, ǫ) in the transductive setting, is upper bounded by:\nE{z} [ 〈πt(xt), ℓt〉 − 〈πt+1(xt), ℓt〉 ] ≤ ǫ · E [ 〈πt(xt), ℓt〉2 ]\nProof. To prove the result we first must introduce some additional terminology. For a sequence of parameters y1:t, let φt ∈ RdK be a vector with φtx,j = ∑ τ≤t:xτ=x ℓ τ (j). The component of this vector corresponding to context x ∈ X and coordinate j ∈ [K] is the cumulative loss associated with that coordinate on the subset of time points when context x appeared. Note that this vector φt is a sufficient statistic, since for any fixed policy π:\nt ∑\nτ=1\nℓ(π, yτ ) = ∑\nx∈X\n∑\nτ≤t:xτ=x 〈π(x), ℓτ 〉 =\n∑ x∈X 〈π(x), φtx〉 (22)\nwhere φtx = ∑ τ≤t:xτ=x ℓ τ .\nWe denote with z ∈ RdK the sufficient statistic that corresponds to the fake sample sequence {z} and with φt the sufficient statistics for the parameter sequence y1:t. Observe that the sufficient statistic for the augmented sequence {z} ∪ y1:t is simply z + φt. For any sequence of parameters y1:T we will be denoting with φ1:T the sequence of d · K dimensional cumulative loss vectors. We will also overload notation and denote with M(φt) = M(y1:t) the best policy on a sequence y1:t with statistics φt.\nConsider a specific sequence y1:T and a specific time step t. Define, for each π ∈ Π, a sparse tuple ytπ = (xt, ℓtπ) where ℓtπ(j) = ℓ\nt(j) if π(xt)(j) = 1 and zero otherwise, i.e. we zero out coordinates of the true loss vector that were not picked by the policy π. Moreover, define with φtπ the sufficient statistic of the sequence φ(y\n1:t−1 ∪ ytπ) for each π. We define 1 + |Π| distributions over |Π|, via their probability density functions, as follows:\npt(π) = Pr[M(z + φt−1) = π]\n∀π∗ ∈ Π : pt+1π∗ (π) = Pr [ M(z + φtπ∗) = π ]\nAt the end of this proof, we will show that pt+1π (π) ≤ pt+1(π). Moreover, we denote for convenience: FTPL t = Ez [〈πt(xt), ℓt〉] = Eπ∼pt [ 〈π(xt), ℓt〉 ]\nBTPL t = Ez [〈πt+1(xt), ℓt〉] = Eπ∼pt+1\n[ 〈π(xt), ℓt〉 ]\nWe will construct a mapping µπ : RdK → RdK such that for any z ∈ RdK , M(z + φtπ) = M(µπ(z) + φ t−1) Notice that µπ(z) = z + φtπ − φt−1. Now,\npt(π) =\n∫\nz\n1[π = M(z + φt−1)]f(z)dz\n=\n∫\nz\n1[π = M(µπ(z) + φ t−1)]f(µπ(z))dz\n=\n∫\nz\n1[π = M(z + φtπ)]f(µπ(z))dz\nNow observe that for any z ∈ RdK : f(µπ(z)) = exp{−ǫ ( ‖z + φtπ − φt−1‖1 − ‖z‖1 )\n}f(z) ≤ exp{−ǫ (\n‖z + φtπ − φt−1‖1 − ‖z + φtπ − φt−1‖1 − ‖φt−1 − φtπ‖1 ) }f(z) ≤ exp{ǫ‖φtπ − φt−1‖1}f(z) = exp{ǫ〈π(xt), ℓt〉}f(z)\nSubstituting in this bound, we have,\npt(π) ≤ exp{ǫ〈π(xt), ℓt〉} · pt+1π (π) ≤ exp{ǫ〈π(xt), ℓt〉} · pt+1(π)\nRe-arranging and lower bounding exp{−x} ≥ (1− x):\npt+1(π) ≥ exp{−ǫ〈π(xt), ℓt〉} · pt(π) ≥ (1− ǫ〈π(xt), ℓt〉) · pt(π) (23)\nUsing the definition of FTPLt and BTPLt, this gives,\nBTPL t =\n∑\nπ\npt+1(π)〈π(xt), ℓt〉 ≥ ∑\nπ\n(1 − ǫ〈π(xt), ℓt〉)pt(π)〈π(xt), ℓt〉\n= FTPLt − ǫ ∑\nπ\npt(π)〈π(xt), ℓt〉2\n= FTPLt − ǫE [ 〈π(xt), ℓt〉2 ]\nWe will finish the proof by showing that pt+1π (π) ≤ pt+1(π) for all π ∈ Π. For succinctness we drop the dependence on t. Notice that for any other policy π′ 6= π\nL(π, z + φtπ) ≤ L(π′, z + φtπ) ⇒ L(π, z + φt) ≤ L(π′, z + φt).\nAnd similarly for strict inequalities. This follows since the loss of π remains unchanged, but the loss of π′ can only go up, since ℓtπ(j) ≤ ℓt(j) (as losses are non-negative). For simplicity assume that π always wins in case of ties, though the argument goes through if we assume a deterministic tie-breaking rule based on some global ordering of policies. Thus,\npt+1(π) = P\n[\n⋂\nπ′\nL(π, z + φt) ≤ L(π′, z + φt) ] ≤ P [ ⋂\nπ′\nL(π, z + φtπ) ≤ L(π′, z + φtπ) ] = pt+1π (π)\nas claimed."
    }, {
      "heading" : "B.4. Bounding Stability: Small Separator Setting",
      "text" : "Finally, we prove the third claim in Theorem 2. This involves a new stability bound for the small separator setting.\nLemma 11 (Stability for small separator). For any t ∈ [T ] and any sequence y1:t of contexts x1:t and losses f1:t with f i : {0, 1}K → RK , the stability of CONTEXT-FTPL(ǫ), when X is a separator, is upper bounded by:\nE{z} [ f t(πt(xt))− f t(πt+1(xt)) ] ≤ 4ǫKd · ‖f t‖2∗\nProof. By the definition of ‖f t‖∗:\nE{z} [ f t(πt(xt))− f t(πt+1(xt)) ] ≤ 2‖f t‖∗ Pr [ πt(xt) 6= πt+1(xt) ] ≤ 2‖f t‖∗ Pr[πt 6= πt+1]\nSince X is a separator, πt 6= πt+1 if and only if there exists a context x ∈ X , such that πt(x) 6= πt+1(x). Otherwise the two policies are identical. Thus we have by two applications of the union bound:\nPr[πt 6= πt+1] ≤ ∑\nx∈X Pr[πt(x) 6= πt+1(x)]\n≤ ∑\nx∈X\n∑\nj∈K\n( Pr[j ∈ πt(x), j /∈ πt+1(x)] + Pr[j /∈ πt(x), j ∈ πt+1(x)] )\nWe bound the probability Pr[j ∈ πt(x), j /∈ πt+1(x)]. We condition on all random variables of {z} except for the random variable ℓx(j), i.e. the random loss placed at coordinate j on the sample associated with context x. Denote the event corresponding to an assignment of all these other random variables as E−xj . Let ℓxj denote a loss vector which is ℓx(j) on the j-th coordinate and zero otherwise. Also let:\nΦ(π) =\nt−1 ∑\nτ=1\nf τ (π(xτ )) + ∑ x′ 6=x 〈π(x′), ℓx′〉+ 〈π(x), ℓx − ℓxj〉 (24)\nLet π∗ = argminπ∈Π:j∈π(x)Φ(π) and π̃ = minπ∈Π:j /∈π(x)Φ(π). The event that {j ∈ πt(x)} happens only if:\nΦ(π∗) + ℓx(j) ≤ Φ(π̃) (25)\nLet and ν = Φ(π̃)− Φ(π∗). Thus j ∈ πt(x) only if:\nℓx(j) ≤ ν (26)\nNow if: ℓx(j) < ν − 2‖f t‖∗ (27)\nthen it is easy to see that {j ∈ πt+1(x)}, since an extra loss of f t(a) ≤ ‖f t‖∗ cannot push j out of the optimal solution. More elaborately, for any other policy π ∈ Π, such that j /∈ π(x), the loss of π∗ including time-step t is bounded as:\nΦ(π∗) + ℓx(j) + f t(π∗(xt)) < Φ(π)− 2‖f t‖∗ + f t(π∗(xt))\n< Φ(π)− ‖f t‖∗ < Φ(π) + f t(π(xt))\nThus any policy π, such that j /∈ π(x) is suboptimal after seeing the loss at time-step t. Thus\nPr[j ∈ πt(x), j /∈ πt+1(x) | E−xj] ≤ Pr[ℓx(j) ∈ [ν − 2‖f t‖∗, ν] | E−xj ]\nSince all other random variables are independent of ℓx(j) and ℓx(j) is a Laplacian with parameter ǫ:\nPr[ℓx(j) ∈ [ν − 2‖f t‖∗, ν] | E−xj ] = Pr[ℓx(j) ∈ [ν − 2‖f t‖∗, ν]]\n= ǫ\n2\n∫ ν\nν−2‖ft‖∗ e−ǫ|z|dz ≤ ǫ 2\n∫ ν\nν−2‖ft‖∗ dz ≤ ǫ‖f t‖∗\nSimilarly it follows that that: Pr[j /∈ πt(x), j ∈ πt+1(x)] ≤ ǫ‖f t‖∗. To sum we get that:\nE{z} [ f t(πt(xt))− f t(πt+1(xt)) ] ≤ 2‖f t‖∗ Pr [ πt 6= πt+1 ] ≤ 4ǫKd · ‖f t‖2∗"
    }, {
      "heading" : "C. Omitted Proofs from Section 4",
      "text" : ""
    }, {
      "heading" : "C.1. Proof of Theorem 3: Transductive Setting",
      "text" : "Consider the expected loss of the bandit algorithm at time-step t, conditional on Ht−1:\nE[〈πt(xt), ℓt〉 | Ht−1] = K ∑\nj=1\nqt(j) · ℓt(j) ≤ K ∑\nj=1\nqt(j) · E [ ℓ̂t(j) | Ht−1 ] +\nK ∑\nj=1\nℓt(j)qt(j) · (1 − qt(j))L (28)\nAs was observed by (Neu & Bartók, 2013), the second quantity can be upper bounded by KeL‖ℓt‖∗, since q(1 − q)L ≤ qe−Lq ≤ 1eL .\nNow observe that: ∑ j∈K q t(j) · E\n[ ℓ̂t(j) | Ht−1 ] is the expected loss of the full feedback algorithm on the sequence of\nlosses it observed and conditional on the history of play. By the regret bound of CONTEXT-FTPL(X, ǫ), given in case 2 of Theorem 2, we have that for any policy π∗:\nE\n\n\nT ∑\nt=1\nK ∑\nj=1\nqt(j) · ℓ̂t(j)\n  ≤ E [ T ∑\nt=1\n〈π∗(xt), ℓ̂t〉 ] + ǫE [ T ∑\nt=1\n∑ π∈Π pt(π)〈π(xt), ℓ̂t〉2\n]\n+ 10\nǫ\n√ dm log(N)\nUsing the fact that expected estimates ℓ̂ are upper bounded by true losses:\nE\n\n\nT ∑\nt=1\nK ∑\nj=1\nqt(j)ℓ̂t(j)\n\n ≤ min π∗∈Π E\n[\nT ∑\nt=1\n〈π∗(xt), ℓ̂t〉 ] + ǫE [ T ∑\nt=1\n∑ π∈Π pt(π)〈π(xt), ℓ̂t〉2\n]\n+ 10\nǫ\n√ dm log(N)\nCombining the two upper bounds, we get that the expected regret of the bandit algorithm is upper bounded by:\nREGRET ≤ ǫE [ T ∑\nt=1\n∑ π∈Π pt(π)〈π(xt), ℓ̂t〉2\n]\n+ 10\nǫ\n√ dm log(N) + K\neL\nT ∑\nt=1\nE [ ‖ℓt‖∗ ]\nNow observe that, by a simple norm inequality and re-grouping:\n∑ π∈Π pt(π)〈π(xt), ℓ̂t〉2 = ∑ π∈Π pt(π)\n\n\n∑\nj∈π(xt) ℓ̂t(j)\n\n\n2\n≤ m ∑\nπ∈Π pt(π)\n∑\nj∈π(xt) ℓ̂t(j)2 = m\n∑\nj∈[K] qt(j)ℓ̂t(j)2\nThus we get:\nREGRET ≤ ǫm T ∑\nt=1\nE\n\n\n∑\nj∈[K] qt(j)ℓ̂t(j)2\n\n+ 10\nǫ\n√ dm log(N) + K\neL\nT ∑\nt=1\nE [ ‖ℓt‖∗ ]\nNow we bound each of the terms in the first summation, conditional on any history of play: ∑\nj∈[K] qt(j)E\n[ ℓ̂t(j)2 | Ht−1 ] = ∑\nj∈[K] qt(j)qt(j)ℓt(j)2E\n[ J t(j)2 | Ht−1, j ∈ πt(xt) ]\nEach J t(j) conditional on Ht−1 and j ∈ πt(xt) is distributed according to a geometric distribution with mean qt(j) truncated at L. Hence, it is stochastically dominated by a geometric distribution with mean qt(j). By known properties, if X is a geometrically distributed random variable with mean q, then E[X2] = V ar(X)+(E[X ])2 = 1−qq2 + 1 q2 = 2−q q2 ≤ 2q2 . Thus we have:\n∑\nj∈[K] qt(j)E\n[ ℓ̂t(j)2 | Ht−1 ] ≤ ∑\nj∈[K] qt(j)2ℓt(j)2\n2\nqt(j)2 = 2\nK ∑\nj=1\nℓt(j)2 ≤ 2K‖ℓt‖2∞\nCombining all the above we get the theorem."
    }, {
      "heading" : "C.2. Proof of Theorem 3: Small Separator Setting",
      "text" : "Consider the expected loss of the bandit algorithm at time-step t, conditional on Ht−1:\nE[〈πt(xt), ℓt〉 | Ht−1] = K ∑\nj=1\nqt(j) · ℓt(j) ≤ K ∑\nj=1\nqt(j) · E [ ℓ̂t(j) | Ht−1 ] +\nK ∑\nj=1\nℓt(j)qt(j) · (1 − qt(j))L (29)\nAs was observed by (Neu & Bartók, 2013), the second quantity can be upper bounded by KeL‖ℓt‖∗, since q(1 − q)L ≤ qe−Lq ≤ 1eL .\nNow observe that: ∑ j∈K q t(j) · E\n[ ℓ̂t(j) | Ht−1 ] is the expected loss of the full feedback algorithm on the sequence of\nlosses it observed and conditional on the history of play. By the regret bound of CONTEXT-FTPL(X, ǫ), given in case 3 of Theorem 2, we have that for any policy π∗:\nE\n\n\nT ∑\nt=1\nK ∑\nj=1\nqt(j) · ℓ̂t(j)\n  ≤ E [ T ∑\nt=1\n〈π∗(xt), ℓ̂t〉 ] + 4ǫKd · T ∑\nt=1\nE\n[ ‖f̂ t‖2∗ ] + 10\nǫ\n√ dm log(N)\n≤ T ∑\nt=1\n〈π∗(xt), ℓ̂t〉+ 4ǫKd · T ∑\nt=1\nE\n[ ‖ℓ̂t‖21 ] + 10\nǫ\n√ dm log(N)\nUsing the fact that expected estimates ℓ̂ are upper bounded by true losses:\nE\n\n\nT ∑\nt=1\nK ∑\nj=1\nqt(j)ℓ̂t(j)\n\n ≤ min π∗∈Π E\n[\nT ∑\nt=1\n〈π∗(xt), ℓ̂t〉 ] + 4ǫKd · T ∑\nt=1\nE\n[ ‖ℓ̂t‖21 ] + 10\nǫ\n√ dm log(N)\nCombining the two upper bounds, we get that the expected regret of the semi-bandit algorithm is upper bounded by:\nREGRET ≤ 4ǫKd · T ∑\nt=1\nE\n[ ‖ℓ̂t‖21 ] + 10\nǫ\n√ dm log(N) + K\neL\nT ∑\nt=1\nE [ ‖ℓt‖∗ ]\nNow we bound each of the terms in the first summation, conditional on any history of play:\nE\n[ ‖ℓ̂t‖21 | Ht−1 ] ≤ mE [ ‖ℓ̂t‖22 ] = m ∑\nj∈[K] E\n[ ℓ̂t(j)2 ] = m ∑\nj∈[K] qt(j)ℓt(j)2E\n[ J t(j)2 | Ht−1, j ∈ πt(xt) ]\nEach J t(j) conditional on Ht−1 and j ∈ πt(xt) is distributed according to a geometric distribution with mean qt(j) truncated at L. Hence, it is stochastically dominated by a geometric distribution with mean qt(j). By known properties, if X is a geometrically distributed random variable with mean q, then E[X2] = V ar(X)+(E[X ])2 = 1−qq2 + 1 q2 = 2−q q2 ≤ 2q2 . Moreover, trivially E[X2] ≤ L2, since X is truncated at L. Thus we have:\nE\n[ ‖ℓ̂t‖21 | Ht−1 ] ≤ m ∑\nj∈[K] qt(j)ℓt(j)2 min\n{\n2\nqt(j)2 , L2\n}\n≤ m‖ℓt‖2∗ ∑\nj∈[K] min\n{\n2\nqt(j) , qt(j)L2\n}\nNow observe that: min {\n2 qt(j) , q\nt(j)L2 }\n≤ 2L, since either 1qt(j) ≤ L or otherwise, qt(j)L2 ≤ 1LL ≤ L. Thus we get:\nE\n[ ‖ℓ̂t‖21 | Ht−1 ] ≤ 2LKm‖ℓt‖2∗\nCombining all the above we get the theorem."
    }, {
      "heading" : "D. Omitted Proofs from Section 6",
      "text" : ""
    }, {
      "heading" : "D.1. Proof of Theorem 6",
      "text" : "Similar to the analysis in Section 3, the proof of the Theorem is broken apart in two main Lemmas. The first lemma is an analogue of Theorem 1 for algorithms that use a predictor. This lemma can be phrased in the general online learning setting analyzed in Section 2. The second Lemma is an anaolgue of our multiplicative stability Lemma 10.\nLet ρt = M({z} ∪ y1:t) (30)\ndenote the policy that would have been played at time-step t if the predictor was equal to the actual loss vector that occured at time-step t. Moreover, for succinctness we will denote with at = πt(xt) and with bt = ρt(xt).\nLemma 12 (Follow vs Be the Leader with Predictors). The regret of a player under the optimistic FTPL and with respect to any π∗ ∈ Π is upper bounded by:\nREGRET ≤ T ∑\nt=1\nE [ ∆Qt(at)−∆Qt(bt) ] + E[ERROR] (31)\nwhere ∆Qt(a) = f t(a)−Qt(a) and ERROR = maxπ∈Π ∑ x∈X〈π(x), ℓx〉 −minπ∈Π ∑ x∈X〈π(x), ℓx〉.\nProof. Consider the augmented sequence (x1, Q1), (x1, f1 − Q1), (x2, Q2), (x2, f2 − Q2), . . ., where each observation (xt, f t) is replaced by two observations (xt, Qt) followed by (xt, f t −Qt). Observe that by linearity of the objective, the two observations cancell out each other at the end, to give the same effect as a single observation of (xt, f t). Moreover, the leader after observing (xt, Qt) is equal to at, whilst after observing (xt, f t −Qt) is equal to bt. Thus by applying Lemma 7 to this augmented sequence we get:\nT ∑\nt=1\n( Qt(at) + f t(bt)−Qt(bt) ) ≤ T ∑\nt=1\n( Qt(π∗(xt)) + f t(π∗(xt))−Qt(π∗(xt)) ) + ERROR\n=\nT ∑\nt=1\nf t(π∗(xt)) + ERROR\nLet BTPLtQ = Q t(at) + f t(bt)−Qt(bt) and FTPLt = f t(at). Then, observe that:\nFTPL t − BTPLtQ = f t(at)−Qt(at)− (f t(bt)−Qt(bt)) = ∆Qt(at)−∆Qt(bt) (32)\nCombining the two properties we get that for any policy π∗:\nT ∑\nt=1\nFTPL t ≤\nT ∑\nt=1\n( ∆Qt(at)−∆Qt(bt) ) +\nT ∑\nt=1\nBTPL t Q\n≤ T ∑\nt=1\n( ∆Qt(at)−∆Qt(bt) ) +\nT ∑\nt=1\nf t(π∗(xt)) + ERROR\nRe-arranging and taking expectation concludes the proof.\nLemma 13 (Stability with Predictors). In the transductive setting:\nE [ ∆Qt(at)−∆Qt(bt) ] ≤ 4ǫK‖f t −Qt‖2∗ (33)"
    }, {
      "heading" : "In the small separator setting:",
      "text" : "E [ ∆Qt(at)−∆Qt(bt) ]\n≤ 4ǫKd‖f t −Qt‖2∗ (34)\nProof. We prove the first part of the Lemma. The second follows along identical arguments. By the definition of ‖f t − Qt‖∗ = ‖∆Qt‖∗ = maxa∈A |∆Qt(a)|, we have:\nE{z} [ ∆Qt(at)−∆Qt(bt) ] ≤ 2‖∆Qt‖∗ Pr [ at 6= bt ]\nNow observe that:\nPr[at 6= bt] ≤ ∑\nj∈K\n( Pr[j ∈ at, j /∈ bt] + Pr[j /∈ at, j ∈ bt] )\nWe bound the probability Pr[j ∈ at, j /∈ bt]. We condition on all random variables of {z} except for the random variable ℓxt(j), i.e. the random loss placed at coordinate j on the sample associated with contextxt. Denote the event corresponding to an assignment of all these other random variables as E−xtj . Let ℓxtj denote a loss vector which is ℓxt(j) on the j-th coordinate and zero otherwise. Also let:\nΦ(π) =\nt−1 ∑\nτ=1\nf τ (π(xτ )) +Qt(π(xt)) + ∑\nx∈X−{xt} 〈π(x), ℓx〉+ 〈π(xt), ℓxt − ℓxtj〉 (35)\nLet π∗ = argminπ∈Π:j∈π(xt)Φ(π) and π̃ = minπ∈Π:j /∈π(xt) Φ(π). The event that {j ∈ at} happens only if:\nΦ(π∗) + ℓxt(j) ≤ Φ(π̃) (36)\nLet and ν = Φ(π̃)− Φ(π∗). Thus j ∈ at only if: ℓxt(j) ≤ ν (37)\nNow if: ℓxt(j) < ν − 2‖∆Qt‖∗ (38)\nthen it is easy to see that {j ∈ bt}, since an extra loss of f t(a) − Qt(a) ≤ ‖∆Qt‖∗ cannot push j out of the optimal solution. More elaborately, for any other policy π ∈ Π, such that j /∈ π(xt), the loss of π∗ including time-step t is bounded as:\nΦ(π∗) + ℓxt(j) + f t(π∗(xt))−Qt(π∗(xt)) < Φ(π)− 2‖∆Qt‖∗ + f t(π∗(xt))−Qt(π∗(xt))\n< Φ(π)− ‖∆Q‖∗ < Φ(π) + f t(π(xt))−Qt(π(xt))\nThus any policy π, such that j /∈ π(xt) is suboptimal after seeing the loss at time-step t. Thus\nPr[j ∈ at, j /∈ bt | E−xtj ] ≤ Pr[ℓxt(j) ∈ [ν − 2‖∆Qt‖∗, ν] | E−xtj ]\nSince all other random variables are independent of ℓxt(j) and ℓxt(j) is a Laplacian with parameter ǫ:\nPr[ℓxt(j) ∈ [ν − 2‖∆Qt‖∗, ν] | E−xtj ] = Pr[ℓxt(j) ∈ [ν − 2‖∆Qt‖∗, ν]]\n= ǫ\n2\n∫ ν\nν−2‖∆Qt‖∗ e−ǫ|z|dz ≤ ǫ 2\n∫ ν\nν−2‖∆Qt‖∗ dz ≤ ǫ · ‖∆Qt‖∗\nSimilarly it follows that that: Pr[j /∈ πt(xt) and j ∈ πt+1(xt)] ≤ ǫ · ‖∆Qt‖∗. To sum we get that:\nE{z} [ ∆Qt(at)−∆Qt(bt) ] ≤ 2‖∆Qt‖∗ Pr [ πt(xt) 6= πt+1(xt) ] ≤ 4ǫK‖∆Qt‖2∗\nThe expected error term is identical to the expected error that we upper bounded in Lemma 8, hence the same bound carries over. Combining the above Lemmas with this observation, yields Theorem 6.\nE. Infinite Policy Classes\nIn this section we focus on the contextual experts problem but consider infinite policy classes. Recall that in this setting, in each round t, the adversary picks a context xt ∈ X and a loss function ℓt ∈ RK≥0, the learner, upon seeing the context xt, chooses an action at ∈ [K], and then suffers loss ellt(at). We showed that as a simple consequence of Theorem 2, that when competing with a set of policies Π ⊂ (X → [K]) with |Π| = N and against an adaptive adversary, CONTEXT-FTPL has regret at most O(d1/4 √\nT log(N)) in the transductive setting and regret at most O(d3/4 √ KT log(N)) in the non-transductive setting with small separator.\nHere we consider the situation where the policy class Π is infinite in size, but has small Natarajan dimension, which generalizes VC-dimension to multiclass problems. Specifically, we prove two results in this section: First we show that in the transductive case, CONTEXT-FTPL can achieve low regret relative to a policy class with bounded Natarajan dimension. Then we show that in the non-transductive case, it is hard in an information-theoretic sense to achieve sublinear regret relative to a policy class with constant Natarajan dimension. Together, these results show that finite Natarajan or VC dimension is sufficient for sublinear regret in the transductive setting, but it is insufficient for sublinear regret in the fully online setting.\nBefore proceeding with the two results, we must introduce the notion of Natarajan dimension, which requires some notation. For a class of functions F from X → [K] and for a sequence X = (x1, . . . , xn) ∈ Xn, define FX = {(f(x1), . . . , f(xn)) ∈ [K]n : f ∈ F} be the restriction of the functions to the points. Let Ψ be a family of mappings from [K] → {0, 1, ⋆}. Let ψ̄ = (ψ1, . . . , ψn) ∈ Ψn be a fixed sequence of such mappings and for a sequence (s1, . . . , sn) ∈ [K]N define ψ̄(s) = (ψ1(s1), . . . , ψ1(sn)) ∈ {0, 1, ⋆}n. We say a sequence X ∈ Xn is Ψ-shattered by F if there exists ψ̄ ∈ Ψn such that:\n{0, 1}n ⊆ {ψ̄(s) : s ∈ FX}\nThe Ψ-dimension of a function class F is the largest n such that there exist a sequence X ∈ Xn that is Ψ-shattered by F . Notice that if K = 2 and Ψ contains only the identity map, then the Ψ-dimension is exactly the VC dimension.\nThe Natarajan dimension is the Ψ dimension for the class ΨN = {ψN,i,j, i, j ∈ [K], j 6= i} where ψN,i,j(a) = 1 if a = i, ψN,i,j(a) = 0 if a = j and ψN,i,j(a) = ⋆ otherwise. Notice that Natarajan dimension is a strict generalization of VC-dimension as ΨN contains only the identity map if K = 2. Thus our result also applies to VC-classes in the two-action case. The main property we will use about function classes with bounded Natarajan Dimension is the following analog of the Sauer-Shelah Lemma:\nLemma 14 (Sauer-Shelah for Natarajan Dimension (Haussler & Long, 1995; Ben-David et al., 1995)). Suppose that F has ΨN dimension at most ν. Then for any set X ∈ Xn, we have:\n|FX | ≤ ( ne(K + 1)2\n2ν\n)ν\nOur positive result for transductive learning with a Natarajan class is the following regret bound for CONTEXT-FTPL,\nCorollary 15. Consider running CONTEXT-FTPL(X, ǫ) in the transductive contextual experts setting with a policy class Π with Natarajan dimension at most ν. Then the algorithm achieves regret against an adaptive and adversarially chosen sequence of contexts and loss functions,\nǫ\nT ∑\nt=1\nE[〈πt(xt), ℓt〉2] + 10 ǫ\n√\ndν log(K) log\n(\nde(K + 1)2\n2ν\n)\n.\nWhen ǫ is set optimally and losses are in [0, 1]K , this is O((dν log(K) log(dK/ν))1/4 √ T ).\nProof. The result is a consequence of the second clause of Theorem 2, using the additional fact that any sequence of contexts X = (x1, . . . , xd) induce a finite policy class ΠX ⊆ [K]d. The fact that Π has Natarajan dimension at most ν means that |ΠX | ≤ ( de(K+1)2\n2ν\n)ν\nby Lemma 14. Therefore, once the d contexts are fixed, as they are in the transductive\nsetting, we are back in the finite policy case and can apply Theorem 2 with N replaced by |ΠX |.\nThus we see that CONTEXT-FTPL has sublinear regret relative to policy classes with bounded Natarajan dimension, even against adaptive adversaries. The second result in this section shows that this result cannot be lifted to the non-transductive setting. Specifically, we prove the following theorem in the section, which shows that no algorithm, including inefficient ones, can achieve sublinear regret against a VC class in the non-transductive setting.\nTheorem 16. Consider an online binary classification problem in one dimension with F ⊂ [0, 1] → {0, 1} denoting the set of all threshold functions. Then there is no learning algorithm that can guarantee o(T ) expected regret against an adaptive adversary. In particular, there exists a policy class of VC dimension one such that no learning algorithm can achieve sublinear regret against an adaptive adversary in the contextual experts problem.\nProof. We define an adaptive adversary and argue that it ensures at least 1/2 expected regret per round. While the adversary does not have access to the random coins of the learner, it can compute the probability that the learner would label any point as {0, 1}. At round t, let pt(x) denote the probability that the learner would label a point x ∈ [0, 1] as 1, and note that this quantity is conditioned on the entire history of interaction. At each round t, the adversary will have played a set of points X+t with positive label and X − t with negative label and she will maintain the invariant that minx∈X+t x > maxx∈X−t x for all t. At every time t, the adversary will play context xt ∈ (maxx∈X−t x,minx∈X+t x). The adversary, knowing the learning algorithm, will compute pt(xt) and assign label yt = 1 if pt(x) < 1/2 and 0 otherwise. The adversary will then update the sets X+t+1 ← X+t ∪ {xt} if yt = 1 and X+t+1 ← X+t otherwise. X−t+1 is updated analogously. Clearly this sequence of contexts maintains the appropriate invariant for the adversary, namely there is always an interval between the positive and negative examples in which he can pick a context. This implies that on the sequence, there is a threshold f⋆ ∈ F that perfectly classifies the points, so its cumulative reward is T . Moreover, by the choice of label selected by the adversary, the expected reward of the learner at round t is at most 1/2, which means the cumulative expected reward of the learner is at most T/2. Thus the regret of the learner is at least T/2."
    } ],
    "references" : [ {
      "title" : "Taming the monster: A fast and simple algorithm for contextual bandits",
      "author" : [ "Agarwal", "Alekh", "Hsu", "Daniel", "Kale", "Satyen", "Langford", "John", "Li", "Lihong", "Schapire", "Robert E" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2014
    }, {
      "title" : "Gambling in a rigged casino: The adversarial multi-armed bandit pproblem",
      "author" : [ "Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Auer et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 1995
    }, {
      "title" : "Online linear optimization and adaptive routing",
      "author" : [ "Awerbuch", "Baruch", "Kleinberg", "Robert" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Awerbuch et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Awerbuch et al\\.",
      "year" : 2008
    }, {
      "title" : "Characterizations of learnability for classes of (0,..., n)-valued functions",
      "author" : [ "Ben-David", "Shai", "Cesa-Bianchi", "Nicolo", "Haussler", "David", "Long", "Philip M" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Ben.David et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 1995
    }, {
      "title" : "Online learning versus offline learning",
      "author" : [ "Ben-David", "Shai", "Kushilevitz", "Eyal", "Mansour", "Yishay" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Ben.David et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 1997
    }, {
      "title" : "Efficient online learning via randomized rounding",
      "author" : [ "Cesa-Bianchi", "Nicolo", "Shamir", "Ohad" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2011
    }, {
      "title" : "How to use expert advice",
      "author" : [ "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Haussler", "David", "Helmbold", "David P", "Schapire", "Robert E", "Warmuth", "Manfred K" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 1997
    }, {
      "title" : "Learning in auctions: Regret is hard, envy is easy",
      "author" : [ "Daskalakis", "Constantinos", "Syrgkanis", "Vasilis" ],
      "venue" : null,
      "citeRegEx" : "Daskalakis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Daskalakis et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient optimal learning for contextual bandits",
      "author" : [ "Dudı́k", "Miroslav", "Hsu", "Daniel", "Kale", "Satyen", "Karampatziakis", "Nikos", "Langford", "John", "Reyzin", "Lev", "Zhang", "Tong" ],
      "venue" : "In Uncertainty and Artificial Intelligence (UAI),",
      "citeRegEx" : "Dudı́k et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dudı́k et al\\.",
      "year" : 2011
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Freund", "Yoav", "Schapire", "Robert E" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Freund et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund et al\\.",
      "year" : 1997
    }, {
      "title" : "A generalization of sauer’s lemma",
      "author" : [ "Haussler", "David", "Long", "Philip M" ],
      "venue" : "Journal of Combinatorial Theory,",
      "citeRegEx" : "Haussler et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Haussler et al\\.",
      "year" : 1995
    }, {
      "title" : "Extracting certainty from uncertainty: regret bounded by variation in costs",
      "author" : [ "Hazan", "Elad", "Kale", "Satyen" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2010
    }, {
      "title" : "Online submodular minimization",
      "author" : [ "Hazan", "Elad", "Kale", "Satyen" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "Hazan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2012
    }, {
      "title" : "Tracking the best expert",
      "author" : [ "Herbster", "Mark", "Warmuth", "Manfred K" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Herbster et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Herbster et al\\.",
      "year" : 1998
    }, {
      "title" : "A generalization of sampling without replacement from a finite universe",
      "author" : [ "Horvitz", "Daniel G", "Thompson", "Donovan J" ],
      "venue" : "Journal of the American Statistical Association (JASA),",
      "citeRegEx" : "Horvitz et al\\.,? \\Q1952\\E",
      "shortCiteRegEx" : "Horvitz et al\\.",
      "year" : 1952
    }, {
      "title" : "Adaptive online prediction by following the perturbed leader",
      "author" : [ "Hutter", "Marcus", "Poland", "Jan" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "Hutter et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2005
    }, {
      "title" : "Online submodular minimization for combinatorial structures",
      "author" : [ "Jegelka", "Stefanie", "Bilmes", "Jeff A" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Jegelka et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Jegelka et al\\.",
      "year" : 2011
    }, {
      "title" : "From batch to transductive online learning",
      "author" : [ "Kakade", "Sham M", "Kalai", "Adam" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Kakade et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2005
    }, {
      "title" : "Efficient algorithms for online decision problems",
      "author" : [ "Kalai", "Adam", "Vempala", "Santosh" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Kalai et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kalai et al\\.",
      "year" : 2005
    }, {
      "title" : "The epoch-greedy algorithm for multi-armed bandits with side information",
      "author" : [ "Langford", "John", "Zhang", "Tong" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Langford et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Langford et al\\.",
      "year" : 2008
    }, {
      "title" : "Achieving all with no parameters: Adanormalhedge",
      "author" : [ "Luo", "Haipeng", "Schapire", "Robert E" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "Luo et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2015
    }, {
      "title" : "An efficient algorithm for learning with semi-bandit feedback",
      "author" : [ "Neu", "Gergely", "Bartók", "Gábor" ],
      "venue" : "In Algorithmic Learning Theory (ALT),",
      "citeRegEx" : "Neu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Neu et al\\.",
      "year" : 2013
    }, {
      "title" : "Optimization, learning, and games with predictable sequences",
      "author" : [ "Rakhlin", "Alexander", "Sridharan", "Karthik" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Rakhlin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rakhlin et al\\.",
      "year" : 2013
    }, {
      "title" : "Online learning with predictable sequences",
      "author" : [ "Rakhlin", "Alexander", "Sridharan", "Karthik" ],
      "venue" : "In Conference on Learning Theorem (COLT),",
      "citeRegEx" : "Rakhlin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rakhlin et al\\.",
      "year" : 2013
    }, {
      "title" : "Fast convergence of regularized learning in games",
      "author" : [ "Syrgkanis", "Vasilis", "Agarwal", "Alekh", "Luo", "Haipeng", "Schapire", "Robert E" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Syrgkanis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Syrgkanis et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Algorithms for contextual learning, such as Hedge (Freund & Schapire, 1997; Cesa-Bianchi et al., 1997) and Exp4 (Auer et al.",
      "startOffset" : 50,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : ", 1997) and Exp4 (Auer et al., 1995), are well-known to have remarkable theoretical properties, being effective even in adversarial, non-stochastic environments, and capable of performing almost as well as the best among an exponentially large family of policies, or rules for choosing actions at each step.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : ", 1997) and Exp4 (Auer et al., 1995), are well-known to have remarkable theoretical properties, being effective even in adversarial, non-stochastic environments, and capable of performing almost as well as the best among an exponentially large family of policies, or rules for choosing actions at each step. However, the space requirements and running time of these algorithms are generally linear in the number of policies, which is far too expensive for a great many applications which call for an extremely large policy space. In this paper, we address this gap between the statistical promise and computational challenge of algorithms for contextual online learning in an adversarial setting. As an approach to solving online learning problems, we posit that the corresponding batch version is solvable. In other words, we assume access to a certain optimization oracle for solving an associated batch-learning problem. Concrete instances of such an oracle include empirical risk minimization procedures for supervised learning, algorithms for the shortest paths problem, and dynamic programming. Such an oracle is central to the Follow-the-PerturbedLeader algorithms of Kalai & Vempala (2005), although these algorithms are not generally efficient since they require separately “perturbing” each policy in the entire",
      "startOffset" : 18,
      "endOffset" : 1198
    }, {
      "referenceID" : 0,
      "context" : "Oracles of this kind have also been used in designing efficient contextual bandit algorithms (Agarwal et al., 2014; Langford & Zhang, 2008; Dudı́k et al., 2011); however, these require a much more benign setting in which contexts and losses are chosen randomly and independently rather than by an adversary.",
      "startOffset" : 93,
      "endOffset" : 160
    }, {
      "referenceID" : 8,
      "context" : "Oracles of this kind have also been used in designing efficient contextual bandit algorithms (Agarwal et al., 2014; Langford & Zhang, 2008; Dudı́k et al., 2011); however, these require a much more benign setting in which contexts and losses are chosen randomly and independently rather than by an adversary.",
      "startOffset" : 93,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "The first is a transductive setting (Ben-David et al., 1997) in which the learner knows the set of arriving contexts a priori, or, less stringently, knows only the set, but not necessarily the actual sequence or multiplicity with which each context arrives.",
      "startOffset" : 36,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "Oracles of this kind have also been used in designing efficient contextual bandit algorithms (Agarwal et al., 2014; Langford & Zhang, 2008; Dudı́k et al., 2011); however, these require a much more benign setting in which contexts and losses are chosen randomly and independently rather than by an adversary. In this paper, for a wide range of problems, we present computationally efficient algorithms for contextual online learning in an adversarial setting, assuming oracle access. We give results for both the full-information and bandit settings. To the best of our knowledge, these results are the first of their kind at this level of generality. Overview of results. We begin by proposing and analyzing in Section 2 a new and general Follow-the-PerturbedLeader algorithm in the style of Kalai & Vempala (2005). This algorithm only accesses the policy class using the optimization oracle.",
      "startOffset" : 94,
      "endOffset" : 815
    }, {
      "referenceID" : 0,
      "context" : "Oracles of this kind have also been used in designing efficient contextual bandit algorithms (Agarwal et al., 2014; Langford & Zhang, 2008; Dudı́k et al., 2011); however, these require a much more benign setting in which contexts and losses are chosen randomly and independently rather than by an adversary. In this paper, for a wide range of problems, we present computationally efficient algorithms for contextual online learning in an adversarial setting, assuming oracle access. We give results for both the full-information and bandit settings. To the best of our knowledge, these results are the first of their kind at this level of generality. Overview of results. We begin by proposing and analyzing in Section 2 a new and general Follow-the-PerturbedLeader algorithm in the style of Kalai & Vempala (2005). This algorithm only accesses the policy class using the optimization oracle. We then apply these results in Section 3 to two settings. The first is a transductive setting (Ben-David et al., 1997) in which the learner knows the set of arriving contexts a priori, or, less stringently, knows only the set, but not necessarily the actual sequence or multiplicity with which each context arrives. In the second, small-separator setting, we assume that the policy space admits the existence of a small set of contexts, called a separator, such that any two policies differ on at least one context from the set. The size of the smallest separator for a particular policy class can be viewed as a new measure of complexity, different from the VC dimension, and potentially of independent interest. We study these for a generalized online learning problem called online combinatorial optimization, which includes as special cases transductive contextual experts, online shortest-path routing, online linear optimization (Kalai & Vempala, 2005), and online submodular minimization (Hazan & Kale, 2012). In Section 4, we extend our results to the bandit setting, or in fact, to the more general semi-bandit setting, using a technique of Neu & Bartók (2013). Among our main results, we obtain regret bounds for the adversarial contextual bandit problem of O(T 3/4 √",
      "startOffset" : 94,
      "endOffset" : 2063
    }, {
      "referenceID" : 24,
      "context" : "Such bounds have various applications, for instance, in obtaining better bounds for playing repeated games (Rakhlin & Sridharan, 2013a; Syrgkanis et al., 2015).",
      "startOffset" : 107,
      "endOffset" : 159
    }, {
      "referenceID" : 22,
      "context" : "Such bounds have various applications, for instance, in obtaining better bounds for playing repeated games (Rakhlin & Sridharan, 2013a; Syrgkanis et al., 2015). Other related work. Contextual, transductive online learning using an optimization oracle was previously studied by Kakade & Kalai (2005), whose work was later extended and improved by Cesa-Bianchi & Shamir (2011) using a generalization of a technique from Cesa-Bianchi et al.",
      "startOffset" : 136,
      "endOffset" : 299
    }, {
      "referenceID" : 22,
      "context" : "Such bounds have various applications, for instance, in obtaining better bounds for playing repeated games (Rakhlin & Sridharan, 2013a; Syrgkanis et al., 2015). Other related work. Contextual, transductive online learning using an optimization oracle was previously studied by Kakade & Kalai (2005), whose work was later extended and improved by Cesa-Bianchi & Shamir (2011) using a generalization of a technique from Cesa-Bianchi et al.",
      "startOffset" : 136,
      "endOffset" : 375
    }, {
      "referenceID" : 5,
      "context" : "Contextual, transductive online learning using an optimization oracle was previously studied by Kakade & Kalai (2005), whose work was later extended and improved by Cesa-Bianchi & Shamir (2011) using a generalization of a technique from Cesa-Bianchi et al. (1997). However, these previous results are for binary classification or other convex losses defined on one-dimensional predictions and outcomes; as such, they are special cases of the much more general setting we consider in the present paper.",
      "startOffset" : 237,
      "endOffset" : 264
    }, {
      "referenceID" : 5,
      "context" : "Contextual, transductive online learning using an optimization oracle was previously studied by Kakade & Kalai (2005), whose work was later extended and improved by Cesa-Bianchi & Shamir (2011) using a generalization of a technique from Cesa-Bianchi et al. (1997). However, these previous results are for binary classification or other convex losses defined on one-dimensional predictions and outcomes; as such, they are special cases of the much more general setting we consider in the present paper. Awerbuch & Kleinberg (2008) present an efficient algorithm for the online shortest paths problem.",
      "startOffset" : 237,
      "endOffset" : 530
    }, {
      "referenceID" : 5,
      "context" : "Contextual, transductive online learning using an optimization oracle was previously studied by Kakade & Kalai (2005), whose work was later extended and improved by Cesa-Bianchi & Shamir (2011) using a generalization of a technique from Cesa-Bianchi et al. (1997). However, these previous results are for binary classification or other convex losses defined on one-dimensional predictions and outcomes; as such, they are special cases of the much more general setting we consider in the present paper. Awerbuch & Kleinberg (2008) present an efficient algorithm for the online shortest paths problem. This can be viewed as solving an adversarial bandit problem with a very particular optimization oracle over an exponentially large but highly structured space of “policies” corresponding to paths in a graph. However, their setting is clearly far more restrictive and structured than ours is. 2. Online Learning with Oracles We start by analyzing the family of Follow the Perturbed Leader algorithms in a very general online learning setting. Parts of this generic formulation follow the recent formulation of Daskalakis & Syrgkanis (2015), but we present a more refined analysis which is essential for our contextual learning result in the next sections.",
      "startOffset" : 237,
      "endOffset" : 1139
    }, {
      "referenceID" : 5,
      "context" : "Contextual, transductive online learning using an optimization oracle was previously studied by Kakade & Kalai (2005), whose work was later extended and improved by Cesa-Bianchi & Shamir (2011) using a generalization of a technique from Cesa-Bianchi et al. (1997). However, these previous results are for binary classification or other convex losses defined on one-dimensional predictions and outcomes; as such, they are special cases of the much more general setting we consider in the present paper. Awerbuch & Kleinberg (2008) present an efficient algorithm for the online shortest paths problem. This can be viewed as solving an adversarial bandit problem with a very particular optimization oracle over an exponentially large but highly structured space of “policies” corresponding to paths in a graph. However, their setting is clearly far more restrictive and structured than ours is. 2. Online Learning with Oracles We start by analyzing the family of Follow the Perturbed Leader algorithms in a very general online learning setting. Parts of this generic formulation follow the recent formulation of Daskalakis & Syrgkanis (2015), but we present a more refined analysis which is essential for our contextual learning result in the next sections. The main theorem of this section is essentially a generalization of Theorem 1.1 of Kalai & Vempala (2005). Consider an online learning problem where at each timestep an adversary picks an outcome y ∈ Y and the algorithm picks a policy π ∈ Π from some policy space Π.",
      "startOffset" : 237,
      "endOffset" : 1361
    }, {
      "referenceID" : 24,
      "context" : "Such results have also found applications in learning in game theoretic environments (Rakhlin & Sridharan, 2013a; Syrgkanis et al., 2015).",
      "startOffset" : 85,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "The main property we will use about function classes with bounded Natarajan Dimension is the following analog of the Sauer-Shelah Lemma: Lemma 14 (Sauer-Shelah for Natarajan Dimension (Haussler & Long, 1995; Ben-David et al., 1995)).",
      "startOffset" : 184,
      "endOffset" : 231
    } ],
    "year" : 2016,
    "abstractText" : "We provide the first oracle efficient sublinear regret algorithms for adversarial versions of the contextual bandit problem. In this problem, the learner repeatedly makes an action on the basis of a context and receives reward for the chosen action, with the goal of achieving reward competitive with a large class of policies. We analyze two settings: i) in the transductive setting the learner knows the set of contexts a priori, ii) in the small separator setting, there exists a small set of contexts such that any two policies behave differently in one of the contexts in the set. Our algorithms fall into the follow the perturbed leader family (Kalai & Vempala, 2005) and achieve regret O(T 3/4 √ K log(N)) in the transductive setting and O(T dK √ log(N)) in the separator setting, where K is the number of actions, N is the number of baseline policies, and d is the size of the separator. We actually solve the more general adversarial contextual semi-bandit linear optimization problem, whilst in the full information setting we address the even more general contextual combinatorial optimization. We provide several extensions and implications of our algorithms, such as switching regret and efficient learning with predictable sequences.",
    "creator" : "LaTeX with hyperref package"
  }
}