{
  "name" : "1210.1258.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unfolding Latent Tree Structures using 4th Order Tensors",
    "authors" : [ "Mariya Ishteva", "Haesun Park" ],
    "emails" : [ "mishteva@cc.gatech.edu", "hpark@cc.gatech.edu", "lsong@cc.gatech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n21 0.\n12 58\nv1 [\ncs .L\nG ]\n3 O\nct 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "Discovering the latent structure from many observed variables is an important yet challenging learning task. The discovered structures can help better understand the domain and lead to potentially better predictive models. Many local search heuristics based on maximum parsimony and maximum likelihood methods have been proposed to address this problem (Semple & Steel, 2003; Zhang, 2004; Heller & Ghahramani, 2005; Teh et al., 2008; Harmeling & Williams, 2010). Their common drawback is that it is difficult to provide consistency guarantees. Furthermore, the number of hidden states often needs to be determined before the structure learning. Or cross-validations are needed to determine the hidden states, which can be very time consuming to run.\nEfficient algorithms with provable performance guarantees have been explored in the phylogenetic tree reconstruction community. One popular algorithm is the neighbor-joining (NJ) algorithm (Saitou & Nei, 1987), where pairs of variables are joined recursively according to a certain distance measure. The NJ algorithm is consistent when the distance measure satisfies the path additive property (Mihaescu et al., 2009). For discrete random variables, the additive distance is defined using the determinant of the joint probability table of a pair of variables (Lake, 1994). However, this definition only applies to the cases where the observed variables and latent variables have the same number of states. When the latent variables represent simpler factors with smaller number of states, the NJ algorithm can perform poorly.\nAnother family of provably consistent reconstruction methods is the quartet-based methods (Semple & Steel, 2003; Erdös et al., 1999). These methods first resolve a set of latent relations\nfor quadruples of observed variables (quartets), and subsequently, stitch them together to form a latent tree. A good quartet test plays an essential role in these methods, as it is called repeatedly by the stitching algorithms. Recently, (Anandkumar et al., 2011) proposed a quartet test using the leading k singular values of the joint probability table, where k is the number of hidden states. This new approach allows k to be different from the number of the observed states. However, it still requires k to be given in advance.\nOur goal is to design a latent structure discovery algorithm which is agnostic to the number of hidden states, since in practice we rarely know this number. The proposed approach is quartet based, where the quartet relations are resolved based on rank properties of 4th order tensors associated with the joint probability tables of quartets. The key insight is that rank properties of the tensor reveal the latent structure behind a quartet. Similar observations have been reported in the phylogenetic community (Eriksson, 2005; Allman & Rhodes, 2006), but they are concerned about the cases where the number of hidden states is larger or equal to the number of observed states. We focus instead on the cases where the number of hidden states is smaller, representing simpler factors. Furthermore, if the joint probability tensor is only approximately given (due to sampling noise) the main rank condition has to be modified. In Allman & Rhodes (2006) such condition is missing and in Eriksson (2005) the condition is heuristically translated to the distance of a matrix to its best rank-k approximation. In contrast, we propose a novel nuclear norm relaxation of the rank condition, discuss its advantages, and provide recovery conditions and finite sample guarantees. Our quartet test is easy to compute since it only involves singular value decomposition of unfolded 4th order tensors.\nUsing the proposed quartet test as a subroutine, the latent tree structure can be recovered in a divide-and-conquer fashion (Pearl & Tarsi, 1986). For d observed variables, the computational complexity of the algorithm is O(d log d), making it scalable to large problems. Under mild conditions, the tree construction algorithm using our quartet test is consistent and stable to estimate given a finite number of samples. In simulations, we compared to alternatives in terms of resolving quartet relations and building the entire latent trees. The proposed approach is among the best performing ones while being agnostic to the number of hidden states k. The latter is an important improvement, since cross validation for finding k is expensive while leading to similar final results. We also applied the new approach to a stock dataset, where it discovered meaningful grouping of stocks according to industrial sectors, and led a latent variable model that fits the data better than the competitors."
    }, {
      "heading" : "2 Latent Tree Graphical Models",
      "text" : "In this paper, we focus on discrete latent variable models where the conditional independence structures are specified by trees. We assume that the d observed variables, O = {X1, . . . ,Xd}, are leaves of the tree and that they all have the same number of states, n. We also assume the dh hidden variables, H = {Xd+1, . . . ,Xd+dh}, have the same1, but unknown, number of states, k, (k ≤ n). Furthermore, we use uppercase letters to denote random variables (e.g., Xi) and lowercase letters their instantiations (e.g., xi).\nFactorization of distribution. The joint distribution of all variables, X = O ∪ H , in a latent tree model is a multi-way table (tensor), P, with d + dh dimensions. Although the tensor\n1Our results are easily generalizable to the case where all hidden variables have different number of states.\nhas O(ndkdh) number of entries, they can be computed from just a polynomial number of parameters due to the latent tree structure. That is P(x1, . . . , xd+dh) = ∏d+dh i=1 P (xi|xπi) where each P (Xi|Xπi) is a conditional probability table (CPT) of a variable Xi and its parent Xπi in the tree.2 This factorization leads to a significant saving in terms of tensor representation: we can represent exponential number of entries using just O(dhk\n2 + dnk) parameters from the CPTs. Throughout the paper, we assume that (A1) all CPTs have full column rank, k.\nStructure learning. Determining the tree topology T is an important and challenging learning problem. The goal is to discover the latent structure based just on samples from observed variables. For simplicity and uniqueness of the tree topology (Pearl, 1988), we assume that (A2) every latent variable has exactly 3 neighbors.\nQuartet. A quadruple of observed variables from a latent tree T is called a quartet (Figure 1). Under assumption (A2), there are 3 ways to connect a quartet, X1,X2,X3, X4, using 2 latent vari-\nables H and G (Figure 2). However, only one of the 3 quartet relations is consistent with T . The\nmapping between quartets and the tree topology T is captured in the following theorem (Buneman, 1971):\nTheorem 1. The set of all quartet relations QT is unique to a latent tree T , and furthermore, T can be recovered from QT in polynomial time.\nQuartet-based tree reconstruction. Motivated by Theorem 1, a family of latent tree recovery algorithms has been designed based on resolving quartet relations. These algorithms first determine one of the 3 ways how 4 variables are connected, and then join together all quartet relations to form a consistent latent tree. For a model with d observed variables, there are O(d4) quartet relations in total (taking all possible combinations of 4 variables). However, we do not necessarily need to resolve all these quartet relations in order to reconstruct the latent tree. A small set of size O(d log d) will suffice for the tree recovery, which makes quartet based methods efficient even for problems with large d (Pearl & Tarsi, 1986; Pearl, 1988). In this paper, we design a new quartet based method. Our main contribution compared to previous approaches is that our method is agnostic to the number of hidden states, k, which is usually unknown in practice.\n2For a latent tree, we can select a latent node as the root, and re-orient all edges away from it to induce consistent parent-child relations. For the root node Xr, P (Xr|Xπr ) = P (Xr)."
    }, {
      "heading" : "3 Resolving Quartet Relations without Knowing the Number of",
      "text" : "Hidden States\nIn this section, we develop a test for resolving the latent relation of a quartet when the number of hidden states is unknown. Our approach makes use of information from the joint probability table of a quartet, which is a 4-way table or 4th order tensor. Suppose that the quartet relation of 4 variables, X1,X2,X3 and X4, is {{1, 2}, {3, 4}}, then the entries in this tensor are specified by\nP(x1, x2, x3, x4) = ∑\nh,g P (x1|h)P (x2|h)P (h, g)P (x3 |g)P (x4|g). (1)\nThis factorization suggests that there exist some low rank structures in the 4th order tensor. To study the rank properties of P(X1,X2,X3,X4), we first relate it to the conditional probability tables, P (X1|H), P (X2|H), P (X3|G), P (X4|G), and the joint probability table, P (H,G) (we abbreviate them as P1|H , P2|H , P3|G, P4|G and PHG, respectively). Using tensor algebra, we have\nP(X1,X2,X3,X4) = 〈T1,T2〉3,\nwith T1 = IH ×1 P1|H ×2 P2|H , T2 = IG ×1 P3|G ×2 P4|G ×3 PHG,\nwhere IH and IG are 3rd order diagonal tensors of size k × k × k with diagonal elements equal to 1. The multiplication ×i denotes a tensor-matrix multiplication with respect to the i-th dimension of the tensor and the rows of the matrix, and 〈·, ·〉3 denotes tensor-tensor multiplication along the third dimension of both tensors3. This formula can be schematically understood as Figure 3. We\nwill start by characterizing the rank properties of P and then exploit them to design a quartet test. Although the proposed approach involves unfolding the tensor and subsequent computation at the matrix level, modeling the problem using tensors provides higher level conceptual understanding of the structure of P. The novelty of our use of low rank tensors is for latent structure discovery.\n3.1 Unfolding the 4th Order Tensor\nNow we consider 3 different reshapings A, B and C of the tensor into matrices (“unfoldings”). These unfoldings contain exactly the same entires as P but in different order. A corresponds to the grouping {{1, 2}, {3, 4}} of the variables, i.e., the rows of A correspond to dimensions 1 and 2 of P, and its columns to dimensions 3 and 4. B corresponds to the grouping {{1, 3}, {2, 4}} and C - to the grouping {{1, 4}, {2, 3}}. Using Matlab’s notation (see appendix, §8 for further explanation),\n3For formal definitions of tensor notations see appendix, §8.\nA = reshape(P, n2, n2); (2) B = reshape(permute(P, [1, 3, 2, 4]), n2 , n2); (3) C = reshape(permute(P, [1, 4, 2, 3]), n2 , n2). (4)\nNext we present useful characterizations of A, B and C, which will be essential for understanding their connection with the latent structure of a quartet. The Kronecker product of two matrices M and M ′ is denoted as M ⊗ M ′, and if they have the same number of columns, their Khatri-Rao product (column-wise Kronecker product), is denoted as M⊙M ′. Then (see appendix §9 for proof), Lemma 2. Assume that {{1, 2}, {3, 4}} is the correct latent structure. The matrices A, B and C can be factorized respectively as (see Figure 4(a) and Figure 4(b) for schematic diagrams)\nThe factorization of A is very different from those of B and C. First, in A, P2|H ⊙ P1|H is a matrix of size n2 × k, and the columns of P2|H interact only with their corresponding columns in P1|H . However, in B, P3|G ⊗ P1|H is a matrix of size n2 × k2, and every column of P1|H interacts with every column of P3|G respectively (similarly for C). Second, in A, the middle factor PHG has size k × k, whereas in B, the entires of PHG appear as the diagonal of a matrix of size k2 × k2 (similarly for C). These differences result in different rank properties of A, B and C which we will exploit to discover the latent structure of a quartet."
    }, {
      "heading" : "3.2 Rank Properties of the Unfoldings",
      "text" : "Under assumption (A1) that all CPTs have full column rank, the factorization of A, B and C in (5), (6) and (7) respectively suggest that (see appendix §9 for more details)\nrank(A) = rank(PHG) = k ≤ rank(B) = rank(C) = nnz(PHG), (8) where nnz(·) denotes the number of nonzero elements. We note that the equality is attained if and only if the relationship between the hidden variables G and H is deterministic, i.e., there is a single nonzero element in each row and in each column of PHG. In this case, the grouping of variables in a quartet can be arbitrary, and we will not consider this case in the paper. More specifically, we have\nTheorem 3. Assume PHG has a few zero entries, then k ≪ k2 ≈ nnz(PHG) and thus\nrank(A) ≪ rank(B) = rank(C). (9)\nThe above theorem reveals a useful difference between the correct grouping of variables and the two incorrect ones. Furthermore, this condition can be easily verified: Given P we can check the rank of its matrix representations A, B and C and thus discover the latent structure of the quartet."
    }, {
      "heading" : "3.3 Nuclear Norm Relaxation for the Rank Condition",
      "text" : "In practice, due to sampling noise all unfolding matrices A, B and C would be nearly full rank, so the rank condition cannot be applied directly. To deal with this, we design a test based on relaxation of the rank condition using nuclear norm\n‖M‖∗ = ∑n\ni=1 σi(M), (10)\nwhich is the sum of all singular values of an (n× n) matrix M . Instead of comparing the ranks of A, B and C, we look for the one with the smallest nuclear norm and declare the latent structure corresponding to it. This simple quartet algorithm is summarized in Algorithm 1. Note that\nAlgorithm 1 i∗ = Quartet(X1, X2, X3, X4)\n1: Estimate P̂(X1,X2,X3,X4) from a set of m i.i.d. samples {(xl1, xl2, xl3, xl4)}ml=1. 2: Unfold P̂ in three different ways into matrices Â, B̂ and Ĉ, and compute their nuclear norms a1 = ‖Â‖∗, a2 = ‖B̂‖∗ and a3 = ‖Ĉ‖∗. 3: Return i∗ = argmini∈{1,2,3} ai.\nAlgorithm 1 works even if the number of hidden states, k, is a priori unknown. This is an important advantage over the idea of learning the structure based on additive distance (Lake, 1994), where k is assumed to be the same as the number of states, n, of the observed variables, or over a recent approach based on quartet test (Anandkumar et al., 2011), where k needs to be specified in advance.\nIn our current context, nuclear norm has a few useful properties. First, it is the tightest convex lower bound of the rank of a matrix (Fazel et al., 2001). This is why4 it is meaningful to compare nuclear norms instead of ranks. Second, it is easy to compute: a standard singular value decomposition will do the job. Third, it is robust to estimate. The nuclear norm of a probability matrix Â based on samples is nicely concentrated around its population quantity (Rosasco et al., 2010). Given a confidence level 1− 2e−τ , an estimate based on m samples satisfies\n|‖A‖∗ − ‖Â‖∗| = ∣∣∣ ∑\ni σi(A)− ∑ i σi(Â) ∣∣∣ ≤ 2 √ 2τ/ √ m. (11)\nFourth, the nuclear norm can be viewed as a measure of dependence between two pairs of variables. For instance, if A corresponds to grouping {{1, 2}, {3, 4}}, ‖A‖∗ measures the dependence between the compound variables {X1,X2} and {X3,X4}. In the community of kernel methods, A is treated as a cross-covariance operator between {X1,X2} and {X3,X4}, and its spectrum has been used to design various dependence measures, such as Hilbert-Schmidt Independence Criterion, which is the sum of squares of all singular values (Gretton et al., 2005a), and kernel constrained covariance, which only takes the largest singular value (Gretton et al., 2005b). Intuitively, our quartet test\n4Note that A, B and C consist of the same elements so their Frobenius norms are the same, i.e., the 3 matrices are readily equally “normalized”.\nsays that: if we group the variables correctly, then cross group dependence should be low, since the groups are separated by two latent variables; however if we group the variables incorrectly, then cross group dependence should be high, since similar variables exist in the two groups."
    }, {
      "heading" : "4 Recovery Conditions and Finite Sample Guarantee for Quartets",
      "text" : "Since nuclear norm is just a convex lower bound of the rank, there might be situations where the nuclear norm does not satisfy the same relation as the rank. That is, it might happen that rank(A) ≤ rank(B) but ‖A‖∗ ≥ ‖B‖∗. In this section, we present sufficient conditions under which nuclear norm returns successful quartet test.\nWhen latent variables H and G are independent, rank(PHG) = 1, since PHG = PHP ⊤ G (P (h, g) = P (h)P (g)). Let {{1, 2}, {3, 4}} be the correct quartet relation. We can obtain simpler characterizations of the 3 unfoldings of P(X1,X2,X3,X4), denoted as A⊥, B⊥ and C⊥ respectively. Using Lemma 2 and the independence of H and G, we have (see appendix, (26)–(27))\nA⊥ = (P2|H ⊙ P1|H) PHP⊤G (P4|G ⊙ P3|G)⊤ = P12(:) P34(:) ⊤, B⊥ = (P3|G ⊗ P1|H)(diag(PG)⊗ diag(PH))(P4|G ⊗ P2|H)⊤ = P34 ⊗ P12,\n(12)\nand rank(A⊥) = 1 ≪ rank(B⊥) which is consistent with Theorem 3. Furthermore, since A⊥ has only one nonzero singular value, we have ‖A⊥‖∗ = ‖A⊥‖F = ‖B⊥‖F ≤ ‖B⊥‖∗ (using ‖M‖F ≤ ‖M‖∗ for any matrix M). Similarly, C⊥ = P43 ⊗ P12 and ‖A⊥‖∗ ≤ ‖C⊥‖∗. Then we know for sure that the nuclear norm quartet test will return the correct topology.\nWhen latent variables H and G are not independent, we treat it as perturbation ∆ away from the independent case, i.e., P̃HG = PHP ⊤ G + ∆. The size of ∆ quantifies the strength of dependence between H and G. Obviously, when ∆ is small, e.g., ∆ = 0, we are back to the independence case and it is easy to discover the correct quartet relation; when it is large, e.g., ∆ = I−PHP⊤G , H and G are deterministically related and the different groupings are indistinguishable. The question is how large can ∆ be while still allowing the nuclear norm quartet test to find the correct latent relation.\nFirst, we require (A3) ∆1 = 0, and ∆⊤1 = 0, where 1 and 0 are vectors of all ones and all zeros. Such perturbation ∆ keeps the marginal distributions PH and PG as in the independent case, since P̃H = P̃HG1 = PHP ⊤ G 1 + ∆1 = PH . Assuming {{1, 2}, {3, 4}} is the correct quartet relation, ∆ also keeps the pairwise marginal distribution P12 as in the independent case, since P12 = P1|H diag(PH)P ⊤ 2|H and the marginal PH is the same before and after the perturbation. Similar reasoning also applies to P34 = P3|G diag(PG)P ⊤ 4|G.\nWe define excessive dependence of the correct and incorrect groupings as\nθ := min{‖B⊥‖∗ − ‖A⊥‖∗, ‖C⊥‖∗ − ‖A⊥‖∗}.\nIt quantifies the changes in dependence when we switch from incorrect groupings to the correct one (in the case when H and G are independent). Note that θ is measured only from pairwise marginals (12), P12 and P34. Using matrix perturbation analysis we can show that (see appendix §11 for proof)\nLemma 4. If ‖∆‖F ≤ θk2+k , then Algorithm 1 returns the correct quartet relation.\nThus, if the excessive dependence θ is large compared to the number of hidden states, the size of the allowable perturbation can be correspondingly larger. In other words, if the dependence between variables within the same group is strong enough compared to the dependence across groups, we allow for larger ∆ and stronger dependence between hidden variables H and G (which is closer to the indistinguishable case). Then under the recovery condition in Lemma 4, and given m i.i.d. observations, we can obtain the following guarantee for the quartet test (see appendix, §13 for proof). Let α = min {‖B‖∗ − ‖A‖∗, ‖C‖∗ − ‖A‖∗}.\nLemma 5. With probability 1− 8e− 132mα2 , Algorithm 1 returns the correct quartet relation."
    }, {
      "heading" : "5 Building Latent Tree from Quartets",
      "text" : "Algorithm. We can use the resolved quartet relations (Algorithm 1) to discover the structure of the entire tree via an incremental divide-and-conquer algorithm (Pearl & Tarsi, 1986; Pearl, 1988), summarized in Algorithm 2 (further details in appendix §10). Joining variable Xi+1 to the current tree of i leaves can be done with O(log i) tests. This amounts to performing O(d log d) quartet tests for building an entire tree of d leaves, which is efficient even if d is large. Moreover, as shown in (Pearl & Tarsi, 1986), this algorithm is consistent.\nAlgorithm 2 T = BuildTree(X1, . . . ,Xd) 1: Connect any 4 variables X1, X2, X3, X4 with 2 latent variables in a tree T using Algorithm 1.\n2: for i = 4, 5, . . . , d− 1 do {insert (i+1)-th leaf Xi+1} 3: Choose root R that splits T into sub-trees T1,T2,T3 of roughly equal size. 4: Choose any triplet (Xi1 ,Xi2 ,Xi3) of leaves from different sub-trees. 5: Test which sub-tree should Xi+1 be joined to: i∗ ← Quartet(Xi+1,Xi1 ,Xi2 ,Xi3). 6: Repeat recursively from step 3 with T := Ti∗. This will eventually reduce to a tree with a single leaf. Join Xi+1 to it via hidden variable. 7: end for\nTree recovery conditions and guarantees. How will the quartet recovery conditions translate to recovery conditions for the entire tree, where each “edge” of a quartet is a path in the tree? What are the finite sample guarantees for the divide-and-conquer algorithm?\nWhen a quartet is taken from a latent tree, each edge of the quartet corresponds to a path in the tree involving a chain of variables (Figure 2). We need to bound the perturbation to each single edge of the tree such that joint path perturbations satisfy edge perturbation conditions from Lemma 4. For a quartet q = {{i1, i2}, {i3, i4}} corresponding to a single edge between H and G, denote the excessive dependence by θq. By adding perturbation ∆q of size smaller than θq k2+k to PHP ⊤ G we can still correctly recover q. Let θmin := minquartet q θq. If we require ‖∆q‖F ≤ θmin k2+k\n, all such quartet relations will be recovered successfully. If we further restrict the size of the perturbation by the smallest value in a marginal probability distribution of a hidden variable, γmin := minhidden node H mini=1...k PH(i), we can guarantee that all quartet relations corresponding\nto a path betweenH and G can also be successfully recovered by the nuclear norm test (see appendix §12). Therefore, we assume that (A4) ‖∆q‖F ≤ min{ θmin k2+k , γmin} for all quartets q in a tree.\nTheorem 6. Algorithm 2 returns the correct tree topology under assumptions (A1)–(A4).\nThe recovery conditions guarantee that all quartet relations can be resolved correctly and simultaneously. Then a consistent algorithm using a subset of the quartet relations should return the correct tree structure. Given m i.i.d. samples, we have the following statistical guarantee for the tree building algorithm (see appendix, §14 for proof). Let αmin := minquartet q αq.\nTheorem 7. With probability 1 − 8 · c · d log d · e− 132mα2min , Algorithm 2 recovers the correct tree topology for a constant c under assumptions (A1)–(A4) .\nWe note that there are better quartet based algorithms for building latent trees with stronger statistical guarantees, e.g. (Erdös et al., 1999). We can adapt our nuclear norm based quartet test to those algorithm as well. However, this is not the main focus of the paper. We choose the divideand-conquer algorithm due to its simplicity, ease of analysis and it illustrates well how our quartet recovery guarantee can be translated into a tree building guarantee."
    }, {
      "heading" : "6 Experiments",
      "text" : "We compared our algorithm with representative algorithms: the neighbor-joining algorithm (NJ) (Saitou & Nei, 1987), a quartet based algorithm of Anandkumar et al. (2011) (Spectral@k), the Chow-Liu neighbor Joining algorithm (CLNJ) (Choi et al., 2011), and an algorithm of Harmeling & Williams (2010) (HW).\nNJ proceeds by recursively joining two variables that are closest according to an additive distance defined as dij = 1 2 log det diagPi − log |detPij | + 12 log det diagPj , where “det” denotes determinant, “diag” is a diagonalization operator, Pij denotes the joint probability table P (Xi,Xj), and Pi and Pj the probability vector P (Xi) and P (Xj) respectively (Lake, 1994). When Pij has rank k < n, log |detPij | is not defined, NJ can perform poorly. Spectral@k uses singular values of Pij to design a quartet test (Anandkumar et al., 2011). For instance, if the true quartet configuration is {{1, 2}, {3, 4}} as in Figure 2, then the quartet needs to satisfy ∏ks=1 σs(P12)σs(P34) > max{ ∏k s=1 σs(P13)σs(P24), ∏k s=1 σs(P14)σs(P23)}. Based on this relation, a confidence interval based quartet test is designed and used as a subroutine for a tree reconstruction algorithm. Spectral@k can handle cases with k < n, but still require k as an input. We will show in later experiments that its performance is sensitive to the choice of k. CLNJ first applies Chow-Liu algorithm (Chow & Liu, 1968) to obtain a fully observed tree and then proceeds by adding latent variables using neighbor joining algorithm. The HW algorithm is a greedy algorithm to learn binary trees by iteratively joining two nodes with a high mutual information. The number of hidden states is automatically determined in the HW algorithm and can be different for different latent variables."
    }, {
      "heading" : "6.1 Resolving Quartet Relations",
      "text" : "We compared our method to NJ and Spectral@k in terms of their ability to recover the quartet relation among four variables. We used quartet with three different configurations for the hidden states: (1) kH = 2 and kG = 4 (small difference); (2) kH = 2, kG = 8 (large difference); and (3)\nkH = 4, kG = 4 (no difference). In all cases, the states of the observed variables were fixed to n = 10. In all cases we started from independent PHG but identity PXi|H and PXi|G, and perturbed them using the following formula P (a = i|b) = P (a=i|b)+ui∑ i P (a=i|b)+ui , where all ui are i.i.d. random variables drawn from Uniform[0, µ]. We then drew random sample from the quartet according to these CPTs. We studied the percentage of correctly recovered quartet relations as we varied the sample size across S = {50, 100, 200, 300, 400, 500, 750, 1000, 1500, 2000} and under two different levels of perturbation (µ = {0.5, 1}). We randomly initialized each experiment 1000 times and report the average quartet recovery performance and the standard error in Figure 5.\nThe proposed method compares favorably to NJ and Spectral@k. The performance of Spectral@k\nvaries a lot depending on the chosen number of singular values k. Our method is free from tuning parameters and often stays among the top performing ones. Especially when the number of hidden states are very different from each other (kH = 2 and kG = 8), our method is leading the second best by a large gap (Figure 5(b) and 5(e)). When both hidden states are the same (kH = kG = 4), the Spectral@k achieves the best performance when the chosen number of singular values k is the same as kH . Note that allowing Spectral@k to use different k resembles using cross validations for finding the best k. It is expensive while our approach performs almost indistinguishable from Spectral@k even it choose the best k."
    }, {
      "heading" : "6.2 Discovering Latent Tree Structure",
      "text" : "We used different tree topologies and sample sizes in this experiment. We generated tree topologies by randomly splitting 16 observed variables recursively into two groups. The recursive splitting stops when there are only two nodes left in a group. We introduced a hidden variable to join the two partitions in each recursion and this gives a latent tree structure. The topology of the tree is controlled by a single splitting parameter β which controls the relative size of the first partition versus the second. If β is close to 0 or 1, we obtain trees of skewed shape, with long path of hidden variables. If β is close to 0.5, the resulting latent trees are more balanced. In our experiments, we experimented with skewed latent trees β = 0.2 and balanced trees β = 0.5. We first generate different random k between 2 and 8 for the hidden states, and then generate the probability models for each tree using the same scheme as in our previous experiment. Here we experimented with perturbation level µ = {0.2, 0.5, 1}.\nWe varied the sample size across S = {50, 100, 200, 500, 1000, 2000}, and measured the error of the constructed tree using Robinson-Foulds metric (Robinson & Foulds, 1981). This measure is a metric over trees of the same number of leaves. It is defined as (a + b) where a is the number of partitions of variables implied by the learned tree but not by the true tree and b is the number of partitions of the variables implied by the true tree but not by the learned tree (in a sense similar to precision and recall score).\nThe tree recovery results are shown in Figure 5(g)-5(l). Again we can see that our proposed method compares favorably to existing algorithms. All through the 6 experimental conditions, the tensor approach and spectral@2 performed the best with sufficiently large sample sizes. Note that we tried out different k for Spectral@k which resembles using cross validations for finding the best k. Even in this case, our approach works comparably without having to know k. HarmelingWilliam’s algorithm performed well in small sample sizes, while CLNJ does not perform well in these experimental conditions."
    }, {
      "heading" : "6.3 Understanding Latent Relations between Stocks",
      "text" : "We applied our algorithm to discover a latent tree structure from a stock dataset. Our goal is to understand how stock prices Xi are related to each other. We acquired closing prices of 59 stocks from 1984 to 2011 (from www.finance.yahoo.com), which provides us 6800 samples. The daily change of each stock price is discretized into 10 values, and we applied our algorithm to build a latent tree. A visualization of the learned tree topologies and discovered groupings are shown in Figure 6.\nWe see nice groupings of stocks according to their industrial sectors. For instance, companies related to petroleum, such as CVX (Chevron), XOM (Exxon Mobil), APA (Apache), COP (Cono-\ncoPhillips), SLB (Schlumberger) and SUN (Sunoco), are grouped into a subtree. Pharmaceutical companies, such as MRK (Merck), PFE (Pfizer), BMY (Bristol Myers Squibb), LLY (Eli Lilly), ABT (Abbott Laboratories), JNJ (Johnson and Johnson) and BAX (Baxter International), are all grouped into a subtree. High-tech companies, such as AMD, MOT (Motorola), HPQ (HewlettPackard), IBM, are grouped into another subtree. There are also subtree for retailers, such as TGT (Target), WMT (Wal-Mart), RSH (RadioShack), subtree for utility service companies, such as DUK (Duke Energy), ED (Consolidated Edison), EIX (Edison), ECX (Exelon), VZ (Verizon), and subtree related to financial companies, such as C (Citigroup), JPM (JPMorgan Chase), and AXP (American Express). We can also see subtree related to financial companies, such as C (Citigroup), JPM (JPMorgan Chase), and AXP (American Express). An interesting observation is that F (Ford Motor) which is well-known for its car manufacturing is also placed in the same branch as these financial companies. This seemingly abnormal structure can be explained by the fact that Ford Motor operates under two segments: Automotive and Financial Services. Its financial services include the operations of Ford Motor Credit Company and other financial services including holding companies, and real estate. In this respect, it is quite interesting that our algorithm discovered this hidden information.\nWe also compared different algorithms in terms of held-out likelihood. We first randomized the data 10 times, and each time used half for training and half for computing the held-out likelihood. Then we estimated the latent binary tree structures using different algorithms. Finally, we fit latent variable models to the discovered structures. The number of the states for all hidden variables, k, were the same in each latent variable model. We experimented with k = 2, 4, 6, 8, 10 to simulate the process of using cross validation to select the best k. The results are presented in Table 1. Note\nthat Harmeling-William’s algorithm automatically discovers k, so it does not use the experimental parameter k. Chow-Liu tree does not contain any hidden variables and hence just one number in the table. CLNJ and Neighbor-joining assume the states for the hidden and observed variables are the same during structure learning. However, in parameter fitting, we can still use different number\nof hidden states k. In this experiment, the structure produced by our tensor approach produced the best held-out likelihood."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose a quartet-based method for discovering the tree structures of latent variable models. The practical advantage of the new method is that we do not need to pre-specify the number of the hidden states, a quantity usually unknown in practice. The key idea is to view the joint probability tables of quadruple of variables as 4th order tensors and then use the spectral properties of the unfolded tensors to design a quartet test. We provide conditions under which the algorithm is consistent and its error probability decays exponentially with increasing the sample size. In both simulated and a real dataset, we demonstrated the usefulness of our methods for discovering latent structures. While in this study we focus on the properties of the 4th order tensor and its various unfoldings, we believe that properties of tensors and methods and algorithms from multilinear algebra will allow to address many other problems arising from latent variable models."
    }, {
      "heading" : "8 Properties and Notations used",
      "text" : "Nuclear and Frobenius norms:\n• Let σi be the singular values of A. Then\n‖A‖∗ = ∑\ni\nσi , ‖A‖2F = ∑\ni\nσ2i and ‖A‖F ≤ ‖A‖∗ . (13)\n• (Nuclear and Frobenius norms are unitarily invariant) For any orthogonal Q we have\n‖A‖∗ = ‖QA‖∗ = ‖AQ‖∗ , ‖A‖F = ‖QA‖F = ‖AQ‖F .\n(14)\n• ‖AB‖∗ ≤ ‖A‖F ‖B‖F ≤ ‖A‖∗‖B‖∗ .\n• Let σi be the singular values of X and σ̃i be the singular values of X̃ = X + E. Then\n‖diag(σ̃i − σi)‖∗ ≤ ‖X̃ −X‖∗ . (15)\nKronecker and Khatri-Rao products:\n(A⊗B)⊤ = A⊤ ⊗B⊤ (16) (A+B)⊗ C = A⊗ C +B ⊗ C (17)\nAB ⊗ CD = (A⊗ C)(B ⊗D) (18) AB ⊙ CD = (A⊗ C)(B ⊙D) (19) ‖A⊗B‖F = ‖A‖F ‖B‖F\nrank(A⊗B) = rank(A) rank(B)\nTensor operations:\nWe use the following tensor-matrix products of a tensor A ∈ RI1×I2×I3 with matrices M (n) ∈ R Jn×In , n = 1, 2, 3:\nmode-1 product: (A •1 M (1))j1i2i3 = ∑I1\ni1=1 ai1i2i3m\n(1) j1i1 ,\nmode-2 product: (A •2 M (2))i1j2i3 = ∑I2\ni2=1 ai1i2i3m\n(2) j2i2 ,\nmode-3 product: (A •3 M (3))i1i2j3 = ∑I3\ni3=1 ai1i2i3m\n(3) j3i3 ,\nwhere 1 ≤ in ≤ In, 1 ≤ jn ≤ Jn. These products can be considered as a generalization of the left and right multiplication of a matrix A with a matrix M. The mode-1 product signifies multiplying\nthe columns (mode-1 vectors) of A with the rows of M (1) and similarly for the other tensor-matrix products.\nThe contracted product C of two tensors A ∈ RI×J×M and B ∈ RK×L×M along their third modes is a 4th order tensor denoted by C = 〈A,B〉3. C ∈ RI×J×K×L and its entries C(i, j, k, l), 1 ≤ i ≤ I; 1 ≤ j ≤ J ; 1 ≤ k ≤ K; 1 ≤ l ≤ L are defined as\nC(i, j, k, l) = ∑M\nm=1 aijm bklm.\nIt can be interpreted as taking inner products of the mode-3 vectors of A and B and storing the results in C.\nThe 3 different reshapings A, B and C (2)–(4) of the tensor P contain exactly the same entires as P but in different order.\n• A corresponds to the grouping {{1, 2}, {3, 4}} of the variables. The rows of A correspond to dimensions 1 and 2 of P, and its columns to dimensions 3 and 4. Suppose all observed variables take values from {1, . . . , n}, then entry of A at (x1+n(x2−1))-th row and (x3+n(x4−1))-th column is equal to P(x1, x2, x3, x4); • B corresponds to the grouping {{1, 3}, {2, 4}}, and its entry at (x1 + n(x3 − 1))-th row and (x2 + n(x4 − 1))-th column is equal to P(x1, x2, x3, x4); • C corresponds to the grouping {{1, 4}, {2, 3}}, and its entry at (x1 + n(x4 − 1))-th row and (x2 + n(x3 − 1))-th column is equal to P(x1, x2, x3, x4)."
    }, {
      "heading" : "9 Matrix Representations A, B, C of P",
      "text" : "From P to A, B, C:\nLet X ∈ Rm×k, Y ∈ Rk×l, Z ∈ Rn×l, X = (x1, . . . , xk) and Z = (z1, . . . , zl). A useful property that we will use in our derivations is the following\nX Y Z⊤ = ∑\ni,j\nxi yij z ⊤ j . (20)\nWe can derive the formula for A starting from the element-wise formula (1)\nP(x1, x2, x3, x4) = ∑\nh,g\nP (x1|h)P (x2|h)P (h, g)P (x3 |g)P (x4|g)\nand placing all entries in the matrix A in the correct order. Note that given h and g we only need one column of each P1|H , P2|H , P3|G and P4|G, which we will denote by (P1|H)h, (P2|H)h, (P3|G)g and (P4|G)g. In order to obtain a matrix such that X1 and X2 are mapped to rows and X3 and X4 are mapped to columns, we need to map all possible products of single element of (P1|H)h and single element of (P2|H)h to rows and and similarly, we need to map all possible products of single element of (P3|G)g and single element of (P4|G)g to columns. This can be done using Khatri-Rao products in the following way\nA = ∑\nh,g\n( (P2|H)h ⊙ (P1|H)h ) (PHG)hg ( (P4|G)g ⊙ (P3|G)g )⊤\n(20) = ( P2|H ⊙ P1|H ) PHG ( P4|G ⊙ P3|G )⊤ .\nThe matrix B is unfolding of P, such that the rows of B correspond to X1 and X3 and the columns of B correspond to X2 and X4. We have\nB = ∑\nh,g\n( (P3|G)g ⊙ (P1|H)h ) (PHG)hg ( (P4|G)g ⊙ (P2|H)h )⊤\n(16) =\n∑\nh,g\n( (P3|G)g ⊗ (P1|H)h ) (PHG)hg ( (P4|G) ⊤ g ⊗ (P2|H)⊤h )\n(18) =\n∑\nh,g\n(PHG)hg ( (P3|G)g(P4|G) ⊤ g ) ⊗ ( (P1|H)h(P2|H) ⊤ h )\n(17) =\n∑\nh\n(∑\ng\n(PHG)hg(P3|G)g(P4|G) ⊤ g\n) ⊗ ( (P1|H)h(P2|H) ⊤ h )\n(20) =\n∑\nh\n( P3|G diag((PHG)h)P ⊤ 4|G ) ⊗ ( (P1|H)h(P2|H) ⊤ h )\n(18) =\n∑\nh\n( P3|G ⊗ (P1|H)h ) diag((PHG)h) ( P⊤4|G ⊗ (P2|H)⊤h )\nblock−(20) = ( P3|G ⊗ P1|H ) diag(PHG(:)) ( P⊤4|G ⊗ P⊤2|H )\n(16) =\n( P3|G ⊗ P1|H ) diag(PHG(:)) ( P4|G ⊗ P2|H )⊤ .\nThe expression for C is derived in a similar way. Other representations of A, B, C:\nUsing the properties in Section 8 and the formulas (5)–(7) for the matrix unfoldings A, B and C, we can derive the following additional formulas,\nA = ( P2|H ⊙ P1|H ) PHG ( P4|G ⊙ P3|G )⊤\n= ( In P2|H ⊙ P1|H IH ) PHG ( In P4|G ⊙ P3|G IG )⊤\n(19) =\n( In ⊗ P1|H ) ( P2|H ⊙ IH ) PHG ( P4|G ⊙ IG )⊤ ( In ⊗ P3|G )⊤\n=   P1|H . . .\nP1|H\n    p (1,1) 2|H p (1,2) 2|H. . .\np (2,1) 2|H ... . . .\n  PHG   p (1,1) 4|G p (1,2) 4|G. . .\np (2,1) 4|G ... . . .\n  ⊤  P3|G . . .\nP3|G\n  ⊤ ,\n(21)\nB = ( P3|G ⊗ P1|H ) diag(PHG(:)) ( P4|G ⊗ P2|H )⊤\n= ( P3|G IG ⊗ In P1|H ) diag(PHG(:)) ( P4|G IG ⊗ In P2|H )⊤\n(18),(16) =\n( P3|G ⊗ In ) ( IG ⊗ P1|H ) diag(PHG(:)) ( IG ⊗ P2|H )⊤ ( P4|G ⊗ In )⊤\n=\n  ( p (1,1) 3|G ) · · · ( p (2,1) 3|G )\n...\n    P1|H . . .\nP1|H\n  diag(PHG(:))   P2|H . . .\nP2|H\n  ⊤  ( p (1,1) 4|G ) · · · ( p (2,1) 4|G )\n...\n  ⊤ ,\n(22) where (p(i,j)) is a diagonal block of size (n× n) with all diagonal elements equal to p(i,j).\nThe formula for C can be obtained from the ones for B by swapping the positions of P3|G and P4|G.\nRank properties of A, B, C:\nIn this section we prove the rank properties used in Section 3.2 of the paper. Lemma. If X ∈ Rm×n, Y ∈ Rn×k, Z ∈ Rl×m, Y has full row rank, and Z has full column rank, then rank(XY ) = rank(X),\nrank(ZX) = rank(X).\nWe assume that all CPTs have full column (or row) rank. Then the first two matrices in (21) also have full column rank. The last two matrices have full row rank. From the lemma, it follows that\nrank(A) = rank(PHG) = k (23)\nAnalogously, the first two matrices in (22) have full column rank. The last two matrices have full row rank. From the lemma, it follows that\nrank(B) = nnz(PHG), (24)\ni.e., generically, rank(B) = k2."
    }, {
      "heading" : "10 Algorithms",
      "text" : "Algorithm 3 Tnext = QuartetTree(T1, T2, T3, X4) Require: Leaf(T ): leaves of a tree T ; 1: for j = 1 to 3 do 2: Xi ← Randomly choose a variable from Leaf(Ti) 3: end for 4: i∗ ← Quartet(X1, X2, X3, X4), Tnext ← Ti∗\nAlgorithm 4 T = Insert(T , T̃ ,Xi) Require: Left(T ) and Right(T ): left and right child branch of the root respectively; T +T ′: return\na new tree connecting the root of two trees by an edge and use the root of T as the new root 1: if |Leaf(T )| = 1 then 2: T ← Form a tree with root R connecting Leaf(T ) and Xi. 3: else 4: Tnext ← QuartetTree(Left(T ), Right(T ), T̃ , Xi) 5: if Tnext = Left(T ) then 6: T ← Insert(Tnext, Right(T ) + T̃ , Xi) 7: else if Tnext = Right(T ) then 8: T ← Insert(Tnext, Left(T ) + T̃ , Xi) 9: end if\n10: end if 11: T ← T + T̃\nAlgorithm 5 T = BuildTree({X1, . . . ,Xd}) 1: Randomly choose X1, X2, X3 and X4 2: i∗ ← Quartet(X1, X2, X3, X4) 3: T ← Form a tree with two connecting hidden variables H and G, where H joins Xi∗ and X4,\nwhile G joins variables in {X1,X2,X3} \\ {Xi∗} 4: for i = 5 to d do 5: Pick a root R from T which split it to three branches of equal sizes, and Tnext ← QuartetTree(Left(T ), Right(T ), Middle(T ), Xi) 6: if Tnext = Left(T ) then 7: T ← Insert(Tnext, Right(T ) + Middle(T ), Xi) 8: else if Tnext = Right(T ) then 9: T ← Insert(Tnext, Left(T ) + Middle(T ), Xi)\n10: else if Tnext = Middle(T ) then 11: T ← Insert(Tnext, Right(T ) + Left(T ), Xi) 12: end if 13: end for"
    }, {
      "heading" : "11 Recovery Conditions for Quartet",
      "text" : "Latent variables H and G are independent. In this case, rank(PHG) = 1, since P (h, g) = P (h)P (g). Applying the relation in Equation 8, we have that rank(A) = 1 ≪ rank(B). Furthermore, since A has only one nonzero singular value, we have ‖A‖∗ = ‖A‖F = ‖B‖F ≤ ‖B‖∗, since ‖M‖F ≤ ‖M‖∗ for any M. In this case, we know for sure that the nuclear norm quartet test will return the correct topology.\nLatent variables H and G are not independent. We analyze this case by treating it as perturbation ∆ away from the PHG in the independent case. We want to characterize how large ∆ can be while still allowing the nuclear norm quartet test to find the correct latent relation. Suppose A⊥ and B⊥ are the unfolding matrices in the case whereH and G are independent. Suppose we add perturbation ∆ to PHG, then A⊥ = ( P2|H⊙P1|H ) PHG ( P4|G⊙P3|G )⊤ and its perturbed version is\nA = ( P2|H ⊙P1|H ) (PHG +∆) ( P4|G ⊙P3|G )⊤ . We want to bound the difference | ‖A⊥‖∗ − ‖A‖∗|. We have\n| ‖A⊥‖∗ − ‖A‖∗| = ∣∣∣ ∑\ni σi(A⊥)−\n∑\ni\nσi(A) ∣∣∣\n≤ ∑\ni |σi(A⊥)− σi(A)|\n(15) ≤ ‖A⊥ −A‖∗ ≤ ∥∥(P2|H ⊙ P1|H ) ∆ ( P4|G ⊙ P3|G )⊤∥∥ ∗\n≤ ∥∥P2|H ⊙ P1|H ∥∥ F ‖∆‖F ∥∥P4|G ⊙ P3|G ∥∥ F\n≤ k ‖∆‖F ,\nsince P2|H ⊙ P1|H and P4|G ⊙ P3|G are CPTs with k columns each, and thus ∥∥P2|H ⊙ P1|H ∥∥2 F ≤ k\nand ∥∥P4|G ⊙ P3|G ∥∥2 F ≤ k.\nAnalogously, B⊥ = ( P3|G ⊗ P1|H ) diag(PHG(:)) ( P4|G ⊗ P2|H )⊤ and its perturbed version\nis B = ( P3|G ⊗ P1|H ) diag(PHG(:) + ∆(:)) ( P4|G ⊗ P2|H )⊤ . We want to bound the difference |‖B⊥‖∗ − ‖B‖∗|. We have\n|‖B⊥‖∗ − ‖B‖∗| = ∣∣∣ ∑\ni σi(B⊥)−\n∑\ni\nσi(B) ∣∣∣\n≤ ∑\ni |σi(B⊥)− σi(B)|\n(15) ≤ ‖B⊥ −B‖∗ ≤ ∥∥(P3|G ⊗ P1|H ) diag(∆(:)) ( P4|G ⊗ P2|H )⊤∥∥ ∗\n≤ ∥∥P3|G ⊗ P1|H ∥∥ F ∥∥ diag(∆(:)) ∥∥ F ∥∥P4|G ⊗ P2|H ∥∥ F\n≤ k2 ‖diag(∆(:))‖F = k2 ‖∆‖F ,\nsince P3|G ⊗ P1|H and P4|G ⊗ P2|H are CPTs with k2 columns, and thus ∥∥P3|G ⊗ P1|H ∥∥2 F ≤ k2 and∥∥P4|G ⊗ P2|H ∥∥2 F ≤ k2.\nTherefore, we get the following upper and lower bound:\n‖A‖∗ ≤ ‖A⊥‖∗ + k ‖∆‖F , ‖B‖∗ ≥ ‖B⊥‖∗ − k2 ‖∆‖F .\nIf we require that\n‖A⊥‖∗ + k ‖∆‖F ≤ ‖B⊥‖∗ − k2 ‖∆‖F ,\nthen we will have ‖A‖∗ ≤ ‖B‖∗.\nWe can derive similar condition for the relationship ‖A‖∗ ↔ ‖C‖∗. Let\nθ := min{‖B⊥‖∗ − ‖A⊥‖∗, ‖C⊥‖∗ − ‖A⊥‖∗}.\nWe thus obtain an upper bound on the allowed perturbation:\n‖∆‖F ≤ θ\nk2 + k . (25)"
    }, {
      "heading" : "12 Recovery Conditions for Latent Tree",
      "text" : "When latent variables H and G are independent, we have that PHG = PHP ⊤ G . In this case,\n‖B⊥‖∗ = ∥∥(P3|G ⊗ P1|H)(diag(PG)⊗ diag(PH))(P4|G ⊗ P2|H)⊤ ∥∥ ∗\n= ∥∥∥(P3|G diag(PG)P⊤4|G)⊗ (P1|H diag(PH)P⊤2|H) ∥∥∥ ∗ = ‖P34 ⊗ P12‖∗ ≥ ‖P34 ⊗ P12‖F\n(26)\nand ‖A⊥‖∗ = ∥∥(P2|H ⊙ P1|H) PHP⊤G (P4|G ⊙ P3|G)⊤ ∥∥ ∗\n= ∥∥P12(:)P34(:)⊤ ∥∥ ∗\n= ∥∥P12(:)P34(:)⊤ ∥∥ F = ‖P34 ⊗ P12‖F\n(27)\nand thus\n‖A⊥‖∗ ≤ ‖B⊥‖∗ . Suppose now that H and G are not independent and thus we have PHG = PHP ⊤ G + ∆. The goal is to characterize all ∆s, such that ‖A‖∗ ≤ ‖B‖∗ still holds for any quartet. From the above formulas it follows that the upper bound on ∆ depends only on pairwise marginal distributions.\nSince the perturbed version of PHP ⊤ G remains a joint probability table, all entries of the perturbation matrix ∆ have to sum to 0, i.e., 1⊤∆(:) = 0. We further assume that each column sum and each row sum of ∆ is also equal to 0, i.e., 1⊤∆ = 0 and ∆1 = 0. In this case, 1⊤∆(:) = 0 is satisfied automatically.\nThe recovery conditions for latent trees can be derived in two steps. The first step is to provide recovery conditions for those quartet relations corresponding to a single edge H − G in the tree (Figure 7, left). In the second step we study quartet relations corresponding to paths H −M1 −M2 − · · · − Ml − G in the tree (Figure 7, right). We provide a condition under which the recovery condition of such quartets is reduced to the recovery condition on quartets from step 1. That is, we provide a condition under which the perturbation on the path is guaranteed to be smaller than the maximum allowed perturbation on an edge.\nLet\nδ := max H−G an edge\n‖∆HG‖F .\nOur goal is to obtain conditions on δ, under which recovery of any quartet relation is guaranteed."
    }, {
      "heading" : "12.1 Quartets Corresponding to a Single Edge",
      "text" : "The first step is readily obtained from §11 if we assume that all CPTs (including PXi1 |H , PXi2 |H , PXi3 |G, PXi4 |G) have full rank. Let θmin = minquarter q θq. From (25), we have\nδ ≤ min ‖B⊥‖∗ − ‖A⊥‖∗ k2 + k = θmin k2 + k . (28)"
    }, {
      "heading" : "12.2 Quartets Corresponding to a Path",
      "text" : "Path of independent latent variables. For the second step, we start again from the fully factorized case (independent case). The joint probability table PHG of the two end points in a path H −M1 −M2 − · · · −Ml −G is\nPHG = PH|M1PM1|M2 · · ·PMl|GPG = PHM1 diag(PM1) −1PM1M2 diag(PM2) −1 · · · diag(PMl)−1PMlG\n= PHP ⊤ M1 diag(PM1) −1PM1P ⊤ M2 diag(PM2) −1 · · · diag(PMl)−1PMlP⊤G = PH(P ⊤ M1 diag(PM1) −1)PM1(P ⊤ M2 diag(PM2) −1) · · · diag(PMl)−1PMlP⊤G = PH1 ⊤PM11\n⊤ · · · 1⊤PMlP⊤G = PHP ⊤ G ,\nwhere we have used P⊤Mi diag(PMi(:)) −1 = 1⊤.\nPath of dependent latent variables. Next, we add perturbation matrices to the joint probability tables associated with each edge Mi − Mj in the tree and assume that the resulting joint probability table PMiMj = PMiP ⊤ Mj\n+ ∆ij has full rank. Furthermore, we assume that the resulting joint probability table PHG of the two end points in a path H −M1 −M2 · · ·Ml −G also\nhas full rank. We have\nPHG = PH|M1PM1|M2 · · ·PMl|GPG = PHM1 diag(PM1) −1PM1M2 diag(PM2) −1 · · · diag(PMl)−1PMlG\n= (PHP ⊤ M1 +∆1) diag(PM1) −1(PM1P ⊤ M2 +∆2) diag(PM2) −1 · · · diag(PMl)−1(PMlP⊤G +∆l) = PHP ⊤ M1 diag(PM1) −1PM1P ⊤ M2 diag(PM2) −1 · · · diag(PMl)−1PMlP⊤G\n+ 0 (terms not involving all the ∆s will all be zero)\n+ ∆1 diag(PM1) −1∆2 diag(PM2) −1 · · · diag(PMl)−1∆l = PHP ⊤ G +∆1 diag(PM1) −1∆2 diag(PM2) −1 · · · diag(PMl)−1∆l . (29)\nThe reason why we do not need to perturb the term diag(PMi) −1 is that if P̃Mi is the perturbed PMi , P̃Mi = P̃MiMj 1 = (PMiP ⊤ Mj +∆ij)1 = PMiP ⊤ MJ 1+ 0 = PMi ,\nsince ∆ij 1 = 0. And the reason why terms not involving all the ∆s will all be zero is that such terms contain either 1⊤∆ = 0⊤ or ∆1 = 0.\nNow, from (29) it follows that the perturbation corresponding to the path H−M1−M2−· · ·− Ml −G is\n∆ := ∆1 diag(PM1) −1∆2 diag(PM2) −1 · · · diag(PMl)−1∆l. (30) Bounding the perturbation on the path. We still need to show under which condition ∆ from (30) will satisfy ‖∆‖F ≤ δ. Assume that the smallest entry in a marginal distribution of an internal node is bounded from below by γmin, i.e.,\nγmin := min hidden node H min i PH(i) .\nThen we have\n‖∆‖F = ∥∥∆1 diag(PM1)−1∆2 diag(PM2)−1 · · ·∆l ∥∥ F\n≤ ∥∥∆1 diag(PM1)−1 ∥∥ F ∥∥∆2 diag(PM2)−1 ∥∥ F · · · ‖∆l‖F ≤ δ l\nγl−1min .\nThe perturbation ∆ on the path H −M1 −M2 · · ·Ml −G is bounded by δ if δ l γl−1min ≤ δ, i.e., if\nδ ≤ γmin. (31)\nFrom (28) and (31) we arrive at the condition for successful quartet test for all quartets\nδ ≤ min {\nθmin k2 + k , γmin\n} .\nIntuitively, it means that the size of the perturbation δ away from independence can not be too large. In particular, it has to be small compared to the smallest marginal probability γmin of a hidden state; it also has to be small compared to the smallest excessive dependence θmin."
    }, {
      "heading" : "13 Statistical Guarantee for the Quartet Test",
      "text" : "Based on the concentration result for nuclear norm in (11), we have that, given m samples, the probability that the finite sample nuclear norm deviates from its true quantity by ǫ := 2 √ 2τ√ m is bounded\nP { ‖Â‖∗ ≥ ‖A‖∗ + ǫ } ≤ 2e−mǫ 2 8 and P { ‖B̂‖∗ ≤ ‖B‖∗ − ǫ } ≤ 2e−mǫ 2 8 , (32)\nwhere we have used τ = mǫ 2\n8 . Now we can derive the probability of making an error for individual quartet test. First, let q = {{i1, i2}, {i3, i4}} and\nα = min {‖B(q)‖∗ − ‖A(q)‖∗, ‖C(q)‖∗ − ‖A(q)‖∗} .\nThen, for sufficiently large m, we can bound the error probability by\nP {Quartet test returns incorrect result}\n= P { ‖Â‖∗ ≥ ‖B̂‖∗ or ‖Â‖∗ ≥ ‖Ĉ‖∗ } ≤ P { ‖Â‖∗ ≥ ‖B̂‖∗ } + P { ‖Â‖∗ ≥ ‖Ĉ‖∗ } (union bound) = P { ‖Â‖∗ − ‖A‖∗ + ‖B‖∗ − ‖B̂‖∗ ≥ ‖B‖∗ − ‖A‖∗ }\n+ P { ‖Â‖∗ − ‖A‖∗ + ‖C‖∗ − ‖Ĉ‖∗ ≥ ‖C‖∗ − ‖A‖∗ }\n≤ P { ‖Â‖∗ − ‖A‖∗ ≥\n‖B‖∗ − ‖A‖∗ 2\n} + P { ‖B‖∗ − ‖B̂‖∗ ≥\n‖B‖∗ − ‖A‖∗ 2\n}\n+ P { ‖Â‖∗ − ‖A‖∗ ≥\n‖C‖∗ − ‖A‖∗ 2\n} + P { ‖C‖∗ − ‖Ĉ‖∗ ≥\n‖C‖∗ − ‖A‖∗ 2\n}\n≤ P { ‖Â‖∗ − ‖A‖∗ ≥ α\n2\n} + P { ‖B‖∗ − ‖B̂‖∗ ≥ α\n2\n}\n+ P { ‖Â‖∗ − ‖A‖∗ ≥ α\n2\n} + P { ‖C‖∗ − ‖Ĉ‖∗ ≥ α\n2\n}\n≤ 8e−mα 2 32"
    }, {
      "heading" : "14 Statistical Guarantee for the Tree Building Algorithm",
      "text" : "Let αq = min {‖B(q)‖∗ − ‖A(q)‖∗, ‖C(q)‖∗ − ‖A(q)‖∗}. We define\nαmin = min quartet q αq.\nFor a latent tree with d observed variables, the tree building algorithm described in the paper requires O(d log d) calls to the quartet test procedure. The probability that the tree is constructed incorrectly is bounded by the probability that either one of these quartet tests returns incorrect\nresult. That is\nP {The latent tree is constructed incorrectly} ≤ P {Either one of the O(d log d) quartet tests returns incorrect result} ≤ c · d log d · P {quartet test returns incorrect result} (union bound)\n≤ 8c · d log d · e−mα 2 32 ,\nwhich implies that the probability of constructing the tree incorrectly decreases exponentially fast as we increase the number of samples m."
    } ],
    "references" : [ {
      "title" : "The identifiability of tree topology for phylogenetic models, including covarion and mixture models",
      "author" : [ "E.S. Allman", "J.A. Rhodes" ],
      "venue" : "Journal of Computational Biology,",
      "citeRegEx" : "Allman and Rhodes,? \\Q2006\\E",
      "shortCiteRegEx" : "Allman and Rhodes",
      "year" : 2006
    }, {
      "title" : "Spectral methods for learning multivariate latent tree structure",
      "author" : [ "A. Anandkumar", "K. Chaudhuri", "D. Hsu", "S. Kakade", "L. Song", "T. Zhang" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2011
    }, {
      "title" : "The recovery of trees from measures of dissimilarity",
      "author" : [ "P. Buneman" ],
      "venue" : "Mathematics in the archaeological and historical sciences,",
      "citeRegEx" : "Buneman,? \\Q1971\\E",
      "shortCiteRegEx" : "Buneman",
      "year" : 1971
    }, {
      "title" : "Analysis of individual differences in multidimensional scaling via an N-way generalization of “Eckart-Young",
      "author" : [ "J. Carroll", "J. Chang" ],
      "venue" : "decomposition. Psychometrika,",
      "citeRegEx" : "Carroll and Chang,? \\Q1970\\E",
      "shortCiteRegEx" : "Carroll and Chang",
      "year" : 1970
    }, {
      "title" : "Learning latent tree graphical models",
      "author" : [ "M. Choi", "V. Tan", "A. Anandkumar", "A. Willsky" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Choi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2011
    }, {
      "title" : "Approximating discrete probability distributions with dependence trees",
      "author" : [ "C. Chow", "C. Liu" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Chow and Liu,? \\Q1968\\E",
      "shortCiteRegEx" : "Chow and Liu",
      "year" : 1968
    }, {
      "title" : "A few logs suffice to build (almost) all trees: Part II",
      "author" : [ "P.L. Erdös", "L.A. Székely", "M.A. Steel", "T.J. Warnow" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Erdös et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Erdös et al\\.",
      "year" : 1999
    }, {
      "title" : "Tree construction using singular value decomposition",
      "author" : [ "N. Eriksson" ],
      "venue" : null,
      "citeRegEx" : "Eriksson,? \\Q2005\\E",
      "shortCiteRegEx" : "Eriksson",
      "year" : 2005
    }, {
      "title" : "A rank minimization heuristic with application to minimum order system approximation",
      "author" : [ "Fazel", "Maryam", "Hindi", "Haitham", "Boyd", "Stephen P" ],
      "venue" : "In American Control Conference,",
      "citeRegEx" : "Fazel et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Fazel et al\\.",
      "year" : 2001
    }, {
      "title" : "Measuring statistical dependence with",
      "author" : [ "A. 2010. Gretton", "O. Bousquet", "A.J. Smola", "B. Schölkopf" ],
      "venue" : null,
      "citeRegEx" : "Gretton et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2010
    }, {
      "title" : "Kernel methods for",
      "author" : [ "A. Gretton", "R. Herbrich", "A.J. Smola", "O. Bousquet", "B. Schölkopf" ],
      "venue" : "International Conference on Algorithmic Learning Theory, pp. 63–77",
      "citeRegEx" : "Gretton et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2005
    }, {
      "title" : "Foundations of the PARAFAC procedure: Model and conditions for an “explana",
      "author" : [ "A. R" ],
      "venue" : "Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "R.,? \\Q2010\\E",
      "shortCiteRegEx" : "R.",
      "year" : 2010
    }, {
      "title" : "tory” multi-mode factor analysis",
      "author" : [ "K. A", "Z. Ghahramani" ],
      "venue" : "UCLA Working Papers in Phonetics,",
      "citeRegEx" : "A. and Ghahramani,? \\Q1970\\E",
      "shortCiteRegEx" : "A. and Ghahramani",
      "year" : 1970
    }, {
      "title" : "Reconstructing evolutionary trees from dna and protein sequences: paralinear distances",
      "author" : [ "J.A. Lake" ],
      "venue" : "Conference on Machine Learning,",
      "citeRegEx" : "Lake,? \\Q2005\\E",
      "shortCiteRegEx" : "Lake",
      "year" : 2005
    }, {
      "title" : "Structuring causal trees",
      "author" : [ "Kaufman", "J. 1988. Pearl", "M. Tarsi" ],
      "venue" : "Journal of Complexity,",
      "citeRegEx" : "Kaufman et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Kaufman et al\\.",
      "year" : 1986
    }, {
      "title" : "On learning with integral operators",
      "author" : [ "L. Rosasco", "M. Belkin", "E.D. Vito" ],
      "venue" : "Journal of Machine",
      "citeRegEx" : "Rosasco et al\\.,? \\Q1981\\E",
      "shortCiteRegEx" : "Rosasco et al\\.",
      "year" : 1981
    }, {
      "title" : "The neighbor-joining method: a new method for reconstructing phylogenetic",
      "author" : [ "M. Nei" ],
      "venue" : "Learning Research,",
      "citeRegEx" : "N. and Nei,? \\Q2010\\E",
      "shortCiteRegEx" : "N. and Nei",
      "year" : 2010
    }, {
      "title" : "Bayesian agglomerative clustering with coalescents",
      "author" : [ "Teh", "Yee Whye", "Daume", "Hal", "Roy", "Daniel" ],
      "venue" : "trees. Molecular Biology and Evolution,",
      "citeRegEx" : "Teh et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 1987
    }, {
      "title" : "Hierarchical latent class models for cluster analysis",
      "author" : [ "L. N" ],
      "venue" : "Neural Information Processing Systems",
      "citeRegEx" : "N.,? \\Q2008\\E",
      "shortCiteRegEx" : "N.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Another family of provably consistent reconstruction methods is the quartet-based methods (Semple & Steel, 2003; Erdös et al., 1999).",
      "startOffset" : 90,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "Recently, (Anandkumar et al., 2011) proposed a quartet test using the leading k singular values of the joint probability table, where k is the number of hidden states.",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "Similar observations have been reported in the phylogenetic community (Eriksson, 2005; Allman & Rhodes, 2006), but they are concerned about the cases where the number of hidden states is larger or equal to the number of observed states.",
      "startOffset" : 70,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "Recently, (Anandkumar et al., 2011) proposed a quartet test using the leading k singular values of the joint probability table, where k is the number of hidden states. This new approach allows k to be different from the number of the observed states. However, it still requires k to be given in advance. Our goal is to design a latent structure discovery algorithm which is agnostic to the number of hidden states, since in practice we rarely know this number. The proposed approach is quartet based, where the quartet relations are resolved based on rank properties of 4th order tensors associated with the joint probability tables of quartets. The key insight is that rank properties of the tensor reveal the latent structure behind a quartet. Similar observations have been reported in the phylogenetic community (Eriksson, 2005; Allman & Rhodes, 2006), but they are concerned about the cases where the number of hidden states is larger or equal to the number of observed states. We focus instead on the cases where the number of hidden states is smaller, representing simpler factors. Furthermore, if the joint probability tensor is only approximately given (due to sampling noise) the main rank condition has to be modified. In Allman & Rhodes (2006) such condition is missing and in Eriksson (2005) the condition is heuristically translated to the distance of a matrix to its best rank-k approximation.",
      "startOffset" : 11,
      "endOffset" : 1256
    }, {
      "referenceID" : 1,
      "context" : "Recently, (Anandkumar et al., 2011) proposed a quartet test using the leading k singular values of the joint probability table, where k is the number of hidden states. This new approach allows k to be different from the number of the observed states. However, it still requires k to be given in advance. Our goal is to design a latent structure discovery algorithm which is agnostic to the number of hidden states, since in practice we rarely know this number. The proposed approach is quartet based, where the quartet relations are resolved based on rank properties of 4th order tensors associated with the joint probability tables of quartets. The key insight is that rank properties of the tensor reveal the latent structure behind a quartet. Similar observations have been reported in the phylogenetic community (Eriksson, 2005; Allman & Rhodes, 2006), but they are concerned about the cases where the number of hidden states is larger or equal to the number of observed states. We focus instead on the cases where the number of hidden states is smaller, representing simpler factors. Furthermore, if the joint probability tensor is only approximately given (due to sampling noise) the main rank condition has to be modified. In Allman & Rhodes (2006) such condition is missing and in Eriksson (2005) the condition is heuristically translated to the distance of a matrix to its best rank-k approximation.",
      "startOffset" : 11,
      "endOffset" : 1305
    }, {
      "referenceID" : 2,
      "context" : "mapping between quartets and the tree topology T is captured in the following theorem (Buneman, 1971):",
      "startOffset" : 86,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "This is an important advantage over the idea of learning the structure based on additive distance (Lake, 1994), where k is assumed to be the same as the number of states, n, of the observed variables, or over a recent approach based on quartet test (Anandkumar et al., 2011), where k needs to be specified in advance.",
      "startOffset" : 249,
      "endOffset" : 274
    }, {
      "referenceID" : 8,
      "context" : "First, it is the tightest convex lower bound of the rank of a matrix (Fazel et al., 2001).",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "(Erdös et al., 1999).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "(2011) (Spectral@k), the Chow-Liu neighbor Joining algorithm (CLNJ) (Choi et al., 2011), and an algorithm of Harmeling & Williams (2010) (HW).",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "Spectral@k uses singular values of Pij to design a quartet test (Anandkumar et al., 2011).",
      "startOffset" : 64,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "We compared our algorithm with representative algorithms: the neighbor-joining algorithm (NJ) (Saitou & Nei, 1987), a quartet based algorithm of Anandkumar et al. (2011) (Spectral@k), the Chow-Liu neighbor Joining algorithm (CLNJ) (Choi et al.",
      "startOffset" : 145,
      "endOffset" : 170
    }, {
      "referenceID" : 1,
      "context" : "We compared our algorithm with representative algorithms: the neighbor-joining algorithm (NJ) (Saitou & Nei, 1987), a quartet based algorithm of Anandkumar et al. (2011) (Spectral@k), the Chow-Liu neighbor Joining algorithm (CLNJ) (Choi et al., 2011), and an algorithm of Harmeling & Williams (2010) (HW).",
      "startOffset" : 145,
      "endOffset" : 300
    } ],
    "year" : 2012,
    "abstractText" : "Discovering the latent structure from many observed variables is an important yet challenging learning task. Existing approaches for discovering latent structures often require the unknown number of hidden states as an input. In this paper, we propose a quartet based approach which is agnostic to this number. The key contribution is a novel rank characterization of the tensor associated with the marginal distribution of a quartet. This characterization allows us to design a nuclear norm based test for resolving quartet relations. We then use the quartet test as a subroutine in a divide-and-conquer algorithm for recovering the latent tree structure. Under mild conditions, the algorithm is consistent and its error probability decays exponentially with increasing sample size. We demonstrate that the proposed approach compares favorably to alternatives. In a real world stock dataset, it also discovers meaningful groupings of variables, and produces a model that fits the data better.",
    "creator" : "LaTeX with hyperref package"
  }
}