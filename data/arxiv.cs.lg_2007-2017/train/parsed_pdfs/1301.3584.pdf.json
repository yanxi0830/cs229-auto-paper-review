{
  "name" : "1301.3584.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 1.\n35 84\nv1 [\ncs .L\nG ]\n1 6\nJa n"
    }, {
      "heading" : "1 Introduction",
      "text" : "Several recent papers tried to address the issue of using better optimization techniques for machine learning, especially for training deep architectures or neural networks of various kinds. Hessian-Free optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms. They usually can be split in two different categories according to their theoretical standpoint: those that make use of second-order information and those that use the geometry of the underlying parameter manifold (natural gradient). Within each group they further differentiate themselves by the approximations they use in order to scale up.\nOne particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012). The core idea behind it is to make use of the forward (renamed to R-operator in Pearlmutter (1994)) and backward pass of automatic differentiation to compute efficiently products between Jacobian or Hessian matrices and vectors. The advantage of this pipeline is that it considers the exact Hessian and only inverts it approximately without explicitly storing the matrix in memory, instead of other approaches that do a more crude approximation of the Hessian (or Fisher) matrix (either diagonal or block-diagonal).\nOur contribution is two-fold. We first show that the Hessian-Free approach of Martens (2010) 1 (and implicitly Krylov Subspace Descent (KSD) algorithm) can be casted into into the framework of natural gradient, showing how these methods could be seen as doing natural gradient rather then second-order optimization. The second contribution of the paper is to look at the Natural Gradient Descent as a learning algorithm and suggest how it contrasts to a typical second order optimization technique. We also provide a detailed derivation of Natural Gradient that aims at eliminating possible confusion regarding the different formulation of the algorithm that are found in the literature.\nThe organization of the paper is as follows. In section 2 we describe the basic concepts behind Natural Gradient Descent. Section 4 talks about the different variants of the algorithm, while section 5 describes the relationship between Hessian-Free optimization and Natural Gradient. Section 6\n1We use the wording “Hessian Free“ to refer to the particular algorithm proposed by Martens (2010), and this should not be confused with the more generic truncated gradient method (Nocedal and Wright, 2000).\nprovides a generic discussion about Natural Gradient Descent, and finally we summarize and draw a few final conclusions in section 7."
    }, {
      "heading" : "2 Natural Gradient",
      "text" : "Natural Gradient can be traced back to Amari’s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; ichi Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000). Le Roux et al. (2007) introduces a different formulation of the algorithm, which even though is called the same, is motivated differently and it is not equivalent with Amari’s version. We will differ this discussion to section 4.\nThe main insight of Natural Gradient is that the model describes a family of density functions parametrized by θ, each value of θ corresponding to a particular member of this family. This family of density functions defines a lower-dimensional Riemannian manifold in RP (where P is the dimensionality of θ) that can be traced out by changing θ. Specifically there are sets of values of θ that result in the same density function (for example due to symmetries in the parametrization) which get map to the same point on the manifold. This surjection defines a lower-dimensional manifold (think of collapsing all θ values that result in the same p to a single point). θ behaves like a coordinate on the manifold and the manifold is curved. Natural Gradient aims at moving along the manifold, i.e. in the functional space, instead of moving in the embedding space RP (which is what normal Gradient Descent does).\nTo gain a better understanding, let us be more precise and consider a family of density functions F : RP → (RN → [0, 1]) over the random variable z ∈ RN parametrized by θ ∈ RP and the loss function that we want to minimize L : RP → R. Any choice of θ ∈ RP defines a particular density function p(z|θ) and by considering all possible θ values, we explore the subset F , which is our low-dimensional manifold. Because we can define a similarity measures between density functions, given by the KL divergence which behaves like a distance measure, we are dealing with a Riemannian manifold whose metric is given by the Fisher Information matrix and the gradient on the manifold by equation (1).\n∇NL(θ) = ∂L(θ)\n∂θ Ez\n[\n(\n∂ log p(z|θ)\n∂θ\n)T ( ∂ log p(z|θ)\n∂θ\n)\n]−1\n= ∂L(θ)\n∂θ G−1 (1)\nTo obtain this result without relying on information geometry, we consider natural gradient to be defined by equation (2), where we constrain ∆θ such that the KL divergence between p(θ) and p(θ + ∆θ). Using this constrain we ensure that we move along the manifold with constant speed, without begin slowed down by its curvature. Note that by moving along the manifold, Natural Gradient is invariant to the parametrization of p(θ) as the change is constant regardless of how p is parametrized.\nargmin∆θ L(θ +∆θ) s. t. KL(p(θ)||p(θ +∆θ)) = const (2)\nAssuming ∆θ → 0, we can approximate the KL divergence by its Taylor series where we drop everything after second order terms as is done in equation (3) and by using the relation\nE\n[\n∂ log p(θ) ∂θ\n]\n= ∂ ∂θ ( ∑ z p(z|θ)) = 0 we are left with only the second term. The Fisher Infor-\nmation Matrix can be obtain from the expected value of the Hessian of log p(z|θ) by noticing that\nEz\n[\n1 p(z|θ) ∂2p(z|θ) ∂θ2\n]\n= ∂ 2 ∂θ2 ( ∑ z p(z|θ)) = 0.\nKL(p(z|θ) ‖ p(z|θ +∆θ)) ≈ ∑\nz\np(θ) log p(θ)− ∑\nz\np(θ)\n[\nlog p(θ) + ∂ log p(θ)\n∂θ ∆θ +\n1 2 ∆θT\n∂2 log p(θ)\n∂θ2 ∆θ\n]\n= 1\n2 ∆θTEz\n[\n− ∂2 log p(θ)\n∂θ2\n]\n∆θ (3)\n= 1\n2 ∆θTEz\n[\n(\n∂ log p(z|θ)\n∂θ\n)T ( ∂ log p(z|θ)\n∂θ\n)\n]\n∆θ (4)\nUsing Lagrange multipliers to solve equation (2), where the KL divergence is approximated by (4) and we approximate L(θ + ∆θ) by its first order Taylor series L(θ) + ∂L(θ)\n∂θ ∆θ we can recover\nequation (1)."
    }, {
      "heading" : "3 Natural Gradient for Neural Networks",
      "text" : "In order to define Natural Gradient for neural networks we need to rely on their probabilistic interpretation (which induces a similarity measure between different parametrization of the model). The family of probability density functions corresponding to some model (as traced by changing the parameters θ) defines a Riemannian manifold whose metric is the Fisher Information Matrix. In what follows we will consider the most typical three output activation functions and the probabilistic interpretation they induce. Note that similar derivations have been done in Park et al. (2000), but we choose to redo them in order to clarify some misconceptions regarding the form Natural Gradient metric which is often assumed to be the uncentered covariance of the gradients.\nWe will use equation (5) to define the loss, where z our random variable becomes z = (x, t), with x being the input to the model and t the target. The probability density function associated to the model becomes the conditional p(t|x, θ), where q(x) describes the data generating distribution of x and q(t|x) is the distribution we want to learn. y is the output of the model, which by an abuse of notation we will use it as either a function (going from the input to the outputs) as well as the vector of output activations. We use lower indices to indicate the i-th element of the function, as for example yi to describe the i-th output unit (either seen as a function or just as a value). r is the output of the model before applying the output activation function σ and ri is the i-th output before applying σ. t(i) and x(i) are the i-th target and input samples of the training set.\nL(θ) = 1\nn\nn ∑\ni\n[ log p(t(i)|y(x(i))) ] = 1\nn\nn ∑\ni\n[ log p(t(i)|σ(r(x(i)))) ]\n(5)\nBecause we have a conditional density function p(x|y) the formulation of natural gradient changes to equation (7) which results in the same formula as before, just that the metric is now an expectation over q(x) of an expectation over p(t|x) (equation (7)). Note that q̃ can be q, case in which we cover the complete functional behaviour of p(t|x) (i.e. for all possible values of x). That is because p(t|x) changes with x. Alternatively however q̃ can be the empirical distribution over a minibatch, which would imply that we care about the behavioral change of p(t|x) over the given minibatch.\nargmin∆θ L(θ +∆θ) s. t. Ex∼q̃(x) [KL(p(t|x, θ)||p(t|x, θ +∆θ))] = const\n(6)\n∇NL(θ) = ∂L(θ)\n∂θ Ex∼q̃(x)\n[\nEt∼p(t|x)\n[\n(\n∂ log p(t|x, θ)\n∂θ\n)T ( ∂ log p(t|x, θ)\n∂θ\n)\n]]−1\n= ∂L(θ)\n∂θ G−1\n(7)"
    }, {
      "heading" : "3.1 Linear activation function",
      "text" : "In the case of linear outputs we assume that each entry of the vector t, ti comes from a Gaussian distribution centered around yi(x) with some standard deviation β. From this it follows that:\np(t|x, θ) = o ∏\ni=1\nN (ti|y(x, θ)i, β 2) (8)\nG = Ex∼q̃\n[\nEt∼N (t|y(x,θ),β2I)\n[\n∑o\ni=1\n(\n∂ log p(ti|y(x,θ)i,β 2\n∂θ\n)T ( ∂ log p(ti|y(x,θ)i,β 2\n∂θ\n)\n]]\n= Ex∼q̃\n[\n∑o\ni=1\n[\nEt∼N (t|y(x,θ),β2I)\n[\n(\n∂(ti−yi) 2\n∂θ\n)T ( ∂(ti−yi) 2\n∂θ\n)\n]]]\n= Ex∼q̃\n[\n∑o\ni=1\n[\nEt∼N (t|y(x,θ),β2I)\n[\n(ti − yi)2 ( ∂yi ∂θ )T ( ∂yi ∂θ )\n]]]\n= Ex∼q̃\n[\n∑o i=1\n[\nEt∼N (t|y(x,θ),βI)\n[ (ti − yi)2 ]\n(\n∂yi ∂θ )T ( ∂yi ∂θ )\n]]\n= Ex∼q̃\n[\n∑o\ni=1\n[\nEt∼N (t|y(x,θ),β2I)\n[ (ti − yi)2 ]\n(\n∂yi ∂θ )T ( ∂yi ∂θ )\n]]\n= Ex∼q̃\n[\n∑o\ni=1\n[\nEt∼N (t|y(x,θ),β2I)\n[ (ti − yi)2 ]\n(\n∂yi ∂θ )T ( ∂yi ∂θ )\n]]\n= β2Ex∼q̃\n[\n∑o i=1\n[\n(\n∂yi ∂θ )T ( ∂yi ∂θ )\n]]\n= β2Ex∼q̃ [ JTyJy ]\n(9)\nWe would use the notation Jy,Jr to define the Jacobian ∂y ∂θ and ∂r ∂θ ."
    }, {
      "heading" : "3.2 Sigmoid activation function",
      "text" : "In the case of the sigmoid units, i,e, y = sigmoid(r), we assume a binomial distribution which gives us:\np(t|x) = ∏\ni\nytii (1 − yi) 1−ti (10)\nlog p gives us the usual cross-entropy error used with sigmoid units. We can compute the Fisher information matrix as follows:\nG = Ex∼q̃\n[\nEt∼p(t|x)\n[\n∑o i=1 (ti−yi)\n2\ny2i (1−yi) 2\n(\n∂yi ∂θ )T ∂yi ∂θ\n]]\n= Ex∼q̃\n[\n∑o i=1 1 yi(1−yi)\n(\n∂yi ∂θ )T ∂yi ∂θ\n]\n= Ex∼q̃\n[\nJTydiag( 1 y(1−y))Jy\n]\n(11)"
    }, {
      "heading" : "3.3 Softmax activation function",
      "text" : "In case of the softmax activation function, y = softmax(r), p(t|x) takes the form of a multinomial:\np(t|x) = o ∏\ni\nytii (12)\nG = Ex∼q̃\n[\no ∑\ni\n1\nyi\n(\n∂yi\n∂θ\n)T ∂yi\n∂θ\n]\n(13)"
    }, {
      "heading" : "4 A different Natural Gradient",
      "text" : "In Le Roux et al. (2007) a different approach is taken to derive Natural Gradient. Specifically one assumes that the gradients computed over different mini-batches are distributed according to a Gaussian centered around the true gradient with some covariance matrix C. By using the uncertainty provided by C we can correct the step that we are taking to maximize the probability of a downward move in generalization error (expected negative log-likelihood), resulting in a formula similar to that of natural gradient (equation (14)).\ng̃ = ∂L(θ) ∂θ C−1\nC = 1 n\n∑\ni\n(\n∂ log p(t(i)|x(i)) ∂θ − 〈 ∂ log p(t(i)|x(i)) ∂θ 〉)T ( ∂ log p(t(i)|x(i)) ∂θ − 〈 ∂ log p(t(i)|x(i)) ∂θ\n〉) (14)\nWhile the probabilistic derivation requires the use of the centered covariance (equation (14), in Le Roux et al. (2007) it is argued that using the uncentered covariance U is equivalent up to a constant resulting in a simplified formula (15) which is usually confused with the metric derived by Amari. The misunderstanding comes from the fact that the equation has the form of an expectation, though the expectation is over q(x, t). Because of the form of the expectation we can not recover KL divergence of equation (6), as we do not do an expectation over the model distribution p but over the empirical distribution q. This means that we can not motivate U as being the metric of the manifold.\nU = 1 n\n∑\ni\n(\n∂ log p(t(i)|x(i)) ∂θ\n)T ( ∂ log p(t(i)|x(i))\n∂θ\n)\n≈ E(x,t)∼q\n[\n(\n∂ log p(t|x) ∂θ\n)T ( ∂ log p(t|x)\n∂θ\n) ] (15)\nNote that if KL(p ‖ q) is small, than U can be seen as an approximation to G but this is not true initially. This shows that the two versions of Natural Gradient are different, one being justified by moving with constant speed along the manifold described by the model distribution, the other one by exploiting the uncertainty in the gradient to make more likely downward moves in generalization error."
    }, {
      "heading" : "5 The extended Gauss-Newton approximation of the Hessian",
      "text" : "Hessian-Free as well as Krylov Subspace Descent rely on the extended Gauss-Newton approximation of the Hessian instead of the actual Hessian. The reason is not computational, as computing both can be done equally fast, but rather better behaviour during learning. This is usually assumed to be caused by the fact that the Gauss-Newton is positive semi-definite by construction, so one has not to worry about negative curvature issues.\nIn this section we show that in fact the extended Gauss-Newton approximation matches perfectly the natural gradient metric, and hence by choosing this specific approximation, one can view both algorithms as being implementations of Natural Gradient rather than typical second order methods.\nTo do so, we follow Schraudolph (2002) and re-write the Hessian as the sum of matrices described in equation (16), where i ranges over the o outputs of the network, and ri denotes the subnetwork that produces the i-th element of r. The extended Gauss-Newton approximation of the Hessian GN is represented by the first term of the equation. One justification for picking this term is that ∂ log p(t(i)|x(i))\n∂ri approaches 0 as L approaches 0 (for the typical pairings of activation functions and\nerror measures), making the approximation exact close to the solution. The first term is also well behaved, being positive semi-definite by construction. For more details see Schraudolph (2002).\nH = 1\nn\n∑\ni\n[\n(\n∂r\n∂θ\n)T ( ∂2 log p(t(i)|x(i))\n∂r2\n)(\n∂r\n∂θ\n)\n+\no ∑\ni=1\n((\n∂ log p(t(i)|x(i))\n∂ri\n)(\n∂2ri\n∂θ\n))\n]\n(16)\nGN = 1\nn\n∑\ni\n[\n(\n∂r\n∂θ\n)T ∂2 log p(t(i)|x(i))\n∂θ2\n(\n∂r\n∂θ\n)\n]\n= Ex∼q̃ [ JTr ( Et∼q̃(t|x) [HL◦r] ) Jr ] (17)\nThe last step of equation (17) is obtained by using the normal assumption that (x(i), t(i)) are i.i.d samples. Our intend is to show that the Gauss-Newton approximation of the Hessian, re-written in equation (17) matches with the Natural Gradient metric for all matching activations and error functions for which it is defined. Specifically it represents an approximation of the metric, where the expectation over q(x) is approximated using training data, which is what is typically done for natural gradient as well.\nFor the linear output units with square errors we can derive the matrix HL◦r following the steps in equation (18). Note that we drop the expectation over q(x) to simplify notation, but we will come back to this point in section 6. The result is summarized in equation 19, where we make use of the fact that r = y, which matches the corresponding natural gradient metric up to a constant.\nHL◦rij,i6=j = ∂2\n∑\nk(rk−tk) 2\n∂ri∂rj\n= ∂2(ri−ti) ∂rj = 0\nHL◦rii = ∂2\n∑\nk(rk−tk) 2\n∂ri∂ri\n= ∂2(ri−ti) ∂ri = 2\n(18)\nGN = 1\nn\n∑\nx(i),t(i)\nJTr HL◦rJr = 1\nn\n∑\nx(i),t(i)\nJTyHL◦yJy = 1\nn\n∑\nx(i)\nJTy (2I)Jy = 2Ex∈q(x) [ JTyJy ]\n(19)\nEquation (20) derives the formula for HL◦r in the case of sigmoid units with cross-entropy error (σ is the sigmoid function). If we insert this back into the Gauss-Newton approximation of the Hessian and re-write the equation in terms of Jy instead of Jr, equation (21), we get, again, the corresponding natural gradient metric.\nHL◦rij,i6=j = ∂2\n∑\nk(−tk log(σ(rk))−(1−tk) log(1−σ(rk))) ∂ri∂rj\n= ∂ ( −ti 1 σ(ri) ∂σ(ri) ∂rj −(1−ti) 1 1−σ(ri) ∂1−σ(ri) ∂ri )\n∂rj\n= ∂ ( −ti 1 σ(ri) σ(ri)(1−σ(ri))+(1−ti) 1 1−σ(ri) σ(ri)(1−σ(ri)) )\n∂rj\n= ∂(−ti+tiσ(ri)+σ(ri))−tiσ(ri)) ∂rj = ∂σ(ri)−ti ∂rj = 0\nHL◦rii = ∂2 ∑ k(−tk log(σ(rk))−(1−tk) log(1−σ(rk))) ∂ri∂ri\n= ... = ∂σ(ri)−ti ∂ri = σ(ri)(1− σ(ri))\n(20)\nGN = 1 n\n∑\nx(i),t(i) J T r HL◦rJr\n= 1 n\n∑\nx(i) J T r diag (y(1 − y))Jr\n= 1 n\n∑\nx(i) J T r diag (y(1 − y)) diag\n(\n1 y(1−y)\n)\ndiag (y(1 − y))Jr\n= 1 n\n∑\nx(i) J T r J T y◦rdiag\n(\n1 y(1−y)\n)\nJy◦rJr\n= Ex∼q̃\n[ JTydiag ( 1 y(1−y) ) Jy ]\n(21)\nThe last matching activation and error function that we consider is the softmax with cross-entropy. The derivation of the Gauss-Newton approximation is given in equation (22).\nHL◦rij,i6=j = ∂2\n∑\nk(−tk log(φ(rk))) ∂ri∂rj\n= ∂ ∑ k − tk φ(rk) ∂φrk) ∂ri\n∂rj\n= ∂ ∑ k(tkφ(ri))−ti ∂rj = ∂φ(ri)−ti ∂rj = −φ(ri)φrj)\nHL◦rii = ∂2\n∑\nk(−tk log(φ(rk))) ∂ri∂ri\n= ... = ∂φ(ri)−ti ∂ri = φ(ri)− φ(ri)φ(ri)\n(22)\nEquation (23) starts from the natural gradient metric and singles out a matrix M in the formula such that the metric can be re-written as the productJTr MJr (similar to the formula for the Gauss-Newton approximation). In (24) we show that indeed M equals HL◦r and hence the natural gradient metric is the same as the extended Gauss-Newton matrix for this case as well. Note that δ is the Kronecker delta, where δij,i6=j = 0 and δii = 1.\nG = Ex∼q̃\n[\n∑o k=1 1 yk\n(\n∂yk ∂θ )T ∂yk ∂θ\n]\nG = Ex∼q̃\n[\nJTr\n(\n∑o\nk=1 1 yk\n(\n∂yk ∂r )T ( ∂yk ∂r )\n)\nJr\n]\nG = Ex∼q̃ [ JTr MJ T r ] G = 1 N ∑ x(i) ( JTr MJ T r )\n(23)\nMij,i6=j = ∑o k=1 1 yk ∂yk ∂ri ∂yk ∂rj\n= ∑o k=1 1 yk yk(δki − yi)yk(δkj − yj) = ∑o\nk=1(δki − yi)yk(δkj − yj) = yiyj ( ∑o k=1 yk)− yiyj − yiyj = yiyj − yiyj − yiyj = −yiyj = −φ(ri)φ(rj)\nMii = ∑o k=1 1 yk ∂yk ∂yi ∂yk ∂rj\n= ∑o\nk=1(δki − yi)yk(δki − yi) = y2i ( ∑o k=1 yk) + yi − 2y 2 i = yi − y2i = φ(ri)− φ(ri)φ(ri)\n(24)"
    }, {
      "heading" : "6 Natural Gradient Descent as a learning algorithm",
      "text" : ""
    }, {
      "heading" : "6.1 Natural Gradient versus Natural Gradient",
      "text" : "The two definitions of the algorithm (using the uncentered covariance of the gradients) versus the manifold metric has different practical implications. While there is no detailed published comparison between the two variants, there are reasons to expect one method to work better than the other.\nOne such reason has to do with numerical stability. One can express G as a sum of n × o outer products (where n is the size of the minibatch over which we estimate the matrix) while U is a sum of only n outer products. Since the number of terms in these sums provides an upper bound on the rank of each matrix, it follows that one could expect that U will be lower rank than G for the same size of the minibatch n.\nThis observation is also made by Schraudolph (2002) to motivate the extended Gauss-Newton matrix as middle ground between Natural Gradient and the true Hessian. Note that Schraudolph (2002)\nassumes a form of natural gradient that uses U as a metric, similar to the what Le Roux et al. (2007) proposed (an assumption made in Martens (2010) as well). In section 5 we showed that the extended Gauss-Newton corresponds in fact to the metric proposed by Amari.\nA second reason is that G ensures that we move with constant speed in the functional space, which means that we should not get stalled near plateaux. Note that plateaux of the error function are intrinsically plateaux of the manifold given our formulation of the error, which only measures log likelihood of p for samples taken from our empirical distribution (t,x) ∈ q. In Park et al. (2000) such plateaux are seen as singularities of the manifold.\nOn the other hand Le Roux’s method is designed to obtain better generalization errors, so both approaches may have different advantages, or both could inherit from each other’s advantages: that has yet to be determined theoretically or experimentally."
    }, {
      "heading" : "6.2 Natural Gradient and Second Order Methods",
      "text" : "Even though Natural Gradient is usually assumed to be a second order method it is more useful, and arguably more correct to think of it as a first order method as we are using the curvature of the manifold and not that of the error function we are trying to minimize. To make this distinction clear we can try to see what information each of the matrices that we invert carry (as it was done in Roux and Fitzgibbon (2010) for Newton’s and Le Roux’s methods).\nThe Hessian answers the question: If I change my parameters θ by a bit, how does the gradient change ? The matrix is used to decide how the gradients change when one moves in the parameter space.\nThe centered or uncentered covariance of the gradients answers the question: If I change my input x by a bit, how does the gradient change? The matrix provides information of how the gradient changes when we move in the input space.\nThe manifold metric answers the question If I change my parameters θ by a bit, how does my probability density function p change ? While this seems similar to the Hessian, the quantities are quite distinct. For one thing we care about the change in p which does not have to correspond to a change in the error or the gradient (the error is just the value of p for certain pairs of samples (x(i), t(i)). The change we measure could happen (if for example the metric is computed over the entire training set or a distinct minibatch) in some other region of space, and would not effect the instantaneous cost that we measure on the minibatch over which we have computed the gradient. In some sense, the metric manifold matches the Hessian, if the Hessian is computed over the same distribution of the input q̃(x) as the metric and all possible value of the targets t. The metric should provide an intuition of how the function changes over its entire domain when we change θ.\nNatural Gradient lends itself very well to the online optimization regime (if one looks at it as a first order method), while second order methods do not. In principle, in order to apply Natural Gradient we need an estimate of the gradient, which can be the stochastic gradient over a single sample and some reliable measure of how our model changes with θ, given by the metric. One good choice is to compute the metric on a held out subset of data, offering this way an unbiased estimate of how p(t|x) changes with θ. Theoretically there is no reason to compute the metric over the same data we compute the gradients (which is what is typically done for second order methods) and it is completely justified to not do so. Given that we do not even need to have targets for the data over which we compute the metric, as G integrates out the random variable t, we could even use unlabeled data as long as it comes from the same distribution q. In case the unlabeled data is noisier than the training set, it is expected that one would require very large batches over which to compute the metric as to reduce the amount of the noise in G.\nBy using such a held out set for computing the metric we stand to gain improvements on the generalization error, in a way that may be related to Le Roux’s algorithm. The reason is simple. Intuitively Natural Gradient moves slowly in some direction d if the metric G indicates that the probability density function p changes quickly as you change θ along d and moves fast otherwise.\nOverfitting of the current minibatch (or even the entire training set) could happen when there is some direction d which is under-represented in the training data, leading the algorithm to believe that moving along d would not cause a large change in the behaviour of p. In such instances, Natural\nGradient Descent makes large steps, and we do not get to explore the different behavioral regimes over which we jump. However if we use extra data besides the training set we might be able to get a better estimate of the metric, which might force us to move slowly along d, capturing the different intermediate steps, which might give better validation errors.\nThis is similar to the intuition behind Le Roux’s version of natural gradient, namely we get to move slower in the direction in which the gradients do not agree and fast where they agree."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "Natural Gradient is a powerful tool for learning complex non-linear functions because while it maintains characterstics of a second order method (like fast convergence), it lends itself easily to the classical learning framework. For example it can be easily used as either an online learning algorithm or a batch algorithm."
    } ],
    "references" : [ {
      "title" : "Differential geometrical methods in statistics",
      "author" : [ "S. Amari" ],
      "venue" : "Lecture notes in statistics, 28.",
      "citeRegEx" : "Amari,? 1985",
      "shortCiteRegEx" : "Amari",
      "year" : 1985
    }, {
      "title" : "Information geometry of Boltzmann machines",
      "author" : [ "S. Amari", "K. Kurata", "H. Nagaoka" ],
      "venue" : "IEEE Trans. on Neural Networks, 3, 260–271.",
      "citeRegEx" : "Amari et al\\.,? 1992",
      "shortCiteRegEx" : "Amari et al\\.",
      "year" : 1992
    }, {
      "title" : "Natural gradient works efficiently in learning",
      "author" : [ "Amari", "S.-I." ],
      "venue" : "Neural Comput., 10(2), 251–276.",
      "citeRegEx" : "Amari and S..I.,? 1998",
      "shortCiteRegEx" : "Amari and S..I.",
      "year" : 1998
    }, {
      "title" : "Improved Preconditioner for Hessian Free Optimization",
      "author" : [ "O. Chapelle", "D. Erhan" ],
      "venue" : "NIPS Workshop on Deep Learning and Unsupervised Feature Learning.",
      "citeRegEx" : "Chapelle and Erhan,? 2011",
      "shortCiteRegEx" : "Chapelle and Erhan",
      "year" : 2011
    }, {
      "title" : "Neural learning in structured parameter spaces - natural Riemannian gradient",
      "author" : [ "S. ichi Amari" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Amari,? \\Q1997\\E",
      "shortCiteRegEx" : "Amari",
      "year" : 1997
    }, {
      "title" : "Topmoumoute online natural gradient algorithm",
      "author" : [ "N. Le Roux", "Manzagol", "P.-A.", "Y. Bengio" ],
      "venue" : "Technical Report 1299, Département d’informatique et recherche opérationnelle, Université de Montréal.",
      "citeRegEx" : "Roux et al\\.,? 2007",
      "shortCiteRegEx" : "Roux et al\\.",
      "year" : 2007
    }, {
      "title" : "Topmoumoute online natural gradient algorithm",
      "author" : [ "N. Le Roux", "Manzagol", "P.-A.", "Y. Bengio" ],
      "venue" : "NIPS’07.",
      "citeRegEx" : "Roux et al\\.,? 2008",
      "shortCiteRegEx" : "Roux et al\\.",
      "year" : 2008
    }, {
      "title" : "Improving first and second-order methods by modeling uncertainty",
      "author" : [ "N. Le Roux", "Y. Bengio", "A. Fitzgibbon" ],
      "venue" : "Optimization for Machine Learning. MIT Press.",
      "citeRegEx" : "Roux et al\\.,? 2011",
      "shortCiteRegEx" : "Roux et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep learning via hessian-free optimization",
      "author" : [ "J. Martens" ],
      "venue" : "ICML, pages 735–742.",
      "citeRegEx" : "Martens,? 2010",
      "shortCiteRegEx" : "Martens",
      "year" : 2010
    }, {
      "title" : "Numerical Optimization",
      "author" : [ "J. Nocedal", "S.J. Wright" ],
      "venue" : "Springer.",
      "citeRegEx" : "Nocedal and Wright,? 2000",
      "shortCiteRegEx" : "Nocedal and Wright",
      "year" : 2000
    }, {
      "title" : "Adaptive natural gradient learning algorithms for various stochastic models",
      "author" : [ "H. Park", "Amari", "S.-I.", "K. Fukumizu" ],
      "venue" : "Neural Networks, 13(7), 755 – 764.",
      "citeRegEx" : "Park et al\\.,? 2000",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2000
    }, {
      "title" : "Fast exact multiplication by the hessian",
      "author" : [ "B.A. Pearlmutter" ],
      "venue" : "Neural Computation, 6, 147–160.",
      "citeRegEx" : "Pearlmutter,? 1994",
      "shortCiteRegEx" : "Pearlmutter",
      "year" : 1994
    }, {
      "title" : "A fast natural newton method",
      "author" : [ "N.L. Roux", "A.W. Fitzgibbon" ],
      "venue" : "J. Fürnkranz and T. Joachims, editors, ICML, pages 623–630. Omnipress.",
      "citeRegEx" : "Roux and Fitzgibbon,? 2010",
      "shortCiteRegEx" : "Roux and Fitzgibbon",
      "year" : 2010
    }, {
      "title" : "Fast curvature matrix-vector products",
      "author" : [ "N.N. Schraudolph" ],
      "venue" : "ICANN, pages 19–26.",
      "citeRegEx" : "Schraudolph,? 2001",
      "shortCiteRegEx" : "Schraudolph",
      "year" : 2001
    }, {
      "title" : "Fast curvature matrix-vector products for second-order gradient descent",
      "author" : [ "N.N. Schraudolph" ],
      "venue" : "Neural Computation, 14(7), 1723–1738.",
      "citeRegEx" : "Schraudolph,? 2002",
      "shortCiteRegEx" : "Schraudolph",
      "year" : 2002
    }, {
      "title" : "Generating text with recurrent neural networks",
      "author" : [ "I. Sutskever", "J. Martens", "G.E. Hinton" ],
      "venue" : "ICML, pages 1017–1024.",
      "citeRegEx" : "Sutskever et al\\.,? 2011",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2011
    }, {
      "title" : "Krylov Subspace Descent for Deep Learning",
      "author" : [ "O. Vinyals", "D. Povey" ],
      "venue" : "AISTATS.",
      "citeRegEx" : "Vinyals and Povey,? 2012",
      "shortCiteRegEx" : "Vinyals and Povey",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian.",
      "startOffset" : 55,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : "First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian.",
      "startOffset" : 99,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "Hessian-Free optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al.",
      "startOffset" : 26,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "Hessian-Free optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al.",
      "startOffset" : 26,
      "endOffset" : 91
    }, {
      "referenceID" : 3,
      "context" : "Hessian-Free optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al.",
      "startOffset" : 26,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al.",
      "startOffset" : 59,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms.",
      "startOffset" : 111,
      "endOffset" : 192
    }, {
      "referenceID" : 8,
      "context" : "One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012).",
      "startOffset" : 210,
      "endOffset" : 225
    }, {
      "referenceID" : 16,
      "context" : "One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012).",
      "startOffset" : 254,
      "endOffset" : 279
    }, {
      "referenceID" : 9,
      "context" : "Section 6 We use the wording “Hessian Free“ to refer to the particular algorithm proposed by Martens (2010), and this should not be confused with the more generic truncated gradient method (Nocedal and Wright, 2000).",
      "startOffset" : 189,
      "endOffset" : 215
    }, {
      "referenceID" : 0,
      "context" : ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms. They usually can be split in two different categories according to their theoretical standpoint: those that make use of second-order information and those that use the geometry of the underlying parameter manifold (natural gradient). Within each group they further differentiate themselves by the approximations they use in order to scale up. One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012).",
      "startOffset" : 117,
      "endOffset" : 700
    }, {
      "referenceID" : 0,
      "context" : ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms. They usually can be split in two different categories according to their theoretical standpoint: those that make use of second-order information and those that use the geometry of the underlying parameter manifold (natural gradient). Within each group they further differentiate themselves by the approximations they use in order to scale up. One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012).",
      "startOffset" : 117,
      "endOffset" : 733
    }, {
      "referenceID" : 0,
      "context" : ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms. They usually can be split in two different categories according to their theoretical standpoint: those that make use of second-order information and those that use the geometry of the underlying parameter manifold (natural gradient). Within each group they further differentiate themselves by the approximations they use in order to scale up. One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012). The core idea behind it is to make use of the forward (renamed to R-operator in Pearlmutter (1994)) and backward pass of automatic differentiation to compute efficiently products between Jacobian or Hessian matrices and vectors.",
      "startOffset" : 117,
      "endOffset" : 969
    }, {
      "referenceID" : 0,
      "context" : ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms. They usually can be split in two different categories according to their theoretical standpoint: those that make use of second-order information and those that use the geometry of the underlying parameter manifold (natural gradient). Within each group they further differentiate themselves by the approximations they use in order to scale up. One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012). The core idea behind it is to make use of the forward (renamed to R-operator in Pearlmutter (1994)) and backward pass of automatic differentiation to compute efficiently products between Jacobian or Hessian matrices and vectors. The advantage of this pipeline is that it considers the exact Hessian and only inverts it approximately without explicitly storing the matrix in memory, instead of other approaches that do a more crude approximation of the Hessian (or Fisher) matrix (either diagonal or block-diagonal). Our contribution is two-fold. We first show that the Hessian-Free approach of Martens (2010) 1 (and implicitly Krylov Subspace Descent (KSD) algorithm) can be casted into into the framework of natural gradient, showing how these methods could be seen as doing natural gradient rather then second-order optimization.",
      "startOffset" : 117,
      "endOffset" : 1479
    }, {
      "referenceID" : 0,
      "context" : ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms. They usually can be split in two different categories according to their theoretical standpoint: those that make use of second-order information and those that use the geometry of the underlying parameter manifold (natural gradient). Within each group they further differentiate themselves by the approximations they use in order to scale up. One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012). The core idea behind it is to make use of the forward (renamed to R-operator in Pearlmutter (1994)) and backward pass of automatic differentiation to compute efficiently products between Jacobian or Hessian matrices and vectors. The advantage of this pipeline is that it considers the exact Hessian and only inverts it approximately without explicitly storing the matrix in memory, instead of other approaches that do a more crude approximation of the Hessian (or Fisher) matrix (either diagonal or block-diagonal). Our contribution is two-fold. We first show that the Hessian-Free approach of Martens (2010) 1 (and implicitly Krylov Subspace Descent (KSD) algorithm) can be casted into into the framework of natural gradient, showing how these methods could be seen as doing natural gradient rather then second-order optimization. The second contribution of the paper is to look at the Natural Gradient Descent as a learning algorithm and suggest how it contrasts to a typical second order optimization technique. We also provide a detailed derivation of Natural Gradient that aims at eliminating possible confusion regarding the different formulation of the algorithm that are found in the literature. The organization of the paper is as follows. In section 2 we describe the basic concepts behind Natural Gradient Descent. Section 4 talks about the different variants of the algorithm, while section 5 describes the relationship between Hessian-Free optimization and Natural Gradient. Section 6 We use the wording “Hessian Free“ to refer to the particular algorithm proposed by Martens (2010), and this should not be confused with the more generic truncated gradient method (Nocedal and Wright, 2000).",
      "startOffset" : 117,
      "endOffset" : 2466
    }, {
      "referenceID" : 0,
      "context" : "2 Natural Gradient Natural Gradient can be traced back to Amari’s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al.",
      "startOffset" : 95,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "2 Natural Gradient Natural Gradient can be traced back to Amari’s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; ichi Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al.",
      "startOffset" : 156,
      "endOffset" : 194
    }, {
      "referenceID" : 0,
      "context" : "2 Natural Gradient Natural Gradient can be traced back to Amari’s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; ichi Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al.",
      "startOffset" : 58,
      "endOffset" : 261
    }, {
      "referenceID" : 0,
      "context" : "2 Natural Gradient Natural Gradient can be traced back to Amari’s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; ichi Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000). Le Roux et al.",
      "startOffset" : 58,
      "endOffset" : 281
    }, {
      "referenceID" : 0,
      "context" : "2 Natural Gradient Natural Gradient can be traced back to Amari’s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; ichi Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000). Le Roux et al. (2007) introduces a different formulation of the algorithm, which even though is called the same, is motivated differently and it is not equivalent with Amari’s version.",
      "startOffset" : 58,
      "endOffset" : 304
    }, {
      "referenceID" : 10,
      "context" : "Note that similar derivations have been done in Park et al. (2000), but we choose to redo them in order to clarify some misconceptions regarding the form Natural Gradient metric which is often assumed to be the uncentered covariance of the gradients.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 5,
      "context" : "4 A different Natural Gradient In Le Roux et al. (2007) a different approach is taken to derive Natural Gradient.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "i ( ∂ log p(t|x) ∂θ − 〈 ∂ log p(t|x) ∂θ T ( ∂ log p(t|x) ∂θ − 〈 ∂ log p(t|x) ∂θ 〉) (14) While the probabilistic derivation requires the use of the centered covariance (equation (14), in Le Roux et al. (2007) it is argued that using the uncentered covariance U is equivalent up to a constant resulting in a simplified formula (15) which is usually confused with the metric derived by Amari.",
      "startOffset" : 189,
      "endOffset" : 208
    }, {
      "referenceID" : 13,
      "context" : "To do so, we follow Schraudolph (2002) and re-write the Hessian as the sum of matrices described in equation (16), where i ranges over the o outputs of the network, and ri denotes the subnetwork that produces the i-th element of r.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "To do so, we follow Schraudolph (2002) and re-write the Hessian as the sum of matrices described in equation (16), where i ranges over the o outputs of the network, and ri denotes the subnetwork that produces the i-th element of r. The extended Gauss-Newton approximation of the Hessian GN is represented by the first term of the equation. One justification for picking this term is that ∂ log p(t|x) ∂ri approaches 0 as L approaches 0 (for the typical pairings of activation functions and error measures), making the approximation exact close to the solution. The first term is also well behaved, being positive semi-definite by construction. For more details see Schraudolph (2002).",
      "startOffset" : 20,
      "endOffset" : 684
    }, {
      "referenceID" : 13,
      "context" : "This observation is also made by Schraudolph (2002) to motivate the extended Gauss-Newton matrix as middle ground between Natural Gradient and the true Hessian.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 13,
      "context" : "This observation is also made by Schraudolph (2002) to motivate the extended Gauss-Newton matrix as middle ground between Natural Gradient and the true Hessian. Note that Schraudolph (2002) 7",
      "startOffset" : 33,
      "endOffset" : 190
    } ],
    "year" : 2017,
    "abstractText" : "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.",
    "creator" : "LaTeX with hyperref package"
  }
}