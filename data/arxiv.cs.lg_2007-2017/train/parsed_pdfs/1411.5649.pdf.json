{
  "name" : "1411.5649.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "No-Regret Learnability for Piecewise Linear Losses",
    "authors" : [ "Arthur Flajolet" ],
    "emails" : [ "flajolet@mit.edu", "jaillet@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n56 49\nv5 [\ncs .L\nG ]\n1 7\nSe p\n√ T ) bound on regret for subdifferentiable\nconvex loss functions with bounded subgradients, by using a reduction to linear loss functions. This suggests that linear loss functions tend to be the hardest ones to learn against, regardless of the underlying decision spaces. We investigate this question in a systematic fashion looking at the interplay between the set of possible moves for both the decision maker and the adversarial environment. This allows us to highlight sharp distinctive behaviors about the learnability of piecewise linear loss functions. On the one hand, when the decision set of the decision maker is a polyhedron, we establish Ω( √ T ) lower bounds on regret for a large class of piecewise linear loss functions with important applications in online linear optimization, repeated zero-sum Stackelberg games, online prediction with side information, and online two-stage optimization. On the other hand, we exhibit o( √ T ) learning rates, achieved by the Follow-The-Leader algorithm, in online linear optimization when the boundary of the decision maker’s decision set is curved and when 0 does not lie in the convex hull of the environment’s decision set. Hence, the curvature of the decision maker’s decision set is a determining factor for the optimal learning rate. These results hold in a completely adversarial setting."
    }, {
      "heading" : "1 Introduction",
      "text" : "Online convex optimization has emerged as a popular approach to online learning, bringing together convex optimization methods to tackle problems where repeated decisions need to be made in an unknown, possibly adversarial, environment. A full-information online convex optimization problem is a repeated zero-sum game between a learner (the player) and the environment (the opponent). There are T time periods. At each round t, the player has to choose ft in a convex set F . Subsequent to the choice of ft, the environment reveals zt ∈ Z and the loss incurred to the player is ℓ(zt, ft), for a loss function ℓ that is convex in its second argument. Both players are aware of all the parameters of the game, namely ℓ, Z , and F , prior to starting the game. Additionally, at the end of each period, the opponent’s move is revealed to the player. The performance of the player is measured in terms of a quantity coined regret, defined as the gap between the accumulated losses incurred by the player and the best performance he could have achieved in hindsight with a non-adaptive strategy:\nrT ((zt)t=1,··· ,T , (ft)t=1,··· ,T ) = T ∑\nt=1\nℓ(zt, ft)− inf f∈F\nT ∑\nt=1\nℓ(zt, f).\nIn this field, one of the primary focus is to design algorithms, i.e., strategies to select (ft)t=1,··· ,T so as to keep the regret as small as possible even when facing an adversarial opponent. Particular\nemphasis is placed on how the regret scales with T because this dependence relates to a notion of learning rate. If rT = o(T ), the player is, in some sense, learning the game in the long-run since the gap between experienced and best achievable average cumulative payoffs vanishes as T → ∞. Furthermore, the smaller the growth rate of rT , the faster the learning. A natural question to ask is what is the best learning rate that can be achieved for a given game (ℓ,Z,F). Mathematically, this is equivalent to characterizing the growth rate of the smallest regret that can be achieved by a player against a completely adversarial opponent, expressed as:\nRT (ℓ,Z,F) = inf f1∈F sup z1∈Z · · · inf fT ∈F sup zT∈Z [\nT ∑\nt=1\nℓ(zt, ft)− inf f∈F\nT ∑\nt=1\nℓ(zt, f) ], (1)\nwhich we refer to as the value of the game (ℓ,Z,F). Aside from pure learning considerations, the growth rate of RT (ℓ,Z,F) has important consequences in a variety of fields where no-regret algorithms are used to compute complex quantities, e.g. Nash equilibria in Game Theory [14] or solutions to optimization problems in convex optimization [7], in which case this growth rate translates into the number of iterations required to compute the quantity with a given precision. We investigate this question in a systematic fashion by looking at the interplay between F and Z for the following class of piecewise linear loss functions:\nℓ(z, f) = max x∈X (z) (C(z)f + c(z))Tx, (2)\nwhere, for any z ∈ Z , C(z) is a matrix, c(z) is a vector, and X (z) ⊂ Rd is either a finite set or a polyhedron {x ∈ Rd | A(z)x ≤ b(z)} with A(z) a matrix and b(z) a vector. This type of loss functions arises in a number of important contexts such as online linear optimization, repeated zerosum Stackelberg games, online prediction with side information, and online two-stage optimization, as illustrated in Section 1.1. Throughout the paper, we make the following standard assumption so as to have the game well-defined.\nAssumption 1 Z is a non-empty compact subset of Rdz and F is a non-empty, convex, and compact subset of Rdf . For any choice of z ∈ Z , the set X (z) is not empty. The loss function ℓ is bounded on Z × F . Moreover, either Z has finite cardinality or ℓ(·, f) is continuous for any f ∈ F .\nContributions. A number of no-regret algorithms developed in the literature can be used as a black box for the settings considered in this paper in order to get O( √ T ) bounds on regret, e.g. Exponential Weights [20], Online Gradient Descent [22], and more generally Online Mirror Descent [9], to cite a few. To get better learning rates, other approaches have been proposed but they usually rely on either the curvature of ℓ, for instance if ℓ is strongly convex in its second argument [8], which is not the case here, or more information about the sequence (zt)t=1,··· ,T , see for example [17], which is not available in the fully adversarial setting. Aside from particular instances, e.g. [6] and [1], it is in general unknown how the interplay between ℓ, Z , and F determines the growth rate of RT (ℓ,Z,F). The main insight from this paper is that the curvature of the decision maker’s set of moves is a determining factor for the growth rate of RT (ℓ,Z,F): if F has rough edges then we are doomed to a rate of Θ( √ T ), otherwise, if it is curved, the rate can be exponentially smaller. Specifically, we show that:\n1. When F is a polyhedron, either RT (ℓ,Z,F) = 0 or RT (ℓ,Z,F) = Ω( √ T ). This lower\nbound applies to online combinatorial optimization where F is a combinatorial set, to many experts settings and repeated zero-sum Stackelberg games where the player resorts to a randomized strategy, as well as to many online prediction problems with side information and online two-stage optimization problems.\n2. When (i) ℓ is linear, (ii) F = {f ∈ Rdf | F (f) ≤ 0}, for F either a strongly convex function or F (f) = ‖f‖F −C where C ≥ 0 and ‖ ‖F is a q−uniformly convex norm with q ∈ [2, 3], and (iii) 0 does not lie in the convex hull of Z , we have RT (ℓ,Z,F) = o( √ T ),\nachieved by the Follow-The-Leader algorithm [12]. This result applies to repeated zerosum games where the player picks a cost vector (e.g. arc costs) of bounded euclidean norm and the opponent chooses an element in a combinatorial set (e.g. a path). This also applies to non-linear loss functions when 0 does not lie in the convex hull of the set of subgradients of ℓ with respect to the second-coordinate by a standard reduction to linear loss functions,\nsee [22]. Note that assumption (iii) is required to get o( √ T ) rates as it is well known that RT (ℓ,Z,F) = Ω( √ T ) for linear losses when 0 lies in the interior of the convex hull of Z , see Section 2."
    }, {
      "heading" : "1.1 Applications",
      "text" : "We list examples of situations where losses of the type (2) arise.\nOnline linear optimization In this setting, the loss function is given by ℓ(z, f) = zTf . This includes, in particular:\n• online combinatorial optimization where the opponent picks a cost in [0, 1]dz and F is defined as the convex hull of a finite set of elements (e.g. paths, spanning trees, and matchings),\n• experts settings where the player picks a distribution over the experts’ advice (in which case F is also a polyhedron) and the opponent reveals a cost for each of the experts.\nIn online linear optimization, regret lower bounds are often derived by introducing a randomized zero-mean i.i.d. opponent, see [1]. However, this is possible only if 0 is in interior of the convex hull of Z , which is typically not the case in online combinatorial optimization. A general feature of online linear optimization that will turn out to be important in the analysis is that there is no loss of generality in assuming that Z is a convex set in the following sense.\nLemma 1 When ℓ(z, f) = zTf , the games (ℓ,Z,F) and (ℓ, conv(Z),F) are equivalent, i.e.:\nRT (ℓ,Z,F) = RT (ℓ, conv(Z),F).\nRepeated zero-sum Stackelberg games A repeated zero-sum Stackelberg game is a repeated zero-sum game with the particularity that one of the players, referred to as the leader, has to commit first to a randomized strategy f without even knowing which of the N other players, indexed by z, he is going to face at the next round. The interaction between the leader and player z ∈ {1, · · · , N} is captured by a payoff matrixM(z). Once the leader is set on a strategy, the identity of the other player is revealed and the latter best-responds to the leader’s strategy, leading to the following expression for the loss function:\nℓ(z, f) = max i=1,··· ,Iz eTiM(z)f,\nwhere Iz is the number of possible moves for player z. We illustrate with a network security problem that has applications in urban network security [10] and fare evasion prevention in transit networks [11]. Consider a directed graph G = (V,E). The leader has a limited number of patrols that can be assigned to arcs in order to intercept the attackers. A configuration γ ∈ Γ corresponds to a valid assignment of patrols to arcs and is represented by a vector (Y γij )(i,j)∈E with Y γ ij = 1 if a patrol is assigned to arc (i, j) and Y γij = 0 otherwise. The leader chooses a mixed strategy f over the set of feasible allocations. Attacker z ∈ {(i1, j1), · · · , (iN , jN )} wants to go from z1 to z2 while minimizing the probability of being intercepted. This interaction is captured by the loss function:\nℓ(z, f) = max x∈X (z)\n∑ γ∈Γ −fγxγ ,\nwith: X (z) = {( max\n(i,j)∈E XπijY γ ij )γ∈Γ | π ∈ Π(z)},\nwhere Π(z) is the set of directed paths joining z1 to z2 in G and Xπij = 1 if (i, j) ∈ π and Xπij = 0 otherwise. The presentation of repeated Stackelberg games given here follows the model introduced by Balcan et al. [3] for general, i.e. not necessarily zero-sum, Stackelberg security games. In this more general setting, the loss function may not be convex and a possible approach, see [3], is to add another layer of randomization which casts the problem back into the realm of online linear optimization.\nOnline prediction with side-information This setting has a slightly different flavor as the opponent provides some side information x before the player gets to pick f ∈ F , subsequent to what the opponent reveals the correct prediction y. Nonetheless, the lower bounds established in this paper also apply to this setting through a reduction to the setting without side-information, as detailed at the end of Section 2. In the standard linear binary prediction problem, where F is a L2 ball, y ∈ {−1, 1}, and x lies in a L2 ball, loss functions of the form (2) are commonly used, e.g. the absolute loss ℓ((x, y), f) = |y − xTf | and the hinge loss ℓ((x, y), f) = max(0, 1− yxTf). This is also true for linear multiclass prediction problems with the multiclass hinge loss:\nℓ(f, (x, y)) = max j=1,··· ,N\n(1{j 6= y}+ f Tj x− f Tyx),\nwhere N denotes the number of classes, y ∈ {1, · · · , N}, and f is a vector obtained by concatenation of the vectors f1, · · · , fN . In the online approach to collaborative filtering, a typical loss function is ℓ(M, (i, j, y)) = |M(i, j) − y| where M is a (user, item) matrix with bounded trace norm, (i, j) is a (user, item) pair, and y is the rating of item j by user i.\nOnline two-stage optimization This setting captures situations where the decision making process can be broken down into two consecutive stages. In the first stage, the player makes a decision represented by f ∈ F . Subsequently, the opponent discloses some information z ∈ Z , e.g. a demand vector, and then the player chooses another decision vector x in the second stage, taking into account this newly available information to optimize his objective function. The loss function takes on the following form:\nℓ(z, f) = cT1f + min x∈Rd\nAf+Bx≤z\ncT2x,\nwhere c1 and c2 are cost vectors and A and B are matrices. Using strong duality, this loss function can be expressed in the canonical form (2). This framework finds applications in the operation of power grids, where z represents the demand in electricity or the availability of various energy sources. Since z is unknown when it is time to set up conventional generators, the decision maker has to adjust the production or buy additional capacity from a spot market to meed the demand, see for example [13].\nCongestion control We consider a variant of the congestion network game described in [4]. A decision maker has to decide how to ship a given set commodities through a network G = (V,E). His decision can be equivalently represented by a flow vector f . Because the amount of commodities is assumed to be substantial, implementing f will cause congestion which will impact the other users of the network, represented by a flow vector z. The problem faced by the decision maker is to cause as little delay as possible to the other users with the additional difficulty that the traffic pattern z is not known ahead of time. Each arc e ∈ E has an associated latency function that is convex in the flow on this arc:\nce(f + z) = max k=1,··· ,K\n(cke · (fe + ze) + ske),\nAs a result, the total delay incurred to the other users can be expressed as:\nℓ(z, f) = ∑\ne∈E ze max k=1,··· ,K (cke · (fe + ze) + ske)."
    }, {
      "heading" : "1.2 Related work",
      "text" : "Asymptotically matching lower and upper bounds on RT (ℓ,Z,F) can be found in the literature for a variety of loss functions although the discussion tends to be restrictive as far as the decision sets F and Z are concerned. The value of the game is shown to be Θ(logT ) for three standard examples of curved loss functions. The first example, studied by Abernethy et al. [1], is the quadratic loss where ℓ(z, f) = z · f + σ‖f‖22 for σ > 0, with Z and F bounded L2 balls. The second, studied by Vovk [21], is the online linear regression setting where the opponent plays z = (y, x) ∈ Z = [−Cy, Cy]× B∞(0, 1) for Cy > 0 (B∞(0, 1) denotes the unit L∞ ball), the loss is ℓ((y, x), f) = (y−x·f)2, and F is an L2 ball. The last one, from Ordentlich and Cover [15], is the log-loss ℓ(z, f) = − log z · f with Z any compact set in Rd and F the simplex of dimension d. For non-curved losses, evidence suggests that the value of the game increases exponentially to Θ( √ T ). Indeed,Ω( √ T ) lower bounds are proved for several instances involving the absolute loss ℓ(z, f) = |z − f | in [5], typically with\nZ = {0, 1} and F = [0, 1]. For purely linear loss functions, Abernethy et al. [1] establish a Ω( √ T ) lower bound on RT (ℓ,Z,F) when Z is an L2 ball centered at 0 and F is either an L2 ball or a bounded rectangle. This result was later generalized in [2] and shown to hold for F a unit ball in any norm centered at 0 and Z its dual ball. Cesa-Bianchi and Lugosi [6] investigate the experts setting, i.e. Z = [0, 1]d and F is the simplex in dimension d, and proves the same Ω( √ T ) lower bound (which also holds if Z is the simplex in dimension d, see [2]). Rakhlin et al. [19] establish Ω( √ T ) lower bounds on regret when ℓ is the absolute loss for a prediction with side-information setting more general than the one considered in this paper where the player picks a function f(·), the opponent picks a pair (x, y), and the loss is ℓ(f(·), (x, y)) = |f(x) − y|. The list of results listed above is far from being exhaustive but provides a good picture of the current state of the art. For each loss function, the intrinsic limitations of online algorithms are well-understood, usually with the construction of a particular example of F and Z for which a lower bound on RT (ℓ,Z,F) asymptotically matches the best guarantee achieved by one of these algorithms. We aim at studying lower bounds on the value of the game in a more systematic fashion using tools rooted in duality theory and sensitivity analysis. All the proofs are deferred to the Appendix.\nNotations For a set S ⊂ Rd, conv(S) (resp. int(S)) refers to the convex hull (resp interior) of this set. When S is a compact, we define P(S) as the set of probability measures on S. For x ∈ Rd, ‖x‖ refers to the L2 norm of x while B2(x, ǫ) denotes the closed L2 ball centered at x with width ǫ. For a collection of random variables (Z1, · · · , Zt), σ(Z1, · · · , Zt) refers to the sigma-field generated by Z1, · · · , Zt. For a random variable Z and a probability distribution p, we write Z ∼ p if Z is distributed according to p."
    }, {
      "heading" : "2 Lower bounds",
      "text" : "Unless otherwise stated, we assume throughout this section that ℓ can be written in the form (2). In particular, ℓ(z, ·) is continuous for any z ∈ Z . We build on a powerful result rooted in von Neumann’s minimax theorem that enables the derivation of tight lower and upper bounds on RT (ℓ,Z,F) by recasting the value of the game in a backward order.\nTheorem 1 From [2]\nRT (ℓ,Z,F) = sup p E[\nT ∑\nt=1\ninf ft∈F E[ℓ(Zt, ft)|Z1, · · · , Zt−1]− inf f∈F\nT ∑\nt=1\nℓ(Zt, f) ],\nwhere the supremum is taken over the distribution p of the random variable (Z1, · · · , ZT ) in ZT .\nAny choice for p yields a lower bound on RT (ℓ,Z,F). The following result identifies a canonical choice for p that leads to Ω( √ T ) lower bounds on regret.\nLemma 2 Adapted from [2] If we can find a distribution p on Z and two points f1 and f2 in argminf∈F E[ℓ(Z, f)] such that ℓ(Z, f1) 6= ℓ(Z, f2) with positive probability for Z ∼ p, then RT (ℓ,Z,F) = Ω( √ T ).\nA distribution p satisfying the requirements of Lemma 2 can be viewed as an equalizing strategy for the opponent. This concept, formalized in [18], roughly refers to randomized strategies played by the opponent that cause the player’s decisions to be completely irrelevant from a regret standpoint. These strategies are intrinsically hard to play against and often lead to tight lower bounds. To gain some intuition about this result, suppose that that the opponent generates an independent copy of Z at each round t, which we denote by Zt. In the adversarial setting considered in this paper, the player is aware of the opponent’s strategy but does not get to see the realization of Zt before committing to a decision. For this reason, at any round, f1 and f2 are optimal moves that are completely equivalent from the player’s perspective. However, in hindsight, i.e. once all the realizations of the Zt’s have been revealed, f1 and f2 are typically not equivalent because ℓ(Zt, f1) 6= ℓ(Zt, f2) with positive probability and one of these two moves will turn out to be\nmax(0,\nT ∑\nt=1\nℓ(Zt, f1)− ℓ(Zt, f2))\nsuboptimal which, in expectations, is of order Ω( √ T ) by the central limit theorem. Given the conditions imposed on p, it is convenient to work with the following equivalence relation.\nDefinition 1 We define the equivalence relation ∼ℓ on F by fa ∼ℓ fb for fa, fb ∈ F if and only if ℓ(z, fa) = ℓ(z, fb) for all z ∈ Z .\nIn what follows, we show, using sensitivity analysis for linear programming, that we can systematically, with the only exception of trivial games defined below, construct a distribution p with support Z such that there are at least two equivalent classes in argminf∈F E[ℓ(Z, f)] whenever F is a polyhedron, for Z ∼ p.\nDefinition 2 The game (ℓ,Z,F) is said to be trivial if and only if ∃f∗ ∈ F such that ∀z ∈ Z, ℓ(z, f∗) ≤ min\nf∈F ℓ(z, f).\nA simple example of a trivial game is (ℓ(z, f) = zf, [0, 1], [0, 1]), where ℓ(z, f) ≥ 0 ∀f ∈ [0, 1] and ∀z ∈ [0, 1], with ℓ(z, f) = 0 if f = 0 irrespective of z. If the game is trivial, the player will always play f∗ irrespective of the time horizon and of the opponent’s strategy observed so far to obtain zero regret. As it turns out, this uniquely identifies trivial games as we establish in Lemma 3.\nLemma 3 For any T ∈ N, RT (ℓ,Z,F) ≥ 0. Moreover in any of the following cases:\n1. Z has finite cardinality, 2. ℓ(·, f) is continuous for any choice of f ∈ F ,"
    }, {
      "heading" : "RT (ℓ,Z,F) = 0 if and only if the game is trivial.",
      "text" : "The following result shows that, in most cases of interest, we can drastically restrict the power of the opponent while still preserving the nature of the game. This enables us to focus on the case where Z is finite.\nLemma 4 Suppose that ℓ(·, f) is continuous for any choice of f ∈ F . If the game (ℓ,Z,F) is not trivial, there exists a finite subset Z̃ ⊆ Z such that the game (ℓ, Z̃,F) is not trivial.\nWe are now ready to present the main results of this section. To the best of our knowledge, these results constitute the first systematic Ω( √ T ) lower bounds on regret obtained for a large class of piecewise linear loss functions.\nTheorem 2 Suppose that F is a polyhedron. In any of the following cases:\n1. Z has finite cardinality, 2. ℓ(·, f) is continuous for any choice of f ∈ F ,\neither the game is trivial or RT (ℓ,Z,F) = Ω( √ T ).\nAn immediate consequence of Theorem 2 for linear games is the following:\nTheorem 3 Suppose that F is a polyhedron and that ℓ(z, f) = zTf . Then, either the game is trivial or RT (ℓ,Z,F) = Ω( √ T ).\nThe proofs rely on Lemma 2 which is based on Theorem 1 and may, as a result, seem rather obscure. We stress that these lower bounds are derived by means of an equalizing strategy. We present this more intuitive view in the Appendix by exhibiting an equalizing strategy in the online linear optimization setting when int(conv(Z)) 6= ∅. Note that Theorems 2 and 3 imply Ω( √ T ) regret for a number of repeated Stackelberg games and online linear optimization problems as discussed in Section 1.1. Furthermore, we stress that Theorem 2 can also be used when F is not a polyhedron but this typically requires a preliminary step which boils down to restricting the opponent’s decision set. For instance, the following well-known result is almost a direct consequence of Theorem 3.\nLemma 5 Suppose that ℓ(z, f) = zTf , that 0 ∈ int(conv(Z)), and that F contains at least two elements. Then RT (ℓ,Z,F) = Ω( √ T ).\nNote that Lemma 5 is consistent with Theorem 3 as the game (ℓ(z, f) = zTf,Z,F) is non-trivial if 0 ∈ int(conv(Z)) as soon as F contains at least two elements. Indeed:\nℓ(ǫ f2 − f1 ‖f2 − f1‖ , f2) > ℓ(ǫ f2 − f1 ‖f2 − f1‖ , f1) and ℓ(ǫ f1 − f2 ‖f2 − f1‖ , f1) > ℓ(ǫ f1 − f2 ‖f2 − f1‖ , f2),\nfor a small enough ǫ > 0 and any pair f1 6= f2 ∈ F . When 0 ∈ int(Z), the opponent has some freedom to play, at each time period, a random vector with expected value zero, making every strategy available to the player equally bad. In other words, any i.i.d. zero-mean distribution is an equalizing strategy for the opponent in this case. A preliminary step is also required to derive Ω( √ T ) lower bounds on regret for prediction problems with side information where F is typically not a polyhedron. We sketch this simple argument for the canonical classification problem with the hinge loss, i.e. the game :\n(ℓ((x, y), f) = max(0, 1− yxTf),Z = B2(0, 1)× {−1, 1},F = B2(0, 1)), but the method readily extends to any of the prediction problems described in Section 1.1. The idea is to restrict the opponent’s decision set by taking a fixed vector x of norm 1 and impose that, at any round t, the opponent’s move be (x, yt) for yt ∈ {−1, 1}. Since ℓ((x, y), f) only depends on f through the scalar product between f and x, the player’s decision set can be equivalently described by this value, which lies in [−1, 1]. Formally, we define a new loss function ℓ̃(y, f) = max(0, 1−yf) with Z̃ = {−1, 1} and F̃ = [−1, 1] and we have:\nRT (ℓ,Z,F) ≥ RT (ℓ̃, Z̃, F̃). Observe now that the game (ℓ̃, Z̃, F̃) is not trivial, that Z is discrete, and that:\nℓ̃(y, f) = max α∈{0,1}\n(α,−yα)T(1, f).\nWe conclude with Theorem 2 that RT (ℓ̃, Z̃, F̃) = Ω( √ T ) which implies that RT (ℓ,Z,F) =\nΩ( √ T ).\nRemark about Lemma 2 We point out that, in general, it is not possible to weaken the assumptions of Lemma 2 (which, in fact, applies to a much more general class of loss functions than the one given by (2)). In particular, finding z ∈ Z such that there are two equivalence classes f1 and f2 in argminf∈F ℓ(z, f) does not guarantee that RT (ℓ,Z,F) = Ω( √ T ) as we illustrate with a counterexample. This is because the result of Lemma 2 is intrinsically tied to the central limit theorem. Consider the following (non-trivial) online linear regression game:\n(ℓ(z, f) = (zTf)2,Z = B2(z∗, 1),F = [f1, f2]), where f1 = (1, 0, · · · , 0), f2 = (0, 1, 0, · · · ), and z∗ = (1, 1, 0, · · · , 0). Observe that ∀z ∈ Z, ∀f ∈ F , zTf ≥ 0. Hence argminf∈F ℓ(z, f) = argminf∈F f Tz. Furthermore, argminf∈F f Tz∗ = [f1, f2] but f1 and f2 are clearly not in the same equivalence class. Yet, even though ℓ is not strongly convex in f , there exists an algorithm achieving O(log(T )) regret, see [21]."
    }, {
      "heading" : "3 Upper bounds",
      "text" : "Looking at Theorem 2, Theorem 3, and Lemma 5, it is tempting to conclude that the existence of pieces where ℓ is linear in its second argument dooms us to Ω( √ T ) regret bounds. We argue that the growth rate of RT (ℓ,Z,F) is determined by a more involved interplay between ℓ, Z , and F so that this assertion requires further examination. In fact, we show that O(log(T )) regret bounds are even possible in online linear optimization. The fundamental reason is that the curvature of the boundary of F can make up for the lack of thereof of ℓ. Curvature is key to enforce stability of the player’s strategy with respect to perturbations in the opponent’s moves. Sometimes, when the predictions are stable, e.g. when ℓ is the square loss ℓ(z, f) = ‖z − f‖2, a very simple algorithm, known as Follow-The-Leader, yields O(log(T )) regret.\nDefinition 3 From [12] The Follow-The-Leader (FTL) strategy consists in playing:\nft ∈ argmin f∈F\n1\nt− 1\nt−1 ∑\nτ=1\nℓ(zτ , f).\nIt is well-known that FTL fails to yield sublinear regret for online linear optimization in general. However, when F has a strongly curved boundary and 0 /∈ conv(Z), the FTL strategy becomes stable, leading to O(log(T )) regret as we next show using sensitivity analysis.\nTheorem 4 Suppose that (i) ℓ is linear, (ii) F = {f ∈ Rdf | F (f) ≤ 0} for F a strongly convex function with respect to the L2 norm, and (iii) 0 /∈ conv(Z). Then, FTL yields O(log(T )) regret.\nAs an example of application of Theorem 4, consider a repeated network game where the player picks the arc costs in a L2 ball, the opponent picks a path, and the loss incurred to the opponent is the sum of the arc costs along the path. In this setting, FTL yields O(log(T )) regret even though the game is not trivial. Theorem 4 also has some implications for non-linear convex loss functions when the boundary of F is curved and 0 is not in the convex hull of the set of subgradients of ℓ with respect to the player’s moves. Indeed suppose that, at any time period t, the player follows the FTL strategy as if the loss function were linear and the past moves of the opponents were y1, · · · , yt−1, i.e.:\nft ∈ argmin f∈F\n1\nt− 1\nt−1 ∑\nτ=1\nyTτf,\nwhere, for any τ = 1, · · · , t − 1, yτ is a subgradient of ℓ(zt, ·) at ft. Then, for any sequence of moves (z1, · · · , zT ), we have:\nrT ((zt)t=1,··· ,T , (ft)t=1,··· ,T ) ≤ T ∑\nt=1\nyTtft − inf f∈F\nT ∑\nt=1\nyTtf\n= O(log(T )).\nIt is, however, a priori unclear whether log(T ) is the optimal growth rate for games satisfying the assumptions of Theorem 4. Quite surprisingly, i.i.d. opponents appear to be particularly weak for this kind of games, incurring at most a O(1) regret lower bound as shown in the following lemma. This is in stark contrast with the situations of Section 2 where the (tight) Ω( √ T ) lower bounds are always derived through i.i.d. opponents.\nLemma 6 Consider the game (ℓ(z, f) = zTf,Z,F) with 0 /∈ conv(Z) and F = B2(0, 1). Any lower bound derived from Theorem 1 with i.i.d. random variables Z1, · · · , ZT ∼ p is O(1) for any choice of p ∈ P(Z).\nAbernethy et al. [2] remark that restricting the study to i.i.d. sequences is in general not enough to get tight bounds for non-linear losses such as ℓ(z, f) = ‖z − f‖2. It turns out that this is also true for linear losses when F is strongly curved and 0 /∈ conv(Z) as the value of the game is in fact Θ(log(T )).\nTheorem 5 When ℓ(z, f) = zTf , 0 /∈ conv(Z), and F = B2(0, 1), the value of the game is: RT (ℓ,Z,F) = Θ(log(T )).\nSo far, we have studied two scenarios that are diametrically opposed in terms of the curvature of the decision sets with polyhedra on one side in Section 2, with Θ( √ T ) regret, and euclidean balls, with Θ(log(T )) regret, on the other side of the spectrum. Interestingly, bridging this gap leads to the rise of intermediate learning rates that can be quantified through the modulus of convexity of F . Specifically, consider any norm ‖ ‖F . The modulus of convexity of the associated unit ball is defined as:\nδF : ǫ → inf ‖f‖\nF ,‖f̃‖ F ≤1\n‖f−f̃‖ F ≥ǫ\n1− ∥ ∥ ∥\n∥ ∥\nf + f̃\n2\n∥ ∥ ∥ ∥ ∥\nF .\nThe norm ‖ ‖F is said to be uniformly convex if its characteristic of convexity is equal to 0, i.e.: sup{ǫ ≥ 0 | δF(ǫ) = 0} = 0.\nPisier [16] show that if ‖ ‖F is uniformly convex, there must exist q ≥ 2 and c > 0 such that δF(ǫ) ≥ cǫq for all ǫ ∈ [0, 2], in which case we say that ‖ ‖F is q−uniformly convex. This parameter quantifies how curved ‖ ‖F balls are and determines the growth rate of the value of the game when F is a ‖ ‖F ball and 0 /∈ conv(Z).\nLemma 7 Consider the game (ℓ(z, f) = zTf,Z,F) with 0 /∈ conv(Z) and F = {f | ‖f‖F ≤ C}, where ‖ ‖F is a q-uniformly convex norm and C ≥ 0. Then, FTL yields regret O(log(T )) if q = 2 and regret O(T q−2 q−1 ) if q ∈ (2, 3].\nSituations where 0 lies on the boundary of Z when ℓ is linear Observe that this situation is not covered by Lemma 5, Theorem 4, Theorem 5, nor Lemma 7. We stress that zero-mean i.i.d. opponents are not helpful to derive Ω( √ T ) regret when 0 lies exactly on the boundary of Z (in fact, in the relative interior of an edge of Z), as we illustrate with an example. Therefore, the growth rate of RT (ℓ,Z,F) remains unknown in this setup. Define Z = conv(z1, z2, z3, z4) with z1 = (−1, 1, 0, 0), z2 = (1,−1, 0, 0), z3 = (0, 0, 0, 1), and z4 = (0, 0, 1, 0). Also define F = [f∗, f∗∗] with f∗ = (0, 0, 0, 0) and f∗∗ = (1, 1,−1, 1). Observe that the game (ℓ(z, f) = zTf,Z,F) is not trivial because argminf∈F f · z3 = {f∗} while argminf∈F f · z4 = {f∗∗}. For any zero-mean i.i.d. opponent Z1, · · · , ZT , the only possibility is to have Zt ∈ [z1, z2]. We get, irrespective of the player’s strategy:\nE[rT ((Zt)t=1,··· ,T , (ft)t=1,··· ,T )] = −E[ inf f∈F\nf · T ∑\nt=1\nZt].\nWe have f∗ ∈ argminf∈F f · z for any z ∈ [z1, z2]. Hence:\nE[rT ((Zt)t=1,··· ,T , (ft)t=1,··· ,T )] = −E[f∗ · T ∑\nt=1\nZt],\nwhich finally yields E[rT ((Zt)t=1,··· ,T , (ft)t=1,··· ,T )] = 0."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Alexander Rakhlin for his valuable input, and in particular, for bringing to our attention the possibility of having o( √ T ) bounds on regret in the linear setting."
    }, {
      "heading" : "4 Appendix: proofs",
      "text" : ""
    }, {
      "heading" : "4.1 Proof of Theorem 1",
      "text" : "The assumptions of Theorem 1 in [2] are satisfied for the game (ℓ,Z,F) using Assumption 1 and the fact that any loss function ℓ of the form (2) is such that ℓ(z, ·) is continuous for any z ∈ Z ."
    }, {
      "heading" : "4.2 Proof of Lemma 1",
      "text" : "The proof follows from the repeated use of the Von Neumann’s minimax theorem developed in [2]. To simplify the presentation, we prove the result when T = 2 but the general proof follows the same principle. We have:\nR2 = inf f1∈F sup z1∈Z inf f2∈F sup z2∈Z [\n2 ∑\nt=1\nℓ(zt, ft)− inf f∈F\n2 ∑\nt=1\nℓ(zt, f) ].\nConsider fixed vectors f1, f2 ∈ F and z1 ∈ Z and define the function M(z2) = ∑2 t=1 ℓ(zt, ft) − inff∈F ∑2 t=1 ℓ(zt, f). Observe that M is convex. Indeed, z2 → ℓ(z1, f1) + ℓ(z2, f2) is affine and z2 → inff∈F ∑2 t=1 ℓ(zt, f) is concave as the infimum of affine functions. Therefore:\nsup z2∈Z M(z2) = sup z2∈conv(Z) M(z2).\nWe obtain:\nR2 = inf f1∈F sup z1∈Z inf f2∈F sup z2∈conv(Z) [\n2 ∑\nt=1\nℓ(zt, ft)− inf f∈F\n2 ∑\nt=1\nℓ(zt, f) ].\nBy randomizing the choice of z2, we can use Von Neumann’s minimax theorem to derive:\nR2 = inf f1∈F sup z1∈Z { ℓ(z1, f1) + sup p2∈P(conv(Z)) { inf f2∈F Ez∼p2ℓ(z, f2)− Ez2∼p2 inf f∈F\n2 ∑\nt=1\nℓ(zt, f) } }.\nFor a fixed f1 ∈ F , define:\nA(z1) = ℓ(z1, f1) + sup p2∈P(conv(Z)) { inf f2∈F Ez∼p2ℓ(z, f2)− Ez2∼p2 inf f∈F\n2 ∑\nt=1\nℓ(zt, f) }.\nObserve that, for a fixed p2 ∈ P(conv(Z)), the function:\nz1 → inf f2∈F Ez∼p2ℓ(z, f2)− Ez2∼p2 inf f∈F\n2 ∑\nt=1\nℓ(zt, f)\nis convex as the difference between a constant and the expected value of the infimum of affine functions. Since the supremum of convex functions is convex, A is convex and supz1∈Z A(z1) = supz1∈conv(Z) A(z1). We derive:\nR2 = inf f1∈F sup z1∈conv(Z) [ ℓ(z1, f1)+ sup p2∈P(conv(Z)) { inf f2∈F Ez∼p2ℓ(z, f2)−Ez2∼p2 inf f∈F\n2 ∑\nt=1\nℓ(zt, f)}].\nTo conclude, we unwind the first step, i.e. we use the minimax theorem in reverse order. This yields:\nR2 = inf f1∈F sup z1∈conv(Z) inf f2∈F sup z2∈conv(Z) [\n2 ∑\nt=1\nℓ(zt, ft)− inf f∈F\n2 ∑\nt=1\nℓ(zt, f) ],\ni.e. R2(ℓ(z, f) = zTf,Z,F) = R2(ℓ(z, f) = zTf, conv(Z),F). Moreover, Z is a compact set which implies that conv(Z) is also a compact set by a standard topological argument. As a result, the game (ℓ(z, f) = zTf, conv(Z),F) also satisfies Assumption 1."
    }, {
      "heading" : "4.3 Proof of Lemma 2",
      "text" : "We follow the analysis of Theorem 19 of [2]. Using Theorem 1 with p taken as the distribution of i.i.d. copies of Z , we get the lower bound:\nRT ≥ T inf f∈F E[ℓ(Zt, f)]− E[ inf f∈F\nT ∑\nt=1\nℓ(Zt, f)]\n≥ T sup f∈{f1,f2} E[ℓ(Zt, f)]− E[ inf f∈{f1,f2}\nT ∑\nt=1\nℓ(Zt, f)]\n≥ E[max{ T ∑\nt=1\nE[ℓ(Zt, f1)]− ℓ(Zt, f1), T ∑\nt=1\nE[ℓ(Zt, f2)]− ℓ(Zt, f2)}]\n≥ E[max{0, T ∑\nt=1\nℓ(Zt, f1)− ℓ(Zt, f2)}],\nwhere we use the fact that inff∈F E[ℓ(Zt, f)] = E[ℓ(Zt, f1)] = E[ℓ(Zt, f2)]. Since ℓ(Z, f2) 6= ℓ(Z, f1) with positive probability, the random variables (ℓ(Zt, f1) − ℓ(Zt, f2))t=1,··· ,T are i.i.d. with zero mean and positive variance and we can conclude with the central limit theorem since ℓ is bounded."
    }, {
      "heading" : "4.4 Proof of Lemma 3",
      "text" : "The fact that RT ≥ 0 is proved in Lemma 3 of [2] and follows from Theorem 1 by taking the Zt’s to be deterministic and all equal to any z ∈ Z . Clearly, if the game is trivial then RT = 0 because this value is attained for f1, · · · , fT = f∗ irrespective of the decisions made by the opponent. Conversely, suppose RT = 0. Consider p to be the product of T uniform distributions on Z . Then, using again Theorem 1:\n0 ≥ E[ T ∑\nt=1\ninf ft∈F E[ℓ(Zt, ft)]− inf f∈F\nT ∑\nt=1\nℓ(Zt, f) ],\nas Z1, · · · , ZT are independent random variables. Since they are also identically distributed, we obtain:\n0 ≥ T · inf f∈F E[ℓ(Z, f)]− E[ inf f∈F\nT ∑\nt=1\nℓ(Zt, f)].\nYet E[ inf f∈F\n∑T t=1 ℓ(Zt, f)] ≤ inf f∈F E[ ∑T t=1 ℓ(Zt, f)] = T · inf f∈F E[ℓ(Z, f)] and we derive:\nT · inf f∈F E[ℓ(Z, f)]− E[ inf f∈F\nT ∑\nt=1\nℓ(Zt, f)] = 0.\nSince ℓ is bounded, Z is compact, and ℓ(z, ·) is continuous for any z ∈ Z , f → E[ℓ(Z, f)] is continuous by dominated convergence so we can take f∗ ∈ argminf∈F E[ℓ(Z, f)] (F is compact). We obtain:\nE[\nT ∑\nt=1\nℓ(Zt, f ∗)− inf\nf∈F\nT ∑\nt=1\nℓ(Zt, f)] = 0.\nAs ∑T t=1 ℓ(Zt, f ∗)− inf\nf∈F\n∑T t=1 ℓ(Zt, f) ≥ 0, we derive that:\n(z1, · · · , zT ) → T ∑\nt=1\nℓ(zt, f ∗)− inf\nf∈F\nT ∑\nt=1\nℓ(zt, f) = 0\nalmost everywhere on ZT . If Z is discrete, this implies equality on ZT , which in particular implies ℓ(z, f∗) = inf\nf∈F ℓ(z, f) for all z ∈ Z and we are done. If, on the other hand, ℓ(·, f) is continuous for\nall f ∈ F , we have: T ∑\nt=1\nℓ(zt, f ∗) ≤\nT ∑\nt=1\nℓ(zt, f), ∀f ∈ F , ∀(z1, · · · , zT ) ∈ Z̃,\nfor Z̃ a subset of Z with Lebesgue measure equal to that of Z . Since a non-empty open set cannot have Lebesgue measure 0, Z̃ is dense in Z and by taking limits in the above inequality for each f ∈ F separately, we conclude that:\nT ∑\nt=1\nℓ(zt, f ∗) ≤\nT ∑\nt=1\nℓ(zt, f), ∀f ∈ F , ∀(z1, · · · , zT ) ∈ Z,\nwhich in particular implies that ℓ(z, f∗) = inf f∈F ℓ(z, f) for all z ∈ Z and the game is trivial."
    }, {
      "heading" : "4.5 Proof of Lemma 4",
      "text" : "Suppose by contradiction that we cannot find such a finite subset. Since Z is compact, it is also separable thus it contains a countable dense subset {zn | n ∈ N}. By assumption, the game (ℓ, {zk | k ≤ n},F) must be trivial for any n, i.e. there exists fn ∈ F such that:\nℓ(zk, fn) ≤ min f∈F ℓ(z, f), ∀k ≤ n.\nSince F is compact, we can find a subsequence of (fn)n∈N such that fn → f∗ ∈ F . Without loss of generality, we continue to refer to this sequence as (fn)n∈N. Taking the limit n → ∞ in the above inequality for any fixed k ∈ N yields:\nℓ(zk, f ∗) ≤ ℓ(zk, f), ∀f ∈ F , ∀k ∈ N.\nConsider a fixed f ∈ F , since {zn | n ∈ N} is dense in Z and since ℓ(·, f∗) and ℓ(·, f) are continuous, we get: ℓ(z, f∗) ≤ ℓ(z, f), ∀f ∈ F , ∀z ∈ Z, which shows that (ℓ,Z,F) is trivial, a contradiction."
    }, {
      "heading" : "4.6 Proof of Theorem 2",
      "text" : "Without loss of generality we can assume that the game is not trivial and that X (z) is finite for any z ∈ Z since otherwise if X (z) is a polyhedron, the maximum in (2) must be attained at an extreme point of X (z) (ℓ is bounded by Assumption 1) and there are finitely many such points for any z. Moreover, we can also assume that Z is discrete by Lemma 4 since, borrowing the notations of Lemma 4, we have: RT (ℓ,Z,F) ≥ RT (ℓ, Z̃,F). Write Z = {zn | n ≤ N} and denote by p0 the uniform distribution on Z , i.e. p0 = 1 N ∑N n=1 δzn , where δzn is the Dirac distribution supported at zn. We may assume that there is a single equivalence class in argminf∈F Ep0 [ℓ(Z, f)], otherwise we are done by Lemma 2. Take f∗ ∈ argminf∈F Ep0 [ℓ(Z, f)]. Since the game (ℓ,Z,F) is not trivial, there exists zk in Z and f∗∗ in F such that ℓ(zk, f∗∗) < ℓ(zk, f∗). Therefore, we can find ǫ > 0 small enough such that (N − 1)ǫ < 1 and:\n(1− (N − 1)ǫ)ℓ(zk, f∗∗) + ∑\nn6=k ǫℓ(zn, f\n∗∗) < (1− (N − 1)ǫ)ℓ(zk, f∗) + ∑\nn6=k ǫℓ(zn, f\n∗).\nDefine p1 as the corresponding distribution, i.e. p1 = (1 − (N − 1)ǫ)δzk + ǫ ∑\nn6=k δzn . By construction, the equivalence class of f∗ is not in argminf∈F Ep1 [ℓ(Z, f)]. Once again, without loss of generality, we may assume that there is a single equivalence class in argminf∈F Ep1 [ℓ(Z, f)], otherwise we are done by Lemma 2. Moreover, we can now redefine f∗∗ as a representative of the only equivalence class contained in argminf∈F Ep1 [ℓ(Z, f)]. We now move on to show that there must exist α ∈ (0, 1) such that there are at least two equivalence classes in argminf∈F Epα [ℓ(Z, f)],\nwhere the distribution pα is defined as pα = (1 − α)p0 + αp1. Observe that minf∈F Epα [ℓ(Z, f)] can be written as the linear program:\nmin q1,··· ,qN ,f\nq · ((1 − α)x0 + αx1)\nsubject to q = (q1, · · · , qN ) qn ≥ (C(zn)f + c(zn))Tx ∀x ∈ X (zn), ∀n = 1, · · · , N f ∈ F , q1, · · · , qN ∈ R\n(3)\nwhere x0 and x1 are vectors of size N defined as follows:\nx0 = 1\nN (1, · · · , 1)\nand x1 = (1− (N − 1)ǫ)(0, · · · , 0, 1, 0, · · · ) + ǫ(1, · · · , 1, 0, 1, · · · , 1), i.e. all the components of x1 are equal to ǫ except for the kth one which is equal to (1− (N − 1)ǫ). We are interested in the function φ : α → argminf∈F Epα [ℓ(Z, f)]. For any f ∈ F , define I(f) = {α ∈ [0, 1] | f ∈ φ(α)}. Since α → Epα [ℓ(Z, f)] is linear in α, I(f) = {α ∈ [0, 1] | f ∈ φ(α)} is a closed interval in [0, 1] for any f ∈ F . Moreover, F being a polyhedron, the feasible set of (3) is also a polyhedron, hence it has a finitely many extreme points. We denote by {f1, · · · , fL} the projection of the set of extreme points onto the f coordinate. Since (3) is a linear program, this shows that, for any α ∈ [0, 1], there exists l ∈ {1, · · · , L} such that fl ∈ φ(α). Therefore, we can write [0, 1] = ∪Ll=1I(fl). We can further simplify this description by assuming that the fl’s belong to different equivalence classes (because I(f) = I(f ′) if f is equivalent to f ′). Now observe that if I(fl) ∩ I(fj) 6= ∅ for all any l 6= j ≤ K , then there are two classes of equivalence in argminf∈F Epα [ℓ(Z, f)] for any α ∈ I(fl) ∩ I(fj) and we are done. Suppose by contradiction that we cannot find such a pair of indices. Because the only way to partition [0, 1] into L < ∞ non-overlapping closed intervals is to have L = 1, we get [0, 1] = I(f1). This implies that f∗ and f∗∗ belong to the same equivalence class, a contradiction."
    }, {
      "heading" : "4.7 Alternative proof of Theorem 2 by exhibiting an equalizing strategy when ℓ is linear",
      "text" : "Using Lemma 1, we can assume without loss of generality that Z is convex. When ℓ is linear, the procedure developed in the proof of Theorem 2 boils down to finding a point z ∈ int(Z) such that | argminf∈F zTf | > 1 and, with further examination, we can also guarantee that there exists ǫ > 0, e ∈ Rn and f1, f2 ∈ argminf∈F zTf such that f1 ∈ argminf∈F(z − xe)Tf while f2 /∈ argminf∈F(z − xe)Tf for all x ∈ (0, ǫ] and symmetrically for x ∈ [−ǫ, 0). Consider a randomized opponentZt = z+(ǫtǫ)e for (ǫt)t=1,··· ,T i.i.d. Rademacher random variables. Then for any player’s strategy:\nE[rT ((Zt)t=1,··· ,T , (ft)t=1,··· ,T )] = T ∑\nt=1\nE[Zt] Tft − E[ inf f∈F f T\nT ∑\nt=1\nZt].\nThis yields:\nE[rT ((Zt)t=1,··· ,T , (ft)t=1,··· ,T )] = T ∑\nt=1\nzTft − E[ inf f∈F\nf T T ∑\nt=1\nZt].\nWe can lower bound the last quantity by:\nE[rT ((Zt)t=1,··· ,T , (ft)t=1,··· ,T )] ≥ T (zTf1)− TE[ inf f∈F\nf T(z + (ǫ · ∑T\nt=1 ǫt T )e)],\nas f1 ∈ argminf∈F zTf , but we could have equivalently picked f2 as f T1z = f T2z. Furthermore, as | ∑ T t=1\nǫt T | ≤ 1, f1 is optimal in the inner optimization problem when ∑T t=1 ǫt ≤ 0 while f2 is optimal when\n∑T t=1 ǫt ≥ 0. Hence:\nE[rT ((Zt)t=1,··· ,T , (ft)t=1,··· ,T )] ≥ T (zTf1)− TE[ f T1 (z + (ǫ · ∑T t=1 ǫt T )e) · 1∑T t=1 ǫt≤0+\nf T2 (z + (ǫ · ∑T t=1 ǫt T )e) · 1∑T t=1 ǫt≥0 ].\nObserve that the term T (zTf1) cancels out and we get:\nE[rT ((Zt)t=1,··· ,T , (ft)t=1,··· ,T )] ≥ E[|∑t=1 ǫt|]\nT · ǫ · (f T1e− f T2e).\nBy Khintchine’s inequality E[|∑Tt=1 ǫt|] ≥ 1√2 √ T . Moreover f T1e − f T2e > 0 because f2 ∈ argminf∈F(z + ǫe) Tf while f1 does not and f T1z = f T 2z. We finally derive\nE[rT ((Zt)t=1,··· ,T , (ft)t=1,··· ,T )] ≥ (f T1e− f T2e)√\n2\n√ T .\nThis enables us to conclude RT = Ω( √ T ) as this shows that for any player’s strategy, there exists a sequence z1, · · · , zT such that rT ((zt)t=1,··· ,T , (ft)t=1,··· ,T ) ≥ E[rT ((Zt)t=1,··· ,T , (ft)t=1,··· ,T )]."
    }, {
      "heading" : "4.8 Proof of Theorem 3",
      "text" : "Straightforward from Theorem 2 since ℓ is jointly continuous."
    }, {
      "heading" : "4.9 Proof of Lemma 5",
      "text" : "Using Lemma 1, we can assume that Z is convex. Consider f1 6= f2 ∈ F and define e = f1−f2‖f1−f2‖ . Since 0 ∈ int(Z), there exists ǫ > 0 such that ǫe and −ǫe are in Z . We restrict the opponent’s decision set by imposing that, at any round t, the opponent’s move be ytǫe for yt ∈ Z̃ = {−1, 1}. Since ℓ(ytǫe, f) only depends on f through the scalar product between f and e, the player’s decision set can equivalently be described by F̃ = {f Te | f ∈ F} which is a closed interval (since F is convex and compact) and thus a polyhedron. Defining a new loss function as ℓ̃(y, f) = yǫf , we have: RT (ℓ,Z,F) ≥ RT (ℓ̃, Z̃, F̃). Observe that the game (ℓ̃, Z̃, F̃) is linear and not trivial, otherwise there would exist f∗ such that eTf∗ ≤ eTf2 and −eTf∗ ≤ −eTf1 which would imply ‖e‖ = 0. With Theorem 3, we conclude RT (ℓ̃, Z̃, F̃) = Ω( √ T ) and thus RT (ℓ,Z,F) = Ω( √ T )."
    }, {
      "heading" : "4.10 Proof of Theorem 4",
      "text" : "A common inequality on the regret incurred for the FTL strategy is:\nrT ((zt)t=1,··· ,T , (ft)t=1,··· ,T ) ≤ T ∑\nt=1\nzTt (ft − ft+1),\nWe use sensitivity analysis to control this last quantity. Specifically we show that the mapping φ : z → argminf,F (f)=0 zTf is Lipschitz on Z . Using this property:\nrT ((zt)t=1,··· ,T , (ft)t=1,··· ,T ) ≤ T ∑\nt=1\n‖zt‖ ‖ft − ft+1‖\n= O(\nT ∑\nt=1\n∥ ∥ ∥ ∥ ∥ 1 t− 1 t−1 ∑\nτ=1\nzτ − 1\nt\nt ∑\nτ=1\nzτ\n∥ ∥ ∥ ∥ ∥ )\n= O(\nT ∑\nt=1\n∥ ∥ ∥ ∥ ∥ 1 t(t− 1) t−1 ∑\nτ=1\nzτ − 1\nt zt\n∥ ∥ ∥ ∥ ∥ )\n= O( T ∑\nt=1\n1\nt(t− 1)\n∥ ∥ ∥ ∥ ∥ t−1 ∑\nτ=1\nzτ\n∥ ∥ ∥ ∥ ∥ + 1 t ‖zt‖)\n= O(\nT ∑\nt=1\n1 t )\n= O(log(T )),\nsince Z is compact. We now move on to show that φ is Lipschitz. As conv(Z) is closed and convex, we can strictly separate 0 from conv(Z). Hence, there exists a 6= 0 ∈ Rd and c > 0 such that a · z > c, ∀z ∈ Z . We get ‖z‖ ≥ c‖a‖ > 0 ∀z ∈ Z . Let us use the shorthand C = c‖a‖ . Take (z1, z2) ∈ Z2 and (f(z1), f(z2)) ∈ φ(z1) × φ(z2). Observe that the constraint qualifications are automatically satisfied at f(z1) and f(z2) as ∇F cannot vanish on {f | F (f) = 0} since F does cannot attain its minimum on this set (F is assumed to contain at least two points). Hence, there exist λ1, λ2 ≥ 0 such that z1 + λ1∇F (f(z1)) = 0 and z2 + λ2∇F (f(z2)) = 0. As z1, z2 6= 0, we must have λ1, λ2 6= 0. We obtain ∇F (f1) = − 1λ1 z1 and ∇F (f(z2)) = − 1 λ2 z2. Since F is strongly convex, there exists β > 0 such that:\n(∇F (f ′)−∇F (f ′′))T(f ′ − f ′′) ≥ β ‖f ′ − f ′′‖2 , for all f ′, f ′′ ∈ F . Applying this inequality for f ′ = f(z1) and f ′′ = f(z2), we obtain:\n( 1\nλ2 z2 −\n1\nλ1 z1)\nT(f(z1)− f(z2)) ≥ β ‖f(z1)− f(z2)‖2 .\nWe can break down the last expression in two pieces:\n1\nλ2 zT2(f(z1)− f(z2)) +\n1\nλ1 zT1(f(z2)− f(z1)) ≥ β ‖f(z1)− f(z2)‖2 .\nObserve that zT2(f(z1) − f(z2)) ≥ 0 since both f(z1) and f(z2) belong to F and since f(z2) is the minimizer of zT2f for f ranging in F . Symmetrically, zT1(f(z2) − f(z1)) ≥ 0. Note that 1 λ1 = 1|λ1| = ‖∇F (f(z1))‖\n‖z1‖ . As ∇F is continuous and F is compact, there exists K > 0 such that ‖∇F (f)‖ ≤ K for any f ∈ F . Hence, we get 1\nλ1 ≤ K C and the same inequality holds for λ2.\nPlugging this upper bound back into the last inequality yields:\nK C (z2 − z1)T(f(z1)− f(z2)) ≥ β‖f(z1)− f(z2)‖2.\nUsing the Cauchy-Schwarz inequality and simplifying on both sides by ‖f(z2)− f(z1)‖ yields: K\nβC ‖z2 − z1‖ ≥ ‖f(z1)− f(z2)‖,\ni.e. K βC ‖z2 − z1‖ ≥ ‖φ(z1)− φ(z2)‖."
    }, {
      "heading" : "4.11 Proof of Lemma 6",
      "text" : "Using Lemma 1, we can assume that Z is convex. Since Z is compact and convex and since 0 /∈ Z , we can strictly separate 0 from Z and find z∗ 6= 0 such that Z ⊆ B2(z∗, α ‖z∗‖) with α < 1. By rescaling Z , we can assume that α ‖z∗‖ = 1 and ‖z∗‖ > 1. In the sequel, σt−1 serves as a shorthand for σ(Z1, · · · , Zt−1). We prove more generally that, for any choice of random variables (Z1, · · · , ZT ) such that E[Zt|σt−1] is constant almost surely, the lower bound on regret derived from Theorem 1 is O(1). Write Zt = z∗+Wt and E[Wt|σt−1] = ct with ‖Wt‖ ≤ 1 and ‖ct‖ ≤ 1. Define w∗ = T · z∗ +∑Tt=1 ct. Observe that ‖w∗‖ ≥ T · ‖z∗‖ − ‖ ∑T t=1 ct‖ ≥ T · (‖z∗‖ − 1) > 0. Write Wt = Xt w∗\n‖w∗‖ + W̃t + ct with W̃ T t w ∗ = 0. Projecting down the equality E[Wt − ct|σt−1] = 0 onto\nw∗, we get E[Xt|σt−1] = 0 and E[W̃t|σt−1] = 0. The bound that results from an application of Theorem 1 is:\nRT ≥ E[‖w∗ + T ∑\nt=1\nWt − ct‖]− T ∑\nt=1\n‖z∗ + ct‖].\nWe now focus on finding an upper bound on the right-hand side. Expanding the first term yields:\n‖w∗ + T ∑\nt=1\nWt − ct‖ =\n√ √ √ √(1 + T ∑\nt=1\nXt ‖w∗‖)\n2 · ‖w∗‖2 + ‖ T ∑\nt=1\nW̃t‖2.\nBy concavity of the squared root function:\nE[‖w∗ + T ∑\nt=1\nWt − ct‖] ≤\n√ √ √\n√‖w∗‖2 · E[(1 + T ∑\nt=1\nXt ‖w∗‖)\n2] + E[‖ T ∑\nt=1\nW̃t‖2].\nWe expand the two inner terms:\nE[(1 +\nT ∑\nt=1\nXt ‖w∗‖ ) 2] = 1 + 2\nT ∑\nt=1\nE[Xt] ‖w∗‖ + 1 ‖w∗‖2 · E[( T ∑\nt=1\nXt) 2].\nLooking at each term individually, we have E[Xt] = E[E[Xt|σt−1] = 0 and E[( ∑T t=1 Xt) 2] = E[( ∑T−1\nt=1 Xt) 2] + 2E[XT · ( ∑T−1 t=1 Xt)] + E[X 2 T ], yet E[XT · ( ∑T−1 t=1 Xt)] = E[E[XT |σT−1] ·\n( ∑T−1 t=1 Xt)] = 0. Hence, E[(1 + ∑T t=1 Xt ‖w∗‖ ) 2] = 1 +\nE[ ∑\nT t=1 X 2 t ]\n‖w∗‖2 . Similarly E[‖ ∑T t=1 W̃t‖2] = ∑T\nt=1 E[‖W̃t‖2]. We obtain:\nE[‖w∗ + T ∑\nt=1\nWt − ct‖] ≤\n√ √ √\n√‖w∗‖2 + T ∑\nt=1\nE[X2t + ‖W̃t‖2].\nRemark that ‖Wt − ct‖ ≤ ‖Wt‖+ ‖ct‖ ≤ 2. Hence, X2t + ‖W̃t‖2 ≤ 2. We obtain:\nE[‖w∗ + T ∑\nt=1\nWt − ct‖] ≤ √ ‖w∗‖2 + 2T .\nWe have √ ‖w∗‖2 + 2T = ‖w∗‖ · √\n1 + 2T‖w∗‖2 ≤ ‖w∗‖ + T‖w∗‖ for T big enough as ‖w∗‖ ≥ T · (‖z∗‖ − 1). Yet ‖w∗‖ = ‖∑Tt=1 z∗ + ct‖ ≤ ∑T t=1 ‖z∗ + ct‖. Hence, the lower bound derived is:\nE[‖w∗ + T ∑\nt=1\nWt − ct‖]− T ∑\nt=1\n‖z∗ + ct‖ ≤ T ‖w∗‖ ≤ 1 ‖z∗‖ − 1 = O(1)."
    }, {
      "heading" : "4.12 Proof of Theorem 5",
      "text" : "Since Z has non-empty interior, we can find z∗ 6= 0 and α ∈ (0, 132 ] such that B2(z∗, α ‖z∗‖) ⊆ Z . Define e as a unit vector orthogonal to z∗ and Z̃ = {z |z = z∗ + (wα ‖z∗‖)e, |w| ≤ 1}. Since Z̃ ⊆ Z , we have:\nRT (ℓ,Z,F) ≥ RT (ℓ, Z̃,F),\nand we can focus on developing a Ω(log(T )) lower bound on regret for the game (ℓ, Z̃,F). Using the minimax reformulation of Theorem 1, we have:\nRT (ℓ, Z̃,F)\n= sup p E\n[\n− T ∑\nt=1\n‖z∗ + (α ‖z∗‖E[Wt|W1, · · · ,Wt−1])e‖+ ∥ ∥ ∥ ∥\n∥\nTz∗ + (α ‖z∗‖ T ∑\nt=1\nWt)e\n∥ ∥ ∥ ∥ ∥ ]\n= sup p E\n  − T ∑\nt=1\n√\n‖z∗‖2 + (α ‖z∗‖E[Wt|W1, · · · ,Wt−1])2 +\n√ √ √\n√T 2 ‖z∗‖2 + (α ‖z∗‖ T ∑\nt=1\nWt)2\n\n\nwhere the supremum is taken over the distribution p of the random variable (W1, · · · ,WT ) in [−1, 1]T . Rearranging this expression yields:\nRT (ℓ, Z̃,F) = ‖z∗‖ sup p E\n\n T\n√\n1 + (α ∑T t=1 Wt T )2 − T ∑\nt=1\n√ 1 + (αE[Wt|W1, · · · ,Wt−1])2  \n= ‖z∗‖ sup p {E[ T (1 +\n∞ ∑\nn=1\n(1 2\nn\n)\nα2n( ∑T t=1 Wt T )2n)\n− T ∑\nt=1\n(1 +\n∞ ∑\nn=1\n( 1 2\nn\n)\nα2nE[Wt|W1, · · · ,Wt−1]2n)]}\n= ‖z∗‖ sup p {α\n2\n2 E\n[\n( ∑T t=1 Wt) 2\nT −\nT ∑\nt=1\nE[Wt|W1, · · · ,Wt−1]2 ]\n+ ∞ ∑\nn=2\n(1 2\nn\n)\nα2nE\n[\n( ∑T t=1 Wt) 2n\nT 2n−1 −\nT ∑\nt=1\nE[Wt|W1, · · · ,Wt−1]2n ] },\nwhere the second equality results from a series expansion (valid since (αE[Wt|W1, · · · ,Wt−1])2, (α ∑T t=1 Wt T )2 ≤ α2 < 1) and the third inequality is derived from Fubini, observing that: ∞ ∑\nn=1\n| (1 2\nn\n) |α2nE[( ∑T\nt=1 Wt T\n)2n] ≤ ∞ ∑\nn=1\nα2n = 1\n1− α2 < ∞\nand similarly: ∞ ∑\nn=1\n| ( 1 2\nn\n) |α2nE[E[Wt|W1, · · · ,Wt−1]2n] ≤ ∞ ∑\nn=1\nα2n = 1\n1− α2 < ∞.\nInterestingly, the first-order term of this series expansion, i.e.\nE\n[\n( ∑T t=1 Wt) 2\nT −\nT ∑\nt=1\nE[Wt|W1, · · · ,Wt−1]2 ] ,\nis precisely the expression of the minimax regret for the game (ℓ(z, f) = (z − f)2, [−1, 1], [−1, 1]) which is known to have optimal regretΘ(log(T )), see Section 7.3 of [2]. This motivates the introduction of the probability distribution p used in [2] to establish the Ω(log(T )) lower bound. Specifically, we use the conditional distributions:\npt(Wt = w|W1, · · · ,Wt−1) = { 1+ctW1:t−1 2 if w = 1 1−ctW1:t−1\n2 if w = −1 t = 2, · · · , T\nwhere W1:t−1 = ∑t−1 τ=1 Wτ and the sequence (ct)t=1,··· ,T is recursively defined as:\ncT = 1\nT\nct−1 = ct + c 2 t t = T, · · · , 2.\nTogether with W1 taken as a Rademacher random variable, these conditional distributions define a joint distribution p as it can be shown that ct ∈ [0, 1t ]. Abernethy et al. [2] show that:\nE\n[\n( ∑T t=1 Wt) 2\nT −\nT ∑\nt=1\nE[Wt|W1, · · · ,Wt−1]2 ] = log(T ) +O(log log(T )). (4)\nHence, it remains to control the terms of order n ≥ 2 in the series expansion. First observe that, by definition:\nE[W 2n1:T ] = E[E[W 2n 1:T |W1, · · · ,WT−1]]\n= E[ 1 + ctW1:T−1\n2 (W1:T−1 + 1)\n2n + 1− ctW1:T−1\n2 (W1:T−1 + 1)\n2n]\n= 1 +\nn ∑\nk=1\n(\n(\n2n\n2k\n)\n+\n(\n2n\n2k − 1\n)\ncT )E[(W1:T−1) 2k],\nwhich implies that:\n|c2n−1T E[W 2n1:T ]− (c2n−1T + 2nc2nT )E[W 2n1:T−1]| ≤ ( 2n\n2(n− 1)\n)\ncT + 2\nn ∑\nk=0\n(\n2n\n2k\n)\nc2T\n≤ 2n2cT + 24nc2T , (5)\nsince cT |W1:T−1| ≤ 1. Additionally, we have, using the recursive definition of the sequence (ct)t=1,··· ,T :\nc2n−1T−1 = (cT + c 2 T ) 2n−1\n=\n2n−1 ∑\nk=0\n( 2n− 1 k ) c2n−1+kT ,\nwhich implies:\n|c2n−1T−1 − (c2n−1T + (2n− 1)c2nT )| ≤ 2n2c2n+1T + 4nc2n+2T . (6)\nUsing E[WT |W1, · · · ,WT−1] = cTW1:T−1, we get:\n|E[c2n−1T W 2n1:T− T ∑\nt=1\nE[Wt|W1, · · · ,Wt−1]2n]|\n≤ |E[(c2n−1T + 2(n− 1)c2nT )W 2n1:T−1]− T−1 ∑\nt=1\nE[Wt|W1, · · · ,Wt−1]2n]|\n+ 2n2cT + 24 nc2T ≤ |E[c2n−1T−1 W 2n1:T−1 − T−1 ∑\nt=1\nE[Wt|W1, · · · ,Wt−1]2n]|\n+ (2n2c2n+1T + 4 nc2n+2T )E[W 2n 1:T−1] + 2n 2cT + 24 nc2T ≤ 4n2cT + 34nc2T ≤ 4n2 1\nT + 34n\n1\nT 2 ,\nwhere the first (resp. second) inequality is obtained by applying (5) (resp. (6)) and the fifth inequality is derived using cT ∈ [0, 1T ] and |W1:T−1| ≤ T − 1. By induction on t, we get:\n|E [\nc2n−1T W 2n 1:T −\nT ∑\nt=1\nE[Wt|W1, · · · ,Wt−1]2n ] | ≤ 4n2 T ∑\nt=1\n1 t + 34n\nT ∑\nt=1\n1\nt2\n≤ 4n2 log(T ) + 4nπ 2\n2 .\nBringing everything together, we derive:\n| ∞ ∑\nn=2\n(1 2\nn\n)\nα2nE\n[\n( ∑T t=1 Wt) 2n\nT 2n−1 −\nT ∑\nt=1\nE[Wt|W1, · · · ,Wt−1]2n ] |\n≤ 4( ∞ ∑\nn=2\n(1 2\nn\n)\nα2nn2) log(T )\n+ (\n∞ ∑\nn=2\n(1 2\nn\n)\n(2α)2n) π2\n2\n≤ 8α4( ∞ ∑\nn=2\nn(n− 1)(α2)n−2) log(T )\n+ (\n∞ ∑\nn=0\n(2α)2n) π2\n2\n≤ 8 α 4 (1 − α2)3 log(T ) + π2 2(1− 2α)\n≤ 8 α 4\n(1 − α2)3 log(T ) + π 2,\nsince α ≤ 14 . Using (4), we conclude that:\nRT (ℓ, Z̃,F) ≥ ‖z∗‖α2\n2 (1− 16 α\n2\n(1 − α2)3 ) log(T ) +O(log log(T )),\nwhich implies that RT (ℓ, Z̃,F) = Ω(log(T )) as α2(1− 16 α 2 (1−α2)3 ) > 0 for α ∈ (0, 132 ]."
    }, {
      "heading" : "4.13 Proof of Lemma 7",
      "text" : "The proof is along the same lines as for Theorem 4. We start with the same inequality:\nrT ((zt)t=1,··· ,T , (ft)t=1,··· ,T ) ≤ T ∑\nt=1\nzTt (ft − ft+1),\nand use sensitivity analysis to control this last quantity. Specifically, we show that the mapping φ : z → argminf∈F zTf is 1q−1 -Hölder continuous on Z , i.e. there exists c > 0 such that:\n‖φ(z1)− φ(z2)‖2 ≤ c ‖z1 − z2‖ 1 q−1\n2 ∀(z1, z2) ∈ Z2. Using this property, we get:\nrT ((zt)t=1,··· ,T , (ft)t=1,··· ,T ) ≤ T ∑\nt=1\n‖zt‖2 ‖ft − ft+1‖2\n= O(\nT ∑\nt=1\n∥ ∥ ∥ ∥ ∥ 1 t− 1 t−1 ∑\nτ=1\nzτ − 1\nt\nt ∑\nτ=1\nzτ\n∥ ∥ ∥ ∥ ∥\n1\nq−1\n2\n)\n= O(\nT ∑\nt=1\n∥ ∥ ∥ ∥ ∥ 1 t(t− 1) t−1 ∑\nτ=1\nzτ − 1\nt zt\n∥ ∥ ∥ ∥ ∥\n1\nq−1\n2\n)\n= O( T ∑\nt=1\n( 1\nt(t− 1)\n∥ ∥ ∥ ∥ ∥ t−1 ∑\nτ=1\nzτ\n∥ ∥ ∥ ∥ ∥\n2\n+ 1\nt ‖zt‖2)\n1\nq−1 )\n= O(\nT ∑\nt=1\n1\nt 1 q−1\n),\nfrom which we derive that rT ((zt)t=1,··· ,T , (ft)t=1,··· ,T ) = O(log(T )) if q = 2 and rT ((zt)t=1,··· ,T , (ft)t=1,··· ,T ) = O(T q−2 q−1 ) if q ∈ (2, 3]. We now move on to show that φ is 1 q−1 -Hölder continuous. Just like in Theorem 4, we can find A > 0 such that ‖z‖2 ≥ A for all z ∈ Z . Take (z1, z2) ∈ Z2 and (f1, f2) ∈ φ(z1) × φ(z2). Since we are optimizing a linear function, we may assume, without loss of generality, that C = 1 and f1 and f2 lie on the boundary of F , i.e. ‖f1‖F = ‖f2‖F = 1. By definition, we have:\n∥ ∥ ∥ ∥ f1 + f2 2 ∥ ∥ ∥ ∥ F ≤ 1− δF (‖f1 − f2‖F ).\nAs a consequence, we have:\nf1 + f2 2 − δF (‖f1 − f2‖F) z2 ‖z2‖F ∈ F .\nWe get:\nzT2( f1 + f2\n2 − δF(‖f1 − f2‖F ) z2 ‖z2‖F ) ≥ inf f∈F zT2f = z T 2f2.\nRearranging this last inequality yields:\nzT2 f1 − f2 2 ≥ ‖z2‖ 2 2 ‖z2‖F δF (‖f1 − f2‖F ),\nwhich implies that:\nzT2 f1 − f2\n2 ≥ K ‖f1 − f2‖q2 ,\nfor some K > 0 independent of z1 and z2 since Z is compact, ‖z2‖2 ≥ A > 0, ‖ ‖F is q-uniformly convex, and by the equivalence of norms in finite dimensions. By optimality of f1, we also have zT1 f2−f1 2 ≥ 0. Summing up the last two inequalities, we get:\n(z2 − z1)T f1 − f2\n2 ≥ K ‖f1 − f2‖q2 ,\nand (by Cauchy-Schwartz): ‖z2 − z1‖2 ≥ 2K ‖f1 − f2‖ q−1 2 ,\nwhich concludes the proof."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "<lb>In the convex optimization approach to online regret minimization, many methods<lb>have been developed to guarantee a O(<lb>√<lb>T ) bound on regret for subdifferentiable<lb>convex loss functions with bounded subgradients, by using a reduction to linear<lb>loss functions. This suggests that linear loss functions tend to be the hardest ones<lb>to learn against, regardless of the underlying decision spaces. We investigate this<lb>question in a systematic fashion looking at the interplay between the set of pos-<lb>sible moves for both the decision maker and the adversarial environment. This<lb>allows us to highlight sharp distinctive behaviors about the learnability of piece-<lb>wise linear loss functions. On the one hand, when the decision set of the deci-<lb>sion maker is a polyhedron, we establish Ω(<lb>√<lb>T ) lower bounds on regret for a<lb>large class of piecewise linear loss functions with important applications in on-<lb>line linear optimization, repeated zero-sum Stackelberg games, online prediction<lb>with side information, and online two-stage optimization. On the other hand, we<lb>exhibit o(<lb>√<lb>T ) learning rates, achieved by the Follow-The-Leader algorithm, in<lb>online linear optimization when the boundary of the decision maker’s decision set<lb>is curved and when 0 does not lie in the convex hull of the environment’s decision<lb>set. Hence, the curvature of the decision maker’s decision set is a determining<lb>factor for the optimal learning rate. These results hold in a completely adversarial<lb>setting.<lb>",
    "creator" : "LaTeX with hyperref package"
  }
}