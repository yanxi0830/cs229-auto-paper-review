{
  "name" : "1505.02377.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bounded-Distortion Metric Learning",
    "authors" : [ "Renjie Liao", "Jianping Shi", "Ziyang Ma" ],
    "emails" : [ "rjliao@cse.cuhk.edu.hk", "jpshi@cse.cuhk.edu.hk", "maziyang08@gmail.com", "dcszj@mail.tsinghua.edu.cn", "leojia@cse.cuhk.edu.hk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Distance metric learning is a fundamental problem in machine learning, since many learning algorithms, e.g., k-nearest neighbors (kNN) and k-means, crucially rely on a “good” metric. The criteria of good metrics may differ in various learning tasks. For instance, in supervised learning, a common criterion is to learn a metric with a low empirical error [39], while in unsupervised learning, a good criterion is to learn a metric that minimizes the intra-cluster distance and simultaneously maximizes the inter-cluster distance [41].\nIn essence, metric learning aims to search a metric embedding to convert the original metric space (e.g., Euclidean) into a new one, which better suits learning tasks with regard to the above criteria. Such an embedding intrinsically induces distortion - a concept in the theory of metric embedding [4], which intuitively measures the effort to reshape the metric space.\nAlthough a large-distorted metric space can have a high degree of freedom to describe data, it may be prone to overfitting. In fact, we will theoretically validate the intuition later by proving that in our case of Mahalanobis metric learning, the generalization bound depends increasingly on the distortion. Moreover, the numerical inaccuracy would also be a problem if the distortion is extremely large. We will show that the distortion of Mahalanobis metric learning is the condition number of the parameter matrix. Inevitably, a large distortion would make the matrix ill-conditioned.\nIn order to balance the fitness of the learned the metric space to training data and the distortion of the underlying metric embedding, we present bounded-distortion metric learning (BDML), a generic framework that imposes a bounded-distortion constraint to the learning objective. While it fits various metric learning objectives, we concentrate on learning Mahalanobis metric space, which leads to a semidefinite programming (SDP) formulation.\nar X\niv :1\n50 5.\n02 37\n7v 1\n[ cs\n.L G\n] 1\n0 M\nWe approach the SDP via a bisection method, which involves solving a sequence of convex feasibility problems with fast multiplicative weights update [23]. Moreover, to deal with the pseudo-metric learning, we apply the spectral decomposition to the parameter matrix and perform joint learning of dimension reduction mapping and metric. We relax the resultant non-convex quadratic constrained quadratic programming (QCQP) to a SDP and achieve the approximation by a randomized algorithm. Theoretical analysis is provided to reveal that distortion has a direct impact on the stability and generalization ability of a class of metric learning algorithms. Experimental results on several benchmark datasets manifest the usefulness of our BDML."
    }, {
      "heading" : "2 Related Work",
      "text" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.\nBased on the type of constraints, we can also classify them into pairwise and triplet-wise ones. Pairwise methods [41, 7] often adds constraints to enforce distances between pairs of dissimilar points are larger than a given threshold. Representative methods in the triplet group are the largemargin nearest neighbor [39] and its variants [24]. They exploit the local triplet constraints to assure that the distance between any point and its different-class neighbour should be at least one unit margin further than the distance between it and its same-class neighbour. Intuitively, if these triplet constraints are well satisfied, the empirical loss of kNN would be small.\nMetric learning is closely related to metric embedding, an important topic in theoretical computer science that has played an important role to design approximation algorithms. One line of research focuses on how to embed a finite metric space into normed spaces with a low distortion [4, 19], i.e., preserving the structure of the original metric space. Metric learning is also related to manifold learning [14] and kernel learning [26]. Learning a distance metric function amounts to learning a kernel function that measures the similarity between points."
    }, {
      "heading" : "3 Bounded-Distortion Metric Learning",
      "text" : "Before going to details, we introduce necessary notations. Sd = {M |M ∈ Rd×d,M> = M} is the space of all d× d real symmetric matrices equipped with the Frobenius inner product A • B = Tr(A>B). The positive semidefinite (PSD) cone and positive definite (PD) cone are denoted as Sd+ = {M |M ∈ Sd,M 0} and Sd++ = {M |M ∈ Sd,M 0} respectively. The convex set PdR = {M |M ∈ Sd++,Tr(M) ≤ R} is also used. The trace bound R in PdR is a parameter to ensure a bounded domain for M . Rd+ denotes the d-dimensional nonnegative orthant."
    }, {
      "heading" : "3.1 Distortion of Metric Embedding",
      "text" : "A definition of metric space is the following. Definition 1. A pair (X , dX ) is called a metric space, where X is a set of points and dX : X ×X → [0,∞) is a distance function satisfying the following conditions for all xi, xj , xk ∈ X :\n• dX (xi, xj) = 0, iff xi = xj ,\n• dX (xi, xj) = dX (xj , xi),\n• dX (xi, xj) + dX (xj , xk) ≥ dX (xi, xk).\nA Mahalanobis metric space is a metric space equipped with a Mahalanobis distance function, which often takes the form of dX (xi, xj) = √ (xi − xj)>M(xi − xj), parameterized by a PD matrix M , i.e., M ∈ Sd++. In the metric learning literatures, a PSD M is usually adopted, thus the induced distance function is a pseudo-metric in the strict sense. We now focus on the PD case and defer the PSD one to Sect. 5. Obviously, the Euclidean space is a special Mahalanobis metric space where M is an identity matrix. Note that we deal with the squared Mahalanobis distance since it does not affect learning methods (e.g., kNN) that are based on relative distances.\nWe can embed one metric space into another with a certain degree of distortion. The formal definition of metric embedding and its distortion are as follows [4, 19], Definition 2. Let (X , dX ) and (Y, dY) be two metric spaces. A mapping f : X → Y is said to be a c-embedding if there exists r > 0 such that for all x, y ∈ X ,\nr · dX (x, y) ≤ dY(f(x), f(y)) ≤ cr · dX (x, y). (1)\nThe distortion of f is defined as the infimum of all c such that f is a c-embedding.\nDistortion is a measure of the distance between two metric spaces and plays an important role in the theory of metric embeddings. Later we will show that distortion is essential to stabilize a class of metric learning algorithms.\nIn Mahalanobis metric learning, given an Euclidean metric space (X , dI), we learn a metric embedding fI→M , which returns us a desired Mahalanobis metric space (X , dM ). We have the following proposition to specify the distortion of this metric embedding. Proposition 1. The distortion of the metric embedding fI→M is the condition number κ(M).\nDue to the page limit, we focus on presenting important results in this paper and defer all proofs to the appendix in the supplementary file."
    }, {
      "heading" : "3.2 Geometric Meaning of Distortion",
      "text" : "Distortion can be intuitively regarded as a complexity measure of metric embedding. From a geometric perspective, it possesses an intrinsically different meaning compared to other complexity measures in previous work, including the log determinant (logDet) [7] and Frobenius norm (Fnorm) [21]. Specifically, we focus on analyzing the metric embedding fI→M and consider an origin-centered ellipsoid E = {x ∈ Rd|x>Mx ≤ 1} for simplicity. Let {λi}di=1 be the set of eigenvalues of M . It is well-known that the logarithmic volume of E is log(V (E)) = log(γ) − 12 logDet(M), where γ is the volume of the unit sphere in R\nd. The squared F-norm of M is defined as ‖M‖2F = ∑d i=1 1/r 4 i , where ri = 1/ √ λi is the length of the i-th semi-axis. The condition number is κ(M) = r2max/r 2 min. In other words, logDet measures the volume variation; F-norm indicates the change of overall lengths of semi-axes; while the condition number describes the length ratio variation between the longest and the shortest semi-axis. As illustrated in Fig. 1, it is possible that a metric with a small logDet or F-norm value is ill-conditioned, i.e., the ellipsoid is extremely elongated. Therefore, rather than focusing on the absolute variation, distortion measures the relative one, thus enabling higher freedom and directly controling the wellconditioning property."
    }, {
      "heading" : "3.3 Pair and Triplet Constrained BDML",
      "text" : "Given a training dataset D = {(xi, yi)}Ni=1, where xi ∈ Rd is a data point and yi is the corresponding class label, our task is to obtain a Mahalnobis distance metric space (X , dM ), where dM is the distance function defined as, dM (xi, xj) = (xi − xj)>M(xi − xj) = M • Xij , and M ∈ Sd++. Here we define Xij = (xi − xj)(xi − xj)> for notation simplicity.\nAlgorithm 1 : A Bisection Method 1: Given the interval of g∗ as [L,U ], tolerance > 0, 2: Repeat 3: ḡ = (L+ U)/2. 4: Solve the convex feasibility problem (4). 5: If problem (4) is feasible: U = ḡ. 6: Else: L = ḡ. 7: Until U − L ≤ 8: Return the final objective value as ḡ.\nWe now present two notable formulations of our bounded-distortion metric learning (BDML), which correspond to two types of constraints in the literature of metric learning, i.e., pairwise ones and triplet-wise ones. Specifically, for any data point xi, we consider its k-nearest neighbours set Ωi. Following [39], two types of neighbor points are distinguished. They are target neighbors that share the same class label with xi, and imposter neighbors that have different class labels with xi.\nLet S = {(i, j)|xj ∈ Ωi, yj = yi} and I = {(i, j)|xj ∈ Ωi, yj 6= yi} be the sets of all index pairs of target neighbors and impostor neighbors respectively. T = {(i, j, k)|(i, j) ∈ S, (i, k) ∈ I} denotes the set of all such index triplets.\nWe define the pair-constrained bounded-distortion metric learning (p-BDML) as\nmin M∈PdR\n1\nn ∑ (i,j)∈S M •Xij\ns.t. M •Xij ≥ µ, ∀(i, j) ∈ I, κ(M) ≤ K, (2)\nwhere n = |S| and K is a parameter to control the upper bound of distortion. Pair-wise constraints are designed to pull two imposter neighbors farther than a margin or push two target neighbors closer than a margin in literatures. Here we only consider the former purpose and minimize the average distance of target neighbors as in [41].\nThe triplet-constrained bounded-distortion metric learning (t-BDML) is formulated as,\nmin M∈PdR\n1\nn ∑ (i,j)∈S M •Xij\ns.t. M •Xik −M •Xij ≥ µ, ∀(i, j, k) ∈ T , κ(M) ≤ K. (3)\nThe above inequality constraint of triplet nearest neighbors ensure that any given point has its impostor neighbors at least one unit margin farther than its target neighbors. Note that the unit margin in [39] can be set as a arbitrarily positive constant, since it only affects the scale of M . In our case, since we consider a bounded domain of M (i.e., M ∈ PdR), the margin µ is treated as a positive parameter.\nNote that the bounded-distortion constraint κ(M) ≤ K implies thatM should be PD since otherwise κ(M) is unbounded. Albeit nonconvex, the condition number function is quasi-convex. It means all its sublevel sets are convex. This property enables us to transform p-BDML and t-BDML to the standard formulation of SDP."
    }, {
      "heading" : "4 A Bisection Algorithm with Multiplicative Weights Update",
      "text" : "We now present a bisection algorithm for approaching our p-BDML and t-BDML, which essentially solves a sequence of convex feasibility problems. For each feasibility problem, we resort to the multiplicative weights update (MWU) method [23, 1], which is a meta algorithm and has many variants in different disciplines. The reason of choosing MWU is that it generates an approximate solution with guaranteed constraint violation – it is important for the analysis in Sec. 6 to hold."
    }, {
      "heading" : "4.1 Sequential Convex Feasibility Problems",
      "text" : "In what follows, we only describe the convex feasibility problem for p-BDML, since the formulation for t-BDML only differs in constraints. We denote the objective function as g(M) = G •M where G = 1n ∑ (i,j)∈S Xij . Its optimal value g\n∗ is assumed to lie in the initial interval [L,U ], where L and U are set as 0 and g(I) respectively. I is the identity matrix. Our bisection algorithm estimates g and narrows down the interval by half in each iteration. The procedure of the bisection algorithm is outlined in Alg. 1.\nSpecifically, if g is not larger than ĝ in one iteration, we solve a convex feasibility problem as\nfind M ∈ PdR, α > 0 s.t. G •M ≤ ḡ,\nM •Xij ≥ µ, ∀(i, j) ∈ I, αI M αKI. (4)\nHere we introduce a positive auxiliary variable α and transform the bounded-distortion constraint into two generalized inequality constraints. The resultant convex feasibility problem can be approximately solved by the efficient MWU method, to be elaborated on in the next section."
    }, {
      "heading" : "4.2 Multiplicative Weights Update Method",
      "text" : "Before applying MWU method, we reformulate the feasibility problem (4) to a general form via introducing slack variables M1 = M − αI and M2 = αKI −M . Then we construct a sparse symmetric matrix Y ∈ P3d+1R of which the block diagonal entries are M , M1, M2 and α. All constraints except P3d+1R in (4) are rewritten as Ji • Y ≥ hi. P 3d+1 R can be deemed as an easy constraint, contrary to each hard constraint Ji • Y ≥ hi. With this change, we obtain the equivalent formulation of (4) as\nfind Y ∈ P3d+1R s.t. Ji • Y ≥ hi, ∀i = 1, ...,m. (5)\nThe number of constraints is m = |I| + 4d2 + 2. We also introduce a closely related feasibility problem as\nfind Y ∈ P3d+1R s.t. ∑m i=1 pi (Ji • Y − hi) ≥ 0. (6)\nHere p = [p1, ..., pm]> is a probability vector, i.e., ∀i, pi ≥ 0 and ∑m i=1 pi = 1. Note that this problem only contains 2 constraints, easy to solve. The relationship between problem (5) and (6) is summarized by the following lemma. Lemma 1. If problem (5) has a feasible solution Y ∗, given any probability p, then Y ∗ is feasible for problem (6). Equivalently, if there exists a probability p such that problem (6) is infeasible, then problem (5) is infeasible.\nWith the lemma, we now describe the MWU method for approaching problem (5). Basically, MWU maintains a weight vector w ∈ Rm+ , where each entry wi represents the importance of the i-th constraint. It iteratively solves problem (6) and updates weights w according to the constraint satisfaction Ji • Y − hi. Intuitively, if one constraint is more satisfied, the corresponding importance should be less and we should decrease its weight.\nIn t-th round of the algorithm, we get a probability vector p(t) by normalizing the nonnegative weights w(t). Then we solve the 2-constraint feasibility problem (6) by maximizing∑m i=1 pi ( Ji • Y (t) − hi ) over P3d+1R . If the maximum value is greater than 0, we take the corresponding Y (t) as a feasible solution. Otherwise (6) is infeasible. We call the solver of problem (6) as an ORACLE. Implementing ORACLE needs to compute the largest eigenvector of the matrix C =∑m i=1 pi[Ji − (hi/R)I], which can be efficiently handled by Lanczos algorithm. Here R is the trace bound parameter in P3d+1R .\nAlgorithm 2 : Multiplicative Weights Update Method\n1: Initialization: Fix a ε ≤ 1/2, for each constraint, associate the weight w(1)i = 1. 2: For t = 1, 2, ..., T : 3: Normalize w(t) to get the probability vector p(t). 4: Call the ORACLE with p(t). 5: If ORACLE succeeds to find a solution Y (t) 6: η(t)i = 1 ρ [Ji • Y\n(t) − hi]. 7: w(t+1)i = w (t) i (1− εη (t) i ). 8: Else 9: Return that the problem is infeasible.\n10: End 11: End 12: Return Ȳ = ( ∑T t=1 Y (t))/T as a final solution.\nAssuming ORACLE obtains a feasible solution Y (t), we denote the normalized satisfaction of i-th constraint of problem (5) as η(t)i = 1 ρ [Ji •Y\n(t)−hi]. Here ρ is called the width parameter, satisfying that ∀i, ∣∣Ji • Y (t) − hi∣∣ ≤ ρ. We update the weights as w(t+1)i = w(t)i (1 − εη(t)i ), where ε is a parameter smaller than 1/2. Hence, w(t+1)i is smaller than w (t) i if η (t) i > 0, and its value increases otherwise.\nThe algorithm is depicted in Alg. 2. After T rounds, the averaged solution Ȳ = ( ∑T t=1 Y\n(t))/T is returned. We have the Theorem 1 following [1] to guarantee that either Ȳ achieves a predefined accuracy or we claim that the original problem (5) is infeasible.\nTheorem 1. Let δ > 0 be a given additive error1. Alg. 2 either solves problem (5) up to δ, or correctly concludes that it is infeasible, making O(ρ\n2 ln(m) δ2 ) calls to the ORACLE."
    }, {
      "heading" : "5 Pseudo-Metric & Dimension Reduction",
      "text" : "In this section, we deal with the case of pseudo-metric, i.e., M is PSD. In the literature of Mahalanobis metric learning, a PSD M is beneficial due to the existence of decomposition M = C>C, where C ∈ Rq×d. The distance function could be rewritten as d(x, y) = ||Cx − Cy||2. It thus removes the PSD constraint and allows flexible dimension reduction by choosing q < d.\nIn our setting, a PSD M could be problematic if it has an unbounded condition number. According to the spectral theorem, decomposition M = Q>ΛQ is applicable, where Λ ∈ Rq×q is a diagonal matrix with eigenvalues of M , Q ∈ Rq×d has orthonormal rows and q is the rank of M . We can also choose different q to form different low rank approximation of M . Hence Q and Λ act as a dimension-reduction mapping and a dimension-wise scaling operation respectively.\nBy replacingM with the decomposition and adding orthogonal constraints forQ in original BDML, we can conduct the pseudo-metric learning via alternatively optimizing Q and Λ. Specifically, when Q is fixed, optimizing the diagonal Λ is just a simple case of original BDML. Nevertheless, optimizing Q with a known Λ is not straightforward. Especially, in the case of p-BDML, when Λ is fixed such that Λ ∈ P and κ(Λ) ≤ K, the problem (2) can be reformulated as below,\nmin Q∈Rq×d\n1\nn ∑ (i,j)∈S Xij • (Q>ΛQ)\ns.t. Xij • (Q>ΛQ) ≥ µ, ∀(i, j) ∈ I, QQ> = I. (7)\nNote this learning problem is nontrivial due to the fact that optimizing Q is a quadratic constrained quadratic programming (QCQP). To overcome the difficulty, we have the following proposition,\n1Additive error up to δ means that, any constraint is violated at most δ, i.e., ∀i, Ji • Y − hi ≥ −δ.\nAlgorithm 3 : Gaussian Randomization Procedure 1: Initialization: Given the optimal solution Q̃∗, iteration number T ′, ratio γ and tolerance . 2: For t = 1, 2, ..., T ′: 3: Sample ξt ∼ N (0, Q̃∗). 4: End 5: ξ = arg minξt 1 n ∑ (i,j)∈S ξ > t X̃ijξt.\n6: Reshape ξ from Rqd×1 to Rq×d. 7: Return ξ as the approximate solution of problem (7).\nProposition 2. Problem (7) can be relaxed to a SDP as following,\nmin Q̃∈Sqd+\n1\nn ∑ (i,j)∈S X̃ij • Q̃\ns.t. X̃ij • Q̃ ≥ µ, ∀(i, j) ∈ I, Auv • Q̃ = buv, ∀(u, v) ∈ C, (8)\nwhere X̃ij = Xij ⊗ Λ and ⊗ stands for Kronecker product. Auv is a block diagonal matrix which contains d identical blocks Buv ∈ Rq×q . (u, v) and (v, u)-th entries of Buv are 1 while others are 0. buv = 2 if u = v, otherwise buv = 0. C = {(u, v) ∈ [q]× [q]|u ≤ v} and [q] = {1, . . . , q}.\nThis proposition shows that when Λ is fixed we can learn Q by solving the above SDP relaxation. Moreover, since the equality constraints in (8) imply Tr(Q̃) = q, we thus can exploit the MWU method again.\nDenoting the optimal objective values of problem (7) and (8) as %qp and %sdp respectively, it is obvious that %sdp ≤ %qp. Hence we aim at up-bounding %qp. Specifically, once we obtained the optimal solution Q̃∗ of problem (8), we construct an approximate solution ξ problem (7) based on a Gaussian randomization procedure shown in Alg. 3. We prove the following theorem to assure that in the worst case Alg. 3 would possibly generate an approximate solution with approximation ratio ω. Theorem 2. If the optimal solution of problem (8) is Q̃∗ and ξ ∈ Rqd is a random vector generated from the real-valued normal distribution N (0, Q̃∗), then for any γ > 0, ≥ 0 and ω ≥ 1, we have,\nProb ( ν ≥ γµ & ζ ≤ & ξ>G̃ξ ≤ ωG̃ • Q̃∗ ) ≥ 1− |I|max ( √ γ,\n2(r − 1)γ π − 2 ) − r exp ( −1 2 ( ω − √ 2ω − 1 )) − rq(q + 1) 2 [ exp ( − (τ − 1) 2 4 ) + exp ( − 2 8rdq2 )] ,\nwhere r = rank(Q̃∗) and τ = √\nq ( 2 rd )1/2 + 1. ν, ζ and G̃ are defined respectively as ν =\nmin(i,j)∈I ξ >X̃ijξ, ζ = max(u,v)∈C |ξ>Auvξ − buv| and G̃ = 1n ∑ (i,j)∈S X̃ij .\nRemark. This theorem indicates that with well chosen γ and , Alg. 3 can generate an approximate solution for (7) with guaranteed approximation ratio even in the worst case. For example, we can consider a real case with q = 10, d = 100 and the number of constraints |I| = 100. By choosing γ = π/16|I|2, = 40q √ rd and with appropriate rank reduction on Q̃∗ as [30], it can be shown that after running Alg. 3 for 100 iteration, we have very high probability2 such that 1ω%qp ≤ %sdp ≤ %qp, where ω = 10. However, the price we pay is that the orthogonal constraints are loosely satisfied. In practice, we found that the resultant approximate solution works well, which indicates that removing orthogonal constraints and transforming Λ to a full rank matrix may also be an alternative modeling choice.\nAs for the pseudo-metric learning of t-BDML, we can still use the above algorithm to obtain an approximate solution. However, Theorem 2 does not stand in this case since not all X̃ij of t-BDML are PSD.\n2The probability is at least 0.999828."
    }, {
      "heading" : "6 Generalization Bound for BDML",
      "text" : "To theoretically investigate whether the distortion has an impact on the generalization ability, we derive the generalization bound of our BDML following the stability analysis of learning algorithms [5, 36].\nBefore diving into the details, we first introduce one assumption that we only consider the case where the metric matrix is full rank. Then we clarify some preliminary notations. Each training sample z inside the training set D is drawn i.i.d. from some unknown distribution D. And the range of z is denoted as Z . Di is a perturbed set of D obtained via replacing i-th sample with a new sample drawn from D, i.e., Di = {D\\zi ∪ z′i}, where z′i ∼ D. We make a mild assumption that all data points are contained in a Γ-ball, i.e., ‖x‖2 ≤ Γ. We denote the learning algorithm asA, the true risk or generalization error as R(A, D) and the empirical risk as Remp(A, D) = 1n ∑ k `(A, zk), where in our case the loss function ` = M •Xij and n = |S|. Based on [5], we define the UniformReplace-One stability as, Definition 3. An algorithmA has Uniform-Replace-One stability β with respect to the loss function ` if ∀D ∈ Zn, ∀i ∈ {1, ..., n},\n||`(AD, ·)− `(ADi , ·)||∞ ≤ β. (9)\nHere AD means the learning algorithm A is trained on the dataset D. Note that our definition is stronger than the one proposed in [36], thus being more restrictive. We have the following lemma, which specifies the uniform-RO stability of our BDML. Lemma 2. The Uniform-Replace-One stability of our BDML algorithm with respect to the given loss function ` is β = 4(K+1)RΓ 2\nd .\nThis lemma indicates that the Uniform-Replace-One stability of A is positively correlated with the bound of distortion K. It means a low distortion of metric embedding would lead to a stable algorithm. Although β does not depend decreasingly on the number of samples n, which may not be seen as stable in some sense [5], it is clear that the stability can be controlled by the distortion. With this stability result, we further prove the following generalization bound, which theoretically explains the relationship between distortion and generalization error. Theorem 3. For any metric learning algorithm A with Uniform-Replace-One stability β with respect to the given loss function `, we have with probability at least 1− δ,\nR(A, D) ≤ Remp(A, D) + 2Γ\n√ KR\ndδ\n( 2KRΓ2\nnd + 3β\n) .\nSpecifically, for our BDML algorithm,\nR(A, D) ≤ Remp(A, D) + 2RΓ2\nd\n√ 2K\nδ\n( K\nn + 6K + 6\n) .\nRemark. There are several interesting things to note regarding to this theorem.\nFirst, it explains our intuitive conjecture that a large distortion would incur overfitting during metric learning. It encourages us to choose a small value of K to improve the generalization ability of A. On the other side, setting K as small as possible is unwise, since it would constrain our hypothesis class too much and thus may increase both the true risk and empirical risk. Therefore, it suggests choosing moderately small values of K in practice via cross validation.\nSecond, the generalization error tends to decrease with the increase of the dimension d. This phenomenon seems to be a bit counter-intuitive since it implies that our method becomes more stable in higher dimensional feature space. However, this does happen only if the previous assumption holds, i.e., M is full rank. In this case, the increase of the dimension squash the range of the spectrum of M since the trace bound R and distortion bound K are fixed. If M is instead rank-deficient, the above analysis does not stand. In particular, the bound will have a dependency on the rank of M and its perturbation. This means that naively increasing the feature dimension by adding zero will not make the bound tighter. We will illustrate this issue in the appendix."
    }, {
      "heading" : "7 Experiments",
      "text" : "We present empirical evaluations of our BDML algorithm on a wide range of tasks, including classification on several UCI datasets [2], domain adaptation on medium-scale datasets [35], and face verification on the large-scale LFW dataset [18]. Before presenting the results, we first discuss a practical strategy to speed up the bisection method, since it is sometimes hard to estimate a tight interval of the optimal objective value in advance. Specifically, we select several fixed upper bounds and then solve the convex feasibility problem (4) in parallel. If the trial is successful, we use it to shrink the upper bound, otherwise we shrink the lower bound. This procedure provides us a largely reduced interval with time cost as small as one call of MWU solver. Note that we set parameters via cross validation. The impact of different parameters and runtime are provided in the appendix due to space limits."
    }, {
      "heading" : "7.1 Classification",
      "text" : "We first conduct classification experiments on several UCI datasets, including Wine, Iris, Diabetes, Segment and Waveform, to validate the effectiveness of our BDML. We randomly split datasets into 70% for training and 30% for testing and report the average test errors and standard deviations by repeating the random splits for 10 times. We compare with the baseline of Euclidean metric and several strong competitors like Xing [41], LMNN [39], ITML [7] and BoostMetric [38]. The neighborhood size of kNN classifier is 3 and all metric M are initialized as the identity matrix. We carefully set other parameters for these methods via cross validation. The results are listed in Table. 1, in which the best ones are bolded. Both our p-BDML and t-BDML perform well on these datasets. Especially, t-BDML is consistently better than p-BDML which validates the effectiveness of triplet constraints as suggested by [39].\nWe then demonstrate how the performance of the kNN classifier varies according to the condition number of the learned metric in Fig. 2. The x-axis is the natural logarithm of the condition number. It is clear from the figure that, the average test errors of both p-BDML and t-BDML first decrease and then increase when the condition numbers become larger. These results provide strong evidence to support our previous analysis that a largely distorted metric space leads to overfitting and a small distortion may result in underfitting. And our BDML effectively controls the distortion, thus improving the generalization ability."
    }, {
      "heading" : "7.2 Domain Adaptation",
      "text" : "We also apply our BDML to domain adaptation problems, under both unsupervised and semisupervised settings. In the former case, we labeled samples in a source domain for training and want to test the unlabeled samples in the target domain. While in the later setting, apart from labeled samples in a source domain, a small number of labeled samples in the target domain are also accessible during training. We use the same dataset as in [35], which contains 2,533 images of 10 categories from 4 domains: Caltech, Amazon, Webcam, and Dslr. We exploit the same 10 categories as [10] to all the four domains. Experiments are repeated with 20 fixed train/test splits offered by [35]. We set the number of neighbors of kNN to 1 as other methods. Since the original SURF feature is of 800 dimension, we perform pseudometric learning with t-BDML and initialize the dimension-reduction mapping via PCA. The size of mapping matrix is set to 30× 800 according to cross-validation.\nTable 2 presents the mean test accuracy and standard errors of various metric learning based methods, including LMNN [39], ITML [35], SGF [11], GFK [10] and DML-eig [42], where A→C means the adaptation from source domain A (i.e., Amazon) to the target domain C (i.e., Caltech). And for fair competition, we adopt the best results of GFK under the PCA subspace setting reported by [10]. The results in the original Euclidean space are denoted as OrigFeat. In most subtasks of these two settings, our t-BDML outperforms than other competitors which demonstrates the strength of the proposed pseudo-metric learning scheme."
    }, {
      "heading" : "7.3 Face Verification",
      "text" : "Finally, we apply our BDML to an unconstrained face verification task, using the large-scale LFW dataset that contains 13,233 face images of 5,749 people. It is challenging due to the large variations of faces in illumination, expression, pose, resolution, etc. There are 6 standard protocols [18] for evaluating results. We use the setting called “Image-Restricted, Label-Free Outside Data”, where we can only access the provided labeled pairs of faces during training. Thus we only compare pseudometric learning of p-BDML since using triplet constraints would violate this setting. The dateset is organized in 10 folders and each of them contains 300 similar pairs of faces and 300 dissimilar ones. The reported accuracy is obtained via cross validation on the provided 10 folds.\nCurrent state-of-the-art methods under this setting often build various classifiers and combine multiple types of visual descriptors. Since we primarily aim at validating the effectiveness of BDML, we do not carry out intensive feature engineering or build complex similarity measurements. Instead,\nwe use the public “funneled” SIFT feature3 and regard the learned distance metric as the similarity measure. Since the dimension of raw SIFT feature is too large (≈ 4k), we reduce it to 800 via PCA before pseudo-metric learning with p-BDML and set the size of dimension-reduction mapping as 300× 800. In Table 3, we present results of p-BDML and other metric learning based methods including Xing [41], ITML [7], LDML [12], KISSME [25], DML-eig [42], Sub-ITML and Sub-ML [6]. We also report the average condition number. The ITML methods optimize a logDet regularizer, which yield condition numbers close to one. It suggests that the metric space is not sufficiently distorted for good generalization performance. While the F-norm based regularization methods (e.g., Sub-ML) yield too large distortion, which is not good for generalization. These results support our theoretical analysis in Sec. 6 again. In contrast, our BDML method obtains an appropriate condition number, which results in decent generalization performance. Full comparison (including ROC curves) with other non-distance based methods is presented in the appendix."
    }, {
      "heading" : "8 Conclusions",
      "text" : "In this paper, we propose the bounded-distortion metric learning (BDML), which well-balances the fitness to data and the distortion of metric embedding. For Mahalanobis metric space, BDML leads to a bounded condition number metric learning method, which possesses intriguing properties. We propose an efficient learning algorithm and further provide theoretical analysis, which explains why the distortion is a key ingredient to ensure good generalization ability. Also, we generalize to pseudo-metric learning and propose an approximate solver based on the semidefinite relaxation and a randomised algorithm. Empirical results validate that our BDML leads to both better generalization and well-conditionness. In future, we would like to extend the distortion to non-Mahalanobis metric and design corresponding approximation algorithms.\n3http://lear.inrialpes.fr/people/guillaumin/data.php"
    }, {
      "heading" : "9 Appendix",
      "text" : ""
    }, {
      "heading" : "9.1 Proofs in Section 3",
      "text" : "Proof of Proposition 1.\nProof. Note that for any two different points x, y, we have,\ndM (f(x), f(y))\ndI(x, y) = (x− y)>M(x− y) (x− y)>(x− y) .\nIt is easy to see that above equation is the Rayleigh quotient of the PSD matrix M . Therefore,\nλmin ≤ dM (f(x), f(y))\ndI(x, y) ≤ λmax,\nwhere λmin and λmax are the minimum and maximum eigenvalue of M respectively. According to the Definition 2, setting r = λmin, we can find that for any c ≥ λmaxλmin , it is always true,\nr · dX (x, y) ≤ dY(f(x), f(y)) ≤ cr · dX (x, y).\nHence the distortion of the metric embedding fI→M is inf{c|c ≥ λmaxλmin } = κ(M).\nQ.E.D."
    }, {
      "heading" : "9.2 Proofs in Section 4",
      "text" : "Proof of Lemma 1.\nProof. If Y ∗ is a feasible solution of problem (5), then Ji • Y ∗ ≥ hi,∀i. Since ∀i, 0 ≤ pi ≤ 1, we have ∑m i=1 pi (Ji • Y ∗ − hi) ≥ 0. Hence, Y ∗ is also a feasible solution of problem (6).\nIf there exists a probability vector p such that the problem (6) is infeasible, then for all Y ∈ P3d+1R , we have ∑m i=1 pi (Ji • Y − hi) < 0. Therefore, the original problem (5) is also infeasible, since\notherwise there exists a solution Ỹ such that Ỹ ∈ P3d+1R and ∑m i=1 pi ( Ji • Ỹ − hi ) ≥ 0.\nQ.E.D.\nProof of Theorem 1.\nProof. In the t-th round, we run the ORACLE with a probability distribution p(t) as input.\nIf the ORACLE declares the problem (6) is infeasible, then due to Lemma. 1, the original problem is correctly concluded as infeasible.\nOn the other hand, if this situation never happens during the iteration, i.e., for any round t, ORACLE succeeds to find a solution Y (t) to problem (6), then we can get the following inequality, ∀t = 1, 2, ..., T ,\nm∑ i=1 p (t) i η (t) i = 1 ρ m∑ i=1 p (t) i ( Ji • Y (t) − hi ) ≥ 0,\nThen, equipped with the above inequality and Theorem 2 in [23], we have, for any constraint i,\n0 ≤ T∑\nt=1\nη (t) i + ε T∑ t=1 ∣∣∣η(t)i ∣∣∣+ ln(m)ε = 1\nρ T∑ t=1 ( Ji • Y (t) − hi ) + ε\nρ T∑ t=1 ∣∣∣Ji • Y (t) − hi∣∣∣+ ln(m) ε\n= (1 + ε)\nρ T∑ t=1 ( Ji • Y (t) − hi ) + 2ε\nρ ∑ t∈T− ∣∣∣Ji • Y (t) − hi∣∣∣+ ln(m) ε\n≤ (1 + ε) ρ T∑ t=1 ( Ji • Y (t) − hi ) + 2εT + ln(m) ε .\nHere T− denotes the set of index t when Ji • Y (t) − hi < 0. Divided by T on both sides of the above inequality and with some rearrangement, we can obtain,\nJi •\n( 1\nT T∑ t=1\nY (t) ) − hi ≥ − ρ\n1 + ε\n( 2ε+ ln(m)\nεT\n) .\nNote that Ȳ = ( ∑T t=1 Y (t))/T is the solution returned by the multiplicative weights update method. Now, if we set ε = C1δρ , T = C2ρ 2 ln(m) δ2 , where C1, C2 are positive real numbers, then we have,\n− ρ 1 + ε\n( 2ε+ ln(m)\nεT\n) = − ρ\nρ+ C1δ (2C1 +\n1\nC1C2 )δ.\nWith some calculation, we can find that if we choose 0 < C1 < 1/2 and set C2 = − 1C1(2C1−1) , e.g., C1 = 1/4, C2 = 8, then Ji • Ȳ − hi ≥ −δ/(1 + δ4ρ ) ≥ −δ for any constraint i.\nQ.E.D."
    }, {
      "heading" : "9.3 Proofs in Section 5",
      "text" : "Proof of Proposition 2.\nProof. We first restate problem (7) as below,\nmin Q∈Rq×d\n1\nn ∑ (i,j)∈S Xij • (Q>ΛQ)\ns.t. Xij • (Q>ΛQ) ≥ µ, ∀(i, j) ∈ I, (10a) QQ> = I. (10b)\nWe can denote the vectorization of Q as ζ and rewrite the above problem as the following standard formulation of quadratic constrained quadratic programming (QCQP),\nmin ζ∈Rqd×1\n1\nn ∑ (i,j)∈S ζ>X̃ijζ\ns.t. ζ>X̃ijζ ≥ µ, ∀(i, j) ∈ I, (11a) ζ>Auvζ = buv, ∀(u, v) ∈ C, (11b)\nwhere the set of margin constraints in (10a) corresponds to the one in (11a) and the set of orthogonal constraints in (10b) corresponds to the one in (11b). Specifically, X̃ij = Xij ⊗ Λ and ⊗ stands for Kronecker product. And we denote the index set of the upper triangular part of a q-dimensional\nsquared matrix as C = {(u, v) ∈ [q]× [q]|u ≤ v} where [q] = {1, 2, . . . , q}. Then for each element (u, v) of C, we have Auv is a block diagonal matrix which contains d identical blocks Buv ∈ Rq×q as following,\nAuv =  Buv Buv . . .\nBuv  where\nu v\nBuv = \n...\n. . . ... 1 . . . . . . 1 ... . . .\n...\n u v\ni.e., (u, v) and (v, u)-th entries of B are 1 while others are 0. And buv = 2 if u = v, otherwise buv = 0.\nBased on [29], to derive the SDP relaxation, we can easily observe the following,\nζ>X̃ijζ = Tr(X̃ijζζ >) = X̃ij • ζζ>.\nThus setting Q̃ = ζζ>, we can write the equivalent form of the above QCQP as below,\nmin ζ∈Rqd×1\n1\nn ∑ (i,j)∈S X̃ij • Q̃\ns.t. X̃ij • Q̃ ≥ µ, ∀(i, j) ∈ I, Auv • Q̃ = buv, ∀(u, v) ∈ C, rank(Q̃) = 1.\nBy removing the last rank constraint, we can obtain the desired SDP relaxation.\nQ.E.D.\nRemark. Note that the orthogonal constraints in the original QCQP problem imply that Tr(Q̃) = q in the SDP relaxation problem.\nBefore obtaining our Theorem 2, we need to introduce three lemmas as below. First, we state the Lemma 1 in [30] as below which gives the polynomial tail bound of the left-side inequality ξ>Hξ < γE[ξ>Hξ]. Reader can refer to the paper for details of the proof. Lemma 3 (Left-side Polynomial Tail Bound). LetH ∈ Sd+, Z ∈ Sd+. Suppose ξ ∈ Rd is a random vector generated from the real-valued normal distribution N (0, Z). Then for any γ > 0,\nProb ( ξ>Hξ < γE[ξ>Hξ] ) < max{√γ, 2(r̄ − 1)γ\nπ − 2 },\nwhere r̄ = min{rank(H), rank(Z)}.\nThen we derive a right-side exponential tail bound as below. Lemma 4 (Right-side Exponential Tail Bound). Let H ∈ Sd+, Z ∈ Sd+. Suppose ξ ∈ Rd is a random vector generated from the real-valued normal distribution N (0, Z). Then for any γ ≥ 1,\nProb ( ξ>Hξ > γE[ξ>Hξ] ) < r̄ exp ( −1\n2\n( γ − √ 2γ − 1 )) ,\nwhere r̄ = min{rank(H), rank(Z)}.\nProof. Let r = rank(Z). Since Z ∈ Sd+, then we can write Z = UU> for some U ∈ Rd×r. We consider the eigen decomposition of the real symmetric matrix U>HU = LDL> = ∑r i=1 λilil > i , where L = [l1, l2, . . . , lr] ∈ Rr×r is an orthogonal matrix and D = diag{λ1, λ2, . . . , λr} with λ1 ≥ λ2 ≥ · · · ≥ λr ≥ 0. Note that λi = 0 for all i > r̄ due to the fact that U>HU has rank at most r̄. Denoting ξ̄ ∼ N (0, Ir), then we can easily check that Uξ̄ ∼ N (0, Z), i.e., Uξ̄ is statistically identical to ξ.\nTherefore, we have that, Prob ( ξ>Hξ > γE[ξ>Hξ] ) = Prob ( ξ̄>U>HUξ̄ > γE[ξ̄>U>HUξ̄] ) = Prob\n( r̄∑ i=1 λi(l > i ξ̄) 2 > γE[ r̄∑ i=1 λi(l > i ξ̄) 2] ) Denoting ui = l>i ξ̄, we have that E[ui] = 0 and E[u2i ] = 1, i.e., ui is a standard normal variable. Hence,\nProb ( ξ>Hξ > γE[ξ>Hξ] ) = Prob ( r̄∑ i=1 λiu 2 i > γE[ r̄∑ i=1 λiu 2 i ] )\n= Prob ( r̄∑ i=1 λiu 2 i > γ r̄∑ i=1 λi )\n= Prob ( r̄∑ i=1 λ̄iu 2 i > γ ) ,\nwhere λ̄i = λi/ ∑r̄ i=1 λi for i = 1, . . . , r̄. Note that ∑r̄ i=1 λ̄i = 1 and λ̄1 ≥ λ̄2 ≥ · · · ≥ λ̄r ≥ 0. Then\nProb ( r̄∑ i=1 λ̄iu 2 i > γ ) = 1− Prob ( r̄∑ i=1 λ̄iu 2 i ≤ γ ) ≤ 1− Prob ( u21 ≤ γ & . . . & u2r̄ ≤ γ\n) = Prob ( u21 > γ || . . . || u2r̄ > γ\n) ≤\nr̄∑ i=1 Prob ( u2i > γ ) ≤ r̄ exp ( −1\n2\n( γ − √ 2γ − 1 )) Note that the last step is due to the inequality in Lemma (7) and the fact that u2i is a χ\n2 random variable with 1 degree of freedom.\nQ.E.D.\nAt last, we derive another two-side exponential tail bound for our own purpose.\nLemma 5 (Two-side Exponential Tail Bound). Let Q̃∗ ∈ Sqd+ be the optimal solution of problem (8), and Auv ∈ Sqd be any constraint matrix corresponding to index set C in problem (8). Suppose ξ ∈ Rqd is a random vector generated from the real-valued normal distributionN (0, Q̃∗). Then for any ≥ 0\nProb ( |ξ>Auvξ − E[ξ>Auvξ]| ≥ ) ≤ r̄ [ exp ( − (τ − 1) 2\n4\n) + exp ( − 2\n8r̄dq2\n)] ,\nwhere r̄ = min{rank(Auv), rank(Q̃∗)} and τ = ( q √ 2 r̄d + 1 ) 1 2 .\nProof. Let r = rank(Q̃∗). Since Q̃∗ ∈ Sqd+ , we can write Q̃∗ = UU> for some U ∈ Rqd×r. And since Auv is symmetric, we can write the eigen-decomposition of matrix U>AuvU = LDL>,\nwhere L = [l1, l2, . . . , lr] ∈ Rr×r is an orthogonal matrix and D = diag{λ1, λ2, . . . , λr} with λ1 ≥ λ2 ≥ · · · ≥ λr ≥ 0. Note that λi = 0 for all i > r̄ due to the fact that U>HU has rank at most r̄. Denoting ξ̄ ∼ N (0, Ir), then we can easily check that Uξ̄ ∼ N (0, Z), i.e., Uξ̄ is statistically identical to ξ.\nTherefore, we have that,\nProb ( |ξ>Auvξ − E[ξ>Auvξ]| ≥ ) = Prob ( | r̄∑ i=1 λi(l > i ξ̄) 2 − r̄∑ i=1 E[λi(l>i ξ̄)2]| ≥ ) Denoting ui = l>i ξ̄, we have that E[ui] = 0 and E[u2i ] = 1, i.e., ui is a standard normal variable. Hence,\nProb ( |ξ>Auvξ − E[ξ>Auvξ]| ≥ ) =Prob ( | r̄∑ i=1 λi(u 2 i − 1)| ≥ )\n≤ Prob\n √√√√ r̄∑\ni=1\nλ2i √√√√ r̄∑ i=1 (u2i − 1)2 ≥  =Prob ||D||F √√√√ r̄∑\ni=1\n(u2i − 1)2 ≥  Note that,\n||D||F = ||U>AuvU ||F ≤ ||Auv||F ||U ||2F = ||Auv||FTr(Q̃∗) ≤ q √ 2d.\nHere the first equality uses the fact that Frobenius norm is rotation-invariant. The second inequality uses the Cauchy-Schwarz inequality. While for the last equality, we can see that if u = v, then ||Auv||F = √ d, otherwise ||Auv||F = √ 2d. Moreover, we have Tr(Q̃∗) = q due to the fact Q̃∗ is the optimal solution of problem (8) satisfying the orthogonal constraints. Therefore, we have that, Prob ( |ξ>Auvξ − E[ξ>Auvξ]| ≥ ) ≤Prob\n( r̄∑ i=1 (u2i − 1)2 ≥ 2 2dq2 )\n=1− Prob ( r̄∑ i=1 (u2i − 1)2 < 2 2dq2 )\n≤1− Prob ( (u21 − 1)2 < 2\n2r̄dq2 & . . . & (u2r̄ − 1)2 <\n2\n2r̄dq2 ) =Prob ( (u21 − 1)2 ≥ 2\n2r̄dq2 || . . . || (u2r̄ − 1)2 ≥\n2\n2r̄dq2 ) ≤\nr̄∑ i=1 Prob ( (u2i − 1)2 ≥\n2\n2r̄dq2\n)\n= r̄∑ i=1 Prob ( |u2i − 1| ≥ q √ 2r̄d )\n= r̄∑ i=1 [ Prob ( u2i ≥ 1 + q √ 2r̄d ) + Prob ( u2i ≤ 1− q √ 2r̄d )] ≤r̄ [ exp ( − (τ − 1) 2\n4\n) + exp ( − 2\n8r̄dq2\n)] .\nwhere τ = ( q √ 2 r̄d + 1 ) 1 2\n. In the last step, we use the exponential tail bound of Chi-square variable in Lemma 7.\nQ.E.D.\nRemark. Note that if u = v then rank(Auv) = d otherwise rank(Auv) = 2d. Thus, we have that r̄ ≤ 2d, which could be used to eliminate the variable r̄ in the tail bound.\nProof of Theorem 2.\nProof. First, We have the following, Prob ( ν ≥ γµ & ζ ≤ & ξ>G̃ξ ≤ ωE[ξ>G̃ξ] ) ≥ 1− Prob ( ∃(i, j) ξ>X̃ijξ < γµ ) − Prob ( ∃(u, v) |ξ>Auvξ − buv| > ) − Prob ( ξ>G̃ξ > ωE[ξ>G̃ξ]\n) ≥ 1−\n∑ (i,j)∈I Prob ( ξ>X̃ijξ < γµ ) − ∑ (u,v)∈C Prob ( |ξ>Auvξ − buv| > ) − Prob ( ξ>G̃ξ > ωE[ξ>G̃ξ] ) = 1− |I|+\n∑ (i,j)∈I Prob ( ξ>X̃ijξ ≥ γµ ) − ∑ (u,v)∈C Prob ( |ξ>Auvξ − buv| > ) − Prob ( ξ>G̃ξ > ωE[ξ>G̃ξ] ) .\nHere we use the fact that ,\nE[ξ>X̃ijξ] = X̃ij • Q̃∗ ≥ µ E[ξ>Auvξ] = Auv • Q̃∗ = buv.\nWe denote\nT1 = ∑\n(i,j)∈I\nProb ( ξ>X̃ijξ ≥ γµ ) ,\nT2 = ∑\n(u,v)∈C\nProb ( |ξ>Auvξ − buv| > ) T3 = Prob ( ξ>G̃ξ > ωE[ξ>G̃ξ] ) .\nand\nr1 = min{max (i,j) rank(X̃ij), rank(Q̃ ∗)},\nr2 = min{max (u,v) rank(Auv), rank(Q̃ ∗)},\nr3 = min{rank(G̃), rank(Q̃∗)}.\nAccording to the first constraints in (8) and the lemma of left-side polynomial tail bound, we have, T1 ≥ ∑\n(i,j)∈I\nProb ( ξ>X̃ijξ ≥ γE[ξ>X̃ijξ] ) ≥ |I| ( 1−max{√γ, 2(r1 − 1)γ π − 2 } ) .\nThen according the second constraints in (8) and the lemma of two-side exponential tail, we have, T2 = ∑\n(u,v)∈C\nProb ( |ξ>Auvξ − E[ξ>Auvξ]| > ) ≤ r2q(q + 1)\n2\n[ exp ( − (τ − 1) 2\n4\n) + exp ( −\n2\n8r2dq2\n)] ,\nwhere τ = ( q √ 2 r2d + 1 ) 1 2\n. At last, according to the lemma of right-side exponential tail bound, we have,\nT3 ≤ r3 exp ( −1\n2\n( ω − √ 2ω − 1 )) .\nTherefore, based on all above inequalities and facts that r1 ≤ r, r2 ≤ r and r3 ≤ r, we can derive that,\nProb ( ν ≥ γµ & ζ ≤ & ξ>G̃ξ ≤ ωG̃ • Q̃∗ ) ≥ 1− |I|max ( √ γ,\n2(r − 1)γ π − 2 ) − r exp ( −1\n2\n( ω − √ 2ω − 1 )) − rq(q + 1)\n2\n[ exp ( − (τ − 1) 2\n4\n) + exp ( − 2\n8rdq2\n)] .\nQ.E.D."
    }, {
      "heading" : "9.4 Proofs in Section 6",
      "text" : "Proof of Lemma 2.\nProof. The given loss function is `(A, Xij) = M •Xij . First, note that,\nM •Xij = (xi − xj)>M(xi − xj),\n= (xi − xj)>M(xi − xj) (xi − xj)>(xi − xj) (xi − xj)>(xi − xj),\nThen, relying on the property of the Rayleigh quotient, we can obtain that,\nλmin||xi − xj ||22 ≤M •Xij ≤ λmax||xi − xj ||22,\nwhere λmax and λmin are the maximum and minimum eigenvalues of M .\nThus, denoting the replace-one dataset and the corresponding metric as Dk and Mk respectively, we can derive that,\n|`(AD, Xij)− `(ADk , Xij)| = ∣∣M •Xij −Mk •Xij∣∣ ,\n≤ ∣∣λmax − λkmin∣∣ ||xi − xj ||22,\n≤ 4Γ2 ( λmax + λ k min ) ,\n≤ 4Γ2(KR d + R d ), ≤ 4(K + 1)RΓ 2\nd ,\nwhere λmax and λkmin are the maximum and minimum eigenvalues of M and M k respectively. The second inequality uses the fact that xi and xj are in a Γ-ball. And the next one relies on that, for both M and Mk, it is true that\nλmax ≤ Kλmin ≤ K d Tr(M) ≤ KR d .\nTherefore, the Uniform-Replace-One stability β = 4(K+1)RΓ 2\nd .\nQ.E.D.\nRemark. Note if our previous assumption is violated, i.e., M is rank-deficient, then this stability result does not stand any more. In particular, we will have, for any M ,\nλmax ≤ Kλmin ≤ K r Tr(M) ≤ KR r ,\nwhere r is the rank of M and r < d. Then, the stability could be rewritten as β = 4(K+1)RΓ 2\nr̄ , where r̄ = min{rank(M), rank(Mk)}. This resultant stability is case-dependent, thus being less favourable. This argument is important for our later explanation why our generalization bound will not become tighter and tighter via trivially increasing the feature dimension.\nTo prove the Theorem 3, we first state one part of Lemma 9 in [5] as below.\nLemma 6 (Variance Bound). For any algorithm A and loss function ` such that 0 ≤ ` ≤ L, we have for any different i, j ∈ {1, . . . , n},\nED [ (R(A, D)−Remp(A, D))2 ] ≤ L 2\n2n + 3LE{D⋃ z′i} [|`(AD, zi)− `(ADi , zi)|] .\nRemark. Here D and Di are defined in Sect. 6 and Di = {D\\zi ∪ z′i}.\nProof of Theorem 3.\nProof. First, for the given loss function `, we have that,\n`(A, Xij) = M •Xij ≤ λmax||xi − xj ||22\n≤ 4KRΓ 2\nd .\nThen due to the definition of uniform-RO stability, we have that E{D⋃ z′i} [|`(AD, zi)− `(ADi , zi)|] ≤ β. Therefore, according to the above lemma of variance bound, we can obtain that,\nED [ (R(A, D)−Remp(A, D))2 ] ≤ 8K 2R2Γ4\nnd2 +\n12KRΓ2β\nd .\nThen based on Chebyshev’s inequality, we can derive that,\nProb (R(A, D)−Remp(A, D) ≥ ) ≤ ED [ (R(A, D)−Remp(A, D))2 ] 2 .\n≤ 1 2\n( 8K2R2Γ4\nnd2 +\n12KRΓ2β\nd\n) .\nSetting the right hand side of the above inequality as δ, we thus have with probability at least 1− δ that,\nR(A, D) ≤ Remp(A, D) + 2Γ\n√ KR\ndδ\n( 2KRΓ2\nnd + 3β\n) .\nBy substituting the Uniform-Replace-One stability β, we can obtain the following specific bound,\nR(A, D) ≤ Remp(A, D) + 2RΓ2\nd\n√ 2K\nδ\n( K\nn + 6K + 6\n) .\nQ.E.D.\nRemark. As aforementioned, there is one counter-intuitive property of our generalization bound that it becomes tighter when the feature dimension d increases. However, it is the case only if our previous full-rank assumption on M holds. If this assumption is violated, e.g., in the sparse high dimensional feature space, the above result does not stand any more. Therefore, it rules out the possibility that trivially increasing the feature dimension by adding zeros will improve the generalization ability."
    }, {
      "heading" : "10 Impact of Parameters",
      "text" : "In this section, we study how the performance of our BDML algorithm varies with several important parameters, including distortion bound K, trace bound R and width ρ. Except running time which needs large scale data, we experiment with all other parameters on the UCI Iris dataset. As in Sect. 7.1, we randomly split the dataset into 70% for training and 30% for testing and report the average test error and its standard deviation by repeating the random splits for 10 times."
    }, {
      "heading" : "10.1 Distortion Bound K",
      "text" : "We first study the effects of distortion bound K. We fix the number of iteration T = 1000, the margin of p-BDML µ = 1, the trace bound R = 100 and the width ρ = 500. And we report the mean condition number, mean test error and the standard deviation of test error. The results are listed in Table 10.1. From the table, we can find that with K increases, the resultant mean condition number becomes larger. It is partly because that as K increases, the bounded-distortion constraint becomes easier to satisfy, thus encouraging MWU method puts more weights on other constraints like the margin ones. Hence the learned metric embedding is more distorted to fitting the training data. Moreover, with K increases, the test error first decreases and then increases which matches our analysis in Sect. 6."
    }, {
      "heading" : "10.2 Trace Bound R and Width ρ",
      "text" : "We now study the effects of trace bound R and width ρ. These two parameters are correlated in a sense that the width ρ should not be much smaller than the trace bound R since otherwise the constraint of width, i.e., ∀i, ∣∣Ji • Y (t) − hi∣∣ ≤ ρ will not stand. We fix the number of iteration T = 1000, the margin of p-BDML µ = 1 and the distortion bound K = 1000. Same measurements are reported in Table 10.2.\nFrom this Table, we can see that if width ρ is set to be much larger than the trace bound R, the resultant mean test error tends to become larger. This may due to the fact that with the same number of iteration, the larger the width, the smaller the overall quality of the solution of the MWU method deteriorates which is matched to the analysis in [23]. On the other side, if width ρ is nearly equal to the trace bound R, the aforementioned constraint of width will be violated sometimes. Therefore, in practice, we find that setting R ≈ 10d and ρ ≈ 5R yield good results. Here d is the dimension of the input feature."
    }, {
      "heading" : "10.3 Running Time",
      "text" : "We now investigate the running time on the LFW dataset due to its high-dimensional feature. Since the main component of our BDML algorithm is the MWU method, we thus study how its running time varies with respect to the maximum number of iteration and dimension of input feature. The trace bound R and width ρ are both fixed as 3d+ 1 as aforementioned. We implement the algorithm as a single-thread MATLAB program. And all our experiments are conducted on a server with Intel Xeon E5 CPU(2.6GHz) and 128G RAM. In particular, we test following values of dimension d of input feature, 10, 50, 100 and 300. And for each dimension, we set the maximum number of iteration as 100, 500, 1000, 2000 and 5000 and keep track of the corresponding running time. The natural logarithmic of all results are plotted in Fig. 3. Generally, for 100-dim feature, it takes around 146s to finish 1000 iterations of MWU."
    }, {
      "heading" : "11 Full Results of Experiments",
      "text" : "We in this section demonstrate the comprehensive results of our experiments.\nT ru\ne P\no s it iv\ne R\na te"
    }, {
      "heading" : "11.1 Domain Adaptation",
      "text" : "For domain adaptation, we show the full results of all 12 possible combinations of source and target domains. In particular, unsupervised and semi-supervised experiments are listed in Table 6 and Table 7 respectively. We include the state-of-art results of max-margin domain transformations (MMDT) [15] on this dataset in Table 7. Note that the comparison with MMDT is somewhat unfair for our method, because it exploits the discrimination power of a max-margin classifier, whereas ours is the simple distance metric learning based 1-NN classifier. However, it is promising that, with such simple classifier, our BDML still achieves state-of-the-art results in some subtasks."
    }, {
      "heading" : "11.2 Face Verification",
      "text" : "In this section, we present full experimental comparisons on LFW dataset. In Table 8, we list various published results on “Image-Restricted, Label-Free Outside Data” setting of LFW dataset. Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-\nCombined [42], Convolutional DBN [17], Sub-SML [6] and DDML-Combined [22]. Among them, “Convolutional DBN” and “DDML-Combined” are deep learning based methods. Suffix “Combined” means the method uses multiple descriptors, e.g., SIFT [28], LBP [33], TPLBP [40], etc. From this table, we can find that, although using only dimension-reduced SIFT feature, our BDML algorithm achieves comparable results with other feature-combined and non-metric learning based ones. Moreover, our method achieves the least stand errors compared to others which indicates that our BDML produces stable metrics. The ROC curves versus others are plotted in Fig. 4."
    }, {
      "heading" : "12 Useful Tail Bound for Chi-square Variables",
      "text" : "We list the following sharp tail bound for chi-square variables.\nLemma 7. [27] Let X ∼ χ2d and ≥ 0, then\nP (X − d ≥ 2 √ d + 2 ) ≤ exp (− )\nP (X − d ≤ −2 √ d ) ≤ exp (− )."
    } ],
    "references" : [ {
      "title" : "The multiplicative weights update method: a meta-algorithm and applications",
      "author" : [ "S. Arora", "E. Hazan", "S. Kale" ],
      "venue" : "Theory of Computing,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Learning a mahalanobis metric from equivalence constraints",
      "author" : [ "A. Bar-Hillel", "T. Hertz", "N. Shental", "D. Weinshall", "G. Ridgeway" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "On lipschitz embedding of finite metric spaces in hilbert space. Israel",
      "author" : [ "J. Bourgain" ],
      "venue" : "Journal of Mathematics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1985
    }, {
      "title" : "Stability and generalization",
      "author" : [ "O. Bousquet", "A. Elisseeff" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "Similarity metric learning for face recognition",
      "author" : [ "Q. Cao", "Y. Ying", "P. Li" ],
      "venue" : "In International Conference on Computer Vision,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Information-theoretic metric learning",
      "author" : [ "J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Latent coincidence analysis: A hidden variable model for distance metric learning",
      "author" : [ "M. Der", "L.K. Saul" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Metric learning by collapsing classes",
      "author" : [ "A. Globerson", "S. Roweis" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2005
    }, {
      "title" : "Geodesic flow kernel for unsupervised domain adaptation",
      "author" : [ "B. Gong", "Y. Shi", "F. Sha", "K. Grauman" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Domain adaptation for object recognition: An unsupervised approach",
      "author" : [ "R. Gopalan", "R. Li", "R. Chellappa" ],
      "venue" : "In International Conference on Computer Vision,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Is that you? metric learning approaches for face identification",
      "author" : [ "M. Guillaumin", "J. Verbeek", "C. Schmid" ],
      "venue" : "In International Conference on Computer Vision,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Discriminant adaptive nearest neighbor classification",
      "author" : [ "T. Hastie", "R. Tibshirani" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1996
    }, {
      "title" : "A geometric take on metric learning",
      "author" : [ "S. Hauberg", "O. Freifeld", "M.J. Black" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Efficient learning of domaininvariant image representations",
      "author" : [ "J. Hoffman", "E. Rodner", "J. Donahue", "T. Darrell", "K. Saenko" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Lfw results using a combined nowak plus merl recognizer",
      "author" : [ "G.B. Huang", "M.J. Jones", "E. Learned-Miller" ],
      "venue" : "In Faces in Real-Life Images Workshop in European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    }, {
      "title" : "Learning hierarchical representations for face verification with convolutional deep belief networks",
      "author" : [ "G.B. Huang", "H. Lee", "E. Learned-Miller" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Labeled faces in the wild: A database for studying face recognition in unconstrained environments",
      "author" : [ "G.B. Huang", "M. Ramesh", "T. Berg", "E. Learned-Miller" ],
      "venue" : "Technical Report",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "Low-distortion embeddings of finite metric spaces",
      "author" : [ "P. Indyk", "J. Matousek" ],
      "venue" : "Handbook of Discrete and Computational Geometry,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2004
    }, {
      "title" : "Neighbourhood components analysis",
      "author" : [ "G. Jacob", "R. Sam", "H. Geoff", "S. Ruslan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Regularized distance metric learning: theory and algorithm",
      "author" : [ "R. Jin", "S. Wang", "Y. Zhou" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "Discriminative deep metric learning for face verification in the wild",
      "author" : [ "Y.-P.T. Junlin Hu", "Jiwen Lu" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Efficient algorithms using the multiplicative weights update method",
      "author" : [ "S. Kale" ],
      "venue" : "PhD Thesis,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2007
    }, {
      "title" : "Non-linear metric learning",
      "author" : [ "D. Kedem", "S. Tyree", "K.Q. Weinberger", "F. Sha", "G.R. Lanckriet" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Large scale metric learning from equivalence constraints",
      "author" : [ "M. Kostinger", "M. Hirzer", "P. Wohlhart", "P.M. Roth", "H. Bischof" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    }, {
      "title" : "Learning the kernel matrix with semidefinite programming",
      "author" : [ "G.R. Lanckriet", "N. Cristianini", "P. Bartlett", "L.E. Ghaoui", "M.I. Jordan" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2004
    }, {
      "title" : "Adaptive estimation of a quadratic functional by model selection",
      "author" : [ "B. Laurent", "P. Massart" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2000
    }, {
      "title" : "Distinctive image features from scale-invariant keypoints",
      "author" : [ "D.G. Lowe" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2004
    }, {
      "title" : "Semidefinite relaxation of quadratic optimization problems",
      "author" : [ "Z.-q. Luo", "W.-k. Ma", "A.-C. So", "Y. Ye", "S. Zhang" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2010
    }, {
      "title" : "Approximation bounds for quadratic optimization with homogeneous quadratic constraints",
      "author" : [ "Z.-Q. Luo", "N.D. Sidiropoulos", "P. Tseng", "S. Zhang" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2007
    }, {
      "title" : "Cosine similarity metric learning for face verification",
      "author" : [ "H.V. Nguyen", "L. Bai" ],
      "venue" : "In Asian Conference of Computer Vision,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2011
    }, {
      "title" : "Hamming distance metric learning",
      "author" : [ "M. Norouzi", "D.J. Fleet", "R. Salakhutdinov" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2012
    }, {
      "title" : "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns",
      "author" : [ "T. Ojala", "M. Pietikainen", "T. Maenpaa" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2002
    }, {
      "title" : "Nonlinear dimensionality reduction by locally linear embedding",
      "author" : [ "S.T. Roweis", "L.K. Saul" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2000
    }, {
      "title" : "Adapting visual category models to new domains",
      "author" : [ "K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2010
    }, {
      "title" : "Learnability, stability and uniform convergence",
      "author" : [ "S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2010
    }, {
      "title" : "Online and batch learning of pseudo-metrics",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "A.Y. Ng" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2004
    }, {
      "title" : "Positive semidefinite metric learning with boosting",
      "author" : [ "C. Shen", "J. Kim", "L. Wang", "A. Van Den Hengel" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2009
    }, {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "K.Q. Weinberger", "L.K. Saul" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2009
    }, {
      "title" : "Descriptor based methods in the wild",
      "author" : [ "L. Wolf", "T. Hassner", "Y. Taigman" ],
      "venue" : "In Workshop on Faces in’Real-Life’Images: Detection, Alignment, and Recognition,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2008
    }, {
      "title" : "Distance metric learning with application to clustering with side-information",
      "author" : [ "E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2003
    }, {
      "title" : "Distance metric learning with eigenvalue optimization",
      "author" : [ "Y. Ying", "P. Li" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 37,
      "context" : "For instance, in supervised learning, a common criterion is to learn a metric with a low empirical error [39], while in unsupervised learning, a good criterion is to learn a metric that minimizes the intra-cluster distance and simultaneously maximizes the inter-cluster distance [41].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 39,
      "context" : "For instance, in supervised learning, a common criterion is to learn a metric with a low empirical error [39], while in unsupervised learning, a good criterion is to learn a metric that minimizes the intra-cluster distance and simultaneously maximizes the inter-cluster distance [41].",
      "startOffset" : 279,
      "endOffset" : 283
    }, {
      "referenceID" : 2,
      "context" : "Such an embedding intrinsically induces distortion - a concept in the theory of metric embedding [4], which intuitively measures the effort to reshape the metric space.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 21,
      "context" : "We approach the SDP via a bisection method, which involves solving a sequence of convex feasibility problems with fast multiplicative weights update [23].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 39,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 99,
      "endOffset" : 110
    }, {
      "referenceID" : 35,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 99,
      "endOffset" : 110
    }, {
      "referenceID" : 1,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 99,
      "endOffset" : 110
    }, {
      "referenceID" : 22,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 131,
      "endOffset" : 143
    }, {
      "referenceID" : 30,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 131,
      "endOffset" : 143
    }, {
      "referenceID" : 12,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 131,
      "endOffset" : 143
    }, {
      "referenceID" : 18,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 167,
      "endOffset" : 174
    }, {
      "referenceID" : 6,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 167,
      "endOffset" : 174
    }, {
      "referenceID" : 19,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 196,
      "endOffset" : 204
    }, {
      "referenceID" : 36,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 196,
      "endOffset" : 204
    }, {
      "referenceID" : 32,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 40,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 244,
      "endOffset" : 248
    }, {
      "referenceID" : 39,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 269,
      "endOffset" : 273
    }, {
      "referenceID" : 7,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 294,
      "endOffset" : 297
    }, {
      "referenceID" : 11,
      "context" : "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.",
      "startOffset" : 308,
      "endOffset" : 312
    }, {
      "referenceID" : 39,
      "context" : "Pairwise methods [41, 7] often adds constraints to enforce distances between pairs of dissimilar points are larger than a given threshold.",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "Pairwise methods [41, 7] often adds constraints to enforce distances between pairs of dissimilar points are larger than a given threshold.",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 37,
      "context" : "Representative methods in the triplet group are the largemargin nearest neighbor [39] and its variants [24].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "Representative methods in the triplet group are the largemargin nearest neighbor [39] and its variants [24].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "One line of research focuses on how to embed a finite metric space into normed spaces with a low distortion [4, 19], i.",
      "startOffset" : 108,
      "endOffset" : 115
    }, {
      "referenceID" : 17,
      "context" : "One line of research focuses on how to embed a finite metric space into normed spaces with a low distortion [4, 19], i.",
      "startOffset" : 108,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "Metric learning is also related to manifold learning [14] and kernel learning [26].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 24,
      "context" : "Metric learning is also related to manifold learning [14] and kernel learning [26].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "The formal definition of metric embedding and its distortion are as follows [4, 19], Definition 2.",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : "The formal definition of metric embedding and its distortion are as follows [4, 19], Definition 2.",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "From a geometric perspective, it possesses an intrinsically different meaning compared to other complexity measures in previous work, including the log determinant (logDet) [7] and Frobenius norm (Fnorm) [21].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 19,
      "context" : "From a geometric perspective, it possesses an intrinsically different meaning compared to other complexity measures in previous work, including the log determinant (logDet) [7] and Frobenius norm (Fnorm) [21].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 37,
      "context" : "Following [39], two types of neighbor points are distinguished.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 39,
      "context" : "Here we only consider the former purpose and minimize the average distance of target neighbors as in [41].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 37,
      "context" : "Note that the unit margin in [39] can be set as a arbitrarily positive constant, since it only affects the scale of M .",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 21,
      "context" : "For each feasibility problem, we resort to the multiplicative weights update (MWU) method [23, 1], which is a meta algorithm and has many variants in different disciplines.",
      "startOffset" : 90,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "For each feasibility problem, we resort to the multiplicative weights update (MWU) method [23, 1], which is a meta algorithm and has many variants in different disciplines.",
      "startOffset" : 90,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "We have the Theorem 1 following [1] to guarantee that either Ȳ achieves a predefined accuracy or we claim that the original problem (5) is infeasible.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 28,
      "context" : "By choosing γ = π/16|I|, = 40q √ rd and with appropriate rank reduction on Q̃∗ as [30], it can be shown that after running Alg.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "To theoretically investigate whether the distortion has an impact on the generalization ability, we derive the generalization bound of our BDML following the stability analysis of learning algorithms [5, 36].",
      "startOffset" : 200,
      "endOffset" : 207
    }, {
      "referenceID" : 34,
      "context" : "To theoretically investigate whether the distortion has an impact on the generalization ability, we derive the generalization bound of our BDML following the stability analysis of learning algorithms [5, 36].",
      "startOffset" : 200,
      "endOffset" : 207
    }, {
      "referenceID" : 3,
      "context" : "Based on [5], we define the UniformReplace-One stability as, Definition 3.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 34,
      "context" : "Note that our definition is stronger than the one proposed in [36], thus being more restrictive.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "Although β does not depend decreasingly on the number of samples n, which may not be seen as stable in some sense [5], it is clear that the stability can be controlled by the distortion.",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 33,
      "context" : "We present empirical evaluations of our BDML algorithm on a wide range of tasks, including classification on several UCI datasets [2], domain adaptation on medium-scale datasets [35], and face verification on the large-scale LFW dataset [18].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 16,
      "context" : "We present empirical evaluations of our BDML algorithm on a wide range of tasks, including classification on several UCI datasets [2], domain adaptation on medium-scale datasets [35], and face verification on the large-scale LFW dataset [18].",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 39,
      "context" : "We compare with the baseline of Euclidean metric and several strong competitors like Xing [41], LMNN [39], ITML [7] and BoostMetric [38].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 37,
      "context" : "We compare with the baseline of Euclidean metric and several strong competitors like Xing [41], LMNN [39], ITML [7] and BoostMetric [38].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "We compare with the baseline of Euclidean metric and several strong competitors like Xing [41], LMNN [39], ITML [7] and BoostMetric [38].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 36,
      "context" : "We compare with the baseline of Euclidean metric and several strong competitors like Xing [41], LMNN [39], ITML [7] and BoostMetric [38].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 37,
      "context" : "Especially, t-BDML is consistently better than p-BDML which validates the effectiveness of triplet constraints as suggested by [39].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 33,
      "context" : "We use the same dataset as in [35], which contains 2,533 images of 10 categories from 4 domains: Caltech, Amazon, Webcam, and Dslr.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "We exploit the same 10 categories as [10] to all the four domains.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 33,
      "context" : "Experiments are repeated with 20 fixed train/test splits offered by [35].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 37,
      "context" : "Table 2 presents the mean test accuracy and standard errors of various metric learning based methods, including LMNN [39], ITML [35], SGF [11], GFK [10] and DML-eig [42], where A→C means the adaptation from source domain A (i.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 33,
      "context" : "Table 2 presents the mean test accuracy and standard errors of various metric learning based methods, including LMNN [39], ITML [35], SGF [11], GFK [10] and DML-eig [42], where A→C means the adaptation from source domain A (i.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "Table 2 presents the mean test accuracy and standard errors of various metric learning based methods, including LMNN [39], ITML [35], SGF [11], GFK [10] and DML-eig [42], where A→C means the adaptation from source domain A (i.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : "Table 2 presents the mean test accuracy and standard errors of various metric learning based methods, including LMNN [39], ITML [35], SGF [11], GFK [10] and DML-eig [42], where A→C means the adaptation from source domain A (i.",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 40,
      "context" : "Table 2 presents the mean test accuracy and standard errors of various metric learning based methods, including LMNN [39], ITML [35], SGF [11], GFK [10] and DML-eig [42], where A→C means the adaptation from source domain A (i.",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 8,
      "context" : "And for fair competition, we adopt the best results of GFK under the PCA subspace setting reported by [10].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "There are 6 standard protocols [18] for evaluating results.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 39,
      "context" : "In Table 3, we present results of p-BDML and other metric learning based methods including Xing [41], ITML [7], LDML [12], KISSME [25], DML-eig [42], Sub-ITML and Sub-ML [6].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : "In Table 3, we present results of p-BDML and other metric learning based methods including Xing [41], ITML [7], LDML [12], KISSME [25], DML-eig [42], Sub-ITML and Sub-ML [6].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 10,
      "context" : "In Table 3, we present results of p-BDML and other metric learning based methods including Xing [41], ITML [7], LDML [12], KISSME [25], DML-eig [42], Sub-ITML and Sub-ML [6].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 23,
      "context" : "In Table 3, we present results of p-BDML and other metric learning based methods including Xing [41], ITML [7], LDML [12], KISSME [25], DML-eig [42], Sub-ITML and Sub-ML [6].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 40,
      "context" : "In Table 3, we present results of p-BDML and other metric learning based methods including Xing [41], ITML [7], LDML [12], KISSME [25], DML-eig [42], Sub-ITML and Sub-ML [6].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 4,
      "context" : "In Table 3, we present results of p-BDML and other metric learning based methods including Xing [41], ITML [7], LDML [12], KISSME [25], DML-eig [42], Sub-ITML and Sub-ML [6].",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 21,
      "context" : "Then, equipped with the above inequality and Theorem 2 in [23], we have, for any constraint i,",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 27,
      "context" : "Based on [29], to derive the SDP relaxation, we can easily observe the following, ζX̃ijζ = Tr(X̃ijζζ >) = X̃ij • ζζ>.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 28,
      "context" : "First, we state the Lemma 1 in [30] as below which gives the polynomial tail bound of the left-side inequality ξ>Hξ < γE[ξ>Hξ].",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "To prove the Theorem 3, we first state one part of Lemma 9 in [5] as below.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 21,
      "context" : "This may due to the fact that with the same number of iteration, the larger the width, the smaller the overall quality of the solution of the MWU method deteriorates which is matched to the analysis in [23].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 13,
      "context" : "We include the state-of-art results of max-margin domain transformations (MMDT) [15] on this dataset in Table 7.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 14,
      "context" : "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 39,
      "context" : "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 40,
      "context" : "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 23,
      "context" : "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 29,
      "context" : "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 40,
      "context" : "Combined [42], Convolutional DBN [17], Sub-SML [6] and DDML-Combined [22].",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 15,
      "context" : "Combined [42], Convolutional DBN [17], Sub-SML [6] and DDML-Combined [22].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "Combined [42], Convolutional DBN [17], Sub-SML [6] and DDML-Combined [22].",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 20,
      "context" : "Combined [42], Convolutional DBN [17], Sub-SML [6] and DDML-Combined [22].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 26,
      "context" : ", SIFT [28], LBP [33], TPLBP [40], etc.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 31,
      "context" : ", SIFT [28], LBP [33], TPLBP [40], etc.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 38,
      "context" : ", SIFT [28], LBP [33], TPLBP [40], etc.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 25,
      "context" : "[27] Let X ∼ χd and ≥ 0, then P (X − d ≥ 2 √ d + 2 ) ≤ exp (− ) P (X − d ≤ −2 √ d ) ≤ exp (− ).",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2015,
    "abstractText" : "Metric learning aims to embed one metric space into another to benefit tasks like classification and clustering. Although a greatly distorted metric space has a high degree of freedom to fit training data, it is prone to overfitting and numerical inaccuracy. This paper presents bounded-distortion metric learning (BDML), a new metric learning framework which amounts to finding an optimal Mahalanobis metric space with a bounded-distortion constraint. An efficient solver based on the multiplicative weights update method is proposed. Moreover, we generalize BDML to pseudo-metric learning and devise the semidefinite relaxation and a randomized algorithm to approximately solve it. We further provide theoretical analysis to show that distortion is a key ingredient for stability and generalization ability of our BDML algorithm. Extensive experiments on several benchmark datasets yield promising results.",
    "creator" : "LaTeX with hyperref package"
  }
}