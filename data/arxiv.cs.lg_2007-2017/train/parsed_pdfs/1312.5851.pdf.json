{
  "name" : "1312.5851.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast Training of Convolutional Networks through FFTs",
    "authors" : [ "Michael Mathieu", "Mikael Henaff" ],
    "emails" : [ "mathieu@cs.nyu.edu", "mbh305@nyu.edu", "yann@cs.nyu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "As computer vision and machine learning aim to solve increasingly challenging tasks, models of greater complexity are required. This in turn requires orders of magnitude more data to take advantage of these powerful models while avoiding overfitting. While early benchmark datasets in machine learning contained thousands or tens of thousands of samples [5, 2, 8], current datasets are of the order of millions [4, 1]. This brings about new challenges as to how to train networks in a feasible amount of time. Even using parallel computing environments, training a network on ImageNet can take weeks [6]. In addition, although inference of labels using a trained network is comparatively fast, real-world applications such as producing labels for all images on the internet can represent a significant cost in terms of time and resources. Therefore, there is an important need to develop fast algorithms for training and inference.\nIn this work, we present a simple algorithm which accelerates training and inference using convolutional networks. The idea is based on performing convolutions as products in the Fourier domain, and reusing transformed feature maps many times. The significant operations in training convolutional networks can all be viewed as convolutions between pairs of 2-D matrices, which can represent input and output feature maps, gradients of the loss with respect to feature maps, or weight kernels. Typically, convolutions are performed for all pairings between two sets of 2-D matrices. By computing the Fourier transforms of the matrices in each set once, we can efficiently perform all convolutions as pairwise products. Although it has long been known that convolutions can be\nar X\niv :1\n31 2.\n58 51\nv1 [\ncs .C\nV ]\ncomputed as products in the Fourier domain, until recently the number of feature maps used in convolutional networks has been too small to make a method like ours effective. When the number of feature maps is large, this method accelerates training and inference by a significant factor, and can lead to a speedup of over an order of magnitude."
    }, {
      "heading" : "2 Theory",
      "text" : ""
    }, {
      "heading" : "2.1 Backpropagation",
      "text" : "The backpropagation algorithm [7] is the standard method to compute the gradient when training a convolutional network. During training, each layer performs three tasks, which we now describe. First we fix some notation: for a given layer, we have a set of input feature maps xf indexed by f , each one being a 2-D image of dimensions n×n. The output is a set of feature maps yf ′ indexed by f ′, which are also 2-D images whose dimension depends on the convolutional kernel and its stride. The layer’s trainable parameters consist of a set of weights wf ′f , each of which is a small kernel of dimensions k × k. In the forward pass, each output feature map is computed as a sum of the input feature maps convolved with the corresponding trainable weight kernel:\nyf ′ = ∑ f xf ∗ wf ′f (1)\nDuring the backward pass, the gradients with respect to the inputs are computed by convolving the transposed weight kernel with the gradients with respect to the outputs:\n∂L\n∂xf =\n∂L\n∂yf ′ ∗ wTf ′f (2)\nThis step is necessary for computing the gradients in (3) for the previous layer. Finally, the gradients of the loss with respect to the weight are computed by convolving each input feature map with the gradients with respect to the outputs:\n∂L\nwf ′f =\n∂L\n∂yf ′ ∗ xf (3)\nNote that ∂L∂yf′ is a 2-D matrix with the same dimensions as the output feature map yf ′ , and that all operations consist of convolutions between various sets of 2-D matrices."
    }, {
      "heading" : "2.2 Algorithm",
      "text" : "The well-known Convolution Theorem states that convolutions in the spatial domain are equivalent to pointwise products in the Fourier domain. Letting F denote the Fourier transform and F−1 its inverse, we can compute convolutions between functions f and g as follows:\nf ∗ g = F−1(F(f) · F(g))\nTypically, this method is used when the size of the convolution kernel is close to that of the input image. Note that a convolution of an image of size n × n with a kernel of size k × k using the direct method requires O(n2k2) operations. The complexity of the FFT-based method requires O(6n2 log n+n2) operations: each FFT requiresO(n2 log n2) = O(2n2 log n), and the pointwise product in the frequency domain requires n2. When the kernel is small, it is usually more efficient to do the convolution in the spatial domain.\nOur algorithm is based on the observation that in all of the operations (1), (2), (3), each of the matrices indexed by f is convolved with each of the matrices indexed by f ′. We can therefore compute the FFT of each matrix once, and all pairwise convolutions can be performed as products\nin the frequency domain. Even though using the FFT-based method may be less efficient for a given convolution, we can effectively reuse our FFTs many times which more than compensates for the overhead.\nThe following analysis makes this idea precise. Assume we have f input feature maps, f ′ output feature maps, images consisting of n × n pixels and kernels of k × k pixels. Also assume we are performing updates over minibatches of size S, and that C represents the hidden constant in the FFT complexity. As an example, using the direct approach (1) will take a total of S ·f ′ ·f ·(n−k+1)2 ·k2 operations. Our approach requires (2C · n2 log n)(S · f + f ′ · f) operations to transform the input feature maps and kernels to the Fourier domain, a total of S ·n2 · f ′ · f additions and multiplications in the Fourier domain, and S · f ′ · (2C · n2 log n) operations to transform the output feature maps back to the spatial domain. The same analysis yields similar complexity estimates for the other operations:\nDirect convolution Our Method∑ f xf ∗ wf ′f S · f ′ · f · n′2 · k2 2Cn2 log n[f ′ · S + f · S + f ′ · f ] + S · f ′ · f · n2\n∂L ∂yf′ ∗ wTf ′f S · f ′ · f · n2 · k2 2Cn′2 log n′[f ′ · S + f · S + f ′ · f ] + S · f ′ · f · n′2\n∂L ∂yf′ ∗ xf S · f ′ · f · k2 · n′2 2Cn log n2[f ′ · S + f · S + f ′ · f ] + S · f ′ · f · n2\nHere n′ = (n− k + 1) represents the size of the output feature map. Note that the high complexity of the direct method for convolution comes from the product of five terms, whereas our method has a sum of products with at most four terms. Figure 2 shows the theoretical number of operations for direct convolution and our FFT method for various input sizes."
    }, {
      "heading" : "2.3 Implementation",
      "text" : "Although conceptually straighforward, a number of challenges relating to GPU implementation needed to be addressed. First, current GPU implementations of the FFT such as cuFFT are designed to parallelize over individual transforms. This can be useful for computing a limited number of transforms on large inputs, but is not suitable for our task since we are performing many FFTs over relatively small inputs. Therefore, we developed a custom CUDA implementation of the CooleyTukey FFT algorithm which enabled us to specify how many threads to allocate to each FFT, while also parallelizing over feature maps and minibatches. Two-dimensional transforms were performed by computing one-dimensional transforms over rows and columns, and was done in-place to save memory on the GPU. As another means to save memory, we used symmetry properties of FFTs of real inputs to only store half of the data in the frequency domain."
    }, {
      "heading" : "3 Experiments",
      "text" : "To test our analysis, we ran a series of experiments comparing the speed of our method compared to the GPU implementations of [6] and of the Torch 7 machine learning environment [3]. Specifically, we compared how each method performed with varying kernel sizes, input sizes and minibatch sizes. The results are shown in Figure 3. For all experiments, we chose 96 input feature maps and 256 output features, which represents a typical configuration of a deep network’s second layer. The functions updateOutput, updateGradInput and accGradParameters correspond to the operations in (1), (2) and (3) respectively. All times are measured in seconds.\nWe see that our method significantly outperforms the other two in nearly all cases. The improvement is especially pronounced for the accGradParameters operation, which is the most computationally expensive. This is likely due to the fact that the convolution we are computing has a large kernel, for which FFTs are better suited in any case, and due to an additional heuristic whereby we sum over minibatches in the Fourier domain, thus minimizing the number of inverse FFTs. Also note that our method performs the same regardless of kernel size, since we pad the kernel to be the same size as the input image before applying the FFT. This enables the use of much larger kernels, which we intend to explore in future work."
    }, {
      "heading" : "4 Discussion",
      "text" : "We have presented a simple and fast algorithm for training and inference using convolutional networks. It outperforms known state-of-the-art implementations in terms of speed, as verified by numerical experiments. In the future we plan to explore the possibilities of learning kernels directly in the Fourier domain, which could also remove the need for specifying kernel sizes as a hyperparameter."
    } ],
    "references" : [ {
      "title" : "The million song dataset",
      "author" : [ "Thierry Bertin-Mahieux", "Daniel P.W. Ellis", "Brian Whitman", "Paul Lamere" ],
      "venue" : "In Proceedings of the 12th International Conference on Music Information Retrieval",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Representing shape with a spatial pyramid kernel",
      "author" : [ "A. Bosch", "A. Zisserman", "X. Munoz" ],
      "venue" : "In Proceedings of the ACM International Conference on Image and Video Retrieval,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2007
    }, {
      "title" : "Torch7: A matlab-like environment for machine learning",
      "author" : [ "Ronan Collobert", "Koray Kavukcuoglu", "Clement Farabet" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object",
      "author" : [ "L. Fei-Fei", "R. Fergus", "Pietro Perona" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Efficient backprop",
      "author" : [ "Y. LeCun", "L. Bottou", "G. Orr", "K. Muller" ],
      "venue" : "Neural Networks: Tricks of the trade. Springer,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1998
    }, {
      "title" : "Musical genre classification of audio signals",
      "author" : [ "G. Tzanetakis", "P. Cook" ],
      "venue" : "IEEE Transactions on Speech and Audio Processing,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "While early benchmark datasets in machine learning contained thousands or tens of thousands of samples [5, 2, 8], current datasets are of the order of millions [4, 1].",
      "startOffset" : 103,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "While early benchmark datasets in machine learning contained thousands or tens of thousands of samples [5, 2, 8], current datasets are of the order of millions [4, 1].",
      "startOffset" : 103,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : "While early benchmark datasets in machine learning contained thousands or tens of thousands of samples [5, 2, 8], current datasets are of the order of millions [4, 1].",
      "startOffset" : 103,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "While early benchmark datasets in machine learning contained thousands or tens of thousands of samples [5, 2, 8], current datasets are of the order of millions [4, 1].",
      "startOffset" : 160,
      "endOffset" : 166
    }, {
      "referenceID" : 0,
      "context" : "While early benchmark datasets in machine learning contained thousands or tens of thousands of samples [5, 2, 8], current datasets are of the order of millions [4, 1].",
      "startOffset" : 160,
      "endOffset" : 166
    }, {
      "referenceID" : 5,
      "context" : "Even using parallel computing environments, training a network on ImageNet can take weeks [6].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "1 Backpropagation The backpropagation algorithm [7] is the standard method to compute the gradient when training a convolutional network.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "3 Experiments To test our analysis, we ran a series of experiments comparing the speed of our method compared to the GPU implementations of [6] and of the Torch 7 machine learning environment [3].",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "3 Experiments To test our analysis, we ran a series of experiments comparing the speed of our method compared to the GPU implementations of [6] and of the Torch 7 machine learning environment [3].",
      "startOffset" : 192,
      "endOffset" : 195
    } ],
    "year" : 2016,
    "abstractText" : "Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.",
    "creator" : "LaTeX with hyperref package"
  }
}