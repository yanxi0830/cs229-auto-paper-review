{
  "name" : "1509.09187.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Haar Scattering Networks",
    "authors" : [ "XIUYUAN CHENG", "XU CHEN" ],
    "emails" : [ "xiuyuan.cheng@yale.edu", "xuchen@princeton.edu", "stephane.mallat@ens.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "An orthogonal Haar scattering transform is a deep network, computed with a hierarchy of additions, subtractions and absolute values, over pairs of coefficients. It provides a simple mathematical model for unsupervised deep network learning. It implements non-linear contractions, which are optimized for classification, with an unsupervised pair matching algorithm, of polynomial complexity. A structured Haar scattering over graph data computes permutation invariant representations of groups of connected points in the graph. If the graph connectivity is unknown, unsupervised Haar pair learning can provide a consistent estimation of connected dyadic groups of points. Classification results are given on image data bases, defined on regular grids or graphs, with a connectivity which may be known or unknown. deep learning, neural network, scattering transform, Haar wavelet, classification, images, graphs 2000 Math Subject Classification: 68Q32, 68T45, 68Q25, 68T05"
    }, {
      "heading" : "1 Introduction",
      "text" : "Deep neural networks appear to provide scalable learning architectures for high-dimensional learning, with impressive results on many different type of data and signals [2]. Despite their efficiency, there is still little understanding on the properties of these architectures. Deep neural networks alternate pointwise linear operators, whose coefficients are optimized with training examples, with pointwise non-linearities. To obtain good classification results, strong constraints are imposed on the network architecture on the support of these linear operators [15]. These constraints are usually derived from an experimental trial and error processes.\nSection 2 introduces a simple deep Haar scattering architecture, which only computes the sum of pairs of coefficients, and the absolute value of their difference. The architecture preserves some important properties of deep networks, while reducing the computational complexity and simplifying their mathematical analysis. Through this architecture, we shall address major questions concerning invariance properties, learning complexity, consistency, and the specialization of such architectures.\nConvolution networks are particular classes of deep networks, which compute translation invariant descriptors of signals defined over uniform grids [15, 27]. Scattering networks were introduced as convolution networks computed with iterated wavelet transforms, to obtain invariants which are stable to deformations [18]. With appropriate architecture constraints on Haar scattering networks, Section 3 defines locally displacement invariant representations of signals defined\nar X\niv :1\n50 9.\n09 18\n7v 1\n[ cs\n.L G\n] 3\non general graphs. In social, sensor or transportation networks, high dimensional data vectors are supported on a graph [28]. In most cases, propagation phenomena require to define translation invariant representations for classification. We show that an appropriate configuration of an orthogonal Haar scattering defines such a translation invariant representation on a graph. It is computed with a product of Haar wavelet transforms on the graph, and is thus closely related to nonorthogonal translation invariant scattering transforms [18].\nThe connectivity of graph data is often unknown. In social or financial networks, we typically have information on individual agents, without knowing the interactions and hence connectivity between agents. Building invariant representations on such graphs requires to estimate the graph connectivity. Such information can be inferred from unlabeled data, by analyzing the joint variability of signals defined on the unknown graph. This paper studies unsupervised learning strategies, which optimize deep network configurations, without class label information.\nMost deep neural networks are fighting the curse of dimensionality by reducing the variance of the input data with contractive non-linearities [25, 2]. The danger of such contractions is to nearly collapse together vectors which belong to different classes. Learning must preserve discriminability despite this variance reduction resulting from contractions. Hierarchical unsupervised architectures have been shown to provide efficient learning strategies [1]. We show that unsupervised learning can optimize an average discriminability by computing sparse features. Sparse unsupervised learning, which is usually NP hard, is reduced to a pair matching problem for Haar scattering. It can thus be computed with a polynomial complexity algorithm. For Haar scattering on graphs, it recovers a hierarchical connectivity of groups of vertices. Under appropriate assumptions, we prove that pairing problems avoid the curse of dimensionality. It can recover an exact connectivity with arbitrary high probability if the training size grows almost linearly with the signal dimension, as opposed to exponentially.\nHaar scattering classification architectures are numerically tested over image databases defined on uniform grids or irregular graphs, whose geometries are either known or estimated by unsupervised learning. Results are compared with state of the art unsupervised and supervised classification algorithms, applied to the same data, with known or unknown geometry. All computations can be reproduced with a software available at www.di.ens.fr/data/scattering/haar."
    }, {
      "heading" : "2 Free Orthogonal Haar Scattering",
      "text" : ""
    }, {
      "heading" : "2.1 Orthogonal Haar filter contractions",
      "text" : "We progressively introduce orthogonal Haar scattering by specializing a general deep neural network. We explain the architecture constraints and the resulting contractive properties.\nThe input network is a positive d-dimensional signal x ∈ (R+)d , which we write S0x = x. We denote by S jx the network layer at the depth j. A deep neural network computes the next network layer by applying a linear operator H j to S jx followed by a non-linear operator. Particular deep network architectures impose that H j preserves distances, up to a constant normalization factor λ [21]:\n‖H jy−H jy′‖= λ ‖y− y′‖ .\nThe network is contractive if it applies a pointwise contraction ρ to each value of the output vector H jS jx. This means that for any (a,b) ∈R2 |ρ(a)−ρ(b)| ≤ |a−b|. Rectifications and sigmoids are examples of such contractions. We use an absolute value ρ(a) = |a| because it preserves the amplitude, and it yields a permutation invariance which will be studied. For any vector y = (y(n))n, the pointwise absolute value is written |y|= (|y(n)|)n. The next network layer is thus:\nS j+1x = |H jS jx| . (1)\nThis transform is iterated up to a maximum depth J ≤ log2(d) to compute the network output SJx. We shall further impose that each layer S jx has the same dimension as x, and hence that H j is an orthogonal operator in Rd , up to the scaling factor λ . Geometrically, S j+1x is thus obtained by rotating S jx with H j, and by contracting each of its coordinate with the absolute value. The geometry of this contraction is thus defined by the choice of the operator H j which adjusts the one-dimensional directions along which the contraction is performed.\nAn orthogonal Haar scattering is implemented with an orthogonal Haar filter H j. The vector H jy regroups the coefficients of y ∈ Rd into d/2 pairs and computes their sums and differences. The rotation H j is thus factorized into d/2 rotations by π/4 in R2, and multiplications by 21/2. The transformation of each coordinate pair (α,β ) ∈ R2 is:\n(α,β ) −→ (α +β , α−β ).\nThe operator |H j| applies an absolute value to each output coordinate, which has no effect on α +β if α ≥ 0 and β ≥ 0, but it removes the sign of their difference:\n(α,β ) −→ (α +β , |α−β |) . (2)\nObserve that this non-linear operator defines a permutation invariant representation of (α,β ). Indeed, the output values are not modified by a permutation of α and β , and the two values of α , β are recovered without order, by\nmax(α,β ) = 1 2 ( α +β + |α−β | ) and min(α,β ) = 1 2 ( α +β −|α−β | ) . (3)\nThe operator |H j| can thus also be interpreted as a calculation of d/2 permutation invariant representations of pairs of coefficients.\nApplying |H j| to S jx computes the next layer S j+1x = |H jS jx|, obtained by regrouping the coefficients of S jx ∈ Rd\ninto d/2 pairs of indices written π j = {π j(2n),π j(2n+1)}0≤n<d/2:\nS j+1x(2n) = S jx(π j(2n))+S jx(π j(2n+1)) , (4)\nS j+1x(2n+1) = |S jx(π j(2n))−S jx(π j(2n+1))| . (5)\nThe pairing π j specifies which index π j(2n+1) is paired with π j(2n), but the ordering index n is not important. It specifies the storing position in S j+1x of the transformed values. For classification applications, π j will be optimized with training examples. This deep network computation is illustrated in Figure 1. The network output SJx is calculated with Jd/2 additions, subtractions and absolute values. Each coefficient of SJx is calculated by cascading J permutation invariant operators over pairs, and thus defines an invariant over a group of 2J coefficients. The network depth J thus corresponds to an invariance scale 2J .\nSince the network is computed by iterating orthogonal linear operators, up to a normalization, and a contractive absolute value, the following theorem proves that it defines a contractive transform, which preserves the norm, up to a normalization. It also proves that an orthogonal Haar scattering transform SJx is obtained by applying an orthogonal matrix to x, which depends upon x and J.\nTheorem 2.1. For any J ≥ 0, and any (x,x′) ∈ R2d\n‖SJx−SJx′‖ ≤ 2J/2‖x− x′‖ . (6)\nMoreover SJx = 2J/2 Mx,J x where Mx,J is an orthogonal matrix which depends on x and J, and\n‖SJx‖= 2J/2‖x‖ . (7)\nProof. Since S j+1x = |H jS jx| where H j is an orthogonal operator multiplied by 21/2,\n‖S j+1x−S j+1x′‖ ≤ ‖H jS jx−H jS jx′‖= 21/2‖S jx−S jx′‖.\nSince S0x = x, equation (6) is verified by induction on j. We can also rewrite\nS j+1x = |H jS jx|= E j,xH jx,\nwhere E j,x is a diagonal matrix where the diagonal entries are ±1, with a sign which depend on S jx. Since 2−1/2H j is orthogonal, 2−1/2E j,xH j is also orthogonal so Mx,J = 2−J/2 ∏Jj=1 E j,xH j is orthogonal, and depends on x and J. It results that ‖SJx‖= 2J/2 ‖x‖."
    }, {
      "heading" : "2.2 Complete representation with bagging",
      "text" : "A single Haar scattering transform looses information since it applies orthogonal operators followed by an absolute value which looses the information of the sign. However, the following theorem proves that x can be recovered from 2J distinct orthogonal Haar scattering transforms, computed with different pairings π j at each layer.\nTheorem 2.2. There exist 2J different orthogonal Haar scattering transforms such that almost all x ∈ Rd can be reconstructed from the coefficients of these 2J transforms.\nThis theorem is proved by observing that a Haar scattering transform is computed with permutation invariants operators over pairs. Inverting these operators allows to recover values of signal pairs but not their locations. However, recombining these values on enough overlapping sets allows one to recover their locations and hence the original signal x. This is proved on the following lemma applied to interlaced pairings. We say that two pairings π0 = {π0(2n),π0(2n+1)}0≤n<d/2 and π1 = {π1(2n),π1(2n+ 1)}0≤n<d/2 are interlaced if there exists no strict subset Ω of {1, ...,d} such that π0 and π1 are pairing elements within Ω. The following lemma shows that a single-layer scattering operator is invertible with two interlaced pairings.\nLemma 2.1. If two pairings π0 and π1 of {1, ...,d} are interlaced then any x ∈ Rd whose coordinates have more than 2 different values can be recovered from the values of S1x computed with π0 and the values of S1x computed with π1.\nProof. Let us consider a triplet n1,n2,n3 where (n1,n2) is a pair in π0 and (n1,n3) is a pair in π1. From S1x computed with π0 we get\nx(n1)+ x(n2), |x(n1)− x(n2)|\nand we saw in (3) that it determines the values of {x(n1),x(n2)} up to a permutations. Similarly, {x(n1),x(n3)} are determined up to a permutation by S1x computed with π1. Then unless x(n1) 6= x(n2) and x(n2) = x(n3) the three values x(n1),x(n2),x(n3) are recovered. The interlacing condition implies that π1 pairs n2 to an index n4 which can not be n3 or n1. Thus, the four values of x(n1),x(n2),x(n3),x(x4) are specified unless x(n4) = x(n1) 6= x(n2) = x(n3). This interlacing argument can be used to extend to {1, . . . ,d} the set of all indices ni for which x(ni) is specified, unless x takes only two values.\nProof of Theorem 2.2. Suppose that the 2J Haar scatterings are associated to the J hierarchical pairings (πε11 , ...,π εJ J ) where ε j ∈ {0,1}, where for each j, π0j and π1j are two interlaced pairings of d elements. The sequence (ε1, ...,εJ) is a binary vector taking 2J different values.\nThe constraint on the signal x is that each of the intermediate scattering coefficients takes more than 2 distinct values, which holds for x ∈ Rd except for a union of hyperplanes which has zero measure. Thus for almost every x ∈ Rd , the theorem follows from applying Lemma 2.1 recursively to the j-th level scattering coefficients for J−1≥ j ≥ 0.\nLemma 2.1 proves that only two pairings is sufficient to invert one Haar scattering layer. The argument proving that 2J pairings are sufficient to invert J layers is quite brute-force. It is conjectured that the number of pairings needed to obtain a complete representation for almost all x ∈ Rd does not need to grow exponentially in J but rather linearly. Theorem 2.2 suggests to define a signal representations by aggregating different Haar orthogonal scattering transforms. We shall see that this bagging strategy is indeed improving classification accuracy."
    }, {
      "heading" : "2.3 Sparse unsupervised learning with adaptive contractions",
      "text" : "A free orthogonal Haar scattering transform of depth J is computed with a pairing at each of the J network layers, which may be chosen freely. We now explain how to optimize these pairings from N unlabeled examples {xi}1≤i≤N . As previously explained, an orthogonal Haar scattering is strongly contractive. Each linear Haar operator rotates the signal space, and the absolute value suppresses the sign of each difference, and hence projects coefficients over a smaller domain. Optimizing the network thus amounts to find the best directions along which to perform the space compression.\nContractions reduce the space volume and hence the variance of scattering vectors but it may also collapse together examples which belong to different classes. To maximize the “average discriminability” among signal examples, we shall thus maximize the variance of the scattering transform over the training set. Following [19], we show that it yields a representation whose coefficients are sparsely excited.\nThe network layers are optimized with a greedy layerwise strategy similar to many deep unsupervised learning algorithms [9, 2], which consists in optimizing the network parameters layer per layer, as the depth j increases. Let us suppose that Haar scattering operators H` are computed for 1≤ ` < j. One can thus compute S jx for any x ∈ Rd . We now explain how to optimize H j to maximize the variance of the next layer S j+1x. The non-normalized empirical variance of S j over the training set {xi}i is\nσ2(S jx) = ∑ i ‖S jxi‖2− ∥∥∥∑ i S jxi ∥∥∥2 .\nThe following proposition, adapted from [19], proves that the scattering variance decreases as the depth increases, up to a factor 2. It gives a condition on H j to maximize the variance of the next layer.\nProposition 2.1. For any j ≥ 0 and x ∈ Rd , σ2(2−( j+1)/2S j+1x) ≤ σ2(2− j/2S jx). Maximizing σ2(S j+1x) given S jx is equivalent to finding H j which minimizes∥∥∥∑\ni H jS jxi ∥∥∥2 = ∑ n ( ∑ i |H jS jxi(n)| )2 . (8)\nProof. Since S j+1x = |H jS jx| and ‖H jS jx‖= 21/2‖S jx‖, we have\nσ2(S j+1x) = ∑ i ‖S j+1xi‖2− ∥∥∥∑ i S j+1xi ∥∥∥2\n= 2 N\n∑ i=1 ‖S jxi‖2− ∥∥∥ N∑ i=1 |H jS jxi| ∥∥∥2. Optimizing σ2(S j+1x) is thus equivalent to minimizing (8). Moreover,\nσ2(S j+1x) = 2 N\n∑ i=1 ‖S jxi‖2− ∥∥∥H j N∑ i=1 S jxi ∥∥∥2 +∥∥∥ N∑ i=1 H jS jxi ∥∥∥2−∥∥∥ N∑ i=1 |H jS jxi| ∥∥∥2 = 2 N\n∑ i=1 ‖S jxi‖2−2‖\nN\n∑ i=1 S jxi‖2 + (∥∥∥ N∑ i=1 H jS jxi ∥∥∥2−∥∥∥ N∑ i=1 |H jS jxi| ∥∥∥2)\n≤ 2 N\n∑ i=1 ‖S jxi‖2−2‖\nN\n∑ i=1 S jxi‖2 = 2σ2(S jx),\nwhich proves the first claim of the proposition.\nThis propocsition relies on the energy conservation ‖H jy‖= 21/2‖y‖. Because of the contraction of the absolute value,\nit proves that the variance of the normalized scattering 2− j/2S jx decreases as j increases. Moreover the maximization of σ2(S j+1x) amounts to minimize a mixed l1 and l2 norm on H jS jxi(n), where the sparsity l1 norm is along the realization index i where as the l2 norm is along the feature index n of the scattering vector.\nMinimizing the first l1 norm for n fixed tends to produce a coefficient indexed by n which is sparsely excited across the examples indexed by i. It implies that this feature is discriminative among all examples. On the contrary, the l2 norm along the index n has a tendency to produce l1 sparsity norms which have a uniformly small amplitude. The resulting “features” indexed by n are thus uniformly sparse.\nBecause H j preserves the norm, the total energy of coefficients is conserved:\n∑ n ∑ i |H jS jxi(n)|2 = 2∑ i ‖S jxi‖2.\nIt results that a sparse representation along the index i implies that H jS jxi(n) is also sparse along n. The same type of result is thus obtained by replacing the mixed l1 and l2 norm (8) by a simpler l1 sparsity norm along both the i and n variables\n∑ n ∑ i |H jS jxi(n)| . (9)\nThis sparsity norm is often used by sparse autoencoders for unsupervised learning of deep networks [2]. Numerical results in Section 4 verify that both norms have very close classification performances.\nFor Haar operators H j, the l1 norm leads to a simpler interpretation of the result. Indeed a Haar filtering is defined by a pairing π j of d integers {1, ...,d}. Optimizing H j amounts to optimize π j, and hence minimize\n∑ n ∑ i |H jS jxi(n)|= ∑ n\n( S jxi(π j(2n))+S jxi(π j(2n+1))+ |S jxi(π j(2n))−S jxi(π j(2n+1))| ) .\nBut ∑n(S jx(π j(2n))+S jx(π j(2n+1))) = ∑n S jx(n) does not depend upon the pairing π j. Minimizing the l1 norm (9) is thus equivalent to minimizing\n∑ n ∑ i |S jxi(π j(2n))−S jxi(π j(2n+1))|. (10)\nIt minimizes the average variation within pairs, and thus tries to regroup pairs having close values. Finding a linear operator H j which minimizes (8) or (9) is a “dictionary learning” problem which is in general an NP hard problem. For a Haar dictionary, we show that it is equivalent to a pair matching problem and can thus be solved with O(d3) operations. For both optimization norms, it amounts to finding a pairing π j which minimizes an additive cost\nC(π j) = ∑ n C(π j(2n),π j(2n+1)), (11)\nwhere C(π j(2n),π j(2n+1)) = ∑i |H jS jxi(n)| for (9) and C(π j(2n),π j(2n+1)) = ( ∑i |H jS jxi(n)| )2 for (8). This linear pairing cost is minimized exactly by the Blossom algorithm with O(d3) operations. Greedy method obtains a 1/2- approximation in O(d2) time [24]. Randomized approximation similar to [12] could also be adapted to achieve a complexity of O(d logd) for very large size problems.\nTheorem 2.2 proves that several Haar scattering transforms are necessary to obtain a complete signal representation. We learn T Haar scattering transforms by dividing the training set {xi}i in T non-overlapping subsets. A different Haar scattering transform is optimized for each training subset. Next section describes a supervised classifier applied to the resulting bag of T Haar scattering transforms."
    }, {
      "heading" : "2.4 Supervised feature selection and classification",
      "text" : "Strong invariants are computed by the supervised classifier which essentially computes adapted linear combinations of Haar scattering coefficients. Bagging T orthogonal Haar scattering representations defines a set of T d scattering coefficients. A supervised dimension reduction is first performed by selecting a subset of scattering coefficients. It is implemented with an orthogonal least square forward selection algorithm [5]. The final supervised classification is implemented\nwith a Gaussian kernel SVM classifier applied to this reduced set of coefficients. We select K scattering coefficient to discriminate each class c from all other classes, and decorrelate these features before applying the SVM classifier. Discriminating a class c from all other classes amounts to approximating the indicator function\nfc(x) = { 1 if x belongs to class c 0 otherwise .\nLet us denote by Φx = {φpx}p≤T d the dictionary of T d scattering coefficients to which is added the constant φ0x = 1. An orthogonal least square linearly approximates fc(x) with a sparse subset of K scattering coefficients {φpk}k≤K which are greedily selected one at a time. To avoid correlations between selected features, it includes a Gram-Schmidt orthogonalization which decorrelates the scattering dictionary relatively to previously selected features. We denote by Φkx = {φ̃ kpx}p the scattering dictionary, which was orthogonalized and hence decorrelated relatively to the first k selected scattering features. For k = 0, we have Φ0x = Φx. At the k+1 iteration, we select φ kpk x ∈Φ\nkx which yields the minimum linear mean-square error over training samples:\n∑ i\n( fc(xi)− k\n∑̀ =0\nα` φ `p`xi )2\n(12)\nBecause of the orthonormalization step, the linear regression coefficients are\nα` = ∑ i fc(xi)φ `p`xi\nand\n∑ i\n( fc(xi)− k\n∑̀ =0\nα` φ `p`xi )2\n= ∑ i | fc(xi)|2−\nk\n∑̀ =0 α2` .\nThe error (12) is thus minimized by choosing φ kpk+1x having a maximum correlation:\nαk = ∑ i fc(xi)φ kpk xi = argmaxp\n( ∑\ni fc(xi)φ kpxi\n) .\nThe scattering dictionary is then updated by orthogonalizing each of its element relatively to the selected scattering feature φ kpkx:\nφ k+1p x = φ k px− ( ∑\ni φ kpxi φ\nk pk xi ) φ kpk x .\nThis orthogonal least square regression greedily selects the K decorrelated scattering features {φ kpk x}0≤k<K for each class c. For a total of C classes, the union of all these features defines a dictionary of size M = KC. They are linear combinations of the original Haar scattering coefficients {φpx}p. In the context of a deep neural network, this dimension reduction can be interpreted as a last fully connected network layer, which takes in input T d scattering coefficients and outputs a vector of size M. The parameter M optimizes the bias versus variance trade-off. It may be set a priori or adjusted by cross validation in order to yield a minimum classification error at the output of the Gaussian kernel SVM classifier.\nA Gaussian kernel SVM classifier is applied to the M-dimensional orthogonalized scattering feature vectors. The Euclidean norm of this vector is normalized to 1. In the applications of Section 4, M is set to 103 and hence remains large. Since the feature vectors lies on a high-dimensional unit sphere, the standard deviation σ of the Gaussian kernel SVM must be of the order of 1. Indeed, a Gaussian kernel SVM performs its classification by fitting separating hyperplane over different balls of radius of radius σ . If σ 1 then the number balls covering the unit sphere grows like σ−M . Since M is large, σ must remain in the order of 1 to insure that there are enough training samples to fit a hyperplane in each ball."
    }, {
      "heading" : "3 Orthogonal Haar Scattering on Graphs",
      "text" : "Signals such as images are sampled on uniform grids. Many classification problems are translation invariant, which motivates the calculation of translation invariant representations. A translation invariant representation can be computed\nby averaging signal samples, but it removes too much information. Wavelet scattering operators [18] are calculated by cascading wavelet transforms and absolute values. Each wavelet transform computes multiscale signal variations on the grid. It yields a large vector of coefficients, whose spatial averaging defines a rich set of translation invariant coefficients.\nData vectors may be defined on non-uniform graphs [28], for example in social, financial or transportation networks. A graph displacement moves data samples on the grid but is not equivalent to a uniform grid translation. Orthogonal Haar scattering transforms on graphs are computed from local multiscale signal variations on the graph. The calculation of displacement invariant features is left to the final supervised classifier, which adapts the averaging to the classification problem. Section 3.1 introduces this Har scattering on a graph as a particular case of orthogonal Haar scattering. Section 3.3 proves that an orthogonal Haar scattering on graphs can be written as a product of wavelet transforms, as usual wavelet scattering operators. When the graph connectivity is unknown, unsupervised learning can calculate a Haar scattering on the unknown graph and estimate the graph connectivity. The consistency of such estimations is studied in Section 3.4."
    }, {
      "heading" : "3.1 Structured orthogonal Haar scattering",
      "text" : "The free orthogonal Haar scattering transform of Section 2 freely associates any two elements of an internal network layer S jx. A Haar scattering on a graph is constructed by pairing elements according to their position in the graph, which requires to structure the pairing and the network layers. We denote by V the set of d vertices of this graph, and assume that d is a power of 2. The vector S jx of size d is structured as a two-dimensional array S jx(n,q) of size 2− jd×2 j. For each j ≥ 0, we shall see that n ∈ {1, ...,2− jd} is a “spatial” index of a set of Vj,n of 2 j graph vertices. The 2 j parameters q are indexing different permutation invariant coefficients computed from the values of x in Vj,n. The input network layer is S0x(n,0) = x(n). We compute S j+1x by pairing the 2− jd rows of S jx. The row pairing\nπ j = { (π j(2n),π j(2n+1)) } 0≤n<2− jd . (13)\nis pairing each (π j(2n),q) with (π j(2n+ 1),q) for 0 ≤ q < 2 j. It imposes a row structure on the free pairing of Section 2.1. Applying the absolute Haar filter (2) to each pair gives\nS j+1x(n,2q) = S jx(π j(2n),q)+S jx(π j(2n+1),q) (14)\nand S j+1x(n,2q+1) = |S jx(π j(2n),q)−S jx(π j(2n+1),q)| . (15)\nApplying these equation for j ≤ J defines a structured Haar network illustrated in Figure 2. If we remove the absolute value from (15) then these equations iterate linear Haar filters and define an orthogonal Walsh transform [6]. The absolute value completely modifies the properties of this transform but Section 3.3 proves that it can be still be written as a product of orthogonal Haar wavelet transforms, alternating with absolute value non-linearities.\nThe following proposition proves that this structured Haar scattering is a transformation on a hierarchical grouping of the graph vertices, derived from the row pairing (13). Let V0,n = {n} for n ∈V . For any j ≥ 0 and n ∈ {1, ...,2− j−1d}, we\ndefine Vj+1,n =Vj,π j(2n)∪Vj,π j(2n+1). (16)\nWe verify by induction on j that it defines a partition V = ∪nVj,n, where each Vj,n is a set of 2 j vertices.\nProposition 3.1. The coefficients {SJx(n,q)}0≤q<2 j are computed by applying a Hadamard matrix to the restriction of x to VJ,n. This Hadamard matrix depends on x, J and n.\nProof. Theorem 2.1 proves that {SJx(n,q)}0≤q<2J is computed by applying am orthogonal transform to x. To prove that it is a Hadamard matrix, it is sufficient to show that its entries are ±1. We verify by induction on j ≤ J that S jx(n,q) only depends on restriction of x to Vj,n, by applying (15) and (14) together with (16). We also verify that each x(v) for v ∈Vj,n appears exactly once in the calculation, with an addition or a subtraction. Because of the absolute value, the addition or subtraction which are 1 and −1 in the Hadamard matrix, which therefore depends upon x, J and n\nAn orthogonal Haar scattering on a graph can thus be interpreted as an adaptive Hadamard transform over groups of vertices, which outputs positive coefficients. Walsh matrices are particular cases of Hadamard matrices. The induction (16) defines sets Vj,n with connected nodes in the graph if for all j and n, each pair (π j(2n),π j(2n+1)) regroups two sets Vj,π j(2n) and Vj,π j(2n+1) which are connected. It means that at least one element of Vj,π j(2n) is connected to one element of Vj,π j(2n+1). There are many possible connected dyadic partitions of any given graph. Figure 3(a,b) shows two different examples of connected graph partitions.\nFor images sampled on a square grid, a pixel is connected with 8 neighbors. A structured Haar scattering can be computed by pairing neighbor image pixels, alternatively along rows and columns as the depth j increases. When j is even, each Vj,n is then a square group of 2 j pixels, as illustrated in Figure 3(c). Shifting such a partition defines a new partition. Neighbor pixels can also be grouped in the diagonal direction which amounts to rotate the sets Vj,n by π/4 to\ndefine a new dyadic partition. Each of these partitions define a different structured Haar scattering. Section 4 applies these structured Haar image scattering to image classification."
    }, {
      "heading" : "3.2 Scattering order",
      "text" : "Scattering coefficients have very different properties depending upon the number of absolute values which are used to compute them. A scattering coefficient of order m is a coefficient computed by cascading m absolute values. Their amplitude have a fast decay as the order m increases, and their locations are specified by the following proposition.\nProposition 3.2. If q = 0 then S jx(n,q) is a coefficient of order 0. Otherwise, S jx(n,q) is a coefficient of order m ≤ j if there exists 0≤ j1 < ... < jm < j such that\nq = m\n∑ k=1\n2 j− jk . (17)\nThere are ( j\nm\n) 2− jd coefficients of order m in S jx.\nProof. This proposition is proved by induction on j. For j = 0 all coefficients are of order 0 since S0x(n,0) = x(n). If S jx(n,q) is of order m then (14) and (15) imply that S j+1x(n,2q) is of order m and S j+1x(n,2q+1) is of order m+1. It results that (17) is valid for j+1 if is valid for j.\nThe number of coefficients S jx(n,q) of order m corresponds to the number of choices for q and hence for 0 ≤ j1 < ... < jm < j, which is ( j m ) . This must be multiplied by the number of indices n which is 2− jd.\nThe amplitude of scattering coefficients typically decreases exponentially when the scattering order m increases, because of the contraction produced by the absolute value. High order scattering coefficients can thus be neglected. This is illustrated by considering a vector x of independent Gaussian random variables of variance 1. The value of S jx(n,q) only depends upon the values of x in Vj,n. Since Vj,n does not intersect with Vj,n′ if n 6= n′, we derive that S j(n,q) and S j(n′,q) are independent. They have same mean and same variance because x is identically distributed. Scattering coefficients are iteratively computed by adding pairs of such coefficients, or by computing the absolute value of their difference. Adding two independent random variables multiplies their variance by 2. Subtracting two independent random variables of same mean and variance yields a new random variable whose mean is zero and whose variance is multiplied by 2. Taking the absolute value reduces the variance by a factor which depends upon its probability distribution. If this distribution is Gaussian then this factor is 1−2/π . If we suppose that this distribution remains approximately Gaussian, then applying m absolute values reduces the variance by approximately (1− 2/π)m. Since there are (J m ) coefficients of order m, their\ntotal normalized variance σ2m,J is approximated by (J m ) (1− 2/π)m. Table 1 shows that (J m ) (1− 2/π)m is indeed of the same order of magnitude as the value σ2m,J computed numerically. This variance becomes much smaller for m > 4. This observation remains valid for large classes of signals x. Scattering coefficients of order m > 4 usually have a negligible energy and are thus removed in classification applications."
    }, {
      "heading" : "3.3 Scattering with orthogonal Haar wavelet bases",
      "text" : "We now prove that scattering coefficients of order m are obtained by cascading m orthogonal Haar wavelet transforms defined on the graph. Haar wavelets can easily be constructed on graphs [7, 26]. Section 3.1 shows that a Haar scattering on a graph is constructed over dyadic partitions {Vj,n}n of V , which are obtained by progressively aggregating vertices by pairing Vj+1,n =Vj,π j(2n)∪Vj,π j(2n+1). We denote by 1V j,n(v) the indicator function of Vj,n in V . A Haar wavelet computes\nthe difference between the sum of signal values over two aggregated sets:\nψ j+1,n = 1V j,π j(2n) −1V j,π j(2n+1) . (18)\nInner products between signals defined on V are written\n〈x,x′〉= ∑ v∈V x(v)x′(v).\nFor any 2J < d, {1VJ,n}0≤n<2−Jd ∪{ψ j,n}0≤n<2− jd,0≤ j<J (19)\nis a family of d orthogonal Haar wavelets which define an orthogonal basis of Rd . The following theorem proves that order m+1 coefficients are obtained by computing the orthogonal Haar wavelet transform of coefficients of order m. The proof is in Appendix A.\nTheorem 3.1. Let q = ∑mk=1 2 j− jk with j1 < ... < jm ≤ j. If jm+1 > jm then for each n≤ 2− j−1d\nS jx(n,q+2 j− jm+1) = ∑ p\nVjm+1 ,p ⊂Vj,n\n|〈S jm x(·,2 jm− jq),ψ jm+1,p〉|. (20)\nwith\nS jmx(.,q ′) =\n2− jm d−1\n∑ n=0 S jmx(n,q ′)1V jm,n .\nIf q = ∑mk=1 2 j− jk and jm+1 > jm then S jmx(n,2 jm− jq) are coefficients of order m whereas S jx(n,q+ 2 j− jm+1) is a coefficient of order m+ 1. Equation (20) proves that a coefficient of order m+ 1 is obtained by calculating the wavelet transform of scattering coefficients of order m, and summing their absolute values. A coefficient of order m+ 1 thus measures the averaged variations of the m-th order scattering coefficients on neighborhoods of size 2 jm+1 in the graph. For example, if x is constant in a Vj,n then S`x(n,q) = 0 if `≤ j and q 6= 0."
    }, {
      "heading" : "3.4 Learning graph connectivity by variation minimization",
      "text" : "In many problems the graph connectivity is unknown. Learning a connected dyadic partitions is easier than learning the full connectivity of a graph, which is typically an NP complete problem. Section 2.3 introduces a polynomial complexity algorithm, which learns pairings in orthogonal Haar scattering networks. For a Haar scattering on a graph, we show that this algorithms amounts to computing dyadic partitions where scattering coefficients have a minimum total variation. The consistency of this pairing algorithm is studied over particular Gaussian stationary processes, and we show that there is no curse of dimensionality.\nSection 2.3 introduces two criteria to optimize the pairing π j of a free orthogonal Haar scattering, from a training set {xi}i≤N . We concentrate on the l1 norm minimization, which has a simpler expression. For a Haar scattering on a graph, the l1 minimization (10) computes a row pairing π j which minimizes\nN\n∑ i=1\nd2−( j+1)\n∑ n=0\n2 j−1\n∑ q=0 |S jxi(π j(2n),q)−S jxi(π j(2n+1),q)|. (21)\nThis optimal pairing regroups vertex sets Vj,π j(2n) and Vj,π j(2n+1) whose scattering coefficients have a minimum total variation.\nSuppose that the N training samples xi are independent realizations of a random vector x. To guarantee that this pairing finds connected sets we must make sure that the total variation minimization favors regrouping neighborhood points, which means that x has some form of regularity on the graph. We also need N to be sufficiently large so that this minimization finds connected sets with high probability, despite statistical fluctuations. Avoiding the curse of dimensionality means that N should not grow exponentially with the signal dimension d.\nTo attack this problem mathematically, we consider a very particular case, where signals are defined on a ring graph, and are thus d periodic. Two indices n and n′ are connected if |n− n′| = 1 mod d. We study the optimization of the first network layer for j = 0, where S0x(n,q) = x(n). The minimization of (21) amounts to compute a pairing π which minimizes\nN\n∑ i=1\n( d/2−1\n∑ n=0 |xi(π(2n))− xi(π(2n+1))|\n) . (22)\nThis pairing is connected if and only if for all n, |π(2n)−π(2n+1)|= 1 mod d.\nThe regularity and statistical fluctuations of x(n) are controlled by supposing that x is a circular stationary Gaussian process. The stationarity implies that its covariance matrix Cov(x(n),x(m)) = Σ(n,m) depends on the distance between points Σ(n,m) = ρ((n−m) mod d). The average regularity depends upon the decay of the correlation ρ(u). We denote by ‖Σ‖op the sup operator norm of Σ. The following theorem proves that the training size N must grow like d logd in order to compute an optimal pairing with a high probability. The constant is inversely proportional to a normalized “correlation gap,” which depends upon the difference between the correlation of neighborhood points and more far away points. It is defined by\n∆ = (√ 1− maxn≥2 ρ(n)\nρ(0) −\n√ 1− ρ(1)\nρ(0)\n)2 . (23)\nTheorem 3.2. Given a circular stationary Gaussian process with ∆ > 0, the pairing which minimizes the empirical total variation (22) has probability larger than 1− ε to be connected if\nN > π3‖Σ‖op 2∆ d ( 3logd− logε ) . (24)\nThe proof is based on the Gaussian concentration inequality for Lipschitz function [20, 23] and is left to Appendix B . Figure 4 displays numerical results obtained with a Gaussian stationary process of dimension d where ρ(1)/ρ(0) = 0.44 and maxn≥2 ρ(n)/ρ(0) = 0.06. The gray level image gives the probability that a pairing is connected when computing this pairing by minimizing the total variation (22), as a function of the dimension d and of the number N of training samples. The black and white points correspond to probabilities 0 and 1 respectively. In this example, we see that the optimization gives a connected pairing with probability 1−ε for N increasing almost linearly with d, which is illustrated by the nearly straight line of dotted points corresponding to ε = 0.2. The Theorem gives an upper bound which grows like d logd, though the constant involved is not tight.\nFor layer j > 1, S jx(n,q) is no longer a Gaussian random vector due to the absolute value non-linearity. However, the result can be extended using a Talagrand-type concentration argument instead of the Gaussian concentration. Numerical experiments presented in Section 4 show that this approach does recover the connectivity of high dimensional images with a probability close to 100% for j ≤ 3, and that the probability decreases as j increases. This seems to be due to the\nfact that the absolute value contractions reduce the correlation gap ∆ between connected coefficients and more far away coefficients when j increases."
    }, {
      "heading" : "4 Numerical classification experiments",
      "text" : "Haar scattering representations are tested on classification problems, over images sampled on a regular grid or an irregular graph. We consider the cases where the grid or the graph geometry is known a priori, or discovered by unsupervised learning. The efficiency of free and structured Haar scattering architectures are compared with state of the art classification results obtained by deep neural network learning, when the graph geometry is known or unknown. Although computations are reduced to additions and subtractions, we show a Haar scattering can get state art results when the graph geometry is unknown even over complex image data bases. For images over a known uniform sampling grid, we show that the simplifications of a Haar scattering produces an error about 20% larger than state of the art unsupervised learning algorithms.\nA Haar scattering classification involves few parameters which are reviewed. The scattering scale 2J ≤ d is the permutation invariance scale. Scattering coefficients are computed up to the a maximum order m, which is set to 4 in all experiments. Indeed, higher order scattering coefficient have a negligible relative energy, which is below 1%, as explained in Section 3.2. The unsupervised learning algorithm computes T different Haar scattering transforms by subdividing the training set in T subsets. Increasing T decreases the classification error but it increases computations. The error decay becomes negligible for T ≥ 40. The supervised dimension reduction selects a final set of M orthogonalized scattering coefficients. We set M = 1000 in all numerical experiments."
    }, {
      "heading" : "4.1 Classification of image digits in MNIST",
      "text" : "MNIST is a data basis with 6× 104 hand-written digit images of size d ≤ 210. There are 10 classes (one per digit) with 5× 104 images for training and 104 for testing. Examples of MNIST images are shown in Figure 5. To test the classification performances of a Haar scattering when the geometry is unknown, we scramble all image pixels with the same unknown random permutations, as shown in Figure 5.\nWhen the image geometry is known, i.e. using non-scrambled images, the best MNIST classification results without data augmentation are given in Table 6a. Deep convolution networks with supervised learning reach an error of 0.53% [15], and unsupervised learning with sparse coding have a slightly larger error of 0.59% [13]. A wavelet scattering computed with iterated Gabor wavelet transforms yields an error of 0.46% [3].\nFor a known image grid geometry, we compute a structured Haar scattering by pairing neighbor image pixels. It builds hierachical square subsets Vj,n illustrated in Figure 3(c). The invariance scale is 2J = 26, which corresponds to blocks of 8× 8 pixels. Random shift and rotations of these pairing define T = 64 different Haar scattering transforms. The supervised classifier of Section 2.4 applied to this structured Haar scattering yields an error of 0.59%.\nMNIST digit classification is a relatively simple problem where the main source of variability are due to deformations of hand-written image digits. In this case, supervised convolution networks, sparse coding, Gabor wavelet scattering and orthogonal Haar scattering have nearly the same classification performances. The fact that a Haar scattering is only based on additions and subtractions does not affect its efficiency.\nFor scrambled images, the connectivity of image pixels is unknown and needs to be learned from data. Table 6b gives the classification results of different learning algorithms. The smallest error of 0.79% is obtained with a Deep Belief optimized with a supervised backpropagation. Unsupervised learning of T = 50 structured Haar scattering followed by a feature selection and a supervised SVM classifier produces an error of 0.90%. Figure 7 gives the classification error rate as a function of T , for different values of maximum scale J. The error rates decrease slowly for T > 10, and do not improve beyond T = 50, which is much smaller than 2J .\nThe unsupervised learning computes connected dyadic partitions Vj,n from scrambled images by optimizing an l1 norm. At scales 1 ≤ 2 j ≤ 23, 100% of these partitions are connected in the original image grid, which proves that the geometry is well estimated at these scales. This is only evaluated on meaningful pixels which do not remain zero on all training images. For j = 4 and j = 5 the percentages of connected partitions are respectively 85% and 67%. The percentage of connected partitions decreases because long range correlations are weaker.\nA free orthogonal Haar scattering does not impose any condition on pairings. It produces a minimum error of 1% for T = 20 Haar scattering transforms, computed up to the depth J = 7. This error rate is higher because the supplement of freedom in the pairing choice increases the variance of the estimation."
    }, {
      "heading" : "4.2 CIFAR-10 images",
      "text" : "CIFAR-10 is a data basis of tiny color images of 32× 32 pixels. It includes 10 classes, such as “dogs”, “cars”, “ships” with a total of 5× 104 training examples and 104 testing examples. There are much more intra-class variabilities than in MNIST digit images, as shown by Figure 8. The 3 color bands are represented with Y,U,V channels, and scattering coefficients are computed independently in each channel.\nWhen the image geometry is known, a structured Haar scattering is computed by pairing neighbor image pixels. The best performance is obtained at the scale 2J = 26 which is below the maximum scale d = 210. Similarly to MNIST, we compute T = 64 connected dyadic partitions for randomly translated and rotated grids. After dimension reduction, the classification error is 21.3%. This error is above state of the art results of unsupervised learning algorithms by about\n20%, but it involves no learning. A minimum error rate of 16.9% is obtained by Receptive Field Learning [11]. The Haar scattering error is also above the 17.8% error obtaiend by a roto-translation invariant wavelet scattering network [22], which computes wavelet transforms along translation and rotation parameters. Supervised deep convolution networks provide an important improvement over all unsupervised techniques and reach an error of 9.8%. The study of these supervised networks is however beyound the scope of this paper. Results are summarized in Table 9a.\nWhen the image grid geometry is unknown, because of random scrambling, Table 9a summarizes results with different algorithms. For unsupervised learning with structured Haar scattering, the minimum classification error is reached at the scale 2J = 27, which maintains some localization information on scattering coefficients. With T = 10 connected dyadic partitions, the error is 27.3%. Table 9b shows that it is 10% below previously reported results on this data basis.\nNearly 100% of the dyadic paritions Vj,n computed from scrambled images are connected in the original image grid, for 1 ≤ j ≤ 4, which shows that the multiscale geometry is well estimated at these fine scales. For j = 5,6 and 7, the proportions of connected partitions are 98%, 93% and 83% respectively. As for MNIST images, the connectivity estimation becomes less precise at large scales. Similarly to MNIST, a free Haar scattering yields a higher classification error of 29.2%, with T = 20 scattering transforms up to layer J = 6."
    }, {
      "heading" : "4.3 CIFAR-100 images",
      "text" : "CIFAR-100 also contains tiny color images of the same size as CIFAR-10 images. It has 100 classes containing 600 images each, of which 500 are training images and 100 are for testing. Our tests on CIFAR-100 follows the same procedures as in Section 4.2. The 3 color channels are processed independently.\nWhen the image grid geometry is known, the results of a structured Haar scattering are summarized in Table 10. The best performance is obtained with the same parameter combination as in CIFAR-10, which is T = 64 and 2J = 26. After dimension reduction, the classification error is 47.4%. As in CIFAR-10, this error is about 20% larger than state of the art unsupervised methods, such as a Nonnegative OMP (39.2%)[17]. A roto-translation wavelet scattering has an error of 43.7%. Deep convolution networks with supervised training produce again a lower error of 34.6%.\nFor scrambled images of unknown geometry, with T = 10 transforms and a depth J = 7, a structured Haar scattering has an error of 52.7%. A free Haar orthogonal scattering has a higher classification error of 56.1%, with T = 10 scattering transforms up to layer J = 6. No such result is reported with another algorithm on this data basis.\nOn all tested image databases, structured Haar scattering has a consistent 7%-10% performance advantage over ‘free’\nHaar scattering, as shown by Table 2. For orthogonal Haar scattering, all reported errors were calculated with an unsupervised learning which minimizes the l1 norm (9) of scattering coefficients, layer per play. As expected, Table 2 shows that minimizing a mixed l1 and l2 norm (8) yields nearly the same results on all data bases."
    }, {
      "heading" : "4.4 Images on a graph over a sphere",
      "text" : "A data basis of irregularly sampled images on a sphere is provided in [4]. It is constructed by projecting the MNIST image digits on d = 4096 points randomly sampled on the 3D sphere, and by randomly rotating these images on the sphere. The random rotation is either uniformly distributed on the sphere or restricted with a smaller variance (small rotations) [4]. The digit ‘9’ is removed from the data set because it can not be distinguished from a ‘6’ after rotation. Examples sphere digits are shown in Figure 11. This geometry of points on the sphere can be described by a graph which connects points having a sufficiently small distance on the sphere.\nThe classification algorithms introduced in [4] take advantage of the known distribution of points on the sphere, with a representation based on the graph Laplacian. Table 3 gives the results reported in [4], with a fully connected neural network, and with a spectral graph Laplacian network.\nAs opposed to these algorithms, the unsupervised structured Haar scattering algorithm does not use this geometric information and learns the graph information by pairing. Computations are performed on a scrambled set of signal values. Haar scattering transforms are calculated up to the maximum scale 2J = d = 212. A total of T = 10 connected dyadic partitions are estimated by unsupervised learning, and the classification is performed from M = 103 selected coefficients. Although the graph geometry is unknown, the structured Haar scattering reduces the error rate both for small and large 3D random rotations.\nIn this case a free orthogonal Haar scattering has a smaller error rate than a structured Haar scattering for small rotations, but a larger error for large rotations. It illustrates the tradeoff between the structural bias and the feature variance in the choice of the algorithms. For small rotation, the variability within classes is smaller and a free scattering can take advantage of more degrees of freedom. For large rotations, the variance is too large and dominates the problem.\nTwo points of the sphere of radius 1 are considered to be connected if their geodesic distance is smaller than 0.1. With this convention, over the 4096 points, each point has on average 8 connected neighbors. The unsupervised Haar learning\nperforms a hierachical pairing of points on the sphere. For small and large rotations, the percentage of connected sets Vj,n remains above 90% for 1≤ j ≤ 4. This is computed over 70% of the points points having a nonneglegible energy. It shows that the multiscale geometry on the sphere is well estimated by hierachical pairings."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This work was supported by the ERC grant InvariantClass 320959."
    }, {
      "heading" : "A Proof of Theorem 3.1",
      "text" : "Proof of Theorem 3.1. We derive from the definition of a scattering transform in equations (3,4) in the text that\nS j+1x(n,2q) = S jx(π j(2n),q)+S jx(π j(2n+1),q) = 〈S jx(·,q),1V j+1,n〉,\nS j+1x(n,2q+1) = |S jx(π j(2n),q)−S jx(π j(2n+1),q)|= |〈S jx(·,q),ψ j+1,n〉|.\nwhere Vj+1,n =Vj,π j(2n)∪Vj,π j(2n+1). Define κ = 2 − jq = ∑mk=1 2− jk . Observe that\n2 jm+1(κ +2− jm+1) = 2 jm+1κ +1 = 2(2 jm+1−1κ)+1,\nthus S jm+1x(n,2 jm+1(κ +2− jm+1)) is calculated from the coefficients S jm+1−1x(n,2 jm+1−1κ) of the previous layer with\nS jm+1x(n,2 jm+1(κ +2− jm+1)) = |〈S jm+1−1x(·,2 jm+1−1κ),ψ jm+1,n〉|. (A.1)\nSince 2 j+1κ = 2 · 2 jκ , the coefficient S jm+1−1x(n,2 jm+1−1κ) is calculated from S jmx(n,2 jmκ) by ( jm+1− 1− jm) times additions, and thus\nS jm+1−1x(n,2 jm+1−1κ) = 〈S jmx(·,2 jmκ),1V jm+1−1,n〉. (A.2)\nCombining equations (A.2) and (A.1) gives\nS jm+1x(n,2 jm+1(κ +2− jm+1)) = |〈S jmx(·,2 jmκ),ψ jm+1,n〉|. (A.3)\nWe go from the depth jm+1 to the depth j ≥ jm+1 by computing\nS jx(n,2 j(κ +2− jm+1)) = 〈S jm+1x(·,2 jm+1(κ +2− jm+1)),1V j,n〉.\nTogether with (A.3) it proves the equation (20) of the proposition. The summation over p,Vjm+1,p ⊂Vj,n comes from the inner product 〈1V jm+1 ,p ,1V j,n〉. This also proves that κ +2 − jm+1 is the index of a coefficient of order m+1."
    }, {
      "heading" : "B Proof of Theorem 3.2",
      "text" : "The theorem is proved by analyzing the concentration of the objective function around its expected value as the sample number N increases. We firstly introduce the Pisier and Maurey’s version of the Gaussian concentration inequality for Lipschitz functions.\nProposition B.1 (gaussian concentration for Lipschitz function [23, 20]). Let z1, ...zm be i.i.d N(0,1) random variabls, and f = f (z1, · · · ,zm) a 1-Lipschitz function, then there exists c0 > 0 so that\nPr[ f −E f > t]< exp{−c0t2} and Pr[ f −E f <−t]< exp{−c0t2}, ∀t > 0.\nIn the above proposition, the constant c0 = 2π2 according to [23] and 1/4 in [20]. To prove the theorem, recall that the pairing problem is computed by minimizing the l1 norm (22) which up to a\nnormalization amounts to compute:\nπ∗ = arg min π∈Πd F(π) with F(π) = 1 N\nN\n∑ i=1 ∑ (u,v)∈π |xi(u)− xi(v)|, (B.1)\nwhere π is a pairing of d elements and we denote by Πd the set of all possible such pairings. The following lemma proves that F(π) is a Lipschitz function of independent gaussian random variables, with a Lipschitz constant equal to ‖Σd‖ 1/2 op , where ‖Σd‖op is the operator norm of the covariance. We prove it on the normalized function f = N1/2d−1/2F .\nLemma B.1. Let xi = Σ 1/2 d zi with zi = (zi(1), ...,zi(d)) T ∼N (0, Id) i.i.d. Given any pairing π ∈Πd , define\nf ({(zi(v)}1≤i≤N,1≤v≤d) = 1√ dN\nN\n∑ i=1 ∑ (u,v)∈π |xi(u)− xi(v)|,\nthen f is a Lipschitz function with constant √ ‖Σd‖op, which does not depend on π .\nProof. With slight abuse of notation, denote by v = π(u) if two nodes u and v are paired by π , then we have\n∂ f ∂ zi(v′) = 1√ dN ∑(u,v)∈π Sgn(xi(u)− xi(v)) ∂ ∂ zi(v′) (xi(u)− xi(v))\n= 1√ dN\nd\n∑ u=1\nSgn(xi(u)− xi(π(u))) ∂\n∂ zi(v′) xi(u)\n= 1√ dN\nd\n∑ u=1 Sgn(xi(u)− xi(π(u)))(Σ1/2d )u,v′\n= 1√ dN (Σ1/2d Si)(v ′),\nwhere Si := (Sgn(xi(u)− xi(π(u))))du=1 is a vector of length d whose entries are ±1. Then\n‖Σ1/2d Si‖ ≤ √ ‖Σd‖opd,\nand it follows that\n‖∇z f‖2 = N\n∑ i=1\nd\n∑ v′=1 ∣∣∣∣ ∂ f∂ zi(v′) ∣∣∣∣2\n= N\n∑ i=1 1 dN ‖Σ1/2d Si‖ 2 ≤ ‖Σd‖op.\nObserve that the eigenvalues of Σd are the discrete Fourier transform coefficients of the periodic correlation function ρ(u)\nρ̂(k) = d−1\n∑ j=0 ρ( j)exp{−i2π jk d }=\nd−1 ∑ j=0 ρ( j)cos(2π jk d ), k = 0, ...,d−1.\nObserve that ∑du=1 Si(u) = 0 for each i, that is, Si is orthogonal to the eigenvector of ρ̂(0). So the Lipschitz constant√ ‖Σd‖op = √ maxk |ρ̂(k)| can be slightly improved to be √ maxk>0 |ρ̂(k)|.\nLet us now prove the claim of Theorem 3.2. Since the pairing has a probability larger than 1− ε to be connected if Pr[π∗ /∈ Π(0)d ] < ε , we need to show that under the inequality (24) the probability Pr[π\n∗ /∈ Π(0)d ] is less than ε . Let us denote\nαu = √\n2 π ·2(1−ρ(u)), and ᾱ2 = min 2≤u≤d/2 αu, (B.2)\nand define\nCρ = c0\n‖Σd‖op ( 1 2 (ᾱ2−α1) )2 . (B.3)\nThen Eqn. (24) can be rewritten as\nCρ N d > 3logd− logε. (B.4)\nAs a result of Proposition B.1 and Lemma B.1, if C = c0‖Σd‖op ·N/d then ∀π ∈Πd ,\nPr[F(π)−EF(π)> δ ]< exp{−Cδ 2} and Pr[F(π)−EF(π)<−δ ]< exp{−Cδ 2}, ∀δ > 0.\nObserve that\nΠd = d/2⋃ m=0 Π(m)d\nwhere Π(m)d are the set of pairings which have m non-neighbor pairs. Π (0) d is the set of pairings which only pair connected nodes in the graph, and for the ring graph Π(0)d = {π (0) 0 ,π (0) 1 } the two of which interlace. For any π ∈Π (m) d , suppose that there are ml pairs in π so that the distance between the two paired nodes is l, m1 = d/2−m, m2 + · · ·+md/2 = m. Recalling the definition of αk in Eq. (B.2), we verify that\nEF(π) = α1( d 2 −m)+α2m2 + · · ·+αd/2md/2 ≥ α1( d 2 −m)+ ᾱ2m\nwhen m≥ 1, and EF(π(0)0 ) = EF(π (0) 1 ) = α1 d 2 . Thus when m≥ 1, EF(π)−EF(π(0)0 )≥ (ᾱ2−α1)m, ∀π ∈Π (m) d .\nDefine δm =\n1 2 (ᾱ2−α1)m, m = 1, ...,d/2,\nand we have that\nPr[π∗ /∈Π(0)d ] = Pr[∃π ∈ d/2⋃ m=1 Π(m)d , F(π)< min{F(π (0) 0 ),F(π (0) 1 )}]\n≤ Pr[F(π(0)0 )> EF(π (0) 0 )+δ1]\n+Pr[F(π(0)0 )< EF(π (0) 0 )+δ1, ∃π ∈ d/2⋃ m=1 Π(m)d , F(π)< F(π (0) 0 )]\n≤ Pr[F(π(0)0 )> EF(π (0) 0 )+δ1]\n+Pr[∃π ∈ d/2⋃ m=1 Π(m)d , F(π)< EF(π)−δm], (by that (ᾱ2−α1)m−δ1 ≥ δm)\n≤ exp{−Cδ 21 }+ d/2\n∑ m=1 |Π(m)d |exp{−Cδ 2 m}\n= exp{−Cρ N d }+\nd/2 ∑ m=1 |Π(m)d |exp{−Cρ N d m2}, (B.5)\nwhere Cρ is as in Eq. (B.3). One can verify the following upper bound for the cardinal number of Π(m)d :\n|Π(m)d | ≤ d2m\n(2m)! .\nWith the crude bound (2m)!≥ 1, the above inequality inserted in (B.5) gives\nPr[π∗ /∈Π(0)d ]≤ exp{−Cρ N d }+\nd/2 ∑ m=1 d2m exp{−Cρ N d m2}. (B.6)\nIf we keep the factor (2m)!, the upper bound for the summation over m can be improved to be\nd2 exp{−Cρ N/d} d/2\n∑ m=1 ((2m)!)−1 ≤ c ·d2 exp{−Cρ N/d}\nwhere c = (e− 1)/2 is an absolute constant. By applying this in the final bound in the theorem, the constant in front of\nlogd is 2 instead of 3. The constant of the theorem is not tight, while the O(d logd) is believed to be the tight order as d increases.\nTo proceed, define the function\ng(x) =−Cρ N d · x2 +(2logd) · x, 1≤ x≤ d 2 ,\nand observe that max1≤x≤d/2 g(x) = g(1) whenever\nlogd Cρ N/d < 1,\nwhich holds as long as Eq. (B.4) is satisfied. Thus we have\nd/2 ∑ m=1 d2m exp{−Cρ N d m2} ≤ d/2 ∑ m=1 d2 exp{−Cρ N d }= d 3 2 exp{−Cρ N d },\nthen the inequality (B.6) becomes\nPr[π∗ /∈Π(0)d ]≤ ( d3 2 +1 ) exp{−Cρ N d } ≤ exp{−Cρ N d +3logd}.\nTo have Pr[π∗ /∈Π(0)d ]< ε , a sufficient condition is therefore\nexp{−Cρ N d +3logd}< ε, (B.7)\nwhich is reduced to Eq. (B.4) and equivalently Eq. (24)."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "<lb>An orthogonal Haar scattering transform is a deep network, computed with a hierarchy of additions, subtractions<lb>and absolute values, over pairs of coefficients. It provides a simple mathematical model for unsupervised deep network<lb>learning. It implements non-linear contractions, which are optimized for classification, with an unsupervised pair match-<lb>ing algorithm, of polynomial complexity. A structured Haar scattering over graph data computes permutation invariant<lb>representations of groups of connected points in the graph. If the graph connectivity is unknown, unsupervised Haar<lb>pair learning can provide a consistent estimation of connected dyadic groups of points. Classification results are given<lb>on image data bases, defined on regular grids or graphs, with a connectivity which may be known or unknown. deep<lb>learning, neural network, scattering transform, Haar wavelet, classification, images, graphs<lb>2000 Math Subject Classification: 68Q32, 68T45, 68Q25, 68T05",
    "creator" : "LaTeX with hyperref package"
  }
}