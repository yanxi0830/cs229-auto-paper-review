{
  "name" : "1605.09619.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Horizontally Scalable Submodular Maximization",
    "authors" : [ "Mario Lucic", "Olivier Bachem", "Morteza Zadimoghaddam", "Andreas Krause" ],
    "emails" : [ "LUCIC@INF.ETHZ.CH", "OLIVIER.BACHEM@INF.ETHZ.CH", "ZADIM@GOOGLE.COM", "KRAUSEA@ETHZ.CH" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Submodularity is a powerful concept that captures the natural notion of diminishing returns. As such, many important machine learning problems can be formulated as constrained submodular maximization. Examples include influence maximization (Kempe et al., 2003), document summarization (Lin & Bilmes, 2011), and active learning (Golovin & Krause, 2011). Solving these optimization problems on a massive scale is a great challenge and trading off computational resources and the approximation quality becomes paramount.\nConsequently, distributed computing has received a great deal of interest. One of the most desirable properties of dis-\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\ntributed algorithms is horizontal scaling – scaling to larger instances by adding more machines of fixed capacity.\nProminent approaches in distributed submodular optimization are often based on a two-round divide and conquer strategy. In the first round, the data is partitioned so that each partition can fit on one machine. Then, the subproblems are solved for each partition in parallel. In the second round, the partial solutions are collected on one machine and used to calculate the final solution. It was recently shown that this class of algorithms enjoys good empirical and theoretical performance, often matching those of the centralized solution (Mirzasoleiman et al., 2013).\nHowever, existing algorithms for distributed submodular maximization are not truly horizontally scalable. In fact, they rely on the implicit assumption that the capacity of each machine grows with the data set size. Consider the submodular maximization problem with a large ground set of size n and cardinality constraint k. Existing approaches perform well in distributing computation in the initial round. However, problems materialize when the size of the union of all solutions computed across the cluster exceeds the capacity of a single machine. At that point, one cannot scale the existing approaches by adding more machines and they will simply break down. For the popular GREEDI algorithm, this occurs if the capacity is less than O( √ nk) (Mirzasoleiman et al., 2013). Since this will invariably happen as n grows, this aspect is critical for true horizontal scaling and cannot be neglected.\nOur contribution. We investigate and address the problems arising from fixed machine capacity. To the best of our knowledge, we present the first horizontally scalable framework for constrained submodular maximization. For a broad class of underlying algorithms, it enables distributed submodular maximization under cardinality constraints. For the popular greedy algorithm, the results extend to hereditary constraints. We empirically evaluate the proposed approach and demonstrate that it achieves performance close to the centralized greedy solution.\nar X\niv :1\n60 5.\n09 61\n9v 1\n[ st\nat .M\nL ]\n3 1\nM ay"
    }, {
      "heading" : "2. Background and Related Work",
      "text" : "Let V be a ground set with cardinality n. Consider a set function f : 2V → R+ ∪ {0}. Function f is monotone iff for any two subsets X ⊆ Y ⊆ V , f(X) ≤ f(Y ). Function f is submodular if and only if for any two subsets X ⊆ Y ⊆ V , and an item e ∈ V \\ Y it holds that\nf(X ∪ {e})− f(X) ≥ f(Y ∪ {e})− f(Y ).\nWe focus on the problem of maximizing a non-negative monotone submodular function subject to a cardinality constraint. Given an integer k, the maximizer of f is\nOPT = arg max S⊆V :|S|≤k f(S).\nThe classic GREEDY algorithm (Nemhauser et al., 1978) which starts with an empty set and iteratively adds items with highest marginal gain achieves a (1 − 1/e)approximation. As the algorithm is not feasible for massive datasets, our goal is to find OPT using a distributed algorithm with value oracle access to f .\nMirzasoleiman et al. (2013) developed the first two-round algorithm whereby in the first round the data is arbitrarily partitioned to m machines and each machine runs GREEDY to select at most k items. The partial solutions are then placed on a single machine for another run of GREEDY. This approach guarantees an approximation factor of 1/Θ(min( √ k,m)). Recently, Barbosa et al. (2015a) showed that instead of assuming an arbitrary partitioning of the data, one obtains an approximation factor of (1−1/e)/2 if the data is randomly partitioned. Mirrokni & Zadimoghaddam (2015) prove that GREEDY computes representative subsets – coresets, and that by selecting O(k) instead of k items on each machine in the first round one obtains an approximation factor of 0.545.\nThe implicit assumption in these algorithms is that that the capacity per machine µ ≥ max(n/m,mk), which implies µ ≥ √ nk, optimizing overm. Hence, the capacity needs to grow with the data set size! As a result, they are not truly\nhorizontally scalable, as the maximum available memory on each machine is essentially fixed.\nKumar et al. (2013) provide two multi-round algorithms for cardinality constrained submodular maximization. The THRESHOLDMR algorithm provides a (1/2−ε)approximation with high probability, inO(1/δ) rounds and using O ( knδ log n ) memory per machine. Furthermore, a variant of the algorithm, GREEDYSCALING, provides a (1−1/e)/(1 + ε) approximation inO((log ∆)/εδ) rounds using O(n log(n)/µ) machines with O ( knδ log n ) capacity per machine, with high probability. There are four critical differences with respect to our work. Firstly, each call to the pruning procedure requires a capacity of √ 2nk log n, with high probability. However, given this capacity our algorithm always terminates in two rounds and we empirically observe approximation ratios very close to one. Secondly, our algorithm requires capacity of knδ to complete in 1/δ rounds, while their result requires capacity of knδ log n. As a result, we require capacity greater than k, while they require capacity of at least k log n which grows with n. In the case of GREEDYSCALING the number of rounds is even greater than O(1/δ), especially for a small ε or a large ∆. Thirdly, for the THRESHOLDMR algorithm one needs to try log(∆)/ thresholds which necessitates n(log ∆)/(µ ) machines compared to our n/µ machines, which is optimal and critical in practice where small ε is desired. Finally, the pruning subprocedure that the aforementioned approach relies on has to be monotone, which is not the case for both the classic greedy and stochastic greedy. In contrast, we can use their thresholding-based algorithm as a compression subprocedure.\nBarbosa et al. (2015b) proved that for any sequential algorithm that achieves an approximation factor of α and is consistent, there exists a randomized distributed algorithm that achieves a (α − ε)-approximation with constant probability in O(1/ε) rounds. In particular, one can achieve a (1 − 1/e − ε)-approximation in O(1/ε) rounds and Θ(k √ nk/ε2) capacity per machine."
    }, {
      "heading" : "3. A Practical Multi-Round Framework",
      "text" : "In contrast to the previous approaches, we focus on scalable constrained submodular maximization with fixed machine capacity. We will first focus on cardinality constrained optimization and later extend the results to optimization under hereditary constraints. To this end, we first formally define the problem and introduce the framework.\nProblem definition. We are given oracle access to a monotone non-negative submodular function f defined over a ground set V of size n and an integer k. Furthermore, we have access to a potentially unlimited set of machines M where each machine has limited capacity µ. Our goal is to compute a solution S ⊆ V, |S| ≤ k under these capacity constraints such that f(S) is competitive with the optimal solution in the case of unlimited capacity.1\nFramework. We propose the following multi-round framework which trades off machine capacity for approximation quality. The main idea is to maintain a set of items A and keep compressing it until it fits on one machine. The compression is performed by a single machine algorithmA which receives a set of items as input and outputs a set of at most k items.\nIn the first round, we set A0 = V , provision m0 = dn/µe machines and randomly partition A0 to those machines. To ensure that different parts of the random partition have almost equal size we perform the partitioning as follows. To partition N items to L parts, we assign each of the L parts dN/Le virtual free locations. We pick items one by one, and for each one we find a location uniformly at random among the available locations in all machines, and assign the item to the chosen location.\nEach machine then runs algorithm A to select at most k items from the items assigned to it. For the second round, we set A1 to be the union of all m0 partial solutions and provision m1 = d|A1|/µe ≤ dkm0/µe = dkn/µ2e machines. The algorithm proceeds iteratively until |At| ≤ µ. We return the set with the maximum value among all partial solutions and the solution on At. Pseudo-code is provided in Algorithm 1 and one round is illustrated in Figure 1. We note that at this point the compression algorithm A is not yet specified.\nTo obtain the approximation guarantee of this framework we first upper bound the number of rounds and then the expected loss in each round. In this tree-based compression scheme, the number of rounds depends on the maximum branching factor allowed by the capacity. On the other hand, the expected loss depends on the probability of pruning elements from the optimal solution in each round.\n1For the moment assume that f can be computed without access to the full dataset (e.g. as in the active set selection problem).\nAlgorithm 1 TREE-BASED COMPRESSION 1: Input: Set V , β-nice algorithm A, k, capacity µ. 2: Output: Set S ⊆ V with |S| ≤ k. 3: S ← ∅ 4: r ← dlogµ/k n/µe+ 1 5: A0 ← V 6: for t← 0 to r − 1 do 7: mt ← d|At|/µe 8: Partition At randomly into mt sets T1, . . . , Tmt . 9: for i← 1 to mt in parallel do 10: Si ← A(Ti) 11: if f(Si) > f(S) then 12: S ← Si 13: At+1 ← ∪mti=1Si 14: return S\nProposition 3.1. Let n ≥ µ > k. Then, the number of rounds performed by Algorithm 1 is bounded by\nr ≤ ⌈ ln(µ/n)/ ln(k/µ) ⌉ + 1 = ⌈ logµ/k n/µ ⌉ + 1.\nThe result follows from the fact that in each round we reduce the size of A by a factor of µ/k.\nTo instantiate the framework, one needs to provide the compression algorithm A that will be run in each round on each machine in parallel. A natural choice is the class of β-nice algorithms (Mirrokni & Zadimoghaddam, 2015) which was introduced in the context of randomized composable coresets for submodular optimization.\nDefinition 3.2. (Mirrokni & Zadimoghaddam, 2015) Consider a submodular set function f . Let A be an algorithm that given any T ⊆ V returns subset A(T ) ⊆ T with size at most k. AlgorithmA is a β-nice algorithm for function f and some parameter β iff for any set T ⊆ V and any item x ∈ T \\ A(T ) the following two properties hold:\nA(T \\ {x}) = A(T ) (1) f(A(T ) ∪ {x})− f(A(T )) ≤ βf(A(T ))/k (2)\nIntuitively, (1) ensures that the output of the algorithm does not depend on the items it did not select, and (2) guarantees that the marginal value of any item that was not selected is bounded by the average contribution of selected items. For instance, the classic GREEDY with consistent tie-breaking is 1-nice, and the THRESHOLDING GREEDY algorithm of Badanidiyuru & Vondrák (2014) is (1 + 2ε)-nice. The STOCHASTIC GREEDY algorithm (Mirzasoleiman et al., 2015) is a good choice for some problems. While the aforementioned algorithm has not been shown to be a β-nice, we demonstrate that it performs well empirically in Section 4.4"
    }, {
      "heading" : "3.1. Approximation Factor for Cardinality Constraints",
      "text" : "For a fixed β-nice algorithm, the approximation factor after multiple rounds depends only on the number of rounds and β. The following theorem relates the available capacity with the approximation guarantee of the proposed distributed multi-round framework. Theorem 3.3. Let f be a monotone non-negative submodular function defined over the ground set V of cardinality n, and k the cardinality constraint. Let A in Algorithm 1 be a β-nice algorithm and µ the capacity of each machine. Then, Algorithm 1 yields a set S of size at most k with\nE [ f(S) ] ≥    1 1+β f(OPT), if µ ≥ n 1 2(1+β)f(OPT), if n > µ ≥ √ nk\n1 r·(1+β)f(OPT), otherwise,\nwith r = dlogµ/k n/µe + 1, using at most O(n/µ) machines. If we use GREEDY as the β-nice algorithm, the approximation factor will be at least (1 − 1/e) for µ ≥ n, (1− 1/e)/2 for µ ≥ √ nk, and 1/2r for arbitrary r.\nThe first two cases are due to the classic analysis of GREEDY and the result of Barbosa et al. (2015a), respectively. We will focus on the third case in which the limited machine capacity gives rise to multiple rounds. To estimate the quality of the compression scheme, we will track how much of OPT is pruned in each round. Clearly, losing a constant fraction would lead to an exponential decrease of the approximation quality with respect to the number of rounds. A more promising approach is based on bounding the additive loss incurred in each round. The follow-\ning Lemma is a generalization of a result from Mirrokni & Zadimoghaddam (2015) in that it holds for any subset, not only OPT. The proof is provided in the Section A of the supplementary materials.\nLemma 3.4. Consider an arbitrary subset B ⊆ V , and a random partitioning of B into L sets T1, T2, · · · , TL. Let Si be the output of algorithm A on Ti. If A is β-nice, for any subset C ⊆ B with size at most k, it holds that\nE [ f(CS) ] ≥ f(C)− (1 + β)E [ max 1≤i≤L f(Si) ]\nwhere CS = C ∩ ( ∪1≤i≤L Si ) .\nThe proof of Theorem 3.3 follows from an iterated application of Lemma 3.4 and a bound on the number of rounds for a fixed capacity.\nProof of Theorem 3.3 Let OPTt be OPT∩At for 0 ≤ t ≤ r+1. In particular, OPT0 is OPT, and OPTr+1 is the items of OPT that survive till the end of algorithm and are present in the last set that the only machine in round r outputs. Since the output set S of Algorithm 1 has the maximum value among the returned sets by all machines in all rounds,\nf(S) ≥ f(OPTr+1).\nTo get the desired approximation factor, it suffices to bound the reduction in the value of remaining optimum items from round t to t + 1 for each 0 ≤ t ≤ r. By applying Lemma 3.4, and setting B = At, L = mt, and C = OPTt, it follows that E[f(OPTt) − f(OPTt+1)] is at most 1 + β\ntimes the maximum value of the mt output sets in round t, and consequently it is at most (1 +β)E[f(S)] by definition of S. Since in the last round we have only one machine, we can get a better bound on f(OPTr)−f(OPTr+1) as follows. From the fact that A is a β-nice algorithm, it follows that\nf(OPTr+1 ∪ {x})− f(OPTr+1) ≤ βf(OPTr+1)/k (3)\nfor any x ∈ OPTr. By submodularity of f , for any pair of sets A and B, it holds that\n∑ x∈A f(B ∪ {x})− f(B) ≥ f(A)− f(B).\nIn particular, summing over all x ∈ OPTr in Equation 3 yields f(OPTr) − f(OPTr+1) ≤ βf(OPTr+1), since there are at most k items in OPTr. We conclude that\nE[f(S)] ≥ E[f(OPTr+1)] ≥ E[f(OPTr)]/(1 + β) ≥ (f(OPT)− (r − 1)(1 + β)E[f(S)])/(1 + β)\nwhich implies\nE[f(S)] ≥ 1 r · (1 + β)f(OPT)."
    }, {
      "heading" : "3.2. Approximation Factor for Hereditary Constraints",
      "text" : "A constraint I is a family of feasible subsets of the ground set V . The constraint I is hereditary if for any S ∈ I all subsets of S are also in I. In submodular maximization under hereditary constraints the goal is to find a feasible set that maximizes f , i.e.,\nOPT = max S∈I f(S).\nExamples include cardinality constraints I = {A ⊆ V : |A| ≤ k}, matroid constraints, where I corresponds to the collection of independent sets of the matroid, knapsack constraints, where I = {A ⊆ V : ∑i∈A wi ≤ b}, as well as arbitrary combinations of such constraints.\nIn this section, we prove that the proposed algorithm can be used for submodular maximization with any arbitrary hereditary constraint I. In particular, if GREEDY is used as the compression procedure, the algorithm achieves approximation factor ofO ( α r ) , where α is the approximation factor of centralized GREEDY for submodular maximization under hereditary constraint I. As such, the results in this section generalize Theorem 3.3.\nThe following theorem relates the available capacity with the approximation guarantee of the distributed multi-round framework using algorithm GREEDY as the β-nice algorithm for any hereditary constraint.\nTheorem 3.5. Let f be a monotone non-negative submodular function defined over the ground set V of cardinality n, and I the hereditary constraint. Let A in Algorithm 1 be GREEDY and µ the capacity of each machine such that n ≥ µ > k. Then, Algorithm 1 yields a set S ∈ I with\nE[f(S)] ≥ α r f(OPT)\nwith r = dlogµ/k n/µe + 1, using at most O(n/µ) machines where α is the approximation factor of GREEDY for maximizing f with constraint I on a single machine.\nThe proof builds upon the rather elegant technique from Barbosa et al. (2015a). We start with defining the Lovász extension of submodular function f . For any vector v = {v1, v2, · · · , vn}, the Lovász extension is defined as\nfL(v) = Eτ∼U [0,1][f({i | vi ≥ τ})].\nWe note that τ is drawn from the uniform distribution on the interval [0, 1]. Submodularity of f implies the following three properties on fL:\n(A) fL(c · v) ≥ c · fL(v) for any 0 ≤ c ≤ 1. (B) Function fL is convex.\n(C) For any S ⊆ V , fL(1S) = f(S) where 1S is the vector with 1 at the i’th entry if i ∈ S, and 0 otherwise.\nWe now have all the prerequisites to prove the main result. Intuitively, our goal is to lower-bound the expected value of the selected elements by a fraction of the expected value of the pruned elements in each round.\nProof of Theorem 3.5 Denote the output of GREEDY on set A by GREEDY(A). For any round 1 ≤ t ≤ r, and machine index 1 ≤ i ≤ mt, let T ti be the set of items sent to machine i at round t. For any item x ∈ V , and round 1 ≤ t ≤ r, let ptx be the probability that item x is not selected by GREEDY if it is sent to a random machine in round t. In other words,\nptx = P(x /∈ GREEDY(T ti ∪ {x})),\nfor i is chosen uniformly at random from {1, 2, . . . ,mt}. Let Ot be the set of these omitted items, i.e.\nOt = {x | x /∈ GREEDY(T ti ∪ {x})}\nfor a random i. Since these items are not selected by GREEDY, adding them to T ti will not change the solution. Therefore GREEDY(T ti ) = GREEDY(T t i ∪ Ot). Let Sti = GREEDY(T t i ) be the solution of machine i in round t. Since GREEDY provides an α approximation, and f is monotone, it holds that\nf(Sti ) = GREEDY(T t i ∪Ot) ≥ α · f(Ot).\nWe conclude that expected value of the solution of a random machine in round t is at least α · E[f(Ot)]. Since the final returned set S has the maximum value over all intermediate solutions it follows that\nE[f(S)] ≥ α · E[f(Ot)]. (4)\nOn the other hand, in the last round all items Ar are collected on one machine, and GREEDY selects a solution among them. For any x ∈ OPT, let qx be the probability that item x is present in the last round (not pruned in any round). Let OPTS to be the set of these items, i.e.\nOPTS = OPT ∩Ar.\nSince GREEDY is an α approximation, the expected value of solution in the last round and consequently the expected value E[f(S)] are both at least α · E[f(OPTS)]. To conclude, we exploit the properties of the Lovász extension as follows. Let q and pt be vectors with qx and ptx at component x, respectively. It follows that\nE[f(OPTS)] + r−1∑\nt=1\nE[f(Ot)]\n= E[fL(1OPTS )] + r−1∑\nt=1\nE[fL(1Ot)]\n≥ fL(E[1OPTS ]) + r−1∑\nt=1\nfL(E[1Ot ])\n≥ fL(q) + r−1∑\nt=1\nfL(pt)\n≥ fL(1OPT) = f(OPT).\nThe first equality is implied by the Lovász extension. It is followed by Jensen’s inequality exploiting the convexity of fL. The second inequality follows by definition. The last inequality is implied by the fact that each x ∈ OPT is either omitted in one of the rounds, or is present in OPTS . We note that the probability of x being omitted in round t is upper bounded by ptx, because it might have been omitted in one of the earlier rounds.\nTherefore, by applying (4) we conclude that\nf(OPT) ≤ E[f(OPTS)] + r−1∑\nt=1\nE[f(Ot)]\n≤ 1 α E[f(S)] + r − 1 α E[f(S)] ≤ r α E[f(S)]\nwhich completes the proof."
    }, {
      "heading" : "4. Experiments",
      "text" : "In this section we empirically validate two main claims of the paper. We first demonstrate that the proposed algorithm scales horizontally. We then show that the available capacity does not have a significant effect on the approximation quality. We compare the performance of the proposed algorithm to the the two-phase RANDGREEDI, centralized GREEDY run on the entire data set, and a randomly selected subset of size k. Finally, in a set of large-scale experiments we investigate the usage of STOCHASTIC GREEDY as the compression subprocedure. The data sets and objective functions are summarized in Table 2."
    }, {
      "heading" : "4.1. Data Sets",
      "text" : "CSN. The Community Seismic Network uses smart phones with accelerometers as inexpensive seismometers for earthquake detection. In Faulkner et al. (2011), 7 GB of acceleration data was recorded from volunteers carrying and operating their phone in normal conditions (walking, talking, on desk, etc.). From this data, 17-dimensional feature vectors were computed (containing frequency information, moments, etc.).\nTINY IMAGES. In our experiments we used two subsets of the Tiny Images data set consisting of 32×32 RGB images, each represented as a 3072 dimensional vector (Torralba et al., 2008). We normalized the vectors to zero mean and unit norm. Following Mirzasoleiman et al. (2013), we select a fixed random subsample of 10 000 elements for evaluation on each machine.\nPARKINSONS. The data set consists of 5875 biomedical voice measurements with 22 attributes from people with early-stage Parkinsons disease (Tsanas et al., 2010). We normalized the vectors to zero mean and unit norm.\nYAHOO! WEBSCOPE R6A. This data set contains a fraction of the user click log for news articles displayed in the Featured Tab of the Today Module on Yahoo! Front Page during the first ten days in May 2009. The data set contains approximately 45 000 000 user visits to the Today Module. In addition to the full data set, we also considered a subset of size 100 000."
    }, {
      "heading" : "4.2. Objective Functions",
      "text" : "Exemplar-based clustering. A classic way to select a set of exemplars that best represent a massive data set is to solve the k-medoid problem (Kaufman & Rousseeuw, 1987) by minimizing the sum of pairwise dissimilarities between exemplars A and elements of the data set V . This problem can be converted to a submodular maximization problem subject to a cardinality constraint as follows: First, define L(S) := 1V minv∈S d(e, v) where d : V ×V → R+ is a distance function, encoding the dissimilarity between elements. Then, the function that measures the reduction in quantization error by using elements in S,\nf(S) = L({e0})− L(S ∪ {e0}),\nis monotone submodular and maximizing f is equivalent to minimizing L (Krause & Golovin, 2012). We performed several exemplar-based clustering experiments with the distance function set to d(x, y) = ||x − y||2 and using the zero vector as the auxiliary element e0. To exactly evaluate f in examplar-based clustering, one needs to have access to the full data set on each machine. Nevertheless, as this function is additively decomposable and bounded, it can be approximated to arbitrary precision by an appropriately scaled sum over a random subsample (Chernoff bound). We select a set of k most representative signals using exemplar based clustering.\nActive set selection. In nonparametric learning (e.g. Sparse Gaussian Processes) we wish to find a set of representative samples. Formally a GP is a joint probability distribution over a (possibly infinite) set of random variables XV , indexed by our ground set V , such that every (finite) subset XS for S = {e1, . . . , es} is distributed according to a multivariate normal distribution, i.e., P (XS = xS) = N(xS ;µS ,ΣS,S). µS = (µe1 , . . . , µes) and ΣS,S = [Kei,ej ](1 ≤ i, j ≤ k) are the prior mean vector and prior covariance matrix, respectively. The covariance matrix is parametrized via a (positive definite kernel) function K. A commonly used kernel function is the squared exponential kernel K(ei, ej) = exp−||ei − ej ||2/h2. Upon observations yA = xA + nA (where nA is a vector of independent Gaussian noise with variance σ2), the predictive distribution of a new data point e ∈ V is a normal distribution P (Xe|yA) = N (µe|A,Σ2e|A), where\nµe|A = µe + Σe,A(ΣA,A + σ 2I)−1)(xA − µA)\nσ2e|A = σ 2 e − Σe,A(ΣA,A + σ2I)−1ΣA,e\nNote that evaluating the latter is computationally expensive as it requires a matrix inversion. Instead, most efficient approaches for making predictions in GPs rely on choosing a small – so called active – set of data points. For instance, in the Informative Vector Machine, one seeks a set\nS such that the information gain f(S) = I(YS ;XV ) = H(XV )−H(XV |YS) = 12 log det(I + σ−2ΣS,S) is maximized. This choice of f(S) is monotone submodular (Krause & Golovin, 2012). We perform several experiments optimizing the active set selection objective with a Gaussian kernel (h = 0.5 and σ = 1)."
    }, {
      "heading" : "4.3. Impact of Capacity on the Approximation factor",
      "text" : "In the first set of experiments, we study the objective function value as a function of the available capacity. We consider three baseline methods: random subset of k elements, centralized GREEDY on the full data set, and the two-round RANDGREEDI by Barbosa et al. (2015a). We use the lazy variant of the GREEDY algorithm (Minoux, 1978) as the β-nice algorithm in our multi-round proposal. For each algorithm we report the ratio between the obtained function value and the one obtained by the centralized GREEDY averaged over 10 trials.\nFigures 2 (a) and (c) show the results of active set selection with k = 50. Figures 2 (b) and (d) show the results of exemplar-based clustering with k = 50. The vertical line represents the lower-bound on the capacity required for the two-round algorithms as per Table 1. We observe that the proposed algorithm TREE achieves performance close to the centralized greedy solution, even with extremely limited capacity of 2k. As expected, if the capacity exceeds√ nk, the performance matches that of RANDGREEDI.\nTable 3 shows the impact of the available capacity on the relative approximation error for various values of k. The last column represents the relative error of a randomly selected subset of size k. We remark that the algorithm achieves a relative error of less than 1% across all data sets."
    }, {
      "heading" : "4.4. Large-scale Experiments",
      "text" : "In the second set of experiments, we apply both GREEDY and STOCHASTIC GREEDY (TREE and STOCHASTICTREE, respectively) as pruning subprocedures. We consider the full WEBSCOPE data set and a subset of 1 000 000\nTINY IMAGES. The capacity is set to a small percentage of the ground set size (0.05% and 0.1%). Furthermore, we consider two instances of STOCHASTIC GREEDY, one with ε = 0.5 and the other with ε = 0.2, both with capacity of 0.05% of the ground set size.\nFigure 2(e) shows the results of active set selection on WEBSCOPE. We observe that both versions of the proposed algorithm match the performance of centralized GREEDY, even when the available capacity is extremely limited. Figure 2(f) shows the results of exemplar-based clustering on TINY. We observe a slight loss in the approximation quality when using STOCHASTIC GREEDY. Our intuition is that the variance introduced by stochastic optimization is larger in the context of exemplar-based clustering."
    }, {
      "heading" : "5. Conclusion",
      "text" : "Existing approaches to distributed submodular maximization rely on the implicit assumption that the capacity of each machine grows as the data set size increases. To the best of our knowledge, we present the first framework for constrained distributed submodular maximization that is\nhorizontally scalable: It scales to larger problem instances by using more machines of limited capacity. Our framework is based on a multi-round approach, whereby in each round a fraction of the elements are discarded until all the remaining elements fit on one machine. We provide approximation guarantees for distributed submodular maximization under cardinality constraints and extend the results to hereditary constraints. The proposed framework adapts to the available capacity: If the capacity is larger than the data set size, it emulates the centralized GREEDY. If the capacity is at least √ nk, it reduces to the existing two-round approaches. Otherwise, it proceeds in multiple rounds. We empirically evaluate the proposed approach on a variety of data sets and demonstrate that the algorithm achieves performance close to the centralized GREEDY solution, even with extremely limited capacity."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank the reviewers for their insightful comments. This research was partially supported by ERC StG 307036 and the Zurich Information Security Center."
    }, {
      "heading" : "A. Detailed Analysis of Lemma 3.4",
      "text" : "Let ∆(x,X) denote the marginal value of adding item x to set X , i.e., ∆(x,X) def= f(X ∪ {x}) − f(X). The submodularity property for X ⊆ Y ⊆ V and item x ∈ V \\ Y can now be expressed as ∆(x,X) ≥ ∆(x, Y ). Let OPT = arg max|S|≤k f(S), and OPT S be the set of selected items of OPT in the partial solutions\nOPTS def = OPT ∩ (∪mi=1Si).\nLemma 3.4. Consider an arbitrary subset B ⊆ V , and a random partitioning of B into L sets T1, T2, · · · , TL. Let Si be the output of algorithm A on Ti. If A is β-nice, for any subset C ⊆ B with size at most k, it holds that\nE [ f(CS) ] ≥ f(C)− (1 + β)E [ max 1≤i≤L f(Si) ]\nwhere CS = C ∩ ( ∪1≤i≤L Si ) .\nProof of Lemma 3.4 The proof builds on the result of Mirrokni & Zadimoghaddam (2015). The critical difference is that it holds for any subset of C, not only OPT. To quantify the contributions of items in C, we introduce the following formalization. Let π be an arbitrary permutation on items of C. For each x ∈ C, define πx to be the set of items that appear prior to x in π. Hence, f(C) = ∑ x∈C ∆(x, π x). Let Gi = (C ∩ Ti) \\ Si. We are going to show the following:\nf(CS) ≥ f(C)− ∑\nx∈C\\CS ∆(x, πx) (5)\n∑\nx∈C\\CS ∆(x, πx) =\nL∑\ni=1\n∑\nx∈Gi ∆(x, πx) ≤ β max 1≤i≤L f(Si) +\nL∑\ni=1\n∑\nx∈Gi (∆(x, πx)−∆(x, Si)) (6)\nE\n[ L∑\ni=1\n∑\nx∈Gi ∆(x, πx)−∆(x, Si)\n] ≤ 1 L E [ L∑\ni=1\nf(Si)\n] ≤ max\n1≤i≤L f(Si) (7)\nGiven these results, we can prove the claim as follows:\nE[f(CS)] ≥ f(C)− ∑\nx∈C\\CS E[∆(x, πx)]\n≥ f(C)− βE [\nmax 1≤i≤L f(Si)\n] + L∑\ni=1\n∑\nx∈Gi E [∆(x, πx)−∆(x, Si)]\n≥ f(C)− βE [\nmax 1≤i≤L f(Si)\n] + max\n1≤i≤L f(Si).\nwhere the expectation is taken over the random partitioning {T1, · · · , TL}. We now prove the intermediate results. Inequality (5) follows from submodularity of f . In particular, by definition of ∆ and πx, it holds that f(C) − f(CS) =∑ x∈C\\CS ∆(x, π\nx ∪ CS). By submodularity, it holds that ∆(x, πx ∪ CS) ≤ ∆(x, πx) since πx is a subset of πx ∪ CS . Therefore f(C)− f(CS) ≤∑x∈C\\CS ∆(x, πx). Inequality (6) follows from the following decomposition. Since x ∈ C is sent to one of the L machines (x ∈ Ti for some 1 ≤ i ≤ L) it holds that\n∑\nx∈C\\CS ∆(x, πx) =\n∑\n1≤i≤L\n∑\nx∈Gi ∆(x, πx)\n=\nL∑\ni=1\n∑\nx∈C∩Ti\\Si (∆(x, πx ∪ Si) + (∆(x, πx)−∆(x, πx ∪ Si)))\nBy submodularity, the first term is at most ∆(x, Si). Furthermore, since x was not selected in machine i, we can upperbound the first term by β f(Si)k . The result follows from f(Si) ≤ max1≤i′≤L f(Si′), and the fact that there are at most k items in C \\ CS = ∪Li=1(C ∩ Ti \\ Si). Finally, to prove Inequality (7) we use randomness of partition {T1, T2, · · · , TL}. The main idea is to show that the sum of the ∆ differences in the third claim is in expectation at most 1m fraction of sum of ∆ differences for a larger set of pairs (i, x). In particular, we show that\nE\n  L∑\ni=1\n∑\nx∈C∩Ti\\Si ∆(x, πx)−∆(x, πx ∪ Si)\n  ≤ 1 L · E [ L∑\ni=1\n∑ x∈C ∆(x, πx)−∆(x, πx ∪ Si)\n] .\nTo simplify the rest of the proof, let A be the left hand side of the above inequality, and B be its right hand side. Also to simplify expressions A and B, we introduce the following notation: For every item x and set T ⊆ B ⊆ V , let h(x, T ) denote ∆(x, πx) − ∆(x, πx ∪ A(T )). Also let 1[x /∈ A(T ∪ {x})] be equal to one if x is not in set A(T ∪ {x}), and zero otherwise. We note that A and B are both separable for different choices of item x and set Ti, and can be rewritten formally using the new notation as follows:\nA = L∑\ni=1\n∑\nx∈C\n∑\nT⊆V \\{x} Pr[Ti = T ∪ {x}]1[x /∈ A(T ∪ {x})]h(x, T ∪ {x})\nB =\nL∑\ni=1\n∑\nx∈C\n∑\nT⊆V \\{x} (Pr[Ti = T ∪ {x}]h(x, T ∪ {x}) + Pr[Ti = T ]h(x, T ))\n≥ L∑\ni=1\n∑\nx∈C\n∑\nT⊆V \\{x} 1[x /∈ A(T ∪ {x})]h(x, T ∪ {x}) (Pr[Ti = T ∪ {x}] + Pr[Ti = T ])\nwhere the inequality is implied by the following observations. Function h is non-negative, so multiplying the sum by 1[x /∈ A(T ∪ {x})] (which is either zero or one) can only decrease its value. We can also replace one h(x, T ) with h(x, T ∪ {x}) which does not change the value of the sum at all because when 1[x /∈ A(T ∪ {x})] = 1 (its only non-zero value), A(T ∪ {x}) is identical to A(T ) using the first property of β-nice algorithms, and thus h(x, T ) = h(x, T ∪ {x}). Now we can compare A, and B as follows: For any set T ⊆ V \\ {x}, we have Pr[Ti = T ] and Pr[Ti = T ∪ {x}] are equal to ( 1 L )|T | ( 1− 1L )|V |−|T | and ( 1 L )|T |+1 ( 1− 1L )|V |−|T |−1 respectively. As a result, it holds that\nPr[Ti = T ∪ {x}] Pr[Ti = T ∪ {x}] + Pr[Ti = T ] = 1 L\nwhich implies that A ≤ B. To complete the proof, it suffices to prove that B ≤ 1LE [∑L i=1 f(Si) ] . For any i,\n∑ x∈C ∆(x, πx ∪ Si) = f(C ∪ Si)− f(Si)\nand ∑ x∈C ∆(x, π x) = f(C). As f is monotone it holds that f(OPT ∪ Si) ≥ f(OPT) which completes the proof."
    } ],
    "references" : [ {
      "title" : "Fast algorithms for maximizing submodular functions",
      "author" : [ "Badanidiyuru", "Ashwinkumar", "Vondrák", "Jan" ],
      "venue" : "In ACMSIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Badanidiyuru et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Badanidiyuru et al\\.",
      "year" : 2014
    }, {
      "title" : "The power of randomization: Distributed submodular maximization on massive datasets",
      "author" : [ "Barbosa", "Rafael", "Ene", "Alina", "Nguyen", "Huy L", "Ward", "Justin" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Barbosa et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Barbosa et al\\.",
      "year" : 2015
    }, {
      "title" : "A new framework for distributed submodular maximization",
      "author" : [ "Barbosa", "Rafael da Ponte", "Ene", "Alina", "Nguyen", "Huy L", "Ward", "Justin" ],
      "venue" : "arXiv preprint arXiv:1507.03719,",
      "citeRegEx" : "Barbosa et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Barbosa et al\\.",
      "year" : 2015
    }, {
      "title" : "Adaptive submodularity: Theory and applications in active learning and stochastic optimization",
      "author" : [ "Golovin", "Daniel", "Krause", "Andreas" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Golovin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Golovin et al\\.",
      "year" : 2011
    }, {
      "title" : "Clustering by means of medoids",
      "author" : [ "Kaufman", "Leonard", "Rousseeuw", "Peter" ],
      "venue" : "Statistical Data Analysis Based on the L1-Norm and Related Methods, pp. North–Holland,",
      "citeRegEx" : "Kaufman et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Kaufman et al\\.",
      "year" : 1987
    }, {
      "title" : "Maximizing the spread of influence through a social network",
      "author" : [ "Kempe", "David", "Kleinberg", "Jon", "Tardos", "Éva" ],
      "venue" : "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Kempe et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kempe et al\\.",
      "year" : 2003
    }, {
      "title" : "Submodular function maximization",
      "author" : [ "Krause", "Andreas", "Golovin", "Daniel" ],
      "venue" : "Tractability: Practical Approaches to Hard Problems,",
      "citeRegEx" : "Krause et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2012
    }, {
      "title" : "Fast greedy algorithms in mapreduce and streaming",
      "author" : [ "Kumar", "Ravi", "Moseley", "Benjamin", "Vassilvitskii", "Sergei", "Vattani", "Andrea" ],
      "venue" : "In ACM Symposium on Parallelism in Algorithms and Architectures,",
      "citeRegEx" : "Kumar et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2013
    }, {
      "title" : "A class of submodular functions for document summarization. In Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pp. 510–520",
      "author" : [ "Lin", "Hui", "Bilmes", "Jeff" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "Lin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2011
    }, {
      "title" : "Accelerated greedy algorithms for maximizing submodular set functions",
      "author" : [ "Minoux", "Michel" ],
      "venue" : "In Optimization Techniques,",
      "citeRegEx" : "Minoux and Michel.,? \\Q1978\\E",
      "shortCiteRegEx" : "Minoux and Michel.",
      "year" : 1978
    }, {
      "title" : "Randomized composable core-sets for distributed submodular maximization",
      "author" : [ "Mirrokni", "Vahab", "Zadimoghaddam", "Morteza" ],
      "venue" : "In ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Mirrokni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mirrokni et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed submodular maximization: Identifying representative elements in massive data",
      "author" : [ "Mirzasoleiman", "Baharan", "Karbasi", "Amin", "Sarkar", "Rik", "Krause", "Andreas" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Mirzasoleiman et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mirzasoleiman et al\\.",
      "year" : 2013
    }, {
      "title" : "Lazier than lazy greedy",
      "author" : [ "Mirzasoleiman", "Baharan", "Badanidiyuru", "Ashwinkumar", "Karbasi", "Amin", "Vondrak", "Jan", "Krause", "Andreas" ],
      "venue" : "In AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Mirzasoleiman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mirzasoleiman et al\\.",
      "year" : 2015
    }, {
      "title" : "An analysis of approximations for maximizing submodular set functions-I",
      "author" : [ "Nemhauser", "George L", "Wolsey", "Laurence A", "Fisher", "Marshall L" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Nemhauser et al\\.,? \\Q1978\\E",
      "shortCiteRegEx" : "Nemhauser et al\\.",
      "year" : 1978
    }, {
      "title" : "80 million tiny images: A large data set for nonparametric object and scene recognition",
      "author" : [ "Torralba", "Antonio", "Fergus", "Rob", "Freeman", "William T" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Torralba et al\\.,? \\Q1958\\E",
      "shortCiteRegEx" : "Torralba et al\\.",
      "year" : 1958
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Examples include influence maximization (Kempe et al., 2003), document summarization (Lin & Bilmes, 2011), and active learning (Golovin & Krause, 2011).",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "It was recently shown that this class of algorithms enjoys good empirical and theoretical performance, often matching those of the centralized solution (Mirzasoleiman et al., 2013).",
      "startOffset" : 152,
      "endOffset" : 180
    }, {
      "referenceID" : 11,
      "context" : "For the popular GREEDI algorithm, this occurs if the capacity is less than O( √ nk) (Mirzasoleiman et al., 2013).",
      "startOffset" : 84,
      "endOffset" : 112
    }, {
      "referenceID" : 13,
      "context" : "The classic GREEDY algorithm (Nemhauser et al., 1978) which starts with an empty set and iteratively adds items with highest marginal gain achieves a (1 − 1/e)approximation.",
      "startOffset" : 29,
      "endOffset" : 53
    }, {
      "referenceID" : 8,
      "context" : "Mirzasoleiman et al. (2013) developed the first two-round algorithm whereby in the first round the data is arbitrarily partitioned to m machines and each machine runs GREEDY to select at most k items.",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "Recently, Barbosa et al. (2015a) showed that instead of assuming an arbitrary partitioning of the data, one obtains an approximation factor of (1−1/e)/2 if the data is randomly partitioned.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "Recently, Barbosa et al. (2015a) showed that instead of assuming an arbitrary partitioning of the data, one obtains an approximation factor of (1−1/e)/2 if the data is randomly partitioned. Mirrokni & Zadimoghaddam (2015) prove that GREEDY computes representative subsets – coresets, and that by selecting O(k) instead of k items on each machine in the first round one obtains an approximation factor of 0.",
      "startOffset" : 10,
      "endOffset" : 222
    }, {
      "referenceID" : 1,
      "context" : "Recently, Barbosa et al. (2015a) showed that instead of assuming an arbitrary partitioning of the data, one obtains an approximation factor of (1−1/e)/2 if the data is randomly partitioned. Mirrokni & Zadimoghaddam (2015) prove that GREEDY computes representative subsets – coresets, and that by selecting O(k) instead of k items on each machine in the first round one obtains an approximation factor of 0.545. The implicit assumption in these algorithms is that that the capacity per machine μ ≥ max(n/m,mk), which implies μ ≥ √ nk, optimizing overm. Hence, the capacity needs to grow with the data set size! As a result, they are not truly horizontally scalable, as the maximum available memory on each machine is essentially fixed. Kumar et al. (2013) provide two multi-round algorithms for cardinality constrained submodular maximization.",
      "startOffset" : 10,
      "endOffset" : 755
    }, {
      "referenceID" : 1,
      "context" : "Recently, Barbosa et al. (2015a) showed that instead of assuming an arbitrary partitioning of the data, one obtains an approximation factor of (1−1/e)/2 if the data is randomly partitioned. Mirrokni & Zadimoghaddam (2015) prove that GREEDY computes representative subsets – coresets, and that by selecting O(k) instead of k items on each machine in the first round one obtains an approximation factor of 0.545. The implicit assumption in these algorithms is that that the capacity per machine μ ≥ max(n/m,mk), which implies μ ≥ √ nk, optimizing overm. Hence, the capacity needs to grow with the data set size! As a result, they are not truly horizontally scalable, as the maximum available memory on each machine is essentially fixed. Kumar et al. (2013) provide two multi-round algorithms for cardinality constrained submodular maximization. The THRESHOLDMR algorithm provides a (1/2−ε)approximation with high probability, inO(1/δ) rounds and using O ( kn log n ) memory per machine. Furthermore, a variant of the algorithm, GREEDYSCALING, provides a (1−1/e)/(1 + ε) approximation inO((log ∆)/εδ) rounds using O(n log(n)/μ) machines with O ( kn log n ) capacity per machine, with high probability. There are four critical differences with respect to our work. Firstly, each call to the pruning procedure requires a capacity of √ 2nk log n, with high probability. However, given this capacity our algorithm always terminates in two rounds and we empirically observe approximation ratios very close to one. Secondly, our algorithm requires capacity of kn to complete in 1/δ rounds, while their result requires capacity of kn log n. As a result, we require capacity greater than k, while they require capacity of at least k log n which grows with n. In the case of GREEDYSCALING the number of rounds is even greater than O(1/δ), especially for a small ε or a large ∆. Thirdly, for the THRESHOLDMR algorithm one needs to try log(∆)/ thresholds which necessitates n(log ∆)/(μ ) machines compared to our n/μ machines, which is optimal and critical in practice where small ε is desired. Finally, the pruning subprocedure that the aforementioned approach relies on has to be monotone, which is not the case for both the classic greedy and stochastic greedy. In contrast, we can use their thresholding-based algorithm as a compression subprocedure. Barbosa et al. (2015b) proved that for any sequential algorithm that achieves an approximation factor of α and is consistent, there exists a randomized distributed algorithm that achieves a (α − ε)-approximation with constant probability in O(1/ε) rounds.",
      "startOffset" : 10,
      "endOffset" : 2364
    }, {
      "referenceID" : 12,
      "context" : "The STOCHASTIC GREEDY algorithm (Mirzasoleiman et al., 2015) is a good choice for some problems.",
      "startOffset" : 32,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "The first two cases are due to the classic analysis of GREEDY and the result of Barbosa et al. (2015a), respectively.",
      "startOffset" : 80,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "The first two cases are due to the classic analysis of GREEDY and the result of Barbosa et al. (2015a), respectively. We will focus on the third case in which the limited machine capacity gives rise to multiple rounds. To estimate the quality of the compression scheme, we will track how much of OPT is pruned in each round. Clearly, losing a constant fraction would lead to an exponential decrease of the approximation quality with respect to the number of rounds. A more promising approach is based on bounding the additive loss incurred in each round. The following Lemma is a generalization of a result from Mirrokni & Zadimoghaddam (2015) in that it holds for any subset, not only OPT.",
      "startOffset" : 80,
      "endOffset" : 644
    }, {
      "referenceID" : 1,
      "context" : "The proof builds upon the rather elegant technique from Barbosa et al. (2015a). We start with defining the Lovász extension of submodular function f .",
      "startOffset" : 56,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "Following Mirzasoleiman et al. (2013), we select a fixed random subsample of 10 000 elements for evaluation on each machine.",
      "startOffset" : 10,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "We consider three baseline methods: random subset of k elements, centralized GREEDY on the full data set, and the two-round RANDGREEDI by Barbosa et al. (2015a). We use the lazy variant of the GREEDY algorithm (Minoux, 1978) as the β-nice algorithm in our multi-round proposal.",
      "startOffset" : 138,
      "endOffset" : 161
    } ],
    "year" : 2016,
    "abstractText" : "A variety of large-scale machine learning problems can be cast as instances of constrained submodular maximization. Existing approaches for distributed submodular maximization have a critical drawback: The capacity – number of instances that can fit in memory – must grow with the data set size. In practice, while one can provision many machines, the capacity of each machine is limited by physical constraints. We propose a truly scalable approach for distributed submodular maximization under fixed capacity. The proposed framework applies to a broad class of algorithms and constraints and provides theoretical guarantees on the approximation factor for any available capacity. We empirically evaluate the proposed algorithm on a variety of data sets and demonstrate that it achieves performance competitive with the centralized greedy solution.",
    "creator" : "LaTeX with hyperref package"
  }
}