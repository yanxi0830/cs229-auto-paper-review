{
  "name" : "1406.5667.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Correlation Clustering with Noisy Partial Information",
    "authors" : [ "Konstantin Makarychev", "Aravindan Vijayaraghavan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 6.\n56 67\nv2 [\ncs .D\nS] 1\n2 M\nay 2\n01 5\n3 n) with high probability, where opt-cost is the value of the optimal solution (for every δ > 0). The second algorithm finds the ground truth clustering with an arbitrarily small classification error η (under some additional assumptions on the instance)."
    }, {
      "heading" : "1 Introduction",
      "text" : "One of the most commonly used algorithmic tools in data analysis and machine learning is clustering – partitioning a corpus of data into groups based on similarity. The data observed in several application domains – e.g., protein-protein interaction data, links between web pages, and social ties on social networks – carry relational information between pairs of nodes, which can be represented using a graph. Clustering based on relational information can reveal important structural information such as functional groups of proteins [Bader and Hogue, 2003, Girvan and Newman, 2002], communities on web and social networks [Fortunato, 2010, Karrer and Newman, 2011], and can be used for predictive tasks such as link prediction [Taskar et al., 2004].\nCorrelation clustering tackles this problem of clustering objects when we are given qualitative information about the similarity or dissimilarity between some pairs of these objects. This qualitative information is represented in the form of a graph G(V,E, c) in which edges E are labeled with signs {+,−}; we denote the set of ‘+’ edges by E+ and the set of ‘−’ edges by E−. Each edge (u, v) in E+ indicates that u and v are similar, and each edge (u, v) ∈ E− indicates that u and v are dissimilar; the cost c(u, v) of the edge shows the amount of similarity or dissimilarity between u and v.1 In the ideal case, this qualitative information is consistent with the intended (“ground truth”) clustering. However, the qualitative information may be noisy due to errors in the observations. Hence, the goal is to find a partition P of G that minimizes the cost of inconsistent edges:\nmin P\n∑\n(u,v)∈E+:P(u)6=P(v)\nc(u, v) + ∑\n(u,v)∈E−:P(u)=P(v)\nc(u, v),\n∗Supported by NSF CAREER award CCF-1150062 and NSF award IIS-1302662. †Supported by the Simons Collaboration on Algorithms and Geometry. 1One can also think of the instance as a graph G(V,E, c) with the edge costs c : E → [−1, 1]. If c(u, v) > 0 then (u, v) ∈ E+,\nand If c(u, v) < 0 then (u, v) ∈ E−.\nwhere P(u) denotes the cluster that contains the vertex u. The objective captures the cost of inconsistent edges – cut edges in E+ and uncut edges in E−. (For a partition P, we say that an edge (u, v) ∈ E is consistent with P if either (u, v) ∈ E+ and P(u) = P(v) or (u, v) ∈ E− and P(u) 6= P(v).)\nNote that the underlying graph G(V,E) can be reasonably sparse; this is desirable since collecting pairwise information can be expensive. One important feature of correlation clustering is that it, unlike most other clustering problems, allows us not to specify the number of clusters. Hence, it is particularly useful when we have no prior knowledge of the number of clusters that the data divides into.\nCorrelation clustering also comes up naturally in MAP inference in graphical models and structured prediction tasks for such tasks as image segmentation, parts-of-speech tagging and dependency parsing in natural language processing [Nowozin and Lampert, 2010, Smith, 2011]. In structured prediction, we are given some observations as input (e.g., image data, sentences), and the goal is to predict a labeling x ∈ X that encodes the high-level information that we would like to infer. For instance, in image segmentation, the variables x ∈ {0, 1}n indicate whether each pixel is in the foreground or background. This is naturally modeled as a Correlation Clustering instance on the set of pixels (with 2 clusters), where edges connect adjacent pixels, and the costs (with signs) are set based on the similarity or dissimilarity of the corresponding pixels in the given image. The clusters in these inference problems then consist of the sets of variables that receive the same assignment in the MAP solution. Correlation clustering is also used in the context of consensus clustering and agnostic learning.\nCorrelation clustering was introduced in [Bansal et al., 2004], and implicitly in [Ben-Dor et al., 1999] as ‘Cluster Editing’. The problem is APX-hard even on complete graphs2 (when we are given the similarity information for every pair of objects) [Charikar et al., 2005]. The state-of-the-art approximation algorithm [Charikar et al., 2005, Demaine et al., 2006] achieves an O(log n) approximation for minimizing disagreements in the worst-case. Furthermore, there is a gap-preserving reduction from the classic Minimum Multicut problem [Charikar et al., 2005, Demaine et al., 2006], for which the current state-ofthe-art algorithm gives a Θ(log n) factor approximation [Garg et al., 1993]. The complementary objective of maximizing agreements is easier from the approximability standpoint, and a 0.766 factor approximation is known [Charikar et al., 2005, Swamy, 2004]. For the special case of complete graphs (with unit costs on edges), small constant factor approximations have been obtained in a series of works [Bansal et al., 2004, Ailon et al., 2008, Chawla et al., 2014]. Instances of Correlation Clustering on complete graphs that satisfy the notion of approximation stability were considered in [Balcan and Braverman, 2009]. To summarize, despite our best efforts, we only know logarithmic factor approximation algorithms for Correlation Clustering; moreover, we cannot get a constant factor approximation for worst-case instances if the Unique Games Conjecture is true.\nHowever, our primary interest in solving Correlation Clustering comes from its numerous applications, and the instances that we encounter in these applications are not worst-case instances. This motivates the study of the average-case complexity of the problem and raises the following question:\nCan we design algorithms with better provable guarantees for realistic average-case models of Correlation Clustering?\nSeveral natural average-case models of Correlation Clustering have been studied previously. Ben-Dor et al. [1999] consider a model in which we start with a ground-truth clustering – an arbitrary partitioning of the vertices – of a complete graph. Initially, edges inside clusters of the ground truth solution are labeled ‘+’ and edges between clusters are labeled ‘-’. We flip the label of each edge (change ‘+’ to ‘−’ and ‘−’ to ‘+’) with probability ε independently at random and obtain a Correlation Clustering instance (the flipped edges\n2This rules out (1 + ǫ) factor approximations for some small constant ǫ > 0.\nmodel the noisy observations) . In fact, this average-case model was also studied in the work [Bansal et al., 2004] that introduced the problem of Correlation Clustering. Mathieu and Schudy consider a generalization of this model where there is an adversary: for each edge, we keep the initial label with probability (1 − ε), and we let the adversary decide whether to flip the edge label or not with probability ε. The major drawback of these models is that they only consider the case of complete graphs, i.e. they require that the Correlation Clustering instance contains similarity information for every pair of nodes. Chen et al. extended the model of [Ben-Dor et al., 1999] from complete graphs to sparser Erdos–Renyi random graphs. In their model, the underlying unlabeled graph G(V,E) comes from an Erdös–Renyi random graph (of edge probability p), and as in [Ben-Dor et al., 1999], the label of each edge is set (independently) to be consistent with the ground truth clustering with probability 1− ε and inconsistent with probability ε.\nWhile these average-case models are natural, they are unrealistic in practice since most real-world graphs are neither dense nor captured by Erdös–Renyi distributions. For instance, real-world graphs in community detection have many structural properties (presence of large cliques, large clustering coefficients, heavy-tailed degree distribution) that are not exhibited by graphs that are generated by Erdös–Renyi models [Newman et al., 2006, Kumar et al., 1999]. Graphs that come up in computer vision applications are sparse with grid-like structure [Yarkony et al., 2012]. Further, these models assume that every pair of vertices have the same amount of similarity or dissimilarity (all costs are unit). Our semi-random model tries to address these issues by assuming very little about the observations – the underlying unlabeled graph G(V,E) – and allowing non-uniform costs."
    }, {
      "heading" : "1.1 Our Semi-random Model",
      "text" : "In this paper, we propose and study a new semi-random model for generating general instances of Correlation Clustering, which we believe captures many properties of real world instances. It generalizes the model of Mathieu and Schudy [2010] to arbitrary graphs G(V,E, c) with costs. A semi-random instance {G(V,E, c), (E+ , E−)} is generated as follows:\n1. The adversary chooses an undirected graph G(V,E, c) and a partition P∗ of the vertex set V (referred to as the planted clustering or ground truth clustering).\n2. Every edge is E is included in set ER independently with probability ε.\n3. Every edge (u, v) ∈ E \\ER with u and v in the same cluster of P∗ is included in E+, and every edge (u, v) ∈ E \\ ER, with u and v in different clusters of P∗ is included in E−.\n4. The adversary adds every edge from ER either to E+ or to E− (but not to both sets).\nThis model can be further generalized to an adaptive semi-random model as described in Section 3.1."
    }, {
      "heading" : "1.2 Our Results",
      "text" : "We develop two algorithms for semi-random instances of Correlation Clustering. The first algorithm gives a polynomial-time approximation scheme (PTAS) for instances from our semi-random model. The second algorithm recovers the planted partition with a small classification error η.\nTheorem 1.1. For every δ > 0, there is a polynomial-time algorithm that given a semi-random instance {G(V,E, c), (E+ , E−)} of Correlation Clustering (with noise probability ε < 1/4), finds a clustering that has disagreement cost (1 + δ) opt-cost +O((1 − 2ε)−4δ−3n log3 n) w.h.p. over the randomness in the instance, where opt-cost is the cost of disagreements of the optimal solution for the instance.\nThe approximation additive term is much smaller than the cost of the planted solution if the average degree ∆ ≫ ε−1polylog n. Note that we compare the performance of our algorithm with the cost of the optimal solution. Further, these guarantees hold even in a more general adaptive semi-random model that is described in Section 3.1.\nThe above result gives a good approximation guarantee with respect to the objective. But what about recovering the ground truth clustering? Our semi-random model is too general to allow recovery. For instance, there could be large disconnected pieces inside some clusters of G, or there could be no edges between some clusters — in both cases, recovery is statistically impossible. Hence, we need some additional conditions for approximate recovery in our model, that guarantee at the very least that the ground truth clustering is uniquely optimal (in a robust sense).\nOur first assumption is that there is mild expansion inside clusters — this connectivity assumption prevents large pieces inside clusters that are almost disconnected, which might get separated in an almost optimal clustering. The second and third assumptions are that there are enough edges from vertices in one cluster to other clusters, to prevent these clusters (or parts of them) from coalescing in near-optimal clusterings. Finally, we assume (approximate) regularity in degrees inside clusters, since it is hard to correctly classify vertices with very few edges incident on them. These assumptions are described formally in Assumptions 5.1. We now informally describe the algorithmic guarantees for approximate recovery:\nTheorem 1.2. There exists a polynomial-time algorithm that given a semi-random instance I = {G = (V,E, c), (E+, E−)} satisfying mild expansion inside clusters, regularity and inter-cluster density conditions (see Assumptions 5.1 for details) finds a partition P with classification error at most 4η w.h.p. over the randomness in the instance, where\nη = C2\n1− 2ε\n(\nn log n\ncost(E)\n)1/12 · ( 1\nβλgap\n)1/2\n. (1)\nOur algorithm outputs a clustering such that only O(ηn) vertices are misclassified (up to a renaming of the clusters). We note that the expansion and regularity assumptions are satisfied by Erdös–Renyi graphs: for instance, such random graphs have strong expansion both inside and between clusters (λgap = 1− o(1)) and have strong concentration of degrees. Our assumptions for recovery are soft: if there is bad expansion inside clusters (λgap is small), or if there are not sufficient edges between vertices in different clusters, we just need more observations (edges) to approximately recover the clusters. We note that the regularity conditions in Assumptions 5.1 are more for convenience and may be significantly relaxed. In particular, the same algorithm and analysis works even when the degrees are approximately regular (up to poly-logarithmic factors, for example) — this irregularity just appears in equation (1) as an extra multiplicative factor. We defer these details to the journal version of our paper."
    }, {
      "heading" : "1.3 Related Work on Semi-random Models",
      "text" : "Over the last two decades, there has been extensive research on average-case complexity of many important combinatorial optimization problems. Semi-random instances typically allow much more structure then completely random instances. Research on semi-random models was initiated by [Blum and Spencer, 1995], who introduced and investigated semi-random models for k-coloring. Semi-random models have also been studied for graph partitioning problems [Feige and Kilian, 1998, Chen et al., 2012, Makarychev et al., 2012, 2014], Independent Set [Feige and Kilian, 1998], Maximum Clique [Feige and Krauthgamer, 2000], Unique Games [Kolla et al., 2011], and other problems. Most related to our work, both in the nature of the model and in the techniques used, is a recent result of [Makarychev et al., 2013] on semi-random instances of Minimum\nFeedback Arc Set. While the techniques used in both papers are conceptually similar, the semidefinite (SDP) relaxation for Correlation Clustering that we use in this paper is very different from the SDP relaxation for Minimum Feedback Arc Set used in [Makarychev et al., 2013]. Further, we get a true 1 + δ approximation scheme (with an extra additive approximation term). This is in contrast to previous semi-random model results [Makarychev et al., 2012, 2013], which compare the cost of the solution that the algorithm finds to the cost of the planted solution. Moreover, this work gives not only a PTAS for the problem, but also a simple algorithm for recovery the ground truth solution.\nMathieu and Schudy recently considered a semi-random model for Correlation Clustering on complete graphs with unit edge costs. Later, Elsner and Schudy conducted an empirical evaluation of algorithms for the complete graph setting. Chen et al. [2014] extended the average-case model of Correlation Clustering to sparser Erdös–Renyi graphs. Very recently, Globerson et al. [2014] considered a semi-random model for Correlation Clustering for recovery in grid graphs and planar graphs, and gave conditions for approximate recovery in terms of an expansion-related condition.\nComparison of Results. The two works that are most similar in the nature of guarantees are [Mathieu and Schudy, 2010] and [Chen et al., 2014]. Mathieu and Schudy designed an algorithm based on semidefinite programming (SDP relaxations with ℓ22-triangle inequality constraints) for their semi-random model on complete graphs. It finds a clustering of cost at most 1 + O(n−1/6) times the cost of the optimal clustering (as long as ε ≤ 1/2 − O(n−1/3)) and manages to approximately recover the ground truth solution (when the clusters have size at least √ n). However, this algorithm only works on complete graphs and assumes unit edge costs. Chen et al. studied the problem on sparser graphs from the Erdös–Renyi distribution, and using weaker convex relaxations gave an algorithm that recovers the ground-truth when p ≥ k2 logO(1) n/n. In the case of Erdös–Renyi graphs, our algorithms obtain similar guarantees for smaller values of k (the implicit dependence on k is a worse polynomial than in [Chen et al., 2014], however). The main advantage of our algorithms is that they work for more general graphs G: the first algorithm requires only that the average degree of G is some poly-log of n, while the second algorithm requires additionally that the graph has a mild expansion and regularity; its performance depends softly on the expansion and regularity parameters of the graph."
    }, {
      "heading" : "1.4 Empirical Results",
      "text" : "This paper focuses on designing an algorithm with provable theoretical guarantees for correlation clustering in a natural semi-random model. We have tested our algorithm to confirm that it is easily implementable and scalable. We used the SDPNAL MATLAB library to solve the semidefinite programming (SDP) relaxation for the problem [Zhao et al., 2010]. We implemented the recovery algorithm from Section 2 in C++, and also used a simple cleanup step that merges small clusters with the larger clusters based on their average inner products (this extra step can only improve our theoretical guarantees). We note that we could solve the SDP relaxation for instances with thousands of vertices since we used a very basic SDP relaxation without ℓ22-triangle inequality constraints.\nWe tested the algorithm on random G(n, p) graphs with 4 planted clusters of size n/4 each, with the error rate (the probability of flipping the label) ε = 0.2. We used the same values of n as were used in [Chen et al., 2014]; we chose values of p smaller than or close to the minimal values for which the algorithm of [Chen et al., 2014] works (Chen et al. do not report the exact values of probabilities p; we took approximate values from Figure 2 in their paper). We summarize our results in Table 1."
    }, {
      "heading" : "2 Overview of the Algorithms and Structural Insights",
      "text" : "SDP relaxation. We use a simple SDP relaxation for the problem [Swamy, 2004]. For every vertex u, we have a unit vector ū. For two vertices u and v, we interpret the inner product 〈ū, v̄〉 ∈ [0, 1] as the indicator of the event: u and v lie in the same partition. The SDP is given below:\nmin P\n∑\n(u,v)∈E+\nc(u, v)(1 − 〈ū, v̄〉) + ∑\n(u,v)∈E−\nc(u, v)〈ū, v̄〉.\nsubject to: for all u, v ∈ V , 〈ū, v̄〉 ∈ [0, 1]; ‖ū‖2 = 1.\nThe intended vector (SDP) solution has one co-ordinate for every cluster of the clustering P: the vector ū for vertex u has 1 in the co-ordinate corresponding to P(u) and 0 otherwise. Hence this SDP is a valid relaxation. We note that this relaxation is weaker than the SDP used in [Mathieu and Schudy, 2010] because it does not have ℓ22-triangle inequalities constraints. Hence, this semidefinite program is more scalable, and it is efficiently solvable for instances with a few thousand nodes.\nApproximation Algorithm (PTAS). We now describe the algorithm that gives a PTAS. Fix a parameter δ = o(1) ∈ (0, 1/2). To simplify the notation, denote by f(u, v) (for (u, v) ∈ E) the SDP value of the edge (without cost):\nf(u, v) = 1− 〈ū, v̄〉 if (u, v) ∈ E+, and f(u, v) = 〈ū, v̄〉, otherwise. (2)\nOur PTAS is based on a surprising structural result about near-integrality of the SDP relaxation on the edges of the graph (see Theorem 3.1 for a formal statement).\nInformal Structural Theorem. In any feasible SDP solution of cost at most OPT , the SDP value of edge f(u, v) ≥ 1− δ for a 1− oδ(1/ log n) fraction of the inconsistent edges (u, v) ∈ E(G).\nHence, the structural result suggests that by removing all edges that contribute at least (1 − δ) to the objective, the remaining instance has a solution of very small cost. We then run the O(log n) worst-case approximation algorithm of [Charikar et al., 2005] or [Demaine et al., 2006] on the remaining graph to obtain a PTAS overall.\nRecovery. The algorithm outlined above finds a solution of near optimal cost. Under additional assumptions, we show that we can in fact design a very simple greedy rounding scheme that can also efficiently recover the ground truth clustering approximately.\nThe structural theorem above shows that the SDP vectors are highly correlated for pairs of adjacent vertices. Under the additional conditions, we show that the vectors are in fact globally clustered according to the ground truth clustering:\nInformal Structural Theorem. When the semi-random instance {G = (V,E, c), E+, E−} satisfies Assumption 5.1, we have w.h.p. that: for a (1 − O(η)) fraction of the clusters P ∗i we can choose centers ui ∈ P ∗i and define cores core(P ∗i ) = {v ∈ P ∗i : ‖v̄ − ūi‖ ≤ 1/10} ⊆ P ∗i (balls of radius 1/10 around centers ūi) such that core(P ∗i ) ≥ (1 − η)|P ∗i | (the core of P ∗i contains all but an η fraction of vertices of P ∗i ) and centers ui are mutually separated by a distance of at least 4/5.\nThe recovery algorithm is a greedy algorithm that finds heavy regions – sets of vectors that are clumped together – and puts them into clusters.\nInput: an optimal SDP solution {ū}u∈V . Output: partition P1, . . . , Pt of V (for some t).\ni = 1, ρcore = 0.1 Define an auxiliary graph Gaux = (V,Eaux) with Eaux = {(u, v) : ‖ū− v̄‖ ≤ ρcore} while V \\ (P1 ∪ . . . Pi−1) 6= ∅\nLet u be the vertex of maximum degree in Gaux[V \\ (P1 ∪ . . . Pi−1)]. Let Pi = {v /∈ P1 ∪ · · · ∪ Pi−1 : (u, v) ∈ Eaux} / note that Pi contains u i = i+ 1\nreturn clusters P1, . . . , Pi−1.\nThis structural result about the global clustering and near integrality of the SDP vectors is consistent with empirical evidence. While our algorithm succeeds when the SDP is tight (as in [Chen et al., 2014]), the analysis of our algorithm also shows how to deal with nearly integral solutions, in which most inner products 〈ū, v̄〉 are only close to 0 or 1 (but may not be tight). We believe that many instances arising in practice have SDP solutions that are nearly integral, but not integral. Hence, we believe that in practice, our algorithm will work better than previously known algorithms."
    }, {
      "heading" : "3 Polynomial-time Approximation Scheme",
      "text" : "In this section, we present the analysis of our polynomial-time approximation scheme for correlation clustering, which we presented in Section 2. The PTAS works in a very general Adaptive Model, which we describe first."
    }, {
      "heading" : "3.1 Adaptive Model",
      "text" : "We study a more general “adaptive” semi-random model. A semi-random instance is generated as follows. We start with a graph G0(V,∅) on n vertices with no edges and a partition P∗ of V into disjoint sets, which we call the planted partition. The adversary adds edges one by one. We denote the edge chosen at step t by et and its cost c(et) ∈ [0, 1]. After the adversary adds an edge et to the set of edges, the nature flips a coin and with probability ε adds e to the set of random edges ER. The next edge et+1 chosen by the adversary may depend on whether et belongs to ER or not. The adversary stops the semi-random process at a stopping time T . Thus, we obtain a graph G∗(V, {e1, . . . , eT }, c) and a set of random edges ER. We denote the set of all edges by E∗ = {e1, . . . , eT }. The adversary may remove some edges belonging to ER from the set E∗. Denote the set of the remaining edges by E. Note that E∗ \\ ER ⊂ E ⊂ E∗.\nOnce the graph G(V,E) and the set ER are generated, we perform steps 3 and 4 from the basic semirandom model for the graph G(V,E) and random set of edges ER ∩ E (as described in Section 1.1). We obtain a semi-random instance. This is the instance the algorithm gets. Of course, the algorithm does not\nget the set of random edges ER. Note that the cost of the planted solution P∗ is at most the cost of the edges ER ∩ E i.e. c ( ER ∩ E )\n, since all edges in E \\ ER are consistent with P∗. This Adaptive Model is more general than the Basic Semi-random model we introduced earlier. The basic semi-random model corresponds to the case when the whole set of edges E∗ is fixed in advance independent of the random choices made in ER, and E = E∗. However, in the adaptive model the edge et can be chosen based on which of the edges e1, . . . et−1 belong to ER. For instance, the adversary can choose edge et from the portion of the graph where many of the previously chosen edges belong to ER."
    }, {
      "heading" : "3.2 Analysis of the Algorithm",
      "text" : "Now we analyze the algorithm presented in Section 2. We need to bound the number of edges removed at the first step (that is, edges (u, v) with f(u, v) > 1 − δ) and the number of edges cut by the O(log n) approximation algorithm at the second step. The SDP contribution of every edge (u, v) removed at the first step is at least c(u, v)(1−δ). Thus the cost of edges removed at the first step is bounded by SDP/(1−δ) ≤ (1 + 2δ)OPT . To bound the cost of the solution produced by the approximation algorithm at the second step, we need to bound the cost of the optimal solution for the remaining instance i.e., the instance with the set of edges {(u, v) ∈ E : f(u, v) ≤ 1− δ}.\nFor any subset of edges F ⊂ E, let c(F ) represent the cost of the edges in F i.e. c(F ) = ∑e∈F c(e). Denote E∗+ and E ∗ −: E ∗ + = {(u, v) : P∗(u) = P∗(v)} and E∗− = {(u, v) : P∗(u) 6= P∗(v)}. Now define a function f∗(u, v), which slightly differs from f(u, v). For all (u, v) ∈ E,\nf∗(u, v) =\n{\n1− 〈ū, v̄〉, if P∗(u) = P∗(v); 〈ū, v̄〉, if P∗(u) 6= P∗(v).\n(3)\nHere, P∗ is the planted partition. Note that P∗ and f∗(u, v) are not known to the algorithm. Observe that f(u, v) = f∗(u, v) if the edge (u, v) is consistent with the planted partition P∗, and f(u, v) = 1− f∗(u, v) otherwise. Our goal is to show that the algorithm removes all but very few edges inconsistent with P∗, i.e., edges (u, v) with f(u, v) = 1 − f∗(u, v). We prove the following theorem in Section 3.3. The proof relies on Theorem 4.3 presented in Section 4.\nTheorem 3.1. Let {G = (V,E, c), (E+, E−)} be a semi-random instance of the correlation clustering problem. Let ER be the set of random edges, and P∗ be the planted partition. Denote by Q ⊂ ER the set of random edges not consistent with P∗. Then, for some universal constant C and every δ, γ > 0, and for Λ = C(1− 2ε)−2γ−2δ−3n log n,\nPr\n\n\n∑\n(u,v)∈Q:f(u,v)≤1−δ\nc(u, v) ≥ Λ+ 6γ 1− 2εc(Q)\n\n = o(1).\nwhere f corresponds to any feasible SDP solution of cost at most OPT .\nRemark 3.1. In the statement of Theorem 3.1, c(Q) is the value of the solution given by the planted solution P∗. If OPT = c(Q), then the planted solution P∗ is indeed an optimal clustering. The function f(u, v) in the theorem that corresponds to the SDP contribution of edge (u, v) could come from any (not necessarily optimal) SDP solution of cost at most OPT . This will be useful in Lemma 3.2.\nLet D = O(log n) be the approximation algorithms of Charikar et al. [2005] or Demaine et al. [2006]. We apply Theorem 3.1 with γ = δ(1−2ε)6D . The cost of edges in {(u, v) ∈ Q : f(u, v) ≤ 1 − δ} is bounded\nby\nΛ+ 6γ\n1− 2εc(Q) ≤ Λ+D −1δ c(Q), (4)\nw.h.p., where Λ = O((1− 2ε)−4δ−3n log3 n). Thus, after removing edges with f(u, v) ≥ (1− δ), the cost of the optimal solution is at most (4) w.h.p. The approximation algorithm finds a solution of cost at most D times (4). Thus, the total cost of the solution returned by the algorithm is at most\n(1 + 2δ)OPT +D × (Λ +D−1δ · c(Q)) = (1 + 3δ)c(Q) +DΛ = (1 + 3δ)c(Q) +O((1− 2ε)−4δ−3n log3 n).\nThe above argument shows that the solution has small cost compared to the cost of the planted solution P∗. We can in fact use Theorem 3.1 to give a true approximation i.e., compared to the cost of the optimal solution OPT . This follows from the following lower bound on OPT in terms of c(Q) for semi-random instances.\nLemma 3.2. In the notation of Theorem 3.1, with probability 1− o(1),\nc(Q) ≤ (1 + 2δ)OPT +O ( (1− 2ε)−4δ−3n log3 n ) .\nProof. Let fOPT correspond to the “integral” SDP solution corresponding to the optimal solution OPT . In this solution, fOPT (u, v) = 1 for positive edges (u, v) which are across different clusters and negative edges (u, v) which are in the same cluster. This SDP solution has cost OPT and satisfies the conditions of Theorem 3.1. Hence, w.h.p., c (Q \\ (Q ∩OPT )) ≤ δD · c(Q) + Λ. Hence,\nc(Q)−OPT ≤ δ D c(Q) + Λ and OPT ≥ (1− δ D ) · c(Q)− Λ.\nWe now conclude the analysis of the algorithm.\nProof of Theorem 1.1. From Theorem 3.1, we get the total cost of the solution is bounded by\n(1 + 2δ)OPT +D × (Λ +D−1δ · c(Q)) = (1 + 2δ)OPT +D × Λ+ δ 1− δ/D · (OPT + Λ)\n≤ (1 + 4δ)OPT + 2DΛ = (1 + 4δ)OPT +O((1− 2ε)−4δ−3n log3 n).\nThis finishes the analysis of the algorithm."
    }, {
      "heading" : "3.3 Structural Theorem – Proof of Theorem 3.1",
      "text" : "We now prove the Structural Theorem (Theorem 3.1) assuming Theorem 4.3. In order to use Theorem 4.3, we need to prove that the set of all SDP solutions to our problem has a small epsilon net. We use the following lemma from Makarychev et al. [2013].\nLemma 3.3 (ITCS, Lemma 2.7). For every graph G = (V,E) on n vertices (V = {1, . . . , n}) with the average degree ∆ = 2|E|/|V |, real M ≥ 1, and γ ∈ (0, 1), there exists a set of matrices W of size at most |W| ≤ exp(O(nM4 log∆\n2γ2 + n log n)) such that: for every collection of vectors L(1), . . . , L(n),\nR(1), . . . R(n) with ‖L(u)‖ = M , ‖R(v)‖ = M and 〈L(u), R(v)〉 ∈ [0, 1], there exists W ∈ W satisfying for every (u, v) ∈ E:\nwuv ≤ 〈L(u), R(v)〉 ≤ wuv + γ; wuv ∈ [0, 1].\nBy letting G be the complete graph, M = 1, L(u) = R(u) = f(u), we get the following corollary.\nCorollary 3.4. For every γ ∈ (0, 1), there exists a set of matrices W of size at most |W| ≤ exp ( O(nγ−2 log n) ) such that: For every collection of vectors {f(u)}, there exists W ∈ W satisfying for every (u, v):\n|wuv − 〈f(u), f(v)〉| ≤ γ.\nDefine f and f∗ as in (2) and (3). Recall, that the algorithm removes all edges (u, v) ∈ E with f(u, v) ≥ (1− γ). We show that the number of edges inconsistent with the planted partition P∗ that are remain in the graph after the fist step of the algorithm is small with high probability.\nProof of Theorem 3.1. For (u, v) ∈ E, let\nX(u,v) =\n{\n1, if (u, v) ∈ ER; −1, otherwise.\nLet Q+ = ER and Q− = E∗ \\ ER. Then, Q ⊂ Q+. Observe, that f(u, v) = f∗(u, v) if (u, v) ∈ E \\Q = Q− and f(u, v) = 1− f∗(u, v) if (u, v) ∈ Q ⊂ Q+. The SDP value is upper bounded by the optimal value OPT , which in turn is at most c(Q). Write,\nSDP = ∑\n(u,v)∈E\nc(u, v)f(u, v) = ∑\n(u,v)∈E\\Q\nc(u, v)f∗(u, v) + ∑\n(u,v)∈Q\nc(u, v)(1 − f∗(u, v)) ≤ c(Q).\nTherefore, ∑\n(u,v)∈E\\Q\nc(u, v)f∗(u, v) ≤ c(Q)− ∑\n(u,v)∈Q\nc(u, v)(1 − f∗(u, v)) = ∑\n(u,v)∈Q\nc(u, v)f∗(u, v).\nWe rewrite this expression as follows, ∑\n(u,v)∈Q∪Q−\nX(u,v)c(u, v)f ∗(u, v) ≥ 0. (5)\nSuppose that ∑\n(u,v)∈Q:f(u,v)≤1−δ\nc(u, v) ≥ Λ + 6γ 1− 2εc(Q).\nFor (u, v) ∈ Q, f(u, v) = 1− f∗(u, v). Thus, {(u, v) ∈ Q : f(u, v) ≤ 1− δ} = {(u, v) ∈ Q : f∗(u, v) ≥ δ}, and\n∑\n(u,v)∈Q\nc(u, v)f∗(u, v) ≥ δΛ + 6δγ 1− 2εc(Q). (6)\nBy Theorem 4.3 and Corollary 3.4, the probability that inequalities (5) and (6) hold is at most\n2 exp ( O(nγ−2δ−2 log n) ) exp ( − 1/5(1− 2ε)2δΛ ) = o(1),\nfor an appropriate choice of the constant C in the bound on Λ."
    }, {
      "heading" : "4 Betting with Stakes Depending on the Outcome",
      "text" : "We first informally describe the theorem we prove in this section. Consider the following game. Assume that we are given a set of vectors W ⊂ [0, 1]m. At every step t, the player (adversary) picks an arbitrary not yet chosen coordinate et ∈ {1, . . . ,m}, and the casino (nature) flips a coin such that with probability ε < 1/2, the player wins, and with probability (1 − ε) > 1/2, the player looses. In the former case, we set Xt = 1; and in the latter case we set Xt = −1. At some point T ≤ m the player stops the game. At that point, he picks a vector w ∈ W and declares that at time t his stake was w(et) dollars. We stress that the vector w may depend on the outcomes Xt. Then, the player’s payoff equals\nT ∑\nt=1\nXtw(et).\nIf the player could pick an arbitrary w after the outcomes Xt are revealed, then clearly he could get a significant payoff by letting w(et) = 1, for Xt = 1, and w(et) = 0, otherwise. However, we assume that the set W of possible bets is relatively small. Then, we show that with high probability the payoff is negative unless the total amount of bets ∑\nt w(et) is very small. The precise statement of the theorem (see below) is slightly more technical.\nThe main idea of the proof is that for any w ∈ W fixed in advance, the player is expected to loose with high probability, since the coin is not fair (ε < 1/2), and thus the casino has an advantage. In fact, the probability that the player wins is exponentially small if the coordinates of w are sufficiently large. Now we union bound over all w’s in W and conclude that with high probability for every w ∈ W , the player’s payoff is negative.\nWhen we apply this theorem to a semi-random instance of Correlation Clustering (with unit costs i.e. c(et) = 1), the stakes are defined by the solution of the SDP: for an edge et = (u, v), w(et) = f∗(u, v). Loosely speaking, we show that since the SDP value is at most OPT , the game is profitable for the adversary. This implies that most stakes f∗(u, v) are close to 0. Now, if an edge (u, v) is consistent with the planted partition P∗, then f(u, v) = f∗(u, v) ≈ 0, and hence we do not remove this edge. On the other hand, if the edge is not consistent with the planted partition, then f(u, v) = 1− f∗(u, v) ≈ 1, hence we remove the edge.\nLemma 4.1. LetW ⊂ [0, 1]m be a set of vectors. Consider a stochastic process (e1,X1, c1), . . . , (eT ,XT , cT ). Each et ∈ {1, . . . ,m} \\ {e1, . . . , et−1}, Xt ∈ {±1}, ct ∈ [0, 1]. Let Ft be the filtration generated by the random variables (e1,X1, c1), . . . , (et,Xt, ct), and F ′t be the filtration generated by the random variables (e1,X1, c1), . . . , (et,Xt, ct) and (et+1, ct+1). The random variable T ∈ {1, . . . ,m} is a stopping time w.r.t. Ft. Each Xt is a Bernoulli random variable independent of F ′t−1.\nXt =\n{\n1, with probability ε;\n−1, with probability 1− ε;\nwhere ε < 1/2. Then, for all Λ > 3(1− 2ε)−2,\nPr ( ∃w ∈ W s.t. T ∑\nt=1\nXtw(et)ct + 1− 2ε\n2\nT ∑\nt=1\nw(et)ct ≥ 0 and T ∑\nt=1\nw(et)ct ≥ Λ ) ≤\n≤ 2|W|e−1/5(1−2ε)2Λ. (7)\nProof. To prove the desired upper bound (7), we estimate the probability that ∑T t=1 Xtw(et)ct+ 1−2ε 2 ∑T t=1 w(et)ct ≥ 0 and ∑T\nt=1 w(et)ct ∈ [Λ′, 2Λ′] for a fixed w ∈ W and Λ′ ≥ Λ. Then we apply the union bound for all w ∈ W , and Λ′ of the form 2iΛ.\nFix a w ∈ W and Λ′ = 2i. Each Xt+1 is independent of F ′t , hence E[Xt+1w(et+1)ct+1 | F ′t ] = E[Xt+1]w(et+1)ct+1 = (2ε− 1)w(et+1)ct+1. Thus,\nSτ ≡ τ ∑\nt=1\n(Xt + 1− 2ε)w(et)ct\nis a martingale. Note that |St+1 − St| ≤ w(et+1)ct+1 ≤ ct+1 and\nVar[Xt+1w(et+1ct+1) | F ′t] = 4ε(1 − ε)w(et+1)2c2t+1 ≤ 4ε(1 − ε)w(et+1)ct+1.\nIf ∑T t=1 Xtw(et)ct + 1−2ε 2 ∑T t=1 w(et)ct ≥ 0 and ∑T t=1 w(et)ct ∈ [Λ′, 2Λ′], then\nST =\n[\nT ∑\nt=1\nXtw(et)ct + (1− 2ε)\n2\nT ∑\nt=1\nw(et)ct\n]\n+ (1− 2ε)\n2\nT ∑\nt=1\nw(et)ct ≥ (1− 2ε)\n2 Λ′,\nand T ∑\nt=1\nVar[Xtw(et)ct | F ′t−1] = 4(ε− ε2) T ∑\nt=1\nw(et)ct ≤ 8ε(1− ε)Λ′.\nNow, by Freedman’s inequality (see Freedman [1975]),\nPr ( ST ≥ (1− 2ε)Λ′ and T ∑\nt=1\nVar[Xtw(et)ct | Ft−1] ≤ 8ε(1 − ε)Λ′ ) ≤ e− (1−2ε)2Λ′2 2((1−2ε)Λ′+8ε(1−ε)Λ′)\n= e− (1−2ε)2Λ′ 5 ,\nand\nPr (\nT ∑\nt=1\nXtw(et)ct ≥ 0 and T ∑\nt=1\nw(et)ct ∈ [Λ′, 2Λ′] ) ≤ Pr ( ST ≥ (1− 2ε)Λ′ and T ∑\nt=1\nw(et) 2c2t ≤ 2Λ′\n)\n≤ e−1/5 (1−2ε)2Λ′ = ( e− 1/5 (1−2ε)2Λ)2 i .\nSumming up this upper bound over all w ∈ W and Λ′ = 2iΛ, we get (7).\nWe now slightly generalize this theorem. In our application, the set of all possible stakes can be infinite, however, we know that there is a relatively small epsilon net for it.\nDefinition 4.2. We say that a set W ⊂ Rm is a γ–net for a set Z ⊂ Rm in the ℓ∞ norm, if for every z ∈ Z , there exists w ∈ W such that ‖z − w‖∞ ≡ maxi{|z(i) − w(i)|} ≤ γ.\nRemark 4.1. If W is a γ–net for Z ⊂ [0, 1]m, then there exists W ′ ⊂ [0, 1]m of the same size as W (|W ′| = |W|), such that for every z ∈ Z , there exists w′ ∈ W ′ satisfying w′(i) ≤ z(i) ≤ w′(i) + 2γ for all i. To obtain W ′ we simply subtract min(γ,w(i)) from each coordinate of w and then truncate each w′(i) at the threshold of 1.\nTheorem 4.3. Consider a stochastic process (e1,X1, c1), . . . , (eT ,XT , cT ) such that each et ∈ {1, . . . ,m}\\ {e1, . . . , et−1}, Xt ∈ {±1} and ct ∈ [0, 1]. Let Ft be the filtration generated by the random variables (e1,X1, c1), . . . , (et,Xt, ct), and F ′t be the filtration generated by the random variables (e1,X1, c1), . . . , (et,Xt, ct) and (et+1, ct+1). The random variable T ∈ {1, . . . ,m} is a stopping time w.r.t. Ft. Each Xt is a Bernoulli random variable independent of F ′t−1.\nXt =\n{\n1, with probability ε;\n−1, with probability 1− ε;\nwhere ε < 1/2. Let Z ⊂ [0, 1]m be a set of vectors having a γ–net in the L∞ norm of size N . Define two random sets depending on {Xt}:"
    }, {
      "heading" : "Q+ = {t : Xt = 1} and Q− = {t : Xt = −1}.",
      "text" : "Then, for all Λ > 3(1 − 2ε)2, we have\nPr ( ∃z ∈ Z, Q⊕ ⊂ Q+ s.t. ∑\nt∈Q⊕∪Q−\nXtz(et)ct ≥ 0\nand ∑\nt∈Q⊕\nz(et)ct ≥ Λ + 6γ 1− 2ε ∑\nt∈Q⊕\nct\n)\n≤ 2Ne−1/5(1−2ε)2Λ. (8)\nProof. Let W be a γ–net for Z . For simplicity of exposition we subtract min(γ,w(i)) from all coordinates of vectors w ∈ W . Thus, we assume that for all z ∈ Z , there exists w ∈ W such that w(i) ≤ z(i) ≤ w(i) + 2γ and w(i) ≥ 0 for all i (see Remark 4.1).\nSuppose that for some z ∈ Z and Q⊕ ⊂ Q+, the inequalities ∑\nt∈Q⊕∪Q−\nXtz(et)ct ≥ 0 (9)\nand ∑\nt∈Q⊕\nz(et)ct ≥ Λ+ 6γ 1− 2ε ∑\nt∈Q⊕\nct (10)\nhold. Pick a w ∈ W , such that w(i) ≤ z(i) ≤ w(i) + 2γ for all i. We replace z(et) with w(et) in (10): ∑\nt∈Q⊕\nw(et)ct ≥ ∑\nt∈Q⊕\n(z(et)− 2γ)ct ≥ Λ+ 4γ (1− 2ε) · ∑\nt∈Q⊕\nct. (11)\nThen,\nT ∑\nt=1\nXtw(et)ct + 1− 2ε\n2\nT ∑\nt=1\nw(et)ct ≥ ∑\nt∈Q⊕∪Q−\nXtw(et)ct + 1− 2ε\n2\n∑\nt∈Q⊕\nw(et)ct (12)\n≥\n\n\n∑\nt∈Q⊕\n(z(et)− 2γ)ct − ∑\nQ−\nz(et)ct\n\n+ 2γ ∑\nt∈Q⊕\nct\n= ∑\nt∈Q⊕∪Q−\nXtz(et)ct ≥ 0.\nBy Lemma 4.1, there exists a w ∈ W satisfying (11) and (12) with probability at most 2Ne−1/5(1−2ε)2Λ. This concludes the proof."
    }, {
      "heading" : "5 Recovery Algorithm",
      "text" : "In this section, we prove Theorem 1.2 that shows that under some additional assumptions on the graph G and partition P∗, we can recover the planted partition P∗ with an arbitrarily small classification error η. The recovery algorithm is a very fast and very simple greedy algorithm (presented in Section 2).\nAssumptions 5.1. Consider a semi-random instance I = {G = (V,E, c), (E+, E−)}. Let P∗ be the planted partition. Denote the clusters of G w.r.t clustering P∗ by P ∗1 , . . . , P ∗k . Let β = c(E∗+)/c(E) (note that E∗+ is the set of edges that lie within clusters) and βij = c({(u, v) : u ∈ P ∗i , v ∈ P ∗j })/c(E) (here, {(u, v) : u ∈ P ∗i , v ∈ P ∗j } is the set of edges between clusters P ∗i and P ∗j ). Assume that the instance I satisfies the following conditions:\n• Cluster Expansion. All induced graphs G[P ∗i ] are spectral expanders with spectral expansion at least λgap; that is, the second smallest eigenvalue of the normalized Laplacian of G[P ∗i ] is at least λgap. • Intercluster Density. For some sufficiently large constant C1, and every two clusters P ∗i and P ∗j ,\nβij > C1\n(1−2ε)2\n(\nn logn c(E)\n)1/6 .\n• Intercluster Regularity. The set of edges between every two clusters P ∗i and P ∗j forms a regular graph with respect to the cost function c: for every u′, u′′ ∈ P ∗i we have c({(u′, v) : v ∈ P ∗j }) = c({(u′′, v) : v ∈ P ∗j }). • Cluster Regularity. All induced graphs G[P ∗i ] are regular graphs with the same degree w.r.t to the cost function c. That is, for some number c0, every cluster P ∗i , and every vertex u ∈ P ∗i , c0 = c({(u, v) ∈ E : v ∈ P ∗i }).\nRemark The Intercluster and Cluster Regularity assumptions can be significantly relaxed; in fact, we only need that degrees are equal up to some multiplicative factor (say, a poly-log factor). We include the regularity assumptions to simplify the exposition.\nDefinition 5.2. Let I = {G = (V,E, c), (E+, E−)} be a semi-random instance of correlation clustering, P∗ be the planted partition, and P ∗1 , . . . , P ∗k be the planted clusters. We say that a partition P of V into clusters P1, . . . , Pt has an η classification error if there is a partial matching between clusters P ∗1 , . . . , P ∗ k and clusters P1, . . . , Pt such that\n∑\nP ∗i is matched with Pj\n|P ∗i ∩ Pj | ≥ (1− η)|V |.\nTheorem 1.2 relies on the following theorem that describes the structure of optimal SDP solutions to semi-random instances of correlation clustering that satisfy conditions in Assumption 5.1.\nTheorem 5.3. Assume that a semi-random instance I = {G = (V,E, c), (E+, E−)} satisfies Assumptions 5.1. Let {ū} be the optimal SDP solution to I . With probability 1 − o(1), there exist a subset of clusters C ⊂ {P ∗1 , . . . , P ∗k } and a vertex ui in each cluster P ∗i satisfying the following properties. Let ρcore = 1/10 and ρinter = 4/5. Let core(P ∗i ) = {v ∈ P ∗i : ‖v̄ − ūi‖ ≤ ρcore} for P ∗i ∈ C, then\n1. | ∪P ∗i ∈C P ∗ i | ≥ (1− η)|V |.\n2. | core(P ∗i )| ≥ (1− η)|P ∗i |.\n3. In particular, ∑ P ∗i ∈C |Pi| ≥ (1− η)2|V |.\n4. ‖ūi − ūj‖ ≥ ρinter for every two distinct clusters P ∗i , P ∗j ∈ C.\nWe now use Theorem 5.3 to prove the recovery guarantees of our algorithm.\nProof of Theorem 1.2. Consider a cluster Pi. Let u be the vertex we choose at iteration i of the while–loop. If Pi intersects a core core(P ∗j ) of a cluster P ∗ j then ‖u− uj‖ ≤ 2ρcore. Note that Pi cannot intersect cores core(P ∗j′) and core(P ∗ j′′) of two distinct clusters P ∗ j′ and P ∗ j′′ , since ‖u− uj′‖+ ‖u−uj′′‖ ≥ ‖uj′ − uj′‖ ≥ ρinter > 4ρcore. Thus each cluster Pi intersects at most the core of one cluster P ∗j . We match every cluster P ∗j ∈ C to the first cluster Pi that intersects core(P ∗j ). Consider a cluster P ∗j ∈ C and the matching cluster Pi. Since core(P ∗j ) ∩ (P1 ∪ · · · ∪ Pi−1) = ∅, we have, in particular, that uj /∈ P1 ∪ · · · ∪ Pi−1 and uj has degree at least | core(P ∗j )| in Gaux[V \\ (P1 ∪ . . . Pi−1)]. Thus the vertex u that we choose at iteration i has degree at least | core(P ∗j )| and |Pi| ≥ | core(P ∗j )|; in particular, |Pi \\ core(P ∗j )| ≥ | core(P ∗j ) \\ Pi|. We have,\n|P ∗j ∩ Pi| ≥ | core(P ∗j ) ∩ Pi| = | core(P ∗j )| − | core(P ∗j ) \\ Pi| ≥ | core(P ∗j )| − |Pi \\ core(P ∗j )|. Note that by Theorem 5.3 (item 3)\n∑\nPi is matched with P ∗j\n|Pi \\ core(P ∗j )| ≤ |V | − | ⋃\nP ∗j ∈C\ncore(P ∗j )| ≤ n− (1− η)2n ≤ 2ηn.\nTherefore, ∑\nPi is matched with P ∗j\n|Pi ∩ P ∗j | ≥ ( ∑\nP ∗j ∈C\n| core(P ∗j )| ) − 2ηn ≥ (1− η)2n− 2η ≥ (1− 4η)n.\nWe proved that the algorithm finds a clustering with classification error at most 4η."
    }, {
      "heading" : "5.1 Structure of the optimal SDP solution: Proof of Theorem 5.3",
      "text" : "We now prove Theorem 5.3 which gives the structure of optimal SDP solutions to semi-random instances of correlation clustering that satisfy conditions in Assumption 5.1. As seen earlier, completing this proof concludes the proof of Theorem 1.2.\nLet δ = γ = (n log n/c(E))1/6. Let Λ and Q be as in Theorem 3.1. Let σ = 6δ/(1 − 2ε). Note that Λ = O(δ/(1 − 2ε)2).\nDefine f as in (2):\nf(u, v) =\n{\n1− 〈ū, v̄〉, if (u, v) ∈ E+; 〈ū, v̄〉, if (u, v) ∈ E−.\n(13)\nConsider the set of edges Eflip = {(u, v) ∈ E : f(u, v) > 1− δ}. Change the sign of each edge in Eflip and obtain a new partitioning of E into positive and negative edges, Ê+ and Ê−:\nÊ+ = E+△Eflip = {(u, v) ∈ E+ : f(u, v) ≤ 1− δ} ∪ {(u, v) ∈ E− : f(u, v) > 1− δ} , Ê− = E−△Eflip = {(u, v) ∈ E− : f(u, v) ≤ 1− δ} ∪ {(u, v) ∈ E+ : f(u, v) > 1− δ} .\nLet us now consider the corresponding instance Î = { G = (V,E, c), (Ê+, Ê−) } . Let f̂ be the analog of\nfunction f for Î:\nf̂(u, v) =\n{\n1− 〈ū, v̄〉, if (u, v) ∈ Ê+ 〈ū, v̄〉, if (u, v) ∈ Ê−\n=\n{\nf(u, v), if (u, v) /∈ Eflip; 1− f(u, v), if (u, v) ∈ Eflip.\n(14)\nSimilarly, let ŜDP = ∑ (u,v)∈E c(u, v)f̂ (u, v) be the cost of the SDP solution {ū} for Î.\nLemma 5.4. With probability 1− o(1), the following properties hold.\n1. c(Q \\ Eflip) ≤ σc(Q) + Λ.\n2. c(Eflip \\Q) ≤ (2δ + σ)c(Q) + Λ.\n3. Then ŜDP ≤ (2δ + σ)c(Q) + Λ.\nProof. 1. From Theorem 3.1, we get that c(Q \\ Eflip) ≤ σc(Q) + Λ with probability 1− o(1). 2. Write c(Eflip \\Q) = c(Eflip)− c(Q ∩Eflip). Now we bound c(Eflip) and c(Q ∩Eflip). Note that\nSDP = ∑\n(u,v)∈E\nc(u, v)f(u, v) ≥ ∑\n(u,v)∈Eflip\nc(u, v)(1 − δ) = (1− δ)c(Eflip).\nHence, c(Eflip) ≤ SDP/(1− δ) ≤ c(Q)/(1 − δ) ≤ (1 + 2δ)c(Q),\nhere, we used that {ū} is an optimal SDP solution and therefore SDP ≤ c(Q). By item 1, c(Q ∩ Eflip) = c(Q)− c(Q \\ Eflip) ≥ (1− σ)c(Q) − Λ. We get that\nc(Eflip \\Q) ≤ (1 + 2δ)c(Q) − (1− σ)c(Q) − Λ = (2δ + σ)c(Q) + Λ.\n3. From the second formula for f̂(u, v) in (14), we get that f(u, v)− f̂(u, v) = 2f(u, v) − 1 ≥ 1− 2δ for (u, v) ∈ Eflip, and f(u, v)− f̂(u, v) = 0 for (u, v) /∈ Eflip. Therefore,\nc(Q)− ŜDP ≥ SDP − ŜDP = ∑\n(u,v)∈E\nc(u, v)(f(u, v) − f̂(u, v))\n= ∑\n(u,v)∈Eflip\nc(u, v)(f(u, v) − f̂(u, v)) ≥ (1− 2δ)c(Eflip) ≥ (1− 2δ)c(Q ∩ Eflip)\n≥ (1− 2δ)((1 − σ)c(Q)− Λ) ≥ (1− 2δ − σ)c(Q) − Λ.\nTherefore, ŜDP ≤ (2δ + σ)c(Q) + Λ.\nWe now bound the total squared Euclidean length of all edges in E∗+.\nLemma 5.5. With probability 1− o(1), we have\n1\n2\n∑\n(u,v)∈E∗+\nc(u, v)‖ū − v̄‖2 ≤ (4δ + 3σ)c(Q) + 3Λ\nProof. Note that for (u, v) ∈ Ê+, 12‖ū− v̄‖2 = f̂(u, v) and thus\n1\n2\n∑\n(u,v)∈Ê+\nc(u, v)‖ū − v̄‖2 ≤ ŜDP .\nAlso, E∗+∩Ê− ⊂ (Q\\Eflip)∪(Eflip \\Q). Thus, by Lemma 5.4, c(E∗+∩Ê−) ≤ c(Q\\Eflip)+c(Eflip \\Q) ≤ 2(δ + σ)c(Q) + 2Λ. We have,\n1\n2\n∑\n(u,v)∈E∗+\nc(u, v)‖ū − v̄‖2 ≤ 1 2\n∑\n(u,v)∈E∗+∩Ê+\nc(u, v)‖u − v‖2 + 1 2\n∑\n(u,v)∈E∗+∩Ê−\nc(u, v)‖u − v‖2\n≤ ŜDP + c(E∗+ ∩ Ê−) = (4δ + 3σ)c(Q) + 3Λ.\nWe are ready to prove Theorem 5.3.\nProof of Theorem 5.3. We assume that η < 1/4 as otherwise the statement of theorem is trivial. Let\nρ2avg = 1\nc(E∗+)\n∑\n(u,v)∈E∗+\nc(u, v)‖ū − v̄‖2 ≤ O(σc(Q) + Λ)/c(E∗+) ≤ O(σ + Λ/c(E))/β.\nLet E∗+(i) = { (u, v) ∈ E∗+ : u, v ∈ P ∗i } be the set of edges within cluster P ∗i . Write\nk ∑\ni=1\n∑\n(u,v)∈E∗+(i)\nc(u, v)‖ū − v̄‖2 = c(E∗+)ρ2avg.\nLet C be the set of clusters P ∗i such that ∑ (u,v)∈E∗+(i) c(u, v)‖ū − v̄‖2 ≤ c(E∗+(i))ρ2avg/η. By Markov’s inequality, ∑\nP ∗i ∈C c(E∗+(i)) ≥ (1 − η)c(E∗+). By the Cluster Regularity condition in Assumptions 5.1,\nc(E∗+(i)) = (|P ∗i |/n)c(E∗+). We get that ∑ P ∗i ∈C |Pi| ≥ (1−η)n and item 1 in the statement of the theorem holds. By the Poincaré inequality3 we have for each cluster P ∗i ∈ C,\n1\n|P ∗i |2 ∑\nu,v∈P ∗i\n‖ū− v̄‖2 ≤ 1 λgap\n1\nc(E∗+(i))\n∑\n(u,v)∈E∗+(i)\nc(u, v)‖ū − v̄‖2 ≤ ρ2avg λgapη .\nTherefore,\nmin u∈P ∗i\n\n\n1\n|P ∗i | ∑\nv∈P ∗i\n‖ū− v̄‖2 \n ≤ 1|P ∗i | ∑\nu∈P ∗i\n\n\n1\n|P ∗i | ∑\nv∈P ∗i\n‖ū− v̄‖2 \n ≤ ρ2avg λgapη .\nThus we can choose ui in each P ∗i ∈ C such that 1|P ∗i | ∑ v∈P ∗ i ‖ūi − v̄‖2 ≤ ρ2avg λgapη . This choice of vertices ui defines sets core(P ∗i ), as in the statement of the theorem. Using again Markov’s inequality, we get that for at least a 1 − η fraction of vertices v in P ∗i , ‖ūi − v̄‖2 ≤ ρ2avg/(λgapη2). From the bound ρ2avg = O(σ + Λ/c(E))/β and formula 1, we get ρ2core ≥ ρ2avg/(λgapη2) and\n| core(P ∗i )| ≥ | { v ∈ P ∗i : ‖ūi − v̄‖2 ≤ ρ2avg/(λgapη2) } | ≥ (1− η)|P ∗i |. 3Recall that the Poincaré inequality states that for every every expander graph H = (VH , EH , cH) with spectral expansion λ\nand every set of vectors {ū}u∈VH , we have 1\n|VH | 2\n∑ u,v∈V ‖ū− v̄‖ 2 ≤ 1 λ·cH(EH ) ∑ (u,v)∈EH cH(u, v)‖ū− v̄‖ 2. Here, we apply\nthe Poincaré inequality to the induced graph G[P ∗i ]\nWe showed that item 2 in the statement of the theorem holds. We get item 3 from items 1 and 2. Finally, we show that ‖ūi − ūj‖ ≥ ρinter for every two distinct clusters P ∗i , P ∗j ∈ C. To this end, we show that there are vertices v′ ∈ core(P ∗i ) and v′′ ∈ core(P ∗j ) such that ‖v̄′ − v̄′′‖ ≥ ρinter + 2ρcore, and thus ‖ūi − ūj‖ ≥ (ρinter + 2ρcore) − ‖ui − v′‖ − ‖uj − v′′‖ ≥ ρinter. Assume to the contrary that ‖v̄′ − v̄′′‖ < ρinter + 2ρcore for every v′ ∈ core(P ∗i ) and v′′ ∈ core(P ∗j ). Let Eij = {\n(v′, v′′) ∈ E : v′ ∈ core(P ∗i ), v′′ ∈ core(P ∗j ) } .\nSince Eij ⊂ E∗−, we have for every (v′, v′′) ∈ Eij \\ (Q△Eflip),\nf̂(v′, v′′) = 〈v̄′, v̄′′〉 = 1− ‖v̄′ − v̄′′‖2/2 ≥ 1− (ρinter + 2ρcore)2/2 = 1/2.\nTherefore, ŜDP ≥\n∑\n(v′,v′′)∈Eij\\(Q△Eflip)\nc(v′, v′′)f(v′, v′′) ≥ c(Eij \\ (Q△Eflip))/2.\nFrom the Intercluster Regularity condition and bounds | core(P ∗i )| ≥ (1 − η)|P ∗i | and | core(P ∗j )| ≥ (1 − η)|P ∗j |, we get c(Eij) ≥ (1− 2η)βijc(E) . By Lemma 5.4,\nc(Q△Eflip) ≤ 2(δ + σ)c(Q) + 2Λ ≤ 2(δ + σ)c(E) + 2Λ.\nBy the Intercluster Density condition in Assumptions 5.1 and our choice of δ, we have\nc(Eij \\ (Q△Eflip)) ≥ ((1− 2η)βij − 2δ − 2σ)c(E) − 2Λ ≥ βijc(E)/3.\nWe get that (2δ + σ)c(Q) + Λ ≥ ŜDP ≥ βijc(E)/6,\nwhich contradicts to the Intercluster Density condition and our choice of δ."
    } ],
    "references" : [ {
      "title" : "Aggregating inconsistent information: Ranking and clustering",
      "author" : [ "Nir Ailon", "Moses Charikar", "Alantha Newman" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Ailon et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ailon et al\\.",
      "year" : 2008
    }, {
      "title" : "An automated method for finding molecular complexes in large protein interaction networks",
      "author" : [ "G D Bader", "C W Hogue" ],
      "venue" : "BMC Bioinformatics,",
      "citeRegEx" : "Bader and Hogue.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bader and Hogue.",
      "year" : 2003
    }, {
      "title" : "Finding low error clusterings",
      "author" : [ "Maria-Florina Balcan", "Mark Braverman" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "Balcan and Braverman.,? \\Q2009\\E",
      "shortCiteRegEx" : "Balcan and Braverman.",
      "year" : 2009
    }, {
      "title" : "Correlation clustering",
      "author" : [ "Nikhil Bansal", "Avrim Blum", "Shuchi Chawla" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Bansal et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2004
    }, {
      "title" : "Clustering gene expression patterns",
      "author" : [ "Amir Ben-Dor", "Ron Shamir", "Zohar Yakhini" ],
      "venue" : "Journal of Computational Biology,",
      "citeRegEx" : "Ben.Dor et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Ben.Dor et al\\.",
      "year" : 1999
    }, {
      "title" : "Coloring random and semi-random k-colorable graphs",
      "author" : [ "Avrim Blum", "Joel Spencer" ],
      "venue" : "J. Algorithms,",
      "citeRegEx" : "Blum and Spencer.,? \\Q1995\\E",
      "shortCiteRegEx" : "Blum and Spencer.",
      "year" : 1995
    }, {
      "title" : "Clustering with qualitative information",
      "author" : [ "Moses Charikar", "Venkatesan Guruswami", "Anthony Wirth" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "Charikar et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Charikar et al\\.",
      "year" : 2005
    }, {
      "title" : "Near optimal LP rounding algorithm for correlation clustering on complete and complete k-partite graphs",
      "author" : [ "Shuchi Chawla", "Konstantin Makarychev", "Tselil Schramm", "Grigory Yaroslavtsev" ],
      "venue" : "CoRR, abs/1412.0681,",
      "citeRegEx" : "Chawla et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chawla et al\\.",
      "year" : 2014
    }, {
      "title" : "Clustering sparse graphs",
      "author" : [ "Yudong Chen", "Sujay Sanghavi", "Huan Xu" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Chen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2012
    }, {
      "title" : "Clustering partially observed graphs via convex optimization",
      "author" : [ "Yudong Chen", "Ali Jalali", "Sujay Sanghavi", "Huan Xu" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Correlation clustering in general weighted graphs",
      "author" : [ "Erik D. Demaine", "Dotan Emanuel", "Amos Fiat", "Nicole Immorlica" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Demaine et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Demaine et al\\.",
      "year" : 2006
    }, {
      "title" : "Bounding and comparing methods for correlation clustering beyond ilp",
      "author" : [ "Micha Elsner", "Warren Schudy" ],
      "venue" : "In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing,",
      "citeRegEx" : "Elsner and Schudy.,? \\Q2009\\E",
      "shortCiteRegEx" : "Elsner and Schudy.",
      "year" : 2009
    }, {
      "title" : "Heuristics for finding large independent sets, with applications to coloring semirandom graphs",
      "author" : [ "Uriel Feige", "Joe Kilian" ],
      "venue" : "In Proceedings of Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Feige and Kilian.,? \\Q1998\\E",
      "shortCiteRegEx" : "Feige and Kilian.",
      "year" : 1998
    }, {
      "title" : "Finding and certifying a large hidden clique in a semirandom graph",
      "author" : [ "Uriel Feige", "Robert Krauthgamer" ],
      "venue" : "Random Struct. Algorithms,",
      "citeRegEx" : "Feige and Krauthgamer.,? \\Q2000\\E",
      "shortCiteRegEx" : "Feige and Krauthgamer.",
      "year" : 2000
    }, {
      "title" : "Community detection in graphs",
      "author" : [ "Santo Fortunato" ],
      "venue" : "Physics Reports,",
      "citeRegEx" : "Fortunato.,? \\Q2010\\E",
      "shortCiteRegEx" : "Fortunato.",
      "year" : 2010
    }, {
      "title" : "On tail probabilities for martingales",
      "author" : [ "David A Freedman" ],
      "venue" : "The Annals of Probability,",
      "citeRegEx" : "Freedman.,? \\Q1975\\E",
      "shortCiteRegEx" : "Freedman.",
      "year" : 1975
    }, {
      "title" : "Approximate max-flow min-(multi)cut theorems and their applications",
      "author" : [ "Naveen Garg", "Vijay V. Vazirani", "Mihalis Yannakakis" ],
      "venue" : "In Proceedings of Symposium on Theory of Computing,",
      "citeRegEx" : "Garg et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 1993
    }, {
      "title" : "Community structure in social and biological networks",
      "author" : [ "M. Girvan", "M.E.J. Newman" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Girvan and Newman.,? \\Q2002\\E",
      "shortCiteRegEx" : "Girvan and Newman.",
      "year" : 2002
    }, {
      "title" : "Tight error bounds for structured prediction",
      "author" : [ "Amir Globerson", "Tim Roughgarden", "David Sontag", "Cafer Yildirim" ],
      "venue" : "CoRR, abs/1409.5834,",
      "citeRegEx" : "Globerson et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Globerson et al\\.",
      "year" : 2014
    }, {
      "title" : "Stochastic blockmodels and community structure in networks",
      "author" : [ "Brian Karrer", "M.E.J. Newman" ],
      "venue" : "Phys. Rev. E,",
      "citeRegEx" : "Karrer and Newman.,? \\Q2011\\E",
      "shortCiteRegEx" : "Karrer and Newman.",
      "year" : 2011
    }, {
      "title" : "How to play unique games against a semi-random adversary: Study of semi-random models of unique games",
      "author" : [ "Alexandra Kolla", "Konstantin Makarychev", "Yury Makarychev" ],
      "venue" : "In Proceeding of Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Kolla et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kolla et al\\.",
      "year" : 2011
    }, {
      "title" : "Trawling the web for emerging cyber-communities",
      "author" : [ "Ravi Kumar", "Prabhakar Raghavan", "Sridhar Rajagopalan", "Andrew Tomkins" ],
      "venue" : "In Computer Networks,",
      "citeRegEx" : "Kumar et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 1999
    }, {
      "title" : "Approximation algorithms for semi-random partitioning problems",
      "author" : [ "Konstantin Makarychev", "Yury Makarychev", "Aravindan Vijayaraghavan" ],
      "venue" : "In Proceedings of Symposium on Theory of Computing,",
      "citeRegEx" : "Makarychev et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Makarychev et al\\.",
      "year" : 2012
    }, {
      "title" : "Sorting noisy data with partial information",
      "author" : [ "Konstantin Makarychev", "Yury Makarychev", "Aravindan Vijayaraghavan" ],
      "venue" : "In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science,",
      "citeRegEx" : "Makarychev et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Makarychev et al\\.",
      "year" : 2013
    }, {
      "title" : "Constant factor approximation for balanced cut in the random PIE model",
      "author" : [ "Konstantin Makarychev", "Yury Makarychev", "Aravindan Vijayaraghavan" ],
      "venue" : "In Proceedings of Symposium on Theory of Computing,",
      "citeRegEx" : "Makarychev et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Makarychev et al\\.",
      "year" : 2014
    }, {
      "title" : "Correlation clustering with noisy input",
      "author" : [ "Claire Mathieu", "Warren Schudy" ],
      "venue" : "In Proceedings of Symposium on Discrete Algorithms,",
      "citeRegEx" : "Mathieu and Schudy.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mathieu and Schudy.",
      "year" : 2010
    }, {
      "title" : "The Structure and Dynamics of Networks: (Princeton Studies in Complexity)",
      "author" : [ "Mark Newman", "Albert-Laszlo Barabasi", "Duncan J. Watts" ],
      "venue" : null,
      "citeRegEx" : "Newman et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Newman et al\\.",
      "year" : 2006
    }, {
      "title" : "Structured learning and prediction in computer vision",
      "author" : [ "Sebastian Nowozin", "Christoph H. Lampert" ],
      "venue" : "Foundations and Trends in Computer Graphics and Vision,",
      "citeRegEx" : "Nowozin and Lampert.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nowozin and Lampert.",
      "year" : 2010
    }, {
      "title" : "Linguistic Structure Prediction. Synthesis Lectures on Human Language Technologies",
      "author" : [ "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Smith.,? \\Q2011\\E",
      "shortCiteRegEx" : "Smith.",
      "year" : 2011
    }, {
      "title" : "Correlation clustering: Maximizing agreements via semidefinite programming",
      "author" : [ "Chaitanya Swamy" ],
      "venue" : "In Proceedings of Symposium on Discrete Algorithms,",
      "citeRegEx" : "Swamy.,? \\Q2004\\E",
      "shortCiteRegEx" : "Swamy.",
      "year" : 2004
    }, {
      "title" : "Fast planar correlation clustering for image segmentation",
      "author" : [ "Julian Yarkony", "Alexander T. Ihler", "Charless C. Fowlkes" ],
      "venue" : "In 12th European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "Yarkony et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yarkony et al\\.",
      "year" : 2012
    }, {
      "title" : "A newton-cg augmented lagrangian method for semidefinite programming",
      "author" : [ "Xinyuan Zhao", "Defeng Sun", "Kim-Chuan Toh" ],
      "venue" : "SIAM J. Optimization,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Correlation clustering was introduced in [Bansal et al., 2004], and implicitly in [Ben-Dor et al.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 4,
      "context" : ", 2004], and implicitly in [Ben-Dor et al., 1999] as ‘Cluster Editing’.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "The problem is APX-hard even on complete graphs2 (when we are given the similarity information for every pair of objects) [Charikar et al., 2005].",
      "startOffset" : 122,
      "endOffset" : 145
    }, {
      "referenceID" : 16,
      "context" : ", 2006], for which the current state-ofthe-art algorithm gives a Θ(log n) factor approximation [Garg et al., 1993].",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "Instances of Correlation Clustering on complete graphs that satisfy the notion of approximation stability were considered in [Balcan and Braverman, 2009].",
      "startOffset" : 125,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : ", 2004, Ailon et al., 2008, Chawla et al., 2014]. Instances of Correlation Clustering on complete graphs that satisfy the notion of approximation stability were considered in [Balcan and Braverman, 2009]. To summarize, despite our best efforts, we only know logarithmic factor approximation algorithms for Correlation Clustering; moreover, we cannot get a constant factor approximation for worst-case instances if the Unique Games Conjecture is true. However, our primary interest in solving Correlation Clustering comes from its numerous applications, and the instances that we encounter in these applications are not worst-case instances. This motivates the study of the average-case complexity of the problem and raises the following question: Can we design algorithms with better provable guarantees for realistic average-case models of Correlation Clustering? Several natural average-case models of Correlation Clustering have been studied previously. Ben-Dor et al. [1999] consider a model in which we start with a ground-truth clustering – an arbitrary partitioning of the vertices – of a complete graph.",
      "startOffset" : 8,
      "endOffset" : 979
    }, {
      "referenceID" : 3,
      "context" : "In fact, this average-case model was also studied in the work [Bansal et al., 2004] that introduced the problem of Correlation Clustering.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "extended the model of [Ben-Dor et al., 1999] from complete graphs to sparser Erdos–Renyi random graphs.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "In their model, the underlying unlabeled graph G(V,E) comes from an Erdös–Renyi random graph (of edge probability p), and as in [Ben-Dor et al., 1999], the label of each edge is set (independently) to be consistent with the ground truth clustering with probability 1− ε and inconsistent with probability ε.",
      "startOffset" : 128,
      "endOffset" : 150
    }, {
      "referenceID" : 30,
      "context" : "Graphs that come up in computer vision applications are sparse with grid-like structure [Yarkony et al., 2012].",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "In fact, this average-case model was also studied in the work [Bansal et al., 2004] that introduced the problem of Correlation Clustering. Mathieu and Schudy consider a generalization of this model where there is an adversary: for each edge, we keep the initial label with probability (1 − ε), and we let the adversary decide whether to flip the edge label or not with probability ε. The major drawback of these models is that they only consider the case of complete graphs, i.e. they require that the Correlation Clustering instance contains similarity information for every pair of nodes. Chen et al. extended the model of [Ben-Dor et al., 1999] from complete graphs to sparser Erdos–Renyi random graphs. In their model, the underlying unlabeled graph G(V,E) comes from an Erdös–Renyi random graph (of edge probability p), and as in [Ben-Dor et al., 1999], the label of each edge is set (independently) to be consistent with the ground truth clustering with probability 1− ε and inconsistent with probability ε. While these average-case models are natural, they are unrealistic in practice since most real-world graphs are neither dense nor captured by Erdös–Renyi distributions. For instance, real-world graphs in community detection have many structural properties (presence of large cliques, large clustering coefficients, heavy-tailed degree distribution) that are not exhibited by graphs that are generated by Erdös–Renyi models [Newman et al., 2006, Kumar et al., 1999]. Graphs that come up in computer vision applications are sparse with grid-like structure [Yarkony et al., 2012]. Further, these models assume that every pair of vertices have the same amount of similarity or dissimilarity (all costs are unit). Our semi-random model tries to address these issues by assuming very little about the observations – the underlying unlabeled graph G(V,E) – and allowing non-uniform costs. 1.1 Our Semi-random Model In this paper, we propose and study a new semi-random model for generating general instances of Correlation Clustering, which we believe captures many properties of real world instances. It generalizes the model of Mathieu and Schudy [2010] to arbitrary graphs G(V,E, c) with costs.",
      "startOffset" : 63,
      "endOffset" : 2162
    }, {
      "referenceID" : 5,
      "context" : "Research on semi-random models was initiated by [Blum and Spencer, 1995], who introduced and investigated semi-random models for k-coloring.",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : ", 2012, 2014], Independent Set [Feige and Kilian, 1998], Maximum Clique [Feige and Krauthgamer, 2000], Unique Games [Kolla et al.",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : ", 2012, 2014], Independent Set [Feige and Kilian, 1998], Maximum Clique [Feige and Krauthgamer, 2000], Unique Games [Kolla et al.",
      "startOffset" : 72,
      "endOffset" : 101
    }, {
      "referenceID" : 20,
      "context" : ", 2012, 2014], Independent Set [Feige and Kilian, 1998], Maximum Clique [Feige and Krauthgamer, 2000], Unique Games [Kolla et al., 2011], and other problems.",
      "startOffset" : 116,
      "endOffset" : 136
    }, {
      "referenceID" : 23,
      "context" : "Most related to our work, both in the nature of the model and in the techniques used, is a recent result of [Makarychev et al., 2013] on semi-random instances of Minimum",
      "startOffset" : 108,
      "endOffset" : 133
    }, {
      "referenceID" : 23,
      "context" : "While the techniques used in both papers are conceptually similar, the semidefinite (SDP) relaxation for Correlation Clustering that we use in this paper is very different from the SDP relaxation for Minimum Feedback Arc Set used in [Makarychev et al., 2013].",
      "startOffset" : 233,
      "endOffset" : 258
    }, {
      "referenceID" : 25,
      "context" : "The two works that are most similar in the nature of guarantees are [Mathieu and Schudy, 2010] and [Chen et al.",
      "startOffset" : 68,
      "endOffset" : 94
    }, {
      "referenceID" : 9,
      "context" : "The two works that are most similar in the nature of guarantees are [Mathieu and Schudy, 2010] and [Chen et al., 2014].",
      "startOffset" : 99,
      "endOffset" : 118
    }, {
      "referenceID" : 9,
      "context" : "In the case of Erdös–Renyi graphs, our algorithms obtain similar guarantees for smaller values of k (the implicit dependence on k is a worse polynomial than in [Chen et al., 2014], however).",
      "startOffset" : 160,
      "endOffset" : 179
    }, {
      "referenceID" : 31,
      "context" : "We used the SDPNAL MATLAB library to solve the semidefinite programming (SDP) relaxation for the problem [Zhao et al., 2010].",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "We used the same values of n as were used in [Chen et al., 2014]; we chose values of p smaller than or close to the minimal values for which the algorithm of [Chen et al.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : ", 2014]; we chose values of p smaller than or close to the minimal values for which the algorithm of [Chen et al., 2014] works (Chen et al.",
      "startOffset" : 101,
      "endOffset" : 120
    }, {
      "referenceID" : 8,
      "context" : "Chen et al. [2014] extended the average-case model of Correlation Clustering to sparser Erdös–Renyi graphs.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "Chen et al. [2014] extended the average-case model of Correlation Clustering to sparser Erdös–Renyi graphs. Very recently, Globerson et al. [2014] considered a semi-random model for Correlation Clustering for recovery in grid graphs and planar graphs, and gave conditions for approximate recovery in terms of an expansion-related condition.",
      "startOffset" : 0,
      "endOffset" : 147
    }, {
      "referenceID" : 29,
      "context" : "We use a simple SDP relaxation for the problem [Swamy, 2004].",
      "startOffset" : 47,
      "endOffset" : 60
    }, {
      "referenceID" : 25,
      "context" : "We note that this relaxation is weaker than the SDP used in [Mathieu and Schudy, 2010] because it does not have l2-triangle inequalities constraints.",
      "startOffset" : 60,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "We then run the O(log n) worst-case approximation algorithm of [Charikar et al., 2005] or [Demaine et al.",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : ", 2005] or [Demaine et al., 2006] on the remaining graph to obtain a PTAS overall.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 9,
      "context" : "While our algorithm succeeds when the SDP is tight (as in [Chen et al., 2014]), the analysis of our algorithm also shows how to deal with nearly integral solutions, in which most inner products 〈ū, v̄〉 are only close to 0 or 1 (but may not be tight).",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "Let D = O(log n) be the approximation algorithms of Charikar et al. [2005] or Demaine et al.",
      "startOffset" : 52,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "Let D = O(log n) be the approximation algorithms of Charikar et al. [2005] or Demaine et al. [2006]. We apply Theorem 3.",
      "startOffset" : 52,
      "endOffset" : 100
    }, {
      "referenceID" : 22,
      "context" : "We use the following lemma from Makarychev et al. [2013]. Lemma 3.",
      "startOffset" : 32,
      "endOffset" : 57
    }, {
      "referenceID" : 15,
      "context" : "Now, by Freedman’s inequality (see Freedman [1975]),",
      "startOffset" : 8,
      "endOffset" : 51
    } ],
    "year" : 2015,
    "abstractText" : "In this paper, we propose and study a semi-random model for the Correlation Clustering problem on arbitrary graphsG. We give two approximation algorithms for Correlation Clustering instances from this model. The first algorithm finds a solution of value (1+δ) opt-cost+Oδ(n log 3 n) with high probability, where opt-cost is the value of the optimal solution (for every δ > 0). The second algorithm finds the ground truth clustering with an arbitrarily small classification error η (under some additional assumptions on the instance).",
    "creator" : "LaTeX with hyperref package"
  }
}