{
  "name" : "1307.7577.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Safe Screening With Variational Inequalities and Its Applicaiton to LASSO",
    "authors" : [ "Jun Liu", "Zheng Zhao", "Jie Wang", "Jieping Ye" ],
    "emails" : [ "jun.liu@sas.com", "zheng.zhao@sas.com", "jie.wang.ustc@asu.edu", "jieping.ye@asu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Sparse learning approaches usually obtain a model with only a few non-zero features and they are able to achieve an interpretable model with high prediction accuracy. Taking advantage of the nature of sparsity, screening techniques have been proposed for accelerating the computation. The strong rules proposed in [2] works very well in practice although they might mistakenly discard active features. To remedy this, safe screening techniques [1, 3, 4], which elliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution, have been proposed. These approaches studied the dual problem and bounded a functional of the dual variable at the target regularization parameter based on an existing solution.\nar X\niv :1\n30 7.\n75 77\nv1 [\ncs .L\nG ]\nIn this note, we propose an approach called Sasvi (Safe screening with variational inequalities) and take LASSO for example in the analysis. Sasvi makes use of the variational inequalities which provide the sufficient and necessary optimality conditions for the dual problem. The existing approaches can be casted as relaxed versions of the proposed Sasvi, and thus Sasvi provides a much stronger screening rule. The monotone properties of Sasvi are studied based on which a sure removal regularization parameter can be identified for each feature. In adddition, the proposed Sasvi is readily extended for solving the generalized sparse linear models."
    }, {
      "heading" : "2 Safe Screening with Variational Inequalities",
      "text" : "For simplicity of discussion, we take LASSO as an example, and discuss the extension to the generalized linear models such as logistic regression in Section 3.\nFor any λ > 0, the dual of the LASSO problem can be derived as follows:\nminβ\n{ 1\n2 ‖Xβ − y||22 + λ‖β‖1\n} (1)\n= minβ maxθ\n{ 〈y −Xβ, λθ〉 − 1\n2 ‖λθ||22 + λ‖β‖1\n} (2)\n= maxθ minβ λ\n{ 〈y,θ〉 − λ\n2 ‖θ||22 − 〈XTθ,β〉+ ‖β‖1\n} (3)\n= maxθ:‖XT θ‖∞≤1 λ 2\n{ −1\n2 ‖θ − y λ ||22 + 1 2 ‖y λ ||22 } . (4)\nEq. (1) is the (primal) Lasso problem. A dual varialbe θ is introduced in (2). It is obvious that the optimal primal variable (β∗) and the optimal dual variable (θ∗) satisfy\nλθ∗ = y −Xβ∗. (5)\nBy exchanging the min and max in Eq. (2), we obtain Eq. (3). Eq. (4) is obtained using the following simple fact\nmin β {−βb+ |β|} =\n{ 0, |b| ≤ 1\n−∞, |b| > 1 (6)\nIn addition, an analysis of Eq. (6) shows that\n|(XTθ∗)i| < 1⇒ β∗i = 0, (7)\nwhere (XTθ∗)i denotes the i-th component of X Tθ∗.\nThe dual problem in Eq. (4) can be rewritten as\nmin θ:‖XT θ‖∞≤1\n1 2 ‖θ − y λ ||22. (8)\nLet ‖XTy‖∞ ≥ λ1 > λ2 > 0 be two distinct regularization parameters. Let β∗1 and β ∗ 2 be the optimal primal variables corresponding to λ1 and λ2, respectively. Let θ∗1 and θ ∗ 2 be the optimal dual variables corresponding to λ1 and λ2, respectively. Given λ1, θ ∗ 1 and λ2, we aim at bounding |xTj θ∗2 | without the actual computation of θ∗2 , where xj denotes the j-th column of X. According to Eq. (7), if |xTj θ∗2 | < 1, then the j-th feature is absent for computing β∗2 ."
    }, {
      "heading" : "2.1 Variational Inequality",
      "text" : "Lemma 1 For the constrained convex optimzation problem:\nmin x∈G f(x), (9)\nwith G beging convex and closed and f(·) being convex and differentiable, x∗ is an optimal solution if and only if\n〈f ′(x∗),x− x∗〉 ≥ 0,∀x ∈ G. (10)\nEq. (10) is the so-called variation inequality for the problem in Eq. (9). Applying Lemma 1, we can represent the optimality conditions for θ∗1 and θ ∗ 2 as the following variational inequalities:\n〈θ∗1 − y\nλ1 ,θ − θ∗1〉 ≥ 0,∀θ : ‖XTθ‖∞ ≤ 1, (11)\n〈θ∗2 − y\nλ2 ,θ − θ∗2〉 ≥ 0,∀θ : ‖XTθ‖∞ ≤ 1. (12)\nPlugging θ = θ∗2 and θ = θ ∗ 1 into Eq. (11) and Eq. (12) respectively, we have\n〈θ∗1 − y\nλ1 ,θ∗2 − θ∗1〉 ≥ 0, (13)\n〈θ∗2 − y\nλ2 ,θ∗1 − θ∗2〉 ≥ 0. (14)\nFor Eq. (12), besides θ = θ∗1 , one may make use of θ = −θ∗1 and θ = ± y\nλmax ,\nwhich are also feasible for the constraints. In addition, if one has multiple existing solutions, one might combine them to form tighter constraints for θ∗2 . However, these are not pursued in this note."
    }, {
      "heading" : "2.2 The Proposed Sasvi",
      "text" : "In the proposed safe screening with variational inequalities (Sasvi), we bound |xTj θ∗2 | by\nmax θ:〈θ∗1− y λ1 ,θ−θ∗1 〉≥0,〈θ− y λ2 ,θ∗1−θ〉≥0\n|〈xj ,θ〉|. (15)\nNote that, θ∗2 belongs to the constraints defined in Eq. 15, and the maximum obtained by Eq. 15 provides an upper-bound for |xTj θ∗2 |.\nThe key of Sasvi is to make use of Eq. (13) and Eq. (14) which are the instantiations of the variational inequalities in Eq. (11) and Eq. (12) for bounding θ∗2 . Next, we discuss how to solve Eq. (15).\nDenote\nθ = θ∗1 + y λ2\n2 + r,a = (\ny\nλ1 − θ∗1)/2,b = (\ny\nλ2 − θ∗1)/2. (16)\nFigure 1 illustrates a and b by lines EB and EC, respectively. The following theorem shows that the angle between a and b is acute.\nTheorem 1 Let y 6= 0, and ‖XTy‖∞ ≥ λ1 > λ2 > 0. We have\nb 6= 0, 〈b,a〉 ≥ 0, (17)\nand the equality holds if and only if λ1 = ‖XTy‖∞. In addition, if λ1 < ‖XTy‖∞ then a 6= 0.\nWith the notations in Eq. (16), Eq. (15) can be rewritten as\nmax r\n|〈xj , θ∗1 + y λ2\n2 〉+ 〈xj , r〉|\nsubject to 〈a, r + b〉 ≤ 0, ‖r‖22 ≤ ‖b‖22. (18)\nThe objective function of Eq. 18 can be represented in the following equivalent form:\nmax ( 〈xj , θ∗1 + y λ2\n2 〉 − 〈−xj , r〉,−〈xj ,\nθ∗1 + y λ2\n2 〉 − 〈xj , r〉\n) , (19)\nwhich indicates that Eq. (18) can be computed by maximizing −〈−xj , r〉 and −〈xj , r〉 over the constraints in the same equation. Maximizing −〈−xj , r〉 and −〈xj , r〉 can be computed by minimzing 〈−xj , r〉 and 〈xj , r〉, and we consider to solve the following minimization problem\nmin r\n〈x, r〉\nsubject to 〈a, r + b〉 ≤ 0, ‖r‖22 ≤ ‖b‖22. (20)\nNext, we show that Eq. (20) admits a closed form solution. If λ1 = ‖XTy‖∞, then a = 0 and the constraint 〈a, r + b〉 ≤ 0 of Eq. (20) can be removed. It is easy to get that r = −x‖b‖2‖x‖2 minimizes Eq. (20) with the minimum function value being −‖x‖2‖b‖2. In our following discussion, we focus on 0 < λ1 < ‖XTy‖∞ and thus a 6= 0 according to Theorem 1. We define\nx⊥ = x− 〈x,a〉 ‖a‖22 a (21)\ny⊥ = x− 〈y,a〉 ‖a‖22 a (22)\n2 , the middle point of E and C. The dashed\nline is θ : 〈θ∗1 − y λ1 ,θ − θ∗1〉 = 0. EX1, EX2, EX3 and EX4 denote ±xj in two subcases: the angle between EB and EX1 (EX4) is larger than the angle between EB and EC, and the angle between EB and EX2 (EX3) is smaller than the angle between EB and EC. R2 and R3 are the maximizers to Eq. (15) with EX2 and EX3 denoting ±xj , and the maximizer to Eq. (15) for EX1 (EX4) denoting ±xj is on the intersection between the dashed line and the ball centered at D with radius ED.\nwhich are the orthonormal projections of x and y onto the null space of a, respectively.\nThe Lagraingian of Eq. (20) can be written as\nL(r, α, β) = 〈x, r〉+ α〈a, r + b〉+ β 2 (‖r‖22 − ‖b‖22), (23)\nwhere α, β ≥ 0 are introduced for the two inequalities, respectively. It is clear that the minimal value of Eq. (20) is lower bounded (the minimum is no less than −‖b‖2‖x‖2 by only considering the constraint ‖r‖22 ≤ ‖b‖22). Therefore, the optimal dual variable β is always positive; otherwise, minimizing Eq. (23) with regard to r achieves −∞.\nSetting the derivative with regard to r to zero, we have\nr = −x− αa\nβ . (24)\nPlugging Eq. (24) into Eq. (23), we obtain the dual problem of Eq. (20):\nmax α,β α〈a,b〉 − 1 2β ‖x + αa‖22 − β 2 ‖b‖22\nsubject to α ≥ 0, β ≥ 0. (25)\nFor given β, we have\nα = max( β〈a,b〉 − 〈x,a〉\n‖a‖22 , 0). (26)\nWe consider two cases. In the first case, we assume that α = 0. We have\nr = −x β , β ≤ 〈x,a〉 〈a,b〉 . (27)\nMaking use of the complementary slackness condition (note the optimal β does not equal to zero), we have\n‖r‖2 = ‖ −x β ‖2 = ‖b‖2. (28)\nThus, we have\nβ = ‖x‖2 ‖b‖2 . (29)\nIncorporating Eq. (27) and Eq. (29), we have\n〈b,a〉 ‖b‖2‖a‖2 ≤ 〈x,a〉 ‖x‖2‖a‖2 , (30)\nso that the angle between a and b is equal to or larger than the angle between x and a. Note that 〈b,a〉 ≥ 0 according to Theorem 1. For an illustration of\na and b, please refer to EB and EC in Figure 1, respectively. For x satisfying Eq. 30, please see EX2 and EX3 in the same figure. In addition, we have\n〈x, r〉 = −‖x‖2‖b‖2. (31)\nIn the second case, Eq. (30) does not hold. We have\nα = β〈a,b〉 − 〈x,a〉\n‖a‖22 . (32)\nPlugging Eq. (32) into Eq. (24), we have\nr = −x‖a‖ 2 2 + β〈a,b〉a− 〈x,a〉a\nβ‖a‖22 (33)\nSince ‖r‖22 = ‖b‖22 (note that the optimal β does not equal to zero), we have\nβ = √ ‖x‖22‖a‖22 − 〈x,a〉2 ‖y‖22‖a‖22 − 〈y,a〉2 = ‖x⊥‖2 ‖b⊥‖2 . (34)\nIn addition, we have\n〈x, r〉 = −‖x⊥‖2‖y⊥‖2 − 〈a,b〉〈x,a〉 ‖a‖22 . (35)\nThe main result is given in the following theorem.\nTheorem 2 Let y 6= 0, and ‖XTy‖∞ ≥ λ1 > λ2 > 0. If a 6= 0, we denote\nx⊥j = xj − 〈xj ,a〉a ‖a‖22 . (36)\n1. If 〈b,a〉‖b‖2‖a‖2 > |〈xj ,a〉| ‖xj‖2‖a‖2 then\n〈xj ,θ∗2〉 ≤ 〈xj ,θ∗1〉+ 1 λ2 − 1λ1 2 [‖x⊥j ‖2‖y⊥‖2 + 〈x⊥j ,y⊥〉] (37)\nand\n− 〈xj ,θ∗2〉 ≤ −〈xj ,θ∗1〉+ 1 λ2 − 1λ1 2 [‖x⊥j ‖2‖y⊥‖2 − 〈x⊥j ,y⊥〉] (38)\n2. If 〈xj ,a〉 > 0 and 〈b,a〉‖b‖2‖a‖2 ≤ 〈xj ,a〉 ‖xj‖2‖a‖2 , then 〈xj ,θ ∗ 2〉 satisfies Eq. (37),\nand − 〈xj ,θ∗2〉 ≤ −〈xj ,θ∗1〉 − 〈xj ,b〉+ ‖xj‖2‖b‖2. (39)\n3 . If 〈xj ,a〉 < 0 and 〈b,a〉‖b‖2‖a‖2 ≤ −〈xj ,a〉 ‖xj‖2‖a‖2 , then\n〈xj ,θ∗2〉 ≤ 〈xj ,θ∗1〉+ 〈xj ,b〉+ ‖xj‖2‖b‖2. (40)\nand −〈xj ,θ∗2〉 satisfies Eq. (38).\n4. If a = 0, then Eq. (39) and Eq. (40) holds.\nAn illustration of Theorem 2 for different cases can be found in Fig. 1. For the first case, the feature xj has a larger angle with a = Xβ∗1λ1\n2 compared to the one between a and b, and the upper-bound of |xTj θ∗2 | is the sum of 1) |xTj θ∗1 |, the inner product between xj and the dual solution at the known regualarization parameter λ1 and 2) 1 λ2 − 1λ1 2 [‖x ⊥ j ‖2‖y⊥‖2±〈x⊥j ,y⊥〉], a quantity that is computed based on the orthonormal projections of x and y onto the null space of a, respectively.\nFor the second and the third cases, the feature xj has a larger angle with\na = Xβ∗1λ1\n2 compared to the one between a and b, and the upper-bound of |xTj θ∗2 | is the sum of 1) |xTj θ∗1 |, the inner product between xj and the dual solution at the known regualarization parameter λ1 and 2) 〈xj ,b〉+‖xj‖2‖b‖2.\nFor the fourth case with λ1 = ‖XTy‖∞, the primal variable β∗1 is a zero vector, and θ∗1 = y λ1\n. This provides the screening rule at the maximal reugularization parameter."
    }, {
      "heading" : "2.3 Relationship to Existing Approaches",
      "text" : "Figure 2 compares the Sasvi approach with the SAFE approach [1] and the DPP approach [3]. It can be observed that the constraint used by Sasvi has much smaller volume compared to both SAFE and DPP, and thus leads to tighter bound for |〈xj ,θ∗2〉|. Next, we show that both SAFE and DPP indeed can be derived from the variational inequalities in Eq. (11) and Eq. (12) followed by relaxations.\nThe SAFE approach starts with θ∗1 , the optimal solution to the dual problem in Eq. (8). Denote G(θ) = 12‖y|| 2 2 − 12‖λ2θ − y|| 2 2. The SAFE approach makes use of the so-called “dual” scaling, and compute the upper-bound of the G(θ) for 0 < λ2 < λ1 as\nγ(λ2) = max s:|s|≤1 G(sθ) = max s:|s|≤1\n1 2 ‖y||22 − 1 2 ‖sλ2θ∗1 − y||22. (41)\nNote that, compared to the SAFE paper, the dual variable θ has been scaled in the formulation in Eq. (41), while the results reproduced here are identical to that of the SAFE paper. Denote s∗ as the optimal solution. Solving Eq. (41), we have s∗ = max(min( 〈θ∗1 ,y〉 λ2‖θ∗1‖2\n, 1),−1) when θ1 6= 0. Applying Lemma 3 in the appendix, we have 〈θ∗1 ,y〉 ≥ λ1‖θ∗1‖2 > λ1‖θ∗1‖2. Thus, we have s∗ = 1. The SAFE approach utilizes the following equation to compute the upper-bound for |〈xj ,θ∗2〉| as\n|〈xj ,θ∗2〉| ≤ max θ:G(θ)≥γ(λ2) |〈xj ,θ〉|\n= max θ:‖θ− yλ2 ||2≤‖s ∗θ∗1− y λ2 ||2 |〈xj ,θ〉| = |〈xj ,y〉| λ2 + ‖xj‖2 ∥∥∥∥s∗θ∗1 − yλ2 ∥∥∥∥ 2 .\n(42)\nIf the bound on the right hand side of Eq. (42) is less than 1, then the j-th feature can be elliminated for λ2. Utilizing ‖XTθ∗1‖∞ ≤ 1 and |s∗| ≤ 1, we can set θ = s∗θ∗1 in Eq. (12) and obtain\n〈θ∗2 − y\nλ2 , s∗θ∗1 − θ∗2〉 ≥ 0, (43)\nwhich leads to\n〈θ∗2− y\nλ2 ,θ∗2−\ny\nλ2 〉−〈θ∗2−\ny\nλ2 , s∗θ∗1−\ny\nλ2 〉 = 〈θ∗2−\ny\nλ2 ,θ∗2−\ny\nλ2 +\ny\nλ2 −s∗θ∗1〉 ≤ 0.\n(44) Making use of 〈θ∗2 − y λ2 , s∗θ∗1 − y λ2 〉 ≤ ‖θ∗2 − y λ2 ‖2‖s∗θ∗1 − y λ2 ‖2, we can obtain\n‖θ∗2 − y\nλ2 ‖2 ≤ ‖s∗θ∗1 −\ny\nλ2 ‖2, (45)\nwhich is the constraint used in Eq. (42). Therefore, the SAFE approach can be treated as generating the constraint for θ∗2 using the variational inequality in Eq. (12) followed by relaxations. Note that, the ball specified by Eq. (44) has half radius of the one by Eq. (45).\nAdding Eq. (13) and Eq. (14), we have\n〈 y λ2 − y λ1 ,θ∗2 − θ∗1〉 ≥ 〈θ∗2 − θ∗1 ,θ∗2 − θ∗1〉. (46)\nIncorporating 〈 yλ2 − y λ1 ,θ∗2 − θ∗1〉 ≤ ‖ y λ2 − yλ1 ‖2‖θ ∗ 2 − θ∗1‖2, we have∥∥∥∥ yλ2 − yλ1 ∥∥∥∥ 2 ≥ ‖θ∗2 − θ∗1‖2 , (47)\nwhich is the one used in the DPP approach [3]. Therefore, although the authors in [3] motivates the DPP approach from the viewpoint of Euclidean projection, the DPP approach can indeed be treated as generating the constraint for θ∗2 using the variational inequality in Eq. (11) and Eq. (12) followed by relaxation. Note that, the ball specified by Eq. (46) has half radius of the one by Eq. (47)."
    }, {
      "heading" : "2.4 Feature Sure Removal Parameter",
      "text" : "We begin with the study on the monotone properties of the upper-bounds established in Theorem 2.\nFor the first case, the upper-bound of |xTj θ∗2 | is monotonely decreasing with regard to λ2 as\n‖x⊥j ‖2‖y⊥‖2 + 〈x⊥j ,y⊥〉\nand ‖x⊥j ‖2‖y⊥‖2 − 〈x⊥j ,y⊥〉\nare non-negative and independent of λ2.\nFor the fourth case, we have θ∗1 = y λ1 since a = 0. Thus b = ( yλ2 − θ ∗ 1)/2 =\n1 λ2 − 1λ2 2 y. As a result,\n−〈xj ,b〉+ ‖xj‖2‖b‖2 = 1 λ2 − 1λ1 2 [−〈xj ,y〉+ ‖xj‖2‖y‖2]\n〈xj ,b〉+ ‖xj‖2‖b‖2 = 1 λ2 − 1λ1 2 [〈xj ,y〉+ ‖xj‖2‖y‖2]\nare monotonely decreasing with regard to λ2. For the second and the third cases, the vector b is dependent on λ2 and the upper-bounds are not necessarily monotone with regard to λ2. This indicates that if λ12 > λ 2 2 the obtained upper-bound for λ 1 2 might be less than that for λ22. This somehow coincides with a property of the solution of LASSO, i.e., a feature that is absent for a given regularization parameter might be active for a smaller regularization parameter. Our following analysis focuses on the monotone properties of the established upper-bounds.\nLemma 2 Let y 6= 0, and ‖XTy‖∞ > λ1 ≥ λ > 0. Denote\nf(λ) = 〈(yλ − θ ∗ 1)/2,a〉\n‖(yλ − θ ∗ 1)/2‖2‖a‖2\n(48)\ng(λ) = 〈(yλ − θ ∗ 1)/2,y〉\n‖(yλ − θ ∗ 1)/2‖2‖y‖2\n(49)\nf(λ) is strictly increasing with regard to λ in (0, λ1]. g(λ) is strictly decreasing with regard to λ in (0, λ1].\nThe main results are provided in the following theorem.\nTheorem 3 Let y 6= 0, and ‖XTy‖∞ ≥ λ1 > λ2 > 0. Define λ2a as\n• If λ1 = ‖XTy‖∞ or if 0 < λ1 < ‖XTy‖∞ and 〈y,a〉‖y‖2‖a‖2 ≥ |〈xj ,a〉| ‖xj‖2‖a‖2 , then\nlet λ2a = 0.\n• If 0 < λ1 < ‖XTy‖∞ and 〈y,a〉‖y‖2‖a‖2 < |〈xj ,a〉| ‖xj‖2‖a‖2 , then let λ2a be the unique\nvalue in (0, λ1] that satisfies f(λ2a) = |〈xj ,a〉| ‖xj‖2‖a‖2 .\nDefine λ2y as\n• If λ1 = ‖XTy‖∞ or if 0 < λ1 < ‖XTy‖∞ and 〈y,a〉‖y‖2‖a‖2 ≥ |〈xj ,y〉| ‖xj‖2‖a‖2 , then\nlet λ2y = λ1.\n• If 0 < λ1 < ‖XTy‖∞ and 〈y,a〉‖y‖2‖a‖2 < |〈xj ,y〉| ‖xj‖2‖a‖2 , then let λ2y be the unique\nvalue in (0, λ1] that satisfies g(λ2y) = |〈xj ,y〉| ‖xj‖2‖y‖2 .\nWe have the following monotone properties on the upper-bounds:\n1. If 〈xj ,a〉 > 0 or if 〈xj ,a〉 ≤ 0 and λ2a ≤ λ2y, then the established upperbound of 〈xj ,θ∗2〉 is monotonically decreasing with regard to λ2 in (0, λ1).\n2. If 〈xj ,a〉 < 0 or if 〈xj ,a〉 ≥ 0 and λ2a ≤ λ2y, then the established upperbound of −〈xj ,θ∗2〉 is monotonically decreasing with regard to λ2 in (0, λ1).\n3. If 〈xj ,a〉 ≤ 0 and λ2a > λ2y, then the established upper-bound of 〈xj ,θ∗2〉 is monotonically decreasing with regard to λ2 in (0, λ2y) and [λ2a, λ1), but monotonically increasing with regard to λ2 in (λ2y, λ2a).\n4. If 〈xj ,a〉 ≥ 0 and λ2a > λ2y, then the established upper-bound of −〈xj ,θ∗2〉 is monotonically decreasing with regard to λ2 in (0, λ2y) and [λ2a, λ1), but monotonically increasing with regard to λ2 in (λ2y, λ2a).\nFrom Theorem 3, one can identify the regularization parameter at which a feature is surely removed from the solution."
    }, {
      "heading" : "3 Extension to Logistic Regression",
      "text" : "The sparse logistic regression can be written as:\nmin β m∑ i=1 log(1 + exp(−yiβTxi)) + λ‖β‖1. (50)\nLet ui = β Txi, we have\nmin β,u:ui=βTxi m∑ i=1 log(1 + exp(−yiui)) + λ‖β‖1. (51)\nIntroducing the dual variable z for the equality constraints ui = β Txi, i =\n1, 2, . . . ,m, we have\nmax z min β,u m∑ i=1 log(1 + exp(−yiui)) + λ‖β‖1 + m∑ i=1 zi(ui − βTxi)\n= max z min u min β { m∑ i=1 log(1 + exp(−yiui)) + m∑ i=1 ziui + λ‖β‖1 − βT m∑ i=1 zixi }\n= max z min u { m∑ i=1 log(1 + exp(−yiui)) + m∑ i=1 ziui + min β (λ‖β‖1 − βT m∑ i=1 zixi) }\n= max z:‖ ∑ i zixi‖∞≤λ min u m∑ i=1 log(1 + exp(−yiui)) + m∑ i=1 ziui\n= max z:‖ ∑ i zixi‖∞≤λ m∑ i=1 ( log( yi yi − zi ) + zi yi log( yi − zi zi ) ) .\n(52)\nLet θi = zi λ . The dual problem of Eq. (50) can be written as\nmin θ:‖XT θ‖∞≤1 − m∑ i=1 ( log( yi/λ yi/λ− θi ) + θi yi/λ log( yi/λ− θi θi ) ) (53)\nLet\nf(θ; y\nλ ) = − m∑ i=1 ( log( yi/λ yi/λ− θi ) + θi yi/λ log( yi/λ− θi θi ) ) (54)\nIt can be easily verified that the function f(θ; yλ ) is convex with regard to θ. Let β∗ and θ∗ be the primal and dual optimals, respectively. If |(XTθ∗)i| < 1 then β∗i = 0.\nThe derivative with regard to θ can be computed as\n∂f(θ; yλ )\n∂θi = − 1 yi/λ log( yi/λ− θi θi ) (55)\nAccording to Lemma 1, the optimality condition via the variational inequality is:\nm∑ i=1 1 yi/λ log( yi/λ− θ∗i θ∗i )(θi − θ∗i ) ≤ 0,∀θ : ‖XTθ‖∞ ≤ 1. (56)\nLet θ∗1 : θ ∗ i,1, i = 1, 2, . . . ,m be the optimal solution corresponding to λ1.\nThe variational inequality that ensures the optimality of θ∗1 is:\nm∑ i=1 1 yi/λ1 log( yi/λ1 − θ∗i,1 θ∗i,1 )(θi − θ∗i,1) ≤ 0,∀θ : ‖XTθ‖∞ ≤ 1. (57)\nLet θ∗2 : θ ∗ i,2, i = 1, 2, . . . ,m be the optimal solution corresponding to λ2.\nThe variational inequality that ensures the optimality of θ∗2 is:\nm∑ i=1 1 yi/λ2 log( yi/λ2 − θ∗i,2 θ∗i,2 )(θi − θ∗i,2) ≤ 0,∀θ : ‖XTθ‖∞ ≤ 1. (58)\nOne can follow the discussion for the LASSO approach in estimating the upper-bound of 〈xj ,θ∗2〉 by\nmax θ |〈xj ,θ〉|\nsubjet to m∑ i=1 1 yi/λ1 log( yi/λ1 − θ∗i,1 θ∗i,1 )(θi − θ∗i,1) ≤ 0\nm∑ i=1 1 yi/λ2 log( yi/λ2 − θi θi )(θ∗i,1 − θi) ≤ 0.\n(59)\nIn Eq. (59), the first constraint is a half space, and the second constraint corresponds to a closed convex set. For sake of computational convenience, one may relax the second inequality, and we leave the derivation of the screening rule based on Eq. (59) to a future work."
    }, {
      "heading" : "4 Experiment",
      "text" : "In this section, we conduct experiments to evaluate the performance of the proposed screening method, that is, Sasvi, on synthetic data sets. We compare the performance of Sasvi with the sequential SAFE rule [1] and the sequential DPP [3] which achieve the state-of-the-art performance for the Lasso problems. Notice that, all of the three methods are “safe” in the sense that the discarded features are guaranteed to have 0 coefficients in the true solution.\nTo measure the performance of the screening methods, we use the rejection ratio, i.e., the ratio between the number of features discarded by the screening methods and the number of features with 0 coefficients in the true solution. We report the rejection ratio of Sasvi, SAFE and DPP along a sequence of 91 tuning parameter values equally spaced on the scale of λ/λmax from 1.0 to 0.1. The general experiment settings are as follows. The entries of the response vector y and the data matrix X are generated i.i.d. from a standard Gaussian distribution, and the correlation between y and each feature xk is uniformly distributed on the interval [−c, c], where c ∈ {0.3, 0.5, 0.8, 1.0}. To reduce the variability, we repeat the computation 20 times and report the average results for each experiment.\nFrom the practical point of view, people are usually more interested in the computational gain resulting from the new methods, i.e., how much speedup can we get via the new methods? To answer this question, we first apply the solver (SLEP1) with wart-start to solve the Lasso problem along the given 91 parameters and report the total running time. Then, for each parameter, we first use the screening methods to identify the inactive features and then apply the solver to solve the Lasso problem with the reduced data matrix. We repeat this process until all of the problems are solved and report the total running time (including screening). To demonstrate the efficiency, we also report the total running time of of the screening methods in discarding the inactive features for the Lasso problem with the given sequence of parameters.\nFigure 3 presents the rejection ratio of Sasvi, SAFE and DPP along the 91 parameters. We can see that the three screening methods are able to discard more inactive features when the value of c increases, i.e., the features become more correlated with the response vector. In all of the cases, Sasvi significantly outperforms SAFE and DPP in discarding inactive features for the Lasso problem.\nTable 1 demonstrates that Sasvi leads to the most computational gain among DPP and SAFE. The speedup of SLEP+Sasvi is approximately 5, 8, 12, 20 times compared with that of SLEP. Although Sasvi is the most expensive screening methods among DPP and SAFE, it is able to discard far more inactive features, leading to a greatly reduced data matrix which needs to be entered the optimization. As a result, Sasvi is the most effective screening method among DPP and SAFE in accelerating the computation.\n1http://www.public.asu.edu/~jye02/Software/SLEP/"
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this note, we propose an approach called Sasvi (Safe screening with variational inequalities). Sasvi makes use of the variational inequalities which provide the sufficient and necessary optimality conditions for the dual problem. The existing approaches can be casted as relaxed versions of the proposed Sasvi, and thus Sasvi provides a much stronger screening rule. The monotone properties of Sasvi are studied based on a sure removal regularization parameter which can be identified for each feature. In adddition, the proposed Sasvi is readily extended for solving the generalized sparse linear models."
    }, {
      "heading" : "6 Appendix",
      "text" : ""
    }, {
      "heading" : "6.1 Proof of Theorem 1",
      "text" : "We begin with two technical lemmas.\nLemma 3 Let y 6= 0. If 0 < λ1 ≤ ‖XTy‖∞, we have\n〈 y λ1 − θ∗1 ,θ∗1〉 ≥ 0. (60)\nProof Since the Euclidean projection of yλ1 onto {θ : ‖X Tθ‖∞ ≤ 1} is θ∗1 , it follows from Lemma 1 that\n〈θ∗1 − y\nλ1 ,θ − θ∗1〉 ≥ 0,∀θ : ‖XTθ‖∞ ≤ 1. (61)\nAs 0 ∈ {θ : ‖XTθ‖∞ ≤ 1}, we have Eq. (60).\nLemma 4 Let y 6= 0. If 0 < λ1 ≤ ‖XTy‖∞, we have\n〈 y λ1 − θ∗1 ,y〉 ≥ 0, (62)\nwhere the equality holds if and only if λ1 = ‖XTy‖∞.\nProof We have\n〈 y λ1 − θ∗1 , y λ1 〉 − 〈 y λ1 − θ∗1 ,θ∗1〉 = 〈 y λ1 − θ∗1 , y λ1 − θ∗1〉 ≥ 0, (63)\nwhere the equality holds if and only if yλ1 = θ ∗ 1 . Incorporating Eq. (60) in Lemma 3 and Eq. (63), we have Eq. (62). The equality holds if and only if y λ1\n= θ∗1 or equivlently λ1 = |XTy‖∞. Now, we are ready to prove Theorem 1. If follows from Eq. (16) and Eq. (62)\n〈b,a〉 = 1 λ2 − 1λ1 2 〈( y λ1 − θ∗1)/2,y〉+ ‖( y λ1 − θ∗1)/2‖2 ≥ 0. (64)\nIt is clear that the equality holds if and only if yλ1 = θ ∗ 1 or equivlently λ1 = |XTy‖∞. In other words, if λ1 < |XTy‖∞ then 〈b,a〉 > 0, which leads to b 6= 0 and a 6= 0."
    }, {
      "heading" : "6.2 Proof of Theorem 2",
      "text" : "We prove the four cases one by one as follows.\nIf 〈b,a〉‖b‖2 > |〈xj ,a〉| ‖xj‖2 , i.e., Eq. (30) does not hold with x = ±xj , we have b 6= 0\nand a 6= 0, and\n〈xj ,θ∗2〉 ≤ max θ:〈θ∗1− y λ1 ,θ−θ∗1 〉≥0,〈θ− y λ2 ,θ∗1−θ〉≥0 〈xj ,θ〉\n= max r:〈a,r+b〉≤0,‖r‖22≤‖b‖22\n〈xj , θ∗1 + y λ2\n2 〉+ 〈xj , r〉\n= 〈xj , θ∗1 + y λ2\n2 〉+ max r:〈a,r+b〉≤0,‖r‖22≤‖b‖22 〈xj , r〉\n= 〈xj , θ∗1 + y λ2\n2 〉 − min r:〈a,r+b〉≤0,‖r‖22≤‖b‖22 〈−xj , r〉\n≤ 〈xj ,a〉+ 〈xj ,θ∗1〉+ 1 λ2 − 1λ1 2 〈xj ,y〉\n+ √ (‖ − xj‖22 −\n〈−xj ,a〉2 ‖a‖22 )(‖b‖22 − 〈b,a〉2 ‖a‖22 ) + 〈a,b〉〈−xj ,a〉 ‖a‖22\n= 〈xj ,θ∗1〉+ 1 λ2 − 1λ1 2 [〈xj ,y〉 − 〈a,y〉 ‖a‖22 〈xj ,a〉] + 1 λ2 − 1λ1 2 √ (‖xj‖22 − 〈xj ,a〉2 ‖a‖22 )(‖y‖22 − 〈y,a〉2 ‖a‖22 ).\n(65)\nThe first inequality follows from the fact that θ∗2 satisfies the constraints in Eq. (15). The first equality follows from rewriting Eq. (15) as Eq. (18). The last inequality utilies Eq. (35) which is the result for the case 〈b,a〉‖b‖2 > |〈xj ,a〉| ‖xj‖2 > 〈−xj ,a〉 ‖xj‖2 by setting x = −xj . The last equality utlizes\n‖b‖22 − 〈b,a〉2\n‖a‖22 = ( 1 λ2 − 1λ1 2 )2(‖y‖22 − 〈y,a〉2 ‖a‖22 ) (66)\nand\n〈a,b〉〈xj ,a〉 ‖a‖22 = 〈xj ,a〉(1 + 〈a,y〉\n1 λ2 − 1λ1 2\n‖a‖22 ), (67)\nwhich can be derived from Eq. (16). Following a similar derivation, we have\n〈−xj ,θ∗2〉 ≤ max θ:〈θ∗1− y λ1 ,θ−θ∗1 〉≥0,〈θ− y λ2 ,θ∗1−θ〉≥0 〈−xj ,θ〉\n= max r:〈a,r+b〉≤0,‖r‖22≤‖b‖22\n〈−xj , θ∗1 + y λ2\n2 〉+ 〈−xj , r〉\n= 〈−xj , θ∗1 + y λ2\n2 〉+ max r:〈a,r+b〉≤0,‖r‖22≤‖b‖22 〈−xj , r〉\n= 〈−xj , θ∗1 + y λ2\n2 〉 − min r:〈a,r+b〉≤0,‖r‖22≤‖b‖22 〈xj , r〉\n≤ 〈−xj ,a〉+ 〈−xj ,θ∗1〉+ 1 λ2 − 1λ1 2 〈−xj ,y〉\n+ √ (‖xj‖22 −\n〈xj ,a〉2 ‖a‖22 )(‖b‖22 − 〈b,a〉2 ‖a‖22 ) + 〈a,b〉〈xj ,a〉 ‖a‖22\n= −〈xj ,θ∗1〉 − 1 λ2 − 1λ1 2 [〈xj ,y〉 − 〈a,y〉 ‖a‖22 〈xj ,a〉] + 1 λ2 − 1λ1 2 √ (‖xj‖22 − 〈xj ,a〉2 ‖a‖22 )(‖y‖22 − 〈y,a〉2 ‖a‖22 ).\n(68)\nThe last inequality utilies Eq. (35) which is the result for the case 〈b,a〉‖b‖2 > |〈xj ,a〉| ‖xj‖2 > 〈xj ,a〉 ‖xj‖2 by setting x = xj .\nIf 〈b,a〉‖b‖2 ≤ 〈xj ,a〉 ‖xj‖2 and 〈xj ,a〉 > 0, we have 〈b,a〉 ‖b‖2 > 〈−xj ,a〉 ‖xj‖2 since 〈b,a〉 ≥ 0 according to Theorem 1. Thus, Eq. (30) does not hold with x = −xj , and we can get Eq. (65) for bounding 〈xj ,θ∗2〉. In addition, Eq. (30) holds with x = xj , and we have\n〈−xj ,θ∗2〉 ≤ max θ:〈θ∗1− y λ1 ,θ−θ∗1 〉≥0,〈θ− y λ2 ,θ∗1−θ〉≥0 〈−xj ,θ〉\n= max r:〈a,r+b〉≤0,‖r‖22≤‖b‖22\n〈−xj , θ∗1 + y λ2\n2 〉+ 〈−xj , r〉\n= 〈−xj , θ∗1 + y λ2\n2 〉+ max r:〈a,r+b〉≤0,‖r‖22≤‖b‖22 〈−xj , r〉\n= 〈−xj , θ∗1 + y λ2\n2 〉 − min r:〈a,r+b〉≤0,‖r‖22≤‖b‖22 〈xj , r〉\n≤ 〈−xj ,θ∗1〉+ 〈−xj ,b〉+ ‖xj‖2‖b‖2 = −〈xj ,θ∗1〉 − 〈xj ,b〉+ ‖xj‖2‖b‖2.\n(69)\nThe last inequality utilizes Eq. (31) with x = xj .\nIf 〈b,a〉‖b‖2 ≤ −〈xj ,a〉 ‖xj‖2 and 〈xj ,a〉 < 0, Eq. (30) holds with x = −xj , and we have\n〈xj ,θ∗2〉 ≤ max θ:〈θ∗1− y λ1 ,θ−θ∗1 〉≥0,〈θ− y λ2 ,θ∗1−θ〉≥0 〈xj ,θ〉\n= max r:〈a,r+b〉≤0,‖r‖22≤‖b‖22\n〈xj , θ∗1 + y λ2\n2 〉+ 〈xj , r〉\n= 〈xj , θ∗1 + y λ2\n2 〉+ max r:〈a,r+b〉≤0,‖r‖22≤‖b‖22 〈xj , r〉\n= 〈xj , θ∗1 + y λ2\n2 〉 − min r:〈a,r+b〉≤0,‖r‖22≤‖b‖22 〈−xj , r〉\n≤ 〈xj ,θ∗1〉+ 〈xj ,b〉+ ‖ − xj‖2‖b‖2 = 〈xj ,θ∗1〉+ 〈xj ,b〉+ ‖xj‖2‖b‖2,\n(70)\nwhere the last inequality utilizes Eq. (31) with x = −xj . In addition, we have 〈b,a〉 ‖b‖2 > 〈xj ,a〉 ‖xj‖2 since 〈b,a〉 ≥ 0 according to Theorem 1 and 〈xj ,a〉 < 0. Thus, Eq. (30) does not hold with x = xj , and we can get Eq. (68) for bounding 〈xj ,θ∗2〉.\nIf a = 0, then Eq. (30) holds with x = ±xj , and we can get Eq. (69) and Eq. (70).\nThis ends the proof of this theorem."
    } ],
    "references" : [ {
      "title" : "Safe feature elimination in sparse supervised learning",
      "author" : [ "Laurent El Ghaoui", "Vivian Viallon", "Tarek Rabbani" ],
      "venue" : "Pacific Journal of Optimization,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Strong rules for discarding predictors in lasso-type problems",
      "author" : [ "Robert Tibshirani", "Jacob Bien", "Jerome Friedman", "Trevor Hastie", "Noah Simon", "Jonathan Taylor", "Ryan J. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society: Series B,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Lasso screening rules via dual polytope projection",
      "author" : [ "Jie Wang", "Binbin Lin", "Pinghua Gong", "Peter Wonka", "Jieping Ye" ],
      "venue" : "Technical report,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Learning sparse representations of high dimensional data on large scale dictionaries",
      "author" : [ "Zhen James Xiang", "Hao Xu", "Peter J. Ramadge" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "The strong rules proposed in [2] works very well in practice although they might mistakenly discard active features.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "To remedy this, safe screening techniques [1, 3, 4], which elliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution, have been proposed.",
      "startOffset" : 42,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "To remedy this, safe screening techniques [1, 3, 4], which elliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution, have been proposed.",
      "startOffset" : 42,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "To remedy this, safe screening techniques [1, 3, 4], which elliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution, have been proposed.",
      "startOffset" : 42,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "3 Relationship to Existing Approaches Figure 2 compares the Sasvi approach with the SAFE approach [1] and the DPP approach [3].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "3 Relationship to Existing Approaches Figure 2 compares the Sasvi approach with the SAFE approach [1] and the DPP approach [3].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "The constraint for θ∗ 2 used by the SAFE [1] approach is the ball centered at C with radius being the smallest distance from C to the points in the line segment EG.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "The constraint for θ∗ 2 used by the DPP [3] approach is the ball centered at E with radius BC.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "which is the one used in the DPP approach [3].",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "Therefore, although the authors in [3] motivates the DPP approach from the viewpoint of Euclidean projection, the DPP approach can indeed be treated as generating the constraint for θ∗ 2 using the variational inequality in Eq.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "We compare the performance of Sasvi with the sequential SAFE rule [1] and the sequential DPP [3] which achieve the state-of-the-art performance for the Lasso problems.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "We compare the performance of Sasvi with the sequential SAFE rule [1] and the sequential DPP [3] which achieve the state-of-the-art performance for the Lasso problems.",
      "startOffset" : 93,
      "endOffset" : 96
    } ],
    "year" : 2017,
    "abstractText" : "The model selected by sparse learning techniques usually has a few non-zero entries. Safe screening—which eliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution—is a technique for improving the computational efficiency while maintaining the same solution. In this note, we propose an approach called Sasvi (Safe screening with variational inequalities) and take LASSO for example in the analysis. Sasvi makes use of the variational inequalities which provide the sufficient and necessary optimality conditions for the dual problem. The existing approaches can be casted as relaxed versions of the proposed Sasvi, and thus Sasvi provides a much stronger screening rule. The monotone properties of Sasvi are studied based on which a sure removal regularization parameter can be identified for each feature. In addition, the proposed Sasvi is readily extended for solving the generalized sparse linear models. Preliminary experimental results are reported.",
    "creator" : "LaTeX with hyperref package"
  }
}