{
  "name" : "1408.6617.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Task-group Relatedness and Generalization Bounds for Regularized Multi-task Learning",
    "authors" : [ "Chao Zhang", "Dacheng Tao", "Tao Hu", "Xiang Li" ],
    "emails" : [ "chao.zhang@dlut.edu.cn).", "dacheng.tao@gmail.com).", "hutaomath@foxmail.com).", "lixiangalixiang@gmail.com)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 8.\n66 17\nv1 [\ncs .L\nG ]\n2 8\nA ug\nKeywords: multi-task learning, generalization bound, task relatedness, consistency, vectorvalued function"
    }, {
      "heading" : "1 Introduction",
      "text" : "There is plenty of empirical evidence to suggest that task-relatedness information improves multitask learning (MTL) over single-task learning (STL) in multiple related task (MRT) scenarios.\n∗C. Zhang is with the School of Mathematical Sciences, Dalian University of Technology, Dalian, Liaoning, 116024, P.R. China. (e-mail: chao.zhang@dlut.edu.cn).\n†D. Tao is with the Centre for Quantum Computation & Intelligent Systems, FEIT, University of Technology, Sydney, NSW 2007, Australia. (e-mail: dacheng.tao@gmail.com).\n‡T. Hu is with the School of Mathematical Sciences, Capital Normal University, Beijing, 100048 , P.R. China. (e-mail: hutaomath@foxmail.com).\n§X. Li is with the School of Mathematical Sciences, Dalian University of Technology, Dalian, Liaoning, 116024, P.R. China. (e-mail: lixiangalixiang@gmail.com).\nTherefore, capturing relatedness information is important for both theoretical and practical investigations of MTL.\nSeveral learning methods have been proposed to address this problem. Evgeniou et al. [10] introduced regularized MTL to link the simultaneous learning process of MRT scenarios to STL problems, in which the regularization terms encode the relatedness between MRTs. However, regularization term design relies on a priori knowledge of tasks. Other methods that model task relatedness let the different tasks share common structures, e.g., backpropagation networks [7] and the structure learning formulation [2]. Argyriou et al. [3] presented a method to learn a lowdimensional representation shared across MRTs, while Zhang and Yeung [23] applied covariance to model three types of relatedness between two tasks: the positive correlation, the negative correlation, and unrelatedness. From the theoretical standpoint, the notion “F -related” has been proposed to study the generalizability of multi-task classification, where if two tasks Z [1],Z [2] are F -related for a given function class F , there exists a function f ∈ F such that P [1] = f(P [2]) or P [2] = f(P [1]) [4, 5]. The interested reader is also referred to other theoretical investigations of MTL [16, 17] and learning theory [19, 8, 1, 24, 13, 12]."
    }, {
      "heading" : "1.1 Overview of Main Results",
      "text" : "As discussed by Micchelli and Pontil [20, 21], MTL can be studied from the viewpoint of vectorvalued function learning. Inspired by [20, 21], we explore the vector-valued framework to study the generalization and consistency properties of regularized MTL (RMTL) and analyze the relationship between the properties of RMTL and task-group relatedness. In particular, we address the following theoretical questions:\n• Under what conditions does RMTL perform better with a smaller task sample size than STL?\n• Under what conditions is RMTL generalizable and can guarantee the consistency of each task during simultaneous learning?\nIn order to answer these questions, we also need to consider: 1) measures of task-group relatedness; 2) the joint probability of MRTs; 3) measures of vector-valued function classes; and 4) the specific deviation and symmetrization inequalities for the vector-valued framework.\nHere, we introduce two types of task-group relatedness: the observed discrepancy-dependence measure (ODDM) and the empirical discrepancy-dependence measure (EDDM) (see Section 3).1 ODDM measures the statistical dependence between events that some tasks have large observed discrepancies and the others have small observed discrepancies. EDDM measures the statistical dependence between events that some tasks have large empirical discrepancies and the others have small empirical discrepancies. In contrast to ODDM, EDDM reflects the asymptotic behavior of the relatedness between two task groups when the sample size goes to infinity.2 We show that ODDM (or EDDM) can exist in three states: negative, positive, and zero, which respectively model three types of relatedness between two task groups: the synergy effect, the negative synergy effect, and unrelatedness.\n1In this paper, the observed discrepancy is defined as the discrepancy between an observation and its expectation, and the empirical discrepancy is defined as the discrepancy between the expectation (i.e., expected risk) and its empirical estimate (i.e., empirical risk).\n2For convenience, we assume that all tasks have the same sample size in this paper.\nSince MTL refers to a process in which MRTs are simultaneously processed, we consider the task joint probability, defined in (4), instead of the task summation probability as in [16, 17, 2, 11]. In task joint probability, the generalization bound for MTL is deemed to be the upper bound of the joint probability that there is at least one task with a large empirical discrepancy in MTL. This bound can also be used to describe the consistency of each task in the MTL learning process. In order to obtain the bound, we present the specific deviation inequalities and the symmetrization inequalities for the vector-valued framework and, meanwhile, introduce the Cartesian product-based uniform entropy number (CPUEN), which is induced from the uniform entropy numbers (UENs) of MRTs.\nBased on the resulting generalization bounds, the theoretical properties of RMTL are analyzed and we show that:\n• the validity of RMTL will theoretically be guaranteed if most of the relatedness between two task groups show a synergy effect. If almost any pair of task groups are predominantly mutual, RMTL performs well with less samples than STL, and the required sample size of each task in RMTL will not increase dramatically, regardless of the (large) number of MRTs (see Remarks 5.1&5.2).\n• there will be a tighter generalization bound for RMTL if the values of EDDMs are negative, i.e., if most of the relatedness between two task groups show a synergy effect. Moreover, we present a sufficient condition to guarantee the consistency of each task in RMTL.\nFurthermore, we obtain the following theoretical findings:\n• The aforementioned sufficient condition can be used to examine whether the given tasks, function classes, and regularization terms are suitable for MTL.\n• The existence of a negative correlation between two tasks is necessary for MTL, which is in accordance with the argument by Zhang and Yeung [23].\n• The generalization bound of RMTL.\n• The relationship between the task relatedness and the generalization performance of RMTL.\n• The sufficient condition to guarantee the consistency of each task in RMTL.\n• The proposed vector-valued framework can be used to study the theoretical properties of vector-valued function learning [21]"
    }, {
      "heading" : "1.2 Organization of the Paper",
      "text" : "The rest of this paper is organized as follows. In Section 2, the main research addressed in this paper, including the task-joint probability and generalization bounds for RMTL, is formalized. In Section 3, two quantities for measuring task-group relatedness are presented and CPUEN is introduced in Section 4 to measure the complexity of the vector-valued function classes. The main results are presented in Section 5, along with a method to examine the validity of MTL. In Section 6, we address the generalization performance results using the covariance information of MRTs and the last section concludes the paper. In Appendix, we first present the deviation inequalities and the symmetrization inequalities for the vector-valued framework (Parts A & B). Finally, the proofs of the main results are given in Part C."
    }, {
      "heading" : "2 Problem Setup",
      "text" : "We first formalize the main research addressed in this paper, including the task-joint probability and generalization bounds for RMTL."
    }, {
      "heading" : "2.1 Regularized Multi-task Learning",
      "text" : "Given a space X ⊂ RI , let X [m] be the input space of the m-th task with the probability distribution D[m] on X and Y [m] ∈ RJ be the corresponding output space (1 ≤ m ≤ M). Let g [m] ∗ : X [m] → Y [m] be the corresponding labeling function. Also, denote the m-th task as Z [m] := X [m] ×Y [m] ⊂ RK with K = I + J . In MTL, let G[1], · · · ,G[M ] ⊂ YX be M function classes corresponding to the learning tasks Z [1], · · · ,Z [M ], respectively. MTL is expected to simultaneously find M functions g̃[1], · · · , g̃[M ] from G[1], · · · ,G[M ] such that each g̃[m] can minimize the expected risk of the corresponding task Z [m] over G[m]:\nE[m](ℓ[m] ◦ g[m]) = ∫ ℓ[m](g[m](x[m]),y[m])dP[m](z[m]), 1 ≤ m ≤ M, (1)\nwhere ℓ[m] and P [m](z[m]) are the loss function and the probability distribution of the task Z [m], respectively, with z[m] := (x[m],y[m])T .\nSince the task distributions P [1], · · · , P [M ] are usually unknown, the target functions g̃[1], · · · , g̃[M ]\ncannot be directly obtained by minimizing the expected risks (1) of MRTs. Instead, the empirical risk minimization (ERM) principle can be used to handle this issue. For each task Z [m], let Z [m] N := {z [m] n }Nn=1 be a set of N i.i.d. samples drawn from Z [m] with z [m] n := (x [m] n ,y [m] n )T . The following is the objective function of RMTL:\nM∑\nm=1\nE [m] N (ℓ [m] ◦ g[m]) + rR(g[1], · · · , g[M ]),\nwhere\nE [m] N (ℓ\n[m] ◦ g[m]) := 1\nN\nN∑\nn=1\nℓ[m](g(x[m]n ),y [m] n ), (2)\nis the empirical risk of the task Z [m], R(g[1], · · · , g[M ]) is the regularization term that is designed to encode the relatedness information between MRTs and r > 0 is the regularization parameter.\nAlternatively, and as mentioned by Kakade et al. [14], the above regularized optimization can be equivalently rewritten as\nmin R(g[1],··· ,g[M])≤c\nM∑\nm=1\nE [m] N (ℓ [m] ◦ g[m]),\nwhere, instead of exploiting the regularization, a hard restriction R(g[1], · · · , g[M ]) ≤ c is set to combine the function classes G[1], · · · ,G[M ], which shrinks the original search space G to GRc . 3\n3For example, if g[m](x[m]) = x[m] for any 1 ≤ m ≤ M , the original search space G is the M -dimensional real\nspace RM . Then, by setting the restriction ∑M\nm=1(x [m])2 ≤ c2, the original space G will become anM -dimensional\nsphere GRc with radius c.\nTherefore, a proper regularization term R(g) can correctly encode the relatedness between MRTs, reduce the computational cost, and improve the generalization performance. However, this design relies on a prior knowledge of the MRTs.\nFrom the vector-valued function learning perspective [20, 21], RMTL aims to find a vector-\nvalued function gN = (g [1] N , · · · , g [M ] N ) T by simultaneously solving the M optimization problems:\nmin g∈GRc\n{ E [m] N (ℓ [m] ◦ g[m]), 1 ≤ m ≤ M } , (3)\nwhere min g∈GRc stands for a component-wise minimum operator defined in Section 2.2."
    }, {
      "heading" : "2.2 Notations of Vector Operations",
      "text" : "For the discussion that follows, it is first necessary to describe some notations of vector operations. Given two vectors, x = (x[1], · · · , x[M ])T and y = (y[1], · · · , y[M ])T , let |x| := (|x[1]|, · · · , |x[M ]|)T and denote the expression x > y (resp. x ≥ y) as x[m] > y[m] (resp. x[m] ≥ y[m]) for any 1 ≤ m ≤ M . Similarly, we denote x < y (resp. x ≤ y) as x[m] < y[m] (resp. x[m] ≤ y[m]) for any 1 ≤ m ≤ M .\nFurthermore, given (a[1], · · · , a[M ])T ∈ RM , we define the component-wise supremum operator\nsup g∈G\n{ (g[1](a[1]), · · · , g[M ](a[M ]))T }\nwith g = (g[1], · · · , g[M ])T as follows: if the vector-valued function g† = (g [1] † , · · · , g [M ] † ) T achieves the supremum over G, each component g [m] † of the vector g† achieves the supremum sup\ng[m]∈G[m] {g[m](a[m])}\nover G[m]. Similarly, we define the component-wise minimum operator as\nmin g∈G\n{ (g[1](a[1]), · · · , g[M ](a[M ]))T } ."
    }, {
      "heading" : "2.3 Task-joint Probability and Generalization Bounds",
      "text" : "In general, the generalization bounds for STL refer to the upper bounds of the supremum\nsup g∈G |E(ℓ ◦ g)− EN (ℓ ◦ g)|\nwith an alternative probability expression\nPr { sup g∈G |E(ℓ ◦ g)− EN(ℓ ◦ g)| > ξ } ,\nwhose upper bound describes the rarity of the event that the empirical discrepancy between the expected risk E(ℓ ◦ g) and the empirical risk EN(ℓ ◦ g) is larger than a given positive constant ξ.\nSince MRTs are processed simultaneously in MTL, the following task-joint probability is straightforward: for any ξ = (ξ[1], · · · , ξ[M ])T > 0,\nPr    sup g∈GRc      |E[1](ℓ[1] ◦ g[1])− E[1]N (ℓ [1] ◦ g[1])| ...\n|E[M ](ℓ[M ] ◦ g[M ])− E[M ]N (ℓ [M ] ◦ g[M ])|\n     6≤   ξ[1] ... ξ[M ]      , (4)\nwhich describes the rarity of the event in RMTL that there is at least one task Z [m] with empirical discrepancy larger than the constant ξ[m]. The upper bound of (4) is the so-called “generalization bound” for RMTL. Compared to the STL bound, the RMTL bound (4) not only reflects the generalization performance of each task, but also the dependence between simultaneously learned tasks, i.e., how the success (or failure) of some tasks affects the performance of the others.\nFor convenience, we further define the loss function class:\nF [m] := {z[m] 7→ ℓ[m](g[m](x[m]),y[m]) : g[m] ∈ G[m]}, 1 ≤ m ≤ M ; (5)\nthe Cartesian product F := F [1] × · · · × F [M ] is called the “vector-valued function class” in the rest of this paper. Similarly, based on the regularized vector-valued function class GRc , we define the regularized loss vector-valued function class by\nFRc := { (ℓ[1] ◦ g[1], · · · , ℓ[M ] ◦ g[M ])T : (g[1], · · · , g[M ])T ∈ GRc } , (6)\nwhich is also termed the regularized vector-valued function class in the remainder of this paper. Briefly, we denote for any f := (f [1], · · · , f [M ])T ∈ F ,\nE[m]f [m] := ∫ f [m](z[m])dP[m](z[m]) ; E\n[m] N f\n[m] := 1\nN\nN∑\nn=1\nf [m](z[m]n ), (7)\nand the generalization bound (4) is equivalently rewritten as Pr { sup f∈FRc { |Ef − ENf |} 6≤ ξ } with\nEf := (E[1]f [1], · · · ,E[M ]f [M ])T\nand EN f := (E [1] N f [1], · · · ,E[M ]N f [M ])T ."
    }, {
      "heading" : "3 Measures of Task-group Relatedness",
      "text" : "Some existing works on task relatedness already describe the relationship between two individual tasks, for instance the F -related [5, 4] notion and covariances [23]. In MTL, it is also necessary to consider the relationship between two task groups. Here, we present two measures of taskgroup relatedness: the observed discrepancy-dependence measure (ODDM) and the empirical discrepancy-dependence measure (EDDM)."
    }, {
      "heading" : "3.1 ODDM",
      "text" : "In probability theory, the dependence between two events A and B can be detected using the quantity Pr{A|B}−Pr{A}, where A and B are positively dependent if the conditional probability Pr{A|B} of A given B is greater than the probability Pr{A} (i.e., Pr{A|B} − Pr{A} > 0), and they are negatively dependent if the inequality is reversed [6, 22]. We introduce ODDM and EDDM to measure the relatedness between two task groups in MTL, based on the quantity.\nDefinition 3.1 Given M tasks Z [1], · · · ,Z [M ] and a regularized vector-valued function class FRc , let Λ := {1, · · · ,M} be an index set and Λ[m] be a subset of Λ with the cardinality of m. For any Λ[m] ⊂ Λ and any ξ = (ξ[1], · · · , ξ[M ])T > 0, ODDM is defined as\nφF(Λ [m], ξ) := sup\nf∈FRc\n{ Pr { AΛ[m] ∣∣BΛ[m] } − Pr { AΛ[m] }} ,\nwhere f = (f [1], · · · , f [M ])T , Λ[m] stands for the complementary set of Λ[m] with Λ[m] ∪ Λ[m] = Λ, and the events AΛ[m] := {s [i] > ξ[i]}i∈Λ[m] and BΛ[m] := {s [i] ≤ ξ[i]} i∈Λ[m] w.r.t. the observed discrepancy s[i] := |E[i]f [i] − f [i](z[i])|\nof the task Z [m].\nAs defined above, ODDMmeasures the dependence between the events that the tasks in group Λ[m] have large observed discrepancies and the tasks in Λ[m] have small observed discrepancies. In fact, ODDM is determined by the inherent characteristics of MRTs, the selection of function classes and the regularization term. It can exist in one of three states:\n• a positive ODDM implies that some functions in the search space FRc will result in a negative synergy effect between the tasks {Z [i]}\ni∈Λ[m] and the others {Z [i]}i∈Λ[m], i.e., the\nsuccess of tasks {Z [i]} i∈Λ[m] will benefit from a performance loss in the others {Z [i]}i∈Λ[m];\n• a negative ODDM means that all functions in FRc will effect the synergy effect on the simultaneous learning process for MRTs, i.e., the success of the tasks {Z [i]}\ni∈Λ[m] contributes\nto improved performance of the others {Z [i]}i∈Λ[m] ;\n• a zero ODDM reflects that some functions inFRc eliminate the relatedness between {Z [i]}i∈Λ[m]\nand {Z [i]} i∈Λ[m] , and the others will effect synergy effect between the two groups."
    }, {
      "heading" : "3.2 EDDM",
      "text" : "Since this paper focuses on ERM-based RMTL, we also need to consider the asymptotic behavior of the dependence between two task groups when the sample size N goes to infinity.\nDefinition 3.2 Following the notations in Definition 3.1 and letting Z [m] N := {z [m] n }Nn=1 be N i.i.d. samples drawn from each task Z [m] (1 ≤ m ≤ M), EDDM is defined as\nϕN FRc (Λ[m], ξ) := Pr { ANΛ[m] ∣∣BNΛ[m] } − Pr { ANΛ[m] } ,\nwhere the events AN Λ[m] := {t[i]N > ξ [i]}i∈Λ[m] and B N Λ[m] := {t[i]N ≤ ξ [i]} i∈Λ[m] with the empirical discrepancy t [i] N := sup\nf∈Prj[i](FRc )\n|E[i]f − E[i]Nf |, (8)\nw.r.t. the sample set Z [m] N drawn from Z [m], and Prj[i](FRc ) stands for the projection of the regularized vector-valued function class FRc onto the function class F [i].\nNote that EDDM measures the dependence between the generalization performances of the two task groups and also has three states:\n• a positive EDDM implies that the successfully learned tasks {Z [i]} i∈Λ[m] benefit from a loss\nin generalization performance of the others {Z [i]}i∈Λ[m];\n• a negative EDDM means that the task groups {Z [i]}i∈Λ[m] and {Z [i]} i∈Λ[m] are mutually\nbeneficial;\n• a zero EDDM with N < ∞ signifies that the two groups are unrelated."
    }, {
      "heading" : "3.3 Empirically Computing ODDM and EDDM",
      "text" : "By the facts that Pr{A|B} = Pr{A,B}/Pr{B} and Pr{A} = E1{A}, ODDM φF(Λ [m], ξ) can be empirically computed in the following way. Letting {z[m]n }Nn=1 be i.i.d. samples drawn from the task Z [m] (1 ≤ m ≤ M), we denote ζj (1 ≤ j ≤ J), ηk (1 ≤ k ≤ K) and θp (1 ≤ p ≤ P ) as the observations of the events AΛ[m] ∧ BΛ[m] , AΛ[m] and BΛ[m] , respectively. Then, an empirical version of ODDM φF(Λ [m], ξ) is given by:\nφ̂F(Λ [m], ξ) := sup\nf∈FRc\n{ J−1 ∑J j=1 1{ζj}\nP−1 ∑P p=1 1{θp} −K−1\nK∑\nk=1\n1{ηk}\n} , (9)\nwhere the expected risk E[i]f [i] in s[i] is approximated by its empirical version E [i] Nf [i].\nRecalling the term t [i] N defined in (8), EDDM ϕ N FRc (Λ[m], ξ) can be approximately computed in the following way. First, fix the sample set {z[i]n }Nn=1 of each task Z [i] (1 ≤ i ≤ M) and replace the expected risk E[i]f with the fixed empirical quantity E [i] Nf w.r.t. {z [i] n }Nn=1. Next, we randomly select L samples from of each task Z [i] to form another empirical risk E[i]L f and denote t̂ [i] L := sup\nf∈Prj[i](FRc )\n|E[i]L f − E [i] Nf | as an estimate of t [i] N . Denote the events AL := {t̂ [i] L > ξ [i]}i∈Λ[m]\nand BL := {t̂ [i] L ≤ ξ [i]} i∈Λ[m]\n. Let ζj (1 ≤ j ≤ J), ηk (1 ≤ k ≤ K) and θp (1 ≤ p ≤ P ) be the observations of the events AL ∧ BL, AL and BL respectively. We then can empirically compute EDDM ϕN\nFRc (Λ[m], ξ) as\nϕ̂N FRc\n(Λ[m], ξ) := J−1\n∑J j=1 1{ζj}\nP−1 ∑P p=1 1{θp} −K−1\nK∑\nk=1\n1{ηk}. (10)\nRemark 3.1 There are two difficulties to implement this method to empirically compute ODDM and EDDM:\n• In general, it is hard to capture the observations of the task-joint events.\n• If the task number M is large, it is highly time-consuming to compute the empirical estimates of ODDM and EDDM for any Λ[m]. To reduce the complexity, one feasible way is to cluster the tasks according to the similarity and select a representative task from each cluster to compute ODDM and EDDM."
    }, {
      "heading" : "4 Cartesian Product-based Uniform Entropy Numbers",
      "text" : "Complexity measures of function classes play an important role in learning theory. Since this paper studies MTL in the vector-valued framework, the classical measures such as the VapnikChervonenkis (VC) dimension and the covering number, are not applicable (or at least cannot be directly applied) to the vector-valued scenario. For example, Ben-David and Borbely [4] applied an extended version of the VC dimension to study the generalization properties of multi-task classification.\nHere, we introduce the Cartesian product-based uniform entropy number (CPUEN) to measure the complexity of the vector-valued function classes. First, we briefly outline the definitions of the covering number and uniform entropy number (UEN) of the scalar-valued function classes. Regarding further details, please refer to Mendelson [18].\nDefinition 4.1 Let F be a function class and d be a metric on F . For any ξ > 0, the covering number of F at radius ξ w.r.t. the metric d, denoted by N (F , ξ, d) is the minimum size of a cover of radius ξ. Furthermore, given a sample set ZN := {zn}Nn=1 drawn from Z, we denote Z′N := {z ′ n} N n=1 as the ghost sample set drawn from Z, such that the ghost sample z ′ n has the same distribution as zn for any 1 ≤ n ≤ N . Denote Z2N := {ZN ,Z′N}. Setting the metric d as the ℓp(Z2N) (p > 0) norm, UEN is defined by\nlnNp(F , ξ, N) := sup ZN lnN (F , ξ, ℓp(ZN )) . (11)\nRecall that the vector-valued function class F is a Cartesian product of the function classes F [1], · · · ,F [M ], i.e., F := F [1] × · · · × F [M ]. For each F [m] (1 ≤ m ≤ M), let Z̃[m]N be the sample set achieving the supremum\nsup Z\n[m] N ∈(Z [m])N\nlnN ( F [m], ξ[m], ℓp(Z [m] N ) ) (12)\nand Ω [m] p,N be one of the covers of F [m] related to the supremum w.r.t. the norm ℓp(Z̃ [m] N ). Therefore, the Cartesian product Ω [1] p,N × · · · × Ω [M ] p,N is also a cover of F with the radius vector ξ := (ξ[1], · · · , ξ[M ])T . Following the above notations, we define the CPUEN of the vector-valued function class F as follows:\nDefinition 4.2 Given a vector-valued function class F , consider a Cartesian product-based cover of the vector-valued function F :\nΩp,N(F , ξ) := { AMp ∈ Ω [1] p,N × · · · × Ω [M ] p,N : A M p ∩F 6= ∅ } .\nThen, CPUEN of F is defined as lnN p(F , ξ, N) := ln |Ωp,N(F , ξ)|.\nIn contrast to the classical UEN [see (11)], CPUEN is induced from the cover of the function class F [m] of each task Z [m] (1 ≤ m ≤ M) with different norms and radiuses instead of introducing a uniform norm in the vector-valued function space F . Although CPUEN is usually larger than the uniform-norm UEN of the vector-valued function class F , the induction setting of CPUEN has a stronger relationship with the prior information-based design of the regularization term and offers convenience to the theoretical analysis of RMTL."
    }, {
      "heading" : "5 Generalization Bounds of RegularizedMulti-task Learn-",
      "text" : "ing\nIn this section, we present the generalization bounds of RMTL and discuss how the task-group relatedness affects the generalization properties of RMTL. Moreover, we give a sufficient condition for the consistency of each task in MRTs."
    }, {
      "heading" : "5.1 Two Special Cases",
      "text" : "Before the formal discussion, we first bound the probabilities of two special events: first, that all tasks have large empirical discrepancies and second, that all tasks have small empirical discrepancies.\nTheorem 5.1 Assume that FRc is a regularized vector-valued function class w.r.t. the constant c, and Z [m] N = {z [m] n }Nn=1 is the set of N i.i.d. samples drawn from the task Z\n[m] (1 ≤ m ≤ M). Let Λ := {1, · · · ,M} be an index set and denote Λ[m] as a subset of Λ with the cardinality of m. Denote Z [m] 2N := {Z [m] N ,Z ′[m] N }. Given ξ = (ξ [1], · · · , ξ[M ])T > 0 and for any N ∈ N such that N ≥ 8Γ(Λ) 1−2Υ(Λ) , it then holds that\nPr { sup f∈FRc ∣∣Ef − ENf ∣∣ > ξ } ≤ 2M+2N 1 ( FRc , ξ/8, 2N ) exp { −N ∑M m=1(ξ [m])2 32M2(b− a)2 } , (13)\nwhere\nΓ(Λ) :=\nM∑\nm=1\n∑\nΛ[m]⊂Λ m(b− a)2∑ i∈Λ[m] (ξ[i])2 , (14)\nand\nΥ(Λ) :=\nM∑\nm=1\n∑\nΛ[m]⊂Λ\nφF(Λ [m], ξ). (15)\nThis theorem shows that if it holds that N ≥ 8Γ(Λ) 1−2Υ(Λ)\n, the probability of sup f∈FRc\n∣∣Ef−EN f ∣∣ > ξ\ncan be bounded by the RHS of (13). Note that if M = 1, since φF(Λ [m], ξ) equals zero, the quantity Υ(Λ) is zero and the bound (13) coincides with the classical result of STL (see Theorem 2.3 of [18]).\nRemark 5.1 In the case of M > 1, the condition N ≥ 8Γ(Λ) 1−2Υ(Λ) should be satisfied when the quantity Υ(Λ) < 1/2: namely, it is necessary for RMTL to satisfy the condition that the taskgroup relatedness between MRTs should mostly be synergistic. Furthermore, RMTL will perform well with less samples than STL size N ≥ 8(b−a) 2\nξ2 if the condition Υ(Λ) ≤ (1−2M−1) holds, which\nimplies that almost any pair of task groups Λ[m] and Λ[m] predominantly promote mutually.4\n4Actually, letting ξ0 := min{ξ [1], · · · , ξ[M ]} and N0 :=\n8(b−a)2\nξ2 0\n, we have 8Γ(Λ) < (2M − 1)N0. Thus, the\ncondition N ≥ 8Γ(Λ)1−2Υ(Λ) holds if N is larger than (2M−1)N0 1−2Υ(Λ) . We can then infer that each task in RMTL will need less samples than the task in STL if the condition 2 M−1\n1−2Υ(Λ) < 1 holds.\nRemark 5.2 If ξ = ξ[1] = · · · = ξ[M ] and each ODDM φF(Λ[m], ξ) reaches the minimum value −1, the sample size N of each task should be larger than the value 8(b−a) 2\n((2M−1)−1+2)ξ2 (M > 1) to\nsupport the inequality (13). This implies that the required sample size of each task in RMTL will approach half of the STL value 8(b− a)2/ξ2 at the rate of 2−M as M → ∞. This finding shows that if the relationship between any pair of task groups Λ[m] and Λ[m] is predominantly synergistic, each task in RMTL needs less samples than STL and the required sample size N in RMTL will not increase dramatically, regardless of a large number of MRTs.\nWe next consider the second special case and present an upper bound of the probability that all tasks have small empirical discrepancies in the simultaneous learning process for MRTs. The following theorem is proved by using the small-deviation techniques [15].\nTheorem 5.2 Following the notations in Theorem 5.1, it then holds that for any ξ = (ξ[1], · · · , ξ[M ])T > 0,\nPr { sup f∈FRc ∣∣Ef −EN f ∣∣ ≤ ξ } ≤ 2M sup f∈FRc Pr { s ≤ 2ξ } , (16)\nwhere s = ( s[1], · · · , s[M ] )T with s[m] := ∣∣E[m]f [m] − f [m](z[m]) ∣∣ for any 1 ≤ m ≤ M .\nThis theorem converts the case of small empirical discrepancies into a simple case, where the LHS of (16) can be bounded by using the probability that the observed discrepancy of each task Z [m] is smaller than 2ξ[m] (1 ≤ m ≤ M). Compared to the case of empirical discrepancies, the RHS of (16) is only determined by the inherent characteristics of MRTs, e.g., the distributions of tasks, the selection of function classes, and the regularization term."
    }, {
      "heading" : "5.2 Main Results",
      "text" : "Based on these two special cases, we obtain the generalization bounds of RMTL and a sufficient condition for the consistency of each task in the simultaneous learning process for MRTs.\nTheorem 5.3 Following the notations of Theorem 5.1, given ξ = (ξ[1], · · · , ξ[M ])T > 0 and for\nany N ∈ N such that N ≥ max 1≤m≤M max Λ[m]⊂Λ\n8Γ(Λ[m])\n1−2Υ(Λ[m]) , it then holds that\nPr { sup f∈FRc ∣∣Ef −EN f ∣∣ 6≤ ξ } ≤ M∑ m=1 ∑\nΛ[m]⊂Λ\n2mPr {{ s[λ] ≤ 2ξ[λ] } λ∈Λ[m] }\n×  ϕNFRc (Λ [m], ξ) + 2m+2N 1 ( Prj FRc Λ[m] , ξΛ[m] 8 , 2N ) exp    −N ∑ λ∈Λ[m] (ξ[λ])2 32M2(b− a)2      , (17)\nwhere Prj FRc\nΛ[m] stands for the projection of FRc on the subspace ∏ λ∈Λ[m] F [λ], ξΛ[m] := ( ξ[λ] ) λ∈Λ[m] ,\nΓ(Λ[m]) and Υ(Λ[m]) are defined in (14). Furthermore, if it is satisfied that for any 1 ≤ m ≤ M and λ[m] ⊂ Λ,\nlim N→+∞ ϕN FRc\n( Λ[m], ξ ) = lim\nN→+∞ lnN 1\n( Prj FRc\nΛ[m] , ξΛ[m]\n8 , 2N\n) = 0, (18)\nit then holds that lim\nN→+∞ Pr { sup f∈FRc ∣∣Ef − EN f ∣∣ 6≤ ξ } = 0. (19)\nIn this theorem, we obtain an upper bound of the joint probability of the event that supf∈FRc ∣∣Ef− EN f\n∣∣ 6≤ ξ and show that the consistency of each task in MTL can be guaranteed if condition (18) is valid. We are concerned with two aspects of the theorem:\n• the RHS of (17) implies that given N < ∞, a smaller value of EDDM ϕN FRc (Λ[m], ξ) will lead\nto a sharper bound, which is in accordance with the argument that the negative EDDM means that the task groups benefit from each other (see Section 3).\n• The asymptotic convergence of the generalization bound is determined by two factors: 1)\nEDDM ϕN FRc\n( Λ[m], ξ ) ; and 2) CPUEN lnN 1 ( Prj FRc\nΛ[m] , ξΛ[m]/8, 2N\n) . In particular, according\nto the classical results of STL (see Theorem 2.3 & Definition 2.5 of [18]), if UEN for each task Z [m] satisfies that lnN1(F [m],ξ[m]/8,2N)\nN converges to zero when N goes to infinity, the\nsecond equality of (18) holds. Note that the convergence of ϕN FRc\n( Λ[m], ξ ) is determined\nby the inherent characteristics of MRTs, e.g., distributions of tasks, selection of function classes, and regularization terms.\nRemark 5.3 Moreover, these theoretical findings cause us to preliminarily examine whether the combination of tasks, function classes, and regularization terms is suitable for the ERM-based RMTL according to the rules that\nΥ(Λ) =\nM∑\nm=1\n∑\nΛ[m]⊂Λ\nφF(Λ [m], ξ) <\n1 2 ,\nand lim\nN→+∞ ϕN FRc\n( Λ[m], ξ ) = 0\nwith ϕN FRc (Λ[m], ξ) ≤ 0 for any Λ[m] ⊂ Λ (1 ≤ m ≤ M)."
    }, {
      "heading" : "6 Generalization Bounds with Covariance Information",
      "text" : "As discussed in Section 3, since ODDM detects the dependence between two task groups, the bound (13) cannot reflect how the individual relatedness between two tasks affects the generalization performance of RMTL for more than two tasks. Here, we consider the generalization results based on the covariance information between every two tasks.\nTheorem 6.1 Follow the notations of Theorem 5.1. Given ξ = (ξ[1], · · · , ξ[M ])T > 0 and for any N ∈ N such that\nN ≥ 8Γ2\n1− 2(Υ(Λ) + Υ2) , (20)\nthen there holds that\nPr { sup f∈FRc ∣∣Ef − ENf ∣∣ > ξ } ≤ 2M+2N 1 ( FRc , ξ/8, 2N ) exp { −N ∑M m=1(ξ [m])2 32M2(b− a)2 } , (21)\nwhere\nΓ2 :=\nM∑\nm=1\n∑\nΛ[m]⊂Λ\nm(b− a)2 ( ∑\ni∈Λ[m] ξ[i]\n)2 , (22)\nand\nΥ2 := M∑\nm=1\n∑\nΛ[m]⊂Λ\n8 ∑\ni1<i2 i1,i2∈Λ[m]\nCovF (i1, i2)\n( ∑ i∈Λ[m] ξ[i] )2 . (23)\nCompared to Theorem 5.1, the condition (20) incorporates the quantity Υ2 which is related to the covariance information. Actually, the quantity Υ2 is derived by replacing ∑ i∈Λ[m] (ξ[i])2 with ( ∑ i∈Λ[m] ξ[i] )2 as shown in the proof of Lemma B.2. From the condition (20), we can find that the bound (21) is valid when Υ(Λ) + Υ2 < 1/2, which means that if the synergetic effect is the main group relatedness in the learning process and some of the correlations between tasks are negative, the learning process will perform well with a small sample size N . Zhang and Yeung [23] have highlighted the necessity of the negative correlation and pointed out that the negative correlation is helpful to reduce the search space in MTL, which is in accordance with our theoretical findings.\nHowever, when M = 1, the bound (21) coincides with the canonical results in STL if and only if the quantity CovF (i, i) equals to zero, i.e., the random variable z of the task Z [i] takes a constant with the probability of one. Since this setting is far away from the practical scenario, unlike the result (17), the bound (21) that encodes covariance information cannot reflect the transition from STL to MTL."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we apply the vector-valued framework to study the generalization performance of RMTL and analyze the relationship between the task-group relatedness and the properties of RMTL. In particular, we introduce two types of task-group relatedness: ODDM and EDDM, and we present CPUEN to measure the complexity of the regularized vector-valued function class FRc . By applying the specific deviation and symmetrization inequalities to the vector-valued framework, we obtain the generalization bound for RMTL and provide a sufficient condition to guarantee the consistency of each task in the simultaneous learning process of MRTs. Finally, we show that the theoretical findings of this paper can examine whether the task settings are suitable for the RMTL mechanism\nBased on the theoretical findings, we summarize the relationship between the generalization properties of RMTL and the task-group relatedness as follows:\n• ODDM is related to the sample size and validity of RMTL (see Theorem 5.1). We first prove that the condition of Υ(Λ) < 1\n2 is necessary for the validity of RMTL and then show\nthat if almost any pair of task groups Λ[m] and Λ[m] predominantly mutually promote, the required sample size N of each task in RMTL will be smaller than that of STL for each\ntask. The sample size will also not increase dramatically, regardless of a large number of MRTs (see Remarks 5.1 & 5.2).\n• EDDM affects the generalization performance of RMTL as follows: 1) a negative EDDM provides a sharper generalization bound; and 2) the asymptotic behavior of EDDM also affects the consistency of the task (see Theorem 5.3).\n• The existence of a negative correlation between two tasks is necessary for MTL, which is in accordance with the relevant argument of [23].\nIn summary, synergistic task-group relatedness is beneficial to the generalization performance of RMTL. In future works, we will focus on the practical applications of the theoretical findings, for instance by improving the empirical computations of ODDM and EDDM (see Remark 5.3) and designing the regularization term for RMTL based on the task-group relatedness."
    }, {
      "heading" : "A Deviation Inequalities for Random Vectors",
      "text" : "To obtain the generalization bounds for RMTL, we need to consider the deviation inequalities for random vectors. The following lemma is derived from [9].\nLet sn = (s [1] n , · · · , s [M ] n )T ∈ RM (1 ≤ n ≤ N) be N i.i.d. random vectors such that\nM∑\nm=1\ns[m]n ≤ 1, for n = 1, · · · , N , (24)\nand s[m]n ≥ 0, for 1 ≤ n ≤ N and 1 ≤ m ≤ M . (25) Note that the components s [1] n , · · · , s [M ] n of sn are not necessarily independent. The mean µ = (µ[1], · · · , µ[M ])T of random vectors sn is expressed as\nµ[m] = E[m]s[m]n , for 1 ≤ m ≤ M . (26)\nLemma A.1 For any ξ = (ξ[1], · · · , ξ[M ])T > 0 such that ∑M\nm=1(µ [m] + ξ[m]) < 1, then there\nholds that\nPr {∣∣∣ 1 N N∑\nn=1\nsn − µ ∣∣∣ > ξ } ≤ 2M exp { −2N M∑\nm=1\n(ξ[m])2 } . (27)\nMoreover, since the vector-valued function f has the range [a, b], let\ns[m]n := f [m](z [m] n )− a\nM(b − a) , 1 ≤ n ≤ N, 1 ≤ m ≤ M, (28)\nand then\nPr {∣∣EN f − Ef ∣∣ > ξ } = Pr {∣∣∣ 1 N N∑\nn=1\nsn − Ef − a\nM(b− a) ∣∣∣ > ξ M(b− a)\n} , (29)\nwhere a = (a, · · · , a)T ∈ RM . Thus, the combination of Lemma A.1 and (29) leads to a Hoeffdingtype deviation inequality for random vectors.\nTheorem A.1 Given a bounded vector-valued function f = (f [1], · · · , f [M ])T with the range [a, b], there holds that for any ξ = (ξ[1], · · · , ξ[M ]) > 0,\nPr {∣∣EN f − Ef ∣∣ > ξ } ≤ 2M exp { −2N M∑\nm=1\n(ξ[m])2\nM2(b− a)2\n} . (30)"
    }, {
      "heading" : "B Symmetrization Inequalities for Random Vectors",
      "text" : ""
    }, {
      "heading" : "B.1 Chebyshev Inequalities for Random Vectors",
      "text" : "Definition B.1 Assume that Z [1], · · · ,Z [M ] are M distributions on R. Let Λ := {1, · · · ,M} be an index set and Λ[m] be a subset of Λ with the cardinality of m. For any Λ[m] ⊂ Λ and any ξ = (ξ[1], · · · , ξ[M ])T > 0, define\nψ(Λ[m], ξ) :=Pr { {s[i] > ξ[i]}i∈Λ[m] ∣∣{s[i] ≤ ξ[i]} i∈Λ[m] } − Pr { {s[i] > ξ[i]}i∈Λ[m] } . (31)\nwhere s[i] is the non-negative random variable of the task Z [i], and Λ[m] stands for the complementary set of Λ[m] with Λ[m] ∪ Λ[m] = Λ.\nLemma B.1 Let s = (s[1], · · · , s[M ])T be a random vector with nonnegative elements and Λ = {1, · · · ,M} be an index set. For any ξ = (ξ[1], · · · , ξ[M ])T > 0, then there holds that\nPr {s 6≤ ξ} ≤ M∑\nm=1\n∑\nΛ[m]⊂Λ\n ψ(Λ[m], ξ) + ∑ i∈Λ[m] E[i] { (s[i])2 }\n∑ i∈Λ[m] (ξ[i])2\n  , (32)\nwhere s 6≤ ξ means that there is at least one index m ∈ Λ such that s[m] > ξ[m], and Λ[m] stands for an index set with the cardinality of m.\nLemma B.2 Let s = (s[1], · · · , s[M ])T be a random vector with nonnegative elements and Λ = {1, · · · ,M} be an index set. For any ξ = (ξ[1], · · · , ξ[M ])T > 0, then there holds that\nPr {s 6≤ ξ} ≤ M∑\nm=1\n∑\nΛ[m]⊂Λ\n  ψ(Λ[m], ξ) + ∑ i∈Λ[m] E[i] { (s[i])2 } + 2 ∑ i<j i,j∈Λ[m] E { s[i]s[j] }\n( ∑ i∈Λ[m] ξ[i] )2\n  , (33)\nwhere s 6≤ ξ means that there is at least one index m ∈ Λ such that s[m] > ξ[m], and Λ[m] stands for an index set with the cardinality of m."
    }, {
      "heading" : "B.2 Symmetrization Inequalities",
      "text" : "By applying ODDM, we can develop the symmetrization inequality for MTL as follows:\nTheorem B.1 Assume that F is a vector-valued function class with the range [a, b]. For any ξ ≥ 0 such that\nN ≥ 8Γ(Λ)\n1− 2Υ(Λ) , (34)\nthen there holds that\nPr { sup f∈F ∣∣Ef − EN f ∣∣ > ξ } ≤ 2Pr { sup f∈F ∣∣E′Nf − EN f ∣∣ > ξ 2 } , (35)\nwhere\nΓ(Λ) :=\nM∑\nm=1\n∑\nΛ[m]⊂Λ m(b− a)2∑ i∈Λ[m] (ξ[i])2 , Υ(Λ) :=\nM∑\nm=1\n∑\nΛ[m]⊂Λ\nφF(Λ [m], ξ),\nΛ = {1, · · · ,M} is an index set and Λ[m] is a subset of Λ with the cardinality of m.\nThe following is the symmetrization result incorporating the covariance information between every two tasks.\nTheorem B.2 Assume that F is a vector-valued function class with the range [a, b]. For any ξ = (ξ[1], · · · , ξ[M ])T > 0 such that\nN ≥ 8Γ2\n1− 2(Υ(Λ) + Υ2) , (36)\nthen there holds that\nPr { sup f∈F { |Ef −EN f } | > ξ } ≤ 2Pr { sup f∈F { |E′Nf −EN f | } > ξ 2 } , (37)\nwhere\nΓ2 :=\nM∑\nm=1\n∑\nΛ[m]⊂Λ\nm(b− a)2 ( ∑\ni∈Λ[m] ξ[i]\n)2 , Υ2 := M∑\nm=1\n∑\nΛ[m]⊂Λ\n8 ∑ i1<i2\ni1,i2∈Λ[m]\nCovF (i1, i2)\n( ∑ i∈Λ[m] ξ[i] )2\nand CovF (i1, i2) is defined as\nCovF(i, j) := max (f [1],··· ,f [M])T∈F\nCov ( f [i](z[i]), f [j](z[j]) ) (38)\nwith z[i] and z[j] (1 ≤ i, j ≤ M) being the random variables of the tasks Z [i] and Z [j], respectively."
    }, {
      "heading" : "C Proofs of Main Results",
      "text" : ""
    }, {
      "heading" : "C.1 Proof of Lemma A.1",
      "text" : "Proof of Lemma A.1. Let t = ∣∣ 1 N ∑N n=1 sn − µ\n∣∣. The event |t| > ξ contains 2M possibilities: for any 1 ≤ m ≤ M , there are m components of the vector t such that t[ik] > ξ[ik] (1 ≤ k ≤ m) and the rest are of the case that t[ik] < −ξ[ik] (1 ≤ k ≤ M − m). For convenience, we also denote {Pi}2 M i=1 as the collection of all 2 M possibilities.\nAccording to Theorem 1 in [9], the following result is valid for any possibility Pi (1 ≤ i ≤ 2M):\nPr {Pi} ≤ M∏\nm=0\n( µ[m]\np[m]\n)p[m]N , (39)\nwhere p[m] = µ[m] + ξ[m] (m = 1, · · · ,M), µ0 = 1 − ∑M m=1 µ [m] and p0 = 1 − ∑M m=1 p\n[m]. Then, we have\nPr {∣∣∣ 1 N N∑\nn=1\nsn − µ ∣∣∣ > ξ } ≤ 2M M∏\nm=0\n( µ[m]\np[m]\n)p[m]N . (40)\nThen, consider\nM∏\nm=0\n( µ[m]\np[m]\n)p[m]N =exp { N M∑\nm=0\np[m] log (µ[m] p[m]\n)}\n=exp { N (( 1− M∑\nm=1\np[m] ) log\n(1− ∑M\nm=1 µ [m]\n1− ∑M\nm=1 p [m]\n) + M∑\nm=1\np[m] log (µ[m] p[m]\n))}\n≤ exp { N ( M∑\nm=1\n( 1− p[m] ) log (1− µ[m] 1− p[m] ) + M∑\nm=1\np[m] log (µ[m] p[m]\n))} (∗)\n= exp { −N M∑\nm=1\n∫ p[m]\nµ[m]\n( p[m]\nx −\n1− p[m]\n1− x\n) dx\n}\n=exp { −N M∑\nm=1\n∫ p[m]\nµ[m]\np[m] − x x(1 − x) dx\n}\n≤ exp { −N M∑\nm=1\n4\n∫ p[m]\nµ[m] (p[m] − x)dx\n}\n=exp { −2N M∑\nm=1\n(p[m] − µ[m])2 } = exp { −2N M∑\nm=1\n(ξ[m])2 } , (41)\nbecause x(1−x) ≤ 1/4 for any x ∈ R, and the step (∗) is followed from the fact that the function f is subadditive if f is concave and f(0) ≥ 0."
    }, {
      "heading" : "C.2 Proof of Lemma B.1",
      "text" : "Proof of Lemma B.1. Given M tasks Z(1), · · · ,Z [M ] and a vector-valued function class F , let Λ := {1, · · · ,M} be an index set and Λ[m] be a subset of Λ with the cardinality of m. For any Λ[m] ⊂ Λ and any ξ = (ξ(1), · · · , ξ[M ])T > 0, define\nψ(Λ[m], ξ) :=Pr { {s[i] > ξ[i]}i∈Λ[m] ∣∣{s[i] ≤ ξ[i]} i∈Λ[m] } − Pr { {s[i] > ξ[i]}i∈Λ[m] } . (42)\nThen, the event s 6≤ ξ contains the following possibilities:\n• P [1]: there is only one index {i} = Λ[1] ⊂ Λ satisfying that s[i] > ξ[i];\n• P [m]: there are only m (1 < m < M) indices {i[1], · · · , i[m]} = Λ[m] ⊂ Λ satisfying that s[ik] > ξ[ik] (1 ≤ k ≤ m);\n• P [M ]: s[m] > ξ[m] holds for any 1 ≤ m ≤ M .\nThus, we have Pr {s 6≤ ξ} = Pr{P [1]}+ · · ·+ Pr{P [M ]}. (43)\nAccording to Chebyshev’s inequality and (42), we have\nPr{P [1]} = M∑\nm=1\n( ψ({m}, ξ) + Pr{s[m] > ξ[m]} ) ≤ M∑\nm=1\n( ψ({m}, ξ) + E{(s[m])2}\n(ξ[m])2\n) , (44)\nand for any 2 ≤ m ≤ M ,\nPr{P [m]} = ∑\nΛ[m]⊂Λ\n( ψ(Λ[m], ξ) + Pr{s[i] > ξ[i] : i ∈ Λ[m]} )\n= ∑\nΛ[m]⊂Λ\n( ψ(Λ[m], ξ) + Pr{(s[i])2 > (ξ[i])2 : i ∈ Λ[m]} )\n≤ ∑\nΛ[m]⊂Λ\n ψ(Λ[m], ξ) + Pr    √ ∑\ni∈Λ[m]\n(s[i])2 >\n√ ∑\ni∈Λ[m]\n(ξ[i])2     \n≤ ∑\nΛ[m]⊂Λ\n  ψ(Λ[m], ξ) + E { ∑ i∈Λ[m] (s[i])2 }\n∑ i∈Λ[m] (ξ[i])2\n \n= ∑\nΛ[m]⊂Λ\n ψ(Λ[m], ξ) + ∑ i∈Λ[m] E{s[i]}2\n∑ i∈Λ[m] (ξ[i])2\n  . (45)\nThe combination of (43), (44) and (45) leads to the result (32). This completes the proof."
    }, {
      "heading" : "C.3 Proof of Lemma B.2",
      "text" : "Proof of Lemma B.2. The event s 6≤ ξ contains the following possibilities:\n• P [1]: there is only one index {i} = Λ[1] ⊂ Λ satisfying that s[i] > ξ[i];\n• P [m]: there are only m (1 < m < M) indices {i[1], · · · , i[m]} = Λ[m] ⊂ Λ satisfying that s[ik] > ξ[ik] (1 ≤ k ≤ m);\n• P [M ]: s[m] > ξ[m] holds for any 1 ≤ m ≤ M .\nThus, we have Pr {s 6≤ ξ} = Pr{P [1]}+ · · ·+ Pr{P [M ]}. (46)\nAccording to Chebyshev’s inequality and (42), we have\nPr{P [1]} = M∑\nm=1\n( ψ({m}, ξ) + Pr{s[m] > ξ[m]} ) ≤ M∑\nm=1\n( ψ({m}, ξ) + E{(s[m])2}\n(ξ[m])2\n) , (47)\nand for any 2 ≤ m ≤ M ,\nPr{P [m]} = ∑\nΛ[m]⊂Λ\n( ψ(Λ[m], ξ) + Pr{s[i] > ξ[i] : i ∈ Λ[m]} )\n≤ ∑\nΛ[m]⊂Λ\n ψ(Λ[m], ξ) + Pr    ∑\ni∈Λ[m]\n(s[i]) > ∑\ni∈Λ[m]\n(ξ[i])     \n≤ ∑\nΛ[m]⊂Λ\n  ψ(Λ[m], ξ) + E {( ∑ i∈Λ[m] s[i] )2 }\n( ∑ i∈Λ[m] ξ[i] )2\n \n= ∑\nΛ[m]⊂Λ\n  ψ(Λ[m], ξ) + ∑ i∈Λ[m] E{(s[i])2}+ 2 ∑ i,j∈Λ[m] i<j E{s[i]s[j]}\n( ∑ i∈Λ[m] ξ[i] )2\n  . (48)\nThe combination of (46), (47) and (48) leads to the result (33). This completes the proof."
    }, {
      "heading" : "C.4 Proof of Theorem B.1",
      "text" : "Proof of Theorem B.1. Let fN = (f̂ [1], · · · , f̂ [M ])T be the vector-valued function achieving the supremum sup f∈F ∣∣Ef −EN f ∣∣.\nAccording to the triangle inequality, we have\n|EfN −EN fN | − |E ′ N fN − EfN | ≤ |E ′ N fN − EN fN |, (49)\nand thus\n1{|EfN−EN fN |>ξ}1{|EfN−E′N fN |≤ξ/2} =1{|EfN−EN fN |>ξ}∧{|E′N fN−EfN |≤ξ/2}\n≤1{|E′N fN−EN fN |>ξ/2} . (50)\nTaking expectations with respect to the ghost samples gives\n1{|EfN−EN fN |>ξ}Pr ′ {∣∣EfN − E′NfN ∣∣ ≤ ξ\n2\n} ≤Pr′ {∣∣E′N fN −EN fN ∣∣ > ξ\n2\n} . (51)\nAccording to Lemma B.1, since the samples z [m] n (1 ≤ m ≤ M, 1 ≤ n ≤ M) are independent of\neach other, we have\nPr′ {∣∣EfN − E′N fN ∣∣ 6≤ ξ 2 }\n=Pr      ∣∣∣ ∑N n=1 ( E[1]f̂ [1] − f̂ [1](z[1]n ) )∣∣∣ ...∣∣∣ ∑N n=1 ( E[M ]f̂ [M ] − f̂ [M ](z[M ]n ) )∣∣∣   6≤   Nξ[1] 2 ... Nξ[M] 2     \n≤Pr      ∑N n=1 ∣∣E[1]f̂ [1] − f̂ [1](z[1]n ) ∣∣\n...∑N n=1 ∣∣E[M ]f̂ [M ] − f̂ [M ](z[M ]n ) ∣∣\n  6≤   Nξ[1] 2 ...\nNξ[M]\n2\n    \n≤ M∑\nm=1\n∑\nΛ[m]⊂Λ\n φF(Λ [m], ξ) + N ∑ i∈Λ[m] E[i] {( E[i]f̂ [i] − f̂ [i](z[i]) )2} N2\n4 ∑ i∈Λ[m] (ξ[i])2\n \n= M∑\nm=1\n∑\nΛ[m]⊂Λ\n φF(Λ[m], ξ) + ∑ i∈Λ[m] 4Var[i] ( f̂ [i](z[i]) )\nN ∑\ni∈Λ[m] (ξ[i])2\n  (∗)\n≤ M∑\nm=1\n∑\nΛ[m]⊂Λ\n φF(Λ[m], ξ) + 4m(b− a)2\nN ∑\ni∈Λ[m] (ξ[i])2\n  , (52)\nwhere the step (∗) is followed from the fact that for each task Z [m] (1 ≤ m ≤ M), the samples {z[m]n }Nn=1 are independent. Hence, we get\n1{|EfN−EN fN |>ξ}\n 1−   M∑\nm=1\n∑\nΛ[m]⊂Λ\nφF(Λ [m], ξ) +\n4m(b− a)2\nN ∑\ni∈Λ[m] (ξ[i])2\n   \n≤Pr′ {∣∣E′N fN − EN fN ∣∣ > ξ 2 } . (53)\nTaking the expectation with respect to the sample collection {Z[m]N } M m=1 of the tasks Z [1], · · · ,Z [M ] and letting\nM∑\nm=1\n∑\nΛ[m]⊂Λ\n( φF(Λ [m], ξ) + 4m(b− a)2\nN ∑\ni∈Λ[m] (ξ[i])2\n) ≤ 1\n2 , (54)\nwe then have for any ξ > 0,\nPr { sup f∈F ∣∣Ef − EN f ∣∣ > ξ } ≤ 2Pr { sup f∈F ∣∣E′Nf − EN f ∣∣ > ξ 2 } .\nThis completes the proof."
    }, {
      "heading" : "C.5 Proof of Theorem B.2",
      "text" : "Proof of Theorem B.2. Let fN = (f̂1, · · · , f̂M) T be the vector-valued function achieving the supremum sup f∈F {∣∣Ef −EN f ∣∣}.\nSimilar to the proof of Theorem B.1, we have\n1{|EfN−EN fN |>ξ}Pr ′ {∣∣EfN − E′NfN ∣∣ ≤ ξ\n2\n} ≤Pr′ {∣∣E′N fN −EN fN ∣∣ > ξ\n2\n} . (55)\nAccording to Lemma B.2, we have\nPr′ {∣∣EfN −E′N fN ∣∣ 6≤ ξ 2 }\n≤Pr      ∑N n=1 ∣∣Ef̂1 − f̂1(z[1]n ) ∣∣\n...∑N n=1 ∣∣Ef̂M − f̂M (z[M ]n ) ∣∣\n  6≤   Nξ[1]/2 ...\nNξ[M ]/2\n    \n≤ M∑\nm=1\n∑\nΛ[m]⊂Λ   φF(Λ [m], ξ) + N ∑ i∈Λ[m] Var ( f̂ [i](z[i]) ) + 2N2 ∑ i1<i2 i1,i2∈Λ[m] Cov ( f̂ (i1)(z(i1)), f̂ (i2)(z(i2)) ) N2\n4 ( ∑ i∈Λ[m] ξ[i] )2\n \n= M∑\nm=1\n∑\nΛ[m]⊂Λ   φF(Λ [m], ξ) + 4 ∑ i∈Λ[m] Var ( f̂ [i](z[i]) ) N ( ∑\ni∈Λ[m] ξ[i]\n)2 +\n8 ∑ i1<i2\ni1,i2∈Λ[m]\nCov ( f̂ (i1)(z(i1)), f̂ (i2)(z(i2)) )\n( ∑ i∈Λ[m] ξ[i] )2\n \n≤ M∑\nm=1\n∑\nΛ[m]⊂Λ\n φF(Λ [m], ξ) + 4m(b− a)2\nN ( ∑\ni∈Λ[m] ξ[i]\n)2 +\n8 ∑\ni1<i2\ni1,i2∈Λ[m]\nCovF (i1, i2)\n( ∑ i∈Λ[m] ξ[i] )2\n  (56)\nMoreover, define\nΓ2 :=\nM∑\nm=1\n∑\nΛ[m]⊂Λ\nm(b− a)2 ( ∑\ni∈Λ[m] ξ[i]\n)2 ,\nand\nΥ2 :=\nM∑\nm=1\n∑\nΛ[m]⊂Λ\n8 ∑\ni1<i2\ni1,i2∈Λ[m]\nCovF (i1, i2)\n( ∑ i∈Λ[m] ξ[i] )2 .\nHence, we get\n1{|EfN−EN fN |>ξ}\n( 1− ( 4Γ2 N +Υ(Λ) + Υ2 )) ≤ Pr′ {∣∣E′N fN −EN fN ∣∣ > ξ 2 } . (57)\nTaking the expectation with respect to {Z[m]N } M m=1 and letting\n4Γ2 N +Υ(Λ) + Υ2 ≤ 1 2 , (58)\nwe then have for any ξ > 0\nPr { sup f∈F ∣∣Ef − EN f ∣∣ > ξ } ≤ 2Pr { sup f∈F ∣∣E′Nf − EN f ∣∣ > ξ 2 } .\nThis completes the proof."
    }, {
      "heading" : "C.6 Proof of Theorem 5.1",
      "text" : "Proof of Theorem 5.1. For any 1 ≤ m ≤ M , consider {ǫ[m]n }Nn=1 as independent Rademacher random variables, i.e., independent {±1}-valued random variables with equal probability of taking either value. Given an {ǫ[m]n }Nn=1 and a Z [m] 2N , denote\n−→ǫ [m] :=(ǫ[m]1 , · · · , ǫ [m] N ,−ǫ [m] 1 , · · · ,−ǫ [m] N ) T ∈ {±1}2N , 1 ≤ m ≤ M, (59)\nand for any f = (f1, · · · , fM)T ∈ F R c ,\n−→ f [m](Z\n[m] 2N ) :=\n( f [m](z′\n[m] 1 ), · · · , f [m](z′ [m] N ), f [m](z [m] 1 ), · · · , f [m](z [m] N )\n)T ∈ [a, b]2N . (60)\nAccording to Theorem B.1, given any ξ > 0 and for any N ∈ N satisfying Condition (34), we have\nPr { sup f∈FRc ∣∣Ef − EN f ∣∣ > ξ }\n≤2Pr { sup f∈FRc ∣∣E′N f − EN f ∣∣ > ξ 2 } (by Theorem B.2)\n=2Pr    sup f∈FRc   ∣∣∣ 1N ∑N n=1 ( f(z′[1]n )− f(z [1] n ) )∣∣∣ ...∣∣∣ 1N ∑N n=1 ( f(z′[M ]n )− f(z [M ] n ) )∣∣∣   >   ξ[1] 2 ... ξ[M] 2     \n=2Pr    sup f∈FRc   ∣∣∣ 1N ∑N n=1 ǫ [1] n ( f(z′[1]n )− f(z [1] n ) )∣∣∣ ...∣∣∣ 1N ∑N n=1 ǫ [M ] n ( f(z′[M ]n )− f(z [M ] n ) )∣∣∣   >   ξ[1] 2 ... ξ[M] 2     \n=2Pr    sup f∈FRc   ∣∣∣ 12N 〈−→ǫ [1],−→f 1(Z[1]2N ) 〉∣∣∣ ...∣∣∣ 12N 〈−→ǫ [M ],−→f M(Z[M ]2N ) 〉∣∣∣   >   ξ[1] 4 ... ξ[M] 4      . (61)\nFor any given sample collection {Z[m]2N } M m=1 of the tasks Z [1], · · · ,Z [M ], let Ωp,N(F R c , ξ/8) be the cover of FRc w.r.t. the radius-vectors ξ/8. Since F R c is composed of the functions with the range [a, b], we assume that the same holds for any h ∈ Ωp,N(F R c , ξ/8). If f† = (f [1] † , · · · , f [M ] † ) T is a vector-valued function that achieves\nsup f∈FRc\n  ∣∣∣ 12N 〈−→ǫ [1],−→f [1](Z[1]2N ) 〉∣∣∣ ...∣∣∣ 12N 〈−→ǫ [M ],−→f [M ](Z[M ]2N ) 〉∣∣∣   >   ξ[1] 4 ... ξ[M] 4   ,\nthere must be an h† = (h [1] † , · · · , h [M ] † ) T ∈ Ωp,N(F R c , ξ/8) such that, for any 1 ≤ m ≤ M ,\n1\n2N\nN∑\nn=1\n( |f [m]† (z ′[m] n )− h [m] † (z ′[m] n )|+ |f [m] † (z [m] n )− h [m] † (z [m] n )| ) < ξ[m]\n8 ,\nand meanwhile, ∣∣∣ 1 2N 〈−→ǫ [m],−→h [M ]† (Z [m] 2N ) 〉∣∣∣ > ξ [m] 8 .\nTherefore, we arrive at\nPr    sup f∈FRc   ∣∣∣ 12N 〈−→ǫ [1],−→f [1](Z[1]2N ) 〉∣∣∣ ...∣∣∣ 12N 〈−→ǫ [M ],−→f [M ](Z[M ]2N ) 〉∣∣∣   >   ξ[1] 4 ... ξ[M] 4     \n≤Pr    sup h∈Ωp,N (F R c ,ξ/8)   ∣∣∣ 12N 〈−→ǫ [1],−→h [1](Z[1]2N ) 〉∣∣∣ ...∣∣∣ 12N 〈−→ǫ [M ],−→h [M ](Z[M ]2N ) 〉∣∣∣   >   ξ[1] 8 ... ξ[M] 8      . (62)\nOn the other hand, given a ξ > 0 and for any N ∈ N satisfying Condition (34),\nPr    sup h∈Ωp,N (F R c ,ξ/8)   ∣∣∣ 12N 〈−→ǫ [1],−→h [1](Z[1]2N ) 〉∣∣∣ ...∣∣∣ 12N 〈−→ǫ [M ],−→h [M ](Z[M ]2N ) 〉∣∣∣   >   ξ[1] 8 ... ξ[M] 8     \n=Pr    sup h∈Ωp,N (F R c ,ξ/8)   ∣∣∣ 1N 〈−→ǫ [1],−→h [1](Z[1]2N ) 〉∣∣∣ ...∣∣∣ 1N 〈−→ǫ [M ],−→h [M ](Z[M ]2N ) 〉∣∣∣   >   ξ[1] 4 ... ξ[M] 4     \n=Pr { sup\nh∈Ωp,N (F R c ,ξ/8)\n∣∣E′Nh−ENh ∣∣ > ξ\n4\n} (similer to (61))\n≤Pr   \n∑\nh∈Ωp,N (F R c ,ξ/8)\n∣∣E′Nh−ENh ∣∣ > ξ\n4   \n≤Pr   \n∑\nh∈Ωp,N (F R c ,ξ/8)\n∣∣Eh−ENh ∣∣ + ∣∣Eh−E′Nh ∣∣ > ξ\n4   \n≤2Pr   \n∑\nh∈Ωp,N (F R c ,ξ/8)\n∣∣Eh− ENh ∣∣ > ξ\n8   \n≤2M+1N 1 ( FRc , ξ/8, 2N ) exp\n{ −N ∑M m=1(ξ [m])2\n32M2(b− a)2\n} . (63)\nThe last inequality of (63) is derived from Definition (4.2) and Theorem A.1. The combination of (61), (62) and (63) leads to the result: given any ξ > 0, there holds that for any N ∈ N satisfying Condition (34),\nPr { sup f∈FRc ∣∣Ef −EN f ∣∣ > ξ } ≤ 2M+2N 1 ( FRc , ξ/8, 2N ) exp { −N ∑M m=1(ξ [m])2 32M2(b− a)2 } .\nThis completes the proof."
    }, {
      "heading" : "C.7 Proof of Theorem 5.2",
      "text" : "Before the formal proof, we present a necessary lemma.\nLemma C.1 Let sn = (s [1] n , · · · , s [M ] n ) ∈ RM (1 ≤ n ≤ N) be N i.i.d. random vectors. Then, there holds that for any ξ = (ξ[1], · · · , ξ[M ])T > 0,\nPr\n{ N∑\nn=1\nsn ≤ Nξ\n} ≤ 2MPr { s1 ≤ 2ξ } . (64)\nProof. For any 1 ≤ m ≤ M , we have\nN∑\nn=1\ns[m]n ≥ N∑\nn=1\ns[m]n 1 {\ns [m] n >2ξ[m]\n} ≥ 2ξ[m] N∑\nn=1\n1{ s [m] n >2ξ[m] }.\nHence, it is followed from the conditional Markov inequality that\nPr\n{ N∑\nn=1\nsn ≤ Nξ\n} ≤Pr      2ξ[1] ∑N n=1 1 { s [1] n >2ξ[1] } ... 2ξ[M ] ∑N\nn=1 1 {\ns [M] n >2ξ[M]\n}\n  ≤ N   ξ[1]\n... ξ[M ]\n    \n=Pr      ∑N n=1 1 { s [1] n ≤2ξ[1] } ...∑N n=1 1 {\ns [M] n ≤2ξ[M]\n}\n  ≥ (1− 2 −1)N   1 ... 1     \n=Pr\n{ N∑\nn=1\n1{ s [1] n ≤2ξ[1]\n} ≥ 2−1N ∣∣ AM2 } Pr { AM2 }\n≤\nE {∑N n=1 1 {\ns [1] n ≤2ξ[1]\n} ∣∣ AM2 }\n2−1N Pr\n{ AM2 }\n≤ NPr\n{ s [1] 1 ≤ 2ξ [1] ∣∣ AM2 }\n2−1N Pr\n{ AM2 } = 2Pr { s [1] 1 ≤ 2ξ [1],AM2 } ,\nwhere AM2 stands for the event that {∑N\nn=1 1{s[m]n ≤2ξ[m]} }M m=2 . Then, following this way, we have\nPr\n{ N∑\nn=1\nsn ≤ Nξ\n} ≤ 2Pr { s [1] 1 ≤ 2ξ [1],AM2 } ≤ 22Pr { s [1] 1 ≤ 2ξ [1], s [2] 1 ≤ 2ξ [2],AM3 }\n≤ · · · ≤ 2MPr { s [1] 1 ≤ 2ξ [1], s [2] 1 ≤ 2ξ [2], · · · , s[M ]1 ≤ 2ξ [M ] } = 2MPr {s1 ≤ 2ξ} .\nThis completes the proof. Next, we come up with the proof of Theorem 5.2. Proof of Theorem 5.2. Let f̂∗ = (f [1] ∗ , · · · , f [M ] ∗ )T be the vector-valued function achieving the supremum supf∈FRc ∣∣Ef −EN f ∣∣. Then, it is followed from Lemma C.1 that\nPr { sup f∈FRc ∣∣Ef −EN f ∣∣ ≤ ξ } =Pr {∣∣Ef∗ −EN f∗ ∣∣ ≤ ξ } ≤ 2MPr {s∗ ≤ 2ξ}\n≤2M sup f∈FRc\nPr { s ≤ 2ξ } ,\nwhere s∗ = ( s [1] ∗ , · · · , s [M ] ∗ )T with s [m] ∗ := ∣∣E[m]f [m]∗ − f [m]∗ (z[m]) ∣∣ for any 1 ≤ m ≤ M ."
    }, {
      "heading" : "C.8 Proof of Theorem 5.3",
      "text" : "Proof of Theorem 5.3. Denote tN = (t [1], · · · , t[M ])T with t[i] := ∣∣E[i]f [i] − E[i]Nf [i] ∣∣ > ξ[i]. The event tN 6≤ ξ contains the following possibilities:\n• P [1]: there is only one index {i} = Λ[1] ⊂ Λ satisfying that t[i] > ξ[i];\n• P [m]: there arem (1 < m < M) indices {i[1], · · · , i[m]} = Λ[m] ⊂ Λ satisfying that t[ik] > ξ[ik]\n(1 ≤ k ≤ m);\n• P [M ]: t[m] > ξ[m] holds for any 1 ≤ m ≤ M .\nThus, we have Pr {tN 6≤ ξ} = Pr{P [1]}+ · · ·+ Pr{P [M ]}. (65)\nThen, the combination of Definition 3.2, Theorems 5.1&5.2 and (65) leads to the result (17).\nMoreover, since Pr {{ s[λ] ≤ 2ξ[λ] } λ∈Λ[m] } ≤ 1 holds for any Λ[m] ⊂ Λ, the result (19) can be directly obtained. This completes the proof."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "In this paper, we study the generalization performance of regularized multi-task learning<lb>(RMTL) in a vector-valued framework, where MTL is considered as a learning process for<lb>vector-valued functions. We are mainly concerned with two theoretical questions: 1) under<lb>what conditions does RMTL perform better with a smaller task sample size than STL? 2)<lb>under what conditions is RMTL generalizable and can guarantee the consistency of each<lb>task during simultaneous learning? In particular, we investigate two types of task-group<lb>relatedness: the observed discrepancy-dependence measure (ODDM) and the empirical<lb>discrepancy-dependence measure (EDDM), both of which detect the dependence between<lb>two groups of multiple related tasks (MRTs). We then introduce the Cartesian product-<lb>based uniform entropy number (CPUEN) to measure the complexities of vector-valued<lb>function classes. By applying the specific deviation and the symmetrization inequalities<lb>to the vector-valued framework, we obtain the generalization bound for RMTL, which<lb>is the upper bound of the joint probability of the event that there is at least one task<lb>with a large empirical discrepancy between the expected and empirical risks. Finally, we<lb>present a sufficient condition to guarantee the consistency of each task in the simultaneous<lb>learning process, and we discuss how task relatedness affects the generalization performance<lb>of RMTL. Our theoretical findings answer the aforementioned two questions.",
    "creator" : "LaTeX with hyperref package"
  }
}