{
  "name" : "1705.07171.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Nestrov’s Acceleration For Second Order Method",
    "authors" : [ "Haishan Ye", "Zhihua Zhang" ],
    "emails" : [ "yhs12354123@gmail.com", "zhzhang@math.pku.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1. Introduction\nOptimization has become an increasingly popular issue in machine learning. Many machine learning models can be reformulated as the following optimization problems:\nmin x∈Rd\nF (x) = 1\nn n∑ i=1 fi(x). (1)\nwhere each fi is the loss with respect to (w.r.t.) the i-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.\nIn the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the computational cost [4, 10, 14]. However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD [8, 16, 17, 20].\nFor the first-order methods which only make use of the gradient information, Nestrov’s acceleration technique is a very useful tool [11]. It greatly improves the convergence of gradient descent [11], proximal gradient descent [2, 12], and stochastic gradient with variance reduction [1, 9], etc.\nar X\niv :1\n70 5.\n07 17\n1v 1\n[ cs\n.L G\nRecently, second-order methods have also received great attention due to their high convergence rate. However, conventional second-order methods are very costly because they take heavy computational cost to obtain the Hessian matrices. To conquer this weakness, one proposed a sub-sampled Newton which only samples a subset of functions fi randomly to construct a sub-sampled Hessian [15, 3, 18] . Pilanci and Wainwright [13] applied the sketching technique to alleviate the computational burden of computing Hessian and brought up sketch Newton. Regularized sub-sampled Newton methods were also devised to deal with the ill-condition problem [5, 15].\nIn the latest work, Ye et al. [19] cast these stochastic second-order procedures into a so-called approximate Newton framework. They showed that if approximate Hessian H(t) satisfies\n[1− (1− π)]∇2F (x(t)) H(t) [1 + (1− π)]∇2F (x(t)), (2)\nwhere 0 < π < 1, then approximate Newton converges with rate 1−π. If H(t) is a poor approximation like π = 1/κ, where κ is the condition number of object function F (x), then approximate Newton has the same convergence rate with gradient descent.\nSince approximate Newton converges with a linear rate, it is natural to ask whether approximate Newton can be accelerated just like gradient descent. If it can be accelerated, can the convergence rate be promoted to 1− √ π compared to original 1−π? In this paper, we aim to introduce Nestrov’s acceleration technique to promote the performance of second-order methods, specifically approximate Newton.\nWe summarize our work and contribution as follows:\n• First, we introduce Nestrov’s acceleration technique to improve the convergence rate of the stochastic second-order methods (approximate Newton). This acceleration is very important especially when n and d are close to each other and object function in question is ill-conditioned. In these cases, it is very hard to construct a good approximate Hessian with low cost.\n• Our theoretical analysis shows that by Nestrov’s acceleration, the convergence rate of approximate Newton can be improved to 1 − √ π from original rate 1 − π where 0 < π < 1 when\nthe object function is quadratic. For general smooth convex functions, we also show that the similar acceleration also holds when the initial point is close to the optimal point.\n• We empirically validate our theory about accelerated second-order algorithms. Our experimental study shows that Nestrov’s acceleration technique can improve approximate Newton methods effectively. Our experiments also reveal a fact that adding curvature information properly can always improve the algorithm’s convergence performance.\n• We propose an accelerated regularized sub-sampled Newton. Compared with state-of-art stochastic first-order methods, our algorithm shows competitive or even better performance. This demonstrates the efficiency of the accelerated second-order method.\nThe remainder of this paper is organized as follows. Section 2 defines notation and introduces preliminaries will be used in this paper. We describe and analyze accelerated second-order methods in detail in Section 3. In Section 4, we propose accelerated regularized sub-sampled Newton and validate our theory empirically. Finally, we conclude our work in Section 5. All proofs are in Appendix.\n2. Notation and Preliminaries\nWe first introduce notation that will be used in this paper. Then, we give some properties of object function that will be used.\nAlgorithm 1 Accelerated Second Order Method. 1: Input: x(0) and x(1) are initial points sufficient close to x∗. θ is the momentum parameter with 0 < θ < 1. H is an approximate Hessian matrix of ∇2F (x∗) with ‖I − [∇2F (x∗)] 1 2H−1[∇2F (x∗)] 1 2 ‖ = 1− π with\n0 < π < 1. 2: for t = 1, . . . until termination do 3: y(t+1) = (1 + θ)x(t) − θx(t−1); 4: x(t+1) = y(t+1) −H−1∇F (y(t+1)). 5: end for\nAlgorithm 2 Extended Accelerated Second Order Method. 1: Input: x(0) and x(1) are initial points sufficient close to x∗. θ is the momentum parameter with 0 < θ < 1.\n2: for t = 1, . . . until termination do 3: y(t+1) = (1 + θ)x(t) − θx(t−1). 4: Construct H(t+1) as an approximation of ∇2F (y(t+1)) satisfying Eqn. (5). 5: x(t+1) = y(t+1) − [ H(t+1) ]−1 ∇F (y(t+1)).\n6: end for\n2.1 Notation\nGiven a matrix A = [aij ] ∈ Rm×n of rank ` and a positive integer k ≤ `, its SVD is given as A = UΣV T = UkΣkV T k +U\\kΣ\\kV T \\k, where Uk and U\\k contain the left singular vectors of A, Vk and V\\k contain the right singular vectors of A, and Σ = diag(σ1, . . . , σ`) with σ1 ≥ σ2 ≥ · · · ≥ σ` > 0 are the nonzero singular values of A. Additionally, ‖A‖ , σ1 is the spectral norm. If A is positive semidefinite, then U = V and the eigenvalue decomposition of A is the same to singular value decomposition. It also holds that λi(A) = σi(A), where λi(A) is the i-th largest eigenvalue of A. Let λmax(A) and λmin(A) denote the largest and smallest eigenvalue of A, respectively.\n2.2 Assumptions\nIn this paper, we focus on the problem described in Eqn. (1). Moreover, we will make the following two assumptions.\nAssumption 1 The objective function F is µ-strongly convex, that is,\nF (y) ≥ F (x) + [∇F (x)]T (y − x) + µ 2 ‖y − x‖2, for µ > 0.\nAssumption 2 ∇F (x) is L-Lipschitz continuous, that is,\n‖∇F (x)−∇F (y)‖ ≤ L‖y − x‖, for L > 0.\nBy Assumptions 1 and 2, we define the condition number of function F (x) as: κ , Lµ . Besides, we will also use the nation of Lipschitz continuity of ∇2F (x) in this paper. We say ∇2F (x) is L̂-Lipschitz continuous if\n‖∇2F (x)−∇2F (y)‖ ≤ L̂‖y − x‖, for L̂ > 0.\n3. Accelerated Second-Order Methods\nIn this section, we apply Nesterov’s acceleration technique to second-order methods and present two accelerated second-order methods in Algorithms 1 and 2. Just like conventional second-order\nmethods, we assume the initial points x(0) and x(1) are sufficient close to the optimal point x∗ in our algorithms.\nIn Algorithm 1, H is a fixed approximate Hessian such that\n‖I − [∇2F (x∗)] 12H−1[∇2F (x∗)] 12 ‖ = 1− π,\nwhere 0 < π < 1. And we update sequence x(t) as follows,{ y(t+1) = (1 + θ)x(t) − θx(t−1), x(t+1) = y(t+1) −H−1∇F (y(t+1)),\n(3)\nwhere θ is chosen in terms of the value of π. We can see that the iteration (3) is much like the update procedure of Nestrov’s accelerated gradient descent but replacing step size with H−1.\nRather fixing approximate Hessian H, we can also construct an approximate Hessian H(t) for each t. If H(t) does not vary heavily, then Nesterov’s acceleration technique still works fine. In this case, we update sequence x(t) as follows y (t+1) = (1 + θ)x(t) − θx(t−1), x(t+1) = y(t+1) − [ H(t+1) ]−1 ∇F (y(t+1)), (4)\nwhere H(t+1) is an approximation of ∇2F (y(t+1)) such that\n(1− (1− π))H(t+1) ∇2F (y(t+1)) (1 + (1− π))H(t+1). (5)\nThe detailed description is depicted in Algorithm 2. If we set θ = 0, the above iteration will reduce to x(t+1) = x(t)− [H(t)]−1∇F (x(t)), where H(t) is an approximation of ∇2F (x(t)). Then Algorithm 2 will become the approximate Newton method defined in [19].\nIn fact, we can regard Algorithm 2 as an extension of Algorithm 1. If H(t) varies very slowly, then the convergence properties of Algorithms 2 and 1 are close.\n3.1 Theoretical Analysis\nWe now discuss the convergence properties of Algorithm 1 and Algorithm 2. We will prove the theoretical convergence properties of Algorithm 1 applied to a quadratic object function, i.e., the least square regression. For a general smooth convex object function, it can be approximated by quadratic functions in a region close to optimal point. Hence, it can result in a similar convergence property with the quadratic objective function.\nBefore convergence analysis, we give an important lemma which is closely related to the convergence behavior of the accelerated second-order methods.\nLemma 1 Let Assumptions 1 and 2 hold. Suppose that ∇2F (x) exists and is continuous in a neighborhood of a minimizer x∗. Let H(t) be an approximation of ∇2F (y(t)). Consider the iteration (4). If x(t) is sufficient close to x∗ then we have the following result\n[∇2F (x∗)]− 12∇F (x(t+1)) = ( I − [∇2F (x∗)] 12 [H(t+1)]−1[∇2F (x∗)] 12 ) × ( (1 + θ) [ ∇2F (x∗)\n]− 12 ∇F (x(t))− θ [∇2F (x∗)]− 12 ∇F (x(t−1))) + o(∇F (x(t))) + o(∇F (x(t−1))).\nAlgorithm 3 Accelerated Regularized Subsample Newton. 1: Input: x(0), 0 < δ < 1, regularizer parameter α, sample size |S|, acceleration parameter θ(t) ; Let y(0) = x(0)\n2: for t = 0, . . . until termination do 3: Select a sample set S, of size |S| and H(t) = 1|S| ∑ j∈S ∇ 2fj(x (t)) + αI; 4: Update y(t+1) = x(t) − [H(t)]−1∇F (x(t)); 5: Update x(t+1) = y(t+1) + θ(t)(y(t+1) − y(t)) 6: end for\nFurthermore, if ∇2F (x) is L̂-Lipschitz continuous, then the above result holds whenever x(t) satisfies\n‖x(t) − x∗‖ ≤ o(1) L̂ .\nFrom Lemma 1, we can see that the convergence property of the second-order methods are mainly determined by I − [∇2F (x∗)] 12 [H(t+1)]−1[∇2F (x∗)] 12 and θ.\nLemma 1 describes such a fact that if x(t−1) and x(t) are sufficient close to x∗, that is, o(F (x(t))) and o(F (x(t−1))) are very small, then the convex function can be well approximated by a quadratic function. Therefore, we will demonstrate the convergence analysis of Algorithm 1 on the least square regression problem. The other quadratic functions can be analyzed in the same way and the convergence rate is also the same.\nThe least square regression is defined as follows\nF (x) = ‖Ax− b‖2, (6)\nwhere A ∈ Rn×d is of full column rank. Because the Hessian of F (x) is ATA, the Lipschitz constant of ∇2F (x) is zero. Hence, the result of Lemma 1 degenerates to\n[∇2F (x∗)]− 1 2∇F (x(t+1)) = ( I − [∇2F (x∗)] 1 2 [H(t+1)]−1[∇2F (x∗)] 1 2 ) × ( (1 + θ) [ ∇2F (x∗) ]− 1 2 ∇F (x(t))− θ [ ∇2F (x∗) ]− 1 2 ∇F (x(t−1)) ) .\nThis equation describes a linear dynamic system which contains the convergence property of Algorithm 1 applied to the least square regression problem. That is,\nTheorem 2 For the least square regression problem (6), we solve it by Algorithm 1 with θ = 1− √ π\n1+ √ π .\nThen after t iterations, we have ‖∇F (x(t))‖ ≤ √ λmax(∇2F (x∗))(1− √ π)t |ξ + ϕt| ,\nwhere ξ and ϕ are constants determined by initial points x(0) and x(1).\nRemark 3 Theorem 2 assumes that the Hessian matrix ATA is positive definite, i.e., A is of full column rank. If A is not full column rank or even if d > n, we can alternatively consider the ridge regression problem:\nmin x ‖Ax− b‖2 + γ‖x‖2, for γ > 0.\nRight now the Hessian matrix ATA+ γI is positive definite and is constant. Thus, Theorem 2 still holds.\nRemark 4 In real applications, it is hard to get the exact value of π in Theorem 2. However, from the proof of Theorem 2, we can see that if θ is close to 1− √ π\n1+ √ π , then the convergence rate is also close\nto 1− √ π.\nFrom Theorem 2, we can see that if we choose θ = 1− √ π\n1+ √ π , then the convergence rate of Algorithm 1\nis 1 − √ π in contrast to 1 − π of the conventional approximate Newton method. If we choose H = (1/L)I and θ = √ L−√µ√ L+ √ µ , where L = λmax(ATA) and µ = λmin(ATA), then Theorem 2 shows\nthat the convergence rate of Nestrov’s acceleration for the least square regression problem is 1− √ µ/L.\nSince a smooth convex function can be well approximated by a quadratic function once x(t) gets into the region close enough to the optimal point, in this case the convergence behavior of Algorithm 1 is also similar to that in Theorem 2.\nAlgorithm 2 is almost the same with Algorithm 1 except the approximate Hessian H(t) varying as t. Hence, Algorithm 2 will bring in more perturbation. If matrix H(t) does not vary heavily, then Nesterov’s acceleration technique still works fine. And Algorithm 2 has the similar convergence behavior with Algorithm 1.\nIn Appendix E, we will analysis the influence of perturbation to convergence properties of Algorithms 1 and 2. We will show that if the perturbation is small, then the convergence properties of Algorithms 1 and 2 are close to the analysis in Theorem 2.\n3.2 Accelerated Regularized Sub-sampled Newton\nIn Section 3.1, we have proposed the theoretical analysis for accelerated second-order methods. Based on the theoretical analysis, we now devise a concrete accelerated second-order method that we call accelerated regularized sub-sampled Newton.\nRegularized sub-sampled Newton (RegSN, Algorithm 4 in Appendix) is an effective alternative to reduce the sample size of sub-sampled Newton when the objective function is ill-conditioned. From Theorem 5 in Appendix B, we can see that RegSN achieves a linear convergence rate if sample size |S| and regularizer α are properly chosen. However, RegSN has its own weakness. We can also observe that it needs a large sample size |S| to achieve a fast convergence rate when K/σ is large, where K and σ are defined in Appendix B.\nTherefore, we apply Nestrov’s acceleration technique to RegSN, giving the accelerated regularized sub-sampled Newton (AccRegSN) in Algorithm 3. We can see that Algorithm 3 provides a concrete construction of the approximate Hessian. Hence, AccRegSN is an implementation of Algorithm 2. Our theoretical analysis shows that AccRegSN has better performance than RegSN if these two algorithms share the same sample size |S| and regularizer α. Hence, we can construct a poor approximate Hessian very efficiently when K/σ is large. At the same time, AccRegSN still has a fast convergence rate.\nBesides, since AccRegSN makes use of more curvature information than Nestrov’s accelerated gradient descent (AGD) method [11], AccRegSN should converge faster than AGD theoretically.\n4. Experiments\nIn Section 3.1, we have shown that Nestrov’s acceleration technique can improve the performance of approximate Newton method theoretically. In this section, we will validate our theory empirically. In particular, we first compare AccRegSN with RegSN on the least square regression to validate the theoretical analysis in Section 3.1. Then we conduct more experiments on a popular machine learning problem called Ridge Logistic Regression, and compare AccRegSN with other state-of-art algorithms.\nThe least square regression is defined in Eqn. (6). In our experiments, A is a 3000× 1000 random matrix with the i.i.d. entries from U(0, 1), where U(0, 1) means uniform distribution on [0, 1]. And b is a 3000× 1 random Gaussian vector of the i.i.d. entries from N(0, 1). The condition number of A is 128.855.\nIn experiments, we set the sample size |S| to be 0.5%n, 5%n and 10%n. The regularizer α is properly chosen according to |S|. AccRegSN and RegSN share the same |S| and α. And θ in AccRegSN is fixed and appropriately selected. We report the experiments result in Figure 1.\nFrom Figure 1, we can see that AccRegSN and RegSN have significant difference in convergence rate and AccRegSN is much faster. This validates the analysis in Section 3. Besides, we can also observe that AccRegSN runs faster as sample size |S| increases. When |S| = 10%n, AccRegSN takes only less than 4000 iterations to achieve an e−30 error while it needs 5000 iterations to get an e−25 error if |S| = 0.5n%.\n4.2 Experiments on the Ridge Logistic Regression\nWe conduct experiments on the Ridge Logistic Regression problem whose objective is\nF (x) = 1\nn n∑ i=1 log[1 + exp(−bi〈ai, x〉)] + λ 2 ‖x‖2, (7)\nwhere ai ∈ Rd is the i-th input vector, and bi ∈ {−1, 1} is the corresponding label. We choose λ = 1n in our experiments.\nWe conduct our experiments on six datasets: ‘gisette’, ‘protein’, ‘svhn’, ‘rcv1’, ‘sido0’, and ‘real-sim’. The first three datasets are dense and the last three ones are sparse. We give the detailed description of the datasets in Table 1. Notice that the size and dimension of dataset are close to each other, so the sketch Newton method [13, 18] can not be used. We compare Algorithm 3 (AccRegSN) with RegSN (Algorithm 4), AGD and SVRG.\nIn our experiments, the sample size |S| and regularizer α of RegSN and AccRegSN are chosen according to Theorem 5. For a fixed |S|, a proper α can be found after several tries. In our experiments, AccRegSN and RegSN pick samples uniformly.\nThe current sub-sampled Hessian H(t) constructed in Algorithm 3 can be written as\nH(t) = ÃT Ã+ (α+ λ)I,\nwhere Ã ∈ R`×d, where ` < n. Notice that if ` < d, we can resort to Woodbury’ identity to compute the inverse of H(t) efficiently. Furthermore, if Ã is sparse, we can use conjugate gradient (Algorithm 5 in Appendix) to obtain an approximation of [H(t)]−1∇F (x(t)) which exploits the sparsity of Ã. In our experiments on sparse datasets, we set tol = 0.01‖∇F (x(t))‖ for conjugate gradient (Algorithm 5 ).\nFor the momentum parameter θ, it is hard to get the best value for AccRegSN just like AGD. However, our theoretical analysis implies that for large sample size |S|, a small θ should chosen. In our experiments, we set θ(t) = tt+16 in Algorithm 3 for the dense datasets and θ\n(t) = tt+30 for the sparse datasets. We set x(0) = 0 for all the datasets and all the algorithms.\nWe report our result in Figure 2. The flops are computed as Appendix A. We can see that AccRegSN converges much faster than RegSN when these two algorithms have the same sample size. This shows Nestrov’s acceleration technique can promote the performance of regularized sub-sampled Newton effectively. We can also observe that AccRegSN outperforms AGD significantly even when the sample size S is 1%n or even less. This validates the fact that adding curvature information is an effective way to improve the ability of accelerated gradient descent.\nCompared with SVRG, we can see that AccRegSN also has better performance on most of the datasets. Specifically, AccRegSN performs much better than SVRG on ‘svhn’. This means that AccRegSN is an efficient algorithm. Furthermore, we can observe that AccRegSN is very robust. It works well in different sample sizes.\nThe experiments also reveal that AccRegSN outperforms the other algorithms especially on datasets that RegSN performs very poor. In fact, poor performance of RegSN means the problem is ill-conditioned. This shows that AccRegSN has advantages when the problem is ill-conditioned. In summary, AccRegSN is good choice in practice.\n4.3 Conclusion of Empirical Study\nThe above experiments show that Nestrov’s acceleration is an effective way to promote the convergence rate of approximate Newton methods. The experiments also show that adding some curvature information always help AGD to obtain a faster convergence rate. Compared with SVRG, AccRegSN still has its own advantages even AccRegSN just picks the training samples uniformly in constructing the approximate Hessian. Therefore, we can conclude that the accelerated second-order method is efficient for a smooth convex object function. In fact, AccRegSN is just a simple demonstration of accelerated second order methods. Obviously, a better sampling strategy in constructing the approximate Hessian can further improve the performance of AccRegSN.\n5. Conclusion\nIn this paper, we have exploited Nestrov’s acceleration technique to promote the performance of second-order methods, specifically approximate Newton. We have presented the theoretical analysis on the convergence properties of accelerated second-order methods, showing that accelerated approximate Newton has higher convergence rate, especially when the approximate Hessian is not a good approximation. Based on our theory, we have developed AccRegSN. Our experiments have shown that our AccRegSN performs much better than the conventional RegSN, which meets our theory well. AccRegSN also has several advantages over other state-of-art algorithms, demonstrating the efficiency of accelerated second-order methods.\nReferences\n[1] Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. arXiv preprint arXiv:1603.05953, 2016.\n[2] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183–202, 2009.\n[3] Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic hessian information in optimization methods for machine learning. SIAM Journal on Optimization, 21 (3):977–995, 2011.\n[4] Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. Better mini-batch algorithms via accelerated gradient methods. In Advances in neural information processing systems, pages 1647–1655, 2011.\n[5] Murat A Erdogdu and Andrea Montanari. Convergence rates of sub-sampled newton methods. In Advances in Neural Information Processing Systems, pages 3034–3042, 2015.\n[6] Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU Press, 2012.\n[7] I. Guyon. Sido: A phamacology dataset. URL http://www.causality.inf.ethz.ch/data/ SIDO.html.\n[8] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315–323, 2013.\n[9] Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. arXiv preprint arXiv:1507.02000, 2015.\n[10] Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. Efficient mini-batch training for stochastic optimization. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 661–670. ACM, 2014.\n[11] Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372–376, 1983.\n[12] Yurii Nesterov et al. Gradient methods for minimizing composite objective function, 2007.\n[13] Mert Pilanci and Martin J. Wainwright. Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence. SIAM Journal on Optimization, 27(1):205–245, 2017.\n[14] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400–407, 1951.\n[15] Farbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled newton methods ii: Local convergence rates. arXiv preprint arXiv:1601.04738, 2016.\n[16] Nicolas L Roux, Mark Schmidt, and Francis R Bach. A stochastic gradient method with an exponential convergence _rate for finite training sets. In Advances in Neural Information Processing Systems, pages 2663–2671, 2012.\n[17] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. arXiv preprint arXiv:1309.2388, 2013.\n[18] Peng Xu, Jiyan Yang, Farbod Roosta-Khorasani, Christopher Ré, and Michael W Mahoney. Sub-sampled newton methods with non-uniform sampling. In Advances in Neural Information Processing Systems, pages 3000–3008, 2016.\n[19] Haishan Ye, Luo Luo, and Zhihua Zhang. A unifying framework for convergence analysis of approximate newton methods. arXiv preprint arXiv:1702.08124, 2017.\n[20] Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. Linear convergence with condition number independent access of full gradients. In Advance in Neural Information Processing Systems 26 (NIPS), pages 980–988, 2013.\nAppendix A. Computation cost of matrix operations\nWe will give the computation cost of basic matrix operations, the result can be found in Matrix Computation [6], or can be calculated easily.\nFor matrix multiplication, given dense matrices B ∈ Rm×n and C ∈ Rn×k, the basic cost of the matrix product B × C is 2mnk flops. It costs 2k · nnz(B) flops for the matrix product B × C when B is sparse, where nnz(B) denotes the number of nonzero entries of B.\nA linear equation with positive matrix can be solved efficiently by Cholesky decomposition and back substitution. Cholesky decomposition of a positive-definite matrix A ∈ Rn×n costs n3/3 flops. To get a solution of n× n triangular system, it needs n2 flops.\nAppendix B. Regularized Sub-sampled Newton\nWe assume that each fi(x) and F (x) in (1) have the following properties:\nmax 1≤i≤n\n‖∇2fi(x)‖ ≤ K <∞, (8)\nλmin(∇2F (x)) ≥ σ > 0. (9)\nThe regularized Sub-sampled Newton method is depicted in Algorithm 4, and we now give its local convergence properties in the following theorem [19].\nTheorem 5 Let F (x) satisfy Assumption 1 and 2. Assume Eqns. (8) and (9) hold, and let 0 < δ < 1, 0 ≤ 1 < 1 and 0 < α be given. Assume β is a constant such that 0 < β < α + σ2 , the subsampled size |S| satisfies |S| ≥ 16K\n2 log(2d/δ) β2 , and H (t) is constructed as in Algorithm 4. Define\n0 = max\n( β − α\nσ + α− β ,\nα+ β\nσ + α+ β\n) ,\nwhich implies that 0 < 0 < 1. And we define ‖x‖M∗ = ‖[M∗]− 1 2x‖. Then Algorithm 4 has the following convergence properties:\n1. There exists a sufficient small value γ, 0 < ν(t) < 1, and 0 < η(t) < 1 such that when ‖x(t) − x∗‖ ≤ γ, each iteration satisfies\n‖∇F (x(t+1))‖M∗ ≤ ( 0 + 2η(t)\n1− 0\n) 1 + ν(t)\n1− ν(t) ‖∇F (x(t))‖M∗ .\nBesides, ν(t) and η(t) will go to 0 as x(t) goes to x∗.\n2. If ∇2F (x(t)) is also Lipschitz continuous with parameter L̂ and x(t) satisfies\n‖x(t) − x∗‖ ≤ µ L̂κ ν(t),\nwhere 0 < ν(t) < 1, then it holds that\n‖∇F (x(t+1))‖M∗ ≤ 0 1 + ν(t)\n1− ν(t) ‖∇F (x(t))‖M∗ +\n2 (1− 0)2 L̂κ µ √ µ\n(1 + ν(t))2\n1− ν(t) ‖∇F (x(t))‖2M∗ .\nAlgorithm 4 Regularized Subsample Newton. 1: Input: x(0), 0 < δ < 1, regularizer parameter α, sample size |S| ; 2: for t = 0, 1, . . . until termination do 3: Select a sample set S, of size |S| and H(t) = 1|S| ∑ j∈S ∇ 2fj(x (t)) + αI;\n4: Update x(t+1) = x(t) − [ H(t) ]−1 ∇F (x(t));\n5: end for\nAppendix C. Proof of Lemma 1\nProof By Taylor’s theorem, we have\n∇F (x(t+1)) =∇F (y(t+1)) +∇2F (y(t+1))(−p(t+1)) + o(p(t+1)) =∇F (y(t+1))−∇2F (y(t+1))[H(t+1)]−1∇F (y(t+1)) + o(p(t+1)) =∇F (y(t+1))−∇2F (x∗)[H(t+1)]−1∇F (y(t+1))− (∇2F (x∗)−∇2F (y(t+1)))[H(t+1)]−1∇F (y(t+1)) + o(p(t+1))\n= [ ∇2F (x∗) ] 1 2 ( I − [∇2F (x∗)] 12 [H(t+1)]−1[∇2F (x∗)] 12 ) [ ∇2F (x∗) ]− 12 ∇F (y(t+1)) + (∇2F (y(t+1))−∇2F (x∗))[H(t+1)]−1∇F (y(t+1)) + o(p(t+1)).\nFor ∇F (y(t+1)), we have\n∇F (y(t+1)) =∇F (x(t) + θs(t)) =∇F (x(t)) + θ∇2F (x(t))(s(t)) + o(s(t)) =∇F (x(t)) + θ(∇F (x(t))−∇F (x(t−1))) + o(s(t)) =(1 + θ)∇F (x(t))− θ∇F (x(t−1)) + o(s(t)).\nBesides, we have\no(s(t)) =o(x(t) − x(t−1)) = o(x(t) − x∗ − (x(t−1) − x∗)) =o(∇F (x(t))) + o(∇F (x(t−1))),\nwhere the last equality is because ∇F (x) is L-Lipschitz continuous. Hence, it holds that\n∇F (y(t+1)) = (1 + θ)∇F (x(t))− θ∇F (x(t−1)) + o(∇F (x(t)) +∇F (x(t−1))) (10)\nFor (∇2F (y(t+1)) − ∇2F (x∗))[H(t+1)]−1∇F (y(t+1)), we show that it is of order o(∇F (y(t+1))) as follows. First, if ∇2F (x) is not Lipschitz continuous, then there exists a γ such that when ‖y − x∗‖ ≤ γ, it holds that\n‖∇2F (y)−∇2F (x∗)‖ = o(1).\nSuch γ exists because ∇2F (x) is continuous near optimal point x∗. Hence, (∇2F (y(t+1)) − ∇2F (x∗))[H(t+1)]−1∇F (y(t+1)) is of order o(∇F (y(t+1))) when y(t+1) is sufficient close to x∗.\nIf ∇2F (x) is L̂-Lipschitz continuous and F (x) is µ-strongly convex, then we have\n‖∇2F (y(t+1))−∇2F (x∗)‖ ≤ L̂‖y(t+1) − x∗‖ ≤ L̂ µ ‖∇F (y(t+1))‖.\nThen, it holds that\n‖(∇2F (y(t+1))−∇2F (x∗))[H(t+1)]−1∇F (y(t+1))‖ = O(‖∇F (y(t+1))‖2).\nAlgorithm 5 Conjugate Gradient Descent Method. 1: Input: A, b,x0, and tol; 2: Set r0 = Ax0 − b, p0 = −r0, k = 0; 3: while ‖rk‖ > tol do 4: Calculate αk =\nrTk rk pT k Apk\n; 5: Calculate xk+1 = xk + αkpk and rk+1 = rk + αkApk; 6: Calculate βk+1 = rTk+1rk+1\nrT k rk and pk+1 = −rk+1 + βk+1pk; 7: k = k + 1; 8: end while 9: Output: xk.\nBesides, because of p(t+1) = [H(t+1)]−1∇F (y(t+1)), we have\no(p(t+1)) = o(∇F (y(t+1))).\nCombining Eqn. (10), we have\n(∇2F (y(t+1))−∇2F (x∗))[H(t+1)]−1∇F (y(t+1)) = o(∇F (x(t))) + o(∇F (x(t−1)))\nand\no(p(t+1)) = o(∇F (x(t))) + o(∇F (x(t−1))).\nHence, we have the following result\n[∇2F (x∗)]− 12∇F (x(t+1)) = ( I − [∇2F (x∗)] 12 [H(t+1)]−1[∇2F (x∗)] 12 )( (1 + θ) [ ∇2F (x∗) ]− 12 ∇F (x(t))− θ [∇2F (x∗)]− 12 ∇F (x(t−1))) + o(∇F (x(t))) + o(∇F (x(t−1))).\nAppendix D. Proof of Theorem 2\nWe first give the following lemma which describes the solution form of the second-order difference equation.\nLemma 6 For the following homogeneous second-order difference equation\nz(t+2) + az(t+1) + bz(t) = 0,\nwe have 1. If a2 > 4b, and α = −(1/2)a+ √ (1/4)a2 − b and β = −(1/2)a− √\n(1/4)a2 − b are two solution of equation x2 + ax+ b = 0, then the solution of above difference equation is of the form\nz(t) = ξαt + ϕβt\n2. If a2 = 4b, let α = −(1/2)a, then the solution of above difference equation is of the form\nz(t) = (ξ + ϕt)αt.\n3. If a2 < 4b, then the solution of above difference equation is of the form\nz(t) = ξαt cos(ωt+ ϕ),\nwhere α = √ b and ω = arccos(−a/(2 √ b)).\nξ and ϕ are two coefficients determined by initial value z(0) and z(1).\nProof of Theorem 2 First, we have\n[∇2F (x∗)]− 12∇F (x(t+1)) = ( I − [∇2F (x∗)] 12H−1[∇2F (x∗)] 12 )( (1 + θ) [ ∇2F (x∗) ]− 12 ∇F (x(t))− θ [∇2F (x∗)]− 12 ∇F (x(t−1))) . Let UΛUT be the svd decomposition of I− [∇2F (x∗)] 12H−1[∇2F (x∗)] 12 , then we have Λ(1, 1) = 1−π. We denote Z(t+1) = UT [∇2F (x∗)]− 12∇F (x(t+1)), then we have the following difference equations\nZ(t+1) = (1 + θ)ΛZ(t) − θΛZ(t−1).\nSince Λ is a diagonal matrix, we have\nz (t+1) i = λi(1 + θ)z (t) i − λiθz (t−1) i , (11)\nwhere z(t)i is the i-th entry of Z (t) and λi = Λ(i, i). Equation (11) is a second-order difference equation. And its solution depends on θ and λi. We first consider case i = 1 with Λ(1, 1) = 1− π. By Lemma 6, we have\n1. If θ < 1− √ π\n1+ √ π , then we have\nz (t) 1 = ξα t + ϕβt,\nwhere\nα = (1− π)(1 + θ) +\n√ (1− π)2(1 + θ)2 − 4(1− π)θ\n2 (12)\nand\nβ = (1− π)(1 + θ)−\n√ (1− π)2(1 + θ)2 − 4(1− π)θ\n2 . (13)\n2. If θ = 1− √ π\n1+ √ π , then we have\nz (t) 1 = (ξ + ϕt)α t\nwith α = 1− √ π.\n3. If θ > 1− √ π\n1+ √ π , then we have\nz (t) 1 = ξα t cos(ωt+ ϕ), with α = √ (1− π)θ and ω = arccos((1− π)(1 + θ)/(2 √ (1− π)θ)).\nξ and ϕ are coefficients determined by z(0)1 and z (1) 1 .\nNow, we consider the i 6= 1 cases. We denote γi = Λ(i, i) with i = 2, ..., d. We have γi ≤ 1− π because 1 − π is the largest singular value of I − [∇2F (x∗)] 12H−1[∇2F (x∗)] 12 . Similar to above analysis, we can calculate the dominating convergence rate αi for z (t) i with fixed θ. It is easy to check that αi ≤ α when θ is set. Hence, the convergence property of Z(t) is mainly decided by sequence z (t) 1 .\nBy choosing θ = 1− √ π\n1+ √ π , after t iterations, we have\nz (t) 1 = (ξ + ϕt)(1−\n√ π)t.\nHence, it holds that ‖Z(t)‖ ≤ |ξ + ϕt|(1− √ π)t. By the definition of Z(t), we obtain the result that\n‖∇F (x(t))‖ ≤ √ λmax(∇2F (x∗))|ξ + ϕt|(1− √ π)t,\nwhere ξ and ϕ are determined by the initial points.\nAppendix E. Convergence Property of Accelerated Second Order Method\nWe will discuss the convergence properties of accelerated second-order methods applying to general smooth convex functions. The idea behind this section is that general smooth convex function can be represent by a quadratic function plus a small perturbation when x(t) is close enough to the optimal point. Algorithm 2 can be regarded as a disturbed version of Algorithm 1.\nTherefore, the convergence behavior of general smooth convex functions can be described by a disturbed second-order difference equation formulated as\nz(t+1) − (1− π)(1 + θ(t+1))z(t) + θ(t+1)(1 + η(t))(1− π)z(t−1) = 0. (14)\nIn Eqn. (14), π is fixed and θ(t) and η(t) vary as t. In fact, a perturbation on π can reduce to a perturbation on θ and η. Hence, we will analyze the influence of the perturbation on θ and η.\nE.1 Intuition\nFor concise representation, we represent Eqn. (14) as\nz(t) − (1− π)(1 + θ)z(t−1) + θ(1 + η)(1− π)z(t−2) = 0, (15)\nand the sequence Ẑ(t) satisfies Eqn. (15). Then θ and η are disturbed by and δ relatively, the difference equation is formalized as\nz(t+1) − (1− π)(1 + (1 + )θ)z(t) + (1 + η)(1 + δ)(1 + )θ(1− π)z(t−1) = 0, (16)\nand the sequence Z(t) which satisfies Eqn. (16). Without loss of generality, we assume z(t−1) and z(t) are the second and third term of the sequence Ẑ(t), that is Ẑ(1) = z(t−1) and Ẑ(2) = z(t). z(t−1) and z(t) are also the first two terms of Z(t), that is Z(0) = z(t−1) and Z(1) = z(t).\nWithout perturbations, Z(2) and Ẑ(3) are equal. We will show that Z(2) = (1 + O( + δ))Ẑ(3). This shows that the convergence rate is only slightly perturbed if the perturbation is small.\nTherefore, if x(0) and x(1) are close to x∗ and H(t) in Algorithm 2 does not vary severely, then Algorithm 1 and Algorithm 2 converge with rate close to 1− √ π if we choose θ close to 1− √ π\n1+ √ π .\nE.2 Proof of Convergence Property\nNow we begin to prove that Z(2) = (1 +O( + δ))Ẑ(3).\nBy Lemma 6, sequence Ẑ(t) and Z(t) are of different solution forms determined by π, θ, and η. We will analyze the value of Z(2) case by case. However, we will not consider Z(2) and Ẑ(3) of the form in the case 2 in Lemma 6. These cases will not happen in real application almost sure.\nBesides, we assume that parameter θ in Eqn. (15) and (16) are close to 1− √ π\n1+ √ π . The parameter η\nis close to 0. Now, we begin to analyze case by case.\ncase (a): We consider the case that Ẑ(1) = ξ̂α̂ cos(ω̂ + ϕ̂) and Ẑ(2) = ξ̂α̂2 cos(2ω̂ + ϕ̂), which are the first two terms of sequence Z(t) = ξαt cos(ωt+ ϕ), that is{\nξ̂α̂ cos(ω̂ + ϕ̂) = ξ cos(ϕ)\nξ̂α̂2 cos(2ω̂ + ϕ̂) = ξα cos(ω + ϕ)\nWe also have\nα̂ = √ (1− π)(1 + η)θ\nα = √ (1− π)(1 + )(1 + η)(1 + δ)θ\nα cos(ω) = (1− π)(1 + (1 + )θ)\n2\nα̂ cos(ω̂) = (1− π)(1 + θ)\n2 .\nAnd z(t+1) = ξα2 cos(2ω + ϕ) satisfies\nξα2 cos(2ω + ϕ) =ξα2 cos(ω + ϕ) cos(ω)− ξα2 sin(ω + ϕ) sin(ω) =ξ̂α̂2 cos(2ω̂ + ϕ̂)α cos(ω)− α √ ξ2α2 − (ξ̂α̂2 cos(2ω̂ + ϕ̂))2 sin(ω).\nBesides, it holds that\nξ2α2 sin2(ω) =ξ̂2α̂2α2 cos2(ω̂ + ϕ̂) sin2(ω) + ( ξ̂α̂α cos(ω̂ + ϕ̂) cos(ω)− ξ̂α̂2 cos(2ω̂ + ϕ̂) )2 =ξ̂2α̂2 ( α2 cos2(ω̂ + ϕ̂) sin2(ω) + α2 cos2(ω̂ + ϕ̂) cos2(ω) + α̂2 cos2(2ω̂ + ϕ̂) ) +\n− ξ̂2α̂2 (2αα̂ cos(ω̂ + ϕ̂) cos(ω) cos(2ω̂ + ϕ̂)) =ξ̂2α̂2 ( α2 cos2(ω̂ + ϕ̂) + α̂2 cos2(2ω̂ + ϕ̂)− 2αα̂ cos(ω̂ + ϕ̂) cos(ω) cos(2ω̂ + ϕ̂) ) .\nHence, we have\nξ2α2 sin2(ω)− (ξ̂α̂2 cos(2ω̂ + ϕ̂))2 sin2(ω) =ξ̂2α̂2 ( α2 cos2(ω̂ + ϕ̂) + α̂2 cos2(2ω̂ + ϕ̂)− 2αα̂ cos(ω̂ + ϕ̂) cos(ω) cos(2ω̂ + ϕ̂) ) −ξ̂2α̂2 ( α̂2 cos2(2ω̂ + ϕ̂) sin2(ω)\n) =ξ̂2α̂2 ( α2 cos2(ω̂ + ϕ̂) + α̂2 cos2(2ω̂ + ϕ̂) cos2(ω)− 2αα̂ cos(ω̂ + ϕ̂) cos(ω) cos(2ω̂ + ϕ̂)\n) =ξ̂2α̂2 (α cos(ω̂ + ϕ̂)− α̂ cos(2ω̂ + ϕ̂) cos(ω))2 .\nWe obtain\nξα2 cos(2ω + ϕ) =ξ̂α̂2 cos(2ω̂ + ϕ̂)α cos(ω) + ξ̂α̂2 cos(2ω̂ + ϕ̂)α cos(ω)− ξ̂α̂α2 cos(ω̂ + ϕ̂).\nFurthermore, we have\nξ̂α̂3 cos(3ω̂ + ϕ̂)− ξα2 cos(2ω + ϕ)\n=ξ̂α̂3 cos(2ω̂ + ϕ̂) cos(ω̂)− ξ̂α̂3 sin(2ω̂ + ϕ̂) sin(ω̂)− ξ̂α̂2 cos(2ω̂ + ϕ̂)α cos(ω)\n− (ξ̂α̂2 cos(2ω̂ + ϕ̂)α cos(ω)− ξ̂α̂α2 cos(2ω̂ + ϕ̂) cos(ω̂)− ξ̂α̂α2 sin(2ω̂ + ϕ̂) sin(ω̂))\n=ξ̂α̂3 cos(2ω̂ + ϕ̂) cos(ω̂)− ξ̂α̂2 cos(2ω̂ + ϕ̂)α cos(ω)\n− (ξ̂α̂2 cos(2ω̂ + ϕ̂)α cos(ω)− ξ̂α̂α2 cos(2ω̂ + ϕ̂) cos(ω̂)\n− (ξ̂α̂3 sin(2ω̂ + ϕ̂) sin(ω̂)− ξ̂α̂α2 sin(2ω̂ + ϕ̂) sin(ω̂)) =− ( θ\n1 + θ\n) ξ̂α̂3 cos(2ω̂ + ϕ̂) cos(ω̂)− ( θ\n1 + θ − − δ − δ\n) ξ̂α̂3 cos(2ω̂ + ϕ̂) cos(ω̂)\n− (− − δ − δ)ξ̂α̂3 sin(2ω̂ + ϕ̂) sin(ω̂)\n=− 2θ 1 + θ ξ̂α̂3 cos(2ω̂ + ϕ̂) cos(ω̂) + 2( + δ + δ)ξ̂α̂3 sin(2ω̂ + ϕ̂) sin(ω̂).\nTherefore, we have ξα2 cos(ω + ϕ) = (1 +O( + δ))ξ̂α̂3 cos(ω̂ + ϕ̂).\nThis means Z(2) = (1 +O( + δ))Ẑ(3).\ncase (b): Now we consider the case that Ẑ(1) = ξ̂α̂+ ϕ̂β̂ and Ẑ(2) = ξ̂α̂2 + ϕ̂β̂2, which are the first two terms of sequence Z(t) = ξαt + ϕβt, that is{\nξ̂α̂+ ϕ̂β̂ = ξ + ϕ\nξ̂α̂2 + ϕ̂β̂2 = ξα+ ϕβ\nwhere\nα̂ = (1− π)(1 + θ)\n2 +\n√ 1\n4 (1− π)2(1 + θ)2 − (1 + η)(1− π)θ\nβ̂ = (1− π)(1 + θ) 2 − √ 1 4 (1− π)2(1 + θ)2 − (1 + η)(1− π)θ\nα = (1− π)(1 + (1 + )θ)\n2 +\n√ 1\n4 (1− π)2(1 + (1 + )θ)2 − (1− π)(1 + η)(1 + δ)(1 + )θ\nβ = (1− π)(1 + (1 + )θ) 2 − √ 1 4 (1− π)2(1 + (1 + )θ)2 − (1− π)(1 + η)(1 + δ)(1 + )θ.\nWe have\nξα2 + ϕβ2 =ξα2 + ϕβα− ϕβα+Bβ2\n=α(ξα+ ϕβ) + ϕβ(β − α)\n=(ξ̂α̂2 + ϕ̂β̂2)α− ϕβ(α− β).\nWe also have\nϕ(α− β) =ξ̂α̂α+ ϕ̂β̂α− ξ̂α̂2 − ϕ̂β̂2.\nHence, we obtain\nξα2 + ϕβ2 =(ξ̂α̂2 + ϕ̂β̂2)α− β(ξ̂α̂α+ ϕ̂β̂α− ξ̂α̂2 − ϕ̂β̂2) =ξ̂α̂2 ( α+ β − αβ\nα̂β̂ β̂\n) + ϕ̂β̂2 ( α+ β − αβ\nα̂β̂ α̂\n) .\nThere exists αβ\nα̂β̂ = (1− π)(1 + η)(1 + δ)(1 + )θ (1 + η)(1− π)θ = (1 + δ)(1 + ).\nHence, we have\nα+ β − αβ α̂β̂ β̂ = 2(1− π)(1 + (1 + )θ) 2 − (1 + δ)(1 + )(1− π)(1 + θ) 2\n+ (1 + δ)(1 + )\n√ 1\n4 (1− π)2(1 + θ)2 − (1 + η)(1− π)θ\n= (1− π)(1− − δ − δ + (1 + − δ − δ)θ)\n2\n+ (1 + δ)(1 + )\n√ 1\n4 (1− π)2(1 + θ)2 − (1 + η)(1− π)θ\n=(1 +O( + δ))α̂.\nSimilarly, we have\nα+ β − αβ α̂β̂ α̂ = (1 +O( + δ))β̂.\nTherefore, we obtain\nξα2 + ϕβ2 = (1 +O( + δ))(ξ̂α̂3 + ϕ̂β̂3)\nThis means Z(2) = (1 +O( + δ))Ẑ(3).\ncase (c): We consider the case that Ẑ(1) = ξ̂α̂ cos(ω̂+ ϕ̂) and Ẑ(2) = ξ̂α̂2 cos(2ω̂+ ϕ̂), which is the first two terms of sequence Z(t) = ξαt + ϕβt, that is{\nξ̂α̂ cos(ω̂ + ϕ̂) = ξ + ϕ\nξ̂α̂2 cos(2ω̂ + ϕ̂) = ξα+ ϕβ\nwhere α̂ = √ (1− π)(1 + η)θ\nα̂ cos(ω̂) = (1− π)(1 + θ)\n2\nα = (1− π)(1 + (1 + )θ)\n2 +\n√ 1\n4 (1− π)2(1 + (1 + )θ)2 − (1− π)(1 + η)(1 + δ)(1 + )θ\nβ = (1− π)(1 + (1 + )θ) 2 − √ 1 4 (1− π)2(1 + (1 + )θ)2 − (1− π)(1 + η)(1 + δ)(1 + )θ.\nWe have\nξα2 + ϕβ2\n=ξα2 + ϕβα− ϕβα+ ϕβ2\n=α(ξα+ ϕβ) + ϕβ(β − α)\n=ξ̂α̂2α cos(2ω̂ + ϕ̂) + βξ̂α̂2 cos(2ω̂ + ϕ̂)− ξ̂α̂αβ cos(ω̂ + ϕ̂)\n=ξ̂α̂2(1− π)(1 + (1 + )θ) cos(2ω̂ + ϕ̂)− (1− π)(1 + η)(1 + δ)(1 + )θξ̂α̂ cos(ω̂ + ϕ̂)\n=2ξ̂α̂2 cos(2ω̂ + ϕ̂) ( 1 + θ\n1 + θ\n) α̂ cos(ω̂)− (1 + δ)(1 + )ξ̂α̂3 cos(ω̂ + ϕ̂).\nWe also have\nξ̂α̂3 cos(3ω̂ + ϕ̂)− (ξα2 + ϕβ2)\n=ξ̂α̂3 cos(2ω̂ + ϕ̂) cos(ω̂)− ξ̂α̂3 sin(2ω̂ + ϕ̂) sin(ω̂) − 2ξ̂α̂2 cos(2ω̂ + ϕ̂) ( 1 + θ\n1 + θ\n) α̂ cos(ω̂) + (1 + δ)(1 + )ξ̂α̂3 cos(ω̂ + ϕ̂)\n=− 2θ 1 + θ ξ̂α̂3 cos(2ω̂ + ϕ̂) cos(ω̂)− ξ̂α̂3 cos(ω̂ + ϕ̂) + (1 + δ)(1 + )ξ̂α̂3 cos(ω̂ + ϕ̂)\n=− 2θ 1 + θ ξ̂α̂3 cos(2ω̂ + ϕ̂) cos(ω̂) + ( + δ + δ)ξ̂α̂3 cos(ω̂ + ϕ̂)\n=O( + δ)ξ̂α̂3 cos(3ω̂ + ϕ̂).\nTherefore, we have ξα2 + ϕβ2 = (1 +O( + δ))ξ̂α̂3 cos(3ω̂ + ϕ̂)\nThis means Z(2) = (1 +O( + δ))Ẑ(3).\ncase (d): Now we consider the case that Ẑ(1) = ξ̂α̂+ ϕ̂β̂ and Ẑ(2) = ξ̂α̂2 + ϕ̂β̂2, which are the first two terms of sequence Z(t) = ξαt cos(ωt+ ϕ), that is{\nξ̂α̂+ ϕ̂β̂ = ξ cos(ϕ)\nξ̂α̂2 + ϕ̂β̂2 = ξα cos(ω + ϕ)\nwhere\nα̂ = (1− π)(1 + θ)\n2 +\n√ 1\n4 (1− π)2(1 + θ)2 − (1− π)(1 + η)θ\nβ̂ = (1− π)(1 + θ) 2 − √ 1 4 (1− π)2(1 + θ)2 − (1− π)(1 + η)θ\nα = √ (1− π)(1 + )(1 + η)(1 + δ)θ\nα cos(ω) = (1− π)(1 + (1 + )θ)\n2 .\nFirst, we can derive that\nξα cos(ω) cos(ϕ)− ξα sin(ω) sin(ϕ) = ξ̂α̂2 + ϕ̂β̂2 ⇒ξα sin(ω) sin(ϕ) = (ξ̂α̂+ ϕ̂β̂)α cos(ω)− ( ξ̂α̂2 + ϕ̂β̂2 ) ⇒ξ2α2 sin2(ω)(1− cos2(ϕ)) = ( (ξ̂α̂+ ϕ̂β̂)α cos(ω)− ( ξ̂α̂2 + ϕ̂β̂2\n))2 ⇒ξ2α2 sin2(ω) = ( (ξ̂α̂+ ϕ̂β̂)α cos(ω)− ( ξ̂α̂2 + ϕ̂β̂2 ))2 + (ξ̂α̂+ ϕ̂β̂)2α2 sin2(ω).\nHence, we have ξ2α2 sin2(ω) = ( (ξ̂α̂+ ϕ̂β̂)α cos(ω)− ( ξ̂α̂2 + ϕ̂β̂2 ))2 + (ξ̂α̂+ ϕ̂β̂)2α2 sin2(ω)\n=(ξ̂α̂+ ϕ̂β̂)2α2 cos2(ω) + ( ξ̂α̂2 + ϕ̂β̂2 )2 − 2 ( ξ̂α̂2 + ϕ̂β̂2 ) (ξ̂α̂+ ϕ̂β̂)α cos(ω)\n+ (ξ̂α̂+ ϕ̂β̂)2α2 sin2(ω) =(ξ̂α̂+ ϕ̂β̂)2α2 + ( ξ̂α̂2 + ϕ̂β̂2 )2 − 2 ( ξ̂α̂2 + ϕ̂β̂2 ) (ξ̂α̂+ ϕ̂β̂)α cos(ω).\nWe have\nξα2 cos(2ω + ϕ)\n=ξα2 cos(ω + ϕ) cos(ω)− ξα2 sin(ω + ϕ) sin(ω) =(ξ̂α̂2 + ϕ̂β̂2)α cos(ω)− α √ ξ2α2 sin2(ω)− (ξ̂α̂2 + ϕ̂β̂2)2 sin2(ω)\n=(ξ̂α̂2 + ϕ̂β̂2)α cos(ω) − α √ (ξ̂α̂+ ϕ̂β̂)2α2 + ( ξ̂α̂2 + ϕ̂β̂2 )2 − 2 ( ξ̂α̂2 + ϕ̂β̂2 ) (ξ̂α̂+ ϕ̂β̂)α cos(ω)− (ξ̂α̂2 + ϕ̂β̂2)2 sin2(ω)\n=(ξ̂α̂2 + ϕ̂β̂2)α cos(ω) + (ξ̂α̂2 + ϕ̂β̂2)α cos(ω)− (ξ̂α̂+ ϕ̂β̂)α2.\nWe also have\nξ̂α̂3 + ϕ̂β̂3 − ξα2 cos(2ω + ϕ) =ξ̂α̂3 + ϕ̂β̂3 − (ξ̂α̂2 + ϕ̂β̂2)α cos(ω)− ( (ξ̂α̂2 + ϕ̂β̂2)α cos(ω)− (ξ̂α̂+ ϕ̂β̂)α2 )\n=ξ̂α̂2(α̂− α cos(ω)) + ϕ̂β̂2(β̂ − α cos(ω))− ( (ξ̂α̂2 + ϕ̂β̂2)α cos(ω)− (1 + )(1 + δ)(ξ̂α̂+ ϕ̂β̂)α̂β̂ )\n=ξ̂α̂2(α̂− α cos(ω)) + ϕ̂β̂2(β̂ − α cos(ω)) − ( (ξ̂α̂2(α cos(ω)− (1 + )(1 + δ)β̂) + ϕ̂β̂2(α cos(ω)− (1 + )(1 + δ)α̂) )\n=ξ̂α̂2(α̂+ β̂ − 2α cos(ω)) + ϕ̂β̂2(α̂+ β̂ − 2α cos(ω)) + ( + δ + δ)(ξ̂α̂2β̂ + ϕ̂β̂2α̂)\n= (1− π)θ(ξ̂α̂2 + ϕ̂β̂2) + ( + δ + δ)(ξ̂α̂2β̂ + ϕ̂β̂2α̂)\n= ( ξ̂α̂3 β̂\n1 + η + ϕ̂β̂3\nα̂\n1 + η\n) + ( + δ + δ)(ξ̂α̂2β̂ + ϕ̂β̂2α̂).\nBy the assumption, α̂ and β̂ are close to each other. Hence, it holds that\nξ̂α̂3 + ϕ̂β̂3 − ξα2 cos(2ω + ϕ) = O( + δ)(ξ̂α̂3 + ϕ̂β̂3).\nThis means Z(2) = (1 +O( + δ))Ẑ(3)."
    } ],
    "references" : [ {
      "title" : "Katyusha: The first direct acceleration of stochastic gradient methods",
      "author" : [ "Zeyuan Allen-Zhu" ],
      "venue" : "arXiv preprint arXiv:1603.05953,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "Amir Beck", "Marc Teboulle" ],
      "venue" : "SIAM journal on imaging sciences,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "On the use of stochastic hessian information in optimization methods for machine learning",
      "author" : [ "Richard H Byrd", "Gillian M Chin", "Will Neveitt", "Jorge Nocedal" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Better mini-batch algorithms via accelerated gradient methods",
      "author" : [ "Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Convergence rates of sub-sampled newton methods",
      "author" : [ "Murat A Erdogdu", "Andrea Montanari" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "An optimal randomized incremental gradient method",
      "author" : [ "Guanghui Lan", "Yi Zhou" ],
      "venue" : "arXiv preprint arXiv:1507.02000,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Efficient mini-batch training for stochastic optimization",
      "author" : [ "Mu Li", "Tong Zhang", "Yuqiang Chen", "Alexander J Smola" ],
      "venue" : "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate o (1/k2)",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "In Soviet Mathematics Doklady,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1983
    }, {
      "title" : "Gradient methods for minimizing composite objective function",
      "author" : [ "Yurii Nesterov" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence",
      "author" : [ "Mert Pilanci", "Martin J. Wainwright" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2017
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "Herbert Robbins", "Sutton Monro" ],
      "venue" : "The annals of mathematical statistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1951
    }, {
      "title" : "Sub-sampled newton methods ii: Local convergence rates",
      "author" : [ "Farbod Roosta-Khorasani", "Michael W Mahoney" ],
      "venue" : "arXiv preprint arXiv:1601.04738,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "A stochastic gradient method with an exponential convergence _rate for finite training sets",
      "author" : [ "Nicolas L Roux", "Mark Schmidt", "Francis R Bach" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "Mark Schmidt", "Nicolas Le Roux", "Francis Bach" ],
      "venue" : "arXiv preprint arXiv:1309.2388,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Sub-sampled newton methods with non-uniform sampling",
      "author" : [ "Peng Xu", "Jiyan Yang", "Farbod Roosta-Khorasani", "Christopher Ré", "Michael W Mahoney" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "A unifying framework for convergence analysis of approximate newton methods",
      "author" : [ "Haishan Ye", "Luo Luo", "Zhihua Zhang" ],
      "venue" : "arXiv preprint arXiv:1702.08124,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the computational cost [4, 10, 14].",
      "startOffset" : 106,
      "endOffset" : 117
    }, {
      "referenceID" : 7,
      "context" : "The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the computational cost [4, 10, 14].",
      "startOffset" : 106,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the computational cost [4, 10, 14].",
      "startOffset" : 106,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "Hence, many variants have been proposed to improve the convergence rate of SGD [8, 16, 17, 20].",
      "startOffset" : 79,
      "endOffset" : 94
    }, {
      "referenceID" : 13,
      "context" : "Hence, many variants have been proposed to improve the convergence rate of SGD [8, 16, 17, 20].",
      "startOffset" : 79,
      "endOffset" : 94
    }, {
      "referenceID" : 14,
      "context" : "Hence, many variants have been proposed to improve the convergence rate of SGD [8, 16, 17, 20].",
      "startOffset" : 79,
      "endOffset" : 94
    }, {
      "referenceID" : 8,
      "context" : "For the first-order methods which only make use of the gradient information, Nestrov’s acceleration technique is a very useful tool [11].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 8,
      "context" : "It greatly improves the convergence of gradient descent [11], proximal gradient descent [2, 12], and stochastic gradient with variance reduction [1, 9], etc.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "It greatly improves the convergence of gradient descent [11], proximal gradient descent [2, 12], and stochastic gradient with variance reduction [1, 9], etc.",
      "startOffset" : 88,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "It greatly improves the convergence of gradient descent [11], proximal gradient descent [2, 12], and stochastic gradient with variance reduction [1, 9], etc.",
      "startOffset" : 88,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "It greatly improves the convergence of gradient descent [11], proximal gradient descent [2, 12], and stochastic gradient with variance reduction [1, 9], etc.",
      "startOffset" : 145,
      "endOffset" : 151
    }, {
      "referenceID" : 6,
      "context" : "It greatly improves the convergence of gradient descent [11], proximal gradient descent [2, 12], and stochastic gradient with variance reduction [1, 9], etc.",
      "startOffset" : 145,
      "endOffset" : 151
    }, {
      "referenceID" : 12,
      "context" : "To conquer this weakness, one proposed a sub-sampled Newton which only samples a subset of functions fi randomly to construct a sub-sampled Hessian [15, 3, 18] .",
      "startOffset" : 148,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "To conquer this weakness, one proposed a sub-sampled Newton which only samples a subset of functions fi randomly to construct a sub-sampled Hessian [15, 3, 18] .",
      "startOffset" : 148,
      "endOffset" : 159
    }, {
      "referenceID" : 15,
      "context" : "To conquer this weakness, one proposed a sub-sampled Newton which only samples a subset of functions fi randomly to construct a sub-sampled Hessian [15, 3, 18] .",
      "startOffset" : 148,
      "endOffset" : 159
    }, {
      "referenceID" : 10,
      "context" : "Pilanci and Wainwright [13] applied the sketching technique to alleviate the computational burden of computing Hessian and brought up sketch Newton.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "Regularized sub-sampled Newton methods were also devised to deal with the ill-condition problem [5, 15].",
      "startOffset" : 96,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "Regularized sub-sampled Newton methods were also devised to deal with the ill-condition problem [5, 15].",
      "startOffset" : 96,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "[19] cast these stochastic second-order procedures into a so-called approximate Newton framework.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "Then Algorithm 2 will become the approximate Newton method defined in [19].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "Besides, since AccRegSN makes use of more curvature information than Nestrov’s accelerated gradient descent (AGD) method [11], AccRegSN should converge faster than AGD theoretically.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "entries from U(0, 1), where U(0, 1) means uniform distribution on [0, 1].",
      "startOffset" : 66,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : "Notice that the size and dimension of dataset are close to each other, so the sketch Newton method [13, 18] can not be used.",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "Notice that the size and dimension of dataset are close to each other, so the sketch Newton method [13, 18] can not be used.",
      "startOffset" : 99,
      "endOffset" : 107
    } ],
    "year" : 2017,
    "abstractText" : "Optimization plays a key role in machine learning. Recently, stochastic second-order methods have attracted much attention due to their low computational cost in each iteration. However, these algorithms might perform poorly especially if it is hard to approximate the Hessian well and efficiently. As far as we know, there is no effective way to handle this problem. In this paper, we resort to Nestrov’s acceleration technique to improve the convergence performance of a class of second-order methods called approximate Newton. We give a theoretical analysis that Nestrov’s acceleration technique can improve the convergence performance for approximate Newton just like for first-order methods. We accordingly propose an accelerated regularized sub-sampled Newton. Our accelerated algorithm performs much better than the original regularized sub-sampled Newton in experiments, which validates our theory empirically. Besides, the accelerated regularized subsampled Newton has good performance comparable to or even better than state-of-art algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}