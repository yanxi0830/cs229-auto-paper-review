{
  "name" : "1705.01091.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "PDE APPROACH TO THE PROBLEM OF ONLINE PREDICTION WITH EXPERT ADVICE: A CONSTRUCTION OF POTENTIAL-BASED STRATEGIES",
    "authors" : [ "DMITRY B. ROKHLIN" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 5.\n01 09\n1v 1\n[ cs\n.L G\n] 2\nM ay\n2 01\n7\n1. Introduction\nLet B be any set. In the problem of online prediction with expert advice a forecaster predicts a sequence (bt) n−1 t=0 , bt ∈ B on the basis of expert opinions f it ∈ A, i = 1, . . . , N , where A is a convex subset of a vector space. More precisely, at round t ∈ {0, . . . , n − 1} forecaster’s guess at is a convex combination of expert advices:\nat = 〈pt, ft〉 := N ∑\ni=1\npitf i t , pt ∈ ∆ := {z ≥ 0 : 〈z, 1〉 = 1} ,\nbased on the available history and current advices: pt = pt((bs) t−1 s=0, (fs) t s=0).\nLet l : A× B 7→ [0, 1] be a loss function. Forecaster’s aim is to keep the regret\nRn =\nn−1 ∑\nt=0\nl(〈pt, ft〉, bt)− min 1≤i≤N\nn−1 ∑\nt=0\nl(f it , bt)\nsmall. This regret Rn measures the quality of predictions by comparing the cumulative loss of the forecaster with that of a best expert, chosen in hindsight.\nWe refer to [4] for more information on this problem. The basic result (see, e.g, [4, Theorem 2.2]) guarantees the existence of a prediction strategy p∗ achieving the uniform bound\nRn ( (p∗t ) n−1 t=0 , (ft) n−1 t=0 , (bt) n−1 t=0 )\n√ n\n≤ C (1.1)\nfor any (bt) n−1 t=0 , (ft) n−1 t=0 under the assumption that l is convex in its first argument. Moreover, this bound cannot be improved without further assumptions: [4, Theorem 3.7]. The inequality (1.1) implies that in the long run on average the forecaster predicts as well as a best expert: Rn/n→ 0, n→ ∞.\nThere are plenty of strategies achieving the bound (1.1). In [3] it was shown that for a rather general class of online learning problems the construction of such strategies can\n2010 Mathematics Subject Classification. 68T05, 68W27, 35K55. Key words and phrases. regret, online learning, potentials, non-linear parabolic PDE, weighted average\nforecaster. 1\nbe based on the notion of potential function. More recently [7] proposed a systematic way for the construction of potentials in the case of randomized prediction, mentioning that “The origin/recipe for “good” potential functions has always been a mystery (at least to the authors).” The authors of [7] considered a recurrence relation, for the value function of a repeated game, determining the optimal regret, and showed that potential functions are related to relaxations of this function, which are consistent with the mentioned recurrence relation. To obtain such relaxations they used upper bounds, developed in the theory of online learning and capturing the complexity of the problem.\nIn this paper we show that for the problem of prediction with expert advice there is another “natural” way for the appearance of potential-based algorithms. As in [7], we consider a repeated game, determining the optimal regret, and the correspondent recurrence relation for the value functions vn. Further, in contrast to [7], we simply pass to the limit as n→ ∞ and get a non-linear parabolic Bellman-Isaacs type partial differential equation in [0, 1]×RN . A rigorous justification of this procedure can be performed within the theory of viscosity solutions. However, being interested only in the construction of prediction strategies, we need not do it! As usual, a Bellman-type equation at least formally produces optimal strategies. More precisely, we consider the strategies, generated by appropriate smooth supersolutions, and then directly check the inequality (1.1), using the argumentation similar to that of the verification method from the theory of optimal control.\nThe described approach is mainly inspired by the paper [6], where there was studied a link between fully non-linear second order (parabolic and elliptic) PDE and repeated games. Its application to the problems of online learning theory was initiated in [10], where an asymptotics of the sequential Rademacher complexity (the last notion was introduced in [8]) of a finite function class was related to the viscosity solution of a G-heat equation. In turn, the result of [10] is based on the central limit theorem under model uncertainty, studied within the same approach in [9].\n2. Prediction game and the limiting PDE\nThe worst-case regret\nRn = sup f0∈AN inf p0∈∆ sup b0∈B . . . sup fn−1∈AN inf pn−1∈∆ sup bn−1∈B\nRn((pt) n−1 t=0 , (ft) n−1 t=0 , (bt) n−1 t=0 )\nis a result of the repeated game between the predictor, an adversary and experts. In this game the adversary has an informational advantage over the predictor and experts, since bt is chosen after the sequences (pj) t j=0, (fj) t j=0 are revealed. Furthermore, the predictor has an informational advantage over the experts, since the choice of pt can be based on (fj) t j=0, (bj) t−1 j=0. Finally, experts can use only the information contained in (pj) t−1 j=0, (bj) t−1 j=0. The adversary and experts play against the predictor, trying to maximize his regret. To get a recurrent formula for Rn let us introduce the family of state processes\nX i,t,x,p,f,bs+1 = X i,t,x,p,f,b s + 1√ n ri(ps, fs, bs), s = t, . . . , n− 1, (2.2)\nri = l(〈pt, ft〉, bt)− l(f it , bt), X i,t,x,p,f,bt = xi ∈ R. Summing up the increments X i,0,0,p,f,bs+1 −X i,0,0,p,f,bs , we obtain\n1√ n Rn((pt) n−1 t=0 , (ft) n−1 t=0 , (bt) n−1 t=0 ) = max { X1,0,0,p,f,bn , . . . , X N,0,0,p,f,b n } .\nLet us introduce the value functions\nvn(t/n, x) = sup ft∈AN inf pt∈∆ sup bt∈B . . . sup fn−1∈AN inf pn−1∈∆ sup bn−1∈B g(X t,x,p,f,bn ),\nvn(1, x) = g(x),\nwhere t = 0, . . . , n− 1, g(x) = max{x1, . . . , xn}. From the dynamic programming theory it is known that vn satisfies the recurrence relations\nvn(t/n, x) = sup f∈AN inf p∈∆ sup b∈B\nvn((t+ 1)/n, x+ r(p, f, b)/ √ n), (2.3)\nt ≤ n−1, r = (r1, . . . , rN). We stress that we need not rigorously justify this and subsequent claims, since our goal is to formally construct prediction strategies. Their verification is delayed to the last step.\nFor a moment imagine that vn is a smooth function, satisfying (2.3) on [0, 1− 1/n]×RN . Then, by Taylor’s formula we get\n0 = sup f∈AN inf p∈∆ sup b∈B\n{√ n〈vnx(t, x), r(p, f, b)〉+ vnt (t, x)\n+ 1\n2 〈vnxx(t, x)r(p, f, b), r(p, f, b)〉+ o(1)\n}\n, (2.4)\nwhere vnx , v n xx are the gradient vector and the Hessian matrix.\nWe will say that the loss function l satisfies the Blackwell condition if\nΓ(γ, f) := {p ∈ ∆ : 〈γ, r(p, f, b)〉 ≤ 0, b ∈ B} 6= ∅ (2.5) for all (γ, f) ∈ RN+ × AN . Clearly, Γ(0, f) = ∆. The Blackwell condition (2.5) is satisfied if l is convex in its first argument. In this case p = γ/〈γ, 1〉 ∈ Γ(γ, f) for γ ∈ RN+\\{0}, since\nN ∑\ni=1\nγiri ( γ 〈γ, 1〉 , f, b ) = 〈γ, 1〉l (〈γ, f〉 〈γ, 1〉 , b ) − N ∑\ni=1\nγil(f i, b) ≤ 0\nby Jensen’s inequality. By the nature of vn these functions are non-decreasing in each xi. Indeed, v\nn(t/n, x) is the optimal worst-case regret if the initial regret with respect to i-th expert at time moment t equals to xi. From (2.4) we get\n0 ≤ sup f∈AN inf p∈Γ(vnx ,f) sup b∈B\n{\nvnt (t, x) + 1\n2 〈vnxx(t, x)r(p, f, b), r(p, f, b)〉+ o(1)\n}\n.\nSo, we expect that the limiting function v satisfies the inequality\n− vt(t, x)−G(vx(t, x), vxx(t, x)) ≤ 0, (2.6)\nG(γ, S) = 1\n2 sup f∈AN inf p∈Γ(γ,f) sup b∈B\n〈Sr(p, f, b), r(p, f, b)〉,\nand the boundary condition v(1, x) = g(x). Note that G(γ, S) ≥ G(γ, S ′), if the symmetric N ×N matrix S − S ′ is non-negative definite. Hence,\n− ut(t, x)−G(ux(t, x), uxx(t, x)) = 0 (2.7) is a fully non-linear parabolic equation (see [5]). Along with (2.7) we consider the boundary condition\nu(1, x) = g(x) = max{x1, . . . , xn}. (2.8)\nThe functions vn are defined on Qn = {0, 1/n, . . . , (n − 1)/n, 1} × RN . To describe their limiting behavior in a rigorous way, one can consider the Barles-Perthame half-relaxed (weak) upper limit:\nv(t, x) = sup{ lim vnk(tk, xk) : Qnk ∋ (tk, xk) → (t, x) and vnk(tk, xk) converges}.\nFrom the results of [1, 2, 6] and the above calculations we expect that v is a viscosity subsolution of (2.7), (2.8). Note, that by the definition,\nlim sup n→∞ 1√ n Rn ≤ v(0, 0). (2.9)\n3. Smooth supersolutions and induced weighted average forecasting strategies\nTake a smooth supersolution w of (2.7), (2.8):\n− wt(t, x)−G(wx(t, x), wxx(t, x)) ≥ 0, w(1, x) ≥ g(x), (3.10) which is non-decreasing in each variable xi. Assuming a comparison result: v ≤ w, we conclude that the inequality (2.9) holds true for w(0, 0) instead of v(0, 0). We also expect that a strategy pt(x) ∈ Γ(wx(t, x), f) will produce the regret, satisfying this bound.\nLet us look for supersolutions of the form w(t, x) = c(1− t) +Φ(x), where c is a constant, Φ(x) ≥ g(x), (3.11)\nand Φ is non-decreasing in each variable. The differential inequality (3.10) implies the condition G(Φx(x),Φxx(x)) ≤ c. This condition is satisfied if\n1 2 sup x∈RN 〈Φxx(x)h, h〉 ≤ c, |hi| ≤ 1, i = 1, . . . , N. (3.12)\nThen by the Blackwell condition (2.5) there exists a vector-function\np∗(x, f) ∈ Γ(Φx(x), f). (3.13) If l is convex in its first argument and Φ is strictly increasing in each variable, then, according to the remark after the formula (2.5), one can take\np∗(x, f) = Φx(x)\n〈Φx(x), 1〉 ∈ Γ(Φx(x), f). (3.14)\nConsider the discrete-time state process (2.2), generated by the prediction strategy, related to p∗:\nX∗,is+1 = X ∗,i s + 1√ n ri(p∗s, fs, bs), p ∗ s = p ∗(X∗s , fs), X ∗ 0 = 0. (3.15)\nNote, that p∗t automatically satisfies the inequality 〈Φx(X∗t ), r(p∗t , ft, bt)〉 ≤ 0 (3.16) which is also called the Blackwell condition: see [3, 4]. For a convex function a 7→ l(a, b) from (3.14) we get a weighted average forecaster:\na∗t = 〈p∗t , ft〉 = 〈Φx(X∗t ), ft〉 〈Φx(X∗t ), 1〉 . (3.17)\nTheorem 1. Let the Blackwell condition (2.5) be satisfied, and let Φ : RN 7→ R be a twice continuously differentiable function, which non-decreases in each variable and meets the conditions (3.11), (3.12). Then a prediction strategy\na∗t = 〈p∗t , f〉, p∗t = p∗(X∗t , ft), t = 0, . . . , n− 1, where p∗ satisfies (3.13) and X∗ is defined by (3.15), produces the regret, satisfying the inequality (1.1) with C = c+ Φ(0).\nProof. For w(t, x) = c(1− t) + Φ(x) by Taylor’s formula we get w((t+ 1)/n,X∗t+1)− w(t/n,X∗t ) = −c/n + Φ(X∗t+1)− Φ(X∗t )\n= −c/n + 〈Φx(X∗t ), r(p∗t , ft, bt)〉/ √ n + 1\n2 〈Φxx(ξt)r(p∗t , ft, bt), r(p∗t , ft, b∗t )〉/n ≤ 0,\nfor some ξt, where the last inequality is implied by (3.16) and (3.12). Now the assertion of the theorem follows from the condition (3.11):\nRn/ √ n = g(X∗n) ≤ w(1, X∗n) = Φ(X∗n) ≤ w(0, X∗0) = c+ Φ(0).\nFollowing [3, 4] we call Φ a potential function. The most natural smooth upper bound for max{x1, . . . , xN}, and hence a candidate for a potential, is a soft-maximum function\nΦ(x) = 1\nη ln\n(\nN ∑\ni=1\neηxi\n)\n, η > 0. (3.18)\nThis function is included in a more general class Φ(x) = ψ (\n∑N i=1 φ(xi)\n)\nconsidered in [3, 4],\nwhere ψ and φ are assumed to be concave and convex respectively. The following inequality is also taken from [3, 4]:\n〈Φxx(x)h, h〉 ≤ ψ′ ( N ∑\nk=1\nφ(xk)\n)\nN ∑\ni=1\nφ′′(xi)h 2 i .\nFor (3.18) we have ψ(x) = η−1 ln x, φ(x) = eηx,\n1 2 〈Φxx(x)h, h〉 ≤ η 2\nN ∑\ni=1\neηxi ∑N\nk=1 e ηxk\nh2i ≤ c = η\n2 , |hi| ≤ 1.\nFor p∗t generated by (3.18), in accordance with Theorem 1 we have\nRn√ n ≤ c+ Φ(0) = η 2 + lnN η =\n√ 2 lnN\nfor an “optimal” choice η = √ 2 lnN (cf. [4, Corollary 2.2]). The formula (3.17) reduces to\na∗t = 〈Φx(X∗t ), ft〉 〈Φx(X∗t ), 1〉 = N ∑\ni=1\neηX ∗,i t\n∑N j=1 e\nηX∗,jt f it =\nN ∑\ni=1\ne−ηL i t/ √ n\n∑N j=1 e\n−ηLjt/ √ n f it ,\nwhere Lit = ∑t−1 s=0 l(f i t , bs) is the cumulative loss of i-th expert. This is a basic version of the exponentially weighted average forecaster: see [4, Chapter 2].\n4. Randomized prediction\nAssume that the forecaster randomly chooses a prediction by taking a sample It from a probability distribution pt = (p 1 t , . . . , p N t ) over {y1, . . . , yN}. His cumulative loss is compared with the cumulative loss of a best fixed prediction:\nn−1 ∑\nt=0\nl(It, bt)− min 1≤i≤N\nn−1 ∑\nt=0\nl(yi, b),\nand the regret is defined as the expectation of this quantity with respect to the induced artificial probability measure:\nRn =\nn−1 ∑\nt=0\nN ∑\nj=1\npjt l(yj , bt)− min 1≤i≤N\nn−1 ∑\nt=0\nl(yi, bt) = min 1≤i≤N\nn−1 ∑\nt=0\nri(pt, bt),\nri(p, b) =\nN ∑\nj=1\npjl(yj , b)− l(yi, b).\nThe game, where the forecaster knows the previous moves: pt = pt(b0, . . . , bt−1), and the adversary knows the prediction algorithm: bt = bt(p0, . . . , pt) but not the predictions It itself, corresponds to the case of an oblivious adversary: [4, Chapter 2]. However, the case of non-oblivious adversary is not interesting for the problem of this form: see [4, Lemma 4.1].\nThe described game is simpler than that considered above, since the “experts”, corresponding to fixed predictions, do not play against the forecaster. Moreover, the condition (2.5) is satisfied regardless of the convexity of l. Repeating the reasoning of Section 2, we get the inequality (2.6) with\nG(γ, S) = 1\n2 inf p∈Γ(γ) sup b∈B\n〈Sr(p, b), r(p, b)〉,\nΓ(γ) =\n{\np ∈ ∆ : N ∑\ni=1\nγi\nN ∑\nj=1\npjl(yj, b)− N ∑\ni=1\nl(yi, b) ≤ 0, b ∈ B } .\nSo, a prediction strategy satisfying\npi,∗t = Φxi(X\n∗ t )\n〈Φx(X∗t ), 1〉 for Φx(X\n∗ t ) 6= 0,\nwhere Φ meets the conditions of Theorem 1, and X∗t is defined by the recursion of the form\n(3.15), produces the regret Rn ≤ C/ √ n. In particular, C = √ 2 lnN for the exponentially weighted average forecaster, discussed after Theorem 1. Finally, we note that the case of internal regret (see [4, Section 4.4]) can be considered in the same way.\n5. Acknowledgments\nThe research is supported by the Russian Science Foundation, project 17-19-01038.\nReferences\n[1] Barles, G., Perthame, B.: Exit time problems in optimal control and vanishing viscosity method. SIAM J. Control Optim. 26(5), 1133–1148 (1988) [2] Barles, G., Souganidis, P.E.: Convergence of approximation schemes for fully nonlinear second order equations. Asymptot. Anal. 4, 271–283 (1991) [3] Cesa-Bianchi, N., Lugosi, G.: Potential-based algorithms in on-line prediction and game theory. Mach. Learn. 51(3), 239261 (2003) [4] Cesa-Bianchi, N., Lugosi, G.: Prediction, learning, and games. Cambridge University Press, New York (2006) [5] Crandall, M., Ishii, H., Lions, P.L.: User’s guide to viscosity solutions of second order partial differential equations. Bull. Amer. Math. Soc. 27(1), 1–67 (1992) [6] Kohn, R., Serfaty, S.: A deterministic-control-based approach to fully nonlinear parabolic and elliptic equations. Commun. Pur. Appl. Math. 63(10), 1298–1350 (2010) [7] Rakhlin, A., Shamir, O., Sridharan, K.: Relax and randomize: from value to algorithms. In: F. Pereira, C.J.C. Burges, L. Bottou, K.Q. Weinberger (eds.) Advances in Neural Information Processing Systems 25, pp. 2141–2149. Curran Associates, Inc. (2012) [8] Rakhlin, A., Sridharan, K., Tewari, A.: Online learning: random averages, combinatorial parameters, and learnability. In: J.D. Lafferty, C.K.I. Williams, J. Shawe-Taylor, R.S. Zemel, A. Culotta (eds.) Advances in Neural Information Processing Systems 23, pp. 1984–1992. Curran Associates, Inc. (2010) [9] Rokhlin, D.: Central limit theorem under uncertain linear transformations. Stat. Probabil. Lett. 107, 191–198 (2015) [10] Rokhlin, D.: Asymptotic sequential Rademacher complexity of a finite function class. Arch. Math. 108(3), 325–335 (2017)\nInstitute of Mathematics, Mechanics and Computer Sciences, Southern Federal Univer-\nsity, Mil’chakova str., 8a, 344090, Rostov-on-Don, Russia\nE-mail address, Dmitry B. Rokhlin: rokhlin@math.rsu.ru"
    } ],
    "references" : [ {
      "title" : "Exit time problems in optimal control and vanishing viscosity method",
      "author" : [ "G. Barles", "B. Perthame" ],
      "venue" : "SIAM J. Control Optim",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1988
    }, {
      "title" : "Convergence of approximation schemes for fully nonlinear second order equations",
      "author" : [ "G. Barles", "P.E. Souganidis" ],
      "venue" : "Asymptot. Anal",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1991
    }, {
      "title" : "Potential-based algorithms in on-line prediction and game theory",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "Mach. Learn. 51(3),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "User’s guide to viscosity solutions of second order partial differential equations",
      "author" : [ "M. Crandall", "H. Ishii", "P.L. Lions" ],
      "venue" : "Bull. Amer. Math. Soc. 27(1),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1992
    }, {
      "title" : "A deterministic-control-based approach to fully nonlinear parabolic and elliptic equations",
      "author" : [ "R. Kohn", "S. Serfaty" ],
      "venue" : "Commun. Pur. Appl. Math. 63(10),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Relax and randomize: from value to algorithms",
      "author" : [ "A. Rakhlin", "O. Shamir", "K. Sridharan" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Online learning: random averages, combinatorial parameters, and learnability",
      "author" : [ "A. Rakhlin", "K. Sridharan", "A. Tewari" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Central limit theorem under uncertain linear transformations",
      "author" : [ "D. Rokhlin" ],
      "venue" : "Stat. Probabil. Lett. 107,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Asymptotic sequential Rademacher complexity of a finite function class",
      "author" : [ "D. Rokhlin" ],
      "venue" : "Arch. Math. 108(3),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Let l : A× B 7→ [0, 1] be a loss function.",
      "startOffset" : 16,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "We refer to [4] for more information on this problem.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 2,
      "context" : "In [3] it was shown that for a rather general class of online learning problems the construction of such strategies can 2010 Mathematics Subject Classification.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "More recently [7] proposed a systematic way for the construction of potentials in the case of randomized prediction, mentioning that “The origin/recipe for “good” potential functions has always been a mystery (at least to the authors).",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 6,
      "context" : "” The authors of [7] considered a recurrence relation, for the value function of a repeated game, determining the optimal regret, and showed that potential functions are related to relaxations of this function, which are consistent with the mentioned recurrence relation.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "As in [7], we consider a repeated game, determining the optimal regret, and the correspondent recurrence relation for the value functions v.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 6,
      "context" : "Further, in contrast to [7], we simply pass to the limit as n→ ∞ and get a non-linear parabolic Bellman-Isaacs type partial differential equation in [0, 1]×RN .",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "Further, in contrast to [7], we simply pass to the limit as n→ ∞ and get a non-linear parabolic Bellman-Isaacs type partial differential equation in [0, 1]×RN .",
      "startOffset" : 149,
      "endOffset" : 155
    }, {
      "referenceID" : 5,
      "context" : "The described approach is mainly inspired by the paper [6], where there was studied a link between fully non-linear second order (parabolic and elliptic) PDE and repeated games.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 9,
      "context" : "Its application to the problems of online learning theory was initiated in [10], where an asymptotics of the sequential Rademacher complexity (the last notion was introduced in [8]) of a finite function class was related to the viscosity solution of a G-heat equation.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 7,
      "context" : "Its application to the problems of online learning theory was initiated in [10], where an asymptotics of the sequential Rademacher complexity (the last notion was introduced in [8]) of a finite function class was related to the viscosity solution of a G-heat equation.",
      "startOffset" : 177,
      "endOffset" : 180
    }, {
      "referenceID" : 9,
      "context" : "In turn, the result of [10] is based on the central limit theorem under model uncertainty, studied within the same approach in [9].",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "In turn, the result of [10] is based on the central limit theorem under model uncertainty, studied within the same approach in [9].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "7) is a fully non-linear parabolic equation (see [5]).",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "From the results of [1, 2, 6] and the above calculations we expect that v is a viscosity subsolution of (2.",
      "startOffset" : 20,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "From the results of [1, 2, 6] and the above calculations we expect that v is a viscosity subsolution of (2.",
      "startOffset" : 20,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "From the results of [1, 2, 6] and the above calculations we expect that v is a viscosity subsolution of (2.",
      "startOffset" : 20,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "16) which is also called the Blackwell condition: see [3, 4].",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "16) which is also called the Blackwell condition: see [3, 4].",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "Following [3, 4] we call Φ a potential function.",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 3,
      "context" : "Following [3, 4] we call Φ a potential function.",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 2,
      "context" : "considered in [3, 4], where ψ and φ are assumed to be concave and convex respectively.",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "considered in [3, 4], where ψ and φ are assumed to be concave and convex respectively.",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "The following inequality is also taken from [3, 4]: 〈Φxx(x)h, h〉 ≤ ψ′ ( N",
      "startOffset" : 44,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "The following inequality is also taken from [3, 4]: 〈Φxx(x)h, h〉 ≤ ψ′ ( N",
      "startOffset" : 44,
      "endOffset" : 50
    } ],
    "year" : 2017,
    "abstractText" : "We consider a sequence of repeated prediction games and formally pass to the limit. The supersolutions of the resulting non-linear parabolic partial differential equation are closely related to the potential functions in the sense of N.Cesa-Bianci, G. Lugosi (2003). Any such supersolution gives an upper bound for forecaster’s regret and suggests a potentialbased prediction strategy, satisfying the Blackwell condition. A conventional upper bound for the worst-case regret is justified by a simple verification argument.",
    "creator" : "LaTeX with hyperref package"
  }
}