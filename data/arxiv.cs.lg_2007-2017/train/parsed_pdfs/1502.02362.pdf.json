{
  "name" : "1502.02362.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Counterfactual Risk Minimization: Learning from Logged Bandit Feedback",
    "authors" : [ "Adith Swaminathan" ],
    "emails" : [ "ADITH@CS.CORNELL.EDU", "TJ@CS.CORNELL.EDU" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 2.\n02 36\n2v 1\n[ cs\n.L G\n] 9\nF eb\n2 01"
    }, {
      "heading" : "1. Introduction",
      "text" : "Log data is one of the most ubiquitous forms of data available, as it can be recorded from a variety of systems (e.g., search engines, recommender systems, ad placement) at little cost. The interaction logs of such systems typically contain a record of the input to the system (e.g. features describing the user), the prediction made by the system (e.g. a recommended list of news articles) and the feedback\n(e.g. number of ranked articles the user read). The feedback, however, provides only partial information – “bandit feedback”– limited to the particular prediction shown by the system. The feedback for all the other predictions the system could have made is typically not known. This makes learning from log data fundamentally different from supervised learning, where “correct” predictions (e.g. the best ranking of news articles for that user) together with a loss function provide full-information feedback.\nIn this paper, we address the problem of batch learning from logged bandit feedback. Unlike online learning with bandit feedback, batch learning does not require interactive experimental control over the system. Furthermore, it enables the reuse of existing data and offline cross-validation techniques for model selection (e.g., “should we perform feature selection?”, “which learning algorithm should we use?”, etc.).\nTo solve this batch-learning problem, we first need a counterfactual estimator (Bottou et al., 2013) of a system’s performance, so that we can estimate how other systems would have performed if they had been in control of choosing predictions. Such estimators have been developed recently for the off-policy evaluation problem (Langford et al., 2011), (Li et al., 2011), (Li et al., 2014a), where data collected from the interaction logs of one bandit algorithm is used to evaluate another system.\nOur approach to batch learning from bandit feedback centers around the insight that, to perform robust learning, it is not sufficient to have just an unbiased estimator of the offpolicy system’s performance. We must also reason about how the variances of these estimators differ across the hypothesis space, and pick the hypothesis that has the best possible guarantee (tightest conservative bound) for its performance. We first prove generalization error bounds analogous to structural risk minimization (Vapnik, 1998) for a stochastic hypothesis family. The constructive nature of these bounds suggests a general principle – Counterfactual Risk Minimization (CRM) – for designing methods for batch learning from bandit feedback.\nUsing the CRM principle, we derive a new learning algorithm – Policy Optimizer for Exponential Models (POEM) – for structured output prediction. The training objective is decomposed using repeated variance linearization, and optimizing it using stochastic gradient descent yields a fast and effective algorithm. We evaluate POEM on several multi-label classification problems, verify that its empirical performance supports the theory, and demonstrate substantial improvement in generalization performance over the state-of-the-art.\nWe review existing approaches in Section 2. The learning setting is detailed in Section 3, and contrasted with supervised learning. In Section 4, we derive the Counterfactual Risk Minimization learning principle and provide a rule of thumb for setting hyper-parameters. In Section 5, we instantiate the CRM principle for structured output prediction using exponential models and construct an efficient decomposition of the objective for stochastic optimization. Empirical evaluations are reported in Section 6 and we conclude with future directions and discussion in Section 7."
    }, {
      "heading" : "2. Related Work",
      "text" : "Existing approaches for batch learning from logged bandit feedback fall into two categories. The first approach is to reduce the problem to supervised learning. In principle, since the logs give us an incomplete view of the feedback for different predictions, one could first use regression to estimate a feedback oracle for unseen predictions, and then use any supervised learning algorithm using this feedback oracle. Such a two-stage approach is known to not generalize well (Beygelzimer & Langford, 2009). More sophisticated techniques using a cost weighted classification (Zadrozny et al., 2003) or the Offset Tree algorithm (Beygelzimer & Langford, 2009) allow us to perform batch learning when the space of possible predictions is small. In contrast, our approach generalizes structured output prediction, with exponential-sized prediction spaces.\nThe second approach to batch learning from bandit feedback uses propensity scoring (Rosenbaum & Rubin, 1983) (Bottou et al., 2013) to derive unbiased estimators from the interaction logs. These estimators are used for a small set of candidate policies, and the best estimated candidate is picked via exhaustive search. In contrast, our approach can be optimized via gradient descent, over hypothesis families (of infinite size) that are equally as expressive as those used in supervised learning.\nOur approach builds on counterfactual estimators that have been developed for off-policy evaluation. The inverse propensity scoring estimator can be optimal when we have a good model of the historical algorithm (Li et al., 2014a), (Li et al., 2014b), and doubly robust estimators are even\nmore efficient when we additionally have a good model of the feedback (Langford et al., 2011). In our work, we focus on the inverse propensity scoring estimator, the results we derive hold equally for the doubly robust estimators.\nIn the current work, we concentrate on the case where the historical algorithm was a stationary, stochastic policy. Techniques like exploration scavenging (Langford et al., 2008) and bootstrapping (Mary et al., 2014) allow us to perform counterfactual evaluation even when the historical algorithm was deterministic or adaptive.\nBeyond the problem of batch learning from bandit feedback, our approach can have implications for several applications that require learning from logged bandit feedback data: warm-starting multi-armed bandits (Shivaswamy & Joachims, 2012), pre-selecting retrieval functions for search engines (Hofmann et al., 2013), and policy evaluation for contextual bandits (Li et al., 2011), to name a few."
    }, {
      "heading" : "3. Learning Setting: Batch Learning with Logged Bandit Feedback",
      "text" : "Consider a structured output prediction problem that takes as input x ∈ X and outputs a prediction y ∈ Y . For example, in multi-label document classification, x could be a news article and y a bitvector indicating the labels assigned to this article. The inputs are assumed drawn from a fixed but unknown distribution Pr(X ), x i.i.d.∼ Pr(X ). Consider the hypothesis space H of stochastic policies. A hypothesis h(Y | x) ∈ H defines a probability distribution over the output space Y , and the hypothesis makes predictions by sampling, y ∼ h(Y | x). Note that this definition of a hypothesis space also includes deterministic hypotheses, where the distributions assign probability 1 to a single y. For notational convenience, denote h(Y | x) by h(x), and the probability assigned by h(x) to y as h(y | x). In interactive learning systems, we only observe feedback δ(x, y) for the y sampled from h(x). In this work, feedback δ : X×Y 7→ R is a cardinal loss that is only observed at the sampled data points. Small values for δ(x, y) indicate user satisfaction with y for x, while large values indicate dissatisfaction. The expected loss – called risk – of a hypothesis R(h) is defined as,\nR(h) = Ex∼Pr(X )Ey∼h(x) [δ(x, y)] . (1)\nThe goal of the system is to minimize risk, or equivalently, maximize expected user satisfaction. The aim of learning is to find a hypothesis h ∈ H that has minimum risk. We wish to re-use the interaction logs of these systems for batch learning. Assume that its historical algorithm acted according to a stationary policy h0(x) (also called logging\npolicy). The data collected from this system is\nD = {(x1, y1, δ1), . . . , (xn, yn, δn)}, (2)\nwhere yi ∼ h0(xi) and δi ≡ δ(xi, yi).\nSampling bias. D cannot be used to estimate R(h) for a new hypothesis h using the estimator typically used in supervised learning. We ideally need either full information about δ(xi, ·) (which reduces to a trivial learning problem) or need samples y ∼ h(xi) to directly estimate R(h). This explains why, in practice, model selection over a small set of candidate systems is typically done via A/B tests, where the candidates are deployed to collect new data sampled according to y ∼ h(x) for each hypothesis h. A relative comparison of the assumptions, hypotheses, and principles used in supervised learning vs. our learning setting is outlined in Table 1. Fundamentally, batch learning with bandit feedback is hard because D is both biased (predictions favored by the historical algorithm will be over-represented) and incomplete (feedback for other predictions will not be available) for learning."
    }, {
      "heading" : "4. Learning Principle: Counterfactual Risk Minimization",
      "text" : "The distribution mismatch between h0 and any hypothesis h ∈ H can be addressed using importance sampling, which corrects the sampling bias as:\nR(h) = Ex∼Pr(X )Ey∼h(x) [δ(x, y)]\n= Ex∼Pr(X )Ey∼h0(x)\n[ δ(x, y) h(y | x) h0(y | x) ] .\nThis motivates the propensity scoring approach. During the operation of the logging policy, we keep track of the propensity, h0(y | x) of the historical system to generate y for x. From these propensity-augmented logs\nD={(x1,y1,δ1,p1), . . . , (xn,yn,δn,pn)}, (3)\nwhere pi≡h0(yi | xi), we can derive an unbiased estimate of R(h) via Monte Carlo approximation,\nR̂(h) = 1\nn\nn ∑\ni=1\nδi h(yi | xi)\npi . (4)\nAt first thought, one may think that directly estimating R̂(h) over h ∈ H and picking the empirical minimizer is a valid learning strategy. Unfortunately, there are several potential pitfalls.\nFirst, this strategy is not invariant to additive transformations of the loss and will give degenerate results if the loss is not appropriately scaled. In Section 4.1, we develop intuition for why this is so, and derive the optimal scaling of δ. For now, assume that ∀x, ∀y, δ(x, y) ∈ [−1, 0].\nSecond, this estimator has unbounded variance, since pi ≃ 0 in D can cause ED [ R̂(h) ] to be arbitrarily far away from\nthe true risk R(h). This problem can be fixed by “clipping” the importance sampling weights (Ionides, 2008)\nRM (h) = ExEy∼h0(x)\n[\nδ(x, y)min\n{ M, h(y |x) h0(y |x) }] ,\nR̂M (h) = 1\nn\nn ∑\ni=1\nδimin\n{ M, h(yi | xi)\npi\n}\n. (5)\nM > 0 is a hyper-parameter chosen to trade-off bias and variance in the estimate, where smaller values of M induce larger bias in the estimate. Optimizing R̂M (h) through exhaustive enumeration over H yields the Inverse Propensity Scoring (IPS) training objective (Bottou et al., 2013)\nĥIPS = argmin h∈H\n{ R̂M (h) } . (6)\nThird, importance sampling typically estimates R̂M (h) of different hypotheses h ∈ H with vastly different variances. Consider two hypotheses h1 and h2, where h1 is similar to h0, but where h2 samples predictions that were not well explored by h0. Importance sampling gives us low-variance estimates for R̂M (h1), but highly variable estimates for R̂M (h2). Intuitively, if we can develop variance-sensitive confidence bounds over the hypothesis space, optimizing a conservative confidence bound should find a h whose R(h) will not be much worse, with high probability.\nGeneralization error bound. To develop a generalization error bound, we first need a concept of capacity for stochastic hypothesis classes. For any stochastic class H, define an auxiliary function class FH = {fh : X ×Y 7→ [0, 1]}. Each h ∈ H corresponds to a function fh ∈ FH,\nfh(x, y) = 1 + δ(x, y)\nM min\n{ M, h(y | x) h0(y | x) } . (7)\nfh is a deterministic, bounded function, and satisfies\nExEy∼h0(x) [fh(x, y)] = 1 +R M (h)/M. (8)\nHence, we can use classic notions of capacity for FH to reason about the convergence of R̂M (h) → RM (h). Recall the covering number N∞(ǫ,F , n) for a function class F (Maurer & Pontil, 2009). Define an ǫ−cover N (ǫ, A, ‖·‖∞) for a set A ⊆ Rn to be the size of the smallest cardinality subset A0 ⊆ A such that A is contained in the union of balls of radius ǫ centered at points in A0, in the metric induced by ‖ · ‖∞. The covering number is,\nN∞(ǫ,F , n) = sup (xi,yi)∈(X×Y)n N (ǫ,F({(xi, yi)}), ‖·‖∞),\nwhere F({(xi, yi)}) is the function class conditioned on sample {(xi, yi)},\nF({(xi, yi)}) = {(f(x1, y1), . . . , f(xn, yn)) : f ∈ F}.\nOur measure for the capacity of our stochastic class H to “fit” a sample of size n shall be N∞( 1n ,FH, 2n). Theorem 1. For a compact notation, define\nuh i ≡ δi min{M,h(yi | xi)/pi}, (9)\nV arh(u) ≡ 1\nn(n− 1)\nn ∑\ni,j=1\n(uh i − uhj)2 2 ,\nQH(n, γ) ≡ log(10 · N∞( 1\nn ,FH, 2n)/γ), 0 < γ < 1.\nWith probability at least 1 − γ in the random vector (x1, y1) · · · (xn, yn), with xi i.i.d.∼ Pr(X ) and yi ∼ h0(xi), and observed losses δ1, . . . , δn, for n ≥ 16 and a hypothesis space H with capacity N∞( 1n ,FH, 2n),\n∀h ∈ H : R(h) ≤ R̂M (h) + √ 18V arh(u)QH(n, γ)/n +M · 15Q(n, γ)/(n− 1).\nProof. Follow the proof of Theorem 6 of (Maurer & Pontil, 2009) with the function class as FH. Use Equations (7),(8) to translate from fh(x, y) to RM (h). R̂M (h) = M · f̂h−1, RM (h) = M · fh − 1, and M2V arh(u) = V arfh(u). Finally, since ∀x, ∀y, δ(x,y) ≤ 0, hence R(h) ≤ RM (h).\nCRM Principle. This generalization error bound is constructive, and it motivates a general principle for designing machine learning methods for batch learning from bandit feedback. In particular, a learning algorithm following this principle should jointly optimize the estimate R̂M (h) as well as its empirical standard deviation, where the latter serves as a data-dependent regularizer.\nĥCRM = argmin h∈H\n{\nR̂M (h) + λ\n√\nV arh(u)\nn\n}\n. (10)\nM > 0 and λ ≥ 0 are regularization hyper-parameters. When λ = 0, we recover the Inverse Propensity Scoring objective of Equation (6). In analogy to Structural Risk\nMinimization (Vapnik, 1998), we call this principle Counterfactual Risk Minimization, since both pick the hypothesis with the tightest upper bound on the true risk R(h)."
    }, {
      "heading" : "4.1. Optimal Loss Scaling",
      "text" : "When performing supervised learning with true labels y∗ and a loss function ∆(y∗, ·), empirical risk minimization using the standard estimator is invariant to additive translation and multiplicative scaling of ∆. The risk estimators R̂(h) and R̂M (h) in bandit learning, however, crucially require δ(·, ·) ∈ [−1, 0]. Consider, for example, the case of δ(·, ·) ≥ 0. The training objectives in Equation (6) (IPS) and Equation (10) (CRM) become degenerate! A hypothesis h ∈ H that completely avoids the sample D (i.e. ∀i = 1, . . . , n, h(yi | xi) = 0) trivially achieves the best possible R̂M (h) (= 0) with 0 variance. This degeneracy arises because when δ(·, ·) ≥ 0, the optimization objectives are a lower bound on the true risk, whereas what we need is an upper bound on R(h).\nFor any bounded loss δ(·, ·) ∈ [▽,△], we have, ∀x\nEy∼h(x)[δ(x, y)]≤△+Ey∼h0(x) [ (δ(x, y)−△) h(y | x) h0(y | x) ] .\nWe assert that this is the tightest possible upper bound possible without additional assumptions. Since the optimization objectives in Equations (6),(10) are unaffected by a constant scale factor (e.g. △ − ▽), we should transform δ 7→ δ′ to derive a conservative training objective w.r.t. δ′,\nδ′ ≡ {δ −△}/{△−▽}."
    }, {
      "heading" : "4.2. Selecting hyper-parameters",
      "text" : "We propose selecting the hyper-parameters M > 0 and λ ≥ 0 via validation. However, we must be careful not to set λ too big. The estimated risk R̂M (h) ∈ [−M, 0] while the variance penalty √\nV arh(u) n\n∈ [\n0, M 2 √ n\n]\n. If λ ≫ 0, a hypothesis h ∈ H that completely avoids D achieves a training objective of 0. As a rule of thumb, we can calibrate λ ≤ λ∗ so that the objective is negative for some h ∈ H. When h0 ∈ H, { R̂M (h0) + λ √ V arh0 (u)\nn\n}\n< 0 is a nat-\nural choice. This way, minimization over H is guaranteed to avoid returning degenerate h."
    }, {
      "heading" : "5. Learning Algorithm: POEM",
      "text" : "We now derive an efficient algorithm for structured output prediction using linear rules from the CRM principle. Classic linear models in supervised learning predict using\nhsupw (x) = argmax y∈Y {w · φ(x, y)} , (11)\nwhere w is a d−dimensional weight vector, and φ(x, y) is a d−dimensional linear feature map. For example, in multilabel document classification, for a news article x and a possible assignment of labels y represented as a bitvector, φ(x, y) could simply be a concatenation of the bag-ofwords features of the document (x), one copy for each of the assigned labels in y, x ⊗ y. Several efficient inference algorithms have been developed to solve Equation (11).\nConsider the following stochastic family Hlin, parametrized by w. A hypothesis hw(x) ∈ Hlin samples y from the distribution\nhw(y | x) = exp(w · φ(x, y))/Z(x), (12)\nZ(x) = ∑ y′∈Y exp(w · φ(x, y′)) is the partition function. This can be thought of as the “soft-max” variant of the “hard-max” rules from Equation (11). Additionally, for a temperature multiplier α > 1, w 7→ αw induces a more “peaked” distribution hαw that preserves the modes of hw, and intuitively, is a “more deterministic” variant of hw.\nhw lies in the exponential family of distributions, and has a simple gradient,\n∇hw(y |x)=hw(y |x) { φ(x,y)−Ey′∼hw(x)[φ(x,y′)] } . (13)\nConsider a bandit-feedback structured-output dataset D = {(x1, y1, δ1, p1), . . . , (xn, yn, δn, pn)}. In multi-label document classification, this data could be collected from an interactive labeling system, each y indicating the labels predicted by the system for a document x, receiving as feedback δ(x, y) how many labels (but not which ones) were correct. To perform learning, first we scale the losses as outlined in Section 4.1. Next, instantiating the CRM principle (Equation (10)) for Hlin, (using notation analogous to that in Theorem 1, adapted for Hlin), yields the POEM training objective.\nPOEM Training Objective:\nw∗ = argmin w∈Rd uw + λ\n√\nV arw(u)\nn + µ‖w‖2, (14)\nwhere\nuw i ≡ δi min{M, exp(w · φ(xi, yi)) pi · Z(xi) },\nuw ≡ n ∑\ni=1\nuw i/n,\nV arw(u) ≡ n ∑\ni=1\n(uw i − uw)2/(n− 1).\nWhile the objective in Equation (14) is not convex in w (even for λ = 0), we find that batch gradient descent (e.g. L-BFGS) and the stochastic gradient approach introduced below find local optima that have good generalization error.\nSoftware implementing POEM is available at http://www.cs.cornell.edu/˜adith/CRM/poem.html for download, as is all the code and data needed to run each of the experiments reported in Section 6."
    }, {
      "heading" : "5.1. Iterated Variance Majorization",
      "text" : "The POEM training objective in Equation (14), specifically the variance term √\nV arw(u), resists stochastic gradient optimization in the presented form. To remove this obstacle, we now develop a Majorization-Minimization scheme that can be shown to converge to a local optimum of the POEM training objective. In particular, we will show how to decompose √\nV arw(u) as a sum of differentiable functions (e.g. ∑\ni uw i or\n∑\ni{uwi}2) so that we can optimize the overall training objective at scale using stochastic gradient descent.\nProposition 1. For any w0,\n√ V arw(u) ≤ Aw0 n ∑\ni=1\nuw i +Bw0\nn ∑\ni=1\n{uwi}2 + Cw0\n= Q(w;w0).\nAw0 ≡ −uw0/{(n− 1) √ V arw0(u)}, Bw0 ≡ 1/{2(n− 1) √ V arw0(u)}, Cw0 ≡ n{uw0}2\n2(n− 1) √ V arw0(u) +\n√\nV arw0(u)\n2 .\nProof. Consider a first order Taylor approximation of √\nV arw(u) around w0, √· is concave. Again Taylor ap-\nproximate −{uw}2, noting that −{·}2 is concave.\nIteratively minimizingwt+1 = argminw Q(w;w t) ensures that the sequence of iterates w1, . . . , wt+1 are successive minimizers of √\nV arw(u). Hence, during an epoch t, POEM proceeds by sampling uniformly i ∼ D, comput-\ning uwi,∇uwi and, for learning rate η, updating\nw ← w− η{∇uwi + λ√ n (Awt∇uwi + 2Bwtuwi∇uwi)}.\nAfter each epoch, wt+1 ← w, and iterated minimization proceeds until convergence."
    }, {
      "heading" : "6. Experiments",
      "text" : "We now empirically evaluate the prediction performance and computational efficiency of POEM. Consider multilabel classification with input x ∈ Rp and prediction y ∈ {0, 1}q. Popular supervised algorithms that solve this problem include Structured SVMs and Conditional Random Fields. In the simplest case, CRF essentially performs logistic regression for each of the q labels independently. As outlined in Section 5, we use a linear feature map: φ(x, y) = x ⊗ y. We conducted experiments on different multi-label datasets collected from the LibSVM repository, with different ranges for p (features), q (labels) and n (samples) represented as summarized in Table 2.\nExperiment methodology. We employ the Supervised 7→ Bandit conversion (Agarwal et al., 2014) method. Here, we take a supervised dataset D = {(x1, y∗1) . . . (xn, y∗n)} and simulate a bandit feedback dataset from a logging policy h0 by sampling yi ∼ h0(xi) and collecting feedback ∆(y∗i , yi). In all the multi-label experiments, ∆(y\n∗(x), y) is the Hamming loss between the supervised label y∗ vs. the sampled label y for input x. Hamming loss is just the number of incorrectly assigned labels (both false positives and false negatives). We can explore different learning strategies (e.g. IPS, CRM, etc.) on D = {(xi, yi, δi ≡ ∆(y∗i , yi), pi ≡ h0(yi | xi))} and obtain learnt weight vectors wips, wcrm, etc. On the test set, we then report the expected loss per instance R(w) = 1\nn\n∑ i Ey∼hw(xi)∆(y ∗ i , y)\nand compare the generalization performance of these learning strategies.\nExperiment setup. For all datasets, we kept aside 25% of ntrain as validation set and treat the rest as the training\nset. For all methods, when optimizing any objective over w, we always begin the optimization fromw = 0 (⇒ hw = uniform(Y)). SGD learning rates follow a 1√\n1+t schedule.\nThe bandit learning methods do not get access to the labeled training set, but merely to labels y sampled from h0. In principle, we could use any arbitrary stochastic policy as h0. We choose a CRF trained on 10% of the training set as h0 using default hyper-parameters (C = 1), since they provide probability distributions amenable to sampling.\nTo create D = {(x1, y1, δ1, p1), . . . , (xn, yn, δn, pn)}, we take two passes through the training set and sample labels from h0. Note that each supervised label is worth ≃ |Y| = 2q bandit feedback labels.\nWe use the validation set for hyper-parameter selection. For CRFs, C ∈ [0.001, . . . , 100], while for the bandit learning approaches, M ∈ [1, . . . , 1000] , µ ∈ [\n10−8, . . . , 1 ] , c ∈ [ 10−4, . . . , 1 ]\nin multiples of 10. c denotes the scaling for the variance penalty, λ = cλ∗, where λ∗ is the calibration factor from Section 4.2.\nFinally, the entire experiment set-up is run 10 times (i.e. h0 trained on randomly chosen 10% subsets, D re-created, and test set performance of different approaches collected) and we report the averaged test set expected error across runs. The expected Hamming loss of h0 is the baseline to beat. Lower loss is better. The IPS objective represents the stateof-the-art for counterfactual learning. We optimize it using L-BFGS and report its test set expected loss. POEM uses our Iterative-Majorization SGD, as outlined in Section 5.1. Finally, we report results from supervised CRF trained on the entire training set as a skyline, despite its unfair advantage of having access to the full-information examples."
    }, {
      "heading" : "6.1. Does variance regularization improve generalization?",
      "text" : "Results are reported in Table 3. We statistically test the performance of POEM against IPS using a one-tailed paired difference t-test at significance level of 0.05 across 10 runs of the experiment, and find POEM to be significantly better than IPS on each dataset. Furthermore, on all datasets POEM learns a hypothesis that substantially improves over the prediction performance of h0. IPS actually fails on the Media dataset, where it returns a hypothesis that is worse than h0. This suggests that the CRM principle is practically useful for designing learning algorithms, and that the variance regularizer indeed provides practical benefit."
    }, {
      "heading" : "6.2. How computationally efficient is POEM?",
      "text" : "To evaluate the efficiency of training POEM, we implemented two versions of the algorithm. For POEM(S) we train the objective using the iterative-majorization\nStochastic gradient method described in Section 5.1. For POEM(B) we optimize the POEM objective via L-BFGS, a Batch gradient method. L-BFGS was also use for training the IPS(B) method. Table 4 shows the time taken (in CPU seconds) to run each method on each dataset, averaged over different validation runs when performing hyper-parameter grid search. Some of the timing results are skewed by outliers, e.g. when under very weak l2−regularization, CRFs tend to take a lot longer to converge. However, in aggregate, it is clear that POEM(S) is able to recover good parameter settings in a fraction of the time of batch L-BFGS optimization, and this is even more pronounced when the number of labels and number of samples grows (the runtime cost is dominated by the computation of Z(x) for each sample). Interestingly, we also find that the optima found by the SGD optimizer tend to generalize better than the optima found by L-BFGS."
    }, {
      "heading" : "6.3. Can MAP predictions derived from stochastic policies perform well?",
      "text" : "For the policies learnt by POEM as shown in Table 3, Table 5 reports the averaged performance of the deterministic predictor derived from them. For a learnt weight vector w, this simply amounts to applying Equation (11). In practice, this method of generating predictions can be substantially faster than sampling since computing the argmax does not require computation of the partition function Z(x) which can be expensive in structured output prediction. From Table 5, we see that the loss of the deterministic predic-\ntor is typically not far from the loss of the stochastic policy, but often slightly better. This suggests that POEM tends to find parameters w that behave almost deterministically (i.e. ‖w‖ is large), which we verified to be true. Moreover, we can also assert that, for any stochastic policy hw, there exists a corresponding deterministic function that has risk no greater than R(w) (and similarly for empirical risk), and hence techniques like variance regularization and l2−regularization become crucial in this setting."
    }, {
      "heading" : "6.4. How does generalization improve with size of D?",
      "text" : "As we collect more data under h0, our generalization error bound indicates that prediction performance should eventually approach that of the optimal hypothesis in the hypothesis space. We can simulate n → ∞ by replaying the training data multiple times, collecting samples y ∼ h0(x). In the limit, we would observe every possible y in the bandit feedback dataset, since h0(x) has non-zero probability of exploring each prediction y. However, the learning rate may be slow, since the exponential model family has very thin tails, and hence may not be an ideal logging distribution to learn from.\nTo ensure that we do not have confounding effects from stochastic optimization and early termination, here we study the L-BFGS variant of POEM(B). Holding all other details of the experiment setup fixed, we vary the number of times we replayed the training set (ReplayCount) to collect samples from h0, and report the performance of a single run of POEM(B) on the Yeast dataset in Figure 1."
    }, {
      "heading" : "6.5. How does quality of h0 affect learning?",
      "text" : "In this experiment, we change the fraction of the training set f · ntrain that was used to train the logging policy; as f is increased, the quality of h0 improves. Intuitively, there’s a trade-off: better h0 probably samples correct predictions more often and so produces a higher quality D to learn from, but it should also be harder to beat h0. We vary f from 10% to 100% while keeping all other conditions identical to the original experiment setup, and again focus on the L-BFGS variant of POEM(B). We report the performance of h0 and POEM(B) for a single experiment run in Figure 2, and find that POEM(B) is able to consistently find a hypothesis at least as good as h0. Moreover, the performance of POEM(B) plateaus with increasing f , in contrast with its behavior in Figure 1, where it consis-\ntently improves with increasing n."
    }, {
      "heading" : "6.6. How does stochasticity of h0 affect learning?",
      "text" : "Finally, the theory suggests that counterfactual learning is only possible when h0 is sufficiently stochastic (the generalization bounds hold with high probability in the samples drawn from h0). Does CRM degrade gracefully when this assumption is violated? We test this by introducing the temperature multiplier w 7→ αw, α > 0 (as discussed in Section 5) into the logging policy. For h0 = hw0 , we scale w0 7→ αw0, to derive a “more deterministic” variant of h0, and generate D ∼ hαw0 . Holding all other experiment conditions fixed, we report the performance of a single run of POEM(B) in Figure 3 as we change α ∈ [0.5, . . . , 32], compared against h0, and the deter-\nministic predictor – h0 map – derived from h0. So long as there is some minimum amount of stochasticity in h0 (α ≤ 22), POEM(B) is still able to find a w that marginally improves upon h0. The margin of improvement is more when h0 is more stochastic. Even when h0 is too deterministic (α ≥ 23), performance of POEM(B) does not degrade too much, suggesting that the CRM principle indeed achieves robust learning."
    }, {
      "heading" : "6.7. Can warm-starting the parameter search help?",
      "text" : "We naı̈vely started our parameter search from w = 0 in all experiments reported so far. However, since our logging policy h0(w0) ∈ Hlin, we can perhaps reach better local minima by warm-starting the search with w = w0. The\ngeneralization performance of POEM(B) with and without warm-starting is shown in Table 6. Warm-starting consistently helps find marginally better optima, suggesting there is further avenue for improvement using clever optimization techniques to solve these non-convex objectives."
    }, {
      "heading" : "7. Conclusion",
      "text" : "Counterfactual risk minimization serves as a robust principle to design algorithms that can learn from a batch of bandit feedback interactions. The key insight for CRM is to expand the classical notion of a hypothesis class to include stochastic policies, reason about variance in the risk\nestimator, and derive a generalization error bound over this hypothesis space. The practical take-away is a simple, datadependent regularizer that guarantees robust learning. We also developed POEM that uses CRM for structured output prediction. POEM can optimize over rich policy families (exponential models corresponding to linear rules in supervised learning), and deal with massive output spaces as efficiently as classical supervised methods. POEM efficiently decomposes the CRM training objective using repeated variance linearization, and optimizes at scale using stochastic gradient descent.\nCRM can more generally apply to supervised learning with non-differentiable losses, since the objective does not require the gradient of the loss function. We also foresee extensions of this work that relax some of the assumptions, e.g., to handle noisy δ(·, ·), to extend to ordinal or co-active feedback, to learn from adaptive or deterministic h0, etc."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This research was funded in part through NSF Awards IIS1247637 and IIS-1217686, and JTCII Cornell-Technion Research Fund, and a gift from Bloomberg."
    } ],
    "references" : [ {
      "title" : "Taming the monster: A fast and simple algorithm for contextual bandits",
      "author" : [ "Agarwal", "Alekh", "Hsu", "Daniel", "Kale", "Satyen", "Langford", "John", "Li", "Lihong", "Schapire", "Robert" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2014
    }, {
      "title" : "The offset tree for learning with partial labels",
      "author" : [ "Beygelzimer", "Alina", "Langford", "John" ],
      "venue" : "In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Beygelzimer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Beygelzimer et al\\.",
      "year" : 2009
    }, {
      "title" : "Reusing historical interaction data for faster online learning to rank for IR",
      "author" : [ "Hofmann", "Katja", "Schuth", "Anne", "Whiteson", "Shimon", "de Rijke", "Maarten" ],
      "venue" : "In Sixth ACM International Conference on Web Search and Data Mining,",
      "citeRegEx" : "Hofmann et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hofmann et al\\.",
      "year" : 2013
    }, {
      "title" : "Truncated importance sampling",
      "author" : [ "Ionides", "Edward L" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "Ionides and L.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ionides and L.",
      "year" : 2008
    }, {
      "title" : "Exploration scavenging",
      "author" : [ "Langford", "John", "Strehl", "Alexander", "Wortman", "Jennifer" ],
      "venue" : "In Proceedings of the 25th International Conference on Machine Learning,",
      "citeRegEx" : "Langford et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Langford et al\\.",
      "year" : 2008
    }, {
      "title" : "Doubly robust policy evaluation and learning",
      "author" : [ "Langford", "John", "Li", "Lihong", "Dudk", "Miroslav" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "Langford et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Langford et al\\.",
      "year" : 2011
    }, {
      "title" : "Counterfactual estimation and optimization of click metrics for search",
      "author" : [ "Li", "Lihong", "Chen", "Shunbao", "Kleban", "Jim", "Gupta", "Ankur" ],
      "venue" : "engines. CoRR,",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "On minimax optimal offline policy evaluation",
      "author" : [ "Li", "Lihong", "Munos", "Rémi", "Szepesvári", "Csaba" ],
      "venue" : "CoRR, abs/1409.3653,",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving offline evaluation of contextual bandit algorithms via bootstrapping techniques",
      "author" : [ "Mary", "Jérémie", "Preux", "Philippe", "Nicol", "Olivier" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing,",
      "citeRegEx" : "Mary et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mary et al\\.",
      "year" : 2014
    }, {
      "title" : "Empirical bernstein bounds and sample-variance penalization",
      "author" : [ "Maurer", "Andreas", "Pontil", "Massimiliano" ],
      "venue" : "In COLT 2009 - The 22nd Conference on Learning Theory,",
      "citeRegEx" : "Maurer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Maurer et al\\.",
      "year" : 2009
    }, {
      "title" : "The central role of the propensity score in observational studies for causal effects",
      "author" : [ "Rosenbaum", "Paul R", "Rubin", "Donald B" ],
      "venue" : null,
      "citeRegEx" : "Rosenbaum et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "Rosenbaum et al\\.",
      "year" : 1983
    }, {
      "title" : "Multi-armed bandit problems with history",
      "author" : [ "Shivaswamy", "Pannagadatta K", "Joachims", "Thorsten" ],
      "venue" : "In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Shivaswamy et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Shivaswamy et al\\.",
      "year" : 2012
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "V. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "Vapnik,? \\Q1998\\E",
      "shortCiteRegEx" : "Vapnik",
      "year" : 1998
    }, {
      "title" : "Costsensitive learning by cost-proportionate example weighting",
      "author" : [ "Zadrozny", "Bianca", "Langford", "John", "Abe", "Naoki" ],
      "venue" : "In Proceedings of the Third IEEE International Conference on Data Mining,",
      "citeRegEx" : "Zadrozny et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zadrozny et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Such estimators have been developed recently for the off-policy evaluation problem (Langford et al., 2011), (Li et al.",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "We first prove generalization error bounds analogous to structural risk minimization (Vapnik, 1998) for a stochastic hypothesis family.",
      "startOffset" : 85,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "More sophisticated techniques using a cost weighted classification (Zadrozny et al., 2003) or the Offset Tree algorithm (Beygelzimer & Langford, 2009) allow us to perform batch learning when the space of possible predictions is small.",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : ", 2014b), and doubly robust estimators are even more efficient when we additionally have a good model of the feedback (Langford et al., 2011).",
      "startOffset" : 118,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "Techniques like exploration scavenging (Langford et al., 2008) and bootstrapping (Mary et al.",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : ", 2008) and bootstrapping (Mary et al., 2014) allow us to perform counterfactual evaluation even when the historical algorithm was deterministic or adaptive.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "Beyond the problem of batch learning from bandit feedback, our approach can have implications for several applications that require learning from logged bandit feedback data: warm-starting multi-armed bandits (Shivaswamy & Joachims, 2012), pre-selecting retrieval functions for search engines (Hofmann et al., 2013), and policy evaluation for contextual bandits (Li et al.",
      "startOffset" : 293,
      "endOffset" : 315
    }, {
      "referenceID" : 12,
      "context" : "In analogy to Structural Risk Minimization (Vapnik, 1998), we call this principle Counterfactual Risk Minimization, since both pick the hypothesis with the tightest upper bound on the true risk R(h).",
      "startOffset" : 43,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "We employ the Supervised 7→ Bandit conversion (Agarwal et al., 2014) method.",
      "startOffset" : 46,
      "endOffset" : 68
    } ],
    "year" : 2017,
    "abstractText" : "We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. These constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method – called Policy Optimizer for Exponential Models (POEM) – for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. POEM is evaluated on several multi-label classification problems showing substantially improved robustness and generalization performance compared to the state-of-the-art.",
    "creator" : "LaTeX with hyperref package"
  }
}