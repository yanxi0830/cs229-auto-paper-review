{
  "name" : "1503.00332.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "JUMP-Means: Small-Variance Asymptotics for  Markov Jump Processes",
    "authors" : [ "Jonathan H. Huggins", "Karthik Narasimhan", "Ardavan Saeedi", "Vikash K. Mansinghka" ],
    "emails" : [ "JHUGGINS@MIT.EDU", "KARTHIKN@CSAIL.MIT.EDU", "ARDAVANS@MIT.EDU", "VKM@MIT.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Markov jump processes (MJPs) are continuous-time, discrete-state Markov processes in which state durations are exponentially distributed according to state-specific rate parameters. A stochastic matrix controls the probability of transitioning between pairs of states. MJPs have been used to construct probabilistic models either when the state of a system is observed directly, such as with disease pro-\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\ngression (Mandel, 2010) and RNA path folding (Hajiaghayi et al., 2014), or when the state is only observed indirectly, as in corporate bond rating (Bladt & Sørensen, 2009). For example, consider the important clinical task of analyzing physiological signals of a patient in order to detect abnormalities. Such signals include heart rate, blood pressure, respiration, and blood oxygen level. For an ICU patient, an abnormal state might be the precursor to a cardiac arrest event while for an epileptic, the state might presage a seizure (Goldberger et al., 2000). How can the latent state of the patient be inferred by a Bayesian modeler, so that, for example, an attending nurse can be notified when a patient enters an abnormal state? MJPs offer one attractive approach to analyzing such physiological signals.\nApplying an MJP model to physiological signals presents a challenge: the number of states is unknown and must be inferred using, for example, Bayesian nonparametric methods. However, efficient inference in nonparametric MJP models is a challenging problem, where existing methods based on particle MCMC scale poorly and mix slowly (Saeedi & Bouchard-Côté, 2011). Current optimization-based methods such as expectation maximization (EM) are inapplicable if the state size is countably infinite; hence, they cannot be applied to Bayesian nonparametric MJP models, as we would like to do for physiological signals.\nFurthermore, although MJPs are viewed as more realistic than their discrete-time counterparts in many fields (Rao & Teh, 2013), degenerate solutions for the maximum likelihood (ML) trajectories for both directly and indirectly observed cases (Perkins, 2009), and non-existence of the ML transition matrix (obtained from EM) for some indirectly observed cases (Bladt & Sørensen, 2009) present inferential challenges. Degenerate ML trajectories occur when some of the jump times are infinitesimal, which severely undermines the practicality of such approaches. For instance, a trajectory which predicts a patient’s seizure for an\nar X\niv :1\n50 3.\n00 33\n2v 3\n[ st\nat .M\nL ]\n5 J\nun 2\ninfinitesimal amount of time is of limited use to the medical staff. Fig. 3 shows an example of the degeneracy problem.\nIn this paper, we take a small-variance asymptotics (SVA) approach to develop an optimization-based framework for efficiently estimating the most probable trajectories (states) for both parametric and nonparametric MJP-based models. Small-variance asymptotics has recently proven to be useful in estimating the parameters and inferring the latent states in rich probabilistic models. SVA extends the wellknown connection between mixtures of Gaussians and kmeans: as the variances of the Gaussians approach zero, the maximum a posteriori solution to the mixture of Gaussians model degenerates to k-means solution (Kulis & Jordan, 2012). The same idea can be applied to obtain wellmotivated objective functions that correspond to a latent variable model for which scalable inference via standard methods like MCMC is challenging. SVA has been applied to (hierarchical) Dirichlet process mixture models (Kulis & Jordan, 2012; Jiang et al., 2012), Bayesian nonparametric latent feature models (Broderick et al., 2013), hidden Markov models (HMMs), and infinite-state HMMs (Roychowdhury et al., 2013).\nWe apply the SVA approach to both parametric and Bayesian nonparametric MJP models to obtain what we call the JUMP-means objective functions. In the parametric case, we derive a novel objective function which does not suffer from maximum likelihood’s solution degeneracy, leading to more stable and robust inference procedures in both the directly observed and hidden state cases. Infinitestate MJPs (iMJPs) are constructed from the hierarchical gamma-exponential process (HΓEP) (Saeedi & BouchardCôté, 2011). In order to apply SVA to iMJPs, we generalize the HΓEP to obtain the first deterministic procedure (we know of) for inference in Bayesian nonparametric MJPs.\nWe evaluate JUMP-means on several synthetic and realworld datasets in both the parametric and Bayesian non-\nparametric cases. JUMP-means performs on par with or better than existing methods, offering an attractive speedaccuracy tradeoff. We obtain significant improvements in the non-parametric case, gaining up to a 20% reduction in mean error on the task of observation reconstruction. In summary, the JUMP-means approach leads to algorithms that 1) are applicable to MJPs with Bayesian nonparametric priors; 2) provide non-degenerate solutions for the most probable trajectories; and 3) are comparable to or outperform other standard methods of inference both in terms of speed and reconstruction accuracy."
    }, {
      "heading" : "2. Background",
      "text" : ""
    }, {
      "heading" : "2.1. Markov Jump Processes",
      "text" : "A Markov jump process (MJP) is defined by (a) a finite (or countable) state space, which we identify with the integers [M ] , {1, . . . ,M}; (b) an initial state probability distribution π; (c) a (stochastic) state transition matrix P with pss = 0 for all s ∈ [M ]; and (d) a state dwell-time rate vector λ , (λ1, . . . , λM ). The process begins in a state s0 ∼ π. When the process enters a state s, it remains there for a dwell time that is exponentially distributed with parameter λs. When the system leaves state s, it transitions to state s′ 6= s with probability pss′ . A trajectory of the MJP is a sequence of states and a dwell time for each state, except for the final state: U , UT , (s0, t0, s1, t1, . . . , sK−1, tK−1, sK). Implicitly, K (and thus U) is a random variable such that tK−1 < T and the system is in state sK at time T . Let S , ST , (s0, s1, . . . , sK) and T , TT , (t0, t1, . . . , tK−1) be the sequences of states and times corresponding to U . The probability of a trajectory is given by\np(U |π, P,λ) = 1[t· ≤ T ]e−λsK (T−t·)πs0 (1) ×∏Kk=1 λsk−1e−λsk−1 tk−1psk−1sk ,\nwhere t· , ∑K−1 k=0 tk and 1[·] is the indicator function. In many cases when the states are directly observed, the initial state and the final state are observed, in which case it is straightforward to obtain a likelihood from (1).\nA hidden state MJP (HMJP) is an MJP in which the states are observed indirectly according to a likelihood model p(x | s), s ∈ [M ], x ∈ X , where X is some observation space. The times of the observations τ = (τ1, . . . , τL) are chosen independent of U , so the probability of the observations X , (x1, . . . , xL) is given by p(X |U , τ ) =∏L `=1 p(x` | sτ`), where, with an abuse of notation, we write sτ for the state of the MJP at time τ ."
    }, {
      "heading" : "2.2. Previous Approaches to MJP Inference",
      "text" : "There are a number of existing approaches to inference and learning in MJPs. An expectation-maximization (EM) algorithm can be derived, but it cannot be applied to models with countably infinite states, so it is not suitable for iMJPs (Lange, 2014) (iMJPs are detailed in Section 4). Moreover, with discretely observed data, the maximumlikelihood estimate with finite entries for the transition matrix obtained from EM may not exist (Bladt & Sørensen, 2005).\nMaximum likelihood inference amounts to finding maxU ln p(U |π, P,λ), which can be carried out efficiently using dynamic programming (Perkins, 2009). However, maximum likelihood solutions for the trajectory are degenerate: only an infinitesimal amount of time is spent in each state, except for the state visited with the smallest rate parameter (i.e., longest expected dwell time). Such a solution is unsatisfying and unintuitive because the dwell times are far from their expected values. Thus, maximum likelihood inference produces results that are unrepresentative of the model behavior.\nMarkov chain Monte Carlo methods have also been developed, but these can be slow and their convergence is often difficult to diagnose (Rao & Teh, 2013). Recently, a more efficient Monte Carlo method was proposed in Hajiaghayi et al. (2014) which is based on particle MCMC (PMCMC) methods (Andrieu et al., 2010). This approach addresses the issue of efficiency, but since it marginalizes over the jump points, it cannot provide probable trajectories."
    }, {
      "heading" : "2.3. Small-variance Asymptotics",
      "text" : "Consider a Bayesian model p(D |Z, θ, σ2)p(Z, θ) in which the likelihood terms contain a variance parameter σ2. Given some data D, a point estimate for the parameters θ and latent variables Z of the model can obtained by maximizing the posterior p(Z, θ |D,σ2) ∝ p(D |Z, θ, σ)p(Z, θ), resulting in a maximum a posteriori (MAP) estimate. In the SVA approach (Broderick et al.,\n2013), the MAP optimization is considered in the limit as the likelihood variance parameter is taken to zero: σ2 → 0. Typically, the small-variance limit leads to a much simpler optimization than the MAP optimization with non-zero variance. For example, the MAP objective for a Gaussian mixture model simplifies to the k-means objective."
    }, {
      "heading" : "3. Parametric MJPs",
      "text" : ""
    }, {
      "heading" : "3.1. Directly Observed MJP",
      "text" : "Consider the task of inferring likely state/dwell-time sequences given O = {(t̃i, s̃i)}Ii=1, the times at which the system was directly observed and the states of the system at those times. For simplicity we assume that t̃0 = 0 and that all times are in the interval [0, T ]. Let s(U , t) be the state of the system following trajectory U at time t. The likelihood of a sequence is\n`(U |O, P,λ) = 1[t· ≤ T ] I∏ i=1 1[s(U , t̃i) = s̃i]\n× (\nK∏ k=1 λsk−1e −λsk−1 tk−1psk−1sk\n) e−λsK (T−t·) (2)\nWe also place a gamma prior on the rate parameters λ (detailed below). Instead of relying on MAP estimation, we apply a small variance asymptotics analysis to obtain a more stable objective function. Following (Jiang et al., 2012), we scale the distributions by an inverse variance parameter β and then maximize the scaled likelihood and prior in the limit β →∞ (i.e., as the variance goes to zero). Scaling the exponential distribution f(t;λ) = λ exp(−λt) produces the two-parameter family with\nln f(t;λ, β) = − β ( λt− ln t− lnλ− β lnβ − ln Γ(β)\nβ +\nln t\nβ\n) ,\nwhich is the density of a gamma distribution with shape parameter β and rate parameter βλ. Hence, the mean of the scaled distribution is 1λ and its variance is 1 λ2β . Letting F (t;λ, β) denote the CDF corresponding to f(t;λ, β), we have 1 − F (t;λ, β) = Γ(β,βλt)Γ(β) , where Γ(·, ·) is the upper incomplete gamma function.\nThe multinomial distribution is scaled by the parameter β̂ , ξβ. Writing the likelihood with the scaled exponential families (and dropping indicator variables) yields:\n`(U|O, P,λ) ∝ exp { −β (\nln Γ(β)− ln Γ(β, βλsK t·) β\n(3)\n+ K−1∑ k=0 ( ξ ln psksk+1 + λsktk − lnλsktk )\n+ K−1∑ k=0 ( −β lnβ − ln Γ(β) β + ln tk β ))} .\nThe modified likelihood is for a jump process which is no longer Markov when β 6= 1. We also place a Gam(αλ, αλµλ) prior on each λi and set αλ = ξλβ. It can be shown (see the Supplementary Material for details) that, when β →∞, the MAP estimation problem becomes\nmin U,λ,P\n{ ξ K−1∑ k=0 ln psksk+1 + K−1∑ k=0 (λsktk − lnλsktk − 1)\n+ 1[λsK t· ≥ 1](λsK t· − lnλsK t· − 1) (4)\n+ ξλ M∑ s=1 (µλλs − lnλs − 1) } .\nThe optimization problem (A.4) is very natural and offers far greater stability than maximum likelihood optimization. As with maximum likelihood, the ln psksk+1 terms penalize transitions with small probability. The term h(tk) , λsktk − lnλsktk − 1 is convex and minimized when tk = 1/λsk , the expected value of the dwell time for state sk. As tk → 0, h(tk) approaches ∞, while for tk 1/λsk , h(tk) grows approximately linearly. Thus, times very close to zero are heavily penalized while times close to the expected dwell time are penalized very little. The term 1[λsK t· ≥ 1](λsK t· − lnλsK t· − 1) penalizes the time t· spent in state sk so far in the same manner as a regular dwell time when t· is greater than the expected value of the dwell-time. However, when t· is less than the expected value there is no cost, which is quite natural since the system may remain in state sk for longer than t· — i.e., there should not be a large penalty for t· being less than its expected value. Finally, parameters ξλ and µλ have a very natural interpretation (cf. (8) below): they correspond to a priori having ξλ dwell times of length µλ for each state.\nComparison to Maximum Likelihood MJP trajectories estimated using maximum likelihood (MLE) are usually trivial, with the system spending almost all its time in a single state (with the smallest λ), with infinitesimal dwell times for the other states. This poor behavior of MLE is due to the fact that the mode of Exp(λ), which is favored by the MLE, is 0, even though the mean is 1/λ.1 The SVA optimization, on the other hand, does give trajectories that are representative of the true behavior because the SVA terms of the form λt− ln(λt)−1 are optimized at 1/λ (i.e., at the mean of Exp(λ)). We demonstrate the superior behavior of the SVA in the concrete example of estimating disease progression in patients in Section 5.\n1Note that placing priors on the rate parameters, as we do, does not affect the degeneracy of the ML trajectory."
    }, {
      "heading" : "3.2. Hidden State MJP",
      "text" : "For an HMJP, the likelihood of a valid trajectory is\np(U |X , τ , P,λ) = (\nL∏ `=1 p(x` | sτ`) )\n× (\nK∏ k=1 λsk−1e −λsk−1 tk−1psk−1sk\n) e−λsK (T−t·). (5)\nHence, the only difference between the directly observed case and the HMJP is the addition of the observation likelihood terms. Because multinomial observations are commonly used in MJP applications, that is the case we consider here. Let N denote the number of possible observations and ρsn be the probability of observing x` = n when sτ` = s. The observation likelihoods are scaled in the same manner as the transition probabilities, but with β̂ = ζβ. Thus, for the HMJP, we obtain:\nmin U,λ,P,ρ\n{ ζ L∑ `=1 ln ρsτ`x` + ξ K−1∑ k=0 ln psksk+1\n+ K−1∑ k=0 (λsktk − lnλsktk − 1) (6) + 1[λsK t· ≥ 1](λsK t· − lnλsK t· − 1)\n+ ξλ M∑ s=1 (µλλs − lnλs − 1) } ."
    }, {
      "heading" : "3.3. Algorithm",
      "text" : "Optimizing the JUMP-means objectives in (A.4) and (6) is non-trivial due to the fact that we do not know the number of jumps in the MJP, and the combinatorial explosion in the sequences with the number of jump points. The terms involving the continuous variables tk (dwell times) present an additional complexity.\nWe therefore resort to an alternating minimization procedure to optimize the JUMP-means objective function, similar in spirit to the one used in Roychowdhury et al. (2013). In each iteration of the optimization process, we first use a modified Viterbi algorithm to obtain the most likely state sequence. Then, we use convex optimization to distribute the jump points optimally with respect to the values from λ for the current state sequence.\nDirectly Observed MJP When optimizing (A.4), there may be many sequences (O’s) available, representing distinct realizations of the process. We use the following algorithm to optimize (A.4):\n1. Initialize the state transition matrix P and rate vector λ with uniform values.\n2. For every observation sequence O, instantiate the jump points by adding one jump point between every pair of observations, in addition to the start and end points.\n3. For each O, use a modified Viterbi algorithm. to find the best state sequence to optimize (A.4), while keeping the jump points fixed. The modified algorithm includes the dwell time penalty terms, which are dependent upon the assignment of states to the time points.\n4. Optimize the dwell times tk with the state sequences of the trajectories fixed.\n5. Optimize P and λ with the other variables fixed. The optimal values can be obtained in closed form. For example, if there is only a single observation sequence O with corresponding inferred trajectory S, then\npmj = nmj∑M j=1 nmj , m, j ∈ [M ] (7)\nλm = ξλ +\n∑ k 1[sk = m]\nξλµλ + ∑ k 1[sk = m]tk , (8)\nwhere nmj denotes to the number of transitions from state m to state j in S.\n6. Repeat steps 3-5 until convergence.\nBeam Search Variant We note that the optimization procedure just described is restrictive since the number of jump points is fixed and the jump points are constrained by the observation boundaries. To eliminate this, we also tested a beam search variant of the algorithm to allow for the creation and removal of jump points, but found it did not have much impact in our experiments.\nHidden State MJP The algorithm to optimize the hidden state MJP JUMP-means objective (6) is similar to that for optimizing (A.4), but with three modifications. First, in place of O, we have the indirect observations of the states X . Second, observation likelihood terms containing ρ are included in the objective minimized by the Viterbi optimization (step 3). Finally, an additional update is performed in step 5 for each of the observation distributions ρm:\nρmn =\n∑ ` 1[sτ` = m]1[x` = n]∑\n` 1[sτ` = m] (9)\nfor m ∈ [M ] and n ∈ [N ]. If each ρm , (ρm1, . . . , ρmN ) is initialized to be uniform, then the algorithm converges to a poor local minimum, so we add a small amount of random noise to each uniform ρm."
    }, {
      "heading" : "4. Bayesian Nonparametric MJPs",
      "text" : "We now consider the Bayesian nonparametric MJP (iMJP) model. The iMJP is based on the hierarchical gammaexponential process (HΓEP), which is constructed from the\ngamma-exponential process (ΓEP). We denote the Moran gamma process with base measure H and rate parameter γ by ΓP(H, γ) (Kingman, 1993). The HΓEP generates a state/dwell-time sequence s0, t1, s1, t2, s2, t3, s3, . . . (with s0 assumed known) according to (Saeedi & BouchardCôté, 2011):\nµ0 ∼ ΓP(α0H0, γ0), (10) µi |µ0 i.i.d.∼ ΓP(µ0, γ), i = 1, 2, . . . , (11) sk | {µi}∞i=0,Uk−1 ∼ µ̄sk−1 , (12) tk | {µi}∞i=0,Uk−1 ∼ Exp(‖µsk−1‖), (13)\nwhere H0 is the base probability measure, α0 is a concentration parameter, Uk , (s0, t1, s1, t2, s2, . . . , tk−1, sk), µ̄i , µi/‖µi‖, and ‖µ‖ denotes the total mass of the measure µ. As in the parametric case, we must replace the exponential distribution in (13) with the scaled exponential distribution. After an appropriate scaling of the rest of the hyperparameters, we obtain the hierarchical gammagamma process (HΓΓP). The definition and properties of the HΓΓP are given in the Supplementary Material.\nLetM denote the number of used states,Km the number of transitions out of statem, and µij the mass on the j-th component of the measure µi. For 0 ≤ i ≤M, 1 ≤ j ≤M , let π̄ij , µ̄ij and for 0 ≤ i ≤M , let π̄i,M+1 , 1− ∑M j=1 µ̄ij . Let t∗m , (t ∗ m1, . . . , t ∗ mKm\n) be the waiting times following state m and define t∗m· , ∑Km j=1 t ∗ mj . In order to retain the effects of the hyperparameters in the asymptotics, set α0 = exp(−ξ1β) and γ0 = κ0 = ξ2. It can then be shown that (see the Supplementary Material for details), when β →∞, the iMJP MAP estimation problem becomes\nmin K,UK ,π̄,ρ ζ L∑ `=1 ln ρsτ`x` + ξ K∑ k=1 ln π̄sk−1sk\n+ ξ1M + M∑ m=1 ξ2KL(π̄0||π̄m) (14)\n− M∑ m=1 {∑Km j=1 ln t ∗ mj −Km ln ([γ + t∗m·]/Km) } .\nLike its parametric counterpart, the Bayesian nonparametric cost function penalizes dwell durations very close to zero via the ln t∗mj terms. In addition, there are penalties for the number of states and the state transitions. The observation likelihood term in (14) favors the creation of new states to minimize the JUMP-means objective, while the state penalty ξ1M and the non-linear penalty term Km ln ([γ + t ∗ m·]/Km) counteracts the formation of a long tail of states with very few data points. The γ hyperparameter introduces an additional, nonlinear cost for each additional state — if a state is occupied for Ω(γ) time, then the γ term for that state does not have much effect on the cost.\nThe KL divergence terms between π̄0 and π̄m arise from the hierarchical structure of the prior, biasing the transition probabilities π̄m to be similar to the prior π̄0."
    }, {
      "heading" : "4.1. Algorithm",
      "text" : "For the iMJP case, we have the extra variables M and {π̄m}Mm=0 to optimize. In addition, the number of variables to optimize depends on the number of states in our model. The major change in the algorithm from the parametric case is that we must propose and then accept or reject the addition of new states. We propose the following algorithm for optimizing the iMJP:\n(1) Initialize ρ, π̄0 and π̄1 with uniform values and set the number of states M = 1.\n(2) For each observation sequence, apply the Viterbi algorithm and update the times using the new objective function in (14), analogously to steps (3) and (4) in the parametric algorithm.\n(3) Perform MAP updates for ρ (as in (9)) and π̄:\nπ̄mj = ξnmj + ξ2π̄0j ξ ∑M j=1 nmj + ξ2π̄0j , m, j ∈ [M ] (15)\nπ̄0j ∝ M∏ m=1 π̄ 1/M mj , j ∈ [M ]. (16)\n(4) For every state pair m,m′ ∈ [M ], form a new state M + 1 by considering all transitions from m to m′ and reassigning all observations x` that were assigned to m′ to the new state. Update π̄ and ρ to estimate the overall objective function for every new set of M + 1 states formed in this way and accept the state set that minimizes the objective. If no such set exists, do not create a new state and revert back to the old π̄ and ρ.\n(5) Repeat steps 2-4 until convergence.\nRemark. If instead of multinomial observations we have Gaussian observations, the parameter ρs is replaced with the mean parameter µs. In this case, we update the mean for each state using the data points assigned to the state, similar to the procedure for k-means clustering (see, e.g., Jiang et al. (2012); Roychowdhury et al. (2013))."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section we provide a quantitative analysis of the JUMP-means algorithm and compare its performance on synthetic and real datasets with standard inference methods for MJPs. For evaluation, we consider multiple sequences of discretely observed data and randomly hold out a subset of the data. We report reconstruction error for performance comparison."
    }, {
      "heading" : "5.1. Parametric Models",
      "text" : "For the parametric models, we compare JUMP-means to maximum likelihood estimation of the MJP parameters learned by EM (Asger & Ledet, 2005), the MCMC method proposed in Rao & Teh (2013) and a simple baseline where we ignore the sequential structure of the data. We run three sets of experiments (2 synthetic, 1 real) for our evaluation.\nSynthetic 1: Directly Observed States For evaluating the model on a directly observed process, we generate 100 different datasets randomly from various MJPs with 10 states. To generate each dataset, we first generate the rows of the transition probability matrix and transition rates independently from Dir(1) and Gam(1, 1), respectively. Next, given the rates and transition probabilities for each dataset, we sample 500 sequences of length 20. We hold out 30% of the observations at random for testing reconstruction error.\nWe run JUMP-means by initializing the algorithm with a uniform transition matrix P and set the rate vector λ to be all ones. We run 300 iterations of the algorithm described in Section 3.3; each iteration is one scan through all the sequences. We set the hyperparameters ξ, ξλ, and µλ equal to 1, 1, and .5, respectively. For MCMC, we initialize the jump points using the time points of the observations. We place independent Dir(1) priors on P and independent Gam(1, 1) priors on λ. We initialize EM with a uniform P and an all-ones λ. We run both MCMC and EM for 300 iterations, then reconstruct observations using the Bayes estimator approximated from the 300 posterior samples. For our baseline we use the most common observation in the dataset as an estimate of the missing observations.\nTable 1 gives the mean reconstruction error across sequences for the various methods. Note that JUMP-means performs better than MCMC, and is almost on par with EM.\nFig. 2(a) shows the average error across all the datasets for each method versus number of iterations. In terms of CPU time, each iteration of JUMP-means (Java), EM (Java), and MCMC (Python) takes 0.3, 1.61 and 42 seconds, respectively. We also ran experiments with the beam search variant described in Section 3.3; however, we did not obtain any significant improvement in results.\nSynthetic 2: Hidden States For the hidden state case, we generate 100 different datasets for MJPs with 5 hidden and 5 observed states, with varying parameters as above. In each dataset there are 500 sequences of length 20. In addition to parameters in the directly observed case, we generate observation likelihood terms for each state from Dir(1).\nWe initialize the transition probabilities and the rate vectors for JUMP-means, MCMC and EM in a fashion similar to the directly observed case. For the observation likelihood ρ, we use Dir(1) as a prior for MCMC, uniform distributions for EM initialization and a uniform probability matrix with a small amount of random noise for JUMP-means initialization. We set ξ, ξλ, µλ as before and ξ to 1.\nWe run each algorithm for 300 iterations. For JUMPmeans, we use the hidden state MJP algorithm described in Section 3.3. Table 1 and Fig. 2(b) again demonstrate that JUMP-means outperforms MCMC by a large margin and performs comparably to EM. The poor performance of MCMC is due to slow mixing over the parameters and state trajectories. The slow mixing is a result of the coupling between the latent states and the observations, which is induced by the observation likelihood.\nDisease Progression in Multiple Sclerosis (MS) Estimating disease progression and change points in patients with Multiple Sclerosis (MS) is an active research area (see, e.g., Mandel (2010)). We can cast the progression of the disease in a single patient as an MJP, with different states representing the various stages of the disease. Obtaining the most-likely trajectory for this MJP can aid in understanding the disease progression and enable better care.\nFor our experiments, we use a real-world dataset collected from a phase III clinical trial of a drug for MS. This dataset\ntracks the progression of the disease for 72 different patients over three years. We randomly hold out 50% of the observations and evaluate on the observation reconstruction task. The observations are values of a disability measure known as EDSS, recorded at different time points. Initialization and hyperparameters are the same as Synthetic 1.\nTable 1 shows that JUMP-means significantly outperforms MCMC, achieving almost a 50% relative reduction in reconstruction error. JUMP-means again achieves comparable results with EM. Fig. 3 (top panel) provides an example of the latent trajectories from JUMP-means and maximum likelihood estimate for a single patient. The MLE trajectory includes two infinitesimal dwell times, which do not reflect realistic behavior of the system (since we do not expect a patient to be in a disease state for an infinitesimal amount of time). On the other hand, the trajectory produced by JUMP-means takes into account the dwell times of the various stages of the disease and provides a more reasonable picture of its progression."
    }, {
      "heading" : "5.2. Nonparametric Model",
      "text" : "Vital Signs Monitoring (MIMIC) We now consider a version of the problem of understanding physiological signals discussed in the introduction. We use data from the MIMIC database (Goldberger et al., 2000; Moody & Mark, 1996), which contains recordings of several vital parameters of ICU patients. Specifically, we consider blood pressure readings of 69 ICU patients collected over a 24-hour period and sub-sample observation sequences of length 32 for each patient, keeping the start and end times fixed.2 For testing, we randomly hold out ∼25% of the observations. To initialize JUMP-means, we choose uniform matrices for ρ, π̄0 and π̄1 and set M = 1. The hyperparameters γ and ξ1 are set to 5, while ζ, ξ, and ξ2 are set to 0.005. Using a Gaussian likelihood model for the observations, we run our model for 50 iterations. We compare with particle MCMC (PMCMC) (Andrieu et al., 2010) and EM. PMCMC is a state-of-the-art inference method for iMJPs (Saeedi & Bouchard-Côté, 2011), which we run for 300 iterations with 100 particles. For PMCMC, we first categorize the readings into the standard four categories for blood pressure provided by NIH3. We run EM with a number of hidden states from 1 to 12 and report the best performance among all the results. For initializing the EM, we use the same setting as the Synthetic 2 case.\nFor evaluation, we consider the time point of a test observation and categorize the mean of the latent state at this time point (using the same categories obtained above) to compare against the actual category. Table 1 shows that JUMP-means significantly outperforms PMCMC and obtains a 21% relative reduction in average error rate. Fig. 2(c) plots the error against iterations of both algorithms. In terms of CPU time, each iteration of JUMP-means (Java) and PMCMC (Java) takes 0.17 and 1.95 seconds, respectively. Compared to EM’s error rate of 25.7%, JUMPmeans reaches a rate of 24.3% without the need to separately train for different number of states. The second-best\n2We use a small dataset for testing since PMCMC cannot easily scale to larger datasets.\n3http://www.nhlbi.nih.gov/health/ health-topics/topics/hbp\nresult for the EM had an error of 45%, which shows the importance of model selection when using EM.\nFig. 3 (bottom) provides an example of the latent trajectory inferred by JUMP-means. The observations are uniquely colored by the latent state they are assigned. We note that the model captures different levels of blood pressure readings and provides a non-degenerate latent trajectory.\nHyperparameters A well-known problem when applying SVA methods is that there are a number of hyperparameters to tune. In our objective functions, some of these hyperparameters (γ, µλ, and ξλ) have natural interpretations so prior knowledge and common sense can be used to set them, but others do not. Fig. 4 shows histograms over the errors we obtain for runs of JUMP-means on the MS and MIMIC datasets with different settings. We can see that a significant fraction of the runs converge to the minimum error, while some settings — in particular when the hyperparameters were of different orders of magnitude — led to larger errors. Hence, the sensitivity study indicates the robustness of JUMP-means to the choice of hyperparameters.\nScaling Fig. 5 shows the total runtime and reconstruction error of the non-parametric JUMP-means algorithm on increasingly large amounts of synthetic data. The algorithm is able to handle up to a million data points with the runtime scaling linearly with data size. Furthermore, the error rate decreases significantly as the amount of data increases. See the Supplementary Material for further details."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We have presented JUMP-means, a new approach to inference in MJPs using small-variance asymptotics. We derived novel objective functions for parametric and Bayesian nonparametric models and proposed efficient algorithms to optimize them. Our experiments demonstrate that JUMPmeans can be used to obtain high-quality non-degenerate estimates of the latent trajectories. JUMP-means offers attractive speed-accuracy tradeoffs for both parametric and nonparametric problems, and achieved state-of-the-art reconstruction accuracy on nonparametric problems."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "Thanks to Monir Hajiaghayi, Matthew Johnson, and Tejas Kulkarni for helpful comments and discussions. JHH was supported by the U.S. Government under FA9550-11C-0028 and awarded by the DoD, Air Force Office of Scientific Research, National Defense Science and Engineering Graduate (NDSEG) Fellowship, 32 CFR 168a."
    }, {
      "heading" : "A. Parametric MJPs for SVA",
      "text" : "To obtain the SVA objective from the parametric MJP model, we begin by scaling the exponential distribution f(t;λ) = λ exp(−λt), which is an exponential family distribution with natural parameter η = −λ, log-partition function ψ(η) = − ln(−η), and base measure ν(dt) = 1 (Banerjee et al., 2005). To scale the distribution, introduce the new natural parameter η̃ = βη and log-partition function ψ̃(η̃) = βψ(η̃/β). The new base measure ν̃(dt) is uniquely defined by the integral equation (see Banerjee et al., 2005, Theorem 5)∫\nexp(η̃t)ν̃(dt) = exp(ψ̃(η̃)) = exp(−β ln(η̃/β)) = β β\nη̃β .\nChoosing ν̃(dt) = t β−1ββ\nΓ(β) dt satisfies the condition, so we have\nf(t;λ, β) = (βλ)β\nΓ(β) tβ−1e−βλt = exp(−βλt+ (β − 1) ln t+ β lnλβ − ln Γ(β))\n= exp { −β ( λt− ln t− lnλ− β lnβ − ln Γ(β)\nβ +\nln t\nβ\n)} .\nIt can now be seen that f(t;λ, β) is the density of a gamma distribution with shape parameter β and rate parameter βλ. Hence, the mean of the scaled distribution is 1λ and its variance is 1 λβ . Letting F (t;λ, β) denote the CDF corresponding to f(t;λ, β), we have 1− F (t;λ, β) = Γ(β,βλt)Γ(β) , where Γ(·, ·) is the upper incomplete gamma function. For the state at the k-th jump we use a 1-of-M representation; that is, sk is an M -dimensional binary random variable which satisfies skm ∈ {0, 1} and ∑M m=1 skm = 1. Hence, we have:\np(sk|sk−1,j = 1) = M∏ m=1 pskmjm . (A.1)\nGiven the Bregman divergence for a multinomial distribution, dφ(sk,pj) = KL(sk||pj) where pj , (pj1, . . . , pjM ), this can be written in terms of exponential family notation in the following form (Banerjee et al., 2005):\np(sk|sk−1,j = 1) = bφ(sk) exp(−dφ(sk,pj)) (A.2)\nwhere bφ(sk) = 1. For a scaled multinomial distribution we have bβ̂φ(sk) exp(−β̂dφ(sk,pj)), where β̂ = ξβ is the scaling parameter for the multinomial distribution. Writing the trajectory probility with the scaled exponential families yields:\np(U|s0, sK , P,λ) ∝ exp { −β (\nln Γ(β)− ln Γ(β, βλsK t·) β + ξ K−1∑ k=0 KL(sk+1||psk)\n+ K−1∑ k=0 ( λsktk − lnλsktk − β lnβ − ln Γ(β) β + ln tk β ))} ,\n(A.3)\n10\nSince β → ∞, we can apply the asymptotic expansions for Γ(·) and Γ(·, ·). In particular, applying Stirling’s formula and the facts in (DLMF) we have:\nβ lnβ − ln Γ(β) β = β lnβ − β lnβ + β + o(β) β → 1\nln Γ(β)− ln Γ(β, βλt) β =\n{ −β−o(β)−β lnλt+βλt\nβ → λt− lnλt− 1 if t ≥ 1λ β ln β−β−β ln β+β+o(β)\nβ → 0 if t < 1λ\nWe also place a Gam(αλ, αλµλ) prior on each λi. With αλ = ξλβ, we obtain\nln p(λs |αλ, αλµλ) = αλ ln(αλµλ) + (αλ − 1) lnλs − ln Γ(αλ)− αλµλλs = ξλβ lnλs − ξλµλβλs + ξλβ + o(β) = −β(ξλµλλs − ξλ lnλs − 1) + o(β).\nHence, when β →∞, obtain\nmin U,λ,P\n{ ξ K−1∑ k=0 KL(sk+1||psk) + K−1∑ k=0 (λsktk − lnλsktk − 1)\n+ 1[λsK t· ≥ 1](λsK t· − lnλsK t· − 1) + ξλ M∑ s=1 (µλλs − lnλs − 1) } (A.4)"
    }, {
      "heading" : "B. Bayesian Nonparametric MJPs for SVA",
      "text" : "First we recall that the Moran gamma process is a distribution over measures. If µ ∼ ΓP(H, γ) is a random measure distributed according to a Moran gamma process with base measure H on the probability space (Ω,F) and rate parameter γ, then for all measurable partitions of Ω, (A1, . . . , A`), µ satisfies\n(µ(A1), . . . , µ(A`)) ∼ Gam(H(A1), γ)× · · · × Gam(H(A`), γ). (B.1)\nThe hierarchical gamma-gamma process (HΓΓP) is defined to be:\nµ0 ∼ ΓP(α0H0, γ0) (B.2) µi |µ0 i.i.d.∼ ΓP(βµ0, γ) i = 1, 2, . . . (B.3) sk | {µi}∞i=0,Uk−1 ∼ µ̄sk−1 (B.4) tk | {µi}∞i=0,Uk−1 ∼ Gam(β, ‖µsk−1‖). (B.5)\nConsider the gamma-gamma process (ΓΓP), defined by (B.3)-(B.5) (with µ0 treated as an arbitrary fixed measure). We now show that the ΓΓP retains the key properties of the ΓEP: conjugacy and exchangeability. Let Ti , ∑k j=1 1[sj−1 = i]tj\nand Fi , ∑k j=1 1[sj−1 = i]δsj be the sufficient statistics of the observations. Proposition B.1. The ΓΓP is a conjugate family: µi | Uk ∼ ΓP(βµ′i, γ′i), where µ′i = µ0 + Fi and γ′i = γ + Ti.\nProof sketch. The proof is analogous to that for Proposition 2 in (Saeedi & Bouchard-Côté, 2011). The key additional insight is that X ∼ Gam(βa, b) and Y |X ∼ Gam(β,X) are conjugate: X |Y ∼ Gam(β(a+ 1), b+ Y ).\nIn order to give the joint distribution of the times T , TK , (t1, . . . , tK), we first derive the predictive distribution for the ΓΓP, (sk+1, tk+1) | Uk. We make use of the following family of densities. Definition B.2 (Shaped Translated Pareto). Let β > 0, α > 0, γ > 0. A random variable S is shaped translated Pareto, denoted S ∼ STP(β, α, γ), if it has density\nf(t) = γαβ\nB(β, αβ)\ntβ−1\n(t+ γ)(1+α)β ,\nwhere B(a, b) = Γ(a)Γ(b)Γ(a+b) is the beta function.\nProposition B.3. The predictive distribution of the ΓΓP is\n(sk+1, tk+1) | Uk ∼ µ̄′sk × STP(β, ‖µ̄′sk‖, γ′sk). (B.6)\nProof. By Proposition B.1, it suffices to show that if µ ∼ ΓP(βµ0, γ), s |µ ∼ µ̄, and t |µ ∼ Gam(β, ‖µ‖), then (s, t) ∼ µ̄× STP(β, κ0, γ), where κ0 , ‖µ0‖. Letting x = ‖µ‖, the distribution of t is\np(t) = ∫ ∞ 0 p(t |x)p(x)dx = ∫ ∞ 0 xβtβ−1e−xt Γ(β) γβκ0xβκ0−1e−γx Γ(βκ0) dx\n= γβκ0tβ−1\nΓ(β)Γ(βκ0) ∫ ∞ 0 xβ(1+κ0)−1e−(γ+t)xdx = γβκ0tβ−1 Γ(β)Γ(βκ0) Γ(β(1 + κ0)) (γ + t)β(1+κ0) .\nWe can now show that the process is exchangeable by exhibiting the joint distribution of waiting times: Proposition B.4. Let t∗m = (t∗m1, . . . , t∗mKm) be the waiting times following statem. Then t ∗ m is an exchangeable sequence with joint distribution\np(t∗m) = Γ(β(κ0 +Km))\nΓ(β)Km\n( ∏Km j=1 t ∗ mj) β−1\n(γ + ∑Km j=1 τmj) β(κ0+Km) (B.7)\nProof sketch. Take the product of the predictive distributions of τm1, . . . , τmKm .\nThe measures {µi}∞i=0 and H0 can be integrated out of the HΓΓP generative model in a manner analogous to the way in the the Chinese restaurant franchise in obtained from the hierarchical Dirichlet process (Teh et al., 2006). However the mass of the measure µ0 cannot be integrated out. We omit details as they are essentially identical to those in case of the HΓEP (Saeedi & Bouchard-Côté, 2011).\nFirst, we consider the case of integrating out {µi}i≥0. Let M denote the number of used states, Km the number of transitions out of state m, and rm the number of states that can be reached from state m in one step. The contribution to the likelihood from the HΓΓP prior is\np(U , κ0 |β, γ0, γ, α0) = p(κ0 |α0, γ0)p(S |β, α0, κ0)p(T |β, γ, κ0)\n∝ κα0−10 e−γ0κ0αM−10 Γ(α0 + 1)\nΓ(α0 + r·) M∏ m=1 (βκ0) rm−1 Γ(βκ0 + 1) Γ(βκ0 +Km)\n× M∏ m=1 Γ(β(κ0 +Km)) Γ(β)Km\n( ∏Km j=1 t ∗ mj) β−1\n(γ + ∑Km j=1 t ∗ mj) β(κ0+Km) ,\nwhere r· , ∑ m rm. Taking the logarithm, using asymptotic expansions for the Gamma terms, and ignoring o(β) terms yields\n(α0 − 1) lnκ0 − γ0κ0 + (M − 1) lnα0 + M∑ m=1 {(rm − 1) lnκ0 + β(κ0 +Km) ln[β(κ0 +Km)]}\nM∑ m=1 { −β(κ0 +Km)−Km[β lnβ − β] + β ∑Km j=1 ln t ∗ mj − β(κ0 +Km) ln (γ + t∗m·) } ,\nwhere t∗m· , ∑Km j=1 t ∗ mj . In order to retain the effects of the hyperparameters in the asymptotics, set α0 = exp(−ξ1β) and γ0 = exp(ξ2β). Thus, κ0 → 0 as β → ∞. We require that lim supβ→∞ κ0γ0 < ∞, so without loss of generality we can choose κ0 = γ−10 = exp(−ξ2β) to obtain\n−β ( ξ1(M − 1) +\nM∑ m=1 { ξ2(rm − 1)− ∑Km j=1 ln t ∗ mj +Km ln ([γ + t ∗ m·]/Km) }) .\nThus, the objective function to minimize is\nζ L∑ `=1 KL(x`||ρsτ` ) + ξ1M + M∑ m=1 { ξ2(rm − 1)− ∑Km j=1 ln t ∗ mj −Km ln ([γ + t∗m·]/Km) } . (B.8)\nAlternatively, the small variance asymptotics can be derived in the case where {µi}i≥0 is not integrated out. To do so, we first rewrite the HΓΓP generative model in an equivalent form, with H0 integrated out:\nπ0 ∼ GEM(α0) (B.9) κ0 ∼ Gam(α0, γ0) (B.10) πi |π0 i.i.d.∼ DP(βκ0π0), i = 1, 2, . . . (B.11) κi |π0 i.i.d.∼ Gam(β, γ), i = 1, 2, . . . (B.12) sk | {πi}∞i=1,Uk−1 ∼ πsk−1 (B.13) tk | {κi}∞i=1,Uk−1 ∼ Gam(β, κsk). (B.14)\nFor 0 ≤ i ≤M, 1 ≤ j ≤M , let π̄i,j , πij and for 0 ≤ i ≤M , let π̄i,M+1 , 1− ∑M j=1 πij . Integrating out {κi}i≥1, the contribution to the likelihood from the HΓΓP prior is now\np(UK , κ0, π̄ |β, γ0, γ, α0) (B.15) = p(κ0 |α0, γ0)p(π̄0 |α0)p(π̄1:M |βκ0π̄0)p(SK | π̄1:M )p(TK |β, γ, κ0) (B.16)\n∝ κα0−10 e−γ0κ0 M∏ i=1 Beta\n( π̄0i 1−∑i−1j=1 π0,j ∣∣∣∣∣1, α0 ) Dir(π̄i |βκ0π̄0) ( K∏ k=1 π̄sk−1,sk ) p(TK |β, γ, κ0) (B.17)\n∝ κα0−10 e−γ0κ0 M∏ i=1 Γ(1 + α0)Γ(α0) ( 1−∑ij=1 π0,j 1−∑i−1j=1 π0,j )α0−1 Γ(βκ0) M+1∏ j=1 π̄ βκ0π̄0j−1 ij Γ(βκ0π̄0j)  ×\nK∏ k=1 π̄βξsk−1,sk × M∏ m=1 Γ(β(κ0 +Km)) Γ(β)Km\n( ∏Km j=1 t ∗ mj) β−1\n(γ + ∑Km j=1 t ∗ mj) β(κ0+Km) .\n(B.18)\nWe use a slightly different limiting process, with γ0 = κ0 = ξ2, a positive constant, and scale the multinomial distributions (B.13) by βξ. Taking the logarithm and and ignoring o(β) terms as before yields\nM∑ i=1 lnα0 + βκ0 lnβκ0 − β + M+1∑ j=1 {−βκ0π̄0,j ln(βκ0π̄0,j) + βκ0π̄0,j + βκ0π̄0,j ln π̄ij}  +\nK∑ k=1 βξ ln π̄sk−1,sk + M∑ m=1 {∑Km j=1 β ln t ∗ mj − βKm ln ([γ + t∗m·]/Km) }\n∼ M∑ i=1 −βξ1 + M+1∑ j=1 {−βκ0π̄0,j ln(π̄0,j) + βκ0π̄0,j ln π̄ij}  +\nK∑ k=1 βξ ln π̄sk−1,sk + M∑ m=1 {∑Km j=1 β ln t ∗ mj − βKm ln ([γ + t∗m·]/Km) } ∼ −β { ξ1M + ξ\nK∑ k=1 ln π̄sk−1,sk + M∑ m=1 { ξ2KL(π̄0||π̄m)− ∑Km j=1 ln t ∗ mj −Km ln ([γ + t∗m·]/Km) }} .\nThus, the objective function to minimize is\nζ L∑ `=1 ln ρsτ`x` + ξ K∑ k=1 ln π̄sk−1,sk + ξ1M\n+ M∑ m=1 { ξ2KL(π̄0||π̄m)− ∑Km j=1 ln t ∗ mj −Km ln ([γ + t∗m·]/Km) } .\n(B.19)"
    }, {
      "heading" : "C. Time-accuracy plots for the experiments",
      "text" : "In the main paper we include the error versus iteration as it is more objective than time-accuracy results. In Fig. C.6, we compare the time-accuracy across different methods for different datasets. EM, PMCMC, and JUMP-means are implemented in Java and MCMC is implemented in Python. To plot the MCMC results, we give a speed boost of 100x in the results to compensate for Python’s slow interpreter. From our experience with scientific computing applications, we believe this is a generous adjustment. Also we note that the EM implementation used in our experiments is not the most optimized in terms of time per iteration. However, our goal is to show that JUMP-means can achieve comparable performance with a reasonable implementation of MCMC and EM."
    }, {
      "heading" : "D. Scaling experiments",
      "text" : "For the scaling experiments we generated 4 datasets consisting of 102 to 105 sequences. All datasets are sampled from a single hidden state MJP with 5 hidden states and 5 possible observations. For the 20 observations in each sequence a Gaussian likelihood is used. Finally, for the held out results, we categorized the observations in 5 bins, removed 30% of the data points and predicted their category."
    } ],
    "references" : [ {
      "title" : "Particle Markov chain Monte Carlo methods",
      "author" : [ "C. Andrieu", "A. Doucet", "R. Holenstein" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Andrieu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Andrieu et al\\.",
      "year" : 2010
    }, {
      "title" : "Statistical inference in evolutionary models of dna sequences via the em algorithm",
      "author" : [ "H. Asger", "J.J. Ledet" ],
      "venue" : "Statistical Applications in Genetics and Molecular Biology,",
      "citeRegEx" : "Asger and Ledet,? \\Q2005\\E",
      "shortCiteRegEx" : "Asger and Ledet",
      "year" : 2005
    }, {
      "title" : "Clustering with bregman divergences",
      "author" : [ "A. Banerjee", "S. Merugu", "I.S. Dhillon", "J. Ghosh" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Banerjee et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2005
    }, {
      "title" : "Statistical inference for discretely observed markov jump processes",
      "author" : [ "M. Bladt", "M. Sørensen" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Bladt and Sørensen,? \\Q2005\\E",
      "shortCiteRegEx" : "Bladt and Sørensen",
      "year" : 2005
    }, {
      "title" : "Efficient estimation of transition rates between credit ratings from observations at discrete time points",
      "author" : [ "M. Bladt", "M. Sørensen" ],
      "venue" : "Quantitative Finance,",
      "citeRegEx" : "Bladt and Sørensen,? \\Q2009\\E",
      "shortCiteRegEx" : "Bladt and Sørensen",
      "year" : 2009
    }, {
      "title" : "MAD-Bayes: MAP-based Asymptotic Derivations from Bayes",
      "author" : [ "T. Broderick", "B. Kulis", "M.I. Jordan" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Broderick et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Broderick et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient Continuous-Time Markov Chain Estimation",
      "author" : [ "M. Hajiaghayi", "B. Kirkpatrick", "L. Wang", "A. BouchardCôté" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Hajiaghayi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hajiaghayi et al\\.",
      "year" : 2014
    }, {
      "title" : "Small-variance asymptotics for exponential family dirichlet process mixture models",
      "author" : [ "K. Jiang", "B. Kulis", "M.I. Jordan" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Jiang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2012
    }, {
      "title" : "Revisiting k-means: New Algorithms via Bayesian Nonparametrics",
      "author" : [ "B. Kulis", "M.I. Jordan" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Kulis and Jordan,? \\Q2012\\E",
      "shortCiteRegEx" : "Kulis and Jordan",
      "year" : 2012
    }, {
      "title" : "Latent Continuous Time Markov Chains for Partially-Observed Multistate Disease Processes",
      "author" : [ "J. Lange" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Lange,? \\Q2014\\E",
      "shortCiteRegEx" : "Lange",
      "year" : 2014
    }, {
      "title" : "Estimating disease progression using panel",
      "author" : [ "M. Mandel" ],
      "venue" : "data. Biostatistics,",
      "citeRegEx" : "Mandel,? \\Q2010\\E",
      "shortCiteRegEx" : "Mandel",
      "year" : 2010
    }, {
      "title" : "A database to support development and evaluation of intelligent intensive care monitoring",
      "author" : [ "G. Moody", "R. Mark" ],
      "venue" : "In Computers in Cardiology,",
      "citeRegEx" : "Moody and Mark,? \\Q1996\\E",
      "shortCiteRegEx" : "Moody and Mark",
      "year" : 1996
    }, {
      "title" : "NIST Handbook of Mathematical Functions",
      "author" : [ "F.W.J. Olver", "D.W. Lozier", "R.F. Boisvert", "Clark", "C.W. (eds" ],
      "venue" : null,
      "citeRegEx" : "Olver et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Olver et al\\.",
      "year" : 2010
    }, {
      "title" : "Maximum likelihood trajectories for continuous-time markov chains",
      "author" : [ "T.J. Perkins" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Perkins,? \\Q2009\\E",
      "shortCiteRegEx" : "Perkins",
      "year" : 2009
    }, {
      "title" : "Fast MCMC sampling for Markov jump processes and extensions",
      "author" : [ "V. Rao", "Y.W. Teh" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Rao and Teh,? \\Q2013\\E",
      "shortCiteRegEx" : "Rao and Teh",
      "year" : 2013
    }, {
      "title" : "Small-Variance Asymptotics for Hidden Markov Models",
      "author" : [ "A. Roychowdhury", "K. Jiang", "B. Kulis" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Roychowdhury et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Roychowdhury et al\\.",
      "year" : 2013
    }, {
      "title" : "Priors over recurrent continuous time processes",
      "author" : [ "A. Saeedi", "A. Bouchard-Côté" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Saeedi and Bouchard.Côté,? \\Q2011\\E",
      "shortCiteRegEx" : "Saeedi and Bouchard.Côté",
      "year" : 2011
    }, {
      "title" : "Hierarchical Dirichlet Processes",
      "author" : [ "Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Teh et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "gression (Mandel, 2010) and RNA path folding (Hajiaghayi et al.",
      "startOffset" : 9,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "gression (Mandel, 2010) and RNA path folding (Hajiaghayi et al., 2014), or when the state is only observed indirectly, as in corporate bond rating (Bladt & Sørensen, 2009).",
      "startOffset" : 45,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "Furthermore, although MJPs are viewed as more realistic than their discrete-time counterparts in many fields (Rao & Teh, 2013), degenerate solutions for the maximum likelihood (ML) trajectories for both directly and indirectly observed cases (Perkins, 2009), and non-existence of the ML transition matrix (obtained from EM) for some indirectly observed cases (Bladt & Sørensen, 2009) present inferential challenges.",
      "startOffset" : 242,
      "endOffset" : 257
    }, {
      "referenceID" : 7,
      "context" : "SVA has been applied to (hierarchical) Dirichlet process mixture models (Kulis & Jordan, 2012; Jiang et al., 2012), Bayesian nonparametric latent feature models (Broderick et al.",
      "startOffset" : 72,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : ", 2012), Bayesian nonparametric latent feature models (Broderick et al., 2013), hidden Markov models (HMMs), and infinite-state HMMs (Roychowdhury et al.",
      "startOffset" : 54,
      "endOffset" : 78
    }, {
      "referenceID" : 15,
      "context" : ", 2013), hidden Markov models (HMMs), and infinite-state HMMs (Roychowdhury et al., 2013).",
      "startOffset" : 62,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "An expectation-maximization (EM) algorithm can be derived, but it cannot be applied to models with countably infinite states, so it is not suitable for iMJPs (Lange, 2014) (iMJPs are detailed in Section 4).",
      "startOffset" : 158,
      "endOffset" : 171
    }, {
      "referenceID" : 13,
      "context" : "Maximum likelihood inference amounts to finding maxU ln p(U |π, P,λ), which can be carried out efficiently using dynamic programming (Perkins, 2009).",
      "startOffset" : 133,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "(2014) which is based on particle MCMC (PMCMC) methods (Andrieu et al., 2010).",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "Recently, a more efficient Monte Carlo method was proposed in Hajiaghayi et al. (2014) which is based on particle MCMC (PMCMC) methods (Andrieu et al.",
      "startOffset" : 62,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : "In the SVA approach (Broderick et al., 2013), the MAP optimization is considered in the limit as the likelihood variance parameter is taken to zero: σ → 0.",
      "startOffset" : 20,
      "endOffset" : 44
    }, {
      "referenceID" : 7,
      "context" : "Following (Jiang et al., 2012), we scale the distributions by an inverse variance parameter β and then maximize the scaled likelihood and prior in the limit β →∞ (i.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 15,
      "context" : "We therefore resort to an alternating minimization procedure to optimize the JUMP-means objective function, similar in spirit to the one used in Roychowdhury et al. (2013). In each iteration of the optimization process, we first use a modified Viterbi algorithm to obtain the most likely state sequence.",
      "startOffset" : 145,
      "endOffset" : 172
    }, {
      "referenceID" : 7,
      "context" : ", Jiang et al. (2012); Roychowdhury et al.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : ", Jiang et al. (2012); Roychowdhury et al. (2013)).",
      "startOffset" : 2,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : ", Mandel (2010)).",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "We compare with particle MCMC (PMCMC) (Andrieu et al., 2010) and EM.",
      "startOffset" : 38,
      "endOffset" : 60
    } ],
    "year" : 2015,
    "abstractText" : "Markov jump processes (MJPs) are used to model a wide range of phenomena from disease progression to RNA path folding. However, maximum likelihood estimation of parametric models leads to degenerate trajectories and inferential performance is poor in nonparametric models. We take a small-variance asymptotics (SVA) approach to overcome these limitations. We derive the small-variance asymptotics for parametric and nonparametric MJPs for both directly observed and hidden state models. In the parametric case we obtain a novel objective function which leads to non-degenerate trajectories. To derive the nonparametric version we introduce the gamma-gamma process, a novel extension to the gamma-exponential process. We propose algorithms for each of these formulations, which we call JUMP-means. Our experiments demonstrate that JUMP-means is competitive with or outperforms widely used MJP inference approaches in terms of both speed and reconstruction accuracy.",
    "creator" : "LaTeX with hyperref package"
  }
}