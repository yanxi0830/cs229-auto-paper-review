{
  "name" : "1706.00687.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Weight Sharing is Crucial to Succesful Optimization",
    "authors" : [ "Shai Shalev-Shwartz", "Ohad Shamir" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 6.\n00 68\n7v 1\n[ cs\n.L G\n] 2\nJ un\nExploiting the great expressive power of Deep Neural Network architectures, relies on the ability to train them. While current theoretical work provides, mostly, results showing the hardness of this task, empirical evidence usually differs from this line, with success stories in abundance. A strong position among empirically successful architectures is captured by networks where extensive weight sharing is used, either by Convolutional or Recurrent layers. Additionally, characterizing specific aspects of different tasks, making them “harder” or “easier”, is an interesting direction explored both theoretically and empirically. We consider a family of ConvNet architectures, and prove that weight sharing can be crucial, from an optimization point of view. We explore different notions of the frequency, of the target function, proving necessity of the target function having some low frequency components. This necessity is not sufficient - only with weight sharing can it be exploited, thus theoretically separating architectures using it, from others which do not. Our theoretical results are aligned with empirical experiments in an even more general setting, suggesting viability of examination of the role played by interleaving those aspects in broader families of tasks."
    }, {
      "heading" : "1 Introduction",
      "text" : "There are many directions from which one can examine Deep Learning (DL). Very popular is the direction of empirical success, where extensive research effort had resulted in state-of-the-art, overwhelming breakthroughs, in a wide range of tasks. One may need to read between the lines to gain insights regarding the difficulties which faced the practitioners on their way to success. This is true, in particular, when regarding the optimization process. While sample complexity issues are usually straightforward to deal with (“add more data”), and expressive power of the used networks is generally more than sufficient, successful optimization, and in particular, success of Gradient Descent (GD), is left as a mystery. What aspects of a task cause the general gradient-based DL approach to succeed or fail?\nIn this paper, we study this question for a simple, yet powerful, ConvNet architecture: one convolutional layer, mapping k image patches, each of dimension d, into k scalars, followed by a non linear activation, a fully connected (FC) layer with ReLU activation, and a final FC layer with one output neuron. Most if not all DL practitioners would have known this “recipe” by heart. We think of k as relatively smaller than d: for example, d = 75 and k = 10, corresponding to a 5× 5× 3 convolution kernel over a small color image. This family of architectures, as trivial and simplistic as it is, can provide us with very fertile ground on which to examine interesting empirical phenomena. We assume that the target function which we are trying to learn is generated by a network of the exact same architecture, and learning is performed with a very large training set. Therefore, there are neither expressiveness nor overfitting issues, which enables us to focus solely on the success of GD.\nAny target function generated by the above architecture, can be thought of as a composition of two functions: the convolutional first layer (with its non linearity), denoted h∗ : Rdk → Rk, subsequently fed into\nthe second part of the network, denoted g∗ : Rk → R. We underscore two properties of DL tasks that control GD’s success or failure. The first property, uses notions of frequency, from Fourier analysis, to characterize “hardness” of a task. The second property, distinguishes between Convolutional layers, in which weights are shared, and Fully Connected (FC) ones.\nSince the target function is the composition g∗ ◦ h∗, it is natural to start by understanding the success of GD when one of the target function’s components is fixed and known, with only the other being learnt. For the case of known h∗, because k is small, it is possible to show that under some mild conditions, the problem of learning g∗ is not hard (see Appendix A). A more interesting case is when g∗ is known, and our task is to learn h∗. In [18], it has been shown that no Gradient Based algorithm can succeed in learning h∗ if g∗ is the parity of the signs of its input. The parity function consists of the highest frequency of the Fourier expansion for functions over the boolean cube. In this paper, we prove that h∗ can be learnt by GD, if the Fourier expansion of g∗ contains both a frequency 1 element and a higher frequency element, namely, a combination both high and low frequencies. We further prove, that this positive result depends on our architecture for learning h∗: if the convolutional layer is replaced by a FC one, then GD will fail. It is the combination of g∗ having a low frequency component, along with the weight sharing in our architecture, that is essential for success. Formal statements of these claims are given in Section 4.\nNaturally, mathematically analyzing the convergence properties of GD in this highly non convex problem, relies on some simplifying assumptions. A major one, is the assumption that g∗ is known. We start the paper, in Section 2, by empirically demonstrating that our theoretical results seem to hold even in the case of learning simultaneously both h∗ and g∗. In Section 4 we prove our main result, but before that, we highlight, in Section 3, the same phenomena, albeit in a simpler setting, where g∗(z) = ckz1 + cos( ∑k i=1 zi). Although somewhat synthetic, this setting does maintain the flavour of separation between low and high frequencies, this time from the perspective of Fourier analysis over Rk. Additionally, it allows for a relatively simple, direct proof technique, showing a computational separation between learning with or without weight sharing, where an exponential gap in time complexity of GD is proven to exist."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Recently, several works have attempted to study the optimization performance of gradient-based methods for neural networks. To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm. Other works, such as [13, 2, 10, 23], show how certain neural networks can be learned under (generally strong) assumptions, but not with standard gradient-based methods. More closer to our work, [1, 4, 7] provide positive learning results using gradient-based algorithms, but do not show the benefit of a convolutional architecture for optimization performance, compared to a fully-connected architecture. The hardness of learning in the case of Boolean functions, using the degree of the target function, was discussed in the statistical queries literature, for instance in [6]. In terms of techniques, our construction is inspired by target functions proposed in [18, 19], and based on ideas from the statistical queries literature (e.g. [3]), to study the difficulty of learning with gradient-based methods."
    }, {
      "heading" : "2 Empirical Demonstration",
      "text" : "The target function we wish to learn is of the form g∗(h∗(x)), where x = (x1, . . . ,xk), with xi ∈ Rd for every i. The function h∗ is parameterized by a vector u0 ∈ Rd and is defined as h∗(x) = (σ(u⊤0 ,x1), . . . , σ(u⊤0 xk)), where we chose σ to be the tanh function, as a smooth approximation of the sign function. We can therefore think of the input to g∗ as approximately being from {±1}k. In our experiment we vary four parameters:\n• The value of g∗ is set to be either g∗low(z) := z1, or g∗high(z) = ∏5 i=1 zi, or g ∗ both(z) = g ∗ low(z) + g ∗ high(z).\n• Weight Sharing (WS) vs. Fully Connected (FC): we also learn a compositional function g(h(x)), and the function h(x) can be either with weight sharing, h(x) = (σ(w⊤0 ,x1), . . . , σ(w ⊤ 0 xk)), where we learn\nthe vector w0 ∈ Rd, or with fully connected architecture, namely, h(x) = (σ(w⊤1 ,x1), . . . , σ(w⊤k xk)), where we learn the vector w = (w1, . . . ,wk).\n• Known vs. Unknown g∗: for the function g we either use g = g∗ or learn g as well by using the following architecture: FC layer with 50 outputs, ReLU, and FC layer with a single output.\n• Input Distribution: we either sample x from a Gaussian distribution, or use real image patches of size 10× 10 from the MNIST data set, normalized to have zero mean.\nWe train all of our networks with SGD, with η = 0.5, batch size 128, and the Squared Loss, for 3000 iterations. The vectors xi were generated by sampling from a normal distribution. The results of these experiments are depicted on the 6 graphs of Figure 1.\nThe graphs reveals several interesting observations. The first is the clear failure, of both WS and FC architectures, for both real and Gaussian data, and for both known and unknown g∗, when the target function is g∗high, and the contrasting success when it is g ∗ low. To explain this difference, let us characterize the g∗s using tools from Fourier analysis of real functions over the boolean cube. The representation of such functions in the Fourier basis can be used to define many different meaningful characterizations. Perhaps one of the most natural ones is the degree, or frequency of the function. Specifically, in our case, g∗low is a basis function of degree 1, while g∗high is a basis function of degree 5. Our theoretical analysis shows that the number of GD iterations required to learn h∗ when g∗ is a basis function grows as ddegree. In our experiment d = 75, or 100 for the MNIST patches, and we observe a clear separation already between degree 1 and degree 5.\nNext, since real-world functions will likely contain several frequencies, it is natural to study functions that combine many basis elements. As a first step, we turn to observe the performance for g∗both. Here, we suddenly see a strong separation between the WS and FC architectures: the optimization converges very quickly for the WS architecture while for the FC one, the high frequency component has not been learnt. Our theoretical analysis proves that, indeed, the number of GD iterations required by the FC architecture still grows as dhigh-degree, while for the WS architecture, the required number of iterations is only polynomial in the high degree.1 Intuitively, the low frequency term directs the single, shared, weight vector towards the optimum. Once h converged to h∗, even if g∗ is unknown, the GD process succeeds in learning it, because k is small. In contrast, without weight sharing, the components of w that appear only in the high degree term are not being learnt, as was the case for g∗high.\nFinally, while our analysis proves the positive and negative results for the case of known g∗ where x is normally distributed, the graphs show that even in the more general case, when g∗ is also being learnt, and even if the data is natural, the picture remains roughly the same.2"
    }, {
      "heading" : "3 Sum of Low and High Degree Waves",
      "text" : "In this section we provide our first separation result between the WS and FC architectures. Let x = (x1, . . . ,xk) ∈ Rdk,w = (w1, . . . ,wk) ∈ Rdk denote input elements, and weight vectors, respectively. Define:\npw(x) = ckw ⊤ 1 x1 + cos\n(\nk ∑\ni=1\nw⊤i xi\n)\n,\nwhere ck is any parameter ≥ 3 √ k. As in the introduction, we define a sub-family of functions, parameterized\n1We emphasize that this exponential gap between WS and FC is due to computational reasons and not due to overfitting. The difference in sample complexity between the two architectures is only a factor of k, and in both cases the training set size is sufficiently large.\n2The only difference across this aspect, when learning a degree 1 parity with a convolutional architecture, between known and unknown g∗, for Gaussian data, is perhaps due to smaller Signal to Noise Ratio in the case of learning g∗, as suggested in [18].\nby u0, and defined as:\npu0(x) = cku ⊤ 0 x1 + cos\n(\nk ∑\ni=1\nu⊤0 xi\n)\n.\nFor simplicity of notation, when using the 0 subscript for the weight vector, we refer to an element of the WS sub-family. Additionally, we use ū0 to denote the vector composed of k duplicates of u0, namely (u0,u0, . . . ,u0). Consider the objective\nF (w) = Ex\n[\n1 2 (pw(x) − pu0(x))2\n]\n,\nwhere x is standard Gaussian. We consider the gap between optimizing a FC architecture, namely, one parameterized by w, and a WS one, parameterized by a single weight vector w0. We note that our choice of ck is merely to simplify the proofs – convergence guarantees can be proven for other choices of ck (including ck = 1), but the proof requires more effort."
    }, {
      "heading" : "3.1 Hardness Result for Optimizing F using GD - FC Architecture",
      "text" : "Theorem 1 Assuming k > 1, the following holds for some numerical constants c1, c2, c3, c4: For any w such that ‖(w2, . . . ,wk)‖ ∈ [√ k−1 3 · ‖u0‖, √ k−1 2 · ‖u0‖ ] , it holds that\n∥ ∥ ∥ ∥\n∂\n∂(w2, . . . ,wk) F (w)\n∥ ∥ ∥ ∥ ≤ c1 √ k‖u0‖ exp ( −c2k‖u0‖2 ) .\nMoreover, for any w such that ‖(w2, . . . ,wk)‖ ≤ √ k−1 2 ‖u0‖, it holds that\nF (w)− F (ū0) ≥ 1− c3 exp(−c4k‖u0‖2). The proof is given in Appendix B.1. To understand the implication of the theorem, consider a gradientbased method, starting at some initial point w(0) such that ‖(w(0)2 , . . . ,w (0) k )‖ ≤ √ k−1 3 · ‖u0‖ (a reasonable assumption). In that case, the theorem implies that the algorithm will need to cross the ring {\n(w2, . . . ,wk) : ‖(w2, . . . ,wk)‖ ∈ [ √ k − 1 3 · ‖u0‖, √ k − 1 2 · ‖u0‖ ]}\nw.r.t. (w2, . . . ,wk), to get to a solution which ensures sub-constant error. However, in that ring, the gradients are essentially exponentially small in k‖u0‖2. This implies that performing gradient steps with any bounded step size, one would need exponentially many iterations (in k‖u0‖2) to achieve sub-constant error."
    }, {
      "heading" : "3.2 Positive Result for Optimizing F using GD - WS Architecture",
      "text" : "We show that when using a WS architecture, the objective is transformed to be strongly convex, making for simple proof techniques being applicable. The proof is given in Appendix B.2.\nTheorem 2 Using the WS architecture, F is strongly convex, minimized at u0, and satisfies maxw0 λmax(∇ 2F (w0))\nminw0 λmin(∇2F (w0)) ≤\n5, where λmax(M) and λmin(M) are the top and bottom eigenvalues of a positive definite matrix M . Hence, gradient descent starting from any point w(0), and with an appropriate step size, will reach a point w satisfying ‖w− u0‖ ≤ ǫ in at most 5 · log(‖winit − u0‖/ǫ) iterations."
    }, {
      "heading" : "4 Sum of Low and High Degree Parities",
      "text" : "This section formalizes our main result, namely, a separation between WS and FC architectures for learning a target function, g∗ ◦ h∗, when g∗ = g∗both is comprised of both high and low frequencies. The negative result for the FC architecture is given in Theorem 3 and the positive result for the WS architecture is given in Theorem 4."
    }, {
      "heading" : "4.1 Definitions, Notation",
      "text" : "Let x denote a k-tuple (x1, . . . ,xk) of input instances, and assume that each xl is i.i.d. standard Gaussian in R\nd. Let σ : R → [−1, 1] be some smooth approximation of the sign function. To simplify the analysis, we use the erf function: erf(x) = 1√\nπ\n∫ x −x e −t2dt. We believe that our analysis holds for additional functions, such\nas the popular tanh function. Define, for k ∈ N, a family of functions H(k)FC , parameterized by w ∈ (Rd)k, and defined by:\np(k) w (x) =\nk ∏\nl=1\nσ(w⊤l xl).\nLet us define a subclass, parameterized by w0 ∈ Rd and denoted H(k)WS , by:\np(k) w0 (x) =\nk ∏\nl=1\nσ(w⊤0 xl).\nNote that the difference between H(k)WS and H (k) FC is that elements in H (k) WS are satisfying the condition that for all l, wl = w0 for some w0 ∈ Rd. For ease of notation, we will refer to elements in H(k)WS and H (k) FC by p (k) w0 and p (k) w , respectively.\nAn extension of these families, denoted H(1,k)WS and H (1,k) FC , is defined, for w0 ∈ Rd and w ∈ (Rd)k respectively, as the sum of the two corresponding functions of the H(1),H(k) classes. Namely, for the FC class,\np(1,k) w (x) = σ(w⊤1 x1) + k ∏\nl=1\nσ(w⊤l xl),\nwith the definition for the WS class following as a special case.\nLet the objective F (k)(w), w.r.t. some target function p (k) u0 be the expected squared loss,\nF (k)(w) = Ex\n[\n1 2 (p(k) w (x)− p(k) u0 (x))2\n]\n,\nwith a similar definition for F (1,k)(w), namely\nF (1,k)(w) = Ex\n[\n1 2 (p(1,k) w (x)− p(1,k) u0 (x))2\n]\n. (1)\nThe following definition and lemma, due to [22], will be useful in our analysis.\nLemma 1 ([22]) Let σ be the erf function. Then, For every pair of vectors u,v ∈ Rd we have\nVσ(u,v) := Ex∼N(0,I) [ σ(w⊤x)σ(u⊤x) ] = 2 π sin−1\n(\n2u⊤v √\n1 + 2‖u‖2 √ 1 + 2‖v‖2\n)\n,\nwhere N(0, I) is the standard Gaussian distribution."
    }, {
      "heading" : "4.2 Non degeneracy depending on ‖u0‖2",
      "text" : "Note that as |σ| < 1, for large values of k, and small values of u⊤0 xl, we have that pu0(x) is vanishing exponentially. We show that in the case of large enough ‖u0‖2, depending on k, the target function’s expected norm is lower bounded, hence overcoming this possible degeneracy.\nLemma 2 If ‖u0‖2 ≥ 12π2 k2 then Ex [ (p(k) u0 (x))2 ] > 1\n4 .\nThe proof is given in Appendix C.1."
    }, {
      "heading" : "4.3 Exact Gradient Expressions",
      "text" : "In order to analyze the dynamics of the Gradient Descent (GD) optimization process, we examine the exact gradient expressions. The proofs to the lemmas are given at Appendix C.2\nRecall the definition of F (1,k) from (1). We first show that F (1,k) equals the sum of F (1) and F (k), due to independence of these two terms.\nLemma 3 For both architectures,\nF (1,k)(w) = F (1)(w) + F (k)(w).\nBased on Lemma 3, we have that ∇F (1,k)(w) = ∇F (1)(w)+∇F (k)(w), hence it suffices to find an explicit expression for ∇F (k)(w), for every k. We have,\n∇F (k)(w) = Ex [ p(k) w (x)g(k) w (x)− p(k) u0 (x)g(k) w (x) ] ,\nwhere g (k) w (x) := ∇wp(k)w (x) is the gradient of the predictor w.r.t. the weight vector w. We first show the following symmetric property.\nLemma 4 For all k,w,w′, and x,\np(k) w (x)g (k) w ′ (x) = p (k) w (−x)g(k) w ′ (−x).\nWe can now proceed to compute that exact gradients.\nLemma 5 Let g (k) l (x) be the gradient of the predictor w.r.t. wl, the weights corresponding to the lth input element. Then\nEx\n[\np(k) w (x)g (k) l (x)\n]\n=\n\n\n∏ j 6=l Vσ(wj ,wj)\n\n · c1(wl)wl,\nwhere 0 < c1(wl) < 1, is independent of k, and:\nEx\n[\np(k) u0 (x)g (k) l (x)\n]\n=\n\n\n∏ j 6=l Vσ(u0,wj)\n\n · b̃l\nfor some vector b̃l ∈ span{wl,u0}, independent of k.\nCorollary 1 For the WS architecture, we have that:\nEx\n[\np(k) w0 (x)g (k) 0 (x)\n]\n= k · Vσ(w0,w0)k−1 · c1(w0)w0,\nwhere 0 < c1(w0) < 1, is independent of k, and:\nEx\n[\np(k) u0 (x)g (k) 0 (x)\n]\n= k · Vσ(u0,w0)k−1 · b̃\nfor some vector b̃ ∈ span{w0,u0}, independent of k.\nThe corollary follows immediately from the fact that the gradient w.r.t. w0, is the sum of gradients of each “duplicate” of w0, when considering pw0 as a member of H(k)FC . Note that, when u⊤0 w0 > 0 (which happens w.p. 1/2 over symmetric initialization, and as we later show, this property is preserved during a run of GD), Vσ(u0,w0) > 0. Hence in such case, the coefficient of b̃ is positive.\n4.4 Hardness Result for Optimizing F (1,k) using GD - FC Architecture\nEquipped with the results of previous sections, we obtain a computational hardness result for learning a target function p (1,k) u0 using GD with the FC architecture, showing that the progress after any polynomial number of iterations, is exponentially small. Proofs are given in Appendix C.3.\nTheorem 3 Consider a GD algorithm for optimizing F (1,k) for the FC architecture, that uses a learning rate rule such that for every t, ηt ∈ (0, 1]. Suppose that every coordinate of wl is initialized i.i.d. uniformly from {±c} for some constant c. Then, with probability of at least 1 − 2ke−d1/3/6 over the random initialization, after T = o(dk/4) iterations, we will have F (1,k)(w(T )) ≥ 1/8. The main idea of the proof is to show that progress in direction which improves the angle between wj and u0 is exponentially small - unless wj gets close to the origin. In that case, it is “stuck” there with no ability to progress. The proof relies on the following lemmas. The first one shows that a “bad” initialization, namely, one for which the initialized vectors are almost orthogonal to u0, happens with overwhelming probability.\nLemma 6 Assume each wl is chosen by sampling uniformly from {±c}d for some c. Then w.p. > 1 − 2ke−0.5d 1/3\nover the initialization w(0), for all l ∈ [k], |〈 wl‖wl‖ , u0 ‖u0‖ 〉| < d −1/3.\nNext, we directly upper bound the value of |Verf(wj ,u0)|, s.t. for cases when wj ,u0 are almost orthogonal, or, when ‖wj‖ is very small, |Verf(wj ,u0)| is small too. Lemma 7 Let c ∈ [0, 1] and assume |〈 wj‖wj‖ , u0 ‖u0‖ 〉| < c, or ‖wj‖ ≤ c/ √ 2. Then\n|Verf(wj ,u0)| < 2 c\nπ < c\nFinally, we use the fact that the target function is non trivial, from Lemma 2, in order to show that in the case when not all of the weight vectors have converged, we suffer high loss.\nLemma 8 Assume that for some j, it holds that either |〈 wj‖wj‖ , u0 ‖u0‖ 〉| < sin π 32 , or ‖wj‖ ≤ 1√2 sin π 32 . Then\nF (1,k)(w) > 1\n8 .\n4.5 Positive Result for Optimizing F (1,k) using GD - WS Architecture\nWe now turn to state our positive result for the WS architecture. The outline of the proof is given in the subsections below, and additional proofs of intermediate results are given at Appendix C.4.\nTheorem 4 Running (projected) GD, with respect to the objective F (1,k) with the WS architecture, and with a constant learning rate for T = poly(k, 1/ǫ) iterations, yields ‖w(T )0 − u0‖ ≤ ǫ. To prove the theorem, we analyze the optimization process by separating it into two phases. During the first, the w0 converges to the direction of u0. Then, in a second phase, its norm converges to that of u0. The combination of the theorems proven in the next sections directly imply Theorem 4."
    }, {
      "heading" : "4.6 Phase 1 - Angle Convergence",
      "text" : "Assume that ‖u0‖2 = 12π2 k2, large enough for non degeneracy of the target function, as shown in Section 4.2. Moreover, we can assume w.l.o.g., that u0 = √ 12 π2 k e2, where e2 = (0, 1, 0, . . . , 0). We can further assume w.l.o.g. that span{u0,w0} = span{e1, e2}. By the random initialization, it holds that w⊤0 u0 > 0 with probability 1/2. We will assume that this is indeed the case. In addition, we will assume, w.l.o.g., that the first two coordinates of w0 are non-negative. Finally, assume that ‖w(t)0 ‖ ≤ ‖u0‖ for every t (if this is not the case, it is standard to add a projection onto this ball).\nTheorem 5 Let α(t) be the angle between u0,w (t) 0 . Then α (t+1) ≤ α(t). Moreover, for every ǫ > 0, there exist T = O((k/ǫ)3) s.t. α(T ) < ǫ."
    }, {
      "heading" : "4.7 Phase 2",
      "text" : "We use the same assumptions as in Section 4.6. We start off with a theorem showing that the gradient of F (1) directs the weights towards the optimum, u0. The proof uses monotonicity of σ, with similar techniques as found in [12, 11, 14].\nTheorem 6 For some L2(s̃) = Θ(1),\n〈w0 − u0,∇F (1)(w0)〉 ≥ L2(s̃)\nk ‖w0 − u0‖2.\nAfter establishing the above result, we next show that when the angle α between w0 and u0 is small (which we’ve shown is the case in polynomial time, after the first phase of optimization, in Section 4.6), the gradient of F (1,k) has the same property as the gradient of F (1), namely, it too points in a good direction. The intuition is that when w0 is close, in terms of angle, to u0, the gradient of F\n(k) becomes more similar to the gradient of F (1), making it helpful too.\nTheorem 7 Assume α(t) < min{tan−1 (\nǫ 2‖u0‖\n)\n, √\nǫ ‖u0‖} and ‖w0 − u0‖ > ǫ. Then\n〈w0 − u0,∇F (1,k)(w0)〉 ≥ L2(s̃)\nk ‖w0 − u0‖2,\nfor L2(s̃) = Θ(1), as in Theorem 6.\nWe now use the above lower bound over the inner product between the gradient and the optimal optimization step, to show that for any ǫ > 0, after a polynomial number of iterations of GD, we converge to a solution w0 for which ‖w0 − u0‖ < ǫ.\nTheorem 8 Assumew(t ′) is such that α(t ′) < min{tan−1 (\nǫ 2‖u0‖\n)\n, √\nǫ ‖u0‖}. Then, after at most poly(k, 1/ǫ)\nadditional iterations we must have that ‖w(t)0 − u0‖ ≤ ǫ .\nAcknowledgements: This research is supported in part by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI), and by the European Research Council (TheoryDL project)."
    }, {
      "heading" : "A Learning g∗ for Small k",
      "text" : "For simplicity of the argument, we consider the problem of learning a function g∗ : {±1}k → R. The argument can be easily extended to the case in which the domain of g∗ is [−1, 1]k under an additional Lipschitzness assumption, which is the case if the activation of h∗ is, for example, the tanh function.\nConsider an arbitrary distribution, D, over {±1}k. Using Lemma 19.2 in [17] we have that if m > 2k/ǫ then, in expectation over the choice of the sample, the probability mass of vectors in {±1}k that does not belong to my sample is at most ǫ. Therefore, finding a function g that agrees with all the points in the sample is sufficient to guarantee that g agrees with g∗ on all but an ǫ-fraction of the vectors in {±1}k.\nNext, consider the problem of fitting a sample (x1, g ∗(x1)), . . . , (xm, g∗(xm)) using a one-hidden-layer network. Several papers have shown (e.g. [13, 16, 5, 20]) that if the number of neurons in the hidden layer is at least m than the optimization surface is “nice” (in particular, no spurious local minima). Furthermore, by a simple random embedding argument, it can be shown that if we randomly pick the weight of the first layer and then freeze them, then with high probability, there are weights for the second layer for which the error is 0 on all the training examples. Learning only the second layer is a convex optimization problem.\nCombining all the above we obtain that there is a procedure that runs in time poly(2k, 1/ǫ) that learns g∗\nto accuracy ǫ. Note that since k is small (in our experiment, we used k = 5), the term 2k is very reasonable. This stands in contrast to the term dk, appearing in our lower bound for learning h∗. Taking d = 75, k = 5 (as in our experiment), the value of dk is huge."
    }, {
      "heading" : "B Proofs of Section 3",
      "text" : ""
    }, {
      "heading" : "B.1 Proofs of Section 3.1",
      "text" : ""
    }, {
      "heading" : "Proof of Theorem 1:",
      "text" : "Firstly, we note that by definition, F (w) equals\nc2k 2 E [ (w⊤1 x1 − u⊤0 x1) ] + 1 2 · E\n\n\n(\ncos\n(\nk ∑\ni=1\nw⊤i xi\n) − cos ( k ∑\ni=1\nu⊤0 xi\n))2 \n\n+ E\n[ (w⊤1 x1 − u⊤0 x1) ( cos ( k ∑\ni=1\nw⊤i xi\n) − cos ( k ∑\ni=1\nu⊤0 xi\n))]\n= c2k 2 (w1 − u0)⊤E[x1x⊤1 ](w1 − u0) + 1 2 · E\n\n\n(\ncos\n(\nk ∑\ni=1\nw⊤i xi\n) − cos ( k ∑\ni=1\nu⊤0 xi\n))2 \n+ 0,\nwhere we used the facts that (x1, . . . ,xk) are symmetrically distributed (hence take any value as well as its negative with equal probability) and that cosine is an even function. We get that\nF (w) = c2k 2 E [ ( w⊤1 x1 − u0x1 )2 ] + 1 2 Ez [ ( cos ( w⊤z ) − cos ( ū⊤0 z ))2 ] , (2)\nwhere z is a standard Gaussian vector in Rkd. We prove the following useful lemma:\nLemma 9 Given some b > 0, and assuming z has a standard Gaussian distribution in Rd, the gradient of 1 2 · Ez [ ( cos(b ·w⊤z) − cos(b · ū⊤0 z) )2 ] w.r.t. w equals\n− b 2 · (φ(2bw) + φ(b(w − u0))− φ(b(w + u0))) ,\nwhere\nφ(a) = exp\n( −‖a‖ 2\n2\n)\n· a.\nProof A straightforward calculation reveals that the gradient equals\n− b · E [( cos(b ·w⊤z)− cos(b · ū⊤0 z) ) · sin(b ·w⊤z)z ] = − b 2 ( E[sin(2b ·w⊤z)z] + E[sin(b(w − ū0)⊤z)z] − E[sin(b(w + u0)⊤z)z] ) .\n−b 2\n2\n( 2 exp ( −2b2‖w‖2 ) w + exp ( −b2‖w− u0‖2 ) (w − u0)− exp ( −b2‖w+ u0‖2 ) (w + u0) ) .\nWe now argue that for any vector a, E[sin(a⊤z)z] = φ(a), (3)\nfrom which the lemma follows. To see this, let za = a ⊤ z ‖a‖2 · a be the component of z in the direction of a, and let z⊥a = z− za be the orthogonal component. Then we have\nE[sin(a⊤z)z] = E[sin(a⊤za)za] + E[sin(a ⊤za)z⊥a].\nThe first term equals a‖a‖2 · E[sin(a⊤z)a⊤z], or equivalently a‖a‖ · Ey [sin(‖a‖y)y], where y has a standard Gaussian distribution. As to the second term, we have that za and z⊥a are statistically independent (since z has a standard Gaussian distribution), so the term equals E[sin(a⊤za)]E[z⊥a] = 0. Overall, we get that the expression above equals\na ‖a‖ · Ey[sin(‖a‖y)y] = a ‖a‖ · ∫ ∞ y=−∞ sin(‖a‖y)y · 1√ 2π exp ( −y 2 2 ) dy.\nUsing integration by parts and the fact that ∫ y cos(by) exp(−ay2) = √ π a exp(−b2/4a) (see [21, equation 15.73]), this equals\n= a\n‖a‖ √ 2π\n· ( − ∫ ∞\ny=−∞ sin(‖a‖y) exp\n(\n−y 2\n2\n) dy + ‖a‖ ∫ ∞\ny=−∞ cos(‖a‖y) exp\n(\n−y 2\n2\n)\ndy\n)\n= a√ 2π\n· ( 0 + √ 2π exp ( −‖a‖ 2\n2\n))\n,\nfrom which (3) follows.\nNow, let us consider the partial derivative of this function w.r.t. ŵ := (w2, . . . ,wk). Using Lemma 9, and letting û0 = (u0, . . . ,u0) to be concatenation of (k − 1) copies of u0, we get that this partial derivative equals\n−1 2\n(\nexp\n( −‖w‖ 2\n2\n)\nŵ + exp\n( −‖w− ū0‖ 2\n2\n) · (ŵ − û0)− exp ( −‖w+ ū0‖ 2\n2\n) · (ŵ − û0) ) ,\nwith norm at most\n1\n2\n(\nexp\n( −‖ŵ‖ 2\n2\n) ‖ŵ‖+ exp ( −‖ŵ− û0‖ 2\n2\n) · ‖ŵ− û0‖+ exp ( −‖ŵ+ û0‖ 2\n2\n) · ‖ŵ− û0‖ ) .\nNow, if ‖ŵ‖ ∈ [√\nk−1 3 ‖u0‖, √ k−1 2 ‖u0‖ ] = [ ‖û0‖ 3 , ‖û0‖ 2 ] , as implied by the assumption stated in the theorem,\nit is easily verified that ‖ŵ − û0‖2 as well as ‖ŵ − û0‖2 are at least ‖û0‖2/3, whereas ‖ŵ − û0‖, ‖ŵ+ û0‖ are at most 2‖û0‖. Moreover, ‖û0‖√k‖u0‖ = √ 1− 1k ∈ [ 1√ 2 , 1 ] .Thus, the displayed equation above can be upper\nbounded by c1 √ k‖u0‖ exp(−c2k‖u0‖2) for some numerical constants c1, c2.\nTo prove the second part of the theorem, we rely on the following lemma:\nLemma 10\nE\n[\n( cos(w⊤x) − cos(v⊤x) )2 ] ≥ 1− exp(−‖w− v‖2/2)− exp(−‖w+ v‖2/2).\nProof Expanding the square and using standard trigonometric identities, we have that the left hand side equals\nE [ cos2(w⊤x) ] + E [ cos2(v⊤x) ] − 2E [ cos(w⊤x) cos(v⊤x) ]\n= 1 + 1\n2 E[cos(2w⊤x)] +\n1 2 E[cos(2v⊤x)]− E [ cos((w − v)⊤x) ] − E [ cos((w + v)⊤x) ] .\nSince for any vector z, E[cos(z⊤x)] = Ey[cos(‖z‖y)] where y has a standard normal distribution on R, and this in turn equals 1√\n2π\n∫\ncos(‖z‖y) exp(−y2/2) = exp(−‖z‖2/2) (see [21, equation 15.73]), the above equals\n1 + 1\n2 exp\n( −2‖w‖2 ) + 1\n2 exp\n( −2‖v‖2 ) − exp ( −‖w− v‖ 2\n2\n) − exp ( −‖w+ v‖ 2\n2\n)\n,\nfrom which the result follows.\nUsing this lemma, and definition of F (w) in (2), we have\n2(F (w)− F (ū0)) ≥ 1− exp(−‖w− u0‖2/2)− exp(−‖w+ u0‖2/2) ≥ 1− exp(−‖ŵ− û0‖2/2)− exp(−‖ŵ+ û0‖2/2).\nMoreover, assuming that ‖ŵ‖ ≤ √ k−1 2 ‖u0‖ = ‖û0‖ 2 (as implied by the assumption stated in the theorem), we have that ‖ŵ−û0‖2 as well as ‖ŵ−û0‖2 are at least ‖û0‖2/4, which in turn equals (k−1)‖u0‖2/4 ≥ k‖u0‖2/8. Plugging to the above, the result follows."
    }, {
      "heading" : "B.2 Proofs of Section 3.2",
      "text" : "Proof of Theorem 2: The fact that u0 minimizes F (·) is immediate from the definition. Also, given that F (·) is strongly convex and satisfies the eigenvalue condition stated in the theorem, the convergence bound for gradient descent follows from standard results (see [15]). Thus, it remains to prove the strong convexity and eigenvalue bounds.\nTo get these bounds, we use the same calculations as in the beginning of the proof of Theorem 1, to rewrite F (w) as\nF (w) = c2k 2 ‖w− u0‖2 + 1 2 · Ez\n[\n( cos( √ k ·w⊤z)− cos( √ k · u⊤0 z) )2 ] , (4)\nwhere z has a standard Gaussian distribution in Rd, and using the fact E[xix ⊤ i ] = I is the identity matrix, and ∑k\ni=1 xi is distributed as a Gaussian with mean 0 and variance kI. The following lemma will be useful in computing the Hessian of F :\nLemma 11 Given some b > 0, and assuming z has a standard Gaussian distribution in Rd, the Hessian of 1 2 · Ez [ ( cos(b ·w⊤z) − cos(b ·w∗⊤z) )2 ] w.r.t. w has a spectral norm upper bounded by 2b2 + b.\nProof Differentiating the gradient as defined in Lemma 9, and noting that ∂∂wφ(ckw) = ck exp(−c2k‖w‖2/2)(I+ c2kww ⊤) for any ck, we get that the Hessian equals\nb2\n2\n(\n− 2 exp ( −2b2‖w‖2 ) (I − 2bww⊤)− exp ( −b 2‖w− u0‖2\n2\n)\n(I − b(w − u0)(w − u0)⊤)\n+ exp\n( −b 2‖w+ u0‖2\n2\n)\n(I − b(w + u0)(w + u0)⊤) )\n= b2\n2\n( −2 exp(−2b2‖w‖2)− exp ( −b 2‖w − u0‖2\n2\n)\n+ exp\n( −b 2‖w + u0‖2\n2\n))\nI\n+ b\n2\n( 4b2 exp(−2b2‖w‖2)ww⊤ + b2 exp ( −b 2‖w− u0‖2\n2\n)\n(w − u0)(w − u0)⊤\n− b2 exp ( −b 2‖w+ u0‖2\n2\n)\n(w + u0)(w + u0) ⊤ ) .\nTherefore, its spectral norm is at most\nb2\n2 · 3 + b 2 ( 2 · exp(−2b2‖w‖2) ( 2b2‖w‖2 ) + exp\n( −b 2‖w− u0‖2\n2\n)\n( b2‖w− u0‖2 )\n+ exp\n( −b 2‖w+ u0‖2\n2\n)\n( b2‖w+ u0‖2 )\n)\n.\nUsing the easily-verified fact that maxz≥0 exp(−z)z = exp(−1), we get that the above is at most\n3b2\n2 +\nb 2 exp(−1)(2 + 1 + 1) = 3b\n2\n2 + 2 exp(−1)b < 2b2 + b\nas required.\nWe are now in place to prove our theorem. Applying Lemma 11 and using the definition of the objective function F (w) at (4), we get that ∇2F (w) has eigenvalues in the range [c2k/2− (2k+ √ k), c2k/2+ (2k+ √ k)].\nSince ck = 3 √ k, we get that every eigenvalue of the Hessian is lower bounded by 9k2 −2k− √ k ≥ 9k2 −3k = 32k,\nand upper bounded by 9k2 +2k+ √ k ≤ 9k2 +3k = 152 k. Since k ≥ 1, this implies that the Hessian is positive definite everywhere (with minimal eigenvalue at least 3/2), hence F is strongly convex. Moreover,\nmaxw λmax(∇2F (w)) minw λmin(∇2F (w)) ≤ 15k/2 3k/2 = 5\nas required."
    }, {
      "heading" : "C Proofs of Section 4",
      "text" : ""
    }, {
      "heading" : "C.1 Proofs of Section 4.2",
      "text" : "Proof of Lemma 2: It suffices to show that Vσ(u0,u0) = Ex1σ(u ⊤ 0 x1) 2 > ( 1− 1k )\n, for in that case, by the independence of x1, . . . ,xk we have\nEx\n[\n(p(k) u0\n(x))2 ] = (Ex1 [σ(u ⊤ 0 x1) 2])k >\n(\n1− 1 k\n)k\n> 1\n4 .\nTo show that Vσ(u0,u0) > 1 − 1k , note that from Lemma 1 we have that Vσ(u0,u0) = 2π sin−1 ( 2‖u0‖2 1+2‖u0‖2 ) . Denote by f(a) the value for which 2π sin −1 ( f(a) 1+f(a) )\n= 1− a. By standard algebraic manipulations we have that\nf(a) = sin(π2 (1− a))\n1− sin(π2 (1− a)) .\nUsing Taylor’s theorem we have that there exists ξ ∈ [π2 (1 − a), π2 ] such that\nsin(π2 (1− a)) = 1− 1\n2\n(π 2 a )2 + 1 6 cos(ξ) (π 2 a )3 = 1− (π 2 a )2\n[\n1 2 − 1 6 cos(ξ) π 2 a\n]\n.\nIt follows that\nf(a) = 1\n(\nπ 2 a )2 [1 2 − 16 cos(ξ)π2a ]\n− 1 ≤ 1 (\nπ 2 a )2 [ 1 2 − 16 cos(ξ)π2 a ]\n≤ 3 (\nπ 2 a\n)2 ,\nwhere in the last inequality we assume that a ∈ [0, 1/2]. Taking a = 1/k and noting that Vσ(u0,u0) monotonically increases with ‖u0‖ we conclude our proof."
    }, {
      "heading" : "C.2 Proofs of Section 4.3",
      "text" : "Proof of Lemma 3: We start by expanding F (1,k)(w):\nF (1,k)(w)\n=Ex [ (pw(x)− pu0(x))2 ]\n=Ex\n[\n(p(1) w (x) − p(1) u0 (x) + p(k) w (x)− p(k) u0\n(x))2 ]\n=Ex\n[\n(p(1) w (x) − p(1) u0\n(x))2 ]\n+ Ex\n[\n(p(k) w (x)− p(k) u0\n(x))2 ]\n+ 2Ex\n[\n(p(1) w (x)− p(1) u0 (x))(p(k) w (x) − p(k) u0\n(x)) ]\n=F (1)(w) + F (k)(w) + 2Ex\n[\n(p(1) w (x) − p(1) u0 (x))(p(k) w (x)− p(k) u0\n(x)) ]\nWe next show that Ex\n[\n(p (1) w (x) − p(1)u0 (x))(p(k)w (x)− p(k)u0 (x))\n]\n= 0. Since σ is anti-symmetric and xk is\nnormal, we have that for every vector w′, Exkσ(w ′⊤xk) = 0. By the independence of the xi’s,\nEx\n[\np(1) w (x) · p(k) w\n(x) ]\n=Ex1p (1) w (x)Ex2...k−1\n[ k−1 ∏\ni=1\nσ(w⊤i xi)\n]\nExk\n[ σ(w⊤k xk) ] = 0\nThe same argument holds for the other terms, and the result follows.\nProof of Lemma 4 We start with the FC setting. Since σ is antisymmetric we clearly have that\np(k) w\n(x) = k ∏\nl=1\nσ(w⊤l xl) = (−1)k k ∏\nl=1\nσ(−w⊤l xl) = (−1)kp(k)w (−x) .\nNext, for every l, let g (k) w\n′ l be the derivative w.r.t. the weights corresponding to the l’th input instance, then\ng (k) w\n′ l (x) =\n\n\n∏ j 6=l σ(w′jxj)\n  σ′(w′lxl)xl = (−1)k   ∏\nj 6=l σ(−w′jxj)\n\nσ′(−w′lxl)(−xl) = (−1)kg(k)w′l (−x) ,\nwhere we used the fact that σ′ is symmetric. The claim follows because (−1)2k = 1. Finally, for the WS setting, we can think of w′ as being k copies of w′0 and then, by standard derivative rules, g (k) w\n′ 0 =\n∑ l g (k) w\n′ l ,\nfrom which the claim follows.\nProof of Lemma 5:. For the first term:\nal :=Ex\n[\np(k) w (x)g (k) l (x)\n]\n= 1\n2 E x: w⊤l xl>0\n[\np(k) w (x)g (k) l (x)\n] + 1\n2 E x: w⊤l xl<0\n[\np(k) w (x)g (k) l (x)\n]\n(0) =\n1 2 E x: w⊤l xl>0 [ p(k) w (x)g (k) l (x) ] + 1 2 E x: w⊤l xl<0 [ p(k) w (−x)g(k)l (−x) ]\n(1) =\n1 2 E x: w⊤l xl>0 [ p(k) w (x)g (k) l (x) ] + 1 2 E x: w⊤l xl>0 [ p(k) w (x)g (k) l (x) ]\n=E x: w⊤l xl>0\n[\np(k) w (x)g (k) l (x)\n]\n=E x: w⊤l xl>0\n\n( ∏\nj\nσ(w⊤j xj))( ∏ j 6=l σ(w⊤j xj))σ ′(w⊤l xl)xl\n\n\n(2) =Ex[k]\\{l}\n\n\n∏ j 6=l σ2(w⊤j xj)\n\n · E xl: w⊤l xl>0\n[ σ(w⊤l xl)σ ′(w⊤l xl)xl ]\n(3) =\n\n\n∏ j 6=l Vσ(wj ,wj)\n\n · E xl: w⊤l xl>0\n[ σ(w⊤l xl)σ ′(w⊤l xl)xl ]\n(4) =\n\n\n∏ j 6=l Vσ(wj ,wj)\n\n · c1(wl)wl .\nIn the above, (0) is from Lemma 4, (1) uses symmetry of the probability of x, and (2), (3) follow from the fact all xl are i.i.d.. To see why (4) is true, and why c1(wl) ≥ 0, note that we can assume w.l.o.g. that wl = (‖wl‖, 0, . . . , 0) (because xl is Gaussian), and in this case it is clear that the first coordinate of E xl: w⊤l xl>0 [ σ(w⊤l xl)σ ′(w⊤l xl)xl ]\nis positive while the rest of the coordinates are zero. For the second part of the lemma, similar arguments give:\nbl :=Ex\n[\np(k) u0 (x)g (k) l (x)\n]\n(0) = E\nx: u⊤0 xl>0\n[\np(k) u0 (x)g (k) l (x)\n]\n=E x: u⊤0 xl>0\n\n\n\n\n∏ j 6=l σ(u⊤0 xj)σ(w ⊤ j xj)\n\n σ(u⊤0 xl)σ ′(w⊤l xl)xl\n\n\n=\n\n\n∏ j 6=l Vσ(u0,wj)\n\n · E x: u⊤0 xl>0\n[ σ(u⊤0 xl)σ ′(w⊤l xl)xl ]\nwhere (0) is a similar transition to that done for al, using Lemma 4. Using again the normality of x, it is easy to see that b̃l := Ex: u⊤0 xl>0 [ σ(u⊤0 xl)σ ′(w⊤l xl)xl ] is in the span of {u,wl}."
    }, {
      "heading" : "C.3 Proofs of Section 4.4",
      "text" : "Proof of Lemma 6: The random variable, 1d w ⊤ l u0 is an average of d random variables, each of which distributed uniformly over {±cu0,i}, and its expected value is zero. Hence, by Hoeffding’s inequality,\nPr [ |w⊤l u0| ‖wl‖ ‖u0‖ > d−1/3 ] = Pr [ 1 d |w⊤l u0| > ‖wl‖ ‖u0‖ d−4/3 ] ≤ 2 exp ( −0.5 d1/3 ) .\nApplying a union bound over the k weight vectors, we conclude our proof.\nProof of Lemma 7: By the symmetry of Verf we can assume w.l.o.g. that u ⊤ 0 wj > 0, and then Verf(wj ,u0) > 0. We can rewrite\nVerf(wj ,u0) = 2 π sin−1\n(\n2u⊤0 wj √\n1 + 2‖u0‖2 √ 1 + 2‖wj‖2\n)\n= 2\nπ sin−1\n\n u⊤0 wj ‖u0‖ ‖wj‖\n· √ 2‖u0‖ √\n1 + ( √ 2‖u0‖)2\n· √ 2 ‖wj‖ √\n1 + ( √ 2‖wj‖)2\n\n\nThe function f(a) = a√ 1+a2 is monotonically increasing over a ∈ [0,∞), where f(a) = 0 and f(a) → 1 as a → ∞. Therefore, all the three terms in the argument of the inverse sign are in [0, 1]. If the first condition in the theorem holds then the first term makes the argument of the inverse sign at most c. If the second condition in the theorem holds then, since f(a) ≤ a, we have that the third term is at most c. The claim now follows immediately because for every c ∈ [0, 1] we have c ≤ sin(c).\nProof of Lemma 8: By Lemma 7, Verf(wj ,u0) < 1 16 . From Lemma 3, we have that F (1,k) ≥ F (k). Also, by Lemma 2, Ex [ (p (k) u0 (x)) 2 ] > 14 . Thus,\nF (1,k) ≥ F (k) = Ex [ (p(k) w (x)− p(k) u0 (x))2 ]\n≥ Ex [ (p(k) w (x))2 ] − 2Ex [ p(k) w (x)p(k) u0 (x) ] + 1\n4\n≥ − 2Ex [ p(k) w (x)p(k) u0 (x) ] + 1\n4\n= − 2 ∏\nl\nVerf(wl,u0) + 1\n4\n≥ − 2 1 16 ∏\nl 6=j |Verf(wl,u0)|+\n1 4 ≥ − 1 8 + 1 4 = 1 8\nProof of Theorem 3 To simplify the notation throughout this proof, whenever we write wl we mean for l ≥ 2. Recall that the gradient w.r.t. wl is equal to:\n∇wlF (1,k)(w(t)) =\n\n\n∏ j 6=l Vσ(wj ,wj)\n\n · c1(wl)wl −\n\n\n∏ j 6=l Vσ(u0,wj)\n\n · b̃l.\nLet T = ⌊ 1ηkd k 3−3⌋. We firstly prove, by induction, that for all t ∈ {0, 1, . . . , T }, at least one of the following holds for every l:\n1. |(w(t)l ) ⊤ u0|\n‖w(t)l ‖ ‖u0‖ ≤ d−1/3 + ηt · d− k−23 +2,\n2. at some t′ ≤ t, it held that ‖w(t ′) l ‖ < 1/d, and ‖w (t) l ‖ < 1/d+ η(t− t′) · d−(k−2)/3+1.\nThis holds w.h.p. for t = 0, by Lemma 6. For the inductive step, note that if the claim holds for some t ≤ T , then by the definition of T we have that ηtd− k−2 3 +2 ≤ d−1/3/k. Hence, by Lemma 7, the coefficient of b̃l is at most (\n1 + 1\nk\n)k−2 · d−(k−2)/3 ≤ e d−(k−2)/3 ≤ d−(k−2)/3+1 ,\nwhere we assume that d ≥ e. Moreover, it is easy to see that ‖b̃l‖ < 1. In addition, it is easy to see that the coefficient of w\n(t) l in the first term of the gradient is positive. Therefore, if the second assumption\nheld for l at time t, then by observing the explicit expression of the gradient, we obtain that the only term which can increase ‖w(t)l ‖ is smaller in magnitude than η d−(k−2)/3+1, and hence, the second assumption holds for t + 1. If on the other hand, the first assumption held for l at time t. If at t + 1, its norm decreases below 1/d, we are done. Otherwise, note that the only term of the gradient which can change the direction of w (t) l is the second one, which is again, smaller in magnitude than η d −(k−2)/3+1. Now, since ‖w(t+1)l ‖ > 1/d, we obtain that the change in angle, dα, between wl,u0, in times t, t + 1, satisfies dα < tan −1 ( η d−(k−2)/3+1\n1/d\n)\n= tan−1(ηd−(k−2)/3+2). Therefore, denoting by α(t) the angle at time t, we have,\n| |(w (t+1) l ) ⊤u0| ‖w(t+1)l ‖ ‖u0‖ − |(w (t) l ) ⊤u0| ‖w(t)l ‖ ‖u0‖ | = | cosα(t+1) − cosα(t)|\n(0) ≤ |α(t+1) − α(t)| ≤ tan−1(ηd−(k−2)/3+2) (1) ≤ ηd−(k−2)/3+2\nwhere (0) and (1) are by the 1-Lipschitzness of cos and tan−1. The inductive step follows immediately. From this proof, combined with Lemma 8, we obtain that for all T = O(dk/4), F (1,k)(w(T )) > 1/8, as required."
    }, {
      "heading" : "C.4 Proofs of Section 4.5",
      "text" : "Proof of Theorem 5: Firstly, we use the lemmas from Section 4.3 to write the gradient as:\n∇F (1,k)(w0) =∇F (1)(w0) +∇F (k)(w0) =c1(w0)w0 − b̃1 + kVσ(w0,w0)k−1 · c1(w0)w0 − kVσ(u0,w0)k−1 · b̃1 =(1 + kVσ(w0,w0) k−1)c1(w0)w0 − (1 + kVσ(u0,w0)k−1)b̃1\nObserve that a gradient update is the subtraction of the two terms (scaled by η) from w0. Hence, the first term does not change the angle α. We further note that this update adds b̃1 to w0 with a positive coefficient. It is therefore sufficient to show that the second term has positive inner product withw⊥ := (− cos(α), sin(α)), see Figure 2.\nRecall that b̃1 = Ex1: u⊤0 x1>0 [σ(u0x1)σ ′(w0x1)x1]. We note that for every x = (θ, r) (in polar coordinates) s.t. θ ∈ [0, π/2− α], we can look at its reflection over the w0 axis, namely, x̃ = (π − 2α − θ, r), and note that:\n• σ′(w0x) = σ′(w0x̃),\n• 〈x,w⊥〉 = −〈x̃,w⊥〉,\n• σ(u0x) < σ(u0x̃).\nLet A1 be the event that θ ∈ [0, π − 2α]. By the above properties, Ex∈A1 [〈σ(u0x)σ′(w0x)x,w⊥〉] =Ex∈A1 : w⊤⊥x<0 [(σ(u0x)− σ(u0x̃))σ ′(w0x)〈x,w⊥〉] > 0, since 〈x,w⊥〉 < 0, (σ(u0x) − σ(u0x̃)) < 0, and σ′(w0x) ≥ 0. Thus, 〈b̃1,w⊥〉 ≥ Pr(Ac1)Ex∈Ac1 [〈σ(u0x1)σ\n′(w0x1)x1,w⊥〉] Moreover, note that for all x ∈ Ac1, the expression in the expectation is non negative. Hence we obtain that 〈b̃1,w⊥〉 ≥ 0 for all b̃1, and indeed, α(t+1) ≤ α(t). We shall now show a positive lower bound. Let A2 be the event that θ ∈ [π − 32α, π − 12α], and 1‖u0‖ ≤ r/ √ 2 < 2‖u0‖ . In the angular aspect, these are, in words, elements with an angle of α/2 around w⊥, see Figure 2 for an illustration. Firstly note that A2 ⊂ Ac1, so from positivity of the inner expression in the expectation:\n〈b̃1,w⊥〉 ≥ Pr(A2)Ex∈A2 [〈σ(u0x1)σ′(w0x1)x1,w⊥〉] Firstly, let us lower bound Pr(A2). In the angular aspect, it is clear that Pr ( θ ∈ [π − 32α, π − 12α] ) = απ .\nFor each such θ, the r/ √ 2 value of x is distributed |N (0, 1)|. It is clear from monotonicity of the Gaussian pdf for positive r, that: Pr (\n1 ‖u0‖ < r/ √ 2 < 2‖u0‖ ) ≥ 1‖u0‖ · 2√ 2π e − 22 2‖u0‖ 2 > 15‖u0‖ . Therefore, we continue:\n〈b̃1,w⊥〉 ≥ α\n5π‖u0‖ min x∈A2 σ(ux) min x∈A2 σ′(wx) min x∈A2 〈x,w⊥〉\nIt is easy to see where each of the minimas is obtained, and hence we have:\n〈b̃1,w⊥〉 ≥ α\n5‖u0‖π σ\n( ‖u0‖ sin(π − α2 )\n‖u0‖\n) σ′ ( ‖w0‖ 2(cosα sin α2 − sinα cos α2 )\n‖u0‖\n)\n(\ncosα cos α2 + sinα sin α 2\n)\n‖u0‖ (0) ≥ α 5‖u0‖2π σ ( ‖u0‖ sin(π − α2 ) ‖u0‖ ) σ′ ( ‖u0‖ −2 sin α2 ‖u0‖ ) ( cos α 2 )\n(1) =\nα 5‖u0‖2π σ ( sin α 2 ) σ′ ( 2 sin α 2 )( cos α 2 )\n≥ α 5‖u0‖2π σ ( sin α 2 ) σ′ ( 2 sin π 4 )( cos π 4 )\n:=f(α, ‖u0‖),\nwhere (0) is from ‖w0‖ ≤ ‖u0‖ (the projection step we might have added does not change the angle) and (1) from symmetry of σ′. Observe that the erf’s derivative at 0 is 1. Combined with the fact that the sine function behaves the same at the neighbourhood of 0, we obtain that σ (\nsin α2 ) = Θ(α2 ), when α → 0. We obtain that f(α, ‖u0‖) = Ω( α 2\n‖u0‖2 ). Now, we use this in order to examine the angular improvement. Since\n‖w0‖ ≤ ‖u0‖, we obtain:\nδ(t+1)α := α (t) − α(t+1) ≥ tan−1\n( f(α(t), ‖u0‖) ‖u0‖ ) .\nFor reasonably large k, the argument of the arctan is smaller than 1, and on the interval [0, 1] the value of arctan is larger than half its argument. This implies that δ (t+1) α ≥ Ω(α2/k3), or\nα(t+1) ≤ ( 1− Ω((α(t))2/k3) ) α(t) .\nThis proves that after O((k/ǫ)3) iterations the value of α must be smaller than ǫ.\nProof of Theorem 6: Let r = ‖u0‖. Fix w0 ∈ Bd(0, r). For some s > 0, define the event As = {x1 : max{|u⊤0 x1|, |w⊤0 x1|, |(u0 −w0)⊤x1|} < s}. Now,\n〈w0 − u0,∇F (1)(w0)〉 =Ex1 [( σ(w⊤0 x1)− σ(u⊤0 x1) ) σ′(w⊤0 x1) · 〈w0 − u0,x1〉 ]\nNote that σ′ > 0, hence σ is monotonically increasing, implying non negativity of the inner expression for all x1. For some s, let L(s) = σ\n′(s). Then, by the properties of σ, for all a ∈ [−s, s], σ′(a) ≥ L(s) > 0. We proceed:\n〈w0 − u0,∇F (1)(w0)〉 (1) ≥Ex1 [( σ(w⊤0 x1)− σ(u⊤0 x1) ) σ′(w⊤0 x1) · 〈w0 − u0,x1〉1As ]\n(2) ≥L(s) · Ex1 [( σ(w⊤0 x1)− σ(u⊤0 x1) ) · 〈w0 − u0,x1〉1As ]\n(3) ≥L2(s) · Ex1 [ 〈w0 − u0,x1〉21As ]\nwhere (1) is from non negativity of the inner expression, (2) from the lower bound over the derivative of σ, applicable from the occurence of As, (3) from the Mean Value Theorem, again applicable for the event As. x1 is standard Gaussian, so Ex1 [ 〈w0 − u0,x1〉2 ] = ‖w0 − u0‖22. We continue to develop the lower bound:\n〈w0 − u0,∇F (1)(w0)〉 ≥L2(s) · Ex1 [ 〈w0 − u0,x0〉21As ]\n=L2(s) · (\nEx1\n[ 〈w0 − u0,x1〉2 ] − Ex1 [ 〈w0 − u0,x1〉21Acs ])\n(1) =L2(s) · ( ‖w0 − u0‖2 − Ex1 [ 〈w0 − u0,x1〉21Acs ]) (2) ≥L2(s) · ( ‖w0 − u0‖2 − ( Ex1 [ 〈w0 − u0,x1〉4 ] Pr(Acs) )1/2 )\n(3) =L2(s) ·\n( ‖w0 − u0‖2 − ‖w0 − u0‖2 (3 Pr(Acs))1/2 )\n=L2(s) · ‖w0 − u0‖2 ( 1− (3 Pr(Acs))1/2 )\nwhere (1) is from the fact x1 is standard Gaussian, (2) is from Cauchy-Schwartz, (3) is from direct computation using the fourth moment of a Gaussian. Let s̃ be sufficiently large, such that (3 Pr(Acs̃)) 1/2 < 1− 1/k. This is possible for s = Θ(1), since, when assuming w.l.o.g. u0 ∈ R2, Pr(u⊤0 x < 1) ≥ 1‖u0‖ 1√ 2π e −1 2‖u0‖ 2 = Ω ( k−1 ) , and the same for w0,u0 −w0. Then,\n〈w0 − u0,∇Fu0(w0)〉 ≥ L2(s̃)\nk · ‖w0 − u0‖2"
    }, {
      "heading" : "C.4.1 Proof of Theorem 7",
      "text" : "Before proving Theorem 7, we state and prove two technical lemmas:\nLemma 12 Let α1 = tan −1 ǫ 2‖u0‖ , and assume α ∈ [0, α1). Then at least one of the following holds:\n• ‖u0 −w0‖ < ǫ,\n• sign (〈w0 − u0,w0〉) = −1.\nProof It is easy to verify that if the first condition doesn’t hold, then the worst case scenario is when α = α1 and w0 lies on the ǫ-sphere around u0. So, from now on we assume that α = α1 and w0 lies on the ǫ-sphere around u0. Consider the vector z, whose angle w.r.t. u0 is α, and such that the angle between z and u0 − z is exactly 90 degrees. Since z and w0 points to the same direction, It is easy to verify that it suffices to show that ‖z− u0‖ < ǫ. To see this, we observe that ‖z‖‖u0‖ = cos(α), hence\n‖z‖ = ‖u0‖ cos tan−1 ǫ\n2‖u0‖ = ‖u0‖\n1 √\n1 + (\nǫ 2‖u0‖\n)2\nBy the Pythagorean theorem,\n‖z− u0‖2 = ‖u0‖2 − ‖z‖2 = ‖u0‖2    1− 1\n1 + (\nǫ 2‖u0‖\n)2\n\n  ≤ ǫ\n2\n4 ,\nwhich concludes our proof.\nLemma 13 Assume 0 ≤ α < min{tan−1 (\nǫ 2‖u0‖\n)\n, √\nǫ ‖u0‖}. Then at least one of the following holds:\n• ‖u0 −w0‖ < ǫ, • ‖w0‖ √\n1+2‖u0‖2√ 1+2‖w0‖2‖u0‖ ≤ cosα.\nProof It is clear that if not ‖u0 −w0‖ < ǫ, then ‖w0‖ < ‖u0‖ − ǫ2 , by the fact α < tan−1 ( ǫ 2‖u0‖ ) . Define the function f(x) = x√ 1+2x2 . We compute its derivatives, and note inequalities for x > 1:\nf ′(x) = 1 + 2x\n2\n1+2x2√ 1 + 2x2 > 0, f ′′(x) =\n4x2 1+2x2 − ( 1 + 2x 2 1+2x2 )\n2x √ 1 + 2x2 < 0.\nObserve that f is monotonically increasing, and concave for x > 1, and in particular, for x = ‖u0‖ > 1. Therefore, f(‖w0‖) < f(‖u0‖ − ǫ/2) < f(‖u0‖)− ǫ·f ′(‖u0‖) 2 . We obtain:\n‖w0‖ √ 1 + 2‖u0‖2 √\n1 + 2‖w0‖2‖u0‖ = f(‖w0‖) f(‖u0‖) ≤ f(‖u0‖)− ǫ·f ′(‖u0‖) 2 f(‖u0‖) = 1− ǫ 2 f ′(‖u0‖) f(‖u0‖)\n= 1− ǫ 2\n(\n1\n‖u0‖ + 2‖u0‖ 1 + 2‖u0‖2 ) ≤ 1− ǫ 2‖u0‖\nFrom observing the Taylor series of cos, we have that cosα ≥ 1 − α22 . Thus, from the assumption 0 ≤ α < √\nǫ ‖u0‖ ,\ncosα ≥ cos √ ǫ ‖u0‖ ≥ 1− ǫ 2‖u0‖ ≥ ‖w0‖\n√\n1 + 2‖u0‖2 √\n1 + 2‖w0‖2‖u0‖ .\nProof of Theorem 7. ∇F (1,k) can be written as:\n∇F (1,k)(w0) = ∇F (1)(w0) +∇F (k)(w0) = c1(w0)w0 − b̃1 + kVσ(w0,w0)k−1 · c1(w0)w0 − kVσ(u0,w0)k−1 · b̃1 = (1 + kVσ(w0,w0)\nk−1)c1(w0)w0 − (1 + kVσ(u0,w0)k−1)b̃1 = ( (1 + kVσ(w0,w0) k−1)− (1 + kVσ(u0,w0)k−1) ) c1(w0)w0 + (1 + kVσ(u0,w0) k−1)∇F (1) = kc1(w0) ( Vσ(w0,w0) k−1 − Vσ(u0,w0)k−1 ) w0 + (1 + kVσ(u0,w0) k−1)∇F (1).\nWe already have, by Theorem 6, that 〈w0 − u0,∇F (1)〉 ≥ L 2(s̃) k ‖w0 − u0‖2, and therefore (as α < π/2, implying (1 + kVσ(u0,w0) k−1) > 1), a similar result applies for this term of the gradient. It is hence sufficient to show that the first term does not worsen the lower bound, namely,\nsign ( 〈w0 − u0, kc1(w0) ( Vσ(w0,w0) k−1 − Vσ(u0,w0)k−1 ) w0〉 ) ≥ 0\nWe observe a few equivalencies:\nsign ( 〈w0 − u0, kc1(w0) ( Vσ(w0,w0) k−1 − Vσ(u0,w0)k−1 ) w0〉 )\n(0) = sign ( 〈w0 − u0, ( Vσ(w0,w0) k−1 − Vσ(u0,w0)k−1 ) w0〉 )\n= sign ( Vσ(w0,w0) k−1 − Vσ(u0,w0)k−1 ) · sign (〈w0 − u0,w0〉) (1) = − sign ( Vσ(w0,w0) k−1 − Vσ(u0,w0)k−1 )\n(2) = − sign\n(\n2w⊤0 w0 √\n1 + 2‖w0‖2 √ 1 + 2‖w0‖2 − 2w\n⊤ 0 u0\n√ 1 + 2‖w0‖2 √ 1 + 2‖u0‖2\n)\n= − sign ( w⊤0 w0 √\n1 + 2‖w0‖2 − w\n⊤ 0 u0\n√\n1 + 2‖u0‖2\n)\nwhere (0) is from the fact kc1(w0) ≥ 0, (1) is from Lemma 12, and (2) is from positivity of w⊤0 u0, and monotonicity of the sin−1 in the explicit expression for Verf , for positive w⊤0 u0. It is left to show w\n⊤ 0 w0 √ 1+2‖u0‖2√\n1+2‖w0‖2 ≤ w⊤0 u0 = ‖w0‖‖u0‖ cosα, equivalent, after rearrangement, to\n‖w0‖ √\n1+2‖u0‖2√ 1+2‖w0‖2‖u0‖ ≤ cosα, which we have from Lemma 13."
    }, {
      "heading" : "C.4.2 Proof of Theorem 8",
      "text" : "Proof of Theorem 8. Combining Theorem 7 with the Cauchy-Schwartz inequality we obtain that,\n‖∇F (1,k)(w0)‖ ≥ L2(s̃)\nk · ‖w0 − u0‖.\nOn the other hand, it is easy to see that ‖∇F (1,k)(w0)‖ ≤ O(k2). Let w(t)0 be the weight vector after the tth iteration of GD, η the learning rate. Assume that for all t′ < t, we did not converge yet, namely, ‖w(t ′)\n0 − u0‖ > ǫ. Then,\n‖w(t+1)0 − u0‖2 − ‖w (t) 0 − u0‖2 =‖w (t) 0 − η∇F (w (t) 0 )− u0‖2 − ‖w (t) 0 − u0‖2\n=− 2η〈∇F (w(t)0 ),w (t) 0 − u0〉+ η2‖∇F (w (t) 0 )‖2\nWe can now use the previous bounds from Theorem 7 to obtain, after rearranging:\n‖w(t+1)0 − u0‖2 =‖w (t) 0 − u0‖2 − 2η〈∇F (w (t) 0 ),w (t) 0 − u0〉+ η2‖∇F (w (t) 0 )‖2\n≤‖w(t)0 − u0‖2 − 2η L2(s̃)\nk · ‖w0 − u0‖2 + η2 k4\nTaking η = L 2(s̃) ǫ2 k5 we obtain that as long as ‖w (t) 0 − u0‖ > ǫ then\n‖w(t+1)0 − u0‖2 ≤ ‖w (t) 0 − u0‖2 −\nL4(s̃) ǫ4\nk6 ,\nwhich implies that after at most poly(1/ǫ, k) iterations we must have ‖w(t)0 − u0‖ ≤ ǫ."
    } ],
    "references" : [ {
      "title" : "Learning polynomials with neural networks",
      "author" : [ "Alexandr Andoni", "Rina Panigrahy", "Gregory Valiant", "Li Zhang" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Provable bounds for learning some deep representations",
      "author" : [ "Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma" ],
      "venue" : "In ICML,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Weakly learning dnf and characterizing statistical query learning using fourier analysis",
      "author" : [ "Avrim Blum", "Merrick Furst", "Jeffrey Jackson", "Michael Kearns", "Yishay Mansour", "Steven Rudich" ],
      "venue" : "In Proceedings of the twenty-sixth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1994
    }, {
      "title" : "Globally optimal gradient descent for a convnet with gaussian inputs",
      "author" : [ "Alon Brutzkus", "Amir Globerson" ],
      "venue" : "arXiv preprint arXiv:1702.07966,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2017
    }, {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "Gérard Ben Arous", "Yann LeCun" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Approximate resilience, monotonicity, and the complexity of agnostic learning",
      "author" : [ "Dana Dachman-Soled", "Vitaly Feldman", "Li-Yang Tan", "Andrew Wan", "Karl Wimmer" ],
      "venue" : "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Sgd learns the conjugate kernel class of the network",
      "author" : [ "Amit Daniely" ],
      "venue" : "arXiv preprint arXiv:1702.08503,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2017
    }, {
      "title" : "Global optimality in tensor factorization, deep learning, and beyond",
      "author" : [ "Benjamin D Haeffele", "René Vidal" ],
      "venue" : "arXiv preprint arXiv:1506.07540,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Identity matters in deep learning",
      "author" : [ "Moritz Hardt", "Tengyu Ma" ],
      "venue" : "arXiv preprint arXiv:1611.04231,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods",
      "author" : [ "Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar" ],
      "venue" : "arXiv preprint arXiv:1506.08473,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Efficient learning of generalized linear and single index models with isotonic regression",
      "author" : [ "Sham M Kakade", "Varun Kanade", "Ohad Shamir", "Adam Kalai" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "The isotron algorithm: High-dimensional isotonic regression",
      "author" : [ "Adam Tauman Kalai", "Ravi Sastry" ],
      "venue" : "In COLT,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "On the computational efficiency of training neural networks",
      "author" : [ "Roi Livni", "Shai Shalev-Shwartz", "Ohad Shamir" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "The landscape of empirical risk for non-convex losses",
      "author" : [ "Song Mei", "Yu Bai", "Andrea Montanari" ],
      "venue" : "arXiv preprint arXiv:1607.06534,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Introductory lectures on convex optimization: A basic course, volume 87",
      "author" : [ "Yurii Nesterov" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "On the quality of the initial basin in overspecified neural networks",
      "author" : [ "Itay Safran", "Ohad Shamir" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Understanding machine learning: From theory to algorithms",
      "author" : [ "Shai Shalev-Shwartz", "Shai Ben-David" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Failures of deep learning",
      "author" : [ "Shai Shalev-Shwartz", "Ohad Shamir", "Shaked Shammah" ],
      "venue" : "arXiv preprint arXiv:1703.07950,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2017
    }, {
      "title" : "Distribution-specific hardness of learning neural networks",
      "author" : [ "Ohad Shamir" ],
      "venue" : "arXiv preprint arXiv:1609.01037,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "No bad local minima: Data independent training error guarantees for multilayer neural networks",
      "author" : [ "Daniel Soudry", "Yair Carmon" ],
      "venue" : "arXiv preprint arXiv:1605.08361,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Schaum’s handbook of formulas and tables",
      "author" : [ "Murray R Spiegel" ],
      "venue" : "MacGraw Hill, New York,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1968
    }, {
      "title" : "Computing with infinite networks. Advances in neural information processing systems, pages",
      "author" : [ "Christopher KI Williams" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "In [18], it has been shown that no Gradient Based algorithm can succeed in learning h∗ if g∗ is the parity of the signs of its input.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 15,
      "context" : "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 19,
      "context" : "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "Other works, such as [13, 2, 10, 23], show how certain neural networks can be learned under (generally strong) assumptions, but not with standard gradient-based methods.",
      "startOffset" : 21,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "Other works, such as [13, 2, 10, 23], show how certain neural networks can be learned under (generally strong) assumptions, but not with standard gradient-based methods.",
      "startOffset" : 21,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "Other works, such as [13, 2, 10, 23], show how certain neural networks can be learned under (generally strong) assumptions, but not with standard gradient-based methods.",
      "startOffset" : 21,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "More closer to our work, [1, 4, 7] provide positive learning results using gradient-based algorithms, but do not show the benefit of a convolutional architecture for optimization performance, compared to a fully-connected architecture.",
      "startOffset" : 25,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "More closer to our work, [1, 4, 7] provide positive learning results using gradient-based algorithms, but do not show the benefit of a convolutional architecture for optimization performance, compared to a fully-connected architecture.",
      "startOffset" : 25,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "More closer to our work, [1, 4, 7] provide positive learning results using gradient-based algorithms, but do not show the benefit of a convolutional architecture for optimization performance, compared to a fully-connected architecture.",
      "startOffset" : 25,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "The hardness of learning in the case of Boolean functions, using the degree of the target function, was discussed in the statistical queries literature, for instance in [6].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 17,
      "context" : "In terms of techniques, our construction is inspired by target functions proposed in [18, 19], and based on ideas from the statistical queries literature (e.",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 18,
      "context" : "In terms of techniques, our construction is inspired by target functions proposed in [18, 19], and based on ideas from the statistical queries literature (e.",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "[3]), to study the difficulty of learning with gradient-based methods.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 17,
      "context" : "The only difference across this aspect, when learning a degree 1 parity with a convolutional architecture, between known and unknown g, for Gaussian data, is perhaps due to smaller Signal to Noise Ratio in the case of learning g, as suggested in [18].",
      "startOffset" : 246,
      "endOffset" : 250
    }, {
      "referenceID" : 21,
      "context" : "(1) The following definition and lemma, due to [22], will be useful in our analysis.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 21,
      "context" : "Lemma 1 ([22]) Let σ be the erf function.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "Lemma 7 Let c ∈ [0, 1] and assume |〈 wj ‖wj‖ , u0 ‖u0‖ 〉| < c, or ‖wj‖ ≤ c/ √ 2.",
      "startOffset" : 16,
      "endOffset" : 22
    }, {
      "referenceID" : 11,
      "context" : "The proof uses monotonicity of σ, with similar techniques as found in [12, 11, 14].",
      "startOffset" : 70,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : "The proof uses monotonicity of σ, with similar techniques as found in [12, 11, 14].",
      "startOffset" : 70,
      "endOffset" : 82
    }, {
      "referenceID" : 13,
      "context" : "The proof uses monotonicity of σ, with similar techniques as found in [12, 11, 14].",
      "startOffset" : 70,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "References [1] Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Alon Brutzkus and Amir Globerson.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Anna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, and Yann LeCun.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Dana Dachman-Soled, Vitaly Feldman, Li-Yang Tan, Andrew Wan, and Karl Wimmer.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Amit Daniely.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Benjamin D Haeffele and René Vidal.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Moritz Hardt and Tengyu Ma.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Adam Tauman Kalai and Ravi Sastry.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] Song Mei, Yu Bai, and Andrea Montanari.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] Yurii Nesterov.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Itay Safran and Ohad Shamir.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] Shai Shalev-Shwartz and Shai Ben-David.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] Ohad Shamir.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] Daniel Soudry and Yair Carmon.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] Murray R Spiegel.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] Christopher KI Williams.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "2 in [17] we have that if m > 2/ǫ then, in expectation over the choice of the sample, the probability mass of vectors in {±1}k that does not belong to my sample is at most ǫ.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 12,
      "context" : "[13, 16, 5, 20]) that if the number of neurons in the hidden layer is at least m than the optimization surface is “nice” (in particular, no spurious local minima).",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 15,
      "context" : "[13, 16, 5, 20]) that if the number of neurons in the hidden layer is at least m than the optimization surface is “nice” (in particular, no spurious local minima).",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "[13, 16, 5, 20]) that if the number of neurons in the hidden layer is at least m than the optimization surface is “nice” (in particular, no spurious local minima).",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 19,
      "context" : "[13, 16, 5, 20]) that if the number of neurons in the hidden layer is at least m than the optimization surface is “nice” (in particular, no spurious local minima).",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 14,
      "context" : "Also, given that F (·) is strongly convex and satisfies the eigenvalue condition stated in the theorem, the convergence bound for gradient descent follows from standard results (see [15]).",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 0,
      "context" : "Therefore, all the three terms in the argument of the inverse sign are in [0, 1].",
      "startOffset" : 74,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "The claim now follows immediately because for every c ∈ [0, 1] we have c ≤ sin(c).",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "For reasonably large k, the argument of the arctan is smaller than 1, and on the interval [0, 1] the value of arctan is larger than half its argument.",
      "startOffset" : 90,
      "endOffset" : 96
    } ],
    "year" : 2017,
    "abstractText" : "Exploiting the great expressive power of Deep Neural Network architectures, relies on the ability to train them. While current theoretical work provides, mostly, results showing the hardness of this task, empirical evidence usually differs from this line, with success stories in abundance. A strong position among empirically successful architectures is captured by networks where extensive weight sharing is used, either by Convolutional or Recurrent layers. Additionally, characterizing specific aspects of different tasks, making them “harder” or “easier”, is an interesting direction explored both theoretically and empirically. We consider a family of ConvNet architectures, and prove that weight sharing can be crucial, from an optimization point of view. We explore different notions of the frequency, of the target function, proving necessity of the target function having some low frequency components. This necessity is not sufficient only with weight sharing can it be exploited, thus theoretically separating architectures using it, from others which do not. Our theoretical results are aligned with empirical experiments in an even more general setting, suggesting viability of examination of the role played by interleaving those aspects in broader families of tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}