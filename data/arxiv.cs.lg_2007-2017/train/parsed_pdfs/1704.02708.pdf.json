{
  "name" : "1704.02708.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Distribution-free Evolvability of Vector Spaces: All it takes is a Generating Set",
    "authors" : [ "Richard Nock", "Frank Nielsen" ],
    "emails" : [ "richard.nock@data61.csiro.au", "frank.nielsen@acm.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "keywords: Evolvability, phenotype/genotype, vector spaces, portfolio selection, Markowitz meanvariance model, Bregman divergence."
    }, {
      "heading" : "1 Introduction",
      "text" : "The classical stochastic models of evolution are population-based, asymptotic and involve possibly complex set of selection mechanisms (Cerf, 1998). On the computational side of Darwinian evolution, Valiant’s model has simplified the evolution steps to that of a single organism evolving with mutations, and put the constraint that near-optimal evolution has to be observable in a polynomial number of iterations (Valiant, 2009). Evolution makes local modifications to a function that acts as an organism. This function maps environmental conditions (or experiences) to outputs. There is an\nar X\niv :1\n70 4.\n02 70\n8v 1\n[ cs\n.L G\n] 1\noptimal such function, unknown, called the target, t. All functions of interest, including t, belong to a class of representations, G. G is said to be evolvable if, roughly speaking, these local modifications yield a close approximation to the target t in polynomial time and with high probability. Approximation is measured according to the functions’ output to experimental conditions.\nThere are two main difficulties in Valiant’s original model. The first is statistical: the Turingcomputable mechanism that performs local modifications, called a mutator, performs random guided mutations, and these mutations are not evaluated on a true discrepancy between the current function and the target, but on an estimated discrepancy, relying on a fixed but unknown distribution sampling the environmental conditions. The second is computational, as we require that the mutated function comes close enough to the optimum in time polynomial in the relevant parameters.\nThere has been quite a large amount of work in the evolvability model (Table 1 below), yet no result has shown so far that complex enough forms of organisms (models) can be evolved in settings that would compose evolvability with unrestricted target, loss and distribution, under the simplest forms of mutator — e.g. constraint-free versions of the “hill climber” of Valiant (2012). Such results would be important because Valiant’s model is an original contender for computational approaches to evolution (Chastain et al., 2014), but no result so far formalizes the idea that a stochastic evolution as simple in mechanisms as the one that governs life-at-large can indeed produce sufficiently fit and complex models in a reduced amount of time. To our knowledge, only the class of singletons over boolean strings has been formally shown to tick most relevant checkboxes (Feldman, 2009), but it is a model space far too simple to fit in.\nTo summarize our contribution, we show that any finite dimensional vector space is a candidate answer for the model class. Our proof is constructive, that is, we explicit all parameters of the evolvability scheme. Noticeably, an efficient mutator can be obtained from any generating set of the vector space. We provide additional characterisations of evolution, as well as toy experiments in unsupervised and supervised learning.\nTo be more specific, we consider complete normed vector spaces (“vector spaces” for short) and mutators that use simple mutations. Each mutation is a linear combination of functions. The mutator acts on any fixed set of these mutations that would just be able to generate the gene encoding space. We make no other assumption on the mutator: the mutations available are therefore the same for any organism, as well as the mutator’s tolerance in the evolvability model. Given these parameters, what we show in essence is:\n(1) any vector space is distribution-free, strictly monotonically evolvable from any of these mutators, using any twice differentiable Bregman divergence as a performance function;\n(2) when the target organism slowly drifts, or is “too complex” for the available mutations, then the same scheme still guarantees approximate convergence to the best evolvable organism. This includes agnostic evolvability as a special case.\nComparison with related works: we do not perform reinitializations of evolution like (Valiant, 2012), the mutator is not organism-dependent like in (Kanade et al., 2010), we have no distribution assumptions like in (Angelino and Kanade, 2014; Kanade et al., 2010) or a requirement to know this distribution like in (Feldman, 2008), and the same scheme can be made agnostic or\nAlgorithm 1 Simple-Evol-Gen(f0) Input: initial representation f0, T ; For t = 0, 1, ..., T − 1\nStep t.1: Sample S as per (10); Step t.2: Compute BENE(ft) and NEUT(ft) using N (ft) as per (7) Step t.3: if BENE(ft) 6= ∅ then sample uniformly ft+1 in BENE(ft) as per (3);\nelse if NEUT(ft) 6= ∅ then sample uniformly ft+1 in NEUT(ft) as per (4); else sample uniformly ft+1 in N (ft);\nReturn fT ;\nhandle drift more significant than some allowed in more restricted settings (Kanade et al., 2010). Also, we do not rely on the trick that model representations “hardcode” the optimisation steps, a beautiful trick but expensive space-wise (Feldman, 2008, 2009; Valiant, 2012). We insists on the no-distribution assumption: in some works, this distribution is constrained, smooth and nice (Angelino and Kanade, 2014), uniform (Michael, 2012; Valiant, 2009), spherically symmetric (Kanade et al., 2010), a product of Gaussians with polynomial variance (Kanade et al., 2010), or with support restricted to a ball (Valiant, 2012), or just known (Feldman, 2008).\nBecause our analysis does not rely on such sophisticated assumptions, the algorithm we end up studying is an extremely simple evolutionary scheme, sketched in Algorithm 1 (Simple-Evol-Gen). The analysis turns out to be quite involved but the simplicity of the algorithm make them potentially amenable to a variety of specific input-, domain- or problem-dependent tunings.\nOne specific part of our analysis is the choice to rely on Bregman divergences for performance functions. We make this choice not just because they generalize performance functions previously used (Kanade et al., 2010; Valiant, 2009). Above all, they have been conveniently axiomatized and are the loss functions appearing, in the clear or in disguise, in numerous geometric, supervised or unsupervised learning problems (Amari and Nagaoka, 2000; Banerjee et al., 2005a,b; Bartlett et al., 2006). One very appealing feature of Bregman divergences is the fact that they are the exhaustive class of distortion measures for several essential properties: (i) when the population minimizer is the population’s average (Abernethy and Frongillo, 2012; Banerjee et al., 2005a; Nock et al., 2016) — which grounds simple statistical estimators with powerful characteristics —, and (ii) when the space is embedded with a dually flat structure (Amari and Nagaoka, 2000, Section 3.4) — which makes it amenable to simple but powerful optimization methods like mirror descent.\nOur work also brings to the fore some new quantitative and qualitative results pertaining to evolution. First, evolution occurs in a model reminiscent of the mean-variance approach of Markowitz (Kitano, 2010; Markowitz, 1952): returns are computed with respect to the target while risk premiums rely solely on the mutation, quantifying the risk of mutations — hence, to be considered beneficial, a mutation shall exhibit sufficient return while incurring limited risk. Second, the proof shows the existence of superior beneficial mutations, displaying significant returns with respect to the premiums (compared to evolvability’s requirements). Third, evolution does not just approximately converge to the optimum: with high probability, it converges strictly monotonically and then may get trapped (in a specific sense) around the optimum for a potentially large number of iterations. Fourth, we characterize analytically an efficient frontier for evolution, previously documented in systems biology (Kitano, 2010); interestingly, when the current organism is “far” from\nthe target (in a specific sense), all superior beneficial mutations are close to the efficient frontier, and therefore display an approximately optimal risk-return tradeoff. We complete our results with preliminary toy experiments (one sketched in Figure 1) in supervised and unsupervised learning, displaying some promising directions for Evolvability to spin out provable stochastic gradient-free optimization algorithms.\nThe rest of this paper is organised as follows: Section §2 details the evolvability model, Section §3 states and gives a high-level proof of our main result and §4 discusses and concludes. For space considerations and to lighten the papers body, an Appendix with three sections (i) provides the complete proof of our main result (§ 6), (ii) states and proves additional results (§ 7), and finally present toy experiments with our simple mutator (§ 8)."
    }, {
      "heading" : "2 Evolvability model",
      "text" : "In Valiant’s Evolvability model, evolution repeatedly operates random mutations over an organism, guided by weak selection. We recall the main components of the model, giving in the relevant cases how we adapt them to our setting (Feldman, 2009; Valiant, 2009).\nTopology of representations — Organisms are represented by functions f : X → Rd of a set G, called the representation class, supposed to be polynomial-time Turing-evaluatable. X is the set of conditions or experiences. For any f ∈ G, a neighborhood function is defined, N (f) ⊆ G, that depends on an accuracy parameter > 0. The size of the neighborhood is required to be polynomial in 1/ , d and the dimension of X, dim(X).\nPerformances of representations — Performances are measured with respect to an unknown but fixed distribution D over X, relatively to an unknown target function t ∈ G. The expected performance of some f ∈ G with respect to t is:\nPerft,ϕ(f,D) . = −Ex∼D[Dϕ(f(x)‖t(x))] , (1)\nwhereDϕ is Bregman divergence with (twice differentiable) generatorϕ : Rd → R (Banerjee et al., 2005a,b; Boissonnat et al., 2010). By extension, the empirical performance realized by f on an i.i.d. sample S is defined as Perft,ϕ(f, S) . = −Ex∼S[Dϕ(f(x)‖t(x))]. Our expected performance (1) generalizes Valiant’s which computes ED[ft]. In Valiant’s setting, d = 1, the output of functions is {−1, 1} and Dϕ is the square loss DSQL, and so Perft,SQL(f,D) = 2(ED[f(x)t(x)] − 1). Notice that we do not necessarily know the complete link between mutations and expression. This is very natural, yet estimating performances in the real world is not just the problem of sampling in D, it is also the problem of observing and measuring the performances. One could name this model a “Petri dish” model of performance evaluation.\nSelection by mutations — A mutator\nMUT : G× N → G ∪ {⊥} (2)\nis a randomized polynomial-time Turing machine that depends upon an accuracy > 0 and a tolerance T > 0. Tolerance is required to be polynomial in , 1/d and 1/dim(X). The mutator returns a so-called “mutant” of some input f ∈ G based on a weak evaluation of the quality of the elements of N (f). More precisely, it takes as input a sample size m > 0, samples i.i.d. a set S of m conditions, and outputs some g ∈ BENE(f) at random if BENE(f) 6= ∅, or else g ∈ NEUT(f) at random if NEUT(f) 6= ∅, using a fixed distribution µ(g, f) with support BENE(f) or NEUT(f). Those two sets BENE(f) and NEUT(f) are defined respectively by:\nBENE(f) .= {g ∈ N (f) : Perft,ϕ(g, S) ≥ Perft,ϕ(f, S) + T} , (3) NEUT(f) .= {g ∈ N (f) : |Perft,ϕ(g, S)− Perft,ϕ(f, S)| < T} . (4)\nIf both sets BENE(f) and NEUT(f) are empty, the mutator outputs ⊥, meaning in that last case that evolution has failed.\nRepresentations — Our framework being non-boolean, we define the models that we evolve. Let [n] .= {0, 1, ..., n} and [n]∗ .= {1, ..., n} where n is a natural integer. First, we have a set of functions {g1, g2, ..., gdG} ⊂ G, each of which is of the form gj : X → Rd for j ∈ [dG] for some dG > 0, with the assumption that ‖gj(x)‖22 ∞,∀x ∈ X (where ” ∞” means finite). Each of them can be thought as encoding the quantitative production of particular proteins under any experimental condition. The set of functions that we evolve lies in the span of {g1, g2, ..., gdG}— which we also denote as G for simplicity —, i.e., consists of linear combinations of functions of G.\nHowever, because we want our model to be general, we do not evolve directly G. For this reason, we define a set of vectors B .= {b1, b2, ..., bdB}, with bi ∈ RdG , that will represent our set of mutations. Each (column) vector, bi . = [b1i · · · b dG i ] > maps to a function bi(x) = ∑ j b j i ·gj(x). Their span, span(B), defines a vector space: this is the vector space we want to evolve. Two interesting cases emerge, that we will cover in our analysis: span(B) ⊂ G, which corresponds to robust evolvability, and span(B) = G but dB dG, which corresponds to an overcomplete (“wasteful”) encoding of functions, or redundancy in the encoding (observed in the living). An organism f that our mutator builds has evolved from some initial f0 and can therefore be represented as f = f0 + ∑ i f\nibi, where [f 1 f 2 ... fdB ] ∈ NdB . Each of these coordinates i is a proxy for the number of times mutation bi was triggered. Finally, To avoid confusion with Rd, we let ‖.‖G denote the L2 norm computed with respect to {g1, g2, ..., gdG}, i.e. the norms of the coordinates in G.\nEvolvability horizon — In the same way as PAC-learnability allows to be polynomial in the size of the target concept, evolvability has to allow a time complexity that depends on some complexity measure with respect to the target organism, and not just the number of description variables, which would be dim(X) in our case — in short, we allow more time to evolve mammals than protozoa. Evolvability results on complex representations alleviate this distinction by putting constraints on representations (Kanade et al., 2010; Valiant, 2012). We integrate this notion in the form of what we call the Evolvability horizon.\nDefinition 1 The Evolvability horizon TD(f0) (TD for short) of some organism f0 with respect to target t is:\nTD . = ⌈ ‖t− f0‖G maxi ‖bi‖G ⌉ . (5)\nTD quantifies the necessary number of mutations to come up with an encoding of an organism “close” to that of t. Any evolution in time o(TD) iterations, using only B, would be bound to fail in the worst case.\nEvolvability — We now provide our definition of evolvability, following (Feldman, 2009; Kanade et al., 2010; Valiant, 2009).\nDefinition 2 Assume the following fixed, for any accuracy > 0: representation class G, mutator neighborhood N and distribution µ, distribution D, generator ϕ, tolerance T. Then G is distribution-free evolvable by mutator MUT(., .) iff for any initial representation f0 and target representation t such that TD ∞, there exist polynomial functions m and T (both polynomial in\nd, dim(X), 1/ , TD) such that ∀0 < ≤ 1, then with probability≥ 1− , the sequence f0, f1, ..., fT with fj . = MUT(fj−1,m), ∀j ∈ [T ]∗, satisfies:\nPerft,ϕ(fT ,D) ≥ − . (6) Our model of evolution holds without initialisation (Feldman, 2008; Valiant, 2009). We have simplified the presentation of the model, in particular removing the notion of “evolution algorithm” and specifying evolution directly from the mutator. This does not weaken the results."
    }, {
      "heading" : "3 Main result: vector spaces are evolvable",
      "text" : "The main notations are summarized in Appendix, Subsection 6.1.\nDefinition 3 A mutator is permissible iff the neighborhood used by the mutator is defined as:\nN (f) . = {f} ⊕ {σαbi : σ ∈ {−1,+1}, bi ∈ B} , (7) for some set B .= {b1, b2, ..., bdB}, where⊕ is Minkowski sum. α > 0 (fixed) is called the magnitude of the mutations and σ is called the polarity of the mutation.\nWe have not detailed the distribution of the mutator, µ (Section 2). In fact, it can be any distribution with full support and (at least) inversely polynomial density, following e.g., (Kanade et al., 2010). We shall consider that it is the simplest of all, the uniform distribution. We also remark that B is not necessarily a basis, nor normal, nor orthogonal. Also, α is the key parameter to be tuned to comply with evolvability. We evolve vector spaces under three assumptions that we unite in a Singularity-Free (SF) setting.\nDefinition 4 The (SF) setting is defined by the following three assumptions: (i) any genome “can be coded”: span(B) = G,\n(ii) any target organism t is “unique”: arg minf Perft,ϕ(f,D) = {t} (∀t ∈ G), and (iii) any non-void genome gets “expressed”: Px∼D[g(x) 6= 0Rd ] > 0,∀g 6= 0G.\nEach of (i-iii) allows to define parameters that will be useful to quantify evolution. We now provide a concise version of our main results, hiding the less important parameters in the corresponding θ, Õ notations (the complete statement of the Theorem is in Theorem 19).\nTheorem 5 (evolvability of vector spaces, concise statement) Assume (SF) holds. Then G is distribution-free evolvable by any permissible mutator MUT, with tolerance:\nT = θ( 2) , (8)\nand magnitude of mutations:\nα = θ( ) . (9)\nThe number of conditions sampled at each iteration satisfies:\nm = Õ ( T 4D 2 log ( dBTD )) . (10)\nFinally, the number of evolution steps T sufficient to comply with ineq. (6) is T = Õ (T 4D/ 2)."
    }, {
      "heading" : "3.1 Proof of Theorem 5 : key steps",
      "text" : "The proof of Theorem 5 relies on two key definitions: (a) evolvability occurs in a mean-divergence model reminiscent of Markowitz’ mean-variance model (Markowitz, 1952), and (b) the analysis is driven by a quantity that ties the expression and encoding of f . Let us start by (a).\nDefinition 6 For any representation f , condition x, magnitude α and polarity σ, we let\nRf,i(x) . = 〈σbi(x), (∇ϕ ◦ t)(x)− (∇ϕ ◦ f)(x)〉 , and (11) Πf,i(x) . = 1\nα Dϕ(f(x)− α · (−σ)bi(x)‖f(x)) (12)\nthe mutator’s return and premium on x given f , omitting σ and α in the notations.\nWe give an equivalent definition for the set of beneficial mutations, using returns and premiums.\nLemma 7 BENE(f) = {f + σαbi ∈ N (f) : ES[Rf,i(x)]− ES[Πf,i(x)] ≥ (T/α)}. (Proof in Appendix, Subsection 6.3) One reason to relate the decomposition in Lemma 7 to a mean-divergence model comes from when Dϕ is Mahalanobis divergence, the model simplifies to an equivalent of Markowitz model (Markowitz, 1952) in which Πf,i(x) = (α/2) · 〈bi(x),Mbi(x)〉 (M symmetric positive definite), i.e. the magnitude of mutations α is exactly Arrow-Pratt measure of absolute risk aversion — since α > 0, evolution is “risk averse”. Since it does not depend on t, the mutator’s premium quantifies the (local) risk of mutating. Hereafter, we let “expected” return and “expected” premium denote the expectation of (11) and (12) over a distribution on X. We now address (b).\nDefinition 8 The phenotype-to-genotype (PG) ratio of f ∈ G given distribution D is:\nρ(f |D) .= ED[‖f(x)‖ 2 2]\n‖f‖G = VarD[‖f(x)‖2] + (ED[‖f(x)‖2])2 ‖f‖G . (13)\nThe PG-divergence between f ∈ G and g ∈ G given distribution D is ρ(f, g|D) .= ρ(f − g|D). A justification for the name of ρ(f, g|D) comes from the fact that ρ(f |D) = ρ(f, 0G|D), 0G representing a “void genome”. The proof of the Theorem relies on two arguments. The first establishes that, provided the mutator samples sufficient conditions m, the set of beneficial mutations is never empty with high probability, as long as the current f is “far” from the optimum, where this distance notion relies on the PG-divergence between f and the target t. In fact, we show a bit more, as in this case mutations may be superior beneficial: we call them superior beneficial because while beneficial mutations shall be proven to yield an improvement of Ω( 2) in performance, those superior beneficial mutations yield a greater increase of Ω( ). In the second argument, we show that when the first argument does not hold anymore, the requirements of evolvability are met, and the number of evolution steps is polynomial in all required parameters, so vector spaces are evolvable. To formalize these two arguments, we need several definitions. There is a basis in B which is important, B∗.\nDefinition 9 Let B∗ ⊆ B be the basis that maximises BG(B′) .= BG(B′)/maxb∈B′ ‖b‖G over all bases B′ ⊆ B, where\nBG(B ′) . = (1− κn(B′)) · (1− κa(B′)) · ∑ b∈B′ ‖b‖G dG\n(14)\nand κn(B′) and κa(B′) denote non-negative reals such that:\nG(B′) = (1− κn(B′) 2 dGA(B′) , (15)\ncos(θV) = 1− (1− κa(B′)) 1 dG−1 . (16)\nHere, G(.) and A(.) are the geometric and arithmetic means1 of the squared norms in B′, and θV . = minb6=b′∈B′ min{|∠b, b′|, |π − ∠b, b′|} ∈ [0, π/2] is the minimal angle between two vectors.\nBG(.) aggregates several key components of the basis in arguments, including volume and norms. Roughly, the larger BG(.), the better for evolution (smaller samples, larger magnitude for mutations). It turns out that this parameter tends to be larger as the basis in argument becomes closer to orthonormality, and so orthonormal bases represent the “easiest” cases for evolution from this standpoint. They turn out to be the ones chosen in (Kanade et al., 2010, Section 6).\nLemma 10 ∀B′ a basis of G, BG(B′) and BG(B′) are strictly positive. Proof If κn(B′) = 1, then one vector in B′ is the null vector, if κa(B′) = 1, then two vectors in B′ are collinear, in whichever case B′ cannot be a basis. Thus, (κn(B′), κa(B′)) ∈ [0, 1)2. Finally, no basis vector can be the null vector, so BG(B′) > 0 and BG(B′) > 0, as claimed.\nWithout loss of generality, we are assume that maxi∈[dB]∗ ‖bi‖G = maxbi∈B∗ ‖bi‖G, and supi,x ω(B′∪ {bi}, x) ≤ supx ω(B∗, x) for any basis B′ ⊆ B and bi ∈ B, where ω(B′, x) . = ∑\nbi∈B′ ‖bi(x)‖22 for any B′ ⊆ B and any x ∈ X. These two assumptions simplify derivations without restricting our results. We also assume supx ω(B∗, x) is polynomial in all genome parameters, d, dim(X)), TD, in order not to laden the polynomial dependences of the evolvability model by one which takes into account the maximal magnitude of expressions.\nLet us denote Ψ .= Ex∼D[G>x Gx] where G>x stacks all gi(x) in column, and 0 ≤ γ ≤ γ′ ∞ (resp. 0 ≤ µ ≤ µ′ ∞) the min/max eigenvalues of the Hessian of ϕ (resp. Ψ).\nLemma 11 Under (SF), γ > 0 and µ > 0.\nThe proof follows from the fact that if γ were zero, then the Bregman divergence would make it possible for some f 6= t to have optimal performance, violating (ii) in (SF). If µ were zero, then some genomes would get expressed only on conditions sets of zero measure, violating (iii) in (SF).\nDefinition 12 Let Gt . = {f ∈ G : Perft,ϕ(f,D) ≥ − }, and\nGt,D . = { f ∈ G : ∃bi ∈ B∗, ρ(f, t|D) ≥ √ edG\nγBG(B∗)\n( τ + αγ′ · ‖bi‖G · ρ(bi|D) +\nT α\n)} ,(17)\nwhere α, τ, T > 0 are fixed beforehand. Let Gt,D ⊇ GMON .= {fj : fj′ ∈ Gt,D,∀j′ ∈ [j]}, where fj . = MUT(fj−1,m), and GMON . = {fj}Tj=1\\GMON the sequence “following” GMON.\n1The Geometric and Arithmetic means of reals z1, z2, ..., zm are G . = ( ∏ j zj) 1/m, A . = (1/m) ·∑j zj .\nHence, GMON is the longest prefix sequence of evolved organisms that are all in Gt,D. Note that we do not assume that GMON 6= ∅. A key property of GMON is that its sequence of organisms has monotonically increasing performances and yields with high probability non-empty beneficial sets. This is our first argument.\nTheorem 13 Assume (SF) holds. Suppose α, τ, T > 0 fixed, and mutator is run for T > 0 iterations, sampling at each iteration a number of conditions\nm = Ω\n( γ′2 supx ω 2(B∗, x)\nτ 2\n( γ′µ′\nγµ · T\n2 D\nB 2\nG(B ∗)\n+ α2 ) log ( dBT )) . (18)\nThen the following holds true with probability ≥ 1− :\nBENE(fj) 6= ∅ , ∀fj ∈ GMON , (19)\nwhere we recall that fj . = MUT(fj−1,m).\n(Proof in Appendix, Subsection 6.4)\nLemma 14 If GMON 6= ∅, then let GMON .= {fj}Tj=j? for some j? ∈ [T ]∗. Then fj? ∈ Gt.\n(Proof in Appendix, Subsection 6.5) Hence, the first element in GMON complies with evolvability requirements in eq. (6). What remains to be shown is that as long as the current organism fj stays in GMON (⊆ Gt,D), the performance increases by a substantial amount, guaranteeing that the following scenario occurs: either at some point it escapes GMON, in which case Lemma 14 guarantees that evolvability requirements are met, or it never escapes Gt,D and after a polynomial number of iterations, it satisfies evolvability requirements as well, achieving our second argument. This is shown in the following Lemma.\nLemma 15 Fix T and α as in (30) and (31), and in eqs (17, 18) let τ .= θ( /U), with:\nU . = γ′ 3 2µ′ 1 2\nγ 3 2µ 1 2\n· 2 √ edG\nBG(B∗) · TD .\nThen, with probability ≥ 1− , (fj ∈ GMON)⇒ Perft,ϕ(fj+1,D) = Perft,ϕ(fj,D) + Ω( 2),∀j ∈ [T ]∗. Finally, the number of evolution steps sufficient for GMON to comply with (6) is T = Õ (T 4D/ 2).\n(Proof in Appendix, Subsection 6.6)"
    }, {
      "heading" : "4 Discussion and conclusion",
      "text" : "Some additional property of our mutator are proven in the Appendix, all being collated in Subsection 7.1. In short, evolution can be trapped around the target provided we constraint a bit more some key parameters that influence tolerance and magnitude of mutations. In this case, when we escape the monotonic sequence that brings evolvability in Theorem 19, the organism is going to stay within the evolvability requirements, for a number of iterations / mutations steps that we can control. Also, evolution can be agnostic (Angelino and Kanade, 2014; Feldman, 2009), i.e. we\ncan alleviate condition (i) in setting (SF) and show that evolution converges to the “best” evolvable organism in terms of performances with high probability. Evolution can also handle target drift (Kanade et al., 2010).\nTable 1 summarizes the main features of our approach and compares with related approaches in the literature. Comparison relies on what can be related to “simple” mechanisms for evolution, with weak assumptions about the task. The approach that has the closest properties to ours is (Feldman, 2009) (Theorem 18), yet it evolves a very simple class of concept (singletons). We have sometimes stretched the results to make them comparable, in particular for those that have the least to do with vector spaces (Feldman, 2009; Diochnos and Turán, 2009). In these papers, the magnitude of mutations cannot be a real (organisms evolved are boolean concepts), yet the mutators flip boolean values as well, so they have “optimal” magnitude from this standpoint. We have not displayed properties regarding drift, since it is known that strict monotonicity is sufficient to handle some drift (Kanade et al., 2010), yet the amount tolerable may be significantly smaller than ours.\nTo summarize, what we have shown is that, to evolve a complete vector space, one merely needs a norm and a set that generates the space. With these two ingredients, a straightforward mutator is enough to observe strict monotonicity in evolution with high probability, and handle agnostic evolvability. Note that the mutator operates local modifications that belong to the same set for any organism. This probably makes it unusable on sophisticated approaches (Feldman, 2008; Valiant, 2012), but we claim that it is in fact very natural (Feldman, 2011). The fact that our positive results span most of the features of positive results proven so far, even outside strict monotonicity (Table 1), shows that strict monotone evolution may be in some cases virtually as powerful as evolution at large from Valiant’s model. Row “non-reflexive neighborhood” is a new feature that we have found nowhere else: the fact that the current organism f does not belong to the mutant set forces the mutator to evolve f without the safety net that reflexive mutations belong to neutral neighbors, which therefore somewhat artificially contain “worst case” evolution.\nOur result generalizes in several directions the result that appears to be the closest to ours (Feldman, 2011) (Theorems 4.1, 4.4), i.e., outside the binary classification framework, the realm of well-behaved losses, single-dimensional outputs and non-agnostic evolvability. Our framework is also more general. Feldman requires the target organism to have minimal non-zero margin over all conditions, which is equivalent to replacing the non-zero probability by a unit probability in assumption (iii) of setting (SF), and therefore weakens the general purpose of the result — even when the minimal margin assumption is reasonable in the restricted binary classification setting. Finally, our analysis displays better dependences on the key parameters α, T,m: inspection of the bounds of Feldman shows that the guarantees on performance increase may be very loose, namely as small as Õ( a) for some potentially large constant a, which is significantly worse than our Ω̃( 2) guarantee in GMON.\nFigure 2 presents a complete synthetic view of the main mechanisms shown in our paper (including in the Appendix), with two properties never explicitly documented before: the fact that the apparent weakness of the mutator (which works regardless of the set that generates G) does not prevent it to be able to “compete” with the best mutators when far from the target, and the fact that evolution may just be trapped “close” to the target when it has succeeded. The agnostic evolution setting relies on an analogue to Bregman orthogonal projection theorems (Amari and Nagaoka, 2000) involving the performance function.\nSeveral of the parameters that emerge from the analysis have been documented in the life\nsciences, such as the existence of the efficient frontier in systems biology (Kitano, 2010), or the importance of phenotypic variance, the numerator of the PG-ratio, considered beneficial in several areas related to biology, agriculture, and evolution (Geiler-Samerotte et al., 2013). A contribution to the model of evolution was the fact that we perform our mutations not directly on the “genes expression”, but on unknown functions on which directly depend the expressions. We chose the dependences to be unknown, but linear. An interesting problem would be to go beyond.\nFinally, Valiant’s PAC model led to the intuition and then existence of some of the most powerful algorithms for supervised learning: boosting algorithms. We do believe that the Evolvability model will not just be useful to describe evolution “below” the polynomial complexity horizon: it might be the basis of efficient (new) stochastic optimization algorithms as well. Our experiments are at the most preliminary, but they display some promising encouragements in this direction, in particular as a contribution of Evolvability to provable gradient-free stochastic optimization."
    }, {
      "heading" : "5 Acknowledgments",
      "text" : "Work started while RN was visiting Sony Computer Science Laboratories, Inc. (Tokyo)."
    }, {
      "heading" : "6 Appendix — Proofs of the results in the main body",
      "text" : "6.1 Basic notations and helper Lemmata t target organism fj evolved organism B′ basis ⊆ B B′ dG × dG transition matrix for basis B′ in orthonormal “gene” basis {g1, g2, .., gdG} δ(x)\n. = (t− f)(x) = GxB∗(t− f), difference of expressions between target t and organism f measured with respect to basis B∗\nδi coordinate i of vector δ ‖.‖G norm in canonical (“gene”) basis of G gi canonical basis vector of G\ngi(x) gene expression output (in Rd) of gj on some x ∈ X Gx\n. = [g1(x)|g2(x)| · · · |gdG(x)] ∈ Rd×dG per-gene output matrix on some x ∈ X\nΨ . = Ex∼D[G>x Gx] ∈ RdG×dG ker(.) null space\nUnless otherwise stated, all organisms are expressed in basis B∗ ⊆ B, that is,\nf(x) = GxB ∗f , (20)\nand the norm of the encoding of f expressed in basis B∗ shall be ‖f‖G .= √ 〈B∗f, B∗f〉 . (21)\nWe use in several places the following Lemmata.\nLemma 16 Under setting (SF), there exists symmetric positive definite matrix M such that ED[Rf,i(x)] = 〈B∗bi,MB∗δ〉,∀bi ∈ B (coordinates of δ .= t− f, bi expressed in basis B∗). Proof Because ϕ is twice differentiable and strictly convex, a Taylor expansion of∇ϕ around t(x) yields\n(∇ϕ ◦ t)(x) = (∇ϕ ◦ f)(x) + H(x)(t− f)(x) = (∇ϕ ◦ f)(x) + H(x)GxB∗δ , (22)\nfor some value of the Hessian H(x) of ϕ, therefore symmetric positive definite. We get\nED[〈bi(x), (∇ϕ ◦ t)(x)− (∇ϕ ◦ f)(x)〉] = ED[〈GxB∗bi, H(x)GxB∗δ〉] = ED[〈B∗bi, G>x H(x)GxB∗δ〉] = 〈B∗bi,ED[G>x H(x)Gx]B∗δ〉 .\n= 〈B∗bi,MB∗δ〉 ,\nwith\nM . = ED[G>x H(x)Gx] . (23)\nBecause of (iii) in (SF), for any g 6= 0G ∈ G, for each x ∈ X for which it is expressed, we have g(x) = GxB ∗g 6= 0Rd and so\n〈B∗g,MB∗g〉 = ∫ X p(x)〈g(x), H(x)g(x)〉dx\n≥ Px∼D[g(x) 6= 0Rd ] · min g(x)6=0Rd 〈g(x), H(x)g(x)〉 > 0 ,\nshowing M is positive definite.\nLemma 17 Under setting (SF), for any f, t ∈ G (coordinates expressed in basis B∗), the following holds, for some M′ symmetric positive definite:\nEx∼D[Dϕ(f(x)‖t(x))] = 〈B∗(f − t),M′B∗(f − t)〉 , (24) Ex∼D[Dϕ(f(x)‖t(x))] ∈ [ γ\n2 · 〈B∗(f − t),ΨB∗(f − t)〉, γ\n′\n2 · 〈B∗(f − t),ΨB∗(f − t)〉\n] .(25)\nProof Both results are a consequence of (Amari and Nagaoka, 2000), that for any twice differentiable ϕ,\nDϕ(f(x)‖t(x)) = 1\n2 · 〈GxB∗(f − t), H(x)GxB∗(f − t)〉 , (26)\nfor some value of the Hessian H(x) of ϕ, therefore symmetric positive definite. Then, we define M′ as\nM′ . = 1\n2 · M , (27)\nwhere M is defined in eq. (23), and so M′ 0. We then use the definition of γ to obtain ineq. (25)."
    }, {
      "heading" : "6.2 Complete statement of Theorem 5",
      "text" : "We first provide a more complete statement of Theorem 5. We now define two key parameters:\nU . = γ′ 3 2µ′ 1 2\nγ 3 2µ 1 2\n· 2 √ edG\nBG(B∗) · TD , (28)\nV . = γ′ · max i∈[dB]∗ {ED[‖bi(x)‖22]} , (29)\nand triples of evolution “knobs”, (zτ , zα, zT) ∈ R3+∗, all absolute constants.\nDefinition 18 Set R ⊂ R3 is defined as the subset of triples (z1, z2, z3) such that (i) z1, z2, z3 > 0, (ii) z3 − z1z2 > 0, (iii) z22 − z2(1− z1) + z3 ≤ 0.\nRemark that R 6= ∅, since for example (1/9, 1/3, 2/27) ∈ R. We now state our main result.\nTheorem 19 (evolvability of vector spaces, complete statement) Assume (SF) holds, and fix any (zτ , zα, zT) ∈ R. G is distribution-free evolvable by any permissible mutator MUT, with tolerance:\nT .= zT\nU2 max{1, V } · 2 = θ( 2) , (30)\nand magnitude of mutations:\nα . = zα\nU max{1, V } · = θ( ) . (31)\nThe number of conditions sampled at each iteration satisfies:\nm = O\n( γ′6µ′2 supx ω 2(B∗, x)\nγ4µ2 · 1 B 4 G(B ∗) · T\n4 D 2 log\n( dBTD )) = Õ ( T 4D 2 log ( dBTD )) .(32)\nFinally, the number of evolution steps T sufficient to comply with ineq. (6) is T = Õ (T 4D/ 2).\nRemark that zτ is defined but not used in the Theorem statement. It shall be used in its proof below."
    }, {
      "heading" : "6.3 Proof of Lemma 7",
      "text" : "By definition,\nBENE(f) = {g ∈ N (f) : Perft,ϕ(g, S) ≥ Perft,ϕ(f, S) + T} = {g ∈ N (f) : ES[〈g(x)− f(x), (∇ϕ ◦ t)(x)− (∇ϕ ◦ f)(x)〉 −Dϕ(g(x)‖f(x))] ≥ T}\n= { f + σαbi ∈ N (f) : ES [〈σbi(x), (∇ϕ ◦ t)(x)− (∇ϕ ◦ f)(x)〉] −ES [ 1 α Dϕ((f + α · σbi)(x)‖f(x)) ] ≥ T α } = { f + σαbi ∈ N (f) : ES[Rf,i(x)]− ES[Πf,i(x)] ≥\nT α\n} , (33)\nas claimed (end of the proof of Lemma 7)."
    }, {
      "heading" : "6.4 Proof of Theorem 13",
      "text" : "The proof of the Theorem consists of the following building blocks:\nBB.1 we show a result more general than eq. (19), namely, over all steps j ∈ [T ]∗ and with high probability:(\n∃bi ∈ B∗ : ρ(fj, t|D) ≥ √ edG\nγBG(B∗)\n( τ + αγ′ · ‖bi‖G · ρ(bi|D) +\nT α )) ⇒ BENE(fj) 6= ∅ . (34)\nThis is more general since we show that eq. (19) holds for all organisms of the evolution sequence that belong to Gt,D, and not just the “first” ones in GMON. To have this with high probability it is sufficient to sample m = Ω̃(maxj ‖t− fj‖2G) conditions, which may be hard to upperbound depending on fj;\nBB.2 we show that, in the subsequence GMON, maxj ‖t− fj‖2G may be conveniently upperbounded.\n↪→ (Proof of [BB.1]) We temporarily drop subscript j in fj for clarity. The proof involves the following three steps. First, we show that for any current representation f , there always exist a mutation whose expected return is at least a (positive) fraction of the PG-divergence between f and the target t. Its proof involves a simple lowerbound on the volume induced by an arbitrary basis of vectors, which may be of independent interest. Second, we show that, in the evolvability setting, this mutation is special: whenever the current representation f is in GMON, there is always σ ∈ {−1, 1}, bi ∈ B such that\nED[Rf,i(x)]− ED[Πf,i(x)] ≥ T α + τ . (35)\nWe shall see that this guarantees equivalently ED[Rf,i(x)] − ED[Πf,i(x)] = Ω( ) — we call this mutation superior beneficial, since the right hand side exceeds the beneficial requirements (eq. (33)) by τ , and furthermore the left hand side is measured on D. Third and last, even when the mutation picked is not superior beneficial, sampling a number of examples large enough is sufficient to guarantee BENE(f) 6= ∅. More precisely, when m is large enough, the sum of the two differences between (ED[Rf,i(x)]−ED[Πf,i(x)]) and (ES[Rf,i(x)]−ES[Πf,i(x)]) in absolute value is at most τ over each of the T iterations, with probability ≥ 1 − . Using (35) then proves the statement of the Theorem because of the definition of BENE(f) in (33).\nLemma 20 Assume (SF) holds. Then for any distribution D and any representations t, f ∈ G with coordinates expressed in basis B∗,\nmax bi∈B∗\n|ED[Rf,i(x)]| ≥ K · ρ(f, t|D) , (36)\nwhere\nK . = γ√ edG ·BG(B∗) , (37)\nand BG(B∗) is the corrected average norm in eq. (14).\nProof: The majority of the proof consists in showing first that ineq. (36) holds for\nK . = γ√ e · ( G(B∗)dG A(B∗)dG−1 · (1− cos(θB′)) dG−1 · (1 + (dG − 1) · cos(θB′)) dG ) 1 2 , (38)\nwhere parameters G(B∗), A(B∗), θB′ are defined in eqs (15, 16). Then, we show that ineq. (36) holds for the expression of K in eq. (37). Define for short:\nδ . = t− f = ∑ i∈dG δibi , (39)\nassuming without loss of generality that B∗ .= {b1, b2, ..., bdG}. From Lemma 16, there exists symmetric positive definite matrix M such that ED[Rf,i(x)] = 〈bi,MB∗δ〉, ∀bi ∈ B∗. This allows to\nget the last equality of:\n〈B∗δ,MB∗δ〉 = ∑ i∈[dG] δi · 〈bi,MB∗δ〉\n≤ ∑ i∈[dG] (δi)2  12 ∑ i∈[dG] 〈bi,MB∗δ〉  12 (40) ≤ √ dG max\ni∈[dG] |〈bi,MB∗δ〉| · ∑ i∈[dG] (δi)2  12\n= √ dG max\ni∈[dG] |ED[Rf,i(x)]| · ∑ i∈[dG] (δi)2  12 . (41) Ineq. (40) is Cauchy-Schwartz inequality. Furthermore, the derivations of Lemma 16 and the definition of γ yields 〈B∗δ,MB∗δ〉 = ED[〈B∗δ, G>x H(x)GxB∗δ〉] ≥ γ · ED[〈GxB∗δ, GxB∗δ〉] . = γ · ED[‖δ(x)‖22]. Combining this with (41) yields:\nmax i∈[dG]\n|ED[Rf,i(x)]| ≥ γED[‖δ(x)‖22]\n√ dG (∑ i∈[dG] (δ i)2 ) 1 2 . (42)\nLet us work on the (∑ j (δ j)2 )1/2 term, and relate it to the norm ‖δ‖G .= √ 〈B∗δ, B∗δ〉, with B∗ be the transition matrix that collects vectors from B∗ in column, expressed in the orthonormal gene basis of G. We now need the following Lemma.\nLemma 21 Under (SF), B∗>B∗ 0. Proof Let φB∗ : G → G the linear form that B∗ represents. Condition (i) in (SF) implies that φB∗ is injective, and therefore ker(φB∗) = {0G}, and thus 〈B∗z, B∗z〉 = 〈z, B∗>B∗z〉 > 0 whenever z 6= 0G (End of the proof of Lemma 21). We denote {(λj, uj)}dGj=1 the pairs (strictly positive eigenvalues in non-decreasing order, orthonormal eigenvectors), with uj ∈ RdG , so that the following decomposition holds: B∗>B∗ = ∑ i λiuiu > i .\nIt comes:\n‖δ‖2G . = 〈B∗δ, B∗δ〉 = ∑ i∈[dG] λi〈δ, ui〉2\n= ∑ i∈[dG] (δi)2  · ∑ i∈[dG] λi cos 2(δ, ui) (43)\n≥ λ1 · ∑ i∈[dG] (δi)2  · ∑ i∈[dG] cos2(v, ui)\n= λ1 · ∑ i∈[dG] (δi)2  , (44) where eq. (43) comes from the fact that ujs are normal and (44) comes from the fact that they are orthogonal. Let us define\nB∗′ . = ( dG∑\ni∈[dG] ‖bi‖2G\n) 1 2\n· B∗ . (45)\nB∗′ satisfies:\ntr(B∗′>B∗′) = dG . (46)\nLet 0 < λ̃1 ≤ λ̃2 ≤ ... ≤ λ̃dG the eigenvalues of B∗′>B∗′, all strictly positive because of Lemma 21. The Arithmetic-Geometric-Harmonic means inequality brings:\ndG∏ i=2\nλ̃i ≤ ( 1\ndG − 1 dG∑ i=2 λ̃i\n)dG−1\n=\n( tr(B∗′>B∗′)− λ̃1\ndG − 1\n)dG−1\n= ( dG − λ̃1 dG − 1 )dG−1\n≤ (\ndG dG − 1\n)dG−1 , (47)\nMultiplying both sides by λ̃1 and reorganising, we get: λ̃1 ≥ ( dG − 1 dG )dG−1 det ( B∗′>B∗′ )\n≥ det ( B∗′>B∗′ )\ne , (48)\nsince function u(n) .= ((n−1)/n)n−1 is strictly decreasing on n ∈ N∗ and has limit lim+∞ u(n) = 1/e. Finally, using eq. (48), we obtain from eq. (45):\nλ1 =\n∑ i∈[dG] ‖bi‖ 2 G\ndG · λ̃1 ≥ ∑ i∈[dG] ‖bi‖ 2 G\ndG · det\n( B∗′>B∗′ ) e\n= 1\ne · det (∑i∈[dG] ‖bi‖2G dG ) 1 dG B∗′>B∗′  = 1\ne · det ( dG∑ i∈[dG] ‖bi‖2G ) dG−1 2dG B∗> ( dG∑ i∈[dG] ‖bi‖2G ) dG−1 2dG B∗  . (49) Let us define\nB̃ . = ( dG∑\ni∈[dG] ‖bi‖2G\n) dG−1 2dG\n· B∗ , (50)\nso that ineq. (49) reads\nλ1 ≥ 1\ne · det\n( B̃ > B̃ ) = 1\ne · vol(B̃)2 , (51)\nwhere we let vol(B̃)2 .= det ( B̃ > B̃ ) denote the squared volume induced by set B̃, since the columns\nof B̃ also define a basis of G, B̃ .= {b̃1, b̃2, ..., b̃dG}, with\nb̃i . =\n( dG∑\ni′∈[dG] ‖bi′‖2G\n) dG−1 2dG\n· bi ,∀i ∈ [dG] . (52)\nPutting eq. (51) and (44) altogether, we obtain∑ i∈[dG] (δi)2  12 ≤ ‖v‖G√ λ1\n≤ √e · ‖v‖G vol(B̃) . (53)\nCombining eq. (42) and this last inequality yields:\nmax i∈[dG]\n|ED[Rf,i(x)]| ≥ γvol(B̃)√ edG · ED[‖δ(x)‖ 2 2]\n‖δ‖G = γvol(B̃)√\nedG · ρ(f − t|D)\n= γvol(B̃)√\nedG · ρ(f, t|D) . (54)\nLet us now work on vol(B̃). Denoting SdG the symmetric group of degree dG, we have:\nvol(B̃) = ( det(B̃ > B̃) ) 1 2\n= ∑ σ∈SdG sign(σ) ∏ i∈[dG] 〈b̃i, b̃σ(i)〉  12\n= ∑ σ∈SdG sign(σ) ∏ i∈[dG] ‖b̃i‖G‖b̃σ(i)‖G cos(b̃i, b̃σ(i))  12 = ∏ i∈[dG] ‖b̃i‖G ∑ σ∈SdG sign(σ) ∏ i∈[dG] cos(b̃i, b̃σ(i))\n 12 . (55) Eq. (55) does not depend on the orientation of the vectors in B̃: changing b̃i to −b̃i keeps the same expression (in each product, exactly two cosines change of sign), and there is one such orientation of all vectors such that all angles are in [0, π/2]. The quantity (55) being then decreasing if all cosines increase, it is minimized by the one in which all angles equal θB, in which case expression (55) admits the simplified lowerbound:\n∏ i∈[dG] ‖b̃i‖G ∑ σ∈SdG sign(σ) dG∏ k=1 cos(b̃k, b̃σ(k))  12 ≥ ∏ i∈[dG] ‖b̃i‖G ∑ σ∈SdG sign(σ) cosυ(σ)(θB∗)  12 .\n= Al √ Aa , (56)\nwhere υ(σ) = |{i : σ(i) 6= i}| counts the number of integers whose position has changed through the permutation σ ∈ SdG , or similarly it is the size of the “deranged” sub-permutation of σ. We now proceed through finding simplified expressions for Al and Aa, the parts that respectively depends on lengthes and angles.\nWe first compute Al. We have from the definition of b̃i in (52): Al = ∏ i∈[dG] ‖b̃i‖G\n= ∏ i∈[dG] ∥∥∥∥∥∥∥ ( dG∑ i′∈[dG] ‖bi′‖2G ) dG−1 2dG · bi ∥∥∥∥∥∥∥ G\n= ( dG∑\ni∈[dG] ‖bi‖2G\n) dG−1 2 ∏\ni∈[dG]\n‖bi‖G\n= ∏ i∈[dG] ‖bi‖2G 1 dG  dG 2 /(∑ i∈[dG] ‖bi‖ 2 G dG ) dG−1 2\n= G\ndG 2 B\nA dG−1 2\nB\n. (57)\nWe now compute Aa. Let us define polynomial PdG(z) by:\nPdG(z) . = ∑ σ∈SdG sign(σ)(1− z)υ(σ) . (58)\nNotice that Aa = PdG(1 − cos(θB∗)). While the max degree of PdG(z) is dG, we now show that PdG(z) involves only two monomials, of degree dG and dG − 1.\nLemma 22 PdG(z) = zdG−1 · (dG − (dG − 1) · z). Proof Denote κk the coefficient of (−z)k in PdG(z), for k ∈ [dG]. It satisfies:\nκk = ∑\nσ∈SdG :υ(σ)≥k\nsign(σ)\n( υ(σ)\nk\n) (59)\n= dG∑ j=k ( dG j )( j k ) ∑ σ∈Sj :υ(σ)=j sign(σ) . (60)\nThe inner sum is the sum of all signs of all derangements of a set of j elements. To compute its expression, define Uj ∈ {0, 1}j×j the matrix whose diagonal elements are 0 and off-diagonal elements are 1. It satisfies\ndet (Uj) = ∑\nσ∈Sj :υ(σ)=j\nsign(σ) . (61)\nUj has eigenvalue −1 of order j− 1 (all vectors with 1 and −1 in two consecutive coordinates and zero elsewhere are corresponding eigenvectors), and since its trace is zero, it also admits j − 1 as\neigenvalue of order 1. It follows from eq. (61):∑ σ∈Sj :υ(σ)=j sign(σ) = (−1)j−1(j − 1) , (62)\nout of which we obtain\nκk = dG∑ j=k ( dG j )( j k ) ∑ σ∈Sj :υ(σ)=j sign(σ)\n= dG∑ j=k ( dG j )( j k ) (−1)j−1(j − 1)\n= dG! k! · dG∑ j=k (−1)j−1(j − 1) (dG − j)!(j − k)!\n= dG! k! · dG−k∑ j=0 (−1)j+k−1(j + k − 1) (dG − k − j)!j!\n= (−1)k−1 ( dG k ) · dG−k∑ j=0 ( dG − k j ) (−1)j(j + k − 1)\n= (−1)k−1 ( dG k ) · [ (k − 1) · dG−k∑ j=0 ( dG − k j ) (−1)j︸ ︷︷ ︸\nΣk\n+ dG−k∑ j=1 ( dG − k j ) (−1)jj︸ ︷︷ ︸\nΣ′k\n] . (63)\nWe observe that\nΣk = (−1 + 1)dG−k = 0 ,∀k ≤ dG − 1 . (64)\nFurthermore,\nΣ′k = dG−k∑ j=1 ( dG − k j ) (−1)jj\n= −(dG − k) · dG−k∑ j=1 ( dG − k − 1 j − 1 ) (−1)j−1\n= −(dG − k) · dG−k−1∑ j=0 ( dG − k − 1 j ) (−1)j = −(dG − k)(−1 + 1)dG−k−1 = 0 ,∀k ≤ dG − 2 . (65)\nWe thus get\nκk = 0 ,∀k ≤ dG − 2 . (66)\nFurthermore, eqs (59) and (62) also yields: κdG−1 = ∑\nσ∈SdG :υ(σ)=dG−1\nsign(σ) + dG · ∑\nσ∈SdG :υ(σ)=dG\nsign(σ)\n= dG · ∑\nσ∈SdG−1:υ(σ)=dG−1\nsign(σ) + dG · ∑\nσ∈SdG :υ(σ)=dG\nsign(σ)\n= (−1)dG−1 · (−dG(dG − 2)) + (−1)dG−1dG(dG − 1) = (−1)dG−1dG , (67)\nκdG = ∑\nσ∈SdG :υ(σ)=dG\nsign(σ)\n= (−1)dG−1(dG − 1) . (68)\nPlugging eqs (65), (67), (68) in (58), we obtain the simplified expression:\nPdG(z) = z dG−1 · (dG − (dG − 1) · z) ,\nas claimed (end of the proof of Lemma 22).\nWe obtain\nAa = (1− cos(θB∗))dG−1 · (1 + (dG − 1) · cos(θB∗)) . (69)\nThere remains to put together eqs (54), (55), (56), (57) and (69) to obtain the statement of (36) with the expression of K in eq. (38), and finish the main part of the proof of Lemma 20.\nTo obtain (37) and finish the proof of Lemma 20, we just have to remark that it follows from eq. (57) and ineq. (15) that\nAl ≥ √ A(B∗)(1− κn(B∗))\n= 1− κn(B∗)√\ndG · ∑ i∈[dG] ‖bi‖2G  12\n≥ (1− κn(B∗)) · ∑\ni∈[dG] ‖bi‖G dG , (70)\nwhere ineq. (70) comes from p-norm inequalities. Finally, because cos(θB∗) ≥ 0, condition (16) yields:\nAa ≥ (1− cos(θB∗))dG−1 ≥ (1− κa(B∗)) , (71)\nbecause of the definition of κa(B∗). Putting altogether ineqs. (55) and (56) with the lowerbounds on ineqs. (70) and (71), we obtain:\nvol(B̃) ≥ (1− κn(B∗)) · (1− κa(B∗)) · ∑\ni∈[dG] ‖bi‖G dG\n= BG(B ∗) . (72)\nFinally, combining ineq. (54) and (72) yields the statement of (36) with the expression of K in eq. (37), as claimed (end of the proof of Lemma 20). The following Lemma now shows that for any current representation f , there always exists a mutation with guaranteed lowerbound on its expected return minus its expected premium, where the lowerbound depends on the PG-ratio of the mutation and the PG-divergence between f and target t.\nLemma 23 Let B∗ ⊆ B be any basis of G, and assume (SF) holds. Then for any distribution D and any representations t, f ∈ G with coordinates expressed in basis B∗, ∃σ ∈ {−1, 1},∃bi ∈ B∗ such that:\nED[Rf,i(x)]− ED[Πf,i(x)] ≥ γ√ edG ·BG(B∗) · ρ(f, t|D)− αγ′ · ‖bi‖G · ρ(bi|D) , (73)\nwhere B∗ ⊆ B is defined in eq. (14). Proof: We use Lemma 20 and Definition (11), with which we obtain that there exists, at any call of the mutator, polarity σ ∈ {−1,+1} and basis vector bi ∈ B∗ such that:\nmax bi∈B∗ |ED[Rf,i(x)]| ≥ γ√ edG ·BG(B∗) · ρ(f, t|D) . (74)\nOn the expected premium’s side, we have for any bi ∈ B∗ whose coordinates are given in B∗,\nED[Πf,i(x)] . = 1\nα ED[Dϕ(f(x)− α · (−σ)bi(x)‖f(x))]\n≤ αγ ′\n2 · 〈B∗bi,ΨB∗bi〉 (75)\n= αγ′\n2 · ED[‖bi(x)‖22] (76)\n= αγ′\n2 · ‖bi‖G · ρ(bi|D) ,∀σ ∈ {−1, 1},∀bi ∈ B∗ . (77)\nEq. (75) uses the definition of M in (23) and Lemma 17. Eq. (76) come from the fact that B∗bi, gives the coordinates of bi in the orthonormal gene basis of G. Putting altogether (74) and (77) with 1/2 factor dropped yields the statement of the Lemma. Notice that terms BG · ρ(f, t|D) and ‖bi‖G · ρ(bi|D) are homogeneous in (73), as both quantify a PG-ratio or divergence, weighted by a (corrected) length of the encoding.\nIt comes from Lemma 23 that in the (SF) setting, for some evolution step j ∈ [T ]∗, if mutant fj satisfies, for some σ ∈ {−1, 1} and bi ∈ B∗,\nρ(fj, t|D) ≥ √ edG\nγBG(B∗)\n( τ + αγ′ · ‖bi‖G · ρ(bi|D) +\nT α\n) , (78)\nthen, chaining with ineq. (73), we get\nED[Rfj ,i(x)]− ED[Πfj ,i(x)] ≥ γ√ edG ·BG(B∗) · ρ(fj, t|D)− αγ′ · ‖bi‖G · ρ(bi|D)\n≥ T α + τ , (79)\ni.e. the mutation involving bi ∈ B∗ is superior beneficial. This does not show however that the mutator will pick one of these mutations, and it does not show that the mutator will pick some bi ∈ B∗. In fact, this is not even enough to show that BENE(f) is not empty since it involves estimates (Lemma 7). To show that BENE(f) is not empty, we now show that with high probability the estimates ES[Rfj ,i(x)] and ES[Πfj ,i(x)] are both within τ/2 of their true values, implying in this case from ineq. (79)\nES[Rfj ,i(x)]− ES[Πfj ,i(x)] ≥ (\nT α + τ\n) − τ = T\nα , (80)\nand thus BENE(fj) as defined in (33) is indeed not empty. Remark this relies on the sole assumption that fj ∈ Gt,D; fj may not be in GMON. The constraint that fj ∈ GMON shall be used to compute the number of conditions needed for ineq. (80) to hold with high probability in the sequence GMON, which shall then be used to prove evolvability.\nLemma 24 Let B∗ ⊆ B be any basis of G, and assume (SF) holds. Then for any distribution D and any representations t, f ∈ G with coordinates expressed in basis B∗, the following inequalities hold over the i.i.d. sampling of S (of size m) according to D:\nPS∼D [∃i, σ : |ES[Rf,i(x)]− ED[Rf,i(x)]| ≥ τ ] ≤ 2dB exp ( − BG(B ∗)2mτ 2\n2eγ′2‖t− f‖2G supx ω2(B∗, x)\n) , (81)\nPS∼D [∃i, σ : |ES[Πf,i(x)]− ED[Πf,i(x)]| ≥ τ ] ≤ 2dB exp ( − 2mτ 2\nα2γ′2 supx ω 2(B∗, x)\n) . (82)\nProof: Both bounds are direct applications of the independent bounded differences inequality (IBDI, (McDiarmid, 1998)). We first prove (81). Again, we let B∗ .= {b1, b2, ..., bdG} without loss of generality, and t− f = δ .= ∑i∈[dG] δibi. Take any σ ∈ {−1, 1}, bi ∈ B (the coordinates of this being in B∗). Using notations from Lemma 16 (eq. (22)), we have\nRf,i(x) . = 〈σbi(x), (∇ϕ ◦ t)(x)− (∇ϕ ◦ f)(x)〉 = σ · 〈GxB∗bi, H(x)GxB∗δ〉 ≤ γ′ · |〈GxB∗bi, GxB∗δ〉| (83) ≤ γ′ · √ 〈GxB∗bi, GxB∗bi〉 · √ 〈GxB∗δ, GxB∗δ〉 (84)\n= γ′ · ‖bi(x)‖2 · ‖t(x)− f(x)‖2 ,∀x ∈ X . (85)\nIneq. (83) follows from the definition of γ′, ineq. (84) is Cauchy-Schwartz. Finally, we know that ∀x ∈ X, letting δ .= t− f ,\n‖t(x)− f(x)‖22 . = 〈GxB∗δ, GxB∗δ〉\n= 〈∑ i∈[dG] δi · GxB∗bi, ∑ i∈[dG] δi · GxB∗bi 〉 . (86)\nFor any reals ai and vectors ui (i ∈ [dG]), we have 〈 ∑ i∈[dG] ai·ui, ∑ i∈[dG] ai·ui〉 ≤ ∑\ni,i′∈[dG] ai‖ui‖ai′‖ui‖ = ( ∑ i∈[dG] ai‖ui‖) 2 ≤ (∑i∈[dG] a2i ) · (∑i∈[dG] ‖ui‖2) (from Cauchy-Schwartz inequality), and so eq. (86) yields\n‖t(x)− f(x)‖22 ≤ ∑ i∈[dG] (δi)2  · ∑ i∈[dG] 〈GxB∗bi, GxB∗bi〉\n= ∑ i∈[dG] (δi)2  · ∑ i∈[dG] ‖bi(x)‖22\n≤ e‖t− f‖ 2 G BG(B∗)2 · ∑ i∈[dG] ‖bi(x)‖22 . (87)\nIneq. (87) follows from ineqs (53) and (72). Putting altogether ineqs (85) and (87), we get\nRf,i(x) ≤ γ′‖bi(x)‖2 · √ e‖t− f‖G BG(B∗) · ∑ i′∈[dG] ‖bi′(x)‖22  12\n≤ √ eγ′‖t− f‖G BG(B∗)\n· ∑\ni′∈[dG]∪{i}\n‖bi′(x)‖22\n= √ eγ′‖t− f‖G · ω(B∗ ∪ {bi}, x) BG(B∗)\n≤ √eγ′‖t− f‖G · ω(B∗, x)\nBG(B∗) , (88)\nby the properties of B∗. Hence, between two sets S and S′ of the same size m and that would differ from a single element, we have:\n|ES[Rf,i(x)]− ES′ [Rf,i(x)]|\n≤ 2 m · √eγ′‖t− f‖G · supx ω(B ∗, x) BG(B∗) . = γ∆(B ∗) . (89)\nThe following bound follows from (McDiarmid, 1998) (Theorem 3.1) and the union bound over B:\nPS∼D [∃i : |ES[Rf,i(x)]− ED[Rf,i(x)]| ≥ τ ] ≤ 2dB exp ( − 2τ 2\nmγ2∆(B ∗)\n) .\nWe replace γ∆(B∗) by its expression in ineq. (89) and obtain (81) . We proceed in the same way for the expected mutator’s premium (82). We know from eq. (26) that, for some value of the Hessian H of ϕ, we have\nΠf,i(x) = α\n2 · 〈GxB∗bi, HGxB∗bi〉\n≤ αγ ′\n2 · |〈GxB∗bi, GxB∗bi〉|\n= αγ′\n2 · ‖bi(x)‖22 , (90)\nwhere the inequality comes from Lemma 11. So, the variation in average premium between two sets S and S′ of the same size m and that would differ from a single condition satisfies:\n|ES[Πf,i(x)]− ES′ [Πf,i(x)]| ≤ 1\nm · αγ′ · sup x ω({i}, x)\n≤ 1 m · αγ′ · sup i′,x ω({i′}, x)\n≤ 1 m · αγ′ · sup x ω(B∗, x) , (91)\nby the properties of B∗, which yields, out of the IBDI (McDiarmid, 1998) and the union bound over B, the following bound:\nPS∼D [∃i, σ : |ES[Πf,i(x)]− ED[Πf,i(x)]| ≥ τ ] ≤ 2dB exp ( − 2mτ 2\nα2γ′2 supx ω 2(B∗, x)\n) ,\nas claimed. Suppose now that the mutator samples a sufficient number m of examples that would ensure that the right hand-sides of (81) and (82) are no more than /T for deviation τ/2 (and not τ ). By the union bound, the probability that there exists a run (among the T ) of the mutator, and some i, σ such that one of the averages on S of Rϕ,i(x) or Πf,i(x) deviates from its respective expectation on D by more than τ/2 is no more than T · ( /T ) = . Hence, with probability ≥ 1− , we shall have at all j ∈ [T ]∗ runs of the mutator and for the corresponding σ, bi picked at each call of the mutator:\n|(ES[Rfj ,i(x)]− ES[Πfj ,i(x)])− (ED[Rfj ,i(x)]− ED[Πfj ,i(x)])| ≤ |ES[Rfj ,i(x)]− ED[Rfj ,i(x)]|+ |ES[Πfj ,i(x)]− ED[Πfj ,i(x)]| ≤ (τ/2) + (τ/2) = τ , (92)\nand hence ineq. (80) holds over all T iterations, thus whenever fj ∈ Gt,D, even when the mutator does not pick superior beneficial mutations, or even mutations from B∗, it still has non-empty BENE(f), as claimed in (34).\nUsing Lemma 24, the sufficient number of conditions m that ensures this is found to be any integer that satisfies:\nm = Ω\n( γ′2 supx ω 2(B∗, x)\nτ 2\n( maxj ‖t− fj‖2G\nBG(B′)2 + α2\n) log ( dBT )) . (93)\n↪→ (Proof of [BB.2]) The bound in ineq. (93) may be problematic as little tells us about ‖t− fj‖G and how big it can be. Fortunately, it is sufficient for evolution that we focus on subset GMON ⊆ Gt,D. In this subset, we can bound ‖t − fj‖G, in a very simple way. Recall that sampling a number of example that complies with ineq. (93) is sufficient to guarantee with high probability that ED[Rfj ,i(x)] − ED[Πfj ,i(x)] ≥ (T/α) − τ where bi is the mutation picked by the mutator, which\nimplies in particular\nPerft,ϕ(f + σαbi,D) ≥ Perft,ϕ(f,D) + α ( T α − τ )\n= Perft,ϕ(f,D) + T − ατ . (94) But\nT − ατ = zT U2 max{1, V } · 2 − zαzτ U2 max{1, V } · 2\n= zT − zαzτ\nU2 max{1, V } · 2\n> 0 , (95)\nbecause (zτ , zα, zT) ∈ R (Definition 18). Hence, as long as f ∈ Gt,D (and so, f ∈ GMON), we are guaranteed that\nPerft,ϕ(fj,D) ≥ Perft,ϕ(f0,D) . (96) To obtain our bound on ‖t− fj‖G, we need the following Lemma.\nLemma 25 Under setting (SF), −(1/2)γ′µ′‖t − f‖2G ≤ Perft,ϕ(f,D) ≤ −(1/2)γµ‖t − f‖2G, ∀f, t ∈ G. Proof Let δ .= f − t. We get from Lemma 17 and the definition of Ψ,\nEx∼D[Dϕ(f(x)‖t(x))] ≤ γ′\n2 · 〈B∗δ,ΨB∗δ〉\n≤ γ ′µ′\n2 · 〈B∗δ, B∗δ〉\n= γ′µ′\n2 · ‖δ‖2G . (97)\nWe would obtain similarly Ex∼D[Dϕ(f(x)‖t(x))] ≥ (γµ)/2‖t− f‖2G. Using Lemma 25 brings\n‖t− fj‖2G ≤ − 2\nγµ · Perft,ϕ(fj,D)\n≤ − 2 γµ · Perft,ϕ(f0,D) (98) ≤ γ ′µ′\nγµ · ‖t− f0‖2G\n= γ′µ′\nγµ · T 2D max i∈[dB]∗ ‖bi‖2G ,∀fj ∈ GMON , (99)\nbecause of the definition of TD. Ineq. (98) comes from ineq. (96). Using the last inequality (99), we obtain that a sufficient condition for m to meet (93) is:\nm = Ω\n( γ′2 supx ω 2(B∗, x)\nτ 2\n( γ′µ′\nγµ · maxi∈[dB]∗ ‖bi‖ 2 G\nB2G(B ′)\n· T 2D + α2 ) log ( dBT )) ,\nbut since maxi∈[dB]∗ ‖bi‖G = maxbi∈B∗ ‖bi‖G by the properties of B∗, then it is sufficient that\nm = Ω\n( γ′2 supx ω 2(B∗, x)\nτ 2\n( γ′µ′\nγµ · T\n2 D\nB 2\nG(B ∗)\n+ α2 ) log ( dBT )) ,\nwhich is eq. (18). This achieves the proof of Theorem 13."
    }, {
      "heading" : "6.5 Proof of Lemma 14",
      "text" : "Because of the definition of Gt,D, whenever f 6∈ Gt,D,\nρ(f, t|D) < √ edG\nγBG(B∗)\n( τ + αγ′ · max\nbi∈B∗ {‖bi‖G · ρ(bi|D)}+ T α\n) . (100)\nIf fj?−1 is in GMON but fj? is not in GMON, then ineq. (100) is satisfied by fj? , along with, because of ineq. (99) on fj?−1 and the triangle inequality,\n‖t− fj?‖G ≤ ‖t− fj?−1‖G + ‖fj?−1 − fj?‖G ≤ ( γ′µ′\nγµ\n) 1 2\n· TD max i∈[dB]∗ ‖bi‖G + ‖fj?−1 − fj?‖G\n=\n( γ′µ′\nγµ\n) 1 2\n· TD max i∈[dB]∗ ‖bi‖G + α‖b‖G (101)\n≤ 2 · ( γ′µ′\nγµ\n) 1 2\n· TD max i∈[dB]∗ ‖bi‖G , (102)\nsince U ≥ 1 and for any (zτ , zα, zT) ∈ R, we can show that we have zα ≤ 1, implying α ≤ 1. We have also let b .= fj? − fj?−1 ∈ B in eq. (101). Multiplying ineq (100) by γ′‖t − fj?‖G and using ineq. (102) yields\nγ′‖t− fj?‖G · ρ(fj? , t|D)\n< γ′‖t− fj?‖G · √ edG\nγBG(B∗)\n( τ + αγ′ · max\nbi∈B∗ {‖bi‖G · ρ(bi|D)}+ T α ) ≤ γ ′ 3 2µ′ 1 2\nγ 3 2µ\n1 2 G\n· 2 √ edG ·\nmaxbi∈B∗ ‖bi‖G BG(B∗)\n· TD · ( τ + αγ′ · max\nbi∈B∗ {‖bi‖G · ρ(bi|D)}+ T α\n) (103)\n= γ′ 3 2µ′ 1 2\nγ 3 2µ\n1 2 G\n· 2 √ edG\nBG(B∗) · TD︸ ︷︷ ︸\n=U\n· ( τ + αγ′ · max\ni∈[dB]∗ {‖bi‖G · ρ(bi|D)}+ T α\n) (104)\n≤ Uτ + αU max{1, V }+ UT α = zτ · + zα · + zT zα · , (105)\nby definition of U, α, V, T in eqs. (28, 29, 30, 31). Ineq. (103) holds because of ineq. (99). Hence, we obtain that\nEx∼D[Dϕ(fj?(x)‖t(x))] ≤ γ′‖t− fj?‖G · ρ(fj? , t|D) ≤ zτ · + zα · +\nzT zα ·\n= ( zτ + zα +\nzT zα\n) · (106)\n≤ , (107) because (zτ , zα, zT) ∈ R and condition (iii) in Definition 18. Ineq. (107) is equivalent to:\nPerft,ϕ(fj? ,D) ≥ − , and so fj? ∈ Gt, as claimed."
    }, {
      "heading" : "6.6 Proof of Lemma 15",
      "text" : "We first provide the complete expression of τ ,\nτ . = zτ U · . (108)\nWe also remove subscripts in organisms for clarity. Under the conditions of Theorem 13, when the current organism f ∈ GMON ⊆ Gt,D, we have BENE(f) 6= ∅ and thus the next mutation picks polarity σ ∈ {−1, 1} and bi ∈ B such that ES[Rf,i(x)] − ES[Πf,i(x)] ≥ T/α. The left-hand side is an average computed over the mutator’s sample S. Its difference with its true value (expectation over D) in absolute value is no more than τ over all T iterations with probability ≥ 1− when m meets bound (18). Hence, with probability≥ 1− , the mutation will always exhibit ED[Rf,i(x)]− ED[Πf,i(x)] ≥ (T/α)− τ , and so, using the definition of BENE in eq. (33) and the expressions of T, α, τ in eqs (30, 31, 108),\nPerft,ϕ(f + σαbi,D) ≥ Perft,ϕ(f,D) + α ( T α − τ )\n= Perft,ϕ(f,D) + T − ατ = Perft,ϕ(f,D) +\nzT U2V · 2 − zαzτ U2V · 2\n= Perft,ϕ(f,D) + zT − zαzτ U2V\n· 2 (109) = Perft,ϕ(f,D) + Ω( 2) ,\nsince (zτ , zα, zT) ∈ R (condition (ii), Definition 18). Hence, as long as f ∈ GMON, we have a guaranteed increase in performances given by ineq. (109). If f never leaves GMON, how long would it take for evolvability conditions to be met ? To compute it, we need the following Lemma.\nLemma 26 Assume (SF) holds. The initial representation f0 satisfies\nED[Dϕ(f0(x)||t(x))] ≤ γ′µ′\n2 max bi∈B∗\n‖bi‖2G · T 2D . (110)\nProof We have from the definition of TD,\nEx∼D[Dϕ(f0(x)‖t(x))] .= −Perft,ϕ(f0,D)\n≤ γ ′µ′\n2 ‖f0 − t‖2G (111)\n≤ γ ′µ′\n2 max i∈[dG]∗\n‖bi‖2G · T 2D\n= γ′µ′\n2 max bi∈B∗\n‖bi‖2G · T 2D .\nIneq. (111) comes from Lemma 25. The last identity comes from the properties of B∗.\nThe way we use Lemma 26 is the following: when the number of iterations T times the minimal performance variation in ineq. (109) exceeds ineq (110), then with high probability f ∈ Gt and thus meets the condition of convergence for evolvability in ineq. (6) (Definition 2). So, we want\nT ≥ U 2V γ′µ′maxi∈[dG]∗ ‖bi‖2G\nzT − zαzτ · T\n2 D\n2 2 . (112)\nTaking into account the expression of U and V , it is sufficient that\nT ≥ 1 zT − zαzτ\n· γ ′5µ′2 maxi∈[dB]∗{ED[‖bi(x)‖22] γ3µ · edG maxi∈[dB]∗ ‖bi‖ 4 G\nB2G(B ∗)\n· T 4 D\n2 , (113)\nthat is, disregarding absolute constants and all other parameters in the Õ notation, it is sufficient that\nT = Õ ( T 4D 2 ) , (114)\nas claimed.\nRemark — To pack the proof of Theorem 19, we finally need to check that the number of examplesm in ineq. (32) is indeed sufficient, which is upperbounded by the number of iterations to satisfy evolvability requirements while staying in GMON, i.e. using T as in ineq . (113). Considering the other parameters, τ in eq. (108), the fact that α is O(1) in eq. (31), we obtain from ineq. (18) that it is sufficient to sample\nm = O\n( γ′6µ′2 supx ω 2(B∗, x)\nγ4µ2G · 1 B 4 G(B ∗) · T\n4 D 2 log\n( dBTD ))\n= Õ ( T 4D 2 log ( dBTD )) , (115)\nas claimed. Notice that we have essentially hidden in the Õ notation of ineqs (114) and (115) the eventual (polynomial) dependences of ω(., .) in TD."
    }, {
      "heading" : "7 Appendix — Additional properties",
      "text" : ""
    }, {
      "heading" : "7.1 Statement of the main results",
      "text" : "Trapping evolution around the target — Theorem 19 relies on the existence of a monotonic sequence (with respect to performances) which leads to satisfying the conditions of evolution. Provided we constrain a bit more set R, we can do more than the requirements of evolvability: when we escape this monotonic sequence, the mutated organism is going to stay within the evolvability requirements, over a number of iterations / mutations steps that we can control.\nDefinition 27 Fix N ∈ N∗. Set RN ⊂ R3 is the subset of triples (z1, z2, z3) such that (i) z1, z2, z3 > 0, (ii) z3 − z1z2 > 0, and (iii) (a+ b)z22 − z2(1− bz1) + bz3 ≤ 0, where a . = N/U and b . = 2γ′/γ.\nIt is worthwhile remarking that RN ⊂ R, and furthermore RN 6= ∅ since we can choose for example:\nz1 = 1\n4b , z2 =\n1\n32(a+ b) , z3 =\n1\n64b(a+ b) . (116)\nTheorem 28 Assume (SF) holds, and (zτ , zα, zT) ∈ RN for some N ∈ N∗, and all other parameters are fixed according to Theorem 13. Let f? be the first organism in the sequence f0, f1, ..., fT to hit Gt. Then, with probability ≥ 1− , fj?+j ∈ Gt,∀j ∈ [N ]. (Proof in Appendix, Subsection 7.2)\nAchieving agnostic evolvability — One important question is what happens when t cannot be evolved from B?, i.e. when alleviating condition (i) in setting (SF). Ideally, we would like evolution to converge to the “best” evolvable organism in terms of performances. To our knowledge, few positive result exist in the agnostic / improper evolvability model (Angelino and Kanade, 2014; Feldman, 2009), and the most unrestricted one holds for extremely simple representations: singletons (Feldman, 2009).\nTheorem 29 If we relax assumption (i) in (SF) but keep (ii, iii), then Theorem 19 holds mutatis mutandis with the replacement of ineq. (6) by:\nPerft,ϕ(fT ,D) ≥ sup f∈span(B) Perft,ϕ(f,D)− . (117)\n(Proof in Appendix, Subsection 7.3)\nEvolvability with target drift — Another important question is what happens when the target organism drifts slowly (Kanade et al., 2010). In (Kanade et al., 2010), there is a sequence of targets t0, t1, ... and the objective is to replace the static requirement in ineq. (6) by one which takes into account the last target tT , when ti is allowed to slightly drift with respect to ti−1 with respect to its performances. In our case, since we separate the encoding from computing performances, we allow the encoding to drift, which is perhaps more natural — drift affects genotype before performances. Also, the evaluation of beneficial and neutral mutations in BENE and NEUT are done for fj−1 with respect to tj−1.\nTheorem 30 Assume (SF) holds and the target organism sequence t0, t1, ... drifts according to:\n‖ti+1 − ti‖G ≤ zT − zαzτ\n2U2V (2 + T 2D maxi∈[dB]∗ ‖bi‖2G) · 4 , ∀i ≥ 0 . (118)\nThen Theorem 19 holds mutatis mutandis with the replacement of ineq. (6) by:\nPerftT ,ϕ(fT ,D) ≥ − . (119) (Proof in Appendix, Subsection 7.4) Up to factors that depend upon γ, γ′, µ, µ′, our model of drift is equivalent to the performance drift model of (Kanade et al., 2010) (see Lemma 25 below), yet, as a function of , we tolerate drifts that are larger by factor θ(1/ 2) than theirs, taking as reference their result on the weakest distribution assumptions (product Gaussians), assumptions that we also alleviate. Dependence on TD is necessary up to some extent, as otherwise worst-case drifts would defeat evolution by artificially increasing the actual horizon to the last target (TD is computed for t0).\nEvolution on the efficient frontier — The fact that beneficial mutations involve a mean-divergence decomposition of the expression asks for the nature of the efficient frontier, that is, the set of mutations that would minimize ED[Πf,i(x)] subject to a fixed ED[Rf,i(x)]. This would give “nature’s best bet” against our, perhaps, modest and small B. To answer this question, we consider the restricted case of Mahalanobis divergence. Let us alleviate the constraint that b involves only elements from B, and, for notational convenience, define b(x) .= ∑dG i=1 b\nigi(x). In this case, finding the efficient frontier is solving, similarly to that of the efficient portfolio (Merton, 1972),\narg min b∈RdG\nΠ(b) . = α\n2 · ED [〈b(x),Mb(x)〉] (120) s.c. {\nED [〈b(x),M(t− f)(x)〉] = r 〈1dG , b〉 = n . (121)\nWe let the “efficient frontier” denote the equation that gives r as a function of Π(.), and for that purpose generalize Ψ to ΨM . = Ex∼D[G>x MGx], for any M ∈ Rd×d.\nTheorem 31 Under setting (SF), the equation of the efficient frontier is\nr = { n·〈1dG ,t−f〉 〈1dG ,Ψ −1 M 1dG 〉 · ( 1± √ (ξ(t− f)− 1)(ξ(b)− 1) )\nif 〈1dG , t− f〉 6= 0 ±n · √ 〈1dG ,Ψ−1M 1dG〉 · 〈t− f,ΨM(t− f)〉 · (ξ(b)− 1) if 〈1dG , t− f〉 = 0 (122)\nwhere ξ(u) .= 〈1dG ,Ψ−1M 1dG〉 · 〈u,ΨMu〉/(〈1dG , u〉)2 ≥ 1 (defined for 〈1dG , u〉 6= 0). (Proof in Appendix, Subsection 7.5) The equation depends on Π(b) since ξ(b) = (2〈1dG ,Ψ−1M 1dG〉/(αn2))· Π(b) ∝ Π(b). The question is now how large can r be independently of the mutation process, under the constraint that the mutator picks b ∝ αb′ with α = O( ) and 〈1dG , b′〉 constant (call it setting “D”). This prevents this mutator to “artificially” beat ours just because of the magnitude of mutations. Let us define vectors `l . = [〈1dG ,Ψ−1M 1dG〉 〈1dG , t−f〉]>, `r . = [〈1dG , t−f〉 〈t−f,ΨM(t−f)〉]> and `∗ .\n= [n r]>. We also put in (D) the constraint ξ(t − f) > 1 (implying det[`l|`r] 6= 0), and the fact that the decomposition `∗ . = vl`l + vr`r satisfies |vl| + |vr| = O(tr[`l|`r]). This implies in particular that n, r cannot be significantly larger than ‖t− f0‖2dG , and so nature cannot have the organism “jump” from far (f0) to close to target (t) in just one or few mutations.\nLemma 32 Under settings (SF + D), returns on the efficient frontier satisfy r = Õ( ).\n(The bound, in Appendix, Subsection 7.6, is explicit without the tilde notation) Hence, in the (D) regime, the mutation mechanism on the efficient frontier enjoys a dependence on of the same order as that of superior beneficial mutations, that always exist in B under setting (SF) alone — if we modify our mutator so that it picks the best mutation at each iteration, like Opt-Sel in (Angelino and Kanade, 2014), then we are guaranteed to have mutations with a near-optimal dependence in throughout all GMON."
    }, {
      "heading" : "7.2 Proof of Theorem 28",
      "text" : "Let fj? ∈ Gt be the first organism in the sequence f0, f1, ..., fT to hit Gt. Because of Theorem 13 and Lemma 14, with high probability, all organisms before fj? belong to GMON. We have two cases, either fj? ∈ GMON or fj? ∈ GMON. We know already that with high probability, as long as the mutated organism fj stays in GMON, its performance cannot decrease; therefore, if fj? ∈ GMON, then all subsequent mutated organism in GMON also belong to Gt. What will be sufficient to show Theorem 28 will be to show that the sequence GMON, clamped to its first element not before fj? (we call it G ?\nMON ), satisfies\nCard(G ?\nMON ∩ Gt) ≥ N . (123)\nLet fj′? denote the first element of G ? MON , with therefore j′? ≥ j? and fj′?−1 ∈ GMON. Let us define δ .\n= t − fj? , δ′′ .\n= ∑N\nk=1 σkαbβ(k) and δ ′ .= δ − δ′′, σk ∈ {−1, 1} and β : [N ] → [dB] gives the\nmutations chosen to evolve further fj′? for N steps. All coordinates for δ, δ ′, δ′′, bβ(k) are expressed in basis B∗. Using eq. (24) in Lemma 17 and the expression of M 0 in (23), and Lemma 17, we\nget, for any N ′ ∈ [N ],\nEx∼D Dϕ fj′? + ∑\nk∈[N ′]\nσkαbβ(k)  (x) ∥∥∥∥∥∥ t(x)  = 1 2 · 〈B∗δ′,MB∗δ′〉\n≤ γ ′\n2 · 〈B∗δ′,ΨB∗δ′〉 . (124)\nWe then observe γ′\n2 · 〈B∗δ′,ΨB∗δ′〉 ≤ γ′ · 〈B∗δ,ΨB∗δ〉+ αγ′ · 〈B∗δ′′,ΨB∗δ′′〉 (125) ≤ γ′ · 〈B∗δ,ΨB∗δ〉+N ′αγ′ · ∑ k∈[N ′] 〈B∗bβ(k),ΨB∗bβ(k)〉 (126)\n= γ′ · 〈B∗δ,ΨB∗δ〉+N ′αγ′ · ∑ k∈[N ′] Ex∼D[‖bβ(k)(x)‖22]\n≤ 2γ ′\nγ · Ex∼D[Dϕ(fj′?(x)‖t(x))] +N ′αγ′ · ∑ k∈[N ′] Ex∼D[‖bβ(k)(x)‖22](127)\n≤ 2γ ′\nγ · Ex∼D[Dϕ(fj′?(x)‖t(x))] +N ′α ·max{1, V } (128)\n= 2γ′\nγ · Ex∼D[Dϕ(fj′?(x)‖t(x))] + N ′zα U\n(129)\n≤ 2γ ′ γ · ( zτ + zα + zT zα ) · + N ′zα U\n(130)\n=\n( 2γ′ γ · ( zτ + zα + zT zα ) + N ′zα U ) ·\n≤ ( 2γ′ γ · ( zτ + zα + zT zα ) + Nzα U ) · (131)\n≤ . (132) Ineqs (125) and (126) hold because for any inner product 〈., .〉 and set {u1, u2, ..., uM},〈∑\nk∈[M ] uk, ∑ k∈[M ] uk\n〉 ≤ M ·\n∑ k∈[M ] 〈uk, uk〉 , (133)\nsince right hand side minus left hand side is ∑\nk 6=k′〈uk − uk′ , uk − uk′〉 ≥ 0. Ineq. (127) comes from Lemma 17. Ineq. (128) holds because of the definition of V in eq. (29). Eq. (129) holds because of the definition of α in eq. (31). Finally, ineq. (130) holds because we know from ineq. (106) that since fj′? 6∈ GMON, then fj′? 6∈ Gt,D, and so\nEx∼D[Dϕ(fj′?(x)‖t(x))] ≤ ( zτ + zα +\nzT zα\n) · , (134)\nand ineq. (132) holds because (zτ , zα, zT) ∈ RN (Definition 27). Indeed, constraint (iii) yields equivalently\n2γ′ γ · ( z2α + zαzτ + zT ) + Nz2α U ≤ zα ,\nwhich, after dividing by zα > 0 yields that the factor in front of in eq. (131) is ≤ 1. Hence, the moment the sequence f0, f1, ... hits Gt, it shall stay inside Gt with high probability for at least N further evolution steps. This ends the proof of Theorem 28."
    }, {
      "heading" : "7.3 Proof of Theorem 29",
      "text" : "Let B∗ a basis for span(B). Suppose dim(span(B∗)) < dG and denote B⊥ any basis for the supplementary space of span(B∗) in G, and B⊥ its matrix in the orthonormal basis of G. Whenever t 6∈ span(B∗), there uniquely exists two vectors tin ∈ span(B∗) and tout ∈ span(B⊥) such that t = tin + tout.\nFor the sake of readability, we represent tin in B∗ and tout in B⊥, such that G 3 t = B∗tin + B⊥tout. This being defined, for any t∗ ∈ span(B∗), we then have, for some M(t, t∗) 0:\nEx∼D[Dϕ(f(x)‖t(x))] = Ex∼D[Dϕ(f(x)‖t∗(x))] + Ex∼D[Dϕ(t∗(x)‖t(x))] + Ex∼D[〈(t∗ − f)(x),∇ϕ ◦ t(x)−∇ϕ ◦ t∗(x)〉] = Ex∼D[Dϕ(f(x)‖t∗(x))] + Ex∼D[Dϕ(t∗(x)‖t(x))]\n+〈B∗(t∗ − f),M(t, t∗)(B⊥tout + B∗tin − B∗t∗)〉 (135) = Ex∼D[Dϕ(f(x)‖t∗(x))] + Ex∼D[Dϕ(t∗(x)‖t(x))]\n+ 〈t∗ − f, B∗>M(t, t∗)B⊥tout〉︸ ︷︷ ︸ =0 +〈B∗(t∗ − f),M(t, t∗)(B∗tin − Bt∗)〉 (136) = Ex∼D[Dϕ(f(x)‖t∗(x))] + Ex∼D[Dϕ(t∗(x)‖t(x))] + 〈t∗ − f, B∗>M(t, t∗)B∗(tin − t∗)〉 . (137)\nThe first equality is the Bregman triangle equality (Amari and Nagaoka, 2000). Eq. (135) comes from Lemma 16 (matrix M(t, t∗) is defined as in (23); our notation puts in emphasis the fact that the matrix depends on t and t∗, but not on f ). We indeed have 〈t∗ − f, B∗>M(t, t∗)B⊥tout〉 = 0 in eq. (136) since we can always choose B⊥ such that:\nB∗>M(t, t∗)B⊥ = 0 . (138)\nTo see that this holds, remark that B∗>M(t, t∗)B⊥ = (P>B∗)>(P>B⊥) for some transfer matrix P. Since it is a transfer matrix, rank(P>B∗) = rank(B∗) = dim(span(B∗)), so we can find d× (dG − dim(span(B∗))) full rank matrix M̃ such that (P>B∗)>M̃ = 0, which allows to pick B⊥ = (P>)−1M̃, define accordingly B⊥ with the column vectors, and therefore ensures eq. (138) satisfied.\nWe need however to check that B∗ and B⊥ are indeed supplementary in G. Suppose that some h belongs to both spans of B∗ and B⊥ as defined here. Let h∗ and h⊥ be its (unique) coordinates in both sets, therefore satisfying B∗h∗ = B⊥h⊥. We obtain B∗h∗ = (P>)−1M̃h⊥, or equivalently (P>B∗)>P>B∗h∗ = (P>B∗)>M̃h⊥ = 0G, implying\nh∗ ∈ ker((P>B∗)>P>B∗) = ker(B∗>M(t, t∗)B∗) ,\nand therefore h∗ = 0B∗ (since M(t, t∗) 0 and B∗ has full rank), and so h⊥ = 0B⊥ , and finally h = 0G, implying span(B∗)∩span(B>) = {0G} and since dim(span(B∗))+dim(span(B⊥)) = dG, B∗ and B⊥ are supplementary in G, as claimed.\nIn eq. (137), 〈t∗ − f, B∗>M(t, t∗)B∗(tin − t∗)〉 zeroes over all f iff tin = t∗ since again ker(B∗>M(t, t∗)B∗) = {0B∗}. So\nEx∼D[Dϕ(f(x)‖t(x))] = Ex∼D[Dϕ(f(x)‖tin(x))] + Ex∼D[Dϕ(tin(x)‖t(x))], ∀f ∈ G ,(139) from which we get, from the non-negativity of Bregman divergences and (ii) in setting (SF),\ntin = arg sup f∈span(B) Perft,ϕ(f,D) .\nThen, since since the rightmost expectation in eq. (139) does not depend on f , we can equivalently reformulate the definitions of BENE and NEUT in eqs (3) and (4) by:\nBENE(f) = {g ∈ N (f) : Perftin,ϕ(g, S) ≥ Perftin,ϕ(f, S) + T} , (140) NEUT(f) = {g ∈ N (f) : |Perftin,ϕ(g, S)− Perftin,ϕ(f, S)| ≤ T} . (141)\nReplacing t by tin in TD, we get that Theorem 19 can now be applied with all three conditions in (SF) and guarantees this time from ineq. (6):\nPerftin,ϕ(fT ,D) ≥ − , (142) and so, from eq. (139),\nPerft,ϕ(fT ,D) = Perftin,ϕ(fT ,D) + sup f∈span(B) Perft,ϕ(f,D)\n≥ sup f∈span(B) Perft,ϕ(f,D)− ,\nas claimed."
    }, {
      "heading" : "7.4 Proof of Theorem 30",
      "text" : "First, we reformulate the definitions of BENE and NEUT in eqs (3) and (4) to fit to the model (Kanade et al., 2010):\nBENE(fj) = {g ∈ N (fj) : Perftj ,ϕ(g, S) ≥ Perftj ,ϕ(fj, S) + T} , (143) NEUT(fj) = {g ∈ N (fj) : |Perftj ,ϕ(g, S)− Perftj ,ϕ(fj, S)| ≤ T} . (144)\nWe also replace Gt,D by a sequence Gtj ,D, so GMON is now the prefix sequence of f0, f1, ... such that fj ∈ Gtj ,D. The definition of Gtj ,D is the same as in (17).\nThe proof consists of three steps: first (part (i)), we show that Lemma 15 still holds when t is allowed to drift following ineq. (118). Then (part (ii)), we show that the bound in m is the same as in (18). Finally (part (iii)), we show that Lemma 14 also holds, completing the proof. Part (i) —To prove the first part, let β(i) denote the index of the b. ∈ B chosen by mutator at step i. We reuse ineq. (109), and get this time\nPerfti+1,ϕ(fi+1,D) . = Perfti+1,ϕ(fi + σαbβ(i),D) ≥ Perfti+1,ϕ(fi,D) + α ( T α − τ )\n= Perfti+1,ϕ(fi,D) + zT − zαzτ U2V · 2 . (145)\nNow, we use the Bregman triangle equality (Amari and Nagaoka, 2000), which yields\nEx∼D[Dϕ(fi(x)‖ti(x))] = Ex∼D[Dϕ(fi(x)‖ti+1(x))] + Ex∼D[Dϕ(ti+1(x)‖ti(x))]\n+Ex∼D[〈(ti+1 − fi)(x),∇ϕ ◦ ti(x)−∇ϕ ◦ ti+1(x)〉] , and so, reorganizing,\nPerfti+1,ϕ(fi,D) = Perfti,ϕ(fi,D) + Ex∼D[Dϕ(ti+1(x)‖ti(x))] +Ex∼D[〈(ti+1 − fi)(x),∇ϕ ◦ ti(x)−∇ϕ ◦ ti+1(x)〉] . (146)\nLemma 16 yields, for some symmetric positive definite M defined in the same way as in (23),\nEx∼D[〈(ti+1 − fi)(x),∇ϕ ◦ ti(x)−∇ϕ ◦ ti+1(x)〉] = 〈B∗(ti+1 − fi),MB∗(ti − ti+1)〉 = −γ′|〈B∗(ti+1 − fi),ΨB∗(ti − ti+1)〉| (147) ≥ −γ′µ′‖ti+1 − fi‖G‖ti − ti+1‖G (148)\n≥ −γ′µ′ ( ‖t0 − fi‖G +\ni+1∑ k=1\n‖tk+1 − tk‖G ) · ‖ti+1 − ti‖G (149)\n. = −γ′µ′ ( ‖t0 − fi‖G +\ni+1∑ k=1 νk\n) · νi+1 , (150)\nwhere we denote νk . = ‖tk+1 − tk‖G. Ineqs (147) and (148) hold because of the definition and properties of Ψ and Cauchy-Schwartz inequality. Inequality (149) holds because of the triangle inequality. Now, we also have Ex∼D[Dϕ(ti+1(x)‖ti(x))] = −Perfti,ϕ(ti+1,D) ≥ (1/2)γµ · νi+1 because of Lemma 25, so if we fold ineq (150) and eq. (146) into ineq. (145), then we get, ∀fi ∈ GMON, Perfti+1,ϕ(fi+1,D)\n≥ Perfti,ϕ(fi,D) + γµ\n2 · νi+1 − γ′µ′\n( ‖t0 − fi‖G +\ni+1∑ k=1 νk\n) · νi+1 +\nzT − zαzτ U2V · 2\n= Perfti,ϕ(fi,D)− νi+1 2 · ( γ′µ′ ( ‖t0 − fi‖G + i+1∑ k=1 νk ) − γµ ) + zT − zαzτ U2V · 2\n≥ Perfti,ϕ(fi,D)− νi+1 2 · ( γ′µ′ (√ γ′µ′ γµ · TD max i∈[dB]∗ ‖bi‖G + i+1∑ k=1 νk ) − γµ ) + zT − zαzτ U2V · 2 (151)\n≥ Perfti,ϕ(fi,D)− νi+1\n2 · γ′µ′ 1 + max { 1, 2γ′µ′ γµ · T 2D max i∈[dB]∗ ‖bi‖2G } ︸ ︷︷ ︸\n. =W\n+ i+1∑ k=1 νk − γµ \n+ zT − zαzτ U2V · 2 , (152)\nwhere ineq. (151) follows from ineq. (99). Now, remark that γ′µ′W − γµ > 0, and suppose we ensure that, if fi ∈ GMON, ∀i ∈ [T ]∗, then\nνi+1 ≤ η · zT − zαzτ\nU2V (γ′µ′W − γµ) √ T · 2 , (153)\nfor some η > 0. In this case, assuming that the outermost parenthesis is non negative, and letting X . = (zT − zαzτ )/(U2V ), we can assert\nPerfti+1,ϕ(fi+1,D) ≥ Perfti,ϕ(fi,D)− ( η · X\n2 √ T · 2 + η2 · γ\n′µ′X2\n2(γ′µ′W − γµ) · 4\n) +X · 2 .(154)\nNow, we want to fing η such that:\nη · X 2 √ T · 2 + η2 · γ\n′µ′X2\n2(γ′µ′W − γµ) · 4 ≤ X 2 · 2 .\nReorganizing, we find that it is sufficient that\nη2 · γ ′µ′X\nγ′µ′W − γµ · 2 + η√ T − 1 ≤ 0 ,\nand so we need\nη ≤ (γ ′µ′W − γµ) 2γ′µ′X √ T · (√ 1 + 4Tγ′µ′X γ′µ′W − γµ · 2 − 1 ) .\nSince √ 1 + x ≥ 1 + (x/2)− x2/8 ≥ 0 for x ∈ [0, 2(1 + √ 3)], it is sufficient that\nη ≤ √ T · 2 · ( 1− Tγ ′µ′X\nγ′µ′W − γµ · 2\n) , (155)\nand we want\nT < γ′µ′W − γµ γ′µ′X · 1 2 2 . (156)\nIn this case, we can check that 4Tγ′µ′X/(γ′µ′W − γµ) < 2 < 2(1 + √\n3). Replacing X and W by their expressions, we want equivalently\nT ≤ U 2V γ′µ′ zT − zαzτ · ( 1 + max { 1, 2γ′µ′ γµ · T 2D max i∈[dB]∗ ‖bi‖2G } − γµ γ′µ′ ) · 1 2 2 . (157)\nNow, we have\n1 + max{1, 2xy} − 1 x ≥ 2y , ∀x ≥ 1,∀y ≥ 0 , (158)\nand so to ensure ineq. (157), it is sufficient to ensure\nT ≤ U 2V γ′µ′maxi∈[dB]∗ ‖bi‖2G\nzT − zαzτ · T\n2 D 2 . (159)\nIn this case, we can fix\nη =\n√ T\n2 · 2 , (160)\nwhich replaces ineq. (153) by\nνi+1 ≤ zT − zαzτ\n2U2V (γ′µ′W − γµ) · 4 ,\nwhich holds if\nνi+1 ≤ zT − zαzτ\n2U2V (2 + T 2D maxi∈[dB]∗ ‖bi‖2G) · 4 . (161)\nIn this case, ineq. (154) becomes\nPerfti+1,ϕ(fi+1,D) ≥ Perfti,ϕ(fi,D) + zT − zαzτ\n2U2V · 2 . (162)\nSo if the drift is bounded as in ineq. (161), then we lose by a factor at most 2 over the improvement without drift as guaranteed in GMON by Lemma 15. We then need to check that the number of iterations in ineq. (112) now becomes\nT ≥ U 2V γ′µ′maxi∈[dG]∗ ‖bi‖2G\nzT − zαzτ · T\n2 D 2 , (163)\nwhose right-hand side matches ineq. (159). Since ineq. (152) is never tight, picking T of the order of the right-hand side of ineq. (163) allows for GMON to comply with ineq. (119) with a number of steps of the same order as for T in Lemma 15. Hence, Lemma 15 still holds.\nPart (ii) — Notice that as long as fj ∈ GMON, we can still bound ‖tj − fj‖2G in the same way as we do in ineq. (99), because ineq. (98) can still be used via the fact that Perfti+1,ϕ(fi+1,D) ≥ Perfti,ϕ(fi,D), as shown by ineq. (162), so the order of the number of examples in ineq. (18) does not change because the order of T does not change.\nPart (iii) — Because of the definition of Gtj ,D, if we let fj? to denote the first organism out of the prefix sequence GMON (fj?−1 is in GMON but fj? is not), and tj? the target at the same index in the target sequence, then ineq. (102) still holds with tj? , and so we shall observe again, in place of ineq. (107)\nEx∼D[Dϕ(fj?(x)‖tj?(x))] ≤ γ′‖tj? − fj?‖G · ρ(fj? , tj? |D) ≤ , (164)\nwhich means\nPerftj? ,ϕ(fj? ,D) ≥ − ,\nand so fj? satisfies ineq. (119), as claimed."
    }, {
      "heading" : "7.5 Proof of Theorem 31",
      "text" : "Let δ .= t− f for short. We solve\nmin Π(b) . = α\n2 · 〈b,ΨMb〉 s.c. { 〈b,ΨMδ〉 = r 〈1dG , b〉 = n , (165)\nLetting λr and λn the two Lagrange multipliers for the two constraints, we obtain the first order condition αΨMb− λrΨMδ − αλn1 = 0, i.e.,\nb = λr α · δ + λn α ·Ψ−1 M 1dG , (166)\nfrom which we obtain, using the constraints, the following system:{ αr = λr · 〈δ,ΨMδ〉+ λn · 〈1dG , δ〉 αn = λr · 〈1dG , δ〉+ λn · 〈1dG ,Ψ−1M 1dG〉 .\nLemma 33 For any symmetric positive definite M, ΨM 0. The lemma is a direct consequence of assumption (iii) in (SF). Lemma 33 yields that ΨM is invertible. To simplify this system, let us denote for short:\nPb . = α\n2 · 〈b,ΨMb〉 ,\nPδ . = α\n2 · 〈δ,ΨMδ〉 ,\nS . = 〈1dG ,Ψ−1M 1dG〉 , ∆\n. = 〈1dG , δ〉 .\nWe obtain the simplified system:{ αr = 2Pδ\nα · λr + ∆ · λn\nαn = ∆ · λr + S · λn , (167)\nadmitting the solution\nλr = α2Sr − α2∆n 2SPδ − α∆2 , (168) λn = 2αPδn− α2∆r 2SPδ − α∆2 . (169)\nWe note that 2SPδ − α∆2 = α(〈δ,ΨMδ〉〈1dG ,Ψ−1M 1dG〉 − (〈1dG , δ〉)2) ≥ 0 from Cauchy-Schwartz inequality. Suppose that 2SPδ − α∆2 > 0. Multiplying eq. (166) by αΨMb yields\n2Pb = λrr + λnn\n= α2Sr2 − α2∆nr\n2SPδ − α∆2 +\n2αPδn 2 − α2∆nr\n2SPδ − α∆2\n= α2Sr2 − 2α2∆nr + 2αPδn2\n2SPδ − α∆2 ,\nthat is, r is solution of\nα2Sr2 − 2α2∆nr + 2(αPδn2 − 2SPbPδ + αPb∆2) = 0 , from which\nr = ∆n S ± 1 αS\n√ α2∆2n2 − 2S(αPδn2 − 2SPbPδ + αPb∆2)\n= ∆n S ± √( ∆n S )2 − 2 α2S (αPδn2 − 2SPbPδ + αPb∆2)\n= ∆n S ± √( ∆n S )2 − ( ∆n S )2( 2S α∆2 · Pδ − 4S2 α2∆2n2 · PbPδ + 2S αn2 · Pb )\n= ∆n S · ( 1± √ 1− (ξ(δ) + ξ(b)− ξ(δ) · ξ(b)) ) = ∆n S · ( 1± √ (ξ(δ)− 1)(ξ(b)− 1) ) , (170)\nwith, whenever 〈1dG , u〉 6= 0,\nξ(u) . = 〈1dG ,Ψ−1M 1dG〉〈u,ΨMu〉\n(〈1dG , u〉)2 ≥ 1 ,\nfrom Cauchy-Schwartz inequality. This ends the proof of Theorem 31 when ∆ 6= 0 and 〈δ,ΨMδ〉〈1dG ,Ψ−1M 1dG〉− (〈1dG , δ〉)2 > 0.\nNow, if ∆ = 0, r is solution of\nα2Sr2 − 2Pδ(2SPb − αn2) = 0 . (171) Remark that 2SPb − αn2 = α(〈1dG ,Ψ−1M 1dG〉〈b,ΨMb〉 − (〈1dG , b〉)2) ≥ 0 (Cauchy-Schwartz inequality and α ≥ 0), so eq. (171) always has a solution,\nr = ± (n S ) · √ 2SPδ α · (ξ(b)− 1) .\nTo finish up, when 2SPδ − α∆2 = 0, system (167) simplifies to{ αSr = ∆ · (∆ · λr + S · λn) αn = ∆ · λr + S · λn ,\nand so r = ∆n/S. We check that 2SPδ−α∆2 = 0 is equivalent to stating ξ(δ) = 1, in which case we also check that eq. (170) becomes r = ∆n/S."
    }, {
      "heading" : "7.6 Proof of Lemma 32",
      "text" : "We shall prove the more explicit bound that r = O(ρ(t− f0) · ), with (here, x ∨ y .= max{x, y})\nρ(u) . = |〈1dG , u〉|dG · ( γ′µ′\nγµ\n) 5 2 · ( 1 ∨ ( 1\nγ′µ′\n) + γµ‖u‖dG√\ndG · 1 ∨ (‖u‖dG√ dG ))2 .\nLet us define for short 0 < ψmin ≤ ψmax the minimal and maximal eigenvalues of ΨM. Let δ .= t−f for short and column vectors\n`l = [ 〈1dG ,Ψ−1M 1dG〉 〈1dG , δ〉 ] , `r = [ 〈1dG , δ〉 〈δ,ΨMδ〉 ] , `∗ = [ n r ] .\nSince 〈1dG ,Ψ−1M 1dG〉 ≤ dG/ψmin and 〈b,ΨMb〉 ≤ ψmax‖b‖2dG , we have\nξ(b) = 〈1dG ,Ψ−1M 1dG〉〈b,ΨMb〉\nn2\n≤ dGψmax n2ψmin · ‖b‖2dG .\nWe have also from eq. (166),\n‖b‖2dG = λ2r α2 · ‖δ‖2dG + 2λrλn α2 · 〈δ,Ψ−1 M 1dG〉+ λ2n α2 · 〈1dG ,Ψ−2M 1dG〉 . (172)\nIt comes from eqs (168, 169),\nλ2r α2 = α2(〈1dG ,Ψ−1M 1dG〉 · r − 〈1dG , δ〉 · n)2 (〈1dG ,Ψ−1M 1dG〉〈δ,ΨMδ〉 − (〈1dG , δ〉)2)2 ,\n= α2 det2[`l|`∗]\ndet2[`l|`r] ,\nλrλn α2 = α2 det[`l|`∗] det[`∗|`r] det2[`l|`r] ,\nλ2n α2 = α2 det2[`∗|`r] det2[`l|`r] .\nUsing the fact that 〈δ,Ψ−1 M 1dG〉 ≤ √ dG‖δ‖dG/ψmin and 〈1dG ,Ψ−2M 1dG〉 ≤ dG/ψ2min, we obtain from eq. (172)\nξ(b) ≤ dGψmax ψmin · (α n )2 · ( ψmin‖δ‖dG · det[`l|`∗] + √ dG · det[`∗|`r] ψmin · det[`l|`r] )2 . (173)\nLet vl, vr be such that `∗ = vl · `l + vr · `r. Such reals are guaranteed to exist since det[`l|`r] = 〈1dG ,Ψ−1M 1dG〉 · 〈δ,ΨMδ〉 − (〈1dG , δ〉)2 6= 0 by assumption. Then det[`∗|`r] = vl · det[`l|`r] and det[`l|`∗] = vr · det[`l|`r]. We get\nξ(b) ≤ dGψmax ψmin · (α n )2 · (√ dG ψmin + ‖δ‖dG )2 · ( | det[`l|`∗]|+ | det[`∗|`r]| det[`l|`r] )2 = dGψmax ψmin · (α n )2 · (√ dG ψmin + ‖δ‖dG )2 · (|vl|+ |vr|)2\n≤ dGψmax ψmin · (α n )2 · (√ dG ψmin + ‖δ‖dG )2 · ( dG ψmin + ψmax‖δ‖2dG )2 . (174)\nAlgorithm 2 Simple-Evol(S) Input: sample S; Initialize f ← f0; For t = 0, 1, ..., T − 1 Step t.1: if t%1000 = 0 then B← Gen(S); Step t.2: Compute BENE(ft) and NEUT(ft) using N (ft) as in (7) Step t.3: if BENE(ft) 6= ∅ then ft+1 ∼unif. BENE(ft);\nelse if NEUT(ft) 6= ∅ then ft+1 ∼unif. NEUT(ft); else ft+1 ∼unif. N (ft);\nReturn fT ;\nThe last inequality comes from assumption (D), since\ntr[`l|`r] = 〈1dG ,Ψ−1M 1dG〉+ 〈δ,ΨMδ〉\n≤ dG ψmin + ψmax‖δ‖2dG .\nIf n = θ(α), then the dependence in the magnitude of mutations disappear and, taking the square root in ineq. (174),\n√ ξ(b) = O (√ dGψmax ψmin (√ dG ψmin + ‖δ‖dG ) · ( dG ψmin + ψmax‖δ‖2dG )) , (175)\nout of which we get, assuming 〈1dG , δ〉 6= 0, the following upperbound for returns on the efficient frontier:\nr = O\n{ |〈1dG , δ〉|ψmax dG · ( 1 + √ dGψmax ψmin (√ dG ψmin + ‖δ‖dG ) · ( dG ψmin + ψmax‖δ‖2dG )) · }\n= O { |〈1dG , δ〉| · ( ψmax ψmin ) 3 2 · ( 1 + ψmin‖δ‖dG√ dG ) · ( dG ψmin + ψmax‖δ‖2dG ) · }\n= O { |〈1dG , δ〉|dG · ( ψmax ψmin ) 5 2 · ( 1 + ψmin‖δ‖dG√ dG ) · ( 1 ψmax + ψmin‖δ‖2dG dG ) · }\n= O { |〈1dG , δ〉|dG · ( ψmax ψmin ) 5 2 · ( 1 ∨ ( 1 ψmax ) + ψmin‖δ‖dG√ dG · 1 ∨ (‖δ‖dG√ dG ))2 · } .\nWe then conclude, noting that we can fix ψmin = γµ and ψmax = γ′µ′."
    }, {
      "heading" : "8 Appendix — Toy experiments",
      "text" : "We have performed some toy experiments to illustrate how evolution works with our simple mutator, in two different settings of machine learning, illustrated in the right column of Table 2.\nThe high-level implementation of the algorithm, Simple-Evol, is sketched in Algorithm 2 (Gen(S) returns a random set from S that generates R2) and explained below. ↪→ Supervised learning: we generate a mixture of 2D spherical Gaussians with random variance and a random number of vectors in each; vectors of each Gaussian are all labeled positive (black) or negative (grey, class picked uniformly at random). The data is not linearly separable, so the optimal linear separator does not have zero error, and we are in the agnostic setting of Theorem 29. The performance chosen is (minus) the square loss. To speed the mutator, we threshold the number m of conditions sampled at each iteration to a maximum of 50000. ↪→ Unsupervised: to guarantee an optimum that can be measured and compared with, the problem is the estimation of a sample mean. The performance of an organism f with respect to target µ̂ is Perfµ,ϕ(f, S) . = −‖f − µ̂S‖22. Notice that the target is the distribution’s expectation. To complicate this easy task, we restrict the computation of µ̂S over 5 vectors chosen at random in\nthe data. Hence, while the expectation of µ̂S is still the sample’s expectation, the variance of µ̂S is large. In Table 2 (top right), the small green dots display the µ̂S picked. They spread on a large portion of the domain around the target. In both supervised and unsupervised experiments, all data are normalized to fit in a disk of unit norm. we implement the mutator and all other parameters as they are given above, picking = 1\n10\nand B consisting of two randomly chosen vectors in the data that generate R2, flipped to get four mutations. Each 1000 evolvability steps, we renew the basis, still completely at random. Sometimes, in particular for the supervised experiment, both BENE and NEUT are empty. In this case of “failure”, we just force evolution’s hand by taking one of the mutants, chosen uniformly at random in the neighborhood. Therefore, there is not other optimization process carried out in our implementation than the weak optimization achieved by the mutator.\nTable 2 presents the results obtained. While it has no pretention whatsoever to bring significant experimental support for the theory developed — and even less to argue for any superiority of our approach against the state of the art —, it displays interesting patterns of convergence. In particular, the conditions for evolvability to be met are achieved quite early in the process, and the the presence of failures in the supervised case does not prevent evolution to reach classifiers close to the optimal classifier in the longer run."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "In Valiant’s model of evolution, a class of representations is evolvable iff a polynomialtime process of random mutations guided by selection converges with high probability to a representation as -close as desired from the optimal one, for any required > 0. Several previous positive results exist that can be related to evolving a vector space, but each former result imposes restrictions either on (re)initialisations, distributions, performance functions and/or the mutator. In this paper, we show that all it takes to evolve a complete normed vector space is merely a set that generates the space. Furthermore, it takes only Õ(1/ 2) steps and it is essentially strictly monotonic, agnostic and handles target drifts that rival some proven in fairly restricted settings. In the context of the model, we bring to the fore new results not documented previously. Evolution appears to occur in a mean-divergence model reminiscent of Markowitz mean-variance model for portfolio selection, and the risk-return efficient frontier of evolution shows an interesting pattern: when far from the optimum, the mutator always has access to mutations close to the efficient frontier. Toy experiments in supervised and unsupervised learning display promising directions for this scheme to be used as a (new) provable gradient-free stochastic optimisation algorithm. keywords: Evolvability, phenotype/genotype, vector spaces, portfolio selection, Markowitz meanvariance model, Bregman divergence.",
    "creator" : "LaTeX with hyperref package"
  }
}