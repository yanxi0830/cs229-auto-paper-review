{
  "name" : "1611.02830.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Learning for Wireless Distributed Computing",
    "authors" : [ "Yi-Hsuan Kao", "Kwame Wright", "Bhaskar Krishnamachari", "Fan Bai" ],
    "emails" : [ "bkrishna}@usc.edu", "fan.bai@gm.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nWe are at the cusp of revolution as the number of connected devices is projected to grow significantly in the near future. These devices, either suffering from stringent battery usage, like mobile devices, or limited processing power, like sensors, are not capable to run computation-intensive tasks independently. Nevertheless, what can these devices do if they are connected and collaborate with each other? The connected devices in the network, sharing resources with each other, provide a platform with abundant computational resources that enables the execution of complex applications [1], [2].\nTraditional cloud services provide access to high performance and reliable servers. However, considering the varying link quality and the long run trip times (RTTs) of a widearea network (WAN) and possibly long setup time, these remote servers might not always be the best candidates to help in scenarios where the access delay is significant [3], [4]. Another approach is to exploit nearby computational resources, including mobile devices, road-side units (RSUs) and local servers. These devices are not as powerful as cloud servers in general, but can be accessed by faster device to device (D2D) communication [5]. In addition to communication over varying wireless links, the workload on a device also affects the amount of resource it can release. Hence, a system has to identify the\navailable resources in the network and decide how to leverage them among a number of possibilities, considering the dynamic environment at run time.\nFigure 1 illustrates the idea of Wireless Distributed Computing. Given an application that consists of multiple tasks, we want to assign them on multiple devices, considering the resource availability so that the system performance, in metrics like energy consumption and application latency, can be improved. These resources that are accessible by wireless connections form a resource network, which is subject to frequent topology changes and has the following features:\nDynamic device behavior: The quantity of the released resource varies with devices, and may also depend on the local processes that are running. Moreover, some of devices may carry microporcessors that are specialized in performing a subset of tasks. Hence, the performance of each device varies highly over time and different tasks and is hard to model as a known and stationary stochastic process.\nHeterogeneous network with intermittent connections: Devices’ mobility makes the connections intermittent, which change drastically in quality within a short time period. Furthermore, different devices may use different protocols to communicate with each other. Hence, the performance of the links between devices is also highly dynamic and variable and hard to model as a stationary process."
    }, {
      "heading" : "A. Why online learning?",
      "text" : "From what we discuss above, since the resource network is subject to drastic changes over time and is hard to be modeled by stationary stochastic processes, we need an algorithm that\nar X\niv :1\n61 1.\n02 83\n0v 1\n[ cs\n.L G\n] 9\nN ov\n2 01\n6\napplies to all possible scenarios, learns the environment at run time, and adapts to changes. Existing works focus on solving optimization problems given known deterministic profile or known stochastic distributions [6], [7]. These problems are hard to solve. More importantly, algorithms that lack learning ability could be harmed badly by statistical changes or mismatch between the profile (offline training) and the runtime environment. Hence, we use an online learning approach, which takes into account the performance during the learning phase, and aim to learn the environment quickly and adapt to changes.\nWe formulate the task assignment problem as an adversarial multi-armed bandit (MAB) problem that does not make any stochastic assumptions on the resource network [8]. We propose MABSTA (Multi-Armed Bandit based Systematic Task Assignment) that learns the environment and makes task assignment at run time. Furthermore, We provide worst-case analysis on the performance to guarantee that MABSTA performs no worse than a provable lower bound in any dynamic environment. To the best of our knowledge, MABSTA is the first online algorithm in this domain of task assignment problems and provides provable performance guarantee."
    }, {
      "heading" : "B. Contributions",
      "text" : "A new formulation of task assignment problems considering general and dynamic environment: We use a novel adversarial multi-armed bandit (MAB) formulation that does not make any assumptions on the dynamic environment. That is, it applies to all realistic scenarios.\nA light algorithm that learns the environment quickly with provable performance guarantee: MABSTA runs with light complexity and storage, and admits performance guarantee and learning time that are significantly improved compared to the existing MAB algorithm.\nBroad applications on wireless device networks: MABSTA enhances collaborative computing over wireless devices, enabling more potential applications on mobile cloud computing, wireless sensor networks and Internet of Things."
    }, {
      "heading" : "II. BACKGROUND ON MULTI-ARMED BANDIT PROBLEMS",
      "text" : "The multi-armed bandit (MAB) problem is a sequential decision problem where at each time an agent chooses over a set of “arms”, gets the payoff from the selected arms and tries to learn the statistical information from sensing them. These formulations have been considered recently in the context of opportunistic spectrum access for cognitive radio wireless networks, but those formulations are quite different from ours in that they focus only on channel allocation and not on also allocating computational tasks to servers [9], [10].\nGiven an online algorithm to a MAB problem, its performance is measured by a regret function, which specifies how much the agent loses due to the unknown information at the beginning [11]. For example, we can compare the performance to a genie who knows the statistics of payoff functions and selects the arms based on the best policy.\nStochastic MAB problems model the payoff of each arm as a stationary random process and aim to learn the unknown information behind it. If the distribution is unknown but is\nknown to be i.i.d. over time, Auer et al. [12] propose UCB algorithms to learn the unknown distribution with bounded regret. However, the assumption on i.i.d. processes does not always apply to the real environment. On the other hand, Ortner et al. [13] assume the distribution is known to be a Markov process and propose an algorithm to learn the unknown state transition probabilities. However, the large state space of Markov process causes our task assignment problem to be intractable. Hence, we need a tractable algorithm that applies to stochastic processes with relaxed assumptions on time-independence stationarity.\nAdversarial MAB problems, however, do not make any assumptions on the payoffs. Instead, an agent learns from the sequence given by an adversary who has complete control over the payoffs [8]. In addition to the well-behaved stochastic processes, an algorithm of adversarial MAB problems gives a solution that generally applies to all bounded payoff sequences and provides the the worst-case performance guarantee.\nAuer et al. [14] propose Exp3, which serves adversarial MAB and yields a sub-linear regret with time (O( √ T )). That is, compared to the optimal offline algorithm, Exp3 achieves asymptotically 1-competitive. However, if we apply Exp3 to our task assignment problem, there will be an exponential number of arms, hence, the regret will grow exponentially with problem size. In this paper, we propose an algorithm providing that the regret is not only bounded by O( √ T ) but also bounded by a polynomial function of problem size."
    }, {
      "heading" : "III. PROBLEM FORMULATION",
      "text" : "Suppose a data processing application consists of N tasks, where their dependencies are described by a directed acyclic graph (DAG) G = (V, E) as shown in Figure 1. That is, an edge (m,n) implies that some data exchange is necessary between task m and task n and hence task n cannot start until task m finishes. There is an incoming data stream to be processed (T data frames in total), where for each data frame t, it is required to go through all the tasks and leave afterwords. There are M available devices. The assignment strategy of data frame t is denoted by a vector xt = xt1, · · · , xtN , where xti denotes the device that executes task i. Given an assignment strategy, stage-wised costs apply to each node (task) for computation and each edge for communication. The cost can correspond to the resource consumption for a device to complete a task, for example, energy consumption.\nIn the following formulation we follow the tradition in MAB literature and focus on maximizing a positive reward instead of minimizing the total cost, but of course these are mathematically equivalent, e.g., by setting reward = maxCost − cost. When processing data frame t, let R(j)i (t) be the reward of executing task i on device j. Let R(jk)mn (t) be the reward of transmitting the data of edge (m,n) from device j to k. The reward sequences are unknown but are bounded between 0 and 1. Our goal is to find out the assignment strategy for each data frame based on the previously observed samples, and compare the performance with a genie that uses the best assignment strategy for all data frames. That is,\nRmaxtotal = max x∈F T∑ t=1  N∑ i=1 R (xi) i (t) + ∑ (m,n)∈E R(xmxn)mn (t)  , (1)\nAlgorithm 1 MABSTA 1: procedure MABSTA(γ, α) 2: wy(1)← 1 ∀y ∈ F 3: for t← 1, 2, · · · , T do 4: Wt ← ∑ y∈F wy(t)\n5: Draw xt from distribution\npy(t) = (1− γ) wy(t)\nWt +\nγ\n|F| (2)\n6: Get rewards {R(x t i) i (t)}Ni=1, {R (xtmx t n)\nmn (t)}(m,n)∈E . 7: Ciex ← {z ∈ F|zi = xti}, ∀i 8: Cmntx ← {z ∈ F|zm = xtm, zn = xtn}, ∀(m,n) 9: for ∀j ∈ [M ], ∀i ∈ [N ] do\nR̂ (j) i (t) =  R (j) i (t)∑ z∈Ciex pz(t) if xti = j,\n0 otherwise. (3)\n10: end for 11: for ∀j, k ∈ [M ], ∀(m,n) ∈ E do\nR̂(jk)mn (t) =  R (jk) mn (t)∑ z∈Cmntx pz(t) if xtm = j, xtn = k,\n0 otherwise. (4)\n12: end for 13: Update for all y\nR̂y(t) = N∑ i=1 R̂ (yi) i (t) + ∑ (m,n)∈E R̂(ymyn)mn (t), (5)\nwy(t+ 1) = wy(t) exp ( αR̂y(t) ) . (6)\n14: end for 15: end procedure\nwhere F represents the set of feasible solutions. The genie who knows all the reward sequences can find out the best assignment strategy, however, not knowing these sequences in advance, our proposed online algorithm aims to learn this best strategy and remain competitive in overall performance."
    }, {
      "heading" : "IV. MABSTA ALGORITHM",
      "text" : "We summarize MABSTA in Algorithm 1. For each data frame t, MABSTA randomly selects a feasible assignment (arm x ∈ F) from a probability distribution that depends on the weights of arms (wy(t)). Then it updates the weights based on the reward samples. From (2), MABSTA randomly switches between two phases: exploitation (with probability 1 − γ) and exploration (with probability γ). At exploitation phase, MABSTA selects an arm based on its weight. Hence, the one with higher reward samples will be chosen more likely. At exploration phase, MABSTA uniformly selects an arm without considering its performance. The fact that MABSTA keeps probing every arms makes it adaptive to the changes of the environment, compared to the case where static strategy plays the previously best arm all the time without knowing that other arms might have performed better currently.\nThe commonly used performance measure for an MAB algorithm is its regret. In our case it is defined as the difference in accumulated rewards (R̂total) compared to a genie that\nknows all the rewards and selects a single best strategy for all data frames (Rmaxtotal in (1)). Auer et al. [14] propose Exp3 for adversarial MAB. However, if we apply Exp3 to our online task assignment problem, since we have an exponential number of arms (MN ), the regret bound will grow exponentially. The following theorem shows that MABSTA guarantees a regret bound that is polynomial with problem size and O( √ T ).\nTheorem 1. Assume all the reward sequences are bounded between 0 and 1. Let R̂total be the total reward achieved by Algorithm 1. For any γ ∈ (0, 1), let α = γM(N+|E|M) , we have\nRmaxtotal − E{R̂total} ≤ (e− 1)γRmaxtotal + M(N + |E|M) lnMN\nγ .\nIn above, N is the number of nodes (tasks) and |E| is the number of edges in the task graph. We leave the proof of Theorem 1 in the appendix. By applying the appropriate value of γ and using the upper bound Rmaxtotal ≤ (N+ |E|)T , we have the following Corollary.\nCorollary 1. Let γ = min{1, √\nM(N+|E|M) lnMN (e−1)(N+|E|)T }, then\nRmaxtotal − E{R̂total} ≤ 2.63 √ (N + |E|)(N + |E|M)MNT lnM.\nWe look at the worst case, where |E| = O(N2). The regret can be bounded by O(N2.5MT 0.5). Since the bound is a concave function of T , we define the learning time T0 as the time when its slope falls below a constant c. That is,\nT0 = 1.73\nc2 (N + |E|)(N + |E|M)MN lnM.\nThis learning time is significantly improved compared with applying Exp3 to our problem, where T0 = O(MN ). As we will show in the numerical results, MABSTA performs significantly better than Exp3 in the trace-data emulation."
    }, {
      "heading" : "V. POLYNOMIAL TIME MABSTA",
      "text" : "In Algorithm 1, since there are exponentially many arms, implementation may result in exponential storage and complexity. However, in the following, we propose an equivalent but efficient implementation. We show that when the task graph belongs to a subset of DAG that appear in practical applications (namely, parallel chains of trees), Algorithm 1 can run in polynomial time with polynomial storage.\nWe observe that in (5), Ry(t) relies on the estimates of each node and each edge. Hence, we rewrite (6) as\nwy(t+ 1) = exp ( α\nt∑ τ=1 Ry(t)\n)\n= exp α N∑ i=1 R̃ (yi) i (t) + α ∑ (m,n)∈E R̃(ymyn)mn (t)  , (7) where\nR̃ (yi) i (t) = t∑ τ=1 R̂ (yi) i , R̃ (ymyn) mn (t) = t∑ τ=1 R̂(ymyn)mn .\nAlgorithm 2 Calculate w(j)N for tree-structured task graph 1: procedure Ω(N,M,G) 2: q ← BFS (G,N) . run BFS from N and store visited\nnodes in order 3: for i← q.end, q.start do . start from the last element 4: if i is a leaf then . initialize ω values of leaves 5:\nω (j) i ← e (j) i\n6: else 7:\nω (j) i ← e (j) i ∏ m∈Ni ∑ ym∈[M ] e (ymj) mi ω (ym) m\n! (j2|j1) i2|i1! (j1) i1\nTo calculate wy(t), it suffices to store R̃ (j) i (t) and R̃ (j,k) mn (t) for all i ∈ [N ], (m,n) ∈ E and j, k ∈ [M ], which cost (NM + |E|M2) storage.\nEquation (3) and (4) require the knowledge of marginal probabilities P{xti = j} and P{xtm = j, xtn = k}. Next, we propose a polynomial time algorithm to calculate them. From (2), the marginal probability can be written as\nP{xti = j} = (1− γ) 1\nWt ∑ y:yi=j wy(t) + γ M .\nHence, without calculating Wt, we have\nP{xti = j} − γ\nM : P{xti = k} −\nγ M = ∑\ny:yi=j\nwy(t) : ∑\ny:yi=k\nwy(t).\n(8)"
    }, {
      "heading" : "A. Tree-structure Task Graph",
      "text" : "Now we focus on how to calculate the sum of weights in (8) efficiently. We start from tree-structure task graphs and solve the more general graphs by calling the proposed algorithm for trees a polynomial number of times.\nWe drop time index t in our derivation whenever the result holds for all time steps t ∈ {1, · · · , T}. For example, R̃\n(j) i ≡ R̃ (j) i (t). We assume that the task graph is a tree with N nodes where the N th node is the root (final task). Let e (j) i = exp(αR̃ (j) i ) and e (jk) mn = exp(αR̃ (jk) mn ). Hence, the sum of exponents in (7) can be written as the product of e(j)i and\n4 5\n6 1 2\n3\n! (k) 4 ! (l) 5\n! (j) 6e (j) 6 e (kj) 46\ne (lj) 56\ne (jk) mn . That is,∑\ny wy(t) = ∑ y N∏ i=1 e (yi) i ∏ (m,n)∈E e(ymyn)mn .\nFor a node v, we use Dv to denote the set of its descendants. Let the set Ev denote the edges connecting its descendants. Formally,\nEv = {(m,n) ∈ E|m ∈ Dv, n ∈ Dv ∪ {v}}.\nThe set of |Dv|-dimensional vectors, {ym}m∈Dv , denotes all the possible assignments on its descendants. Finally, we define the sub-problem, ω(j)i , which calculates the sum of weights of all possible assignment on task i’s descendants, given task i is assigned to device j. That is,\nω (j) i = e (j) i ∑ {ym}m∈Di ∏ m∈Di e(ym)m ∏ (m,n)∈Ei e(ymyn)mn . (9)\nFigure 2 shows an example of a tree-structure task graph. Task 4 and 5 are the children of task 6. From (9), if we have ω (k) 4 and ω (l) 5 for all k and l, ω (j) 6 can be solved by\nω (j) 6 = e (j) 6 ∑ k,l e (kj) 46 ω (k) 4 e (lj) 56 ω (l) 5 .\nIn general, the relation of weights between task i and its children m ∈ Ni is given by the following equation.\nω (j) i = e (j) i ∑ {ym}m∈Ni ∏ m∈Ni e (ymj) mi ω (ym) m\n= e (j) i ∏ m∈Ni ∑ ym∈[M ] e (ymj) mi ω (ym) m . (10)\nAlgorithm 2 summarizes our approach to calculate the sum of weights of a tree-structure task graph. We first run breath first search (BFS) from the root node. Then we start solving the sub-problems from the last visited node such that when solving task i, it is guaranteed that all of its child tasks have been solved. Let din denote the maximum in-degree of G (i.e., the maximum number of in-coming edges of a node). Running BFS takes polynomial time. For each sub-problem, there are at most din products of summations over M terms. In total, Algorithm 2 solves NM sub-problems. Hence, Algorithm 2 runs in Θ(dinNM2) time."
    }, {
      "heading" : "B. More general task graphs",
      "text" : "All of the nodes in a tree-structure task graph have only one out-going edge. For task graphs where there exists a node that has multiple out-going edges, we decompose the task graph into multiple trees and solve them separately and combine the solutions in the end. In the following, we use an example of a task graph that consists of serial trees to illustrate our approach.\nFigure 3 shows a task graph that has two trees rooted by task i1 and i2, respectively. Let the sub-problem, ω (j2|j1) i2|i1 , denote the sum of weights given that i2 is assigned to j2 and i1 is assigned to j1. To find ω (j2|j1) i2|i1 , we follow Algorithm 2 but consider the assignment on task i1 when solving the subproblems on each leaf m. That is,\nω jm|j1 (m|i1) = e (j1jm) i1m e(jm)m .\nThe sub-problem, ω(j2)i2 , now becomes the sum of weights of all possible assignment on task i2’s descendants, including task 1’s descendants, and is given by\nω (j2) i2\n= ∑\nj1∈[M ] w\n(j2|j1) i2|i1 w (j1) i1 . (11)\nFor a task graph that consists of serial trees rooted by i1, · · · , in in order, we can solve ω(jr)ir , given previously solved ω(jr|jr−1)ir|ir−1 and ω (jr−1) ir−1 . From (11), to solve ω(j2)i2 , we have to solve w(j2|j1)i2|i1 for j1 ∈ {1, · · · ,M}. Hence, it takes O(dinn1M 2) + O(Mdinn2M 2) time, where n1 (resp. n2) is the number of nodes in tree i1 (resp. i2). Hence, to solve a serial-tree task graph, it takes O(dinNM3) time.\nOur approach can be generalized to more complicated DAGs, like the one that contains parallel chains of trees (parallel connection of Figure 3), in which we solve each chain independently and combine them from their common root N . Most of the real applications can be described by these families of DAGs where we have proposed polynomial time MABSTA to solve them. For example, in [15], the three benchmarks fall in the category of parallel chains of trees. In Wireless Sensor Networks, an application typically has a tree-structured workflow [16]."
    }, {
      "heading" : "C. Marginal Probability",
      "text" : "From (8), we can calculate the marginal probability P{xti = j} if we can solve the sum of weights over all possible assignments given task i is assigned to device j. If task i is the root (node N ), then Algorithm 2 solves ω(j)i = ∑ y:yi=j\nwy(t) exactly. If task i is not the root, we can still run Algorithm 2 to solve [ω(j ′) p ]yi=j , which fixes the assignment of task i to device j when solving from i’s parent p. That is,\n[ω(j ′) p ]yi=j = e (j′) p e (jj′) ip ω (j) i ∏ m∈Np\\{i} ∑ ym e(ymj ′) mp ω (ym) m .\nHence, in the end, we can solve [ω(j ′)\nN ]yi=j from the root and∑ y:yi=j wy(t) = ∑ j′∈[M ] [ω(j ′) r ]yi=j .\nSimilarly, the P{xtm = j, xtn = k} can be achieved by solving the conditional sub-problems on both tasks m and n.\nAlgorithm 3 Efficient Sampling Algorithm 1: procedure SAMPLING(γ) 2: s← rand() . get a random number between 0 and 1 3: if s < γ then 4: pick an x ∈ [M ]N uniformly 5: else 6: for i← 1, · · · , N do 7: [ω\n(j) i ]xt1,··· ,xti−1 ← Ω(N,M,G)xt1,··· ,xti−1\n8: P{xti = j|xt1, · · · , xti−1} ∝ [ω (j) i ]xt1,··· ,xti−1\n9: end for 10: end if 11: end procedure"
    }, {
      "heading" : "D. Sampling",
      "text" : "As we can calculate the marginal probabilities efficiently, we propose an efficient sampling policy summarized in Algorithm 3. Algorithm 3 first selects a random number s between 0 and 1. If s is less than γ, it refers to the exploration phase, where MABSTA simply selects an arm uniformly. Otherwise, MABSTA selects an arm based on the probability distribution py(t), which can be written as\npy(t) = P{xt1 = y1} · P{xt2 = y2|xt1 = y1} · · ·P{xtN = yN |xt1 = y1, · · · , xtN−1 = yN−1}.\nHence, MABSTA assigns each task in order based on the conditional probability given the assignment on previous tasks. For each task i, the conditional probability can be calculate efficiently by running Algorithm 2 with fixed assignment on task 1, · · · , i− 1."
    }, {
      "heading" : "VI. NUMERICAL EVALUATION",
      "text" : "In this section, we first examine how MABSTA adapts to dynamic environment. Then, we perform trace-data emulation to verify MABSTA’s performance guarantee and compare it with other algorithms."
    }, {
      "heading" : "A. MABSTA’s Adaptivity",
      "text" : "Here we examine MABSTA’s adaptivity to dynamic environment and compare it to the optimal strategy that relies on the existing profile. We use a two-device setup, where the task execution costs of the two devices are characterized by two different Markov processes. We neglect the channel communication cost so that the optimal strategy is the myopic strategy. That is, assigning the tasks to the device with the highest belief that it is in “good” state [17]. We run our experiment with an application that consists of 10 tasks and processes the incoming data frames one by one. The environment changes at the 100th frame, where the transition matrices of two Markov processes swap with each other. From Figure 4, there exists an optimal assignment (dashed line) so that the performance remains as good as it was before the 100th frame. However, myopic strategy, with the wrong information of the transition matrices, fails to adapt to the changes. From (2), MABSTA not only relies on the result of previous samples but also keeps exploring uniformly (with probability γ\nMN for each arm).\nHence, when the performance of one device degrades at 100th frame, the randomness enables MABSTA to explore another device and learn the changes."
    }, {
      "heading" : "B. Trace-data Emulation",
      "text" : "To obtain trace data representative of a realistic environment, we run simulations on a large-scale wireless sensor network / IoT testbed. We create a network using 10 IEEE 802.15.4-based wireless embedded devices, and conduct a set of experiments to measure two performance characteristics utilized by MABSTA, namely channel conditions and computational resource availability. To assess the channel conditions, the time it takes to transfer 500 bytes of data between every pair of motes is measured. To assess the resource availability of each device, we measure the amount of time it takes to run a simulated task for a uniformly distributed number of iterations. The parameters of the distribution are shown in Table I. Since latency is positively correlated with device’s energy consumption and the radio transmission power is kept constant in these experiments, it can also be used as an index\nfor energy cost. We use these samples as the reward sequences in the following emulation.\nWe present our evaluation as the regret compared to the offline optimal solution in (1). For real applications the regret can be extra energy consumption over all nodes, or extra processing latency over all data frames. Figure 6 validates MABSTA’s performance guarantee for different problem sizes. From the cases we have considered, MABSTA’s regret scales with O(N1.5M).\nWe further compare MABSTA with two other algorithms as shown in Figure 7 and Figure 8. Exp3 is proposed for adversarial MAB in [14]. Randomized baseline simply selects an arm uniformly for each data frame. Applying Exp3 to our task assignment problem results in the learning time grows exponentially with O(MN ). Hence, Exp3 is not competitive in our scheme, in which the regret grows nearly linear with T as randomized baseline does. In addition to original MABSTA, we propose a more aggressive scheme by tuning γ provided in MABSTA. That is, for each frame t, setting\nγt = min\n{ 1, √ M(N + |E|M) lnMN\n(e− 1)(N + |E|)t\n} . (12)\nFrom (2), the larger the γ, the more chance that MABSTA will do exploration. Hence, by exploring more aggressively at the beginning and exploiting the best arm as γ decreases with t, MABSTA with varying γ learns the environment even faster and remains competitive with the offline optimal solution, where the ratio reaches 0.9 at early stage. That is, after first 5000 frames, MABSTA already achieves the performance at least 90% of the optimal one. In sum, these empirical trace-based evaluations show that MABSTA scales well and outperforms the state of the art in adversarial online learning algorithms (EXP3). Moreover, it typically does significantly better in practice than the theoretical performance guarantee."
    }, {
      "heading" : "VII. APPLICATIONS TO WIRELESS DEVICE NETWORKS",
      "text" : "MABSTA is widely applicable to many realistic scenarios, including in the following device networks."
    }, {
      "heading" : "A. Mobile Cloud Computing",
      "text" : "Computational offloading - migrating intensive tasks to more resourceful servers, has been a widely-used approach to augment computing on a resource-constrained device [18]. The performance of computational offloading on cellular networks varies with channel and server dynamics. Instead of solving deterministic optimization based on profiling, like MAUI [19], or providing a heuristic without performance guarantee, like Odessa [15], MABSTA can be applied to learn the optimal offloading decision (task assignment) in dynamic environment.\nB. Vehicular Ad Hoc Networks (VANETs)\nApplications on VANETs are acquiring commercial relevance recently. These applications, like content downloading, rely on both vehicle-to-vehicle (V2V) and vehicle-toinfrastructure (V2I) communications [20]. Computational offloading, or service discovery over VANETs are promising approaches with the help by road-side units and other vehicles [21]. How to leverage these intermittent connections and\nremote computational resources efficiently requires continuous run-time probing, which cannot be done by historical profiling due to fast-changing environment."
    }, {
      "heading" : "C. Wireless Sensor Networks and IoT",
      "text" : "Wireless Sensor Networks (WSN) suffer from stringent energy usage on each node in real applications. These sensors are often equipped with functional microprocessors for some specific tasks. Hence, in some cases, WSN applications face the dilemma of pre-processing on less powerful devices or transmitting raw data to back-end processors [16]. Depending on channel conditions, MABTSA can adapt the strategies by assigning pre-processing tasks on front-end sensors when channel is bad, or simply forwarding raw data when channel is good. Moreover, MABSTA can also consider battery status so that the assignment strategy adapts to the battery remaining on each node in order to prolong network lifetime.\nIn the future IoT networks, fog computing is a concept similar to wireless distributed computing but scales to larger number of nodes and generalized heterogeneity on devices, communication protocols and deployment environment [22]. With available resources spread over the network, a high level programming model is necessary, where an interpreter takes care of task assignment and scheduling at run time [23]. No single stochastic process can model this highly heterogeneous scheme. As an approach to stochastic online learning optimization, MABSTA provides a scalable approach and performance guarantee for this highly dynamic run-time environment."
    }, {
      "heading" : "VIII. CONCLUSION",
      "text" : "With increasing number of devices capable of computing and communicating, the concept of Wireless Distributed Computing enables complex applications which a single device cannot support individually. However, the intermittent and heterogeneous connections and diverse device behavior make the performance highly-variant with time. In this paper, we have proposed a new online learning formulation for wireless distributed computing that does not make any stationary stochastic assumptions about channels and devices. We have presented MABSTA, which, to the best of our knowledge, is the first online learning algorithm tailored to this class of problems. We have proved that MABSTA can be implemented efficiently and provides performance guarantee for all\ndynamic environment. The trace-data emulation has shown that MABSTA is competitive to the optimal offline strategy and is adaptive to changes of the environment. Finally, we have identified several wireless distributed computing applications where MABSTA can be employed fruitfully."
    }, {
      "heading" : "APPENDIX A PROOF OF THEOREM 1",
      "text" : "We first prove the following lemmas. We will use more condensed notations like R̂(yi)i for R̂ (yi) i (t) and R̂ (ymyn) mn for R̂ (ymyn) mn (t) in the prove where the result holds for each t."
    }, {
      "heading" : "A. Proof of lemmas",
      "text" : "Lemma 1.∑\ny∈F py(t)R̂y(t) = N∑ i=1 R (xti) i (t) + ∑ (m,n)∈E R (xtmx t n) mn (t).\nProof:\n∑ y∈F py(t)R̂y(t) = ∑ y∈F py  N∑ i=1 R̂ (yi) i + ∑ (m,n)∈E R̂(ymyn)mn  = ∑ i ∑ y pyR̂ (yi) i + ∑ (m,n) ∑ y pyR̂ (ymyn) mn , (14)\nwhere ∑ y pyR̂ (yi) i = ∑ y∈Ciex py R (xti) i∑ z∈Ciex pz = R (xti) i , and similarly, ∑ y pyR̂ (ymyn) mn = R (xtmx t n) mn .\nApplying the result to (14) completes the proof. Lemma 2. For all y ∈ F , we have\nE{R̂y(t)} = N∑ i=1 R (yi) i (t) + ∑ (m,n)∈E R(ymyn)mn (t).\nProof:\n∑ y∈F py(t)R̂y(t) 2 = ∑ y∈F py  N∑ i=1 R̂ (yi) i + ∑ (m,n)∈E R̂(ymyn)mn 2\n= ∑ y∈F py ∑ i,j R̂ (yi) i R̂ (yj) j + ∑ (m,n),(u,v) R̂(ymyn)mn R̂ (yuyv) uv + 2 ∑ i ∑ (m,n) R̂ (yi) i R̂ (ymyn) mn  (13)\nE{R̂y(t)} = N∑ i=1 E{R̂(yi)i }+ ∑ (m,n)∈E E{R̂(ymyn)mn }, (15)\nwhere\nE{R̂(yi)i } = P{x t i = yi}\nR (yi) i∑\nz∈Ciex pz = R\n(yi) i ,\nand similarly, E{R̂(ymyn)mn } = R(ymyn)mn . Lemma 3. If F = {x ∈ [M ]N}, then for M ≥ 3 and |E| ≥ 3,∑\ny∈F py(t)R̂y(t)\n2 ≤ |E| MN−2 ∑ y∈F R̂y(t).\nProof: We first expand the left-hand-side of the inequality as shown in (13) at the top of this page. In the following, we derive the upper bound for each term in (13) for all i ∈ [N ], (m,n) ∈ E .\n∑ y pyR̂ (yi) i R̂ (yj) j = ∑\ny∈Ciex∩C j ex\npy R\n(xti) i R (xtj) j∑ z∈Ciex pz · ∑ z∈Cjex pz\n≤ R(x t j)\nj\nR (xti) i∑\nz∈Ciex pz\n= R (xtj) j R̂ (xti) i ≤ 1\nMN−1 ∑ y R̂ (yi) i (16)\nThe first inequality in (16) follows by Ciex ∩Cjex is a subset of Cjex and the last inequality follows by R̂ (yi) i = R̂ (xti) i for all y in Ciex. Hence,∑ i,j ∑ y pyR̂ (yi) i R̂ (yj) j ≤ 1 MN−2 ∑ y ∑ i R̂ (yi) i . (17)\nSimilarly,∑ (m,n),(u,v) ∑ y pyR̂ (ymyn) mn R̂ (yuyv) uv ≤ |E| MN−2 ∑ y ∑ (m,n) R̂(ymyn)mn . (18) For the last term in (13), following the similar argument gives\n∑ y pyR̂ (yi) i R̂ (ymyn) mn = ∑ y∈Ciex∩Cmntx py R (xti) i R (xtmx t n) mn∑ z∈Ciex pz · ∑ z∈Cmntx pz\n≤ R(x t mx t n) mn R (xti) i∑\nz∈Ciex pz\n= R (xtmx t n) mn R̂ (xti) i ≤ 1\nMN−1 ∑ y R̂ (yi) i .\nHence,∑ i ∑ (m,n) ∑ y pyR̂ (yi) i R̂ (ymyn) mn ≤ |E| MN−1 ∑ y ∑ i R̂ (yi) i . (19)\nApplying (17), (18) and (19) to (13) gives\n∑ y∈F py(t)R̂y(t) 2\n≤ ∑ y∈F [ ∑ i ( 1 MN−2 + 2 |E| MN−1 )R̂ (yi) i + ∑ (m,n) |E| MN−2 R̂(ymyn)mn ]\n≤ |E| MN−2 ∑ y∈F R̂y(t). (20)\nThe last inequality follows by the fact that 1 MN−2 + 2|E| MN−1\n≤ |E|\nMN−2 for M ≥ 3 and |E| ≥ 3. For M = 2, we have∑\ny∈F py(t)R̂y(t)\n2 ≤ M + 2 |E| MN−1 ∑ y∈F R̂y(t).\nSince we are interested in the regime where (20) holds, we will use this result in our proof of Theorem 1.\nLemma 4. Let α = γM(N+|E|M) , if F = {x ∈ [M ] N}, then for all y ∈ F , all t = 1, · · · , T , we have αR̂y(t) ≤ 1.\nProof: Since ∣∣Ciex∣∣ ≥ MN−1 and |Cmntx | ≥ MN−2 for all i ∈ [N ] and (m,n) ∈ E , each term in R̂y(t) can be upper bounded as\nR̂ (yi) i ≤\nR (yi) i∑\nz∈Ciex pz ≤ 1 MN−1 γ MN = M γ , (21)\nR̂ (yi−1yi) i ≤ R (yi−1yi) i∑ z∈Citx pz ≤ 1 MN−2 γ MN = M2 γ . (22)\nHence, we have\nR̂y(t) = N∑ i=1 R̂ (yi) i + ∑ (m,n)∈E R̂(ymyn)mn\n≤ NM γ + |E|M 2 γ = M γ (N + |E|M). (23)\nLet α = γM(N+|E|M) , we achieve the result."
    }, {
      "heading" : "B. Proof of Theorem 1",
      "text" : "Proof: Let Wt = ∑\ny∈F wy(t). We denote the sequence of decisions drawn at each frame as x = [x1, · · · ,xT ], where xt ∈ F denotes the arm drawn at step t. Then for all data frame t,\nWt+1 Wt = ∑ y∈F wy(t) Wt exp ( αR̂(y)(t) ) = ∑ y∈F py(t)− γ|F| 1− γ exp ( αR̂(y)(t) )\n≤ ∑ y∈F py(t)− γ|F| 1− γ ( 1 + αR̂(y)(t) + (e− 2)α2R̂(y)(t)2 ) (24)\n≤1 + α 1− γ  N∑ i=1 R (xti) i (t) + ∑ (m,n)∈E R (xtmx t n) mn (t)  + (e− 2)α2\n1− γ |E| MN−2 ∑ y∈F R̂y(t). (25)\nEq. (24) follows by the fact that ex ≤ 1 + x + (e − 2)x2 for x ≤ 1. Applying Lemma 1 and Lemma 3 we arrive at (25). Using 1 + x ≤ ex and taking logarithms at both sides,\nln Wt+1 Wt ≤ α 1− γ  N∑ i=1 R (xti) i (t) + ∑ (m,n)∈E R (xtmx t n) mn (t)  + (e− 2)α2\n1− γ |E| MN−2 ∑ y∈F R̂y(t).\nTaking summation from t = 1 to T gives\nln WT+1 W1 ≤ α 1− γ R̂total +\n(e− 2)α2 1− γ |E| MN−2 T∑ t=1 ∑ y∈F R̂y(t).\n(26) On the other hand,\nln WT+1 W1 ≥ ln wz(T + 1) W1 = α T∑ t=1 R̂z(t)− lnMN , ∀z ∈ F .\n(27) Combining (26) and (27) gives R̂total ≥ (1−γ) T∑ t=1 R̂z(t)−(e−2)α |E| MN−2 T∑ t=1 ∑ y∈F R̂y(t)− lnMN α . (28) Eq. (28) holds for all z ∈ F . Choose x? to be the assignment strategy that maximizes the objective in (1). Now we take expectations on both sides based on x1, · · · ,xT and use Lemma 2. That is, T∑ t=1 E{R̂x?(t)} = T∑ t=1 [ N∑ i=1 R (x?i ) i (i)+ ∑ (m,n)∈E R (x?mx ? n) mn (t)] = R max total,\nand\nT∑ t=1 ∑ y∈F E{R̂y(t)}\n= T∑ t=1 ∑ y∈F  N∑ i=1 R (yi) i (t) + ∑ (m,n)∈E R(ymyn)mn (t)  ≤MNRmaxtotal. Applying the result to (28) gives\nE{R̂total} ≥ (1− γ)Rmaxtotal − |E|M2(e− 2)αRmaxtotal − lnMN\nα .\nLet α = γM(N+|E|M) , we arrive at\nRmaxtotal − E{R̂total} ≤ (e− 1)γRmaxtotal + M(N + |E|M) lnMN\nγ ."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Abstract—There has been a growing interest for Wireless Distributed Computing (WDC), which leverages collaborative computing over multiple wireless devices. WDC enables complex applications that a single device cannot support individually. However, the problem of assigning tasks over multiple devices becomes challenging in the dynamic environments encountered in real-world settings, considering that the resource availability and channel conditions change over time in unpredictable ways due to mobility and other factors. In this paper, we formulate a task assignment problem as an online learning problem using an adversarial multi-armed bandit framework. We propose MABSTA, a novel online learning algorithm that learns the performance of unknown devices and channel qualities continually through exploratory probing and makes task assignment decisions by exploiting the gained knowledge. For maximal adaptability, MABSTA is designed to make no stochastic assumption about the environment. We analyze it mathematically and provide a worstcase performance guarantee for any dynamic environment. We also compare it with the optimal offline policy as well as other baselines via emulations on trace-data obtained from a wireless IoT testbed, and show that it offers competitive and robust performance in all cases. To the best of our knowledge, MABSTA is the first online algorithm in this domain of task assignment problems and provides provable performance guarantee.",
    "creator" : "LaTeX with hyperref package"
  }
}