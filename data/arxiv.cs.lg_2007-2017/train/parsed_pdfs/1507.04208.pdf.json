{
  "name" : "1507.04208.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Combinatorial Cascading Bandits",
    "authors" : [ "Branislav Kveton" ],
    "emails" : [ "kveton@adobe.com", "zhengwen@yahoo-inc.com", "azin.ashkan@technicolor.com", "szepesva@cs.ualberta.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose combinatorial cascading bandits, a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one. The weights of the items are binary, stochastic, and drawn independently of each other. The agent observes the index of the first chosen item whose weight is zero. This observation model arises in network routing, for instance, where the learning agent may only observe the first link in the routing path which is down, and blocks the path. We propose a UCB-like algorithm for solving our problems, CombCascade; and prove gap-dependent and gap-free upper bounds on its n-step regret. Our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting, a non-linear reward function and partial observability. We evaluate CombCascade on two real-world problems and show that it performs well even when our modeling assumptions are violated. We also demonstrate that our setting requires a new learning algorithm."
    }, {
      "heading" : "1 Introduction",
      "text" : "Combinatorial optimization [16] has many real-world applications. In this work, we study a class of combinatorial optimization problems with a binary objective function that returns one if and only if the weights of all chosen items are one. The weights of the items are binary, stochastic, and drawn independently of each other. Many popular optimization problems can be formulated in our setting. Network routing is a problem of choosing a routing path in a computer network that maximizes the probability that all links in the chosen path are up. Recommendation is a problem of choosing a list of items that minimizes the probability that none of the recommended items are attractive. Both of these problems are closely related and can be solved using similar techniques (Section 2.3).\nCombinatorial cascading bandits are a novel framework for online learning of the aforementioned problems where the distribution over the weights of items is unknown. Our goal is to maximize the expected cumulative reward of a learning agent in n steps. Our learning problem is challenging for two main reasons. First, the reward function is non-linear in the weights of chosen items. Second, we only observe the index of the first chosen item with a zero weight. This kind of feedback arises frequently in network routing, for instance, where the learning agent may only observe the first link in the routing path which is down, and blocks the path. This feedback model was recently proposed in the so-called cascading bandits [10]. The main difference in our work is that the feasible set can be arbitrary. The feasible set in cascading bandits is a uniform matroid.\nar X\niv :1\n50 7.\n04 20\n8v 3\n[ cs\n.L G\n] 1\nStochastic online learning with combinatorial actions has been previously studied with semi-bandit feedback and a linear reward function [8, 11, 12], and its monotone transformation [5]. Established algorithms for multi-armed bandits, such as UCB1 [3], KL-UCB [9], and Thompson sampling [18, 2]; can be usually easily adapted to stochastic combinatorial semi-bandits. However, it is non-trivial to show that the algorithms are statistically efficient, in the sense that their regret matches some lower bound. Kveton et al. [12] recently showed this for CombUCB1, a form of UCB1. Our analysis builds on this recent advance but also addresses two novel challenges of our problem, a non-linear reward function and partial observability. These challenges cannot be addressed straightforwardly based on Kveton et al. [12, 10].\nWe make multiple contributions. In Section 2, we define the online learning problem of combinatorial cascading bandits and propose CombCascade, a variant of UCB1, for solving it. CombCascade is computationally efficient on any feasible set where a linear function can be optimized efficiently. A minor-looking improvement to the UCB1 upper confidence bound, which exploits the fact that the expected weights of items are bounded by one, is necessary in our analysis. In Section 3, we derive gap-dependent and gap-free upper bounds on the regret of CombCascade, and discuss the tightness of these bounds. In Section 4, we evaluate CombCascade on two practical problems and show that the algorithm performs well even when our modeling assumptions are violated. We also show that CombUCB1 [8, 12] cannot solve some instances of our problem, which highlights the need for a new learning algorithm."
    }, {
      "heading" : "2 Combinatorial Cascading Bandits",
      "text" : "This section introduces our learning problem, its applications, and also our proposed algorithm. We discuss the computational complexity of the algorithm and then introduce the co-called disjunctive variant of our problem. We denote random variables by boldface letters. The cardinality of set A is |A| and we assume that min ∅ = +∞. The binary and operation is denoted by ∧, and the binary or is ∨."
    }, {
      "heading" : "2.1 Setting",
      "text" : "We model our online learning problem as a combinatorial cascading bandit. A combinatorial cascading bandit is a tuple B = (E,P,Θ), where E = {1, . . . , L} is a finite set of L ground items, P is a probability distribution over a binary hypercube {0, 1}E , Θ ⊆ Π∗(E), and:\nΠ∗(E) = {(a1, . . . , ak) : k ≥ 1, a1, . . . , ak ∈ E, ai 6= aj for any i 6= j} is the set of all tuples of distinct items from E. We refer to Θ as the feasible set and to A ∈ Θ as a feasible solution. We abuse our notation and also treat A as the set of items in solution A. Without loss of generality, we assume that the feasible set Θ covers the ground set, E = ∪Θ.\nLet (wt)nt=1 be an i.i.d. sequence of n weights drawn from distribution P , where wt ∈ {0, 1} E . At time t, the learning agent chooses solution At = (at1, . . . ,a t |At|) ∈ Θ based on its past observations and then receives a binary reward:\nrt = min e∈At wt(e) = ∧ e∈At wt(e)\nas a response to this choice. The reward is one if and only if the weights of all items in At are one. The key step in our solution and its analysis is that the reward can be expressed as rt = f(At,wt), where f : Θ× [0, 1]E → [0, 1] is a reward function, which is defined as:\nf(A,w) = ∏ e∈A w(e) , A ∈ Θ , w ∈ [0, 1]E .\nAt the end of time t, the agent observes the index of the first item in At whose weight is zero, and +∞ if such an item does not exist. We denote this feedback by Ot and define it as:\nOt = min { 1 ≤ k ≤ |At| : wt(atk) = 0 } .\nNote that Ot fully determines the weights of the first min {Ot, |At|} items in At. In particular: wt(a t k) = 1{k < Ot} k = 1, . . . ,min {Ot, |At|} . (1)\nAccordingly, we say that item e is observed at time t if e = atk for some 1 ≤ k ≤ min {Ot, |At|}. Note that the order of items in At affects the feedback Ot but not the reward rt. This differentiates our problem from combinatorial semi-bandits.\nThe goal of our learning agent is to maximize its expected cumulative reward. This is equivalent to minimizing the expected cumulative regret in n steps:\nR(n) = E [ ∑n t=1R(At,wt)] ,\nwhere R(At,wt) = f(A∗,wt) − f(At,wt) is the instantaneous stochastic regret of the agent at time t and A∗ = arg maxA∈Θ E [f(A,w)] is the optimal solution in hindsight of knowing P . For simplicity of exposition, we assume that A∗, as a set, is unique.\nA major simplifying assumption, which simplifies our optimization problem and its learning, is that the distribution P is factored:\nP (w) = ∏ e∈E Pe(w(e)) , (2)\nwhere Pe is a Bernoulli distribution with mean w̄(e). We borrow this assumption from the work of Kveton et al. [10] and it is critical to our results. We would face computational difficulties without it. Under this assumption, the expected reward of solution A ∈ Θ, the probability that the weight of each item in A is one, can be written as E [f(A,w)] = f(A, w̄), and depends only on the expected weights of individual items in A. It follows that:\nA∗ = arg maxA∈Θ f(A, w̄) .\nIn Section 4, we experiment with two problems that violate our independence assumption. We also discuss implications of this violation.\nSeveral interesting online learning problems can be formulated as combinatorial cascading bandits. Consider the problem of learning routing paths in Simple Mail Transfer Protocol (SMTP) that maximize the probability of e-mail delivery. The ground set in this problem are all links in the network and the feasible set are all routing paths. At time t, the learning agent chooses routing path At and observes if the e-mail is delivered. If the e-mail is not delivered, the agent observes the first link in the routing path which is down. This kind of information is available in SMTP. The weight of item e at time t is an indicator of link e being up at time t. The independence assumption in (2) requires that all links fail independently. This assumption is common in the existing network routing models [6]. We return to the problem of network routing in Section 4.2.\n2.2 CombCascade Algorithm\nOur proposed algorithm, CombCascade, is described in Algorithm 1. This algorithm belongs to the family of UCB algorithms. At time t, CombCascade operates in three stages. First, it computes the upper confidence bounds (UCBs) Ut ∈ [0, 1]E on the expected weights of all items in E. The UCB of item e at time t is defined as:\nUt(e) = min { ŵTt−1(e)(e) + ct−1,Tt−1(e), 1 } , (3)\nwhere ŵs(e) is the average of s observed weights of item e, Tt(e) is the number of times that item e is observed in t steps, and ct,s = √ (1.5 log t)/s is the radius of a confidence interval around ŵs(e) after t steps such that w̄(e) ∈ [ŵs(e) − ct,s, ŵs(e) + ct,s] holds with a high probability. After the UCBs are computed, CombCascade chooses the optimal solution with respect to these UCBs:\nAt = arg maxA∈Θ f(A,Ut) .\nFinally, CombCascade observes Ot and updates its estimates of the expected weights based on the weights of the observed items in (1), for all items atk such that k ≤ Ot. For simplicity of exposition, we assume that CombCascade is initialized by one sample w0 ∼ P . If w0 is unavailable, we can formulate the problem of obtaining w0 as an optimization problem on Θ with a linear objective [12]. The initialization procedure of Kveton et al. [12] tracks observed items and adaptively chooses solutions with the maximum number of unobserved items. This approach is computationally efficient on any feasible set Θ where a linear function can be optimized efficiently.\nCombCascade has two attractive properties. First, the algorithm is computationally efficient, in the sense that At = arg maxA∈Θ ∑ e∈A log(Ut(e)) is the problem of maximizing a linear function on\nAlgorithm 1 CombCascade for combinatorial cascading bandits. // Initialization Observe w0 ∼ P ∀e ∈ E : T0(e)← 1 ∀e ∈ E : ŵ1(e)← w0(e)\nfor all t = 1, . . . , n do // Compute UCBs ∀e ∈ E : Ut(e) = min { ŵTt−1(e)(e) + ct−1,Tt−1(e), 1 } // Solve the optimization problem and get feedback At ← arg maxA∈Θ f(A,Ut) Observe Ot ∈ {1, . . . , |At| ,+∞}\n// Update statistics ∀e ∈ E : Tt(e)← Tt−1(e) for all k = 1, . . . ,min {Ot, |At|} do e← atk Tt(e)← Tt(e) + 1\nŵTt(e)(e)← Tt−1(e)ŵTt−1(e)(e) + 1{k < Ot}\nTt(e)\nΘ. This problem can be solved efficiently for various feasible sets Θ, such as matroids, matchings, and paths. Second, CombCascade is sample efficient because the UCB of solution A, f(A,Ut), is a product of the UCBs of all items in A, which are estimated separately. The regret of CombCascade does not depend on |Θ| and is polynomial in all other quantities of interest."
    }, {
      "heading" : "2.3 Disjunctive Objective",
      "text" : "Our reward model is conjuctive, the reward is one if and only if the weights of all chosen items are one. A natural alternative is a disjunctive model rt = maxe∈At wt(e) = ∨ e∈At wt(e), the reward is one if the weight of any item in At is one. This model arises in recommender systems, where the recommender is rewarded when the user is satisfied with any recommended item. The feedback Ot is the index of the first item in At whose weight is one, as in cascading bandits [10].\nLet f∨ : Θ× [0, 1]E→ [0, 1] be a reward function, which is defined as f∨(A,w) = 1− ∏ e∈A(1− w(e)). Then under the independence assumption in (2), E [f∨(A,w)] = f∨(A, w̄) and:\nA∗ = arg max A∈Θ f∨(A, w̄) = arg min A∈Θ ∏ e∈A (1− w̄(e)) = arg min A∈Θ f(A, 1− w̄) .\nTherefore, A∗ can be learned by a variant of CombCascade where the observations are 1 −wt and each UCB Ut(e) is substituted with a lower confidence bound (LCB) on 1− w̄(e):\nLt(e) = max { 1− ŵTt−1(e)(e)− ct−1,Tt−1(e), 0 } .\nLet R(At,wt) = f(At, 1 −wt) − f(A∗, 1 −wt) be the instantaneous stochastic regret at time t. Then we can bound the regret of CombCascade as in Theorems 1 and 2. The only difference is that ∆e,min and f∗ are redefined as:\n∆e,min = minA∈Θ:e∈A,∆A>0 f(A, 1− w̄)− f(A∗, 1− w̄) , f∗ = f(A∗, 1− w̄) ."
    }, {
      "heading" : "3 Analysis",
      "text" : "We prove gap-dependent and gap-free upper bounds on the regret of CombCascade in Section 3.1. We discuss these bounds in Section 3.2."
    }, {
      "heading" : "3.1 Upper Bounds",
      "text" : "We define the suboptimality gap of solution A = (a1, . . . , a|A|) as ∆A = f(A∗, w̄)− f(A, w̄) and the probability that all items in A are observed as pA = ∏|A|−1 k=1 w̄(ak). For convenience, we define\nshorthands f∗= f(A∗, w̄) and p∗= pA∗ . Let Ẽ = E \\A∗ be the set of suboptimal items, the items that are not in A∗. Then the minimum gap associated with suboptimal item e ∈ Ẽ is:\n∆e,min = f(A ∗, w̄)−maxA∈Θ:e∈A,∆A>0 f(A, w̄) .\nLet K = max {|A| : A ∈ Θ} be the maximum number of items in any solution and f∗ > 0. Then the regret of CombCascade is bounded as follows.\nTheorem 1. The regret of CombCascade is bounded as R(n) ≤ K f∗ ∑ e∈Ẽ 4272 ∆e,min log n+ π2 3 L.\nProof. The proof is in Appendix A. The main idea is to reduce our analysis to that of CombUCB1 in stochastic combinatorial semi-bandits [12]. This reduction is challenging for two reasons. First, our reward function is non-linear in the weights of chosen items. Second, we only observe some of the chosen items.\nOur analysis can be trivially reduced to semi-bandits by conditioning on the event of observing all items. In particular, let Ht = (A1,O1, . . . ,At−1,Ot−1,At) be the history of CombCascade up to choosing solution At, the first t − 1 observations and t actions. Then we can express the expected regret at time t conditioned onHt as:\nE [R(At,wt) |Ht] = E [∆At(1/pAt)1{∆At > 0, Ot ≥ |At|} |Ht]\nand analyze our problem under the assumption that all items in At are observed. This reduction is problematic because the probability pAt can be low, and as a result we get a loose regret bound.\nWe address this issue by formalizing the following insight into our problem. When f(A, w̄) f∗, CombCascade can distinguish A from A∗ without learning the expected weights of all items in A. In particular, CombCascade acts implicitly on the prefixes of suboptimal solutions, and we choose them in our analysis such that the probability of observing all items in the prefixes is “close” to f∗, and the gaps are “close” to those of the original solutions.\nLemma 1. Let A = (a1, . . . , a|A|) ∈ Θ be a feasible solution and Bk = (a1, . . . , ak) be a prefix of k ≤ |A| items of A. Then k can be set such that ∆Bk ≥ 12∆A and pBk ≥ 1 2f ∗.\nThen we count the number of times that the prefixes can be chosen instead of A∗ when all items in the prefixes are observed. The last remaining issue is that f(A,Ut) is non-linear in the confidence radii of the items in A. Therefore, we bound it from above based on the following lemma.\nLemma 2. Let 0 ≤ p1, . . . , pK ≤ 1 and u1, . . . , uK ≥ 0. Then:∏K k=1 min {pk + uk, 1} ≤ ∏K k=1 pk + ∑K k=1 uk .\nThis bound is tight when p1, . . . , pK = 1 and u1, . . . , uK = 0.\nThe rest of our analysis is along the lines of Theorem 5 in Kveton et al. [12]. We can achieve linear dependency on K, in exchange for a multiplicative factor of 534 in our upper bound.\nWe also prove the following gap-free bound.\nTheorem 2. The regret of CombCascade is bounded as R(n) ≤ 131\n√ KLn log n\nf∗ + π2 3 L.\nProof. The proof is in Appendix B. The key idea is to decompose the regret of CombCascade into two parts, where the gaps ∆At are at most and larger than . We analyze each part separately and then set to get the desired result."
    }, {
      "heading" : "3.2 Discussion",
      "text" : "In Section 3.1, we prove two upper bounds on the n-step regret of CombCascade: Theorem 1: O(KL(1/f∗)(1/∆) log n) , Theorem 2: O( √ KL(1/f∗)n log n) ,\nwhere ∆ = mine∈Ẽ ∆e,min. These bounds do not depend on the total number of feasible solutions |Θ| and are polynomial in any other quantity of interest. The bounds match, up to O(1/f∗) factors,\nthe upper bounds of CombUCB1 in stochastic combinatorial semi-bandits [12]. Since CombCascade receives less feedback than CombUCB1, this is rather surprising and unexpected. The upper bounds of Kveton et al. [12] are known to be tight up to polylogarithmic factors. We believe that our upper bounds are also tight in the setting similar to Kveton et al. [12], where the expected weight of each item is close to 1 and likely to be observed.\nThe assumption that f∗ is large is often reasonable. In network routing, the optimal routing path is likely to be reliable. In recommender systems, the optimal recommended list often does not satisfy a reasonably large fraction of users."
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluate CombCascade in three experiments. In Section 4.1, we compare it to CombUCB1 [12], a state-of-the-art algorithm for stochastic combinatorial semi-bandits with a linear reward function. This experiment shows that CombUCB1 cannot solve all instances of our problem, which highlights the need for a new learning algorithm. It also shows the limitations of CombCascade. We evaluate CombCascade on two real-world problems in Sections 4.2 and 4.3."
    }, {
      "heading" : "4.1 Synthetic",
      "text" : "In the first experiment, we compare CombCascade to CombUCB1 [12] on a synthetic problem. This problem is a combinatorial cascading bandit with L = 4 items and Θ = {(1, 2), (3, 4)}. CombUCB1 is a popular algorithm for stochastic combinatorial semi-bandits with a linear reward function. We approximate maxA∈Θ f(A,w) by minA∈Θ ∑ e∈A(1 − w(e)). This approximation is motivated by\nthe fact that f(A,w) = ∏ e∈A w(e) ≈ 1− ∑ e∈A(1− w(e)) as mine∈E w(e)→ 1. We update the estimates of w̄ in CombUCB1 as in CombCascade, based on the weights of the observed items in (1).\nWe experiment with three different settings of w̄ and report our results in Figure 1. The settings of w̄ are reported in our plots. We assume that wt(e) are distributed independently, except for the last plot where wt(3) = wt(4). Our plots represent three common scenarios that we encountered in our experiments. In the first plot, arg maxA∈Θ f(A, w̄) = arg minA∈Θ ∑ e∈A(1− w̄(e)). In this case, both CombCascade and CombUCB1 can learn A∗. The regret of CombCascade is slightly lower than that of CombUCB1. In the second plot, arg maxA∈Θ f(A, w̄) 6= arg minA∈Θ ∑ e∈A(1 − w̄(e)). In this case, CombUCB1 cannot learn A∗ and therefore suffers linear regret. In the third plot, we violate our modeling assumptions. Perhaps surprisingly, CombCascade can still learn the optimal solution A∗, although it suffers higher regret than CombUCB1."
    }, {
      "heading" : "4.2 Network Routing",
      "text" : "In the second experiment, we evaluate CombCascade on a problem of network routing. We experiment with six networks from the RocketFuel dataset [17], which are described in Figure 2a.\nOur learning problem is formulated as follows. The ground set E are the links in the network. The feasible set Θ are all paths in the network. At time t, we generate a random pair of starting and end nodes, and the learning agent chooses a routing path between these nodes. The goal of the agent is to maximizes the probability that all links in the path are up. The feedback is the index of the first link in the path which is down. The weight of link e at time t, wt(e), is an indicator of link e being\nup at time t. We model wt(e) as an independent Bernoulli random variable wt(e) ∼ B(w̄(e)) with mean w̄(e) = 0.7 + 0.2 local(e), where local(e) is an indicator of link e being local. We say that the link is local when its expected latency is at most 1 millisecond. About a half of the links in our networks are local. To summarize, the local links are up with probability 0.9; and are more reliable than the global links, which are up only with probability 0.7.\nOur results are reported in Figure 2b. We observe that the n-step regret of CombCascade flattens as time n increases. This means that CombCascade learns near-optimal policies in all networks."
    }, {
      "heading" : "4.3 Diverse Recommendations",
      "text" : "In our last experiment, we evaluate CombCascade on a problem of diverse recommendations. This problem is motivated by on-demand media streaming services like Netflix, which often recommend groups of movies, such as “Popular on Netflix” and “Dramas”. We experiment with the MovieLens dataset [13] from March 2015. The dataset contains 138k people who assigned 20M ratings to 27k movies between January 1995 and March 2015.\nOur learning problem is formulated as follows. The ground set E are 200 movies from our dataset: 25 most rated animated movies, 75 random animated movies, 25 most rated non-animated movies, and 75 random non-animated movies. The feasible set Θ are all K-permutations of E where K/2 movies are animated. The weight of item e at time t, wt(e), indicates that item e attracts the user at time t. We assume that wt(e) = 1 if and only if the user rated item e in our dataset. This indicates that the user watched movie e at some point in time, perhaps because the movie was attractive. The user at time t is drawn randomly from our pool of users. The goal of the learning agent is to learn a list of items A∗ = arg maxA∈Θ E [f∨(A,w)] that maximizes the probability that at least one item is attractive. The feedback is the index of the first attractive item in the list (Section 2.3). We would like to point out that our modeling assumptions are violated in this experiment. In particular, wt(e) are correlated across items e because the users do not rate movies independently. The result is that A∗ 6= arg maxA∈Θ f∨(A, w̄). It is NP-hard to compute A∗. However, E [f∨(A,w)] is submodular and monotone in A, and therefore a (1 − 1/e) approximation to A∗ can be computed greedily. We denote this approximation by A∗ and show it for K = 8 in Figure 3a.\nOur results are reported in Figure 3b. Similarly to Figure 2b, the n-step regret of CombCascade is a concave function of time n for all studied K. This indicates that CombCascade solutions improve over time. We note that the regret does not flatten as in Figure 2b. The reason is that CombCascade does not learn A∗. Nevertheless, it performs well and we expect comparably good performance in other domains where our modeling assumptions are not satisfied. Our current theory cannot explain this behavior and we leave it for future work."
    }, {
      "heading" : "5 Related Work",
      "text" : "Our work generalizes cascading bandits of Kveton et al. [10] to arbitrary combinatorial constraints. The feasible set in cascading bandits is a uniform matroid, any list of K items out of L is feasible. Our generalization significantly expands the applicability of the original model and we demonstrate this on two novel real-world problems (Section 4). Our work also extends stochastic combinatorial semi-bandits with a linear reward function [8, 11, 12] to the cascade model of feedback. A similar model to cascading bandits was recently studied by Combes et al. [7].\nOur generalization is significant for two reasons. First, CombCascade is a novel learning algorithm. CombUCB1 [12] chooses solutions with the largest sum of the UCBs. CascadeUCB1 [10] chooses K items out of L with the largest UCBs. CombCascade chooses solutions with the largest product of the UCBs. All three algorithms can find the optimal solution in cascading bandits. However, when the feasible set is not a matroid, it is critical to maximize the product of the UCBs. CombUCB1 may learn a suboptimal solution in this setting and we illustrate this in Section 4.1.\nSecond, our analysis is novel. The proof of Theorem 1 is different from those of Theorems 2 and 3 in Kveton et al. [10]. These proofs are based on counting the number of times that each suboptimal item is chosen instead of any optimal item. They can be only applied to special feasible sets, such a matroid, because they require that the items in the feasible solutions are exchangeable. We build on the recent work of Kveton et al. [12] to achieve linear dependency on K in Theorem 1. The rest of our analysis is novel.\nOur problem is a partial monitoring problem where some of the chosen items may be unobserved. Agrawal et al. [1] and Bartok et al. [4] studied partial monitoring problems and proposed learning algorithms for solving them. These algorithms are impractical in our setting. As an example, if we formulate our problem as in Bartok et al. [4], we get |Θ| actions and 2L unobserved outcomes; and the learning algorithm reasons over |Θ|2 pairs of actions and requires O(2L) space. Lin et al. [15] also studied combinatorial partial monitoring. Their feedback is a linear function of the weights of chosen items. Our feedback is a non-linear function of the weights.\nOur reward function is non-linear in unknown parameters. Chen et al. [5] studied stochastic combinatorial semi-bandits with a non-linear reward function, which is a known monotone function of an unknown linear function. The feedback in Chen et al. [5] is semi-bandit, which is more informative than in our work. Le et al. [14] studied a network optimization problem where the reward function is a non-linear function of observations."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We propose combinatorial cascading bandits, a class of stochastic partial monitoring problems that can model many practical problems, such as learning of a routing path in an unreliable communication network that maximizes the probability of packet delivery, and learning to recommend a list of attractive items. We propose a practical UCB-like algorithm for our problems, CombCascade, and prove upper bounds on its regret. We evaluate CombCascade on two real-world problems and show that it performs well even when our modeling assumptions are violated.\nOur results and analysis apply to any combinatorial action set, and therefore are quite general. The strongest assumption in our work is that the weights of items are distributed independently of each other. This assumption is critical and hard to eliminate (Section 2.1). Nevertheless, it can be easily relaxed to conditional independence given the features of items, along the lines of Wen et al. [19]. We leave this for future work. From the theoretical point of view, we want to derive a lower bound on the n-step regret in combinatorial cascading bandits, and show that the factor of f∗ in Theorems 1 and 2 is intrinsic."
    }, {
      "heading" : "A Proof of Theorem 1",
      "text" : "Our proof has four main parts. In Appendix A.1, we bound the regret associated with the event that our high-probability confidence intervals do not hold. In Appendix A.2, we change counted events, from partially-observed suboptimal solutions to their fully-observed prefixes. In Appendix A.3, we bound the number of times that any suboptimal prefix can be chosen instead of the optimal solution A∗. In Appendix A.4, we apply the counting argument of Kveton et al. [12] and finish our proof.\nLet Rt = R(At,wt) be the stochastic regret of CombCascade at time t, where At and wt are the solution and the weights of the items at time t, respectively. Let:\nEt = { ∃e ∈ E s.t. ∣∣w̄(e)− ŵTt−1(e)(e)∣∣ ≥ ct−1,Tt−1(e)} be the event that w̄(e) is outside of the high-probability confidence interval around ŵTt−1(e)(e) for at least one item e ∈ E at time t; and let Et be the complement of event Et, the event that w̄(e) is in the high-probability confidence interval around ŵTt−1(e)(e) for all items e ∈ E at time t. Then we can decompose the expected regret of CombCascade as:\nR(n) = E [ n∑ t=1 1{Et}Rt ] + E [ n∑ t=1 1 { Et } Rt ] . (4)\nA.1 Confidence Intervals Fail\nThe first term in (4) is easy to bound because Rt is bounded and our confidence intervals hold with high probability. In particular, Hoeffding’s inequality yields that for any e, s, and t:\nP (|w̄(e)− ŵs(e)| ≥ ct,s) ≤ 2 exp[−3 log t] ,\nand therefore:\nE [ n∑ t=1 1{Et} ] ≤ ∑ e∈E n∑ t=1 t∑ s=1 P (|w̄(e)− ŵs(e)| ≥ ct,s)\n≤ 2 ∑ e∈E n∑ t=1 t∑ s=1 exp[−3 log t] ≤ 2 ∑ e∈E n∑ t=1 t−2 ≤ π 2 3 L .\nSince Rt ≤ 1, E [ ∑n t=1 1{Et}Rt] ≤ π2 3 L.\nA.2 From Partially-Observed Solutions to Fully-Observed Prefixes\nLet Ht = (A1,O1, . . . ,At−1,Ot−1,At) be the history of CombCascade up to choosing solution At, the first t− 1 observations and t actions. Let E [· |Ht] be the conditional expectation given this history. Then we can rewrite the expected regret at time t conditioned onHt as:\nE [Rt |Ht] = E [∆At1{∆At > 0} |Ht] = E [\n∆At pAt\n1{∆At > 0, Ot ≥ |At|} ∣∣∣∣Ht]\nand analyze our problem under the assumption that all items in At are observed. This reduction is problematic because the probability pAt can be low, and as a result we get a loose regret bound. To address this problem, we introduce the notion of prefixes.\nLet A = (a1, . . . , a|A|). Then B = (a1, . . . , ak) is a prefix of A for any k ≤ |A|. In the rest of our analysis, we treat prefixes as feasible solutions to our original problem. Let Bt be a prefix of At as defined in Lemma 1. Then ∆Bt ≥ 12∆At and pBt ≥ 1 2f ∗, and we can bound the expected regret at time t conditioned onHt as:\nE [Rt |Ht] = E [\n∆At pBt\n1{∆At > 0, Ot ≥ |Bt|} ∣∣∣∣Ht]\n≤ 4 f∗ E [∆Bt1{∆Bt > 0, Ot ≥ |Bt|} |Ht] . (5)\nNow we bound the second term in (4):\nE [ n∑ t=1 1 { Et } Rt ] (a) = n∑ t=1 E [ 1 { Et } E [Rt |Ht] ] (b) ≤ 4 f∗ E [ n∑ t=1 ∆Bt1 { Et, ∆Bt > 0, Ot ≥ |Bt| }] . (6)\nEquality (a) is due to the tower rule and that 1 { Et }\nis only a function of Ht. Inequality (b) follows from the upper bound in (5).\nA.3 Counting Suboptimal Prefixes\nLet:\nFt = 2 ∑ e∈B̃t cn,Tt−1(e) ≥ ∆Bt , ∆Bt > 0, Ot ≥ |Bt|  (7) be the event that suboptimal prefix Bt is “hard to distinguish” from A∗, where B̃t = Bt \\A∗ is the set of suboptimal items in Bt. The goal of this section is to bound (6) by a function of Ft.\nWe bound ∆Bt1 { Et, ∆Bt > 0, Ot ≥ |Bt| } from above for any suboptimal prefix Bt. Our bound is proved based on several facts. First, Bt is a prefix of At, and hence f(Bt,Ut) ≥ f(At,Ut) for any Ut. Second, when CombCascade chooses At, f(At,Ut) ≥ f(A∗,Ut). It follows that:∏\ne∈Bt Ut(e) = f(Bt,Ut) ≥ f(At,Ut) ≥ f(A∗,Ut) = ∏ e∈A∗ Ut(e) .\nNow we divide both sides by ∏ e∈A∗∩Bt Ut(e):∏\ne∈B̃t\nUt(e) ≥ ∏\ne∈A∗\\Bt\nUt(e)\nand substitute the definitions of the UCBs from (3):∏ e∈B̃t min { ŵTt−1(e)(e) + ct−1,Tt−1(e), 1 } ≥ ∏ e∈A∗\\Bt min { ŵTt−1(e)(e) + ct−1,Tt−1(e), 1 } .\nSince Et happens, ∣∣w̄(e)− ŵTt−1(e)(e)∣∣ < ct−1,Tt−1(e) for all e ∈ E and therefore:∏\ne∈A∗\\Bt\nmin { ŵTt−1(e)(e) + ct−1,Tt−1(e), 1 } ≥ ∏ e∈A∗\\Bt w̄(e)\n∏ e∈B̃t min { ŵTt−1(e)(e) + ct−1,Tt−1(e), 1 } ≤ ∏ e∈B̃t min { w̄(e) + 2ct−1,Tt−1(e), 1 } .\nBy Lemma 2:∏ e∈B̃t min { w̄(e) + 2ct−1,Tt−1(e), 1 } ≤ ∏ e∈B̃t w̄(e) + 2 ∑ e∈B̃t ct−1,Tt−1(e) . Finally, we chain the last four inequalities and get:∏ e∈B̃t w̄(e) + 2 ∑ e∈B̃t ct−1,Tt−1(e) ≥ ∏ e∈A∗\\Bt w̄(e) ,\nwhich further implies that: 2 ∑ e∈B̃t ct−1,Tt−1(e) ≥ ∏ e∈A∗\\Bt w̄(e)− ∏ e∈B̃t w̄(e)\n≥ ∏\ne∈A∗∩Bt w̄(e)︸ ︷︷ ︸ ≤1\n ∏ e∈A∗\\Bt w̄(e)− ∏ e∈B̃t w̄(e)  = ∆Bt .\nSince cn,Tt−1(e) ≥ ct−1,Tt−1(e) for any time t ≤ n, the event Ft in (7) happens. Therefore, we can bound the right-hand side in (6) as:\nE [ n∑ t=1 ∆Bt1 { Et, ∆Bt > 0, Ot ≥ |Bt| }] ≤ E [ R̂(n) ] ,\nwhere:\nR̂(n) = n∑ t=1 ∆Bt1{Ft} . (8)\nA.4 CombUCB1 Analysis of Kveton et al. [12]\nIt remains to bound R̂(n) in (8). Note that the event Ft can happen only if the weights of all items in Bt are observed. As a result, R̂(n) can be bounded as in stochastic combinatorial semi-bandits. The key idea of our proof is to introduce infinitely-many mutually-exclusive events and then bound the number of times that these events happen when a suboptimal prefix is chosen [12]. The event i at time t is:\nGi,t = {less than β1K items in B̃t were observed at most α1 K2\n∆2Bt log n times,\n. . . ,\nless than βi−1K items in B̃t were observed at most αi−1 K2\n∆2Bt log n times,\nat least βiK items in B̃t were observed at most αi K2\n∆2Bt log n times,\nOt ≥ |Bt|} , where we assume that ∆Bt > 0; and the constants (αi) and (βi) are defined as:\n1 = β0 > β1 > β2 > . . . > βk > . . .\nα1 > α2 > . . . > αk > . . . ,\nand satisfy limi→∞ αi = limi→∞ βi = 0. By Lemma 3 of Kveton et al. [12], Gi,t are exhaustive at any time t when (αi) and (βi) satisfy:\n√ 6 ∞∑ i=1 βi−1 − βi√ αi ≤ 1 .\nIn this case:\nR̂(n) = n∑ t=1 ∆Bt1{Ft} = ∞∑ i=1 n∑ t=1 ∆Bt1{Gi,t, ∆Bt > 0} .\nNow we introduce item-specific variants of events Gi,t and associate the regret at time t with these events. In particular, let:\nGe,i,t = Gi,t ∩ { e ∈ B̃t, Tt−1(e) ≤ αi K2\n∆2Bt log n } be the event that item e is not observed “sufficiently often” under event Gi,t. Then it follows that:\n1{Gi,t, ∆Bt > 0} ≤ 1\nβiK ∑ e∈Ẽ 1{Ge,i,t, ∆Bt > 0}\nbecause at least βiK items are not observed “sufficiently often” under event Gi,t. Therefore, we can bound R̂(n) as:\nR̂(n) ≤ ∑ e∈Ẽ ∞∑ i=1 n∑ t=1 1{Ge,i,t, ∆Bt > 0} ∆Bt βiK .\nLet each item e be in Ne suboptimal prefixes and ∆e,1 ≥ . . . ≥ ∆e,Ne be the gaps of these prefixes, ordered from the largest gap to the smallest. Then R̂(n) can be further bounded as:\nR̂(n) ≤ ∑ e∈Ẽ ∞∑ i=1 n∑ t=1 Ne∑ k=1 1{Ge,i,t, ∆Bt = ∆e,k} ∆e,k βiK\n(a) ≤ ∑ e∈Ẽ ∞∑ i=1 n∑ t=1 Ne∑ k=1 1\n{ e ∈ B̃t, Tt−1(e) ≤ αi K2\n∆2e,k log n, ∆Bt = ∆e,k, Ot ≥ |Bt| } ∆e,k βiK\n(b) ≤ ∑ e∈Ẽ ∞∑ i=1 αiK log n βi\n[ ∆e,1 1\n∆2e,1 + Ne∑ k=2 ∆e,k\n( 1\n∆2e,k − 1 ∆2e,k−1 )] (c) < ∑ e∈Ẽ ∞∑ i=1 αiK log n βi 2 ∆e,Ne\n= ∑ e∈Ẽ K 2 ∆e,Ne [ ∞∑ i=1 αi βi ] log n ,\nwhere inequality (a) follows from the definition of Ge,i,t and inequality (b) is from solving:\nmax A1:n,O1:n n∑ t=1 Ne∑ k=1 1 { e ∈ B̃t, TA1:n,O1:nt−1 (e) ≤ αi K2 ∆2e,k log n, ∆Bt = ∆e,k, Ot ≥ |Bt| } ∆e,k βiK ,\nwhere A1:n = (A1, . . . , An) is a sequence of n solutions, O1:n = (O1, . . . , On) is a sequence of n observations, TA1:n,O1:nt (e) is the number of times that item e is observed in t steps under A1:n and O1:n, Bt is the prefix of At as defined in Lemma 1, and B̃t = Bt \\A∗. Inequality (c) is by Lemma 3 of Kveton et al. [11]:[\n∆e,1 1\n∆2e,1 + Ne∑ k=2 ∆e,k\n( 1\n∆2e,k − 1 ∆2e,k−1\n)] < 2\n∆e,Ne .\nFor the same (αi) and (βi) as in Theorem 4 of Kveton et al. [12], ∑∞ i=1 αi βi < 267. Moreover, since ∆Bt ≥ 12∆At for any solution At and its prefix Bt, we have ∆e,Ne ≥ 1 2∆e,min. Now we chain all inequalities and get:\nR(n) ≤ 4 f∗\nE [ R̂(n) ] + π2\n3 L ≤ K f∗ ∑ e∈Ẽ 4272 ∆e,min log n+ π2 3 L ."
    }, {
      "heading" : "B Proof of Theorem 2",
      "text" : "The key idea is to decompose the regret of CombCascade into two parts, where the gaps ∆At are at most and larger than . In particular, note that for any > 0:\nR(n) = E [ n∑ t=1 1{∆At ≤ ε}Rt ] + E [ n∑ t=1 1{∆At > ε}Rt ] . (9)\nThe first term in (9) can be bounded trivially as:\nE [ n∑ t=1 1{∆At ≤ ε}Rt ] = E [ n∑ t=1 ∆At1{∆At ≤ ε, ∆At > 0} ] ≤ n\nbecause ∆At ≤ ε. The second term in (9) can be bounded in the same way as R(n) in Theorem 1. The only difference is that ∆e,min ≥ for all e ∈ Ẽ. Therefore:\nE [ n∑ t=1 1{∆At > ε}Rt ] ≤ K f∗ ∑ e∈Ẽ 4272 ∆e,min log n+ π2 3 L ≤ 4272KL f∗ log n+ π2 3 L .\nNow we chain all inequalities and get:\nR(n) ≤ 4272KL f∗ log n+ n+ π2 3 L .\nFinally, we choose =\n√ 4272KL log n\nf∗n and get:\nR(n) ≤ 2 √ 4272\n√ KLn log n\nf∗ + π2 3 L < 131\n√ KLn log n\nf∗ + π2 3 L ,\nwhich concludes our proof."
    }, {
      "heading" : "C Technical Lemmas",
      "text" : "Lemma 1. Let A = (a1, . . . , a|A|) ∈ Θ be a feasible solution and Bk = (a1, . . . , ak) be a prefix of k ≤ |A| items of A. Then k can be set such that ∆Bk ≥ 12∆A and pBk ≥ 1 2f ∗.\nProof. We consider two cases. First, suppose that f(A, w̄) ≥ 12f ∗. Then our claims hold trivially for k = |A|. Now suppose that f(A, w̄) < 12f ∗. Then we choose k such that:\nf(Bk, w̄) ≤ 1\n2 f∗ ≤ pBk .\nSuch k is guaranteed to exist because ⋃|A| i=1[f(Bi, w̄), pBi ] = [f(A, w̄), 1], which follows from the facts that f(Bi, w̄) = pBiw̄(ai) for any i ≤ |A| and pB1 = 1. We prove that ∆Bk ≥ 12∆A as:\n∆Bk = f ∗ − f(Bk, w̄) ≥\n1 2 f∗ ≥ 1 2 ∆A .\nThe first inequality is by our assumption and the second one holds for any solution A.\nLemma 2. Let 0 ≤ p1, . . . , pK ≤ 1 and u1, . . . , uK ≥ 0. Then: K∏ k=1 min {pk + uk, 1} ≤ K∏ k=1 pk + K∑ k=1 uk .\nThis bound is tight when p1, . . . , pK = 1 and u1, . . . , uK = 0.\nProof. The proof is by induction on K. Our claim clearly holds when K = 1. Now choose K > 1 and suppose that our claim holds for any 0 ≤ p1, . . . , pK−1 ≤ 1 and u1, . . . , uK−1 ≥ 0. Then:\nK∏ k=1 min {pk + uk, 1} = min {pK + uK , 1} K−1∏ k=1 min {pk + uk, 1}\n≤ min {pK + uK , 1} ( K−1∏ k=1 pk + K−1∑ k=1 uk )\n≤ pK K−1∏ k=1 pk + uK K−1∏ k=1\npk︸ ︷︷ ︸ ≤1\n+ min {pK + uK , 1}︸ ︷︷ ︸ ≤1 K−1∑ k=1 uk\n≤ K∏ k=1 pk + K∑ k=1 uk ."
    } ],
    "references" : [ {
      "title" : "Asymptotically efficient adaptive allocation schemes for controlled i.i.d. processes: Finite parameter space",
      "author" : [ "Rajeev Agrawal", "Demosthenis Teneketzis", "Venkatachalam Anantharam" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1989
    }, {
      "title" : "Analysis of Thompson sampling for the multi-armed bandit problem",
      "author" : [ "Shipra Agrawal", "Navin Goyal" ],
      "venue" : "In Proceeding of the 25th Annual Conference on Learning Theory,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2002
    }, {
      "title" : "An adaptive algorithm for finite stochastic partial monitoring",
      "author" : [ "Gabor Bartok", "Navid Zolghadr", "Csaba Szepesvari" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Combinatorial multi-armed bandit: General framework, results and applications",
      "author" : [ "Wei Chen", "Yajun Wang", "Yang Yuan" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Analysis of point-to-point packet delay in an operational network",
      "author" : [ "Baek-Young Choi", "Sue Moon", "Zhi-Li Zhang", "Konstantina Papagiannaki", "Christophe Diot" ],
      "venue" : "In Proceedings of the 23rd Annual Joint Conference of the IEEE Computer and Communications Societies,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "Learning to rank: Regret lower bounds and efficient algorithms",
      "author" : [ "Richard Combes", "Stefan Magureanu", "Alexandre Proutiere", "Cyrille Laroche" ],
      "venue" : "In Proceedings of the 2015 ACM SIGMET- RICS International Conference on Measurement and Modeling of Computer Systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations",
      "author" : [ "Yi Gai", "Bhaskar Krishnamachari", "Rahul Jain" ],
      "venue" : "IEEE/ACM Transactions on Networking,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "The KL-UCB algorithm for bounded stochastic bandits and beyond",
      "author" : [ "Aurelien Garivier", "Olivier Cappe" ],
      "venue" : "In Proceeding of the 24th Annual Conference on Learning Theory,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Cascading bandits: Learning to rank in the cascade model",
      "author" : [ "Branislav Kveton", "Csaba Szepesvari", "Zheng Wen", "Azin Ashkan" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Matroid bandits: Fast combinatorial optimization with learning",
      "author" : [ "Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Hoda Eydgahi", "Brian Eriksson" ],
      "venue" : "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Tight regret bounds for stochastic combinatorial semi-bandits",
      "author" : [ "Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesvari" ],
      "venue" : "In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Sequential learning for multi-channel wireless network monitoring with channel switching costs",
      "author" : [ "Thanh Le", "Csaba Szepesvari", "Rong Zheng" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Combinatorial partial monitoring game with linear feedback and its applications",
      "author" : [ "Tian Lin", "Bruno Abrahao", "Robert Kleinberg", "John Lui", "Wei Chen" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Combinatorial Optimization",
      "author" : [ "Christos Papadimitriou", "Kenneth Steiglitz" ],
      "venue" : "Dover Publications, Mineola, NY,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1998
    }, {
      "title" : "Measuring ISP topologies with Rocketfuel",
      "author" : [ "Neil Spring", "Ratul Mahajan", "David Wetherall" ],
      "venue" : "IEEE / ACM Transactions on Networking,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "William. R. Thompson" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1933
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Combinatorial optimization [16] has many real-world applications.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "This feedback model was recently proposed in the so-called cascading bandits [10].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "Stochastic online learning with combinatorial actions has been previously studied with semi-bandit feedback and a linear reward function [8, 11, 12], and its monotone transformation [5].",
      "startOffset" : 137,
      "endOffset" : 148
    }, {
      "referenceID" : 10,
      "context" : "Stochastic online learning with combinatorial actions has been previously studied with semi-bandit feedback and a linear reward function [8, 11, 12], and its monotone transformation [5].",
      "startOffset" : 137,
      "endOffset" : 148
    }, {
      "referenceID" : 11,
      "context" : "Stochastic online learning with combinatorial actions has been previously studied with semi-bandit feedback and a linear reward function [8, 11, 12], and its monotone transformation [5].",
      "startOffset" : 137,
      "endOffset" : 148
    }, {
      "referenceID" : 4,
      "context" : "Stochastic online learning with combinatorial actions has been previously studied with semi-bandit feedback and a linear reward function [8, 11, 12], and its monotone transformation [5].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 2,
      "context" : "Established algorithms for multi-armed bandits, such as UCB1 [3], KL-UCB [9], and Thompson sampling [18, 2]; can be usually easily adapted to stochastic combinatorial semi-bandits.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "Established algorithms for multi-armed bandits, such as UCB1 [3], KL-UCB [9], and Thompson sampling [18, 2]; can be usually easily adapted to stochastic combinatorial semi-bandits.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "Established algorithms for multi-armed bandits, such as UCB1 [3], KL-UCB [9], and Thompson sampling [18, 2]; can be usually easily adapted to stochastic combinatorial semi-bandits.",
      "startOffset" : 100,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "Established algorithms for multi-armed bandits, such as UCB1 [3], KL-UCB [9], and Thompson sampling [18, 2]; can be usually easily adapted to stochastic combinatorial semi-bandits.",
      "startOffset" : 100,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "[12] recently showed this for CombUCB1, a form of UCB1.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12, 10].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 9,
      "context" : "[12, 10].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 7,
      "context" : "We also show that CombUCB1 [8, 12] cannot solve some instances of our problem, which highlights the need for a new learning algorithm.",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "We also show that CombUCB1 [8, 12] cannot solve some instances of our problem, which highlights the need for a new learning algorithm.",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "The key step in our solution and its analysis is that the reward can be expressed as rt = f(At,wt), where f : Θ× [0, 1] → [0, 1] is a reward function, which is defined as: f(A,w) = ∏",
      "startOffset" : 113,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "The key step in our solution and its analysis is that the reward can be expressed as rt = f(At,wt), where f : Θ× [0, 1] → [0, 1] is a reward function, which is defined as: f(A,w) = ∏",
      "startOffset" : 122,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "e∈A w(e) , A ∈ Θ , w ∈ [0, 1] .",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : "[10] and it is critical to our results.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "This assumption is common in the existing network routing models [6].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "First, it computes the upper confidence bounds (UCBs) Ut ∈ [0, 1] on the expected weights of all items in E.",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "If w0 is unavailable, we can formulate the problem of obtaining w0 as an optimization problem on Θ with a linear objective [12].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 11,
      "context" : "[12] tracks observed items and adaptively chooses solutions with the maximum number of unobserved items.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "The feedback Ot is the index of the first item in At whose weight is one, as in cascading bandits [10].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "Let f∨ : Θ× [0, 1]→ [0, 1] be a reward function, which is defined as f∨(A,w) = 1− ∏ e∈A(1− w(e)).",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "Let f∨ : Θ× [0, 1]→ [0, 1] be a reward function, which is defined as f∨(A,w) = 1− ∏ e∈A(1− w(e)).",
      "startOffset" : 20,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "The main idea is to reduce our analysis to that of CombUCB1 in stochastic combinatorial semi-bandits [12].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "[12].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "the upper bounds of CombUCB1 in stochastic combinatorial semi-bandits [12].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "[12] are known to be tight up to polylogarithmic factors.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12], where the expected weight of each item is close to 1 and likely to be observed.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "1, we compare it to CombUCB1 [12], a state-of-the-art algorithm for stochastic combinatorial semi-bandits with a linear reward function.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "1 Synthetic In the first experiment, we compare CombCascade to CombUCB1 [12] on a synthetic problem.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : "We experiment with six networks from the RocketFuel dataset [17], which are described in Figure 2a.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "[10] to arbitrary combinatorial constraints.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "Our work also extends stochastic combinatorial semi-bandits with a linear reward function [8, 11, 12] to the cascade model of feedback.",
      "startOffset" : 90,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "Our work also extends stochastic combinatorial semi-bandits with a linear reward function [8, 11, 12] to the cascade model of feedback.",
      "startOffset" : 90,
      "endOffset" : 101
    }, {
      "referenceID" : 11,
      "context" : "Our work also extends stochastic combinatorial semi-bandits with a linear reward function [8, 11, 12] to the cascade model of feedback.",
      "startOffset" : 90,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "[7].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "CombUCB1 [12] chooses solutions with the largest sum of the UCBs.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 9,
      "context" : "CascadeUCB1 [10] chooses K items out of L with the largest UCBs.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 9,
      "context" : "[10].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] to achieve linear dependency on K in Theorem 1.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "[1] and Bartok et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] studied partial monitoring problems and proposed learning algorithms for solving them.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4], we get |Θ| actions and 2 unobserved outcomes; and the learning algorithm reasons over |Θ| pairs of actions and requires O(2) space.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "[15] also studied combinatorial partial monitoring.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[5] studied stochastic combinatorial semi-bandits with a non-linear reward function, which is a known monotone function of an unknown linear function.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] is semi-bandit, which is more informative than in our work.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 12,
      "context" : "[14] studied a network optimization problem where the reward function is a non-linear function of observations.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "References [1] Rajeev Agrawal, Demosthenis Teneketzis, and Venkatachalam Anantharam.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2] Shipra Agrawal and Navin Goyal.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Gabor Bartok, Navid Zolghadr, and Csaba Szepesvari.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Wei Chen, Yajun Wang, and Yang Yuan.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Baek-Young Choi, Sue Moon, Zhi-Li Zhang, Konstantina Papagiannaki, and Christophe Diot.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Richard Combes, Stefan Magureanu, Alexandre Proutiere, and Cyrille Laroche.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Yi Gai, Bhaskar Krishnamachari, and Rahul Jain.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Aurelien Garivier and Olivier Cappe.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[14] Thanh Le, Csaba Szepesvari, and Rong Zheng.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[15] Tian Lin, Bruno Abrahao, Robert Kleinberg, John Lui, and Wei Chen.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[16] Christos Papadimitriou and Kenneth Steiglitz.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[17] Neil Spring, Ratul Mahajan, and David Wetherall.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[18] William.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] and finish our proof.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] It remains to bound R̂(n) in (8).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "The key idea of our proof is to introduce infinitely-many mutually-exclusive events and then bound the number of times that these events happen when a suboptimal prefix is chosen [12].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 11,
      "context" : "[12], Gi,t are exhaustive at any time t when (αi) and (βi) satisfy: √ 6 ∞ ∑",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11]: [ ∆e,1 1 ∆e,1 + Ne ∑",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12], ∑∞ i=1 αi βi < 267.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2015,
    "abstractText" : "We propose combinatorial cascading bandits, a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one. The weights of the items are binary, stochastic, and drawn independently of each other. The agent observes the index of the first chosen item whose weight is zero. This observation model arises in network routing, for instance, where the learning agent may only observe the first link in the routing path which is down, and blocks the path. We propose a UCB-like algorithm for solving our problems, CombCascade; and prove gap-dependent and gap-free upper bounds on its n-step regret. Our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting, a non-linear reward function and partial observability. We evaluate CombCascade on two real-world problems and show that it performs well even when our modeling assumptions are violated. We also demonstrate that our setting requires a new learning algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}