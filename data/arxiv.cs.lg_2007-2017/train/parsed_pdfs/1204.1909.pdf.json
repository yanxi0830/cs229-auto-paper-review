{
  "name" : "1204.1909.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Knapsack based Optimal Policies for Budget–Limited Multi–Armed Bandits",
    "authors" : [ "Long Tran–Thanh", "Archie Chapman", "Alex Rogers", "Nicholas R. Jennings" ],
    "emails" : [ "ltt08r@ecs.soton.ac.uk." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 4.\n19 09\nv1 [\ncs .A\nI] 9\nA pr\n2 01"
    }, {
      "heading" : "1 Introduction",
      "text" : "The standard multi–armed bandit (MAB) problem was originally proposed by Robbins (1952), and presents one of the clearest examples of the trade–off between exploration and exploitation in reinforcement learning. In the standard MAB problem, there are K arms of a single machine, each of which delivers rewards that are independently drawn from an unknown distribution when an arm of the machine is pulled. Given this, an agent must choose which of these arms to pull. At each time step, it pulls one of the machine’s arms and receives a reward or payoff. The agent’s goal is to maximise its return; that is, the expected sum of the rewards its receives over a sequence of pulls. As the reward distributions differ from arm to arm, the goal is to find the arm with the highest expected payoff as early as possible, and then to keep playing using that best arm. However, the agent does not know the rewards for the arms, so it must sample them in order to learn which is the optimal one. In other words, in order to choose the optimal arm (exploitation) the agent first has to estimate the mean rewards of all of the arms (exploration). In the standard MAB, this trade–off has been effectively balanced by decision–making policies such as upper confidence bound (UCB) and ǫn–greedy (Auer et al., 2002).\n∗School of Electronics and Computer Science, University of Southampton, UK. Con-\ntact:ltt08r@ecs.soton.ac.uk. †The University of Sydney Business School Sydney, Australia.\nHowever, this MAB model gives an incomplete description of the sequential decision–making problem facing an agent in many real–world scenarios. To this end, a variety of other related models have been studied recently, and, in particular, a number of researchers have focused on MABs with budget constraints, where arm–pulling is costly and is limited by a fixed budget (Antos et al., 2008; Bubeck et al., 2009; Guha and Munagala, 2007). In these models, the agent’s exploration budget limits the number of times it can sample the arms in order to estimate their rewards, which defines an initial exploration phase. In the subsequent cost–free exploitation phase, an agent’s policy is then simply to pull the arm with the highest expected reward. However, in many settings, it is not only the exploration phase, but the exploitation phase that is also limited by a cost budget. To address this limitation, a new bandit model, the budget–limited MAB, was introduced (Tran-Thanh et al. 2010). In this model, pulling an arm is again costly, but crucially both the exploration and exploitation phases are limited by a single budget. This type of limitation is well motivated by several real–world applications. For example, in many wireless sensor network applications, a sensor node’s actions, such as sampling or data forwarding, consume energy, and therefore the number of actions is limited by the capacity of the sensor’s batteries (Padhy et al. 2010). Furthermore, many of these scenarios require that sensors learn the optimal sequence of actions that can be performed, with the goal of maximising the long term value of the actions they take (Tran-Thanh et al., 2011). In such settings, each action can be considered as an arm, with a cost equal to the amount of energy needed to perform that task. Now, because the battery is limited, both the exploration (i.e. learning the rewards tasks) and exploitation (i.e. taking the optimal actions given reward estimates) phases are budget limited.\nAgainst this background, Tran-Thanh et al. (2010) showed that the budget– limited MAB cannot be derived from any other existing MAB model, and therefore, previous MAB learning methods are not suitable to efficiently deal with this problem. Thus, they proposed a simple budget–limited ε–first approach for the budget–limited MAB. This splits the overall budget B into two portions, the first εB of which is used for exploration, and the remaining (1 − ε)B for exploitation. However, this budget–limited ε–first method suffers from a number of drawbacks. First, the performance of ε–first approaches depend on the value of ε chosen. In particular, high values guarantee accurate exploration but inefficient exploitation, and vice versa. Given this, finding a suitable ε for a particular problem instance is a challenge, since settings with different budget limits or arm costs (which are not known beforehand) will typically require different values of ε. In addition, even with a good ε value, the method typically provides poor efficiency in terms of minimising its performance regret (defined as the difference between its performance and that of the optimal policy), which is a standard measure of performance. In particular, the regret bound that ε– first provides is O ( B 2 3 ) , where B is the budget limit, whereas the theoretical\nbest possible regret bound is typically a logarithmic function of the number of pulls1 (Lai and Robbins, 1985).\nTo address this shortcoming, in this paper we propose two new learning algorithms, called KUBE (for knapsack–based upper confidence bound explo-\n1 Note that in the budget–limited MAB, the budget B determines the number of pulls. Thus, a logarithmic function of the number of pulls is also a logarithmic function of the budget.\nration and exploitation) and fractional KUBE, that do not explicitly separate exploration from exploitation. Instead, they explore and exploit at the same time by adaptively choosing which arm to pull next, based on the current estimates of the arms’ rewards. In more detail, at each time step, KUBE calculates the best set of arms that provides the highest total upper confidence bound of the estimated expected reward, and still fits into the residual budget, using an unbounded knapsack model to determine this best set (Kellerer et al., 2004). However, since unbounded knapsack problems are known to be NP–hard, the algorithm uses an efficient approximation method taken from the knapsack literature, called the density–ordered greedy approach, in order to estimate the best set (Kohli et al., 2004). Following this, KUBE then uses the frequency that each arm occurs within this approximated best set as a probability with which to randomly choose an arm to pull in the next time step. The reward that is received is then used to update the estimate of the upper confidence bound of the pulled arm’s expected reward, and the unbounded knapsack problem is solved again. The intuition behind this algorithm is that if we know the real value of the arms, then the budget–limited MAB can be reduced to an unbounded knapsack problem, where the optimal solution is to subsequently pull from the set of arms that forms the solution of the knapsack problem. Given this, by randomly choosing the next arm from the current best set at each time step, the agent generates an accurate estimate of the true optimal solution (i.e. real best set of arms), and, accordingly, the sequence of pulled arms will converge to this optimal set. In a similar vein, fractional KUBE also estimates the best set of arms that provides the highest total upper confidence bound of the estimated expected reward at each time step, and uses the frequency that each arm occurs within this approximated best set as a probability to randomly pull the arms. However, instead of using the density–ordered greedy to solve the underlying unbounded knapsack problem, fractional KUBE relies on a computationally less expensive approach, namely the fractional relaxation based algorithm (Kellerer et al., 2004). Given this, fractional KUBE requires less computation than KUBE.\nTo analyse the performance of KUBE and its fractional counterpart in terms of minimising the regret, we devise proveably asymptotically optimal upper bounds on their performance regret. That is, our proposed upper bounds differ from the best possible one only with a constant factor. Following this, we numerically evaluate the performance of the proposed algorithms against a state–of–the–art method, namely the buget–limited ε–first approach, in order to demonstrate that our algorithms are the first that can achieve this optimal bound. In addition, we show that KUBE typically outperforms its fractional counterpart by up to 40%, however, this results in an increased computational cost (from O (K) to O (K lnK)). Given this, the main contributions of this paper are:\n• We introduce KUBE and fractional KUBE, the first budget–limited MAB learning algorithms that proveably achieve a O (lnB) theoretical upper bound on the regret, where B is the budget limit.\n• We demonstrate that with an increased computational cost, KUBE outperforms fractional KUBE in the experiments. We also show that while both algorithms achieve logarithmic regret bounds, the buget–limited ε– first approaches fail to do so.\nThe paper is organised as follows: Next we describe the budget–limited MAB. We then introduce our two learning algorithms in Section 3. In Section 4 we provide regret bounds on the performance of the proposed algorithms. Following this, Section 5 presents an empirical comparison of KUBE and its fractional counterpart with the ε–first approach. Section 6 concludes."
    }, {
      "heading" : "2 Model Description",
      "text" : "The budget–limited MAB model consists of a machine with K arms, one of which must be pulled by the agent at each time step. By pulling arm i, the agent has to pay a pulling cost, denoted with ci, and receives a non–negative reward drawn from a distribution associated with that specific arm. The agent has a cost budget B, which it cannot exceed during its operation time (i.e. the total cost of pulling arms cannot exceed this budget limit). Now, since reward values are typically bounded in real–world applications, we assume that each arm’s reward distribution has bounded supports. Let µi denote the mean value of the rewards that the agent receives from pulling arm i. Within our model, the agent’s goal is to maximise the sum of rewards it earns from pulling the arms of the machine, with respect to the budget B. However, the agent has no initial knowledge of the µi of each arm i, so it must learn these values in order to deduce a policy that maximises its sum of rewards. Given this, our objective is to find the optimal pulling algorithm, which maximises the expectation of the total reward that the agent can achieve, without exceeding the cost budget B.\nFormally, let A be an arm–pulling algorithm, giving a finite sequence of pulls. Let NAi (B) be the random variable that represents the number of pulls of arm i by A, with respect to the budget limit B. Since the total cost of the sequence A cannot exceed B, we have:\nP\n(\nK ∑\ni\nNAi (B) ci ≤ B\n)\n= 1. (1)\nLet G (A) be the total reward earned by using A to pull the arms. The expectation of G (A) is:\nE [G (A)] =\nK ∑\ni\nE [ NAi (B) ] µi. (2)\nThen, let A∗ denote an optimal solution that maximises the expected total reward, that is:\nA∗ = argmax A\nK ∑\ni\nE [ NAi (B) ] µi. (3)\nNote that in order to determine A∗, we have to know the value of µi in advance, which does not hold in our case. Thus, A∗ represents a theoretical optimum value, which is unachievable in general.\nNevertheless, for any algorithm A, we can define the regret for A as the difference between the expected cumulative reward for A and that of the theoretical optimum A∗. More precisely, letting R (A) denote the regret, we have:\nR (A) = E [G (A∗)]−E [G (A)] . (4)\nGiven this, our objective is to derive a method of generating a sequence of arm pulls that minimises this regret for the class of MAB problems defined above."
    }, {
      "heading" : "3 The Algorithms",
      "text" : "Given the model described in the previous section, we now introduce two learning methods, KUBE and fractional KUBE, that efficiently deal with the challenges discussed in Section 1. Recall that at each time step of the algorithms, we determine the optimal set of arms that provides the best total estimated expected reward. Due to the similarities of our MAB to unbounded knapsack problems when the rewards are known, we use techniques taken from the unbounded knapsack domain. Thus, in this section, we first introduce the unbounded knapsack problem, and then show how to use knapsack methods in our algorithms."
    }, {
      "heading" : "3.1 The Unbounded Knapsack Problem",
      "text" : "The unbounded knapsack problem is formulated as follows. A knapsack of weight capacity C is to be filled with some set ofK different types of items. Each item type i ∈ K has a corresponding value vi and weight wi, and the problem is to select a set that maximises the total value of items in the knapsack, such that their total weight does not exceed the knapsack capacity C. That is, the goal is to find the non–negative integers {xi} K i=1 that maximise:\nK ∑\ni=1\nxivi, (5)\ns.t.\nK ∑\ni=1\nxiwi ≤ C,\n∀i ∈ {1, . . . ,K} : xi integer.\nNote that this problem is a generalisation of the standard knapsack problem, in which xi ∈ {0, 1}; that is, each item type contains only one item, and we can either choose it or not. The unbounded knapsack problem is NP–hard. However, near–optimal approximation methods have been proposed to solve it (a detailed survey can be found in (Kellerer et al., 2004)). Among these approximation methods, a simple, but efficient approach is the density–ordered greedy algorithm, and here we make use of this method. In more detail, the density– ordered greedy algorithm has O (K logK) computational complexity, where K is the number of item types (Kohli et al., 2004). This algorithm works as follows. Let vi/wi denote the density of type i. To begin, the item types are sorted in order of their density, which is an operation of O (K logK) computational complexity. Next, in the first round of this algorithm, as many units of the highest density item are selected as is feasible without exceeding the knapsack capacity. Then, in the second round, the densest item of the remaining feasible items is identified, and as many units of it as possible are selected. This step is repeated until there are no feasible items left (i.e. at most K rounds).\nAnother way to approximate the optimal solution of the unbounded knapsack problem is the fractional relaxation based algorithm. This relaxes the original problem to its fractional version. In particular, within the fractional unbounded knapsack problem we allow xi to be fractional. Now, it is easy to show that the optimal solution of the fractional unbounded knapsack is to solely choose I∗ = argmaxi vi/wi (i.e. I ∗ is the item type with the highest density)\n(Kellerer et al., 2004). That is, if x∗ = 〈x∗1, . . . , x ∗ 1〉 denotes the optimal solution of the fractional unbounded knapsack, then x∗I∗ = C/wI∗ , while ∀j 6= I ∗, xj = 0. Given this, within the original unbounded knapsack problem (where xi are integers), the fractional relaxation based algorithm chooses xI∗ = ⌊C/wI∗⌋, and xj = 0, ∀j 6= I\n∗. It can easily shown that the complexity of this algorithm is O (K), which is the cost of determining the highest density type."
    }, {
      "heading" : "3.2 KUBE",
      "text" : "The KUBE algorithm is depicted in Algorithm 1. Here, let t denote the time step, and Bt denote the residual budget at time t ≥ 1, respectively. Note that at the start (i.e. t = 1), B1 = B, where B is the total budget limit. At each subsequent time step, t, KUBE first checks that arm pulling is feasible. That is, it is feasible only if at least one of the arms can be pulled with the remaining budget. Specifically, if Bt < minj cj (i.e. the residual budget is smaller than the lowest pulling cost), then KUBE stops (steps 3− 4).\nIf arm pulling is still feasible, KUBE first pulls each arm once in the initial phase (steps 6−7). Following this, at each time step t > K, it estimates the best set of arms according to their upper confidence bound using the density–ordered greedy approximation method applied to the following problem:\nmax\nK ∑\ni=1\nmi,t\n(\nµ̂i,ni,t +\n√\n2 ln t\nni,t\n)\n(6)\ns.t. K ∑\ni=1\nmi,tci ≤ Bt, ∀i, t : mi,t integer.\nIn the above expression, µ̂i,ni,t is the current estimate of arm i’s expected reward (calculated as the average reward received so far from pulling arm i), ni,t is the number of pulls of arm i until time step t, and √\n2 ln t ni,t is the size of the\nupper confidence interval. The goal, then, is to find integers {mi,t}i∈K such that Equation 6 is maximised, with respect to the residual budget limit Bt (n.b. from here on, we drop the subscript i ∈ K on this set). Since this problem is NP–hard, we use the density–ordered greedy method to find a near–optimal set of arms (step 9). Note that the upper confidence bound on arm i’s density is:\nµ̂i,ni,t ci +\n√\n2 ln t ni,t ci . (7)\nLet M∗(Bt) = {m ∗ i,t} be this method’s solution to the problem in Equation 6, giving us the desired set of arms, where m∗i,t is an index of arm i that indicates how many times arm i is taken into account within the set. Using {m∗i,t}, KUBE randomly chooses the next arm to pull, i(t), by selecting arm i with probability (step 10):\nP (i (t) = i) = m∗i,t\n∑K k=1 m ∗ k,t\n. (8)\nAfter the pull, it then updates the estimated upper bound of the chosen arm, and the residual budget limit Bt (steps 12− 13).\nAlgorithm 1 The KUBEAlgorithm\n1: t = 1; Bt = B; γ > 0; 2: while pulling is feasible do 3: if Bt < mini ci then 4: STOP! {pulling is not feasible} 5: end if 6: if t ≤ K then 7: Initial phase: play arm i (t) = t; 8: else 9: use density–ordered greedy to calculate M∗(Bt) = {m ∗ i,t}, the solution\nof Equation 6;\n10: randomly pull i (t) with P (i (t) = i) = m∗i,t ∑\nK k=1 m ∗ k,t\n;\n11: end if 12: update the estimated upper bound of arm i (t); 13: Bt+1 = Bt − ci(t); t = t+ 1; 14: end while\nThe intuition behind KUBE is the following. By repeatedly drawing the next arm to pull from a distribution formed by the current estimated approximate best set, the expected reward of KUBE equals the average reward for following the optimal solution to the corresponding unbounded knapsack problem, given the current reward estimates. If the true values of the arms were known, then this would imply that the average performance of KUBE efficiently converges to the optimal solution of the unbounded knapsack problem reduced from the budget–limited MAB model. It is easy to show that the optimal solution of this knapsack model forms the theoretical optimal policy of the budget–limited MAB in case of having full information. Put differently, if the mean reward value of each arm is known, then the budget–limited problem can be reduced to the unbounded knapsack problem, and thus, the optimal solution of the knapsack problem is the optimal solution of the budget–limited MAB as well. In addition, by combining the upper confidence bound with the estimated mean values of the arms, we guarantee that an arm that is not yet sampled many times may be pulled more frequently, since its upper confidence interval is large. Thus, we explore and exploit at the same time (for more details, see (Audibert et al., 2009; Auer et al., 2002)). Note that, by using the density–ordered greedy method, KUBE achieves a O (K lnK) computational cost per time step."
    }, {
      "heading" : "3.3 Fractional KUBE",
      "text" : "We now turn to the fractional version of KUBE, which follows the underlying concept of KUBE. It also approximates the underlying unbounded knapsack problem at each time step t in order to determine the frequency of arms within the estimated best set of arms. However, it differs from KUBE by using the fractional relaxation based method to approximate the unbounded knapsack in Step 9 of Algorithm 1. Crucially, fractional KUBE uses the fractional relaxation based algorithm to solve the following fractional unbounded knapsack problem at each t:\nmax\nK ∑\ni=1\nmi,t\n(\nµ̂i,ni,t +\n√\n2 ln t\nni,t\n)\ns.t.\nK ∑\ni=1\nmi,tci ≤ Bt. (9)\nRecall that within KUBE, the frequency of arms within the approximated solution of the unbounded knapsack forms a probability distribution from which the agent randomly pulls the next arm. Now, since the fractional relaxation based algorithm solely chooses the arm (i.e. item type) with the highest estimated confidence bound–cost ratio (i.e. item density), fractional KUBE does not need to randomly choose an arm. Instead, at each time step t, it pulls the arm that maximises ( µ̂i,ni,t/ci + √ 2 ln t ni,t/ci ) . That is, fractional KUBE can also be seen as\nthe budget–limited version of UCB (see (Auer et al., 2002) for more details of UCB).\nComputation–wise, by replacing the density–ordered greedy with the fractional relaxation based algorithm, fractional KUBE decreases the computational cost to O (K) per time step. In what follows, we show that both KUBE and its fractional counterpart achieve asymptotically optimal regret bounds."
    }, {
      "heading" : "4 Performance Analysis",
      "text" : "We now focus on the analysis of the expected regret of KUBE and fractional KUBE, defined by Equation 4. To this end, in this section we: (i) derive an upper bound on the regret of the algorithms, and (ii) show that these bounds are asymptotically optimal.\nTo begin, let us state some simplifying assumptions and define some useful terms. Without loss of generality, for ease of exposition we assume that the reward distribution of each arm has support in [0, 1], and that the pulling cost ci ≥ 1 for each i (our result can be scaled for different size supports and costs as appropriate). Let I∗ = argmaxi µi/ci be the arm with the highest true mean value density. For the sake of simplicity, we assume that I∗ is unique (however, our proofs do not exploit this fact at all). Let dmin = minj 6=I∗ {µI∗/cI∗ − µj/cj} denote the minimal true mean value density difference of arm I∗ and that of any other arm j. In addition, let cmin = minj cj and cmax = maxj cj denote the smallest and largest pulling costs, respectively. Then let δj = cj − cI∗ be the difference of arm j’s pulling cost and the minimal pulling cost. Similarly, let ∆j = µI∗ − µj denote the difference of the highest true mean value and that of arm j. Note that both δj and ∆j could be negative values, since I\n∗ does not necessarily have the highest true mean value, nor the smallest pulling cost. In addition, let T denote the finite–time operating time of the agent.\nNow, we first analyse the performance of KUBE. In what follows, we first estimate the number of times we pull arm j 6= I∗, instead of I∗. Based on this result, we estimate E [T ], the average number of pulls of KUBE. This bound guarantees that KUBE always pulls “enough” arms so that the difference of the number of pulls in the theoretical optimal solution and that of KUBE is small, compared to the size of the budget. By using the estimated value of E [T ], we then show that KUBE achieves a O (ln (B)) worst case regret on average. In more detail, we get:\nTheorem 1 (Main result 1) For any budget size B > 0, the performance\nregret of KUBE is at most:\n(\n8\nd2min +\n(\ncmax cmin\n)2 )\n\n\n∑\n∆j>0\n∆j + ∑\nδj>0\nδj\ncI∗\n\n ln\n(\nB\ncmin\n)\n+\n\n\n∑\n∆j>0\n∆j + ∑\nδj>0\nδj\ncI∗\n\n\n(\nπ2\n3 + 1\n)\n+ 1 .\nIt is easy to show that for each j 6= I∗, at least one between δj and ∆j has to be positive. This implies that ( ∑\n∆j>0 ∆j +\n∑ δj>0 δj cI∗\n)\n> 0. That is, the\nperformance regret of KUBE (i.e. R (KUBE)) is upper–bounded by O (lnB). To prove this theorem, we will make use of the following version of the Chernoff– Hoeffding concentration inequality for bounded random variables:\nTheorem 2 (Chernoff–Hoeffding inequality (Hoeffding, 1963)) Let X1, X2, . . . , Xn denote the sequence of random variables with common range [0, 1], such that for any 1 ≤ t ≤ n, we have E [Xt|X1, . . . , Xt−1] = µ. Let Sn = 1 n ∑n t=1 Xt. Given this, for any δ ≥ 0, we have:\nP (Sn ≥ µ+ δ) ≤ e −2nδ2 , P (Sn ≤ µ− δ) ≤ e −2nδ2 .\nThe proof can be found, for example, in Hoeffding (1963). We now focus on the performance analysis of KUBE. To this end, we introduce some further notation. Let T denote the number of pulls of KUBE. In addition, let Nj (T ) denote the number of times KUBE pulls arm j up to time step T .\nIn what follows, we first devise an upper bound for Nj (T ) for all j 6= I ∗. That is, we estimate the number of times we pull arm j 6= I∗, instead of I∗. Based on this result, we estimate the average number of pulls of KUBE (i.e. E [T ]). This bound guarantees that KUBE always pulls “enough” arms so that the difference between the number of pulls in the theoretical optimal solution and that of KUBE is small, compared to the size of the budget. By using the estimated value of E [T ], we then show that KUBE achieves a O (ln (B)) worst case regret on average. We now state the following:\nLemma 3 Suppose that KUBE pulls the arms T times. If j 6= I∗, then:\nE [Nj (T ) |T ] ≤\n(\n8\nd2min +\n(\ncmax cmin\n)2 )\nln (T ) + π2\n3 + 1.\nThat is, the number of times KUBE pulls an arm j 6= I∗ is at most O (ln (T )). To prove this lemma, let us first refresh some of the terms that are used: i (t) is the arm pulled by KUBE at time t; when refering to a set of arms {mj,t}, mj,t is the number of pulls of arm j; M ∗(Bt) = {m ∗ i,t} is the density–ordered greedy approximate solution to unbounded knapsack problem in Equation 6, where m∗i,t is the number of arm i’s pulls in this set; and I ∗ = argmaxi µi ci is the arm with the highest true mean value density. In addition, Î (t) =\nargmaxj\n{\nµ̂j,nj,t cj +\n√\n2 ln t nj,t cj\n}\nis the arm with the highest estimated density confi-\ndence bound at time step t. In order to prove Lemma 3, we rely on the following lemmas:\nLemma 4 Suppose that the total number of pulls KUBE makes of the arms is T , and that at each time step t, the residual budget is Bt (note that here B1 = B). For any 0 < t ≤ T , we have:\ncmin Bt ≤ 1 T − t+ 1 .\nLemma 5 Suppose that the total number of pulls KUBE makes of the arms is T . For any 0 < t ≤ T , we have:\nP (i (t) = j|T ) ≤ P ( Î (t) = j|T ) +\n(\ncmax cmin\n)2 1\nT − t+ 1 .\nProof of Lemma 4. At the beginning of time step t, the residual budget is Bt. Since the total number of pulls is T , with respect to Bt, KUBE can still make T − t+ 1 pulls (including the pull at time step t). This indicates that:\nBt ≥ ci(t) + ci(t+1) + · · ·+ ci(T ) ≥ (T − t+ 1) cmin.\nwhich directly implies the inequality in Lemma 4.\nProof of Lemma 5. We assume that the value of T is given. For the slight abuse of notation, we drop the conditional of T notation to simplify the proof (i.e. all the probabilities are considered to be conditional to T ), and we will explicitly denote it when necessary. First, we consider a particular value of Bt. Thus, we have:\nP (i (t) = j|Bt) = ∑\n{mi,t}\nP (i (t) = j|M∗ (Bt) = {mi,t})P (M ∗ (Bt) = {mi,t}).\n(10) Recall that the density–ordered greedy approach first repeatedly adds arm Î (t) to set {mi,t} until it is not feasible. It is easy to show that after adding arm Î (t) as many times as possible (i.e. m\nÎ(t),t times) to the set, the residual budget is at\nmost c Î(t) (or otherwise we could still add arm Î (t) one more time). Therefore:\n∑\ni6=Î(t)\nmi,t ≤ c Î(t)\ncmin . (11)\nThat is, the total count of arm pulls other than Î (t) in the set is at most c Î(t)\ncmin .\nThis inequality comes from the fact that we can construct a set with the greatest number of arm pulls by only adding the arm with the smallest cost. Similarly, we have:\nK ∑\nk=1\nmk,t ≥ Bt cmax , (12)\nbecause we can construct a set with the smallest number of arm pulls by only adding the arm with the greatest cost. Combining Equations 11 and 12 gives:\n∑\ni6=Î(t) mi,t ∑K\nk=1 mk,t ≤\nc Î(t) cmin Bt cmax ≤ ( cmax cmin )2 cmin Bt . (13)\nThe last inequality is obtained from the fact that c Î(t) ≤ cmax. Now, recall that KUBE chooses arm j to pull with probability mj,t ∑\nK k=1 mk,t\n. This implies that:\nP ( i (t) = j|M∗ (Bt) = {mi,t} )\n= P ( i (t) = j, Î (t) = j|M∗ (Bt) = {mi,t} )\n+ P ( i (t) = j, Î (t) 6= j|M∗ (Bt) = {mi,t} ) .\nThis can be upper bounded by:\nP ( i (t) = j|M∗ (Bt) = {mi,t} )\n≤ m\nÎ(t),t ∑K k=1 mk,t P ( Î (t) = j|M∗ (Bt) = {mi,t} )\n(14)\n+\n∑\ni6=Î(t) mi,t ∑K k=1 mk,t P ( Î (t) 6= j|M∗ (Bt) = {mi,t} ) .\nThe right hand side can be further upper bounded as follows:\nP ( i (t) = j|M∗ (Bt) = {mi,t} )\n≤ P ( Î (t) = j|M∗ (Bt) = {mi,t} ) +\n∑\ni6=Î(t) mi,t ∑K\nk=1 mk,t\n≤ P ( Î (t) = j|M∗ (Bt) = {mi,t} ) +\n(\ncmax cmin )2 cmin Bt . (15)\nThe last inequality is obtained from Equation 13. Substituting Equation 15 into Equation 10 gives:\nP (i (t) = j|Bt) ≤ ∑\n{mi,t}\n(\nP ( Î (t) = j|M∗ (Bt) = {mi,t} ) +\n(\ncmax cmin\n)2 cmin\nBt\n)\nP (M∗ (Bt) = {mi,t})\n≤ P ( Î (t) = j|Bt ) +\n(\ncmax cmin\n)2 cmin\nBt\n≤ P ( Î (t) = j|Bt ) +\n(\ncmax cmin\n)2 1\nT − t+ 1 . (16)\nThe last inequality is obtained from Lemma 4. Now we study the general case, where Bt is not fixed. By summing up Equation 16 over all possible value of Bt, we have:\nP (i (t) = j|T ) = ∑\nBt\nP (i (t) = j|T, Bt)P (Bt|T )\n≤ ∑\nBt\n(\nP ( Î (t) = j|T, Bt ) +\n(\ncmax cmin\n)2 1\nT − t+ 1\n)\nP (Bt|T )\n≤ P ( Î (t) = j|T ) +\n(\ncmax cmin\n)2 1\nT − t+ 1 . (17)\nwhich concludes the proof. Based on Lemmas 4 and 5, Lemma 3 can be proved as follows:\nProof of Lemma 3. We assume that the value of T is already given. Again, for the slight abuse of notation, we drop the conditional of T notation to simplify the proof, and we will explicitly denote it when necessary. In this case, the proof of the theorem for that particular value of T is along the same lines as that of Theorem 1 of (Auer et al., 2002). In particular, recall that Nj (T ) denotes the expectation of number of times KUBE pulls an arm j 6= I∗ until time step T . Given this, we have the following:\nE [Nj (T )] = 1 + T ∑\nt=K+1\nP (i (t) = j)\n≤ 1 +\nT ∑\nt=K+1\nP ( Î (t) = j ) +\nT ∑\nt=K+1\n(\ncmax cmin\n)2 1\nT − t+ 1\n≤ l +\nT ∑\nt=K+1\nP ( Î (t) = j,Nj (t) ≥ l ) +\nT ∑\nt=K+1\n(\ncmax cmin\n)2 1\nT − t+ 1\n(18)\nfor any l ≥ 1. Now, let bt,s = √ 2 ln t s . Considering the second term on the right hand side of Equation 18, we have:\nT ∑\nt=K+1\nP ( Î (t) = j, Nj (t) ≥ l ) = T ∑\nt=K+1\nP\n(\nµ̂I∗,NI∗ (t)\ncI∗ +\nbt,NI∗ (t)\ncI∗ ≤\nµ̂j,Nj (t)\ncj +\nbt,Nj(t)\ncj , Nj (t) ≥ l\n)\n≤ T ∑\nt=K+1\nP\n(\nmin 1≤s≤t\n{\nµ̂I∗,s\ncI∗ +\nbt,s cI∗\n}\n≤ max l≤sj≤t\n{\nµ̂j,sj\ncj +\nbt,sj\ncj\n})\n≤ T ∑\nt=1\nt ∑\ns=1\nt ∑\nsj=1\nP\n(\nµ̂I∗,s\ncI∗ +\nbt,s cI∗ ≤ µ̂j,sj cj + bt,sj cj\n)\n.\n(19)\nIf it is true that µ̂I∗,s cI∗ + bt,s cI∗ ≤ µ̂j,sj cj + bt,sj cj , then at least one of the following three statements must also hold:\nµ̂I∗,s cI∗ + bt,s cI∗ ≤ µI∗ cI∗ , (20)\nµj cj ≤ µ̂j,sj cj + bt,sj cj , (21)\nµI∗ cI∗ ≤ µj cj + 2bt,sj cj . (22)\nThat is, we get:\nP\n(\nµ̂I∗,s cI∗ + bt,s cI∗ ≤ µ̂j,sj cj + bt,sj cj\n)\n≤P\n(\nµ̂I∗,s cI∗ + bt,s cI∗ ≤ µI∗ cI∗\n)\n+\n+ P\n(\nµj cj ≤ µ̂j,sj cj + bt,sj cj\n)\n+ P\n(\nµI∗ cI∗ ≤ µj cj + 2bt,sj cj\n)\n.\n(23)\nApplying the Chernoff–Hoeffding inequalities to the first two terms on the right hand side of Equation 23 gives:\nP\n(\nµ̂I∗,s cI∗ + bt,s cI∗ ≤ µI∗ cI∗\n)\n= P (µ̂I∗,s + bt,s ≤ µI∗) ≤ exp { −2b2t,ss } = exp {−4 ln t} = t−4\n(24)\nP\n(\nµj cj ≤ µ̂j,sj cj + bt,sj cj\n)\n= P ( µj ≤ µ̂j,sj + bt,sj )\n≤ exp {\n−2b2t,sjsj\n}\n= exp {−4 ln t} = t−4.\n(25)\nOn the other hand, for l ≥ 8 lnT d2min , Equation 22 is false, since:\nµI∗ cI∗ − µj cj − 2bt,sj cj ≥ µI∗ cI∗ − µj cj − 2bt,sj\n≥ µI∗\ncI∗ − µj cj − 2\n√\n2 ln t\nl\n≥ µI∗\ncI∗ − µj cj − 2\n√\n2 ln t 8 lnT d2min\n≥ µI∗\ncI∗ − µj cj − dmin\n≥ µI∗\ncI∗ − µj cj − dj = 0. (26)\nHere note that cj ≥ 1, sj ≥ l ≥ 8 lnT d2min , and t ≤ T . If l ≥ 8 lnT d2min , then P (\nµI∗ cI∗ ≤ µj cj + 2bt,sj cj\n)\n= 0. Substituting this and Equations 23, 24 and 25\ninto Equation 19 gives:\nT ∑\nt=K+1\nP ( Î (t) = j,Nj (t) ≥ l ) ≤\nT ∑\nt=1\nt ∑\ns=1\nt ∑\nsj=1\n2t−4 ≤ π2\n3 , (27)\nfor any l ≥ ⌈\n8 lnT d2min\n⌉\n. Note that the last inequality is obtained from the Riemann\nZeta Function for value of 2 (i.e. ∑∞ t=1 t −2 = π\n2\n6 ) (Ivic, 1985). Now, consider the third term on the right hand side of Equation 18. By\nusing Lemma 4, we get:\nT ∑\nt=1\n(\ncmax cmin\n)2 1\nT − t+ 1 ≤\n(\ncmax cmin\n)2\nln (T ). (28)\nWe now combine Equations 27 and 28 together, and we set l = 8 lnT d2min +1, which gives:\nE [Nj (T )] ≤ 8 lnT\nd2min + 1 +\nπ2\n3 +\n(\ncmax cmin\n)2\nln (T )\nfor any given value of T , which concludes the proof. From Lemma 3, we can show the following:\nLemma 6 Suppose that the total budget size is B. If T denotes the total number of pulls of KUBE then we have:\nE [T ] ≥ B\ncI∗ −\n(\n8\nd2min +\n(\ncmax cmin\n)2 )\n∑\nδj>0\nδj cI∗ ln\n(\nB\ncmin\n)\n− ∑\nδj>0\nδj cI∗\n(\nπ2\n3 + 1\n)\n− 1\nwhere E [T ] is the expected number of pulls using KUBE.\nThat is, the difference between B cI∗ and the number of pulls of KUBE is at most O ( ln (\nB cmin\n))\n.\nProof of Lemma 6. Since KUBE pulls arms until none are feasible, by definition:\nP\n(\nT ∑\nt=1\nci(t) ≤ B − cmin\n)\n= 1.\nTaking the expectation of ∑T\nt=1 ci(t) over T and {mj,t} (i.e. the set of i (t)) gives:\nB − cmin ≤ ET,{i(t)}\n[\nT ∑\nt=1\nci(t)\n]\n= ET\n[\nT ∑\nt=1\nEi(t) [ ci(t) ]\n]\n≤ ET\n\n\nT ∑\nt=1\nK ∑\nj=1\ncjP (i (t) = j|T )\n\n\n≤ ET\n\n\nT ∑\nt=1\n\ncI∗ + ∑\nδj>0\nδjP (i (t) = j|T )\n\n\n\n\n≤ ET [T ] cI∗ +ET\n\n\n∑\nδj>0\nδj\n(\nT ∑\nt=1\nP (i (t) = j|T )\n)\n\n\n≤ ET [T ] cI∗ +ET\n\n\n∑\nδj>0\nδj\n((\n8\nd2min +\n(\ncmax cmin\n)2 )\nln (T ) + π2\n3 + 1\n)\n\n (29)\n≤ ET [T ] cI∗ + ∑\nδj>0\nδj\n((\n8\nd2min +\n(\ncmax cmin\n)2 )\nln\n(\nB\ncmin\n)\n+ π2\n3 + 1\n)\n. (30)\nEquation 29 is obtained from Lemma 3, while Equation 30 comes from the fact that T ≤ B\ncmin with probability 1. In addition, the third inequality is obtained\nfrom the fact that δj can be smaller than 0 for some j, and thus, we can further upper bound by only summing up δjP (i (t) = j|T ) over arms that have δj > 0. Now, by dividing both sides with cI∗ , we obtain:\nB\ncI∗ − cmin cI∗ − ∑\nδj>0\nδj cI∗\n((\n8\nd2min +\n(\ncmax cmin\n)2 )\nln\n(\nB\ncmin\n)\n+ π2\n3 + 1\n)\n≤ ET [T ] .\nBy using the fact that cmin cI∗ ≤ 1, we obtain the stated formula. Note that if we relax the budget–limited MAB problem so that the number of pulls can be\nfractional, then it is easy to show that the optimal pulling policy of this relaxed model is to repeatedly pull arm I∗ only. In this case, B\ncI∗ is the number of\npulls of this optimal policy. Lemma 6 indicates that the number of pulls that KUBE produces does not significantly differ from that of the optimal policy of the fractional budget–limited MAB (i.e. the difference is a logarithmic function of the number of pulls). We can now derive the regret bound of KUBE from Lemma 6 as follows: Proof of Theorem 1. Recall thatE [ GB (A∗) ] denotes the expected performance of the theoretical optimal policy. It is obvious that E [ GB (A∗) ]\n≤ BµI∗ cI∗\n, since the latter is the optimal solution of the fractional budget–limited MAB problem. This indicates that:\nRB (KUBE) = E [ GB (A∗) ] −E [ GB (KUBE) ]\n≤ BµI∗\ncI∗ −ET,{i(t)}\n[\nT ∑\nt=1\nµi(t)\n]\n≤ BµI∗\ncI∗ −ET\n[\nT ∑\nt=1\nEi(t) [ µi(t) ]\n]\n≤ ET\n[\nBµI∗\ncI∗ −\nT ∑\nt=1\nEi(t) [ µi(t) ]\n]\n≤ ET\n\n\nBµI∗\ncI∗ −\nT ∑\nt=1\nK ∑\nj\nµjP (i (t) = j|T )\n\n\n≤ ET\n\n\n(\nB\ncI∗ − T\n) µI∗ + T ∑\nt=1\n µI∗ − K ∑\nj\nµjP (i (t) = j|T )\n\n\n\n\n≤ ET\n[\nB\ncI∗ − T\n]\nµI∗ +ET\n\n\nT ∑\nt=1\n∑\n∆j>0\n∆jP (i (t) = j|T )\n\n\n≤ ET\n[\nB\ncI∗ − T\n]\nµI∗ +ET\n\n\n∑\n∆j>0\n∆jE [Nj (T ) |T ]\n\n . (31)\nNote that since ∆j can be smaller than 0 for some arm j, we can further upper bound RB (KUBE) by only summing up ∆jE [Nj (T ) |T ] over arms with ∆j > 0 (see the last two inequalities). Applying Lemma 6 to the first term and Lemma 3\nto the second term on the right hand side of Equation 31 gives:\nRB (KUBE) ≤\n\n\n(\n8\nd2min +\n(\ncmax cmin\n)2 )\n∑\nδj>0\nδj cI∗ ln\n(\nB\ncmin\n)\n+ ∑\nδj>0\nδj cI∗\n(\nπ2\n3 + 1\n)\n+ 1\n\nµI∗+\n+ET\n\n\n∑\n∆j>0\n∆j\n((\n8\nd2min +\n(\ncmax cmin\n)2 )\nln (T ) + π2\n3 + 1\n)\n\n\n≤\n(\n8\nd2min +\n(\ncmax cmin\n)2 )\n∑\nδj>0\nδj cI∗ ln\n(\nB\ncmin\n)\n+ ∑\nδj>0\nδj cI∗\n(\nπ2\n3 + 1\n)\n+ 1+\n+ ∑\n∆j>0\n∆j\n((\n8\nd2min +\n(\ncmax cmin\n)2 )\nln\n(\nB\ncmin\n)\n+ π2\n3 + 1\n)\nwhich concludes the proof. Note that the last equation is obtained from the facts that µI∗ ≤ 1 and T ≤\nB cmin with probability 1. In a similar vein, we can show that the regret of fractional KUBE is bounded\nas follows:\nTheorem 7 (Main result 2) For any budget size B > 0, the performance regret of fractional KUBE is at most\n8\nd2min\n\n\n∑\n∆j>0\n∆j + ∑\nδj>0\nδj\ncI∗\n\n ln\n(\nB\ncmin\n)\n+\n\n\n∑\n∆j>0\n∆j + ∑\nδj>0\nδj\ncI∗\n\n\n(\nπ2\n3 + 1\n)\n+ 1 .\nProof of Theorem 7. We follow the concept that is similar to the proof of Theorem 1. Given this, we only highlight the steps that are different from the previous proofs. For the sake of simplicity, we use the notations previously introduced for the performance analysis of KUBE. In particular, let T denote the random variable that represents the number of pulls that fractional KUBE uses. Let Nj (T ) denote the number of times that the corresponding pulling algorithm pulls arm j up to time step T . Similar to Lemma 3, we first show that within the fractional KUBE algorithm, we have:\nE [Nj (T ) |T ] ≤ 8\nd2min ln (T ) +\nπ2\n3 + 1. (32)\nIn so doing, note that\nE [Nj (T ) |T ] = 1 + T ∑\nt=K+1\nP (i (t) = j|T ) ≤ l+ T ∑\nt=K+1\nP (i (t) = j,Nj (t) ≥ l|T )\n(33)\nfor any l ≥ 1. Now, using similar techniques from the proof of Lemma 3, we can easily show that\nT ∑\nt=K+1\nP (i (t) = j,Nj (t) ≥ l|T ) ≤\nT ∑\nt=1\nt ∑\ns=1\nt ∑\nsj=1\n2t−4 ≤ π2\n3 ,\nfor any l ≥ ⌈\n8 lnT d2min\n⌉\n. By substituting this into Equation 33, we obtain Equa-\ntion 32. Next, we show that\nE [T ] ≥ B\ncI∗ −\n8\nd2min\n∑\nδj>0\nδj cI∗ ln\n(\nB\ncmin\n)\n− ∑\nδj>0\nδj cI∗\n(\nπ2\n3 + 1\n)\n− 1. (34)\nThis can be derived from Equation 32 by using techniques similar to the proof of Lemma 6. This implies that\nRB (KUBE) = E [ GB (A∗) ] −E [ GB (KUBE) ]\n≤ BµI∗\ncI∗ −ET,{i(t)}\n[\nT ∑\nt=1\nµi(t)\n]\n≤ BµI∗\ncI∗ −ET\n[\nT ∑\nt=1\nEi(t) [ µi(t) ]\n]\n≤ ET\n[\nBµI∗\ncI∗ −\nT ∑\nt=1\nEi(t) [ µi(t) ]\n]\n≤ ET\n\n\nBµI∗\ncI∗ −\nT ∑\nt=1\nK ∑\nj\nµjP (i (t) = j|T )\n\n\n≤ ET\n\n\n(\nB\ncI∗ − T\n)\nµI∗ +\nT ∑\nt=1\n\nµI∗ −\nK ∑\nj\nµjP (i (t) = j|T )\n\n\n\n\n≤ ET\n[\nB\ncI∗ − T\n]\nµI∗ +ET\n\n\nT ∑\nt=1\n∑\n∆j>0\n∆jP (i (t) = j|T )\n\n\n≤ ET\n[\nB\ncI∗ − T\n]\nµI∗ +ET\n\n\n∑\n∆j>0\n∆jE [Nj (T ) |T ]\n\n . (35)\nBy substituting Equations 33 and 34 into this, we obtain\nRB (KUBE) ≤ 8\nd2min\n∑\nδj>0\nδj cI∗ ln\n(\nB\ncmin\n)\n+ ∑\nδj>0\nδj cI∗\n(\nπ2\n3 + 1\n)\n+ 1+\n+ ∑\n∆j>0\n∆j\n(\n8\nd2min ln\n(\nB\ncmin\n)\n+ π2\n3 + 1\n)\nwhich concludes the proof. Having established a regret bound for the two algorithms, we now move on to show that they produce optimal behaviour, in terms of minimising the regret. In more detail, we state that:\nTheorem 8 (Main result 3) For any arm pulling algorithm, there exists a constant C ≥ 0, and a particular instance of the budget–limited MAB problem, such that the regret of that algorithm within that particular problem is at least C lnB.\nProof of Theorem 8. By setting all of the arms’ pulling costs equal to c ≥ 0, any standard MAB problem can be reduced to a budget–limited MAB. This implies that the number of pulls within this MAB is guaranteed to be B\nc = T (i.e. T is\ndeterministic). According to (Lai and Robbins, 1985), the best possible regret that an arm pulling algorithm can achieve within the domain of standard MABs is C ln (T ). Therefore, if there is an algorithm within the domain of budget– limited that provides better regret than C ln (\nB c\n)\n= C lnT , then it also provides better regret bounds for standard MABs.\nThe results in Theorem 1 and 7 can be interpreted to the standard MAB domain as follows. The standard MAB can be reduced to a budget–limited MAB by setting all the pulling costs to be the same. Given this, B/cmin = T in any sequence of pulls. This implies that both KUBE and fractional KUBE achieve O (lnT ) regret within the standard MAB domain, which is optimal (Auer et al., 2002; Lai and Robbins, 1985).\nNote that the regret bound of fractional KUBE is better (i.e. the constant factor within the regret bound of fractional KUBE is smaller than that of KUBE). However, this does not indicate that fractional KUBE has better performance in practice. One possible reason is that these bounds are not tight. In fact, as we will demonstrate in Section 5, KUBE typically outperforms its fractional counterpart by up to 40%."
    }, {
      "heading" : "5 Performance Evaluation",
      "text" : "In the previous section, we showed that the two algorithms provide asymptotically optimal regret bounds, and that the theoretical regret bound of fractional KUBE is tighter than that of KUBE. In addition, we also demonstrated that fractional KUBE outperforms KUBE in terms of computational complexity. However, it might be the case that these bounds are not tight, and thus, fractional KUBE is less practical than KUBE in real–world applications, as is the case with the standard MAB algorithm, where simple but not optimal methods (e.g. ε–first, or ε–greedy) typically outperform more advanced, theoretically optimal, algorithms (e.g. POKER(Vermorel and Mohri, 2005), or UCB). Given this, we now evaluate the performance of both algorithms through extensive simulations, in order to determine their efficiency in practice. We also compare the performance of the proposed algorithms against that of different budget–limited ε–first approaches. In particular, we show that both of our algorithms outperform the budget–limited ε–first algorithms. In addition, we also demonstrate that KUBE typically achieves lower regret than its fractional counterpart.\nNow, note that if the pulling costs are homogeneous — that is, the pulling cost of the arms do not significantly differ from each other — then the performance of the density–ordered greedy algorithm does not significantly differ from that of the fractional relaxation based (Kellerer et al., 2004). Indeed, since the pulling costs are similar, it is easy to show that the density–ordered greedy approach typically stops after one round, and thus, results in similar behaviour to the fractional relaxation based method. On the other hand, if the pulling costs are more diverse (i.e. the pulling costs of the arms differ from each other), then the performance of the density–ordered greedy algorithm becomes more efficient than that of the fractional relaxation based algorithm. Given this, in order to compare the performance of KUBE and its fractional counterpart, we set three\ntest cases, namely: bandits with (i) homogeneous pulling costs; (ii) moderately diverse pulling costs; and (iii) extremely diverse costs. In particular, within the homogeneous case, the pulling costs are randomly and independently chosen from the interval [5, 10]. In addition, the pulling costs are set to be between [1, 10] within the moderately diverse case, and between [1, 20] in the extremely diverse case, respectively. The reward distribution of each arm i is set to be a truncated Gaussian, with mean µi, randomly taken from interval [10, 20], variance σ2i = µi 2 , and with supports [0, 2µi]. In addition, we set number of arms K to be 100. Our results are shown in Figure 1. These plots show the performance of each algorithm divided by ln B cmin\n, and the error bars represent the 95% confidence intervals. By doing this, we can see that the performance regret of both algorithms is O (\nln B cmin\n)\n, since in each test case, their performance converges to\nC ln B cmin (after it is divided by ln B cmin ), where C is some constant factor. From the numerical results, we can see that both KUBE and fractional KUBE differ from the best possible solution by small constant factors (i.e. C), since the limit of their convergence is typically low (i.e. it varies between 4 and 7 in the test cases), compared to the regret value of the algorithm. In addition, we can also see that fractional KUBE algorithm is typically outperformed by KUBE. The reason is that the density–ordered greedy algorithm provides a better approximation than the fractional relaxation based approach to the underlying unbounded knapsack problem. This implies that KUBE converges to the optimal pulling policy faster than its fractional counterpart. In particular, as expected, the performance of the algorithms are similar to each other in the homogeneous case, where the density–ordered greedy method shows similar behaviour to the fractional relaxation based approach. In contrast, KUBE clearly achieves better performance (i.e. lower regret) within the diverse cases. Specifically, within the moderately diverse case, KUBE outperforms its fractional counterpart by up to 40% (i.e. the regret of KUBE is 40% lower than that of the fractional KUBE algorithm). In addition, the performance improvement of KUBE is typically around 30% in the extremely diverse case. This implies that, although the current theoretical regret bounds are asymptotically optimal, they are not tight.\nApart from this, we can also observe that both of our algorithms outperform the budget–limited ε–first approaches. In particular, KUBE and its fractional counterpart typically achieves less regret by up to 70% and 50% than the budget–limited ε–first approaches, respectively. Note that the performance of the proposed algorithms are typically under the line O(B 2 3 (lnB)−1), while the budget–limited ε–first approaches achieve larger regrets. This implies that our proposed algorithms are the first methods that achieve logarithmic regret bounds."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we introduced two new algorithms, KUBE and fractional KUBE, for the budget–limited MAB problem. These algorithms sample each arm in an initial phase. Then, at each subsequent time step, they determine a best set of arms, according to the agent’s current reward estimates plus a confidence interval based on the number of samples taken of each arm. In particular, KUBE uses the density–ordered greedy algorithm to determine this best set of arms. In\ncontrast, fractional KUBE relies on the fractional relaxation based algorithm. KUBE and its fractional counterpart then use this best set as a probability distribution with which to randomly choose the next arm to pull. As such, both algorithms do not explicitly separate exploration from exploitation. We have also provided a O ln (B) theoretical upper bound for the performance regret of both algorithms, where B is the budget limit. In addition, we proved that the provided bounds are asymptotically optimal, that is, they differ from the best possible regret by only a constant factor. Finally, through simulation, we have demonstrated that KUBE typically outperforms its fractional counterpart up to 40%, however, with an increased computational cost. In particular, the average computational complexity of KUBE per time step is O (K lnK), while this value is O (K) for fractional KUBE.\nOne of the implications of the numerical results is that although fractional KUBE has a better bound on its performance regret than KUBE, the latter typically ourperforms the former in practice. Given this, our future work consists of improving the results of Theorems 1 and 7 to determine tighter upper bounds can be found. In addition, we aim to extend the budget–limited MAB model to settings where the reward distributions are dynamically changing, as is the case in a numer of real–world problems. This, however, is not trivial, since both of our algorithms rely on the assumption that the expected value of the rewards is static, and thus, the estimates converge to their real value."
    } ],
    "references" : [ {
      "title" : "Active learning in multi-armed bandits",
      "author" : [ "A. Antos", "V. Grover", "Cs. Szepesvári" ],
      "venue" : "In Proceedings of the Nineteenth International Conference on Algorithmic Learning Theory,",
      "citeRegEx" : "Antos et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Antos et al\\.",
      "year" : 2008
    }, {
      "title" : "Exploration-exploitation tradeoff using variance estimates in multi-armed bandits",
      "author" : [ "J-Y. Audibert", "R. Munos", "Cs. Szepesvári" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Audibert et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Audibert et al\\.",
      "year" : 2009
    }, {
      "title" : "Finite–time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Pure exploration for multi-armed bandit problems",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "In Proceedings of the Twentieth international conference on Algorithmic Learning Theory,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2009
    }, {
      "title" : "Approximation algorithms for budgeted learning problems",
      "author" : [ "S. Guha", "K. Munagala" ],
      "venue" : "In Proceedings of the Thirty-Ninth Annual ACM symposium on Theory of Computing,",
      "citeRegEx" : "Guha and Munagala.,? \\Q2007\\E",
      "shortCiteRegEx" : "Guha and Munagala.",
      "year" : 2007
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "W. Hoeffding" ],
      "venue" : "ournal of the American Statistical Association,",
      "citeRegEx" : "Hoeffding.,? \\Q1963\\E",
      "shortCiteRegEx" : "Hoeffding.",
      "year" : 1963
    }, {
      "title" : "The Riemann Zeta Function",
      "author" : [ "A. Ivic", "editor" ],
      "venue" : null,
      "citeRegEx" : "Ivic and editor.,? \\Q1985\\E",
      "shortCiteRegEx" : "Ivic and editor.",
      "year" : 1985
    }, {
      "title" : "Average performance of greedy heuristics for the integer knapsack problem",
      "author" : [ "R. Kohli", "R. Krishnamurti", "P. Mirchandani" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "Kohli et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Kohli et al\\.",
      "year" : 2004
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "Advances in Applied Mathemathics,",
      "citeRegEx" : "Lai and Robbins.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai and Robbins.",
      "year" : 1985
    }, {
      "title" : "A utility-based adaptive sensing and multihop communication protocol for wireless sensor networks",
      "author" : [ "P. Padhy", "R.K. Dash", "K. Martinez", "N.R. Jennings" ],
      "venue" : "ACM Transactions on Sensor Networks,",
      "citeRegEx" : "Padhy et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Padhy et al\\.",
      "year" : 2010
    }, {
      "title" : "Some aspects of the sequential design of experiments",
      "author" : [ "H. Robbins" ],
      "venue" : "Bulletin of the AMS,",
      "citeRegEx" : "Robbins.,? \\Q1952\\E",
      "shortCiteRegEx" : "Robbins.",
      "year" : 1952
    }, {
      "title" : "Epsilon–first policies for budget–limited multi–armed bandits",
      "author" : [ "L. Tran-Thanh", "A. Chapman", "J.E. Munoz de Cote", "A. Rogers", "N.R. Jennings" ],
      "venue" : "In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence,",
      "citeRegEx" : "Tran.Thanh et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Tran.Thanh et al\\.",
      "year" : 2010
    }, {
      "title" : "Long–term information collection with energy harvesting wireless sensors: A multiarmed bandit based approach",
      "author" : [ "L. Tran-Thanh", "A. Rogers", "N.R. Jennings" ],
      "venue" : "Journal of Autonomous Agents and Multi-Agent Systems,",
      "citeRegEx" : "Tran.Thanh et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Tran.Thanh et al\\.",
      "year" : 2011
    }, {
      "title" : "Multi-armed bandit algorithms and empirical evaluation",
      "author" : [ "J. Vermorel", "M. Mohri" ],
      "venue" : "European Conference on Machine Learning,",
      "citeRegEx" : "Vermorel and Mohri.,? \\Q2005\\E",
      "shortCiteRegEx" : "Vermorel and Mohri.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "In the standard MAB, this trade–off has been effectively balanced by decision–making policies such as upper confidence bound (UCB) and ǫn–greedy (Auer et al., 2002).",
      "startOffset" : 145,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : "1 Introduction The standard multi–armed bandit (MAB) problem was originally proposed by Robbins (1952), and presents one of the clearest examples of the trade–off between exploration and exploitation in reinforcement learning.",
      "startOffset" : 88,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "To this end, a variety of other related models have been studied recently, and, in particular, a number of researchers have focused on MABs with budget constraints, where arm–pulling is costly and is limited by a fixed budget (Antos et al., 2008; Bubeck et al., 2009; Guha and Munagala, 2007).",
      "startOffset" : 226,
      "endOffset" : 292
    }, {
      "referenceID" : 3,
      "context" : "To this end, a variety of other related models have been studied recently, and, in particular, a number of researchers have focused on MABs with budget constraints, where arm–pulling is costly and is limited by a fixed budget (Antos et al., 2008; Bubeck et al., 2009; Guha and Munagala, 2007).",
      "startOffset" : 226,
      "endOffset" : 292
    }, {
      "referenceID" : 4,
      "context" : "To this end, a variety of other related models have been studied recently, and, in particular, a number of researchers have focused on MABs with budget constraints, where arm–pulling is costly and is limited by a fixed budget (Antos et al., 2008; Bubeck et al., 2009; Guha and Munagala, 2007).",
      "startOffset" : 226,
      "endOffset" : 292
    }, {
      "referenceID" : 11,
      "context" : "To address this limitation, a new bandit model, the budget–limited MAB, was introduced (Tran-Thanh et al. 2010).",
      "startOffset" : 87,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "For example, in many wireless sensor network applications, a sensor node’s actions, such as sampling or data forwarding, consume energy, and therefore the number of actions is limited by the capacity of the sensor’s batteries (Padhy et al. 2010).",
      "startOffset" : 226,
      "endOffset" : 245
    }, {
      "referenceID" : 12,
      "context" : "Furthermore, many of these scenarios require that sensors learn the optimal sequence of actions that can be performed, with the goal of maximising the long term value of the actions they take (Tran-Thanh et al., 2011).",
      "startOffset" : 192,
      "endOffset" : 217
    }, {
      "referenceID" : 0,
      "context" : "To this end, a variety of other related models have been studied recently, and, in particular, a number of researchers have focused on MABs with budget constraints, where arm–pulling is costly and is limited by a fixed budget (Antos et al., 2008; Bubeck et al., 2009; Guha and Munagala, 2007). In these models, the agent’s exploration budget limits the number of times it can sample the arms in order to estimate their rewards, which defines an initial exploration phase. In the subsequent cost–free exploitation phase, an agent’s policy is then simply to pull the arm with the highest expected reward. However, in many settings, it is not only the exploration phase, but the exploitation phase that is also limited by a cost budget. To address this limitation, a new bandit model, the budget–limited MAB, was introduced (Tran-Thanh et al. 2010). In this model, pulling an arm is again costly, but crucially both the exploration and exploitation phases are limited by a single budget. This type of limitation is well motivated by several real–world applications. For example, in many wireless sensor network applications, a sensor node’s actions, such as sampling or data forwarding, consume energy, and therefore the number of actions is limited by the capacity of the sensor’s batteries (Padhy et al. 2010). Furthermore, many of these scenarios require that sensors learn the optimal sequence of actions that can be performed, with the goal of maximising the long term value of the actions they take (Tran-Thanh et al., 2011). In such settings, each action can be considered as an arm, with a cost equal to the amount of energy needed to perform that task. Now, because the battery is limited, both the exploration (i.e. learning the rewards tasks) and exploitation (i.e. taking the optimal actions given reward estimates) phases are budget limited. Against this background, Tran-Thanh et al. (2010) showed that the budget– limited MAB cannot be derived from any other existing MAB model, and therefore, previous MAB learning methods are not suitable to efficiently deal with this problem.",
      "startOffset" : 227,
      "endOffset" : 1902
    }, {
      "referenceID" : 8,
      "context" : ", where B is the budget limit, whereas the theoretical best possible regret bound is typically a logarithmic function of the number of pulls (Lai and Robbins, 1985).",
      "startOffset" : 141,
      "endOffset" : 164
    }, {
      "referenceID" : 7,
      "context" : "However, since unbounded knapsack problems are known to be NP–hard, the algorithm uses an efficient approximation method taken from the knapsack literature, called the density–ordered greedy approach, in order to estimate the best set (Kohli et al., 2004).",
      "startOffset" : 235,
      "endOffset" : 255
    }, {
      "referenceID" : 7,
      "context" : "In more detail, the density– ordered greedy algorithm has O (K logK) computational complexity, where K is the number of item types (Kohli et al., 2004).",
      "startOffset" : 131,
      "endOffset" : 151
    }, {
      "referenceID" : 1,
      "context" : "Thus, we explore and exploit at the same time (for more details, see (Audibert et al., 2009; Auer et al., 2002)).",
      "startOffset" : 69,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "Thus, we explore and exploit at the same time (for more details, see (Audibert et al., 2009; Auer et al., 2002)).",
      "startOffset" : 69,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "That is, fractional KUBE can also be seen as the budget–limited version of UCB (see (Auer et al., 2002) for more details of UCB).",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "To prove this theorem, we will make use of the following version of the Chernoff– Hoeffding concentration inequality for bounded random variables: Theorem 2 (Chernoff–Hoeffding inequality (Hoeffding, 1963)) Let X1, X2, .",
      "startOffset" : 188,
      "endOffset" : 205
    }, {
      "referenceID" : 5,
      "context" : "To prove this theorem, we will make use of the following version of the Chernoff– Hoeffding concentration inequality for bounded random variables: Theorem 2 (Chernoff–Hoeffding inequality (Hoeffding, 1963)) Let X1, X2, . . . , Xn denote the sequence of random variables with common range [0, 1], such that for any 1 ≤ t ≤ n, we have E [Xt|X1, . . . , Xt−1] = μ. Let Sn = 1 n ∑n t=1 Xt. Given this, for any δ ≥ 0, we have: P (Sn ≥ μ+ δ) ≤ e −2nδ , P (Sn ≤ μ− δ) ≤ e −2nδ . The proof can be found, for example, in Hoeffding (1963). We now focus on the performance analysis of KUBE.",
      "startOffset" : 82,
      "endOffset" : 529
    }, {
      "referenceID" : 2,
      "context" : "In this case, the proof of the theorem for that particular value of T is along the same lines as that of Theorem 1 of (Auer et al., 2002).",
      "startOffset" : 118,
      "endOffset" : 137
    }, {
      "referenceID" : 8,
      "context" : "According to (Lai and Robbins, 1985), the best possible regret that an arm pulling algorithm can achieve within the domain of standard MABs is C ln (T ).",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "This implies that both KUBE and fractional KUBE achieve O (lnT ) regret within the standard MAB domain, which is optimal (Auer et al., 2002; Lai and Robbins, 1985).",
      "startOffset" : 121,
      "endOffset" : 163
    }, {
      "referenceID" : 8,
      "context" : "This implies that both KUBE and fractional KUBE achieve O (lnT ) regret within the standard MAB domain, which is optimal (Auer et al., 2002; Lai and Robbins, 1985).",
      "startOffset" : 121,
      "endOffset" : 163
    }, {
      "referenceID" : 13,
      "context" : "POKER(Vermorel and Mohri, 2005), or UCB).",
      "startOffset" : 5,
      "endOffset" : 31
    } ],
    "year" : 2012,
    "abstractText" : "In budget–limited multi–armed bandit (MAB) problems, the learner’s actions are costly and constrained by a fixed budget. Consequently, an optimal exploitation policy may not be to pull the optimal arm repeatedly, as is the case in other variants of MAB, but rather to pull the sequence of different arms that maximises the agent’s total reward within the budget. This difference from existing MABs means that new approaches to maximising the total reward are required. Given this, we develop two pulling policies, namely: (i) KUBE; and (ii) fractional KUBE. Whereas the former provides better performance up to 40% in our experimental settings, the latter is computationally less expensive. We also prove logarithmic upper bounds for the regret of both policies, and show that these bounds are asymptotically optimal (i.e. they only differ from the best possible regret by a constant factor).",
    "creator" : "LaTeX with hyperref package"
  }
}