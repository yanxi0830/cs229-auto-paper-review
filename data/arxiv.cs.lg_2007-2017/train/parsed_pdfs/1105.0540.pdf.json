{
  "name" : "1105.0540.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Pruning nearest neighbor cluster trees",
    "authors" : [ "Samory Kpotufe" ],
    "emails" : [ "SAMORY@TUEBINGEN.MPG.DE", "ULRIKE.LUXBURG@TUEBINGEN.MPG.DE" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n10 5.\n05 40\nv2 [\nst at\n.M L\n] 5\nM ay\n2 01\nOur first contribution is a statistical analysis that reveals how certain subgraphs of a k-NN graph form a consistent estimator of the cluster tree of the underlying distribution of points. Our second and perhaps most important contribution is the following finite sample guarantee. We carefully work out the tradeoff between aggressive and conservative pruning and are able to guarantee the removal of all spurious cluster structures at all levels of the tree while at the same time guaranteeing the recovery of salient clusters. This is the first such finite sample result in the context of clustering."
    }, {
      "heading" : "1. Introduction",
      "text" : "In this work, we consider the nearest neighbor (k-NN) graph where each sample point is linked to its nearest neighbors. These graphs are widely used in machine learning and data mining applications, and interestingly there is still much to understand about their expressiveness. In particular we would like to better understand what such a graph on a finite sample of points might reveal about the cluster structure of the underlying distribution of points. More importantly we are interested in whether one can identify spurious structures that are artifacts of sampling variability, i.e. spurious structures that are not representative of the true cluster structure of the distribution.\nOur first contribution is in exposing more of the richness\nAppearing in Proceedings of the 28 th International Conference on Machine Learning, Bellevue, WA, USA, 2011. Copyright 2011 by the author(s)/owner(s).\nFigure 1. A density f (black line) and its cluster tree (dashed). The CCs of 3 level sets are shown in lighter color at the bottom.\nof k-NN graphs. Let Gn be a k-NN graph over an nsample from a distributionF with density f . Previous work (Maier et al., 2009) has shown that the connected components (CC) of a given level set of f can be approximated by the CCs of some subgraph of Gn, provided the level set satisfies certain boundary conditions. However it remained unclear whether or when all level sets of f might satisfy these conditions, in other words, whether the CCs of any level set can be recovered. We show under mild assumptions on f that CCs of any level set can be recovered by subgraphs of Gn for n sufficiently large. Interestingly, these subgraphs are obtained in a rather simple way: just remove points from the graph in decreasing order of their k-NN radius (distance to the k’th nearest neighbor), and we obtain a nested hierarchy of subgraphs which approximates the cluster tree of F , i.e. the nested hierarchy formed by the level sets of f (see Figure 1, also Section 2.1).\nOur second, and perhaps more important contribution is in providing the first concrete approach in the context of clustering that guarantees the pruning of all spurious cluster structures at any tree level. We carefully work out the tradeoff between pruning “aggressively” (and potentially removing important clusters) and pruning “conservatively” (with the risk of keeping spurious clusters) and derive tuning settings that require no knowledge of the underlying distribution beyond an upper bound on f . We can thus guarantee in a finite sample setting that (a) all clusters remaining at any level of the pruned tree correspond to CCs of some level set of f , i.e. all spurious clusters are pruned away, and (b) salient clusters are still discovered, where the\ndegree of saliency depends on the sample size n. We can show furthermore that the pruned tree remains a consistent estimator of the underlying cluster tree, i.e. the CCs of any level set of f are recovered for sufficiently large n. Interestingly, the pruning procedure is not tied to the k-NN method, but is based on a simple intuition that can be applied to other cluster tree methods (see Section 3).\nOur results rely on a central “connectedness” lemma (Section 5.2) that identifies which CCs of f remain connected in the empirical tree. This is done by analizing the way in which k-NN radii vary along a path in a dense region."
    }, {
      "heading" : "1.1. Related work",
      "text" : "Recovering the cluster tree of the underlying density is a clean formalism of hierarchical clustering proposed in 1981 by J. A. Hartigan (Hartigan, 1981). Hartigan showed in the same seminal paper that the single-linkage algorithm is a consistent estimator of the cluster tree for densities on R. For Rd, d > 1 it is known that the empirical cluster tree of a consistent density estimate is a consistent estimator of the underlying cluster tree (see e.g. (Wong & Lane, 1983)), unfortunately there is no known algorithm for computing this empirical tree. Nonetheless, the idea has led to the development of interesting heuristics based on first estimating density, then approximating the cluster tree of the density estimate in high dimension (Wong & Lane, 1983; Stueltze & Nugent, 2010).\nMany other related work such as (Rigollet & Vert, 2009; Singh et al., 2009; Maier et al., 2009; Rinaldo & Wasserman, 2010) consider the task of recovering the CCs of a single level set, the closest to the present work being (Maier et al., 2009) which uses a k-NN graph for level set estimation. As previously discussed, level set estimation however never led to a consistent estimator of the cluster tree, since these results typically impose technical requirements on the level set being recovered but do not work out how or when these requirements might be satisfied by all level sets of a distribution.\nA recent insightful paper of Chaudhuri & Dasgupta (2010) presents the first provably consistent algorithm for estimating the cluster tree. At each level of the empirical cluster tree, they retain only those samples whose k-NN radii are below a scale parameter r which indexes the level; CCs at this level are then discovered by building an rneighborhood graph on the retained samples. This is similar to an earlier generalization of single-linkage by Wishart (1969) which however was given without a convergence analysis. The k-NN tree studied here differs in that, at an equivalent level r, points are connected to the subset of their k-nearest neighbors retained at that level. One practical appeal of our method is its simplicity: we need only remove points from an initial k-NN graph to obtain the var-\nious levels of the empirical cluster tree.\n(Chaudhuri & Dasgupta, 2010) provides finite sample results for a particular setting of k ≈ logn. In contrast our finite sample results are given for a wide range of values of k, namely for logn . k . n1/O(d). In both cases the finite sample results establish natural separation conditions under which the CCs of level sets are recovered (see Theorem 1). The result of (Chaudhuri & Dasgupta, 2010) however allows the possibility that some empirical clusters are just artifacts of sampling variability. We provide a simple pruning procedure that ensures that clusters discovered empirically at any level correspond to true clusters at some level or the underlying cluster tree. Note that this can be trivially guaranteed by returning a single cluster at all levels, so we additionally guarantee that the algorithm discovers salient modes of the density, where the saliency depends on empirical quantities (see Theorem 2).\nA recent archived paper (Rinaldo et al., 2010) also treats the problem of false clusters in cluster tree estimation, but the result is not algorithmic as they only consider the cluster tree of an empirical density estimate, and do not provide a way to compute this cluster tree.\nThere exist many pruning heuristics in the literature which typically consist of removing small clusters (Maier et al., 2009; Stueltze & Nugent, 2010) using some form of thresholding. The difficulty with these approaches is in how to define small without making strong assumptions on the unknown underlying distribution, or on the tree level being pruned (levels correspond to different resolutions or cluster sizes). Moreover, even the assumption that spurious clusters must be small does not necessarily hold. Consider for example a cluster made up of two large regions connected by a thin bridge of low mass; the two large regions can easily appear as two separate clusters in a finite sample. Some more sophisticated methods such as (Stueltze & Nugent, 2009) do not rely on cluster size for pruning, instead they return confidence values for the empirical clusters based on various notions of cluster stability; unfortunately they do not provide finite sample guarantees. Our pruning guarantees the removal of all spurious clusters, large and small (see Figure 2); we make no assumption on the shape of clusters beyond a smoothness assumption on the density; we provide a simple tuning parameter whose setting requires just an upper bound on the density."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "Assume the finite dataset X = {Xi}ni=1 is drawn i.i.d. from a distribution F over Rd with density function f . We start with some simple definitions related to k-NN operations. All balls, unless otherwise specified, denote closed balls in Rd.\nDefinition 1 (k-NN radii). For x ∈ X , let rk,n(x) denote the radius of the smallest ball centered at x containing k points from X \\ {x}. Also, let rk(x) denote the radius of the smallest ball centered at x of F -mass k/n. Definition 2 (k-NN and mutual k-NN graphs). The kNN graph is that whose vertices are the points in X, and where Xi is connected to Xj iff Xi ∈ B(Xj , θrk(Xj)) or Xj ∈ B(Xi, θrk(Xi)) for some θ > 0. The mutual k-NN graph is that where Xi is connected to Xj iff Xi ∈ B(Xj , θrk(Xj)) and Xj ∈ B(Xi, θrk(Xi))."
    }, {
      "heading" : "2.1. Cluster tree",
      "text" : "Definition 3 (Connectedness). We say A ⊂ Rd is connected if for every x, x′ ∈ A there exists a continuous 1−1 function P : [0, 1] 7→ A where P (0) = x and P (1) = x′. P is called a path in A between x and x′.\nThe cluster tree of f will be denoted {G(λ)}λ>0, where G(λ) are the CCs of the level set {x : f(x) ≥ λ}. Notice that {G(λ)}λ>0 forms a (infinite) tree hierarchy where for any two components A,A′, either A ∩ A′ = ∅ or one is a descendant of the other, i.e A ⊂ A′ or A′ ⊂ A."
    }, {
      "heading" : "3. Algorithm",
      "text" : "Definition 4 (k-NN density estimate). Define the density estimate at x ∈ Rd as :\nfn(x) . =\nk\nn · vol (B(x, rk,n(x))) =\nk\nn · vdrdk,n(x) ,\nwhere vd is the volume of the unit ball in Rd.\nLet Gn be the k-NN or mutual k-NN graph. For λ > 0 define Gn(λ) as the subgraph of Gn containing only vertices in {Xi : fn(Xi) ≥ λ} and corresponding edges. The CCs of {Gn(λ)}λ>0 form a tree: let An and A′n be two such CCs, either An ∩ A′n = ∅ or one is a descendant of the other, i.e. An is a subgraph of A′n or vice versa. To simplify notation, we let the set {Gn(λ)}λ>0 denote the empirical cluster tree before pruning.\nPruning\nThe pruning procedure (Algorithm 1) consists of simple lookups: it reconnects CCs at level λ if they are part of the same CC at level λ − ǫ̃ where the tuning parameter ǫ̃ ≥ 0 controls how aggressively we prune. We show its behavior on a finite sample in Figure 2.\nThe intuition behind the procedure is the following. Suppose An, A′n ⊂ X are disconnected at some level λ in the empirical tree before pruning. However, they ought to be connected, i.e. their vertices belong to the same CC A at the highest level where they are all contained in the underlying cluster tree. Then, key sample points from A that\nwould have kept them connected are missing at level λ in the empirical tree. These key points have fn values lower than λ, but probably not much lower. By looking down to a lower level near λ we find that An, A′n are connected and thus detect the situation. Notice that this intuition is not tied to the k-NN cluster tree but can be applied to any other cluster tree procedure. All that is required is that all points from A (as discussed above) be connected at some level in the tree close to λ.\nAlgorithm 1 Prune Gn(λ) Given: tuning parameter ǫ̃ ≥ 0, same for all levels. G̃n(λ)← Gn(λ). if λ > ǫ̃ then\nConnect components An, A′n of G̃n(λ) if they are part of the same component of Gn(λ− ǫ̃).\nelse Connect all G̃n(λ). end if\nIt is not hard to see that the CCs of the pruned subgraphs{ G̃n(λ) }\nλ>0 still form a tree. We will hence denote the\npruned empirical tree by { G̃n(λ) }\nλ>0 ."
    }, {
      "heading" : "4. Results Overview",
      "text" : "We make the following assumptions on the density f .\n(A.1) ∃F > 0, supx∈Rd f(x) ≤ F . (A.2) f is Hoelder-continuous, i.e. there exists L, α > 0\nsuch that for all x, x′ ∈ Rd,\n|f(x)− f(x′)| ≤ L ‖x− x′‖α .\nTheorem 1 below is a finite sample result that establishes conditions under which samples from a connected subset\nof Rd remain connected in the empirical cluster tree, and samples from two disconnected subsets of Rd remain disconnected even after pruning. Essentially, for k sufficiently large, points from connected subsets A remain connected below some level. Also, provided k is not too large, disjoint subsets A and A′ which are separated by a large enough region of low density (relative to n, k and ǫ̃), remain disconnected above some level.\nWe require the following two definitions.\nDefinition 5 (Envelope of A ⊂ Rd). Let A ⊂ Rd and for r > 0, define: A+r . = {y : ∃x ∈ A, y ∈ B(x, r)} .\nDefinition 6 ((ǫ, r)-separated sets ). A,A′ ⊂ Rd are (ǫ, r)separated if there exists a separating set S such that every path in Rd between A and A′ intersects S, and\nsup x∈S+r f(x) ≤ inf x∈A∪A′ f(x)− ǫ.\nTheorem 1. Suppose f satisfies (A.1) and (A.2). Let Gn be the k-NN or mutual k-NN graph. Let δ > 0 and define ǫk . = 11F √ ln(2n/δ)/k. There exist C and C′ = C′(F) such that, for\nC ( max { 1, √ 2/θ })d d ln(n/δ)\n≤ k ≤ C′ ( F √ ln(n/δ) )2(α+d)/(3α+d) n2α/(3α+d) (1)\nthe following holds with probability at least 1 − 3δ simultaneously for subsets A of Rd.\n(a) Let A be a connected subset of Rd, and let λ . =\ninfx∈A f(x) > 2ǫk. All points in A ∩ X belong to the same CC of G̃n(λ− 2ǫk).\n(b) Let A and A′ be two disjoints subsets of Rd, and define λ = infx∈A∪A′ f(x). Recall that ǫ̃ ≥ 0 is the tuning parameter. Suppose A and A′ are (ǫ, r)-separated for\nǫ = 6ǫk + 2ǫ̃ and r = θ2 (4k/vdnλ) 1/d. Then A ∩X and A′ ∩X are disconnected in G̃n(λ− 2ǫk).\nTheorem 1 above, although written in terms of G̃n, applies also to Gn by just setting ǫ̃ = 0. The theorem implies consistency of both pruned and unpruned k-NN trees under mild additional conditions. Some such conditions are illustrated in the corollary below. A nice practical aspect of the pruning procedure is that consistency is obtained for a wide range of settings of ǫ̃ and k as functions of n.\nCorollary 1 (Consistency). Suppose that f satisfies (A.1) and (A.2) and that, in addition, F is supported on a compact set, and for any λ > 0, there are finitely many components in G(λ). Assume that, as n → ∞, ǫ̃ = ǫ̃(n) → 0 and k/ logn → 0 while k = k(n) satisfies (1).\nFor any A ⊂ Rd, let An denote the smallest component of { G̃n(λ) }\nλ>0 containing A ∩ X. Fix λ > 0. We have\nlimn→∞ P (∀A,A′ ∈ G(λ), An is disjoint from A′n) = 1.\nProof. Let A and A′ be separate components of G(λ). The assumptions ensure that all paths between A and A′ traverse a compact set S satisfying λ−maxx∈S f(x) .= ǫS > 0 (see Lemma 14 of (Chaudhuri & Dasgupta, 2010)). Let ǫ = 6ǫk + 2ǫ̃ and r = θ2 (4k/vdnλ)\n1/d. By uniform continuity of f , there exists N1 such that for n > N1, r is small enough so that λ − maxx∈S+r f(x) > ǫS/2. Also, there exists N2 > N1 such that for n > N2, ǫ < ǫS/2, in other words supx∈S+r f(x) ≤ λ− ǫ. Since Gn(λ) is finite, there exists N such that for n > N , all pairs A,A′ have a suitable (ǫ, r)-separating set S. Thus by Theorem 1, for n > N , with probability at least 1− 3δ, ∀A,A′ ∈ G(λ), A ∩X and A′ ∩X are fully contained in G̃n(λ− 2ǫk) and are disjoint. They are thus disjoint at any higher level, so An and A′n are also disjoint.\nThe above holds for all δ > 0, so the statement follows.\nWhile Theorem 1 establishes that a connected set A remains connected below some level, it does not guarantee against parts of A becoming disconnected at higher levels, creating spurious clusters. Note that the removal of spurious clusters can be trivially guaranteed by just letting the parameter ǫ̃ very large, but the ability of the algorithm to discover true clusters is necessarily affected. We are interested in how to set ǫ̃ in order to guarantee the removal of spurious clusters while still recovering important ones.\nTheorem 2 guarantees that, by setting ǫ̃ as Ω(ǫk) (recall ǫk from Theorem 1), separate CCs of the empirical cluster tree correspond to actual clusters of the (unknown) underlying distribution, i.e. all spurious clusters are removed. The setting of ǫ̃ only requires an upper-boundF on the density f 1. Note that, under such a setting, consistency is maintained per Corollary 1, and in light of Theorem 1 (b), we can expect that interesting clusters are discovered. In particular the following salient modes of f are discovered.\nDefinition 7 ((ǫ, r)-salient mode). An (ǫ, r)-salient mode is a leaf node A of the cluster tree {G(λ)}λ>0 which has an ancestor Ak ⊃ A (possibly A itself) satisfying:\n(i) Ak is the ancestor of a single leaf of {G(λ)}λ>0, namely A.\n(ii) Ak is large: ∃x ∈ Ak, B(x, rk(x)) ⊂ Ak. 1We might just use maxi∈[n] fn(Xi) in practice, which in\nlight of Lemma 1 can be a good surrogate for F (see Figure 3).\n(iii) Ak is sufficiently separated from other components at its level: let λ . = infx∈Ak f(x); Ak and\n({x : f(x) ≥ λ} \\Ak) are (ǫ, r)-separated.\nNotice that, under the assumptions of Corollary 1, every mode of f is (ǫ, r)-salient for sufficiently large k and 1/ǫ̃.\nTheorem 2 (Pruning guarantees). Let δ > 0. Under the assumptions of Theorem 1, the following holds with probability at least 1− 3δ.\n(a) Suppose the tuning parameter ǫ̃ ≥ 3ǫk. Consider two disjoint CCs An and A′n at the same level in{ G̃n(λ) }\nλ>0 . Let V be the union of vertices of An\nand A′n, and define λ . = infx∈V f(x). The vertices of An and those of A′n are in separate CCs of G(λ).\n(b) Let ǫ = 6ǫk + 2ǫ̃ and r = θ2 (4k/vdnλ) 1/d. There\nexists a 1 − 1 map from the set of (ǫ, r)-salient modes to the leaves of the empirical tree { G̃n(λ) }\nλ>0 .\nThe behavior of both the k-NN and mutual k-NN tree, as guaranteed in Theorem 2, is illustrated in Figure 3."
    }, {
      "heading" : "5. Analysis",
      "text" : "Theorem 1 follows from lemmas 3 and 6 below. These two lemmas depend on the events described by lemmas 1, 2 and 4 which happen with a combined probability of at least 1− 3δ for a confidence parameter δ > 0.\nTheorem 2 follows from lemmas 5 and 7 below. These two lemmas also depend on the events described by lemmas 1, 2 and 4 which happen with a combined probability of at least 1− 3δ."
    }, {
      "heading" : "5.1. Maintaining Separation",
      "text" : "In this section we establish conditions under which points from two disconnected subsets of Rd remain disconnected in the empirical tree, even after pruning.\nThe following is an important lemma which establishes the estimation error of fn relative to f on the sample X. Interestingly, although of independent interest, we could not find this sort of finite sample statement in the literature on k-NN2, at least not under our assumptions. The proof, presented as supplement in the appendix, is a bit involved and starts with some intuition from an asymptotic analysis of (Devroye & Wagner, 1977) combined with a form of the Chernoff bound found in (Angluin & Valiant, 1979).\nLemma 1. Suppose f satisfies (A.1) and (A.2). There exists C = C(F) such that for δ > 0, for ǫ = 11F √ ln(2n/δ)/k and\n121 ln(2n/δ)\n≤ k ≤ C ( F √ ln(2n/δ) )2(α+d)/(3α+d) n2α/(3α+d),\nwe have with probability at least 1 − δ that supXi∈X |fn(Xi)− f(Xi)| ≤ ǫ.\nThe next lemma bounds rk,n(Xi) in terms of rk(Xi), and hence, in terms of the density at Xi. The proof is provided as supplement in the appendix.\nLemma 2. Suppose f satisfies (A.1) and (A.2). Fix λ > 0 and let Lλ .= {x : f(x) ≥ λ}.\n(a) Let r . = 12 (λ/2L)\n1/α. We have ∀x, x′ ∈ Rd, ‖x− x′‖ ≤ 2r =⇒ |f(x)− f(x′)| ≤ λ/2. If in addition x ∈ Lλ, it follows that f(x)/2 ≤ f(x′) ≤ 2f(x).\n(b) Suppose k ≤ 2−(d+3)vd(2L)−d/αλ(d+α)/αn. We have\n∀x ∈ Lλ, rk(x) ≤ min { 2−3/dr, ( 2k\nvdnf(x)\n)1/d} .\nFor δ > 0, if in addition k ≥ 192 ln(2n/δ), we have with probability at least 1− δ that for all Xi ∈ X∩Lλ\n2−3/drk(Xi) ≤ rk,n(Xi) ≤ 23/drk(Xi).\nThe main separation lemma is next. It says that if A and A′ are separated by a sufficiently large low density region, then they remain separated in the empirical tree.\n2There are however many asymptotic analyses of k-NN methods such as (Devroye & Wagner, 1977).\nLemma 3 (Separation). Suppose f satisfies (A.1) and (A.2). Let Gn be the k-NN or mutual k-NN graph. Define ǫk . = 11F √ ln(2n/δ)/k, and let δ > 0. There exists C = C(F) such that, for 192 ln(2n/δ) ≤ k\n≤ C ( F √ ln(n/δ) )2(α+d)/(3α+d) n2α/(3α+d),\nthe following holds with probability at least 1 − 2δ simultaneously for any two disjoint subsets A,A′ of Rd.\nLet λ = infx∈A∪A′ f(x). If A and A′ are (ǫ, r)-separated for ǫ = 6ǫk+2ǫ̃ and r = θ2 (4k/vdnλ)\n1/d, then A∩X and A′ ∩X are disconnected in Gn(λ− 2ǫk − ǫ̃) and therefore in G̃n(λ− 2ǫk).\nProof. Applying Lemma 1, it’s immediate that, with probability at least 1 − δ, all points of any A ∪ A′ ∩ X are in Gn(λ − ǫk) and lower levels, and no point from S+r ∩X is in Gn(λ − 5ǫk − 2ǫ̃) or higher levels. Thus any path between A and A′ in Gn(λ − 2ǫk − ǫ̃) must have an edge through the center x ∈ S of a ball B(x, r) ⊂ S+r. This edge must therefore have length greater than 2r. We just need to show that no such edge exists in Gn(λ− 2ǫk − ǫ̃). Let V be the set of points (vertices) in Gn(λ−2ǫk− ǫ̃). By Lemma 1, minXi∈V f(Xi) ≥ λ−3ǫk−ǫ̃. Given the density assumption on S, λ ≥ 6ǫk + 2ǫ̃ so minXi∈V f(Xi) ≥ λ/2 and V ⊂ Lǫk . Now, given the range of k, Lemma 2 holds for the level set Lǫk . It follows that with probability at least 1−δ (uniform over any such choice of A,A′ since the event is a function of Lǫk ),\nmax Xi∈V rk,n(Xi) ≤ 23/d max Xi∈V\nrk(Xi) ≤ 2r\nθ .\nThus, edge lengths in Gn(λ− 2ǫk − ǫ̃) are at most 2r."
    }, {
      "heading" : "5.1.1. IDENTIFYING MODES",
      "text" : "As a corollary to Lemma 3, we can guarantee in Lemma 5 that certain salient modes are recovered by the empirical cluster tree. For this to happen, we require in Definition 7 (ii) that an (ǫ, r)-salient mode A is contained in a sufficiently large set Ak so that we sample points near the mode.\nWe start with the following VC lemma establishing conditions under which subsets of Rd contain samples from X.\nLemma 4 (Lemma 5.1 of (Bousquet et al., 2004)). Suppose C is a class of subsets of Rd. Let SC(2n) denote the 2n-shatter coefficient of C. Let Fn denote the empirical distribution over n samples drawn i.i.d from F . For δ > 0, with probability at least 1− δ,\nsup A∈C F(A)−Fn(A)√ F(A)\n≤ 2 √\nlogSC(2n) + log 4/δ n .\nLemma 5 (Modes). Suppose f satisfies (A.1) and (A.2). Let Gn be the k-NN or mutual k-NN graph. Let δ > 0. There exist C and C′ = C′(F) such that, for\nCd ln(n/δ)\n≤ k ≤ C′ ( F √ ln(n/δ) )2(α+d)/(3α+d) n2α/(3α+d)\nthe following holds with probability at least 1 − 3δ. Let ǫ = 6ǫk + 2ǫ̃ and r = θ2 (4k/vdnλ)\n1/d. There exists a 1− 1 map from the set of (ǫ, r)-salient modes to the leaves of the empirical tree { G̃n(λ) }\nλ>0 .\nProof. First, with probability at least 1 − δ, for any (ǫ, r)salient mode A, there are samples in X from the containing set Ak (as defined in Definition 7). To arrive at this we apply Lemma 4 for the class C of all possible balls B ∈ Rd, (for this class SC(2n) ≤ (2n)d+1). We have with probability at least 1− δ that for all B, Fn(B) > 0 whenever\nF(B) ≥ Cd ln(n/δ) n > 4 (d+ 1) log(2n) + log(4/δ) n ,\nwhere C is appropriately chosen to satisfy the last inequality. Now, from the definition of Ak, there exists x such that B(x, rk(x)) ⊂ Ak, while we have F(B(x, rk(x))) = k/n ≥ Cd ln(n/δ)/n, implying that Fn(Ak) ≥ Fn(B(x, rk(x))) ≥ 1/n. As a consequence of the above argument, there is a finite number m of (ǫ, r)-salient modes since each contributes some points to the final sample X. We can therefore arrange them as { Ai\n}m i=1\nso that for i < j, we have λi ≤ λj where λi = infx∈Ai\nk f(x). An injective map can now be constructed iteratively as follows.\nStarting with i = 1, we have by Lemma 3 that, with probability at least 1 − 2δ, Aik ∩ X is disconnected in G̃n(λi − 2ǫk) from all Ajk, j > i. Let U be the union of those CCs of G̃n(λi−2ǫk) containing points from Aik∩X. We’ve already established that U contains no point from any Ajk, j > i. For i > 1, U also contains no point from any Ajk, j < i. This is because, again by Lemma 3, A j k ∩X is disconnected in G̃n(λj − 2ǫk) from Aik ∩ X, therefore disconnected from U since all CCs in U remain connected at lower levels. Now, since U is disconnected from all Ajk, j 6= i, we can just map Ai to any leaf rooted in U , Ai being the unique image of such a leaf."
    }, {
      "heading" : "5.2. Maintaining Connectedness",
      "text" : "In this section we show that sample points from a connected subset A of Rd remain connected in the empirical cluster tree before pruning (therefore also after pruning).\nSimilar to (Chaudhuri & Dasgupta, 2010), for any two points x, x′ ∈ A ∩ X we uncover a path in Gn near\na path P in A that connects the two. The path in Gn (the dashed path depicted below) consists of a sequence x1 = x, x2, . . . , xi = x\n′ of sample points from balls centered on the path P in A (the solid path depicted below). The intuition is that P is a high density route near which we can find enough sample points to connect x and x′.\nx\nx ′\nThe balls centered on P must be chosen sufficiently small and consecutively close so that consecutive terms xi, xi+1 are adjacent in Gn. In (Chaudhuri & Dasgupta, 2010), points are adjacent (at any particular level) whenever they are less than some scale r apart; one can therefore choose balls of the same radius o(r) and consecutively o(r) close. In our particular case, no single scale determines adjacency. Adjacency is determined by the various nearest-neighbor radii and this creates a multiscale effect that complicates the analysis. One way to handle (and effectively get rid of) this multiscale effect is to choose balls on P of the same radius r corresponding to the smallest possible nearestneighbor radius in Gn (restricted to A∩X). However, in order to get samples in such small balls one would need rather large sample size n, so the idea results in weak bounds. We instead use an inductive argument which keeps track of the various scales, the intuition being that nearest-neighborradii have to change slowly along the path P from x to x′.\nLemma 6 (Connectedness). Suppose f satisfies (A.1) and (A.2). Let Gn be the k-NN or mutual k-NN graph. Define ǫk . = 11F √ ln(2n/δ)/k and let δ > 0. There exist C and C′ = C′(F) such that, for\nC ( max { 1, √ 2/θ })d d ln(n/δ)\n≤ k ≤ C′ ( F √ ln(n/δ) )2(α+d)/(3α+d) n2α/(3α+d),\nthe following holds with probability at least 1 − 3δ simultaneously for all connected subsets A of Rd.\nLet λ . = infx∈A f(x) > 2ǫk. All points in A ∩X belong to the same CC of Gn(λ− 2ǫk), therefore of G̃n(λ− 2ǫk).\nProof. First, let C and C′ be large enough for lemmas 1 and 2 to hold. Define r\n. = 12 (ǫk/2L) 1/α. By Lemma 2 (a), we have that f(x) ≥ λ− ǫk/2 for any x ∈ A+r. Applying Lemma 1, it follows that with probability at least 1 − δ (uniform over choices of A), all points of A+r ∩X are in Gn(λ − 2ǫk). We will show that A ∩ X is connected in Gn(λ − 2ǫk) possibly through points in A+r \\A. In particular, any x, x′ ∈ A ∩ X are connected through a sequence {xi}i>1 , xi ∈ A+r ∩ X built according to the\nfollowing procedure. Let P be a path in A between x and x′. Define τ . = min { 1, θ/ √ 2 } .\nStarting at i = 1 (x1 = x), set xi+1 = x′ if ‖xi − x′‖ ≤ θmin {rk,n(xi), rk,n(x′)}, and we’re done, otherwise: Let yi be the point in P∩B ( xi, τ2 −9/drk,n(xi) ) farthest along the path P from x, i.e. P−1(yi) is highest in the set. Define the half-ball\nH(yi) . = {z : ‖z − y‖ < τ2−18/drk,n(xi),\n(z − yi) · (xi − yi) ≥ 0}.\nPick xi+1 in H(yi) ∩X, and continue.\nThe rest of the argument will proceed inductively as follows. First, assume that xi ∈ A+r and that yi exists. This is necessarily the case for x1, y1. Assume xi+1 6= x′. We will show that xi+1 exists, is also in A+r, and is adjacent to xi in Gn. It will follow that yi+1 must exist (if the process does not end) and is distinct from y1, . . . , yi. We’ll then argue that the process must also end.\nTo see that xi+1 exists (under the aforementioned assumptions), we apply Lemma 4 for the class C of all possible half-balls H(y) centered at y ∈ Rd (for this class SC(2n) ≤ (2n)2d+1). We have with probability at least 1− δ that for all H(y), Fn(H(y)) > 0 whenever\nF(H(y)) ≥ C0d ln( n δ )\nn >\n(8d+ 4) log(2n) + 4 log(4δ )\nn ,\nwhere C0 is appropriately chosen to satisfy the last inequality. We next show F(H(yi)) satisfies the first inequality. We first apply Lemma 2 on Lǫk ⊃ A+r (this inclusion was established earlier). We have with probability at least 1 − δ (uniform over all A) that for xi ∈ A+r, rk,n(xi) ≤ 23/drk(xi) ≤ r. Thus, for all z ∈ H(yi),\n‖z − xi‖ ≤ 2 · τ2−9/drk,n(xi) ≤ 2 · τ2−9/dr ≤ 2r, (2)\nimplying by the same Lemma 2 that f(z) ≥ f(xi)/2. Now, from Lemma 1, fn(xi) ≤ f(xi) + ǫk ≤ 2f(xi). We can thus write\nF(H(yi)) ≥ 1\n4 vol\n( B(yi, τ2 −18/drk,n(xi)) ) f(xi)\n= τd2−20 vol (B(xi, rk,n(xi))) f(xi)\n≥ τd2−21 vol (B(xi, rk,n(xi))) fn(xi)\n= τd2−21 k n ≥ C0d ln(n/δ) n , for C ≥ 221C0.\nTherefore there is a point xi+1 in H(yi) ∩X. In addition xi+1 ∈ A+r since it is within r of yi ∈ A.\nNext we establish that there is an edge between xi and xi+1 in Gn. To this end we relate rk,n(xi+1) to rk,n(xi) by first relating rk(xi+1) to rk(xi). Remember that for z ∈ A+r we have rk(z) < r so that for any z′ ∈ B(z, rk(z)) we have f(z)/2 ≤ f(z′) ≤ 2f(z). Also recall that we always have ‖xi − xi+1‖ ≤ 2r (see (2)), implying f(xi+1) < 2f(xi). We then have\nvdr d k(xi) ·\n1 2 f(xi) ≤ k n ≤ vdrdk(xi+1) · 2f(xi+1)\n≤ vdrdk(xi+1) · 4f(xi), where for the first two inequalities we used the fact that both balls B(xi, rk(xi)) and B(xi+1, rk(xi+1)) have the same mass k/n. It follows that\nrk,n(xi+1) ≥ 2−3/drk(xi+1) ≥ 2−6/drk(xi) ≥ 2−9/drk,n(xi), (3)\nimplying 2−9/drk,n(xi) ≤ min {rk,n(xi), rk,n(xi+1)}. We then get\n‖xi − xi+1‖2 = ‖xi − yi‖2 + ‖xi+1 − yi‖2\n− (xi − yi) · (xi+1 − yi) ≤ ‖xi − yi‖2 + ‖xi+1 − yi‖2 ≤ 2τ2 ·min { r2k,n(xi), r 2 k,n(xi+1) } ≤ θ2 min { r2k,n(xi), r 2 k,n(xi+1) } ,\nmeaning xi and xi+1 are adjacent in Gn.\nFinally we argue that yi+1 must exist. By (3) above we have\n‖xi+1 − yi‖ < τ2−18/drk,n(xi) ≤ τ2−9/drk,n(xi+1),\nin other words the ball B ( xi+1, τ2 −9/drk,n(xi+1) )\ncontains yi ∈ P in its interior. It follows by continuity of P that there is a point yi+1 in this ball further along the path from xi than yi. Thus, recursively all yi’s must be distinct, implying that all xi’s must be distinct. Since all xi’s belong to the finite sample X the process must eventually terminate."
    }, {
      "heading" : "5.2.1. PRUNING OF SPURIOUS BRANCHES",
      "text" : "As a corollary to Lemma 6 we can guarantee in Lemma 7 that the pruning procedure will remove all spurious branchings, and hence, all spurious clusters.\nLemma 7 (Pruning). Let δ > 0. Under the assumptions of Lemma 6, the following holds with probability at least 1− 3δ, provided ǫ̃ ≥ 3ǫk. Consider two disjoint CCs An and A′n at the same level in{ G̃n(λ) }\nλ>0 . Let V be the union of vertices of An and\nA′n, and define λ . = infx∈V f(x). The vertices of An and those of A′n are in separate CCs of G(λ).\nProof. Let λn = minx∈V fn(x) be the level in the empirical tree containing An, A′n. By Lemma 1, supx∈X |fn(x)− f(x)| ≤ ǫk so λn ≤ λ + ǫk. Thus, we must have λ > 2ǫk, since otherwise λn ≤ ǫ̃ implying G̃n(λn) must have a single connected component.\nNow suppose points in V were in the same componentA of G(λ). By Lemma 6, all of A ∩X is connected in Gn(λ − 2ǫk) and at lower levels. By the last argumentλn− ǫ̃ ≤ λ− 2ǫk so the pruning procedure reconnects An and A′n."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Sanjoy Dasgupta for interesting discussions which helped improve presentation."
    }, {
      "heading" : "A. Proof of Lemma 1",
      "text" : "Lemma 1 follows as a corollary to Lemma 9 below.\nWe’ll often make use of the following form of the Chernoff bound.\nLemma 8 ((Angluin & Valiant, 1979)). Let N ∼ Bin(n, p). Then for all 0 < t ≤ 1,\nP (N > (1 + t)np) ≤ exp ( −t2np/3 ) , P (N < (1− t)np) ≤ exp ( −t2np/3 ) .\nLemma 9. Suppose the density function f satisfies:\n(a) f is uniformly continuous on Rd. In other words, ∀ǫ > 0, ∃cǫ s.t. for all balls B where vol (B) ≤ cǫ we have supx,x′∈B |f(x)− f(x′)| < ǫ/2.\n(b) ∃F , supx∈Rd f(x) = F .\nFix 0 < ǫ < F , let n ≥ 2, and k < n. If k/nǫ ≤ cǫ/4 then\nP ( sup Xi∈X |f(Xi)− fn(Xi)| > ǫ ) ≤ 2n exp ( − ǫ 2k 120F 2 ) .\nProof. We’ll be using the short-hand notation Bk,n(x) . = B(x, rk,n(x)) for readability in what follows.\nWe start with the simple bound:\nP ( sup Xi∈X |f(Xi)− fn(Xi)| > ǫ ) ≤ P (∃Xi ∈ X, fn(Xi) > f(Xi) + ǫ)+ P (∃Xi ∈ X, fn(Xi) < f(Xi)− ǫ)\n= P ( ∃Xi ∈ X, vol (Bk,n(Xi)) <\nk\nn(f(Xi) + ǫ)\n) +\n(4)\nP ( ∃Xi ∈ X, f(Xi) > ǫ, vol (Bk,n(Xi)) >\nk\nn(f(Xi)− ǫ)\n)\n(5)\nWe handle (4) and (5) by first fixing i and conditioning on Xi = x. We start with (4):\nP ( ∃Xi ∈ X, vol (Bk,n(Xi)) <\nk\nn(f(Xi) + ǫ)\n)\n≤ n ∫\nx\nP ( vol (Bk,n(x)) <\nk\nn(f(x) + ǫ)\n) dF(x), (6)\nwhere the inner probability is over the choice of X \\ {Xi = x} for i fixed. In what follows we use the notation Fn−1 to denote the empirical distribution over X \\ {Xi = x}. Assume vol (Bk,n(x)) < k/n(f(x) + ǫ) < k/nǫ < cǫ. Then by the uniform continuity assumption on f we have\nF (Bk,n(x)) < (f(x) + ǫ/2) k\nn(f(x) + ǫ)\n= ( 1− ǫ\n2(f(x) + ǫ)\n) k\nn ≤\n( 1− ǫ\n4F ) k n\nNow let B(x) be the ball centered at x with F -mass (1− ǫ/4F ) (k/n). Since by the above, F (Bk,n(x)) < F (B(x)), we also have that Fn (Bk,n(x)) < Fn (B(x)). This implies that\nF (B(x)) < ( 1− ǫ\n4F ) k n− 1 = ( 1− ǫ 4F ) Fn (Bk,n(x))\n≤ ( 1− ǫ\n4F\n) Fn (B(x)) .\nIn other words, let t = ǫ/(4F − ǫ), applying the Chernoff bound of Lemma 8, we have\nP (vol (Bk,n(x)) < k/n(f(x) + ǫ))\n≤ P (Fn (B(x)) > (1 + t)F (B(x))) ≤ exp ( −t2(n− 1)F (B(x)) /3 ) ≤ exp ( −ǫ2k/96F 2 ) .\nCombine with (6) to complete the bound on (4).\nWe now turn to bounding (5). We proceed as before by fixing i and integrating over Xi = x where f(x) > ǫ, that is\nP ( ∃Xi ∈ X, f(Xi) > ǫ, vol (Bk,n(Xi)) >\nk\nn(f(Xi)− ǫ)\n)\n≤ n ∫\nx,f(x)>ǫ\nP ( vol (Bk,n(x)) >\nk\nn(f(x)− ǫ)\n) dF(x),\n(7)\nwhere again the probability is over the choice of X \\ {Xi = x}. Now, we can no longer infer how much f deviates within Bk,n(x) from just the event in question (as we did for the other direction). The trick (inspired by (Devroye & Wagner, 1977)) is to consider a related ball.\nLet B(x) be the ball centered at x of volume k/n(f(x) − 3ǫ/4). Then\nvol (Bk,n(x)) > k\nn(f(x)− ǫ) > vol (B(x))\n=⇒ Fn−1 (B(x)) ≤ k − 1 n− 1 < k n .\nSince vol (B(x)) < 4k/ǫ < cǫ, we have by the uniform continuity of f that\nF (B(x)) > k(f(x)− ǫ/2) n(f(x)− 3ǫ/4) >\n( 1 + ǫ\n4F ) k n\n> ( 1 + ǫ\n4F\n) Fn−1 (B(x)) .\nwe thus have for t = ǫ/(4F + ǫ), and using Lemma 8 that\nP ( vol (Bk,n(x)) >\nk\nn(f(x) − ǫ)\n)\n≤ P (Fn (B(x)) ≤ (1− t)F (B(x))) ≤ exp ( −t2(n− 1)F (B(x)) ) ≤ exp ( −ǫ2k/120F 2 ) .\nCombine with (7) to complete the bound on (5).\nThe final result is proved by then combining the bounds on (4) and (5).\nProof of Lemma 1. For any 0 < ǫ < 1, let cǫ = vd2\n−d (ǫ/2L)d/α so that whenever for balls B, vol (B) < cǫ, the radius r of B is less than 12 (ǫ/2L)\n1/α. Thus, supx,x′∈B |f(x)− f(x′)| ≤ L(2r)α < ǫ/2. Now, for the settings of ǫ and k in the lemma statement, we have\n0 < ǫ < F and 4k\nnǫ < vd2 −d ( ǫ 2L )d/α = cǫ,\nso we can apply Lemma 9 to get\nP ( sup Xi∈X |f(Xi)− fn(Xi)| > ǫ ) ≤ 2n exp ( − ǫ 2k 120F 2 )\n< δ."
    }, {
      "heading" : "B. Proof of Lemma 2",
      "text" : "Lemma 2 follows as a corollary to Lemma 10 below.\nLemma 10. Consider a subset A of Rd such that there exists r, satisfying\n∀x ∈ A, ‖x− x′‖ < 2r =⇒ 1 2 f(x) ≤ f(x′) ≤ 2f(x).\nAssume Xi ∈ X ∩ A. We have\nP ( rk,n(Xi) ≥ 23/drk(Xi) | rk(Xi) < 2−3/dr )\n≤ exp (−k/12) , P ( rk,n(Xi) ≤ 2−3/drk(Xi) | rk(Xi) < 2−3/dr )\n≤ exp (−k/192) .\nProof. Let Xi ∈ X, and fix Xi = x ∈ A such that rk(x) < 2−3/dr. We automatically have\n1 2 vol (B(x, rk(x))) f(x) ≤ F (B(x, rk(x)))\n≤ 2 vol (B(x, rk(x))) f(x).\nWe similarly have\nF ( B(x, 23/drk(x)) ) ≥ vol ( B(x, 23/drk(x)) ) f(x) 2\n≥ 8 vol (B(x, rk(x))) f(x)\n2\n≥ 2F (B(x, rk(x))) = 2 k\nn .\nAgain, similarly\nk\n32n =\n1\n32 F (B(x, rk(x))) ≤ F\n( B(x, 2−3/drk(x)) )\n≤ 1 2 F (B(x, rk(x))) = k 2n .\nThus by Lemma 8,\nP ( rk,n(x) > 2 3/drk(x) ) ≤\nP ( Fn−1 ( B(x, 23/drk(x)) ) < k\nn ≤ 1 2 F ( B(x, 23/drk(x))\n))\n≤ exp ( −(n− 1)F ( B(x, 23/drk(x)) ) /12 ) ≤ exp (−k/12) ,\nand\nP ( rk,n(x) < 2 −3/drk(x) ) ≤\nP ( Fn−1 ( B(x, 2−3/drk(x)) ) > k\nn ≥ 2F\n( B(x, 2−3/drk(x))\n))\n≤ exp ( −(n− 1)F ( B(x, 2−3/drk(x)) ) /3 ) ≤ exp (−k/192) .\nConclude by integrating these probabilities over possible values of Xi = x ∈ A.\nProof of Lemma 2. Part (a) follows directly from the Holder assumption on f . For part (b), notice that\nsup x∈Lλ\nvdr d k(x)λ ≤ inf x∈Lλ F (B(x, rk(x))) =\nk\nn\nso that supx∈Lλ rk(x) ≤ 2−3/dr for the setting of k. Now using part (a) again we have for all x ∈ Lλ\nvdr d k(x) ·\nf(x)\n2 ≤ F (B(x, rk(x))) =\nk n ,\nso rk(x) ≤ (2k/vdnf(x))1/d. Finally, the probabilistic statement is obtained by applying Lemma 10 and a union-bound over X ∩ Lλ."
    } ],
    "references" : [ {
      "title" : "Fast probabilistic algorithms for Hamiltonian circuits and matchings",
      "author" : [ "D. Angluin", "L.G. Valiant" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Angluin and Valiant,? \\Q1979\\E",
      "shortCiteRegEx" : "Angluin and Valiant",
      "year" : 1979
    }, {
      "title" : "Introduction to statistical learning theory",
      "author" : [ "O. Bousquet", "S. Boucheron", "G. Lugosi" ],
      "venue" : "Lecture Notes in Artificial Intelligence,",
      "citeRegEx" : "Bousquet et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bousquet et al\\.",
      "year" : 2004
    }, {
      "title" : "Rates of convergence for the cluster tree",
      "author" : [ "K. Chaudhuri", "S. Dasgupta" ],
      "venue" : "Neural Information Processing Systems,",
      "citeRegEx" : "Chaudhuri and Dasgupta,? \\Q2010\\E",
      "shortCiteRegEx" : "Chaudhuri and Dasgupta",
      "year" : 2010
    }, {
      "title" : "The strong uniform consistency of nearest neighbor density estimates",
      "author" : [ "L.P. Devroye", "T.J. Wagner" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Devroye and Wagner,? \\Q1977\\E",
      "shortCiteRegEx" : "Devroye and Wagner",
      "year" : 1977
    }, {
      "title" : "Consistency of single linkage for high-density clusters",
      "author" : [ "J.A. Hartigan" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Hartigan,? \\Q1981\\E",
      "shortCiteRegEx" : "Hartigan",
      "year" : 1981
    }, {
      "title" : "Optimal construction of k-nearest neighbor graphs for identifying noisy clusters",
      "author" : [ "M. Maier", "M. Hein", "U. von Luxburg" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Maier et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Maier et al\\.",
      "year" : 2009
    }, {
      "title" : "Fast rates for plug-in estimators of density level",
      "author" : [ "P. Rigollet", "R. Vert" ],
      "venue" : "sets. Bernouilli,",
      "citeRegEx" : "Rigollet and Vert,? \\Q2009\\E",
      "shortCiteRegEx" : "Rigollet and Vert",
      "year" : 2009
    }, {
      "title" : "Generalized density clustering",
      "author" : [ "A. Rinaldo", "L. Wasserman" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Rinaldo and Wasserman,? \\Q2010\\E",
      "shortCiteRegEx" : "Rinaldo and Wasserman",
      "year" : 2010
    }, {
      "title" : "Stability of density based clustering",
      "author" : [ "A. Rinaldo", "A. Singh", "R. Nugent", "L. Wasserman" ],
      "venue" : null,
      "citeRegEx" : "Rinaldo et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rinaldo et al\\.",
      "year" : 2010
    }, {
      "title" : "Adaptive hausdorff estimation of density level sets",
      "author" : [ "A. Singh", "C. Scott", "R. Nowak" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Singh et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2009
    }, {
      "title" : "Clustering with confidence: A binning approach",
      "author" : [ "W. Stueltze", "R. Nugent" ],
      "venue" : "International Federation Classification Societies Conference,",
      "citeRegEx" : "Stueltze and Nugent,? \\Q2009\\E",
      "shortCiteRegEx" : "Stueltze and Nugent",
      "year" : 2009
    }, {
      "title" : "A generalized single linkage method for estimating the cluster tree of a density",
      "author" : [ "W. Stueltze", "R. Nugent" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "Stueltze and Nugent,? \\Q2010\\E",
      "shortCiteRegEx" : "Stueltze and Nugent",
      "year" : 2010
    }, {
      "title" : "Mode analysis: A generalization of nearest neighbor which reduces chaining effects",
      "author" : [ "D. Wishart" ],
      "venue" : "Numerical Taxonomy,",
      "citeRegEx" : "Wishart,? \\Q1969\\E",
      "shortCiteRegEx" : "Wishart",
      "year" : 1969
    }, {
      "title" : "A kth nearest neighbor clustering procedure",
      "author" : [ "M. Wong", "T. Lane" ],
      "venue" : "Journal of the Royal Statistical Society Series B,",
      "citeRegEx" : "Wong and Lane,? \\Q1983\\E",
      "shortCiteRegEx" : "Wong and Lane",
      "year" : 1983
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Previous work (Maier et al., 2009) has shown that the connected components (CC) of a given level set of f can be approximated by the CCs of some subgraph of Gn, provided the level set satisfies certain boundary conditions.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "Hartigan (Hartigan, 1981).",
      "startOffset" : 9,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "Many other related work such as (Rigollet & Vert, 2009; Singh et al., 2009; Maier et al., 2009; Rinaldo & Wasserman, 2010) consider the task of recovering the CCs of a single level set, the closest to the present work being (Maier et al.",
      "startOffset" : 32,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "Many other related work such as (Rigollet & Vert, 2009; Singh et al., 2009; Maier et al., 2009; Rinaldo & Wasserman, 2010) consider the task of recovering the CCs of a single level set, the closest to the present work being (Maier et al.",
      "startOffset" : 32,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : ", 2009; Rinaldo & Wasserman, 2010) consider the task of recovering the CCs of a single level set, the closest to the present work being (Maier et al., 2009) which uses a k-NN graph for level set estimation.",
      "startOffset" : 136,
      "endOffset" : 156
    }, {
      "referenceID" : 12,
      "context" : "This is similar to an earlier generalization of single-linkage by Wishart (1969) which however was given without a convergence analysis.",
      "startOffset" : 66,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "A recent archived paper (Rinaldo et al., 2010) also treats the problem of false clusters in cluster tree estimation, but the result is not algorithmic as they only consider the cluster tree of an empirical density estimate, and do not provide a way to compute this cluster tree.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "There exist many pruning heuristics in the literature which typically consist of removing small clusters (Maier et al., 2009; Stueltze & Nugent, 2010) using some form of thresholding.",
      "startOffset" : 105,
      "endOffset" : 150
    }, {
      "referenceID" : 1,
      "context" : "1 of (Bousquet et al., 2004)).",
      "startOffset" : 5,
      "endOffset" : 28
    } ],
    "year" : 2011,
    "abstractText" : "Nearest neighbor (k-NN) graphs are widely used in machine learning and data mining applications, and our aim is to better understand what they reveal about the cluster structure of the unknown underlying distribution of points. Moreover, is it possible to identify spurious structures that might arise due to sampling variability? Our first contribution is a statistical analysis that reveals how certain subgraphs of a k-NN graph form a consistent estimator of the cluster tree of the underlying distribution of points. Our second and perhaps most important contribution is the following finite sample guarantee. We carefully work out the tradeoff between aggressive and conservative pruning and are able to guarantee the removal of all spurious cluster structures at all levels of the tree while at the same time guaranteeing the recovery of salient clusters. This is the first such finite sample result in the context of clustering.",
    "creator" : "LaTeX with hyperref package"
  }
}