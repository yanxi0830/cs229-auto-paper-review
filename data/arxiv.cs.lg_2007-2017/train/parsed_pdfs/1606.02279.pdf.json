{
  "name" : "1606.02279.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Semi-supervised structured output prediction by local linear regression and sub-gradient descent",
    "authors" : [ "Ru-Ze Liang", "Wei Xie", "Weizhi Li", "Jing-Yan Wang", "Jingbin Wang" ],
    "emails" : [ "ruzeliang@outlook.com", "wei.xie@vanderbilt.edu", "weizhili2014@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 6.\n02 27\n9v 3\n[ cs\n.L G\n] 1\n6 A\nI. INTRODUCTION\nTraditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8]. However, in many machine learning applications, the forms of outputs of the prediction are structured. The structured outputs include vector, tree nodes, graph, sequence, etc. For example, in the part-of-speech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [9], [10]. In real-world application, many outputs are not available for the training data points, leading to the problem of semi-supervised structured output prediction. Many works have been done for the problem of semi-supervised structured output prediction. Altun et al. [11] proposed the problem of semi-supervised learning with structured outputs. Brefeld and Scheffer [12] proposed to solve the problem of semisupervised structured output prediction by learning in the space of input-output space, and using co-training method. Suzuki et al. [13] proposed a hybrid method to solve the\nproblem of semi-supervised structured output learning. Jiang et al. [14] proposed to regularize the structured outputs by the manifold constructed from the input space directly.\nAll the above semi-supervised structured output prediction methods learn one single predictor for the entire data set, ignoring the different local distributions of different neighborhoods of data points. However, from our observation, the local distributions of different neighborhoods of the data may be significantly different, and this might have a significant effect to the prediction of structured outputs. Thus using one single global predictor for all the different neighborhoods are not suitable. In this paper, we learn multiple local linear structured output predictor for different neighborhoods to model the local distributions, instead of learning one single predictor for the entire data [15], [16], and we also propose to learn the missing structured outputs for a semi- supervised data set simultaneously. For each data point, the local distribution around this data point by its k nearest neighborhood was presented, and we try to model it by learning a local linear structured output predictor. In order to create a local linear structured output predictor, we minimize an upper bound of the structured losses of the data points in this neighborhood, as well as the squared ℓ2 norm of the predictor parameter vector. Some data points are shared by different neighborhoods, playing the role of bridging different local distributions. To solve the problem, we develop an iterative algorithm by using the gradient descent method.\nThe rest parts of this paper are organized as follows. In section II, the proposed maximum top precision similarity learning method is introduced. In section III, the experiments in four benchmark data sets are shown. In section IV, the paper is concluded with future works."
    }, {
      "heading" : "II. PROPOSED METHOD",
      "text" : "Suppose we have a training set X = L ∪ U , where L = {(xi, yi)} l i=1 contains l labeled data points, U = {xi} n i=l+1 contains u = n− l unlabeled data points, and xi and yi ∈ Y are the input vector and structured output of the i-th data point respectively. We propose to learn a local predictor for the neighborhood of each data points, and present the neighborhood of the i-th data point as the set of its k nearest neighbors, Ni. Given a candidate structured output, y, and an input feature vector, xj ∈ Ni, we use a joint representation Φ(xj , y) ∈ Rm to match them, and then we use a local linear predictor to predict the structured outputs for the data points in Ni,\ny∗j = argmax y∈Y w⊤i Φ(xj , y), ∀ j : xj ∈ Ni. (1)\nWe propose to learn the complete outputs simultaneously, {yi}|ni=1, for all the data points. We also assume that the outputs of the data points in L is equal to the true outputs, i.e., yi = yi, for i : (xi, yi) ∈ L.\nIn each neighborhood, Ni, we learn wi and yj |j:xj∈Ni jointly by minimizing a structured loss function ∆ and the squared ℓ2 norm of wi simultaneously\nmin wi,yj|j:xj∈Ni\n1\nk\n∑\nj:xj∈Ni\n∆(yj , y ∗ j ) +\nC 2 ‖wi‖22,\ns.t. yj = yj , j : (xj , yi) ∈ L,\n(2)\nwhere C is a scale parameter. The upper bound of ∆(yj , y∗j ) is given as\n∆(yj , y ∗ j ) ≤ w ⊤ i\n( Φ(xj , z∗j )− Φ(xj , yj) ) +∆(yj , z ∗ i,j), (3)\nwhere\nz∗i,j = argmax y′ j ∈Y\n[ w⊤i ( Φ(xj , y′j)− Φ(xj , yj) ) +∆(yj , y ′ j) ] .\n(4) We approximate the upper bound of the structured loss based on the lower bound approximation method of the structure learning of the Bayesian network [17], [18]. Fan et al. [17] proposed to tighten the upper and lower bounds of the breadthfirst branch and bound algorithm for the learning of Bayesian network structures. The informed variable groupings are used to create the pattern databases to tighten the lower bound, while the anytime learning algorithm is used to tighten the upper bound. These strategies show good performance in the learning process of the Bayesian network structures. The work of [17] is a contribution of major significance to the bound approximation community, and our upper bound approximation method is also based on these strategies. Fan et al. [18] further proposed to improve the lower bound function of static k-cycle conflict heuristic for the learning of Bayesian network structures. This work is used to guild the search of the most promising search spaces. It uses a partition of the random variables of a data set, and their research methodologies are\nbased on the importance of the partition. A new partition method was proposed, and it uses the information extracted from the potentially optimal parent sets.\nThe minimization problem can be transferred to be the following problem,\nmin wi,yj|j:xj∈Ni\n1\nk\n∑\nj:xj∈Ni\n[ w⊤i ( Φ(xj , z∗i,j)− Φ(xj , yj) )\n+∆(yj, z ∗ i,j)\n] + C\n2 ‖wi‖22\ns.t. yj = yj , j : (xj , yi) ∈ L.\n(5)\nWe propose to combine them into one single problem over the entire data set,\nmin (wi,yi)|ni=1\nn ∑\ni=1\n\n\n1\nk\n∑\nj:xj∈Ni\n[ w⊤i ( Φ(xj , z∗i,j)− Φ(xj , yj) )\n+∆(yj , z ∗ i,j)\n] + C\n2 ‖wi‖22\n)\ns.t. yi = yi, i : (xi, yi) ∈ L. (6)\nTo solve the problem in (6), we use an iterative algorithm. In each iteration, we first update wi|ni=1, and then update yi| n i=1 one by one.\n• Updating wi by sub-gradient descent algorithm If we only consider wi|ni=1, the problem in (6) is transferred to\nmin wi|ni=1\nn ∑\ni=1\n\n\n1\nk\n∑\nj:xj∈Ni\n[ w⊤i ( Φ(xj , z∗i,j)− Φ(xj , yj) )]\n+ C\n2 ‖wi‖22 = g(wi)\n\n ,\n(7) We use the sub-gradient descent algorithm to update wi, and the sub-gradient function of g(wi) is\n∇g(wi) = 1\nk\n∑\nj:xj∈Ni\n[( Φ(xj , z∗i,j)− Φ(xj , yj) )] + Cwi,\n(8) and wi is updated as follows,\nwi ← wi − η∇g(wi). (9)\n• Updating yi|ni=1 If we only consider yi| n i=1, the problem\nof (6) is transferred to\nmin yi|ni=1\nn ∑\ni=1\n\n\n1\nk\n∑\nj:xj∈Ni\n[\n∆(yj , z ∗ i,j)− w ⊤ i Φ(xj , yj)\n]\n\n\ns.t. yi = yi, i : (xj , yi) ∈ L. (10)\nWe solve the n outputs one by one, and the problem in (10) is reduced to be\nmin yi\n∑\ni′:xi∈Ni′\n{\n1\nk\n[\n∆(yi, z ∗ i′,i)− w ⊤ i′Φ(xi, yi)\n]\n}\ns.t. yi = yi, if (xi, yi) ∈ L,\n(11)\nwith regard to only one output. We discuss the solution of this problem in two cases.\n– (xi, yi) ∈ L:\nyi = yi. (12)\n– xi ∈ U :\nyi = argmin y∈Y\n∑\ni′:xi∈Ni′\n(\n1\nk\n[\n∆(y, z∗i′,i)\n−w⊤i′Φ(xi, y) ]\n)\n.\n(13)\nWe develop an iterative algorithm to learn the local structured output predictor and the outputs jointly, as given in Algorithm 1.\n• Algorithm 1. Iterative training algorithm of semisupervised learning of local structured output predictor. • Inputs: Training set, X . • Inputs: Maximum iteration number, T . • Initialize (wi, yi)|ni=1; • For t = 1, · · · , T\n– Update the upper bound parameters – For i = 1, · · · , n\n∗ For j : xj ∈ Ni ∗ Update z∗i,j according to (4)\n– Update the local predictor parameters – For i = 1, · · · , n – Update wi according to (9) – Update the structured outputs – For i = 1, · · · , n – Update yi according to (12) and (13)\n• Output: wi|ni=1."
    }, {
      "heading" : "III. EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "A. Experiment setup",
      "text" : "We use two benchmark data sets in our experiments.\n• SUN data set:The class labels of this image data set is organized as a tree structure. The tree has 15 different leaves [19]. For each class, we randomly select 200 images from the data set to conduct our experiments, thus there are 3,000 images in our data set in total. The structured output of a data point is a leave of the tree. We code it as a vector x. The joint representation is given as the tensor product of x and y.\nΦ(x, y) = x ⊗ y. (14)\nThe loss function between y and y∗, ∆(y, y∗), is defined as the height of their first common ancestor. • Spanish news wire article sentence data set: This data set contains 300 sentences, and each sentence is used as a data point in our experiment [20]. The length of each sentence is 9, and the corresponding output of a sentence, y, is a sequence of labels of non-name and named entities. The joint representation Φ(x, y) of a sentence, x, and a sequence of labels, y, is defined as the histogram of state transition, and a set of emission features. The loss function to compare y and y∗ is defined as a 0-1 loss.\nWe use the 10-fold cross validation strategy to split the training and test subsets. The training set is also randomly split to be a labeled set and an unlabeled set. The average structured loss over the test set is used to evaluate the performance of the proposed algorithm."
    }, {
      "heading" : "B. Experimental results",
      "text" : "We compare the proposed algorithm to the algorithms proposed by Altun et al. [11], Brefeld and Scheffer [12], Suzuki et al. [13], and Jiang et al. [14]. Our algorithm is named as semi-supervised local structured output prediction algorithm (SSLSOP). The average losses of the compared algorithms over three different data sets are given in Table I. It is obvious that the proposed algorithm outperforms all competing algorithms significantly.\nWe are also interested in the running time of the proposed method, SSLSOP, and its competing methods. The running time of these methods over four benchmark data sets are given in Fig. 1, which shows that the proposed method, SSLSOP, consumes the second shortest running time over three data\nsets. The least time consuming algorithm is the one proposed by Altun et al. [11], however, its prediction results are not accurate."
    }, {
      "heading" : "IV. CONCLUSION",
      "text" : "In this paper, we investigate the problem of semi-supervised learning of structured output predictor. To handle the problem of diverse of the local distributions, we propose to learn local structured output predictors for neighborhoods of different data points. Moreover, we also propose to learn the missing outputs of the unlabeled data points. We build a new minimization problem to learn the local structured output predictors and the missing structured outputs simultaneously. This problem is modeled as the joint minimization of the local predictor complexity and the local structured output loss. The problem is optimized by gradient descent, and we design an iterative algorithm to learn the local predictors. The experiments are implemented over benchmark data sets including natural image classification data set and sentence part-of-speech tagging data set. In the future, we will study how to fit the proposed algorithm to big data sets, by using big data processing framework, such as Map-Reduce of Hadoop software. We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43]."
    } ],
    "references" : [ {
      "title" : "An effective image representation method using kernel classification",
      "author" : [ "H. Wang", "J. Wang" ],
      "venue" : "2014 IEEE 26th International Conference on Tools with Artificial Intelligence (ICTAI 2014), 2014, pp. 853–858.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multiple kernel multivariate performance learning using cutting plane algorithm",
      "author" : [ "J. Wang", "H. Wang", "Y. Zhou", "N. McDonald" ],
      "venue" : "Systems, Man, and Cybernetics (SMC), 2015 IEEE International Conference on. IEEE, 2015, pp. 1870–1875.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Supervised cross-modal factor analysis for multiple modal data classification",
      "author" : [ "J. Wang", "Y. Zhou", "K. Duan", "J.J.-Y. Wang", "H. Bensmail" ],
      "venue" : "Systems, Man, and Cybernetics (SMC), 2015 IEEE International Conference on. IEEE, 2015, pp. 1882–1888.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Supervised learning of sparse context reconstruction coefficients for data representation and classification",
      "author" : [ "X. Liu", "J. Wang", "M. Yin", "B. Edwards", "P. Xu" ],
      "venue" : "Neural Computing and Applications, pp. 1–9, 2015.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multi-kernel learning for multivariate performance measures optimization",
      "author" : [ "F. Lin", "J. Wang", "N. Zhang", "J. Xiahou", "N. McDonald" ],
      "venue" : "Neural Computing and Applications, pp. 1–13, 2016.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Doubly regularized portfolio with risk minimization",
      "author" : [ "W. Shen", "J. Wang", "S. Ma" ],
      "venue" : "Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014, pp. 1286–1292.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Transaction costs-aware portfolio optimization via fast lowner-john ellipsoid approximation",
      "author" : [ "W. Shen", "J. Wang" ],
      "venue" : "Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015, pp. 1854–1860.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Portfolio choices with orthogonal bandit learning",
      "author" : [ "W. Shen", "J. Wang", "Y.-G. Jiang", "H. Zha" ],
      "venue" : "Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015, pp. 974–980.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The relationship of dependency relations and parts of speech in hungarian",
      "author" : [ "V. Vincze" ],
      "venue" : "Journal of Quantitative Linguistics, vol. 22, no. 1, pp. 44–54, 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Evaluating word embeddings and a revised corpus for part-of-speech tagging in portuguese",
      "author" : [ "E. Fonseca", "J. G Rosa", "S. Alusio" ],
      "venue" : "Journal of the Brazilian Computer Society, vol. 21, no. 1, p. 14, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Maximum margin semisupervised learning for structured variables",
      "author" : [ "Y. Altun", "M. Belkin", "D.A. Mcallester" ],
      "venue" : "Advances in neural information processing systems, 2005, pp. 33–40.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Semi-supervised learning for structured output variables",
      "author" : [ "U. Brefeld", "T. Scheffer" ],
      "venue" : "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 145–152.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Semi-supervised structured output learning based on a hybrid generative and discriminative approach.",
      "author" : [ "J. Suzuki", "A. Fujino", "H. Isozaki" ],
      "venue" : "EMNLP-CoNLL,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2007
    }, {
      "title" : "Manifold regularization in structured output space for semi-supervised structured output prediction",
      "author" : [ "F. Jiang", "L. Jia", "X. Sheng", "R. LeMieux" ],
      "venue" : "Neural Computing and Applications, pp. 1–10, 2015.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Semi-supervised distance metric learning based on local linear regression for data clustering",
      "author" : [ "H. Zhang", "J. Yu", "M. Wang", "Y. Liu" ],
      "venue" : "Neurocomputing, vol. 93, pp. 100–105, 2012.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Local ridge regression for face recognition",
      "author" : [ "H. Xue", "Y. Zhu", "S. Chen" ],
      "venue" : "Neurocomputing, vol. 72, no. 4, pp. 1342–1346, 2009.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Tightening bounds for bayesian network structure learning",
      "author" : [ "X. Fan", "C. Yuan", "B. Malone" ],
      "venue" : "Proceedings of the 28th AAAI Conference on Artificial Intelligence, 2014, pp. 2439 – 2445.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "An improved lower bound for bayesian network structure learning",
      "author" : [ "X. Fan", "C. Yuan" ],
      "venue" : "Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015, pp. 2439 – 2445.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sun database: Large-scale scene recognition from abbey to zoo",
      "author" : [ "J. Xiao", "J. Hays", "K. Ehinger", "A. Oliva", "A. Torralba" ],
      "venue" : "Computer vision and pattern recognition (CVPR), 2010 IEEE conference on. IEEE, 2010, pp. 3485–3492.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Introduction to the conll-2002 shared task: Languageindependent named entity recognition",
      "author" : [ "T.K. Sang" ],
      "venue" : "Proceedings of the 6th conference on natural language learning, pp. 155–158.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Computational modeling of magnetic nanoparticle targeting to stent surface under high gradient field",
      "author" : [ "S. Wang", "Y. Zhou", "J. Tan", "J. Xu", "J. Yang", "Y. Liu" ],
      "venue" : "Computational mechanics, vol. 53, no. 3, pp. 403–412, 2014.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Biomarker binding on an antibody-functionalized biosensor surface: the influence of surface properties, electric field, and coating density",
      "author" : [ "Y. Zhou", "W. Hu", "B. Peng", "Y. Liu" ],
      "venue" : "The Journal of Physical Chemistry C, vol. 118, no. 26, pp. 14 586–14 594, 2014.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Structure design of vascular stents",
      "author" : [ "Y. Liu", "J. Yang", "Y. Zhou", "J. Hu" ],
      "venue" : "Multiscale simulations and mechanics of biological materials, pp. 301– 317, 2013.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Modeling nanoparticle targeting to a vascular surface in shear flow through diffusive particle dynamics",
      "author" : [ "B. Peng", "Y. Liu", "Y. Zhou", "L. Yang", "G. Zhang", "Y. Liu" ],
      "venue" : "Nanoscale research letters, vol. 10, no. 1, pp. 1–9, 2015.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Mechanical response of cardiovascular stents under vascular dynamic bending",
      "author" : [ "J. Xu", "J. Yang", "N. Huang", "C. Uhl", "Y. Zhou", "Y. Liu" ],
      "venue" : "Biomedical engineering online, vol. 15, no. 1, p. 1, 2016.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Inferring clinical workflow efficiency via electronic medical record utilization",
      "author" : [ "Y. Chen", "W. Xie", "C.A. Gunter", "D. Liebovitz", "S. Mehrotra", "H. Zhang", "B. Malin" ],
      "venue" : "AMIA Annual Symposium Proceedings, vol. 2015. American Medical Informatics Association, 2015, p. 416.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Supporting regularized logistic regression privately and efficiently",
      "author" : [ "W. Li", "H. Liu", "P. Yang", "W. Xie" ],
      "venue" : "PloS one, vol. 11, no. 6, p. e0156479, 2016.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Securema: protecting participant privacy in genetic association meta-analysis",
      "author" : [ "W. Xie", "M. Kantarcioglu", "W.S. Bush", "D. Crawford", "J.C. Denny", "R. Heatherly", "B.A. Malin" ],
      "venue" : "Bioinformatics, p. btu561, 2014.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multilayer joint gait-pose manifolds for human gait motion modeling",
      "author" : [ "M. Ding", "G. Fan" ],
      "venue" : "IEEE Transactions on Cybernetics, vol. 45, no. 11, pp. 2413–2424, 2015.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Articulated and generalized gaussian kernel correlation for human pose estimation",
      "author" : [ "——" ],
      "venue" : "IEEE Transactions on Image Processing, vol. 25, no. 2, pp. 776–789, 2016.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Image annotation with incomplete labelling by modelling image specific structured loss",
      "author" : [ "X. Xu", "A. Shimada", "H. Nagahara", "R.-i. Taniguchi", "L. He" ],
      "venue" : "IEEJ Transactions on Electrical and Electronic Engineering, vol. 11, no. 1, pp. 73–82, 2016.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Surgical wound debridement sequentially characterized in a porcine burn model with multispectral imaging",
      "author" : [ "D.R. King", "W. Li", "J.J. Squiers", "R. Mohan", "E. Sellke", "W. Mo", "X. Zhang", "W. Fan", "J.M. DiMaio", "J.E. Thatcher" ],
      "venue" : "Burns, vol. 41, no. 7, pp. 1478–1487, 2015.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Outlier detection and removal improves accuracy of machine learning approach to multispectral burn diagnostic imaging",
      "author" : [ "W. Li", "W. Mo", "X. Zhang", "J.J. Squiers", "Y. Lu", "E.W. Sellke", "W. Fan", "J.M. DiMaio", "J.E. Thatcher" ],
      "venue" : "Journal of biomedical optics, vol. 20, no. 12, pp. 121 305– 121 305, 2015.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multispectral and photoplethysmography optical imaging techniques identify important tissue characteristics in an animal model of tangential burn excision",
      "author" : [ "J.E. Thatcher", "W. Li", "Y. Rodriguez-Vaqueiro", "J.J. Squiers", "W. Mo", "Y. Lu", "K.D. Plant", "E. Sellke", "D.R. King", "W. Fan" ],
      "venue" : "Journal of Burn Care & Research, vol. 37, no. 1, pp. 38–52, 2016.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Burn injury diagnostic imaging device’s accuracy improved by outlier detection and removal",
      "author" : [ "W. Li", "W. Mo", "X. Zhang", "Y. Lu", "J.J. Squiers", "E.W. Sellke", "W. Fan", "J.M. DiMaio", "J.E. Thatcher" ],
      "venue" : "SPIE Defense+ Security. International Society for Optics and Photonics, 2015, pp. 947 206–947 206.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multispectral imaging burn wound tissue classification system: a comparison of test accuracies of several common machine learning algorithms",
      "author" : [ "J.J. Squiers", "W. Li", "D.R. King", "W. Mo", "X. Zhang", "Y. Lu", "E.W. Sellke", "W. Fan", "J.M. DiMaio", "J.E. Thatcher" ],
      "venue" : "2016.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Margin-based over-sampling method for learning from imbalanced datasets",
      "author" : [ "X. Fan", "K. Tang", "T. Weise" ],
      "venue" : "Proceedings of the 15th Pacific- Asia Conference on Knowledge Discovery and Data Mining (PAKDD- 2011). Springer Berlin Heidelberg, 2011, pp. 309–320.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Finding optimal bayesian network structures with constraints learned from data",
      "author" : [ "X. Fan", "B. Malone", "C. Yuan" ],
      "venue" : "Proceed. of the 30th Conf. on Uncertainty in Artificial Intelligence (UAI-2014), 2014, pp. 200–209.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Enhanced maximum auc linear classifier",
      "author" : [ "X. Fan", "K. Tang" ],
      "venue" : "Fuzzy Systems and Knowledge Discovery (FSKD), 2010 Seventh International Conference on, vol. 4. IEEE, 2010, pp. 1540–1544.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A new variable sampling control scheme at fixed times for monitoring the process dispersion",
      "author" : [ "L. Shi", "C. Zou", "Z. Wang", "K.C. Kapur" ],
      "venue" : "Quality and Reliability Engineering International, vol. 25, no. 8, pp. 961–972, 2009.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A synthesis of feedback and feedforward control for process improvement under stationary and nonstationary time series disturbance models",
      "author" : [ "L. Shi", "K.C. Kapur" ],
      "venue" : "Quality and Reliability Engineering International, vol. 31, no. 3, pp. 343–354, 2015.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Semisupervised coupled dictionary learning for cross-modal retrieval in internet images and texts",
      "author" : [ "X. Xu", "Y. Yang", "A. Shimada", "R.-i. Taniguchi", "L. He" ],
      "venue" : "Proceedings of the 23rd Annual ACM Conference on Multimedia Conference. ACM, 2015, pp. 847–850.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Optimizing top precision performance measure of content-based image retrieval by learning similarity function",
      "author" : [ "R.-Z. Liang", "L. Shi", "H. Wang", "J. Meng", "J.J.-Y. Wang", "Q. Sun", "Y. Gu" ],
      "venue" : "Pattern Recognition (ICPR), 2016 23st International Conference on. IEEE, 2016.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 6,
      "context" : "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : "Traditional machine learning methods learn model to predict binary class labels, or a continuous response [1], [2], [3], [4], [5], [6], [7], [8].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 8,
      "context" : "For example, in the part-of-speech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [9], [10].",
      "startOffset" : 240,
      "endOffset" : 243
    }, {
      "referenceID" : 9,
      "context" : "For example, in the part-of-speech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [9], [10].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 10,
      "context" : "[11] proposed the problem of semi-supervised learning with structured outputs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "Brefeld and Scheffer [12] proposed to solve the problem of semisupervised structured output prediction by learning in the space of input-output space, and using co-training method.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 12,
      "context" : "[13] proposed a hybrid method to solve the problem of semi-supervised structured output learning.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] proposed to regularize the structured outputs by the manifold constructed from the input space directly.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "In this paper, we learn multiple local linear structured output predictor for different neighborhoods to model the local distributions, instead of learning one single predictor for the entire data [15], [16], and we also propose to learn the missing structured outputs for a semi- supervised data set simultaneously.",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 15,
      "context" : "In this paper, we learn multiple local linear structured output predictor for different neighborhoods to model the local distributions, instead of learning one single predictor for the entire data [15], [16], and we also propose to learn the missing structured outputs for a semi- supervised data set simultaneously.",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 16,
      "context" : "(4) We approximate the upper bound of the structured loss based on the lower bound approximation method of the structure learning of the Bayesian network [17], [18].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 17,
      "context" : "(4) We approximate the upper bound of the structured loss based on the lower bound approximation method of the structure learning of the Bayesian network [17], [18].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 16,
      "context" : "[17] proposed to tighten the upper and lower bounds of the breadthfirst branch and bound algorithm for the learning of Bayesian network structures.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "The work of [17] is a contribution of major significance to the bound approximation community, and our upper bound approximation method is also based on these strategies.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 17,
      "context" : "[18] further proposed to improve the lower bound function of static k-cycle conflict heuristic for the learning of Bayesian network structures.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "The tree has 15 different leaves [19].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 13,
      "context" : "[14] 0.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] 0.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "504 Brefeld and Scheffer [12] 0.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : "[13] 0.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[36] Altun et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[33] Brefeld and Scheffer [34]Suzuki et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[33] Brefeld and Scheffer [34]Suzuki et al.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 34,
      "context" : "[35] 0 30 60 90 120 150",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[36] Altun et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[33] Brefeld and Scheffer [34]Suzuki et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[33] Brefeld and Scheffer [34]Suzuki et al.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 34,
      "context" : "[35] 0 10 20 30 40",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "• Spanish news wire article sentence data set: This data set contains 300 sentences, and each sentence is used as a data point in our experiment [20].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 10,
      "context" : "[11], Brefeld and Scheffer [12], Suzuki et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[11], Brefeld and Scheffer [12], Suzuki et al.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 12,
      "context" : "[13], and Jiang et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11], however, its prediction results are not accurate.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 21,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 22,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 23,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 24,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 25,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 26,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 27,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 28,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 29,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 30,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 31,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 32,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 201,
      "endOffset" : 205
    }, {
      "referenceID" : 33,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 34,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 35,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 36,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 276,
      "endOffset" : 280
    }, {
      "referenceID" : 37,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 282,
      "endOffset" : 286
    }, {
      "referenceID" : 38,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 288,
      "endOffset" : 292
    }, {
      "referenceID" : 39,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 314,
      "endOffset" : 318
    }, {
      "referenceID" : 40,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 320,
      "endOffset" : 324
    }, {
      "referenceID" : 41,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 364,
      "endOffset" : 368
    }, {
      "referenceID" : 42,
      "context" : "We also want to apply the proposed method to various applications, such as computational biology and health care [21], [22], [23], [24], [25], [26], [27], [28], computer vision [29], [30], [31], [32], [33], [34], [35], [36], natural language processing, information retrieval [37], [38], [39], importance sampling [40], [41], and multimedia information processing [42], [43].",
      "startOffset" : 370,
      "endOffset" : 374
    } ],
    "year" : 2016,
    "abstractText" : "We propose a novel semi-supervised structured output prediction method based on local linear regression in this paper. The existing semi-supervise structured output prediction methods learn a global predictor for all the data points in a data set, which ignores the differences of local distributions of the data set, and the effects to the structured output prediction. To solve this problem, we propose to learn the missing structured outputs and local predictors for neighborhoods of different data points jointly. Using the local linear regression strategy, in the neighborhood of each data point, we propose to learn a local linear predictor by minimizing both the complexity of the predictor and the upper bound of the structured prediction loss. The minimization problem is solved by sub-gradient descent algorithms. We conduct experiments over two benchmark data sets, and the results show the advantages of the proposed method.",
    "creator" : "LaTeX with hyperref package"
  }
}