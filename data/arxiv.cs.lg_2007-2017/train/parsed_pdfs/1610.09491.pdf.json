{
  "name" : "1610.09491.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Wilsun Xu" ],
    "emails" : [ "k.shaloudegi16@imperial.ac.uk", "a.gyorgy@imperial.ac.uk", "szepesva@ualberta.ca", "wxu@ualberta.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 0.\n09 49\n1v 1\n[ cs\n.L G\n] 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "Energy efficiency is becoming one of the most important issues in our society. Identifying the energy consumption of individual electrical appliances in homes can raise awareness of power consumption and lead to significant saving in utility bills. Detailed feedback about the power consumption of individual appliances helps energy consumers to identify potential areas for energy savings, and increases their willingness to invest in more efficient products. Notifying home owners of accidentally running stoves, ovens, etc., may not only result in savings but also improves safety. Energy disaggregation or non-intrusive load monitoring (NILM) uses data from utility smart meters to separate individual load consumptions (i.e., a load signal) from the total measured power (i.e., the mixture of the signals) in households.\nThe bulk of the research in NILM has mostly concentrated on applying different data mining and pattern recognition methods to track the footprint of each appliance in total power measurements. Several techniques, such as artificial neural networks (ANN) [Prudenzi, 2002, Chang et al., 2012, Liang et al., 2010], deep neural networks [Kelly and Knottenbelt, 2015], k-nearest neighbor (k-NN) [Figueiredo et al., 2012, Weiss et al., 2012], sparse coding [Kolter et al., 2010], or ad-hoc heuristic methods [Dong et al., 2012] have been employed. Recent works, rather than turning electrical events into features fed into classifiers, consider the temporal structure of the data[Zia et al., 2011, Kolter and Jaakkola, 2012, Kim et al., 2011, Zhong et al., 2014, Egarter et al., 2015, Guo et al., 2015], resulting in state-of-the-art performance [Kolter and Jaakkola, 2012]. These works usually model the individual appliances by independent hidden Markov models (HMMs), which leads to a factorial HMM (FHMM) model describing the total consumption.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nFHMMs, introduced by Ghahramani and Jordan [1997], are powerful tools for modeling times series generated from multiple independent sources, and are great for modeling speech with multiple people simultaneously talking [Rennie et al., 2009], or energy monitoring which we consider here [Kim et al., 2011]. Doing exact inference in FHMMs is NP hard; therefore, computationally efficient approximate methods have been the subject of study. Classic approaches include sampling methods, such as MCMC or particle filtering [Koller and Friedman, 2009] and variational Bayes methods [Wainwright and Jordan, 2007, Ghahramani and Jordan, 1997]. In practice, both methods are nontrivial to make work and we are not aware of any works that would have demonstrated good results in our application domain with the type of FHMMs we need to work and at practical scales.\nIn this paper we follow the work of Kolter and Jaakkola [2012] to model the NILM problem by FHMMs. The distinguishing features of FHMMs in this setting are that (i) the output is the sum of the output of the underlying HMMs (perhaps with some noise), and (ii) the number of transitions are small in comparison to the signal length. FHMMs with the first property are called additive. In this paper we derive an efficient, convex relaxation based method for FHMMs of the above type, which significantly outperforms the state-of-the-art algorithms. Our approach is based on revisiting relaxations to the integer programming formulation of Kolter and Jaakkola [2012]. In particular, we replace the quadratic programming relaxation of Kolter and Jaakkola, 2012 with a relaxation to an semi-definite program (SDP), which, based on the literature of relaxations is expected to be tighter and thus better. While SDPs are convex and could in theory be solved using interior-point (IP) methods in polynomial time [Malick et al., 2009], IP scales poorly with the size of the problem and is thus unsuitable to our large scale problem which may involve as many a million variables. To address this problem, capitalizing on the structure of our relaxation coming from our FHMM model, we develop a novel variant of ADMM [Boyd et al., 2011] that uses Moreau-Yosida regularization and combine it with a version of randomized rounding that is inspired by the the recent work of Park and Boyd [2015]. Experiments on synthetic and real data confirm that our method significantly outperforms other algorithms from the literature, and we expect that it may find its applications in other FHMM inference problems, too."
    }, {
      "heading" : "1.1 Notation",
      "text" : "Throughout the paper, we use the following notation: R denotes the set of real numbers, Sn+ denotes the set of n × n positive semidefinite matrices, I{E} denotes the indicator function of an event E (that is, it is 1 if the event is true and zero otherwise), 1 denotes a vector of appropriate dimension whose entries are all 1. For an integer K , [K] denotes the set {1, 2, . . . ,K}. N (µ,Σ) denotes the Gaussian distribution with mean µ and covariance matrix Σ. For a matrix A, trace(A) denotes its trace and diag(A) denotes the vector formed by the diagonal entries of A."
    }, {
      "heading" : "2 System Model",
      "text" : "Following Kolter and Jaakkola [2012], the energy usage of the household is modeled using an additive factorial HMM [Ghahramani and Jordan, 1997]. Suppose there areM appliances in a household. Each of them is modeled via an HMM: let Pi ∈ RKi×Ki denote the transition-probability matrix of appliance i ∈ [M ], and assume that for each state s ∈ [Ki], the energy consumption of the appliance is constant µi,s (µi denotes the corresponding Ki-dimensional column vector (µi,1, . . . , µi,Ki)\n⊤). Denoting by xt,i ∈ {0, 1}Ki the indicator vector of the state st,i of appliance i at time t (i.e., xt,i,s = I{st,i=s}), the total power consumption at time t is ∑ i∈[M ] µ ⊤ i xt,i, which we assume is observed with some additive zero mean Gaussian noise of variance σ2: yt ∼ N ( ∑ i∈[M ] µ ⊤ i xt,i, σ 2).1\nGiven this model, the maximum likelihood estimate of the appliance state vector sequence can be obtained by minimizing the log-posterior function\nargmin xt,i\nT ∑\nt=1\n(yt − ∑M i=1 x ⊤ t,iµi) 2\n2σ2 −\nT−1 ∑\nt=1\nM ∑\ni=1\nx⊤t,i(logPi)xt+1,i\nsubject to xt,i ∈ {0, 1}Ki, 1⊤xt,i = 1, i ∈ [M ] and t ∈ [T ],\n(1)\n1Alternatively, we can assume that the power consumption yt,iof each appliance is normally distributed with mean µ⊤i xt,i and variance σ 2 i , where σ 2 = ∑ i∈[M] σ 2 i , and yt = ∑ i∈[M] yt,i.\nwhere logPi denotes a matrix obtained from Pi by taking the logarithm of each entry.\nIn our particular application, in addition to the signal’s temporal structure, large changes in total power (in comparison to signal noise) contain valuable information that can be used to further improve the inference results (in fact, solely this information was used for energy disaggregation, e.g., by Dong et al., 2012, 2013, Figueiredo et al., 2012). This observation was used by Kolter and Jaakkola [2012] to amend the posterior with a term that tries to match the large signal changes to the possible changes in the power level when only the state of a single appliance changes.\nFormally, let ∆yt = yt+1 − yt, ∆µ (i) m,k = µi,k − µi,m, and define the matrices Et,i ∈ R Ki×Ki by (Et,i)m,k = (∆yt −∆µ (i) m,k)\n2/(2σ2diff), for some constant σdiff > 0. Intuitively, (Et,i)m,k is the negative log-likelihood (up to a constant) of observing a change ∆yt in the power level when appliance i transitions from state m to state k under some zero-mean Gaussian noise with variance σ2diff. Making the heuristic approximation that the observation noise and this noise are independent (which clearly does not hold under the previous model), Kolter and Jaakkola [2012] added the term (−\n∑T−1 t=1 ∑M i=1 x ⊤ t,iEt,ixt+1,i) to the objective of (1), arriving at\nargmin xt,i f(x1, . . . , xT ) :=\nT ∑\nt=1\n(yt − ∑M i=1 x ⊤ t,iµi) 2\n2σ2 −\nT−1 ∑\nt=1\nM ∑\ni=1\nx⊤t,i(Et,i + logPi)xt+1,i\nsubject to xt,i ∈ {0, 1}Ki, 1⊤xt,i = 1, i ∈ [M ] and t ∈ [T ] .\n(2)\nIn the rest of the paper we derive an efficient approximate solution to (2), and demonstrate that it is superior to the approximate solution derived by Kolter and Jaakkola [2012] with respect to several measures quantifying the accuracy of load disaggregation solutions."
    }, {
      "heading" : "3 SDP Relaxation and Randomized Rounding",
      "text" : "There are two major challenges to solve the optimization problem (2) exactly: (i) the optimization is over binary vectors xt,i; and (ii) the objective function f , even when considering its extension to a convex domain, is in general non-convex (due to the second term). As a remedy we will relax (2) to make it an integer quadratic programming problem, then apply an SDP relaxation and randomized rounding to solve approximately the relaxed problem. We start with reviewing the latter methods."
    }, {
      "heading" : "3.1 Approximate Solutions for Integer Quadratic Programming",
      "text" : "In this section we consider approximate solutions to the integer quadratic programming problem\nminimize f(x) = x⊤Dx+ 2d⊤x subject to x ∈ {0, 1}n,\n(3)\nwhere D ∈ Sn+ is positive semidefinite, and d ∈ R n. While an exact solution of (3) can be found by enumerating all possible combination of binary values within a properly chosen box or ellipsoid, the running time of such exact methods is nearly exponential in the number n of binary variables, making these methods unfit for large scale problems.\nOne way to avoid exponential running times is to replace (3) with a convex problem with the hope that the solutions of the convex problems can serve as a good starting point to find high-quality solutions to (3). The standard approach to this is to linearize (3) by introducing a new variable X ∈ Sn+ tied to x trough X = xx\n⊤, so that x⊤Dx = trace(DX), and then relax the nonconvex constraints X = xx⊤, x ∈ {0, 1}n to X xx⊤, diag(X) = x, x ∈ [0, 1]n. This leads to the relaxed SDP problem\nminimize trace(D⊤X) + 2d⊤x\nsubject to\n[\n1 x⊤ x X\n]\n0, diag(X) = x, x ∈ [0, 1]n (4)\nBy introducing X̂ =\n[\n1 x⊤\nx X\n]\nthis can be written in the compact SDP form\nminimize trace(D̂⊤X̂) subject to X̂ 0, AX̂ = b . (5)\nwhere D̂ =\n[\n0 d⊤\nd D\n]\n∈ Sn+1+ , b ∈ R m and A : Sn+ → R m is an appropriate linear operator. This\ngeneral SDP optimization problem can be solved with arbitrary precision in polynomial time using interior-point methods [Malick et al., 2009, Wen et al., 2010]. As discussed before, this approach becomes impractical in terms of both the running time and the required memory if either the number of variables or the optimization constraints are large [Wen et al., 2010]. We will return to the issue of building scaleable solvers for NILM in Section 5.\nNote that introducing the new variable X , the problem is projected into a higher dimensional space, which is computationally more challenging than just simply relaxing the integrality constraint in (3), but leads to a tighter approximation of the optimum (c.f., Park and Boyd, 2015; see also Lovász and Schrijver, 1991, Burer and Vandenbussche, 2006).\nTo obtain a feasible point of (3) from the solution of (5), we still need to change the solution x to a binary vector. This can be done via randomized rounding [Park and Boyd, 2015, Goemans and Williamson, 1995]: Instead of letting x ∈ [0, 1]n, the integrality constraint x ∈ {0, 1}n in (3) can be replaced by the inequalities xi(xi − 1) ≥ 0 for all i ∈ [n]. Although these constraints are nonconvex, they admit an interesting probabilistic interpretation: the optimization problem\nminimize Ew∼N (µ,Σ)[w ⊤Dw + 2d⊤w]\nsubject to Ew∼N (µ,Σ)[wi(wi − 1)] ≥ 0, i ∈ [n], µ ∈ R n, Σ 0\nis equivalent to\nminimize trace((Σ + µµ⊤)D) + 2d⊤µ\nsubject to Σi,i + µ 2 i − µi ≥ 0, i ∈ [n],\n(6)\nwhich is in the form of (4) with X = Σ + µµ⊤ and x = µ (above, Ex∼P [f(x)] stands for ∫\nf(x)dP (x)). This leads to the rounding procedure: starting from a solution (x∗, X∗) of (4), we randomly draw several samples w(j) from N (x∗, X∗ − x∗x∗⊤), round w(j)i to 0 or 1 to obtain x(j), and keep the x(j) with the smallest objective value. In a series of experiments, Park and Boyd [2015] found this procedure to be better than just naively rounding the coordinates of x∗."
    }, {
      "heading" : "4 An Efficient Algorithm for Inference in FHMMs",
      "text" : "To arrive at our method we apply the results of the previous subsection to (2). To do so, as mentioned at the beginning of the section, we need to change the problem to a convex one, since the elements of the second term in the objective of (2), −x⊤t,i(Et,i + logPi)xt+1,i are not convex. To address this issue, we relax the problem by introducing new variables Zt,i = xt,ix⊤t+1,i and replace the constraint Zt,i = xt,ix⊤t+1,i with two new ones:\nZt,i1 = xt,i and Z⊤t,i1 = xt+1,i.\nTo simplify the presentation, we will assume that Ki = K for all i ∈ [M ]. Then problem (2) becomes\nargmin xt,i\nT ∑\nt=1\n{\n1\n2σ2 ( yt − x ⊤ t µ )2 − p⊤t zt\n}\nsubject to xt ∈ {0, 1} MK , t ∈ [T ],\nẑt ∈ {0, 1} MKK, t ∈ [T − 1],\n1 ⊤xt,i = 1, t ∈ [T ] and i ∈ [M ],\nZt,i1 ⊤ = xt,i, Z ⊤ t,i1 ⊤ = xt+1,i , t ∈ [T − 1] and i ∈ [M ],\n(7)\nwhere x⊤t = [x ⊤ t,1, . . . , x ⊤ t,M ], µ ⊤ = [µ⊤1 , . . . , µ ⊤ M ], z ⊤ t = [vec(Zt,1) ⊤, . . . , vec(Zt,M )⊤] and p⊤t = [vec(Et,1 + logP1), . . . , vec(logPT )], with vec(A) denoting the column vector obtained\nAlgorithm 1 ADMM-RR: Randomized rounding algorithm for suboptimal solution to (2) Given: number of iterations: itermax, length of input data: T Solve the optimization problem (8): Run Algorithm 2 to get X∗t and z ∗ t\nSet xbestt := z ∗ t and X best t := X ∗ t for t = 1, . . . , T for t = 2, . . . , T − 1 do Set x := [xbestt−1 ⊤ , xbestt ⊤ , xbestt+1 ⊤ ]⊤\nSet X := block(Xbestt−1 , X best t , X best t+1 ) where block(·, ·) constructs block diagonal matrix from input arguments Set fbest := ∞ Form the covariance matrix Σ := X − xxT and find its Cholesky factorization LL⊤ = Σ. for k = 1, 2, . . . , itermax do\nRandom sampling: zk := x+ Lw, where w ∼ N (0, I) Round zk to the nearest integer point xk that satisfies the constraints of (7) If fbest > ft(xk) then update xbestt and X best t from the corresponding entries of x k and xkxk ⊤\n, respectively\nend for end for\nby concatenating the columns of A for a matrix A. Expanding the first term of (7) and following the relaxation method of Section 3.1, we get the following SDP problem:2\narg min Xt,zt\nT ∑\nt=1\ntrace(D⊤t Xt) + d ⊤ t zt\nsubject to AXt = b, BXt + Czt + EXt+1 = g,\nXt 0, Xt, zt ≥ 0 .\n(8)\nHere A : SMK+1+ → R m, B, E : SMK+1+ → R m′ and C ∈ RMKK×m ′\nare all appropriate linear operators, and the integers m and m′ are determined by the number of equality constraints, while\nDt = 1\n2σ2\n[\n0 −ytµ⊤\n−ytµ µµ⊤\n]\nand dt = pt. Notice that (8) is a simple, though huge-dimensional SDP\nproblem in the form of (5) where D̂ has a special block structure.\nNext we apply the randomized rounding method from Section 3.1 to provide an approximate solution to our original problem (2). Starting from an optimal solution (z∗, X∗) of (8) , and utilizing that we have an SDP problem for each time step t, we obtain Algorithm 1 that performs the rounding sequentially for t = 1, 2, . . . , T . However we run the randomized method for three consecutive time steps, since Xt appears at both time steps t − 1 and t + 1 in addition to time t (cf., equation 9). Following Park and Boyd [2015], in the experiments we introduce a simple greedy search within Algorithm 1: after finding the initial point xk , we greedily try to objective the target value by change the status of a single appliance at a single time instant. The search stops when no such improvement is possible, and we use the resulting point as the estimate."
    }, {
      "heading" : "5 ADMM Solver for Large-Scale, Sparse Block-Structured SDP Problems",
      "text" : "Given the relaxation and randomized rounding presented in the previous subsection all that remains is to find X∗t , z ∗ t to initialize Algorithm 1. Although interior point methods can solve SDP problems efficiently, even for problems with sparse constraints as (4), the running time to obtain an ǫ optimal solution is of the order of n3.5 log(1/ǫ) [Nesterov, 2004, Section 4.3.3], which becomes prohibitive in our case since the number of variables scales linearly with the time horizon T .\nAs an alternative solution, first-order methods can be used for large scale problems [Wen et al., 2010]. Since our problem (8) is an SDP problem where the objective function is separable, ADMM is a promising candidate to find a near-optimal solution. To apply ADMM, we use the MoreauYosida quadratic regularization [Malick et al., 2009], which is well suited for the primal formulation we consider. When implementing ADMM over the variables (Xt, zt)t, the sparse structure of our\n2The only modification is that we need to keep the equality constraints in (7) that are missing from (3).\nAlgorithm 2 ADMM for sparse SDPs of the form (8) Given: length of input data: T , number of iterations: itermax. Set the initial values to zero. W 0t , P 0 t , S 0 = 0, λ0t = 0, ν 0 t = 0, and r 0 t , h 0 t = 0\nSet µ = 0.001 {Default step-size value} for k = 0, 1, . . . , itermax do\nfor t = 1, 2, . . . , T do Update P kt , W k t , λ k, Skt , r k t , h k t , and ν k t , respectively, according to (11) (Appendix A).\nend for end for\nconstraints allows to consider the SDP problems for each time step t sequentially:\narg min Xt,zt\ntrace(D⊤t Xt) + d ⊤ t zt\nsubject to AXt = b,\nBXt + Czt + EXt+1 = g,\nBXt−1 + Czt−1 + EXt = g,\nXt 0, Xt, zt ≥ 0 .\n(9)\nThe regularized Lagrangian function for (9) is3\nLµ =trace(D⊤X) + d⊤z + 1\n2µ ‖X − S‖2F +\n1\n2µ ‖z − r‖22 + λ ⊤(b−AX)\n+ ν⊤(g − BX − Cz − EX+) + ν ⊤ − (g − BX− − Cz− − EX)\n− trace(W⊤X)− trace(P⊤X)− h⊤z,\n(10)\nwhere λ, ν, W ≥ 0, P 0, and h ≥ 0 are dual variables, and µ > 0 is a constant. By taking the derivatives of Lµ and computing the optimal values of X and z, one can derive the standard ADMM updates, which, due to space constraints, are given in Appendix A. The final algorithm, which updates the variables for each t sequentially, is given by Algorithm 2.\nAlgorithms 1 and 2 together give an efficient algorithm for finding an approximate solution to (2) and thus also to the inference problem of additive FHMMs."
    }, {
      "heading" : "6 Learning the Model",
      "text" : "The previous section provided an algorithm to solve the inference part of our energy disaggregation problem. However, to be able to run the inference method, we need to set up the model. To learn the HMMs describing each appliance, we use the method of Kontorovich et al. [2013] to learn the transition matrix, and the spectral learning method of Anandkumar et al. [2012] (following Mattfeld, 2014) to determine the emission parameters.\nHowever, when it comes to the specific application of NILM, the problem of unknown, time-varying bias also needs to be addressed, which appears due to the presence of unknown/unmodeled appliances in the measured signal. A simple idea, which is also followed by Kolter and Jaakkola [2012], is to use a “generic model” whose contribution to the objective function is downweighted. Surprisingly, incorporating this idea in the FHMM inference creates some unexpected challenges.4\nTherefore, in this work we come up with a practical, heuristic solution tailored to NILM. First we identify all electric events defined by a large change ∆yt in the power usage (using some adhoc threshold). Then we discard all events that are similar to any possible level change ∆µ(i)m,k. The remaining large jumps are regarded as coming from a generic HMM model describing the unregistered appliances: they are clustered into K − 1 clusters, and an HMM model is built where each cluster is regarded as power usage coming from a single state of the unregistered appliances. We also allow an “off state” with power usage 0.\n3We drop the subscript t and replace t+ 1 and t− 1 with + and − signs, respectively. 4For example, the incorporation of this generic model breaks the derivation of the algorithm of\nKolter and Jaakkola [2012]. See Appendix B for a discussion of this."
    }, {
      "heading" : "7 Experimental Results",
      "text" : "We evaluate the performance of our algorithm in two setups:5 we use a synthetic dataset to test the inference method in a controlled environment, while we used the REDD dataset of Kolter and Johnson [2011] to see how the method performs on non-simulated, “real” data. The performance of our algorithm is compared to the structured variational inference (SVI) method of Ghahramani and Jordan [1997], the method of Kolter and Jaakkola [2012] and that of Zhong et al. [2014]; we shall refer to the last two algorithms as KJ and ZGS, respectively."
    }, {
      "heading" : "7.1 Experimental Results: Synthetic Data",
      "text" : "The synthetic dataset was generated randomly (the exact procedure is described in Appendix C). To evaluate the performance, we use normalized disaggregation error as suggested by Kolter and Jaakkola [2012] and also adopted by Zhong et al. [2014]. This measures the reconstruction error for each individual appliance. Given the true output yt,i and the estimated output ŷt,i (i.e. ŷt,i = µ ⊤ i x̂t,i), the error measure is defined as\nNDE = √ ∑\nt,i(yt,i − ŷt,i) 2/ ∑ t,i (yt,i) 2 .\nFigures 1 and 2 show the performance of the algorithms as the number HMMs (M ) (resp., number of states, K) is varied. Each plot is a report for T = 1000 steps averaged over 100 random models and realizations, showing the mean and standard deviation of NDE. Our method, shown under the label ADMM-RR, runs ADMM for 2500 iterations, runs the local search at the end of each 250 iterations, and chooses the result that has the maximum likelihood. ADMM is the algorithm which applies naive rounding. It can be observed that the variational inference method is significantly outperformed by all other methods, while our algorithm consistently obtained better results than its competitors, KJ coming second and ZGS third."
    }, {
      "heading" : "7.2 Experimental Results: Real Data",
      "text" : "In this section, we also compared the 3 best methods on the real dataset REDD [Kolter and Johnson, 2011]. We use the first half of the data for training and the second half for testing. Each HMM (i.e.,\n5Our code is available online at https://github.com/kiarashshaloudegi/FHMM_inference.\nappliance) is trained separately using the associated circuit level data, and the HMM corresponding to unregistered appliances is trained using the main panel data. In this set of experiments we monitor appliances consuming more than 100 watts. ADMM-RR is run for 1000 iterations, and the local search is run at the end of each 250 iterations, and the result with the largest likelihood is chosen. To be able to use the ZGS method on this data, we need to have some prior information about the usage of each appliance; the authors suggestion is to us national energy surveys, but in the lack of this information (also about the number of residents, type of houses, etc.) we used the training data to extract this prior knowledge, which is expected to help this method.\nDetailed results about the precision and recall of estimating which appliances are ‘on’ at any given time are given in Table 1. In Appendix D we also report the error of the total power usage assigned to different appliances (Table 2), as well as the amount of assigned power to each appliance as a percentage of total power (Figure 3). As a summary, we can see that our method consistently outperformed the others, achieving an average precision and recall of 60.97% and 78.56%, with about 50% better precision than KJ with essentially the same recall (38.68/75.02%), while significantly improving upon ZGS (17.97/36.22%). Considering the error in assigning the power consumption to different appliances, our method achieved about 30−35% smaller error (ADMM-RR: 2.87%, KJ: 4.44%, ZGS: 3.94%) than its competitors.\nIn our real-data experiments, there are about 1 million decision variables: M = 7 or 6 appliances (for phase A and B power, respectively) with K = 4 states each and for about T = 30, 000 time steps for one day, 1 sample every 6 seconds. KJ and ZGS solve quadratic programs, increasing their memory usage (14GB vs 6GB in our case). On the other hand, our implementation of their method, using the commercial solver MOSEK inside the Matlab-based YALMIP [Löfberg, 2004], runs in 5 minutes, while our algorithm, which is purely Matlab-based takes 5 hours to finish. We expect that an optimized C++ version of our method could achieve a significant speed-up compared to our current implementation."
    }, {
      "heading" : "8 Conclusion",
      "text" : "FHMMs are widely used in energy disaggregation. However, the resulting model has a huge (factored) state space, making standard inference FHMM algorithms infeasible even for only a handful of appliances. In this paper we developed a scalable approximate inference algorithm, based on a semidefinite relaxation combined with randomized rounding, which significantly outperformed the state of the art in our experiments. A crucial component of our solution is a scalable ADMM method that utilizes the special block-diagonal-like structure of the SDP relaxation and provides a good initialization for randomized rounding. We expect that our method may prove useful in solving other FHMM inference problems, as well as in large scale integer quadratic programming."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported in part by the Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning and by NSERC. K. is indebted to Pooria Joulani and Mohammad Ajallooeian, whom provided much useful technical advise, while all authors are grateful for Zico Kolter for sharing his code."
    }, {
      "heading" : "A ADMM updates",
      "text" : "In this section we derive the ADMM updates for the regularized Lagrangian Lµ given by (10). Taking derivatives with respect to X and z and setting them to zeros, we get\n∇XLµ = D + 1\nµ (X − S)−A⊤λ− B⊤ν − E⊤ν− −W − P = 0,\nX∗ = S + µ(A⊤λ+ B⊤ν + E⊤ν− +W + P −D)\nand\n∇zLµ = d+ 1\nµ (z − r)− C⊤ν − h = 0,\nz∗ = r + µ(C⊤ν + h− d) .\nSubstituting X = X∗ and z = z∗ in (10) defines L̂µ. Then the standard ADMM iteration yields\nP k+1 = argmin P 0 L̂µ(S k, P,W k, λk, νk, νk−),\nW k+1 = arg min W≥0 L̂µ(S k, P k+1,W, λk, νk, νk−),\nλk+1 = argmin λ L̂µ(S k, P k+1,W k+1, λ, νk, νk−),\nSk+1 = Sk + µ(A⊤λk+1 + B⊤νk + E⊤νk+1− +W k+1 + P k+1 −D),\nrk+1 = rk + µ(C⊤νk + hk − d),\nhk+1 = argmin h≥0 L̂µ(r k+1, νk),\nνk+1 = argmin ν L̂µ(S k+1, P k+1,W k+1, λk+1, ν, νk+1− , h k+1, rk+1).\nBy rearranging the terms in L̂µ, the following update equations can be found:\nW k+1 =max{(D −A⊤λk − B⊤νk − E⊤νk− − P k −D − Sk/µ),0},\nP k+1 =(D −A⊤λk − B⊤νk − E⊤νk− −W k −D − Sk/µ)+,\nλk+1 = 1\nµ (AA⊤)†\n(\nb−A(B⊤νk + E⊤νk− +W k+1 + P k+1 −D)\n)\n,\nhk+1 =max{d− C⊤νk − rk/µ,0},\nνk+1 = 1\nµ (BB⊤ + CC⊤ + EE⊤)†\n( g − B ( Sk+1 + µ(A⊤λk+1 + E⊤νk+1− +W k+1 + P k+1 −D) )\n−C ( rk+1 + µ(hk+1 − d) ) − E ( Sk+ + µ(A ⊤λk+ + B ⊤νk+ +W k + + P k + −D) ))\n. (11)\nHere max : X × X → X works elementwise, and for any square matrix A, A† denotes the MoorePenrose pseudo inverse, and for any real symmetric matrix A, A+ is the projection of A onto the positive semidefinite cone (if the spectral decomposition of A is given by A = ∑\ni λiviv ⊤ i , where λi\nand vi are the ith eigenvalue and eigenvector of A, respectively, then A+ = ∑\nλi>0 λiviv ⊤ i ). Note\nthat the projections are done on matrices of small size. Note also that the pseudo-inverses of the matrices involved need only be calculated once."
    }, {
      "heading" : "B Discussion of the Derivation in Kolter and Jaakkola [2012] in the Presence of the “Generic Model”",
      "text" : "The “generic model” affects the derivation of the algorithm of Kolter and Jaakkola [2012] as follows. The authors of this paper claim to derive the final optimization problem given in equation (15) of their paper from (9) and (10) as follows: equation (9) defines the problem minz∈Z,Q∈A f1(z,Q), while (10) defines the problem minz′∈Z′,Q∈A f2(z′, Q) where z′ = g(z). Here, z, z′ are variables that describe the state of the “generic model” over time.\nThe claim in the paper is that with some set B (coming from their “one-at-a-time” constraint), minz∈Z,z′∈Z′,Q∈A∩B,z′=g(z) f1(z, A) + f2(z\n′, Q) is equivalent to the minimization problem in equation (15). However, careful checking the derivation shows that (15) is equivalent to minz∈Z,z′∈Z′,Q∈A∩B f1(z, A) + minz′∈Z′ f2(z ′, Q), which is smaller in general."
    }, {
      "heading" : "C Generating the Synthetic Dataset",
      "text" : "The synthetic dataset used in the experiments was generated in the following way: The power levels corresponding to each on state (µ) were generated uniformly at random from [100, 4500] with the additional constraint that the difference of any two non-zero levels must be greater than 100 (to encourage identifiability). The levels for “off states” were set to 0. The transition matrices for each appliance were generated the following way: diagonal elements for “off states” were drawn uniformly at random from [0, 35] and for on-states from [0, 30], while non-diagonal elements were selected from [0, 1] to ensure sparse transitions. Finally, the data matrices were normalized to ensure they are proper transition matrices. The output of each appliance was subject to an additive Gaussian noise with variance σ ∈ [0, 6] selected proportionally to the energy consumption level of the given on state, and 1 for off states."
    }, {
      "heading" : "D Additional Results for the Real-Data Experiment",
      "text" : "In Table 1 we provided prediction and recall values for our experiments on real data. As promised, here we provide some additional results about these experiments: Table 2 presents the total power usage assigned to different appliances, and Figure 3 shows the amount of assigned power to each appliance."
    } ],
    "references" : [ {
      "title" : "Non-Intrusive Signature Extraction for Major Residential Loads",
      "author" : [ "M. Dong", "Meira", "W. Xu", "C.Y. Chung" ],
      "venue" : "Smart Meters. IEEE Transactions on Smart Grid,",
      "citeRegEx" : "Dong et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2012
    }, {
      "title" : "PALDi: Online Load Disaggregation via Particle Filtering",
      "author" : [ "D. Egarter", "V.P. Bhuvana", "W. Elmenreich" ],
      "venue" : "IEEE Transactions on Smart Grid,",
      "citeRegEx" : "Egarter et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Egarter et al\\.",
      "year" : 2013
    }, {
      "title" : "Factorial Hidden Markov Models",
      "author" : [ "Z. Ghahramani", "M. Jordan" ],
      "venue" : "Monitoring (NILM) Systems. Neurocomputing,",
      "citeRegEx" : "Ghahramani and Jordan.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ghahramani and Jordan.",
      "year" : 2012
    }, {
      "title" : "Implementing spectral methods for hidden Markov models with real-valued emissions",
      "author" : [ "C. Mattfeld" ],
      "venue" : "Journal on Optimization,",
      "citeRegEx" : "Mattfeld.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mattfeld.",
      "year" : 2009
    }, {
      "title" : "Leveraging smart meter data to recognize home appliances",
      "author" : [ "M. Weiss", "A. Helfenstein", "F. Mattern", "T. Staake" ],
      "venue" : null,
      "citeRegEx" : "Weiss et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2007
    }, {
      "title" : "Note that the projections are done on matrices of small size. Note also that the pseudo-inverses of the matrices involved need only be calculated once",
      "author" : [ ],
      "venue" : "B Discussion of the Derivation in Kolter and Jaakkola",
      "citeRegEx" : "..,? \\Q2012\\E",
      "shortCiteRegEx" : "..",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", 2010], or ad-hoc heuristic methods [Dong et al., 2012] have been employed.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "FHMMs, introduced by Ghahramani and Jordan [1997], are powerful tools for modeling times series generated from multiple independent sources, and are great for modeling speech with multiple people simultaneously talking [Rennie et al.",
      "startOffset" : 21,
      "endOffset" : 50
    }, {
      "referenceID" : 2,
      "context" : "FHMMs, introduced by Ghahramani and Jordan [1997], are powerful tools for modeling times series generated from multiple independent sources, and are great for modeling speech with multiple people simultaneously talking [Rennie et al., 2009], or energy monitoring which we consider here [Kim et al., 2011]. Doing exact inference in FHMMs is NP hard; therefore, computationally efficient approximate methods have been the subject of study. Classic approaches include sampling methods, such as MCMC or particle filtering [Koller and Friedman, 2009] and variational Bayes methods [Wainwright and Jordan, 2007, Ghahramani and Jordan, 1997]. In practice, both methods are nontrivial to make work and we are not aware of any works that would have demonstrated good results in our application domain with the type of FHMMs we need to work and at practical scales. In this paper we follow the work of Kolter and Jaakkola [2012] to model the NILM problem by FHMMs.",
      "startOffset" : 21,
      "endOffset" : 919
    }, {
      "referenceID" : 2,
      "context" : "FHMMs, introduced by Ghahramani and Jordan [1997], are powerful tools for modeling times series generated from multiple independent sources, and are great for modeling speech with multiple people simultaneously talking [Rennie et al., 2009], or energy monitoring which we consider here [Kim et al., 2011]. Doing exact inference in FHMMs is NP hard; therefore, computationally efficient approximate methods have been the subject of study. Classic approaches include sampling methods, such as MCMC or particle filtering [Koller and Friedman, 2009] and variational Bayes methods [Wainwright and Jordan, 2007, Ghahramani and Jordan, 1997]. In practice, both methods are nontrivial to make work and we are not aware of any works that would have demonstrated good results in our application domain with the type of FHMMs we need to work and at practical scales. In this paper we follow the work of Kolter and Jaakkola [2012] to model the NILM problem by FHMMs. The distinguishing features of FHMMs in this setting are that (i) the output is the sum of the output of the underlying HMMs (perhaps with some noise), and (ii) the number of transitions are small in comparison to the signal length. FHMMs with the first property are called additive. In this paper we derive an efficient, convex relaxation based method for FHMMs of the above type, which significantly outperforms the state-of-the-art algorithms. Our approach is based on revisiting relaxations to the integer programming formulation of Kolter and Jaakkola [2012]. In particular, we replace the quadratic programming relaxation of Kolter and Jaakkola, 2012 with a relaxation to an semi-definite program (SDP), which, based on the literature of relaxations is expected to be tighter and thus better.",
      "startOffset" : 21,
      "endOffset" : 1519
    }, {
      "referenceID" : 2,
      "context" : "FHMMs, introduced by Ghahramani and Jordan [1997], are powerful tools for modeling times series generated from multiple independent sources, and are great for modeling speech with multiple people simultaneously talking [Rennie et al., 2009], or energy monitoring which we consider here [Kim et al., 2011]. Doing exact inference in FHMMs is NP hard; therefore, computationally efficient approximate methods have been the subject of study. Classic approaches include sampling methods, such as MCMC or particle filtering [Koller and Friedman, 2009] and variational Bayes methods [Wainwright and Jordan, 2007, Ghahramani and Jordan, 1997]. In practice, both methods are nontrivial to make work and we are not aware of any works that would have demonstrated good results in our application domain with the type of FHMMs we need to work and at practical scales. In this paper we follow the work of Kolter and Jaakkola [2012] to model the NILM problem by FHMMs. The distinguishing features of FHMMs in this setting are that (i) the output is the sum of the output of the underlying HMMs (perhaps with some noise), and (ii) the number of transitions are small in comparison to the signal length. FHMMs with the first property are called additive. In this paper we derive an efficient, convex relaxation based method for FHMMs of the above type, which significantly outperforms the state-of-the-art algorithms. Our approach is based on revisiting relaxations to the integer programming formulation of Kolter and Jaakkola [2012]. In particular, we replace the quadratic programming relaxation of Kolter and Jaakkola, 2012 with a relaxation to an semi-definite program (SDP), which, based on the literature of relaxations is expected to be tighter and thus better. While SDPs are convex and could in theory be solved using interior-point (IP) methods in polynomial time [Malick et al., 2009], IP scales poorly with the size of the problem and is thus unsuitable to our large scale problem which may involve as many a million variables. To address this problem, capitalizing on the structure of our relaxation coming from our FHMM model, we develop a novel variant of ADMM [Boyd et al., 2011] that uses Moreau-Yosida regularization and combine it with a version of randomized rounding that is inspired by the the recent work of Park and Boyd [2015]. Experiments on synthetic and real data confirm that our method significantly outperforms other algorithms from the literature, and we expect that it may find its applications in other FHMM inference problems, too.",
      "startOffset" : 21,
      "endOffset" : 2337
    }, {
      "referenceID" : 0,
      "context" : ", by Dong et al., 2012, 2013, Figueiredo et al., 2012). This observation was used by Kolter and Jaakkola [2012] to amend the posterior with a term that tries to match the large signal changes to the possible changes in the power level when only the state of a single appliance changes.",
      "startOffset" : 5,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "[2012] (following Mattfeld, 2014) to determine the emission parameters. However, when it comes to the specific application of NILM, the problem of unknown, time-varying bias also needs to be addressed, which appears due to the presence of unknown/unmodeled appliances in the measured signal. A simple idea, which is also followed by Kolter and Jaakkola [2012], is to use a “generic model” whose contribution to the objective function is downweighted.",
      "startOffset" : 18,
      "endOffset" : 360
    }, {
      "referenceID" : 3,
      "context" : "[2012] (following Mattfeld, 2014) to determine the emission parameters. However, when it comes to the specific application of NILM, the problem of unknown, time-varying bias also needs to be addressed, which appears due to the presence of unknown/unmodeled appliances in the measured signal. A simple idea, which is also followed by Kolter and Jaakkola [2012], is to use a “generic model” whose contribution to the objective function is downweighted. Surprisingly, incorporating this idea in the FHMM inference creates some unexpected challenges.4 Therefore, in this work we come up with a practical, heuristic solution tailored to NILM. First we identify all electric events defined by a large change ∆yt in the power usage (using some adhoc threshold). Then we discard all events that are similar to any possible level change ∆μ m,k. The remaining large jumps are regarded as coming from a generic HMM model describing the unregistered appliances: they are clustered into K − 1 clusters, and an HMM model is built where each cluster is regarded as power usage coming from a single state of the unregistered appliances. We also allow an “off state” with power usage 0. We drop the subscript t and replace t+ 1 and t− 1 with + and − signs, respectively. For example, the incorporation of this generic model breaks the derivation of the algorithm of Kolter and Jaakkola [2012]. See Appendix B for a discussion of this.",
      "startOffset" : 18,
      "endOffset" : 1376
    }, {
      "referenceID" : 2,
      "context" : "The performance of our algorithm is compared to the structured variational inference (SVI) method of Ghahramani and Jordan [1997], the method of Kolter and Jaakkola [2012] and that of Zhong et al.",
      "startOffset" : 101,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "The performance of our algorithm is compared to the structured variational inference (SVI) method of Ghahramani and Jordan [1997], the method of Kolter and Jaakkola [2012] and that of Zhong et al.",
      "startOffset" : 101,
      "endOffset" : 172
    }, {
      "referenceID" : 2,
      "context" : "The performance of our algorithm is compared to the structured variational inference (SVI) method of Ghahramani and Jordan [1997], the method of Kolter and Jaakkola [2012] and that of Zhong et al. [2014]; we shall refer to the last two algorithms as KJ and ZGS, respectively.",
      "startOffset" : 101,
      "endOffset" : 204
    } ],
    "year" : 2016,
    "abstractText" : "We develop a scalable, computationally efficient method for the task of energy disaggregation for home appliance monitoring. In this problem the goal is to estimate the energy consumption of each appliance over time based on the total energy-consumption signal of a household. The current state of the art is to model the problem as inference in factorial HMMs, and use quadratic programming to find an approximate solution to the resulting quadratic integer program. Here we take a more principled approach, better suited to integer programming problems, and find an approximate optimum by combining convex semidefinite relaxations randomized rounding, as well as a scalable ADMM method that exploits the special structure of the resulting semidefinite program. Simulation results both in synthetic and real-world datasets demonstrate the superiority of our method.",
    "creator" : "LaTeX with hyperref package"
  }
}