{
  "name" : "1505.04073.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices",
    "authors" : [ "Jie Wang", "Jieping Ye" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Multi-task feature learning (MTFL) is a powerful technique in boosting the predictive performance by learning multiple related classification/regression/clustering tasks simultaneously. However, solving the MTFL problem remains challenging when the feature dimension is extremely large. In this paper, we propose a novel screening rule—that is based on the dual projection onto convex sets (DPC)—to quickly identify the inactive features—that have zero coefficients in the solution vectors across all tasks. One of the appealing features of DPC is that: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks. Thus, by removing the inactive features from the training phase, we may have substantial savings in the computational cost and memory usage without sacrificing accuracy. To the best of our knowledge, it is the first screening rule that is applicable to sparse models with multiple data matrices. A key challenge in deriving DPC is to solve a nonconvex problem. We show that we can solve for the global optimum efficiently via a properly chosen parametrization of the constraint set. Moreover, DPC has very low computational cost and can be integrated with any existing solvers. We have evaluated the proposed DPC rule on both synthetic and real data sets. The experiments indicate that DPC is very effective in identifying the inactive features—especially for high dimensional data—which leads to a speedup up to several orders of magnitude."
    }, {
      "heading" : "1 Introduction",
      "text" : "Empirical studies have shown that learning multiple related tasks (MTL) simultaneously often provides superior predictive performance relative to learning each tasks independently (Ando and Zhang, 2005, Argyriou et al., 2008, Bakker and Heskes, 2003, Evgeniou et al., 2005, Zhang et al., 2006, Chen et al., 2013). This observation also has solid theoretical foundations (Ando and Zhang, 2005, Baxter, 2000, Ben-David and Schuller, 2003, Caruana, 1997), especially when the training sample size is small for each task. One popular MTL method especially for high-dimensional data is multi-task feature learning (MTFL), which uses the group Lasso penalty to ensure that all tasks select a common set of features (Argyriou et al., 2007). MTFL has found great success in many real-world applications including but not limited to: breast cancer classification (Zhang et al., 2010), disease progression prediction (Zhou et al., 2012), gene data analysis (Kim and Xing, 2009), and neural semantic basis discovery (Liu et al., 2009a). A major issue in MTFL—that is of great practical importance—is to develop efficient solvers (Liu et al., 2009b, Sra, 2012, Wang et al., 2013a). However, it remains challenging to apply the MTFL models to large-scale problems.\nar X\niv :1\n50 5.\n04 07\n3v 1\n[ cs\n.L G\n] 1\n5 M\nay 2\nThe idea of screening has been shown to be very effective in scaling the data and improving the efficiency of many popular sparse models, e.g., Lasso (El Ghaoui et al., 2012, Wang et al., 2013b, Wang et al., Xiang et al., 2011, Tibshirani et al., 2012), nonnegative Lasso Wang and Ye (2014), group Lasso (Wang et al., 2013b, Wang et al., Tibshirani et al., 2012), mixed-norm regression (Wang et al., 2013a), `1-regularized logistic regression (Wang et al., 2014b), sparse-group Lasso (Wang and Ye, 2014), support vector machine (SVM) (Ogawa et al., 2013, Wang et al., 2014a), and least absolute deviations (LAD) (Wang et al., 2014a). Essentially, screening aims to quickly identify the zero components in the solution vectors such that the corresponding features—called inactive features (e.g., Lasso)—or data samples—called non-support vectors (e.g., SVM)—can be removed from the optimization. Therefore, the size of the data matrix and the number of variables to be computed can be significantly reduced, which may lead to substantial savings in the computational cost and memory usage without sacrificing accuracy. Compared to the solvers without screening, the speedup gained by the screening methods can be several orders of magnitude.\nHowever, we note that all the existing screening methods are only applicable to sparse models with a single data matrix. Therefore, motivated by the challenges posed by large-scale data and the promising performance of existing screening methods, we propose a novel framework for developing effective and efficient screening rules for a popular MTFL model via the dual projection onto convex sets (DPC). The framework of DPC extends the state-of-the-art screening rule, called EDPP (Wang et al.), for the standard Lasso problem (Tibshirani, 1996)—that assumes a single data matrix—to a popular MTFL model—that involves multiple data matrices across different tasks. To the best of our knowledge, DPC is the first screening rule that is applicable to sparse models with multiple data matrices.\nThe DPC screening rule detects the inactive features by maximizing a convex function over a convex set containing the dual optimal solution, which is a nonconvex problem. To find the region containing the dual optimal solution, we show that the corresponding dual problem can be formulated as a projection problem—which admits many desirable geometric properties—by utilizing the bilinearity of the inner product. Then, by a carefully chosen parameterization of the constraint set, we transform the nonconvex problem to a quadratic programming problem over one quadratic constraint (QP1QC) (Gay, 1981), which can be solved for the global optimum efficiently. Experiments on both synthetic and real data sets indicate that the speedup gained by DPC can be orders of magnitude. Moreover, DPC shows better performance as the feature dimension increases, which makes it a very competitive candidate for the applications of very high-dimensional data.\nWe organize the rest of this paper as follows. In Section 2, we briefly review some basics of a popular MTFL model. Then, we derive the dual problem in Section 3. Based on an indepth analysis of the geometric properties of the dual problem and the dual feasible set, we present the proposed DPC screening rule in Section 4. In Section 5, we evaluate the DPC rule on both synthetic and real data sets. We conclude this paper in Section 6. Please refer to the supplement for proofs not included in the main text.\nNotation: Denote the `2 norm by ‖ · ‖. For x ∈ Rn, let its ith component be xi, and the diagonal matrix with the entries of x on the main diagonal be diag(x). For a set of positive integers {Nt : t = 1, . . . , T, ∑T t=1Nt = N}, we denote the tth subvector of x ∈ RN by xt such that x = (xT1 , . . . ,x T T ) T , where xt ∈ RNt for t = 1, . . . , T . For vectors x,y ∈ Rn, we use 〈x, y〉 and xTy interchangeably to denote the inner product. For a matrix M ∈ Rm×n, let mi, mj , and mij be its i\nth row, jth column and (i, j)th entry, respectively. We define the (2, 1)-norm of M by ‖M‖2,1 = ∑m i=1 ‖mi‖. For two matrices A,B ∈ Rm×n, we define their inner product by\n〈A,B〉 = tr(ATB). Let I be the identity matrix. For a convex function f(·), let ∂f(·) be its subdifferential. For a vector x and a convex set C, the projection operator is:\nPC(x) := argminy∈C 1 2‖y − x‖."
    }, {
      "heading" : "2 Basics",
      "text" : "In this section, we briefly review some basics of a popular MTFL model and mention several equivalent formulations.\nSuppose that we have T learning tasks {(Xt,yt) : t = 1, . . . , T}, where Xt ∈ RNt×d is the data matrix of the tth task with Nt samples and d features, and yt ∈ RNt is the corresponding response vector. A widely used MTFL model (Argyriou et al., 2007) takes the form of\nmin W∈Rd×T ∑T t=1 1 2‖yt −Xtwt‖ 2 + λ‖W‖2,1, (1)\nwhere wt ∈ Rd is the weight vector of the tth task and W = (w1, . . . ,wT ). Because the ‖·‖2,1-norm induces sparsity on the rows of W , the weight vectors across all tasks share the same sparse pattern. We note that the model in (1) is equivalent to several other popular MTFL models.\nThe first example introduces a positive weight parameter ρt for t = 1, . . . , T to each term in the loss function:\nmin W∈Rd×T ∑T t=1 1 2ρt ‖yt −Xtwt‖2 + λ‖W‖2,1,\nwhich reduces to (1) by setting ỹt = yt√ ρt and X̃t = Xt√ ρt .\nThe second example introduces another regularizer to (1):\nmin W∈Rd×T ∑T t=1 1 2‖yt −Xtwt‖ 2 + λ‖W‖2,1 + ρ‖W‖2F ,\nwhere ρ is a positive parameter and ‖ · ‖F is the Frobenius norm. Let I ∈ Rd×d be the identity matrix and 0 be the d-dimensional vector with all zero entries. By letting\nX̄t = (X T t , √ 2ρtI) T , ȳt = (y T t ,0 T )T , t = 1, . . . , T,\nwe can also simplify the above MTFL model to (1). In this paper, we focus on developing the DPC screening rule for the MTFL model in (1)."
    }, {
      "heading" : "3 The Dual Problem",
      "text" : "In this section, we show that we can formulate the dual problem of the MTFL model in (1) as a projection problem by utilizing the bilinearity of the inner product.\nWe first introduce a new set of variables:\nzt = yt −Xtwt, t = 1, . . . , T. (2)\nThen, the MTFL model in (1) can be written as\nmin W,z ∑T t=1 1 2‖zt‖ 2 + λ‖W‖2,1, (3)\ns.t. zt = yt −Xtwt, t = 1, . . . , T.\nLet λθ ∈ RN be the vector of Lagrangian multipliers. Then, the Lagrangian of (1) is\nL(W, z; θ) = ∑T\nt=1\n1 2‖zt‖ 2 + λ‖W‖2,1 (4) + λ ∑T\nt=1 〈θt,yt −Xtwt − zt〉.\nTo get the dual problem, we need to minimize L(W, z; θ) over W and z. We can see that\n0 = ∇z L(W, z; θ)⇒ argminz L(W, z; θ) = λθ. (5)\nFor notational convenience, let f(W ) = λ‖W‖2,1 − λ ∑T\nt=1 〈θt, Xtwt〉.\nThus, to minimize L(W, z; θ) with respect to W , it is equivalent to minimize f(W ), i.e.,\n{Ŵ : 0 ∈ ∂W L(Ŵ , z; θ)} = {Ŵ : 0 ∈ ∂ f(Ŵ )}.\nBy the bilinearity of the inner product, we can decouple f(W ) into a set of independent subproblems. Indeed, we can rewrite the second term of f(W ) as∑T\nt=1 〈θt, Xtwt〉 = ∑T t=1 〈XTt θt,wt〉 = 〈M,W 〉, (6)\nwhere M = (XT1 θ1, . . . , X T T θT ). Eq. (6) expresses 〈M,W 〉 by the sum of the inner products of the corresponding columns. By the bilinearity of the inner product, we can also express 〈M,W 〉 by the sum of the inner products of the corresponding rows:∑T\nt=1 〈θt, Xtwt〉 = 〈M,W 〉 = ∑d `=1 〈m`,w`〉. (7)\nDenote the jth column of Xt by x (t) j . We can see that\nm` = (〈x(1)` , θ1〉, 〈x (2) ` , θ2〉, . . . , 〈x (T ) ` , θT 〉). (8)\nMoreover, as ‖W‖2,1 = ∑d `=1 ‖w`‖, Eqs. (7) implies that:\nf(W ) = λ ∑d\n`=1 f (`)(w`),\nwhere f (`)(w`) = ‖w`‖ − 〈m`,w`〉. Thus, to minimize f(W ), we can minimize each f (`)(w`) separately. The subdifferential counterpart of the Fermat’s rule (Bauschke and Combettes, 2011), i.e., 0 ∈ ∂f (`)(ŵ`), yields:\nm` ∈ { ŵ`/‖ŵ`‖, if ŵ` 6= 0, {u ∈ Rd : ‖u‖ ≤ 1}, if ŵ` = 0,\n(9)\nwhere ŵ` is the minimizer of f (`)(·). We note that Eq. (9) implies ‖m`‖ ≤ 1. If this is not the case, then f `(·) is not lower bounded (see the supplements for discussions), i.e., minw` f `(w`) = −∞. Thus, by Eqs. (5) and (9), the dual function is\nq(θ) = minW,z L(W, z; θ) (10)\n=\n{ −λ22 ‖θ‖\n2 + λ〈θ,y〉, ‖m`‖ ≤ 1, ∀ ` ∈ {1, . . . , d}, −∞, otherwise.\nMaximizing q(θ) yields the dual problem of (1) as follows:\nmax θ\n1 2‖y‖ 2 − λ22 ∥∥y λ − θ ∥∥2 , (11) s.t. ∑T t=1 〈x(t)` , θt〉 2 ≤ 1, ` = 1, . . . , d.\nIt is evident that the problem in (11) is equivalent to\nmin θ\n1 2 ∥∥y λ − θ ∥∥2 , (12) s.t. ∑T t=1 〈x(t)` , θt〉 2 ≤ 1, ` = 1, . . . , d.\nIn view of (12), it is indeed a projection problem. Let F be the feasible set of (12). Then, the optimal solution of (12), denoted by θ∗(λ), is the projection of y/λ onto F , namely,\nθ∗(λ) = PF (y λ ) . (13)"
    }, {
      "heading" : "4 The DPC Rule",
      "text" : "In this section, we present the proposed DPC screening rule for the MTFL model in (1). Inspired by the Karush-Kuhn-Tucker (KKT) conditions (Güler, 2010), in Section 4.1, we first present the general guidelines. The most challenging part lies in two folds: 1) we need to estimate the dual optimal solution as accurately as possible; 2) we need to solve a nonconvex optimization problem. In Section 4.2, we give an accurate estimation of the dual optimal solution based on the geometric properties of the projection operators. Then, in Section 4.3, we show that we can efficiently solve for the global optimum to the nonconvex problem. We present the DPC rule for the MTFL model (1) in Section 4.4."
    }, {
      "heading" : "4.1 Guidelines for Developing DPC",
      "text" : "We present the general guidelines to develop screening rules for the MTFL model (1) via the KKT conditions.\nLet W ∗(λ) = (w∗1(λ), . . . ,w ∗ T (λ)) be the optimal solution (1). By Eqs. (2), (5) and (9), the\nKKT conditions are:\nyt = Xtw ∗ t (λ) + λθ ∗ t (λ), t = 1, . . . , T, (14)\ng`(θ ∗(λ)) ∈ { 1, if (w`)∗(λ) 6= 0, [−1, 1], if (w`)∗(λ) = 0, ` = 1, . . . , d. (15)\nwhere (w`)∗(λ) is the `th row of W ∗(λ), and\ng`(θ) = ∑T\nt=1 〈x(t)` , θt〉 2, ` = 1, . . . , d. (16)\nFor ` = 1, . . . , d, Eq. (15) yields\ng`(θ ∗(λ)) < 1⇒ (w`)∗(λ) = 0. (R)\nThe rule in (R) provides a method to identify the rows in W ∗(λ) that have only zero entries. However, (R) is not applicable to real applications, as it assumes knowledge of θ∗(λ), and solving the dual problem (12) could be as expensive as solving the primal problem (1). Inspired by SAFE (El Ghaoui et al., 2012), we can first estimate a set Θ that contains θ∗(λ), and then relax (R) as follows:\nmaxθ∈Θ g`(θ) < 1⇒ (w`)∗(λ) = 0, ` = 1, . . . , d. (R∗)\nTherefore, to develop a screening rule for the MTFL model in (1), (R∗) implies that: 1) we need to estimate a region Θ—that turns out to be a ball (please refer to Section 4.2)—containing θ∗(λ); 2) we need to solve the maximization problem—that turns out to be nonconvex (please refer to Section 4.3)—on the left hand side of (R∗)."
    }, {
      "heading" : "4.2 Estimation of the Dual Optimal Solution",
      "text" : "Based on the geometric properties of the dual problem (12) that is a projection problem, we first derive the closed form solutions of the primal and dual problems for specific values of λ in Section 4.2.1, and then give an accurate estimation of θ∗(λ) for the general cases in Section 4.2.2."
    }, {
      "heading" : "4.2.1 Closed form solutions",
      "text" : "The primal and dual optimal solutions W ∗(λ) and θ∗(λ) are generally unknown. However, when the value of λ is sufficiently large, we expect that W ∗(λ) = 0, and θ∗(λ) = yλ by Eq. (14). The following theorem confirms this.\nTheorem 1. For the MTFL model in (1), let\nλmax = max `=1,...,d √∑T t=1 〈x(t)` ,y〉2. (17)\nThen, the following statements are equivalent: y λ ∈ F ⇔ θ ∗(λ) = yλ ⇔W ∗(λ) = 0⇔ λ ≥ λmax.\nRemark 1. Theorem 1 indicates that: both the primal and dual optimal solutions of the MTFL model (1) admit closed form solutions for λ ≥ λmax. Thus, we will focus on the cases with λ ∈ (0, λmax) in the rest of this paper."
    }, {
      "heading" : "4.2.2 The general cases",
      "text" : "Theorem 1 gives a closed form solution of θ∗(λ) for λ ≥ λmax. Therefore, we can estimate θ∗(λ) with λ < λmax in terms of a known θ\n∗(λ0). Specifically, we can simply set λ0 = λmax and utilize the result θ∗(λmax) = y/λmax. To make this paper self-contained, we first review some geometric properties of projection operators.\nTheorem 2. (Ruszczyński, 2006) Let C be a nonempty closed convex set. Then, for any point ū, we have\nu = PC(u)⇔ u− u ∈ NC(u),\nwhere NC(u) = {v : 〈v,u′ − u〉 ≤ 0, ∀u′ ∈ C} is called the normal cone to C at u ∈ C.\nAnother useful property of the projection operator in estimating θ∗(λ) is the so-called firmly nonexpansiveness.\nTheorem 3. (Bauschke and Combettes, 2011) Let C be a nonempty closed convex subset of a Hilbert space H. The projection operator with respect to C is firmly nonexpansive, namely, for any u1,u2 ∈ H,\n‖PC(u1)− PC(u2)‖2 + ‖(I − PC)(u1)− (I − PC)(u2)‖2\n≤ ‖u1 − u2‖2. (18)\nThe firmly nonexpansiveness of projection operators leads to the following useful result.\nCorollary 4. Let C be a nonempty closed convex subset of a Hilbert space H and 0 ∈ C. For any u ∈ H, we have:\n1. ‖PC(u)‖2 + ‖u− PC(u)‖2 ≤ ‖u‖2. 2. 〈u,u− PC(u)〉 ≥ 0.\nRemark 2. Part 1 of Corollary 4 indicates that: if a closed convex set C contains the origin, then, for any point u, the norm of its projection with respect to C is upper bounded by the norm of ‖u‖. The second part is a useful consequence of the first part and plays a crucial role in the estimation of the dual optimal solution (see Theorem 5).\nWe are now ready to present an accurate estimation of the dual optimal solution θ∗(λ).\nTheorem 5. For the MTFL model in (1), suppose that θ∗(λ0) is known with λ0 ∈ (0, λmax]. Let g` be given by Eq. (16) for ` = 1, . . . , d, and\n`∗ ∈ { argmax`=1,...,d g`(y) } . (19)\nFor any λ ∈ (0, λ0), we define\nn(λ0) =  y λ0 − θ∗(λ0), if λ0 ∈ (0, λmax), ∇g`∗ ( y λmax ) , if λ0 = λmax.\n(20)\nr(λ, λ0) = y λ − θ ∗(λ0), (21)\nr⊥(λ, λ0) = r(λ, λ0)− 〈n(λ0), r(λ, λ0)〉 ‖n(λ0)‖2 n(λ0). (22)\nThen, the following holds: 1. n(λ) ∈ NF (θ∗(λ)), 2. 〈y,n(λ0)〉 ≥ 0, 3. 〈r(λ, λ0),n(λ0)〉 ≥ 0, 4. ∥∥θ∗(λ)− (θ∗(λ0) + 12r⊥(λ, λ0))∥∥ ≤ 12‖r⊥(λ, λ0)‖.\nConsider Theorem 5. Part 1 characterizes θ∗(λ) via the normal cone. Parts 2 and 3 illustrate key geometric identities that lead to the accurate estimation of θ∗(λ) in part 4 (see supplement for details).\nRemark 3. The estimation of the dual optimal solution in DPC and EDPP (Wang et al.)—that is for Lasso—are both based on the geometric properties of the projection operators. Thus, the formulas of the estimation in Theorem 5 are similar to that of EDPP. However, we note that the estimations in DPC and EDPP are determined by the completely different geometric structures of the corresponding dual feasible sets. Problem (12) implies that the dual feasible set of the MTFL model (1) is much more complicated than that of Lasso—which is a polytope (the intersection of a set of closed half spaces). Therefore, the estimation of the dual optimal solution in DPC is much more challenging than that of EDPP, e.g., we need to find a vector in the normal cone to the dual feasible set at y/λmax [see n(λmax)].\nFor notational convenience, let\no(λ, λ0) = θ ∗(λ0) +\n1 2 r⊥(λ, λ0). (23)\nTheorem 5 implies that θ∗(λ) lies in the ball:\nΘ(λ, λ0) = { θ : ‖θ − o(λ, λ0)‖ ≤ 1\n2 ‖r⊥(λ, λ0)‖\n} . (24)"
    }, {
      "heading" : "4.3 Solving the Nonconvex Problem",
      "text" : "In this section, we solve the optimization problem in (R∗) with Θ given by Θ(λ, λ0) [see Eq. (24)], namely,\ns`(λ, λ0) = max θ∈Θ(λ,λ0)\n{ g`(θ) = ∑T t=1 〈x(t)` , θt〉 2 } . (25)\nAlthough g`(·) and Θ(λ, λ0) are convex, problem (25) is nonconvex, as it is a maximization problem. However, we can efficiently solve for the global optimal solutions to (25) by transforming it to a QP1PC via a parametrization of the constraint set. We first cite the following result.\nTheorem 6. (Gay, 1981) Let H be a symmetric matrix and D be a positive definite matrix. Consider\nmin ‖Du‖≤∆\nψ(u) = 1\n2 uTHu + qTu, (26)\nwhere ∆ > 0. Then, u∗ minimizes ψ(u) over the constraint set if and only if there exists α∗ ≥ 0— that is unique—such that (H + α∗DTD)u∗ is positive semidefinite,\n(H + α∗DTD)u∗ = −q, (27) ‖Du∗‖ = ∆, ifα∗ > 0. (28)\nWe are now ready to solve for s`(λ, λ0).\nTheorem 7. Let o = o(λ, λ0) and u ∗ be the optimal solution of problem (26) with ∆ = 12‖r ⊥(λ, λ0)‖, D = I,\nH =− diag(2‖x‖(1)` , . . . , 2‖x‖ (T ) ` ), q =− (\n2‖x(1)` ‖|〈x (1) ` ,o1〉|, . . . , 2‖x (T ) ` ‖|〈x (T ) ` ,oT 〉|\n)T ,\nnamely, there exists a α∗ ≥ 0 such that α∗ and u∗ solve Eqs. (27) and (28). Let\nρ` = max t=1,...,T\n‖x(t)` ‖, I` = { t∗ : ‖x(t∗)` ‖ = ρ` } .\nThen, the following hold: 1. α∗ is unique, and α∗ ≥ 2ρ`. 2. We define ū ∈ RT by\nūt = { −qt/(htt + 2ρ`), if t /∈ I`, 0, otherwise.\nThen, we have\nα∗ ∈\n{ 2ρ`, if ‖ū‖ ≤ ∆, and 〈x (t∗) ` ,ot∗〉 = 0, for t∗ ∈ I`,\n(2ρ`,∞), otherwise.\n3. Let V = {v ∈ RT : vt = 0 for t /∈ I`, ‖ū + v‖ = ∆}. Then, we have\nu∗ ∈ { ū + v, v ∈ V, if α∗ = 2ρ`, −(H + α∗I)−1q, otherwise.\n4. The maximum value of problem (25) is given by\ns`(λ, λ0) = ∑T\nt=1 〈x(t)` ,ot〉\n2 + α∗\n2 ∆2 − 1 2 qTu∗.\nProof. We first transform problem (25) to a QP1PC by a parameterization of Θ(λ, λ0):\nΘ(λ, λ0)\n=   o1 + u1θ1...\noT + uT θT\n : ‖u‖ ≤ r, ‖θt‖ ≤ 1, , t = 1, . . . , T  ,\nwhere u = (u1, . . . , uT ) T . We define\nh`(u, θ) = g`   o1 + u1θ1...\noT + uT θT\n  .\nThus, problem (25) becomes\ns`(λ, λ0) = max ‖u‖≤∆\n{ max\n{θ:‖θt‖≤1,t=1,...,T} h`(u, θ)\n} .\nBy the Cauchy-Schwartz inequality, for a fixed u, we have\nφ(u) = max {θ:‖θt‖≤1,t=1,...,T} h`(u, θ)\n= ∑T\nt=1 u2t ‖x (t) ` ‖ 2 + 2|ut|‖x(t)` ‖|〈x (t) ` ,ot〉|+ 〈x (t) ` ,ot〉 2.\nLet −ψ(u) = ∑T\nt=1 u 2 t ‖x (t) ` ‖ 2 + 2ut‖x(t)` ‖|〈x (t) ` ,ot〉|. We can see that\nmax‖u‖≤r φ(u) = max‖u‖≤r −ψ(u) + ∑T\nt=1 〈x(t)` ,ot〉 2.\nThus, problem (25) becomes\ns`(λ, λ0) = −min‖u‖≤r ψ(u) + ∑T\nt=1 〈x(t)` ,ot〉 2.\nTherefore, to solve (25), it suffices to solve problem (26) with ∆, D, H, and q as in the theorem. The statement follows immediately from Theorem 6.\nRemark 4. To develop the DPC rule, (R∗) implies that we only need the maximum value of problem (25). Thus, Theorem 6 does not show the global optimal solutions. However, in view of the proof, we can easily compute the global optimal solutions in terms of α∗ and u∗.\nComputing α∗ and u∗ Consider Theorem 7. If ‖ū‖ ≤ ∆ and 〈x(t∗)` ,ot∗〉 = 0 for t∗ ∈ I`, then α∗ and u∗ admit closed form solutions. Otherwise, α∗ is strictly larger than 2ρ`, which implies that H + α∗I is positive definite and invertible. If this is the case, we apply Newton’s method (Gay, 1981) to find α∗ as follows. Let\nϕ(α) = ‖(H + αI)−1q‖−1 −∆−1.\nBecause ϕ(·) is strictly increasing on (2ρ`,∞), α∗ is the unique root of ϕ(·) on (2ρ`,∞). Let α0 = 2ρ`. Then, the k th iteration of Newton’s method to solve ϕ(α∗) = 0 is:\nuk =− (H + αk−1I)−1q, (29) αk =αk−1 + ‖uk‖2 ‖uk‖ −∆\n∆uTk (H + αk−1I) −1uk\n. (30)\nAs pointed out by Moré and Sorensen (1983), Newton’s method is very efficient to find α∗ as ϕ(α) is almost linear on (2ρ`,∞). Our experiments indicates that five iterations usually leads to an accuracy higher than 10−15."
    }, {
      "heading" : "4.4 The Proposed DPC Rule",
      "text" : "As implied by R∗, we present the proposed screening rule, DPC, for the MTFL model (1) in the following theorem.\nTheorem 8. For the MTFL model (1), suppose that θ∗(λ0) is known with λ0 ∈ (0, λmax]. Then, we have\ns`(λ, λ0) < 1⇒ (w`)∗(λ) = 0, λ ∈ (0, λ0),\nwhere s`(λ, λ0) is given by Theorem 7.\nIn real applications, the optimal parameter value of λ is generally unknown. Commonly used approaches to determine an appropriate value of λ, such as cross validation and stability selection, need to solve the MTFL model over a grid of tuning parameter values λ1 > λ2 > . . . > λK, which is very time consuming. Inspired by the ideas of Strong Rule (Tibshirani et al., 2012) and SAFE (El Ghaoui et al., 2012), we develop the sequential version of DPC. Specifically, suppose that the optimal solution W ∗(λk) is known. Then, we apply DPC to identify the inactive features of MTFL model (1) at λk+1 via W ∗(λk). We repeat this process until all W ∗(λk), k = 1, . . . ,K are computed.\nCorollary 9. DPC For the MTFL model (1), suppose that we are given a sequence of parameter values λmax = λ0 > λ1 > . . . > λK. Then, for any k = 1, 2, . . . ,K− 1, if W ∗(λk) is known, we have\ns`(λk+1, λk) < 1⇒ (w`)∗(λk+1) = 0,\nwhere s`(λ, λ0) is given by Theorem 7.\nWe omit the proof of Corollary 9 since it is a direct application of Theorem 8."
    }, {
      "heading" : "5 Experiments",
      "text" : "We evaluate DPC on both synthetic and real data sets. To measure the performance of DPC, we report the rejection ratio, namely, the ratio of the number of inactive features identified by DPC to the actual number of inactive features. We also report the speedup, i.e., the ratio of the running time of solver without screening to the running time of solver with DPC. The solver is from the SLEP package (Liu et al., 2009c). For each data set, we solve the MTFL model in (1) along a sequence of 100 tuning parameter values of λ equally spaced on the logarithmic scale of λ/λmax from 1.0 to 0.01. We only evaluate DPC since no existing screening rule is applicable for the MTFL model in (1)."
    }, {
      "heading" : "5.1 Synthetic Studies",
      "text" : "We perform experiments on two synthetic data sets, called Synthetic 1 and Synthetic 2, that are commonly used in the literature (Tibshirani et al., 2012, Zou and Hastie, 2005). Both synthetic 1 and Synthetic 2 have 50 tasks. Each task contains 50 samples. For t = 1, . . . , 50, the true model is\nyt = Xtw ∗ t + 0.01 , ∼ N(0, 1).\nFor Synthetic 1, the entries of each data matrix Xt are i.i.d. standard Gaussian with pairwise correlation zero, i.e., corr ( x\n(t) i ,x (t) j ) = 0. For Synthetic 2, the entries of each data matrix Xt\nare drawn from i.i.d. standard Gaussian with pairwise correlation 0.5|i−j|, i.e., corr ( x\n(t) i ,x (t) j\n) =\n0.5|i−j|. To construct w∗t , we first randomly select 10% of the features. Then, the corresponding components of w∗t are populated from a standard Gaussian, and the remaining ones are set to 0. For both Synthetic 1 and Synthetic 2, we set the feature dimension to 10000, 20000, and 50000, respectively. For each setting, we run 20 trials and report the average performance in Fig. 1 and Table 1.\nFig. 1 shows the rejection ratios of DPC on Synthetic 1 and Synthetic 2. For all the six settings, the rejection ratios of DPC are higher than 90%, even for small parameter values. This demonstrates one of the advantages of DPC, as previous empirical studies (El Ghaoui et al., 2012, Tibshirani et al., 2012, Wang et al.) indicate that the capability of screening rules in identifying inactive features usually decreases as the parameter value decreases. Moreover, Fig. 1 also shows that as the feature dimension increases, the rejection ratios of DPC become higher—that is very close to 1. This implies that the potential capability of DPC in identifying the inactive features on high-dimensional data sets would be even more significant.\nTable 1 presents the running time of the solver with and without DPC. The speedup is very significant, which is up to 60 times. Take Synthetic 1 for example. When the feature dimension is 50000, the solver without DPC takes about 40.68 hours to solve problem (1) at 100 paramater values. In contrast, combined with DPC, the solver only takes less than one hour to solve the same 100 problems—which leads to a speedup about 60 times. Table 1 also shows that the computational cost of DPC is very low—which is negligible compared to that of the solver without screening. Moreover, as the rejection ratios of DPC increases with feature dimension growth (see Fig. 1),\nTable 1 shows that the speedup by DPC increases as well."
    }, {
      "heading" : "5.2 Experiments on Real Data Sets",
      "text" : "We perform experiments on three real data sets: 1) the TDT2 text data set (Cai et al., 2009); 2) the animal data set (Lampert et al., 2009); 3) the Alzheimers Disease Neuroimaging Initiative (ADNI) data set (http://adni.loni.usc.edu/).\nThe Animal Data Set The data set consists of 30475 images of 50 animals classes. By following the experiment settings in Kang et al. (2011), we choose 20 animal classes in the data set: antelope, grizzly-bear, killer-whale, beaver, Dalmatian, Persiancat, horse, german- shepherd, blue-whale, Siamese-cat, skunk, ox, tiger, hippopotamus, leopard, moose, spidermonkey, humpbackwhale, elephant, and gorilla. We construct 20 tasks, where each of them is a classification task of one type of animal against all the others. For the tth task, we first randomly select 30 samples from the tth class as the positive samples; and then we randomly select 30 samples from all the other classes as the negative samples. We make use of all the seven sets of features kindly provided by Lampert et al. (2009): color histogram features, local self-similarity features, PyramidHOG (PHOG) features, SIFT features, colorSIFT features, SURF features, and DECAF features. Thus, each image is represented by a 15036-dimensional vectors. Hence, the data matrix Xt of the t\nth task is of 60× 15036, where t = 1, . . . , 20.\nThe TDT2 Data Set The original data set contains 9394 documents of 30 categories. Each document is represented by a 36771-dimensional vector. Similar to the Animal data set, we construct 30 tasks, each of which is a classification task of one category against all the others (Amit et al., 2007). Also, for the tth task, we first randomly select 50 samples from the tth category as the positive samples, and then we randomly select 50 samples from all the other categories as the negative samples. Moreover, we remove the features that have only zero entries, thus leaving us 24262 features. Hence, the data matrix Xt of the t\nth task is of 100× 24262, where t = 1, . . . , 30. The ADNI Data Set The data set consists of 747 patients with 504095 single nucleotide polymorphisms (SNPs), and the volume of 93 brain regions for each patient. We first randomly select 20 brain regions. Then, for each region, we randomly select 50 patients, and utilize the corresponding SNPs data as the data matrix and the volumes of that brain region as the response. Thus, we have 20 tasks, each of which is a regression task. The data matrix Xt of the t\nth task is of 50× 504095, where t = 1, . . . , 20.\nFig. 2 shows the rejection ratios of DPC—that are above 90%—on the aforementioned three real data sets. In particular, the rejection ratios of DPC on the ADNI data set are higher than 99% at the 100 parameter values. Table 1 shows that the resulting speedup is very significant—that is up to 270 times. We note that the feature dimension of the ADNI data set is more than half million. Without screening, Table 1 shows that the solver takes about seven days (approximately one week) to compute the MTFL model (1) at 100 parameter values. However, integrated with the DPC screening rule, the solver computes the 100 solutions in about half an hour. The experiments again indicate that DPC provides better performance (in terms of rejection ratios and speedup) for higher dimensional data sets."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose a novel screening method for the MTFL model in (1), called DPC. The DPC screening rule is based on an indepth analysis of the geometric properties of the dual problem and the dual feasible set. To the best of our knowledge, DPC is the first screening rule that is applicable to sparse models with multiple data matrices. DPC is safe in the sense that the identified features by DPC are guaranteed to have zero coefficients in the solution vectors across all tasks. Experiments on synthetic and real data sets demonstrate that DPC is very effective in identifying the inactive features, which leads to a substantial savings in computational cost and memory usage without sacrificing accuracy. Moreover, DPC is more effective as the feature dimension increases, which makes DPC a very competitive candidate for the applications of very high-dimensional data. We plan to extend DPC to more general MTFL models, e.g., the MTFL models with multiple regularizers."
    }, {
      "heading" : "A Discussions regarding to the Dual Problem of (1)",
      "text" : "Although Eq. (9) implies that ‖m`‖ ≤ 1, this might not be the case. Thus, we need to consider the following two cases.\n(i) If Eq. (9) holds, we can see that 〈m`,w`〉 = ‖w`‖ and thus\nmin w`\nf (`)(w`) = 0. (31)\nTherefore, we have\nmin W f(W ) = 0. (32)\n(ii) If Eq. (9) does not hold, i.e., ‖m`‖ > 1, we would have\ninf w`\nf (`)(w`) = −∞, (33)\nand thus\nmin W\nf(W ) = −∞. (34)\nTo see this, we define w(`)(t) = t m `\n‖m`‖ and thus\n〈m`,w(`)(t)〉 = t‖m`‖.\nThen, we have\nf (`)(w`(t)) = t(1− ‖m`‖). (35)\nBecause ‖m`‖ > 1, the above equation yields\ninf w` f (`)(w`) ≤ lim t→∞ f (`) 2 (w `(t)) = −∞. (36)\nThe above discussion implies that\nmin W f(W ) = { 0, if ‖m`‖ ≤ 1, ` = 1, . . . , d, −∞, otherwise.\n(37)"
    }, {
      "heading" : "B Proof of Theorem 1",
      "text" : "Proof. For notational convenience, let\n1. y\nλ ∈ F ;\n2. θ∗(λ) = y\nλ ;\n3. W ∗(λ) = 0;\n4. λ ≥ λmax.\nEq. (13) implies that 1 is equivalent to 2. (2 ⇔ 3) Suppose that 2 holds. Eq. (14) implies that Xtw∗t (λ) = 0 for t = 1, . . . , T . Denote the objective function of the MTFL model (1) by f(W ). We claim that W ∗(λ) must be zero. To see this, let W ∗ (λ) 6= 0 be another optimal solution of (1) and thus Xtw̄∗t (λ) = 0 for t = 1, . . . , T . However, it is evident that f(W ∗(λ)) < f(W ∗ (λ)). This leads to a contradiction. Thus, the optimal solution W ∗(λ) is zero and we have proved 2 ⇒ 3. The converse direction, i.e., 2 ⇐ 3 is a direct consequence of Eq. (14).\n(1 ⇔ 4) It is evident that 1 holds if and only if y/λ is a feasible solution of problem (12), namely, all constraints in (12) holds at y/λ. By plugging y/λ into the constraints in (12), we can see that the feasibility of y/λ is equivalent to 4. Thus, we can see that 1 is equivalent to 4. This completes the proof."
    }, {
      "heading" : "C Proof of Corollary 4",
      "text" : "Proof. 1. To show part 1, we only need to set u1 = u and u2 = 0, and then plug them into the inequality (18) [note that PC(0) = 0 since 0 ∈ C].\n2. Part 1 implies that ‖PC(u)‖ ≤ ‖u‖. Thus, we have\n‖u‖2 ≥ ‖u‖‖PC(u)‖ ≥ 〈u,PC(u)〉,\nwhich is equivalent to the statement in part 2. The proof is completed."
    }, {
      "heading" : "D Proof of Theorem 5",
      "text" : "We first cite some useful properties of the projection operators.\nLemma 10. (Ruszczyński, 2006, Bauschke and Combettes, 2011) Let C be a nonempty closed convex set of a Hilbert space and u ∈ C. Then\n1. NC(u) = {v : PC(u + v) = u}.\n2. PC(u + v) = u, ∀v ∈ NC(u).\n3. Let u /∈ C and u = PC(u). Then, PC(u + t(u− u)) = u for all t ≥ 0.\nWe are now ready to prove Theorem 5\nProof.\n(i) For λ ∈ (0, λmax), Theorem 1 implies that y/λ /∈ F . Thus, the statement holds for λ ∈ (0, λmax) by Theorem 2 and Eq. (13) [let ū = y/λ and u = θ∗(λ)]. To show the statement holds at λmax, Theorem 2 indicates that we need to show〈\n∇g`∗ ( y\nλmax\n) , θ − y\nλmax\n〉 ≤ 0, ∀θ ∈ F . (38)\nBecause g`∗(·) is convex, we have (Ruszczyński, 2006) g`∗(θ)− g`∗ ( y\nλmax\n) ≥ 〈 ∇g`∗ ( y\nλmax\n) , θ − y\nλmax\n〉 . (39)\nNote that, g` is the constraint function of the dual problem in (12). Thus, for any dual feasible solution θ ∈ F , it is evident that g`∗(θ) ≤ 1. Moreover, Eq. (17) implies that g`∗(y/λmax) = 1. Therefore, the left hand of the inequality (39) must be non-positive, which yields inequality (38). Thus, the statement holds.\n(ii) A direct application of part 2 of Corollary 4 yields〈 y\nλ0 ,n(λ0)\n〉 = 〈 y\nλ0 ,\ny\nλ0 − θ∗(λ0)\n〉 ≥ 0, ∀λ0 ∈ (0, λmax).\nWhen λ0 = λmax, by noting that n(λmax) = ∇g`∗( y λmax ), we have\n〈 y\nλmax ,n(λmax)\n〉 = T∑ t=1 2 〈 x (t) `∗ , y λmax 〉2 ≥ 0.\nThus, the statement holds.\n(iii) By Eq. (21), we have 〈r(λ, λ0),n(λ0)〉 = ( 1\nλ − 1 λ0\n) 〈y,n(λ0)〉+ 〈 y\nλ0 − θ∗(λ0),n(λ0)\n〉 . (40)\nBy Eqs. (20) and (13), the second term on the right hand side of Eq. (40) is nonnegative for all λ0 ∈ (0, λmax]. The fact that 0 ∈ F yields 〈\n0− y λmax ,n(λmax)\n〉 ≤ 0.\nThus, the first term on the right hand side of Eq. (40) is nonnegative for λ0 = λmax. For λ0 ∈ (0, λmax), part 2 of Corollary 4, Eqs. (13) and (20) imply that〈\ny\nλ0 ,\ny\nλ0 − PF\n( y\nλ0\n)〉 = 〈 y\nλ0 ,n(λ0)\n〉 ≥ 0.\nThus, the first term on the right hand side of Eq. (40) is nonnegative for λ0 ∈ (0, λmax). As a result, the inner product 〈r(λ, λ0),n(λ0)〉 is nonnegative.\n(iv) We define\nθ(t) = θ∗(λ0) + tn(λ0). (41)\nPart 1 of Lemma 10 implies that\nPF (θ(t)) = θ ∗(λ0), ∀ t ≥ 0. (42)\nThe nonexpansiveness of the projection operators yields [let u1 = y/λ and u2 = θ(t) and plug them into (18)]∥∥∥PF (y\nλ\n) − PF (θ(t)) ∥∥∥2 + ‖(PF − Id)(y λ ) − (PF − Id)(θ(t))‖2 ≤ ∥∥∥y λ − θ(t) ∥∥∥2 , ∀ t ≥ 0. By Eqs. (13), (42) and (21), the above inequality reduces to\n‖θ∗(λ)− θ∗(λ0)‖2 + ‖θ∗(λ)− θ∗(λ0)− (r(λ, λ0)− tn(λ0))‖2 ≤ ‖r(λ, λ0)− tn(λ0)‖2, ∀ t ≥ 0. (43)\nLet us consider\nmin t≥0\nr(t) = ‖r(λ, λ0)− tn(λ0)‖2. (44)\nBecause r(t) is a quadratic function of t, we can see that\nmin t≥0 r(t) = { ‖r(λ, λ0)‖2, if 〈r(λ, λ0),n(λ0)〉 < 0, ‖r⊥(λ, λ0)‖2, if 〈r(λ, λ0),n(λ0)〉 ≥ 0.\nBecause of part 3, we have\nmin t≥0\nr(t) = ‖r⊥(λ, λ0)‖2 (45)\nargmin t≥0 r(t) = 〈r(λ, λ0),n(λ0)〉 ‖n(λ0)‖2\n(46)\nPlugging Eqs. (45) and (46) into (43) yields the statement, which completes the proof.\nThe proof is complete."
    } ],
    "references" : [ {
      "title" : "Uncovering shared structures in multiclass classification",
      "author" : [ "Y. Amit", "M. Fink", "N. Srebro", "S. Ullman" ],
      "venue" : "In Proceedings of the 24th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Amit et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Amit et al\\.",
      "year" : 2007
    }, {
      "title" : "A framework for learning predictive structures from multiple tasks and unlabeled data",
      "author" : [ "R. Ando", "T. Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Ando and Zhang.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ando and Zhang.",
      "year" : 2005
    }, {
      "title" : "Multi-task feature learning",
      "author" : [ "A. Argyriou", "T. Evgeniou", "M. Pontil" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Argyriou et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Argyriou et al\\.",
      "year" : 2007
    }, {
      "title" : "Convex multi-task feature learning",
      "author" : [ "A. Argyriou", "T. Evgeniou", "M. Pontil" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Argyriou et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Argyriou et al\\.",
      "year" : 2008
    }, {
      "title" : "Task clustering and gating for bayesian multictask learning",
      "author" : [ "B. Bakker", "T. Heskes" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bakker and Heskes.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bakker and Heskes.",
      "year" : 2003
    }, {
      "title" : "Convex Analysis and Monotone Operator Theory in Hilbert Spaces",
      "author" : [ "H.H. Bauschke", "P.L. Combettes" ],
      "venue" : null,
      "citeRegEx" : "Bauschke and Combettes.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bauschke and Combettes.",
      "year" : 2011
    }, {
      "title" : "A model for inductive bias learning",
      "author" : [ "J. Baxter" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Baxter.,? \\Q2000\\E",
      "shortCiteRegEx" : "Baxter.",
      "year" : 2000
    }, {
      "title" : "Exploiting task relatedness for multiple task learning",
      "author" : [ "S. Ben-David", "R. Schuller" ],
      "venue" : "In Proceedings of Computational Learning Theory,",
      "citeRegEx" : "Ben.David and Schuller.,? \\Q2003\\E",
      "shortCiteRegEx" : "Ben.David and Schuller.",
      "year" : 2003
    }, {
      "title" : "Probabilistic dyadic data analysis with local and global consistency",
      "author" : [ "Deng Cai", "Xuanhui Wang", "Xiaofei He" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Cai et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2009
    }, {
      "title" : "Multitask learning",
      "author" : [ "R. Caruana" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Caruana.,? \\Q1997\\E",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1997
    }, {
      "title" : "A convex formulation for learning shared structures from multiple tasks",
      "author" : [ "J. Chen", "L. Tang", "J. Liu", "J. Ye" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Chen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2013
    }, {
      "title" : "Safe feature elimination in sparse supervised learning",
      "author" : [ "L. El Ghaoui", "V. Viallon", "T. Rabbani" ],
      "venue" : "Pacific Journal of Optimization,",
      "citeRegEx" : "Ghaoui et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ghaoui et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning multiple tasks with kernel methods",
      "author" : [ "T. Evgeniou", "C. Micchelli", "M. Pontil" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Evgeniou et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Evgeniou et al\\.",
      "year" : 2005
    }, {
      "title" : "Computing optimal locally constrained steps",
      "author" : [ "D. Gay" ],
      "venue" : "SIAM Journal on Scientific and Statistical Computing,",
      "citeRegEx" : "Gay.,? \\Q1981\\E",
      "shortCiteRegEx" : "Gay.",
      "year" : 1981
    }, {
      "title" : "Learning with whom to share in multi-task feature learning",
      "author" : [ "Z. Kang", "K. Grauman", "F. Sha" ],
      "venue" : "In Proceedings of the 28th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Kang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2011
    }, {
      "title" : "Tree-guided group lasso for multi-task regression with structured sparsity",
      "author" : [ "S. Kim", "E. Xing" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Kim and Xing.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kim and Xing.",
      "year" : 2009
    }, {
      "title" : "Learning to detect unseen object classes by betweenclass atttribute transfer",
      "author" : [ "C. Lampert", "H. Nickisch", "S. Harmeling" ],
      "venue" : "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Lampert et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lampert et al\\.",
      "year" : 2009
    }, {
      "title" : "Bsparsity coordinate descent procedures for the multi-task with applications to neural semantic basis discovery",
      "author" : [ "H. Liu", "M. Palatucci", "J. Zhang" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Liu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "Multi-task feature learning with efficient `2,1-norm minimization",
      "author" : [ "J. Liu", "S. Ji", "J. Ye" ],
      "venue" : "In The 25th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Liu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "SLEP: Sparse Learning with Efficient Projections",
      "author" : [ "J. Liu", "S. Ji", "J. Ye" ],
      "venue" : "Arizona State University,",
      "citeRegEx" : "Liu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "Computing a trust region step",
      "author" : [ "J. Moré", "D. Sorensen" ],
      "venue" : "SIAM Journal on Scientific and Statistical Computing,",
      "citeRegEx" : "Moré and Sorensen.,? \\Q1983\\E",
      "shortCiteRegEx" : "Moré and Sorensen.",
      "year" : 1983
    }, {
      "title" : "Safe screening of non-support vectors in pathwise SVM computation",
      "author" : [ "K. Ogawa", "Y. Suzuki", "I. Takeuchi" ],
      "venue" : "In Proceedings of the 30th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Ogawa et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ogawa et al\\.",
      "year" : 2013
    }, {
      "title" : "Nonlinear Optimization",
      "author" : [ "A. Ruszczyński" ],
      "venue" : null,
      "citeRegEx" : "Ruszczyński.,? \\Q2006\\E",
      "shortCiteRegEx" : "Ruszczyński.",
      "year" : 2006
    }, {
      "title" : "Fast projections onto mixed-norm balls with applications",
      "author" : [ "S. Sra" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "Sra.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sra.",
      "year" : 2012
    }, {
      "title" : "Regression shringkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society Series B,",
      "citeRegEx" : "Tibshirani.,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani.",
      "year" : 1996
    }, {
      "title" : "Strong rules for discarding predictors in lasso-type problems",
      "author" : [ "R. Tibshirani", "J. Bien", "J. Friedman", "T. Hastie", "N. Simon", "J. Taylor" ],
      "venue" : "Journal of the Royal Statistical Society Series B,",
      "citeRegEx" : "Tibshirani et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tibshirani et al\\.",
      "year" : 2012
    }, {
      "title" : "Two-layer feature reduction for sparse-group lasso via decomposition of convex sets",
      "author" : [ "J. Wang", "J. Ye" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Wang and Ye.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang and Ye.",
      "year" : 2014
    }, {
      "title" : "Efficient mixed-norm regularization: Algorithms and safe screening methods",
      "author" : [ "J. Wang", "J. Liu", "J. Ye" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Lasso screening rules via dual polytope projection",
      "author" : [ "J. Wang", "J. Zhou", "P. Wonka", "J. Ye" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Wang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Scaling SVM and least absolute deviations via exact data reduction",
      "author" : [ "J. Wang", "P. Wonka", "J. Ye" ],
      "venue" : "In Proceedings of the 31th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "A safe screening rule for sparse logistic regression",
      "author" : [ "J. Wang", "J. Zhou", "J. Liu", "P. Wonka", "J. Ye" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning sparse representation of high dimensional data on large scale dictionaries",
      "author" : [ "Z.J. Xiang", "H. Xu", "P.J. Ramadge" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Xiang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Xiang et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning multiple related tasks using latent independent component analysis",
      "author" : [ "J. Zhang", "Z. Ghahramani", "Y. Yang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2006
    }, {
      "title" : "Probabilistic multi-task feature selection",
      "author" : [ "Y. Zhang", "D. Yeung", "Q. Xu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2010
    }, {
      "title" : "Modeling disease progression via fused sparse group lasso",
      "author" : [ "J. Zhou", "J. Liu", "V. Narayan", "J. Ye" ],
      "venue" : "In International Conference On Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2012
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "H. Zou", "T. Hastie" ],
      "venue" : "Journal of the Royal Statistical Society Series B,",
      "citeRegEx" : "Zou and Hastie.,? \\Q2005\\E",
      "shortCiteRegEx" : "Zou and Hastie.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "One popular MTL method especially for high-dimensional data is multi-task feature learning (MTFL), which uses the group Lasso penalty to ensure that all tasks select a common set of features (Argyriou et al., 2007).",
      "startOffset" : 191,
      "endOffset" : 214
    }, {
      "referenceID" : 33,
      "context" : "MTFL has found great success in many real-world applications including but not limited to: breast cancer classification (Zhang et al., 2010), disease progression prediction (Zhou et al.",
      "startOffset" : 120,
      "endOffset" : 140
    }, {
      "referenceID" : 34,
      "context" : ", 2010), disease progression prediction (Zhou et al., 2012), gene data analysis (Kim and Xing, 2009), and neural semantic basis discovery (Liu et al.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 15,
      "context" : ", 2012), gene data analysis (Kim and Xing, 2009), and neural semantic basis discovery (Liu et al.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 26,
      "context" : ", 2014b), sparse-group Lasso (Wang and Ye, 2014), support vector machine (SVM) (Ogawa et al.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 24,
      "context" : "), for the standard Lasso problem (Tibshirani, 1996)—that assumes a single data matrix—to a popular MTFL model—that involves multiple data matrices across different tasks.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 13,
      "context" : "Then, by a carefully chosen parameterization of the constraint set, we transform the nonconvex problem to a quadratic programming problem over one quadratic constraint (QP1QC) (Gay, 1981), which can be solved for the global optimum efficiently.",
      "startOffset" : 176,
      "endOffset" : 187
    }, {
      "referenceID" : 11,
      "context" : ", Lasso (El Ghaoui et al., 2012, Wang et al., 2013b, Wang et al., Xiang et al., 2011, Tibshirani et al., 2012), nonnegative Lasso Wang and Ye (2014), group Lasso (Wang et al.",
      "startOffset" : 12,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "A widely used MTFL model (Argyriou et al., 2007) takes the form of",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "The subdifferential counterpart of the Fermat’s rule (Bauschke and Combettes, 2011), i.",
      "startOffset" : 53,
      "endOffset" : 83
    }, {
      "referenceID" : 22,
      "context" : "(Ruszczyński, 2006) Let C be a nonempty closed convex set.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "(Bauschke and Combettes, 2011) Let C be a nonempty closed convex subset of a Hilbert space H.",
      "startOffset" : 0,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : "(Gay, 1981) Let H be a symmetric matrix and D be a positive definite matrix.",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 13,
      "context" : "If this is the case, we apply Newton’s method (Gay, 1981) to find α∗ as follows.",
      "startOffset" : 46,
      "endOffset" : 57
    }, {
      "referenceID" : 20,
      "context" : "As pointed out by Moré and Sorensen (1983), Newton’s method is very efficient to find α∗ as φ(α) is almost linear on (2ρ`,∞).",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 25,
      "context" : "Inspired by the ideas of Strong Rule (Tibshirani et al., 2012) and SAFE (El Ghaoui et al.",
      "startOffset" : 37,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "2 Experiments on Real Data Sets We perform experiments on three real data sets: 1) the TDT2 text data set (Cai et al., 2009); 2) the animal data set (Lampert et al.",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : ", 2009); 2) the animal data set (Lampert et al., 2009); 3) the Alzheimers Disease Neuroimaging Initiative (ADNI) data set (http://adni.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "Similar to the Animal data set, we construct 30 tasks, each of which is a classification task of one category against all the others (Amit et al., 2007).",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 13,
      "context" : "By following the experiment settings in Kang et al. (2011), we choose 20 animal classes in the data set: antelope, grizzly-bear, killer-whale, beaver, Dalmatian, Persiancat, horse, german- shepherd, blue-whale, Siamese-cat, skunk, ox, tiger, hippopotamus, leopard, moose, spidermonkey, humpbackwhale, elephant, and gorilla.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 13,
      "context" : "By following the experiment settings in Kang et al. (2011), we choose 20 animal classes in the data set: antelope, grizzly-bear, killer-whale, beaver, Dalmatian, Persiancat, horse, german- shepherd, blue-whale, Siamese-cat, skunk, ox, tiger, hippopotamus, leopard, moose, spidermonkey, humpbackwhale, elephant, and gorilla. We construct 20 tasks, where each of them is a classification task of one type of animal against all the others. For the tth task, we first randomly select 30 samples from the tth class as the positive samples; and then we randomly select 30 samples from all the other classes as the negative samples. We make use of all the seven sets of features kindly provided by Lampert et al. (2009): color histogram features, local self-similarity features, PyramidHOG (PHOG) features, SIFT features, colorSIFT features, SURF features, and DECAF features.",
      "startOffset" : 40,
      "endOffset" : 713
    }, {
      "referenceID" : 22,
      "context" : "Because g`∗(·) is convex, we have (Ruszczyński, 2006) g`∗(θ)− g`∗ ( y λmax ) ≥ 〈 ∇g`∗ ( y λmax ) , θ − y λmax 〉 .",
      "startOffset" : 34,
      "endOffset" : 53
    } ],
    "year" : 2015,
    "abstractText" : "Multi-task feature learning (MTFL) is a powerful technique in boosting the predictive performance by learning multiple related classification/regression/clustering tasks simultaneously. However, solving the MTFL problem remains challenging when the feature dimension is extremely large. In this paper, we propose a novel screening rule—that is based on the dual projection onto convex sets (DPC)—to quickly identify the inactive features—that have zero coefficients in the solution vectors across all tasks. One of the appealing features of DPC is that: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks. Thus, by removing the inactive features from the training phase, we may have substantial savings in the computational cost and memory usage without sacrificing accuracy. To the best of our knowledge, it is the first screening rule that is applicable to sparse models with multiple data matrices. A key challenge in deriving DPC is to solve a nonconvex problem. We show that we can solve for the global optimum efficiently via a properly chosen parametrization of the constraint set. Moreover, DPC has very low computational cost and can be integrated with any existing solvers. We have evaluated the proposed DPC rule on both synthetic and real data sets. The experiments indicate that DPC is very effective in identifying the inactive features—especially for high dimensional data—which leads to a speedup up to several orders of magnitude.",
    "creator" : "LaTeX with hyperref package"
  }
}