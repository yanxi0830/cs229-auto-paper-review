{
  "name" : "1704.03298.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The MATLAB Toolbox SciXMiner: User’s Manual and Programmer’s Guide",
    "authors" : [ "Ralf Mikut", "Andreas Bartschat", "Wolfgang Doneit", "Jorge Ángel González Ordiano", "Benjamin Schott", "Johannes Stegmaier", "Simon Waczowicz", "Markus Reischl" ],
    "emails" : [ "ralf.mikut@kit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The MATLAB Toolbox SciXMiner:\nUser’s Manual and Programmer’s Guide\nRalf Mikut, Andreas Bartschat, Wolfgang Doneit, Jorge Ángel González Ordiano, Benjamin\nSchott, Johannes Stegmaier, Simon Waczowicz, Markus Reischl\nKarlsruhe Institute of Technology (KIT), Institute for Applied Computer Science\nP.O. Box 3640, 76021 Karlsruhe, Germany\nPhone: ++49/721/608-25731, Fax: ++49/721/608-25702\nEmail: ralf.mikut@kit.edu\nVersion 2017a (12.04.2017)\nar X\niv :1\n70 4.\n03 29\n8v 1\n[ cs\n.L G\n] 1\n1 A\npr 2\n01 7\nii\nContents\nContents iii"
    }, {
      "heading" : "1 Motivation 1",
      "text" : ""
    }, {
      "heading" : "2 Installation 3",
      "text" : ""
    }, {
      "heading" : "3 Methods 4",
      "text" : "3.1 Handling and functionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.2 Problem description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 3.3 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.4 Further toolboxes for classification (selection) . . . . . . . . . . . . . . . . . . . . . . . 8"
    }, {
      "heading" : "4 Working with SciXMiner 9",
      "text" : "4.1 Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 4.2 Import and export of projects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n4.2.1 Import of data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 4.2.2 Export of data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 4.2.3 Manual creation of SciXMiner projects . . . . . . . . . . . . . . . . . . . . . . 13\n4.3 Automation of analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 4.4 Errors and warnings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.5 Generation of application-specific extension packages . . . . . . . . . . . . . . . . . . . 17 4.6 Known errors and problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18"
    }, {
      "heading" : "5 Sample projects 19",
      "text" : "5.1 Building data set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 5.1.2 Preparation of the data set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 5.1.3 Application of cluster algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5.1.4 Time series prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n5.2 Iris data set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.2.2 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.2.3 Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29"
    }, {
      "heading" : "6 Menu items 32",
      "text" : "6.1 Menu items ’File’ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n6.1.1 Load project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 6.1.2 Save project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 6.1.3 Save project as... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\niii\niv Contents\n6.1.4 Mean value project (based on the selected output variable) . . . . . . . . . . . . 32 6.1.5 Export data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 6.1.6 Import data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 6.1.7 Fusion of projects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 6.1.8 Apply SciXMiner batch file . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 6.1.9 Apply SciXMiner batch file (debug mode) . . . . . . . . . . . . . . . . . . . . . 35 6.1.10 Apply SciXMiner batch file (step and debug mode) . . . . . . . . . . . . . . . . 35 6.1.11 Normative data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 6.1.12 Data mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 6.1.13 Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 6.1.14 Exit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n6.2 Menu items ’Edit’ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 6.2.1 Select . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 6.2.2 Extract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 6.2.3 Convert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 6.2.4 Delete . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 6.2.5 Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 6.2.6 Outlier detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 6.2.7 Category . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 6.2.8 Rename... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 6.2.9 Generating trigger time series . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 6.2.10 Edit trigger time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.2.11 Looking for missing data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.3 Menu items ’View’ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.3.1 Classes for selected data points . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.3.2 Number of terms for selected data points . . . . . . . . . . . . . . . . . . . . . 48 6.3.3 Time series (TS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 6.3.4 Single features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 6.3.5 Single features (multivariate) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 6.3.6 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 6.3.7 Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 6.3.8 Aggregated features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 6.3.9 Output variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 6.3.10 Spectrogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 6.3.11 Morlet spectrogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 6.3.12 Cross and Auto Correlation Functions . . . . . . . . . . . . . . . . . . . . . . . 56"
    }, {
      "heading" : "6.3.13 FFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58",
      "text" : "6.3.14 Fuzzy systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 6.3.15 Decision tree (LaTeX) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 6.3.16 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 6.3.17 Self Organizing Maps... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 6.3.18 Data point distances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 6.3.19 Project report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 6.3.20 Recoding table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 6.4 Menu items ’Data mining’ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 6.4.1 Selection and evaluation of single features . . . . . . . . . . . . . . . . . . . . . 61 6.4.2 Evaluation of time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 6.4.3 Evaluation of output variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\nContents v\n6.4.4 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 6.4.5 Time series classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 6.4.6 Hierarchical Bayes classifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 6.4.7 Fuzzy systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 6.4.8 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 6.4.9 Association analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 6.4.10 Self Organizing Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 6.4.11 Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 6.4.12 Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n6.5 Menu items ’Extras’ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.5.1 Play macro... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.5.2 Play macro (debug mode)... . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.5.3 Record macro... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.5.4 Stop macro record . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.5.5 Edit macro... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.5.6 Reset macro names . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 6.5.7 Execute M-file... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 6.5.8 Edit M-file... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 6.5.9 Edit SciXMiner batch file ... . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 6.5.10 Generate PDF project report (needs Latex) . . . . . . . . . . . . . . . . . . . . 71 6.5.11 Translate German Gait-CAD m-files and macros into English . . . . . . . . . . 72 6.5.12 Choose application-specific extension packages... . . . . . . . . . . . . . . . . . 72 6.5.13 Search path for m-files and plugins . . . . . . . . . . . . . . . . . . . . . . . . 72 6.5.14 Matlab Parallel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 6.6 Menu items ’Favorites’ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.6.1 Edit user-defined favorites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.6.2 Delete all favorites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.7 Menu items ’Window’ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.7.1 Close figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.7.2 Arrange figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.7.3 Logarithmic scaling of current figures . . . . . . . . . . . . . . . . . . . . . . . 74 6.7.4 Remove Latex codes in MATLAB figures . . . . . . . . . . . . . . . . . . . . . 74 6.7.5 Update font and font size in figures . . . . . . . . . . . . . . . . . . . . . . . . 74 6.7.6 Plot all figures as images in files . . . . . . . . . . . . . . . . . . . . . . . . . . 74 6.8 Menu items ’Help’ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 6.8.1 Show SciXMiner documentation (PDF) . . . . . . . . . . . . . . . . . . . . . . 74 6.8.2 About SciXMiner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 6.8.3 License information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74"
    }, {
      "heading" : "7 Control elements 75",
      "text" : "7.1 Control elements for ’Project overview’ . . . . . . . . . . . . . . . . . . . . . . . . . . 75 7.2 Control elements for ’Time series: General options’ . . . . . . . . . . . . . . . . . . . . 76 7.3 Control elements for ’Time series: Extraction - Parameters’ . . . . . . . . . . . . . . . . 78 7.4 Control elements for ’Plugin sequence’ . . . . . . . . . . . . . . . . . . . . . . . . . . 81 7.5 Control elements for ’Single features’ . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 7.6 Control elements for ’Data preprocessing’ . . . . . . . . . . . . . . . . . . . . . . . . . 86 7.7 Control elements for ’View: Single features’ . . . . . . . . . . . . . . . . . . . . . . . . 91 7.8 Control elements for ’View: Time series’ . . . . . . . . . . . . . . . . . . . . . . . . . 94\nvi Contents\n7.9 Control elements for ’View: Spectrogram, FFT, CCF’ . . . . . . . . . . . . . . . . . . . 97 7.10 Control elements for ’View: Classification and regression’ . . . . . . . . . . . . . . . . 100 7.11 Control elements for ’Data mining: Classification of single features’ . . . . . . . . . . . 102 7.12 Control elements for ’Data mining: Classification of time series’ . . . . . . . . . . . . . 105 7.13 Control elements for ’Data mining: Regression’ . . . . . . . . . . . . . . . . . . . . . . 111 7.14 Control elements for ’Data mining: Clustering’ . . . . . . . . . . . . . . . . . . . . . . 113 7.15 Control elements for ’Data mining: Special methods’ . . . . . . . . . . . . . . . . . . . 115 7.16 Control elements for ’Data mining: Statistical options’ . . . . . . . . . . . . . . . . . . 134 7.17 Control elements for ’Data mining: Validation’ . . . . . . . . . . . . . . . . . . . . . . 136 7.18 Control elements for ’General options’ . . . . . . . . . . . . . . . . . . . . . . . . . . . 138"
    }, {
      "heading" : "8 Feature extraction from time series 141",
      "text" : "8.1 Definition of feature types by plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 8.2 Standard plugins in SciXMiner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 8.3 Plugins for single features from single features . . . . . . . . . . . . . . . . . . . . . . 144 8.4 Defining intervals via files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 8.5 Definition of a feature ontology by categories . . . . . . . . . . . . . . . . . . . . . . . 145"
    }, {
      "heading" : "9 Conclusions and perspectives 147",
      "text" : "A Important file structures 148\nB Important internal data structures 150"
    }, {
      "heading" : "C Needed Standard Toolboxes 152",
      "text" : "D Included External Toolboxes (GNU-License) 153"
    }, {
      "heading" : "E Symbols and abbreviations 157",
      "text" : ""
    }, {
      "heading" : "F Known errors and problems 158",
      "text" : "G Version history 160 G.1 Versions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 G.2 Selected changes between Gait-CAD version 2014b and SciXMiner version 2016b . . . 160 G.3 Selected changes between SciXMiner versions 2016b and 2017a . . . . . . . . . . . . . 161"
    }, {
      "heading" : "H Plugins 162",
      "text" : "Bibliography 181"
    }, {
      "heading" : "1 Motivation",
      "text" : "The Matlab toolbox SciXMiner is designed for the visualization and analysis of time series and features with a special focus to classification problems. It was developed at the Institute of Applied Computer Science of the Karlsruhe Institute of Technology (KIT), a member of the Helmholtz Association of German Research Centres in Germany. The aim was to provide an open platform for the development and improvement of data mining methods and its applications to various medical and technical problems.\nSciXMiner bases on Matlab (tested for the version 2017a). Many functions do not require additional standard toolboxes but some parts of Signal, Statistics and Wavelet toolboxes are used for special cases. The decision to a Matlab-based solution was made to use the wide mathematical functionality of this package provided by The Mathworks Inc. MATLAB R©and Simulink R©are registered trademarks of The MathWorks, Inc.\nSciXMiner is controlled by a graphical user interface (GUI) with menu items and control elements like popup lists, checkboxes and edit elements. This makes it easier to work with SciXMiner for inexperienced users. Furthermore, an automatization and batch standardization of analyzes is possible using macros. The standard Matlab style using the command line is also available.\nSciXMiner is an open source software. The download page is\nhttp://sourceforge.net/projects/SciXMiner/.\nIt is licensed under the conditions of the GNU General Public License (GNU-GPL) of The Free Software Foundation (see http://www.fsf.org/).\nThe toolbox bases on earlier internal versions of Gait-CAD [64, 72]. Please refer to [80] if you use SciXMiner for your scientific work.\nSciXMiner contains a base toolbox and various application-specific extension packages. Up to know, six extension packages were published:\n• analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7],\n• object tracking [102, 101],\n• tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]),\n• feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71],\n• image processing and feature extraction for segmentation of grains in asphalt samples [92], and\n• extended measures and visualization to evaluate the quality of data and regression models [28]."
    }, {
      "heading" : "2 Chapter 1. Motivation",
      "text" : "This manual is organized as follows: Chapter 2 explains the installation procedure. Chapter 3 outlines the implemented functionality of SciXMiner followed by some recommendations for working with SciXMiner. The analysis of two benchmark data sets is discussed in Chapter 5. Detailed information for the use of menu items (Chapter 6) and control elements (Chapter 7) follow. Chapter 8 present possible application-specific extensions for the feature extraction from time series.\nThe appendix provide information about file formats (Appendix A), internal data structures (Appendix B), necessary standard toolboxes (Appendix C) and integrated GNU-GPL-Matlab toolboxes of different groups (Appendix D), a list of symbols and abbreviations (Appendix E) and a list of known bugs and problems (Appendix F)."
    }, {
      "heading" : "2 Installation",
      "text" : "The file SciXMiner-Installer.exe contains all necessary files as self-extracting executable. After starting the destination folder for SciXMiner has to be selected, e.g. d:\\matlab\\scixminer\\.\nIn a next step, the recent user path of the installed MATLAB version must be defined. An example is C:\\Dokumente und Einstellungen\\firstname.surname\\My Documents\\MATLAB. SciXMiner uses a subdirectory called scixminer of this directory to save options files. The user needs writing permissions for this directory.\nA starting file called scixminer.m is individually created for this computer and stored in the user path. For a different user on the same computer, scixminer.m must be copied to his/her user path. Alternatively, scixminer.m can be copied to any other path of the MATLAB search path (matlabpath).\nAlternatively, a zipped version scixminer.zip can be used. For the installation, the following steps are necessary:\n1. extract in a directory,\n2. copy start file admin\\scixminer.m in a directory of the matlab work directory (check using command matlabpath),\n3. modify the directories in this file with the directory from the first step.\nSciXMiner is now ready installed and can be started with scixminer.\nFor developers, a SVN repository is available at: https : //svn.code.sf.net/p/scixminer/scixminer.\nPlease check SciXMiner out in a directory on your computer, e.g. d : scixminer."
    }, {
      "heading" : "3 Methods",
      "text" : ""
    }, {
      "heading" : "3.1 Handling and functionality",
      "text" : "Data mining methods are useful in searching for unknown or partially known relations in large data sets (KDD: Knowledge Discovery from Databases). A well known definition is given by [30]:\nData mining is a step in the KDD process that consists of applying data analysis and discovery algorithms that produce a particular enumeration of patterns (or models) over the data.\nPattern describes typical significant characteristics in features of the data set. Hereby, a feature is an input variable for the data mining algorithm, which is relevant with respect to the data mining problem. In this manual every input variable is regarded as possible feature, as it may be helpful solving the problem.\nStarting with a verbalized data mining problem, an adequate formalization has to be found. This formalization influences as well the collection of the training data set from an (external) data base (i. e. by special import routines, like HeiDATAProViT in gait analysis [94]) as the collective of possible evaluation measures in SciXMiner (Figure 3.1)."
    }, {
      "heading" : "3.2. Problem description 5",
      "text" : "SciXMiner permits a comfortable handling of numerous algorithms for the\n• selection of data points (i. e. detection of outliers, discarding of incomplete data points and features, selection of parts of data sets),\n• extraction of features (i. e. spectrograms, FFT analysis, correlation analysis, linear filtering, calculation of extrema, mean values, fuzzification etc.),\n• evaluation of features and their selection (i. e. multivariate analysis of variances, t-test, information measures, regression analysis),\n• aggregation of features (synonym: feature transformation, e.g. Discriminant Analysis, Principal Component Analysis (PCA), Independent Component Analysis (ICA)),\n• supervised and unsupervised classification (i. e. decision trees, cluster algorithms, Bayes classifier, Artificial Neural Networks (ANN), Nearest Neighbor algorithms, Support Vector Machines (SVM), fuzzy systems) and\n• validation strategies (i. e. cross-validation, bootstrap).\nAdditionally, there are various possibilities to visualize results, import and export data sets, automatically log results and process steps in text and LATEXfiles, rename variables etc.\nDepending on the availability, functions from different toolboxes are called, thereunder standard Matlab functions, functions from internal Matlab toolboxes (see Appendix C), free available Matlab toolboxes (FastICA1, SVM and Kernel Methods Matlab Toolbox [24]2, SOM Toolbox [109]3, lp_solve4, see Appendix D) and many SciXMiner internal functions."
    }, {
      "heading" : "3.2 Problem description",
      "text" : "A mandatory item to start a calculation is a training data set with n = 1, . . . , N data points, each containing\n• sz time series (matrices XTS [n] with xTS,r[k, n] ∈ R, r = 1, . . . , sz , k = 1, . . . ,K sample points),\n• s features (vectors x[n] ∈ R with xl[n], l = 1, . . . , s) and\n• sy discrete output variables (vector y[n] ∈ N+ with yj [n], j = 1, . . . , sy).\nHere, R is the set of real numbers and N+ the set of natural numbers. Ordinal, interval-scaled, and rational-scaled features may be processed. Ordinal features are discrete in value with respect to order scale (i. e. quantities with values like very small, small, middle, and big). The values do not contain any information about the semantics of their distances. If distances between all values are the same the scale is called interval-scaled (i. e. temperature in [◦C] or [◦F]). Rational-scaled values additionally contain a natural zero-point (i. e. length in [m], temperature in [K]).\n1http://www.cis.hut.fi/projects/ica/fastica/ 2http://asi.insa-rouen.fr/∼arakotom/toolbox/index.html 3http://www.cis.hut.fi/projects/somtoolbox/ 4http://lpsolve.sourceforge.net/5.5/"
    }, {
      "heading" : "6 Chapter 3. Methods",
      "text" : "Besides further information like a priori preferences for features may be processed.\nThe purpose is mainly the generation of static or quasi-static estimations\nŷj [N + 1] =f(x[N + 1]) resp. (3.1)\nŷj [N + 1] =f(x[N + 1](XTS [N + 1])) (3.2)\nfor a data point N + 1 with an unknown output variable, as well es the generation of intermediate results like tables or catalogues of relevant features for certain problems.\nThe management of multiple output variables (i. e. diagnoses with respect to diseases in medical applications, decisions for therapies, qualitative evaluations of therapy successes, gender, age-groups etc.) for each data point allows a flexible selection of multiple classification problems. Additionally, input and output variables may be changed in dependence of the problem.\nThe training data set is given by a binary Matlab project file, containing matrices with standardized names (i. e. d_orgs for time series, d_org for features and code_alle for output variables). Additionally, matrices and structures are possible (not mandatory) to denote textual identifiers and further information. Missing information is compensated by standard values and identifiers. Using 1 GB of RAM a SciXMiner project file may have a size up to 500 MB without causing any problems with memory. Larger project files are problematic, due to the chosen data structures (arrays instead of cell arrays)."
    }, {
      "heading" : "3.3 Further reading",
      "text" : "A comprehensive illustration of all algorithms mentioned in this section can not be done within this manual. Therefore, some examples for further reading are given:\n• basic knowledge about multivariate statistics and classification [107, 69, 49] and specialties for time series [73],\n• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58],\n• cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64],\n• decision trees (basics: [16, 85], implemented design algorithms [50, 76]),\n• fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]\n• a priori relevances [114, 76],\n• Independent Component Analysis [47, 106],\n• Support Vector Machines [18, 24, 98],\n• k-Nearest Neighbor Methods [26],\n• Artificial Neural Networks [42],\n• feature aggregation and selection using wrapper approaches [88],\n• validation strategies [69], and"
    }, {
      "heading" : "3.3. Further reading 7",
      "text" : "• alternative data mining software as Weka [41], Knime [14], Apache Spark’s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]\nSelected SciXMiner applications are listed in Table 3.1.\nFurther details about parameterization are given in the description of menu items (Chapter 6) and control elements (Chapter 7)."
    }, {
      "heading" : "8 Chapter 3. Methods",
      "text" : ""
    }, {
      "heading" : "3.4 Further toolboxes for classification (selection)",
      "text" : "• PRTools (http://www.prtools.org/) University of Delft (The Netherlands): Matlab toolbox, e.g. Principal Component Analysis, Subspace classifiers, Artificial Neural Networks, without GUI, free for academic use\n• NEFCLASS (http://fuzzy.cs.uni-magdeburg.de/nefclass/) University Magdeburg (Germany): JAVA, e.g. Neuro-Fuzzy classifiers, free for academic use"
    }, {
      "heading" : "4 Working with SciXMiner",
      "text" : ""
    }, {
      "heading" : "4.1 Handling",
      "text" : "The graphical user interface (GUI) of SciXMiner contains menu items and control elements like listboxes, checkboxes and text fields. They are implemented using Matlab functions (uicontrol, uimenu), which are partially called by encapsulated SciXMiner functions with additional functionality. These elements are using callback functions to exchange data with variables from the workspace and call functions which are independent from the GUI. Thus, the Matlab-typical way of programming using a command prompt and variables is possible, too.\nThe elements of the graphical user interface are described in the following sections."
    }, {
      "heading" : "4.2 Import and export of projects",
      "text" : "SciXMiner offers the opportunity for an import and export of projects from and into ASCII files. The possible options will be explained in the next two sections with a focus to the import and export of time series. Some differences for the export of single features will be also explained."
    }, {
      "heading" : "4.2.1 Import of data",
      "text" : "Data can be imported from one file or from many files in a directory. The latter option is also possible for existing subdirectories, but requires special naming conventions.\nFor the import of time series, each data point is read from a separate file. It contains the time series in the columns and the sample points in the rows. For single features, all data points are normally read from one file containing the single features in the columns and the data points in the rows. An import from multiple files is also possible to characterize different output classes by the file and directory names.\nThe window in Fig. 4.1 will be shown for an import using File - Import data - From a directory....\nThe option ”Searching in subdirectories” defines if subdirectories are scanned for matching files or not. The option ”Write in separate projects” defines that separate SciXMiner projects are generated for each import file. These separate projects can later fused by File - Fusion of projects.\n”File extension” specifies all files for the import. Wildcards are not possible (e.g. .t*)! The ”Separator for output variables” controls the assignment of output variables. As a consequence, it is extremely important for a comfortable assignment of output variables to data points - especially for time series. It can be specified for directory names and for file names."
    }, {
      "heading" : "10 Chapter 4. Working with SciXMiner",
      "text" : "Example:\nThe import of three files\nd:\\prj\\names\\Anna\\Post-therapeutic.txt\nd:\\prj\\names\\Anna\\Pre-therapeutic.txt\nd:\\prj\\names\\Peter\\Post-therapeutic.txt\nfrom a folder d:\\prj\\names and the file extension ’*.txt’ generates two terms for the first output variable variables (”Anna”, ”Peter”) and two terms for the second output variable (”Pre-therapeutic”, ”Post-therapeutic”).\nFor File - Import data - From a file..., the single file to be imported will be selected by a standard window. After this, a slightly different window is shown to specify the import, but the lower part of the windows agrees with Fig. 4.1.\nInside a file, two different separators are expected for the separation of columns (e.g. tabulators for different time series or single features) and decimal numbers (e.g. ”Point”: 17.3, ”Comma: 17,3”). Such different separators for decimal numbers are necessary for compatibility reason for some foreign language versions as e.g. German software.\nThe option ”First row contains names” reads the names of the single features and time series from the first row of the import file. Furthermore, a fix number of the rows at the beginning of the file (”Read from"
    }, {
      "heading" : "4.2. Import and export of projects 11",
      "text" : "row”) resp. all rows starting with a specified sign (”Ignore rows starting with”) can be ignored. This is especially useful for the import of files with a header. Empty rows will be ignored too.\nThe option ”Import as” switches between an import of single features and an import of time series. A simultaneous import is not possible at the moment.\nThe option ”Normalization of time series length” is only necessary if time series with different length will be imported. SciXMiner assumes always identical lengths. Different options exist: a resampling by the standard Matlab command (from the signal toolbox) to get the length of the longest time series, a filling of shorter time series with zeros or the last valid value.\nThe last option ”Import with” switches between different import techniques. The ”Normal mode” generates a temporary file after a possible separator correction. It is normally the best version especially for large files. ”Write copy and read again” make the same line by line. ”Line-Option” is a standard line-wise reading by Matlab.\n”Standard-ASCII” uses the Matlab command load -ascii and works only, if the file contains only numbers (without variable names etc.). This option has a reduced functionality but it can be very fast.\n”Structured (with Strings)” tries to import numbers and strings for files with single features. The strings are converted into linguistic terms of output variables. This option should be used together with the option ”Write in separate projects”.\n”Importdata (MATLAB)” uses the MATLAB function ”importdata”. It tries to import numbers and strings for files with single features. The strings are converted into linguistic terms of output variables. All columns with a string element in the first data row are interpreted as string columns. Such data is read from ”importdata” as ”textdata”. This option is faster than ”Structured (with Strings)”."
    }, {
      "heading" : "4.2.2 Export of data",
      "text" : "For the export of time series, each data point is written into a separate file. It contains the time series in the columns and the sample points in the rows. The possible options are controlled be the window in Fig. 4.2.\nThe target directory must be defined manually for compatibility reasons with Matlab Version 5.3 (due to a missing window for the selection of directories). A copy from the clipboard is possible. The target directory must exist.\nThe directory and file structure names are mainly determined by the following options.\n”Write output variables in subdirectories” generates subdirectories for the first (n − 1) output variables and codes the n-th output variable in the file name. The file for each data point is saved in the directories and file with the corresponding name.\nOtherwise, the file name is generated by all terms. The parts are separated by the separator defined by ”Separator for the output variables for file names”. The option ”None” works without separators.\nIf more than one data point for a term combination exists, a further directory splitting will be done. In this case, the file name is only the number of the data point.\n”File extension” control the extension for the generated files.\nExample:"
    }, {
      "heading" : "12 Chapter 4. Working with SciXMiner",
      "text" : "An export of time series into the destination folder d:\\prj\\names in a project with the output variables ”Names” (terms: ”Anna”, ”Maria”, ”Thomas”, ”Peter”) and ”Examination” (terms: ”Pre-therapeutic”, ”Post-therapeutic”) give the following file structure using the option ”Write output variables in subdirectories”:\nd:\\prj\\names\\Anna\\Post-therapeutic.txt\nd:\\prj\\names\\Anna\\Pre-therapeutic.txt\nd:\\prj\\names\\Peter\\Post-therapeutic.txt\nd:\\prj\\names\\Peter\\Pre-therapeutic.txt\nd:\\prj\\names\\Maria\\Post-therapeutic.txt\nd:\\prj\\names\\Maria\\Pre-therapeutic.txt\nd:\\prj\\names\\Thomas\\Post-therapeutic.txt\nd:\\prj\\names\\Thomas\\Pre-therapeutic.txt\nIf more than one data point for a term combination exist, the directory and file names are slightly different, e.g.:\nd:\\prj\\names\\Anna\\Post-therapeutic\\1.txt\n4.2. Import and export of projects 13\nd:\\prj\\names\\Anna\\Post-therapeutic\\5.txt\nThe separators for columns and decimal numbers are the same as for the import.\nThe option ”Write names in first row” writes the names of the single features and time series in the first row of the export file.\n”Export only selected data points and features” uses the recent selection in the project to reduce the data for the export.\nFor the export of single features, all data points are written in one file. It contains the single features in the columns and the data points in the rows.\nTwo differences exist for the export of single features:\n• A dialog box is shown for a file selection. Single features will be always exported into one file.\n• An additional option ”Export output variables as single features” add all output variables to the single features for the export. It simplifies the matching by data points and output classes. For time series, this task is solved by the file and directory names."
    }, {
      "heading" : "4.2.3 Manual creation of SciXMiner projects",
      "text" : "In this section the creation of a SciXMiner project file is described, either manual or semi-automatic, if a fully automatic import is not possible. An overview of identifiers which are used by SciXMiner is given in Appendix B.\nSciXMiner projects are binary Matlab files. These files contain variables and at least one of the following identifiers has to be defined within the project file:\n• d_orgs: contains time series of the project. 3D-array with dimension N ×K × sz\n• d_org: contains features of the project. 2D-array with dimension N × s.\nN denotes the number of data points, K denotes the number of sample points of the time series, sz is the number of time series and s is the number of features (see Section 3.2).\nThe most comfortable way is to use the function generate_new_scixminer_project.m. This function supports the conversion of MATLAB variables into SciXMiner projects. All possible elements are explained in the following text and in the help text of the function, including some examples for the conversion.\nIf output variables are known, they may be saved in an identifier called code_alle.\nThis identifier contains an N × sy matrix with the sy different types of output variables in the columns and the class assignments of N data points in the rows. Thereby, each class is represented by a (positive) integer number. If this identifier is not defined, the class assignment of each data point is set to 1.\nOptionally, the following identifier may be defined. In order to simplify the handling of SciXMiner, the definition is recommended:"
    }, {
      "heading" : "14 Chapter 4. Working with SciXMiner",
      "text" : "• Names for output variables may be given in the identifier bez_code. This identifier contains a string-matrix, where the number of rows has to match the number of output variables. The number of columns depends on the length of the strings. An easy method to create bez_code is the integrated Matlab function strvcat. If bez_code is not defined, the output variables are named ”y1”, ”y2”, ...\n• The classes of the output variable may also be named by an identifier. Therefore, the struct zgf_y_bez with the dimension sy ×max(my,i) is used. my,i denotes the number of linguistic terms of the i-th output variable. The identifier is contained in the field name.\nExample: There are two output variables, containing two respectively four classes. The identification of these classes can be done using zgf_y_bez(1,1).name = ’Anna’; zgf_y_bez(1,2).name = ’Maria’; zgf_y_bez(1,3).name = ’Thomas’; zgf_y_bez(1,4).name = ’Peter’; zgf_y_bez(2,1).name = ’pre-therapeutic’; zgf_y_bez(2,2).name = ’post-therapeutic’;\n• The identifiers of time series are arranged in a matrix called var_bez. The number of rows of the matrix is equal to the number of time series within the project. The number of columns depends on the length of the identifiers (equal to bez_code).\n• The identifiers of features are arranged in a matrix called dorgbez. The number of rows of the matrix is equal to the number of features within the project. The number of columns depends on the length of the identifiers (equal to bez_code).\nAnother two variables may be inserted optionally, although the use is rather untypical:\n• Project-specific struct projekt: This struct might contain additional information for the project. It will read out during loading the project and are written to parameter.projekt.\n• To weight features so called a priori relevances may be used. They are multiplied by the calculated feature relevances. Thus, features with doubtful values based on difficult environments for sensor measurements may be downgraded, to force the feature selection to use reliable features for classification. These a priori relevances have to be stored in a variable called interpret_merk. interpret_merk is a s× 1 vector.\nThe project has to be saved using the suffix ”.prjz”. An easy way is the use of the Matlab command save. As additional parameter -mat has to be used."
    }, {
      "heading" : "4.3 Automation of analysis",
      "text" : "The main strategy elements for the automation of analysis processes are\n• macros,"
    }, {
      "heading" : "4.3. Automation of analysis 15",
      "text" : "• SciXMiner batch files and\n• generated PDF files with project results.\nMacros are files containing sequences of clicked menu items and control elements. A manual modification of this file is possible due to its textual Matlab syntax. They can be recorded, executed, and modified (see remarks at Extras - Record macro... and example in Section 5.1).\nTo Save a SciXMiner project automatically in a macro, the name of the project must be manually defined inside a macro to avoid the input from a GUI. This can be done by adding the following lines in a macro:\n%adds ’_new’ to the name of the current SciXMiner project, %e.g. myproject.prjz will be saved as myproject_new.prjz next_function_parameter = [parameter.projekt.datei ’_new’];\n%% save the new project eval(gaitfindobj_callback(’MI_Speichern’));\nTo execute a macro inside a other macro, the following lines must be added in the calling macro:\n%defines the name of the macro %here: computing the exponential function for all selected single features next_function_parameter = ’feature_plugin_exp.makrog’;\n%execute the macro execute_macro_inside_macro;\nA SciXMiner batch file is a file for the automatic execution of macros for one or more projects. The batch file might contain one project or a complete directory with all SciXMiner projects in all subdirectories. Optionally, a file with a list of control elements with the extension *.uihdg is loaded by File - Options - Load options. For each of these projects, the following list of macros or *.m-files or variable assignments into the fields of the variable gaitcad_extern.user is executed. These variable assignments have to be complete matlab commands. They are valid for all following macros. A recursive definition with other batch files is possible.\nDuring execution, the related projects are loaded and the macros are executed. All files must be exist. Errors does not caused a stop of the execution. Errors are only written in a file called error.log.\nThe files must hold the following scheme:\n1-Inf x following block: [ 1 x *.batch file or [ 1 x *.prjz file or directory 0-1 x *.uihdg file 1-Inf x *.makrog file or *.m file or variable assignments in gaitcad_extern.user]]\nExample for a SciXMiner batch file:\nbuilding.prjz building_regression.makrog show_report.makrog building_trigger.makrog building_teilen.makrog building_day.prjz building_cluster.makrog gaitcad_extern.user.test = 1; show_report.makrog"
    }, {
      "heading" : "16 Chapter 4. Working with SciXMiner",
      "text" : "All project, batch, macro, option and m files can be written with absolute directory names. Relative directory names should be predefined in the variable gaitcad_extern.user. This strategy simplifies the modification of SciXMiner batch files with complex function calls, e.g., for the transfer to a computer with a different folder structure. Here, a project and a macro directory as well as the recent working directory (pwd) are supported.\nAs an example, the directories can be defined in a SciXMiner batch file called local_directories.batch:\ngaitcad_extern.user.project_directory = ’d:\\projects’ gaitcad_extern.user.macro_directory = ’d:\\macros’\nThe main SciXMiner batch file can be organized as follows:\nlocal_directories.batch project_directory\\building.prjz macro_directory\\building_regression.makrog macro_directory\\show_report.m macro_directory\\building_trigger.makrog macro_directory\\building_teilen.makrog project_directory\\building_day.prjz macro_directory\\building_cluster.makrog gaitcad_extern.user.test = 1; macro_directory\\show_report.m\nSciXMiner batch files can be started from the GUI or using the Matlab command line:\ngaitbatchfile = ’d:\\matlab\\scixminer\\prj\\building_demo.batch’;scixminerbatch;\nA special option is the use of an empty project called noproject.prjz. After loading, the menu items can be enabled. Consequently, otherwise disabled menu items as e.g. the import of text files can be executed.\nPDF files can be generated for the documentation of project containing figures and latex files. This option requires a Latex installation (Open-Source). Further remarks can be found under Extras - Generate PDF project report (needs Latex).\nUsing these strategy elements, even larger projects with a high computational effort or complete directories with many projects can be processed automatically."
    }, {
      "heading" : "4.4 Errors and warnings",
      "text" : "Many errors and warnings in functions are displayed in a separated message box (see example for a warning message in Fig. 4.3).\nThree different options are available for a warning message:\n• ’Continue’: The function will be continued.\n• ’Cancel’: The function will be canceled. An error message ’Canceled by user!’ will be shown in the Matlab command window.\n• ’Ignore all warnings’: It is similar to ’Continue’, but all future warning methods will be ignored for the rest of the SciXMiner session."
    }, {
      "heading" : "4.5 Generation of application-specific extension packages",
      "text" : ""
    }, {
      "heading" : "4.5. Generation of application-specific extension packages 17",
      "text" : ""
    }, {
      "heading" : "18 Chapter 4. Working with SciXMiner",
      "text" : "with functions as e.g. menu_elements_diagnosis.m (Menu items), control_elements_diagnosis.m (Control elements), optionen_felder_diagnosis.m (location of control elements). In addition, the templates for callback functions are can be used for the integration of own functions using any parameter from the control elements or available variables."
    }, {
      "heading" : "4.6 Known errors and problems",
      "text" : "The toolbox was tested using Matlab 2017a. However, we may have overlooked one or another bug. To some extent, there arise problems based on Matlab itself. Especially, many problems can be traced back to a missing downward compatibility from Matlab which leads to differences in the syntax of some commands and demands for long-winded compromises.\nRegarding the activation and deactivation of menu items, we decided to activate some menu items although the recently chosen parameters do not permit the execution of the underlying function. However, in this way the user is enabled to recognize and to correct his mistake. If the menu items remained deactivated until correct parameters are chosen, the user would have to try various combinations without getting an output.\nErrors in functions are normally caught by separate warning and error messages. Possible Matlab editor settings ”Stop if Error” or ”Stop if Warning” lead the user in the case of warnings or errors to the corresponding source code. For safe use of Matlab, these options are better deactivated.\nKnown bugs and problems can be looked up in detail in Appendix F."
    }, {
      "heading" : "5 Sample projects",
      "text" : ""
    }, {
      "heading" : "5.1 Building data set",
      "text" : ""
    }, {
      "heading" : "5.1.1 Introduction",
      "text" : "The goal of the Building-data set was to examine the effect of different parameters on the power consumption of a known building [59]. It contains a single example with ten time series (e.g. year, month, day, hour, energy consumption, temperature) and no further features. The time series are recorded for 175 days, 24 measures a day (every hour).\nWith this data set the following topics are addressed:\n• splitting of time series and manipulation of data sets\n• usage of macros and plugins\n• application of cluster algorithms\nThe given problem is the examination and comparison of the power consumption over a day.\nThe subdirectory \"prj\" of the SciXMiner installation contains the Building data set as a SciXMiner project file (building.prjz)."
    }, {
      "heading" : "5.1.2 Preparation of the data set",
      "text" : "For the analysis of the power consumption over a day the time series have to be split in several data points, so that only the data of a single day is included in the time series. This is done via Edit - Convert - Separate time series with trigger events.... The time series is split with respect to a given trigger signal.\nA typical application is the extraction of segments of time series out of long recordings. Examples for trigger events are the occurrence of errors, start of a new measurement or external events. The trigger signal contains only zeros, except the sample points where a trigger event occurs. At these sample points, the amplitude of the trigger signal equals the linguistic term of the output variable.\nFirst, since this data set contains no defined trigger events, a trigger signal has to be created. Three different options exist\n• using a graphical editor for a manual definition by Edit - Generating trigger time series resp. Edit - Edit trigger time series,"
    }, {
      "heading" : "20 Chapter 5. Sample projects",
      "text" : "• using a macro, or\n• using a plugin for extraction of features or time series.\nThe result is the same for all approaches: a time series containing values unequal to 0 at trigger events. This time series is called trigger signal.\nIn this project, the trigger events are extracted out of a time series containing the hours of a day. The trigger events are defined as the sample points at which the hour declines from 23 to 0.\nFor the sake of completeness, we consider all three approaches for the extraction of the trigger events:"
    }, {
      "heading" : "5.1.2.1 Graphical generation and editing of a trigger time series",
      "text" : "A trigger time series can be generated or edited with the window in Fig 5.1. The details are explained in Edit - Generating trigger time series. Fig. 5.1 shows four trigger events for the sample points 23, 47, 71 and 95 with two different classes for the output variable (weekend: 1 and working day: 2). The displayed time series show the ”Hour” and the ”Energy”. The temporal resolution is tuned by the two fields in the lower left corner (left: length of the shown time series segment, here: 110, right: each n-th element is shown, here: 1)."
    }, {
      "heading" : "5.1. Building data set 21",
      "text" : "The disadvantage of this methods is very high manual effort for long time series. Consequently, the two following methods are better:"
    }, {
      "heading" : "5.1.2.2 Extraction of trigger signal by use of macros",
      "text" : "The following code have been created automatically by recording of a macro. The used time series and interval for the extraction are specific for every project. Thus, a macro is generally not usable for different projects.\nThe recording contains the following commands (applied directly in the GUI):\n• Extras - Record macro...\n• Edit - Extract - Time series -> Time series, Time series -> Single features...\n• Selection of time series \"Day\", interval \"Whole time series\" and plugin \"Velocity (V)\". Finish the selection by a click on \"OK\"\n• Extras - Stop macro record\nThe content of the macro file is as follows:\n% MAKRO AUSWAHLFENSTER Z e i t r e i h e −> Z e i t r e i h e , Z e i t r e i h e −> Einze lmerkmal 2 auswahl . gen = [ ] ;\nauswahl . gen {1}={ ’DAY’ } ; auswahl . gen {2}={ ’ Whole t ime s e r i e s ( 0 . . . 1 0 0 % ) ’ } ;\n5 auswahl . gen {3}={ ’ V e l o c i t y (V) ’ } ; e v a l ( g a i t f i n d o b j _ c a l l b a c k ( ’ MI_Extraktion_ZRZR ’ ) ) ; e v a l ( g e t ( f i g u r e _ h a n d l e ( s i z e ( f i g u r e _ h a n d l e , 1 ) , 1 ) , ’ c a l l b a c k ’ ) ) ;\nDue to the computation of the velocity in SciXMiner by the formula\nf ′(t) = f(t+ 1)− f(t− 1)\n2\nthe trigger signal contains two sample points with values unequal to zero at the trigger events. Thus, the macro has to be expanded by the following code.\n% The new t ime s e r i e s i s t h e l a s t one 2 d a y _ a b l = s q u e e z e ( d _o rgs ( 1 , : , end ) ) ;\n% S ea rc h f o r a l l s amples wi th v a l u e u n e q u a l t o z e r o indx = f i n d ( d a y _ a b l ~= 0) ;\n5 % Two s u b s e q u e n t samples a r e z e r o . The second % i s t h e wanted sample . S e t t h e f i r s t t o z e r o . d_ o r gs ( 1 , i ndx ( 1 : 2 : end ) , end ) = 0 ; 8 % The f i r s t day i s n o t i d e n t i f i e d by t h i s a p p r o a c h . % S e t i t manua l ly : d_ o r gs ( 1 , 1 , end ) = 1 ;\n11 % The a m p l i t u d e o f t h e t r i g g e r s i g n a l c o r r e s p o n d s t o t h e % c l a s s o f t h i s t r i a l . Add t e m p o r a r i l y a new c l a s s % c o n t a i n i n g t h e s u b s e q u e n t number o f t h e day : 14 i ndx = f i n d ( s q u e e z e ( d _o rgs ( 1 , : , end ) ) ~= 0) ; d_ o r gs ( 1 , indx , end ) = [ 1 : l e n g t h ( i ndx ) ] ; % C l e a r used v a r i a b l e s 17 c l e a r d a y _ a b l i ndx ;"
    }, {
      "heading" : "22 Chapter 5. Sample projects",
      "text" : "Applying this macro by Extras - Play macro... leads to a new time series which is called \"Day V\".\nThe subdirectory \"prj\" of the SciXMiner installation contains the file of the macro (comments in German) (building_trigger.makrog)."
    }, {
      "heading" : "5.1.2.3 Extraction of trigger signal by plugins",
      "text" : "A further approach of extracting a trigger signal is the usage of plugins. A plugin is a function which is automatically included in SciXMiner. The related M file has to be in the subdirectory \"\\plugins\\mgenerierung\". The code of a plugin for the extraction of the trigger signal is as follows (for a detailed description of plugins refer to Chapter 8):\n1 f u n c t i o n [ da tenOut , r e t , i n f o ] = p l u g i n _ n s p r u n g ( p a r a s , d a t e n I n )\na n z _ z r = 1 ; 4 i n f o = s t r u c t ( ’ b e s c h r e i b u n g ’ , ’ Jump t o zero ’ , ’ b e z e i c h n e r ’ , ’ NSprung ’ , ’\nanz_zr ’ , anz_zr , ’ anz_em ’ , 0 , ’ l a e n g e _ z r ’ , p a r a s . p a r . l a e n g e _ z e i t r e i h e , ’ typ ’ , ’TS ’ ) ;\ni n f o . einzug_OK = 0 ; i n f o . r i c h t u n g _ e n t f e r n e n = 0 ;\n7 i n f o . a n z _ b e n o e t i g t _ z r = 1 ;\ni n f o . e x p l a n a t i o n = s t r c a t ( ’ s e t v a l u e = 1 f o r a jump t o z e r o s , and v a l u e = 0 o t h e r w i s e . ’ ) ;\n10\ni f ( n a r g i n < 2 | i s e m p t y ( d a t e n I n ) ) d a t e n O u t = [ ] ;\n13 r e t = [ ] ; r e t u r n ;\nend ; 16\n%d e t e c t z e r o v a l u e s d a t e n O u t . d a t _ z r = ( d a t e n I n . d a t == 0) ;\n19\n%d e t e c t a jump form a non−z e r o v a l u e t o a z e r o v a l u e d a t e n O u t . d a t _ z r ( : , 2 : end , : ) = d a t e n O u t . d a t _ z r ( : , 2 : end , : ) & ( d a t e n I n . d a t ( : , 1 :\ns i z e ( d a t e n I n . da t , 2 ) −1 , : ) ~=0) ; 22\nr e t . u n g u e l t i g = 0 ; r e t . b e z e i c h n e r = ’ NSprung ’ ;\nA sample of the new time series is zero if the sample of the original time series is zero and the preceding sample of the original time series is unequal to zero. For the extraction of the trigger signal we have to apply the plugin by Edit - Extract - Time series -> Time series, Time series -> Single features... with the selections \"Original data (time series)\": \"Hour\", \"interval\": \"Whole time series\" and plugin \"Jump to zero (NSprung)\". This plugin is unable to detect the first day, since there is no \"jump\" to zero. Thus, the splitting of the time series by usage of the macro leads to one data point more than with extraction by plugin.\nThe subdirectory \"prj\" of the SciXMiner installation contains the file of the plugin (plugin_nsprung.m). It is automatically detected when a project is loaded."
    }, {
      "heading" : "5.1. Building data set 23",
      "text" : ""
    }, {
      "heading" : "5.1.2.4 Splitting of the time series",
      "text" : "After extracting the trigger signal, the project is ready to be converted. This is done by Edit - Convert - Separate time series with trigger events.... The options are set as shown in the following figure (use \"Hour NSprung\" as time series if the trigger signal is extracted by the plugin).\nIf the first option is set to \"Workspace\" instead of \"time series\", variables from the workspace can be used as trigger signals. Since the extraction of the single trials should be start exactly at a trigger event and end 23 sample points after the trigger event (resulting in 24 hours after the trigger event), the offset is set to [0, 23].\nBy clicking on \"OK\" the project is saved with a new name and loaded automatically. The new project contains the same amount of time series (10), but 175 examples instead of 1 (resp. 174 after extracting the trigger signal by the plugin)."
    }, {
      "heading" : "5.1.3 Application of cluster algorithms",
      "text" : "In this section the application of clustering is shown. It is applied to the time series \"Energy\" in the project containing the split time series. If you did not apply the conversion of the time series described in the previous sections please use the project \"building_day.prjz\", included in the subdirectory \"prj\" of the SciXMiner installation.\nIn the options window Control elements: Time series: General options the time series \"Energy\" is selected. The settings for the cluster algorithm are shown in Fig. 5.3.\nThe clustering is started by a click at Data mining - Clustering - Design and apply. If the computation is finished the result can be visualized by View - Clustering - Cluster memberships (sorted by data points).\nThe clusters correspond to weekday (Cluster 2, green), resp. weekend or vacations (Cluster 1, red). The rather long red interval belongs to Christmas time. To visualize the single time series with respect to the cluster membership, the following steps have to be accomplished (as a prerequisite, Control element:"
    }, {
      "heading" : "24 Chapter 5. Sample projects",
      "text" : "Data mining: Clustering - Append cluster as output variable has to be set to \"new cluster number\"): Set the output variable to the new one created by application of the cluster algorithm (option window Control elements: Time series: General options) and the time series to \"Energy\". By clicking at View -"
    }, {
      "heading" : "5.1. Building data set 25",
      "text" : "Time series (TS) - Original time series every example of time series \"Energy\" is plotted as single curve (cf. Fig. 5.5). The class means of the time series are plotted by clicking at View - Time series (TS) - Mean time series.\nAll commands are included in the macro building_cluster.makrog in subdirectory \"prj\" of the SciXMiner installation."
    }, {
      "heading" : "5.1.4 Time series prediction",
      "text" : "In this section the prediction of the energy time series of the building dataset is shown. The used settings for the regression are included in Fig. 5.6. All time series must be selected, including the energy time series!\nIn this example, all selected time series are used for the regression. An example using linear regression coefficients for a feature preselection is shown later using the iris dataset. The samples of the previous day are used as features for the regression (samples k − 24, k − 25, . . . , k − 48). To edit the samples normal Matlab syntax is allowed (e.g. -48:-24 or -72:-24:-168). The sample 0 is automatically removed from the vector to avoid trivial prediction.\nNecessary settings for the polynomial are the degree (here: 2, Control element: Data mining: Special methods - Polynomial degree) and the maximal number of internal features (here: 4, Control element: Data mining: Special methods - Maximal number of internal features). The maximal number of internal features specifies how many coefficients should be computed by the regression algorithm. A selection of 10 time series and 24 samples leads to a total amount of possible coefficients of 240. In this case, 4 are selected by the regression algorithm."
    }, {
      "heading" : "26 Chapter 5. Sample projects",
      "text" : "By clicking Data mining - Regression - Design and apply the regression algorithm computes the coefficients and a prediction of the time series. A graphical examination of the result can be obtained by the functions included in View - Regression (see Fig. 5.7 for a scatter plot of the output variable and the estimation).\nThe scatter plot includes all values of the output variable versus the estimation of the output variable for all samples. A disadvantage of this plot is the lost time information. But since the prediction of the output variable is added as a new time series to the project all build-in visualizations can be used, e.g. the visualization of the original time series versus the predicted time series. To force SciXMiner to plot both time series in the same subplot, the option Control element: View: Time series - Show time series as subplots must be turned off. A section of such a plot is included in Fig. 5.8.\nBy Data mining - Evaluation of time series - Linear regression coefficients (univariate) univariate re-"
    }, {
      "heading" : "5.2.2 Classification",
      "text" : ""
    }, {
      "heading" : "5.2.1 Introduction",
      "text" : ""
    }, {
      "heading" : "5.2 Iris data set",
      "text" : ""
    }, {
      "heading" : "5.2. Iris data set 27",
      "text" : ""
    }, {
      "heading" : "150 data points 2 features ... Complete ...",
      "text" : ""
    }, {
      "heading" : "28 Chapter 5. Sample projects",
      "text" : ""
    }, {
      "heading" : "5.2.3 Regression",
      "text" : ""
    }, {
      "heading" : "5.2. Iris data set 29",
      "text" : ""
    }, {
      "heading" : "30 Chapter 5. Sample projects",
      "text" : "this example the degree of the polynomial is set to 1 and the maximal number of internal features to 4 (see the example using the building dataset for a description of this option).\nThe design and application of the regression is started by Data mining - Regression - Design and apply. After exiting the algorithm the command window contains:\nFitness of regression: Mean absolute value: 0.154808 Correlation coefficient between true value and estimation: 0.964228\nto give a first impression of the regression’s quality. The estimation of the feature \"petal width\" has been added as a new feature. Make sure that you exclude the estimated feature from the list of used features for further regressions. Select a feature selection method including \"selected features\" in the description. (e.g.. \"linear regression coefficients (multivariat, selected features)\"). The output variable is automatically removed from the list of used features, but not estimations of the output variable.\nMore detailed information about the regression’s quality can be obtained by the functions in View - Regression, e.g. the real value versus the estimated value, see picture 5.13 and the coefficients of the polynomial model.\nThe used polynomial is:\npetal width = −0.722 + 0.427 · petal length (5.1) + 0.103 · sepal width\nAn examination of single regression coefficients is possible via Data mining - Selection and evaluation of single features - Linear regression coefficients (univariate) (visualization of the result by View - Single features - Show feature relevances (sorted table))."
    }, {
      "heading" : "5.2. Iris data set 31",
      "text" : ""
    }, {
      "heading" : "6 Menu items",
      "text" : ""
    }, {
      "heading" : "6.1 Menu items ’File’",
      "text" : "This point includes all file operations."
    }, {
      "heading" : "6.1.1 Load project",
      "text" : "A SciXMiner project file *.prjz will be loaded and default values are added. Necessary variables are ”d_org” or ”d_orgs”. In addition, the variables ”code”, ”code_alle”, ”dorgbez”, ”var_bez”, ”zgf_y_bez”, ”bez_code”, ”optionen”, ”projekt”, ”interpret_merk” are read."
    }, {
      "heading" : "6.1.2 Save project",
      "text" : "saves the recent SciXMiner project *.prjz using the same file name. This function saves the data set (but without analysis results as designed classifiers) and optionally the current settings of the control elements (if chosen in Control elements: General options)."
    }, {
      "heading" : "6.1.3 Save project as...",
      "text" : "saves the SciXMiner project file *.prjz with a new name (see File - Save project)."
    }, {
      "heading" : "6.1.4 Mean value project (based on the selected output variable)",
      "text" : "generates a new SciXMiner project with averaged values for all linguistic terms of the selected output variable. In the new project, only one data point per linguistic term exists."
    }, {
      "heading" : "6.1.5 Export data",
      "text" : "exports time series or single features in an ASCII file. The separators for columns and decimal numbers can be selected in the opened configuration window.\n• Time series in multiple files...: The time series of the given project (variable ”d_orgs”) will be exported into different files. We assume at least one output variable. If more output variables exist, the first (n-1) output variables\n32"
    }, {
      "heading" : "6.1. Menu items ’File’ 33",
      "text" : "build a directory structure, the information about the n-th output variable is encoded in the file name. Alternatively, all n output variables can be encoded in the file name if the option ”Separator for output variables in file name” is chosen.\n• Single features in a file...: The single feature of the given project (variable ”d_org”) will be exported as file (all data points in one file)."
    }, {
      "heading" : "6.1.6 Import data",
      "text" : "includes all functions for the import of data from files.\n• From a directory...: imports data from a directory structure. Its definition is explained in Section 4.2 and for the menu item File - Export data - Single features in a file.... The parallel import of single features and time series is impossible. Output variables should be included as single features in the file and can converted after import using Edit - Convert - Selected output variable→ Single feature. Some import versions support more complex data formats including strings (see Section 4.2 ”Import and export of projects” in the SciXMiner documentation). Such strings are converted in output variables.\n• From a file...: imports data from a file. The filenames encode the output classes. The parallel import of single features and time series is impossible. Output variables should be included as single features in the file and can converted after import using Edit - Convert - Selected output variable → Single feature.\nSome import versions support more complex data formats including strings (see Section 4.2 ”Import and export of projects” in the SciXMiner documentation). Such strings are converted in output variables.\n• Interactive from a file: imports data with the MATLAB Import Wizard.\n• Import of time series names from file: imports identifiers for time series from a file. It is expected that exactly one identifier exists for every time series. The identifiers could either be separated by line breaks (i.e. one identifier per row) or by a separating character (i.e. tabulator, comma, or semicolon).\n• Import of single feature names from file: see File - Import data - Import of time series names from file, but for single features instead of time series."
    }, {
      "heading" : "6.1.7 Fusion of projects",
      "text" : "contains functions for the fusion of existing SciXMiner projects to one joint project called fusion.prjz. It uses all projects from a directory selected by a configuration window. Incompatible projects will be ignored. Four different types are available."
    }, {
      "heading" : "34 Chapter 6. Menu items",
      "text" : "• Additional time series and single features: fuses data from all SciXMiner projects in a directory by adding the data as new time series and single features. The number of data points, the length of time series, and the number and values of output variables must be identical for all projects.\nExample:\nSciXMiner-Projekt a.prjz with 5 data points, 2 time series, 1 single feature, and 1 output variable\nSciXMiner-Projekt b.prjz with 5 data points, 2 time series, 1 single feature, and 1 output variable\nSciXMiner-Projekt fusion.prjz with 5 data points, 4 time series, 2 single feature, and 1 output variable\n• Additional data points: fuses data from all SciXMiner projects in a directory by adding the data as new data points. The number and names of single features, time series and output variables and the length of time series must be identical for all projects. Linguistic terms of the output variable are matched by their names.\nExample:\nSciXMiner-Projekt a.prjz with 5 data points, 2 time series, 1 single feature, and 1 output variable\nSciXMiner-Projekt b.prjz with 5 data points, 2 time series, 1 single feature, and 1 output variable\nSciXMiner-Projekt fusion.prjz with 10 data points, 2 time series, 1 single feature, and 1 output variable\n• Additional time series and single features (tolerant for the selected output variable and data points, NaNs are deleted): like File - Fusion of projects - Additional time series and single features, but more tolerant to differences between data points. The values of the selected output variable of the selected data points (e.g. an ID resp. identifier) in the open project are used as the base for the fusion. Each data point will be deleted if the corresponding data points in the other project for the fusion cannot be found. Non selected data points are not used for fusion.\n• Additional time series and single features (tolerant for the selected output variable and data points, NaNs are retained): like File - Fusion of projects - Additional time series and single features, but more tolerant to differences between data points. The values of the selected output variable of the selected data points (e.g. an ID resp. identifier) in the open project are used as the base for the fusion. The fused time series resp. single features of each data point are set to NaN if the corresponding data points in the other project for the fusion cannot be found. Non selected data points are not used for fusion.\n• Additional events, event types and data points:"
    }, {
      "heading" : "6.1.8 Apply SciXMiner batch file",
      "text" : "executes a SciXMiner batch file with the extension *.batch. It can include one or more projects or even directories containing projects. Optionally, a file with a list of control elements with the extension *.uihdg is loaded by File - Options - Load options. For each of these projects, a list of macros is executed. A recursive definition with other batch projects is possible."
    }, {
      "heading" : "6.1. Menu items ’File’ 35",
      "text" : ""
    }, {
      "heading" : "6.1.9 Apply SciXMiner batch file (debug mode)",
      "text" : "like File - Apply SciXMiner batch file, but with a stop after errors and warnings. This option is useful for debugging SciXMiner batch files. After a stop, the processing can be continued by return."
    }, {
      "heading" : "6.1.10 Apply SciXMiner batch file (step and debug mode)",
      "text" : "like File - Apply SciXMiner batch file (debug mode), but with an additional step-wise execution of each macro in a SciXMiner batch file (continue with return)."
    }, {
      "heading" : "6.1.11 Normative data",
      "text" : "contains all operation for the handling of normative data. Examples for normative data are measurements of healthy people in medical data analysis or a desired process behavior for technical diagnosis. Normative data are defined by mean value and standard deviations for all time series and single features. They can be used for a comparison to a freely definable reference (e.g. for a typical behavior of a plant).\n• Load normative data: loads normative data (mean and standard deviation) for time series and single features from a ’*.norm’ file.\n• Save normative data: saves normative data (mean values and standard deviations) for time series and single features into a ’*.norm’ file. In detail, the variables ”dorgbez”, ”mstd”, ”mstd_em”, ”my”, ”my_em”, ”parameter”, ”titelzeile”, ”var_bez” will be saved. This function requires an executing of File - Normative data - Mean value to normative data for the generation of normative data.\n• Mean value to normative data: uses the recent data selection for the generation of normative data. Therefore, only one class (= linguistic value for the output variable) has to be selected (e.g. in medicine: healthy people, for technical applications: error-free and normal behavior of a process or device)."
    }, {
      "heading" : "6.1.12 Data mining",
      "text" : "This menu item contains functions for loading and saving of parameters from designed data mining systems.\n• Load fuzzy system: loads a designed fuzzy system from a file with the extension ”*.fuzzy”. The assignment of input (single features) and output variables is based on the variable names. The file must contain the variables ”gaitcad_struct”, ”merkmale_projekt”, and ”mode”. The use of files from other projects gives misleading results, especially in case of identical variable names.\nThe loaded fuzzy system is not yet directly usable as a fuzzy classifier. This application requires an export by Data mining - Fuzzy systems - Export fuzzy system to classifier."
    }, {
      "heading" : "36 Chapter 6. Menu items",
      "text" : "• Load fuzzy system (only membership functions): loads only the membership functions of a pre-designed fuzzy system in a file and sets Control element: Data mining: Special methods - Type of membership function to ”Fix”.\n• Save fuzzy system: exports a designed fuzzy system in a file. The results are saved in the variables ”gaitcad_struct”, ”merkmale_projekt”, and ”mode”.\n• Export ANSI-C-Code fuzzy rulebase: exports the recent fuzzy rulebase in a C file. This C file needs a pointer to a vector x containing the selected input variables (single features) and to a pointer of membership values of the output variable. The numbers of the input variables are explained in the function header. The output of the function is the linguistic term of the output variable. The feature numbers start with x[1] instead of x[0]!\n• Load classifier: loads a designed classifier from a file with the extension ”*.class”. It contains all settings specified in Control elements: Data mining: Classification of single features and Control elements: Data mining: Special methods during the design. The assignment of input (single features) and output variables is based on the variable names. The file must contain the variables ”gaitcad_struct”, ”merkmale_projekt”, and ”mode”. The use of files from other projects might give misleading results, especially in case of identical variable names.\n• Save classifier: exports a designed classifier in a file. The results are saved in the variables ”gaitcad_struct”, ”merkmale_projekt”, and ”mode”. They contain all settings defined by the parameters in Control elements: Data mining: Classification of single features and Control elements: Data mining: Special methods.\n• Export ANSI C Code of the Bayes classifier: exports the recent Bayes classifier with feature selection and feature transformation into an ANSI C file.\n• Load regression model: loads a designed regression model from a file with the extension ”*.regression”. It contains all steps specified by Control elements: Data mining: Regression and Control elements: Data mining: Special methods during the design. The assignment of input (single features) and output variables is based on the variable names. The file must contain the variables ”gaitcad_struct”, ”merkmale_projekt”, and ”mode”. The use of files from other projects might give misleading results, especially in case of identical variable names.\n• Save regression model: exports a designed regression model in a file. The results are saved in the variables ”gaitcad_struct”, ”merkmale_projekt”, and ”mode”. They contain all steps defined by the parameters in Control elements: Data mining: Regression and Control elements: Data mining: Special methods."
    }, {
      "heading" : "6.1.13 Options",
      "text" : "contains all functions for loading and saving SciXMiner options.\n6.2. Menu items ’Edit’ 37\n• Load options: restores all options for the control elements as defined in the loaded option file with extension *.uihdg.\n• Save options: saves all options for the control elements and the command lines of the plugins into an option file with extension *.uihdg.\n• Save standard options: saves all options for the control elements and the command lines of the plugins into an option file with extension standard_options.uihdg. This file will be loaded at each SciXMiner start.\n• Load frequency list: loads a MATLAB file for the definition of frequency regions and overtones.\n6.1.14 Exit\nexits the application."
    }, {
      "heading" : "6.2 Menu items ’Edit’",
      "text" : "contains all functions to edit a data set. It includes the selection of data points, time series and single features, the extraction of new single features from time series or other single features, several conversions, functions for deletion and operations for a data preprocessing."
    }, {
      "heading" : "6.2.1 Select",
      "text" : "contains functions to select data points, single features and time series.\n• All data points: selects all data points and removes any chosen selection of data points.\n• Data points using classes ...: performs a selection of data points using a conjunction and disjunction of linguistic terms of output variables. It includes\n1. a disjunction of all selected terms of one output variable and\n2. a conjunction of the so selected data points for all output variables.\nA selection of ’ALL’ means that no data point is excluded due to its value for this output variable. The result should be evaluated using View - Classes for selected data points.\n• Data points using numbers ...: enables a manual selection of data points using its numbers.\n• Data point from GUI: selects the data points defined by Control element: General options - Manual selection of data points. This function is especially useful if the data point numbers for a desired selection are known."
    }, {
      "heading" : "38 Chapter 6. Menu items",
      "text" : "• Random data points (defined percentage of selected data points): selects a pre-defined percentage of the selected data points by chance. The percentage value is defined by Control element: General options - Percent for selection.\n• Data points via most frequent terms: selects all data points belonging to frequent terms for the selected output variable. The minimal number of data points is defined by Control element: Data preprocessing - Minimal number of data points in one class.\nExample: 100 data points for o.k., 50 data points for Error Type A, 2 data points for Error Type B; Parameter 10; Selection of 150 data points for o.k. and Error Type A\n• Data points via values of single features: selects all selected data points with logical true values in Control element: Data preprocessing - Feature values for selection for the first selected single features. Multiple queries have to be executed step by step.\nExample for the selection of all values between 5 and 10 for a single features: 1. Select single feature, 2. Write in Control element: Data preprocessing - Feature values for selection ”>5”, 3. Execute Edit - Select - Data points via values of single features, 4. Write in Control element: Data preprocessing - Feature values for selection ”<10”, 5. Execute Edit - Select - Data points via values of single features.\n• Deselect missing values for selected time series: deselects all data points from the selected data points if at least one selected time series contains missing values. The data points will no be deleted.\n• Deselect missing values for selected single features: deselects all data points from the selected data points if at least one selected single features contains missing values. The data points will no be deleted.\n• All time series: selects all time series of a project. Pushing the button ”ALL” left from control element Control element: Time series: General options - Selection of time series (TS) gives the same result.\nA selection of some time series can be done by Control element: Time series: General options - Selection of time series (TS) or the related edit field.\n• All single features: selects all single features of a project. Pushing the button ”ALL” left from control element Control element: Single features - Selection of single feature(s) (SF) gives the same result.\nA selection of some single features can be done by Control element: Single features - Selection of single feature(s) (SF) or the related edit field."
    }, {
      "heading" : "6.2.2 Extract",
      "text" : "contains all functions for the extraction of time series from time series and single features from time series or other single features.\n• Time series→ Time series, Time series→ Single features...: opens a configuration window to extract new time series or single features from existing time series. Here, the existing time series, the segment and the type of the extracted feature will be selected. The extraction is done by plugins (see Section 8.1)."
    }, {
      "heading" : "6.2. Menu items ’Edit’ 39",
      "text" : "• Time series→ Time series, Time series→ Single features (via plugin sequence)...: similar to Edit - Extract - Time series → Time series, Time series → Single features..., but uses a pre-defined sequence of plugins for extraction. In the configuration windows, only the existing time series and the segments will be selected. The feature extraction uses the sequence of plugins defined by Control element: Plugin sequence - Selection of plugins. The scheduling of the plugins might be changed in the edit field from Control element: Plugin sequence - Selection of plugins. All temporary results will be deleted.\n• Single features→ Single features (with the selected feature aggregation from Options-Data Mining: Classification of single features): extracts new single features from existing single features. It uses the parameters for feature selection and aggregation from View - Classification.\n• Statistics for single features for linguistic terms of the selected output variable: extracts new single features for each selected single feature and each data point. These new features are minimum, maximum, median, mean value and number of all selected data points with the same linguistic term of the selected output variables.\n• Term statistics for output variable...: extracts a new output variable for each output variable that is selected in the following configuration window. Each data point gets the most frequent term for all selected data points with the same linguistic term of Control element: Time series: General options - Selection of output variable. This function is especially useful to summarize data points that are replicas of the same measurement.\n• Save time series segment for feature extraction: defines the selected segment of the time series as new segment for the feature extraction. The selection is explained in Control element: Time series: General options - Time series segment from and Control element: Time series: General options - to."
    }, {
      "heading" : "6.2.3 Convert",
      "text" : "contains all functions to convert time series, single features, classes, and data points to each other.\n• Selected single features→ Output variables: converts the selected single features into output variables. The same values are used as linguistic values if the features have discrete values with not more values than a defined maximal number (defined by Control element: Single features - Number of terms for Single feature→Output variable) or if the option Control element: Single features - All values was chosen. Otherwise, a discretization is done by automatically designed fuzzy membership functions and a subsequent generation of crisp membership functions. Its number of values is chosen by Control element: Single features - Number of terms for Single feature→Output variable. The term name depends on the maximum of the fuzzy membership function (e.g. ”appr. 7.5”).\n• Selected single features→ Time series: This function assumes a temporal order of all existing single features. It converts s selected single features withN data points into 1 time series withK = s sample points. If the project yet contains time series, the number of sample points must be equal to s. Alternatively, all existing time series might be deleted."
    }, {
      "heading" : "40 Chapter 6. Menu items",
      "text" : "• Selected single feature (Timestamp)→ Single features (Year, month, day, hour, minute, second): assumes that the selected single feature is a MATLAB timestamp. This function generates new single features with numbers for the year, month, day, hour, minute and second.\n• Selected single feature (Timestamp)→Weekday: assumes that the selected single feature is a MATLAB time stamp. The result is converted in a new output variable containing the weekday.\n• All single features: Data point→ Time series: The function assumes a temporal order of all data points. The data points of all single features are converted to sample points of time series. Existing time series will be deleted. These functions make a visualization of data points as time series possible.\nExample:\nA project consists of 100 data points and 2 single features. The function generates a project with 2 time series and 100 sample points.\n• All single features: Data point→ Time series (class-wise): The function assumes a temporal order of all data points with identical values for the selected output variable. The data points of all single features are converted to sample points of time series. Existing time series will be deleted. All output variables with identical values for all new time series remain output variables. Output variables with different values are converted to new time series. Incomplete sample points are set to zero. The number of valid sample points is saved in a new single feature. These functions make a visualization of data points as time series possible.\nExample:\nA project consists of 102 data points and 2 single features. Blocks of 10 data points belong to a sensor (sensor number in first output variable). The second output variable codes the number of a measurement (1..10 for sensor 1..9, 1..12 for sensor 10).\nThe first output variable is selected. The function generates 10 new data points with 3 time series (from both single features and the second output variable) and 12 sample points. The 11th and 12th sample point is zero for the first ten data points.\n• All single features: Data point→ Time series (with sample point in output variable): converts the data points into time series, each single feature will generate a separate time series. The selected output variable must contain the sample point number in its term numbers.\n• Data points→ Single features and Single features→ Data points: exchanges data points vs. single features in a SciXMiner project or vice versa (exchange s vs. N , see Appendix Important Internal Data Structures). Existing time series will be deleted.\n• Selected time series: Sample points→ Single features: generates a new single feature from each sample of a time series. The new features are named with the time series name and ”SP k” for the k-th sample.\nExample:\nA project consists of one data point with two time series and 100 samples, the second and third time series are selected. With Edit - Convert - Selected time series: Sample points→ Single features, 200 new single features are generated by the 100 samples of the time series 2 and 3."
    }, {
      "heading" : "6.2. Menu items ’Edit’ 41",
      "text" : "• All time series: Sample points→ Data points: converts the sample points of all time series into data points of single features. The number of the sample point is saved as new feature. All single features of the old project will be deleted. The results are saved as new project. This enables a classification of time series with a separate class assignment of each sample point.\nThis function is an inverse function for Edit - Convert - All single features: Data point → Time series.\nExample:\nA project consists of 1 data point with 2 time series and 100 sample points. The menu items Edit - Convert - All time series: Sample points → Data points generates a new project without time series, but with 3 new single features (both time series and the number of the sample point) and 100 data points.\n• Time series→ Data points: converts time series into data points in a SciXMiner project (converts sz into N , see Appendix Important Internal Data Structures). This function is available if only one data point exist.\n• Separate time series with trigger events...: A time series will be segmented using a trigger time series and the resulting segments are written as new time series into data points from the variable d_orgs. In this way, events can be easily processed. A detail description can be found in Section 5.1.\nThe trigger time series is expected to be the same for all data points.\n• Shorten time series: contains all functions to shorten time series, e.g. for an easier handling of large projects.\n• Selected time series→ Project-specific time scale: converts the selected time series into a project-specific time scale. Here, only one time series has to be selected. The conversion is based on the values for the first selected data point.\n• Selected output variable→ Single feature: copies the selected output variable into an additional single feature.\n• Selected output variable→ Single features (requires digits in term names): converts the selected output variable into a single feature if the names of the linguistic terms contain numbers.\n• Selected output variable (with filenames) → Multiple output variables (path components, file): assumes filenames containing the complete path as terms of the selected output variable. New output variables will be generated. Their terms result from splitting these filenames into separate subdirectories and the filenames. The file extension (e.g. .dat) is ignored.\n• Selected output variable (Date)→ Single feature (Timestamp): interprets the terms of the selected output variable as data, the date format is defined by Control element: Data preprocessing - Date format. A single feature is generated based of a conversion of date and time into a MATLAB timestamp format.\n• Selected output variable (with filenames) → Multiple output variables (path components, file, extension):"
    }, {
      "heading" : "42 Chapter 6. Menu items",
      "text" : "like Edit - Convert - Selected output variable (with filenames)→ Multiple output variables (path components, file), but with extension as separate output variable.\n• Estimated output variable→ Output variable: converts the estimated output variable generated by a classifier (Data mining - Classification - Apply) to a new output variable.\n• Estimated output variable→ Single feature: converts the result of the last classification into a new discrete-valued single feature. This feature is called ”Estimation” + the name of the selected output variable. This menu item can be selected if a valid classification result exists resulting from an applied classifier.\n• Estimated output variable (percental)→ Single features: converts the estimated percental membership values (resulting from the last application of a classifier) to classes (linguistic terms) to single features.\n• Errors of regression model→ Single features: exports the error of the recent regression model into a single feature.\n• Add output with identical term for all data points: adds an additional output variable with a term value of 1 for all data points (useful for visualizations in a project with many output terms).\n• Class→ Class: Combine terms (OR-operation with data point selection, name of the current output variable): generates a new output variable with two linguistic terms. It results from the selected output variables and the selected data points. The first linguistic term consists of all selected data points, the second from all other data points. This is a comfortable way to combine multiple terms. Later, they can be labeled by the renaming function.\nExample:\n1. Select output variable ”Person” with linguistic terms Anna, Maria, Thomas and Peter\n2. Select terms Anna and Maria in Edit - Select - Data points using classes ...\n3. Select menu item Edit - Convert - Class → Class: Combine terms (OR-operation with data point selection, name of the current output variable)\n4. A new output variable ”Person (OR)” with terms ”Anna, Maria” and ”Rest: Thomas, Peter” will be generated\n5. It can be renamed with Edit - Rename... in ”Sex” with the new terms ”Female” and ”Male”.\n• Class→ Class: conjunction (AND) of output variables ...: generates a new output variable. The linguistic terms results from a conjunction of a set of output variables which are selected in the following configuration window.\nExample:\nConjunction of ”Name” (Terms: ”Anna”, ”Maria”, ”Thomas”, ”Peter”) and ”Examination” (”Pretherapeutic”, ”Post-therapeutic”) results in a new output variable ”AND: Name Examination” with 8 terms (”Anna Pre-therapeutic”, ..., ”Peter Post-therapeutic”).\n• Resort linguistic terms (order from GUI): sorts the linguistic terms of the selected output variable using the new order from Control element: General options - Order of linguistic terms."
    }, {
      "heading" : "6.2. Menu items ’Edit’ 43",
      "text" : "• Clean up all variable names (start and end blanks): deletes end blanks in the names of output variables, time series and single features."
    }, {
      "heading" : "6.2.4 Delete",
      "text" : "deletes data points, time series, single features, or output variables.\n• Selected data points: deletes all selected data points. Finally, the data points will be renumbered.\n• Unselected data points: deletes all non-selected data points. Finally, the data points will be renumbered.\n• Double single features, time series, and output variables: deletes doubles of single features, time series, and output variables with identical names. Such doubles result normally from a multiple feature extraction.\n• Selected time series: deletes all selected time series.\n• Unselected time series: deletes all non-selected time series.\n• Selected sample points: deletes all selected sample points out of all time series.\n• Unselected sample points: deletes all unselected sample points.\n• All single features: deletes all single features.\n• Selected single features: deletes selected single features.\n• Unselected single features: deletes unselected single features.\n• Output variable ...: deletes output variables. An additional configuration window is shown to select these output variables (multiple selection by SHIFT + left mouse button or CTRL + left mouse button).\n• Non-existing terms of the output variable: deletes these terms of the output variables without any data point in the SciXMiner project. This makes some functions (e.g., the selection of data points based on terms) easier to use.\n• Missing data: removes time series or single features with NaN- or Inf values and time series containing only zeros for all sample points of a data point. All cases are interpreted as missing values for these data points and time series resp. single features."
    }, {
      "heading" : "44 Chapter 6. Menu items",
      "text" : "Consequently, either the data point or the time series resp. the single feature will be deleted. The preference is controlled by the threshold in Control element: Data preprocessing - Threshold for deleting [Perc. missing data].\nExample: Threshold 20 %\n1. All time series with missing values for more than 20 % of the data points will be deleted.\n2. All single features with missing values for more than 20 % of the data points will be deleted.\n3. All remaining data points with missing values will be deleted.\nTip: Misleading standard values for missing values of a single feature (e.g. -1) can be set to NaN values using a macro to enable a unified handling."
    }, {
      "heading" : "6.2.5 Sorting",
      "text" : "alphabetic sorting of names in a project (i.e., very useful for fusing of projects with identical variable names in different order).\n• Single features: sorts single features in a project alphabetically.\n• Time series (TS): sorts time series in a project alphabetically.\n• Output variables: sorts output variables in a project alphabetically.\n• Images: sorts images in a project alphabetically."
    }, {
      "heading" : "6.2.6 Outlier detection",
      "text" : "generates a classifier to detect outliers for the selected single features and data points. It can be applied to unknown data points. Some hints to the algorithms are given in [19].\n• Design data set (selected data points, options from classification): The outlier detection bases on an one class classification problem. It decides if a data point belongs to this one class or not.\nHowever, the implemented methods for the feature selection and classification assume data points of at least two classes. To solve this problem, an appropriate data set must be generated. A first step to prepare this data set is the feature selection in Control element: Data mining: Classification of single features - Selection of single features. Normally, the outlier detection uses the ”selected features”.\n• Design (selected data points, designed data set): designs a classifier for the outlier detection. This step uses the data set prepared by Edit - Outlier detection - Design data set (selected data points, options from classification). Three different methods might be used: an one-class method, a distance-based method, and a density-based method. The method is chosen by Control elements: Data preprocessing."
    }, {
      "heading" : "6.2. Menu items ’Edit’ 45",
      "text" : "The one-class method uses a SVM optimization proposed by [23]. The distance-based method computes mean value and standard deviation for the selected data points and uses the Mahalanobis distance to the mean value as criterion. The density-based method evaluates the number of data points in a pre-defined neighborhood.\n• Apply (selected data points, designed data set): applies the classifier design by Edit - Outlier detection - Design (selected data points, designed data set) for the outlier detection. The parameter in Control element: Data preprocessing - Threshold for outliers tunes the threshold for this decision. The value of Control element: Data preprocessing - Save result as output variable decides if the result is added as new output variable.\nBy the menu point Edit - Outlier detection - Compute class discriminant functions (selected data points, designed data set) the outlier detection is applied for all classes. It generates a struct called ”ausreisser” containing indices of outlier data points including the values for the decision (e.g. distances). The indices address the data point numbers of all existing data points of the project.\nThe function does not select data points, e.g. to delete outliers. This functionality is done by adding the outlier decision as new output variable (see Control element: Data preprocessing - Save result as output variable) and the use of data point selection in Edit - Select - Data points using classes ....\n• Design and apply (selected data points, designed data set): designs and applies the outlier detection.\n• Compute class discriminant functions (selected data points, designed data set): analyzes for all selected data points the linguistic terms of the output variable and computes for each class limits for the outlier detection. The resulting parameters are saved in the variable ”klassen_grenzen” for each class. The element ”lp_detect” describes the parameters and the element ”ausreisser” contains the application results to the selected data points.\nIf ”New output variable” or ”Replace identical output variable” is chosen in Control element: Data preprocessing - Save result as output variable, the result is saved as output variable. It can be used to select outlier e.g. for a later visualization and deletion."
    }, {
      "heading" : "6.2.7 Category",
      "text" : "Categories summarize similar single features and time series for different analysis and visualization functions (see Section 8.5, [64]). They classify single features and time series into different output classes of multiple output variables called categories (e.g. according to the sensor type or to the kind of feature extraction). The matching is done by category files with the extension *.categories.\n• Select single features ...: enables a selection of single features by a conjunction or disjunction of multiple categories and their linguistic terms. The function assumes computed categories using Edit - Category - Compute single features.\n• Compute single features: assigns categories for all existing single features using their names. For safety reasons, the categories will be deleted if the number of single features is changed (by adding or deleting single features)."
    }, {
      "heading" : "46 Chapter 6. Menu items",
      "text" : "• Compute a priori relevances: computes a priori relevances of single features using the existing categories.\n• A priori relevances of selected features...: set the a priori relevances for all selected single features to a defined value.\n• Selection of time series ...: enables a selection of time series by a conjunction or disjunction of multiple categories and their linguistic terms. The function assumes computed categories using Edit - Category - Compute time series.\n• Compute time series: assigns categories for all existing time series using their names. For safety reasons, the categories will be deleted if the number of time series will be changed (by adding or deleting time series).\n• Search time series related to the selected single features: selects all time series belonging to the selected features. The criterion is an extraction of the selected single features from the related time series."
    }, {
      "heading" : "6.2.8 Rename...",
      "text" : "This function opens a window to rename time series, single features, output variables, and the corresponding linguistic terms (possible discrete values).\nThe upper element is used to select the type of variable to be renamed. In the element below, one time series, single feature, or output variable will be selected represented by the (old) name. For linguistic terms, an additional element is shown to the selected the term.\nThe new name must be written in the lowest element and saved by pressing ENTER.\nThe corresponding figure shows Fig. 6.1."
    }, {
      "heading" : "6.2.9 Generating trigger time series",
      "text" : "shows a separate figure (TS navigator) to generate a trigger time series (see Section 5.1.2.1 and Fig. 5.1).\nThe upper part of the figure shows time series and existing trigger events (red, dotted thin lines). The selection of the time series is done in the element at the right bottom corner of the figure. The class number for a trigger event is at the trigger line. The standard value is one. The time series window can be zoomed by two control elements in the left bottom corner (left element: length of time series segment, right: each n-th sample point will be shown). The grid is switched on or off by the corresponding element. The slide bar below the time series enables a time shift.\nA click to the time series part of the window selects a new sample point (black dashed line). A new trigger event is added for this sample point by clicking the ”Add” button. The list of recent trigger events is shown with the syntax sample point (class), e.g. sample point k=23 and class type 1: 23(1). The selected trigger event is marked. Its class label might be changed by ”+” und ”-”. The use of ”→” resp. ”←” shifts the temporal position of the event. It can be removed by the ”Remove” button. ”Go to” moves to the selected event.\n”Import” loads a variable with a pre-defined trigger time series from Matlab workspace. The length must be equal to the length of time series of the project.\nThe button ”Save trigger time series” saves the time series in the project and close the figure."
    }, {
      "heading" : "6.3. Menu items ’View’ 47",
      "text" : ""
    }, {
      "heading" : "6.2.10 Edit trigger time series",
      "text" : "edits an existing trigger time series selected by Control element: Time series: General options - Selection of time series (TS). A wrong selection leads to an error message or a time series with many trigger events. The handling is explained for Edit - Generating trigger time series."
    }, {
      "heading" : "6.2.11 Looking for missing data",
      "text" : "searches for missing values and shows the results for time series vs. data points and single features vs. data points. See Edit - Delete - Missing data for the definition of missing values."
    }, {
      "heading" : "6.3 Menu items ’View’",
      "text" : "contains all functions for visualization including many results of the applied algorithms."
    }, {
      "heading" : "6.3.1 Classes for selected data points",
      "text" : "shows the selected data points with their output variables as text file."
    }, {
      "heading" : "48 Chapter 6. Menu items",
      "text" : ""
    }, {
      "heading" : "6.3.2 Number of terms for selected data points",
      "text" : "shows the number of terms for each output variable and all selected data points. Only frequent terms are listed in detail, terms with one or two data points are only summarized. This function is suited for large projects, because in contrast to View - Classes for selected data points not all infrequent terms and data point descriptions are listed. The result is saved in a file called [project_name]_ind_terms.txt."
    }, {
      "heading" : "6.3.3 Time series (TS)",
      "text" : "contains all functions for the visualization of time series.\nIn addition, the figures can be used for the definition of segments from time series and of trigger events. The definition of segments is done by zooming to the region of interest and pushing the ”Select time series segment” item in the menu bar of the figure. The segment is now defined by the minimal and maximal value of the x-axis. A more detailed explanation is given in Control element: Time series: General options - Time series segment from and Control element: Time series: General options - to.\n• Original time series: shows all selected data points of the selected time series (Control element: Time series: General options - Selection of time series (TS)) vs. time.\n• Mean time series: shows the mean values of all selected time series (Control element: Time series: General options - Selection of time series (TS)) versus time for all linguistic terms (classes) of the selected output variable.\n• Standard deviation time series: shows the standard deviations of all selected time series (Control element: Time series: General options - Selection of time series (TS)) versus time for all linguistic terms (classes) of the selected output variable.\n• Mean and standard deviation time series: shows the mean values and standard deviations of all selected time series (Control element: Time series: General options - Selection of time series (TS)) versus time for all linguistic terms (classes) of the selected output variable.\n• Scatterplot time series: shows all sample points of the selected time series as scatter plot. The number of selected time series must be even. The corresponding linguistic terms of the selected data points are coded by color.\nExample:\nFour time series x1[k], ..., x4[k] with K = 100 sample points and N = 2 data points are selected. The results are two figures with x2 = f(x1) resp. x4 = f(x3) with 200 points each.\n• Poincare plot time series (2D): show the selected time series as 2D-Poincare plot (x-axis: x[k], y-axis x[k+1]). Subsequent time points can be plotted as connected or unconnected points, depending on the selection in Control element: View: Time series - Poincare plot: Connect points."
    }, {
      "heading" : "6.3. Menu items ’View’ 49",
      "text" : "• Poincare plot time series (3D): show the selected time series as 3D-Poincare plot (x-axis: x[k], y-axis x[k+1], z-axis x[k+2]). Subsequent time points can be plotted as connected or unconnected points, depending on the selection in Control element: View: Time series - Poincare plot: Connect points.\n• Transformations vectors for feature extraction: defines if data-dependent transformations for the feature extractions are computed from the recent data, saved and/or loaded from file. The main difference is the handling of e.g. membership functions or a principal component analysis. The saving in a file is necessary for a design with a training data set and a later usage for test data by applying the same parameters. The data is saved in a file called [project name].plugpar.\n• Bode plot for filter in plugin: shows the Bode plot for the selected filter in the plugin called plugin_filter.m. The related parameters are defined in Control element: Plugin sequence - Plugin parameter. The plot is influenced by Control element: Time series: General options - Sampling frequency of time series and Control element: Time series: General options - Unit.\n• Show all feature relevances (sorted table): shows relevances of time series sorted by descending values.\n• Show all feature relevances (unsorted table): shows relevances of time series sorted by time series numbers."
    }, {
      "heading" : "6.3.4 Single features",
      "text" : "contains all functions for the visualization of single features and their derived features.\n• Single features vs. output classes: shows term-wise histograms for the first selected single feature on the x-axis versus the number of the corresponding linguistic term of the selected output variable on y-axis.\n• Single features vs. single features: shows a scatter plot of the selected single features. The functions shows pairwise scatter plots (x2 = f(x1), x4 = f(x3) etc.) if more than three features were selected.\n• Manual class assignment via single features: enables an interactive assignment of linguistic terms of output variables or the selection of data points with a figure containing two selected output variables. Here, a figure will be opened with different options in the menu bar.\n”Mark” contains all operations for the handling of a list with interactively selected data points. This list is initialized as empty list. Different menu items exist. ”Add” adds all visible data points to the list depending on the recent axes scaling. With this option, data points can be selected by zooming and subsequent adding. ”Remove” is used to remove all visible data points from the list. All marked data points are shown with a circle mark. ”Reset” removes all data points and generates an empty list. Finally, the list can be transferred to an output variable (”Save as output variable”) or used to select data points for further analysis (”Select data points”).\nExisting corresponding images for the visible data can be displayed using ”Show related images”."
    }, {
      "heading" : "50 Chapter 6. Menu items",
      "text" : "Alternatively, ”Change: Term of the output variable ...” can assign new linguistic terms to all visible data points for the selected output variable. Here, all existing terms or a new term (”New term”) are available.\nThe figure can be opened different times with different single features. ”Refresh” updates all open figures with the new linguistic terms or the entries of the list.\n• Discrete single features (2D histogram): shows a two-dimensional histogram for the first two selected single features. Error messages are generated if the single features have continuous values or if only one or more than two single feature have been selected.\n• Discrete single features (2D histogram with output classes): shows a two-dimensional histogram for two selected single features. In contrast to View - Single features - Discrete single features (2D histogram), separate bars are shown for different linguistic terms of the selected output variable. Error messages are generated if the single features have continuous values or if only one or more than two single feature have been selected.\n• Mean and standard deviation: plots mean values and standard deviations for all selected single features in a file. The computation is done for each class of the selected output variable separately.\n• Mean, standard deviation, minimum, maximum: like View - Single features - Mean and standard deviation, but with additional minimum and maximum values.\n• Median, Minimum, Maximum: like View - Single features - Mean and standard deviation, but with median, minimum and maximum values.\n• Absolute values: writes the values of all selected single features and data points in a file tmp.txt and opens the file in an editor window. It contains also the corresponding value of the output variable. The features and output variables are tabulator separated and can easily be exported to other programs.\n• Boxplot (with Statistics Toolbox!): plots boxplots for all selected data points, single features and the selected output variable using the function boxplot.m of the Statistic Toolbox.\n• Membership function: plots a figure for each selected single feature containing the membership functions of the recent fuzzy system. Such a fuzzy system is designed by Data mining - Fuzzy systems - Design (single rules), Data mining - Fuzzy systems - Design (rule base) or Data mining - Fuzzy systems - Import fuzzy system from classifier. New membership functions will be generated for this figure if a fuzzy system does not exist or if the single feature is not a part of the fuzzy system.\n• Membership function and total histogram: like View - Single features - Membership function, but with an additional histogram describing the selected data points of the single feature.\n• Membership function and class histogram: like View - Single features - Membership function, but with separate histograms for each class of the output variable belonging to the selected data points of the single feature.\n6.3. Menu items ’View’ 51\n• Entropy: shows the entropy balance (input entropy, output entropy, and mutual information) for each selected single feature vs. the selected output variable. The single feature will be discretized using the parameters from Control element: Data mining: Special methods - Number of linguistic terms and Control element: Data mining: Special methods - Type of membership function.\n• Correlation coefficients (Pearson): writes relevant (linear) Pearson correlation coefficients between all selected output variables in a file. The relevance is defined as a larger absolute value than the threshold in Control element: Data mining: Statistical options - Threshold for correlation coefficient. The function is only able to show linear relations and assumes normal distributions of all features. Optionally, the computation can be restricted by Control element: Data mining: Statistical options - Show correlation only for one selected feature to the selected single features defined by Control element: Data mining: Regression - Output variable of regression. The reporting of p-values for correlation coefficients unequal zero is switched on and off by Control element: Data mining: Statistical options - Show p values for correlation. A method for the correction of multiple tests can be defined by Control element: Data mining: Statistical options - Bonferroni correction.\n• Correlation coefficients (Spearman): like View - Single features - Correlation coefficients (Pearson), but with Spearman correlation coefficients. They evaluate rank correlations of features. This function gives better results for nonlinear but monotone relations between features.\n• Correlation coefficients (Kendall): like View - Single features - Correlation coefficients (Pearson), but with Kendall”s tau coefficient. They evaluate rank correlations of features. This function gives better results for nonlinear but monotone relations between features.\n• Show correlation coefficients: shows linear correlation coefficients of selected single features as figure. Green values visualize positive, red values negative correlations. The color intensity is a measure for the absolute value of the correlation coefficient. Hereby, the correlation coefficient of a type according to Control element: Data mining: Statistical options - Type for correlation parameters is used.\n• Show feature relevances (graphic): summarizes calculated feature relevances as figure(s). The main advantage is a first overview about problems with a large set of single features.\nFor univariate relevances, the relevance is shown versus the feature number.\nFor multivariate relevances, two figures will be shown:\nThe first figure visualizes step-wise relevance improvements of each feature as bars versus the feature number. The lowest bar shows the univariate relevance. The second bar contains improvements for the multivariate relevance by the second feature after selecting the best univariate feature in first step. All following bars show improvements of step-wise selections of the sm = 3, 4, ... feature after preselecting sm − 1 feature in the steps before. Important features are characterized by high bars.\nThe second figure contains the calculated multivariate relevances for the selection the 1-st,..., smth feature. The relevances assume also the step-wise preselection of sm − 1 best feature in the steps before."
    }, {
      "heading" : "52 Chapter 6. Menu items",
      "text" : "• Show feature relevances (sorted table): shows all computed feature relevances sorted by descending relevance values.\n• Show feature relevances (unsorted table): shows all computed feature relevances sorted by ascending feature numbers.\n• Show a priori feature relevances (unsorted table): shows all a priori relevances sorted by ascending feature numbers.\n• Add feature relevances to an archive: saves the recent feature relevances to a archive, e.g., to compare different methods for feature evaluation. This archive can be saved as separate SciXMiner project using View - Single features - Save archive with feature relevances.\n• Save archive with feature relevances: save the recent archive with feature relevances as separate SciXMiner project ([Name of the Project ’_feature_relevances.prjz’]). In this SciXMiner project, the single features are represented as terms of an output variable and the values of the feature relevances are saved as single features."
    }, {
      "heading" : "6.3.5 Single features (multivariate)",
      "text" : "groups multidimensional visualizations for single features.\n• Parallel coordinates: plots the selected single features in parallel coordinates form.\n• Andrews Plot: plots the selected single features as an \"‘Andrews Plot\"’.\n• Glyph Plot: plots a glyph plot of the selected features.\n• Heatmap: plots a heatmap of the selected single features."
    }, {
      "heading" : "6.3.6 Classification",
      "text" : "contains all functions for the visualization of classification results.\n• Result: shows a scatterplot of the last classification result in the (optionally aggregated) feature space of the classifier. The kind of chosen linguistic terms (e.g. true class assignment, result of the classifier, a different output variable defined by Control element: View: Single features - Different output variable etc.) can be switched by the options in Control element: View: Classification and regression - Display classes for output variables.\n• 2D classification with covariance matrixes: like View - Classification - Result, but adds estimated covariance matrices for the linguistic terms of the output variable. This menu item is only available if a Bayes classifier was applied."
    }, {
      "heading" : "6.3. Menu items ’View’ 53",
      "text" : "• 2D plot classifier with support vectors: like View - Classification - Result, but marks Support vectors by triangles. This menu item is only available if a Support Vector Machine was applied as classifier.\n• ROC curve...: computes a Receiver-Operating-Characteristic curve (ROC curve). It shows all possible compromises between sensitivity and specificity with respect to the classifier decision for one selected linguistic term of the output variable. This term is chosen by a separate configuration window. The tuning parameter for the ROC curve is the gradual class membership for a decision of the last applied classifier.\n• Time series classification error vs. time: shows the classification error of a time series classification vs. time.\n• Used features of the time series classifier: shows a figure with the selected time series (marked by points) of a time series classifier vs. time. The method of feature selection and the number of selected features are controlled by Control element: Single features - Selection of single feature(s) (SF) resp. Control element: Data mining: Classification of single features - Number of selected features."
    }, {
      "heading" : "6.3.7 Regression",
      "text" : "shows the results of the most recent applied regression model. The selection of data points is fixed to the application of the regression model.\n• Output variable and estimation: shows the output variable of the regression vs. the estimation by the regression model.\n• Output variable and error: shows the output variable of the regression vs. the regression error.\n• Input variable(s), output variable, and regression function (2D resp. 3D): shows the function of the estimated regression model. It is computed by a grid of additional data points. The number of data points is defined by Control element: View: Classification and regression - Number of grid points. The grid is tuned for the analysis of the interpolation and extrapolation behavior. In addition, the values of the input variables vs. the output variables during the design are plotted as scatterplot. If a normalization or aggregation of features was chosen, the visualization shows these features instead of the original ones.\nThe function is only available for regression models with one or two inputs.\n• Input variable(s), output variable, and regression function (top view 3D): like View - Regression - Input variable(s), output variable, and regression function (2D resp. 3D), but with a two-dimensional projection (overhead) of the function. The estimation of the output variable is color-coded.\n• Protocol regression model application into file: protocol the errors statistics of the regression model to a file.\n• GUI for multidimensional visualization: opens a GUI to create a 3D extraction out of a regression model. Two input parameters can be set"
    }, {
      "heading" : "54 Chapter 6. Menu items",
      "text" : "to variable, by typing ”x” resp. ”y” into the box. The input parameters left can be set to a constant value, by typing the value into the box. This Visualization works on the same principles as View - Regression - Input variable(s), output variable, and regression function (2D resp. 3D). Moreover, the region for the scatterplot can be defined with the columns ”Min:” and ”Max:”.\nThe function is only available for existing regression models with more than two inputs.\n• Generate macro for multidimensional visualization: creates and opens a macro for 3D visualization of a regression model. Further information on handling can be found within the macro.\n• Apply macro for multidimensional visualization: executes the macro created by View - Regression - Generate macro for multidimensional visualization. Alternatively, the macro can be started the usual way via Extras - Play macro....\n• Coefficients of the polynomial model: writes the coefficients of the recent polynomial model for the regression into a file called *_poly.tex resp. *_poly.txt.\n• Structure of the MLP net: shows the structure of a MLP net for a regression model. Excitatory links are green, inhibitory links red. The color intensity is a measure for the strength of a link.\n• Multidimensional response surface (RSM, with Statistics Toolbox): opens the Matlab function rstools.m for the visualization of a Multidimensional response surface (RSM) with the selected single features and the selected output variable of the regression Control element: Data mining: Regression - Output variable of regression."
    }, {
      "heading" : "6.3.8 Aggregated features",
      "text" : "contain all functions for the visualization of aggregated single features. It includes the transformation matrices for the dimension reduction and the related membership functions for the aggregated single features.\n• Eigenvectors and transformation vectors: shows the factor loadings resp. weights of the single features for the generation of the aggregated features. For each aggregated features, a different color is chosen.\n• Factor loadings (2D eigenvectors or transformation vectors): plots the elements of the first two transformation vectors for feature aggregation as scatterplot in a two-dimensional space.\n• Membership function: plots for each aggregated feature a figure with the membership functions for all linguistic terms.\n• Membership function and total histogram: like View - Aggregated features - Membership function, but with an additional histogram for all selected data points.\n• Membership function and class histogram: like View - Aggregated features - Membership function, but with separate subfigures with histograms for each term of the selected output variable."
    }, {
      "heading" : "6.3. Menu items ’View’ 55",
      "text" : ""
    }, {
      "heading" : "6.3.9 Output variables",
      "text" : "contain all functions for the visualization of output variables.\n• Qualitative output variables: plots the selected output variable vs. the number of the related data points.\n• 2D histogram...: shows the distribution of two output variables as histogram. Both output variables are selected by a configuration window.\n• Term number of output variable: writes the number of selected data points for all linguistic terms of the output variable in a protocol file."
    }, {
      "heading" : "6.3.10 Spectrogram",
      "text" : "contains all functions for the visualization of spectrograms.\n• Compute and show: computes and shows a separate spectrogram for each selected data point and each selected time series. Each spectrogram is normalized to values between zero and one (see View - Spectrogram - Compute and show (mean for selected data points) resp. View - Spectrogram - Compute and show (mean for selected class) for mean values of spectrograms). A warning is shown for very large numbers of spectrograms due to many selected data points.\nIn many cases, the interpretability of spectrograms might be improved by adapting the selection of Control element: View: Spectrogram, FFT, CCF - Function for color bar and the related parameters. This controls the color scaling especially to a finer resolution for lower amplitudes. The compromise between time and frequency resolution is tuned by Control element: View: Spectrogram, FFT, CCF - Window size [sample points].\n• Compute: similar to View - Spectrogram - Compute and show, but only for computation of spectrograms. This saves time especially for large project with a script-based generation.\n• Show: similar to View - Spectrogram - Compute and show but only for visualization. This saves time by avoiding a new computation, e.g. during tuning of visualization parameters.\n• Principal component analysis for spectrograms: makes a Principal Component Analysis on the recent computed spectrogram. The number of principal components is defined by Control element: View: Spectrogram, FFT, CCF - Number of principal components for spectrogram. The result are figures with eigenvectors vs. frequency, percentages of eigenvalues, and aggregated features vs. time. The function is able to detect and visualize dominating signals containing a mixture of different frequencies. The interpretation might be sophisticated due to window effects. The window length is defined by (Control element: View: Spectrogram, FFT, CCF - Window size [sample points])."
    }, {
      "heading" : "56 Chapter 6. Menu items",
      "text" : "• Compute and show (mean for selected data points): computes firstly a separate spectrogram for each selected data point and each selected time series. Each spectrogram is normalized to values between zero and one. In a next step, the mean value over all data points of the computed spectrograms are computed and finally shown. The resulting ”mean spectrogram” gives only a qualitative impression due to the artifacts caused by separate normalization.\n• Compute and show (mean for selected class): similar to View - Spectrogram - Compute and show (mean for selected data points), but with separate mean values of spectrograms for all existing classes of the selected output variable. Due to the separate normalization, the result is rather qualitatively."
    }, {
      "heading" : "6.3.11 Morlet spectrogram",
      "text" : "contains all functions for the visualization of Morlet spectrograms.\n• Compute and show (selected data points and time series): computes new time series for the selected time series by convolution with complex Morlet wavelets. After the convolution, the new time series are filtered by an IIR-Filter. The IIR parameters are defined by Control element: Time series: Extraction - Parameters - Parameter IIR filter. The results are shown as tree-dimensional plots (x axis: time, y axis: frequencies, color: coefficient of the filtered signal).\nThe frequency range of the Morlet wavelets is defined by Control element: Time series: Extraction - Parameters - Frequencies (FIL, Morlet spectrogram) and Control element: Time series: Extraction - Parameters - Morlet wavelet: eigenfrequency. The parameters describe the lower and higher limit. Control element: View: Spectrogram, FFT, CCF - Morlet spectrogram: Frequency stepwidth could be used to adapt the step length (default: 1).\nThe spectrogram refers to absolute values or to relative values to a baseline. These parameters could be modified in Control element: View: Spectrogram, FFT, CCF - Morlet spectrogram (relative to baseline) and Control element: View: Spectrogram, FFT, CCF - Sample points baseline.\n• Compute and show (mean for selected class): similar to View - Morlet spectrogram - Compute and show (selected data points and time series). Here, the mean values will be computed for the different classes using the selected data points resulting in a mean Morlet spectrogram for each linguistic term.\n• Plot only: shows again the computed Morlet spectrogram."
    }, {
      "heading" : "6.3.12 Cross and Auto Correlation Functions",
      "text" : "contains all functions to show Cross and Auto Correlation Functions.\n• Separately for each data point: computes and shows auto resp. Cross Correlation Functions for each selected data point. The selection of time series is done in a separate configuration window with two fields. The selection"
    }, {
      "heading" : "6.3. Menu items ’View’ 57",
      "text" : "of identical time series leads to auto correlation functions, different time series produce Cross Correlation Functions. A multiple selection of time series is possible. See also View - Cross and Auto Correlation Functions - Mean for selected data points and View - Cross and Auto Correlation Functions - Mean for classes for a fast overview for many selected data points.\n• Separately for selected data points (Short-time analysis): similar to View - Cross and Auto Correlation Functions - Separately for each data point, but with a computation and visualization of a short-time correlation analysis. The lengths of the time window is chosen in Control element: View: Spectrogram, FFT, CCF - Window size [sample points]. This function deals with an overview about time-variant changes of correlation.\n• Mean values of correlation coefficients (defined time shift): similar to View - Cross and Auto Correlation Functions - Mean for selected data points, but extract only one sample of the computed Cross resp. Auto Correlation Function. This sample is controlled by a time shift defined in Control element: View: Spectrogram, FFT, CCF - Time shift Tau [SP]. Hereby, the correlation coefficient of a type according to Control element: Data mining: Statistical options - Type for correlation parameters is used. The results are displayed in a text file for all relevant correlations with absolute values larger than Control element: Data mining: Statistical options - Threshold for correlation coefficient. Finally, all correlations are shown in a color figure similar to View - Single features - Show correlation coefficients.\nThe main advantage of this function is a fast analysis of correlations between many time series.\n• Class mean values of correlation coefficients (with defined time shift): similar to View - Cross and Auto Correlation Functions - Mean values of correlation coefficients (defined time shift), but with separate computations for each class (see also View - Cross and Auto Correlation Functions - Mean for classes).\n• Mean for selected data points: works as View - Cross and Auto Correlation Functions - Separately for each data point, but computes mean values for all selected data points. The function is only enabled if at least two data points have been selected.\n• Mean for classes: works as View - Cross and Auto Correlation Functions - Separately for each data point, but computes mean values for all selected data points of each existing class using the selected output variable. The function is only enabled if at least two data points have been selected.\n• Correlation function for different data points and selected time series: computes and shows Cross Correlation Functions for all selected combinations of data points and time series. The selection of data points is done by a separate configuration window. Only one data point should be selected in its upper control element. In the lower control element, a multiple selection is possible (CTRL + left mouse button). The Cross Correlation Functions are generated for each possible combination of reference with comparison data points. The selection of the time series is shown in the main window.\nExample:\nReference data point: 1,\nComparison data points: 3, 4, 5,\nCombinations: 1-3, 1-4, 1-5."
    }, {
      "heading" : "58 Chapter 6. Menu items",
      "text" : "• Correlation function for different data points and selected time series (mean value): like View - Cross and Auto Correlation Functions - Correlation function for different data points and selected time series, but mean values for the correlation functions will be computed. As a result, for each time series only one function is generated.\n• Correlation coefficients selected data point and time series (pre-defined time-shift Tau): like View - Cross and Auto Correlation Functions - Correlation function for different data points and selected time series, but the data points are selected by the main window. A correlation coefficient will be computed for each combination of data points and a predefined time shift τ . The result is shown as color matrix. Hereby, the correlation coefficient of a type according to Control element: Data mining: Statistical options - Type for correlation parameters is used.\n• Mean correlation coefficients over time series for selected data points and time series (defined time shift Tau): like View - Cross and Auto Correlation Functions - Correlation coefficients selected data point and time series (pre-defined time-shift Tau), but mean values for the different time series will be computed."
    }, {
      "heading" : "6.3.13 FFT",
      "text" : "includes all functions for computation and analysis of the Fast-Fourier-Transformation (FFT).\n• Compute and show FFT (selected data points and time series): computes a Fast-Fourier-Transformation (FFT) for all selected time series and data points and plots the results. The control element Control element: View: Spectrogram, FFT, CCF - Plot FFT vs. period length (instead of frequency) switches between a plot vs. period lengths and frequency (default value).\nThe function reduces the number of sampling points to the next smaller power of two to compute a FFT and to avoid window artifacts (e.g. 5000 sampling points→ reduction to 4096).\n• Show dominant frequencies (sorted by amplitude): shows the most dominant frequencies of the FFT sorted by amplitudes. The number is defined under Control element: View: Spectrogram, FFT, CCF - Number of dominant frequencies for visualization.\n• Show dominant frequencies (sorted by frequency): shows the most dominant frequencies of the FFT sorted by frequencies. The number is defined under Control element: View: Spectrogram, FFT, CCF - Number of dominant frequencies for visualization."
    }, {
      "heading" : "6.3.14 Fuzzy systems",
      "text" : "contains all functions for the visualization of a fuzzy system designed by Data mining - Fuzzy systems - Design (single rules), Data mining - Fuzzy systems - Design (rule base) or by a chosen fuzzy classifier.\n• Show rules (with variable names): shows a list of the recent fuzzy rules. It results from loading a rule base from file, from generating single rules or rule bases. The input and output variables are written in form of the variable names."
    }, {
      "heading" : "6.3. Menu items ’View’ 59",
      "text" : "• Show rules (with variable numbers): like View - Fuzzy systems - Show rules (with variable names), but uses the numbers of input variables and a symbolic output variable y.\n• Show rules (figure): shows a fuzzy rules with a maximum number of three input variables in a figure.\nThe corresponding figure shows Fig. 6.2."
    }, {
      "heading" : "6.3.16 Clustering",
      "text" : ""
    }, {
      "heading" : "6.3.15 Decision tree (LaTeX)",
      "text" : "60 Chapter 6. Menu items\n• Clustergram: computes a two-dimensional clustering for the selected single features and data points. This command executes the function ”clustergram” of the MATLAB Bioinformatics Toolbox."
    }, {
      "heading" : "6.3.17 Self Organizing Maps...",
      "text" : "contains all functions for the visualization of Self Organizing Maps.\n• Positions: shows the positions of the weight vectors of Self Organizing Maps in the (single) feature space (for 2D using the MATLAB function plotsompos.m).\n• Hits: show the number of hits per neuron for Self-Organizing Maps (based on the design)."
    }, {
      "heading" : "6.3.18 Data point distances",
      "text" : "contains all functions to compute data point distances.\n• Compute (selected data points): computes pairwise distances between all selected data points. Here, the Frobenius norm for all selected single features will be applied. In addition, an optional normalization defined by Control element: Data mining: Classification of single features - Normalization of single features is used.\n• Compute (selected data points vs. manual selection): computes pairwise distances between all selected data points vs. all data points in Control element: General options - Manual selection of data points.\n• Compute (searching for neighbors of the first element of the manual selection): computes the nearest neighbors for the first data point in Control element: General options - Manual selection of data points using the data point distances to the selected data points.\n• Sort (VAT algorithm): resorts data point in the plot based on VAT (visual assessment of tendency) algorithm to improve the interpretation of data point distance matrices (similar data points are grouped near together).\n• Show vector: shows the computed data point distances in vector form (x: data point number, y: distance).\n• Show matrix: shows the computed data point distances in matrix form (x,y: data point numbers, color: distance).\n• Show neighbors: plots the k nearest neighbors computed by View - Data point distances - Compute (searching for neighbors of the first element of the manual selection) in the MATLAB workspace. The value of k is chosen by Control element: Data mining: Special methods - k."
    }, {
      "heading" : "6.4. Menu items ’Data mining’ 61",
      "text" : ""
    }, {
      "heading" : "6.3.19 Project report",
      "text" : "shows the information about the recent project (name, number of data points etc.) and all selected parameters in the control elements."
    }, {
      "heading" : "6.3.20 Recoding table",
      "text" : "shows modified class numbers if an output variable was not original numbered with 1 ... n in the code variable."
    }, {
      "heading" : "6.4 Menu items ’Data mining’",
      "text" : "contains all functions for the supervised and unsupervised classification and for the regression of single features and time series."
    }, {
      "heading" : "6.4.1 Selection and evaluation of single features",
      "text" : "contains all functions for the evaluation and selection of single features using data mining methods.\n• ANOVA, univariate: computes feature relevances with the ANOVA method. In contrast to the MANOVA method redundancies between features are not investigated. The method assumes a normal distribution for each output class (linguistic term) in the input space.\n• MANOVA, multivariate: computes feature relevances with the MANOVA method. It performs a step-wise selection by adding one best single feature to an existing set of selected features. The selection terminates if the desired number of features in Control element: Data mining: Classification of single features - Number of selected features is reached or if the next feature does not increase the feature relevance of the set of features. The method takes redundancies between features into account.\n• Information theoretic measures: similar to Data mining - Selection and evaluation of single features - ANOVA, univariate, but the method does not assume a normal distribution.\nAll features are fuzzified and the mutual information per output entropy for the selected output variable is computed (evaluation measure of the ID3 method). However, a split by existing membership functions is used instead of a new computed binary split in each node [76].\n• Classification accuracy: similar to Data mining - Selection and evaluation of single features - MANOVA, multivariate, but computes a relative classification error as performance measure (see e.g. Eq. (3.2) in [87]). In each selection step, the design Bayes classifier bases on the pre-selected features for former steps and a new candidate feature.\n• Classification accuracy (univariate): like Data mining - Selection and evaluation of single features - Classification accuracy, but only for the selection of one single feature."
    }, {
      "heading" : "62 Chapter 6. Menu items",
      "text" : "• Fuzzy classification accuracy: like Data mining - Selection and evaluation of single features - Classification accuracy, but with the mean membership value resp. probability for the true classification decision as evaluation measure (see e.g. Eq. (3.4) in [87]).\nExample:\nData point 1: true class 1, gradual decision with membership degree 0.9 for class 1\nData point 2: true class 2, gradual decision with membership degree 0.6 for class 2\nEvaluation = (0.9+0.6)/2 = 0.75 (instead of 1.0 using Data mining - Selection and evaluation of single features - Classification accuracy)\n• Fuzzy classification accuracy (univariate): like Data mining - Selection and evaluation of single features - Fuzzy classification accuracy, but only for the selection of one feature.\n• Weighted fuzzy classification accuracy: like Data mining - Selection and evaluation of single features - Classification accuracy, but with the weighted mean membership value resp. probability for the true classification decision as evaluation measure (see e.g. Eq. (3.6) and (3.7) in [87]). The weighting leads to a higher preference of true classification results in comparison to Data mining - Selection and evaluation of single features - Fuzzy classification accuracy.\n• Weighted fuzzy classification accuracy (univariate): like Data mining - Selection and evaluation of single features - Weighted fuzzy classification accuracy, but only for the selection of one feature.\n• Compute T-test (with Statistics Toolbox!): tests the selected single features for statistical relevant difference between all pairs of linguistic terms of the selected output variable and shows the results. Only selected data points are used. A method for the correction of multiple tests can be defined by Control element: Data mining: Statistical options - Bonferroni correction. The Statistical Toolbox for Matlab is required for this function.\n• Paired T-Test (with Statistics Toolbox!): similar to Data mining - Selection and evaluation of single features - Compute T-test (with Statistics Toolbox!), but performs a paired test between two classes expressed by linguistic terms of an output variable. Therefor, for each data point belonging to a linguistic term a unique relationship to a particular data point of the other linguistic term is assumed. This relation is defined by identical values for all other output variables (e.g. patient ID,...). A method for the correction of multiple tests can be defined by Control element: Data mining: Statistical options - Bonferroni correction.\nTypical examples are pre-therapeutic and post-therapeutic data of a collective of patients. For the test, exactly one pre-therapeutic and one post-therapeutic data point must exist for each included patient. In general, this pairwise relation is not given for all data points of a project (e.g. containing healthy subjects without post-therapeutic data points). For a valid test, it can be generated by an appropriate selection of all patient data points which have to be included.\n• Compute Wilcoxon ranksum test (with Statistics Toolbox!): similar to Data mining - Selection and evaluation of single features - Compute T-test (with Statistics Toolbox!), but performs a non-parametric Wilcoxon-Ranksum-Test. The test results are also valid for data with other distributions beyond normal distributions. A method for the correction of"
    }, {
      "heading" : "6.4. Menu items ’Data mining’ 63",
      "text" : "multiple tests can be defined by Control element: Data mining: Statistical options - Bonferroni correction.\n• Test of normal distribution: tests the selected single features to normal distribution and protocols the result, for all selected data points and for each single class of the selected output variable. Here, the test defined by Control element: Data mining: Statistical options - Test of normal distribution is applied. The Statistical Toolbox for Matlab is required for this function.\n• Chi-Square test output variable vs. discrete-valued single feature: like Data mining - Evaluation of output variables - Chi-Square test for contingency tables of output variables, but for the selected output variable and the selected single features. The latter one are assumed to have discrete values.\n• Linear regression coefficients (univariate): absolute values for univariate linear regression coefficients (square root of the coefficient of determinationR2). They deal with the prognosis of the single feature defined by Control element: Data mining: Regression - Output variable of regression. The input variables are all single features except the output variable and all previous estimations.\nUnivariate relevances ignore correlations between different input variables.\nA better combination might be found by Data mining - Selection and evaluation of single features - Linear regression coefficients (multivariate). If Control element: Data mining: Regression - Preselection of features is marked, all selected input variables are preselected as first features.\nThe computed relevances can be shown e.g. by View - Single features - Show feature relevances (sorted table).\n• Linear regression coefficients (multivariate): like Data mining - Selection and evaluation of single features - Linear regression coefficients (univariate), but with multivariate instead of univariate regression coefficients.\n• Regression accuracy (univariate): like Data mining - Selection and evaluation of single features - Linear regression coefficients (univariate), but with complete regression models defined by Control elements: Data mining: Regression instead of univariate regression coefficients. The evaluation measure is the regression coefficient between the true value and the estimation generated by the regression model.\n• Regression accuracy (multivariate): like Data mining - Selection and evaluation of single features - Linear regression coefficients (univariate), but with regression models with multiple inputs.\n• Regression accuracy via improvement of the mean error (univariate): evaluates the fitness of a regression model via the relative improvement of the mean absolute error in contrast to the trivial model resulting from the mean value of the output variable.\n• Regression accuracy via improvement of the mean error (multivariate): like Data mining - Selection and evaluation of single features - Regression accuracy via improvement of the mean error (univariate), but with regression models with multiple inputs."
    }, {
      "heading" : "64 Chapter 6. Menu items",
      "text" : ""
    }, {
      "heading" : "6.4.2 Evaluation of time series",
      "text" : "contains all functions for an automatic evaluation of time series.\n• ANOVA (with sample point for Time series→ Single feature): computes for all time series a relevance using a univariate variance analysis ANOVA and selects the best time series. The analysis is only performed for the sampling point chosen by Control element: Time series: Extraction - Parameters - Sample point for Time series→Single feature. The number of selected time series is defined by Control element: Data mining: Classification of single features - Number of selected features.\n• MANOVA (with sample point for Time series→ Single feature): see Data mining - Evaluation of time series - ANOVA (with sample point for Time series→ Single feature) but using a multivariate analysis of variances (MANOVA).\n• Compute best sample points: contains all functions to search for the best sample points of a time series for a subsequent feature selection.\n• Feature maps (all time series, selected data points): computes feature relevances of all time series and shows the results as scatter plot (x axis: Time, y axis: number of time series, color: feature relevance) [19]. The resulting so-called feature map visualizes the contained information to predict the selected output variable from the selected data points using different methods.\n• Linear regression coefficients (univariate): computes absolute values for univariate linear regression coefficients (square root of the coefficient of determination R2). They deal with the prognosis of the time series defined by Control element: Data mining: Regression - Output variable of regression. The input variables are past ([k − i]) and/or present [k] sample points of all or selected time series defined by Control element: Data mining: Regression - Feature selection resp. Control element: Data mining: Regression - Sample points. Univariate relevances ignore correlations between different input variables.\nA better combination might be found by Data mining - Evaluation of time series - Linear regression coefficients (multivariate). The value of Control element: Data mining: Regression - Preselection of features is ignored, no preselection is done.\nThe computed relevances can be shown e.g. by View - Time series (TS) - Show all feature relevances (sorted table).\nOnly one data point can be selected.\n• Linear regression coefficients (multivariate): like Data mining - Evaluation of time series - Linear regression coefficients (univariate), but with multivariate instead of univariate regression coefficients."
    }, {
      "heading" : "6.4.3 Evaluation of output variables",
      "text" : "contains functions for the evaluation of output variables."
    }, {
      "heading" : "6.4. Menu items ’Data mining’ 65",
      "text" : "• Chi-Square test for contingency tables of output variables: performs a Pearson Chi-Square-Test (using the function ”crosstab” of the MATLAB Statistics Toolbox) that the output variables are pairwise independent (null hypothesis). The p-value indicates that this null hypothesis is rejected by chance. The results are valid for sample size >5 data points in each cell of the cross tabulation. Otherwise for cross tabulations with only two values 0 and 1, the (more conservative) Two-tail Exact Fisher Test is used in the implementation of [108] (function ”fisherextest”). A method for the correction of multiple tests can be defined by Control element: Data mining: Statistical options - Bonferroni correction."
    }, {
      "heading" : "6.4.4 Classification",
      "text" : "contains all functions for a complete classification run based on single features (incl. feature selection and aggregation).\n• Design: trains a classifier based on the selected data points. The output variable for the supervised classification is defined by Control element: Time series: General options - Selection of output variable (also found in other windows, see e.g. Control elements: Data mining: Classification of single features). This run includes all steps defined by Control elements: Data mining: Classification of single features (optional feature selection, normalization, feature aggregation, normalization of aggregated features, and classification itself). The run is defined by all parameters from Control elements: Data mining: Classification of single features and Control elements: Data mining: Special methods.\n• Apply: applies a classifier design by Data mining - Classification - Design to all selected data points. This application includes all designed steps for feature selection, normalization, feature aggregation, normalization of aggregated features, and the classification itself.\n• Design and apply: performs a subsequent processing of Data mining - Classification - Design and Data mining - Classification - Apply."
    }, {
      "heading" : "6.4.5 Time series classification",
      "text" : "contains all functions to classify time series.\n• Design: designs a time series classifier using the parameters in Control elements: Data mining: Classification of time series. The different classifier types are described in [22]. The detailed parameters for the classifiers are defined by Control elements: Data mining: Special methods.\n• Apply: applies a classifier designed by Data mining - Time series classification - Design to the selected data points.\n• Design and apply: performs a subsequent processing of Data mining - Time series classification - Design and Data mining - Time series classification - Apply.\n66 Chapter 6. Menu items\n• Time aggregation: aggregates classifier decisions over the time using the filter parameters in (Control element: Data mining: Classification of time series - Filtering of results) resulting in modified classifier decisions.\n• Time aggregation with result plot: like Data mining - Time series classification - Time aggregation, but with an additional plot of the results."
    }, {
      "heading" : "6.4.6 Hierarchical Bayes classifier",
      "text" : "contains functions for the design and application of hierarchical Bayes classifiers.\n• Design: designs a Hierarchical Bayes classifier by a step-wise separation of single classes from all other classes [87]. The maximum number of selected and aggregated features are defined by (Control element: Data mining: Classification of single features - Number of selected features) resp. Control element: Data mining: Classification of single features - Number of aggregated features. The feature aggregation is done by a Discriminant Analysis with optimization. This function always uses a Bayes classifier.\n• Apply: applies a classifier designed by Data mining - Hierarchical Bayes classifier - Design to all selected data points.\n• Design and apply: performs a subsequent processing of Data mining - Hierarchical Bayes classifier - Design and Data mining - Hierarchical Bayes classifier - Apply."
    }, {
      "heading" : "6.4.7 Fuzzy systems",
      "text" : "contain all functions for the design and application of fuzzy systems. The design algorithms are described in [76].\n• Design (single rules): searches for relevant fuzzy rules using only the selected single features and the selected output variable. This design does not consider the cooperation of the designed rules in a rule base (e.g. to avoid redundancies). Preexisting rules will be deleted.\n• Design (rule base): searches for a relevant fuzzy rule base using only the selected single features and the selected output variable. This design searches in first step for single rules and collects a small set of cooperating rules by an optimization. The evaluation measure prefers rules without redundancies and a complete coverage of the input space. Preexisting rules will be deleted.\n• Delete rules: open a window to delete fuzzy rules manually."
    }, {
      "heading" : "6.4. Menu items ’Data mining’ 67",
      "text" : "• Design membership functions for single features: designs membership functions for all single features using the method specified by Control element: Data mining: Special methods - Type of membership function.\n• Export fuzzy system to classifier: exports a designed fuzzy system as classifier. A design by Data mining - Fuzzy systems - Design (rule base) corresponds to a classifier design using the selected features without an aggregation. A design by Data mining - Fuzzy systems - Design (single rules) is also possible, but it leads in many cases to bad results due to redundant rules and missing priorities of good single rules.\n• Import fuzzy system from classifier: imports a fuzzy system from a designed fuzzy classifier. This function is only available if ”fuzzy classifier” was chosen in Control element: Data mining: Classification of time series - Classifier type during last design. It is useful to list and to visualize fuzzy rules from classifiers (see also View - Fuzzy systems).\n• Import fuzzy system from regression model: imports a fuzzy system from a regression model designed by Data mining - Regression - Design resp. Data mining - Regression - Design and apply. Before designing, ”Fuzzy system” has to be selected in Control element: Data mining: Regression - Type."
    }, {
      "heading" : "6.4.8 Clustering",
      "text" : "contains all functions for the design and application of clusters.\n• Design and apply: contains different methods to search for subgroups with fuzzy cluster methods (FCM) for the selected time series and resp. or single features (see e.g. [64]). The design methods use only the selected data points. The application for the cluster assignment is done for all data points. The necessary parameters and methods as well as the options for the conversion of clusters into new output variables are defined in Control elements: Data mining: Clustering. Here, own clustering functions are applied.\n• Design and apply (Statistic Toolbox): computes clusters for the selected single features and resp. or time series. The necessary parameters and methods as well as the options for the conversion of clusters into new output variables are defined in Control elements: Data mining: Clustering. In contrast to Data mining - Clustering - Design and apply, the crisp and hierarchical cluster algorithms of the Statistic Toolbox of Matlab (e.g. ”pdist”, ”linkage” and ”cluster”) are used. A detailed description can be found in the handbook of this toolbox."
    }, {
      "heading" : "6.4.9 Association analysis",
      "text" : "performs an Association analysis for all output variables. Here, an implementation of Narine Manukyan was integrated. The results are saved as file. If this analysis should include single features, these single features have to be converted to output variables by Edit - Convert - Selected single features→ Output variables."
    }, {
      "heading" : "68 Chapter 6. Menu items",
      "text" : "Output variables with too many linguistic terms will be ignored, the maximum of term numbers is defined by Control element: Data mining: Special methods - Ignore output variables with many terms. Only rules with a minimum confidence and support are shown (Definition in Control element: Data mining: Special methods - Minimum confidence resp. Control element: Data mining: Special methods - Minimum support)."
    }, {
      "heading" : "6.4.10 Self Organizing Maps",
      "text" : "contains the functions for the design and application of Self Organizing Maps. Here, the function selforgmap.m from MATLAB Neural Network Toolbox is used.\n• Design: designs Self Organizing Maps for all selected single features with the function selforgmap.m from MATLAB Neural Network Toolbox. The dimensionality is defined by Control element: Data mining: Special methods - Dimension, the number of neurons per dimension by Control element: Data mining: Special methods - Number of neurons and the number of training epochs by Control element: Data mining: Special methods - Number of learning epochs.\n• Apply: applies a Self Organizing Map designed by Data mining - Self Organizing Maps - Design to the selected data points. A new output variable with the number of winner neurons in the terms is generated."
    }, {
      "heading" : "6.4.11 Regression",
      "text" : "contains all functions for the design and application of regression models.\n• Design: designs a regression model using the selected data points and the parameters chosen in Control elements: Data mining: Regression.\nThe related options (regression of time series or single features, feature selection and aggregation, normalization, regression method) are chosen in Control elements: Data mining: Regression. The methods can be selected and parameterized using Control elements: Data mining: Special methods.\n• Apply: applies the recent regression model to the selected data points. The recent model results from the last design step or a loaded regression model.\nHereby, all related operations (regression of time series or single features, feature selection, and aggregation, normalization, regression method) are executed.\n• Design and apply: performs a subsequent processing of Data mining - Regression - Design and Data mining - Regression - Apply."
    }, {
      "heading" : "6.4. Menu items ’Data mining’ 69",
      "text" : ""
    }, {
      "heading" : "6.4.12 Validation",
      "text" : "validates the classification of single features and time series.\n• Classification of single features: validates classifiers. Possible validation strategies are a cross-validation (with diverse subtypes) and a bootstrap method (selection by Control element: Data mining: Validation - Validation strategy). The menu item Data mining - Classification - Design is performed for each produced training data set using the defined options. The resulting classifier is applied for each validation data set by means of Data mining - Classification - Apply. The results are aggregated and written into protocol files.\n• Classification of single features (selected macros): similar to Data mining - Validation - Classification of single features but with macros for classifier design and application instead of the corresponding menu items. The macros are defined by a configuration window or by the values of the variables ”makro_lern” (design) resp. ”makro_test” (application) for a script-based technique.\n• Hierarchical Bayes Classifier for single features: similar to Data mining - Validation - Classification of single features but for Hierarchical Bayes classifiers designed by Data mining - Hierarchical Bayes classifier - Design.\n• Time series classification: see Data mining - Validation - Time series classification (selected macros). Here, the design and application of the time series classifiers are executed. A selection of user-defined macros is not possible.\n• Time series classification (selected macros): validates time series classifiers. Two macros are executed for the classifier design and its application. Both macros are selected by a configuration window. Different types of cross-validation and bootstrap are available as validation strategies. The parameters are tuned by Control elements: Data mining: Validation. The results are separately computed for each trial. In addition, the mean value and the standard deviation for all trials are shown.\nFor the cross-validation of time series, a variable zr_fehl_proz is introduced in the struct relevanz_cv_alle. It contains the classification error vs. time. The mean value and the standard deviation is defined by the minimum error of each trial.\n• Regression: see Data mining - Validation - Regression (own macros). Here, the design and application of the regression models are executed using the standard macros ”regr_em_en.makrog” (design) and ”regr_em_an.makrog” (application). A selection of user-defined macros is not possible.\n• Regression (own macros): validates regression models. Two macros are executed for the regression model design and its application. Each macro is selected by a configuration window. Different types of cross-validation and bootstrap are available as validation strategies. The parameters are tuned by Control elements: Data mining: Validation. The results (regression error and (Pearson) correlation coefficient) are separately computed for each trial. In addition, the mean value and the standard deviation for all trials are shown."
    }, {
      "heading" : "70 Chapter 6. Menu items",
      "text" : ""
    }, {
      "heading" : "6.5 Menu items ’Extras’",
      "text" : "contains all functions to work with macros and for the administration of application-specific extension packages."
    }, {
      "heading" : "6.5.1 Play macro...",
      "text" : "loads and runs a recorded macro. If an error message about a missing macro in the search path occurs, possible reasons are deleted macros or errors during the last macro run. In latter case, the use of Extras - Reset macro names might help."
    }, {
      "heading" : "6.5.2 Play macro (debug mode)...",
      "text" : "executes a macro in debug mode. The selected macro is copied into a m-file called ”makro_m_file.m” and it is started. With this option, the complete debugging functionality MATLAB is available. After a new start, SciXMiner checks if ”makro_m_file.m” was changed manually. If yes, it asks if these changes should be transferred also to the macro."
    }, {
      "heading" : "6.5.3 Record macro...",
      "text" : "records a sequence of clicked menu items and control elements as macro file *.makrog. A manual modification of this file is possible due to its textual Matlab syntax. The macro name is defined by an additional window. The background of the SciXMiner window is yellow during recording. All menu items are executed in parallel. The recording is stopped by Extras - Stop macro record. Some functions (especially file operations) are not available during macro recording. They can be added manually by using the related callbacks from the file menu_elements.m. A critical evaluation of the recorded macros is strongly recommended especially for complex tasks.\nA modified assignment of plotted figures (e.g. to plot multiple figures into subplots instead of separate windows) is possible by Control element: General options - For macros: plot always in current figure. Such commands (e.g. subplot) should be placed directly before the menu item in the macro to avoid a plot into wrong figures (especially in the SciXMiner main window)."
    }, {
      "heading" : "6.5.4 Stop macro record",
      "text" : "terminates the record of a macro."
    }, {
      "heading" : "6.5.5 Edit macro...",
      "text" : "opens an existing macro in an editor window for a manual modification."
    }, {
      "heading" : "6.5.6 Reset macro names",
      "text" : "resets the names of the recent macro in the Matlab workspace. This function is important after an occurring error during the macro execution. Otherwise, problems during the execution of different macros are possible."
    }, {
      "heading" : "6.5. Menu items ’Extras’ 71",
      "text" : ""
    }, {
      "heading" : "6.5.7 Execute M-file...",
      "text" : "executes a M file."
    }, {
      "heading" : "6.5.8 Edit M-file...",
      "text" : "opens an existing M-file in an editor window for a manual modification."
    }, {
      "heading" : "6.5.9 Edit SciXMiner batch file ...",
      "text" : "opens an existing SciXMiner batch file in an editor window for a manual modification."
    }, {
      "heading" : "6.5.10 Generate PDF project report (needs Latex)",
      "text" : "generates PDF reports using Latex for the given project or all projects within a directory. Latex must be installed and the functions texify and dvipdfm must be found in the Windows search path.\nThe function generates a subdirectory ”report” in the recent working directory. Here, all new or modified files for the project report are saved. The title page is generated from the Latex file ”reporttemplate.tex”, located in the directory ”standardmakros” of SciXMiner. It can be modified for individual needs. Only for the first report, the template content is copied to a new file ”project_results_*.*” in the ”report” directory. After this, it can be also modified to individual needs only for this project or directory.\nThe following contents are added if they exist:\nOnce per PDF:\n1. File ”general_comments.tex”. Here, all project related metadata can be added (e.g. experiment planning).\n2. Error protocol ”error.log”.\n3. All jpeg images located in a subdirectory ”images”.\nOnce per project:\n1. All jpeg image generated by SciXMiner for a project.\n2. All Latex files generated by SciXMiner for a project. Here, only the parts between the begin and the end of the document are used. Optional sections SciXMiner parameters are removed starting from the first subsection{Parameter}.\nAll modified files (and only these, not the images and unmodified Latex files) are written in the subdirectory ”report”. They should not be modified manually to an overwriting by the next generation of a project report.\n• for the current project: generated the PDF project report only for the recent project.\n• for all projects in the directory: generated the PDF project report for all projects in the recent directory. The different projects are included as sections. The name of the report is set to the name of the directory."
    }, {
      "heading" : "72 Chapter 6. Menu items",
      "text" : ""
    }, {
      "heading" : "6.5.11 Translate German Gait-CAD m-files and macros into English",
      "text" : "this menu item supports the translation of German Gait-CAD m-files and macros into English. It converts strings in all macros and m-files of a chosen directory using a dictionary. The related directory and a further directory for a safety copy are selected by a separate window."
    }, {
      "heading" : "6.5.12 Choose application-specific extension packages...",
      "text" : "switch packages with application-specific extensions on or off. This selection will be done after the next SciXMiner start. At the moment, the extension packages ”Peptides”, ”Images and Videos” and ”Tracking” are available. The beta versions of the extension packages ”Gait analysis” and ”Text mining” are available on request."
    }, {
      "heading" : "6.5.13 Search path for m-files and plugins",
      "text" : "adds a directory to the MATLAB search path to use m-files and plugins in this directory in SciXMiner.\n• Permanent: adds a directory permanently to the MATLAB search path to use m-files and plugins in this directory in SciXMiner. This information is saved in read_gaitcad_searchpath.m.\n• Temporary for the session: adds a directory for the recent SciXMiner session to the MATLAB search path to use m-files and plugins in this directory in SciXMiner.\n• Reset (permanent search path): resets the permanent search path, that was added by Extras - Search path for m-files and plugins - Permanent and saved in read_gaitcad_searchpath.m.\n• Reset (only temporarily for the session): resets the search path for the SciXMiner session, that was defined by Extras - Search path for m-files and plugins - Temporary for the session."
    }, {
      "heading" : "6.5.14 Matlab Parallel",
      "text" : "controls the use of configurations of the MATLAB Parallel Computing Toolbox.\n• Start: starts the use of the selected configuration of the MATLAB Parallel Computing Toolbox. Here, the configuration with the name defined by Control element: General options - Configuration name for MATLAB parallel is used (see also Parallel - Manage Configurations in the menu of the MATLAB command window).\n• Stop: stops the use of the recent configuration of the MATLAB Parallel Computing Toolbox."
    }, {
      "heading" : "6.6. Menu items ’Favorites’ 73",
      "text" : ""
    }, {
      "heading" : "6.6 Menu items ’Favorites’",
      "text" : "contains all functions for a fast access to frequently used or user-defined menu points and macros. All deactivated entries (e.g. due to missing single features) are shown with a light gray color."
    }, {
      "heading" : "6.6.1 Edit user-defined favorites",
      "text" : "opens a configuration window to add and delete menu points from the favorite list.\n6.6.2 Delete all favorites\ndeletes all favorites."
    }, {
      "heading" : "6.7 Menu items ’Window’",
      "text" : "activates or closes all open Matlab figures except the SciXMiner main window."
    }, {
      "heading" : "6.7.1 Close figures",
      "text" : "closes all figure except the SciXMiner main window."
    }, {
      "heading" : "6.7.2 Arrange figures",
      "text" : "contains different options for the placement of figures.\n• Horizontal: places all figures horizontally.\n• Vertical: places all figures vertically.\n• Cascade: places all figures as a cascade.\n• Position of the current figure: places all figures to the position of the last opened figure. This command is especially useful for the comparison of figures with small differences by switching between figures with the same size and position."
    }, {
      "heading" : "74 Chapter 6. Menu items",
      "text" : ""
    }, {
      "heading" : "6.7.3 Logarithmic scaling of current figures",
      "text" : "applies a logarithmic scale to all open MATLAB figures.\n• only x axis: applies a logarithmic scale to the x-axis of all open MATLAB figures.\n• only y-axis: applies a logarithmic scale to the y-axis of all open MATLAB figures.\n• x- and y axis: applies a logarithmic scale to the x- and the y-axis of all open MATLAB figures."
    }, {
      "heading" : "6.7.4 Remove Latex codes in MATLAB figures",
      "text" : "removes Latex-style variables interpreted as equation (e.g. x1 instead of x_1) from all open MATLAB figures."
    }, {
      "heading" : "6.7.5 Update font and font size in figures",
      "text" : "sets font type and size in all open MATLAB figures to the values defined by Control element: View: Single features - Font and Control element: View: Single features - Font size."
    }, {
      "heading" : "6.7.6 Plot all figures as images in files",
      "text" : "plots the content of all figures in files. The filename is generated from the project and the figure name. The file type is defined by Control element: General options - File type for images. The image in the file depends on the size on the monitor."
    }, {
      "heading" : "6.8 Menu items ’Help’",
      "text" : "contains SciXMiner version and license information as well as SciXMiner handbooks."
    }, {
      "heading" : "6.8.1 Show SciXMiner documentation (PDF)",
      "text" : "opens the SciXMiner handbook and help file as PDF."
    }, {
      "heading" : "6.8.2 About SciXMiner",
      "text" : "contains the version information and the contact address to the developer team."
    }, {
      "heading" : "6.8.3 License information",
      "text" : "shows the license file for the GNU licence."
    }, {
      "heading" : "7 Control elements",
      "text" : ""
    }, {
      "heading" : "7.1 Control elements for ’Project overview’",
      "text" : "75"
    }, {
      "heading" : "76 Chapter 7. Control elements",
      "text" : ""
    }, {
      "heading" : "7.2 Control elements for ’Time series: General options’",
      "text" : "• Selection of output variable: selects an output variable. This selection influences many functions as the evaluation of single features and time series, the design of classifiers, almost all visualization functions etc.\n• Sampling frequency of time series: defines the sampling frequency for the time series in the data set. It is assumed that this value is equal for all time series. The parameter mainly influences the visualization of time series using time or frequency.\n• Unit: defines the physical quantity for time series (e.g. sampling frequency in kHz, per day etc.). The parameter mainly influences the visualization of time series or spectrograms using time or frequency.\n• Selection of time series (TS): selects time series for visualization and all following processing steps. The selection can be done by mouse clicks in the listbox, by writing the numbers in the edit field on the left hand side or by the ’ALL’ button to select all time series. The different fields will be synchronized after the input.\nTIP: An input in the edit field enables a selection with a different order (e.g. 2-4-1 for a visualization) in contrast to the listbox."
    }, {
      "heading" : "7.2. Control elements for ’Time series: General options’ 77",
      "text" : "• Complete time series: selects all sample points of the time series.\n• Time series segment from: defines the begin of a time series segment for the visualization of sample points. This parameter might also defined by a zooming in a time series visualization (View - Time series (TS)) and pushing the button ”Select time series segment” in the menu bar. The begin is the left limit of the current x-axis.\nThe button ”Complete time series” set the time series segment to all sample points.\n• to: defines the end of a time series segment for the visualization of sample points. This parameter might also defined by a zooming in a time series visualization (View - Time series (TS)) and pushing the button ”Select time series segment” in the menu bar. The end is the right limit of the current x-axis.\nThe button ”Complete time series” set the time series segment to all sample points.\n• Forbid segments of length 1: forbid time series segments of length 1. This option is almost always useful except some special cases for feature extraction."
    }, {
      "heading" : "78 Chapter 7. Control elements",
      "text" : ""
    }, {
      "heading" : "7.3 Control elements for ’Time series: Extraction - Parameters’",
      "text" : "• Data-dependent feature extraction in plugins: defines the handling of plugins with data-dependent transformation functions (and not only datadependent output variables for time series and single features!). Typical examples are a transformation matrix for a Principal Component Analysis or fuzzy membership functions. Possible options are a computation with and without saving the results into a *.plugpar file with the same project name or a loading from these file. By loading, the name of the time series, the plugin description, and the segments must be the same. The handling has to be separately implemented for each plugin!\n• Sample point for Time series→Single feature: select the sample point for all functions using only one sample point for a conversion from time series to single features.\n• Parameter IIR filter: defines the filter parameter a for an Infinite-Impulse-Response (IIR) filter. The filter is a first-order lowpass with xf [k + 1] = a ∗ xf [k] + (1 − a) ∗ x[k]. The parameter defines the influence of old values. A higher value leads to a slower behavior due to a higher weighting of past values. Some functions and plugins for the feature extraction use this parameter. For stability reasons, the value is limited to [0, 1]."
    }, {
      "heading" : "7.3. Control elements for ’Time series: Extraction - Parameters’ 79",
      "text" : "• IIR parameter (aF, aS, aSigma): defines the parameters for three different IIR filters for trend and standard deviation computation (see [90]).\n• Filter order (FIL): defines the order of the filter for feature extraction.\n• High-: switches to a high-pass filter for feature extraction.\n• Low-: switches to a low-pass filter for feature extraction.\n• Bandpass: switches to a band-pass filter for feature extraction.\n• Frequencies (FIL, Morlet spectrogram): defines the cutoff frequencies for different filters. The parameter is used for a Butterworth filter in feature extraction and for Morlet spectrograms. A high-pass filter and a low-pass filter use only the first value, whereas the band-pass filter uses both values.\n• Wavelet: selects the wavelet type for a wavelet decomposition (see e.g. [63] or the documentation of the Matlab Wavelet Toolbox).\n• Matlab wavelet decomposition: uses the Matlab implementation instead of the implementation of [63] for the wavelet decomposition. It requires the Matlab Wavelet Toolbox, but it is faster especially in case of many data points and long time series.\n• Wavelets: number of levels: defines the number of levels for a wavelet decomposition. It computes for the half of the sampling frequency fA an high-pass filter and a low-pass filter. The high-pass in level i has a cutoff frequency of (i = 1, . . .) with\nfA 2i+1 .\nThe cut-off frequency for the low-pass filter is computed accordingly.\n• Morlet wavelet: frequency: defines the frequency for the generation of new time series by means of complex Morlet wavelets. This frequency describes the center of the frequency range which will not be damped by the wavelet. The width of this range is defined by the eigenfrequency of the Morlet wavelet (Control element: Time series: Extraction - Parameters - Morlet wavelet: eigenfrequency). For the computation of Morlet spectrograms, the parameter from Control element: Time series: Extraction - Parameters - Frequencies (FIL, Morlet spectrogram) is used instead of this parameter.\n• Causal Morlet wavelet: temporal shift the complex Morlet wavelet for a causal filtering. In addition, the wavelet will be limited in time.\n• Morlet wavelet: eigenfrequency: see Control element: Time series: Extraction - Parameters - Morlet wavelet: frequency."
    }, {
      "heading" : "80 Chapter 7. Control elements",
      "text" : "• Shortening of time series: Window length or x-th sample point: defines the window lengths (Edit - Convert - Shorten time series - Shortening by window methods) resp. the step width (Edit - Convert - Shorten time series - Use only each x-th sample point) to short time series.\n• Shortening of time series: method: selects the method for shortening all time series in a project using Control element: Time series: Extraction - Parameters - Shortening of time series: Window length or x-th sample point. First, a window of with a length defined by Edit - Convert - Shorten time series - Shortening by window methods is chosen. Second, the new value for the short time series is computed by the mean, median, minimum, or maximum value of these windows. The windows do not overlap.\n• Match time series length to: defines the length for the shortening of the time series via Edit - Convert - Shorten time series - Match time series lengths (use 0 and NaN as undefined values). The new length of the time series is fixed (percent: 100, per thousand 1000) or the shortest time series in the data set is used. The length of the shortest time series is defined by the sample after which the time series is only zero, Inf or NaN."
    }, {
      "heading" : "7.4. Control elements for ’Plugin sequence’ 81",
      "text" : ""
    }, {
      "heading" : "7.4 Control elements for ’Plugin sequence’",
      "text" : "• Show plugins: select the shown plugins depending on type (e.g. time series→ time series).\n• Add: adds the selected plugins from Control element: Plugin sequence - Selection of plugins to a plugin sequence.\n• Update plugins: reads the available plugins from all plugin files. It is useful for the development of plugins to avoid SciXMiner restarts.\n• Show plugin list: writes the characteristics and descriptions of all available plugins in a file and displays this file.\n• Selection of plugins: selects the plugins used by Edit - Extract - Time series → Time series, Time series → Single features (via plugin sequence).... Only a definition in the edit window guarantees the selected order of the plugins. A definition in the list box ignores this order and sorts the plugins by their numbers.\n• Plugin parameter: set the parameters of the selected plugin from Control element: Plugin sequence - Selection of"
    }, {
      "heading" : "82 Chapter 7. Control elements",
      "text" : "plugins resp. Control element: Plugin sequence - Selected plugin sequence. The switch between multiple parameters is done by Control element: Plugin sequence - No..\n• No.: selects a specific parameter for plugins with multiple parameters.\n• Forward: moves the selected plugins in Control element: Plugin sequence - Selected plugin sequence to earlier positions in the sequence.\n• XPIWIT: Execute pipeline stepwise: execute plugins in XPIWIT separately. In this option, for each plugin a separate call of XPIWIT.exe is executed. This option is only used by the extension package ”Images and Videos”.\n• Down: moves the selected plugins in Control element: Plugin sequence - Selected plugin sequence to later positions in the sequence.\n• Delete: deletes the recent plugin from the plugin sequence.\n• Delete all: deletes all plugins from a plugin sequence.\n• Load: loads a plugin sequence from a plugin sequence file (*.plugseq). The matching of the plugins is done by the function names.\n• Selected plugin sequence: shows the recent plugin sequence. The plugin sequence is performed step by step using Control element: Plugin sequence - Execute resp. Edit - Extract - Time series→ Time series, Time series → Single features (via plugin sequence)... (plugins for time series) or Images and Videos - Apply plugin sequence to the selected images (plugins for images). Existing parameters of the plugins are set by Control element: Plugin sequence - Plugin parameter. The order of plugins can be changed by Control element: Plugin sequence - Down etc.\nWARNING! If the plugin sequence is recorded by a macro, each plugin must be added separately and its parameters have to be chosen before the next plugin can be added. Otherwise, the macros recording might cause problems if the plugin sequence contains more than one plugin with the same name. In this case, the parameter of the last plugin with the same name are adapted.\n• Save: saves a plugin sequence with the defined plugin parameters in a plugin sequence file (*.plugseq).\n• Show results: defines the visualization and save options for plugin sequences of images. Intermediate results are the result of all plugins except the last one. ”Show” means a visualization as image in a separate Matlab figure, ”Save” the generation of a new image file and the generation of a new image type.\nPlugins in projects without images ignores this option and set it to ”Save final result”.\n• Show parameters: shows all used parameters in a plugin sequence."
    }, {
      "heading" : "7.4. Control elements for ’Plugin sequence’ 83",
      "text" : "• Save performance log file: defines if computing times for single plugins or complete plugin sequences should be written in a log file ”*_PerformanceLog.csv” in the recent project directory.\n• Execute: executes the recent plugin sequence in Control element: Plugin sequence - Selected plugin sequence.\n• Use the same transformation matrix (e.g. PCA) for time series reduction of all data points: influences the reduction of time series by the Principal Component Analysis. If the option is marked, an identical transformation matrix is used for all data points. Otherwise, a separate transformation matrix is computed for each data point."
    }, {
      "heading" : "84 Chapter 7. Control elements",
      "text" : ""
    }, {
      "heading" : "7.5 Control elements for ’Single features’",
      "text" : "• Selection of output variable: selects an output variable. This selection influences many functions as the evaluation of single features and time series, the design of classifiers, almost all visualization functions etc.\n• Selection of single feature(s) (SF): selects single features for visualization and all following processing steps. The selection can be done by mouse clicks in the listbox, by writing the numbers in the edit field on the left hand side or by the ’ALL’ button to select all single features. The different fields will be synchronized after the input.\nTIP: An input in the edit field enables a selection with a different order (e.g. 2-4-1 for a visualization) in contrast to the listbox.\n• Number of terms for Single feature→Output variable: defines the number of terms for a new output variable generated by Edit - Convert - Selected single features→ Output variables (see also Control element: Single features - All values). The transformation is done by a maximum defuzzification. The membership functions are computed by a cluster method.\n• All values: uses all different values of a feature as separate linguistic terms (see Edit - Convert - Selected"
    }, {
      "heading" : "7.5. Control elements for ’Single features’ 85",
      "text" : "single features→ Output variables). The value of Control element: Single features - Number of terms for Single feature→Output variable will be ignored. Example:\nA single feature has the values 1, 2.5, 3.5 and 7. The result is an output variable with four terms called ”ca. 1”, ”ca. 2.5”, ”ca. 3.5” and ”ca. 7”. In contrast, new terms with the number defined in Control element: Single features - Number of terms for Single feature→Output variable will be computed if the checkbox is deactivated.\n• A priori feature relevances: switches on the use of a priori relevances of single features. For feature selection, features with higher a priori relevances are preferred. The values are between zero (bad feature) and one (preferable feature).\nThey can be\n- defined in the project file (variable interpret_merk)\n- generated by means of categories, or\n- manually modified (e.g. by Control element: Single features - A priori feature relevances (fix value)).\n• Preference exponent alpha (Interpretability): defines an exponent for feature relevances using a priori relevances. The exponent weights the first value of the a priori relevances mostly associated with an interpretability value. A value of zero ignores the interpretability, large values prefer features with higher interpretability.\n• A priori feature relevances (fix value): sets a defined value for a priori relevances for a manual setting using Edit - Category - A priori relevances of selected features... - set to a fix value (from GUI) for all selected single features.\n• Preference exponent beta (Implementability): defines an exponent for feature relevances using a priori relevances. The exponent weights the second value of the a priori relevances mostly associated with the implementability. A value of zero ignores the implementability, large values prefer features with higher implementability."
    }, {
      "heading" : "86 Chapter 7. Control elements",
      "text" : ""
    }, {
      "heading" : "7.6 Control elements for ’Data preprocessing’",
      "text" : "• Save result as output variable: save the result of the outlier detection as a new output variable.\nThe option ”new output variable” adds always a new output variable with the result of the outlier detection. The variable name is ”Outlier” with the name of the used method.\nThe option ”Replace identical output variable” replace the results in an existing output variable computed by the same method. Otherwise, a new output variable is added.\n• Threshold for outliers: defines a threshold parameter if a data point is classified as outlier or not (see Edit - Outlier detection - Apply (selected data points, designed data set)).\nThe meaning of the parameter depends on the method:\nSVM - a value smaller than zero (tolerated distance to the class border)\ndistance-based: distance value\ndensity-based: number of neighbors\nThe level curves in the visualization are helpful for the parameter definition (see Control element: View: Classification and regression - Show outlier detection)."
    }, {
      "heading" : "7.6. Control elements for ’Data preprocessing’ 87",
      "text" : "• Manual selection of outliers: removes the data points with the indices in the edit field from the training data set. This function supports the outlier detection in complicated cases because these data points are not evaluated as good measurements. Depending on the related positions in feature space, it normally increases the probability of an automatic outlier detection of these data points by applying the outlier detection. All selected data points are used if the field is empty or ”[]” is written in the field.\nDuring the application, all data points will be evaluated.\nThe number of data points is shown if Control element: View: Single features - Show data point number is marked.\n• Method: selects the classifier type for the outlier detection.\nThe one-class method optimizes the coefficients ai and b of the function: f(z) = ∑\ni aiK(z,xi)+ b, with an unknown data point z and the data points xi of the training data set. K is a kernel function known from Support Vector Machines. A data point z is handled as outlier if f(z) < Threshold (see [23] for algorithm details). The threshold is defined by Control element: Data preprocessing - Threshold for outliers.\nThe distance-based method uses the Mahalanobis distance to the mean value of the training data set. All data points with a distance larger than Control element: Data preprocessing - Threshold for outliers (positive value) are handled as outliers.\nThe density-based method classifies all data points without a minimal number of neighbors (defined by Control element: Data preprocessing - Threshold for outliers) as outliers. The neighborhood is defined by a maximal ([0,1]-normalized Euclidean) distance. The distance is tuned by Control element: Data preprocessing - Max. distance.\n• Compute class discriminant functions: switches between hard and soft limits for the one-class method. For hard limits, all training data are handled as class members. For soft limits, some training data could be automatically classified as outliers. It makes especially sense if some data points in the training data set look suspicious.\n• Kernel order: set the order of the kernel of the SVM. It is used for the classification with the SVM and by the SVM-based outlier detection.\n• Use cityblock distance: switches on the 1-norm (city block distance) instead of the Euclidean distance for the Gaussian RBF kernel in the one-class method of the outlier detection (see Control element: Data mining: Special methods - Kernel). This results normally in rougher class borders.\n• Penalty term Lambda (for soft limits): tunes the number of training data accepted as outliers. A larger value reduces the number of outliers.\nThis parameter is only used if ”soft limits” are selected in Control element: Data preprocessing - Compute class discriminant functions.\n• Minimal number of data points in one class: minimal number of data points for the selection with Edit - Select - Data points via most frequent terms."
    }, {
      "heading" : "88 Chapter 7. Control elements",
      "text" : "• Threshold for deleting [Perc. missing data]: defines the percental threshold in data preprocessing for the deletion of time series or single features in case of missing values. Otherwise, each data point with at least one missing value in any single feature or data point will be deleted. A time series contains missing values if at least one sample point is missing. Missing values must be coded with NaN values or Inf values. Time series with zero values for all sample points are also considered as missing values.\nExample:\nProject with 100 data points and 15 % missing values (data point 1-15) for time series x1 and 5 % missing values for time series x2 (data point 96-100) and threshold of 10 %: Time series x1 will be completely deleted, data points 96-100 will be deleted.\n• Feature values for selection: value range for the selection of data points based on single feature values in Control element: Data preprocessing - Feature values for selection.\n• Date format: defines the date format for the conversion of dates and hours in an output variable into a MATLAB timestamp format using Edit - Convert - Selected output variable (Date) → Single feature (Timestamp)."
    }, {
      "heading" : "7.6. Control elements for ’Data preprocessing’ 89",
      "text" : "• Save result as output variable: save the result of the outlier detection as a new output variable.\nThe option ”new output variable” adds always a new output variable with the result of the outlier detection. The variable name is ”Outlier” with the name of the used method.\nThe option ”Replace identical output variable” replace the results in an existing output variable computed by the same method. Otherwise, a new output variable is added.\n• Threshold for outliers: defines a threshold parameter if a data point is classified as outlier or not (see Edit - Outlier detection - Apply (selected data points, designed data set)).\nThe meaning of the parameter depends on the method:\nSVM - a value smaller than zero (tolerated distance to the class border)\ndistance-based: distance value\ndensity-based: number of neighbors\nThe level curves in the visualization are helpful for the parameter definition (see Control element: View: Classification and regression - Show outlier detection).\n• Manual selection of outliers: removes the data points with the indices in the edit field from the training data set. This function supports the outlier detection in complicated cases because these data points are not evaluated as"
    }, {
      "heading" : "90 Chapter 7. Control elements",
      "text" : "good measurements. Depending on the related positions in feature space, it normally increases the probability of an automatic outlier detection of these data points by applying the outlier detection. All selected data points are used if the field is empty or ”[]” is written in the field.\nDuring the application, all data points will be evaluated.\nThe number of data points is shown if Control element: View: Single features - Show data point number is marked.\n• Method: selects the classifier type for the outlier detection.\nThe one-class method optimizes the coefficients ai and b of the function: f(z) = ∑\ni aiK(z,xi)+ b, with an unknown data point z and the data points xi of the training data set. K is a kernel function known from Support Vector Machines. A data point z is handled as outlier if f(z) < Threshold (see [23] for algorithm details). The threshold is defined by Control element: Data preprocessing - Threshold for outliers.\nThe distance-based method uses the Mahalanobis distance to the mean value of the training data set. All data points with a distance larger than Control element: Data preprocessing - Threshold for outliers (positive value) are handled as outliers.\nThe density-based method classifies all data points without a minimal number of neighbors (defined by Control element: Data preprocessing - Threshold for outliers) as outliers. The neighborhood is defined by a maximal ([0,1]-normalized Euclidean) distance. The distance is tuned by Control element: Data preprocessing - Max. distance.\n• Max. distance: defines the accepted maximum distance for a neighborhood (see Control element: Data mining: Special methods - Evaluate minimum number of neighbors).\n• Minimal number of data points in one class: minimal number of data points for the selection with Edit - Select - Data points via most frequent terms.\n• Threshold for deleting [Perc. missing data]: defines the percental threshold in data preprocessing for the deletion of time series or single features in case of missing values. Otherwise, each data point with at least one missing value in any single feature or data point will be deleted. A time series contains missing values if at least one sample point is missing. Missing values must be coded with NaN values or Inf values. Time series with zero values for all sample points are also considered as missing values.\nExample:\nProject with 100 data points and 15 % missing values (data point 1-15) for time series x1 and 5 % missing values for time series x2 (data point 96-100) and threshold of 10 %: Time series x1 will be completely deleted, data points 96-100 will be deleted.\n• Feature values for selection: value range for the selection of data points based on single feature values in Control element: Data preprocessing - Feature values for selection.\n• Date format: defines the date format for the conversion of dates and hours in an output variable into a MATLAB timestamp format using Edit - Convert - Selected output variable (Date) → Single feature (Timestamp)."
    }, {
      "heading" : "7.7. Control elements for ’View: Single features’ 91",
      "text" : ""
    }, {
      "heading" : "7.7 Control elements for ’View: Single features’",
      "text" : "• Show figures: switches between color-coded, number-coded (for the number of the linguistic term of the output variable), and (black and white) symbol-coded features and time series.\n• Color style: modifies the visualization of output variables by means of different color and style combinations for time series and single features. It defines the mapping of linguistic terms to colors and styles.\n• User-defined colors: defines the order of user-defined colors for the visualization of linguistic terms of the selected output variable in time series and single feature plots. Here, the color abbreviations of Matlab are used (see help text for the function plot). The option is only active if User-defined (color or symbol) resp. User-defined (color and symbol)) is selected in Control element: View: Single features - Color style and Colored is selected in Control element: View: Single features - Show figures.\n• User-defined symbols: defines the order of user-defined symbols for the visualization of linguistic terms of the selected output variable in time series and single feature plots. Here, the symbol abbreviations of Matlab are used (see help text for the function plot). The option is only active if User-defined (color or symbol) resp. User-defined (color and symbol)) is selected in Control element: View: Single"
    }, {
      "heading" : "92 Chapter 7. Control elements",
      "text" : "features - Color style and Black-and-white symbol is selected in Control element: View: Single features - Show figures.\n• Different output variable: shows the results of classification and regression with a free selectable output variable if ”Different output variable” was chosen in Control element: View: Classification and regression - Display classes for output variables.\n• SF display x-y-turn: rotates a figure by switching the x- and y-axes for a two-dimensional plot.\n• Inverse plot order terms: plots all data points in reverse order of the terms of the output variables.\n• Show data point number: switches the display of data point numbers on or off.\n• Evaluate only selected single features: evaluates only the selected single features.\n• Show legend: plot the legend with the linguistic terms of the output variable.\n• Show term names instead of data point numbers: shows the term names instead of the data point number in figures. The option is only active if Control element: View: Single features - Show data point number is activated.\n• Relative ratio noise term: relative ratio (relative to the axis scale of each single feature in the original plot) for an additional noise (see Control element: View: Single features - Add noise term).\n• Add noise term: adds a uniformly distributed noise with the factor from Control element: View: Single features - Relative ratio noise term to the shown single features. This make sense for the easier visualization of discrete-valued single features.\n• Centers for histogram bins: defines bin centers for histograms manually (e.g. 0:10:100). This bin centers are used if Control element: View: Single features - Automatic is deactivated.\n• Automatic: defines the bin centers for histograms automatically.\n• Shown feature relevances in lists: defines the number of shown features and their relevance values in relevance lists. The value is only used if Control element: View: Single features - All is switched off. The reduction of the numbers allows more compact lists for documentation purposes.\n• All: shows all features and their relevance values in relevance lists.\n• Sorting of lists: defines the order of single features in lists and text files: ascending or descending sorted resp. unsorted."
    }, {
      "heading" : "7.7. Control elements for ’View: Single features’ 93",
      "text" : "• Decimal symbol in German format (comma): defines a German style number format with commas as decimal point.\n• Output variables protocol files absolute values: defines the protocol option of output variables in files generated by View - Single features - Absolute values. Possible values are all, the selected or none output variables.\n• Classes as rows: switches the style of protocol files for mean values etc. as generated by View - Single features - Mean, standard deviation, minimum, maximum. If the option is activated, each class is written in a row and each single feature in a column. Otherwise, each single feature is written in a row and each class in a column.\n• Font: defines the font type, that is set by Window - Update font and font size in figures for all open MATLAB figures.\n• Font size: defines the font size, that is set by Window - Update font and font size in figures for all open MATLAB figures.\n• Marker size: defines the marker size in plots of single features."
    }, {
      "heading" : "94 Chapter 7. Control elements",
      "text" : ""
    }, {
      "heading" : "7.8 Control elements for ’View: Time series’",
      "text" : "• Show figures: switches between color-coded, number-coded (for the number of the linguistic term of the output variable), and (black and white) symbol-coded features and time series.\n• Color style: modifies the visualization of output variables by means of different color and style combinations for time series and single features. It defines the mapping of linguistic terms to colors and styles.\n• User-defined colors: defines the order of user-defined colors for the visualization of linguistic terms of the selected output variable in time series and single feature plots. Here, the color abbreviations of Matlab are used (see help text for the function plot). The option is only active if User-defined (color or symbol) resp. User-defined (color and symbol)) is selected in Control element: View: Single features - Color style and Colored is selected in Control element: View: Single features - Show figures.\n• User-defined symbols: defines the order of user-defined symbols for the visualization of linguistic terms of the selected output variable in time series and single feature plots. Here, the symbol abbreviations of Matlab are used (see help text for the function plot). The option is only active if User-defined (color or symbol) resp. User-defined (color and symbol)) is selected in Control element: View: Single"
    }, {
      "heading" : "7.8. Control elements for ’View: Time series’ 95",
      "text" : "features - Color style and Black-and-white symbol is selected in Control element: View: Single features - Show figures.\n• Percental (ignore sampling frequency): switches to a percental x-axis for the visualization of time series. The time series length is set to 100%. The chosen sampling frequency will be ignored.\n• Sample points: switches to the number of sample points on the x-axis for the visualization of time series. The chosen sampling frequency will be ignored.\n• Time [Unit]: switches to the time on the x-axis for the visualization of time series. The chosen sampling frequency in Control element: Time series: General options - Sampling frequency of time series will be used to transform sample points to the time. The time starts always with zero.\n• Project-specific: uses a project-specific time scale defined by (parameter.)projekt.timescale for the visualization of time series. This struct contains the elements .name for the name of the time scale and .time for the time values of each sample point. This option is useful for non-equidistant time scales or other onedimensional scales (e.g. mass for mass spectroscopy). An existing time series can be converted to a project-specific time scale by Edit - Convert - Selected time series→ Project-specific time scale.\n• Show time series as subplots: plots selected time series to individual axes, if the option is activated.\nOtherwise, all time series are plotted to the same axis. Different time series are displayed with different colors. The color mapping to linguistic terms of output values gets lost.\n• Imageplot Time series (Color coding): shows time series in a color code. The scale is tuned by Control element: View: Spectrogram, FFT, CCF - Limits for color axis and Control element: View: Spectrogram, FFT, CCF - Function for color bar, the colormap by Control element: View: Spectrogram, FFT, CCF - Colormap. The selected data points are sorted by number or by the linguistic terms of the selected output variable depending on the value of Control element: View: Time series - Sorting classes for image plot.\nIf only one time series is visualized, the option is switched off temporarily.\n• Sorting classes for image plot: switches between sorted by number or class-wise by the linguistic terms of the selected output variable. The selection is only relevant if Control element: View: Time series - Imageplot Time series (Color coding) is activated.\n• Show legend: plot the legend with the linguistic terms of the output variable.\n• Show data point number: switches the display of data point numbers on or off.\n• Poincare plot: Connect points: defines if the sample points in a Poincare plot will be connected by lines or not."
    }, {
      "heading" : "96 Chapter 7. Control elements",
      "text" : "• Show normative data: shows normative data as light-gray region in the visualization of time series. Normative data has to be produced by File - Normative data - Mean value to normative data (internally from the project) or by File - Normative data - Load normative data from a file.\n• Normative data in foreground: the visualization of many original time series may overlay the normative data. Activate this option to plot the normative data in the foreground.\n• Logarithmic visualization: select an optional logarithmic scaling of times and resp. or amplitudes of time series for the visualization.\n• Linewidth: modifies the line width for the visualization of time series. It makes sense e.g. for the export of figures in LATEXfigures with values of e.g. 1.5 or 2.\n• Font: defines the font type, that is set by Window - Update font and font size in figures for all open MATLAB figures.\n• Font size: defines the font size, that is set by Window - Update font and font size in figures for all open MATLAB figures."
    }, {
      "heading" : "7.9. Control elements for ’View: Spectrogram, FFT, CCF’ 97",
      "text" : ""
    }, {
      "heading" : "7.9 Control elements for ’View: Spectrogram, FFT, CCF’",
      "text" : "• Window size [sample points]: defines the window size for the computation of spectrograms. It controls the compromise between a temporal and frequency resolution. A small value prefers a better temporal resolution. The value is automatically reduced to the next power of two to preserve the efficiency of the FFT algorithm (e.g. 27 = 128, 210 =1024 etc.). The value is limited by the number of sample points.\n• Normalization for crosscorrelation: defines the mode of normalization for Auto and Cross Correlation Functions:\n- ”biased”, scales to the length of the time series, see MATLAB function xcorr\n- ”unbiased”, scales to (length of time series - lag), see MATLAB function xcorr (less robust results for large lags)\n- ”coeff”, normalizes the result to autocorrelation 1 with lag 0, see MATLAB function xcorr\n- ”coeff_local” - as ”biased”, but with an additional mean correction and variance normalization to one (identical to ”coeff_mean”, hold for compatibility reasons)\n- ”none” - without normalization, see MATLAB function xcorr\nIn addition, options with mean value correction (+”_mean”) and linear trend compensation (+”_detrend”) of the time series exist."
    }, {
      "heading" : "98 Chapter 7. Control elements",
      "text" : "• Copy correlation results to workspace: defines if the auto or cross-correlation functions compute by View - Cross and Auto Correlation Functions - Separately for each data point are saved into a workspace variable kkfs.\n• Time shift Tau [SP]: sets the time shift for the visualization of correlation coefficients in View - Cross and Auto Correlation Functions - Mean values of correlation coefficients (defined time shift) and View - Cross and Auto Correlation Functions - Class mean values of correlation coefficients (with defined time shift). A value of zero uses the same sample point for both time series.\n• Show correlation figures: defines if the figures of correlation functions are shown or not. The disabling is useful if the results are saved into a variable (see Control element: View: Spectrogram, FFT, CCF - Copy correlation results to workspace).\n• Morlet spectrogram: Frequency stepwidth: defines distances for frequencies, e.g. 1:100:10000.\n• Frequencies (FIL, Morlet spectrogram): defines the cutoff frequencies for different filters. The parameter is used for a Butterworth filter in feature extraction and for Morlet spectrograms. A high-pass filter and a low-pass filter use only the first value, whereas the band-pass filter uses both values.\n• Sample points baseline: see Control element: View: Spectrogram, FFT, CCF - Morlet spectrogram (relative to baseline).\n• Morlet spectrogram (relative to baseline): computes the Morlet spectrogram relatively to a baseline if the option is activated (see View - Morlet spectrogram - Compute and show (selected data points and time series)). Instead of absolute value, the temporal changes will be shown. The limits of the baseline are defined by Control element: View: Spectrogram, FFT, CCF - Sample points baseline.\n• Reduce sampling points for Morlet spectrogram: reduces the number of shown sample points, because exported figures with Morlet spectrograms need a lot of memory. If a value k is greater than one, only each k-th sample point will be shown. The loss of information is usually acceptable for small values.\n• Number of principal components for spectrogram: defines the number of principal components for View - Spectrogram - Principal component analysis for spectrograms.\n• Show color bar: shows the mapping between numbers and the color code for some visualizations as e.g. spectrograms.\n• Show phase response: shows the phase response in addition to the amplitudes of a spectrogram.\n• Function for color bar: changes the color code for the mapping of numbers (e.g. for spectrograms). It is useful especially for a better resolution for small values of amplitudes. Here, the roots square or the inverse exponential function increase the resolution for small amplitude values. The parameters Control element:"
    }, {
      "heading" : "7.9. Control elements for ’View: Spectrogram, FFT, CCF’ 99",
      "text" : "View: Spectrogram, FFT, CCF - Exponent for exponential function resp. Control element: View: Spectrogram, FFT, CCF - Index of the root control the quantitative characteristic.\n• Exponent for exponential function: see Control element: View: Spectrogram, FFT, CCF - Function for color bar (exponent for exponential characteristic). A larger value increases the resolution for smaller amplitudes.\n• Index of the root: see Control element: View: Spectrogram, FFT, CCF - Function for color bar (n-fold root). A larger value increases the resolution for smaller amplitudes.\n• Colormap: changes the color style for some functions (e.g. for spectrograms). The colormaps are explained in the Matlab documentation (e.g. for Jet: help jet). The colormap 1 - Gray values inverts the gray map: 0 is now white and 1 black.\n• Limits for color axis: defines user-defined limits for some visualizations (e.g. for spectrograms). Larger and lower values are limited to the minimal resp. maximal value. All values between are scaled to identical values. This option is useful for an identical color code in different figures with different minimal or maximal values. The option is switched off by Control element: View: Spectrogram, FFT, CCF - Automatic (esp. for root and exponential).\n• Automatic (esp. for root and exponential): uses an automatic scaling (minimum to maximum value) to compute the colormap. Otherwise, the values from Control element: View: Spectrogram, FFT, CCF - Limits for color axis are used as limits.\n• Number of dominant frequencies for visualization: defines the number of shown frequencies in a frequency list file (*.freq).\n• Plot FFT vs. period length (instead of frequency): plot the FFT results vs. the period length. If the option is deactivated, the results are shown vs. frequency.\n100 Chapter 7. Control elements"
    }, {
      "heading" : "7.10 Control elements for ’View: Classification and regression’",
      "text" : "• Display classes for output variables: switches the visualization of classification results with View - Classification - Result and similar functions:\n”only learning data:” shows the true class assignments for the learning data set.\n”only classification:” shows the classifier decisions.\n”DS-No misclassification:” shows the true class assignments for the learning data set with the data point number for misclassified data points.\n”Class-No misclassification:” shows the true class assignments for the learning data set with the number of the class decision for misclassified data points.\n• Show discriminant functions: plots the discrimination functions for the class borders using the recent classifier in the feature space.\n• Number of grid points: defines the number of grid points (in x- and y-direction) for the computation of regression functions or discriminant functions for class borders. The function will be applied to each grid point, resulting in (number of grid points)*(number of grid points) computations for two-dimensional problems.\n7.10. Control elements for ’View: Classification and regression’ 101\n• Different output variable: shows the results of classification and regression with a free selectable output variable if ”Different output variable” was chosen in Control element: View: Classification and regression - Display classes for output variables.\n• Show outlier detection: contains the options for the outlier detection.\n”None:” computes the result, but does not show any visualization.\n”only result:” shows the classification of the selected data points as outliers or normal data points.\n”Result with contour plot:” shows the classification of the selected data points as outliers or normal data points. In addition, lines with the values of the decision functions are plotted. The values of the feature space are expanded. The corresponding parameter can be changed in Control element: View: Classification and regression - Number of grid points.\n”only discriminant function:” shows only the class borders in form of discriminant functions.\n• Regression visualization: Normalize and aggregate input features: show the results of the estimated output variable of the regression model in View - Regression - Input variable(s), output variable, and regression function (2D resp. 3D) as a function of normalized and aggregated single features if such features were chosen in the design of the regression, see parameters in Control elements: Data mining: Regression. If not, the option does not influence the figure. If the option is switched off, the estimated output variable is shown as function of the original single features without normalization and aggregation. For more than two single features, View - Regression - GUI for multidimensional visualization has to be used.\n102 Chapter 7. Control elements"
    }, {
      "heading" : "7.11 Control elements for ’Data mining: Classification of single features’",
      "text" : "contains the most important elements for the parameterization of data mining methods. The parameterization of the chosen classifier is done using Control elements: Data mining: Special methods.\n• Selection of output variable: selects an output variable. This selection influences many functions as the evaluation of single features and time series, the design of classifiers, almost all visualization functions etc.\n• Selection of single features: defines the method for the feature selection (e.g. use the recent selected features, automatic selection by ANOVA), see Data mining - Selection and evaluation of single features.\n• Number of selected features: defines the maximum number of single features for an automatic feature selection.\n• Normalization of single features: performs a normalization of features before a classification. It improves the results for all methods which are sensitive against different scaled features. Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.\n7.11. Control elements for ’Data mining: Classification of single features’ 103\n• Preselection of features: preselects all manually selected features (by Control element: Single features - Selection of single feature(s) (SF)) for an automatic feature selection.\nExample:\nFeatures x5 and x7 are manually selected, selection of four features by ANOVA, best features are x8, x9, x10, x13 leads to the selection of x5, x7, x8, x9.\n• Downgrading of correlated single features: downgrades a feature to a relevance of zero if it correlates significantly to at least one better feature. This significance is measured by a positive (linear) Pearson correlation coefficient with a value larger than a threshold define by the Control element: Data mining: Statistical options - Threshold for correlation coefficient). The aim of this function is the generation of a short list with different features. Otherwise, some very similar features might dominate at the first places of the list.\n• Feature aggregation: defines the method for the linear feature aggregation by means of a multiplication with a weighting matrix. Possible options are ”No aggregation”, a (linear) Discriminant Analysis, a Discriminant Analysis followed by a numerical optimization (”DA with optimization”, see Control element: Data mining: Classification of single features - Criterion for optimized DA for the optimization criterion), a ”Principal Component Analysis (PCA)”, an ”Independent Component Analysis (ICA)” or the sum resp. mean value.\n• Number of aggregated features: defines the number of aggregated features (sd) in a classification (see e.g. [87]). The value is ignored if ”No aggregation” is chosen in Control element: Data mining: Classification of single features - Feature aggregation. A value of sd = 1 is always used for ”Sum” and ”Mean value”.\n• Criterion for optimized DA: specifies the criterion for a Discriminant Analysis with (numerical) optimization (see also Control element: Data mining: Classification of single features - Feature aggregation). Available options are an a maximization of the classification accuracy (”best_class”) or a maximization of the minimal distance between two classes using class-specific covariance matrixes as metric (”best_ldf”) [87].\n• Normalization of aggregated features: performs a normalization of the aggregated features before a classification. It improves the results for all methods which are sensitive against different scaled features. Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.\n• Chosen classifier: defines the type of the classifier. Possible options are a ”Bayes classifier”, an ”Artificial Neural Network”, a ”Support vector machine”, a ”k-nearest neighbor” classifier in two different implementation options, a ”Fuzzy classifier”, a ”Decision tree” and Ensemble learning (fitensemble). Due to incompatibilities, the parameter of Control element: Data mining: Classification of single features - Multi-class problems might be modified as well. Here, an additional warning is shown in the Matlab command window.\nThe selected classifier type is parameterized in more detail by Control elements: Data mining: Special methods.\n104 Chapter 7. Control elements\n• Multi-class problems: selects the approach for the optional decomposition of problems with more than two classes of the output variable:\n1. Pure multi-class problems solve the problem without a decomposition.\n2. The one-against-all method decomposes a problem with C classes in C two-class problems with class c vs. the union of all remaining classes.\n3. The one-against-one method decomposes a problem with C classes in all pairwise problems with class c vs. class j (j 6= c). For the cases 2 and 3, a fusion of the single results is automatically done.\n• Save confusion matrix in file: saves the confusion matrix for the training data set of the recent classifier in a file, if the option is activated.\n• Graphical evaluation of classification results: opens a figure with classification results in the feature space of the classifier (if the option was activated).\n7.12. Control elements for ’Data mining: Classification of time series’ 105"
    }, {
      "heading" : "7.12 Control elements for ’Data mining: Classification of time series’",
      "text" : "• Trigger time series: selects the trigger time series out of the existing time series. It is zero until the trigger event, contains increasing values to measure the number of samples since the trigger and zeros after the end of the recent event. If no trigger time series was selected, the sample numbers of the complete time series are used: It starts with a value of one for the first sample point and ends up with the length of the time series.\n• Classifier type: selects a classifier type for the time series:\nK1 contains only one classifier and ignores the trigger events. K5 is similar, but it uses the time since the trigger event as additional input variable. K2 is a special kind of single feature classifier for all sample points and aggregates results via a discriminant analysis. K3 and K4 have separate sub-classifiers for each sample point since the trigger event. K3 has a fix selection of time series, K4 a variable one for each sample point. TSK-Fuzzy is similar to K4, but it aggregates similar sample points by means of fuzzy sets.\n[22, 73] give a detailed description of the classifiers.\nThe feature selection and the underlying classifiers (e.g. Bayes classifiers) are parameterized using Control elements: Data mining: Classification of single features and Control element: Data mining: Special methods - Method.\n106 Chapter 7. Control elements\n• Time-weighting of time series relevances: gives the opportunity to prefer an earlier classification, e.g. for the feature selection of the K1 and K3 classifier. The method modifies the feature relevances by\nRk = R\nk − ktrig , for k > (ktrig + 0.15 · ktrig)\nktrig is the sample point of the last trigger event. It increases feature relevances near the trigger event in contrast to later ones.\n• Filtering of results: select the method for filtering. The following options are available:\n”No” filter is used..\n”IIR-Filter” uses the filter parameters in Control element: Data mining: Classification of time series - Parameter for IIR filter.\n”Classification error for training data” prefers samples with a better classification accuracy in the training data set in contrast to samples with a lower accuracy. This results in time-variant filter constants.\nThe ”Most frequent decision in a window” selects the estimated class with the highest frequency in a sliding window. The window length is defined in Control element: Data mining: Classification of time series - Temporal aggregation of features. The first sample points with a sample number up to the window length use all sample points up to this time for decision.\n• Results for filtering: switches between absolute and relative values as input values for the fusion by filtering. As an example, absolute values of a Bayes classifier are the estimated a-posteriori probabilities, relative values the percentage of the estimated a-posteriori probabilities for the given class relative to all classes.\n• Parameter for IIR filter: set the parameter for the IIR filtering for the temporal fusion of classification results in time series classification (see Control element: Data mining: Classification of time series - Filtering of results).\n• Temporal aggregation of features: tunes the temporal aggregation of sample points before the classification. The number and the names of time series remain unchanged.\nA selection of ”No” does not use a temporal aggregation.\nThe ”Mean value” is computed in a sliding window defined by Control element: Data mining: Classification of time series - Temporal aggregation: Window length and Control element: Data mining: Classification of time series - Temporal aggregation: Step width.\n”Minimum”, ”Maximum” and ”ROM” (Range of Motion) work with the same window, but with different operators.\nThe aggregation is always causal, i.e. only past sample points are aggregated. The first window does not start before the trigger event.\n• Temporal aggregation: Window length: defines the length of the sliding window for the temporal aggregation of features (see also Control\n7.12. Control elements for ’Data mining: Classification of time series’ 107\nelement: Data mining: Classification of time series - Temporal aggregation of features). The aggregation is always causal, i.e. only past sample points are aggregated. The first window does not start before the trigger event. Consequently, the first window terminates at ”Trigger event + Window length - 1”.\n• Temporal aggregation: Step width: step width for the distance between two sliding windows. If all sample points should be used, a value of ”1” must be chosen.\n• K5: use each x. sample point: The classifier K5 handles a training data set of a classification problem for time series as a samplewise classification problem with the time since the trigger event as an additional feature. As a consequence, it has a high memory consumption because all sample points are handled as separate data points. It might cause problems up to Matlab crashes especially for Support Vector Machines (SVM). The option reduces the number of sample points for the training of a K5 classifier.\n108 Chapter 7. Control elements\n• Trigger time series: selects the trigger time series out of the existing time series. It is zero until the trigger event, contains increasing values to measure the number of samples since the trigger and zeros after the end of the recent event. If no trigger time series was selected, the sample numbers of the complete time series are used: It starts with a value of one for the first sample point and ends up with the length of the time series.\n• Classifier type: selects a classifier type for the time series:\nK1 contains only one classifier and ignores the trigger events. K5 is similar, but it uses the time since the trigger event as additional input variable. K2 is a special kind of single feature classifier for all sample points and aggregates results via a discriminant analysis. K3 and K4 have separate sub-classifiers for each sample point since the trigger event. K3 has a fix selection of time series, K4 a variable one for each sample point. TSK-Fuzzy is similar to K4, but it aggregates similar sample points by means of fuzzy sets.\n[22, 73] give a detailed description of the classifiers.\nThe feature selection and the underlying classifiers (e.g. Bayes classifiers) are parameterized using Control elements: Data mining: Classification of single features and Control element: Data mining: Special methods - Method.\n• Time-weighting of time series relevances: gives the opportunity to prefer an earlier classification, e.g. for the feature selection of the K1 and\n7.12. Control elements for ’Data mining: Classification of time series’ 109\nK3 classifier. The method modifies the feature relevances by\nRk = R\nk − ktrig , for k > (ktrig + 0.15 · ktrig)\nktrig is the sample point of the last trigger event. It increases feature relevances near the trigger event in contrast to later ones.\n• Filtering of results: select the method for filtering. The following options are available:\n”No” filter is used..\n”IIR-Filter” uses the filter parameters in Control element: Data mining: Classification of time series - Parameter for IIR filter.\n”Classification error for training data” prefers samples with a better classification accuracy in the training data set in contrast to samples with a lower accuracy. This results in time-variant filter constants.\nThe ”Most frequent decision in a window” selects the estimated class with the highest frequency in a sliding window. The window length is defined in Control element: Data mining: Classification of time series - Temporal aggregation of features. The first sample points with a sample number up to the window length use all sample points up to this time for decision.\n• Results for filtering: switches between absolute and relative values as input values for the fusion by filtering. As an example, absolute values of a Bayes classifier are the estimated a-posteriori probabilities, relative values the percentage of the estimated a-posteriori probabilities for the given class relative to all classes.\n• Parameter for IIR filter: set the parameter for the IIR filtering for the temporal fusion of classification results in time series classification (see Control element: Data mining: Classification of time series - Filtering of results).\n• Temporal aggregation of features: tunes the temporal aggregation of sample points before the classification. The number and the names of time series remain unchanged.\nA selection of ”No” does not use a temporal aggregation.\nThe ”Mean value” is computed in a sliding window defined by Control element: Data mining: Classification of time series - Temporal aggregation: Window length and Control element: Data mining: Classification of time series - Temporal aggregation: Step width.\n”Minimum”, ”Maximum” and ”ROM” (Range of Motion) work with the same window, but with different operators.\nThe aggregation is always causal, i.e. only past sample points are aggregated. The first window does not start before the trigger event.\n• Temporal aggregation: Window length: defines the length of the sliding window for the temporal aggregation of features (see also Control element: Data mining: Classification of time series - Temporal aggregation of features). The aggregation is always causal, i.e. only past sample points are aggregated. The first window does not start before the trigger event. Consequently, the first window terminates at ”Trigger event + Window length - 1”.\n110 Chapter 7. Control elements\n• Temporal aggregation: Step width: step width for the distance between two sliding windows. If all sample points should be used, a value of ”1” must be chosen.\n• Number of clusters for TSK-Fuzzy: The TSK fuzzy classifier computes clusters to determine features with similar feature relevances vs. time. The parameter sets the number of clusters to be found. An interval value (e.g. 2:5) computes a set of clusters for each candidate number of clusters. The number with the first local minimum of the separation index will be selected.\n• Show cluster memberships: shows the result of the clustering by the search for time regions. It finds similar feature relevances for the time series classifier.\n• Threshold for fuzzy cluster: contains a threshold used for deletion of clusters representing very short regions. It represents the maximal value of the membership function. A large value simplifies the resulting classifier by the reduction of the number of linguistic terms for the TSK fuzzy classifier. A too low value might lead to an oversimplification.\n7.13. Control elements for ’Data mining: Regression’ 111"
    }, {
      "heading" : "7.13 Control elements for ’Data mining: Regression’",
      "text" : "• Feature type (input): switches between a regression model for single features with the model y = f(x1, ..., xs) and for time series with the model y[k] = f(x1[k − i], ..., x1[k − j], ..., xs[k − i], ..., xs[k − j]). The selection of input variables xi controls Control element: Data mining: Regression - Feature selection, the selection of output variables is done by Control element: Data mining: Regression - Output variable of regression. For time series, the sample points i, ..., j for the time shift are selected by Control element: Data mining: Regression - Sample points.\nThe selection of the related model is only possible, if single features resp. time series exist in the project.\n• Feature selection: selects the input variables of the regression model. Possible selections are ”All features”, ”Selected features” (in Control element: Single features - Selection of single feature(s) (SF) or Control element: Time series: General options - Selection of time series (TS)) and automatically by univariate resp. multivariate regression coefficients. To set the number of automatically selected features, use Control element: Data mining: Regression - Number of selected features.\n• Number of selected features: defines the number of selected features by an automatic feature selection. For time series, each\n112 Chapter 7. Control elements\ncombination of time series and sample points is handled as separate feature. As an example, x1[k − 1] and x1[k − 2] are two features. The value will be ignored if ”All features” or ”Selected features” are chosen in Control element: Data mining: Regression - Feature selection.\n• Preselection of features: preselects all manually selected features (by Control element: Single features - Selection of single feature(s) (SF)) for an automatic feature selection for regression of single features.\n• Normalization: performs a normalization of features before a regression. It improves the results especially for Artificial Neural Networks. Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.\n• Feature aggregation: defines the method for the linear feature aggregation by means of a multiplication with a weighting matrix. Possible options are ”No aggregation”, a ”Principal Component Analysis” or the sum resp. mean value.\n• Number of aggregated features: defines the number of aggregated features (sd) in a regression (see e.g. [87]). The value is ignored if ”No aggregation” is chosen in Control element: Data mining: Regression - Feature aggregation. A value of sd = 1 is always used for ”Sum” and ”Mean value”.\n• Normalization of aggregated features: performs a normalization of aggregated features before a regression. It improves the results especially for Artificial Neural Networks. Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.\n• Output variable of regression: selects a single feature or a time series at sample point [k] as output of the regression model. Control element: Data mining: Regression - Feature type (input) switches between single features and time series.\n• Type: selects the method for the regression model. The methods are parameterized in detail using Control elements: Data mining: Special methods.\n• Sample points: defines the time shift for the regression of time series. The values must not be positive avoiding acausal models. If a time series is input and output variable, the value k = 0 is deleted for this time series.\nExample:\nThe values ”0 -1 -24” with the time series x1, x2 as input variables and the time series x1 as output variable are chosen. The resulting model is x1[k] = f(x1[k−1], x1[k−24], x2[k], x2[k−1], x2[k− 24]).\n7.14. Control elements for ’Data mining: Clustering’ 113"
    }, {
      "heading" : "7.14 Control elements for ’Data mining: Clustering’",
      "text" : "• Feature classes: switches between a clustering of the selected time series (using the defined segment), the selected single features, or a co-clustering of time series and single features.\n• Number of clusters: defines the number of clusters to be computed by the cluster algorithms. A multiple selection (e.g. 2:8) is possible. In this case, the number is defined according to the first local minimum of the separation index.\n• Iteration steps: defines the maximal number of iteration steps of the cluster algorithm, if Control element: Data mining: Clustering - Unlimited iteration steps is deactivated. If the algorithms converge before this step, the algorithm is also terminated.\n• Unlimited iteration steps: executes the cluster algorithms until the internally defined convergence criterion was reached.\n• Distance measure: selects the metric for the distance between each data point and the cluster prototype.\n• Distance measure for the noise cluster: defines the method for the definition of a noise cluster [27] for a Fuzzy C-Means method. The\n114 Chapter 7. Control elements\noption ”None” does not generate a noise cluster. Alternatively, a new cluster is defined to which all data points have the same distance. Outliers are mainly assigned to this noise cluster. As a consequence, the influence of outliers to the other clusters is reduced. Possible distance measures are the mean distance of all data points to all other clusters [27] or the median of these distances. For the latter option, the distance is scaled by Control element: Data mining: Clustering - Factor for the distance to the noise cluster.\n• Factor for the distance to the noise cluster: defines a positive scaling factor for the distance to a noise cluster. A large value reduces the assignment of data points to the noise cluster (see also Control element: Data mining: Clustering - Distance measure for the noise cluster).\n• Fuzzifier for clustering: defines the fuzzifier in the fitness function of the Fuzzy C-Means method. The value should be at least 1.1 for numerical reasons. A higher value generates increases the fuzziness of the cluster membership values. For a value of 1, a crisp k-means method is used.\n• Compute start prototypes for clusters: selects the algorithms to compute start prototypes of the clusters. The possible options are ”equally distributed” clusters in the input space, ”random positions” of start prototypes, or ”random data points” selected as start prototypes.\n• Append cluster as output variable: adds the results of the cluster algorithms (the found crisp cluster memberships) to the project in form of a new output variable.\n• Video: shows the iterations of the cluster algorithms as video.\n• Plot original TS: shows the existing time series during the visualization of the cluster iterations. Possible options are ”all time series”, ”mean values”, and ”none”.\n• Fusion algorithm (Statistic Toolbox only): select the fusion algorithm for the hierarchical cluster algorithm of the Statistic Toolbox of MATLAB (see documentation of this toolbox).\n7.15. Control elements for ’Data mining: Special methods’ 115"
    }, {
      "heading" : "7.15 Control elements for ’Data mining: Special methods’",
      "text" : "• Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n• Metrics of Bayes classifier: defines the metric for the Bayes classifier.\n• Use a priori probabilities: switches the use of estimated a priori probabilities of the different classes for the Bayes classifier on resp. off. The estimation uses the training data set.\n116 Chapter 7. Control elements\n• Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n• Artificial Neural Network: Type: sets the type of the Artificial Neural Network. The available options include a Multi Layer Perceptron Net (MLP) and a Radial Base Function Net (RBF). The nets are used from the Neural Network Toolbox of Matlab.\n• MLP: number of output neurons: defines the number of output neurons for MLP nets. Possible options are ”one neuron” (values of the output variable: 1 to maximum class number) or ”one neuron per class” (values of the output variable: 0 ...1 for the related class membership). The latter case generates a higher number of parameters in the net, but gives often better results. A main reason is a better fit for more complex class topologies in the input space.\nFor regression models, only one output neuron will be used.\n• Number of neurons per layer: defines the number of neurons for the hidden layer.\n• MLP: learning algorithm: defines the learning algorithms for the MLP net (see documentation of the Neural Network Toolbox).\n7.15. Control elements for ’Data mining: Special methods’ 117\n• MLP: neuron type hidden layer: defines the type of neurons in the input layer of a MLP net (see documentation of the Neural Network Toolbox).\n• MLP: input weighting: describes the method to combine the input of a neuron with the weights. The option ”dotprod” uses a multiplication.\n• MLP: input function: describes the method to combine the weighted inputs of a neuron. The standard option ”netsum” adds all weighted inputs.\n• Error goal: defines the error goal for the neural net. If the goal is reached, the design process will be terminated.\n• Number of learning epochs: defines the maximum number of learning epochs for MLP nets (see documentation of the Neural Network Toolbox).\n• MLP: Visualization step width: defines the number of iterations between the visualizations of the MLP net. The value is only relevant if Control element: Data mining: Special methods - MLP: plot is activated.\nThe design process of a neural net is restarted after each visualization with the last net as starting parameter. However, some internal strategy parameter for the design might be lost for some learning algorithms. Larger values mostly result in lower errors and faster convergence.\n• MLP: plot: switches plots for the recent learning progress of MLP nets on or off (see documentation of the Neural Network Toolbox).\n• Show training GUI: switches the use of the Learning GUI window of the Neural Network Toolbox on or off.\n118 Chapter 7. Control elements\n• Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n• Artificial Neural Network: Type: sets the type of the Artificial Neural Network. The available options include a Multi Layer Perceptron Net (MLP) and a Radial Base Function Net (RBF). The nets are used from the Neural Network Toolbox of Matlab.\n• Number of neurons per layer: defines the number of neurons for the hidden layer.\n• Error goal: defines the error goal for the neural net. If the goal is reached, the design process will be terminated.\n• RBF: Spread: defines the spread of the Radial Base Function. A larger spread smoothes the function approximation. Too large spreads can cause numerical problems.\n7.15. Control elements for ’Data mining: Special methods’ 119\n• Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n• Artificial Neural Network: Type: sets the type of the Artificial Neural Network. The available options include a Multi Layer Perceptron Net (MLP) and a Radial Base Function Net (RBF). The nets are used from the Neural Network Toolbox of Matlab.\n• Number of layers: defines the number of hidden layers of feedforward nets.\n• Number of neurons per layer: defines the number of neurons for the hidden layer.\n• MLP: learning algorithm: defines the learning algorithms for the MLP net (see documentation of the Neural Network Toolbox).\n• Error goal: defines the error goal for the neural net. If the goal is reached, the design process will be terminated.\n120 Chapter 7. Control elements\n• Number of learning epochs: defines the maximum number of learning epochs for MLP nets (see documentation of the Neural Network Toolbox).\n• MLP: Visualization step width: defines the number of iterations between the visualizations of the MLP net. The value is only relevant if Control element: Data mining: Special methods - MLP: plot is activated.\nThe design process of a neural net is restarted after each visualization with the last net as starting parameter. However, some internal strategy parameter for the design might be lost for some learning algorithms. Larger values mostly result in lower errors and faster convergence.\n• MLP: plot: switches plots for the recent learning progress of MLP nets on or off (see documentation of the Neural Network Toolbox).\n• Show training GUI: switches the use of the Learning GUI window of the Neural Network Toolbox on or off.\n7.15. Control elements for ’Data mining: Special methods’ 121\n• Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n• Artificial Neural Network: Type: sets the type of the Artificial Neural Network. The available options include a Multi Layer Perceptron Net (MLP) and a Radial Base Function Net (RBF). The nets are used from the Neural Network Toolbox of Matlab.\n• Number of neurons: defines the number of neurons per dimension for the design of Self Organizing Maps with Data mining - Self Organizing Maps - Design. The number is used for all dimensions, e.g., a value of 8 with a dimensionality of 2 means the design of a net with 8x8 neurons.\n• Dimension: defines the dimensionality for the design of Self Organizing Maps with Data mining - Self Organizing Maps - Design.\n• Number of learning epochs: defines the maximum number of learning epochs for MLP nets (see documentation of the Neural Network Toolbox).\n• Show training GUI: switches the use of the Learning GUI window of the Neural Network Toolbox on or off.\n122 Chapter 7. Control elements\n• Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n• Use SVM-internal one-against-one or one-against-all: uses the internal decomposition of multi-class problems of the SVM toolbox. The function is faster and uses less memory compared to the external decomposition of SciXMiner. This external decomposition works with the one-against-one resp. the one-against-all option in Control element: Data mining: Classification of single features - Multi-class problems.\n• Kernel: defines the used kernel function for Support Vector Machines [18]. Homogenous polynomial kernels (without bias term), polynomial kernels (with bias term), and Gaussian radial base functions are implemented. The order is tuned by Control element: Data preprocessing - Kernel order. A first-order polynomial kernel means a linear hyperplane.\n• Kernel order: set the order of the kernel of the SVM. It is used for the classification with the SVM and by the SVM-based outlier detection.\n• Penalty term C: tunes the compromise between a small number of classification errors and the simplification of the hyperplanes to discriminate classes for the SVM. Larger values of C lead to a higher number of classification errors to simplify the hyperplanes.\n7.15. Control elements for ’Data mining: Special methods’ 123\n• Epsilon: defines the width of the insensitive region of the loss function in Support Vector Regression.\n124 Chapter 7. Control elements\n• Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n• k: defines the number of the k neighbors for the k-nearest neighbor classifier.\n• Metrics of k-NN: defines the metric to compute the nearest neighbors.\n• Distance weighting: enables a weighting of different distances.\nIf ”None” is selected, all neighbors contribute equally to the decision (standard option).\n”Inverse linear” prefers the nearest neighbor by a weighting with 1. The farthest of the k neighbors is weighted with 0. All neighbors between have an equidistant weighting, e.g. 0.25, 0.5 and 0.75 for k = 4.\n”Inverse distance” uses 1/Distance as weight to prefer neighbors with smaller distances. A neighbor with distance 0 gets a maximum weight of 1/1E − 10. ”Inverse exponential” is similar but with more equal weights by using e−d with the distance d.\n• Region size: specifies the type of environment around a data point. If the option Control element: Data min-\n7.15. Control elements for ’Data mining: Special methods’ 125\ning: Special methods - Use all neighbors in a region was activated, all neighbors in the defined environment are included as neighbors (and not necessary only the k nearest).\n• Use all neighbors in a region: includes all neighbors in an environment defined by Control element: Data mining: Special methods - Region size. The classifier normalizes all values to an interval [0, . . . , 1]. Consequently, the value should be smaller than 1!\nIf less than k neighbors are localized in the environment, only the k nearest neighbors are used.\n• Minimum neighbor number in max. distance: defines the minimum number of neighbors in a maximum accepted distance defined by Control element: Data preprocessing - Max. distance to make a decision.\n• Evaluate minimum number of neighbors: defines if a minimum number of neighbors in an environment (defined by a distance) should be used. If the number is not reached and the option is activated, the decision is set to rejection. The distance is tuned by Control element: Data preprocessing - Max. distance, the number by Control element: Data mining: Special methods - Minimum neighbor number in max. distance.\n• Max. distance: defines the accepted maximum distance for a neighborhood (see Control element: Data mining: Special methods - Evaluate minimum number of neighbors).\n126 Chapter 7. Control elements\n• Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n• k: defines the number of the k neighbors for the k-nearest neighbor classifier.\n7.15. Control elements for ’Data mining: Special methods’ 127\n• Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n• Type of membership function: defines the algorithm for the design of a membership function (see Control element: Data mining: Special methods - Number of linguistic terms).\n”Median” generates terms with approximately equal data point frequencies for all terms in the training data set.\n”Equal distribution” generate terms with similar distances between the maximum values of the membership functions.\n”Clustering” uses cluster prototypes as parameters.\n”Fix” set the parameters of the membership functions to the values defined by Control element: Data mining: Special methods - Parameters MBF (fix).\nThe choice ”with interpretability” rounds the found parameters using a method described in [76] to improve the interpretability.\nRemark:\nAll design methods set the random number generator of Matlab to a fix value. It improves the reproducibility of found membership functions. Nevertheless, all methods with random elements might be sensitive to the state of the random number generator (e.g. cluster algorithms with random\n128 Chapter 7. Control elements\nstart clusters). It can also influence other methods using these membership functions, e.g. by different selected features in a decision tree (e.g. Data mining - Selection and evaluation of single features - Information theoretic measures).\n• Number of linguistic terms: defines the desired number of linguistic terms for the design of the membership functions. The designed membership functions are standard partitions: The first and the last term have trapezoidal membership functions, middle terms triangular membership functions. The membership functions sum up to one for each value of the variable.\nThe term number might be automatically reduced by the designed method, e.g. if only two different values exist and three terms should be designed.\n• Parameters MBF (fix): defines the parameters of the membership functions if ”fix” was chosen in Control element: Data mining: Special methods - Type of membership function.\n• Type of decision tree: defines the splitting criterion for the design of the decision tree. The type ID3 uses a maximized mutual information. The type C4.5 maximizes the mutual information divided by the input entropy. In addition, a statistical correction of the entropy can be chosen. It reduces the influence of an overestimation of feature relevances for small training data sets and avoids overfitted trees.\nAll trees use the design fuzzy membership functions instead of computed binary splits for each node.\n• One-against-all-trees: uses C different decision trees for the rule generation. The trees are generated by a one-against-all approach of a class against all other classes.\n• Exponent for clearness: defines the evaluation method for the pruning of fuzzy rules [76]. An increasing clearness prefers highly specialized error-free rules to more generalized rules with some errors.\n• Inference: defines the inference method for the fuzzy system. The MIN-MAX inference uses the minimum operator for all conjunctions (e.g. for the aggregation) and the maximum for all disjunctions (e.g. for the accumulation of all rules with identical conclusions. The SUM-PROD inference uses the bounded sum for all disjunctions and the product for all conjunctions. An additional correction of overlapping rules [75] was implemented for artifacts by rules with the same or concurrent conclusions in case of the SUM-PROD inference.\n• Number of rules for a rulebase: defines the maximal number of selected rules in a fuzzy rulebase.\n• Looking for cooperating rules from rulebase: switches the search for cooperating rule bases on or off. If the option is off, all found single rules are used for classification or regression.\n• Significance level: influences the statistical evaluation of fuzzy rules. Values towards 100 % tends to the generation of only few, but very confident rules.\n7.15. Control elements for ’Data mining: Special methods’ 129\n• Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n• Type of membership function: defines the algorithm for the design of a membership function (see Control element: Data mining: Special methods - Number of linguistic terms).\n”Median” generates terms with approximately equal data point frequencies for all terms in the training data set.\n”Equal distribution” generate terms with similar distances between the maximum values of the membership functions.\n”Clustering” uses cluster prototypes as parameters.\n”Fix” set the parameters of the membership functions to the values defined by Control element: Data mining: Special methods - Parameters MBF (fix).\nThe choice ”with interpretability” rounds the found parameters using a method described in [76] to improve the interpretability.\nRemark:\nAll design methods set the random number generator of Matlab to a fix value. It improves the reproducibility of found membership functions. Nevertheless, all methods with random elements might be sensitive to the state of the random number generator (e.g. cluster algorithms with random\n130 Chapter 7. Control elements\nstart clusters). It can also influence other methods using these membership functions, e.g. by different selected features in a decision tree (e.g. Data mining - Selection and evaluation of single features - Information theoretic measures).\n• Number of linguistic terms: defines the desired number of linguistic terms for the design of the membership functions. The designed membership functions are standard partitions: The first and the last term have trapezoidal membership functions, middle terms triangular membership functions. The membership functions sum up to one for each value of the variable.\nThe term number might be automatically reduced by the designed method, e.g. if only two different values exist and three terms should be designed.\n• Type of decision tree: defines the splitting criterion for the design of the decision tree. The type ID3 uses a maximized mutual information. The type C4.5 maximizes the mutual information divided by the input entropy. In addition, a statistical correction of the entropy can be chosen. It reduces the influence of an overestimation of feature relevances for small training data sets and avoids overfitted trees.\nAll trees use the design fuzzy membership functions instead of computed binary splits for each node.\n• Significance level: influences the statistical evaluation of fuzzy rules. Values towards 100 % tends to the generation of only few, but very confident rules.\n7.15. Control elements for ’Data mining: Special methods’ 131\n• Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n• Polynomial degree: defines the maximal degree m of a polynomial function y = a0 + a1x+ ...+ amxm.\n• Set a0 to zero: forces a polynomial regression model to estimate a model without a constant term called a0.\n• Maximal number of internal features: enables an internal feature reduction during the computation of the polynomial model. It is useful especially for the selection of terms with the most relevant power coefficients.\n• Weight matrix: selects a weighting matrix for data points during a Least Square estimation. ”None” uses equal weights (1), ”inverse proportional to class frequency” prefers data points from rare linguistic terms of the selected output variable.\n132 Chapter 7. Control elements\n• Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n• Fit: specifies the method for the interpolation with the MATLAB function fit (part of the Curve Fitting Toolbox). For more details type ”help fit” in the MATLAB command window. The following methods are implemented (in parentheses: number of input variables and specialties): weibull (1D, no negative inputs), exponential (1D), fourier (1D), gauss (1D), cubicinterp (1D or 2D, no interpolation), pchipinterp (1D, no interpolation), poly curve (1D or 2D), power (1D, no negative inputs), rational (1D), sinus (1D), cubicspline (1D, no interpolation), smoothingspline (1D), biharmonicinterpolation (2D), local linear regr (2D), local quadratic regr (2D).\n• Parameter 1: select context-sensitive the first parameter for the selected method in Control element: Data mining: Special methods - Fit. This parameter is not used by every method.\n• Parameter 2: select context-sensitive the second parameter for the selected method in Control element: Data mining: Special methods - Fit. This parameter is not used by every method.\n7.15. Control elements for ’Data mining: Special methods’ 133\n• Method: parameterizes a method for the design of a classifier or regression model. The elements are contextsensitive to the method selected by Control element: Data mining: Special methods - Method.\n• Ignore output variables with many terms: defines the number of linguistic terms to ignore output variables with many terms (e.g., names or IDs) in an association analysis.\n• Number of rules: defines the maximum number of rules for an association analysis.\n• Minimum support: defines the minimal support P(A AND B) of a rule IF A THEN B.\n• Minimum confidence: defines the minimal confidence P(B|A) of a rule IF A THEN B.\n• Sorting: defines the sorting order for the found rules by support or by confidence.\n134 Chapter 7. Control elements"
    }, {
      "heading" : "7.16 Control elements for ’Data mining: Statistical options’",
      "text" : "• Type for correlation parameters: defines the type of correlation (Linear: Pearson; Rank-oriented: Kendall, Spearman) to be used in visualizations or output files for correlations.\n• Threshold for correlation coefficient: defines a threshold for the correlation coefficient. A larger value is interpreted as a relevant pairwise correlation between two features. This value influences e.g. the listing of relevant correlation coefficients or the downgrading of correlated features in the feature selection.\n• Show correlation only for one selected feature: computes only the correlations of all selected single features to the feature defined by Control element: Data mining: Regression - Output variable of regression.\n• p-value for t test: defines the p-value for the t-test (accepted error probability of a statistical test). Usual values are 0.01...0.05.\n• Show all: defines whether all tested or only significant different single features (measured by the t-test) for the output variable should be shown.\n7.16. Control elements for ’Data mining: Statistical options’ 135\n• Bonferroni correction: defines if statistics tests and correlations are computed with or without (”no”) corrections for multiple testings. A Bonferroni corrections divides the α values in Control element: Data mining: Statistical options - p-value for t test by the number of tests. The less conservative BonferroniHolm correction divides α by the number of test - the ranking of the tested values + 1. It accepts all successful tests before the first rejected test.\n• Test of normal distribution: selects a methods to test the normality of a distribution using Data mining - Selection and evaluation of single features - Test of normal distribution. Possible values are Chi Square Test, Lillie Test, Anderson Darling Test and Jarque Bera Test from the MATLAB Statistics Toolbox.\n• Show p values for correlation: defines the option for the reporting of p-values for correlation coefficients unequal zero in the correlation visualization (View - Single features - Correlation coefficients (Pearson) etc.).\n• Norm (Data point distances): select the used norm to compute data point distances.\n136 Chapter 7. Control elements"
    }, {
      "heading" : "7.17 Control elements for ’Data mining: Validation’",
      "text" : "• n-fold cross-validation: defines the number n of parts for a n-fold cross-validation. As an example, a value of 10 means that the training data set is separated in 10 parts. For each run, 9 parts are used as training data and one part as validation data set. Consequently, a complete trial of a cross-validation requires 10 runs containing each selected data point exactly one times in the validation data set. Typical values are n = 2 . . . 10.\nThe value will be ignored for the selection of ’Leave-one-out’ (n is equal to the number of selected data points) or ’Bootstrap’ in Control element: Data mining: Validation - Validation strategy.\n• Number of trials: defines the number of trials for a cross-validation. Internally, a random assignment of data points to the different parts will be chosen. This assignment takes the output variables into account to get an approximately equal distribution of linguistic output terms for the different parts. As a consequence, the results of different trials will differ. A higher number of trials costs computing time, but improves the estimation for the classification accuracy. Typical values are 1...50 depending on the necessary computing time.\nThe value will be ignored for the selection of ’Leave-one-out’ in Control element: Data mining: Validation - Validation strategy (always 1 due to the deterministic selection of training data).\n7.17. Control elements for ’Data mining: Validation’ 137\n• Validation strategy: switches the validation type between cross-validation, leave-one-out (N -fold cross-validation with the number N of selected data points and exactly one data point in each validation data set) and bootstrap method (training data set: random selection of data points with replacement resulting usually in a multiple selection of some data points, validation data set: all none selected data points) (see e.g. [29]).\n• Plot in file: controls if the validation results are written into a protocol file or not.\n• Crossvalidation (separated by classes): use the classes of the selected output variables to split the dataset. As a consequence, the data points belonging to a class are used as validation dataset, all other data points as training dataset. As usual for a crossvalidation, this split is rotated so that each selected data points is used exactly once as in the validation dataset.\n138 Chapter 7. Control elements"
    }, {
      "heading" : "7.18 Control elements for ’General options’",
      "text" : "• TEX protocol: generates most generated protocol files in a LATEXformat. Otherwise, ASCII files are produced.\n• For tex tables: show tabularx: switches the tabularx style in the generation of Latex tables on or off.\n• Memory-optimized feature generation: modifies the algorithm for feature extraction from time series. If the option is chosen, the computing time for extraction increases and the risk of memory problems decreases.\n• Percentages 2D histogram: switches the generation of percentage information (per row and per column) in the file containing quantitative information of 2D histograms on or off.\n• Save options by saving the project: switches the saving of options (all GUI elements) on or off, if a project is saved.\n• Show tool tips: switches the context-sensitive help for control elements on or off, if the mouse arrow is over an element.\n7.18. Control elements for ’General options’ 139\n• Invisible figures in SciXMiner batch files: hides all figures during execution of a SciXMiner batch file. This option is useful for parallel working on the same computer without disturbances of opening figures. The SciXMiner main figure is also hidden. If a batch file stops by error, the hidden figures can be restored by the input of the command ”restore_figures” in the MATLAB command window.\n• Detail plot of function interna: influences the level detail for some texts and figures with intermediate results (marked = more details).\n• Show menu tips: if activated, a short description of the clicked menu item is shown as part of the project overview (Control elements: Project overview).\n• Show short protocol: switches off the complete listing for all control elements in the protocol files. Here, only some project characteristics are written into the files.\n• For macros: plot always in current figure: plots all visualizations in the recent figure. This option is useful for the generation of user-defined subplots in macros. However, it requires a manual modification of the macro with figure and sub-figure commands. Otherwise, figures are overwritten without any warning.\n• Project names as part of the variable names: adds project names during the project fusion for new single features and time series to mark the source project of a single feature or time series.\n• For macros: stop if error: switches off the error handling with try/catch in macros. It allows a better backtracking of error messages.\n• Output variable: few terms first: shows in View - Classes for selected data points the output variables with only few terms first. This option is useful in projects with many linguistic terms and data points.\n• Open files in MATLAB editor: defines whether generated text files (e.g., for feature evaluation) will be immediately opened by the MATLAB editor or not.\n• Number format for result plots: defines the format of numbers for most visualization and protocol functions, especially the number of digits before and after the decimal point . ”%g” is the automatically chosen standard Matlab format, ”%f” means floating points. The parameter does not influence export functions.\n• Name conventions for feature generation: defines the name conventions for extracted features. Possible options are a combination of name fragments by blanks or underscores.\n• File type for images: defines the file type for the hardcopy of all open figures into files. The abbreviations are the same as for the MATLAB print command. The EPS format is subdivided into black-and-white images (eps) and color images (epsc).\n140 Chapter 7. Control elements\n• Manual selection of data points: sets the numbers of data points for the selection with Edit - Select - Data point from GUI.\n• Percent for selection: defines the percentage of selected data points by using Edit - Select - Random data points (defined percentage of selected data points).\n• Save mode for MATLAB file: defines the compatibility mode for binary MATLAB files. ’-V6’ is compatible to the MATLAB versions 5 and 6, ’-V7.0’ to version 7 and ’-V7.3’ to version 7.3. For lower versions, only a reduced set of modes is available.\n• Load plugins at start: defines the modus of loading and initializing of plugins for feature extraction during loading a project: never, always, only if the project contains time series or images.\n• Order of linguistic terms: defines a new order for the linguistic terms of the selected output variable. The resorting is done by Edit - Convert - Resort linguistic terms (order from GUI).\n• Configuration name for MATLAB parallel: defines the name of the configuration for the MATLAB Parallel Computing Toolbox that can be started by Extras - Matlab Parallel - Start (see also Parallel - Manage Configurations in the menu of the MATLAB command window)."
    }, {
      "heading" : "8 Feature extraction from time series",
      "text" : ""
    }, {
      "heading" : "8.1 Definition of feature types by plugins",
      "text" : "The feature extraction of time series is realized with plugins. Please notice that SciXMiner was actually developed in German language, so some variable names are German. Unfortunately, it was impossible to change this inconvenience.\nPlugins are single Matlab functions named plugin_*.m, which are included in a special directory of the SciXMiner installation (...\\scixminer\\plugins\\mgenerierung) or in the directory which contains the loaded project. To work with SciXMiner properly, they have to abide by the following format:\nfunction [datenOut, ret, info] = pluginfct(paras, datenIn).\nThe return value info contains the following elements:\n• info.beschreibung: short description of the new feature with one or two words (e.g. minimum, maximum, velocity). Only one description is allowed, independent from the numbers of computed features. The description must not start with a hyphen (minus)!\n• info.bezeichner: short identifier of the new feature (e.g. MIN, MAX, V). The matrix has to contain one identifier per feature or time series (every row is a single identifier). Use str2mat(’String’,’OtherString’) to create a matrix with one identifier per row.\n• info.explanation: explanation of the plugin (approx. 1-2 sentences)\n• info.explanation_long: optionally an additional longer text, used only for documentation\n• info.anz_zr: numbers of time series extracted by the plugin (has to be 0, if info.anz_em 6= 0)\n• info.anz_benoetigt_zr: numbers of time series needed for the computation of the new time series. If > 1 the first anz_benoetigt_zr time series are always given to the plugin. The plugin have to check the correct amount of time series! If the plugin is able to deal with an arbitrary amount of time series (e.g. computation of minimum), use Inf for anz_benoetigt_zr.\n• info.anz_em: numbers of features extracted by the plugin (has to be 0, if info.anz_zr 6= 0)\n• info.anz_benoetigt_em: number of input single features of the plugin (will be used in future versions)\n• info.anz_im: number of images generated in the plugin (only used together with Image Extension Package)\n142 Chapter 8. Feature extraction from time series\n• info.anz_benoetigt_im: number of images needed for the generation of new images or single features (only used together with Image Extension Package)\n• info.callback: optional callback function to avoid the transfer of images into functions (only used together with Image Extension Package)1\n• info.einzug_OK: If the plugin is able to deal with intervals other than the complete time series, set einzug_OK to 1, otherwise to 0. For a description of intervals, see Section 8.4).\n• info.richtung_entfernen: special parameter for plugins for the gait analysis. Removes identifier \"left\" or \"right\" if this variable is set to 1. The default is 0.\n• info.typ: specifies the type of the result: ’TS’: time series, ’SF’: feature. The type ’Norm’ requires additional normative data. For images, the additional types ’IM’ (image) and ’IMSlice’ (3D stack of images) are in preparation. A plugin must not extract both time series and features.\nMissing info elements are replaces by standard entries if it is possible.\nPlugins can contain parameters. Here, the use of one or more plugin parameters with edit or popup elements is possible. For each parameter p, the following definitions exist:\n• info.commandline.description{p}: name of the parameters\n• info.commandline.parameter_commandline{p}: default value (string, number or vector of numbers; scalar for popup elements means number of the selected string)\n• info.commandline.popup_string{p}: string with alternatives, e.g. ’yes|no’ (only for popup elements)\n• info.commandline.tooltext{p}: text for tool tips\n• info.commandline.wertebereich{p}: possible range for numbers in edit elements, the two elements means the minimum and maximum value, e.g. {1 Inf }. The use of variables is possible, e.g. {1 ’paras.par.laenge_zeitreihe’ } for the length of the time series\n• info.commandline.ganzzahlig{p}: optional restriction for the use of integer values in edit elements, if the value is 1\n• info.commandline.format{p}: ignoring of the internal format supervision, if the value is ”any”\nThe parameters are defined using Control element: Plugin sequence - Plugin parameter for the recent selected plugin in Control element: Plugin sequence - Selection of plugins resp. Control element: Plugin sequence - Selected plugin sequence. The change between different parameters of a plugin is done using Control element: Plugin sequence - No.. All parameters can be modified using a macro.\nIn a plugin, the access to the parameters is possible with paras.parameter_commandline{2} for the second parameter etc.\nThe inputs are as follows:\n• datenIn 1Operations on images needs more time if images are transferred to functions. As an alternative, the callbacks are used in\nplugin sequences to allow a direct access to the variable myimage in the workspace.\n8.1. Definition of feature types by plugins 143\n– datenIn.dat: matrix of the input data (size paras.par.anz_dat × sample points in interval × info.anz_benoetigt_zr). The third dimension is missing, if info.anz_benoetigt_zr == 1!\n– datenIn.ref : data of a reference.)\n• paras\n– paras.par: par vector from SciXMiner – paras.code: currently selected output variable – paras.code_alle: all output variables – paras.var_bez: identifier of time series – paras.merk_red: number of features to select – paras.einzuege: interval from which the data has been copied out of the complete time series\n(size 1× 2) – paras.ind_zr_merkmal: indices of selected time series (size 1× anz_benoetigt_zr). – paras.anz_gew_merk: number of selected time series – paras.parameter: SciXMiner parameter struct (contains options from the GUI) – paras.parameter_commandline: cell array with values of the plugin parameters\nThe parameters are set in the file \"merkmalsgenerierung_plugins.m\". One can add new elements to the parameter struct. Use the Matlab command isfield to check, whether an element exists in the struct. The following elements has been added for some plugins:\n– iirfilter: parameter for an IIR-Filter (included due to plugin_iirfilter) – iirfilter_aS_aL_aSigma: Some plugins (e.g. Trend and standard deviation estimation) use\nmultiple IIR-Filters\n– abtastfrequenz: sampling rate of the time series [Hz] – samplepunkt: single sample for the computation of a new feature (e.g. extraction of a specific\nsample out of a given time series\nIf the plugin is called with an empty matrix for datenIn, the info-Struct has to be returned by the plugin. datenOut and ret remain empty. This proceeding is necessary to query some information about the existing plugins. The paras-Struct is always committed by the calling function. The first rows of the plugin could be:\ninfo = struct(’identifier’, ’MIN’, ’anz_zr’, 1, ... ’anz_em’, 0, ’typ’, ’ZR’, ...); if (nargin < 2 | isempty(datenIn)) datenOut = []; ret = []; return; end;\nThe outputs have to contain the following elements:\n• datenOut:\n– datenOut.dat_zr: new time series, if the plugin extracts time series (size paras.par.anz_dat × paras.par.laenge_zeitreihe × info.anz_zr). Empty otherwise.\n144 Chapter 8. Feature extraction from time series\n– datenOut.dat_em: new features, if the plugin extracts features (size paras.par.anz_dat × info.anz_em). Empty otherwise.\n• ret:\n– ret.ungueltig: set to 1, if the extraction of the feature or time series has failed. The result of the plugin is not accepted by SciXMiner.\n– ret.bezeichner: contains the identifier of the time series or features if the amount of extracted time series or features depends on parameters in the GUI (e.g. Principal Component Analysis or IIR-Filter). If this element exists and is not empty it is used as identifier instead of the identifier from the info-Struct. The identifier from the info-Struct must not be changed."
    }, {
      "heading" : "8.2 Standard plugins in SciXMiner",
      "text" : "A list of include plugins in the standard installation of SciXMiner is shown in Section H. In a project, the list of all available plugins can be shown by Control element: Plugin sequence - Show plugins."
    }, {
      "heading" : "8.3 Plugins for single features from single features",
      "text" : "Plugins for the extraction of new single features from existing single features are not yet implemented in a formal way. Two different alternatives exist:\n• implementation of individual plugins via macros (see two examples in the directory standardmakros: feature_plugin_div2.makrog for the division of two selected single features and feature_plugin_log.makrog to compute the logarithm of all selected features)\n• computing aggregated features (see Edit - Extract - Single features -> Single features (with the selected feature aggregation from Options-Data Mining: Classification of single features))"
    }, {
      "heading" : "8.4 Defining intervals via files",
      "text" : "Intervals are used to restrict the feature extraction to specific sample points of the time series (only available for plugins with info.einzug_OK = 1.\nAfter loading a SciXMiner project, all *.einzug files in the current directory and in the subdirectory plugins/einzuggenerierung of the SciXMiner installation are imported. An interval file is an ASCII file including a chart of the intervals. The rows are separated with line breaks, columns with tabs. To avoid an empty interval do not add a line break after the last interval. The first row of the file has to include the following four column names (separated by tabs):\nIdentifier Short Identifier Start Stop\nThe intervals can be defined in the following rows, e.g.:\nStandphase ST_P 1 [FootOff] Schwungphase SW_P [FootOff]+1 -1\n8.5. Definition of a feature ontology by categories 145\nSpecial features are:\n• In the definition, features and mathematical operations can be used, e.g. [OppositeFootOff]+([OppositeFootContact]-[OppositeFootOff])/2. Pay attention, that used features exist in the project!\n• The end of a time series can be labeled by -1 or maxtime.\nFunction calls or variables from the Matlab workspace can not be used."
    }, {
      "heading" : "8.5 Definition of a feature ontology by categories",
      "text" : "Categories are useful to define an ontology for single features and time series. Here, similar features and time series can be grouped into classes. They defined one or more categories. Such categories might be used in different analysis and visualization functions (see [64, 114]).\nThe assignment to categories can be done by plugins, segments, or name fragments. It is defined in one or more files with the extension *.categories. Here, all files in the current working directory or in the subdirectory ”plugins/mgenerierung” of SciXMiner or a user-defined extension package are considered.\nThese functions are text files and have a pre-defined format:\n#CategoryName ’Temporal derivation’\n#DefaultTermName ’No’\n#TermName ’Velocity’ #TermRelevance 0.8 ’plugin_geschwindigkeit’ ’plugin_geschwindigkeit_kausal’ ’_V’ ’ V ’\n#TermName ’Acceleration’ #TermRelevance 0.6 ’plugin_beschleunigung’ ’_A’ ’ A ’\nThe following key words are used:\n• #CategoryName: name of the category.\n• #TermName: name of a term of the recent category followed by a string. In the next rows, identifiers for a mapping of time series or single features are defined. Such identifiers can be name fragments (e.g. ’_V’), names of used plugins (e.g. ’plugin_geschwindigkeit’) or names of segments using the key word einzug_short_description (e.g. einzug_ST_P). Consequently, each time series or single feature containing any of these identifiers belongs to the\n146 Chapter 8. Feature extraction from time series\nterm ’Velocity’ of the category ’Temporal derivation’ if ’_V’ is part of the name, the plugin ’plugin_geschwindigkeit’ was used for feature extraction by Edit - Extract - Time series -> Time series, Time series -> Single features....\n• #DefaultTermName (optional): defines the name for the default term. It used if as time series or a single feature does not belong to an other term of the category. Its standard name is ’unknown’.\n• #TermRelevance (optional): defines the a priori relevances for the recent term. It might be defined with one or two values between zero and one separated by a space. The first value is the interpretability weighted by the exponent alpha, the second the implementability weighted by the exponent beta. If the values are missing, the standard value is one."
    }, {
      "heading" : "9 Conclusions and perspectives",
      "text" : "This manual describes the functionality of the open source Matlab toolbox SciXMiner. The aim of this toolbox is to provide an interface to apply and compare data mining methods. The architecture of the toolbox is chosen in a way that the developer crew from Karlsruhe, Germany and/or other developer crews are easily able to enlarge the toolbox by further algorithms. Everyone is kindly invited to support the further development of SciXMiner.\nThanks:\nThanks to all the busy programmers, developers of algorithms, testers, especially to Rüdiger Alshut, Alessandro Angelin, Martin Ashby, Sebastian Beck, Sebastian Braun, Ole Burmeister, Joachim Dieterle, Patrick Gerland, Sebastian Gollmer, Andreas Gommlich, Lutz Gröll, Markus Grube, Eduard Hübner, Jens Jäkel, Sina Keller, Thilo Krüger, Jessica Legradi, Tobias Loose, Mihai Lipovei, Jörg Matthes, Dimitrios Patikas, Sebastian Pfeiffer, Tim Pychynski, Matthias Schablowski, Oliver Schill, Martin Wieland, Sebastian Wolf, Mohamed Zayani and Baifan Zhou. The support by the Deutsche Forschungsgemeinschaft (German research association) within the project ’Diagnosis support in gait analysis” was a great help to build the gait analysis-specific part and thus, to build a basis for the further development of the toolbox. The Collaborative Research Center for Humanoid Robots, also sponsored by DFG, has inspired us to many technical applications. The present contribution was supported by the Helmholtz Association in the program ”BioInterfaces in Technology and Medicine” and under the Joint Initiative “Energy System 2050– A Contribution of the Research Field Energy”.\n147\nA Important file structures\n• A SciXMiner batch file (*.batch) is an ASCII file. It contains directories, projects, options and macros for automated analyses.\n• A classifier file (*.class) is a Matlab file. It contains data to apply a designed classifier as klass_single struct array. It includes the selected single features, aggregation transformations and the classifier itself as well as chosen standardizations. The mapping of single features and of the output variables uses the feature names. One-against-x classifiers are described by a vector- shaped klass_single struct.\n• A frequency list file (*.freq) is a Matlab file. It contains a struct freqlist with two substructs: freqlist.segments characterizes segments with elements freqlist.segments.f (vector with two elements for the lower and the upper frequency) and .name (segment name). freqlist.multiples characterizes overtones with freqlist.segments.f (scalar for frequency), .ftol (tolerance for overtones) and .name (overtone name). An example is shown in sounds.freq in the directory standardmakros.\n• A fuzzy system file (*.fuzzy) is a Matlab file. It contains membership functions and a fuzzy rulebase. The mapping to single features and to the output variable is done by variable names.\n• A segment file (*.einzug) is an ASCII file. It defines segments for the feature extraction from time series (see Section 8.4 for details).\n• A macro file (*.makrog) is a text file containing a sequence of pushed menu items and control elements. This file can be manually modified, e.g. by adding other Matlab commands.\n• A plugin sequence file (*.plugseq) is (binary) Matlab file. It contains a plugin sequence with plugin parameters.\n• A SciXMiner project file (*.prjz) is a (binary) Matlab file. It contains at least the three-dimensional matrix of time series (d_orgs) or the matrix with single features (d_org). In addition, some additional data can be included as e.g. names of time series. This file can be loaded and saved using the SciXMiner GUI or the Matlab command window (see Section 4.2.3).\n• A LATEXfile (*.tex) is an ASCII file containing various results. It is prepared to be included using LATEX. The user can switch between normal text files and LATEXfiles with the control element Control element: General options - TEX protocol. Some examples for result protocols are rule bases (*_cp*.tex), feature relevances (*_alle_merkmale.tex, *_besten_merkmale.tex), mean values (*_MWST.tex) or correlation coefficients (*_CORR.tex).\n• A text file (*.txt) is an ASCII file containing various results. It can be opened with any text editor. The user can switch between text files and LATEXfiles with the control element Control element: General options - TEX protocol. The results are similar to LATEXfiles.\n149\n• An option file (*.uihdg) is a Matlab file containing all options defined by the control elements in Chapter 7.\nB Important internal data structures\nPlease notice that SciXMiner was actually developed in German language. From this time, some variable names are German. Unfortunately, it was impossible to change this inconvenience.\nName Dimension Remarks bez_code (sy, any) Vector (string array) with names of output variables cluster_ergebnis (Struct) Struct for results and parameters of cluster methods code (N, 1) Vector of the selected output variable, classes must be coded as serial integer values between 1 and my code_alle (N, sy) Matrix of all existing output variables, classes must be coded\nas serial integer values between 1 and my code contains a redundant copy for the selected output.\nd_org (N, s) Matrix of single features d_orgs (N,K, sz) Matrix of time series (WARNING! Definition as global vari-\nable to avoid memory problems due to multiple copies in the called functions)\ndorgbez (s, any) Matrix (string array). The rows contain names of single features. If the names are not defined in the project, standard names as x1 ... xs are chosen fuzzy_system (Struct) Struct for a designed fuzzy system gaitcad_extern (Struct) Struct for different variables (especially for macros and\nSciXMiner batch files), which are reloaded after the start of SciXMiner\nind_auswahl (Nred, 1) Indices of selected data points interpret_merk (s, 1) a priori relevances of single features (set to an empty variable\nby deactivation of Control element: Single features - A priori feature relevances)\ninterpret_merk_rett (s, 1) saved a priori relevances of single features (are not changed by deactivation of Control element: Single features - A priori feature relevances, but are necessary to restore interpret_merk after (re-activation)) klass_single (Struct) Struct with settings and parameters for feature classifiers klass_zr (Struct) Struct with settings and parameters for time-series classifiers L (Struct) Struct with settings for decision theory. It can be initialized\nas empty. At the moment in use: L.ld: Matrix of decision costs for the recent output variable, L.ld_alle (struct): Matrix of decision costs of all output variables\n151\nName Dimension Remarks makro_lern (1, any) contains the macro as string, which is used for the design of classifier in a validation process makro_test (1, any) contains the macro as string, which is used for the application of classifier in a validation process mu_y (N,my) matrix, membership values of the estimated output variable as a result of the fuzzy analysis parameter (Struct) Struct with parameters about the project. A detailed description is given later-on in this chapter pos (N, 1) contains the estimated class membership for a chosen classifier. Data points which are not classified are marked with 0. prz (N, 1) contains the relative probability for a class estimation. Data points which are not classified are marked with 0. var_bez (sz, beliebig) matrix (string array), rows contain the denotations of the time\nseries. They are set to standard terms, if they are not defined within the project file. After the last row of this variable the output variable is appended.\nzgf (s+ 1,max(ml,my)) parameter of the membership functions (MBF) of the linguistic terms of the s features and of the output variable in the element s + 1, features: only trapezoid MBFs at the margins and triangular MBFs for all others permitted, output variable: singletons zgf_bez (s+ 1,max(ml,my)) array, contains the denotation of the linguistic terms of the s features and of the output variable in the element name. It is set to standard terms if it is not defined within the project file. zgf_y_bez (sy,max(my,i)) array, contains the denotations of the linguistic terms of the output variables in the element name. It is set to standard terms if it is not defined within the project file.\nThe parameter-Struct contains three elements, allgemein, gui, projekt. These elements again contain Structs.\nparameter.allgemein contains general information about paths, extensions, and status variables.\nparameter.gui encapsulates manages the control elements and contains variables, which correspond to the control elements.\nBy encapsulated functions it is ensured, that these variables always contain the current content. Furthermore, the setting of these variables and the following actualization of the graphical user interface is implemented, too.\nparameter.projekt contains values of the recent project, for instance file name, path of the project and further status variables. In parameter.projekt.abtastfrequenz the sampling frequency of the time series within the project may be saved."
    }, {
      "heading" : "C Needed Standard Toolboxes",
      "text" : "Some functions need standard toolboxes from Matlab:\nthe Signal Processing Toolbox is used\n• to filter time-series (functions filter and butter),\n• to calculate spectrograms (function specgram),\n• to calculate Cross and Auto Correlation Functions (function xcorr),\n• to calculate Matlab wavelet decompositions (function interp) and\n• to import time series with varying lengths (function resample)\nThe use of resample is checked when time-series are imported. If an error occurs, a warning will be displayed and the import is proceeded with appended zeros.\nThe Neural Network Toolbox is necessary, to calculate and apply a Artificial Neural Network as classifier.\nThe Statistic Toolbox is necessary,\n• to calculate paired or an unpaired t-test,\n• to use k-NN classifiers from this toolbox with other metrics than the Euclidean (function pdist) and\n• to use the cluster-function (out of this toolbox).\nThe Wavelet Toolbox is needed, to do a Matlab wavelet decomposition. If this toolbox is not available, a SciXMiner-integrated version may be used.\nD Included External Toolboxes (GNU-License)\nICA − Toolbox 3 % FastICA f o r Mat lab 5 . x\n% V e r s i o n 2 . 1 , J a n u a r y 15 2001 % C o p y r i g h t ( c )\n6 %Hugo Gäver t , Jarmo Hur r i , J aakko S ä r e l ä , and Aapo Hyvär inen (GNU−GPL) .\nf a s t i c a .m 9 f p i c a .m\npcamat .m remmean .m\n12 w h i t e e n v .m\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 15\n% SVM and K er ne l Methods Mat lab Toolbox \\ c i t e { Canu03 } % h t t p : / / a s i . i n s a−rouen . f r / ~ arako tom / t o o l b o x / i n d e x . h tml\n18 % C o p y r i g h t S CANU − scanu@insa−rouen . f r (GNU−GPL)\n21 c o u t .m monqp .m s v m c l a s s .m 24 svmkerne l .m s v m m u l t i c l a s s .m s v m m u l t i c l a s s o n e a g i a n s t o n e .m 27 s v m m u l t i v a l .m s v m m u l t i v a l o n e a g i a n s t o n e .m svmreg .m 30 svmval .m\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 33\nSOM Toolbox % V e r s i o n 2 . 0 be t a , May 30 2002\n36 % C o p y r i g h t 1997−2000 by Esa Alhoniemi , Johan Himberg , % Juha P a r h a n k a n g a s and Juha Vesan to (GNU−GPL) % h t t p : / / www. c i s . h u t . f i / p r o j e c t s / somtoo lbox / 39 knn .m s o m _ e u c d i s t 2 .m v i s _ v a l u e t y p e .m\n42\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n154 Appendix D. Included External Toolboxes (GNU-License)\nl p _ s o l v e 5 . 5 . 0 . 7 45\n% h t t p : / / l p s o l v e . s o u r c e f o r g e . n e t / 5 . 5 / % Co−d e v e l o p e r s : Michel B e r k e l a a r , K j e l l E ik l and , P e t e r N o t e b a e r t\n48 % L i c e n c e t e r m s : GNU LGPL ( L e s s e r G e n e r a l P u b l i c L i c e n c e ) % C i t a t i o n p o l i c y : G e n e r a l r e f e r e n c e s a s p e r LGPL % Module s p e c i f i c r e f e r e n c e s a s s p e c i f i e d t h e r e i n\n51\nmxlpso lve . d l l l p s o l v e 5 5 . d l l\n54 mxlpso lve .m lp_maker .m\n57 The ∗ .DLL have t o be c o p i e d i n < m a t l a b r o o t > / b i n / !\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 60 Kolmogorov−Smirnov−T e s t f o r d i s t r i b u t i o n s\n% Armin Günther , U n i v e r s i t ä t G r e i f s w a l d , Germany 63\nk s t e s t .m\n66 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− XML r e a d (BSD l i c e n c s e )\n69 %J a r e k Tuszynsk i , SAIC %h t t p : / / www. mathworks . com / m a t l a b c e n t r a l / f i l e e x c h a n g e /12907− x m l i o t o o l s\n72 xmlread .m\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 75 F i s h e r e x t e s t : F i s h e r ’ s Exac t P r o b a b i l i t y T e s t\n%T r u j i l l o −O r t i z , A. , R . Hernandez−Walls , A. Cas t ro−Perez , L . 78 %Rodr iguez−Cardozo , N.A. Ramos−Delgado and R . Garc ia−Sanchez .\n%(2004) . F i s h e r e x t e s t : F i s h e r ’ s Exac t P r o b a b i l i t y T e s t . A %MATLAB f i l e . [WWW document ] . h t t p : / / www. mathworks . com /\n81 %m a t l a b c e n t r a l / f i l e e x c h a n g e / l o a d F i l e . do ? o b j e c t I d =5957\nf i s h e r e x t e s t .m 84\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− VAT ( v i s u a l a s s e s s m e n t o f t e n d e n c y ) − an a l g o r i t h m t o improve t h e\nv i s u a l i z a t i o n o f d a t a p o i n t d i s t a n c e m a t r i c e s 87\n% C o p y r i g h t by Timothy Havens , Michigan Tech % h t t p : / / www. ece . mtu . edu / ~ t h a v e n s / code /VAT.m\n90\nv a t .m\n93 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\nA s s o c i a t i o n A n a l y s i s ( A p r i o r i A lgo r i t hm ) : 96 Given a s e t o f t r a n s a c t i o n s , f i n d r u l e s t h a t w i l l p r e d i c t t h e o c c u r r e n c e o f\nan i t em based on t h e o c c u r r e n c e s o f o t h e r\n155\ni t e m s i n t h e t r a n s a c t i o n\n99 % a u t h o r : N a r in e Manukyan 0 7 / 0 8 / 2 0 1 3 % C o p y r i g h t ( c ) 2013 , N a r i ne Manukyan\n102 f i n d R u l e s .m\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 105\nU i p i c k f i l e s : GUI program t o s e l e c t f i l e s and / o r f o l d e r s\n108\n% V e r s i o n : 1 . 1 5 , 2 March 2012 % Author : Douglas M. Schwarz\n111 % Email : dmschwarz= i e e e ∗org , dmschwarz= u r g r a d ∗ r o c h e s t e r ∗ edu % R e a l _ e m a i l = r e g e x p r e p ( Email , { ’ = ’ , ’∗ ’ } , { ’@’ , ’ . ’ } ) % h t t p : / / www. mathworks . com / m a t l a b c e n t r a l / f i l e e x c h a n g e /10867− u i p i c k f i l e s −− u i g e t f i l e −on−s t e r o i d s 114\nu i p i c k f i l e s .m u i p i c k f i l e s _ l i c e n s e . t x t\n117\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− MATLAB2TIKZ Save f i g u r e i n n a t i v e LaTeX ( TikZ / P g f p l o t s ) .\n120\n% C o p y r i g h t ( c ) 2008−−2014, Nico Schloemer < n i c o . schloemer@gmai l . com>\n123 m2tUpdater .m c l e a n f i g u r e .m f i g u r e 2 d o t .m 126 m 2 t I n p u t P a r s e r .m m a t l a b 2 t i k z .m\n129 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− LIBSVM\n132 % h t t p : / / www. c s i e . n t u . edu . tw / ~ c j l i n / l i b sv m % V e r s i o n : 3 . 2 0 % Author : Chih−Chung Chang and Chih−Jen Lin 135 % L i c e n s e : m o d i f i e d BSD l i c e n s e\nl ibsvm −3 . 2 0 \\∗ .∗ 138\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− ARESLab\n141\n% h t t p : / / www. cs . r t u . l v / j e k a b s o n s / % V e r s i o n : 1 . 1 3 . 0\n144 % Author : G i n t s J e k a b s o n s ( g i n t s . j e k a b s o n s @ r t u . l v ) % L i c e n s e : GNU GPL\n147 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− CSV r e a d\n150 % h t t p s : / / g i t h u b . com / g i d e o n g f e l l e r / m a t l ab / b lob / m a s t e r / r ead_mixed_csv .m\n156 Appendix D. Included External Toolboxes (GNU-License)\n% l i c e n s e : none\n153 r ead_mixed_csv .m"
    }, {
      "heading" : "E Symbols and abbreviations",
      "text" : "Symbol Meaning ACF Auto correlation function ANOVA Analysis of Variances CCF Cross Correlation Function CCOEF Cross Correlation Function DA Discriminant analysis FFT Fast Fourier transformation ICA Independent Component Analysis k-NN k-nearest-neighbor MBF Membership function MEAN Mean value MLP Multi-layer perceptron MANOVA Multivariate Analysis of Variances PCA Principal Component Analysis SF Single feature STD Standard deviation SVM Support Vector Machines TS Time series K Number of sample points m Number of linguistic terms of all features ml Number of linguistic terms of the l-th feature xl my Number of linguistic terms (classes) of the output variable N Number of data points s Number of features sm Number of selected features sd Number of transformed (aggregated) features sz Number of time-series"
    }, {
      "heading" : "F Known errors and problems",
      "text" : "• SciXMiner needs certain variable, which must not be overwritten or deleted. For instance, the variable parameter encapsulates the control elements and enables SciXMiner to internally access these variables. If this Variable is deleted or does not contain the needed elements, a further execution of SciXMiner is not possible. The only possible solution is a restart. Further necessary variables are listed in Appendix B.\n• To execute a macro as well the needed features and/or time series have to exist in within the project as the output variables. There may be the possibility that a macro has to be recorded separately for each project to ensure a proper functionality.\n• If a macro is recorded, it saves the names of selected elements in a popup window for a reconstruction. If the name is not available during a replay, an error message ”Inconsistent selection fields (see MATLAB command window for further details)” is shown. In this case, you should verify the available elements in this field. Possible reasons are new names for variables or changed names for program options.\n• Frequently opening and closing of the SciXMiner GUI leads to a slower processing of figures. This problem may be traced back to an incorrect deletion of control and menu items in Matlab.\n• At the moment, massive problems with lp_solve occur due to version conflicts.\n• Using the novelty detection with the one-class method the following error message occurs: ”Error! lp_solve library not found. Please copy *.dll from the toolbox path in ...\\bin”: The novelty detection with the one-class method uses an external toolbox, which needs a dynamic link libraries (DLL). Matlab is not able to find these libraries, if they are not contained in the general Matlab path for libraries. This directory is located in ”<matlabroot>\\bin”. The directory <matlabroot> is given in Matlab by using the command matlabroot. The following files have to be copied from the SciXMiner subdirectory ”toolbox” to the above mentioned directory: ”mxlpsolve.dll” and ”lpsolve55.dll”.\n• Applying the Matlab wavelet decomposition the following error message occurs: ”Error using ==> interp Length of data sequence must be at least 9 You either need more data or a shorter filter (L).”: The integrated Matlab function calculates time-series which are shorter than the original time series. A plugin interpolates them to the correct length. The used command for interpolation (interp) interrupts if time series are too short. Indeed, the used number of sample points may be reduced for the interpolation. However, this error correlates with another error in the wavelet decomposition, which originates from a too high number of levels. To solve the problem reduce the number of levels or deactivate the Matlab wavelet decomposition. Then, an alternative implementation will be used.\n• Errors occur for the macro recording if popup elements have digits as first characters.\n159\n• Missing MATLAB functions can cause SciXMiner errors in older MATLAB versions:\n– The MATLAB function ”iscolumn” is available from MATLAB version 2011a.\nG Version history\nG.1 Versions\n• earlier Gait-CAD-Versions, see version history in Gait-CAD documentation\n• Version 2016b (13.03.2017)\n• Version 2017a (12.04.2017)\nG.2 Selected changes between Gait-CAD version 2014b and SciXMiner version 2016b\n• integration of support vector regression\n• option to generate polynomial models without absolute value (a0 term)\n• handling of timestamp data (conversion from strings in linguistic terms of output variables to timestamps resp. from timestamps to date and time)\n• visualization of multidimensional single features\n• optional hiding of figures in batch files\n• extended conversion options from data points into time series\n• export of the error of the recent regression model into a single feature\n• deletion of selected or unselected sample points in time series\n• deletion of unselected single features\n• additional style options for protocol files of classes and single features\n• switchable percentage statistics in 2D histograms\n• various internal changes related to the transfer from Gait-CAD to SciXMiner (naming, internal code optimizations etc.)\n• large variety of bug fixes\n• compatibility to MATLAB 2016b\n160\nG.3. Selected changes between SciXMiner versions 2016b and 2017a 161\nG.3 Selected changes between SciXMiner versions 2016b and 2017a\n• compatibility to MATLAB 2017a"
    }, {
      "heading" : "H Plugins",
      "text" : "• COG SF (COG): computes the center of gravity of a time series or a time series segment.\n– Function name: plugin_cog_em.m – Type: SF – Time series: 1 inputs, 0 outputs, Segments possible: yes – Single features: 0 inputs, 1 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Maximum (MAX): computes the maximum of a time series or a time series segment as single feature.\n– Function name: plugin_max.m – Type: SF – Time series: 1 inputs, 0 outputs, Segments possible: yes – Single features: 0 inputs, 1 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Maximum position (MAPO): computes the position of the maximum (number of the sample point) for a time series or a time series segment as single feature.\n– Function name: plugin_mapo.m – Type: SF – Time series: 1 inputs, 0 outputs, Segments possible: yes – Single features: 0 inputs, 1 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Mean value SF (MEAN): computes the mean of a time series or a time series segment as single feature.\n– Function name: plugin_mean_em.m\n162\n163\n– Type: SF – Time series: 1 inputs, 0 outputs, Segments possible: yes – Single features: 0 inputs, 1 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Mean value SF NaN (MEAN): computes the mean of a time series or a time series segment as single feature. NaN values in a time series will be ignored.\n– Function name: plugin_mean_em_nan.m – Type: SF – Time series: 1 inputs, 0 outputs, Segments possible: yes – Single features: 0 inputs, 1 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Median SF (MEDIAN): computes the median of a time series or a time series segment as single feature.\n– Function name: plugin_median_em.m – Type: SF – Time series: 1 inputs, 0 outputs, Segments possible: yes – Single features: 0 inputs, 1 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Median SF NaN (MEDIAN): computes the median of a time series or a time series segment as single feature. NaN values in a time series will be ignored.\n– Function name: plugin_median_em_nan.m – Type: SF – Time series: 1 inputs, 0 outputs, Segments possible: yes – Single features: 0 inputs, 1 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Minimum (MIN): computes the minimum of a time series or a time series segment as single feature.\n– Function name: plugin_min.m – Type: SF\n164 Appendix H. Plugins\n– Time series: 1 inputs, 0 outputs, Segments possible: yes – Single features: 0 inputs, 1 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Minimum position (MIPO): computes the position of the minimum (number of the sample point) for a time series or a time series segment as single feature.\n– Function name: plugin_mipo.m – Type: SF – Time series: 1 inputs, 0 outputs, Segments possible: yes – Single features: 0 inputs, 1 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Norm deviation (absolute value) (ND Abs): computes the mean absolute norm deviation of a time series in a segment as single feature (mean value of Eq. (3) in [114] for a segment)\n– Function name: plugin_normzahl_betrag.m – Type: SF – Time series: 1 inputs, 0 outputs, Segments possible: yes – Single features: 0 inputs, 1 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Norm deviation (direction) (ND Dir): computes the mean value of the directed deviation from a time series to a norm time series in a segment as a single feature. (mean value of the directed norm deviation for a segment, Eq. (3) in [114] without the absolute value in the nominator)\n– Function name: plugin_normzahl_richtung.m – Type: SF – Time series: 1 inputs, 0 outputs, Segments possible: yes – Single features: 0 inputs, 1 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Range of Motion (ROM): computes the range (of motion) of a time series or a time series segment as single feature.\n– Function name: plugin_rom.m – Type: SF\n165\n– Time series: 1 inputs, 0 outputs, Segments possible: yes\n– Single features: 0 inputs, 1 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 0\n• STD SF (STD SF): computes the standard deviation of all values of a time series or a time series segment. Normalization to K instead of K-1\n– Function name: plugin_std_em.m\n– Type: SF\n– Time series: 1 inputs, 0 outputs, Segments possible: yes\n– Single features: 0 inputs, 1 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 0\n• Sum SF (SUM): computes the sum if a time series or of a time series segment as single feature.\n– Function name: plugin_sum_em.m\n– Type: SF\n– Time series: 1 inputs, 0 outputs, Segments possible: yes\n– Single features: 0 inputs, 1 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 0\n• TS->DISCR SF MEAN (TERMQ1TERMQ2TERMQ3TERMQ4TERMQ5): computes the frequencies for (crisp) discretizations in time series.\n– Function name: plugin_discr_em.m\n– Type: SF\n– Time series: 1 inputs, 0 outputs, Segments possible: yes\n– Single features: 0 inputs, 5 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 2\n∗ Number of terms (Number of terms for discretization) ∗ Type of membership function (defines the algorithm for the design of a membership\nfunction (see ”Control element: Data mining: Special methods - Number of linguistic terms”). ”Median” generates terms with approximately equal data point frequencies for all terms in the training data set.\n166 Appendix H. Plugins\n”Equal distribution” generate terms with similar distances between the maximum values of the membership functions. ”Clustering” uses cluster prototypes as parameters. ”Fix” set the parameters of the membership functions to the values defined by ”Control element: Data mining: Special methods - Parameters MBF (fix)”. The choice ”with interpretability” rounds the found parameters using a method described in Literature: Mikut05 to improve the interpretability. Remark: All design methods set the random number generator of Matlab to a fix value. It improves the reproducibility of found membership functions. Nevertheless, all methods with random elements might be sensitive to the state of the random number generator (e.g. cluster algorithms with random start clusters). It can also influence other methods using these membership functions, e.g. by different selected features in a decision tree (e.g. ”Data mining - Selection and evaluation of single features - Information theoretic measures”). )\n• TS->FUZZY SF MEAN (TERM1TERM2TERM3TERM4TERM5): computes the frequency of fuzzy terms in a fuzzified time series as single features.\n– Function name: plugin_fuzzy_em.m – Type: SF – Time series: 1 inputs, 0 outputs, Segments possible: yes – Single features: 0 inputs, 5 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 2\n∗ Number of terms (Number of terms for discretization) ∗ Type of membership function (defines the algorithm for the design of a membership\nfunction (see ”Control element: Data mining: Special methods - Number of linguistic terms”). ”Median” generates terms with approximately equal data point frequencies for all terms in the training data set. ”Equal distribution” generate terms with similar distances between the maximum values of the membership functions. ”Clustering” uses cluster prototypes as parameters. ”Fix” set the parameters of the membership functions to the values defined by ”Control element: Data mining: Special methods - Parameters MBF (fix)”. The choice ”with interpretability” rounds the found parameters using a method described in Literature: Mikut05 to improve the interpretability. Remark: All design methods set the random number generator of Matlab to a fix value. It improves the reproducibility of found membership functions. Nevertheless, all methods with random elements might be sensitive to the state of the random number generator (e.g. cluster algorithms with random start clusters). It can also influence other methods using these membership functions, e.g. by different selected features in a decision tree (e.g. ”Data mining - Selection and evaluation of single features - Information theoretic measures”). )\n167\n• TS->PC SF (PC1PC2): computes new single features using a Principal Component Analysis of a time series or a time series segment (K -> s_d).\n– Function name: plugin_zrhkem.m – Type: SF – Time series: 1 inputs, 0 outputs, Segments possible: yes – Single features: 0 inputs, 2 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 2\n∗ Number of aggregated PCA features (Number s_d of principal components from a time series (Transformation number of sample points K -> s_d))\n∗ Normalize standard deviations (defines the normalization for the variances of the sample points)\n• TS->SF (TSSP): extracts the value of the time series for one sample point as single feature.\n– Function name: plugin_zr_em.m – Type: SF – Time series: 1 inputs, 0 outputs, Segments possible: none – Single features: 0 inputs, 1 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Number of the sample point (Sample point for Time series -> Single feature)\n• below/above norm mean value (ND Sign): computes a single feature indicating values above or below the norm time series\n– Function name: plugin_normzahl_mittelwert.m – Type: SF – Time series: 1 inputs, 0 outputs, Segments possible: yes – Single features: 0 inputs, 1 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Absolute value (ABS): rectifies a time series by computing the absolute values\n– Function name: plugin_abs.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs\n168 Appendix H. Plugins\n– Direct callback: none – Number of parameters: 0\n• Acausal median of a window (FE-MED-AC): sets all values of the time series in the window to the median value of the window. Here, an acausal filter is used.\n– Function name: plugin_zr_fenster_medac.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Window length (determines the window length in sample points for the filtering with a sliding window.)\n• Acceleration (A): computes the second time derivation of a time series. see Eq. (3.3) in [64]).\n– Function name: plugin_beschleunigung.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Time (defines the scaling (per sample point resp. per time using the sampling frequency from the GUI))\n• Addition of time series (ADDTS): adds all selected time series.\n– Function name: plugin_add_zr.m – Type: TS – Time series: Inf inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Change sign (SIGNIN): multiplies a time series with −1\n– Function name: plugin_vorzeichen_umkehr.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs\n169\n– Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Compute square root (ROOT): Compute root of the amplitudes of a time series.\n– Function name: plugin_root.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Compute trend (Trend): Here, two first order IIR filters with the parameter aF (fast) and aS (slow) are used. The computation is explained in [90].\n– Function name: plugin_iirfilter_trend.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Parameter aF aS (to 0: no smoothing, to 1: strong smoothing. aSlow must be greater than aFast.)\n• Difference of two time series (DIFF): computes the difference between the first and second time series.\n– Function name: plugin_diffzr.m – Type: TS – Time series: 2 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Estimate standard deviation (StdTS): applies different digital lowpass filters x_f [k + 1] = a ∗ x_f [k] + (1 − a) ∗ x[k] with parameters aFast, aSlow and aSigma to compute the standard deviation. see Eq. (2-6) in [90] with aF = aSigma, aL = aSlow, aS = aFast.\n– Function name: plugin_iirfilter_stdschaetzer.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none\n170 Appendix H. Plugins\n– Single features: 0 inputs, 0 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 1\n∗ Parameters aFast aSlow aSigma (to 0: no smoothing, to 1: strong smoothing. aSlow must be greater than aFast.)\n• Filtered maximum (Fil-MAX): sliding maximum value with exponential forgetting. Here, a causal filter is used.\n– Function name: plugin_zr_gefiltert_max.m\n– Type: TS\n– Time series: 1 inputs, 1 outputs, Segments possible: none\n– Single features: 0 inputs, 0 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 0\n• Filtered minimum (Fil-MIN): sliding minimum value with exponential forgetting. Here, a causal filter is used.\n– Function name: plugin_zr_gefiltert_min.m\n– Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none\n– Single features: 0 inputs, 0 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 0\n• Filtering (FIL): filters a time series. A Butterworth filter is used.This plugin needs the Signal Processing Toolbox of Matlab. The Bode plot can be shown under Show - Time series.\n– Function name: plugin_filter.m\n– Type: TS\n– Time series: 1 inputs, 1 outputs, Segments possible: none\n– Single features: 0 inputs, 0 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 4\n∗ Filter type (determines the filter characteristics in the frequency domain.) ∗ Frequencies (defines the critical frequencies for the filter using the unit of the sample\nfrequency (e.g. in Hz). The second parameter is only used for bandpass filters.) ∗ Filter order (FIL) (determines the order of a filter.)\n171\n∗ Initial values (computes initial values of the filter, Filtic+Static assumes a steady state filter at k=1, computed with MATLAB function filtic.m)\n• Filtering with Morlet wavelet (Morl):\n– Function name: plugin_morletfilter.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 3\n∗ Morlet wavelet: frequency (This frequency describes the center of the region which is not damped by the Morlet-Wavelet.)\n∗ Morlet wavelet: frequency (The width of the region is defined by the eigenfrequency of the Morlet wavelet)\n∗ Causal Morlet wavelet (Use the same transformation matrix (e.g. PCA) for time series reduction of all data points)\n• IIR filter (IIR): applies a digital lowpass filter x_f [k + 1] = a ∗ x_f [k] + (1− a) ∗ x[k].\n– Function name: plugin_iirfilter.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Parameter a (to 0: no smoothing, to 1: strong smoothing)\n• Individual norm deviation (NABW): Here, the plugins IIR (Parameter aF) and StdTS are used for the estimation\n– Function name: plugin_normabweichung.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Parameters aFast aSlow aSigma (to 0: no smoothing, to 1: strong smoothing. aSlow must be greater than aFast.)\n• Jerk (J): computes the jerk (3rd time derivation) of a time series (acausal).\n172 Appendix H. Plugins\n– Function name: plugin_ruck.m\n– Type: TS\n– Time series: 1 inputs, 1 outputs, Segments possible: none\n– Single features: 0 inputs, 0 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 1\n∗ Time (defines the scaling (per sample point resp. per time using the sampling frequency from the GUI))\n• Jump to zero (ZEROJump): set value = 1 for a jump to zeros, and value = 0 otherwise.\n– Function name: plugin_nsprung.m\n– Type: TS\n– Time series: 1 inputs, 1 outputs, Segments possible: none\n– Single features: 0 inputs, 0 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 0\n• Logarithm 10 TS (LOG10): computes the logarithm to the base 10 for all values of a time series.\n– Function name: plugin_log10_zr.m\n– Type: TS\n– Time series: 1 inputs, 1 outputs, Segments possible: none\n– Single features: 0 inputs, 0 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 0\n• Maximum of a window (FE-MAX): sets all values of the time series in the window to the maximum value of the window. Here, a causal filter is used.\n– Function name: plugin_zr_fenster_max.m\n– Type: TS\n– Time series: 1 inputs, 1 outputs, Segments possible: none\n– Single features: 0 inputs, 0 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 1\n∗ Window length (determines the window length in sample points for the filtering with a sliding window.)\n173\n• Maximum of multiple time series (MAXTS): computes the maximum for all time series in the configuration window.\n– Function name: plugin_max_zr.m – Type: TS – Time series: Inf inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Mean of a window (FE-MEAN): sets all values of the time series in the window to the mean value of the window. Here, a causal filter is used.\n– Function name: plugin_zr_fenster_mean.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Window length (determines the window length in sample points for the filtering with a sliding window.)\n• Mean value of multiple time series (MEANTS): computes the mean for all time series in the configuration window.\n– Function name: plugin_mean_zr.m – Type: TS – Time series: Inf inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Median of a window (FE-MED): sets all values of the time series in the window to the median value of the window. Here, a causal filter is used.\n– Function name: plugin_zr_fenster_median.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none\n174 Appendix H. Plugins\n– Number of parameters: 1\n∗ Window length (determines the window length in sample points for the filtering with a sliding window.)\n• Minimum of a window (FE-MIN): sets all values of the time series in the window to the minimum value of the window. Here, a causal filter is used.\n– Function name: plugin_zr_fenster_min.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Window length (determines the window length in sample points for the filtering with a sliding window.)\n• Minimum of multiple time series (MINTS): computes the minimum for all time series in the configuration window.\n– Function name: plugin_min_zr.m – Type: TS – Time series: Inf inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Multiplication of time series (MULTTS): multiplies all time series that are selected in the configuration window.\n– Function name: plugin_mult_zr.m – Type: TS – Time series: Inf inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Multiplication with a constant (CONST): Multiplication with a constant.\n– Function name: plugin_mult_const.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs\n175\n– Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Constant gain\n• Norm deviation time series absolute value (NDTS ABS): computes the absolute value of the norm deviation as time series. (Eq. (3) in [114])\n– Function name: plugin_normzeitreihe_abs.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Norm time series (NDTS): computes the deviation from a time series to a norm time series as a new time series. (Eq. (3.9) in [64])\n– Function name: plugin_normzeitreihe.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Normalized time series (NORM): normalizes a time series or a time series segment.\n– Function name: plugin_normalized_ts.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Type of normalization (defines the type of normalization for a time series)\n• Normalized to mean value (NORMMEAN): normalizes the time series to the mean value normalizes the time series to the mean value\n– Function name: plugin_mean_norm_ts.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none\n176 Appendix H. Plugins\n– Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• ROM of a window (FE-ROM): sets all values of the time series in the window to the range of the window. Here, a causal filter is used.\n– Function name: plugin_zr_fenster_rom.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Window length (determines the window length in sample points for the filtering with a sliding window.)\n• Relative ratio of two time series (RELRAT): computes the relative ratio of the first time series to the sum of both time series selected in the configuration window.\n– Function name: plugin_verhzr.m – Type: TS – Time series: 2 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Remove trend (DETREND): corrects the linear trend of a time series (MATLAB function detrend) corrects the linear trend of a time series (MATLAB function detrend)\n– Function name: plugin_detrend.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Detrend method (<html>Direction of model:<br>linear - removes a continuous, piecewise linear trend;<br>constant - removes just the mean value<html>)\n• Sorted time series (SORT TS): sorts all values of a time series in ascending order.\n– Function name: plugin_sort_zr.m\n177\n– Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• Square (SQR): computes the square of the amplitudes of a time series.\n– Function name: plugin_square.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 0\n• TS->DISCR TS MEAN (TERMQ): computes a discretized time series with a tunable number of terms for discretization.\n– Function name: plugin_discr_zr.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: yes – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 2\n∗ Number of terms (Number of terms for discretization) ∗ Type of membership function (defines the algorithm for the design of a membership\nfunction (see ”Control element: Data mining: Special methods - Number of linguistic terms”). ”Median” generates terms with approximately equal data point frequencies for all terms in the training data set. ”Equal distribution” generate terms with similar distances between the maximum values of the membership functions. ”Clustering” uses cluster prototypes as parameters. ”Fix” set the parameters of the membership functions to the values defined by ”Control element: Data mining: Special methods - Parameters MBF (fix)”. The choice ”with interpretability” rounds the found parameters using a method described in Literature: Mikut05 to improve the interpretability. Remark: All design methods set the random number generator of Matlab to a fix value. It improves the reproducibility of found membership functions. Nevertheless, all methods with random elements might be sensitive to the state of the random number generator (e.g. cluster algorithms with random start clusters). It can also influence other methods\n178 Appendix H. Plugins\nusing these membership functions, e.g. by different selected features in a decision tree (e.g. ”Data mining - Selection and evaluation of single features - Information theoretic measures”). )\n• TS->PC TS (PCTS1PCTS2): computes new time series by means of a Principal Component Analysis of time series (Transformation number of time series s_z -> s_d).\n– Function name: plugin_zrhkzr.m\n– Type: TS\n– Time series: Inf inputs, 2 outputs, Segments possible: none\n– Single features: 0 inputs, 0 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 3\n∗ Number of aggregated PCA features (Number s_d of principal components computed of a time series) ∗ Normalize standard deviations (defines the normalization for the variances of the sample\npoints) ∗ Transformation matrix (Use the same transformation matrix (e.g. PCA) for time series\nreduction of all data points)\n• Time shift (SHIFT): shift the time series K samples (positive values shift to the future).\n– Function name: plugin_shift_ts.m\n– Type: TS\n– Time series: 1 inputs, 1 outputs, Segments possible: none\n– Single features: 0 inputs, 0 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 1\n∗ Shift of a time series (positive values shift to the future, negative values to the past)\n• Value in region (REGION): set all values in a region of a time series to 1 and all other to 0.\n– Function name: plugin_timeseries_region.m\n– Type: TS\n– Time series: 1 inputs, 1 outputs, Segments possible: none\n– Single features: 0 inputs, 0 outputs\n– Images: 0 inputs, 0 outputs\n– Direct callback: none\n– Number of parameters: 2\n∗ Lower threshold (Lower threshold for the binarization of the time series) ∗ Upper threshold (Upper threshold for the binarization of the time series)\n179\n• Value larger than threshold (THRES): set all values of a time series above a threshold to 1, all other values to 0.\n– Function name: plugin_timeseries_threshold.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Threshold (Threshold for a binarization of time series)\n• Velocity (V): computes the first time derivation of a time series. The result is a single time series. Here, an acausal filter is used. see Eq. (3.2) in [64])\n– Function name: plugin_geschwindigkeit.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Time (defines the scaling (per sample point resp. per time using the sampling frequency from the GUI))\n• Velocity (causal) (V_kausal): computes the first time derivation of a time series. The result is a single time series. Here, a causal filter is used.\n– Function name: plugin_geschwindigkeit_kausal.m – Type: TS – Time series: 1 inputs, 1 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs – Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 1\n∗ Time (defines the scaling (per sample point resp. per time using the sampling frequency from the GUI))\n• Wavedec (db1, Low 4 db1, High 1db1, High 2db1, High 3db1, High 4):\n– Function name: plugin_wavedec.m – Type: TS – Time series: 1 inputs, 5 outputs, Segments possible: none – Single features: 0 inputs, 0 outputs\n180 Appendix H. Plugins\n– Images: 0 inputs, 0 outputs – Direct callback: none – Number of parameters: 3\n∗ Wavelet (defines the wavelet type.) ∗ Wavelets: number of levels (defines the number of levels.) ∗ Matlab wavelet decomposition (defines the used implementation.)"
    } ],
    "references" : [ {
      "title" : "KEEL Data-mining Software Tool: Data Set Repository, Integration of Algorithms and Experimental Analysis Framework",
      "author" : [ "J. ALCALÁ", "A. FERNÁNDEZ", "J. LUENGO", "J. DERRAC", "S. GARCÍA", "L. SÁNCHEZ", "F. HER- RERA" ],
      "venue" : "Journal of Multiple-Valued Logic and Soft Computing",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Konzept für Bildanalysen in Hochdurchsatz-Systemen am Beispiel des Zebrabärblings",
      "author" : [ "R. ALSHUT" ],
      "venue" : "Dissertation, Karlsruher Institut für Technologie (KIT),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Methods for Automated High-Throughput Toxicity Testing using Zebrafish Embryos",
      "author" : [ "R. ALSHUT", "J. LEGRADI", "U. LIEBEL", "L. YANG", "J. VAN WEZEL", "U. STRÄHLE", "R. MIKUT", "M. REISCHL" ],
      "venue" : "Lecture Notes in Artificial Intelligence",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Automatische Klassifikation von Bildzeitreihen für toxikologische Hochdurchsatz- Untersuchungen",
      "author" : [ "R. ALSHUT", "R. MIKUT", "J. LEGRADI", "U. LIEBEL", "U. STRÄHLE", "G. BRETTHAUER", "M. REIS- CHL" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "The Irises of the Gaspé Peninsula",
      "author" : [ "E. ANDERSON" ],
      "venue" : "Bulletin of the American Iris Society",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1935
    }, {
      "title" : "Application of Data Mining Methods for Power Forecast of Wind Power Plants",
      "author" : [ "A. ARNOLDT", "S. KÖNIG", "R. MIKUT", "P. BRETSCHNEIDER" ],
      "venue" : "Proc., 9th International Workshop on Large-scale Integration of Wind Power and Transmission Networks for Offshore Wind Farms, Quebec,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "XPIWIT - An XML Pipeline Wrapper for the Insight Toolkit",
      "author" : [ "A. BARTSCHAT", "E. HÜBNER", "M. REISCHL", "R. MIKUT", "J. STEGMAIER" ],
      "venue" : "Bioinformatics",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Neues Konzept zur Bewegungsanalyse und -synthese für Humanoide Roboter basierend auf Vorbildern aus der Biologie",
      "author" : [ "C. BAUER" ],
      "venue" : "Dissertation, Karlsruher Institut für Technologie, KIT Scientific Publishing,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Hardware Design and Mathematical Modeling for an Artificial Pneumatic Spine for a Biped Humanoid Robot",
      "author" : [ "C. BAUER", "M. ENGELMANN", "I. GAISER", "T. STEIN", "A. FISCHER", "R. MIKUT", "S. SCHULZ" ],
      "venue" : "Proc., German Conference on Robotics, S. 7–11,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Human-like Reflexes for Robotic Manipulation using Leaky Integrate-and-Fire Neurons",
      "author" : [ "C. BAUER", "G. MILIGHETTI", "W. YAN", "R. MIKUT" ],
      "venue" : "Proc., IEEE/RSJ International Conference on Intelligent Robots and Systems",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Konzept für einen biologisch inspirierten, semi-passiven pneumatisch angetriebenen zweibeinigen Prothesen- Roboter-Hybrid",
      "author" : [ "C. BAUER", "M. MORS", "A. FISCHER", "T. STEIN", "R. MIKUT", "S. SCHULZ" ],
      "venue" : "at-Automatisierungstechnik",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Ein Beitrag zum automatischen Entwurf von Fuzzy-Entscheidungssystemen bei unvollständiger Information",
      "author" : [ "S. BECK" ],
      "venue" : "Dissertation, Universität Karlsruhe, Universitätsverlag Karlsruhe,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    }, {
      "title" : "A Cost-Sensitive Learning Algorithm for Fuzzy Rule-Based Classifiers",
      "author" : [ "S. BECK", "R. MIKUT", "J. JÄKEL" ],
      "venue" : "Mathware and Soft Computing",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2004
    }, {
      "title" : "KNIME: The Konstanz Information Miner",
      "author" : [ "M.R. BERTHOLD", "N. CEBRON", "F. DILL", "T.R. GABRIEL", "T. KÖTTER", "T. MEINL", "P. OHL", "C. SIEB", "K. THIEL", "B. WISWEDEL" ],
      "venue" : "Data Analysis, Machine Learning and Applications,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Data Mining with Graphical Models",
      "author" : [ "C. BORGELT" ],
      "venue" : "Dissertation, O.-v.-Guericke Universität Magdeburg,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2000
    }, {
      "title" : "Classification and Regression Trees",
      "author" : [ "L. BREIMAN", "J.H. FRIEDMAN", "R.A. OLSHEN", "C.J. STONE" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1984
    }, {
      "title" : "Neue Methodik zur Modellierung und zum Entwurf keramischer Aktorelemente",
      "author" : [ "B.W. BRÜCKNER" ],
      "venue" : "Dissertation, Karlsruher Institut für Technologie (KIT),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "A Tutorial on Support Vector Machines for Pattern Recognition. Knowledge Discovery and Data Mining",
      "author" : [ "C. BURGES" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1998
    }, {
      "title" : "Analyse von Zeitreihen in der Medizin: Informationsgehalt, Klassifikation und Unsicherheit",
      "author" : [ "O. BURMEISTER" ],
      "venue" : "Proc., 16. Workshop Computational Intelligence,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Zeitvariante Klassifikatoren zur Analyse und Interpretation multimodaler Biosignale und deren Anwendung in der Prothetik und Rehabilitation",
      "author" : [ "O. BURMEISTER" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "Data-Mining-Analysen mit der MATLAB-Toolbox Gait-CAD",
      "author" : [ "O. BURMEISTER", "M. REISCHL", "G. BRETTHAUER", "R. MIKUT" ],
      "venue" : "at-Automatisierungstechnik",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Zeitvariante Klassifikatoren zur Steuerung von Brain Machine Interfaces und Neuroprothesen",
      "author" : [ "O. BURMEISTER", "M. REISCHL", "L. GRÖLL", "R. MIKUT" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2006
    }, {
      "title" : "A Linear Programming Approach to Novelty Detection",
      "author" : [ "C. CAMPBELL", "K.P. BENNETT" ],
      "venue" : "Proc., Neural Information Processing Systems Conference,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2000
    }, {
      "title" : "SVM and Kernel Methods Matlab Toolbox",
      "author" : [ "S. CANU", "Y. GRANDVALET", "A. RAKOTOMAMONJY" ],
      "venue" : "Perception Systèmes et Information, INSA de Rouen, Rouen, France,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2003
    }, {
      "title" : "Hg.): Medical Data Mining and Knowledge Discovery, Bd. 60 von Studies in Fuzziness and Soft Computing",
      "author" : [ "K. CIOS" ],
      "venue" : "Heidelberg: Physica,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2001
    }, {
      "title" : "Nearest Neighbor Pattern Classification",
      "author" : [ "T. COVER", "P. HART" ],
      "venue" : "IEEE Transactions on Information Theory",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1967
    }, {
      "title" : "Robust Fuzzy Clustering of Relational Data",
      "author" : [ "R. DAVÉ", "S. SEN" ],
      "venue" : "IEEE Transactions on Fuzzy Systems",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2002
    }, {
      "title" : "DaMoQ: An Open Source MATLAB Toolbox for Data and Model Quality Assessment",
      "author" : [ "W. DONEIT", "R. MIKUT", "L. GRÖLL", "T. PYCHYNSKI", "M. REISCHL" ],
      "venue" : "at-Automatisierungstechnik",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2017
    }, {
      "title" : "Cross-Validation and the Bootstrap: Estimating the Error Rate of a Prediction Rule",
      "author" : [ "B. EFRON", "R. TIBSHIRANI" ],
      "venue" : "Techn. Ber. TR-477,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1995
    }, {
      "title" : "From Data Mining to Knowledge Discovery in Databases",
      "author" : [ "U. FAYYAD", "G. PIATETSKY-SHAPIRO", "P. SMYTH" ],
      "venue" : "AI Magazine",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1996
    }, {
      "title" : "Interpretability of Linguistic fuzzy Rule-based Systems: An Overview of Interpretability Measures",
      "author" : [ "M.J. GACTO", "R. ALCALÁ", "F. HERRERA" ],
      "venue" : "Information Sciences",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2011
    }, {
      "title" : "Automated High Throughput Mapping of Promoter- Enhancer Interactions in Zebrafish Embryos",
      "author" : [ "J. GEHRIG", "M. REISCHL", "E. KALMAR", "M. FERG", "Y. HADZHIEV", "A. ZAUCKER", "C. SONG", "S. SCHINDLER", "U. LIEBEL", "F. MÜLLER" ],
      "venue" : "Nature Methods",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2009
    }, {
      "title" : "Robust Adaptive Fault Detection using Global State Information and Application to Mobile Working Machines",
      "author" : [ "P. GERLAND", "D. GROSS", "H. SCHULTE", "A. KROLL" ],
      "venue" : "In: Conference on Control and Fault-Tolerant Systems (SysTol),",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2010
    }, {
      "title" : "Probability-based Global State Detection of Complex Technical Systems and Application to Mobile Working Machines",
      "author" : [ "P. GERLAND", "H. SCHULTE", "A. KROLL" ],
      "venue" : "Control Conference (ECC),",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2009
    }, {
      "title" : "Nearest-Neighbor Based Non-Parametric Probabilistic Forecasting with Applications in Photovoltaic Systems",
      "author" : [ "J.Á. GONZÁLEZ ORDIANO", "W. DONEIT", "S. WACZOWICZ", "L. GRÖLL", "R. MIKUT", "V. HAGEN- MEYER" ],
      "venue" : "Proc., 26. Workshop Computational Intelligence,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2016
    }, {
      "title" : "Photovoltaic Power Forecasting using Simple Data-driven Models without Weather Data. Computer Science - Research and Development",
      "author" : [ "J.Á. GONZÁLEZ ORDIANO", "S. WACZOWICZ", "M. REISCHL", "R. MIKUT", "V. HAGENMEYER" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2016
    }, {
      "title" : "Ein neues Konzept zur Diagnose elektrochemischer Sensoren am Beispiel von pH- Glaselektroden. Dissertation, Karlsruher Institut für Technologie (KIT), KIT Scientific Publishing",
      "author" : [ "M. GRUBE" ],
      "venue" : "Vorbereitung,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2011
    }, {
      "title" : "Small Angle X-ray Scattering as a High-throughput Method to Classify Antimicrobial Modes of Action",
      "author" : [ "A. VON GUNDLACH", "V. GARAMUS", "T. GORNIAK", "H. DAVIES", "M. REISCHL", "R. MIKUT", "K. HILPERT", "A. ROSENHAHN" ],
      "venue" : "Biochimica et Biophysica Acta (BBA)-Biomembranes",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2016
    }, {
      "title" : "Scalable Machine-Learning Algorithms for Big Data Analytics: A Comprehensive Review. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",
      "author" : [ "P. GUPTA", "A. SHARMA", "R. JINDAL" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2016
    }, {
      "title" : "The WEKA Data Mining Software: An Update",
      "author" : [ "M. HALL", "E. FRANK", "G. HOLMES", "B. PFAHRINGER", "P. REUTEMANN", "I.H. WITTEN" ],
      "venue" : "Association for Computing Machinery SIGKDD Explorations Newsletter",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2009
    }, {
      "title" : "Neural Networks: A Comprehensive Foundation",
      "author" : [ "S. HAYKIN" ],
      "venue" : "Upper Saddle River, NJ: Prentice Hall,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 1994
    }, {
      "title" : "The UCI KDD Archive",
      "author" : [ "S. HETTICH", "S.D. BAY" ],
      "venue" : "University of California, Department of Information and Computer Science. http://kdd.ics.uci.edu,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 1999
    }, {
      "title" : "In vivo Assessment of Tissue Compatibility and Functionality of a Polyimide Cuff Electrode for Recording Afferent Peripheral Nerve Signals",
      "author" : [ "B. HIEBL", "S. BOG", "R. MIKUT", "C. BAUER", "O. GEMEINHARDT", "F. JUNG", "T. KRÜGER" ],
      "venue" : "Applied Cardiopulmonary Pathophysiology",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2010
    }, {
      "title" : "Gene Responses in the Central Nervous System of Zebrafish Embryos Exposed to the Neurotoxicant Methyl Mercury",
      "author" : [ "N.Y. HO", "L. YANG", "J. LEGRADI", "O. ARMANT", "M. TAKAMIYA", "S. RASTEGAR", "U. STRÄHLE" ],
      "venue" : "Environmental Science & Technology",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2013
    }, {
      "title" : "Fuzzy Cluster Analysis",
      "author" : [ "F. HÖPPNER", "F. KLAWONN", "R. KRUSE" ],
      "venue" : "New York: John Wiley,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 1999
    }, {
      "title" : "Survey on Independent Component Analysis",
      "author" : [ "A. HYVÄRINEN" ],
      "venue" : "Neural Computing Surveys",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 1999
    }, {
      "title" : "Data Clustering: 50 Years Beyond K-means",
      "author" : [ "A.K. JAIN" ],
      "venue" : "Pattern Recognition Letters",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2010
    }, {
      "title" : "Statistical Pattern Recognition: A Review",
      "author" : [ "A.K. JAIN", "R.P.W. DUIN", "J. MAO" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2000
    }, {
      "title" : "Tree-Oriented Hypothesis Generation for Interpretable Fuzzy Rules",
      "author" : [ "J. JÄKEL", "L. GRÖLL", "R. MIKUT" ],
      "venue" : "Proc., 7th European Congress on Intelligent Techniques and Soft Computing",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 1999
    }, {
      "title" : "Datenbasierte Analyse und Modellbildung zur Abschätzung spezifischer Gefahren des Klimawandels für Straßen-Methodik und Szenarien",
      "author" : [ "S. KELLER" ],
      "venue" : null,
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2015
    }, {
      "title" : "Feedback-Driven Design of Normalization Techniques for Biological Images Using Fuzzy Formulation of a Priori Knowledge",
      "author" : [ "A. KHAN", "M. REISCHL", "B. SCHWEITZER", "C. WEISS", "R. MIKUT" ],
      "venue" : "Studies in Computational Intelligence",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2013
    }, {
      "title" : "A Benchmark Data Set to Evaluate the Illumination Robustness of Image Processing Algorithms for Object Segmentation and Classification",
      "author" : [ "A.U.M. KHAN", "R. MIKUT", "M. REISCHL" ],
      "venue" : "PLoS One",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2015
    }, {
      "title" : "A New Feedback-Based Method for Parameter Adaptation in Image Processing Routines",
      "author" : [ "A.U.M. KHAN", "R. MIKUT", "M. REISCHL" ],
      "venue" : "PloS one",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2016
    }, {
      "title" : "Fuzzy Control methodenorientiert",
      "author" : [ "H. KIENDL" ],
      "venue" : "München: Oldenbourg,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 1997
    }, {
      "title" : "Optimization of Oncocin for Antibacterial Activity using a SPOT Synthesis Approach: Extending the Pathogen Spectrum to Staphylococcus aureus",
      "author" : [ "D. KNAPPE", "S. RUDEN", "S. LANGANKE", "T. TIKKOO", "J. RITZER", "R. MIKUT", "L.L. MARTIN", "R. HOFFMANN", "K. HILPERT" ],
      "venue" : "Amino Acids",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2016
    }, {
      "title" : "Identification of Non-visual Photomotor Response Cells in the Vertebrate Hindbrain",
      "author" : [ "D. KOKEL", "T. DUNN", "M. AHRENS", "R. ALSHUT", "C.Y. CHEUNG", "L. SAINT-AMANT", "G. BRUNI", "R. MATEUS", "T. VAN HAM", "T. SHIRAKI", "Y. FUKADA", "D. KOJIMA", "J.- R. YEH", "R. MIKUT", "J. VON LINTIG", "F. ENGERT", "R. PETERSON" ],
      "venue" : "The Journal of Neuroscience",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2013
    }, {
      "title" : "Tuned Data Mining: A Benchmark Study on Different Tuners",
      "author" : [ "W. KONEN", "P. KOCH", "O. FLASCH", "T. BARTZ-BEIELSTEIN", "M. FRIESE", "B. NAUJOKS" ],
      "venue" : "Proc., 13th Annual Conference on Genetic and Evolutionary",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2011
    }, {
      "title" : "The Great Energy Predictor Shootout",
      "author" : [ "J.F. KREIDER", "J.S. HABERL" ],
      "venue" : "Proc., ASHRAE Meeting,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 1993
    }, {
      "title" : "Computational Intelligence",
      "author" : [ "A. KROLL" ],
      "venue" : "De Gruyter Oldenbourg,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2016
    }, {
      "title" : "Foundations of Fuzzy Systems",
      "author" : [ "R. KRUSE", "F.K.J. GEBHARDT" ],
      "venue" : "New York: John Wiley,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 1994
    }, {
      "title" : "A Survey of Open Source Tools for Machine Learning with Big Data in the Hadoop Ecosystem",
      "author" : [ "S. LANDSET", "T.M. KHOSHGOFTAAR", "A.N. RICHTER", "T. HASANIN" ],
      "venue" : "Journal of Big Data",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2015
    }, {
      "title" : "Evaluation of Biometric Signal Characteristics for Movement Classification. Diplomarbeit, Universität Bukarest",
      "author" : [ "M. LIPOVEI" ],
      "venue" : "Forschungszentrum Karlsruhe,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2004
    }, {
      "title" : "Konzept für eine modellgestützte Diagnostik mittels Data Mining am Beispiel der Bewegungsanalyse",
      "author" : [ "T. LOOSE" ],
      "venue" : null,
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2004
    }, {
      "title" : "STUCKY, K.-U.; KUEHNAPFEL, U.: First Evaluation Results Using the New Electrical Data Recorder for Power Grid Analysis",
      "author" : [ "H. MAASS", "H. CAKMAK", "W. SUESS", "A. QUINTE", "W. JAKOB" ],
      "venue" : "IEEE Transactions on Instrumentation and Measurement",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2013
    }, {
      "title" : "Data Processing of High Rate Low Voltage Distribution Grid Recordings for Smart Grid Monitoring and Analysis",
      "author" : [ "H. MAASS", "H.K. CAKMAK", "F. BACH", "R. MIKUT", "A. HARRABI", "W. SÜSS", "W. JAKOB", "K.-U. STUCKY", "U.G. KÜHNAPFEL", "V. HAGENMEYER" ],
      "venue" : "EURASIP Journal on Advances in Signal Processing",
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 2015
    }, {
      "title" : "An Automated and High-throughput Photomotor Response Platform for Chemical Screens",
      "author" : [ "D. MARCATO", "R. ALSHUT", "H. BREITWIESER", "R. MIKUT", "U. STRÄHLE", "C. PYLATIUK", "R. PERAVALI" ],
      "venue" : "Proc., 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBS),",
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 2015
    }, {
      "title" : "MLlib: Machine Learning in Apache Spark",
      "author" : [ "X. MENG", "J. BRADLEY", "B. YUVAZ", "E. SPARKS", "S. VENKATARAMAN", "D. LIU", "J. FREEMAN", "D. TSAI", "M. AMDE", "S OWEN" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "68",
      "shortCiteRegEx" : "68",
      "year" : 2016
    }, {
      "title" : "Machine Learning, Neural and Statistical Classification",
      "author" : [ "D. MICHIE", "D. SPIEGELHALTER", "C. TAYLOR" ],
      "venue" : null,
      "citeRegEx" : "69",
      "shortCiteRegEx" : "69",
      "year" : 1994
    }, {
      "title" : "Data Mining in der Medizin und Medizintechnik",
      "author" : [ "R. MIKUT" ],
      "venue" : "Universitätsverlag Karlsruhe,",
      "citeRegEx" : "70",
      "shortCiteRegEx" : "70",
      "year" : 2008
    }, {
      "title" : "Computer-based Analysis, Visualization, and Interpretation of Antimicrobial Peptide Activities",
      "author" : [ "R. MIKUT" ],
      "venue" : "Methods in Molecular Biology",
      "citeRegEx" : "71",
      "shortCiteRegEx" : "71",
      "year" : 2010
    }, {
      "title" : "The Open Source Matlab Toolbox Gait-CAD and its Application to Bioelectric Signal Processing",
      "author" : [ "R. MIKUT", "O. BURMEISTER", "S. BRAUN", "M. REISCHL" ],
      "venue" : "Proc., DGBMT-Workshop Biosignalverarbeitung,",
      "citeRegEx" : "72",
      "shortCiteRegEx" : "72",
      "year" : 2008
    }, {
      "title" : "Takagi-Sugeno-Kang Fuzzy Classifiers for a Special Class of Time-Varying Systems",
      "author" : [ "R. MIKUT", "O. BURMEISTER", "L. GRÖLL", "M. REISCHL" ],
      "venue" : "IEEE Transactions on Fuzzy Systems",
      "citeRegEx" : "73",
      "shortCiteRegEx" : "73",
      "year" : 2008
    }, {
      "title" : "Interpretable Features for the Activity Prediction of Short Antimicrobial Peptides",
      "author" : [ "R. MIKUT", "K. HILPERT" ],
      "venue" : "Using Fuzzy Logic. International Journal of Peptide Research and Therapeutics",
      "citeRegEx" : "74",
      "shortCiteRegEx" : "74",
      "year" : 2009
    }, {
      "title" : "Inference Methods for Partially Redundant Rule Bases. In: Fuzzy Control: Theory and Practice (HAMPEL, R.; WAGENKNECHT, M.; CHAKER, N., Hg.)",
      "author" : [ "R. MIKUT", "J. JÄKEL", "L. GRÖLL" ],
      "venue" : "Advances in Soft Computing,",
      "citeRegEx" : "75",
      "shortCiteRegEx" : "75",
      "year" : 2000
    }, {
      "title" : "Interpretability Issues in Data-Based Learning of Fuzzy Systems",
      "author" : [ "R. MIKUT", "J. JÄKEL", "L. GRÖLL" ],
      "venue" : "Fuzzy Sets and Systems",
      "citeRegEx" : "76",
      "shortCiteRegEx" : "76",
      "year" : 2005
    }, {
      "title" : "Data Mining Tools. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",
      "author" : [ "R. MIKUT", "M. REISCHL" ],
      "venue" : null,
      "citeRegEx" : "77",
      "shortCiteRegEx" : "77",
      "year" : 2011
    }, {
      "title" : "Data Mining in Medical Time Series",
      "author" : [ "R. MIKUT", "M. REISCHL", "O. BURMEISTER", "T. LOOSE" ],
      "venue" : "Biomedizinische Technik",
      "citeRegEx" : "78",
      "shortCiteRegEx" : "78",
      "year" : 2006
    }, {
      "title" : "Improving Short Antimicrobial Peptides Despite Elusive Rules for Activity",
      "author" : [ "R. MIKUT", "S. RUDEN", "M. REISCHL", "F. BREITLING", "R. VOLKMER", "K. HILPERT" ],
      "venue" : "Biochimica et Biophysica Acta (BBA)-Biomembranes",
      "citeRegEx" : "79",
      "shortCiteRegEx" : "79",
      "year" : 2016
    }, {
      "title" : "SciXMiner: A MATLAB Toolbox for Data Mining of Multidimensional Data",
      "author" : [ "R. MIKUT", "J. STEGMAIER", "A. BARTSCHAT", "W. DONEIT", "J. ÁNGEL GONZÁLEZ ORDIANO", "N. PETER", "B. SCHOTT", "S. WACZOWICZ", "M. REISCHL" ],
      "venue" : null,
      "citeRegEx" : "80",
      "shortCiteRegEx" : "80",
      "year" : 2017
    }, {
      "title" : "Neues Konzept für ein modulares Robotersystem zur automatischen Untersuchung von Zebrabärblingen in Hochdurchsatzverfahren",
      "author" : [ "A. PFRIEM" ],
      "venue" : null,
      "citeRegEx" : "81",
      "shortCiteRegEx" : "81",
      "year" : 2016
    }, {
      "title" : "Modelling the Labyrinth Seal Discharge Coefficient Using Data Mining Methods",
      "author" : [ "T. PYCHYNSKI", "G. BLESINGER", "R. MIKUT", "K. DULLENKOPF", "H.-J. BAUER" ],
      "venue" : "Proc., ASME TURBO EXPO; Glasgow,",
      "citeRegEx" : "82",
      "shortCiteRegEx" : "82",
      "year" : 2010
    }, {
      "title" : "Comparison of Surface EMG Monitoring Electrodes for Long-term Use in Rehabilitation Device Control",
      "author" : [ "C. PYLATIUK", "M. MÜLLER-RIEDERER", "A. KARGOV", "S. SCHULZ", "O. SCHILL", "M. REISCHL", "G. BRETTHAUER" ],
      "venue" : "Proc., International Conference on Rehabilitation Robotics,",
      "citeRegEx" : "83",
      "shortCiteRegEx" : "83",
      "year" : 2009
    }, {
      "title" : "Automatic Zebrafish Heartbeat Detection and Analysis for Zebrafish Embryos",
      "author" : [ "C. PYLATIUK", "D. SANCHEZ", "R. MIKUT", "R. ALSHUT", "M. REISCHL", "S. HIRTH", "W. ROT- TBAUER", "S. JUST" ],
      "venue" : "Zebrafish",
      "citeRegEx" : "84",
      "shortCiteRegEx" : "84",
      "year" : 2014
    }, {
      "title" : "Programs for Machine Learning",
      "author" : [ "QUINLAN", "J.R.: C" ],
      "venue" : null,
      "citeRegEx" : "85",
      "shortCiteRegEx" : "85",
      "year" : 1993
    }, {
      "title" : "Targeting Mycobacterium tuberculosis and other Microbial Pathogens using Improved Synthetic Antibacterial Peptides",
      "author" : [ "S. RAMON-GARCIA", "R. MIKUT", "C. NG", "S. RUDEN", "R. VOLKMER", "M. REISCHL", "K. HILPERT", "C.J. THOMPSON" ],
      "venue" : "Antimicrobial Agents and Chemotherapy",
      "citeRegEx" : "86",
      "shortCiteRegEx" : "86",
      "year" : 2013
    }, {
      "title" : "Ein Verfahren zum automatischen Entwurf von Mensch-Maschine-Schnittstellen am Beispiel myoelektrischer Handprothesen",
      "author" : [ "M. REISCHL" ],
      "venue" : null,
      "citeRegEx" : "87",
      "shortCiteRegEx" : "87",
      "year" : 2006
    }, {
      "title" : "Optimized Classification of Multiclass Problems Applied to EMG-Control of Hand Prostheses",
      "author" : [ "M. REISCHL", "L. GRÖLL", "R. MIKUT" ],
      "venue" : "Proc., IEEE International Joint Conference on Neural Networks,",
      "citeRegEx" : "88",
      "shortCiteRegEx" : "88",
      "year" : 2004
    }, {
      "title" : "Evaluation of Data Mining Approaches for the Control of Multifunctional Arm Prostheses",
      "author" : [ "M. REISCHL", "L. GRÖLL", "R. MIKUT" ],
      "venue" : "Integrated Computer-Aided Engineering",
      "citeRegEx" : "89",
      "shortCiteRegEx" : "89",
      "year" : 2011
    }, {
      "title" : "Steuerungs- und Signalverarbeitungskonzepte für eine multifunktionale Handprothese",
      "author" : [ "M. REISCHL", "R. MIKUT", "C. PYLATIUK", "S. SCHULZ", "S. BECK", "G. BRETTHAUER" ],
      "venue" : "at- Automatisierungstechnik",
      "citeRegEx" : "90",
      "shortCiteRegEx" : "90",
      "year" : 2002
    }, {
      "title" : "Einfluss von Trainingseffekten auf die Parameteradaption für Mensch-Maschine- Schnittstellen in der Medizintechnik",
      "author" : [ "M. REISCHL", "M.R. TUGA", "L. MEISTER", "E. ALBERG", "W. DONEIT", "D. LIEBETANZ", "R. RUPP", "R. MIKUT" ],
      "venue" : "at-Automatisierungstechnik",
      "citeRegEx" : "91",
      "shortCiteRegEx" : "91",
      "year" : 2016
    }, {
      "title" : "Asphalt Image Miner: A Tool for Automatic Quantification of Grains",
      "author" : [ "M. REISCHL", "A. WITTENBERG", "C. KARCHER", "R. MIKUT" ],
      "venue" : "Asphalt Samples. at-Automatisierungstechnik",
      "citeRegEx" : "92",
      "shortCiteRegEx" : "92",
      "year" : 2014
    }, {
      "title" : "Data Analytics: Models and Algorithms for Intelligent Data Analysis",
      "author" : [ "T.A. RUNKLER" ],
      "venue" : null,
      "citeRegEx" : "93",
      "shortCiteRegEx" : "93",
      "year" : 2016
    }, {
      "title" : "HeiDATAProVIT-Heidelberg Data Archiving, Tag Assembling, Processing and Visualization Tool",
      "author" : [ "M. SCHABLOWSKI", "J. SCHWEIDLER", "R. RUPP" ],
      "venue" : "Computer Methods and Programs in Biomedicine",
      "citeRegEx" : "94",
      "shortCiteRegEx" : "94",
      "year" : 2004
    }, {
      "title" : "Konzept zur Analyse der Lokomotion auf dem Laufband bei inkompletter Querschnittlähmung mit Verfahren der nichtlinearen Dynamik",
      "author" : [ "M. SCHABLOWSKI-TRAUTMANN" ],
      "venue" : null,
      "citeRegEx" : "95",
      "shortCiteRegEx" : "95",
      "year" : 2006
    }, {
      "title" : "Konzept zur automatisierten Anpassung der neuronalen Schnittstellen bei nichtinvasiven Neuroprothesen. Dissertation, Karlsruher Institut für Technologie",
      "author" : [ "O. SCHILL" ],
      "venue" : "KIT Scientific Publishing,",
      "citeRegEx" : "96",
      "shortCiteRegEx" : "96",
      "year" : 2014
    }, {
      "title" : "Automatic Adaptation of a Self-Adhesive Multi-Electrode Array for Active Wrist Joint Stabilization in Tetraplegics",
      "author" : [ "O. SCHILL", "R. RUPP", "C. PYLATIUK", "S. SCHULZ", "M. REISCHL" ],
      "venue" : "Proc., IEEE Toronto International Conference–Science and Technology for Humanity,",
      "citeRegEx" : "97",
      "shortCiteRegEx" : "97",
      "year" : 2009
    }, {
      "title" : "Support Vector Learning",
      "author" : [ "B. SCHÖLKOPF" ],
      "venue" : "München: Oldenbourg,",
      "citeRegEx" : "98",
      "shortCiteRegEx" : "98",
      "year" : 1997
    }, {
      "title" : "Robust Individual Circadian Parameter Estimation for Biosignal-based Personalisation of Cancer Chronotherapy",
      "author" : [ "B. SCHOTT", "J. STEGMAIER", "A. ARBAUD", "M. REISCHL", "R. MIKUT", "F. LÉVI" ],
      "venue" : "Proc., Workshop Biosignal Processing, Berlin, arXiv preprint arXiv:1604.02909,",
      "citeRegEx" : "99",
      "shortCiteRegEx" : "99",
      "year" : 2016
    }, {
      "title" : "Zebrafish Biosensor for Toxicant Induced Muscle Hyperactivity",
      "author" : [ "M. SHAHID", "M. TAKAMIYA", "J. STEGMAIER", "V. MIDDEL", "M. GRADL", "N. KLÜVER", "R. MIKUT", "T. DICKMEIS", "S. SCHOLZ", "S. RASTEGAR", "L. YANG", "U. STRÄHLE" ],
      "venue" : "Scientific Reports",
      "citeRegEx" : "100",
      "shortCiteRegEx" : "100",
      "year" : 2016
    }, {
      "title" : "New Methods to Improve Large-Scale Microscopy Image Analysis with Prior Knowledge and Uncertainty",
      "author" : [ "J. STEGMAIER" ],
      "venue" : "Dissertation, Karlsruhe Institute of Technology,",
      "citeRegEx" : "101",
      "shortCiteRegEx" : "101",
      "year" : 2016
    }, {
      "title" : "Information Fusion of Image Analysis, Video Object Tracking, and Data Mining of Biological Images using the Open Source MATLAB Toolbox Gait-CAD",
      "author" : [ "J. STEGMAIER", "R. ALSHUT", "M. REISCHL", "R. MIKUT" ],
      "venue" : "Biomedizinische Technik (Biomedical Engineering)",
      "citeRegEx" : "102",
      "shortCiteRegEx" : "102",
      "year" : 2012
    }, {
      "title" : "Real-Time Three-Dimensional Cell Segmentation in Large-Scale Microscopy Data of Developing Embryos",
      "author" : [ "J. STEGMAIER", "F. AMAT", "W.B. LEMON", "K. MCDOLE", "Y. WAN", "G. TEODORO", "R. MIKUT", "P.J. KELLER" ],
      "venue" : "Developmental Cell",
      "citeRegEx" : "103",
      "shortCiteRegEx" : "103",
      "year" : 2016
    }, {
      "title" : "Automation Strategies for Large-Scale 3D",
      "author" : [ "J. STEGMAIER", "B. SCHOTT", "E. HÜBNER", "M. TRAUB", "M. SHAHID", "M. TAKAMIYA", "A. KO- BITSKI", "V. HARTMANN", "R. STOTZKA", "J. VAN WEZEL", "A. STREIT", "G.U. NIENHAUS", "U. STRÄHLE", "M. REISCHL", "R. MIKUT" ],
      "venue" : "Image Analysis. at-Automatisierungstechnik",
      "citeRegEx" : "104",
      "shortCiteRegEx" : "104",
      "year" : 2016
    }, {
      "title" : "Automated Prior Knowledge-Based Quantification of Neuronal Patterns in the Spinal Cord of Zebrafish",
      "author" : [ "J. STEGMAIER", "M. SHAHID", "M. TAKAMIYA", "L. YANG", "S. RASTEGAR", "M. REISCHL", "U. STRÄHLE", "R. MIKUT" ],
      "venue" : "Bioinformatics",
      "citeRegEx" : "105",
      "shortCiteRegEx" : "105",
      "year" : 2014
    }, {
      "title" : "Independent Component Analysis: An Introduction",
      "author" : [ "J. STONE" ],
      "venue" : "Trends in Cognitive Sciences",
      "citeRegEx" : "106",
      "shortCiteRegEx" : "106",
      "year" : 2002
    }, {
      "title" : "Multivariate Analysis",
      "author" : [ "M.M. TATSUOKA" ],
      "venue" : "New York: Macmillan,",
      "citeRegEx" : "107",
      "shortCiteRegEx" : "107",
      "year" : 1988
    }, {
      "title" : "RAMOS-DELGADO; GARCIA-SANCHEZ, R.: Fisherextest: Fisher’s Exact Probability Test",
      "author" : [ "A. TRUJILLO-ORTIZ", "R. HERNANDEZ-WALLS", "A. CASTRO-PEREZ", "N.L. RODRIGUEZ-CARDOZO" ],
      "venue" : "A MATLAB file. [WWW document]",
      "citeRegEx" : "108",
      "shortCiteRegEx" : "108",
      "year" : 2004
    }, {
      "title" : "SOM Toolbox for MAT- LAB",
      "author" : [ "J. VESANTO", "J. HIMBERG", "E. ALHONIEMI", "J. PARHANKANGAS" ],
      "venue" : "Techn. Ber., Helsinki University of Technology,",
      "citeRegEx" : "109",
      "shortCiteRegEx" : "109",
      "year" : 2000
    }, {
      "title" : "Data mining to analyse the effects of price signals on household electricity customers",
      "author" : [ "S. WACZOWICZ", "S. KLAIBER", "P. BRETSCHNEIDER", "I. KONOTOP", "D. WESTERMANN", "M. REISCHL", "R. MIKUT" ],
      "venue" : "at-Automatisierungstechnik",
      "citeRegEx" : "110",
      "shortCiteRegEx" : "110",
      "year" : 2014
    }, {
      "title" : "Virtual Storages as Theoretically Motivated Demand Response Models for Enhanced Smart Grid Operations",
      "author" : [ "S. WACZOWICZ", "M. REISCHL", "S. KLAIBER", "P. BRETSCHNEIDER", "I. KONOTOP", "D. WESTER- MANN", "V. HAGENMEYER", "R. MIKUT" ],
      "venue" : "Energy Technology",
      "citeRegEx" : "111",
      "shortCiteRegEx" : "111",
      "year" : 2016
    }, {
      "title" : "Data Mining with Rattle and R: The Art of Excavating Data for Knowledge Discovery",
      "author" : [ "G. WILLIAMS" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "112",
      "shortCiteRegEx" : "112",
      "year" : 2011
    }, {
      "title" : "Gait analysis may help to distinguish hereditary spastic paraplegia from cerebral palsy",
      "author" : [ "S. WOLF", "F. BRAATZ", "D. METAXIOTIS", "P. ARMBRUST", "T. DREHER", "L. DÖDERLEIN", "R. MIKUT" ],
      "venue" : "Gait & Posture",
      "citeRegEx" : "113",
      "shortCiteRegEx" : "113",
      "year" : 2011
    }, {
      "title" : "Automated Feature Assessment in Instrumented Gait Analysis",
      "author" : [ "S. WOLF", "T. LOOSE", "M. SCHABLOWSKI", "L. DÖDERLEIN", "R. RUPP", "H.J. GERNER", "G. BRET- THAUER", "R. MIKUT" ],
      "venue" : "Gait & Posture",
      "citeRegEx" : "114",
      "shortCiteRegEx" : "114",
      "year" : 2006
    }, {
      "title" : "Which Functional Impairments are the Main Contributors to Pelvic Anterior Tilt during Gait in Individuals with Cerebral Palsy",
      "author" : [ "S.I. WOLF", "R. MIKUT", "A. KRANZL", "T. DREHER" ],
      "venue" : "Gait & Posture",
      "citeRegEx" : "115",
      "shortCiteRegEx" : "115",
      "year" : 2014
    }, {
      "title" : "Interaction of Blood Components with Cathelicidins and their Modified Versions",
      "author" : [ "K. YU", "B.F. LAI", "J. GANI", "R. MIKUT", "K. HILPERT", "J.N. KIZHAKKEDATHU" ],
      "venue" : "Biomaterials",
      "citeRegEx" : "116",
      "shortCiteRegEx" : "116",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 62,
      "context" : "The toolbox bases on earlier internal versions of Gait-CAD [64, 72].",
      "startOffset" : 59,
      "endOffset" : 67
    }, {
      "referenceID" : 70,
      "context" : "The toolbox bases on earlier internal versions of Gait-CAD [64, 72].",
      "startOffset" : 59,
      "endOffset" : 67
    }, {
      "referenceID" : 78,
      "context" : "Please refer to [80] if you use SciXMiner for your scientific work.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 100,
      "context" : "• analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], • object tracking [102, 101], • tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), • feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], • image processing and feature extraction for segmentation of grains in asphalt samples [92], and • extended measures and visualization to evaluate the quality of data and regression models [28].",
      "startOffset" : 32,
      "endOffset" : 42
    }, {
      "referenceID" : 99,
      "context" : "• analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], • object tracking [102, 101], • tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), • feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], • image processing and feature extraction for segmentation of grains in asphalt samples [92], and • extended measures and visualization to evaluate the quality of data and regression models [28].",
      "startOffset" : 32,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : "• analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], • object tracking [102, 101], • tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), • feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], • image processing and feature extraction for segmentation of grains in asphalt samples [92], and • extended measures and visualization to evaluate the quality of data and regression models [28].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 100,
      "context" : "• analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], • object tracking [102, 101], • tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), • feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], • image processing and feature extraction for segmentation of grains in asphalt samples [92], and • extended measures and visualization to evaluate the quality of data and regression models [28].",
      "startOffset" : 132,
      "endOffset" : 142
    }, {
      "referenceID" : 99,
      "context" : "• analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], • object tracking [102, 101], • tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), • feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], • image processing and feature extraction for segmentation of grains in asphalt samples [92], and • extended measures and visualization to evaluate the quality of data and regression models [28].",
      "startOffset" : 132,
      "endOffset" : 142
    }, {
      "referenceID" : 31,
      "context" : "• analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], • object tracking [102, 101], • tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), • feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], • image processing and feature extraction for segmentation of grains in asphalt samples [92], and • extended measures and visualization to evaluate the quality of data and regression models [28].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 69,
      "context" : "• analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], • object tracking [102, 101], • tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), • feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], • image processing and feature extraction for segmentation of grains in asphalt samples [92], and • extended measures and visualization to evaluate the quality of data and regression models [28].",
      "startOffset" : 345,
      "endOffset" : 349
    }, {
      "referenceID" : 90,
      "context" : "• analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], • object tracking [102, 101], • tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), • feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], • image processing and feature extraction for segmentation of grains in asphalt samples [92], and • extended measures and visualization to evaluate the quality of data and regression models [28].",
      "startOffset" : 439,
      "endOffset" : 443
    }, {
      "referenceID" : 27,
      "context" : "• analysis of images and videos [102, 101] including a link to the ITK-based pipeline generation tool XPIWIT [7], • object tracking [102, 101], • tissue detection and fluorescence quantification for zebrafish larvae (algorithms from [32]), • feature extraction and peptide optimization based on amino-acid distributions and chemical descriptors [71], • image processing and feature extraction for segmentation of grains in asphalt samples [92], and • extended measures and visualization to evaluate the quality of data and regression models [28].",
      "startOffset" : 541,
      "endOffset" : 545
    }, {
      "referenceID" : 29,
      "context" : "A well known definition is given by [30]: Data mining is a step in the KDD process that consists of applying data analysis and discovery algorithms that produce a particular enumeration of patterns (or models) over the data.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 92,
      "context" : "by special import routines, like HeiDATAProViT in gait analysis [94]) as the collective of possible evaluation measures in SciXMiner (Figure 3.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 23,
      "context" : "Depending on the availability, functions from different toolboxes are called, thereunder standard Matlab functions, functions from internal Matlab toolboxes (see Appendix C), free available Matlab toolboxes (FastICA1, SVM and Kernel Methods Matlab Toolbox [24]2, SOM Toolbox [109]3, lp_solve4, see Appendix D) and many SciXMiner internal functions.",
      "startOffset" : 256,
      "endOffset" : 260
    }, {
      "referenceID" : 107,
      "context" : "Depending on the availability, functions from different toolboxes are called, thereunder standard Matlab functions, functions from internal Matlab toolboxes (see Appendix C), free available Matlab toolboxes (FastICA1, SVM and Kernel Methods Matlab Toolbox [24]2, SOM Toolbox [109]3, lp_solve4, see Appendix D) and many SciXMiner internal functions.",
      "startOffset" : 275,
      "endOffset" : 280
    }, {
      "referenceID" : 105,
      "context" : "• basic knowledge about multivariate statistics and classification [107, 69, 49] and specialties for time series [73],",
      "startOffset" : 67,
      "endOffset" : 80
    }, {
      "referenceID" : 67,
      "context" : "• basic knowledge about multivariate statistics and classification [107, 69, 49] and specialties for time series [73],",
      "startOffset" : 67,
      "endOffset" : 80
    }, {
      "referenceID" : 47,
      "context" : "• basic knowledge about multivariate statistics and classification [107, 69, 49] and specialties for time series [73],",
      "startOffset" : 67,
      "endOffset" : 80
    }, {
      "referenceID" : 71,
      "context" : "• basic knowledge about multivariate statistics and classification [107, 69, 49] and specialties for time series [73],",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 29,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 14,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 24,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 76,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 91,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 56,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 44,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 46,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 62,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 15,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 201,
      "endOffset" : 209
    }, {
      "referenceID" : 83,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 201,
      "endOffset" : 209
    }, {
      "referenceID" : 48,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 241,
      "endOffset" : 249
    }, {
      "referenceID" : 74,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 241,
      "endOffset" : 249
    }, {
      "referenceID" : 53,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 277,
      "endOffset" : 294
    }, {
      "referenceID" : 59,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 277,
      "endOffset" : 294
    }, {
      "referenceID" : 58,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 277,
      "endOffset" : 294
    }, {
      "referenceID" : 48,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 326,
      "endOffset" : 342
    }, {
      "referenceID" : 74,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 326,
      "endOffset" : 342
    }, {
      "referenceID" : 12,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 326,
      "endOffset" : 342
    }, {
      "referenceID" : 11,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 326,
      "endOffset" : 342
    }, {
      "referenceID" : 30,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 388,
      "endOffset" : 396
    }, {
      "referenceID" : 74,
      "context" : "• basics in data mining [30, 15, 25, 78, 93] including tuning concepts [58], • cluster algorithms and Fuzzy C-Means (FCM): basics: [46, 48], specialties for time series [64], • decision trees (basics: [16, 85], implemented design algorithms [50, 76]), • fuzzy systems (basics: [117, 55, 61, 60], implemented design algorithms [50, 76, 13, 12]), , specialities to improve interpretability [31, 76]",
      "startOffset" : 388,
      "endOffset" : 396
    }, {
      "referenceID" : 112,
      "context" : "• a priori relevances [114, 76], • Independent Component Analysis [47, 106], • Support Vector Machines [18, 24, 98], • k-Nearest Neighbor Methods [26], • Artificial Neural Networks [42], • feature aggregation and selection using wrapper approaches [88], • validation strategies [69], and",
      "startOffset" : 22,
      "endOffset" : 31
    }, {
      "referenceID" : 74,
      "context" : "• a priori relevances [114, 76], • Independent Component Analysis [47, 106], • Support Vector Machines [18, 24, 98], • k-Nearest Neighbor Methods [26], • Artificial Neural Networks [42], • feature aggregation and selection using wrapper approaches [88], • validation strategies [69], and",
      "startOffset" : 22,
      "endOffset" : 31
    }, {
      "referenceID" : 45,
      "context" : "• a priori relevances [114, 76], • Independent Component Analysis [47, 106], • Support Vector Machines [18, 24, 98], • k-Nearest Neighbor Methods [26], • Artificial Neural Networks [42], • feature aggregation and selection using wrapper approaches [88], • validation strategies [69], and",
      "startOffset" : 66,
      "endOffset" : 75
    }, {
      "referenceID" : 104,
      "context" : "• a priori relevances [114, 76], • Independent Component Analysis [47, 106], • Support Vector Machines [18, 24, 98], • k-Nearest Neighbor Methods [26], • Artificial Neural Networks [42], • feature aggregation and selection using wrapper approaches [88], • validation strategies [69], and",
      "startOffset" : 66,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "• a priori relevances [114, 76], • Independent Component Analysis [47, 106], • Support Vector Machines [18, 24, 98], • k-Nearest Neighbor Methods [26], • Artificial Neural Networks [42], • feature aggregation and selection using wrapper approaches [88], • validation strategies [69], and",
      "startOffset" : 103,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "• a priori relevances [114, 76], • Independent Component Analysis [47, 106], • Support Vector Machines [18, 24, 98], • k-Nearest Neighbor Methods [26], • Artificial Neural Networks [42], • feature aggregation and selection using wrapper approaches [88], • validation strategies [69], and",
      "startOffset" : 103,
      "endOffset" : 115
    }, {
      "referenceID" : 96,
      "context" : "• a priori relevances [114, 76], • Independent Component Analysis [47, 106], • Support Vector Machines [18, 24, 98], • k-Nearest Neighbor Methods [26], • Artificial Neural Networks [42], • feature aggregation and selection using wrapper approaches [88], • validation strategies [69], and",
      "startOffset" : 103,
      "endOffset" : 115
    }, {
      "referenceID" : 25,
      "context" : "• a priori relevances [114, 76], • Independent Component Analysis [47, 106], • Support Vector Machines [18, 24, 98], • k-Nearest Neighbor Methods [26], • Artificial Neural Networks [42], • feature aggregation and selection using wrapper approaches [88], • validation strategies [69], and",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 40,
      "context" : "• a priori relevances [114, 76], • Independent Component Analysis [47, 106], • Support Vector Machines [18, 24, 98], • k-Nearest Neighbor Methods [26], • Artificial Neural Networks [42], • feature aggregation and selection using wrapper approaches [88], • validation strategies [69], and",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 86,
      "context" : "• a priori relevances [114, 76], • Independent Component Analysis [47, 106], • Support Vector Machines [18, 24, 98], • k-Nearest Neighbor Methods [26], • Artificial Neural Networks [42], • feature aggregation and selection using wrapper approaches [88], • validation strategies [69], and",
      "startOffset" : 248,
      "endOffset" : 252
    }, {
      "referenceID" : 67,
      "context" : "• a priori relevances [114, 76], • Independent Component Analysis [47, 106], • Support Vector Machines [18, 24, 98], • k-Nearest Neighbor Methods [26], • Artificial Neural Networks [42], • feature aggregation and selection using wrapper approaches [88], • validation strategies [69], and",
      "startOffset" : 278,
      "endOffset" : 282
    }, {
      "referenceID" : 90,
      "context" : "Application Data Domain References Asphalt grain analysis Images Engineering [92] Brain-Machine-Interfaces Time series Medicine [70] Cell classification Images and single features Biology [52]",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 68,
      "context" : "Application Data Domain References Asphalt grain analysis Images Engineering [92] Brain-Machine-Interfaces Time series Medicine [70] Cell classification Images and single features Biology [52]",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 50,
      "context" : "Application Data Domain References Asphalt grain analysis Images Engineering [92] Brain-Machine-Interfaces Time series Medicine [70] Cell classification Images and single features Biology [52]",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 16,
      "context" : "Ceramic actuator optimization Time series and single features Engineering [17]",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 97,
      "context" : "Circadian parameter estimation Time series Medical technology [99]",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 49,
      "context" : "Climate change events on streets Time series Engineering [51]",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 81,
      "context" : "Control of prosthesis with muscle signals Time series Medical technology [83, 87, 89, 91, 97, 96]",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 85,
      "context" : "Control of prosthesis with muscle signals Time series Medical technology [83, 87, 89, 91, 97, 96]",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 87,
      "context" : "Control of prosthesis with muscle signals Time series Medical technology [83, 87, 89, 91, 97, 96]",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 89,
      "context" : "Control of prosthesis with muscle signals Time series Medical technology [83, 87, 89, 91, 97, 96]",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 95,
      "context" : "Control of prosthesis with muscle signals Time series Medical technology [83, 87, 89, 91, 97, 96]",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 94,
      "context" : "Control of prosthesis with muscle signals Time series Medical technology [83, 87, 89, 91, 97, 96]",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "Heartbeat detection of zebrafish Videos Biology [2, 84, 81]",
      "startOffset" : 48,
      "endOffset" : 59
    }, {
      "referenceID" : 82,
      "context" : "Heartbeat detection of zebrafish Videos Biology [2, 84, 81]",
      "startOffset" : 48,
      "endOffset" : 59
    }, {
      "referenceID" : 79,
      "context" : "Heartbeat detection of zebrafish Videos Biology [2, 84, 81]",
      "startOffset" : 48,
      "endOffset" : 59
    }, {
      "referenceID" : 99,
      "context" : "Light-sheet microscopy 3D-Images, 3DVideos Biology [101, 103, 104]",
      "startOffset" : 51,
      "endOffset" : 66
    }, {
      "referenceID" : 101,
      "context" : "Light-sheet microscopy 3D-Images, 3DVideos Biology [101, 103, 104]",
      "startOffset" : 51,
      "endOffset" : 66
    }, {
      "referenceID" : 102,
      "context" : "Light-sheet microscopy 3D-Images, 3DVideos Biology [101, 103, 104]",
      "startOffset" : 51,
      "endOffset" : 66
    }, {
      "referenceID" : 32,
      "context" : "Mobile working machines Time series Engineering [33, 34] Modeling of labyrinth seals Single features Engineering [82]",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 33,
      "context" : "Mobile working machines Time series Engineering [33, 34] Modeling of labyrinth seals Single features Engineering [82]",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 80,
      "context" : "Mobile working machines Time series Engineering [33, 34] Modeling of labyrinth seals Single features Engineering [82]",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "Morphology analysis of zebrafish Images Biology [2, 3, 100, 105]",
      "startOffset" : 48,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "Morphology analysis of zebrafish Images Biology [2, 3, 100, 105]",
      "startOffset" : 48,
      "endOffset" : 64
    }, {
      "referenceID" : 98,
      "context" : "Morphology analysis of zebrafish Images Biology [2, 3, 100, 105]",
      "startOffset" : 48,
      "endOffset" : 64
    }, {
      "referenceID" : 103,
      "context" : "Morphology analysis of zebrafish Images Biology [2, 3, 100, 105]",
      "startOffset" : 48,
      "endOffset" : 64
    }, {
      "referenceID" : 19,
      "context" : "Movement analysis Time series Medicine [20, 95, 113, 114, 115] Object segmentation of hardware items Images Engineering [53, 54]",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 93,
      "context" : "Movement analysis Time series Medicine [20, 95, 113, 114, 115] Object segmentation of hardware items Images Engineering [53, 54]",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 111,
      "context" : "Movement analysis Time series Medicine [20, 95, 113, 114, 115] Object segmentation of hardware items Images Engineering [53, 54]",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 112,
      "context" : "Movement analysis Time series Medicine [20, 95, 113, 114, 115] Object segmentation of hardware items Images Engineering [53, 54]",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 113,
      "context" : "Movement analysis Time series Medicine [20, 95, 113, 114, 115] Object segmentation of hardware items Images Engineering [53, 54]",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 51,
      "context" : "Movement analysis Time series Medicine [20, 95, 113, 114, 115] Object segmentation of hardware items Images Engineering [53, 54]",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 52,
      "context" : "Movement analysis Time series Medicine [20, 95, 113, 114, 115] Object segmentation of hardware items Images Engineering [53, 54]",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 37,
      "context" : "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]",
      "startOffset" : 59,
      "endOffset" : 88
    }, {
      "referenceID" : 54,
      "context" : "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]",
      "startOffset" : 59,
      "endOffset" : 88
    }, {
      "referenceID" : 69,
      "context" : "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]",
      "startOffset" : 59,
      "endOffset" : 88
    }, {
      "referenceID" : 72,
      "context" : "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]",
      "startOffset" : 59,
      "endOffset" : 88
    }, {
      "referenceID" : 84,
      "context" : "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]",
      "startOffset" : 59,
      "endOffset" : 88
    }, {
      "referenceID" : 77,
      "context" : "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]",
      "startOffset" : 59,
      "endOffset" : 88
    }, {
      "referenceID" : 114,
      "context" : "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]",
      "startOffset" : 59,
      "endOffset" : 88
    }, {
      "referenceID" : 42,
      "context" : "Optimization of antimicrobial peptides Sequences Chemistry [38, 56, 71, 74, 86, 79, 116] Peripheral nerve signals of rats Time series Biology [44]",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 36,
      "context" : "pH sensor diagnosis Time series Engineering [37] Photo-Motor Response Images, Time series Biology [57, 67]",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 55,
      "context" : "pH sensor diagnosis Time series Engineering [37] Photo-Motor Response Images, Time series Biology [57, 67]",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 65,
      "context" : "pH sensor diagnosis Time series Engineering [37] Photo-Motor Response Images, Time series Biology [57, 67]",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 34,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 35,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 87,
      "endOffset" : 101
    }, {
      "referenceID" : 9,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 87,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 87,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 87,
      "endOffset" : 101
    }, {
      "referenceID" : 20,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 145,
      "endOffset" : 171
    }, {
      "referenceID" : 63,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 145,
      "endOffset" : 171
    }, {
      "referenceID" : 64,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 145,
      "endOffset" : 171
    }, {
      "referenceID" : 108,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 145,
      "endOffset" : 171
    }, {
      "referenceID" : 109,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 145,
      "endOffset" : 171
    }, {
      "referenceID" : 2,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 202,
      "endOffset" : 212
    }, {
      "referenceID" : 3,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 202,
      "endOffset" : 212
    }, {
      "referenceID" : 43,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 202,
      "endOffset" : 212
    }, {
      "referenceID" : 5,
      "context" : "Photovoltaic systems Time series Engineering [35, 36] Robotics Time series Engineering [8, 10, 9, 11] Smart energy grids Time series Engineering [21, 40, 65, 66, 110, 111] Toxicity tests Images Biology [3, 4, 45] Wind power systems Time series Engineering [6]",
      "startOffset" : 256,
      "endOffset" : 259
    }, {
      "referenceID" : 39,
      "context" : "• alternative data mining software as Weka [41], Knime [14], Apache Spark’s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "• alternative data mining software as Weka [41], Knime [14], Apache Spark’s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 66,
      "context" : "• alternative data mining software as Weka [41], Knime [14], Apache Spark’s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "• alternative data mining software as Weka [41], Knime [14], Apache Spark’s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 110,
      "context" : "• alternative data mining software as Weka [41], Knime [14], Apache Spark’s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]",
      "startOffset" : 126,
      "endOffset" : 131
    }, {
      "referenceID" : 75,
      "context" : "• alternative data mining software as Weka [41], Knime [14], Apache Spark’s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]",
      "startOffset" : 148,
      "endOffset" : 160
    }, {
      "referenceID" : 60,
      "context" : "• alternative data mining software as Weka [41], Knime [14], Apache Spark’s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]",
      "startOffset" : 148,
      "endOffset" : 160
    }, {
      "referenceID" : 38,
      "context" : "• alternative data mining software as Weka [41], Knime [14], Apache Spark’s machine learning library [68], Keel [1], Rattle/R [112], see surveys in [77, 62, 39]",
      "startOffset" : 148,
      "endOffset" : 160
    }, {
      "referenceID" : 57,
      "context" : "The goal of the Building-data set was to examine the effect of different parameters on the power consumption of a known building [59].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 22,
      "context" : "Since the extraction of the single trials should be start exactly at a trigger event and end 23 sample points after the trigger event (resulting in 24 hours after the trigger event), the offset is set to [0, 23].",
      "startOffset" : 204,
      "endOffset" : 211
    }, {
      "referenceID" : 41,
      "context" : "The Iris data set (downloadable from UCI-Repository [43]) is one of the most-famous benchmark data sets for the comparison of classifiers.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "The data set contains 150 examples of three different irises, every single iris is represented by 50 examples [5].",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 0,
      "context" : "The feature numbers start with x[1] instead of x[0]!",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : "Some hints to the algorithms are given in [19].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 22,
      "context" : "The one-class method uses a SVM optimization proposed by [23].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 62,
      "context" : "5, [64]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 74,
      "context" : "However, a split by existing membership functions is used instead of a new computed binary split in each node [76].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 85,
      "context" : "2) in [87]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 85,
      "context" : "4) in [87]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 85,
      "context" : "7) in [87]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 18,
      "context" : "• Feature maps (all time series, selected data points): computes feature relevances of all time series and shows the results as scatter plot (x axis: Time, y axis: number of time series, color: feature relevance) [19].",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 106,
      "context" : "Otherwise for cross tabulations with only two values 0 and 1, the (more conservative) Two-tail Exact Fisher Test is used in the implementation of [108] (function ”fisherextest”).",
      "startOffset" : 146,
      "endOffset" : 151
    }, {
      "referenceID" : 21,
      "context" : "The different classifier types are described in [22].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 85,
      "context" : "• Design: designs a Hierarchical Bayes classifier by a step-wise separation of single classes from all other classes [87].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 74,
      "context" : "The design algorithms are described in [76].",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 62,
      "context" : "[64]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "For stability reasons, the value is limited to [0, 1].",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 88,
      "context" : "• IIR parameter (aF, aS, aSigma): defines the parameters for three different IIR filters for trend and standard deviation computation (see [90]).",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 61,
      "context" : "[63] or the documentation of the Matlab Wavelet Toolbox).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 61,
      "context" : "• Matlab wavelet decomposition: uses the Matlab implementation instead of the implementation of [63] for the wavelet decomposition.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 22,
      "context" : "A data point z is handled as outlier if f(z) < Threshold (see [23] for algorithm details).",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "The neighborhood is defined by a maximal ([0,1]-normalized Euclidean) distance.",
      "startOffset" : 42,
      "endOffset" : 47
    }, {
      "referenceID" : 22,
      "context" : "A data point z is handled as outlier if f(z) < Threshold (see [23] for algorithm details).",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "The neighborhood is defined by a maximal ([0,1]-normalized Euclidean) distance.",
      "startOffset" : 42,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 85,
      "context" : "[87]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 85,
      "context" : "Available options are an a maximization of the classification accuracy (”best_class”) or a maximization of the minimal distance between two classes using class-specific covariance matrixes as metric (”best_ldf”) [87].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 0,
      "context" : "Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 21,
      "context" : "[22, 73] give a detailed description of the classifiers.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 71,
      "context" : "[22, 73] give a detailed description of the classifiers.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 21,
      "context" : "[22, 73] give a detailed description of the classifiers.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 71,
      "context" : "[22, 73] give a detailed description of the classifiers.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 85,
      "context" : "[87]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Two approaches are implemented; to the interval [0, 1] or to a mean value of zero and a standard deviation of one under the assumption of a normal distribution.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 26,
      "context" : "• Distance measure for the noise cluster: defines the method for the definition of a noise cluster [27] for a Fuzzy C-Means method.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 26,
      "context" : "Possible distance measures are the mean distance of all data points to all other clusters [27] or the median of these distances.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : "• Kernel: defines the used kernel function for Support Vector Machines [18].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 74,
      "context" : "The choice ”with interpretability” rounds the found parameters using a method described in [76] to improve the interpretability.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 74,
      "context" : "• Exponent for clearness: defines the evaluation method for the pruning of fuzzy rules [76].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 73,
      "context" : "An additional correction of overlapping rules [75] was implemented for artifacts by rules with the same or concurrent conclusions in case of the SUM-PROD inference.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 74,
      "context" : "The choice ”with interpretability” rounds the found parameters using a method described in [76] to improve the interpretability.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 28,
      "context" : "[29]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 62,
      "context" : "Such categories might be used in different analysis and visualization functions (see [64, 114]).",
      "startOffset" : 85,
      "endOffset" : 94
    }, {
      "referenceID" : 112,
      "context" : "Such categories might be used in different analysis and visualization functions (see [64, 114]).",
      "startOffset" : 85,
      "endOffset" : 94
    }, {
      "referenceID" : 112,
      "context" : "(3) in [114] for a segment)",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 112,
      "context" : "(3) in [114] without the absolute value in the nominator)",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 62,
      "context" : "3) in [64]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 88,
      "context" : "The computation is explained in [90].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 88,
      "context" : "(2-6) in [90] with aF = aSigma, aL = aSlow, aS = aFast.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 112,
      "context" : "(3) in [114])",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 62,
      "context" : "9) in [64])",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 62,
      "context" : "2) in [64])",
      "startOffset" : 6,
      "endOffset" : 10
    } ],
    "year" : 2017,
    "abstractText" : null,
    "creator" : "LaTeX with hyperref package"
  }
}