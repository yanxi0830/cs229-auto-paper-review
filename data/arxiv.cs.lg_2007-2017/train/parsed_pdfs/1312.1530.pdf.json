{
  "name" : "1312.1530.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bandit Online Optimization Over the Permutahedron",
    "authors" : [ "Nir Ailon", "Kohei Hatano", "Eiji Takimoto" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n31 2.\n15 30\nv2 [\ncs .L\nG ]\n6 J\ni=1 πt(i)st(i). We study the problem in two regimes. In the first regime, st is a point in the polytope dual to the permutahedron. Algorithm CombBand of Cesa-Bianchi et al (2009) guarantees a regret of O(n √ T log n) after T steps. Unfortunately, CombBand requires at each step an n-by-n matrix permanent computation, a #P -hard problem. Approximating the permanent is possible in the impractical running time of O(n10), with an additional heavy inverse-polynomial dependence on the sought accuracy. We provide an algorithm of slightly worse regret O(n3/2 √ T ) but with more realistic time complexity O(n3) per step. The technical contribution is a bound on the variance of the Plackett-Luce noisy sorting process’s ‘pseudo loss’, obtained by establishing positive semi-definiteness of a family of 3-by-3 matrices of rational functions in exponents of 3 parameters.\nIn the second regime, st is in the hypercube. For this case we present and analyze an algorithm based on Bubeck et al.’s (2012) OSMD approach with a novel projection and decomposition technique for the permutahedron. The algorithm is efficient and achieves a regret of O(n √ T ), but for a more restricted space of possible loss vectors."
    }, {
      "heading" : "1 Introduction",
      "text" : "Consider a game in which, at each step, a player plays a permutation of some ground set V = {1, . . . , n}, and then suffers (and observes) a loss. We model the loss as a sum over the items of some latent quality of the item, weighted by its position in the permutation. The game is repeated, and the items’ quality can adversarially change over time. The game models many scenarios in which the player is an online system (say, a search/recommendation engine) presenting a ranked list of items (results/products) to a stream of users. A user’s experience\nis positive if she perceives the quality of the top items on the list as higher than those at the bottom. The goal of the system is to create a total positive experience for its users.\nThere is a myriad of methods for modelling ranking loss functions in the literature, especially (but not exclusively) for information retrieval. Our choice allows us to study the problem in the framework of online combinatorial optimization in the bandit setting, and to obtain highly nontrivial results improving on state of the art in either run time or regret bounds. More formally, we study online linear optimization over the the n-permutahedron action set, defined as the convex closure of all vectors in Rn consisting of n distinct coordinates taking values in [n] := {1, . . . , n} (permutations). At each step t = 1, . . . , T , the player outputs an action πt and suffers a loss π ′ tst = ∑n i=1 πt(i)st(i) , where st ∈ Rn is the vector of “item qualities” chosen by some adversary who knows the player’s strategy but doesn’t control their random coins. The performance of the player is the difference between their total loss and that of the optimal static player, who plays the best (in hindsight) single permutation π∗ throughout. This difference is known as regret. Note that, given s1, . . . , sT , π\n∗ can be computed by sorting the coordinates of\n∑T t=1 st in decreasing order. This is aligned with our\npractical requirement that items with higher quality should be placed first, and those with lower quality should be last."
    }, {
      "heading" : "2 Results, Techniques and Contribution",
      "text" : "Our first of two results, stated as Theorem 1, is for the setting in which at each step the loss is uniformly bounded (by 1 for simplicity) in absolute value for all possible permutations. Equivalently, the vectors st belong to the polytope that is dual to the permutahedron. Our algorithm, BanditRank, plays permutations from a distribution known as the Plackett-Luce model (see [12]) which is widely used in statistics and econometrics (see eg [3]). It uses an inverse covariance matrix of the distribution in order to obtain an unbiased loss vector estimator, which is a standard technique [6]. The main technical difficulty (Lemma 2) is in bounding second moment properties of Plackett-Luce, by establishing positive semidefiniteness of a certain family of 3 by 3 matrices. The lemma is interesting in its own right as a tool for studying distributions over permutations. The expected regret of our algorithm is O(n3/2 √ T ) for T steps, with running time of O(n3) per time step. This result should be compared to CombBand of [6], where a framework for playing bandit games over combinatorially structured sets was developed. Their techniques extend that of [7]. In each step, it draws a permutation from a distribution that assigns to each permutation π a probability of eη ∑t τ=1 π\n′ s̃τ , where s̃t is a pseudo-loss vector at time t, an unbiased estimator of the loss vector st. Their algorithm guarantees a regret of O(n √ T logn), which is better than ours by a factor of Θ( √\nn/ logn). However, its computational requirements are much worse. In order to draw permutations, they need to compute nonnegative n by n matrix permanents. Unfortunately, nonnegative permanent computation is #P -hard, as shown by [14]. On the other hand, a\ngroundbreaking result of [11] presents a polynomial time approximation scheme for permanent, which runs in time O(n10) for fixed accuracy. To make things worse, the dependence in the accuracy is inverse polynomial, implying that, even if we could perform arbitrarily accurate floating point operations, the total running time would be super linear in T , because a regret dependence of √ T over T steps requires accuracy inverse polynomial in T . (Our algorithm does not suffer from this problem.) From a practical point of view, the runtime dependence of CombBand in both n and T is infeasible for even modest cases. For example, our algorithm can handle online ranking of n = 100 items in an order of few millions of operations per game iteration. In contrast, approximating the permanent of a 100-by-100 positive matrix is utterly impractical.\nWe note that independently of our work, Hazan et al. [9] have improved the state-of-the-art general purpose algorithm for linear bandit optimization, implying an algorithm with regret O(n √ T ) for our problem, but with worse running time Õ(n4).1\nIn our second result in Section 5 we further restrict st to have ℓ1 norm of 1/n. (Note that this restriction is contained in |π′tst| ≤ 1 by Hölder). We present and analyze an algorithm OSMDRank based on the bandit algorithm OSMD of [5] with projection and decomposition techniques over the permutahedron ([15, 13]). The projection is defined in terms of the binary relative entropy divergence. The restriction allows us to obtain an expected regret bound of O(n √ T ) (a √ logn improvement over CombBand). The running time is O(n2+ nτ(n)), where τ(n) is the time complexity for some numerical procedure, which is O(n2) in a fixed precision machine.\nWe note previous work on playing the permutahedron online optimization game in the full information case, namely, when st is known for each t. As far as we know, Helmbold et al. [10] were the first to study a more general version of this problem, where the action set is the vertex set of the Birkhoff-von-Neumann polytope (doubly-stochastic matrices). Suehiro et al. [13] studied the problem by casting it as a submodularly constrained optimization problem, giving near optimal regret bounds, and more recently Ailon [1] both provided optimal regret bounds with improved running time and established tight regret lower bounds."
    }, {
      "heading" : "3 Definitions and Problem Statement",
      "text" : "Let V be a ground set of n items. For simplicity, we identify V with [n] := {1, . . . n}. Let Sn denote the set of n! permutations over V , namely bijections over [n]. By convention, we think of π(v) for v ∈ V as the position of v ∈ V in the ranking, where we think of lower numbered positions as more favorable. For distinct u, v ∈ V , we say that u ≺π v if π(u) < π(v) (in words: u beats v). We use [u, v]π as shorthand for the indicator function of the predicate u ≺π v.\n1The running time is a product of Õ(n3) number of Markov chain steps required for drawing a random point from a convex set under a log-concave distribution, and O(n logn) time to test whether a point lies in the permutahedron. By Õ we hide poly-logarithmic factors.\nThe convex closure of Sn is known as the permutahedron polytope. It will be more convenient for us to consider a translated version of the permutahedron, centered around the origin. More precisely, for π ∈ Sn we let π̂ denote\nπ̂ := (π(1)− (n+ 1)/2, π(2)− (n+ 1)/2, . . . , π(n)− (n+ 1)/2) .\nIt will be convenient to define a symmetrized version of the permutation set Ŝn := {π̂ : π ∈ Sn}. The symmetrized n-permutahedron, denoted P̂n is the convex closure of Ŝn. Symmetrization allows us to work with a polytope that is centered around the origin. Generalization our result to standard (un-symmetrized) permutations is a simple technicality that will be explained below. The notation u ≺π̂ v and [u, v]π̂ is defined as for π ∈ Sn in an obvious manner.\nAt each step t = 1, . . . , T , an adversary chooses and hides a nonnegative vector st ∈ Rn ≡ RV , which assigns an elementwise quality measure st(v) for any v ∈ V . The player-algorithm chooses a permutation π̂t ∈ Ŝn, possibly random, and suffers an instantaneous loss\nℓt := π̂ ′ tst =\n∑ v∈V π̂t(v)st(v) . (3.1)\nThe total loss Lt is defined as ∑T t=1 ℓt. We will work with the notion of regret, defined as the difference Lt − L∗t , where L∗T = minπ̂∈Ŝn ∑T t=1 π̂ ′st. We let π̂∗ denote any minimizer achieving L∗T in the RHS.\nFor any π̂ ∈ Ŝn and s ∈ Rn, the dot-product π̂′s can be decomposed over pairs: π̂′s = 12 ∑\nu6=v[u, v]π(s(v) − s(u)). This makes the symmetrized permutahedron easier to work with. Nevertheless, our results also apply to the non-symmetrized permutahedron as well, as we shall see below.\nThroughout, the notation ∑\nu6=v means summation over distinct, ordered pairs of elements u, v ∈ V , and∑u<v means summation over distinct, unordered pairs.2 The uniform distribution over Ŝn will be denoted Un.\nThe smallest eigenvalue of a PSD matrix A is denoted λmin(A). The norm ‖·‖2 will denote spectral norm (Euclidean norm for a vector). To avoid notation such as C,C′, C′′, C1 for universal constants, the expression C will denote a “general positive constant” that may change its value as necessary. For example, we may write C = 3C + 5.\n4 Algorithm BanditRank and its Guarantee\nFor this section, we will assume that the instantaneous losses are uniformly bounded by 1, in absolute value: For all t and π̂ ∈ Ŝn, |π̂′st| ≤ 1. Equivalently, using geometric language, the loss vectors belong to a polytope which is dual to the permutahedron.\nNow consider Algorithm 1. It maintains, at each time step t, a weight vector wt ∈ Rn. At each time step, it draws a random permutation π̂t from a mixture\n2We will only use expressions of the form ∑\nu<v f(u, v) for symmetric functions satisfying f(u, v) = f(v, u).\nDt of the uniform distribution over Ŝn and a distribution PLn(w) which we define shortly. The distribution mixture is determined by a parameter γ. The algorithm then plays the permutation π̂t and thereby suffers the instantaneous loss defined in (3.1). The weights are consequently updated by adding an unbiased estimator s̃t of st (computed using the pseudo-inverse covariance matrix corresponding to Dt), multiplied by another parameter η > 0.\nThe Plackett-Luce Random Sorting Procedure: The distribution PLn(w) over Ŝn, parametrized by w ∈ Rn, is defined by the following procedure. To choose the first (most preferred) item, the procedure draws a random item, assigning probability proportional to ew(u) for each u ∈ V . It then removes this item from the pool of available items, and iteratively continues to choose the second item, then third and so on. As claimed in the introduction, this random permutation model is well studied in statistics. An important well known property of the distribution is that it can be equivalently defined as a Random Utility Model (RUM) [12, 16]: To draw a permutation, add a random iid noise variable following the Gumbel distribution to each weight, and then sort the items of V in decreasing value of noisy-weights.3 The RUM characterization implies, in particular, that for any two disjoint pairs of element (u, v) and (u′, v′), the events u ≺π v and u′ ≺π v′ are statistically independent if π is drawn from PLn(w), for any w. This fact will be used later. We are finally ready to state our main result, bounding the expected regret of the algorithm.\nTheorem 1. If algorithm BanditRank (Algorithm 1) is executed with parameters γ = O(n3/2/ √ T ) and η = O(γ/n), then the expected regret (with respect to the game defined by the symmetrized permutahedron) is at most O(n3/2 √ T ). The running time of each iteration is O(n3). Additionally, there exists an algorithm with the same expected regret bound and running time with respect to the standard permutahedron (assuming the vectors st uniformly satisfy |π′st| ≤ 1, ∀π ∈ Sn.)\nThe proof uses a standard technique used e.g. in Cesa-Bianchi et al.’s CombBand [6], which is itself an adaptation of Auer et al.’s Exp3 [2] from the finite case to the structured combinatorial case. The distribution from which the actions π̂t are drawn in the algorithm differ from the distribution used in CombBand, and give rise to the technical difficulty of variance estimation, resolved in Lemma 2.\nProof. Let Tn denote the set of tournaments over [n]. More precisely, an element A ∈ Tn is a subset of [n]× [n] with either (u, v) ∈ A or (v, u) ∈ A (but not both) for all u < v. We extend our previous notation so that u ≺A v is equivalent to the predicate (u, v) ∈ A.\n3The Gumbel distribution, also known as doubly-exponential, has a cdf of e−e −x .\nAlgorithm 1 Algorithm BanditRank(n, η, γ, T ) (assuming |π̂′st| ≤ 1 for all t and π̂ ∈ Ŝn) 1: given: ground set size n, positive parameters η, γ (γ ≤ 1), time horizon T 2: set w0(u) = 0 for all u ∈ V = [n] 3: for t = 1..T do 4: let distribution Dt over Ŝn denote a mixture of Un (with probability γ)\nand PLn(wt−1) (with probability 1− γ) 5: draw and output π̂t ∼ Dt 6: observe and suffer loss ℓt (= π̂ ′ tst) 7: s̃t = ℓtP + t π̂t where Pt = Eσ̂∼Dt [σ̂σ̂\n′] 8: set wt = wt−1 + ηs̃ 9: end for\nFor any pair π̂ ∈ Ŝn and w ∈ Rn, p(π̂|w) denotes the probability assigned to π̂ ∈ Ŝn by PLn(w). Slightly abusing notation, we define the following shorthand:\np(u ≺ v|w) := ∑ π̂:u≺π̂v p(π|w) = e\nw(u)\new(u) + ew(v)\np(u ≺ v ≺ z|w) := ∑ π̂:u≺π̂v≺π̂z p(π̂|w) = e\nw(u)+w(v)\n(ew(u) + ew(v) + ew(z))(ew(v) + ew(z)) .\nThe last two right hand sides are easily derived from the definition of the distribution PLn(w), see also e.g. [12]. We also define the following abbreviations:\np(u ≺ vz |w) := p(u ≺ v ≺ z|w) + p(u ≺ z ≺ v|w) = ew(u)\new(u) + ew(v) + ew(z)\n(4.1)\np(uv ≺ z|w) := p(u ≺ v ≺ z|w) + p(v ≺ u ≺ z|w)\n= ew(u)+w(v)\new(u) + ew(v) + ew(z)\n(\n1\new(v) + ew(z) +\n1\new(u) + ew(z)\n)\n(4.2)\nWe will also need to define a distribution over the set of tournaments Tn. The distribution, BT Ln(w) is parametrized by a weight vector w ∈ Rn. Drawing A ∼ BT Ln(w) is done by independently setting, for all u < v in V ,\n(u, v) ∈ A with probability p(u ≺ v|w) = e w(u)\new(u) + ew(v)\n(v, u) ∈ A with probability p(v ≺ u|w) = e w(v)\new(u) + ew(v) .\n(Note that the distribution is equivalently defined as the product distribution, over all u < v in V , of the Bradley-Terry-Luce pairwise preference model, hence\nthe name BT Ln. We refer to [12] for definition and history of the BradleyTerry-Luce model.)\nFor A ∈ Tn, we denote by p̃(A|w) the probability ∏ u≺Av p(u ≺ v|w) of drawing A from BT Ln(w). The proof of the theorem proceeds roughly as the main result upper bounding the expected regret of CombBand in [6]. The following technical lemma is required in anticipation of a major hurdle (inequality (4.5). We believe the inequality is interesting in its own right as a probabilistic statement on permutation and tournament distributions.\nLemma 2. Let s, w ∈ Rn. Let π̂ ∼ PLn(w) and A ∼ BT Ln(w) be drawn independently. Define X1 = ∑\nu,v: u≺π̂v(s(v)−s(u)) = π̂ ′s, X2 = ∑ u,v: u≺Av(s(v)− s(u)). Then E[X22 ] ≤ E[X21 ].\n(Note that clearly, E[X2] = E[X1], so the lemma in fact upper bounds the variance of X2 by that of X1.) The proof of the lemma is deferred to Section 4.1.\nContinuing the proof of Theorem 1, we let q(π|w) denote the probability of drawing π from the mixture of the uniform distribution (with probability γ) and PLn(w) (with probability (1 − γ). Similarly to above, q(u ≺ v|w) denotes ∑\nπ̂:u≺π̂v q(π̂|w). By these definitions,\nq(π̂|w) = (1− γ)p(π̂|w) + γ n!\nq(u ≺ v|w) = (1− γ)p(u ≺ v|w) + γ 2 . (4.3)\nThe analysis proceeds by defining a potential function: Wt(u, v) := e 1 2η(wt(u)−wt(v))+ e 1 2 η(wt(v)−wt(u)). The quanatity of interest will be E [ ∑\nu<v\n∑ t log Wt(u,v)\nWt−1(u,v)\n]\n,\nwhere the expectation is taken over all random coins used by the algorithm throughout T steps. This quantity will be bounded from above and from below, giving rise to a bound on the expected total loss, expressed using the optimal static loss. On the one hand,\n∑\nu<v\nlog Wt(u, v)\nWt−1(u, v) =\n∑\nu<v\nlog\n(\ne 1 2 (wt(u)−wt(v))\nWt−1(u, v) +\ne 1 2 (wt(v)−wt(u))\nWt−1(u, v)\n)\n= ∑\nu<v\nlog\n(\ne 1 2 (wt−1(u)−wt−1(v))e 1 2η(s̃t(u)−s̃t(v))\nWt−1(u, v) +\ne 1 2 (wt(v)−wt(u))e 1 2η(s̃t(v)−s̃t(u))\nWt−1(u, v)\n)\n= ∑\nu<v\nlog ( p(u ≺ v|wt−1)e 1 2η(s̃t(u)−s̃t(v)) + p(v ≺ u|wt−1)e 1 2 η(s̃t(v)−s̃t(u)) )\n= log\n(\n∑\nA∈Tn p̃(A|wt−1)e\n1 2 η ∑ u≺Av\n(s̃t(u)−s̃t(v)) )\n.\nWe will now assume that η is small enough so that for all A ∈ Tn and for all t,\nη\n∣ ∣ ∣ ∣ ∣ ∣ ∑ (u,v)∈A (s̃t(u)− s̃t(v)) ∣ ∣ ∣ ∣ ∣ ∣ ≤ 1 . (4.4)\n(This will be shortly enforced.) Using ex ≤ 1 + x+ x2 ∀x ∈ [−1/2, 1/2],\n∑\nu,v\nlog Wt(u, v)\nWt−1(u, v) ≤ log\n[\n∑\nA∈Tn p̃(A|wt−1)\n(\n1 + η\n2\n∑\nu≺Av (s̃t(u)− s̃t(v))\n+ η2\n4\n(\n∑\nu≺Av (s̃t(u)− s̃t(v))\n)2 \n\n\n\n= log\n\n1 + η\n2 EA∼BT Ln(wt−1)\n\n\n∑\nu≺Av (s̃t(u)− s̃t(v)) +\nη2\n4\n(\n∑\nu≺Av (s̃t(u)− s̃t(v))\n)2 \n\n\n\n≤ log\n\n1 + η\n2 Eπ̂∼PLn(wt−1)\n\n\n∑\nu≺π̂v (s̃t(u)− s̃t(v)) +\nη2\n4\n(\n∑\nu≺π̂v (s̃t(u)− s̃t(v))\n)2 \n\n\n .\n(4.5)\nwhere we used Lemma 2 in the last inequality (together with the fact that the marginal probability of the event “u ≺Y v” is identical for both Y ∼ PLn(wt−1) and Y ∼ BT Ln(wt−1)). Henceforth, for any π̂ ∈ Ŝ, we let ℓ̃t(π̂) := π̂′s̃t = ∑\nu≺π̂v(s̃(v)− s̃(u)). Using 4.3 and the fact that log(1+x) ≤ x for all x, we get\n∑\nu<v\nlog Wt(u, v)\nWt−1(u, v)\n≤ η 2 ∑\nu6=v\nq(u ≺ v|wt−1)− γ2 1− γ (s̃t(u)− s̃t(v)) + η2 4 ∑\nπ̂∈Ŝn\nq(π|wt−1)− γn! 1− γ ℓ̃t(π̂) 2\n≤ −η 2(1− γ) ∑\nπ̂∈Ŝn\nqt(π̂|wt−1)ℓ̃t(π̂) + η2\n4(1− γ) ∑\nπ̂∈Ŝn\nqt(π̂|wt−1)ℓ̃t(π̂)2 .\nWe now note that (1) ∑ π̂∈Ŝ qt(π̂|wt−1)ℓ̃t = ℓt (following the properties of matrix pseudo-inverse in Line 7 in Algorithm 1), and (2) ∑\nπ̂∈Ŝn qt(π̂|wt−1)ℓ̃t(π) 2] ≤\nn (see top of page 31 together with Lemma 15 in [6]). Applying these inequalities, and then taking expectations over the algorithm’s randomness and summing for t = 1, . . . , T , we get\nT ∑\nt=1\nE\n[\n∑\nu,v\nlog Wt(u, v)\nWt−1(u, v)\n]\n≤ − η 2(1− γ)E[LT ] +\nη2\n8(1− γ)nT .\nOn the other hand,\nT ∑\nt=1\nE\n[\n∑\nu,v\nlog Wt(u, v)\nWt−1(u, v)\n]\n≥ ∑\nu,v\nE\n[ log ( [u, v]π∗e 1 2 (wT (u)−wT (v)) + [v, u]π∗e 1 2 (wT (u)−wT (v))) )] − ∑\nu,v\nlog 2\n= 1\n2\n∑\nu,v\n(E [[u, v]π∗(wT (u)− wT (v)) + [v, u]π∗(wT (u)− wT (v))])− ( n\n2\n)\nlog 2\n= η\n2\n∑\nu,v\n(\nE\n[\n[u, v]π∗ ∑\nt\n(s̃t(u)− s̃t(v)) + [v, u]π∗ ∑\nt\n(s̃t(u)− s̃t(v)) ]) − ( n\n2\n)\nlog 2\n= η\n2\n∑\nu,v\n(\n[u, v]π∗ ∑\nt\n(st(u)− st(v)) + [v, u]π∗ ∑\nt\n(st(u)− st(v)) ) − ( n\n2\n)\nlog 2\n= −η 2 L∗T −\n(\nn\n2\n)\nlog 2 ,\nwhere L∗T is the total loss of a player who chooses the best permutatation π̂ ∗ ∈ Ŝn in hindsight. Combining, we obtain η 2(1−γ)E[Lt] ≤ η 2L ∗ T+ n2 2 log 2+ η2 4(1−γ)nT . Multiplying both sides by 2(1− γ)/η yields\nE[LT ] ≤ L∗T + γ|L∗T |+ n2 log 2\nη +\nη 2 nT . (4.6)\nWe shall now work to impose (4.4).\nmax t max A∈T (V )\n∣ ∣ ∣ ∣ ∣ ∣ ∑ (u,v)∈A (s̃t(u)− s̃t(v)) ∣ ∣ ∣ ∣ ∣ ∣ ≤ max t √ ∑ v∈V s̃t(v)2\n√ √ √ √ √ (n−1)/2 ∑\ni=−(n−1)/2 i2 ≤ Cmax t ‖s̃t‖2n3/2 ,\nwhere the left inequality is Cauchy-Schwartz. We now note that ‖s̃t‖2 ≤ |ℓt|‖P+t ‖2‖π̂t‖2. Clearly ‖π̂‖2 is bounded above by Cn3/2. Also ‖P+t ‖2 equals 1/λmin(Pt). By Weyl’s inequality λmin(Pt) ≥ γλmin(Eτ̂∼Un [τ̂ τ̂ ′]). It is an exercise to check that λmin(Eτ̂∼Un [τ̂ τ̂\n′]) ≥ Cn2. We conclude (also recalling that |ℓt| ≤ 1) that maxt ‖s̃t‖2 ≤ C/(n1/2γ). Combining, we shall satisfy (4.7) by imposing η ≤ γ/(Cn). Plugging in (4.6), we get\nE[LT (Alg)] ≤ L∗T + γ|L∗T |+ Cn3\nγ + CγT . (4.7)\nChoosing γ = √ Cn3\nT gives E[LT (Alg)] ≤ L∗T + Cn 3/2 √ T |L∗T |+ n3/2 √ T .\nThis concludes the required result for the symmetrized case, because |L∗T | ≤ T . For the standard permutahedron, we notice that for any π ∈ Sn and its symmetrized counterpart π̂ ∈ Ŝn, and any vector s ∈ Rn, π′s − π̂′s =\nn−1 2 ∑ v∈V s(v) =: f(s). Equivalently, we can write π ′s = (π̂′, 1)(s; f(s)), where (·, a) appends the scalar a to the right of a row vector and (·; a) appends to the bottom of a column vector. Algorithm 1 can be easily adjusted to work with action set Ŝn × {1}. For the proof, we keep the same potential function. The technical part of the proof is lower bounding the smallest eigenvalue of the expectation of τ̂ τ̂ ′, where τ̂ is now drawn from the uniform distribution on Ŝn × {1}. We omit these simple details for lack of space."
    }, {
      "heading" : "4.1 Proof of Lemma 2",
      "text" : "The expression E[X21 ] can be written as\nE[X21 ] = ∑ u6=v p(u ≺ v|w)((s(v) − s(u))2\n+ ∑ |{u,v,u′,v′}|=4 p(u ≺ v ∧ u′ ≺ v′|w) (s(v) − s(u))(s(v′)− s(u′)) + ∑\nu6=v,u′ 6=v′\n|{u,v,u′,v′}|=3\np(u ≺ v ∧ u′ ≺ v′|w) (s(v) − s(u))(s(v′)− s(u′)) , (4.8)\nwhere p(u ≺ v ∧ u′ ≺ v′|w) is the probability that both u ≺π̂ v and u′ ≺π̂ v′ with π̂ ∼ PLn(w). Similarly,\nE[X22 ] = ∑ u6=v p(u, v|w)((s(v) − s(u))2\n+ ∑ |{u,v,u′,v′}|=4 p(u ≺ v|w)p(u′ ≺ v′|w) (s(v) − s(u))(s(v′)− s(u′)) + ∑\nu6=v,u′ 6=v′\n|{u,v,u′,v′}|=3\np(u ≺ v|w)p(u′ ≺ v′|w) (s(v) − s(u))(s(v′)− s(u′)) .\n(4.9)\nSince Plackett-Luce is a random utility model (see [12]), it is clear that whenever a pair of pairs u 6= v, u′ 6= v′ satisfies |{u, v, u′, v′}| = 4, p(u ≺ v ∧ u′ ≺ v′|w) = p(u ≺ v|w)p(u′ ≺ v′|w). Hence, it suffices to prove that the third summand in the RHS of (4.9) is upper bounded by the third summand in the RHS of (4.8). But now notice the following identity:\n∑\nu6=v,u′ 6=v′\n|{u,v,u′,v′}|=3\n≡ ∑\n∆⊆V |∆|=3\n∑\nu6=v,u′ 6=v′\nu,v,u′,v′∈∆ |{u,v,u′,v′}|=3\n.\nThis last sum rearrangement implies that it suffices to prove that for any ∆ of\ncardinality 3,\nF2(∆) := ∑\nu6=v,u′ 6=v′\nu,v,u′ ,v′∈∆\n|{u,v,u′,v′}|=3\np(u, v|w)p(u′, v′|w) (s(v) − s(u))(s(v′)− s(u′))\n≤ ∑\nu6=v,u′ 6=v′\nu,v,u′,v′∈∆\n||{u,v,u′,v′}|=3\np(u, v ∧ u′, v′|w) (s(v) − s(u))(s(v′)− s(u′)) =: F1(∆) .\nIf we now denote ∆ = {a, b, c}, then both F1(∆) and F2(∆) are quadratic forms in s(a), s(b), s(c) (for fixed w). It hence suffices to prove that H(∆) := F1(∆)−F2(∆) is a positive semi-definite form in s(∆) := (s(a), s(b), s(c))′. We now write\nH(∆) = s(∆)′\n\n\nHaa 1 2Hab 1 2Hac 1 2Hab Hbb 1 2Hbc 1 2Hac 1 2Hbc Hcc\n\n s(∆) .\nThe matrix is singular, because clearly H(∆) = F1(∆) = F2(∆) = 0 whenever s(a) = s(b) = s(c). To prove positive semi-definiteness, by Sylvester’s criterion it hence suffices to show that the diagonal element Haa ≥ 0 and that the principal 2-by-2 minor determinant HaaHbb − 14H2ab ≥ 0. Using the definitions, together with the properties of PLn(w), a technical (but quite tedious) algebraic derivation (see Appendix A for details) gives\nHaa = 4es(a)+s(b)+s(c)\n(es(a) + es(b))(es(a) + es(c))(es(a) + es(b) + es(c)) . (4.10)\nSimilarly, by symmetry, Hbb = 4es(a)+s(b)+s(c)\n(es(b)+es(a))(es(b)+es(c))(es(a)+es(b)+es(c)) . From a\nsimilar (yet more tedious) technical algebraic calculation which we omit, one gets: (see Appendix A for details):\nHab = −8es(a)+s(b)+2s(c)\n(es(a) + es(b))(es(a) + es(c))(es(b) + es(c))(es(a) + es(b) + es(c)) . (4.11)\nOne now verifies, using (4.10)-(4.11), the identity\nHaaHbb− 1\n4 H2ab =\n16e2s(a)+2s(b)+2s(c)\n(es(a) + es(b))2(es(a) + es(c))(es(b) + es(c))(es(a) + es(b) + es(c))2 .\nIt remains to notice, trivially, that Haa ≥ 0 and HaaHbb − 14H2ab ≥ 0 for all possible values of s(a), s(b), s(c). The proof of the lemma is concluded."
    }, {
      "heading" : "5 Bandit Algorithm based on Projection and",
      "text" : "Decomposition\nIn this section, we propose another bandit algorithm OSMDRank, described in Algorithm 2. We will be working under the more restricted assumption that\nAlgorithm 2 Algorithm OSMDRank(n, η, γ, T ) (assuming ‖st‖1 ≤ 1 and π̂t ∈ Q̂n for all t )\n1: given: ground set size n, positive parameters η, γ (γ ≤ 1), time horizon T 2: let x1 = 0 ∈ Q̂n. (Note that x1 = argmina∈Q̂n F (a)) 3: for t = 1, . . . , T do 4: let x̃t = (1− γ)xt (Note that ãt ∈ Q̂n since the origin 0 and xt are in Q̂n and x̃t is a convex combination of them). 5: output πt = Decomposition(x̃t) (i.e., choose πt so that E[πt] = x̃t) and\nsuffer loss ℓt (= π ′ tst)\n6: let distribution Dt over [−1, 1]ndenote a mixture of the uniform distribution over the canonical basis with random sign (with probability γ) and a Radmacher distribution over {−1, 1}n with parameter (1 + xt,i)/2 for each i = 1, . . . , n (with probability 1− γ) 7: estimate the loss vector s̃t = ℓtP + t πt, where Pt = Eσ∼Dt [σσ\n′] 8: let xt+ 12 = ∇F\n∗(F (xt)− ηs̃t) 9: let xt+1 = Projection(xt+ 12 ) (that is, xt+1 = minx∈Q̂n DF (x, xt+ 12 ))\n10: end for\nsup ‖st‖1 ≤ 1 and sup ‖π̂t‖∞ ≤ 1. This in particular implies that |π̂′tst| ≤ 1, as before. But now we shall achieve a better expected regret of O(n √ T ).\nWe prefer, for reasons clarified shortly, to require that the actions π̂t are vertices of the rescaling Q̂n := 2 n−1 P̂n ∈ [−1, 1]n of the symmetrized permutahedron. That is, sup ‖π̂t‖∞ ≤ 1 (and sup ‖st‖1 ≤ 1). This will allow us to work with the following standard regularizer F : [−1, 1]n → R+: F (x) = 12 ∑n i=1 ((1 + x) ln(1 + x) + (1 − x) ln(1 − x)). The regularizer F (x) is the key to the OSMD (Online Stochastic Mirror Descent) algorithm of Bubeck et al. [5], on which our algorithm is based. OSMD is a bandit algorithm over the hypercube domain [−1, 1]n and a variant of Follow the Regularized Leader (FTRL, e.g., [8]) for linear loss functions. To apply this algorithm, we need a new projection and decomposition technique for the polytope Q̂n, as well as a slightly modified perturbation step in line 4 of Algorithm 2. Our algorithm OSMDRank has the following two procedures:\n1. Projection: Given a point xt ∈ [−1, 1]n, return argminyt∈Q̂n ∆F (yt, xt), where ∆F is the Bregman divergence defined wr.t. F , i.e., ∆F (y, x) = F (y)− F (x)−∇F (x)′(y − x) (also known as binary relative entropy).4\n2. Decomposition: Given yt ∈ Q̂n from the the projection step, output a random vertex π̂t of Q̂n such that E[π̂t] = yt.\nThe decomposition can be done using the technique of [15], which runs in O(n logn) time. (To be precise, the method there was defined for the standard\n4Note that the binary relative entropy is different from the relative entropy, where the relative entropy is defined as Rel(p, q) =\n∑n i=1 pi ln pi qi for probability distributions p and q\nover [n].\npermutahedron; The adjustments for the symmetrized version are trivial.) For notational purposes, we define f := ∇F , and notice that f(x)i = 12 ln 1+xi1−xi , and its inverse function f−1 is given by f−1(y)i = eyi−1 eyi+1 . Our projection procedure is presented in Algorithm 3.\nLemma 3. (i) Given q ∈ [−1, 1]n, Algorithm 3 outputs the projection of q onto Q̂n, with respect to the regularizer F . (ii) The time complexity of the algorithm is O(nτ(n) + n2), where τ(n) is the time complexity to perform step 4.\nskecth. Our projection algorithm is an extension of that in [13] and our proof follows a similar argument in [13]. For simplicity, we assume that elements in q are sorted in descending order, i.e., q1 ≥ q2 ≥ · · · ≥ qn. This can be achieved in time O(n log n) by sorting q. Then, it can be shown that projection preserves the order in q by using Lemma 1 in [13]. That is, the projection p of q satisfies p1 ≥ p2 ≥ · · · ≥ pn. So, if the conditions 2n−1 ∑i j=1 pj ≤ ∑i j=1( n+1 2 − j), for i = 1, . . . , n− 1, are satisfied, then other inequality constraints are satisfied as well since for any S ⊂ [n] such that |S| = i, ∑j∈S pj ≤ ∑i j=1 pj . Therefore, relevant constraints for projection onto Q̂n are only linearly many. By following a similar argument in [13], we can show that the output p indeed satisfies the KKT optimality conditions for projection, which completes the proof of the first statement. Finally, the algorithm terminates in time O(nτ(n)+ n2) since the number of iteration is at most n and each iteration takes O(n + τ(n)) time, which completes the second statement of the lemma.\nNote that with respect to other regularizers (e.g. relative entropy or Euclidean norm squared), a different projection scheme is possible in time O(n2) (see [15, 13] for the details). It is an open question whether an O(n2) algorithm can be devised with respect to the binary relative entropy we need here. In our case, we need to solve a numerical optimization problem by, say, binary search. Note that the time τ(n) is reasonably small: In fact, we can perform the binary search over the domain [−1, 1] for each dimension i. Therefore, if the precision is a fixed constant, the binary search ends in time O(n) for each dimension. In that case, τ(n) is O(n2). We are ready to present our main result for this section.\nTheorem 4. For η = O(n √ 1/T ) and γ = O( √ 1/T ), Algorithm OSMDRank has expected regret O(n √ T ) and running time O(n2 + nτ(n)) per step, where τ(n) is the time for a numerical optimization step depending on n. Additionally, there exists an algorithm with the same expected regret bound and running time with respect to the standard permutahedron (assuming ‖st‖1 ≤ 1/n).\nsketch. The algorithm OSMDRank is a modification of OSMD for the hypercube [−1, 1]n obtained by adding (1) a projection step and (2) a decomposition step. Standard techniques show that adding the projection step does not increase the expected regret bound (see, e.g., chapters 5 and 7 on OMD and OSMD of Bubeck’s lecture notes [4]). The key facts are: (i) A variant of Theorem 2 of [5] (regret bound of OSMD) holds for OSMD with Projection, (ii) E[πt] = (1−γ)xt,\nAlgorithm 3 Projection onto Q̂n 1: given (q1, . . . , qn) ∈ [−1, 1]n satisfying q1 ≥ q2 ≥ · · · ≥ qn. (This assumption\nholds by renaming the indices, and reverting to their original names at the end).\n2: set i0 = 0 3: for k = 1, . . . , n do 4: for each i = ik−1 + 1, . . . , n, set δki = minδ∈R δ subject to:\n∑i j=ik−1+1 f−1(f(qj)− δ) ≤ 2n−1 ∑i j=ik−1+1 ( n+1 2 − j ) .\n5: ik = argmaxi:ik−1<i≤n δ k i . In case of multiple minimizers, choose largest as ik. 6: set pj = f\n−1(f(qj)− δkik) for j = ik−1 + 1, . . . , ik 7: if ik = n, then break 8: end for 9: return (p1, . . . , pn) ′\nand (iii) The estimated loss is the same one used in OSMD for the hypercube [−1, 1]n . Once these three conditions are satisfied, we can prove a regret bound of OSMDRank by following the proof of Theorem 5 in Bubeck et al. [5]. In addition, the running time of OSMD per trial is O(n) [5]. Combining Lemma 3 for the projection and the analysis of the decomposition from [15], the proof of the first statement is concluded. The statement related to the standard permutahedron holds based on the affine transformation between the standard permutahedron and Q̂n."
    }, {
      "heading" : "6 Future Work",
      "text" : "The main open question is whether there is an algorithm of expected regret O(n √ T ) and time O(n3) in the setting of Section 4. Another interesting line of research is to study other ranking polytopes. For example, given any strictly monotonically increasing function f : R 7→ R we can consider as an action set fn(Sn), defined as f n(Sn) := {(f(π(1)), f(π(2)), . . . , f(π(n))) : π ∈ Sn}."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Ailon acknowledges the generous support of a Marie Curie Reintegration Grant PIRG07-GA-2010-268403, an Israel Science Foundation (ISF) grant 127/133 and a Jacobs Technion-Cornell Innovation Institute (JTCII) grant."
    }, {
      "heading" : "A Derivations in proof of Lemma 2",
      "text" : "By definition, and then by applying the properties of the distribution PLn(w),\nHaa = [p(a ≺ b ∧ a ≺ c|w) + p(b ≺ a ∧ c ≺ a|w) − p(b ≺ a ∧ a ≺ c|w)− p(c ≺ a ∧ a ≺ b|w)] − [p(a ≺ b|w)p(a ≺ c|w) + p(b ≺ a|w)p(c ≺ a|w)− p(a ≺ b|w)p(c ≺ a|w)\n−p(a ≺ c|w)p(b ≺ a|w)] (A.1)\np(a ≺ b ∧ a ≺ c|w) = e s(a)\nes(a) + es(b) + es(c) (A.2)\np(b ≺ a ∧ c ≺ a|w) = e s(b) es(a) + es(b) + es(c) es(c) es(a) + es(c) +\nes(c) es(a) + es(b) + es(c) es(b) es(a) + es(b)\n(A.3)\np(b ≺ a ∧ a ≺ c|w) = e s(b) es(a) + es(b) + es(c) es(a) es(a) + es(c) (A.4) p(c ≺ a ∧ a ≺ b|w) = e s(c)\nes(a) + es(b) + es(c) es(a) es(a) + es(b) (A.5)\nPlugging (A.2)-(A.5) in (A.1) and simplifying results in (4.10). One now verifies:\nHab = [p(a ≺ c ∧ b ≺ c|w) + p(c ≺ a ∧ c ≺ b|w)− 3p(a ≺ c ∧ c ≺ b|w)− 3p(b ≺ c ∧ c ≺ a|w)] − [−p(a ≺ b|w)p(a ≺ c|w) − p(b ≺ a|w)p(c ≺ a|w) + p(a ≺ b|w)p(c ≺ a|w)\n+ p(a ≺ b|w)p(a ≺ c|w) + p(a ≺ b|w)p(b ≺ c|w) + p(b ≺ a|w)p(c ≺ b|w) − p(b ≺ a|w)p(b ≺ c|w) − p(a ≺ b|w)p(c ≺ b|w) + p(a ≺ b|w)p(b ≺ c|w) +p(b ≺ a|w)p(c ≺ b|w) − p(b ≺ a|w)p(b ≺ c|w) − p(a ≺ b|w)p(c ≺ b|w) −p(a ≺ c|w)p(b ≺ c|w) − p(c ≺ a|w)p(c ≺ b|w) + p(a ≺ c|w)p(c ≺ b|w)\n+p(c ≺ a|w)p(b ≺ c|w)]\nAgain using identities (A.2)-(A.5) and simplifying, gives (4.11)"
    } ],
    "references" : [ {
      "title" : "Improved Bounds for Online Learning Over the Permutahedron and Other Ranking Polytopes",
      "author" : [ "Nir Ailon" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "Assessing the potential demand for electric cars",
      "author" : [ "S Beggs", "S Cardell", "J Hausman" ],
      "venue" : "Journal of Econometrics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1981
    }, {
      "title" : "Introduction to Online Optimization",
      "author" : [ "Sébastien Bubeck" ],
      "venue" : "http://www.princeton.edu/~bubeck/BubeckLectureNotes.pdf,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Towards Minimax Policies for Online Linear Optimization with Bandit Feedback",
      "author" : [ "Sébastien Bubeck", "Nicolò Cesa-Bianchi", "Sham M. Kakade" ],
      "venue" : "In Proceedings of 25th Annual Conference on Learning Theory (COLT",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Combinatorial bandits",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "The price of bandit information for online optimization",
      "author" : [ "Varsha Dani", "Thomas P. Hayes", "Sham Kakade" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "The convex optimization approach to regret minimization",
      "author" : [ "Elad Hazan" ],
      "venue" : "Optimization for Machine Learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Volumetric spanners and their applications to machine learning",
      "author" : [ "Elad Hazan", "Zohar Shay Karnin", "Raghu Mehka" ],
      "venue" : "CoRR, abs/1312.6214,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Learning Permutations with Exponential Weights",
      "author" : [ "David P. Helmbold", "Manfred K. Warmuth" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "A polynomial-time approximation algorithm for the permanent of a matrix with nonnegative entries",
      "author" : [ "Mark Jerrum", "Alistair Sinclair", "Eric Vigoda" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2004
    }, {
      "title" : "Analyzing and Modeling Rank Data",
      "author" : [ "John I. Marden" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1995
    }, {
      "title" : "Online Prediction under Submodular Constraints",
      "author" : [ "Daiki Suehiro", "Kohei Hatano", "Shuji Kijima", "Eiji Takimoto", "Kiyohito Nagano" ],
      "venue" : "In Proceedings of 23th Annual Conference on Algorithmic Learning Theory (ALT",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "The complexity of computing the permanent",
      "author" : [ "Leslie G. Valiant" ],
      "venue" : "Theor. Comput. Sci.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1979
    }, {
      "title" : "Online Linear Optimization over Permutations",
      "author" : [ "Shota Yasutake", "Kohei Hatano", "Shuji Kijima", "Eiji Takimoto", "Masayuki Takeda" ],
      "venue" : "In Proceedings of the 22nd International Symposium on Algorithms and Computation (ISAAC",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "The relationship between Luce’s choice axiom, Thurstone’s theory of comparative judgment, and the double exponential distribution",
      "author" : [ "J. Yellott" ],
      "venue" : "Journal of Mathematical Psychology,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1977
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Our algorithm, BanditRank, plays permutations from a distribution known as the Plackett-Luce model (see [12]) which is widely used in statistics and econometrics (see eg [3]).",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "Our algorithm, BanditRank, plays permutations from a distribution known as the Plackett-Luce model (see [12]) which is widely used in statistics and econometrics (see eg [3]).",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 5,
      "context" : "It uses an inverse covariance matrix of the distribution in order to obtain an unbiased loss vector estimator, which is a standard technique [6].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "This result should be compared to CombBand of [6], where a framework for playing bandit games over combinatorially structured sets was developed.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "Their techniques extend that of [7].",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 13,
      "context" : "Unfortunately, nonnegative permanent computation is #P -hard, as shown by [14].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "groundbreaking result of [11] presents a polynomial time approximation scheme for permanent, which runs in time O(n) for fixed accuracy.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : "[9] have improved the state-of-the-art general purpose algorithm for linear bandit optimization, implying an algorithm with regret O(n √ T ) for our problem, but with worse running time Õ(n).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "We present and analyze an algorithm OSMDRank based on the bandit algorithm OSMD of [5] with projection and decomposition techniques over the permutahedron ([15, 13]).",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "We present and analyze an algorithm OSMDRank based on the bandit algorithm OSMD of [5] with projection and decomposition techniques over the permutahedron ([15, 13]).",
      "startOffset" : 156,
      "endOffset" : 164
    }, {
      "referenceID" : 12,
      "context" : "We present and analyze an algorithm OSMDRank based on the bandit algorithm OSMD of [5] with projection and decomposition techniques over the permutahedron ([15, 13]).",
      "startOffset" : 156,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : "[10] were the first to study a more general version of this problem, where the action set is the vertex set of the Birkhoff-von-Neumann polytope (doubly-stochastic matrices).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] studied the problem by casting it as a submodularly constrained optimization problem, giving near optimal regret bounds, and more recently Ailon [1] both provided optimal regret bounds with improved running time and established tight regret lower bounds.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "[13] studied the problem by casting it as a submodularly constrained optimization problem, giving near optimal regret bounds, and more recently Ailon [1] both provided optimal regret bounds with improved running time and established tight regret lower bounds.",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 11,
      "context" : "An important well known property of the distribution is that it can be equivalently defined as a Random Utility Model (RUM) [12, 16]: To draw a permutation, add a random iid noise variable following the Gumbel distribution to each weight, and then sort the items of V in decreasing value of noisy-weights.",
      "startOffset" : 124,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "An important well known property of the distribution is that it can be equivalently defined as a Random Utility Model (RUM) [12, 16]: To draw a permutation, add a random iid noise variable following the Gumbel distribution to each weight, and then sort the items of V in decreasing value of noisy-weights.",
      "startOffset" : 124,
      "endOffset" : 132
    }, {
      "referenceID" : 5,
      "context" : "’s CombBand [6], which is itself an adaptation of Auer et al.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 1,
      "context" : "’s Exp3 [2] from the finite case to the structured combinatorial case.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 11,
      "context" : "[12].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "We refer to [12] for definition and history of the BradleyTerry-Luce model.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 5,
      "context" : "The proof of the theorem proceeds roughly as the main result upper bounding the expected regret of CombBand in [6].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "We now note that (1) ∑ π̂∈Ŝ qt(π̂|wt−1)l̃t = lt (following the properties of matrix pseudo-inverse in Line 7 in Algorithm 1), and (2) ∑ π̂∈Ŝn qt(π̂|wt−1)l̃t(π) ] ≤ n (see top of page 31 together with Lemma 15 in [6]).",
      "startOffset" : 212,
      "endOffset" : 215
    }, {
      "referenceID" : 11,
      "context" : "9) Since Plackett-Luce is a random utility model (see [12]), it is clear that whenever a pair of pairs u 6= v, u′ 6= v′ satisfies |{u, v, u′, v′}| = 4, p(u ≺ v ∧ u′ ≺ v′|w) = p(u ≺ v|w)p(u′ ≺ v′|w).",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 4,
      "context" : "[5], on which our algorithm is based.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : ", [8]) for linear loss functions.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 14,
      "context" : "The decomposition can be done using the technique of [15], which runs in O(n logn) time.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "Our projection algorithm is an extension of that in [13] and our proof follows a similar argument in [13].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "Our projection algorithm is an extension of that in [13] and our proof follows a similar argument in [13].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 12,
      "context" : "Then, it can be shown that projection preserves the order in q by using Lemma 1 in [13].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "By following a similar argument in [13], we can show that the output p indeed satisfies the KKT optimality conditions for projection, which completes the proof of the first statement.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 14,
      "context" : "relative entropy or Euclidean norm squared), a different projection scheme is possible in time O(n) (see [15, 13] for the details).",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 12,
      "context" : "relative entropy or Euclidean norm squared), a different projection scheme is possible in time O(n) (see [15, 13] for the details).",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 3,
      "context" : ", chapters 5 and 7 on OMD and OSMD of Bubeck’s lecture notes [4]).",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "The key facts are: (i) A variant of Theorem 2 of [5] (regret bound of OSMD) holds for OSMD with Projection, (ii) E[πt] = (1−γ)xt, 13",
      "startOffset" : 49,
      "endOffset" : 52
    } ],
    "year" : 2014,
    "abstractText" : "The permutahedron is the convex polytope with vertex set consisting of the vectors (π(1), . . . , π(n)) for all permutations (bijections) π over {1, . . . , n}. We study a bandit game in which, at each step t, an adversary chooses a hidden weight weight vector st, a player chooses a vertex πt of the permutahedron and suffers an observed instantaneous loss of ∑n i=1 πt(i)st(i). We study the problem in two regimes. In the first regime, st is a point in the polytope dual to the permutahedron. Algorithm CombBand of Cesa-Bianchi et al (2009) guarantees a regret of O(n √ T log n) after T steps. Unfortunately, CombBand requires at each step an n-by-n matrix permanent computation, a #P -hard problem. Approximating the permanent is possible in the impractical running time of O(n), with an additional heavy inverse-polynomial dependence on the sought accuracy. We provide an algorithm of slightly worse regret O(n √ T ) but with more realistic time complexity O(n) per step. The technical contribution is a bound on the variance of the Plackett-Luce noisy sorting process’s ‘pseudo loss’, obtained by establishing positive semi-definiteness of a family of 3-by-3 matrices of rational functions in exponents of 3 parameters. In the second regime, st is in the hypercube. For this case we present and analyze an algorithm based on Bubeck et al.’s (2012) OSMD approach with a novel projection and decomposition technique for the permutahedron. The algorithm is efficient and achieves a regret of O(n √ T ), but for a more restricted space of possible loss vectors.",
    "creator" : "LaTeX with hyperref package"
  }
}