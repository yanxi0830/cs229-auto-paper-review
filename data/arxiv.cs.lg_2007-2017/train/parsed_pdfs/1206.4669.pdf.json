{
  "name" : "1206.4669.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Sparse Additive Functional and Kernel CCA",
    "authors" : [ "Sivaraman Balakrishnan", "Kriti Puniyani", "John Lafferty" ],
    "emails" : [ "sbalakri@cs.cmu.edu", "kpuniyan@cs.cmu.edu", "lafferty@galton.uchicago.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Canonical correlation analysis (Hotelling, 1936), is a classical method for finding correlations between the components of two random vectors X ∈ Rp1 and Y ∈ Rp2 . Given a set of n paired observations (X1, Y1), . . . , (Xn, Yn), we form the design matrices X ∈ Rn×p1 and Y ∈ Rn×p2 and find vectors u ∈ Rp1\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nand v ∈ Rp2 that are solutions to the optimization\nargmax u,v\n1 n uTXTYv (1)\ns.t. 1 n uTXTXu ≤ 1 1 n vTYTYv ≤ 1,\nwhere the columns of X and Y have been standardized to have mean zero and standard deviation one. This is the sample version of the problem of maximizing the correlation between the linear combinations uTX and vTY , assuming the random variables have mean zero.\nCCA can serve as a valuable dimension reduction tool, allowing one to quickly zoom in on interesting phenomena shared by multiple data sets. This tool is increasingly attractive in genomic data analysis, where researchers perform multiple assays per item. For instance, data including DNA copy number (or comparative genomic hybridization, CGH), gene expression, and single nucleotide polymorphism (SNP) information can be collected on a common set of patients. Witten et al. (2009) present examples of recent studies involving such data.\nWhen the data are high dimensional, as is often the case for genomic data, the classical formulation of CCA is not meaningful, since the sample covariance matrices XTX and YTY are singular. This has motivated different approaches to sparse CCA, which regularizes (1) by suitable sparsity-inducing ℓ1 penalties (Witten et al., 2009; Witten & Tibshirani, 2009; Parkhomenko et al., 2007; Chen & Liu, 2012). Sparsity can lead to more interpretable models, reduced computational cost, and favorable statistical properties for high dimensional data. Existing methods for CCA are, however, restricted in that they attempt to find linear combinations of the variables—interesting correlations need not be linear. The need for this flexibility motivates the nonparametric approaches we consider in this paper.\nThe general nonparametric analogue of (1) is\nargmax f,g\n1\nn\nn∑\ni=1\nf(Xi)g(Yi) (2)\ns.t. 1\nn\nn∑\ni=1\nf2(Xi) ≤ 1 1\nn\nn∑\ni=1\ng2(Yi) ≤ 1\nwhere f and g are restricted to belong to an appropriate class of smooth functions. Bach & Jordan (2003) introduce a version of this called kernel CCA by applying the “kernel trick” to the CCA problem. Kernel CCA allows flexible nonparametric modeling of correlations, solving (2) with additional regularization to enforce smoothness of the functions f and g in appropriate reproducing kernel Hilbert spaces. However, this general nonparametric model suffers from the curse of dimensionality, as the number of samples required for consistency grows exponentially with the dimension. It is thus necessary to further restrict the complexity of possible functions. We consider the class of additive models which can be written as\nf(x1, x2, . . . , xp1) =\np1∑\nj=1\nfj(xj) (3)\ng(y1, y2, . . . , yp2) =\np2∑\nk=1\ngk(yk) (4)\nin terms of univariate component functions (Hastie & Tibshirani, 1986). In the regression setting, such models no longer require the sample size to be exponential in the dimension; however, they only have strong statistical properties in low dimensions. Recently, several authors have shown how sparse additive models for regression can be efficiently estimated even when p > n (Ravikumar et al., 2009; Koltchinskii & Yuan, 2010; Meier et al., 2009; Raskutti et al., 2010).\nIn this paper we propose two additive nonparametric formulations of CCA, one over a family of RKHSs and another over Sobolev spaces without a reproducing kernel. In the low-dimensional setting where we do not enforce sparsity, the formulation over Sobolev spaces is closely related to the Alternating Conditional Expectations (ACE) formulation of nonparametric regression due to Breiman & Friedman (1985). In addition to formulating algorithms for the optimizations, we provide risk consistency guarantees for the global risk minimizer in the high dimensional regime where min(p1, p2) > n.\nAn important consideration is that sparse nonparametric CCA is biconvex, but not jointly convex in f and g. This is true even for the linear CCA model,\nwhich is a special case of the model we propose. In the absence of the sparsity constraints the linear problem reduces to a generalized eigenvalue problem which can be efficiently solved. This remains true in the nonparametric case as well. Over an RKHS, the problem without sparsity is a generalized eigenvalue problem where Gram matrices replace the data covariance matrices. In the population setting over the Sobolev spaces we consider, Breiman & Friedman (1985) show that the problem reduces to an eigenvalue problem with respect to conditional expectation operators.\nReturning to the nonconvex sparse CCA problem, Witten et al. (2009) and Parkhomenko et al. (2007) suggest using the solution to the nonsparse version of the problem to initialize sparse CCA; Chen & Liu (2012) use several random initializations. As we show in simulations, both approaches can lead to poor results, even in the linear case. To address this issue, we propose and study a simple marginal thresholding step to reduce the dimensionality, in the spirit of the diagonal thresholding of Johnstone & Lu (2009) and the SURE screening of Fan & Song (2010). This results in a three step procedure where after preprocessing we use the nonsparse version of our problem to determine a good initialization for the sparse formulation.\nIn Sections 2 and 3 we briefly describe the additive Sobolev and RKHS function spaces over which we work, introduce our two nonparametric CCA formulations, and discuss their optimization. In Section 4 we address the non-convexity of the formulations and initialization strategies. In Section 5 we summarize the theoretical guarantees of these procedures when p1, p2 > n and in Section 6 we describe some simulations and real data experiments."
    }, {
      "heading" : "2. Sparse additive kernel CCA",
      "text" : "Recall the linear CCA problem (1). We will now derive its additive generalization over RKHSs. Let Fj ⊂ L2(µ(xj)) be a reproducing kernel Hilbert space of univariate functions on the domain of Xj , and let Gk ⊂ L2(µ(yk)) be a reproducing kernel Hilbert space of univariate functions on the domain Yk, for each j = 1, . . . , p1 and k = 1, . . . , p2. We assume that E[fj(Xj)] = 0 and E[gk(Yk)] = 0 for all fj ∈ Fj , and gk ∈ Gk for each j and k. This is necessary to enforce model identifiability. In practice, we will always work with centered Gram matrices to enforce this (see Bach & Jordan (2003)). Denote by F = {f = ∑p1j=1 fj(xj)|fj ∈ Fj} and G = {g = ∑p2k=1 gk(yk)|gk ∈ Gk} the sets of additive functions of x and y, respectively.\nWe are given n independent tuples of the form (Xi, Yi) n i=1 where Xi = {Xi1, . . . , Xip1} and Yi = {Yi1, . . . , Yip2}, and positive definite kernel functions on each covariate of X and Y . We denote the Gram matrix for the jth X covariate by Kxj and for the k th Y covariate by Kyk.\nWe will need to regularize the CCA problem to enforce smoothness and sparsity of the functions. The two norms\n‖fj‖Fj = √ 〈fj , fj〉Fj ‖fj‖2 = √ 1 n ∑n i=1 f 2 j (Xij)\nplay an important role in our approach. We can now formulate the sparse additive kernel CCA (SAKCCA) problem as\nmax f∈F,g∈G\n1\nn\nn∑\ni=1\nf(Xi)g(Yi) subject to\n(5)\n1\nn\nn∑\ni=1\nf2(Xi) + γf\np1∑\nj=1\n‖fj‖2Fj ≤ 1 p1∑\nj=1\n‖fj‖2 ≤ Cf\n1\nn\nn∑\ni=1\ng2(Yi) + γg\np2∑\nk=1\n‖gk‖2Gk ≤ 1 p2∑\nk=1\n‖gk‖2 ≤ Cg.\nfor given regularization parameters γf , γg, Cf and Cg. As with the group LASSO, constraining∑\nj ‖fj‖2 encourages sparsity amongst the functions fj Ravikumar et al. (2009). As stated, this is an infinite dimensional optimization problem over Hilbert spaces. However, a straightforward application of the representer theorem shows that it is equivalent to the following finite dimensional optimization problem:\nmax α,β\n1\nn\n\n\np1∑\nj=1\nKxjαj\n\n\n( p2∑\nk=1\nKykβk\n)\nsubject to\n(6)\n1\nn\n\n\np1∑\nj=1\nKxjαj\n\n\nT \n\np1∑\nj=1\nKxjαj\n\n+ γf\np1∑\nj=1\nαTj Kxjαj ≤ 1\n1\nn\n( p2∑\nk=1\nKykβk\n)T ( p2∑\nk=1\nKykβk\n)\n+ γg\np2∑\nk=1\nβTk Kykβk ≤ 1\np1∑\nj=1\n√\n1 n αTj K T xjKxjαj ≤ Cf ,\np2∑\nk=1\n√\n1 n βTk K T ykKykβk ≤ Cg.\nHere α is an (n× p1) matrix, αj is its jth column, β is an (n× p2) matrix and βk is its kth column. The problem (6) is not convex. However, if we fix the function g (or equivalently the coefficients β) the problem is convex in f (equivalently α), and vice-versa.\nThis biconvexity leads to a natural optimization strategy for (6) which we describe below. However, this procedure only guarantees convergence to a local optimum and in practice we still need to be able to find a good initialization.\nIn the absence of the sparsity penalty the problem becomes an additive form of kernel CCA (Bach & Jordan, 2003). One could also consider alternative formulations that, for instance, separate the smoothness and variance constraints. One attractive feature of our formulation is that without the sparsity constraint the problem can be reduced to a generalized eigenvalue computation which can be solved optimally. This leads us to a strategy of biconvex optimization that mirrors the linear algorithm of Witten et al. (2009); specifically, initialize by solving the problem without the sparsity constraints, fix α and optimize for β and vice-versa until convergence. As our experiments will show this is indeed a good strategy when p1, p2 < n. However, new ideas, to be described in Section 4, are necessary to scale this to the high dimensional setting where p1, p2 > n."
    }, {
      "heading" : "3. Sparse additive functional CCA",
      "text" : "We now formulate an optimization problem for sparse additive functional CCA (SA-FCCA), and derive a scalable backfitting procedure for this problem. Here we work directly over the Hilbert spaces L2(µ(x)) and L2(µ(y)). We will denote by Sj the subspace of µ(xj) measurable functions with mean 0, with the usual inner product 〈fj , f ′j〉 = E ( fj(Xj)f ′ j(Xj) ) , and similarly Tk for the functions of y. To enforce smoothness we consider functions lying in a ball in a second order Sobolev space. We further assume the functions are uniformly bounded, and the measures µ are supported on a compact subset of a Euclidean space with Lebesgue measure λ. For a fixed uniformly bounded, orthonormal basis ψjk with respect to λ we have\nFj = { fj ∈ Sj : fj =\n∞∑\nk=0\nβjkψjk, ∞∑\nk=0\nβ2jkk 4 ≤ C2\n}\nand similarly for Gk. We will call these the smooth functions, and denote by F and G the set of smooth additive functions over the respective Hilbert spaces.\nOur formulation of sparse additive functional CCA is the optimization\nmax f∈F, g∈G\n1\nn\nn∑\ni=1\nf(Xi)g(Yi) (7)\ns.t. 1\nn\np1∑\nj=1\nn∑\ni=1\nf2j (Xij) ≤ 1, p1∑\nj=1\n‖fj‖2 ≤ Cf\n1\nn\np2∑\nk=1\nn∑\ni=1\ng2k(Yik) ≤ 1, p2∑\nk=1\n‖gk‖2 ≤ Cg\nwhere the ‖.‖2 norm is defined as in additive kernel CCA. This problem is superficially similar to (2); however, there are three important differences. First, we don’t regularize for smoothness but instead work directly over a Sobolev space of smooth functions. Secondly, we do not constrain the variance of the function f . Instead, in the spirit of “diagonal penalized CCA” of Witten et al. (2009) we constrain the sum of the variances of the individual fjs. This choice is made primarily because it leads to backfitting updates that have a particularly simple and intuitive form. Perhaps most importantly, we can no longer appeal to the representer theorem since we are not working over RKHSs.\nWe study the population version of this problem to derive a biconvex backfitting procedure to directly optimize this criterion. The sample version of the algorithm is described in Algorithm 1, and a complete derivation is part of the supplementary material. To gain some intuition for this procedure we describe one special case of the population algorithm, where g is fixed and both constraints on f are tight. Consider the Lagrangian problem\nmax f min λ≥0,γ≥0\nE[f(X)g(Y )]− λ(‖f‖22 − 1)− γ(‖f‖1 − Cf ).\nThe norms are defined as ‖f‖1 = ∑p1\nj=1\n√\nE(f2j (xj))\nand ‖f‖22 = ∑p1 j=1 E(f 2 j (xj)). For simplicity, consider the case when λ, γ > 0, and denote a ≡ g(Y ). We now can derive a coordinate ascent style procedure where we optimize over fj holding the other functions fixed. The Fréchet derivative w.r.t. fj in the direction η gives one of the KKT conditions E[(a− 2λfj − γνj)η] = 0 for all η in the Hilbert space Hj , where the subdifferential is νj =\nfj√ E(f2 j ) if √ E(f2j ) is not 0, and\nis the set {uj ∈ Hj |E(u2j ) ≤ 1} if √ E(f2j ) = 0.\nUsing iterated expectations the KKT condition can be written as E[(E(a |Xj) − 2λfj − γνj)η] = 0. Denote E(a |Xj) ≡ Pj . In particular, if we consider η = E[(E(a |Xj) − 2λfj − γνj ], we can see that E[(E(a |Xj)−2λfj −γνj)] = 0, i.e., E(a |Xj)−2λfj − γνj = 0 almost everywhere. Then if √\nE(P 2j ) ≤ γ, we have fj = 0, and we arrive\nat the following soft thresholding update:\nfj = 1\n2λ\n\n1− γ√ E(P 2j )\n\n\n+\nPj .\nNow, going back to the constrained version, we need to select γ and λ so that the two constraints are tight. To get the sample version of this update we replace the conditional expectation Pj by an estimate Sja, where Sj is a locally linear smoother.\nAlgorithm 1 Biconvex backfitting for SA-FCCA input {(Xi, Yi)}, parameters Cf , Cg, initial g(Yi)\n1. Compute smoothing matrices Sj and Tk. 2. Fix g. For each j, set fj ← Sjgλ where λ =√ ∑p1\nj=1(g TSTj Sjg)\n3. if ∑p1\nj=1 ‖fj‖2 ≤ Cf , break else let Fm denote the functions with maximum ‖.‖2 norm. Set all other functions to 0. For each f ∈ Fm, set f ← Cff|Fm|‖f‖2 . If∑p1\nj=1 ‖fj‖22 ≤ 1, break\nelse set fj ← (\n1− γ√ ‖Sjg‖2\n)\n+\nSjg λ where λ =\n√\n∑p1 j=1 ∥ ∥ ∥\n(\n1− γ√ ‖Sjg‖2\n)\n+\nSjg ∥ ∥ ∥ 2\n2 and γ is cho-\nsen so that ∑p1\nj=1\n√\ngTSTj Sjg = Cf\n4. Center by setting each fj ← fj −mean(fj). 5. Fix f and repeat above to update g. Iterate\nboth updates till convergence.\noutput Final functions f , g"
    }, {
      "heading" : "4. Marginal Thresholding",
      "text" : "The formulations of SA-KCCA and SA-FCCA above are not jointly convex, but are biconvex. Hence, iterative optimization algorithms may not be guaranteed to reach the globally optimal solution. To address this issue, we first run the algorithms without any sparsity constraint. The resulting nonsparse collections of functions are then used as initializations for the algorithm that incorporates the sparsity penalties. While such initialization works well for low dimensional problems, as p increases, the performance of the estimator goes down (Figure 1). To extend the algorithms to the high dimensional scenario, we propose marginal thresholding as a screening method to reject irrelevant variables and run the SA-FCCA and SA-KCCA models on the reduced dimensionality problem. For each pair\nof variables Xi and Yj , we fit marginal functions to that pair by optimizing the criteria in either Equation (6) or Equation (7) without the sparsity constraints since we only consider one X and one Y covariate at a time. We then compute the correlation on held out data. This constructs a matrix M of size p1 × p2 with (i, j) entry of the matrix representing an estimate of the marginal correlation between fi(Xi) and gj(Yj). We then threshold the entries of M to obtain a subset of variables on which to run SA-FCCA and SAKCCA. Theorem 5.3 discusses the theoretical properties of marginal thresholding as a screening procedure, and Section 6.2 presents results on marginal thresholding for high dimensional problems."
    }, {
      "heading" : "5. Main theoretical results",
      "text" : "In this section we will characterize both the functional and kernel marginal thresholding procedures and study the theoretical properties of the estimators (6) and (7). We will state the main theorems and defer all proofs to the supplementary material.\nThe theoretical characterization of these procedures relies on uniform large deviation inequalities for the covariance between functions. For simplicity in this section we will assume all the univariate spaces are identical. In the RKHS case we restrict our attention to functions in a ball of a constant radius in the Hilbert space associated with a reproducing kernel K. In the functional case the univariate space is a second order Sobolev space where the integral of the square of the second derivative is bounded by a constant. With some abuse of notation we will denote these spaces C. We are interested in controlling the quantity\nΘn = sup fj ,gk\n∣ ∣ ∣ ∣ ∣ 1 n n∑\ni=1\nfj(Xij)gk(Yik)− E(fj(Xj)gk(Yk)) ∣ ∣ ∣ ∣ ∣\nwhere fj , gk ∈ C, j ∈ {1, . . . , p1}, k ∈ {1, . . . , p2}. All results extend to the case when each covariate is endowed with a possibly distinct function space.\nLemma 5.1 (Uniform bound over RKHS) Assume supx |K(x, x)| ≤ M < ∞, for functions fj(x) = ∑n i=1 αijKx(x,Xij), gk(y) =\n∑n i=1 βikKy(y, Yik)\nP\n\n   Θn ≥ ζ + C\n√\nlog ((p1p2)/δ)\nn ︸ ︷︷ ︸\nǫ\n\n   ≤ δ\nwhere C is a constant depending only on M , and ζ = maxj,k 2 nEX∼xj ,Y∼yk √∑n i=1 K(Xij , Xij)K(Yik, Yik)\nNote that ζ is independent of the dimensions p1 and p2 and that under the assumption that K is bounded, ζ = O(1/ √ n). In some cases however this term can be much smaller. The second term depends only logarithmically on p1 and p2 and this weak dependence is the main reason our proposed procedures are consistent even when p1, p2 > n.\nLemma 5.2 (Uniform bound for Sobolev spaces) Assume ‖f‖∞ ≤ M ≤ ∞, then\nP\n\n   Θn ≥ C1√ n + C2\n√\nlog ((p1p2)/δ)\nn ︸ ︷︷ ︸\nǫ\n\n   ≤ δ\nwhere C1 and C2 depend only on M .\nLemma 5.1 is proved via a Rademacher symmetrization argument of Bartlett & Mendelson (2002) (see also Gretton et al. (2004)) while Lemma 5.2 is based on a bound on the bracketing integral of the Sobolev space (see Ravikumar et al. (2009)). The Rademacher bound gives a distribution dependent bound which can in some cases lead to faster rates.\nWe are now ready to characterize the marginal thresholding procedure described in Section 4. To study marginal thresholding we need to define relevant and irrelevant covariates. For each covariate Xj , denote\nαj = sup fj ,gk∈C,k∈{1,...,p2} E(fj(Xj)gk(Yk))\nwith E(f2j ) ≤ 1,E(g2k) ≤ 1. A covariate Xj is considered irrelevant if αj = 0 and relevant if αj > 0. Similarly, for each Yk we associate βk defined analogously.\nNow, assume that for every pair of covariates, we find the maximizer of the SA-FCCA or SA-KCCA objective over the given sample, over the appropriate class C and with E(f2j ) ≤ 1,E(g2k) ≤ 1. Recall that for marginal thresholding we do not enforce sparsity. The global maximization of the SA-KCCA objective can be efficiently carried out since it is equivalent to a generalized eigenvalue problem. For SA-FCCA however, the backfitting procedure is only guaranteed to find the global maximizer in the population setting.\nTheorem 5.3 Given P (Θn ≥ ǫ) ≤ δ.\n1. With probability at least 1−δ, marginal thresholding at ǫ has no false inclusions. 2. Further, if we have that αj or βk ≥ 2ǫ then under the same 1 − δ probability event marginal thresholding at ǫ correctly includes the relevant covariate Xj or Yk.\nThe importance of Lemmas 5.1 and 5.2 is that they provide values at which to threshold the marginal covariances. In particular, notice that the minimum sample covariance that can be reliably detected, with no false inclusions, falls rapidly with n and approaches zero even when p1, p2 > n.\nIn the spirit of early results on the LASSO of Juditsky & Nemirovski (2000); Greenshtein & Ritov (2004) we will establish the risk consistency or persistence of the empirical maximizers of the two objectives. Although we cannot guarantee that we find these empirical maximizers due to the non-convexity this result shows that with good initialization the formulations (6) and (7) can lead to solutions which have good statistical properties in high dimensions.\nFor SA-KCCA we will assume that our algorithm maximizes\n1\nn\nn∑\ni=1\n\n\np1∑\nj=1\nµjfj(Xij)\n\n\n[ p2∑\nk=1\nγkgk(Yik)\n]\nover the classes\nF = { f :f(x) = p1∑\nj=1\nµjfj(xj),Efj = 0,Ef 2 j = 1,\n‖µ‖1 ≤ Cf , ‖µ‖22 + γf p1∑\nj=1\n‖fj‖2H ≤ 1 }\nG = { g :g(x) = p2∑\nk=1\nγkgk(yk),Egk = 0,Eg 2 k = 1,\n‖γ‖1 ≤ Cg, ‖γ‖22 + γg p2∑\nk=1\n‖fk‖2H ≤ 1 }\nand for SA-FCCA we will assume that our algorithm maximizes the same objective over the same class without the RKHS constraint but which are instead in a Sobolev ball of constant radius. Denote these solutions (f̂ , ĝ).\nWe will compare to an oracle which maximizes the population covariance\ncov(f, g) ≡ E\n\n\np1∑\nj=1\nµjfj(xj)\n\n\n[ p2∑\nk=1\nγkgk(yk)\n]\nDenote this maximizer by (f∗, g∗). Our main result will show that these procedures are persistent, i.e., cov(f∗, g∗)− cov(f̂ , ĝ) → 0 even if p1, p2 > n.\nTheorem 5.4 (Persistence) If p1p2 ≤ en ξ for some ξ < 1 and CfCg = o(n (1−ξ)/2), then SA-FCCA and SA-KCCA are persistent over their respective function classes."
    }, {
      "heading" : "6. Experiments",
      "text" : ""
    }, {
      "heading" : "6.1. Non-linear correlations",
      "text" : "We compare SA-FCCA and SA-KCCA with two models, sparse additive linear CCA (SCCA) (Witten et al., 2009) and kernel CCA (KCCA) (Bach & Jordan, 2003). Figure 1 shows the performance of each model, when run on data with n = 150 samples in p1 = 15, p2 = 15 dimensions, where only one relevant variable is present in X and Y (the remaining dimensions are Gaussian random noise). We report two metrics to measure whether the correct correlations are being captured by the different methods - (a) test correlation on 200 samples, using the estimated functions, and (b) precision and recall in identifying the correct variables involved in the correlation estimation. Each result is averaged over 10 repeats of the experiment. SinceKCCA uses all data dimensions in finding correlations, its precision and recall are not reported.\nWhen the relationship between the relevant variables is linear, all methods identify the correct variables and have high test correlation. While KCCA should be able to identify non-linear correlations, since it is strongly affected by the curse of dimensionality, it has poor test correlation even in p = 15 dimensions.\nBoth SA-FCCA and SA-KCCA correctly identify the relevant variables in all cases, and have high test correlation."
    }, {
      "heading" : "6.2. Marginal thresholding",
      "text" : "We now test the efficiency of marginal thresholding by running an experiment for n = 150, p1 = 150, p2 = 150. We generate multiple relevant variables as: fi(Xi) = cos (π\n2 Xi\n)\n, i ∈ {1, 3}, fi(Xi) = X2i , i ∈ {2, 4}\nYj =\n4∑\ni=1;i6=j\nfi(Xi) +N (0, 0.12) j ∈ {1, 2, 3, 4}\nThus, there are four relevant variables in each data set. X and Y are sampled from a uniform distribution, and standardized before computing fi(Xi). Each fi(Xi) is\nalso standardized before computing Yj . We repeat the experiment by generating data 10 times, and report results in Table 2. Bandwidth in the different methods was selected using a plug-in estimator of the median distance between points in a single dimension. The sparsity and smoothness parameters for all methods were tuned using permutation tests, as described in Witten et al. (2009), assuming that Cf = Cg = C, and γf = γg = γ.\nWe ran marginal thresholding by splitting the data into equal sized train and held out data, fitting marginal functions on the train data, computing functional correlation on the held out data, and picking a threshold so that n/5 elements of the thresholded correlation matrix are non-zero. We found that in all experiments, marginal thresholding always selected the relevant variables for the subsampled data. Table 2 shows the precision, recall and test correlations for the different methods. As can be expected, SA-FCCA and SA-KCCA are able to correctly identify the relevant variables, and the estimated functions have high correlation on test data.\nWe visualize the effect of the parameter tuning by plotting regularization paths, as the sparsity parameter is varied (n=100, p1=p2=12). For SA-FCCA and SAKCCA, the norm of each function is plotted, and for sparse linear CCA, the absolute values of the entries of u and v are shown. Figure 3 shows how, unlike SCCA, SA-FCCA and SA-KCCA are able to separate the relevant and non-relevant variables over the entire range of the sparsity parameter."
    }, {
      "heading" : "6.3. Application to DLBCL data",
      "text" : "We apply our non-linear CCA models to a data set of comparative genomic hybridization (CGH) and gene expression measurements from 203 diffuse large B-cell lymphoma (DLBCL) biopsy samples (Lenz, 2008). We obtained 1500 CGH measurements from chromosome\n1 of the data, and 1500 gene expression measurements from genes on chromosome 1 and 2 of the data. The data was standardized,andWinsorized so that the data lies within two times the mean absolute deviation.\nWe used marginal thresholding to reduce the dimensionality of the problem, and then ran SA-FCCA. Permutation tests were used to pick an appropriate bandwidth and sparsity parameter, as described in Witten et al. (2009). We found that the model picked interesting non-linear relationships between CGH and gene expression data. Figure 2 shows the functions extracted by the SA-FCCA model from this data. Even though this data has been previously analyzed using linear models, we do not necessarily expect gene expression measurements from Affymetrix chips to be linearly correlated with array CGH measurements, even if the specific CGH mutation is truly affecting the gene expression. Further, the extracted functions in Figure 2 suggest that the changes in gene expression are dependent on the CGH measurements via a saturation function - as the copy number increases, the gene expression increases, until it saturates to a fixed level, beyond which increasing the copy numbers does not lead to an increase in expression. From a systems biology view point, such a prediction seems reasonable since single CGH mutations will not affect other pathways that are required to be activated for large changes in gene expression."
    }, {
      "heading" : "7. Discussion",
      "text" : "In this paper we introduced two proposals for nonparametric CCA and demonstrated their effectiveness both in theory and practice. Several interesting questions and extensions remain. CCA is often run on more than two data sets, and one is often interested in more than just the principal canonical direction. Chen & Liu (2012) have proposed group sparse linear CCA for situations when a grouping of the covariates is known. These extensions all have natural nonparametric analogues which would be interesting to explore. As in the case of regression (Koltchinskii & Yuan, 2010), the KCCA formulation considered in this paper can also be generalized to involve multiple kernels and kernels over groups of variables in a straightforward way.\nWhile thresholding marginal correlations one can imagine exploiting the structure in the correlations. In particular, in the (p1 × p2) marginal correlations matrix we are looking for a bicluster of high entries in the matrix. Leveraging this structure could potentially allow us to detect weaker marginal correlations. Finally, an important application of kernel CCA is as a contrast function in independence testing. The additive formulations we have proposed allow for independence testing over more restricted alternatives but can be used to construct interpretable tests of independence."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Research supported in part by NSF grant IIS-1116730, AFOSR contract FA9550-09-1-0373, and NIH grant R01-GM093156-03."
    } ],
    "references" : [ {
      "title" : "Kernel independent component analysis",
      "author" : [ "Bach", "Francis R", "Jordan", "Michael I" ],
      "venue" : null,
      "citeRegEx" : "Bach et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bach et al\\.",
      "year" : 2003
    }, {
      "title" : "Rademacher and Gaussian complexities: Risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "JMLR, 3:463–482,",
      "citeRegEx" : "Bartlett and Mendelson,? \\Q2002\\E",
      "shortCiteRegEx" : "Bartlett and Mendelson",
      "year" : 2002
    }, {
      "title" : "Estimating optimal transformations for multiple regression and correlation",
      "author" : [ "Breiman", "Leo", "Friedman", "Jerome H" ],
      "venue" : "JASA, 80(391):pp",
      "citeRegEx" : "Breiman et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Breiman et al\\.",
      "year" : 1985
    }, {
      "title" : "Sure independence screening in generalized linear models with NP-dimensionality",
      "author" : [ "Fan", "Jianqing", "Song", "Rui" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "Fan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2010
    }, {
      "title" : "Persistence in highdimensional linear predictor selection and the virtue of overparametrization",
      "author" : [ "Greenshtein", "Eitan", "Ritov", "Ya’acov" ],
      "venue" : null,
      "citeRegEx" : "Greenshtein et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Greenshtein et al\\.",
      "year" : 2004
    }, {
      "title" : "Behaviour and convergence of the constrained covariance",
      "author" : [ "A. Gretton", "A. Smola", "O. Bousquet", "R. Herbrich", "B. Schoelkopf", "N. Logothetis" ],
      "venue" : "Technical Report 130, MPI for Biological Cybernetics,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2004
    }, {
      "title" : "Generalized additive models",
      "author" : [ "Hastie", "Trevor", "Tibshirani", "Robert" ],
      "venue" : "Statistical Science, 1(3):pp",
      "citeRegEx" : "Hastie et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 1986
    }, {
      "title" : "Relations between two sets of variates",
      "author" : [ "Hotelling", "Harold" ],
      "venue" : "Biometrika, 28(3/4):pp",
      "citeRegEx" : "Hotelling and Harold.,? \\Q1936\\E",
      "shortCiteRegEx" : "Hotelling and Harold.",
      "year" : 1936
    }, {
      "title" : "On consistency and sparsity for principal components analysis in high dimensions",
      "author" : [ "Johnstone", "Iain M", "Lu", "Arthur Yu" ],
      "venue" : "JASA, 104(486):682–693,",
      "citeRegEx" : "Johnstone et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Johnstone et al\\.",
      "year" : 2009
    }, {
      "title" : "Functional aggregation for nonparametric regression",
      "author" : [ "Juditsky", "Anatoli", "Nemirovski", "Arkadii" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "Juditsky et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Juditsky et al\\.",
      "year" : 2000
    }, {
      "title" : "Sparsity in multiple kernel learning",
      "author" : [ "Koltchinskii", "Vladimir", "Yuan", "Ming" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "Koltchinskii et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Koltchinskii et al\\.",
      "year" : 2010
    }, {
      "title" : "Molecular subtypes of diffuse large Bcell lymphoma arise by distinct genetic pathways",
      "author" : [ "Lenz", "G. et. al" ],
      "venue" : "Proc. Natl. Acad. Sci. U.S.A.,",
      "citeRegEx" : "Lenz and al.,? \\Q2008\\E",
      "shortCiteRegEx" : "Lenz and al.",
      "year" : 2008
    }, {
      "title" : "High-dimensional additive modeling",
      "author" : [ "Meier", "Lukas", "van de Geer", "Sara", "Bühlmann", "Peter" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "Meier et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Meier et al\\.",
      "year" : 2009
    }, {
      "title" : "Genomewide sparse canonical correlation of gene expression with genotypes",
      "author" : [ "E Parkhomenko", "D Tritchler", "J. Beyene" ],
      "venue" : "BMC Proc,",
      "citeRegEx" : "Parkhomenko et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Parkhomenko et al\\.",
      "year" : 2007
    }, {
      "title" : "Minimax-optimal rates for sparse additive models over kernel classes via convex programming",
      "author" : [ "Raskutti", "Garvesh", "Wainwright", "Martin J", "Yu", "Bin" ],
      "venue" : null,
      "citeRegEx" : "Raskutti et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Raskutti et al\\.",
      "year" : 2010
    }, {
      "title" : "Sparse additive models",
      "author" : [ "Ravikumar", "Pradeep", "Lafferty", "John", "Liu", "Han", "Wasserman", "Larry" ],
      "venue" : "JRSSB (Statistical Methodology),",
      "citeRegEx" : "Ravikumar et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ravikumar et al\\.",
      "year" : 2009
    }, {
      "title" : "Extensions of sparse canonical correlation analysis with applications to genomic data",
      "author" : [ "D.M. Witten", "R.J. Tibshirani" ],
      "venue" : "Stat Appl Genet Mol Biol,",
      "citeRegEx" : "Witten and Tibshirani,? \\Q2009\\E",
      "shortCiteRegEx" : "Witten and Tibshirani",
      "year" : 2009
    }, {
      "title" : "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis",
      "author" : [ "Witten", "Daniela M", "Tibshirani", "Robert", "Hastie", "Trevor" ],
      "venue" : null,
      "citeRegEx" : "Witten et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Witten et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Witten et al. (2009) present examples of recent studies involving such data.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 17,
      "context" : "This has motivated different approaches to sparse CCA, which regularizes (1) by suitable sparsity-inducing l1 penalties (Witten et al., 2009; Witten & Tibshirani, 2009; Parkhomenko et al., 2007; Chen & Liu, 2012).",
      "startOffset" : 120,
      "endOffset" : 212
    }, {
      "referenceID" : 13,
      "context" : "This has motivated different approaches to sparse CCA, which regularizes (1) by suitable sparsity-inducing l1 penalties (Witten et al., 2009; Witten & Tibshirani, 2009; Parkhomenko et al., 2007; Chen & Liu, 2012).",
      "startOffset" : 120,
      "endOffset" : 212
    }, {
      "referenceID" : 15,
      "context" : "Recently, several authors have shown how sparse additive models for regression can be efficiently estimated even when p > n (Ravikumar et al., 2009; Koltchinskii & Yuan, 2010; Meier et al., 2009; Raskutti et al., 2010).",
      "startOffset" : 124,
      "endOffset" : 218
    }, {
      "referenceID" : 12,
      "context" : "Recently, several authors have shown how sparse additive models for regression can be efficiently estimated even when p > n (Ravikumar et al., 2009; Koltchinskii & Yuan, 2010; Meier et al., 2009; Raskutti et al., 2010).",
      "startOffset" : 124,
      "endOffset" : 218
    }, {
      "referenceID" : 14,
      "context" : "Recently, several authors have shown how sparse additive models for regression can be efficiently estimated even when p > n (Ravikumar et al., 2009; Koltchinskii & Yuan, 2010; Meier et al., 2009; Raskutti et al., 2010).",
      "startOffset" : 124,
      "endOffset" : 218
    }, {
      "referenceID" : 16,
      "context" : "Returning to the nonconvex sparse CCA problem, Witten et al. (2009) and Parkhomenko et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "(2009) and Parkhomenko et al. (2007) suggest using the solution to the nonsparse version of the problem to initialize sparse CCA; Chen & Liu (2012) use several random initializations.",
      "startOffset" : 11,
      "endOffset" : 37
    }, {
      "referenceID" : 13,
      "context" : "(2009) and Parkhomenko et al. (2007) suggest using the solution to the nonsparse version of the problem to initialize sparse CCA; Chen & Liu (2012) use several random initializations.",
      "startOffset" : 11,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "(2009) and Parkhomenko et al. (2007) suggest using the solution to the nonsparse version of the problem to initialize sparse CCA; Chen & Liu (2012) use several random initializations. As we show in simulations, both approaches can lead to poor results, even in the linear case. To address this issue, we propose and study a simple marginal thresholding step to reduce the dimensionality, in the spirit of the diagonal thresholding of Johnstone & Lu (2009) and the SURE screening of Fan & Song (2010).",
      "startOffset" : 11,
      "endOffset" : 456
    }, {
      "referenceID" : 13,
      "context" : "(2009) and Parkhomenko et al. (2007) suggest using the solution to the nonsparse version of the problem to initialize sparse CCA; Chen & Liu (2012) use several random initializations. As we show in simulations, both approaches can lead to poor results, even in the linear case. To address this issue, we propose and study a simple marginal thresholding step to reduce the dimensionality, in the spirit of the diagonal thresholding of Johnstone & Lu (2009) and the SURE screening of Fan & Song (2010). This results in a three step procedure where after preprocessing we use the nonsparse version of our problem to determine a good initialization for the sparse formulation.",
      "startOffset" : 11,
      "endOffset" : 500
    }, {
      "referenceID" : 15,
      "context" : "As with the group LASSO, constraining ∑ j ‖fj‖2 encourages sparsity amongst the functions fj Ravikumar et al. (2009). As stated, this is an infinite dimensional optimization problem over Hilbert spaces.",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 17,
      "context" : "This leads us to a strategy of biconvex optimization that mirrors the linear algorithm of Witten et al. (2009); specifically, initialize by solving the problem without the sparsity constraints, fix α and optimize for β and vice-versa until convergence.",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 17,
      "context" : "Instead, in the spirit of “diagonal penalized CCA” of Witten et al. (2009) we constrain the sum of the variances of the individual fjs.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 5,
      "context" : "1 is proved via a Rademacher symmetrization argument of Bartlett & Mendelson (2002) (see also Gretton et al. (2004)) while Lemma 5.",
      "startOffset" : 94,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "1 is proved via a Rademacher symmetrization argument of Bartlett & Mendelson (2002) (see also Gretton et al. (2004)) while Lemma 5.2 is based on a bound on the bracketing integral of the Sobolev space (see Ravikumar et al. (2009)).",
      "startOffset" : 94,
      "endOffset" : 230
    }, {
      "referenceID" : 17,
      "context" : "We compare SA-FCCA and SA-KCCA with two models, sparse additive linear CCA (SCCA) (Witten et al., 2009) and kernel CCA (KCCA) (Bach & Jordan, 2003).",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 17,
      "context" : "The sparsity and smoothness parameters for all methods were tuned using permutation tests, as described in Witten et al. (2009), assuming that Cf = Cg = C, and γf = γg = γ.",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 17,
      "context" : "Permutation tests were used to pick an appropriate bandwidth and sparsity parameter, as described in Witten et al. (2009). We found that the model picked interesting non-linear relationships between CGH and gene expression data.",
      "startOffset" : 101,
      "endOffset" : 122
    } ],
    "year" : 2012,
    "abstractText" : "Canonical Correlation Analysis (CCA) is a classical tool for finding correlations among the components of two random vectors. In recent years, CCA has been widely applied to the analysis of genomic data, where it is common for researchers to perform multiple assays on a single set of patient samples. Recent work has proposed sparse variants of CCA to address the high dimensionality of such data. However, classical and sparse CCA are based on linear models, and are thus limited in their ability to find general correlations. In this paper, we present two approaches to high-dimensional nonparametric CCA, building on recent developments in high-dimensional nonparametric regression. We present estimation procedures for both approaches, and analyze their theoretical properties in the high-dimensional setting. We demonstrate the effectiveness of these procedures in discovering nonlinear correlations via extensive simulations, as well as through experiments with genomic data.",
    "creator" : "LaTeX with hyperref package"
  }
}