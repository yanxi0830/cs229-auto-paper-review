{
  "name" : "1612.01205.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimal and Adaptive Off-policy Evaluation in Contextual Bandits",
    "authors" : [ "Yu-Xiang Wang", "Alekh Agarwal", "Miroslav Dudík" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Contextual bandits refer to a learning setting where the learner repeatedly observes a context, takes an action and observes a reward signal for the quality of the chosen action in the observed context. Crucially, there is no information on the quality of all the remaining actions that were not chosen for the context. As an example, consider online movie recommendation where the context describes information about a user, actions are possible movies to recommend and a reward can be whether the user enjoys the recommended movie. The framework applies equally well to several other applications such as online advertising, web search, personalized medical treatment, etc. The goal of the learner is to come up with a policy, that is a scheme for mapping contexts into actions. A common question which arises in such settings is, given a candidate target policy, what is the expected reward it obtains? A simple way of answering the question is by letting the policy choose actions (such as make movie recommendations to users), and compute the reward it obtains. Such online evaluation, is typically costly and time consuming since it involves exposing users to an untested experimental policy, and does not easily scale to evaluating the performance of many different policies.\nOff-policy evaluation refers to an alternative paradigm for answering the same question. Suppose we have existing logs from the existing system, which might be choosing actions from a very different logging policy than the one we seek to evaluate. Can we estimate the expected reward of the target policy? This question has been extensively researched in the contextual bandit model (see, e.g., [Li et al., 2011, Bottou et al., 2013, Oh and Scheuren, 1983, Little and Rubin, 2002] and references therein). In particular, there are several estimators which are unbiased under mild assumptions, such as inverse propensity scoring (IPS) [Horvitz and Thompson, 1952], and sharp estimates on their mean squared error (MSE) for policy evaluation are well-known [Dudík et al., 2014].\nar X\niv :1\n61 2.\n01 20\n5v 1\n[ st\nat .M\nL ]\n4 D\nec 2\nWhile the IPS-style methods make no attempt at all to model the underlying dependence of rewards on contexts and actions, such information is often available. The simplest approach to off-policy evaluation, given such a model, is to simply use the model to predict the reward for the target policy’s action on each context. We call this estimator the model-based approach or the direct method (DM). The key drawback of DM is that it can be arbitrarily biased when the model is misspecified. Some approaches, such as the doubly-robust method (DR) [Dudík et al., 2014] (also see the references therein for its origin in statistics and application in causal inference, e.g., [Robins and Rotnitzky, 1995, Bang and Robins, 2005]), combine the model with an IPS-style unbiased estimation and remain consistent, with known estimates for their MSE.\nAll these works focus on developing specific methods alongside upper bounds on their MSE. Little work, on the other hand, exists on the question of the fundamental statistical hardness of off-policy evaluation and the optimality (or the lack of) of the existing methods. A notable exception is the recent work of Li et al. [2015], who study off-policy evaluation in multi-armed bandits—a special case of our setting, without any contexts—and provide a minimax lower bound on the MSE. Their result shows the suboptimality of IPS (and DR) due to an excessive variance of the importance weights. This result is rather intriguing as it hints at one of two possibilities: (i) IPS and variants are also suboptimal for contextual bandit setting and we should develop better estimators, or (ii) the contextual bandit setting has qualitatively different upper and lower bounds that match. In this quest, our paper makes the following key contributions:\n1. We provide the first rate-optimal lower bound on the MSE for off-policy evaluation in contextual bandits. In contrast with context-free multi-armed bandits [Li et al., 2015], our lower bound matches the MSE upper bound for IPS up to constants, so long as the contexts have a non-degenerate distribution. This highlights the challenges of the contextual setting; even if the reward as a function of contexts and actions has no variance, the lower bound stays non-trivial in contrast with context-free multi-armed bandits.\n2. We propose a new class of estimators called the SWITCH estimators, that adaptively interpolate between an available reward model and IPS. We show that SWITCH has MSE no worse than IPS in the worst case, but is robust to large importance weights. We also show that SWITCH can have a drastically smaller variance than alternatives for combining IPS with a reward model, such as DR.\n3. We conduct experiments showing that the new estimator performs significantly better than existing approaches on simulated contextual bandit problems using real-life multiclass classification data sets.\nThe rest of this paper is organized as follows. In Section 2 and 3, we recap the formal setup of off-policy evaluation and present a lower bound on the minimax risk for model-free off-policy evaluation in contextual bandits. In Section 4, we discuss the need to have adaptive estimators that can take advantage of a given reward model and present the SWITCH estimator along with its theoretical properties. Lastly, the experimental comparison of the proposed methods and existing approaches are given in Section 5."
    }, {
      "heading" : "2 Setup and related work",
      "text" : "We use the standard setup for off-policy evaluation in contextual bandits as in prior works Dudík et al. [2014]. We start with the formal setup and then discuss the related work."
    }, {
      "heading" : "2.1 Formal setup",
      "text" : "In contextual bandit problems, the learning agent observes a context x, takes an action a and observes a scalar reward measuring the quality of the chosen action for the context. Here the context x is a feature vector from a finite set X , possibly {0, 1}d or a machine-precision encoding of Rd for some large d. The stationary\ndistribution of contexts is denoted by λ. Actions, denoted as a, are drawn from a finite set A. Rewards r have a distribution conditioned on x and a denoted by D(r|x, a). The decision rule of the agent is called a policy, which is a function from contexts to distributions over actions to allow for randomized action choice. We will use µ(a|x) and π(a|x) to denote the logging and target policies respectively. Given a policy π which is a distribution over actions given contexts, we extend it to a joint distribution over triples (x, a, r), where x is drawn according to λ, action a according to π(a|x), and r according to D(r|x, a). With this notation, the goal of off-policy evaluation is to evaluate the expected reward of π, given samples where actions are chosen according to µ. Formally, given n samples of the form (xi, ai, ri, pi), where pi = µ(ai | xi), we wish to compute the value of π:\nvπ = Eπ[r], where we have x ∼ λ, a ∼ π(· | x), and r ∼ D(r | a, x). (1)\nIn order to correct for the mismatch in the action distributions under µ and π, it is typical to use importance weights, and it will be convenient to introduce the shorthand ρ(x, a) = π(a|x)/µ(a|x). In order to facilitate consistent estimation, it is typical to assume that ρ(x, a) 6= ∞, meaning that whenever π(a|x) > 0, then µ(a|x) > 0 as well. We will make this same assumption throughout our upper and lower bounds. We conclude the setup by giving one concrete example for estimating the quantity vπ to aid the reader’s intuition. The simplest estimator in this setting, called Inverse Propensity Scoring (IPS) [Horvitz and Thompson, 1952] is defined as:\nv̂πIPS = n∑ i=1 ρ(xi, ai)ri. (2)\nIn the sequel, we will discuss properties of this estimator, as well as improvements upon it for off-policy evaluation."
    }, {
      "heading" : "2.2 Related work",
      "text" : "We now review the related work. There are several works that focus on creating estimators for the off-policy evaluation problem, and provide upper bounds on the Mean Squared Error (MSE) of these estimators, as discussed in the introduction. The study of lower bounds on the MSE of any possible estimator for this problem is relatively new, on the other hand. Li et al. [2015] first studied this question under a minimax setup, but focused on the multi-arm bandits problem. As we will show, the naive extension of their results to contextual bandits yields a suboptimal lower bound. Jiang and Li [2016] built on top of Li et al. [2015] proves a Cramer-Rao-style lower bound for policy-evaluation in reinforcement learning, but the assumption of a small number of observed states precludes contextual bandits from being a special case. Cortes et al. [2010] studied the related covariate shift problem in the statistical learning setting and proved upper bounds and lower bounds of the minimax excess risk that depends on the Renyi divergence of the two covariate distributions. This is related to our problem in the following sense: we can formulate off-policy evaluation as a statistical learning problem of predicting reward r using feature vector (x, a), and off-policy evaluation corresponds to exactly the covariate shift problem (when we know how the covariate has shifted). However, as the minimax problems are defined differently1, their results (especially the lower bound) are not applicable to our problem.\nOn the complementary question of developing techniques for off-policy evaluation and understanding their theoretical properties, there is a considerably larger body of work in both the statistics and machine learning literature. In statistics, this is studied as the problem of mean estimation for treatment regimes in the causal inference literatre (see e.g. [Horvitz and Thompson, 1952, Holland, 1986, Bang and Robins, 2005, Rotnitzky et al., 2012] as well as the references in [Dudík et al., 2014]). The “doubly robust” estimation techniques\n1Off policy evaluation focuses on estimating the policy-value, while statistical learning focuses on prediction accuracy measured by the expected loss (in terms of the excess risk).\n[Cassel et al., 1976, Robins and Rotnitzky, 1995], were recently used for off-policy value estimation in the contextual bandits problem [Dudík et al., 2011, 2014] and reinforcement learning [Jiang and Li, 2016]. These also provide error estimates for the proposed estimators, which we will compare with the lower bounds that we will obtain. The doubly robust techniques also have a flavor of incorporating existing reward models, an idea which we will also leverage in the development of the SWITCH estimator in Section 4. We note that similar ideas were also recently investigated in the context of reinforcement learning by Thomas and Brunskill [2016]."
    }, {
      "heading" : "3 Limits of off-policy evaluation",
      "text" : "In this section we will present our main result on lower bounds for off-policy evaluation. We first present the minimax framework to study such problems, before presenting the result and its implications."
    }, {
      "heading" : "3.1 Minimax framework",
      "text" : "Off-policy evaluation is a statistical estimation problem, where the goal is to estimate vπ given n iid samples generated according to a policy µ. We study this problem in a standard minimax framework and seek to answer the following question. What is the smallest mean square error (MSE) that any estimator can achieve in the worst case over a large class of contextual bandit problems. As is usual in the minimax setting, we want the class of problems to be rich enough so that the estimation problem is not trivialized, and to be small enough so that the lower bounds are not driven by complete pathologies. In our problem, we make the choice to fix λ, µ and π, and only take worst case over a class of reward distributions. This allows the upper and lower bounds, as well as the estimators to adapt with and depend on λ, µ and π in interesting ways. The family of reward distributions D(r | x, a) that we study is a natural generalization of the class studied by Li et al. [2015] for multi-armed bandits.\nTo formulate our class of reward distributions, assume we are given maps Rmax : X ×A → R+ and σ : X ×A → R+. The class of conditional distributionsR(σ,Rmax) is defined as\nR(σ,Rmax) := { D(r|x, a) : 0 ≤ ED[r|x, a] ≤ Rmax(x, a) and\nVarD[r|x, a] ≤ σ2(x, a) for all x, a } .\nNote that σ and Rmax are allowed to change over contexts and actions. Formally, let an estimator be any function v̂ : (X ×A× R)n → R that takes n data points collected by µ and outputs an estimate of vπ. The minimax risk of off-policy evaluation over the classR(σ2, Rmax) is defined as\nRn(π;λ, µ, σ,Rmax) := inf v̂ sup D(r|x,a)∈R(σ,Rmax)\nE [ (v̂ − vπ)2 ] . (3)\nRecall that the expectation here is taken over the n samples collected according to µ, along with any randomness in the estimator. The main goal of this section is to obtain a lower bound on the minimax risk. To state our bound, recall that ρ(x, a) = π(a | x)/µ(a | x) is an importance weight at (x, a). We make the following technical assumption on our problem instances, described by tuples of the form (π, λ, µ, σ,Rmax):\nAssumption 1. There exists > 0 such that Eµ [ (ρRmax) 2+ ] and Eµ [ (ρσ)2+ ] are finite.\nThis assumption is fairly mild, as it is only a slight strengthening of the assumption that Eµ[(ρRmax)2] and Eµ[(ρσ)2] be finite, which is required for consistency of IPS (see, e.g., the bound on the variance of IPS in Dudík et al. [2014], which assumes the finiteness of these second moments). Our assumption holds for instance whenever the context space is finite and π cannot pick actions that receive zero probability under µ,\nsince both ρ and Rmax are bounded in this case. The latter assumption is quite standard and important in the off-policy evaluation literature as we observed in Section 2.1."
    }, {
      "heading" : "3.2 Minimax lower bound for off-policy evaluation",
      "text" : "With the minimax setup in place, we now give our main lower bound on the minimax risk of off-policy evaluation and discuss its conclusions. In order to describe our result, we define one convenient piece of notation\nC := 22+ ·max\n{ Eµ [ (ρRmax) 2+ ]2\nEµ [ (ρRmax)2\n]2+ , Eµ [ (ρσ)2+ ]2 Eµ [ (ρσ)2 ]2+ }\n(4)\nTheorem 1 (Minimax lower bound). For any problem instance satisfying Assumption 1 with and any n ≥ max { 5C1/ , C2/ Eµ[σ2/R2max] } , the minimax risk satisfies the lower bound\nRn(π;λ, µ, σ,Rmax) ≥ 1\n220n\n( Eµ [ ρ2σ2 ] + Eµ [ ρ2R2max ]( 1− 110λ0 log(4/λ0) )) ,\nwhere λ0 = maxx∈X λ(x) is the largest probability of a single context.\nThe theorem is proved in Appendix A and uses a reduction to hypothesis testing, followed by an argument due to Le Cam. We now discuss several aspects of the above result.\nPre-conditions of the theorem The theorem assumes the existence of a (problem-dependent) constant C which depends on variance moments of the importance weighted rewards. The stated lower bound holds as long as the number of samples n is large enough relative to C and some other parameters. Assuming that the constant C is finite, then the condition on n is eventually satisfied as long as the random variable σ/Rmax has a bounded variance. We believe this is quite reasonable, since the problems of interest typically have heavy tails in the importance weights but not the reward functions. In terms of the importance weights, we require the random variables ρRmax and ρσ to have appropriate moment bounds. In particular, whenever Assumption 1 is satisfied, then it is easy to see that C < ∞. In particular, under tail decay conditions on these two random variables, the precondition of the theorem is met for a reasonable number of samples. An extreme case where the preconditions will be violated are if Rmax ≡ 0, in which case the estimation problem is indeed trivial. For the remainder of this discussion, we will assume that n is appropriately large so that the preconditions of the theorem are met.\nComparison with upper bounds In typical contextual bandit settings, the contexts are drawn from either a continuous distribution, or an extremely large, but finite set. In either case, the probability of observing any particular context is vanishingly small, so that it is reasonable to expect λ0 → 0 as n→∞. Consequently, the lower bound becomes Ω ( Eµ[ρ2(σ2 +R2max)]/n ) . Comparing with the upper bounds on the MSE for existing estimators, we find that this lower bound precisely matches the upper bound for the IPS estimator [Horvitz and Thompson, 1952] (defined in Equation 2). We present the upper bound here for completeness.\nLemma 1 (IPS estimator [Horvitz and Thompson, 1952]). With v̂πIPS defined in Equation 2, we have\nE(v̂πIPS − vπ)2 ≤ 1\nn\n( Eµ[ρ2σ2] + 1\n4 Eµ[ρ2R2max]\n) .\nWe conclude that IPS is unimprovable, in the worst case, beyond constant factors. Another implication is that the lower bound is sharp for our setting, and cannot be improved any further unless we change the minimax framework to consider smaller classes of reward distributions or allow other side information. In particular, we can combine the upper and lower bounds for a precise characterization of the minimax risk.\nCorollary 1. Under conditions of Theorem 1, for sufficiently small λ0 and large enough n:\ninf v̂ sup D(r|a,x)∈R(σ2,Rmax)\nEµ(v̂ − vπ)2 = Θ [ 1\nn\n( Eµ[ρ2σ2] + Eµ[ρ2R2max] )] .\nComparison with the multi-armed bandit setting of Li et al. [2015] The most closely related prior result was due to Li et al. [2015], who showed matching upper and lower bounds on the minimax risk for multi-armed bandits. The somewhat surprising conclusion of their work was the sub-optimality of IPS, which might appear at odds with our conclusion regarding IPS above. However, this difference actually highlights the additional challenges in contextual bandits beyond multi-armed bandits. This is best illustrated in a noiseless setting, where σ = 0 in the rewards. This makes the multi-armed bandit problem trivial, we can just measure the reward of each arm with one pull and find out the optimal choice. However, there is still a non-trivial lower bound of Ω(Eµ[ρ2R2max]/n) in the contextual bandit setting, which is exactly the upper bound on the MSE of IPS when the rewards have no noise.\nThis difference crucially relies on λ0 being suitably small relative to the sample size n. When the number of contexts is small, independent estimation for each context can be done in a noiseless setting as observed by Li et al. [2015]. However, once the context distribution is rich enough, then even with noiseless rewards, there is significant variance in the value estimates based on which contexts were observed. This distinction is further highlighted in the proof of Theorem 1, and is obtained by combining two separate lower bounds. The first lower bound considers the case of noisy rewards, and is a relatively straightforward generalization of the proof of Li et al. [2015]. The second lower bound focuses on noiseless rewards, and shows how the variance in a rich context distribution allows the environment to essentially simulate noisy rewards, even when the reward signal itself is noiseless."
    }, {
      "heading" : "4 Incorporating model-based approaches in policy evaluation",
      "text" : "Amongst our twin goals of optimal and adaptive estimators, the discussion so far has centered around the optimality of the IPS estimators in a minimax sense. However, real datasets seldom display worst-case behavior, and in this section we discuss approaches to leverage additional structure in the data, when such knowledge is available. We begin with the necessary setup, before introducing our new estimator and its properties. Throughout this section, we drop the superscript π from value estimators, as the evaluation policy π is fixed throughout this discussion."
    }, {
      "heading" : "4.1 The need for model-based approaches",
      "text" : "As we have seen in the last section, the model-free approach has a information-theoretic limit that depends quadratically on Rmax, σ and importance weight ρ. This is good news in that it implies the existence of an optimal estimator–IPS. However, it also substantially limits what policies can be evaluated, due to the quadratic dependence on ρ. If µ(a | x) is small for some actions, as is typical when number of actions is large or in real systems with a cost for exploration, the policies π which can be reliably evaluated cannot put much mass on such actions either without resulting in unreliable value estimates. The key reason for this limitation is that the setup so far allows completely arbitrary reward models—E[r | x, a] can change arbitrarily across different actions and contexts. Real data sets are seldom so pathological, and often we have substantial intuition about contexts and actions which obtain similar rewards, based on the specific application. It is natural to ask how can we leverage such prior information, and develop better estimators that improve upon the minimax risk in such favorable scenarios.\nIn the presence of additional domain information, a common approach is to construct an explicit model for the expected reward, given context and action. Based on the existing logs according to µ: (xi, ai, ri)ni=1, one can then fit the parameters in such a model to form a reward estimator r̂(x, a). The task of policy evaluation is now simply performed by scoring π according to r̂ as\nv̂DM = 1\nn n∑ i=1 ∑ a∈A π(a|xi)r̂(xi, a), (5)\nwhere the DM stands for direct method, a name for this approach which has been used in prior works [Dudík et al., 2014]. This approach appears attractive when r̂ is a close to E[r | x, a]. Crucially, r̂ can be evaluated for any x, a pair, meaning that there is no need to do use importance weights unlike in IPS. This suggests that we might completely eliminate any dependence on ρ using this approach., and it is easily seen that given any estimator r̂ such th which takes values in [0, Rmax(x, a)] for any x, a, the variance satisfies:\nVar(v̂DM) ≤ 1\nn Eπ[R2max] =\n1 n Eµ[ρR2max]. (6)\nThat is, there is a linear, rather than quadratic dependence on ρ, unlike in the worst case minimax risk of Corollary 1. The catch, however, is that such estimators can have an uncontrolled bias of O(Eπ[Rmax]) in the worst case, meaning that even asymptotic consistency is not guaranteed.\nPrior works have addressed the bias of DM, while trying to reduce the variance of IPS by using doubly robust estimators. A concrete estimator previously used for off-policy evaluation in Dudík et al. [2014] is defined as\nv̂DR = 1\nn n∑ i=1\n[ (ri − r̂(xi, ai))π(ai)\nµ(ai) + ∑ a∈A π(a|xi)r̂(xi, ai)\n] . (7)\nThe theory of these methods suggests that the variance of v̂πDR can be smaller than IPS, if r̂ is a good reward estimator. Does this imply that DR is the right model-based adaptive estimator?\nAn interesting benchmark to consider is when one has a prefect reward estimator, that is r̂(x, a) = E[r | x, a] almost surely. In this case, v̂DM has no bias, and hence its MSE is identical to its variance, with a linear dependence on importance weights as in Equation 6. However, based on the results of Dudík et al. [2014], the MSE of the DR estimator in this special case is\nMSE(v̂DR) ≤ 1\nn\n( Eµ[ρ2σ2] + Eµ[ρR2max] ) . (8)\nCrucially, the MSE still has a dependence on ρ2 in it as long as the variance σ is non-trivial. Comparing Equations 6 and 8, it would be better to use v̂DM instead in this case. Also note that ρ instead of ρ2 term multiplying Rmax does not contradict the lower bound in Theorem 1, since the bound (8) assumes access to the expected reward function which is information an estimator does not have in our minimax setup and which cannot be computed from the samples in general.\nWe conclude that while DR improves upon IPS in some cases, the estimator still does not adapt enough in the presence of good reward models, when the rewards have a non-trivial variance. We next present an estimator which utilizes the reward model in a different way, with an eye towards a more favorable bias-variance tradeoff."
    }, {
      "heading" : "4.2 The SWITCH estimators to incorporate reward models",
      "text" : "We now present a class of estimators which incorporate any available reward models very differently from the DR approach. The starting point for our method is the observation that insistence on maintaining unbiasedness\nputs the DR estimator at one extreme end of the bias-variance tradeoff. Prior works have considered ideas such as truncating the rewards, or importance weights when the importance weights get large (see e.g. Bottou et al. [2013] for a detailed discussion), which can often reduce the variance drastically at the cost of a little bias. We take the intuition a step further, and propose to estimate the rewards for actions differently, based on whether they have a large or a small importance weight given a context. When importance weights are small, we continue to use our favorite unbiased estimators, but switch to using the (potentially biased) reward model on the actions with large importance weights. Here small and large are defined via some threshold parameter τ . Varying this parameter between 0 and∞ leads to a family of estimators which we call the SWITCH estimators as they switch between a model-free and model-based approach.\nWe now formalize this intuition, and begin by decomposing the value of π according to importance weights:\nvπ = Eπ[r] = Eπ[r1(ρ ≤ τ)] + Eπ[r1(ρ > τ)] = Eµ[ρr1(ρ ≤ τ)] + Ex∼λ [∑ a∈A ED[r|x, a]π(a|x)1(ρ(x, a) > τ) ] .\nConceptually, we split our problem into two. The first problem always has small importance weights, so we can use unbiased estimators such as IPS or DR as before. The second problem, where importance weights are large, is essentially addressed by DM. Writing this out leads to the following estimator:\nv̂SWITCH = 1\nn n∑ i=1 [riρi1(ρi ≤ τ)] + 1 n n∑ i=1 ∑ a∈A r̂(xi, a)π(a|xi)1(ρ(xi, a) > τ). (9)\nNote that the above estimator specifically uses IPS on the first part of the problem. We will mention an alternative using the DR estimator for the first part at the end of this section. We first present a bound on the MSE of the SWITCH estimator using IPS, for a given choice of the threshold τ .\nTheorem 2. Let (a, x) := r̂(a, x)− E[r|a, x] be the bias of r̂ and assume r̂(x, a) ∈ [0, Rmax(x, a)] almost surely. Then for every n = 1, 2, 3, . . . , and for the τ > 0 used in Equation 9, we have\nMSE(v̂SWITCH) ≤ 2\nn\n{ Eµ [( σ2 +R2max ) ρ21(ρ ≤ τ) ] +Eµ [ ρR2max1(ρ > τ) ] +Eµ [ ρ ∣∣ ρ > τ]2π(ρ > τ)2} ,\nwhere quantities Rmax, σ, ρ, and are functions of the random variables x and a, and we recall the use of π and µ as joint distributions over (x, a, r) tuples.\nRemark 1. The proposed estimator is an interpolation of the DM and IPS estimators. By taking τ =, we get SWITCH coincides with DM while τ →∞ yields IPS. Several estimators related to SWITCH have been studied in the literature, and we discuss a couple of them here.\n• A special case of SWITCH uses r̂ ≡ 0, meaning that all the actions with large importance weights are essentially eliminated from consideration. This approach, with a specific choice of τ was described in Bottou et al. [2013] and will be evaluated in the experiments under the name Trimmed IPS.\n• Thomas and Brunskill [2016] study a similar estimator in the more general context of reinforcement learning. Their approach can be seen as using a number of candidate threshold τ ’s and then evaluating the policy as a weighted sum of the estimates corresponding to each τ . They address the questions of picking these thresholds and the weights in a specific manner for their estimator called MAGIC, and we discuss these aspects in more detail in the following subsection.\nRemark 2. The MSE bound in Theorem 2 is easily interpreted. The first term is the MSE of IPS on the region where it is used, while the second and third terms capture the variance and squared bias of DM on its region respective. Consequently, our matches the result for IPS when τ →∞, meaning that the SWITCH estimators are also minimax optimal when τ is appropriately chosen. At the other extreme, we can consider the case of τ = 0 and a perfect reward predictor so that ≡ 0. In this case, the MSE bound matches that of DM in Equation 6. More generally, the estimators are robust to heavy-tails in the distribution of importance weights, unlike both IPS and DR estimators.\nRemark 3. The policy value in the region where the importance weights are small can be estimated using any unbiased approach rather than just IPS. For instance, we can use the DR, giving rise to the estimator, which we denote SWITCH-DR. For any reward estimator r̂′ to construct the DR estimator, we get\nv̂SWITCH-DR = 1\nn n∑ i=1 [ (ri − r̂′i)ρi1(ρi ≤ τ) + ∑ a∈A r̂′(xi, a)π(a|xi)1(ρ(xi, a) ≤ τ) ]\n+ 1\nn n∑ i=1 ∑ a∈A r̂(xi, a)π(a|xi)1(ρ(xi, a) > τ). (10)\nNote that r̂ and r̂′ need not be the same direct estimators. When they are indeed the same, v̂SWITCH-DR can be rewritten as essentially a modified doubly robust estimator where all the importance weights above τ are clipped to zero, so that the importance weighted part of DR makes no contribution.\nThe analysis in Theorem 2 still applies, replacing the variance of IPS with that of DR from Dudík et al. [2014]. Since no independence was required in our analysis between the IPS and the DM parts of the estimator, the result is also robust to the use of a common data-dependent estimator r̂ = r̂′ in SWITCH-DR (10)."
    }, {
      "heading" : "4.3 Automatic parameter tuning",
      "text" : "So far we have discussed the properties of the SWITCH estimators, assuming that the parameter τ is chosen well. On the other hand, the estimator can be as bad as DM in the worst-case if τ is poorly chosen, as evidenced by the bias term in Theorem 2. For the estimators to be useful in practice, it becomes essential to have a procedure to select a good value of τ . A natural criterion for selecting τ would be to pick one that minimizes the MSE of the resulting estimator. Since we do not know the precise MSE (as vπ is unknown), an alternative is to minimize a data-dependent estimate for it. Recalling that the MSE can be written as the sum of variance and squared bias, we estimate and bound the terms individually.\nWe can estimate the variance of the SWITCH estimator in a straightforward manner from the data. Let Yi(τ) denote the estimated value that π obtains on the data point xi according to the SWITCH estimator with the threshold τ , that is\nYi(τ) := riρi1(ρi ≤ τ) + ∑ a∈A r̂(xi, a)π(a|xi)1(ρ(xi, a) > τ) and Ȳ (τ) = 1 n n∑ i=1 Yi(τ),\nSince the xi are i.i.d., the variance of v̂SWITCH can be simply estimated as\nVar(v̂SWITCH−τ ) = 1\nn Var(v̂SWITCH−τ (x1)) ≈\n1\nn2 n∑ i=1 (Yi(τ)− Ȳ (τ))2 =: V̂arτ , (11)\nwhere the approximation above is clearly consistent since the random variables Yi are appropriately bounded as long as the rewards are bounded, with the importance weights capped at the threshold τ . Note that we have explicitly denoted the dependence on τ in the SWITCH estimator above.\nNext we turn to the bias term. For understanding bias, we look at the MSE bound in Theorem 2, and observe that the last term in that theorem is precisely an upper bound on the bias. Rather than using a direct bias estimate, which would require knowledge of the error in r̂, we will upper bound this term. For now, let us assume that the function Rmax(x, a) is known. In many practical applications, this is not a limiting assumption at all, where an apriori bound on the rewards is known ahead of time anyways. Then we can upper bound the bias term as\nBias2(v̂SWITCH) ≤ Eµ[ρ 2|ρ > τ ]π(ρ > τ)2 ≤ Eµ[ρR2max|ρ > τ ]π(ρ > τ)2\n≈\n[ 1\nn n∑ i=1 Eπ ( R2max|ρ > τ, xi )] [ 1 n n∑ i=1 π(ρ > τ |xi) ]2 =: B̂ias 2 τ . (12)\nNote that this estimate of the squared bias is not as straightforward as the one for variance, since we independently use sample-based unbiased approximations of the expectation of Rmax and the probability of large importance weights, and then multiply them. However, standard arguments can be used to show that the resulting estimates are still consistent under mild conditions. As a special case, if Rmax ≡ R, where the uniform upper bound R is known, then the first average simplifies to R making the bias estimate rather straightforward.\nWith these estimates, we pick the value of τ that minimizes\nτ̂ = argmin τ\nV̂arτ + B̂ias 2 τ . (13)\nNotice that the upper bound on the bias is rather conservative in that it upper bounds the error of DM at the largest possible value for every data point. This has the effect of favoring the use of the unbiased part in SWITCH whenever possible, unless the variance would overwhelm even an arbitrarily biased DM. While this might appear overly conservative, this does imply the minimax optimality of the SWITCH estimator using τ̂ almost immediately: the incurred bias is no more than our upper bound, and we incur it only when the minimax optimal IPS estimator would be suffering an even larger variance.\nFinally, we should mention that this development is quite related to the MAGIC estimator of Thomas and Brunskill [2016], which was discussed following Theorem 2. The key differences are that we pick only one threshold τ , while they compute the estimates with many different τ ’s. Rather than picking just one of these estimates, they further learn a more general weighting over the estimates. In that sense, our estimator can be seen as a special case which puts the entire weight on one choice. Like us, they also pick this weighting function by optimizing a bias variance tradeoff. However, we use very different estimates for bias and variance than their estimator. In the experiments, we did evaluate their approach for picking the threshold τ and found that the choice τ̂ in Equation 13 generally works better."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we conduct an experimental evaluation of the proposed SWITCH estimators. We will be using the same 10 UCI data sets that was used previously by Dudík et al. [2011] and convert the multi-class classification problem to contextual bandits by\n1. making it a sequential learning task, where the learner receives one example at a time, and guesses its label using policy µ.\n2. providing a reward of 1 (and hence revealing the correct label) only if the guess is correct; otherwise providing a 0-reward. Note that this means Rmax ≡ 1 is a valid bound.\nThe target policy π we used is the deterministic decision of a learned logistic regression classifier, while the logging policy µ we used is the probability estimates of a learned logistic regression classifier on a covariate-shifted data set. We simulate the covariate-shifted data set using the same technique as in Dudík et al. [2011], which follows standard practice as was described by Gretton et al. [2009].\nIn each data set with n rows, we treat the uniform distribution over the data set itself as a surrogate of the population distribution so that we know the ground truth of the rewards. Then, in the simulator, we randomly draw i.i.d. data sets of size [100, 200, 500, 1000, 2000, 5000, 10000, ...] until it reaches n, each repeating for 500 times, then compare different candidate estimators’ MSE, estimated using the empirical averages of the square error over the 500 replicates, which we can calculate exactly since we know vπ. It is worth pointing out that for some of the baselines that we compare to, e.g., IPS and DR, their distribution of square errors can have very large variance due to the potentially large importance weights. This leads to very large error bars if we aggregate their MSE with even 500 replicates. To circumvent this issue, we report a clipped version of the MSE that truncates any square errors to 1 if it is larger than 1, namely\nMSE = E[(v̂ − vπ)2 ∧ 1].\nThis allows us to get valid confidence intervals for our empirical estimates of this quantity. Note that this does not change the MSE estimate of our approach at all, but is significantly more favorable towards IPS and DR. In this section, whenever we refer to“MSE”, we are referring to the truncated version.\nWe will compare the SWITCH-IPS and SWITCH-DR against the following baselines:\n1. IPS estimator\n2. Direct method via logistic regression\n3. Doubly robust estimator\n4. Truncated and reweighted IPS\n5. Trimmed IPS\nThe doubly robust is constructed by randomly splitting the data set into two halves, estimating DM on one half, combining with IPS on the other half, then switching the two sets and doing it again. The final estimate is the average of the two. This approach was suggested by Dudík et al. [2011], and is standard practice of doubly robust estimators when the direct method (or the oracle estimator) needs to be estimated from the data.\nWe also consider two additional variants of IPS, where importance weights are either capped at τ and renormalized [see, e.g., Bembom and van der Laan, 2008], or the terms with weights larger than τ are removed altogether as described in Bottou et al. [2013]. Note that the trimmed IPS is a special case of SWITCH when the direct estimator ≡ 0.\nFor both our methods (as well as the two IPS variants) we tune the parameter τ by the method described in Section 4.3. So everything is implementable in practice. As a reference, we are also including in the plot the results for the optimally tuned τ from hindsight, as well as the alternative ensemble approach via MAGIC. This allows us to gauge how well our automatic parameter tuning procedure works on these data sets.\nIn addition, we repeated the same experiments for a more challenging setting where the feedbacks are noisy. This is simulated by only revealing the correct reward with probability 0.5 and outputting a random coin toss with probability the other 0.5. Theoretically, this should lead to bigger σ2 and larger variance in all estimators.\nIn order to stay comparable across data sets, we use relative MSE with respect to the IPS. For each estimator v̂, report the relative MSE with respect to the IPS estimator\nRel.MSE(v̂) = MSE(v̂)\nMSE(v̂IPS) ,\nsuch that the numerical value is comparable across data sets and is independent to n. The results are summarized in Figure 12, in which methods that achieve smaller values of MSE are towards the top-left corner of the plot. In general, we want to maximize the area under the curve as these are CDF plots, and we notice the logarithmic scale for the relative MSE values on the X-axis. As we see, SWITCH-DR dominates all other methods and our empirical tuning of τ is not too far from the optimal possible. The advantage of SWITCH-DR is even stronger in the noisy-reward setting, where we add label noise to UCI data.\nIn Figure 2, we illustrate the convergence of MSE as n increases. In particular, we selected three data sets and show how SWITCH and SWITCH-DR perform against baselines in three typical cases: (i) when the direct method works better than IPS and DR; (ii) when the direct method works well initially but then as n gets large outperformed by IPS and DR; (iii) when the direct method works extremely poorly. In the first two cases, SWITCH-DR outperforms both DM and IPS, while DR only improves over IPS slightly. In the third case, SWITCH-DR performs about as well as IPS and DR despite that DM is very bad. In all cases, SWITCH-DR is robust to additional noise in the reward."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper we carried out minimax analysis of off-policy evaluation in contextual bandits and showed that IPS is optimal in the worst-case. This result highlights the need for using side information, potentially\n2For the clarity of the presentation, we excluded SWITCH, which also significantly outperforms either IPS but is dominated by SWITCH-DR. For the same reason, we combined the Trim and Truncated/reweighted IPS.\nprovided by modeling the reward directly, especially when importance weights are too large. Given this observation, we proposed a new class of estimators called SWITCH that can be used to combine any importance sampling estimators, including IPS and DR, with DM. The estimator involves adaptively switching to DM when the importance weights are large and switching to either IPS or DR when the importance weights are small. We showed that the new estimator has favorable theoretical properties and also works well on real-world data."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The work was partially completed during YW’s internship at Microsoft Research NYC from May 2016 - Aug 2016. The authors would like to thank Lihong Li and John Langford for helpful discussions, Edward Kennedy for bringing our attention to related problems and recent development in causal inference and Wei He for pointing us to [Sun, 2006] for rigorous a measure-theoretic treatment to a continuum of random variables and the corresponding law of large numbers.\nYW was supported by NSF Award BCS-0941518 to CMU Statistics, a grant by Singapore NRF under its International Research Centre @ Singapore Funding Initiative, and a Baidu Scholarship."
    }, {
      "heading" : "A Proof of Theorem 1",
      "text" : "In this appendix we prove the minimax bound of Theorem 1. The result is obtained by combining the following two lower bounds:\nTheorem 3 (Lower bound 1). For each problem instance such that Eµ[ρ2σ2] <∞, we have\nRn(π;λ, µ, σ,Rmax) ≥ log 2Eµ[ρ2σ2]\n8n\n1− Eµ[ρ2σ21 ( ρσ2 > Rmax √ nEµ[ρ2σ2]/(2 log 2) )] Eµ[ρ2σ2] 2 . Theorem 4 (Lower bound 2). For each problem instance such that Eµ[ρ2R2max] <∞, we have\nRn(π;λ, µ, σ,Rmax)\n≥ log 2Eµ[ρ 2R2max]\n8n\n1− 8λ0 log(4/λ0) log 2 − Eµ [ ρ2R2max1 ( ρRmax > √ nEµ[ρ2R2max]/(16 log 2) )] Eµ[ρ2R2max] 2 , where λ0 = maxx∈X λ(x) is the largest probability of a single context.\nThe first bound captures the intrinsic difficulty in the variance of reward. The second result shows the additional dependence on R2max, even when σ ≡ 0, whenever the distribution λ is sufficiently spread out over contexts, as quantified by the largest context probability λ0. We next show how these two lower bounds yield Theorem 1 and then return to their proofs.\nProof of Theorem 1. We begin by simplifying the two lower bounds. Assume that Assumption 1 holds with and let p = 1 + /2 and q = 1 + 2/ , i.e., 1/p+ 1/q = 1. Then the definition of C means that\nC1/( q) = C1/(2+ ) = 2 ·max Eµ [ (ρ2R2max) 2+ 2 ] 2 2+ Eµ [ ρ2R2max ] , Eµ[(ρ2σ2) 2+ 2 ] 22+ Eµ [ ρ2σ2 ] \n= 2 ·max\n{ Eµ [ (ρ2R2max) p ]1/p\nEµ [ ρ2R2max ] , Eµ[(ρ2σ2)p]1/p Eµ [ ρ2σ2 ] } , (14) and the assumption on n implies that\nn ≥ max { (16 log 2)C1/ , (2 log 2)C2/ Eµ[σ2/R2max] } . (15)\nFirst, we simplify the correction term in the lower bound of Theorem 3. Using Hölder’s inequality and Eq. (14), we have\nEµ [ ρ2σ21 ( ρσ2 > Rmax √ nEµ[ρ2σ2]/(2 log 2) )] ≤ Eµ [( ρ2σ2\n)p]1/p · Pµ[ρσ2 > Rmax√nEµ[ρ2σ2]/(2 log 2)]1/q ≤ 1\n2 Eµ[ρ2σ2] · C1/( q) · Pµ\n[ ρσ2/Rmax > √ nEµ[ρ2σ2]/(2 log 2) ]1/q\nand by Markov’s inequality, Cauchy-Schwartz inequality, and Eq. (15)\n≤ 1 2 Eµ[ρ2σ2] · C1/( q) ·\n( Eµ [ ρσ · (σ/Rmax) ]√ nEµ[ρ2σ2]/(2 log 2) )1/q\n≤ 1 2 Eµ[ρ2σ2] · C1/( q) ·\n(√ Eµ[ρ2σ2] · √ Eµ[σ2/R2max]√\nnEµ[ρ2σ2]/(2 log 2)\n)1/q\n= 1\n2 Eµ[ρ2σ2] ·\n( C2/ · 2 log 2\nn · Eµ[σ2/R2max]\n)1/2q ≤ 1\n2 Eµ[ρ2σ2] . (16)\nFor the correction term in Theorem 4, we similarly have\nEµ [ ρ2R2max1 ( ρRmax > √ nEµ[ρ2R2max]/(16 log 2) )] ≤ Eµ [( ρ2R2max\n)p]1/p · Pµ[ρRmax >√nEµ[ρ2R2max]/(16 log 2)]1/q ≤ 1\n2 Eµ[ρ2R2max] · C1/( q) · Pµ\n[ ρ2R2max > nEµ[ρ2R2max]/(16 log 2) ]1/q and by Markov’s inequality and Eq. (15)\n≤ 1 2 Eµ[ρ2R2max] · C1/( q) ·\n( Eµ[ρ2R2max]\nnEµ[ρ2R2max]/(16 log 2) )1/q = 1\n2 Eµ[ρ2R2max] ·\n( C1/ · 16 log 2\nn\n)1/q ≤ 1\n2 Eµ[ρ2R2max] . (17)\nUsing Eq. (16), the bound of Theorem 3 simplifies as\nRn(π;λ, µ, σ,Rmax)\n≥ log 2Eµ[ρ 2σ2]\n8n\n1− Eµ[ρ2σ21 ( ρσ2 > Rmax √ nEµ[ρ2σ2]/(2 log 2) )] Eµ[ρ2σ2] 2\n≥ log 2Eµ[ρ 2σ2]\n8n\n( 1− 1\n2\n)2 =\nlog 2Eµ[ρ2σ2] 32n .\nSimilarly, by Eq. (17), Theorem 4 simplifies as\nRn(π;λ, µ, σ,Rmax)\n≥ log 2Eµ[ρ 2R2max]\n8n\n1− 8λ0 log(4/λ0) log 2 − Eµ [ ρ2R2max1 ( ρRmax > √ nEµ[ρ2R2max]/(16 log 2) )] Eµ[ρ2R2max] 2\n≥ log 2Eµ[ρ 2R2max]\n8n\n[ 1− 8λ0 log(4/λ0)\nlog 2 − 1 2 ]2 ≥ log 2Eµ[ρ 2R2max]\n8n\n(( 1\n2\n)2 − 2 · 1\n2 · 8λ0 log(4/λ0) log 2\n) =\nlog 2Eµ[ρ2R2max] 32n\n( 1− 32λ0 log(4/λ0)\nlog 2\n) ,\nwhere in the last line we used (a− b)2 ≥ a2 − 2ab. Combining the two simplified bounds yields\nRn(π;λ, µ, σ,Rmax)\n≥ 1 2 · log 2Eµ[ρ 2σ2] 32n + 1 2 · log 2Eµ[ρ 2R2max] 32n\n( 1− 32λ0 log(4/λ0)\nlog 2 ) ≥ log 2\n64n\n[ Eµ[ρ2σ2] + Eµ[ρ2R2max] ( 1− 32\nlog 2 · λ0 log(4/λ0) )] ≥ 1\n220n\n[ Eµ[ρ2σ2] + Eµ[ρ2R2max] ( 1− 110λ0 log(4/λ0) )] .\nIt remains to prove Theorems 3 and 4. They are both proved by a reduction to hypothesis testing, and invoke Le Cam’s argument to lower-bound the error in this testing problem. As in most arguments of this nature, the key novelty lies in the construction of an appropriate testing problem that leads to the desired lower bounds. Before proving the theorems, we recall the basic result of Le Cam which underlies our proofs. We point the reader to the excellent exposition of Lafferty et al. [2008, Section 36.4] on more details about Le Cam’s argument.\nTheorem 5 (Le Cam’s method Lafferty et al., 2008, Theorem 36.8). Let P be a set of distributions. For any pair P0, P1 ∈ P ,\ninf θ̂ sup P∈P\nEP [d(θ̂, θ(P ))] ≥ ∆\n4\n∫ [pn0 ∧ pn1 ]dx ≥ ∆\n8 e−nDKL(P0‖P1). (18)\nwhere ∆ = d(θ(P0), θ(P1))\nWhile the proofs of the two theorems share a lot of similarities, they have to use reductions to slightly different testing problems given the different mean and variance constraints in the two results. We begin with the proof of Theorem 3, which has a simpler construction.\nA.1 Proof of Theorem 3\nThe basic idea of this proof is to reduce the problem of policy evaluation to that of Gaussian mean estimation where there is a mean associated with each x, a pair. We now describe our construction.\nCreating a family of problems Since we aim to show a lower bound on the hardness of policy evaluation in general, it suffices to show a particular family of hard problem instances, such that every estimator requires the stated number of samples on at least one of the problems in this family. Recall that our minimax setup assumes that π, µ and λ are fixed and the only aspect of the problem which we can design is the conditional reward distribution D(r | x, a). For Theorem 3, this choice is further constrained to satisfy E[r | x, a] ≤ Rmax(x, a) and Var(r | x, a) ≤ σ2. In order to describe our construction, it will be convenient to define the shorthand E[r | x, a] = η(x, a). We will identify a problem in our family with the function η(x, a) as that will be the only changing element in our problems. For a chosen η, the policy evaluation question boils down to estimating vπη = E[r(x, a)], where the contexts x are chosen according to λ, actions are drawn from π(x, a) and the reward distribution Dη(r | x, a) is a normal distribution with mean η(x, a)\nDη(r | x, a) = N (η(x, a), σ2).\nClearly this choice meets the variance constraint by construction, and satisfies the upper bound so long as η(x, a) ≤ Rmax(x, a) almost surely. Since the evaluation policy π is fixed throughout, we will drop the superscript and use vη to denote verπ in the remainder of the proofs. With some abuse of notation, we also\nuse Eη[·] to denote expectations where contexts and actions are drawn based on the fixed choices λ and µ corresponding to our data generating distribution, and the rewards drawn from η. We further use Pη to denote this entire joint distribution over (x, a, r) triples.\nGiven this family of problem instances, it is easy to see that for any pair of η1, η2 which are both pointwise upper bounded by Rmax, we have the lower bound:\nRn(λ, π, µ, σ 2, Rmax) ≥ inf\nv̂ max η∈η1,η2\nEη [\n(v̂ − vη)2︸ ︷︷ ︸ `η(v̂)\n] ,\nwhere we have introduced the shorthand `ηi(v̂) to denote the squared error of v̂ to vηi . For a parameter > 0 to be chosen later, we can further lower bound this risk as\nRn(λ, π, µ, σ 2, Rmax) ≥ max ηi∈η1,η2 Eη[`η(v̂)] ≥ max ηi∈η1,η2 Pη(`η ≥ )\n≥ 2 (Pη1(`η1(v̂) ≥ ) + Pη2(`η2(v̂) ≥ )) , (19)\nwhere the last inequality lower bounds the maximum by the average. So far we have been working with an estimation problem. We next describe how to reduce this to a hypothesis testing problem.\nReduction to hypothesis testing For turning our estimation problem into a testing problem, the idea is to identify a pair η1, η2 such that they are far enough from each other so that any estimator which gets a small estimation loss can essentially identify whether the data generating distribution corresponds to Pη1 or Pη2 . In order to do this, we take any estimator v̂ and identify a corresponding test statistic which maps v̂ into one of η1, η2. The way to do this is essentially identified in Equation 19, and we describe it next.\nNote that since we are constructing a hypothesis test for the specific family of distributions Pη1 and Pη2 , it is reasonable to consider test statistics which have knowledge of η1 and η2, and hence the corresponding distributions. Consequently, these tests also know the true policy values vη1 and vη2 and the only uncertainty is which of them gave rise to the observed data samples. Therefore, for any estimator v̂, we can a associate a statistic φ(v̂) = argminη {`η1(v̂), `η2(v̂)}.\nGiven this hypothesis test, we are interested in its error rate Pη(φ(v̂) 6= η). We first relate the estimation error of v̂ to the error rate of the test. Suppose for now that\n`η1(v̂) + `η1(v̂) ≥ 2 , (20)\nso that at least one of the losses is at least . Suppose further that φ(v̂) = η1, so that we know `η1(v̂) ≤ `η2(v̂). Then we are assured that `η1(v̂) ≤ assures the test’s correctness by Equation 20, since the other loss is guaranteed to be at least in this condition. The reasoning for the loss under η2 being small is similar, so that a valid upper bound on our test’s error (under a uniform choice of η1, η2) is given by\nmax η∈η1,η2\nPη(φ(v̂) 6= η) = 1\n2 (Pη1(φ(v̂) 6= η1) + Pη2(φ(v̂) 6= η2))\n≤ 1 2 (Pη1(`η1(v̂) ≥ ) + Pη2(`η2(v̂) ≥ ) ≤ 1 Rn(λ, π, µ, σ 2, Rmax), (21)\nwhere the final inequality uses our earlier lower bound in Equation 19.\nIt remains to establish our earlier supposition (20) to finish the relating our testing and estimation problems. Assume for now that η1 and η2 are chosen such that\n(vη1 − vη2) 2 ≥ 4 . (22)\nThen an application of Cauchy-Shwartz inequality (a+ b)2 ≤ 2a2 + 2b2 yields\n4 ≤ (vη1 − vη2) 2 ≤ 2(v̂ − vη1)2 + 2(v̂ − vη2)2 ≤ 2`η1(v̂) + 2`η2(v̂),\nwhich yields the posited bound 19.\nInvoking Le Cam’s argument So far we have identified a hypothesis testing problem and a test statistic whose error is upper bounded in terms of the minimax risk of our problem. In order to complete the proof, we now place a lower bound on the error of this test statistic. Recall the result of Le Cam 18, which places an upper bound on the attainable error in any testing problem. In our setting, this translates to\nmax η∈η1,η2 Pη(φ(v̂) 6= η) ≥ exp (−nDKL(Pη1 || Pη2)) .\nSince the distribution of the rewards is a spherical Gaussian, the KL-divergence is given by the squared distance between the means, scaled by the variance, that is\nDKL(Pη1 || Pη2) = E [ (η1(x, a)− η2(x, a))2\n2σ2\n] ,\nwhere we recall that the contexts and actions are drawn from λ and µ respectively. Since we would like the probability of error in the test to be a constant, it suffices to choose η1 and η2 such that\nE [ (η1(x, a)− η2(x, a))2\n2σ2\n] ≤ log 2\nn . (23)\nPicking the parameters So far, we have not made any concrete choices for η1 and η2, apart from some constraints which we have introduced along the way. Note that we have the constraints (22) and (23) which try to ensure that η1 and η2 are not too close that an estimator does not have to identify the true parameter, or too far that the testing problem becomes trivial. Additionally, we have the upper and lower bounds of 0 and Rmax on η1 and η2. In order to reason about these constraints, it is convenient to set η2 ≡ 0, and pick η1(x, a) = η1(x, a)− η2(x, a) = ∆(x, a). We now write all our constraints in terms of ∆.\nNote that vη2 is now 0, so that the first constraint (22) is equivalent to\nvη1 = Eη1 [ρ(x, a)r(x, a)] = E∆[ρ(x, a)r(x, a)] ≥ 2 √ ,\nwhere the importance weighting function ρ is introduced since Pη1 is based on choosing actions according to µ and we seek to evaluate π. The second constraint (23) is also straightforward\nE [ ∆2\n2σ2\n] ≤ log 2\nn .\nThe bound constraints are respected by imposing 0 ≤ ∆(x, a) ≤ Rmax(x, a) almost surely. The minimax lower bound is then yielded by the largest in the constraint (22) such that the other two constraints can be satisfied. This gives rise to the following variational characterization of the minimax lower bound\nmax ∆\nsuch that E∆[ρ(x, a)r(x, a)] ≥ 2 √ ,\nE [ ∆2\n2σ2\n] ≤ log 2\nn and\n0 ≤ ∆(x, a) ≤ Rmax(x, a).\nInstead of finding the optimal solution, we show a feasible setting of ∆ here. We set\n∆ = min\n{ ασ2ρ\nEµ[ρ2σ2] , Rmax\n} , where α = √ 2 log 2\nEµ[ρ2σ2] n . (24)\nThis setting satisfies the bound constraints by construction. A quick substitution also verifies that the second constraint is met. Consequently, it suffices to set to the value attained in the first constraint. Substituting the value of ∆ in the constraint, we see that\nE∆[ρ(x, a)r(x, a)] = Ex∼λ,a∼µ[ρ(x, a)∆(x, a)] ≥ Ex∼λ,a∼µ [ ρ ασ2ρ\nEµ[ρ2σ2] 1(ρσ2α ≥ RmaxEµ[ρ2σ2]) ] = α ( 1− Eµ[ρ\n2σ21(ρσ2α ≤ RmaxEµ[ρ2σ2])] Eµ[ρ2σ2]\n) .\nTaking squares and dividing by 4 (since this is the lower bound on 2 √ yields the statement of the theorem.\nA.2 Proof of Theorem 4\nWe now give the proof of Theorem 4. This shares a lot of common ideas with the proof of Theorem 3, but with a crucial difference. In Theorem 3, there is non-trivial noise in the reward function, unlike in Theorem 4. This allowed the proof to work with just two candidate mean reward functions, since any realization in the data is corrupted with noise. However, in the absence of added noise, the task of mean identification becomes rather trivial: an estimator can just check whether η1 or η2 matches the observations exactly.\nIn order to avoid this pathology, we instead construct a richer family of reward functions which will be described next. In this construction, rather than just two mean rewards, the construction will involve a randomized design of the expected reward function from an appropriate prior distribution. The draw of the mean reward from a prior will essentially generate noise even though any given problem is noiseless. The construction will also highlight the crucial sources of difference between the contextual and multi-armed bandit problems, since the arguments rely on having a rich context distribution.\nCreating a family of problems As before, we fix π, µ and λ and parametrize the problem distribution in terms of the mean reward function η(x, a). However, now η(x, a) is itself a random variable, which is drawn from a prior distribution. The distribution of η(x, a) is a scaled Bernoulli, parametrized by a prior function θ(x, a) as described below:\nη(x, a) =\n{ Rmax(x, a) with probability θ(x, a)\n0 with probability 1− θ(x, a) . (25)\nWe now set Dη(r | x, a) = η(x, a). This clearly satisfies the constraints on the mean 0 ≤ E[r | x, a] ≤ Rmax(x, a) by construction, and also Var(r | x, a) = 0 as per the setting of Theorem 4. The goal of an estimator is to take n samples generated according with x ∼ λ, a | x ∼ µ and r | x, a ∼ Dη and output an estimate v̂ such that Eη[(v̂ − vπη )2] is small. We recall our earlier shorthand vη to denote the value of π under the reward distribution generated by η. For showing a lower bound on this quantity, it is clearly sufficient to pick any prior distribution governed by a parameter θ as in Equation 25, and instead lower bound Eθ [ Eη[(v̂ − vη)2 | η] ] . If the expectation is large for an estimator v̂, then there must be at least one function η(x, a) which induces a large error for v̂ which is the goal of the lower bound. Consequently, we focus in the proof on lower bounding this quantity, which can be decomposed by Cauchy-Shwartz inequality into:\nEθ [ Eη[(v̂ − vη)2 | η] ] ≥ 1 2 Eθ [ Eη[(v̂ − Eθ[vη])2 | η] ] − Eθ [ (vη − Eθ[vη])2 ] .\nThen we can further take a worst-case over all problems in the above inequality to obtain\nsup η Eη[(v̂ − vη)2] ≥ sup θ\nEθ [ Eη[(v̂ − vη)2 | η] ] ≥ sup\nθ\n1 2 Eθ [ Eη[(v̂ − Eθ[vη])2 | η] ] ︸ ︷︷ ︸\nT1\n−Eθ [ (vη − Eθ[vη])2 ]︸ ︷︷ ︸ T2 . (26)\nThis decomposition is rather interesting. It says that the expected MSE of an estimator in estimating vη can be related to the MSE of the same estimator in estimating the quantity Eθ[vη], as long as the variance of the quantity vη under the distribution generated by θ is not too large. This is a very important observation, since we can now choose to instead study the MSE of an estimator in estimating Eθ[vη] as captured by T1. Unlike the distribution Dη which is degenerate, this problem has a non-trivial noise arising from the randomized draw of η according to θ. Thus we can use similar techniques as the proof of Theorem 4, albeit where the reward distribution is a scaled Bernoulli instead of Gaussian. For now, we focus on controlling T1, and T2 will be handled later.\nIn order to bound T1, we will consider two carefully designed choices θ1 and θ2 to induce two different problem instances and show that T1 is large for any estimator under one of the two parameters. In doing this, it would be convenient to use the additional shorthand `θ(v̂) = (v̂ − Eθ[vη])2. Proceeding as in the proof of Theorem 3, we have\nT1 = sup θ\nEθ [ Eη[(v̂ − Eθ[vη])2 | η] ] ≥ sup\nθ Eθ [Eη[`θ(v̂) | η]]\n≥ sup θ Pθ (`θ(v̂) ≥ ) ≥ max θ∈{θ1,θ2} Pθ (`θ(v̂) ≥ )\n≥ 2 [Pθ1 (`θ1(v̂) ≥ ) + Pθ2 (`θ2(v̂) ≥ )] .\nReduction to hypothesis testing As in the proof of Theorem 3, we now reduce the estimation problem into a hypothesis test for whether the data was generating according to the parameters θ1 or θ2. The arguments here are similar to the earlier proof, and we will be a bit terse in this presentation.\nAs before, our hypothesis test has entire knowledge ofDη as well as θ1 and θ2. Consequently, we construct a test based on picking θ1 whenever `θ1(v̂) ≤ `θ2(v̂). As before, we will ensure that |Eθ1 [vη]−Eθ2 [vη]| ≥ 2 √ so that for any estimator v̂, we have `θ1(v̂) + `θ2(v̂) ≥ 2 .\nBy a similar argument, we further conclude that the error of our hypothesis test under a uniform prior on the choices of θ is at most\n1 2 [Pθ1 (`θ1(v̂) ≥ ) + Pθ2 (`θ2(v̂) ≥ )]\nInvoking Le Cam’s argument Once again, we can lower bound the error rate of our test by invoking the result of Le Cam. Doing so, requires an upper bound on the KL-divergence DKL(Pθ1‖Pθ2). The only difference from our earlier argument is that these distributions are now Bernoullis instead of Normals based on the construction in Equation 25. More formally, we have\nDKL(Pθ1‖Pθ2) = ∫ log p(r; θ1(x, a))\np(r; θ2(x, a)) dp(r; θ1(x, a)) dp(x) dµ(a|x)\n= EDKL(Ber(θ1(x, a)‖Ber(θ2(x, a)))).\nInvoking Lemma 4, we can further upper bound this as\nDKL(Pθ1‖Pθ2) ≤ 1 4 Eµ [ (θ1(x, a)− θ2(x, a))2 ] .\nPicking the parameters It remains to carefully choose θ1 and θ2. We define θ2 to be a uniform prior: θ2(x, a) ≡ 0.5, and let θ1(x, a) = θ2(x, a) + ∆(x, a), where ∆(x, a) will be chosen to satisfy certain constraints as before. Collecting the constraints, we get the maximization problem\nmax ∆\nsuch that Eµ[ρ(x, a)∆(x, a)Rmax(x, a)] ≥ 2 √ ,\n1 4 E [ ∆(x, a)2 ] ≤ log 2 n and 0 ≤ ∆(x, a) ≤ 0.5.\nFor some α > 0 to be determined shortly, we pick\n∆ = min\n{ ρRmaxα\nEµρ2R2max , 0.5\n} .\nThe bound constraints are satisfied by construction and we set α = √\n4 log 2Eµρ2R2max/n to satisfy the second constraint. A feasible choice of then yields\n2 √ = Eµ[ρ(x, a)∆(x, a)Rmax(x, a)] ≥ Eµ\n[ ρ2R2maxα1(ρRmax ≤ Eµ[ρ2R2max]/2α)\nEµ[ρ2α2] ] = α− α Eµ [ ρ2R2max1(ρRmax ≤ Eµ[ρ2R2max]/2α)\n] Eµ[ρ2R2max] .\nCollecting our arguments this far, we have established that\nT1 ≥ 2 [Pθ1 (`θ1(v̂) ≥ ) + Pθ2 (`θ2(v̂) ≥ )]\n≥ 2 exp(−nDKL(Pθ1‖Pθ2)) ≥ 4\n≥ log 2Eµρ 2R2max\n4n\n1− Eµ ( ρ2R2max1 ({ ρRmax > √ nEµ(ρ2R2max)/(16 log 2) })) Eµρ2R2max 2\nIn order to complete the proof, we need to further upper bound T2 in the decomposition (26). This is done in Lemma 2, and plugging this upper bound in Equation 26 completes the proof.\nLemma 2. Let λ(x) be any context distribution. Then for any distribution θ over the random variable η(x, a) = E[r | x, a], satisfying 0 ≤ η(x, a) ≤ Rmax(x, a), we have\nEθ [ (vη − Eθ[vη])2 ] ≤ λ0 log(4/λ0)Eµ[R2maxρ2] ,\nwhere λ0 = maxx∈X λ(x).\nProof. We start by bounding the range of (vη − Eθ[vη])2, viewed as a random variable under θ. From the definition of vη,\n0 ≤ vη ≤ Eπ[Rmax] = Eµ[ρRmax] ,\nso also 0 ≤ Eθ[vη] ≤ Eµ[ρRmax]. Hence, |vη − Eθ[vη]| ≤ Eµ[ρRmax], and we obtain the bound\n(vη − Eθ[vη])2 ≤ (Eµ[ρRmax])2 ≤ Eµ[ρ2R2max], (27)\nbecause ρRmax ≥ 0. The proof proceeds by applying Hoeffding’s inequality to control the probability that (vη−Eθ[vη])2 ≥ t2 for a suitable t. Then we can, with high probability, use the bound (vη − Eθ[vη])2 ≥ t2, and with the remaining small probability apply the bound of Eq. (27).\nTo apply Hoeffding’s inequality, we write vη explicitly as vη = ∑ x∈X ∑ a∈A η(x, a)ρ(x, a)µ(a|x)λ(x) =: ∑ x∈X ∑ a∈A Y (x, a).\nThat is, the quantity vη can be written as a sum of a finite number of independent random variables, where we use the fact that the number of contexts is finite. Note that µ(a|x) ≤ 1 and λ(x) ≤ λ0, so each summand obeys\n0 ≤ Y (x, a) ≤ Rmax(x, a)ρ(x, a) √ µ(a|x)λ(x) √ λ(x) ≤ Rmax(x, a)ρ(x, a) √ µ(a|x)λ(x)λ0 .\nBy Hoeffding’s inequality, we have P(|vη − Eθvη| ≥ t) ≤ 2 exp { − 2t\n2∑ x∈X ∑ a∈AR 2 max(x, a)ρ 2(x, a)µ(a|x)λ(x)λ0 } = 2 exp { − 2t 2\nλ0Eµ[R2maxρ2]\n} .\nTake t = √ λ0 log(4/λ0)Eµ[R2maxρ2]/2 in the above bound, which yields\nP [ (vη − Eθvη)2 ≥ t2 ] = P [ |vη − Eθvη| ≥ t ] ≤ λ0\n2 .\nThus, using Eq. (27), we have\nEθ [ (vη − Eθvη)2 ] ≤ t2P [ (vη − Eθvη)2 < t2 ] + Eµ[ρ2R2max] · P [ (vη − Eθvη)2 ≥ t2 ] ≤ t2 + Eµ[ρ2R2max] ·\nλ0 2\n= λ0 log(4/λ0)Eµ[R2maxρ2]\n2 + Eµ[ρ2R2max] · λ0 2\n≤ λ0 log(4/λ0)Eµ[R2maxρ2] ."
    }, {
      "heading" : "B Other proofs",
      "text" : "Proof of Lemma 1. The variance (therefore MSE) of the importance weighting estimator can be decomposed into two parts\nVar (v̂IPS) = Eµ(Var(v̂IPS|x, a) + Varµ(E(v̂IPS|x, a)) = 1\nn Eµ[ρ2Var(r|x, a)] +\n1 n Varµ[ρE(r|x, a)]\n≤ 1 n Eµ[ρ2σ2] + 1 n Eµ[ρ2E(r|x, a)2] ≤ 1 n Eµ[ρ2σ2] + 1 n Eµ[ρ2R2max]\nProof of Theorem 2. Denote Ax ⊆ A to be Ax = { a ∈ A ∣∣∣π(a|x)µ(a|x) ≤ τ}. For brevity, we also denote Ai := Axi . We decompose the mean square error and control each term separately.\nMSE(v̂SWITCH) = |Ev̂SWITCH − vπ|2 + Var(v̂SWITCH).\nWe first calculate the bias. Note that bias is incurred only in the terms that fall in ACx , so that\nEv̂SWITCH − vπ = E ∑ a∈Acx r̂(x, a)π(a|x) − E ∑ a∈Acx E(r|x, a)π(a|x)  = Eπ [(r̂(x, a)− E(r|x, a))1(a ∈ Acx)] = Eπ [r̂(x, a)− E(r|x, a)| a ∈ Acx]Pπ(a ∈ Acx) = Eπ[ (x, a)| a ∈ Acx]Pπ(a ∈ Acx),\nwhere we recall that (x, a) = r̂(x, a)− E[r| x, a]. Next we upper bound the variance. Note that the variance contributions from the IPS part and the DM part are not independent, since the indicators ρ(xi, a) > τ and ρ(xi, a) ≤ τ are mutually exclusive. To simplify the analysis, we use the following inequality that holds for any random variable X and Y :\nVar(X + Y ) ≤ 2Var(X) + 2Var(Y ).\nThis allows us to calculate the variance of each part separately.\nVar(v̂SWITCH) ≤2 Var\n( 1\nn n∑ i=1 [riρi1(ai ∈ Ai)]\n) + 2 Var ( 1\nn n∑ i=1 ∑ a∈A r̂(x, a)π(a|x)1(a ∈ ACx )\n)\n= 2\nn Varµ [rρ1(a ∈ Ax)] +\n2 n Var ∑ a∈Acx r̂(x, a)π(a|x)  = 2\nn EµVar [rρ1(a ∈ Ax)| x, a] +\n2 n VarµE[rρ1(a ∈ Ax)| x, a] + 2 n Var ∑ a∈Acx r̂(x, a)π(a|x)  ≤ 2 n EµVar [rρ1(a ∈ Ax)| x, a] + 2 n Eµ(E[rρ1(a ∈ Ax)| x, a])2 + 2 n E ∑ a∈Acx r̂(x, a)π(a|x) 2 ≤ 2 n Eµ [ σ2ρ21(a ∈ Ax) ] + 2 n Eµ [ R2maxρ 21(a ∈ Ax) ] + 2 n E (∑ a∈Ac r̂(x, a)π(a|x) )2 .\nTo complete the proof, note that the first term is further upper bounded using Jensen’s inequality as\nE ∑ a∈Acx r̂(x, a)π(a|x) 2 = E ∑ a∈ACx π(a|x) 2( ∑ a∈Aci r̂(x, a)π(a|x)∑ a∈ACx π(a|x) )2 ≤ E\n∑ a∈ACx π(a|x) ∑ a∈ACx r̂(x, a)2π(a|x)  ≤ Eπ [ R2max1(ρ > τ) ] ,\nwhere the final inequality uses ∑\na∈ACx π(a|x) ≤ 1 and r̂(x, a) ∈ [0, Rmax(x, a)] almost surely. Combining the bias and variance bounds, we get the stated MSE upper bound."
    }, {
      "heading" : "C Utility Lemmas",
      "text" : "Lemma 3 (Hoeffding, 1963, Theorem 2). Let Xi ∈ [ai, bi] and X1, ..., Xn are drawn independently. Then the empirical mean X̄ = 1n(X1 + ...+Xn) obeys\nP(|X̄ − E[X̄]| ≥ t) ≤ 2e − 2n 2t2∑n i=1 (bi−ai)2 .\nLemma 4 (Bernoulli KL-divergence). For 0 < p, q < 1, we have\nDKL(Ber(p)‖Ber(q)) ≤ (p− q)2( 1\nq +\n1\n1− q ).\nProof.\nDKL(Ber(p)‖Ber(q)) = p log( p q ) + (1− p) log(1− p 1− q )\n≤ pp− q q + (1− p)q − p 1− q = (p− q)2 q + (p− q) + (p− q) 2 1− q + (q − p)\n= (p− q)2(1 q + 1 1− q )."
    } ],
    "references" : [ {
      "title" : "Doubly robust estimation in missing data and causal inference models",
      "author" : [ "Heejung Bang", "James M Robins" ],
      "venue" : null,
      "citeRegEx" : "Bang and Robins.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bang and Robins.",
      "year" : 2005
    }, {
      "title" : "Data-adaptive selection of the truncation level for inverseprobability-of-treatment-weighted estimators",
      "author" : [ "Oliver Bembom", "Mark J van der Laan" ],
      "venue" : null,
      "citeRegEx" : "Bembom and Laan.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bembom and Laan.",
      "year" : 2008
    }, {
      "title" : "Counterfactual reasoning and learning systems: the example of computational advertising",
      "author" : [ "Léon Bottou", "Jonas Peters", "Joaquin Quinonero Candela", "Denis Xavier Charles", "Max Chickering", "Elon Portugaly", "Dipankar Ray", "Patrice Y Simard", "Ed Snelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bottou et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bottou et al\\.",
      "year" : 2013
    }, {
      "title" : "Some results on generalized difference estimation and generalized regression estimation for finite populations",
      "author" : [ "Claes M Cassel", "Carl E Särndal", "Jan H Wretman" ],
      "venue" : null,
      "citeRegEx" : "Cassel et al\\.,? \\Q1976\\E",
      "shortCiteRegEx" : "Cassel et al\\.",
      "year" : 1976
    }, {
      "title" : "Learning bounds for importance weighting",
      "author" : [ "Corinna Cortes", "Yishay Mansour", "Mehryar Mohri" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Cortes et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cortes et al\\.",
      "year" : 2010
    }, {
      "title" : "Doubly robust policy evaluation and learning",
      "author" : [ "Miroslav Dudík", "John Langford", "Lihong Li" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Dudík et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dudík et al\\.",
      "year" : 2011
    }, {
      "title" : "Doubly robust policy evaluation and optimization",
      "author" : [ "Miroslav Dudík", "Dumitru Erhan", "John Langford", "Lihong Li" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "Dudík et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dudík et al\\.",
      "year" : 2014
    }, {
      "title" : "Covariate shift by kernel mean matching",
      "author" : [ "Arthur Gretton", "Alex Smola", "Jiayuan Huang", "Marcel Schmittfull", "Karsten Borgwardt", "Bernhard Schölkopf" ],
      "venue" : "Dataset shift in machine learning,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2009
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "Wassily Hoeffding" ],
      "venue" : "Journal of the American statistical association,",
      "citeRegEx" : "Hoeffding.,? \\Q1963\\E",
      "shortCiteRegEx" : "Hoeffding.",
      "year" : 1963
    }, {
      "title" : "Statistics and causal inference",
      "author" : [ "Paul W Holland" ],
      "venue" : "Journal of the American statistical Association,",
      "citeRegEx" : "Holland.,? \\Q1986\\E",
      "shortCiteRegEx" : "Holland.",
      "year" : 1986
    }, {
      "title" : "A generalization of sampling without replacement from a finite universe",
      "author" : [ "Daniel G Horvitz", "Donovan J Thompson" ],
      "venue" : "Journal of the American statistical Association,",
      "citeRegEx" : "Horvitz and Thompson.,? \\Q1952\\E",
      "shortCiteRegEx" : "Horvitz and Thompson.",
      "year" : 1952
    }, {
      "title" : "Doubly robust off-policy evaluation for reinforcement learning",
      "author" : [ "Nan Jiang", "Lihong Li" ],
      "venue" : "In ICML’16,",
      "citeRegEx" : "Jiang and Li.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jiang and Li.",
      "year" : 2016
    }, {
      "title" : "Unbiased offline evaluation of contextual-banditbased news article recommendation algorithms",
      "author" : [ "Lihong Li", "Wei Chu", "John Langford", "Xuanhui Wang" ],
      "venue" : "In Proceedings of the fourth ACM international conference on Web search and data mining,",
      "citeRegEx" : "Li et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2011
    }, {
      "title" : "Toward minimax off-policy value estimation",
      "author" : [ "Lihong Li", "Rémi Munos", "Csaba Szepesvári" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Statistical analysis with missing data",
      "author" : [ "Roderick JA Little", "Donald B Rubin" ],
      "venue" : null,
      "citeRegEx" : "Little and Rubin.,? \\Q2002\\E",
      "shortCiteRegEx" : "Little and Rubin.",
      "year" : 2002
    }, {
      "title" : "Weighting adjustment for unit nonresponse",
      "author" : [ "H Lock Oh", "Frederick J Scheuren" ],
      "venue" : "Incomplete data in sample surveys,",
      "citeRegEx" : "Oh and Scheuren.,? \\Q1983\\E",
      "shortCiteRegEx" : "Oh and Scheuren.",
      "year" : 1983
    }, {
      "title" : "Semiparametric efficiency in multivariate regression models with missing data",
      "author" : [ "James M Robins", "Andrea Rotnitzky" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Robins and Rotnitzky.,? \\Q1995\\E",
      "shortCiteRegEx" : "Robins and Rotnitzky.",
      "year" : 1995
    }, {
      "title" : "Improved double-robust estimation in missing data and causal inference models",
      "author" : [ "Andrea Rotnitzky", "Quanhong Lei", "Mariela Sued", "James M Robins" ],
      "venue" : null,
      "citeRegEx" : "Rotnitzky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rotnitzky et al\\.",
      "year" : 2012
    }, {
      "title" : "The exact law of large numbers via fubini extension and characterization of insurable risks",
      "author" : [ "Yeneng Sun" ],
      "venue" : "Journal of Economic Theory,",
      "citeRegEx" : "Sun.,? \\Q2006\\E",
      "shortCiteRegEx" : "Sun.",
      "year" : 2006
    }, {
      "title" : "Data-efficient off-policy policy evaluation for reinforcement learning",
      "author" : [ "Philip S Thomas", "Emma Brunskill" ],
      "venue" : "In ICML’16,",
      "citeRegEx" : "Thomas and Brunskill.,? \\Q2016\\E",
      "shortCiteRegEx" : "Thomas and Brunskill.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Since in the multi-armed bandit problem the IPS is suboptimal [Li et al., 2015], our result highlights the difficulty of the contextual setting with non-degenerate context distributions.",
      "startOffset" : 62,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "In particular, there are several estimators which are unbiased under mild assumptions, such as inverse propensity scoring (IPS) [Horvitz and Thompson, 1952], and sharp estimates on their mean squared error (MSE) for policy evaluation are well-known [Dudík et al.",
      "startOffset" : 128,
      "endOffset" : 156
    }, {
      "referenceID" : 6,
      "context" : "In particular, there are several estimators which are unbiased under mild assumptions, such as inverse propensity scoring (IPS) [Horvitz and Thompson, 1952], and sharp estimates on their mean squared error (MSE) for policy evaluation are well-known [Dudík et al., 2014].",
      "startOffset" : 249,
      "endOffset" : 269
    }, {
      "referenceID" : 6,
      "context" : "Some approaches, such as the doubly-robust method (DR) [Dudík et al., 2014] (also see the references therein for its origin in statistics and application in causal inference, e.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : ", [Robins and Rotnitzky, 1995, Bang and Robins, 2005]), combine the model with an IPS-style unbiased estimation and remain consistent, with known estimates for their MSE. All these works focus on developing specific methods alongside upper bounds on their MSE. Little work, on the other hand, exists on the question of the fundamental statistical hardness of off-policy evaluation and the optimality (or the lack of) of the existing methods. A notable exception is the recent work of Li et al. [2015], who study off-policy evaluation in multi-armed bandits—a special case of our setting, without any contexts—and provide a minimax lower bound on the MSE.",
      "startOffset" : 31,
      "endOffset" : 501
    }, {
      "referenceID" : 13,
      "context" : "In contrast with context-free multi-armed bandits [Li et al., 2015], our lower bound matches the MSE upper bound for IPS up to constants, so long as the contexts have a non-degenerate distribution.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 5,
      "context" : "We use the standard setup for off-policy evaluation in contextual bandits as in prior works Dudík et al. [2014]. We start with the formal setup and then discuss the related work.",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "The simplest estimator in this setting, called Inverse Propensity Scoring (IPS) [Horvitz and Thompson, 1952] is defined as: v̂ IPS = n ∑",
      "startOffset" : 80,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : ", 2012] as well as the references in [Dudík et al., 2014]).",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "Li et al. [2015] first studied this question under a minimax setup, but focused on the multi-arm bandits problem.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 5,
      "context" : "Jiang and Li [2016] built on top of Li et al.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 5,
      "context" : "Jiang and Li [2016] built on top of Li et al. [2015] proves a Cramer-Rao-style lower bound for policy-evaluation in reinforcement learning, but the assumption of a small number of observed states precludes contextual bandits from being a special case.",
      "startOffset" : 0,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "Cortes et al. [2010] studied the related covariate shift problem in the statistical learning setting and proved upper bounds and lower bounds of the minimax excess risk that depends on the Renyi divergence of the two covariate distributions.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : ", 2011, 2014] and reinforcement learning [Jiang and Li, 2016].",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "[Cassel et al., 1976, Robins and Rotnitzky, 1995], were recently used for off-policy value estimation in the contextual bandits problem [Dudík et al., 2011, 2014] and reinforcement learning [Jiang and Li, 2016]. These also provide error estimates for the proposed estimators, which we will compare with the lower bounds that we will obtain. The doubly robust techniques also have a flavor of incorporating existing reward models, an idea which we will also leverage in the development of the SWITCH estimator in Section 4. We note that similar ideas were also recently investigated in the context of reinforcement learning by Thomas and Brunskill [2016].",
      "startOffset" : 1,
      "endOffset" : 654
    }, {
      "referenceID" : 12,
      "context" : "The family of reward distributions D(r | x, a) that we study is a natural generalization of the class studied by Li et al. [2015] for multi-armed bandits.",
      "startOffset" : 113,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : ", the bound on the variance of IPS in Dudík et al. [2014], which assumes the finiteness of these second moments).",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 10,
      "context" : "Comparing with the upper bounds on the MSE for existing estimators, we find that this lower bound precisely matches the upper bound for the IPS estimator [Horvitz and Thompson, 1952] (defined in Equation 2).",
      "startOffset" : 154,
      "endOffset" : 182
    }, {
      "referenceID" : 10,
      "context" : "Lemma 1 (IPS estimator [Horvitz and Thompson, 1952]).",
      "startOffset" : 23,
      "endOffset" : 51
    }, {
      "referenceID" : 12,
      "context" : "Comparison with the multi-armed bandit setting of Li et al. [2015] The most closely related prior result was due to Li et al.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "Comparison with the multi-armed bandit setting of Li et al. [2015] The most closely related prior result was due to Li et al. [2015], who showed matching upper and lower bounds on the minimax risk for multi-armed bandits.",
      "startOffset" : 50,
      "endOffset" : 133
    }, {
      "referenceID" : 12,
      "context" : "Comparison with the multi-armed bandit setting of Li et al. [2015] The most closely related prior result was due to Li et al. [2015], who showed matching upper and lower bounds on the minimax risk for multi-armed bandits. The somewhat surprising conclusion of their work was the sub-optimality of IPS, which might appear at odds with our conclusion regarding IPS above. However, this difference actually highlights the additional challenges in contextual bandits beyond multi-armed bandits. This is best illustrated in a noiseless setting, where σ = 0 in the rewards. This makes the multi-armed bandit problem trivial, we can just measure the reward of each arm with one pull and find out the optimal choice. However, there is still a non-trivial lower bound of Ω(Eμ[ρR max]/n) in the contextual bandit setting, which is exactly the upper bound on the MSE of IPS when the rewards have no noise. This difference crucially relies on λ0 being suitably small relative to the sample size n. When the number of contexts is small, independent estimation for each context can be done in a noiseless setting as observed by Li et al. [2015]. However, once the context distribution is rich enough, then even with noiseless rewards, there is significant variance in the value estimates based on which contexts were observed.",
      "startOffset" : 50,
      "endOffset" : 1131
    }, {
      "referenceID" : 12,
      "context" : "Comparison with the multi-armed bandit setting of Li et al. [2015] The most closely related prior result was due to Li et al. [2015], who showed matching upper and lower bounds on the minimax risk for multi-armed bandits. The somewhat surprising conclusion of their work was the sub-optimality of IPS, which might appear at odds with our conclusion regarding IPS above. However, this difference actually highlights the additional challenges in contextual bandits beyond multi-armed bandits. This is best illustrated in a noiseless setting, where σ = 0 in the rewards. This makes the multi-armed bandit problem trivial, we can just measure the reward of each arm with one pull and find out the optimal choice. However, there is still a non-trivial lower bound of Ω(Eμ[ρR max]/n) in the contextual bandit setting, which is exactly the upper bound on the MSE of IPS when the rewards have no noise. This difference crucially relies on λ0 being suitably small relative to the sample size n. When the number of contexts is small, independent estimation for each context can be done in a noiseless setting as observed by Li et al. [2015]. However, once the context distribution is rich enough, then even with noiseless rewards, there is significant variance in the value estimates based on which contexts were observed. This distinction is further highlighted in the proof of Theorem 1, and is obtained by combining two separate lower bounds. The first lower bound considers the case of noisy rewards, and is a relatively straightforward generalization of the proof of Li et al. [2015]. The second lower bound focuses on noiseless rewards, and shows how the variance in a rich context distribution allows the environment to essentially simulate noisy rewards, even when the reward signal itself is noiseless.",
      "startOffset" : 50,
      "endOffset" : 1579
    }, {
      "referenceID" : 6,
      "context" : "where the DM stands for direct method, a name for this approach which has been used in prior works [Dudík et al., 2014].",
      "startOffset" : 99,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "where the DM stands for direct method, a name for this approach which has been used in prior works [Dudík et al., 2014]. This approach appears attractive when r̂ is a close to E[r | x, a]. Crucially, r̂ can be evaluated for any x, a pair, meaning that there is no need to do use importance weights unlike in IPS. This suggests that we might completely eliminate any dependence on ρ using this approach., and it is easily seen that given any estimator r̂ such th which takes values in [0, Rmax(x, a)] for any x, a, the variance satisfies: Var(v̂DM) ≤ 1 n Eπ[R max] = 1 n Eμ[ρR max]. (6) That is, there is a linear, rather than quadratic dependence on ρ, unlike in the worst case minimax risk of Corollary 1. The catch, however, is that such estimators can have an uncontrolled bias of O(Eπ[Rmax]) in the worst case, meaning that even asymptotic consistency is not guaranteed. Prior works have addressed the bias of DM, while trying to reduce the variance of IPS by using doubly robust estimators. A concrete estimator previously used for off-policy evaluation in Dudík et al. [2014] is defined as",
      "startOffset" : 100,
      "endOffset" : 1082
    }, {
      "referenceID" : 5,
      "context" : "However, based on the results of Dudík et al. [2014], the MSE of the DR estimator in this special case is",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "Bottou et al. [2013] for a detailed discussion), which can often reduce the variance drastically at the cost of a little bias.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "This approach, with a specific choice of τ was described in Bottou et al. [2013] and will be evaluated in the experiments under the name Trimmed IPS.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "This approach, with a specific choice of τ was described in Bottou et al. [2013] and will be evaluated in the experiments under the name Trimmed IPS. • Thomas and Brunskill [2016] study a similar estimator in the more general context of reinforcement learning.",
      "startOffset" : 60,
      "endOffset" : 180
    }, {
      "referenceID" : 5,
      "context" : "The analysis in Theorem 2 still applies, replacing the variance of IPS with that of DR from Dudík et al. [2014]. Since no independence was required in our analysis between the IPS and the DM parts of the estimator, the result is also robust to the use of a common data-dependent estimator r̂ = r̂′ in SWITCH-DR (10).",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 19,
      "context" : "Finally, we should mention that this development is quite related to the MAGIC estimator of Thomas and Brunskill [2016], which was discussed following Theorem 2.",
      "startOffset" : 92,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "We will be using the same 10 UCI data sets that was used previously by Dudík et al. [2011] and convert the multi-class classification problem to contextual bandits by 1.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "We simulate the covariate-shifted data set using the same technique as in Dudík et al. [2011], which follows standard practice as was described by Gretton et al.",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "We simulate the covariate-shifted data set using the same technique as in Dudík et al. [2011], which follows standard practice as was described by Gretton et al. [2009]. In each data set with n rows, we treat the uniform distribution over the data set itself as a surrogate of the population distribution so that we know the ground truth of the rewards.",
      "startOffset" : 74,
      "endOffset" : 169
    }, {
      "referenceID" : 4,
      "context" : "This approach was suggested by Dudík et al. [2011], and is standard practice of doubly robust estimators when the direct method (or the oracle estimator) needs to be estimated from the data.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : ", Bembom and van der Laan, 2008], or the terms with weights larger than τ are removed altogether as described in Bottou et al. [2013]. Note that the trimmed IPS is a special case of SWITCH when the direct estimator ≡ 0.",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 18,
      "context" : "The authors would like to thank Lihong Li and John Langford for helpful discussions, Edward Kennedy for bringing our attention to related problems and recent development in causal inference and Wei He for pointing us to [Sun, 2006] for rigorous a measure-theoretic treatment to a continuum of random variables and the corresponding law of large numbers.",
      "startOffset" : 220,
      "endOffset" : 231
    } ],
    "year" : 2016,
    "abstractText" : "We consider the problem of off-policy evaluation—estimating the value of a target policy using data collected by another policy—under the contextual bandit model. We establish a minimax lower bound on the mean squared error (MSE), and show that it is matched up to constant factors by the inverse propensity scoring (IPS) estimator. Since in the multi-armed bandit problem the IPS is suboptimal [Li et al., 2015], our result highlights the difficulty of the contextual setting with non-degenerate context distributions. We further consider improvements on this minimax MSE bound, given access to a reward model. We show that the existing doubly robust approach, which utilizes such a reward model, may continue to suffer from high variance even when the reward model is perfect. We propose a new estimator called SWITCH which more effectively uses the reward model and achieves a superior bias-variance tradeoff compared with prior work. We prove an upper bound on its MSE and demonstrate its benefits empirically on a diverse collection of datasets, often seeing orders of magnitude improvements over a number of baselines.",
    "creator" : "LaTeX with hyperref package"
  }
}