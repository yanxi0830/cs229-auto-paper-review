{
  "name" : "1205.4839.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Off-Policy Actor-Critic",
    "authors" : [ "Thomas Degris", "Richard S. Sutton" ],
    "emails" : [ "thomas.degris@inria.fr", "whitem@cs.ualberta.ca", "sutton@cs.ualberta.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The reinforcement learning framework is a general temporal learning formalism that has, over the last few decades, seen a marked growth in algorithms and applications. Until recently, however, practical online\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nmethods with convergence guarantees have been restricted to the on-policy setting, in which the agent learns only about the policy it is executing.\nIn an off-policy setting, on the other hand, an agent learns about a policy or policies different from the one it is executing. Off-policy methods have a wider range of applications and learning possibilities. Unlike onpolicy methods, off-policy methods are able to, for example, learn about an optimal policy while executing an exploratory policy (Sutton & Barto, 1998), learn from demonstration (Smart & Kaelbling, 2002), and learn multiple tasks in parallel from a single sensorimotor interaction with an environment (Sutton et al., 2011). Because of this generality, off-policy methods are of great interest in many application domains.\nThe most well known off-policy method is Q-learning (Watkins & Dayan, 1992). However, while Q-Learning is guaranteed to converge to the optimal policy for the tabular (non-approximate) case, it may diverge when using linear function approximation (Baird, 1995). Least-squares methods such as LSTD (Bradtke & Barto, 1996) and LSPI (Lagoudakis & Parr, 2003) can be used off-policy and are sound with linear function approximation, but are computationally expensive; their complexity scales quadratically with the number of features and weights. Recently, these problems have been addressed by the new family of gradientTD (Temporal Difference) methods (e.g., Sutton et al., 2009), such as Greedy-GQ (Maei et al., 2010), which are of linear complexity and convergent under off-policy training with function approximation.\nAll action-value methods, including gradient-TD methods such as Greedy-GQ, suffer from three important limitations. First, their target policies are deterministic, whereas many problems have stochastic optimal policies, such as in adversarial settings or in partially observable Markov decision processes. Second, finding the greedy action with respect to the action-\nar X\niv :1\n20 5.\n48 39\nv1 [\ncs .L\nG ]\n2 2\nM ay\n2 01\nvalue function becomes problematic for larger action spaces. Finally, a small change in the action-value function can cause large changes in the policy, which creates difficulties for convergence proofs and for some real-time applications.\nThe standard way of avoiding the limitations of actionvalue methods is to use policy-gradient algorithms (Sutton et al., 2000) such as actor-critic methods (e.g., Bhatnagar et al., 2009). For example, the natural actor-critic, an on-policy policy-gradient algorithm, has been successful for learning in continuous action spaces in several robotics applications (Peters & Schaal, 2008).\nThe first and main contribution of this paper is to introduce the first actor-critic method that can be applied off-policy, which we call Off-PAC, for Off-Policy Actor–Critic. Off-PAC has two learners: the actor and the critic. The actor updates the policy weights. The critic learns an off-policy estimate of the value function for the current actor policy, different from the (fixed) behavior policy. This estimate is then used by the actor to update the policy. For the critic, in this paper we consider a version of Off-PAC that uses GTD(λ) (Maei, 2011), a gradient-TD method with eligibitity traces for learning state-value functions. We define a new objective for our policy weights and derive a valid backward-view update using eligibility traces. The time and space complexity of Off-PAC is linear in the number of learned weights.\nThe second contribution of this paper is an off-policy policy-gradient theorem and a convergence proof for Off-PAC when λ = 0, under assumptions similar to previous off-policy gradient-TD proofs.\nOur third contribution is an empirical comparison of Q(λ), Greedy-GQ, Off-PAC, and a soft-max version of Greedy-GQ that we call Softmax-GQ, on three benchmark problems in an off-policy setting. To the best of our knowledge, this paper is the first to provide an empirical evaluation of gradient-TD methods for off-policy control (the closest known prior work is the work of Delp (2011)). We show that Off-PAC outperforms other algorithms on these problems."
    }, {
      "heading" : "1. Notation and Problem Setting",
      "text" : "In this paper, we consider Markov decision processes with a discrete state space S, a discrete action spaceA, a distribution P : S × S ×A → [0, 1], where P (s′|s, a) is the probability of transitioning into state s′ from state s after taking action a, and an expected reward function R : S×A×S → R that provides an expected reward for taking action a in state s and transitioning\ninto s′. We observe a stream of data, which includes states st ∈ S, actions at ∈ A, and rewards rt ∈ R for t = 1, 2, . . . with actions selected from a fixed behavior policy, b(a|s) ∈ (0, 1].\nGiven a termination condition γ : S → [0, 1] (Sutton et al., 2011), we define the value function for π : S×A → (0, 1] to be:\nV π,γ(s) = E [rt+1 + . . .+ rt+T |st = s] ∀s ∈ S (1)\nwhere policy π is followed from time step t and terminates at time t + T according to γ. We assume termination always occurs in a finite number of steps.\nThe action-value function, Qπ,γ(s, a), is defined as:\nQπ,γ(s, a) =∑ s′∈S P (s′|s, a)[R(s, a, s′) + γ(s′)V π,γ(s′)] (2) for all a ∈ A and for all s ∈ S. Note that V π,γ(s) =∑ a∈A π(a|s)Qπ,γ(s, a), for all s ∈ S.\nThe policy πu : A×S → [0, 1] is an arbitrary, differentiable function of a weight vector, u ∈ RNu , Nu ∈ N, with πu(a|s) > 0 for all s ∈ S, a ∈ A. Our aim is to choose u so as to maximize the following scalar objective function:\nJγ(u) = ∑ s∈S db(s)V πu,γ(s) (3)\nwhere db(s) = limt→∞ P (st = s|s0, b) is the limiting distribution of states under b and P (st = s|s0, b) is the probability that st = s when starting in s0 and executing b. The objective function is weighted by db because, in the off-policy setting, data is obtained according to this behavior distribution. For simplicity of notation, we will write π and implicitly mean πu."
    }, {
      "heading" : "2. The Off-PAC Algorithm",
      "text" : "In this section, we present the Off-PAC algorithm in three steps. First, we explain the basic theoretical ideas underlying the gradient-TD methods used in the critic. Second, we present our off-policy version of the policy-gradient theorem. Finally, we derive the forward view of the actor and convert it to a backward view to produce a complete mechanistic algorithm using eligibility traces."
    }, {
      "heading" : "2.1. The Critic: Policy Evaluation",
      "text" : "Evaluating a policy π consists of learning its value function, V π,γ(s), as defined in Equation 1. Since it is often impractical to explicitly represent every\nstate s, we learn a linear approximation of V π,γ(s): V̂ (s) = vTxs where xs ∈ RNv , Nv ∈ N, is the feature vector of the state s, and v ∈ RNv is another weight vector.\nGradient-TD methods (Sutton et al., 2009) incrementally learn the weights, v, in an off-policy setting, with a guarantee of stability and a linear per-time-step complexity. These methods minimize the λ-weighted mean-squared projected Bellman error:\nMSPBE(v) = ||V̂ −ΠTλ,γπ V̂ ||2D\nwhere V̂ = Xv; X is the matrix whose rows are all xs; λ is the decay of the eligibility trace; D is a matrix with db(s) on its diagonal; Π is a projection operator that projects a value function to the nearest representable value function given the function approximator; and Tλ,γπ is the λ-weighted Bellman operator for the target policy π with termination probability γ (e.g., see Maei & Sutton, 2010). For a linear representation, Π = X(XTDX)−1XTD.\nIn this paper, we consider the version of Off-PAC that updates its critic weights by the GTD(λ) algorithm introduced by Maei (2011)."
    }, {
      "heading" : "2.2. Off-policy Policy-gradient Theorem",
      "text" : "Like other policy gradient algorithms, Off-PAC updates the weights approximately in proportion to the gradient of the objective:\nut+1 − ut ≈ αu,t∇uJγ(ut) (4)\nwhere αu,t ∈ R is a positive step-size parameter. Starting from Equation 3, the gradient can be written:\n∇uJγ(u) = ∇u [∑ s∈S db(s) ∑ a∈A π(a|s)Qπ,γ(s, a) ] = ∑ s∈S db(s) ∑ a∈A [∇uπ(a|s)Qπ,γ(s, a)\n+ π(a|s)∇uQπ,γ(s, a) ]\nThe final term in this equation, ∇uQπ,γ(s, a), is difficult to estimate in an incremental off-policy setting. The first approximation involved in the theory of OffPAC is to omit this term. That is, we work with an approximation to the gradient, which we denote g(u) ∈ RNu , defined by\n∇uJγ(u) ≈ g(u) = ∑ s∈S db(s) ∑ a∈A ∇uπ(a|s)Qπ,γ(s, a)\n(5) The two theorems below provide justification for this approximation.\nTheorem 1 (Policy Improvement). Given any policy parameter u, let\nu′ = u + αg(u)\nThen there exists an > 0 such that, for all positive α < ,\nJγ(u ′) ≥ Jγ(u)\nFurther, if π has a tabular representation (i.e., separate weights for each state), then V πu′ ,γ(s) ≥ V πu,γ(s) for all s ∈ S.\n(Proof in Appendix).\nIn the conventional on-policy theory of policy-gradient methods, the policy-gradient theorem (Marbach & Tsitsiklis, 1998; Sutton et al., 2000) establishes the relationship between the gradient of the objective function and the expected action values. In our notation, that theorem essentially says that our approximation is exact, that g(u) = ∇uJγ(u). Although, we can not show this in the off-policy case, we can establish a relationship between the solutions found using the true and approximate gradient:\nTheorem 2 (Off-Policy Policy-Gradient Theorem). Given U ⊂ RNu a non-empty, compact set, let\nZ̃ = {u ∈ U | g(u) = 0} Z = {u ∈ U | ∇uJγ(u) = 0}\nwhere Z is the true set of local maxima and Z̃ the set of local maxima obtained from using the approximate gradient, g(u). If the value function can be represented by our function class, then Z ⊂ Z̃. Moreover, if we use a tabular representation for π, then Z = Z̃.\n(Proof in Appendix).\nThe proof of Theorem 2, showing that Z = Z̃, requires tabular π to avoid update overlap: updates to a single parameter influence the action probabilities for only one state. Consequently, both parts of the gradient (one part with the gradient of the policy function and the other with the gradient of the action-value function) locally greedily change the action probabilities for only that one state. Extrapolating from this result, in practice, more generally a local representation for π will likely suffice, where parameter updates influence only a small number of states. Similarly, in the non-tabular case, the claim will likely hold if γ is small (the return is myopic), again because changes to the policy mostly affect the action-value function locally.\nFortunately, from an optimization perspective, for all u ∈ Z̃\\Z, Jγ(u) < minu′∈Z Jγ(u′), in other words,\nZ represents all the largest local maxima in Z̃ with respect to the objective, Jγ . Local optimization techniques, like random restarts, should help ensure that we converge to larger maxima and so to u ∈ Z. Even with the true gradient, these approaches would be incorporated into learning because our objective, Jγ , is non-convex."
    }, {
      "heading" : "2.3. The Actor: Incremental Update Algorithm with Eligibility Traces",
      "text" : "We now derive an incremental update algorithm using observations sampled from the behavior policy. First, we rewrite Equation 5 as an expectation:\ng(u) = E [∑ a∈A ∇uπ(a|s)Qπ,γ(s, a) ∣∣∣∣∣s ∼ db ]\n= E [∑ a∈A b(a|s)π(a|s) b(a|s) ∇uπ(a|s) π(a|s) Qπ,γ(s, a) ∣∣∣∣∣s ∼ db ]\n= E [ ρ(s, a)ψ(s, a)Qπ,γ(s, a) ∣∣s ∼ db, a ∼ b(·|s)] = Eb [ρ(s, a)ψ(s, a)Q π,γ(s, a)]\nwhere ρ(s, a) = π(a|s)b(a|s) , ψ(s, a) = ∇uπ(a|s) π(a|s) , and we introduce the new notation Eb [·] to denote the expectation conditional on the states and actions being generated by the behavior policy, as indicated above. A standard result (e.g., see Sutton et al., 2000) is that an arbitrary function of state can be introduced into the above equation as a baseline without changing the expected value. We use the approximate state-value function provided by the critic, V̂ (s), in this way:\ng(u) = Eb\n[ ρ(s, a)ψ(s, a) ( Qπ,γ(s, a)− V̂ (s) )] The next step is to replace the action value, Qπ,γ(s, a), by the off-policy λ-return. Consider a sequence of states, actions, and rewards {st, at, rt+1}∞t=0 generated by following the behavior policy. Then g(u) can be approximated by:\ng(u) ≈ ĝ(u) = Eb [ ρ(st, at)ψ(st, at) ( Rλt − V̂ (s) )] where the off-policy λ-return is defined by:\nRλt = rt+1 + (1− λ)γ(st+1)V̂ (st+1) + λγ(st+1)ρ(st+1, at+1)R λ t+1\nThe quantity ĝ(u) is an approximation because the λreturn involves the approximate state-value function, V̂ . Finally, based on this equation, we can write the forward view of Off-PAC:\nut+1 − ut = αu,tρ(st, at)ψ(st, at) ( Rλt − V̂ (s) )\nAlgorithm 1 The Off-PAC algorithm\nInitialize the vectors ev, eu, and w to zero Initialize the vectors v and u arbitrarily Initialize the state s For each step:\nChoose an action, a, according to b(·|s) Observe resultant reward, r, and next state, s′ δ ← r + γ(s′)vTxs′ − vTxs ρ← πu(a|s)/b(a|s) Update the critic (GTD(λ) algorithm):\nev ← ρ (xs + γ(s)λev) v← v + αv [ δev − γ(s′)(1− λ)(wTev)xs ] w← w + αw [ δev − (wTxs)xs\n] Update the actor:\neu ← ρ [ ∇uπu(a|s) πu(a|s) + γ(s)λeu ] u← u + αuδeu\ns← s′\nThe forward view is useful for understanding and analyzing algorithms, but for a mechanistic implementation it must be converted to a backward view that does not involve the λ-return. The key step, proved in the appendix, is the observation that\nρ(st, at)ψ(st, at)E [ Rλt − V̂ (st) ∣∣∣st, at] = E [δtet|st, at] (6)\nwhere δt = rt+1 + γ(st+1)V̂ (st+1) − V̂ (st) is the conventional temporal difference error, and et ∈ RNu is the eligibility trace of ψ, updated by:\net = ρ(st, at) (ψ(st, at) + λet−1)\nFinally, combining the three previous equations, the backward view of the actor update can be written simply as:\nut+1 − ut = αu,tδtet\nThe complete Off-PAC algorithm is given above as Algorithm 1. Note that although the algorithm is written in terms of states s and s′, it really only ever needs access to the corresponding feature vectors, xs and xs′ . Also note that Off-PAC is fully incremental and has per-time step computation and memory complexity that is linear in the number of weights, Nu +Nv.\nWith discrete actions, a common policy distribution is the Gibbs distribution, which uses a linear combination of features π(a|s) = e uTφs,a∑ b e uTφs,b where φs,a are state-action features for state s, action a, and where\nψ(s, a) = ∇uπ(a|s)π(a|s) = φs,a − ∑ b π(b|s)φs,b. The stateaction features, φs,a, are potentially unrelated to the feature vectors xs used in the critic."
    }, {
      "heading" : "3. Convergence Analysis",
      "text" : "Our algorithm has the same recursive stochastic form as the off-policy value-function algorithms\nut+1 = ut + αt(h(ut,vt) +Mt+1)\nwhere h : RN → RN is a differentiable function and {Mt}t≥0 is a noise sequence. Following previous offpolicy gradient proofs (Maei, 2011), we study the behavior of the ordinary differential equation\nu̇(t) = u(h(u(t),v))\nThe two updates (for the actor and for the critic) are not independent on each time step; we analyze two separate ODEs using a two timescale analysis (Borkar, 2008). The actor update is analyzed given fixed critic parameters, and vice versa, iteratively (until convergence). We make the following assumptions.\n(A1) The policy viewed as a function of u, π(·)(a|s) : RNu → (0, 1], is continuously differentiable, ∀s ∈ S, a ∈ A.\n(A2) The update on ut includes a projection operator, Γ : RNu → RNu , that projects any u to a compact set U = {u | qi(u) ≤ 0, i = 1, . . . , s} ⊂ RNu , where qi(·) : RNu → R are continuously differentiable functions specifying the constraints of the compact region. For u on the boundary of U , the gradients of the active qi are linearly independent. Assume the compact region is large enough to contain at least one (local) maximum of Jγ .\n(A3) The behavior policy has a minimum positive value bmin ∈ (0, 1]: b(a|s) ≥ bmin ∀s ∈ S, a ∈ A\n(A4) The sequence (xt,xt+1, rt+1)t≥0 is i.i.d. and has uniformly bounded second moments.\n(A5) For every u ∈ U (the compact region to which u is projected), V π,γ : S → R is bounded.\nRemark 1: It is difficult to prove the boundedness of the iterates without the projection operator. Since we have a bounded function (with range (0, 1]), we could instead assume that the gradient goes to zero exponentially as u → ∞, ensuring boundedness. Previous work, however, has illustrated that the stochasticity in practice makes convergence to an unstable equilibrium unlikely (Pemantle, 1990); therefore, we avoid restrictions on the policy function and do not include the projection in our algorithm\nFinally, we have the following (standard) assumptions on features and step-sizes.\n(P1) ||xt||∞ <∞, ∀t, where xt ∈ RNv\n(P2) Matrices C = E[xtxt T], A = E[xt(xt − γxt+1)T]\nare non-singular and uniformly bounded. A, C and E[rt+1xt] are well-defined because the distribution of (xt,xt+1, rt+1) does not depend on t.\n(S1) αv,t, αw,t, αu,t > 0, ∀t are deterministic such that∑ t αv,t = ∑ t αw,t = ∑ t αu,t =∞ and ∑ t α 2 v,t <\n∞, ∑ t α 2 w,t <∞ and ∑ t α 2 u,t <∞ with αu,t αv,t → 0.\n(S2) Define H(A) . = (A + AT)/2 and let\nλmin(C −1H(A)) be the minimum eigenvalue of the matrix C−1H(A)1. Then αw,t = ηαv,t for some η > max(0,−λmin(C−1H(A))).\nRemark 2: The assumption αu,t/αv,t → 0 in (S1) states that the actor step-sizes go to zero at a faster rate than the value function step-sizes: the actor update moves on a slower timescale than the critic update (which changes more from its larger step sizes). This timescale is desirable because we effectively want a converged value function estimate for the current policy weights, ut. Examples of suitable step sizes are αv,t = 1 t , αu,t = 1 1+t log t or αv,t = 1 t2/3 , αu,t = 1 t . (with αw,t = ηαv,t for η satisfying (S2)).\nThe above assumptions are actually quite unrestrictive. Most algorithms inherently assume bounded features with bounded value functions for all policies; unbounded values trivially result in unbounded value function weights. Common policy distributions are smooth, making π(a|s) continuously differentiable in u. The least practical assumption is that the tuples (xt,xt+1, rt+1) are i.i.d., in other words, Martingale noise instead of Markov noise. For Markov noise, our proof as well as the proofs for GTD(λ) and GQ(λ), require Borkar’s (2008) two-timescale theory to be extended to Markov noise (which is outside the scope of this paper). Finally, the proof for Theorem 3 assumes λ = 0, but should extend to λ > 0 similarly to GTD(λ) (see Maei, 2011, Section 7.4, for convergence remarks).\nWe give a proof sketch of the following convergence theorem, with the full proof in the appendix.\nTheorem 3 (Convergence of Off-PAC). Let λ = 0 and consider the Off-PAC iterations with GTD(0)2 for the critic. Assume that (A1)-(A5), (P1)-(P2) and (S1)(S2) hold. Then the policy weights, ut, converge to Ẑ = {u ∈ U | ĝ(u) = 0} and the value function weights, vt, converge to the corresponding TD-solution with probability one.\nProof Sketch: We follow a similar outline to the two timescale analysis for on-policy policy gradient\n1Minimum exists as all eigenvalues real-valued (Lemma 4) 2GTD(0) is GTD(λ) with λ = 0, not the different algorithm called GTD(0) by Sutton, Szepesvari & Maei (2008)\nactor-critic (Bhatnagar et al., 2009) and for nonlinear GTD (Maei et al., 2009). We analyze the dynamics for our two weights, ut and zt T = (wt Tvt\nT), based on our update rules. The proof involves satisfying seven requirements from Borkar (2008, p. 64) to ensure convergence to an asymptotically stable equilibrium."
    }, {
      "heading" : "4. Empirical Results",
      "text" : "This section compares the performance of Off-PAC to three other off-policy algorithms with linear memory and computational complexity: 1) Q(λ) (called QLearning when λ = 0), 2) Greedy-GQ (GQ(λ) with a greedy target policy), and 3) Softmax-GQ (GQ(λ) with a Softmax target policy). The policy in Off-PAC is a Gibbs distribution as defined in section 2.3.\nWe used three benchmarks: mountain car, a pendulum problem and a continuous grid world. These problems all have a discrete action space and a continuous state space, for which we use function approximation. The behavior policy is a uniform distribution over all the possible actions in the problem for each time step. Note that Q(λ) may not be stable in this setting (Baird, 1995), unlike all the other algorithms.\nThe goal of the mountain car problem (see Sutton & Barto, 1998) is to drive an underpowered car to the top of a hill. The state of the system is composed of the current position of the car (in [−1.2, 0.6]) and its velocity (in [−.07, .07]). The car was initialized with a position of -0.5 and a velocity of 0. Actions are a throttle of {−1, 0, 1}. The reward at each time step is −1. An episode ends when the car reaches the top of the hill on the right or after 5,000 time steps.\nThe second problem is a pendulum problem (Doya, 2000). The state of the system consists of the angle (in radians) and the angular velocity (in [−78.54, 78.54]) of the pendulum. Actions, the torque applied to the base, are {−2, 0, 2}. The reward is the cosine of the angle of the pendulum with respect to its fixed base. The pendulum is initialized with an angle and an angular velocity of 0 (i.e., stopped in a horizontal position). An episode ends after 5,000 time steps.\nFor the pendulum problem, it is unlikely that the behavior policy will explore the optimal region where the pendulum is maintained in a vertical position. Consequently, this experiment illustrates which algorithms make best use of limited behavior samples.\nThe last problem is a continuous grid-world. The state is a 2-dimensional position in [0, 1]2. The actions are the pairs {(0.0, 0.0), (−.05, 0.0), (.05, 0.0), (0.0,−.05), (0.0, .05)}, representing moves in both dimensions. Uniform noise in [−.025, .025] is added\nto each action component. The reward at each time step for arriving in a position (px, py) is defined as: −1 + −2(N (px, .3, .1) · N (py, .6, .03) + N (px, .4, .03)·N (py, .5, .1)+N (px, .8, .03)·N (py, .9, .1)) where N (p, µ, σ) = e− (p−µ)2 2σ2 /σ √\n2π. The start position is (0.2, 0.4) and the goal position is (1.0, 1.0). An episode ends when the goal is reached, that is when the distance from the current position to the goal is less than 0.1 (using the L1-norm), or after 5,000 time steps. Figure 1 shows a representation of the problem.\nThe feature vectors xs were binary vectors constructed according to the standard tile-coding technique (Sutton & Barto, 1998). For all problems, we used ten tilings, each of roughly 10 × 10 over the joint space of the two state variables, then hashed to a vector of dimension 106. An addition feature was added that was always 1. State-action features, ψs,a, were also 106 + 1 dimensional vectors constructed by also hashing the actions. We used a constant γ = 0.99. All the weight vectors were initialized to 0. We performed a parameter sweep to select the following parameters: 1) the step size αv for Q(λ), 2) the step-sizes αv and αw for the two vectors in Greedy-GQ, 3) αv, αw and the temperature τ of the target policy distribution for Softmax-GQ and 4) the step sizes αv, αw and αu for Off-PAC. For the step sizes, the sweep was done over the following values: {10−4, 5 · 10−4, 10−3, . . . , .5, 1.} divided by 10+1=11, that is the number of tilings plus 1. To compare TD methods to gradient-TD methods, we also used αw = 0. The temperature parameter, τ , was chosen from {.01, .05, .1, .5, 1, 5, 10, 50, 100} and λ from {0, .2, .4, .6, .8, .99}. We ran thirty runs\nwith each setting of the parameters.\nFor each parameter combination, the learning algorithm updates a target policy online from the data generated by the behavior policy. For all the problems, the target policy was evaluated at 20 points in time during the run by running it 5 times on another instance of the problem. The target policy was not updated during evaluation, ensuring that it was learned only with data from the behavior policy.\nFigure 2 shows results on three problems. SoftmaxGQ and Off-PAC improved their policy compared to the behavior policy on all problems, while the improvements for Q(λ) and Greedy-GQ is limited on the continuous grid world. Off-PAC performed best on all problems. On the continuous grid world, Off-PAC was the only algorithm able to learn a policy that reliably found the goal after 5,000 episodes (see Figure 1). On all problems, Off-PAC had the lowest standard error."
    }, {
      "heading" : "5. Discussion",
      "text" : "Off-PAC, like other two-timescale update algorithms, can be sensitive to parameter choices, particularly the step-sizes. Off-PAC has four parameters: λ and the\nthree step sizes, αv and αw for the critic and αu for the actor. In practice, the following procedure can be used to set these parameters. The value of λ, as with other algorithms, will depend on the problem and it is often better to start with low values (less than .4). A common heuristic is to set αv to 0.1 divided by the norm of the feature vector, xs, while keeping the value of αw low. Once GTD(λ) is stable learning the value function with αu = 0, αu can be increased so that the policy of the actor can be improved. This corroborates the requirements in the proof, where the step-sizes should be chosen so that the slow update (the actor) is not changing as quickly as the fast inner update to the value function weights (the critic).\nAs mentioned by Borkar (2008, p. 75), another scheme that works well in practice is to use the restrictions on the step-sizes in the proof and to also subsample updates for the slow update. Subsampling updates means only updating every {tN, t ≥ 0}, for some N > 1: the actor is fixed in-between tN and (t+ 1)N while the critic is being updated. This further slows the actor update and enables an improved value function estimate for the current policy, π.\nIn this work, we did not explore incremental natural\nactor-critic methods (Bhatnagar et al., 2009), which use the natural gradient as opposed to the conventional gradient. The extension to off-policy natural actorcritic should be straightforward, involving only a small modification to the update and analysis of this new dynamical system (which will have similar properties to the original update).\nFinally, as pointed out by Precup et al. (2006), offpolicy updates can be more noisy compared to onpolicy learning. The results in this paper suggest that Off-PAC is more robust to such noise because it has lower variance than the action-value based methods. Consequently, we think Off-PAC is a promising direction for extending off-policy learning to a more general setting such as continuous action spaces."
    }, {
      "heading" : "6. Conclusion",
      "text" : "This paper proposed a new algorithm for learning control off-policy, called Off-PAC (Off-Policy ActorCritic). We proved that Off-PAC converges in a standard off-policy setting. We provided one of the first empirical evaluations of off-policy control with the new gradient-TD methods and showed that Off-PAC has the best final performance on three benchmark problems and consistently has the lowest standard error. Overall, Off-PAC is a significant step toward robust off-policy control."
    }, {
      "heading" : "7. Acknowledgments",
      "text" : "This work was supported by MPrime, the Alberta Innovates Centre for Machine Learning, the Glenrose Rehabilitation Hospital Foundation, Alberta Innovates— Technology Futures, NSERC and the ANR MACSi project. Computational time was provided by Westgrid and the Mésocentre de Calcul Intensif Aquitain."
    }, {
      "heading" : "Appendix: see http://arxiv.org",
      "text" : ""
    }, {
      "heading" : "Baird, L. (1995). Residual algorithms: Reinforcement",
      "text" : "learning with function approximation. In Proceedings of the Twelfth International Conference on Machine Learning, pp. 30–37. Morgan Kaufmann."
    }, {
      "heading" : "Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., Lee, M.",
      "text" : "(2009). Natural actor-critic algorithms. Automatica 45 (11):2471–2482. Borkar, V. S. (2008). Stochastic approximation: A dynamical systems viewpoint. Cambridge Univ Press."
    }, {
      "heading" : "Bradtke, S. J., Barto, A. G. (1996). Linear least-squares",
      "text" : "algorithms for temporal difference learning. Machine Learning 22 :33–57."
    }, {
      "heading" : "Delp, M. (2010). Experiments in off-policy reinforcement",
      "text" : "learning with the GQ(λ) algorithm. Masters thesis,\nUniversity of Alberta.\nDoya, K. (2000). Reinforcement learning in continuous time and space. Neural computation 12 :219–245. Lagoudakis, M., Parr, R. (2003). Least squares policy iteration. Journal of Machine Learning Research 4 :1107– 1149. Maei, H. R., Sutton, R. S. (2010). GQ(λ): A general gradient algorithm for temporal-difference prediction learning with eligibility traces. In Proceedings of the Third Conf. on Artificial General Intelligence. Maei, H. R. (2011). Gradient Temporal-Difference Learning Algorithms. PhD thesis, University of Alberta."
    }, {
      "heading" : "Maei, H. R., Szepesvári, C., Bhatnagar, S., Precup, D.,",
      "text" : "Silver, D., Sutton, R. S. (2009). Convergent temporaldifference learning with arbitrary smooth function approximation. Advances in Neural Information Processing Systems 22 :1204–1212."
    }, {
      "heading" : "Maei, H. R., Szepesvári, C., Bhatnagar, S., Sutton, R. S.",
      "text" : "(2010). Toward off-policy learning control with function approximation. Proceedings of the 27th International Conference on Machine Learning. Marbach, P., Tsitsiklis, J. N. (1998). Simulation-based optimization of Markov reward processes. Technical report LIDS-P-2411. Pemantle, R. (1990). Nonconvergence to unstable points in urn models and stochastic approximations. The Annals of Probability 18 (2):698–712. Peters, J., Schaal, S. (2008). Natural actor-critic. Neurocomputing 71 (7):1180–1190."
    }, {
      "heading" : "Precup, D., Sutton, R.S., Paduraru, C., Koop, A., Singh,",
      "text" : "S. (2006). Off-policy learning with recognizers. Neural Information Processing Systems 18. Smart, W.D., Pack Kaelbling, L. (2002). Effective reinforcement learning for mobile robots. In Proceedings of International Conference on Robotics and Automation, volume 4, pp. 3404–3410. Sutton, R. S., Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press."
    }, {
      "heading" : "Sutton, R. S., McAllester, D., Singh, S., Mansour, Y.",
      "text" : "(2000). Policy gradient methods for reinforcement learning with function approximation. Advances in Neural Information Processing Systems 12. Sutton, R. S., Szepesvári, Cs., Maei, H. R. (2008). A convergent O(n) algorithm for off-policy temporaldifference learning with linear function approximation. In Advances in Neural Information Processing Systems 21, pp. 1609–1616."
    }, {
      "heading" : "Sutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S.,",
      "text" : "Silver, D., Szepesvári, Cs., Wiewiora, E. (2009). Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 993–1000."
    }, {
      "heading" : "Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski,",
      "text" : "P. M., and Precup, D. (2011). Horde: A scalable realtime architecture for learning knowledge from unsupervised sensorimotor interaction. In Proceedings of the 10th International Conference on Autonomous Agents and Multiagent Systems. Watkins, C. J. C. H., Dayan, P. (1992). Q-learning. Machine Learning 8 (3):279–292."
    }, {
      "heading" : "A. Appendix of Off-Policy Actor-Critic",
      "text" : ""
    }, {
      "heading" : "A.1. Policy Improvement and Policy Gradient Theorems",
      "text" : "Theorem 1 [Off-Policy Policy Improvement Theorem] Given any policy parameter u, let\nu′ = u + αg(u)\nThen there exists an > 0 such that, for all positive α < ,\nJγ(u ′) ≥ Jγ(u)\nFurther, if π has a tabular representation (i.e., separate weights for each state), then V πu′ ,γ(s) ≥ V πu,γ(s) for all s ∈ S.\nProof. Notice first that for any (s, a), the gradient ∇uπ(a|s) is the direction to increase the probability of action a according to function π(·|s). For an appropriate step size αu,t (so that the update to πu′ increases the objective with the action-value function Qπu,γ , fixed as the old action-value function), we can guarantee that\nJγ(u) = ∑ s∈S db(s) ∑ a∈A πu(a|s)Qπu,γ(s, a)\n≤ ∑ s∈S db(s) ∑ a∈A πu′(a|s)Qπu,γ(s, a)\nNow we can proceed similarly to the Policy Improvement theorem proof provided by Sutton and Barto (1998) by extending the right-hand side using the definition of Qπ,γ(s, a) (equation 2):\nJγ(ut) ≤ ∑ s∈S db(s) ∑ a∈A πu′(a|s)E [rt+1 + γt+1V πu,γ(st+1)|πu′ , γ]\n≤ ∑ s∈S db(s) ∑ a∈A πu′(a|s)E [rt+1 + γt+1rt+2 + γt+2V πu,γ(st+2)|πu′ , γ]\n...\n≤ ∑ s∈S db(s) ∑ a∈A πu′(a|s)Qπu′ ,γ(s, a) = Jγ(u ′)\nThe second part of the Theorem has similar proof to the above. With a tabular representation for π, we know that the gradient satisfies: ∑\na∈A πu(a|s)Qπu,γ(s, a) ≤ ∑ a∈A πu′(a|s)Qπu,γ(s, a)\nbecause the probabilities can be updated independently for each state with separate weights for each state.\nNow for any s ∈ S: V πu,γ(s) = ∑ a∈A πu(a|s)Qπu,γ(s, a)\n≤ ∑ a∈A πu′(a|s)Qπu,γ(s, a)\n≤ ∑ a∈A πu′(a|s)E [rt+1 + γt+1V πu,γ(st+1)|πu′ , γ]\n≤ ∑ a∈A πu′(a|s)E [rt+1 + γt+1rt+2 + γt+2V πu,γ(st+2)|πu′ , γ]\n...\n≤ ∑ a∈A πu′(a|s)Qπu′ ,γ(s, a) = V πu′ ,γ(s)\nTheorem 2 [Off-Policy Policy Gradient Theorem] Let Z = {u ∈ U | ∇uJγ(u) = 0} and Z̃ = {u ∈ U | g(u) = 0}, which are both non-empty by Assumption (A2). If the value function can be represented by our function class, then\nZ ⊂ Z̃\nMoreover, if we use a tabular representation for π, then\nZ = Z̃\nProof. This theorem follows from our policy improvement theorem.\nAssume there exists u∗ ∈ Z such that u∗ /∈ Z̃. Then ∇u∗Jγ(u) = 0 but g(u∗) 6= 0. By the policy improvement theorem (Theorem 1), we know that Jγ(u ∗ + αu,tg(u ∗)) > Jγ(u), for some positive αu,t. However, this is a contradiction, as the true gradient is zero. Therefore, such an u∗ cannot exist.\nFor the second part of the theorem, we have a tabular representation, in other words, each weight corresponds to exactly one state. Without loss of generality, assume each state s is represented with m ∈ N weights, indexed by let is,1 . . . is,m in the vector u. Therefore, for any state, s∑\ns′∈S db(s′) ∑ a∈A ∂ ∂uis,j πu(a|s′)Qπu,γ(s′, a) = db(s) ∑ a∈A ∂ ∂uis,j πu(a|s)Qπu,γ(s, a) . = g1(uis,j )\nAssume there exists s ∈ S such that g1(uis,j ) = 0 ∀j but there exists 1 ≤ k ≤ m for g2(uis,k) . =∑\ns′∈S d b(s′) ∑ a∈A πu(a|s′)\n∂ ∂uis,k Qπu,γ(s′, a) such that g2(uis,k) 6= 0. ∂∂uisQ πu,γ(s′, a) can only increase the\nvalue of Qπu,γ(s, a) locally (i.e., shift the probabilities of the actions to increase return), because it cannot change the value in other states (uis is only used for state s and the remaining weights are fixed when this partial derivative is computed). Therefore, since g2(uis,k) 6= 0, we must be able to increase the value of state s by changing the probabilities of the actions in state s\n=⇒ m∑ j=1 ∑ a∈A ∂ ∂uis,j πu(a|s)Qπu,γ(s, a) 6= 0\nwhich is a contradiction (since we assumed g1(uis,j ) = 0 ∀j). Therefore, in the tabular case, whenever ∑ s d b(s) ∑ a∇uπu(a|s)Qπu,γ(s, a) = 0, then∑\ns d b(s) ∑ a πu(a|s)∇uQπu,γ(s, a) = 0, implying that Z̃ ⊂ Z. Since we already know that Z ⊂ Z̃, then\nwe can conclude that for a tabular representation for π, Z = Z̃.\nA.2. Forward/Backward view analysis\nIn this section, we prove Equation 6, the key relationship between the forward and backward views: ρ(st, at)ψ(st, at)E [ Rλt − V̂ (st) ∣∣∣st, at] = E [δtet|st, at] where in these expectations, and in all the expectations in this section, the indicated state–action pair, (st, at), is considered given (non-stochastic) and the future states and actions are presumed given stochastically by the environment and the behavior policy respectively. The expectation is over these future random variables. We also assume that the behavior policy is stationary and that the Markov chain is aperiodic and irreducible (i.e., we have reached the limiting distribution, db over s ∈ S).\nTo simplify the notation for this section, we define ρt = ρ(st, at), ψt = ψ(st, at), γt = γ(st), and δ λ t = R λ t − V̂ (st). With this notation, the expression to be proved can be written:\nρtψtE [ δλt ∣∣st, at] = E [δtet|st, at]\nProof. First we note that δλt , which might be called the forward-view TD error, can be written recursively:\nδλt = R λ t − V̂ (st)\n= rt+1 + (1− λ)γt+1V̂ (st+1) + λγt+1ρt+1Rλt+1 − V̂ (st) = rt+1 + γt+1V̂ (st+1)− λγt+1V̂ (st+1) + λγt+1ρt+1Rλt+1 − V̂ (st)\n= rt+1 + γt+1V̂ (st+1)− V̂ (st) + λγt+1 ( ρt+1R λ t+1 − V̂ (st+1) ) = δt + λγt+1 ( ρt+1R λ t+1 − ρt+1V̂ (st+1)− (1− ρt+1)V̂ (st+1)\n) = δt + λγt+1 ( ρt+1δ λ t+1 − (1− ρt+1)V̂ (st+1) ) (7)\nwhere δt = rt+1 + γt+1V̂ (st+1)− V̂ (st) is the conventional one-step TD error.\nWith this groundwork, we are ready to prove the main result. To simplify notation, in the expectations below the conditioning on st, at is suppressed:\nρtψtE [ δλt ]\n= E [ ρtψt ( δt + λγt+1 ( ρt+1δ λ t+1 − (1− ρt+1)V̂ (st+1) ))] (using (7))\n= E [ρtψtδt] + E [ ρtψtλγt+1ρt+1δ λ t+1 ] − ρtψtλE [ γt+1(1− ρt+1)V̂ (st+1) ] = E [ρtψtδt] + E [ ρtψtλγt+1ρt+1δ λ t+1 ] − ρtψtλ\n∑ s′ P (s′|st, at)γ(s′)\n( 1−\n∑ a′ b(a′|s′)ρ(s′, a′)\n) V̂ (s′)\n= E [ρtψtδt] + E [ ρtψtλγt+1ρt+1δ λ t+1 ] − ρtψtλ ∑ s′ P (s′|st, at)γ(s′)\n( 1−\n∑ a′ b(a′|s′)π(a ′|s′) b(a′|s′)\n) V̂ (s′)\n= E [ρtψtδt] + E [ ρtψtλγt+1ρt+1δ λ t+1 ] = E [ρtψtδt] + E [ ρt−1ψt−1λγtρtδ λ t ] (shifting by 1, because all time steps are the same)\n= E [ρtψtδt] + E [ ρt−1ψt−1λγtρt ( δt + λγt+1 ( ρt+1δ λ t+1 − (1− ρt+1)V̂ (st+1) ))] = E [ρtψtδt] + E [ρt−1ψt−1λγtρtδt] + E [ ρt−1ψt−1λγtρtλγt+1ρt+1δ λ t+1\n] = E [ρtδt (ψt + λγtρt−1ψt−1)] + E [ λ2ρt−2ψt−2γt−1ρt−1γtρtδ λ t\n] = E [ρtδt (ψt + λγtρt−1ψt−1)] + E [ λ2ρt−2ψt−2γt−1ρt−1γtρt ( δt + λγt+1 ( ρt+1δ λ t+1 − (1− ρt+1)V̂ (st+1)\n))] = E [ρtδt (ψt + λγtρt−1ψt−1)] + E [ λ2ρt−2ψt−2γt−1ρt−1γtρtδt ] + E [ λ2ρt−2ψt−2γt−1ρt−1γtρtλγt+1ρt+1δ λ t+1\n] = E [ρtδt (ψt + λγtρt−1 (ψt−1 + λγt−1ρt−2ψt−2))] + E [ λ3ρt−3ψt−3γt−2ρt−2γt−1ρt−1γtρtδ λ t\n] ...\n= E [ρtδt (ψt + λγtρt−1 (ψt−1 + λγt−1ρt−2 (ψt−2 + λγt−2ρt−3 . . .)))]\n= E [δtet]\nwhere et = ρt (ψt + λγtet−1)."
    }, {
      "heading" : "A.3. Convergence Proofs",
      "text" : "Our algorithm has the same recursive stochastic form that the two-timescale off-policy value-function algorithms have:\nut+1 = ut + αt(h(ut, zt) +Mt+1)\nzt+1 = zt + αt(f(ut, zt) +Nt+1)\nwhere x ∈ Rd, h : Rd → Rd is a differentiable functions, {αt}k≥0 is a positive step-size sequence and {Mt}k≥0 is a noise sequence. Again, following the GTD(λ) and GQ(λ) proofs, we study the behavior of the ordinary differential equation\nu̇(t) = h(u(t), z)\nSince we have two updates, one for the actor and one for the critic, and those time updates are not linearly separable, we have to do a two timescale analysis (Borkar, 2008). In order to satisfy the conditions for the two-timescale analysis, we will need the following assumptions on our objective, the features and the step-sizes. Note that it is difficult to prove the boundedness of the iterates without the projection operator we describe below, though the projection was not necessary during experiments.\n(A1) The policy function, π(·)(a|s) : RNu → [0, 1], is continuously differentiable in u, ∀s ∈ S, a ∈ A.\n(A2) The update on ut includes a projection operator, Γ : RNu → RNu that projects any u to a compact set U = {u | qi(u) ≤ 0, i = 1, . . . , s} ⊂ RNu , where qi(·) : RNu → R are continuously differentiable functions specifying the constraints of the compact region. For each u on the boundary of U , the gradients of the active qi are considered to be linearly independent. Assume that the compact region, U , is large enough to contain at least one local maximum of Jγ .\n(A3) The behavior policy has a minimum positive weight for all actions in every state, in other words, b(a|s) ≥ bmin ∀s ∈ S, a ∈ A, for some bmin ∈ (0, 1].\n(A4) The sequence (xt,xt+1, rt+1)t≥0 is i.i.d. and has uniformly bounded second moments.\n(A5) For every u ∈ U (the compact region to which u is projected), V πu,γ : S → R is bounded.\n(P1) ||xt||∞ <∞, ∀t, where xt ∈ RNv\n(P2) The matrices C = E[xtxt T] and A = E[xt(xt − γxt+1)T] are non-singular and uniformly bounded. A, C\nand E[rt+1xt] are well-defined because the distribution of (xt,xt+1, rt+1) does not depend on t.\n(S1) αv,t, αw,t, αu,t > 0, ∀t are deterministic such that ∑ t αv,t = ∑ t αw,t = ∑ t αu,t = ∞ and ∑ t α 2 v,t < ∞,∑\nt α 2 w,t <∞ and ∑ t α 2 u,t <∞ with αu,t αv,t → 0.\n(S2) Define H(A) . = (A + AT)/2 and let χmin(C −1H(A)) be the minimum eigenvalue of the matrix C−1H(A). Then αw,t = ηαv,t for some η > max 0,−χmin(C−1H(A)).\nTheorem 3 (Convergence of Off-PAC) Let λ = 0 and consider the Off-PAC iterations for the critic (GTD(λ), i.e., TDC with importance sampling correction) and the actor (for weights ut). Assume that (A1)-(A5), (P1)(P2) and (S1)-(S2) hold. Then the policy weights, ut, converge to Ẑ = {u ∈ U | ĝ(u) = 0} and the value function weights, vt, converge to the corresponding TD-solution with probability one.\nProof. We follow a similar outline to that of the two timescale analysis proof for TDC (Sutton et al., 2009). We will analyze the dynamics for our two weights, ut, and zt T = (wt Tvt\nT), based on our update rules. We will take ut as the slow timescale update and zt as the fast inner update.\nFirst, we need to rewrite our updates for v, w and u, amenable to a two timescale analysis:\nvt+1 = vt + αv,tρt[δtxt − γxtTwxt] wt+1 = wt + αv,tη[ρtδtxt − xtTwxt] zt+1 = zt + αv,tρt[Gut,t+1zt + qut,t+1] (8)\nut+1 = Γ ( ut + αu,tδt\n∇utπt(at|st) b(at|st)\n) (9)\nwhere ρt = ρ(st, at), δt = rt+1 + γ(st+1)V̂ (st+1)− V̂ (st), η = αw,t/αv,t, qut,t+1T = (ηρtrt+1xtT, ρtrt+1xtT), and\nGut,t+1 =\n( −ηxtxtT ηρt(ut)xt(γxt+1 − xt)T\n−γρt(ut)xt+1xtT ρt(ut)xt(γxt+1 − xt)T\n) .\nNote that Gu = E[Gu,t|u] and qu = E[qu,t|u] are well defined because we assumed that the process (xt,xt+1, rt+1)t≥0 is i.i.d., 0 < ρt ≤ b−1min, and we have fixed ut. Now we can define h and f :\nh(zt,ut) = Gutzt + qut\nf(zt,ut) = E [ δt ∇utπt(at|st) b(at|st) |zt,ut ]\nMt+1 = (Gut,t+1 −Gut) zt + qut,t+1 − qut Nt+1 = δt ∇utπt(at|st) b(at|st) − f(zt,ut)\nWe have to satisfy the following conditions from Borkar (2008, p. p64):\n(B1) h : RNu+2Nv → R2Nv and f : RNu+2Nv → RNu are Lipschitz.\n(B2) αv,t, αu,t ∀t are deterministic and ∑ t αv,t = ∑ t αu,t =∞, ∑ t α 2 v,t <∞, ∑ t α 2 u,t <∞, αu,t αv,t → 0 (i.e., the\nsystem in Equation 9 moves on a slower timescale than Equation 8).\n(B3) The sequences {Mt}k≥0 and {Nt}k≥0 are Martingale difference sequences w.r.t. the increasing σ-fields, Ft . = σ(zm,um,Mm, Nm, m ≤ n) (i.e., E[Mt+1|Ft] = 0)\n(B4) For some constant K > 0, E[||Mt+1||2|Ft] ≤ K(1+||xt||2+||yt||2) and E[||Nt+1||2|Ft] ≤ K(1+||xt||2+||yt||2) holds for any k ≥ 0.\n(B5) The ODE ż(t) = h(z(t),u) has a globally asymptotically stable equilibrium χ(u) where χ : RNu → RNv is a Lipschitz map.\n(B6) The ODE u̇(t) = f(χ(u(t)),u(t)) has a globally asymptotically stable equilibrium, u∗.\n(B7) supt(||zt||+ ||ut||) <∞, a.s.\nAn asymptotically stable equilibrium for a dynamical system is an attracting point for which small perturbations still cause convergence back to that point. If we can verify these conditions, then we can use Theorem 2 by Borkar (2008) that states that (zt,ut)→ (χ(u∗),u∗) a.s. Note that the previous actor-critic proofs transformed the update to the negative update, assuming they were minimizing costs, −R, rather than maximizing and so converging to a (local) minimum. This is unnecessary because we simply need to prove we have a stable equilibrium, whether a maximum or minimum; therefore, we keep the update as in the algorithm and assume a (local) maximum.\nFirst note that because we have a bounded function, π(:)(s, a) : U → (0, 1], we can more simply satisfy some of the properties from Borkar (2008). Mainly, we know our policy function is Lipschitz (because it is bounded and continuously differentiable), so we know the gradient is bounded, in other words, there exists B∇u ∈ R such that ||∇uπ(a|s)|| ≤ B∇u.\nFor requirement (B1), h is clearly Lipschitz because it is linear in z and ρt(u) is continuously differentiable and bounded (ρt(u) ≤ b−1min). f is Lipschitz because it is linear in v and ∇uπ(a|s) is bounded and continuously differentiable (making Jγ with a fixed Q̂ π,γ continuously differentiable with a bounded derivative).\nRequirement (B2) is satisfied by our assumptions.\nRequirement (B3) is satisfied by the construction of Mt and Nt.\nFor requirement (B4), we can first notice that Mt satisfies the requirement because rt+1,xt and xt+1 have uniformly bounded second moments (which is the justification used in the TDC proof (Sutton et al., 2009) and because 0 < ρt ≤ b−1min.\nE[||Mt+1||2|Ft] = E[|| (Gut,t −Gut) zt + (qut,t − qut)||2|Ft] ≤ E[|| (Gut,t −Gut) zt||2 + ||(qut,t − qut)||2|Ft] ≤ E[||c1zt||2 + c2|Ft] ≤ K(||zt||2 + 1) ≤ K(||zt||2 + ||ut||2 + 1)\nwhere the second inequality is by the Cauchy Schwartz inequality, (Gut,t−Gut)zt ≤ c1|zt| and ||qut,t−qut ||2 ≤ c2 (because rt+1,xt and xt+1 have uniformly bounded second moments), with c1, c2 ∈ R+. When then simply set K = max(c1, c2).\nFor Nt, since the iterates are bounded as we show below for requirement (B7) (giving supt ||ut|| < Bu and supt ||zt|| < Bz for some Bu, Bz ∈ R. ), we see that\nE[||Nt+1||2|Ft]\n≤ E [∣∣∣∣∣∣∣∣δt∇utπt(at|st)b(at|st) ∣∣∣∣∣∣∣∣2 + ∣∣∣∣∣∣∣∣E [δt∇utπt(at|st)b(at|st) |zt,ut ]∣∣∣∣∣∣∣∣2 |Ft ]\n≤ E [∣∣∣∣∣∣∣∣δt∇utπt(at|st)b(at|st) ∣∣∣∣∣∣∣∣2 + E [∣∣∣∣∣∣∣∣δt∇utπt(at|st)b(at|st) ∣∣∣∣∣∣∣∣2 |zt,ut ] |Ft ]\n≤ 2E [∣∣∣∣ δtb(at|st) ∣∣∣∣2 ||∇utπt(at|st)||2|Ft ]\n≤ 2 b2min\nE [ |δt|2B2∇u|Ft ] ≤ K(||vt||2 + 1) ≤ K(||zt||2 + ||ut||2 + 1)\nfor some K ∈ R because E[|δ|2|Ft] ≤ c1(1 + ||vt||) for some c1 ∈ R because rt+1,xt and xt+1 have uniformly bounded second moments and since ||∇uπ(a|s)|| ≤ B∇u ∀ s ∈ S, a ∈ A (as stated above because π(a|s) is Lipschitz continuous).\nFor requirement (B5), we know that every policy, π, has a corresponding bounded V π,γ (by assumption). We need to show that for each u, there is a globally asymptotically stable equilibrium of the system, h(z(t),u) (which has yet to be shown for weighted importance sampling TDC, i.e., GTD(λ = 0)). To do so, we use the Hartman-Grobman Theorem, that requires us to show that G has all negative eigenvalues. For readability, we show this in a separate lemma (Lemma 4 below). Using Lemma 4, we know that there exists a function χ : RNu → RNv such that χ(u) = (vuT wuT) T\n, where vu is the unique TD-solution value-function weights for policy π and wu is the corresponding expectation estimate. This function, χ, is continuously differentiable with bounded gradient (by Lemma 5 below) and is therefore a Lipschitz map.\nFor requirement (B6), we need to prove that our update f(χ(·), ·) has an asymptotically stable equilibrium. This requirement can be relaxed to a local rather than global asymptotically stable equilibrium, because we simply need convergence. Our objective function, Jγ , is not concave because our policy function, π(a|s) may not be concave in u. Instead, we need to prove that all (local) equilibria are asymptotically stable.\nWe define a vector field operator, Γ̂ : C(RNu)→ C(RNu) that projects any gradients leading outside the compact\nregion, U , back into U :\nΓ̂(g(y)) = lim h→0 Γ(y + hg(y))− y h\nBy our forward-backward view analysis and from the same arguments following from Lemma 3 by Bhatnagar et al. (2009), we know that the ODE u̇(t) = f(χ(u(t)),u(t)) is g(u). Given that we have satisfied requirements 1-5 and given our step-size conditions, using standard arguments (c.f. Lemma 6 in Bhatnagar et al., 2009), we can deduce that ut converges almost surely to the set of asymptotically stable fixed points, Z̃, of u̇ = Γ̂g(u).\nFor requirement (B7), we know that ut is bounded because it is always projected to U . Since u stays in U , we know that v stays bounded (by assumption, otherwise V π,γ would not be bounded) and correspondingly w(v) must stay bounded, by the same argument as by Sutton et al. (2009). Therefore, we have that supt ||ut|| < Bu and that supt ||zt|| < Bρ for some Bu, Bz ∈ R.\nLemma 4. Under assumptions (A1)-(A5), (P1)-(P2) and (S1)-(S2), for any fixed set of actor weights, u ∈ U , the GTD(λ = 0) update for the critic weights, vt, converge to the TD solution with probability one.\nProof. Recall that\nGu,t+1 =\n( −ηxtxtT ηρt(u)xt(γxt+1 − xt)T\n−γρt(u)xt+1xtT ρt(u)xt(γxt+1 − xt)T\n) .\nand Gu = E [Gu,t], meaning\nGu = ( −ηC −ηAρ(u) −Fρ(u)T −Aρ(u) ) .\nwhere Fρ(u) = γE [ ρt(u)xt+1xt T ] , with Cρ(u) = Aρ(u)− Fρ(u). For the remainder of the proof, we will simply write Aρ and Cρ, because it is clear that we have a fixed u ∈ U .\nBecause GTD(λ) is solely for value function approximation, the feature vector, x, is only dependent on the state: E [ ρtxtxt T ] = ∑ st,at d(st)b(at|st)ρtx(st)xtT\n= ∑ st,at d(st)π(at|st)x(st)xtT\n= ∑ st d(st)x(st)xt T (∑ at π(at|st) ) = ∑ st d(st)x(st)xt T = E[xtxt T]\nbecause ∑ at π(at|st) = 1. A similar argument shows that E [ ρtxt+1xt T ] = E [ xt+1xt T ] . Therefore, we get that Fρ(u) = γE[xxt T] and Aρ(u) = E[xt(γxt+1 − xt)T]. The expected value of the update, G, therefore, is that same as for TDC, which has been shown to converge under our assumptions (see Maei, 2011).\nLemma 5. Under assumptions (A1)-(A5), (P1)-(P2) and (S1)-(S2), let χ : U → V be the map from policy weights to corresponding value function, V π,γ , obtained from using GTD(λ = 0) (proven to exist by Lemma 4). Then χ is continuously differentiable with a bounded gradient for all u ∈ U .\nProof. To show that χ is continuous, we use the Weierstrass definition (δ − definition). Because χ(u) = −G(u)−1q(u) = zu, which is a complicated function of u, we can luckily break it up and prove continuity about parts of it. Recall that 1) the inverse of a continuous function is continuous at every point that represents a non-singular matrix and 2) the multiplication of two continuous functions is continuous. Since G(u) is always nonsingular, we simply need to proof that a(u) → G(u) and b(u) → q(u) are continuous. G(u) is composed of\nseveral block matrices, including C, Fρ(u) and Aρ(u). We will start by showing that u → Fρ(u) is continuous, where Fρ(u) = −E [ ηρt(u)xt+1xt T ∣∣b]. The remaining entries are similar.\nTake any s ∈ S, a ∈ A, and u ∈ U . We know that π(a|s) : U → [0, 1] is continuous for all u ∈ U (by assumption). Let 1 =\nγ|A|E[xt+1xtT|b] (well-defined because E\n[ xt+1xt T ∣∣b] is nonsingular). Then we know there exists a δ > 0\nsuch that for any u2 ∈ U with ||u1 − u2|| < δ, then ||πu1(at|st)− πu2(at|st)|| < 1. Now ||Fρ(u1)− Fρ(u2)|| = γ||E [ ρt(u1)xt+1xt T ] − E [ ρt(u2)xt+1xt T ] ||\n= γ ∣∣∣∣∣ ∣∣∣∣∣∑ st,at db(st)b(at|st) πu1(at|st) b(at|st) xt+1xt T − ∑ st,at db(st)b(at|st) πu2(at|st) b(at|st) xt+1xt T ∣∣∣∣∣ ∣∣∣∣∣\n= γ ∣∣∣∣∣ ∣∣∣∣∣∑ st,at db(st)[πu1(at|st)− πu2(at|st)]xt+1xtT ∣∣∣∣∣ ∣∣∣∣∣\n< γ ∑ st,at db(st)||πu1(at|st)− πu2(at|st)||xt+1xtT\n< γ 1 ∑ st,at db(st)xt+1xt T\n= γ 1|A|E [ xt+1xt T ∣∣b] =\nTherefore, u→ Fρ(u) is continuous. This same process can be done for Aρ(u) and E [ρt(u)rtxt|b] in q(u).\nSince u→ G and u→ q are continuous for all u, we know that χ(u) = −G(u)−1q(u) is continuous.\nThe above can also be accomplished to show that ∇uχ is continuous, simply by replacing π with ∇uπ above. Finally, because our policy function is Lipschitz (because it is bounded and continuously differentiable), we know that is has a bounded gradient. As a result, the gradient of χ is bounded (since we have nonsingular and bounded expectation matrices), which would again follow from a similar analysis as above."
    } ],
    "references" : [ {
      "title" : "Residual algorithms: Reinforcement learning with function approximation",
      "author" : [ "L. Baird" ],
      "venue" : "Proceedings of the Twelfth International Conference on Machine Learning, pp. 30–37. Morgan Kaufmann.",
      "citeRegEx" : "Baird,? 1995",
      "shortCiteRegEx" : "Baird",
      "year" : 1995
    }, {
      "title" : "Natural actor-critic algorithms",
      "author" : [ "S. Bhatnagar", "R.S. Sutton", "M. Ghavamzadeh", "M. Lee" ],
      "venue" : "Automatica 45 (11):2471–2482.",
      "citeRegEx" : "Bhatnagar et al\\.,? 2009",
      "shortCiteRegEx" : "Bhatnagar et al\\.",
      "year" : 2009
    }, {
      "title" : "Stochastic approximation: A dynamical systems viewpoint",
      "author" : [ "V.S. Borkar" ],
      "venue" : "Cambridge Univ Press.",
      "citeRegEx" : "Borkar,? 2008",
      "shortCiteRegEx" : "Borkar",
      "year" : 2008
    }, {
      "title" : "Linear least-squares algorithms for temporal difference learning",
      "author" : [ "S.J. Bradtke", "A.G. Barto" ],
      "venue" : "Machine Learning 22 :33–57.",
      "citeRegEx" : "Bradtke and Barto,? 1996",
      "shortCiteRegEx" : "Bradtke and Barto",
      "year" : 1996
    }, {
      "title" : "Experiments in off-policy reinforcement learning with the GQ(λ) algorithm",
      "author" : [ "M. Delp" ],
      "venue" : "Masters thesis,",
      "citeRegEx" : "Delp,? 2010",
      "shortCiteRegEx" : "Delp",
      "year" : 2010
    }, {
      "title" : "Reinforcement learning in continuous time and space",
      "author" : [ "K. Doya" ],
      "venue" : "Neural computation 12 :219–245.",
      "citeRegEx" : "Doya,? 2000",
      "shortCiteRegEx" : "Doya",
      "year" : 2000
    }, {
      "title" : "Least squares policy iteration",
      "author" : [ "M. Lagoudakis", "R. Parr" ],
      "venue" : "Journal of Machine Learning Research 4 :1107– 1149.",
      "citeRegEx" : "Lagoudakis and Parr,? 2003",
      "shortCiteRegEx" : "Lagoudakis and Parr",
      "year" : 2003
    }, {
      "title" : "GQ(λ): A general gradient algorithm for temporal-difference prediction learning with eligibility traces",
      "author" : [ "H.R. Maei", "R.S. Sutton" ],
      "venue" : "Proceedings of the Third Conf. on Artificial General Intelligence.",
      "citeRegEx" : "Maei and Sutton,? 2010",
      "shortCiteRegEx" : "Maei and Sutton",
      "year" : 2010
    }, {
      "title" : "Gradient Temporal-Difference Learning Algorithms",
      "author" : [ "H.R. Maei" ],
      "venue" : "PhD thesis, University of Alberta.",
      "citeRegEx" : "Maei,? 2011",
      "shortCiteRegEx" : "Maei",
      "year" : 2011
    }, {
      "title" : "Convergent temporaldifference learning with arbitrary smooth function approximation",
      "author" : [ "H.R. Maei", "C. Szepesvári", "S. Bhatnagar", "D. Precup", "D. Silver", "R.S. Sutton" ],
      "venue" : "Advances in Neural Information Processing Systems 22 :1204–1212.",
      "citeRegEx" : "Maei et al\\.,? 2009",
      "shortCiteRegEx" : "Maei et al\\.",
      "year" : 2009
    }, {
      "title" : "Toward off-policy learning control with function approximation",
      "author" : [ "H.R. Maei", "C. Szepesvári", "S. Bhatnagar", "R.S. Sutton" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning.",
      "citeRegEx" : "Maei et al\\.,? 2010",
      "shortCiteRegEx" : "Maei et al\\.",
      "year" : 2010
    }, {
      "title" : "Simulation-based optimization of Markov reward processes",
      "author" : [ "P. Marbach", "J.N. Tsitsiklis" ],
      "venue" : "Technical report LIDS-P-2411.",
      "citeRegEx" : "Marbach and Tsitsiklis,? 1998",
      "shortCiteRegEx" : "Marbach and Tsitsiklis",
      "year" : 1998
    }, {
      "title" : "Nonconvergence to unstable points in urn models and stochastic approximations",
      "author" : [ "R. Pemantle" ],
      "venue" : "The Annals of Probability 18 (2):698–712.",
      "citeRegEx" : "Pemantle,? 1990",
      "shortCiteRegEx" : "Pemantle",
      "year" : 1990
    }, {
      "title" : "Natural actor-critic",
      "author" : [ "J. Peters", "S. Schaal" ],
      "venue" : "Neurocomputing 71 (7):1180–1190.",
      "citeRegEx" : "Peters and Schaal,? 2008",
      "shortCiteRegEx" : "Peters and Schaal",
      "year" : 2008
    }, {
      "title" : "Off-policy learning with recognizers",
      "author" : [ "D. Precup", "R.S. Sutton", "C. Paduraru", "A. Koop", "S. Singh" ],
      "venue" : "Neural Information Processing Systems 18.",
      "citeRegEx" : "Precup et al\\.,? 2006",
      "shortCiteRegEx" : "Precup et al\\.",
      "year" : 2006
    }, {
      "title" : "Effective reinforcement learning for mobile robots",
      "author" : [ "W.D. Smart", "L. Pack Kaelbling" ],
      "venue" : "Proceedings of International Conference on Robotics and Automation, volume 4, pp. 3404–3410.",
      "citeRegEx" : "Smart and Kaelbling,? 2002",
      "shortCiteRegEx" : "Smart and Kaelbling",
      "year" : 2002
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Sutton and Barto,? 1998",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "R.S. Sutton", "D. McAllester", "S. Singh", "Y. Mansour" ],
      "venue" : "Advances in Neural Information Processing Systems 12.",
      "citeRegEx" : "Sutton et al\\.,? 2000",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2000
    }, {
      "title" : "A convergent O(n) algorithm for off-policy temporaldifference learning with linear function approximation",
      "author" : [ "R.S. Sutton", "Szepesvári", "Cs.", "H.R. Maei" ],
      "venue" : "Advances in Neural Information Processing Systems 21, pp. 1609–1616.",
      "citeRegEx" : "Sutton et al\\.,? 2008",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2008
    }, {
      "title" : "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
      "author" : [ "R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "Szepesvári", "Cs.", "E. Wiewiora" ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine",
      "citeRegEx" : "Sutton et al\\.,? 2009",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2009
    }, {
      "title" : "Horde: A scalable realtime architecture for learning knowledge from unsupervised sensorimotor interaction",
      "author" : [ "R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "D. Precup" ],
      "venue" : "Proceedings of the 10th International Conference on Autonomous Agents",
      "citeRegEx" : "Sutton et al\\.,? 2011",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2011
    }, {
      "title" : "Q-learning",
      "author" : [ "Watkins", "C.J.C.H.", "P. Dayan" ],
      "venue" : "Machine Learning 8 (3):279–292.",
      "citeRegEx" : "Watkins et al\\.,? 1992",
      "shortCiteRegEx" : "Watkins et al\\.",
      "year" : 1992
    }, {
      "title" : "An asymptotically stable equilibrium for a dynamical system is an attracting point for which small perturbations still cause convergence back to that point",
      "author" : [ "supt(||zt||+ ||ut", "a.s" ],
      "venue" : null,
      "citeRegEx" : "<∞ and a.s.,? \\Q2008\\E",
      "shortCiteRegEx" : "<∞ and a.s.",
      "year" : 2008
    }, {
      "title" : "2009), we can deduce that ut converges almost surely to the set of asymptotically stable fixed points, Z̃, of u̇ = Γ̂g(u). For requirement (B7), we know that ut is bounded because it is always projected to U . Since u stays in U , we know that v stays bounded (by assumption, otherwise V π,γ would not be bounded) and correspondingly w(v) must stay",
      "author" : [ "Bhatnagar" ],
      "venue" : null,
      "citeRegEx" : "Bhatnagar,? \\Q2009\\E",
      "shortCiteRegEx" : "Bhatnagar",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Unlike onpolicy methods, off-policy methods are able to, for example, learn about an optimal policy while executing an exploratory policy (Sutton & Barto, 1998), learn from demonstration (Smart & Kaelbling, 2002), and learn multiple tasks in parallel from a single sensorimotor interaction with an environment (Sutton et al., 2011).",
      "startOffset" : 310,
      "endOffset" : 331
    }, {
      "referenceID" : 0,
      "context" : "However, while Q-Learning is guaranteed to converge to the optimal policy for the tabular (non-approximate) case, it may diverge when using linear function approximation (Baird, 1995).",
      "startOffset" : 170,
      "endOffset" : 183
    }, {
      "referenceID" : 10,
      "context" : ", 2009), such as Greedy-GQ (Maei et al., 2010), which are of linear complexity and convergent under off-policy training with function approximation.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 17,
      "context" : "The standard way of avoiding the limitations of actionvalue methods is to use policy-gradient algorithms (Sutton et al., 2000) such as actor-critic methods (e.",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 8,
      "context" : "For the critic, in this paper we consider a version of Off-PAC that uses GTD(λ) (Maei, 2011), a gradient-TD method with eligibitity traces for learning state-value functions.",
      "startOffset" : 80,
      "endOffset" : 92
    }, {
      "referenceID" : 4,
      "context" : "To the best of our knowledge, this paper is the first to provide an empirical evaluation of gradient-TD methods for off-policy control (the closest known prior work is the work of Delp (2011)).",
      "startOffset" : 180,
      "endOffset" : 192
    }, {
      "referenceID" : 20,
      "context" : "Given a termination condition γ : S → [0, 1] (Sutton et al., 2011), we define the value function for π : S×A → (0, 1] to be:",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 19,
      "context" : "Gradient-TD methods (Sutton et al., 2009) incrementally learn the weights, v, in an off-policy setting, with a guarantee of stability and a linear per-time-step complexity.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 8,
      "context" : "In this paper, we consider the version of Off-PAC that updates its critic weights by the GTD(λ) algorithm introduced by Maei (2011).",
      "startOffset" : 120,
      "endOffset" : 132
    }, {
      "referenceID" : 17,
      "context" : "In the conventional on-policy theory of policy-gradient methods, the policy-gradient theorem (Marbach & Tsitsiklis, 1998; Sutton et al., 2000) establishes the relationship between the gradient of the objective function and the expected action values.",
      "startOffset" : 93,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : "Following previous offpolicy gradient proofs (Maei, 2011), we study the behavior of the ordinary differential equation u̇(t) = u(h(u(t),v))",
      "startOffset" : 45,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "The two updates (for the actor and for the critic) are not independent on each time step; we analyze two separate ODEs using a two timescale analysis (Borkar, 2008).",
      "startOffset" : 150,
      "endOffset" : 164
    }, {
      "referenceID" : 12,
      "context" : "Previous work, however, has illustrated that the stochasticity in practice makes convergence to an unstable equilibrium unlikely (Pemantle, 1990); therefore, we avoid restrictions on the policy function and do not include the projection in our algorithm",
      "startOffset" : 129,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "For Markov noise, our proof as well as the proofs for GTD(λ) and GQ(λ), require Borkar’s (2008) two-timescale theory to be extended to Markov noise (which is outside the scope of this paper).",
      "startOffset" : 80,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "Proof Sketch: We follow a similar outline to the two timescale analysis for on-policy policy gradient Minimum exists as all eigenvalues real-valued (Lemma 4) GTD(0) is GTD(λ) with λ = 0, not the different algorithm called GTD(0) by Sutton, Szepesvari & Maei (2008)",
      "startOffset" : 253,
      "endOffset" : 265
    }, {
      "referenceID" : 1,
      "context" : "actor-critic (Bhatnagar et al., 2009) and for nonlinear GTD (Maei et al.",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : ", 2009) and for nonlinear GTD (Maei et al., 2009).",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "Note that Q(λ) may not be stable in this setting (Baird, 1995), unlike all the other algorithms.",
      "startOffset" : 49,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "The second problem is a pendulum problem (Doya, 2000).",
      "startOffset" : 41,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "actor-critic methods (Bhatnagar et al., 2009), which use the natural gradient as opposed to the conventional gradient.",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : "Finally, as pointed out by Precup et al. (2006), offpolicy updates can be more noisy compared to onpolicy learning.",
      "startOffset" : 27,
      "endOffset" : 48
    } ],
    "year" : 2017,
    "abstractText" : "This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in offpolicy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, however, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of offpolicy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems. The reinforcement learning framework is a general temporal learning formalism that has, over the last few decades, seen a marked growth in algorithms and applications. Until recently, however, practical online Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s). methods with convergence guarantees have been restricted to the on-policy setting, in which the agent learns only about the policy it is executing. In an off-policy setting, on the other hand, an agent learns about a policy or policies different from the one it is executing. Off-policy methods have a wider range of applications and learning possibilities. Unlike onpolicy methods, off-policy methods are able to, for example, learn about an optimal policy while executing an exploratory policy (Sutton & Barto, 1998), learn from demonstration (Smart & Kaelbling, 2002), and learn multiple tasks in parallel from a single sensorimotor interaction with an environment (Sutton et al., 2011). Because of this generality, off-policy methods are of great interest in many application domains. The most well known off-policy method is Q-learning (Watkins & Dayan, 1992). However, while Q-Learning is guaranteed to converge to the optimal policy for the tabular (non-approximate) case, it may diverge when using linear function approximation (Baird, 1995). Least-squares methods such as LSTD (Bradtke & Barto, 1996) and LSPI (Lagoudakis & Parr, 2003) can be used off-policy and are sound with linear function approximation, but are computationally expensive; their complexity scales quadratically with the number of features and weights. Recently, these problems have been addressed by the new family of gradientTD (Temporal Difference) methods (e.g., Sutton et al., 2009), such as Greedy-GQ (Maei et al., 2010), which are of linear complexity and convergent under off-policy training with function approximation. All action-value methods, including gradient-TD methods such as Greedy-GQ, suffer from three important limitations. First, their target policies are deterministic, whereas many problems have stochastic optimal policies, such as in adversarial settings or in partially observable Markov decision processes. Second, finding the greedy action with respect to the actionar X iv :1 20 5. 48 39 v1 [ cs .L G ] 2 2 M ay 2 01 2 Off-Policy Actor-Critic value function becomes problematic for larger action spaces. Finally, a small change in the action-value function can cause large changes in the policy, which creates difficulties for convergence proofs and for some real-time applications. The standard way of avoiding the limitations of actionvalue methods is to use policy-gradient algorithms (Sutton et al., 2000) such as actor-critic methods (e.g., Bhatnagar et al., 2009). For example, the natural actor-critic, an on-policy policy-gradient algorithm, has been successful for learning in continuous action spaces in several robotics applications (Peters & Schaal, 2008). The first and main contribution of this paper is to introduce the first actor-critic method that can be applied off-policy, which we call Off-PAC, for Off-Policy Actor–Critic. Off-PAC has two learners: the actor and the critic. The actor updates the policy weights. The critic learns an off-policy estimate of the value function for the current actor policy, different from the (fixed) behavior policy. This estimate is then used by the actor to update the policy. For the critic, in this paper we consider a version of Off-PAC that uses GTD(λ) (Maei, 2011), a gradient-TD method with eligibitity traces for learning state-value functions. We define a new objective for our policy weights and derive a valid backward-view update using eligibility traces. The time and space complexity of Off-PAC is linear in the number of learned weights. The second contribution of this paper is an off-policy policy-gradient theorem and a convergence proof for Off-PAC when λ = 0, under assumptions similar to previous off-policy gradient-TD proofs. Our third contribution is an empirical comparison of Q(λ), Greedy-GQ, Off-PAC, and a soft-max version of Greedy-GQ that we call Softmax-GQ, on three benchmark problems in an off-policy setting. To the best of our knowledge, this paper is the first to provide an empirical evaluation of gradient-TD methods for off-policy control (the closest known prior work is the work of Delp (2011)). We show that Off-PAC outperforms other algorithms on these problems. 1. Notation and Problem Setting In this paper, we consider Markov decision processes with a discrete state space S, a discrete action spaceA, a distribution P : S × S ×A → [0, 1], where P (s′|s, a) is the probability of transitioning into state s′ from state s after taking action a, and an expected reward function R : S×A×S → R that provides an expected reward for taking action a in state s and transitioning into s′. We observe a stream of data, which includes states st ∈ S, actions at ∈ A, and rewards rt ∈ R for t = 1, 2, . . . with actions selected from a fixed behavior policy, b(a|s) ∈ (0, 1]. Given a termination condition γ : S → [0, 1] (Sutton et al., 2011), we define the value function for π : S×A → (0, 1] to be: V (s) = E [rt+1 + . . .+ rt+T |st = s] ∀s ∈ S (1) where policy π is followed from time step t and terminates at time t + T according to γ. We assume termination always occurs in a finite number of steps. The action-value function, Q(s, a), is defined as: Q(s, a) = ∑ s′∈S P (s′|s, a)[R(s, a, s′) + γ(s′)V π,γ(s′)] (2) for all a ∈ A and for all s ∈ S. Note that V (s) = ∑ a∈A π(a|s)Q(s, a), for all s ∈ S. The policy πu : A×S → [0, 1] is an arbitrary, differentiable function of a weight vector, u ∈ Ru , Nu ∈ N, with πu(a|s) > 0 for all s ∈ S, a ∈ A. Our aim is to choose u so as to maximize the following scalar objective function:",
    "creator" : "LaTeX with hyperref package"
  }
}