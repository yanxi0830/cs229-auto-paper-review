{
  "name" : "1307.5302.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Kernel Adaptive Metropolis-Hastings",
    "authors" : [ "Dino Sejdinovic", "Heiko Strathmann", "Maria Lomeli Garcia", "Christophe Andrieu", "Arthur Gretton" ],
    "emails" : [ "DINO@GATSBY.UCL.AC.UK", "UCABHST@GATSBY.UCL.AC.UK", "MLOMELI@GATSBY.UCL.AC.UK", "C.ANDRIEU@BRISTOL.AC.UK", "ARTHUR.GRETTON@GMAIL.COM" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 7.\n53 02\nv3 [\nst at\n.M L\n] 1\n2 Ju\nn 20"
    }, {
      "heading" : "1. Introduction",
      "text" : "The choice of the proposal distribution is known to be crucial for the design of Metropolis-Hastings algorithms, and methods for adapting the proposal distribution to increase the sampler’s efficiency based on the history of the Markov chain have been widely studied. These methods often aim to learn the covariance structure of the target distribution, and adapt the proposal accordingly. Adaptive MCMC samplers were first studied by Haario et al. (1999;\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\n2001), where the authors propose to update the proposal distribution along the sampling process. Based on the chain history, they estimate the covariance of the target distribution and construct a Gaussian proposal centered at the current chain state, with a particular choice of the scaling factor from Gelman et al. (1996). More sophisticated schemes are presented by Andrieu & Thoms (2008), e.g., adaptive scaling, component-wise scaling, and principal component updates.\nWhile these strategies are beneficial for distributions that show high anisotropy (e.g., by ensuring the proposal uses the right scaling in all principal directions), they may still suffer from low acceptance probability and slow mixing when the target distributions are strongly nonlinear, and the directions of large variance depend on the current location of the sampler in the support. In the present work, we develop an adaptive Metropolis-Hastings algorithm in which samples are mapped to a reproducing kernel Hilbert space, and the proposal distribution is chosen according to the covariance in this feature space (Schölkopf et al., 1998; Smola et al., 2001). Unlike earlier adaptive approaches, the resulting proposal distributions are locally adaptive in input space, and oriented towards nearby regions of high density, rather than simply matching the global covariance structure of the distribution. Our approach combines a move in the feature space with a stochastic step towards the nearest input space point, where the feature space move can be analytically integrated out. Thus, the implementation of the procedure is straightforward: the proposal is simply a multivariate Gaussian in the input space, with location-dependent covariance which is informed by the feature space representation of the target. Furthermore, the resulting Metropolis-Hastings sampler only requires the ability to evaluate the unnormalized density of the target (or its unbiased estimate, as in Pseudo-Marginal MCMC of Andrieu & Roberts, 2009), and no gradient evaluation is needed, making it applicable to situations where more sophisticated schemes based on Hamiltonian Monte Carlo (HMC) or Metropolis Adjusted\nLangevin Algorithms (MALA) (Roberts & Stramer, 2003; Girolami & Calderhead, 2011) cannot be applied.\nWe begin our presentation in Section 2, with a brief overview of existing adaptive Metropolis approaches; we also review covariance operators in the RKHS. Based on these operators, we describe a sampling strategy for Gaussian measures in the RKHS in Section 3, and introduce a cost function for constructing proposal distributions. In Section 4, we outline our main algorithm, termed Kernel Adaptive Metropolis-Hastings (MCMC Kameleon). We provide experimental comparisons with other fixed and adaptive samplers in Section 5, where we show superior performance in the context of Pseudo-Marginal MCMC for Bayesian classification, and on synthetic target distributions with highly nonlinear shape."
    }, {
      "heading" : "2. Background",
      "text" : "Adaptive Metropolis Algorithms. Let X = Rd be the domain of interest, and denote the unnormalized target density on X by π. Additionally, let Σt = Σt(x0, x1, . . . , xt−1) denote an estimate of the covariance matrix of the target density based on the chain history {xi}t−1i=0 . The original adaptive Metropolis at the current state of the chain state xt = y uses the proposal\nqt(·|y) = N (y, ν2Σt), (1)\nwhere ν = 2.38/ √ d is a fixed scaling factor from Gelman et al. (1996). This choice of scaling factor was shown to be optimal (in terms of efficiency measures) for the usual Metropolis algorithm. While this optimality result does not hold for Adaptive Metropolis, it can nevertheless be used as a heuristic. Alternatively, the scale ν can also be adapted at each step as in Andrieu & Thoms (2008, Algorithm 4) to obtain the acceptance rate from Gelman et al. (1996), a∗ = 0.234.\nRKHS Embeddings and Covariance Operators. According to the Moore-Aronszajn theorem (Berlinet & Thomas-Agnan, 2004, p. 19), for every symmetric, positive definite function (kernel) k : X ×X → R, there is an associated reproducing kernel Hilbert space Hk of real-valued functions on X with reproducing kernel k. The map ϕ : X → Hk, ϕ : x 7→ k(·, x) is called the canonical feature map of k. This feature map or embedding of a single point can be extended to that of a probability measure P on X : its kernel embedding is an element µP ∈ Hk, given by µP = ´\nk(·, x) dP (x) (Berlinet & Thomas-Agnan, 2004; Fukumizu et al., 2004; Smola et al., 2007). If a measurable kernel k is bounded, it is straightforward to show using the Riesz representation theorem that the mean embedding µk(P ) exists for all probability measures on X . For many interesting\nbounded kernels k, including the Gaussian, Laplacian and inverse multi-quadratics, the kernel embedding P 7→ µP is injective. Such kernels are said to be characteristic (Sriperumbudur et al., 2010; 2011), since each distribution is uniquely characterized by its embedding (in the same way that every probability distribution has a unique characteristic function). The kernel embedding µP is the representer of expectations of smooth functions w.r.t. P , i.e., ∀f ∈ Hk, 〈f, µP 〉Hk = ´\nf(x)dP (x). Given samples z = {zi}ni=1 ∼ P , the embedding of the empirical measure is µz = 1n ∑n i=1 k(·, zi). Next, the covariance operator CP : Hk → Hk for a probability measure P is given by CP = ´\nk(·, x) ⊗ k(·, x) dP (x) − µP ⊗ µP (Baker, 1973; Fukumizu et al., 2004), where for a, b, c ∈ Hk the tensor product is defined as (a ⊗ b)c = 〈b, c〉Hk a. The covariance operator has the property that ∀f, g ∈ Hk, 〈f, CP g〉Hk = EP (fg)− EP fEP g. Our approach is based on the idea that the nonlinear support of a target density may be learned using Kernel Principal Component Analysis (Kernel PCA) (Schölkopf et al., 1998; Smola et al., 2001), this being linear PCA on the empirical covariance operator in the RKHS, Cz = 1 n ∑n i=1 k(·, zi) ⊗ k(·, zi) − µz ⊗ µz, computed on the sample z defined above. The empirical covariance operator behaves as expected: applying the tensor product definition gives 〈f, Czg〉Hk = 1 n ∑n i=1 f(zi)g(zi) − (\n1 n ∑n i=1 f(zi)\n) (\n1 n ∑n i=1 g(zi)\n)\n. By analogy with algorithms which use linear PCA directions to inform M-H proposals (Andrieu & Thoms, 2008, Algorithm 8), nonlinear PCA directions can be encoded in the proposal construction, as described in Appendix C. Alternatively, one can focus on a Gaussian measure on the RKHS determined by the empirical covariance operator Cz rather than extracting its eigendirections, which is the approach we pursue in this contribution. This generalizes the proposal (1), which considers the Gaussian measure induced by the empirical covariance matrix on the original space."
    }, {
      "heading" : "3. Sampling in RKHS",
      "text" : "We next describe the proposal distribution at iteration t of the MCMC chain. We will assume that a subset of the chain history, denoted z = {zi}ni=1, n ≤ t− 1, is available. Our proposal is constructed by first considering the samples in the RKHS associated to the empirical covariance operator, and then performing a gradient descent step on a cost function associated with those samples.\nGaussian Measure of the Covariance Operator. We will work with the Gaussian measure on the RKHS Hk with mean k(·, y) and covariance ν2Cz, where z = {zi}ni=1 is the subset of the chain history. While there is no analogue\nof a Lebesgue measure in an infinite dimensional RKHS, it is instructive (albeit with some abuse of notation) to denote this measure in the “density form” N (f ; k(·, y), ν2Cz) ∝ exp (\n− 12ν2 〈 f − k(·, y), C−1 z (f − k(·, y)) 〉\nHk\n)\n. As Cz is\na finite-rank operator, this measure is supported only on a finite-dimensional affine space k(·, y) +Hz, where Hz = span {k(·, zi)}ni=1 is the subspace spanned by the canonical features of z. It can be shown that a sample from this measure has the form f = k(·, y) + ∑ni=1 βi [k(·, zi)− µz] , where β ∼ N (0, ν2n I) is isotropic. Indeed, to see that f has the correct covariance structure, note that:\nE [(f − k(·, y))⊗ (f − k(·, y))]\n= E\n\n\nn ∑\ni=1\nn ∑\nj=1\nβiβj (k(·, zi)− µz)⊗ (k(·, zj)− µz)\n\n\n= ν2\nn\nn ∑\ni=1\n(k(·, zi)− µz)⊗ (k(·, zi)− µz) = ν2Cz.\nDue to the equivalence in the RKHS between a Gaussian measure and a Gaussian Process (GP) (Berlinet & Thomas-Agnan, 2004, Ch. 4), we can think of the RKHS samples f as trajectories of the GP with mean m(x) = k(x, y) and covariance function\nκ(x, x′) = cov [f(x), f(x′)]\n= ν2\nn\nn ∑\ni=1\n(k(x, zi)− µz(x)) (k(x′, zi)− µz(x′)) .\nThe covariance function κ of this GP is therefore the kernel k convolved with itself with respect to the empirical measure associated to the samples z, and draws from this GP therefore lie in a smaller RKHS; see Saitoh (1997, p. 21) for details.\nObtaining Target Samples through Gradient Descent. We have seen how to obtain the RKHS sample f = k(·, y) + ∑ni=1 βi [k(·, zi)− µz] from the Gaussian measure in the RKHS. This sample does not in general have a corresponding pre-image in the original domain X = Rd; i.e., there is no point x∗ ∈ X such that f = k(·, x∗). If there were such a point, then we could use it as a proposal in the original domain. Therefore, we are ideally looking for a point x∗ ∈ X whose canonical feature map k(·, x∗) is close to f in the RKHS norm. We consider the optimization problem\nargmin x∈X ‖k (·, x)− f‖2Hk =\nargmin x∈X\n{\nk(x, x)− 2k(x, y)− 2 n ∑\ni=1\nβi [k(x, zi)− µz(x)]\n}\n.\nIn general, this is a non-convex minimization problem, and may be difficult to solve (Bakir et al., 2003). Rather than solving it for every new vector of coefficients β, which would lead to an excessive computational burden for every proposal made, we simply make a single descent step along the gradient of the cost function,\ng(x) = k(x, x)− 2k(x, y)− 2 n ∑\ni=1\nβi [k(x, zi)− µz(x)] ,\n(2) i.e., the proposed new point is\nx∗ = y − η∇xg(x)|x=y + ξ, where η is a gradient step size parameter and ξ ∼ N (0, γ2I) is an additional isotropic ’exploration’ term after the gradient step. It will be useful to split the scaled gradient at y into two terms as η∇xg(x)|x=y = η (ay −Mz,yHβ), where ay = ∇xk(x, x)|x=y − 2∇xk(x, y)|x=y ,\nMz,y = 2 [∇xk(x, z1)|x=y, . . . ,∇xk(x, zn)|x=y] (3) is a d×n matrix, and H = I− 1n1n×n is the n×n centering matrix.\nFigure 1 plots g(x) and its gradients for several samples of β-coefficients, in the case where the underlying z-samples are from the two-dimensional nonlinear Banana target distribution of Haario et al. (1999). It can be seen that g may have multiple local minima, and that it varies most along the high-density regions of the Banana distribution."
    }, {
      "heading" : "4. MCMC Kameleon Algorithm",
      "text" : ""
    }, {
      "heading" : "4.1. Proposal Distribution",
      "text" : "We now have a recipe to construct a proposal that is able to adapt to the local covariance structure for the current chain\nMCMC Kameleon Input: unnormalized target π, subsample size n, scaling parameters ν, γ, adaptation probabilities {pt}∞t=0, kernel k,\n• At iteration t+ 1, 1. With probability pt, update a random subsample\nz = {zi}min(n,t)i=1 of the chain history {xi} t−1 i=0 ,\n2. Sample proposed point x∗ from qz(·|xt) = N (xt, γ2I + ν2Mz,xtHM⊤z,xt), where Mz,xt is given in Eq. (3) and H = I − 1n1n×n is the centering matrix,\n3. Accept/Reject with the Metropolis-Hastings acceptance probability A(xt, x∗) in Eq. (4),\nxt+1 =\n{\nx∗, w.p. A(xt, x∗), xt, w.p. 1−A(xt, x∗).\nstate y. This proposal depends on a subset of the chain history z, and is denoted by qz(·|y). While we will later simplify this proposal by integrating out the moves in the RKHS, it is instructive to think of the proposal generating process as:\n1. Sample β ∼ N (0, ν2I) (n× 1 normal of RKHS coefficients).\n• This represents an RKHS sample f = k(·, y) + ∑n\ni=1 βi [k(·, zi)− µz] which is the goal of the cost function g(x).\n2. Move along the gradient of g: x∗ = y − η∇xg(x)|x=y + ξ. • This gives a proposal x∗|y, β ∼ N (y − ηay + ηMz,yHβ, γ\n2I) (d × 1 normal in the original space).\nOur first step in the derivation of the explicit proposal density is to show that as long as k is a differentiable positive definite kernel, the term ay vanishes.\nProposition 1. Let k be a differentiable positive definite kernel. Then ay = ∇xk(x, x)|x=y − 2∇xk(x, y)|x=y = 0.\nSince ay = 0, the gradient step size η always appears together with β, so we merge η and the scale ν of the βcoefficients into a single scale parameter, and set η = 1 henceforth. Furthermore, since both p(β) and pz(x∗|y, β) are multivariate Gaussian densities, the proposal density qz(x ∗|y) = ´ p(β)pz(x ∗|y, β)dβ can be computed analytically. We therefore get the following closed form expression for the proposal distribution.\nProposition 2. qz(·|y) = N (y, γ2I + ν2Mz,yHM⊤z,y).\nProofs of the above Propositions are given in Appendix A.\nWith the derived proposal distribution, we proceed with the standard Metropolis-Hastings accept/reject scheme, where the proposed sample x∗ is accepted with probability\nA(xt, x ∗) = min\n{ 1, π(x∗)qz(xt|x∗) π(xt)qz(x∗|xt) } , (4)\ngiving rise to the MCMC Kameleon Algorithm. Note that each π(x∗) and π(xt) could be replaced by their unbiased estimates without impacting the invariant distribution (Andrieu & Roberts, 2009). The constructed family of proposals encodes local structure of the target distribution, which is learned based on the subsample z. Figure 2 depicts the regions that contain 95% of the mass of the proposal distribution qz(·|y) at various states y for a fixed subsample z, where the Banana target is used (details in Section 5). More examples of proposal contours can be found in Appendix B."
    }, {
      "heading" : "4.2. Properties of the Algorithm",
      "text" : "The update schedule and convergence. MCMC Kameleon requires a subsample z = {zi}ni=1 at each iteration of the algorithm, and the proposal distribution qz(·|y) is updated each time a new subsample z is obtained. It is well known that a chain which keeps adapting the proposal distribution need not converge to the correct target (Andrieu & Thoms, 2008). To guarantee convergence, we introduce adaptation probabilities {pt}∞t=0, such that pt → 0 and ∑∞\nt=1 pt = ∞, and at iteration t we update the subsample z with probability pt. As adaptations occur with decreasing probability, Theorem 1 of Roberts & Rosenthal (2007) implies that the resulting algorithm is ergodic and converges to the correct target. Another straightforward way to guarantee convergence is to fix the set z = {zi}ni=1 after a “burn-in” phase; i.e., to stop adapting Roberts & Rosenthal (2007, Proposition 2). In this case, a “burn-in” phase is used to get a rough sketch of the shape of the distribution: the initial samples need not\ncome from a converged or even valid MCMC chain, and it suffices to have a scheme with good exploratory properties, e.g., Welling & Teh (2011). In MCMC Kameleon, the term γ allows exploration in the initial iterations of the chain (while the subsample z is still not informative about the structure of the target) and provides regularization of the proposal covariance in cases where it might become ill-conditioned. Intuitively, a good approach to setting γ is to slowly decrease it with each adaptation, such that the learned covariance progressively dominates the proposal.\nSymmetry of the proposal. In Haario et al. (2001), the proposal distribution is asymptotically symmetric due to the vanishing adaptation property. Therefore, the authors compute the standard Metropolis acceptance probability. In our case, the proposal distribution is a Gaussian with mean at the current state of the chain xt = y and covariance γ2I + ν2Mz,yHM ⊤ z,y , where Mz,y depends both on the current state y and a random subsample z = {zi}ni=1 of the chain history {xi}t−1i=0 . This proposal distribution is never symmetric (as covariance of the proposal always depends on the current state of the chain), and therefore we use the Metropolis-Hastings acceptance probability to reflect this.\nRelationship to MALA and Manifold MALA. The Metropolis Adjusted Langevin Algorithm (MALA) algorithm uses information about the gradient of the log-target density at the current chain state to construct a proposed point for the Metropolis step. Our approach does not require that the log-target density gradient be available or computable. Kernel gradients in the matrix Mz,y are easily obtained for commonly used kernels, including the Gaussian kernel (see section 4.3), for which the computational complexity is equal to evaluating the kernel itself. Moreover, while standard MALA simply shifts the mean of the proposal distribution along the gradient and then adds an isotropic exploration term, our proposal is centered at the current state, and it is the covariance structure of the proposal distribution that coerces the proposed points to belong to the high-density regions of the target. It would be straightforward to modify our approach to include a drift term along the gradient of the log-density, should such information be available, but it is unclear whether this would provide additional performance gains. Further work is required to elucidate possible connections between our approach and the use of a preconditioning matrix (Roberts & Stramer, 2003) in the MALA proposal; i.e., where the exploration term is scaled with appropriate metric tensor information, as in Riemannian manifold MALA (Girolami & Calderhead, 2011)."
    }, {
      "heading" : "4.3. Examples of Covariance Structure for Standard Kernels",
      "text" : "The proposal distributions in MCMC Kameleon are dependant on the choice of the kernel k. To gain intuition re-\ngarding their covariance structure, we give two examples below.\nLinear kernel. In the case of a linear kernel k(x, x′) = x⊤x′, we obtain Mz,y = 2 [ ∇xx⊤z1|x=y, . . . ,∇xx⊤zn|x=y ]\n= 2Z⊤, so the proposal is given by qz(·|y) = N (y, γ2I + 4ν2Z⊤HZ); thus, the proposal simply uses the scaled empirical covariance Z⊤HZ just like standard Adaptive Metropolis (Haario et al., 1999), with an additional isotropic exploration component, and depends on y only through the mean.\nGaussian kernel. In the case of a Gaussian kernel\nk(x, x′) = exp\n(\n−‖x−x ′‖2 2\n2σ2\n)\n, since ∇xk(x, x′) = 1 σ2 k(x, x ′)(x′ − x), we obtain\nMz,y = 2\nσ2 [k(y, z1)(z1 − y), . . . , k(y, zn)(zn − y)] .\nConsider how this encodes the covariance structure of the target distribution:\nRij = γ 2 δij\n+ 4ν2(n− 1)\nσ4n\nn ∑\na=1\n[k(y, za)] 2 (za,i − yi)(za,j − yj)\n− 4ν2\nσ4n\n∑\na 6=b\nk(y, za)k(y, zb)(za,i − yi)(zb,j − yj). (5)\nAs the first two terms dominate, the previous points za which are close to the current state y (for which k(y, za) is large) have larger weights, and thus they have more influence in determining the covariance of the proposal at y.\nMatérn kernel. In the Matérn family of kernels\nkϑ,ρ(x, x ′) = 2\n1−ϑ\nΓ(ϑ)\n(\n‖x−x′‖ 2\nρ\n)ϑ\nKϑ\n(\n‖x−x′‖ 2\nρ\n)\n, where\nKϑ is the modified Bessel function of the second kind, we obtain a form of the covariance structure very similar to that of the Gaussain kernel. In this case, ∇xkϑ,ρ(x, x′) =\n1 2ρ2(ϑ−1)kϑ−1,ρ(x, x ′)(x′−x), so the only difference (apart from the scalings) to (5) is that the weights are now determined by a “rougher” kernel kϑ−1,ρ of the same family."
    }, {
      "heading" : "5. Experiments",
      "text" : "In the experiments, we compare the following samplers: (SM) Standard Metropolis with the isotropic proposal q(·|y) = N (y, ν2I) and scaling ν = 2.38/ √ d, (AMFS) Adaptive Metropolis with a learned covariance matrix and fixed scaling ν = 2.38/ √ d, (AM-LS) Adaptive Metropolis with a learned covariance matrix and scaling learned to bring the acceptance rate close to α∗ = 0.234 as described in Andrieu & Thoms (2008, Algorithm 4), and (KAMH-LS) MCMC Kameleon with the scaling ν learned\nin the same fashion (γ was fixed to 0.2), and which also stops adapting the proposal after the burn-in of the chain (in all experiments, we use a random subsample z of size n = 1000, and a Gaussian kernel with bandwidth selected according to the median heuristic). We consider the following nonlinear targets: (1) the posterior distribution of Gaussian Process (GP) classification hyperparameters (Filippone & Girolami, 2014) on the UCI glass dataset, and (2) the synthetic banana-shaped distribution of Haario et al. (1999) and a flower-shaped disribution concentrated on a circle with a periodic perturbation."
    }, {
      "heading" : "5.1. Pseudo-Marginal MCMC for GP Classification",
      "text" : "In the first experiment, we illustrate usefulness of the MCMC Kameleon sampler in the context of Bayesian classification with GPs (Williams & Barber, 1998). Consider the joint distribution of latent variables f , labels y (with covariate matrix X), and hyperparameters θ, given by\np(f ,y, θ) = p(θ)p(f |θ)p(y|f),\nwhere f |θ ∼ N (0,Kθ), with Kθ modeling the covariance between latent variables evaluated at the input covariates: (Kθ)ij = κ(xi,x′j |θ) = exp ( − 12 ∑D d=1 (xi,d−x ′ j,d) 2\nℓ2 d\n)\nand θd = log ℓ2d. We restrict our attention to the binary logistic classifier; i.e., the likelihood is given by p(yi|fi) = 11−exp(−yifi) where yi ∈ {−1, 1}. We pursue a fully Bayesian treatment, and estimate the posterior of the hyperparameters θ. As observed by Murray & Adams (2012), a Gibbs sampler on p(θ, f |y), which samples from p(f |θ, y) and p(θ|f , y) in turn, is problematic, as p(θ|f , y) is extremely sharp, drastically limiting the amount that any Markov chain can update θ|f , y. On the other hand, if we directly consider the marginal posterior p(θ|y) ∝ p(y|θ)p(θ) of the hyperparameters, a much less peaked distribution can be obtained. However, the marginal likelihood p(y|θ) is intractable for non-Gaussian likelihoods p(y|f), so it is not possible to analytically integrate out the latent variables. Recently developed pseudo-marginal MCMC methods (Andrieu & Roberts, 2009) enable exact\ninference on this problem (Filippone & Girolami, 2014), by replacing p(y|θ) with an unbiased estimate\np̂(y|θ) := 1 nimp\nnimp ∑\ni=1\np(y|f (i))p(f (i)|θ)\nq(f (i)|θ) , (6)\nwhere { f (i) }nimp\ni=1 ∼ q(f |θ) are nimp importance sam-\nples. In Filippone & Girolami (2014), the importance distribution q(f |θ) is chosen as the Laplacian or as the Expectation Propagation (EP) approximation of p(f |y, θ) ∝ p(y|f)p(f |θ), leading to state-of-the-art results. We consider the UCI Glass dataset (Bache & Lichman, 2013), where classification of window against non-window glass is sought. Due to the heterogeneous structure of each of the classes (i.e., non-window glass consists of containers, tableware and headlamps), there is no single consistent set of lengthscales determining the decision boundary, so one expects the posterior of the covariance bandwidths θd to have a complicated (nonlinear) shape. This is illustrated by the plot of the posterior projections to the dimensions 2 and 7 (out of 9) in Figure 3. Since the ground truth for the hyperparameter posterior is not available, we initially ran 30 Standard Metropolis chains for 500,000 iterations (with a 100,000 burn-in), kept every 1000-th sample in each of the chains, and combined them. The resulting samples were used as a benchmark, to evaluate the performance of shorter single-chain runs of SM, AM-FS, AMLS and KAMH-LS. Each of these algorithms was run for 100,000 iterations (with a 20,000 burnin) and every 20-th sample was kept. Two metrics were used in evaluating the performance of the four samplers, relative to the large-scale benchmark. First, the distance ∥\n∥µ̂θ − µbθ ∥ ∥ 2 was computed\nbetween the mean µ̂θ estimated from each of the four sampler outputs, and the mean µbθ on the benchmark sample (Fig. 4, left), as a function of sample size. Second, the MMD (Borgwardt et al., 2006; Gretton et al., 2007) was computed between each sampler output and the benchmark sample, using the polynomial kernel (1 + 〈θ, θ′〉)3; i.e., the comparison was made in terms of all mixed moments of order up to 3 (Fig. 4, right). The figures indicate that KAMH-LS approximates the benchmark sample better than the competing approaches, where the effect is especially pronounced in the high order moments, indicating that KAMH-LS thoroughly explores the distribution support in a relatively small number of samples. We emphasise that, as for any pseudo-marginal MCMC scheme, neither the likelihood itself, nor any higherorder information about the marginal posterior target p(θ|y), are available. This makes HMC or MALA based approaches such as (Roberts & Stramer, 2003; Girolami & Calderhead, 2011) unsuitable for this problem, so it is very difficult to deal with strongly nonlinear posterior targets. In contrast, as indicated in this example, the MCMC Kameleon scheme is able to effectively sample\nModerately twisted 8-dimensional B(0.03,100) target; iterations: 40000, burn-in: 20000\nfrom such nonlinear targets, and outperforms the vanilla Metropolis methods, which are the only competing choices in the pseudo-marginal context.\nIn addition, since the bulk of the cost for pseudo-marginal MCMC is in importance sampling in order to obtain the acceptance ratio, the additional cost imposed by KAMHLS is negligible. Indeed, we observed that there is an increase of only 2-3% in terms of effective computation time in comparison to all other samplers, for the chosen size of the chain history subsample (n = 1000)."
    }, {
      "heading" : "5.2. Synthetic examples",
      "text" : "Banana target. In Haario et al. (1999), the following family of nonlinear target distributions is considered. Let X ∼ N (0,Σ) be a multivariate normal in d ≥ 2 dimensions, with Σ = diag(v, 1, . . . , 1), which undergoes the transformation X → Y , where Y2 = X2 + b(X21 − v), and Yi = Xi for i 6= 2. We will write Y ∼ B(b, v). It is clear that EY = 0, and that B(y; b, v) = N (y1; 0, v)N (y2; b(y21−v), 1) d ∏\nj=3\nN (yj ; 0, 1).\nFlower target. The second target distribution we consider is the d-dimensional flower target F(r0, A, ω, σ), with\nF(x; r0, A,ω, σ) =\nexp\n(\n−\n√\nx2 1 + x2 2 − r0 −A cos (ωatan2 (x2, x1))\n2σ2\n)\n× d ∏\nj=3\nN (xj ; 0, 1).\nThis distribution concentrates around the r0-circle with a periodic perturbation (with amplitude A and frequency ω) in the first two dimensions.\nIn these examples, exact quantile regions of the targets can be computed analytically, so we can directly assess performance without the need to estimate distribution distances on the basis of samples (i.e., by estimating MMD to the benchmark sample). We compute the following measures of performance (similarly as in Haario et al. (1999); Andrieu & Thoms (2008)) based on the chain after burn-in: average acceptance rate, norm of the empirical mean (the true mean is by construction zero for all targets), and the deviation of the empirical quantiles from the true quantiles. We consider 8-dimensional target distributions: the moderately twisted B(0.03, 100) banana target (Figure 5, top) and the strongly twisted B(0.1, 100) banana target (Figure 5, middle) and F(10, 6, 6, 1) flower target (Figure 5, bottom).\nThe results show that MCMC Kameleon is superior to the competing samplers. Since the covariance of the proposal\nadapts to the local structure of the target at the current chain state, as illustrated in Figure 2, MCMC Kameleon does not suffer from wrongly scaled proposal distributions. The result is a significantly improved quantile performance in comparison to all competing samplers, as well as a comparable or superior norm of the empirical mean. SM has a significantly larger norm of the empirical mean, due to its purely random walk behavior (e.g., the chain tends to get stuck in one part of the space, and is not able to traverse both tails of the banana target equally well). AM with fixed scale has a low acceptance rate (indicating that the scaling of the proposal is too large), and even though the norm of the empirical mean is much closer to the true value, quantile performance of the chain is poor. Even if the estimated covariance matrix closely resembles the true global covariance matrix of the target, using it to construct proposal distributions at every state of the chain may not be the best choice. For example, AM correctly captures scalings along individual dimensions for the flower target (the norm of its empirical mean is close to its true value of zero) but fails to capture local dependence structure. The flower target, due to its symmetry, has an isotropic covariance in the first two dimensions – even though they are highly dependent. This leads to a mismatch in the scale of the covariance and the scale of the target, which concentrates on a thin band in the joint space. AM-LS has the “correct” acceptance rate, but the quantile performance is even worse, as the scaling now becomes too small to traverse high-density regions of the target."
    }, {
      "heading" : "6. Conclusions",
      "text" : "We have constructed a simple, versatile, adaptive, gradientfree MCMC sampler that constructs a family of proposal distributions based on the sample history of the chain. These proposal distributions automatically conform to the local covariance structure of the target distribution at the current chain state. In experiments, the sampler outperforms existing approaches on nonlinear target distributions, both by exploring the entire support of these distributions, and by returning accurate empirical quantiles, indicating faster mixing. Possible extensions include incorporating additional parametric information about the target densities, and exploring the tradeoff between the degree of subsampling of the chain history and convergence of the sampler.\nSoftware. Python implementation of MCMC Kameleon is available at https://github.com/karlnapf/kameleon-mcmc.\nAcknowledgments. D.S., H.S., M.L.G. and A.G. acknowledge support of the Gatsby Charitable Foundation. We thank Mark Girolami for insightful discussions and the anonymous reviewers for useful comments."
    }, {
      "heading" : "A. Proofs",
      "text" : "Proposition 1. Let k be a differentiable positive definite kernel. Then ∇xk(x, x)|x=y − 2∇xk(x, y)|x=y = 0.\nProof. Since k is a positive definite kernel there exists a Hilbert space H and a feature map ϕ : Rd → H, such that k(x, x′) = 〈ϕ(x), ϕ(x′)〉H. Consider first the map τ : Rd → R, defined by τ(x) = k(x, x). We write τ = ψ ◦ ϕ, where ψ : H → R, ψ(f) = ‖f‖2H . We can obtain ∇xk(x, x)|x=y from the Fréchet derivativeDτ(y) ∈ B(Rd,R) of τ at y, which to each y ∈ Rd associates a bounded linear operator from Rd to R (Steinwart & Christmann, 2008, Definition A.5.14). By the chain rule for\nFréchet derivatives (Steinwart & Christmann, 2008, Lemma A.5.15(b)), the value of Dτ(y) at some x′ ∈ Rd is\n[Dτ(y)] (x′) = [Dψ (ϕ(y)) ◦Dϕ(y)] (x′),\nwhere Dϕ(y) ∈ B(Rd,H), and Dψ (ϕ(y)) ∈ B(H,R). The derivative Dϕ of the feature map exists whenever k is a differentiable function (Steinwart & Christmann, 2008, Section 4.3). It is readily shown that Dψ [ϕ(y)] = 2 〈ϕ(y), ·〉H, so that\n[Dτ(y)] (x′) = 2 〈ϕ(y), [Dϕ(y)] (x′)〉H .\nNext, we consider the map κy(x) = k(x, y) = 〈ϕ(x), ϕ(y)〉H, i.e., κy = ψy ◦ ϕ where ψy(f) = 〈f, ϕ(y)〉H. Since ψy is a linear scalar function on H, Dψy (f) = 〈ϕ(y), ·〉H. Again, by the chain rule:\n[Dκy(y)] (x ′) = [Dψy (ϕ(y)) ◦Dϕ(y)] (x′)\n= 〈ϕ(y), [Dϕ(y)] (x′)〉H ,\nand thus (Dτ(y)− 2Dκy(y)) (x′) = 0, for all x′ ∈ Rd, and we obtain equality of operators. Since Fréchet derivatives can also be written as inner products with the gradients, (∇xk(x, x)|x=y − 2∇xk(x, y)|x=y)⊤ x′ = (Dτ(y) − 2Dκy(y)) (x′) = 0, ∀x′ ∈ Rd, which proves the claim. Proposition 2. qz(·|y) = N (y, γ2I + ν2Mz,yHM⊤z,y).\nProof. We start with\np(β)p(x∗|y, β) = 1 (2π) n+d 2 γdνn exp\n(\n− 1 2ν2 β⊤β\n)\n· exp ( − 1 2γ2 (x∗ − y −Mz,yHβ)⊤ (x∗ − y −Mz,yHβ) )\n= 1\n(2π) n+d 2 γdνn\nexp\n(\n− 1 2γ2 (x∗ − y)⊤ (x∗ − y) )\n· exp ( −1 2 ( β⊤ ( 1 ν2 I + 1 γ2 HM⊤ z,yMz,yH ) β − 2 γ2 β⊤HM⊤ z,y(x ∗ − y) )) .\nNow, we set\nΣ−1 = 1\nν2 I +\n1\nγ2 HM⊤ z,yMz,yH\nµ = 1\nγ2 ΣHM⊤ z,y(x ∗ − y),\nand application of the standard Gaussian integral ˆ\nexp\n(\n−1 2 ( β⊤Σ−1β − 2β⊤Σ−1µ )\n)\ndβ =\n(2π)n/2 √ det Σ exp ( 1\n2 µ⊤Σ−1µ\n)\n,\nleads to\nqz(x ∗|y) =\n√ detΣ\n(2π) d 2 γdνn\nexp\n(\n− 1 2γ2 (x∗ − y)⊤ (x∗ − y) )\n· exp ( 1\n2 µ⊤Σ−1µ\n)\n.\nThis is just a d-dimensional Gaussian density where both the mean and covariance will, in general, depend on y. Let us consider the exponent\n− 1 2γ2 (x∗ − y)⊤ (x∗ − y) + 1 2 µ⊤Σ−1µ =\n−1 2\n{\n1\nγ2 (x∗ − y)⊤ (x∗ − y)\n− 1 γ4\n(x∗ − y)⊤Mz,yHΣHM⊤z,y (x∗ − y) } =\n−1 2 { (x∗ − y)⊤R−1 (x∗ − y) } ,\nwhere R−1 = 1γ2 (I − 1γ2Mz,yHΣHM⊤z,y). We can simplify the covariance R using the Woodbury identity to obtain:\nR = γ2(I − 1 γ2 Mz,yHΣHM ⊤ z,y) −1\n= γ2 ( I +Mz,yH ( γ2Σ−1 −HM⊤ z,yMz,yH )−1 HM⊤ z,y )\n= γ2\n(\nI +Mz,yH\n(\nγ2 ν2 I\n)−1\nHM⊤ z,y\n)\n= γ2I + ν2Mz,yHM ⊤ z,y.\nTherefore, the proposal density is qz(·|y) = N (y, γ2I + ν2Mz,yHM⊤z,y)."
    }, {
      "heading" : "B. Further details on synthetic experiments",
      "text" : "Proposal contours for the Flower target. The d-dimensional flower target F(r0, A, ω, σ) is given by\nF(x; r0, A, ω, σ) = exp ( − √ x21 + x 2 2 − r0 −A cos (ωatan2 (x2, x1))\n2σ2\n)\nN (x3:d; 0, I).\nThis distribution concentrates around the r0-circle with a periodic perturbation (with amplitude A and frequency ω) in the first two dimensions. For A = 0, we obtain a band around the r0-circle, which we term the ring target. Figure 6 gives the contour plots of the MCMC Kameleon proposal distributions on two instances of the flower target.\nConvergence statistics for the Banana target. Figure 7 illustrates how the norm of the mean and quantile deviation (shown for 0.5-quantile) for the strongly twisted Banana target decrease as a function of the number of iterations. This shows that the trends observed in the main text persist along the evolution of the whole chain."
    }, {
      "heading" : "C. Principal Components Proposals",
      "text" : "An alternative approach to the standard adaptive Metropolis, discussed in Andrieu & Thoms (2008, Algorithm 8), is to extract m ≤ d principal eigenvalue-eigenvector pairs {(λj , vj)}mj=1 from the estimated covariance matrix Σz and use the proposal that takes form of a mixture of one-dimensional random walks along the principal eigendirections\nqz (·|y) = m ∑\nj=1\nωjN (y, ν2j λjvjv⊤j ). (7)\nIn other words, given the current chain state y, the j-th principal eigendirection is chosen with probability ωj (choice ωj = λj/ ∑m l=1 λl is suggested), and the proposed point is\nx∗ = y + ρνj √ λjvj , (8)\nwith ρ ∼ N (0, 1). Note that each eigendirection may have a different scaling factor νj in addition to the scaling with the eigenvalue.\nWe can consider an analogous version of the update (8) performed in the RKHS\nf = k(·, y) + ρνj √ λjvj , (9)\nwith m ≤ n principal eigenvalue-eigenfunction pairs {(λj ,vj)}mj=1. It is readily shown that the eigenfunctions vj = ∑n\ni=1 α̃ (j) i [k(·, zi)− µz] lie in the subspace Hz induced by z, and that the coefficients vectors α̃(j) =\n(\nα̃ (j) 1 · · · α̃ (j) n\n)⊤\nare proportional to the eigenvector of the centered kernel matrix HKH , with normalization chosen so that ‖vj‖2Hk = (\nα̃(j) )⊤ HKHα̃(j) = λj ∥ ∥α̃(j) ∥ ∥ 2\n2 = 1 (so that the eigenfunctions have the unit RKHS norm). Therefore, the update (9)\nhas form\nf = k(·, y) + n ∑\ni=1\nβ (j) i [k(·, zi)− µz] ,\nwhere β(j) = ρνj √ λj α̃ (j). But α(j) = √ λjα̃ (j) are themselves the (unit norm) eigenvectors of HKH, as\n∥ ∥α(j) ∥ ∥\n2 2 =\nλj ∥ ∥α̃(j) ∥ ∥\n2 2 = 1. Therefore, the appropriate scaling with eigenvalues is already included in the β-coefficients, just like in\nthe MCMC Kameleon, where the β-coefficients are isotropic.\nNow, we can construct the MCMC PCA-Kameleon by simply substituting β-coefficients with ρνjα(j), where j is the selected eigendirection, and νj is the scaling factor associated to the j-th eigendirection. We have the following steps:\n1. Perform eigendecomposition of HKH to obtain the m ≤ n eigenvectors {αj}mj=1 .\n2. Draw j ∼ Discrete [ω1, . . . , ωm]\n3. ρ ∼ N (0, 1)\n4. x∗|y, ρ, j ∼ N (y + ρνjMz,yHα(j), γ2I) (d× 1 normal in the original space)\nSimilarly as before, we can simplify the proposal by integrating out the scale ρ of the moves in the RKHS.\nProposition 3. qz(·|y) = ∑m j=1 ωjN (y, γ2I + ν2jMz,yHα(j) ( α(j) )⊤ HM⊤ z,y).\nProof. We start with\np(ρ)p(x∗ | y, ρ, j) ∝ exp [ − 1 2γ2 (x∗ − y)⊤(x∗ − y) ]\n· exp [\n−1 2\n{(\n1 + ν2j γ2 ( α(j) )⊤ HM⊤ z,yMz,yHα (j)\n)\nρ2 − 2ρ νj γ2 ( α(j) )⊤ HM⊤ z,y (x\n∗ − y) }] .\nBy substituting\nσ−2 = 1+ ν2j γ2 ( α(j) )⊤ HM⊤ z,yMz,yHα (j),\nµ = σ2 ( νj γ2 ( α(j) )⊤ HM⊤ z,y (x ∗ − y) ) ,\nwe integrate out ρ to obtain:\np (x∗ | y, j) ∝ exp [\n−1 2\n{\n1\nγ2 (x∗ − y)⊤(x∗ − y)−\nν2j σ 2\nγ4 (x∗ − y)⊤ Mz,yHα(j)\n( α(j) )⊤\nHM⊤ z,y (x\n∗ − y) }]\n= exp\n[\n−1 2 (x∗ − y)⊤ R−1 (x∗ − y)\n]\nwhere R−1 = 1γ2 ( I − ν 2 j σ 2 γ2 Mz,yHα (j) ( α(j) )⊤ HM⊤ z,y ) . We can simplify the covariance R using the Woodbury identity to obtain:\nR = γ2(I − ν2j σ 2\nγ2 Mz,yHα\n(j) ( α(j) )⊤\nHM⊤ z,y) −1\n= γ2\n I + ν2j σ 2\nγ2 Mz,yHα\n(j)\n(\n1− ν2j σ 2\nγ2\n( α(j) )⊤\nHM⊤ z,yMz,yHα (j)\n)−1 (\nα(j) )⊤\nHM⊤ z,y\n\n\n= γ2\n(\nI + ν2j γ2 Mz,yHα (j) ( α(j) )⊤ HM⊤ z,y\n)\n= γ2I + ν2jMz,yHα (j)\n( α(j) )⊤\nHM⊤ z,y.\nThe claim follows after summing over the choice j of the eigendirection (w.p. ωj)."
    } ],
    "references" : [ {
      "title" : "The pseudo-marginal approach for efficient Monte Carlo computations",
      "author" : [ "C. Andrieu", "G.O. Roberts" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "Andrieu and Roberts,? \\Q2009\\E",
      "shortCiteRegEx" : "Andrieu and Roberts",
      "year" : 2009
    }, {
      "title" : "A tutorial on adaptive MCMC",
      "author" : [ "C. Andrieu", "J. Thoms" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "Andrieu and Thoms,? \\Q2008\\E",
      "shortCiteRegEx" : "Andrieu and Thoms",
      "year" : 2008
    }, {
      "title" : "Joint measures and cross-covariance operators",
      "author" : [ "C. Baker" ],
      "venue" : "Transactions of the American Mathematical Society,",
      "citeRegEx" : "Baker,? \\Q1973\\E",
      "shortCiteRegEx" : "Baker",
      "year" : 1973
    }, {
      "title" : "Learning to find preimages",
      "author" : [ "G. Bakir", "J. Weston", "B. Schölkopf" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bakir et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bakir et al\\.",
      "year" : 2003
    }, {
      "title" : "Reproducing Kernel Hilbert Spaces in Probability and Statistics",
      "author" : [ "A. Berlinet", "C. Thomas-Agnan" ],
      "venue" : null,
      "citeRegEx" : "Berlinet and Thomas.Agnan,? \\Q2004\\E",
      "shortCiteRegEx" : "Berlinet and Thomas.Agnan",
      "year" : 2004
    }, {
      "title" : "Integrating structured biological data by kernel maximum mean discrepancy",
      "author" : [ "K.M. Borgwardt", "A. Gretton", "M.J. Rasch", "Kriegel", "H.-P", "B. Schölkopf", "A.J. Smola" ],
      "venue" : "Bioinformatics (ISMB),",
      "citeRegEx" : "Borgwardt et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Borgwardt et al\\.",
      "year" : 2006
    }, {
      "title" : "Pseudo-marginal Bayesian inference for Gaussian Processes",
      "author" : [ "M. Filippone", "M. Girolami" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Filippone and Girolami,? \\Q2014\\E",
      "shortCiteRegEx" : "Filippone and Girolami",
      "year" : 2014
    }, {
      "title" : "Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces",
      "author" : [ "K. Fukumizu", "F.R. Bach", "M.I. Jordan" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Fukumizu et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Fukumizu et al\\.",
      "year" : 2004
    }, {
      "title" : "Efficient Metropolis jumping rules",
      "author" : [ "A. Gelman", "G.O. Roberts", "W.R. Gilks" ],
      "venue" : "In Bayesian statistics,",
      "citeRegEx" : "Gelman et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Gelman et al\\.",
      "year" : 1994
    }, {
      "title" : "Riemann manifold Langevin and Hamiltonian Monte Carlo methods",
      "author" : [ "M. Girolami", "B. Calderhead" ],
      "venue" : "Journal of the Royal Statistical Society: Series B,",
      "citeRegEx" : "Girolami and Calderhead,? \\Q2011\\E",
      "shortCiteRegEx" : "Girolami and Calderhead",
      "year" : 2011
    }, {
      "title" : "A kernel method for the two-sample problem",
      "author" : [ "A. Gretton", "K. Borgwardt", "M. Rasch", "B. Schölkopf", "A. Smola" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Gretton et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2007
    }, {
      "title" : "Adaptive Proposal Distribution for Random Walk Metropolis Algorithm",
      "author" : [ "H. Haario", "E. Saksman", "J. Tamminen" ],
      "venue" : "Comput. Stat.,",
      "citeRegEx" : "Haario et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Haario et al\\.",
      "year" : 1999
    }, {
      "title" : "Slice sampling covariance hyperparameters of latent Gaussian models",
      "author" : [ "I. Murray", "R.P. Adams" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Murray and Adams,? \\Q2012\\E",
      "shortCiteRegEx" : "Murray and Adams",
      "year" : 2012
    }, {
      "title" : "Coupling and ergodicity of adaptive Markov chain Monte Carlo algorithms",
      "author" : [ "G.O. Roberts", "J.S. Rosenthal" ],
      "venue" : "J. Appl. Probab., 44(2):458–475,",
      "citeRegEx" : "Roberts and Rosenthal,? \\Q2007\\E",
      "shortCiteRegEx" : "Roberts and Rosenthal",
      "year" : 2007
    }, {
      "title" : "Langevin diffusions and Metropolis-Hastings algorithms",
      "author" : [ "G.O. Roberts", "O. Stramer" ],
      "venue" : "Methodol. Comput. Appl. Probab.,",
      "citeRegEx" : "Roberts and Stramer,? \\Q2003\\E",
      "shortCiteRegEx" : "Roberts and Stramer",
      "year" : 2003
    }, {
      "title" : "Integral transforms, reproducing kernels, and their applications",
      "author" : [ "S. Saitoh" ],
      "venue" : "Pitman Research Notes in Mathematics",
      "citeRegEx" : "Saitoh,? \\Q1997\\E",
      "shortCiteRegEx" : "Saitoh",
      "year" : 1997
    }, {
      "title" : "Nonlinear component analysis as a kernel eigenvalue problem",
      "author" : [ "B. Schölkopf", "A.J. Smola", "Müller", "K.-R" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Schölkopf et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 1998
    }, {
      "title" : "A Hilbert space embedding for distributions",
      "author" : [ "A. Smola", "A. Gretton", "L. Song", "B. Schölkopf" ],
      "venue" : "In Proceedings of the Conference on Algorithmic Learning Theory (ALT),",
      "citeRegEx" : "Smola et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Smola et al\\.",
      "year" : 2007
    }, {
      "title" : "Regularized principal manifolds",
      "author" : [ "A.J. Smola", "S. Mika", "B. Schölkopf", "R.C. Williamson" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Smola et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Smola et al\\.",
      "year" : 2001
    }, {
      "title" : "Hilbert space embeddings and metrics on probability measures",
      "author" : [ "B. Sriperumbudur", "A. Gretton", "K. Fukumizu", "G. Lanckriet", "B. Schölkopf" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Sriperumbudur et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sriperumbudur et al\\.",
      "year" : 2010
    }, {
      "title" : "Universality, characteristic kernels and RKHS embedding of measures",
      "author" : [ "B. Sriperumbudur", "K. Fukumizu", "G. Lanckriet" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Sriperumbudur et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sriperumbudur et al\\.",
      "year" : 2011
    }, {
      "title" : "Bayesian learning via stochastic gradient Langevin dynamics",
      "author" : [ "M. Welling", "Y.W. Teh" ],
      "venue" : "In Proc. of the 28th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Welling and Teh,? \\Q2011\\E",
      "shortCiteRegEx" : "Welling and Teh",
      "year" : 2011
    }, {
      "title" : "Bayesian classification with Gaussian processes",
      "author" : [ "C.K.I. Williams", "D. Barber" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Williams and Barber,? \\Q1998\\E",
      "shortCiteRegEx" : "Williams and Barber",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "In the present work, we develop an adaptive Metropolis-Hastings algorithm in which samples are mapped to a reproducing kernel Hilbert space, and the proposal distribution is chosen according to the covariance in this feature space (Schölkopf et al., 1998; Smola et al., 2001).",
      "startOffset" : 231,
      "endOffset" : 275
    }, {
      "referenceID" : 18,
      "context" : "In the present work, we develop an adaptive Metropolis-Hastings algorithm in which samples are mapped to a reproducing kernel Hilbert space, and the proposal distribution is chosen according to the covariance in this feature space (Schölkopf et al., 1998; Smola et al., 2001).",
      "startOffset" : 231,
      "endOffset" : 275
    }, {
      "referenceID" : 8,
      "context" : "Based on the chain history, they estimate the covariance of the target distribution and construct a Gaussian proposal centered at the current chain state, with a particular choice of the scaling factor from Gelman et al. (1996). More sophisticated schemes are presented by Andrieu & Thoms (2008), e.",
      "startOffset" : 207,
      "endOffset" : 228
    }, {
      "referenceID" : 8,
      "context" : "Based on the chain history, they estimate the covariance of the target distribution and construct a Gaussian proposal centered at the current chain state, with a particular choice of the scaling factor from Gelman et al. (1996). More sophisticated schemes are presented by Andrieu & Thoms (2008), e.",
      "startOffset" : 207,
      "endOffset" : 296
    }, {
      "referenceID" : 8,
      "context" : "38/ √ d is a fixed scaling factor from Gelman et al. (1996). This choice of scaling factor was shown to be optimal (in terms of efficiency measures) for the usual Metropolis algorithm.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "38/ √ d is a fixed scaling factor from Gelman et al. (1996). This choice of scaling factor was shown to be optimal (in terms of efficiency measures) for the usual Metropolis algorithm. While this optimality result does not hold for Adaptive Metropolis, it can nevertheless be used as a heuristic. Alternatively, the scale ν can also be adapted at each step as in Andrieu & Thoms (2008, Algorithm 4) to obtain the acceptance rate from Gelman et al. (1996), a = 0.",
      "startOffset" : 39,
      "endOffset" : 455
    }, {
      "referenceID" : 7,
      "context" : "This feature map or embedding of a single point can be extended to that of a probability measure P on X : its kernel embedding is an element μP ∈ Hk, given by μP = ́ k(·, x) dP (x) (Berlinet & Thomas-Agnan, 2004; Fukumizu et al., 2004; Smola et al., 2007).",
      "startOffset" : 181,
      "endOffset" : 255
    }, {
      "referenceID" : 17,
      "context" : "This feature map or embedding of a single point can be extended to that of a probability measure P on X : its kernel embedding is an element μP ∈ Hk, given by μP = ́ k(·, x) dP (x) (Berlinet & Thomas-Agnan, 2004; Fukumizu et al., 2004; Smola et al., 2007).",
      "startOffset" : 181,
      "endOffset" : 255
    }, {
      "referenceID" : 19,
      "context" : "Such kernels are said to be characteristic (Sriperumbudur et al., 2010; 2011), since each distribution is uniquely characterized by its embedding (in the same way that every probability distribution has a unique characteristic function).",
      "startOffset" : 43,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "Next, the covariance operator CP : Hk → Hk for a probability measure P is given by CP = ́ k(·, x) ⊗ k(·, x) dP (x) − μP ⊗ μP (Baker, 1973; Fukumizu et al., 2004), where for a, b, c ∈ Hk the tensor product is defined as (a ⊗ b)c = 〈b, c〉Hk a.",
      "startOffset" : 125,
      "endOffset" : 161
    }, {
      "referenceID" : 7,
      "context" : "Next, the covariance operator CP : Hk → Hk for a probability measure P is given by CP = ́ k(·, x) ⊗ k(·, x) dP (x) − μP ⊗ μP (Baker, 1973; Fukumizu et al., 2004), where for a, b, c ∈ Hk the tensor product is defined as (a ⊗ b)c = 〈b, c〉Hk a.",
      "startOffset" : 125,
      "endOffset" : 161
    }, {
      "referenceID" : 16,
      "context" : "Our approach is based on the idea that the nonlinear support of a target density may be learned using Kernel Principal Component Analysis (Kernel PCA) (Schölkopf et al., 1998; Smola et al., 2001), this being linear PCA on the empirical covariance operator in the RKHS, Cz = 1 n ∑n i=1 k(·, zi) ⊗ k(·, zi) − μz ⊗ μz, computed on the sample z defined above.",
      "startOffset" : 151,
      "endOffset" : 195
    }, {
      "referenceID" : 18,
      "context" : "Our approach is based on the idea that the nonlinear support of a target density may be learned using Kernel Principal Component Analysis (Kernel PCA) (Schölkopf et al., 1998; Smola et al., 2001), this being linear PCA on the empirical covariance operator in the RKHS, Cz = 1 n ∑n i=1 k(·, zi) ⊗ k(·, zi) − μz ⊗ μz, computed on the sample z defined above.",
      "startOffset" : 151,
      "endOffset" : 195
    }, {
      "referenceID" : 3,
      "context" : "In general, this is a non-convex minimization problem, and may be difficult to solve (Bakir et al., 2003).",
      "startOffset" : 85,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "Figure 1 plots g(x) and its gradients for several samples of β-coefficients, in the case where the underlying z-samples are from the two-dimensional nonlinear Banana target distribution of Haario et al. (1999). It can be seen that g may have multiple local minima, and that it varies most along the high-density regions of the Banana distribution.",
      "startOffset" : 189,
      "endOffset" : 210
    }, {
      "referenceID" : 11,
      "context" : "95% contours (red) of proposal distributions evaluated at a number of points, for the first two dimensions of the banana target of Haario et al. (1999). Underneath is the density heatmap, and the samples (blue) used to construct the proposals.",
      "startOffset" : 131,
      "endOffset" : 152
    }, {
      "referenceID" : 11,
      "context" : "In Haario et al. (2001), the proposal distribution is asymptotically symmetric due to the vanishing adaptation property.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : ",∇xxzn|x=y ] = 2Z, so the proposal is given by qz(·|y) = N (y, γI + 4νZHZ); thus, the proposal simply uses the scaled empirical covariance ZHZ just like standard Adaptive Metropolis (Haario et al., 1999), with an additional isotropic exploration component, and depends on y only through the mean.",
      "startOffset" : 182,
      "endOffset" : 203
    }, {
      "referenceID" : 11,
      "context" : "We consider the following nonlinear targets: (1) the posterior distribution of Gaussian Process (GP) classification hyperparameters (Filippone & Girolami, 2014) on the UCI glass dataset, and (2) the synthetic banana-shaped distribution of Haario et al. (1999) and a flower-shaped disribution concentrated on a circle with a periodic perturbation.",
      "startOffset" : 239,
      "endOffset" : 260
    }, {
      "referenceID" : 5,
      "context" : "Second, the MMD (Borgwardt et al., 2006; Gretton et al., 2007) was computed between each sampler output and the benchmark sample, using the polynomial kernel (1 + 〈θ, θ′〉); i.",
      "startOffset" : 16,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : "Second, the MMD (Borgwardt et al., 2006; Gretton et al., 2007) was computed between each sampler output and the benchmark sample, using the polynomial kernel (1 + 〈θ, θ′〉); i.",
      "startOffset" : 16,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "In Haario et al. (1999), the following family of nonlinear target distributions is considered.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : "We compute the following measures of performance (similarly as in Haario et al. (1999); Andrieu & Thoms (2008)) based on the chain after burn-in: average acceptance rate, norm of the empirical mean (the true mean is by construction zero for all targets), and the deviation of the empirical quantiles from the true quantiles.",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 11,
      "context" : "We compute the following measures of performance (similarly as in Haario et al. (1999); Andrieu & Thoms (2008)) based on the chain after burn-in: average acceptance rate, norm of the empirical mean (the true mean is by construction zero for all targets), and the deviation of the empirical quantiles from the true quantiles.",
      "startOffset" : 66,
      "endOffset" : 111
    } ],
    "year" : 2014,
    "abstractText" : null,
    "creator" : "LaTeX with hyperref package"
  }
}