{
  "name" : "1202.3890.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "PAC Bounds for Discounted MDPs",
    "authors" : [ "Tor Lattimore", "Marcus Hutter" ],
    "emails" : [ "tor.lattimore@anu.edu.au", "marcus.hutter@anu.edu.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 2.\n38 90\nv1 [\ncs .L\nG ]\n1 7\nContents 1 Introduction 2 2 Notation 2 3 Estimation 3 4 Upper Confidence Reinforcement Learning Algorithm 3 5 Upper PAC Bounds 5 6 Eliminating the Assumption 11 7 Lower PAC Bound 11 8 Conclusion 13 References 13 A Proof of Lower PAC Bound 14 B Technical Results 18 C Proof of Lemma 8 20 D Constants 23 E Table of Notation 24\nKeywords\nReinforcement learning; sample-complexity; exploration exploitation; PAC-MDP; Markov decision processes."
    }, {
      "heading" : "1 Introduction",
      "text" : "The goal of reinforcement learning is to construct algorithms that learn to act optimally, or nearly so, in unknown environments. In this paper we restrict our attention to finite state discounted MDPs with unknown transitions. The performance of reinforcement learning algorithms in this setting can be measured in a number of ways, for instance by using regret or PAC bounds (Kakade, 2003). We focus on the latter, which is a measure of the number of time-steps where an algorithm is not near-optimal with high probability. Many previous algorithms have been shown to be PAC with varying bounds (Kakade, 2003; Strehl and Littman, 2005; Strehl et al., 2006, 2009; Szita and Szepesvári, 2010; Auer, 2011).\nWe modify the Upper Confidence Reinforcement Learning (UCRL) algorithm of Auer et al. (2010); Auer (2011); Strehl and Littman (2008) and, under the assumption that there are at most two possible next-states for each state/action pair, prove a PAC bound of\nÕ\n( |S ×A|\nǫ2(1− γ)3 log\n1\nδ\n) .\nThis bound is an improvement1 on the previous best (Auer, 2011) and published best (Szita and Szepesvári, 2010), which are\nÕ\n( |S ×A|\nǫ2(1− γ)4 log\n1\nδ\n) and Õ ( |S ×A|\nǫ2(1− γ)6 log\n1\nδ\n)\nrespectively. The additional assumption is unfortunate and is probably unnecessary as discussed in Section 6.\nWe also present a matching (up to logarithmic factors) lower bound that is both larger and more general than the previous best given by Strehl et al. (2009). The class of MDPs used in the counter-example satisfy the assumption used in the upper bound."
    }, {
      "heading" : "2 Notation",
      "text" : "Unfortunately, we found it impossible to reduce the amount of notation and number of constants. While we have endeavoured to define everything before we use it, readers are encouraged to consult the tables of notation and constants found in the appendix.\nGeneral. N = {0, 1, 2, · · · } is the natural numbers. For the indicator function we write [[x = y]] = 1 if x = y and 0 if x 6= y. We use ∧ and ∨ for logical and/or respectively. If A is a set then |A| is its size and A∗ is the set of all finite ordered subsets. Unless otherwise mentioned, log represents the natural logarithm. For random variable X we write EX and VarX for its expectation and variance respectively. We make frequent use of the progression zi = 2 i − 2 for i ≥ 1. Define a set Z(a) := {zi : 1 ≤ i ≤ argmini {zi ≥ a}}.\nMarkov Decision Process. An MDP is a tuple M = (S,A, p, r, γ) where S and A are finite sets of states and actions respectively. r : S → [0, 1] is the reward function. p : S ×A × S → [0, 1] is the transition function and γ ∈ (0, 1) the discount rate. A\n1In this slightly restricted setting.\nstationary policy π is a function π : S → A mapping a state to an action. We write ps ′\ns,a as\nthe probability of moving from state s to s′ when taking action a and ps ′ s,π := p s′ s,π(s). The\nvalue of policy π in M and state s is V πM (s) := r(s) + γ ∑ s′∈S p s′ s,πV π M (s\n′). We view V πM either as a function V πM : S → R or a vector V π M ∈ R |S| and similarly ps,a ∈ [0, 1] |S| is a vector. The optimal policy of M is defined π∗M := argmaxπ V π M . Common MDPs are M , M̂ and M̃ , which represent the true MDP, the estimated MDP using empirical transition probabilities and a model. We write V := VM , V̂ := VM̂ and Ṽ := VM̃ for their values respectively. Similarly, π̂∗ := π∗\nM̂ and in general, variables with an MDP as a subscript\nwill be written with a hat, tilde or nothing as appropriate and the subscript omitted."
    }, {
      "heading" : "3 Estimation",
      "text" : "In the next section we will introduce the new algorithm, but first we give an intuitive introduction to the type of parameter estimation required to prove sample-complexity bounds for MDPs. The general idea is to use concentration inequalities to show the empiric estimate of a transition probability approaches the true probability exponentially fast in the number of samples gathered. There are a wide variety of concentration inequalities, each catering to a slightly different purpose. We improve on previous work by using Bernstein’s inequality, which takes variance into account (unlike Hoeffding). The following example demonstrates the need for Bernstein’s inequality when estimating the value functions of MDPs. It also gives insight into the workings of the proof in the next two sections.\ns0 r = 1\ns1 r = 0\n1− p\np\n1− q\nq Consider the Markov reward process on the right with two states where rewards are shown inside the states and transition probabilities on the edges. Note this is not an MDP because there are no actions. We are only concerned with how well the value can be approximated. Assume p > γ, q arbitrarily large (but not 1) and let p̂ be the empiric estimate of p and consider the error in our estimated value and the true value while in state s0. One can show that\n∣∣∣V (s0)− V̂ (s0) ∣∣∣ ≈ |p̂− p|\n(1− γ)2 . (1)\nTherefore if V − V̂ is to be estimated to within ǫ accuracy, we need |p̂ − p| < ǫ(1 − γ)2. Now suppose we bound |p̂−p| via a standard Hoeffding bound, then with high probability |p̂−p| . √ L/n where n is the number of visits to state s0 and L = log(1/δ). Therefore to obtain an error less than ǫ(1− γ)2 we need n > L ǫ2(1−γ)4 visits to state s0, which is already too many for a bound in terms of 1/(1 − γ)3. If Bernstein’s inequality is used instead, then |p̂ − p| . √\nLp(1− p)/n and so n > Lp(1−p)ǫ2(1−γ)4 is required, but Equation (1) depends\non p > γ. Therefore n > L ǫ2(1−γ)3 visits are sufficient. If p < γ then Equation (1) can be improved."
    }, {
      "heading" : "4 Upper Confidence Reinforcement Learning Algorithm",
      "text" : "UCRL is based on the optimism principle for solving the exploration/exploitation dilemma. It is model-based in the sense that at each time-step the algorithm acts according to a\nmodel (in this case an MDP, M̃) chosen from a model class. The idea is to choose the smallest model class guaranteed to contain the true model with high probability and act according to the most optimistic model within this class. With a good choice of model class this guarantees a policy that biases its exploration towards unknown states that may yield good rewards while avoiding states that are known to be bad. The approach has been successful in obtaining uniform sample complexity (or regret) bounds in various domains where the exploration/exploitation problem is an issue (Lai and Robbins, 1985; Agrawal, 1995; Auer et al., 2002; Strehl and Littman, 2005; Auer and Ortner, 2007; Auer et al., 2010; Auer, 2011).\nUnfortunately, to prove our new bound we needed to make an assumption about the transition probabilities of the true MDP. We do not believe this assumption is crucial, but it substantially eases the analysis by removing some dependencies in the more general problem. In Section 6 we present an approach to remove the assumption as well as some intuition into why this ought to be possible, but non-trivial.\nAssumption 1. The true unknown MDP, M , satisfies ps ′ s,a = 0 for all but two s ′ ∈ S denoted sa+, sa− ∈ S.2\nThe pseudo-code of UCRL can be found below, but first we define a knownness index, κ. If n is the number of times a state/action pair has been visited then κ(ι, n) is the knownness of that state/action pair at level ι. The knownness of a state increases with the number of visits, is bounded by |S| and is always a natural number. The reason for defining these now is that UCRL will only perform an update when the knownness index of some states would be changed by an update. Unfortunately, the definition below is unlikely to be very intuitive. A more thorough explanation of knownness is given in Section 5.\nDefinition 2 (Knownness). Define constants\nwmin := ǫ(1− γ)\n4|S| wι := 2\nιwmin ιmax :=\n⌈ 1\nlog 2 log\n8|S|\nǫ(1− γ)2\n⌉\nI := {0, 1, · · · , ιmax} K := Z(|S|).\nWe define the knownness index, κ : I × N → K by\nκ(ι, n) := max { z ∈ K : z ≤ n\nwιm\n} ,\nwhere m ∈ Õ (\n1 ǫ2(1−γ)2 log |S×A| δ\n) is defined in Appendix D.\nNote that the existence of the function ExtendedValueIteration is proven and an algorithm given by Strehl and Littman (2008).\n2Note that sa+ and sa− are dependent on (s, a) and are known to the algorithm.\nAlgorithm 1 UCRL\n1: t = 1, k = 1, n(s, a) = n(s, a, s′) = 0 for all s, a, s′ and s1 is the start state. 2: H := 11−γ log 8|S| ǫ(1−γ) , L1 := log 2 δ1 and δ1 := δ 2|S×A|2|K×I| 3: loop 4: p̂sa +\ns,a := n(s, a)/max {1, n(s, a, sa +)} and p̂sa\n− s,a := 1− p̂ sa+ s,a\n5: Mk := { M̃ : |p̃sa + s,a − p̂ sa+ s,a | ≤ ConfidenceInterval(p̃ sa+ s,a , n(s, a)), ∀(s, a) }\n6: M̃ = ExtendedValueIteration(Mk) 7: πk = π̃ ∗ 8: v(s, a) = v(s, a, s′) = 0 for all s, a, s′\n9: while κ(ι, n(s, a) + v(s, a)) = κ(ι, n(s, a)), ∀(s, a), ι ∈ I do 10: Act 11: Delay and Update 12: function Delay 13: for j = 1 → H do 14: Act 15: function Update 16: n(s, a) = n(s, a) + v(s, a) and n(s, a, s′) = n(s, a, s′) + v(s, a, s′) ∀s, a, s′ and k = k + 1\n17: function Act 18: at = πk(st) 19: st+1 ∼ pst,at ⊲ Sample from MDP 20: v(st, at) = v(st, at) + 1 and v(st, at, st+1) = v(st, at, st+1) + 1 and t = t+ 1 21: function ExtendedValueIteration(M) 22: return optimistic M̃ ∈ M such that V ∗ M̃ (s) ≥ V ∗ M̃ ′ (s) for all s ∈ S and M̃ ′ ∈ M. 23: function ConfidenceInterval(p, n)\n24: return min\n{√ 2L1p(1−p)\nn + 2L13n , √ L1 2n\n}"
    }, {
      "heading" : "5 Upper PAC Bounds",
      "text" : "We present two new PAC bounds. The first improves on all previous analysis, but relies on Assumption 1. The second is completely general, but gains an additional dependence on |S| leading to a PAC bound in terms of |S|2 and 1/(1− γ)3. This bound is worse than the previous best in terms of |S|, but better in terms 1/(1 − γ).\nTheorem 3. Let M be the true MDP satisfying Assumption 1. Let π be the actual (nonstationary) policy of UCRL (Algorithm 1), then V ∗(st)− V π(st) > ǫ for at most\nHUmax +HEmax ∈ Ø |S ×A|\nǫ2(1− γ)3 log\n|S ×A|\nδǫ(1 − γ) log2 |S| log2\n|S|\nǫ(1− γ) log2 log\n1\n1− γ\ntime-steps with probability at least 1− δ. (Umax and Emax are defined in Appendix D.)\nNote that although πk is stationary, the global policy of UCRL is non-stationary. Despite this, we will abuse notation by allowing ourselves to write V π(st), whereas really V π should depend on the entire history. Fortunately, when UCRL is not delaying, the policy π is nearly stationary in the sense that it will be so for the next H time-steps. This allows us to work almost entirely with stationary policies and so discard the cumbersome notation required for non-stationary policies.\nTheorem 4. Let M be the true MDP (possibly not satisfying Assumption 1) then there exists a policy π such that V ∗(st) − V π(st) > ǫ for at most |S| log 3 |S|(EmaxH + UmaxH) time-steps with probability at least 1− δ.\nThe proof of Theorem 4 is omitted, but follows easily by converting an arbitrary MDP with |S| states into a functionally equivalent MDP with O(|S|2) states that satisfies Assumption 1. This is done by adding a tree of 2|S| states for each state/action pair and rescaling γ.\nProof Overview. The proof of Theorem 3 borrows components from the work of Auer et al. (2010), Strehl and Littman (2008) and Szita and Szepesvári (2010).\n1. Bound the number of updates by |S ×A| log |S×A||K×I| , which follows from the algorithm and the definition of knownness. This bounds the number of delaying time-steps to Õ( 11−γ |S ×A| log |S×A| |K×I| ) time-steps, which is insignificant from the point of view of\nTheorem 3.\n2. Show that the true MDP remains in the model class Mk for all k.\n3. Use the optimism principle to show that if M ∈ Mk and V ∗ − V π > ǫ then |Ṽ πk −\nV πk | > ǫ/2. This key fact shows that if π is not nearly-optimal at some timestep t then the true value and model value of πk differ and so some information is (probably) gained by following this policy.\n4. The final component is to bound the number of time-steps when π is not nearlyoptimal.\nEpisodes and phases. UCRL operates in episodes, which are blocks of time-steps ending when update is called. The length of each episode is not fixed, instead, an episode ends when the knownness of a state changes. We often refer to time-step t and episode k and unless there is ambiguity we will not define k and just assume it is the episode in which t resides. A delay phase is the period of H contiguous time-steps where UCRL is in the function delay, which happens immediately before an update. An exploration phase is a period of H time-steps starting at t where t is not in a delay phase and where Ṽ πk(st) − V\nπk(st) ≥ ǫ/2. Exploration phases do note overlap. More formally, the starts of exploration phases, t1, t2, · · · , are defined inductively\nt1 := min { t : Ṽ πk(st)− V πk(st) ≥ ǫ/2 ∧ t is not in a delay phase } ti := min { t : t ≥ ti−1 +H ∧ Ṽ πk(st)− V πk(st) ≥ ǫ/2 ∧ t is not in a delay phase } .\nNote there need not, and with high probability will not, be infinitely many such ti. The exploration phases are only used in the analysis, they are not known to UCRL.\nWeights and variances. We define the weight3 of state/action pair (s, a) as follows.\nwπ(s, a|s′) := [[(s′, π(s′)) = (s, a)]] + γ ∑\ns′′\nps ′′ s′,π(s′)w π(s, a|s′′) wt(s) := w πk(s, πk(s)|st).\n3Also called the discounted future state distribution in Kakade (2003).\nAs usual, w̃ and ŵ are defined as above but with p replaced by p̃ and p̂ respectively. Think of wt(s) as the expected number of discounted visits to state/action pair (s, πk(s)) while following policy πk starting in state st. The important point is that this value is approximately equal to the expected number of visits to state/action pair (s, πk(s)) within the next H time-steps. We also define the local variance of the value function. These measure the variability of values while following policy π.\nσπ(s)2 := ps,π · V π2 − [ps,π · V π]2 σ̃π(s)2 := p̃s,π · Ṽ π2 − [p̃s,π · Ṽ π]2.\nThe active set. We will shortly see that states with small wt(s) cannot influence the differences in value functions. Thus we define an active set of states where wt(s) is not tiny. At each time-step t define the active set Xt by\nXt := { s : wt(s) > ǫ(1− γ)\n4|S| =: wmin\n} .\nKnownness. We now expand on the concept of knownness and explain its purpose. We write nt(s, a) for the value of n(s, a) at time-step t and nt(s) := nt(s, πk(s)) where k is the episode associated with time-step t. Let t be some non-delaying time-step and suppose s is active (s ∈ Xt). Now let ιt(s) := argminιwt(s) > wι and note that ιt(s) ∈ I. We define a partition of the active set Xt by\nKt(κ, ι) := {s ∈ Xt : ιt(s) = ι ∧ κt(ιt(s), nt(s)) = κ} .\nThe set Kt(κ, ι) represents a set of states that have comparable weights and visit counts. We will show that if |Kt(κ, ι)| ≤ κ for all κ, ι then the values Ṽ and V are reasonably close. This result forms a key stage in the proof of Theorem 3 because it shows that if π is not nearly-optimal at time-step t then there exists a Kt(κ, ι) that is quite large and where states have not been visited sufficiently. Furthermore, the weights wt(s) where s ∈ Kt(κ, ι) are large enough that some learning is expected to occur.\nAnalysis. The proof of Theorem 3 follows easily from three key lemmas.\nLemma 5. The following hold:\n1. The total number of updates is bounded by Umax := |S ×A| log |S×A| |K×I| . 2. If M ∈ Mk and t is not in a delay phase and V ∗(st)− V π(st) > ǫ then\nṼ πk(st)− V πk(s) > ǫ/2.\nLemma 6. M ∈ Mk for all k with probability at least 1− δ/2.\nLemma 7. The number of exploration phases is bounded by Emax with probability at least 1− δ/2.\nThe proofs of the lemmas are delayed while we apply them to prove Theorem 3.\nProof of Theorem 3. By Lemma 6, M ∈ Mk for all k with probability 1 − δ/2. By Lemma 7 we have that the number of exploration phases is bounded by Emax with probability 1− δ/2. Now if t is not in a delaying or exploration phase and M ∈ Mk then by Lemma 5, π is nearly-optimal. Finally note that the number of updates is bounded by\nUmax and so the number of time-steps in delaying phases is at most HUmax. Therefore UCRL is nearly-optimal for all but HUmax +HEmax time-steps with probability 1− δ.\nWe now turn our attention to proving Lemmas 5, 6 and 7. Of these, only Lemma 7 presents a substantial challenge.\nProof of Lemma 5. For part 1 we note that for ι ∈ I the knownness of a state/action pair at level ι satisfies κ ∈ K. Since the knownness index for each ι is non-decreasing and an update only occurs when an index is increased, the total number of updates is bounded by Umax := |S ×A||K × I|.\nThe proof of part 2 is closely related to the approach taken by Strehl and Littman (2008). Recall that M̃ is chosen optimistically by extended value iteration. This generates an MDP, M̃ , such that V ∗ M̃ (s) ≥ V ∗ M̃ ′ (s) for all M̃ ′ ∈ Mk. Since we have assumed M ∈ Mk we have that Ṽ πk(s) ≡ V ∗\nM̃ (s) ≥ V ∗M (s). Therefore Ṽ πk(st)−V π(st) > ǫ. Finally\nnote that t is a non-delaying time-step and so policy π will remain stationary and equal to πk for at least H time-steps. Using the definition of the horizon, H, we have that |V π(st)− V πk(st)| < ǫ/2. Therefore Ṽ πk(st)− V πk(st) > ǫ/2 as required.\nProof of Lemma 6. In the previous lemma we showed that there are at most Umax updates. Therefore we only need to check M ∈ Mk for each k up to Umax. Fix an (s, a) pair and apply the best of either Bernstein or Hoeffding inequalities to show that |p̂sa +\ns,a −p sa+ s,a | ≤ ConfidenceInterval(p̂ sa+ s,a −p sa+ s,a , n(s, a))) with probability 1− δ1. Setting\nδ1 := δ 2|S×A|Umax ≡ δ2|S×A|2|K×I| and applying the union bound completes the proof.\nWe are now ready to work on Lemma 7. The proof follows from two lemmas:\n1. If t is the start of an exploration phase then there exists a (κ, ι) such that |Kt(κ, ι)| > κ.\n2. If |Kt(κ, ι)| > κ for sufficiently many t then sufficient information is gained that some state/action pair must have an increase in knownness.\nLemma 8. Let t be a non-delaying time-step and assume M ∈ Mk. If |Kt(κ, ι)| ≤ κ for all κ, ι ∈ K then |Ṽ πk(st)− V πk(st)| ≤ ǫ/2.\nThe full proof is long, technical and has been relegated to Appendix B. We provide a sketch, but first we need some useful results about MDPs and the differences in value functions.\nLemma 9. Let M and M̃ be two Markov decision processes differing only in transition probabilities and π be a stationary policy then\nV π(st)− Ṽ π(st) = γ\n∑\ns\nwt(s)(ps,π − p̃s,π) · Ṽ π.\nProof sketch. Drop the π superscript and write V (st) = r(st) + γ ∑ st+1 p st+1 st,π V (st+1).\nThen V (st)− Ṽ (st) = γ[pst,π − p̃st,π] · Ṽ + γ ∑ st+1 p st+1 st,π [V (st+1)− Ṽ (st+1)]. The result is obtained by continuing to expand the second term of the right hand side.\nLemma 10. If M ∈ Mk at time-step t and Ṽ := Ṽ πk then\n|(ps,πk − p̃s,πk) · Ṽ | ≤\n√ 8L1σ̃πk(s)2\nnt(s) +\n2\n1− γ\n( L1\nnt(s)\n)3/4 +\n4L1 3nt(s)(1 − γ) ,\nwhere σ̃πk(s)2 := p̃s,a · Ṽ 2 − [ p̃s,a · Ṽ ]2 .\nThe idea is to note that M,M̃ are in Mk and apply the definition of the confidence intervals. The full proof is subsumed in the proof of the more general Lemma 33 in Appendix C. The following lemma bounds the expected total discounted local variance. Lemma 11. For any stationary π and M̃ , ∑\ns∈S w̃t(s)σ̃ π(s)2 ≤ 1 γ2(1−γ)2 .\nSee the paper of Sobel (1982) for a proof.\nProof sketch of Lemma 8. For ease of notation we drop references to πk. We approximate w(s) ≈ w̃(s) and |(ps,πk − p̃s,πk) · Ṽ | . √ L1σ̃(s)2 n(s) . Using Lemma 9\n|Ṽ (st)− V (st)| ≡ ∣∣∣∣∣γ ∑\ns∈S\nwt(s)(ps,πk − p̃s,πk) · Ṽ ∣∣∣∣∣ . ∣∣∣∣∣ ∑\ns∈X\nwt(s)(ps,πk − p̃s,πk) · Ṽ ∣∣∣∣∣ (2)\n. ∑\ns∈X\nwt(s)\n√ L1σ̃(s)2\nn(s) .\n∑\nκ,ι∈K×I\n∑\ns∈K(κ,ι)\n√ L1w̃t(s)σ̃(s)2\nκm (3)\n≤ ∑\nκ,ι∈K×I\n√√√√L1|K(κ, ι)| κm ∑\ns∈K(κ,ι)\nw̃t(s)σ̃(s)2 ≤\n√ L1|K × I|\nmγ2(1− γ)2 , (4)\nwhere in Equation (2) we used Lemma 9 and the fact that states not in X are visited very infrequently. In Equation (3) we used the approximations for (p − p̃) · Ṽ , the definition of K(κ, ι) and the approximation w ≈ w̃. In Equation (4) we used the Cauchy-Schwartz inequality,4 the fact that κ ≥ |K(κ, ι)| and Lemma 11. Substituting m := 20L1|K×I||D| 2\nǫ2(1−γ)2+2/β\ncompletes the proof. The extra terms in m are needed to cover the errors in the approximations made here.\nThe full proof requires formalising the approximations made at the start of the sketch above. The second approximation is comparatively easy while the showing that w(s) ≈ w̃(s) requires substantial work.\nThe following lemmas are used to show that |Kt(κ, ι)| cannot be larger than κ for too many time-steps with high probability. Combined with Lemma 8 above this will be sufficient to bound the number of exploration phases. Let t be the start of an exploration phase and define νt(s) to be the number of visits to state s within the next H time-steps. Formally, νt(s) := ∑t+H−1 i=t [[st = s]].\nLemma 12. Let t be the start of an exploration phase and wt(s) ≥ wmin then Eνt(s) ≥ wt(s)/2.\n4|〈1, v〉| ≤ ‖1‖2 ‖v‖2.\nProof sketch. Use the definition of the horizon to show that wt(s) is not much larger than a bounded-horizon version. Compare Eνt(s, πt(s)) and the definition of wt(s).\nLemma 13. Let N be as in Appendix D. If |Kti(κ, ι)| > κ for 4N exploration phases t1, t2, · · · , t4N then ∑4N i=1 ∑ s∈Kti(κ,ι) νti(s, π(s)) ≥ Nκwι with probability at least 1− δ1.\nProof. As in the previous proof we drop π superscripts and denote Ki := Kti(κ, ι). Define\nνi := ∑\ns,a∈Ki\nνti(s) Eνi = ∑\ns∈Ki\nEνti(s).\nNow |Ki| > κ and so by Lemma 12 we have Eνi ≥ κwι/2. We now prepare to use Bernstein’s inequality. Let Xi = νi − Eνi, µ := 1 4N ∑4N i=1Eνi and σ 2 := 14N ∑4N\ni=1VarXi then\nP\n{ 4N∑\ni=1\nνi ≤ Nwικ\n} ≤ P { 4N∑\ni=1\nνi ≤ 4N∑\ni=1\nEνi/2\n}\n= P\n{ 4N∑\ni=1\n[νi −Eνi] ≤ − 4N∑\ni=1\nEνi/2\n} ≤ 2 exp ( −\n4Nµ2\n8σ2 + 16µ3(1−γ)\n) .\nSetting this equal to δ1 and solving for 4N gives\n4N ≥ 8σ2 + 16µ3(1−γ)\nµ2 log\n2\nδ1 =\n[ 8σ2\nµ2 +\n16\n3µ(1− γ)\n] log 2\nδ1 .\nNaively bounding σ2/µ2 ≤ 1/((1 − γ)µ) and noting that µ ≥ wmin/2 leads to\n4N ≥ 14|S ×A|\nǫ(1− γ)2 log\n2\nδ1 .\nSince 4N satisfies this, the result is complete.\nProof of Lemma 7. We proceed in two stages. First we bound the total number of useful visits before |K(κ, ι)| ≤ κ. We then show this number of visits occurs after Õ(m) exploration phases with high probability.\nBounding the number of useful visits. A visit to state/action pair (s, a) in timestep t is (κ, ι)-useful if κ(ι, nt(s, a)) = κ. Fixing a (κ, ι) we bound the number of (κ, ι)useful visits to state/action pair (s, a). Suppose t1 < t2 and κ(ι, nt1(s, a)) = κ and nt2(s, a) − nt1(s, a) ≥ mwι(2κ + 2) then κ(ι, nt3(s, a)) > κ for all t3 ≥ t2. Therefore for each (κ, ι) pair there at most 6|S ×A|mwικ visits that are (κ, ι)-useful.\nBounding the number of exploration phases. Let N := 6|S ×A|m and t be the start of an exploration phase. Therefore Ṽ πk(st) − V\nπk(st) > ǫ/2 and so by Lemma 8 there exists a (κ, ι) ∈ K such that |S| ≥ |K(κ, ι)| > κ. If |Kti(κ, ι)| > κ at the start of 4N exploration phases, t1, t2, · · · , t4N then by Lemma 13\nP    4N∑\ni=1\n∑\ns,a∈Kti(κ,ι)\nvti(s, a) ≤ Nwικ    ≤ δ1.\nTherefore by the union bound there are at most Emax := 4N |K × I| exploration phases with probability 1− δ1|K × I| ≡ 1− |K × I|\nδ 2|S×A|Umax > 1− δ/2."
    }, {
      "heading" : "6 Eliminating the Assumption",
      "text" : "The upper bound in the previous section could only be proven using Assumption 1. In this section we describe a possible approach to generalising the proof and why this may be non-trivial. In the work above we used the assumption to bound (ps,π − p̃s,π) · Ṽ\n∗ .√ L1σ̃π(s)2/n. A natural approach to generalising this comes from Bernstein’s inequality (Theorem 30). If V π ∈ R|S| is a value function independent of p̂ then Bernstein’s inequality can be used to show that (ps,π − p̂s,π) · V π . √ L1σπ(s)2/n. This suggests we adjust our model class by letting π := π̃∗ and changing the condition to (p̃s,π − p̂s,π) · Ṽ ∗ .√\nL1σ̃π(s)2/n. We might then bound (ps,π− p̃s,π) · Ṽ ∗ ≡ (ps,π− p̂s,π) · Ṽ ∗+(p̂s,π− p̃s,π) · Ṽ ∗. The right term is then bounded by the conditions on the model class and the left term can perhaps be bounded by noting that ps,π is the true probability distribution. Unfortunately, there are a few problems with this approach:\n1. Bounding (ps,π − p̂s,π) · Ṽ ∗ does not result in a bound in terms of σ̃π(s)2. This issue\ncan be solved by again applying Bernstein’s inequality to bound (ps,a − p̂s,a) · Ṽ ∗2 .\n2. The value Ṽ ∗ is not in general independent of p̂. This is because M̃ must be chosen to satisfy the conditions on (p̂s,π − p̃s,π) · Ṽ ∗, which depends on p̂. This dependence\nviolates the conditions of Bernstein’s inequality when trying to bound (ps,π−p̂s,π)·Ṽ ∗. The dependence is intuitively quite weak, but nevertheless presents problems for rigorous proof.\n3. The last problem is that extended value iteration is no longer a trivial operation (even granting infinite computation). The problem is that the condition (ps,a − p̂s,a) · V ∗\nis not local to (s, a), it also depends on the choice of ps′,a′ for (s ′, a′) ∈ S ×A. This complication is probably resolvable, but the formal demonstration of extended value iteration is no longer so easy.\nProgress. The first issue above can be solved, as remarked, by bounding (ps,a− p̂s,a) · Ṽ ∗2 using another Bernstein inequality. The problem here is that this condition must now be added to the definition of the model class. The second issue is non-trivial and we cannot claim to have made progress there. We did manage to show that extended value iteration can be extended to the case where the only constraints take the form (p̃s,π − p̂s,π) · Ṽ\n∗ .√ L1σ̃π(s)2/n. In this case it can be shown the existence of a globally optimistic MDP. Unfortunately if you add constraints on higher moments, (p̃s,π − p̂s,π) · Ṽ 2 then results become substantially more complex. Note that in the complete proof of Lemma 8 we used higher moments still, but this is not required. Lemma 8 can be proven using only bounds on (p̃ − p̂) · Ṽ ∗ and (p̃− p̂) · Ṽ ∗ 2 ."
    }, {
      "heading" : "7 Lower PAC Bound",
      "text" : "We now turn our attention to proving a matching lower bound. The approach is similar to that of Strehl et al. (2009), but we make two refinements to improve the bound to depend on 1/(1−γ)3 and remove the policy restrictions. The first is to add a delaying state where no information can be gained, but where an algorithm may still fail to be PAC. The second is more subtle and will be described in the proof.\nDefinition 14. A non-stationary policy is a function π : S∗ → A.\nTheorem 15. Let π be a (possibly non-stationary) policy depending on S,A, r, γ, ǫ and δ, then there exists a Markov decision process Mhard such that V ∗(st)− V π(st) > ǫ for at least N time-steps with probability at least δ where\nN := c1|S ×A|\nǫ2(1− γ)3 log c2 δ\nand c1, c2 > 0 are independent of the policy π as well as all inputs S,A, ǫ, δ, γ.\nThe proof can found in Appendix A, but we give the counter-example MDP and intuition.\nCounter Example. We prove Theorem 15 for a class of MDPs where S = {0, 1,⊕,⊖} and A = {1, 2, · · · , |A|}. The rewards and transitions for a single action are depicted in the diagram on the right where ǫ(a∗) = 16ǫ(1 − γ) for some a∗ ∈ A and ǫ(a) = 0 for all other actions. Some remarks:\n1. States ⊕ and ⊖ are almost completely absorbing and confer maximum/minimum rewards respectively. 2. The transitions are independent of actions for all states except state 1. From this state, actions lead uniformly to ⊕/⊖ except for one action, a∗, which has a slightly higher probability of transitioning to state ⊕. Thus a∗ is the optimal action in state 1. 3. State 0 has an absorption rate such that, on average, a policy will stay there for 1/(1−γ) time-steps.\nIntuition. The MDP above is very bandit-like in the sense that once a policy reaches state 1 it should choose the action most likely to lead to state ⊕ whereupon it will either be rewarded or punished (visit state ⊕ or ⊖). Eventually it will return to state 1 when the whole process repeats. This suggests a PAC-MDP algorithm can be used to learn the bandit with p(a) := p⊕1,a. We can then make use of a theorem of Mannor and Tsitsiklis (2004) on bandit sample-complexity to show that the number of times a∗ is not selected is at least\nÕ\n( 1\nǫ2(1− γ)2 log\n1\nδ\n) . (5)\nImproving the bound to depend on 1/(1−γ)3 is intuitively easy, but technically somewhat annoying. The idea is to consider the value differences in state 0 as well as state 1. State 0 has the following properties:\n1. The absorption rate is sufficiently large that any policy remains in state 0 for around 1/(1 − γ) time-steps.\n2. The absorption rate is sufficiently small that the difference in values due to bad actions planned in state 1 still matter while in state 0.\nWhile in state 0 an agent cannot make an error in the sense that V ∗(0) − Q∗(0, a) = 0 for all a. But we are measuring V ∗(0) − V π(0) and so an agent can be penalised if its policy upon reaching state 1 is to make an error. Suppose the agent is in state 0 at some time-step before moving to state 1 and making a mistake. On average it will stay in state 0 for roughly 1/(1− γ) time-steps during which time it will plan a mistake upon reaching state 1. Thus the bound in Equation (5) can be multiplied by 1/(1 − γ). The proof is harder because an agent need not plan to make a mistake in all future time-steps when reaching state 1 before eventually doing so in one time-step. Note that Strehl et al. (2009) proved their theorem for a specific class of policies while Theorem 15 holds for all policies."
    }, {
      "heading" : "8 Conclusion",
      "text" : "Summary. We presented matching upper and lower bounds on the number of time-steps when a reinforcement learning algorithm can be nearly-optimal with high probability. While the lower bound is completely general, the upper bound depends on the assumption that there are at most two next-states for each state/action pair. This assumption aside, the new upper bound improves on the previously best known bound of Auer (2011). If the assumption is dropped then the new proof can be used to construct an algorithm that is better than the bound of Auer (2011) in terms of 1/(1− γ), but worse in |S|. The lower bound, which comes without assumptions, improves on the work of Strehl et al. (2009) by being both larger and more general. The class of MDPs used for the counter-example do satisfy Assumption 1 and so the upper and lower bounds now match in this restricted case.\nRunning Time. We did not analyze the running time of our version of UCRL, but expect analysis similar to that of Strehl and Littman (2008) can be used to show that UCRL can be approximated to run in polynomial time with no cost to sample-complexity.\nAcknowledgements. Thanks to Peter Sunehag for his careful reading and useful suggestions."
    }, {
      "heading" : "A Proof of Lower PAC Bound",
      "text" : "The proof makes use of a simple form of bandit and Theorem 16, which lower bounds the sample-complexity of bandit algorithms. We need some new notation required for non-stationary policies and bandits.\nHistory Sequences. We write s1:t = s1, s2, · · · , st for the history sequence of length t. Histories can be concatenated, so s1:t⊕ = s1, s2, · · · , st,⊕ where ⊕ ∈ S.\nBandits. An A-armed bandit is a vector p : A → [0, 1]. A policy interacts with a bandit sequentially. In time-step t some arm at is played whereupon the policy receives reward 1 with probability p(a) and reward 0 otherwise. This is repeated over all timesteps. More formally, a bandit policy is a function π : {0, 1}∗ → A. The optimal arm is defined a∗ := argmaxa p(a). A policy dependent on ǫ, δ and A has sample-complexity T := T (A, ǫ, δ) if for all bandits the arm chosen on time-step T satisfies p(a∗)− p(aT ) ≤ ǫ with probability at least 1− δ.\nTheorem 16 (Mannor and Tsitsiklis, 2004). There exist positive constants c1, c2, ǫ0, and δ0, such that for every A ≥ 2, ǫ ∈ (0, ǫ0) and δ ∈ (0, δ0) there exists a bandit p ∈ [0, 1] A such that\nT (A, ǫ, δ) ≥ c1 |A|\nǫ2 log c2 δ\nwith probability at least δ.\nRemark 17. The bandit used in the proof of Theorem 16 satisfies p(a) = 12 for all a except a∗ which has p(a∗) := 12 + ǫ.\nWe now prepare to prove Theorem 15. For the remainder of this section let π be an arbitrary policy and Mhard be the MDP of Figure 2. As in previous work we write V π := V πMhard. The idea of the proof will be to use Theorem 16 to show that π cannot be approximately correct in state 1 too often. Then use this to show that while in state 0 before-hand it is also not approximately correct.\nDefinition 18. Let s1:∞ ∈ S ∞ be the sequence of states seen by policy π and for arbitrary history s1:t let\n∆(s1:t) := V ∗(s1:t)− V π(s1:t).\nLemma 19. If γ ∈ (0, 1), p := 1/(2 − γ) and q := 2− 1/γ then\np 1 4(1−γ) > 3/4 and ∞∑\nt=0\npt(1− p)γt = 1\n2 .\nProof sketch. Both results follow from the geometric series and easy calculus.\nThe following lemma lower-bounds ∆(s1:t) if sub-optimal action a 6= a ∗ is taken in state 1.\nLemma 20. Let s1:t be a history such that st = 1 and a := π(s1:t) 6= a ∗ then\n∆(s1:t) ≥ 8ǫ.\nProof. The result essentially follows from the definition of the value function.\n∆(s1:t) ≡ V ∗(s1:t)− V π(s1:t)\n= γ [ p⊕1,a∗V ∗(s1:t⊕) + p ⊖ 1,a∗V ∗(s1:t⊖) ] − γ [ p⊕1,aV π(s1:t⊕) + p ⊖ 1,aV π(s1:t⊖) ] = γ\n2 [V ∗(s1:t⊕)− V π(s1:t⊕) + V ∗(s1:t⊖)− V π(s1:t⊖)] + γǫ(a ∗)V ∗(s1:t⊕)\n≥ 8ǫ,\nwhere we used the definition of the value function and MDP, Mhard.\nWe now define time-intervals where the policy is in state 0. Recall we chose the absorption in this state such that the expected number of time-steps a policy remains there is approximately 1/(1− γ). We define the intervals starting when a policy arrives in state 0 and ending when it leaves to state 1.\nDefinition 21. Define t01 := 1 and\nt0i := min {t : t > ti−1 ∧ st = 0 ∧ st−1 6= 0} t 1 i := min { t− 1 : st = 1 ∧ t > t 0 i } .\nDefine the intervals Ii := [t 0 i , t 1 i ] ⊆ N. We call interval Ii the ith phase.\nNote the following facts:\n1. Since all transition probabilities are non-zero, t0i and t 1 i exist for all i ∈ N with\nprobability 1. 2. |Ii| is the number of time-steps spent in state 0 before moving to state 1. 3. The values |Ii| are independent of π and each other.\nDefinition 22. Suppose t ∈ N and st = 0 and define the weight of action a, wt(a) by\nwt(a) :=\n∞∑\nk=0\npk(1− p)γk[[π(s1:t0 k1) = a]].\nLemma 23. ∑\na∈A wt(a) = 1 2 for all t where st = 0.\nProof. We use Lemma 19.\n∑\na∈A\nwi(a) ≡ ∑\na∈A\n∞∑\nk=0\npk(1− p)γk[[π(s1:t0 k1) = a]]\n= ∞∑\nk=0\npk(1− p)γk = 1\n2\nas required.\nDefinition 24. Define random variables Ai and Xi by\nAi := [[|Ii| ≥ 1/[16(1 − γ)] ∧ ∑\na6=a∗\nwt0i (a) ≥ 1/4]] Xi := [[|Ii| ≥ 1/[4(1 − γ)]]]\nIntuitively, Xi is the event that the ith phase lasts at least 1/[4(1− γ)] time-steps. Ai is the event that the ith phase lasts at least 1/[16(1 − γ)] time-steps and the combined weight of sub-optimal actions at the start of a phase is at least 1/4. The following lemma shows that at least two thirds of all phases have Xi = 1 with high probability. Lemma 25. For all n ∈ N, P {∑n\ni=1Xi ≤ 2 3n\n} ≤ 2e−n/72.\nProof. Preparing to use Hoeffding’s bound,\nP {Xi = 1} := P {|Ii| ≥ 1/[4(1 − γ)]} = p 1/[4(1−γ)] > 3/4,\nwhere we used the definitions of Xi, Ii and Lemma 19. Therefore EXi > 3/4.\nP\n{ n∑\ni=1\nXi ≤ 2\n3 n\n} ≤ P { n∑\ni=1\nXi ≤ 1\n12 n+ nEXi\n} = P { n∑\ni=1\nXi −EXi ≤ 1\n12 n\n} ≤ 2e−n/72\nwhere we applied basic inequalities followed by Hoeffding’s bound.\nLemma 26. If γ > 34 and ∑ a6=a∗ wt(a) ≥ 1 4 then ∑ a6=a∗ wt+k(a) ≥ 1 8 for all t ∈ N and k satisfying 0 ≤ k ≤ 1/[16(1 − γ)].\nProof. Working from the definitions.\n1 4 ≤\n∑\na6=a∗\nwt0i (a) ≡\n∞∑\nj=0\npj(1− p)γj [[π(s1:t0i 0 j) 6= a∗]]\n=\nk−1∑\nj=0\npj(1− p)γj [[π(s1:t0i 0 j) 6= a∗]] + pkγk\n∑\na6=a∗\nwa(s1:t0i 0 k)\n≤ (1− p)\nk−1∑\nj=0\npjγj + pkγk ∑\na6=a∗\nwa(s1:t0i 0 k)\nRearranging, setting 0 ≤ k ≤ 1/[16(1 − γ)] and using the geometric series completes the proof.\nSo far, none of our results have been especially surprising. Lemma 25 shows that at least two thirds of all phases have length exceeding 1/[4(1 − γ)] with high probability. Lemma 26 shows that if at the start of a phase π assigns a high weight to the sub-optimal actions, then it does so throughout the entire phase. The following lemma is more fundamental. It shows that the number of phases where π assigns a high weight to the sub-optimal actions is of order 1\nǫ2(1−γ)2 log 1δ with high probability.\nLemma 27. Let N := c1A ǫ2(1−γ)2 log c2δ with constants as in Theorem 16 then\n∣∣∣∣∣∣   i : ∑\na6=a∗\nwt0i (a) >\n1 4 ∧ i < 2N + 1    ∣∣∣∣∣∣ > N\nwith probability at least δ.\nThe idea is similar to that in (Strehl et al., 2009). Assume a policy exists that doesn’t satisfy the condition above and then use it to learn the bandit defined by p(a) := p⊕1,a. Proof. Let p(a) := p⊕1,a be a bandit and use π to learn bandit p using Algorithm 2 below, which returns an action abest defined as\nabest := argmax a\n2N∑\ni=1\nāi, āi := argmax a′\nwt0i (a′)\nBy Theorem 16, the strategy in Algorithm 2 must fail with probability at least δ. Therefore with probability at least δ, abest 6= a\n∗. However abest is defined as the majority action of all the āi and so for at least N time-steps āi 6= a\n∗. Suppose wt0i (a) > 14 , then by Lemma\n23, ∑\na6=a∗ wt0i (a) < 1 4 and āi ≡ argmaxawt0i (a) = a ∗. This implies that with probability\nδ, for at least N time-steps ∑\na6=a∗ wt0i (a) > 14 as required.\nAlgorithm 2 Learn Bandit\nt = 1, st = 0, k = 0 loop\nat = π(s1:t) if st = 1 then\nr ∼ p(at) ⊲ sample from bandit if r = 1 then\nst+1 = ⊕ else\nst+1 = ⊖\nk = k + 1 if k = 2N then\nabest = argmaxa ∑2N\ni=1[[a = argmaxa′ wt0i (a′)]]\nexit else\nst+1 ∼ pst,at ⊲ sample from MDP\nProof of Theorem 15. Suppose Ai = 1 and 0 ≤ k ≤ 1/[16(1− γ)] then s1:t0i+k = s1:t0i\n0k\nand\n∆(s1:ti+k) =\n∞∑\nt=0\npt(1− p)γt∆(s1:ti+k0 t1) (6)\n≥\n∞∑\nt=0\npt(1− p)γt ∑\na6=a∗\n[[π(s1:t0i+k 0t1) = a]]8ǫ (7)\n≥ ∑\na6=a∗\nwt0i+k (a)8ǫ (8)\n≥ ǫ, (9)\nwhere Equation (6) follows from the definition of Mhard and the value function. Equation (7) by Lemma 20. Equation (8) by the definition of wti+k(a) and Equation (8) by Lemma 26. Thus for each i where Ai = 1, policy π makes at least 1/[16(1−γ)] ǫ-errors. The proof is completed by showing that Ai = 1 for at least N/6 time-steps with probability at least δ, which follows easily from Lemma 27 and Lemma 25.\nDependence on S is added trivially by chaining arbitrarily many such Markov decision processes together.\nRemark 28. Dependence on S log S can possibly be added by a similar technique used by Strehl et al. (2009), but details could be messy."
    }, {
      "heading" : "B Technical Results",
      "text" : "Theorem 29 (Hoeffding Inequality). Let X1, · · · ,Xn be independent [0, 1]-valued random variables with probability 1. Then\nP {∣∣∣∣∣ 1 n n∑\ni=1\nXi −EXi ∣∣∣∣∣ ≥ ǫ } ≤ 2e−2ǫ 2n.\nTheorem 30 (Bernstein’s Inequality (Bernstein, 1924)). Let X1, · · · ,Xn be independent real-valued random variables with zero mean and variance VarXi = σ 2 i . If |Xk| < c with probability one then\nP {∣∣∣∣∣ 1 n n∑\ni=1\nXi ∣∣∣∣∣ ≥ ǫ } ≤ 2e − ǫ 2n 2σ2+2cǫ/3 ,\nwhere σ2 := 1n ∑n i=1 σ 2 i .\nWe can use Hoeffding and Bernstein to bound the gaps |p− p̂| and |p̂− p̃| we now want to combine these together in a nice way to bound |p − p̃|.\nLemma 31. Let p, p̂, p̃ ∈ [0, 1] satisfy\n|p− p̂| ≤ min {CI1, CI2} ,\nwhere\nCI1 :=\n√ 2p(1− p)\nn log\n2 δ + 2 3n log 2 δ CI2 :=\n√ 1\n2n log\n2 δ .\nThen\n|p − p̃| ≤\n√ 8p̃(1− p̃)\nn log\n2 δ + 2\n( 1\nn log\n2\nδ\n)3 4\n+ 4\n3n log\n2\nδ\nProof. Using the first confidence interval\n|p − p̂| ≤\n√ 2p(1− p)\nn log\n2 δ + 2 3n log 2 δ\nAssume without loss of generality that 1 − p ≥ 1 − p̃ (the case where p ≥ p̃ is identical. Therefore\n|p− p̂| ≤\n√ 2p̃(1− p̃)\nn log\n2 δ +\n√ 2(p− p̃)(1− p̃)\nn log\n2 δ + 2 3n log 2 δ\n≤\n√ 2p̃(1− p̃)\nn log\n2 δ +\n√√√√4 √\n1 2n log 2 δ\nn log\n2 δ + 2 3n log 2 δ\n=\n√ 2p̃(1− p̃)\nn log\n2 δ + 8 1 4\n( 1\nn log\n2\nδ\n) 3 4\n+ 2\n3n log\n2 δ ,\nwhere we used the second confidence interval and algebra. Bounding |p̂ − p̃| by the first confidence interval leads to\n|p − p̃| ≤\n√ 8p̃(1− p̃)\nn log\n2 δ + 2\n( 1\nn log\n2\nδ\n)3 4\n+ 4\n3n log\n2\nδ\nas required."
    }, {
      "heading" : "C Proof of Lemma 8",
      "text" : "We need to define some higher “moments” of the value function. This is somewhat unfortunate as it complicates the proof, but may be unavoidable.\nDefinition 32. We define the space of bounded value/reward functions R by\nR(i) :=   v ∈ [ 0, ( 1 1− γ )i]|S|    ⊂ R |S|.\nLet π be some stationary policy. For rd ∈ R(d) define values V π d by the Bellman equations\nV πd (s) = rd(s) + γ ∑\ns′\nps ′ s,πV π d (s ′).\nAdditionally,\nσπd (s) 2 := ps,π · V π d 2 − [ps,π · V π d ] 2 .\nNote that Vd ∈ R(d+ 1) and σ 2 d ∈ R(2d + 2). Let r0 ∈ R(0) be the true reward function r0(s) := r(s) and define a recurrence by r2d+2(s) := σ π d (s) 2. We define r̃d, r̂d, Ṽ π d , V̂ π d and σ̃πd , σ̂ π d similarly but where all parameters have hat/tilde.\nThe following lemma generalises Lemma 10.\nLemma 33. Let M ∈ Mk at time-step t then\n|(ps,π − p̃s,π) · Ṽ π d | ≤\n√ 8L1σ̃πd (s) 2\nnt(s) + 2\n( L1\nnt(s)\n)3 4 1\n(1− γ)d+1 + 4L1 3nt(s)(1− γ)d+1\nProof. Drop references to π and let p := psa + s,π , p̃ := p̃ sa+ s,π and n := nt(s). SinceM,M̃ ∈ Mk then apply Lemma 31 to obtain\n|p− p̃| ≤\n√ 8L1p̃(1− p̃)\nn + 2 ( L1 n ) 3 4 + 4L1 3n\nAssume without loss of generality that Ṽd(sa +) ≥ Ṽd(sa −). Therefore we have\n|(ps,π − p̃s,π) · Ṽd| ≤\n√ 8L1p̃(1− p̃)\nn\n( Ṽd(sa +)− Ṽd(sa −) ) + 2 ( L1 n ) 3 4 1 (1− γ)d+1\n+ 4L1\n3n(1− γ)d+1 , (10)\nwhere we used Assumption 1 and the fact that Vd ∈ Rd+1. p̃(1− p̃) ( Ṽd(sa +)− Ṽd(sa −) )2 = p̃(1− p̃) ( Ṽd(sa +)2 + Ṽd(sa −)2 − 2Ṽd(sa +)Ṽd(sa −) )\n= p̃Ṽd(sa +)2 + (1− p̃)Ṽd(sa −)2 − ( p̃Ṽd(sa +) + (1− p̃)Ṽd(sa −) )2\n= σ̃d(s) 2.\nSubstituting into Equation (10) completes the proof.\nProof of Lemma 8. For ease of notation we drop π and t super/subscripts. Let\n∆d := ∣∣∣∣∣ ∑\ns∈S\n[w(s) − w̃(s)]rd(s) ∣∣∣∣∣ ≡ |Ṽd(st)− Vd(st)|.\nUsing Lemma 9\n∆d = γ ∣∣∣∣∣ ∑\ns∈S\nw(s)(ps − p̃s) · Ṽd ∣∣∣∣∣\n≤ ǫ\n4(1− γ)d + ∣∣∣∣∣ ∑\ns∈X\nw(s)(p − p̃) · Ṽd ∣∣∣∣∣\n≤ ǫ\n4(1− γ)d +Ad +Bd + Cd,\nwhere\nAd := ∑\ns∈X\nw(s) √ 8L1σ̃2d n(s) Bd := ∑\ns∈X\nw(s) 4L1\n3n(s)(1 − γ)d+1 Cd :=\n∑\ns∈X\nw(s)2 ( L1 n(s) )3/4 .\nThe expressions Bd and Cd are substantially easier to bound than Ad. First we give a naive bound on Ad, which we use later.\nAd ≤ ∑\ns∈X\n√ 8w(s)σ̃2d(s)L1\nn(s) ≡\n∑\nκ,ι∈K×I\n∑\ns∈K(κ,ι)\n√ 8w(s)σ̃2d(s)L1\nn(s) (11)\n≤ ∑\nκ,ι∈K×I\n√√√√8L1|K(κ, ι)| mκ ∑\ns∈K(κ,ι)\nw(s)σ̃2d(s) ≤ ∑\nκ,ι∈K×I\n√√√√8L1 m ∑\ns∈K(κ,ι)\nw(s)σ̃2d(s)\n(12)\n≤ √√√√8|K × I|L1 m ∑\nκ,ι∈K\n∑\ns∈K(κ,ι)\nw(s)σ̃2d(s) ≤\n√ 8|K × I|L1\nm\n∑\ns∈X\nw(s)σ̃2d(s) (13)\n≤ √ 8|K × I|L1 m(1− γ)2d+3 , (14)\nwhere in Equation (11) we used the definitions of Ad and K. In Equation (12) we applied Cauchy-Schwartz and the assumption that |K(κ)| ≤ κ. In Equation (13) we used Cauchy-Schwartz again and the definition of K. Finally we apply the trivial bound of∑\nw(s)σ̃2d(s) ≤ 1/(1 − γ) 2d+3. Unfortunately this bound is not sufficient for our needs. The solution is approximate w(s) by w̃(s) and use Lemma 11 to improve the last step\nabove.\nAd ≤\n√ 8|K × I|L1\nm\n∑\ns∈S\nw(s)σ̃2d(s) (15)\n≡\n√ 8|K × I|L1\nm\n∑\ns∈S\nw̃(s)σ̃2d(s) + 8|K × I|L1\nm\n∑\ns∈S\n(w(s)− w̃(s))σ̃2d(s) (16)\n≤ √ 8|K × I|L1 m(1− γ)2d+2 + 8|K × I|L1 m ∆2d+2, (17)\nwhere Equation (15) is as in the naive bound. Equation (16) is substituting w(s) for w̃(s) and Equation (16) uses the definition of ∆. Therefore\n∆d ≤ ǫ\n4(1− γ)d +Bd + Cd +\n√ 8L1|K × I|\nm\n[ 1\n(1− γ)2d+2\n] + √ 8|K × I|L1\nm ∆2d+2.\nExpanding the recurrence up to β leads to\n∆0 ≤ 8 ∑\nd∈D−{β}\n( L1|K × I|\nm\n)d/(d+2) [ ǫ 4(1− γ)d +Bd + Cd + √ L1|K × I| m [ 1 (1− γ)2d+2 ]]2/(d+2)\n+ 8\n( L1|K × I|\nm\n)β/(β+2) [ 2 √ L1|K × I|\nm(1− γ)2β+3 +Bβ + Cβ\n]2/(β+2) , (18)\nwhere we used the naive bound to control Aβ. The bounds on Bd and Cd are somewhat easier, and follow similar lines to the naive bound on Ad.\nBd ≡ ∑\ns∈X\nw(s) 4L1\n3n(s)(1 − γ)d+1 = 4L1 3(1 − γ)d+1\n∑\nκ,ι∈K\n|K(κ, ι)\nmκ ≤ 4|K × I|L1 3m(1− γ)d+1\nCd ≡ 2 ∑\ns∈X\nw(s) ( L1 n(s) ) 3 4 1 (1− γ)d+1 ≤\n2\n(1− γ)d+1+1/4\n( |K × I|L1\nm\n) 3 4\n.\nLetting m := 20L1|K×I||D| 2\nǫ2(1−γ)2+2/β completes the proof."
    }, {
      "heading" : "D Constants",
      "text" : "The proof of Theorem 3 uses many constants, which can be hard to keep track of. For convenience we list them below, including approximate upper/lower bounds as appropriate.\nConstant O/Ω\nιmax := ⌈ 1 log 2 log 8|S| ǫ(1−γ)2 ⌉ Ølog |S|ǫ(1−γ) β := ⌈\n1 2 log 2 log 1 1−γ ⌉ Ω ( log 11−γ )\n|D| := |Z(β)| Ølog log 11−γ\n|K| := |Z(|S|)| Ølog |S|\n|I| := ιmax + 1 Ølog |S|\nǫ(1−γ)\n|K × I| := |K||I| Ølog |S| log |S|ǫ(1−γ)\nH := 11−γ log 8|S| ǫ(1−γ) Ø 1 1−γ log |S| ǫ(1−γ)\nwmin := ǫ(1−γ) 4|S| Ω ( ǫ(1−γ) |S| )\nδ1 := δ 2|S×A|Umax Ø δ\n|S×A|2 log |S| log |S|\nǫ(1−γ)\nL1 := log 2 δ1\nØlog |S×A|δǫ(1−γ)\nm := 20L1|K×I||D| 2\nǫ2(1−γ)2+2/β Ø 1 ǫ2(1−γ)2 log |S×A|δǫ(1−γ) log |S| log |S| ǫ(1−γ) log 2 log 11−γ\nN := 6|S ×A|m Ø |S×A| ǫ2(1−γ)2 log |S×A|δǫ(1−γ) log |S| log |S| ǫ(1−γ) log 2 log 11−γ\nEmax := 4N |K × I| Ø |S×A| ǫ2(1−γ)2 log |S×A|δǫ(1−γ) log 2 |S| log2 |S|ǫ(1−γ) log 2 log 11−γ\nUmax := |S ×A||K × I| Ø|S ×A| log |S| log |S|\nǫ(1−γ)"
    }, {
      "heading" : "E Table of Notation",
      "text" : "S,A Finite sets of states and actions respectively.\nγ The discount fact. Satisfies γ ∈ (0, 1).\nǫ The required accuracy.\nδ The probability that an algorithm makes more mistakes than its sample-complexity.\nN The natural numbers, starting at 0.\nlog The natural logarithm.\n∧,∨ Logical and/or respectively.\nEX,VarX The expectation and variance of random variable X respectively.\nzi zi := 2 i − 2.\nZ(a) Defined as a set of all zi up to and including a. Formally Z(a) := {zi : i ≤ argmini {zi ≥ a}}. Contains approximately log a elements.\nπ A policy.\np The transition function, p : S × A × S → [0, 1]. We also write ps ′\ns,a := p(s, a, s ′) for the probability of transitioning to state s′\nfrom state s when taking action a. ps ′ s,π := p s′ s,π(s). ps,a ∈ [0, 1] |S| is the vector of transition probabilities.\np̂, p̃ Other transition probabilities, as above.\nr The reward function r : S → A.\nM The true MDP. M := (S,A, p, r, γ).\nM̂ The MDP with empirically estimated transition probabilities. M̂ := (S,A, p̂, r, γ).\nM̃ An MDP in the model class, M. M̃ := (S,A, p̃, r, γ).\nV πM The value function for policy π in MDP M . Can either be viewed as a function V πM : S → R or vector V π M ∈ R |S|.\nṼ π, V̂ π The values of policy π in MDPs M̃ and M̂ respectively.\nπ∗ ≡ π∗M The optimal policy in MDP M .\nπ̃∗ ≡ π∗ M̃\nThe optimal policy in M̃ .\nπ̂∗ ≡ π∗ M̂\nThe optimal policy in M̂ .\nπk The (stationary) policy at used in episode k.\nnt(s, a) The number of visits to state/action pair (s, a) at time-step t.\nnt(s, a, s ′) The number of visits to state s′ from state s when taking action\na at time-step.\nnt(s) The number of visits to state/action pair (s, πt(s)) at time-step t.\nvtk(s, a) If tk is the start of an exploration phase then this is the total number of visits to state (s, a) in that exploration phase.\nst, at The state and action in time-step t respectively.\nV πd A higher “moment” value function. See Definition 32. σπd (s) 2 The variance of Vd(s\n′) when taking action π(s) in state s′. Defined in Definition 32.\nL1 Defined as log(2/δ1).\nD Defined as Z(β).\nwt(s) The expected discounted number of visits to state s, πk(s) while following policy πk.\nXt The active set containing states s where w(s) ≥ wmin.\nK A set if indices, K := Z(|S|).\nI A set of indices, I := {0, 1, 2, · · · , ιmax}.\nKt(κ, ι) A set of states that have\nwt(s) ∈ [wι, 2wι) ∧ nt(s) ∈ m[κwι, (2κ + 2)wι).\nNote that ⋃\nκ,ιKt(κ, ι) contains all states with w(s) ≥ wmin."
    } ],
    "references" : [ {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Neural Information Processing Systems",
      "citeRegEx" : "Auer et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2007
    }, {
      "title" : "Near-optimal regret bounds for reinforcement",
      "author" : [ "Mach. Learn" ],
      "venue" : null,
      "citeRegEx" : "Learn.,? \\Q2002\\E",
      "shortCiteRegEx" : "Learn.",
      "year" : 2002
    }, {
      "title" : "On a modification of Chebyshev’s inequality and of the error formula of Laplace",
      "author" : [ "S. Bernstein" ],
      "venue" : "Mathématique des Annales Scientifiques des Institutions Savantes de l’Ukraine,",
      "citeRegEx" : "Bernstein.,? \\Q1924\\E",
      "shortCiteRegEx" : "Bernstein.",
      "year" : 1924
    }, {
      "title" : "On The Sample Complexity Of Reinforcement Learning",
      "author" : [ "S. Kakade" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Kakade.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kakade.",
      "year" : 2003
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T. Lai", "H. Robbins" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Lai and Robbins.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai and Robbins.",
      "year" : 1985
    }, {
      "title" : "The sample complexity of exploration in the multi-armed bandit problem",
      "author" : [ "S. Mannor", "J. Tsitsiklis" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Mannor and Tsitsiklis.,? \\Q2004\\E",
      "shortCiteRegEx" : "Mannor and Tsitsiklis.",
      "year" : 2004
    }, {
      "title" : "The variance of discounted Markov decision processes",
      "author" : [ "M. Sobel" ],
      "venue" : "Journal of Applied Probability,",
      "citeRegEx" : "Sobel.,? \\Q1982\\E",
      "shortCiteRegEx" : "Sobel.",
      "year" : 1982
    }, {
      "title" : "A theoretical analysis of model-based interval estimation",
      "author" : [ "A. Strehl", "M. Littman" ],
      "venue" : "In Proceedings of the 22nd international conference on Machine learning,",
      "citeRegEx" : "Strehl and Littman.,? \\Q2005\\E",
      "shortCiteRegEx" : "Strehl and Littman.",
      "year" : 2005
    }, {
      "title" : "An analysis of model-based interval estimation for Markov decision processes",
      "author" : [ "A. Strehl", "M. Littman" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Strehl and Littman.,? \\Q2008\\E",
      "shortCiteRegEx" : "Strehl and Littman.",
      "year" : 2008
    }, {
      "title" : "PAC model-free reinforcement learning",
      "author" : [ "A. Strehl", "L. Li", "E. Wiewiorac", "J. Langford", "M. Littman" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning,",
      "citeRegEx" : "Strehl et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Strehl et al\\.",
      "year" : 2006
    }, {
      "title" : "Reinforcement learning in finite MDPs: PAC analysis",
      "author" : [ "A. Strehl", "L. Li", "M. Littman" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Strehl et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Strehl et al\\.",
      "year" : 2009
    }, {
      "title" : "Model-based reinforcement learning with nearly tight exploration complexity bounds",
      "author" : [ "I. Szita", "C. Szepesvári" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Szita and Szepesvári.,? \\Q2010\\E",
      "shortCiteRegEx" : "Szita and Szepesvári.",
      "year" : 2010
    }, {
      "title" : "T (A, ǫ, δ) if for all bandits the arm chosen on time-step T satisfies p(a∗)− p(aT ) ≤ ǫ with probability at least 1− δ",
      "author" : [ ],
      "venue" : "(Mannor and Tsitsiklis,",
      "citeRegEx" : "T,? \\Q2004\\E",
      "shortCiteRegEx" : "T",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "The performance of reinforcement learning algorithms in this setting can be measured in a number of ways, for instance by using regret or PAC bounds (Kakade, 2003).",
      "startOffset" : 149,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : "Many previous algorithms have been shown to be PAC with varying bounds (Kakade, 2003; Strehl and Littman, 2005; Strehl et al., 2006, 2009; Szita and Szepesvári, 2010; Auer, 2011).",
      "startOffset" : 71,
      "endOffset" : 178
    }, {
      "referenceID" : 7,
      "context" : "Many previous algorithms have been shown to be PAC with varying bounds (Kakade, 2003; Strehl and Littman, 2005; Strehl et al., 2006, 2009; Szita and Szepesvári, 2010; Auer, 2011).",
      "startOffset" : 71,
      "endOffset" : 178
    }, {
      "referenceID" : 11,
      "context" : "Many previous algorithms have been shown to be PAC with varying bounds (Kakade, 2003; Strehl and Littman, 2005; Strehl et al., 2006, 2009; Szita and Szepesvári, 2010; Auer, 2011).",
      "startOffset" : 71,
      "endOffset" : 178
    }, {
      "referenceID" : 0,
      "context" : "We modify the Upper Confidence Reinforcement Learning (UCRL) algorithm of Auer et al. (2010); Auer (2011); Strehl and Littman (2008) and, under the assumption that there are at most two possible next-states for each state/action pair, prove a PAC bound of",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "We modify the Upper Confidence Reinforcement Learning (UCRL) algorithm of Auer et al. (2010); Auer (2011); Strehl and Littman (2008) and, under the assumption that there are at most two possible next-states for each state/action pair, prove a PAC bound of",
      "startOffset" : 74,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "We modify the Upper Confidence Reinforcement Learning (UCRL) algorithm of Auer et al. (2010); Auer (2011); Strehl and Littman (2008) and, under the assumption that there are at most two possible next-states for each state/action pair, prove a PAC bound of",
      "startOffset" : 74,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "This bound is an improvement1 on the previous best (Auer, 2011) and published best (Szita and Szepesvári, 2010), which are",
      "startOffset" : 83,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "We also present a matching (up to logarithmic factors) lower bound that is both larger and more general than the previous best given by Strehl et al. (2009). The class of MDPs used in the counter-example satisfy the assumption used in the upper bound.",
      "startOffset" : 136,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "The approach has been successful in obtaining uniform sample complexity (or regret) bounds in various domains where the exploration/exploitation problem is an issue (Lai and Robbins, 1985; Agrawal, 1995; Auer et al., 2002; Strehl and Littman, 2005; Auer and Ortner, 2007; Auer et al., 2010; Auer, 2011).",
      "startOffset" : 165,
      "endOffset" : 302
    }, {
      "referenceID" : 7,
      "context" : "The approach has been successful in obtaining uniform sample complexity (or regret) bounds in various domains where the exploration/exploitation problem is an issue (Lai and Robbins, 1985; Agrawal, 1995; Auer et al., 2002; Strehl and Littman, 2005; Auer and Ortner, 2007; Auer et al., 2010; Auer, 2011).",
      "startOffset" : 165,
      "endOffset" : 302
    }, {
      "referenceID" : 7,
      "context" : "Note that the existence of the function ExtendedValueIteration is proven and an algorithm given by Strehl and Littman (2008). Note that sa and sa are dependent on (s, a) and are known to the algorithm.",
      "startOffset" : 99,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "The proof of Theorem 3 borrows components from the work of Auer et al. (2010), Strehl and Littman (2008) and Szita and Szepesvári (2010).",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "The proof of Theorem 3 borrows components from the work of Auer et al. (2010), Strehl and Littman (2008) and Szita and Szepesvári (2010).",
      "startOffset" : 59,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "The proof of Theorem 3 borrows components from the work of Auer et al. (2010), Strehl and Littman (2008) and Szita and Szepesvári (2010). 1.",
      "startOffset" : 59,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "Also called the discounted future state distribution in Kakade (2003).",
      "startOffset" : 56,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "The proof of part 2 is closely related to the approach taken by Strehl and Littman (2008). Recall that M̃ is chosen optimistically by extended value iteration.",
      "startOffset" : 64,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "See the paper of Sobel (1982) for a proof.",
      "startOffset" : 17,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "The approach is similar to that of Strehl et al. (2009), but we make two refinements to improve the bound to depend on 1/(1−γ)3 and remove the policy restrictions.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "We can then make use of a theorem of Mannor and Tsitsiklis (2004) on bandit sample-complexity to show that the number of times a∗ is not selected is at least",
      "startOffset" : 37,
      "endOffset" : 66
    }, {
      "referenceID" : 9,
      "context" : "Note that Strehl et al. (2009) proved their theorem for a specific class of policies while Theorem 15 holds for all policies.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : "This assumption aside, the new upper bound improves on the previously best known bound of Auer (2011). If the assumption is dropped then the new proof can be used to construct an algorithm that is better than the bound of Auer (2011) in terms of 1/(1− γ), but worse in |S|.",
      "startOffset" : 0,
      "endOffset" : 102
    }, {
      "referenceID" : 8,
      "context" : "This assumption aside, the new upper bound improves on the previously best known bound of Auer (2011). If the assumption is dropped then the new proof can be used to construct an algorithm that is better than the bound of Auer (2011) in terms of 1/(1− γ), but worse in |S|.",
      "startOffset" : 0,
      "endOffset" : 234
    }, {
      "referenceID" : 7,
      "context" : "The lower bound, which comes without assumptions, improves on the work of Strehl et al. (2009) by being both larger and more general.",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 7,
      "context" : "We did not analyze the running time of our version of UCRL, but expect analysis similar to that of Strehl and Littman (2008) can be used to show that UCRL can be approximated to run in polynomial time with no cost to sample-complexity.",
      "startOffset" : 99,
      "endOffset" : 125
    } ],
    "year" : 2012,
    "abstractText" : "We study upper and lower bounds on the sample-complexity of learning nearoptimal behaviour in finite-state discounted Markov Decision Processes (MDPs). For the upper bound we make the assumption that each action leads to at most two possible next-states and prove a new bound for a UCRL-style algorithm on the number of time-steps when it is not Probably Approximately Correct (PAC). The new lower bound strengthens previous work by being both more general (it applies to all policies) and tighter. The upper and lower bounds match up to logarithmic factors.",
    "creator" : "LaTeX with hyperref package"
  }
}