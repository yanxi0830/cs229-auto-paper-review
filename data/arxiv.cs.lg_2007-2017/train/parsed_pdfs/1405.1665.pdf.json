{
  "name" : "1405.1665.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Lower Bound for High-Dimensional Statistical Learning Problem via Direct-Sum Theorem",
    "authors" : [ "Ankit Garg", "Tengyu Ma", "Huy L. Nguyễn" ],
    "emails" : [ "garg@cs.princeton.edu.", "tengyu@cs.princeton.edu.", "hlnguyen@cs.princeton.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n16 65\nv1 [\ncs .L\nG ]\n7 M\nay 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "The last decade has witnessed a tremendous growth in the amount data involved in machine learning tasks. In many cases, data volume has outgrown the capacity of a single machine and it is increasingly common that learning tasks are performed in a distributed fashion on many machines. Beside traditional aspects of computation such as the running time and memory usage, communication has emerged as an important resource and sometimes the bottleneck of the whole system. A lot of recent works in machine learning are devoted to understanding the amount of communication needed in distributed learning tasks [BBFM12, IPSV12b, IPSV12a, ZDJW13].\nIn this paper, we study the relation between the dimensionality and the communication complexity of statistical estimation problems. Most modern statistical problems are characterized by high dimensionality. Thus, it is natural to ask the following meta question:\nHow does the communication cost scale in the dimensionality? We study this question via the problem of estimating the mean θ of an unknown d dimensional normal distribution in the distributed setting. In this problem, the samples from the unknown distribution are distributed among m different machines. The goal is to estimate the mean θ at the optimal minimax rate while communicating as few bits as possible. We show that in this simplest setting, one really needs to deal with different dimensions individually.\nTheorem 1.1. [Informal] To estimate the mean of a d-dimensional Gaussian in the distributed setting with error R, one must pay Ω(d) times the minimum communication cost needed for estimating the mean of one dimensional Gaussian with error R/d.\n∗Department of Computer Science, Princeton University, email: garg@cs.princeton.edu. †Department of Computer Science, Princeton University, email: tengyu@cs.princeton.edu. ‡Department of Computer Science, Princeton University, email: hlnguyen@cs.princeton.edu.\nThe work [ZDJW13] showed a lower bound on the communication cost for this problem when d = 1. Our technique when applied to their theorem immediately yields a lower bound equal to d times the lower bound for the one dimension problem for any choice of d. See Theorem 3.3 for the precise statement.\nWe use tools from the recent development in communication complexity and information complexity. There has been a lot of work on the paradigm of studying communication complexity via the notion of information complexity [CSWY01, BYJKS04, BR11, BBCR13, BEO+13]. Information complexity can be thought of as a proxy for communication complexity that is especially accurate for solving multiple copies of the same problem simultaneously [BR11]. It has become a standard tool for proving so-called “direct-sum” results, namely the fact that the amount of resources required for solving d copies of a problem in parallel is equal to d times the amount required for one copy. In other words, there is no saving from solving many copies of the same problem in batch and the trivial solution of solving each of them separately is optimal. Our result can be viewed as a direct sum theorem for communication complexity for statistical estimation problems: the amount of communication needed for solving an estimation problem in d dimensions is equal to d times the amount of communication needed for the same problem in one dimension. The proof technique is directly inspired by the notion of conditional information complexity [BYJKS04], which was used to prove direct sum theorems and lower bounds for streaming algorithms. We believe this is a fruitful connection and can lead to more lower bounds in statistical machine learning.\nOur techniques. Consider the problem of estimating some parameter θ ∈ Rd of a d-dimensional distribution from samples. To prove a lower bound for the d dimensional problem using an existing lower bound for one dimensional problem, we demonstrate a reduction that uses the (hypothetical) protocol for d dimensions to construct a better protocol for the one dimensional problem. The new protocol for the one dimensional problem works as follows: the one dimensional problem is embedded in a random coordinate of the d dimensional problem and the rest of the coordinates are filled in independently according to the prior of θ. As the other coordinates are independent from the input, they are shared among the machines before the protocol starts. When the machines get the samples, they proceed to simulate the protocol for the d-dimensional problem.\nThe first question to ask is, what is this “simulation” protocol for the one dimensional problem good for. It has the same communication cost as the protocol for the d-dimensional problem, and that doesn’t seem to help. But it turns out, while the communication cost remains the same, the information cost goes down by a factor of d. The information is somehow smeared across all the dimensions. This is consistent with a general paradigm in mathematics, when certain discrete quantities are not that well behaved, but there continuous relaxations are, and are often used as a proxy to study the discrete quantities. Here we are using information cost as a proxy for communication cost. Using the “simulation” protocol, we prove that the information cost of the d-dimensional problem is d times the information cost of the one dimensional problem. Now using the fact that information cost is less than the communication cost, we get a lower bound for the communication cost of the d dimensional problem in terms of the information cost of the one dimensional problem. The work [ZDJW13] already showed a lower bound on the information cost of the one dimensional problem."
    }, {
      "heading" : "2 Notation and Setup",
      "text" : "Statistical parameter estimation Let P be a family of distributions over X . Let θ : P → Θ denote a function defined on P. We are given samples X1, . . . ,Xn from some P ∈ P, and are asked to estimate θ(P ). Let θ̂ : X n → Θ be such an estimator, and θ̂(X1, . . . ,Xn) is the corresponding estimate.\nDefine the squared loss R of the estimator to be\nR(θ̂, θ) = E θ̂,X,θ\n[\n‖θ̂(X1, . . . ,Xn)− θ(P )‖ 2 2\n]\nIn the high-dimensional case, let Pd := {~P = P1 × · · · × Pd : Pi ∈ P} be the family of product distributions over X d. Let ~θ : Pd → Θd ⊂ Rd be the d-dimensional function obtained by applying θ point-wise ~θ (P1 × · · · × Pd) = (θ(P1), . . . , θ(Pd)).\nThroughout this paper, we consider the case when P = {N (θ, σ2) : θ ∈ [−1, 1]} for some fixed\nσ. Therefore, in the high-dimensional case, Pd = {N (~θ , σ2Id) : ~θ ∈ [−1, 1] d}. We use ~̂θ to denote the d-dimensional estimator. For clarity, in this paper, we always use~· to indicate a vector in high dimensions. Multi-Machine setting: There aremmachines. Machine j receives n samples ~X(j,1), . . . , ~X(j,n) ∈ X d from the distribution ~P . The machines communicate via a publicly shown blackboard. When a machine writes a message on the blackboard, all other machines can see the content of the message. Note that this model captures both point-to-point communication as well as broadcast communication. Therefore, our lower bounds in this model apply to both the message passing setting and the broadcast setting. We denote the transcript of the communication as Y . A deterministic function ~̂θ is then applied to the transcript Y to get the estimation of the mean ~̂θ(Y ). Let letter j be reserved for index of the machine and k for the samples and letter i for the dimension. In other words, ~X (j,k) i is the ith-coordinate of kth sample of machine j. Private/public randomness: We allow the protocol to use both private and public randomness, which is crucial. The public randomness is used purely for convenience in the proof and is not counted toward the total communication because it can be shared among machines before the start of the protocol. Alternatively, because the protocol works well on average over all public randomness, there exists a fixing of the public randomness so that the protocol still works as well as the average. This particular fixing of the public randomness gives a protocol that uses no public randomness at all while performing just as well as the one with public randomness. On the other hand, the use of private randomness is extremely crucial. With a little bit of thought, one can be convinced that the machines can use private randomness to hide information from other machines in a protocol. Indeed, we will see that in the direct sum argument, the simulation protocol for one dimension, private randomness plays a very important role. Lets denote public and private randomness of the protocol by Rpub and Rpriv respectively.\nWe define the squared loss of a protocol Π by\nR\n(\n(Π, ~̂θ), ~θ\n)\n= E ~θ , ~X,Y,Rpub,Rpriv\n[‖~̂θ(Y )− ~θ ‖2]\nInformation cost: We define information cost IC(Π) of protocol Π as follows:\nIC(Π) = I( ~X ;Y | ~θ ,Rpub)\nPrivate randomness doesn’t explicitly appear in the definition of information cost but if affects it. Note that the information cost is a lower bound on the communication cost:\nIC(Π) = I( ~X ;Y | ~θ ) ≤ H(Y ) ≤ length of Y"
    }, {
      "heading" : "3 Distributed Statistical Learning",
      "text" : "We start by formally defining the our task and the mean-squared loss and information cost of a protocol.\nDefinition 1. We say a protocol and estimator pair (Π, ~̂θ) solves task T (d,m, n, σ2,Ddθ) with information cost C and mean-squared loss R, if for ~θ randomly chosen from Ddθ , m machines, each of which takes n samples from N (~θ , σ2Id) as input, can run the protocol Π and get transcript Y so that the followings are true:\nE[‖~̂θ(Y )− ~θ ‖ 2] = R (1)\nI( ~X ;Y | ~θ ) = C (2)\nTheorem 3.1. [Direct-sum Theorem] If (Π, ~̂θ) solves the task T (d,m, n, σ2,Vd) with information cost C and squared loss R, there exists (Π′, θ̂) that solves the task T (1,m, n, σ2,V) with information cost at most 4C/d and squared loss 4R/d.\nProof. For each i ∈ [d], we could define the following protocol Πi and estimator θ̂i induced by the d-dimensional estimator ~̂θ. Πi is described as Protocol 1.\nInputs : Machine j gets samples X(j,1), . . . ,X(j,n) distributed according to N (θ, σ2), where θ ∼ V.\n1. All machines publicly sample θ̆−i distributed according to V d−1.\n2. Machine j privately samples X̆ (j,1) −i , . . . , X̆ (j,n) −i distributed according to N (θ̆−i, σ 2Id−1). Let\nX̆(j,k) = (X̆ (j,k) 1 , . . . , X̆ (j,k) i−1 ,X (j,k), X̆ (j,k) i+1 , . . . , X̆ (j,k) d ).\n3. All machines run protocol Π on data X̆ and get transcript Yi. The estimator θ̂i is θ̂i(Yi) =\n~̂θ(Yi)i i.e. the i th coordinate of the d-dimensional estimator.\nProtocol 1: Πi\nThe role of private randomness can be crucially seen here. It is very important for the machines to privately get samples in coordinates other than i for the information cost to go down by a factor of d. Lets denote the private and public randomness of the protocol Πi as Rpriv and Rpub respectively. We prove that Πi does a good job in the average sense by the following two lemmas:\nLemma 1. If θ ∼ V and ~θ ∼ Vd, then\nd ∑\ni=1\nR ( (Πi, θ̂i), θ ) = R ( (Π, ~θ ), ~θ )\nProof. Note that\nR ( (Πi, θ̂i), θ )\n= E θ,X,Yi,Rpriv,Rpub\n[(θ̂i(Yi)− θ) 2]\n= E θ,X,Yi,Rpriv,Rpub\n[(~̂θ(Yi)i − θ) 2]\nHence\nd ∑\ni=1\nR ( (Πi, θ̂i), θ ) = d ∑\ni=1\nE θ,X,Yi,Rpriv,Rpub\n[(~̂θ(Yi)i − θ) 2]\n= E ~θ , ~X,Y\n[ d ∑\ni=1\n(~̂θ(Y )i − ~θ i) 2]\n= E ~θ , ~X,Y\n[‖~̂θ(Y )− ~θ ‖2]\nThe second equality follows from the fact that the joint distribution of θ,X, Yi, Rpriv, Rpub is the same as the distribution of ~θ , ~X, Y . Also the marginal distributions of ~θ are the same as the distribution of θ.\nLemma 2. If θ ∼ V and ~θ ∼ Vd, then\nd ∑\ni=1\nIC(Πi) ≤ IC(Π)\nProof. Recall under (Πi, θ̂i), machines prepare X̆, which has the same distribution as ~X in the problem T (d,m, n, σ2,Vd). Also the joint distribution of ~Xi, Y, ~θ is the same as the distribution of X,Yi, θ, θ̆−i. Therefore, we have that\nI( ~Xi;Y | ~θ ) = I(X;Yi | θ, θ̆−i)\nSince IC(Πi) = I(X;Yi | θ,Rpub) = I(X;Yi | θ, θ̆−i), we have that\nd ∑\ni=1\nIC(Πi) = d ∑\ni=1\nI(X;Yi | θ, θ̆−i)\n= d ∑\ni=1\nI( ~Xi;Y | ~θ )\nSince the distribution of ~X conditioned on ~θ isN (~θ , σ2Id), ~X1, . . . , ~Xd are independent conditioned on ~θ . Hence\nd ∑\ni=1\nI( ~Xi;Y | ~θ ) ≤ I( ~X ;Y | ~θ ) = IC(Π)\nThe inequality is true because of the following:\nI( ~X ;Y | ~θ ) = d ∑\ni=1\nI( ~Xi;Y | ~θ , ~X1, . . . , ~Xi−1)\n= d ∑\ni=1\n( H( ~Xi | ~θ , ~X1, . . . , ~Xi−1)−H( ~Xi | Y, ~θ , ~X1, . . . , ~Xi−1) )\n= d ∑\ni=1\n( H( ~Xi | ~θ )−H( ~Xi | Y, ~θ , ~X1, . . . , ~Xi−1) )\n≥ d ∑\ni=1\n( H( ~Xi | ~θ )−H( ~Xi | Y, ~θ ) )\n= d ∑\ni=1\nI( ~Xi;Y | ~θ )\nThe third equality is true because ~X1, . . . , ~Xd are independent conditioned on ~θ . The inequality follows from the fact that conditioning decreases entropy.\nBy Lemma 1 and Lemma 2 and a Markov argument, there exists an i ∈ {1, . . . , d} such that\nR ( (Πi, θ̂i), θ ) ≤ 4 d ·R ( (Π, ~θ ), ~θ )\nand\nIC(Πi) ≤ 4\nd · IC(Π)\nThen the pair (Π′, θ̂) = (Πi, θ̂i) solves the task T (1,m, n, σ 2,V) with information cost at most 4C/d and squared loss 4R/d.\nWe are going to apply the theorem above to the one-dimensional lower bound by [ZDJW13]. This theorem is not explicitly stated in the paper but is implicit in the proof of Theorem 1 in their paper. Also they do not mention this, but their techniques are general enough to prove lower bounds on the information cost for protocols with private randomness. Also in their case, the definition of information cost is a bit different. They do not condition on the prior of θ, but since in the one dimensional case, this prior is just over {±δ}, conditioning on it can reduce the mutual information by at most 1 bit.\nI(X;Y | θ,Rpub) ≥ I(X;Y |Rpub)− 1\nTheorem 3.2. [ZDJW13] Let V be the uniform distribution over {±δ}, where δ2 ≤ min ( 1, σ 2 log(m)\nn\n)\n.\nIf (Π, θ̂) solves the task T (1,m, n, σ2,V) with information cost C and squared loss R, then either C ≥ Ω ( σ2\nδ2n log(m)\n)\nor R ≥ δ2/10.\nThe corollary below directly follows from Theorem 3.2 and Theorem 3.1.\nCorollary 3.1. Let V be the uniform distribution over {±δ}, where δ2 ≤ min ( 1, σ 2 logm n ) . If (Π, θ̂) solves the task T (1,m, n, σ2,Vd) with information cost C and squared loss R, then either C ≥ Ω ( dσ2\nδ2n logm\n)\nor R ≥ dδ2/40.\nThis immediately proves the main theorem of the paper.\nTheorem 3.3. If (Π, θ̂) estimates the mean of N (~θ , σ2Id), where ~θ ∈ [−1, 1] d, with mean-squared loss R, and communication cost B. Then\nR ≥ Ω\n(\nmin\n{\nd2σ2\nnB logm ,\ndσ2\nn logm ,d\n})\nAs a corollary, to achieve the optimal mean-squared loss R = dσ 2\nmn for the case when data is on a\nsingle machine, the communication cost is at least B = Ω (\ndm logm\n)\nProof. Apply corollary 3.1 with the trivial bound C ≤ B. We divide into two cases depending on whether B ≥ 1 c · max ( dσ2 n logm , d log2 m ) or not, c > 1 is a constant to be specified later. If B ≥ 1 c ·max ( dσ2 n logm , d log2 m ) , choose δ2 = 1 c · dσ 2 nB logm . Then we have that δ 2 ≤ min ( 1, σ 2 logm n ) , and hence we can apply corollary 3.1. Also\nC ≤ B = 1\nc ·\ndσ2\nδ2n logm\nChoose c is such that this violates the lower bound on C in corrollary 3.1. Thus, we must have R ≥ dδ2/40 ≥ Ω ( d2σ2\nnB logm\n)\n. On the other hand, if B ≤ 1 c · max\n(\ndσ2\nn logm , d log2 m\n)\n, choose δ2 =\ndσ2\nnmax\n(\ndσ2\nn log m , d\nlog2 m\n) logm . Again δ2 ≤ min\n(\n1, σ 2 logm n\n)\nand\nC ≤ B ≤ 1\nc ·max\n(\ndσ2\nn logm ,\nd\nlog2 m\n)\n= 1\nc ·\ndσ2\nδ2n logm\nHence R ≥ dδ2/40 ≥ Ω ( min { dσ2 n logm , d }) . Combining the two cases, we get\nR ≥ Ω\n(\nmin\n{\nd2σ2\nnB logm ,\ndσ2\nn logm ,d\n})"
    } ],
    "references" : [ {
      "title" : "How to compress interactive communication",
      "author" : [ "Boaz Barak", "Mark Braverman", "Xi Chen", "Anup Rao" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Barak et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Barak et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed learning, communication complexity and privacy",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Shai Fine", "Yishay Mansour" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2012
    }, {
      "title" : "A tight bound for set disjointness in the message-passing model",
      "author" : [ "Mark Braverman", "Faith Ellen", "Rotem Oshman", "Toniann Pitassi", "Vinod Vaikuntanathan" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Braverman et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Braverman et al\\.",
      "year" : 2013
    }, {
      "title" : "Information equals amortized communication",
      "author" : [ "Mark Braverman", "Anup Rao" ],
      "venue" : "In FOCS, pages 748–757,",
      "citeRegEx" : "Braverman and Rao.,? \\Q2011\\E",
      "shortCiteRegEx" : "Braverman and Rao.",
      "year" : 2011
    }, {
      "title" : "An information statistics approach to data stream and communication complexity",
      "author" : [ "Ziv Bar-Yossef", "T.S. Jayram", "Ravi Kumar", "D. Sivakumar" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "Bar.Yossef et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bar.Yossef et al\\.",
      "year" : 2004
    }, {
      "title" : "Informational complexity and the direct sum problem for simultaneous message complexity",
      "author" : [ "Amit Chakrabarti", "Yaoyun Shi", "Anthony Wirth", "Andrew Chi-Chih Yao" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Chakrabarti et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Chakrabarti et al\\.",
      "year" : 2001
    }, {
      "title" : "Efficient protocols for distributed classification and optimization",
      "author" : [ "Hal Daumé III", "Jeff M. Phillips", "Avishek Saha", "Suresh Venkatasubramanian" ],
      "venue" : "In ALT,",
      "citeRegEx" : "III et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "III et al\\.",
      "year" : 2012
    }, {
      "title" : "Protocols for learning classifiers on distributed data",
      "author" : [ "Hal Daumé III", "Jeff M. Phillips", "Avishek Saha", "Suresh Venkatasubramanian" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "III et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "III et al\\.",
      "year" : 2012
    }, {
      "title" : "Information-theoretic lower bounds for distributed statistical estimation with communication constraints",
      "author" : [ "Yuchen Zhang", "John C. Duchi", "Michael I. Jordan", "Martin J. Wainwright" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We explore the connection between dimensionality and communication cost in distributed learning problems. Specifically we study the problem of estimating the mean ~ θ of an unknown d dimensional normal distribution in the distributed setting. In this problem, the samples from the unknown distribution are distributed among m different machines. The goal is to estimate the mean ~ θ at the optimal minimax rate while communicating as few bits as possible. We show that in this simple setting, the communication cost scales linearly in the number of dimensions i.e. one needs to deal with different dimensions individually.",
    "creator" : "LaTeX with hyperref package"
  }
}