{
  "name" : "1302.2553.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning",
    "authors" : [ "Odalric-Ambrym Maillard", "Phuong Nguyen" ],
    "emails" : [ "odalricambrym.maillard@gmail.com", "nmphuong@cecs.anu.edu.au", "rortner@unileoben.ac.at", "daniil@ryabko.net" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 2.\n25 53\nv2 [\ncs .L\nG ]\n√ T ), with all con-\nstants reasonably small. This is optimal in T since O( √ T ) is the optimal regret in the setting of learning in a (single discrete) MDP."
    }, {
      "heading" : "1. Introduction",
      "text" : "In Reinforcement Learning (RL), an agent has to learn a task through interactions with the environment. The standard RL framework models the interaction of the agent and the environment as a finite-state Markov\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\ndecision process (MDP). Unfortunately, the real world is not (always) a finite-state MDP, and the learner often has to find a suitable state-representation model: a function that maps histories of actions, observations, and rewards provided by the environment into a finite space of states, in such a way that the resulting process on the state space is Markovian, reducing the problem to learning in a finite-state MDP. However, finding such a model is highly non-trivial. One can come up with several representation models, many of which may lead to non-Markovian dynamics. Testing which one has the MDP property one by one may be very costly or even impossible, as testing a statistical hypothesis requires a workable alternative assumption on the environment. This poses a challenging problem: find a generic algorithm that, given several staterepresentation models only some of which result in an MDP, gets (on average) at least as much reward as an optimal policy for any of the Markovian representations. Here we do not test the MDP property but propose to use models as long as they provide high enough rewards.\nMotivation. One can think of specific scenarios where the setting of several state-representation models is applicable. First, these models can be discretisations of a continuous state space. Second, they may be discretisations of the parameter space: this scenario has been recently considered (Ortner & Ryabko, 2012) for learning in a continuous-state MDP with Lipschitz continuous rewards and transition probabilities where the Lipschitz constants are unknown; the models are discretisations of the parameter space. A simple example is when the process is a second-order\nMarkov process with discrete observations: in this case a model that maps any history to the last two observations is a Markov model; a detailed illustration of such an example can be found, e.g., in Section 4 of (Hutter, 2009). More generally, one can try and extract some high-level discrete features from (continuous, high-dimensional) observations provided by the environment. For example, the observation is a video input capturing a game board, different maps attempt to extract the (discrete) state of the game, and we assume that at least one map is correct. Some popular classes of models are context trees (McCallum, 1996), which are used to capture short-term memories, or probabilistic deterministic finite automata (Vidal et al., 2005), a very general class of models that can capture both short-term and long-term memories. Since only some of the features may exhibit Markovian dynamics and/or be relevant, we want an algorithm able to exploit whatever is Markovian and relevant for learning. For more details and further examples we refer to (Maillard et al., 2011).\nPrevious work. This work falls under the framework of providing performance guarantees on the average reward of a considered algorithm. In this setting, the optimal regret of a learning algorithm in a finitestate MDP is O( √ T ). This is the regret of UCRL2 (Jaksch et al., 2010) and Regal.D (Bartlett & Tewari, 2009). Previous work on this problem in the RL literature includes (Kearns & Singh, 2002; Brafman & Tennenholtz, 2003; Strehl et al., 2006). Moreover, there is currently a big interest in finding practical state representations for the general RL problem where the environment’s states and model are both unknown, e.g. U-trees (McCallum, 1996), MC-AIXI-CTW (Veness et al., 2011), ΦMDP (Hutter, 2009), and PSRs (Singh et al., 2004). Another approach in which possible models are known but need not be MDPs was considered in (Ryabko& Hutter, 2008).\nFor the problem considered in this paper, (Maillard et al., 2011) recently introduced the BLB algorithm that, given a finite set Φ of staterepresentation models, achieves regret of order√ |Φ|T 2/3 (where |Φ| is the number of models) in respect to the optimal policy associated with any model that is Markovian. BLB is based on uniform exploration of all representation models and uses the performance guarantees of UCRL2 to control the amount of time spent on non-Markov models. It also makes use of some internal function in order to guess the MDP diameter (Jaksch et al., 2010) of a Markov model, which leads to an additive term in the regret bound that may be exponential in the true diameter,\nwhich means the order T 2/3 is only valid for possibly very large T .\nContribution. We propose a new algorithm called OMS (Optimistic Model Selection), that has regret of order √ |Φ|T , thus establishing performance that is optimal in terms of T , without suffering from an unfavorable additive term in the bound and without compromising the dependence on |Φ|. This demonstrates that taking into consideration several possibly non-Markovian representation models does not significantly degrade the performance of an algorithm, as compared to knowing in advance which model is the right one. The proposed algorithm is close in spirit to the BLB algorithm. However, instead of uniform exploration it uses the principle of “optimism” for model selection, choosing the model promising the best performance.\nOutline. Section 2 introduces the setting; Section 3 presents our algorithm OMS; its performance is analysed in Section 4; proofs are in Sections 5, and Section 6 concludes."
    }, {
      "heading" : "2. Setting",
      "text" : "Environment. For each time step t = 1, 2, . . ., let Ht := O × (A ×R ×O)t−1 be the set of histories up to time t, where O is the set of observations, A is a finite set of actions and R = [0, 1] is the set of possible rewards. We consider the problem of reinforcement learning when the learner interacts sequentially with some unknown environment: first some initial observation h1 = o1 ∈ H1 = O is provided to the learner, then at any time step t > 0, the learner chooses an action at ∈ A based on the current history ht ∈ Ht, then receives the immediate reward rt and the next observation ot+1 from the environment. Thus, ht+1 is the concatenation of ht with (at, rt, ot+1).\nState representation models. Let Φ be a set of state-representation models. A state-representation model φ ∈ Φ is a function from the set of histories H := ⋃t>1 Ht to a finite set of states Sφ. For a model φ, the state at step t under φ is denoted by st,φ := φ(ht) or simply st when φ is clear from context. For the sake of simplicity, we assume that Sφ∩Sφ′ = ∅ for φ 6= φ′. Further, we set S := ⋃φ∈Φ Sφ. A particular role will be played by state-representation models that induce a Markov decision process (MDP). An MDP is defined as a decision process in which at any discrete time t, given action at, the probability of immediate reward rt and next observation ot+1, given the past history ht, only depends on the current observation ot. That is, P (ot+1, rt|htat) = P (ot+1, rt|ot, at).\nObservations in this process are called states of the environment. We say that a state-representationmodel φ is a Markov model of the environment, if the process (st,φ, at, rt), t ∈ N is an MDP. This MDP is denoted as M(φ). We will always assume that such MDPs are weakly communicating, that is, for each pair of states x1, x2 there exists k ∈ N and a sequence of actions α1, . . . , αk ∈ A such that P (sk+1,φ = x2|s1,φ = x1, a1 = α1, . . . , ak = αk) > 0. It should be noted that there may be infinitely many state-representation models under which an environment is Markov.\nProblem description. Given a finite set Φ which includes at least one Markov model, we want to construct a strategy that performs as well as the algorithm that knows any Markov model φ ∈ Φ, including its rewards and transition probabilities. For that purpose we define for any Markov model φ ∈ Φ the regret of any strategy at time T , cf. (Jaksch et al., 2010; Bartlett & Tewari, 2009; Maillard et al., 2011), as\n∆(φ, T ) := Tρ⋆(φ) − T∑\nt=1\nrt ,\nwhere rt are the rewards received when following the proposed strategy and ρ⋆(φ) is the optimal average reward in φ, i.e., ρ⋆(φ) := ρ(M(φ), π⋆φ) := limT→∞ 1 T E [∑T t=1 rt(π ⋆ φ) ] where rt(π ⋆ φ) are the rewards received when following the optimal policy π⋆φ for φ. Note that for weakly communicating MDPs the optimal average reward indeed does not depend on the initial state. One could replace Tρ⋆(φ) with the expected sum of rewards obtained in T steps (following the optimal policy) at the price of an additional O( √ T ) term."
    }, {
      "heading" : "3. Algorithm",
      "text" : "High-level overview. The OMS algorithm we propose (shown in detail as Algorithm 1) proceeds in episodes k = 1, 2, . . ., each consisting of several runs j = 1, 2, . . .. In each run j of some episode k, starting at time t = tk,j , OMS chooses a policy πk,j applying the optimism in face of uncertainty principle twice. First, in line 6, OMS considers for each model φ ∈ Φ a set of admissible MDPs Mt,φ (defined via confidence intervals for the estimates so far), and computes a socalled optimistic MDP M+t (φ) ∈ Mt,φ and an associated optimal policy π+t (φ) on M + t (φ) such that the average reward ρ(M+t (φ), π + t (φ)) is maximized. Then (line 7) OMS chooses the model φk,j ∈ Φ which maximizes the average reward πk,j := π + t (φk,j) penalized by a term intuitively accounting for the “complexity” of the model, similar to the REGAL algorithm of (Bartlett & Tewari, 2009).\nAlgorithm 1 Optimistic Model Selection (OMS) Require: Set of models Φ0, parameter δ ∈ [0, 1]. 1: Set t := 1, k := 0, and Φ := Φ0. 2: while true do\n3: k := k + 1, j := 1, sameEpisode := true 4: while sameEpisode do 5: tk,j := t 6: ∀φ ∈ Φ, use EVI to compute optimistic MDP\nM+t (φ) ∈ Mt,φ and (near-)optimal policy π+t (φ) with approximate optimistic average reward ρ̂+tk,j (φ).\n7: Choose model φk,j ∈ Φ such that\nφk,j = argmax φ∈Φ\n{ ρ̂+tk,j (φ)−pen(φ; tk,j) } . (1)\n8: Define ρk,j := ρ̂ + tk,j (φk,j), πk,j := π + tk,j\n(φk,j). 9: sameRun := true.\n10: while sameRun do 11: Choose action at := πk,j(st), get reward rt, observe next state st+1 ∈ Sk,j := Sφk,j . 12: Set testFail := true iff the sum of the col-\nlected rewards so far from time tk,j is less than ℓk,jρk,j − lobk,j(t), (2) where ℓk,j := t− tk,j + 1.\n13: if testFail then 14: sameRun := false, sameEpisode := false 15: Φ := Φ \\ {φk,j} 16: if Φ = ∅ then Φ := Φ0 end if 17: else if vk(st, at) = Ntk(st, at) then 18: sameRun := false, sameEpisode := false 19: else if ℓk,j = 2 j then 20: sameRun := false, j := j + 1 21: end if 22: t := t+ 1 23: end while 24: end while\n25: end while\nThe policy πk,j is then executed until either (i) run j reaches the maximal length of 2j steps (line 19), (ii) episode k terminates when the number of visits in some state has been doubled (line 17), or (iii) the executed policy πk,j does not give sufficiently high average reward (line 12). Note that OMS assumes each model to be Markov, as long as it performs well. Otherwise the model is eliminated (line 15).\nDetails. We continue with some details of the algorithm. In the following, Sφ := |Sφ| denotes the number of states under model φ, S := |S| is the total number of states, and A := |A| is the number of actions. Further,\nδt := δ/36t 2 is the confidence parameter for time t.\nAdmissible models. First, the set of admissible MDPs Mt,φ the algorithm considers at time t for each model φ ∈ Φ is defined to contain all MDPs with state space Sφ and with rewards r and transition probabilities p satisfying\n∥∥p(·|s, a)− p̂t(·|s, a) ∥∥ 1 6\n√ 2 log(2SφSφAt/δt)\nNt(s,a) , (3)\n∣∣r(s, a) − r̂t(s, a) ∣∣ 6\n√ log(2SφAt/δt)\n2Nt(s,a) , (4)\nwhere p̂t(·|s, a) and r̂t(s, a) are respectively the empirical transition probabilities and mean rewards (at time t) for taking action a at state s, and Nt(s, a) is the number of times action a has been chosen in state s up to time t. (If a hasn’t been chosen in s so far, we set Nt(s, a) to 1.) It can be shown (cf. Appendix C.1 of Jaksch et al. (2010)) that the mean rewards r and the transition probabilities p of a Markovian staterepresentation φ satisfy (3) and (4) at time t for all s ∈ Sφ and a ∈ A, each with probability at least 1 − δt, making Markov models admissible with high probability.\nExtended Value Iteration. For computing a nearoptimal policy π+t (φ) and a corresponding optimistic MDP M+t (φ) ∈ Mt,φ (line 6), OMS applies for each φ ∈ Φ extended value iteration (EVI) (Jaksch et al., 2010) with precision parameter t−1/2. EVI computes optimistic approximate state values u+t,φ = (u + t,φ(s))s ∈ R Sφ just like ordinary value iteration (Puterman, 1994) with an additional optimization step for choosing the transition kernel maximizing the average reward. The (approximate) average reward ρ̂+t (φ) of π+t (φ) in M + t (φ) then is given by\nρ̂+t (φ) = min { r+t (s, π + t (φ, s))\n+ ∑\ns′\np+t (s ′|s)u+t,φ(s′)− u+t,φ(s), s ∈ Sφ\n} , (5)\nwhere r+t and p + t are the rewards and transition probabilities of M+t (φ) under π + t (φ). It can be shown (Jaksch et al., 2010) that ρ̂+t (φ) > ρ ⋆(φ) − 2/ √ t.\nPenalization term. At time t = tk,j , we define the empirical value span of the optimistic MDP M+t (φ) as sp(u+t,φ) := maxs∈Sφ u + t,φ(s)−mins∈Sφ u+t,φ(s), and the penalization term considered in (1) for each model φ is given by\npen(φ; t) := 2−j/2 c(φ; t) sp(u+t,φ)\n+ 2−j/2 c′(φ; t) + 2−j sp(u+t,φ),\nwhere the constants are given by\nc(φ; t) := 2 √ 2SφA log(2SφSφAt/δt) + 2 √ 2 log( 1δt ),\nc′(φ; t) := 2 √ 2SφA log(2SφAt/δt) .\nDeviation from the optimal reward. Let ℓk,j := t− tk,j +1, and vk,j(s, a) be the total number of times a has been played in s during run j in episode k (or until current time t if j is the current run). Similarly, we write vk(s, a) for the respective total number of visits during episode k. (Note that by the assumption Sφ∩Sφ′ = ∅ for φ 6= φ′, the state implicitly determines the respective model.) Then for the test (2) that decides whether the chosen model φk,j gives sufficiently high reward, we define the allowed deviation from the optimal average reward in the optimistic model for any t > tk,j in run j as\nlobk,j(t) := 2 ∑\ns∈Sk,j\n∑\na∈A\n√ 2vk,j(s, a) log\n( 2Sk,jAtk,j δtk,j )\n2sp+k,j\n∑\ns∈Sk,j\n∑\na∈A\n√ 2vk,j(s, a) log\n( 2Sk,jSk,jAtk,j δtk,j )\n+ 2sp+k,j √ 2ℓk,j log(1/δtk,j ) + sp + k,j , (6)\nwhere sp+k,j := sp(u + tk,j ,φk,j\n) and Sk,j := Sφk,j . Intuitively, the first two terms correspond to the estimation error of the transition kernel and the rewards, while the last one is due to stochasticity of the sampling process."
    }, {
      "heading" : "4. Main result",
      "text" : "We now provide the main result of this paper, an upper bound on the regret of our OMS strategy. The bound involves the diameter of a Markov model φ, D(φ), which is defined as the expected minimum time required to reach any state starting from any other state in the MDP M(φ) (Jaksch et al., 2010).\nTheorem 1 Let φ⋆ be an optimal model, i.e. φ⋆ ∈ argmax { ρ⋆(φ) |φ ∈ Φ, φ is Markovian } . Then the regret ∆(φ⋆, T ) of OMS (with parameter δ) w.r.t. φ⋆ after any T > SA steps is upper bounded by\n( 8D⋆S⋆ + 4 √ S⋆ )√ A log ( 48S⋆AT 3\nδ\n) log ( 2T SA )\n× (√( AS + |Φ| ) T + ( AS + |Φ| ) log ( 2T SA ))\n+ ( ρ⋆ +D⋆ )( AS + |Φ| ) log2 ( 2T SA )\nwith probability higher than 1− δ, where ρ⋆ := ρ⋆(φ⋆), S⋆ := Sφ⋆, and D ⋆ := D(φ⋆)."
    }, {
      "heading" : "In particular, if for all φ ∈ Φ, Sφ 6 B, then S 6 B|Φ| and hence with high probability",
      "text" : "∆(φ⋆, T ) = Õ ( D⋆AB3/2 √ |Φ|T ) .\nComparison with the BLB algorithm. Compared to the results obtained by (Maillard et al., 2011) the regret bound in Theorem 1 has improved dependence of T 1/2 (instead of T 2/3) with respect to the horizon (up to logarithmic factors). Moreover, the new bound avoids a possibly large constant for guessing the diameter of the MDP representation, as unlike BLB, the current algorithm does not need to know the diameter. These improvements were possible since unlike BLB (which uses uniform exploration over all models, and applies UCRL2 as a “black box”) we employ optimistic exploration of the models, and do a more indepth analysis of the “UCRL2 part” of our algorithm.\nOn the other hand, we lose in lesser parameters: the multiplicative term in the new bound is S⋆A √ S 6 S⋆A √ |Φ|B (assuming that all representations induce a model with no more than Sφ 6 B states), whereas the corresponding factor in the bound of (Maillard et al., 2011) is S⋆ √ A|Φ|. Thus, we currently lose a factor √ AB. Improving on the dependency on the state spaces is an interesting question: one may note that the algorithm actually only chooses models not much more complex (in terms of the diameter and the state space) than the best model. However, it is not easy to quantify this in terms of a concrete bound.\nAnother interesting question is how to reuse the information gained on one model for evaluation of the others. Indeed, if we are able to propagate information to all models, a log(|Φ|) dependency as opposed to the current √ |Φ| seems plausible. However, in the current formulation, a policy can be completely uninformative for the evaluation of other policies in other models. In general, this heavily depends on the internal structure of the models in Φ. If all models induce state spaces that have strictly no point in common, then it seems hard or impossible to improve on √ |Φ|.\nWe also note that it is possible to replace the diameter in Theorem 1 with the span of the optimal bias vector just as for the REGAL algorithm (Bartlett & Tewari, 2009) by suitably modifying the OMS algorithm. However, unlike UCRL2 and OMS for which computation of optimistic model and respective (near-)optimal policy can be performed by EVI, this modified algorithm (as REGAL) relies on finding the solution to a constraint optimization problem, efficient computation of which is still an open problem.\n5. Regret analysis of the OMS strategy\nThe proof of Theorem 1 is divided into two parts. In Section 5.1, we first show that with high probability all Markovian state-representation models will collect sufficiently high reward according to the test in (2). This also means that the regret of any Markov model is not too large. This in turn is used in Section 5.2 to show that also the optimistic model employed by OMS (which is not necessarily Markov) does not lose too much with respect to an optimal policy in an arbitrary Markov model. In our proof we use analysis similar to (Jaksch et al., 2010) and (Bartlett & Tewari, 2009).\n5.1. Markov models pass the test in (2)\nAssume that φk,j ∈ Φ is a Markovmodel. We are going to show that φk,j will pass the test on the collected rewards in (2) of the algorithm at any step t w.h.p.\nInitial decomposition. First note that at time t when the test is performed, we have∑\ns∈Sk,j\n∑ a∈A vk,j(s, a) = ℓk,j = t− tk,j + 1, so that\nℓk,jρk,j − t∑\nτ=tk,j\nrτ\n= ∑\ns∈Sk,j\n∑\na∈A\nvk,j(s, a) ( ρk,j − r̂tk,j :t(s, a) ) , (7)\nwhere r̂tk,j :t(s, a) is the empirical average reward collected for choosing a in s from time tk,j to the current time t in run j of episode k. Let r+k,j(s, a) be the optimistic rewards of the model M+tk,j (φk,j) under policy πk,j and P + k,j the respective optimistic transition matrix. Set vk,j := (vk,j(s, πk,j(s)))s ∈ RSk,j , and let u+k,j := (u + tk,j ,φk,j\n(s))s ∈ RSk,j be the state value vector given by EVI. By (5) and noting that vk,j(s, a) = 0 when a 6= πk,j(s) or s /∈ Sk,j , we get\nℓk,jρk,j − t∑\nτ=tk,j\nrτ = ∑\ns,a\nvk,j(s, a) ( ρ̂+k,j(φk,j)− r+k,j(s, a) )\n+ ∑\ns,a\nvk,j(s, a) ( r+k,j(s, a)− r̂tk,j :t(s, a) )\n6 v⊤k,j ( P+k,j − I ) u+k,j\n+ ∑\ns,a\nvk,j(s, a) ( r+k,j(s, a)− r̂tk,j :t(s, a) ) . (8)\nWe continue bounding each of the two terms on the right hand side of (8) separately.\nControl of the second term. Writing r(s, a) for the mean reward for choosing a in s (this is well-defined,\nsince we assume the model is Markov), we have\nr+k,j(s, a)− r̂tk,j :t(s, a) = ( r+k,j(s, a)− r̂tk,j (s, a) )\n+ ( r̂tk,j (s, a)− r(s, a) ) + ( r(s, a)− r̂tk,j :t(s, a) ) .\nThe terms of this decomposition are controlled. That is, using that M(φk,j) is an admissible model according to (4) with probability 1 − δtk,j (by applying the results of measure concentration in Appendix C.1 of (Jaksch et al., 2010) to the quantity r̂tk,j (s, a)), and the mere definition of r+k,j(s, a), and since Ntk(s, a) 6 Nt(s, a), we deduce that with probability higher than 1− δtk,j ,\n∑\ns,a\nvk,j(s, a) (( r+k,j(s, a)− r̂tk,j (s, a) )\n+ ( r̂tk,j (s, a)− r(s, a)\n))\n6 2 ∑\ns,a\nvk,j(s, a)√ 2Ntk(s, a)\n√ log ( 2Sk,jAtk,j\nδtk,j\n)\n6 ∑\ns,a\n√ 2vk,j(s, a) log ( 2Sk,jAtk,j\nδtk,j\n) . (9)\nOn the other hand, using again the results of measure concentration in Appendix C.1 of (Jaksch et al., 2010), and that vk,j(s, a) 6 Ntk(s, a) 6 tk,j , we deduce by a union bound over Sk,jAtk,j events that with probability higher than 1− δtk,j we get\n∑\ns,a\nvk,j(s, a) ( r(s, a)− r̂tk,j :t(s, a) )\n6 ∑\ns,a\nvk,j(s, a)√ 2vk,j(s, a)\n√ log ( 2Sk,jAtk,j\nδtk,j\n)\n6 ∑\ns,a\n√ 2vk,j(s, a) log ( 2Sk,jAtk,j\nδtk,j\n) . (10)\nControl of the first term. For the first term in (8), let us first notice that, since the rows of P+k,j sum to 1,( P+k,j − I ) u+k,j is invariant under a translation of the vector u+k,j . In particular, we can replace u + k,j with the quantity h+k,j , where\nh+k,j(s) := u + k,j(s)−min { u+k,j(s) | s ∈ Sk,j } .\nThen, we make use of the decomposition\nv⊤k,j ( P+k,j − I ) u+k,j = (11)\nv⊤k,j ( P+k,j −Pk,j ) h+k,j + v ⊤ k,j ( Pk,j − I ) h+k,j ,\nwhere Pk,j denotes the transition matrix corresponding to the MDP M(φk,j) under policy πk,j . Since\nboth matrices are close to the empirical transition matrix P̂tk,j at time tk,j , we can control the first term of this expression.\nFirst part of the first term. Indeed, since sp+k,j = ‖h+k,j‖∞, we have for the first term in (11), using the decomposition p+k,j(·|s) − pk,j(·|s) = ( p+k,j(·|s) −\np̂tk,j (·|s) ) + ( p̂tk,j (·|s)− pk,j(·|s) ) together with a concentration result and the definition of p+k,j , that with probability higher than 1− δtk,j v⊤k,j ( P+k,j −Pk,j ) h+k,j (12)\n= ∑\ns,a,s′\nvk,j(s, a) ( p+k,j(s ′|s)− pk,j(s′|s) ) h+k,j(s ′)\n6 ∑\ns,a\nvk,j(s, a) ∥∥p+k,j(·|s)− pk,j(·|s) ∥∥ 1 · ∥∥h+k,j ∥∥ ∞\n6 ∑\ns,a\n2 vk,j(s, a)\n√ 2 log(2Sk,jSk,jAtk,j/δtk,j )\nNtk (s,a)\n∥∥h+k,j ∥∥ ∞\n6 2 sp+k,j\n∑\ns,a\n√ 2vk,j(s, a) log ( 2Sk,jSk,jAtk,j\nδtk,j\n) .\nSecond part of the first term. The second term of (11) can be rewritten using a martingale difference sequence. That is, let es ∈ RSk,j be the unit vector with coordinates 0 for all s′ 6= s. Following (Jaksch et al., 2010) we set Xτ := ( p(·|sτ , aτ )− e⊤sτ+1 ) h+k,j and get\nv⊤k,j ( Pk,j − I ) h+k,j (13)\n=\nt∑\nτ=tk,j\n( p(·|sτ , aτ )− e⊤sτ ) h+k,j\n= ( e⊤st+1 − e⊤stk,j + t∑\nτ=tk,j\n( p(·|sτ , aτ )− e⊤sτ+1 )) h+k,j\n=\nt∑\nτ=tk,j\nXτ + h + k,j(st+1)− h+k,j(stk,j )\n=\nt∑\nτ=tk,j\nXτ + u + k,j(st+1)− u+k,j(stk,j ) .\nNow the sequence {Xτ}tk,j6τ6t is a martingale difference sequence with\n|Xτ | 6 ∥∥p(·|sτ , aτ )− e⊤sτ+1 ∥∥ 1 sp+k,j 6 2sp + k,j .\nThus, an application of Azuma-Hoeffding’s inequality (cf. Lemma 10 and its application in Jaksch et al. (2010)) to (13) yields\nv⊤k,j ( Pk,j − I ) h+k,j\n6 2sp+k,j √ 2ℓk,j log(1/δtk,j) + sp + k,j (14)\nwith probability higher than 1 − δtk,j . Together with (12) this concludes the control of the first term of (8).\nPutting all steps together. Combining (8), (9), (10), (11), (12), and (14), we deduce that at each time t of run j in episode k, any Markovian model φk,j passes the test in (2) with probability higher than 1− 4δtk,j . Further, it passes all the tests in run j with probability higher than 1− 4δtk,j2j ."
    }, {
      "heading" : "5.2. Regret analysis",
      "text" : "Next, let us consider a model φk,j ∈ Φ, not necessarily Markovian, that has been chosen at time tk,j . Let t+1 be the time when one of the three stopping conditions in the algorithm (lines 12, 17, and 19) is met. Thus OMS employs the model φk,j between tk,j and t + 1, until a new model is chosen after the step t + 1. Noting that rτ ∈ [0, 1] and that the total length of the run is (t + 1) − tk,j + 1 = ℓk,j + 1 we can bound the regret ∆k,j of run j in episode k by\n∆k,j := (ℓk,j + 1)ρ ⋆ −\nt+1∑\nτ=tk,j\nrτ\n6 ℓk,j ( ρ⋆ − ρk,j ) + ρ⋆ + ℓk,jρk,j − t∑\nτ=tk,j\nrτ .\nSince by assumption the test in (2) has been passed for all steps τ ∈ [tk,j , t], we have\n∆k,j 6 ℓk,j ( ρ⋆ − ρk,j ) + ρ⋆ + lobk,j(t), (15)\nand we continue bounding the terms of lobk,j(t).\nStopping criterion based on the visit counter. Since ∑\ns,a vk,j(s, a) = ℓk,j 6 2 j, by Cauchy-Schwarz\ninequality ∑\ns,a\n√ vk,j(s, a) 6 2 j/2 √ Sk,jA. Plugging\nthis into the definition (6) of lobk,j , we deduce from (15) that\n∆k,j 6 ℓk,j ( ρ⋆ − ρk,j ) + ρ⋆ (16)\n+sp+k,j + 2 j/2sp+k,jc(φk,j ; tk,j) + 2 j/2c′(φk,j ; tk,j) .\nSelection procedure with penalization. Now, by definition of the algorithm, for any optimal Markov model φ⋆ defined in the statement of Theorem 1, whenever M(φ⋆) is admissible, i.e. M(φ⋆) ∈ Mtk,j ,φ⋆ and was not eliminated during all runs before run j in episode k, we have ρk,j − pen(φk,j ; tk,j) > ρ̂+k,j(φ⋆)− pen(φ⋆; tk,j) > ρ ⋆ − pen(φ⋆; tk,j)− 2t−1/2k,j , or equiva-\nlently\nρ⋆ − ρk,j 6 pen(φ⋆; tk,j)− pen(φk,j ; tk,j) + 2t−1/2k,j 6 2−j/2c(φ⋆; tk,j) sp(u\n+ tk,j ,φ⋆ )\n+2−j/2c′(φ⋆; tk,j) + 2 −jsp(u+tk,j ,φ⋆) −2−j/2c(φk,j ; tk,j) sp+k,j −2−j/2c′(φk,j ; tk,j)− 2−jsp+k,j + 2t −1/2 k,j . (17)\nNoting that ℓk,j 6 2 j and recalling that when M(φ⋆) is admissible, the span of the corresponding optimistic model is less than the diameter of the true model, i.e. sp(u+tk,j ,φ⋆) 6 D ⋆, see (Jaksch et al., 2010), and we obtain from (16), (17), and a union bound that\n∆k,j 6 ρ ⋆ +D⋆ + 2j/2D⋆c(φ⋆; tk,j)\n+ 2j/2c′(φ⋆; tk,j) + 2 j+1t −1/2 k,j (18)\nwith probability higher than\n1− ∑\nk′,j′;tk′,j′<tk,j\n4δtk′,j′ 2 j′ − 2δtk,j . (19)\nThe sum in (19) comes from the event that φ⋆ passes all tests (and is admissible) for all runs in all episodes previous to time tk,j , and 2δtk,j comes from the event that φ⋆ is admissible at time tk,j . We conclude in the following by summing ∆k,j over all runs and episodes.\nSumming over runs and episodes. Let Jk be the total number of runs in episode k, and let KT be the total number of episodes up to time T . Noting that c(φ⋆; tk,j) 6 c(φ ⋆;T ) and c′(φ⋆; tk,j) 6 c ′(φ⋆;T ) as well as using that 2tk,j > 2 j (so that 2j+1t −1/2 kj 6\n2 √ 2 · 2j/2), summing (18) over all runs and episodes gives\n∆(φ⋆, T ) =\nKT∑\nk=1\nJk∑\nj=1\n∆k,j 6 ( ρ⋆ +D⋆ ) KT∑\nk=1\nJk (20)\n+ ( D⋆c(φ⋆;T ) + c′(φ⋆;T ) + 2 √ 2 ) KT∑\nk=1\nJk∑\nj=1\n2j/2,\nwith probability higher than 1 −∑KTk=1 ∑Jk j=1 4δtk,j2 j , where we used a union bound over all events considered in (19) for the control of all the ∆k,j terms, avoiding redundant counts (such as the admissibility of φ⋆ at time tk,j). Now, using the definition of δtk,j and the fact that 2tk,j > 2 j , we get that\n4δtk,j2 j =\n2jδ\n9t2k,j 6\n2jδ\n2tk,j(tk,j + 2j)\n= δ 2tk,j − δ 2(tk,j + 2j) 6\ntk,j+2 j −1∑\nt=tk,j\nδ\n2t2 ,\nwhere the last inequality follows by a series-integral comparison, using that t 7→ t−2 is a decreasing function. Thus, we deduce that the bound (20) is valid with probability at least 1−∑∞t=1 δ2t2 > 1−δ for all T , and it remains to bound the double sum ∑ k ∑ j 2 j/2.\nFrom the number of runs... First note that by definition of the total number of episodes KT we must have\nT >\nKT∑\nk=1\nJk−1∑\nj=1\n2j =\nKT∑\nk=1\n( 2Jk − 2 ) , (21)\nwhich implies also that we have the bound\nKT∑\nk=1\nJk∑\nj=1\n2j = 2\nKT∑\nk=1\n( 2Jk − 2 ) + 2KT 6 2T + 2KT .\nFurther, by Jensen’s inequality we get\nKT∑\nk=1\nJk−1∑\nj=1\n2j/2 6 √∑KT\nk=1 Jk √∑KT k=1 ∑Jk j=1 2 j\n6 √∑KT k=1 Jk √ 2T + 2KT . (22)\nNow, to bound the total number of runs ∑KT\nk=1 Jk, using Jensen’s inequality and (21), we deduce\nKT∑\nk=1\nJk =\nKT∑\nk=1\nlog2(2 Jk) 6 KT log2 ( 1 KT KT∑\nk=1\n2Jk )\n6 KT log2 ( T KT + 2 ) 6 KT log2 ( 2T KT ) , (23)\nand thus it remains to deal with KT .\n... to the number of episodes. First recall that an episode is terminated when either the number of visits in some state-action pair (s, a) has been doubled (line 17 of the algorithm) or when the test on the accumulated rewards has failed (line 12). We know that with probability at least 1− δ the optimal Markov model is not eliminated from Φ, while non-Markov models failing the test are deleted from Φ. Therefore, with probability 1 − δ the number of episodes terminated with a model failing the test is upper bounded by |Φ| − 1. Next, let us consider the number of episodes which are ended since the number of visits in some stateaction pair (s, a) has been doubled. Let K(s, a) be the number of episodes which ended after the number of visits in (s, a) has been doubled, and let T (s, a) be the number of steps in these episodes. As it may happen that in an episode the number of visits is doubled in more than one state-action pair, we assume that K(s, a) and T (s, a) count only the episodes/steps where (s, a) is the first state-action pair\nfor which this happens. It is easy to see that K(s, a) 6 1 + log2 T (s, a) = log2 2T (s, a) for T (s, a) > 0. Then the bound ∑ s∈S ∑ a∈A log2 2T (s, a) on the total number of these episodes is maximal under the constraint∑ s∈S ∑ a∈A T (s, a) = T when T (s, a) = T SA for all (s, a). This shows that the total number of episodes KT is upper bounded by\nKT 6 SA log2 ( 2T SA ) + |Φ| − 1 (24)\nwith probability 1− δ, provided that T > SA. Putting all steps together. Combining (20), (22) and (23) we get ∆(φ⋆, T ) 6( ρ⋆ + D⋆ ) KT log2 ( 2T KT ) + ( D⋆c(φ⋆;T ) + c′(φ⋆;T ) +\n2 √ 2 )√ 2KT log2 ( 2T KT )( T +KT ) . Hence, by (24) and the definition of c, c′, the regret of OMS is, with probability higher than 1− δ, bounded by ∆(φ⋆, T ) 6 ( ρ⋆ +D⋆ )( SA+ |Φ| ) log22( 2T SA )\n+ ( 2D⋆ √ 2S⋆A log ( 2S ⋆ 24S⋆AT 3\nδ\n) + 2D⋆ √ 2 log ( 24T 2\nδ\n)\n+2 √ 2S⋆A log ( 48S⋆AT 3\nδ\n) + 2 √ 2 )\n× log2 ( 2T SA )(√( SA+ |Φ| ) 2T+ ( SA+ |Φ| ) log2 ( 2T SA )) ,\nand we may conclude the proof with some minor simplifications."
    }, {
      "heading" : "6. Outlook",
      "text" : "The first natural question about the performance guarantees obtained is whether they are optimal. We know from the corresponding lower-bounds for learning MDPs (Jaksch et al., 2010) that the dependence on T we get for OMS is indeed optimal. Among other parameters, perhaps the most important one is the number of models |Φ|; here we conjecture that the√ |Φ| dependence we obtain is optimal, but this remains to be proven. Other parameters are the size of the action and state spaces for each model; here we lose with respect to the precursor BLB algorithm (see the remark after Theorem 1), and thus have room for improvement. It may be possible to obtain a better dependence for OMS at the expense of more sophisticated analysis. Note, however, that so far there are no known algorithms for learning even a single MDP that would have known optimal dependence on all these parameters. Another important direction for future research is infinite sets Φ of models; perhaps, countably infinite sets is the natural first step, with separable — in a suitable sense — continuously-parametrized general classes of models being a foreseeable extension. A problem with\nthe latter formulation is that one would need to formalize the notion of a model being close to a Markovian model and quantify the resulting regret."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the French National Research Agency (ANR-08-COSI-004 project EXPLORA), by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement 270327 (CompLACS) and 216886 (PASCAL2), the Nord-Pas-de-Calais Regional Council and FEDER through CPER 2007-2013, the Austrian Science Fund (FWF): J 3259-N13, and the Australian Research Council Discovery Project DP120100950, NICTA."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "We consider an agent interacting with an environment in a single stream of actions, observations, and rewards, with no reset. This process is not assumed to be a Markov Decision Process (MDP). Rather, the agent has several representations (mapping histories of past interactions to a discrete state space) of the environment with unknown dynamics, only some of which result in an MDP. The goal is to minimize the average regret criterion against an agent who knows an MDP representation giving the highest optimal reward, and acts optimally in it. Recent regret bounds for this setting are of order O(T ) with an additive term constant yet exponential in some characteristics of the optimal MDP. We propose an algorithm whose regret after T time steps is O( √ T ), with all constants reasonably small. This is optimal in T since O( √ T ) is the optimal regret in the setting of learning in a (single discrete) MDP.",
    "creator" : "LaTeX with hyperref package"
  }
}