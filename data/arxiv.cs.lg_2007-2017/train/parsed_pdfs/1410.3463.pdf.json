{
  "name" : "1410.3463.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Mining Block I/O Traces for Cache Preloading with Sparse Temporal Non-parametric Mixture of Multivariate Poisson",
    "authors" : [ "Lavanya Sita Tekumalla", "Chiranjib Bhattacharyya" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 0.\n34 63\nv1 [\ncs .O\nS] 1\n3 O\nct 2\n01 4\nExisting caching strategies, in the storage domain, though well suited to exploit short range spatiotemporal patterns, are unable to leverage long-range motifs for improving hitrates. Motivated by this, we investigate novel Bayesian non-parametric modeling(BNP) techniques for count vectors, to capture long range correlations for cache preloading, by mining Block I/O traces. Such traces comprise of a sequence of memory accesses that can be aggregated into highdimensional sparse correlated count vector sequences.\nWhile there are several state of the art BNP algorithms for clustering and their temporal extensions for prediction, there has been no work on exploring these for correlated count vectors. Our first contribution addresses this gap by proposing a DP based mixture model of Multivariate Poisson (DP-MMVP) and its temporal extension(HMM-DP-MMVP) that captures the full covariance structure of multivariate count data. However, modeling full covariance structure for count vectors is computationally expensive, particularly for high dimensional data. Hence, we exploit sparsity in our count vectors, and as our main contribution, introduce the Sparse DP mixture of multivariate Poisson(Sparse-DPMMVP), generalizing our DP-MMVP mixture model, also leading to more efficient inference. We then discuss a temporal extension to our model for cache preloading.\nWe take the first step towards mining historical data, to capture long range patterns in storage traces for cache preloading. Experimentally, we show a dramatic improvement in hitrates on benchmark traces and lay the groundwork for further research in storage domain to reduce latencies using data mining techniques to capture long range motifs.\n1 Introduction\nBayesian non-parametric modeling, while well explored for mixture modeling of categorical and real valued data, has not been explored for multivariate count data. We explore BNP models for sparse correlated count vectors to mine block I/O traces from enterprise storage servers for Cache Preloading.\n∗This work was done in collaboration with NetApp, Inc. †Indian Institute of Science\nExisting caching policies in systems domain, are either based on eviction strategies of removing the least relevant data from cache (Ex: Least Recently Used a.k.a LRU) or read ahead strategies for sequential access patterns. These strategies are well suited for certain types of workloads where nearby memory accesses are correlated in extremely short intervals of time, typically in milli-secs. However, often in real workloads, we find correlated memory accesses spanning long intervals of time (See fig 1), exhibiting no discernible correlations over short intervals of time (see fig 2).\nThere has been no prior work on analyzing trace data to learn long range access patterns for predicting future accesses. We explore caching alternatives to automatically learn long range spatio-temporal correlation structure by analyzing the trace using novel BNP techniques for count data, and exploit it to pro-actively preload data into cache and improve hitrates.\nCapturing long range access patterns in Trace Data: Block I/O traces comprise of a sequence of memory block access requests (often spanning millions per day). We are interested in mining such traces to capture spatio-temporal correlations arising from repetitive long range access patterns (see fig 1). For instance, every time a certain file is read, a similar sequence of accesses might be requested.\nWe approach this problem of capturing the longrange patterns by taking a more aggregated view of the data to understand longer range dependencies. We partition both the memory and time into discrete chunks and constructing histograms over memory bins for each time slice (spanning several seconds) to get a coarser view of the data. Hence, we aggregate the data into a sequence of count vectors, one for each time slice, where each component of the count vector records the count of memory access in a specific bin (a large region of memory blocks) in that time interval. Thus, a trace is transformed into a sequence of count vectors.\nThus, the components of count vector instances after aggregation are correlated within each instance since memory access requests are often characterized by spatial correlation, where adjacent regions of memory are likely to be accessed together. This leads to a rich covariance structure. Further, due to the long range temporal dependencies in access patterns (fig 1), the sequence of aggregated count vectors are also temporally correlated. Count vectors thus obtained by aggregating over time and space are also sparse, where only a small portion of memory is accessed in any time interval (fig 2). Hence a small subset of count vector dimensions have significant non zero values. Modeling such sparse correlated count vector sequences to understand their spatio-temporal structure remains unaddressed.\nModeling Sparse Correlated Count Vector Sequences: A common technique for modeling temporal correlations are Hidden Markov Models(HMMs) which are mixture model extensions for temporal data. Owing to high variability in access patterns inherent in storage traces, finite mixture models do not suffice for our application since the number of mixture components varies often based on the type of workload being modeled and the kind of access patterns. BNP techniques address this issue by automatically adjusting the number of mixture components based on complexity of data. Non-parametric clustering with Dirichlet Process(DP) mixtures and their temporal variants have been extensively studied over the past decade [15], [16]. However, to the best of our knowledge we are not aware of such models in the context of count data, particularly temporally correlated sparse count vectors.\nPoisson distribution is a natural prior for counts and the Multivariate Poisson(MVP) for correlated count vectors. However, owing to the structure of multivariate Poisson and its computational intractability [17] non parametric mixture modeling for multivariate count vectors has received less attention. Hence, we first bridge this gap by paralleling the development of DP based non-parametric mixture models and their temporal extensions for multivariate count data along the lines of those for Gaussians and multinomials .\nModeling the full covariance structure using the MVP is often computationally expensive. Hence, we further exploit the sparsity in data and introduce sparse mixture models for count vectors and their temporal extensions. We propose a sparse MVP Mixture modeling the covariance structure over a select subset of dimensions for each cluster. We are not aware of any prior work that models sparsity in count vectors.\nThe proposed predictive models showed dramatic hitrate improvement on several real world traces. At the same time, these count modeling techniques are of independent interest outside the caching problem as they can apply to a wide variety of settings such as text mining, where often counts are used.\nContributions: Our first contribution, is the DP based non-parametric mixture of Multivariate Poisson (DP-MMVP) and its temporal extensions (HMM-DPMMVP) which capture the full covariance structure of correlated count vectors. Our next contribution, is to exploit the sparsity in data, and proposing a novel technique for non parametric clustering of sparse high dimensional count vectors with the sparse DPmixture of Multivariate Poisson (Sparse-DP-MMVP). This methodology not only leads to a better fit for sparse multidimensional count data but is also computationally more tractable than modeling full covariance. We then discuss a temporal extension, Sparse-HMMDP-MMVP, for cache preloading. We are not aware of any prior work that addresses non-parametric modeling of sparse correlated count vectors.\nAs our final contribution, we take the first steps in outlining a framework for cache preloading to capture long range spatio-temporal dependencies in memory accesses. We perform experiments on real-world benchmark traces showing dramatic hitrate improvements. In particular, for the trace in Fig 1, our preloading yielded a 0.498 hitrate over 0.001 of baseline (without preloading), a 498X improvement (trace MT2: Tab 2).\n2 Related Work\nBNP for sparse correlated count vectors: Poisson distribution is a natural prior for count data. But the multivariate Poisson(MVP) [9] has seen limited use due to its computational intractability due to the\ncomplicated form of the joint probability function[17]. There has been relatively little work on MVP mixtures [8, 2, 13, 14]. On the important problem of designing MVP mixtures with an unknown number of components, [13] is the only reference we are aware of. The authors explore MVP mixture with an unknown number of components using a truncated Poisson prior for the number of mixture components, and perform uncollapsed RJMCMC inference. DP based models are a natural truncation free alternative that are well studied and amenable to hierarchical extensions [16, 4] for temporal modeling which are of immediate interest to the caching problem. To the best of our knowledge, there has been no work that examines a truncation free non-parametric approach with DP-based mixture modeling for MVP. Another modeling aspect we address is the sparsity of data. A full multivariate emission density over all components for each cluster may result in over-fitting due to excessive number of parameters introduced from unused components. We are not aware of any work on sparse MVP mixtures. There has been some work on sparse mixture models for Gaussian and multinomial densities [11] [18]. However they are specialized to the individual distributions and do not apply here. Finally, we investigate sparse MVP models for temporally correlated count vectors. We are not aware of any prior work that investigates mixture models for temporally correlated count vectors.\nCache Preloading: Preloading has been studied before [20] in the context of improving cache performance on enterprise storage servers for the problem of cache warm-up, of a cold cache by preloading the most recently accessed data, by analyzing block I/O traces. Our goal is however different, and more general, in that we are seeking to improve the cumulative hit rate by exploiting long ranging temporal dependencies even in the case of an already warmed up cache. They also serve as an excellent reference for state of the art caching related studies and present a detailed study of the properties of MSR traces. We have used the same MSR traces as our benchmark. Fine-grained prefetching tehniques to exploit short range correlations [6, 12], some specialized for sequential workload types [5] (SARC) have been investigated in the past. Our focus, however is to work with general non-sequential workloads, to capture long range access patterns, exploring prediction at larger timescales. Improving cache performance by predicting future accesses based on modeling file-system events was studied in [10]. They operate over NFS traces containing details of file system level events. This technique is not amenable for our setting, where the only data source is block I/O traces, with no file system level data."
    }, {
      "heading" : "3 A framework for Cache Preloading based on mining Block I/O traces",
      "text" : "In this section we briefly describe the caching problem and describe our framework for cache preloading.\n3.1 The Caching Problem:Application data is usually stored on a slower persistent storage medium like hard disk. A subset of this data is usually stored on cache, a faster storage medium. When an application makes an I/O request for a specific block, if the requested block is in cache, it is serviced from cache. This constitutes a cache hit with a low application latency (in microseconds). Else, in the event of a Cache miss, the requested block is first retrieved from hard disk into cache and then serviced from cache leading to much higher application latency (in milliseconds).\nThus, the application’s performance improvement is measured by hitrate = #cachehits#cachehits+#cachemisses .\n3.2 The Cache Preloading Strategy:Our strategy involves observing a part of the trace Dlr for some period of time and deriving a model, which we term as Learning Phase. We then use this model to keep predicting appropriate data to place in cache to improve hitrates in Operating Phase at the end of each time slice (ν secs) for the rest of the trace Dop.\nIn terms of the execution time of our algorithms, while the learning phase can take a few hours, the operational phase, is designed to run in time much less than the slice length of ν secs. In this paper we restrict ourselves to learning from a fixed initial portion of the trace. In practice, the learning phase can be repeated periodically, or even done on an online fashion.\nData Aggregation: As the goal is to improve hitrates by preloading data exploiting long range dependencies, we capture this by aggregating trace data into count vector sequences. We consider a partitioning of addressable memory (LBA Range) into M equal bins. In the learning phase, we divide the trace Dlr into Tlr fixed length time interval slices of length ν seconds each. Let A1, . . . , ATlr be the set of actual access requests in each interval of ν seconds. We now aggregate the trace into a sequence of Tlr count vectors X1, . . . XTlr ∈ Z\nM , each of M dimensions. Each count vector Xt is a histogram of accesses in At over M memory bins in the tth time slice of the trace spanning ν seconds.\n3.3 Learning Phase (Learning a latent variable model):The input to the learning phase is a set of sparse count vectors X1, . . . , XTlr ∈ Z\nM , correlated within and across instances obtained from a block I/O trace as described earlier. These count vectors can often be intrinsically grouped into cohesive clusters which arise as a result of long range access patterns (see Figure 1) that repeat over time albeit with some randomness.\nHence we would like to explore unsupervised learning techniques based on clustering for these count vectors, that capture temporal dependencies between count vector instances and the correlation within instances.\nHidden Markov Models(HMM), are a natural choice of predictive models for such temporal data. In a HMM, latent variables Zt ∈ {1, . . . ,K} are introduced that follow a markov chain. Each Xt is generated based on the choice of Zt, inducing a clustering of count vectors. In the learning phase, we learn the HMM parameters, denoted by θ.\nOwing to the variability of access patterns in trace data, a fixed value of K is not suitable for use in realistic scenarios motivating the use of non-parametric techniques of clustering. In section 4 we propose the HMMDP-MMVP, a temporal model for non-parametric clustering of correlated count vector sequences capturing their full covariance structure, followed by the SparseHMM-DP-MMVP in section 5, that exploits the sparsity in count vectors to better model the data, also leading to more efficient inference.\nAs an outcome of the learning phase, we have a HMM based model with appropriate parameters, that provides predictive ability to infer the next hidden state on observing a sequence of count vectors. However, since the final prediction required is that of memory accesses, we maintain a map from every value of hidden state k to the set of all raw access requests from various time slices during training that were assigned latent state k. H(k) = ∪{t|Zt=k}At, for ∀k."
    }, {
      "heading" : "3.4 The Operating Phase (Prediction for",
      "text" : "Preloading):Having observed {X1, . . . XTlr} aggregated from Dlr, the learning phase learns a latent variable model. In the Operating Phase, as we keep observing Dop, after the time interval t′, the data is incrementally aggregated into a sequence {X ′1, . . .X ′ t′}. At this point, we would like the model to predict the best possible choice of blocks to load into cache for interval t′ + 1 with knowledge of aggregated data {X ′1, . . . X ′ t′}.\nThis prediction happens in two steps. In the first step, our HMM based model Sparse-HMM-DPMMVP infers hidden state Z ′t′+1 from observations {X ′1, . . . , X ′ t′}, using a Viterbi style algorithm as follows. (3.1)\n(X ′t′+1, {Z ′ r} t′+1 r=1 ) = argmax\n(X′ t′+1 ,{Z′r} t′+1 r=1 )\np({X ′r} t′+1 r=1 , {Z ′ r} t′+1 r=1 |θ)\nNote the slight deviation from usual Viterbi method as X ′t′+1 is not yet observed. We also note that alternate strategies based on MCMC might be possible based on Bayesian techniques to infer the hidden state Z ′t′+1. However, in the operating phase, the execution time becomes important and is required to be much smaller than ν, the slice length. Hence we explore such a Viterbi\nbased technique, that is quite efficient and runs in a very small fraction of ν in practice for each prediction. The algorithm is detailed in the supplementary material.\nIn the second step, having predicted the hidden state Z ′t′+1, we would now like to load the appropriate accesses. Our prediction scheme consists of loading all accesses defined by H(Z ′t′+1) into the cache (with H as defined previously)."
    }, {
      "heading" : "4 Mixture Models with Multivariate Poisson for correlated count vector sequences",
      "text" : "We now describe non-parametric temporal models for correlated count vectors based on the MVP [9] mixtures. MVP [9] distributions are natural models for understanding multi-dimensional count data. There has been no work on exploring DP-based mixture models for count data or for modeling their temporal dependencies.\nHence, we first parallel the development of nonparametric MVP mixtures along the lines of DP based multinomial mixtures [16]. To this end we propose DP-MMVP, a DP based MVP mixture and propose a temporal extension HMM-DP-MMVP along the lines of HDP-HMM[16] for multinomial mixtures.\nHowever a more interesting challenge lies in designing algorithms of scalable complexity for high dimensional correlated count vectors. We address this in our next section( 5) by introducing the Sparse-MVP that exploits sparsity in data. DP mixtures of MVP and their sparse counterparts lead to different inference challenges addressed in section 6.\n4.1 Preliminaries:We first recall some definitions. A probability distribution G ∼ DP (α,H), when G = ∑∞\nk=1 βkδθk , β ∼ GEM(α), θk ∼ H, k = 1 . . . where H is a diffused measure. Probability measures G1 , . . . , GJ follow Hierarchical Dirichlet process(HDP)[16] if"
    }, {
      "heading" : "Gj ∼ DP (α,G0), j = 1 . . . J where G0 ∼ DP (α,H)",
      "text" : "HMMs are popular models for temporal data. However, for most applications there are no clear guidelines for fixing the number of HMM states. A DP based HMM model, HDP-HMM, [16] alleviats this need. Let X1, . . . , XT be observed data instances. Further, for any L ∈ Z, we introduce notation [L] = {1, . . . , L}. The HDP-HMM is defined as follows. β ∼ GEM(γ)\nπk|β, αk ∼ DP (αk, β), and θk|H ∼ H, k = 1, 2, . . .\nZt|Zt − 1, π ∼ πZt−1 , and Xt|Zt ∼ fZt(θk), t ∈ [T ]\nCommonly used base distributions for H are the multivariate Gaussian and multinomial distributions. There has been no work in exploring correlated count vector emissions. In our setting, we explore MVP emissions with H being an appropriate prior for parameter θk of the MVP distribution.\nThe Multivariate Poisson(MVP): Let ā, b̄ > 0. A random vector, X ∈ ZM is Multivariate Poisson(MVP) distributed, denoted by X ∼ MV P (Λ), if\nX = Y 1M Alternately, Xj =\nM∑\nl=1\nYjl, ∀j ∈ [M ]\nwhere ∀j ≤ l ∈ [M ], λl,j = λj,l ∼ Gamma(ā, b̄)\nYj,l = Yl,j ∼ Poisson(λj,l)(4.2)\nand 1M is a M dimensional vector of all 1s. It is useful to note that E(X) = Λ1M , where Λ is an M ×M symmetric matrix with entries λj,l and Cov(Xj , Xl) = λj,l. Setting λj,l = 0, j 6= l yields Xi = Yi,i which we refer to as the Independent Poisson (IP) model as Yi,i for each dimension i are independently Poisson distributed."
    }, {
      "heading" : "4.2 DP Mixture of Multivariate Poisson",
      "text" : "(DP-MMVP): In this section we define DP-MMVP, a DP based non-parametric mixture model for clustering correlated count vectors. We propose to use a DP based prior, G ∼ DP (α,H), where H is a suitably chosen Gamma conjugate prior for the parameters of MVP, Λ = {Λk : k = 1, . . .}, k being cluster identifier. We define DP-MMVP as follows.\nλkjl ∼ Gamma(ā, b̄),∀j ≤ l ∈ [M ], k = 1, . . .\nβ ∼ GEM(α) and G =\n∞∑\nk=1\nβkδΛk\nZt|β ∼ Mult(β)∀t ∈ [T ]\nXt|Zt ∼ MV P (ΛZt),∀t ∈ [T ](4.3)\nwhere T is the number of observations and (Λk)jl = (Λk)lj = λkjl. We also note that the DP Mixture of Independent Poisson (DP-MIP) can be similarly defined by restricting λk,j,l = 0, ∀j 6= l, k = 1, . . ..\n4.3 Temporal DP Mixture of MVP (HMM-DP-MMVP):DP-MMVP model does not capture temporal correlations that are useful for prediction problem of cache preloading. To this end we propose HMM-DPMMVP, a temporal extension of the previous model, as follows. Let Xt ∈ Z\nM , t ∈ [T ] be a temporal sequence of correlated count vectors.\nλkjl ∼ Gamma(ā, b̄) ∀j ≤ l ∈ [M ], k = 1, . . .\nβ ∼ GEM(γ) πk|β, αk ∼ DP (αk, β)∀k = 1, . . .\nZt|Zt − 1, π ∼ πZt−1 ,∀t ∈ [T ]\nXt|Zt ∼ MV P (ΛZt),∀t ∈ [T ](4.4)\nThe HMM-DP-MMVP incorporates the HDP-HMM structure into DP-MMVP in equation (4.3). This model can again be restricted to the special case of diagonal covariance MVP giving rise to the HMM-DP-MIP by extending the DP-MIP model. The HMM-DP-MIP models the temporal dependence, but not the spatial correlation coming from trace data."
    }, {
      "heading" : "5 Modeling with Sparse Multivariate Poisson:",
      "text" : "We now introduce the Sparse Multivariate Poisson (SMVP). Full covariance MVP, defined with (\nM 2\n)\nlatent variables (in Y) is computationally expensive during inference for higher dimensions. However, vectors Xt emanating from traces are often very sparse with only a few significant components and most components close to 0. While there has been work on sparse multinomial[19] and sparse Gaussian[7] mixtures, there has been no work on sparse MVP Mixtures. We propose the SMVP by extending the MVP to model sparse count vectors. We then extend this to Sparse-DP-MMVP for a non-parametric mixture setting and finally propose the temporal extension Sparse-HMM-DP-MMVP."
    }, {
      "heading" : "5.1 Sparse Multivariate Poisson distribution",
      "text" : "(SMVP):We introduce the SMVP as follows. Consider an indicator vector b ∈ {0, 1}M , that denotes whether\na dimension is active or not. Let λ̂j ≥ 0, ∀j ∈ [M ] and b ∈ {0, 1}M . We define X ∼ SMV P (Λ, λ̂, b) as: X = Y 1M where ∀j ≤ l ∈ [M ]\n(5.5) Yj,l ∼ Poisson(λj,l)bjbl + Poisson(λ̂j)(1− bj)δ(j, l))\nwhere Λ is a symmetric positive matrix with (Λ)jl = λjl. If bj = 1, bl = 1 then Yj,l is distributed as Poisson(λj,l). However if bj = 0, variables Yj,j are distributed as Poisson(λ̂j). The selection variables bj decide if the jth dimension is active. Otherwise we consider any emission at the jth dimension noise, modulated by Poisson(λ̂j), independent of other dimensions. Parameter λ̂j is close to zero for the extraneous noise dimensions and is common across clusters.\nWith Sparse-MVP, we are defining a full covariance MVP for a subset of dimensions while the rest of the dimensions are inactive and hence modeled independantly (with a small mean to account for noise). The full covariance MVP is a special case of Sparse-MVP where all dimensions are active.\n5.2 DP Mixture of Sparse Multivariate Poisson: In this section, we propose the Sparse-DPMMVP, extending our DP-MMVP model. For every mixture component k we introduce an indicator vector bk ∈ {0, 1}\nM . Hence, bk,j denotes whether a dimension j is active for mixture component k.\nA natural prior for selection variables, bkj , is Bernoulli Distribution, while a Beta distribution is a natural conjugate prior for the parameter ηj of the Bernaulli. ηj ∼ Beta(a ′, b′), bkj ∼ Bernoulli(ηj), j ∈ [M ], k = 1, . . . . The priors for parameters Λ and λ̂ are again decided based on conjugacy, where ∀j ≤ l ∈\n[M ] λj,l have a gamma prior, Let â, b̂ > 0. We model λ̂ to have a common Gamma prior for inactive dimen-\nsions over all clusters. λ̂j ∼ Gamma(â, b̂), ∀j ∈ [M ] . The Sparse-DP-MMVP is defined as:\nηj ∼ Beta(a ′ , b ′), bk,j ∼ Bernoulli(ηj), j ∈ [M ], k = 1 . . .\nλ̂j ∼ Gamma(â, b̂), j ∈ [M ]\nλkjl ∼ Gamma(ā, b̄), {j ≤ l ∈ [M ] : bk,j = bk,l = 1}, k = 1, . . .\nG = ∞∑\nk=1\nβkδΛk , β ∼ GEM(α)\nCluster selection variables Zt|β ∼ Mult(β) and\nXt|Zt,Λ, λ̂, bZt ∼ SMV P (ΛZt , λ̂, bZt), t ∈ [T ](5.6)\nDP-MMVP is a special case of Sparse-DP-MMVP where all dimensions of all clusters are active.\n5.3 Temporal Sparse Multivariate Poisson Mixture:We now define Sparse-HMM-DPMMVP, by extending Sparse-DP-MMVP to also capture Temporal correlation between instances by incorporating HDP-HMM into the Sparse-DP-MMVP:\nηj ∼ Beta(a ′ , b ′), bk,j ∼ Bernoulli(ηj), j ∈ [M ], k = 1 . . .\nλ̂j ∼ Gamma(â, b̂), j ∈ [M ]\nλkjl ∼ Gamma(ā, b̄), {j ≤ l ∈ [M ] : bk,j = bk,l = 1}, k = 1, . . .\nβ ∼ GEM(γ) and πk|β, αk ∼ DP (αk, β), k = 1, . . .\nZt|Zt − 1, π ∼ πZt−1 , t ∈ [T ]\nXt|Zt,ΛZt , λ̂, bZt ∼ SMV P (ΛZt , λ̂, bZt), t ∈ [T ](5.7)\nThe plate diagram for the Sparse-HMM-DP-MMVP model is shown in figure 3. We again note that HMM-\nDP-MMVP model described in section 4.3 is a restricted form of Sparse-HMM-DP-MMVP where bk,j is fixed to 1. The Sparse-HMM-DP-MMVP captures the spatial correlation inherent in the trace data and the long range temporal dependencies, at the same time exploiting sparseness, reducing the number of latent variables.\n6 Inference\nWhile inference for non-parametric HMMs is well explored [16][4], MVP and Sparse-MVP emissions introduce additional challenges due to the latent variables involved in the definition of the MVP and the introduction of sparsity in a DP mixture setting for the MVP.\nWe discuss the inference of DP-MMVP in detail in the supplementary material. In this section, we give a brief overview of collapsed Gibbs sampling inference for HMM-DP-MMVP and Sparse-HMM-DP-MMVP. More details are again in the supplementary material.\nThroughout this section, we use the following notation: Y = {Yt : t ∈ [T ]}, Z = {Zt : t ∈ [T ] and X = {Xt : t ∈ [T ]. A set with a subscript starting with a hyphen(−) indicates the set of all elements except the index following the hyphen. The latent variables to be sampled are Zt, t ∈ [T ], Yt,j,l, j ≤ l ∈ [M ], t ∈ [T ] and bk,j , j ∈ [M ], k = 1, . . . (For the sparse model). Further we have β = {β1, . . . , βK , βK+1 = ∑∞ r=K+1 βr}. and an auxiliary variable mk is introduced as a latent variable to aid the sampling of β based on the direct sampling procedure from HDP[16]. Updates for m and β are similar to [4], as detailed in algorithm 1. The latent variables Λ and π are collapsed.\nSampling Zt, t ∈ [T ] While the update for this variable is similar to that in [16][4], the likelihood term p(Y |Zt = k, Z−t, Y−t,b\nold; ā, b̄) differs due to the MVP based emissions. For the HMM-DP-MMVP, this term can be evaluated by integrating out the λs. Similarly for Sparse-HMM-DP-MMVP when k is an existing componant (for which bk is known).\nHowever, for the Sparse-HMM-DP-MMVP, an additional complication arises for the case of a new componant, since we do not know the bK+1 value, requiring summing over all possibilities of bK+1, leading to exponential complexity. Hence, we evaluate this numerically (see supplementary material for details). This process is summarized in algorithm 1.\nSampling Yt,j,l, j ≤ l ∈ [M ], t ∈ [T ] : The Yt,j,l latent variables in the MVP definition, differentiate the inference procedure of an MVP mixture from standard inference for DP mixtures. Further, the large number of Yt,j,l variables ( n 2 )\nalso leads to computationally expensive inference for higher dimensions motivating sparse modeling. In, Sparse-HMM-DP-MMVP, only those Yt,j,l values are updated for which bZt,j = bZt,l = 1.\nWe have, for each dimension j, Xt,j = ∑M\nl=1 Yt,j,l. To preserve this constraint, suppose for row j, we sample Yt,j,l, j 6= l, Yt,j,j becomes a derived quantity as Yt,j,j = Xt,j − ∑M\np=1,p6=j Yt,p,j . We also note that, updating the value of Yt,j,l impacts the value of only two other random variables i.e Yt,j,j and Yt,l,l. The final update for Yt,j,l, j 6= l can be obtained by integrating out Λ.\n(full expression in alg 1, more details: Appendix B).\nAlgorithm 1: Inference: Sparse-HMM-DP-MMVP Inference steps(The steps for HMM-DP-MMVP are similar and\nare shown as alternate updates in brackets)\nRepeat until convergence for t = 1, . . . , T do\n// Sample Zt from\np(Zt = k|Z−t,−(t+1), zt+1 = l,b old, X, β, Y ;α, ā, b̄)\n∝ p(Zt = k, Z−t,−(t+1),t+1 = l|β;α, ā, b̄)\np(Y |Zt = k, Z−t, Y−t,b old; ā, b̄)\n//Case 1: For HMM-DP-MMVP (with bk,j = 1 ∀j,∀k) //and Sparse-HMM-DP-MMVP For existing k\np(Y |Zt = k, Z−t, Y−t,b old; ā, b̄) ∝ Π\nj≤l∈[M] F\nbk,jbk,l k,j,l\nΠ j∈[M] F̂j\nWhere Fk,j,l = Γ(ā + Sk,j,l)\n(b̄+ nk) (ā+Sk,j,l) Π\nt̄:Zt̄=k Yt̄,j,l!\nand F̂j = Γ(â + Ŝj)\n(b̂ + n̂j) (â+Ŝj) Π\nt,j:bZt,j=0 Yt,j,j !\n, With Ŝj = ∑ t Yt,j,j(1 − bZt,j) , n̂j = ∑\nt(1− bZt,j) and Sk,j,l = ∑ t̄ Yt̄,j,lδ(Zt̄, k), for j ≤ l ∈ [M ] //Case 2: For Sparse-HMM-DP-MMVP for new k, //compute following numerically,where bold = {b1, . . . , bK}\np(Y |bold, Zt = K + 1, Z−t, Y−t; ā, b̄) = ∑\nbK+1\np(bK+1|b old, η)\np(Y |bold, bK+1, Zt = K + 1, Z−t, Y−t; ā, b̄) for j ≤ l ∈ [M ] do\nif bZt,j = bZt,k = 1 then // Sample Yt,j,l from\np(Yt,j,l|Y−t,j,l, Z, ā, b̄) ∝ Fk,j,lFk,j,jFk,l,l\nSet Yt,j,j = Xt,j − ∑M\nj̄=1 Yt,j,j̄\nSet Yt,l,l = Xt,l − ∑M\nl̄=1 Yt,l,l̄\nfor j = 1, . . . ,M, k = 1, . . . , K do // Sample bk,j (for Sparse-HMM-DP-MMVP) from\np(bk,j |b−k,j , Y, Z) ∼ p(bk,j |b−k,j)p(Y |bk,j , b−k,j , Z; āb̄)\np(bk,j |b−k,j) ∝ c−kj + bk,j + a ′ − 1\nK + a′ + b′ − 1\nwhere c−kj = ∑K k̄ 6=k,k̄=1 bk,j is the number of clusters\n(excluding k) with dimension j active for k = K, . . . ,M, k = 1, . . . do\nmk = 0 for i = 1, . . . , nk do\nu ∼ Ber( αβk i+αβk ), if (u == 1)mk ++\n[β1β2 . . . βKβK+1]|m,γ ∼ Dir(m1, . . . ,mk , γ)\nUpdate for bk,j: For Sparse-HMM-DP-MMVP, the update for bk,j is obtained by integrating out η to evaluate p(bk,j |b−k,j) and computing the likelihood term by integrating out λ, λ̂ as before. This is shown in algorithm 1 (see supplementary material for more details).\nTrain time Complexity Comparison: SparseHMM-DP-MMVP vs HMM-DP-MMVP: The\ninference procedure for both models is similar, with different updates shown in Algorithm 1. For the HMMDP-MMVP all dimensions are active for all clusters. We sample (\nM 2\n)\nrandom variables for the symmetric matrix Yt in this step for each t ∈ [T ]. On the other hand, for Sparse-HMM-DP-MMVP with m̄k active components in cluster k, we sample only (\nm̄ 2\n)\nrandom variables which is a significant improvement when m̄ << M .\n7 Experimental Evaluation\nWe perform experiments on benchmark traces, to evaluate our models, in terms of likelihood and also evaluate their effectiveness for the caching problem, by measuring hitrates using our predictive models.\n7.1 Datasets:We perform experiments on diverse enterprise workloads : 10 publicly available real world Block I/O traces (MT 1-10), commonly used benchmark in storage domain, collected at Microsoft Research Cambridge[3] and 1 NetApp internal workload (NT1). See dataset details and choice of traces in Appendix C.\nWe divide the available trace into two parts Dlr that is aggregated into Tlr count vectors and D op that is aggregated into Top count vectors. In our initial experimentation, for aggregation, we fix the number of memory bins M=10 (leading to 10 dim count vectors), length of time slice ν=30 seconds. Further, we use a test train split of 50% for both experiments such that Tlr = Top. (Later, we also perform some experiments to study the impact of some of these parameters with M = 100 dimensions on some of the traces.)\n7.2 Experiments:We perform two types of experiments, to understand how well our model fits data in terms of likelihood, the next to show how our model and our framework can be used to improve cache hitrates.\n7.2.1 Experiment Set 1: Likelihood Comparison:We show a likelihood comparison between HMMDP-MMVP, Sparse-DP-MMVP and baseline model HMM-DP-MIP. We train the three models using the inference procedure detailed in section 6 on Tlr and compute Log-likelihood on the held out test trace Top. The results are tabulated in Table 1.\nResults: We observe that the Sparse-HMM-DPMMVP model performs the best in terms of likelihood, while the HMM-DP-MMVP outperforms HMM-DPMIP by a large margin. Poor performance of HMM-DPMIP clearly shows that spatial correlation present across the M dimensions is an important aspect and validates the necessity for the use of a Multivariate Poisson model over an independence assumption between the dimensions. Superior performance of Sparse-HMM-DPMMVP over HMM-DP-MMVP is again testimony to the fact that there exists inherent sparsity in the data and this is better modeled by the sparse model.\n7.2.2 Experiment Set 2: Hitrates:We compute hitrate, on each of the 11 traces, with a baseline simulator without preloading and an augmented simulator with the ability to preload predicted blocks every ν = 30s. Both the simulators use LRU for eviction. Off the shelf simulators for preloading are not available for our purpose and construction of the baseline simulator and that with preloading are described in detail in supplementary material- Appendix C.\nResults: We see in the barchart in figure 4 prediction improves hitrates over LRU baseline without preloading. We see that our augmented simulator gives order of magnitude better hitrates on certain traces (0.52 with preloading against 0.0002 with plain LRU).\nEffect of Training Data Size: We expect to capture long range dependencies in access patterns when we observe a sufficient portion of the trace for training where such dependencies manifest. Ideally we would like to run our algorithm in an online setting, where periodically, all the data available is used to update the model. Our model can be easily adapted to such a situation. In this paper, however, we experiment in a setting where we always train the model on 50% (see supplementary material for an explanation of the figure 50%) of available data for each trace and use this model to make predictions for the rest of the trace.\nEffect of M (aggregation granularity): To understand the impact M (count vector dimensionality),\nwe pick some of the best performing traces from the previous experiment (barchart in figure 4) and repeat our experiment with M=100 features, a finer 100 bin memory aggregation leading to 100 dimensional count vectors. We find that we beat baseline by an even higher margin with M=100. We infer this could be attributed to the higher sensitivity of our algorithm to detail in traces leading to superior clustering. This experiment also brings to focus the training time of our algorithm. We observed that the Sparse-HMM-DP-MMVP outperforms HMM-DP-MMVP not only in terms of likelihood and hitrates but also in terms of training time. We fixed the training time to at most 4 hours to run our algorithms and report hitrates in table 2. We find that Sparse-HMM-DP-MMVP ran to convergence while HMM-DP-MMVP did not finish even a single iteration for most traces in this experiment. This corroborates our understanding that handling sparsity reduces the number of latent variables in HMM-DP-MMVP, improving inference efficiency translating to faster training time, particularly for higher dimensional count vectors.\n7.3 Discussion of Results:We observe both from table 1 and the barchart (fig 4) that HMM-DPMMVP outperforms HMM-DP-MIP, and Sparse-HMMDP-MMVP performs the best, outperforming HMMDP-MMVP in terms of likelihood and hitrates, showing that traces indeed exhibit spatial correlation that is effectively modeled by the full covariance MVP and that handling sparsity leads to a better fit of the data.\nThe best results are tabulated in Table 2 where we observe that when using 100 bins Sparse-HMM-DP-\nMVP model achieves an average hitrate of h = 0.565, 30 times improvement over LRU without preloading, h = 0.0186. On all the other traces, LRU without preloading is outperformed by the sparse-HMM-DPMMVP, the improvement being dramatic for 4 of the traces. On computing average hitrate for the 11 traces in figure 4, we see 58% hitrate improvement.\nChoice of Baselines: We did not consider a parametric baseline as it is clearly not suitable for our caching application. Traces have different access patterns with varying detail (leading to varying number of clusters: fig 2). A parametric model is clearly infeasible in a realistic scenario. Further, due to lack of existing predictive models for count vector sequences, we use a HMM-DP-MIP baseline for our models.\nExtensions and Limitations: While we focus on capturing long range correlations, our framework can be augmented with other algorithms geared towards specific workload types for capturing short range correlations, like sequential read ahead and Sarc [5] to get even higher hitrates. We hope to investigate this in future.\nWe have shown that our models lead to dramatic improvement for a subset of traces and work well for the rest of our diverse set of traces. We note that there may not be discernable long range correlations present in all traces. However, we have shown, that when we can predict, the scale of its impact is huge. There are occasions, when the prediction set is larger than cache where we would have to understand the temporal order of predicted reads, to help efficiently schedule preloads. Cache size, prediction size and preload frequency, all play an important role to evaluate the full impact of our method. Incorporating our models within a full-fledged storage system involves further challenges, such as real time trace capture, smart disk scheduling algorithms for preloading, etc. These issues are beyond the scope of the paper, but form the basis for future work.\n8 Conclusions\nWe have proposed DP-based mixture models (DPMMVP, HMM-DP-MMVP) for correlated count vectors that capture the full covariance structure of multivariate count data. We have further explored the sparsity in our data and proposed models (Sparse-DP-MMVP and Sparse-HMM-DP-MMVP) that capture the correlation within a subset of dimensions for each cluster, also leading to more efficient inference algorithms. We have taken the first steps in outlining a preloading framework for leveraging long range dependencies in block I/O Traces to improve cache hitrates. Our algorithms achieve a 30X hitrate improvement on 4 real world traces, and outperform baselines on all traces. References\n[1] D. J. Aldous. In École d’été de probabilités de Saint-\nFlour, XIII—1983, Lecture Notes in Math., pages 1– 198. Springer, Berlin, 1985. [2] L. M. Dimitris Karlis. Journal of Statistical Planning and Inference, 2007. [3] A. R. Dushyanth Narayanan, A Donnelly. Write offloading: Practical power management for enterprise storage. USENIX, FAST, 2008. [4] E. Fox, E. Sudderth, M. Jordan, and A. Willsky. A Sticky HDP-HMMwith Application to Speaker Diarization. Annals of Applied Statistics, 2011. [5] B. S. Gill and D. S. Modha. Sarc: Sequential prefetching in adaptive replacement cache. In UATC, 2005. [6] M. M. Gokul Soundararajan and C. Amza. Contextaware prefetching at the storage server. In USENIX ATC. USENIX, 2008. [7] A. K. Jain, M. Law, and M. Figueiredo. Feature Selection in Mixture-Based Clustering. In NIPS, 2002. [8] D. Karlis and L. Meligkotsidou. Finite mixtures of multivariate poisson distributions with application. Journal of Statistical Planning and Inference, 137(6):1942–1960, June 2007. [9] K. Kawamura. The structure of multivariate poisson distribution. Kodai Mathematical Journal, 1979. [10] T. M. Kroeger and D. D. E. Long. Predicting file system actions from prior events. In UATC, 1996. [11] M. H. C. Law, A. K. Jain, and M. A. T. Figueiredo. Feature selection in mixture-based clustering. In NIPS, pages 625–632, 2002. [12] C. Z. S. S. M. Li, Z. and Y. Zhou. Mining block correlations in storage systems. In FAST. USENIX, 2004. [13] L. Meligkotsidou. Bayesian multivariate poisson mixtures with an unknown number of components. Statistics and Computing, 17, Iss 2, pp 93-107, 2007. [14] A. M. Schmidt and M. A. Rodriguez. Modelling multivariate counts varying continuously in space. In Bayesian Statistics Vol 9. 2010. [15] Y. W. Teh. Dirichlet processes. In Encyclopedia of Machine Learning. Springer, 2010. [16] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical dirichlet processes. Journal of American Statistical Association, 2004. [17] P. Tsiamyrtzis and D. Karlis. Strategies for efficient computation of multivariate poisson probabilities. Communications in Statistics (Simulation and Computation), Vol. 33, No. 2, pp. 271292, 2004. [18] B. D. M. Wang, Chong. Decoupling sparsity and smoothness in the discrete hierarchical dirichlet process. In NIPS. [19] C. Wang and D. Blei. Decoupling sparsity and smoothness in the discrete hierarchical dirichlet process. In NIPS. 2009. [20] Y. Zhang, G. Soundararajan, M. W. Storer, L. N. Bairavasundaram, S. Subbiah, A. C. Arpaci-Dusseau, and R. H. Arpaci-Dusseau. Warming up storage-level caches with bonfire. In USENIX Conference on File and Storage Technologies. USENIX, 2013.\nSupplementary Material: Mining Block I/O Traces for Cache Preloading with Sparse Temporal Non-parametric Mixture of Multivariate Poisson\nAppendix A: Prediction Method The prediction problem in the operational phase involves finding the best Z ′t+1 using θ to solve equation 3.1. We describe a Viterbi like dynamic programming algorithm to solve this problem for Sparse-MVP emissions. (A similar procedure can be followed for MVP emissions).\nWe note that alternate strategies based on MCMC might be possible based on Bayesian inference for the variable under question. However, in the operation phase, the execution time becomes important and is required to be much smaller than ν, the slice length. Hence we explore the following dynamic programming based procedure that is efficient and runs in a small fraction of slice length ν.\nAt the end of the learning phase, we estimate the values of θ = {Λ, π, b, λ̂}, by obtaining Λ, λ̂, π as the mean of their posterior, and b by thresholding the mean of its posterior and use these as parameters during prediction. A standard approach to obtain the most likely decoding of the hidden state sequence is the Viterbi algorithm, a commonly used dynamic programming technique that finds\n{Z ′∗s } t\ns=1 = argmax {Z′s} t s=1\np(X ′1, ...X ′ t′ , {Z ′ s}\nt s=1)|θ)\nLet ω(t, k) be the highest probability along a single path ending with Z ′t = k. Further, let ω(t, k) = max {Z′s} t s=1 p({X ′s} t s=1, {Z ′ s} t s=1)|θ). We have\nω(t+ 1, k) = argmax k′=1,...,K\nω(t, k′)πk′,kSMV P (X ′ t+1; θ)\nHence, in the standard setting of viterbi algorithm, having observed X ′t+1, the highest probability estimate of the latent variables is found as Z ′\n∗ t+1 = argmax\n1≤k≤K ω(t+\n1, k). However, the evaluation of MVP and hence the evaluation of the SMVP pmf involves exponential complexity due to integrating out the Y variables. While there are dynamic programming based approaches explored for MVP evaluation [17], we resort to a simple approximation. Let µk,i = ∑M\nj=1 λk,i,jbk,ibk,j+(1−bk,i)λ̂j , i ∈ [M ], k ∈ [K]. We consider Xt,i|Zt = k ∼ Poisson(µk,i) when Xt ∼ SMV P (Λk, λ̂) (since the sum of independent Poisson random variables is again a Poisson random variable). Hence we compute p(Xt|Zt = k, µk) = Π M i=1Poisson(Xt,i;µk,i).\nIn our setting, we require finding the most likely Z ′∗t+1 without having observed X ′ t+1 to address our\nprediction problem from section 3. Hence we define the following optimization problem that tries to maximize the objective function over the value of X ′t+1 along with the latent variables {Z ′s} t+1 s=1.\nω′(t+ 1, k) = max {Z′s} t+1 s=1,X ′ t+1 p({X ′s} t+1 s=1, {Z ′ s} t+1 s=1)|θ)\nHowever, since mode of Poisson is also its mean,\n(9.8) ω′(t+ 1, k) = Poisson(µk|µk) max k=1,...,K ω′(t, k)πk,l\nFrom equation 9.8, we have a dynamic programming algorithm similar to Viterbi algorithm (detailed in algorithm 2).\nAlgorithm 2: Prediction Algorithm\nInitial Iteration: Before X1 is observed ω′(1, k) = π0kPoisson(µk;µk)∀k Z∗1 = Argmaxk ω\n′(1, k) Initial Iteration: After X1 is observed ω(1, k) = π0kPoisson(X1;µk)∀k for t = 2, . . . T do\nBefore Xt is observed ω′(t, l) = maxk(ω(t− 1, k)πkl)Poisson(µl, µl)∀k Z∗t = Argmaxk ω\n′(t, l) After Xt is observed ω(t, l) = maxk(ω(t− 1, k)πkl)Poisson(Xt, µl)∀k Ψ(t, l) = Argmaxk (ω(t− 1, k)πkl)\nFinding the Path ZT = Argmaxkω(t,K) for Data points t = T − 1, T − 2 . . . 1 do\nZT = Ψ(t+ 1, Z(t+ 1))"
    }, {
      "heading" : "Appendix B: Inference Elaborated",
      "text" : "In this section of supplementary material we discuss the inference procedure for DP-MMVP, HMM-DPMMVP and Sparse-HMM-DP-MMVP more elaborately adding some details that could not be accomodated in the original paper. Our Collapsed Gibbs Sampling inference procedure is described in the rest of this section.\nWe first outline the inference for DP-MMVP model in section 10.1 , followed by the HMM-DP-MMVP, its temporal extension in section 10.2. Then, in section 10.3, we describe the inference for the Sparse-HMMDP-MMVP model extending the previous procedure.\n10.1 Inference : DP-MMVP:The existance of Yt,j,l latent variables in the MVP definition differentiates the inference procedure of an MVP mixture from\nstandard inference for DP mixtures. (The large number of Yt,j,l variables also leads to computationally expensive inference for higher dimensions motivating sparse modeling).\nWe collapse Λ variables exploiting the PoissonGamma conjugacy for faster mixing. The latent variables Zt, t ∈ [T ], and Yt,j,l, j ≤ l ∈ [M ], t ∈ [T ] require to be sampled. Throughout this section, we use the following notation: Y = {Yt : t ∈ [T ]}, Z = {Zt : t ∈ [T ] and X = {Xt : t ∈ [T ]. A set with a subscript starting with a hyphen(−) indicates the set of all elements except the index following the hyphen. Update for Zt: The update for cluster assignments Zt are based on the conditional obtained on integrating out G, based on the CRP[1] process leading to the following product.\np(Zt = k|Z−t, X, β, Y ;α, ā, b̄) ∝ p(Zt = k|Z−t;α)fk(Yt)\n∝\n{\nn−tk fk(Yt) k ∈ [K]\nαfk(Yt) k=K+1 (10.9)\nWhere n−tk, = ∑ t̄6=t δ(Zt̄, k). The second term fk(Yt) = p(Yt, Y−t|Zt = k, Z−t; ā, b̄) can be simplified by integrating out the Λ variables based on their conjugacy. Let Sk,j,l = ∑ t̄ Yt̄,j,lδ(Zt̄, k), for j ≤ l ∈ [M ] and\nFk,j,l = Γ(ā+ Sk,j,l)\n(b̄+ nk)(ā+Sk,j,l) Π t̄:Zt̄=k\nYt̄,j,l! (10.10)\nBy collapsing Λ, fk(Yt) ∝ Π 1<=j<=l<=M Fk,j,l(10.11)\nUpdate for Yt,j,l: This is the most expensive\nstep since we have to update ( M 2 )\nvariables for each observation t. The Λ variables are collapsed, owing to the Poisson-Gamma conjugacy due to the choice of a gamma prior for the MVP.\nIn each row j of Yt, Xt,j = ∑M\nl=1 Yt,j,l. To preserve this constraint, suppose for row j, we sample Yt,j,l, j 6= l, Yt,j,j becomes a derived quantity as Yt,j,j = Xt,j − ∑M\np=1,p6=j Yt,p,j . The update for Yt,j,l, j 6= l can be obtained by integrating out Λ to get an expression similar to that in equation 10.10. We however note that, updating the value of Yt,j,l impacts the value of only two other random variables i.e Yt,j,j and Yt,l,l. Hence we get the following update for Yt,j,l\np(Yt,j,l|Y−t,j,l, Z, ā, b̄) ∝ Fk,j,lFk,j,jFk,l,l(10.12)\nThe support of Yt,j,l, a positive, integer valued random variable, can be restricted as follows for efficient computation. We have Yt,j,j = Xt,j− ∑M p=1,p6=j Yp,j ≥ 0\nSimilarly, Yt,l,l = Xt,l − ∑M\np=1,p6=l Yp,l ≥ 0. Hence, we can reduce the support of Yt,j,l to the following: (10.13)\n0 ≤ Yt,j,l ≤ min\n\n(Xt,j −\nM ∑\np=1,p6=l\nYp,j), (Xt,l −\nM ∑\np=1,p6=j\nYp,l)\n\n\n10.2 Inference : HMM-DP-MMVP:The latent variables from the HMM-DP-MMVP model that require to be sampled include Zt, t ∈ [T ], Yt,j,l, j, l ∈ [M ], t ∈ [T ] , and β = {β1, . . . , βK , βK+1 = ∑∞ r=K+1 βr}. Additionally an auxiliary variable mk (denoting the cardinality of the partitions generated by the base DP) is introduced as a latent variable to aid the sampling of β based on the direct sampling procedure from HDP[16]. The latent variables Λ and π are collapsed to facilitate faster mixing. The procedure for sampling of Yt,j,l, j, l ∈ [M ], t ∈ [T ] is the same as that for DP-MMVP (eq: 10.12). Updates for m and β are similar to [4], detailed in algorithm 1. We now discuss the remaining updates. Update for Zt: The update for cluster assignment for the HMM-DP-MMVP while similar to that that of DP-MMVP also considers the temporal dependency between the hidden states. Similar to the procedure outlined in [16][4] we have:\np(Zt = k|Z−t,−(t+1), zt+1 = l, X, β, Y ;α, ā, b̄)\n∝ p(Zt = k, z−t,−(t+1), zt+1 = l|β;α, ā, b̄)fk(Yt) (10.14)\nWhere fk(Yt) = p(Yt, Y−t|Zt = k, Z−t; ā, b̄). The first term can be evaluated to the following by integrating out π as\np(Zt = k|Z−t,−(t+1), Zt+1 = l, β;α) =\n\n\n\n(n−tzt−1,k + αβk) αβl+(n\n−(t) k,l +δ(Zt−1,k)δ(k,l))\nα+n −(t) k,. +δ(Zt−1,k) k ∈ [K]\n(αβK+1) αβl) (α) k=K+1\n(10.15)\nWhere n−tk,l = ∑\nt̄6=t,t̄6=t+1 δ(Zt̄, k)δ(Zt̄+1, l). The second term fk(Yt) is obtained from the equation 10.11."
    }, {
      "heading" : "10.3 Inference : Sparse-HMM-DP-MMVP:",
      "text" : "Sparse-HMM-DP-MMVP Inference is computationally less expensive due to the selective modeling of covariance structure. However, inference for Sparse-HMMDP-MVPM requires sampling of bk,j , j ∈ [M ], k = 1, . . . in addition to latent variables in section 10.2 introducing challenges in the non-parametric setting that we discuss in this section. Note: Variables, η,Λ and λ̂ are collapsed for faster mixing. Update for bk,j : The update can be written as a product: (10.16) p(bk,j |b−k,j , Y, Z) ∼ p(bk,j |b−k,j)p(Y |bk,j , b−k,j , Z; āb̄)\nBy integrating out η, we simplify the first term as follows where c−kj = ∑K\nk̄ 6=k,k̄=1 bk,j is the number of clusters (excluding k) with dimension j active.\np(bk,j |b−k,j) ∝ c−kj + bk,j + a ′ − 1\nK + a′ + b′ − 1\nThe second term can be simplified as follows in terms of Fk,j,l as defined in equation 10.10 by collapsing the Λ variables and F̂j obtained from integrating out the λ̂ variables.\np(Y |bk,j , b−k,j , Z; āb̄) ∝ Π j≤l∈[M ]\nF bk,jbk,l k,j,l Π\nj∈[M ] F̂j\n(10.17)\nWhere F̂j = Γ(â+ Ŝj)\n(b̂ + n̂j)(â+Ŝj) Π t,j:bZt,j=0\nYt,j,j ! (10.18)\n. And Ŝj = ∑ t Yt,j,j(1− bZt,j) and n̂j = ∑\nt(1 − bZt,j) Update for Zt : Let b\no = {bk : k ∈ [K]} be the variables selecting active dimensions for the existing clusters. The update for cluster assignments Zt, t ∈ [T ] while similar to the direct assignment sampling algorithm of HDP[16], has to handle the case of evaluating the probability of creating a new cluster with an unknown bk+1 .\nThe conditional for Zt can be written as a product of two terms as that in equation 10.2\np(Zt = k|Z−t,−(t+1), zt+1 = l,b old, X, β, Y ;α, ā, b̄)\n∝ p(Zt = k, Z−t,−(t+1),t+1 = l|β;α, ā, b̄)\np(Yt|Zt = k, Z−t, Y−t,b old; ā, b̄)(10.19)\nThe first term can be simplified in a way similar to [4]. To evaluate the second term, two cases need to be considered.\nExisting topic (k ∈ [K]) : In this case, the second term p(Yt|b\noZt = k, Z−t, Y−t; ā, b̄) can be simplified by integrating out the Λ variables as in equation (10.17).\nNew topic (k = K + 1) : In this case, we wish to compute p(Yt|b\no, Zt = K + 1, Z−t, Y−t; ā, b̄). Since this expression is not conditioned on bK+1, evaluation of this term requires summing out bK+1 as follows.\np(Yt|b o, Zt = K + 1, Z−t, Y−t; ā, b̄) =\n∑\nbK+1\np(bK+1|b o, η)p(Yt|b o, bK+1, Zt = K + 1, Z−t, Y−t; ā, b̄)\nEvaluating this summation involves exponential complexity. Hence we resort to a simple numerical approximation as follows. Let us denote p(Yt|b o, bK+1, Zt = K + 1, Z−t, Y−t; ā, b̄) as h(bK+1)\nThe above expression can be viewed as an expectation EbK+1 [h(bK+1)|b\no]. and can be approximated numerically by drawing samples of bK+1 with probability p(bK+1|b\no). We use Metropolis Hastings algorithm to get a fixed number S of samples using the proposal distribution that flips each element of b independently with a small probability p̂. The intuition here is that we expect the feature selection vector for new cluster, bK+1 to be reasonably close to bZoldt , the selection vector corresponding to the previous cluster assignment for this data point. In our experiments we set S=20 and p̂=0.2 to give reasonable results.\nWe note that in [18], the authors address a similar problem of feature selection, however in a multinomial DP-mixture setting, by collapsing the b selection variable. However, their technique is specific to sparse Multinomial DP-mixtures. Update for Yt,j,l: The update for Yt,j,l is similar to that in section 10.1 with the following difference. We sample only {Yt,j,l : bj = 1, bl = 1} and the rest of the elements of Y are set to 0 with the exception of diagonal elements for the inactive dimensions. We note that for the inactive dimensions {j : j ∈ [M ], bZt,j = 0}, the value of Xt,j = Yt,j,j and hence can be set directly from the observed data without sampling.\nFor the active dimensions, {Yt,j,l : bj = 1, bl = 1, j ≤ l ∈ [M ]} we sample using a procedure similar to that in section 10.1 by sampling Yt,j,l, j 6= l to preserve the constraintXt,j = ∑M\nl=1 Yt,j,l, restricting the support of the random variable in a procedure similar to section 10.1.\np(Yt,j,l|Y−t,j,l, Z, ā, b̄, â, b̂) ∝ Fk,j,lFk,j,jFk,l,l Π 1<=j<=M F̂j\n(10.20)\nAppendix C: Experiment Details 11.4 Dataset Details:We perform experiments on publicly available real world block I/O traces from enterprise servers at Microsoft Research Cambridge [3]. They represent diverse enterprise workloads. These are about 36 traces comprising about a week worth of data, thus allowing us to study long ranging temporal dependencies. We eliminated 26 traces that are write heavy (write percentage > 25%) as we are focused on read cache. See Table 3 for the datasets and their read percentages. We present our results on the remaining 10 traces. We also validated our results on one of our internal workloads, NT1 comprising data collected over 24 hours.\nWe divide the available trace into two parts Dlr\nAlgorithm 3: Inference: Sparse-HMM-DP-MMVP Inference steps(The steps for HMM-DP-MMVP are similar and are shown as alternate updates in brackets)\nrepeat for t = 1, . . . , T do\nSample Zt from Eqn 10.3 (Alt: Eqn 10.2) for j ≤ l ∈ [M ] do\nif bZt,j = bZt,k = 1 then Sample Yt,j,l from Eqn 10.20 (Alt: Eqn 10.12)\nSet Yt,j,j = Xt,j − ∑M\nj̄=1 Yt,j,j̄\nSet Yt,l,l = Xt,l − ∑M\nl̄=1 Yt,l,l̄ for j = 1, . . . ,M, k = 1, . . . ,K do\nSample bk,j from Eqn 10.16 (Alt: Set bk,j = 1M )\nfor k = K, . . . ,M, k = 1, . . . do mk = 0 for i = 1, . . . , nk do\nu ∼ Ber( αβk i+αβk ), if (u == 1)mk ++\n[β1β2 . . . βKβK+1]|m, γ ∼ Dir(m1, . . . ,mk, γ) until convergence;\nthat is aggregated into Tlr count vectors and D op that is aggregated into Top count vectors. We use a split of 50% data for learning phase and 50% for operation phase for our experiments such that Tlr = Top.\n11.5 Design of Simulator:The design of our baseline simulator and that with preloading is described below.\nBaseline: LRU Cache Simulator: We build a cache simulator that services access requests from the trace maintaining a cache. When a request for a new block comes in, the simulator checks the cache first. If the block is already in the cache it records a hit, else it records a miss and adds this block to the cache. The cache has a limited size (fixed to 5% the total trace size). When the cache is full, and a new block is to be added to the cache, the LRU replacement policy is used to\nselect an existing block to remove. We use the hitrates obtained by running the traces on this simulator as our baseline.\nLRU Cache Simulator with Preloading: In this augmented simulator, at the end of every ν = 30s, predictions are made using the framework described in Section 3 and loaded into the cache (evicting existing blocks based on the LRU policy as necessary). While running the trace, hits and misses are kept track of, similar to the previous setup. The cache size used is the same as that in the previous setting.\n11.6 Hitrate Values: Figure 4 in our paper shows a barchart of hitrates for comparison. In table 4 of this section, the actual hitrate values are provided comparing preloading with Sparse-HMM-DP-MMVP and that with baseline LRU simulator without preloading. We note that we show a dramatic improvement in hitrate in 4 of the traces while we beat the baseline without preloading in most of the other traces.\n11.7 Effect of Training Data Size:We expect to capture long range dependencies in access patterns when we observe a sufficient portion of the trace for training where such dependencies manifest. We show this by running our algorithm for different splits of train and test data (corresponding to the learning phase and the operational phase) for NT1 trace.\nWe observe that when we see at least 50% of the trace, there is a marked improvement in hitrate for the NT1 trace. Hence we use 50% as our data for training for our experimentation.\nIn a real world setting, we expect the amount of data required for training to vary across workloads. To adapt our methodology in such a setting periodic retraining to update the model with more and more data for learning as it is available is required. Exploring an\nonline version of our models might also prove useful in such settings."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "Existing caching strategies, in the storage do-<lb>main, though well suited to exploit short range spatio-<lb>temporal patterns, are unable to leverage long-range<lb>motifs for improving hitrates. Motivated by this,<lb>we investigate novel Bayesian non-parametric model-<lb>ing(BNP) techniques for count vectors, to capture long<lb>range correlations for cache preloading, by mining Block<lb>I/O traces. Such traces comprise of a sequence of<lb>memory accesses that can be aggregated into high-<lb>dimensional sparse correlated count vector sequences.<lb>While there are several state of the art BNP algo-<lb>rithms for clustering and their temporal extensions for<lb>prediction, there has been no work on exploring these<lb>for correlated count vectors. Our first contribution ad-<lb>dresses this gap by proposing a DP based mixture model<lb>of Multivariate Poisson (DP-MMVP) and its temporal<lb>extension(HMM-DP-MMVP) that captures the full co-<lb>variance structure of multivariate count data. However,<lb>modeling full covariance structure for count vectors is<lb>computationally expensive, particularly for high dimen-<lb>sional data. Hence, we exploit sparsity in our count<lb>vectors, and as our main contribution, introduce the<lb>Sparse DP mixture of multivariate Poisson(Sparse-DP-<lb>MMVP), generalizing our DP-MMVP mixture model,<lb>also leading to more efficient inference. We then discuss<lb>a temporal extension to our model for cache preloading.<lb>We take the first step towards mining historical<lb>data, to capture long range patterns in storage traces for<lb>cache preloading. Experimentally, we show a dramatic<lb>improvement in hitrates on benchmark traces and lay<lb>the groundwork for further research in storage domain<lb>to reduce latencies using data mining techniques to<lb>capture long range motifs.",
    "creator" : "LaTeX with hyperref package"
  }
}