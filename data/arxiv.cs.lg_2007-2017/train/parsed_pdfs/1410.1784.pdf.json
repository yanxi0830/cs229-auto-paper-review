{
  "name" : "1410.1784.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Stochastic Discriminative EM",
    "authors" : [ "Andrés R. Masegosa" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 0.\n17 84\nv1 [\ncs .L\nG ]\n2 O\nct 2\nStochastic discriminative EM (sdEM) is an online-EM-type algorithm for discriminative training of probabilistic generative models belonging to the exponential family. In this work, we introduce and justify this algorithm as a stochastic natural gradient descent method, i.e. a method which accounts for the information geometry in the parameter space of the statistical model. We show how this learning algorithm can be used to train probabilistic generative models by minimizing different discriminative loss functions, such as the negative conditional loglikelihood and the Hinge loss. The resulting models trained by sdEM are always generative (i.e. they define a joint probability distribution) and, in consequence, allows to deal with missing data and latent variables in a principled way either when being learned or when making predictions. The performance of this method is illustrated by several text classification problems for which a multinomial naive Bayes and a latent Dirichlet allocation based classifier are learned using different discriminative loss functions."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Online learning methods based on stochastic approximation theory [21] have been a promising research direction to tackle the learning problems of the so-called Big Data era [1, 10, 12]. Stochastic gradient descent (SGD) is probably the best known example of this kind of techniques, used to solve a wide range of learning problems [9]. This algorithm and other versions [29] are usually employed to train discriminative models such as logistic regression or SVM [10].\nThere also are some successful examples of the use of SGD for discriminative training of probabilistic generative models, as is the case of deep belief networks [19]. However,\nthis learning algorithm cannot be used directly for the discriminative training of general generative models. One of the main reasons is that statistical estimation or risk minimization problems of generative models involve the solution of an optimization problem with a large number of normalization constraints [26], i.e. those which guarantee that the optimized parameter set defines a valid probabilistic model. Although successful solutions to this problem have been proposed [16, 22, 26, 32], they are based on adhoc methods which cannot be easily extended to other statistical models, and hardly scale to large data sets.\nStochastic approximation theory [21] has also been used for maximum likelihood estimation (MLE) of probabilistic generative models with latent variables, as is the case of the online EM algorithm [13, 30]. This method provides efficient MLE estimation for a broad class of statistical models (i.e. exponential family models) by sequentially updating the so-called expectation parameters. The advantage of this approach is that the resulting iterative optimization algorithm is fairly simple and amenable, as it does not involve any normalization constraints.\nIn this paper we show that the derivation of Sato’s online EM [30] can be extended for the discriminative learning of generative models by introducing a novel interpretation of this algorithm as a natural gradient algorithm [3]. The resulting algorithm, called stochastic discriminative EM (sdEM), is an online-EM-type algorithm that can train generative probabilistic models belonging to the exponential family using a wide range of discriminative loss functions, such as the negative conditional log-likelihood or the Hinge loss. In opposite to other discriminative learning approaches [26], models trained by sdEM can deal with missing data and latent variables in a principled way either when being learned or when making predictions, because at any moment they always define a joint probability distribution. sdEM could be used for learning using large scale data sets due to its stochastic approximation nature and, as we will show, because it allows to compute the natural gradient of the loss function with no extra cost [3]. Moreover, if allowed by the generative model and the discriminative loss\nfunction, the presented algorithm could potentially be used interchangeably for classification or regression or any other prediction task. But in this initial work, sdEM is only experimentally evaluated in classification problems.\nThe rest of this paper is organized as follows. Section 2 provides the preliminaries for the description of the sdEM algorithm, which is detailed in Section 3. A brief experimental evaluation is given in Section 4, while Section 5 contains the main conclusions of this work."
    }, {
      "heading" : "2 PRELIMINARIES",
      "text" : ""
    }, {
      "heading" : "2.1 MODEL AND ASSUMPTIONS",
      "text" : "We consider generative statistical models for prediction tasks, where Y denotes the random variable (or the vectorvalue random variable) to be predicted, X denotes the predictive variables, and y⋆ denotes a prediction, which is made according to y⋆ = argmaxy p(y, x|θ).\nAssumption 1. The generative data model belongs to the exponential family with a natural (or canonical) parametrization\np(y, x|θ) ∝ exp(〈s(y, x), θ〉 −Al(θ))\nwhere θ is the so-called natural parameter which belongs to the so-called natural parameter space Θ ∈ ℜK , s(y, x) is the vector of sufficient statistics belonging to a convex set S ⊆ ℜK , 〈·, ·〉 denotes the dot product and Al is the log partition function.\nAssumption 2. We are given a conjugate prior distribution p(θ|α) of the generative data model\np(θ|α) ∝ exp(〈s(θ), α〉 −Ag(α))\nwhere the sufficient statistics are s(θ) = (θ,−Al(θ)) and the hyperparameter α has two components (ᾱ, ν). ν is a positive scalar and ᾱ is a vector also belonging to S [6]."
    }, {
      "heading" : "2.2 DUAL PARAMETERIZATION AND ASSUMPTIONS",
      "text" : "The so-called expectation parameter µ ∈ S can also be used to parameterize probability distributions of the exponential family. It is a dual set of the model parameter θ [2]. This expectation parameter µ is defined as the expected vector of sufficient statistics with respect to θ:\nµ , E [s(y, x)|θ] = ∫\ns(y, x)p(y, x|θ)dydx = ∂Al(θ)/∂θ\n(1)\nThe transformation between θ and µ is one-to-one: µ is a dual set of the model parameter θ [2]. Therefore, Equation (1) can be inverted as: θ = θ(µ). That is to say, for each θ ∈ Θ we always have an associated µ ∈ S and both parameterize the same probability distribution.\nFor obtaining the natural parameter θ associated to an expectation parameter µ, we need to make use of the negative of the entropy,\nH(µ) , ∫\np(y, x|θ(µ)) ln p(y, x|θ(µ))dydx = supθ∈Θ〈µ, θ〉 −Al(θ)\n(2)\nUsing the above function, the natural parameter θ can be explicitly expressed as\nθ = θ(µ) = ∂H(µ)/∂µ (3)\nEquations (1), (2), (3) define the Legendre-Fenchel transform.\nAnother key requirement of our approach is that it should be possible to compute the transformation from µ to θ in closed form:\nAssumption 3. The transformation from the expectation parameter µ to the natural parameter θ, which can be expressed as\nθ(µ) = argmax θ∈Θ 〈µ, θ〉 −Al(θ) (4)\nis available in closed form.\nThe above equation is also known as the maximum likelihood function, because θ( 1\nn ∑n i=1 s(yi, xi)) gives the max-\nimum likelihood estimation θ⋆ for a data set with n observations {(y1, x1), . . . , (yn, xn)}.\nFor later convenience, we show the following relations between the Fisher Information matrices I(θ) and I(µ) for the probability distributions p(y, x|θ) and p(y, x|θ(µ)), respectively [25]:\nI(θ) = ∂2Al(θ)\n∂θ∂θ =\n∂µ ∂θ = I(µ)−1 (5)\nI(µ) = ∂2H(µ)\n∂µ∂µ =\n∂θ ∂µ = I(θ)−1 (6)"
    }, {
      "heading" : "2.3 THE NATURAL GRADIENT",
      "text" : "Let W = {w ∈ ℜK} be a parameter space on which the function L(w) is defined. When W is a Euclidean space with an orthonormal coordinate system, the negative gradient points in the direction of steepest descent. That is, the negative gradient−∂L(w)/∂w points in the same direction as the solution to:\nargmin dw\nL(w + dw) subject to ||dw||2 = ǫ2 (7)\nfor sufficiently small ǫ, where ||dw||2 is the squared length of a small increment vector dw connecting w and w + dw. This justifies the use of the classical gradient descent method for finding the minimum of L(w) by taking steps (of size ρ) in the direction of the negative gradient:\nwt+1 = wt − ρ ∂L(wt)\n∂w (8)\nHowever, when W is a Riemannian space [4], there are no orthonormal linear coordinates, and the squared length of vector dw is defined by the following equation,\n||dw||2 = ∑\nij\ngij(w)dwidwj (9)\nwhere the K ×K matrix G = (gij) is called the Riemannian metric tensor, and it generally depends on w. G reduces to the identity matrix in the case of the Euclidean space [4].\nIn a Riemannian space, the steepest descent direction is not anymore the traditional gradient. That is, −∂L(w)/∂w is not the solution of Equation (7) when the squared length of the distance of dw is defined by Equation (9). Amari [3] shows that this solution can be computed by premultiplying the traditional gradient by the inverse of the Riemannian metric G−1,\nTheorem 1. The steepest descent direction or the natural gradient of L(w) in a Riemannian space is given by\n− ∂̃L(w)\n∂̃w = −G−1(w)\n∂L(w)\n∂w (10)\nwhere ∂̃L(w)/∂̃w denotes the natural gradient.\nAs argued in [3], in statistical estimation problems we should used gradient descent methods which account for the natural gradient of the parameter space, as the parameter space of a statistical model (belonging to the exponential family or not) is a Riemannian space with the Fisher information matrix of the statistical model I(w) as the tensor metric [2], and this is the only invariant metric that must be given to the statistical model [2]."
    }, {
      "heading" : "2.4 SATO’S ONLINE EM ALGORITHM",
      "text" : "Sato’s online EM algorithm [30] is used for maximum likelihood estimation of missing data-type statistical models. The model defines a probability distribution over two random or vector-valued variables X and Z , and is assumed to belong to the exponential family:\np(z, x|θ) ∝ exp(〈s(z, x), θ〉 −Al(θ))\nwhere (z, x) denotes a so-called complete data event. The key aspect is that we can only observe x, since z is an unobservable event. In consequence, the loss function ℓ(x, θ)1 is defined by marginalizing z: ℓ(x, θ) = − ln ∫ p(z, x)dz.\nThe online setting assumes the observation of a non-finite data sequence {(xt)}t≥0 independently drawn according to the unknown data distribution π. The objective function that EM seeks to minimize is given by the following expectation: L(θ) = E [ℓ(x, θ)|π].\n1We derive this algorithm in terms of minimization of a loss function to highlight its connection with sdEM.\nSato [30] derived the stochastic updating equation of online EM by relying on the free energy formulation, or lower bound maximization, of the EM algorithm [24] and on a discounting averaging method. Using our own notation, this updating equation is expressed as follows,\nµt+1 = (1− ρt)µt + ρtEz[s(z, xt|θ(µt)]\n= µt + ρt (Ez[s(z, xt|θ(µt)]− µt)\n= µt + ρt ∂ℓ(xt, θ(µt))\n∂θ (11)\nwhere Ez[s(z, xt|θ(µt)] denotes the expected sufficient statistics, Ez[s(z, xt|θ(µt)] = ∫ s(z, xt)p(z|xt, θ(µt))dz.\nHe proved the convergence of the above iteration method by casting it as a second order stochastic gradient descent using the following equality,\n∂ℓ(x, θ)\n∂θ =\n∂µ\n∂θ\n∂ℓ(x, θ(µ))\n∂µ = I(µ)−1\n∂ℓ(x, θ(µ))\n∂µ (12)\nThis equality is obtained by firstly applying the chain rule, followed by the equality shown in Equation (5). It shows that online EM is equivalent to a stochastic gradient descent with I(µt)−1 as coefficient matrices [9].\nSato noted that that the third term of the equality in Equation (12) resembles a natural gradient (see Theorem 1), but he did not explore the connection. But the key insights of the above derivation, which were not noted by Sato, is that Equation (12) is also valid for other loss functions different from the marginal log-likelihood; and that the convergence of Equation (11) does not depend on the formulation of the EM as a “lower bound maximization” method [24]."
    }, {
      "heading" : "3 STOCHASTIC DISCRIMINATIVE EM",
      "text" : ""
    }, {
      "heading" : "3.1 THE sdEM ALGORITHM",
      "text" : "We consider the following supervised learning setup. Let us assume that we are given a data set D with n observations {(y1, x1), . . . , (yn, xn)}. We are also given a discriminative loss function2 ℓ(yi, xi, θ). For example, it could be the negative conditional log-likelihood (NCLL) ℓ(yi, xi, θ) = − ln p(yi, xi|θ) + ln ∫\np(y, xi|θ)dy = − ln p(yi|xi, θ). Our learning problem consists in minimizing the following objective function:\nL(θ) =\nn ∑\ni=1\nℓ(yi, xi, θ)− ln p(θ|α)\n= E [ℓ(y, x, θ)|π]− 1\nn ln p(θ|α) (13)\nwhere π is now the empirical distribution of D and E [ℓ(y, x, θ)|π] the empirical risk. Although the above\n2The loss function is assumed to satisfy the mild conditions given in [9]. E.g., it can be a non-smooth function, such as the Hinge Loss.\nloss function is not standard in the machine learning literature, we note that when ℓ is the negative log-likelihood (NLL), we get the classic maximum a posterior estimation. This objective function can be seen as an extension of this framework.\nsdEM is presented as a generalization of Sato’s online EM algorithm for finding the minimum of an objective function in the form of Equation (13) (i.e. the solution to our learning problem). The stochastic updating equation of sdEM can be expressed as follows,\nµt+1 = µt − ρtI(µt) −1 ∂ℓ̄(yt, xt, θ(µt))\n∂µ (14)\nwhere (yt, xt) denotes the t-th sample, randomly generated from π, and the function ℓ̄ has the following expression: ℓ̄(yt, xt, θ(µt)) = ℓ((yt, xt, θ(µt)) + 1/n ln p(θ(µt)). We note that this loss function satisfies the following equality, which is the base for a stochastic approximation method [21], E [ ℓ̄(yt, xt, θ(µ))|π ] = L(θ(µ)).\nSimilarly to Amari’s natural gradient algorithm [3], the main problem of sdEM formulated as in Equation (14) is the computation of the inverse of the Fisher information matrix at each step, which becomes even prohibitive for large models. The following result shows that this can be circumvented when we deal with distributions of the exponential family:\nTheorem 2. In the exponential family, the natural gradient of a loss function with respect to the expectation parameters equals the gradient of the loss function with respect to the natural parameters,\nI(µ)−1 ∂ℓ̄(y, x, θ(µ))\n∂µ =\n∂ℓ̄(y, x, θ)\n∂θ\nSketch of the proof. We firstly need to prove that I(µ) is a valid Riemannian tensor metric and, hence, the expectation parameter space has a Riemanian structure defined by the metric I(µ) and the definition of the natural gradient makes sense. This can be proved by the invariant property of the Fisher information metric to one-to-one reparameterizations or, equivalently, transformations in the system of coordinates [2, 4]. I(µ) is a Riemannian metric because it is the Fisher information matrix of the reparameterized model p(y, x|θ(µ)), and the reparameterization is one-toone, as commented in Section 2.2.\nThe equality stated in the theorem follows directly from Sato’s derivation of the online EM algorithm (Equation (12)). This derivation shows that we can avoid the computation of I(µ)−1 by using the natural parameters instead of the expectation parameters and the function θ(µ).\nTheorem 1 simplifies the sdEM’s updating equation to,\nµt+1 = µt − ρt ∂ℓ̄(yt, xt, θ(µt))\n∂θ (15)\nsdEM can be interpreted as a stochastic gradient descent algorithm iterating over the expectation parameters and guided by the natural gradient in this Riemannian space.\nAlgorithm 1 Stochastic Discriminative EM (sdEM) Require: D is randomly shuffled.\n1: µ0 = ᾱ; (initialize according to the prior) 2: θ0 = θ(µ0); 3: t = 0; 4: repeat 5: for i = 1, . . . , n do 6: E-Step: µt+1 = µt − 1(1+λt) ∂ℓ̄(yi,xi,θt) ∂θ ;\n7: Check-Step: µt+1 = Check(µt+1,S);\n8: M-Step: θt+1 = θ(µt+1); 9: t = t+ 1;\n10: end for 11: until convergence 12: return θ(µt);\nAn alternative proof to Theorem 2 based on more recent results on information geometry has been recently given in [27]. The results of that work indicate that sdEM could also be interpreted as a mirror descent algorithm with a Bregman divergence as a proximitiy measure. It is beyond the scope of the paper to explore this relevant connection."
    }, {
      "heading" : "3.2 CONVERGENCE OF sdEM",
      "text" : "In this section we do not attempt to give a formal proof of the convergence of sdEM, since very careful technical arguments would be needed for this purpose [9]. We simply go through the main elements that define the convergence of sdEM as an stochastic approximation method [21].\nAccording to Equation (14), sdEM can be seen as a stochastic gradient descent method with the inverse of the Fisher information matrix I(µ)−1 as a coefficient matrix [9]. As we are dealing with exponential families, these matrices are always positive-definite. Moreover, if the gradient ∂ℓ̄(y, x, θ)/∂θ can be computed exactly (in Section 3.4 we discuss what happens when this is not possible), from Theorem 2, we have that it is an unbiased estimator of the natural gradient of the L(θ(µ)) defined in Equation 13,\nE\n[\n∂ℓ̄(y, x, θ)\n∂θ |π\n]\n= I(µ)−1 ∂L(θ(µ))\n∂µ (16)\nHowever, one key difference in terms of convergence between online EM and sdEM can be seen in Equation (11): µt+1 is a convex combination between µt and the expected sufficient statistics. Then, µt+1 ∈ S during all the iterations. As will be clear in the next section, we do not have this same guarantee in sdEM, but we can take advantage of the log prior term of Equation (13) to avoid this problem. This term plays a dual role as both “regularization”\nterm and log-barrier function [31] i.e. a continuous function whose value increases to infinity as the parameter approaches the boundary of the feasible region or the support of p(θ(µ)|α) 3. Then, if the step sizes ρt are small enough (as happens near convergence), sdEM will always stays in the feasible region S, due to the effect of the log prior term. The only problem is that, in the initial iterations, the step sizes ρt are large, so one iteration can jump out of the boundary of S. The method to avoid that depends on the particular model, but for the models examined in this work it seems to be a simple check in every iteration. For example, as we will see in the experimental section when implementing a multinomial Naive Bayes, we will check at every iteration that each sufficient statistic or “word count” is always positive. If a “word count” is negative at some point, we will set it to a very small value. As mentioned above, this does not hurt the convergence of sdEM because in the limit this problem disappears due the effect of the log-prior term.\nThe last ingredient required to assess the convergence of a stochastic gradient descent method is to verify that the sequence of step sizes satisfies: ∑ ρt = ∞, ∑ ρ2t < ∞.\nSo, if the sequence (µt)t≥0 converges, it will probably converge to the global minimum (µ⋆, θ⋆ = θ(µ⋆)) if L(θ) is convex, or to a local minimum if L(θ) is not convex [9].\nFinally, we give an algorithmic description of sdEM in Algorithm 1. Following [11], we consider steps sizes of the form ρt = (1 + λt)−1, where λ is a positive scalar4. As mentioned above, the “Check-Step” is introduced to guarantee that µt is always in S. Like the online EM algorithm [30, 13], Algorithm 1 resembles the classic expectation maximization algorithm [15] since, as we will see in the next section, the gradient is computed using expected\n3The prior p would need to be suitably chosen. 4Our experiments suggest that trying λ ∈ {1, 0.1, 0.01,\n0.001, . . .} suffices for obtaining a quick convergence.\nsufficient statistics. Assumption 3 guarantees that the maximization step can be performed efficiently. This step differentiates sdEM from classic stochastic gradient descent methods, where such a computation does not exist."
    }, {
      "heading" : "3.3 DISCRIMINATIVE LOSS FUNCTIONS",
      "text" : "As we have seen so far, the derivation of sdEM is complete except for the definition of the loss function. We will discuss now how two well known discriminative loss functions can be used with this algorithm."
    }, {
      "heading" : "Negative Conditional Log-likelihood (NCLL)",
      "text" : "As mentioned above, this loss function is defined as follows:\nℓCL(yt, xt, θ) = − ln p(yt, xt|θ) + ln\n∫\np(y, xt|θ)dy\nAnd its gradient is computed as\n∂ℓCL(yt, xt, θ)\n∂θ = −s(yt, xt) + Ey[s(y, xt)|θ]\nwhere the sufficient statistic s(yt, xt) comes from the gradient of the ln p(yt, xt|θ) term in the NCLL loss, and the expected sufficient statistic Ey[s(y, xt)|θ] = ∫\ns(y, xt)p(y|xt, θ)dy, comes from the gradient of the ln ∫\np(y, xt|θ)dy term in the NCLL loss. As mentioned above, the computation of the gradient is similar to the expectation step of the classic EM algorithm.\nThe iteration equation of sdEM for the NCLL loss is detailed in Table 1. We note that in the case of multi-class prediction problems the integrals of the updating equation are replaced by sums over the different classes of the class variable Y . We also show the updating equation for the negative log-likelihood (NLL) loss for comparison purposes."
    }, {
      "heading" : "The Hinge loss",
      "text" : "Unlike the previous loss which is valid for continuous and discrete (and vector-valued) predictions, this loss is only valid for binary or multi-class classification problems.\nMargin-based loss functions have been extensively used and studied by the machine learning community for binary and multi-class classification problems [5]. However, in our view, the application of margin-based losses (different from the negative conditional log-likelihood) for discriminative training of probabilistic generative models is scarce and based on ad-hoc learning methods which, in general, are quite sophisticated [26]. In this section, we discuss how sdEM can be used to minimize the empirical risk of one of the most used margin-based losses, the Hinge loss, in binary and multi-class classification problems. But, firstly, we discuss how Hinge loss can be defined for probabilistic generative models.\nWe build on LeCun et al.’s ideas [23] about energy-based learning for prediction problems. LeCun et al. [23] define the Hinge loss for energy-based models as follows,\nmax(0, 1− (E(ȳt, xt, w)− E(yt, xt, w))\nwhere E(·) is the energy function parameterized by a parameter vector w, E(yt, xt, w) is the energy associated to the correct answer yt and E(ȳt, xt, w) is the energy associated to the most offending incorrect answer, ȳt = argminy 6=yt E(y, xt, w). Predictions y\n⋆ are made using y⋆ = argminy E(y, xt, w\n⋆) when the parameter w⋆ that minimizes the empirical risk is found.\nIn our learning settings we consider the minus logarithm of the joint probability, − ln p(yt, xt|θ), as an energy function. In consequence, we define the hinge loss as follows\nℓhinge(yt, xt, θ) = max(0, 1− ln p(yt, xt|θ)\np(ȳt, xt|θ) ) (17)\nwhere ȳt denotes here too the most offending incorrect answer, ȳt = argmaxy 6=yt p(y, xt|θ).\nThe gradient of this loss function can be simply computed as follows\n∂ℓhinge(yt, xt, θ)\n∂θ =\n\n\n\n0 if ln p(yt,xt|θ) p(ȳt,xt|θ) > 1\n−s(yt, xt) + s(ȳt, xt) otherwise\nand the iteration equation for minimizing the empirical risk of the Hinge loss is also given in Table 1."
    }, {
      "heading" : "3.4 PARTIALLY OBSERVABLE DATA",
      "text" : "The generalization of sdEM to partially observable data is straightforward. We denote by Z the vector of nonobservable variables. sdEM will handle statistical models which define a probability distribution over (y, z, x) which belongs to the exponential family (Assumption 1). Assumption 2 and 3 remain unaltered.\nThe tuple (y, z, x) will denote the complete event or complete data, while the tuple (y, x) is the observed event or the observed data. So we assume that our given data set D with n observations is expressed as {(y1, x1), . . . , (yn, xn)}. So sdEM’s Equation (14) and (15) are the same, with the only difference that the natural gradient is now defined using the inverse of the Fisher information matrix for the statistical model p(y, z, x|θ(µ)). The same happens for Theorem 2.\nThe NCLL loss and the Hinge loss are equally defined as in Section 3.3, with the only difference that the computation of p(yt, xt|θ) and p(xt|θ) requires marginalization over z, p(yt, xt|θ) = ∫\np(yt, z, xt|θ)dz, p(xt|θ) = ∫\np(y, z, xt|θ)dydz. The updating equations for sdEM under partially observed data for the NCLL and Hinge loss are detailed in Table 2. New expected sufficient statistics need to be computed,\nEz [s(yt, z, xt)|θ] = ∫\ns(yt, z, xt)p(z|yt, xt, θ)dz and Eyz[s(y, z, xt)|θ] = ∫\ns(y, z, xt)p(y, z|xt, θ)dydz. As previously, we also show the updating equation for the negative log-likelihood (NLL) loss for comparison purposes."
    }, {
      "heading" : "3.5 sdEM AND APPROXIMATE INFERENCE",
      "text" : "For many interesting models [8], the computation of the expected sufficient statistics in the iteration equations shown in Table 1 and 2 cannot be computed in closed form. This is not a problem as far as we can define unbiased estimators for these expected sufficient statistics, since the equality of Equation (16) still holds. As it will be shown in the next section, we use sdEM to discriminatively train latent Dirichlet allocation (LDA) models [8]. Similarly to [28], for this purpose we employ collapsed Gibbs sampling to compute the expected sufficient statistics, Ez [s(yt, z, xt)|θ], as it guarantees that at convergence samples are i.i.d. according to p(z|yt, xt, θ)."
    }, {
      "heading" : "4 EXPERMINTAL ANALYSIS",
      "text" : ""
    }, {
      "heading" : "4.1 TOY EXAMPLE",
      "text" : "We begin the experimental analysis of sdEM by learning a very simple Gaussian naive Bayes model composed by a binary class variable Y and a single continuous predictor X . Hence, the conditional density of the predictor given the class variable is assumed to be normally distributed. The interesting part of this toy example is that the training data is generated by a different model: π(y = −1) = 0.5, π(x|y = −1) ∼ N(0, 3) and π(x|y = 1) ∼\n0.8 · N(−5, 0.1) + 0.2 · N(5, 0.1). Figure 1 shows the histogram of the 30,000 samples generated from the π distribution. The result is a mixture of 3 Gaussians, one in the center with a high variance associated to y = −1 and two narrows Gaussians on both sides associated to y = 1.\nsdEM can be used by considering 6 (non-minimal) sufficient statistics: N (−1) and N (1) as “counts” associated to both classes, respectively; S(−1) and S(1) as the “sum” of the x values associated to classes y = −1 and y = 1, respectively; and V (−1) and V (1) as the “sum of squares” of the x values for each class. We also have five parameters which are computed from the sufficient statistics as follows: Two for the prior of class p(y = −1) = p(−1) = N (−1)/(N (−1)+N (1)) and p(1) = N (1)/(N (−1)+N (1)); and four for the two Gaussians which define the conditional of X given Y , µ(−1) = S(−1)/N (−1), σ(−1) = √\nV (−1)/N (−1) − (S(−1)/N (−1))2, and equally for µ(1)\nand σ(1).\nThe sdEM’s updating equations for the NCLL loss can be written as follows\nN (k) t+1 = N (k) t + ρt(I[yt = k]− pt(k|xt)) + ρt n\nS (k) t+1 = (1 − ρt n )S (k) t + ρtxt (I[yt = k]− pt(k|xt))\nV (k) t+1 = (1 − ρt n )V (k) t + ρtx 2 t (I[yt = k]− pt(k|xt)) + ρt n\nwhere k indexes both classes, k ∈ {−1, 1}, I[·] denotes the indicator function, pt(k|xt) is an abbreviation of p(y = k|xt, θt), and θt is the parameter vector computed from the sufficient statistics at the t-th iteration.\nSimilarly, the sdEM’s updating equations for the Hinge loss can be written as follows,\nN (k) t+1 = N (k) t + kytρtI[ln\npt(yt|xt) pt(ȳt|xt) < 1] + ρt n\nS (k) t+1 = (1− ρt n )S (k) t + kytρtxtI[ln pt(yt|xt) pt(ȳt|xt) < 1]\nV (k) t+1 = (1 − ρt n )V (k) t + kytρtx 2 t I[ln pt(yt|xt) pt(ȳt|xt) < 1] + ρt n\nwhere the product kyt is introduced in the updating equations to define the sign of the sum, and the indicator function I[·] defines when the hinge loss is null.\nIn the above set of equations we have considered as a conjugate prior for the Gaussians a three parameter NormalGamma prior, ν = 1 and ᾱ1 = 0 for S(k) and ᾱ2 = 1 for V (k) [6, page 268], and a Beta prior with ν = 0 and ᾱ = 1 for N (k). We note that these priors assign zero probability to “extreme” parameters p(k) = 0 (i.e. N (k) = 0) and σ(k) = 0 (i.e. V (k)/N (k) − (S(k)/N (k))2 = 0).\nFinally, the“Check-step” (see Algorithm 1) performed before computing θt+1, and which guarantees that all sufficient statistics are correct, is implemented as follows:\nN (k) t+1 = max(N (k) t+1, ρt n )\nV (k) t+1 = max(V (k) t+1,\n(S (k) t+1) 2\nN (k) t+1\n+ ρt n )\nI.e., when the N (k) “counts” are negative or too small or when the V (k) values lead to negative or null deviations σ(k) ≤ 0, they are fixed with the help of the prior term.\nThe result of this experiment is given in Figure 1 and clearly shows the different trade-offs of both loss functions compared to maximum likelihood estimation. It is interesting to see how a generative model which does not match the underlying distribution is able to achieve a pretty high prediction accuracy when trained with a discrimintaive loss function (using the sdEM algorithm)."
    }, {
      "heading" : "4.2 sdEM FOR TEXT CLASSIFICATION",
      "text" : "Next, we briefly show how sdEM can be used to discriminatively train some generative models used for text classification, such as multinomial naive Bayes and a similar classifier based on latent Dirichlet allocation models [8]. Supplementary material with full details of these experiments and the Java code used in this evaluation can be download at: http://sourceforge.net/projects/sdem/"
    }, {
      "heading" : "Multinomial Naive Bayes (MNB)",
      "text" : "MNB assumes that words in documents with the same class or label are distributed according to an independent multinomial distribution. sdEM can be easily applied to train this\nmodel. The sufficient statistics are the “prior class counts” and the “word counts” for each class. The updating equations and the check step are the same as those of N (k)t in the previous toy example. Parameters of the MNB are computed simply through normalization operations. Two different conjugate Dirichlet distributions were considered: A “Laplace prior” where ᾱi = 1; and a ”Log prior” where ᾱi = “logarithm of the number of words in the corpus”. We only report analysis for “Laplace prior” in the case of NCLL loss and for “Log prior” in the case of Hinge loss. Other combinations show similar results, although NCLL was more sensitive to the chosen prior.\nWe evaluate the application of sdEM to MNB with three well-known multi-class text classification problems: 20Newsgroup (20 classes), Cade (12 classes) and Reuters21578-R52 (52 classes). Data sets are stemmed. Full details about the data sets and the train/test data sets split used in this evaluation can be found in [14].\nFigure 2 shows the convergence behavior of sdEM with λ =1e-05 when training a MNB by minimizing the Hinge loss (Hinge-MNB). In this figure, we plot the evolution of the Hinge loss but also the evolution of the NCLL loss and the normalized perplexity (i.e. the perplexity measure [8] divided by the number of training documents) at each epoch. We can see that there is a trade-off between the different losses. E.g., Hinge-MNB decreases the Hinge loss (as expected) but tends to increase the NCLL loss, while it\nonly decreases perplexity at the very beginning.\nFigure 3 displays the evolution of the classification accuracy of two MNBs trained minimizing the NCLL loss and the Hinge loss using sdEM. We compare them to: the standard MNB with a “Laplace prior”; the L2-regularized Logistic Regression; and the primal L2-regularized SVM. The two later methods were taken from the Liblinear toolkit v.18 [17]. As can be seen, sdEM is able to train simple MNB models with a performance very close to that provided by highly optimized algorithms."
    }, {
      "heading" : "Latent Dirichlet Allocation (LDA)",
      "text" : "We briefly show the results of sdEM when discriminatively training LDA models. We define a classification model equal to MNB, but where the documents of the same class are now modeled using an independent LDA model. We implement this model by using, apart from the “prior class counts”, the standard sufficient statistics of the LDA model, i.e. “words per hidden topic counts”, associated to each class label. Similarly to [28], we used an online Collapsed Gibbs sampling method to obtain, at convergence, unbiased estimates of the expected sufficient statistics (see Table 2).\nThis evaluation was carried out using the standard train/test split of the Reuters21578-R8 (8 classes) and web-kb (4 classes) data sets [14], under the same preprocessing than in the MNB’s experiments. Figure 4 shows the results of this comparison using 2-topics LDA models trained with the NCLL loss (NCLL-LDA), the Hinge loss (HingeLDA), and also the NLL loss (NLL-LDA) following the updating equations of Table 2. We compared these results with those returned by supervised-LDA (sLDA) [7] using the same prior, but this time with 50 topics because less topics produced worse results. We see again how a simple generative model trained with sdEM outperforms much more sophisticated models."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "We introduce a new learning algorithm for discriminative training of generative models. This method is based on a novel view of the online EM algorithm as a stochastic natural gradient descent algorithm for minimizing general discriminative loss functions. It allows the training of a wide set of generative models with or without latent variables, because the resulting models are always generative. Moreover, sdEM is comparatively simpler and easier to implement (and debug) than other ad-hoc approaches."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work has been partially funded from the European Union’s Seventh Framework Programme for research, technological development and demonstration under grant agreement no 619209 (AMIDST project)."
    }, {
      "heading" : "Appendices",
      "text" : "This supplementary material aims to extend, detail and complement the experimental evaluation of sdEM given in the main paper. The structure of this document is as follows. Section A details the experimental evaluation of the multinomial naive Bayes classifier and introduces new experiments comparing with the stochastic gradient descent algorithm [10]. The experimental evaluation of sdEM applied to latent Dirichlet allocation models is detailed and extended in Section B. Section C points to the software repository where all the software code used in this experimental evaluation can be downloaded to reproduce all these results."
    }, {
      "heading" : "A Multinomial Naive Bayes for text classification",
      "text" : ""
    }, {
      "heading" : "Description of the algorithm",
      "text" : "As commented in the main paper, a multinomial Naive Bayes (MNB) classifier assumes that the words of the documents with the same class labels are distributed according to an independent multinomial probability distribution. In this section we evaluate the use of sdEM to discriminatively train MNB models using the NCLL and the Hinge loss functions. In the first case, such a model would be related to a logistic regression model; while in the second case we will obtain a model directly related to a linear support vector machine classifier [20].\nThe general updating equations for this problem can be found in the main paper in Table 2. But a detailed pseudocode description is now given in Algorithm 2 for the NCLL loss and in Algorithm 4 for the Hinge loss. In both cases, the sufficient statistics are the ”prior class counts” stored in the matrix C and the ”word counts per class” stored in the matrix N . Matrix M is introduced to allow efficient computations of the posterior probability of the class variable given a document d, p(Y |d,N,M,C, γ). How this posterior is computed is detailed in Algorithm 3. In that way, the computational complexity of processing a label-document pair is linear in the number of words of the document. Finally, the function Normalize(·, ·) produces a multinomial probability by normalizing the vector of counts. The second argument contains the prior correction considered\nin this normalization, i.e. the value which is added to each single component of the count vector to avoid null probabilities, similar to what is done in Algorithm 3.\nIn both algorithms, we consider a Dirichlet distribution prior for the multinomial distributions. As detailed in the header of these algorithms, two different priors are considered: prior P1, with Dirichlet’s metaparameters αk = 1; and prior P2 with αk = ln |W |, where |W | denotes the total number of different words in the corpus. In both cases, the prior assigns null probability to parameters lying in the ”border” of the parameter space (i.e. when a null probability is assigned to some word).\nAs commented in the ”toy example” of the main paper in Section 4.1, the parametrization that we chose for this Dirichlet prior makes that the ν parameter, arising in the exponential family form of this prior (see Assumption 2 of the main paper), be equal to null, ν = 0. This can be seen when expressing the Dirichlet distribution in the following exponential form:\nDir(θ1, . . . , θk;α1, . . . , αK) =\n∏\nk Γ(αk)\nΓ( ∑ k αk) θα1−11 . . . θ αK−1 K\n= exp\n(\n∑\nk\n(αk − 1) ln θk + ∑\nk\nln Γ(αk)− ln Γ( ∑\nk\nαk)\n)\nThe second and third terms inside the exponent in the above equation correspond to the log partition function Ag(α) of the prior. The first term correspond to the dot product between the sufficient statistics (ln θ1, . . . , ln θK) and the natural parameters (α1 − 1, . . . , αk − 1). As can be seen, the ν parameter can be obviated in this definition, i.e. ν = 0."
    }, {
      "heading" : "Experimental Evaluation",
      "text" : "As detailed in the main paper, we evaluate the application of sdEM to MNB with three well-known multi-\nclass text classification problems: 20Newsgroup, Cade and Reuters21578-R52. These data sets are stemmed. Full details about the data sets and the train/test data sets split used in this evaluation can be found in [14]. Although Table 3 shows some of the main statistics of these data sets.\nFigure 5 (a), Figure 5 (b), Figure 6 (a) and Figure 6 (b) show the convergence behavior of sdEM with λ =1e-05 5 when training the MNB by minimizing the NCLL loss (NCLL-MNB) and by minimizing the Hinge loss (HingeMNB), respectively. In both cases, we plot the evolution of the NCLL loss, the Hinge loss and the normalized perplexity (i.e. the perplexity measure [8] divided by the number of training documents) of the trained model at each epoch. We can see that there is a trade-off between the different losses. For example, Hinge-MNB decreases the Hinge loss (as expected) but tends to increase the NCLL loss, while it only decreases perplexity at the very beginning. This last trend is much stronger when considering the P1 prior. A similar behavior can be observed for NCLL-MNB, with the main difference that the NCLL loss is an upper bound of the Hinge loss, and then when NCLL-MNB minimizes the NCLL loss it also minimizes the Hinge loss. Here it can be also observed that the perplexity remains quite stable specially for P1.\nFigure 5 (c) and Figure 6 (c) displays the evolution of the classification accuracy for the above models. We compare it to the standard MNB with a “Laplace prior” 6 and\n5Other values yield similar results and offer stable convergence, although at lower peace.\n6A “Log prior” was also evaluated but reported much worse results.\nwith L2-regularized Logistic Regression and primal L2regularized SVM implemented in the Liblinear toolkit v.18 [17]. For the case of the NCLL loss, the models seem to be more dependent of the chosen prior, specially for the Cade dataset. In any case, we can see that sdEM is able to train simple MNB models with a performance very close to that provided by highly optimized algorithms.\nA new set of experiments is included in this analysis comparing the MNB models learnt with sdEM with the classic stochastic gradient descent (SGD) algorithm. This evaluation is made using the Amazon12 and ACL-IMDB data sets (whose main details can be found in Table 3). We choose these data sets because they are binary classification problems, which are very well defined problems for logistic regression and linear SVM models. How SGD is used to train this model can be seen in [11].\nIn this evaluation we simply plot the evolution of the classification accuracy of the SGD algorithm when training a linear classifier using the NCLL loss (NCLL-SGD) and the Hinge loss (Hinge-SGD) with a L2 regularization for different learning rates or decreasing steps ρt. SGD is implemented as detailed in [11], where the weight of the regularized term is fixed to 1e-4. As recommended in [11], learning rates ρt for SGD are computed as follows: ρt = λ 1+λ·0.0001·t . We also look at the evolution of the classification accuracy of NCLL-MNB and Hinge-MNB with different priors in these two data sets and using different learning rates. In both cases, the plotted learning rates ρt are selected by using different λ values of the form λ ∈ {1, 0.1, 0.01, 0.001, 0.0001, 0.00001, . . .}. These results are shown in Figures 7 and 8. In each case, we con-\nsider the 5 consecutive λ values with the quickest convergence speed."
    }, {
      "heading" : "B Latent Dirichlet Allocation (LDA) for text classification",
      "text" : ""
    }, {
      "heading" : "Description of the algorithm",
      "text" : "As commented in the main paper, we depart from a classification model similar to MNB, but where the documents of the same class are now modeled using an independent LDA model instead of a multinomial distribution. The generative process of each label-document pair in the corpus would be as follows [8]:\n1. Choose class label y ∼ p(y|θY ), a multinomial probability.\n2. Choose N ∼ Poisson(ξy), the length of the document follows a Poisson distribution.\n3. Choose φy ∼ Dir(αy), a Dirichlet distribution with dimension |Z| (the meta-parameters are set to 1/|Z| in the experimental evaluation).\n4. For each of the N words wn:\n(a) Choose a topic zn ∼ Multinomial(φy) with dimension |Z|.\n(b) Choose a word wn ∼ p(wn|zn, βy), a multinomial probability conditioned on the topic zn.\nIn our case the unknown parameters are the βy for each class label, which defines the multinomial distribution of the step 4 (b) and the parameter θY which defines the prior.\nWe denote by d to a document as a bag of words d = {w1, . . . , wN} and we denote by zd to a particular hidden topic assignment vector for the words in d. Then the sufficient statistics for this model would be a three dimensional matrix indexed by k ∈ {1, ..., |Y |}, z ∈ {1, . . . , |Z|} and w ∈ {1, . . . , |W |}, where |W | denotes again the total number of different words in the corpus. The (k, z, w)-th component of this sufficient statistics matrix is computed as follows:\nsk,z,w(y, zd, d) = I[y = k] ∑\nn\nI[zn = z]I[wn = w]\nAs previously commented in the main paper, these sufficient statistics would correspond to the ”words per hidden topic counts”. By adding the ”prior class counts”, we would complete all the sufficient statistics that define this classification model.\nAs also commented in the main paper, similarly to [28], we used an online Collapsed Gibbs sampling method to obtain, at convergence, unbiased estimates of the expected sufficient statistics (see Section 3.5 in the main paper). This collapsed Gibbs sampling method makes used of the analytical marginalization of the parameter φy and samples in turn each of the indicator variables z1, . . . , zN . The probability of an indicator variable zn conditioned on all the words of the document and all the other indicators variables can be computed as follows:\np(zn|y, {zn′}n′ 6=n, d) ∝ βy,zn,wn · (S (−wn) zn + α) (18)\nwhere S(−wn)z = ∑\nn′ 6=n I[zn′ = z] and βy,zn,wn is the component of the β parameter vector which defines the probability that the n-th word in document d is equal to wn given that hidden topic is zn and the class label of the document is y, p(wn|zn, βy).\nThe above equation defines a Markov chain that when it is run generates unbiased samples from its stationary distribution, p(zn|d, y) (after discarding the first burn-in samples). So, we could then compute the expected sufficient statistics required to apply the sdEM algorithm over these models. Let us note that under our online settings the β parameter of Equation (18) is fixed to the values βt−1 estimated in the previous step and the this online collapsed Gibbs sampler only requires that the simulation is conditioned to the latent variables of the current observed document (i.e. it does not involve the hidden topics of the other documents in the corpus as happens with its batch counterpart).\nIn Algorithm 5 and Algorithm 7, we give a pseudocode description of the sdEM algorithm when applied to the this LDA classification model when using the NCLL and the Hinge loss functions, respectively. As can be seen, this algorithms does not directly relate to the standard LDA implementation, because we employ the same simplification used in the implementation7 of the sLDA algorithm [7] for multi-class prediction. This simplification assumes that all the occurrences of the same word in a document share the same hidden topic. The first effect of this assumption is that the number of hidden variables is reduced and the algorithm is much quicker. Whether this simplifying assumption has a positive or negative effect in the classification performance of the models is not evaluated here.\nLet us also see in the pseudo-code of these two algorithms, that Hinge-LDA will tend to be computationally more efficient than NCLL-LDA, because Hinge-LDA does not update any parameter when it classifies a document with a margin higher than 1. However, NCLL-LDA always updates all the parameters. When we deal with a high number of classes, this may imply a great difference in the computational performance. But this is something which is not evaluated in this first experimental study.\nWe also use a heuristic method8 to initialize the hidden topics variables zn of the incoming document which consists in sampling the hidden topics according to Equation 18, where S(−wn)zn is computed on-the-fly i.e. for the first word is a vector of zeros and, then, it is updated according to the sampled topics. It is similar to running collapsed Gibbs sampling for one iteration.\nWe emphasis again that these algorithms are based on the updating equations given in the Table 2 of the main paper.\n7Code available at http://www.cs.cmu.edu/∼chongw/slda/ 8It is proposed in http://shuyo.wordpress.com/2011/06/27/collapsed-gibbs-sampling-estimation-for-latent-dirichlet-allocation-3/."
    }, {
      "heading" : "Experimental Evaluation",
      "text" : "As previously commented in the paper, this evaluation was carried out using the standard train/test split of the Reuters21578-R8 and Web-KB data sets [14], under the same preprocessing than in the MNB’s experiments. In Table 3 some statistics about these data sets are given.\nWe used sdEM to train 2-topics LDA classification by minimizing the NCLL loss (NCLL-LDA), by minimizing the Hinge loss (Hinge-LDA), and also by minimizing the negative log-likelihood loss (NLL-LDA), following the updating equations of Table 2 in the main paper. We remind that at Figure 3 in the main paper, we show the results of the comparison of the classification accuracy of these models with the results obtained by supervised-LDA (sLDA) [7] using the same prior, but using 50 topics because with less topics it produced worse results.\nWe plot here at Figure 9, the convergence behavior at the training phase of the above models. The aim is to highlight how there is again a similar trade-off between the different losses when we train this model by minimizing a discriminative loss function such as NCLL or Hinge loss w.r.t. when we train this same model by minimizing a ”generative loss” such as the negative log-likelihood (NLL).\nLooking at these figures we can see like neither NCLLLDA nor Hinge-LDA decrease the perplexity loss in opposite to NLL-LDA. We can also see that NLL-LDA does decrease either the NCLL or the Hinge loss but not so successfully as NCLL-LDA or Hinge-LDA."
    }, {
      "heading" : "C sdEM Java Code",
      "text" : "All the code used to build all the experiments presented in this supplemental material or in the main paper can be downloaded from the following code repository ”https://sourceforge.net/projects/sdem/” (in ”Files” tab). This code is written in Java and mostly builds on Weka [18] data structures.\nAlgorithm 2 sdEM for Multinomial Naive Bayes with the NCLL loss. |d| denotes the number of different words in the current document d and |w|d to the number of times word w appears in document d. |W | denotes the total number of different words in the corpus. Require: D is randomly shuffled. Require: α value as prior count for each word. Two values are considered α = 1 and α = ln |W |.\n1: ∀k, w N [k][w] = α; C[k] = 1.0; M [k] = α ∗ |W |; 2: t = 0; 3: γ = 0; 4: repeat 5: for each label-document pair (y, d) do 6: t = t+ 1; 7: ρ = 11+λ·t 8: γ = γ + α · ρ\nn\n9: for each distinct word w in the document d do 10: N [y][w] = N [y][w] + ρ · |w|d · (1 − p(Y = y|d,N,M,C, γ)); 11: M [y] = M [y] + ρ · |w|d · (1− p(Y = y|d,N,M,C, γ)); 12: for k = 1, ..., |Y | : k 6= y do 13: oldV al = N [k][w]; 14: N [k][w] = N [k][w]− ρ · |w|d · ps(Y = k|d,N,M,C, γ); 15: N [k][w] = max(N [k][w], 0); 16: M [k] = M [k] + (N [k][w] − oldV al); 17: end for 18: end for 19: C[y] = C[y] + ρ · (1 − p(Y = y|d,N,M,C, γ)); 20: for k = 1, ..., |Y | : k 6= y do 21: C[k] = C[k]− ρ · p(Y = k|d,N,M,C, γ); 22: C[k] = max(C[k], 0); 23: end for 24: end for 25: until convergence 26: N̄ = Normalize(N, γ); 27: C̄ = Normalize(C, γ); 28: return N̄ and C̄;\nAlgorithm 3 Compute predictions P (Y = k|d,N,M,C, γ) with Multinomial Naive Bayes. |d| denotes the number of different words in the current document d and |w|d denotes the number of times word w appears in document d. The function ”Logs2Probs” simply exponentiate the log values and then normalize. Require: N , M , C, γ with non-negative values.\n1: ∀k LogDC[k] = 0.0; 2: for k = 1, ..., |Y | do 3: LogDC[k] = ln(C[k] + γ); 4: sumW = 0; 5: for each distinct word w in the document d do 6: LogDC[k] = LogDC[k] + |w|d · ln(N [y][w] + γ); 7: sumW = sumW + |w|d; 8: end for 9: LogDC[k] = LogDC[k]− sumW · ln(M [k] + γ · |d|);\n10: end for 11: return Logs2Probs(LogDC);\nAlgorithm 4 sdEM for Multinomial Naive Bayes with the Hinge loss. |d| denotes the number of different words in the current document d and |w|d denotes the number of times word w appears in document d. |W | denotes the total number of different words in the corpus. Require: D is randomly shuffled. Require: α value as prior count for each word. Two values are considered α = 1 and α = ln |W |.\n1: ∀k, w N [k][w] = α; C[k] = 1.0; M [k] = α ∗ |d|; 2: t = 0; 3: γ = 0; 4: repeat 5: for each label-document pair (y, d) do 6: t = t+ 1; 7: ρ = 11+λ·t 8: γ = γ + α · ρ\nn\n9: ȳ = argmaxy′ 6=y p(Y = y ′|x);\n10: if (ln p(Y = y|x)− ln p(Y = ȳ|x)) > 1 then 11: Go for the next document; 12: end if 13: for each distinct word w in the document d do 14: N [y][w] = N [y][w] + ρ · |w|d · (1 − p(Y = y|d,N,M,C, γ)); 15: M [y] = M [y] + ρ · |w|d · (1− p(Y = y|d,N,M,C, γ)); 16: oldV al = N [ȳ][w]; 17: N [ȳ][w] = N [ȳ][w]− ρ · |w|d · p(Y = ȳ|d,N,M,C, γ); 18: N [ȳ][w] = max(N [ȳ][w], 0); 19: M [k] = M [k] + (N [ȳ][w] − oldV al); 20: end for 21: C[y] = C[y] + ρ · (1 − p(Y = y|d,N,M,C, γ)); 22: C[ȳ] = C[ȳ]− ρ · p(Y = ȳ|d,N,M,C, γ); 23: C[ȳ] = max(C[ȳ], 0); 24: end for 25: until convergence 26: N̄ = Normalize(N, γ); 27: C̄ = Normalize(C, γ); 28: return N̄ and C̄;\nAlgorithm 5 sdEM for the LDA based classifier using the NCLL loss. |d| denotes the number of different words in the current document d, |w|d denotes the number of times word w appears in document d and |Z| denotes the number of hidden topics in the LDA model. Require: D is randomly shuffled. Require: η defines the prior for the ”word per topic counts”. In the experiments, it is fixed to η = 0.1.\n1: ∀k, z, w N [k][z][w] = η|Z| ; C[k] = 1.0; M [k][z] = |W | η |Z| ; 2: t = 0; 3: γ = 0; 4: repeat 5: for each label-document pair (y, d) do 6: t = t+ 1; 7: ρ = 11+λ·t 8: γ = γ + η|Z| · ρ n\n9: OnlineLDA(d,N [y],M [y],ρ, γ, ̟ = (1− p(Y = y|d,N,M,C, γ))); 10: for k = 1, ..., |Y | : k 6= y do 11: OnlineLDA(d,N [k],M [k],ρ, γ, ̟ = −p(Y = k|d,N,M,C, γ)); 12: end for 13: C[y] = C[y] + ρ · (1 − p(Y = y|d,N,M,C, γ)); 14: for k = 1, ..., |Y | : k 6= y do 15: C[k] = C[k]− ρ · p(Y = k|d,N,M,C, γ); 16: C[k] = max(C[k], 0); 17: end for 18: end for 19: until convergence 20: N̄ = Normalize(N, γ); 21: C̄ = Normalize(C, γ); 22: return N̄ and C̄;\nAlgorithm 6 OnlineLDA(d,N ,M ,ρ, γ, ̟). The vector s would correspond to the expected sufficient statistics for d computed by online collpased Gibbs sampling. Require: d,N ,M ,ρ, γ, ̟ properly computed.\n1: s = OnlineCollapsedGibbsSampling(d,N,M,γ); 2: for each distinct word w in the document d do 3: for z=1,...,|Z| do 4: oldV al = N [z][w]; 5: N [z][w] = N [z][w] + ρ ·̟ · s[z][w]; 6: N [z][w] = max(N [z][w], 0); 7: M [z] = M [z] + (N [z][w]− oldV al); 8: end for 9: end for\nAlgorithm 7 sdEM for the LDA based classifier using the Hinge loss. |d| denotes the number of different words in the current document d, |w|d denotes the number of times word w appears in document d and |Z| denotes the number of hidden topics in the LDA model. Require: D is randomly shuffled. Require: η defines the prior for the ”word per topic counts”. In the experiments, it is fixed to η = 0.1.\n1: ∀k, z, w N [k][z][w] = η|Z| ; C[k] = 1.0; M [k][z] = |W | η |Z| ; 2: t = 0; 3: γ = 0; 4: repeat 5: for each label-document pair (y, d) do 6: t = t+ 1; 7: ρ = 11+λ·t 8: γ = γ + η|Z| · ρ n 9: ȳ = argmaxy′ 6=y p(Y = y ′|x);\n10: if (ln p(Y = y|x)− ln p(Y = ȳ|x)) > 1 then 11: Go for the next document; 12: end if 13: OnlineLDA(d,N [y],M [y],ρ, γ, ̟ = (1− p(Y = y|d,N,M,C, γ))); 14: OnlineLDA(d,N [ȳ],M [ȳ],ρ, γ, ̟ = −p(Y = ȳ|d,N,M,C, γ)); 15: C[y] = C[y] + ρ · (1 − p(Y = y|d,N,M,C, γ)); 16: C[ȳ] = C[ȳ]− ρ · p(Y = ȳ|d,N,M,C, γ); 17: C[ȳ] = max(C[ȳ], 0); 18: end for 19: until convergence 20: N̄ = Normalize(N, γ); 21: C̄ = Normalize(C, γ); 22: return N̄ and C̄;"
    } ],
    "references" : [ {
      "title" : "Bayesian posterior sampling via stochastic gradient Fisher scoring",
      "author" : [ "Sungjin Ahn", "Anoop Korattikara Balan", "Max Welling" ],
      "venue" : "In ICML,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Differential-geometrical methods in statistics",
      "author" : [ "Shun-ichi Amari" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1985
    }, {
      "title" : "Natural gradient works efficiently in learning",
      "author" : [ "Shun-Ichi Amari" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1998
    }, {
      "title" : "Methods of information geometry, volume 191",
      "author" : [ "Shun-ichi Amari", "Hiroshi Nagaoka" ],
      "venue" : "American Mathematical Soc.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "Convexity, classification, and risk bounds",
      "author" : [ "Peter L Bartlett", "Michael I Jordan", "Jon D McAuliffe" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Bayesian theory, volume 405",
      "author" : [ "José M Bernardo", "Adrian FM Smith" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Supervised topic models",
      "author" : [ "David M Blei", "Jon D McAuliffe" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Latent Dirichlet allocation",
      "author" : [ "David M Blei", "Andrew Y Ng", "Michael I Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2003
    }, {
      "title" : "Online learning and stochastic approximations",
      "author" : [ "L. Bottou" ],
      "venue" : "On-line learning in neural networks, 17(9)",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "L. Bottou" ],
      "venue" : "Proceedings of COMPSTAT’2010, pages 177–186. Springer",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Stochastic gradient descent tricks",
      "author" : [ "L. Bottou" ],
      "venue" : "Neural Networks: Tricks of the Trade, pages 421–436. Springer",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The tradeoffs of large scale learning",
      "author" : [ "L. Bottou", "O. Bousquet" ],
      "venue" : "NIPS, volume 4, page 2",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "On-line expectation– maximization algorithm for latent data models",
      "author" : [ "Olivier C", "E. Moulines" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Improving Methods for Single-label Text Categorization",
      "author" : [ "Ana Cardoso-Cachopo" ],
      "venue" : "PdD Thesis,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "Maximum likelihood from incomplete data via the EM algorithm",
      "author" : [ "Arthur P Dempster", "Nan M Laird", "Donald B Rubin" ],
      "venue" : "Journal of the Royal statistical Society,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1977
    }, {
      "title" : "Structural extension to logistic regression: Discriminative parameter learning of belief net classifiers",
      "author" : [ "Greiner" ],
      "venue" : "Mach. Learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2005
    }, {
      "title" : "Liblinear: A library for large linear classification",
      "author" : [ "R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin" ],
      "venue" : "JMLR, 9:1871–1874",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "The weka data mining software: an update",
      "author" : [ "Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H Witten" ],
      "venue" : "ACM SIGKDD explorations newsletter,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Text categorization with support vector machines: Learning with many relevant features",
      "author" : [ "Thorsten Joachims" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1998
    }, {
      "title" : "Stochastic approximation algorithms and applications",
      "author" : [ "Harold Joseph Kushner", "G George Yin" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1997
    }, {
      "title" : "DiscLDA: Discriminative learning for dimensionality reduction and classification",
      "author" : [ "S. Lacoste-Julien", "F. Sha", "M.I. Jordan" ],
      "venue" : "NIPS, volume 83, page 85",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A tutorial on energy-based learning",
      "author" : [ "Yann LeCun", "Sumit Chopra", "Raia Hadsell", "M Ranzato", "F Huang" ],
      "venue" : "Predicting structured data,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2006
    }, {
      "title" : "A view of the EM algorithm that justifies incremental, sparse, and other variants",
      "author" : [ "Radford M Neal", "Geoffrey E Hinton" ],
      "venue" : "In Learning in graphical models,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1998
    }, {
      "title" : "Statistical exponential families: A digest with flash cards",
      "author" : [ "Frank Nielsen", "Vincent Garcia" ],
      "venue" : "arXiv preprint arXiv:0911.4863,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "Maximum margin Bayesian network classifiers",
      "author" : [ "F. Pernkopf", "M. Wohlmayr", "S. Tschiatschek" ],
      "venue" : "IEEE Trans. PAMI, 34(3):521–532",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The information geometry of mirror descent",
      "author" : [ "Garvesh Raskutti", "Sayan Mukherjee" ],
      "venue" : "arXiv preprint arXiv:1310.7780,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Online maximum-likelihood estimation for latent factor models",
      "author" : [ "D. Rohde", "O. Cappe" ],
      "venue" : "In Statistical Signal Processing Workshop (SSP),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2011
    }, {
      "title" : "Efficient estimations from a slowly convergent Robbins-Monro process",
      "author" : [ "David Ruppert" ],
      "venue" : "Technical report, Cornell University Operations Research and Industrial Engineering,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1988
    }, {
      "title" : "Convergence of on-line EM algorithm",
      "author" : [ "Masa-aki Sato" ],
      "venue" : "In Proc. of the Int. Conf. on Neural Information Processing,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2000
    }, {
      "title" : "Numerical optimization",
      "author" : [ "SJ Wright", "J Nocedal" ],
      "venue" : "volume 2. Springer New York",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Online learning methods based on stochastic approximation theory [21] have been a promising research direction to tackle the learning problems of the so-called Big Data era [1, 10, 12].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "Online learning methods based on stochastic approximation theory [21] have been a promising research direction to tackle the learning problems of the so-called Big Data era [1, 10, 12].",
      "startOffset" : 173,
      "endOffset" : 184
    }, {
      "referenceID" : 9,
      "context" : "Online learning methods based on stochastic approximation theory [21] have been a promising research direction to tackle the learning problems of the so-called Big Data era [1, 10, 12].",
      "startOffset" : 173,
      "endOffset" : 184
    }, {
      "referenceID" : 11,
      "context" : "Online learning methods based on stochastic approximation theory [21] have been a promising research direction to tackle the learning problems of the so-called Big Data era [1, 10, 12].",
      "startOffset" : 173,
      "endOffset" : 184
    }, {
      "referenceID" : 8,
      "context" : "Stochastic gradient descent (SGD) is probably the best known example of this kind of techniques, used to solve a wide range of learning problems [9].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 28,
      "context" : "This algorithm and other versions [29] are usually employed to train discriminative models such as logistic regression or SVM [10].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "This algorithm and other versions [29] are usually employed to train discriminative models such as logistic regression or SVM [10].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 18,
      "context" : "There also are some successful examples of the use of SGD for discriminative training of probabilistic generative models, as is the case of deep belief networks [19].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 25,
      "context" : "One of the main reasons is that statistical estimation or risk minimization problems of generative models involve the solution of an optimization problem with a large number of normalization constraints [26], i.",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 15,
      "context" : "Although successful solutions to this problem have been proposed [16, 22, 26, 32], they are based on adhoc methods which cannot be easily extended to other statistical models, and hardly scale to large data sets.",
      "startOffset" : 65,
      "endOffset" : 81
    }, {
      "referenceID" : 21,
      "context" : "Although successful solutions to this problem have been proposed [16, 22, 26, 32], they are based on adhoc methods which cannot be easily extended to other statistical models, and hardly scale to large data sets.",
      "startOffset" : 65,
      "endOffset" : 81
    }, {
      "referenceID" : 25,
      "context" : "Although successful solutions to this problem have been proposed [16, 22, 26, 32], they are based on adhoc methods which cannot be easily extended to other statistical models, and hardly scale to large data sets.",
      "startOffset" : 65,
      "endOffset" : 81
    }, {
      "referenceID" : 20,
      "context" : "Stochastic approximation theory [21] has also been used for maximum likelihood estimation (MLE) of probabilistic generative models with latent variables, as is the case of the online EM algorithm [13, 30].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "Stochastic approximation theory [21] has also been used for maximum likelihood estimation (MLE) of probabilistic generative models with latent variables, as is the case of the online EM algorithm [13, 30].",
      "startOffset" : 196,
      "endOffset" : 204
    }, {
      "referenceID" : 29,
      "context" : "Stochastic approximation theory [21] has also been used for maximum likelihood estimation (MLE) of probabilistic generative models with latent variables, as is the case of the online EM algorithm [13, 30].",
      "startOffset" : 196,
      "endOffset" : 204
    }, {
      "referenceID" : 29,
      "context" : "In this paper we show that the derivation of Sato’s online EM [30] can be extended for the discriminative learning of generative models by introducing a novel interpretation of this algorithm as a natural gradient algorithm [3].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 2,
      "context" : "In this paper we show that the derivation of Sato’s online EM [30] can be extended for the discriminative learning of generative models by introducing a novel interpretation of this algorithm as a natural gradient algorithm [3].",
      "startOffset" : 224,
      "endOffset" : 227
    }, {
      "referenceID" : 25,
      "context" : "In opposite to other discriminative learning approaches [26], models trained by sdEM can deal with missing data and latent variables in a principled way either when being learned or when making predictions, because at any moment they always define a joint probability distribution.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "sdEM could be used for learning using large scale data sets due to its stochastic approximation nature and, as we will show, because it allows to compute the natural gradient of the loss function with no extra cost [3].",
      "startOffset" : 215,
      "endOffset" : 218
    }, {
      "referenceID" : 5,
      "context" : "ν is a positive scalar and ᾱ is a vector also belonging to S [6].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "It is a dual set of the model parameter θ [2].",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "The transformation between θ and μ is one-to-one: μ is a dual set of the model parameter θ [2].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 24,
      "context" : "For later convenience, we show the following relations between the Fisher Information matrices I(θ) and I(μ) for the probability distributions p(y, x|θ) and p(y, x|θ(μ)), respectively [25]:",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 3,
      "context" : "However, when W is a Riemannian space [4], there are no orthonormal linear coordinates, and the squared length of vector dw is defined by the following equation,",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "G reduces to the identity matrix in the case of the Euclidean space [4].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "Amari [3] shows that this solution can be computed by premultiplying the traditional gradient by the inverse of the Riemannian metric G, Theorem 1.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 2,
      "context" : "As argued in [3], in statistical estimation problems we should used gradient descent methods which account for the natural gradient of the parameter space, as the parameter space of a statistical model (belonging to the exponential family or not) is a Riemannian space with the Fisher information matrix of the statistical model I(w) as the tensor metric [2], and this is the only invariant metric that must be given to the statistical model [2].",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 1,
      "context" : "As argued in [3], in statistical estimation problems we should used gradient descent methods which account for the natural gradient of the parameter space, as the parameter space of a statistical model (belonging to the exponential family or not) is a Riemannian space with the Fisher information matrix of the statistical model I(w) as the tensor metric [2], and this is the only invariant metric that must be given to the statistical model [2].",
      "startOffset" : 355,
      "endOffset" : 358
    }, {
      "referenceID" : 1,
      "context" : "As argued in [3], in statistical estimation problems we should used gradient descent methods which account for the natural gradient of the parameter space, as the parameter space of a statistical model (belonging to the exponential family or not) is a Riemannian space with the Fisher information matrix of the statistical model I(w) as the tensor metric [2], and this is the only invariant metric that must be given to the statistical model [2].",
      "startOffset" : 442,
      "endOffset" : 445
    }, {
      "referenceID" : 29,
      "context" : "Sato’s online EM algorithm [30] is used for maximum likelihood estimation of missing data-type statistical models.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 29,
      "context" : "Sato [30] derived the stochastic updating equation of online EM by relying on the free energy formulation, or lower bound maximization, of the EM algorithm [24] and on a discounting averaging method.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 23,
      "context" : "Sato [30] derived the stochastic updating equation of online EM by relying on the free energy formulation, or lower bound maximization, of the EM algorithm [24] and on a discounting averaging method.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 8,
      "context" : "It shows that online EM is equivalent to a stochastic gradient descent with I(μt) as coefficient matrices [9].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 23,
      "context" : "But the key insights of the above derivation, which were not noted by Sato, is that Equation (12) is also valid for other loss functions different from the marginal log-likelihood; and that the convergence of Equation (11) does not depend on the formulation of the EM as a “lower bound maximization” method [24].",
      "startOffset" : 307,
      "endOffset" : 311
    }, {
      "referenceID" : 8,
      "context" : "Although the above The loss function is assumed to satisfy the mild conditions given in [9].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 20,
      "context" : "We note that this loss function satisfies the following equality, which is the base for a stochastic approximation method [21], E [",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "Similarly to Amari’s natural gradient algorithm [3], the main problem of sdEM formulated as in Equation (14) is the computation of the inverse of the Fisher information matrix at each step, which becomes even prohibitive for large models.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "This can be proved by the invariant property of the Fisher information metric to one-to-one reparameterizations or, equivalently, transformations in the system of coordinates [2, 4].",
      "startOffset" : 175,
      "endOffset" : 181
    }, {
      "referenceID" : 3,
      "context" : "This can be proved by the invariant property of the Fisher information metric to one-to-one reparameterizations or, equivalently, transformations in the system of coordinates [2, 4].",
      "startOffset" : 175,
      "endOffset" : 181
    }, {
      "referenceID" : 26,
      "context" : "An alternative proof to Theorem 2 based on more recent results on information geometry has been recently given in [27].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "In this section we do not attempt to give a formal proof of the convergence of sdEM, since very careful technical arguments would be needed for this purpose [9].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 20,
      "context" : "We simply go through the main elements that define the convergence of sdEM as an stochastic approximation method [21].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 8,
      "context" : "According to Equation (14), sdEM can be seen as a stochastic gradient descent method with the inverse of the Fisher information matrix I(μ) as a coefficient matrix [9].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 30,
      "context" : "term and log-barrier function [31] i.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "So, if the sequence (μt)t≥0 converges, it will probably converge to the global minimum (μ, θ = θ(μ)) if L(θ) is convex, or to a local minimum if L(θ) is not convex [9].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 10,
      "context" : "Following [11], we consider steps sizes of the form ρt = (1 + λt), where λ is a positive scalar4.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 29,
      "context" : "Like the online EM algorithm [30, 13], Algorithm 1 resembles the classic expectation maximization algorithm [15] since, as we will see in the next section, the gradient is computed using expected The prior p would need to be suitably chosen.",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "Like the online EM algorithm [30, 13], Algorithm 1 resembles the classic expectation maximization algorithm [15] since, as we will see in the next section, the gradient is computed using expected The prior p would need to be suitably chosen.",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : "Like the online EM algorithm [30, 13], Algorithm 1 resembles the classic expectation maximization algorithm [15] since, as we will see in the next section, the gradient is computed using expected The prior p would need to be suitably chosen.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "Margin-based loss functions have been extensively used and studied by the machine learning community for binary and multi-class classification problems [5].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 25,
      "context" : "However, in our view, the application of margin-based losses (different from the negative conditional log-likelihood) for discriminative training of probabilistic generative models is scarce and based on ad-hoc learning methods which, in general, are quite sophisticated [26].",
      "startOffset" : 271,
      "endOffset" : 275
    }, {
      "referenceID" : 22,
      "context" : "’s ideas [23] about energy-based learning for prediction problems.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 22,
      "context" : "[23] define the Hinge loss for energy-based models as follows,",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "For many interesting models [8], the computation of the expected sufficient statistics in the iteration equations shown in Table 1 and 2 cannot be computed in closed form.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 7,
      "context" : "As it will be shown in the next section, we use sdEM to discriminatively train latent Dirichlet allocation (LDA) models [8].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 27,
      "context" : "Similarly to [28], for this purpose we employ collapsed Gibbs sampling to compute the expected sufficient statistics, Ez [s(yt, z, xt)|θ], as it guarantees that at convergence samples are i.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "Next, we briefly show how sdEM can be used to discriminatively train some generative models used for text classification, such as multinomial naive Bayes and a similar classifier based on latent Dirichlet allocation models [8].",
      "startOffset" : 223,
      "endOffset" : 226
    }, {
      "referenceID" : 13,
      "context" : "Full details about the data sets and the train/test data sets split used in this evaluation can be found in [14].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : "the perplexity measure [8] divided by the number of training documents) at each epoch.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 16,
      "context" : "18 [17].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 27,
      "context" : "Similarly to [28], we used an online Collapsed Gibbs sampling method to obtain, at convergence, unbiased estimates of the expected sufficient statistics (see Table 2).",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 13,
      "context" : "This evaluation was carried out using the standard train/test split of the Reuters21578-R8 (8 classes) and web-kb (4 classes) data sets [14], under the same preprocessing than in the MNB’s experiments.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : "We compared these results with those returned by supervised-LDA (sLDA) [7] using the same prior, but this time with 50 topics because less topics produced worse results.",
      "startOffset" : 71,
      "endOffset" : 74
    } ],
    "year" : 2014,
    "abstractText" : "Stochastic discriminative EM (sdEM) is an online-EM-type algorithm for discriminative training of probabilistic generative models belonging to the exponential family. In this work, we introduce and justify this algorithm as a stochastic natural gradient descent method, i.e. a method which accounts for the information geometry in the parameter space of the statistical model. We show how this learning algorithm can be used to train probabilistic generative models by minimizing different discriminative loss functions, such as the negative conditional loglikelihood and the Hinge loss. The resulting models trained by sdEM are always generative (i.e. they define a joint probability distribution) and, in consequence, allows to deal with missing data and latent variables in a principled way either when being learned or when making predictions. The performance of this method is illustrated by several text classification problems for which a multinomial naive Bayes and a latent Dirichlet allocation based classifier are learned using different discriminative loss functions.",
    "creator" : "LaTeX with hyperref package"
  }
}